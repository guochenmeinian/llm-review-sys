# An Analysis of Tokenization: Transformers under Markov Data

 Nived Rajaraman

UC Berkeley

nived.rajaraman@berkeley.edu &Jiantao Jiao

UC Berkeley

jiantao@berkeley.edu &Kannan Ramchandran

UC Berkeley

jiantao@berkeley.edu

###### Abstract

While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple \(k^{\text{th}}\)-order Markov processes for \(k>1\), transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically are incredibly slow or fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al., 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from \(k^{\text{th}}\)-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data.

## 1 Introduction

The training of language models is typically not an end-to-end process. Language models are often composed of a "tokenizer", which encodes a sequence of characters into a sequence of token ids, which map to substrings. The subsequent language modeling task is carried out by a neural network or transformer, which is pre-trained and fine-tuned on large datasets. The ideal goal is to jointly train the tokenizer and transformer with end-to-end accuracy as the objective. This is a challenging problem to solve efficiently, and thus, the tokenizer is generally adapted on a portion of the training dataset and frozen before the transformer is trained. In practice, byte-level/character level models such as ByT5 (Xue et al., 2022) and Canine(Clark et al., 2022) which avoid tokenization often perform worse for the reason that semantic relationships can be harder to capture at the character level (Libovicky et al., 2021; Itzhak and Levy, 2021).

Though used most commonly, tokenization at the subword level often has sharp edges. Test sequences may contain rare tokens which were never seen in the training dataset. The presence of such tokens may induce undesirable behavior in the outputs of models (Rumbelow and Watkins, 2023; Kharitonov et al., 2021; Yu et al., 2021) and present an attack surface for bad actors. Moreover, tokenized models struggle on tasks that involve manipulation at the character level, such as spelling out words or reversing sentences. For similar reasons, LLMs with standard tokenizers also struggle to carry out basic arithmetic (Golkar et al., 2023). Despite this brittleness, tokenization is used in nearly all state-of-the-art LLM architectures.

In this paper, we introduce a statistical formulation for tokenization for next-word-prediction. We study the class of models transformers are observed to express empirically under simple data generating processes, which often can have simpler descriptions. Taking a step back, rather than focusing on proxy evaluation metrics, which lead to an ever-changing goalpost, we focus on understanding the behavior of the end-to-end cross-entropy loss, \(\mathcal{L}(\cdot)\). In this paper, we study a simplification of real world data generating processes and study the case where data sources are \(k^{\text{th}}\)-order Markov processes. Within this framework we can compare tokenizers against each other, and in the process capture several interesting phenomena. Our main results are as follows,

1. There are very simple \(k^{\text{th}}\)-order Markov processes such that in the absence of any tokenization, transformers trained on data drawn this source empirically predict characters according to a unigram model. This phenomenon is observed under a wide variety of hyperparameter choices. This is problematic because unigram models such as that induced by the stationary distribution are poor at modeling Markovian data and suffer from a high cross-entropy loss. This phenomenon was also recently observed in Makkuva et al. (2024).
2. When trained with tokenization, transformers are empirically observed to break through this barrier and are able to capture the probability of sequences under the Markov distribution near-optimally. In other words, in the presence of tokenization, transformers appear to achieve near-optimal cross-entropy loss. This phenomenon is observed with a multitude of tokenizers used commonly in practice.
3. We analyze a toy tokenizer which adds all length-\(k\) sequences into the dictionary and show that as dictionary size grows, unigram models trained on the tokens get better at modeling the probabilities of sequences drawn from Markov sources. We then theoretically prove that tokenizers used in practice, such as the LZW tokenizer (Zouhar et al., 2023) and a variant of the BPE tokenizer (Gage, 1994; Sennrich et al., 2016) which are learnt from data also satisfy this property but require much smaller dictionaries to achieve any target cross-entropy loss.

In our framework, the most challenging hurdle and the biggest departure from previous work such as (Zouhar et al., 2023) is the element of generalization - understanding how a tokenizer performs on new sequences that it was not trained on. This generalization turns out to be a delicate phenomenon - we show in Appendix D that there exist tokenizers which generalize poorly in the sense that they may compress the dataset they are trained on into a short sequence of tokens, but fail to generalize to new sequences. In Appendix E we show that there exist dictionaries which generalize well (in the sense of having low cross-entropy loss) to new sequences under one encoding algorithm, but completely fail to generalize under another.

### Related Work

Tokenization has a long history of empirical study in natural language processing. In the literature, a number of tokenizers have been developed for various domains such as math (Singh and Strouse, 2024), code (Zheng et al., 2023; Parr, 2013) and morphology-aware tokenizers for different languages like Japanese (Tolmachev et al., 2018; Den et al., 2007) and Arabic (Alyafeai et al., 2023) among many others. In modern LLMs, the most commonly used tokenizers are variants of BPE (Gage, 1994), Wordpiece (Schuster and Nakajima, 2012) and the Unigram tokenizer (Kudo, 2018) which learn a dictionary from data, rather than hard-coding language dependent rules. There has been a long line of work interpreting tokenization from various lenses (Grefenstette and Tapanainen, 1994; Palmer, 2000; Zouhar et al., 2023).

The theoretical study of transformers has also received much attention recently. We discuss the closest relatives to our work below. Edelman et al. (2024) study the learning trajectory of transformers trained on data drawn from \(1^{\text{st}}\)-order Markov chains. While the authors empirically observe that the models eventually learn to predict tokens correctly according to the Markov kernel, simplicity bias slows down optimization - the models initially predict tokens according to a unigram model (in context unigrams), which delays learning the optimal solution. This phenomenon was also observed in Makkuva et al. (2024). On the positive side, Nichani et al. (2024) study an in-context causal learning task that generalizes learning in-context bigrams for \(1^{\text{st}}\)-order Markov processes and analyze the trajectory of gradient descent.

Notation.All logarithms are base \(e\), unless specified otherwise. The Shannon entropy \(H(X)\) of a categorical random variable \(X\) is \(-\sum_{x\in\text{supp}(X)}p(x)\log p(x)\). \(H_{\text{Ber}}(p)\) captures the entropy of a Bernoulli random variable with parameter \(p\). The notation \(O_{p,q,r}(f(n))\) (likewise \(\Omega_{\{\cdot\}}\) and \(\Theta_{\{\cdot\}}\)) indicate that the underlying constant depends polynomially on the parameters \(p,q\) and \(r\) and \(\widetilde{O}(f(n))\) (likewise, \(\widetilde{\Theta}\) and \(\widetilde{\Omega}\)) ignores \(\operatorname{polylog}(n)\) terms. For a set \(S\), \(S^{\star}=\cup_{k=1}^{\infty}S^{k}\), the set of all sequences with elements drawn from \(S\). For a sequence \(\bm{t}\), \(\bm{t}_{i:j}=(\bm{t}_{i},\bm{t}_{i+1},\cdots,\bm{t}_{j})\) returns a slice.

## 2 Formulation

We consider a setting where the learner's objective is to learn a language model which models probabilities of sequences over an input alphabet \(\mathcal{A}\). The data to be modeled is generated according to an unknown probability model \(P:\mathcal{A}^{\star}\to[0,1]\) over strings. A tokenizer is a tuple \(\mathcal{T}=(\text{Dict},\text{enc}(\cdot),\text{dec}(\cdot))\). Here \(\text{Dict}\) is a collection of tokens The encoding function \(\text{enc}(\cdot):\mathcal{A}^{\star}\to\text{Dict}^{\star}\), maps strings of characters to a sequence of tokens, and likewise, the decoding function \(\text{dec}(\cdot):\text{Dict}^{\star}\to\mathcal{A}^{\star}\) maps a sequence of tokens to a string of characters. We assume that the tokenizer is "consistent", namely, \(\text{dec}(\text{enc}(\cdot))\) is the identity function.

We consider a setting where the learner has access to a training dataset which is a sequence of length \(n\) sampled from a data source1. We study the likelihood maximization problem, where the objective of the learner is to learn an end to end model such that the cross-entropy loss is minimized. In the presence of tokenization, we have a model of the form \(Q_{\text{end}}=Q\circ\text{enc}(\cdot)\) where \(Q\) is a joint distribution across sequences of tokens when the tokenizer corresponding to \(\text{enc}(\cdot)\) is used. The cross-entropy loss, i.e. the log-perplexity, can be written down as,

Footnote 1: This can be thought of as the concatenation of all the individual sequences in the training dataset.

\[\mathcal{L}_{m}(Q_{\text{end}})\triangleq-\mathbb{E}[\log Q(\text{enc}(\bm{s} ))],\] (1)

with the objective to minimize it. Here, the expectation is over \(\bm{s}\), a fresh test sequence of length \(m\) sampled from the data generating process. Fixing a tokenizer, let \(\mathcal{Q}\) denote a family of joint distributions over tokens (i.e. likelihood models). The objective is to jointly design a tokenizer (with encoding function \(\text{enc}(\cdot)\)) and likelihood model \(Q\in\mathcal{Q}\) with small test loss \(\mathcal{L}_{m}(Q\circ\text{enc}(\cdot))\).

Finally, for a dictionary \(\text{Dict}\), the unigram family of models, \(\mathcal{Q}_{\text{1-gram}}\), is defined as below: \(Q\in\mathcal{Q}_{\text{1-gram}}\) associates probability \(Q(\bm{t}_{1},\bm{t}_{2},\cdots,\bm{t}_{j})=Q_{\#}(j)\prod_{i=1}^{j}Q_{\text{ tok}}(\bm{t}_{i})\) to the sequence of tokens \(\bm{t}_{1},\cdots,\bm{t}_{j}\) for measures \(Q_{\#}\) and \(Q_{\text{tok}}\) supported on \(\mathbb{N}\) and \(\text{Dict}\) respectively.

### Data generating process

In this paper, we consider a simplification of real-world data generating processes by considering the case where the data generating distribution is a \(k^{\text{th}}\)-order Markov process over characters. Studying the behavior of transformers trained on Markov data was the subject of the works Makkuva et al. (2024) and Edelman et al. (2024), where a number of interesting phenomena were unearthed. When a transformer is trained on data from certain simple Markov processes like the one considered in Figure 1, a very peculiar phenomenon occurs - within a reasonably large number of iterations, the transformer fails to improve beyond the loss incurred by the best unigram model. This phenomenon is reproducible across a wide number of hyperparameters, including the number of feed-forward layers in the model, the embedding dimension, and the number of attention heads. In Figure 2(a) this is made clearer - the transformer fails to improve its test loss beyond that of the best unigram model.

Figure 1: \(2\)_-state switching process._ The above state diagram describes the distribution of \(X_{n}\) conditioned on \(X_{n-1}\). \(k^{th}\)_-order extension:_ the conditional probability of \(X_{n}\) only depends on \(X_{n-k}\) through the kernel, \(\Pr(X_{n}=1|X_{n-k}=0)=p\) and \(\Pr(X_{n}=0|X_{n-k}=1)=q\).

How bad can a unigram model be? It turns out that the gap between the cross-entropy of the best unigram model and that of the optimal model can be characterized precisely.

**Theorem 2.1**.: _Consider any ergodic data source with stationary distribution over characters \(\pi\). The unconstrained optimal likelihood model achieves cross-entropy loss, \(\min_{Q}\mathcal{L}_{m}(Q)=H(P)\). In contrast, the cross-entropy loss under any unigram model \(Q\in\mathcal{Q}_{1\text{-gram}}\) satisfies, \(\mathcal{L}_{m}(Q)\geq mH(\pi)\)._

The ratio of the optimal loss \(H(P)\), and the optimal unigram loss, \(mH(\pi)\) can be arbitrarily large. In particular, for the switching chain in Figure 1, as \(p,q\to 0\), the ratio diverges to \(\infty\).

While transformers are a powerful class of models, it is concerning that they fail to learn very simple distributions such as \(k^{\text{th}}\)-order Markov processes. Why do they work so well in practice if they can be so slow to learn Markovian data? It turns out that there is a simple missing ingredient in all the architectures considered so far: tokenization. All the models trained in Figure 2(a) operate on raw character sequences drawn from the stochastic source. To understand the role of tokenization, we run another experiment and train the transformer on sequences generated from the stochastic source which are encoded into tokens by a BPE tokenizer learnt from data. The transformer now operates on sequences of tokens, rather than sequences of individual symbols. In Figure 2(b) we plot the results of this experiment - in the presence of tokenization, the cross-entropy loss of the end-to-end model breaks past the unigram barrier and approaches the optimal bound within a small number of iterations.

Let's peek into the model a bit more and understand its behavior. In Figure 2 we run the following experiment: we sample a random sequence of length \(2000\) from a Markov chain and feed it into the transformer after tokenization, resulting in \(\approx 500\) tokens. We plot the next-token distribution predicted by the transformer at every single position in the input, generated by autoregressive masking. In Figure 2 we stitch together these next-token distributions, each of which is a narrow column heatmap. Visually, we observe that the plot is approximately homogeneous along the \(x\)-axis, implying that the next-token distribution learned does not depend strongly on the prefix at that position. Thus the transformer learns what is essentially a unigram model.

Thus, we come to a surprising conclusion: the behavior of the transformer on the \(k^{\text{th}}\)-order switching source in Figure 1 with and without tokenization is essentially the same. In both cases, the model learns a unigram model over the tokens - in the absence of tokenization this unigram model is in fact the stationary distribution induced by the source. If the transformer learns a unigram model in both cases, how come there is such a large gap in performance between the two? To understand this in more detail, we analyze a toy tokenizer. As a simplification, we will analyze the behavior of an arbitrary, but exact unigram model under this tokenizer.

## 3 Unigram models under tokenization

Let's consider a toy tokenizer which assigns all possible substrings of length \(r\) as tokens in the dictionary and study what happens when a unigram model is trained on the tokenized sequences. The

Figure 2: Token distribution returned by the transformer tokenized by a learnt BPE encoder with a dictionary size of \(20\). A test sequence is generated from the stochastic source and encoded into a token sequence \(\bm{t}\). Each narrow vertical column represents the distribution over next tokens returned by the transformer when the first \(x\) tokens of \(\bm{t}\) are fed into the model, where \(x\) is varied from \(0\) to the length of \(\bm{t}\). For most values of \(x\), the model appears to predict the same distribution over the next token.

total dictionary size \(d=2^{r}\). A sequence of characters is mapped to a sequence of tokens by simply chunking it into a sequences of \(r\) characters which are replaced by the corresponding token index2. The resulting stochastic process on the tokens is still Markovian, but over a state space of size \(2^{r}\). For any unigram model \(Q\) on the tokens, the cross-entropy loss can be written down as,

Footnote 2: The last few characters which do not add up to \(r\) in total are left untokenized. These boundary effects will not matter as the test sequences grow in length

\[\mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot))=\mathbb{E}\left[\sum\nolimits_{ \boldsymbol{t}\in\mathsf{enc}(\boldsymbol{s})}\log(1/Q_{\text{tok}}( \boldsymbol{t}))\right]+\Theta(\log(m)),\]

where we choose \(Q_{\text{\#}}=\operatorname{Unif}([m])\), which contributes an additive \(\log(m)\) to the loss. Choosing \(Q_{\text{tok}}(\boldsymbol{t})=\pi(\boldsymbol{t}_{1})\prod_{i=1}^{r-1}P( \boldsymbol{t}_{i+1}|\boldsymbol{t}_{i})\) as the stationary probability the Markov process associates with \(\boldsymbol{t}\),

\[\frac{1}{m}\mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot)) \approx-\frac{1}{m}\mathbb{E}\left[\log(P(\boldsymbol{s})+\sum \nolimits_{i=0}^{m/k-1}\log\left(\frac{\pi(\boldsymbol{s}_{ki+1})}{P( \boldsymbol{s}_{ki+1}|\boldsymbol{s}_{ki})}\right)\right]\] \[\overset{(i)}{\approx}\frac{1}{m}H(P)+\frac{1}{mk}\left(mH(\pi) -H(P)\right)\] \[=\frac{H(P)}{m}\left(1-\frac{1}{\log_{2}(d)}\right)+\frac{H(\pi) }{\log_{2}(d)}.\] (2)

the approximation in \((i)\) uses the fact that as \(m\) grows large, \(\frac{1}{m}\sum_{i=0}^{m/k}\log(P(\boldsymbol{s}_{ki+\ell+1}|\boldsymbol{s}_{ ki+\ell})\) approaches \(\frac{H(P)}{k}\). With \(d=2\) (i.e., \(r=1\)), we recover the performance of the character tokenizer in Theorem 2.1. An immediate implication of this simple calculation is that as \(m\to\infty\), there is a unigram model which is nearly optimal as the dictionary size grows to \(\infty\).

While this toy tokenizer allows us to glean this intuition behind why tokenization allows unigram models to be near-optimal, there are some obvious issues. One, the tokenizer does not adapt to the distribution of the data. Indeed, for the switching Markov source in Figure 1, as \(p=q=\delta\to 0\), the source contains increasingly longer sequences of contiguous \(0\)'s and \(1\)'s. In this case, it makes since to have a dictionary containing such sequences, rather than all possible length-\(r\) sequences, many of which would be seen very few times (if at all) in a test sequence. At a more technical level, in eq. (2), to get to a cross-entropy loss of \(2H(P)\), the size of the dictionary required by the toy tokenizer is \(e^{mH(\pi)/H(P)}\). As discussed in Example A.1 for the switching Markov process with \(p=q=\delta\), this dictionary size can be extremely large and scales exponentially (in \(1/\delta\)) as \(e^{1/\delta\log(1/\delta)}\) when \(\delta\) is

Figure 3: Transformers trained on the order-\(2\) switching Markov process (Figure 1) with \(p=q=0.8\). On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size \(10\) learnt from data.

small. In general, on stochastic sources on a much larger alphabet, such as English/ASCII, this toy tokenizer would result in a prohibitively large dictionary.

Larger dictionaries are usually correlated with the presence of rare tokens which appear infrequently at training time. This presents a problem in practice - a lot more data is often required to see enough examples of such tokens to learn good embeddings for them. More importantly, in the absence of this volume of data, rare tokens present an attack surface to elicit undesirable behavior in the model (Rumbelow and Watkins, 2023). In practice, this issue present with the toy tokenizer is, to an extent, resolved by using tokenization algorithms such as BPE or Wordpiece, which learn dictionaries from data. In the process, they are able to avoid learning extremely rare tokens, by enforcing a lower bound on the number of their occurrences in the training data to be allocated as a token. By minimizing the number of such rare tokens, the model is able to utilize its token budget in a more efficient manner.

We now introduce the main theoretical result of this paper, showing that with the appropriate tokenization algorithm with a token budget of \(d\), a unigram model is not only asymptotically able to achieve the optimal cross-entropy loss, but also requires far smaller dictionaries to match the performance of the toy tokenizer considered earlier. In order to avoid dealing with the transient characteristics of the source, we consider the cross-entropy loss in eq. (1) under the assumption that the test sequences \(\bm{s}\) are of length \(m\to\infty\). Namely, define the normalized loss,

\[\mathcal{L}(\cdot)=\lim_{m\to\infty}\frac{1}{m}\mathcal{L}_{m}(\cdot)\]

**Theorem 3.1**.: _Consider a Markov data generating process which satisfies Assumption 3.2. Let \(d\) denote a budget on the size of the dictionary. Then, there exists a tokenizer with at most \(d\) tokens and encoding function \(\mathsf{enc}(\cdot)\), such that,_

\[\min_{\mathcal{Q}\in\mathcal{Q}_{1:\text{\emph{pow}}}}\mathcal{L}(Q\circ \mathsf{enc}(\cdot))\leq\frac{1}{1-\varepsilon}\min_{Q^{\prime}}\mathcal{L}(Q ^{\prime})\] (3)

_where \(\varepsilon\) is \(\log(1/\delta)/0.99\log(d)\)3. Furthermore, a tokenizer satisfying eq. (3) with probability \(\geq 1-d^{-\Omega_{\delta}(\log(d))}\) can be learnt from a dataset of \(\widetilde{O}_{\delta}(d)\) characters._

Footnote 3: \(\varepsilon\) is assumed to be \(<1\) in this statement. The constant \(0.99\) can be made arbitrarily close to \(1\).

The tokenizers considered in this theorem are far more efficient with their token budget than the toy tokenizer - to achieve a cross entropy loss within a factor \(2\) of optimal, the dictionary size required by these tokenizer is \(d\approx 1/\delta^{2}\) on any source satisfying Assumption 3.2. In comparison, the toy tokenizer requires a dictionary size of \(e^{1/\delta\log(1/\delta)}\) to achieve the same error. We show that the LZW tokenizer proposed in (Zouhar et al., 2023) achieves the upper bound in eq. (3) when trained on a dataset of size \(\widetilde{O}(d)\). Likewise, we also show that a sequential variant of BPE achieves the upper bound in eq. (3) up to a factor of \(2\) and with a worse dependency in \(\varepsilon\) when trained on a dataset of size \(\widetilde{O}(d^{2})\). What is interesting is that neither of these algorithms explicitly learn a unigram likelihood model, \(Q\), while constructing the dictionary. Yet they are able to perform as well as the tokenizers which are jointly optimized with a likelihood model, such as the Unigram tokenizer (Kudo, 2018).

Key insight.While the toy tokenizer provides a high level intuition as to why tokenization might enable unigram models to model Markov sources well, here we present a different explanation which captures tokenization from an operational viewpoint. Tokenizers which do a good job at learning patterns in the data and assigning these frequent patterns as tokens in the dictionary are compatible with an i.i.d. model over tokens. A hypothetical example motivating this point: consider a tokenizer such that the distribution of tokens in the encoding of a fresh string sampled from the source is distributed i.i.d., except that whenever the token \(\bm{t}^{\prime}\) appears, it is always followed by \(\bm{t}^{\prime\prime}\). An i.i.d. model on the tokens is a poor approximation since \(P(\bm{t}^{\prime}\bm{t}^{\prime\prime})\gg P(\bm{t}^{\prime})P(\bm{t}^{\prime \prime})\). However, by merging \(\bm{t}^{\prime}\) and \(\bm{t}^{\prime\prime}\) into a new token \(\bm{t}\) and adding this to the dictionary, the new distribution over tokens is i.i.d. In general, this motivates why it is desirable for a tokenizer to allocate new tokens to substrings which appear next to each other frequently, i.e. a pattern in the data. As more tokens are added to the dictionary, one might expect the cross-entropy loss incurred by the best unigram model to improve.

### Learning patterns in the source

The main result of this section is a generic reduction: dictionaries which typically encode new strings into a few long tokens (defined in a formal sense in Theorem 3.4), result in tokenizers achievingnear-optimal cross-entropy loss. We prove this result for Markovian sources under a regularity assumption, which is that the associated connectivity graph of the chain is complete. The analogous assumption for \(k^{\text{th}}\)-order sources is that the transition kernel is entry-wise bounded away from \(0\). This assumption is satisfied by all the sources considered in the paper thus far, such as the \(k^{\text{th}}\)-order switching processes in Figure 1.

**Assumption 3.2** (Data generating process).: Assume that the data source is an ergodic Markov process with transition \(P(\cdot|\cdot)\) and stationary distribution \(\pi\). Assume that \(\min_{a,a^{\prime}\in\mathcal{A}}P(a^{\prime}|a)\triangleq\delta>0\).

_Remark 3.3_.: Assumption 3.2 (and its \(k^{\text{th}}\)-order extension) impose that there is a small but non-zero probability of observing any particular symbol after any preceding sequence. This limits the applicability of these processes in real-world scenarios where such a phenomenon may not occur. However, our motivation for this assumption is different: \(\delta\) allows parameterizing the Markov process in a way which interpolates between i.i.d. (\(\delta=1/|\mathcal{A}|\)) and highly non-i.i.d. (\(\delta\to 0\)).

For a substring \(\bm{s}\) and a character \(a\), define \(P(\bm{s}|a)=P(\bm{s}_{1}|a)\prod_{i=2}^{|\bm{s}|}P(\bm{s}_{i}|\bm{s}_{i-1})\) denote the conditional probability of the substring \(\bm{s}\). We now state the main result of this section.

**Theorem 3.4** (Bound on cross-entropy loss of dictionaries under greedy encoder).: _Consider a source satisfying Assumption 3.2 and any tokenizer \(\mathcal{T}\) equipped with the greedy encoder, \(\text{enc}_{\text{gre}}(\cdot)\) with finitely long tokens. Define, \(P(\bm{t})=\mathbb{E}_{a\sim\pi}[P(\bm{t}|a)]\) and suppose \(H(Q_{\text{MLE}},P)\geq\frac{1}{\varepsilon}\log(1/\delta)\) for some \(\varepsilon<1\). Then,_

\[\min_{Q\in\mathcal{Q}_{1:\text{pw}}}\mathcal{L}(Q\circ\text{enc}_{\text{gre} }(\cdot))\leq\frac{\min_{Q}\mathcal{L}(Q)}{1-\varepsilon}.\]

Interpretation.\(H(Q_{\text{MLE}},P)=\mathbb{E}_{\bm{t}\sim Q_{\text{MLE}}}[\log(1/P(\bm{t}))]\) is large when the encoder places higher mass (i.e. larger values of \(Q_{\text{MLE}}(\cdot)\)) on tokens which have low probability under \(P\), i.e. which correspond to longer substrings. Intuitively, this metric is higher for tokenizers which typically use long tokens (i.e. low \(P(\cdot)\)) to encode new strings.

### LZW tokenizer

In this section we study the Lempel-Ziv-Welch (LZW) based tokenization scheme introduced by Zouhar et al. (2023) and establish guarantees of the form of Theorem 3.1 for this tokenizer.

**Definition 3.5** (LZW tokenizer).: Iterating from left to right, the shortest prefix of the training dataset which does not already exist as a token is assigned as the next token in the dictionary. This substring is removed and the process is iterated on the remainder of the dataset. The tokenizer uses the greedy encoding algorithm (Definition A.3) to encode new strings into tokens.

_An example of the LZW tokenizer:_ For the dataset \(010011\), the dictionary created is \(\{0,1,00,11\}\).

The LZW tokenizer is based on the LZW algorithm for compression (Ziv and Lempel, 1978; Welch, 1984). The dictionary satisfies the property that if some substring \(\bm{s}^{\prime}\) exists as a token in the dictionary, then all of its prefixes must also belong to the dictionary. In the next theorem, we show that the LZW tokenizer approximately achieves the optimal cross-entropy loss.

**Theorem 3.6**.: _Suppose the LZW tokenizer is trained on a dataset of length at most \(d\) (thereby learning a dictionary with at most \(d\) tokens). For Markov sources satisfying Assumption 3.2, with probability \(\geq 1-d^{-O_{\delta}(\log(d))}\), the resulting tokenizer satisfies,_

\[\min_{Q\in\mathcal{Q}_{1:\text{pw}}}\mathcal{L}(Q\cdot\text{enc}_{\text{gre} }(\cdot))\leq\frac{\min_{Q}\mathcal{L}(Q)}{1-\varepsilon}.\]

_where \(\varepsilon=\frac{\log(1/\delta)}{0.99\log(d)}4\)._

The proof of this result considers all substrings \(\bm{t}\) with \(P(\bm{t})\geq 1/d^{0.99}\). These substrings are reasonably high probability and observed many times in a dataset of \(\widetilde{\Omega}(d)\) characters. We show that with high probability, the LZW tokenizer learns _all_ of these substrings as tokens in the dictionary. Now, when processing a new string, since the greedy algorithm only emits the longest substringwhich matches a token, every token allocated must fall on the "boundary" of this set, having \(P(\bm{t})\leq O(1/d^{0.99})\). By definition, this means that \(H(Q_{\text{MLE}},P)=\mathbb{E}_{\bm{t}\sim Q_{\text{MLE}}}[\log(1/P(\bm{t}))]=0.99 \log(d)\). Combining this with Theorem 3.4 completes the proof. At a high level, on the infinite tree of substrings \(\mathcal{A}^{\star}\) we study which nodes are populated as tokens by LZW. This structure forms a Digital Search Tree (DST) and prior work analyzes the mean and variance of the profile of the DST under various source processes (Jacquet et al., 2001; Drmota and Szpankowski, 2011; Hun and Vallee, 2014; Drmota et al., 2021). A detailed proof of Theorem 3.6 is provided in Appendix A.6.

## 4 Experimental Results

Experiment 1 (Figures 3(a) and 3(b))In this experiment we study the order-\(1\) switching Markov chain. Transformers without tokenization empirically achieve a small cross-entropy on this learning task as seen in Figure 3(a) and earlier in Makkuva et al. (2024). We vary hyperparameters to find the smallest untokenized model which achieves a loss within \(10\%\) of the optimal-cross entropy within \(300\) epochs. Fixing a token dictionary size of \(20\), we also find the smallest tokenized model which achieves the same loss. Although the smallest model with tokenization is larger than the smallest model without tokenization in terms of the number of parameters, the wall-clock time taken to optimize the model to any target test loss is observed to be smaller. Thus, tokenization appears to reduce the compute time required to train the model to a target test loss in the toy example we consider. In Figure 3(b) we compare models with the same architecture trained with and without tokenization5. The model with tokenization appears to converge more quickly, although the limiting error achieved is subtly higher in comparison with the model without tokenization.

Footnote 5: The model with tokenization has width equal to the typical length of sequences _after_ encoding, which is smaller.

Experiment 1 (Figure 5).In this experiment, we train tokenizers on the Wikitext-103-raw-v1 dataset (Merity et al., 2016) and compare the performance of unigram models trained on the GLUE dataset as the model size scales. Since the character-level tokenizer operates on a fixed vocabulary, in order to compare with the other tokenizers, we plot the number of unique \(k\)-grams observed in the training dataset along the \(x\)-axis. While this is not an apples-to-apples comparison, we use the number of unique \(k\)-grams in the dataset as a proxy for the complexity of the likelihood model trained.

Figure 4: Test loss vs. wall-clock time for the tokenized and untokenized models when trained on the order-\(1\) switching Markov chain (Figure 1) with \(p=q=0.8\). The tokenizer used is BPE.

One may also use the total number of possible \(k\)-grams as a proxy; however a large fraction of these \(k\)-grams would likely never be observed in a real dataset (especially as \(k\) grows).

Experiment 2 (Table 1).In this experiment, we compare the cross entropy loss of the best unigram model trained on pre-trained tokenizers on an array of datasets. All the considered tokenizers have dictionary sizes in the range 31K-51K. The best bigram model under the character tokenizer is consistently outperformed by the best unigram likelihood model trained under a number of pre-trained tokenizers on a variety of datasets: Rotten Tomatoes (8.5K sequences), GLUE (105K), Yelp review (650K) and Wikitext-103-v1 (1.8M).

### Additional theoretical results

We present some additional theoretical results in the appendix which we discuss briefly below. In Appendix B, we do a theoretical study of the cross-entropy loss achieved by the popular BPE tokenizer. We show that a variant of BPE achieves the upper bound on the RHS of eq.3 (Theorem 3.1) up to a factor approaching \(2\) as the dictionary size grows. It is an interesting question for future research to understand whether this factor of \(2\) can be removed, since transformers are observed to achieve the near-optimal cross-entropy loss as the dictionary size grows (cf. Figure 2(b)). In Appendix C, we prove finite sample bounds on the end-to-end model under the LZW tokenizer with a smoothed empirical estimator as the unigram model. This analysis reveals that there is a sweet spot for the dictionary size - too small a dictionary, and the statistical error floor is significant, too large a dictionary, and the statistical error incurred by the likelihood model dominates the overall loss. We also take a closer look into the aspect of generalization for tokenizers, which arises from the fact that the tokenizer is evaluated on data that it was not trained on. Prior work such as Zouhar et al. (2023b) show that BPE is an approximation algorithm for finding the sequence of merges which minimizes the size of the compressed dataset. This does not imply any guarantees on the end-to-end performance, or even compression power of the tokenizer on new sequences. In particular, in Appendix D we show that there exist tokenizers which compress the dataset into a short sequence of tokens, but do so in a way which fails to generalize to new sequences. Thus measuring the performance of a tokenizer necessitates understanding its behavior on data it was not trained on. In Appendix E, we show a different kind of intricacy - there exist tokenizers under which the best unigram model achieves low cross-entropy loss. However, the same dictionary under a different encoding algorithm performs nearly as poorly as the character-level tokenizer. The interaction between the dictionary and encoding algorithm is a poorly studied subject in the tokenization literature; this result emphasizes the importance of understanding this relationship.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & RT & Wiki & Yelp & GLUE \\ \hline BERT & 1.58 & 1.55 & 1.60 & 1.50 \\ \hline Tinyllama & 1.75 & 1.84 & 1.82 & 1.70 \\ \hline GPT-neox & 1.57 & 1.64 & 1.66 & 1.48 \\ \hline Mistral & 1.69 & 1.80 & 1.75 & 1.66 \\ \hline Phi-2 & 1.54 & 1.62 & 1.64 & 1.45 \\ \hline \hline Character & 2.40 & 2.45 & 2.46 & 2.38 \\ \hline \end{tabular}
\end{table}
Table 1: Cross-entropy loss estimates (using eq.55) of unigram models trained on pre-trained tokenizers under a number of datasets. The last row (blue) is the character level tokenizer, on which a more powerful bigram model is trained. BERT is based on Wordpiece, and the remaining tokenizers are BPE based. The character-level tokenizer we use is ByT5.

Figure 5: _Performance vs. dictionary size._ Tokenizers are trained on the Wikitext-103 dataset. For all other tokenizers we train unigram models while for the the character-level tokenizer, we train \(k\)-gram models for \(k\in\{1,2,3,4\}\). Likelihood models are trained on the GLUE dataset. The parentheses indicates the number of distinct observed \(k\)-grams, which lower bounds the \(k\)-gram model complexity.

Open questions

In this section, we discuss some limitations of our work and open questions stemming from them. We show that when transformers are trained with or without tokenization, they learn to approximately represent \(k\)-gram models for different values of \(k\). Transformers are capable of representing far more complex behavior, which are elicited under more complex data generating processes. Extending our formulation to these settings presents an avenue to develop an even better understanding of tokenization, and would allow finer-grained comparisons between tokenizers. The behavior and role of tokenizers may be very different in these contexts. Below we discuss some concrete questions.

Our theory assumes that the underlying Markov chain has every transition occurring with non-zero probability, which is a limitation. However, the analysis for the toy tokenizer in eq.2 shows that when the dictionary size scales as \(\exp(mH(\pi)/H(P))\), even in the absence of Assumption3.2, the tokenizer achieves the optimal cross-entropy to within a factor of \(2\). This leads to the following conjecture.

**Conjecture 1**.: In the spirit of eliminating Assumption3.2, is it possible to establish a version of Theorem3.1 applicable to data drawn from any Markov chain, where \(\varepsilon=\log(1/\delta)/0.99\log(d)\) is replaced by \(\varepsilon=\log(mH(\pi)/H(P))/0.99\log(d)\).

In AppendixB, we analyze a variant of the BPE tokenizer, which carries out a version of sample splitting, and establish a weaker variant of Theorem3.1 for this tokenizer. This is to simplify the statistical dependencies arising from the fact that while learning its dictionary, BPE makes a run over the entire training dataset each time a new token is added. It remains an open question to analyze and establish a variant of Theorem3.1 for the standard BPE tokenizer.

## 6 Conclusion

We present a theoretical framework to compare and analyze different tokenization algorithms. We study the end-to-end cross-entropy loss of the tokenizer + likelihood model, and focus on the case where the data generating process is Markovian. We empirically observe that transformers with tokenization are drastically more efficient at learning \(k^{\text{th}}\)-order Markov processes, compared to without tokenization. We prove that algorithms such as LZW and a sequential variant of BPE learn tokenizers such that the best unigram likelihood model trained on them approaches the cross-entropy loss of the optimal likelihood model, as the vocabulary size \(d\) grows.

## 7 Acknowledgements

JJ and NR were partially supported by NSF Grants IIS-1901252 and CCF-2211209. KR was partially supported by NSF Grant CCF-2211209.

## References

* Alyafeai et al. [2023] Zaid Alyafeai, Maged S Al-shaibani, Mustafa Ghaleb, and Irfan Ahmad. Evaluating various tokenizers for arabic text classification. _Neural Processing Letters_, 55(3):2911-2933, 2023.
* Braess and Sauer [2004] Dietrich Braess and Thomas Sauer. Bernstein polynomials and learning theory. _Journal of Approximation Theory_, 128(2):187-206, 2004.
* Chen [2018] Yen-Chi Chen. Stochastic modeling of scientific data, Autumn 2018.
* Clark et al. [2022] Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efficient tokenization-free encoder for language representation. _Transactions of the Association for Computational Linguistics_, 10:73-91, 2022.
* Den et al. [2007] Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, Atsushi Yamada, Nobuaki Minematsu, Kiyotaka Uchimoto, and Hanae Koiso. The development of an electronic dictionary for morphological analysis and its application to japanese corpus linguistics, Oct 2007. URL https://repository.ninjal.ac.jp/api/records/2201.

Michael Drmota and Wojciech Szpankowski. The expected profile of digital search trees. _Journal of Combinatorial Theory, Series A_, 118(7):1939-1965, 2011.
* Drmota et al. [2021] Michael Drmota, Michael Fuchs, Hsien-Kuei Hwang, and Ralph Neininger. Node profiles of symmetric digital search trees: Concentration properties. _Random Structures & Algorithms_, 58(3):430-467, 2021.
* Edelman et al. [2024] Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains. _arXiv preprint arXiv:2402.11004_, 2024.
* Eisner et al. [2015] Tanja Eisner, Balint Farkas, Markus Haase, and Rainer Nagel. _Operator theoretic aspects of ergodic theory_, volume 272. Springer, 2015.
* Gage [1994] Philip Gage. A new algorithm for data compression. _C Users Journal_, 12(2):23-38, 1994.
* Golkar et al. [2023] Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: A continuous number encoding for large language models. _arXiv preprint arXiv:2310.02989_, 2023.
* Gray and Gray [2009] Robert M Gray and RM Gray. _Probability, random processes, and ergodic properties_, volume 1. Springer, 2009.
* Grefenstette and Tapanainen [1994] Gregory Grefenstette and Pasi Tapanainen. What is a word, what is a sentence?: problems of tokenisation. 1994.
* Han et al. [2021] Yanjun Han, Soham Jana, and Yihong Wu. Optimal prediction of markov chains with and without spectral gap. _Advances in Neural Information Processing Systems_, 34:11233-11246, 2021.
* Hun and Vallee [2014] Kanal Hun and Brigitte Vallee. Typical depth of a digital search tree built on a general source. In _2014 Proceedings of the Eleventh Workshop on Analytic Algorithmics and Combinatorics (ANALCO)_, pages 1-15. SIAM, 2014.
* Itzhak and Levy [2021] Itay Itzhak and Omer Levy. Models in a spelling bee: Language models implicitly learn the character composition of tokens. _arXiv preprint arXiv:2108.11193_, 2021.
* Jacquet et al. [2001] Philippe Jacquet, Wojciech Szpankowski, and Jing Tang. Average profile of the lempel-ziv parsing scheme for a markovian source. _Algorithmica_, 31:318-360, 2001.
* Kharitonov et al. [2021] Eugene Kharitonov, Marco Baroni, and Dieuwke Hupkes. How bpe affects memorization in transformers. _arXiv preprint arXiv:2110.02782_, 2021.
* Kudo [2018] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. _arXiv preprint arXiv:1804.10959_, 2018.
* Larsson and Moffat [2000] N Jesper Larsson and Alistair Moffat. Off-line dictionary-based compression. _Proceedings of the IEEE_, 88(11):1722-1732, 2000.
* Libovicky et al. [2021] Jindrich Libovicky, Helmut Schmid, and Alexander Fraser. Why don't people use character-level machine translation? _arXiv preprint arXiv:2110.08191_, 2021.
* Makkuva et al. [2024] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers via markov chains. _arXiv preprint arXiv:2402.04161_, 2024.
* Mann et al. [2020] Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, et al. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.
* Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_, 2016.
* Mourtada and Gaiffas [2022] Jaouad Mourtada and Stephane Gaiffas. An improper estimator with optimal excess risk in misspecified density estimation and logistic regression. _The Journal of Machine Learning Research_, 23(1):1384-1432, 2022.
* Mourtada et al. [2021]Assaf Naor, Shravas Rao, and Oded Regev. Concentration of markov chains with bounded moments. 2020.
* Navarro and Russo (2008) Gonzalo Navarro and Luis MS Russo. Re-pair achieves high-order entropy. In _DCC_, page 537. Citeseer, 2008.
* Nichani et al. (2024) Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. _arXiv preprint arXiv:2402.14735_, 2024.
* Palmer (2000) David D Palmer. Tokenisation and sentence segmentation. _Handbook of natural language processing_, pages 11-35, 2000.
* Parr (2013) Terence Parr. _The Definitive ANTLR 4 Reference_. Pragmatic Bookshelf, Raleigh, NC, 2 edition, 2013. ISBN 978-1-93435-699-9. URL https://www.safaribooksonline.com/library/view/the-definitive-antlr/9781941222621/.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Rumelow and Watkins (2023) Jessica Rumelow and Matthew Watkins. Solidgoldmagikarp. https://www.alignmentforum.org/posts/aPJEbBSo6rAFoLdg/solidgoldmagikarp-plus-prompt-generation, 2023.
* Schuster and Nakajima (2012) Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In _2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)_, pages 5149-5152. IEEE, 2012.
* Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162.
* Singh and Strouse (2024) Aaditya K Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in frontier llms. _arXiv preprint arXiv:2402.14903_, 2024.
* Tolmachev et al. (2018) Arseny Tolmachev, Daisuke Kawahara, and Sadao Kurohashi. Juman++: A morphological analysis toolkit for scriptio continua. In Eduardo Blanco and Wei Lu, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 54-59, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2010. URL https://aclanthology.org/D18-2010.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.
* Welch (1984) Terry A. Welch. A technique for high-performance data compression. _Computer_, 17(06):8-19, 1984.
* Xue et al. (2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models. _Transactions of the Association for Computational Linguistics_, 10:291-306, 2022.
* Yu et al. (2021) Sangwon Yu, Jongyoon Song, Heeseung Kim, Seong-min Lee, Woo-Jong Ryu, and Sungroh Yoon. Rare tokens degenerate all tokens: Improving neural text generation via adaptive gradient gating for rare token embeddings. _arXiv preprint arXiv:2109.03127_, 2021.
* Zheng et al. (2023) Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. Outline, then details: Syntactically guided coarse-to-fine code generation. In _International Conference on Machine Learning_, pages 42403-42419. PMLR, 2023.
* Ziv and Lempel (1978) Jacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding. _IEEE transactions on Information Theory_, 24(5):530-536, 1978.

* Zouhar et al. [2023a] Vilem Zouhar, Clara Meister, Juan Gastaldi, Li Du, Mrinmaya Sachan, and Ryan Cotterell. Tokenization and the noiseless channel. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5184-5207, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.284. URL https://aclanthology.org/2023.acl-long.284.
* Zouhar et al. [2023b] Vilem Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, and Ryan Cotterell. A formal perspective on byte-pair encoding. _arXiv preprint arXiv:2306.16837_, 2023b.

## Appendix A Analysis of LZW: Proofs of Theorems 3.4 and 3.6

### Notation and definitions

For each character \(a\in\mathcal{A}\) let \(\mathcal{T}_{a}^{\star}\) denote an infinite tree, with root vertex \(\emptyset\), and subsequent vertices labelled by strings \(\bm{t}\in\mathcal{A}^{\star}\). The edge from a parent vertex \(\bm{t}\) to any child \(\bm{t}a^{\prime}\) is labelled with the probability \(P(\bm{t}a^{\prime}|\bm{t})\) unless \(\bm{t}=\emptyset\), in which case the edge probability is \(P(a^{\prime}|a)\). An infinite trajectory sampled on the tree \(\mathcal{T}_{a}^{\star}\) corresponds to an infinite string sampled from the stochastic source conditioned on the first character of the string being \(a\). In this paper we only consider ergodic sources (Gray and Gray, 2009) for which we can define the "entropy rate". The entropy rate fundamentally captures the compressibility of the source, and can be defined as \(H_{\infty}\triangleq\lim_{m\to\infty}\frac{1}{m}H(P)\) where \(\bm{s}\) is a length \(m\) string drawn from the source. By Theorem 2.1, \(H_{\infty}\), captures \(\min_{Q}\mathcal{L}(Q)\).

### A basic result about the optimal achievable cross-entropy loss

The ratio of \(H(P)\) and \(mH(\pi)\) can be made arbitrarily large for the switching Markov chains in Figure 1 as the switching probabilities \(p\) and \(q\) approach \(0\) or \(1\). See Example A.1 for more details.

_Example A.1_.: Consider the switching Markov process in Figure 1 on \(\{0,1\}\) with \(p=q=1-\delta\). For this process, \(\lim_{m\to\infty}\frac{1}{m}H(P)=H_{\mathsf{Ber}}(\delta)=\delta\log(1/\delta )+(1-\delta)\log(1/(1-\delta))\), but \(\pi=\{1/2,1/2\}\) and so \(H(\pi)=H_{\mathsf{Ber}}(1/2)=\log(2)\). The ratio \(\lim_{m\to\infty}\frac{mH(\pi)}{H(P)}\) goes to \(\infty\) as \(\delta\to 0\).

### Proof of Theorem 2.1

We first characterize the minimum achievable cross-entropy loss \(\mathcal{L}_{m}(Q)\) without any restrictions on the likelihood model class \(\mathcal{Q}\). Choosing \(Q(\mathsf{enc}(\bm{s}))=Q(\bm{s})=P(\bm{s})\), the true probability of the sequence \(\bm{s}\), we have \(\mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot))=H(\bm{s})\) where \(H(\cdot)\) is the entropy function. It is not that difficult to see that this is also the minimum cross-entropy loss that can be achieved. For any distribution \(Q\),

\[\mathcal{L}_{m}(Q) =\mathbb{E}[\log(1/Q(\bm{s})]\] \[=\mathbb{E}[\log(P(\bm{s})/Q(\bm{s})]+\mathbb{E}[\log(1/P(\bm{s}))]\] \[=H(P)+D_{\text{KL}}(P\|Q).\]

On the other hand, the cross-entropy loss under any unigramal model \(Q\in\mathcal{Q}_{\text{1-gram}}\) satisfies,

\[\frac{1}{m}\mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot)) \stackrel{{(i)}}{{=}}-\frac{1}{m}\sum_{i=1}^{m} \mathbb{E}[\log Q_{\text{tok}}(\bm{t}_{i})]-\frac{1}{m}\mathbb{E}[\log Q_{ \#}(m)]\] \[\stackrel{{(ii)}}{{\geq}}-\sum_{a\in\mathcal{A}}\pi( a)\log Q_{\text{tok}}(a)\] \[\geq H(\pi)\]

where in \((i)\), we use the definition of the unigram model \(Q\), and in \((ii)\), \(\pi\) is the stationary distribution over characters induced by the stochastic source, and the ergodicity of the source is used. The last equation lower bounds \(H(X,Y)\geq H(X)\).

### Maximum likelihood unigram model

A number of our results (Theorems 3.4 and 3.6 to name a few) are related to bounding \(\min_{Q\in\mathcal{Q}_{\text{1-gram}}}\mathcal{L}(Q\circ\mathsf{enc}(\cdot))\) for some tokenizer \(\mathcal{T}\). In this section we introduce the maximum likelihood unigram model which captures the optimizer over \(Q\) for any given tokenizer.

For the character level tokenizer, an examination of Theorem 2.1 shows that the optimal unigram likelihood model associates probability \(Q_{\text{tok}}(a)=\pi(a)\), i.e. the limiting fraction of times the character \(a\) is observed in the sequence. More generally, for a non-trivial tokenizer, the corresponding optimal unigram model \(Q_{\text{tok}}^{\star}(\bm{t})\) ends up being the limiting expected fraction of times \(\bm{t}\) is observed in an encoding of a sequence. This is the maximum likelihood unigram model, which we formally define below. The unigram MLE likelihood model associates probability,

\[Q_{\text{MLE}}(\bm{t})\leftarrow\lim_{m\to\infty}\mathbb{E}\left[\frac{n_{\bm {t}}}{\sum_{\bm{t}}n_{\bm{t}}}\right]\] (4)

to each token, where \(n_{\bm{t}}\) is the random variable capturing the number of occurrences of the token \(\bm{t}\) in the encoding of the length-\(m\) string \(\bm{s}\). Restricting the class of likelihood models to the unigram models, \(\mathcal{Q}_{\text{1-gram}}\), \(Q_{\text{MLE}}\) captures the model which minimizes eq. (1).

The unigram MLE model cannot be computed without an infinite amount of data, but can be approximated well with a finite amount of data, which forms the basis for Theorem C.1. For certain encoding algorithms, we can show that the quantity \(n_{\bm{t}}/\sum_{\bm{t}}n_{\bm{t}}\) asymptotically converges to its expectation (Lemma A.4). This is the reason the unigram model in eq. (4) is referred to as a "maximum likelihood" model, since \(\lim_{m\to\infty}n_{\bm{t}}/\sum_{\bm{t}}n_{\bm{t}}\) is the limit as \(|\bm{s}|=m\to\infty\) of the solution to the following likelihood maximization problem: given a sequence \(\bm{s}\), find the distribution over tokens, \(Q\), which maximizes

\[\prod_{\bm{t}\in\mathsf{enc}(\bm{s})}Q(\bm{t})\equiv\prod_{\bm{t}\in\text{Dlet }}\left(Q(\bm{t})\right)^{n_{\bm{t}}}.\]

As discussed previously, the unigram MLE model over tokens in eq. (4) induces a joint distribution over sequences of tokens by looking at the product of the marginal probabilities of the composed tokens; in particular,

\[Q_{\text{MLE}}(\bm{t}_{1},\cdots,\bm{t}_{j})=Q_{\text{MLE}}(j)\prod_{i=1}^{j}Q _{\text{MLE}}(\bm{t}_{i}),\]where \(Q_{\text{MLE}}(j)\) is a distribution on the total number of tokens generated and is instantiated as \(\text{Unif}([m])\).

_Remark A.2_.: Note that the unigram MLE model specifies a distribution over tokens which is a function of the underlying encoding algorithm, \(\mathsf{enc}(\cdot)\). Different encoders result in different population level distributions over tokens, and consequently different unigram MLE models.

**Definition A.3** (greedy encoder).: Given a dictionary \(\mathsf{Dict}\), the greedy encoder \(\mathsf{enc}_{\text{gre}}(\bm{s})\) encodes a string \(\bm{s}\) into tokens by greedily matching from left to right, the largest substring that exists as a token in \(\mathsf{Dict}\). This substring is then removed and the process iterated on the remainder of \(\bm{s}\). The greedy decoder \(\mathsf{dec}_{\text{gre}}(\cdot)\) is a lookup table - a sequence of tokens is decoded by replacing each occurrence of a token by the corresponding substring it maps to in \(\mathsf{Dict}\).

**Lemma A.4**.: \(\lim_{m\to\infty}\frac{n_{\bm{t}}}{\sum_{\bm{t}^{\prime}}\bm{t}^{\prime}} \overset{\text{a.e.}}{=}\lim_{m\to\infty}\mathbb{E}\left[\frac{n_{\bm{t}}}{ \sum_{\bm{t}^{\prime}}n_{\bm{t}^{\prime}}}\right]\) _for any tokenizer having a finite vocabulary and finitely long tokens, using the greedy encoder._

Proof.: This result is essentially true because under the greedy encoder, the tokens in an encoding of a fresh string \(\bm{t}\) may be generated by an \(r^{th}\)-order Markov process for some \(r\). For such processes, the Cesaro average of the state distributions converges to a stationary distribution of the process (i.e., the Krylov-Bogolyubov argument).

Tokens are generated as follows. Suppose the previous tokens generated were \(\bm{t}_{1},\bm{t}_{2},\cdots,\bm{t}_{i}\). The next token \(\bm{t}_{i+1}\) is sampled by drawing an infinite trajectory from \(\mathcal{T}^{*}_{a}\) for \(a\sim P(\cdot|\bm{t}_{i})\) and returning the longest prefix \(\bm{t}\) of this trajectory which is a token in \(\mathsf{Dict}\), conditional on satisfying the conditions, \(\bm{t}_{j}\bm{t}_{j+1}\cdots\bm{t}_{i}\bm{t}\not\in\mathsf{Dict}\) for all \(j\in\{1,2,\cdots,i\}\). This process is repeated sequentially to generate all the tokens.

Suppose the length of the longest token in the dictionary is \(\ell_{\max}\). Then, the distribution from a which a token is sampled depends on at most the previous \(\ell_{\max}\) tokens. The reason for this is that the dependency of the \((i+1)^{th}\) token, \(\bm{t}_{i+1}\), on the previously sampled tokens emerges in the constraint \(\bm{t}_{j}\bm{t}_{j+1}\cdots\bm{t}_{i}\bm{t}_{i+1}\not\in\mathsf{Dict}\), satisfied by any candidate \(\bm{t}_{i+1}\). Since each token is of length at least one, this condition is vacuously satisfied if \(j<i-\ell_{\max}\).

With this view, the evolution of the state, defined as \(\mathsf{state}_{r}=(\bm{t}_{r\ell_{\max}},\bm{t}_{r\ell_{\max}-1},\cdots,\bm{t }_{(r-1)\ell_{\max}})\) evolves in a Markovian fashion. By the Krylov-Bogolyubov argument (cf. Proposition 4.2 in Chen (2018)), the time averaged visitation frequencies of a Markov chain coordinate-wise asymptotically converges to its expectation, almost surely. This expectation exists by Theorems 8.5 and 8.22 of Eisner et al. (2015) which shows that for a matrix \(A\) such that \(\sup_{t\in\mathbb{N}}\|A^{t}\|_{\bm{\varphi}}<\infty\) the limit \(\lim_{t\to\infty}\frac{1}{t}\sum_{i=1}^{t}A^{i}\) exists. For the finite-state Markov transition \(A\) which captures the token generation process, condition \(\sup_{t\in\mathbb{N}}\|A^{t}\|_{\bm{\varphi}}\leq|\mathsf{Dict}|^{\ell_{\max} }<\infty\). This means that the limit of the time averaged state distribution exists. Moreover, for any initial distribution \(\pi_{0}\) over tokens, \(\pi=\lim_{t\to\infty}\frac{1}{t}\sum_{i=1}^{t}\pi_{0}A^{i}\) satisfies the condition \(\pi A=\pi\), implying that the limiting time-averaged state distribution is a stationary distribution of \(A\). Since the limiting time-averaged measure on the state \(\mathsf{state}_{r}=(\bm{t}_{r\ell_{\max}},\cdots,\bm{t}_{r\ell_{\max}-1},\cdots,\bm{t}_{(r-1)\ell_{\max}})\) exists, this implies that the limiting time-averaged measure of \(\bm{t}_{r\ell_{\max}-r^{\prime}}\) for each \(r^{\prime}\in\{0,1,\cdots,\ell_{\max}\}\) exists. By taking the uniform average over \(r^{\prime}\) and \(r\), the limiting time-averaged measure of \(\bm{t}_{i}\) over \(i\in\mathbb{N}\) exists. 

### Proof of Theorem 3.4

Consider a string \(\bm{s}\) of length \(m\to\infty\) which is encoded into a sequence of tokens \((\bm{t}_{i}:i\in[|\mathsf{enc}_{\text{gre}}(\bm{s})|])\). By the Asymptotic Equipartition Property (AEP) for ergodic sources, i.e. the Shannon-McMillan-Breiman theorem,

\[\Pr\left(\lim_{m\to\infty}-\frac{1}{m}\log P(\bm{s})=H_{\infty}\right)=1.\] (5)

Here \(\lim_{m\to\infty}\frac{H(P)}{m}\) also happens to be the entropy rate of the source. We use this property to bound the length of the greedy encoding, \(|\mathsf{enc}_{\text{gre}}(\bm{s})|\). Indeed, the probability of \(\bm{s}\) may be decomposed as,

\[P(\bm{s})=P(\bm{t}_{1})\prod_{i=2}^{|\mathsf{enc}_{\text{gre}}(\bm{s})|}P\big{(} \bm{t}_{i}\big{|}\bm{t}_{i-1}\big{)})\leq\prod_{i=1}^{|\mathsf{enc}_{\text{gre }}(\bm{s})|}\max_{a\in\mathcal{A}}P\big{(}\bm{t}_{i}|a\big{)}.\]Noting that \(\delta\min_{a}P(\bm{t}|a)\geq\max_{a}P(\bm{t}|a)\), up to a \(\delta\) factor we may replace the max over \(a\) by an expectation over \(a\sim\pi\) where \(\pi\) is the stationary distribution of the stochastic source. In particular,

\[P(\bm{s})\leq\prod_{i=1}^{|\mathsf{enc}_{\text{gre}}(\bm{s})|}P(\bm{t}_{i})/\delta.\]

By invoking the AEP, eq. (5),

\[\lim_{m\to\infty}\frac{1}{m}\sum\nolimits_{i=1}^{|\mathsf{enc}_{\text{gre}}( \bm{s})|}-\log\big{(}P(\bm{t}_{i})\big{)}-\log(1/\delta)\big{)}\stackrel{{ \text{a.s.}}}{{\leq}}H_{\infty}\]

Recall that the greedy encoder satisfies Lemma A.4 and for any \(\bm{t}\in\mathsf{Dict}\), \(\lim_{m\to\infty}\frac{n_{\bm{t}}}{|\mathsf{enc}_{\text{gre}}(\bm{s})|} \stackrel{{\text{a.s.}}}{{=}}Q_{\text{MLE}}(\bm{t})\). Furthermore, note that for any token \(\bm{t}\in\mathsf{Dict}\), \(P(\bm{t})>\delta^{|\bm{t}|}>0\), and \(|\mathsf{enc}_{\text{gre}}(\bm{s})|\leq m\) surely. By almost sure convergence,

\[\lim_{m\to\infty}\frac{|\mathsf{enc}_{\text{gre}}(\bm{s})|}{m} \sum\nolimits_{\bm{t}\in\mathsf{Dict}}-\frac{n_{\bm{t}}}{|\mathsf{enc}_{\text {gre}}(\bm{s})|}\left(\log\big{(}P(\bm{t})-\log(1/\delta)\big{)}\right)\] \[\stackrel{{\text{a.s.}}}{{=}}\lim_{m\to\infty}\frac{| \mathsf{enc}_{\text{gre}}(\bm{s})|}{m}\left(H(Q_{\text{MLE}},P)-\log(1/\delta)\right)\]

Furthermore, utilizing the assumption that \(\varepsilon H(Q_{\text{MLE}},P)\geq\log(1/\delta)\) satisfied by the tokenizer,

\[\lim_{m\to\infty}\frac{(1-\varepsilon)|\mathsf{enc}_{\text{gre}}(\bm{s})| \left(H(Q_{\text{MLE}},P)\right)}{m}\stackrel{{\text{a.s.}}}{{ \leq}}H_{\infty}.\] (6)

Now we are ready to bound the expected cross-entropy loss of the tokenizer. Define the unigram model \(P_{\pi}(\bm{t}_{1},\bm{t}_{2},\cdots,\bm{t}_{j})=P_{\text{unif}}(j)\prod_{i=1} ^{j}P(\bm{t}_{i})\) where \(P_{\text{unif}}\) is the uniform measure over \([m]\). Note that we have the inequality \(\min_{Q\in\mathcal{Q}_{1:\text{gun}}}\lim_{m\to\infty}\frac{1}{m}\mathcal{L} _{m}(Q\circ\mathsf{enc}_{\text{gre}}(\cdot))\leq\lim_{m\to\infty}\frac{1}{m} \mathcal{L}_{m}(P_{\pi}\circ\mathsf{enc}_{\text{gre}}(\cdot))\) and therefore, it suffices to upper bound the RHS. In particular,

\[\mathcal{L}_{m}(P_{\pi}\circ\mathsf{enc}_{\text{gre}}(\cdot)) =-\mathbb{E}[\log P_{\text{unif}}(|\mathsf{enc}_{\text{gre}}(\bm{ s})|)]-\mathbb{E}\left[\sum\nolimits_{\bm{t}\in\mathsf{enc}_{\text{gre}}(\bm{s})} \log\big{(}P(\bm{t})\big{)}\right]\] \[\leq\log(m)-\mathbb{E}\left[\sum\nolimits_{\bm{t}\in\mathsf{enc} _{\text{gre}}(\bm{s})}\log\big{(}P(\bm{t})\big{)}\right]\] (7)

where the last inequality uses the fact that \(P_{\text{unif}}(|\mathsf{enc}_{\text{gre}}(\bm{s})|)=1/m\). Note that as \(m\to\infty\), by assumption on the tokenizer, the fraction of times the token \(\bm{t}\) appears in the encoding of \(\bm{s}\) converges almost surely converges to \(Q_{\text{MLE}}(\bm{t})\). Since \(|\mathsf{enc}_{\text{gre}}(s)|\leq m\) surely and \(P(\bm{t})>\delta^{|\bm{t}|}>0\), by an application of the Dominated Convergence Theorem,

\[-\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[\sum\nolimits_{\bm{t }\in\mathsf{enc}_{\text{gre}}(\bm{s})}\log\big{(}P(\bm{t})\big{)}\right] =-\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[|\mathsf{enc}_{ \text{gre}}(\bm{s})|\cdot\sum\nolimits_{\bm{t}\in\mathsf{Dict}}Q_{\text{MLE}} (\bm{t})\log\big{(}P(\bm{t})\big{)}\right]\] \[=\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[|\mathsf{enc}_{\text {gre}}(\bm{s})|H(Q_{\text{MLE}},P)\right]\] (8)

Combining eq. (7) with eq. (8) and setting \(\lim_{m\to\infty}\log(m)/m=0\), and invoking eq. (6),

\[\min_{Q\in\mathcal{Q}_{1:\text{gun}}}\lim_{m\to\infty}\frac{1}{m} \mathcal{L}_{m}(Q_{\text{MLE}}\circ\mathsf{enc}_{\text{gre}}(\cdot)) =\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[|\mathsf{enc}_{\text{gre }}(\bm{s})|H(Q_{\text{MLE}},P)\right]\] \[\leq\frac{H_{\infty}}{1-\varepsilon}.\] (9)

By Theorem 2.1, we have that \(\min_{Q}\lim_{m\to\infty}\frac{1}{m}\mathcal{L}_{m}(Q\circ\mathsf{enc}_{\text{gre }}(\cdot))=\lim_{m\to\infty}\frac{H(P)}{m}=H_{\infty}\), which uses the fact that the source is ergodic. Combining with eq. (9) completes the proof.

### Heavy-hitter dictionaries and a proof of Theorem 3.6

In this section we prove Theorem 3.6 and introduce the notion of a heavy-hitting dictionary. At a high level, these dictionaries contain all the substrings which have reasonably high probability of being observed many times in a dataset of size \(n=\widetilde{\Omega}_{\delta}(d)\). We first show in Lemma A.6 that heavy hitting dictionaries generalize well in the sense of having \(H(Q_{\text{MLE}},P)\) being large (in conjunction with Theorem 3.4 this implies an upper bound on the cross-entropy loss of the best unigram model). Next, we will prove that the LZW algorithm (Definition 3.5) results in a heavy hitting dictionary with high probability.

**Definition A.5** (\(\beta\)-heavy-hitting dictionary).: A token \(\bm{t}\) of a dictionary is said to be maximal if there exists an arbitrary substring containing \(\bm{t}\) as a strict prefix, and in addition, \(\bm{t}\) is also the largest prefix of the substring which is a token. A dictionary \(\mathsf{Dict}\) is said to be \(\beta\)-heavy hitting if the set of maximal tokens is a subset of \(\{\bm{s}^{\prime}:\max_{a\in\mathcal{A}}P(\bm{s}^{\prime}|a)\leq 1/d^{\beta}\}\).

A pictorial depiction of the heavy hitting property is illustrated in Figure 6.

**Lemma A.6**.: _For a \(\beta\)-heavy-hitting dictionary, with the greedy encoder, \(H(Q_{\text{MLE}},P)\geq\beta\log(d)\)._

Proof.: Note that the greedy encoder assigns tokens only among the set of maximal substrings (save for potentially the last token). If every maximal substring has \(\max_{a\in\mathcal{A}}P(\bm{s}|a)\leq 1/d^{\beta}\), by the heavy-hitting property, for any token \(\bm{t}\),

\[P(\bm{t})\leq\max_{a\in\mathcal{A}}P(\bm{s}^{\prime}|a)\leq 1/d^{\beta}.\]

Therefore,

\[H(Q_{\text{MLE}},P)=\mathbb{E}_{\bm{t}\sim Q_{\text{MLE}}}[\log(1/P(\bm{t}))] \geq\beta\log(d).\]

Define \(\mathcal{M}_{\beta}=\{\bm{t}:\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq\delta/d^{ \beta}\}\). These are the set of "high-probability" substrings under the stochastic source. We will show that for \(\beta\) bounded away from \(1\), with high probability, every substring in \(\mathcal{M}_{\beta}\) is added as a token to the dictionary in a run of the LZW tokenizer (Definition 3.5). Note that if every substring in \(\mathcal{M}_{\beta}\) is assigned as a token by LZW, then the algorithm must be \(\beta\)-heavy hitting since there always exists a maximal token on the "boundary" of the set \(\mathcal{M}_{\beta}\) which is strictly contained in \(\{\bm{s}^{\prime}:\max_{a\in\mathcal{A}}P(\bm{s}^{\prime}|a)\leq 1/d^{\beta}\}\).

Figure 6: The circled nodes indicates substrings which are tokens in \(\mathsf{Dict}\). Red nodes indicate the set of “maximal tokens”, which are the set of tokens which the greedy encoder assigns, leaving out those which can only be assigned as the last token of some string. Tokens like “\(b\)” are never assigned by the greedy encoder (save as the last token of the encoding of a string) since any sufficiently long trajectory starting with \(b\) must have a longer prefix which is also a token, namely, one of \(ba\), \(bc\), \(bba\), \(bbb\) or \(bbc\). The vertices of the tree which are assigned by the greedy encoder as tokens (together with all their prefixes) forms a cut of the tree, which marks the dotted red line. The heavy hitting property asserts that this cut is uniformly far away from the root node \(\emptyset\), and that every vertex \(\bm{s}\) marked red has \(P(\bm{s})\leq 1/d^{\beta}\).

**Lemma A.7**.: _Every substring in \(\mathcal{M}_{\beta}\) has length at most \(\ell_{*}\triangleq\delta^{-1}(\beta\log(d)+\log(1/\delta))\)._

Proof.: Note that \(\min_{a,a^{\prime}\in\mathcal{A}}P(a|a^{\prime})=\delta\), which implies that the probability of any transition must be bounded away from \(1\), i.e., \(\max_{a,a^{\prime}\in\mathcal{A}}P(a|a^{\prime})\leq 1-\delta\). This implies that,

\[\max_{a\in\mathcal{A}}P(\bm{t}|a)\leq(1-\delta)^{|\bm{t}|}\leq e^{-\delta|\bm{ t}|}.\] (10)

By definition, for any substring \(\bm{t}\in\mathcal{M}_{\beta}\), \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq\delta/d^{\beta}\). In conjunction with eq.10, this implies the statement of the lemma. 

In the remainder of this section, let \(n\) be the size of the dataset on which LZW is run. We show that the number of tokens added to the dictionary by LZW, \(d\), is \(\widetilde{\Theta}_{\delta}(n)\). Rather than running the algorithm with early stopping (i.e., ceasing to add new tokens once the budget is hit), instead, we assume that the algorithm runs on a prefix of the dataset of length \(d\). The number of tokens added this way cannot exceed \(d\).

**Lemma A.8**.: _With probability \(\geq 1-d^{-\Omega(\log(d/\delta)/\delta)}\), in a run of the LZW algorithm, no substring \(\bm{t}\) added as a token to the dictionary satisfies \(|\bm{t}|\geq\ell_{\max}\triangleq 4\log(d|\mathcal{A}|)/\delta\)._

Proof.: Consider any \(s\in\mathbb{N}\) and any substring \(\bm{t}\) of length \(s\). In order for \(\bm{t}\) to be assigned as a token, each of its prefixes must disjointly appear at least once in the string. Since there are at most \(d\) tokens, we can upper bound the probability that \(\bm{t}\) is assigned as a token as,

\[P(\bm{t}\text{ is assigned as a token}) \leq\binom{d}{s}\prod_{i=1}^{s}\max_{a\in\mathcal{A}}P(\bm{t}_{ 1:i}|a)\] \[\overset{(i)}{\leq}\binom{d}{s}(1-\delta)^{s(s-1)/2}\] \[\leq e^{s\log(d)-\delta s(s-1)/2},\]

where \((i)\) uses the fact that \(\max_{a\in\mathcal{A}}P(\bm{t}_{1:i})\leq\prod_{j=1}^{i}\max_{a\in\mathcal{A}} P(\bm{t}_{j}|a)\leq(1-\delta)^{i}\). By union bounding across the \(|\mathcal{A}|^{s}\) strings of length \(s\),

\[P(\text{any length $s$ string is assigned as a token})\leq e^{s\log(|\mathcal{A}|)+s\log(d)- \delta s(s-1)/2}.\]

When \(s=4\log(d|\mathcal{A}|)/\delta+1\triangleq\ell_{\max}+1\), the RHS is upper bounded by \(e^{-\delta\ell_{\max}^{2}/4}\leq d^{-\Omega(\log(d/\delta)/\delta)}\). With the same small probability, no substring of length \(s^{\prime}>s\) can become a token, since their length-\(s\) prefixes are never assigned as tokens. 

**Corollary A.9**.: _With probability \(\geq 1-d^{-\Omega_{\delta}(\log(d))}\), learns a dictionary with at least \(d^{\star}=d/\ell_{\max}\) tokens when run on a training sequence of length \(n\) drawn from a stochastic source satisfying Assumption3.2._

**Lemma A.10**.: _For any constant \(\beta<1\), with probability \(\geq 1-d^{-\Omega(\log(d/\delta)/\delta)}-\exp(-\widetilde{\Omega}_{\delta}( d^{1-\beta}))\) over the source dataset, every substring in \(\mathcal{M}_{\beta}\) is added as a token to the dictionary in a run of the LZW algorithm. In other words, with the same probability, the LZW tokenizer results in a \(\beta\)-heavy hitting dictionary._

By CorollaryA.9, note that with high probability the LZW tokenizer adds at least \(d^{\star}\) tokens to the dictionary when processing a length \(d\) training sequence in entirety. In this proof, instead of generating \(d\) samples, we sequentially sample \(d^{\star}\) tokens from their joint distribution, and generate a dictionary from these samples. From CorollaryA.9, with high probability this results in at most \(d\) samples being generated, implying that the dictionary generated by sampling \(d^{\star}\) tokens is a subset of the dictionary generated by a full run of the LZW tokenizer. Here, we use the fact that the LZW tokenizer adds tokens to the dictionary in a left to right fashion, and therefore a subset of the dictionary learnt by the LZW tokenizer can be generated by processing a portion of the dataset.

Next we consider a joint view for generating the dataset from the stochastic source and the dictionary learnt by LZW simultaneously. The stochastic source is sampled as a sequence of tokens. Suppose the last character of the previous token was \(a^{\prime}\). Sample a character \(a\sim P(\cdot|a^{\prime})\) and an infinite trajectory on the tree \(\mathcal{T}_{a}^{\star}\). Consider the first node visited in this trajectory which does not alreadyexist as a token in the dictionary. The substring corresponding to this node is added as a token in the dictionary. By repeating this process, the dictionary and the source dataset are constructed sequentially and simultaneously. As alluded to before, we truncate this token sampling process to repeat at most \(d^{*}\) times, which results in a subset of the dictionary output by the LZW algorithm with high probability (Corollary A.9). This is simply a variant of the "Poissonization" trick to avoid statistical dependencies across tokens. Denote the set of infinite trajectories generated on the forest \(\{\mathcal{T}_{a}^{*}:a\in\mathcal{A}\}\) as \(\{\textsf{traj}_{i}:i\in[d^{*}]\}\).

With this view of the sampling process, observe that if the substring \(\bm{t}\) sampled was a prefix of \(\textsf{traj}_{i}\) at least \(|\bm{t}|\) times across different values of \(i\), then \(\bm{t}\) must be assigned as a token. In particular, in each of these \(|\bm{t}|\) trajectories, each of the prefixes of \(\bm{t}\) is assigned as a token. With this observation, the event that \(\bm{t}\) is not assigned as a token is contained in the event that \(\bm{t}\) is visited at most \(|\bm{t}|-1\) times across the \(d^{*}\) trajectories. Observe that,

\[P(\bm{t}\text{ is not assigned as a token})\leq\sum_{i=0}^{|\bm{t}|-1}\binom{d^{ *}}{i}\max_{a\in\mathcal{A}}(P(\bm{t}|a))^{i}(1-P(\bm{t}|a))^{d^{*}-i}.\]

Since we aim to upper bound this probability across the substrings in \(\bm{t}\in\mathcal{M}_{\beta}\), note that \((i)\)\(\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq\delta/d^{\beta}\), and \((ii)\) tokens in \(\mathcal{M}_{\beta}\) have length at most \(\ell_{*}=\delta^{-1}(\beta\log(d)+\log(1/\delta))\) (Lemma A.7), implying there are at most \(2|\mathcal{A}|^{\ell_{*}}\) substrings in this set. By union bounding,

\[P(\exists\bm{t}\in\mathcal{M}_{\beta}\text{ not assigned as a token})\leq 2| \mathcal{A}|^{\ell_{*}}\sum_{i=0}^{\ell_{*}-1}\binom{d^{*}}{i}\max_{x\geq\delta /d^{\beta}}x^{i}(1-x)^{d^{*}-i}.\] (11)

**Case I.** For \(i\leq\ell_{*}\) and \(x\geq 1/2\),

\[|\mathcal{A}|^{\ell_{*}}\binom{d^{*}}{i}x^{i}(1-x)^{d^{*}-i} \leq|\mathcal{A}|^{\ell_{*}}\frac{(d^{*})^{\ell_{*}}}{2^{d^{*}/2}}\] \[\leq 2^{\ell_{*}\log(d^{*}|\mathcal{A}|)-d^{*}/2}\] \[\leq 2^{-\Omega_{\beta,\delta}(d^{*})},\] (12)

where the last inequality uses the fact that \(\ell_{*}=O_{\beta,\delta}(\log(d))\).

**Case II.** For \(i\leq\ell_{*}\) and \(\delta/d^{\beta}\leq x\leq 1/2\),

\[|\mathcal{A}|^{\ell_{*}}\binom{d^{*}}{i}x^{i}(1-x)^{d^{*}-i} \leq|\mathcal{A}|^{\ell_{*}}\binom{d^{*}}{i}(1-x)^{d^{*}}\] \[\leq|\mathcal{A}|^{\ell_{*}}(d^{*})^{\ell_{*}}e^{-d^{*}x}\] \[\leq e^{\ell_{*}\log(|\mathcal{A}|)+\ell_{*}\log(d^{*})-d^{*}x}\] \[\leq e^{-\Omega(\delta^{2}n/d^{\beta}/\log(d/\delta))}\] \[\leq e^{-\Omega(\delta^{2}d^{1-\beta}/\log(d/\delta))},\] (13)

where the last inequality uses the fact that \(\ell_{*}=O(\log(d))\), \(x\geq\delta/d^{\beta}\), \(d^{*}=\Omega(d\delta/\log(d/\delta))\). By combining eq.12 and eq.13 with eq.11 completes the proof, as long as \(\beta\) is a constant bounded away from \(1\).

**Lemma A.11**.: _Fix a constant \(\gamma>0\). Then, with probability \(\geq 1-d^{-\Omega_{\gamma,\delta}(\log(d))}\), none of the substrings in the set \(\mathcal{N}_{\gamma}=\left\{\mathbf{s}^{\prime}:\max_{a\in\mathcal{A}}P( \mathbf{s}^{\prime}|a)\leq\delta/d^{1+\gamma}\right\}\) are assigned as tokens in a run of LZW._

Proof.: Define the following set of substrings,

\[S_{\gamma}=\left\{\mathbf{t}:\delta/d^{1+\gamma/2}\leq\max_{a\in\mathcal{A}}P( \mathbf{t}|a)\leq 1/d^{1+\gamma/2}\right\}\]

Since the width of this band is sufficiently large, by Assumption3.2 every substring \(\bm{t}\) such that \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\leq\delta/d^{1+\gamma/2}\) has at least one prefix which falls in \(S_{\gamma}\), and denote the longest such prefix \(\bm{t}_{\gamma}\). Define \(T_{\gamma}=\{\bm{t}_{\gamma}:\bm{t}\in\mathcal{N}_{\gamma}\}\) as the set of longest prefixes in \(S_{\gamma}\). Intuitively, if we think of the strings in \(S_{\gamma}\) (or \(T_{\gamma}\)) as being intermediate in length, the strings in \(\mathcal{N}_{\gamma}\) can be thought of as being particularly long: the value of \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\) for any \(\bm{t}\in T_{\gamma}\) and for any \(\bm{t}\in\mathcal{N}_{\gamma}\) are separated by a factor of at least \(1/d^{\gamma/2}\). In particular, since the probability of any character is lower bounded by \(\delta\), each substring in \(\bm{t}\in\mathcal{N}_{\gamma}\) must be at least \(\Delta=\frac{\gamma\log(d)}{2\log(1/\delta)}\) symbols longer than its corresponding longest prefix in \(T_{\gamma}\), \(\bm{t}_{\gamma}\). An implication of this is that for \(\bm{t}\) to be assigned as a token, \(\bm{t}_{\gamma}\) must be observed at least \(\Delta+1\) times disjointly in \(\bm{s}\). However, note that \(\bm{t}_{\gamma}\) already has low marginal probability to begin with (\(\ll 1/d\)) so the odds of seeing this substring so many times disjointly is very small. Furthermore, note that \(T_{\gamma}\) has at most \(d^{1+\gamma/2}/\delta\) substrings, which allows the probability of this event occurring simultaneously across all substrings in \(T_{\gamma}\) to be controlled by union bound. Under this condition, none of the substrings in \(\mathcal{N}_{\gamma}\) are made into tokens.

In order to argue that the dictionary _does not_ contain certain tokens, we may argue this property about any superset of the dictionary. In contrast, in Lemma A.10, we construct a subset of the dictionary by running LZW on the concatenation of \(d^{*}\) tokens sampled from their joint distribution. The superset we consider here is just to sample \(d\) tokens from their joint distribution and concatenate them together to result in a string of length \(\geq d\), and running LZW on this sequence (which simply would result in these \(d\) tokens). As in Lemma A.10, let \(\{\bm{t}\mathsf{raj}_{i}:i\in[d]\}\) denote the infinite trajectories generated from the Markov chain which are truncated to result in tokens. A sufficient condition for the event that no substring \(\bm{t}\in\mathcal{N}_{\gamma}\) is assigned as a token by LZW is to the event that every substring \(\bm{t}^{\prime}\in T_{\gamma}\) is observed as a prefix of \(\mathsf{traj}_{i}\) for \(\Delta\) or fewer choices of \(i\in[d]\). To this end define \(\mathcal{E}(\bm{t}^{\prime})\) as the event that \(|i\in[d]:\bm{t}^{\prime}\) is a prefix of \(\mathsf{traj}_{i}|\leq\Delta\). Then,

\[\Pr(\mathcal{E}(\bm{t}^{\prime})) \leq\binom{n}{\Delta}(\max_{a\in\mathcal{A}}P(\bm{t}^{\prime}|a))^ {\Delta}\] \[\overset{(i)}{\leq}e^{\Delta\log(n)}\left(\frac{1}{d^{1+\gamma/2} }\right)^{\Delta}\] \[\leq e^{-\frac{\gamma}{2}\Delta\log(d)},\] (14)

where \((i)\) uses the fact that \(\max_{a\in\mathcal{A}}P(\bm{t}^{\prime}|a)\leq 1/d^{1+\gamma/2}\) since the substring \(\bm{t}^{\prime}\) belongs to \(T_{\gamma}\).

Note that the number of substrings in \(S_{\gamma}\) (and by extension, \(T_{\gamma}\)) is at most \(O_{\delta}(d^{1+\gamma/2})\). Recall that these substrings satisfy the condition \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq\delta/d^{1+\gamma/2}\). Observe that,

\[\frac{\delta|S_{\gamma}|}{d^{1+\gamma/2}} \leq\sum_{\bm{t}\in S_{\gamma}}\max_{a\in\mathcal{A}}P(\bm{t}|a)\] \[\leq\sum_{\bm{t}\in S_{\gamma}}\sum_{a\in\mathcal{A}}P(\bm{t}|a) \leq|\mathcal{A}|\leq\frac{1}{\delta}.\]

This implies that there are at most \(d^{1+\gamma/2}/\delta^{2}\) substrings in \(S_{\gamma}\). Finally, in conjunction with eq. (14),

\[P(\exists\ \bm{t}^{\prime}\in S_{\gamma}:\mathcal{E}(\bm{t}^{\prime})) \leq\frac{d^{1+\gamma/2}}{\delta^{2}}e^{-\frac{\gamma}{2}\Delta\log(d)}=d^{- \Omega_{\gamma,\delta}(\log(d))},\]

which implies that with high probability, no token in \(S_{\gamma}\) is observed as a prefix of \(\bm{s}^{i}\) for more than \(\Delta\) choices of the index \(i\in[d]\). Under this event, no substring in \(\mathcal{N}_{\gamma}\) is assigned as a token. 

#### a.6.1 Proof of Theorem 3.6

Choosing \(\beta=0.99\) in Lemma A.10, with probability \(\geq 1-d^{-\Omega_{\delta}(\log(d))},\) the LZW tokenizer results in a \(0.99\)-heavy-hitting dictionary. As a consequence of Lemma A.6, this implies that under the same event,

\[H(Q_{\text{MLE}},P)\geq 0.99\log(d).\]

Finally, combining with Theorem 3.4 completes the proof.

Additional Theoretical Results I: A sequential variant of BPE

While the main results in the paper focused on understanding the limits of tokenization under a bound on the dictionary size, in this section we take a more practical look and try to analyze tokenizers used commonly in practice. The Byte-Pair-Encoding (BPE) algorithm (Gage, 1994; Sennrich et al., 2016), discovered in the compression literature as RePAIR (Larsson and Moffat, 2000; Navarro and Russo, 2008) was proposed as a faster alternative to LZW. It remains as one of the most commonly implemented tokenizers in natural language processing for various downstream tasks (Radford et al., 2019; Mann et al., 2020; Touvron et al., 2023). A large proportion of open source and commercial LLMs currently use BPE as the tokenization algorithm of choice, such as GPT-2/3, Llama 1/2 and Mistral-7B to name a few.

The BPE algorithm is based on constructing the dictionary iteratively by merging pairs of tokens to result in a tokens. In each iteration, the pair of tokens which appear most frequently next to each other are merged together into a single token. Subsequently, every occurrence of the pair of tokens are replaced by the newly added token, breaking ties arbitrarily. The dictionary is thus an ordered mapping of the form \(\bm{t}\leftarrow(\bm{t}^{\prime},\bm{t}^{\prime\prime})\). To encode a new string, the BPE encoder iterates through the dictionary and for each rule \(\bm{t}\leftarrow(\bm{t}^{\prime},\bm{t}^{\prime\prime})\) replaces every consecutive occurrence of \(\bm{t}^{\prime}\) and \(\bm{t}^{\prime\prime}\) by the token \(\bm{t}\) breaking ties arbitrarily.

To warm up our main results, it is worth understanding the behavior of the BPE tokenizer in a bit more detail. Unlike the toy tokenizer, it is a priori unclear whether unigram models trained on sequences tokenized by BPE even asymptotically (in the dictionary size) achieve the optimal cross-entropy loss. Indeed, for \(\delta>0\), consider a training sequence of length \(m\) of the form,

\[\bm{s}=\underbrace{\left(\underbrace{01\cdots 01}_{2/\delta}\underbrace{10 \cdots 10}_{2/\delta}\right)}_{\times\frac{m\delta}{4}}\] (15)

The probability that this sequence is generated by the order-\(2\) switching Markov source with \(p=q=\delta\) is,

\[\approx(1-\delta)^{\frac{m\delta}{4}\times\frac{1}{2}\times(1-\delta)}(\delta )^{\frac{m\delta}{4}\times 4}=e^{-H(P)},\]

which uses the fact that \(H(P)=m\delta\log(1/\delta)+m(1-\delta)\log(1/(1-\delta))\). This implies that even though the string has exponentially small probability, it is one of the typical sequences for this order-\(2\) Markov source. Let's understand what happens when the BPE tokenizer is trained on this dataset. Assuming that ties are broken arbitrarily, consider the run of the BPE algorithm detailed in Table 2. Here, we assume that \(1/\delta-1\) is a power of \(2\) and denote \(r=\log_{2}(1/\delta-1)\). The algorithm first merges \(0\) and \(1\) into a single token \(\bm{t}_{1}\), which results in a long sequence of the form \(\bm{t}_{1}\cdots\cdots\bm{t}_{1}\bm{t}_{1}\bm{t}_{1}\cdots\cdots\bm{t}_{1}0\) repeated \(m\delta/4\) times. In subsequent rounds, the tokens \((\bm{t}_{1},\bm{t}_{1})\) is merged into \(\bm{t}_{2}\), then \((\bm{t}_{2},\bm{t}_{2})\) is merged into \(\bm{t}_{3}\) and so on, until is no longer possible. Finally, the resulting sequence is a repeating sequence of \(5\) tokens where within each sequence, no pair of tokens appears more than once next to each other. Eventually these \(5\) tokens are merged into a single token labelled \(\bm{t}_{r+4}\), and in subsequent rounds the tokens \((\bm{t}_{r+4},\bm{t}_{r+4})\) are merged into \(\bm{t}_{r+5}\), \((\bm{t}_{r+5},\bm{t}_{r+5})\) is merged into \(\bm{t}_{r+6}\) and so on, until is no longer possible.

Observe that in the initial training dataset the substrings \(0000\) and \(1111\) never appears as a contiguous sequence. However, in a test sequence of length \(m\) sampled from the \(2^{\text{nd}}\)-order Markov source, with high probability these substrings disjointly occur \(\Theta(m)\) times each. The learnt dictionary associates each such disjoint occurrence of these substrings with at least \(1\) token, for \(0000\), the \(3^{\text{rd}}\)\(0\) must necessarily be tokenized as the token "\(0\)". Likewise, in \(1111\), the \(3^{\text{rd}}\)\(1\) must necessarily be tokenized as the token "\(1\)". Therefore, when a new test string of length \(m\) is tokenized, with high probability the tokens "\(0\)" and "\(1\)" form a constant fraction of the total collection of tokens.

Thus on freshly sampled test sequences, the BPE tokenizer appears to behave like the character-level tokenizer on a constant fraction of the input sequence. In particular, a simple calculation shows that the cross-entropy loss of any unigram model trained on this tokenizer must be far from the optimal bound of \(mH_{\textsc{BPE}}(\delta)\) especially as \(\delta\) becomes smaller,

\[\min_{Q\in\mathcal{Q}_{1:\text{gen}}}\mathcal{L}_{m}(Q\circ\mathsf{ enc}(\cdot))\] \[\geq\min_{Q\in\mathcal{Q}_{1:\text{gen}}}\mathbb{E}\left[n_{0} \log(1/Q_{\text{tok}}(0)+n_{1}\log(1/Q_{\text{tok}}(1)\right]\] \[\stackrel{{(i)}}{{\geq}}\Omega(m)\cdot\min_{Q\in \mathcal{Q}_{1:\text{gen}}}\left(\log(1/P_{\text{tok}}(0)+\log(1/Q_{\text{tok} }(1)\right)\] \[\geq\Omega(m).\]

where \((i)\) uses the fact that \(\mathbb{E}[n_{0}],\mathbb{E}[n_{1}]\in\Omega(m)\) and the last inequality uses \(P_{\text{tok}}(0)P_{\text{tok}}(1)\leq 1/4\) (AM-GM inequality) since they sum up to at most \(1\). The purpose of this example is to show that there exist pathological training datasets which appear to be drawn from a stochastic source, but on which BPE fails to learn a good dictionary for the source. Thus proving a result such as Theorem 3.1 for BPE would require arguing that training datasets such as that in eq. (15) are unlikely to be seen.

The analysis of the standard variant of BPE turns out to be complicated for other reasons too. After every token is added the training dataset becomes a mix of all the previously added tokens, and arguing about the statistics of which pair of tokens appears most frequently for the next iteration becomes involved. For instance, adding \(00\) as a token may reduce the frequency of occurrence of the substring \(01\), but will not affect \(11\). Thus, even though \(01\) may a priori have been seen more frequently, it may not be chosen by BPE as the next token after \(00\).

To avoid dealing with these issues, we consider a sequential/sample-splitting variant of BPE. At a high level, the algorithm breaks down a dataset of size \(\Theta(d^{2})\) into \(d\) chunks and learns at most \(1\) token from each chunk. The algorithm iterates over the chunks and finding the pair of tokens which appear most frequently next to each other in each chunk and adding it to the dictionary if it appears more than \(\log(d)\) times. Every consecutive occurrence of the pair of tokens is replaced by the newly assigned token in the dataset. Thus, in each iteration \(i\), at most \(1\) token is added, depending on the statistics of the \(i^{\text{th}}\) chunk and the tokens added so far to the dictionary. Based on the final size of the dictionary a different encoder/decoder pair is used - if the algorithm adds sufficiently many tokens to the dictionary, the greedy encoder is used, and if not, a parallel implementation of BPE's encoding algorithm is used (Definition B.1). A formal description of the algorithm is in Algorithm 1.

**Definition B.1** (BPE.split encoder).: The BPE.split encoder parses a new string into tokens as follows. The algorithm partitions the string into contiguous chunks of length \(d\). Then, BPE's encoder is applied on each chunk, which iterates through DS and replaces \(t^{\prime}t^{\prime\prime}\) by \(t\) for every rule \(t\leftarrow(t^{\prime},t^{\prime\prime})\) in DS, breaking ties arbitrarily. The individual token sequences are finally spliced together and returned.

The main result of this section is that up to a small additive error, Algorithm 1 approaches a \(2\)-approximation to the optimal cross-entropy loss.

\begin{table}
\begin{tabular}{|c|c|} \hline Initial & \(01\cdots\cdots\cdots 0110\cdots\cdots 10\cdots\) \\ \hline \(\bm{t}_{1}\leftarrow(0,1)\) & \(\bm{t}_{1}\cdots\cdots\bm{t}_{1}\bm{t}_{1}\cdots\bm{t}_{1}0\cdots\) \\ \(\bm{t}_{2}\leftarrow(\bm{t}_{1},\bm{t}_{1})\) & \(\bm{t}_{2}\cdots\bm{t}_{2}\bm{t}_{2}\cdots\bm{t}_{2}0|\cdots\) \\ \(\vdots\) & \(\vdots\) \\ \(\bm{t}_{r}\leftarrow(\bm{t}_{r-1},\bm{t}_{r-1})\) & \(\bm{t}_{r}\bm{t}_{1}\bm{t}_{r}0|\cdots\) \\ \hline \(\bm{t}_{r+1}\leftarrow(\bm{t}_{r},\bm{t}_{1})\) & \(\bm{t}_{r+1}\bm{t}_{r}0|\cdots\) \\ \(\bm{t}_{r+2}\leftarrow(\bm{t}_{r},0)\) & \(\bm{t}_{r+1}\bm{t}_{r+2}|\cdots\) \\ \(\bm{t}_{r+3}\leftarrow(\bm{t}_{r+1},1)\) & \(\bm{t}_{r+3}\bm{t}_{r+2}|\cdots\) \\ \(\bm{t}_{r+4}\leftarrow(\bm{t}_{r+3},\bm{t}_{r+2})\) & \(\bm{t}_{r+4}|\cdots\) \\ \hline \(\bm{t}_{r+5}\leftarrow(\bm{t}_{r+4},\bm{t}_{r+4})\) & \(\bm{t}_{r+5}|\cdots\) \\ \(\bm{t}_{r+6}\leftarrow(\bm{t}_{r+5},\bm{t}_{r+5})\) & \(\bm{t}_{r+6}|\cdots\) \\ \(\vdots\) & \(\vdots\) \\ \hline \end{tabular}
\end{table}
Table 2: A representation of the behavior of BPE when trained on the dataset in eq. (15). We assume that \(1/\delta-1\) is a power of \(2\) and define \(r=\log_{2}(1/\delta-1)\).

**Theorem B.2**.: _For any \(\epsilon\in(0,1)\), run Algorithm 1 on a dataset of \(n=\Theta(d^{2})\) characters to learn a dictionary with at most \(d\) tokens. The resulting tokenizer \(\mathcal{T}\) satisfies with probability \(\geq 1-e^{-\Omega(d\epsilon^{2})}\),_

\[\min_{Q\in\mathfrak{O}_{1:\text{gen}}}\mathcal{L}(Q\circ\textsf{enc}(\cdot)) \leq(2+\varepsilon)\min_{Q}\mathcal{L}(Q)+\epsilon\]

_where \(\varepsilon=O\left(\frac{\log(1/\epsilon)\log^{3}(1/\delta)}{\epsilon\delta^{ 9}\log(d)}\right)\)._

While the guarantees established for the sequential BPE tokenizer are weaker than those in Theorem 3.1, the analysis turns out to be quite involved. Theorem B.2 implies that unigram models trained on the sequential BPE tokenizer asymptotically approach the optimal cross-entropy loss up to a factor of \(2\).

The formal proof of this result is presented in Appendix B. What is the intuition behind using a different encoder in Algorithm 1 depending on the number of tokens in the dictionary? When the number of tokens in the dictionary is smaller than \(d_{0}\), we know that on a \(1-d_{0}/d\) fraction of the iterations of Algorithm 1, a token is _not_ added to the dictionary, i.e. every pair of tokens already appears at most \(\log(d)\) times together. This is a datapoint of "evidence" that under the dictionary in that iteration, the BPE encoder is already good at encoding new strings (of length \(\Theta(d)\)) in a way where pairs of tokens do not appear consecutively with high frequency. Since future dictionaries only have more rules appended to them, dictionaries only get better at encoding new strings into tokens where pairs do not frequently appear consecutively. In other words, the BPE encoder satisfies a monotonicity property. It remains to show that dictionaries which encode new sequences in a way where no pair of tokens appear too frequently have large \(H(Q_{\text{MLE}},P)\) (to invoke Theorem 3.4). This follows from ideas introduced in (Navarro and Russo, 2008).

The case where the number of tokens is large (\(\geq d_{0}\)) turns out to present significant technical challenges for analyzing the BPE encoder. There is no longer much "evidence" that the dictionary in each iteration is good at encoding strings since in a large number of iterations a pair of tokens appear consecutively with high frequency. Analyzing the greedy encoder also presents its own challenges - although the algorithm has allocated a large number of tokens, it is possible that there are short tokens \(\bm{t}\) which are maximal (i.e. they are not prefixes of other tokens). This is similar to the problem encountered by BPE when trained on the dataset in eq.15 - although the algorithm has allocated a large number of tokens, the token \(1\) is maximal since every other token begins with the character \(0\)However, it turns out that such tokens, although present in the dictionary, are not observed frequently while encoding a fresh string drawn from the source.

### Analysis of Algorithm 1

In this section we prove a rephrased version of Theorem B.2 which implies the statement in the main paper. Define \(d_{0}=\frac{\epsilon d}{2\log(4|\mathcal{A}|)}\).

**Theorem B.3** (Rephrased Theorem B.2).: _Run Algorithm 1 on a dataset of \(n=\Theta(d^{2})\) characters to learn a dictionary with at most \(d\) tokens. The resulting tokenizer \(\mathcal{T}\) satisfies one of the following \(3\) conditions,_

1. _Either,_ \(|\textsf{Dict}|>d_{0}\)_, and,_ \[\min_{Q\in\mathcal{Q}_{1\text{-}\text{\emph{gen}}}}\mathcal{L}(Q\circ \textsf{enc}(\cdot))\leq\frac{H_{\infty}}{1-\varepsilon}.\] _Here,_ \(\varepsilon=O\left(\frac{\log^{3}(1/\delta)\log(1/\varepsilon)}{\epsilon \delta^{9}\log(d)}\right)\)_._
2. \(\Pr(|\textsf{Dict}|<d_{0})=e^{-\Omega(\varepsilon^{2}d/\log^{2}(1/\delta))}\)_, or,_
3. _Conditional on_ \(|\textsf{Dict}|<d_{0}\)_, with probability_ \(\geq 1-e^{-\Omega(\epsilon^{2}d/\log^{2}(1/\delta))}\)_,_ \[\min_{Q\in\mathcal{Q}_{1\text{-}\text{\emph{gen}}}}\mathcal{L}(Q\circ\textsf {enc}(\cdot))\leq\left(1-\frac{2d_{0}}{d}\right)\left(2H_{\infty}+O\left( \frac{1}{\log(d)}\right)\right)+\frac{2d_{0}}{d}\log(4|\mathcal{A}|).\] _With the choice of_ \(d_{0}=\epsilon d/2\log(4|\mathcal{A}|)\) _we get the statement of Theorem_ B.2_._

### Analysis for the large dictionary case: \(|\textsf{Dict}|>d_{0}\)

In the large dictionary case, Algorithm 1 uses the greedy encoder/decoder pair in conjunction with the dictionary. The proof of Theorem B.2 relies on establishing that the cross-entropy \(H(Q_{\text{MLE}},P)\) of the tokenizer is large. Namely, we prove that,

**Lemma B.4**.: _In Algorithm 1, assuming at least \(d_{0}\) tokens are allocated,_

\[H(Q_{\text{MLE}},P)=\Omega\left(\frac{\epsilon\delta^{9}\log(d)}{\log(1/ \epsilon)\log^{3}(1/\delta)}\right).\]

To show this, it suffices to argue that conditioned on any previous set of tokens, with nontrivial probability over the underlying string generated from the stochastic source, the next token is long (i.e. having conditional probability at most \(O(1/\sqrt{d})\)).

**Lemma B.5**.: _Suppose that in a run of Algorithm 1, at least \(d_{0}\) tokens are allocated. Suppose a set of tokens \(\bm{t}_{1},\cdots,\bm{t}_{k}\) have been sampled so far by the greedy encoder. Let \(T_{i+1}\) be the random variable which denotes the next token returned by the greedy encoder, where the randomness comes from the underlying string being tokenized. Then,_

\[\Pr\left(P(T_{i+1}|\bm{t}_{i})\leq 1/\sqrt{C\delta d}\Big{|}\bm{t}_{1},\cdots, \bm{t}_{i}\right)\geq\frac{d_{0}\delta^{6}(1-\delta)^{2}}{8Cd\Delta|\mathcal{ A}|\log(2|\mathcal{A}|)n_{D}}=\Omega\left(\frac{\epsilon\delta^{9}}{\log^{3}(1/ \delta)\log(1/\epsilon)}\right)\]

Proof sketch of Lemma b.5.: The proof will proceed in \(2\) parts. We first show in Lemma B.9 that there is a set \(D_{\text{valid}}\) of \(\Omega(d)\) tokens in the dictionary which are neither prefixes nor suffixes of any other token in \(\textsf{Dict}\). The reason for considering this set of tokens is twofold,

1. Irrespective of what the previous set of tokens were, it is legal for a token \(D_{\text{valid}}\) to be sampled in the current step by the greedy encoder, since for any candidate \(\bm{t}\in D_{\text{valid}}\), by definition, \(\bm{t}_{j}\cdots\bm{t}_{i}\bm{t}\not\in\textsf{Dict}\) for every \(j\leq i\).

2. Suppose a sequence of tokens \(\bm{t}_{1},\cdots,\bm{t}_{i}\) have already been sampled, ending with the character \(a\). Then, we may sample the next token using rejection sampling. Sample \(a^{\prime}\sim P(\cdot|a)\) and an infinitely long trajectory on \(\mathcal{T}^{\star}_{a^{\prime}}\). Return the last token on this trajectory which belongs to \(\mathsf{Dict}\), and if it so happens that \(\exists j\in[i]\) such that \(\bm{t}_{j}\cdots\bm{t}_{i}\bm{t}\in\mathsf{Dict}\), then reject this trajectory and repeat. Since all the tokens in \(D_{\text{valid}}\) are not prefixes of another token, any trajectory which reaches a token in \(D_{\text{valid}}\) must terminate the rejection sampling process.

Next, in Lemma B.10, we show that since the number of tokens in \(D_{\text{valid}}\) is sufficiently large, \(\Omega(d)\), with constant (in \(d\)) probability, a trajectory rolled out in the first round of the rejection sampling process will reach a token \(\bm{t}\in D_{\text{valid}}\) which has small probability, i.e. \(\max_{a\in A}P(\bm{t}|a)\leq 1/\mathsf{poly}(d)\). By the previous arguments, this must mean that the rejection sampling process terminates on this "low probability" token, resulting in the statement of the lemma. 

Proof of Theorem b.3.1 and Lemma b.4It is easy to see why Lemma B.5 implies a lower bound on the cross entropy \(H(Q_{\text{MLE}},P)\) of the tokenizer. By Lemma A.4 for the greedy encoder,

\[Q_{\text{MLE}}(\bm{t})=\lim_{m\to\infty}\mathbb{E}\left[\frac{n_{\bm{t}}}{ \sum_{\bm{t}^{\prime}}n_{\bm{t}^{\prime}}}\right]\stackrel{{\text {a.s.}}}{{=}}\lim_{m\to\infty}\frac{n_{\bm{t}}}{\sum_{\bm{t}^{\prime}}n_{\bm{t} ^{\prime}}}.\] (16)

Since the limit \(m\to\infty\) of the RHS exists by Lemma A.4, we may let \(m\to\infty\) in any way we like, and in particular we may simply sample \(i^{\star}\) tokens, \(\bm{t}_{1},\cdots,\bm{t}_{i^{\star}}\) sequentially according to the process in Figure 7. Here, the first token sampled is returned by generating an infinitely long string on \(\mathcal{T}^{\star}_{a}\) where \(a\sim\pi\) and then truncating this trajectory to the longest token which belongs to \(\mathsf{Dict}\). Subsequently for every \(i>1\), \(\bm{t}_{i}\) is generated by sampling a fresh infinitely long string from \(\mathcal{T}^{\star}_{a}\) where \(a\) is sampled from the \(P(\cdot|a^{\prime})\) where \(a^{\prime}\) is the last character \(\bm{t}_{i-1}\) and then returning the largest prefix of this string which is a token in \(\mathsf{Dict}\), conditioned on \(\bm{t}_{j}\cdots\bm{t}_{i-1}\bm{t}_{i}\not\in\mathsf{Dict}\) for any \(j<i\).

Figure 7: _Jointly generating a sequence and its greedy encoding:_ In this example we use the greedy encoder under the dictionary composed of all the substrings shadowed red. The first character (\(a\)) is sampled from the stationary distribution. Then an infinite string is sampled on the tree with \(a\) as root (green path). The last substring on this path which is a token (\(\bm{t}_{1}=abb\)) is returned by the greedy encoder. Then the next character \(x=b\) is sampled from the source conditioned on the previous character (\(b\)) and further conditioned on \(\bm{t}_{1}x\not\in\mathsf{Dict}\). Finally, another infinite string is sampled on the tree with \(x=b\) as root (purple path) and the last substring on this path which is a token (\(\bm{t}_{2}=ba\)) is returned by the greedy encoder. Repeating this process, we can generate a string, here, \(abbba\cdots\), as well as its greedy encoding, (\(abb,ba,\cdots\)).

and concatenate the corresponding substrings to get an \(m=\sum_{i=1}^{i}|\bm{t}_{i}|\) length character string. Letting \(i^{\star}\to\infty\), we must have \(m\to\infty\) surely since \(m\geq i^{\star}\). In this view, eq. (16) can be rewritten as,

\[Q_{\text{MLE}}(\bm{t})=\lim_{i^{\star}\to\infty}\frac{n_{\bm{t}}}{i^{\star}}= \lim_{i^{\star}\to\infty}\frac{1}{i^{\star}}\sum_{i=1}^{i^{\star}}\mathbb{I}( \bm{t}_{i}=\bm{t})\overset{\text{a.s.}}{=}\lim_{i^{\star}\to\infty}\frac{1}{i^ {\star}}\sum_{i=1}^{i^{\star}}\mathbb{E}[\mathbb{I}(\bm{t}_{i}=\bm{t})|\bm{t}_{ 1},\cdots,\bm{t}_{i-1}]\] (17)

where the last inequality follows by the sequential nature of the token sampling process and a martingale argument. Consider the set of tokens \(T\) such that \(\bm{t}\in T\) satisfies \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\leq\sqrt{1/C\delta^{3}d}\). From eq. (17), summing across \(\bm{t}\in T\), we have that,

\[\sum_{\bm{t}\in T}Q_{\text{MLE}}(\bm{t})\overset{\text{a.s.}}{=}\lim_{i^{ \star}\to\infty}\frac{1}{i^{\star}}\sum_{i=1}^{i^{\star}}\Pr\left(\bm{t}_{i} \in T|\bm{t}_{1},\cdots,\bm{t}_{i-1}\right)=\Omega\left(\frac{\epsilon\delta^ {9}}{\log^{3}(1/\delta)\log(1/\epsilon)}\right)\] (18)

where in the last inequality, we use Lemma B.5 and the fact that \(\delta\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq\min_{a\in\mathcal{A}}P(\bm{t}|a)\). Therefore,

\[H(Q_{\text{MLE}},P)\geq\sum_{\bm{t}\in T}Q_{\text{MLE}}(\bm{t})\log(1/P(\bm{t }))\geq\sum_{\bm{t}\in T}Q_{\text{MLE}}(\bm{t})\log(\sqrt{C\delta^{3}d})\]

where in \((i)\) we use the fact that for \(\bm{t}\in T\), \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\leq 1/\sqrt{C\delta^{3}d}\), which implies that \(P(\bm{t})\leq 1/\sqrt{C\delta^{3}d}\). Finally, combining with eq. (18) completes the proof of Lemma B.4. Furthermore, since the cross-entropy \(H(Q_{\text{MLE}},P)\) was established to be large, by invoking the reduction in Theorem 3.4, we complete the proof of Theorem B.3.1.

Notation.For each \(a\in\mathcal{A}\) and \(j\in\mathbb{N}\cup\{0\}\), define a _level set_ of substrings,

\[S_{j}^{a}=\left\{(1-\delta)^{j+1}<P(\bm{t}|\bm{t}_{1}=a)\leq(1-\delta)^{j}\right\}\]

Figure 8: The circled nodes indicate substrings which are tokens in \(\mathsf{Dict}\). The red boundary is the set of substrings \(\bm{t}\) such that \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq 1/Cd\). By Lemma B.8, none of the nodes which fall outside this boundary are assigned as tokens in a run of Algorithm 1. The set of circled substrings are the set of tokens in \(\mathsf{Dict}\). Among them, the ones circled green are the tokens in \(D_{\text{valid}}\), which are not prefixes or suffixes of any other tokens in \(\mathsf{Dict}\). Substrings such as \(cb\) or \(ba\) which are tokens in \(\mathsf{Dict}\) do not belong to \(D_{\text{valid}}\) because they are prefixes of longer tokens (in this case, \(cbb\) and \(bab\) respectively). On the other hand, substrings like \(ab\) do not belong to \(D_{\text{valid}}\) since they are suffixes of tokens in \(\mathsf{Dict}\), in this case, \(bab\). Lemma B.9 asserts that the number of tokens in \(D_{\text{valid}}\) are \(\Omega(d)\) in number, assuming that \(\mathsf{Dict}\) has \(\Omega(d)\) tokens to begin with.

where \(\bm{t}_{1}\) denotes the first character of \(\bm{t}\). And likewise, define the sets \(S_{j}=\cup_{a\in\mathcal{A}}S_{j}^{a}\), \(S_{\leq j}^{a}\) and \(S_{\geq j}^{a}\) as the union of \(S_{j^{\prime}}^{a}\) over \(j^{\prime}\geq j\), \(j^{\prime}\leq j\) and \(S_{\leq j}\) and \(S_{\geq j}\) as the union of \(S_{\leq j}^{a}\) and \(S_{\geq j}^{a}\) over \(a\in\mathcal{A}\). Furthermore for a large universal constant \(C>0\), define parameters,

\[\Delta=\frac{\log(\delta)}{\log(1-\delta)}\asymp\Theta\left(\frac{\log(1/ \delta)}{\delta}\right);\quad n_{D}=1-\frac{2\log(4Cd/\delta d_{0})}{\log(1- \delta)}\asymp\Theta\left(\frac{\log(1/\epsilon\delta)}{\delta}\right).\] (19)

We first begin by stating a folklore result: every pair of tokens assigned by a merging-based dictionary generation algorithm have distinct character representations.

**Lemma B.6**.: _If Algorithm 1 assigns a new token in some round, it's character representation must be distinct from that of all previously assigned tokens._

Proof.: A pictorial proof is in Figure 9. We will prove this result by contradiction. Suppose \(\bm{t}\) and \(\bm{t}^{\prime}\) are tokens which decode to the same character substring, \(\bm{s}^{\prime}\). Consider all occurrences of \(\bm{s}^{\prime}\) in the dataset which in some iteration encode into \(\bm{t}^{\prime}\) or \(\bm{t}^{\prime\prime}\), and denote these disjoint locations \(\mathcal{S}\). Recall that at these locations, \(\bm{s}^{\prime}\) eventually is assumed to map to a singular token \(\bm{t}^{\prime}\) or \(\bm{t}^{\prime\prime}\). Therefore, at every step in the merging process these occurrences of \(\bm{s}^{\prime}\) must perfectly map to a sequence of tokens.

Now consider the merging process at the first time before any of the rules corresponding to tokens in \(\bm{t}^{\prime}\) or \(\bm{t}^{\prime\prime}\) are implemented. Prior to this time, all the occurrences of \(\bm{s}^{\prime}\) corresponding to the locations in \(\mathcal{S}\) have not been tokenized yet. When the first rule corresponding to one of the tokens in \(\{\bm{t}^{\prime},\bm{t}^{\prime\prime}\}\) is implemented, all the strings in \(\mathcal{S}\) must be modified identically. This uses the fact that we can isolate each of these occurrences of \(\bm{s}^{\prime}\) while carrying out the merging process, since each location must be distinct. At every step, the encodings of these copies of \(\bm{s}^{\prime}\) must be the same, and therefore \(\bm{t}^{\prime}\) and \(\bm{t}^{\prime\prime}\) cannot be two distinct tokens. 

**Lemma B.7**.: _The size of the level set \(S_{j}^{a}\) is bounded by \((1-\delta)^{-(j+1)}\)._

Proof.: Since the probability of any transition is at most \(1-\delta\), this implies that any infinite trajectory on the tree \(\mathcal{T}_{a}^{\star}\) can intersect at most one vertex in \(S_{j}^{a}\). Therefore, \(\sum_{\bm{t}\in S_{j}^{a}}P(\bm{t}|\bm{t}_{1}=a)\leq 1\). By the lower bound on \(P(\bm{t}|\bm{t}_{1}=a)\) for \(\bm{t}\in S_{j}^{a}\), this implies the statement of the lemma.

Figure 9: A pictorial representation of the proof of Lemma B.6

Next we show that with high probability none of the substrings \(\bm{t}\) having probability mass (under \(P\)) of at most \(\delta/Cd\) conditioned on the first character, are assigned as tokens by Algorithm 1.

**Lemma B.8**.: _In a run of Algorithm 1, for a sufficiently large constant \(C>0\), with probability \(d^{-\Omega(1)}\mathsf{poly}(1/\delta)\) all assigned tokens \(\bm{t}\in\mathsf{Dict}\) satisfy \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq 1/Cd\). In other words, none of the substrings in \(S_{\geq j^{\star}}\) are added as tokens to the dictionary in a run of Algorithm 1, where,_

\[j^{\star}\triangleq\log(\delta/Cd)/\log(1-\delta)\]

Proof.: Consider some \(j\geq j^{\star}\) and \(a\in\mathcal{A}\) and substring \(\bm{t}\in S_{j}^{a}\). In the \(i^{th}\) stage of the algorithm where \(\mathsf{text}_{i}\) is being processed, for \(\bm{t}\) to be assigned as a token, at the very least, \(\bm{t}\) must appear at least \(\log(d)\) times disjointly in \(\mathsf{text}_{i}\). Therefore,

\[P(\bm{t}\in S_{j}^{a}\text{ is assigned as a token in }\mathsf{ text}_{i}) \leq\binom{d}{\log(d)}\left(\max_{a\in\mathcal{A}}P(\bm{t}|a) \right)^{\log(d)}\] \[\leq d^{\log(d)}\left(\frac{1}{Cd}(1-\delta)^{j-j^{\star}}\right) ^{\log(d)}\] \[\leq d^{-\log(C)}(1-\delta)^{(j-j^{\star})\log(d)}\]

Union bounding over \(S_{j}^{a}\) over \(j\geq j^{\star}\) using the bound on \(|S_{j}^{a}|\) in Lemma B.7, and over \(a\in\mathcal{A}\) and \(i\in[d]\) results in the bound,

\[P(\bm{t}\in S_{\geq j^{\star}}\text{ is assigned as a token in step }i\text{ for some }i\in[d])\leq d^{-\Omega(1)}\sum_{j\geq j^{\star}}\frac{(1-\delta)^{(j-j^{ \star})\log(d)}}{(1-\delta)^{j+1}}\leq\frac{d^{-\Omega(1)}}{\delta(1-\delta)}\]

**Lemma B.9**.: _Consider the set of tokens \(D_{\text{valid}}\) which are not a prefix or a suffix of any other token in \(\mathsf{Dict}\). That is, \(D_{\text{valid}}=\{\bm{t}\in\mathsf{Dict}:\exists\bm{s}:\bm{st}\in\mathsf{Dict}\}\cap\{\bm{t}\in\mathsf{Dict}:\exists\bm{s}:\bm{ts}\in \mathsf{Dict}\}\). If \(|\mathsf{Dict}|\geq d_{0}\), then,_

\[|D_{\text{valid}}|\geq\frac{d_{0}}{4n_{D}}.\]

_where \(n_{D}\) is defined in eq. (19)._

Proof.: For any token \(\bm{t}\in D_{\text{valid}}\), there may be at most \(2|\bm{t}|\) tokens which are suffixes or prefixes of it and belong to \(\mathsf{Dict}\). More importantly, every token in \(\mathsf{Dict}\) not belonging to \(D_{\text{valid}}\) must either be a prefix or a suffix of some token in \(D_{\text{valid}}\). Split the suffixes and prefixes of the tokens in \(D_{\text{valid}}\) into four sets,

1. \(S_{\text{suff},\min}=\bigcup_{\bm{t}\in D_{\text{valid}}}\{\bm{t}^{\prime}\in \mathsf{Dict}:\bm{t}^{\prime}\in\mathsf{suffix}(\bm{t}),\ |\bm{t}^{\prime}|\leq|\bm{t}|-n_{D}\}\),
2. \(S_{\text{suff},\max}=\bigcup_{\bm{t}\in D_{\text{valid}}}\{\bm{t}^{\prime}\in \mathsf{Dict}:\bm{t}^{\prime}\in\mathsf{suffix}(\bm{t}),\ |\bm{t}^{\prime}|>|\bm{t}|-n_{D}\}\),
3. \(S_{\text{pre},\min}=\bigcup_{\bm{t}\in D_{\text{valid}}}\{\bm{t}^{\prime}\in \mathsf{Dict}:\bm{t}^{\prime}\in\mathsf{pre}(\bm{t}),\ |\bm{t}^{\prime}|\leq|\bm{t}|-n_{D}\}\),
4. \(S_{\text{pre},\max}=\bigcup_{\bm{t}\in D_{\text{valid}}}\{\bm{t}^{\prime}\in \mathsf{Dict}:\bm{t}^{\prime}\in\mathsf{pre}(\bm{t}),\ |\bm{t}^{\prime}|>|\bm{t}|-n_{D}\}\).

where \(n_{D}\) is defined in eq. (19). Note from Lemma B.8 that all the tokens \(\bm{t}\in\mathsf{Dict}\) all satisfy \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq 1/Cd\). Therefore, the tokens in \(S_{\text{pre},\min}\) and \(S_{\text{suff},\min}\) all satisfy, \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\geq d/C(1-\delta)^{n_{D}}\). By summing Lemma B.7 over appropriate \(j\), we get that \(|S_{\text{pre},\min}|+|S_{\text{suff},\min}|\leq 2Cd(1-\delta)^{n_{D}-1}/\delta\).

On the other hand, corresponding to any \(\bm{t}\in D_{\text{valid}}\), there are at most \(n_{D}\) tokens in \(S_{\text{pre},\max}\) or \(S_{\text{suff},\max}\) and and therefore \(|S_{\text{pre},\max}|,|S_{\text{suff},\max}|\leq n_{D}\cdot|D_{\text{valid}}|\). Since every token in \(\mathsf{Dict}\) either belongs to \(D_{\text{valid}}\) or is a suffix of some token in \(D_{\text{valid}}\), \(S_{\text{pre},\min}\cup S_{\text{suff},\min}\cup S_{\text{suff},\max}=|\mathsf{ Dict}|\) and,

\[2n_{D}\cdot|D_{\text{valid}}|+\frac{2C(1-\delta)^{n_{D}-1}d}{\delta}\geq d_{0}\]Recalling the choice of \(n_{D}=1-\frac{2\log(4Cd/\delta d_{0})}{\log(1-\delta)}\), we get that,

\[|D_{\text{valid}}|\geq\frac{d_{0}}{4n_{D}}.\]

**Lemma B.10**.: _Suppose Algorithm 1 assigns at least \(d_{0}\) tokens. For any character \(a\in\mathcal{A}\), sample an \(a^{\prime}\sim P(\cdot|a)\) and an infinite trajectory on the tree \(\mathcal{T}_{a^{\prime}}^{\star}\), denoted traj. Then,_

\[\mathbb{E}_{a^{\prime}\sim P(\cdot|a)}\left[\Pr_{\bm{t}\bm{a}\sim \mathcal{T}_{a^{\prime}}^{\star}}\left(\min_{\bm{t}\in\bm{t}\bm{a}\cap D_{ \text{valid}}}P(\bm{t}|a)\leq\sqrt{\delta/Cd}\bigg{|}a^{\prime}\right)\right] \geq\frac{d_{0}\delta^{6}(1-\delta)^{2}}{8Cd\Delta|\mathcal{A}|n_{D}}.\]

_where the notation \(\mathcal{T}_{a^{\prime}}^{\star}\) is used to overload the distribution over infinite trajectories on \(\mathcal{T}_{a^{\prime}}^{\star}\). The parameters \(n_{D}\) and \(\Delta\) are defined in eq.19._

Proof.: By Lemma B.8, recall that the \(\geq d_{0}\) tokens assigned in a run of Algorithm 1, with high probability, are substrings in \(S_{\leq j^{\star}}\). For any \(a\in\mathcal{A}\), the total number of substrings in \(S_{\leq j^{\star}}\) can be bounded as,

\[|S_{\leq j^{\star}}|\leq\sum_{a\in\mathcal{A}}\sum_{j=0}^{j^{\star}}|S_{j}^{a} |\leq\sum_{a\in\mathcal{A}}\sum_{j=0}^{j^{\star}}\frac{1}{(1-\delta)^{j+1}} \leq\frac{C|\mathcal{A}|d}{\delta(1-\delta)}.\] (20)

In order to prove this result, we use a counting argument and the fact that no tokens in \(S_{>j^{\star}}\) are assigned. Consider some character \(a\) and all the leaves in the forest \(S_{\leq j^{\star}}\). Since every transition has \(\geq\delta\) probability of occurring, across all leaf nodes \(\bm{t}\in S_{\leq j^{\star}}\), \(P(\bm{t}|a^{\prime})\) are within a \(\delta^{2}(1-\delta)\) factor of each other across different \(a^{\prime}\in\mathcal{A}\). In particular, by counting the number of paths in \(\mathcal{T}^{\star}\) (i.e. paths in \(\mathcal{T}_{a}^{\star}\) from \(\emptyset\) to leaf nodes in \(S_{\leq j^{\star}}\) across \(a\in\mathcal{A}\)) along which a token in Dict exists in \(S_{\geq j^{\star}/2}\), we can also compute the probability mass across such trajectories up to a factor of \(\delta^{2}(1-\delta)\).

Taking the union across \(a\in\mathcal{A}\), consider the paths in \(\mathcal{T}_{a}^{\star}\) from \(\emptyset\) to leaf nodes in \(S_{\leq j^{\star}}^{a}\). From Lemma B.9, \(\sum_{j\leq j^{\star}}|D_{\text{valid}}\cap S_{j}|\geq d_{0}/4n_{D}\), where \(n_{D}=1-2\log(4Cd/\delta d_{0})/\log(1-\delta)\). Note that for sufficiently large \(d=\Omega(\log(1/\epsilon\delta)/\delta^{5})\), by Lemma B.7, \(\sum_{j\leq j^{\star}/2}|S_{j}|=\sqrt{Cd/\delta}/\delta(1-\delta)\leq d_{0}/ 8n_{D}\). Therefore,

\[\sum_{j^{\star}/2<j\leq j^{\star}}|D_{\text{valid}}\cap S_{j}|\geq\frac{d_{0 }}{8n_{D}}.\] (21)

Define \(\Delta=\log(\delta)/\log(1-\delta)\). Combining eq.21 with eq.20 and applying the probabilistic method, there exists an \(i^{\star}\geq j^{\star}/2\) such that,

\[\frac{|D_{\text{valid}}\cap(S_{i^{\star}+1}\cup\cdots\cup S_{i^{\star}+ \Delta})|}{|S_{i^{\star}+1}\cup\cdots\cup S_{i^{\star}+\Delta}|}\geq\frac{ \delta(1-\delta)d_{0}}{8Cd|\mathcal{A}|n_{D}}.\] (22)

Note that \(\Delta\) is chosen to be sufficiently large, so that every infinite trajectory on \(\mathcal{T}_{a^{\prime}}^{\star}\) must intersect at least once with the band of vertices \(S_{i^{\star}+\Delta+1}^{a^{\prime}}\cup\cdots\cup S_{i+2\Delta}^{a^{\prime}}\). Note that this band is different from the one considered in eq.22. Define \(L_{a^{\prime}}\) as the set of longest prefixes across infinite trajectories in \(\mathcal{T}_{a^{\prime}}^{\star}\) which belong to \(S_{i^{\star}+\Delta+1}^{a^{\prime}}\cup\cdots\cup S_{i+2\Delta}^{a^{\prime}}\).

Note that our objective is to show that an infinite trajectory sampled on \(\mathcal{T}_{a^{\prime}}^{\star}\) where \(a^{\prime}\sim P(\cdot|a)\), has a long prefix in Dict. We can truncate this trajectory to lower bound this probability, and therefore, we assume that the infinite trajectories on \(\mathcal{T}_{a^{\prime}}^{\star}\) terminate once they reach a substring in \(L_{a^{\prime}}\). Furthermore, note that although \(\Delta\) is large, it is still a constant depending on \(\delta\). Therefore, the band of states \(S_{i^{\star}+\Delta+1}^{a^{\prime}}\cup\cdots\cup S_{i^{\star}+2\Delta}^{a^{ \prime}}\) is not too wide, and all the substrings in \(L_{a^{\prime}}\) have approximately similar probabilities to each other. In particular, for any character \(a\in\mathcal{A}\), and for any \(a^{\prime}\in\mathcal{A}\) and \(\bm{t}\in L_{a^{\prime}}\), decomposing \(P(\bm{t}|a)\) as \(P(\bm{t}|\bm{t}_{1}=a^{\prime})P(a^{\prime}|a)\),

\[\delta^{2}(1-\delta)\cdot(1-\delta)^{i+\Delta}\stackrel{{(i)}}{{ \leq}}P(\bm{t}|a)\stackrel{{(ii)}}{{\leq}}(1-\delta)^{i+\Delta}.\] (23)

Inequality \((i)\) follows from the fact that all transition probabilities are at least \(\delta\), so every leaf node in \(L_{a^{\prime}}\) must have \(P(\bm{t}|\bm{t}_{1}=a^{\prime})\geq(1-\delta)^{i+2\Delta+1}\), and the fact that \(P(a^{\prime}|a)\geq\delta\). Inequality \((ii)\)follows similarly from the fact that \(\bm{t}\) is a leaf node of \(L_{a^{\prime}}\) and therefore \(P(\bm{t}|\bm{t}_{1}=a^{\prime})\leq(1-\delta)^{i+\Delta}\). Therefore, instead of bounding the probability of any event under the distribution over substrings in \(L_{a^{\prime}}\) induced by truncating the infinite strings sampled on \(\mathcal{T}^{*}_{a^{\prime}}\), it suffices to count the fraction of substrings in \(L_{a^{\prime}}\) satisfying the event (which are equivalent up to a \(\delta(1-\delta)\) factor). Define,

\[\mathsf{pre}(\bm{t})=(\bm{t}_{1},\bm{t}_{1:2},\bm{t}_{1:3},\cdots,\bm{t}_{1:| \bm{t}|})\]

As the set of prefixes of \(\bm{t}\) (including \(\bm{t}\)). Note that at most \(\Delta\) of the prefixes of any substring \(\bm{t}\) can intersect with \(S^{a}_{i^{\star}+1}\cup\cdots\cup S^{a}_{i^{\star}+\Delta}\). Therefore,

\[\sum_{a^{\prime}\in\mathcal{A}} \sum_{\bm{t}\in L_{a^{\prime}}}\bm{1}(\mathsf{pre}(\bm{t})\cap D_ {\text{valid}}\cap(S^{a^{\prime}}_{i^{\star}+1}\cup\cdots\cup S^{a^{\prime}}_{ i^{\star}+\Delta})\neq\emptyset)\] \[\geq\sum_{a^{\prime}\in\mathcal{A}}\sum_{\bm{t}\in L_{a^{\prime} }}\frac{|\mathsf{pre}(\bm{t})\cap D_{\text{valid}}\cap(S^{a^{\prime}}_{i^{ \star}+1}\cup\cdots\cup S^{a^{\prime}}_{i^{\star}+\Delta})|}{\Delta}\] \[\stackrel{{(i)}}{{\geq}}\sum_{a^{\prime}\in\mathcal{ A}}\frac{|D_{\text{valid}}\cap(S^{a^{\prime}}_{i^{\star}+1}\cup\cdots\cup S^{a^{ \prime}}_{i^{\star}+\Delta})|}{\Delta}\] \[\stackrel{{(ii)}}{{\geq}}\frac{\delta d_{0}(1-\delta )}{8Cd\Delta|\mathcal{A}|n_{D}}\sum_{a^{\prime}\in\mathcal{A}}|S^{a^{\prime}}_ {i^{\star}+1}\cup\cdots\cup S^{a^{\prime}}_{i^{\star}+\Delta}|\] \[\stackrel{{(iii)}}{{\geq}}\frac{\delta^{3}d_{0}(1- \delta)}{8Cd\Delta|\mathcal{A}|n_{D}}\sum_{a^{\prime}\in\mathcal{A}}|L_{a^{ \prime}}|,\]

where \((i)\) uses the fact that the prefixes of \(\bm{t}\in L_{a^{\prime}}\) cover all the substrings in \(S^{a^{\prime}}_{\leq i^{\star}+\Delta}\), and therefore \(\cup_{\bm{t}\in L_{a^{\prime}}}\mathsf{pre}(\bm{t})\supset S^{a^{\prime}}_{i^ {\star}+1}\cup\cdots\cup S^{a^{\prime}}_{i^{\star}+\Delta}\), and \((ii)\) uses eq. (22). Finally, \((iii)\) uses the fact that \(\Delta\) is not too large, and therefore, for any substring \(\bm{t}^{\prime}\in S^{a^{\prime}}_{i^{\star}+1}\cup\cdots\cup S^{a^{\prime}}_ {i^{\star}+\Delta}\), there are at most \(1/(1-\delta)^{2\Delta}=1/\delta^{2}\) substrings \(\bm{t}\in L_{a^{\prime}}\) which contain it as a prefix. This means, \(|L_{a^{\prime}}|\leq|S^{a^{\prime}}_{i^{\star}+1}\cup\cdots\cup S^{a^{\prime}}_ {i^{\star}+\Delta}|/\delta^{2}\). After dividing by \(\sum_{a^{\prime}\in\mathcal{A}}|L_{a^{\prime}}|\) on both sides, this implies,

\[\mathbb{E}_{a^{\prime}\sim\mathrm{Unif}(\mathcal{A})}\left[\Pr_{\bm{t}\sim \mathrm{Unif}(L_{a^{\prime}})}\left(\mathsf{pre}(\bm{t})\cap D_{\text{valid}} \cap(S^{a^{\prime}}_{i^{\star}+1}\cup\cdots\cup S^{a^{\prime}}_{i^{\star}+ \Delta})\neq\emptyset\Big{|}a^{\prime}\right)\right]\geq\frac{\delta^{3}d_{0}(1- \delta)}{8Cd\Delta|\mathcal{A}|n_{D}}.\] (24)

The event inside the inner probability term is the event that an infinitely long string (truncated at \(L_{a^{\prime}}\)) has a prefix which lies in \(D_{\text{valid}}\) and which intersects with \(S^{a^{\prime}}_{i^{\star}+1}\cup\cdots\cup S^{a^{\prime}}_{i^{\star}+\Delta}\), which implies that it has probability \(P(\bm{t}|a)\leq\sqrt{\delta/Cd}\). Therefore, we have that for any \(a\in\mathcal{A}\), sampling an \(a^{\prime}\sim P(\cdot|a)\) and an infinite trajectory \(\mathsf{traj}\sim\mathcal{T}^{*}_{a^{\prime}}\),

\[\mathbb{E}_{a^{\prime}\sim P(\cdot|a)}\left[\Pr_{\bm{t}\bm{\mathsf{aj}}\sim \mathcal{T}^{*}_{a^{\prime}}}\left(\min_{\bm{t}\in\mathsf{traj}\cap D_{\text{ valid}}}P(\bm{t}|a)\leq\sqrt{\delta/Cd}\bigg{|}a^{\prime}\right)\right]\] \[\stackrel{{(i)}}{{\geq}}\delta^{2}(1-\delta)\cdot \mathbb{E}_{a^{\prime}\sim P(\cdot|a)}\left[\Pr_{\bm{t}^{\prime}\sim\mathrm{ Unif}(L_{a^{\prime}})}\left(\min_{\bm{t}\in\mathsf{pre}(\bm{t}^{\prime})\cap D_{\text{ valid}}}P(\bm{t}|a)\leq\sqrt{\delta/Cd}\bigg{|}a^{\prime}\right)\right]\] \[\stackrel{{(ii)}}{{\geq}}\delta^{2}(1-\delta)\cdot \mathbb{E}_{a^{\prime}\sim P(\cdot|a)}\left[\Pr_{\bm{t}^{\prime}\sim\mathrm{ Unif}(L_{a^{\prime}})}\left(\mathsf{pre}(\bm{t}^{\prime})\cap D_{\text{valid}}\cap(S^{a^{\prime}}_{i^{ \star}+1}\cup\cdots\cup S^{a^{\prime}}_{i^{\star}+\Delta})\neq\emptyset\Big{|}a^{ \prime}\right)\right]\] \[\stackrel{{(iii)}}{{\geq}}\delta^{3}(1-\delta)\cdot \mathbb{E}_{a^{\prime}\sim\mathrm{Unif}(\mathcal{A})}\left[\Pr_{\bm{t}^{\prime} \sim\mathrm{Unif}(L_{a^{\prime}})}\left(\mathsf{pre}(\bm{t}^{\prime})\cap D_{ \text{valid}}\cap(S^{a}_{i^{\star}+1}\cup\cdots\cup S^{a}_{i^{\star}+\Delta}) \neq\emptyset\big{|}a^{\prime}\right)\right]\] \[\geq\delta^{3}(1-\delta)\cdot\frac{\delta^{3}d_{0}(1-\delta)}{8Cd \Delta|\mathcal{A}|n_{D}}.\]

Here \((i)\) follows by truncating the trajectory \(\mathsf{traj}\) to terminate at a node in \(\cup_{a^{\prime}\in\mathcal{A}}L_{a^{\prime}}\) and from eq. (23), \((ii)\) follows by arguing that \(i^{\star}\leq j^{\star}/2\) and therefore if a prefix of \(\bm{t}^{\prime}\) lies in \(S^{a^{\prime}}_{i^{\star}+1}\cup\cdots\cup S^{a^{\prime}}_{i^{\star}+\Delta}\), then it must have \(P(\bm{t}|a)\leq\sqrt{\delta/Cd}\). Inequality \((iii)\) follows by noting that all the transitions \(P(a^{\prime}|a)\) have probability \(\geq\delta\), and the last inequality follows from eq. (24).

### Proof of Lemma b.5

Lemma B.10 concludes that given any previous sequence of tokens terminating in a character \(a\), with constant probability, an infinite trajectory sampled from \(\mathcal{T}^{*}_{a^{\prime}}\) with \(a^{\prime}\sim P(\cdot|a)\) has as prefix, a substring \(\bm{t}\), which not only has low probability, with \(P(\bm{t}|a)\leq\sqrt{\delta/Cd}\), but also belongs to the subset of tokens \(D_{\text{valid}}\). Note that regardless of the previously sampled tokens, it is legal to sample any token in \(D_{\text{valid}}\) as the current token, since by definition, these tokens are not the suffixes of any other tokens in \(\mathsf{Dict}\). Moreover, if any trajectory on \(\mathcal{T}^{*}_{a^{\prime}}\) reaches a token in \(D_{\text{valid}}\), then it must be largest token along that trajectory, since none of the tokens in \(D_{\text{valid}}\) are prefixes of another token in \(\mathsf{Dict}\).

Consider generating a new token by rejection sampling. Suppose the set of previous tokens \(\bm{t}_{1},\cdots,\bm{t}_{i}\) end in some character \(a\). Sample the next character \(a^{\prime}\sim P(\cdot|a)\) and an infinite trajectory on \(\mathcal{T}^{*}_{a^{\prime}}\). If it reaches an illegal token \(\bm{t}\) such that \(\bm{t}_{j}\bm{t}_{j+1}\cdots\bm{t}_{i}\bm{t}\) already exists in \(\mathsf{Dict}\), this token is rejected and the trajectory is resampled. By the prefix-free property of these tokens, if this trajectory visits a token in \(D_{\text{valid}}\), it must immediately be output as the next token. Note that this probability is lower bounded by,

\[\mathbb{E}_{a^{\prime}\sim P(\cdot|a)}\left[\Pr_{\bm{t}\bm{a}\sim\mathcal{T}^ {*}_{a^{\prime}}}\left(\min_{\bm{t}\in\mathsf{traif}\cap D_{\text{valid}}}P( \bm{t}|a)\leq\sqrt{\delta/Cd}\Big{|}a^{\prime}\right)\right]\]

which is lower bounded by \(\mathsf{poly}(\epsilon,\delta)\), the subject of Lemma B.10. Therefore with this probability, the process terminates in the first step with a token in \(D_{\text{valid}}\) being sampled.

### Analysis in the small dictionary case

In this section, we will prove Theorem B.3.2 and Theorem B.3.3. In particular we show that, either,

1. The dictionary is small with low probability. i.e., \(\Pr(|\mathsf{Dict}|<d_{0})=e^{-\Omega(\epsilon^{2}d/\log^{2}(1/\delta))}\), or,
2. Or conditioned on the dictionary being small, \(|\mathsf{Dict}|<d_{0}\), with high probability \(\geq 1-e^{-\Omega(\epsilon^{2}d/\log^{2}(1/\delta))}\), \[\min_{Q\in\mathcal{Q}_{1:\text{gen}}}\mathcal{L}(Q\circ\mathsf{enc}(\cdot)) \leq 4\left(1-\frac{2d_{0}}{d}+O\left(\frac{1}{\log(d)}\right)\right)H_{ \infty}+\frac{2d_{0}}{d}\cdot\log(2|\mathcal{A}|).\]

For \(i\in[d]\), define the indicator random variable,

\[X(\bm{s}^{\prime},\mathsf{Dict})=\bm{1}(\exists\text{a pair of tokens in }\mathsf{enc}_{\text{BPE}}(\bm{s}^{\prime})\text{ under }\mathsf{Dict}\text{ appears at least }\log(d)\text{ times}).\]

which captures the event that the string \(\bm{s}^{\prime}\) is compressed well by the dictionary \(\mathsf{Dict}\) under the sequential encoder.

Let \(\mathsf{Dict}_{i}\) denote the dictionary stored by Algorithm 1 right after \(\mathsf{text}_{i}\) is processed. The key insight behind this lemma is the following statement, asserting that the sequential encoder satisfies a "monotonicity" property: for any \(j\) and string \(\bm{s}^{\prime}\), if there exists a pair of tokens appearing more than \(\log(d)\) times consecutively in the sequential encoding of \(\bm{s}^{\prime}\) under \(\mathsf{Dict}_{j}\), then there must exist a pair of tokens appearing more than \(\log(d)\) times consecutively in the greedy encoding of \(\bm{s}^{\prime}\) under \(\mathsf{Dict}_{i}\) for any \(i<j\). This implies that \(X(\bm{s}^{\prime},\mathsf{Dict}_{j})\leq X(\bm{s}^{\prime},\mathsf{Dict}_{i})\) if \(i<j\) for any string \(\bm{s}^{\prime}\). This monotonicity property implies that the last dictionary output by the learner, \(\mathsf{Dict}_{d}\) sequentially encodes a \(1-\epsilon\) fraction of the previously seen texts, \(\mathsf{text}_{i}\) in a way where every pair of tokens appears at most \(\log(d)\) times. While \(\mathsf{Dict}_{d}\) is correlated with these texts, we can circumvent this correlation by using a martingale argument to prove the statement of the lemma.

**Lemma B.11**.: _Let \(\mathsf{Dict}\) be the dictionary returned by Algorithm 1. Then,_

\[\min\left\{\Pr\left(\mathbb{E}\big{[}X\big{(}\bm{s}^{\prime},\mathsf{Dict} \big{)}\big{|}\mathsf{Dict}\big{]}\geq 2d_{0}/d\Big{|}|\mathsf{Dict}|<d_{0} \right),\Pr\left(|\mathsf{Dict}|<d_{0}\right)\right\}\leq e^{-\epsilon^{2} d/8\log^{2}(1/\delta)}.\]

_where \(\bm{s}^{\prime}\) is a fresh substring of length \(d\) sampled from the stochastic source._

Proof.: Let \(\mathsf{Dict}_{i}\) denote the state of dictionary returned by Algorithm 1 right after \(\mathsf{text}_{i}\) is processed. Then, \(\mathsf{Dict}_{d}\) is the final dictionary returned by Algorithm 1. Suppose \(\mathbb{E}\big{[}X\big{(}\bm{s}^{\prime},\mathsf{Dict}_{d}\big{)}\big{|} \mathsf{Dict}_{d}\big{]}\geq 2d_{0}/d\)where \(\bm{s}^{\prime}\) is a fresh substring of length \(d\) sampled from the stochastic source. Using monotonicity of the sequential encoder, almost surely for any string \(\bm{s}^{\prime}\), \(X(\bm{s}^{\prime},\mathsf{Dict}_{i})\leq X(\bm{s}^{\prime},\mathsf{Dict}_{j})\) for any \(j>i\). Therefore,

\[\mathbb{E}\big{[}X\big{(}\bm{s}^{\prime},\mathsf{Dict}_{d}\big{)}\big{|} \mathsf{Dict}_{d}\big{]}\geq 2d_{0}/d\implies\sum\nolimits_{i=1}^{d-1} \mathbb{E}\big{[}X\big{(}\bm{s}^{\prime},\mathsf{Dict}_{i}\big{)}\big{|} \mathsf{Dict}_{i}\big{]}\geq 2d_{0}\cdot\frac{d-1}{d}\] (25)

Note in this expectation, \(\bm{s}^{\prime}\) is an independent string of length \(d\) sampled from the stochastic source. Since \(\mathsf{Dict}_{i}\) and \(\mathsf{text}_{i+1}\) are independent, we may instead write,

\[\sum\nolimits_{i=1}^{d-1}\mathbb{E}\big{[}X\big{(}\mathsf{text}_{i+1},\mathsf{ Dict}_{i}\big{)}\big{|}\mathsf{Dict}_{i},\mathsf{text}_{i},\mathsf{Dict}_{i-1}, \cdots,\mathsf{Dict}_{1},\mathsf{text}_{1}\big{]}\geq 2d_{0}\cdot\frac{d-1}{d}.\]

For brevity, denote \(X_{i}=X(\mathsf{text}_{i+1},\mathsf{Dict}_{i})\) and define the filtration \(\mathcal{F}_{i}=\sigma(\{\mathsf{text}_{1},\mathsf{Dict}_{1},\cdots,\mathsf{ text}_{i},\mathsf{Dict}_{i}\})\). Note that \(\sum_{j=1}^{i}X_{j}-\mathbb{E}[X_{j}|\mathcal{F}_{i}]\) forms a martingale sequence under the filtration \(\{\mathcal{F}_{i}:i\in[d]\}\). Therefore, by the Azuma-Hoeffding inequality, for any \(\eta>0\),

\[\Pr\left(\sum\nolimits_{i=1}^{d-1}\mathbb{E}[X_{i}|\mathcal{F}_{i}]-X_{i} \leq-\eta\right)\leq e^{-\eta^{2}}.\] (26)

Under Case I, we have that \(\sum_{i=1}^{d}X_{i}\leq d_{0}\). Therefore, from eq. (25) and eq. (26),

\[\Pr\left([\mathsf{Dict}]<d_{0};\ \mathbb{E}\left[X(\bm{s}^{ \prime},\mathsf{Dict})\big{|}\mathsf{Dict}\right]\geq 2d_{0}/d\right) \leq\Pr\left(\sum_{i=1}^{d-1}X_{i}<d_{0};\ \sum_{i=1}^{d-1}\mathbb{E}\left[X_{i}|\mathcal{F}_{i}\right]\geq 2d _{0}\cdot\frac{d-1}{d}\right)\] \[\leq\Pr\left(\sum_{i=1}^{d-1}\mathbb{E}\left[X_{i}|\mathcal{F}_{i }\right]-X_{i}\geq d_{0}\cdot\frac{d-2}{d}\right)\] \[\leq e^{-d_{0}^{2}(1-2/d)^{2}}\] \[\leq e^{-d_{0}^{2}/2}=e^{-\varepsilon^{2}d/8\log^{2}(1/\delta)}.\]

Finally, using the inequality \(\Pr(A,B)=\Pr(A|B)\Pr(B)\geq(\min\{\Pr(A),\Pr(B)\})^{2}\) completes the proof. 

Proofs of Theorem b.3.2 and Theorem b.3.3If \(\Pr(|\mathsf{Dict}|<d_{0})\leq\epsilon^{-\varepsilon^{2}d/8\log^{2}(1/\delta)}\) the proof of Theorem b.3.2 concludes. Otherwise, consider the case \(\Pr(|\mathsf{Dict}|<d_{0})>\epsilon^{-\varepsilon^{2}d/8\log^{2}(1/\delta)}\), whereby, \(\mathbb{E}[X(\bm{s}^{\prime},\mathsf{Dict})|\mathsf{Dict}]\leq 2d_{0}/d\) with probability \(\geq 1-e^{-\varepsilon^{2}d/8\log^{2}(1/\delta)}\) conditioned on \(|\mathsf{Dict}|<d_{0}\) by Lemma b.11. Recall that when \(|\mathsf{Dict}|<d_{0}\), Algorithm 1 uses a parallel implementation of the sequential encoder which chunks a new string into pieces of length \(d\), denoted \(\{\mathsf{chunk}_{i}:i\in[d]\}\) and uses the sequential encoder under \(\mathsf{Dict}_{d}\) to tokenize each chunk. Note that since the source is Markovian, the chunked process \(\{\mathsf{chunk}_{i}=(X_{id+1},X_{id+2},\cdots,X_{(i+1)d}):i=1,2,\cdots\}\) is also Markovian and ergodic. Therefore, by a similar limiting argument as in Lemma A.4, using the Krylov-Bogolyubov argument (cf. Proposition 4.2 in Chen (2018)) for Markov processes, we have that,

\[\lim_{\ell\to\infty}\frac{\sum_{i=1}^{\ell}X(\mathsf{chunk}_{i},\mathsf{Dict})}{\ell}= \mathbb{E}[X(\bm{s}^{\prime},\mathsf{Dict})]\leq\frac{2d_{0}}{d}.\]

where \(\bm{s}^{\prime}\) is a fresh string of length \(d\) sampled with initial state distribution as the stationary measure of the stochastic source. On the remaining (limiting) \(1-2d_{0}/d\) fraction of the chunks, their sequential encodings have every pair of tokens appearing at most \(\log(d)\) times consecutively. Using Theorem 1 of Navarro and Russo (2008), the number of tokens in the encoding of each of these chunks cannot be too large, and satisfies,

\[|\mathsf{enc}_{\mathsf{BPE}}(\mathsf{chunk}_{i})|\cdot\log|\mathsf{ enc}_{\mathsf{BPE}}(\mathsf{chunk}_{i})|\leq 2dH_{\infty}+O(d/\log(d))\] \[\implies |\mathsf{enc}_{\mathsf{BPE}}(\mathsf{chunk}_{i})|\cdot\log d\leq 2dH_{ \infty}+O(d/\log(d))\] (27)

For the (limiting) \(2d_{0}/d\) fraction of the "bad" chunks, their sequential encodings may have one or more pairs of tokens which appear more than \(\log(d)\) times consecutively.

Define \(\mathcal{E}_{i}=\{X(\mathsf{chunk}_{i},\mathsf{Dict})=1\}\) where \(\mathsf{Dict}=\mathsf{Dict}_{d}\) is the dictionary returned by Algorithm 1 and consider the unigram model \(Q_{\mathsf{uni}}(\bm{t})=\frac{1}{2}Q_{1}(\bm{t})+\frac{1}{2}Q_{2}(\bm{t})\), which is the uniform mixture of two models,

\[Q_{1}(\bm{t})\propto\frac{1}{(2|\mathcal{A}|)^{|\bm{t}|}},\quad \text{ and }\quad Q_{2}(\bm{t})=\mathbb{E}\left[\frac{n_{\bm{t}}^{1}}{|\mathsf{enc}_{ \mathsf{BPE.split}}(\mathsf{chunk}_{1})|}\Bigg{|}\mathcal{E}_{1}^{c}\right],\]

and let \(Q_{\mathsf{uni}}(\bm{t}_{1},\cdots,\bm{t}_{i})=Q_{\#}(j)\prod_{j=1}^{i}Q_{ \mathsf{uni}}(\bm{t}_{i})\) for some distribution \(Q_{\#}(i)\) over the number of tokens to be chosen later. We will analyze the case where the total number of chunks \(\ell\) is finite and take the limit \(m\to\infty\) later. Then, the overall loss of the algorithm is,

\[\mathcal{L}_{m}(Q_{\mathsf{uni}}\circ\mathsf{enc}(\cdot))\] \[=-\mathbb{E}[\log Q_{\mathsf{uni}}(\mathsf{enc}_{\mathsf{BPE.split }}(\bm{s}))]\] \[=-\sum_{\bm{t}\in\mathsf{Dict}}\mathbb{E}[n_{\bm{t}}\log Q_{ \mathsf{uni}}(\bm{t})+\log Q_{\mathsf{uni}}(|\mathsf{enc}_{\mathsf{BPE.split }}(\bm{s})|)]\] \[\overset{(i)}{=}-\sum_{i=1}^{\ell}\mathbb{E}\left[\sum_{\bm{t} \in\mathsf{Dict}}n_{\bm{t}}^{i}\log Q_{\mathsf{uni}}(\bm{t})\Bigg{|}\mathcal{E }_{i}\right]+\log(m)\] \[=-\sum_{i=1}^{\ell}\mathbb{E}\left[\sum_{\bm{t}\in\mathsf{Dict }}n_{\bm{t}}^{i}\log Q_{\mathsf{uni}}(\bm{t})\Bigg{|}\mathcal{E}_{i}\right] \Pr(\mathcal{E}_{i})+\mathbb{E}\left[\sum_{\bm{t}\in\mathsf{Dict}}n_{\bm{t}}^ {i}\log Q_{\mathsf{uni}}(\bm{t})\Bigg{|}\mathcal{E}_{i}^{c}\right]\Pr(\mathcal{ E}_{i}^{c})+\log(m).\] (28)

where \(n_{\bm{t}}^{i}\) is the number of times \(\bm{t}\) is observed in the BPE encoding of \(\mathsf{chunk}_{i}\) and \((i)\) uses the fact that \(|\mathsf{enc}_{\mathsf{BPE.split}}(\bm{s})|\) follows some distribution supported on \([m]\), which implies its entropy is upper bounded by \(\log(m)\). First observe that,

\[\sum_{i=1}^{\ell}\mathbb{E}\left[\sum_{\bm{t}\in\mathsf{Dict}}n_{ \bm{t}}^{i}\log Q_{\mathsf{uni}}(\bm{t})\Bigg{|}\mathcal{E}_{i}^{c}\right] \leq\sum_{i=1}^{\ell}\mathbb{E}\left[|\mathsf{enc}_{\mathsf{BPE}} (\mathsf{chunk}_{i})|\cdot\sum\nolimits_{\bm{t}\in\mathsf{Dict}}\frac{n_{\bm{t }}^{i}}{|\mathsf{enc}_{\mathsf{BPE}}(\mathsf{chunk}_{i})|}\log Q_{\mathsf{uni} }(\bm{t})\Bigg{|}\mathcal{E}_{i}^{c}\right]\] \[\overset{(i)}{\leq}\ell\left(\frac{2dH_{\infty}+O(d/\log(d))}{ \log(d)}\right)\sum\nolimits_{\bm{t}\in\mathsf{Dict}}Q_{2}(\bm{t})\log Q_{ \mathsf{uni}}(\bm{t})\]

where \((i)\) uses the upper bound on \(|\mathsf{enc}_{\mathsf{BPE.split}}(\mathsf{chunk}_{i})|\) under the event \(\mathcal{E}_{i}^{c}\) (eq. (27)). Since \(Q_{\mathsf{uni}}(\bm{t})=\frac{1}{2}Q_{1}(\bm{t})+\frac{1}{2}Q_{2}(\bm{t})\geq \frac{1}{2}Q_{2}(\bm{t})\) and \(Q_{2}\) is a distribution supported on at most \(d\) tokens, this term results in the upper bound,

\[\sum_{i=1}^{\ell}\mathbb{E}\left[\sum_{\bm{t}\in\mathsf{Dict}}n_{\bm{t}}^{i} \log Q_{\mathsf{uni}}(\bm{t})\Bigg{|}\mathcal{E}_{i}^{c}\right]\leq\ell\left( \frac{2dH_{\infty}+O(d/\log(d))}{\log(d)}\right)\log(2d).\] (29)

On the other hand, since \(Q_{\mathsf{uni}}(\bm{t})\geq\frac{1}{2}Q_{1}(\bm{t})\),

\[\sum_{i=1}^{\ell}\mathbb{E}\left[\sum_{\bm{t}\in\mathsf{Dict}}n_{ \bm{t}}^{i}\log(1/Q_{\mathsf{uni}}(\bm{t}))\Bigg{|}\mathcal{E}_{i}\right] \leq\sum_{i=1}^{\ell}\mathbb{E}\left[\sum_{\bm{t}\in\mathsf{Dict }}n_{\bm{t}}^{i}\log(2/Q_{1}(\bm{t}))\Bigg{|}\mathcal{E}_{i}\right]\] \[\leq\sum_{i=1}^{\ell}\mathbb{E}\left[\sum_{\bm{t}\in\mathsf{Dict}}n_ {\bm{t}}^{i}\left(\log(2)+|\bm{t}|\log(2|\mathcal{A}|)\right)\Bigg{|}\mathcal{E} _{i}\right]\] \[\leq\ell d\log(2)+\ell d\log(2|\mathcal{A}|)\] (30)

where the last inequality uses the fact that \(\sum_{\bm{t}\in\mathsf{Dict}}n_{\bm{t}}^{i}\leq d\) and \(\sum_{\bm{t}\in\mathsf{Dict}}|\bm{t}|n_{\bm{t}}^{i}=d\) computes the length of \(\mathsf{chunk}_{i}\).

Overall, since \(\sum_{i=1}^{\ell}\Pr(\mathcal{E}_{i})\leq 2d_{0}/d\) by eq. (27), combining this with eqs. (28) to (30),

\[\mathcal{L}_{m}(Q_{\mathsf{uni}}\circ\mathsf{enc}(\cdot))\leq\left(1-\frac{2d_{0} }{d}\right)\ell\left(\frac{2dH_{\infty}+O(d/\log(d))}{\log(d)}\right)\log(2d)+ \frac{2d_{0}}{d}\ell d\log(4|\mathcal{A}|).\]Dividing throughout by the length of the character sequence \(m\in[d(\ell-1),d\ell]\) and letting \(\ell\to\infty\),

\[\min_{Q\in\mathcal{Q}_{1\text{-}\text{gun}}}\mathcal{L}(Q\circ\mathsf{enc}( \cdot))\leq\mathcal{L}(Q_{\text{uni}}\circ\mathsf{enc}(\cdot))\leq\left(1- \frac{2d_{0}}{d}\right)\left(2H_{\infty}+O\left(\frac{1}{\log(d)}\right)\right) +\frac{2d_{0}}{d}\log(4|\mathcal{A}|).\]

## Appendix C Additional Theoretical Results II: Learning the likelihood model

The guarantees we prove in Theorems 3.1, 3.6 and B.2 on various tokenizers assume that the downstream model is trained optimally. In practice, these models are trained from a finite dataset and the sample complexity of learning this likelihood model scales with the number of tokens in the dictionary. In this section, we step away from the transformer architecture and focus on analyzing the performance of a simple estimator for the unigram model based on Laplace smoothing. We leave the problem of analyzing the finite-sample statistical error of simple transformer models trained with gradient descent as an interesting open direction for future research.

The result of Theorem 3.1 establishes that under appropriate assumptions on the Markov source, there exists a tokenizer \(\mathcal{T}\) and a unigram model over tokens \(Q^{\star}\in\mathcal{Q}_{1\text{-}\text{gram}}\) such that,

\[\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[\log(1/Q^{\star}( \mathsf{enc}(\boldsymbol{s}))\right]\right.\] \[\left.\qquad\qquad\qquad\leq(1+\varepsilon)\cdot\lim_{m\to\infty} \frac{1}{m}\mathbb{E}\left[\log(1/P(\boldsymbol{s}))\right]\right.\]

Or in other words,

\[\lim_{m\to\infty}\frac{1}{m}\mathsf{KL}(P,Q^{\star}(\mathsf{enc}(\cdot)))\leq \varepsilon\cdot\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[\log(1/P( \boldsymbol{s}))\right].\]

This implies that with the appropriate tokenization, the measure associated to the string by the best unigram model over tokens is close to that induced by the true Markov distribution over characters in KL divergence. In this section, we establish finite-sample guarantees on learning \(Q^{\star}\) specifically for the LZW tokenizer. The approach we consider for distribution learning is a smoothed Laplace estimator described in more detail in Algorithm 2.

For any constant \(\theta\in(0,1)\), define \(\mathcal{E}_{\theta}\) as the event that every maximal token \(\boldsymbol{t}\) (Definition A.5) in the LZW dictionary satisfies \(1/d^{1-\theta}\geq\max_{a}P(\boldsymbol{t}|a)\geq\delta/d^{1+\theta}\). By Lemmas A.10 and A.11 when the LZW tokenizer is trained on a dataset of size \(\widetilde{\Omega}_{\delta}(d)\) drawn from a stochastic source satisfying Assumption 3.2, \(\mathcal{E}_{\theta}\) occurs with probability \(\geq 1-d^{-\Omega_{\theta,\delta}(\log(d))}\).

**Theorem C.1**.: _Consider any constant \(\theta\in(0,1)\), failure probability \(\eta\in(0,1)\) and approximation error \(\xi\in(0,1)\). Assume that the learnt LZW tokenizer \(\mathcal{T}_{\text{LZW}}\) satisfies the event \(\mathcal{E}_{\theta}\), which occurs with probability \(\geq 1-d^{-\Omega_{\theta,\delta}(\log(d))}\). Assume that \(d^{1-3\theta}\geq 1+\delta^{-2}\) and that the stochastic source satisfies Assumption 3.2. For an absolute constant \(C>0\), assume that the size of the training dataset is at least \(n_{\text{lm}}^{\star}(\xi)\), where,_

\[n_{\text{lm}}^{\star}\triangleq\frac{Cd^{1+\theta}\log^{3}(d/\eta\delta) \log\log(d/\eta)}{\delta\xi^{2}}\]

_Then, Algorithm 2 learns a unigram model \(\widehat{Q}\) such that,_

\[\mathcal{L}(\widehat{Q}\circ\mathsf{enc}_{\text{gr}}(\cdot))\leq(1+\xi)\min _{Q\in\mathcal{Q}_{1\text{-}\text{gw}}}\mathcal{L}(Q\circ\mathsf{enc}_{\text{gr }}(\cdot))\]

_with probability \(\geq 1-\eta\)._

In conjunction with Theorem 3.6, this gives end-to-end guarantees on the cross-entropy loss of the LZW tokenizer (with vocabulary size \(\leq d\)) with the Laplace estimator as the downstream unigram model. We instantiate this result choosing \(\theta=0.01\) in Theorem C.1.

**Corollary C.2**.: _Choose any \(\xi\in(0,1)\). Suppose the data source satisfies Assumption 3.2. On a dataset of size \(\widetilde{\Omega}_{\delta}(d)\) drawn from the source, train an LZW tokenizer \(\mathcal{T}_{\text{LZW}}\) with \(d\) tokens. Subsequently, using Algorithm 2, learn a unigram model \(\widehat{Q}\) using a dataset of size at least \(\widetilde{\Omega}(d^{1.01}/\delta\xi^{2})\) drawn from the source. Then, with probability \(\geq 1-d^{-\Omega_{\delta}(\log(d))}\),_

\[\mathcal{L}(\widehat{Q}\circ\mathsf{enc}_{\text{gr}}(\cdot))\leq\frac{1+ \xi}{1-\varepsilon}\min_{Q}\mathcal{L}(Q),\]

_where \(\varepsilon=\frac{\log(1/\delta)}{0.99\log(d)}\)._The analysis of Theorem C.1 relies on showing that the distribution over tokens induced when a string sampled from the data source is encoded into tokens by the greedy encoder and the LZW dictionary is a Markov process. In general, given a set of previously sampled tokens \(\bm{t}_{1},\cdots,\bm{t}_{i}\), the next token \(\bm{t}_{i+1}\) is sampled from the distribution \(P(\bm{t}_{i+1}|\bm{t}_{i};\forall j\in[i],\ \bm{t}_{i-j+1}\cdots\bm{t}_{i}\bm{t}_{i+1} \not\in\mathsf{Dict})\). The conditioning is to simply guarantee that the previous tokens which were sampled were indeed maximal, since if \(\bm{t}_{i}\bm{t}_{i+1}\in\mathsf{Dict}\), then the previous token returned would in fact have been this longer token and not \(\bm{t}_{i}\) and likewise for \(\bm{t}_{i-1}\bm{t}_{i}\bm{t}_{i+1}\) and so on). While in general, this process is complicated and depends on all the previous tokens sampled, for the LZW dictionary, we show that the conditioning \(\{\forall j\in[i],\ \bm{t}_{i-j+1}\cdots\bm{t}_{i}\bm{t}_{i+1}\not\in \mathsf{Dict}\}\) can be removed, thereby resulting in a simple Markov process over tokens.

Furthermore, we establish that this Markov process has a relatively large spectral gap. The optimal unigram model ends up being the stationary distribution over tokens induced by greedy encoder. Given the large spectral gap of the Markov process over tokens, estimating the stationary distribution of this process in KL divergence ends up being closely related to estimating a distribution from i.i.d. samples in the same metric. For this problem, the de-facto choice of estimator is the Laplace estimator, and several existing results provide finite-sample bounds on the KL divergence (Braess and Sauer, 2004; Han et al., 2021; Mourtada and Gaiffas, 2022). The Laplace estimator (Line 6 of Algorithm 2) is simply a smoothed empirical estimate to account for the degeneracy of the KL divergence in its second argument as any coordinate approaches \(0\). The non-i.i.d.ness of the Markov process is circumvented by using concentration inequalities which are a function of the spectral gap (Naor et al., 2020).

``` Input: A training dataset of size \(n_{\text{lm}}\), likelihood model class \(\mathcal{Q}\), likelihood model training algorithm TrainLM Output: Likelihood model \(Q\in\mathcal{Q}\).
1: Tokenize the training dataset into a sequence of tokens \(\mathcal{T}=(\bm{t}_{1},\cdots,\bm{t}_{i})\).
2: Train a likelihood model \(Q\) on the tokenized dataset \(\mathcal{T}\) using the TrainLM\((\mathcal{T},\mathcal{Q})\) subroutine. // In the case of \(\mathcal{Q}=\mathcal{Q}_{1\text{-gram}}\) use the Laplace estimator def TrainLM\((\mathcal{T},\mathcal{Q}_{1\text{-gram}})\):
3: Truncate the dataset to the first \(n^{\prime}=\lfloor n_{\text{lm}}/\ell_{\max}\rfloor\) tokens where \(\ell_{\max}=4\log(d|\mathcal{A}|)/\delta\). Let the truncated dataset be \(\mathcal{T}_{\text{trunc}}\)
4: Construct the unigram model \(\widehat{Q}\) with \(\widehat{Q}_{\#}=\text{Unif}([m])\) and \(\widehat{Q}_{\text{tok}}(\bm{t})=\frac{n_{\bm{t}}+1}{n_{\bm{t}}+|\mathsf{Dict}|}\). // \(n_{\bm{t}}\) is the number of times \(\bm{t}\) appears in \(\mathcal{T}_{\text{trunc}}\). // Test sequences are assumed to be of length \(m\). ```

**Algorithm 2** Training likelihood model on tokens

### Proof of Theorem C.1

Since \(\mathcal{T}_{\text{LZW}}\) uses the greedy encoder, the cross-entropy loss of the unigram model learnt by Algorithm 2 is,

\[\mathcal{L}(\widehat{Q}\circ\mathsf{enc}_{\text{gre}}(\cdot))- \min_{Q\in\mathcal{Q}_{1\text{-gram}}}\mathcal{L}(Q\circ\mathsf{enc}_{\text{gre }}(\cdot))\] \[=\max_{Q\in\mathcal{Q}_{1\text{-gram}}}\lim_{m\to\infty}\frac{1}{m} \mathbb{E}[\log(Q(\mathsf{enc}_{\text{gre}}(\bm{s}))/\widehat{Q}(\mathsf{enc }_{\text{gre}}(\bm{s})))]\] \[\overset{(i)}{=}\max_{Q\in\mathcal{Q}_{1\text{-gram}}}\lim_{m\to \infty}\frac{1}{m}\mathbb{E}\left[\left|\mathsf{enc}_{\text{gre}}(\bm{s}) \right|\sum_{\bm{t}\in\mathsf{Dict}}\frac{n_{\bm{t}}}{|\mathsf{enc}_{\text{gre }}(\bm{s})|}\log(Q_{\text{tok}}(\bm{t})/\widehat{Q}_{\text{tok}}(\bm{t})) \right]+\frac{\log(m)}{m}\] \[\overset{(ii)}{\leq}\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[ \left|\mathsf{enc}_{\text{gre}}(\bm{s})\right|\sum_{\bm{t}\in\mathsf{Dict}} \frac{n_{\bm{t}}}{|\mathsf{enc}_{\text{gre}}(\bm{s})|}\log\left(\frac{n_{\bm{t} }/|\mathsf{enc}_{\text{gre}}(\bm{s})|}{\widehat{Q}_{\text{tok}}(\bm{t})} \right)\right]+\frac{\log(m)}{m}\]

where in \((i)\) we use the fact that \(\widehat{Q}_{\#}=\text{Unif}([m])\) and in \((ii)\) we take the \(\max\{\cdot\}\) inside the limit and the expectation (Fatou's lemma and Jensen's inequality) and plug in the maximizer of the negative cross-entropy, \(Q_{\text{tok}}(\bm{t})=\frac{n_{\bm{t}}}{|\mathsf{enc}_{\text{gre}}(\bm{s})|}\). Note that \(\lim_{m\to\infty}\frac{n_{\bm{t}}}{|\mathsf{enc}_{\text{gre}}(\bm{s})|}\overset{ \text{as}}{=}Q_{\text{MLE}}(\bm{t})\) by Lemma A.4.

Moreover, since \(|\mathsf{enc}(\bm{s})|/m\leq 1\) and \(\widehat{Q}_{\text{tok}}(\bm{t})>0\) surely, by the Dominated Convergence Theorem,

\[\mathcal{L}(\widehat{Q}\circ\mathsf{enc}_{\text{gre}}(\cdot))-\min_{Q\in \mathcal{Q}_{1\text{-gen}}}\mathcal{L}(Q\circ\mathsf{enc}_{\text{gre}}(\cdot)) \leq\lim_{m\to\infty}\frac{1}{m}\mathbb{E}[|\mathsf{enc}_{\text{gre}}(\bm{s}) |]\cdot\mathsf{KL}(Q_{\text{MLE}},\widehat{Q}_{\text{tok}})\] (31)

By eq. (6), we have that for any tokenizer using the greedy encoder,

\[\lim_{m\to\infty}\frac{|\mathsf{enc}_{\text{gre}}(\bm{s})|\left(H(Q_{\text{ MLE}},P)-\log(1/\delta)\right)}{m}\stackrel{{\text{a.s.}}}{{\leq}}H_{\infty}.\]

Furthermore under the event \(\mathcal{E}_{\theta}\) which implies that the learnt dictionary is \((1-\theta)\)-heavy hitting (cf. Definition A.5), which implies that,

\[H(Q_{\text{MLE}},P)\geq(1-\theta)\log(d).\]

Therefore, by almost sure boundedness, we have that,

\[\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[|\mathsf{enc}_{\text{gre}}(\bm{s} )|\right]\leq\frac{H_{\infty}}{(1-\theta)\log(d)-\log(1/\delta)}\leq\frac{ \min_{Q\in\mathcal{Q}_{1\text{-gen}}}\mathcal{L}(Q\circ\mathsf{enc}(\cdot))}{ (1-\theta)\log(d)-\log(1/\delta)}\]

Putting this together with eq. (31), we have that,

\[\mathcal{L}(\widehat{Q}\circ\mathsf{enc}_{\text{gre}}(\cdot))\leq\left(1+ \mathsf{KL}(Q_{\text{MLE}},\widehat{Q}_{\text{tok}})\right)\min_{Q\in \mathcal{Q}_{1\text{-gen}}}\mathcal{L}(Q\circ\mathsf{enc}_{\text{gre}}(\cdot)),\] (32)

which uses the assumption \((1-\theta)\log(d)\geq 1+\log(1/\delta)\). In the remainder of the proof we upper bound the KL term.

By the law of large numbers established in eq. (34) and the fact that \(\frac{n_{\bm{t}}}{\sum_{\bm{t}^{\prime}}n_{\bm{t}^{\prime}}}\in[0,1]\), we have that,

\[Q_{\text{MLE}}(\bm{t})=\lim_{m\to\infty}\mathbb{E}\left[\frac{n_{\bm{t}}}{ \sum_{\bm{t}^{\prime}}n_{\bm{t}^{\prime}}}\right]=\lim_{m\to\infty}\frac{ \mathbb{E}\left[n_{\bm{t}}\right]}{\mathbb{E}\left[\sum_{\bm{t}^{\prime}}n_{ \bm{t}^{\prime}}\right]}=\pi(\bm{t}),\]

where \(\pi(\bm{t})\) denote the stationary distribution over tokens induced by the greedy encoding process, which exists for the LZW tokenizer. This distribution is in fact an ergodic Markov process, as we discuss next.

By Lemmas A.10 and A.11, for any constant \(\theta\in(0,1)\), with probability \(\geq 1-d^{-\Omega_{\theta,\delta}(\log(d))}\), every maximal token in the the LZW dictionary satisfies \(1/d^{1-\theta}\geq\max_{a}P(\bm{t}|a)\geq\delta/d^{1+\theta}\). Let \(S_{\text{gre}}\) denote the set of tokens which have a non-zero probability (over a string drawn from the Markov source) of being chosen by the greedy encoder while encoding the string. More importantly, note that for any sequence of tokens \(\bm{t}_{1},\cdots,\bm{t}_{i}\), the next token is necessarily in \(S_{\text{gre}}\) and can be any token in this set. The reason for this is that for any \(\bm{t}_{i},\bm{t}\in S_{\text{gre}}\), the concatenation \(\bm{t}_{i}\bm{t}\not\in S_{\text{gre}}\) since \(\max_{a\in\mathcal{A}}P(\bm{t}_{i}\bm{t}|a)\leq 1/\delta d^{2(1-\theta)}\), which is smaller than the \(\max_{a\in\mathcal{A}}P(\bm{t}^{\prime}|a)\geq\delta/d^{1+\theta}\) for any token \(\bm{t}^{\prime}\in S_{\text{gre}}\) as long as \(d^{1-3\theta}\geq 1/\delta^{2}\). This constraint implies that in the sampling procedure in Figure 7, it suffices to drop the conditioning on the event \(\bm{t}_{j}\bm{t}_{j+1}\cdots\bm{t}_{i}\bm{t}\not\in\mathsf{Dict}\) while sampling the next token \(\bm{t}\). This condition automatically implies that the sequence of tokens conditionally follows a Markov process with \(\Pr(\bm{t}_{i+1}=\bm{t}|\bm{t}_{1},\cdots,\bm{t}_{i})=P(\bm{t}|\mathsf{last}( \bm{t}_{i}))\). Since the probability of every transition is lower bounded, this means that the Markov chain is ergodic. Moreover, the pseudo-spectral gap (Naor et al., 2020), \(1-\lambda\) can be lower bounded by the Dobrushin contraction coefficient, \(\kappa\),

\[1-\lambda\leq\kappa \triangleq\max_{(\bm{t},\bm{t}^{\prime})\in\mathsf{Dict}^{2}}\| \Pr(\cdot|\bm{t})-\Pr(\cdot|\bm{t}^{\prime})\|_{\text{TV}}\] \[=\max_{(\bm{t},\bm{t}^{\prime})\in\mathsf{Dict}^{2}}1-\sum_{\bm{t }^{\prime\prime}\in\mathsf{Dict}}\min\{\Pr(\bm{t}^{\prime\prime}|\bm{t}),\Pr( \bm{t}^{\prime\prime}|\bm{t}^{\prime})\}\] \[\leq 1-\delta d/d^{1+\theta}\] \[=1-\delta d^{-\theta}.\] (33)

Recall that the learner is given a training dataset of \(n_{\text{lm}}\) characters to train the likelihood model. By Lemma A.8, with probability \(\geq 1-d^{-\Omega(\log(d/\delta)/\delta)}\), in the run of the LZW tokenization algorithm, every token in the dictionary has length at most \(\ell_{\max}=4\log(d|\mathcal{A}|)/\delta\). Therefore, suppose the learner always truncates the dataset to the first \(n^{\prime}=\lfloor n_{\text{ln}}/\ell_{\max}\rfloor\) tokens and runs the Laplace estimator on this truncated dataset. With this, we move onto upper bounding,

\[\text{KL}(Q_{\text{MLE}},\widehat{Q}_{\text{tok}})=\sum_{\bm{t}\in\text{Dict}} \pi(\bm{t})\log\left(\pi(\bm{t})/\widehat{Q}_{\text{tok}}(\bm{t})\right)\]

which necessitates lower bounding \(\widehat{Q}_{\text{tok}}(\bm{t})\) for every \(\bm{t}\). Recall that the learner's estimate \(\widehat{Q}(\bm{t})\) in Algorithm 2 is the Laplace estimator, \(\frac{n_{\bm{t}}+1}{\sum_{\bm{t}^{\prime}}n_{\bm{t}^{\prime}}+\text{Dict}}\), where \(\{n_{\bm{t}}:\bm{t}\in\text{Dict}\}\) is computed by truncating the dataset to the first \(n^{\prime}\) tokens. Firstly, by invoking Corollary 1.3 of Naor et al. (2020) for the function \(n_{\bm{t}}=\sum_{i=1}^{n^{\prime}}\mathbb{I}(\bm{t}_{i}=\bm{t})\),

\[\Pr\left(|n_{\bm{t}}-\mathbb{E}[n_{\bm{t}}]|\geq c\sqrt{\frac{\mathbb{E}[n_{ \bm{t}}]}{1-\lambda}\cdot\log(1/\eta)}\right)\leq\eta\] (34)

for a universal constant \(c>0\). In particular, this implies that with probability \(\geq 1-\eta\), simultaneously for all \(\bm{t}\),

\[|n_{\bm{t}}-\mathbb{E}[n_{\bm{t}}]|\leq\Delta_{\bm{t}} \triangleq\sqrt{\frac{d^{\theta}}{\delta}\mathbb{E}[n_{\bm{t}}] \cdot\log(|\text{Dict}|/\eta)}\text{, and, }\mathbb{E}[n_{\bm{t}}]-n_{\bm{t}}\geq \mathbb{E}[n_{\bm{t}}].\]

Under this event, for any \(\bm{t}\), the estimate is lower bounded by,

\[\widehat{Q}_{\text{tok}}(\bm{t})=\frac{n_{\bm{t}}+1}{n^{\prime}+| \text{Dict}|} \geq\frac{\mathbb{E}[n_{\bm{t}}]+1-\min\{\mathbb{E}[n_{\bm{t}}], \Delta_{\bm{t}}\}}{n^{\prime}+|\text{Dict}|}\] \[\geq\max\left\{\pi(\bm{t})-\frac{(\Delta_{\bm{t}}-1)\,n^{\prime}+ |\text{Dict}|\mathbb{E}[n_{\bm{t}}]}{(n^{\prime})^{2}+n^{\prime}|\text{Dict}|}, \;\frac{1}{n^{\prime}+|\text{Dict}|}\right\}\] \[\geq\max\left\{\pi(\bm{t})-\frac{\Delta_{\bm{t}}n^{\prime}+|\text {Dict}|\mathbb{E}[n_{\bm{t}}]}{(n^{\prime})^{2}},\;\frac{1}{n^{\prime}+|\text{ Dict}|}\right\}\]

Suppose the following condition is satisfied,

\[n^{\prime}=\frac{4rd^{\theta}|\text{Dict}|\log(|\text{Dict}|/\eta)}{\delta}\] (C1)

for some \(r\geq 4\). Under this condition, we have that \(n^{\prime}\geq 2\sqrt{r}\Delta\) and \(n^{\prime}\geq 4r|\text{Dict}|\).

**Case I.**\(\Delta_{\bm{t}}n^{\prime}\geq|\text{Dict}|\mathbb{E}[n_{\bm{t}}]\).

In this case, we have the upper bound,

\[\widehat{Q}_{\text{tok}}(\bm{t}) \geq\max\left\{\pi(\bm{t})-\frac{2\Delta_{\bm{t}}}{n^{\prime}},\; \frac{1}{n^{\prime}+|\text{Dict}|}\right\}\] \[=\max\left\{\pi(\bm{t})-2\frac{\sqrt{\frac{d^{\theta}}{\delta} \mathbb{E}[n_{\bm{t}}]\cdot\log(|\text{Dict}|/\eta)}}{n^{\prime}},\;\frac{1}{ n^{\prime}+|\text{Dict}|}\right\}\] \[\geq\max\left\{\pi(\bm{t})-\sqrt{\frac{\pi(\bm{t})}{r|\text{Dict}| }},\;\frac{1}{2n^{\prime}}\right\}.\]

where the last inequality uses eq. (C1).

Consider two sub-cases,

**Sub-case I.**\(\pi(\bm{t})\geq 2/r|\text{Dict}|\). Define this event \(\mathcal{C}_{\text{I}}\).

Here,

\[\pi(\bm{t})\log(\pi(\bm{t})/\widehat{Q}_{\text{tok}}(\bm{t}))\leq- \pi(\bm{t})\log\left(1-\sqrt{\frac{1}{\pi(\bm{t})r|\text{Dict}|}}\right)\leq \frac{3}{2}\sqrt{\frac{\pi(\bm{t})}{r|\text{Dict}|}}.\] (35)

[MISSING_PAGE_FAIL:39]

Additional Theoretical Results III: The generalization ability of tokenizers

The proofs of the upper bounds in the paper (Theorems 3.6 and B.2) relied on showing that the entropy \(H(Q_{\text{MLE}},P)\) is large, or in other words, the algorithm typically encodes new strings into long length (i.e. low probability under \(P\)) tokens. This statement about generalization to new strings is fundamentally different from having a tokenizer which compresses the training dataset well. In other words, consider the following modification: the measure \(Q_{\text{MLE}}\) is defined as the expected empirical distribution over tokens when a new string is encoded into tokens, and not on the source dataset used to construct the dictionary. Suppose the definition of \(Q_{\text{MLE}}\) is changed to the empirical distribution over tokens in the source dataset. Under this new definition of the MLE unigram model, the largeness of the \(H(Q_{\text{MLE}},P)\) metric, in a sense, captures compressing the source dataset well. However, we show that in general, this does not result in good tokenizers that minimize the population cross-entropy loss, suffering from \(\min_{Q\in\mathcal{Q}_{1\text{-pm}}}\mathcal{L}(Q\circ\textsf{enc}(\cdot)) \approx H(\pi)\gg H_{\infty}\).

**Theorem D.1**.: _Consider the stochastic source in example A.1 having entropy rate \(H_{\infty}=\delta\log(1/\delta)+(1-\delta)\log(1/(1-\delta))\). Consider a training dataset of size \(n\). For a dictionary Dict and \(\bm{t}\in\text{Dict}\), define \(\widehat{Q}_{\text{MLE}}(\bm{t})=\frac{n_{\bm{t}}(\bm{s}_{\text{src}})}{| \textsf{enc}(\bm{s}_{\text{src}})|}\) as the empirical distribution over tokens induced by the greedy encoder when encoding the training dataset, \(\bm{s}_{\text{src}}\). There exists a dictionary Dict such that with probability \(\geq 1-e^{-\Omega(\sqrt{n})}\) over the training dataset,_

\[H(\widehat{Q}_{\text{MLE}},P_{\gamma})\geq nH_{\infty}(1-O(n^{-1/4}))\]

_is large. However, for this dictionary, for any encoding algorithm (including the greedy encoder), the resulting tokenizer \(\mathcal{T}=(\text{Dict},\emptyset,\textsf{enc}(\cdot),\textsf{dec}(\cdot))\) satisfies,_

\[\min_{Q\in\mathcal{Q}_{1\text{-pm}}}\mathcal{L}(Q\circ\textsf{enc}(\cdot)) \geq(1-\varepsilon)H(\pi)\]

_where \(\varepsilon=2ne^{-nH_{\infty}(1-O(n^{-1/4}))}\)._

Proof.: Suppose the entire training dataset was compressed into a single token, \(\bm{t}_{\text{src}}\). The dictionary is \(\mathcal{A}\cup\bm{t}_{\text{src}}\). In the following argument, we show that the number of occurrences, \(n_{\bm{t}_{\text{src}}}\), of the entire training dataset \(\bm{t}_{\text{src}}\) in a new string of length \(m\) generated from the stochastic source, \(\bm{s}\), converges to its expectation as \(m\to\infty\). Let \(\pi_{n}^{(i)}\) denote the stationary distribution of the Markov process induced by the stochastic source over length-\(n\) strings with a shift of \(i\) from the starting position, and let \(n_{\bm{t}}^{(i)}\) denote the number of times \(\bm{t}\) appears in the training dataset starting at the position \(i+rn\) for some \(r>0\). Then,

\[\lim_{m\to\infty}\frac{n_{\bm{t}_{\text{src}}}}{m}=\frac{1}{n}\lim_{m\to \infty}\sum_{i=0}^{n-1}\frac{n_{\bm{t}_{\text{src}}}^{(i)}}{m/n}\overset{ \text{as.}}{=}\frac{1}{n}\sum_{i=0}^{n-1}\mathbb{E}_{\bm{t}^{\prime}\sim\pi_{n }^{(i)}}[P(\bm{t}_{\text{src}}|\bm{t}^{\prime})]\leq\max_{a\in\mathcal{A}}P( \bm{t}_{\text{src}}|a).\] (38)

The second equation follows by considering the Markov process induced over length \(n\) strings and applying the Krylov-Bogolyubov argument for ergodic and homogeneous Markov processes.

In Lemma D.2, we show that with probability \(\geq 1-e^{-\Omega(\sqrt{n})}\), the token \(\bm{t}_{\text{src}}\) constructed from the source dataset satisfies, \(\max_{a\in\mathcal{A}}P(\bm{t}|a)\leq e^{-nH_{\infty}(1-O(n^{-1/4}))}\). In other words, the source string has exponentially small probability. Combining this with eq. (38), with probability \(\geq 1-e^{-\Omega(\sqrt{n})}\) over the source dataset, the number of occurrences of the substring \(\bm{t}_{\text{src}}\) in a new string \(\bm{s}\) is upper bounded by,

\[\lim_{m\to\infty}\frac{n_{\bm{t}_{\text{src}}}}{m}\overset{\text{as.}}{\leq}e^ {-nH_{\infty}(1-O(n^{-1/4}))}\triangleq\varepsilon/2n.\]

By the Krylov-Bogolyubov argument, for each \(a\in\mathcal{A}=\{0,1\}\), \(\lim_{m\to\infty}\frac{n_{a}}{m}\overset{\text{as.}}{=}\pi(a)\). More importantly, the number of times \(a\) is made as a token is upper bounded by \(n_{a}\) and lower bounded by \(n_{a}-nn_{\bm{t}_{\text{src}}}\). Therefore,

\[(1-\varepsilon)\pi(a)=\pi(a)-\frac{\varepsilon}{2}\overset{\text{as.}}{\leq} \lim_{m\to\infty}\frac{n_{a}}{m}\overset{\text{as.}}{\leq}\pi(a)=\frac{1}{2}\] (39)Finally, putting everything together,

\[\min_{Q\in\mathcal{Q}_{1:\text{pmn}}}\lim_{m\to\infty}\frac{1}{m} \mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot)) =\min_{Q\in\mathcal{Q}_{1:\text{pmn}}}\lim_{m\to\infty}-\frac{1}{m} \mathbb{E}\left[\log(Q_{\#}(|\mathsf{enc}(\bm{s})|)+\sum\nolimits_{\bm{t}\in \mathsf{Dict}}n_{\bm{t}}\log Q_{\text{tok}}(\bm{t})\right]\] \[\geq\min_{Q\in\mathcal{Q}_{1:\text{pmn}}}\lim_{m\to\infty}-\frac{ 1}{m}\mathbb{E}\left[\sum\nolimits_{a\in\mathcal{A}}n_{a}\log Q_{\text{tok}}(a)\right]\] \[\overset{(i)}{\geq}\min_{Q\in\mathcal{Q}_{1:\text{pmn}}}-(1- \varepsilon)\sum\nolimits_{a\in\mathcal{A}}\pi(a)\log Q_{\text{tok}}(a)\] \[\geq(1-\varepsilon)H(\pi).\]

where \((i)\) follows from the lower bound on \(n_{a}/m\) in eq.39. This completes the proof. 

**Lemma D.2**.: _With probability \(\geq 1-e^{-\Omega(\sqrt{n})}\) over the source dataset,_

\[\max_{a\in\mathcal{A}}P(\bm{t}_{\text{src}}|a)\leq e^{-nH(\delta)(1-O(n^{-1/4 }))}.\]

Proof.: Let \(X\) denote the number of \(i\in[n-1]\) such that \(\bm{s}_{i}\neq\bm{s}_{i+1}\) in \(\bm{s}\), the stochastic source. Since the transition of the Markov process only depends on whether the next character is the same as the previous character, we can write down,

\[\max_{a\in\mathcal{A}}\log P(\bm{t}_{\text{src}}|a)=-(X+1)\log(\delta)-(n-1-X )\log(1-\delta).\]

Note that \(X\) is a sum of \(n-1\) i.i.d. random variables, since \(\mathsf{l}(\bm{s}_{i}\neq\bm{s}_{i+1})\sim\mathsf{Ber}(\delta)\) does not depend on whether \(\bm{s}_{i}=0\) or \(=1\). In particular, by Hoeffding's inequality, we have that with probability \(\geq 1-e^{-\Omega(\sqrt{n})}\),

\[\left|\frac{1}{n}\max_{a\in\mathcal{A}}\log P(\bm{t}_{\text{src}}|a)-H(\delta )\right|\leq O\left(n^{-1/4}\right),\]

which uses the fact that \(\mathbb{E}[X]=\delta(n-1)\) and \(H_{\infty}=\delta\log(1/\delta)+(1-\delta)\log(1/(1-\delta))\). Taking an exponential on both sides proves the statement of the lemma. 

Appendix E Additional Theoretical Results IV: Interaction between the dictionary and encoding algorithm

In this section, we show another kind of barrier to generalization, which brings out the relationship between the encoding algorithm and the dictionary. We show that there exist dictionaries which generalize under the minimal encoder, i.e. the encoding algorithm which encodes a string into the shortest number of possible tokens, but at the same time, completely fail to generalize under the greedy encoder. This means that in the process of constructing good tokenizers, it does not suffice to think about the dictionary in isolation. Its interaction with the encoding algorithm is pertinent.

**Definition E.1** (minimal encoder).: The minimal encoder parses a new string into the fewest possible number of tokens from the dictionary as possible. Ties are broken arbitrarily.

**Theorem E.2**.: _There exists a stochastic source parameterized by \(\delta\in(0,0.5)\) and a dictionary \(\mathsf{Dict}\) such that under the minimal encoder/decoder pair, the resulting tokenizer, \(\mathcal{T}=\langle\mathsf{Dict},\emptyset,\mathsf{enc}_{\text{min}}(\cdot), \mathsf{dec}_{\text{min}}(\cdot)\rangle\) generalizes near-optimally,_

\[\min_{Q\in\mathcal{Q}_{1:\text{pmn}}}\mathcal{L}(Q\circ\mathsf{enc}_{\text{ min}}(\cdot))\leq 1.273H_{\infty}.\] (40)

_Here the entropy rate of the source, \(H_{\infty}\), is \(\delta\log(\sqrt{2}/\delta)+(1-\delta)\log(1/(1-\delta))\). However, the same dictionary \(\mathsf{Dict}\) under the greedy encoder/decoder pair, i.e. \(\mathcal{T}^{\prime}=(\mathsf{Dict},\emptyset,\mathsf{enc}_{\text{gre}}( \cdot),\mathsf{dec}_{\text{gre}}(\cdot))\), generalizes poorly, suffering from cross-entropy scaling as,_

\[\min_{Q\in\mathcal{Q}_{1:\text{pmn}}}\mathcal{L}(Q\circ\mathsf{enc}_{\text{gre} }(\cdot))\geq\frac{1-o_{\delta}(1)}{3}H(\pi).\] (41)

_where the entropy of the stationary distribution of the source is \(H(\pi)=\frac{1}{2}\log(8)\) and the \(1-o_{\delta}(1)\) term is \((1-\delta)^{2}(1+\delta)^{-1}\)._cross-entropy loss of the tokenizer is a constant multiple away from that achieved by the character-level tokenizer. The separation between eq. (40), and eq. (41) only manifests as \(\delta\) becomes smaller and smaller.

In this section, we prove that generalization of a dictionary is a function of the underlying tokenization algorithm used. In particular, the greedy encoder is not universal, and there exists dictionaries under the minimum-length encoder/decoder which achieve small cross-entropy loss, which do not generalize under the greedy encoder/decoder.

We split the proof of Theorem E.2 into two parts. We first define the stochastic source and dictionary we consider. Then we show that under the minimum-length encoder, the asymptotic cross-entropy loss is upper bounded by \(H_{\infty}\) up to a constant. Finally, we show that under the greedy-encoder, the same dictionary suffers from high cross-entropy loss, which is a constant factor away from that of the character encoder.

### Stochastic source and dictionary.

Consider an extension of the switching Markov source in example A.1 to \(\mathcal{A}=\{0,1,2\}\). The Markov chain is described in Figure 10. The transition of the Markov chain is \(P(0|0)=P(1|1)=P(2|2)=1-\delta\), and \(P(1|0)=P(2|1)=\delta\) and \(P(2|1)=P(0|1)=\delta/2\), with the remaining transitions being \(0\)-probability. For a parameter \(\ell>0\) to be instantiated later, define \(S_{1}\) (resp. \(S_{0}\), \(S_{2}\)) as the set of all-\(1\) (resp. all-\(0\), all-\(2\)) strings of length \(\leq\ell-1\), including the empty string. Consider a dictionary composed of the following set of tokens, \(\{1\bm{s}:\bm{s}\in S_{0}\cup S_{1}\cup S_{2}\}\). Therefore, the tokens follow the template \(10\cdots 0\), \(11\cdots 1\) or \(12\cdots 2\) and are of length at most \(\ell\). \(\ell\) is chosen to be \(1+2\log(1/\delta)/\delta\).

Although we use the minimal encoder in the statement of Theorem E.2, for the purpose of analysis, define the following encoding algorithm: if the new string is prefixed by \(10\cdots 0\) or \(12\cdots 2\), select the largest prefix which exists in dictionary and assign it as a token. If the new string starts with a sequence \(11\cdots 1\) of length \(x\), consider the first \(\max\{\ell,x-1\}\) length prefix and assign it as a token. Finally, if the string starts with \(0\) or \(2\), assign that character as token. Once the first token has been assigned, remove it and repeat.

### Minimal encoder achieves the optimal cross-entropy loss up to a constant.

First consider a simplification of the overall cross-entropy loss,

\[\min_{Q\in\mathfrak{Q}_{1:\text{gen}}}\lim_{m\to\infty}\frac{1} {m}\mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot))\] \[\qquad=\min_{Q\in\mathfrak{Q}_{1:\text{gen}}}\lim_{m\to\infty}- \frac{1}{m}\mathbb{E}\left[\log Q_{\#}(|\mathsf{enc}_{\text{min}}(\bm{s})|)+ \sum\nolimits_{\bm{t}\in\mathsf{Dict}}n_{\bm{t}}\log Q_{\text{lek}}(\bm{t})\right]\] (42) \[\qquad\leq\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[\log(m)+| \mathsf{enc}_{\text{min}}(\bm{s})|\log|\mathsf{Dict}|\right],\] (43)

where in the last inequality we upper bound by choosing \(Q_{\#}=\operatorname{Unif}([m])\) and \(Q_{\text{lek}}(\bm{t})=1/|\mathsf{Dict}|\). Note that \(|\mathsf{Dict}|\leq 2\ell+1\) and letting \(\lim_{m\to\infty}\log(m)/m=0\),

\[\min_{Q\in\mathfrak{Q}_{1:\text{gen}}}\lim_{m\to\infty}\frac{1} {m}\mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot)) \leq\lim_{m\to\infty}\frac{1}{m}\mathbb{E}[|\mathsf{enc}_{\text{ min}}(\bm{s})|\log(2\ell+1)]\] \[\leq\lim_{m\to\infty}\frac{1}{m}\mathbb{E}\left[|\mathsf{enc}(\bm {s})|\log(2\ell+1)\right],\] (44)

Figure 10: order-\(1\) Markov source used in the proof of Theorem E.2

where in \((i)\), we replace \(|\mathsf{enc}_{\mathsf{min}}(\bm{s})|\) by \(|\mathsf{enc}(\bm{s})|\), which is the encoder we define in Appendix E.1. By definition of the minimal encoder, \(|\mathsf{enc}_{\mathsf{min}}(\bm{s})|\leq|\mathsf{enc}(\bm{s})|\) surely. Recall that the encoder \(\mathsf{enc}(\cdot)\) processes strings in a sequential (left-to-right) manner. In particular, by a similar argument as Lemma A.4, we can show that under this encoder, the limit \(n_{t}/\sum_{t^{\prime}}n_{t^{\prime}}\) almost surely converges to its expectation. More importantly, since, \(\sum_{t\in\mathsf{Dict}}|\bm{t}|n_{t}=m\), we have that,

\[\lim_{m\to\infty}\frac{|\mathsf{enc}(\bm{s})|}{m}\stackrel{{ \text{\tiny a.s.}}}{{=}}\frac{1}{\mathbb{E}_{t\sim Q_{\text{\tiny MLE}}} \|\bm{t}\|}.\]

converges to some limit almost surely. Therefore, from eq. (44),

\[\min_{Q\in\mathcal{Q}_{1:\text{\tiny gain}}}\lim_{m\to\infty}\frac{1}{m} \mathcal{L}_{m}(Q\circ\mathsf{enc}(\cdot))\leq\text{ess}\limsup_{m\to\infty} \frac{|\mathsf{enc}(\bm{s})|}{m}\log(2\ell+1).\] (45)

where the essential lim-sup captures the almost sure limit \(1/\mathbb{E}_{t\sim Q_{\text{\tiny MLE}}}\|\bm{t}\|\). The almost sure convergence of \(|\mathsf{enc}(s)|/m\) also implies that we can let the limit \(m\) go to \(\infty\) in any manner, and the limit will remain the same. In particular, consider a process parameterized by \(i^{\star}\) for generating the source string, such that surely \(m\geq i^{\star}\), where the total number of characters, \(m\), is a random variable. As \(i^{\star}\to\infty\), we will also have \(m\to\infty\) surely, and so the limit of \(|\mathsf{enc}(\bm{s})|/m\) under this modified stochastic process should also converge to the same limit.

Rather than sampling a string of a fixed length \(m\) from the source, consider the following sampling model: for \(i^{\star}\to\infty\), sample \(i^{\star}\) geometric random variables \(X_{1},\cdots,X_{i^{\star}}\stackrel{{\text{i.i.d.}}}{{\sim}} \mathsf{Geo}(\delta)\) and construct the source string as the concatenation of \(i^{\star}\) strings alternating between successive \(1\)'s and successive \(0\)'s or \(2\)'s (with the choice between the two made uniformly at random), with the \(i^{th}\) string of length \(X_{i}+1\). The overall number of characters sampled, \(m\), is surely at least \(i^{\star}\).

Under this stochastic process, the size of the encoding of the string is upper bounded by,

\[|\mathsf{enc}(\bm{s})|\leq|X_{1}+1|+\sum_{i=2}^{i^{\star}}\left(1+(X_{i}+1- \ell)_{+}\right)\]

This bound follows from the fact that in any substring \(\bm{s}^{\prime}\) of successive \(1\)'s followed by a substring \(\bm{s}^{\prime\prime}\) of successive \(0\)'s or \(2\)'s, the encoder tokenizes the first \(\max\{\ell,|\bm{s}^{\prime}|-1\}\) length prefix of \(\bm{s}^{\prime}\) as a token, and the remaining characters in \(\bm{s}^{\prime}\) into individual tokens except the last. Then, the last character of \(\bm{s}^{\prime}\) and the first \(\max\{\ell-1,|\bm{s}^{\prime\prime}|\}\) characters of \(\bm{s}^{\prime\prime}\) are assigned as token. The remainder of \(\bm{s}^{\prime\prime}\) is assigned as individual tokens. Each of \(\bm{s}^{\prime}\) or \(\bm{s}^{\prime\prime}\) of length \(x\), is allocated into at most \(1+(x+1-\ell)_{+}\) tokens.

For any \(i\), \(\Pr(X_{i}\geq u)=(1-\delta)^{u}\), and therefore, summing over \(u\geq\ell\), we get that \(\mathbb{E}[(X_{i}+1-\ell)_{+}]=\frac{(1-\delta)^{\ell-1}}{\delta}\). With \(\ell=1+2\log(1/\delta)/\delta\), this expectation is upper bounded by \(\delta\). Therefore,

\[\lim_{i^{\star}\to\infty}\frac{\mathbb{E}[|\mathsf{enc}(\bm{s})|]}{i^{\star}} \leq\lim_{i^{\star}\to\infty}\frac{1}{i^{\star}}\mathbb{E}\left[|X_{1}|+\sum _{i=2}^{i^{\star}}\left(1+(X_{i}+1-\ell)_{+}\right)\right]\leq 1+\delta\]

More importantly, by the strong law of large numbers for a sum of independent random variables, \((|X_{1}+1|+\sum_{i=2}^{i^{\star}}(1+(X_{i}+1-\ell)_{+}))/i^{\star}\), and therefore \(|\mathsf{enc}(\bm{s})|/i^{\star}\) is asymptotically almost surely upper bounded as,

\[\lim_{i^{\star}\to\infty}\frac{|\mathsf{enc}(\bm{s})|}{i^{\star}}\stackrel{{ \text{\tiny a.s.}}}{{\leq}}1+\delta,\] (46)

On the other hand, the number of characters generated, \(m\), equals \(\sum_{i=1}^{i^{\star}}(X_{i}+1)\), and satisfies, \(\lim_{i^{\star}\to\infty}\mathbb{E}[m]/i^{\star}=1+\delta^{-1}\). By another application of the strong law of large numbers for a sum of independent random variables,

\[\lim_{i^{\star}\to\infty}\frac{m}{i^{\star}}\stackrel{{\text{ \tiny a.s.}}}{{=}}1+\delta^{-1}.\] (47)

By combining eqs. (46) and (47), we have that,

\[\lim_{i^{\star}\to\infty}\frac{|\mathsf{enc}(\bm{s})|}{m}\stackrel{{ \text{\tiny a.s.}}}{{\leq}}\frac{1+\delta}{1+\delta^{-1}}=\frac{1}{\delta}.\]Finally, combining with eq. (45) and the ensuing discussion, we may upper bound the limiting cross-entropy loss by,

\[\min_{Q\in\mathcal{Q}_{1:\text{\tiny pmn}}}\lim_{m\to\infty}\frac{1}{m}\mathcal{L} _{m}(Q\circ\mathsf{enc}(\cdot))\leq\delta\log(2\ell+1)=\delta\log(3+4\log(1/ \delta)/\delta).\]

Note for this Markovian source, it is a short calculation to see that,

\[H_{\infty}=\mathbb{E}_{x\sim\pi}[H(P(\cdot|x))]=\delta\log(\sqrt{2}/\delta)+(1 -\delta)\log(1/(1-\delta))\]

Note that for any \(\delta\leq 1/2\), numerical evaluation gives the inequality,

\[1\leq\frac{\delta\log(3+4\log(1/\delta)/\delta)}{H_{\infty}}\leq 1.273\]

with the approximation factor improving as \(\delta\) becomes smaller. Therefore, this tokenizer achieves a normalized cross-entropy loss which asymptotically scales as a constant multiple of the entropy rate of the source.

### Greedy-encoder achieves poor cross-entropy loss

Note that the greedy encoder picks the largest prefix of the string which is a token, assigns and removes it, and iterates on the rest of the string. The greedy encoder's behavior is easy to analyze - every string of consecutive \(1\)'s in the new string is broken into chunks of length \(\ell\) (save potentially the last chunk) and each chunk is assigned as a token in \(\{1\bm{s}:\bm{s}\in S_{\bm{1}}\}\subset\mathsf{Dict}\). If the length of this substring of successive \(1\)'s is not \(1,\ell+1,2\ell+1,\cdots\), or in general, \(\equiv 1\mod\ell\), every character in the next sequence, composed of \(0\)'s or \(2\)'s is tokenized into individual characters.

Similar to eq. (42) to eq. (43), consider a simplification of the overall cross-entropy loss,

\[\min_{Q\in\mathcal{Q}_{1:\text{\tiny pmn}}}\lim_{m\to\infty}\frac {1}{m}\mathcal{L}_{m}(Q\circ\mathsf{enc}_{\text{gre}}(\cdot))\] \[=\min_{Q\in\mathcal{Q}_{1:\text{\tiny pmn}}}\lim_{m\to\infty}- \frac{1}{m}\mathbb{E}\left[\log Q_{\#}(|\mathsf{enc}_{\text{gre}}(\bm{s})|)+| \mathsf{enc}_{\text{gre}}(\bm{s})|\sum\nolimits_{\bm{t}\in\mathsf{Dict}} \frac{n_{\bm{t}}}{|\mathsf{enc}_{\text{gre}}(\bm{s})|}\log Q_{\text{tok}}(\bm{ t})\right]\] \[\geq\min_{Q\in\mathcal{Q}_{1:\text{\tiny pmn}}}\lim_{m\to\infty}- \frac{1}{m}\mathbb{E}\left[|\mathsf{enc}_{\text{gre}}(\bm{s})|\sum\nolimits_{Q _{\text{mk}}(\bm{t})>0}Q_{\text{MLE}}(\bm{t})\log Q_{\text{tok}}(\bm{t})\right],\]

where the last equation uses the fact that by Lemma A.4, for the greedy encoder, \(\lim_{m\to\infty}\frac{n_{\bm{t}}}{|\mathsf{enc}_{\text{gre}}(\bm{s})|} \stackrel{{\bm{s}\to\infty}}{{=}}Q_{\text{MLE}}(\bm{t})\). The minimizer of this objective subject to \(\sum_{\bm{t}\in\mathsf{Dict}:Q_{\text{MLE}}(\bm{t})>0}Q_{\text{tok}}(\bm{t})\leq 1\) is \(Q_{\text{tok}}(\bm{t})=Q_{\text{MLE}}(\bm{t})\) resulting in the inequality,

\[\min_{Q\in\mathcal{Q}_{1:\text{\tiny pmn}}}\lim_{m\to\infty}\frac{1}{m} \mathcal{L}_{m}(Q\circ\mathsf{enc}_{\text{gre}}(\cdot))\geq\lim_{m\to\infty} \frac{1}{m}\mathbb{E}\left[|\mathsf{enc}_{\text{gre}}(\bm{s})|H(Q_{\text{MLE}}) \right],\] (48)

where we use the convention \(0\log(1/0)\triangleq\lim_{P\to 0}P\log(1/P)=0\) and therefore we may sum over tokens such that \(Q_{\text{MLE}}(\bm{t})=0\) for free.

Considering the same geometric sampling model as in Appendix E.2, and Lemma A.4, we may study the almost sure limit \(Q_{\text{MLE}}(\bm{t})=\lim_{m\to\infty}n_{\bm{t}}/|\mathsf{enc}_{\text{gre}}( \bm{s})|\) by computing \(\lim_{i^{*}\to\infty}n_{\bm{t}}/|\mathsf{enc}_{\text{gre}}(\bm{s})|\) under the geometric sampling model since the almost sure limit exists. Recall that in the geometric sampling model, we generate the overall source string by concatenating \(i^{*}\) strings of length \(X_{1}+1,\cdots,X_{i^{*}}+1\) where \(X_{i}\sim\mathsf{Geo}(\delta)\), with the strings alternating between successive \(1\)'s and successive \(0\)'s or \(2\)'s (with the choice between the two made by the flip of a fair coin). For \(x\in\{0,1,2\}\), let \(\mathcal{E}_{i}(x)\) denote the event that \(X_{i}\) is a string composed only of all \(x\)'s. The length of the greedy encoding of \(\bm{s}\) is lower bounded by,

\[|\mathsf{enc}_{\text{gre}}(\bm{s})|\geq\sum_{i=1}^{i^{*}}X_{i}\cdot\mathbb{I}(X _{i-1}\not\equiv 1\mod\ell)\mathbb{I}(\mathcal{E}_{i}(0)\cup\mathcal{E}_{i}(2)).\] (49)Which captures for the fact that all 0's and \(2\)'s are encoded into singular tokens unless the previous string of \(1\)'s was of length \(\equiv 1\mod\ell\). By the law of large numbers of the RHS of eq. (49), the following a.a.s. lower bound is satisfied,

\[\lim_{i^{\star}\to\infty}\frac{|\mathsf{enc}_{\mathsf{gre}}(\bm{s})| }{i^{\star}}\stackrel{{\text{\scriptsize{as.}}}}{{\geq}}\frac{1} {2\delta}\left(1-\sum_{u=0}^{\infty}\delta(1-\delta)^{\ell u+1}\right)=\frac{1 }{2\delta}\left(1-\frac{\delta(1-\delta)}{1-(1-\delta)^{\ell}}\right)\geq\frac{ 1-\delta}{2\delta},\] (50)

where the last inequality uses the fact that \(\ell=1+2\log(1/\delta)/\delta\). Likewise, observe that, \(|\mathsf{enc}_{\mathsf{gre}}(\bm{s})|\leq m\) surely, and following the analysis in Appendix E.2 of eq. (47), we have that,

\[\lim_{i^{\star}\to\infty}\frac{|\mathsf{enc}_{\mathsf{gre}}(\bm{s})|}{i^{\star }}\leq\lim_{i^{\star}\to\infty}\frac{m}{i^{\star}}\stackrel{{ \text{\scriptsize{as.}}}}{{=}}1+\delta^{-1}.\] (51)

For \(x\in\{0,2\}\), observe that the expected number of times the token \(x\) is observed in the encoding of \(\bm{s}\), \(n_{x}\) can be written as,

\[n_{x}\geq\sum_{i=1}^{i^{\star}}\left((X_{i}+1)\cdot\mathbb{I}(X_{i-1}\not\equiv 1 \mod\ell)\right)\mathbb{I}(\mathcal{E}_{i}(x)).\] (52)

In particular, taking the expectation of eq. (52),

\[\mathbb{E}[n_{x}|\mathcal{E}_{1}(0)\cup\mathcal{E}_{1}(2)],\ \mathbb{E}[n_{x}|\mathcal{E}_{1}(1)]\geq\frac{i^{\star}-1}{4}(1+\delta^{-1}) \left(1-\sum_{u=0}^{\infty}\delta(1-\delta)^{\ell u+1}\right)\geq\frac{i^{ \star}-1}{4}\cdot\frac{1-\delta^{2}}{\delta}.\] (53)

Note that in any realization of the geometric sampling process, in eq. (52), either the odd indexed substrings are all-\(1\)'s or the even indexed substrings are all-\(1\)'s. Therefore, surely, all the non-zero terms in the above summation are of the same parity. Moreover, since the \(i^{th}\) term in the sum only depends on \(X_{i}\) and \(X_{i-1}\), conditioned on whether the non-zero parities are even or odd, \(n_{x}\) can be written as a sum of \(\approx i^{\star}/2\) mutually independent terms. By the strong law of large numbers on each of the conditional processes, eqs. (52) and (53) implies that for \(x\in\{0,2\}\),

\[\lim_{i^{\star}\to\infty}\frac{n_{x}}{i^{\star}}\stackrel{{\text {\scriptsize{as.}}}}{{\geq}}\frac{1-\delta^{2}}{4\delta}.\]

To upper bound \(n_{x}\), note that it is upper bounded by the number of times the character \(x\) appears in the source string, which by the strong law of large numbers a.a.s (after normalizing by \(i^{\star}\)), scales as \(1/4\delta\). Finally, to bound \(Q_{\text{MLE}}(\bm{t})\) which is the sequential nature of the encoder, using a similar proof as Lemma A.4, we can show that \(n_{\bm{t}}/\sum_{\bm{t}^{\prime}}n_{\bm{t}^{\prime}}\) converges to the unigram MLE model for this tokenizer. For the token \(x\in\{0,2\}\),

\[\lim_{i^{\star}\to\infty}\frac{n_{x}}{|\mathsf{enc}(\bm{s})|}=Q_{\text{MLE}}(x )\leq\mathbb{E}\left[\lim_{i^{\star}\to\infty}\frac{n_{x}}{n_{2}+n_{0}}\right]\] (54)

Using the a.a.s. upper and lower bounds on \(|\mathsf{enc}(\bm{s})|\), \(n_{0}\) and \(n_{2}\) derived in eqs. (51) and (54), we arrive at lower and upper bounds on \(Q_{\text{MLE}}(x)\) for \(x\in\{0,2\}\),

\[\frac{1}{4}\approx\frac{1-\delta}{4}=\frac{(1-\delta^{2})}{4\delta(1+\delta^{ -1})}\leq Q_{\text{MLE}}(x)\leq\frac{1}{2(1-\delta^{2})}\approx\frac{1}{2}.\]

Since there are at least two tokens having probability bounded away from \(0\) and \(1\) by a constant under the MLE unigram model, the entropy of \(Q_{\text{MLE}}\) must also be lower bounded by a constant. Indeed,

\[H(Q_{\text{MLE}})\geq 2\min_{\frac{1-\delta}{4}\leq y\leq\frac{1}{2(1-\delta^{2} )}}y\log(1/y).\]

It is easy to verify that for \(\delta\leq 0.5\), the minimizer is achieved at \(y=\frac{1-\delta}{4}\), which leads to the lower bound,

\[H(Q_{\text{MLE}})\geq\left(\frac{1-\delta}{2}\right)\log\left(\frac{4}{1- \delta}\right)\]Finally, combining this lower bound on \(H(Q_{\text{MLE}})\) with eq. (48), we have that,

\[\min_{Q\in\mathcal{Q}:\text{\sf{pm}}}\lim_{m\to\infty}\frac{1}{m} \mathcal{L}_{m}(Q\circ\text{\sf{enc}}(\cdot)) =\lim_{i^{\star}\to\infty}\mathbb{E}\left[\frac{|\text{\sf{enc}}_{ \text{gre}}(\bm{s})|}{m}H(Q_{\text{MLE}})\right]\] \[\geq\lim_{i^{\star}\to\infty}\mathbb{E}\left[\frac{|\text{\sf{enc }}_{\text{gre}}(\bm{s})|}{m}\right]\cdot\left(\frac{1-\delta}{2}\right)\log \left(\frac{4}{1-\delta}\right)\] \[\geq\frac{(1-\delta)^{2}}{3(1+\delta)}H(\pi)\]

where \((i)\) follows from the lower bound on \(|\text{\sf{enc}}_{\text{gre}}(\bm{s})|\) in eq. (50) with the almost sure limit of \(m\) in eq. (47) and noting that \(|\text{\sf{enc}}_{\text{gre}}(\bm{s})|/m\leq 1\) surely. The last inequality follows by simplifying using \(\pi=(1/4,1/2,1/4)\) and \(H(\pi)=\frac{1}{2}\log(8)\).

## Appendix F Experiment details

Experiment 1 (Figures 3(a) and 3(b)).In this and previous experiments (Figures 2, 2(a) and 2(b)), we train the transformers on a single GPU on an \(8\times\) A100 node. The wall-clock time measured does not count time spent in validation loss evaluations. The hyperparameter choices are listed in Table 3.

Experiment 2 (Table 1).We evaluate pre-trained tokenizers on various datasets. In this experiment, we do not evaluate the likelihood model on test sequences, rather, we estimate the cross-entropy of the best unigram model by using the approximation,

\[-\mathbb{E}\Bigg{[}\sum_{\bm{t}\in\text{Dlet}}n_{\bm{t}}\log Q_{\text{MLE}}( \bm{t})\Bigg{]}\approx-\sum_{\bm{t}\in\text{Dlet}}\widehat{n}_{\bm{t}}\log( \widehat{Q}(\bm{t}))\] (55)

where \(\widehat{Q}(\bm{t})=\frac{\widehat{n}_{\bm{t}}}{\sum_{\bm{t}}\widehat{n}_{\bm {t}}}\) is the MLE unigram model learnt from a finite dataset, which we choose here as GLUE (Wang et al., 2019), and \(\widehat{n}_{\bm{t}}\) is the number of times the token \(\bm{t}\) is observed in the encoding of the dataset. This approximation allows us to separate the error stemming from learning a suboptimal likelihood model which tends to have higher sample complexity requirements and focus on the asymptotic error of the tokenizer.

\begin{table}
\begin{tabular}{l l} \hline \hline Architecture & GPT-2 \\ \hline Batch size & Grid-searched in \(\{8,16,32\}\) \\ Gradient acc. steps & \(1\) \\ \hline Tokenizer dictionary size & \(\{10,20\}\) \\ Tokenizer dataset size & \(10,000\) \\ \hline Optimizer & AdamW \((\beta_{1}=0.9,\beta_{2}=0.95)\) \\ Learning rate & \(0.002\) \\ Scheduler & Cosine \\ \# Iterations & \(8000\) \\ Weight decay & \(1\times 10^{-3}\) \\ \hline Dropout & \(0\) \\ Sequence length & \(512\) \\ Embedding dimension & Grid-searched in \(\{10,20,30,40\}\) \\ \# layers & Grid-searched in \(\{1,2,4,8\}\) \\ \# heads & Grid-searched in \(\{1,2,4,8,16\}\) \\ \hline Repetitions & \(5\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameter choicesWe use Monte-carlo sampling to approximate the cross-entropy loss estimator in eq. (55). These approximations tends to underestimate the true cross-entropy loss due to the concavity of \(x\log(1/x)\) close to \(0\). In general, the gap between the approximation and the true error is expected to grow with \(k\). Therefore, the true difference between the estimate of the best unigram model on a tokenizer and the best \(k\)-gram model for \(k\geq 2\) on the character level tokenizer is likely to be larger than the reported figures.

Experiment 3 (Figure 5).We train the LZW, BPE, Unigram and Wordpiece tokenizers with dictionary sizes \(\{5000,6000,8000,12000,20000,32000,50000,8000\}\). The cross-entropy loss incurred by the best \(1\)-gram model is estimated using eq. (55) while for \(k\)-gram models for \(k\geq 2\), we use Monte-carlo sampling to estimate the cross-entropy of the empirical \(k\)-gram model computed using the GLUE dataset. For the \(k\)-gram models trained on the character level tokenizer, since the vocabulary size is fixed, we instead plot the number of distinct \(k\)-grams on the \(x\)-axis. While this is not a true measure of the number of parameters in the underlying \(k\)-gram model, we use this as a proxy for the same.

NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA]  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper lists an empirical phenomenon (justified in Fig. 2) and theoretical contributions justified in Theorems 3.1, 3.3 and 3.5 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Remark 3.3 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.

* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Assumption 3.2 Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The code has been released along with the rest of the submission. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. * The author is interested in the design of the paper, and the author is interested in the design of the paper.

* If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
* If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Instructions provided in the jupyter notebook. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Table 3 Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: All plots which allow for it, contain standard error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Appendix F contains this information. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: No NeurIPS code of ethics were violated. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work is a primarily theoretical study on the behavior of tokenization on toy problems (learning Markov chains). The societal impact of this research is not likely to be significant. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No models with a high risk for misuse were trained or released. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Code has been properly credited, via citing the relevant paper. Guidelines:* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets released. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No crowdsourcing or research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.