# iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models

 Tianyu Chen\({}^{\ast}\)\({}^{\dagger}\)  Kevin Bello\({}^{\ddagger\lx@sectionsign}\)  Bryon Aragam\({}^{\ddagger}\)  Pradeep Ravikumar\({}^{\lx@sectionsign}\)

\({}^{\dagger}\)Department of Statistics and Data Science, University of Texas at Austin

\({}^{\ddagger}\)Booth School of Business, University of Chicago

\({}^{\lx@sectionsign}\)Machine Learning Department, Carnegie Mellon University

Work done while at the Department of Statistics at the University of Chicago. Correspondence to kbello@cs.cmu.edu.

###### Abstract

Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the underlying causal structure is often unknown, and estimating it from data remains a challenging task. In many situations, however, the end goal is to localize the changes (shifts) in the causal mechanisms between related datasets instead of learning the full causal structure of the individual datasets. Some applications include root cause analysis, analyzing gene regulatory network structure changes between healthy and cancerous individuals, or explaining distribution shifts. This paper focuses on identifying the causal mechanism shifts in two or more related datasets over the same set of variables--_without estimating the entire DAG structure of each SCM_. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we assume that each SCM belongs to the more general class of _nonlinear_ additive noise models (ANMs). A key technical contribution of this work is to show that the Jacobian of the score function for the _mixture distribution_ allows for the identification of shifts under general non-parametric functional mechanisms. Once the shifted variables are identified, we leverage recent work to estimate the structural differences, if any, for the shifted variables. Experiments on synthetic and real-world data are provided to showcase the applicability of this approach. Code implementing the proposed method is open-source and publicly available at https://github.com/kevinsbello/iSCAN.

## 1 Introduction

Structural causal models (SCMs) are powerful models for representing causal relationships among variables in a complex system [54, 58]. Every SCM has an underlying graphical structure that is generally assumed to be a directed acyclic graph (DAG). Identifying the DAG structure of an SCM is crucial since it enables reasoning about interventions [54]. Nonetheless, in most situations, scientists can only access _observational_ or _interventional_ data, or both, while the true underlying DAG structure remains _unknown_. As a result, in numerous disciplines such as computational biology [66, 30, 20], epidemiology [64], medicine [61, 62], and econometrics [34, 28, 18], it is critically important to develop methods that can estimate the entire underlying DAG structure based on available data. This task is commonly referred to as causal discovery or structure learning, for which a variety of algorithms have been proposed over the last decades.

Throughout this work, we make the assumption of causal sufficiency (i.e., non-existence of unobserved confounders). Under this condition alone, identifying the underlying DAG structure is not possible in general, and remains worst-case NP-complete [14, 16]. Indeed, prominent methods such as PC [71]and GES [15] additionally require the arguably strong faithfulness assumption [77] to consistently estimate, in large samples, the Markov equivalent class of the underlying DAG. However, these methods are not consistent in high-dimensions unless one additionally assumes sparsity or small maximum-degree of the true DAG [36, 50, 78]. Consequently, the existence of hub nodes, which is a well-known feature in several networks [5, 6, 7], significantly complicates the DAG learning problem.

In many situations, however, the end goal is to _detect shifts (changes) in the causal mechanisms_ between two (or more) related SCMs rather than recovering the _entire_ underlying DAG structure of each SCM. For example, examining the mechanism changes in the gene regulatory network structure between healthy individuals and those with cancer may provide insights into the genetic factors contributing to the specific cancer; within biological pathways, genes could regulate various target gene groups depending on the cellular environment or the presence of particular disease conditions [32, 60]. In these examples, while the individual networks could be _dense_, the number of mechanism shifts could be _sparse_[69, 74, 55]. Finally, in root cause analysis, the goal is to identify the sources that originated observed changes in a joint distribution; this is precisely the setting we study in this work, where we model the joint distributions via SCMs, as also done in [52, 33].

In more detail, we focus on the problem of identifying mechanism shifts given datasets from two or more environments (SCMs) over the same observables. We assume that each SCM belongs to the class of additive noise models (ANMs) [29], i.e., each variable is defined as a nonlinear function over a subset of the remaining variables plus a random noise (see Section 2 for formal definitions). Importantly, we _do not_ make any structural assumptions (e.g., sparsity, small maximum-degree, or bounded tree-width) on the individual DAGs. Even though ANMs are well-known to be identifiable [29, 59], we aim to detect the _local distribution changes_ without estimating the full structures individually. See Figure 1 for a toy example of what we aim to estimate. A similar setting to this problem was studied in [82, 23] albeit in the restrictive linear setting. Finally, it is worth noting that even with _complete knowledge of the entire structure of each SCM_, assessing changes in _non-parametric_ functions across different groups or environments remains a very challenging problem [see for instance, 44].

Contributions.Motivated by recent developments on causal structure learning of ANMs [65], we propose a two-fold algorithm that (1) Identifies shifted variables (i.e., variables for which their causal mechanism has changed across the environments); and (2) If needed, for each shifted variable, estimates the structural changes among the SCMs. More concretely, we make the following set of contributions:

* To identify shifted variables (Definition 3), we prove that the variance of the diagonal elements of the Hessian matrix associated with the log-density of the _mixture distribution_ unveils information to detect distribution shifts in the leaves of the DAGs (see Theorem 1). Due to this result, our algorithm (Algorithm 1) iteratively chooses a particular leaf variable and determines whether or not such variable is shifted. Importantly, this detection step **does not** rely on any structural assumptions on the individual DAGs, and can consistently detect distribution shifts for _non-parametric functionals_ under very mild conditions such as second-order differentiability.
* To identify structurally shifted edges (Definition 4), we propose a nonparametric local parents recovery method (Algorithm 2) based on a recent measure of conditional dependence [3]. In addition, based on recent results in [4], we provide a theoretical justification for the asymptotic consistency of Algorithm 2 in Theorem 2. Importantly, since structural changes can only occur on _shifted nodes_, this second step can be conducted much more efficiently when the sparse mechanism shift hypothesis [69] holds, which posits that only a small subset of the causal model's mechanisms change.
* We empirically demonstrate that our method can outperform existing methods such as DCI, which is tailored for linear models, as well as related methods for estimating unknown intervention targets such as UT-IGSP [72]. See Section 5 and Appendix C for more details. Moreover, in Section 5.2, we provide experiments on an ovarian cancer dataset, thus, showcasing the applicability of our method.

## 2 Preliminaries and Background

In this section we introduce notation and formally define the problem setting. We use \([d]\) to denote the set of integers \(\{1,\ldots,d\}\). Let \(G=([d],E)\) be a DAG with node set \([d]\) and a set of directed edges \(E\subset[d]\times[d]\), where any \((i,j)\in E\) indicates and edge from \(i\) to \(j\). Also let \(X=(X_{1},\ldots,X_{d})\) denote a \(d\)-dimensional vector of random variables. An SCM \(\mathcal{M}=(X,f,\mathbb{P}_{N})\) over \(d\) variables is generally defined as a collection of \(d\) structural equations of the form:

\[X_{j}=f_{j}(\mathrm{PA}_{j},N_{j}),\forall j\in[d],\] (1)

where \(\mathrm{PA}_{j}\subseteq\{X_{1},\ldots,X_{d}\}\setminus\{X_{j}\}\) are the _direct causes_ (or parents) of \(X_{j}\); \(f=\{f_{j}\}_{j=1}^{d}\) is a set of _functional mechanisms_\(f_{j}:\mathbb{R}^{|\mathrm{PA}_{j}|+1}\to\mathbb{R}\); and \(\mathbb{P}_{N}\) is a joint distribution2 over the noise variables \(N_{j}\), which we assume to be jointly independent3. Moreover, the underlying graph \(G\) of an SCM is constructed by drawing directed edges for each \(X_{k}\in\mathrm{PA}_{j}\) to \(X_{j}\). We henceforth assume this graph to be acyclic, i.e., a DAG. Finally, every SCM \(\mathcal{M}\) defines a unique distribution \(\mathbb{P}_{X}\) over the variables \(X\)[Proposition 6.3 in 58], which by the independence of the noise variables (a.k.a. the Markovian assumption), \(\mathbb{P}_{X}\) admits the following factorization:

Footnote 2: We will always assume the existence of a density function w.r.t. the Lebesgue measure.

\[\mathbb{P}(X)=\prod_{j=1}^{d}\mathbb{P}(X_{j}\mid\mathrm{PA}_{j}),\] (2)

where \(\mathbb{P}(X_{j}\mid\mathrm{PA}_{j})\) is referred as the _causal mechanism_ of \(X_{j}\).

Figure 1: Illustration of two different environments (see Definition 2) in (1c) and (1d), both originated from the underlying SCM in (1a) with structural equations given in (1b). Between the two environments, we observe a change in the causal mechanisms of variables \(X_{3}\) and \(X_{5}\)—the red nodes in (1e). Specifically, for \(X_{5}\), we observe that its _functional dependence_ changed from \(X_{4}\) in \(\mathcal{E}_{1}\) to \(X_{3}\) in \(\mathcal{E}_{2}\). For \(X_{3}\), its _structural dependence_ has not changed between \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\), and only its functional changed from \(\mathrm{sinc}(X_{1})\) in \(\mathcal{E}_{1}\) to the sigmoid function \(\sigma(X_{1})\) in \(\mathcal{E}_{2}\). Finally, in (1e), the red edges represent the _structural_ changes in the mechanisms. The non-existence of an edge from \(X_{1}\) to \(X_{3}\) indicates that the structural relation between \(X_{1}\) and \(X_{3}\) is invariant.

The model above is often too general due to problems of identifiability. In this work we will consider that the noises are additive.

**Definition 1** (Additive noise models (ANMs)).: _An additive noise model is an SCM \(\mathcal{M}=(X,f,\mathbb{P}_{N})\) as in (1), where each structural assignment has the form:_

\[X_{j}=f_{j}(\mathrm{PA}_{j})+N_{j},\forall j\in[d].\]

Depending on the assumptions on \(f_{j}\) and \(N_{j}\), the underlying DAG of an ANM can be identifiable from observational data. E.g., when \(f_{j}\) is linear and \(N_{j}\) is Gaussian, in general one can only identify the Markov equivalence class (MEC) of the DAG, assuming faithfulness [54]. For linear models, an exception arises when assuming equal error variances [56; 78; 47], or non-Gaussian errors [70]. In addition, when \(f_{j}\) is nonlinear on each component and three times differentiable then the DAG is also identifiable [59; 29]. Very recently, Rolland et al. [65] proved DAG identifiability when \(f_{j}\) is nonlinear on each component and \(N_{j}\) is Gaussian, using information from the score's Jacobian.

### Data from multiple environments

Throughout this work we assume that we observe a collection of datasets, \(\mathcal{D}=\{\bm{X}^{h}\}_{h=1}^{H}\), from \(H\) (possibly different) environments. Each dataset \(\bm{X}^{h}=\{X^{h,i}\}_{i=1}^{m_{h}}\) from environment \(h\) contains \(m_{h}\) (possibly non-independent) samples from the joint distribution \(\mathbb{P}_{X}^{h}\), i.e., \(\bm{X}^{h}\in\mathbb{R}^{m_{h}\times d}\). We consider that each environment originates from soft interventions4[54] of an _unknown_ underlying SCM \(\mathcal{M}^{*}\) with DAG structure \(G^{*}\) and joint distribution \(\mathbb{P}^{*}(X)=\prod_{j=1}^{d}\mathbb{P}^{*}(X_{j}\mid\mathrm{PA}_{j}^{*})\). Here \(\mathrm{PA}_{j}^{*}\) denotes the parents (direct causes) of \(X_{j}\) in \(G^{*}\). Then, an environment arises from manipulations or shifts in the causal mechanisms of a _subset_ of variables, transforming from \(\mathbb{P}^{*}(X_{j}\mid\mathrm{PA}_{j}^{*})\) to \(\widetilde{\mathbb{P}}(X_{j}\mid\widetilde{\mathrm{PA}}_{j})\). Throughout, we will make the common modularity assumption of causal mechanisms [54; 38], which postulates that an intervention on a node \(X_{j}\) only changes the mechanism \(\mathbb{P}(X_{j}\mid\mathrm{PA}_{j})\), while all other mechanisms \(\mathbb{P}(X_{i}\mid\mathrm{PA}_{i})\), for \(i\neq j\), remain unchanged.

Footnote 4: These types of interventions are more realistic in practice than “hard” or perfect interventions. However, note that we allow a soft intervention on a variable to remove some or all of its causes, where the latter is also known as an stochastic hard intervention.

**Definition 2** (Environment).: _An environment \(\mathcal{E}_{h}=(X,f^{h},\mathbb{P}_{N}^{h})\), with joint distribution \(\mathbb{P}_{X}^{h}\) and density \(p_{x}^{h}\), independently results from an SCM \(\mathcal{M}^{*}\) by intervening on an unknown subset \(S^{h}\subseteq[d]\) of causal mechanisms, that is, we can factorize the joint distribution \(\mathbb{P}^{h}(X)\) as follows:_

\[\mathbb{P}^{h}(X)=\prod_{j\in[d]}\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}_{j}^{h})= \prod_{j\in S^{h}}\widetilde{\mathbb{P}}^{h}(X_{j}\mid\widetilde{\mathrm{PA} }_{j}^{h})\prod_{j\notin S^{h}}\mathbb{P}^{*}(X_{j}\mid\mathrm{PA}_{j}^{*}),\] (3)

_where \(\widetilde{\mathrm{PA}}_{j}^{h}\) is a (possibly empty) subset of the underlying causal parents \(\mathrm{PA}_{j}^{*}\), i.e., \(\widetilde{\mathrm{PA}}_{j}^{h}\subseteq\mathrm{PA}_{j}^{*}\); and, \(\mathbb{P}^{*}(X_{j}\mid\mathrm{PA}_{j}^{*})\) are the invariant mechanisms._

**Remark 1**.: _In the literature [e.g., 55], it is common to find the assumption that in a soft intervention the direct causes remain invariant, i.e., \(\widetilde{\mathrm{PA}}_{j}^{h}=\mathrm{PA}_{j}^{*}\) for all \(j\in S^{h},h\in[H]\). In this work we consider a more general setting where none, some, or all of the direct causes of an intervened node are removed, i.e., \(\widetilde{\mathrm{PA}}_{j}^{h}\subseteq\mathrm{PA}_{j}^{*}\) for all \(j\in S^{h},h\in[H]\)._

We next define shifted nodes (variables).

**Definition 3** (Shifted node).: _Given \(H\) environments \(\{\mathcal{E}_{h}=(X,f^{h},\mathbb{P}_{N}^{h})\}_{h=1}^{H}\) originated from an ANM \(\mathcal{M}^{*}\), a node \(j\) is called a shifted node if there exists \(h,h^{\prime}\in[H]\) such that:_

\[\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}_{j}^{h})\neq\mathbb{P}^{h^{\prime}}(X_{j} \mid\mathrm{PA}_{j}^{h^{\prime}}).\]

To conclude this section, we formally define the problem setting.

**Problem setting.** Given \(H\) datasets \(\{\bm{X}^{h}\}_{h=1}^{H}\), where \(\bm{X}^{h}\sim\mathbb{P}_{X}^{h}\) consists of \(m_{h}\) (possibly non-independent) samples from the environment distribution \(\mathbb{P}_{X}^{h}\) originated from an underlying ANM \(\mathcal{M}^{*}\), estimate the set of shifted nodes and structural differences.

We note that [82; 23] have study the problem setting above for \(H=2\), assuming _linear functions_\(f_{j}^{h}\), and Gaussian noises \(N_{j}^{h}\). In this work, we consider a more challenging setting where \(f_{j}^{h}\) are nonparametric functions (see Section 3 for more details).

### Related Work

First we mention works most closely related to ours. The problem of learning the difference between _undirected_ graphs has received much more attention than the directed case. E.g., [88; 46; 85; 19] develop algorithms for estimating the difference between Markov random fields and Ising models. See [87] for recent developments in this direction. In the directed setting, [82; 23] propose methods for directly estimating the difference of linear ANMs with Gaussian noise. More recently, [67] studied the setting where a dataset is generated from a mixture of SCMs, and their method is capable of detecting conditional distributions changes; however, due to the unknown membership of each sample, it is difficult to test for structural and functional changes. Moreover, in contrast to ours, all the aforementioned work on the directed setting rely on some form of faithfulness assumption.

**Causal discovery from a single environment.** One way to identify mechanism shifts (albeit inefficient) would be to estimate the individual DAGs for each environment and then test for structural differences across the different environments. A few classical and recent methods for learning DAGs from a single dataset include: Constraint-based algorithms such as PC and FCI [71]; in score-based methods, we have greedy approaches such as GES [16], likelihood-based methods [56; 47; 59; 2; 1; 29], and continuous-constrained learning [89; 51; 39; 8]. Order-based methods [75; 41; 24; 65; 48], methods that test for asymmetries [70; 12], and hybrid methods [50; 76]. Finally, note that even if we _perfectly estimate each individual DAG_ (assuming identifiable models such as ANMs), applying these methods would only identify _structural_ changes. That is, for variables that have the same parents across all the environments, we would require an additional step to identify _distributional_ changes.

**Testing functional changes in multiple datasets.** Given the parents of a variable \(X_{j}\), one could leverage prior work [44; 25; 9; 26] on detecting heterogeneous functional relationships. However, we highlight some important limitations. Several methods such as [25; 9; 26] only work for one dimensional functionals and assume that the datasets share the exact same design matrix. Although [44] relaxes this assumption and extends the method to multivariate cases, the authors assume that the covariates (i.e., \(\mathrm{PA}_{j}^{h}\)) are sampled from the _same distribution_ across the environments, which is a strong assumption in our context since ancestors of \(X_{j}\) could have experienced mechanism shifts. Finally, methods such as [53] and [11], although nonparametric, need knowledge about the parent set \(\mathrm{PA}_{j}\) for each variable, and they assume that \(\mathrm{PA}_{j}\) is same across different environments.

**Causal discovery from heterogeneous data.** Another well-studied problem is to learn the underlying DAG of the SCM \(\mathcal{M}^{*}\) that originated the different environments. Under this setting, [83] provided a characterization of the \(\mathcal{I}\)-MEC, a subset of the Markov equivalence class. [55] provided DAG-identifiability results by leveraging sparse mechanism shifts and relies on identifying such shifts, which this work aims to solve. [10] developed an estimator considering unknown intervention targets. [79] primarily focuses on linear SEM and does not adapt well to nonlinear scenarios. Also assuming linear models, [22; 21] applied ideas from linear invariant causal prediction [ICP, 57] and ICM to identify the causal DAG. [72] proposes a nonparametric method that can identify the intervention targets; however, this method relies on nonparametric CI tests, which can be time-consuming and sample inefficient. [49] introduced the joint causal inference (JCI) framework, which can also estimate intervention nodes. However, this method relies on an assumption that the intervention variables are fully connected, a condition that is unlikely to hold in practice. [31] introduced a two-stage approach that removes functional restrictions. First, they used the PC algorithm using all available data to identify the MEC. Then, the second step aims to orient the remaining edges based on a novel measure of mechanism dependence. Finally, we note that a common assumption in the aforementioned methods is the knowledge of which dataset corresponds to the observational distribution; without such information, their assumptions on the type of interventions would not hold true. In contrast, our method does not require knowledge of the observational distribution.

## 3 Identifying Causal Mechanism Shifts via Score Matching

In this section, we propose iSCAN (_identifying Shifts in Causal Additive Noise models_), a method for detecting shifted nodes (Definition 3) based only on information from the Jacobian of the score of the data distribution5.

Let \(\bm{X}\) be the row concatenation of all the datasets \(\bm{X}^{h}\), i.e., \(\bm{X}=[(\bm{X}^{1})^{\top}\ |\ \cdots\ |\ (\bm{X}^{H})^{\top}]^{\top}\in\mathbb{R}^{m\times d}\), where \(m=\sum_{h=1}^{H}m_{h}\). The pooled data \(\bm{X}\) can be interpreted as a mixture of data from the \(H\) different environments. To account for this mixture, we introduce the probability mass \(w_{h}\), which represents the probability that an observation belongs to environment \(h\), i.e., \(\sum_{h=1}^{H}w_{h}=1\). Let \(\mathbb{Q}(X)\) denote the distribution of the mixture data with density function \(q(x)\), i.e., \(q(x)=\sum_{h=1}^{H}w_{h}p^{h}(x)\).

In the sequel, we use \(s^{h}(x)\equiv\nabla\log p^{h}(x)\) to denote the score function of the joint distribution of environment \(h\) with density \(p^{h}(x)\). Also, we let \(s(x)\equiv\nabla\log q(x)\) to denote the score function of the mixture distribution with density \(q(x)\). We will make the following assumptions on \(f^{h}_{j}\) and \(N^{h}_{j}\).

**Assumption A**.: _For all \(h\in[H],j\in[d]\), the functional mechanisms \(f^{h}_{j}(\mathrm{PA}^{h}_{j})\) are assumed to be non-linear in every component._

**Assumption B**.: _For all \(j\in[d],h\in[H]\), the pdf of the real-valued noise \(N^{h}_{j}\) denoted by \(p^{h}_{N_{j}}\) satisfies \(\frac{\partial^{2}}{(\partial n^{h}_{j})^{2}}\log p^{h}_{N_{j}}(n^{h}_{j})=c^{ h}_{j}\) where \(c^{h}_{j}\) is a non-zero constant. Moreover, \(\mathbb{E}[N^{h}_{j}]=0\)._

For an ANM, Rolland et al. [65] showed that under Assumption A and assuming zero-mean Gaussian noises (which satisfies Assumption B), the diagonal of the Jacobian of the score function reveals the leaves of the underlying DAG. We next instantiate their result in our context.

**Proposition 1** (Lemma 1 in [65, 68]).: _For an environment \(\mathcal{E}_{h}\) with underlying DAG \(G^{h}\) and pdf \(p^{h}(x)\), let \(s^{h}(x)=\nabla\log p^{h}(x)\) be the associated score function. Then, under Assumptions A and B, for all \(j\in[d]\), we have:_

\[\text{Node $j$ is a leaf in $G^{h}$}\iff\mathrm{Var}_{X}\left[\frac{\partial s^{h}_{j }(X)}{\partial x_{j}}\right]=0.\]

Motivated by the ideas of leaf-identifiability from the score's Jacobian in a _single_ ANM, we next show that the _score's Jacobian of the mixture distribution_ can help reveal mechanism shifts among the different environments.

**Theorem 1**.: _For all \(h\in[H]\), let \(G^{h}\) and \(p^{h}(x)\) denote the underlying DAG structure and pdf of environment \(\mathcal{E}_{h}\), respectively, and let \(q(x)\) be the pdf of the mixture distribution of the \(H\) environments such that \(q(x)=\sum_{h=1}^{H}w_{h}p^{h}(x)\). Also, let \(s(x)=\nabla\log q(x)\) be the associated score function. Then, under Assumptions A, and B, we have:_

1. _If_ \(j\) _is a leaf in all DAGs_ \(G^{h}\)_, then_ \(j\) _is a shifted node if and only if_ \(\mathrm{Var}_{X}\left[\frac{\partial s_{j}(X)}{\partial x_{j}}\right]>0\)_._
2. _If_ \(j\) _is not a leaf in at least one DAG_ \(G^{h}\)_, then_ \(\mathrm{Var}_{X}\left[\frac{\partial s_{j}(X)}{\partial x_{j}}\right]>0\)_._

Theorem 1 along with Proposition 1 suggests a way to identify shifted nodes. Namely, to use Proposition 1 to identify a common leaf, and then use Theorem 1 to test if such a leaf is a shifted node or not. We then proceed to remove the leaf and repeat the process. See Algorithm 1. Note that due to the fact that each environment is a result of an intervention (Definition 2) on an underlying ANM \(\mathcal{M}^{*}\), it follows that the leaves in \(G^{*}\) will remain leaves in each DAG \(G^{h}\).

```
0: Datasets \(\bm{X}^{1},\dots,\bm{X}^{H}\).
0: Shifted variables set \(\widehat{S}\), and topological sort \(\hat{\pi}\).
1: Initialize \(\widehat{S}=\{\}\), \(\hat{\pi}=(\ )\), \(\mathcal{N}=\{1,\dots,d\}\)
2: Set \(\bm{X}=[(\bm{X}^{1})^{\top}\ |\ \cdots\ |\ (\bm{X}^{H})^{\top}]^{\top}\in\mathbb{R}^{m\times d}\).
3:while\(\mathcal{N}\neq\emptyset\)do
4:\(\forall h\in[H],\mathrm{Var}^{h}\leftarrow\mathrm{Var}_{\bm{X}^{h}}\left[\mathrm{ diag}(\nabla^{2}\log p^{h}(x))\right]\).
5:\(\mathrm{Var}\leftarrow\mathrm{Var}_{\bm{X}}\left[\mathrm{diag}(\nabla^{2}\log q(x))\right]\)
6:\(L\leftarrow\bigcap_{h\in[H]}\left\{j\ |\ \mathrm{Var}^{h}_{j}=0,j\in[d]\right\}\). \(\triangleright\) Identify leaves.
7:\(\widehat{S}\leftarrow\widehat{S}\ \bigcup\ \left\{j\ |\ \mathrm{Var}_{j}\neq 0,j\in L\right\}\)\(\triangleright\) Identify shifted nodes.
8:\(\mathcal{N}\leftarrow\mathcal{N}-\{L\}\)
9:\(\forall l\in L\), remove the \(l\)-th column of \(\bm{X}^{h}\), \(\forall h\in[H]\), and \(\bm{X}\).
10:\(\hat{\pi}\leftarrow(L,\hat{\pi})\). ```

**Algorithm 1****iSCAN**--Identifying Shifts in Causal Additive Noise models.

**Remark 2**.: _See Appendix A for a practical implementation of Alg. 1. Finally, note that Alg. 1 also estimates a valid topological sort for the different environments by leveraging Proposition 1._

### Score's Jacobian estimation

Since the procedure to estimate \(\mathrm{Var}_{q}[\frac{\partial s_{j}(x)}{\partial x_{j}}]\) is similar for estimating \(\mathrm{Var}_{p^{h}}[\frac{\partial s_{j}^{h}(x)}{\partial x_{j}}]\) for each \(h\in[H]\), in this section we discuss the estimation for \(\mathrm{Var}_{q}[\frac{\partial s_{j}(x)}{\partial x_{j}}]\), which involves computing the diagonal of the Hessian of \(\log q(x)\). To estimate this quantity, we adopt a similar approach to the method in [45; 65]. First, we estimate the first-order derivative of \(\log q(x)\) by Stein's identity [73]:

\[\mathbb{E}_{q}\left[\bm{h}(x)\nabla\log q(x)^{\top}+\nabla\bm{h}(x)\right]=0,\] (4)

where \(\bm{h}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d^{\prime}}\) is any test function such that \(\lim_{x\rightarrow\infty}\bm{h}(x)q(x)=0\). Once we have estimated \(\nabla\log q(x)\), we can proceed to estimate the Hessian's diagonal by using second-order Stein's identity:

\[\mathbb{E}_{q}[\bm{h}(x)\text{diag}(\nabla^{2}\log q(x))^{\top}]=\mathbb{E}_{ q}[\nabla_{\text{diag}}^{2}\bm{h}(x)-\bm{h}(x)\text{diag}(\nabla\log q(x) \nabla\log q(x)^{\top})]\] (5)

Using eq.(4) and eq.(5), we can estimate the Hessian's diagonal at each data point. Thus allowing us to obtain an estimate of \(\mathrm{Var}_{q}\left[\frac{\partial s_{j}(x)}{\partial x_{j}}\right]\). See Appendix A.1 for additional details.

**Remark 3** (Consistency of Algorithm 1).: _The estimators in eq.(6) and eq.(7), given in Appendix A.1, correspond to Monte Carlo estimators using eq.(4) and (5), respectively, then the error of the estimators tend to zero as the number of samples goes to infinity. See for instance the discussion in Section 3.1 in [45]. We empirically explore the consistency of Algorithm 1 in Figure 2._

**Remark 4** (Computational Complexity).: _Since we adopt the kernel-based estimator, \(\mathsf{SCORE}\), from [65]. The computational complexity for the estimation of the score's Jacobian in a single environment is \(\mathcal{O}(dm_{h}^{3})\). In Algorithm 1, computation is dominated by the \(\mathsf{SCORE}\) function applied to the pooled data \(\bm{X}\in\mathbb{R}^{m\times d}\). Therefore, the overall complexity of Algorithm 1 is \(\mathcal{O}(dm^{3})\). See Figure 2._

## 4 On Identifying Structural Differences

After estimating the set of shifted nodes \(\widehat{S}\) through Algorithm 1, it is of high interest to predict which causal relations between a shifted node and its parents have undergone changes across the environments. The meaning of a change in a causal relationship can vary based on the context and the estimation objective. This section primarily centers on structural changes, elaborated further below, while additional discussion about other types of changes is available in Appendix D.

**Definition 4** (Structurally shifted edge).: _For a given shifted node \(X_{j}\), an edge \(X_{i}\to X_{j}\) is called a structurally shifted edge if \(\exists h,h^{\prime}\in[H]\) such that \(X_{i}\in\mathrm{PA}_{j}^{h}\) and \(X_{i}\notin\mathrm{PA}_{j}^{h^{\prime}}\)._

In other words, a structurally shifted edge is an edge that exists in one environment but not in another, indicating a change in the underlying structure of the causal mechanism. To detect the structurally shifted edges, we will estimate the parents of each shifted node in \(\widehat{S}\) for all environments \(\mathcal{E}_{h}\).

Figure 2: (Left) F1 score of the output of Alg. 1 w.r.t. to the true set of shifted nodes. For different number of nodes, we observe how iSCAN recovers the true set of shifted nodes as the number of samples increases, thus empirically showing its consistency. (Right) Runtime vs number of nodes for different number of samples. We corroborate the linear dependence of the time complexity on \(d\).

**Remark 5**.: _Note that under the sparse mechanism shift hypothesis [69], i.e., \(|S|\ll d\), estimating the parents of each shifted node is much more efficient than estimating the entire individual structures._

Kernel regression and variable selection.A potential strategy to estimate structurally shifted edges involves employing the estimated topological order \(\hat{\pi}\) obtained from Algorithm 1. If this estimated topological order remains valid across all environments, it can serve as a guide for the nonparametric variable selection process to identify the parents of a shifted node \(X_{j}\). Specifically, we can regress the shifted node \(X_{j}\) on its predecessors \(\widehat{\mathrm{Pre}}(X_{j})\) and proceed with a nonparametric variable selection procedure. Here \(\widehat{\mathrm{Pre}}(X_{j})\) consists of the set of nodes that appear before \(X_{j}\) in the estimated topological order \(\hat{\pi}\). To achieve that, there exist various methods under the hypothesis testing framework [42; 17; 63], and bandwidth selection procedures [40]. These methods offer consistency guarantees, but their time complexity might be problematic. Kernel regression, for example, has a time complexity of \(\mathcal{O}(m^{3})\), and requires an additional bandwidth selection procedure, usually with a time complexity of \(\mathcal{O}(m^{2})\). Consequently, it becomes imperative to find a more efficient method for identifying parents locally.

Feature ordering by conditional independence (FOCI).An alternative efficient approach for identifying the parents is to leverage the feature ordering method based on conditional independence proposed by Azadkia and Chatterjee [3]. This method provides a measure of conditional dependency between variables with a time complexity of \(\mathcal{O}(m\log m)\). By applying this method, we can perform fast variable selection in a nonparametric setting. See Algorithm 4 in Appendix A.3.

**Theorem 2** (Consistency of Algorithm 4).: _Under Assumption C, given in Appendix B.2, if the estimated topological order \(\hat{\pi}\) output from Algorithm 1 is valid for all environments, then the output \(\widehat{\mathrm{PA}}_{j}^{h}\) of Algorithm 4 is equal to the true parents \(\mathrm{PA}_{j}^{h}\) of node \(X_{j}\) with high probability, for all \(h\in[H]\)._

Motivated by Theorem 2, we next present Algorithm 2, a procedure to estimate the structurally shifted edges. Given the consistency of Alg. 1 and Alg. 2, it follows that combining both algorithms will correctly estimate the true set of shifted nodes and structural shifted edges, asymptotically.

```
0: Data \(\{\bm{X}^{h}\}_{h\in[H]}\), topological order \(\hat{\pi}\), shifted nodes \(\widehat{S}\)
0: Structurally shifted edges set \(\widehat{E}\)
1: Initialize \(\widehat{E}=\emptyset\)
2:for\(X_{j}\) in \(\widehat{S}\)do
3:for\(h\) in \([H]\)do
4: Estimate \(\widehat{\mathrm{PA}}_{j}^{h}\) from Alg. 4 (FOCI) with input \(\{\widehat{\mathrm{Pre}}(\bm{X}_{j}^{h}),\bm{X}_{j}^{h}\}\)
5:if\(\exists X_{k},h,h^{\prime}\) such that \(X_{k}\in\widehat{\mathrm{PA}}_{j}^{h},X_{k}\notin\widehat{\mathrm{PA}}_{j}^{h^ {\prime}}\)then
6:\(\widehat{E}\leftarrow\widehat{E}\cup(X_{k},X_{j})\) ```

**Algorithm 2** Identifying structurally shifted edges

## 5 Experiments

We conducted a comprehensive evaluation of our algorithms. Section 5.1 focuses on assessing the performance of iSCAN (Alg. 1) for identifying shifted variables. In Section 5.2, we apply iSCAN for identifying shifted nodes along with FOCI (Alg. 2) for estimating structural changes, on apoptosis data. Also, in App. C, we provide additional experiments including: _(i)_ Localizing shifted nodes without structural changes (App. C.1), and where the functionals are sampled from Gaussian processes (App. C.1): _(ii)_ Localizing shifted nodes and estimating structural changes when the underlying graphs are different; and _(iii)_ Evaluating iSCAN using the elbow method for selecting shifted nodes (see App. C.3 and Remark 6). Code is publicly available at https://github.com/kevinsbello/iSCAN.

### Synthetic experiments on shifted nodes

Graph models.We generated random graphs using the Erdos-Renyi (ER) and scale free (SF) models. For a given number of variables \(d\), ER\(k\) and SF\(k\) indicate an average number of edges equal to \(kd\).

**Data generation.** We first sampled a DAG, \(G^{1}\), of \(d\) nodes according to either the ER or SF model for env. \(\mathcal{E}_{1}\). For env. \(\mathcal{E}_{2}\), we initialized its DAG structure from env. \(\mathcal{E}_{1}\) and produced structural changes by randomly selecting \(0.2\cdot d\) nodes from the non-root nodes. This set of selected nodes \(S\), with cardinality \(|S|=0.2d\), correspond to the set of "shifted nodes". In env. \(\mathcal{E}_{2}\), for each shifted node \(X_{j}\in S\), we uniformly at random deleted at most three of its incoming edges, and use \(D_{j}\) to denote the parents whose edges to \(X_{j}\) were deleted; thus, the DAG \(G^{2}\) is a subgraph of \(G^{1}\). Then, in \(\mathcal{E}_{1}\), each \(X_{j}\) was defined as follows:

\[X_{j}=\sum_{i\in\mathrm{PA}_{j}^{1}\setminus D_{j}}\sin(X_{i}^{2})+\sum_{i\in D _{j}}4\cos(2X_{i}^{2}-3X_{i})+N_{j}\]

In \(\mathcal{E}_{2}\), each \(X_{j}\) was defined as follows:

\[X_{j}=\sum_{i\in\mathrm{PA}_{j}^{2}}\sin(X_{i}^{2})+N_{j}\]

**Experiment details.** For each simulation, we generated \(500\) data points per environment, i.e., \(m_{1}=500,m_{2}=500\) and \(m=1000\). The noise variances were set to 1. We conducted 30 simulations for each combination of graph type (ER or SF), noise type (Gaussian, Gumbel, and Laplace), and number of nodes (\(d\in\{10,20,30,50\}\)). The running time was recorded by executing the experiments on an Intel Xeon Gold 6248R Processor with 8 cores. For our method, we used \(\eta=0.05\) for eq.(6) and eq.(7), and a threshold \(t=2\) (see Alg. 3).

**Evaluation.** We compared the performance of ISCAN against several baselines, which include: DCI [82], the approach by [11], CITE [79], KCD [53], SCORE [65], and UT-IGSP [72]. Figure 3 illustrates the results for ER4 and SF4 graphs. We note that iSCAN consistently outperforms other baselines in terms of F1 score across all scenarios. Importantly, note how the performance of some baselines, like DCI, CITE, Budhathoki's, and SCORE, degrades faster for graphs with hub nodes, a property of SF graphs. In contrast, iSCAN performs similarly, as it is not dependent on structural assumptions on the individual DAGs. Additionally, it is worth noting that our method exhibits faster computational time than KCD, Budhathoki's, and SCORE, particularly for larger numbers of nodes.

In Appendix C.1, we provide experiments on sparser graphs such as ER2/SF2, and denser graphs such as ER6/SF6. We also include Precision and Recall in all plots in the supplement.

### Experiments on apoptosis data

We conducted an analysis on an ovarian cancer dataset using iSCAN (Algorithm 1) to identify shifted nodes and Algorithm 2 to detect structurally shifted edges (SSEs). This dataset had previously been analyzed using the DPM method [88] in the undirected setting, and the DCI method [82] in the linear setting. By applying our method, we were able to identify the shifted nodes and SSEs in the dataset (see Figure 3(a)). Our analysis revealed the identification of two hub nodes in the apoptosis

Figure 3: Experiments on ER4 and SF4 graphs. See the experiment details above. The points indicate the average values obtained from these simulations. The error bars depict the standard errors. Our method iSCAN (light blue) consistently outperformed baseline methods in terms of F1 score.

pathway: BIRC3, and PRKAR2B. The identification of BIRC3 as a hub node was consistent with the results obtained by the DPM and DCI methods. Additionally, our analysis also identified PRKAR2B as a hub node, which was consistent with the result obtained by the DCI method. Indeed, BIRC3, in addition to its role in inhibiting TRAIL-induced apoptosis, has been investigated as a potential therapeutic target in cancer treatment including ovarian cancer[35; 81]; whereas PRKAR2B has been identified as an important factor in the progression of ovarian cancer cells. The latter serves as a key regulatory unit involved in the growth and development of cancer cells [84; 13].

## 6 Conclusion

In this work, we showed a novel connection between score matching and identifying causal mechanism shifts among related heterogeneous datasets. This finding opens up a new and promising application for score function estimation techniques.

Our proposed technique consists of three modules. The first module evaluates the Jacobian of the score under the _individual_ distributions and the _mixture_ distribution. The second module identifies shifted features (variables) using the estimated Jacobians, allowing us to pinpoint the nodes that have undergone a mechanism shift. Finally, the third module aims to estimate structurally shifted edges, a.k.a. the difference DAG, by leveraging the information from the identified shifted nodes and the estimated topological order. _It is important to note that our identifiability result in Theorem 1 is agnostic to the choice of the score estimator._

The strength of our result lies in its capability to recover the difference DAG in non-linear Additive Noise Models (ANMs) without making any assumptions about the parametric form of the functions or statistical independencies. This makes our method applicable in a wide range of scenarios where non-linear relationships and shifts in mechanisms are present.

### Limitations and future work

While our work demonstrates the applicability of score matching in identifying causal mechanism shifts in the context of nonlinear ANMs, there are several limitations and areas for future exploration:

_Extension to other families of SCMs_: Currently, our method is primarily focused on ANMs where the noise distribution satisfies Assumption B, e.g., Gaussian distributions. It would be valuable to investigate the application of score matching in identifying causal mechanism shifts in other types of SCMs. Recent literature, such as [48], has extended score matching to additive Models with arbitrary noise for finding the topological order. Expanding our method to accommodate different noise models would enhance its applicability to a wider range of real-world scenarios.

_Convergence rate analysis_: Although the score matching estimator is asymptotically consistent, the convergence rate remains unknown in general. Understanding the convergence properties of the estimator is crucial for determining the sample efficiency and estimating the required number of samples to control the estimation error within a desired threshold. Further theoretical developments, such as [37], on score matching estimators would provide valuable insights into the performance and sample requirements of iSCAN.

Figure 4: Results on apoptosis data.

[MISSING_PAGE_FAIL:11]

* Friedman et al. [2000] Friedman, N., Linial, M., Nachman, I. and Pe'er, D. [2000], Using bayesian networks to analyze expression data, _in_ 'Proceedings of the fourth annual international conference on Computational molecular biology', pp. 127-135.
* Ghassami et al. [2018] Ghassami, A., Kiyavash, N., Huang, B. and Zhang, K. [2018], 'Multi-domain causal structure learning in linear systems', _Advances in neural information processing systems_**31**.
* Ghassami et al. [2017] Ghassami, A., Salehkaleybar, S., Kiyavash, N. and Zhang, K. [2017], 'Learning causal structures using regression invariance', _Advances in Neural Information Processing Systems_**30**.
* Ghoshal et al. [2019] Ghoshal, A., Bello, K. and Honorio, J. [2019], 'Direct learning with guarantees of the difference dag between structural equation models', _arXiv preprint arXiv:1906.12024_.
* Ghoshal and Honorio [2018] Ghoshal, A. and Honorio, J. [2018], Learning linear structural equation models in polynomial time and sample complexity, _in_ 'Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics', Vol. 84 of _Proceedings of Machine Learning Research_, PMLR, pp. 1466-1475.
* Hall and Hart [1990] Hall, P. and Hart, J. D. [1990], 'Bootstrap test for difference between means in nonparametric regression', _Journal of the American Statistical Association_**85**(412), 1039-1049.
* Hardle and Marron [1990] Hardle, W. and Marron, J. S. [1990], 'Semiparametric comparison of regression curves', _The Annals of Statistics_ pp. 63-89.
* Hastie [2017] Hastie, T. J. [2017], Generalized additive models, _in_ 'Statistical models in S', Routledge, pp. 249-307.
* Hoover et al. [2009] Hoover, K. D., Demiralp, S. and Perez, S. J. [2009], 'Empirical identification of the vector autoregression: The causes and effects of us m2', _The methodology and practice of econometrics: a Festschrift in honour of David F. Hendry_ pp. 37-58.
* Hoyer et al. [2008] Hoyer, P., Janzing, D., Mooij, J. M., Peters, J. and Scholkopf, B. [2008], 'Nonlinear causal discovery with additive noise models', _Advances in neural information processing systems_**21**.
* Hu et al. [2018] Hu, P., Jiao, R., Jin, L. and Xiong, M. [2018], 'Application of causal inference to genomic analysis: advances in methodology', _Frontiers in Genetics_**9**, 238.
* Huang et al. [2020] Huang, B., Zhang, K., Zhang, J., Ramsey, J., Sanchez-Romero, R., Glymour, C. and Scholkopf, B. [2020], 'Causal discovery from heterogeneous/nonstationary data', _The Journal of Machine Learning Research_**21**(1), 3482-3534.
* Hudson et al. [2009] Hudson, N. J., Reverter, A. and Dalrymple, B. P. [2009], 'A differential wiring analysis of expression data correctly identifies the gene containing the causal mutation', _PLoS computational biology_**5**(5), e1000382.
* Ikram et al. [2022] Ikram, A., Chakraborty, S., Mitra, S., Saini, S., Bagchi, S. and Kocaoglu, M. [2022], 'Root cause analysis of failures in microservices through causal discovery', _Advances in Neural Information Processing Systems_**35**, 31158-31170.
* Imbens [2020] Imbens, G. W. [2020], 'Potential outcome and directed acyclic graph approaches to causality: Relevance for empirical practice in economics', _Journal of Economic Literature_**58**(4), 1129-1179.
* Johnstone et al. [2008] Johnstone, R. W., Frew, A. J. and Smyth, M. J. [2008], 'The trail apoptotic pathway in cancer onset, progression and therapy', _Nature Reviews Cancer_**8**(10), 782-798.
* Kalisch and Buhlman [2007] Kalisch, M. and Buhlman, P. [2007], 'Estimating high-dimensional directed acyclic graphs with the pc-algorithm.', _Journal of Machine Learning Research_**8**(3).
* Koehler et al. [2022] Koehler, F., Heckett, A. and Risteski, A. [2022], 'Statistical efficiency of score matching: The view from isoperimetry', _arXiv preprint arXiv:2210.00726_.
* Koller and Friedman [2009] Koller, D. and Friedman, N. [2009], _Probabilistic graphical models: principles and techniques_, MIT press.
* Lachapelle et al. [2019] Lachapelle, S., Brouillard, P., Deleu, T. and Lacoste-Julien, S. [2019], 'Gradient-based neural dag learning', _arXiv preprint arXiv:1906.02226_.
* Lafferty and Wasserman [2008] Lafferty, J. and Wasserman, L. [2008], 'Rodeo: Sparse, greedy nonparametric regression'.
* Larranaga et al. [1996] Larranaga, P., Kuijpers, C. M., Murga, R. H. and Yurramendi, Y. [1996], 'Learning bayesian network structures by searching for the best ordering with genetic algorithms', _IEEE transactions on systems, man, and cybernetics-part A: systems and humans_**26**(4), 487-493.

* [42] Lavergne, P. and Vuong, Q. [2000], 'Nonparametric significance testing', _Econometric Theory_**16**(4), 576-601.
* [43] Li, C., Shen, X. and Pan, W. [2023], 'Nonlinear causal discovery with confounders', _Journal of the American Statistical Association_ pp. 1-10.
* [44] Li, X., Jiang, B. and Liu, J. S. [2021], 'Kernel-based partial permutation test for detecting heterogeneous functional relationship', _Journal of the American Statistical Association_ pp. 1-19.
* [45] Li, Y. and Turner, R. E. [2017], 'Gradient estimators for implicit models', _arXiv preprint arXiv:1705.07107_.
* [46] Liu, S., Suzuki, T., Relator, R., Sese, J., Sugiyama, M., Fukumizu, K. et al. [2017], 'Support consistency of direct sparse-change learning in markov networks', _The Annals of Statistics_.
* [47] Loh, P.-L. and Buhlmann, P. [2014], 'High-Dimensional Learning of Linear Causal Networks via Inverse Covariance Estimation', _Journal of Machine Learning Research_.
* [48] Montagna, F., Nocei, N., Rosasco, L., Zhang, K. and Locatello, F. [2023], 'Causal discovery with score matching on additive models with arbitrary noise', _arXiv:2304.03265_.
* [49] Mooij, J. M., Magliacane, S. and Claassen, T. [2020], 'Joint causal inference from multiple contexts', _The Journal of Machine Learning Research_**21**(1), 3919-4026.
* [50] Nandy, P., Hauser, A. and Maathuis, M. H. [2018], 'High-dimensional consistency in score-based and hybrid structure learning', _The Annals of Statistics_**46**(6A), 3151-3183.
* [51] Ng, I., Ghassami, A. and Zhang, K. [2020], 'On the role of sparsity and dag constraints for learning linear dags', _Advances in Neural Information Processing Systems_**33**, 17943-17954.
* [52] Paleyes, A., Guo, S., Scholkopf, B. and Lawrence, N. D. [2023], Dataflow graphs as complete causal graphs, _in_ '2023 IEEE/ACM 2nd International Conference on AI Engineering-Software Engineering for AI (CAIN)', IEEE, pp. 7-12.
* [53] Park, J., Shalit, U., Scholkopf, B. and Muandet, K. [2021], Conditional distributional treatment effect with kernel conditional mean embeddings and u-statistic regression, _in_ 'International Conference on Machine Learning', PMLR, pp. 8401-8412.
* [54] Pearl, J. [2009], _CAUSALITY: Models, Reasoning, and Inference_, 2nd edn, Cambridge University Press.
* [55] Perry, R., Von Kugelgen, J. and Scholkopf, B. [2022], 'Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis', _arXiv preprint arXiv:2206.02013_.
* [56] Peters, J. and Buhlmann, P. [2014], 'Identifiability of gaussian structural equation models with equal error variances', _Biometrika_**101**(1), 219-228.
* [57] Peters, J., Buhlmann, P. and Meinshausen, N. [2016], 'Causal inference by using invariant prediction: identification and confidence intervals', _Journal of the Royal Statistical Society. Series B (Statistical Methodology)_ pp. 947-1012.
* [58] Peters, J., Janzing, D. and Scholkopf, B. [2017], _Elements of causal inference: foundations and learning algorithms_, The MIT Press.
* [59] Peters, J., Mooij, J. M., Janzing, D. and Scholkopf, B. [2014], 'Causal discovery with continuous additive noise models'.
* [60] Pimanda, J. E., Ottersbach, K., Knezevic, K., Kinston, S., Chan, W. Y., Wilson, N. K., Landry, J.-R., Wood, A. D., Kolb-Kokocinski, A., Green, A. R. et al. [2007], 'Gata2, fli1, and scl form a recursively wired gene-regulatory circuit during early hematopoietic development', _Proceedings of the National Academy of Sciences_**104**(45), 17692-17697.
* [61] Plis, S. M., Calhoun, V. D., Weisend, M. P., Eichele, T. and Lane, T. [2010], 'Meg and fmri fusion for non-linear estimation of neural and bold signal changes', _Frontiers in neuroinformatics_**4**, 114.
* [62] Plis, S. M., Weisend, M. P., Damaraju, E., Eichele, T., Mayer, A., Clark, V. P., Lane, T. and Calhoun, V. D. [2011], 'Effective connectivity analysis of fmri and meg data collected under identical paradigms', _Computers in biology and medicine_**41**(12), 1156-1165.
* [63] Racine, J. [1997], 'Consistent significance testing for nonparametric regression', _Journal of Business & Economic Statistics_**15**(3), 369-378.

* Robins et al. [2000] Robins, J. M., Hernan, M. A. and Brumback, B. [2000], 'Marginal structural models and causal inference in epidemiology', _Epidemiology_ pp. 550-560.
* Rolland et al. [2022] Rolland, P., Cevher, V., Kleindessner, M., Russell, C., Janzing, D., Scholkopf, B. and Locatello, F. [2022], Score matching enables causal discovery of nonlinear additive noise models, _in_ 'International Conference on Machine Learning', PMLR, pp. 18741-18753.
* Sachs et al. [2005] Sachs, K., Perez, O., Pe'er, D., Lauffenburger, D. A. and Nolan, G. P. [2005], 'Causal protein-signaling networks derived from multiparameter single-cell data', _Science_**308**(5721), 523-529.
* Saeed et al. [2020] Saeed, B., Panigrahi, S. and Uhler, C. [2020], Causal structure discovery from distributions arising from mixtures of dags, _in_ 'International Conference on Machine Learning', PMLR, pp. 8336-8345.
* Sanchez et al. [2022] Sanchez, P., Liu, X., O'Neil, A. Q. and Tsaftaris, S. A. [2022], 'Diffusion models for causal discovery via topological ordering', _arXiv preprint arXiv:2210.06201_.
* Scholkopf et al. [2021] Scholkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A. and Bengio, Y. [2021], 'Toward causal representation learning', _Proceedings of the IEEE_**109**(5), 612-634.
* Shimizu et al. [2006] Shimizu, S., Hoyer, P. O., Hyvarinen, A., Kerminen, A. and Jordan, M. [2006], 'A linear non-gaussian acyclic model for causal discovery.', _Journal of Machine Learning Research_.
* Spirtes et al. [2000] Spirtes, P., Glymour, C. N., Scheines, R. and Heckerman, D. [2000], _Causation, prediction, and search_, MIT press.
* Squires et al. [2020] Squires, C., Wang, Y. and Uhler, C. [2020], Permutation-based causal structure learning with unknown intervention targets, _in_ 'Conference on Uncertainty in Artificial Intelligence', PMLR, pp. 1039-1048.
* Stein [1972] Stein, C. [1972], A bound for the error in the normal approximation to the distribution of a sum of dependent random variables, _in_ 'Proc. Sixth Berkeley Symp. Math. Stat. Prob.', pp. 583-602.
* Tanay et al. [2005] Tanay, A., Regev, A. and Shamir, R. [2005], 'Conservation and evolvability in regulatory networks: the evolution of ribosomal regulation in yeast', _Proceedings of the National Academy of Sciences_**102**(20), 7203-7208.
* Teyssier and Koller [2012] Teyssier, M. and Koller, D. [2012], 'Ordering-based search: A simple and effective algorithm for learning bayesian networks', _arXiv preprint arXiv:1207.1429_.
* Tsamardinos et al. [2006] Tsamardinos, I., Brown, L. E. and Aliferis, C. F. [2006], 'The max-min hill-climbing Bayesian network structure learning algorithm', _Machine Learning_**65**(1), 31-78.
* Uhler et al. [2013] Uhler, C., Raskutti, G., Buhlmann, P. and Yu, B. [2013], 'Geometry of the faithfulness assumption in causal inference', _The Annals of Statistics_ pp. 436-463.
* Van de Geer and Buhlmann [2013] Van de Geer, S. and Buhlmann, P. [2013], '\(\ell_{0}\)-penalized maximum likelihood for sparse directed acyclic graphs', _The Annals of Statistics_**41**(2), 536-567.
* Varici et al. [2021] Varici, B., Shanmugam, K., Sattigeri, P. and Tajer, A. [2021], 'Scalable intervention target estimation in linear models', _Advances in Neural Information Processing Systems_**34**, 1494-1505.
* Voorman et al. [2014] Voorman, A., Shojaie, A. and Witten, D. [2014], 'Graph estimation with joint additive models', _Biometrika_**101**(1), 85-101.
* Vucic and Fairbrother [2007] Vucic, D. and Fairbrother, W. J. [2007], 'The inhibitor of apoptosis proteins as therapeutic targets in cancer', _Clinical cancer research_**13**(20), 5995-6000.
* Wang et al. [2018] Wang, Y., Squires, C., Belyaeva, A. and Uhler, C. [2018], 'Direct estimation of differences in causal graphs', _Advances in neural information processing systems_**31**.
* Yang et al. [2018] Yang, K., Katcoff, A. and Uhler, C. [2018], Characterizing and learning equivalence classes of causal dags under interventions, _in_ 'International Conference on Machine Learning', PMLR, pp. 5541-5550.
* Yoon et al. [2018] Yoon, H., Jang, H., Kim, E.-Y., Moon, S., Lee, S., Cho, M., Cho, H. J., Ko, J. J., Chang, E. M., Lee, K.-A. et al. [2018], 'Knockdown of prkar2b results in the failure of oocyte maturation', _Cellular Physiology and Biochemistry_**45**(5), 2009-2020.
* Yuan et al. [2017] Yuan, H., Xi, R., Chen, C. and Deng, M. [2017], 'Differential network analysis via lasso penalized D-trace loss', _Biometrika_.

* [86] Zhang, K., Peters, J., Janzing, D. and Scholkopf, B. [2012], 'Kernel-based conditional independence test and application in causal discovery', _arXiv preprint arXiv:1202.3775_.
* [87] Zhao, B., Wang, Y. S. and Kolar, M. [2020], 'Fudge: Functional differential graph estimation with fully and discretely observed curves', _arXiv preprint arXiv:2003.05402_.
* [88] Zhao, S. D., Cai, T. T. and Li, H. [2014], 'Direct estimation of differential networks', _Biometrika_**101**(2), 253-268.
* [89] Zheng, X., Aragam, B., Ravikumar, P. K. and Xing, E. P. [2018], 'Dags with no tears: Continuous optimization for structure learning', _NeurIPS_.

## Appendix A Practical Implementation

In this section, we present a more practical version of Alg. 1 that considers estimation errors, see Alg. 3. First, we provide more details of the score's Jacobian estimation.

### Practical Version of SCORE

Let \(\bm{X}=\{x^{1},\ldots,x^{m}\}\) be a dataset of \(m\) possibly non-independent but identically distributed samples. From Li and Turner [45], we next present the estimator for the point-wise first-order partial derivative, corresponding to eq.(4):

\[\bm{\hat{G}}=-(\bm{K}+\eta\bm{I})^{-1}\langle\nabla,\bm{K}\rangle\] (6)

where \(\bm{H}=(h(x^{1}),\ldots,h(x^{m}))\in\mathbb{R}^{d^{\prime}\times m}\), \(\overline{\nabla\bm{h}}=\frac{1}{m}\sum_{k=1}^{m}\nabla\bm{h}(x^{k})\), \(\bm{K}=\bm{H}^{\top}\bm{H}\), \(K_{ij}=\kappa(x^{i},x^{j})=\bm{h}(x^{i})^{\top}\bm{h}(x^{j}),\langle\nabla,\bm {K}\rangle=m\bm{H}^{T}\overline{\nabla\bm{h}}\), \(\langle\nabla,\bm{K}\rangle_{ij}=\sum_{k=1}^{m}\nabla_{x^{k}_{j}}(x^{i},x^{j})\), and \(\eta\geq 0\) is a regularization parameter. Here \(\bm{\hat{G}}\) is used to approximate \(\bm{G}\equiv(\nabla\log p(x^{1}),\ldots,\nabla\log p(x^{m}))^{\top}\in\mathbb{ R}^{m\times d}\).

From [65], we now present the estimator for the diagonal elements of the score's Jacobian at the sample points, i.e. \(\bm{J}\equiv(\mathrm{diag}(\nabla^{2}\log p(x^{1})),\ldots(\mathrm{diag}( \nabla^{2}\log p(x^{m})))^{\top}\in\mathbb{R}^{m\times d}\), the estimator of \(\bm{J}\) is:

\[\bm{\hat{J}}=-\mathrm{diag}\left(\bm{\hat{G}}\bm{\hat{G}}^{\top}\right)+(\bm {K}+\eta\bm{I})^{-1}\langle\nabla^{2}_{\mathrm{diag}},\bm{K}\rangle\] (7)

where \(\bm{H}=(h(x^{1}),\ldots,h(x^{m}))\in\mathbb{R}^{d^{\prime}\times m}\), \(\overline{\nabla^{2}_{\mathrm{diag}}\bm{h}}=\frac{1}{m}\sum_{k=1}^{m}\nabla^{ 2}_{\mathrm{diag}}\bm{h}(x^{k})\), \((\nabla^{2}_{\mathrm{diag}}\bm{h}(x))_{ij}=\frac{\partial^{2}h_{i}(x)}{ \partial x^{j}_{i}}\), \(\bm{K}=\bm{H}^{\top}\bm{H}\), \(\bm{K}_{ij}=\kappa(x^{i},x^{j})=\bm{h}(x^{i})^{\top}\bm{h}(x^{j}),\langle\nabla ^{2}_{\mathrm{diag}},\bm{K}\rangle=m\bm{H}^{T}\overline{\nabla^{2}_{\mathrm{ diag}}\bm{h}}\), \(\langle\nabla^{2}_{\mathrm{diag}},\bm{K}\rangle_{ij}=\sum_{k=1}^{m}\frac{ \partial^{2}\kappa(x^{i},x^{k})}{(\partial x^{j}_{i})^{2}}\), and \(\eta\geq 0\) is a regularization parameter.

In the sequel, we use \(\mathsf{SCORE}(\bm{X})\) to denote the procedure to compute the sample variance for the estimator of the diagonal of the score's Jacobian via eq.(7).

### Practical Version of Algorithm 1

Let \(\widehat{\mathrm{Var}}^{h}\) be a \(d\)-dimensional vector, where \(d\) is the number of nodes. We introduce a \(d\)-dimensional vector \(\mathsf{rank}^{h}\), which represents the index of each element in \(\widehat{\mathrm{Var}}^{h}\) after a non-decreasing sorting. For example, if \(\widehat{\mathrm{Var}}^{h}=(5.2,3.1,4.5,1.6)\), then \(\mathsf{rank}^{h}=(3,1,2,0)\). Furthermore, we define a \(d\)-dimensional vector \(\mathsf{rank}\) as the element-wise summation of \(\mathsf{rank}^{h}\) over all \(h\in[H]\). In other words, \(\mathsf{rank}\) is calculated as \(\mathsf{rank}=\sum_{h\in[H]}\mathsf{rank}^{h}\).

Recall that in Section 3.1 we remarked that we leverage the \(\mathsf{SCORE}\) approach from Rolland et al. [65] for estimating \(\mathrm{diag}(\nabla^{2}\log p(x))\) at each data point. Recall also that our identifiability result (Theorem 1) depends on determining whether a leaf node has variance \(\mathrm{Var}_{q}(\frac{\partial s_{j}(x)}{\partial x_{j}})=0\). In practice, it is unrealistic to simply test for the equality \(\mathrm{Var}_{L}=0\) since \(\mathrm{Var}_{L}\) carries out errors due to finite samples. Instead, we define the following statistic for each estimated leaf node \(L\) (Line 10 in Algorithm 3):

\[\mathsf{stats}_{L}=\frac{\mathrm{Var}_{L}}{\min_{h}\mathrm{Var}_{L}^{h}+ \epsilon}.\] (8)

[MISSING_PAGE_FAIL:17]

## Appendix B Detailed Proofs

### Proof of Theorem 1

To prove Theorem 1 we will make use of the following lemmas.

**Lemma 1**.: _Let \(\{a_{h}\}_{h=1}^{H}\) and \(\{b_{h}\}_{h=1}^{H}\) be two sequences of real numbers, where \(a_{h}>0,\forall h\). Then we have:_

\[\left(\sum_{h=1}^{H}a_{h}b_{h}^{2}\right)\left(\sum_{h=1}^{H}a_{h}\right)- \left(\sum_{h=1}^{H}a_{h}b_{h}\right)^{2}\geq 0,\]

_with equality if and only if \(b_{i}=b_{j},\forall j\neq i\in[H]\)._

Proof.: We can invoke the Cauchy-Schwarz inequality with vectors \(\bm{u}=(\sqrt{a_{1}},\ldots,\sqrt{a_{H}})\), and \(\bm{v}=(b_{1}\sqrt{a_{1}},\ldots,b_{H}\sqrt{a_{H}})\), then we have:

\[(\bm{u}^{\top}\bm{v})^{2}\leq\|\bm{u}\|_{2}^{2}\|\bm{v}\|_{2}^{2},\]

which proves the inequality. The equality holds if and only if \(\bm{u}\) and \(\bm{v}\) are linearly dependent, i.e., when \(b_{i}=b_{j}\) for all \(i\neq j\in[H]\). 

**Lemma 2**.: _For any \(j\), if \(\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}_{j}^{h})=\mathbb{P}^{h^{\prime}}(X_{j}\mid \mathrm{PA}_{j}^{h^{\prime}})\), then \(c_{j}^{h}=c_{j}^{h^{\prime}}\)._

Proof.: Denote the associated density of \(\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}_{j}^{h})\) when \(X_{j}=x_{j}\) as \(p_{N_{j}}^{h}(x_{j}-f_{j}^{h}(\mathrm{PA}_{j}^{h}))\) and let \(u=x_{j}-f_{j}^{h}(\mathrm{PA}_{j}^{h})\)

\[\frac{\partial^{2}}{\partial(x_{j})^{2}}\log p_{N_{j}}^{h}(x_{j}-f _{j}^{h}(\mathrm{PA}_{j}^{h}))\] \[= \frac{\partial\log p_{N_{j}}^{h}(u)}{\partial u}\frac{\partial^{2 }u}{\partial(x_{j})^{2}}+\frac{\partial^{2}\log p_{N_{j}}^{h}(u)}{\partial u^ {2}}\left(\frac{\partial u}{\partial x_{j}}\right)^{2}\] \[= 0+c_{j}^{h}=c_{j}^{h}\]

where we use the fact that \(\frac{\partial u}{\partial x_{j}}=1,\frac{\partial^{2}u}{\partial(x_{j})^{2}}=0\). Then it immediate follows that if \(\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}_{j}^{h})=\mathbb{P}^{h^{\prime}}(X_{j}\mid \mathrm{PA}_{j}^{h^{\prime}})\), then \(c_{j}^{h}=c_{j}^{h^{\prime}}\) 

**Lemma 3**.: _For any \(j\), under Assumption B, \(\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}(\mathrm{PA}_ {j}^{h}))=\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h^{\prime}}(x_{j}-f _{j}^{h^{\prime}}(\mathrm{PA}_{j}^{h^{\prime}}))\) if and only if \(\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}_{j}^{h})=\mathbb{P}^{h^{\prime}}(X_{j}\mid \mathrm{PA}_{j}^{h^{\prime}})\), where \(p^{h}\) and \(p^{h^{\prime}}\) are the probability density functions corresponding to the probability measures \(\mathbb{P}^{h}\) and \(\mathbb{P}^{h^{\prime}}\) when \(X_{j}=x_{j}\)._

Figure 5: Statistic in eq.(8) for each node sorted in non-increasing order. In this case, node index 5 corresponds to the _elbow point_, allowing us to estimate nodes 5 and 8 as shifted nodes.

Proof.: Denote the associated density of \(\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}^{h}_{j})\) when \(X_{j}=x_{j}\) as \(p^{h}_{N_{j}}(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_{j}))\), we proceed as follows:

\[\frac{\partial}{\partial x_{j}}\log p^{h}_{N_{j}}(x_{j}-f^{h}_{j}( \mathrm{PA}^{h}_{j}))=\frac{\partial}{\partial x_{j}}\log p^{h^{\prime}}_{N_{j} }(x_{j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}))\] \[\iff \log p^{h}_{N_{j}}(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_{j}))=\log p^{h^ {\prime}}_{N_{j}}(x_{j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}))+const\] \[\iff p^{h}_{N_{j}}(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_{j}))=p^{h^{ \prime}}_{N_{j}}(x_{j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}))\cdot e ^{const}\] \[\Rightarrow \int_{\mathbb{R}}p^{h}_{N_{j}}(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_ {j}))\mathrm{d}x_{j}=e^{const}\cdot\int_{\mathbb{R}}p^{h^{\prime}}_{N_{j}}(x_{ j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}))\mathrm{d}x_{j}\] \[\Rightarrow \int_{\mathbb{R}}p^{h}_{N_{j}}(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_ {j}))\mathrm{d}(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_{j}))=e^{const}\cdot\int_{ \mathbb{R}}p^{h^{\prime}}_{N_{j}}(x_{j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{ \prime}}_{j}))\mathrm{d}(x_{j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}))\] \[\Rightarrow 1=1\cdot e^{const}\] \[\Rightarrow const=0\]

Here, \(const\) is a constant that is independent of \(x_{j}\). Integrating both sides with respect to \(x_{j}\) and using the fact that \(\int p^{h}(x)\mathrm{d}x=1\), we conclude that \(const=0\). Hence, we can establish the following:

\[\frac{\partial}{\partial x_{j}}\log p^{h}_{N_{j}}(x_{j}-f^{h}_{j} (\mathrm{PA}^{h}_{j}))=\frac{\partial}{\partial x_{j}}\log p^{h^{\prime}}_{N_{ j}}(x_{j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}))\] \[\iff p^{h}_{N_{j}}(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_{j}))=p^{h^{ \prime}}_{N_{j}}(x_{j}-f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}))\] \[\iff \mathbb{P}^{h}(X_{j}\mid\mathrm{PA}^{h}_{j})=\mathbb{P}^{h^{\prime }}(X_{j}\mid\mathrm{PA}^{h^{\prime}}_{j})\]

Proof of Theorem 1.: Let us first expand the log density of the mixture distribution:

\[\log q(x)=\log\left(\sum_{h=1}^{H}w_{h}p^{h}(x)\right)\]

Then, recall that \(s(x)=\nabla\log q(x)\), the \(j\)-entry reads:

\[s_{j}(x) =\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)} \left[\frac{\partial}{\partial x_{j}}\log p^{h}(x_{j}\mid\mathrm{PA}^{h}_{j})+ \sum_{i\in\mathrm{CH}^{h}_{j}}\frac{\partial}{\partial x_{j}}\log p^{h}(x_{i} \mid\mathrm{PA}^{h}_{i})\right]\] \[=\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)} \left[\frac{\partial}{\partial x_{j}}\log p^{h}_{N_{j}}\left(x_{j}-f^{h}_{j}( \mathrm{PA}^{h}_{j})\right)+\sum_{i\in\mathrm{CH}^{h}_{j}}\frac{\partial}{ \partial x_{j}}\log p^{h}_{N_{i}}\left(x_{i}-f^{h}_{i}(\mathrm{PA}^{h}_{i}) \right)\right]\] (9)

Condition (i).First we will prove condition (i). That is, given a leaf node \(X_{j}\) in all DAGs \(G^{h}\), \(X_{j}\) is not a shifted node (i.e. an invariant node) if and only if \(\mathrm{Var}(\frac{\partial s_{j}(x)}{\partial x_{j}})=0\).

If \(x_{j}\) is a leaf node in all the DAGs \(G^{h}\), then \(\mathrm{CH}^{h}_{j}=\emptyset,\forall h\in[H]\), and we can write eq.(9) as:

\[s_{j}(x)=\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)}\frac{ \partial}{\partial x_{j}}\log p^{h}_{N_{j}}\left(x_{j}-f^{h}_{j}(\mathrm{PA}^{h}_{ j})\right)\]

We use \(\mathrm{Den}(\frac{\partial s_{j}(x)}{\partial x_{j}})\) and \(\mathrm{Num}(\frac{\partial s_{j}(x)}{\partial x_{j}})\) to denote the denominator and numerator of \(\frac{\partial s_{j}(x)}{\partial x_{j}}\), respectively. Then we have:

\[\mathrm{Den}(\frac{\partial s_{j}(x)}{x_{j}})=\left(\sum_{k=1}^{H}w_{k}p^{k}(x) \right)^{2}\]\[\mathrm{Num}(\frac{\partial s_{j}(x)}{x_{j}}) =\left[\sum_{h=1}^{H}w_{h}p^{h}(x)\frac{\partial^{2}}{\partial x_{j} ^{2}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}(\mathrm{PA}_{j}^{h}))+w_{h}p^{h}(x) \left(\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}( \mathrm{PA}_{j}^{h}))\right)^{2}\right]\] \[\times\left[\sum_{k=1}^{H}w_{k}p^{k}(x)\right]-\left[\sum_{h=1}^{ H}w_{h}p^{h}(x)\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}( \mathrm{PA}_{j}^{h}))\right]^{2}\]

Now, dividing \(\mathrm{Num}(\frac{\partial s_{j}(x)}{\partial x_{j}})\) over \(\mathrm{Den}(\frac{\partial s_{j}(x)}{\partial x_{j}})\), we obtain:

\[\frac{\partial s_{j}(x)}{\partial x_{j}}=\frac{\mathrm{Num}( \frac{\partial s_{j}(x)}{x_{j}})}{\mathrm{Den}(\frac{\partial s_{j}(x)}{x_{j} })} =\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x) }\frac{\partial^{2}}{\partial x_{j}^{2}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}( \mathrm{PA}_{j}^{h}))\] \[+\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)} \left(\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}( \mathrm{PA}_{j}^{h}))\right)^{2}\] \[-\left[\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k} (x)}\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}(\mathrm{ PA}_{j}^{h}))\right]^{2}\] (10)

Note that since \(x_{j}\notin\mathrm{PA}_{j}^{h}\), the function \(f_{j}^{h}(\mathrm{PA}_{j}^{h})\) is independent of \(x_{j}\).

Let \(a_{h}=w_{h}p^{h}(x)\), and let \(b_{h}=\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}( \mathrm{PA}_{j}^{h}))\). Then, the last two summands of the RHS of eq.(10) can be written as:

\[\frac{1}{\left(\sum_{h=1}^{H}a_{h}\right)^{2}}\left[\left(\sum_{h=1}^{H}a_{h}b _{h}^{2}\right)\left(\sum_{h=1}^{H}a_{h}\right)-\left(\sum_{h=1}^{H}a_{h}b_{h} \right)^{2}\right]\geq 0,\] (11)

where the last inequality holds from Lemma 1. Then, by Lemma 3, we have that \(b_{h}=b_{h^{\prime}}\iff\mathbb{P}^{h}(X_{j}\mid\mathrm{PA}_{j}^{h})=\mathbb{ P}^{h^{\prime}}(X_{j}\mid\mathrm{PA}_{j}^{h^{\prime}})\) for all \(h,h^{\prime}\in[H]\). Then if \(b_{h}=b_{h^{\prime}}\) holds, by Lemma 2, we have \(c_{j}^{h}=c_{j}^{h^{\prime}}\coloneqq c_{j}\) and then the first term for eq.(10) boils down to a constant \(c_{j}\). Finally, from Lemma 1, we have that equality in eq.(11) holds if and only if \(b_{h}=b_{h^{\prime}}\) for all \(h,h^{\prime}\in[H]\). Thus, we conclude that:

If \(X_{j}\) is a leaf node for all \(G^{h}\), then \(X_{j}\) is not a shifted node \(\iff\frac{\partial s_{j}(x)}{\partial x_{j}}=c_{j}\),

where \(\frac{\partial s_{j}(x)}{\partial x_{j}}=c_{j}\) is equivalent to \(\mathrm{Var}_{q}(\frac{\partial s_{j}(x)}{\partial x_{j}})=0\).

Condition (ii).We now prove that if \(\mathrm{Var}_{q}(\frac{\partial s_{j}(x)}{\partial x_{j}})>0\), then only one of the following two cases holds: Case 1) \(X_{j}\) is a leaf node for all \(G^{h}\) and a shifted node. Case 2) \(X_{j}\) is not a leaf node in at least one DAG \(G^{h}\).

Case 1 follows immediately from the proof of condition (i) above.

For Case 2, we study whether there exists a non-leaf node \(X_{j}\) with \(\mathrm{Var}_{q}(\frac{\partial s_{j}(x)}{\partial x_{j}})=0\). Taking the partial derivative of \(s_{j}(x)\) in eq.(9) w.r.t. \(x_{j}\), we have:

\[\frac{\partial s_{j}(x)}{\partial x_{j}} =\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)} \left(\frac{\partial^{2}}{\partial x_{j}^{2}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h} (\mathrm{PA}_{j}^{h}))+\sum_{i\in\mathrm{CH}_{j}^{h}}\frac{\partial^{2}}{ \partial x_{j}^{2}}\log p_{N_{i}}^{h}(x_{i}-f_{i}^{h}(\mathrm{PA}_{i}^{h}))\right)\] \[+\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)} \left(\frac{\partial}{\partial x_{j}}\log p_{N_{j}}^{h}(x_{j}-f_{j}^{h}( \mathrm{PA}_{j}^{h}))+\sum_{i\in\mathrm{CH}_{j}^{h}}\frac{\partial}{\partial x_{j}} \log p_{N_{i}}^{h}(x_{i}-f_{i}^{h}(\mathrm{PA}_{i}^{h}))\right)^{2}\]\[-\left[\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)} \left(\frac{\partial}{\partial x_{j}}\log p^{h}_{N_{j}}(x_{j}-f^{h}_{j}(\mathrm{ PA}^{h}_{j}))+\sum_{i\in\mathrm{CH}^{h}_{j}}\frac{\partial}{\partial x_{j}}\log p^{h}_{N_{i}}(x _{i}-f^{h}_{i}(\mathrm{PA}^{h}_{i}))\right)\right]^{2}\]

By Assumptions B, we have \(\frac{\partial^{2}}{\partial x_{j}^{2}}\log p^{h}_{N_{j}}(x_{j}-f^{h}_{j}( \mathrm{PA}^{h}_{j}))=c^{h}_{j}\). For simplicity, let \(a_{h}=\frac{\partial}{\partial x_{j}}\log p^{h}_{N_{j}}(x_{j}-f^{h}_{j}( \mathrm{PA}^{h}_{j}))+\sum_{i\in\mathrm{CH}^{h}_{j}}\frac{\partial}{\partial x _{j}}\log p^{h}_{N_{i}}(x_{i}-f^{h}_{i}(\mathrm{PA}^{h}_{i}))\). Then, we have:

\[\frac{\partial s_{j}(x)}{\partial x_{j}}=\sum_{h=1}^{H}\frac{w_{ h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)}c^{h}_{j}+\underbrace{\sum_{h=1}^{H} \frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)}\sum_{i\in\mathrm{CH}^{h}_{j} }\frac{\partial^{2}}{\partial x_{j}^{2}}\log p^{h}_{N_{i}}(x_{i}-f^{h}_{i}( \mathrm{PA}^{h}_{i}))}_{\text{term 1}}\] \[+\underbrace{\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{ k}p^{k}(x)}a^{2}_{h}-\left(\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{ k}p^{k}(x)}a_{h}\right)^{2}}_{\text{term 2}}.\] (12)

We prove that \(\frac{\partial^{2}}{\partial x_{j}^{2}}\log p^{h}_{N_{i}}(x_{i}-f^{h}_{i}( \mathrm{PA}^{h}_{i}))\), is not constant under any circumstance, by contradiction. Let \(G^{h}\) be an environment's DAG where \(X_{j}\) is not a leaf, and let \(X_{u}\in\mathrm{CH}^{h}_{j}\) such that \(X_{u}\notin\cup_{i\in\mathrm{CH}^{h}_{j}}\mathrm{PA}^{h}_{i}\). Note that \(X_{\mathrm{u}}\) always exist since \(X_{j}\) is not a leaf, and it suffices to pick a child \(X_{u}\) appearing at the latest position in the topological order of \(G^{h}\). Now suppose that \(\frac{\partial^{2}}{\partial x_{j}^{2}}\log p^{h}_{N_{u}}(x_{u}-f^{h}_{u}( \mathrm{PA}^{h}_{u}))=a\), where \(a\) is a constant. Then we have:

\[\frac{\partial}{\partial x_{j}}\log p^{h}_{N_{u}}(x_{u}-f^{h}_{u} (\mathrm{PA}^{h}_{u})) =ax_{j}+g(x_{-j}),\] \[\frac{\partial}{\partial x_{j}}f^{h}_{u}(\mathrm{PA}^{h}_{u}) \cdot\frac{\partial}{\partial n_{u}}\log p^{h}_{N_{u}}(n_{u}) =ax_{j}+g(x_{-j}).\]

By deriving both sides w.r.t. \(x_{u}\), we obtain:

\[\frac{\partial}{\partial x_{j}}f^{h}_{u}(\mathrm{PA}^{h}_{u}) \cdot\frac{\partial^{2}}{\partial n_{u}^{2}}\log p^{h}_{N_{u}}(n_{u}) =\frac{\partial g(x_{-j})}{\partial x_{u}}\] \[\frac{\partial}{\partial x_{j}}f^{h}_{u}(\mathrm{PA}^{h}_{u}) \cdot c^{h}_{j} =\frac{\partial g(x_{-j})}{\partial x_{u}}.\]

Since the RHS does not depend on \(x_{j}\), then \(\frac{\partial f^{h}_{u}}{\partial x_{j}}\) cannot depend on \(x_{j}\) neither, implying that \(f^{h}_{u}\) is linear in \(x_{j}\), thus contradicting the non-linearity assumption (Assumption A). Consequently, it becomes evident that term 1 cannot be a constant, regardless of whether the node \(X_{j}\) has undergone a shift or not.

Now let us take a look to term 2 in eq.(12). We have:

\[\sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)}a^{2}_{h}-\left( \sum_{h=1}^{H}\frac{w_{h}p^{h}(x)}{\sum_{k=1}^{H}w_{k}p^{k}(x)}a_{h}\right)^{2} \geq 0,\]

where the inequality follows by Jensen's inequality. Thus we conclude that if \(X_{j}\) is a non-leaf node, we have \(\mathrm{Var}_{q}(\frac{\partial s_{j}(x)}{\partial x_{j}})>0\). 

### Proof of Theorem 2

To proof the theorem we will need the following assumptions:

**Assumption C**.: _Let \(\mathrm{MB}^{h}_{j}\) denote the Markov Blanket of node \(X_{j}\) under environment \(h\), then assume * _There are non-negative real number_ \(\beta\) _and_ \(C\) _such that for any subset_ \(X_{\bm{S}}\subseteq\mathrm{Pre}(X_{j}^{h})\) _of size_ \(\leq 1/\delta+2\)_, any_ \(x,x^{\prime}\in\mathbb{R}^{|X_{\bm{S}}|}\) _and any_ \(t\in\mathbb{R}\)_,_ \[|\operatorname{\mathbb{P}}(X_{j}^{h}\geq t\mid X_{\bm{S}}=x)- \operatorname{\mathbb{P}}(X_{j}^{h}\geq t\mid X_{\bm{S}}=x^{\prime})\mid\leq C (1+\|x\|^{\beta}+\|x^{\prime}\|^{\beta})\|x-x^{\prime}\|\] _where_ \(|X_{\bm{S}}|\) _is the size of the set_ \(X_{\bm{S}}\)_._
* _There are positive numbers_ \(C_{1}\) _and_ \(C_{2}\) _such that for any_ \(X_{\bm{S}}\) _of size_ \(\leq 1/\delta+2\) _and any_ \(t>0\)_,_ \(\operatorname{\mathbb{P}}(\|X_{\bm{S}}\|\geq t)\geq C_{1}e^{-C_{2}t}\)__
* _For any subset_ \(X_{\bm{S}}\subseteq\mathrm{Pre}(X_{j}^{h})\) _such that_ \(X_{\bm{S}}\subsetneq\mathrm{MB}_{j}^{h}\)_, there exists_ \(X_{i}\) _with_ \(X_{i}\in\mathrm{MB}_{j}^{h_{i}}\backslash X_{\bm{S}}\)_, such that for any_ \(X_{j}\) _with_ \(X_{j}\notin\mathrm{MB}_{j}^{h}\)_,_ \[Q(X_{\bm{S}}\cup\{X_{i}\})-Q(X_{\bm{S}}\cup\{X_{j}\})\geq\delta/4\qquad Q(X_{ \bm{S}})=\int\mathrm{Var}(\operatorname{\mathbb{P}}(X_{j}^{h}\geq t\mid X_{ \bm{S}}))d\mu(t)\]

_where \(\delta\) is the largest number such that for any subset \(X_{\bm{S}}\) from \(\mathrm{Pre}(X_{j}^{h})\), there is some \(X_{i}\notin X_{\bm{S}}\) such that \(Q(X_{\bm{S}}\cup\{X_{i}\})\geq Q(X_{\bm{S}})+\delta\)._

Proof.: Under Assumption C, from Theorem 3.1 in [4], we have

\[\operatorname{\mathbb{P}}(\widehat{\mathrm{MB}}_{j}^{h}=\mathrm{MB}_{j}^{h}) \geq 1-C_{3}e^{-C_{4}m}\]

where \(C_{3}\) and \(C_{4}\) are constants that depend only on the data generation process, and \(m\) is the number of samples. Since the estimated topological order \(\hat{\pi}\) is assumed to be valid for all environments, we can conclude that node \(X_{j}\) is a leaf node in the input data \(\{\mathrm{Pre}(X_{j}^{h}),X_{j}^{h}\}\) for all \(h\). As a result, we have \(\mathrm{MB}_{j}^{h}=\mathrm{PA}_{j}^{h}\) based on the Markov blanket definition. Therefore, the output of Algorithm 4 is equal to the true parent set \(\mathrm{PA}_{j}^{h}\) with high probability. 

### Proof of Theorem 3 in Appendix D

To prove Theorem 3 we will make use of the following lemmas.

**Lemma 4**.: _Suppose \(\bm{X}\) is an \(n\times k_{x}\) dimension matrix, \(\bm{Y}\) is an \(n\times k_{y}\) dimension matrix. Let the columns of the concatenated matrix \(\bm{Z}=(\bm{X},\bm{Y})\) be linearly independent. Consider \(\tilde{\beta}_{x}\), a \(k_{x}\)-dimensional vector, and \(\tilde{\beta}_{y}\) and \(\beta_{y}\), both \(k_{y}\)-dimensional vectors. If \(\bm{X}\tilde{\beta}_{x}+\bm{Y}\tilde{\beta}_{y}=\bm{Y}\beta_{y}\), then it follows that \(\tilde{\beta}_{y}=\beta_{y}\) and \(\tilde{\beta}_{x}=\bm{0}\)._

Proof.: \[\bm{X}\tilde{\beta}_{x}+\bm{Y}\tilde{\beta}_{y}-\bm{Y}\beta_{y}=(\bm{X},\bm{Y })\begin{pmatrix}\tilde{\beta}_{x}\\ \tilde{\beta}_{y}\end{pmatrix}-(\bm{X},\bm{Y})\begin{pmatrix}\bm{0}_{k_{x}}\\ \beta_{y}\end{pmatrix}=\bm{Z}\begin{pmatrix}\tilde{\beta}_{x}\\ \tilde{\beta}_{y}-\beta_{y}\end{pmatrix}=0\]

Since \(\bm{Z}\) has full column rank, then the null space of \(\bm{Z}\) is \(\bm{0}\), which implies \(\tilde{\beta}_{x}=\bm{0}\), \(\tilde{\beta}_{y}=\beta_{y}\). 

**Lemma 5**.: _For any \(h\in[H]\), if_

\[\sum_{k\in\mathrm{Pre}(X_{j})}\Psi_{jk}\tilde{\beta}_{jk}^{h}=\sum_{k\in \mathrm{PA}_{j}^{h}}\Psi_{jk}\beta_{jk}^{h},\]

_then \(\tilde{\beta}_{jk}^{h}=\beta_{jk}^{h}\) if \(k\in\mathrm{PA}_{j}^{h}\), and \(\tilde{\beta}_{jk}^{h}=0\) if \(k\notin\mathrm{PA}_{j}^{h}\)._

Proof.: Rearrange the set \(\mathrm{Pre}(X_{j})\) so that \(\mathrm{Pre}(X_{j})=\{X_{k_{1}},X_{k_{2}},\ldots,X_{k_{m}},X_{k_{m+1}},\ldots,X_ {k_{p}}\}\), where \(\{X_{k_{1}},\ldots,X_{k_{m}}\}=\mathrm{Pre}(X_{j})\setminus\mathrm{PA}_{j}\), and \(\{X_{k_{m+1}},\ldots,X_{k_{p}}\}=\mathrm{PA}_{j}\). Then let \(\bm{X}=(\Psi_{k_{1}},\ldots,\Psi_{k_{m}})\), \(\bm{Y}=(\Psi_{k_{m+1}},\ldots,\Psi_{k_{p}})\), and \(\bm{Z}=(\bm{X},\bm{Y})\). By the linear independence property of the basis functions of \(\mathrm{Pre}(X_{j})\), we have that the columns of \(\bm{Z}\) are linearly independent. Also, let

\[\tilde{\beta}_{x}=\begin{pmatrix}\tilde{\beta}_{jk_{1}}^{h}\\ \vdots\\ \tilde{\beta}_{jk_{m}}^{h}\end{pmatrix},\quad\tilde{\beta}_{y}=\begin{pmatrix} \tilde{\beta}_{jk_{m+1}}^{h}\\ \vdots\\ \tilde{\beta}_{jk_{p}}^{h}\end{pmatrix},\quad\beta_{y}=\begin{pmatrix}\beta_{jk_{m+ 1}}^{h}\\ \vdots\\ \beta_{jk_{p}}^{h}\end{pmatrix}\]Then we must have:

\[\sum_{k\in\operatorname{Pre}(X_{j})}\Psi_{jk}\tilde{\beta}_{jk}^{h}= \sum_{k\in\operatorname{PA}_{j}^{h}}\Psi_{jk}\beta_{jk}^{h}\quad\Rightarrow \quad\bm{X}\tilde{\beta}_{x}+\bm{Y}\tilde{\beta}_{y}=\bm{Y}\beta_{y}.\]

Then by Lemma 4, we have \(\tilde{\beta}_{jk}^{h}=\beta_{jk}^{h}\) if \(k\in\operatorname{PA}_{j}^{h}\), \(\tilde{\beta}_{jk}^{h}=0\) if \(k\notin\operatorname{PA}_{j}^{h}\). 

Proof of Theorem 3.: We know that \(\operatorname{Pre}(X_{j})\) contains the ancestors of \(X_{j}\). Then, in environment \(h\), we have:

\[\mathbb{E}_{p^{h}}[X_{j}\mid\operatorname{Pre}(X_{j})] =\mathbb{E}_{p^{h}}[f_{j}^{h}(\operatorname{PA}_{j}^{h})\mid \operatorname{Pre}(X_{j})]+\mathbb{E}_{p^{h}}[N_{j}\mid\operatorname{Pre}(X_{j })]\] \[=f_{j}^{h}(\operatorname{PA}_{j}^{h}),\]

where the last equality follows since the first conditional expectation is equal to \(f_{j}^{h}(\operatorname{PA}_{j})\), due to \(\operatorname{PA}_{j}\subseteq\operatorname{Pre}(X_{j})\). Moreover, in the second conditional expectation term, we have that \(N_{j}\) is marginally independent of \(\operatorname{Pre}(X_{j})\) by the d-separation criterion. Thus the conditional expectation of \(N_{j}\) equals to the marginal expectation of \(N_{j}\), which is 0. Finally,

\[\sum_{k\in\operatorname{Pre}(X_{j})}\Psi_{jk}\tilde{\beta}_{jk}^{h}=\mathbb{E} _{p^{h}}[X_{j}\mid\operatorname{Pre}(X_{j})]=f_{j}^{h}(\operatorname{PA}_{j}^{ h})=\sum_{k\in\operatorname{PA}_{j}^{h}}\Psi_{jk}\beta_{jk}^{h}\]

By Lemma 5, we have \(\tilde{\beta}_{jk}^{h}=\beta_{jk}^{h}\) if \(k\in\operatorname{PA}_{j}^{h}\), \(\tilde{\beta}_{jk}^{h}=0\) if \(k\notin\operatorname{PA}_{j}^{h}\). 

## Appendix C Additional Experiments

This section provides a thorough evaluation of the pipeline of our method. We begin by assessing the performance of our method in detecting the shifted nodes. Subsequently, we extend the evaluation to include the recovery of the structurally shifted edges.

### Experiments on detecting shifted nodes

**Graph models.** We ran experiments by generating adjacency matrices using the Erdos-Renyi (ER) and Scale free (SF) graph models. For a given number of variables \(d\), ER\(k\) and SF\(k\) indicate an average number of edges equal to \(kd\).

**Data generation process.** We first sampled a Directed Acyclic Graph (DAG) according to either the Erdos-Renyi (ER) model or the Scale-Free (SF) model for environment \(\mathcal{E}_{1}\).

For environment \(\mathcal{E}_{2}\), we used the same DAG structure as in environment \(\mathcal{E}_{1}\), ensuring a direct comparison between the two environments. To introduce artificial shifted nodes, we randomly selected \(0.2\cdot d\) nodes from the non-root nodes, where \(d\) represents the total number of nodes in the DAG. These selected nodes were considered as the "shifted nodes," denoted as \(S\), with \(|S|=0.2d\).

The functional relationship between a node \(X_{j}\) and its parents in environment \(\mathcal{E}_{1}\) was defined as follows:

\[X_{j}=\sum_{i\in\operatorname{PA}_{j}}\sin(X_{i}^{2})+N_{j},\]

while for environment \(\mathcal{E}_{2}\), we defined the functional relationships between each node and its parents by:

\[X_{j}=\begin{cases}\sum_{i\in\operatorname{PA}_{j}}\sin(X_{i}^{2} )+N_{j},&\text{if}\quad X_{j}\notin S,\\ \sum_{i\in\operatorname{PA}_{j}}4\cos(2X_{i}^{2}-3X_{i})+N_{j},&\text{if} \quad X_{j}\in S.\end{cases}\]

**Experiment detail.** In each simulation, we generated 500 data points, with the variances of the noise set to 1. We conducted 30 simulations for each combination of graph type, noise type, and number of nodes. The running time was recorded by executing the experiments on an Intel Xeon Gold 6248R Processor with 8 cores. For our method, we used the hyperparameters eta_G = \(0.005\), eta_H = \(0.005\), and threshold \(t=2\) (see Algorithm 3).

[MISSING_PAGE_EMPTY:24]

Figure 8: Shifted nodes detection in ER6 and SF6 graphs. For each point, we conducted 30 simulations as described in Section C.1. The points indicate the average values obtained from these simulations, while the error bars depict the standard errors. For each simulation, 500 samples were generated. Our method iSCAN (green) consistently outperformed DCI (red) in terms of F1 score, precision, and recall.

Figure 7: Shifted nodes detection in ER4 and SF4 graphs. For each point, we conducted 30 simulations as described in Section C.1. The points indicate the average values obtained from these simulations, while the error bars depict the standard errors. For each simulation, 500 samples were generated. Our method iSCAN (green) consistently outperformed DCI (red) in terms of F1 score, precision, and recall.

#### c.1.1 Experiments in detecting shifted nodes from Gaussian process

**Data generation process.** We first sampled a DAG according to the ER or SF model. In our experiment, we considered two environments, \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\), with the same DAG structure. Each node in the graph had a functional relationship with its parents defined as \(X_{j}=f_{j}^{h}(\mathrm{PA}_{j}^{h})+N_{j}\), where \(N_{j}\) is an independent standard Gaussian variable. Recall that the superscript \(h\) denotes the function for environment \(\mathcal{E}_{h}\).

To introduce shifted nodes, we randomly selected \(0.2\cdot d\) nodes from the non-root nodes, denoted as \(S\), to be the shifted nodes. In other words, \(|S|=0.2d\). For the non-shifted nodes \(X_{j}\) (i.e \(j\notin S\)), we set \(f_{j}^{1}=f_{j}^{2}\). However, for each shifted node \(X_{j}\) in \(S\), we changed its functional relationship with its parents to \(X_{j}=2\cdot f_{j}^{2}(\mathrm{PA}_{j}^{2})+N_{j}\).

To test our method in a more general setting involving nonlinear functions, we followed the approach in [65, 39]. Specifically, for _non-shifted nodes_, we generated the link functions \(f_{j}^{1}\) by sampling Gaussian processes with a half unit bandwidth RBF kernel, and we set \(f_{j}^{2}=f_{j}^{1}\). _For shifted nodes_, \(X_{j}\in S\), we generated the link functions \(f_{j}^{1}\) and \(f_{j}^{2}\) by sampling Gaussian processes with a half unit bandwidth RBF kernel independently. This allowed us to simulate different functional relationships for the shifted nodes across the two environments.

**Experiment detail.** In each simulation, we generated 1000 data points, with the variances of the noise set to 1. We conducted 30 simulations for each combination of graph type, noise type, and number of nodes. The running time was recorded by executing the experiments on an Intel Xeon Gold 6248R Processor with 8 cores. For our method, we used the hyperparameters eta_G \(=0.005\), eta_H \(=0.005\), and elbow \(=\) True (see Remark 6).

**Evaluation.** We conducted a comparative performance analysis between our proposed Algorithm 1 (iSCAN, green) and the DCI (red) method. The results for ER2 and SF2 graphs under Gaussian, Gumbel, and Laplace noise distributions are shown in Figure 9. In certain cases, our method may underperform DCI in terms of precision, resulting in a lower F1 score. However, it is important to note that our method consistently outperforms DCI in terms of recall score.

Furthermore, Figure 10 and Figure 11 present the results for ER4/SF4, and ER6/SF6 graphs. In terms of precision, our method exhibits competitive performance and, in many cases, outperforms DCI. Notably, iSCAN consistently surpasses DCI in terms of recall score and F1 score.

These findings emphasize the strengths of our proposed method in accurately detecting shifted nodes and edges, particularly in terms of recall and overall performance. In denser graphs, our method demonstrates a superior ability to recover shifted nodes compared to DCI. This suggests that our method is well-suited for scenarios where the graph structure is more complex and contains a larger number of nodes and edges. The improved performance of our method in such settings further highlights its potential in practical applications and its ability to handle more challenging tasks.

Top-k precision.We have observed that in some cases, the precision of our method underperformed DCI. We attribute this to the elbow method rather than stats\({}_{L}\). To further investigate this, we conducted an analysis using only stats\({}_{L}\) and measured the precision based on different criteria. Specifically, we identified nodes as shifted if their stats\({}_{L}\) ranked first, first two, or within the top k, denoted as top-1 precision, top-2 precision, and top-k precision, respectively, where \(k=|S|\).

Figure 12 presents the results of precision for top-1, top-2, and top-k criteria under various graph models and noise combinations. In most cases, the precision exceeds \(80\%\) and even approaches \(100\%\). These results indicate that when using stats\({}_{L}\) alone, our method still provides accurate information about shifted nodes. The findings suggest that the lower precision observed in Figure 9 can be attributed to the elbow strategy rather than the effectiveness of stats\({}_{L}\). Overall, this analysis strengthens the reliability and usefulness of stats\({}_{L}\) in accurately identifying shifted nodes in our method.

Figure 10: Experiments on detection of shifted nodes in ER4/SF4 graphs using Gaussian processes. Details described in Appendix C.1.1. The error bars represent the standard errors.

Figure 9: Experiments on detection of shifted nodes in ER2/SF2 graphs using Gaussian processes. Details described in Appendix C.1.1. The error bars represent the standard errors.

Figure 11: Experiments on detection of shifted nodes in ER6/SF6 graphs using Gaussian processes. Details described in Appendix C.1.1. The error bars represent the standard errors.

Figure 12: Top 1, 2 and K performance of iSCAN where functionals are sampled from Gaussian processes. Details described in Appendix C.1.1. The error bars represent the standard errors.

### Experiments on estimating structural shifts

**Data generation.** We first sampled a DAG, \(G^{1}\), of \(d\) nodes according to either the ER or SF model for env. \(\mathcal{E}_{1}\). For env. \(\mathcal{E}_{2}\), we initialized its DAG structure from env. \(\mathcal{E}_{1}\) and produced structural changes by randomly selecting \(0.2\cdot d\) nodes from the non-root nodes. This set of selected nodes \(S\), with cardinality \(|S|=0.2d\), correspond to the set of "shifted nodes". In env. \(\mathcal{E}_{2}\), for each shifted node \(X_{j}\in S\), we uniformly at random deleted at most three of its incoming edges, and use \(D_{j}\) to denote the parents whose edges to \(X_{j}\) were deleted; thus, the DAG \(G^{2}\) is a subgraph of \(G^{1}\). Then, in \(\mathcal{E}_{1}\), each \(X_{j}\) was defined as follows:

\[X_{j}=\sum_{i\in\mathrm{PA}_{j}^{1}\setminus D_{j}}\sin(X_{i}^{2})+\sum_{i\in D _{j}}4\cos(2X_{i}^{2}-3X_{i})+N_{j}\]

In \(\mathcal{E}_{2}\), each \(X_{j}\) was defined as follows:

\[X_{j}=\sum_{i\in\mathrm{PA}_{j}^{2}}\sin(X_{i}^{2})+N_{j}\]

**Experiment details.** For each simulation, we generated \(500\) data points per environment, i.e., \(m_{1}=500,m_{2}=500\) and \(m=1000\). The noise variances were set to 1. We conducted 30 simulations for each combination of graph type (ER or SF), noise type (Gaussian, Gumbel, and Laplace), and number of nodes (\(d\in\{10,20,30,50\}\)). The running time was recorded by executing the experiments on an Intel Xeon Gold 6248R Processor with 8 cores. For our method, we used \(\eta=0.05\) for eq.(6) and eq.(7), and a threshold \(t=2\) (see Alg. 3).

In the case of the method introduced by Budhathoki et al. [11], we employed Kernel Conditional Independence (KCI) tests [86] for conducting conditional independence tests. As for CITE, KCD, UT-IGSP, and SCORE, we used their respective default parameter settings provided within their packages. Additionally, for SCORE, we employed it to estimate the DAGs independently for different environments and then compared the recovered DAGs to identify the shifted nodes. Given that Budhathoki's and KCD methods require information about the parents \(\mathrm{PA}_{j}\) for each node \(X_{j}\), we employed the SCORE method to find the parent sets \(\mathrm{PA}_{j}\).

**Evaluation.** In this experiment, we assessed the performance of our method in two aspects: detecting shifted nodes and recovering the strucutral changes (**difference DAG**). For the evaluation of shifted node detection, we measured F1 score, recall, and precision. In the evaluation of difference DAG recovery, we compared the estimated difference DAG with the ground truth difference DAG using F1 score. Additionally, we considered the running time of the methods as another evaluation criterion.

Figures 13, 14, and 15 illustrate our method's performance in detecting shifted nodes across varying numbers of nodes and sparsity levels of the graphs. Our method consistently outperformed baselines in terms of F1 score, precision, and recall.

Figures 16 showcase the performance of our method in recovering difference DAG across different noise distribution, different numbers of nodes and sparsity levels in the graphs. Our method achieves higher F1 score in recovering the difference DAG compared with DCI.

Figure 14: Shifted nodes detection performance in ER4/SF4. See App. C.2 for experimental details. iSCAN (light blue) consistently outperformed baselines in terms of F1 score, precision, and recall.

Figure 13: Shifted nodes detection performance in ER2/SF2. See App. C.2 for experimental details. iSCAN (light blue) consistently outperformed baselines in terms of F1 score, precision, and recall.

Figure 16: Difference DAG recovery performance in all different graphs. iSCAN-FOCI (green) consistently outperformed DCI (red) in terms of F1 score.

Figure 15: Shifted nodes detection performance in ER6/SF6. See App. C.2 for experimental details. iSCAN consistently outperformed baselines in terms of F1 score, precision, and recall.

### Performance of Alg. 3 using the elbow method

In this section, we aim to understand the performance of our method when using the elbow approach discussed in Remark 6, random functions for shifted nodes, and different noise variances per variable within an environment.

**Data generation process.** We first sampled a Directed Acyclic Graph (DAG) according to either the Erdos-Renyi (ER) model or the Scale-Free (SF) model for environment \(\mathcal{E}_{1}\).

For environment \(\mathcal{E}_{2}\), we used the same DAG structure as in environment \(\mathcal{E}_{1}\), ensuring a direct comparison between the two environments. To introduce artificial shifted edges, we randomly selected \(0.2\cdot d\) nodes from the non-root nodes, where \(d\) represents the total number of nodes in the DAG. These selected nodes correspond to shifted nodes, denoted as \(S\), with \(|S|=0.2d\). For each shifted node \(X_{j}\in S\), we uniformly and randomly deleted \(3\) edges originating from its parents for environment \(\mathcal{E}_{2}\). The parent nodes whose edges to \(X_{j}\) were deleted are denoted as \(D_{j}\).

The functional relationship between shifted node \(X_{j}\) and its parents \(D_{j}\) in environment \(\mathcal{E}_{1}\) was defined as follows:

\[X_{j}=\sum_{i\in\mathrm{PA}_{j},i\notin D_{j}}\sin(X_{i}^{2})+\sum_{i\in D_{j }}c_{ij}\cdot f_{ij}(-2X_{i}^{3}+3X_{i}^{2}+4X_{i})+N_{j},\]

where \(c_{ij}\sim\mathrm{Uniform}([-5,-2]\cup[2,5])\), and \(f_{ij}\) is a function from \(\{\mathrm{sinc}(\cdot),\mathrm{cos}(\cdot)\}\) chosen uniformly at random. For environment \(\mathcal{E}_{2}\), where the adjacency matrix has undergone deletions, we defined the functional relationship between each node and its parents as follows:

\[X_{j}=\sum_{i\in\mathrm{PA}_{j}}\sin(X_{i}^{2})+N_{j}\]

**Experiment detail.** In each simulation, we generated \(\{500,1000\}\) data points, with the variances of the noises set uniformly at random in \([0.25,0.5]\). We tested three types of noise distributions, namely, the Normal, Laplace, and Gumbel distributions. We conducted a 100 simulations for each combination of graph type, noise type, and number of nodes. The running time was recorded by executing the experiments on an Intel Xeon Gold 6248R Processor with 8 cores. For our method, we used the hyperparameter \(\eta=0.001\). Different from the hard threshold of \(t=2\) used in previous experiments, we now used the elbow approach to determine the set of shifted nodes. To automatically select the elbow we made use of the Python package Kneed7, with hyperparameters curve='convex', direction='decreasing', online=online, interp_method='interpld'.

Footnote 7: We used the latest version found at: https://kneed.readthedocs.io/en/stable/.

**Evaluation.** In this experiment, we assessed the performance of our method in two aspects: detecting shifted nodes and recovering the difference DAG. For the evaluation of shifted node detection, we measured F1 score, recall, and precision. In the evaluation of difference DAG recovery, we compared the estimated difference DAG with the ground truth difference DAG using F1 score.

In Figures 17 and 18 we present the performances when using the elbow approach discussed in Remark 6. In Figure 17, we note that iSCAN performs similarly for number of samples \(500\) and \(1000\). We also show the top-1, top-2, and top-k precision of iSCAN when choosing the first, first 2, and first k variables of stats (see Algorithm 3) after sorting in decreasing order, respectively. We remark that the superbly performance of iSCAN in top-1 or top-2 precision suggests that in situations that is difficult to choose a threshold for Algorithm 3, the practioner can consider that the first or first two variables of stats are more likely to be shifted nodes. Finally, in Figure 18 we show that iSCAN outperforms DCI in recovering the underlying structural difference.

## Appendix D

Figure 17: Shifted nodes detection performance in ERk and SFk for \(k\in\{2,4,6\}\). For each point in each subplot, we conducted 100 simulations as described in Section C.3. The points indicate the average values obtained from these simulations. The error bars depict the standard errors. Our method iSCAN (green) consistently outperformed DCI (red) in terms of F1 score, precision, and recall.

Figure 18: Difference DAG recovery performance in all different graphs. For each point in each subplot, we conducted 100 simulations as described in Section C.3. The points indicate the average values obtained from these simulations. The error bars depict the standard errors. Our method iSCAN with FOCI (green) consistently outperformed DCI (red) in terms of F1 score.

Additional Discussion on Shifted Edges

In Section 4, we focused on estimating structural changes across the environments (Definition 4). However, in some situations it might be of interest to determine whether the _functional relationship_ between two variables has changed across the environments. The latter could have multiple interpretations, in this section, we elaborate on a particular type of functional change via partial derivatives.

**Definition 5** (functionally shifted edge).: _Given environments \(\mathcal{E}=(X,f^{h},\mathbb{P}_{N}^{h})\) for \(h\in[H]\), an edge \((X_{i}\to X_{j})\) is called a functionally shifted edge if there exists \(h,h^{\prime}\in[H]\) such that:_

\[\frac{\partial}{\partial x_{i}}f^{h}_{j}(\mathrm{PA}^{h}_{j})\neq\frac{ \partial}{\partial x_{i}}f^{h^{\prime}}_{j}(\mathrm{PA}^{h^{\prime}}_{j}).\]

Without further assumptions about the functional form of \(f^{h}_{j}\), certain ill-posed situations may arise under Definition 5. Let us consider the following example.

**Example 1**.: _Let \(\mathcal{E}_{A}\) and \(\mathcal{E}_{B}\) be two environments, each consisting of three nodes. Let the structural equations for node \(X_{3}\) be: \(X_{3}^{A}=\exp\left(X_{1}^{A}+X_{2}^{A}\right)+N_{3}\), and \(X_{3}^{B}=\exp\left(2\cdot X_{1}^{B}+X_{2}^{B}\right)+N_{3}\). In this scenario, one could consider that the causal relationship \(X_{2}\to X_{3}\) has not changed. However, we note that \(\frac{\partial f^{A}_{j}}{\partial x_{i}^{A}}\neq\frac{\partial f^{B}_{j}}{ \partial x_{i}^{B}}\), thus, testing for changes in the partial derivative would yield a false discovery for the non-shifted edge \(X_{2}\to X_{3}\)._

Ill-posed situations such as the above example can be avoided by additional assumptions on the functional mechanisms. We next discuss a sufficient condition where the partial derivative test for functional changes is well-defined.

**Assumption D** (Additive Models).: _Let \(S\) be the set of shifted nodes across all the \(H\) environments. Then, for all \(j\in S,h\in[H]\):_

\[f^{h}_{j}(\mathrm{PA}^{h}_{j})=a^{h}_{j}+\sum_{k\in\mathrm{PA}^{h}_{j}}f^{h}_{ jk}(X_{k}),\]

_where \(a^{h}_{j}\) is a constant, \(f^{h}_{jk}\) is a nonlinear function, where \(f^{h}_{jk}(\cdot)\) lies in some space of function class \(\mathcal{F}\)._

**Remark 7**.: _Assumption D amounts to modelling each variable as a generalized linear model [27]. It is widely used in nonparametrics and causal discovery [12, 43, 80]. Moreover, it not only provides a practical framework but also makes the definition of shifted edges (as per Definition 5) well-defined and reasonable._

**Remark 8**.: _Note that Assumption D makes assumptions only on the set of shifted nodes. This is because the set of invariant nodes can be identified regardless of the their type of structural equation, and it is also clear these nodes cannot have any type of shift._

Now consider a function class \(\mathcal{F}\), which incorporates the use of basis functions to model the additive components \(f^{h}_{jk}\). Specifically, we express \(f^{h}_{jk}(x_{k})=\Psi^{h}_{jk}(x_{k})\beta^{h}_{jk}\), where feature mapping \(\Psi^{h}_{jk}\) is a \(1\times r\) matrix whose columns represent the basis functions and \(\beta^{h}_{jk}\) is an \(r\)-dimensional vector containing the corresponding coefficients. Moreover we assume that the functions \(f^{1}_{jk},\ldots,f^{H}_{jk}\) share a same feature mapping \(\Psi^{1}_{jk}(\cdot)=,\ldots,=\Psi^{H}_{jk}(\cdot)\) but can have different coefficients \(\beta^{h}_{jk}\) across the \(H\) environments. The latter has been assumed in prior work, e.g., [44]. The approach of using a basis function approximation is widely adopted in nonparametric analysis, and it has been successfully employed in various domains such as graph-based methods [80], and the popular CAM framework [12]. Then, under Assumption D and Definition 5, we present the following proposition:

**Proposition 2**.: _Under Assumption D, an edge \((X_{i}\to X_{j})\) is a functionally shifted edge, as in Definition 5, if and only if the basis coefficients are different. That is,_

\[\frac{\partial f^{h}_{j}}{\partial x_{i}}\neq\frac{\partial f^{h^{\prime}}_{ j}}{\partial x_{i}}\iff\beta^{h}_{ji}\neq\beta^{h^{\prime}}_{ji}.\]

Proof.: We have,

\[\frac{\partial f^{h}_{j}}{\partial x_{i}}=\frac{\mathrm{d}f^{h}_{ji}(x_{i})}{ \mathrm{d}x_{i}}=\frac{\mathrm{d}(\Psi_{ji}(x_{i})\beta^{h}_{ji})}{\mathrm{d}x _{i}}=\frac{\mathrm{d}\Psi_{ji}(x_{i})}{\mathrm{d}x_{i}}\beta^{h}_{ji}.\]Then,

\[\frac{\partial f^{h}_{j}}{\partial x_{i}}-\frac{\partial f^{h^{\prime}}_{j}}{ \partial x_{i}}=\frac{\mathrm{d}\Psi_{ji}(x_{i})}{\mathrm{d}x_{i}}(\beta^{h}_{ ji}-\beta^{h^{\prime}}_{ji})\neq\mathbf{0}\iff\beta^{h}_{ji}-\beta^{h^{\prime}}_{ ji}\neq\mathbf{0}\]

The last \(\iff\) relation is due to the linear independence of the basis functions \(\Psi_{ji}\), then the null space of \(\nicefrac{{\mathrm{d}\Psi_{ji}}}{{\mathrm{d}x_{i}}}\) can only be the zero vector \(\mathbf{0}\). 

Note that the output of Algorithm 1 also estimates a topological order \(\hat{\pi}\). However, the _exact parents_ of a node \(X_{j}\) across the environments are not known, and they are possibly different. To estimate the coefficients without knowledge of the exact parents, we can consider the set \(\widehat{\mathrm{Pre}}(X_{j})\), which consists of nodes located before \(X_{j}\) in the topological order \(\hat{\pi}\). By regressing \(X_{j}\) on \(\widehat{\mathrm{Pre}}(X_{j})\) for each environment, we can obtain coefficient estimations, which are the same coefficients obtained by regressing \(X_{j}\) on its exact parents, in large samples.

**Theorem 3**.: _In large samples, let \(\{\tilde{\beta}^{h}_{jk}\}_{k\in\mathrm{Pre}(X_{j})}\) be the coefficients obtained by regressing \(X_{j}\) on the feature mapping of \(\mathrm{Pre}(X_{j})\), and let \(\{\beta^{h}_{jk}\}_{k\in\mathrm{PA}_{j}}\) be the coefficients obtained by regressing \(X_{j}\) on the feature mapping of \(\mathrm{PA}_{j}\). Then, \(\tilde{\beta}^{h}_{jk}=\beta^{h}_{jk}\) if \(k\in\mathrm{PA}^{h}_{j}\), and \(\tilde{\beta}^{h}_{jk}=0\) if \(k\in\mathrm{Pre}(X_{j})\setminus\mathrm{PA}^{h}_{j}\)._

Proof.: Proof can be found in Appendix B.3. 

Motivated by Theorem 3, and given an estimated \(\{\tilde{\beta}^{h}_{jk}\}_{k\in\widehat{\mathrm{Pre}}(X_{j})}\), one could conduct a hypothesis testing as follows:

\[H_{0}:\tilde{\beta}^{1}_{jk}=\cdots=\tilde{\beta}^{H}_{jk}\] (13)

If the null hypothesis \(H_{0}\) is rejected, it indicates that there is evidence of a functionally shifted edge between nodes \(X_{k}\) and \(X_{j}\) across the environments. In this paper we leave the hypothesis test unspecified to allow for any procedure that can test eq.(13).

```
0: Sample data \(\bm{X}^{1},\ldots,\bm{X}^{H}\), shifted nodes set \(\widehat{S}\), topological order \(\hat{\pi}\), significance level \(\alpha\).
0: Set of functionally shifted edges \(\widehat{E}\)
1:for\(j\in\widehat{S}\)do
2: Estimate \(\tilde{\beta}^{h}_{jk}\) for all \(k\in\widehat{\mathrm{Pre}}(X_{j})\) and \(h\in[H]\)
3:for\(k\in\widehat{\mathrm{Pre}}(X_{j})\)do
4: Conduct hypothesis testing \(H_{0}\) (equation 13) under significant level \(\alpha\).
5: If \(H_{0}\) is rejected, add edge \((X_{k}\to X_{j})\) to \(\widehat{E}\). ```

**Algorithm 5** Functionally shifted edges detection