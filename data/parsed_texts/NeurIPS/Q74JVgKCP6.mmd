# Near-Optimality of Contrastive Divergence Algorithms

 Pierre Glaser Kevin Han Huang Arthur Gretton

Gatsby Computational Neuroscience Unit, University College London

pierreglaser@gmail.com, han.huang.20@ucl.ac.uk, arthur.gretton@gmail.com

###### Abstract

We perform a non-asymptotic analysis of the contrastive divergence (CD) algorithm, a training method for unnormalized models. While prior work has established that (for exponential family distributions) the CD iterates asymptotically converge at an \(O(n^{-1/3})\) rate to the true parameter of the data distribution, we show, under some regularity assumptions, that CD can achieve the parametric rate \(O(n^{-1/2})\). Our analysis provides results for various data batching schemes, including the fully online and minibatch ones. We additionally show that CD can be near-optimal, in the sense that its asymptotic variance is close to the Cramer-Rao lower bound.

## 1 Introduction

Describing data using probability distributions is a central task in multiple scientific and industrial disciplines [1; 2; 3]. Since the true distribution of the data is generally unknown, such a task requires finding an estimator of the true distribution among a model class that best describes the available data. An estimator can be characterized at multiple levels of granularity: at the highest level lies _consistency_[4], a property which states that as the number of available data points increases, a given estimator will converge to the one best describing the data distribution. At a lower level, a consistent estimator can be further characterized by its convergence rate, a quantity upper-bounding its distance to the true distribution as a function of the number of samples. A convergence rate can be either _asymptotic_, e.g. hold only in the limit of an infinite sample size, or _non-asymptotic_, in which case the rate also holds for finite sample sizes. In their simplest form, convergence rates are provided in big-\(O\) notation, discarding finer grained information such as asymptotically dominated quantities as well as multiplicative constants. These constants play a role in the so-called _asymptotic variance_ of the estimator, which is a precise descriptor of an estimator's statistical efficiency. Convergence rates and asymptotic variances have been the subject of extensive research in the statistical literature; in particular, well-known lower bounds exists regarding both the best possible (asymptotic) convergence rate of an estimator and its best possible asymptotic variance. These results set a clear frame of reference to interpret individual convergence rates, and are routinely present in the analysis of modern statistical algorithms such as noise-contrastive estimation [5; 6] or score matching [7; 8; 9].

In this work, we focus on cases where (1) the true data distribution admits a density with respect to some known base measure, and (2) the model class is parametrized by a finite-dimensional parameter. In this setting, provided that the true distribution belongs to the model class, a celebrated result in statistical estimation states that the model maximizing the average log-likelihood both achieves the best possible asymptotic convergence rate (called the _parametric rate_) and the best possible asymptotic variance, called the Cramer-Rao bound (see, e.g. [10]). While this result shows that Maximum Likelihood Estimators (MLE) are asymptotically optimal, fitting them is complicated by computational hurdles when using models with intractable normalizing constants. Such _unnormalized models_ are common in the Machine Learning literature due to their high flexibility [11; 12]; their weakness however lies in the fact that expectations under these models have no unbiased approximation. For this reason, popular approximation algorithms such as unbiased gradient-basedstochastic optimization of the empirical log-likelihood cannot _a priori_ be used, as the gradient of the normalizing constant is given by an expectation under the model distribution.

The Contrastive Divergence (CD) algorithm [13] is a popular approach that circumvents this issue by using a Markov Chain Monte Carlo (MCMC) algorithm to approximate the gradient of the log-likelihood. Unnormalized models trained with Contrastive Divergence have been shown to reach competitive performance in high-dimensional tasks such as image [14; 15; 16], text [17], and protein modeling [18; 19], or neuroscience [20]. A consistency analysis of the Contrastive Divergence algorithm is delicate, however: indeed, the _optimization error_ e.g. the difference between the estimate returned by CD and the MLE, is likely to be non-negligible as compared with _statistical error_ - the distance between the MLE and the true distribution - and thus cannot be discarded, as often done when analyzing estimators that minimize tractable objectives [8; 5]. Recent work [21] elegantly established asymptotic \(O(n^{-1/3})\)-consistency of the CD estimator for unnormalized exponential families when using only a _finite_ number of MCMC steps. Key to their argument is the fact that the bias of the CD gradient estimate decreases as iterates approach the data distribution. However, as noted by the authors, their work left open the question of whether and under what conditions CD might achieve \(O(n^{-1/2})\)-consistency.

ContributionsIn this work, we answer this question by providing a non-asymptotic analysis of the CD algorithm for unnormalized exponential families. While existing convergence bounds [21] were derived for the "full batch" setting, where the CD gradient is estimated using the full dataset at each iteration, our analysis covers both the online setting (where data points are processed one at a time without replacement), and the offline setting with multiple data reuse strategies (including full batch).

In the online case (Section 3), we show, under a restricted set of assumptions compared to Jiang et al. [21], that the CD iterates can converge to the true distribution at the parametric \(O(n^{-1/2})\) rate. Our analysis reveals that CD contains two sources of approximation: a bias term, and a variance term. These sources are almost independent of each other, in the sense that decreasing the bias by increasing the number of MCMC steps will not decrease the variance. The impact of these two sources of approximation transparently propagates in our resulting bounds: in particular, as the bias of the CD algorithm goes to 0, our bounds recover well-known results in online stochastic optimization [22]. Finally, we study the asymptotic variance of an estimator obtained by averaging the CD iterates, a classic acceleration technique in stochastic optimization [23]. We show that provided that the number of steps \(m\) is sufficiently large, the _asymptotic_ variance of this estimator matches (up to a factor 4) the Cramer-Rao bound.

Next, we study the offline setting (Section 4), where the CD gradient is estimated by reusing (potentially random) subsets of a finite dataset. We show that a similar result to the online setup holds, up to an additional correlation term that arises from data reuse, and present several approaches to control this term. We improve over the results of [21] by showing a non-asymptotic and near-parametric rate at \(O((\log n)^{1/2}n^{-1/2})\) under their conditions, and also illustrate how different rates can be obtained under a variety of conditions. Our results also show an interesting tradeoff between the effect of initialization and the statistical error as a function of batch size.

In summary, we establish the near-optimality of a variety of Contrastive Divergence algorithms for unnormalized exponential families in the so called "long-run" regime, where the number of MCMC steps is high enough to ensure that the CD gradient bias is sufficiently offset by the convexity of the negative log-likelihood.

## 2 Contrastive Divergence in Unnormalized Exponential Families

Unnormalized Exponential Families Exponential families (EF) [24; 25] form a well-studied class of probability distributions, given by

\[p_{\psi}(\mathrm{d}x)\coloneqq e^{\psi^{\top}\phi(x)-\log Z(\psi)}c(\mathrm{d }x),\quad Z(\psi)\coloneqq\int_{\mathcal{X}}e^{\psi^{\top}\phi(x)}c(\mathrm{d }x).\] (1)

Here, \(\mathcal{X}\ni x\) is the _data_ or _sample space_, which we set to be a subset of \(\mathbb{R}^{d}\) for some \(d\in\mathbb{N}^{*}\), although our results are readily extendable to more general measurable spaces. \(c\) is a measure on \(\mathcal{X}\) called the _base_ or _carrier_ measure. When \(\mathcal{X}\subseteq\mathbb{R}^{d}\), \(c\) is often set to be the corresponding Lebesgue measure. \(\psi\in\Psi\subseteq\mathbb{R}^{p}\) is a finite-dimensional parameter called the _natural parameter_, and \(\phi:\mathbb{R}^{d}\longmapsto\mathbb{R}^{p}\) is a function called the _sufficient statistics_, which, alongside with the base measure, fully describes an exponential family. Finally, \(\log Z(\psi)\), the _log-normalizing_ (or _cumulant_) function, is a quantity ensuring that \(p_{\psi}\) integrates to \(1\) over \(\mathcal{X}\). Crucially, we will not assume that \(\log Z(\psi)\) admits a closed form expression for all \(\psi\). The latter fact provides the practitioner with a great deal of flexibility in designing the model class: indeed, the only requirement that should be satisfied prior to performing statistical estimation is to have \(Z(\psi)<+\infty\) for all \(\psi\), something that can be readily verified and is often the case in practice. The drawback of unnormalized EFs is the fact that sampling (and thus approximating expectations under the model) cannot usually be performed in an unbiased manner. Instead, inference in unnormalized EFs is often performed using tools from the Bayesian Inference literature, such as MCMC [26]. Unnormalized EFs belong to the larger class of _unnormalized models_[27, 28, 7, 6], of the form \(e^{-E_{\psi}(x)-\log Z(\psi)}c(\mathrm{d}x),\ Z(\psi)=\int e^{-E_{\psi}(x)}c( \mathrm{d}x)\), for some parametrized function \(E_{\psi}:\mathbb{R}^{d}\longmapsto\mathbb{R}\) referred to as the _energy_. Unnormalized models thus take the flexibility of unnormalized EFs one step further by allowing the (negative) unnormalized log-density to be an arbitrary function \(E_{\psi}\) of \(x\) and \(\psi\), instead of requiring a linear dependence on \(\psi\) as in Equation 1. We focus in this work on unnormalized EFs due to the multiple computational benefits they provide, as explained in the next section, but we believe that extending our analysis to more general unnormalized models is an interesting avenue for future work.

**Statistical Estimation in Unnormalized Exponential Families using Contrastive Divergence** We now review the Contrastive Divergence algorithm, an algorithm used to fit unnormalized models, and our main object of study in this work. The general setting is the following: we assume access to \(n\) i.i.d. samples \((X_{1},\ldots,X_{n})\) drawn from some unknown distribution \(p^{\star}\), which we assume belongs to \(\mathcal{P}_{\psi}\), e.g. \(p^{\star}=p_{\psi^{\star}}\) for some \(\psi^{\star}\in\Psi\). Given these samples, we aim to perform _statistical estimation_, e.g. find a parameter \(\psi_{n}\) within \(\Psi\) that should approach \(\psi^{\star}\) as \(n\) grows.

The starting point of the Contrastive Divergence algorithm is the unfortunate realization that Maximum Likelihood Estimation, which corresponds to minimizing the cross-entropy \(\mathcal{L}(\psi)\coloneqq-\mathbb{E}_{p_{n}}\log\mathrm{d}p_{\psi}/\mathrm{d}c\) between the model \(p_{\psi}\) and the empirical data distribution \(p_{n}\coloneqq 1/n\sum_{i=1}^{n}\delta_{X_{i}}\), cannot be performed using exact (possibly stochastic) gradient-based optimization, as the gradient \(\nabla_{\psi}\mathcal{L}(\psi)\) of \(\mathcal{L}\) with respect to the parameter \(\psi\) contains an expectation under the model distribution \(p_{\psi}\). Indeed, the cross entropy and its gradient are given by

\[\begin{cases}\mathcal{L}(\psi)&=-\frac{1}{n}\sum_{i=1}^{n}\phi(X_{i})^{\top} \psi+\log Z(\psi)\\ \nabla_{\psi}\mathcal{L}(\psi)&=-\frac{1}{n}\sum_{i=1}^{n}\phi(X_{i})+\mathbb{ E}_{p_{\psi}}\phi.\end{cases}\] (2)

The second line follows from the well known identity \(\nabla_{\psi}\log Z(\psi)\coloneqq\mathbb{E}_{p_{\psi}}\phi\); we refer to [25, Proposition 3.1] for a proof. The Contrastive Divergence algorithm circumvents this issue by running approximate stochastic gradient descent (SGD) on \(\mathcal{L}\), where the intractable expectation in \(\nabla_{\psi}\log Z\) is estimated using an MCMC algorithm initialized at the empirical data distribution. In more details, given a number of epochs \(T\), a sequence of data batches \(B_{t,j}\) of size \(B\) (e.g. \(B_{t,j}\in\llbracket 1,n\rrbracket^{B},1\leq t\leq T,1\leq j\leq N\lceil n/B\rceil\)), and a family of _Markov kernels_\(\{k_{\psi},\psi\in\Psi\}\) each with invariant distribution \(p_{\psi}\), at the \(j^{th}\) minibatch of epoch \(t\), \(\nabla_{\psi}\log Z(\psi_{t,j-1})\) is approximated by \(\frac{1}{B}\sum_{i\in B_{t,j}}\phi(\tilde{X}_{i}^{m})\), where \(\tilde{X}_{i}^{m}\) is produced by running the recursion \(\tilde{X}_{i}^{k}\sim k_{\psi_{t}}(\tilde{X}_{i}^{k-1},\cdot),\ \tilde{X}_{i}^{0}=X_{i}\) up to \(k=m\). Throughout the paper, we will refer to the conditional distribution of \(\tilde{X}_{i}^{m}\) given \(X_{i}\) as \(k_{\psi}^{m}(X_{i},\cdot)\). The resulting gradient estimate arising from combining this approximation with the other (tractable) sum over the data samples present in \(\nabla_{\psi}\mathcal{L}(\psi)\), which we refer to as the _CD gradient_ and denote as \(h_{t}\), is thus

\[h_{t,j}\coloneqq\tfrac{1}{B}\sum_{i\in B_{t,j}}\phi(X_{i})-\tfrac{1}{B}\sum_{ i\in B_{t,j}}\phi(\tilde{X}_{i}^{m})=\tfrac{1}{B}\sum_{i\in B_{t,j}}\left(\phi(X_{i})- \phi(\tilde{X}_{i}^{m})\right).\] (3)

Key to the behavior and analysis of the CD algorithm is the strategy employed to generate minibatches \(B_{t,j}\). The case where \(T=1\), \(B=1\), and \(B_{1,j}=\{j\}\) will be referred to as _online_ CD, while the variant where \(T>1\), and each batch \(B_{t,j}\) draws \(B\) indices (with or without replacement) from \(\llbracket 1,n\rrbracket\) will be referred to as _offline_ CD. In online CD, each data point is present in one and one batch only, while in offline CD, data points are reused across batches. From a statistical perspective, we will see that online CD can be analyzed in a remarkably simple way, while offline CD introduces additional correlations that require care to be controlled. Both settings come with their advantages and drawbacks, as we will see in the next section. The CD algorithms we study will employ decreasing step size schedules \((\eta_{t})_{t\geq 0}\) of the form \(\eta_{t}=Ct^{-\beta}\), where \(C>0\) is the initial leaning rate and \(\beta\in[0,1]\). We lay out online CD and offline CD in Algorithms 1 and 2. Note that our algorithms include a projection step on the parameter space \(\Psi\) to account for the case where \(\Psi\) is compact. In the case \(\Psi=\mathbb{R}^{p}\), this step can be omitted. Next we depart from the setting of [21] and start by analyzing online CD.

## 3 Non-asymptotic analysis of Online CD

### Preliminaries and Assumptions

Recall that the chi-squared divergence between two probability measures \(p\) and \(q\) is defined as: \(\chi^{2}(p,q)\coloneqq\int(\frac{\mathrm{d}p}{\mathrm{d}q}(x)-1)^{2}q(\mathrm{d}x)\) if \(p\ll q\), and \(+\infty\) otherwise. Here, \(p\ll q\) denotes that \(p\) is absolutely continuous with respect to \(q\) and \(\mathrm{d}p/\mathrm{d}q\) is the Radon-Nikodym derivative [29] of \(p\) with respect to \(q\). Let \(L^{2}(p_{\psi})\) be the space of square-integrable functions with respect to \(p_{\psi}\). For a function \(f\in L^{2}(p_{\psi})\), we define

\[\alpha(f,\psi)=\frac{\left(\int(f-\mathbb{E}_{p_{\psi}}f)(y)\,k_{\psi}(x, \mathrm{d}y)\right)^{2}p_{\psi}(\mathrm{d}x))^{1/2}}{\left(\int(f-\mathbb{E}_{ p_{\psi}}f)(x)^{2}p_{\psi}(\mathrm{d}x)\right)^{1/2}}\] (4)

which is a measure of how quick a Markov chain with kernel \(k_{\psi}\) mixes, relative to the function \(f\)[30]. With these definitions in hand, we now state the assumptions required by our analysis of online CD. These assumptions form a strict subset of the assumptions considered in prior work [21], which required additional regularity and tail conditions on the Markov kernels \(k_{\psi}\).

**Assumption A1**.: \(\mathcal{P}_{\psi}\) is a subset of a _regular_ and _minimal_[25, Section 3.2] exponential family with natural parameter domain \(\mathcal{D}\subseteq\mathbb{R}^{p}\), \(\Psi\) is a _convex and compact_ subset of \(\mathcal{D}\), and \(\psi^{*}\) lies in the interior of \(\Psi\).

**Assumption A2**.: There exists a constant \(C_{\chi}>0\) such that \(\chi^{2}(p_{\psi^{*}},p_{\psi})\leq C_{\chi}^{2}\|\psi-\psi^{*}\|^{2}\)

**Assumption A3**.: \(\alpha\coloneqq\sup\{\alpha(f,\psi),\,f\in\{\phi_{i}\}_{i=1}^{p}\cup\{\phi_{i }\phi_{j}\}_{i,j=1}^{p},\,\psi\in\Psi\}<1\), where \(\phi_{i}\) is the \(i\)-th component of the function \(\phi\), and \(\phi_{i}^{2}\) is the \(i\)-th component of the function \(x\longmapsto\phi(x)^{2}\).

A well known property of EFs [25, Proposition 3.1] is that their negative cross-entropy (against any other measure) is \(C^{\infty}\), convex, and strictly so if the exponential family is minimal (meaning that the set of sufficient statistic functions \(\phi_{i}\) are not linearly dependent). Leaving aside the issue of intractable expectations, this convexity suggests that \(\mathcal{L}\) can be efficiently minimized using stochastic approximation algorithms [31; 22]. The compactness of \(\Psi\) provided by Assumption A1 thus ensures, by the extreme value theorem [32], the existence of finite positive constants \(\mu\) and \(L\) defined as:

\[\mu\coloneqq\min_{\psi\in\Psi}\lambda_{\min}\left(\nabla_{\psi}^{2}\mathcal{L }(\psi)\right),\quad L\coloneqq\max_{\psi\in\Psi}\lambda_{\max}\left(\nabla_{ \psi}^{2}\mathcal{L}(\psi)\right),\] (5)

where \(\nabla_{\psi}^{2}\mathcal{L}\) is the Hessian of \(\mathcal{L}\) with respect to \(\psi\). \(\mu\) (called the _strong convexity_ constant) and \(L\) (a bound controlling the smoothness of the problem) play a critical role in the analysis of convex optimization algorithms [31]. While it is possible to obtain convergence rates in non-smooth or non-strongly-convex settings, our analysis follows the spirit of [21] by leveraging the strong convexity of the problem to compensate for the bias introduced by using CD gradients instead of unbiased stochastic gradients.

Assumption A2 allows link variations in distribution space to variations in parameter space, and will be instrumental to control the bias of the CD gradient. Note that since

[MISSING_PAGE_FAIL:5]

While we precisely investigate the impact of the residual variance term in the next section, we now unify \(\sigma_{\star}\) and \(\sigma_{t}\) by introducing

\[\sigma\coloneqq\sup_{\psi\in\Psi}(\mathbb{E}_{p_{\psi}}\|\phi-\mathbb{E}_{p_{ \psi}}\phi\|^{2})^{1/2}.\] (8)

\(\sigma\) is an upper bound on the noise induced _both_ by the CD gradient and by the online setup, and was used in prior work [21]. Note that by the properties of \(\log Z\), \(\sigma^{2}\) also equals \(\sup_{\psi\in\Psi}\operatorname{tr}(\nabla^{2}_{\psi}\mathcal{L}(\psi))\), where \(\operatorname{tr}(A)\) is the trace of \(A\in\mathbb{R}^{p\times p}\), and thus finite by the extreme value theorem. The following theorem is obtained by invoking standard unrolling arguments in the convex optimization literature. In the next result, we use the function \(\varphi_{\gamma}(t)\), defined as \(\varphi_{\gamma}(t)=\frac{t^{\gamma-1}}{\gamma}\) if \(\gamma\neq 0\), and \(\log t\) if \(\gamma=0\).

**Theorem 3.2**.: _Fix \(n\geq 1\). Let \((\psi_{t})_{0\leq t\leq n}\) be the iterates produced by Algorithm 1, and define \(\delta_{t}\coloneqq\mathbb{E}\left\|\psi_{t}-\psi^{\star}\right\|^{2}\). Moreover, assume that \(m>\frac{\log(\sigma C_{\chi}/\mu)}{\log\left|\alpha\right|}\), i.e. \(\tilde{\mu}_{m}\coloneqq\mu-\alpha^{m}\sigma C_{\chi}>0\). Then under Assumptions A1, A2 and A3, for \(\eta_{t}=Ct^{-\beta}\) with \(C>0\), we have:_

\[\delta_{n}\leq\begin{cases}2\exp\left(4\tilde{L}C^{2}\varphi_{1-2\beta}(n) \right)\exp\left(-\frac{\tilde{\mu}_{m}C}{4}n^{1-\beta}\right)\left(\delta_{0 }+\frac{\tilde{\sigma}_{m}^{2}}{L^{2}}\right)+\frac{4C\tilde{\sigma}_{m}^{2} }{\tilde{\mu}_{m}n^{\beta}},&\text{ if }0\leq\beta<1\\ \frac{\exp\left(2\tilde{L}^{2}C^{2}\right)}{n^{\beta C}}\left(\delta_{0}+\frac {\tilde{\sigma}_{m}^{2}}{L^{2}}\right)+2\tilde{\sigma}_{m}^{2}C^{2}\frac{ \varphi_{\tilde{\mu}_{m}C/2-1}(n)}{n^{\tilde{\mu}_{m}C/2}},&\text{ if }\beta=1\;,\end{cases}\]

_where \(\tilde{\sigma}_{m}=\sigma^{2}(2+2\alpha^{2m})+\alpha^{m/2}\left\|\log Z\right\| _{3,\infty}^{2}C_{\chi}^{2}\) and \(\tilde{L}=(L^{2}+\alpha^{m/2})^{1/2}\). Consequently, if \(\eta_{n}=\frac{C}{n}\) with an initial learning rate \(C>2\tilde{\mu}_{m}^{-1}\), we have \(\sqrt{\delta_{n}}\leq 2\tilde{\sigma}_{m}C\sqrt{\frac{\tilde{\mu}_{m}C}{\tilde{ \mu}_{m}C-2}}\frac{1}{\sqrt{n}}+o\left(\frac{1}{\sqrt{n}}\right)\)._

Theorem 3.2 is proved in Appendix D.3. It shows that the iterates produced by online CD will converge to the true parameter \(\psi^{\star}\) at the rate \(O(n^{-1/2})\) provided that the number of steps \(m\) is sufficiently large, improving over the asymptotic \(O(n^{-1/3})\) rate of [21], while imposing slightly weaker conditions on the number of steps \(m\) (see (21, Theorem 2.1)). This proves that online CD can be asymptotically competitive with other methods for training unnormalized models, such as Noise Contrastive Estimation [6], or Score Matching [7]. However, the asymptotic variance of \(\psi_{t}\) (e.g. the multiplicative factor in front of the \(O(n^{-1/2})\) term) is likely to be suboptimal, e.g. much larger than the Cramer-Rao bound, given by the trace of the inverse of the Fisher information matrix [25]. Given the statistical optimality of MLE, and the fact that CD in an approximate MLE method, this motivates the further goal or obtaining a CD estimator with near-optimal statistical properties. In the next section, we achieve this goal by showing that averaging the iterates \(\psi_{t}\) will produce a near statistically-optimal estimator, in a sense that we will make precise.

#### 3.2.2 Towards statistical optimality with averaging

Polyak-Ruppert averaging [23] is a simple yet surprisingly effective way to construct an asymptotically optimal estimator \(\bar{\psi}_{n}\coloneqq\frac{1}{n}\sum_{t=1}^{n}\psi_{i}\) from a sequence of iterates \((\psi_{t})_{0\leq t\leq n}\) obtained by running a standard online SGD algorithm [22]. As shown in [22], when the objective is the cross-entropy of a model, and assuming the unbiased stochastic gradients are available, averaging yields an estimator \(\overline{\psi}\) with the asymptotic variance \(\operatorname{tr}(\mathcal{I}(\psi^{\star})^{-1})/n\), where \(\mathcal{I}(\psi)\coloneqq\operatorname{Cov}_{p_{\phi}}\), \(\phi\) is the Fisher information matrix of the data distribution \(p_{\psi^{\star}}\). \(\mathcal{I}(\psi^{\star})^{-1}\) being the Cramer-Rao _lower bound_ on asymptotic variances of statistical estimators [10], this estimator \(\bar{\psi}_{n}\) is asymptotically optimal. The following theorem shows conditions under which averaging CD iterates can give rise to a near-optimal estimator.

**Theorem 3.3** (Contrastive Divergence with Polyak-Ruppert averaging).: _Let \((\psi_{t})_{t\geq 0}\) the sequence of iterates obtained by running the CD algorithm with a learning rate \(\eta_{t}=Ct^{-\beta}\) for \(\beta\in(\frac{1}{2},1)\). Define \(\bar{\psi}_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\psi_{i}\). Then, under the same assumptions as Theorem 3.2, and assuming additionally that \(m\coloneqq m(n)>\frac{(1-\beta)\log n}{2\left|\log\alpha\right|}\), we have, for all \(n\geq 1\),_

\[\left(\mathbb{E}\left\|\overline{\psi}_{n}-\psi^{\star}\right\|^{2}\right)^{1/ 2}\;\leq\;2\sqrt{\frac{\operatorname{tr}(\mathcal{I}(\psi^{\star})^{-1})}{n}}+o (n^{-1/2})\]

_Consequently, we have that \(\limsup_{n\to\infty}n\mathbb{E}(\left\|\overline{\psi}_{n}-\psi^{\star}\right\|^ {2})\leq 4\operatorname{tr}(\mathcal{I}(\psi^{\star})^{-1})\)._

Theorem 3.3, alongside with a statement which includes the asymptotic order of the residual term, is proved in Appendix D.4. It shows that at the cost of an increase in _computational_ complexity of the entire algorithm from \(O(n)\) to \(O(n\log n)\), \(\bar{\psi}_{n}\) will be a near-optimal statistical estimator of \(\psi^{\star}\)While this increase in complexity emerges from the bias of CD, the additional variance of CD results in an asymptotic variance inflated by a factor of \(4\) compared to the Cramer-Rao bound.

Theorem 3.3 concludes our analysis of online CD. Despite their asymptotic near-optimality, the bounds provided for online CD and its averaged version have weaknesses: the online CD iterates are not robust to choices of C. On the other hand, as shown in Appendix D.4, the bound of the averaged iterates contain higher-order terms that could be large in intermediate sample regimes. Next, we show that offline CD, which processes data points multiple times, can alleviate these issues.

## 4 Non-asymptotic analysis of offline CD

In practice, CD gradient approximation schemes are commonly used within an offline stochastic gradient descent (SGD) algorithm, where one is given the full size-\(n\) dataset upfront and each update uses some stochastic subset of the data. We study CD under offline SGD with replacement (SGDw), i.e. Algorithm 2 with batches \(B_{t,j}\) being i.i.d. uniform draws of size-\(B\) subsets of \([n]\), and include SGD without replacement in Appendix B.2. To do so, we follow the setting of prior work on offline CD [21], which established its asymptotic \(O(n^{-\frac{1}{3}})\) consistency. We show that by slightly strengthening a moment assumption used in [21], the offline CD iterates converge to the true parameter at a near-parametric \(O((\log n)^{\frac{1}{2}}n^{-\frac{1}{2}})\) rate. Our proof proceeds by controlling a "tail probability" term specific to the offline setting which characterizes the strength of the correlations between the offline CD iterates and the training data. While, as we show, the assumptions of [21] provide a tail control sufficient to obtain a near-parametric rate, other strategies are possible to obtain convergence guarantees. In particular, we show that non-asymptotic convergence can be obtained by either (1) relaxing assumptions on the Markov kernel required by prior work, or (2) making a specific mixing assumption the Markov chain.

### Background: Asymptotic consistency of offline CD in subexponential settings

Prior work [21] has established _asymptotic_\(O(n^{-\frac{1}{3}})\) consistency of the (averaged) offline CD iterates in the full-batch case. We summarize their results and assumptions below.

**Assumption A4**.: There exists \(\nu\geq 2\) s.t. for all \(m\in\mathbbm{N}\), there is \(\kappa_{\nu;m}<\infty\) s.t.

\[\sup_{x\in\mathcal{X}}\,\sup_{\psi\in\Psi}\,\left(\mathbb{E}\big{\|}\phi(K_{ \psi}^{m}(x))-\mathbb{E}[\phi(K_{\psi}^{m}(x))]\big{\|}^{\nu}\right)^{1/\nu} \;\leq\;\kappa_{\nu;m}\;.\]

**Assumption A5**.: There exists some \(C_{m}>0\) such that, for all \(\psi_{1},\psi_{2}\in\Psi,\sup_{x\in\mathcal{X}}\|\mathbb{E}[\phi(K_{\psi_{1}}^ {m}(x))]-\mathbb{E}[\phi(K_{\psi_{2}}^{m}(x))]\|\leq C_{m}\|\psi_{1}-\psi_{2}\|\).

**Assumption A6**.: There exist some \(\sigma_{m},\zeta_{m}>0\) such that, for any \(z\in\mathbbm{R}^{p}\) with \(\|z\|\leq\zeta_{m}\), \(\mathbb{E}[e^{z^{\top}(\phi(K_{\psi}^{m}(X_{\psi}))-\mathbb{E}[\phi(K_{\psi_{ \psi}^{m}(X_{\psi})}^{m}(X_{\psi}))])}]\leq e^{\sigma_{m}^{2}\|z\|^{2}/2}\).

**Theorem 4.1** (Theorem 2.1 of [21]).: _Assume assumptions A1,A2, A3, A4 (for \(\nu=2\)), A5 and A6. Let \(\psi_{t,1}\) be the \(t\)-th iterate of offline CD with full-batch gradient descent and constant stepsize \(\eta_{t}=C\), i.e the iterates produced by Algorithm 2 using \(B_{t,1}=\llbracket 1,n\rrbracket\). Then for any learning rate \(C\) and number of Markov kernel steps \(m\) satisfying \(\mu-\alpha^{m}\sigma C_{\chi}-\frac{C}{2}(L+\alpha^{m}\sigma C_{\chi})^{2}>0\), we have, for some \(A_{m}>0\),_

\[\lim_{n\to\infty}\mathbb{P}\left(\limsup_{T\to\infty}\left\|\frac{1}{T}\sum_{ t=1}^{T}\psi_{t,1}^{\mathrm{SGDw}}-\psi^{\star}\right\|>A_{m}n^{-\frac{1}{3}} \right)=0\]

This result shows convergence of the _averaged_ full-batch CD iterates to the true parameter in the large \(n\) and \(T\) limit. As discussed, this result is asymptotic both in \(n\) and \(T\): the probability of the error exceeding \(A_{m}n^{-\frac{1}{3}}\) goes to 0 as \(n\to\infty\) and \(T\to\infty\), but at an unknown rate. Moreover, the \(O(n^{-\frac{1}{3}})\) does not match the optimal \(O(n^{-\frac{1}{3}})\) rate.

### Sharpening offline CD bounds in subexponential settings

#### 4.2.1 Non-asymptotic \(\tilde{O}(n^{-1/2})\)-consistency

As a first result, we show that under the assumptions of [21] (except for a slightly stronger \(\nu>2\) moment assumption in A4), \(\psi_{T,N}^{\mathrm{SGDw}}\) in fact achieves a near-parametric rate. The most general version of our result holds for any learning rate schedule of the form \(Ct^{-\beta},\beta\in[0,1]\), and for offline SGDwith arbitrary batch sizes \(B\), with data drawn either with or without replacement across batches. For simplicity, we first present our result assuming full batch (\(B=n,N=1,\psi_{t,j}^{\mathrm{SGDw}}=\psi_{t,1}^{\mathrm{GDw}}\) for \(t\geq 1\)) SGD with constant step sizes \(\eta_{t}=C\), which is the setting of [21]. Analogue bounds holding for the other mentioned batching and step sizes schedules can be found in Appendix B.

**Theorem 4.2**.: _Assume the setup of Theorem 4.1, except that Assumption A4 holds for some \(\nu>2\), and that \(\tilde{\mu}_{m}\ =\ \mu-\alpha^{m}\sigma C_{\chi}\ >\ 4CL^{2}\). Let \(\delta_{t,j}^{\mathrm{SGDw}}\coloneqq\mathbb{E}\|\psi_{t,j}^{\mathrm{SGDw}}- \psi^{*}\|^{2}\). Then, we have:_

\[\sqrt{\delta_{T,1}^{\mathrm{SGDw}}}\leq E_{1}^{T,1}\sqrt{\delta_{0,0}^{ \mathrm{SGDw}}}\ +\ C^{\prime}(p,\nu,m,\Psi)\Big{(}\tfrac{\sqrt{\log n}}{\sqrt{n}}+\tfrac{1}{ \sqrt{n}}\Big{)}\Big{(}\tfrac{e^{\frac{\tilde{\mu}_{m}C}{2}}}{\tilde{\mu}_{m}C }+\tfrac{E_{2}^{T,1}}{L^{2}C^{2}}\,\Big{)}\] (9)

_where \(E_{1}^{T,1}\), \(E_{2}^{T,1}\) are functions decreasing exponentially in \(T\), and \(C^{\prime}(p,\nu,m,\Psi)\) is a constant in \(n,T\). Consequently,_

\[\lim_{T\to\infty}\sqrt{\delta_{T,1}^{\mathrm{SGDw}}}\leq\tfrac{e^{\frac{\tilde {\mu}_{m}C}{2}}}{\tilde{\mu}_{m}C}C^{\prime}(p,\nu,m,\Psi,\beta)\Big{(}\tfrac{ \sqrt{\log n}}{\sqrt{n}}+\tfrac{1}{\sqrt{n}}\Big{)}\.\]

The precise values of all the constants can be found in Theorem B.1 (for \(E_{1}^{T,1}\), \(E_{2}^{T,1}\)) and Lemma B.3 (for \(C^{\prime}(p,\nu,m,\Psi,\beta)\)), including their expressions for \(N>1\) and \(\beta\in[0,1]\). We comment on the main differences between our result and the one of [21]. First our bound holds for _any_ epoch \(T\) and number of samples \(n\). Second, fixing \(n\) but taking \(T\to\infty\), the final bound matches the parametric \(O(\sqrt{n})\) up to a \(\sqrt{\log(n)}\) factor, a significant improvement over the \(O(n^{-\frac{1}{3}})\) rate of [21]. Finally, we control an \(L_{2}\) error, which is a stronger control than a high probability bound by Markov's inequality; we hypothesize this is the reason why a slightly stronger moment assumption is required for our setup, compared to the one used for the high probability bound in [21].

Inspecting Equation 9, we notice the presence of two _transient_ terms, and a _stationary term_, reminiscent of the structure of upper bound of Theorem 3.2. The transient terms (i.e. the ones containing \(E_{1}^{T,1}\) and \(E_{2}^{T,1}\)) vanish exponentially fast in the total number of CD updates \(T\). However, unlike in online CD where the number of updates and the number of samples are tied (e.g. \(T=n\)), these two values are now _decoupled_, and these terms can be made arbitrarily small by increasing the number of gradient steps \(T\) without having to collect more samples \(n\). The stationary term, which is the only one remaining in the limit of \(T\to\infty\), decreases with \(n\) at a rate that is independent of hyperparameters like the step size \(C\) or the learning rate schedule \(\beta\) (see Lemma B.3). In that sense, offline CD compares favorably to online CD, whose rate is sensitive to \(\beta\) and \(C\), and averaged online CD, whose bound contains higher-order (in \(n\)) terms which can be large in the moderate \(n\) regime. On the other hand, the stationary term in offline CD is asymptotically suboptimal: its rate is larger (while only up to a log factor) than the best-case \(O(\sqrt{n})\) one achieved by online CD algorithms, and the leading constant does not match the optimal one.

#### 4.2.2 Proof of Theorem 4.2

The high-level proof of Theorem 4.2 follows a similar strategy as the online one: first, derive a recursion for the quantity \(\delta_{t,1}^{\mathrm{SGDw}}\coloneqq\mathbb{E}\|\psi_{t,1}^{\mathrm{SGDw}}- \psi^{*}\|^{2}\), then unroll it explicitly to obtain a final bound on \(\delta_{T,1}^{\mathrm{SGDw}}\). The main difference to online CD is the presence of an additional offline-specific correlation between the iterates and the data. We thus break down the proof into three steps: (1) deriving a controllable, uniform-in-time upper bound of the data-iterate correlations, (2) deriving and unrolling a recursion on \(\delta_{t,1}^{\mathrm{SGDw}}\) containing this new term, and (3) controlling that term to obtain a final bound on \(\delta_{T,1}^{\mathrm{SGDw}}\).

Step 1:characterizing the data-iterate correlations in offline CDIn offline CD, at each epoch \(t\geq 1\), the iterate \(\psi_{t-1,1}^{\mathrm{SGDw}}\) and the data samples \(X_{i}\) are correlated: this is because these samples may have been used in previous epochs \(t^{\prime}<t-1\) to obtain the \(\psi_{t^{\prime},1}^{\mathrm{SGDw}}\), which themselves influenced \(\psi_{t-1,1}\). With such correlations, we now have \(\mathbb{P}(X_{i}|\psi_{t-1,1}^{\mathrm{SGDw}})\neq\mathbb{P}(X_{i})\), preventing us from obtaining an unrollable recursion on \(\delta_{t,1}^{\mathrm{SGDw}}\) by first marginalizing \(X_{i}\) out to obtain an upper bound of \(\mathbb{E}\left[\big{\|}\psi_{t,1}^{\mathrm{SGDw}}-\psi^{*}\big{\|}^{2}|\psi_{t -1,1}^{\mathrm{SGDw}}\right]\) that only depends on \(\big{\|}\psi_{t-1,1}^{\mathrm{SGDw}}-\psi^{*}\big{\|}\), and then marginalizing over \(\psi_{t-1,1}^{\mathrm{SGDw}}\) to obtain a recursion as in Lemma 3.1. As this problem would not have occurred had we used "fresh samples" (e.g. i.i.d copies of \(X_{i}\) not present in the training data) to perform our update, the core of the proof lies in controlling the following quantity:

\[\Delta(\psi_{t,1}^{\mathrm{SGDw}})\coloneqq\!\!\left\|\,\frac{1}{n}\sum_{i\leq n }\left(\mathbb{E}\big{[}\phi\big{(}K_{i;\psi_{t,1}^{\mathrm{SGDw}}}^{m}(X_{i}) \big{)}\big{|}\psi_{t,1}^{\mathrm{SGDw}},X_{i}\big{]}-\mathbb{E}\big{[}\phi \big{(}K_{i;\psi_{t,1}^{\mathrm{SGDw}}}^{m}(X_{1}^{\prime})\big{)}\big{|}\, \psi_{t,1}^{\mathrm{SGDw}}\big{]}\big{)}\right\|\]

where \(X_{1}^{\prime}\) is an i.i.d. copy of \(X_{1}\). \(\Delta(\psi_{t,1}^{\mathrm{SGDw}})\) is the expected (over the data and iterates) error between a quantity that allows to obtain a recursion (the rightmost term) and the one actually used by offline CD (the leftmost term). To control it, we upper-bound it using a tail decomposition:

\[\mathbb{E}[\Delta(\psi_{t,1}^{\mathrm{SGDw}})^{2}]\leq\epsilon^{2}+(\sup_{t} \mathbb{E}[\Delta(\psi_{t,1}^{\mathrm{SGDw}})^{\nu}])^{2/\nu}\sup_{t} \mathbb{P}\left(\Delta(\psi_{t,1}^{\mathrm{SGDw}})>\epsilon\right)^{\frac{ \nu-2}{\nu}}\coloneqq\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon)^{2}\] (10)

We invoke an additional assumption to ensure that \((\mathbb{E}[\Delta(\psi_{t,1}^{\mathrm{SGDw}})^{\nu}])^{2/\nu}\) is finite; in the results of [21], this is automatically implied by assumptions A4 and A6. For simplicity we assume the same bounding constant \(\kappa_{\nu;m}\).

**Assumption A7**.: There exists \(\nu\geq 2\) s.t. for all \(m\in\mathds{N}\), \(\kappa_{\nu;m}\) from A4 moreover verifies

\[\sup_{\psi\in\Psi}\,\left(\mathbb{E}\big{\|}\phi(K_{\psi}^{m}(X_{1}))-\mathbb{ E}[\phi(K_{\psi}^{m}(X_{1}))]\big{\|}^{\nu}\right)^{1/\nu}\;\leq\;\kappa_{\nu;m}\;.\]

Note the similarity of this assumption with assumption A4: the only difference is that \(X_{1}\) is now a random training point instead of an deterministic (arbitrary) one. Ensuring assumption A7 in addition to assumption A4 thus requires controlling a \(\nu\)-th order moment, instead of all moments as implied by assumption A6.

Step 2: Deriving and unrolling the recursion on \(\delta_{t,1}^{\mathrm{SGDw}}\)The right-hand side of Equation (10) does not depend on \(t\), allowing for the derivation of an "unrollable" recursion on \(\delta_{t,1}^{\mathrm{SGDw}}\) and its subsequent unrolling, which is performed in the following theorem. For simplicity, we again assume \(\beta=0\) and \(N=1\) and defer the general case to Theorem B.1 in appendix.

**Theorem 4.3** (Convergence up to a tail control).: _Assume A1, A2, A3, A4 and A7. Let \(\eta_{t}=C\) for some \(C>0\), and assume that \(\tilde{\mu}_{m}\;=\;\mu-\alpha^{m}\sigma C_{\chi}\;>\;4CL^{2}\). Then for any \(\epsilon>0\),_

\[\sqrt{\delta_{T,1}^{\mathrm{SGDw}}}\leq E_{1}^{T,1}\sqrt{\delta_{0,0}^{ \mathrm{SGDw}}}\;+\;C\left(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon) +\frac{5\sigma+5\kappa_{\nu,m}}{\sqrt{n}}\right)\left(\frac{e^{\frac{\tilde{ \mu}_{m}}{2}C}}{\tilde{\mu}_{m}C}+\frac{E_{2}^{T,1}}{L^{2}C^{2}}\,\right)\]

_where \(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon)\) is defined in Equation (10)._

Note that in the general, non-full batch \(B\leq n\) case, \(\frac{5\sigma+5\kappa_{\nu,m}}{\sqrt{n}}\) is replaced by \(\frac{5\sigma+5\kappa_{\nu,m}}{\sqrt{B}}\) (see Theorem B.1). Under our bounds, obtaining consistency thus requires setting \(B\equiv B(n)\underset{n\rightarrow\infty}{\rightarrow}+\infty\).

Step 3: Controlling the tail probability termTheorem 4.3 is just one step away from the final bound of Theorem 4.2: it remains to control the tail term \(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon)\). Under the assumptions of [21], minimizing \(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon)\) over \(\epsilon\) yields the following result:

**Lemma 4.4**.: _Assume the setup of Theorem 4.2. Let \(n\in\mathds{N}\) be sufficiently large s.t. \(\frac{\log n}{n}<\frac{\sigma_{n}^{2}\zeta_{m}^{2}}{p+\nu-2}\). Denote \(r_{\Psi}\) as the radius of the smallest sphere in \(\mathbbm{R}^{p}\) that contains \(\Psi\), which is finite under A1. Then \(\inf_{\epsilon>0}\!\!\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(\epsilon)\leq\)_

\[\left(\frac{3\sigma_{m}\sqrt{p((\nu-2)p+2\nu)}}{\sqrt{\nu-2}}+ \kappa_{\nu;m}2^{\frac{\nu-2}{2\nu}}(r_{\Psi})^{\frac{(\nu-2)p}{2\nu}}\Big{(} 1+\frac{2C_{m}(\nu-2)^{1/2}}{\sigma_{m}p^{1/2}((\nu-2)p+2\nu)^{1/2}}\Big{)} ^{\frac{(\nu-2)p}{2\nu}}\right)\frac{\sqrt{\log n}}{\sqrt{n}}.\]

To obtain this result, we control the moment term \((\sup_{t}\mathbb{E}[\Delta(\psi_{t,1}^{\mathrm{SGDw}})^{\nu}])\) using A7, and we control the tail probability term \(\sup_{t}\mathbb{P}(\Delta(\psi_{t,1}^{\mathrm{SGDw}})^{\nu}])\) as in [21, Lemma 3.1] using an union bound, a covering argument and A6. Theorem 4.2 then follows by plugging Lemma 4.4 into Theorem 4.3.

### Consistency of offline CD: beyond subexponential tails.

As discussed above, the general unrolling result of Theorem 4.3 holds without the subexponentiality assumption A6; this assumption was only used in Lemma 4.4 to control \(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon)\). We now discuss two alternative ways to control this quantity without requiring subexponential tails. The first generalizes the idea of Jiang et al. [21], while the second exploits mixing of the Markov chain \(K_{\psi}^{m}(x)\) as \(m\rightarrow\infty\). As before we only state partial results (full batch, \(\beta=0\)) and defer the full explicit bounds to Appendix B.3.

Control via Markov InequalityOur first alternative uses Markov Inequality to yield the following.

**Theorem 4.5**.: _Assume the setup of Theorem 4.3 and additionally that A5 holds. Then_

\[\inf_{\epsilon>0}\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(\epsilon) \leq\tilde{C}(p,\nu,m,\Psi)\,n^{-\frac{(\nu-2)\nu}{2(\nu^{2}+(\nu -2)p)}}\,\] \[\lim_{T\to\infty}\sqrt{\delta_{T,1}^{\mathrm{SGDw}}} \leq\tilde{C}^{\prime}(p,\nu,m,\Psi)\big{(}n^{-\frac{(\nu-2)\nu} {2(\nu^{2}+(\nu-2)p)}}+\frac{1}{\sqrt{n}}\big{)},\]

_where \(\tilde{C}\) and \(\tilde{C}^{\prime}\) are functions whose explicit expressions are given in Lemma B.4 in the appendix._

In the case \(p=1\) and \(\nu=3\), the sub-optimal error from Theorem 4.5 reads \(O(n^{-3/20})\). Theorems 4.2 and 4.5 reveal that, depending on the tail condition imposed on the noise introduced by the Markov kernel, the convergence rate of offline CD varies: A subexponential tail, as assumed in prior work, in fact leads to near-parametric rate. Meanwhile, consistency can be obtained without assuming subexponentiality, albeit at a sub-optimal rate.

Control via Markov chain mixing.Alternatively, notice that \(\mathbb{E}[\Delta(\psi_{t,1}^{\mathrm{SGDw}})^{2}]\) involves an average of \(\mathbb{E}\big{[}\phi\big{(}K_{\psi_{t,1}^{\mathrm{SGDw}}}^{m}(X_{i})^{2} \big{)}\big{|}X_{i},\psi_{t,1}^{\mathrm{SGDw}}\big{]}-\mathbb{E}\big{[}\phi \big{(}K_{\psi_{t,1}^{\mathrm{SGDw}}}^{m}(X_{1}^{\prime})\big{)}\big{|}\psi_{ t,1}^{\mathrm{SGDw}}\big{]}\). When \(m\to\infty\), the effect of initialization vanishes, and one may expect the difference to converge to zero. We defer to Lemma B.5 in the appendix to show that, under a \(\phi\)-discrepancy mixing condition ([35]) with a mixing coefficient \(\tilde{\alpha}\in[0,1)\),

\[\inf_{\epsilon>0}\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(\epsilon)\ =O(\kappa_{\nu;m}\tilde{\alpha}^{\frac{(\nu-2)m}{3\nu-2}})\quad\text{ and}\quad\lim_{T\to\infty}\sqrt{\delta_{T,1}^{\mathrm{SGDw}}}\ =O\Big{(}\kappa_{\nu;m}\tilde{\alpha}^{\frac{(\nu-2)m}{3\nu-2}}+\frac{\sigma+ \kappa_{\nu;m}}{\sqrt{n}}\Big{)}\.\]

As \(m\to\infty\), this recovers the parametric rate \(O(n^{-1/2})\). This alternative convergence guarantee comes at the cost of requiring \(m\), the number of Markov chain steps, to grow with the sample size \(n\).

**Remark** (Examples).: In our main results (Theorems 3.2, 3.3 and 4.3) and the tail condition for offline SGD (Theorem 4.2), we employed a weaker set of assumptions than those in [21] (except for the mild \(\nu>2\) moment assumption in (A4)). Consequently, our results apply to all three examples studied in [21]: A bivariate Gaussian model with unknown mean and random-scan Gibbs sampler, a fully visible Boltzmann machine with random-scan Gibbs sampler, and an exponential-family random graph model with a Metropolis-Hastings sampler.

## 5 Related Work

Central to this paper is the prior work of Jiang et al. [21], which provided a rigorous theoretical foundation to analyze the convergence of full-batch CD, and which we refine. The study of optimization with biased gradient descent has attracted a lot of attention in recent years [36; 37; 38; 39]. These works, while closely connected to ours, analyze algorithms with different implementation choices than the CD algorithm: i.i.d. noise setup [36], or setup where a persistent Markov chain is maintained through the iterations [36; 37; 38; 39]. The latter is akin to a variant of the CD algorithm, called the persistent CD [40]. In contrast, our analysis focus on the CD algorithm that restarts a batch of Markov chains from the data distribution at every iteration. Finally, there is a rich body of work on convergence guarantees for offline multi-pass SGD [41; 42; 43; 44; 45; 46]. A notable difference of our analysis is that we are primarily concerned with statistical errors associated with convergence to the true parameter \(\psi^{*}\) in number of samples \(n\), and not the commonly studied convergence rate in number of epochs \(T\). Consequently, most of our work for the offline setup goes into handling the correlations that accumulate by reusing data across epochs.

## 6 Discussion

In this work, we provide a non-asymptotic analysis of the Contrastive Divergence algorithms, showing, in the online setting, their potential to converge at the parametric rate and to have near-optimal asymptotic variance, and proving a near-parametric rates in the offline setting, significantly extending prior results. Our results apply to unnormalized exponential families: despite their flexibility, these models only cover log-densities with linear relationships on the model parameters. We believe that extending our results to more general forms of unnormalized models is an important direction for future work.

### Acknowledgments and Disclosure of Funding

All authors acknowledge support from the Gatsby Charitable Foundation.

## References

* [1] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. _Proceedings of the National Academy of Sciences_, 117(48):30055-30062, 2020.
* [2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [4] Aad W Van der Vaart. _Asymptotic statistics_, volume 3. Cambridge university press, 2000.
* [5] Holden Lee, Chirag Pabbaraju, Anish Prasad Sevekari, and Andrej Risteski. Pitfalls of gaussians as a noise distribution in nce. In _The Eleventh International Conference on Learning Representations_, 2023.
* [6] Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, 2010.
* [7] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 2005.
* [8] Frederic Koehler, Alexander Heckett, and Andrej Risteski. Statistical efficiency of score matching: The view from isoperimetry. In _The Eleventh International Conference on Learning Representations_, 2022.
* [9] Chirag Pabbaraju, Dhruv Rohatgi, Anish Prasad Sevekari, Holden Lee, Ankur Moitra, and Andrej Risteski. Provable benefits of score matching. _Advances in Neural Information Processing Systems_, 36, 2024.
* [10] George Casella and Roger L Berger. _Statistical inference_. Cengage Learning, 2021.
* [11] Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann machines and deep belief networks. _Neural computation_, 20(6):1631-1649, 2008.
* [12] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. _Advances in Neural Information Processing Systems_, 32, 2019.
* [13] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. _Neural computation_, 2002.
* [14] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of mcmc-based maximum likelihood learning of energy-based models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020.
* [15] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run mcmc toward energy-based model. _Advances in Neural Information Processing Systems_, 2019.
* [16] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence training of energy- based models. _arXiv preprint arXiv:2021.01316_, 2021.
* [17] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based constrained text generation with langevin dynamics. _Advances in Neural Information Processing Systems_, 35:9538-9551, 2022.
* [18] Deqian Kong, Bo Pang, Tian Han, and Ying Nian Wu. Molecule design by latent space energy-based modeling and gradual distribution shifting. In _Uncertainty in Artificial Intelligence_, pages 1109-1120. PMLR, 2023.

* [19] Csilla Varnai, Nikolas S Burkoff, and David L Wild. Efficient parameter estimation of generalizable coarse-grained protein force fields using contrastive divergence: a maximum likelihood approach. _Journal of chemical theory and computation_, 9(12):5718-5733, 2013.
* [20] Pierre Glaser, Michael Arbel, Arnaud Doucet, and Arthur Gretton. Maximum likelihood learning of energy-based models for simulation-based inference. 2022.
* [21] Bai Jiang, Tung-Yu Wu, Yifan Jin, and Wing H Wong. Convergence of contrastive divergence algorithm in exponential family. _The Annals of Statistics_, 46(6A):3067-3098, 2018.
* [22] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. _Advances in neural information processing systems_, 24, 2011.
* [23] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. _SIAM journal on control and optimization_, 30(4):838-855, 1992.
* [24] Lawrence D Brown. Fundamentals of statistical exponential families: with applications in statistical decision theory. Ims, 1986.
* [25] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. _Foundations and Trends(r) in Machine Learning_, 1(1-2):1-305, 2008.
* [26] Charles J Geyer. Introduction to markov chain monte carlo. _Handbook of markov chain monte carlo_, 20116022:45, 2011.
* [27] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* [28] Yang Song and Diederik P Kingma. How to train your energy-based models. _arXiv preprint arXiv:2101.03288_, 2021.
* [29] Joseph L Doob. _Measure theory_, volume 143. Springer Science & Business Media, 2012.
* [30] David A Levin and Yuval Peres. _Markov chains and mixing times_, volume 107. American Mathematical Soc., 2017.
* [31] Yurii Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2013.
* [32] Gary Harris and Clyde Martin. Shorter notes: The roots of a polynomial vary continuously as a function of the coefficients. _Proceedings of the American Mathematical Society_, pages 390-392, 1987.
* [33] Frank Nielsen and Richard Nock. On the chi square and higher-order chi distances for approximating f-divergences. _IEEE Signal Processing Letters_, 21(1):10-13, 2013.
* [34] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. _Analysis and geometry of Markov diffusion operators_, volume 103. Springer, 2014.
* [35] Maxim Rabinovich, Aaditya Ramdas, Michael I Jordan, and Martin J Wainwright. Function-specific mixing times and concentration away from equilibrium. 2020.
* [36] Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased stochastic approximation scheme. In _Conference on Learning Theory_, pages 1944-1974. PMLR, 2019.
* [37] Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. _Advances in neural information processing systems_, 31, 2018.
* [38] Thinh T Doan. Finite-time analysis of markov gradient descent. _IEEE Transactions on Automatic Control_, 68(4):2140-2153, 2022.
* [39] Yves F Atchade, Gersende Fort, and Eric Moulines. On perturbed proximal gradient algorithms. _Journal of Machine Learning Research_, 18(10):1-33, 2017.

* [40] Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In _Proceedings of the 25th international conference on Machine learning_, 2008.
* [41] Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability of randomized learning algorithms. _Journal of Machine Learning Research_, 6(1), 2005.
* [42] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pages 1225-1234. PMLR, 2016.
* [43] Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. _Journal of Machine Learning Research_, 18(97):1-47, 2017.
* [44] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. _Advances in Neural Information Processing Systems_, 31, 2018.
* [45] Nicole Mucke, Gergely Neu, and Lorenzo Rosasco. Beating sgd saturation with tail-averaging and minibatching. _Advances in Neural Information Processing Systems_, 32, 2019.
* [46] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Risk bounds of multi-pass sgd for least squares in the interpolation regime. _Advances in Neural Information Processing Systems_, 35:12909-12920, 2022.
* [47] Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. Sgd without replacement: Sharper rates for general smooth convex functions. In _International Conference on Machine Learning_, pages 4703-4711. PMLR, 2019.
* [48] Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of sgd without replacement. In _International Conference on Machine Learning_, pages 7964-7973. PMLR, 2020.
* [49] Dimitri Bertsekas. _Convex optimization theory_, volume 1. Athena Scientific, 2009.
* [50] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* [51] S. W. Dharmadhikari, V. Fabian, and K. Jogdeo. Bounds on the moments of martingales. _Ann. Math. Statist._, pages 1719-1723, 1968.

## Supplementary Material for "Near-Optimality of Contrastive Divergence Algorithms"

The supplementary material provides the proofs of the main results of the paper:

Section B states full explicit bounds for the offline CD algorithm.

Section C collects a list of useful tools for our proofs. These include the properties of \(\varphi_{\gamma}\) introduced before Theorem 3.2 in the main text, as well as several contraction and integrability results.

Section D provides the proofs for the online CD algorithm.

Sections E, F and G contain the proofs about the offline CD algorithm and the tail control.

## Appendix A Notations

Throughout the proofs, we will denote by \(P_{\psi}^{m}\) the following operator from \(L^{2}(p_{\psi})\) to itself:

\[P_{\psi}^{m}f(x)\coloneqq\int k^{m}(x,x^{\prime})f(x^{\prime})p_{\psi}(x^{ \prime})\mathrm{d}x^{\prime}.\] (11)

Here, \(k^{m}(x,x^{\prime})\) is the \(m\)-iterated version of some Markov transition kernel \(k_{\psi}\), e.g.:

\[k_{\psi}^{m}(x,x^{\prime})\coloneqq\int k_{\psi}(x,x_{1})\dots k_{\psi}(x_{m- 2},x_{m-1})\dots k_{\psi}(x_{m-1},x^{\prime})\mathrm{d}x_{1}\dots\mathrm{d}x_{ m-1}.\] (12)

\(\mathrm{Proj}_{\Psi}:\mathbb{R}^{p}\longmapsto\Psi\) denotes the projection operator onto the convex set \(\Psi\), e.g.

\[\mathrm{Proj}_{\Psi}(\psi)\coloneqq\operatorname*{argmin}_{\psi^{\prime}\in \Psi}\left\|\psi-\psi^{\prime}\right\|.\]

We also frequently use the following function, used in standard convex optimization results [22].

\[\varphi_{\gamma}(t)=\begin{cases}\frac{t^{\gamma}-1}{\gamma}&\text{ if }\gamma \neq 0\\ \log t&\text{ if }\gamma=0\end{cases}\]

which is defined on \(\mathbb{R}_{+}\setminus\{0\}\).

## Appendix B Additional results for offline SGD

In this section, we provide the full statements on error bounds for SGD with replacement (SGDw), SGD with reshuffling (SGDo) and tail moment bounds, which complement the results in Section 4. Proofs are deferred to Appendix F, which make use of \(L_{2}\) approximation by auxiliary gradient updates derived in Appendix E.

NotationsDenote the SGDw iterates by \((\psi_{t,j}^{\mathrm{SGDw}})_{t\in\mathbb{N},j\leq N}\) and let \(X_{1}^{\prime}\) be an i.i.d. copy of \(X_{1}\). Throughout the remaining of the appendix, we define given \(\epsilon>0\) and \(n\in\mathbb{N}\)

\[\vartheta_{n,m,T}^{\mathrm{SGDw}}(\epsilon) \coloneqq\sup_{\begin{subarray}{c}t\in[T]\\ j\in[N]\end{subarray}}\mathbb{P}\Bigg{(}\frac{\left\|\sum_{i=1}^{n}\left( \mathbb{E}\big{[}\phi\big{(}K_{\psi_{t,j}^{\mathrm{SGDw}}}^{m}(X_{i})\big{)} \big{|}X_{i},\psi_{t-1,j}^{\mathrm{SGDw}}\big{]}-\mathbb{E}\big{[}\phi\big{(} K_{\psi_{t,j}^{\mathrm{SGDw}}}^{m}(X_{1}^{\prime})\big{)}\big{|}\psi_{t-1,j}^{ \mathrm{SGDw}}\big{]}\right)\right\|}{n}\!>\epsilon\Bigg{)}.\] \[=\sup_{t,j}\mathbb{P}\langle\Delta(\psi_{t,j}^{\mathrm{SGDw}})>\epsilon\]

and \(\vartheta_{n,m,T}^{\mathrm{SGDw}}(\epsilon)\) analogously. Using these notations, we can redefine the quantity \(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon)\) in the main as \(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}(\epsilon)\coloneqq\sqrt{\epsilon^{2 }+\kappa_{\nu;m}^{2}\big{(}\vartheta_{n,m,T}^{\mathrm{SGDw}}(\epsilon)\big{)} ^{\frac{\nu-2}{\nu}}}\).

### An explicit finite-sample bound for SGDw

In the result below, we write \(\delta_{t,j}^{\mathrm{SGDw}}\coloneqq\mathbb{E}\left\|\psi_{t,j}^{\mathrm{SGDw}}- \psi^{*}\right\|^{2}\) and, for a fixed \(\epsilon>0\), the quantity

\[\sigma_{n,T}^{\mathrm{SGDw}}\ =\varepsilon_{n,m,T;\nu}^{\mathrm{SGDw}}( \epsilon)+\frac{5\sigma+5\kappa_{m}}{\sqrt{B}}\ =\ \sqrt{\epsilon^{2}+\kappa_{m}^{2}\big{(}\vartheta_{n,m,T}^{ \mathrm{SGDw}}(\epsilon)\big{)}^{\frac{\nu-2}{\nu}}}+\frac{5\sigma+5\kappa_{m }}{\sqrt{B}}\.\]

**Theorem B.1**.: _Assume A1 (where \(\Psi\) may be non-compact), A2, A3, A4 and A7. Let \(\eta_{t}=Ct^{-\beta}\) for some \(\beta\in[0,1]\) and \(C>0\), and assume that \(m>\frac{\log(\sigma C_{\chi}/\mu)}{\log|\alpha|}\) s.t. \(\tilde{\mu}_{m}=\mu-\alpha^{m}\sigma C_{\chi}>0\) as in Theorem 3.2. Then for any \(\epsilon>0\), \(\sqrt{\delta_{T,N}^{\mathrm{SGDw}}}\) is upper bounded by_

\[\begin{cases}E_{1}^{T,N}\sqrt{\delta_{0,0}^{\mathrm{SGDw}}}\,+\,C\sigma_{n,T }^{\mathrm{SGDw}}\Big{(}\frac{4e^{\frac{\tilde{\mu}_{m}CN}{(T+1)^{1/2}}}}{ \tilde{\mu}_{m}C}+2N(1+\tilde{\mu}_{m}C)^{N-1}\varphi_{\frac{1}{2}-L^{2}C^{2}N }(T+1)\,E_{2}^{T,N}\Big{)}\\ \hskip 113.811024pt\text{for }\beta=\frac{1}{2}\,\end{cases}\]

_where \(E_{1}^{T,N}\) and \(E_{2}^{T,N}\) are two decreasing functions in \(T\) defined by_

\[E_{1}^{T,N} \coloneqq\ \exp\Big{(}1-N\tilde{\mu}_{m}C\varphi_{1-\beta}(T+1)+ \frac{NL^{2}C^{2}}{2}\varphi_{1-2\beta}(T+1)\Big{)}\,\] \[E_{2}^{T,N} \coloneqq\ \exp\Big{(}-\frac{N\tilde{\mu}_{m}C}{2}\varphi_{1-\beta}(T+1)+2NL^ {2}C^{2}\varphi_{1-2\beta}(T+1)\Big{)}\.\]

We emphasize that the full result above holds for any \(\beta\in[0,1]\), which in particular includes the constant step size \(\beta=0\) regime considered by [21]. When \(\beta=0\), for \(E_{1}^{T,N}\) and \(E_{2}^{T,N}\) to decay to zero as \(T\to\infty\), we additionally need the condition

\[\tilde{\mu}_{m}\ =\ \mu-\alpha^{m}\sigma C_{\chi}\ >\ 4CL^{2}\.\]

This is almost identical to the condition used in [21, Equation 2.5, Theorem 2.1], except that \(4L^{2}\) gets replaced by \(\frac{1}{2}(L+\alpha^{m}\sigma C_{\chi})^{2}\). Notably this says that an additional step size condition is needed for our results to hold in the constant step size regime, but not necessary for a decreasing step size.

### Results for SGDo

SGD with reshuffling (SGDo, also called SGD without replacement) is an optimization scheme that is also widely used in practice compared to SGDo and online SGD. In the context of CD, it corresponds to Algorithm 2 with batches chosen as

\[(B_{t,1},\ldots,B_{t,N})\ =\pi(\{1,\ldots,n\})\,\]

where \(\pi\) is a uniform draw of the permutation group on \(n\) elements. We denote the iterates of SGDo \((\psi_{t,j}^{\mathrm{SGDo}})_{t\in\mathds{N},j\in[N]}\). Analogously to \(\vartheta_{n,m,T}^{\mathrm{SGDw}}\), we define, for \(X_{1}^{\prime}\) an i.i.d. copy of \(X_{1}\), \(\epsilon>0\) and \(n\in\mathds{N}\), the tail probability term

\[\vartheta_{n,m,T}^{\mathrm{SGDo}}(\epsilon)\coloneqq\sup_{\begin{subarray}{ c}t\in[T]\\ j\in[N]\end{subarray}}\mathbb{P}\left(\frac{\left\|\sum_{i=1}^{n}\big{(} \mathbb{E}\big{[}\phi\big{(}K_{\psi_{t-1,j}^{\mathrm{SGDo}}}^{m}(X_{i}) \big{)}\big{|}X_{i},\psi_{t-1,j}^{\mathrm{SGD}}\big{]}-\mathbb{E}\left[\phi \big{(}K_{\psi_{t-1,j}^{\mathrm{SGDo}}}^{m}(X_{1}^{\prime})\big{)}\big{|} \psi_{t-1,j}^{\mathrm{SGDo}}\big{]}\right)\right\|}{n}>\epsilon\right)\.\]

Also denote \(\varepsilon_{n,m,T;\nu}^{\mathrm{SGDo}}(\epsilon)=\sqrt{\epsilon^{2}+\kappa_{m}^ {2}\big{(}\vartheta_{n,m,T}^{\mathrm{SGDo}}(\epsilon)\big{)}^{\frac{\nu-2}{ \nu}}}\) and \(\sigma_{n,T}^{\mathrm{SGDw}}=\varepsilon_{n,m,T;\nu}^{\mathrm{SGDo}}(\epsilon)+ \frac{5\sigma+5\kappa_{m}}{\sqrt{B}}\). The following result says that \(\psi_{t,j}^{\mathrm{SGDo}}\) enjoys exactly the same convergence guarantee as \(\psi_{t,j}^{\mathrm{SGDo}}\) in Theorem 4.3. The statement is identical to that of Theorem B.1 and is stated in full for completeness; see Appendix F.2 for the proof, which is a slight adaptation of the proof for Theorem B.1. As before we write \(\delta_{t,j}^{\mathrm{SGDo}}\coloneqq\mathbb{E}\left\|\psi_{t,j}^{\mathrm{SGDo}}- \psi^{*}\right\|^{2}\).

**Theorem B.2** (Convergence of CD-SGDo).: _Assume A1 (where \(\Psi\) may be non-compact), A2, A3, A4 and A7. Let \(\eta_{t}=Ct^{-\beta}\) for some \(\beta\in[0,1]\) and \(C>0\), and assume that \(m>\frac{\log(\sigma C_{\chi}/\mu)}{\log|\alpha|}\) s.t. \(\tilde{\mu}_{m}=\mu-\alpha^{m}\sigma C_{\chi}>0\) as in Theorem 3.2. Then for any \(\epsilon>0\), \(\sqrt{\delta^{\mathrm{SGDo}}_{T,N}}\) is upper bounded by_

\[\begin{cases}E_{1}^{T,N}\sqrt{\delta^{\mathrm{SGDo}}_{0,0}}\,+\,C \sigma^{\mathrm{SGDo}}_{n,T}\Big{(}\frac{\frac{\tilde{\mu}_{m}CN}{4e^{(T+1)^{ 1/2}}}}{\tilde{\mu}_{m}C}+2N(1+\tilde{\mu}_{m}C)^{N-1}\varphi_{\frac{1}{2}-L^ {2}C^{2}N}(T+1)\,E_{2}^{T,N}\Big{)}\\ &\text{for }\beta=\frac{1}{2}\,\\ E_{1}^{T,N}\sqrt{\delta^{\mathrm{SGDo}}_{0,0}}\,+C\sigma^{\mathrm{SGDo}}_{n,T }\Big{(}\frac{4}{\tilde{\mu}_{m}C}+\frac{3N\big{(}1+\frac{L^{2}C^{2}}{2}\big{)} ^{N-1}\,e^{2L^{2}C^{2}N}\,\log(T+1)}{(T+1)(\tilde{\mu}_{m}CN)/2}\Big{)}&\text{ for }\beta=1\,\\ E_{1}^{T,N}\sqrt{\delta^{\mathrm{SGDo}}_{0,0}}\,+\,C\sigma^{\mathrm{SGDo}}_{n,T }\Big{(}\frac{2^{2\beta+1}}{\tilde{\mu}_{m}C}\frac{\tilde{\mu}_{m}C}{e^{2(1- \beta)}\,\frac{N}{(T+1)^{\beta}}}+\frac{3^{\beta}(1+\tilde{\mu}_{m}C)^{N-1}(T+ 2)^{\beta}}{L^{2}C^{2}}\,E_{2}^{T,N}\Big{)}&\text{otherwise}\,\end{cases}\]

_where \(E_{1}^{T,N}\) and \(E_{2}^{T,N}\) are two decreasing functions in \(T\) defined by_

\[E_{1}^{T,N} \coloneqq\,\exp\Big{(}1-N\tilde{\mu}_{m}C\varphi_{1-\beta}(T+1)+ \frac{NL^{2}C^{2}}{2}\varphi_{1-2\beta}(T+1)\Big{)}\,\] \[E_{2}^{T,N} \coloneqq\,\exp\Big{(}-\frac{N\tilde{\mu}_{m}C}{2}\varphi_{1- \beta}(T+1)+2NL^{2}C^{2}\varphi_{1-2\beta}(T+1)\Big{)}\.\]

**Remark.** We also remark that existing works [47, 48] show that the standard SGDo typically gives a faster convergence rate in \(T\) than SGDw. An analogous result for the CD setup would involve additional technical hurdles of jointly controlling the correlations across minibatches and from reusing data samples, and we defer this to future work.

### Explicit tail control

We now provide the full explicit tail control bounds. All results in this section hold directly for \(\varepsilon^{\mathrm{SGDo}}_{\nu;n,m,T}(\epsilon)\) and \(\delta^{\mathrm{SGDo}}_{T,N}\), and we omit them here. In the result below, we denote \(r_{\Psi}\) as the radius of the smallest sphere in \(\mathbbm{R}^{p}\) that contains \(\Psi\), which is finite under A1.

**Lemma B.3**.: _Assume A5 and A6. Let \(n\in\mathbbm{N}\) be sufficiently large s.t. \(\frac{\log n}{n}<\frac{\sigma_{n}^{2}\zeta_{m}^{2}}{p+\nu-2}\). Then_

\[\inf_{\epsilon>0} \varepsilon^{\mathrm{SGDw}}_{\nu;n,m,T}(\epsilon)\ \leq\] \[\left(\frac{3\sigma_{m}\sqrt{p((\nu-2)p+2\nu)}}{\sqrt{\nu-2}}+ \kappa_{\nu;m}2^{\frac{\nu-2}{2\nu}}(r_{\Psi})^{\frac{(\nu-2)p}{2\nu}}\Big{(} 1+\frac{2C_{m}(\nu-2)^{1/2}}{\sigma_{m}p^{1/2}((\nu-2)p+2\nu)^{1/2}}\Big{)}^{ \frac{(\nu-2)p}{2\nu}}\right)\frac{\sqrt{\log n}}{\sqrt{n}}\.\]

_In particular, if we additionally assume the conditions of Theorem B.1, we have_

\[\lim_{T\to\infty}\sqrt{\delta^{\mathrm{SGDw}}_{T,N}}\ \leq\ C^{\prime}(p,\nu,m,\Psi,\beta)\Big{(} \frac{\sqrt{\log n}}{\sqrt{n}}+\frac{1}{\sqrt{B}}\Big{)}\]

_where_

\[C^{\prime}(p,\nu,m,\Psi,\beta)\ \coloneqq\ \frac{8(1+5\sigma+5\kappa_{m})}{ \tilde{\mu}_{m}}\] \[\times\ \left(\frac{3\sigma_{m}\sqrt{p((\nu-2)p+2\nu)}}{\sqrt{\nu-2}}+ \kappa_{\nu;m}2^{\frac{\nu-2}{2\nu}}(r_{\Psi})^{\frac{(\nu-2)p}{2\nu}}\Big{(} 1+\frac{2C_{m}(\nu-2)^{1/2}}{\sigma_{m}p^{1/2}((\nu-2)p+2\nu)^{1/2}}\Big{)}^{ \frac{(\nu-2)p}{2\nu}}\right)\,.\]

**Lemma B.4**.: _Assume the conditions of Theorem 4.3 and additionally that A5 holds. Then_

\[\inf_{\epsilon>0}\varepsilon^{\mathrm{SGDw}}_{\nu;n,m,T}(\epsilon) \ \leq\big{(}3C_{m}+\kappa^{\nu/2}_{\nu;m}(r_{\Psi})^{\frac{(\nu-2)p}{2 \nu}}C_{m}^{-\frac{\nu-2}{2}}3^{\frac{(\nu-2)p}{2\nu}}\big{)}\,n^{-\frac{(\nu- 2)\nu}{2(\nu^{2}+(\nu-2)p)}}\,\] \[\lim_{T\to\infty}\big{(}\delta^{\mathrm{SGDw}}_{T,N}\big{)}^{1/2}\ \leq\tilde{C}^{\prime}(p,\nu,m,\Psi,\beta)\big{(}n^{-\frac{(\nu-2) \nu}{2(\nu^{2}+(\nu-2)p)}}+B^{-1/2}\big{)}\,\]

_where_

\[\tilde{C}^{\prime}(p,\nu,m,\Psi)\ \coloneqq\ \frac{8(1+5\sigma+5\kappa_{m})}{ \tilde{\mu}_{m}}\big{(}3C_{m}+\kappa^{\nu/2}_{\nu;m}(r_{\Psi})^{\frac{(\nu-2)p}{2 \nu}}C_{m}^{-\frac{\nu-2}{2}}3^{\frac{(\nu-2)p}{2\nu}}\big{)}\.\]The next result considers a \(\phi\)-discrepancy mixing condition ([35]), which is a mixing assumption on \(K_{\psi}^{m}\) but with respect to a specific test function \(\phi\), and we impose it uniformly over \(\psi\in\Psi\). We also recall that \(X_{1}^{\psi}\sim p_{\psi}\).

**Lemma B.5**.: _Assume that there exist \(\tilde{\alpha}\in[0,1)\) and \(\tilde{C}_{K}>0\) such that, for all \(\psi\in\Psi\) and \(x\in\mathcal{X}\), \(\|\mathbb{E}[\phi(K_{\psi}^{m}(x))]-\mathbb{E}[\phi(X_{1}^{\psi})]\|\leq\tilde {C}_{K}\tilde{\alpha}^{m}\). Then_

\[\inf_{\epsilon>0}\,\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(\epsilon)\;\leq\; \big{(}1+2^{\frac{\nu-2}{2\nu}}\kappa_{\nu;m}(\tilde{C}_{K})^{\frac{\nu-2}{2\nu }}\big{)}\tilde{\alpha}^{\frac{(\nu-2)m}{3\nu-2}}\;.\]

_In particular, if we additionally assume the conditions of Theorem 4.3, we have_

\[\lim_{T\to\infty}\sqrt{\delta_{T,N}^{\mathrm{SGDw}}}\;\leq\;\frac{8}{\mu- \alpha^{m}\sigma C_{\chi}}\Big{(}\big{(}1+2^{\frac{\nu-2}{2\nu}}\kappa_{\nu ;m}(\tilde{C}_{K})^{\frac{\nu-2}{2\nu}}\big{)}\tilde{\alpha}^{\frac{(\nu-2)m}{ 3\nu-2}}+\frac{5\sigma+5\kappa_{m}}{\sqrt{B}}\Big{)}\;.\]

## Appendix C Auxiliary Tools

### Properties of \(\varphi_{\gamma}\)

The following lemma collects some identities used in [22].

**Lemma C.1**.: \(\varphi_{\gamma}\) _satisfies the following properties:_

1. \(\varphi_{\gamma}\) _is increasing on_ \(\mathbb{R}_{+}\) _for all_ \(\gamma\) _;_
2. \(\varphi_{\gamma}(t)\leq\frac{t^{\gamma}}{\gamma}\) _for_ \(\gamma>0\)_, and_ \(\varphi_{\gamma}(t)\leq-\frac{1}{\gamma}\) _for_ \(\gamma<0\) _;_
3. \(\varphi_{1-\beta}(t)\geq t^{1-\beta}\) _for_ \(\beta\in(0,1]\) _;_
4. \(\varphi_{\gamma}(t)-\varphi_{\gamma}(\frac{t}{2})\geq\frac{1}{2}x^{\gamma}\) _for_ \(\gamma\in(0,1]\)__.

The next lemma provides some additional results on \(\varphi_{\gamma}\).

**Lemma C.2**.: \(\varphi_{\gamma}\) _satisfies the following properties:_

1. \(\varphi_{\gamma}\) _is positive on_ \(t>0\) _and increasing for every_ \(\gamma\in\mathbbm{R}\)_;_
2. _For_ \(1\leq t_{1}\leq t_{2}\) _and_ \(\beta\geq 0\)_, we have_ \[\varphi_{1-\beta}(t_{2}+1)-\varphi_{1-\beta}(t_{1})\;\leq\;\sum_{t=t_{1}}^{t_ {2}}t^{-\beta}\;\leq\;2\left(\varphi_{1-\beta}(t_{2}+1)-\varphi_{1-\beta}(t_{1 })\right)\,.\] _If instead_ \(\beta<0\)_, we have_ \[\frac{1}{2}\big{(}\varphi_{1-\beta}(t_{2}+1)-\varphi_{1-\beta}(t_{1})\big{)} \;\leq\;\sum_{t=t_{1}}^{t_{2}}t^{-\beta}\;\leq\;\varphi_{1-\beta}(t_{2}+1)- \varphi_{1-\beta}(t_{1})\;;\]
3. _For_ \(1\leq t_{1}\leq t_{2}\) _and_ \(\gamma\neq 0\)_, we have_ \[t_{1}^{\gamma-1}\;\leq\varphi_{\gamma}(t_{2})-\varphi_{\gamma}(t_{1})\;\leq \;t_{2}^{\gamma-1}\hskip 56.905512pt\text{if }\gamma\geq 1\;,\] \[t_{2}^{-(1-\gamma)}\;\leq\varphi_{\gamma}(t_{2})-\varphi_{\gamma}(t _{1})\;\leq\;t_{1}^{-(1-\gamma)}\hskip 56.905512pt\text{if }\gamma\leq 1\;;\]
4. _Let_ \(1\leq t_{1}<t_{2}\) _and_ \(\kappa,\beta\geq 0\)_. If_ \(\kappa\neq 1\) _and_ \(a>0\)_, we have_ \[\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\exp\big{(}a\,\varphi_{1-\kappa}(t-1)\big{)} \;\leq\;\frac{(t_{2}+1)^{\max\{\kappa-\beta,0\}}}{a}\,\exp\big{(}a\,\varphi_{1 -\kappa}(t_{2}+1)\big{)}\;,\] _and if_ \(\kappa\neq 1\) _and_ \(a<0\)_, we have_ \[\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\exp\big{(}a\,\varphi_{1-\kappa}(t)\big{)} \;\leq\;\frac{(t_{2}+1)^{\max\{\kappa-\beta,0\}}}{(-a)}\,\exp\big{(}a\,\varphi_ {1-\kappa}(t_{1})\big{)}\;.\]

Proof of Lemma C.2.: (i) follows from checking \(\gamma>0\), \(\gamma=0\) and \(\gamma<0\) respectively. The first set of bounds in (ii) follow by noting that \(t\mapsto t^{-\beta}\) is decreasing for \(\beta\geq 0\):

\[\sum_{t=t_{1}}^{t_{2}}t^{-\beta}\;\geq\;\int_{t_{1}}^{t_{2}+1}t^{- \beta}dt\;=\;\varphi_{1-\beta}(t_{2}+1)-\varphi_{1-\beta}(t_{1})\;,\] \[\sum_{t=t_{1}}^{t_{2}}t^{-\beta}\;\leq 2\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\;\leq\;2\int_{t_{1}-1}^{t_{2}}(t+1 )^{-\beta}dt\;=\;2\big{(}\varphi_{1-\beta}(t_{2}+1)-\varphi_{1-\beta}(t_{1}) \big{)}\;.\]The second set of bounds follows from noting that \(t\mapsto t^{-\beta}\) is increasing for \(\beta<0\):

\[\sum_{t=t_{1}}^{t_{2}}t^{-\beta} \ \geq\ \frac{1}{2}\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\ \geq\ \frac{1}{2}\int_{t_{1}-1}^{t_{2}}(t+1)^{-\beta}dt\ =\ \frac{1}{2}\big{(}\varphi_{1-\beta}(t_{2}+1)- \varphi_{1-\beta}(t_{1})\big{)}\;,\] \[\sum_{t=t_{1}}^{t_{2}}t^{-\beta} \ \leq\ \int_{t_{1}}^{t_{2}+1}t^{-\beta}dt\ =\ \varphi_{1-\beta}(t_{2}+1)-\varphi_{1-\beta}(t_{1})\;.\]

For (iii), we note that for \(\gamma\neq 0\),

\[\varphi_{\gamma}(t_{2})-\varphi_{\gamma}(t_{1})\ =\frac{t_{2}^{\gamma}-t_{1}^{ \gamma}}{\gamma}\;,\]

so by the mean value theorem,

\[\inf_{t_{1}\leq t\leq t_{2}}t^{\gamma-1}\ \leq\ \varphi_{\gamma}(t_{2})- \varphi_{\gamma}(t_{1})\ \leq\ \sup_{t_{1}\leq t\leq t_{2}}t^{\gamma-1}\;.\]

The desired bounds then follow from an explicit computation of the infimum and the maximum in each of the two cases \(\gamma\geq 1\) and \(\gamma\leq 1\).

For (iv), we first consider the case \(\kappa\neq 1\) and \(a>0\). Then

\[\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\exp\big{(}a\,\varphi_{1- \kappa}(t-1)\big{)}\ =\ e^{-\frac{a}{1-\kappa}}\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\exp\Big{(} \frac{at^{1-\kappa}}{1-\kappa}\Big{)}\] \[\qquad\qquad\qquad\qquad\leq e^{-\frac{a}{1-\kappa}}\max_{t_{1} \leq t\leq t_{2}}\Big{(}\frac{(t+1)^{\kappa}}{(t+1)^{\beta}}\Big{)}\sum_{t=t_ {1}}^{t_{2}}(t+1)^{-\kappa}\exp\Big{(}\frac{at^{1-\kappa}}{1-\kappa}\Big{)}\] \[\qquad\qquad\qquad\qquad\leq e^{-\frac{a}{1-\kappa}}(t_{2}+1)^{ \max\{\kappa-\beta,0\}}\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\kappa}\exp\Big{(}\frac{ at^{1-\kappa}}{1-\kappa}\Big{)}\;.\]

Since, for \(x\geq 0\), \(x\mapsto(x+1)^{-\kappa}\) is decreasing and \(x\mapsto\exp(ax^{1-\kappa}/(1-\kappa))\) is increasing, we have that for \(t_{1}\leq t\leq t_{2}\) and \(x\in[t,t+1]\),

\[(t+1)^{-\kappa}\ \leq\ x^{-\kappa}\qquad\quad\text{ and }\qquad\exp(at^{1-\kappa}/(1-\kappa))\ \leq\ \exp(ax^{1-\kappa}/(1-\kappa))\;.\]

This implies that

\[\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\exp\big{(}a\,\varphi_{1- \kappa}(t-1)\big{)}\] \[\qquad\qquad\qquad\leq(t_{2}+1)^{\max\{\kappa-\beta,0\}}e^{-\frac {a}{1-\kappa}}\sum_{t=t_{1}}^{t_{2}}\int_{t}^{t+1}x^{-\kappa}\exp\Big{(}\frac{ ax^{1-\kappa}}{1-\kappa}\Big{)}\,dx\] \[\qquad\qquad\qquad\qquad\qquad\qquad\leq\frac{(t_{2}+1)^{\max\{ \kappa-\beta,0\}}}{a}\,e^{-\frac{a}{1-\kappa}}\,e^{\frac{a(t_{2}+1)^{1-\kappa}}{ 1-\kappa}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad=\frac{(t_{2 }+1)^{\max\{\kappa-\beta,0\}}}{a}\,\exp\big{(}a\,\varphi_{1-\kappa}(t_{2}+1) \big{)}\;.\]

The main difference in the case \(\kappa\neq 1\) and \(a<0\) is that we now use \(x\mapsto\exp(a(x+1)^{1-\kappa}/(1-\kappa))\) is decreasing to obtain, for \(t_{1}\leq t\leq t_{2}\) and \(x\in[t,t+1]\),

\[\exp(a(t+1)^{1-\kappa}/(1-\kappa))\ \leq\ \exp(ax^{1-\kappa}/(1-\kappa))\;.\]

A similar argument then yields

\[\sum_{t=t_{1}}^{t_{2}}(t+1)^{-\beta}\exp\big{(}a\,\varphi_{1- \kappa}(t)\big{)}\] \[\leq(t_{2}+1)^{\max\{\kappa-\beta,0\}}e^{-\frac{a}{1-\kappa}}\sum_ {t=t_{1}}^{t_{2}}(t+1)^{-\kappa}\exp\Big{(}\frac{a(t+1)^{1-\kappa}}{1-\kappa} \Big{)}\] \[\leq(t_{2}+1)^{\max\{\kappa-\beta,0\}}e^{-\frac{a}{1-\kappa}}\int_ {t_{1}}^{t_{2}+1}x^{-\kappa}\exp\Big{(}\frac{ax^{1-\kappa}}{1-\kappa}\Big{)}dx\] \[=\frac{(t_{2}+1)^{\max\{\kappa-\beta,0\}}}{a}\,\big{(}\exp\big{(}a \,\varphi_{1-\kappa}(t_{2}+1)\big{)}-\exp\big{(}a\,\varphi_{1-\kappa}(t_{1}) \big{)}\big{)}\] \[\leq\frac{(t_{2}+1)^{\max\{\kappa-\beta,0\}}}{(-a)}\,\exp\big{(}a \,\varphi_{1-\kappa}(t_{1})\big{)}\;.\]We also need the following lemma, which is useful for controlling the accumulation of errors from the noise terms over iterations.

**Lemma C.3**.: _For any \(a,b\geq 0\), \(T,N\in\mathbb{N}\) and \(\kappa,\beta\geq 0\) such that \(bt^{-\beta}-at^{-\kappa}\leq 1\) for all \(1\leq t\leq T\), we have that_

\[\prod_{t=1}^{T}(1-bt^{-\beta}+at^{-\kappa})^{N}\ \leq\ \exp\big{(}-bN\,\varphi_{1- \beta}(T+1)+aN\,\varphi_{1-\kappa}(T+1)\big{)}\;.\]

_Moreover, for any \(\zeta\geq 0\), we have that_

\[\sum_{t=1}^{T}t^{-\zeta}\,\Big{(}\sum_{j=1}^{N}(1-bt^{-\beta}+at^ {-\kappa})\Big{)}\,\prod_{s=t+1}^{T}(1-bs^{-\beta}+as^{-\kappa})^{N}\] \[\leq Q_{1}+\exp\Big{(}-\tfrac{bN}{2}\,\varphi_{1-\beta}(T+1)+4aN\, \varphi_{1-\kappa}(T+1)\Big{)}\,Q_{2}\;,\]

_where_

\[Q_{1}\ \coloneqq\ \begin{cases}\frac{2^{2\zeta+1}(T+3)^{\max\{\beta- \zeta,0\}}}{b}\exp\Big{(}\frac{bN}{2(1-\beta)(T+1)^{\beta}}\Big{)}&\text{ if }\beta\neq 1\,,\,b>0\;,\\ 2N\varphi_{1-\zeta+bN/2}(T+1)\exp\Big{(}-\tfrac{bN}{2}\,\varphi_{1-\beta}(T+1) \Big{)}&\text{ if }\beta=1\,\text{or}\,b=0\;,\end{cases}\]

_and_

\[Q_{2}\ \coloneqq\ \begin{cases}\frac{3^{\zeta}(1+a)^{N-1}}{2a}(T+2)^{ \max\{\kappa-\zeta,0\}}&\text{ if }\kappa\neq 1\text{ and }a>0\;,\\ 2N(1+a)^{N-1}\,\varphi_{1-\zeta-2aN}(T+1)&\text{ if }\kappa=1\text{ or }a=0\;.\end{cases}\]

_In the special case \(\zeta=\beta=1<\kappa\), we have_

\[\sum_{t=1}^{T}t^{-\zeta}\,\Big{(}\sum_{j=1}^{N}(1-bt^{-\beta}+at^ {-\kappa})^{j-1}\Big{)}\,\prod_{s=t+1}^{T}(1-bs^{-\beta}+as^{-\kappa})^{N}\] \[\leq\ \tfrac{4}{b}+\frac{3N(1+a)^{N-1}\,e^{\frac{bN}{a-1}}\,\log(T+1)}{(T+1) ^{\frac{bN}{2}}}\;.\]

Proof of Lemma c.3.: By assumption, \(bt^{-\beta}-at^{-\kappa}\leq 1\) for all \(1\leq t\leq T\). Since \(0\leq 1-x\leq e^{-x}\) for all \(x\leq 1\), we have that for any \(1\leq t_{1}\leq t_{2}\leq T\),

\[\prod_{t=t_{1}}^{t_{2}}(1-bt^{-\beta}+at^{-\kappa})^{N}\ \leq\ \exp\bigg{(}-bN\sum_{t=t_{1}}^{t_{2}}t^{-\beta}+aN\sum_{t=t_{1}}^{t_{2}}t^{- \kappa}\bigg{)}\;.\] (13)

Applying this to the first quantity of interest followed by noting that \(a,b\geq 0\) and using Lemma C.2(ii), we obtain the first bound that

\[\prod_{t=1}^{T}(1-bt^{-\beta}+at^{-\kappa})^{N} \ \leq\ \exp\bigg{(}-bN\sum_{t=1}^{T}t^{-\beta}+aN\sum_{t=1}^{T}t^{- \kappa}\bigg{)}\] \[\leq\ \exp\big{(}-bN\,\varphi_{1-\beta}(T+1)+aN\,\varphi_{1-\kappa}(T+1) \big{)}\;.\]

For the second bound, we define

\[t_{0}\ \coloneqq\ \sup\Big{\{}t\leq T\,\Big{|}\,\,\tfrac{b}{2}\leq at^{-( \kappa-\beta)}\Big{\}}\;.\]Then by noting that \(1-bt^{-\beta}+at^{-\kappa}\geq 0\) for all \(1\leq t\leq T\) again,we can bound the quantity of interest as

\[\sum_{t=1}^{T}t^{-\zeta}\,\Big{(}\sum_{j=1}^{N}(1-bt^{-\beta}+at^{- \kappa})^{j-1}\Big{)}\,\prod_{s=t+1}^{T}(1-bs^{-\beta}+as^{-\kappa})^{N}\] \[=\,\sum_{t=t_{0}+1}^{T}t^{-\zeta}\,\Big{(}\sum_{j=1}^{N}(1-bt^{- \beta}+at^{-\kappa})^{j-1}\Big{)}\,\prod_{s=t+1}^{T}(1-bs^{-\beta}+as^{-\kappa} )^{N}\] \[\quad+\Big{(}\prod_{s=t_{0}+1}^{T}(1-bs^{-\beta}+as^{-\kappa}) \Big{)}\] \[\quad\quad\times\Big{(}\sum_{t=1}^{t_{0}}t^{-\zeta}\,\Big{(}\sum_ {j=1}^{N}(1-bt^{-\beta}+at^{-\kappa})^{j-1}\Big{)}\,\prod_{s=t+1}^{t_{0}}(1-bs^ {-\beta}+as^{-\kappa})^{N}\Big{)}\] \[\leq\,\sum_{t=t_{0}+1}^{T}t^{-\zeta}\,\Big{(}\sum_{j=1}^{N}\left( 1-\tfrac{b}{2}\,t^{-\beta}\right)^{j-1}\Big{)}\,\prod_{s=t+1}^{T}\left(1- \tfrac{b}{2}\,s^{-\beta}\right)^{N}\] \[\leq\,N\times\underbrace{\sum_{t=t_{0}+1}^{T}t^{-\zeta}\,\prod_{s =t+1}^{T}\left(1-\tfrac{b}{2}\,s^{-\beta}\right)^{N}}_{=:S_{1}}\] \[\quad+N(1+a)^{N-1}\times\underbrace{\Big{(}\prod_{s=t_{0}+1}^{T} \left(1-\tfrac{b}{2}\,s^{-\beta}\right)^{N}\Big{)}}_{=:S_{3}}\times\underbrace{ \Big{(}\sum_{t=1}^{t_{0}}t^{-\zeta}\,\prod_{s=t+1}^{t_{0}}(1+as^{-\kappa})^{N }\Big{)}}_{=:S_{2}}\.\]

In the last line, we have used that \(0\leq 1-\tfrac{b}{2}t^{-\beta}\leq 1\) for \(t\geq t_{0}+1\) and \(1+at^{-\kappa}\leq 1+a\). To control the three quantities, we first note that by (13), we have

\[S_{3} \leq\,\exp\Big{(}-\tfrac{bN}{2}\sum_{s=1}^{T}s^{-\beta}\Big{)} \exp\Big{(}\tfrac{bN}{2}\sum_{s=1}^{t_{0}}s^{-\beta}\Big{)}\] \[\stackrel{{(a)}}{{\leq}}\exp\Big{(}-\tfrac{bN}{2}\sum _{s=1}^{T}s^{-\beta}\Big{)}\exp\Big{(}aN\sum_{s=1}^{t_{0}}s^{-\kappa}\Big{)}\] \[\stackrel{{(b)}}{{\leq}}\exp\Big{(}-\tfrac{bN}{2}\, \varphi_{1-\beta}(T+1)+2aN\,\varphi_{1-\kappa}(T+1)\Big{)}\.\]

In \((a)\) above, we have noted that \(\tfrac{b}{2}\leq as^{-(\kappa-\beta)}\) for \(s\leq t_{0}\); in \((b)\), we have used \(t_{0}\leq T\) and Lemma C.2(ii) with \(a,b\geq 0\). In the special case \(\beta=1<\kappa\), the above yields

\[S_{3} \leq(T+1)^{-\frac{bN}{2}}\exp\Big{(}2aN\,\tfrac{1-(T+1)^{-(\kappa- 1)}}{\kappa-1}\Big{)}\] \[\leq(T+1)^{-\frac{bN}{2}}e^{\frac{2aN}{\kappa-1}}\.\]

We now control \(S_{2}\). By (13) again, we have

\[S_{2} \leq\,\sum_{t=1}^{t_{0}}t^{-\zeta}\exp\Big{(}aN\sum_{s=t+1}^{t_{0 }}s^{-\kappa}\Big{)}\] \[\leq\,\sum_{t=1}^{T}t^{-\zeta}\exp\Big{(}aN\sum_{s=t+1}^{T}s^{- \kappa}\Big{)}\] \[\stackrel{{(c)}}{{\leq}}\exp\big{(}2aN\varphi_{1- \kappa}(T+1)\big{)}\times\Big{(}\sum_{t=1}^{T}t^{-\zeta}\exp\big{(}-2aN\varphi_ {1-\kappa}(t+1)\big{)}\Big{)}\] \[\stackrel{{(d)}}{{\leq}}3^{\zeta}\exp\big{(}2aN \varphi_{1-\kappa}(T+1)\big{)}\,\sum_{t=1}^{T}(t+2)^{-\zeta}\exp\big{(}-2aN \varphi_{1-\kappa}(t+1)\big{)}\.\]

In \((c)\) above, we have applied Lemma C.2(ii); in \((d)\), we have noted that \(\sup_{t\in\mathbb{N}}(t+2)^{\beta}/t^{\beta}=3^{\beta}\). If \(\kappa\neq 1\) and \(a>0\), we can apply Lemma C.2(iv) to get that

\[S_{2} \leq\,\tfrac{3^{\zeta}}{2aN}(T+2)^{\max\{\kappa-\zeta,0\}}\,\exp \big{(}2aN\varphi_{1-\kappa}(T+1)\big{)}\] \[=\tfrac{Q_{2}}{N(1+a+c)^{N-1}}\,\exp\big{(}2aN\varphi_{1-\kappa}(T +1)\big{)}\.\]If \(\kappa=1\) or \(a=0\), the bound from \((c)\) above reads

\[S_{2} \leq\,\exp\big{(}2aN\varphi_{1-\kappa}(T+1)\big{)}\,\sum_{t=1}^{T}t^ {-\zeta}(t+1)^{-2aN}\] \[\leq\,\exp\big{(}2aN\varphi_{1-\kappa}(T+1)\big{)}\,\sum_{t=1}^{T }t^{-\zeta-2aN}\] \[\leq 2\,\varphi_{1-\zeta-2aN}(T+1)\,\exp\big{(}2aN\varphi_{1- \kappa}(T+1)\big{)}\ =\ \frac{Q_{2}}{N(1\,+\,a)^{N-1}}\,\exp\big{(}2aN\varphi_{1- \kappa}(T+1)\big{)}\,\]

where we have used Lemma C.2(ii) in the last line. Now consider the special case with \(\zeta=1<\kappa\), the bound from \((d)\) becomes

\[S_{2} \leq 3\exp\big{(}2aN\varphi_{1-\kappa}(T+1)\big{)}\Big{(}\sum_{t=1 }^{T}(t+2)^{-1}\exp\big{(}-2aN\varphi_{1-\kappa}(t+1)\big{)}\Big{)}\] \[\leq 3\exp\Big{(}2aN\,\frac{1\,-\,(T+1)^{-(\kappa-1)}}{\kappa\,- \,1}\Big{)}\sum_{t=1}^{T}(t+2)^{-1}\] \[\leq 3e^{\frac{2aN}{\kappa}1}\log(T+1)\.\]

We are left with controlling \(S_{1}\), which follows from a similar strategy as controlling \(S_{2}\):

\[S_{1} \stackrel{{\eqref{eq:S_1}}}{{\leq}} \sum_{t=t_{0}+1}^{T}t^{-\zeta}\exp\Big{(}-\frac{bN}{2}\sum_{s=t+1}^{ T}s^{-\beta}\Big{)}\] \[\leq\,\sum_{t=1}^{T}t^{-\zeta}\exp\Big{(}-\frac{bN}{2}\sum_{s=t+1 }^{T}s^{-\beta}\Big{)}\] \[\stackrel{{(a)}}{{\leq}} \exp\Big{(}-\frac{bN}{2}\varphi_{1-\beta}(T+1)\Big{)}\sum_{t=1}^{ T}t^{-\zeta}\exp\Big{(}\frac{bN}{2}\,\varphi_{1-\beta}(t+1)\Big{)}\] (14) \[\leq 4^{\zeta}\exp\Big{(}-\frac{bN}{2}\,\varphi_{1-\beta}(T+1) \Big{)}\sum_{t=1}^{T}(t+3)^{-\zeta}\exp\Big{(}\frac{bN}{2}\,\varphi_{1-\beta} (t+1)\Big{)}\.\]

In \((a)\) above, we used Lemma C.2(ii). For \(\beta\neq 1\) and \(b\neq 0\), we can apply Lemma C.2(iv) with \(\frac{b}{2}>0\) to obtain

\[S_{1} \leq 4^{\zeta}\exp\Big{(}-\frac{bN}{2}\,\varphi_{1-\beta}(T+1)\Big{)} \frac{(T\,+\,3)^{\max\{\beta-\zeta,0\}}}{bN/2}\exp\Big{(}\frac{bN}{2}\varphi_ {1-\beta}(T+3)\Big{)}\] \[=\frac{\frac{2^{2\zeta+1}(T+3)^{\max\{\beta-\zeta,0\}}}{bN}\exp \Big{(}\frac{bN}{2(1-\beta)}\Big{(}(T+3)^{1-\beta}-(T+1)^{1-\beta}\Big{)} \Big{)}\] \[\stackrel{{(b)}}{{\leq}} \frac{2^{2\zeta+1}(T+3)^{\max\{\beta-\zeta,0\}}}{bN}\exp\Big{(} \frac{bN}{2(1-\beta)(T+1)^{\beta}}\Big{)}\ =\ \frac{Q_{1}}{N}\.\]

In \((b)\), we have used Lemma C.2(iii) with \(1-\beta\leq 1\). Meanwhile, if \(\beta=1\) or \(b=0\), we have

\[S_{1} \leq\,\exp\Big{(}-\frac{bN}{2}\,\varphi_{1-\beta}(T+1)\Big{)}\sum _{t=1}^{T}t^{-\zeta}(t+1)^{bN/2}\] \[\leq\,2\varphi_{1-\zeta+bN/2}(T+1)\exp\Big{(}-\frac{bN}{2}\, \varphi_{1-\beta}(T+1)\Big{)}\ =\ \frac{Q_{1}}{N}\.\]

For the special case with \(\zeta=\beta=1\), the bound in (14) becomes

\[S_{1} \leq\,\exp\Big{(}-\frac{bN}{2}\,\varphi_{0}(T+1)\Big{)}\sum_{t=1} ^{T}t^{-1}\exp\Big{(}\frac{bN}{2}\,\varphi_{0}(t+1)\Big{)}\] \[=(T+1)^{-\frac{bN}{2}}\sum_{t=1}^{T}t^{-1}(t+1)^{\frac{bN}{2}}\] \[\leq(T+1)^{-\frac{bN}{2}}\sum_{t=1}^{T}t^{-(1-\frac{bN}{2})}\] \[\stackrel{{(c)}}{{\leq}} 2(T+1)^{-\frac{bN}{2}}\,\varphi_{\frac{bN}{2}}(T+1)\ =\ 2(T+1)^{-\frac{bN}{2}}\,\frac{(T+1)^{bN/2}-1}{bN/2}\ \leq\ \frac{4}{bN}\.\]

In \((c)\), we have used Lemma C.2(ii) for both the case \(1-\frac{bN}{2}\leq 0\) and \(1-\frac{bN}{2}\geq 0\).

Combining the bounds for the general cases, we obtain the first desired inequality that

\[\sum_{t=1}^{T}t^{-\zeta}\,\Big{(}\sum_{j=1}^{N}(1-bt^{-\beta}+at ^{-\kappa})^{j-1}\Big{)}\,\prod_{s=t+1}^{T}(1-bs^{-\beta}+as^{-\kappa})^{N}\] \[\leq\ Q_{1}+\exp\Big{(}-\frac{b}{2}\,\varphi_{1-\beta}(T+1)+u \varphi_{1-\xi}(T+3)+4a\,\varphi_{1-\kappa}(T+1)\Big{)}\,Q_{2}\.\]For the special case \(\zeta=\beta=1<\kappa,\gamma\), combining the earlier bounds gives

\[\sum_{t=1}^{T}t^{-\zeta}\left(\sum_{j=1}^{N}(1-bt^{-\beta}+at^{- \kappa})^{j-1}\right)\,\prod_{s=t+1}^{T}(1-bs^{-\beta}+as^{-\kappa})^{N}\\ \leq\ \tfrac{4}{b}+\tfrac{3N(1+a)^{N-1}\,e^{\tfrac{4bN}{k-1}\,\log(T +1)}}{(T+1)^{\tfrac{8N}{2}}}\.\]

### Contraction and integrability results

The next result is a standard result in convex analysis, needed to handle projections performed in Algorithms 1 and 2.

**Lemma C.4**.: _Let \(\Psi\) a be convex subset of \(\mathbb{R}^{p}\). Let \(\psi^{\star}\in\Psi\). Then, for all \(\psi\in\mathbb{R}^{p}\), we have:_

\[\|\mathrm{Proj}_{\Psi}(\psi)-\psi^{\star}\|\leq\|\psi-\psi^{\star}\|\]

Proof.: We have:

\[\|\psi-\psi^{\star}\|^{2} =\|\psi-\mathrm{Proj}_{\Psi}(\psi)+\mathrm{Proj}_{\Psi}(\psi)- \psi^{\star}\|^{2}\] \[=\|\psi-\mathrm{Proj}_{\Psi}(\psi)\|^{2}+2\left\langle\psi- \mathrm{Proj}_{\Psi}(\psi),\mathrm{Proj}_{\Psi}(\psi)-\psi^{\star}\right\rangle +\|\mathrm{Proj}_{\Psi}(\psi)-\psi^{\star}\|^{2}\]

Since by [49, Proposition 1.1.9], we have:

\[\left\langle\psi-\mathrm{Proj}_{\Psi}(\psi),\psi^{\prime}-\mathrm{Proj}_{\Psi }(\psi)\right\rangle\leq 0\]

for all \(\psi^{\prime}\in\Psi\), we can use this inequality at \(\psi^{\prime}=\psi^{\star}\in\Psi\) to obtain:

\[\left\|\psi-\psi^{\star}\right\|^{2}\geq\left\|\psi-\mathrm{Proj}_{\Psi}(\psi) \right\|^{2}\]

and the result follows by taking the square root. 

We now state two lemmas that guarantee an amount of integrability sufficient to our analysis.

**Lemma C.5**.: _Let \(p,q\in\mathcal{P}(\mathcal{X})\) such that \(\tfrac{\mathrm{d}p}{\mathrm{d}q}\) exists, and such that \(\chi^{2}(p;q)<+\infty\). Assume that \(f\in L^{2}(q)\) Then \(|\mathbb{E}_{p}f|<+\infty\)._

Proof.: By assumption, \(f\in L^{2}(q)\). Moreover, \(\chi^{2}(p,q)<+\infty\), and thus we have \(\tfrac{\mathrm{d}p}{\mathrm{d}q}-1\in L^{2}(q)\). Thus, the inner product is finite, and

\[\Big{|}\int f\Big{(}\tfrac{\mathrm{d}p}{\mathrm{d}q}-1\Big{)} \mathrm{d}q\Big{|}=\Big{|}\int f\mathrm{d}p-\int f\mathrm{d}q\Big{|}=|\mathbb{ E}_{p}f-\mathbb{E}_{q}f|\coloneqq M<+\infty\] \[\implies M-|\mathbb{E}_{q}f|<\mathbb{E}_{p}f<M+|\mathbb{E}_{q}f|\]

**Lemma C.6**.: _For all \(\psi\in\Psi\), for all \(m\geq 1\), and for all \(k\geq 1\), we have:_

\[\mathbb{E}_{p_{\psi^{\star}}}\left\|P_{\psi}^{m}\phi\right\|^{k}<+\infty\]

Proof.: By analycity of the log partition function \(\psi\longmapsto\log Z(\psi)\), we have \(\int\left\|\phi\right\|^{k}\mathrm{d}p_{\psi}(x)<+\infty\) for all \(\psi\in\Psi\), and thus, the function \(x\longmapsto\left\|\phi\right\|^{k}(x)\in L^{2}(p_{\psi})\) for all \(\psi\). Consequently, \(P_{\psi}^{m}\left\|\phi\right\|^{k}\in L^{2}(p_{\psi})\). We can apply Lemma C.5 to \(P_{\psi}^{m}\left\|\phi\right\|^{k}\) to obtain \(\mathbb{E}_{\psi^{\star}}P^{m}\|\phi\|^{k}<+\infty\) for all \(k\geq 1\) and for all \(m\geq 0\). As a by-product, we obtain \(P_{\psi}^{m}\left\|\phi\right\|^{k}\in L^{2}(p_{\psi^{\star}})\), and thus so \(\left\|P^{m}\phi\right\|^{k}\). 

The following lemma is used multiple time in our analysis.

**Lemma C.7**.: _Assume A.3. Let \(q\) be a positive integer. Let \(f\coloneqq(f_{1},\dots,f_{q})\) such that \(f_{k}\in\left\{\phi_{i}\right\}_{i=1}^{p}\cup\left\{\phi_{i}\phi_{j}\right\}_ {i,j=1}^{p}\) for \(k\in[q]\). Then, for all \(\psi\in\Psi\), we have_

\[\left\|\mathbb{E}_{p_{\psi^{\star}}}\left[P_{\psi}^{m}(f-\mathbb{E}_{p_{\psi}}f) \right]\right\|\leq\alpha^{m}C_{\chi}\left(\mathbb{E}_{p_{\psi}}\left[\left\|f -\mathbb{E}_{p_{\psi}}f\right\|^{2}\right]\right)^{1/2}\left\|\psi-\psi^{ \star}\right\|\]Proof.: Let us note first that

\[\left\|\mathbb{E}_{p_{\psi^{*}}},P_{\psi}^{m}\left(f-\mathbb{E}_{p_{ \psi}}f\right)\right\|^{2} \stackrel{{(a)}}{{=}}\sum_{i=1}^{q}\left(\int\!P_{ \psi}^{m}\left(f_{i}-\mathbb{E}_{p_{\psi}}f_{i}\right)(x)\left(p_{\psi^{*}}(x) -p_{\psi}(x)\right)\mathrm{d}x\right)^{2}\] \[=\sum_{i=1}^{q}\left(\int\!P_{\psi}^{m}\left(f_{i}-\mathbb{E}_{p_{ \psi}}f_{i}\right)(x)\left(\frac{\mathrm{d}p_{\psi^{*}}}{\mathrm{d}p_{\psi}}(x )-1\right)p_{\psi}(x)\mathrm{d}x\right)^{2}\] \[\stackrel{{(b)}}{{\leq}}\left(\int\left(\frac{ \mathrm{d}p_{\psi}^{*}}{\mathrm{d}p_{\psi}}(x)-1\right)^{2}p_{\psi}(x)\mathrm{ d}x\right)\sum_{i=1}^{q}\int\!P_{\psi}^{m}\left(f_{i}-\mathbb{E}_{p_{\psi}}f_{i} \right)(x)^{2}p_{\psi}(x)\mathrm{d}x\] \[\leq\chi^{2}(p_{\psi},p_{\psi^{*}})\sum_{i=1}^{q}\left\|P_{\psi} ^{m}(f_{i}-\mathbb{E}_{p_{\psi}}f_{i})\right\|_{L^{2}(p_{\psi})}\] \[\stackrel{{(c)}}{{\leq}}\alpha^{2m}\chi^{2}(p_{\psi },p_{\psi^{*}})\sum_{i=1}^{q}\left\|f_{i}-\mathbb{E}_{p_{\psi}}f_{i}\right\|_{ L^{2}(p_{\psi})(\mathbb{R}^{d})}\] \[\stackrel{{(d)}}{{\leq}}\alpha^{2m}C_{\chi}^{2} \left\|\psi-\psi^{*}\right\|^{2}\mathbb{E}_{p_{\psi}}\left\|f-\mathbb{E}_{p_{ \psi}}f\right\|^{2}\;.\]

Here, we used the fact that \(P_{\psi}^{m}\) admits \(p_{\psi}\) as an invariant measure in \((a)\)[34, Eq. (1.2.2)], the Cauchy-Schwarz inequality in \((b)\) 

### Miscellaneous

**Lemma C.8**.: _Let \(f:\Psi\to\mathbb{R}^{p}\) be a differentiable function in the interior of \(\Psi\subseteq\mathbb{R}^{p}\). For \(\psi\in\Psi\), define \(\sigma_{\min}(\psi)\coloneqq\inf_{\theta\in\Psi,\|\theta\|=1}\theta^{\top} \nabla f(\psi)\theta\) and \(\sigma_{\max}(\psi)\coloneqq\sup_{\theta\in\Psi,\|\theta\|=1}\theta^{\top} \nabla f(\psi)\theta\) with respect to the Jacobian matrix \(\nabla f(\psi)\). Then for any \(\psi_{1},\psi_{2}\in\Psi\), we have that_

\[\inf_{\psi\in\Psi}\sigma_{\min}(\psi)\;\leq\;(\psi_{1}-\psi_{2})^{\top}\big{(} f(\psi_{1})-f(\psi_{2})\big{)}\;\leq\;\sup_{\psi\in\Psi}\sigma_{\max}(\psi)\]

Proof of Lemma C.8.: By the mean value theorem, there exists some \(a\in(0,1)\) such that

\[(\psi_{1}-\psi_{2})^{\top}\big{(}f(\psi_{1})-f(\psi_{2})\big{)} =(\psi_{1}-\psi_{2})^{\top}\big{(}f(1\times\psi_{1}+0\times\psi_{ 2})-f(0\times\psi_{1}+0\times\psi_{2})\big{)}\] \[=(\psi_{1}-\psi_{2})^{\top}\nabla f(a\psi_{1}+(1-a)\psi_{2})( \psi_{1}-\psi_{2})\] \[=\|\psi_{1}-\psi_{2}\|^{2}\,\frac{(\psi_{1}-\psi_{2})^{\top}}{\| \psi_{1}-\psi_{2}\|}\,\nabla f(a\psi_{1}+(1-a)\psi_{2})\,\frac{\psi_{1}-\psi_{ 2}}{\|\psi_{1}-\psi_{2}\|}\;.\]

Plugging in the definition of \(\sigma_{\max}\) gives the desired upper bound and similarly \(\sigma_{\min}\) implies the lower bound. 

## Appendix D Proofs for Online CD

### Auxiliary Lemmas for Online CD

We recall the following notations used in the next lemmas, namely \(\sigma_{\psi}\coloneqq\mathbb{E}_{p_{\psi}}\|\phi-\mathbb{E}_{p_{\psi}}\phi \|^{2}\), \(\sigma_{\star}\coloneqq\sigma_{\psi^{*}}\) and \(\sigma\coloneqq\sup_{\psi\in\Psi}\sigma_{\psi}\).

We now provide two intermediary lemmas necessary to analyze the impact of variance in the CD gradient. The strategy is similar in both of them: we change the integration from \(p_{\psi^{*}}\) to \(p_{\psi}\) to obtain contraction, at the cost of an additional term scaling with \(C_{\chi}\left\|\psi-\psi^{\star}\right\|\). We obtain exact constants that we choose to describe in terms of the smoothness parameters of the problem, e.g. the \(k^{th}\) derivatives of the log partition function \(\log Z\), which, for \(k\geq 2\), equals the \(k^{th}\) derivative of the negative cross-entropy model w.r.t \(p_{\psi^{*}}\).

Second Moment convergenceThe following lemmas guarantee the second moment of a sample from \(k^{m}_{\psi}p_{\psi^{*}}\) approaches the second moment of a sample from the target distribution \(p_{\psi}\).

**Lemma D.1**.: _Under A1, A2 and A3, for all \(\psi\in\Psi\), we have:_

\[\left|\mathbb{E}_{p_{\psi}},P_{\psi}^{m}\left\|\phi\right\|^{2}-\mathbb{E}_{p_{ \psi}}\|\phi\|^{2}\right|\leq\alpha^{m}C_{\chi}\left\|\psi-\psi^{\star}\right\| \left\|\log Z\right\|_{1,\infty}\]_where_

\[\left\|\log Z\right\|_{1,\infty}\coloneqq\sup_{\psi\in\Psi} \sum_{i=1}^{p}(4\partial_{i}^{1}\log Z(\psi)^{2}\partial_{i}^{2} \log Z(\psi)+2\partial_{i}^{2}\log Z(\psi)^{2}+4\partial_{i}^{1}\log Z(\psi) \partial_{i}^{3}\log Z(\psi)\] \[\qquad\qquad+\partial_{i}^{4}\log Z(\psi))^{1/2}<+\infty\]

Proof.: Applying Lemma C.7 to each \(f_{i}\coloneqq\phi_{i}^{2}\), we have

\[\left|\mathbb{E}_{p_{\psi}},P_{\psi}^{m}\phi_{i}^{2}-\mathbb{E}_{ p_{\psi}}\phi_{i}^{2}\right| =\alpha^{m}C_{\chi}\left\|\psi-\psi^{\star}\right\|\left(\mathbb{E }_{p_{\psi}}\left(\phi_{i}^{2}-\mathbb{E}_{p_{\psi}}\phi_{i}^{2}\right)^{2} \right)^{1/2}\] \[=\alpha^{m}C_{\chi}\left\|\psi-\psi^{\star}\right\|\left( \mathbb{E}_{p_{\psi}}\phi_{i}^{4}-(\mathbb{E}_{p_{\psi}}\phi_{i}^{2})^{2} \right)^{1/2}\]

We map the two moments to derivatives of \(\log Z(\psi)\), since the \(k^{th}\) derivative of \(\log Z(\psi)\) is the \(k^{th}\) cumulant. It can be shown, using the multivariate moment to cumulant mapping, that

\[\mathbb{E}\phi_{i}^{4} =\frac{\partial\log Z^{4}}{\partial\psi_{i}}+6\frac{\partial\log Z ^{2}}{\partial\psi_{i}}\frac{\partial^{2}\log Z}{\partial\psi_{i}^{2}}+3\left( \frac{\partial^{2}\log Z}{\partial\psi_{i}^{2}}\right)^{2}+4\frac{\partial \log Z}{\partial\psi_{i}}\frac{\partial^{3}\log Z}{\partial\psi_{i}^{3}}+ \frac{\partial^{4}\log Z}{\partial\psi_{i}^{4}}\] \[=\partial_{i}^{1}\log Z(\psi)^{4}+6\partial_{i}^{1}\log Z(\psi)^ {2}\partial_{i}^{2}\log Z(\psi)+3\partial_{i}^{2}\log Z(\psi)^{2}+4\partial_{i }^{1}\log Z(\psi)\partial_{i}^{3}\log Z(\psi)\] \[\quad+\partial_{i}^{4}\log Z(\psi)\]

where \(\partial_{i}^{k}\log Z(\psi)\) denotes the \(k^{th}\) derivative of \(\log Z\) with respect to \(\psi_{i}\). On the other hand,

\[\mathbb{E}\phi_{i}^{2} =\partial_{i}^{1}\log Z(\psi)^{2}+\partial_{i}^{2}\log Z(\psi)\] \[\implies(\mathbb{E}\phi_{i}^{2})^{2} =\partial_{i}^{1}\log Z(\psi)^{4}+2\partial_{i}^{1}\log Z(\psi)^{2} \partial_{i}^{2}\log Z(\psi)+\partial_{i}^{2}\log Z(\psi)^{2}\]

implying

\[\mathbb{E}_{p_{\psi}}\phi_{i}^{4}-(\mathbb{E}_{p_{\psi}^{2}} \phi_{i}^{2})^{2} =4\partial_{i}^{1}\log Z(\psi)^{2}\partial_{i}^{2}\log Z(\psi)+2 \partial_{i}^{2}\log Z(\psi)^{2}+4\partial_{i}^{1}\log Z(\psi)\partial_{i}^{3} \log Z(\psi)\] \[\qquad\qquad+\partial_{i}^{4}\log Z(\psi)\]

The result follows by summing over \(i\), since:

\[|\mathbb{E}_{p_{\psi^{\star}}},P_{\psi}^{m}\|\phi\|^{2}-\mathbb{E}_{p_{\psi}} \|\phi\|^{2}\leq\sum_{i=1}^{d}|\mathbb{E}_{p_{\psi^{\star}}}P_{\psi}^{m}\phi_ {i}^{2}-\mathbb{E}_{p_{\psi}}\phi_{i}^{2}|\]

Note that \(\left\|\log Z\right\|_{1,\infty}\) is finite since \(\Psi\) is compact and \(\log Z\) is analytic. 

Squared First Moment convergenceThe next lemma provides convergence (in squared absolute value) of the first moment of the \(m\)-iterated Markov kernel \(k_{\psi}^{m}\).

**Lemma D.2**.: _Under A1, A2 and A3, for all \(\psi\in\Psi\), we have_

\[\left\|\mathbb{E}_{p_{\psi^{\star}}},P_{\psi}^{m}\phi\|^{2}-\left\|\mathbb{E}_ {p_{\psi}}\phi\right\|^{2}\right|\leq\alpha^{m}\sigma_{\psi}^{2}+C_{\chi} \alpha^{m/4}\left\|\log Z\right\|_{2,\infty}\left\|\psi-\psi^{\star}\right\|\]

_where_

\[\left\|\log Z\right\|_{2,\infty}\coloneqq\sup_{\psi\in\Psi}\sum_{i=1}^{p} \left(F(\psi)\partial_{i}^{2}\log Z(\psi)\right)^{1/4}+2\left|\partial_{i}^{1} \log Z(\psi)\partial_{i}^{2}\log Z(\psi)^{1/2}\right|\]

_and_

\[F(\psi)\coloneqq 15\partial_{i}^{2}\log Z(\psi)^{3}+10\partial_{i}^{3}\log Z( \psi)^{2}+15\partial_{i}^{2}\log Z(\psi)\partial_{i}^{4}\log Z(\psi)+\partial_{ i}^{6}\log Z(\psi)\]

Proof.: We have:

\[(\mathbb{E}_{p_{\psi^{\star}}},P_{\psi}^{m}\phi_{i})^{2} =(\mathbb{E}_{p_{\psi^{\star}}},P_{\psi}^{m}\phi_{i}-\mathbb{E}_{ p_{\psi}}\phi_{i}+\mathbb{E}_{p_{\psi}}\phi_{i})^{2}\] \[=(\mathbb{E}_{p_{\psi^{\star}}},P^{m}\phi_{i}-\mathbb{E}_{p_{\psi}} \phi_{i})^{2}+2\mathbb{E}_{p_{\psi}}\phi_{i}\left(\mathbb{E}_{p_{\psi^{\star}}} \,P^{m}\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i}\right)\] \[\quad+(\mathbb{E}_{\psi}\phi_{i})^{2}\] \[\implies|\mathbb{E}_{p_{\psi^{\star}}}(P_{\psi}^{m}\phi_{i})^{2} -(\mathbb{E}_{p_{\psi}}\phi_{i})^{2}| \leq\underbrace{\left|(\mathbb{E}_{p_{\psi^{\star}}},P_{\psi}^{m} \left(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i}\right)\right)^{2}}_{\Delta_{2}}\] \[\qquad\qquad+2\underbrace{\left|\mathbb{E}_{p_{\psi}}\phi_{i}\, \mathbb{E}_{p_{\psi^{\star}}},P^{m}\left(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i} \right)\right|}_{\Delta_{2}}\]

[MISSING_PAGE_FAIL:26]

Similarly to the previous lemma, one can upper bound \(\mathbb{E}(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i})^{6}\) using the _centered_ moment to cumulant formula:

\[\mathbb{E}_{p_{\psi}}(\phi_{i}-\mathbb{E}\phi_{i})^{6}= 15\partial_{i}^{2}\log Z(\psi)^{3}+10\partial_{i}^{3}\log Z(\psi)^ {2}+15\partial_{i}^{2}\log Z(\psi)\partial_{i}^{4}\log Z(\psi)+\partial_{i}^{6} \log Z(\psi)\] \[=:F(\psi)\]

To get a full description of \(\left\|\log Z\right\|_{2,\infty}\):

\[\left\|\log Z\right\|_{2,\infty}=\sup_{\psi\in\Psi}\sum_{i=1}^{p}\left(F(\psi) \partial_{i}^{2}\log Z(\psi)\right)^{1/4}+2\left|\partial_{i}^{1}\log Z(\psi) \partial_{i}^{2}\log Z(\psi)^{1/2}\right|\;.\]

**Lemma D.3**.: _Under A1, A2 and A3, for all \(\psi\in\Psi\), we have_

\[\left|\mathbb{E}_{p_{\psi^{*}}}\left\|P_{\psi}^{m}\phi\right\|^{2}-\left\| \mathbb{E}_{p_{\psi}}\phi\right\|^{2}\right|\leq\alpha^{m}\sigma_{\psi}^{2}+ C_{\chi}\alpha^{m/4}\left\|\log Z\right\|_{2,\infty}\left\|\psi-\psi^{*}\right\|\]

_where_

\[\left\|\log Z\right\|_{2,\infty}:=\sup_{\psi\in\Psi}\sum_{i=1}^{p}\left(F(\psi )\partial_{i}^{2}\log Z(\psi)\right)^{1/4}+2\left|\partial_{i}^{1}\log Z(\psi )\partial_{i}^{2}\log Z(\psi)^{1/2}\right|\]

_and_

\[F(\psi):=15\partial_{i}^{2}\log Z(\psi)^{3}+10\partial_{i}^{3}\log Z(\psi)^{ 2}+15\partial_{i}^{2}\log Z(\psi)\partial_{i}^{4}\log Z(\psi)+\partial_{i}^{ 6}\log Z(\psi)\]

Proof.: We have:

\[\mathbb{E}_{p_{\psi^{*}}}\left(P_{\psi}^{m}\phi_{i}\right)^{2} =\mathbb{E}_{p_{\psi^{*}}}(P_{\psi}^{m}\phi_{i}-\mathbb{E}_{p_{ \psi}}\phi_{i}+\mathbb{E}_{p_{\psi}}\phi_{i})^{2}\] \[=\mathbb{E}_{p_{\psi^{*}}}(P^{m}\phi_{i}-\mathbb{E}_{p_{\psi}} \phi_{i})^{2}+2\mathbb{E}_{p_{\psi}}\phi_{i}\,\mathbb{E}_{p_{\psi^{*}}}P^{m}( \phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i})\] \[\quad+(\mathbb{E}_{\psi}\phi_{i})^{2}\] \[\implies\left|\mathbb{E}_{p_{\psi^{*}}}(P_{\psi}^{m}\phi_{i})^{2} -(\mathbb{E}_{p_{\psi}}\phi_{i})^{2}\right| \leq\underbrace{\left|\mathbb{E}_{p_{\psi^{*}}}(P^{m}\left(\phi_{ i}-\mathbb{E}_{p_{\psi}}\phi_{i}\right))^{2}\right|}_{\Delta_{1}}\] \[\qquad\qquad+2\left|\underbrace{\mathbb{E}_{p_{\psi}}\phi_{i}\, \mathbb{E}_{p_{\psi^{*}}}P^{m}\left(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i} \right)}_{\Delta_{2}}\right|\]

where

\[\Delta_{1} =\underbrace{\mathbb{E}_{p_{\psi}}(P_{\psi}^{m}(\phi_{i}-\mathbb{ E}_{\psi}\phi_{i}))^{2}}_{\Delta_{1,1}}+\underbrace{\mathbb{E}_{p_{\psi}} \left(\left(P_{\psi}^{m}\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i}\right)^{2} \left(\frac{\mathrm{d}p_{\psi^{*}}}{\mathrm{d}p_{\psi}}-1\right)\right)}_{ \Delta_{1,2}}\] \[\stackrel{{(a)}}{{\leq}}\alpha^{2m}\mathbb{E}_{\psi}( \phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i})^{2}+C_{\chi}\left\|\psi-\psi^{*} \right\|\left(\mathbb{E}_{p_{\psi}}(P^{m}(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{ i})^{4}\right)^{1/2}\] \[\stackrel{{(b)}}{{\leq}}\alpha^{2m}\mathbb{E}_{\psi}( \phi_{i}-\mathbb{E}_{\psi}\phi_{i})^{2}+C_{\chi}\left\|\psi-\psi^{*}\right\| \left(\mathbb{E}_{p_{\psi}}P^{m}(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i})^{2} \right)^{1/4}\left(\mathbb{E}_{p_{\psi}}P^{m}(\phi_{i}-\mathbb{E}_{p_{\psi}} \phi_{i})^{6}\right)^{1/4}\] \[\stackrel{{(c)}}{{\leq}}\alpha^{2m}\mathbb{E}_{\psi}( \phi_{i}-\mathbb{E}_{\psi}\phi_{i})^{2}+\alpha^{m/2}C_{\chi}\left\|\psi-\psi^{*} \right\|\left(\mathbb{E}_{p_{\psi}}(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i})^{2} \right)^{1/4}\left(\mathbb{E}_{p_{\psi}}(\phi_{i}-\mathbb{E}_{p_{\psi}}\phi_{i} )^{6}\right)^{1/4}.\]

In \((a)\), we used the restricted spectral gap Assumption A3 for \(\Delta_{1,1}\), and the Cauchy-Schwarz inequality combined with Assumption A2 for \(\Delta_{1,2}\). In \((b)\), we used the Cauchy-Schwarz once again, and in \((c)\) we used the fact that \(P_{\psi}^{m}\) is a contraction in \(L^{6}(p_{\psi})\) and another invocation of the spectral gap assumption A3 As an aside, note that a simpler result can be obtained by making regularity assumption on the mapping \(\psi\longmapsto P_{\psi}^{m}\). Assuming that \(\psi\longmapsto P_{\psi}^{m}(x)\) is uniformly \(L_{m}\)-Lipschitz across \(x\in\mathcal{X}\) for instance (as done in [21, Assumption 5]), the second term \(\Delta_{1,2}\) of \(\Delta_{1}\) could have been handled using

\[\Delta_{1,2} \leq 2\mathbb{E}_{p_{\psi^{*}}}\left\|P_{\psi}^{m}\phi-\mathbb{E}_{ \psi^{*}}\phi\right\|^{2}+2\left\|\mathbb{E}_{p_{\psi}}\phi-\mathbb{E}_{p_{ \psi^{*}}}\phi\right\|^{2}\] \[\leq 4(L_{m}\left\|\psi-\psi^{*}\right\|+\sigma_{\star}^{2}\alpha^{ 2m}+2\left\|\mathbb{E}_{p_{\psi}}\phi-\mathbb{E}_{p_{\psi^{*}}}\phi\right\|^{2})\] \[\leq 4(L_{m}\left\|\psi-\psi^{*}\right\|+\sigma_{\star}^{2}\alpha^{ 2m}+2L^{2}\left\|\psi-\psi^{*}\right\|)\]

[MISSING_PAGE_FAIL:28]

\(\Delta_{1,1}+\Delta_{1,2}\) form the differentiable stochastic gradient \(g_{t}\) of Equation 6. Note that \(\Delta_{1,1}\) is mean-zero, and \(\Delta_{2}\) is mean-zero conditionally on \(X_{t}\). Consequently, \(\mathbb{E}\left\langle\Delta_{2},\Delta_{3}\right\rangle=\mathbb{E}\left\langle \Delta_{2},\Delta_{1,1}\right\rangle=\mathbb{E}\left\langle\Delta_{1,1},\Delta_ {2}\right\rangle=0\), and the only mixed-terms that remain to be controlled are \(\mathbb{E}\left\langle\Delta_{1,1},\Delta_{3}\right\rangle\) and \(\mathbb{E}\left\langle\Delta_{1,2},\Delta_{3}\right\rangle\). We first control the unmixed terms, and the simple ones first: we have \(\mathbb{E}\left\|\Delta_{1,1}\right\|^{2}=\sigma_{\star}^{2}\), as well as \(\mathbb{E}\left\|\Delta_{1,2}\right\|^{2}\leq L^{2}\left\|\psi-\psi^{\star} \right\|^{2}\). For \(\Delta_{2}\), we have:

\[\mathbb{E}_{k_{\psi}^{m}(x,\cdot)}\left\|\Delta_{2}\right\|^{2} =\mathbb{E}_{k_{\psi}^{m}(x,\cdot)}\|\phi(k_{\psi}^{m}(x,\cdot)) \|^{2}-\|\mathbb{E}_{k_{\psi}^{m}(x,\cdot)}\phi(k_{\psi}^{m}(x,\cdot))\|^{2}\] \[=P_{\psi}^{m}\|\phi(x)\|^{2}-\|P_{\psi}^{m}\phi(x)\|^{2}\]

We can invoke Lemmas D.3 and D.1, which guarantee

\[\left|\mathbb{E}_{p_{\psi^{\star}}}\|P_{\psi}^{m}\phi\|^{2}-\left\| \mathbb{E}_{p_{\psi}}\phi\right\|^{2}\right| \leq\] \[\left|\mathbb{E}_{p_{\psi^{\star}}}P_{\psi}^{m}\|\phi\|^{2}- \mathbb{E}_{p_{\psi}}\|\phi\|^{2}\right| \leq\]

to obtain

\[\mathbb{E}_{p_{\psi^{\star}}},\mathbb{E}_{k_{\psi}^{m}(x,\cdot)} \left\|\Delta_{2}\right\|^{2}\] \[=\mathbb{E}_{p_{\psi}}\left\|\phi\right\|^{2}-\left\|\mathbb{E} \phi\right\|^{2}+\alpha^{2m}\sigma_{\psi}^{2}+C_{\chi}\alpha^{m/2}\left(\left\| \log Z\right\|_{1,\infty}+\left\|\log Z\right\|_{2,\infty}\right)\left\|\psi- \psi^{\star}\right\|\] \[=\mathbb{E}\left\|\phi-\mathbb{E}\phi\right\|^{2}+\alpha^{2m} \sigma_{\psi}^{2}+\alpha^{m/2}\left\|\log Z\right\|_{3,\infty}C_{\chi}\left\| \psi-\psi^{\star}\right\|\]

where \(\left\|\log Z\right\|_{3,\infty}:=2\max\left(\left\|\log Z\right\|_{1,\infty},\left\|\log Z\right\|_{2,\infty}\right)\).

For \(\Delta_{3}\), notice that \(\Delta_{3}\) is precisely the term \(\Delta_{1}\) in Lemma D.3, and we can thus bound it by

\[\mathbb{E}_{p_{\psi^{\star}}}\left\|\Delta_{3}\right\|^{2}\leq\sigma_{\psi}^{2 }\alpha^{2m}+\alpha^{m/2}\left\|\log Z\right\|_{2,\infty}C_{\chi}\left\|\psi- \psi^{\star}\right\|\]

Finally, we simply bound \(2\mathbb{E}\left\langle\Delta_{1,1},\Delta_{3}\right\rangle\) by \(\mathbb{E}\left\|\Delta_{1,1}\right\|^{2}+\mathbb{E}\left\|\Delta_{3}\right\|^ {2}\), and \(2\mathbb{E}\left\langle\Delta_{1,2},\Delta_{3}\right\rangle\) by \(\mathbb{E}\left\|\Delta_{1,2}\right\|^{2}+\mathbb{E}\left\|\Delta_{3}\right\|^ {2}\). Putting everything together, we have:

\[\mathbb{E}\left\|h_{t}(\psi)\right\|^{2} =\mathbb{E}\left\|\Delta_{1,1}\right\|^{2}+\mathbb{E}\left\| \Delta_{1,2}\right\|^{2}+\mathbb{E}\left\|\Delta_{2}\right\|^{2}+\mathbb{E} \left\|\Delta_{2}\right\|^{2}+2\mathbb{E}\left\|\Delta_{3}\right\|^{2}+2 \mathbb{E}\left\langle\Delta_{1,1},\Delta_{3}\right\rangle+2\mathbb{E}\left\langle \Delta_{1,2},\Delta_{3}\right\rangle\] \[\leq 2\sigma_{\star}^{2}+2L^{2}\left\|\psi-\psi^{\star}\right\|^{2}+ \sigma_{\psi}^{2}+4(\sigma_{\psi}^{2}\alpha^{2m}+\alpha^{m/2}\left\|\log Z \right\|_{3,\infty}C_{\chi}\left\|\psi-\psi^{\star}\right\|)\] \[\leq 2\sigma_{\star}^{2}+2\sigma_{\psi}^{2}+2L^{2}\left\|\psi-\psi^{ \star}\right\|^{2}+4(\sigma_{\psi}^{2}\alpha^{2m}+\alpha^{m/2}\left\|\log Z \right\|_{3,\infty}C_{\chi}\left\|\psi-\psi^{\star}\right\|)\]

### Proof of the SGD recursion (Lemma 3.1)

We are now ready to provide an SGD-style recursion for the expected squared distance to the optimum \(\mathbb{E}\left\left[\left\|\psi_{t}-\psi^{\star}\right\|^{2}\right]\).

**Lemma** (Restatement of Lemma 3.1).: _Let \(\psi_{t}\) be the iterates produced by Algorithm 1. Denote \(\delta_{t}=\mathbb{E}\left[\left\|\psi_{t}-\psi^{\star}\right\|^{2}\right]\), \(\sigma_{\star}=(\mathbb{E}_{p_{\psi^{\star}}}\|\phi-\mathbb{E}_{p_{\psi^{\star}}} \phi\|^{2})^{1/2}\), and \(\sigma_{t}=(\mathbb{E}_{p_{\psi_{t}}}\|\phi-\mathbb{E}_{p_{\psi_{t}}}\phi\|^{2} )^{1/2}\). Then, under Assumptions A1, A2 and A3, for all \(t\geq 1\), we have:_

\[\delta_{t}\leq(1-2\eta_{t}\tilde{\mu}_{m,t-1}+2\eta_{t}^{2}L^{2})\delta_{t-1}+ \eta_{t}^{2}\tilde{\sigma}_{m,t-1}^{2}+4\alpha^{m/2}\eta_{t}^{2}\left\|\log Z \right\|_{2,\infty}C_{\chi}\delta_{t-1}^{1/2}\]

_where \(\left\|\log Z\right\|_{3,\infty}\) is a constant, \(\tilde{\mu}_{m,t}\coloneqq\mu-\alpha^{m}\sigma_{t}C_{\chi}\), and \(\tilde{\sigma}_{m,t}\coloneqq(\sigma_{\star}^{2}+\sigma_{t}^{2}+2\sigma_{t}^{2} \alpha^{2m})^{1/2}\)._

Proof.: In this proof, we note \((\mathcal{F}_{t})_{t\geq 0}\), the increasing family of \(\sigma\)-algebras generated by the random variables \((X_{t})_{t\geq 0}\sim p_{\psi^{\star}}\) and the Markov chain samples \(\tilde{X}_{t}^{m}|X_{t},\psi_{t}\sim k_{\psi_{t}}^{m}(X_{t},\cdot)\). We decompose the integrand of \(\delta_{t}\) as follows:

\[\left\|\psi_{t}-\psi_{n}^{\star}\right\|^{2} =\left\|\operatorname{Proj}_{\Psi}(\psi_{t-1}-\eta_{t}h_{t}(\psi_{t -1}))-\psi_{n}^{\star}\right\|^{2}\] \[\leq\left\|\psi_{t-1}-\eta_{t}h_{t}(\psi_{t-1})-\psi_{n}^{\star} \right\|^{2}\quad\text{(By Lemma C.4)}\] \[=\left\|\psi_{t-1}-\psi^{\star}\right\|^{2}-2\eta_{t}\left\langle h _{t}(\psi_{t-1}),\psi_{t-1}-\psi_{n}^{\star}\right\rangle+\eta_{t}^{2}\left\|h_{t }(\psi_{t-1})\right\|^{2}\]The first term is (up to an averaging operation) the previous iterate. The middle term will ensure (provided \(m\) is large enough) contraction of the expected distance to the optimum. Finally, the third term can be described by Lemma D.4, and essentially behaves like the second moment of a log-likelihood stochastic gradient. Indeed, noting \(g(\psi)\coloneqq-\mathbb{E}_{p_{\psi^{*}}}\phi+\mathbb{E}_{p_{\psi}}\phi\) the expectation of \(g_{t}\) w.r.t \(x_{t}\) (which is the gradient of the negative cross-entropy between \(p_{\psi}\) and \(p_{\psi^{*}}\)), we have:

\[\langle h_{t}(\psi_{n,t-1}),\psi_{t-1}-\psi^{*}_{n}\rangle=\langle g(\psi_{n,t- 1}),\psi_{t-1}-\psi^{*}_{n}\rangle+\underbrace{\langle(h_{t}(\psi_{n,t-1})-g( \psi_{n,t-1})),\psi_{t-1}-\psi^{*}_{n}\rangle}_{\Delta}\]

and applying Lemma C.7, we get that

\[h_{t}(\psi_{t-1})-g(\psi_{t-1}) =\phi(k^{m}(X_{t},\cdot))-\mathbb{E}_{p_{\psi}}\phi\] \[\implies\mathbb{E}_{p_{\psi}}\mathbb{E}_{k^{m}_{\psi}(X_{t}, \cdot)}\left[h_{t}(\psi_{t-1})-g(\psi_{t-1})|\mathcal{F}_{t-1}\right] =P^{m}_{\psi}\phi-\mathbb{E}_{p_{\psi}}\phi\;,\]

meaning

\[\left\lvert\mathbb{E}\left[\Delta|\mathcal{F}_{t-1}\right]\right\rvert \leq\left\lVert\mathbb{E}_{p_{\psi}},P^{m}_{\psi}(\phi-\mathbb{E} _{p_{\psi}}\phi)\right\rVert\left\lVert\psi_{t-1}-\psi^{*}\right\rVert\] \[\leq\alpha^{m}C_{\chi}(\mathbb{E}_{p_{\psi_{t-1}}}\left\lVert f- \mathbb{E}_{p_{\psi_{t-1}}}f\right\rVert^{2})^{1/2}\left\lVert\psi-\psi^{*} \right\rVert^{2}\] \[\leq\alpha^{m}\sigma_{t-1}C_{\chi}\left\lVert\psi-\psi^{*}\right\rVert ^{2}\]

On the other hand, by applying Lemma C.8 to \(g\), we have:

\[\langle g(\psi_{t-1}),\psi_{t-1}-\psi^{*}\rangle=\langle g(\psi_{t-1})-g(\psi^ {*}),\psi_{t-1}-\psi^{*}\rangle\geq\mu\left\lVert\psi_{t-1}-\psi^{*}\right\rVert ^{2}\]

Combining the above results, we obtain:

\[\mathbb{E}_{p_{\psi}}\mathbb{E}_{k^{m}_{\psi}(x,\cdot)}\left[ \left\lVert\psi_{t}-\psi^{*}\right\rVert^{2}|\mathcal{F}_{t-1}\right]\] \[\leq(1-2\eta_{t}(\mu-\alpha^{m}\sigma_{t-1}C_{\chi}))\|\psi_{t-1 }-\psi^{*}\|^{2}\] \[\quad+\eta_{t}^{2}(2\sigma_{\star}^{2}+2\sigma_{t-1}^{2}+2L^{2} \left\lVert\psi_{t-1}-\psi^{*}\right\rVert^{2}+4(\sigma_{t-1}^{2}\alpha^{2m}+ \alpha^{m/2}\left\lVert\log Z\right\rVert_{3,\infty}C_{\chi}\left\lVert\psi_{t -1}-\psi^{*}\right\rVert))\]

And the result follows by integrating over \(\mathcal{F}_{t-1}\). 

### Proof of Online CD convergence

We now prove Theorem 3.2. The recursion of Lemma 3.1 is almost identifiable, up to a cross-term of second order, with the one of an SGD algorithm as presented in the setting of [22, Theorem 1]. To make the identification exact, we use the bound \(4\alpha^{m/2}\eta_{t}^{2}\left\lVert\log Z\right\rVert_{3,\infty}C_{\chi} \delta_{t-1}^{1/2}\leq 2\alpha^{m/2}\eta_{t}^{2}\delta_{t}+2\alpha^{m/2}\eta_{t}^{2} \left\lVert\log Z\right\rVert_{3,\infty}^{2}C_{\chi}^{2}\), yielding the following recursion:

\[\delta_{t}\leq(1-2\eta_{t}(\mu-\alpha^{m}\sigma C_{\chi})+2\eta_{t}^{2}(L^{2}+ \alpha^{m/2}))\delta_{t-1}+(\sigma^{2}(2+2\alpha^{2m})+\alpha^{m/2}\left\lVert \log Z\right\rVert_{3,\infty}^{2}C_{\chi}^{2})\eta_{t}^{2}\]

where we used the fact that \(\tilde{\sigma}_{m,t}\leq\sigma\). This recursion is of the same form as the one studied in [22, Equation 6, Theorem 1] given by:

\[\delta_{t}\leqslant\left(1-2\mu\gamma_{t}+2L^{2}\gamma_{t}^{2}\right)\delta_ {t-1}+2\sigma^{2}\gamma_{t}^{2}\]

by identifying:

\[\sigma^{2} \leftarrow\sigma^{2}(2+2\alpha^{2m})+\alpha^{m/2}\left\lVert\log Z \right\rVert_{3,\infty}^{2}\eqqcolon\tilde{\sigma}_{m}^{2}\] \[L^{2} \leftarrow(L^{2}+\alpha^{m/2})\eqqcolon\tilde{L}^{2}\] \[\mu \leftarrow\mu-\alpha^{m}\sigma C_{\chi}\eqqcolon\tilde{\mu}_{m}\] \[\gamma_{t} \leftarrow\eta_{t}\]

We can use the same unrolling strategy as theirs (the only condition required to proceed is that \(\tilde{\mu}_{m}>\tilde{L}\), which automatically holds since \(\mu<L\)), and we obtain

\[\delta_{n}\leqslant\begin{cases}2\exp\left(4\tilde{L}C^{2}\varphi_{1-2\beta}(n )\right)\exp\left(-\frac{\tilde{\mu}_{m}C}{4}n^{1-\beta}\right)\left(\delta_ {0}+\frac{\tilde{\sigma}_{m}^{2}}{\tilde{L}^{2}}\right)+\frac{4C\tilde{\sigma} _{m}^{2}}{\tilde{\mu}_{m}n^{\beta}},&\text{if }0\leqslant\beta<1\\ \frac{\exp\left(2\tilde{L}^{2}C^{2}\right)}{n^{\beta}m^{C}}\left(\delta_{0}+ \frac{\tilde{\sigma}_{m}^{2}}{\tilde{L}^{2}}\right)+2\tilde{\sigma}_{m}^{2}C^{ 2}\frac{\varphi_{\tilde{\mu}_{m}C/2-1}(n)}{n^{\beta}m^{C/2}},&\text{if }\beta=1.\end{cases}\]

### Proof of online CD with averaging (Theorem 3.3)

We first restate the theorem in its complete form.

**Theorem** (Contrastive Divergence with Polyak-Ruppert averaging).: _Let \((\psi_{t})_{t\geq 0}\) the sequence of iterates obtained by running the CD algorithm with a learning rate \(\eta_{t}=Ct^{-\beta}\) for \(\beta\in(\frac{1}{2},1).\) Define \(\bar{\psi}_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\psi_{i}\). Then, under the same assumptions as Theorem 3.2 we have, for all \(n\geq 1\),_

\[\sqrt{\mathbb{E}\left\|\overline{\psi}_{n}-\psi^{\star}\right\|^{2}}\ \leq\ 2\sqrt{\frac{\mathrm{tr}(\mathcal{I}(\psi)^{-1})}{n}}+\mathcal{O}\left(n^{ \max\left(-\left(\frac{1}{2}+\frac{\beta}{2}\right),-\beta,\frac{\beta}{2}-1,- \left(\frac{\beta}{2}+m\frac{\log\alpha}{\log n}\right)\right)\right)}\]

_Where \(\mathcal{I}(\psi^{\star})\coloneqq\mathrm{Cov}_{p_{\psi^{\star}}}\)\([\phi]\) is the Fisher information matrix of the data distribution. Additionally, if \(m>\frac{(1-\beta)\log n}{2\lfloor\log\alpha\rfloor}\), we have \(\sqrt{\mathbb{E}\left\|\overline{\psi}_{n}-\psi^{\star}\right\|^{2}}\ \leq\ 2\sqrt{\frac{ \mathrm{tr}(\mathcal{I}(\psi)^{-1})}{n}}+o\left(n^{-1/2}\right)\)._

Throughout the proof, we will denote by \(h_{n}\) the standard online CD gradient defined in Equation 3:

\[h_{n}(\psi_{n-1})=-\phi(X_{n})+\phi(k_{\psi_{n-1}}^{m}(\cdot,X_{n})),\quad X_{ n}\sim p_{\psi^{\star}},\quad\forall n\in\mathbb{N}\setminus\{0\},\]

as well as

\[\overline{h}(\psi_{n-1})\coloneqq\mathbb{E}\left[h(\psi_{n-1})\,|\,\psi_{n-1} \right]=\mathbb{E}_{p_{\psi^{\star}}}\phi(X_{n})+\mathbb{E}_{p_{\psi^{\star}}} \mathbb{E}_{k_{\psi_{n-1}}^{m}}\phi(k_{\psi_{n-1}}^{m}(X_{n},\cdot)).\]

We start by establishing some intermediate lemmas.

**Lemma D.5**.: _Under Assumptions A1, A2, A3, the online CD iterates produced by Algorithm 1 using \(\eta_{t}=Ct^{-\beta}\) for \(\beta\in(\frac{1}{2},1)\) verify_

\[\frac{1}{n}\sqrt{\sum_{i=1}^{n}\left(\mathbb{E}\left[\left\|\psi_{i}-\psi^{ \star}\right\|^{2}\right]\right)}=\mathcal{O}(n^{-\frac{1}{2}-\frac{\beta}{2} }).\]

Proof.: Let us note \(\delta_{n}=\mathbb{E}\left[\left\|\psi_{n}-\psi^{\star}\right\|^{2}\right]\). Summing the r.h.s of Theorem 3.2, we have

\[\sum_{i=1}^{n}\delta_{i}\leq\sum_{i=1}^{n}\frac{4C\bar{\sigma}_{m}^{2}}{\bar{ \mu}_{m}i^{\beta}}+2\left(\delta_{0}+\frac{\bar{\sigma}_{m}^{2}}{\bar{L}^{2}} \right)\underbrace{\sum_{i=1}^{n}e^{4\bar{L}C^{2}\varphi_{1-2\beta}(i)}e^{- \frac{\bar{\mu}_{m}\bar{C}}{4}n^{1-\beta}}}_{A_{3}}\]

\[\implies\frac{1}{n}\sqrt{\sum_{i=1}^{n}\delta_{i}}\leq\frac{1}{n}\sqrt{ \frac{4C\bar{\sigma}_{m}^{2}}{\bar{\mu}_{m}}\varphi_{1-\beta}(n)}+\frac{1}{n} \sqrt{\left(2(\delta_{0}+\frac{\bar{\sigma}_{m}^{2}}{\bar{L}^{2}})A_{3} \right)}=\mathcal{O}\left(n^{-\frac{1}{2}-\frac{\beta}{2}}\right).\]

where \(A_{3}\) is finite if \(\beta<1\), and \(A_{3}=O(n)\) otherwise [22]. 

**Lemma D.6**.: _Under Assumptions A1, A2, A3, the online CD iterates produced by Algorithm 1 using \(\eta_{t}=Ct^{-\beta}\) for \(\beta\in(\frac{1}{2},1)\) verify_

\[\frac{1}{n}\sqrt{\sum_{i=1}^{n}\left(\mathbb{E}\left[\left\|\psi_{i}-\psi^{ \star}\right\|^{2}\right]\right)^{1/2}}=\mathcal{O}(n^{-\frac{1}{2}-\frac{ \beta}{4}}).\]

Proof.: Let us note \(\delta_{n}=\mathbb{E}\left[\left\|\psi_{n}-\psi^{\star}\right\|^{2}\right]\). Applying \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\) to the r.h.s of Theorem 3.2, we have

\[\sum_{i=1}^{n}\delta_{i}^{1/2}\leq\frac{2C^{1/2}\bar{\sigma}_{m}}{2\bar{\mu}_{ m}^{1/2}}\sum_{i=1}^{n}i^{-\beta/2}+\sqrt{2\Big{(}\delta_{0}+\frac{\bar{\sigma}_{m} ^{2}}{\bar{L}^{2}}\Big{)}}\underbrace{\sum_{i=1}^{n}e^{2\bar{L}^{2}C^{2}\varphi _{1-2\beta}(i)}e^{-\frac{\bar{\mu}_{m}\bar{C}}{8}i^{1-\beta}}}_{A_{4}}\]

\[\implies\frac{1}{n}\sqrt{\sum_{i=1}^{n}\delta_{i}^{1/2}}\leq\frac{1}{n}\sqrt{ \frac{2C^{1/2}\bar{\sigma}_{m}}{\bar{\mu}_{m}^{1/2}}\varphi_{1-\beta/2}(n)}+ \frac{1}{n}\left(\sqrt{2\left(\delta_{0}+\frac{\bar{\sigma}_{m}^{2}}{\bar{L}^{2 }}\right)}A_{4}\right)^{1/2}=\mathcal{O}\left(n^{-\frac{1}{2}-\frac{\beta}{4} }\right).\]

where \(A_{4}\) is finite if \(\beta<1\), and \(A_{4}=O(n)\) otherwise [22].

**Lemma D.7**.: _Under Assumptions A1, A2, A3, the online CD iterates produced by Algorithm 1 using \(\eta_{t}=Ct^{-\beta}\) for \(\beta\in(\frac{1}{2},1)\) verify_

\[\sqrt{\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}h_{i}(\psi_{i-1})\right\| ^{2}\right]}=\mathcal{O}\left(n^{\frac{\beta}{2}-1}\right).\]

Proof.: The result follows from the fact that \(h_{t}\) verifies

\[h_{t}(\psi_{t-1})\ =\ \frac{1}{\eta_{t}}(\psi_{t-1}-\psi_{t})\.\]

A similar quantity was handled in the case of standard SGD [22, Theorem 3], and the only condition needed to reuse their steps is that \((\psi_{t})_{t\leq n}\) satisfies an upper bound of the same form as the one [22, Theorem 1] derived. This is precisely the nature of our bound of \(\psi_{t}\) estblished in Theorem 3.2, with \(\tilde{\mu}_{m},\tilde{\sigma}_{m},\tilde{L}\). Borrowing on their result, we have:

\[\sqrt{\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}h_{i}(\psi _{i-1})\right\|^{2}\right]} \leq\frac{4\sigma_{m}\beta}{C^{1/2}n\tilde{\mu}_{m}}\varphi_{ \beta/2}(n)+\frac{4\beta}{Cn\tilde{\mu}_{m}^{1/2}}\Big{(}\delta_{0}+\frac{ \tilde{\sigma}_{m}^{2}}{\tilde{L}^{2}}\Big{)}^{1/2}A_{2}\] \[\quad+\frac{1}{n\tilde{\mu}_{m}^{1/2}}\Big{(}\frac{1}{C}+2 \tilde{L}\Big{)}\delta_{0}^{1/2}+\frac{2\tilde{L}}{n\tilde{\mu}_{m}^{1/2}} \frac{2C^{1/2}\tilde{\sigma}_{m}}{\tilde{\mu}_{m}^{1/2}}\varphi_{1-\beta}(n)^ {1/2}\] \[\quad+\frac{4\tilde{L}}{n\tilde{\mu}_{m}^{1/2}}\Big{(}\delta_{0}+ \frac{\tilde{\sigma}_{m}^{2}}{\tilde{L}^{2}}\Big{)}^{1/2}A_{2}^{1/2}\]

where \(\tilde{\mu}_{m},\tilde{\sigma}_{m}\) and \(\tilde{L}\) are defined in 3.2, and

\[A_{2}\ =\ \sum_{k=1}^{n}e^{\frac{-\tilde{\sigma}_{m}\mathbb{C}}{16}k^{1- \beta}+16\tilde{L}_{1}^{4}C^{4}\varphi_{1-2\beta}(k)}\qed\]

**Lemma D.8**.: _Under Assumptions A1, A2, A3, the online CD iterates produced by Algorithm 1 using \(\eta_{t}=Ct^{-\beta}\) for \(\beta\in(\frac{1}{2},1)\) verify_

\[\frac{1}{n}\sum_{i=1}^{n}\sqrt{\mathbb{E}\left[\left\|\psi_{i}-\psi^{*} \right\|^{4}\right]}=\mathcal{O}(n^{-\beta}).\]

Proof.: We proceed as in the proof of [22, Theorem 3], first establishing a recurrence for \(\mathbb{E}\left[\left\|\psi_{i}-\psi^{*}\right\|^{4}\right]\), and then unrolling it. We have

\[\mathbb{E}\left[\left\|\psi_{n}-\psi^{*}\right\|^{4}\mid\mathcal{ F}_{n-1}\right]\leqslant \left\|\psi_{n-1}-\psi^{*}\right\|^{4}+6\eta_{n}^{2}\left\|\psi_{ n-1}-\psi^{*}\right\|^{2}\mathbb{E}\left[\left\|h_{n}\left(\psi_{n-1} \right)\right\|^{2}\mid\mathcal{F}_{n-1}\right]\] \[+\eta_{n}^{4}\mathbb{E}\left[\left\|h_{n}\left(\psi_{n-1} \right)\right\|^{4}\mid\mathcal{F}_{n-1}\right]\] \[-4\eta_{n}\left\|\psi_{n-1}-\psi^{*}\right\|^{2}\left\langle\psi_ {n-1}-\psi^{*},\mathbb{E}\left[h_{n}\right]\right\rangle\] \[+4\eta_{n}^{3}\left\|\psi_{n-1}-\psi^{*}\right\|\mathbb{E}\left[ \left\|h_{n}\left(\psi_{n-1}\right)\right\|^{3}\mid\mathcal{F}_{n-1}\right].\]

The second and fourth terms will be controlled using results from our previous sections. For simplicity, we don't attempt to relate the moments of \(\left\|h_{n}\right\|^{4}\) as precisely as before. Instead we use:

\[\mathbb{E}_{p_{\psi^{*}}}\left[\left\|h_{n}\right\|^{k}\right]\leq 2^{k-1} \left(\mathbb{E}_{p_{\psi^{*}}}\mathbb{E}_{k_{p_{\psi_{n-1}}}^{k_{m}}}\left[ \left\|\phi(k_{\psi_{n-1}}^{m}(X_{n},\cdot))\right\|^{k}\right]+\mathbb{E}_{p_ {\psi^{*}}}\left[\left\|\phi(X_{n})\right\|^{k}\right]\right)\]

And let us note \(\tau=\left(\sup_{\psi\in\Psi}\mathbb{E}_{p_{\psi}}\left\|\phi\right\|^{8} \right)^{1/8}\). We have, for \(k\geq 4\),

\[\mathbb{E}_{p_{\psi^{*}}}\mathbb{E}_{k_{p_{\psi_{n-1}}}^{m}}\| \phi(k_{\psi_{n-1}}^{m}(X_{n},\cdot))\|^{k} \leq\mathbb{E}_{p_{\psi_{n-1}}}\|\phi\|^{k}+C_{\chi}(\mathbb{E}_{p _{\psi_{n-1}}}(\left\|\phi\right\|^{k}-\mathbb{E}_{p_{\psi_{n-1}}}\left\|\phi \right\|^{k})^{2})^{1/2}\|\psi_{n-1}-\psi^{*}\|\] \[\leq\tau^{k}+2C_{\chi}\tau^{k}\|\psi_{n-1}-\psi^{*}\|\]Where we used the fact that \(P_{\psi_{n-1}}^{m}\) is a contraction, and \((\mathbb{E}\left\|\phi\right\|^{k})^{1/k}\) is an increasing function of \(k\). On the other hand, we simply have \(\mathbb{E}_{p_{\psi^{*}}}\left\|\phi(X_{n},\cdot)\right\|^{k}\leq\tau^{k}\). Plugging this into the previous equation, we obtain

\[\mathbb{E}\left[\left\|\psi_{n}-\psi^{*}\right\|^{4}\mid\mathcal{F }_{n-1}\right]\leqslant \left\|\psi_{n-1}-\psi^{*}\right\|^{4}\] \[+6\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{*}\right\|^{2}\left(4\tau^{ 2}+4C_{\chi}\tau^{2}\left\|\psi_{n-1}-\psi^{\star}\right\|\right)\] \[+\eta_{n}^{4}(16\tau^{4}+16C_{\chi}\tau^{4}\left\|\psi_{n-1}-\psi^ {\star}\right\|)\] \[-4\eta_{n}\left\|\psi_{n-1}-\psi^{*}\right\|^{2}\left\langle\psi _{n-1}-\psi^{*},\mathbb{E}\left[h_{n}|\mathcal{F}_{n-1}\right]\right\rangle\] \[+4\eta_{n}^{3}\left\|\psi_{n-1}-\psi^{*}\right\|\left(8\tau^{3}+ 8C_{\chi}\tau^{3}\left\|\psi_{n-1}-\psi^{*}\right\|\right)\]

To simplify the recursion, we use the four following inequalities:

\[\tau^{2}\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{*}\right\|^{3} \leq\frac{1}{2}(\tau^{2}\eta_{n}^{2}(\left\|\psi_{n-1}-\psi^{*} \right\|^{2}+\tau^{2}\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{*}\right\|^{4})\] \[\tau^{4}\eta_{n}^{4}\left\|\psi_{n-1}-\psi^{*}\right\| \leq(\tau^{4}\eta_{n}^{4}+\frac{1}{4}\tau^{4}\eta_{n}^{4}\left\| \psi_{n-1}-\psi^{*}\right\|^{4}))\] \[\eta_{n}^{3}\tau^{3}\left\|\psi_{n-1}-\psi^{*}\right\| \leq\frac{1}{2}(\eta_{n}^{2}\tau^{2}\left\|\psi_{n-1}-\psi^{*} \right\|^{2}+16\eta_{n}^{4}\tau^{4}\] \[\tau^{3}\eta_{n}^{3}\left\|\psi_{n-1}-\psi^{*}\right\|^{2} \leq\frac{1}{2}(\eta_{n}^{4}\tau^{4}+\left\|\psi_{n-1}-\psi^{*} \right\|^{2}\eta_{n}^{2}\tau^{2})\]

Injecting them in our recursion, we obtain:

\[\mathbb{E}\left[\left\|\psi_{n}-\psi^{*}\right\|^{4}\mid\mathcal{ F}_{n-1}\right]\leqslant \left\|\psi_{n-1}-\psi^{*}\right\|^{4}\] \[+12\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{*}\right\|^{2}\tau^{2}\] \[+12C_{\chi}\tau^{2}\eta_{n}^{2}\|\psi_{n-1}-\psi^{*}\|^{2}\] \[+12C_{\chi}\tau^{2}\eta_{n}^{2}\|\psi_{n-1}-\psi^{*}\|^{4}\] \[+16\eta_{n}^{4}\tau^{4}\] \[+16\eta_{n}^{4}\tau^{4}\] \[+16\eta_{n}^{4}C_{\chi}\tau^{4}\] \[+16\eta_{n}^{2}C_{\chi}\tau^{2}\left\|\psi_{n-1}-\psi^{*}\right\| ^{2}\,\]which, after further simplifications, yields

\[\mathbb{E}\big{[} \left\|\psi_{n}-\psi^{\star}\right\|^{4}\mid\mathcal{F}_{n-1}\big{]}\] \[\leq\left\|\psi_{n-1}-\psi^{\star}\right\|^{4}(1-4\eta_{n}\tilde{ \mu}_{m}+12C_{\chi}\eta_{n}^{2}\tau^{2}+4C_{\chi}\tau^{4}\eta_{n}^{4})\] \[+\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{\star}\right\|^{2}(28(1+C_{ \chi})\tau^{2})+32\eta_{n}^{4}(\tau^{4}(1+C_{\chi}))\] \[\leq\left\|\psi_{n-1}-\psi^{\star}\right\|^{4}(1-4\eta_{n}\tilde{ \mu}_{m}+12(1+C_{\chi})\eta_{n}^{2}\tau^{2}+4(1+C_{\chi})\tau^{4}\eta_{n}^{4})\] \[+28\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{\star}\right\|^{2}((1+C_{ \chi})\tau)^{2}+32\eta_{n}^{4}(1+C_{\chi})\tau)^{4}\] \[\leq\left\|\psi_{n-1}-\psi^{\star}\right\|^{4}(1-4\eta_{n}\tilde{ \mu}_{m}+12\eta_{n}^{2}((1+C_{\chi})\tau)^{2}+4((1+C_{\chi})\tau)^{4}\eta_{n}^ {4})\] \[+28\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{\star}\right\|^{2}((1+C_{ \chi})\tau)^{2}+32\eta_{n}^{4}(1+C_{\chi})\tau)^{4}\] \[\leq\left\|\psi_{n-1}-\psi^{\star}\right\|^{4}(1-4\eta_{n}\tilde{ \mu}_{m}+12\eta_{n}^{2}(2(1+C_{\chi})\tau)^{2}+16\eta_{n}^{2}(2(1+C_{\chi}) \tau)^{3}\] \[+4(2(1+C_{\chi})\tau)^{4}\eta_{n}^{4})+20\eta_{n}^{2}\left\|\psi_ {n-1}-\psi^{\star}\right\|^{2}(2(1+C_{\chi})\tau)^{2}+16\eta_{n}^{4}(2(1+C_{ \chi})\tau)^{4}\] \[\leq\left\|\psi_{n-1}-\psi^{\star}\right\|^{4}(1-4\eta_{n}\tilde{ \mu}_{m}+12\eta_{n}^{2}(2(1+C_{\chi})\tau+L)^{2}+16\eta_{n}^{2}(2(1+C_{\chi}) \tau+L)^{3}\] \[+4(2(1+C_{\chi})\tau+L)^{4}\eta_{n}^{4})+20\eta_{n}^{2}\left\| \psi_{n-1}-\psi^{\star}\right\|^{2}(2(1+C_{\chi})\tau)^{2}+16\eta_{n}^{4}(2(1+ C_{\chi})\tau)^{4}\] \[+4(2(1+C_{\chi})\tau+L)^{4}\eta_{n}^{4})+20\eta_{n}^{2}\left\| \psi_{n-1}-\psi^{\star}\right\|^{2}(2(1+C_{\chi})\tau)^{2}+16\eta_{n}^{4}(2(1+ C_{\chi})\tau)^{4}\] \[\leq\left\|\psi_{n-1}-\psi^{\star}\right\|^{4}(1-4\eta_{n}\tilde{ \mu}_{m}+12\eta_{n}^{2}\tilde{L}_{1}^{2}+16\eta_{n}^{2}\tilde{L}_{1}^{3}+4 \tilde{L}_{1}^{4}\eta_{n}^{4})\] \[+20\eta_{n}^{2}\left\|\psi_{n-1}-\psi^{\star}\right\|^{2}\tilde{ \tau}_{1}^{2}+16\eta_{n}^{4}\tilde{\tau}_{2}^{4}\]

where we defined \(\tilde{\tau}_{1}\coloneqq 2(1+C_{\chi})\tau\) and \(\tilde{L}_{1}\coloneqq 2(1+C_{\chi})\tau+L\). This recursion is of the form of the one studied in [22, Equation 32] (note that by design, \(\tilde{L}\geq\tilde{\mu}_{m}\).) The steps performed to bound \(\mathbb{E}\left[\left\|\psi_{i}-\psi^{\star}\right\|^{4}\right]\) thus follow from their derivations, and we obtain:

\[\frac{1}{n}\sqrt{\sum_{i=1}^{n}\mathbb{E}\left\|\psi_{i-1}-\psi^{ \star}\right\|^{4}}\] \[\leq\frac{C\tilde{\tau}_{1}^{2}}{2n}\Big{(}C^{1/2}\varphi_{1-3 \beta/2}(n)+\tilde{\mu}_{m}^{-1/2}\varphi_{1-\beta}(n)\Big{)}\] \[\quad+\frac{\sqrt{20}C^{1/2}\tilde{\tau}_{1}}{2n}A_{1}\exp\left(2 4\tilde{L}_{1}^{4}C^{4}\right)\Big{(}\delta_{0}+\frac{\tilde{\mu}_{m}\mathbb{E }\left\|\theta_{0}-\theta^{\star}\right\|^{4}}{20C\tilde{\tau}_{1}^{2}}+2 \tilde{\tau}_{1}^{2}C^{3}\tilde{\mu}_{m}+8\tilde{\tau}_{1}^{2}C^{2}\Big{)}^{1/2}\] \[=\mathcal{O}\left(n^{-\beta}\right),\]

where

\[A_{1}=\sum_{k=1}^{n}e^{\frac{-\beta C}{16}k^{1-\beta}+16\tilde{L}_{1}^{4}C^{4} \varphi_{1-2\beta}(k)}\]

and we have \(A(1)<+\infty\) if \(\beta<1\), and \(A(1)=O(n)\) otherwise. 

**Lemma D.9**.: _For all \(\psi\in\Psi\), we have:_

\[\left\|\mathrm{Cov}\left[\phi(k_{\psi}^{m}(X_{n},\cdot))\right]- \mathrm{Cov}_{p_{\psi}}\left[\phi(X_{n})\right]\right\|_{\mathrm{F}}\] \[\leq\alpha^{m}C_{\chi}(\bar{\tau}^{1/2}+2\|\log Z\|_{4,\infty} \sigma)\|\psi-\psi^{\star}\|+\alpha^{2m}C_{\chi}^{2}\sigma^{2}\left\|\psi-\psi^{ \star}\right\|^{2}.\]

_where \(\bar{\tau}\coloneqq\sup_{\psi\in\Psi}\mathbb{E}_{p_{\psi}}\left[\left\|\phi\phi^{ \top}-\mathbb{E}_{p_{\psi}}\!\left[\phi\phi^{\top}\right]\right\|_{F}^{2} \right]<+\infty\) and \(\|\log Z\|_{4,\infty}\coloneqq\sup_{\psi\in\Psi}\left\|\mathbb{E}_{\psi}\phi \right\|\leq\sup_{\psi\in\Psi}\sum_{i=1}^{d}\partial_{i}^{2}\log Z(\psi)^{2}\)._

Proof.: We have

\[\mathrm{Cov}\left[\phi(k_{\psi}^{m}(X_{n},\cdot))\right]=\mathbb{E}_{p_{\psi^{ \star}}}P_{\psi}^{m}\left[\phi\phi^{\top}\right]-\left(\mathbb{E}_{p_{\psi^{ \star}}}[P_{\psi}^{m}\phi]\right)\left(\mathbb{E}_{p_{\psi^{\star}}}[P_{\psi}^{m }\phi]^{\top}\right)\]

Looking at the second moment first, we have

\[\mathbb{E}_{p_{\psi^{\star}}}\left[P_{\psi}^{m}\phi\phi^{\top}\right]-\mathbb{E}_ {p_{\psi}}\phi\phi^{\top}=\underbrace{\mathbb{E}_{p_{\psi^{\star}}}P_{\psi}^{m}( \phi\phi^{\top}-\mathbb{E}_{p_{\psi}}\phi\phi^{\top})}_{\Delta_{1}}.\]

Applying lemma C.7 to the \(\mathbb{R}^{d^{2}}\)-valued function \(f\) given by \(f_{ij}\coloneqq\phi_{i}\phi_{j}-\mathbb{E}_{p_{\psi}}\phi_{i}\phi_{j}\),

\[\left\|\Delta_{1}\right\|_{\mathrm{F}}\leq\left\|\psi-\psi^{\star}\right\| \alpha^{m}C_{\chi}\sqrt{\mathbb{E}_{p_{\psi}}\left[\left\|\phi\phi^{\top}- \mathbb{E}_{p_{\psi}}\phi\phi^{\top}\right\|_{F}^{2}\right]}\leq\bar{\tau}^{1/2 }\alpha^{m}C_{\chi}\left\|\psi-\psi^{\star}\right\|.\]We now investigate the first moment. We have

\[\mathbb{E}_{p_{\psi^{\star}}}\left[P_{\psi}^{m}\phi\right]=\underbrace{ \mathbb{E}_{p_{\psi^{\star}}}\left[P_{\psi}^{m}\phi\right]-\mathbb{E}_{p_{\psi} }\left[\phi\right]}_{\Delta_{2,1}}+\mathbb{E}_{p_{\psi_{m}}}\phi\] \[\implies\underbrace{(\mathbb{E}_{p_{\psi^{\star}}}P_{\psi}^{m} \phi)(\mathbb{E}_{p_{\psi^{\star}}}P_{\psi}^{m}\phi)^{\top}-\mathbb{E}_{p_{\psi }}\phi\mathbb{E}_{p_{\psi}}\phi^{\top}}_{\Delta_{2}}=\Delta_{2,1}\Delta_{2,1}^{ T}+\Delta_{2,1}\mathbb{E}_{p_{\psi}}\phi^{\top}+\mathbb{E}_{p_{\psi}}\phi \Delta_{2,1}^{\top}\]

and thus, applying Lemma C.7 on \(\Delta_{2,1}\), we have:

\[\left\|\Delta_{2}\right\|_{\mathrm{F}} \leq\left\|\Delta_{2,1}\Delta_{2,1}^{\top}+\Delta_{2,1}\mathbb{E} _{p_{\psi}}\left[\phi^{\top}\right]+\mathbb{E}_{p_{\psi}}\left[\phi\right] \Delta_{2,1}^{\top}\right\|_{\mathrm{F}}\] \[\leq\left\|\Delta_{2,1}\Delta_{2,1}^{\top}\right\|_{\mathrm{F}}+2 \left\|\Delta_{2,1}\right\|\left\|\mathbb{E}_{p_{\psi}}\left[\phi\right]\right\|\] \[\leq\alpha^{2m}\sigma^{2}C_{\chi}^{2}\left\|\psi-\psi^{\star} \right\|^{2}+2\left\|\log Z\right\|_{4,\infty}\alpha^{m}C_{\chi}\sigma\left\| \psi-\psi^{\star}\right\|,\]

where we used that \(\left\|\Delta_{2,1}\Delta_{2,1}^{\top}\right\|_{\mathrm{F}}=\left\|\Delta_{2,1 }\right\|^{2}\). We can now combine our two matrix moment bounds to obtain

\[\left\|\mathrm{Cov}\left[\phi(k_{\psi}^{m}(X_{n},\cdot))\right]- \mathrm{Cov}_{p_{\psi}}\left[\phi(X_{n})\right]\right\|_{\mathrm{F}}=\left\| \Delta_{1}+\Delta_{2}\right\|_{\mathrm{F}}\] \[\leq\alpha^{m}C_{\chi}(\bar{\tau}^{1/2}+2\left\|\log Z\right\|_{4,\infty}\sigma)\|\psi-\psi^{\star}\|+\alpha^{2m}C_{\chi}^{2}\sigma^{2}\left\| \psi-\psi^{\star}\right\|^{2}.\]

**Lemma D.10**.: _Under Assumptions A1, A2, A3 it holds that_

\[\sqrt{\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}f^{\prime \prime}(\psi^{\star})^{-1}(\overline{h}(\psi_{i-1})-h_{i}(\psi_{i-1}))\right\| ^{2}\right]}\,\leq\,2\sqrt{\frac{\mathrm{tr}(\mathcal{I}(\psi^{\star})^{-1})} {n}}\] \[\quad+\frac{\left\|\mathcal{I}(\psi^{\star})^{-2}\right\|_{ \mathrm{F}}^{1/2}}{n}\left((M+\alpha^{m}C_{\chi}(\bar{\tau}^{1/2}+2\|\log Z\|_ {4,\infty}\sigma))^{1/2}\left(\sum_{i=1}^{n}\delta_{i-1}^{1/2}\right)^{1/2}\right.\] \[\quad+\left.\alpha^{2m}C_{\chi}^{2}\sigma^{2}\left(\sum_{i=1}^{n} \delta_{i-1}\right)^{1/2}\right)\]

_where \(M\coloneqq\sup_{\psi\in\Psi}\left\|\nabla^{3}\log Z(\psi)\right\|_{\mathrm{op }(\left\|\cdot\right\|_{F},\left\|\cdot\right\|_{F})}<+\infty\)._

Proof.: \[f^{\prime\prime}(\psi^{\star})^{-1}(\overline{h}(\psi_{n-1})-h_{ n}(\psi_{n-1}))\] \[\quad=f^{\prime\prime}(\psi^{\star})^{-1}\underbrace{(\phi(X_{n}) -\mathbb{E}_{p_{\psi^{\star}}}\phi)}_{\Delta_{1,n}}+f^{\prime\prime}(\psi^{ \star})^{-1}\underbrace{(\phi(k_{\psi_{n-1}}^{m}(X_{n}))-\mathbb{E}_{p_{\psi^{ \star}}}\mathbb{E}_{k_{\psi_{n-1}}^{m}}\phi(k_{\psi_{n-1}}^{m}(X_{n},\cdot)))}_ {\Delta_{2,n}}\] \[\quad=f^{\prime\prime}(\psi^{\star})^{-1}\Delta_{1,n}+f^{\prime \prime}(\psi^{\star})^{-1}\Delta_{2,n}\]

Noting \(\Delta\) the l.h.s of Lemma D.10, we have, summing over \([n]\), and using Minkowski's inequality,

\[\Delta\ \leq\ \frac{1}{n}\sqrt{\mathbb{E}\left\|\sum_{i=1}^{n}f^{\prime \prime}(\psi^{\star})^{-1}\Delta_{1,i}\right\|^{2}}+\frac{1}{n}\sqrt{\mathbb{E} \left\|\sum_{i=1}^{n}f^{\prime\prime}(\psi^{\star})^{-1}\Delta_{2,i}\right\|^{2}}\]

Note that this step was made possible because we are looking at the square-root of the variance, which is unlike the recursion in Lemma 3.1. This allows to separate the terms and use fewer intermediaries than in the proof of Lemma 3.1.

Since both \(\Delta_{1,n}\) and \(\Delta_{2,n}\) are martingale differences with respect to the filtration \(\mathcal{F}_{n-1}\), the covariance terms vanish, and we have

\[\Delta \leq\frac{1}{n}\sqrt{\sum_{i=1}^{n}\mathbb{E}\left[\left\|f^{\prime \prime}(\psi^{\star})^{-1}\Delta_{1,i}\right\|^{2}\right]}+\frac{1}{n}\sqrt{ \sum_{i=1}^{n}\mathbb{E}\left[\left\|f^{\prime\prime}(\psi^{\star})^{-1} \Delta_{2,i}\right\|^{2}\right]}\] \[\leq\frac{1}{n}\sqrt{\sum_{i=1}^{n}\operatorname{tr}(f^{\prime \prime}(\psi^{\star})^{-1}\mathbb{E}\left[\Delta_{1,i}\Delta_{1,i}^{\top}f^{ \prime\prime}(\psi^{\star})^{-1})\right]}\] \[\quad+\frac{1}{n}\sqrt{\sum_{i=1}^{n}\operatorname{tr}(f^{\prime \prime}(\psi^{\star})^{-1}\mathbb{E}\left[\Delta_{2,i}\Delta_{2,i}^{\top}f^{ \prime\prime}(\psi^{\star})^{-1})\right]}\] \[\leq\sqrt{\frac{\operatorname{tr}(\mathcal{I}(\psi^{\star})^{-1 })}{n}}+\frac{1}{n}\sqrt{\sum_{i=1}^{n}\operatorname{tr}(f^{\prime\prime}(\psi ^{\star})^{-1}\text{Cov}\left[\phi(k_{\psi_{i-1}}^{m}(x_{i}))\right]f^{\prime \prime}(\psi^{\star})^{-1})}\] \[\leq\sqrt{\frac{\operatorname{tr}(\mathcal{I}(\psi^{\star})^{-1 })}{n}}+\frac{1}{n}\Big{(}\sum_{i=1}^{n}\operatorname{tr}(f^{\prime\prime}( \psi^{\star})^{-1}(\text{Cov}_{p_{\psi^{\star}}}\phi+(\text{Cov}_{p_{\psi_{i-1} }}\phi-\text{Cov}_{p_{\psi^{\star}}}\phi)\] \[\quad+(\text{Cov}\,\phi(k_{\psi_{i-1}}^{m}(x_{i}))-\text{Cov}_{p_ {\psi_{i-1}}}\phi))f^{\prime\prime}(\psi^{\star})^{-1})\Big{)}^{1/2}\] \[\leq 2\sqrt{\frac{\operatorname{tr}(\mathcal{I}(\psi^{\star})^{-1 })}{n}}+\frac{\sqrt{\operatorname{tr}(\mathcal{I}(\psi^{\star})^{-2})}}{n} \Big{(}\sum_{i=1}^{n}\|(\text{Cov}_{p_{\psi_{n-1}}}\phi-\text{Cov}_{p_{\psi^{ \star}}}\phi\|_{\text{F}}\] \[\quad+\|\text{Cov}\,\phi(k_{\psi}^{m}(x_{i}))-\text{Cov}_{p_{\psi }}\phi\|_{\text{F}}\Big{)}^{1/2}\] \[\stackrel{{(a)}}{{\leq}}2\sqrt{\frac{\operatorname{ tr}(\mathcal{I}(\psi^{\star})^{-1})}{n}}+\frac{\sqrt{\operatorname{tr}(\mathcal{I}( \psi^{\star})^{-2})}}{n}\left((M+\alpha^{m}C_{\chi}(\bar{\tau}^{1/2}+2\|\log Z \|_{4,\infty}\sigma)^{1/2})\times\right.\] \[\quad\left.\sqrt{\sum_{i=1}^{n}\|\psi_{i-1}-\psi^{\star}\|}+ \alpha^{m}C_{\chi}\sigma\sqrt{\sum_{i=1}^{n}\|\psi_{i-1}-\psi^{\star}\|^{2}}\right)\] \[\stackrel{{(b)}}{{\leq}}2\sqrt{\frac{\operatorname{ tr}(\mathcal{I}(\psi^{\star})^{-1})}{n}}+\frac{\sqrt{\operatorname{tr}( \mathcal{I}(\psi^{\star})^{-2})}}{n}\left((M+\alpha^{m}C_{\chi}(\bar{\tau}^{1/ 2}+2\|\log z\|_{4,\infty}\sigma))^{1/2}\sqrt{\sum_{i=1}^{n}\delta_{i}^{1/2}}\right.\] \[\quad\left.+\alpha^{m}C_{\chi}\sigma\sqrt{\sum_{i=1}^{n}\delta_{i }}\right)\]

In \((a)\) we used Lemma D.10, the cyclicity of the trace, \(\operatorname{tr}(A^{\top}B)\leq\left\|AB\right\|_{\text{F}}\leq\left\|A\right\| _{\text{F}}\left\|B\right\|_{\text{F}}\), and the fact that since \(\operatorname{Cov}_{p_{\psi_{i-1}}}\left[\phi(x_{i})\right]=\nabla_{\psi}^{2} \mathcal{L}(\psi_{i-1})\), by analycity of \(\mathcal{L}\), there exists a constant \(M\coloneqq\sup_{\psi\in\Psi}\left\|\nabla^{3}\log Z(\psi)\right\|_{\text{op}( \left\|\cdot\right\|,\left\|\cdot\right\|_{F})}\) such that

\[\|\nabla_{\psi}^{2}\mathcal{L}(\psi_{i-1})-\nabla_{\psi}^{2}\mathcal{L}(\psi^{ \star})\|_{\text{F}}\leq M\left\|\psi_{i-1}-\psi^{\star}\right\|.\]

In \((b)\) we used Jensen's inequality to get \(\mathbb{E}\left[\left\|\psi_{i-1}-\psi^{\star}\right\|\right]\leq\sqrt{ \mathbb{E}\left[\left\|\psi_{i-1}-\psi^{\star}\right\|^{2}\right]}=\delta_{i-1}^ {1/2}\). 

We are now ready to prove Theorem 3.3.

Proof of Theorem 3.3.: It holds that:

\[f^{\prime\prime}(\psi^{\star})(\psi_{n-1}-\psi^{\star}) =f^{\prime}(\psi_{n-1})-f^{\prime}(\psi^{\star})+(f^{\prime\prime }(\psi^{\star})(\psi_{n-1}-\psi^{\star})-f^{\prime}(\psi_{n-1})+f^{\prime}( \psi^{\star}))\] \[=h_{n}(\psi_{n-1})-f^{\prime}(\psi^{\star})+(f^{\prime\prime}( \psi^{\star})(\psi_{n-1}-\psi^{\star})-f^{\prime}(\psi_{n-1})+f^{\prime}(\psi^{ \star}))\] \[\quad+(f^{\prime}(\psi_{n-1})-\bar{h}(\psi_{n-1}))+(\bar{h}(\psi_{n -1})-h_{n}(\psi_{n-1})).\]Applying on both sides: (a) a summation over \(i\in[n]\), (b) a multiplication by \(f^{\prime\prime}(\psi^{\star})^{-1}\), (c) \(\sqrt{\mathbb{E}[\|\cdot\|^{2}]}\), and using Minkowski's inequality on the r.h.s, we obtain

\[\sqrt{\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}\psi_{i}- \psi^{\star}\right\|^{2}\right]}\leq\underbrace{\sqrt{\mathbb{E}\left[\left\| \frac{1}{n}\sum_{i=1}^{n}f^{\prime\prime}(\psi^{\star})^{-1}h_{i}(\psi_{i-1}) \right\|^{2}\right]}}_{(i)}\] \[\qquad+\underbrace{\sqrt{\mathbb{E}\left[\left\|\frac{1}{n}\sum _{i=1}^{n}f^{\prime\prime}(\psi^{\star})^{-1}(f^{\prime\prime}(\psi^{\star})( \psi_{i-1}-\psi^{\star})-f^{\prime}(\psi_{i-1})\right\|^{2}\right]}}_{(ii)}\] \[\qquad+\underbrace{\sqrt{\mathbb{E}\left[\left\|\frac{1}{n}\sum _{i=1}^{n}f^{\prime\prime}(\psi^{\star})^{-1}(f^{\prime}(\psi_{i-1})-\bar{h}( \psi_{i-1}))\right\|^{2}\right]}}_{(iii)}\] \[\qquad+\underbrace{\sqrt{\mathbb{E}\left[\left\|\frac{1}{n}\sum _{i=1}^{n}f^{\prime\prime}(\psi^{\star})^{-1}(\bar{h}(\psi_{i-1})-h_{i}(\psi_ {i-1}))\right\|^{2}\right]}}_{(iv)}.\]

\((i)\) and \((ii)\) have direct analogues in the proofs of prior work [22] on the convergence of (unbiased) SGD with Polyak-Ruppert averaging, and will be bounded similarly. \((iii)\) captures the bias of the CD algorithm, while \((iv)\) captures the variance.

Bounding \((i)\)Using Lemma D.7, we have \((i)=\mathcal{O}(n^{\frac{\beta}{2}-1})\).

Bounding \((ii)\)Since \(\log Z(\psi)\) is analytic, there exists some constant \(M^{\prime}\) such that

\[\|f^{\prime\prime}(\psi^{\star})(\psi_{i-1}-\psi^{\star})-f^{\prime}(\psi_{i-1 })\|\leq M^{\prime}\|\psi_{i-1}-\psi^{\star}\|^{2}.\]

Thus, we have:

\[(ii)\leq\frac{M^{\prime}}{n}\sqrt{\mathbb{E}\left[\left(\sum_{i=1}^{n}\|\psi_ {i}-\psi^{\star}\|^{2}\right)^{2}\right]}\leq\frac{M^{\prime}}{n}\sum_{i=1}^{ n}\sqrt{\mathbb{E}\left[\|\psi_{i}-\psi^{\star}\|^{4}\right]}=\mathcal{O}(n^{- \beta})\]

where the second-to-last inequality used Minkowski's inequality, and the last applied Lemma D.8.

Bounding \((iii)\)By Minkowski's inequality, we have:

\[(iii)\ \leq\ \frac{1}{n}\sum_{i=1}^{n}\sqrt{\mathbb{E}\left[\left\|f^{\prime}( \psi_{n-1})-\bar{h}(\psi_{n-1})\right\|^{2}\right]}\]

Moreover, using lemma C.7, we have:

\[\left\|f^{\prime}(\psi_{n-1})-\bar{h}(\psi_{n-1})\right\| =\left\|\mathbb{E}_{p_{\psi_{n-1}}}\phi-\mathbb{E}_{p_{\psi_{n- 1}}}\mathbb{E}_{k^{m}_{\psi_{n-1}}}\phi\right\|\] \[\leq\alpha^{m}\sqrt{\mathbb{E}_{p_{\psi_{n-1}}}\left\|\phi- \mathbb{E}_{p_{\psi_{n-1}}}\phi\right\|^{2}}C_{\chi}\left\|\psi_{n-1}-\psi^{ \star}\right\|\] \[\leq\alpha^{m}\sigma C_{\chi}\left\|\psi_{n-1}-\psi^{\star}\right\|.\]

We thus obtain

\[(iii)\ \leq\ \frac{\alpha^{m}C_{\chi}}{n}\sum_{i=1}^{n}(\mathbb{E}\left\|\psi_{i- 1}-\psi^{\star}\right\|^{2})^{1/2}\ =\ \frac{\alpha^{m}C_{\chi}}{n}\sum_{i=1}^{n}\delta_{i-1}^{1/2}.\]

Recalling that \(\delta_{i}\) satisfies Theorem 3.2, we have that \(\delta_{d}^{\frac{1}{2}}=\mathcal{O}(n^{-\beta/2})\), and we thus have \(\sum_{i=1}^{n}\delta_{i}^{1/2}=\mathcal{O}(n^{1-\frac{\beta}{2}})\). By squaring the result of Lemma D.6, we have \(\sum_{i=1}^{n}\delta_{i}^{1/2}=\mathcal{O}(n^{-1-\frac{\beta}{2}})\), and thus, we obtain that \((iii)=\mathcal{O}(\alpha^{m}n^{-\beta/2})=\mathcal{O}(n^{-(\frac{\beta}{2}+m \frac{\log\alpha}{\log\alpha})})\).

Bounding \((iv)\)We have

\[(iv)\] \[\stackrel{{(a)}}{{\leq}}2\sqrt{\frac{\operatorname{tr} \left(\mathcal{I}(\psi^{\star})^{-1}\right)}{n}}+\frac{\|\mathcal{I}(\psi^{ \star})^{-2}\|_{\mathbb{F}}^{1/2}}{n}\left((M+\alpha^{m}C_{\chi}(\bar{\tau}+2\| \log Z\|_{4,\infty}\sigma))^{1/2}\sqrt{\sum_{i=1}^{n}\delta_{i-1}^{1/2}}\right.\] \[\left.+\alpha^{m}C_{\chi}\sigma\sqrt{\sum_{i=1}^{n}\delta_{i-1}}\right)\] \[\stackrel{{(b)}}{{\leq}}2\sqrt{\frac{\operatorname{ tr}\left(\mathcal{I}(\psi^{\star})^{-1}\right)}{n}}+\mathcal{O}(n^{-\frac{1}{2}- \frac{\partial}{4}}).\]

Where in \((a)\), we used Lemma D.10 and in \((b)\), we used Lemma D.6 and D.5.

Final boundPutting everything together, we have that:

\[\sqrt{\mathbb{E}\left\|\overline{\psi}_{n}-\psi^{\star}\right\|^{2}}\leq 2 \sqrt{\frac{\operatorname{tr}(\mathcal{I}(\psi)^{-1})}{n}}+\mathcal{O}(n^{ \max\left(-\left(\frac{1}{2}+\frac{\partial}{4}\right),-\beta,\frac{\partial} {2}-1,-\left(\frac{\partial}{2}+m\frac{\|\log\alpha\|}{\log n}\right)\right)}).\]

If, furthermore, \(m>\frac{(1-\beta)\log n}{2|\log\alpha|}\), we have

\[\max\left(-\left(\frac{1}{2}+\frac{\beta}{4}\right),-\beta,\frac{\beta}{2}-1,-\left(\frac{\beta}{2}+m\frac{|\log\alpha|}{\log n}\right)\right)<-\frac{1}{ 2},\]

which concludes the proof. 

## Appendix E \(L_{2}\) approximation by auxiliary gradient updates

In this section, we consider different gradient update schemes starting from some random initialization \(\theta_{\mathrm{init}}\), and control the \(L_{2}\) distance between the different updates and the deterministic target \(\psi^{\star}\in\Psi\).

Notation.Recall the notation that \(X_{1},\ldots,X_{n}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}p_{\psi^{ \star}}\), \(B=n/N\) and for \(\psi\in\Psi\), \(K_{\psi}(x)\sim k_{\psi}^{m}(x,\ ^{\star})\). We also write, for \(m\in\mathbb{N}\cup\{\infty\}\),

\[K_{1;\psi}^{m}(x)\,,\,\ldots\,,\,K_{n;\psi}^{m}(x)\,\stackrel{{ \mathrm{i.i.d.}}}{{\sim}}k_{\psi}^{m}(x,\ ^{\star})\]

Let \(\theta^{\mathrm{init}}\) be some \(\Psi\)-valued random initialization that is possibly correlated with \(X_{1},\ldots,X_{n}\). We capture the effect of correlation through the following quantities: For \(\epsilon>0\) and \(\nu>2\), let

\[\vartheta_{n,m}^{\mathrm{init}}(\epsilon) \,\coloneqq\,\mathbb{P}\left(\frac{\left\|\sum_{i=1}^{n}\mathbb{ E}\left[\phi\left(K_{\theta^{\mathrm{init}}}^{m}(X_{i})\right)\left|\,X_{i}, \theta^{\mathrm{init}}\right]-\mathbb{E}\left[\phi\left(K_{\theta^{\mathrm{ init}}}^{m}(X_{i}^{\prime})\right)\left|\,\theta^{\mathrm{init}}\right] \right.}{n}>\epsilon\right)\,,\] \[\text{and}\quad\varepsilon_{n,m;\nu}^{\mathrm{init}}(\epsilon) \,\coloneqq\,\sqrt{\epsilon^{2}+\kappa_{\nu;m}^{2}\left(\vartheta_{n,m}^{ \mathrm{init}}(\epsilon)\right)^{\frac{\epsilon}{\nu^{2}-2}}}\.\]

We also consider the i.i.d. samples, drawn independently of \(X_{1},\ldots,X_{n}\) and on a given \(\psi\in\Psi\), as

\[X_{1}^{\psi},\ldots,X_{n}^{\psi}\ \stackrel{{\mathrm{i.i.d.}}}{{\sim}}\ p_{\psi}\.\]

For notational clarity, we shall use \(\theta_{m,B}\) to denote parameters arising from an one-step update, where the subscripts \(m,B\) represent performing the one-step update with length-\(m\) Markov chains and with batch size \(B\). This is to be distinguished from \(\psi_{t}\) elsewhere in the text, which denotes the parameter from the actual multi-step CD algorithm and the subscript \(t\) denotes the \(t\)-th CD iterate.

Gradient update schemes.We consider five different updates. Let \(X_{1}^{\prime}\) be an i.i.d. copy of \(X_{1}\) drawn independently of all other random variables. The SGD-with-replacement update is given by

\[\theta_{m,B}^{\mathrm{SGDw}}\ \coloneqq\ F_{m,B}^{\mathrm{SGDw}}(\theta^{\mathrm{init}})\,\quad\text{ where }F_{m,B}^{\mathrm{SGDw}}(\psi)\ \coloneqq\ \psi-\frac{\eta}{B}\sum_{i\in S^{w}}\left(\phi(X_{i})-\phi\left(K_{i;\psi}^{m}(X_ {i})\right)\right)\]and \(S^{w}\) is a uniformly drawn size-\(B\) subset of \([n]\). The SGD-without-replacement update, after renormalizing the learning rate, is given by the \(N\)-fold function composition

\[\theta^{\mathrm{SGDw}}_{m,B}\ \coloneqq\ F^{\mathrm{SGDw}}_{m,B;N}\circ\ldots\circ F^{\mathrm{SGDw}}_{m,B;1}( \theta^{\mathrm{init}})\,\] \[\text{where}\quad F^{\mathrm{SGDw}}_{m,B;j}(\psi)\ \coloneqq\ \psi-\frac{\eta}{ NB}\sum_{i\in S^{o}_{j}}\Big{(}\phi(X_{i})-\phi\big{(}K^{m}_{i;\psi}(X_{i}) \big{)}\Big{)}\ \ \text{for each}\ j\in[N]\,\]

and \(S^{o}_{j}\)'s are disjoint size-\(B\) random subsets of \([n]\), defined by \((S^{o}_{1},\ldots,S^{o}_{N})=\pi([n])\) for a uniformly drawn element \(\pi\) of the permutation group on \(n\) objects. The full-batch gradient update is given by

\[\theta^{\mathrm{GD}}_{m}\ \coloneqq\ F^{\mathrm{GD}}_{m}(\theta^{\mathrm{ init}})\,\quad\text{ where}\quad F^{\mathrm{GD}}_{m}(\psi)\ \coloneqq\ \psi-\frac{\eta}{n}\sum_{i\leq n}\Big{(}\phi(X_{i})-\phi\big{(}K^{m}_{i;\psi} (X_{i})\big{)}\Big{)}\.\]

The full-batch gradient update with an infinite-length Markov chain is given by

\[\theta^{\mathrm{GD}}_{\infty}\ \coloneqq\ F^{\mathrm{GD}}_{\infty}(\theta^{ \mathrm{init}})\,\qquad\text{ where}\quad\quad F^{\mathrm{GD}}_{\infty}(\psi)\ \coloneqq\ \psi-\frac{\eta}{n}\sum_{i\leq n}\Big{(}\phi(X_{i})-\phi(X^{\psi}_{1})\Big{)}\.\]

The population gradient update with an infinite-length Markov chain is given by

\[\theta^{\mathrm{pop}}\ \coloneqq\ f^{\mathrm{pop}}(\theta^{\mathrm{init}})\, \qquad\text{ where}\quad f^{\mathrm{pop}}(\psi)\ \coloneqq\ \psi-\eta\,\mathbb{E}\big{[}\phi(X_{1})-\phi(X^{\psi}_{1})\big{]}\,\]

where we use the lowercase \(f\) to emphasize that \(f^{\mathrm{pop}}\) is a deterministic function.

The forthcoming results are summarized below:

\[\theta^{\mathrm{SGDw}}_{m,B}\ \stackrel{{\mathrm{Lemma\ E.1}}}{{ \approx}}\ \theta^{\mathrm{GD}}_{m}\ \stackrel{{\mathrm{Lemma\ E.2}}}{{ \approx}}\ \theta^{\mathrm{GD}}_{\infty}\ \stackrel{{\mathrm{Lemma\ E.3}}}{{ \approx}}\ \theta^{\mathrm{pop}}\ \stackrel{{\mathrm{Lemma\ E.4}}}{{ \approx}}\ \psi^{*}\.\]

**Lemma E.1**.: _Let \(\mathcal{F}_{n}\) be the sigma algebra generated by \(\{X_{i},K^{m}_{i;\mathrm{init}}(X_{i})\,|\,1\leq i\leq n\}\). Then_

\[\mathbb{E}\big{[}\theta^{\mathrm{SGDw}}_{m,B}-\theta^{\mathrm{ GD}}_{m}\,\big{|}\,\theta^{\mathrm{init}},\mathcal{F}_{n}\big{]}\ =0\qquad\text{ almost surely}\.\]

_Moreover, under A1 and A7, we have_

\[\mathbb{E}\big{\|}\theta^{\mathrm{SGDw}}_{m,B}-\theta^{\mathrm{GD}}_{m}\big{\|} ^{2}\ \leq\frac{4\eta^{2}(\sigma^{2}+\kappa^{2}_{\nu;m})}{B}\,\mathbb{I}_{\{B<n\}}\.\]

Proof of Lemma e.1.: Write \(A\coloneqq(A_{1},\ldots,A_{n})\), where

\[A_{i}\ \coloneqq\ \big{(}\phi(X_{i})-\phi\big{(}K^{m}_{i;\mathrm{init}}(X_{i })\big{)}\big{)}-\mathbb{E}\big{[}\phi(X_{1})-\phi\big{(}K^{m}_{i;\mathrm{ init}}(X_{1})\big{)}\big{]}\.\]

Since \(S^{w}\) is uniformly drawn from all size-\(B\) subsets of \([n]\) and independently of all other variables, we have that almost surely

\[\mathbb{E}\big{[}\theta^{\mathrm{SGDw}}_{m,B}-\theta^{\mathrm{ GD}}_{m}\,\big{|}\,\theta^{\mathrm{init}},\mathcal{F}_{n}\big{]}\ =\ \mathbb{E}\Big{[}\frac{\eta}{B}\sum_{i\in S^{w}}A_{i}-\frac{\eta}{n}\sum_{i\leq n }A_{i}\,\Big{|}\,A\Big{]}\ =\ 0\.\]

To prove the remaining bound, we note that the above relation implies \(\theta^{\mathrm{SGDw}}_{m,B}-\theta^{\mathrm{GD}}_{m}\) is zero-mean. By the law of total variance, we have

\[\mathbb{E}\big{\|}\theta^{\mathrm{SGDw}}_{m,B}-\theta^{\mathrm{ GD}}_{m}\big{\|}^{2}\ =\ \text{Tr}\,\text{Cov}\big{[}\theta^{\mathrm{SGDw}}_{m,B}-\theta^{\mathrm{ GD}}_{m}\big{]}\ =\ \text{Tr}\,\mathbb{E}\,\text{Cov}\big{[}\theta^{\mathrm{SGDw}}_{m,B}-\theta^{ \mathrm{GD}}_{m}\,\big{|}\,\theta^{\mathrm{init}},\mathcal{F}_{n}\big{]}\] \[\ =\ \eta^{2}\,\text{Tr}\,\mathbb{E}\bigg{[}\,\mathbb{E}\Big{[}\Big{(} \frac{1}{B}\sum_{i\in S^{w}}A_{i}\Big{)}\Big{(}\frac{1}{B}\sum_{i\in S^{w}}A_{ i}\Big{)}^{\top}\,\Big{|}\,A\Big{]}-\Big{(}\frac{1}{n}\sum_{i\leq n}A_{i} \Big{)}\Big{(}\frac{1}{n}\sum_{i\leq n}A_{i}\Big{)}^{\top}\,\bigg{]}\.\]

To compute the covariance, recall that \(S^{w}\) is a uniformly drawn size-\(B\) subset of \([n]\) with \(B=n/N\). Let \(\mathcal{P}_{N}([n])\) be the collection of all partitions of \([n]\) into \(N\) size-\(B\) subsets. We can generate \(S^{w}\) by the following two-step process:

1. Uniformly draw a partition \(P^{\prime}=(P^{\prime}_{1},\ldots,P^{\prime}_{N})\) from \(\mathcal{P}_{N}([n])\);
2. Uniformly sample an index \(K\) from \([N]\) and set \(S^{w}=P^{\prime}_{K}\).

Then we have, almost surely

\[\mathbb{E}\Big{[}\Big{(}\frac{1}{B}\sum_{i\in S^{w}}A_{i}\Big{)} \Big{(}\frac{1}{B}\sum_{i\in S^{w}}A_{i}\Big{)}^{\top}\Big{|}\,A\Big{]}-\Big{(} \frac{1}{n}\sum_{i\leq n}A_{i}\Big{)}\Big{(}\frac{1}{n}\sum_{i\leq n}A_{i} \Big{)}^{\top}\] \[=\ \frac{1}{|\mathcal{P}_{N}([n])|}\sum_{P^{\prime}\in\mathcal{P}_{N }([n])}\Big{(}\frac{1}{N}\sum_{k\leq N}\frac{1}{B^{2}}\sum_{i,j\in P_{k}^{ \prime}}A_{i}A_{j}^{\top}-\frac{1}{n^{2}}\sum_{i,j\leq n}A_{i}A_{j}^{\top}\, \Big{)}\] \[=\ \frac{1}{|\mathcal{P}_{N}([n])|}\sum_{P^{\prime}\in\mathcal{P}_{N }([n])}\Big{(}\frac{1}{NB^{2}}\sum_{k\leq N}\sum_{i,j\in P_{k}^{\prime}}A_{i}A _{j}^{\top}-\frac{1}{N^{2}B^{2}}\sum_{k,l\leq N}\sum_{i\in P_{k}^{\prime},j\in P _{l}^{\prime}}A_{i}A_{j}^{\top}\,\Big{)}\] \[=\ \frac{1}{|\mathcal{P}_{N}([n])|}\sum_{P^{\prime}\in\mathcal{P}_{N }([n])}\Big{(}\frac{N-1}{N^{2}B^{2}}\sum_{k\leq N}\sum_{i,j\in P_{k}^{\prime}} A_{i}A_{j}^{\top}-\frac{1}{N^{2}B^{2}}\sum_{k\neq l}\sum_{i\in P_{k}^{\prime},j\in P _{l}^{\prime}}A_{i}A_{j}^{\top}\,\Big{)}\,.\]

By noting that \(A_{i}\)'s are exchangeable, we obtain

\[\mathbb{E}\big{\|}\theta_{m,B}^{\mathrm{SGDw}}- \theta_{m}^{\mathrm{GD}}\big{\|}^{2}\ =\ \eta^{2}\text{Tr}\,\mathbb{E}\bigg{[}\, \tfrac{N-1}{NB}A_{1}A_{1}^{\top}+\tfrac{(N-1)(B-1)}{NB}A_{1}A_{2}^{\top}-\tfrac {N-1}{N}A_{1}A_{2}^{\top}\,\bigg{]}\] \[=\eta^{2}\text{Tr}\,\mathbb{E}\bigg{[}\,\tfrac{N-1}{NB}A_{1}A_{1} ^{\top}-\tfrac{N-1}{NB}A_{1}A_{2}^{\top}\,\bigg{]}\] \[=\tfrac{\eta^{2}(N-1)}{NB}\big{(}\mathbb{E}\|A_{1}\|^{2}-\mathbb{ E}\langle A_{1},A_{2}\rangle\big{)}\] \[\overset{(a)}{\leq}\frac{2\eta^{2}}{B}\mathbb{E}\|A_{1}\|^{2}\] \[=\tfrac{2\eta^{2}}{B}\mathbb{E}\Big{\|}\big{(}\phi(X_{1})-\mathbb{ E}[\phi(X_{1})]\big{)}-\Big{(}\phi\big{(}K_{i;\theta^{\mathrm{init}}}^{m}(X_{1}) \big{)}-\mathbb{E}\Big{[}\phi\big{(}K_{i;\theta^{\mathrm{init}}}^{m}(X_{1}) \big{)}\Big{]}\Big{)}\Big{\|}^{2}\] \[\leq\frac{4\eta^{2}}{B}\Big{(}\text{Tr}\,\text{Cov}[\phi(X_{1})]+ \text{Tr}\,\text{Cov}\Big{[}\phi\big{(}K_{i;\theta^{\mathrm{init}}}^{m}(X_{1}) \big{)}\Big{]}\Big{)}\] \[\overset{(b)}{\leq}\frac{4\eta^{2}(\sigma^{2}+\kappa_{\nu;m}^{2}) }{B}\.\]

In \((a)\), we have used a Cauchy-Schwarz inequality; in \((b)\), we have used A1 and A7. Finally we note that if \(B=n\), \(\theta_{m,B}^{\mathrm{SGDw}}=\theta_{m}^{\mathrm{GD}}\) almost surely, which implies the desired bound. 

**Lemma E.2**.: _Denote \(\mathcal{A}_{n}\) as the sigma algebra generated by \(\theta^{\mathrm{init}},X_{1},\ldots,X_{n}\). Under A1, A2, A3 and A7, we have that for any \(\epsilon>0\) and \(\nu>2\),_

\[\mathbb{E}\,\big{\|}\mathbb{E}\big{[}\theta_{m}^{\mathrm{GD}}- \theta_{\infty}^{\mathrm{GD}}\,\big{|}\,\mathcal{A}_{n}\big{]}\big{\|}^{2}\ \leq\ \eta^{2}\Big{(}\alpha^{m}\sigma C_{\chi}\,\sqrt{\mathbb{E}\|\theta^{\mathrm{ init}}-\psi^{*}\|^{2}}\,+\,\varepsilon_{n,m;\nu}^{\mathrm{init}}(\epsilon) \Big{)}^{2}\,\] \[\mathbb{E}\,\|\theta_{m}^{\mathrm{GD}}-\theta_{\infty}^{\mathrm{ GD}}\|^{2}\ \leq\ \eta^{2}\Big{(}\Big{(}\alpha^{m}\sigma C_{\chi}\,\sqrt{\mathbb{E}\|\theta^{ \mathrm{init}}-\psi^{*}\|^{2}}\,+\,\varepsilon_{n,m;\nu}^{\mathrm{init}}( \epsilon)\Big{)}^{2}+\tfrac{\kappa_{\nu;m}^{2}+\sigma^{2}}{n}\Big{)}\.\]

Proof of Lemma e.2.: The main challenge arises from the possible correlation between \(\theta^{\mathrm{init}}\) and \(X_{1},\ldots,X_{n}\). First note that for any \(\epsilon>0\), \(\nu>2\) and a real-valued random variable \(Y\), by Holder's inequality, we have

\[\mathbb{E}[Y^{2}] =\mathbb{E}\big{[}Y^{2}\,\mathbb{I}_{\{Y\leq\epsilon\}}+Y^{2}\, \mathbb{I}_{\{Y>\epsilon\}}\big{]}\] \[\leq\epsilon^{2}+\mathbb{E}[Y^{2}\mathbb{I}_{\{Y>\epsilon\}}]\ \leq\ \epsilon^{2}+(\mathbb{E}[Y^{\nu}])^{2/\nu}\mathbb{P}(Y> \epsilon)^{(\nu-2)/\nu}\.\] (15)

Also note the useful inequality that for two real-valued random vectors (possibly correlated) \(V_{1},V_{2}\), we have

\[\mathbb{E}[\|V_{1}+V_{2}\|^{2}]\ \leq\ \mathbb{E}[(\|V_{1}\|+\|V_{2}\|)^{2}] \leq\mathbb{E}\|V_{1}\|^{2}+2\sqrt{(\mathbb{E}\|V_{1}\|^{2})( \mathbb{E}\|V_{2}\|^{2})}+\mathbb{E}\|V_{2}\|^{2}\] \[=\big{(}\sqrt{\mathbb{E}\|V_{1}\|^{2}}+\sqrt{\mathbb{E}\|V_{2}\|^{2} }\big{)}^{2}\.\] (16)

Now to control the first quantity of interest, by using a triangle inequality, we have

\[\mathbb{E}\,\big{\|}\mathbb{E}\big{[}\theta_{m}^{\mathrm{GD}}- \theta_{\infty}^{\mathrm{GD}}\,\big{|}\,\mathcal{A}_{n}\big{]}\big{\|}^{2}\ =\mathbb{E}\Big{\|}\,\mathbb{E}\Big{[}\frac{\eta}{n}\sum_{i\leq n} \big{(}\phi\big{(}K_{i;\theta^{\mathrm{init}}}^{m}(X_{i})\big{)}-\phi\big{(}X_{i}^{ \theta^{\mathrm{init}}}\big{)}\big{)}\,\Big{|}\,\mathcal{A}_{n}\Big{]}\Big{\|}^{2}\] \[\overset{(\ref{eq:1})}{\leq}\eta^{2}\left(\sqrt{\mathbb{E}[\Delta_{ 1}^{2}]}+\sqrt{\mathbb{E}[\Delta_{2}^{2}]}\right)^{2}\,,\]where

\[\Delta_{1} \coloneqq\bigg{\|}\,\mathbb{E}\Big{[}\frac{1}{n}\sum_{i\leq n}\big{(} \phi\big{(}K^{m}_{i;\mathrm{init}}(X_{i})\big{)}-\mathbb{E}\big{[}\phi\big{(}K^{ m}_{i;\mathrm{init}}(X^{\prime}_{1})\big{)}\,\big{|}\,\theta^{\mathrm{init}}\big{]} \big{)}\,\bigg{|}\,\mathcal{A}_{n}\Big{]}\bigg{\|}\] \[=\bigg{\|}\,\frac{1}{n}\sum_{i\leq n}\big{(}\mathbb{E}\big{[}\phi \big{(}K^{m}_{i;\mathrm{init}}(X_{i})\big{)}\,\big{|}\,\theta^{\mathrm{init}},X _{i}\big{]}-\mathbb{E}\big{[}\phi\big{(}K^{m}_{i;\mathrm{init}}(X^{\prime}_{1}) \big{)}\,\big{|}\,\theta^{\mathrm{init}}\big{]}\big{)}\bigg{\|}\,,\] \[\Delta_{2} \coloneqq\bigg{\|}\,\mathbb{E}\Big{[}\frac{1}{n}\sum_{i\leq n} \big{(}\mathbb{E}\big{[}\phi\big{(}K^{m}_{i;\mathrm{init}}(X^{\prime}_{1}) \big{)}\,\big{|}\,\theta^{\mathrm{init}}\big{]}-\phi\big{(}K^{\theta^{ \mathrm{init}}}_{i}\big{)}\big{)}\,\bigg{|}\,\mathcal{A}_{n}\Big{]}\bigg{\|}\] \[=\bigg{\|}\,\mathbb{E}\big{[}\phi\big{(}K^{m}_{1;\mathrm{init}}(X ^{\prime}_{1})\big{)}-\phi\big{(}X^{\theta^{\mathrm{init}}}_{1}\big{)}\, \big{|}\,\theta^{\mathrm{init}}\big{]}\bigg{\|}\,,\]

and \(X^{\prime}_{1}\) is an i.i.d. copy of \(X_{1}\) and in particular independent of \(\theta^{\mathrm{init}}\). \(\Delta_{1}\) is controlled via (15):

\[\mathbb{E}[\Delta_{1}^{2}] \leq\epsilon^{2}+(\mathbb{E}[\Delta_{1}^{\nu}])^{2/\nu}\,\mathbb{ P}(\Delta_{1}>\epsilon)^{\nu/(\nu-2)}\] \[\stackrel{{(a)}}{{\leq}}\epsilon^{2}+\Big{(}\mathbb{ E}\Big{\|}\phi(K^{m}_{1;\mathrm{init}}(X_{1}))-\mathbb{E}\Big{[}\phi(K^{m}_{1; \mathrm{init}}(X^{\prime}_{1}))\,\Big{|}\,\theta^{\mathrm{init}}\Big{]}\Big{\|} ^{\nu}\Big{)}^{2/\nu}\] \[\qquad\times\mathbb{P}\bigg{(}\,\frac{\|\,\Sigma_{i\leq n}\,( \mathbb{E}[\phi(K^{m}_{i,\mathrm{init}}(X_{i}))\,|\,\theta^{\mathrm{init}},X_{i }]-\mathbb{E}[\phi\big{(}K^{m}_{i;\mathrm{init}}(X^{\prime}_{1})\,|\,\theta^{ \mathrm{init}}\big{]})\|}{n}>\epsilon\bigg{)}^{\frac{\nu-2}{\nu}}\] \[\stackrel{{(b)}}{{\leq}}\epsilon^{2}+\kappa^{2}_{\nu;m }\big{(}\vartheta^{\mathrm{init}}_{n,m}(\epsilon)\big{)}^{\frac{\nu-2}{\nu}}\] \[=\big{(}\varepsilon^{\mathrm{init}}_{n,m;\nu}(\epsilon)\big{)}^{ 2}\,.\] (17)

In \((a)\), we have plugged in the definition of \(\Delta_{1}\) and applied a Jensen's inequality with respect to the empirical average; in \((b)\), we have used A7 to bound the \(\nu\)-th moment term as

\[\Big{(}\mathbb{E}\Big{\|}\phi(K^{m}_{1;\mathrm{init}}(X_{1}))- \mathbb{E}\Big{[}\phi(K^{m}_{1;\mathrm{init}}(X^{\prime}_{1}))\,\Big{|}\, \theta^{\mathrm{init}}\Big{]}\Big{\|}^{\nu}\Big{)}^{1/\nu}\] \[=\Big{(}\mathbb{E}\,\Big{[}\mathbb{E}\Big{[}\Big{\|}\phi(K^{m}_{ 1;\mathrm{init}}(X_{1}))-\mathbb{E}\Big{[}\phi(K^{m}_{1;\mathrm{init}}(X^{ \prime}_{1}))\,\Big{|}\,\theta^{\mathrm{init}}\Big{]}\Big{]}\Big{|}^{\nu} \Big{|}\,\theta^{\mathrm{init}}\Big{]}\,\Big{]}\Big{)}^{1/\nu}\] \[\leq\,\sup_{\psi\in\Psi}\Big{(}\mathbb{E}\Big{[}\Big{\|}\phi(K^{ m}_{1;\mathrm{i}}(X_{1}))-\mathbb{E}\big{[}\phi(K^{m}_{1;\mathrm{i}}(X^{ \prime}_{1}))\,\Big{)}\,\Big{\|}^{\nu}\,\Big{]}\Big{)}^{1/\nu}\] \[=\,\sup_{\psi\in\Psi}\Big{(}\mathbb{E}\Big{[}\Big{\|}\phi(K^{m}_{ 1;\mathrm{i}}(X_{1}))-\mathbb{E}\big{[}\phi(K^{m}_{1;\mathrm{i}}(X_{1}))\, \big{]}\Big{\|}^{\nu}\,\Big{]}\Big{)}^{1/\nu}\ \leq\ \kappa_{\nu;m}\]

and recalled the definitions of \(\vartheta^{\mathrm{init}}_{n,m}\) and \(\varepsilon^{\mathrm{init}}_{n,m;\nu}\). On the other hand,

\[\mathbb{E}[\Delta_{2}^{2}]\ =\ \mathbb{E}\Big{\|}\,\int_{\mathbb{R}^{d}} \phi(x)(K^{m}_{1;\mathrm{init}}p_{\psi^{*}})(x)dx-\mathbb{E}\big{[}\phi(X^{ \theta^{\mathrm{init}}}_{1})\big{|}\theta^{\mathrm{init}}\big{]}\Big{\|}^{2}\] \[\stackrel{{(a)}}{{=}}\mathbb{E}\Big{\|}\,\int_{ \mathbb{R}^{d}}(K^{m}_{1;\mathrm{init}}\phi)(x)\,p_{\psi^{*}}(x)dx-\mathbb{E}_ {p_{\mathrm{init}}}[\,\phi\,]\Big{\|}^{2}\] \[\stackrel{{(b)}}{{=}}\mathbb{E}\Big{\|}\int_{ \mathbb{R}^{d}}\big{(}K^{m}_{1;\mathrm{init}}\big{(}\phi-\mathbb{E}_{p_{ \mathrm{init}}}[\,\phi\,]\big{)}\big{)}(x)\times p_{\psi^{*}}(x)dx\Big{\|}^{2}\] \[\stackrel{{(c)}}{{=}}\mathbb{E}\Big{\|}\int_{ \mathbb{R}^{d}}\big{(}K^{m}_{1;\mathrm{init}}\big{(}\phi-\mathbb{E}_{p_{ \mathrm{init}}}[\,\phi\,]\big{)}\big{)}(x)\times(p_{\psi^{*}}(x)-p_{\mathrm{ init}}(x))dx\Big{\|}^{2}\] \[\stackrel{{(d)}}{{=}}\sum_{l=1}^{d}\mathbb{E}\Big{(} \int_{\mathbb{R}^{d}}\big{(}K^{m}_{1;\mathrm{init}}\big{(}\phi-\mathbb{E}_{p_{ \mathrm{init}}}[\,\phi\,]\big{)}\big{)}(x)^{\top}e_{l}\times p_{\mathrm{init}}(x) \times\frac{p_{\psi^{*}}(x)-p_{\mathrm{init}}(x)}{p_{\mathrm{init}}(x)}\,dx\Big{)}^ {2}\] \[\stackrel{{(e)}}{{\leq}}\sum_{l=1}^{d}\mathbb{E}\Big{[} \Big{(}\int_{\mathbb{R}^{d}}\big{(}\big{(}K^{m}_{t1;\mathrm{init}}\big{(}\phi- \mathbb{E}_{p_{\mathrm{init}}}[\,\phi\,]\big{)}\big{)}(x)^{\top}e_{l} \big{)}^{2}p_{\mathrm{init}}(x)dx\] \[\qquad\qquad\qquad\times\int_{\mathbb{R}^{d}}\Big{(}\frac{p_{ \psi^{*}}(x)-p_{\mathrm{init}}(x)}{p_{\mathrm{init}}(x)}\Big{)}^{2}p_{\mathrm{ init}}(x)dx\Big{)}^{2}\Big{]}\] \[\stackrel{{(f)}}{{\leq}}\sum_{l=1}^{d}\mathbb{E} \Big{(}\alpha^{2m}\,\text{Tr}\,\text{Cov}\big{[}\phi(X^{\theta^{\mathrm{init}}}_{1} )\,\big{|}\,\theta^{\mathrm{init}}\big{]}\,\chi^{2}(p_{\psi^{*}},p_{\theta^{ \mathrm{init}}})\Big{)}^{2}\] \[\stackrel{{(g)}}{{\leq}}\alpha^{2m}\sigma^{2}C^{2}_{ \chi}\,\mathbb{E}\|\theta^{\mathrm{init}}-\psi^{*}\|^{2}\.\]

In \((a)\), we have used that \((Kf)(x)=\int K(x,y)f(y)dy\); in \((b)\), we have used that the Markov operator leaves the constant function invariant; in \((c)\), we used that \(K_{1;\mathrm{init}}\) leaves \(p_{\theta^{\mathrm{init}}}\) invariant; in \((d)\), we denoted \((e_{l})_{l\leq d}\) as the standard basis vectors of \(\mathbb{R}^{d}\) and multiplied and divided by \(p_{\theta^{\mathrm{init}}}(x)\); in \((e)\), we have used a Cauchy-Schwarz inequality; in \((f)\), we have used the definition of the spectral gap \(\alpha\) in A3; in \((g)\), we have used A1 and A2. Combining the bounds gives the first inequality that

\[\mathbb{E}\left\|\mathbb{E}\!\left[\theta_{m}^{\mathrm{GD}}\!-\! \theta_{\infty}^{\mathrm{GD}}\;\big{|}\,\mathcal{A}_{n}\right]\right\|^{2}\; \leq\;\eta^{2}\Big{(}\alpha^{m}\sigma C_{\chi}\,\sqrt{\mathbb{E}\|\theta^{ \mathrm{init}}-\psi^{*}\|^{2}}\,+\,\varepsilon_{n,m;\nu}^{\mathrm{init}}( \epsilon)\Big{)}^{2}\;.\]

Now we handle the second quantity by conditioning on \(\theta^{\mathrm{init}}\) and perform a bias-variance decomposition:

\[\mathbb{E}\!\left\|\theta_{m}^{\mathrm{GD}}-\theta_{\infty}^{ \mathrm{GD}}\right\|^{2} =\eta^{2}\,\mathbb{E}\!\left[\,\mathbb{E}\!\left[\left\|\frac{1} {n}\sum\nolimits_{i\leq n}\big{(}\phi\big{(}K_{i;\theta^{\mathrm{init}}}^{m}(X _{i})\big{)}-\phi(X_{i}^{\theta^{\mathrm{init}}})\big{)}\right\|^{2}\,\Big{|} \,\mathcal{A}_{n}\,\right]\,\right]\] \[=\eta^{2}(Q_{B}+Q_{V})\;,\]

where

\[Q_{B} \coloneqq\mathbb{E}\!\left\|\mathbb{E}\!\left[\frac{1}{n}\sum \nolimits_{i\leq n}\big{(}\phi\big{(}K_{i;\theta^{\mathrm{init}}}^{m}(X_{i}) \big{)}-\phi(X_{i}^{\theta^{\mathrm{init}}})\,\Big{|}\,\mathcal{A}_{n}\big{]} \right\|^{2}\;,\] \[Q_{V} \coloneqq\mathbb{E}\!\left[\mathrm{Tr}\!\left(\mathrm{Cov}\! \left[\frac{1}{n}\sum\nolimits_{i\leq n}\phi\big{(}K_{i;\theta^{\mathrm{init}}}^ {m}(X_{i})\big{)}\Big{|}\mathcal{A}_{n}\right]+\mathrm{Cov}\!\left[\frac{1}{n }\sum\nolimits_{i\leq n}\phi(X_{i}^{\theta^{\mathrm{init}}})\Big{|}\theta^{ \mathrm{init}}\right]\right)\right]\;.\]

Note that the covariance terms separate because \(X_{i}^{\theta^{\mathrm{init}}}\) is independent of \(K_{i;\theta^{\mathrm{init}}}^{m}(X_{i})\) conditioning on \(\theta^{\mathrm{init}}\). \(\eta^{2}Q_{B}\) is exactly the quantity controlled above, so it suffices to bound the variance term \(Q_{V}\). By explicitly computing the second covariance term while noting that \(X_{1}^{\theta^{\mathrm{init}}},\ldots,X_{n}^{\theta^{\mathrm{init}}}\) are conditionally i.i.d. given \(\theta^{\mathrm{init}}\) and \(K_{i;\theta^{\mathrm{init}}}^{m}(X_{i})\)'s are conditionally independent across \(1\leq i\leq n\) given \(\mathcal{A}_{n}\), we have

\[Q_{V} =\frac{\sum_{i\leq n}\mathbb{E}\!\left[\mathrm{Tr}\,\mathrm{Cov} \!\left[\phi(K_{i;\theta^{\mathrm{init}}}^{m}(X_{i}))\,|\,\theta^{\mathrm{init }},X_{i}\right]\right]}{n^{2}}+\frac{\mathbb{E}\!\left[\mathrm{Tr}\,\mathrm{Cov} \!\left[\phi(X_{1}^{\theta^{\mathrm{init}}})|\theta^{\mathrm{init}}\right] \right]}{n}\] \[\stackrel{{(a)}}{{=}}\frac{\mathbb{E}\!\left[\mathrm{ Tr}\,\mathrm{Cov}\!\left[\phi(K_{1;\theta^{\mathrm{init}}}^{m}(X_{1}))\,|\,\theta^{ \mathrm{init}},X_{1}\right]\right]}{n}+\frac{\mathbb{E}\!\left[\mathrm{Tr}\, \mathrm{Cov}\!\left[\phi(X_{1}^{\theta^{\mathrm{init}}})|\theta^{\mathrm{init} }\right]\right]}{n}\] \[\leq\frac{\kappa_{\nu;m}^{2}+\sigma^{2}}{n}\;,\]

where we have used A4, A7 and A1 in the last line. Combining the bounds, we obtain that

\[\mathbb{E}\left\|\theta_{m}^{\mathrm{GD}}-\theta_{\infty}^{ \mathrm{GD}}\right\|^{2} =\eta^{2}(Q_{B}+Q_{V})\] \[\leq\eta^{2}\Big{(}\Big{(}\alpha^{m}\sigma C_{\chi}\,\sqrt{ \mathbb{E}\|\theta^{\mathrm{init}}-\psi^{*}\|^{2}}\,+\,\varepsilon_{n,m;\nu}^ {\mathrm{init}}(\epsilon)\Big{)}^{2}+\frac{\kappa_{\nu;m}^{2}+\sigma^{2}}{n} \Big{)}\;.\]

**Lemma E.3**.: _Under A1, \(\mathbb{E}\left\|\theta_{\infty}^{\mathrm{GD}}-\theta^{\mathrm{pop}}\right\|^{2} \,\leq\,\frac{4\eta^{2}\sigma^{2}}{n}\;.\)_

Proof of Lemma e.3.: Since both \(F_{\infty}^{\mathrm{GD}}\) and \(f^{\mathrm{pop}}\) involve infinite-length Markov chains, the initializations do not matter and we can decouple the stochasticity of \(X_{i}\) and \(K_{i;\theta^{\mathrm{init}}}\). In particular,

\[\mathbb{E}\left\|\theta_{\infty}^{\mathrm{GD}}-\theta^{\mathrm{pop }}\right\|^{2} =\eta^{2}\,\mathbb{E}\left\|\frac{1}{n}\sum\nolimits_{i\leq n}\big{(} \phi(X_{i})-\phi\big{(}X_{i}^{\theta^{\mathrm{init}}}\big{)}\big{)}-\mathbb{E} \big{[}\phi(X_{1})-\phi\big{(}X_{1}^{\theta^{\mathrm{init}}}\big{)}\big{]} \right\|^{2}\] \[\leq\eta^{2}\big{(}Q_{1}^{\prime}+2\sqrt{Q_{1}^{\prime}Q_{2}^{ \prime}}+Q_{2}^{\prime}\big{)}\;,\]

where

\[Q_{1}^{\prime} \coloneqq\mathbb{E}\left\|\frac{1}{n}\sum\nolimits_{i\leq n} \big{(}\phi(X_{i})-\mathbb{E}\!\left[\phi(X_{1})\right]\big{)}\right\|^{2}\;=\; \frac{\mathrm{Tr}\,\mathrm{Cov}\!\left[\phi(X_{1})\right]}{n}\;=\;\frac{ \mathrm{Tr}\,\nabla_{\theta}^{2}\log Z(\psi^{*})}{n}\;\leq\;\frac{\sigma^{2}}{n}\;,\] \[Q_{2}^{\prime} \coloneqq\mathbb{E}\left\|\frac{1}{n}\sum\nolimits_{i\leq n} \big{(}\phi(X_{i}^{\theta^{\mathrm{init}}})-\mathbb{E}\!\left[\phi\big{(}X_{1}^{ \theta^{\mathrm{init}}}\big{)}\big{)}\right\|\right\|^{2}\] \[=\frac{\mathbb{E}\!\left[\mathrm{Tr}\,\mathrm{Cov}\!\left[\phi(X_{ 1}^{\theta^{\mathrm{init}}})|\theta^{\mathrm{init}}\right]\right]}{n}\;=\;\frac{ \mathbb{E}\!\left[\mathrm{Tr}\,\nabla_{\theta}^{2}\log Z(\theta^{\mathrm{init}}) \right]}{n}\;\leq\;\frac{\sigma^{2}}{n}\;.\]

In the computations above, we have used the relation \(\nabla_{\theta}^{2}\log Z(\theta)=\mathrm{Cov}_{X\sim p_{\theta}}[\phi(X)]\) and the assumption \(\sup_{\theta\in\Psi}\mathrm{tr}(\nabla_{\theta}^{2}\log Z(\theta))=\sigma^{2}\) from A1. This implies the desired bound.

**Lemma E.4**.: _Under A1, \(\mathbb{E}\left\|\theta^{\mathrm{pop}}-\psi^{*}\right\|^{2}\leq\left(1-2\mu\eta+L ^{2}\eta^{2}\right)\mathbb{E}\left\|\theta^{\mathrm{init}}-\psi^{*}\right\|^{2}\)._

Proof of Lemma e.4.: Recall that

\[f^{\mathrm{pop}}(\theta^{\prime})\ =\ \theta-\eta\,\mathbb{E}\big{[}\phi(X_{1})- \phi(X_{1}^{\theta^{\prime}})\big{]}\ =\ \theta-\eta\left(\nabla_{\psi}\log Z(\psi^{*})-\nabla_{\psi}\log Z(\theta^{ \prime})\right)\,.\]

By construction, \(f^{\mathrm{pop}}\) is deterministic and \(f^{\mathrm{pop}}(\psi^{*})=\psi^{*}\). By plugging in the recursions and expanding the square, we get that

\[\mathbb{E}\left\|\theta^{\mathrm{pop}}-\psi^{*}\right\|^{2}\ =\ \mathbb{E}\left\|f^{ \mathrm{pop}}(\theta^{\mathrm{init}})-f^{\mathrm{pop}}(\psi^{*})\right\|^{2}\] \[\ =\mathbb{E}\left\|(\theta^{\mathrm{init}}-\psi^{*})-\eta( \nabla_{\psi}\log Z(\theta^{\mathrm{init}})-\nabla_{\psi}\log Z(\psi^{*})) \right\|^{2}\] \[\ \ \ \ \ +\eta^{2}\,\mathbb{E}\left\|\nabla_{\psi}\log Z( \theta^{\mathrm{init}})-\nabla_{\psi}\log Z(\psi^{*})\right\|^{2}\] \[\ \ \ \ \leq\mathbb{E}\left\|\theta^{\mathrm{init}}-\psi^{*} \right\|^{2}-2\mu\eta\,\mathbb{E}\left\|\theta^{\mathrm{init}}-\psi^{*}\right\| ^{2}+L^{2}\eta^{2}\,\mathbb{E}\left\|\theta^{\mathrm{init}}-\psi^{*}\right\|^ {2}\,.\]

In the last line, we have recalled \(\inf_{\psi\in\Psi}\lambda_{\min}(\nabla_{\psi}^{2}\log Z(\psi))=\mu\) and \(\sup_{\theta\in\Psi}\lambda_{\max}(\nabla_{\psi}^{2}\log Z(\psi))=L\) by A1 and applied Lemma C.8. Combining the coefficients gives the desired statement. 

## Appendix F Proofs for offline SGD

We prove Theorem B.1 (which directly implies Theorem 4.3) and Theorem B.2 in this section. The key ingredient of both proofs is Lemma F.1 below, which provides an iterative error bound for the SGD-with-replacement scheme by combining different approximation bounds in Appendix E. Throughout this section, we denote \(\delta_{t,j}^{\mathrm{SGDw}}\coloneqq\mathbb{E}\left\|\psi_{t,j}^{\mathrm{SGDw}}- \psi^{*}\right\|^{2}\).

**Lemma F.1**.: _Under A1, A2, A3, A4 and A7, we have that for \(1\leq j\leq N-1\),_

\[\sqrt{\delta_{t,j}^{\mathrm{SGDw}}}\ \leq\ \left(1-\eta_{t}\Big{(}\mu- \alpha^{m}\sigma C_{\chi}-\tfrac{L^{2}}{2}\eta_{t}\Big{)}\right)\sqrt{\delta_ {t,j-1}^{\mathrm{SGDw}}}\,+\,\eta_{t}\Big{(}\varepsilon_{n,m,t;\nu}^{\mathrm{ SGDw}}(\epsilon)+\tfrac{5\sigma+5\kappa_{\nu;m}}{\sqrt{B}}\Big{)}\,\]

_where \(1-\eta_{t}\big{(}\mu-\alpha^{m}\sigma C_{\chi}-\tfrac{L^{2}}{2}\eta_{t}\big{)}>0\)._

Proof of Lemma f.1.: We first remark that in view of Lemma C.4, the projection step in Algorithm 2 does not increase \(\delta_{t,j}^{\mathrm{SGDw}}\), so it suffices to bound \(\delta_{t,j}^{\mathrm{SGDw}}\) as if projection is not performed on \(\psi_{t,j}^{\mathrm{SGDw}}\). To apply the results from Appendix E, we identify \(\theta^{\mathrm{init}}=\psi_{t,j-1}^{\mathrm{SGDw}}\) and \(\eta=\eta_{t}\), which allows us to write \(\psi_{t,j}^{\mathrm{SGDw}}=\theta_{m,B}^{\mathrm{SGDw}}\). This also implies \(\mathbb{E}\|\theta^{\mathrm{init}}-\psi^{*}\|^{2}=\delta_{t,j-1}^{\mathrm{SGDw}}\) and \(\varepsilon_{n,m;\nu}^{\mathrm{init}}(\epsilon)\leq\varepsilon_{\nu;m,m,t}^{ \mathrm{SGDw}}(\epsilon)\).

By adding and subtracting the auxiliary gradient updates followed by expanding the square, we obtain

\[\delta_{t,j}^{\mathrm{SGDw}} =\mathbb{E}\left\|\theta_{m,B}^{\mathrm{SGDw}}-\theta_{m}^{\mathrm{ GD}}+\theta_{m}^{\mathrm{GD}}-\theta_{\infty}^{\mathrm{GD}}+\theta_{\infty}^{ \mathrm{GD}}-\theta^{\mathrm{pop}}+\theta^{\mathrm{pop}}-\psi^{*}\right\|^{2}\] \[\stackrel{{(a)}}{{=}} \mathbb{E}\big{\|}\theta_{m,B}^{\mathrm{SGDw}}-\theta_{m}^{ \mathrm{GD}}\big{\|}^{2}+\mathbb{E}\big{\|}\theta_{m}^{\mathrm{GD}}-\theta_{ \infty}^{\mathrm{GD}}\big{\|}^{2}+\mathbb{E}\big{\|}\theta_{\infty}^{\mathrm{ GD}}-\theta^{\mathrm{pop}}\big{\|}^{2}+\mathbb{E}\big{\|}\theta_{\infty}^{\mathrm{pop}}- \theta^{\mathrm{pop}}\big{\|}^{2}+\mathbb{E}\big{\|}\theta^{\mathrm{pop}}- \psi^{*}\big{\|}^{2}\] \[+2\,\mathbb{E}\left\langle\mathbb{E}\big{[}\theta_{m,B}^{\mathrm{ gpw}}-\theta_{m}^{\mathrm{GD}}\big{|}\,\theta^{\mathrm{init}},\mathcal{F}_{n} \big{]}\,,\theta^{\mathrm{pop}}-\psi^{*}\right\rangle+2\,\mathbb{E}\left\langle \theta_{m}^{\mathrm{GD}}-\theta_{\infty}^{\mathrm{GD}}\,,\,\theta_{\infty}^{ \mathrm{GD}}-\theta^{\mathrm{pop}}\right\rangle\] \[+2\,\mathbb{E}\left\langle\mathbb{E}\big{[}\theta_{m}^{\mathrm{GD} }-\theta_{\infty}^{\mathrm{GD}}\big{|}\,\mathcal{A}_{n}\right],\,\theta^{ \mathrm{pop}}-\psi^{*}\right\rangle+2\,\mathbb{E}\left\langle\theta_{\infty}^ {\mathrm{GD}}-\theta_{\infty}^{\mathrm{pop}}\,,\,\theta^{\mathrm{pop}}-\psi^{*}\right\rangle\] \[\stackrel{{(b)}}{{\leq}} \frac{4\eta_{t}^{2}(\sigma^{2}+\kappa_{\nu;m}^{2})}{\sqrt{B}} \mathbb{I}_{\{B<n\}}+\eta_{t}^{2}\Big{(}\Big{(}\alpha^{m}\sigma C_{\chi}\, \sqrt{\delta_{t,j-1}^{\mathrm{SGDw}}}\,+\,\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw }}(\epsilon)\Big{)}^{2}+\frac{\kappa_{\nu;m}^{2}+\sigma^{2}}{n}\Big{)}\] \[+\frac{4\eta_{t}^{2}\sigma^{2}}{n}+(1-2\mu\eta_{t}+L^{2}\eta_{t}^ {2})\delta_{t,j-1}^{\mathrm{SGDw}}\] \[+2\frac{2\eta_{t}\sqrt{\sigma^{2}+\kappa_{\nu;m}^{2}}}{\sqrt{B}} \mathbb{I}_{\{B<n\}}\eta_{t}\Big{(}\alpha^{m}\sigma C_{\chi}\,\sqrt{\delta_{t, j-1}^{\mathrm{SGDw}}}\,+\,\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}( \epsilon)+\frac{\sqrt{\kappa_{\nu;m}^{2}+\sigma^{2}}}{\sqrt{n}}\Big{)}\] \[+2\frac{2\eta_{t}\sqrt{\sigma^{2}+\kappa_{\nu;m}^{2}}}{\sqrt{B}} \mathbb{I}_{\{B<n\}}\frac{2\eta_{t}\sigma}{\sqrt{n}}+0\] \[+2\eta_{t}\Big{(}\alpha^{m}\sigma C_{\chi}\,\sqrt{\delta_{t,j-1}^ {\mathrm{SGDw}}}\,+\,\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}(\epsilon)+ \frac{\sqrt{\kappa_{\nu;m}^{2}+\sigma^{2}}}{\sqrt{n}}\Big{)}\frac{2\eta_{t} \sigma}{\sqrt{n}}\] \[+2\eta_{t}\Big{(}\alpha^{m}\sigma C_{\chi}\,\sqrt{\delta_{t,j-1}^ {\mathrm{SGDw}}}\,+\,\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}(\epsilon) \Big{)}\sqrt{(1-2\mu\eta_{t}+L^{2}\eta_{t}^{2})\delta_{t,j-1}^{\mathrm{SGDw}}}\] \[+2\frac{2\eta_{t}\sigma}{\sqrt{n}}\sqrt{(1-2\mu\eta_{t}+L^{2}\eta _{t}^{2})\delta_{t,j-1}^{\mathrm{SGDw}}}\] \[\stackrel{{(c)}}{{\leq}} \Big{(}1-2\mu\eta_{t}+L\eta_{t}^{2}+\eta_{t}^{2}\alpha^{2m}\sigma ^{2}C_{\chi}^{2}+2\eta_{t}\alpha^{m}\sigma C_{\chi}\sqrt{1-2\mu\eta_{t}+L^{2} \eta_{t}^{2}}\,\Big{)}\times\delta_{t,j-1}^{\mathrm{SGDw}}\] \[+2\bigg{(}\eta_{t}^{2}\alpha^{m}\sigma C_{\chi}\varepsilon_{n,m,t; \nu}^{\mathrm{SGDw}}(\epsilon)+\frac{2\eta_{t}^{2}(\sqrt{\sigma^{2}+\kappa_{ \nu;m}^{2}}+\sigma)\,\alpha^{m}\sigma C_{\chi}}{\sqrt{B}}\] \[\qquad+\eta_{t}\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}(\epsilon) \sqrt{1-2\mu\eta_{t}+L_{2}\eta_{t}^{2}}+\frac{2\eta_{t}\sigma\sqrt{1-2\mu\eta _{t}+L^{2}\eta_{t}^{2}}}{\sqrt{n}}\,\bigg{)}\times\sqrt{\delta_{t,j-1}^{ \mathrm{SGDw}}}\] \[+\eta_{t}^{2}\Big{(}\frac{4(\sigma^{2}+\kappa_{\nu;m}^{2})}{B}+ \big{(}\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}(\epsilon)\big{)}^{2}+\frac{5 \kappa_{\nu;m}^{2}+9\sigma^{2}}{B}+\frac{4\sqrt{\sigma^{2}+\kappa_{\nu;m}^{2} }}{\sqrt{B}}\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}(\epsilon)\] \[\qquad\qquad+\frac{8\sigma\sqrt{\sigma^{2}+\kappa_{\nu;m}^{2}}}{B}+ \frac{4\sigma\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}(\epsilon)}{\sqrt{B}}\Big{)}\] \[=:(A_{1})\times\delta_{t,j-1}^{\mathrm{SGDw}}+(A_{2})\times \sqrt{\delta_{t,j-1}^{\mathrm{SGDw}}}+(A_{3})\;.\]

In \((a)\), we have expanded the square, used \(\mathcal{F}_{n}\) defined in Lemma E.1 and \(\mathcal{A}_{n}\) defined in Lemma E.2, and noted that \(\theta^{\mathrm{pop}}-\psi^{*}\) is almost surely constant given \(\theta^{\mathrm{init}}\); in \((b)\), we have applied Lemmas E.1, E.2, E.3 and E.4 under A1, A2, A3, A4 and A7, and used \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) for \(a,b\geq 0\); in \((c)\), we have grouped the terms by powers of \(\delta_{t,j-1}^{\mathrm{SGDw}}\), bounded the indicator function from above by \(1\) and used \(B\leq n\) to replace \(n\) in the denominator by \(B\). While the computation above is complicated, we remark that the key steps are taking the conditional expectations for the cross-terms in \((b)\), which gives us a tighter bound than directly applying a triangle inequality of the form \(\mathbb{E}\|Y_{1}+Y_{2}\|^{2}\leq(\sqrt{\mathbb{E}\|Y_{1}\|^{2}}+\sqrt{\mathbb{E} \|Y_{2}\|^{2}})\). To further simplify the bounds, we seek to bound each coefficient by a square, which yields

\[(A_{1}) =\Big{(}\eta_{t}\alpha^{m}\sigma C_{\chi}+\sqrt{1-2\mu\eta_{t}+L \eta_{t}^{2}}\,\Big{)}^{2}\,\] \[(A_{2}) \leq 2\Big{(}\eta_{t}\alpha^{m}\sigma C_{\chi}+\sqrt{1-2\mu\eta_{t}+L \eta_{t}^{2}}\,\Big{)}\times\ \eta_{t}\Big{(}\varepsilon_{n,m,t;\nu}^{\rm SGDw}(\epsilon)+\frac{4\sigma+2 \kappa_{\nu;m}}{\sqrt{B}}\Big{)}\,\] \[(A_{3}) \leq\eta_{t}^{2}\Big{(}\big{(}\varepsilon_{n,m,t;\nu}^{\rm SGDw}( \epsilon)\big{)}^{2}+\frac{8\sigma^{2}+4\kappa_{\nu;m}^{2}}{\sqrt{B}}\varepsilon _{n,m,t;\nu}^{\rm SGDw}(\epsilon)+\frac{21\sigma^{2}+17\kappa_{\nu;m}^{2}}{B} \Big{)}\] \[\leq\eta_{t}^{2}\Big{(}\varepsilon_{n,m,t;\nu}^{\rm SGDw}( \epsilon)+\frac{5\sigma+5\kappa_{\nu;m}}{\sqrt{B}}\Big{)}^{2}\.\]

This implies

\[\delta_{t,j}^{\rm SGDw} \leq(A_{1})\times\delta_{t,j-1}^{\rm SGDw}+(A_{2})\times\sqrt{ \delta_{t,j-1}^{\rm SGDw}}+(A_{3})\] \[\leq\Big{(}\Big{(}\eta_{t}\alpha^{m}\sigma C_{\chi}+\sqrt{1-2\mu \eta_{t}+L\eta_{t}^{2}}\Big{)}\sqrt{\delta_{t,j-1}^{\rm SGDw}}+\eta_{t}\Big{(} \varepsilon_{n,m,t;\nu}^{\rm SGDw}(\epsilon)+\frac{5\sigma+5\kappa_{\nu;m}}{ \sqrt{B}}\Big{)}\Big{)}^{2}\.\]

Now note that since \(\mu\leq L\) by the definitions in A1, \(2\mu\eta_{t}-L^{2}\eta_{t}^{2}\leq 2L\eta_{t}-L^{2}\eta_{t}^{2}\leq 1\). By using \(\sqrt{1-x}\leq 1-\frac{x}{2}\) for all \(x\leq 1\), we get that

\[\sqrt{1-2\mu\eta_{t}+L^{2}\eta_{t}^{2}}\ \leq\ 1-\mu\eta_{t}+\frac{L^{2}}{2} \eta_{t}^{2}\.\]

Substituting this into the earlier bound and taking a square-root, we obtain the desired bound that

\[\sqrt{\delta_{t,j}^{\rm SGDw}}\ \leq\ \Big{(}1-\eta_{t}\Big{(}\mu-\alpha^{m} \sigma C_{\chi}-\frac{L^{2}}{2}\eta_{t}\Big{)}\Big{)}\,\sqrt{\delta_{t,j-1}^{ \rm SGDw}}\ +\ \eta_{t}\Big{(}\varepsilon_{n,m,t;\nu}^{\rm SGDw}(\epsilon)+\frac{5\sigma+5 \kappa_{\nu;m}}{\sqrt{B}}\Big{)}\.\]

Moreover, since \(L\geq\mu\) by definition from A1, we have

\[1-\eta_{t}\Big{(}\mu-\alpha^{m}\sigma C_{\chi}-\frac{L^{2}}{2}\eta_{t}\Big{)} \ =\Big{(}\frac{L}{\sqrt{2}}\eta_{t}-1\Big{)}^{2}+(\sqrt{2}\,L-\mu)\eta_{t}+ \alpha^{m}\sigma C_{\chi}\eta_{t}\ >\ 0\,\]

which finishes the proof. \(\square\)

### Proof of Theorem b.1

Under A1, A2, A3, A4 and A7, Lemma F.1 implies

\[\sqrt{\delta_{t,j}^{\rm SGDw}}\ \leq\Big{(}1-\tilde{\mu}_{m}Ct^{-\beta}+ \frac{L^{2}C^{2}}{2}t^{-2\beta}\Big{)}\,\sqrt{\delta_{t,j-1}^{\rm SGDw}}\,+ \,Ct^{-\beta}\,\sigma_{n,T}^{\rm SGDw}\]

for \(1\leq t\leq T\) and \(1\leq j\leq N\), where we have used \(\tilde{\mu}_{m}=\mu-\alpha^{m}\sigma C_{\chi}\), \(\eta_{t}=Ct^{-\beta}\) and

\[\varepsilon_{n,m,t;\nu}^{\rm SGDw}(\epsilon)+\frac{5\sigma+5\kappa_{\nu;m}}{ \sqrt{B}}\ \leq\ \varepsilon_{n,m,T,\nu}^{\rm SGDw}(\epsilon)+\frac{5\sigma+5\kappa_{\nu;m}}{ \sqrt{B}}\ =\ \sigma_{n,T}^{\rm SGDw}\.\]

By an induction on \(j=1,\ldots,N\), we have

\[\sqrt{\delta_{t,N}^{\rm SGDw}}\ \leq\Big{(}1-\tilde{\mu}_{m}Ct^{-\beta}+ \frac{L^{2}C^{2}}{2}t^{-2\beta}\Big{)}^{N}\sqrt{\delta_{t,0}^{\rm SGDw}}\] \[\qquad\qquad+\ C\sigma_{n,T}^{\rm SGDw}\,t^{-\beta}\,\sum_{j=1}^{ N}\Big{(}1-\tilde{\mu}_{m}Ct^{-\beta}+\frac{L^{2}C^{2}}{2}t^{-2\beta}\Big{)}^{j-1}\.\]

By noting that \(\delta_{t,0}^{\rm SGDw}=\delta_{t-1,N}^{\rm SGDw}\) almost surely for \(t\geq 2\) and using another induction on \(t=1,\ldots,T\),

\[\sqrt{\delta_{T,N}^{\rm SGDw}}\ \leq Q_{0}\,\sqrt{\delta_{0,0}^{\rm SGDw}}\ +\ C\sigma_{n,T}^{\rm SGDw}\sum_{t=1}^{T}Q_{t}\,A_{t}\,t^{-\beta}\,\] (18)

where

\[Q_{t}\coloneqq\prod_{s=t+1}^{T}\Big{(}1-\tilde{\mu}_{m}Cs^{-\beta}+\frac{L^{2} C^{2}}{2}s^{-2\beta}\Big{)}^{N}\ \ \text{and}\ \ A_{t}\coloneqq\sum_{j=1}^{N}\Big{(}1-\tilde{\mu}_{m}Ct^{-\beta}+\frac{L^{2}C^{2} }{2}t^{-2\beta}\Big{)}^{j-1}\.\]

Note that by Lemma F.1, we also have that \(1-\tilde{\mu}_{m}Ct^{-\beta}+\frac{L^{2}C^{2}}{2}t^{-2\beta}>0\). This allows us to apply Lemma C.3 to obtain

\[\kappa_{0}\ \leq\ \exp\Big{(}1-N\tilde{\mu}_{m}C\varphi_{1-\beta}(T+1)+\frac{ NL^{2}C^{2}}{2}\varphi_{1-2\beta}(T+1)\Big{)}\ =\ E_{1}^{T,N}\.\]To control \(\sum_{t=1}^{T}Q_{t}A_{t}t^{-\beta}\), recall that \(\beta\in[0,1]\), \(\tilde{\mu}_{m}>0\), \(\frac{L^{2}C^{2}}{2}>0\), and that

\[E_{2}^{T,N}\ =\ \exp\Big{(}-\frac{N\tilde{\mu}_{m}C}{2}\varphi_{1-\beta}(T+1)+2 NL^{2}C^{2}\varphi_{1-2\beta}(T+1)\Big{)}\.\]

We can now apply Lemma C.3: If \(\beta\not\in\{\frac{1}{2},1\}\), then

\[\sum_{t=1}^{T}Q_{t}\,A_{t}\,t^{-\beta}\ \leq\frac{2^{2\beta+1}}{\tilde{\mu}_{m}C}e^{ \frac{\tilde{\mu}_{m}C}{2(1-\beta)}\frac{N}{(T+1)^{\beta}}}+\frac{3^{\beta}(1 +\tilde{\mu}_{m}C)^{N-1}(T+2)^{\beta}}{L^{2}C^{2}}\,E_{2}^{T,N}\.\]

If \(\beta=\frac{1}{2}\), i.e. \(2\beta=1\), we have

\[\sum_{t=1}^{T}Q_{t}\,A_{t}\,t^{-\beta}\ \leq\frac{4}{\tilde{\mu}_{m}C}e^{ \frac{\tilde{\mu}_{m}CN}{(T+1)^{1/2}}}+2N(1+\tilde{\mu}_{m}C)^{N-1}\varphi_{ \frac{1}{2}-L^{2}C^{2}N}(T+1)\,E_{2}^{T,N}\.\]

If \(\beta=1\), we get that

\[\sum_{t=1}^{T}Q_{t}\,A_{t}\,t^{-\beta}\ \leq\frac{4}{\tilde{\mu}_{m}C}+ \frac{3N(1+\frac{L^{2}C^{2}}{2})^{N-1}e^{2L^{2}C^{2}N}\,\log(T+1)}{(T+1)^{( \tilde{\mu}_{m}CN)/2}}\]

Substituting the bounds into (18), we get the desired bound that \(\sqrt{\delta_{T,N}^{\mathrm{SGDw}}}\) is upper bounded by

\[\begin{cases}E_{1}^{T,N}\sqrt{\delta_{0,0}^{\mathrm{SGDw}}}\,+\,C\sigma_{n,T}^{ \mathrm{SGDw}}\Big{(}\frac{4^{\frac{\tilde{\mu}_{m}CN}{(T+1)^{1/2}}}}{\tilde{ \mu}_{m}C}+2N(1+\tilde{\mu}_{m}C)^{N-1}\varphi_{\frac{1}{2}-L^{2}C^{2}N}(T+1 )\,E_{2}^{T,N}\Big{)}\\ \hskip 113.811024pt\text{for }\beta=\frac{1}{2}\,\\ E_{1}^{T,N}\sqrt{\delta_{0,0}^{\mathrm{SGDw}}}\,+\,C\sigma_{n,T}^{\mathrm{SGDw}}\Big{(}\frac{4}{\tilde{\mu}_{m}C}+\frac{3N(1+ \frac{L^{2}C^{2}}{2})^{N-1}\,e^{2L^{2}C^{2}N}\,\log(T+1)}{(T+1)^{(\tilde{\mu}_ {m}CN)/2}}\Big{)}\hskip 113.811024pt\text{for }\beta=1\,\\ E_{1}^{T,N}\sqrt{\delta_{0,0}^{\mathrm{SGDw}}}\,+\,C\sigma_{n,T}^{\mathrm{SGDw}}\Big{(}\frac{2^{2\beta+1}}{\tilde{\mu}_{m}C}e^{ \frac{\tilde{\mu}_{m}C}{2(1-\beta)}\frac{N}{(T+1)^{\beta}}}+\frac{3^{\beta}(1 +\tilde{\mu}_{m}C)^{N-1}(T+2)^{\beta}}{L^{2}C^{2}}\,E_{2}^{T,N}\Big{)}\ \text{ otherwise}\.\end{cases}\]

### Proof of Theorem B.2

Recall that \(\delta_{t,j}^{\mathrm{SGDo}}\coloneqq\mathbb{E}\left\|\psi_{t,j}^{\mathrm{SGDo}}-\psi^{\star}\right\|^{2}\). To apply the results from Appendix E to \(\psi_{t,j}^{\mathrm{SGDo}}\), we condition on \(\mathcal{S}_{t}^{p}\), which in particular fixes \(S_{t,j}^{o}\), the last size-\(B\) subset of \([n]\) chosen. We then identify \(\theta^{\mathrm{init}}=\psi_{t,j-1}^{\mathrm{SGDo}}\), \(\eta=\eta_{t}\) and the dataset used as \(\mathcal{D}_{t,j}^{o}\coloneqq(X_{i}\,:\,i\in S_{t,j}^{o})\), which allows us to identify \(\psi_{t,j}^{\mathrm{SGDo}}=\theta_{m,B}^{\mathrm{GD}}\), the full-batch gradient descent update using \(\mathcal{D}_{t,j}^{o}\). Meanwhile, we note that almost surely \(\theta_{m}^{\mathrm{GD}}=\theta_{m,B}^{\mathrm{SGDw}}\), the SGD-with-replacement iterate that uses the full dataset \(\mathcal{D}_{t,j}^{o}\). Observe that the proof of Lemma F.1 holds with \(\delta_{t,j-1}^{\mathrm{SGDo}}\) replaced by any random initialization \(\theta^{\mathrm{init}}\) possibly correlated with \(X_{1},\ldots,X_{n}\), which allows us to obtain

\[\sqrt{\delta_{t,j}^{\mathrm{SGDo}}}\ \leq\ \Big{(}1-\eta_{t}\Big{(}\mu-\alpha^{m} \sigma C_{\chi}-\frac{L^{2}}{2}\eta_{t}\Big{)}\Big{)}\,\sqrt{\delta_{t,j-1}^{ \mathrm{SGDo}}}\,+\,\eta_{t}\Big{(}\varepsilon_{n,m,t;\nu}^{\mathrm{SGDw}}( \epsilon)+\frac{5\sigma+5\kappa_{\nu;m}}{\sqrt{B}}\Big{)}\.\]

Since the error recursion for \(\delta_{t,j}^{\mathrm{SGDo}}\) is identical to that of \(\delta_{t,j}^{\mathrm{SGDw}}\) in Lemma F.1, the proof of Theorem 4.3 follows directly, thereby yielding an identical result for \(\delta_{T,N}^{\mathrm{SGDo}}\) as with \(\delta_{T,N}^{\mathrm{SGDw}}\) in Theorem 4.3. 

## Appendix G Proofs for tail probability bounds in offline SGD

We present the proofs for results in Appendix B.3 that control the tail probability terms \(\vartheta_{\nu;n,m,T}^{\mathrm{SGDw}}\) and \(\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}\).

Proof of Lemma b.3.: For \(\delta>0\), let \(N_{\delta}\) be the \(\delta\)-covering number of \(\Psi\), which satisfies \(N_{\delta}\leq\big{(}r_{\Psi}(1+2/\delta)\big{)}^{p}\) (Example 5.8, [50]). Note also that by the Jensen's inequality applied to \(\mathbb{E}[\,\star\,|X_{1}]\) and Assumption A.6, there exist some \(\sigma_{m},\zeta_{m}>0\) such that, for any \(z\in\mathbbm{R}^{p}\) with \(\|z\|\leq\zeta_{m}\),

\[\mathbb{E}[e^{z^{\top}(\mathbb{E}[\phi(K_{\psi^{\star}}^{m}(X_{1}))|X_{1}]- \mathbb{E}[\phi(K_{\psi^{\star}}^{m}(X_{1}))])}]\ \leq\mathbb{E}[e^{z^{\top}(\phi(K_{\psi^{\star}}^{m}(X_{1}))- \mathbb{E}[\phi(K_{\psi^{\star}}^{m}(X_{1})))]}]\ \leq\ \mathbb{E}[e^{z^{\top}(\phi(K_{\psi^{\star}}^{m}(X_{1}))- \mathbb{E}[\phi(K_{\psi^{\star}}^{m}(X_{1})))]}]\ \leq\ e^{\sigma_{m}^{2}\,\|z\|^{2}/2}\.\]

[MISSING_PAGE_EMPTY:47]

where we have used assumption A4 in the last line. This implies

\[\left(\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(3C_{m}\delta)\right)^{2} =9C_{m}^{2}\delta^{2}+\kappa_{\nu;m}^{2}\Big{(}\vartheta_{n,m,T}^{ \mathrm{SGDw}}(3C_{m}\delta)\Big{)}^{(\nu-2)/\nu}\] \[\leq 9C_{m}^{2}\delta^{2}+\kappa_{\nu;m}^{\nu}(r_{\Psi})^{\frac{( \nu-2)p}{\nu}}C_{m}^{-(\nu-2)}\times\frac{(1+2\delta^{-1})^{\frac{(\nu-2)p}{ \nu}}}{n^{\frac{\nu-2}{2}}\delta^{\nu-2}}\.\]

Choosing \(\delta=n^{-\frac{(\nu-2)p}{2(\nu^{2}+(\nu-2)p)}}\leq 1\), we get that

\[\inf_{\epsilon>0}\left(\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}} \left(\epsilon\right)\right)^{2} \leq\ \left(\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(3C_{m}\delta) \right)^{2}\] \[\leq 9C_{m}^{2}n^{-\frac{(\nu-2)\nu}{\nu^{2}+(\nu-2)p}}+\kappa_{ \nu;m}^{\nu}(r_{\Psi})^{\frac{(\nu-2)p}{\nu}}C_{m}^{-(\nu-2)}3^{\frac{(\nu-2 )p}{\nu}}n^{-\frac{(\nu-2)\nu}{\nu^{2}+(\nu-2)p}}\] \[=\left(9C_{m}^{2}+\kappa_{\nu;m}^{\nu}(r_{\Psi})^{\frac{(\nu-2)p }{\nu}}C_{m}^{-(\nu-2)}3^{\frac{(\nu-2)p}{\nu}}\right)n^{-\frac{(\nu-2)\nu}{ \nu^{2}+(\nu-2)p}}\.\]

Taking a squareroot across and using \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) for \(a,b>0\) gives the desired bound. The limiting result follows by substituting this bound into Theorem B.1. 

Proof of Lemma B.5.: By a Markov's inequality and a Jensen's inequality with respect to the empirical average, we have

\[\vartheta_{n,m,T}^{\mathrm{SGDw}}(\epsilon) \leq\ \sup_{t\in[T],j\in[N]}\frac{\sum_{i=1}^{n}\mathbb{E}\Big{[} \mathbb{E}\Big{[}\phi\Big{(}K_{\psi_{t-1,j}^{\mathrm{SGDw}}}^{m}(X_{i}) \Big{)}\Big{|}X_{i},\psi_{t-1,j}^{\mathrm{SGDw}}\Big{]}-\mathbb{E}\Big{[} \phi\Big{(}K_{\psi_{t-1,j}^{\mathrm{SGDw}}}^{m}(X_{1}^{\prime})\Big{)}\Big{|} \psi_{t-1,j}^{\mathrm{SGDw}}\Big{]}\Big{]}}{n\epsilon}\] \[=:\sup_{t\in[T],j\in[N]}\frac{\sum_{i=1}^{n}A_{tji}}{n\epsilon}\.\]

Meanwhile by a triangle inequality and the ergodicity assumption, we have

\[A_{tji} \leq\mathbb{E}\Big{\|}\mathbb{E}\Big{[}\phi\Big{(}K_{\psi_{t-1,j }^{\mathrm{SGDw}}}^{m}(X_{i})\Big{)}\Big{|}X_{i},\psi_{t-1,j}^{\mathrm{SGDw}} \Big{]}-\mathbb{E}\Big{[}\phi\Big{(}K_{1}^{\psi_{t-1,j}^{\mathrm{SGDw}}} \Big{)}\Big{|}\psi_{t-1,j}^{\mathrm{SGDw}}\Big{]}\Big{\|}\] \[\leq 2\tilde{C}_{K}\tilde{\alpha}^{m}\.\]

This implies \(\vartheta_{n,m,T}^{\mathrm{SGDw}}(\epsilon)\leq 2\tilde{C}_{K}\tilde{\alpha}^{m} \epsilon^{-1}\), and therefore

\[\big{(}\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(\epsilon)\big{)}^{2} \ \leq\ \epsilon^{2}+2^{\frac{\nu-2}{\nu}}\kappa_{\nu;m}^{2}(\tilde{C}_{K})^{\frac{ \nu-2}{\nu}}\tilde{\alpha}^{\frac{(\nu-2)m}{\nu}}\epsilon^{-\frac{\nu-2}{\nu}}\.\]

Choosing \(\epsilon=\tilde{\alpha}^{(\nu-2)m/(3\nu-2)}\) gives

\[\inf_{\epsilon>0}\big{(}\varepsilon_{\nu;n,m,T}^{\mathrm{SGDw}}(\epsilon) \big{)}^{2} \ \leq\ \big{(}1+2^{\frac{\nu-2}{\nu}}\kappa_{\nu;m}^{2}(\tilde{C}_{K})^{\frac{ \nu-2}{\nu}}\big{)}\tilde{\alpha}^{\frac{2(\nu-2)m}{3\nu-2}}\.\]

Taking a squareroot across and using \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) for \(a,b>0\) gives the desired bound. The limiting result follows by substituting this bound into Theorem B.1.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract summarizes the theoretical results of our paper. We cleary mention that while CD _can_ achieve these rates, assumptions are needed to guarantee this. We also mention the model class studied in this paper, namely (unnormalized) exponential family distributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We mention limitations multiple times in the main body of our work: after introducing the assumptions in Section 3.1, we mention that spectral gaps may not be verified for heavy tail distributions, or act numerically unfavorably for multimodal distributions, and that constants may be large in practice. In the discussion (Section 6), we reflect back on these limitations, and we mention the limitations of our considered model class (unnormalized exponential families). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best

[MISSING_PAGE_FAIL:50]

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments, and thus no code or data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: As a theory paper, we believe that our work does not breach the code of conduct. With this work, we aim to advance the understanding of statistically efficient algorithms, a domain which has the potential to improve the overall _computational_ efficiency of machine learning algorithms. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We believe that our work will not have the negative impacts mentioned (e.g., disinformation, surveillance, fairness, privacy, security considerations). We believe that advancing the domain of statistical effiency has the potential to improve the overall computational efficiency of machine learning algorithms, which, all else left equal, could constitute a positive societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Since our work does not include experiments nor data, and only analyzes existing models, we do not believe that our work requires any specific guidelines. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [NA] Justification: The paper does not use existing assets. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work does not release any assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.