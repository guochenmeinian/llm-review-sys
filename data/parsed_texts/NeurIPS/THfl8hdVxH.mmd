# White-Box Transformers via Sparse Rate Reduction

 Yaodong Yu\({}^{1}\) Sam Buchanan\({}^{2}\) Druv Pai\({}^{1}\) Tianzhe Chu\({}^{1}\) Ziyang Wu\({}^{1}\) Shengbang Tong\({}^{1}\)

Benjamin D. Haeffele\({}^{3}\) Yi Ma\({}^{1}\)

\({}^{1}\)University of California, Berkeley \({}^{2}\)TTIC \({}^{3}\)Johns Hopkins University

###### Abstract

In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called _sparse rate reduction_. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of _white-box_ transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at https://github.com/Ma-Lab-Berkeley/CRATE.

## 1 Introduction

In recent years, deep learning has seen tremendous empirical success in processing massive amounts of high-dimensional and multi-modal data. Much of this success is owed to effective learning of the data distribution and then transforming the distribution to a parsimonious, i.e. _structured and compact_, representation [39, 50, 52, 62], which facilitates many downstream tasks (e.g., in vision, classification [23, 40], recognition and segmentation [25, 38, 77], and generation [31, 65, 66]). To this end, many models and methods have been proposed and practiced, each with its own strengths and limitations. Here, we give several popular methods a brief accounting as context for a complete understanding and unification that we seek in this work.

Transformer models and self-attention.Transformers [28] are one of the latest popular models for learning a representation for high-dimensional structured data, such as text [28, 30, 37], images [40, 75], and other types of signals [48, 57]. After the first block, which converts each data point (such as a text corpus or image) into a set or sequence of _tokens_, further processing is performed on the token sets, in a medium-agnostic manner [28, 40]. A cornerstone of the transformer model is the so-called _self-attention layer_, which exploits the statistical correlations among the sequence of tokens to refine the token representation. Transformers have been highly successful in learning compact representations that perform well on many downstream tasks. Yet the transformer networkarchitecture is empirically designed and lacks a rigorous mathematical interpretation. In fact, the output of the attention layer itself has several competing interpretations [68; 78]. As a result, the statistical and geometric relationship between the data distribution and the final representation learned by a transformer largely remains a mysterious black box.

Diffusion models and denoising.Diffusion models [22; 34; 41; 43; 44] have recently become a popular method for learning the data distribution, particularly for generative tasks and natural image data which are highly structured but notoriously difficult to effectively model [3; 5]. The core concept of diffusion models is to start with features sampled from a Gaussian noise distribution (or some other standard template) and _iteratively denoise_ and deform the feature distribution until it converges to the original data distribution. This process is computationally intractable if modeled in just one step [61], so it is typically broken into multiple incremental steps. The key to each step is the so-called _score function_, or equivalently [13] an estimate for the "optimal denoising function"; in practice this function is modeled using a generic black-box deep network. Diffusion models have shown effectiveness at learning and sampling from the data distribution [56; 60; 65]. However, despite some recent efforts [81], they generally do not establish any clear correspondence between the initial features and data samples. Hence, diffusion models themselves do not offer a parsimonious or interpretable representation of the data distribution.

Structure-seeking models and rate reduction.In both of the previous two methods, the representations were constructed implicitly as a byproduct of solving a downstream task (e.g., classification or generation/sampling) using deep networks. However, one can also explicitly learn a representation of the data distribution as a task in and of itself; this is most commonly done by trying to identify and represent low-dimensional structures in the input data. Classical examples of this paradigm include model-based approaches such as sparse coding [2; 29] and dictionary learning [17; 21; 47], out of which grew early attempts at designing and interpreting deep network architectures [18; 32]. More recent approaches build instead from a model-free perspective, where one learns a representation through a sufficiently-informative pretext task (such as compressing similar and separating dissimilar data in contrastive learning [45; 69; 80], or maximizing the information gain in the class of maximal coding rate reduction methods [6; 46; 55]). Compared to black-box deep learning approaches, both model-based and model-free representation learning schemes have the advantage of being more interpretable: they allow users to explicitly design desired properties of the learned representation [46; 55; 63]. Furthermore, they allow users to construct new white-box forward-constructed deep network architectures [11; 55; 59] by _unrolling the optimization strategy for the representation learning objective_, such that each layer of the constructed network implements an iteration of the optimization algorithm [11; 53; 55]. Several recent works [71; 74; 76] consider the connections between transformer architectures [28] and unrolled optimization. Unfortunately, in this paradigm, if the desired properties are narrowly defined, it may be difficult to achieve good practical performance on large real-world datasets.

Our contributions, and outline of this work.In this work, we aim to remedy the limitations of these existing methods with a more unified framework for designing transformer-like network architectures that leads to both mathematical interpretability and good practical performance. To this end, we propose to learn a sequence of _incremental mappings_ to obtain a most _compressed and sparse_ representation for the input data (or their token sets) that optimizes _a unified objective function_ known as the sparse rate reduction, specified later in (1). The goal of the mapping is illustrated in Figure 1. Within this framework, we unify the above three seemingly disparate approaches and show that _transformer-like deep network layers can be naturally derived from unrolling iterative

Figure 1: The ‘main loop’ of the crate white-box deep network design. After encoding input data \(\bm{X}\) as a sequence of tokens \(\bm{Z}^{0}\), crate constructs a deep network that transforms the data to a canonical configuration of low-dimensional subspaces by successive _compression_ against a local model for the distribution, generating \(\bm{Z}^{\ell+1/2}\), and _sparsification_ against a global dictionary, generating \(\bm{Z}^{\ell+1}\). Repeatedly stacking these blocks and training the model parameters via backpropagation yields a powerful and interpretable representation of the data.

optimization schemes to incrementally optimize the sparse rate reduction objective._ In particular, our contributions and outline of the paper are as follows:

* In Section 2.2 we show, using an idealized model for the token distribution, that if one _iteratively denoises_ the tokens towards a family of low-dimensional subspaces, the associated score function assumes an explicit form similar to a self-attention operator seen in transformers.
* In Section 2.3 we derive the multi-head self-attention layer as an unrolled gradient descent step to minimize the lossy coding rate part of the rate reduction, showing another interpretation of the self-attention layer as compressing the token representation.
* In Section 2.4 we show that the multi-layer perceptron which immediately follows the multi-head self-attention in transformer blocks can be interpreted as (and replaced by) a layer which incrementally optimizes the remaining part of the sparse rate reduction objective by constructing a sparse coding of the token representations.
* In Section 2.5 we use this understanding to create a new white-box (fully mathematically interpretable) transformer architecture called crate (i.e., Coding RAte reduction TransformEr), where each layer performs a _single step_ of an alternating minimization algorithm to optimize the sparse rate reduction objective.

Hence, within our framework, the learning objective function, the deep learning architecture, and the final learned representation _all become white boxes_ that are fully mathematically interpretable. As the experiments in Section 3 show, the crate networks, despite being simple, can already learn the desired compressed and sparse representations on large-scale real-world datasets and achieve performance on par with much more heavily engineered transformer networks (such as ViT) on a wide variety of tasks (e.g., classification and transfer learning).

## 2 Technical Approach and Justification

### Objective and Approach

We consider a general learning setup associated with real-world signals. We have some random variable \(\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}\) which is our data source; each \(\bm{x}_{i}\in\mathbb{R}^{D}\) is interpreted as a _token1_, and the \(\bm{x}_{i}\)'s may have arbitrary correlation structures. We use \(\bm{Z}=[\bm{z}_{1},\dots,\bm{z}_{N}]\in\mathbb{R}^{d\times N}\) to denote the random variable which defines our representations. Each \(\bm{z}_{i}\in\mathbb{R}^{d}\) is the representation of the corresponding token \(\bm{x}_{i}\). We are given \(B\geq 1\) i.i.d. samples \(\bm{X}_{1},\dots,\bm{X}_{B}\sim\bm{X}\), whose tokens are \(\bm{x}_{i,b}\). The representations of our samples are denoted \(\bm{Z}_{1},\dots,\bm{Z}_{B}\sim\bm{Z}\), and those of our tokens are \(\bm{z}_{i,b}\). Finally, for a given network, we use \(\bm{Z}^{\ell}\) to denote the output of the first \(\ell\) layers when given \(\bm{X}\) as input. Correspondingly, the sample outputs are \(\bm{Z}_{i}^{\ell}\) and the token outputs are \(\bm{z}_{i,b}^{\ell}\).

Footnote 1: For language transformers, tokens roughly correspond to words [28], while for vision transformers, tokens correspond to image patches [40].

Objective for learning a structured and compact representation.Following the framework of rate reduction [55], we contend that the goal of representation learning is to find a feature mapping \(f\colon\bm{X}\in\mathbb{R}^{D\times N}\to\bm{Z}\in\mathbb{R}^{d\times N}\) which transforms input data \(\bm{X}\in\mathbb{R}^{D\times N}\) with a potentially nonlinear and multi-modal distribution to a (piecewise) _linearized and compact_ feature representation \(\bm{Z}\in\mathbb{R}^{d\times N}\). While the joint distribution of tokens \((\bm{z}_{i})_{i=1}^{N}\) in \(\bm{Z}\) may be sophisticated (and task-specific), we further contend that it is reasonable and practical to require that the target marginal distribution of individual tokens \(\bm{z}_{i}\) should be highly compressed and structured, amenable for compact coding. Particularly, we require the distribution to be _a mixture of low-dimensional (say \(K\)) Gaussian distributions_, such that the \(k^{\mathrm{th}}\) Gaussian has mean \(\bm{0}\in\mathbb{R}^{d}\), covariance \(\bm{\Sigma}_{k}\sim\bm{0}\in\mathbb{R}^{d\times d}\), and support spanned by the orthonormal basis \(\bm{U}_{k}\in\mathbb{R}^{d\times p}\). We denote \(\bm{U}_{[K]}=(\bm{U}_{k})_{k=1}^{K}\) to be the set of bases of all Gaussians. Hence to maximize the _information gain_[62] for the final token representation, we wish to maximize the rate reduction [6; 46] of the tokens, i.e., \(\max_{\bm{Z}}\Delta R(\bm{Z};\bm{U}_{[K]})=R(\bm{Z})-R^{c}(\bm{Z};\bm{U}_{[K]})\), where \(R\) and \(R^{c}\) are estimates of lossy coding rates to be formally defined in (7) and (8). This also promotes token representations \(\bm{z}_{i}\) from different Gaussians to be _incoherent_[46]. Since rate reduction is an intrinsic measure of goodness for the representation, it is invariant to arbitrary rotations of the representations. Therefore, to ensure the final representations are amenable to more compact coding, we would like to transform the representations (and their supporting subspaces) so that they become _sparse_ with respect to the standard coordinates of the resulting representation space.2 The combined rate reduction and sparsification process is illustrated in Figure 1. Computationally, we may combine the above two goals into a unified objective for optimization:

Footnote 2: That is, having the fewest nonzero entries.

\[\max_{f\in\mathcal{F}}\mathbb{E}_{\bm{Z}}\big{[}\Delta R(\bm{Z};\bm{U}_{[K]})- \lambda\|\bm{Z}\|_{0}\big{]}=\max_{f\in\mathcal{F}}\mathbb{E}_{\bm{Z}}\big{[}R( \bm{Z})-R^{c}(\bm{Z};\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}\big{]}\text{ s.t. }\bm{Z}=f(\bm{X}),\] (1)

where the \(\ell^{0}\) norm \(\|\bm{Z}\|_{0}\) promotes the sparsity of the final token representations \(\bm{Z}=f(\bm{X})\).3 We call this objective "_sparse rate reduction_."

Footnote 3: To simplify the notation, we will discuss the objective for one sample \(\bm{X}\) at a time with the understanding that we always mean to optimize the expectation.

White-box deep architecture as unrolled incremental optimization.Although easy to state, each term of the above objective can be computationally very challenging to optimize [55; 70]. Hence it is natural to take an approximation approach that realizes the global transformation \(f\) optimizing (1) through a concatenation of multiple, say \(L\), simple _incremental and local_ operations \(f^{\ell}\) that push the representation distribution towards the desired parsimonious model distribution:

\[f\colon\bm{X}\xrightarrow{f^{0}}\bm{Z}^{0}\to\dots\to\bm{Z}^{\ell} \xrightarrow{f^{\ell}}\bm{Z}^{\ell+1}\to\dots\to\bm{Z}^{L}=\bm{Z},\] (2)

where \(f^{0}:\mathbb{R}^{D}\to\mathbb{R}^{d}\) is the pre-processing mapping that transforms input tokens \(\bm{x}_{i}\in\mathbb{R}^{D}\) to their token representations \(\bm{z}_{i}^{1}\in\mathbb{R}^{d}\).

Each incremental _forward mapping_\(\bm{Z}^{\ell+1}=f^{\ell}(\bm{Z}^{\ell})\), or a "layer", transforms the token distribution to _optimize_ the above sparse rate reduction objective (1), conditioned on the distribution of its input tokens \(\bm{Z}^{\ell}\). In contrast to other unrolled optimization approaches such as the ReduNet [55], we _explicitly model_ the distribution of \(\bm{Z}^{\ell}\) at each layer, say as a mixture of linear subspaces or sparsely generated from a dictionary. The model parameters are learned from data (say via _backward propagation_ with end-to-end training). This separation of forward "optimization" and backward "learning" clarifies the mathematical role of each layer as an operator transforming the distribution of its input, whereas the input distribution is in turn modeled (and subsequently learned) by the parameters of the layer.

We show that we can derive these incremental, local operations through an unrolled optimization perspective to achieve (1) through Sections 2.3 to 2.5. Once we decide on using an incremental approach to optimizing (1), there are a variety of possible choices to achieve the optimization. Given a model for \(\bm{\dot{Z}}^{\ell}\), say a mixture of subspaces \(\bm{U}_{[K]}\), we opt for a two-step _alternating minimization_ process with a strong conceptual basis: first in Section 2.3, we _compress_ the tokens \(\bm{Z}^{\ell}\) via a gradient step to minimize the coding rate term \(\min_{\bm{Z}}R^{c}(\bm{Z};\bm{U}_{[K]})\); second, in Section 2.4, we _sparsify_ the compressed tokens, with a suitably-relaxed proximal gradient step on the difference of the sparsity penalty and the expansion term, i.e., \(\min_{\bm{Z}}[\lambda\|\bm{Z}\|_{0}-R(\bm{Z})]\). Both actions are applied incrementally and repeatedly, as each \(f^{\ell}\) in (2) is instantiated with these two steps.

### Self-Attention via Denoising Tokens Towards Multiple Subspaces

There are many different ways to optimize the objective (1) incrementally. In this work, we propose arguably _the most basic_ scheme. To help clarify the intuition behind our derivation and approximation, in this section (and Appendix A.1) we study a largely idealized model which nevertheless captures the essence of nearly the whole process and particularly reveals the reason why self-attention-like operators arise in many contexts. Assume that \(N=1\), and the single token \(\bm{x}\) is drawn i.i.d. from an unknown mixture of Gaussians \((\mathcal{N}(\bm{0},\bm{\Sigma}_{k}))_{k=1}^{K}\) supported on low-dimensional subspaces with orthonormal bases \(\bm{U}_{[K]}=(\bm{U}_{k})_{k=1}^{K}\) and corrupted with additive Gaussian noise \(\bm{w}\sim\mathcal{N}(\bm{0},\bm{I})\), i.e.,

\[\bm{x}=\bm{z}+\sigma\bm{w},\] (3)

where \(\bm{z}\) is distributed according to the mixture. Our goal is simply to transform the distribution of the noisy token \(\bm{x}\) to the mixture of low-dimensional Gaussians \(\bm{z}\). Towards incremental construction of a representation \(f\) for this model following (2), we reason inductively: if \(\bm{z}^{\ell}\) is a noisy token (3) at noise level \(\sigma^{\ell}\), it is natural to produce \(\bm{z}^{\ell+1}\) by denoising at the level \(\sigma^{\ell}\). In the mean-square sense, the optimal estimate is \(\mathbb{E}[\bm{z}\mid\bm{z}^{\ell}]\), which has a variational characterization (e.g. [12]):

\[\mathbb{E}[\bm{z}\mid\cdot]=\operatorname*{arg\,min}_{f}\ \mathbb{E}_{\bm{z},\bm{w}}\Big{[}\big{\|}f(\bm{z}+\sigma^{\ell}\bm{w})-\bm{z} \big{\|}_{2}^{2}\Big{]}.\] (4)Setting \(\bm{z}^{\ell+1}=\mathbb{E}[\bm{z}\mid\bm{z}^{\ell}]\), (4) thus characterizes the next stage of (2) in terms of an optimization objective based on a _local signal model_ for \(\bm{z}^{\ell}\). Moreover, letting \(\bm{x}\mapsto q^{\ell}(\bm{x})\) denote the density of \(\bm{z}^{\ell}\), Tweedie's formula [13] allows us to express the optimal representation solving (4) in closed-form:

\[\bm{z}^{\ell+1}=\bm{z}^{\ell}+(\sigma^{\ell})^{2}\nabla_{\bm{x}}\log q^{\ell} (\bm{z}^{\ell}).\] (5)

Tweedie's formula expresses the optimal representation in terms of an additive correction (in general a nonlinear function of \(\bm{z}^{\ell}\)) to the noisy observations by the gradient of the _log-likelihood_ of the distribution of the noisy observations, giving the optimal representation a clear interpretation as an incremental perturbation to the current noisy distribution \(q^{\ell}\). This connection is well-known in the areas of estimation theory and inverse problems [1, 13, 14, 19, 20, 27, 42], and more recently has found powerful applications in the training of generative models for natural images [4, 15, 22, 43, 44]. Here, we can calculate a closed-form expression for this _score function_\(\nabla_{\bm{x}}\log q^{\ell}\), which, when combined with (5) and some technical assumptions4, gives the following approximation (shown in Appendix A.1). Let \(\otimes\) denote the Kronecker product; then we have

Footnote 4: Such as \(\sigma\) being smaller than the nonzero eigenvalues of \(\bm{\Sigma}_{k}\) and the normalization assumption \(\pi_{i}\det(\bm{\Sigma}_{i}+\sigma^{2}\bm{I})^{-1/2}=\pi_{j}\det(\bm{\Sigma}_{ j}+\sigma^{2}\bm{I})^{-1/2}\) for all \(i,j\in[K]\), where \(\pi_{k}\) is the mixture proportion for the \(k^{\rm th}\) Gaussian.

\[\bm{z}^{\ell+1}\approx[\bm{U}_{1},\dots,\bm{U}_{K}]\left[\mathrm{diag}\!\left( \mathrm{softmax}\!\left(\frac{1}{2(\sigma^{\ell})^{2}}\begin{bmatrix}\|\bm{U} _{1}^{*}\bm{z}^{\ell}\|_{2}^{2}\\ \vdots\\ \|\bm{U}_{K}^{*}\bm{z}^{\ell}\|_{2}^{2}\end{bmatrix}\right)\right)\otimes\bm{I} _{p}\right]\begin{bmatrix}\bm{U}_{1}^{*}\bm{z}^{\ell}\\ \vdots\\ \bm{U}_{K}^{*}\bm{z}^{\ell}\end{bmatrix},\] (6)

This operation resembles a self-attention layer in a standard transformer architecture with \(K\) heads, sequence length \(N=1\), the "query-key-value" constructs being replaced by a single linear projection \(\bm{U}_{k}^{*}\bm{z}^{\ell}\) of the token \(\bm{z}^{\ell}\), and the aggregation of head outputs (conventionally modeled by an MLP) done with the two leftmost matrices in (6). We thus derive the following useful interpretation, which we will exploit in the sequel: _Gaussian denoising against a mixture of subspaces model leads to self-attention-type layers in the transformation \(f\)_. Given an initial sample \(\bm{x}\) following the model (3), we can repeatedly apply local transformations to the distribution with (6) in order to realize the incremental mapping \(f\colon\bm{x}\to\bm{z}\) in (2).5 These insights will guide us in the design of our white-box transformer architecture in the upcoming subsections.

Footnote 5: This statement can be made mathematically rigorous by exploiting a deep connection between neural ODEs and diffusion models, following ideas in Song et al. [44] and Chen et al. [72].

### Self-Attention via Compressing Token Sets through Optimizing Rate Reduction

In the last subsection, we have seen that the multi-head attention in a transformer resembles the score-matching operator that aims to transform a token \(\bm{z}^{\ell}\) towards a mixture of subspaces (or degenerate Gaussians). Nevertheless, to carry out such an operation on any data, one needs to first learn or estimate, typically from finite samples, the parameters of the mixture of (degenerate) Gaussians, which is known to be a challenging task [6, 24]. This challenge is made even harder because in a typical learning setting, the given set of tokens are _not_ i.i.d. samples from the mixture of subspaces. The joint distribution among these tokens can encode rich information about the data--for example, co-occurrences between words or object parts in language and image data (resp.)--which we should also learn. Thus, we should compress / denoise / transform such a set of tokens together. To this end, we need a measure of quality, i.e., compactness, for the resulting representation of the set of tokens. A natural measure of the compactness of such a set of tokens is the (lossy) coding rate to encode them up to a certain precision \(\epsilon>0\)[6, 46]. For a zero-mean Gaussian, this measure takes a closed form. If we view the tokens in \(\bm{Z}\in\mathbb{R}^{d\times N}\) as drawn from a single zero-mean Gaussian, an estimate of their (lossy) coding rate, subject to quantization precision \(\epsilon>0\), is given in [6] as:

\[R(\bm{Z})\doteq\frac{1}{2}\log\!\det\!\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm {Z}^{*}\bm{Z}\right)=\frac{1}{2}\log\!\det\!\left(\bm{I}+\frac{d}{N\epsilon^{ 2}}\bm{Z}\bm{Z}^{*}\right)\!.\] (7)

In practice, the data distribution is typically multi-modal, say an image set consisting of many classes or a collection of image patches as in Figure 1. It is more appropriate to require that the set of tokens map to a mixture of, say \(K\), subspaces (degenerate Gaussians) [55]. As before we denote the (to be learned) bases of these subspaces as \(\bm{U}_{[K]}=(\bm{U}_{k})_{k=1}^{K}\), where \(\bm{U}_{k}\in\mathbb{R}^{d\times p}\). Although the joint distribution of the tokens \(\bm{Z}\) is unknown, the desired marginal distribution of each token \(\bm{z}_{i}\) is a mixture of subspaces. So we may obtain an upper bound of the coding rate for the token set \(\bm{Z}\) by projecting its tokens onto these subspaces and summing up the respective coding rates:

\[R^{c}(\bm{Z};\bm{U}_{[K]})=\sum_{k=1}^{K}R(\bm{U}_{k}^{*}\bm{Z})= \frac{1}{2}\sum_{k=1}^{K}\mathrm{logdet}\Big{(}\bm{I}+\frac{p}{N\epsilon^{2}}( \bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})\Big{)}.\] (8)

We would like to compress (or denoise) the set of tokens against these subspaces by minimizing the coding rate. The gradient of \(R^{c}(\bm{Z};\bm{U}_{[K]})\) is

\[\nabla_{\bm{Z}}R^{c}(\bm{Z};\bm{U}_{[K]})=\frac{p}{N\epsilon^{2} }\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{*}\bm{Z}\left(\bm{I}+\frac{p}{N\epsilon^ {2}}(\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})\right)^{-1}.\] (9)

The above expression approximates the residual of each projected token \(\bm{U}_{k}^{*}\bm{z}_{i}\) regressed by other tokens \(\bm{U}_{k}^{*}\bm{z}_{j}\)[55]. But, differently from [55], not all tokens in \(\bm{Z}\) are from the same subspace. Hence, to denoise each token with tokens from its own group, we can compute their similarity through an auto-correlation among the projected tokens as \((\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})\) and convert it to a distribution of membership with a softmax, namely \(\mathrm{softmax}((\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z}))\). Then, as we show in Appendix A.2, if we only use similar tokens to regress and denoise each other, then a gradient step on the coding rate with learning rate \(\kappa\) can be naturally approximated as follows:

\[\bm{Z}^{\ell+1/2}=\bm{Z}^{\ell}-\kappa\nabla_{\bm{Z}}R^{c}(\bm{Z} ^{\ell};\bm{U}_{[K]})\approx\left(1-\kappa\cdot\frac{p}{N\epsilon^{2}}\right) \bm{Z}^{\ell}+\kappa\cdot\frac{p}{N\epsilon^{2}}\cdot\texttt{MSSA}(\bm{Z}^{ \ell}\ \mid\ \bm{U}_{[K]}),\] (10)

where \(\texttt{MSSA}\) is defined through an SSA operator as:

\[\texttt{SSA}(\bm{Z}\mid\bm{U}_{k}) \doteq(\bm{U}_{k}^{*}\bm{Z})\,\mathrm{softmax}((\bm{U}_{k}^{*}\bm {Z})^{*}(\bm{U}_{k}^{*}\bm{Z})),\quad k\in[K],\] (11) \[\texttt{MSSA}(\bm{Z}\mid\bm{U}_{[K]}) \doteq\frac{p}{N\epsilon^{2}}\cdot[\bm{U}_{1},\ldots,\bm{U}_{K}] \begin{bmatrix}\texttt{SSA}(\bm{Z}\mid\bm{U}_{1})\\ \vdots\\ \texttt{SSA}(\bm{Z}\mid\bm{U}_{K})\end{bmatrix}.\] (12)

Here the SSA operator in (11) resembles the _attention operator_ in a typical transformer [28], except that here the linear operators of value, key, and query are all set to be _the same_ as the subspace basis, i.e., \(\bm{V}=\bm{K}=\bm{Q}=\bm{U}_{k}^{*}\).6 Hence, we name \(\texttt{SSA}(\cdot\left|\bm{U}_{k}\right.):\mathbb{R}^{d\times N}\to\mathbb{R }^{p\times N}\) the **S**ubspace **S**elf-**A**ttention (SSA) operator (more details and justification can be found in (72) in Appendix A.2). Then, the whole \(\texttt{MSSA}\) operator in (12), formally defined as \(\texttt{MSSA}(\cdot\left|\bm{U}_{[K]}\right.):\mathbb{R}^{d\times N}\to\mathbb{R }^{d\times N}\) and called the **M**ulti-Head **S**ubspace **S**elf-**A**ttention (MSSA) operator, aggregates the attention head outputs by averaging using model-dependent weights, similar in concept to the popular multi-head self-attention operator in existing transformer networks. The overall gradient step (10) resembles the multi-head self-attention implemented with a skip connection in transformers.

Footnote 6: We note a recent suggestion of Hinton [51] that it is more sensible to set the “value, key, and query” projection matrices in a transformer to be equal. Our derivation in this section confirms this mathematically.

Notice that if we have \(N=1\) tokens as well as take an aggressive gradient step (\(\kappa=1\)) and tune the quantization error (\(\epsilon=\sqrt{p/N}\)), the multi-head subspace self-attention operator in (12) becomes the ideal denoiser defined in (6), with the one minor difference that the aggregation of the heads is done by a linear function here, while in (6) it is done by a nonlinear mixture-of-experts type function.7 This provides two very related interpretations of the multi-head self-attention operator, as denoising and compression against a mixture of low-dimensional subspaces.

Footnote 7: This suggests that we could also consider such a mixture of expert type aggregation of the multiple attention heads. In this work, we use linear aggregation, and leave evaluation of more variants for future work.

### MLP via Iterative Shrinkage-Thresholding Algorithms (ISTA) for Sparse Coding

In the previous subsection, we focused on how to compress a set of tokens against a set of (learned) low-dimensional subspaces. Optimizing the remaining terms in the sparse rate reduction objective (1), including the non-smooth term, serves to sparsify the compressed tokens, hence leading to a more compact and structured (i.e., _parsimonious_) representation. From (1) and (7), this term is

\[\max_{\bm{Z}}\left[R(\bm{Z})-\lambda\|\bm{Z}\|_{0}\right]=\min_{ \bm{Z}}\left[\lambda\|\bm{Z}\|_{0}-\frac{1}{2}\,\mathrm{logdet}\bigg{(}\bm{I}+ \frac{d}{N\epsilon^{2}}\bm{Z}^{*}\bm{Z}\bigg{)}\right],\] (13)where \(R(\bm{Z})\) denotes the coding rate of the whole token set, as defined in (7). In addition to sparsification via the \(\|\bm{Z}\|_{0}\) term, the expansion term \(R(\bm{Z})\) in (13) promotes diversity and non-collapse of the representation, a highly desirable property. However, prior work has struggled to realize this benefit on large-scale datasets due to poor scalability of the gradient \(\nabla_{\bm{Z}}R(\bm{Z})\), which requires a matrix inverse [55].

To simplify things, we therefore take a different approach to trading off between representational diversity and sparsification: we posit a (complete) incoherent or orthogonal dictionary \(\bm{D}\in\mathbb{R}^{d\times d}\), and ask to sparsify the intermediate iterates \(\bm{Z}^{\ell+1/2}\) with respect to \(\bm{D}\). That is, \(\bm{Z}^{\ell+1/2}=\bm{D}\bm{Z}^{\ell+1}\) where \(\bm{Z}^{\ell+1}\) is more sparse. The dictionary \(\bm{D}\) is global, i.e., is used to sparsify all tokens simultaneously.

By the incoherence assumption, we have \(\bm{D}^{*}\bm{D}\approx\bm{I}_{d}\); thus from (7) we have \(R(\bm{Z}^{\ell+1})\approx R(\bm{D}\bm{Z}^{\ell+1})=R(\bm{Z}^{\ell+1/2})\). Thus we approximately solve (13) with the following program:

\[\bm{Z}^{\ell+1}=\operatorname*{arg\,min}_{\bm{Z}}\|\bm{Z}\|_{0}\quad\text{ subject to}\quad\bm{Z}^{\ell+1/2}=\bm{D}\bm{Z}.\] (14)

The above sparse representation program is usually solved by relaxing it to an unconstrained convex program, known as LASSO:

\[\bm{Z}^{\ell+1}=\operatorname*{arg\,min}_{\bm{Z}}\Big{[}\lambda\|\bm{Z}\|_{1 }+\|\bm{Z}^{\ell+1/2}-\bm{D}\bm{Z}\|_{F}^{2}\Big{]}.\] (15)

In our implementation, motivated by Sun et al. [33] and Zarka et al. [35], we also add a non-negative constraint to \(\bm{Z}^{\ell+1}\),

\[\bm{Z}^{\ell+1}=\operatorname*{arg\,min}_{\bm{Z}\geq\bm{0}}\Big{[}\lambda\| \bm{Z}\|_{1}+\|\bm{Z}^{\ell+1/2}-\bm{D}\bm{Z}\|_{F}^{2}\Big{]},\] (16)

which we then incrementally optimize by performing an unrolled proximal gradient descent step, known as an ISTA step [8], to give the update:

\[\bm{Z}^{\ell+1}=\operatorname{ReLU}(\bm{Z}^{\ell+1/2}+\eta\bm{D}^{*}(\bm{Z}^{ \ell+1/2}-\bm{D}\bm{Z}^{\ell+1/2})-\eta\lambda\bm{1})\doteq\texttt{ISTA}(\bm{Z }^{\ell+1/2}\mid\bm{D}).\] (17)

In Appendix A.3, we will show one can arrive at a similar operator to the above ISTA-like update for optimizing (13) by properly linearizing and approximating the rate term \(R(\bm{Z})\).

### The Overall White-Box crate Architecture

By combining the above two steps:

1. (Sections 2.2 and 2.3) Local denoising and compression of tokens within a sample towards a mixture-of-subspace structure, leading to the multi-head subspace self-attention block - MSSA;

Figure 2: One layer of the CRATE architecture. The full architecture is simply a concatenation of such layers, with some initial tokenizer and final task-specific architecture (i.e., a classification head).

2. (Section 2.4) Global compression and sparsification of token sets across all samples through sparse coding, leading to the sparsification block - \(\mathtt{ISTA}\);

we can get the following rate-reduction-based transformer layer, illustrated in Figure 2,

\[\bm{Z}^{\ell+1/2}\doteq\bm{Z}^{\ell}+\mathtt{MSSA}(\bm{Z}^{\ell}\mid\bm{U}^{ \ell}_{[K]}),\qquad\bm{Z}^{\ell+1}\doteq\mathtt{ISTA}(\bm{Z}^{\ell+1/2}\mid\bm {D}^{\ell}).\] (18)

Composing multiple such layers following the incremental construction of our representation in (2), we obtain a white-box transformer architecture that transforms the data tokens towards a compact and sparse union of incoherent subspaces.

This model has the parameters \((\bm{U}^{\ell}_{[K]})^{L}_{\ell=1}\) and \((\bm{D}^{\ell})^{L}_{\ell=1}\), which are learned from data via _back-propagation_. Notably, in each layer \(\ell\), the learned \(\bm{U}^{\ell}_{[K]}\) retain their interpretation as incoherent bases for supporting subspaces for the mixture-of-Gaussians model at layer \(\ell\), and the learned \(\bm{D}^{\ell}\) retains its interpretation as a sparsifying dictionary at layer \(\ell\). We emphasize that the parameters \(\bm{U}^{\ell}_{[K]}\) and \(\bm{D}^{\ell}\) are dependent on the layer \(\ell\) -- that is, we learn a different set of parameters at each layer. This is because at each layer we learn an approximate local parametric model for the input data distribution, then use that learned model to construct the layer operators that transform the distribution. Our procedure of parameterizing the data distribution at each layer distinguishes this work from previous works on unrolled optimization for neural networks such as the ReduNet [55]. Our interpretation clarifies the roles of the network forward pass (given local signal models at each layer, denoise/compress/sparsify the input) and the backward pass (learn the local signal models from data via supervision).

We note that in this work, at each stage of our construction, we have chosen arguably the _simplest possible_ construction to use. We can substitute each part of this construction, so long as the new part maintains the same conceptual role, and obtain another white-box architecture. Nevertheless, our such-constructed architecture, called crate (i.e., Coding RAte TransformEr), connects to existing transformer models, obtains competitive results on real-world datasets, and is fully mathematically interpretable.

## 3 Experiments

In this section, we conduct experiments to study the performance of our proposed white-box transformer crate on real-world datasets and tasks. As the analysis in Section 2 suggests, either the compression or the sparsification step can be achieved through various alternative design choices or strategies. crate arguably adopts the most basic choices and so our goal with the experiments is _not_ simply to compete with other heavily engineered transformers while using such a rudimentary design. Rather, our goals are twofold. First, unlike any empirically designed black-box networks that are usually evaluated only on end-to-end performance, the white-box design of our network allows us to _look inside_ the deep architecture and verify if layers of the learned network indeed perform their design objective--say performing incremental optimization for the objective (1). Second, despite their simplicity, our experiments will actually reveal the vast practical potential of our so-derived crate architectures since, as we will show, they already achieve very strong performance on large-scale real-world datasets and tasks. In the remainder of this section we highlight a selection of results; additional experimental details and results can be found in Appendix B.

Model architecture.We implement the architecture that is described in Section 2.5, with minor modifications that are described in Appendix B.1. We consider different model sizes of crate by varying the token dimension \(d\), number of heads \(K\), and the number of layers \(L\). We consider four model sizes in this work: crate-Tiny, crate-Small, crate-Base, and crate-Large. A PyTorch-style pseudocode can be found in Appendix B.1, which contains more implementation details. For training using supervised classification, we first take the \(\mathtt{CLS}\) token \(\overline{\bm{z}}_{b}=\bm{z}_{1,b}^{L+1}\) of for each sample, then apply a linear layer; the output of this linear layer \(\bm{u}_{b}\doteq\bm{W}\overline{\bm{z}}_{b}\) is used as input to the standard cross-entropy loss. The overall loss averages over all samples \(b\in[B]\).

Datasets and optimization.We mainly consider ImageNet-1K [9] as the testbed for our architecture. Specifically, we apply the Lion optimizer [73] to train crate models with different model sizes. Meanwhile, we also evaluate the transfer learning performance of crate: by considering the models trained on ImageNet-1K as pre-trained models, we fine-tune crate on several commonly used downstream datasets (CIFAR10/100, Oxford Flowers, Oxford-IIT-Pets). More details about the training and datasets can be found in Appendix B.1.

### In-depth Layer-wise Analysis of crate

**Do layers of crate achieve their design goals?** As described in Section 2.3 and Section 2.4, the MSSA block is designed to optimize the compression term \(R^{c}(\bm{Z})\) and the ISTA block to sparsify the token representations (corresponding to the sparsification term \(\|\bm{Z}\|_{0}\)). To understand whether crate indeed optimizes these terms, for each layer \(\ell\), we measure (i) the compression term \(R^{c}(\bm{Z}^{\ell+1/2})\) on the MSSA block outputs \(\bm{Z}^{\ell+1/2}\); and (ii) sparsity \(\|\bm{Z}^{\ell+1}\|_{0}\) on the ISTA block outputs \(\bm{Z}^{\ell+1}\). Specifically, we evaluate these two terms by using training/validation samples from ImageNet-1K. Both terms are evaluated at the per-sample level and averaged over \(B=10^{3}\) samples.

Figure 3 shows the plots of these two key measures at all layers for the learned crate-small model. We find that as the layer index \(\ell\) increases, both the compression and the sparsification terms improve in most cases. The increase in the sparsity measure of the last layer is caused by the extra linear layer for classification.8 These results suggest that crate aligns well with the original design goals: once learned, it essentially learns to gradually compress and sparsity the representations through its layers. In addition, we also measure the compression and sparsification terms on crate models with different model sizes as well as intermediate model checkpoints and the results are shown by plots in Figure 5 of Appendix B.2. The observations are very consistent across all different model sizes--both the compression and sparsification terms improve in most scenarios. Models with more layers tend to optimize the objectives more effectively, confirming our understanding of each layer's roles.

Footnote 8: Note that the learned sparse (tokens) features need to be mixed in the last layer for predicting the class. The phenomenon of increase in the sparsity measure at the last layer suggests that each class of objects may be associated with a number of features, and some of these features are likely to be shared across different classes.

To see the effect of learning, we present the evaluations on crate-Small trained with different number of epochs in Figure 4. When the model is not trained enough (e.g. untrained), the architecture does not optimize the objectives effectively. However, during training--learning better subspaces \(\bm{U}_{[K]}^{\ell}\) and dictionaries \(\bm{D}^{\ell}\)--the designed blocks start to optimize the objectives much more effectively.

Visualizing layer-wise token representations.To gain a better understanding of the token representations of crate, we visualize the output of each ISTA block at layer \(\ell\) in Figure 6 of Appendix B.2. Specifically, we visualize the \(\bm{Z}^{\ell+1}\) via heatmap plots. We observe that the output \(\bm{Z}^{\ell+1}\) becomes more sparse as the layer increases. Moreover, besides the sparsity, we also find that \(\bm{Z}^{\ell+1}\) becomes

Figure 4: The compression term \(R^{c}(\bm{Z})\) (_left_) and sparsification term \(\|\bm{Z}\|_{0}/(d\cdot N)\) (_right_) across models trained with different numbers of epochs. (Model: crate-Base).

Figure 3: _Left_: The compression term \(R^{c}(\bm{Z}^{\ell+1/2})\) of the MSSA outputs at different layers. _Right_: the sparsity of the ISTA output block, \(\|\bm{Z}^{\ell+1}\|_{0}/(d\cdot N)\), at different layers. (Model: crate-Small).

more structured (i.e., low-rank), which indicates that the set of token representations become closer to linear subspaces, confirming our mental picture of the geometry of each layer (as in Figure 1).

Visualizing layer-wise subspaces in multi-head self-attention.We now visualize the \(\bm{U}_{[K]}^{\ell}\) matrices used in the MSSA block. In Section 2.3, we assumed that \(\bm{U}_{[K]}^{\ell}\) were incoherent to capture different "views" of the set of tokens. In Fig. 7 of Appendix B.2, we first normalize the columns in each \(\bm{U}_{k}^{\ell}\), then we visualize the \([\bm{U}_{1}^{\ell},\dots,\bm{U}_{K}^{\ell}]^{{}^{\ast}}[\bm{U}_{1}^{\ell}, \dots,\bm{U}_{K}^{\ell}]\in\mathbb{R}^{pK\times pK}\). The \((i,j)\)-th block in each sub-figure corresponds to \((\bm{U}_{i}^{\ell})^{\ast}\bm{U}_{j}^{\ell}\) for \(i,j\in[K]\) at a particular layer \(\ell\). We find that the learned \(\bm{U}_{[K]}^{\ell}\) are approximately incoherent, which aligns well with our assumptions. One interesting observation is that the \(\bm{U}_{[K]}^{\ell}\) becomes more incoherent when the layer index \(\ell\) is larger, which suggests that the token representations are more separable. This mirrors the situation in other popular deep networks [58].

### Evalutions of crate on Large Real-World Datasets and Tasks

We now study the empirical performance of the proposed networks by measuring their top-1 accuracy on ImageNet-1K as well as transfer learning performance on several widely used downstream datasets. We summarize the results in Table 1. As our designed architecture leverages parameter sharing in both the attention block (MSSA) and the MLP block (ISTA), our crate-Base model (22.08 million) has a similar number of parameters to the ViT-Small (22.05 million).

From Table 1, we find that with a similar number of model parameters, our proposed network achieves similar ImageNet-1K and transfer learning performance as ViT, despite the simplicity and interpretability of our design. Moreover, with the same set of training hyperparameters, we observe promising scaling behavior in crate--we consistently improve the performance by scaling up the model size. For comparison, directly scaling ViT on ImageNet-1K does not always lead to consistent performance improvement measured by top-1 accuracy [40]. To summarize, we achieve promising performance on real-world large-scale datasets by directly implementing our principled architecture.

## 4 Conclusion

In this paper, we propose a new theoretical framework that allows us to derive deep transformer-like network architectures as incremental optimization schemes to learn compressed and sparse representation of the input data (or token sets). The so derived and learned deep architectures are not only fully mathematically interpretable, but also consistent on a layer-by-layer level with their design objective. Despite being arguably the simplest among all possible designs, these networks already demonstrate performance on large-scale real-world datasets and tasks close to seasoned transformers. We believe this work truly helps bridge the gap between theory and practice of deep neural networks as well as help unify seemingly separate approaches to learning and representing data distributions. Probably more importantly for practitioners, our framework provides theoretical guidelines to design and justify new, potentially more powerful, deep architectures for representation learning.

\begin{table}
\begin{tabular}{l c c c c|c c} \hline \hline
**Datasets** & crate-T & crate-S & crate-B & crate-L & ViT-T & ViT-S \\ \hline \hline \# parameters & 6.09M & 13.12M & 22.80M & 77.64M & 5.72M & 22.05M \\ \hline ImageNet & 66.7 & 69.2 & 70.8 & 71.3 & 71.5 & 72.4 \\ ImageNet Real. & 74.0 & 76.0 & 76.5 & 77.4 & 78.3 & 78.4 \\ \hline CIFAR10 & 95.5 & 96.0 & 96.8 & 97.2 & 96.6 & 97.2 \\ CIFAR100 & 78.9 & 81.0 & 82.7 & 83.6 & 81.8 & 83.2 \\ Oxford Flowers-102 & 84.6 & 87.1 & 88.7 & 88.3 & 85.1 & 88.5 \\ Oxford-IIT-Pets & 81.4 & 84.9 & 85.3 & 87.4 & 88.5 & 88.6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Top 1 accuracy of crate on various datasets with different model scales when pre-trained on ImageNet. For ImageNet/ImageNetReaL, we directly evaluate the top-1 accuracy. For other datasets, we use models that are pre-trained on ImageNet as initialization and the evaluate the transfer learning performance via fine-tuning.

## Acknowledgements

We thank the anonymous reviewers for their helpful comments. Yaodong Yu would like to thank Kwan Ho Ryan Chan for the valuable discussions we had regarding visualizing tokens in vision transformers. Yaodong Yu acknowledges support from the joint Simons Foundation-NSF DMS grant #2031899. Yi Ma acknowledges support from ONR grant N00014-22-1-2102 and the joint Simons Foundation-NSF DMS grant #2031899. This work was partially supported by NSF 1704458, the Northrop Grumman Mission Systems Research in Applications for Learning Machines (REALM) initiative, NIH NIA 1R01AG067396, and ARO MURI W911NF-17-1-0304.

## References

* [1] Charles M Stein. "Estimation of the Mean of a Multivariate Normal Distribution". _The Annals of Statistics_ 9.6 (Nov. 1981), pp. 1135-1151.
* [2] Bruno A Olshausen and David J Field. "Sparse coding with an overcomplete basis set: A strategy employed by V1?" _Vision research_ 37.23 (1997), pp. 3311-3325.
* [3] David L Donoho and Carrie Grimes. "Image Manifolds which are Isometric to Euclidean Space". _Journal of mathematical imaging and vision_ 23.1 (July 2005), pp. 5-24.
* [4] Aapo Hyvarinen. "Estimation of Non-Normalized Statistical Models by Score Matching". _Journal of machine learning research: JMLR_ 6.24 (2005), pp. 695-709.
* [5] Michael B Wakin, David L Donoho, Hyeokho Choi, and Richard G Baraniuk. "The multiscale structure of non-differentiable image manifolds". _Wavelets XI_. Vol. 5914. SPIE. 2005, pp. 413-429.
* [6] Yi Ma, Harm Derksen, Wei Hong, and John Wright. "Segmentation of multivariate mixed data via lossy data coding and compression". _PAMI_ (2007).
* [7] Maria-Elena Nilsback and Andrew Zisserman. "Automated flower classification over a large number of classes". _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_. IEEE. 2008, pp. 722-729.
* [8] Amir Beck and Marc Teboulle. "A fast iterative shrinkage-thresholding algorithm for linear inverse problems". _SIAM journal on imaging sciences_ 2.1 (2009), pp. 183-202.
* [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "Imagenet: A large-scale hierarchical image database", _2009 IEEE conference on computer vision and pattern recognition_. Ieee. 2009, pp. 248-255.
* [10] Alex Krizhevsky, Geoffrey Hinton, et al. "Learning multiple layers of features from tiny images" (2009).
* [11] Karol Gregor and Yann LeCun. "Learning fast approximations of sparse coding". _Proceedings of the 27th International Conference on International Conference on Machine Learning_. Omnipress. 2010, pp. 399-406.
* [12] Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harrow Walk. _A Distribution-Free Theory of Nonparametric Regression_. Springer New York, Dec. 2010.
* [13] Bradley Efron. "Tweedie's Formula and Selection Bias". _Journal of the American Statistical Association_ 106.496 (2011), pp. 1602-1614.
* [14] Martin Raphan and Eero P Simoncelli. "Least squares estimation without priors or supervision". _Neural computation_ 23.2 (Feb. 2011), pp. 374-420.
* [15] Pascal Vincent. "A connection between score matching and denoising autoencoders". _Neural computation_ 23.7 (July 2011), pp. 1661-1674.
* [16] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. "Cats and dogs". _2012 IEEE conference on computer vision and pattern recognition_. IEEE. 2012, pp. 3498-3505.
* [17] Daniel A Spielman, Huan Wang, and John Wright. "Exact Recovery of Sparsely-Used Dictionaries" (June 2012). arXiv: 1206.5882 [cs.LG].
* [18] Joan Bruna and Stephane Mallat. "Invariant scattering convolution networks". _IEEE transactions on pattern analysis and machine intelligence_ 35.8 (Aug. 2013), pp. 1872-1886.
* [19] Peyman Milanfar. "A Tour of Modern Image Filtering: New Insights and Methods, Both Practical and Theoretical". _IEEE Signal Processing Magazine_ 30.1 (Jan. 2013), pp. 106-128.

* [20] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. "Plug-and-Play priors for model based reconstruction". _2013 IEEE Global Conference on Signal and Information Processing_. Dec. 2013, pp. 945-948. 5.
* [21] Remi Gribonval, Rodolphe Jenatton, and Francis Bach. "Sparse and spurious: dictionary learning with noise and outliers" (July 2014). arXiv: 1407.5155 [cs.LG].
* [22] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" (Mar. 2015). arXiv: 1503.03585 [cs.LG].
* [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep Residual Learning for Image Recognition". _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. June 2016, pp. 770-778. 1, 35.
* [24] Rene Vidal, Yi Ma, and Shankar Sastry. _Generalized Principal Component Analysis_. Springer Verlag, 2016.
* [25] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. "Mask R-CNN" (Mar. 2017). arXiv: 1703.06870 [cs.CV].
* [26] Ilya Loshchilov and Frank Hutter. "Decoupled weight decay regularization". _arXiv preprint arXiv:1711.05101_ (2017). 26.
* [27] Yaniv Romano, Michael Elad, and Peyman Milanfar. "The Little Engine That Could: Regularization by Denoising (RED)". _SIAM journal on imaging sciences_ 10.4 (Jan. 2017), pp. 1804-1844. 5.
* [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. "Attention is all you need". _Advances in neural information processing systems_ 30 (2017). 1-3, 6.
* [29] Yubei Chen, Dylan Paiton, and Bruno Olshausen. "The sparse manifold transform". _Advances in neural information processing systems_ 31 (2018). 2.
* [30] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding". _arXiv preprint arXiv:1810.04805_ (2018). 1.
* [31] Tero Karras, Samuli Laine, and Timo Aila. "A Style-Based Generator Architecture for Generative Adversarial Networks" (Dec. 2018). arXiv: 1812.04948 [cs.NE].
* [32] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. "Theoretical Foundations of Deep Learning via Sparse Representations: A Multilayer Sparse Model and Its Connection to Convolutional Neural Networks". _IEEE Signal Processing Magazine_ 35.4 (July 2018), pp. 72-89. 2.
* [33] Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. "Supervised deep sparse coding networks". _2018 25th IEEE International Conference on Image Processing (ICIP)_. IEEE. 2018, pp. 346-350. 7.
* [34] Yang Song and Stefano Ermon. "Generative Modeling by Estimating Gradients of the Data Distribution" (July 2019). arXiv: 1907.05600 [cs.LG].
* [35] John Zarka, Louis Thiry, Tomas Angles, and Stephane Mallat. "Deep network classification by scattering and homotopy dictionary learning". _arXiv preprint arXiv:1910.03561_ (2019). 7.
* [36] Lucas Beyer, Olivier J Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. "Are we done with imagenet?" _arXiv preprint arXiv:2006.07159_ (2020). 26.
* [37] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. "Language models are few-shot learners". _Advances in neural information processing systems_ 33 (2020), pp. 1877-1901. 1.
* [38] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. "End-to-End Object Detection with Transformers" (May 2020). arXiv: 2005.12872 [cs.CV].
* [39] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. "A Simple Framework for Contrastive Learning of Visual Representations". _Proceedings of the 37th International Conference on Machine Learning_. Ed. by Hal Daume Iii and Aarti Singh. Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 1597-1607. 1.

* [40] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. "An image is worth 16x16 words: Transformers for image recognition at scale". _arXiv preprint arXiv:2010.11929_ (2020).
* [41] Jonathan Ho, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models". _Advances in Neural Information Processing Systems_ 33 (2020), pp. 6840-6851.
* [42] Zahra Kadkhodaie and Eero P Simoncelli. "Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser" (July 2020). arXiv: 2007.13640 [cs.CV].
* [43] Jiaming Song, Chenlin Meng, and Stefano Ermon. "Denoising Diffusion Implicit Models" (Oct. 2020). arXiv: 2010.02502 [cs.LG].
* [44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. "Score-Based Generative Modeling through Stochastic Differential Equations" (Nov. 2020). arXiv: 2011.13456 [cs.LG].
* [45] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. "What makes for good views for contrastive learning?" _Advances in neural information processing systems_ 33 (2020), pp. 6827-6839.
* [46] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. "Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction". _Advances in Neural Information Processing Systems_ 33 (2020), pp. 9422-9434.
* [47] Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. "Complete dictionary learning via 1 4-norm maximization over the orthogonal group". _The Journal of Machine Learning Research_ 21.1 (2020), pp. 6622-6689.
* [48] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. "Vivit: A video vision transformer". _Proceedings of the IEEE/CVF international conference on computer vision_. 2021, pp. 6836-6846.
* [49] Florentin Guth, John Zarka, and Stephane Mallat. "Phase collapse in neural networks". _arXiv preprint arXiv:2110.05283_ (2021).
* [50] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. "Masked Autoencoders Are Scalable Vision Learners" (Nov. 2021). arXiv: 2111.06377 [cs.CV].
* [51] Geoffrey Hinton. _How to represent part-whole hierarchies in a neural network_. 2021. arXiv: 2102.12627 [cs.CV].
* [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. "Learning Transferable Visual Models From Natural Language Supervision". _Proceedings of the 38th International Conference on Machine Learning_. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 8748-8763.
* [53] Bahareh Tolooshams and Demba Ba. "Stable and Interpretable Unrolled Dictionary Learning". _arXiv preprint arXiv:2106.00058_ (2021).
* [54] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. "MLP-Mixer: An all-MLP Architecture for Vision" (May 2021). arXiv: 2105.01601 [cs.CV].
* [55] Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction". _Journal of Machine Learning Research_ 23.114 (2022), pp. 1-103.
* [56] Hongrui Chen, Holden Lee, and Jianfeng Lu. "Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions". _arXiv preprint arXiv:2211.01916_ (2022).
* [57] Yuan Gong, Andrew Rouditchenko, Alexander H Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, and James R Glass. "Contrastive audio-visual masked autoencoder". _The Eleventh International Conference on Learning Representations_. 2022.
* [58] Hangfeng He and Weijie J Su. "A law of data separation in deep learning". _arXiv preprint arXiv:2210.17020_ (2022).

* [59] Geoffrey Hinton. _The Forward-Forward Algorithm: Some Preliminary Investigations_. 2022. arXiv: 2212.13345 [cs.LG].
* [60] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. "Elucidating the design space of diffusion-based generative models". _arXiv preprint arXiv:2206.00364_ (2022).
* [61] Frederic Koehler, Alexander Heckett, and Andrej Risteski. "Statistical Efficiency of Score Matching: The View from Isoperimetry" (Oct. 2022). arXiv: 2210.00726 [cs.LG].
* [62] Yi Ma, Doris Tsao, and Heung-Yeung Shum. "On the principles of parsimony and self-consistency for the emergence of intelligence". _Frontiers of Information Technology & Electronic Engineering_ 23.9 (2022), pp. 1298-1323.
* [63] Druv Pai, Michael Psenka, Chih-Yuan Chiu, Manxi Wu, Edgar Dobriban, and Yi Ma. "Pursuit of a discriminative representation for multiple subspaces via sequential games". _arXiv preprint arXiv:2206.09120_ (2022).
* [64] Mary Phuong and Marcus Hutter. "Formal algorithms for transformers". _arXiv preprint arXiv:2207.09238_ (2022).
* [65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. "High-resolution image synthesis with latent diffusion models". _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022, pp. 10684-10695.
* [66] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding" (May 2022). arXiv: 2205.11487 [cs.CV].
* [67] Asher Trockman, Devin Willmott, and J Zico Kolter. "Understanding the Covariance Structure of Convolutional Filters" (Oct. 2022). arXiv: 2210.03651 [cs.CV].
* [68] Rene Vidal. _Attention: Self-Expression Is All You Need_. Unpublished; available: https://openreview.net/forum?id=MmujBClawFo.
* [69] Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. "Rethinking minimal sufficient representation in contrastive learning". _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022, pp. 16041-16050.
* [70] John Wright and Yi Ma. _High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications_. Cambridge University Press, 2022.
* [71] Yongyi Yang, Zengfeng Huang, and David P Wipf. "Transformers from an Optimization Perspective". _Advances in Neural Information Processing Systems_. Ed. by S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, and A Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 36958-36971.
* [72] Sitan Chen, Giannis Daras, and Alexandros G Dimakis. "Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers" (Mar. 2023). arXiv: 2303.03384 [cs.LG].
* [73] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. "Symbolic discovery of optimization algorithms". _arXiv preprint arXiv:2302.06675_ (2023).
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. June 2023, pp. 1-5.
* [75] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. "Scaling vision transformers to 22 billion parameters". _arXiv preprint arXiv:2302.05442_ (2023).
* [76] Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Horng Chau, Mohammed J Zaki, and Dmitry Krotov. "Energy Transformer" (Feb. 2023). arXiv: 2302.07253 [cs.LG].
* [77] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. "Segment Anything" (Apr. 2023). arXiv: 2304.02643 [cs.CV].

* [78] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. "A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity". _arXiv preprint arXiv:2302.06015_ (2023).
* [79] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar. "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers". _The Eleventh International Conference on Learning Representations_. 2023.
* [80] Ravid Shwartz-Ziv and Yann LeCun. "To Compress or Not to Compress-Self-Supervised Learning and Information Theory: A Review". _arXiv preprint arXiv:2304.09355_ (2023).
* [81] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. "Consistency models". _arXiv preprint arXiv:2303.01469_ (2023).

**Appendix**

## Appendix A Technical Details from Section 2

### Companion to Section 2.2

We first wish to re-iterate the core contributions of our approach in Section 2.2 at a slightly more technical level. Connections between denoising and score matching are well-understood [60], and computing the optimal denoising function (i.e., the conditional expectation) against a mixture-of-Gaussian model is a rather simple computation giving existing tools such as Tweedie's formula [13]. These are not our main contributions. Instead, the main contributions of Section 2.2 are two-fold:

* First, we demonstrate a mechanism to learn representations via denoising within a idealized mixture of Gaussian data model for a single token (i.e., with sequence length \(N=1\)).
* Second, we illustrate the similarities between a such-derived representation learning scheme and existing self-attention layers within the transformer (with sequence length \(1\)), thus demonstrating an interpretation of the self-attention layer as a generalized mechanism to denoise against a mixture-of-Gaussian-marginal model for a set of tokens.

Now we produce the proofs alluded to in Section 2.2, which mostly form the technical aspects of the first listed contribution. To simplify the proofs, we use the following notation correspondences: \(\bm{x}\mapsto\bm{z}^{\ell}\), \(\bm{z}\mapsto\bm{z}^{\ell+1}\), and \(\sigma\mapsto\sigma^{\ell}\).

**Proposition 1**.: _Let \(\bm{u}_{1},\ldots,\bm{u}_{K}\in\mathbb{R}^{d}\) be independent and have distribution \(\bm{u}_{k}\sim\mathcal{N}(\bm{0},\bm{\Sigma}_{k})\) for \(\bm{\Sigma}_{k}\succeq 0\), and let \(\bm{z}\) take value \(\bm{u}_{k}\) with probability \(\pi_{k}>0\). Let \(\bm{w}\sim\mathcal{N}(\bm{0},\bm{I}_{d})\) be independent of \(\bm{z}\). Let \(\bm{x}\doteq\bm{z}+\sigma\bm{w}\). Let \(\bm{x}\mapsto q(\bm{x})\) be the density of \(\bm{x}\). We define_

\[\bm{M}_{k}\doteq(\bm{\Sigma}_{k}+\sigma^{2}\bm{I}_{d})^{-1/2}\] (19)

_and assume that \(\pi_{i}\det(\bm{M}_{i})=\pi_{j}\det(\bm{M}_{j})\) for all \(1\leq i\leq j\leq K\). Then we have_

\[\nabla_{\bm{x}}\log q(\bm{x})\] (20) \[=-\left[\bm{M}_{1},\cdots,\bm{M}_{K}\right]\left[\operatorname{ diag}\!\left(\operatorname{softmax}\!\left(-\frac{1}{2}\begin{bmatrix}\|\bm{M}_{1}^{*} \bm{x}\|_{2}^{2}\\ \vdots\\ \|\bm{M}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\right)\otimes\bm{I}_{d} \right]\begin{bmatrix}\bm{M}_{1}^{*}\bm{x}\\ \vdots\\ \bm{M}_{K}^{*}\bm{x}\end{bmatrix},\] (21)

_where \(\otimes\) denotes the Kronecker product, i.e., the block matrix defined by_

\[\bm{A}\otimes\bm{B}=\begin{bmatrix}A_{11}\bm{B}&\cdots&A_{1n}\bm{B}\\ \vdots&\ddots&\vdots\\ A_{m1}\bm{B}&\cdots&A_{mn}\bm{B}\end{bmatrix}\] (22)

Proof.: Let \(u\) be the multinomial random variable such that \(\bm{z}=\bm{z}_{u}\), so that \(u\) has probability mass function \(\pi\). Then by the law of total probability, we have

\[\nabla_{\bm{x}}\log q(\bm{x}) =\nabla_{\bm{x}}\log\sum_{k=1}^{K}q(\bm{x}\mid k)\pi_{k}\] (23) \[=\frac{\sum_{k=1}^{K}\pi_{k}\nabla_{\bm{x}}q(\bm{x}\mid k)}{\sum_ {k=1}^{K}q(\bm{x}\mid k)\pi_{k}}\] (24)

where \(q(\bm{x}\mid k)\) is the conditional density of \(\bm{x}\) given the event \(\{u=k\}\). To compute this quantity, note that _conditional on the value of \(u\)_, we have

\[\bm{x}=\bm{z}_{u}+\sigma\bm{w}\sim\mathcal{N}(\bm{0},\bm{\Sigma}_{u}+\sigma^{2 }\bm{I}_{d}).\] (25)

Thus we have

\[q(\bm{x}\mid k)=\frac{1}{\sqrt{(2\pi)^{d}\det(\bm{\Sigma}_{k}+\sigma^{2}\bm{ I}_{d})}}\exp\!\left(-\frac{1}{2}\bm{x}^{*}(\bm{\Sigma}_{k}+\sigma^{2}\bm{I}_{d})^{- 1}\bm{x}\right)\!,\] (26)

This gives

\[\nabla_{\bm{x}}q(\bm{x}\mid k)=-q(\bm{x}\mid k)\cdot(\bm{\Sigma}_{k}+\sigma^{2 }\bm{I}_{d})^{-1}\bm{x}.\] (27)Putting this all together, we get

\[\nabla_{\bm{x}}\log q(\bm{x})\] (28) \[=-\frac{\sum_{k=1}^{K}q(\bm{x}\mid k)\pi_{k}\cdot(\bm{\Sigma}_{k}+ \sigma^{2}\bm{I}_{d})^{-1}\bm{x}}{\sum_{k=1}^{K}q(\bm{x}\mid k)\pi_{k}}\] (29) \[=-\frac{\sum_{k=1}^{K}\pi_{k}\det(\bm{\Sigma}_{k}+\sigma^{2}\bm{ I}_{d})^{-1/2}\exp\bigl{(}-\tfrac{1}{2}\bm{x}^{*}(\bm{\Sigma}_{k}+\sigma^{2} \bm{I}_{d})^{-1}\bm{x}\bigr{)}\cdot(\bm{\Sigma}_{k}+\sigma^{2}\bm{I}_{d})^{-1} \bm{x}}{\sum_{k=1}^{K}\pi_{k}\det(\bm{\Sigma}_{k}+\sigma^{2}\bm{I}_{d})^{-1/2} \exp\bigl{(}-\tfrac{1}{2}\bm{x}^{*}(\bm{\Sigma}_{k}+\sigma^{2}\bm{I}_{d})^{-1} \bm{x}\bigr{)}}.\] (30)

Now define \(\bm{M}_{k}\doteq(\bm{\Sigma}_{k}+\sigma^{2}\bm{I}_{d})^{-1/2}\). With this notation, we have

\[\nabla_{\bm{x}}\log q(\bm{x}) =-\frac{\sum_{k=1}^{K}\pi_{k}\det(\bm{M}_{k})\exp\bigl{(}-\tfrac{ 1}{2}\bm{x}^{*}\bm{M}_{k}\bm{M}_{k}^{*}\bm{x}\bigr{)}\cdot\bm{M}_{k}\bm{M}_{k}^ {*}\bm{x}}{\sum_{k=1}^{K}\pi_{k}\det(\bm{M}_{k})\exp\bigl{(}-\tfrac{1}{2}\bm{ x}^{*}\bm{M}_{k}\bm{M}_{k}^{*}\bm{x}\bigr{)}}\] (31) \[=-\frac{\sum_{k=1}^{K}\pi_{k}\det(\bm{M}_{k})\exp\bigl{(}-\tfrac{1 }{2}\|\bm{M}_{k}^{*}\bm{x}\|_{2}^{2}\bigr{)}\cdot\bm{M}_{k}\bm{M}_{k}^{*}\bm{x }}{\sum_{k=1}^{K}\pi_{k}\det(\bm{M}_{k})\exp\bigl{(}-\tfrac{1}{2}\bm{x}^{*}\bm{ M}_{k}\bm{M}_{k}^{*}\bm{x}\bigr{)}}.\] (32)

Given our assumption that each \(\pi_{k}\det(\bm{M}_{k})\) is the same, we have

\[\nabla_{\bm{x}}\log q(\bm{x})\] (33) \[=-\frac{\sum_{k=1}^{K}\pi_{k}\det(\bm{M}_{k})\exp\bigl{(}-\tfrac{ 1}{2}\|\bm{M}_{k}^{*}\bm{x}\|_{2}^{2}\bigr{)}\cdot\bm{M}_{k}\bm{M}_{k}^{*}\bm {x}}{\sum_{k=1}^{K}\exp\bigl{(}-\tfrac{1}{2}\|\bm{M}_{k}^{*}\bm{x}\|_{2}^{2} \bigr{)}}\] (34) \[=-\frac{\sum_{k=1}^{K}\exp\bigl{(}-\tfrac{1}{2}\|\bm{M}_{k}^{*}\bm {x}\|_{2}^{2}\bigr{)}\cdot\bm{M}_{k}\bm{M}_{k}^{*}\bm{x}}{\sum_{k=1}^{K}\exp \bigl{(}-\tfrac{1}{2}\|\bm{M}_{k}^{*}\bm{x}\|_{2}^{2}\bigr{)}}\] (35) \[=-\sum_{k=1}^{K}\bm{e}_{k}^{*}\operatorname{softmax}\!\left(- \frac{1}{2}\begin{bmatrix}\|\bm{M}_{1}^{*}\bm{x}\|_{2}^{2}\\ \vdots\\ \|\bm{M}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\!\bm{M}_{k}\bm{M}_{k}^{*} \bm{x}\] (36) \[=-\left[\bm{M}_{1},\dots,\bm{M}_{K}\right]\left[\operatorname{ diag}\!\left(\operatorname{softmax}\!\left(-\frac{1}{2}\begin{bmatrix}\|\bm{M}_{1}^{*}\bm{x} \|_{2}^{2}\\ \vdots\\ \|\bm{M}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\right)\otimes\bm{I}_{d} \right]\begin{bmatrix}\bm{M}_{1}^{*}\bm{x}\\ \vdots\\ \bm{M}_{K}^{*}\bm{x}\end{bmatrix}.\] (37)

Now we provide a final justification for the result cited in Section 2.2.

**Approximation 2**.: _In the setting of Proposition 1, diagonalize \(\bm{\Sigma}_{k}=\bm{U}_{k}\bm{\Lambda}_{k}\bm{U}_{k}^{*}\) where \(\bm{U}_{k}\in\mathbb{R}^{d\times p}\) is orthogonal and \(\bm{\Lambda}_{k}\succ\bm{0}\in\mathbb{R}^{p\times p}\) is diagonal.9 Then we have the approximation_

Footnote 9: This assumption can be easily relaxed to \(\bm{\Lambda}_{k}\succeq\bm{0}\) for all \(k\), but requires some more notation to handle, and the form of the solution does not change. Thus we handle the case where all matrices are full rank for simplicity.

\[\mathbb{E}[\bm{z}\mid\bm{x}]\approx[\bm{U}_{1},\dots,\bm{U}_{K}] \left[\operatorname{diag}\!\left(\operatorname{softmax}\!\left(\frac{1}{2 \sigma^{2}}\begin{bmatrix}\|\bm{U}_{1}^{*}\bm{x}\|_{2}^{2}\\ \vdots\\ \|\bm{U}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\right)\otimes\bm{I}_{p} \right]\begin{bmatrix}\bm{U}_{1}^{*}\bm{x}\\ \vdots\\ \bm{U}_{K}^{*}\bm{x}\end{bmatrix}.\] (38)

Proof.: We have

\[\nabla_{\bm{x}}\log q(\bm{x}) =-\sum_{k=1}^{K}\bm{e}_{k}^{*}\operatorname{softmax}\!\left(- \frac{1}{2}\begin{bmatrix}\|\bm{M}_{1}^{*}\bm{x}\|_{2}^{2}\\ \vdots\\ \|\bm{M}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\!\bm{M}_{k}\bm{M}_{k}^{*} \bm{x}\] (39) \[=-\sum_{k=1}^{K}\bm{e}_{k}^{*}\operatorname{softmax}\!\left(-\frac{ 1}{2\sigma^{2}}\begin{bmatrix}\|\sigma\bm{M}_{1}^{*}\bm{x}\|_{2}^{2}\\ \vdots\\ \|\sigma\bm{M}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\!\bm{M}_{k}\bm{M}_{k}^{*} \bm{x}\] (40)

[MISSING_PAGE_FAIL:18]

\[=-\frac{1}{\sigma^{2}}\bm{x}+\frac{1}{\sigma^{2}}\left[\bm{U}_{1}, \cdots,\bm{U}_{K}\right]\left[\mathrm{diag}\!\left(\mathrm{softmax}\!\left(\frac{1 }{2\sigma^{2}}\begin{bmatrix}\|\bm{U}_{1}^{*}\bm{x}\|_{2}^{2}\\ \vdots\\ \|\bm{U}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\right)\otimes\bm{I}_{p} \right]\begin{bmatrix}\bm{U}_{1}^{*}\bm{x}\\ \vdots\\ \bm{U}_{K}^{*}\bm{x}\end{bmatrix}.\] (59)

Plugging this into Tweedie's formula, we have

\[\mathbb{E}[\bm{z}\mid\bm{x}]\approx[\bm{U}_{1},\cdots,\bm{U}_{K}]\left[ \mathrm{diag}\!\left(\mathrm{softmax}\!\left(\frac{1}{2\sigma^{2}}\begin{bmatrix} \|\bm{U}_{1}^{*}\bm{x}\|_{2}^{2}\\ \vdots\\ \|\bm{U}_{K}^{*}\bm{x}\|_{2}^{2}\end{bmatrix}\right)\right)\otimes\bm{I}_{p} \right]\begin{bmatrix}\bm{U}_{1}^{*}\bm{x}\\ \vdots\\ \bm{U}_{K}^{*}\bm{x}\end{bmatrix}.\] (60)

_Remark 3_.: Although Approximation 2 is stated as an approximation rather than as a proposition, we believe it should be possible without too much extra work to convert it into a statement of asymptotic equivalence as \(\sigma\to 0\) (in particular, holding for \(\sigma\) below the smallest (nonzero) eigenvalue of any \(\bm{\Sigma}_{k}\). Most approximations taken in the derivation of Approximation 2 can immediately be turned into asymptotic claims; the only slightly delicate point is treating the softmax, which can be accomplished using standard "high temperature" convergence behavior of the softmax function (in particular, as \(\sigma\to 0\) in our expressions, the softmax concentrates on the "best head").

### Companion to Section 2.3

We again wish to re-iterate the core contribution of our approach in Section 2.3. The application of a compression perspective to representation learning has been discussed before, for example in the line of maximal coding rate reduction works [46]. In Section 2.3, we provide the following contributions and developments to this perspective:

* We propose a generalized coding rate function \(R^{c}(\cdot;\bm{U}_{[K]})\) which measures the coding rate with respect to a set of subspaces \(\bm{U}_{[K]}\) as opposed to a set of classes (as in [46, 55]), making the underlying formulation unsupervised.
* We then show how if we adopt the framework of alternating minimization of the sparse rate reduction objective, then unrolling the first alternating step -- gradient descent on this coding rate objective -- nearly exactly recovers the common multi-head attention mechanism found in transformer networks (except that the query/key/value operators are all the same operation \(\bm{U}_{k}^{*}\) now, which we interpret as projection onto a single subspace).

In the process of the second contribution, and in the following proofs, we make some simple approximations and technical assumptions. The validity of these assumptions may be explored, and the approximations refined, altogether providing a more complex (and possibly more performant) resulting self-attention like operator. For the sake of technical clarity and simplicity in this work, we make perhaps the _simplest possible choices_. As a result, we _do not_ claim that our network is optimally designed, but rather that the principles we develop in this work (compression, denoising, sparsification, unrolled optimization) can provide the backbone for far superior and more interpretable network architectures in the future on sundry tasks. As it is, with our straightforward, simple, and interpretable design, we still obtain meaningful conceptual results and very solid empirical performance.

We now give the derivation of the approximation alluded to in Section 2.3.

**Approximation 4**.: _Let \(\bm{Z}\in\mathbb{R}^{d\times N}\) have unit-norm columns, and \(\bm{U}_{[K]}=(\bm{U}_{1},\dots,\bm{U}_{K})\) such that each \(\bm{U}_{k}\in\mathbb{R}^{d\times p}\) is an orthogonal matrix, the \((\bm{U}_{k})_{k=1}^{K}\) are incoherent, and the columns of \(\bm{Z}\) approximately lie on \(\bigcup_{k=1}^{K}\mathrm{Span}(\bm{U}_{k})\). Let \(\gamma=\frac{p}{N\epsilon^{2}}\). Let \(\kappa>0\). Then_

\[\bm{Z}-\kappa\nabla_{\bm{Z}}R^{c}(\bm{Z}\mid\bm{U}_{[K]})\approx(1-\kappa \gamma)\bm{Z}+\kappa\gamma\,\texttt{MSSA}(\bm{Z}|\bm{U}_{[K]}),\] (61)

_where as in Section 2.3 we have_

\[\texttt{SSA}(\bm{Z}|\bm{U}_{k}) =(\bm{U}_{k}^{*}\bm{Z})\,\mathrm{softmax}((\bm{U}_{k}^{*}\bm{Z})^ {*}(\bm{U}_{k}^{*}\bm{Z})),\] (62) \[\texttt{MSSA}(\bm{Z}|\bm{U}_{[K]}) =\gamma\left[\bm{U}_{1},\dots,\bm{U}_{K}\right]\begin{bmatrix} \texttt{SSA}(\bm{Z}|\bm{U}_{1})\\ \vdots\\ \texttt{SSA}(\bm{Z}|\bm{U}_{K})\end{bmatrix},\] (63)_where \(\operatorname{softmax}(\cdot)\) is the softmax operator (applied to each column of an input matrix), i.e.,_

\[\operatorname{softmax}(\bm{v}) =\frac{1}{\sum_{i=1}^{n}e^{v_{i}}}\begin{bmatrix}e^{v_{1}}\\ \vdots\\ e^{v_{n}}\end{bmatrix},\] (64) \[\operatorname{softmax}([\bm{v}_{1},\dots,\bm{v}_{K}]) =\left[\operatorname{softmax}(\bm{v}_{1}),\dots,\operatorname{ softmax}(\bm{v}_{K})\right].\] (65)

Proof.: According to (9), the gradient \(\nabla_{\bm{Z}}R^{c}(\bm{Z};\bm{U}_{[K]})\) is

\[\nabla_{\bm{Z}}R^{c}(\bm{Z};\bm{U}_{[K]})=\gamma\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_ {k}^{*}\bm{Z}\left(\bm{I}+\gamma(\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z })\right)^{-1}.\] (66)

Notice that according to [55], the gradient is precisely the residual of a ridge regression for each (projected) token \(\bm{U}_{k}^{*}\bm{z}_{i}\) using other projected tokens \(\bm{U}_{k}^{*}\bm{z}_{j}\) as the regressors, hence being the residual of an auto-regression.

However, as we have seen in the work of ReduNet [55], computing the inverse \(\left(\bm{I}+\gamma(\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})\right)^{-1}\) can be expensive. Hence for computational efficiency, we may approximate it with the first order term of its von Neumann expansion:

\[\nabla_{\bm{Z}}R^{c}(\bm{Z};\bm{U}_{[K]}) =\gamma\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{*}\bm{Z}\Big{(}\bm{I} +\gamma(\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})\Big{)}^{-1}\] (67) \[\approx\gamma\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{*}\bm{Z}\Big{(} \bm{I}-\gamma(\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})\Big{)}\] (68) \[=\gamma\sum_{k=1}^{K}\bm{U}_{k}\Big{(}\bm{U}_{k}^{*}\bm{Z}-\gamma \bm{U}_{k}^{*}\bm{Z}[(\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})]\Big{)}\] (69)

Notice that the term \((\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})\) is the auto-correlation among the projected tokens. As the tokens \(\bm{Z}\) may be from different subspaces, we would prefer to use only tokens that belong to the _same_ subspace to regress and compress themselves. Hence we may convert the above correlation term into a subspace-membership indicator with a softmax operation, whence (69) becomes

\[\nabla_{\bm{Z}}R^{c}(\bm{Z};\bm{U}_{[K]}) \approx \gamma\sum_{k=1}^{K}\bm{U}_{k}\Big{(}\bm{U}_{k}^{*}\bm{Z}-\gamma \bm{U}_{k}^{*}\bm{Z}[(\bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z})]\Big{)}\] (70) \[\approx \gamma\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{*}\bm{Z}-\gamma^{2} \sum_{k=1}^{K}\bm{U}_{k}\Big{(}\bm{U}_{k}^{*}\bm{Z}\operatorname{softmax}(( \bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z}))\Big{)}\] (71)

Then, we can rewrite the above approximation to the gradient of \(R^{c}\) as:

\[\nabla_{\bm{Z}}R^{c}(\bm{Z};\bm{U}_{[K]}) \approx\gamma\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{*}\bm{Z}-\gamma^ {2}\sum_{k=1}^{K}\bm{U}_{k}\left(\bm{U}_{k}^{*}\bm{Z}\operatorname{softmax}(( \bm{U}_{k}^{*}\bm{Z})^{*}(\bm{U}_{k}^{*}\bm{Z}))\right)\] (72) \[=\gamma\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{*}\bm{Z}-\gamma^{2} \sum_{k=1}^{K}\bm{U}_{k}\operatorname{\texttt{SSA}}(\bm{Z}\ \ |\ \bm{U}_{k})\] (73) \[=\underbrace{\left(\gamma\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{*} \right)\bm{Z}}_{\approx\gamma\bm{Z}}-\gamma^{2}\left[\bm{U}_{1},\cdots,\bm{U} _{K}\right]\begin{bmatrix}\operatorname{\texttt{SSA}}(\bm{Z}\ |\ \bm{U}_{1})\\ \vdots\\ \operatorname{\texttt{SSA}}(\bm{Z}\ |\ \bm{U}_{K})\end{bmatrix}\] (74) \[\approx\gamma\bm{Z}-\gamma^{2}\left[\bm{U}_{1},\cdots,\bm{U}_{K} \right]\begin{bmatrix}\operatorname{\texttt{SSA}}(\bm{Z}\ |\ \bm{U}_{1})\\ \vdots\\ \operatorname{\texttt{SSA}}(\bm{Z}\ |\ \bm{U}_{K})\end{bmatrix}.\] (75)Thus the gradient descent step with learning rate \(\kappa>0\) gives

\[\bm{Z}-\kappa\nabla_{\bm{Z}}R^{c}(\bm{Z}\mid\bm{U}_{[K]})\approx(1-\kappa\gamma) \bm{Z}+\kappa\gamma^{2}\left[\bm{U}_{1},\dots,\bm{U}_{K}\right]\begin{bmatrix} \mathsf{SSA}(\bm{Z}|\bm{U}_{1})\\ \vdots\\ \mathsf{SSA}(\bm{Z}|\bm{U}_{K})\end{bmatrix}.\] (76)

### Companion to Section 2.4

We again wish to re-iterate the core contribution of our approach in Section 2.4.

* Within the framework of alternating minimization of the sparse rate reduction objective, we show that the second alternating step -- gradient descent on the overall coding rate plus a sparse regularization term -- has heuristic connections to a particular LASSO optimization.
* We show that the unrolling of the proximal gradient step to solve this LASSO optimization resembles the MLP which immediately follows the self-attention layer within transformer blocks.

In the main text, our connection between the second step of the alternating minimization and the LASSO optimization was high-level and heuristic. In some sense, the choice to pose the minimization step as a LASSO was a _simple, reliable, and interpretable choice_ which works well in practice, but is nonetheless not backed up by rigorous theoretical justification. In the following subsection, we provide a mathematical justification for a reformulation of the minimization step using a majorization-minimization framework. We further show that the associated unrolled optimization step bears a strong resemblance to the ISTA step. This confirms our earlier discussion -- we took the _simplest possible choice_ in designing crate, but by more rigorous derivation we can uncover alternative operators which nonetheless have the same conceptual function and may perform better in practice.

Assumptions.In this section, we present a rigorous optimization analysis of an incremental minimization approach to the objective (13). We will show that under two simplifying assumptions, namely

1. The columns of \(\bm{Z}^{\ell+1/2}\) are normalized, in the sense that \(\operatorname{diag}((\bm{Z}^{\ell+1/2})^{*}\bm{Z}^{\ell+1/2})=\bm{1}\);10 Footnote 10: This is a natural assumption in transformer-type architectures such as crate due to the use of layerNorm blocks—although these blocks (indeed, as we use them in crate) include trainable mean and scale offsets as well as an additional mean subtraction operation [64], they are initialized to have zero mean and unit norm, hence this assumption corresponds to an analysis of the network at its initialization.
2. We have \(d\geq N\),11 and the columns of \(\bm{Z}^{\ell+1/2}\) are orthogonal, so that \((\bm{Z}^{\ell+1/2})^{*}\bm{Z}^{\ell+1/2}=\bm{I}\).12 Footnote 11: This assumption is without loss of generality, as we will see in the analysis below. The reason is that \(\bm{Z}^{*}\bm{Z}\) and \(\bm{Z}^{*}\bm{Z}\) have the same nonzero eigenvalues regardless of the shape of \(\bm{Z}\), which implies that \(\log\det(\bm{I}+\alpha\bm{Z}^{*}\bm{Z})=\log\det(\bm{I}+\alpha\bm{Z}\bm{Z}^{*})\). In particular, interpreting the norms appropriately (with a slight abuse of notation, we have \(\varphi(\bm{Z})=\varphi(\bm{Z}^{*})\), so for the purposes of analysis we can always proceed as though \(\bm{Z}\) is a tall matrix (as long as we do not use any special properties of \(\alpha\) in our derivation).
3. We have \(d\geq N\),11 and the columns of \(\bm{Z}^{\ell+1/2}\) are orthogonal, so that \((\bm{Z}^{\ell+1/2})^{*}\bm{Z}^{\ell+1/2}=\bm{I}\).12

Footnote 11: This assumption is without loss of generality, as we will see in the analysis below. The reason is that \(\bm{Z}^{*}\bm{Z}\) and \(\bm{Z}^{*}\bm{Z}\) have the same nonzero eigenvalues regardless of the shape of \(\bm{Z}\), which implies that \(\log\det(\bm{I}+\alpha\bm{Z}^{*}\bm{Z})=\log\det(\bm{I}+\alpha\bm{Z}\bm{Z}^{*})\). In particular, interpreting the norms appropriately (with a slight abuse of notation, we have \(\varphi(\bm{Z})=\varphi(\bm{Z}^{*})\), so for the purposes of analysis we can always proceed as though \(\bm{Z}\) is a tall matrix (as long as we do not use any special properties of \(\alpha\) in our derivation).

Footnote 12: This assumption is strictly stronger than the previous one, and strictly stronger than an assumption of incoherence on the columns. It corresponds to the representation \(\bm{Z}^{\ell+1/2}\) being non-collapsed, which we expect to hold at initialization due to the projections \(\bm{U}_{|K|}\) being random.

The approach leads to an update iteration that is equal to a slightly simplified version of the ISTA block (17). We see this as a justification for our derivation in Section 2.4, which obtained the ISTA block by introducing an additional simplifying assumption on the distribution of the data at layer \(\ell\).

Analysis.Following (16), we will consider the natural relaxation of the \(\ell_{0}\) "norm" to the \(\ell^{1}\) norm, and incorporate a nonnegativity constraint. Consider the objective

\[\varphi(\bm{Z})=\lambda\|\bm{Z}\|_{1}+\chi_{\{\bm{Z}\geq\bm{0}\}}(\bm{Z})- \underbrace{\frac{1}{2}\log\det\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right)}_{R (\bm{Z})},\] (77)

where \(\bm{Z}\in\mathbb{R}^{d\times N}\) and \(\alpha=d/N\varepsilon^{2}\), and \(\chi_{\{\bm{Z}\geq\bm{0}\}}\) denotes the characteristic function for the set of elementwise-nonnegative matrices \(\bm{Z}\). As in Appendix A.2, we calculate

\[\nabla_{\bm{Z}}R(\bm{Z})=\alpha\bm{Z}\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z} \right)^{-1}.\] (78)We consider an incremental optimization scheme for the highly nonlinear and nonconvex objective \(\varphi\). Following Section 2.3, we optimize locally at a "post-compression" iterate \(\bm{Z}^{\ell+1/2}\). We follow the standard proximal majorize-minimize framework [70] for incremental/local optimization: this begins with the second-order Taylor expansion for the smooth part of \(\varphi\) in a neighborhood of the current iterate \(\bm{Z}^{\ell+1/2}\):

\[\begin{split} R(\bm{Z})=R(\bm{Z}^{\ell+1/2})&+\Big{\langle} \nabla_{\bm{Z}}R(\bm{Z}^{\ell+1/2}),\bm{Z}-\bm{Z}^{\ell+1/2}\Big{\rangle}\\ &+\int_{0}^{1}(1-t)\Big{\langle}\bm{Z}-\bm{Z}^{\ell+1/2},\nabla^{ 2}R(\bm{Z}_{t})\left(\bm{Z}-\bm{Z}^{\ell+1/2}\right)\Big{\rangle}\;\mathrm{d}t,\end{split}\] (79)

where for any \(\bm{Z}\in\mathbb{R}^{d\times N}\), \(\bm{Z}_{t}=t\bm{Z}^{\ell+1/2}+(1-t)\bm{Z}\). The proximal majorization-minimization approach alternates two steps to minimize \(\varphi\):

1. First, use assumptions on \(\bm{Z}^{\ell+1/2}\) to derive an upper bound on the operator norm of the Hessian \(\nabla^{2}R(\bm{Z})\) over the effective domain of the optimization problem. We will write \(L\) for this (uniform) upper bound. This yields a quadratic upper bound for the smooth part of the objective \(\varphi\).
2. Then, alternately minimize the _smooth part_ of the quadratic upper bound as a function of \(\bm{Z}\), and take a _proximal step_ on the nonsmooth part. It can be shown [70] that corresponds to the iteration \[\bm{Z}^{+}=\mathrm{prox}_{\frac{\lambda}{L}(\|\;\cdot\;\|_{1}+\chi_{\{\bm{Z} \geq 0\}})}\left(\bm{Z}+\frac{1}{L}\nabla_{\bm{Z}}R(\bm{Z})\right)\] (80) In the alternating minimization setting of this paper for optimizing (1), we only take one such step, starting at \(\bm{Z}^{\ell+1/2}\).

We will instantiate this program below, showing quantitative error bounds related to our assumptions above as necessary. Rather than directly applying the iteration (80), we will derive it below under our aforementioned assumptions.

Starting at (79), our first task is to upper bound the quadratic residual. This corresponds to estimating

\[\Big{\langle}\bm{Z}-\bm{Z}^{\ell+1/2},\nabla^{2}R(\bm{Z}_{t}) \left(\bm{Z}-\bm{Z}^{\ell+1/2}\right)\Big{\rangle}\] (81) \[\leq\sup_{t\in[0,1]}\left\|\nabla^{2}R(\bm{Z}_{t})\right\|_{\ell ^{2}\to\ell^{2}}\left\|\bm{Z}-\bm{Z}^{\ell+1/2}\right\|_{\mathrm{F}}^{2}\] (82)

with Cauchy-Schwarz. Using Lemma 5, we can estimate the operator norm term in the previous bound in terms of properties of \(\bm{Z}^{\ell+1/2}\). We need to bound

\[\alpha\sup_{\|\bm{\Delta}\|_{\bm{r}}\leq 1}\big{\|}\big{(}\bm{\Delta}-\alpha\bm{Z}_{t}( \bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}(\bm{Z}_{t}^{*}\bm{\Delta}+\bm{ \Delta}^{*}\bm{Z}_{t})\big{)}\left(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t}\right) ^{-1}\big{\|}_{\mathrm{F}},\] (83)

and Lemma 6 gives that this term is no larger than \(9\alpha/4\) for any \(\bm{Z}\) and any \(t\). With this estimate and (79), we have a quadratic upper bound for \(-R(\bm{Z})\):

\[-R(\bm{Z})\leq-R(\bm{Z}^{\ell+1/2})+\Big{\langle}-\nabla_{\bm{Z}}R(\bm{Z}^{ \ell+1/2}),\bm{Z}-\bm{Z}^{\ell+1/2}\Big{\rangle}+\frac{9\alpha}{8}\Big{\|} \bm{Z}-\bm{Z}^{\ell+1/2}\Big{\|}_{\mathrm{F}}^{2}.\] (84)

Meanwhile, by our assumptions above, we have

\[-\nabla_{\bm{Z}}R(\bm{Z}^{\ell+1/2})=-\alpha\bm{Z}^{\ell+1/2}\left(\bm{I}+ \alpha\bm{I}\right)^{-1}=-\frac{\alpha}{1+\alpha}\bm{Z}^{\ell+1/2}.\] (85)

We now minimize the preceding quadratic upper bound as a function of \(\bm{Z}\). Differentiating, the minimizer \(\bm{Z}_{\mathrm{opt}}\) is calculated as

\[\bm{Z}_{\mathrm{opt}}=\left(1+\frac{4}{9(1+\alpha)}\right)\bm{Z}^{\ell+1/2},\] (86)

and it is well-known that the proximal operator of the sum of \(\chi_{\{\bm{Z}\geq\bm{0}\}}\) and \(\lambda\|\cdot\|_{1}\) is simply the one-sided soft-thresholding operator [70]

\[\mathrm{prox}_{\chi_{\{\bm{Z}\geq 0\}}+\lambda\|\;\cdot\;\|_{1}}\left(\bm{Z} \right)=\max\{\bm{Z}-\lambda\bm{1},\bm{0}\},\] (87)where the maximum is applied elementwise. As in Section 2.4, we may write this elementwise maximum simply as \(\mathrm{ReLU}\). Thus, one step of proximal majorization-minimization under our simplifying assumptions takes the form

\[\bm{Z}^{\ell+1}=\mathrm{ReLU}\left(\left(1+\frac{4}{9(1+\alpha)}\right)\bm{Z}^{ \ell+1/2}-\frac{4\lambda}{9\alpha}\bm{1}\right).\] (88)

Finally, we point out one additional elaboration which introduces the dictionary \(\bm{D}\) that appears in the ISTA block in Section 2.4. Notice that for any orthogonal \(\bm{D}\), one has \(R(\bm{D}\bm{Z})=R(\bm{Z})\) for every \(\bm{Z}\). This symmetry implies equivariance properties of \(\nabla_{\bm{Z}}R(\bm{Z})\) and \(\nabla_{\bm{Z}}^{2}R(\bm{Z})\): for every \(\bm{Z}\) and every \(\bm{\Delta}\) and every orthogonal \(\bm{D}\),

\[\bm{D}\nabla_{\bm{Z}}R(\bm{Z}) =\nabla_{\bm{Z}}R(\bm{D}\bm{Z}),\] (89) \[\langle\bm{D}\bm{\Delta},\nabla_{\bm{Z}}^{2}R(\bm{Z})\left(\bm{D }\bm{\Delta}\right)\rangle =\langle\bm{\Delta},\nabla_{\bm{Z}}^{2}R(\bm{D}\bm{Z})\left(\bm{ \Delta}\right)\rangle.\] (90)

Hence the quadratic Taylor expansion (79) can be written equivalently as

\[R(\bm{Z})=R(\bm{D}^{*}\bm{Z}^{\ell+1/2}) +\left\langle\nabla_{\bm{Z}}R(\bm{D}^{*}\bm{Z}^{\ell+1/2}),\bm{Z }-\bm{Z}^{\ell+1/2}\right\rangle\] (91) \[+\int_{0}^{1}(1-t)\Big{\langle}\bm{Z}-\bm{Z}^{\ell+1/2},\nabla^{ 2}R(\bm{D}^{*}\bm{Z}_{t})\left(\bm{Z}-\bm{Z}^{\ell+1/2}\right)\Big{\rangle} \,\mathrm{d}t,\]

for any orthogonal \(\bm{D}\). The significance of this is that we have obtained an expression equivalent to (79), but with \(\bm{Z}^{\ell+1/2}\) replaced by \(\bm{D}^{*}\bm{Z}^{\ell+1/2}\); moreover, because our approximation arguments above are not affected by left-multiplication of \(\bm{Z}^{\ell+1/2}\) by an orthogonal matrix (this operation does not change the norms of the columns of \(\bm{Z}^{\ell+1/2}\), or their correlations, and hence the matrix's incoherence), we can apply exactly the same line of reasoning above to obtain that an equivalent proximal majorization-minimization iteration is given by

\[\bm{Z}^{\ell+1}=\mathrm{ReLU}\left(\left(1+\frac{4}{9(1+\alpha)}\right)\bm{D} ^{*}\bm{Z}^{\ell+1/2}-\frac{4\lambda}{9\alpha}\bm{1}\right),\] (92)

for any orthogonal dictionary \(\bm{D}\). This gives an update quite similar to the ISTA block (17) in the case where the dictionary used in Section 2.4 is orthogonal, but without a skip connection.

We thus obtain a natural white-box version of this part of the architecture, along with the natural interpretation _that its purpose is to sparsify the compressed tokens \(\bm{Z}^{\ell+1/2}\) in a (learnable) dictionary_, which accords with recent empirical studies [79].

Other architectures?As we mentioned at the start of this section, the preceding derivation is performed in the most elementary possible setting in order to demonstrate the majorization-minimization approach for layer design. More precise approximations or assumptions may lead to superior layer designs that better optimize the target objective (1) (and in particular (13)). We mention two here:

1. **Beyond exactly-incoherent features**: our derivations above assumed that the incoming representations \(\bm{Z}^{\ell+1/2}\) were already maximal for the expansion term \(R\) in (13). It is desirable to obtain a 'perturbative' derivation, which applies in cases where \(\bm{Z}^{\ell+1/2}\) is not fully orthogonal, but instead near-orthogonal, in particular _incoherent_[70]. The derivations above can be adapted to this setting; the perturbation bounds become slightly more delicate, and the ultimate layer (92) changes to involve additional normalization.
2. **Beyond orthogonal dictionaries**: The symmetries of the expansion term \(R\) in (13) may be followed to lead to a pair of dictionaries \(\bm{D}\) and \(\bm{D}^{\prime}\) and an objective that sparsifies \(\bm{D}\bm{Z}^{\prime}\). This type of transformation is suggestive of popular architectures that mix over tokens [54, 67], however we consider the simpler form \(\bm{D}\bm{Z}\) in this work. In addition, we have focused for simplicity on orthogonal dictionaries \(\bm{D}\); as in the previous bullet, one may consider in a similar way dictionaries \(\bm{D}\) which are complete and near-orthogonal. Adapting the derivation to _overcomplete dictionaries_ is an interesting future direction that we expect to improve the scalability of crate; one avenue to achieve this could be increasing the number of projections \(\bm{U}_{[K]}\) and their embedding dimensions.

#### a.3.1 Auxiliary Lemmas

**Lemma 5**.: _Consider the function_

\[R(\bm{Z})=\frac{1}{2}\log\det\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right),\] (93)

_where \(\alpha>0\) is a constant. Then we have_

\[\nabla_{\bm{Z}}R(\bm{Z})=\alpha\bm{Z}\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z} \right)^{-1},\] (94)

_and the Hessian operator \(\nabla_{\bm{Z}}^{2}R(\bm{Z})\colon\mathbb{R}^{d\times N}\to\mathbb{R}^{d\times N}\) satisfies that for any \(\bm{\Delta}\in\mathbb{R}^{d\times N}\),_

\[\nabla_{\bm{Z}}^{2}R(\bm{Z})\left(\bm{\Delta}\right)\] (95) \[=\alpha\bm{\Delta}\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right)^{-1 }-\alpha^{2}\bm{Z}\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right)^{-1}\left(\bm{Z} ^{*}\bm{\Delta}+\bm{\Delta}^{*}\bm{Z}\right)\left(\bm{I}+\alpha\bm{Z}^{*}\bm {Z}\right)^{-1}.\] (96)

Proof.: The gradient calculation follows from [46], for example. For the Hessian, we use the usual approach to calculating derivatives: if \(\bm{\Delta}\) is any matrix with the same shape as \(\bm{Z}\) and \(t>0\),

\[\nabla_{\bm{Z}}^{2}R(\bm{Z})\left(\bm{\Delta}\right)=\frac{\partial}{ \partial t}\Big{|}_{t=0}\left[t\mapsto\nabla_{\bm{Z}}R(\bm{Z}+t\bm{\Delta}) \right],\] (97)

valid since \(R\) is smooth. We have

\[\nabla_{\bm{Z}}R(\bm{Z}+t\bm{\Delta})\] \[= \alpha(\bm{Z}+t\bm{\Delta})\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}+ \alpha t\left[\bm{Z}^{*}\bm{\Delta}+\bm{\Delta}^{*}\bm{Z}+t\bm{\Delta}^{*}\bm {\Delta}\right]\right)^{-1}\] \[= \alpha(\bm{Z}+t\bm{\Delta})\left(\bm{I}+\alpha t\left(\bm{I}+ \alpha\bm{Z}^{*}\bm{Z}\right)^{-1}\left[\bm{Z}^{*}\bm{\Delta}+\bm{\Delta}^{*} \bm{Z}+t\bm{\Delta}^{*}\bm{\Delta}\right]\right)^{-1}\left(\bm{I}+\alpha\bm{Z }^{*}\bm{Z}\right)^{-1}\] \[= \alpha(\bm{Z}+t\bm{\Delta})\left(\sum_{k=0}^{\infty}(-\alpha t)^ {k}\left(\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right)^{-1}\left[\bm{Z}^{*}\bm{ \Delta}+\bm{\Delta}^{*}\bm{Z}+t\bm{\Delta}^{*}\bm{\Delta}\right]\right)^{k} \right)\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right)^{-1},\]

where in the fourth line we require that \(t\) is sufficiently close to \(0\) in order to invoke the Neumann series. First, notice that the term involving \(\bm{\Delta}^{*}\bm{\Delta}\) does not play a role in the final expression: after we differentiate with respect to \(t\) and take a limit \(t\to 0\), terms arising due to differentiation of \(t\mapsto t\bm{\Delta}^{*}\bm{\Delta}\) go to zero, because whenever the summation index \(k>0\) we have a term \((-\alpha t)^{k}\) that goes to zero as \(t\to 0\). We thus obtain with the product rule

\[\frac{\partial}{\partial t}\bigg{|}_{t=0}\left[t\mapsto\nabla_{ \bm{Z}}R(\bm{Z}+t\bm{\Delta})\right]\] (98) \[=\alpha\bm{\Delta}\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right)^{-1} -\alpha^{2}\bm{Z}\left(\bm{I}+\alpha\bm{Z}^{*}\bm{Z}\right)^{-1}\left(\bm{Z}^ {*}\bm{\Delta}+\bm{\Delta}^{*}\bm{Z}\right)\left(\bm{I}+\alpha\bm{Z}^{*}\bm{ Z}\right)^{-1}.\] (99)

**Lemma 6**.: _One has_

\[\sup_{\|\bm{\Delta}\|_{\mathrm{F}}\leq 1}\left\|\left(\bm{\Delta}- \alpha\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}(\bm{Z}_{t}^{*} \bm{\Delta}+\bm{\Delta}^{*}\bm{Z}_{t})\right)\left(\bm{I}+\alpha\bm{Z}_{t}^{* }\bm{Z}_{t}\right)^{-1}\right\|_{\mathrm{F}}\leq\frac{9}{4}.\] (100)

Proof.: Fix \(\bm{\Delta}\) satisfying \(\|\bm{\Delta}\|_{\mathrm{F}}\leq 1\). By the triangle inequality,

\[\left\|\left(\bm{\Delta}-\alpha\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t }^{*}\bm{Z}_{t})^{-1}(\bm{Z}_{t}^{*}\bm{\Delta}+\bm{\Delta}^{*}\bm{Z}_{t}) \right)\left(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t}\right)^{-1}\right\|_{ \mathrm{F}}\] (101) \[\leq\left\|\bm{\Delta}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{- 1}\right\|_{\mathrm{F}}+\alpha\big{\|}\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*} \bm{Z}_{t})^{-1}(\bm{Z}_{t}^{*}\bm{\Delta}+\bm{\Delta}^{*}\bm{Z}_{t})(\bm{I}+ \alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\big{\|}_{\mathrm{F}}.\] (102)

For the first term, we note that

\[\left\|\bm{\Delta}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\right\|_{ \mathrm{F}}=\left\|\left((\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\otimes \bm{I}\right)\mathrm{vec}(\bm{\Delta})\right\|_{\mathrm{F}},\] (103)

and since \((\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\preceq\bm{I}\), we obtain from Cauchy-Schwarz13

Footnote 13: Recall that the eigenvalues of a Kronecker product of symmetric matrices are the tensor product of the eigenvalues (with multiplicity).

\[\left\|\bm{\Delta}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\right\|_{ \mathrm{F}}\leq\|\bm{\Delta}\|_{\mathrm{F}}.\] (104)We can use a similar idea to control the second term. We have from the triangle inequality

\[\left\|\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}(\bm{Z}_ {t}^{*}\bm{\Delta}+\bm{\Delta}^{*}\bm{Z}_{t})(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_ {t})^{-1}\right\|_{\mathrm{F}}\] (105) \[\quad\leq\left\|\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^ {-1}\bm{Z}_{t}^{*}\bm{\Delta}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1} \right\|_{\mathrm{F}}\] (106) \[\quad\quad+\left\|(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\bm {Z}_{t}^{*}\bm{\Delta}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\bm{Z}_{t}^{ *}\right\|_{\mathrm{F}}.\] (107)

For the first term, we have

\[\left\|\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\bm{ Z}_{t}^{*}\bm{\Delta}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\right\|_{ \mathrm{F}}\] (108) \[\quad=\left\|((\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\otimes \bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\bm{Z}_{t}^{*})\;\mathrm{ vec}(\bm{\Delta})\right\|_{\mathrm{F}}\] (109) \[\quad\leq\sigma_{\max}\left((\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t })^{-1}\right)\sigma_{\max}\left(\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_ {t})^{-1}\bm{Z}_{t}^{*}\right)\|\bm{\Delta}\|_{\mathrm{F}}\] (110) \[\quad\leq\frac{1}{\alpha}\|\bm{\Delta}\|_{\mathrm{F}}.\] (111)

The last estimate follows from a computation using the SVD of \(\bm{Z}_{t}\). Meanwhile, we have for the second term by a similar argument (using the fact that the singular values of \(\bm{A}\) and \(\bm{A}^{*}\) are identical for any matrix \(\bm{A}\))

\[\left\|(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\bm{Z}_{t}^{* }\bm{\Delta}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\bm{Z}_{t}^{*}\right\| _{\mathrm{F}} \leq\sigma_{\max}\left((\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^ {-1}\bm{Z}_{t}^{*}\right)^{2}\|\bm{\Delta}\|_{\mathrm{F}}\] (112) \[\leq\frac{1}{4\alpha}\|\bm{\Delta}\|_{\mathrm{F}},\] (113)

where once again the estimate follows from a computation involving the SVD of \(\bm{Z}_{t}\) (together with the fact that the function \(\sigma\mapsto\sigma/(1+\alpha\sigma^{2})\) is bounded on \(\sigma\geq 0\) by \(1/(2\sqrt{\alpha})\)). Putting it together, we have obtained

\[\left\|\left(\bm{\Delta}-\alpha\bm{Z}_{t}(\bm{I}+\alpha\bm{Z}_{t}^{*}\bm{Z}_{ t})^{-1}(\bm{Z}_{t}^{*}\bm{\Delta}+\bm{\Delta}^{*}\bm{Z}_{t})\right)(\bm{I}+ \alpha\bm{Z}_{t}^{*}\bm{Z}_{t})^{-1}\right\|_{\mathrm{F}}\leq\frac{9}{4}\| \bm{\Delta}\|_{\mathrm{F}},\] (114)

which gives the claim after taking suprema.

Additional Experiments and Details

In this section, we provide details about our experiments, and report the results of additional experiments that were not covered in the main text. crate takes arguably the most basic design choices possible, and so we do _not_ attempt to directly compete with state-of-the-art performance from heavily engineered and empirically designed transformers. The results of our experiments are meant to convey a few core messages:

* _Despite not being engineered to compete with the state-of-the-art, crate performs strongly on large-scale real-world datasets_, including classification on ImageNet-1K. crate also achieves strong transfer learning performance.
* _Because our model is designed through unrolled optimization of a well-understood objective, each layer is interpretable._ In particular, we can analyze the performance of crate, as well as design network modifications, on a _layer-wise basis_. This is powered by an arguably unparalleled level of insight into the role of each operator in our network.
* _We make the simplest possible choices during the design of crate, but these can be changed easily while keeping the same framework_. We study a few modifications later in this section (Appendix B.4) and show that they do not significantly hurt empirical performance, but emphasize here that there is significant potential for improvement with different architecture choices (and in particular a different theoretical analysis).

### Implementation details

In this subsection, we provide more details for implementing crate on vision tasks.

#### b.1.1 Architecture of crate

Architectural modifications.Compared to the conceptual architecture proposed in Sections 2.5 and 3, we make the following change for the sake of implementation simplicity:

* In the compression step, replace the term \(\frac{p}{Ne^{2}}\left[\bm{U}_{1},\dots,\bm{U}_{K}\right]\) in the MSSA operator with another trainable parameter \(\bm{W}\in\mathbb{R}^{d\times pK}\). Thus the MSSA block becomes \[\texttt{MSSA}(\bm{Z}\mid\bm{U}_{[K]},\bm{W})\doteq\bm{W}\begin{bmatrix} \texttt{SSA}(\bm{Z}\mid\bm{U}_{1})\\ \vdots\\ \texttt{SSA}(\bm{Z}\mid\bm{U}_{K})\end{bmatrix}.\] (115)

PyTorch **code for crate.** We provide PyTorch-style code for implementing our proposed network architecture. Algorithm 1 defines the overall architecture, Algorithm 2 and Algorithm 3 contain details for the transformer block, self-attention block (MSSA-block), and MLP block (ISTA-block).

#### b.1.2 Training Setup

Pre-training on ImageNet-1K.We apply the Lion optimizer [73] for pre-training both crate and ViT models. We configure the learning rate as \(2.4\times 10^{-4}\), weight decay as 0.5, and batch size as 2,048. We incorporate a warm-up strategy with a linear increase over 5 epochs, followed by training the models for a total of 150 epochs with cosine decay. For data augmentation, we only apply the standard techniques, random cropping and random horizontal flipping, on the ImageNet-1K dataset. We apply label smoothing with smoothing parameter \(0.1\). One training epoch of crate\(-Base\) takes around 240 seconds using 16 A100 40GB GPUs.

Fine-tuning.We fine-tune our pre-trained crate and ViT models on the following target datasets: CIFAR10/CIFAR100 [10], Oxford Flowers-102 [7], Oxford-IIIT-Pets [16]. We also evaluate our pre-trained models on the commonly used ImageNet Real [36] benchmark. For each fine-tuning task, we use the AdamW optimizer [26]. We configure the learning rate as \(5\times 10^{-5}\), weight decay as 0.01, and batch size to be 512. To allow transfer learning, we first resize our input data to 224. For data augmentations, we also adopt several standard techniques: random cropping, random horizontal flipping, and random augmentation (with number of transformations \(n=2\) and magnitude of transformations \(m=14\)).14```
#ClassViT_dictionarydefinition CRATE:  # initialization  definit(self,image_size,patch_size,num_classes,dim,depth,heads,  mlp_dim,pool='cls',channels=3,dim_head=64,dropout=0.,  emb_dropout=0.):  #definepatch,imagedimensionsandnumberofpatches  image_height,image_width=pair(image_size)  patch_height,patch_width=pair(patch_size)  num_patches=(image_height//patch_height)*(image_width//  patch_width)  patch_dim=channels*patch_height*patch_width
#definepatchembedding,positionalembedding,dropout,andtransformer  self.to_patch_embedding=Sequential(Rearrange,LayerNorm(patch_dim),  Linear(patch_dim,dim),LayerNorm(dim))  self.pos_embedding=Parameter(random(1,num_patches+1,dim))  self.cls_token=Parameter(random(1,1,dim))  self.dropout=Dropout(emb_dropout)  self.transformer=Transformer(dim,depth,heads,dim_head,mlp_dim,  dropout)  #definepooling,latentlayer,andMLPhead  self.pool=pool  self.to_latent=Identity()  self.mlp_head=Sequential(LayerNorm(dim),Linear(dim,num_classes))
#forwardpass  defforward(self,img):  x=self.to_patch_embedding(img)  b,n,_=shape(x)  cls_tokens=repeat(self.cls_token,'11d->b1d',b=b)  x=concatenate((cls_tokens,x),dim=1)  x+=self.pos_embedding[:,:(n+1)]  x=self.dropout(x)  x=self.transformer(x)  x=mean(x,dim=1)ifself.pool=='mean'elsex[:,0]  x=self.to_latent(x)  returnself.mlp_head(x) ```

**Algorithm 1** PyTorch-style pseudocode for crateNetwork

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])
#forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)+x  x =ff(x_)  returnx ```

**Algorithm 2** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])
#forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)+x  x =ff(x_)  returnx ```

**Algorithm 3** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)+x  x =ff(x_)  returnx ```

**Algorithm 4** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)+x  x =ff(x_)  returnx ```

**Algorithm 5** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)+x  x =ff(x_)  returnx ```

**Algorithm 6** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)+x  x =ff(x_)  returnx ```

**Algorithm 7** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)+x  x =ff(x_)  returnx ```

**Algorithm 8** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 9** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 10** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 11** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 12** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #definelayers  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 13** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #definelayers  self.layers.append([LayerNorm(dim,FeedForward(dim,mlp_dim,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 14** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 15** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 16** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth=depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim,heads,dim_head,  dropout))])  #forwardpass  defforward(self,x):  forattn,ffinself.layers:  x_=attn(x)  x =ff(x_)  returnx ```

**Algorithm 17** Pytorch Style Pseudocode for Transformer Block in crate

```
#ClassTransformerdefinition classTransformer:  #initialization  definit(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):  #definelayers  self.layers=[]  self.depth =depth  for_inrange(depth):  self.layers.append([LayerNorm(dim,Attention(dim

```
#ClassFeedForwarddefinition classFeedForward: #initialization definit(self,dim,hidden_dim,dropout=0.,step_size=0.1,lambd=0.1): self.weight=Parameter(Tensor(dim,dim)) init.kaiming_uniform_(self.weight) self.step_size=step_size self.lambd=lambd
#forwardpass defforward(self,x): x1=linear(x,self.weight,bias=None) grad_1=linear(x1,self.weight.t(),bias=None) grad_2=linear(x,self.weight.t(),bias=None) grad_update=self.step_size*(grad_2-grad_1)-self.step_size* self.lambd output=relu(x+grad_update) returnoutput
#ClassAttentiondefinition classAttention: #initialization definit(self,dim,heads=8,dim_head=64,dropout=0.): inner_dim=dim_head*heads project_out=not(heads==1anddim_head==dim) self.heads=heads self.scale=dim_head**-0.5 self.attend=Softmax(dim=-1) self.dropout=Dropout(dropout) self.qkv=Linear(dim,inner_dim,bias=False) self.to_out=Sequential(Linear(inner_dim,dim),Dropout(dropout))if project_outelsenn.Identity() #forwardpass defforward(self,x): w=rearrange(self.qkv(x),'bn(hd)->bhnd',h=self.heads) dots=matmul(w,w.transpose(-1,-2))*self.scale attn=self.attend(dots) attn=self.dropout(attn) out=matmul(attn,w) out=rearrange(out,'bhnd->bn(hd)') returnself.to_out(out) ```

**Algorithm 3** Pseudocode for Attention and FeedForward

### Experimental Results

In this subsection, we provide additional experimental results on crate, including layer-wise measurements, visualizations, as well as ablation studies.

#### b.2.1 Layer-wise Evaluation and Visualization

Layer-wise evaluation of compression and sparsity.Similar to Figure 3, we conduct the layer-wise evaluation of compression term and sparsity for crate-Tiny, crate-Base, and crate-Large. We observe similar behavior as mentioned in Section 3.1: both the compression term and the sparsity term improves as the layer index increases.

Figure 5: _Left:_ The compression term \(R^{c}(\bm{Z}^{\ell+1/2})\) of the MSSA outputs at different layers. _Right_: the sparsity of the ISTA output block, \(\|\bm{Z}^{\ell+1}\|_{0}/(d\cdot N)\), at different layers.

[MISSING_PAGE_EMPTY:30]

[MISSING_PAGE_EMPTY:32]

Figure 11: Visualizing layer-wise token \(\bm{Z}^{\ell}\) representations at each layer \(\ell\). To enhance the visual clarity, we randomly extract a 50\(\times\)50 sub-matrix from \(\bm{Z}^{\ell}\) for display purposes. (_Sample 4_)

Figure 10: Visualizing layer-wise token \(\bm{Z}^{\ell}\) representations at each layer \(\ell\). To enhance the visual clarity, we randomly extract a 50\(\times\)50 sub-matrix from \(\bm{Z}^{\ell}\) for display purposes. (_Sample 3_)

### crate Ablation

Hyperparameters of crate.In Table 2, we present evaluation of crate trained with various parameters. More specifically, we investigate the effect of number of epochs, weight decay, learning rate, step size \((\eta)\) and the regularization term \((\lambda)\) in ISTA block. As shown in Table 2, crate demonstrates consistently satisfactory performance across a diverse range of hyperparameters.

### Exploring Architecture Variants

In this section, we explore the two following alternative architectures. One architecture involves a modification to the attention mechanism, while the other involves a modification to the sparsification mechanism. Again, we re-emphasize that these choices, although principled, are entirely modular and the choices we make here still lead to very simple architectures. A more sophisticated analysis may lead to different, more complicated architectures that perform better in practice. The architectures we experiment with are:

* Compression-inspired attention mechanism: revert the change in (115). That is, the attention mechanism implements (11) and (12) directly.
* Majorization-minimization proximal step sparsification: instead of (17), implement (92).

We obtain the following classification results in Table 3. After conducting additional simplifications to the network architecture (i.e., imposing additional constraints to the network architecture design), we discover that crate maintains reasonable performance on ImageNet-1K.

### Sparse Coding vs. Non-Negative Sparse Coding

In the main body, we used a non-negative sparse coding formulation (16) to obtain the ISTA block as an unrolled proximal gradient step in (17). Suppose that we hadn't done this, and instead directly computed an ISTA block using an unrolled proximal gradient step on (15). Such a block would give the following update rule:

\[\bm{Z}^{\ell+1}=S_{\lambda\eta}(\bm{Z}^{\ell+1/2}+\eta\bm{D}^{*}(\bm{Z}^{\ell +1/2}-\bm{D}\bm{Z}^{\ell+1/2})),\] (116)

where \(S_{\lambda\eta}\) is the soft-thresholding function

\[S_{\lambda\eta}(x)=\text{sgn}(x)\cdot(|x|-\lambda\eta)_{+}.\] (117)

\begin{table}
\begin{tabular}{l|c c c|c c|c} \hline \hline
**Model** & **epoch** & **weight decay** & **lr** & \(\eta\) (ISTA) & \(\lambda\) (ISTA) & ImageNet \\ \hline \hline crate-B & 150 (default) & 0.5 (default) & \(2.4\times 10^{-4}\) & 0.1 & 0.1 & 70.8 \\ \hline \hline crate-B & 150 & 0.5 & \(2.4\times 10^{-4}\) & _0.02_ & 0.1 & 70.7 \\ \hline crate-B & 150 & 0.5 & \(2.4\times 10^{-4}\) & _0.5_ & 0.1 & 66.7 \\ \hline crate-B & 150 & 0.5 & \(2.4\times 10^{-4}\) & 0.1 & _0.02_ & 70.8 \\ \hline \hline crate-B & 150 & 0.5 & \(2.4\times 10^{-4}\) & 0.1 & _0.5_ & 70.5 \\ \hline \hline crate-B & _90_ & 0.5 & \(2.4\times 10^{-4}\) & 0.1 & 0.1 & 69.5 \\ \hline crate-B & _300_ & 0.5 & \(2.4\times 10^{-4}\) & 0.1 & 0.1 & 70.9 \\ \hline crate-B & 150 & _1.0_ & \(2.4\times 10^{-4}\) & 0.1 & 0.1 & 70.3 \\ \hline crate-B & 150 & _0.05_ & \(2.4\times 10^{-4}\) & 0.1 & 0.1 & 70.2 \\ \hline crate-B & 150 & 0.5 & \(4.8\times 10^{-4}\) & 0.1 & 0.1 & 70.2 \\ \hline crate-B & 150 & 0.5 & \(1.2\times 10^{-4}\) & 0.1 & 0.1 & 70.3 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Top 1 accuracy of crate on various datasets with different architecture design variants when trained on ImageNet.

\begin{table}
\begin{tabular}{l c c|c} \hline \hline
**Model** & **MSSA-block** & **ISTA-block** & **ImageNet** \\ \hline \hline crate-B & default & default & 70.8 \\ \hline crate-B & Eq. (11) and (12) & default & 63.3 \\ crate-B & default & Eq. (92) & 68.6 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Top 1 accuracy of crate on various datasets with different architecture design variants when trained on ImageNet.

applied element-wise to its input matrix. The resulting architecture would be an alternative to crate; below, we discuss some empirical and theoretical similarities and differences between the two formulations and architectures.

Empirical evaluation of soft-thresholding-based architecture.We summarize the results of crate with soft-thresholding activation in Table 4. We use \(\lambda=10\) in \(S_{\lambda\eta}\) and set all other hyperparameters the same as in the original CRATE-Base evaluation on ImageNet-1K. We find that such a soft-thresholding model achieves slightly worse performance-a drop of 3.2% top-1 accuracy--compared to the default crate-Base (with ReLU activation).

Potential theoretical justification for the performance differential.Previous work [49] studied the phase collapse mechanism for understanding the non-linearities and the convolutional filters used in CNNs such as ResNet [23] on classification tasks. Specifically, they found that replacing the phase collapses with thresholding operators which enforce sparsity largely degrades the classification performance of CNNs. The effect of phase collapse analyzed in [49] is to better separate out the means of different classes within a classification task. This may account in part for the increase in classification accuracy reported in Table 4. On the other hand, we believe that the CRATE architecture will be applicable beyond just classification tasks. In CRATE, the purpose of the training process is to learn the local signal models at each layer (see e.g., Section 2.5). From this perspective, so long as the downstream training task requires semantically meaningful representations of the data distribution, the exact training configuration is of secondary importance. In particular, we may use self-supervised learning methods such as (masked) autoencoding to learn the signal models, whence there may not be any well-defined notion of class mean. In such cases, _a priori_ we may expect both soft thresholding and nonnegative soft thresholding to perform comparably well. We leave the verification of this to future work.

Theoretical justification for non-negative sparse coding.The sparse rate reduction formulation (1) does not include a non-negative constraint, and the token representations have marginal distribution equal to a mixture of zero-mean Gaussians (which are symmetric around \(\mathbf{0}\)). Below, we argue that the non-negative sparse rate reduction optimization and the regular sparse rate reduction optimization engender representations which are qualitatively similar in many ways, which confirms our conceptual understanding of how crate performs structured representation learning.

First, we formalize the non-negative sparse rate reduction optimization problem. Let \(\chi\) be the characteristic function (with codomain \(\{0,\infty\}\)) of its input proposition. Then the non-negative analogue to (1) is

\[\max_{f\in\mathcal{F}}[\Delta R(\bm{Z};\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}- \chi(\bm{Z}\geq 0)]\qquad\text{where}\qquad\bm{Z}=f(\bm{X}).\] (118)

Although formal analysis of the optimal points of the sparse rate reduction maximization problem (1) or its nonnegative variant (118) is out of scope of this work, we see that the rate reduction maximization (i.e., \(\max_{f\in\mathcal{F}}[\Delta R(\bm{Z};\bm{U}_{[K]})]\) has optimal points characterized similarly to [46, Theorem A.6], namely that the representation of each distribution in the mixture is supported on a subspace with nearly isotropic covariance on this subspace, and the supporting subspaces are (nearly) orthogonal. Adding the sparsity term \(\lambda\|\bm{Z}\|_{0}\) for some regularizer \(\lambda\) would enforce the axis-alignment of the supporting subspaces; when adding in addition the nonnegativity term \(\chi(\bm{Z}\geq 0)\), following through the proof of [46, Theorem A.6] suggests that the argument goes through with suitable modifications (in particular, considering the conclusions for the covariance rather than \(\bm{Z}\bm{Z}^{\top}\)). This sketch suggests that the statistical and geometric properties of the optimal representation remain the same when adding the non-negative constraint to the sparse rate reduction formulation. We leave a detailed proof to future work.

### Pre-training on ImageNet-21K

We investigate a larger pre-training dataset for training CRATE. Specifically, we first pretrain on ImageNet-21K [9], which contains 14 million images, and then fine-tuned on ImageNet-1K. As

\begin{table}
\begin{tabular}{l l c|c c c} \hline \hline
**Model** & **MSSA-block** & **ISTA-block** & **ImageNet** & **CIFAR 10\({}^{*}\)** & **CIFAR 100\({}^{*}\)** \\ \hline \hline crate-B & default & ReLU activation (default) & 70.8\% & 96.8\% & 82.7\% \\ \hline crate-B & default & soft-thresholding activation & 67.6\% & 96.0\% & 76.8\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: Top 1 accuracy of crate on ImageNet-1k with different architecture design variants. The soft-thresholding activation \(S_{\lambda\eta}\) is defined in Eq. (117).

[MISSING_PAGE_FAIL:36]