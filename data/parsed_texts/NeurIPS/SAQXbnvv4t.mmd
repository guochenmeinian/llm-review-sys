# AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data

 Zifan Song\({}^{1,2}\)\({}^{*}\) Yudong Wang\({}^{2}\)\({}^{*}\) Wenwei Zhang\({}^{2}\)\({}^{*}\) Kuikun Liu\({}^{2}\)

Chengqi Lyu\({}^{2}\) Demin Song\({}^{2}\) Qipeng Guo\({}^{2}\) Hang Yan\({}^{2}\)

Dahua Lin\({}^{2,3,4}\) Kai Chen\({}^{2}\)\({}^{\dagger}\) Cairong Zhao\({}^{1}\)\({}^{\dagger}\)

\({}^{1}\)Tongji University \({}^{2}\)Shanghai AI Laboratory

\({}^{3}\)MMLab, The Chinese University of Hong Kong

\({}^{4}\)HKGAI under InnoHK

###### Abstract

Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained LLMs. In this paper, we present _AlchemistCoder_, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed _AlchemistPrompts_, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that _AlchemistCoder_ holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence. Source code and models are available at https://github.com/InternlM/AlchemistCoder.

+
Footnote †: Corresponding author

+
Footnote †: Corresponding author

## 1 Introduction

Closed-source Large Language Models (LLMs) like ChatGPT and GPT-4 [33, 34] exhibit impressive code intelligence by learning on large-scale and diverse code corpus, which also benefits many other applications, such as math reasoning [6], embodied control [25], and agent [46]. Since open-source LLMs [40] still lag behind closed-source LLMs [34] in this field, there has been growing interest in investigating the acquisition of code capabilities by developing specialized Code LLMs [35, 14].

The training of Code LLMs mainly goes through pre-training and fine-tuning stages [35]. Pioneer

Figure 1: Performance scatter plot (_top right_ is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our _AlchemistCoder_ series demonstrates astonishing performance across all open-source Code LLMs.

works [5; 32; 22; 36; 3] have amassed extensive code data for pre-training, while recent open-source models [30; 44] highlight the effectiveness of high-quality or targeted code fine-tuning datasets. Despite these advancements, current fine-tuning methods mainly rely on a specific type of code-related question-answering dataset, unlike the pre-training stage that integrates code-related corpus from various sources [35]. Such a discrepancy indicates that the fine-tuning data may lack the necessary diversity to fully stimulate the capabilities of base models, resulting in limited performance, generalization, and robustness.

To overcome the limitations in quality and diversity within single-source data, we pioneer to explore integrating multi-source data for Code LLM fine-tuning. However, this is a non-trivial paradigm and blindly integrating multi-source data can potentially lead to inferior performance (_e.g._, the DirectlyMix-L-7B model in Fig. 1). To track this, we unveil inherent conflicts in multi-source code corpora, including conflicting code language requirements and response styles. Inspired by hindsight relabeling [1; 48], we propose to design data-specific prompts to harmonize the inherent conflicts for multi-source data integration, better eliciting the performance of base models. We term this form of prompts as _AlchemistPrompts_, inspired by the power and definition of _Alchemists_:

_"Alchemist: Someone Who Transforms Things for the Better."_ ---- Merriam Webster

Specifically, we first integrate several open-source code datasets and conduct instruction evolution [30] based on some of them (Fig. 2(a, b)). As shown in Fig. 2(c), for instruction-response pairs of different sources, we adopt one LLM to generate _AlchemistPrompts_ that accurately and explicitly describe the characteristics as requirements of the response to enrich the instructions. In-depth, the efficacy of _AlchemistPrompts_ is twofold: 1) Harmonization between different data sources: _AlchemistPrompts_ generated from the same LLM have similar styles and can bridge the style differences between sources, while the introduction of _AlchemistPrompt_-customized data, accounting for only 5%, achieves a balance between data diversity and domain gaps; 2) Harmonization within instruction-response pairs: As fine-grained and data-specific prompts, _AlchemistPrompts_ are designed to augment instructions with specific programming languages, algorithm concepts, and other code-related information involved in responses, which can refine the alignment within instruction-response pairs and enhance the instruction-following abilities of fine-tuned models.

Apart from the conventional problem-solution data, we argue that the progression of code data (_e.g._, data evolution, cleaning, and quality evaluation) reflects higher-level capabilities and offers valuable insights for the enhancement of Code LLMs. Consequently, we delineate the construction of data into three integral tasks for training: instruction evolution, data filtering, and code review (see Fig. 2 (d)), facilitating enhanced code comprehension capabilities.

We conduct extensive experiments with various base models [40; 35; 14] and develop the instruction-tuned _AlchemistCoder_ series. As shown in Fig. 1, on two mainstream code benchmarks, HumanEval

Figure 2: Overview for developing _AlchemistCoder_ series. We first integrate high-quality open-source data (a) and conduct data evolution based on them (b). Then, we adopt _AlchemistPrompt_ to harmonize their inherent conflicts (c) and construct code comprehension data (d). We use a mix of these data to fine-tune various pre-trained LLMs to obtain our _AlchemistCoder_ models.

and MBPP, _AlchemistCoder_ holds a clear lead among all models of equivalent size (6.7B/7B), and rivals or even surpasses larger models (15B/33B/70B), demonstrating harmonized and formidable code capabilities. Furthermore, we delve into the effectiveness of _AlchemistPrompts_ and discern that they alleviate the misalignment between instructions and responses within the data. Remarkably, _AlchemistPrompts_ allow the code corpus to also significantly improve the general capability of Code LLMs, as demonstrated by the improvements on MMLU, BBH, and GSM8K. Our main contributions are summarized as follows:

* Our work pioneers to integrate multi-source data for Code LLM fine-tuning to overcome the limitations of quality and diversity inherent in single-source data.
* We unveil inherent conflicts within multi-source code corpora and introduce _AlchemistPrompts_, revealing the power of hindsight tuning for code generation, aiming to harmonize the conflicts among sources and bridge the alignment within instruction-response pairs.
* We propose to incorporate data construction process into the fine-tuning data and design code comprehension tasks, including instruction evolution, data filtering, and code review, endowing comprehensive code capabilities.
* Extensive ablation and analytical studies confirm the efficacy of our key concepts for enhancing the diversity, quality, and cost-effectiveness of Code LLM fine-tuning data. Through instruction tuning on various base models, we develop the _AlchemistCoder_ series, surpassing all Code LLMs of the same size on a wide spectrum of code benchmarks.

## 2 Method

To more comprehensively elicit the capability of the base LLMs, we first construct multi-source data for fine-tuning (SS 2.1), which is harmonized by _AlchemistPrompts_ to take effect(SS 2.2). Code comprehension tasks are also constructed to further improve the performance(SS 2.3). We also discuss the details and statistics of the filtered and harmonized multi-source data in SS 2.4.

### Multi-source data construction

To fully elicit the capability of code LLMs, we first collect the fine-tuning data from multiple sources (Fig. 2(a)) and adopt the instruction evolution [30] to improve the complexity of the instructions (Fig. 2(b)). However, integrating multi-source data for instruction tuning poses challenges. Naturally,

Figure 3: Examples of inherent conflicts (_e.g._, various styles and quality) within multi-source code corpora. By applying _AlchemistPrompt_-customized instructions that are more consistent with the responses, the diversity from multiple sources can be effectively managed and utilized, thereby improving the quality of our fine-tuning data and the instruction-following capabilities of the fine-tuned models.

one code-related question can be solved by different coding languages with various algorithms or response styles (_e.g._, with or without reasoning). When naively combing data curated by different developers with different LLMs, the model tends to learn to answer similar questions with different coding languages and response styles, as depicted in Fig. 3. On the one hand, learning diverse responses may elicit different capability aspects of the base models. On the other hand, since the learned responses to similar instructions often diverge due to implicit human intentions, the LLMs tend to be unaligned (to our expectation) after the fine-tuning on the directly mixed data (_e.g._, we cannot expect which coding language the LLMs will use in real-world applications), resulting in inferior performance. Therefore, directly mixing multi-source data is not a promising solution and can be detrimental.

### AlchemistPrompt

To harmonize the inherent conflicts within multi-source data, we propose to customize data-specific prompts called _AlchemistPrompts_, (Fig. 2(c)), inspired by the concept of hindsight [1; 48]. Specifically, we employ GPT-4 [34] to play the role of an _Alchemist_ and design the prompt as illustrated in Fig. 4 to obtain _AlchemistPrompts_. For instance, for an instruction of 'Write code to find the shortest path from one vertex to all other vertices in a graph', if the response involves Python code of a Bellman-Ford algorithm with dynamic programming, we would expect to customize the instruction with an _AlchemistPrompt_ of 'Please generate Python code for the following task and attempt to use the concept of Dynamic Programming'.

For the selection of data customized by _AlchemistPrompts_, we calculate the differences in perplexities of generating responses with/without given instructions, called Conditional Perplexity Discrepancy (CPD). Then, we selectively chose data with higher CPD values for _AlchemistPrompt_ harmonizations. We treat CPD as an indicator of how data affects the complexity of model-generated responses under given conditions (_i.e._, instructions), and its calculation formula is \(\mathrm{CPD}=\mathrm{Perplexity}(\mathrm{conditional\_instruction}+\mathrm{ response})-\mathrm{Perplexity}(\mathrm{response})\). The level of CPD reflects the impact of the conditional instruction on the complexity of the generated response. Specifically, a high CPD indicates that the perplexity of the generated response significantly increases under the presence of a conditional instruction, which usually reflects a poor alignment between the instruction and the response, the instruction may be unclear or not specific enough, or insufficient contextual information, thereby increasing the difficulty of model response generation. By analyzing high CPD values, we can identify cases where instructions and responses are poorly aligned and more effectively optimize data quality.

The adjustments to data made by _AlchemistPrompts_ are relatively minor and well-calibrated. Our ablation study indicates that the optimal performance can be achieved by incorporating _AlchemistPrompts_ into only 5% samples, striking a balance between the diversity and domain gap resulting from the fusion of multi-source data. Crucially, by retrospectively analyzing responses and reinterpreting them as alternate goals, the _AlchemistPrompts_ serve to elevate the condition/goal of data. This hindsight integration [1; 48; 26] allows for a more nuanced and adaptive learning process, enhancing not only the models' comprehension of data but also refining instruction-following capabilities.

### Code comprehension task

The existing training datasets for Code LLMs [24; 4; 39; 30; 44] primarily focus on the code generation task consisting of programming problems and their corresponding code solutions. However, we

Figure 4: Detailed prompt designed for generating data-specific _AlchemistPrompts_.

contend that beyond this, the process of constructing code data demonstrates higher-level abilities. Consequently, we devise three code comprehension tasks relevant to data construction, including instruction evolution, data filtering, and code review (Fig. 2(d)).

**Instruction evolution.** Inspired by the concept of instruction evolution [45; 30], we employ GPT-3.5 [33] to construct instruction evolution task data, which entails augmenting the requirements for instructions and providing detailed explanations for programming tasks. Integrating the instruction evolution task aids the model in discerning the disparities before and after evolutions, thereby deepening the comprehension of programming requirements, code complexity, task decomposition, and other code-related concepts.

**Data filtering.** We identify four categories of low-quality data from multiple sources: (a) responses that are excessively short and lack code, (b) code that fails to compile, (c) code with poor clarity, and (d) code that does not adhere to the requirement in the instruction regarding its organization in function form. Each instruction in the data filtering task presents the model with a low-quality sample and prompts the model to classify it into one of the four categories. The data filtering task entails recycling the filtered-out data by offering counterexamples, thereby assisting the model in generating fewer low-quality responses.

**Code review.** In this task, we require the model to review a piece of code and assign scores between 0 and 10 for correctness and clarity separately. Additionally, the model is expected to provide suggestions for code improvement and present the refined code. To obtain higher-quality data, we utilize GPT-4 [34] to generate code reviews and select cases that are more representative, particularly those with average correctness and clarity scores exceeding 8 or falling below 6. Simultaneously, we focus on instances where one aspect exhibits severe deficiencies, _i.e._, the score of correctness or clarity is equal to or below 4.

### Data cleaning and decontamination

In practice, we have established a set of filtering rules to enhance our data cleaning and purification procedures. These rules involve excluding samples based on various criteria, such as response length (either too short or too long), absence of code or insufficient code content, non-compilable code, code failing test cases (pertinent to certain samples), responses structured in notebook form, and instances with excessive textual descriptions preceding the code. After conducting an extensive series of validation experiments, we conclusively decide to eliminate low-quality data meeting either of the following conditions: (a) responses that are excessively brief and lack code. Such responses typically offer direct answers to the instructions, neglecting both the code solution and explanatory annotations. Additionally, these samples frequently present overly simplistic questions in the instructions; (b) code solutions that are non-compilable or fail test cases (relevant to specific samples).

Concurrently, following [13], we employ N-gram similarity, cosine distance of code embeddings, and edit distance of code syntax trees to calculate the similarity between training data and samples in HumanEval and MBPP. We subsequently discard samples through this process of data filtering and deduplication, resulting in the removal of approximately 6% of the dataset.

### Harmonized AlchemistCoder dataset

Our _AlchemistCoder_ dataset (\(\sim\)200M tokens) comprises four types of multi-source data, encompassing open-source datasets and three types of data constructed by us. Specifically, (a) open-source datasets including Evol-Instruct-Code-80k-v1 [10], CodeExercise-Python-27k [9], and evol-codealpaca-v1 [39], (b) EvolCode data generated from gpt-3.5-turbo following [30], (c) data customized by _AlchemistPrompts_, and (d) data of the code comprehension tasks (_i.e._, instruction evolution, data filtering, and code review).

We visualize the distributions of data sources and programming languages using two circular graphs in Fig. 5. Concurrently, Fig. 6 reports a distribution of text description lengths and code lines. Compared to CodeAlpaca [4] and OOS-INSTRUCT [44], our _AlchemistCoder_ dataset presents a notably diverse distribution and maintains moderate overall text description and code lengths, benefiting significantly from the integration of multi-source data along with _AlchemistPrompts_ and code comprehension tasks. This is instrumental in contributing to a comprehensive evolution of code capability.

## 3 Experiments

In this section, we report results on various benchmarks of code generation and conduct ablation experiments. Furthermore, we present analytical studies to provide a more in-depth demonstration of the efficacy of our _AlchemistCoder_.

### Benchmarks and implementation details

**Benchmarks.** We adopt six code benchmarks: HumanEval [5], HumanEval+ [27], HumanEval-X [49], MBPP [2], MBPP+ [27], and DS-1000 [21]. In addition, we access three mainstream benchmarks (MMLU [15], BBH [37], and GSM8K [8]) to evaluate generalization abilities. All evaluation and benchmark details can be found in Appendix SSD.

**Baselines.** We compare with the following competitive baselines. Closed-Source Models: GPT-3.5-Turbo [33] and GPT-4-Turbo [34]. Open-Source Models: Llama 2 [40], CodeLlama [35], StarCoder [22], WizardCoder [30], DeepSeek-Coder [14], and Magicoder [44].

**Supervised fine-tuning.** We adopt Llama-2-7B, CodeLlama-Python-7B, and DeepSeek-Coder-Base-6.7B as the base models and fine-tune all the base models for 2 epochs using 32 NVIDIA A100-80GB GPUs. We set the initial learning rate, minimum learning rate, and optimizer warmup steps and at 1e-4, 6e-6, and 15, respectively. We use Adam optimizer [28] and choose a batch size of 2 with a sequence length of 8192.

### Evaluation on code generation task

**Results on python code generation.** We first access HumanEval and MBPP to evaluate the capability of the _AlchemistCoder_ series for Python code generation. These benchmarks necessitate models to generate code based on the function definitions and subsequently pass the test cases. Models are evaluated in zero-shot on HumanEval and 3-shot on MBPP. The comprehensive comparisons in Tab. 1 and Fig. 1 demonstrate the impressive capabilities of _AlchemistCoder_ models. From the results, _AlchemistCoder-L_ attains a remarkable performance boost of 42.7% and 28.4% pass@1 scores on HumanEval and MBPP respectively, compared to Llama 2-7B. Notably, _AlchemistCoder-DS_ elevates the pass@1 scores to 79.9% and 77.0% on these benchmarks, holding an overall improvement of 33.3%. Moreover, our _AlchemistCoder_ series with 7B parameters outperforms larger models (_e.g._,WizardCoder-CL-34B and CodeLlama-Instruct-70B) and rivals with GPT-3.5-Turbo, significantly bridging the performance gap between closed-source and open-source models.

**Results on multilingual code generation.** We compare the pass@1 accuracy of the base models and the corresponding fine-tuned _AlchemistCoder_ models on Humaneval-X [49]. The results presented in Tab. 2 demonstrate that the _AlchemistCoder_ series exhibits great improvements (exceeding 50%) for multilingual code generation, delivering comprehensive code capabilities.

**Results on code generation for data science.** We further conduct the evaluation of data science code completion on DS-1000 [21]. According to Tab. 3, _AlchemistCoder_ models exhibit a notable improvement of up to 19.2% in overall performance compared to the base models. Particularly,

\begin{table}
\begin{tabular}{l|l l l l l l} \hline \hline
**Model** & **Python** & **C++** & **Go** & **Java** & **JS** & **Avg** \\ \hline \hline \multirow{2}{*}{Llama 2} & 14.0 & 11.0 & 6.1 & 11.0 & 14.0 & 11.2 \\  & 31.7 & 27.4 & 12.8 & 25.6 & 32.9 & 26.1 \\  & **4chemistCoder-L** & **56.7** & **31.1** & **25.6** & **45.1** & **41.5** & **37.1** \\ \hline \multirow{2}{*}{CodLlama-Python} & 37.8 & 33.5 & 30.5 & 36.4 & 35.4 & 35.4 \\  & 68.3 & 47.6 & 36.9 & 34.8 & **57.9** & 49.6 \\  & **74.4** & **53.1** & **42.7** & **64.0** & 52.4 & **57.3** \\ \hline \multirow{2}{*}{DeepSeek-Coder-Base} & 47.6 & 45.1 & 38.4 & 56.1 & 43.9 & 46.2 \\  & 72.6 & **63.4** & 51.8 & 70.7 & 66.5 & 65.0 \\ \hline \multirow{2}{*}{_AlchemistCoder-DS_} & **79.9** & 62.2 & **59.8** & **72.0** & **68.9** & **68.6** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of pass@1 on HumanEval-X. Table 3: Pass@1 results of models with 6.7B/7B We present the multilingual code capabilities of our _AlchemistCoder_ with the respective base models and competitors (6.7B/7B).

\begin{table}
\begin{tabular}{l|l l l l l l l} \hline \hline
**Model** & **Params** & **Base Model** & **HumanEval (+)** & **MBPP (+)** & **Average (+)** \\ \hline \hline \multicolumn{6}{l}{_Closed-source Models_} \\ \hline GPT-3.5-Turbo [33] & - & - & 72.6 (65.9) & 81.7 (69.4) & 77.2 (67.7) \\ GPT-4:Turbo [34] & - & - & **85.4 (81.7)** & **83.0 (70.7)** & **84.2 (76.2)** \\ \hline \multicolumn{6}{l}{_Open-source Models_} \\ \hline Llama 2-Chat [40] & 70B & Llama 2 & 31.7 (26.2) & 52.1 (38.6) & 41.9 (32.4) \\ CodeLlama-Python [35] & 70B & Llama 2 & 57.9 (50.0) & 72.4 (52.4) & 65.2 (51.2) \\ CodeLlama-Instruct [35] & 70B & CodeLlama & **65.2 (58.5)** & **73.5 (55.1)** & **69.4 (56.8)** \\ \hline \multirow{2}{*}{CodElama-Python [35]} & 34B & Llama 2 & 51.8 (43.9) & 67.2 (50.4) & 59.5 (47.2) \\ WizardCoder-CL [30] & 34B & CodeLlama-Python & 73.2 (56.7) & 73.2 (51.9) & 73.2 (54.3) \\ DeepSeek-Coder-Instr [14] & 33B & DeepSeek-Coder-Base & **78.7 (67.7)** & **78.7 (59.7)** & **78.7 (63.7)** \\ \hline \multirow{2}{*}{StarCoder [22]} & 15B & - & 34.1 (33.5) & 55.1 (43.4) & 44.6 (38.5) \\ CodeLlama-Python [35] & 13B & Llama 2 & 42.7 (36.6) & 61.2 (**45.6)** & 52.0 (41.1) \\ WizardCoder-SC [30] & 15B & StarCoder & **51.9 (45.7)** & **61.9 (44.9)** & **56.9 (45.3)** \\ \hline Llama 2 [40] & 7B & - & 14.0 (10.4) & 26.1 (17.5) & 20.1 (14.0) \\ StarCoder [22] & 7B & - & 24.4 (21.3) & 33.1 (29.2) & 28.8 (25.3) \\ CodeLlama-Python [35] & 7B & Llama 2 & 37.8 (33.5) & 57.6 (42.4) & 47.7 (38.0) \\ WizardCoder-CL [30] & 7B & CodeLlama-Python & 48.2 (42.1) & 56.6 (42.4) & 52.4 (42.3) \\ DeepSeek-Coder-Base [14] & 6.7B & - & 47.6 (41.5) & 70.2 (53.6) & 58.9 (47.6) \\ Magicoder-CL [44] & 7B & CodeLlama-Python & 60.4 (49.4) & 64.2 (46.1) & 62.3 (47.8) \\ Magicoder-CL [44] & 7B & CodeLlama-Python & 70.7 (60.4) & 68.4 (49.1) & 69.6 (54.8) \\ Magicoder-DS [44] & 6.7B & DeepSeek-Coder-Base & 66.5 (55.5) & 74.4 (55.6) & 71.0 (55.6) \\ DeepSeek-Coder-Instruct [14] & 6.7B & DeepSeek-Coder-Base & 73.8 (69.5) & 72.7 (55.6) & 73.3 (62.6) \\ MagicoderS-DS [44] & 6.7B & DeepSeek-Coder-Base & 76.8 (65.2) & 75.7 (56.1) & 76.3 (60.7) \\ \hline \multicolumn{6}{l}{_AlchemistCoder-L (ours)_} & 7B & Llama 2 & 56.7 (52.4) & 54.5 (49.6) & 55.6 (51.0) \\ \multicolumn{6}{l}{_AlchemistCoder-CL (ours)_} & 7B & CodeLlama-Python & 74.4 (68.3) & 68.5 (55.1) & 71.5 (61.7) \\ \multicolumn{6}{l}{_AlchemistCoder-DS (ours)_} & 6.7B & DeepSeek-Coder-Base & **79.9 (75.6)** & **77.0 (60.2)** & **78.5 (67.9)** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results of pass@1 on HumanEval (HumanEval+) and MBPP (MBPP+) benchmarks. We report the results of HumanEval and MBPP consistently from the EvalPlus [27] and the **bold** scores denote the best performance among models of the same size.

_AlchemistCoder-CL_ achieves an astonishing overall accuracy of 40.3% with relatively better performance in all libraries, demonstrating powerful capabilities in data science workflows.

### Ablation study

**The Recipe of _AlchemistPrompts_.** As illustrated in Sec. 2.2, _AlchemistPrompts_ can further align the instructions and responses of data samples and harmonize the domain gap between multiple sources. Code data from different sources may vary significantly in language style and content, including question types, code style, presence of comments, test cases, etc. Therefore, multi-source data mixing is a double-edged sword: it provides necessary diversity but can also bring large domain gaps. Adding concise corpus generated from the same Alchemist model (_i.e._, _AlchemistPrompts_ with similar language styles) to a small amount of data can effectively bridge this gap while maintaining diversity. To find the appropriate recipe of _AlchemistPrompts_ that maintains a balance between data diversity and domain gap, we conduct ablation experiments on the proportion (0% to 20%) of data customized by _AlchemistPrompts_. We adopt two settings: (a) augment the original data with its customized variant and report the results of fine-tuning for 2 epochs on CodeLlama-Python-7B; (b) replace the original data and report the results of fine-tuning for the same steps (_i.e._, keeping the number of tokens used consistent). As shown in Fig. 7, _AlchemistCoder_ is particularly enhanced when the proportion of customized data increases from 1% to 5%, and nearly peaks in performance at 5%. Thus, we introduce _AlchemistPrompts_ into 5% of the training set to balance the performance gain and the generation cost. Additionally, both two strategies effectively enhance the performance and validate the efficacy of our approach. To push the limit of _AlchemistCoder_, we employ the augmentation strategy in our performance experiments. In addition, we present detailed experimental results from the multi-source integration and harmonization process in our Appendix SSC to offer a more in-depth demonstration of the _AlchemistPrompts_ efficacy as data complexity scales.

**Efficacy of the code comprehension tasks.** We conduct an ablation study on the key components of the code comprehension tasks to ascertain their individual contributions to the overall performance. As reported in Tab. 4, compared to the baselines (the first and second rows), the model demonstrates

\begin{table}
\begin{tabular}{c c c c c c|c c} \hline \hline
**Multi-source** & **Data** & _AlchemistPrompt_ & **Instruction** & **Data** & **Code** & **HumanEval** & **MBPP** \\
**Integration** & **Decontamination** & **Harmonization** & **Evolution Task** & **Filtering Task** & **Review Task** & **(Pass@1)** & **(Pass@1)** \\ \hline - & - & - & - & - & - & - & 37.8 & 57.6 \\ ✓ & - & - & - & - & - & 54.6 (16.8\(\uparrow\)) & 57.9 (0.3\(\uparrow\)) \\ ✓ & ✓ & - & - & - & - & 59.8 (5.2\(\uparrow\)) & 58.2 (0.3\(\uparrow\)) \\ ✓ & ✓ & ✓ & - & - & - & 72.0 (12.2\(\uparrow\)) & 63.4 (5.2\(\uparrow\)) \\ ✓ & ✓ & ✓ & ✓ & - & - & 71.3 (0.7\(\downarrow\)) & 65.8 (2.4\(\uparrow\)) \\ ✓ & ✓ & ✓ & ✓ & ✓ & - & 73.8 (2.5\(\uparrow\)) & 67.7 (1.9\(\uparrow\)) \\ ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & **74.4** (**0.6\(\uparrow\)) & **68.5** (0.8\(\uparrow\)) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation study on the effectiveness of multi-source harmonization (_i.e._, Multi-source Integration, Data Decontamination, and _AlchemistPrompt_ Harmonization) and code understanding tasks (_i.e._, Instruction Evolution Task, Data Filtering Task, and Code Review Task) for the _AlchemistCoder-CL-7B_ model, evaluated on the HumanEval and MBPP benchmarks.

Figure 7: Ablation study on the proportion of _AlchemistPrompt_-customized data conducted on _AlchemistCoder-CL-7B_. Left: Augment the original data. Right: Replace the original data.

enhanced performance on both benchmarks following the incremental incorporation of code comprehension task data. Notably, the improvement (5.1% regard to the pass@1 metric) is particularly remarkable on MBPP. This indicates the significant contribution of all code comprehension tasks to furthering programming capabilities.

### Analytical study

_AlchemistPrompts_** harmonize the discrepancy between instructions and responses.** To in-depth verify the efficacy of _AlchemistPrompts_, we calculate the Conditional Perplexity Discrepancy (CPD, refer to Sec. 2.2) values of our fine-tuning data harmonized by _AlchemistPrompts_, _i.e._, the difference between \(\mathrm{Perplexity}(\mathrm{conditional\_instruction}+\mathrm{response})\) and \(\mathrm{Perplexity}(\mathrm{response})\). The CPD value quantifies the difficulty change in generating responses before and after adding specific inputs (_e.g._, instructions) to the model (the smaller the value, the easier it becomes). Specifically, we adopt the instructions before and after customization by _AlchemistPrompts_ for comparison, and provide the Kernel Density Estimation of CPD in Fig. 8. Clearly, the latter (green) gains smaller overall CPD values, indicating that _AlchemistPrompts_ are beneficial for prediction and can provide effective contextual information. Furthermore, we randomly select 10 groups of these samples and use UMAP [31] to map their feature representations into a 2-D space in the right of Fig. 8. From the fact that the solid lines are generally shorter than the dashed lines, our _AlchemistPrompts_ can harmonize the discrepancy between instructions and responses, leading to higher-quality data for attaining improved instruction-following ability.

_AlchemistCoder_** models are better generalists.** To further analyze the comprehensive capabilities of our _AlchemistCoder_, we conduct evaluations on more diversified benchmarks, including MMLU [15] for multitask language understanding, BBH [37] for comprehensive reasoning, and GSM8K [8] for mathematical ability. The results are presented in Tab. 5 and illustrate that the _AlchemistCoder_ models exhibit an overall performance increase of 6.4%, 13.6%, and 14.5% over the base models Llama 2, CodeLlama-Python, and DeepSeek-Coder-Base, respectively. Notably, CodeLlama-Python presents inferior performance on these benchmarks relative to Llama 2, indicating the discrepancy between natural language processing and code capabilities of open-source models. Such divergence can be attributed to "catastrophic forgetting" [11; 29; 20], often occurring when fine-tuning is exclusively concentrated on domain-specific data. By leveraging harmonized multi-source data, our _AlchemistCoder_ series models achieve enhanced reasoning abilities, better instruction-following abilities, and improved context understanding, which contribute to develop better generalists.

**Error case analysis.** To meticulously dissect the improvements brought by our method, we provide an analysis of error cases on HumanEval and MBPP. We compare models before and after the introduction of _AlchemistPrompts_ and code understanding task data. The bar chart shown in Fig. 9 (top) indicates that these two types of key data help to better handle compilation errors (_i.e._, SyntaxError, NameError, and ValueError), and eliminate the occurrence of no code written in the responses. On the other hand, the results of Fig. 9 (bottom) on MBPP suggest that the _AlchemistCoder_ series incorporated with these two types of data attains stronger programming logic, as evidenced by the clear reduction in the 'Wrong Answer' error cases.

Figure 8: In-depth analysis of the efficacy from _AlchemistPrompts_. Left: Kernel Density Estimation of Conditional Perplexity Discrepancy. Right: UMAP visualization of 10 instruction/response groups.

## 4 Related Work

**Code large language models.** Early researches [5; 32; 22] focus on collecting massive amounts of code data to develop pretrained Code LLMs. Recent efforts [30; 47; 44] are dedicated to fine-tuning these pretrained models with specific instructional data to further the coding abilities. For instance, WizardCoder [30] and Magicoder [44] construct their instruction tuning datasets based on CodeAlpaca [4] and the stack [18] dataset, respectively. In this work, we develop the _AlchemistCoder_ series by instruction tuning on optimized multi-source data instead of single-category data as in previous methods, endowing astonishing and harmonized code capability.

**Instruction tuning.** Instruction tuning aims to enhance LLMs via fine-tuning pre-trained LLMs using samples of instruction/response pairs. Obtaining high-quality data for instruction tuning is typically challenging and extensive works have been dedicated to this endeavor. For instance, Alpaca [38] employs self-instruct [42] to generate instruction-following demonstrations. WizardLM [45] introduces Evol-Instruct and transforms the instruction data into more complex variants. In addition to Evol-Instruct, we also incorporate the data construction process itself as a form of data into the training. Moreover, although previous works [16; 43; 23; 16] utilize multiple fine-tuning datasets, we harmonize multi-source data at a fine-grained level.

**Learning from hindsight.** The concept of learning from hindsight [26] has been explored in goal-conditioned learning [17; 12]. Hindsight Experience Replay (HER) [1] is designed to re-label rewards and facilitate learning from sparse feedback. Korbak _et al._[19] study the influence of human preferences during pre-training, showing improved performance when models are aligned with human preferences. Previous work primarily serves as an alternative to RLFT, utilizing HER to leverage (suboptimal) historical data for model learning. We focus on harmonizing the inherent conflicts within multi-source data through hindsight, to fully tap into the potential of base models.

## 5 Conclusion

In this paper, we propose an effective framework for integrating multi-source data to fine-tune Code LLMs, addressing the limitations in quality and diversity inherent within a single-source dataset. This is a non-trivial paradigm and we pioneer to unveil inherent conflicts in multi-source code corpora. To resolve this challenge, we innovatively design data-specific _AlchemistPrompts_, inspired by hindsight relabeling. Additionally, we make the first effort of integrating the data construction process as code comprehension tasks into the training process. These key concepts enhance the diversity, quality, and cost-effectiveness of code fine-tuning data, facilitating the development of the _AlchemistCoder_ series models with significantly improved and comprehensive coding capabilities.

## 6 Acknowledgments

This work is supported by National Natural Science Fund of China (62076184, 62473286) in part by Shanghai Natural Science Foundation (22ZR1466700). Besides, this project is funded in part by the Hong Kong Generative AI Research and Development Center (HKGAI) under the Innovation and Technology Commission (ITC)'s InnoHK. Dahua Lin is a PI of HKGAI under the InnoHK.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline
**Model** & **MMLU** & **BBH** & **GSM8K** & **Avg** \\ \hline Llama 2 & 41.1 & 34.6 & 16.8 & 30.8 \\ Code.lama & 31.5 & **42.7** & 14.4 & 29.5 \\ _AlchemistCoder-L_ & **43.9** & **42.7** & **25.0** & **37.2** \\ \hline Cold.lama-Python & 26.1 & 26.7 & 6.6 & 19.8 \\ Magicoder-CL & 33.0 & **41.5** & 18.8 & 31.1 \\ _AlchemistCoder-CL_ & **42.1** & 39.3 & **20.2** & **33.9** \\ \hline DeepSeek-Coder-Base & 34.0 & 12.8 & 22.0 & 22.9 \\ Magicoders-DS & 34.4 & 43.8 & 14.3 & 30.8 \\ _AlchemistCoder-DS_ & **38.5** & **45.6** & **28.0** & **37.4** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results of models (6.7B/7B) on various benchmarks, including MMLU for multitask language understanding, BBH for comprehensive reasoning, and GSM8K for mathematical ability.

Figure 9: Analysis of error case proportions on HumanEval (top) and MBPP (bottom). \({}^{\blacktriangle}\) represents the models fine-tuned without _AlchemistPrompts_ and the code comprehension task data.

## References

* [1]M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba (2017) Hindsight experience replay. Advances in neural information processing systems30. Cited by: SS1.
* [2]J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. (2021) Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Cited by: SS1.
* [3]Z. Cai, M. Cao, H. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, P. Chu, et al. (2024) Interlmlm2 technical report. arXiv preprint arXiv:2403.17297. Cited by: SS1.
* [4]S. Chaudhry (2023) Code alpaca: an instruction-following llama model for code generation. Cited by: SS1.
* [5]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. (2021) Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cited by: SS1.
* [6]W. Chen, X. Ma, X. Wang, and W. W. Cohen (2022) Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks. CoRRabs/2211.12588. Cited by: SS1.
* [7]X. Chen, M. Lin, N. Scharli, and D. Zhou (2023) Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. Cited by: SS1.
* [8]K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Junc, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. (2021) Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Cited by: SS1.
* [9]codefuse ai. The codeexercise-python-27k dataset. Note: https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k External Links: Link Cited by: SS1.
* [10]codefuse ai. The evolionstrucode dataset. Note: https://huggingface.co/datasets/codefuse-ai/Evol-instruction-66k External Links: Link Cited by: SS1.
* [11]G. Dong, H. Yuan, K. Lu, C. Li, M. Xue, D. Liu, W. Wang, Z. Yuan, C. Zhou, and J. Zhou (2023) How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492. Cited by: SS1.
* [12]D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, et al. (2022) Red teaming language models to reduce harms: methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858. Cited by: SS1.
* [13]S. Gunasekar, Y. Zhang, J. Aneja, C. C. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, et al. (2023) Textbooks are all you need. arXiv preprint arXiv:2306.11644. Cited by: SS1.
* [14]D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al. (2024) Deepseek-coder: when the large language model meets programming-the rise of code intelligence. arXiv preprint arXiv:2401.14196. Cited by: SS1.
* [15]D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2020) Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Cited by: SS1.
* [16]H. Ivison, Y. Wang, V. Pyatkin, N. Lambert, M. Peters, P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy, et al. (2023) Camels in a changing climate: enhancing lm adaptation with tub 2. arXiv preprint arXiv:2311.10702. Cited by: SS1.
* [17]L. P. Kaelbling (1993) Learning to achieve goals. In IJCAI, Vol. 2, pp. 1094-8. Cited by: SS1.
* [18]D. Kocetkov, R. Li, L. Ben Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. (2022) The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533. Cited by: SS1.
* [19]T. Korbak, K. Shi, A. Chen, R. Vinayak Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez (2023) Pretraining language models with human preferences. In International Conference on Machine Learning, pp. 17506-17533. Cited by: SS1.
* [20]S. Kotha, J. Mitchell Springer, and A. Raghunathan (2023) Understanding catastrophic forgetting in language models via implicit inference. arXiv preprint arXiv:2309.10105. Cited by: SS1.
* [21]Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W. Yih, D. Fried, S. Wang, and T. Yu (2023) Ds-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [22]R. Li, L. Ben Allal, Y. Zi, N. Muennighoff, D. Koetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. (2023) StarCoder: may the source be with you!. arXiv preprint arXiv:2305.06161. Cited by: SS1.
* [23]R. Li, L. Ben Allal, Y. Zi, N. Muennighoff, D. Koetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. (2023) StarCoder: may the source be with you!. arXiv preprint arXiv:2305.06161. Cited by: SS1.
* [24]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Li, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [25]Y. Li, L. Ben Allal, Y. Zi, N. Muennighoff, D. Koetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. (2023) StarCoder: may the source be with you!. arXiv preprint arXiv:2305.06161. Cited by: SS1.
* [26]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [27]Y. Li, L. Ben Allal, Y. Zi, N. Muennighoff, D. Koetkov, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) StarCoder: may the source be with you!. arXiv preprint arXiv:2305.06161. Cited by: SS1.
* [28]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [29]Y. Li, L. Ben Allal, Y. Zi, N. Muennighoff, D. Koetkov, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) StarCoder: may the source be with you!. arXiv preprint arXiv:2305.06161. Cited by: SS1.
* [30]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [31]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [32]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [33]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [34]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [35]Y. Li, L. Ben Allal, Y. Zi, N. Muennighoff, D. Koetkov, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) StarCoder: may the source be with you!. arXiv preprint arXiv:2305.06161. Cited by: SS1.
* [36]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The 3D-1000: a natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. Cited by: SS1.
* [37]Y. Li, L. Ben Allal, J. Li, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Mou, C. Aki, J. Chim, et al. (2023) The* [23] Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, and Stan Z Li. Semireward: A general reward model for semi-supervised learning. _arXiv preprint arXiv:2310.03013_, 2023.
* [24] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. _Science_, 378(6624):1092-1097, 2022.
* [25] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In _ICRA_, pages 9493-9500. IEEE, 2023.
* [26] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. _arXiv preprint arXiv:2302.02676_, 3, 2023.
* [27] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. _arXiv preprint arXiv:2305.01210_, 2023.
* [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [29] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. _arXiv preprint arXiv:2308.08747_, 2023.
* [30] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. WizardCoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_, 2023.
* [31] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. _arXiv preprint arXiv:1802.03426_, 2018.
* [32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. _arXiv preprint arXiv:2203.13474_, 2022.
* [33] OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt/, 2022.
* [34] OpenAI. Gpt-4 technical report. Technical report, 2023.
* [35] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code lama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* [36] Demin Song, Honglin Guo, Yunhua Zhou, Shuhao Xing, Yudong Wang, Zifan Song, Wenwei Zhang, Qipeng Guo, Hang Yan, Xipeng Qiu, et al. Code needs comments: Enhancing code llms with comment augmentation. _arXiv preprint arXiv:2402.13013_, 2024.
* [37] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.
* [38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
* [39] theblackcat102. The evolved code alpaca dataset. https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1, 2023.
* [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [41] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. _arXiv preprint arXiv:2309.11235_, 2023.
* [42] Yizhong Wang, Yeganan Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hananeh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* [43] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. _arXiv preprint arXiv:2306.04751_, 2023.
* [44] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. _arXiv preprint arXiv:2312.02120_, 2023.
* [45] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_, 2023.

* [46] Ke Yang, Jiaeteng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai. If LLM is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. _CoRR_, abs/2401.00812, 2024.
* [47] Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishuijie Zhao, Wenxiang Hu, and Quifeng Yin. Waxecoder: Widespread and versatile enhanced instruction tuning with refined data generation. _arXiv preprint arXiv:2312.14187_, 2023.
* [48] Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E Gonzalez. The wisdom of hindsight makes language models better instruction followers. _arXiv preprint arXiv:2302.05206_, 2023.
* [49] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. _arXiv preprint arXiv:2303.17568_, 2023.

## Appendix

In the Appendix sections, we discuss limitations (SSA), ethical considerations and broader impacts (SSB), additional experimental results (SSC), benchmark and evaluation details (SSD), and more details of our _AlchemistCoder_ fine-tuning data (SSE).

## Appendix A Limitations

Currently, GPT-4 holds an advantage in generating high-quality responses, and thus has been chosen as our _Alchemist_ model. Compared to methods that heavily rely on GPT-4 to generate entire new datasets, we have been striving to minimize our dependence on GPT-4. Instead of using GPT-4 to generate data from scratch, we optimize a small amount of data. In the ablation experiments shown in Figure 7, we have verified that achieving optimal performance only requires using GPT-4 to generate _AlchemistPrompts_ for 5% of the data. Furthermore, the generation tasks we designed only require very short responses (less than 50 words), significantly reducing the token usage of GPT-4. Despite these efforts, the generation of _AlchemistPrompts_ is still a significant cost. We will explore fine-tuning open-source models to achieve the free generation of _AlchemistPrompts_ in the future. For data bias, Our _AlchemistPrompts_ enhance the instruction-following abilities of models, which can potentially mitigate biases. For example, if the model has a bias towards responding with Python code when the programming language is not specified, the inclusion of programming language declarations in _AlchemistPrompts_ helps to alleviate this bias. We have not delved into the aspect of data bias yet and will explore it in the future.

## Appendix B Ethical Considerations and Broader Impacts

We use publicly available datasets, benchmarks, and models for training and evaluation, free from any possible harm toward individuals or groups. The generated data are relevant to code-related tasks and no personal identification information is involved. Furthermore, we adopt ChatGPT to polish the writing and assist with language. For broader impacts, _AlchemistCoder_ enhances code generation and generalization through multi-source fine-tuning, promising improved software development efficiency, democratization of programming, and educational benefits. However, it also raises concerns about malicious use, intellectual property issues, and skill degradation. To ensure the responsible release of _AlchemistCoder_ models, we will implement controlled access, provided usage guidelines, and engaged with the research community, thereby mitigating the risks of misuse or dual-use.

## Appendix C Additional Experimental Results

### Details of fine-tuning tokens

In Tab. A1, we provide details of the training corpus used for fine-tuned Code LLMs.

### Data complexity and multi-source integration

Our research demonstrates that integrating data from multiple sources significantly increases data complexity and diversity, as evidenced by the broader distributions of code and description lengths shown in Figure 6 of the manuscript. While this integration facilitates the model's ability to learn richer feature representations, it also heightens the demands on the model to manage inputs of varying styles, formats, and quality. _AlchemistCoder_ addresses this challenge by introducing _AlchemistPrompts_, which help conduct harmonizations across various data sources and within instruction-response pairs. To offer a more in-depth demonstration of the _AlchemistPrompts_ efficacy as data complexity scales, we present detailed experimental results from the multi-source integration and harmonization process in Tab. A2

## Appendix D Benchmark and Evaluation Details

[MISSING_PAGE_EMPTY:15]

lowing prior works [49; 7; 44], we use the greedy decoding strategy and focus on comparing the pass@1 metric.

### Mhpp/mhpp+

The MBPP (Mostly Basic Python Programming) benchmark [2] consists of around 1,000 Python challenges, crowd-sourced to test basic programming skills, including fundamentals and standard library use. Aimed at beginners, each challenge offers a description, solution, and three tests for verifying solution accuracy. MBPP+ [27] is an extension of the MBPP benchmark, utilizing a subset of hand-verified problems from MBPP-sanitized to ensure tasks are well-defined and unambiguous. For the evaluation on MBPP and MBPP+, we adopt the three-shot prompt shown in Fig. A2.

### HumanEval-X

HumanEval-X [49] is a comprehensive benchmark that assesses the capabilities of code generation models across multiple programming languages, including Python, C++, Java, JavaScript, and Go. It consists of 820 meticulously created data samples, each accompanied by test cases, making it an invaluable resource for evaluating and improving multilingual code generation models. The benchmark aims to provide insights into the models' proficiency in solving diverse coding challenges and their accuracy in generating functionally correct code in different languages. For evaluation on HumanEval-X, we do not use specific prompts and follow the original test prompts.

### Ds-1000

The DS-1000 benchmark [21] adapts 1000 different data science coding problems each with unit tests from StackOverflow and checks both execution semantics and surface-form constraints. These realistic problems are drawn from seven popular data science libraries in Python, including Matplotlib (plt), NumPy (np), Pandas (pd), SciPy (scp), Scikit-Learn (sk), PyTorch (py), and TensorFlow (tf). DS-1000 has two modes: completion and insertion, and here we only evaluate completion, as the basic CodeLlama-Python does not support insertion. For evaluation on DS-1000, we do not use specific prompts and follow the original test prompts.

### Mmlu

The Massive Multitask Language Understanding (MMLU) benchmark [15] is an evaluation framework designed to measure the depth and breadth of knowledge that LLMs possess. It accomplishes this by testing these models across 57 varied tasks in both zero-shot and few-shot scenarios. The tasks encompass a wide array of topics, including basic math, American history, computer science, law, and more, challenging the models to leverage their acquired knowledge to solve complex problems. MMLU seeks to emulate the multifaceted way in which human knowledge and problem-solving skills are assessed, offering a comprehensive gauge of a model's ability to understand and apply information across multiple domains. For evaluation on MMLU, we do not use specific prompts and follow the original test prompts.

### Bbh

The BIG-Bench Hard (BBH) Benchmark [37] is a specialized evaluation framework tailored to rigorously test the capabilities of LLMs. This benchmark targets a selection of tasks that have historically proven challenging for LLMs, focusing on areas where models typically do not exceed average human performance. The BBH Benchmark aims to push the boundaries of what LLMs can achieve by emphasizing complex reasoning, deep understanding, and nuanced interpretation, setting a high bar for model development and performance evaluation. For evaluation on BBH, we do not use specific prompts and follow the original test prompts.

### Gsm8k

The GSM8K (Grade School Math 8,000) benchmark [8] serves as a rigorous evaluation framework for testing the mathematical problem-solving prowess of LLMs. This benchmark comprises a dataset of 8,500 diverse and high-quality math word problems at the grade school level, designed to challenge LLMs with tasks necessitating advanced, multi-step reasoning abilities. GSM8K's primary aim is to gauge how well these models can parse, understand, and solve math problems, thereby offering a comprehensive measure of their capacity for logical reasoning and mathematical computation. By incorporating such a specialized benchmark, researchers can better understand the extent to which LLMs can mimic human-like reasoning in solving complex mathematical scenarios. For evaluation on GSM8K, we do not use specific prompts and follow the original test prompts.

## Appendix E AlchemistCoder Dataset Details

### _AlchemistPrompt_

We provide two samples of _AlchemistPrompts_ in Fig. A4 and Fig. A5.

### Code comprehension task data

For instruction evolution task data, we provide two samples in Fig. A6 and Fig. A7. For data filtering task data, we provide two samples in Fig. A8 and Fig. A9. For code review task data, we design prompt as illustrated in Fig. A3 to obtain high-quality code review task data and we provide two samples in Fig. A10 and Fig. A11.

[MISSING_PAGE_EMPTY:18]

Figure A6: Example #1 of instruction evolution task data.

## Appendix A

## Appendix B

## Appendix B

Figure A10: Example #1 of code review task data. For clarity, the prompt for generating data is omitted.

## Appendix A

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly outline the primary contributions and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of our work in Appendix SSA. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Our work focuses on experimental research to advance the boundaries of code intelligence and does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have fully disclosed the datasets, models, and experimental procedures used in detail. Benchmarks and implementation details can be found in Sec. 3.1 and Appendix SSD. More details of our _AlchemistCoder_ fine-tuning data are provided in Appendix SSE. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided code with sufficient instructions in our supplemental material. We have released our code, data, and _AlchemistCoder_ series models at https://internlm.github.io/AlchemistCoder. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have specified all the training and test details. Benchmarks and implementation details can be found in Sec. 3.1 and Appendix SSD. More details of our _AlchemistCoder_ fine-tuning data are provided in Appendix SSB. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper provides extensive information about the statistical significance of our experiments. The factors of variability are discussed, including the impact of different data sources and the harmonization process using _AlchemistPrompts_. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The experiments compute resources have been discussed in Sec. 3.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We fully follow the ethics guidelines of NeurIPS and the ethical considerations have been discussed in Appendix SSB. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discuss both potential positive societal impacts and negative societal impacts of the work performed in Appendix SSB. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: The safeguards have been discussed in Appendix SSB. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use publicly available datasets, benchmarks, and models for training and evaluation, free from any possible harm toward individuals or groups. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The paper introduces new models that are well-documented. Documentation includes details about the dataset, code, and model, such as training procedures, licenses, limitations, and consent from data sources. We have released our code, data, and _AlchemistCoder_ series models at https://internlm.github.io/AlchemistCoder. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.