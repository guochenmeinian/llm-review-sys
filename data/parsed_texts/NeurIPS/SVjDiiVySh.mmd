# Improving CLIP Training with Language Rewrites

Lijie Fan\({}^{1,2,*}\) &Dilip Krishnan\({}^{1}\) &Phillip Isola\({}^{2}\) &Dina Katabi\({}^{2}\) &Yonglong Tian\({}^{1,*}\)

\({}^{1}\)Google Research, \({}^{2}\)MIT CSAIL, \({}^{*}\)equal contribution

###### Abstract

Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly selects either the original texts or the rewritten versions as text augmentations for each image. Extensive experiments on CC3M, CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with language rewrites significantly improves the transfer performance without computation or memory overhead during training. Specifically for ImageNet zero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.

## 1 Introduction

Pre-trained vision-language multi-modal encoders, exemplified by CLIP [44], have proven to be extremely useful in learning transferable features from paired image and text data. CLIP's training process can leverage two scalable paradigms: data and compute. Firstly, the availability of large-scale vision-language paired data [49; 48] enables effective training at a substantial scale. Secondly, CLIP's utilization of language and image co-embeddings grants it favorable scaling properties with respect to compute resources [27]. Consequently, CLIP embeddings consistently outperform other visual pre-training approaches such as SimCLR [8] or MAE [22] across various downstream tasks [27]. Follow-up methods that build upon language-image pre-training, such as SLIP [41] and FLIP [35], exhibit similar advantageous scaling performance.

The CLIP framework is built upon contrastive learning, which typically relies on data augmentations to prevent overfitting and the learning of ineffective shortcuts [8; 47]. However, in the CLIP training process, this beneficial feature is applied exclusively to image inputs, which undergo augmentations in every epoch. In contrast, text inputs are neglected and remain unchanged throughout training, lacking any form of augmentation. This input asymmetry leads to scenarios where the same text is consistently paired with slightly augmented images, while the augmented version of the same image is always paired with the exact same words. Such asymmetry presents two issues. Firstly, the image encoders receive limited supervision from the language side since the same image is consistently paired with the same words. Consequently, the language aspect provides less guidance to the imageencoders. Secondly, the text encoders repeatedly encounter the exact same texts in each epoch, which increases the risk of text overfitting and significantly impacts zero-shot transferability.

Hence, it becomes crucial to incorporate a suitable augmentation strategy for the text inputs. However, existing language augmentation methods are not sufficiently effective. Most previous approaches [58] focus on word-level treatments like replacement or masking, which have limited impact on enriching text structures and are comparatively weaker than image augmentations. Currently, there is a lack of language rewriting strategies that can effectively augment sentences while preserving key concepts and meanings. Such strategies are urgently needed for CLIP training to ensure optimal performance.

Meanwhile, alongside the development of CLIP, the field has witnessed significant advancements in large language models (LLMs) like GPT models [5; 44; 45] and LaMDA [3]. These LLMs have seen tremendous growth in terms of data, computational resources, and performance. Instruction fine-tuned models such as ChatGPT [2] and Bard [1] have also emerged, incorporating fine-tuning through supervised and reinforcement learning. These models have exhibited exceptional performance surpassing human capabilities across a wide range of natural language tasks. Motivated by these advancements, we naturally explored the potential of leveraging LLMs to effectively generate diverse rewritten versions of a given text. While a straightforward approach would involve directly employing instruction fine-tuned models like ChatGPT or Bard, their closed-source nature renders it infeasible to use them for rewriting the hundreds of millions of image descriptions in the datasets. Fortunately, open-sourced LLMs such as LLaMA [56], despite lacking fine-tuning with instructions, possess excellent In-Context Learning (ICL) capabilities, enabling predictions with a limited context. By thoughtfully designing contextual examples, LLaMA can generate diverse and rich text rewrites for the entire dataset.

Building upon this foundation, we propose Language augmented CLIP (LaCLIP), a straightforward yet highly effective approach for enhancing CLIP model performance by harnessing the power of LLMs. Our method leverages ICL using LLaMA to generate diverse variants of each caption within the text-image pairs of a given pre-training dataset. To facilitate ICL prompts, we have devised multiple strategies to generate a small set of meta-input-output caption pairs. These strategies involve utilizing ChatBots, human rewriters, or existing image captioning datasets. Once we have acquired the meta-input-output pairs, we employ them as examples to prompt LLaMA, enabling the rewriting of millions of texts within the entire dataset. Unlike existing strategies for text rewriting [58; 50], which tend to preserve the sentence structure, LLMs exhibit the remarkable ability to generate language rewrites with greater richness and diversity. This is attributed to their emergent properties and extensive training data. Following the caption rewriting process conducted by LLaMA ICL, each image is now accompanied by a collection of diverse captions resulting from the rewriting process. Utilizing these rewritten texts, we proceed to train CLIP models with augmentation also on the text side. The text augmentation could be performed by randomly selecting one out of the many captions associated with each image.

Extensive experiments on various pretraining datasets at different scales demonstrate our proposed LaCLIP could significantly improve the transferability of CLIP. For instance, on the LAION-400M dataset [49], we observe a notable improvement over CLIP in the zero-shot performance on ImageNet, increasing from 62.0% to 64.4%. We firmly believe that this strategy presents a simple, scalable approach that contributes to the array of training strategies available for training image embeddings.

## 2 Related Works

**Vision-Language models.** There are a number of earlier works demonstrating the effectiveness of learning visual representations from the supervision of corresponding text [25; 31; 14; 62]. Contrastive Language-Image Pretraining (CLIP) [44] has attracted significant attention due to its superior representation learning and zero-shot transfer ability. This performance is achieved through contrastive learning on top of image and text features. Another related approach is ALIGN [24], which achieves similar performance with larger and noisier datasets. There have been numerous follow-up works that attempt to improve the efficacy and data efficiency of CLIP training. SLIP [41] and DC-CLIP [34] proposes to improve the performance by incorporating self-supervised training techniques. FILIP [60] proposes to leverage cross-modal fine-grained alignment between image patches and text words. CoCa [61] introduces an additional decoder and captioning loss. LIT [63] proposes to boost zero-shot transferring performance by fine-tuning the text encoders. BLIP series [33; 32] include additional captioners and incorporates iterative image captioning within the training pipeline, which intricately link the generated captions with the image content. However, most of these follow-up works introduce additional training inputs and losses, which can have a negative impact on training efficiency and memory consumption.

**Text augmentation and Large Language Models.** The success of natural language processing (NLP) tasks is strongly dependent on the quality and quantity of available data. Deep neural networks benefit from more data [40; 58]. Common practices in data augmentation include synonym replacement [58], random masking [30], and back translation [50]. The advent of self-supervised large language models like BERT [16] and GPT series [45; 46] has been a game-changer, as they do not require labeled data and can scale up to web-level data to achieve superior transfer ability. Recently, even larger foundation models with billion-level parameters have emerged, revolutionizing the NLP community. Models like the 175B GPT-3 [5] and 540B PaLM [10] achieve superior performance on various NLP tasks. GPT-3 also demonstrates the few-shot in-context learning ability of large language models [5]. Open-sourced LLaMA [56] also achieve comparable performances on various benchmarks. In addition to them, ChatGPT and Bard are chatbots trained with reinforcement learning human feedback (RLHF), and have achieved human-comparable performances on various language understanding tasks [6].

## 3 Improving CLIP with Language Rewrites

This section outlines the core design of our LaCLIP framework, highlighting the key components and strategies involved. We provide a comprehensive description of our approach, including the generation of a small set of meta-input-output text pairs from diverse sources, the process of rewriting image descriptions across the entire dataset using LLM ICL, and the enhanced CLIP training strategies incorporating these rewritten descriptions.

### Preliminary

**CLIP.** The Contrastive Language-Image Pretraining (CLIP) method has proven to be highly effective to train vision models using language supervision. In this framework, a large batch of \(N\) paired images and text \(\{x_{I},x_{T}\}\) are sampled from the training dataset during each training step. The images are pre-processed using data augmentations, and image and text features are extracted using dedicated encoders and normalization functions \(f_{I}\) and \(f_{T}\). The image text features are used to compute the InfoNCE loss, where the paired images and text form the positive pairs and the unpaired ones are treated as negative samples. The training loss can be formulated as follows:

\[L_{I}=-\sum_{i=1}^{N}\log\frac{\exp\left(\text{sim}(f_{I}(\text{aug}_{I}(x_{I} ^{i})),f_{T}(x_{T}^{i}))/\tau\right)}{\sum_{k=1}^{N}\exp\left(\text{sim}(f_{I}( \text{aug}_{I}(x_{I}^{i})),f_{T}(x_{T}^{k}))/\tau\right)},\] (1)

In this scenario, \((x_{I}^{i},x_{T}^{i})\) is the \(i^{th}\) image-text pair, and \(\text{aug}_{I}()\) denotes the image augmentation functions. The \(\text{sim}(\cdot,\cdot)\) function measures distance using the dot product, while the temperature \(\tau\) is a learnable parameter that scales the logits. To simplify the presentation, we only show the training loss iterating over images. A symmetrical loss \(L_{T}\) that iterates over texts is also computed during the training process. The total training loss is \(L=(L_{I}+L_{T})/2\).

**Language Rewrites as Text Augmentation.** In Equation 1, the standard CLIP loss applies augmentation exclusively to images, leaving the text inputs unchanged throughout the whole training process. Recognizing this gap, we propose to generate text augmentations, denoted as \(\text{aug}_{T}\), where \(\text{aug}_{T}(x_{T})\) is utilized as the input for \(f_{T}\) instead of the original \(x_{T}\). In the following subsections, we introduce the methodology employed to generate these text augmentations using LLMs, as well as the integration process during CLIP training. By addressing this gap, we aim to enhance the training process and expand the benefits of augmentation to the text inputs, leading to improved performance and a more comprehensive learning framework.

### Meta-Input-Output Text Pair Generation

A recently uncovered property of autoregressive LLMs is In-Context Learning (ICL) [5; 39]. This property allows LLMs to learn a new task by conditioning on a few examples and then make predictions for a test input. To harness ICL for text rewriting, we first need to generate several rewriting examples that can be used as examples in the prompt context. We name these examples meta-input-output text pairs. To this end, we explored different strategies for generating these pairs:

* **Rewriting with Chatbots.** We randomly sample texts from image-text datasets, and prompt ChatGPT[2] and Bard[1] web portals to generate target texts using a prompt such as "Rewrite this caption of an image vividly, and keep it less than thirty words:". Illustrations of this process can be found in Figure 1. Here we leverage the extremely powerful rewriting abilities of these models to provide modified captions that keep the essence of the original caption but change the style and details. This ensures that the semantics associated with the corresponding image do not change, which is important for representation learning purposes.
* **MSCOCO Sampling.** Multiple text descriptions for the same image are available in many existing image captioning datasets. To leverage this characteristic, we utilize the widely used MS-COCO dataset [36]. Within this dataset, each image is associated with five distinct text descriptions, which have been meticulously annotated by human workers. From this dataset, we randomly select a subset of images. For each selected image, we choose one description as the meta-input text and another as the meta-output text.
* **Human Rewriting.** We randomly sample several image-text pairs from various image-text datasets. To ensure diverse and varied textual variations, we engage human annotators and task them with rewriting the captions based on the content depicted in the corresponding observed images. This process results in the creation of meta-input-output pairs, consisting of the original text and the rewritten version by human annotators.

Through the utilization of diverse generation strategies, we acquire four distinct types (_ChatGPT_, _Bard_, _COCO_, and _Human_) of meta-input-output text pairs, which then serve as valuable examples within the input context for the In-Context Learning (ICL) framework. For each specific strategy, we randomly select 16 original captions from image-text datasets and generate target captions using that strategy, resulting in a total of 16 meta-input-output pairs. These pairs encompass a range of sources and variations, facilitating a comprehensive and diverse training experience for our framework.

### Large scale Language Rewriting

Generating rewrites for hundreds of millions of texts using closed-source models like ChatGPT or Bard is impractical due to the significant financial and time costs associated with API usage. Therefore, to facilitate the rewriting of text samples within any given image-text dataset, we employ LLaMA [56]--an open-source state-of-the-art large language model known for its robust performance in text completion tasks. Despite not being fine-tuned with instructions, LLaMA exhibits exceptional ICL capabilities. Leveraging the meta-input-output text pairs generated as described in Section 3.2, we employ LLaMA's ICL ability to rewrite every text entry within the image-text dataset.

Given a text sample to be rewritten, we formulate a context input as the following three parts: Firstly, we include a sentence that informs the LLM about the task of rewriting image descriptions. This serves as an initial contextual clue for the LLM to understand the objective at hand. The second part of the context encompasses three examples sampled from the meta-input-output pairs described in Section 3.2.We randomly three distinct meta-input-output caption pairs from a specific strategy (e.g.,

Figure 1: Illustration of using ChatGPT to generate meta-input-output pairs: we first sample source captions randomly from a few datasets. We then use prompts such as “Rewrite this image caption” to guide the ChatGPT model to generate rewritten target captions. The resulting target captions have different sentence structure than the source texts but, crucially, keep the major objects and subjects intact (in bold). These meta-input-output pairs are later used as contexts for ICL.

ChatGPT). Each pair is clearly separated by a "\(=>\)" symbol. These pairs provide explicit instances that showcase the desired rewriting behavior for the LLM to learn from. The additional random sampling process further enable the LLaMA model to generate more diverse text rewrites. Finally, the last part of the context includes the text sample that requires rewriting, followed by the separation symbol. This ensures that the LLM receives the specific text to be rewritten as part of its context input. By incorporating these three parts, we create a comprehensive context that guides the LLM in effectively generating diverse and contextually appropriate text rewrites.

Utilizing the constructed context input as a prompt, LLaMA exhibits its ability to perform text completion and generate rewritten versions of the corresponding text samples. This process is conducted for each text sample present in the pre-training image-text dataset. Specifically, we employ the LLaMA-7B model to generate four distinct rewrites for every text sample in the dataset, with each rewrite corresponding to one of the four different meta-input-output sources (_ChatGPT_, _Bard_, _COCO_, or _Human_). It takes 7 hours to generate one rewrite for the entire CC3M dataset on a 8 A100 GPU machine. By incorporating multiple sources and leveraging the capabilities of LLaMA, we ensure the generation of diverse and contextually relevant text rewrites for each text sample within the dataset.

### LaCLIP: Training CLIP with Language Augmentations

Having generated \(M\) different rewrites for each caption (in our case, \(M=4\)), we are now able to bridge the augmentation gap between the image and text inputs, thereby presenting the training strategy for our LaCLIP framework. The key addition to the CLIP framework is the augmentation function for the text inputs, which can be easily implemented through a straightforward random sampling process, where we randomly select a text sample from either the original text or one of the generated rewrites:

\[\text{aug}_{T}(x_{T})\sim\text{Uniform}([x_{T0},x_{T1}\dots,x_{TM}])\] (2)

Here \(x_{Tk}\) is the \(k^{th}\) rewritten sample for \(x_{T}\). For the sake of simplicity, we denote the original text as \(x_{T0}\). The training loss over the images becomes:

\[L_{I}=-\sum_{i=1}^{N}\log\frac{\exp\left(\text{sim}(f_{I}(\text{aug}_{I}(x_{I }^{i})),f_{T}(\text{aug}_{T}(x_{T}^{i})))/\tau\right)}{\sum_{k=1}^{N}\exp \left(\text{sim}(f_{I}(\text{aug}_{I}(x_{I}^{i})),f_{T}(\text{aug}_{T}(x_{ T}^{k})))/\tau\right)},\] (3)

The only difference with the original CLIP training here is the additional text augmentation \(\text{aug}_{T}\), and all other parts remains the same, which does not bring any additional computation or parameter overheads compared to original CLIP during training. By incorporating text augmentations into CLIP, we introduce variability and diversity into the training data, enabling the model to learn from both the original text and the augmented versions. This simple yet effective strategy enhances the training process and contributes to the overall performance and adaptability of the LaCLIP framework.

Figure 2: Illustration of our proposed in-context learning based strategy for language rewriting: The left box depicts the input context for LLaMA, responsible for text completion. The blue and green texts represent the meta-input-output text pairs, with blue indicating the input and green indicating the output. These pairs are defined in Section 3.2 and visualized in Fig. 1. The final blue line represents the text from the image-text datasets intended for rewriting. On the right box, we showcase the completion result generated by the open-source 7B parameter LLaMA model [56], which represents the rewritten version of the text of interest.

## 4 Experiments

**Datasets.** Our experiments were conducted on four different image-text datasets at different scale: Conceptual Captions 3M (CC3M) [51], Conceptual Captions 12M (CC12M) [7], RedCaps [15], and LAION-400M[49]. RedCaps is a 12M-instance dataset collected exclusively from Reddit, potentially exhibiting distinct distributions compared to other datasets. The majority of our ablation studies were performed on the CC12M dataset. We evaluate all the models on ImageNet and 15 common downstream datasets like Food101 [4], SUN397 [59] and FGVCAircraft [38]. Appendix A contains the details for all datasets.

**Training Parameters.** For most of our experiments on CC3M, CC12M, and RedCaps, we utilized the ViT-B/16 architecture [17] and trained the models with a batch size of 8,192 and the AdamW optimizer [26]. Additionally, we explored the ViT-L/16 and ViT-S/16 architectures in ablation studies. For LAION-400M, we used both the ViT-B/32 and ViT-B/16 architecture with a batch size of 32,768, and followed the exact training setup outlined in [44], training the model for 32 epochs. Appendix B contains a detailed breakdown of our training hyperparameters.

**Evaluation Setup.** We consider three evaluation metrics for the trained models: Zero-Shot (**ZS**) classification accuracy, Few-Shot (**FS**) classification accuracy and Linear Probing (**LP**) accuracy. For zero-shot classification, we adopt the same prompt templates as described in the CLIP paper [44]. The class text embeddings are used to compute distance with the image feature, and images are classified to class with the shortest distance. For few-shot classification, we follow the set up in [44] and perform 5-way 5-shot classification with a weighted kNN classifier on top of the frozen features. For linear probing, following [44; 18], we freeze the pre-trained image encoder and extract features for every image in the downstream dataset. We then train a linear classifier using L-BFGS optimizer on top of the extracted features. ZS and LP are evaluated on both ImageNet (**IN**) and 15 Downstream (**DS**) datasets. FS are evaluated on the same downstream datasets. In the ablation studies we report the perforamnce on **IN** and the mean on **DS**.

### Zero-shot Evaluation

We provide a comprehensive analysis of the zero-shot transfer performance on ImageNet and downstream datasets in Table 1. Remarkably, across all pretrained datasets, our LaCLIP approach achieves a significant performance improvement over the baseline CLIP model on both ImageNet and down

\begin{table}
\begin{tabular}{c c|c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Data**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Architecture**} & \multicolumn{3}{c}{**ViT-B/32**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} \\ \cline{3-3} \cline{8-19}  & & & & & & & & & & & & & & & & & & & \\ \hline \multicolumn{19}{c}{_Model Architecture: ViT-B/32_} \\ \hline \multirow{3}{*}{LAION-400M} & CLIP & **79.9** & 91.8 & 72.0 & 64.6 & 77.0 & 15.8 & 49.9 & 84.8 & 89.3 & 64.4 & 95.3 & 43.2 & 60.6 & 36.9 & 14.5 & 62.7 & 62.0 \\  & LaCLIP & 79.7 & **92.4** & **73.0** & **64.9** & **81.9** & **20.8** & **55.4** & **87.2** & **91.8** & **70.3** & **97.3** & **50.6** & **61.5** & **49.4** & **16.0** & **66.1** & **64.4** \\ \hline \multicolumn{19}{c}{_Model Architecture: ViT-B/16_} \\ \hline \multirow{3}{*}{CC3M} & CLIP & 10.3 & 54.9 & 21.8 & 25.0 & 0.8 & 1.4 & 10.5 & 12.8 & 43.3 & 10.2 & 77.6 & 14.1 & 19.1 & **6.9** & 0.6 & 20.6 & 15.8 \\  & LaCLIP & **14.2** & **57.1** & **27.5** & **35.1** & **1.6** & **1.6** & **16.6** & **15.6** & **52.7** & **14.7** & **86.2** & **15.0** & **24.3** & 6.4 & **1.0** & **24.6** & **21.5** \\ \hline \multirow{3}{*}{CC12M} & CLIP & 50.8 & 64.9 & 38.5 & 44.7 & 24.1 & 2.4 & 19.4 & 64.1 & 77.4 & 33.2 & 91.0 & 20.1 & 38.9 & 7.3 & 5.1 & 38.8 & 40.2 \\  & LaCLIP & **60.7** & **75.1** & **43.9** & **57.0** & **36.3** & **5.6** & **31.0** & **72.4** & **83.3** & **39.9** & **95.1** & **27.3** & **44.3** & **12.7** & **8.9** & **46.2** & **48.4** \\ \cline{1-1} \cline{2-19}  & SLIP & 52.5 & 80.7 & 46.3 & 48.8 & 24.9 & 2.3 & 25.1 & 58.6 & 77.6 & 29.2 & 89.1 & **25.8** & 36.6 & 6.0 & 5.7 & 40.6 & 42.1 \\  & LaSLIP & **62.9** & **82.0** & **50.2** & **59.6** & **32.2** & **4.4** & **30.1** & **70.6** & **82.4** & **37.4** & **95.0** & 20.4 & **45.6** & **10.1** & **9.2** & **46.1** & **49.7** \\ \hline \multirow{3}{*}{RedCaps} & CLIP & 81.5 & 70.4 & 39.9 & 33.2 & 19.2 & 1.9 & 19.7 & **82.7** & 72.8 & 53.9 & **92.8** & 23.3 & 33.6 & **8.3** & 6.2 & 42.6 & 42.9 \\  & LaCLIP & **85.0** & **74.8** & **40.7** & **40.3** & **21.3** & **2.2** & **23.9** & 78.2 & **76.4** & **59.0** & 91.4 & **27.1** & **41.3** & 5.6 & **7.6** & **45.0** & **46.2** \\ \hline \multirow{3}{*}{LAION-400M} & CLIP & 85.5 & 93.0 & 71.7 & 66.8 & 83.5 & 16.7 & 52.8 & 90.1 & 91.2 & 63.9 & 97.3 & 42.4 & 63.3 & **46.2** & 17.8 & 65.5 & 67.0 \\  & LaCLIP & **86.5** & **93.5** & **73.9** & **67.9** & **87.1** & **24.2** & **58.9** & **90.9** & **92.4** & **73.1** & **98.4** & **48.3** & **65.8** & 46.1 & **19.6** & **68.4** & **69.3** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Zero-shot transfer evaluation of different models. Performance on ImageNet and 15 common downstream datasets are reported. We highlight the best performance of each setting in **bold**. We see that regardless of the scale of the pre-training dataset, LaCLIP outperforms CLIP [44] and LaSLIP outperforms SLIP [41], by up to \(8.2\%\) absolute accuracy.

stream datasets. For instance, when training models on the CC12M dataset, our LaCLIP method achieves over 8% improvement in absolute top-1 accuracy on ImageNet and 7% improvement on average over the other downstream datasets. LaCLIP and CLIP share the exact same amount of parameters and computation cost during training.

**Adaptability to other methods.** It is noteworthy that LaCLIP is compatible with other techniques intended to enhance CLIP's performance. Once the augmented texts are generated, integrating LaCLIP into any CLIP-based framework can be achieved seamlessly without incurring additional computational or memory overhead. As demonstrated in Table 1, we applied language augmentation to the SLIP framework and yield LaSLIP, resulting in significant performance improvements across all evaluation metrics. Notably, even though SLIP already incorporates additional self-supervision to enhance CLIP's performance, our proposed language augmentation further boosts its effectiveness. This showcases the generalization capability of our proposed text augmentation strategy.

**Generalization to Larger Datasets.** Consistently, the results highlight the substantial margin by which LaCLIP outperforms CLIP across various datasets. Noteworthy is the scalability of our method with dataset size, as it demonstrates improvement even when trained on the massive LAION-400M dataset, which contains hundreds of millions of data points1. These findings suggest that our LaCLIP approach can be seamlessly integrated as a plug-and-play component for training vision-language foundation models.

Footnote 1: The version used in our experiment contains \(\sim\)340M samples, slightly less than original due to link rot. We use OpenCLIP implementation (https://github.com/mlfoundations/open_clip) and achieves 62.0% ImageNet zero-shot accuracy for CLIP, comparable to their model with 62.9% trained on the full dataset.

### Few-Shot & Linear-Probing

We present the 5-way 5-shot classification performance in Table 2 and the linear-probing performance in Table 3. Our approach consistently outperforms vanilla CLIP or SLIP in the vast majority of cases. Interestingly, SLIP performs worse than vanilla CLIP in the few-shot setting, despite introducing additional self-supervision from the image side. However, by incorporating our proposed language augmentation strategy, SLIP's few-shot performance improves, surpassing vanilla CLIP. This result highlights the effectiveness of text augmentations in the few-shot setting.

Furthermore, it is important to highlight that the improvements observed in the few-shot and linear-probing results are solely achieved through the utilization of image encoders. This demonstrates the efficacy of our proposed text augmentation approach in enhancing not only the joint image-text embedding space, which aligns image and text features more effectively, but also the quality of the

\begin{table}
\begin{tabular}{c c|c c c c c c c c c c c c c c|c} \hline \hline \multirow{2}{*}{**Data**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model Architecture:**_VIT-B/32_} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Model**} \\  & & & & & & & & & & & & & & & & & & & & & \\ \hline \multirow{3}{*}{LAION-400M} & CLIP & 92.5 & 87.2 & 89.0 & 98.0 & 98.5 & 78.9 & 87.4 & 94.5 & 99.2 & 99.0 & 96.1 & **82.8** & **94.3** & 79.8 & 49.7 & 88.5 \\  & LaCLIP & **93.5** & **91.0** & **90.7** & **98.2** & **99.1** & **82.2** & **87.5** & **95.7** & **99.4** & **99.2** & **97.2** & 80.1 & 94.2 & **80.4** & **52.2** & **89.4** \\ \hline \multirow{3}{*}{CC3M} & CLIP & 67.6 & 64.2 & 73.6 & 94.1 & 54.4 & 46.1 & 74.4 & 76.7 & 93.3 & 94.3 & 84.6 & **81.4** & **87.1** & 66.9 & 37.3 & 73.1 \\  & LaCLIP & **70.0** & **69.1** & **76.8** & **95.2** & **57.6** & **49.2** & **75.8** & **77.4** & **95.2** & **95.0** & **89.5** & 81.1 & 85.5 & **71.0** & 37.3 & **75.0** \\ \hline \multirow{3}{*}{CC12M} & CLIP & 87.0 & 77.5 & 82.1 & 97.0 & 96.0 & 83.3 & 91.1 & 98.2 & 97.6 & 92.6 & **83.4** & 91.2 & 70.6 & 44.3 & 83.3 \\  & LaCLIP & **89.9** & **83.5** & **89.0** & **98.5** & **68.1** & **84.9** & **93.4** & **98.9** & **98.4** & **95.9** & 83.0 & **92.4** & **76.4** & **85.8** \\ \cline{1-1}  & SLIP & 87.6 & 79.2 & 83.0 & 97.5 & 85.6 & 56.4 & 85.8 & 88.1 & 97.7 & 97.1 & 92.5 & **84.9** & 91.0 & 62.4 & 43.0 & 82.1 \\  & LaSLIP & **90.5** & **84.9** & **86.6** & **98.1** & **91.6** & **61.0** & **86.7** & **89.8** & **97.8** & **97.8** & **94.2** & 84.0 & **92.8** & **65.8** & **45.4** & **84.5** \\ \hline \multirow{3}{*}{RedCaps} & CLIP & 94.4 & 80.6 & 85.3 & 95.9 & 88.5 & 54.5 & **82.6** & **94.5** & 97.8 & 99.0 & 94.8 & 84.9 & 91.3 & 75.3 & 40.6 & 84.0 \\  & LaCLIP & **95.8** & **81.4** & **85.4** & **96.2** & **90.9** & **58.8** & 82.4 & 94.1 & **98.0** & **99.2** & **95.6** & **86.2** & **92.1** & **76.5** & **42.6** & **85.0** \\ \hline \multirow{3}{*}{LAION-400M} & CLIP & 95.0 & 90.1 & 90.7 & 98.2 & 99.2 & 80.8 & 88.7 & 96.2 & 99.5 & 99.4 & 97.1 & **84.5** & 95.0 & 77.7 & 55.1 & 89.8 \\  & LaCLIP & **95.8** & **92.7** & **91.9** & **98.4** & **99.5** & **86.1** & **89.0** & **97.1** & **99.6** & **99.5** & **98.1** & 82.9 & 95.0 & **80.9** & **57.9** & **91.0** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Few-shot transfer evaluation of different models. We report 5-way, 5-shot classification accuracy for all downstream datasets. We highlight the best performance of each setting in **bold**. Similar to zero-shot, in nearly all cases, pre-training using language rewrites outperforms vanilla pre-training.

[MISSING_PAGE_EMPTY:8]

is that during the meta-input-output generation process, humans have the advantage of viewing the corresponding image, which allows them to generate more accurate and diverse rewrites.

**Different Backbone Architecture.** We further investigate the performance of LaCLIP using different backbone architectures. Table 6 summarize the results obtained with ViT-S/16, ViT-B/16, and ViT-L/16. We observe that LaCLIP scales with model size and consistently outperforms vanilla CLIP across all network architectures. These findings highlight the effectiveness of LaCLIP in improving the performance of CLIP models, irrespective of the underlying backbone architecture.

**Comparison with Pre-trained Text Encoder.** To deepen the comprehension on whether LaCLIP outperforms vanilla CLIP simply due to a better text encoder, we conducted experiments comparing LaCLIP with CLIP models trained with a pre-trained text encoder. Here we employed the BERT model as the pre-trained text encoder. The experiment result in Table 8 demonstrates that fine-tuning based on the pre-trained text encoder exhibits some improvements, whereas freezing the pre-trained text encoder weights substantially degrades performance. This observation aligns with the findings in LiT [63]. In contrast, LaCLIP consistently outperforms all configurations with a pre-trained text encoder, underscoring the benefit and necessity for explicit sentence augmentation strategies.

## 5 Multi-Text Training Loss with LaCLIP

It is important to highlight that once we have generated multiple text augmentations, and with a slight tolerance for computational cost, we can create multi-positive training pairs for each training iteration. These pairs are formed by pairing each image with not only the original text but also with all the rewritten versions of the text. By adopting this approach, we introduce a Multi-Text version

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Backbone**} & \multicolumn{3}{c}{ZS} & \multicolumn{3}{c}{LP} \\ \cline{3-7}  & **DS** & **IN** & FS & \multicolumn{2}{c}{**DS**} & **IN** \\ \hline \multirow{3}{*}{ViT-S/16} & CLIP & 36.3 & 36.9 & 82.2 & 77.0 & 67.1 \\  & LaCLIP & **44.1** & **46.3** & **84.5** & **78.0** & **69.1** \\ \hline \multirow{3}{*}{ViT-B/16} & CLIP & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & LaCLIP & **46.2** & **48.4** & **85.8** & **80.5** & **72.3** \\ \hline \multirow{3}{*}{ViT-L/16} & CLIP & 42.6 & 44.0 & 85.1 & 81.3 & 72.9 \\  & LaCLIP & **46.6** & **49.1** & **86.8** & **81.9** & **73.7** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation on different network architectures.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Augment**} & \multicolumn{2}{c}{ZS} & \multicolumn{3}{c}{LP} \\ \cline{2-7}  & **DS** & **IN** & FS & \multicolumn{2}{c}{**DS**} & **IN** \\ \hline \multirow{3}{*}{N/A (CLIP)} & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & **AV** & **AV** & 42.1 & 42.9 & 83.6 & 79.5 & 70.4 \\  & **AV** & **AV** & 24.5 & 23.2 & 80.3 & 74.9 & 66.0 \\ \hline LaCLIP & **AV** & **AV** & **46.2** & **48.4** & **85.8** & **80.5** & **72.3** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance comparison of CLIP training with different text augmentations.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{ZS} & \multicolumn{2}{c}{LP} \\ \cline{3-7}  & & **DS** & **IN** & & **DS** & **IN** \\ \hline \multirow{3}{*}{CC12M} & CLIP & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & LaCLIP & **46.2** & 48.4 & 85.8 & 80.5 & 72.3 \\  & LaCLIP-MT & 45.2 & **49.0** & 85.8 & **80.6** & **72.4** \\ \hline \multirow{3}{*}{RedCaps} & CLIP & 42.6 & 42.9 & 84.0 & 79.6 & 71.8 \\  & LaCLIP & 45.0 & 46.2 & 85.0 & 80.0 & 71.9 \\ \cline{1-1}  & LaCLIP-MT & **46.1** & **48.1** & **85.3** & **80.3** & **72.4** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance comparison of CLIP, LaCLIP and LaCLIP-MT on different datasets.

Figure 3: ImageNet zero-shot accuracy with different num of text augments.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Backbone**} & \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{ZS} & \multicolumn{2}{c}{LP} \\ \cline{3-7}  & & **DS** & **IN** & FS & \multicolumn{2}{c}{**DS**} & **IN** \\ \hline \multirow{3}{*}{ViT-S/16} & CLIP & 36.3 & 36.9 & 82.2 & 77.0 & 67.1 \\  & LaCLIP & **44.1** & **46.3** & **84.5** & **78.0** & **69.1** \\ \hline \multirow{3}{*}{ViT-B/16} & CLIP & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & LaCLIP & **46.2** & **48.4** & **85.8** & **80.5** & **72.3** \\ \hline \multirow{3}{*}{ViT-L/16} & CLIP & 42.6 & 44.0 & 85.1 & 81.3 & 72.9 \\  & LaCLIP & **46.6** & **49.1** & **86.8** & **81.9** & **73.7** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Performance comparison of LaCLIP and CLIP models trained with different pre-trained text encoder configurations.

of LaCLIP, referred to as LaCLIP-MT, which incorporates a multi-positive contrastive training loss. The training process iterates through all the images in the following manner:

\[L_{I*}=-\frac{1}{M}\sum_{i=1}^{N}\sum_{j=0}^{M}\log\frac{\exp\left( \text{sim}(f_{I}(\text{aug}_{I}(x_{I}^{i})),f_{T}(x_{Tj}^{i}))/\tau\right)}{\sum _{k=1}^{N}\exp\left(\text{sim}(f_{I}(\text{aug}_{I}(x_{I}^{i})),f_{T}(x_{Tj}^{ k}))/\tau\right)},\] (4)

Since each text can still be paired with a single image, the training loss that iterates through all the texts remains unchanged, with the only difference being that it now iterates over all of the texts instead of just the augmented ones. Consequently, the final training loss is given by the average of the image loss (\(L_{I*}\)) and the text loss (\(L_{T}\)), resulting in \(L=(L_{I*}+L_{T})/2\).

In order to showcase the efficacy of the multi-positive contrastive training loss in boosting the performance of LaCLIP, we conducted additional experiments on the CC12M and RedCaps datasets. The results of these experiments are summarized in Table 7, which compares the performance of LaCLIP-MT with both LaCLIP and vanilla CLIP. The results clearly indicate that LaCLIP-MT could further improve upon LaCLIP across most metrics. By allowing each image to be paired with all of the diverse texts describing its content, LaCLIP-MT leverages the additional and richer supervision from the language modality to enhance the formation of image-text embeddings. This improvement highlight the benefits of the multi-positive contrastive training loss in facilitating better alignment between images and diverse text descriptions.

## 6 Conclusion, Limitations and Broader Impact

Conclusion.We have introduced LaCLIP, a straightforward yet highly effective CLIP training strategy that incorporates text augmentations through text rewriting, leveraging the in-context learning capabilities of LLMs. Through this simple and versatile approach, we have demonstrated significant improvements in the performance of CLIP embeddings across various pre-training scales and datasets. Additionally, we have proposed a novel multi-text training loss to further enhance the training process. As LLMs continue to improve in performance and in-context learning capabilities, our approach stands to directly benefit from these advancements.

**Limitations.** While the training process itself does not entail any additional memory or computation overhead compared to vanilla CLIP, the process of generating text rewrites using LLMs can be computationally expensive, requiring significant GPU resources and taking hours for large datasets. Additionally, the quality of the rewritten text generated by LLaMA is not filtered, which may result in some irrelevant details that do not align well with the corresponding images. This misalignment could impact the transferability of the learned embeddings to downstream tasks. To address these limitations, future work could focus on developing more efficient methods for generating text rewrites using LLMs, reducing the computational burden without sacrificing performance. Furthermore, techniques for filtering the rewritten texts could be explored, aiming to retain only the most relevant and accurate versions while discarding those with misleading details. This would enable the model to learn a better embedding space that is robust and transferable across different downstream datasets, improving overall performance and alignment between vision and text encoders.

**Broader Impact.** We propose a general text augmentation strategy that can generate diverse rewrites for any given text. This strategy not only improves the performance of vision-language models but also has the potential to enhance models in pure natural language processing tasks, such as language understanding and reasoning. On the other hand, we acknowledge that LLMs are trained on large-scale web data, which may contain factual errors and hallucinations. Consequently, the rewritten versions of texts may also inherit these limitations. Therefore, we encourage researchers to implement additional data filtering methods before deploying these models in real-world scenarios. Additionally, the current LLM-based rewriting strategy requires significant GPU/TPU computation, which can contribute to a higher carbon footprint. However, it is also possible that such rewriting strategy can significantly reduce the number of training iterations for larger models to reach similar performances as vanilla CLIP.

## Acknowledgements

We would like to thank Mathilde Caron for the insightful early manuscript review, and the anonymous reviewers for their helpful comments and suggestions. Additionally, we appreciate the support and discussions with the VisCAM team at Google Research.

## References

* [1]B. Bad. https://bard.google.com/.
* [2] ChatGPT. https://chat.openai.com/.
* [3] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot. _arXiv preprint arXiv:2001.09977_, 2020.
* [4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative components with random forests. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13_, pages 446-461. Springer, 2014.
* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [6] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3558-3568, 2021.
* [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [9] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. _Proceedings of the IEEE_, 105(10):1865-1883, 2017.
* [10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [11] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3606-3613, 2014.
* [12] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011.
* [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [14] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11162-11173, 2021.
* [15] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. _arXiv preprint arXiv:2111.11431_, 2021.
* [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
** [18] Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models transfer? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5414-5423, 2021.
* [19] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. _IEEE transactions on pattern analysis and machine intelligence_, 28(4):594-611, 2006.
* [20] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* [21] Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu, Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Ishan Misra. Vissl. https://github.com/facebookresearch/vissl, 2021.
* [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* [23] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 12(7):2217-2226, 2019.
* [24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International Conference on Machine Learning_, pages 4904-4916. PMLR, 2021.
* [25] Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII 14_, pages 67-84. Springer, 2016.
* [26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [27] Skanda Koppula, Yazhe Li, Evan Shelhamer, Andrew Jaegle, Nikhil Parthasarathy, Relja Arandjelovic, Joao Carreira, and Olivier Henaff. Where should i spend my flops? efficiency evaluations of visual pre-training methods. _arXiv preprint arXiv:2209.15589_, 2022.
* [28] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 554-561, 2013.
* [29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [30] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained transformer models. _arXiv preprint arXiv:2003.02245_, 2020.
* [31] Ang Li, Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. Learning visual n-grams from web data. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 4183-4192, 2017.
* [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.
* [34] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. _arXiv preprint arXiv:2110.05208_, 2021.
* [35] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. _arXiv preprint arXiv:2212.00794_, 2022.
* [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.

* [37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [38] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [39] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. _arXiv preprint arXiv:2110.15943_, 2021.
* [40] Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural networks in nlp applications? _arXiv preprint arXiv:1603.06111_, 2016.
* [41] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pre-training. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXVI_, pages 529-544. Springer, 2022.
* [42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_, pages 722-729. IEEE, 2008.
* [43] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _2012 IEEE conference on computer vision and pattern recognition_, pages 3498-3505. IEEE, 2012.
* [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [47] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? _Advances in neural information processing systems_, 34:4974-4986, 2021.
* [48] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.
* [49] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* [50] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. _arXiv preprint arXiv:1511.06709_, 2015.
* [51] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.
* [52] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. _Advances in neural information processing systems_, 30, 2017.
* [53] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In _The 2011 international joint conference on neural networks_, pages 1453-1460. IEEE, 2011.
* [54] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. _Communications of the ACM_, 59(2):64-73, 2016.
* [55] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International conference on machine learning_, pages 10347-10357. PMLR, 2021.

* [56] Hugo Touvron, Thibaut Lavril, Gautier Izaard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [57] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* [58] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text classification tasks. _arXiv preprint arXiv:1901.11196_, 2019.
* [59] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE computer society conference on computer vision and pattern recognition_, pages 3485-3492. IEEE, 2010.
* [60] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: fine-grained interactive language-image pre-training. _arXiv preprint arXiv:2111.07783_, 2021.
* [61] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.
* [62] Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, and Baldo Faieta. Multimodal contrastive training for visual representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6995-7004, 2021.
* [63] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18123-18133, 2022.

[MISSING_PAGE_FAIL:15]

fair comparison. For models trained on other datasets, we use a subset of 7 templates recommended by [44] to expedite the evaluation process.

**Few-shot Classification** Following the settings in [18], we evaluate the 5-way 5-shot performance across 15 downstream datasets. We use Prototypical Networks [52] as classifier on top of the features extracted from vision encoders without data augmentation. Only Resize followed by CenterCrop is applied here for all images. We evaluate each model for 600 randomly sampled episodes, and for each episode, images are sampled from the combination of training, validation and testing sets. We always sample 15 images for each class as query set. The mean accuracy across all episodes are reported in the main paper, and we also report the 95% confidence interval in the appendix.

**Linear-Probing** For linear probing on ImageNet, we keep the image encoder frozen and train a Linear Classifier on the extracted features. The only augmentation applied is RandomHorizontalFlip. We sweep the base learning rate across the range of [0.002, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05] and report the best performance achieved. The learning rate is scaled linearly based on the actual batch size, following the approach outlined in [20]. Details of all other hyperparameters can be found in Table A5. For linear probing on all other downstream datasets, we train a logistic regression layer on top of the frozen features extracted from the vision encoders, without applying any data augmentation. The model is optimized using L-BFGS with Scikit-learn, and the maximum number of iterations is set to 500. To determine the optimal \(\ell_{2}\) regularization term for each model and dataset, we perform a sweep across 45 steps that are logarithmically spaced ranging from \(10^{-6}\) to \(10^{5}\) on the validation set. For the final results, we fit the model on the combined training and validation sets and report the performance on the separate test set.

## Appendix C Meta-input-output Details

### Meta-input-output Pairs

Here we provide the exact 16 meta-input-output pairs we used as templates for all four set ups: ChatGPT, Bard, Human and MSCOCO, described in Section 3.2. We use 'Source' to represent the meta-input text we sampled from the image-text datasets, and use 'Target' to represent the meta-output text generated by each of the strategies. Note the meta-input-output pairs showed in Figure 1 and Figure 2 in the main text are for illustration only, please refer to this section for the real pairs used in the experiments.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c|c c} \hline \hline \multirow{2}{*}{Model} & Patch & Input & Embedding & \multicolumn{3}{c|}{Vision Transformer} & \multicolumn{3}{c|}{Text Transformer} & Vocab & Text \\  & size & resolution & dimension & Layers & Width & Heads & Layers & Width & Heads & size & length \\ \hline ViT-S/16 & 16 & 224 & 512 & 12 & 384 & 12 & 12 & 512 & 8 & & & \\ ViT-B/16 & 16 & 224 & 512 & 12 & 768 & 12 & 12 & 512 & 8 & 49,408 & 77 \\ ViT-B/32 & 32 & 224 & 512 & 12 & 768 & 12 & 12 & 512 & 8 & & & \\ ViT-L/16 & 16 & 224 & 512 & 24 & 1024 & 16 & 12 & 512 & 8 & & & \\ \hline \hline \end{tabular}
\end{table}
Table A2: Encoder details.

\begin{table}
\begin{tabular}{l l c c} \hline \hline Dataset & Metric & Categories & Train Size & Test Size \\ \hline Food-101 [4] & Accuracy & 101 & 75,750 & 25,250 \\ CIFAR-10 [29] & Accuracy & 10 & 50,000 & 10,000 \\ CIFAR-100 [29] & Accuracy & 100 & 50,000 & 10,000 \\ SUN397 [59] & Accuracy & 397 & 19,850 & 19,850 \\ Stanford Cars [28] & Accuracy & 196 & 8,144 & 8,041 \\ FGVC Aircraft [38] & Mean per class & 100 & 6,667 & 3,333 \\ DTD [11] & Accuracy & 47 & 3,760 & 1,880 \\ Oxford Pets [43] & Mean per class & 37 & 3,680 & 3,669 \\ Caltech-101 [19] & Mean per class & 102 & 3,060 & 6,085 \\ Oxford Flowers [42] & Mean per class & 102 & 2,040 & 6,149 \\ STL-10 [12] & Accuracy & 10 & 1,000 & 8,000 \\ EuroSAT [23] & Accuracy & 10 & 10,000 & 5,000 \\ REISSC45 [9] & Accuracy & 45 & 25,200 & 6,300 \\ GTSRB [53] & Accuracy & 43 & 26,640 & 12,630 \\ Country211 [44, 54] & Accuracy & 211 & 42,200 & 21,100 \\ \hline \hline \end{tabular}
\end{table}
Table A1: Details of the downstream classification datasets.

[MISSING_PAGE_EMPTY:17]

8. **Source:** fight over a loose ball **Target:** Intense competition as players struggle to gain control of a loose ball during the game.
9. **Source:** love this winter picture by person. **Target:** Mesmerizing winter landscape by person: serene snowy scenery with gentle snowflakes, skillfully framed with perfect contrast and depth.
10. **Source:** one of several paths through the woods. **Target:** A narrow forest path, one among many weaving through the lush trees, underbrush, and dappled sunlight.
11. **Source:** ripe strawberries falling through the water. **Target:** Juicy ripe strawberries plummeting through a stream of water, splashing and creating ripples in the liquid.
12. **Source:** a city reflected on a red sunglasses. **Target:** The cityscape reflected on a pair of red sunglasses, creating a distorted but fascinating view of the urban environment.
13. **Source:** man driving a car through the mountains **Target:** A man confidently navigating a winding mountain road with breathtaking views.
14. **Source:** maritime museum from the historical dock. **Target:** A museum dedicated to seafaring history, located on a historic dock where visitors can view a collection of artifacts and vessels.
15. **Source:** tree hollow and green leaves of a tree top in summer **Target:** Amidst lush green leaves on the top of a tree, a hollow creates a natural shelter, typical of summer foliage.
16. **Source:** musician of musical group performs on stage on the first day of festival **Target:** On the opening day of the festival, a musician from a musical group performs energetically on stage to a lively crowd.

**Bard:**

1. **Source:** man driving a car through the mountains **Target:** A man drives his car through the mountains, the road winding its way through the towering peaks.
2. **Source:** a bicycle hanging above the entrance to a store **Target:** A bicycle suspended from a chain above the entrance to a bike shop advertises their services.
3. **Source:** government agency released underwater footage of the unique movements of starfish **Target:** New footage released by government agency shows starfish swimming in unique ways, shedding light on their underwater behavior.
4. **Source:** unique red chair among other white chairs at the stadium **Target:** Red chair stands out among white seats at stadium, a beacon of color.
5. **Source:** person looks comfortable as he connects with a free - kick during the session and is in line to return against hull on saturday **Target:** The player strikes a free-kick with confidence during training, looking sharp and ready for his return to the team against Hull on Saturday.
6. **Source:** animal in front of a white background **Target:** An animal stands out against the stark white background, its feathers on full display.
7. **Source:** a mother and daughter lying on a lawn **Target:** A mother and daughter embrace in the grass, enjoying the warmth of the sun and the sound of birdsong.
8. **Source:** sign is seen outside the home **Target:** A for sale sign stands in the front yard, a reminder that change is always on the horizon.
9. **Source:** portrait of person against an abstract background stock photo **Target:** person's face stands out against a swirling, abstract background, their expression one of mystery and intrigue.
10. **Source:** state flag waving on an isolated white background. **Target:** The state flag waves proudly against a stark white background. The colors of the flag are vibrant and the design is intricate, representing the history and culture of the state
11. **Source:** actor wears a gorgeous blush pink coloured gown at festival. **Target:** Actor stuns in blush pink gown at festival, her beauty a match for the flowers in bloom.
12. **Source:** person answering the phones again at the office. **Target:** A friendly voice answers the phone, eager to help the caller with their needs.

13. **Source:** little boy sitting on the grass with drone and remote controller

**Target:** A young boy's eyes light up as he launches his drone into the sky, his imagination taking flight with it.

14. **Source:** golfer competes during day held

**Target:** Golfer focuses on the ball as she competes in a tournament on a beautiful day.

15. **Source:** golden fish in a bowl

**Target:** A golden fish swims lazily in a bowl, its scales shimmering in the sunlight. The fish is a beautiful shade of orange, with black spots on its fins and tail.

16. **Source:** businessman with smartphone sitting on ledge by the sea

**Target:** A businessman relaxes on a seaside ledge, checking his phone and enjoying the view.

**Human:**

1. **Source:** Honey butermilk biscuits on a cooling rack being drizzled with honey

**Target:** A warm stack of freshly baked honey buttermilk biscuits, sit on a cooling rack as they are drizzled with golden honey

2. **Source:** happy corgi time

**Target:** Delighted corgi stands in the hallway, looking at its owner

3. **Source:** <PERSON> dog looking at dirt from the ground

**Target:** <Person>'s dog, lying on the ground, looks at the dirt

4. **Source:** navy vintage pants - lime green bag - ivory Maison Simons t-shirt - Zara clogs

**Target:** A young beautiful lady wearing navy vintage pants and ivory Maison Simons t-shirt, is holding a lime green bag.

5. **Source:** Ooak Barbie City Shine

**Target:** A custom-made Barbie doll with a city-inspired look shines brightly

6. **Source:** Real Wedding on a NYC Rooftop

**Target:** a couple is kissing each other during their rooftop wedding in NYC

7. **Source:** the proud of my beloved italian bracco after leg amputation due to a tumor.

**Target:** my italian bracco lied down proudly under the sunshile, despite of leg amputation due to a tumor.

8. **Source:** Pineapple Wearing Headphones Art Print by Philip Haynes

**Target:** An art from Philip Haynes depicts a pineapple that wears headphones

9. **Source:** Omnions thunderclouds behind the Capitol Building

**Target:** Thunderclouds loom over the Capitol Building, casting a dark shadow

10. **Source:** Steampunk woman with gun

**Target:** A fierce and stylish steampunk woman holds a toy revolver in her hands

11. **Source:** a new watch with some old friends

**Target:** The watch sits besides a cartoon picture, evoking memories of cherished times shared with long-time friends

12. **Source:** Particularly important to Africa is the East African Highland Banana (EAHB), a staple food for 80 million people. Uganda alone has about 120 varieties of this type of banana.

**Target:** An African man holds a bunch of bananas, which is particularly important to Africa

13. **Source:** Electric Blue Guitar There Goes My Hero, Rock The Vote, <PERSON>, <PERSON>, Music Photo, Red Eyes, Photo Quotes, Electric Blue, Music Lyrics

**Target:** <PERSON> is playing an electric blue guitar, eyes bloodshot from the stage lights

14. **Source:** Advanced Bicycle Skills Video - Valuable Video for Safe Cycl

**Target:** A Cyclist is demonstrating advanced bicycle skills in a video that will help people stay safe.

15. **Source:** grilled turkey pesto sandwich

**Target:** A grilled turkey pesto sandwich with melted cheese and fresh arugula is served on a plate.

16. **Source:** Actress <PERSON> during the launch of international fashion brand Forever 21 store at a mall in Mumbai on Saturday, October 12th, 2013.

**Target:** The young beautiful actress attended the launch of fashion brand Forever 21 at a mall.

**MSCOCO:**

For the meta-input-output sampling using the MSCOCO strategy, we utilize the fact that there are five different captions associated with each image. In our approach, we randomly select two texts from the available five, with one serving as the meta-input and the other as the meta-output. Below is a list of the captions we employ for this purpose.

1. **Caption 1 :** A herd of goats walking down a road way.

**Caption 2 :** Three lambs stand next to each other and look different directions.

**Caption 3 :** The animals standing in the clearing are 3 varieties of sheep.

**Caption 4 :** Three small sheep are standing on a road.

**Caption 5 :** Some animals are standing on a dirt path

**Caption 1 :** A boy is preparing to toss a frisbie while another boy is sitting in the background in a park.

**Caption 2 :** Several people are out in the woods on a path playing a game.

**Caption 3 :** A man in a park playing a throwing game.

**Caption 4 :** A group of people that are hanging out together.

**Caption 5 :** A boy gets ready to throw a frisbee

**Caption 1 :** A pizza sitting on top of a metal pan.

**Caption 2 :** The large pepperoni pizza is covered with chives.

**Caption 3 :** A pizza that is sitting on a tray.

**Caption 4 :** A large pizza with toppings sitting on a tray.

**Caption 5 :** a pizza with fresh basil tomato sauco and cheese baked

**Caption 1 :** A woman sits on top of a motorcycle in a parade.

**Caption 2 :** Woman wearing starts on helmet and shorts rides motorcycle

**Caption 3 :** A woman wearing attire that matches her motorcycle is driving on.

**Caption 4 :** A person that is on top of a motorcycle.

**Caption 5 :** Woman on a motorcycle rides in a parade

**Caption 1 :** the people are sampling wine at a wine tasting.

**Caption 2 :** Group of people tasting wine next to some barrels.

**Caption 3 :** People are gathered around a man tasting wine.

**Caption 4 :** A man pouring wine from casks for patrons

**Caption 5 :** People gather around a table while sampling wine.

**Caption 1 :** A herd of sheep walking down a street in front of a bus.

**Caption 2 :** There are three animals walking down the road.

**Caption 3 :** a van is stuck behind a few traveling goats

**Caption 4 :** a van that has some kind of animal out front of it

**Caption 5 :** A herd of animals walking down the road behind a truck.

**Caption 1 :** A sandwich with meat and cheese sits on a plate with a small salad.

**Caption 2 :** A sandwich with cheese and a bowl with a salad.

**Caption 3 :** Two plates with sandwic on them next to a bowl of vegetables.

**Caption 4 :** A long sandwich and a salad is on a plate.

**Caption 5 :** a sandwich and a bowl of vegetables on a plate

**Caption 1 :** A NASA airplane carrying a space shuttle on its back.

**Caption 2 :** A large plan with a smaller plan on top of it.

**Caption 3 :** A NASA airplane carrying the old Space Shuttle

**Caption 4 :** A NASA airplane glides through the sky while carrying a shuttle.

**Caption 5 :** This jet is carrying a space shuttle on it

**Caption 1 :** A one way sign under a blue street sign.

**Caption 2 :** a view from below of a one way sign

**Caption 3 :** A street sign stating that the road is one way beneath a blue sky.

**Caption 4 :** A "One Way" street sign pointing to the right.

**Caption 5 :** A one way road sign mounted above a street sign.

**Caption 1 :** A bowl of food containing broccoli and tomatoes.

**Caption 2 :** A large salad is displayed in a silver metal bowl.

**Caption 3 :** A bowl of food with tomatoes, sliced apples, and other greens

**Caption 4 :** A silver bowl filled with various produce discards.

**Caption 5 :** The salad in the bowl contains many fresh fruits and vegetables.

**Caption 1 :** a cake made to look like it has candy decorations on it

**Caption 2 :** A photograph of a highly decorated cake on a table.

**Caption 3 :** A cake decorated with lollipops and a piece of pie.

**Caption 4 :** A piece of cake with lolypops, pie and caterpillar designs.

**Caption 5 :** A layered cake with sweet treats and a caterpillar as decorations.

**Caption 1 :** A young man riding a skateboard on a cement walkway.

**Caption 2 :** a guy riding a skateboard by a car

**Caption 3 :** A young man on a skateboard near a car

**Caption 4 :** an image of a boy on a skateboard doing tricks

**Caption 5 :** A young man is riding on his skateboard.

13. **Caption 1 :** A small brown dog sitting on display behind a window. **Caption 2 :** A small fuzzy dog stares longingly out a window. **Caption 3 :** The dog is brown shaggy with a red collar. **Caption 4 :** A dog sits alone and stares out of a window. **Caption 5 :** A furry and cute dog sitting in a window looking outside. 14. **Caption 1 :** A herd of sheep standing on a lush green hillside. **Caption 2 :** Several animals standing on the side of a hill. **Caption 3 :** A number of sheep eat on a steep grassy hill. **Caption 4 :** a couple of sheep are standing in some grass **Caption 5 :** The side of a small hill of grass with several sheep grazing in the grass and houses in the background on the upper hill. 15. **Caption 1 :** The tennis player on the blue court has his racquet raised. **Caption 2 :** A man swinging a tennis racket at a pro tennis match. **Caption 3 :** A tennis player wearing a NIKE shirt swings his racket **Caption 4 :** Man posing in front of the camera holding up a tennis racket. **Caption 5 :** A man wearing a white shirt playing tennis. 16. **Caption 1 :** A surfer riding a wave in a tempestuous ocean **Caption 2 :** Man in body suit surfing on a large wave. **Caption 3 :** A surfer is sideways on a wave of water on a surfboard. **Caption 4 :** The surfer is riding sideways along a wave. **Caption 5 :** a surfer wearing a wet suit is surfing on a white board

### Detailed Experiment Results on Meta-Input-Output

We present a detailed analysis of the experiment results comparing different meta-input-output strategies. Specifically, for each of the four meta-input-output strategy (_ChatGPT, Bard, Human, MSCOCO_), we use this specific strategy as example candidates for LLAMA ICL, and generate a rewrite for every text in CC12M. Then we train four LaCLIP models, each model trained with the original captions and the rewrite version of one specific meta-input-output strategy. The comprehensive results of these experiments are summarized in Table A6. The results indicate that different meta-input-output strategy achieves similar performance.

\begin{table}

\end{table}
Table A6: Performance comparison of LaCLIP trained with different meta-input-output strategies on CC12M.

## Appendix D Augmentation Strategy Details

To help understand the effect of our proposed language rewriting strategy by LLaMA ICL, here we compare our proposed strategy with two widely used language augmentation baselines: EDA [58] and back translation [50].

* **EDA** contains four types of different randomly performed augmentation operations: Synonym Replacement, Random Insertion, Random Swap, and Random Deletion. We used the official implementation and kept all the default parameters as used in [58].
* **Back Translation** first translates the text to another language and then translate it back to English to generate slightly different version of the text. We chose four different languages for our experiments: Spanish, French, German and Italic languages.

### Augmentation Qualitative Comparison

We begin by presenting qualitative comparisons of different text augmentation strategies. It is observed that the EDA and back translation approaches primarily focus on word-level modifications, often preserving the sentence structures and leading to limited diversity in the rewritten texts. In contrast, our LLM-based augmentation strategy demonstrates the ability to substantially alter the sentence structure while maintaining the original key concepts and meaning intact. This results in more effective, diverse, and enriched rewritten texts, highlighting the superiority of our approach over word-based methods.

* **Original:** Handmade mirror in a wooden toned frame texture cracked paint with reflection green apple on the table. Handmade mirror in a wooden toned frame texture cracked stock photos **EDA Aug 1:** handmade mirror in a wooden toned frame texture cracked on with reflection green apple paint the table handmade mirror in a wooden toned frame texture cracked stock photos **EDA Aug 2:** handmade mirror in a wooden toned frame texture cracked point with reflection green apple angstrom unit on the table handmade mirror in a wooden toned frame texture cracked stock photos **Back Translation Spanish:** Hand-made mirror in the toned wood frame cracked texture paint with green apple reflection on the table. Hand-made mirror in the toned wood frame cracked texture stock photos **Back Translation French:** Hand-made mirror in a wood toned frame texture cracked point with green apple reflection on the table. Hand-made mirror in a wood toned frame texture cracked stock photos **Back Translation German:** Handmade mirror in a wooden toned frame texture cracked color with reflection green apple on the table. Handmade mirror in a wooden toned frame texture cracked stock photos **LLM ChatGPT:** Mirror on wooden painted frame, over cracked stone, with green apple on the table. Handmade mirror in a wooden toned frame texture cracked stock photo **LLM Bard:** Green apple, still life, studio photography, abstract background **LLM MSCOCO:** mirror on a table **LLM Human:** A closeup on a cracked mirror that reflects an apple on the table. The frame is covered in a golden toned wood grain texture.
* **Original:** Traffic jam on the road, a lot of cars which go towards each other and to the different directions **EDA Aug 1:** traffic jam on the road a lot of cars which go towards each other and to the different focus **EDA Aug 2:** traffic jam on the road a lot of cars which go towards each other and dissimilar to the different directions **Back Translation Spanish:** Traffic jam on the road, a lot of cars going towards the other and to the different directions **Back Translation French:** Traffic jams on the road, lots of cars going towards each other and towards different directions **Back Translation German:** Traffic jam on the road, a lot of cars that go to each other and to the different directions **LLM ChatGPT:** Traffic jam on the road, with a lot of cars moving in different directions, as well as towards each other. **LLM Bard:** A lot of cars line the street, all heading toward the same intersection.

**LLM MSCOCO:** Traffic jam on the road with cars going in different directions, some cars are on the same lane, others are on different lanes.

**LLM Human:** A traffic jam on the road. There are a lot of cars and many of them are going in different directions.

**Original:** Green young small newborn tree growing on concrete road among dry fallen leaves given concept of the life beginning or strong surviving

**EDA Aug 1:** green young small newborn tree growing on concrete road among dry fallen leaves given concept of the life beginning or strong surviving

**EDA Aug 2:** green young small new sprung tree growing on concrete road among dry fallen leaves given conception of the life beginning or strong surviving

**Back Translation Spanish:** Young green small newborn tree growing on the concrete road between fallen dry leaves given the concept of the beginning of life or strong survival

**Back Translation French:** Young young-born green tree growing on the concrete road among the dried leaves fallen according to the concept of the beginning of life or strong survivor

**Back Translation German:** Green young little newborn tree growing on concrete road under dry fallen leaves given concept of life beginning or strong survival

**LLM ChatGPT:** Bold forest green, from endless tangle of vines, create a strong vertical accent on the concrete road. On the right side, two brown fallen leaves, littering the road, sit quietly and gracefully, almost as though they are praying.

**LLM Bard:** Green young small newborn tree growing on concrete road among dry fallen leaves giving concept of the life beginning or strong surviving

**LLM MSCOCO:** A bare tree trunk surrounded by fallen leaves.

**LLM Human:** a young and tall green tree on a concrete road among dry fallen leaves, its leaves on the floor. Concept is a strong surviving in harsh environment

**Original:** Caused by the european solid wood floor lamp creative decorative floor lamp floor implant instome jeanquese wood living room bedroom lights

**EDA Aug 1:** room by the european solid wood floor lamp creative decorative floor lamp floor implant instome japanese wood bedroom caused living lights

**EDA Aug 2:** caused house by the european solid wood floor lamp creative decorative floor lamp floor lamp minimalist home japanese wood living have room bedroom lights

**Back Translation Spanish:** Caused by European solid wood floor lamp European decorative floor lamp creative minimalist floor lamp Japanese home wood living room bedroom lights

**Back Translation French:** Cause of the European floor lamp in solid wood decorative floor lamp creative floor lamp minimalist floor lamp Japanese house living room light room bedroom

**Back Translation German:** Conditioned by the European solid wood floor lamp lamp creative decorative floor lamp floor lamp minimalist house Japanese wood living room bedroom lights

**LLM ChatGPT:** Due to the european style lighting system, the decorative floor lamp will enhance your living room decor.

**LLM Bard:** Because of the european solid wood floor lamp, creative floor lamp, decorative floor lamp, floor lamp, minimalist home, japanese wood living room, bedroom lights.

**LLM MSCOCO:** A furniture store with lamps on the shelves. One lamp is lit.

**LLM Human:** The european solid wood floor lamp creative decorative floor lamp floor lamp minimalist home japanese wood living room bedroom lights

**Original:** A man, likely a railroad employee, stands in the forefront of the station by the tracks. In the background, groups of people lean against the building, perhaps waiting for a train. In the foreground is the switch tower.

**EDA Aug 1:** a man likely a railroad employee stands in the forefront of the station by the tracks in the background groups of inch people lean against the building perhaps waiting for a hulk train in the foreground is the transposition switch tower

**EDA Aug 2:** a military personnel likely a railroad employee stands in the forefront of the station by the tracks in the background groups of people lean against the building perhaps ready and waiting for a train in the foreground is the throw tower

**Back Translation Spanish:** A man, probably a railroad employee, is at the forefront of the station by the tracks. Deep down, groups of people lean on the building, perhaps waiting for a train. In the foreground is the switch tower.

**Back Translation French:** A man, probably a railway employee, stands at the vanguard of the station by the tracks. In the background, groups of people lean against the building, perhaps waiting for a train.

**Back Translation German:** A man, probably a railway worker, is standing at the top of the stationon the tracks. In the background, groups of people are leaning against the building, perhaps waiting for a train.

**LLM ChatGPT:** A man, likely a railroad employee, stands by the tracks in front of the station. In the background, groups of people lean against the building. In the foreground is the switch tower.

**LLM Bard:** man leaning against the rail station and switch tower in a dark night with a fog

**LLM MSCOCO:** A portrait of the man in the front of the station is seen in the upper left. In the lower right is a man leaning on a post with his arms crossed.

**LLM Human:** An image of a man, likely a railroad employee, standing in the foreground of a train station by the tracks. In the background are groups of people, some leaning against the building, which could be waiting for a train. In the foreground are the tracks with a switch tower in the distance.

### Detailed Experiment Results on Augmentation Strategy

We conducted a quantitative comparison of different augmentation strategies while ensuring a fair evaluation by generating a consistent number of augmented texts per original sentence (i.e., 4).

For the EDA strategy, we created 4 distinct versions of each sentence by randomly applying their predefined augmentation operations. As for the back translation approach, we translated the original texts into four different languages (Spanish, French, German, and Italic languages) and then back to English, resulting in 4 rewritten versions of the original texts. In our LLM-based augmentation, we used LLaMA ICL to generate 4 augmentations prompted by the 4 predefined meta-input-output pairs (ChatGPT, Bard, Human, and MSCOCO).

A comprehensive comparison of these strategies is presented in Table A7. The results demonstrate that while the baseline augmentation strategies improve the performance of the vanilla CLIP baseline, our proposed LLM-based augmentation strategy consistently achieves superior results across various datasets and evaluation metrics, outperforming the other augmentation methods significantly.

[MISSING_PAGE_EMPTY:25]

## Appendix E Number of Augmentations per Original Text

We conducted experiments to investigate how the performance varies with the number of augmentations generated for each text and the differences between augmentation strategies as the number of augmentations per original text increases. We examined the performance of each strategy with 0 to 4 augmentations per original text, where 0 corresponds to vanilla CLIP without any text augmentation. Specifically, for each specific number of augmentations \(k\): For EDA, we selected k versions out of the 4 generated versions. In the case of back translation, we used _Spanish, Spanish+French, Spanish+French+German_, and _Spanish+French+German+Italic languages_ for \(k=1,2,3,4\), respectively. Regarding our LLM-based augmentation, we used _ChatGPT, ChatGPT+Bard, ChatGPT+Bard+MSCOCO_, and _ChatGPT+Bard+MSCOCO+Human_ as augmentations corresponding to \(k=1,2,3,4\), respectively.

The detailed comparison can be found in Table A8. From the results, we observe that the performance of the baseline augmentation strategies does not scale well with the number of augmentations per sentence, indicating limited diversity in the rewritten texts. This aligns with the findings in [58], where the best results are obtained with four different augmentations. In contrast, LaCLIP trainedwith our LLM-based augmentation demonstrates good scalability with the number of augmentations. This can be attributed to the rich and diverse nature of LLaMA ICL in the rewriting process, allowing for continued performance improvement with more augmentations.

## Appendix F t-SNE Visualizations

To gain a deeper understanding of the distinctions between the features learned from LaCLIP and vanilla CLIP, as well as the impact of different augmentation strategies used in LaCLIP training, we visualize the vision encoder features on different downstream datasets using t-SNE [57] in Figure A1. We generate feature visualizations for CIFAR-10, Food101, STL-10, and EuroSAT datasets, as they provide sufficient samples per class for meaningful visualizations. Other datasets have a limited number of samples per class in the test set, making it difficult to generate reliable visualizations. For Food101 we visualize the features from the first 10 classes.

The visualization reveals that LaCLIP trained with our proposed LLM-based rewriting strategy exhibits clearer class boundaries and more distinct clusters compared to other approaches. This observation suggests that language augmentations not only enhance the performance of text encoders, but also improve the ability of vision encoders to learn a more effective image embedding space that is well-suited for downstream tasks.

## Appendix G Detailed Experiment Results for LaCLIP-MT

In Table A9, we present a detailed performance comparison among CLIP, LaCLIP, and the Multi-Text version LaCLIP-MT, as introduced in Section 5.

The pre-training was performed on CC12M and RedCaps datasets. The results highlight the potential of the multi-text version of the CLIP loss to enhance the performance of LaCLIP even further. By pairing each image with all corresponding texts, the vision encoder receives more diverse supervision during training iterations. he improvements are particularly significant for the RedCaps dataset, where LaCLIP-MT achieves an additional 1.9% increase in zero-shot classification accuracy on ImageNet.

## Appendix H Detailed Experiment Results for Different Backbone

In Table A10, we present the detailed experiment results on CC12M using different backbone architectures, including ViT-S/16, ViT-B/16, and ViT-L/16 encoders. The results consistently demonstrate that our proposed LaCLIP outperforms the vanilla CLIP baseline across all backbone architectures. This highlights the scalability of LaCLIP, as it consistently improves performance on various downstream tasks while leveraging encoders of different sizes.

## Appendix I Ablation on LLaMA model

We performed two ablation studies on the LLaMA model to assess the impact of modifying key components on the performance of LaCLIP. The studies focused on two factors: model size and temperature. By systematically investigating these factors, we aimed to shed light on their influence and provide valuable insights into the effectiveness and adaptability of the LLM-based augmentation approach. All experiments were conducted on LaCLIP using a single text augmentation strategy with the _ChatGPT_ meta-input-output prompting pairs. The models were pre-trained on the CC12M dataset.

**Model Size.** Given that LLaMA offers multiple models with varying numbers of parameters, including 7B, 13B, 33B, and 65B, it is widely acknowledged that larger models tend to excel in NLP tasks involving reasoning and comprehension. Building upon this observation, we sought to explore the potential benefits of incorporating larger LLaMA models into our framework, with the aim of enhancing the performance of LaCLIP on downstream tasks.

To investigate whether the use of larger LLaMA models would yield improved results, we conducted a series of experiments where LaCLIP was trained using text augmented by LLaMA models of different sizes. We compared the performance of LaCLIP across these different configurations and summarized the results in Table A11.

Through our analysis, we have observed that even the smallest and relatively lightweight LLaMA model (7B) is sufficient to significantly boost the performance of LaCLIP on CLIP. Although larger LLaMA models showed some improvement on certain downstream datasets, the overall impact was relatively modest in our experimental setups focused on training vision-language models. It is worth mentioning that different model sizes may benefit from different temperature settings during the sampling process, and we leave this as a topic for future research. In the following sections, we specifically examine the effect of temperature on the 7B model.

**Temperature.** The temperature parameter plays a crucial role in the LLaMA token sampling process as it controls the balance between diversity and precision in the generated text. Higher values of temperature increase text diversity, but excessively high values can introduce random words or non-English tokens, negatively impacting the results.

\begin{table}

\end{table}
Table A11: Ablation study on LaCLIP trained with text rewrites generated with different LLaMA model size on CC12M.

We conducted experiments with temperature values ranging from \(0.3\) to \(1.1\), and the detailed results of employing different temperatures for LLaMA generation are provided in Table A12. The results show that overall the performance is quite robust across temperatures. Generally as the temperature increases, the performance initially improves, reaching a peak around \(\tau=0.9\), and then begins to decline. Therefore, \(\tau=0.9\) appears to be the optimal temperature for text rewriting in the context of text augmentation, and we consistently use this value in all of our experiments.

## Appendix J Ablation on Non-contrastive Training

Language rewrites techniques used in LaCLIP holds potential for broader applications. In order to evaluate the impact of the language augmentation strategy on non-contrastive vision-language pre-training methods, we integrated this strategy into Virtex's training pipeline [14], leading to the formation of Language-augmented Virtex (La-Virtex). We replicated the identical setup in their official implementation and trained the two models on the CC12M dataset. Table A13 shows the linear classification performance on the PASCAL VOC07 dataset of the two pre-trained models. The result demonstrates that the incorporation of language rewrites in La-Virtex outperforms the standard Virtex. This indicates that language augmentation could potentially be more generic and beneficial to non-contrastive vision-language model training methods as well.

## Appendix K Training and Validation Curves

We show the training and validation curve on CC3M, CC12M, RedCaps and LAION-400M datasets in Figure A2. The results demonstrate LaCLIP consistently achieves higher validation accuracy and higher training loss across different datasets. This indicates language augmentation is improving the model's generalization ability rather than its optimization process. This is because Language augmentations used in LaCLIP makes the pre-training task more challenging and therefore improves the generalization ability of the model.

## Appendix L Visualization of Examples being Corrected

To provide a qualitative understanding of the categories most impacted by language augmentation, we present examples from the three categories with the most significant accuracy improvements by LaCLIP on the ImageNet validation set in Figure A3. This demonstrate LaCLIP has the ability to distinguish between some fine-grained categories where vanilla CLIP faces challenges.

## Appendix M Detailed Experiment Results for Pre-trained Text-encoder

In Table A14, we present the detailed experiment results on CC12M between LaCLIP and CLIP models trained with different text encoder setups. For the CLIP models where pre-trained text encoders are used, we replaced the text encoder and tokenizer with the pre-trained BERT-Base model while keeping all other parameters to be the same. The experiments with pre-trained BERT encoder are conducted in two distinct setups: (a) fine-tuning the entire model, and (b) freezing the weights of the text encoder. The results shows using the pre-trained text encoder as initialization and fine-tunethe entire model can bring some benefit to vanilla CLIP training, while freezing the text encoder weights will result in performance drop. In the meantime, LaCLIP shows superior performance and outperforms all pre-trained text encoder counterparts, demonstrating the effectiveness of language augmentation strategy.

\begin{table}

\end{table}
Table 14: Ablation study between LaCLIP and CLIP models trained with different pre-trained Text Encoder (**Text-Enc**) setups on CC12M. For the models with a pre-trained text encoder, we use the pre-trained BERT-base model.