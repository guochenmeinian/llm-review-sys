# GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies

 Takahiro Mimori

Waseda University

RIKEN AIP

takahiro.mimori@aoni.waseda.jp

&Michiaki Hamada

Waseda University

AIST-Waseda CBBD-OIL

Nippon Medical School

mhamada@waseda.jp

###### Abstract

Phylogenetic inference, grounded in molecular evolution models, is essential for understanding the evolutionary relationships in biological data. Accounting for the uncertainty of phylogenetic tree variables, which include tree topologies and evolutionary distances on branches, is crucial for accurately inferring species relationships from molecular data and tasks requiring variable marginalization. Variational Bayesian methods are key to developing scalable, practical models; however, it remains challenging to conduct phylogenetic inference without restricting the combinatorially vast number of possible tree topologies. In this work, we introduce a novel, fully differentiable formulation of phylogenetic inference that leverages a unique representation of topological distributions in continuous geometric spaces. Through practical considerations on design spaces and control variates for gradient estimations, our approach, GeoPhy, enables variational inference without limiting the topological candidates. In experiments using real benchmark datasets, GeoPhy significantly outperformed other approximate Bayesian methods that considered whole topologies.

## 1 Introduction

Phylogenetic inference, the reconstruction of tree-structured evolutionary relationships between biological units, such as genes, cells, individuals, and species ranging from viruses to macro-organisms, is a fundamental problem in biology. As the phylogenetic relationships are often indirectly inferred from molecular observations, including DNA, RNA and protein sequences, Bayesian inference has been an essential tool to quantify the uncertainty of phylogeny. However, due to the complex nature of the phylogenetic tree object, which involve both a discrete topology and dependent continuous variables for branch lengths, the default approach for the phylogenetic inference has typically been an Markov-chain Monte Carlo (MCMC) method [29], enhanced with domain-specific techniques such as a mixed strategy for efficient exploration of tree topologies.

As an alternative approach to the conventional MCMCs, a variational Bayesian approach for phylogenetic inference was proposed in [42] (VBPI), which has subsequently been improved in the expressive powers of topology-dependent branch length distributions [39, 40]. Although these methods have presented accurate joint posterior distributions of topology and branch lengths for real datasets, they required reasonable preselection of candidate tree topologies to avoid a combinatorial explosion in the number of weight parameters beforehand. There have also been proposed variational approaches [24, 16] on top of the combinatorial sequential Monte Carlo method (CSMC[34]), which iteratively updated weighted topologies without the need for the preselection steps. However, the fidelity of the joint posterior distributions was still largely behind that of MCMC and VBPI as reported in [16].

In this work, we propose a simple yet effective scheme for parameterizing a binary tree topological distribution with a transformation of continuous distributions. We further formulate a novel differentiable variational Bayesian approach named GeoPhy 1 to approximate the posterior distribution of the tree topology and branch lengths without preselection of candidate topologies. By constructing practical topological models based on Euclidean and hyperbolic spaces, and employing variance reduction techniques for the stochastic gradient estimates of the variational lower bound, we have developed a robust algorithm that enables stable, gradient-based phylogenetic inference. In our experiments using real biological datasets, we demonstrate that GeoPhy significantly outperforms other approaches, all without topological restrictions, in terms of the fidelity of the marginal log-likelihood (MLL) estimates to gold-standard provided with long-run MCMCs.

Footnote 1: Our implementation is found at https://github.com/mlm0r1/geophy

Our contributions are summarized as follows:

* We propose a representation of tree topological distributions based on continuous distributions, thereby we construct a fully differentiable method named GeoPhy for variational Bayesian phylogenetic inference without restricting the support of the topologies.
* By considering variance reduction in stochastic gradient estimates and simple construction of topological distributions on Euclidean and hyperbolic spaces, GeoPhy offers unprecedently close marginal log-likelihood estimates to gold standard MCMC runs, outperforming comparable approaches without topological preselections.

## 2 Background

### Phylogenetic models

Let \(\tau\) be represent an unrooted binary tree topology with \(N\) leaf nodes (tips), and let \(B_{\tau}\) denote a set of evolutionary distances defined on each of the branches of \(\tau\). A phylogenetic tree \((\tau,B_{\tau})\) represents an evolutionary relationship between \(N\) species, which is inferred from molecular data, such as DNA, RNA or protein sequences obtained for the species. Let \(Y=\{Y_{ij}\in\Omega\}_{1\leq i\leq N,1\leq j\leq M}\) be a set of aligned sequences with length \(M\) from the species, where \(Y_{ij}\) denote a character (base) of the \(i\)-th sequence at \(j\)-th site, and is contained in a set of possible bases \(\Omega\). For DNA sequences, \(\Omega\) represents a set of 4-bit vectors, where each bit represents 'A', 'T', 'G', or 'C'. A likelihood model of the sequences \(P(Y|\tau,B_{\tau})\) is determined based on evolutionary assumptions. In this study, we follow a common practice for method evaluations [42; 40] as follows: \(Y\) is assumed to be generated from a Markov process along the branches of \(\tau\) in a site-independent manner; The base mutations are assumed to follow the Jukes-Cantor model [12]. The log-likelihood \(\ln P(Y|\tau,B_{\tau})\) can be calculated using Felsenstein's pruning algorithm [5], which is also known as the sum-product algorithm, and differentiable with respect to \(B_{\tau}\).

### Variational Bayesian phylogenetic inference

The variational inference problem for phylogenetic trees, which seeks to approximate the posterior probability \(P(\tau,B_{\tau}|Y)\), is formulated as follows:

\[\min_{Q}D_{\mathrm{KL}}\left(Q(\tau)Q(B_{\tau}|\tau)\|P(\tau,B_{\tau}|Y) \right),\] (1)

where \(D_{\mathrm{KL}}\), \(Q(\tau)\) and \(Q(B_{\tau}|\tau)\) denote the Kullback-Leibler divergence, a variational tree topology distribution, and a variational branch length distribution, respectively. The first variational Bayesian phylogenetic inference method (VBPI) was proposed by Zhang and Matsen IV [42], which has been successively improved for the expressiveness of \(Q(B_{\tau}|\tau)\)[39; 40]. For the expression of variational topology mass function \(Q(\tau)\), they all rely on a subsplit Bayesian network (SBN) [41], which represents tree topology mass function \(Q(\tau)\) as a product of conditional probabilities of splits (i.e., bipartition of the tip nodes) given their parent splits. However, SBN necessitates a preselection of the likely set of tree topologies, which hinders an end-to-end optimization strategy of the distribution over all the topologies and branch lengths.

### Learnable topological features (LTFs)

Zhang [40] introduced a deterministic embedding method for tree topologies, termed learnable topological features (LTFs), which provide unique signatures for each tree topology. Specifically, given a set of nodes \(V\) and edges \(E\) of a tree topology \(\tau\), a node embedding function \(f_{\rm emb}:V\rightarrow\mathbb{R}^{d}\) is determined to minimize the Dirichlet energy defined as follows:

\[\sum_{(u,v)\in E}\left\|f_{\rm emb}(u)-f_{\rm emb}(v)\right\|^{2}.\] (2)

When feature vectors for the tip nodes are determined, Zhang [40] showed that the optimal \(f_{\rm emb}\) for the interior nodes can be efficiently computed with two traversals of the tree topology. They also utilized \(f_{\rm emb}\) to parameterize the conditional distribution of branch length given tree topology \(Q(B_{\tau}|\tau)\) by using graph neural networks with the input node features \(\{f_{\rm emb}(v)|v\in V\}\).

### Hyperbolic spaces and probability distributions

Hyperbolic spaces are known to be able to embed hierarchical data with less distortion in considerably fewer dimensions than Euclidean spaces [32]. This property has been exploited to develop various data analysis applications such as link predictions for relational data [26], as well as phylogenetic analyses [21, 19]. For representing the uncertainty of embedded data on hyperbolic spaces, the wrapped normal distribution proposed in [25] has appealing characteristics: it is easy to sample from and evaluate the density of the distribution.

The Lorentz model \(\mathbb{H}^{d}\) is a representation of the \(d\)-dimensional hyperbolic space, which is a submanifold of \(d+1\) dimensional Euclidean space. Denote \(\mu\in\mathbb{H}^{d}\) and \(\Sigma\in\mathbb{R}^{d\times d}\) be a location and scale parameters, respectively, a random variable \(z\in\mathbb{H}^{d}\) that follows a wrapped normal distribution \(\mathcal{WN}(\mu,\Sigma)\) is defined as follows:

\[u\sim\mathcal{N}(0,\Sigma),\quad z=\exp_{\mu}\circ\mathrm{PT}_{\mu^{o}\to \mu}(u),\] (3)

where \(\mu^{o}\), \(\exp_{\mu}:T_{\mu}\mathbb{H}^{d}\rightarrow\mathbb{H}^{d}\), and \(\mathrm{PT}_{\mu^{o}\to\mu}:T_{\mu^{o}}\mathbb{H}^{d}\to T_{\mu} \mathbb{H}^{d}\) denote the origin, the exponential map, and the parallel transport defined on the Lorentz model. Also, the probability density \(\mathcal{WN}(\mu,\Sigma)\) has a closed form and is differentiable with respect to the parameters \(\mu,\Sigma\). More details and properties are summarized in Appendix A.

Figure 1: The proposed scheme for constructing a variational distribution \(Q(\tau,B_{\tau})\) of a tree topology and branch lengths by using a distribution \(Q(z)\) defined on the continuous space. **Left**: The marginal distributions of four tip nodes \(Q(z_{1}),\ldots,Q(z_{4})\). **Middle**: Coordinates of the tip nodes \(z=\{z_{1},\ldots,z_{4}\}\) sampled from the distribution \(Q(z)\), and a tree topology \(\tau(z)\) determined from \(z\). **Right**: A set of branch lengths \(B_{\tau}\) (red figures) of the tree \(\tau\) is sampled from a topology dependent distribution \(Q(B_{\tau}|\tau)\).

Proposed methods

### Geometric representations of tree topology ensembles

Considering the typically infeasible task of parameterizing the probability mass function of unrooted tree topologies \(\mathcal{T}\), which requires \((2N-5)!!-1\) degrees of freedom, we propose an alternative approach. We suggest constructing the mass function \(Q(\tau)\) through a transformation of a certain probability density \(Q(z)\) over a continuous domain \(\mathcal{Z}\), as follows:

\[Q(\tau):=\mathbb{E}_{Q(z)}[\mathbb{I}[\tau=\tau(z)]],\] (4)

where \(\tau:\mathcal{Z}\rightarrow\mathcal{T}\) denotes a deterministic link function that maps \(N\) coordinates to the corresponding tree topology (Fig. 1). Note that we have overloaded \(\tau\) to represent both a variable and function for notational simplicity. An example of the representation space \(\mathcal{Z}\) is a product of the Euclidean space \(\mathbb{R}^{N\times d}\) or hyperbolic space \(\mathbb{H}^{N\times d}\), where \(d\) denotes the dimension of each tip's representation coordinate. For the link function, we can use \(\tau(z)=T_{\mathrm{N}j}\circ D(z)\), where \(D:\mathcal{Z}\rightarrow\mathbb{R}^{N\times N}\) denotes a function that takes \(N\) coordinates and provides a distance matrix between those based on a geometric measure such as the Euclidean or hyperbolic distance. \(T_{\mathrm{N}j}:\mathbb{R}^{N\times N}\rightarrow\mathcal{T}\) denotes a map that takes this distance matrix and generates an unrooted binary tree topology of their phylogeny, determined using the Neighbor-Joining (NJ) algorithm [31]. While the NJ algorithm offers a rooted binary tree topology accompanied by estimated branch lengths, we only use the topology information and remove the root node from it to obtain the unrooted tree topology \(\tau\in\mathcal{T}\).

### Derivation of variational lower bound

Given a distribution of tip coordinates \(Q(z)\) and an induced tree topology distribution \(Q(\tau)\) according to equation (4), the variational lower bound (1) is evaluated as follows:

\[\mathcal{L}[Q]=\mathbb{E}_{Q(z)}\left[\mathbb{E}_{Q(B_{\tau}|\tau(z))}\left[ \ln\frac{P(Y,B_{\tau}|\tau(z))}{Q(B_{\tau}|\tau(z))}\right]+\ln\frac{P(\tau(z) )}{Q(\tau(z))}\right].\] (5)

Thanks to the deterministic mapping \(\tau(z)\), we can obtain an unbiased estimator of \(\mathcal{L}[Q]\) by sampling from \(Q(z)\) without summing over the combinatorial many topologies \(\mathcal{T}\). However, even when the density \(Q(z)\) is computable, the evaluation of \(\ln Q(\tau)\) remains still infeasible according to the definition (4). We resolve this issue by introducing the second lower bound with respect to a conditional variational distribution \(R(z|\tau)\) as follows:

\[\mathcal{L}[Q,R]=\mathbb{E}_{Q(z)}\left[\mathbb{E}_{Q(B_{\tau}|\tau(z))}\left[ \ln\frac{P(Y,B_{\tau}|\tau(z))}{Q(B_{\tau}|\tau(z))}\right]+\ln\frac{P(\tau(z) )R(z|\tau(z))}{Q(z)}\right].\] (6)

This formulation is similar in structure to the variational auto-encoders [14], where \(Q(\tau|z)\) and \(R(z|\tau)\) are viewed as a probabilistic decoder and encoder of data \(\tau\), respectively. However, unlike typical auto-encoders, we design \(Q(\tau|z)\) to be deterministic, and instead adjust \(Q(z)\) to approximate \(Q(\tau)\).

**Proposition 1**.: Assuming that \(\operatorname{supp}R(z|\tau)\supseteq\operatorname{supp}Q(z|\tau)\), the inequality \(\ln P(Y)\geq\mathcal{L}[Q]\geq\mathcal{L}[Q,R]\) holds, where \(\operatorname{supp}\) denotes the support of distribution. The first and the second equality holds when \(Q(\tau,B_{\tau})=P(\tau,B_{\tau}|Y)\) and \(R(z|\tau)=Q(z|\tau)\), respectively.

Proof.: The first variational lower bound of the marginal log-likelihood is given as follows:

\[\ln P(Y)\geq\ln P(Y)-D_{\mathrm{KL}}\left[Q(\tau,B_{\tau})\|P(\tau,B_{\tau}|Y )\right]=\mathbb{E}_{Q(\tau,B_{\tau})}\left[\ln\frac{P(Y,\tau,B_{\tau})}{Q( \tau,B_{\tau})}\right]:=\mathcal{L}[Q],\] (7)

where the equality condition of the first inequality holds when \(Q(\tau,B_{\tau})=P(\tau,B_{\tau}|Y)\). Since we have defined \(Q(\tau)\) in equation (4), we can further transform the lower bound as \(\mathcal{L}[Q]\)

\[\mathcal{L}[Q]=\mathbb{E}_{Q(z)}\left[\sum_{\tau\in\mathcal{T}}\mathbb{I}[ \tau=\tau(z)]\,\mathbb{E}_{Q(B_{\tau}|\tau)}\left[\ln\frac{P(Y,B_{\tau}|\tau) }{Q(B_{\tau}|\tau)}+\ln\frac{P(\tau)}{Q(\tau)}\right]\right],\] (8)

from which equation (5) immediately follows. Hence the inequality \(\ln P(Y)\geq\mathcal{L}[Q]\) and its equality condition \(Q(\tau,B_{\tau})=P(\tau,B_{\tau}|Y)\) have been proven.

Next, the entropy term \(-\operatorname{\mathbb{E}}_{Q(z)}[\ln Q(\tau(z))]=-\operatorname{\mathbb{E}}_{Q(z)} [\ln Q(\tau)]\) in equation (5) can be transformed to derive further lower bound as follows:

\[-\operatorname{\mathbb{E}}_{Q(\tau)}[\ln Q(\tau)] \geq-\operatorname{\mathbb{E}}_{Q(\tau)}[\ln Q(\tau)+D_{\mathrm{KL }}\left(Q(z|\tau)\|R(z|\tau)\right)]\] \[=-\operatorname{\mathbb{E}}_{Q(\tau)Q(z|\tau)}\left[\ln\frac{Q( \tau)Q(z|\tau)}{R(z|\tau)}\right]=-\operatorname{\mathbb{E}}_{Q(z)}\left[\ln \frac{Q(z)}{R(z|\tau(z))}\right],\]

where the last equality is derived by using the relation \(\operatorname{\mathbb{E}}_{Q(\tau)Q(z|\tau)}[\cdot]=\operatorname{\mathbb{E}} _{Q(z)}[\sum_{\tau}\mathbb{I}[\tau=\tau(z)]\cdot]\) and \(Q(\tau)Q(z|\tau)=Q(z)\mathbb{I}[\tau=\tau(z)]\). The equality condition of the first inequality holds when \(R(z|\tau)=Q(z|\tau)\). Hence, the inequality \(\mathcal{L}[Q]\geq\mathcal{L}[Q,R]\) and the equality condition is proven. 

Similar to Burda et al. [1], we can also derive a tractable importance-weighted lower-bound of the model evidence (IW-ELBO), which is used for estimating the marginal log-likelihood (MLL), \(\ln P(Y)\), or an alternative lower-bound objective for maximization. The details and derivations are described in Appendix B.

### Differentiable phylogenetic inference with GeoPhy

```
1:\(\theta,\phi,\psi\leftarrow\) Initialize variational parameters
2:while not converged do
3:\(\epsilon_{z}^{(1:K)},\epsilon_{B}^{(1:K)}\leftarrow\)Random samples from distributions \(p_{z}\), \(p_{B}\)
4:\(z^{(1:K)}\gets h_{\theta}(\epsilon_{z}^{(1:K)})\)
5:\(B_{\tau}^{(1:K)}\gets h_{\phi}(\epsilon_{B}^{(1:K)},\tau(z^{(1:K)}))\)
6:\(\widehat{g}_{\theta},\widehat{g}_{\phi},\widehat{g}_{\psi}\leftarrow\)Estimate the gradients \(\nabla_{\theta}\mathcal{L}\), \(\nabla_{\phi}\mathcal{L}\), \(\nabla_{\psi}\mathcal{L}\)
7:\(\theta,\phi,\psi\leftarrow\)Update parameters using an SGA algorithm, given gradients \(\widehat{g}_{\theta},\widehat{g}_{\phi},\widehat{g}_{\psi}\)
8:endwhile
9:return\(\theta,\phi,\psi\) ```

**Algorithm 1** GeoPhy algorithm

Here, we introduce parameterized distributions, \(Q_{\theta}(z)\), \(Q_{\phi}(B_{\tau}|\tau)\), and \(R_{\psi}(z|\tau)\), aiming to optimize the variational objective \(\mathcal{L}[Q_{\theta,\phi},R_{\psi}]\) using stochastic gradient ascent (SGA). The framework, which we term GeoPhy, is summarized in Algorithm 1. While aligning with black-box variational inference [27] and its extension to phylogenetic inference [42], our unique lower bound objective enables us to optimize the distribution over entire phylogenetic trees within continuous geometric spaces.

Gradients of lower boundWe derive the gradient terms for stochastic optimization of \(\mathcal{L}\), omitting parameters \(\theta,\phi,\psi\) for notational simplicity. We assume that \(Q_{\theta}(z)\) and \(Q_{\phi}(B_{\tau}|\tau(z))\) are both reparameterizable; namely, we can sample from these distributions as follows:

\[z=h_{\theta}(\epsilon_{z}),\quad B_{\tau}=h_{\phi}(\epsilon_{B},\tau(z)),\] (9)

where the terms \(\epsilon_{z}\sim p_{z}(\epsilon_{z})\) and \(\epsilon_{B}\sim p_{B}(\epsilon_{B})\) represent sampling from parameter-free distributions \(p_{z}\) and \(p_{B}\), respectively, and the functions \(h_{\theta}\) and \(h_{\phi}\) are differentiable with respect to \(\theta\) and \(\phi\), respectively. Although we cannot take the derivative of \(\tau(z)\) with respect to \(z\), the gradient of \(\mathcal{L}\) with respect to \(\theta\) is evaluated as follows:

\[\nabla_{\theta}\mathcal{L} =\operatorname{\mathbb{E}}_{Q_{\theta}(z)}\left[(\nabla_{\theta} \ln Q_{\theta}(z))\left(\operatorname{\mathbb{E}}_{Q_{\phi}(B_{\tau}|\tau(z) )}\left[\ln\frac{P(Y,B_{\tau}|\tau(z))}{Q_{\phi}(B_{\tau}|\tau(z))}\right]+ \ln P(\tau(z))R_{\psi}(z|\tau(z))\right)\right]\] \[\quad+\nabla_{\theta}\mathbb{H}[Q_{\theta}(z)],\] (10)

where \(\mathbb{H}\) denotes the differential entropy. The gradient of \(\mathcal{L}\) with respect to \(\phi\) and \(\psi\) are evaluated as follows:

\[\nabla_{\phi}\mathcal{L}=\operatorname{\mathbb{E}}_{Q_{\theta}(z)} \operatorname{\mathbb{E}}_{p_{B}(\epsilon)}\left[\nabla_{\phi}\ln\frac{P(Y,B_ {\tau}=h_{\phi}(\epsilon,\tau)|\tau(z))}{Q_{\phi}(B_{\tau}=h_{\phi}(\epsilon, \tau)|\tau(z))}\right],\quad\nabla_{\psi}\mathcal{L}=\operatorname{\mathbb{E} }_{Q_{\theta}(z)}\left[\nabla_{\psi}\ln R_{\psi}(z|\tau(z))\right],\] (11)

where we assume a tractable density model for \(R_{\psi}(z|\tau)\).

Gradient estimators and variance reductionFrom equation (10), an unbiased estimator of \(\nabla_{\theta}\mathcal{L}\), using \(K\)-sample Monte Carlo samples, can be derived as follows:

\[\widehat{g}_{\theta}^{(K)}=\frac{1}{K}\sum_{k=1}^{K}\left(\nabla_{\theta}\ln Q_{ \theta}(z^{(k)})\cdot f(z^{(k)},B_{\tau}^{(k)})-\nabla_{\theta}\ln Q_{\theta}( h_{\theta}(\epsilon_{z}^{(k)}))\right),\] (12)

where we denote \(\epsilon_{z}^{(k)}\sim p_{z}\,z^{(k)}=h_{\theta}(\epsilon_{z}^{(k)})\), \(\epsilon_{B}^{(k)}\sim p_{B}\), \(B_{\tau}^{(k)}=h_{\phi}(\epsilon_{B}^{(k)},\tau(z^{(k)}))\), and

\[f(z,B_{\tau}):=\ln\frac{P(Y,B_{\tau}|\tau(z))}{Q_{\phi}(B_{\tau}|\tau(z))}+\ln P (\tau(z))R_{\psi}(z|\tau(z)).\] (13)

Note that we explicitly distinguish \(z\) and \(h_{\theta}(\epsilon_{z})\) to indicate the target of differentiation with respect to \(\theta\).

In practice, it is known to be crucial to reducing the variance of the gradient estimators proportional to the score function \(\nabla_{\theta}\ln Q_{\theta}\) to make optimization feasible. This issue is addressed by introducing a control variate \(c(z)\), which has a zero expectation \(\mathbb{E}_{z}[c(z)]=0\) and works to reduce the variance of the term by subtracting it from the gradient estimator. For the case of \(K>1\), it is known to be effective to use simple Leave-one-out (LOO) control variates [15, 28] as follows:

\[\widehat{g}_{\theta,\mathrm{LOO}}^{(K)}=\frac{1}{K}\sum_{k=1}^{K}\left[\nabla _{\theta}\ln Q_{\theta}(z^{(k)})\cdot\left(f(z^{(k)},B_{\tau}^{(k)})-\overline {f_{k}}(z^{(\setminus k)},B_{\tau}^{(\setminus k)})\right)-\nabla_{\theta} \ln Q_{\theta}(h_{\theta}(\epsilon_{z}^{(k)}))\right],\] (14)

where we defined \(\overline{f_{k}}(z^{\setminus(k)},B_{\tau}^{(\setminus k)}):=\frac{1}{K-1} \sum_{k^{\prime}=1,k^{\prime}\neq k}^{K}f(z^{(k)},B_{\tau}^{(k)})\), which is a constant with respect to the \(k\)-the sample.

We can also adopt a gradient estimator called LAX [7], which uses a learnable surrogate function \(s_{\chi}(z)\) differentiable in \(z\) to form control variates as follows:

\[\widehat{g}_{\theta,\mathrm{LAX}}=\left(\nabla_{\theta}\ln Q_{\theta}(z) \right)(f(z,B_{\tau})-s_{\chi}(z))+\nabla_{\theta}s_{\chi}(h_{\theta}(\epsilon _{z}))-\nabla_{\theta}\ln Q_{\theta}(h_{\theta}(\epsilon_{z})),\] (15)

where we write the case with \(K=1\) for simplicity. To optimize the surrogate function \(s_{\chi}\), we use the estimator \(\widehat{g}_{\chi}=\nabla_{\chi}\left\langle\widehat{g}_{\theta}^{2}\right\rangle\)2, where \(\left\langle\cdot\right\rangle\) denotes the average of the vector elements. We summarize the procedure with LAX estimator in Algorithm 2.

Footnote 2: \(\widehat{g}_{\chi}\) is an unbiased estimator of \(\nabla\left\langle\mathbb{V}_{Q(z)}[(\widehat{g}_{\theta})]\right\rangle\) as shown in [7].

We also consider other options for the gradient estimators, such as the combinations of the LOO and LAX estimators, the gradient of IW-ELBO, and its variance-reduced estimator called VIMCO [22] in our experiments. For the remaining gradients terms \(\nabla_{\phi}\mathcal{L}\) and \(\nabla_{\psi}\mathcal{L}\), we can simply employ reparameterized estimators as follows:

\[\widehat{g}_{\phi}=\nabla_{\phi}\ln\frac{P(Y,B_{\tau}=h_{\phi}(\epsilon_{B}, \tau)|\tau(z))}{Q_{\phi}(B_{\tau}=h_{\phi}(\epsilon_{B},\tau)|\tau(z))},\quad \widehat{g}_{\psi}=\nabla_{\psi}\ln R_{\psi}(z|\tau(z)),\] (16)

where we denote \(\epsilon_{B}\sim p_{B}\) and \(z\sim Q_{\theta}\). More details of the gradient estimators are summarized in Appendix B.

Variational distributionsTo investigate the basic effectiveness of GeoPhy algorithm, we employ simple constructions for the variational distributions \(Q_{\theta}(z)\), \(Q_{\phi}(B_{\tau}|\tau)\), and \(R_{\psi}(z|\tau)\). We use an independent distribution for each tip node coordinate, i.e. \(Q_{\theta}(z)=\prod_{i=1}^{N}Q_{\theta_{i}}(z_{i})\), where we use a \(d\)-dimensional normal or wrapped normal distribution for the coordinates of each tip node \(z_{i}\). For the conditional distribution of branch lengths given tree topology, \(Q_{\phi}(B_{\tau}|\tau)\), we use the diagonal lognormal distribution whose location and scale parameters are given as a function of the LTFs of the topology \(\tau\), which is comprised of GNNs as proposed in [40]. For the model of \(R_{\psi}(z|\tau)\), we also employ an independent distribution: \(R_{\psi}(z|\tau)=\prod_{i=1}^{N}R_{\psi_{i}}(z_{i}|\tau)\), where, we use the same type of distribution as \(Q_{\theta_{i}}(z_{i})\), independent of \(\tau\).

Related work

Differentiability for discrete optimizationDiscrete optimization problems often suffer from the lack of informative gradients of the objective functions. To address this issue, continuous relaxation for discrete optimization has been actively studied, such as a widely-used reparameterization trick with the Gumbel-softmax distribution [10; 20]. Beyond categorical variables, recent approaches have further advanced the continuous relaxation techniques to more complex discrete objects, including spanning trees [33]. However, it is still nontrivial to extend such techniques to the case of binary tree topologies. As outlined in equation (4), we have introduced a distribution over binary tree topologies \(\mathcal{T}\) derived from continuous distributions \(Q(z)\). This method facilitates a gradient-based optimization further aided by variance reduction techniques.

Gradient-based algorithms for tree optimizationFor the hierarchical clustering (HC), which reconstructs a tree relationship based on the distance measures between samples, gradient-based algorithms [23; 2; 3] have been proposed based on Dasgupta's cost function [4]. In particular, Chami et al. [2] proposed to decode tree topology from hyperbolic coordinates while the optimization is performed for a relaxed cost function, which is differentiable with respect to the coordinates. However, these approaches are not readily applicable to more general problems, including phylogenetic inference, as their formulations depend on the specific form of the cost functions.

Phylogenetic analysis in hyperbolic spaceThe approach of embedding phylogenetic trees into hyperbolic spaces has been explored for visualization and an interpretation of novel samples with existing phylogeny [21; 11]. For the inference task, a maximum-likelihood approach was proposed in [35], which however assumed a simplified likelihood function of pairwise distances. A recent study [19] proposed an MCMC-based algorithm for sampling \((\tau,B_{\tau})\), which were linked from coordinates \(z\) using the NJ algorithm [31]. However, there remained the issue of an unevaluated Jacobian determinant, which posed a challenge in evaluating inference objectives. Given that we only use topology \(\tau\) as described in equation (4), the variational lower bound for the inference can be unbiasedly evaluated through sampling, as shown in Proposition 1.

## 5 Experiments

We applied GeoPhy to perform phylogenetic inference on biological sequence datasets of 27 to 64 species compiled in Lakner et al. [17].

Models and trainingAs a prior distribution of \(P(\tau)\) and \(P(B_{\tau}|\tau)\), we assumed a uniform distribution over all topologies, and an exponential distribution \(\mathrm{Exp}(10)\) independent for all branches, respectively, as commonly used in the literature [42; 16]. For the neural network used in the parameterization of \(Q_{\phi}(B_{\tau}|\tau)\), we employed edge convolutional operation (EDGE), which was well-performed architecture in [40]. For the stochastic gradient optimizations, we used the Adam optimizer [13] with a learning rate of 0.001. We trained for GeoPhy until one million Monte Carlo tree samples were consumed for the gradient estimation of our loss function. This number equals the number of likelihood evaluations (NLEs) and is used for a standardized comparison of experimental runs [34; 42]. The marginal log-likelihood (MLL) value is estimated with 1000 MC samples. More details of the experimental setup are found in Appendix C.

Initialization of coordinatesWe initialized the mean parameters of the tip coordinate distribution \(Q_{\theta}(z)\) with the multi-dimensional scaling (MDS) algorithm when \(Q_{\theta}\) was given as normal distributions. For \(Q_{\theta}\) comprised of wrapped normal distributions, we used the hyperbolic MDS algorithm (hMDS) proposed in [32] for the initialization. For a distance matrix used for MDS and hMDS, we used the Hamming distance between each pair of the input sequences \(Y\) as similar to [19]. For the scale parameters, we used \(0.1\) for all experiments. For \(R_{\psi}(z)\), we used the same mean parameters as \(Q_{\theta}(z)\) and 1.0 for the scale parameters.

### Exploration of stable learning conditions

To achieve a stable and fast convergence in the posterior approximations, we compared several control variates (CVs) for the variance reduction of the gradients by using DS1 dataset [8]. Wedemonstrate that the choice of control variates is crucial for optimization (Fig. 2). In particular, adaptive control variates (LAX) for individual Monte Carlo samples and the leave-one-out (LOO) CVs for multiple Monte Carlo samples (with \(K=3\)) significantly accelerate the convergence of the marginal log-likelihood (MLL) estimates, yielding promising results comparable to VBPI-GNN. Although IW-ELBO was effective when \(Q(z)\) was comprised of diagonal Normal distributions, no advantages were observed for the case with full covariance matrices. Similar tendencies in the MLL estimates were observed for the other dimension \((d=3,4)\) and wrapped normal distributions (Appendix D.1).

### Comparison of different topological distributions

We investigated effective choices of tip coordinate distributions \(Q(z)\), which yielded tree topologies, by comparing different combinations of distribution types (normal \(\mathcal{N}\) or wrapped normal \(\mathcal{WN}\)), space dimensions (\(d=2,3,4\)), and covariance matrix types (diagonal or full), across selected CVs (LAX, LOO, and LOO+LAX). While overall performance was stable and comparable for the range of configurations, the most flexible ones: the normal and wrapped normal distributions with \(d=4\) and a full covariance matrix, indicated relatively high MLL estimates within their respective groups (Table 2 in Appendix D), implying the importance of flexibility in the variational distributions.

### Performance evaluation across eight benchmark datasets

To demonstrate the inference performance of GeoPhy, we compared the marginal log-likelihood (MLL) estimates for the eight real datasets (DS1-8) [8; 6; 37; 9; 17; 43; 38; 30]. The gold-standard values were obtained using the stepping-stone (SS) algorithm [36] in MrBayes [29], wherein each evaluation involved ten independent runs, each with four chains of 10,000,000 iterations, as reported in [42]. In Table 1, we summarize the MLL estimates of GeoPhy and other approximate Bayesian inference approaches for the eight datasets. While VBPI-GNN [40] employs a preselected set of tree topologies as its support set before execution, it is known to provide reasonable MLL estimates near the reference values. The other approaches including CSMC [34], VCSMC [24], \(\phi\)-CSMC [16] and GeoPhy (ours) tackles the more challenging general problem of model optimization by considering all candidate topologies without preselection. We compared the GeoPhy for two configurations of \(Q(z)\): a wrapped normal distribution \(\mathcal{WN}\) with a 4-dimensional full covariance matrix, and a 2-dimensional diagonal matrix, with three choices of CVs for optimization. Results for other settings, including those of the normal distributions in Euclidean spaces, are provided in Table 3 in Appendix D. There, we noted a slight advantage of wrapped normal distributions over their Euclidean counterparts. In

Figure 2: Comparison of marginal log-likelihood (MLL) estimates for DS1 dataset with different control variates. For a variational distribution, \(Q(z)\), an independent two-dimensional Normal distribution with a diagonal (diag) or full covariance matrix was used for each tip node. \(K\) stands for the number of Monte Carlo samples used for gradient estimation. For reference, we show the mean MLL value (gray dashed lines; \(-7108.42\pm 0.18\)) estimated with the MrBayes stepping-stone (SS) method in [42]. We also show MLL estimates obtained with VBPI-GNN (red dashed lines) using VIMCO estimator \((K=10)\). The iterations are counted as the number of likelihood evaluations (NLEs). Legend: NoCV stands for no control variates, LOO for leave-one-out CV, and IW for the case of using importance-weighted ELBO as the learning objective.

[MISSING_PAGE_FAIL:9]

progressively aligns with those obtained via MCMC (Fig. 3). Furthermore, we also observed that the Robinson-Foulds (RF) distance between the consensus trees derived from the topological distribution \(Q(\tau)\) and MCMC (MrBayes) are highly aligned with the MLL estimates (Fig. 4 First to Third), underscoring the validity of MLL-based evaluations. While GeoPhy provides a tree topology mode akin to MCMCs with a close MLL value, it may require more expressivity in \(Q(z)\) to fully represent tree topology distributions. In Fig. 4 Fourth, we illustrate the room for improvement in the expressivity of \(Q(\tau)\) in representing the bipartition frequency of tree samples, where VBPI-GNN aligns more closely with MrBayes. Further analysis is found in Appendix D.3.

In terms of execution time, we compared GeoPhy with VBPI-GNN and MrBayes, obtaining results that are promising and comparable to VBPI-GNN (Fig. 10 Left). Though we have not included CSMC-based methods in the comparison due to deviations in MLL values, they tend to capitalize on parallelism for faster execution as demonstrated in [16].

## 6 Conclusion

We developed a novel differential phylogenetic inference framework named GeoPhy, which optimized a variational distribution of tree topology and branch lengths without the preselection of candidate topologies. We also proposed a practical implementation for stable model optimization through choices of distribution models and control variates. In experiments conducted with real sequence datasets, GeoPhy consistently outperformed other approximate Bayesian methods that considered whole topologies.

## 7 Limitations and Future work

Although GeoPhy exhibits remarkable performance on standard benchmarks without preselecting topologies, it may be necessary to use more expressive distributions for \(Q(z)\) than independent parametric distributions to reach a level of performance comparable to state-of-the-art VBPI or gold-standard MCMC evaluations. Another limitation of our study lies in its exclusive focus on efficiency, measured in terms of the number of likelihood evaluations (NLEs), without paying significant attention to optimizing the computational cost per iteration. The general design of our variational distribution allows us to replace the tree link function such as UPGMA as presented in Fig. 9 Right. Future work should explore alternative design choices in continuous tree topology representations and the link functions to address these limitations. Extending our framework to include complex models remains crucial. Given the potential of scalable phylogenetic methods to enhance our understanding of viral and bacterial evolution, further exploration could substantially impact public health and disease control.

Figure 4: **First** to **Third**: Marginal log-likelihood (MLL) estimates and Robinson-Foulds (RF) distance between the consensus trees obtained from topology distribution \(Q(\tau)\) and MrBayes (MCMC) for datasets DS1, DS3, and DS7, respectively. **Fourth**: Comparison of bipartition frequencies derived from the posterior distribution of tree topologies for MrBayes, VBPI-GNN, and GeoPhy (ours).

## Acknowledgments and Disclosure of Funding

We appreciate the valuable comments on our study provided by Dr. Tsukasa Fukunaga of Waseda University and the anonymous reviewers. TM was partially supported by JSPS KAKENHI JP20H04239 and JST ACT-X JPMJAX22AI. MH was partially supported by JST CREST JPMJCR21F1 and AMED JP22ama121055, JP21ae0121049 and JP21gm0010008.

## References

* [1] Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Yoshua Bengio and Yann LeCun, editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL http://arxiv.org/abs/1509.00519.
* [2] Ines Chami, Albert Gu, Vaggos Chatziafratis, and Christopher Re. From trees to continuous embeddings and back: Hyperbolic hierarchical clustering. _Advances in Neural Information Processing Systems_, 33:15065-15076, 2020.
* [3] Eli Chien, Puoya Tabaghi, and Olgica Milenkovic. Hyperaid: Denoising in hyperbolic spaces for tree-fitting and hierarchical clustering. _arXiv preprint arXiv:2205.09721_, 2022.
* [4] Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_, pages 118-127, 2016.
* [5] Joseph Felsenstein. Maximum likelihood and minimum-steps methods for estimating evolutionary trees from data on discrete characters. _Systematic Biology_, 22(3):240-249, 1973.
* [6] James R Garey, Thomas J Near, Michael R Nonnemacher, and Steven A Nadler. Molecular evidence for acanthocephala as a subtaxon of rotifera. _Journal of Molecular Evolution_, 43:287-292, 1996.
* [7] Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. In _International Conference on Learning Representations_, 2018.
* [8] S Blair Hedges, Kirk D Moberg, and Linda R Maxson. Tetrapod phylogeny inferred from 18s and 28s ribosomal rna sequences and a review of the evidence for amniote relationships. _Molecular Biology and Evolution_, 7(6):607-633, 1990.
* [9] Daniel A Henk, Alex Weir, and Meredith Blackwell. Laboulbeniopsis termitarius, an ectopara-site of termites newly recognized as a member of the laboulbeniomycetes. _Mycologia_, 95(4):561-564, 2003.
* [10] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. _arXiv preprint arXiv:1611.01144_, 2016.
* [11] Yueyu Jiang, Puoya Tabaghi, and Siavash Mirarab. Phylogenetic placement problem: A hyperbolic embedding approach. In _Comparative Genomics: 19th International Conference, RECOMB-CG 2022, La Jolla, CA, USA, May 20-21, 2022, Proceedings_, pages 68-85. Springer, 2022.
* [12] Thomas H Jukes, Charles R Cantor, et al. Evolution of protein molecules. _Mammalian protein metabolism_, 3:21-132, 1969.
* [13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [14] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [15] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get a baseline for free! 2019.

* [16] Hazal Koptagel, Oskar Kviman, Harald Melin, Negar Safinianaini, and Jens Lagergren. Vaiphy: a variational inference based algorithm for phylogeny. In _Advances in Neural Information Processing Systems_, 2022.
* [17] Clemens Lakner, Paul Van Der Mark, John P Huelsenbeck, Bret Larget, and Fredrik Ronquist. Efficiency of markov chain monte carlo tree proposals in bayesian phylogenetics. _Systematic biology_, 57(1):86-103, 2008.
* [18] Marc Law, Renjie Liao, Jake Snell, and Richard Zemel. Lorentzian distance learning for hyperbolic representations. In _International Conference on Machine Learning_, pages 3672-3681. PMLR, 2019.
* [19] Matthew Macaulay, Aaron E Darling, and Mathieu Fourment. Fidelity of hyperbolic space for bayesian phylogenetic inference. _PLoS computational biology_, 2023.
* [20] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. _arXiv preprint arXiv:1611.00712_, 2016.
* [21] Hirotaka Matsumoto, Takahiro Mimori, and Tsukasa Fukunaga. Novel metric for hyperbolic phylogenetic tree embeddings. _Biology Methods and Protocols_, 6(1):bpab006, 2021.
* [22] Andriy Mnih and Danilo Rezende. Variational inference for monte carlo objectives. In _International Conference on Machine Learning_, pages 2188-2196. PMLR, 2016.
* [23] Nicholas Monath, Manzil Zaheer, Daniel Silva, Andrew McCallum, and Amr Ahmed. Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 714-722, 2019.
* [24] Antonio Khalil Moretti, Liyi Zhang, Christian A Naesseth, Hadiah Venner, David Blei, and Itsik Pe'er. Variational combinatorial sequential monte carlo methods for bayesian phylogenetic inference. In _Uncertainty in Artificial Intelligence_, pages 971-981. PMLR, 2021.
* [25] Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A wrapped normal distribution on hyperbolic space for gradient-based learning. In _International Conference on Machine Learning_, pages 4693-4702. PMLR, 2019.
* [26] Maximillian Nickel and Douwe Kiela. Poincare embeddings for learning hierarchical representations. _Advances in neural information processing systems_, 30, 2017.
* [27] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In _Artificial intelligence and statistics_, pages 814-822. PMLR, 2014.
* [28] Lorenz Richter, Ayman Boustati, Nikolas Nusken, Francisco Ruiz, and Omer Deniz Akyildiz. Vargrad: a low-variance gradient estimator for variational inference. _Advances in Neural Information Processing Systems_, 33:13481-13492, 2020.
* [29] Fredrik Ronquist, Maxim Teslenko, Paul Van Der Mark, Daniel L Ayres, Aaron Darling, Sebastian Hohna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. Mrbayes 3.2: efficient bayesian phylogenetic inference and model choice across a large model space. _Systematic biology_, 61(3):539-542, 2012.
* [30] Amy Y Rossman, John M McKemy, Rebecca A Pardo-Schultheiss, and Hans-Josef Schroers. Molecular studies of the bionectriaceae using large subunit rdma sequences. _Mycologia_, 93(1):100-110, 2001.
* [31] Naruya Saitou and Masatoshi Nei. The neighbor-joining method: a new method for reconstructing phylogenetic trees. _Molecular biology and evolution_, 4(4):406-425, 1987.
* [32] Frederic Sala, Chris De Sa, Albert Gu, and Christopher Re. Representation tradeoffs for hyperbolic embeddings. In _International conference on machine learning_, pages 4460-4469. PMLR, 2018.

* Struminsky et al. [2021] Kirill Struminsky, Artyom Gadetsky, Denis Rakitin, Danil Karpushkin, and Dmitry P Vetrov. Leveraging recursive gumbel-max trick for approximate inference in combinatorial spaces. _Advances in Neural Information Processing Systems_, 34:10999-11011, 2021.
* Wang et al. [2015] Liangliang Wang, Alexandre Bouchard-Cote, and Arnaud Doucet. Bayesian phylogenetic inference using a combinatorial sequential monte carlo method. _Journal of the American Statistical Association_, 110(512):1362-1374, 2015.
* Wilson [2021] Benjamin Wilson. Learning phylogenetic trees as hyperbolic point configurations. _arXiv preprint arXiv:2104.11430_, 2021.
* Xie et al. [2011] Wangang Xie, Paul O Lewis, Yu Fan, Lynn Kuo, and Ming-Hui Chen. Improving marginal likelihood estimation for bayesian phylogenetic model selection. _Systematic biology_, 60(2):150-160, 2011.
* Yang and Yoder [2003] Ziheng Yang and Anne D Yoder. Comparison of likelihood and bayesian methods for estimating divergence times using multiple gene loci and calibration points, with application to a radiation of cute-looking mouse lemur species. _Systematic biology_, 52(5):705-716, 2003.
* Yoder and Yang [2004] Anne D Yoder and Ziheng Yang. Divergence dates for malagasy lemurs estimated from multiple gene loci: geological and evolutionary context. _Molecular Ecology_, 13(4):757-773, 2004.
* Zhang [2020] Cheng Zhang. Improved variational bayesian phylogenetic inference with normalizing flows. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 18760-18771. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/d96409bf894217686ba124d47356686c9-Paper.pdf.
* Zhang [2023] Cheng Zhang. Learnable topological features for phylogenetic inference via graph neural networks. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=hVVVV7p64WL.
* Zhang and Matsen [2018] Cheng Zhang and Frederick A Matsen IV. Generalizing tree probability estimation via bayesian networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems 31_, pages 1449-1458. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/7418-generalizing-tree-probability-estimation-via-bayesian-networks.pdf.
* Zhang and Matsen [2019] Cheng Zhang and Frederick A. Matsen IV. Variational bayesian phylogenetic inference. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=SJVmjjR9FX.
* Zhang and Blackwell [2001] Ning Zhang and Meredith Blackwell. Molecular phylogeny of dogwood anthracose fungus (discula destructiva) and the diaporthales. _Mycologia_, 93(2):355-365, 2001.

Details of Background

### Summary of calculations for wrapped normal distributions

The wrapped normal distribution, as proposed in [25], is a probability distribution defined on hyperbolic spaces, which is easy to sample from and evaluate its probability density at arbitrary coordinates in the hyperbolic spaces. In the following, we provide a detailed summary of the calculation involved in applying the wrapped normal distributions within the context of the Lorentz model of hyperbolic spaces.

The Lorentz model, denoted as \(\mathbb{H}^{d}\), represents the \(d\)-dimensional hyperbolic space as a submanifold of a \(d+1\) dimensional Euclidean space. Given \(u,v\in\mathbb{R}^{d+1}\), we can define the pseudo-inner product and pseudo-norm as follows:

\[\left\langle u,v\right\rangle_{L}:=-u_{0}v_{0}+\sum_{j=1}^{d}u_{j}v_{j},\quad \left\|u\right\|_{L}:=\sqrt{\left\langle u,u\right\rangle_{L}}.\] (17)

The distance between hyperbolic coordinates \(\nu,\mu\in\mathbb{H}^{d}\) is defined as follows:

\[\mathrm{d}(\nu,\mu):=\cosh^{-1}(-\left\langle\nu,\mu\right\rangle_{L}),\] (18)

where \(\cosh^{-1}\) denotes the inverse function of the hyperbolic cosine function. Consider hyperbolic coordinates \(\nu,\mu\in\mathbb{H}^{d}\) and tangent vectors \(u\in T_{\mu}\mathbb{H}^{d}\) and \(v\in T_{\nu}\mathbb{H}^{d}\). An exponential map \(\exp_{\mu}(u)\in\mathbb{H}^{d}\), a logarithm map \(\log_{\mu}(\nu)\in T_{\mu}\mathbb{H}^{d}\), and a parallel transport map \(\mathrm{PT}_{\nu\rightarrow\mu}(v)\in T_{\mu}\mathbb{H}^{d}\), can be calculated as follows:

\[\exp_{\mu}(u) =\cosh(\left\|u\right\|_{L})\mu+\sinh(\left\|u\right\|_{L})\frac {u}{\left\|u\right\|_{L}},\] (19) \[\log_{\mu}(\nu) =\frac{\cosh^{-1}(\alpha)}{\sqrt{\alpha^{2}-1}}\left(\nu-\alpha \mu\right),\] (20) \[\mathrm{PT}_{\nu\rightarrow\mu}(v) =v+\frac{\left\langle\mu-\alpha\nu,v\right\rangle_{L}}{\alpha+1} (\nu+\mu),\] (21)

where we denote \(\alpha=-\left\langle\nu,\mu\right\rangle_{L}\), and \(\cosh^{-1}\) represents the inverse function of \(\cosh\).

Given location and scale parameters denoted as \(\mu\in\mathbb{H}^{d}\) and \(\Sigma\in\mathbb{R}^{d\times d}\), respectively, the procedure for sampling from a wrapped normal distribution \(z\sim\mathcal{WN}(\mu,\Sigma)\) defined over \(\mathbb{H}^{d}\) is given as follows:

\[z=\exp_{\mu}\circ\mathrm{PT}_{\mu^{o}\rightarrow\mu}(u),\quad u_{1:d}\sim \mathcal{N}(0,\Sigma),\] (22)

where \(\mu^{o}=(1,0,\ldots,0)^{\top}\) denotes the origin of the \(\mathbb{H}^{d}\). Note that we set \(u_{0}=0\), and \(u:=u_{0:d}\in T_{\mu^{o}}\mathbb{H}^{d}\) represents a tangent vector at the origin \(T_{\mu^{o}}\mathbb{H}^{d}\).

From the sampling definition in equation (22), the probability density function \(\mathcal{WN}(z;\mu,\Sigma)\) can be derived as follows:

\[\log\mathcal{WN}(z;\mu,\Sigma)=\log\mathcal{N}\left(u;\,0,\Sigma\right)-(d-1 )\ln\left(\frac{\sinh\left\|u\right\|_{L}}{\left\|u\right\|_{L}}\right),\] (23)

where \(u\) is defined as \(u=\mathrm{PT}_{\mu\rightarrow\mu^{o}}\circ\log_{\mu}(z)\). For detailed derivation, we refer to Appendix A of [25].

### GNN-based parameterization for variational branch length distributions

In this work, we employ a variational branch length distribution \(Q_{\phi}(B_{\tau}|\tau)\) parameterized with a graph neural network (GNN) as described in [40]. In concrete, each of the branch lengths follows an independent lognormal distribution, where its location and scale parameters are predicted with a GNN that takes the tree topology \(\tau\) and the learnable topological features (LTFs) of the topology \(\tau\), which are computed with a method described in [40]. Below, we summarize an architecture that we use in this study.

Branch length parameterizationsLet \(V_{\tau}\) and \(E_{\tau}\) respectively represent the sets of nodes and branch edges for a given unrooted binary tree topology \(\tau\). The input to the GNN consists of node features represented by LTFs denoted as \(\{h_{v}^{(0)}\}_{v\in V_{\tau}}\). These features undergo transformation \(L\) times as follows:

\[\{h_{v}^{(L)}\}_{v\in V_{\tau}}=\mathrm{GNN}(\{h_{v}^{(0)}\}_{v\in V_{\tau}})=g ^{(L)}\circ\cdots\circ g^{(1)}(\{h^{(0)}\}_{v\in V_{\tau}}),\] (24)

where we set \(L=2\). The function \(g^{(\ell)}\) represents a GNN layer. For this function, we utilize edge convolutional layers, which will be described in more detail in the following paragraph.

Next, the last node features \(h^{(L)}\) are transformed to output parameters of edge length as follows:

\[\widetilde{h}_{v} =\mathrm{MLP}_{V}(h_{v}^{(L)}), (\forall v\in V_{\tau})\] (25) \[\widetilde{m}_{(v,u)} =\mathrm{MAX}(\widetilde{h}_{v},\widetilde{h}_{u}), (\forall(v,u)\in E_{\tau})\] (26) \[\mu_{(v,u)},\log\sigma_{(v,u)} =\mathrm{MLP}_{E}(\widetilde{m}_{(v,u)}) (\forall(v,u)\in E_{\tau})\] (27)

where \(\mathrm{MLP}_{N}\), \(\mathrm{MAX}\), and \(\mathrm{MLP}_{E}\) denotes a multi-layer perceptron for node features with two hidden layers, the element-wise max operation, and a multi-layer perceptron with a hidden layer that outputs the location and scale parameter \((\mu_{e},\sigma_{e})\) of the lognormal distributions for each edge \(e\in\mathbb{E}_{\tau}\). For each of the hidden layers employed in \(\mathrm{MLP}_{N}\) and \(\mathrm{MLP}_{E}\), we set its width to \(100\) and apply the ELU activation function after the linear transformation of input values.

Edge convolutional layersIn a previous study [40], a GNN with edge convolutional layers, referred to as EDGE, demonstrated strong performance when predicting the posterior tree distributions. In EDGE, the function \(g^{(\ell)}\) transforms node features \(\{h_{v}^{(\ell)}\}_{v\in V_{\tau}}\) according to the following scheme:

\[\{h_{v}^{(\ell+1)}\}_{v\in V_{\tau}}=g^{(\ell)}(\{h_{v}^{(\ell)}\}_{v\in V_{ \tau}}),\] (28)

where \(g^{(\ell)}\) is comprised of the edge convolutional operation with the exponential linear unit (ELU) activation function. Specifically, the transformation with the layer \(g^{(\ell)}\) is computed as follows:

\[e_{u\to v}^{(\ell)} =\mathrm{MLP}^{(\ell)}\left(h_{v}^{(\ell)}\|h_{u}^{(\ell)}-h_{v}^ {(\ell)}\right),\quad\forall u\in N_{\tau}(v)\] (29) \[h_{v}^{\prime\ell+1} =\mathrm{AGG}_{u\in N_{\tau}(v)}^{(\ell)}e_{u\to v}^{(\ell)},\] (30) \[h_{v}^{(\ell+1)} =\mathrm{ELU}\left(h_{v}^{\prime(\ell+1)}\right),\] (31)

where \(N_{\tau}(v)\) represents a set of neighboring nodes connected to node \(v\) in the tree topology \(\tau\), \(\|\) refers to the concatenation operation of elements, \(\mathrm{MLP}^{(\ell)}\) denotes a full connection layer and the exponential linear unit (ELU) activation unit, and \(\mathrm{AGG}^{(\ell)}\) represents an aggregation operation that takes the maximum value of neighboring edge features \(e_{u\to v}^{(\ell)},\forall u\in N(v)\) for each element.

## Appendix B Variational Lower Bounds and Gradient Estimators

### Variational lower bound

In Proposition 1, we present that the following functional is a lower bound of the marginal log-likelihood \(\ln P(Y)\).

\[\mathcal{L}[Q,R] :=\mathbb{E}_{Q(z,B_{\tau})}\left[\ln F^{\prime}(z,B_{\tau}) \right]=\mathbb{E}_{Q(z)}\left[\mathbb{E}_{Q(B_{\tau}|z)}[\ln F(z,B_{\tau})]- \ln Q(z)\right]\] (32) \[\leq\ln P(Y),\] (33)

where \(F\) and \(F^{\prime}\) are respectively defined as follows:

\[F(z,B_{\tau}):=\frac{P(Y,B_{\tau}|\tau(z))}{Q(B_{\tau}|\tau(z))}P(\tau(z))R(z| \tau(z)),\quad F^{\prime}(z,B_{\tau}):=\frac{F(z,B_{\tau})}{Q(z)}.\] (34)

### Gradient estimators for variational lower bound

The gradient of \(\mathcal{L}[Q_{\theta,\phi},R_{\psi}]\) with respect to \(\theta\) is given by

\[\nabla_{\theta}\mathcal{L} =\nabla_{\theta}\operatorname{\mathbb{E}}_{Q_{\theta}(z)}\left[ \operatorname{\mathbb{E}}_{Q_{\phi}(B_{\tau}|z)}[\ln F(z,B_{\tau})]-\ln Q_{ \theta}(z)\right]\] (35) \[=\operatorname{\mathbb{E}}_{Q_{\theta}(z)}\left[\left(\nabla_{ \theta}\ln Q_{\theta}(z)\right)\left(\operatorname{\mathbb{E}}_{Q_{\phi}(B_{ \tau}|\tau(z))}\left[\ln\frac{P(Y,B_{\tau}|\tau(z))}{Q_{\phi}(B_{\tau}|\tau(z)) }\right]+\ln P(\tau(z))R_{\psi}(z|\tau(z))\right)\right]\] \[\quad+\nabla_{\theta}\mathbb{H}[Q_{\theta}(z)],\] (36)

where \(\mathbb{H}\) denotes the differential entropy. We assume that \(Q_{\phi}(B_{\tau}|\tau)\) is reparameterizable as in [40]: namely, \(B_{\tau}\) can be sampled through \(B_{\tau}=h_{\phi}(\epsilon_{B},\tau)\), where \(\epsilon_{B}\sim p_{B}(\epsilon_{B})\), where \(p_{B}(\epsilon)\) and \(h_{\phi}\) denote a parameter-free base distribution and a differentiable function with \(\phi\), respectively. Consequently, the gradient of \(\mathcal{L}\) with respect to \(\phi\) is evaluated as follows:

\[\nabla_{\phi}\mathcal{L} =\nabla_{\phi}\operatorname{\mathbb{E}}_{Q_{\theta}(z)} \operatorname{\mathbb{E}}_{Q_{\phi}(B_{\tau}|z)}[\ln F(z,B_{\tau})]\] (37) \[=\operatorname{\mathbb{E}}_{Q_{\theta}(z)}\operatorname{\mathbb{E }}_{p_{B}(\epsilon_{B})}\left[\nabla_{\phi}\ln\frac{P(Y,B_{\tau}=h_{\phi}( \epsilon_{B},\tau)|\tau(z))}{Q_{\phi}(B_{\tau}=h_{\phi}(\epsilon_{B},\tau)| \tau(z))}\right].\] (38)

Lastly, the gradient of \(\mathcal{L}\) with respect to \(\psi\) can be evaluated with a tractable density model \(R_{\psi}(z|\tau)\) as follows:

\[\nabla_{\psi}\mathcal{L}=\nabla_{\psi}\operatorname{\mathbb{E}}_{Q_{\theta}(z )}\operatorname{\mathbb{E}}_{Q_{\phi}(B_{\tau}|z)}[\ln F(z,B_{\tau})]= \operatorname{\mathbb{E}}_{Q_{\theta}(z)}\left[\nabla_{\psi}\ln R_{\psi}(z| \tau(z))\right].\] (39)

Given samples \(\epsilon_{z}\sim p_{z}\) and \(\epsilon_{B}\sim p_{B}\), we can compute \(z=h_{\theta}(\epsilon_{z})\), \(\tau(z)\), and \(B_{\tau}=h_{\phi}(\epsilon_{B},\tau(z))\). Then, the below equations are estimators of gradients \(\nabla_{\theta}\mathcal{L}\), \(\nabla_{\phi}\mathcal{L}\), and \(\nabla_{\psi}\mathcal{L}\), respectively:

\[\widehat{g}_{\theta} =\nabla_{\theta}\ln Q_{\theta}(z)\cdot\ln F(z,B_{\tau})-\nabla_{ \theta}\ln Q_{\theta}(h_{\theta}(\epsilon_{z})),\] (40) \[\widehat{g}_{\phi} =\nabla_{\phi}\ln F(z,h_{\phi}(\epsilon_{B},\tau(z)))=\nabla_{ \phi}\ln\frac{P(Y,h_{\phi}(\epsilon_{B},\tau(z))|\tau(z))}{Q_{\phi}(h_{\phi}( \epsilon_{B},\tau(z))|\tau(z))},\] (41) \[\widehat{g}_{\psi} =\nabla_{\psi}\ln F(z,h_{\phi}(\epsilon_{B},\tau(z)))=\nabla_{ \psi}\ln R_{\psi}(z|\tau(z)).\] (42)

The gradients can be computed through the auto-gradient of the following target:

\[\widehat{\mathcal{L}}^{\prime}=\ln Q_{\theta}(z)\cdot\operatorname{detach}[f(z,B _{\tau})]+f(z,h_{\phi}(\epsilon_{B},\tau(z)))-\ln Q_{\theta}(h_{\theta}( \epsilon_{z})),\] (43)

where we denote \(f(z,B_{\tau})=\ln F(z,B_{\tau})\), and \(\operatorname{detach}[\cdot]\) refers to an operation that blocks backpropagation through its argument. For clarity in terms of differentiability with respect to the parameters, we distinguish between expressions (\(z\), \(B_{\tau}\)) and (\(h_{\theta}(\epsilon_{z})\), \(h_{\phi}(\epsilon_{B},\tau(z))\)).

### Multi-sample gradient estimators

Given a \(K\) set of Monte Carlo (MC) samples from \(Q_{\theta,\phi}(z,B_{\tau})\), i.e. \(\{\epsilon_{Z}^{(k)},z^{(k)}=h_{\theta}(\epsilon_{Z}^{(k)})\}_{k=1}^{K}\) and \(\{\epsilon_{B}^{(k)},B_{\tau}^{(k)}=h_{\phi}(\epsilon_{B}^{(k)},\tau(z^{(k)})) \}_{k=1}^{K}\), we can simply estimate \(\nabla L_{\theta}[Q_{\theta,\phi},R_{\psi}]\) as follows:

\[\widehat{g}_{\theta}^{(K)}=\frac{1}{K}\sum_{k=1}^{K}\left(\nabla_{\theta}\ln Q_ {\theta}(z^{(k)})\cdot f(z^{(k)},B_{\tau}^{(k)})-\nabla_{\theta}\ln Q_{\theta} (h_{\theta}(\epsilon_{z}^{(k)}))\right).\] (44)

As a simple extension of equation (43), the gradients are obtained through an auto-gradient computation of the following target:

\[\widehat{\mathcal{L}}^{{}^{\prime}(K)}=\frac{1}{K}\sum_{k=1}^{K}\left(\ln Q_{ \theta}(z^{(k)})\cdot\operatorname{detach}[f(z^{(k)},B_{\tau}^{(k)})]+f(z^{(k) },h_{\phi}(\epsilon_{B}^{(k)},\tau(z^{(k)})))-\ln Q_{\theta}(h_{\theta}( \epsilon_{z}^{(k)}))\right),\] (45)

### Leave-one-out (LOO) control variates for variance reduction

For the term of \(K\)-sample gradient estimator \(\widehat{g}_{\theta}^{(K)}\) proportional to the score function \(\nabla_{\theta}\ln Q_{\theta}\), a leave-one-out (LOO) variance reduction is known to be effective [15, 28], which is denoted as follows:

\[\widehat{g}_{\mathrm{LOO},\theta}^{(K)}=\frac{1}{K}\sum_{k=1}^{K}\left[\nabla_{ \theta}\ln Q_{\theta}(z^{(k)})\cdot\left(f(z^{(k)},B_{\tau}^{(k)})-\overline{f_ {k}}(z^{(\setminus k)},B_{\tau}^{(\setminus k)})\right)-\nabla_{\theta}\ln Q_{ \theta}(h_{\theta}(\epsilon_{z}^{(k)}))\right],\] (46)where \(\overline{f_{k}}\) denotes:

\[\overline{f_{k}}(z^{(\setminus k)},B_{\tau}^{(\setminus k)}):=\frac{1}{K-1}\sum_ {k^{\prime}=1,k^{\prime}\neq k}^{K}f(z^{(k^{\prime})},B_{\tau}^{(k^{\prime})}).\] (47)

To employ the LOO gradient estimator for \(\theta\), the target of auto-gradient computation in equation (45) needs to be adjusted as follows:

\[\widehat{\mathcal{L}}^{(K)}_{\,\,\,\mathrm{LOO}}=\frac{1}{K}\sum _{k=1}^{K}\left(\ln Q_{\theta}(z^{(k)})\cdot\mathrm{detach}[f(z^{(k)},B_{\tau} ^{(k)})-\overline{f_{k}}(z^{(\setminus k)},B_{\tau}^{(\setminus k)})]\right.\] \[\left.+f(z^{(k)},h_{\phi}(\epsilon_{B}^{(k)},\tau(z^{(k)})))-\ln Q _{\theta}(h_{\theta}(\epsilon_{z}^{(k)}))\right),\] (48)

### Lax estimators for adaptive variance reduction

```
1:\(\theta,\phi,\psi,\chi\leftarrow\) Initialize variational parameters
2:while not converged do
3:\(\epsilon_{z}^{(1:K)},\epsilon_{B}^{(1:K)}\leftarrow\) Random samples from distributions \(p_{z}\), \(p_{B}\)
4:\(z^{(1:K)}\gets h_{\theta}(\epsilon_{z}^{(1:K)})\)
5:\(B_{\tau}^{(1:K)}\gets h_{\phi}(\epsilon_{B}^{(1:K)},\tau(z^{(1:K)}))\)
6:\(\widehat{g}_{\theta},\widehat{g}_{\phi},\widehat{g}_{\psi}\leftarrow\) Estimate the gradients \(\nabla_{\theta}\mathcal{L}\), \(\nabla_{\phi}\mathcal{L}\), \(\nabla_{\psi}\mathcal{L}\)
7:\(\widehat{g}_{\chi}\leftarrow\nabla_{\chi}\left\langle\widehat{g}_{\theta}^{2}\right\rangle\)
8:\(\theta,\phi,\psi,\chi\leftarrow\) Update parameters using an SGA algorithm, given gradients \(\widehat{g}_{\theta},\widehat{g}_{\phi},\widehat{g}_{\psi},\widehat{g}_{\chi}\)
9:endwhile
10:return\(\theta,\phi,\psi\) ```

**Algorithm 2** GeoPhy algorithm with LAX

The LAX estimator [7] is a stochastic gradient estimator based on a surrogate function, which can be adaptively learned to reduce the variance regarding the term \(\nabla_{\theta}\ln Q_{\theta}(z)\). In our case, the LAX estimator is given as follows:

\[\widehat{g}_{\mathrm{LAX},\theta}:=\nabla_{\theta}\ln Q_{\theta}(z)\cdot(f(z, B_{\tau})-s_{\chi}(z))+\nabla_{\theta}s_{\chi}(h_{\theta}(\epsilon_{z})).\] (49)

We summarize the modified procedure of GeoPhy with LAX in Algorithm 2. As we assume \(Q_{\theta}(z)\) is differentiable with respect to \(z\), we can also use a modified estimator as follows:

\[\widehat{g}_{\mathrm{LAX},\theta}:=\nabla_{\theta}\ln Q_{\theta}(z)\cdot(f(z, B_{\tau})-s_{\chi}(z))+\nabla_{\theta}s_{\chi}(h_{\theta}(\epsilon_{z}))- \nabla_{\theta}\ln Q_{\theta}(h_{\theta}(\epsilon_{z})).\] (50)

Since it is favorable to reduce the variance of \(\widehat{g}_{\mathrm{LAX},\theta}\), we optimize \(\chi\) to minimize the following objective as proposed in [7]:

\[\left\langle\mathbb{V}_{Q_{\theta}(z)}[\widehat{g}_{\theta}]\right\rangle:= \frac{1}{n_{\theta}}\sum_{i=1}^{n_{\theta}}\mathbb{V}_{Q_{\theta}(z)}[ \widehat{g}_{\theta_{i}}]=\frac{1}{n_{\theta}}\sum_{i=1}^{n_{\theta}}\left( \mathbb{E}_{Q_{\theta}(z)}[\widehat{g}_{\theta_{i}}^{2}]-\mathbb{E}_{Q_{\theta }(z)}[\widehat{g}_{\theta_{i}}]^{2}\right),\] (51)

where \(n_{\theta}\) denotes the dimension of \(\theta\). As the gradient in equation (49) is given as an unbiased estimator of \(\nabla_{\theta}\mathcal{L}\), which is not dependent on \(\chi\), we can use the relation \(\nabla_{\chi}\,\mathbb{E}_{Q_{\theta}(z)}[\widehat{g}_{\mathrm{LAX},\theta_{i }}]=0\). Therefore, the unbiased estimator of the gradient \(\nabla_{\chi}\left\langle\mathbb{V}_{Q_{\theta}(z)}[\widehat{g}_{\mathrm{LAX}, \theta}]\right\rangle\) is given as follows:

\[\widehat{g}_{\chi}=\frac{1}{n_{\theta}}\sum_{i=1}^{n_{\theta}}\nabla_{\chi} \widehat{g}_{\mathrm{LAX},\theta_{i}}^{2}.\] (52)

As we require the gradient of \(\nabla_{\theta}\mathcal{L}\) with respect to \(\chi\) for the optimization, we use different objectives for auto-gradient computation with respect to \(\theta\) and the other parameters \(\phi\) and \(\psi\) as follows:

\[\widehat{\mathcal{L}}^{\prime}{}_{\mathrm{LAX},\theta} =\ln Q_{\theta}(z)\cdot(\mathrm{detach}[f(z,B_{\tau})]-s_{\chi}(z))+s _{\chi}(h_{\theta}(\epsilon_{z}))-\ln Q_{\theta}(h_{\theta}(\epsilon_{z})),\] (53) \[\widehat{\mathcal{L}}^{\prime}{}_{\phi,\psi} =f(z,h_{\phi}(\epsilon_{B},\tau(z))).\] (54)

### Lax estimators with multiple MC-samples

For the cases with \(K\) MC-samples, we use LAX estimators by differentiating the following objectives:

\[\widehat{\mathcal{L}}^{\prime}_{\mathrm{LAX},\theta}=\frac{1}{K}\sum_{k=1}^{K} \left(\ln Q_{\theta}(z^{(k)})\cdot\left(\mathrm{detach}[f(z^{(k)},B_{\tau}^{(k)}) ]-s_{\chi}(z^{(k)})\right)+s_{\chi}(h_{\theta}(\epsilon_{z}^{(k)}))-\ln Q_{ \theta}(h_{\theta}(\epsilon_{z}^{(k)}))\right),\] (55)

\[\widehat{\mathcal{L}}^{\prime}_{\phi,\psi}=\frac{1}{K}\sum_{k=1}^{K}f(z^{(k)}, h_{\phi}(\epsilon_{B}^{(k)},\tau(z^{(k)}))).\] (56)

When we combine LAX estimators with LOO control variates. the target for auto-gradient computation changes to the following:

\[\widehat{\mathcal{L}}^{\prime}_{\mathrm{LOO+LAX},\theta}=\frac{ 1}{K}\sum_{k=1}^{K}\left(\ln Q_{\theta}(z^{(k)})\cdot\left(\mathrm{detach}[f(z^{(k)},B_{ \tau}^{(k)})-\overline{f_{k}}(z^{(\setminus k)},B_{\tau}^{(\setminus k)})]-s_ {\chi}(z^{(k)})\right)\right.\] \[\left.+s_{\chi}(h_{\theta}(\epsilon_{z}^{(k)}))-\ln Q_{\theta}(h_ {\theta}(\epsilon_{z}^{(k)}))\right).\] (57)

We note that \(\widehat{\mathcal{L}}^{\prime}_{\phi,\psi}\) is not affected by the introduction of LOO control variates.

### Derivation of importance-weighted evidence lower bound (IW-ELBO)

An importance-weighted evidence lower bound (IW-ELBO) [1], is a tighter lower bound of the log-likelihood \(\ln P(Y)\) than ELBO. For our model, a conventional \(K\)-sample IW-ELBO is given as follows:

\[\mathcal{L}_{\mathrm{IW}}^{(K)}[Q]:=\mathbb{E}_{Q(z^{(1)},B_{\tau}^{(1)}) \cdots Q(z^{(K)},B_{\tau}^{(K)})}\left[\ln\frac{1}{K}\sum_{k=1}^{K}\frac{P(Y,B _{\tau}^{(k)},\tau(z^{(k)}))}{Q(B_{\tau}^{(k)},\tau(z^{(k)}))}\right].\] (58)

The fact that \(\mathcal{L}_{\mathrm{IW}}^{(K)}[Q]\) is the lower bound of \(\ln P(Y)\) is directly followed from Theorem 1 in [1]. However, as our model cannot directly evaluate the mass function \(Q(\tau)\), we must resort to considering the second lower bound, similar to the case of \(K=1\) as depicted in Proposition 1. We define the \(K\)-sample tractable IW-ELBO as follows:

\[\mathcal{L}_{\mathrm{IW}}^{(K)}[Q,R]:=\mathbb{E}_{Q(z^{(1)},B_{\tau}^{(1)}) \cdots Q(z^{(K)},B_{\tau}^{(K)})}\left[\ln\frac{1}{K}\sum_{k=1}^{K}F^{\prime} (z^{(k)},B_{\tau}^{(k)})\right],\] (59)

where \(F^{\prime}\) is defined in equation (34). We will prove in Theorem 1 that \(\mathcal{L}_{\mathrm{IW}}^{(K)}[Q,R]\) serves as a lower bound of the \(\ln P(Y)\). Although this inequality holds when \(K=1\), as shown by \(\ln P(Y)\geq\mathcal{L}[Q]\geq\mathcal{L}[Q,R]\) in Proposition 1, the relationship is less obvious when \(K>1\). Before delving into that, we prepare the following proposition.

**Proposition 2**.: Given \(Q(z,\tau)\) as defined in equation 4 and an arbitrary conditional distribution \(R(z|\tau)\), it follows that

\[\mathbb{E}_{R(z|\tau)}[\mathbb{I}[\tau=\tau(z)]]\leq 1,\] (60)

where setting \(R(z|\tau)=Q(z|\tau)\) is a sufficient condition for the equality to hold.

Proof.: The inequality immediately follows from the definition as follows:

\[\mathbb{E}_{R(z|\tau)}[\mathbb{I}[\tau=\tau(z)]]\leq\mathbb{E}_{R(z|\tau)}[1]=1.\] (61)

Next, when we set \(R(z|\tau)=Q(z|\tau)\), the condition for equality is satisfied as follows:

\[\mathbb{E}_{Q(z|\tau)}[\mathbb{I}[\tau=\tau(z)]]=\frac{\mathbb{E}_{Q(z)}[ \mathbb{I}[\tau=\tau(z)]^{2}]}{Q(\tau)}=\frac{Q(\tau)}{Q(\tau)}=1,\] (62)

where we have used the definition of \(Q(\tau):=\mathbb{E}_{Q(z)}[\mathbb{I}[\tau=\tau(z)]]\) from equation (4) and the resulting relation \(Q(z|\tau)Q(\tau)=\mathbb{I}[\tau=\tau(z)]Q(z)\)

**Theorem 1**.: Given \(Q(z,\tau)\) as defined in equation 4 and an arbitrary conditional distribution \(R(z|\tau)\) that satisfies \(\mathrm{supp}R(z|\tau)\supseteq\mathrm{supp}Q(z|\tau)\), for any natural number \(K>1\), the following relation holds:

\[\ln P(Y)\geq\mathcal{L}_{\mathrm{IW}}^{(K)}[Q,R]\geq\mathcal{L}_{\mathrm{IW}}^{ (K-1)}[Q,R].\] (63)

Additionally, if \(F^{\prime}(z,B_{\tau})\) is bounded and \(\forall\tau,\mathbb{E}_{R(z|\tau)}[\mathbb{I}[\tau=\tau(z)]]=1\), then \(\mathcal{L}_{\mathrm{IW}}^{(K)}[Q,R]\) approaches \(\ln P(Y)\) as \(K\to\infty\).

Proof.: We first show that for any natural number \(K>M\),

\[\mathcal{L}_{\mathrm{IW}}^{(K)}[Q,R]\geq\mathcal{L}_{\mathrm{IW}}^{(M)}[Q,R].\] (64)

For simplicity, we denote \(Q(z^{(k)},B_{\tau}^{(k)})\) and \(F^{\prime}(z^{(k)},B_{\tau}^{(k)})\) as \(Q_{k}\) and \(F_{k}^{\prime}\), respectively, in the following discussion. Let \(U_{M}^{K}\) represent a uniform distribution over a subset with \(M\) distinct indices chosen from the \(K\) indices \(\{1,\ldots,K\}\). Similar to the approach used in [1], we will utilize the following relationship:

\[\frac{1}{K}\sum_{k=1}^{K}F_{k}^{\prime}=\mathbb{E}_{\{i_{1},\ldots,i_{M}\} \sim U_{M}^{K}}\left[\frac{1}{M}\sum_{m=1}^{M}F_{i_{m}}^{\prime}\right].\] (65)

Now, the inequality (64) is derived as follows:

\[\mathbb{E}_{Q_{1}\cdots Q_{K}}\left[\ln\left(\frac{1}{K}\sum_{k=1 }^{K}F_{k}^{\prime}\right)\right] =\mathbb{E}_{Q_{1}\cdots Q_{K}}\left[\ln\mathbb{E}_{\{i_{1}, \ldots,i_{m}\}\sim U_{M}^{K}}\left[\left(\frac{1}{M}\sum_{m=1}^{M}F_{i_{m}}^{ \prime}\right)\right]\right]\] (66) \[\geq\mathbb{E}_{Q_{1}\cdots Q_{K}}\left[\mathbb{E}_{\{i_{1}, \ldots,i_{m}\}\sim U_{M}^{K}}\left[\ln\left(\frac{1}{M}\sum_{m=1}^{M}F_{i_{m}} ^{\prime}\right)\right]\right]\] (67) \[=\mathbb{E}_{Q_{1}\cdots Q_{M}}\left[\ln\left(\frac{1}{M}\sum_{ m=1}^{M}F_{m}^{\prime}\right)\right],\] (68)

where we have also used Jensen's inequality.

Next, we show that \(\ln P(Y)\geq\mathcal{L}_{\mathrm{IW}}^{(K)}[Q,R]\). We again use Jensen's inequality as follows:

\[\mathcal{L}_{\mathrm{IW}}^{(K)}[Q,R] =\mathbb{E}_{Q_{1}\cdots Q_{K}}\left[\ln\frac{1}{K}\sum_{k=1}^{K }F_{k}^{\prime}\right]\] (69) \[\leq\ln\mathbb{E}_{Q_{1}\cdots Q_{K}}\left[\frac{1}{K}\sum_{k=1} ^{K}F_{k}^{\prime}\right]=\ln\mathbb{E}_{Q(z,B_{\tau})}\left[F^{\prime}(z,B_{ \tau})\right].\] (70)

The last term is further transformed as follows:

\[\ln\mathbb{E}_{Q(z,B_{\tau})}\left[F^{\prime}(z,B_{\tau})\right] =\ln\mathbb{E}_{Q(z,B_{\tau})}\left[\frac{P(Y,B_{\tau}|\tau(z))R (z|\tau(z))}{Q(B_{\tau}|\tau(z))Q(z)}\right]\] (71) \[=\ln\mathbb{E}_{Q(z,B_{\tau})}\left[\frac{P(Y,B_{\tau}|\tau(z))R (z|\tau(z))}{Q(z,B_{\tau})}\right]\] (72) \[=\ln\mathbb{E}_{Q(z)}\left[\frac{P(Y,\tau(z))R(z|\tau(z))}{Q(z)}\right]\] (73) \[=\ln\sum_{\tau^{\prime}\in\mathcal{T}}\mathbb{E}_{Q(z)}\left[ \frac{P(Y,\tau^{\prime})R(z|\tau^{\prime})}{Q(z)}\mathbb{I}[\tau^{\prime}=\tau (z)]\right]\] (74) \[=\ln\sum_{\tau^{\prime}\in\mathcal{T}}P(Y,\tau^{\prime})\, \mathbb{E}_{R(z|\tau^{\prime})}\left[\mathbb{I}[\tau^{\prime}=\tau(z)]\right]\] (75) \[\leq\ln\sum_{\tau^{\prime}\in\mathcal{T}}P(Y,\tau^{\prime})=\ln P (Y),\] (76)where, in the transition from the first to the second row, we employed the following relation:

\[Q(B_{\tau}|\tau(z))=\sum_{\tau\in\mathcal{T}}Q(B_{\tau}|\tau)\mathbb{I}[\tau=\tau (z)]=\sum_{\tau\in\mathcal{T}}Q(B_{\tau}|\tau)Q(\tau|z)=Q(B_{\tau}|z),\] (77)

and we have used Proposition 2 for the last inequality.

Finally, we will show that the following convergence property assuming that \(F(z,B_{\tau})\) is bounded:

\[\mathcal{L}^{(K)}_{\mathrm{IW}}[Q,R]\to\ln\left(\sum_{\tau^{\prime}\in \mathcal{T}}P(Y,\tau^{\prime})\,\mathbb{E}_{R(z|\tau^{\prime})}\,\mathbb{I}[ \tau^{\prime}=\tau(z)]\right)\qquad\qquad(K\to\infty).\] (78)

From the strong law of large numbers, it follows that \(\frac{1}{K}\sum_{k=1}^{K}F^{\prime}_{k}\) converges to the following term almost surely as \(K\to\infty\):

\[\mathbb{E}_{Q(z_{k},B_{k})}\left[F^{\prime}(z_{k},B_{k})\right]=\sum_{\tau^{ \prime}\in\mathcal{T}}P(Y,\tau^{\prime})\,\mathbb{E}_{R(z|\tau^{\prime})}\, \mathbb{I}[\tau^{\prime}=\tau(z)],\] (79)

where we have employed the same transformations as used from equation (71) to (75). Observe that the _r.h.s_ term of the equation (78) equals to \(\ln P(Y)\) when \(\forall\tau^{\prime}\in\mathcal{T},\mathbb{E}_{R(z|\tau^{\prime})}[\mathbb{I} [\tau^{\prime}=\tau(z)]]=1\), which completes the proof. 

Estimation of marginal log-likelihoodFor the estimation of \(\ln P(Y)\), we employ \(\mathcal{L}^{(K)}[Q,R]\) with \(K=1,000\) similar to [40]. From Theorem 1, IW-ELBO \(\mathcal{L}^{(K)}[Q,R]\) is at least a better lower bound of \(\ln P(Y)\) than ELBO \(\mathcal{L}[Q,R]\), and converges to \(\ln P(Y)\) when \(\forall\tau,\mathbb{E}_{R(z|\tau)}[\mathbb{I}[\tau=\tau(z)]]=1\). According to Proposition 2, this equality condition is satisfied when we set \(R(z|\tau)=Q(z|\tau)\), which is approached by maximizing \(\mathcal{L}[Q,R]\) with respect to \(R\) as indicated in Proposition 1.

### Gradient estimators for IW-ELBO

The gradient of IW-ELBO \(\mathcal{L}^{(K)}_{\mathrm{IW}}[Q_{\theta,\phi},R_{\psi}]\) with respect to \(\theta\) is given by

\[\nabla_{\theta}\mathcal{L}^{(K)}_{\mathrm{IW}} =\mathbb{E}_{Q_{\theta,\phi}(z^{(1)},B^{(1)}_{\tau})\cdots Q_{ \theta,\phi}(z^{(K)},B^{(K)}_{\tau})}\left[\sum_{k=1}^{K}w_{k}(z^{(1:K)},B^{(1 :K)})\nabla_{\theta}\ln F^{\prime}(z^{(k)},B^{(k)}_{\tau})\right]\] \[+\mathbb{E}_{Q_{\theta,\phi}(z^{(1)},B^{(1)}_{\tau})\cdots Q_{ \theta,\phi}(z^{(K)},B^{(K)}_{\tau})}\left[\sum_{k=1}^{K}\nabla_{\theta}\ln Q_ {\theta}(z^{(k)})\cdot\ell(z^{(1:K)},B^{(1:K)})\right],\] (80)

where we have defined

\[w_{k}(z^{(1:K)},B^{(1:K)}_{\tau}) :=\frac{F^{\prime}(z^{(k)},B^{(k)}_{\tau})}{\sum_{k^{\prime}=1}^{ K}F^{\prime}(z^{(k^{\prime})},B^{(k^{\prime})}_{\tau})},\] (81) \[\ell(z^{(1:K)},B^{(1:K)}_{\tau}) :=\ln\left(\frac{1}{K}\sum_{k^{\prime}=1}^{K}F^{\prime}(z^{(k^{ \prime})},B^{(k^{\prime})}_{\tau})\right).\] (82)

Similarly, as \(F^{\prime}\) is differentiable with respect to \(B_{\tau}\), and \(B_{\tau}=h_{\phi}(\epsilon_{B},\tau)\) is differentiable with respect to \(\phi\), the gradient of \(\mathcal{L}^{(K)}_{\mathrm{IW}}\) with respect to \(\phi\) can be evaluated as follows:

\[\nabla_{\phi}\mathcal{L}^{(K)}_{\mathrm{IW}} =\mathbb{E}_{Q_{\theta}(z^{(1)})\cdots Q_{\theta}(z^{(K)})}\, \mathbb{E}_{p_{B}(\epsilon^{(1)}_{B})\cdots p_{B}(\epsilon^{(K)}_{B})}\left[ \sum_{k=1}^{K}w_{k}(z^{(1:K)},B^{(1:K)}_{\tau})\nabla_{\phi}\ln F^{\prime}(z^ {(k)},h_{\phi}(\epsilon^{(k)},\tau))\right].\] (83)

Since \(\nabla_{\theta}\ln F^{\prime}(z^{(k)},B^{(k)}_{\tau})=-\nabla_{\theta}\ln Q_{ \theta}(z^{(k)})\) from equation (34), an unbiased estimator of the gradient \(\nabla_{\theta}\mathcal{L}^{(K)}\) is given as follows:

\[\widehat{g}^{(K)}_{\mathrm{IW},\theta}:=\sum_{k=1}^{K}\nabla_{\theta}\ln Q_{ \theta}(z^{(k)})\cdot\left[-w_{k}(z^{(1:K)},B^{(1:K)}_{\tau})+\ell(z^{(1:K)},B ^{(1:K)}_{\tau})\right].\] (84)The remaining gradient estimators are given as follows:

\[\widehat{g}^{(K)}_{\mathrm{IW},\phi} =\sum_{k=1}^{K}w_{k}(z^{(1:K)},B_{\tau}^{(1:K)})\nabla_{\phi}\ln F( z^{(k)},h_{\phi}(\epsilon^{(k)},\tau)),\] (85) \[\widehat{g}^{(K)}_{\mathrm{IW},\phi} =\sum_{k=1}^{K}w_{k}(z^{(1:K)},B_{\tau}^{(1:K)})\nabla_{\psi}\ln F (z^{(k)},h_{\phi}(\epsilon^{(k)},\tau)).\] (86)

In total, the target for computing auto-gradient for these gradients is given as follows:

\[\widehat{\mathcal{L}}^{\prime(K)}_{\mathrm{IW}} =\sum_{k=1}^{K}\ln Q_{\theta}(z^{(k)})\cdot\mathrm{detach}\left[ -w_{k}(z^{(1:K)},B_{\tau}^{(1:K)})+\ell(z^{(1:K)},B_{\tau}^{(1:K)})\right]\] \[+\sum_{k=1}^{K}\mathrm{detach}[w_{k}(z^{(1:K)},B_{\tau}^{(1:K)})] \ln F(z^{(k)},h_{\phi}(\epsilon_{B}^{(k)},\tau(z^{(k)}))).\] (87)

### VIMCO estimators

The VIMCO estimator [22] aims at reducing the variance in each of the terms proportional to \(\ell(z^{(1:K)},B^{(1:K)})\) in equation (84). This is accomprished by forming for a control variate \(\ell_{k}(z^{(\setminus k)},B^{(\setminus B_{\tau})})\) that consists of only variables other than \(z^{(k)}\) and \(B^{(k)}\) for every \(k\) as follows:

\[\widehat{g}^{(K)}_{\theta,\mathrm{VIMCO}} :=\sum_{k=1}^{K}\nabla_{\theta}\ln Q_{\theta}(z^{(k)})\cdot\left[ -w_{k}(z^{(1:K)},B_{\tau}^{(1:K)})+\ell(z^{(1:K)},B_{\tau}^{(1:K)})-\overline{ \ell}_{k}(z^{(\setminus k)},B_{\tau}^{(\setminus k)})\right],\] (88)

where we denote that

\[\overline{\ell}_{k}(z^{(\setminus k)},B^{(\setminus k)}) :=\ln\left(\frac{1}{K}\left(\overline{F^{\prime}}_{k}(z^{(\setminus k )},B_{\tau}^{(\setminus k)})+\sum_{k^{\prime}=1,k^{\prime}\neq k}^{K}F^{ \prime}(z^{(k^{\prime})},B_{\tau}^{(k^{\prime})})\right)\right),\] (89) \[\overline{F^{\prime}}_{k}(z^{(\setminus k)},B_{\tau}^{(\setminus k )}) :=\exp\left(\frac{1}{K-1}\sum_{k^{\prime}=1,k^{\prime}\neq k}^{K}\ln F^{ \prime}(z^{(k^{\prime})},B_{\tau}^{(k^{\prime})})\right).\] (90)

Note that the unbiasedness of \(\widehat{g}_{\theta,\mathrm{VIMCO}}\) can be confirmed through the following observation:

\[\mathbb{E}_{Q_{\theta,\phi}(z^{(k)},B_{\tau}^{(k)})}\left[\nabla _{\theta}\ln Q_{\theta}(z^{(k)})\cdot\overline{\ell}_{k}(z^{(\setminus k)}, B_{\tau}^{(\setminus k)})\right]\] \[=\overline{\ell}_{k}(z^{(\setminus k)},B_{\tau}^{(\setminus k)} )\cdot\mathbb{E}_{Q_{\theta,\phi}(z^{(k)},B_{\tau}^{(k)})}\left[\nabla_{ \theta}\ln Q_{\theta}(z^{(k)})\right]\] \[=\overline{\ell}_{k}(z^{(\setminus k)},B_{\tau}^{(\setminus k)} )\cdot\mathbb{E}_{Q_{\theta,\phi}(z^{(k)})}\left[\nabla_{\theta}\ln Q_{\theta} (z^{(k)})\right]=0.\] (91)

To employ the VIMCO estimator for \(\theta\), the target for auto-gradient computation as stated in equation (87) needs to be adjusted as follows:

\[\widehat{\mathcal{L}}^{\prime(K)}_{\mathrm{VIMCO}} =\sum_{k=1}^{K}\ln Q_{\theta}(z^{(k)})\cdot\mathrm{detach}\left[- w_{k}(z^{(1:K)},B_{\tau}^{(1:K)})+\ell(z^{(1:K)},B_{\tau}^{(1:K)})- \overline{\ell}_{k}(z^{(\setminus k)},B_{\tau}^{(\setminus k)})\right]\] \[+\sum_{k=1}^{K}\mathrm{detach}[w_{k}(z^{(1:K)},B_{\tau}^{(1:K)})] \ln F(z^{(k)},h_{\phi}(\epsilon_{B}^{(k)}),\tau(z^{(k)})).\] (92)

## Appendix C Experimental Details

### Training process

For the training of GeoPhy, we continued the stochastic gradient descent process until a total of 1,000,000 Monte Carlo (MC) tree samples were consumed. Specifically, if \(K\) MC-samples were used per step, we performed up to 1,000,000 / K steps. It is noteworthy that the number of MC samples equaled the number of likelihood evaluations (NLEs), which provided us with a basis for comparing convergence speed between different runs, as shown in Fig. 2. In all of our experiments, we used Adam optimizer with an initial learning rate of \(0.0001\). The learning rate was then multiplied by \(0.75\) after every 200,000 steps. Similar to approaches taken by Zhang and Matsen IV [42], we incorporated an annealing procedure during the initial consumption of 100,000 MC samples. Specifically, we replaced the likelihood function in the lower bound with \(P(Y|B_{\tau},\tau)^{\beta}\) and linearly increased the inverse temperature \(\beta\) from \(0.001\) to \(1\) throughout the iterations. Note that all the estimations of marginal log-likelihood (MLL) were performed with \(\beta\) set to \(1\).

For each estimation of MLL values, GeoPhy and VBPI-GNN used IW-ELBO with 1000 samples, whereas all CSMC-based methods used 2048 particle weights [16].

### Variational branch length distribuions

For the variational branch length distribution \(Q_{\phi}(B_{\tau}|\tau)\), we followed an architecture of [40]; namely, each branch length independently followed a lognormal distribution which was parameterized with a graph neural network (GNN). Details are described in Appendix A.2.

### LAX estimators

As input features of a surrogate function \(s_{\chi}(z)\) used in the LAX estimators, we employed a flattened vector of coordinates \(z\in\mathbb{R}^{N\times d}\) when \(z\) resides in Euclidean space. In cases where the coordinates were \(z\in\mathbb{H}^{d}\), we first transformed \(z\) with a logarithm map \(\log_{\mu^{\alpha}}z\in T_{\mu^{\alpha}}\mathbb{H}^{d}\), then omitted their constant value \(0\)-th elements and subsequently flattened the result. We implemented a simple multi-layer perceptron (MLP) network with a single hidden layer of width \(10Nd\) and a subsequent sigmoid linear unit (SiLU) activation function as the neural network to output \(s_{\chi}(z)\).

### Replication of MLL estimates with MrBayes SS

Given the observed discrepancies in marginal log-likelihood (MLL) estimates obtained with the MrBayes stepping-stone (SS) method between references [42] and [16], we replicated the MrBayes SS runs using MrBayes version 3.2.7a. The script we used is provided below.

BEGIN MRBAYES; set autoclose=yes nowarn=yes Seed=123 Swapseed=123; lset nst=1; preset statefreqpr=fixed(equal); preset brlenspr=Unconstrained:exp(10.0); ss ngem=10000000 nruns=10 nchains=4 printfreq=1000 samplefreq=100 \ savebrlens=yes filename=mrbayes_ss_out; END;

We incorporated the results in the row named Replication in Table 3, where the values aligned more closely with those found in [42]. We deduced that the prior distribution used in [16] might have been set differently as the current default values of \(\mathrm{brlenspr}\) are \(\mathrm{Unconstrained}:\mathrm{GammaDir}(1.0,0.100,1.0,1.0)\)3, which deviates from the model assumption used for the benchmarks. We observed that the line \(\mathrm{brlenspr}\) was not included in the code provided in Appendix F of [16]. Having been able to replicate the results found in [42], we opted to use their values as a reference in Table 1.

Footnote 3: https://github.com/NBISweden/MrBayes/blob/develop/doc/manual/Manual_MrBayes_v3.2.pdf

### Visualization of tree topologies

In Fig. 3, we visualized the sum of the probability densities for tip node distribution \(\sum_{i=1}^{N}Q(z_{i})\) by projecting each hyperbolic coordinate \(z_{i}\in\mathbb{H}^{d}\) onto the Poincare coordinates \(\overline{z}_{ik}=z_{ik}/\left(1+z_{i0}\right)\)\((k=1,\ldots,d)\), then applying a transformation \(\overline{z}_{i}\mapsto\overline{z}_{i}^{\prime}=\tanh\left(a\left\|\overline{ z}_{i}\right\|_{2}\right)\cdot\overline{z}_{i}/\left\|\overline{z}_{i}\right\|_{2}\)with \(a=2.1\) to emphasize the central region. To display the density \(Q\) in the new coordinates, the Jacobian term was also considered to evaluate the density \(Q(\overline{z}_{i}^{\prime})\).

For the comparison of consensus tree topologies, we plotted the edges of the tree by connecting each of their end node coordinate pairs with a geodesic line. The coordinate in \(\mathbb{H}^{d}\) of the \(i\)-th tip node was determined as the location parameter \(\mu_{i}\in\mathbb{H}^{d}\) of the wrapped normal distribution \(Q(z_{i})=\mathcal{WN}(z_{i};\mu_{i},\Sigma_{i})\). Let \(\xi_{u}\in\mathbb{H}^{d}\) denotes the coordinate of an interior node \(u\), we defined \(\xi_{u}\) by using the Lorentzian centroid operation \(\mathcal{C}\)[18] as follows:

\[\xi_{u}:=\mathcal{C}(\{c_{s}\}_{s\in\mathcal{S}_{\tau}(u)},\{\nu_{s}\}_{s\in \mathcal{S}_{\tau}(u)})=\frac{\widetilde{\xi}_{u}}{\sqrt{-\left\langle \widetilde{\xi}_{u},\widetilde{\xi}_{u}\right\rangle_{L}}},\] (93)

where \(\widetilde{\xi}_{u}:=\sum_{s\in\mathcal{S}_{\tau}(u)}\nu_{s}c_{s}\) denote an unnormalized sum of weighted coordinates, \(s\in\mathcal{S}_{\tau}(u)\) denote a subset of tip node indices partitioned by the interior node \(u\) in the tree topology \(\tau\), \(c_{s}:=\mathcal{C}(\{\mu_{i}\}_{i\in s},\{1\}_{i\in s})\) denote the Lorentzian centroid of the tip nodes contained in the subset \(s\), and \(\nu_{s}=N-|s|\) denote the number of the tip nodes in the complement set of \(s\) where \(|s|\) represents the number of tip nodes in the subset \(s\). As an unrooted tree topology \(\tau\) can be identified by the set of tip node partitions introduced by the interior nodes of \(\tau\), the same unrooted tree topologies give the same set of interior coordinates \(\{\xi_{u}\}_{u\in V}\) according to equation (93).

## Appendix D Additional Results

### Marginal log-likelihood estimates throughout iterations for DS1 dataset

In Fig. 2, we highlight several key findings of training processes. Specifically, (1) control variates were crucial for optimizations; (2) The LAX estimator for individual Monte Carlo (MC) samples (\(K=1\)) and leave-one-out (LOO) CVs for multiple MC samples were effective; (3) IW-ELBO and VIMCO were not particularly effective when a full diagonal matrix is used for \(Q(z)\). In Fig. 5 and 6, we show that these findings hold consistently across various control variates and configurations of \(Q(z)\), including, different types of distributions (normal \(\mathcal{N}\) and wrapped normal \(\mathcal{WN}\)), embedding dimensions (\(d=2\), \(3\), and \(4\)), and varying numbers of MC samples (\(K=1\), \(2\), and \(3\)).

### Marginal log-likelihood estimates for eight datasets

We present more comprehensive results in Table 3, extending upon the data from Table 1. This table showcases the marginal log-likelihood (MLL) estimates obtained with various GeoPhy configurations and other conventional methods for the datasets DS1-DS8. Once again, GeoPhy demonstrates its superior performance, consistently outperforming CSMC-based approaches that do not require preselection of tree topologies across the majority of configurations and datasets. This reaffirms the stability and excellence of our approach. Additionally, we found that a \(Q(z)\) configuration using a \(4\)-dimensional wrapped normal distribution with a full covariance matrix was the most effective among the tested configurations.

### Analysis of tree topology distributions

To assess the accuracy and expressivity of GeoPhy's tree topology distribution, we contrasted \(Q(\tau)\) with MCMC tree samples. For our analysis, we considered experimental runs using LAX estimators with either \(K=1\) or \(3\). For a configuration of \(Q(z)\), We employed a diagonal or multivariate wrapped normal distribution with \(2\), \(4\), \(5\), and \(6\) dimensions. We sampled 1000 MC trees from \(Q(\tau)\) for each experiment and then deduced the majority consensus tree. As depicted in Fig. 8 across the eight datasets, the Robinson-Foulds (RF) distances between the majority consensus tree of \(Q(\tau)\) and the one obtained with MrBayes were well aligned with the MLL values.

To highlight the expressivity limitation of \(Q(\tau)\) obtained from a simple independent distribution of \(Q(z)\), we compiled three statistics related to the tree topology samples for DS1, DS4, and DS7 datasets (Table 4), wherein we employed the model with the least RF distance for each dataset. For DS1 and DS4, GeoPhy's tree topologies were less diverse than those of MrBayes and VBPI-GNN and had more concentration on the most frequent topology. For more diffused tree DS7, GeoPhy also showed diverse tree samples with a close diversity index to the other two.

We also summarize the frequency of bipartitions observed for tree topology samples in Fig. 8. The intermediate values between 0 and 1 in this frequency plot reveal topology diversities, where VBPI-GNN aligns more closely with MrBayes compared to GeoPhy. For DS1 and DS4, GeoPhy tends to take values near zero and one in the slope region. For DS7, while GeoPhy traced the curve more accurately, fluctuations around this curve highlight potential room for improvement.

### Runtime comparison

We measured CPU runtimes using a single thread on a 20-core Xeon processor. In Fig. 10, we compared single-core runtime of MrBayes, VBI-GNN, and GeoPhy (ours). As MrBayes is written in C and highly optimized its per-iteration computation time is much faster than current variational methods. In general, the per-step efficiency of GeoPhy is comparable to that of VBPI-GNN. Although the convergence of MLL estimates tends to be slow in terms of the number of consumed samples for larger \(K\) (Fig. 9 Left), the total CPU-time slightly decreases as \(K\) increases (Fig. 10 Left). We also included the estimated runtime across the eight datasets and the different model configurations, where the number of species ranges from \(27\) to \(64\) (Fig. 10 Right).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{\(\mathcal{N}\) (diag)} & \multicolumn{3}{c}{\(\mathcal{N}\) (full)} \\ \cline{2-7}  & \(d=2\) & \(d=3\) & \(d=4\) & \(d=2\) & \(d=3\) & \(d=4\) \\ \hline K=1 (LAX) & \(-7126.79\) & \(-7130.11\) & \(-7123.95\) & \(-7129.70\) & \(\bm{-7119.51}\) & \(-7120.03\) \\  & (10.06) & (10.63) & (12.03) & (6.14) & (10.90) & (11.92) \\ K=2 (LOO) & \(-7122.84\) & \(-7125.72\) & \(-7121.79\) & \(-7121.23\) & \(-7130.43\) & \(\bm{-7114.92}\) \\  & (11.09) & (13.08) & (12.52) & (14.62) & (10.82) & (8.32) \\ K=2 (LOO+LAX) & \(-7129.26\) & \(-7128.18\) & \(-7116.96\) & \(-7121.74\) & \(-7129.45\) & \(\bm{-7115.14}\) \\  & (8.35) & (9.83) & (10.39) & (10.73) & (10.23) & (8.26) \\ K=3 (LOO) & \(-7130.60\) & \(-7124.61\) & \(\bm{-7120.88}\) & \(-7132.35\) & \(-7128.26\) & \(-7124.62\) \\  & (10.66) & (12.21) & (13.15) & (6.89) & (9.80) & (12.33) \\ K=3 (LOO+LAX) & \(-7128.36\) & \(-7125.66\) & \(\bm{-7119.81}\) & \(-7122.76\) & \(-7123.67\) & \(-7123.37\) \\  & (9.77) & (10.95) & (11.71) & (10.81) & (11.23) & (11.28) \\ \hline  & \multicolumn{3}{c}{\(\mathcal{WN}\) (diag)} & \multicolumn{3}{c}{\(\mathcal{WN}\) (full)} \\ \cline{2-7}  & \(d=2\) & \(d=3\) & \(d=4\) & \(d=2\) & \(d=3\) & \(d=4\) \\ \hline K=1 (LAX) & \(-7126.89\) & \(-7133.78\) & \(-7122.10\) & \(-7124.63\) & \(-7119.59\) & \(\bm{-7111.55}\) \\  & (10.06) & (13.57) & (12.29) & (8.09) & (10.84) & (0.07) \\ K=2 (LOO) & \(-7121.13\) & \(-7125.60\) & \(-7121.80\) & \(\bm{-7114.99}\) & \(-7130.40\) & \(-7116.14\) \\  & (13.06) & (13.04) & (12.52) & (4.66) & (10.80) & (10.66) \\ K=2 (LOO+LAX) & \(-7120.90\) & \(-7120.96\) & \(-7126.49\) & \(-7119.81\) & \(-7129.58\) & \(\bm{-7116.21}\) \\  & (10.10) & (13.08) & (12.01) & (9.78) & (10.24) & (10.75) \\ K=3 (LOO) & \(-7130.67\) & \(-7124.54\) & \(-7120.94\) & \(-7125.94\) & \(-7122.85\) & \(\bm{-7119.77}\) \\  & (10.67) & (12.20) & (13.11) & (13.07) & (11.96) & (11.80) \\ K=3 (LOO+LAX) & \(-7128.40\) & \(-7125.69\) & \(-7125.78\) & \(\bm{-7115.19}\) & \(-7120.96\) & \(-7116.09\) \\  & (9.78) & (13.02) & (13.10) & (8.16) & (13.12) & (10.67) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparative results of the mean MLL estimates for DS1 dataset obtained with GeoPhy for different combinations of the distribution \(Q(z)\), per-step Monte Carlo samples (\(K\)), and control variates (CVs). Each of the mean MLL values is obtained from five independent runs with 1000 MC samples at the last steps. Standard deviations are shown in parentheses. The bold and underlined numbers represent the best (highest) values in row-wise and column-wise comparisons, respectively.

Figure 5: Comparison of marginal log-likelihood (MLL) estimates for DS1 dataset using different control variates and configurations of \(Q(z)\). We configured \(Q(z)\) to be an independent normal distribution of dimensions \(d=2\), \(3\), or \(4\) with either a diagonal (diag) or full covariance matrix. The number of MC samples was set to \(K=1\), \(2\), or \(3\). For a detailed explanation of the legend, refer to the caption of Fig. 2.

Figure 6: Comparison of marginal log-likelihood (MLL) estimates for DS1 dataset using different control variates and configurations of \(Q(z)\). We employed wrapped normal distributions for \(Q(z)\) instead of normal distributions shown in Fig. 5. The dimensions \(d=2\), \(3\), or \(4\) and the number of MC samples \(K=1\), \(2\), or \(3\) are the same as in the referenced figure.

[MISSING_PAGE_EMPTY:27]

Figure 8: Comparison of bipartition frequencies from the posterior tree topology distributions of MrBayes, VBPI-GNN, and GeoPhy (ours), for DS1, DS4, and DS7 datasets. The bipartitions were ordered by descending frequency as seen in MrBayes.

Figure 7: Marginal log-likelihood (MLL) estimates and Robinson-Foulds (RF) distance between the consensus trees obtained from topology distribution \(Q(\tau)\) and MrBayes (MCMC) for datasets DS1-DS8. Each dot denotes an individual experimental run. The term dim refers to the dimension of wrapped normal distributions used for \(Q(z)\). To avoid dot overlap in the plot, RF metrics are randomly jittered within \(\pm 0.2\). The dashed line illustrates the MLL value estimated by MrBayes.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Statistics & MrBayes & VBPI-GNN & GeoPhy \\ \hline  & (a) & 0.874 & 0.891 & 0.362 \\ DS1 & (b) & 0.268 & 0.313 & 0.793 \\  & (c) & 42 & 44 & 11 \\ \hline  & (a) & 0.895 & 0.895 & 0.678 \\ DS4 & (b) & 0.277 & 0.285 & 0.554 \\  & (c) & 208 & 90 & 58 \\ \hline  & (a) & 0.988 & 0.991 & 0.996 \\ DS7 & (b) & 0.022 & 0.028 & 0.022 \\  & (c) & 753 & 315 & 553 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparative results of tree topology diversity statistics for DS1, DS4, and DS7 datasets using MrBayes, VBPI-GNN, and GeoPhy. Statistics presented include (a) the Simpsons diversity index, (b) the frequency of the most frequent topology, and (c) the number of topologies accounting for the top 95% cumulative frequency.

Figure 10: **Left**: Single-core runtime comparison of MrBayes, VBPI-GNN, and GeoPhy (ours). MrBayes SS* represents one of ten runs used for MrBayes SS. VBPI-GNN is trained for 400k iterations with \(K=10\) MC samples. For VBPI-GNN* and GeoPhy (ours), we represent runtimes for training with 1M MC samples. For GeoPhy, we employed a 2d-diagonal normal distribution for \(Q(z)\) and LOO+LAX for control variates (CVs). **Right**: CPU-time per 1k MC samples for two types of distribution \(Q(z)\) across eight datasets. We compared LOO or LOO+LAX for CVs.

Figure 9: **Left**: Comparison of convergence curves for different per-step MC samples (\(K=2,3,5,10\)) in dataset DS1. **Right**: Comparison of convergence in dataset DS1 for different link functions, NJ and UPGMA, that map coordinates into tree topologies \(\tau(z)\).