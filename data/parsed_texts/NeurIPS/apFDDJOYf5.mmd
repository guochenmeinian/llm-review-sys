[MISSING_PAGE_FAIL:1]

We present a method for jointly training feed-forward generalizable 3D neural scene representation and camera trajectory estimation, self-supervised only by re-rendering losses on video frames, completely without ground-truth camera poses or depth maps. We propose to leverage single-image neural scene representations and differentiable rendering to lift frame-to-frame optical flow to 3D scene flow. We then estimate \(\mathrm{SE}(3)\) camera poses via a robust, weighted least-squares solver on the scene flow field. Regressed poses are used to re-construct the underlying 3D scene from video frames in a feed-forward pass, where weights are shared with the neural scene representation leveraged in camera pose estimation.

We validate the efficacy of our model for feed-forward novel view synthesis and online camera pose estimation on the real-world RealEstate10K and KITTI datasets, as well as the challenging CO3D dataset. We further demonstrate results on in-the-wild scenes in Ego4D and Walking Tours streamed from YouTube. We demonstrate generalization of camera pose estimation to out-of-distribution scenes and achieve robust performance on trajectories on which a state-of-the-art SLAM approach, ORB-SLAM3 [22], struggles.

To summarize, the contributions of our work include:

* We present a new formulation of camera pose estimation as a weighted least-squares fit of an \(\mathrm{SE}(3)\) pose to a 3D scene flow field obtained via differentiable rendering.
* We combine our camera pose estimator with a multi-frame 3D reconstruction model, unlocking end-to-end, self-supervised training of camera pose estimation and 3D reconstruction.
* We demonstrate that our method performs robustly across diverse real-world video datasets, including indoor, self-driving, and object-centric scenes.

## 2 Related Work

Generalizable Neural Scene Representations.Recent progress in neural fields [23; 24; 25] and differentiable rendering [10; 26; 27; 28; 29; 30; 31] have enabled novel approaches to 3D reconstruction from few or single images [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19], but require camera poses both at training and test time. An exception is recently proposed RUST [32], which can be trained for novel view synthesis without access to camera poses, but does not reconstruct 3D scenes explicitly and does not yield explicit control over camera poses. We propose a method that is similarly trained self-supervised on real video, but yields explicit camera poses and 3D scenes in the form of radiance fields. We outperform RUST on novel view synthesis and demonstrate strong out-of-distribution generalization by virtue of 3D structure.

SLAM and Structure-from-Motion (SfM).SfM methods [33; 34; 35], and in particular, COLMAP [34], are considered the de-facto standard approach to obtaining accurate geometry and camera poses from video. Recent progress on differentiable rendering has enabled joint estimation of radiance fields and camera poses via gradient descent [36; 37; 38; 39; 40], enabling subsequent high-quality novel view synthesis. Both approaches require offline per-scene optimization. In contrast, SLAM methods usually run online [41; 22; 42], but are notoriously unreliable on rotation-heavy trajectories or scenes with sparse visual features. Prior work proposes differentiable SLAM to learn priors over camera poses and geometry [43; 44], but requires ground-truth camera poses for training. Recent work has also explored how differentiable rendering may be directly combined with SLAM [45; 46; 47; 48], usually using a conventional SLAM algorithm as a backbone and focusing on the single-scene overfitting case. We propose a fully self-supervised method to train generalizable neural scene representations without camera poses, outperforming prior work on generalizable novel view synthesis without camera poses. We do _not_ claim state-of-the-art camera pose estimation, but provide an analysis of camera pose quality nevertheless, demonstrating robust performance on sequences that are challenging to state-of-the-art SLAM algorithms, ORB-SLAM3 [22] and Droid-SLAM [43].

Neural Depth and Camera Pose Estimation.Prior work has demonstrated joint self-supervised learning of camera pose and monocular depth [49; 50; 20; 51] or multi-plane images [52]. These approaches leverage a neural network to _directly_ regress camera poses with the primary goal of training high-quality monocular depth predictors. They are empirically limited to sequences with simple camera trajectories, such as self-driving datasets, and do not enable dense, large-baseline novel view synthesis. We ablate our flow-based camera pose estimation with a similar neural network-based approach. Most closely related to our work are approaches that infer per-timestep 3D voxel grids and train a CNN to regress frame-to-frame poses [53; 54]. We benchmark with the most recent approach in this line of work, Video Autoencoder [53]. Lastly, we strongly encourage the reader to peruse impressive concurrent work DBARF [55], which also regresses camera poses alongside a generalizable NeRF. Key differences are that we leverage a pose solver based on 3D-lifted optical flow for real-time odometry versus predicting iterative updates to pose and depth via a neural network. Further, we extensively demonstrate our method's performance on rotation-dominant video sequences, in contrast to a focus on forward-facing scenes. Lastly, we solely rely on the generalizable scene representation in contrast to leveraging a monocular depth model for pose estimation.

## 3 Learning 3D Scene Representations from Unposed Videos

Our model learns to map a monocular video with frames \(\{\mathbf{I}_{t}\}_{t=1}^{N}\) as well as off-the-shelf optical flow \(\{\mathbf{V}_{t}\}_{t=1}^{N-1}\) to per-frame camera poses \(\{\mathbf{P}_{t}\}_{t=1}^{N}\) and a 3D scene representation \(\Phi\) in a single feed-forward pass. We leverage known intrinsic parameters when available, but may predict them if not. We will first introduce the generalizable 3D scene representation \(\Phi\). We then discuss how we leverage \(\Phi\) for feed-forward camera pose estimation, where we lift optical flow into 3D scene flow and solve for pose via a weighted least-squares \(\mathrm{SE}(3)\) solver. Finally, we discuss how we obtain supervision for both the 3D scene representation and pose estimation by re-rendering RGB and optical flow for all frames. An overview of our method is presented in Fig. 1.

**Notation.** It will be convenient to treat images sometimes as discrete tensors, such as \(\mathbf{I}_{t}\in\mathbb{R}^{H\times W\times 3}\), and sometimes as functions \(I:\mathbb{R}^{2}\rightarrow\mathbb{R}^{3}\) over 2D pixel coordinates \(\mathbf{p}\in\mathbb{R}^{2}\). We will denote functions in italic \(I\), while we denote the corresponding tensors sampled on the pixel grid in bold as \(\mathbf{I}\).

### Defining Our Image-Conditioned 3D Scene Representation

First, we introduce the generalizable 3D scene representation we aim to train. Our discussion assumes known camera poses; in the subsequent section we will describe how we can use our scene representation to estimate them instead. We parameterize our 3D scene as a Neural Radiance Field (NeRF) [28], such that \(\Phi\) is a function that maps a 3D coordinate \(\mathbf{x}\) to a color \(\mathbf{c}\) and density \(\sigma\) as \(\Phi(\mathbf{x})=(\sigma,\mathbf{c})\). To render the color for a ray \(\mathbf{r}\), points \((\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{n})\) are sampled along \(\mathbf{r}\) between predefined near and far planes \([t_{1},t_{f}]\), fed into \(\Phi\) to produce corresponding color and density tuples \([(\sigma_{1},\mathbf{c}_{1}),(\sigma_{2},\mathbf{c}_{2}),...,(\sigma_{n}, \mathbf{c}_{n})]\), and alpha-composited to produce a final color value \(C(\mathbf{r})\):

\[C(\mathbf{r})=\sum_{i=1}^{N}T_{i}(1-\text{exp}(-\sigma_{i}\delta_{i}))\mathbf{ c}_{i},\text{ where }T_{i}=\text{exp}(-\sum_{j=1}^{i-1}\sigma_{j}\delta_{j}),\] (1)

Figure 1: **Method Overview. Given a set of video frames, our method first computes frame-to-frame camera poses (left) and then re-renders the input video (right). To estimate pose between two frames, we compute off-the-shelf optical flow to establish 2D correspondences. Using single-view pixelNeRF [1], we obtain a surface point cloud as the expected 3D ray termination point for each pixel, \(\mathbf{X}\), \(\mathbf{Y}\) respectively. Because \(\mathbf{X}\) and \(\mathbf{Y}\) are pixel-aligned, optical flow allows us to compute 3D scene flow as the difference of corresponding 3D points. We then find the camera pose \(\mathbf{P}\in\text{SE}(3)\) that best explains the 3D flow field by solving a weighted least-squares problem with flow confidence weights \(\mathbf{W}\). Using all frame-to-frame poses, we re-render all frames. We enforce an RGB loss and a flow loss between projected pose-induced 3D scene flow and 2D optical flow. Our method is trained end-to-end, assuming only an off-the-shelf optical flow estimator.**

where \(\delta_{i}=t_{i+1}-t_{i}\) is the distance between adjacent samples. By compositing sample locations instead of colors, we can compute an expected ray-surface intersection point \(S(\mathbf{r})\in\mathbb{R}^{3}\):

\[S(\mathbf{r})=\sum_{i=1}^{N}T_{i}(1-\text{exp}(-\sigma_{i}\delta_{i}))\mathbf{x }_{i}.\] (2)

We require a _generalizable_ NeRF that is not optimized for each scene separately, but instead predicted in a feed-forward pass by an encoder that takes a set of \(M\) context images and corresponding camera poses \(\{(\mathbf{I}_{i},\mathbf{P}_{i})\}_{i}^{M}\) as input. We denote such a generalizable radiance field reconstructed from images \(\mathbf{I}_{i}\) as \(\Phi(\mathbf{x}\mid\{(\mathbf{I}_{i},\mathbf{P}_{i})\}_{i}^{M})\). Many such models have been proposed [1-10]. We base our model on pixelNeRF [1], which we briefly discuss in the following - please find further details in the supplement. pixelNeRF first extracts per-image features \(\mathbf{F}_{i}\) from each input image \(\mathbf{I}_{i}\). A given 3D coordinate \(\mathbf{x}\) is first projected onto the image plane of each context image \(\mathbf{I}_{i}\) via the known camera pose and intrinsic parameters to yield pixel coordinates \(\mathbf{p}_{i}\). We then retrieve the features \(F_{i}(\mathbf{p}_{i})\) at that pixel coordinate. Color and density \((\sigma,\mathbf{c})\) at \(\mathbf{x}\) are then predicted by a neural network that takes as input the features \(\{F_{i}(\mathbf{p}_{i})\}_{i}^{M}\) and the coordinates of \(\mathbf{x}\) in the coordinate frame of each camera, \(\{\mathbf{P}_{i}^{-1}\mathbf{x}\}_{i}^{M}\). Importantly, we can condition pixelNeRF on varying numbers of context images, i.e., we may run pixelNeRF with only a _single_ context image as \(\Phi(\mathbf{x}\mid(\mathbf{I},\mathbf{P}))\), or with a set of \(M>1\) context images \(\Phi(\mathbf{x}\mid(\mathbf{I}_{i},\mathbf{P}_{i})\}_{i}^{M})\).

### Lifting Optical Flow to Scene Flow with Neural Scene Representations

Equipped with our generalizable 3D representation \(\Phi\), we now describe how we utilize it to lift optical flow into confidence-weighted 3D scene flow. Later, our pose solver will fit a camera pose to the

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c}  & \multicolumn{2}{c|}{CO3D-Hydrants} & \multicolumn{2}{c|}{CO3D-10} & \multicolumn{2}{c|}{RealEstate} & \multicolumn{2}{c}{KITTI} \\ Model & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) \\ \hline Vid-AE [53] & 0.3427 & 18.56 & 0.3889 & 18.50 & 0.4173 & 18.03 & 0.3272 & 17.65 \\ RUST [32] & 0.6126 & 17.50 & 0.6145 & 17.71 & 0.5692 & 18.07 & 0.5584 & 15.44 \\ Ours & **0.2250** & **23.94** & **0.2687** & **24.17** & **0.2224** & **24.25** & **0.1928** & **23.39** \\ \end{tabular}
\end{table}
Table 1: **Quantitative Comparison on View Synthesis. On the task of view synthesis, our method outperforms other unposed methods by wide margins.**

Figure 2: **Video Reconstruction Results. Our model reconstructs video frames from sparse context frames with higher fidelity than all baselines. While VidAE’s renderings often appear with convincing texture, they are often not aligned with the ground truth. RUST’s renderings are well aligned but are blurry due to their coarse set latent representation.**

estimated scene flow. Given two sequential frames \(\mathbf{I}_{t-1},\mathbf{I}_{t}\) we first use an off-the-shelf method [56] to estimate backwards optical flow \(V_{t}:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}\). The optical flow \(V_{t}\) maps a 2D pixel coordinate \(\mathbf{p}\) to a 2D flow vector, such that we can determine the corresponding pixel coordinate in \(\mathbf{I}_{t-1}\) as \(\mathbf{p}^{\prime}=\mathbf{p}+V_{t}(\mathbf{p})\).

We will now lift pixel coordinates \(\mathbf{p}_{t}\) and \(\mathbf{p}_{t-1}\) to an estimate of the 3D points that they observe in the coordinate frame of their respective cameras. To achieve this, we cast rays from the camera centers through the corresponding pixel coordinates \(\mathbf{p}_{t}\) and \(\mathbf{p}_{t-1}\) using the intrinsics matrix \(\mathbf{K}\). Specifically, we compute \(\mathbf{r}_{t}=\mathbf{K}^{-1}\tilde{\mathbf{p}}_{t}\) and \(\mathbf{r}_{t-1}=\mathbf{K}^{-1}\tilde{\mathbf{p}}_{t-1}\), where \(\tilde{\mathbf{p}}\) represents the homogeneous coordinate \(\left(\begin{smallmatrix}\mathbf{p}\\ 1\end{smallmatrix}\right)\). Next, we sample points along the rays \(\mathbf{r}_{t}\) and \(\mathbf{r}_{t-1}\) and query our pixelNeRF model in the single-view setting. This involves invoking \(\Phi(\cdot|(\mathbf{I}_{t},\mathbb{I}_{4\times 4}))\) and \(\Phi(\cdot|(\mathbf{I}_{t-1},\mathbb{I}_{4\times 4}))\), i.e., pixelNeRF is run with only the respective frame as the context view and the identity matrix \(\mathbb{I}\) as the camera pose. Applying the ray-intersection integral defined in Eq. 2 to the pixelNeRF estimates, we obtain a pair of 3D points \((\mathbf{x}_{t},\mathbf{x}_{t-1})\) corresponding to the estimated surfaces observed by pixels \(\mathbf{p}_{t}\) and \(\mathbf{p}_{t-1}\), respectively. We repeat this estimation for all optical flow correspondences, resulting in two sets of surface point clouds, \(\mathbf{X},\mathbf{X}^{\prime}\in\mathbb{R}^{H\times W\times 3}\). Equivalently, we may view this as defining the 3D scene flow as \(\mathbf{X}^{\prime}-\mathbf{X}\).

**Flow confidence weights.** We further utilize a confidence weight for each flow correspondence. To accomplish this, we employ a neural network \(\Psi\), which takes image features \(F_{t}(\mathbf{p}),F_{t-1}(\mathbf{p}^{\prime})\) as input for every pixel correspondence pair \((\mathbf{p},\mathbf{p}^{\prime})\). The network maps these features to a weight \(\mathbf{w}\), denoted as \(\Psi(F_{t}(\mathbf{p}),F_{t-1}(\mathbf{p}^{\prime}))=\mathbf{w}\in[0,1]\). \(\Psi\) can importantly overcome several failure modes which lead to faulty pose estimation, including incorrect optical flow, such as in areas of occlusions, dynamic objects, such as pedestrians, or challenging geometry estimates, such as sky regions. We show in Fig. 9 that \(\Psi\) indeed learns such content-based rules.

**Predicting Intrinsic Camera Parameters K.** Camera intrinsics are often approximately known, either published by the manufacturer, saved in video metadata, or calibrated once. Nevertheless, for purposes of large-scale training, we leverage a simple scheme to predict the camera field-of-view for a video sequence. We feed the feature map of the first frame \(\mathbf{F}_{0}\) into a convolutional encoder that directly regresses the field of view. We assume that the optical center is at the sensor center. We find that this approach enables us to train on real-world video from YouTube, though more sophisticated schemes [57] will no doubt improve performance further.

\begin{table}

\end{table}
Table 2: **Quantitative Pose Estimation Comparison. In (a) we compare against VideoAutoencoder [53] on short-sequence odometry estimation (\(20\) frames), reporting the ATE. In (b) we compare against ORB-SLAM3 [22] and DROID-SLAM [43] on long sequences (\(\sim\)200 frames) from the CO3D 10-Category dataset. We separately report scores on the top and bottom 50% of sequences (“Top” and “Bot.”) in terms of quality of ground-truth poses as indicated by the dataset. We report ATE and percent of sequences tracked (“Tracked”). ORB-SLAM3 fails to track over half of these challenging sequences.**

Figure 3: **Qualitative Pose Estimation Comparison**. On short sequences, we compare our pose estimation to Video Autoencoder [53], and on long sequences, we compare our method’s sliding-window estimations against the per-scene optimization BARF [38]. The trajectory for the bicycle sequence was obtained using a model trained on hydrant sequences: despite never having seen a bicycle before, our model predicts accurate poses.

### Camera Pose Estimation as Explaining the Scene Flow Field

We will now estimate the camera pose between frame \(\mathbf{I}_{t}\) and \(\mathbf{I}_{t-1}\). In the previous section, we lifted the input optical flow into scene flow, producing 3D correspondences \(\mathbf{X},\mathbf{X}^{\prime}\), or, equivalently, 3D scene flow. We cast camera pose estimation as the problem of finding the rigid-body motion that best explains the observed scene flow field, or the transformation mapping points in \(\mathbf{X}\) to \(\mathbf{X}^{\prime}\), while considering confidence weights \(\mathbf{W}\). Note that below, we will refer to the matrices \(\mathbf{X}\), \(\mathbf{X}^{\prime}\), and \(\mathbf{W}\) as column vectors, with their spatial dimensions flattened.

We use a weighted Procrustes formulation to solve for the rigid transformation that best aligns the set of points \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\). The standard orthogonal Procrustes algorithm solves for the \(\mathrm{SE}(3)\) pose such that it minimizes the least squares error:

\[\operatorname*{arg\,min}_{\mathbf{P}\in\mathrm{SE}(3)}\lVert\tilde{\mathbf{ X}}-\mathbf{P}\tilde{\mathbf{X}}^{\prime}\rVert_{2}^{2},\] (3)

with \(\mathbf{P}=\left(\begin{smallmatrix}\mathbf{R}&\mathbf{t}\\ 0&1\end{smallmatrix}\right)\) as a rigid-body pose with rotation \(\mathbf{R}\) and translation \(\mathbf{t}\) and homogeneous \(\tilde{\mathbf{X}}=\left(\begin{smallmatrix}\mathbf{X}\\ 1\end{smallmatrix}\right)\). In other words, the minimizer of this loss is the rigid-body transformation that best maps \(\mathbf{X}\) onto \(\mathbf{X}^{\prime}\), and, in a static scene, is therefore equivalent to the sought-after camera pose.

As noted by Choy et al. [58], this formulation equally weights all correspondences. As noted in the previous section, however, this would make our pose estimation algorithm susceptible to both incorrect correspondences as well as correspondences that should be down-weighted by nature of belonging to parts of the scene that are specular, dynamic, or have low confidence in their geometry estimate. Following [58], we thus minimize a _weighted_ least-squares problem:

\[\operatorname*{arg\,min}_{\mathbf{P}\in\mathrm{SE}(3)}\lVert\mathbf{W}^{1/2}( \tilde{\mathbf{X}}-\mathbf{P}\tilde{\mathbf{X}}^{\prime})\rVert_{2}^{2}\] (4)

with the diagonal weight matrix \(\mathbf{W}=\text{diag}(\mathbf{w})\). Conveniently, this least-squares problem admits a closed-form solution, efficiently calculated via Singular Value Decomposition, as derived in [58]:

\[\mathbf{R}=\mathbf{U}\mathbf{S}\mathbf{V}^{T}\text{ and } \mathbf{t}=(\mathbf{X}-\mathbf{R}\mathbf{X}^{\prime})\mathbf{W}\mathbf{1}, \text{ where }\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}=\mathbf{S}\mathbf{V} \mathbf{D}(\Sigma_{\mathbf{X}^{\prime}\mathbf{X}}),\] (5) \[\Sigma_{\mathbf{X}^{\prime}\mathbf{X}}=\mathbf{X}\mathbf{K} \mathbf{W}\mathbf{K}\mathbf{X}^{T},\mathbf{K}=\mathbb{I}-\sqrt{\mathbf{w}} \sqrt{\mathbf{w}}^{T},\text{ and }\mathbf{S}=\text{diag}(1,...,\text{det}(\mathbf{U})\text{det}( \mathbf{V})).\] (6)

**Composing frame-to-frame poses.** Solving this weighted least-squares problem for each subsequent frame-to-frame pair yields camera transformations \((\mathbf{P}_{2}^{\prime},\mathbf{P}_{3}^{\prime},\ldots,\mathbf{P}_{n}^{\prime})\), aligning each \(\mathbf{I}_{t}\) to its predecessor \(\mathbf{I}_{t-1}\). We compose frame-to-frame transformations to yield camera poses \((\mathbf{P}_{1},\mathbf{P}_{2},\ldots,\mathbf{P}_{n})\) relative to the first frame, such that \(\mathbf{P}_{1}=\mathbb{I}_{3\times 3}\), concluding our camera pose estimation module.

### Supervision via Differentiable Video and Flow Re-Rendering

We have discussed our generalizable neural scene representation \(\Phi\) and our camera pose estimation module. We will now discuss how we derive supervision to train both modules end-to-end. We have two primary loss signals: First, a photometric loss \(\mathcal{L}_{\mathrm{RGB}}\) scores the visual fidelity of re-rendered video frames. Second, a pose-induced flow loss \(\mathcal{L}_{\mathrm{pose}}\) scores how similar the flow induced by thepredicted camera transformations and surface estimations are to optical flow estimated by RAFT [59]. Our model is trained on short (\(\sim\)15 frames) video sequences.

**Photometric Loss.** Our photometric loss \(\mathcal{L}_{\text{RGB}}\) comprises two terms: A multi-context loss and a single-context loss. The multi-context loss ensures that the full 3D scene is reconstructed accurately. Here, we re-render each frame of the input video sequence \(\mathbf{I}_{t}\), using its estimated camera pose \(\mathbf{P}_{t}\) and multiple context images \(\{(\mathbf{I}_{j},\mathbf{P}_{j})\}_{j}^{J}\). The single-context loss ensures that the single-context pixelNeRF used to estimate surface point clouds \(\mathbf{X}_{t}\) in the pose estimation module is accurate.

\[\mathcal{L}_{\text{RGB}}=\frac{1}{N}\sum_{i=t}^{N}\underbrace{\left\|\mathbf{ I}_{t}-C(\mathbf{P}_{t}\mid\{(\mathbf{I}_{j},\mathbf{P}_{j})\}_{j}^{J})\right\|_{2}^{2}}_{ \text{Multi-Context Loss}}+\underbrace{\left\|\mathbf{I}_{t}-C(\mathbb{I}_{4\times 4 }\mid(\mathbf{I}_{t},\mathbb{I}_{4\times 4}))\right\|_{2}^{2}}_{\text{ Single-Context Loss}},\] (7)

where, in a slight abuse of notation, we have overloaded the rendering function \(C(\mathbf{P}|\{(\mathbf{I}_{j},\mathbf{P}_{j})\}_{j}^{J})\) defined in Eq. 1 as rendering out the full image obtained by rendering a pixelNeRF with context images \(\{(\mathbf{I}_{j},\mathbf{P}_{j})\}_{j}^{J}\) from camera pose \(\mathbf{P}\). \(N\) refers to the number of frames in the video sequence and \(J\) refers to the subset of context frames use for re-rendering the entire sequence. We first attempted picking the first frame only, however, found that this does _not_ converge due to the uncertainty of the 3D scene given only the first frame: single-view pixelNeRF will generate blurry estimates for parts of the scene that have high uncertainty, such as occluded regions or previously unobserved background.

**Pose-Induced Flow Loss.** An additional, powerful source of supervision for both the estimated geometry and camera poses can be obtained by comparing the optical flow induced by the predicted surface point clouds and pose with the off-the-shelf optical flow. We define this pose-induced flow loss as

\[\mathcal{L}_{\text{pose}}=\frac{1}{N-1}\sum_{t=1}^{N-1}\left\|\mathbf{V}_{t}-( \pi(\mathbf{P}_{t}^{-1}\cdot\mathbf{P}_{t+1}\cdot\mathbf{X}_{t+1})-\mathbf{u }\mathbf{v})\right\|_{2}^{2},\] (8)

with projection operator \(\pi(\cdot)\) and grid of pixel coordinates \(\mathbf{u}\mathbf{v}\in\mathbb{R}^{2}\). Intuitively, this transforms the surface point cloud of frame \(t+1\) into the coordinate frame of frame \(t\) and projects it onto that image plane. For every pixel coordinate \(\mathbf{p}\) at timestep \(t+1\), this yields a corresponding pixel coordinate \(\mathbf{p}^{\prime}\) at timestep \(t\), which we compare against the input optical flow.

### Test-time Inference

After training our model on a large dataset of short video sequences, we may infer both camera poses and a radiance field of such a short sequence in a single forward pass, without test-time optimization.

**Sliding Window Inference for Odometry on Longer Trajectories.** Our method estimates poses for short (\(\sim\)15 frames) subsequences in a single feed-forward pass. To handle longer trajectories that exceed the capacity of a single batch, we divide a given video into non-overlapping subsequences. We estimate poses for each subsequence individually and compose them to create an aggregated trajectory estimate. This approach allows us to estimate trajectories for longer video sequences.

**Test-Time Adaptation.** Frame-to-frame camera pose estimation methods, both conventional and the proposed method, accumulate pose estimation error over the course of a sequence. SLAM and SfM methods usually have a mechanism to correct for drift by globally optimizing over all poses and closing loops in the pose graph [60]. We do not have such a mechanism, but propose fine-tuning our model on specific scenes for more accurate feed-forward pose and 3D estimation. For a given video sequence, we may fine-tune our pre-trained model using our standard photometric and flow losses on sub-sequences of the video. Note that this is _not_ equivalent to per-scene optimization or _direct_

Figure 6: **Wide-Baseline View Synthesis. Given an input video without poses, our model first infers camera poses and can then render wide-baseline novel views of the underlying 3D scene, where we use the first, middle, and final frame of the video as context views.**

optimization of camera poses and a radiance field, as performed e.g. in BARF [38]: neither camera poses nor the radiance field are free variables. Instead, we fine-tune the weights of our convolutional inference backbone and MLP renderer for more accurate feed-forward prediction.

## 4 Experiments

We benchmark our method on generalizable novel view synthesis on the RealEstate10k [62], CO3D [63], and KITTI [64] datasets. We provide further analysis on the Tanks & Temples dataset and in-the-wild scenes from Ego4D [65] and YouTube. Though we do not claim to perform state-of-the-art camera pose estimation, we nevertheless provide an analysis of the accuracy of our estimated camera poses. Please find more results, as well as precise hyperparameters, implementation, and dataset details, in the supplemental document and video. We utilize camera intrinsic parameters where available, predicting them only for the in-the-wild Ego4D and WalkingTours experiments.

Pose Estimation.We first evaluate our method on pose estimation against the closest self-supervised neural network baseline, Video Autoencoder (VidAE) [53]. We then analyze the robustness of our pose estimation with ORB-SLAM3 [66] and DROID-SLAM [43] as references. Finally, we benchmark with BARF [38], a single-scene unposed NeRF baseline. Tab. 1(a) compares accuracy of estimated poses of our method and VidAE on all four datasets. The performance gap is most pronounced on the challenging CO3D dataset, but even on simpler, forward-moving datasets, RealEstate10k and KITTI, our method significantly outperforms VidAE. Next, we analyse the robustness of our pose estimation module on CO3D, using SfM methods ORB-SLAM3 and DROID-SLAM as references. See Tab. 1(b) and Fig. 4 for quantitative and qualitative results. To account for inaccuracies in the provided CO3D poses we utilize as ground-truth, we additionally report separate results for the top and bottom 50% of sequences, ranked based on the pose confidence scores provided by the authors. Although we do not employ any secondary pose method as a proxy ground truth for the bottom half of sequences, this division serves as an approximate indication of the level of difficulty each sequence poses from a SfM perspective. On both subsets, our method outperforms both DROID-SLAM and ORB-SLAM3. Also note that ORB-SLAM3 fails to track poses for over half (50.7%) of the sequences. On the sequences where ORB-SLAM3 succeeds, our method predicts poses significantly more accurately. Even on the sequences where ORB-SLAM3 fails, our performance does not degrade (.025 ATE). Lastly, we compare against the single-scene unposed NeRF baseline, BARF. Since BARF requires \(\sim\)one day to reconstruct a single sequence, we evaluate on two representative sequences: a forward-walking sequence on RealEstate10K, and an outside-in trajectory on CO3D. We plot recovered trajectories in Fig. 3. While BARF fails to recover the correct trajectory shape on the CO3D scene, our method produces a trajectory that more accurately reflects the ground-truth looping structure.

Novel View Synthesis.We compare against VidAE [53] and RUST [32] on the task of novel view synthesis. Tab. 1 and Fig. 6 report quantitative and qualitative results respectively. Our method outperforms both baselines significantly. Since VidAE fails to capture the pose distribution on the CO3D datasets, its novel view renderings generally do not align with the ground truth. On RealEstate10K and KITTI, their method successfully captures the pose distribution, but still struggles

Figure 7: **Fine-tuned Pose Estimation and View Synthesis on Large-Scale, Out-of-Distribution Scene. We evaluate our RealEstate10K-trained model on a significantly out-of-distribution scene from the Tanks and Temples dataset [61], first without any scene-adaptation and then with. Even with this significant distribution gap, our method’s estimated trajectory captures the looping structure of the ground truth, albeit with accumulated drift. After a scene-adaptation fine-tuning stage (around 7 hours), our model estimates poses which align closely with the ground truth. We also plot the trajectory estimated by BARF [38], which fails to capture the correct pose distribution.**

[MISSING_PAGE_FAIL:9]

Discussion

Limitations.While we believe our method makes significant strides, it still has several limitations. As an odometry method, it accumulates drift and has no loop closure mechanism. Our model further does not currently incorporate scene dynamics, but recent advancements in dynamic NeRF papers [68, 69, 70] present promising perspectives for future research.

Conclusion.We have introduced FlowCam, a model capable of regressing camera poses and reconstructing a 3D scene in a single forward pass from a short video. Our key contribution is to factorize camera pose estimation as first lifting optical flow to pixel-aligned scene flow via differentiable rendering, and then solving for camera pose via a robust least-squares solver. We demonstrate the efficacy of our approach on a variety of challenging real-world datasets, as well as in-the-wild videos. We believe that they represent a significant step towards enabling scene representation learning on uncurated, real-world video.

Acknowledgements.This work was supported by the National Science Foundation under Grant No. 2211259, by the Singapore DSTA under DST000ECI20300823 (New Representations for Vision), and by the Amazon Science Hub. The Toyota Research Institute also partially supported this work. This article solely reflects the opinions and conclusions of its authors and no other entity.

## References

* [1] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In _Proc. CVPR_, 2021.
* [2] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In _Proc. CVPR_, 2020.
* [3] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline stereo pairs. In _Proc. CVPR_, 2023.
* [4] Alex Trevithick and Bo Yang. Grf: Learning a general radiance field for 3d representation and rendering. In _Proc. ICCV_, pages 15182-15192, 2021.
* [5] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patch-based neural rendering. In _European Conference on Computer Vision_. Springer, 2022.
* [6] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard Pons-Moll. Stereo radiance fields (srf): Learning view synthesis for sparse views of novel scenes. In _Proc. CVPR_, pages 7911-7920, 2021.
* [7] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In _CVPR_, 2021.
* [8] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. _arXiv preprint arXiv:2304.02602_, 2023.
* [9] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. _Proc. CVPR_, 2022.
* [10] Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. In _Proc. NeurIPS_, volume 32, 2019.
* [11] Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, and David Novotny. Unsupervised learning of 3d object categories from videos in the wild. In _Proc. CVPR_, pages 4700-4709, 2021.
* [12] Prafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov, Rares Andrei Ambrus, Adrien Gaidon, William T Freeman, Fredo Durand, Joshua B Tenenbaum, and Vincent Sitzmann. Neural groundplans: Persistent neural scene representations from a single image. In _Proc. ICLR_.
* [13] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. In _Proc. NeurIPS_, 2021.
* [14] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In _Proc. ICCV_, pages 14124-14133, 2021.

* [15] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. _arXiv preprint arXiv:2111.13152_, 2021.
* [16] Shamit Lal, Mihir Prabhudesai, Ishita Mediratta, Adam W Harley, and Katerina Fragkiadaki. Coconets: Continuous contrastive 3d scene representations. In _Proc. CVPR_, pages 12487-12496, 2021.
* [17] Jonas Kulhanek, Erik Derner, Torsten Sattler, and Robert Babuska. Viewformer: Nerf-free neural rendering from few images using transformers. In _European Conference on Computer Vision (ECCV)_, 2022.
* [18] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. MINE: Towards continuous depth MPI with NeRF for novel view synthesis. In _Proc. ICCV_, pages 12578-12588, 2021.
* [19] Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer for nerf-based view synthesis from a single input image. _arXiv preprint arXiv:2207.05736_, 2022.
* [20] Clement Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In _Proc. CVPR_, pages 270-279, 2017.
* [21] Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In _CVPR_, 2019.
* [22] Carlos Campos, Richard Elvira, Juan J Gomez Rodriguez, Jose MM Montiel, and Juan D Tardos. Orb-slam3: An accurate open-source library for visual, visual-inertial, and multimap slam. _IEEE Transactions on Robotics_, 37(6):1874-1890, 2021.
* [23] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. In _Proc. EUROGRAPHICS STAR_, 2022.
* [24] Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In _Proc. NeurIPS_, 2020.
* [25] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Proc. NeurIPS_, 2020.
* [26] Thu H Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yongliang Yang. Rendernet: A deep convolutional network for differentiable rendering from 3d shapes. In _Proc. NeurIPS_, volume 31, 2018.
* [27] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Niessner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In _Proc. CVPR_, 2019.
* [28] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In _Proc. ECCV_, pages 405-421, 2020.
* [29] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escaping plato's cave: 3d shape from adversarial rendering. In _Proc. ICCV_, pages 9984-9993, 2019.
* [30] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Niessner, et al. State of the art on neural rendering. In _Computer Graphics Forum_, volume 39, pages 701-727. Wiley Online Library, 2020.
* [31] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. _Proc. NeurIPS_, 33:2492-2502, 2020.
* [32] Mehdi S. M. Sajjadi, Aravindh Mahendran, Thomas Kipf, Etienne Pot, Daniel Duckworth, Mario Lucic, and Klaus Greff. RUST: Latent Neural Scene Representations from Unposed Imagery. _CVPR_, 2023.
* [33] Richard Hartley and Andrew Zisserman. _Multiple view geometry in computer vision_. Cambridge university press, 2003.
* [34] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Proc. CVPR_, 2016.
* [35] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day. _Communications of the ACM_, 54(10):105-112, 2011.

* [36] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. Nerf-: Neural radiance fields without known camera parameters. _arXiv preprint arXiv:2102.07064_, 2021.
* [37] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting neural radiance fields for pose estimation. In _Proc. IROS_, pages 1323-1330. IEEE, 2021.
* [38] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In _Proc. CVPR_, 2021.
* [39] Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, and Dmitry Lagun. Melon: Nerf with unposed images using equivalence class estimation. _arXiv preprint arXiv:2303.08096_, 2023.
* [40] Wenjing Bian, Zirui Wang, Kejie Li, Jiawang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. 2023.
* [41] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. 2015.
* [42] Jakob Engel, Thomas Schops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In _Proc. ECCV_, 2014.
* [43] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. 2021.
* [44] Krishna Murthy Jatavallabhula, Ganesh Iyer, and Liam Paull. Grad-slam: Dense slam meets automatic differentiation. In _Proc. ICRA_. IEEE, 2020.
* [45] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In _Proc. CVPR_, pages 12786-12796, 2022.
* [46] Antoni Rosinol, John J Leonard, and Luca Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance fields. _arXiv preprint arXiv:2210.13641_, 2022.
* [47] Chi-Ming Chung, Yang-Che Tseng, Ya-Ching Hsu, Xiang-Qian Shi, Yun-Hung Hua, Jia-Fong Yeh, Wen-Chin Chen, Yi-Ting Chen, and Winston H Hsu. Orbee-slam: A real-time monocular visual slam with orb features and nerf-realized mapping. _arXiv preprint arXiv:2209.13274_, 2022.
* [48] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davison. imap: Implicit mapping and positioning in real-time. In _Proc. ICCV_, pages 6229-6238, 2021.
* [49] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In _Proc. CVPR_, 2017.
* [50] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In _Proc. CVPR_, 2018.
* [51] Vitor Guizilini, Kuan-Hui Lee, Rares Ambrus, and Adrien Gaidon. Learning optical flow, depth, and scene flow without real-world labels. _Proc. ICRA_, 2022.
* [52] Yang Fu, Ishan Misra, and Xiaolong Wang. Multiplane nerf-supervised disentanglement of depth and camera pose from videos. _arXiv preprint arXiv:2210.07181_, 2022.
* [53] Zihang Lai, Sifei Liu, Alexei A Efros, and Xiaolong Wang. Video autoencoder: self-supervised disentanglement of static 3d structure and motion. In _Proc. ICCV_, 2021.
* [54] Hsiao-Yu Fish Tung, Ricson Cheng, and Katerina Fragkiadaki. Learning spatial common sense with geometry-aware recurrent networks. In _Proc. CVPR_, 2019.
* [55] Yu Chen and Gim Hee Lee. Dbarf: Deep bundle-adjusting generalizable neural radiance fields. _arXiv preprint arXiv:2303.14478_, 2023.
* [56] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In _Proc. CVPR_, pages 402-419. Springer, 2020.
* [57] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Matzen, Matthew Sticha, and David F. Fouhey. Perspective fields for single image camera calibration. _Proc. CVPR_, 2023.
* [58] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep global registration. In _Proc. CVPR_, 2020.

* [59] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In _Proc. ECCV_, 2020.
* [60] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In _Proc. CVPR_, pages 1611-1621, 2021.
* [61] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. _Proc. TOG_, 36(4), 2017.
* [62] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. _Proc. TOG_, 2018.
* [63] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _Proc. ICCV_, pages 10901-10911, 2021.
* [64] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? The KITTI vision benchmark suite. In _Proc. CVPR_, 2012.
* [65] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proc. CVPR_, pages 18995-19012, 2022.
* [66] Carlos Campos, Richard Elvira, Juan J. Gomez Rodriguez, Jose M. M. Montiel, and Juan D. Tardos. ORB-SLAM3: An accurate open-source library for visual, visual-inertial, and multimap SLAM. _IEEE Transactions on Robotics_, pages 1874-1890, dec 2021.
* [67] Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. Digging into self-supervised monocular depth prediction. In _Proc. ICCV_, 2019.
* [68] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In _Proc. CVPR_, 2023.
* [69] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In _Proc. CVPR_, 2021.
* [70] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. _Proc. ICCV_, 2021.