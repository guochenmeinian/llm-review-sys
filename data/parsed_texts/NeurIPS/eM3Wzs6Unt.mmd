# Learning Variational Temporal Abstraction Embeddings in Option-Induced MDPs

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The option framework in hierarchical reinforcement learning has notably advanced the automatic discovery of temporally-extended actions from long-horizon tasks. However, existing methods often struggle with ineffective exploration and unstable updates when learning action and option policies simultaneously. Addressing these challenges, we introduce the Variational Markovian Option Critic (VMOC), an off-policy algorithm with provable convergence that employs variational inference to stabilize updates. VMOC naturally integrates maximum entropy as intrinsic rewards to promote the exploration of diverse and effective options. Furthermore, we adopt low-cost option embeddings instead of traditional, computationally expensive option triples, enhancing scalability and expressiveness. Extensive experiments in challenging Mujoco environments validate VMOC's superior performance over existing on-policy and off-policy methods, demonstrating its effectiveness in learning coherent and diverse option sets suitable for complex tasks.

## 1 Introduction

Recent advancements in deep reinforcement learning (DRL) have demonstrated significant successes across a variety of complex domains, such as mastering the human level of atari [36] and Go [44] games. These achievements underscore the potential of combining reinforcement learning (RL) with powerful function approximators like neural networks [5] to tackle intricate tasks that require nuanced control over extended periods. Despite these breakthroughs, Deep RL still faces substantial challenges, such as insufficient exploration in dynamic environments [18; 13; 42], inefficient learning associated with temporally extended actions [6; 9] and long horizon tasks [30; 4], and vast amounts of samples required for training proficient behaviors [16; 40; 15].

One promising area for addressing these challenges is the utilization of hierarchical reinforcement learning (HRL) [11; 2; 12], a diverse set of strategies that decompose complex tasks into simpler, hierarchical structures for more manageable learning. Among these strategies, the option framework [47], developed on the Semi-Markov Decision Process (SMDP), is particularly effective at segmenting non-stationary task stages into temporally-extended actions known as options. Options are typically learned through a maximum likelihood approach that aims to maximize the expected rewards across trajectories. In this framework, options act as temporally abstracted actions executed over variable time steps, controlled by a master policy that decides when each option should execute and terminate. This structuring not only simplifies the management of complex environments but also enables the systematic discovery and execution of temporal abstractions over long-horizon tasks [24; 23].

However, the underlying SMDP framework is frequently undermined by three key challenges: 1) Insufficient exploration and degradation [20; 37; 23]. As options are unevenly updated using conventional maximum likelihood methods [4; 10; 45; 25; 26], the policy is quickly saturated with early rewarding observations. This typically results in focusing on only low-entropy options that leadto local optima rewards, causing a single option to either dominate the entire policy or switch every timestep. Such premature convergence limits option diversity significantly. 2) Sample Inefficiency. The semi-Markovian nature inherently leads to sample inefficiency [47; 29]: each policy update at the master level extends over multiple time steps, thus consuming a considerable volume of experience samples with relatively low informational gain. This inefficiency is further exacerbated by the prevalence of on-policy option learning algorithms [4; 52], which require new samples to be collected simultaneously from both high-level master policies and low-level action policies at each gradient step, and thus sample expensive. 3) Computationally expensive. Options are conventionally defined as triples [4] with intra-option policies and termination functions, often modeled using neural networks which are expensive to optimize. These challenges collectively limit the broader adoption and effectiveness of the option framework in real-world scenarios, particularly in complex continuous environments where scalability and stability are critical [14; 34; 26].

To address these challenges, we introduce the Variational Markovian Option Critic (VMOC), a novel off-policy algorithm that integrates the variational inference framework on option-induced MDPs [35]. We first formulate the optimal option-induced SMDP trajectory as a probabilistic inference problem, presenting a theoretical convergence proof of the variational distribution under the soft policy iteration framework [19]. Similar to prior variational methods [31], policy entropy terms naturally arise as intrinsic rewards during the inference procedure. As a result, VMOC not only seeks high-reward options but also maximizes entropy across the space, promoting extensive exploration and maintaining high diversity. We implements this inference procedure as an off-policy soft actor critic [19] algorithm, which allows reusing samples from replay buffer and enhances sample efficiency. Furthermore, to address the computational inefficiencies associated with conventional option triples, we follow [35] and employ low-cost option embeddings rather than complex neural network models. This not only simplifies the training process but also enhances the expressiveness of the model by allowing the agent to capture a more diverse set of environmental dynamics.

Our contributions can be summarized as follows:

* We propose a variational inference approach within the maximum entropy framework to enhance diverse and robust exploration of options.
* We implement an off-policy algorithm that improves sample efficiency.
* We introduce option embeddings into latent variable policies and enhance expressiveness and computational cost-effectiveness of option representations.
* We conduct extensive experiments in OpenAI Gym Mujoco [49] environments, demonstrating that VMOC significantly outperforms other option-based variants in terms of exploration capabilities, sample efficiency, and computational efficiency.

## 2 Preliminary

### Control as Structured Variational Inference

Conventionally, the control as inference framework [19; 31; 19; 53] is derived using the maximum entropy objective. In this section, we present an alternative derivation from the perspective of structured variational inference. We demonstrate that this approach provides a more concise and intuitive pathway to the same theoretical results, where the maximum entropy principle naturally emerges through the direct application of variational inference techniques.

Traditional control methods focus on directly maximizing rewards, often resulting in suboptimal tradeoffs between exploration and exploitation. By reinterpreting the control problem as a probabilistic inference problem, the control as inference framework incorporates both the reward structure and environmental uncertainty into decision-making, providing a more robust and flexible approach to policy optimization. In this framework, optimality is represented by a binary random variable \(\mathcal{E}\in\{0,1\}\)1. The probability of optimality given a state-action pair \((\mathbf{s},\mathbf{a})\) is denoted as \(P(\mathcal{E}=1\mid\mathbf{s},\mathbf{a})=\exp(r(\mathbf{s},\mathbf{a}))\), which is an exponential function of the conventional reward function \(r(\mathbf{s},\mathbf{a})\) that measures the desirability of an action in a specific state. Focusing on \(\mathcal{E}=1\) captures the occurrence of optimal events. For simplicity, we will use \(\mathcal{E}\) instead of \(\mathcal{E}=1\) in the following text to avoid cluttered notations. The joint distribution over trajectories \(\tau=(\mathbf{s}_{1},\mathbf{a}_{1},\ldots,\mathbf{s}_{T},\mathbf{a}_{T})\) given optimality is expressed as:

\[P(\tau|\mathcal{E}_{1:T})\propto P(\tau,\mathcal{E}_{1:T})=P(\mathbf{s}_{1}) \prod_{t=1}^{T-1}P(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})P(\mathcal{E }_{t}|\mathbf{s}_{t},\mathbf{a}_{t})\]

where \(P(\mathbf{s}_{1})\) is the initial state distribution, \(P(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})\) is the dynamics model. As explained in [19; 31], direct optimization of \(P(\tau\mid\mathcal{E}_{1:T})\) can result in an optimistic policy that assumes a degree of control over the dynamics. One way to correct this risk-seeking behavior [31] is through structured variational inference. In our case, the goal is to approximate the optimal trajectory \(P(\tau)\) with the variational distribution:

\[q(\tau)=P(\mathbf{s}_{1})\prod_{t=1}^{T-1}P(\mathbf{s}_{t+1}\mid\mathbf{s}_{t },\mathbf{a}_{t})q(\mathbf{a}_{t}\mid\mathbf{s}_{t})\]

where the initial distribution \(P(\mathbf{s}_{1})\) and transition distribution \(P(\mathbf{s}_{t+1}\mid\mathbf{s}_{t},\mathbf{a}_{t})\) is set to be the true environment dynamics from \(P(\tau)\). The only variational term is the variational policy \(q(\mathbf{a}_{t}\mid\mathbf{s}_{t})\), which is used to approximate the optimal policy \(P(\mathbf{a}_{t}\mid\mathbf{s}_{t},\mathcal{E}_{1:T})\). Under this setting, the environment dynamics will be canceled out from the optimization objective between \(P(\tau\mid\mathcal{E})\) and \(q(\tau)\), thus explicitly disallowing the agent to influence its dynamics and correcting the risk-seeking behavior.

With the variational distribution at hand, the conventional maximum entropy framework can be recovered through a direct application of standard structural variational inference [28]:

\[\log P(\mathcal{E}_{1:T}) =\mathcal{L}(q(\tau),P(\tau,\mathcal{E}_{1:T}))+D_{\mathrm{KL}}(q (\tau)\parallel P(\tau|\mathcal{E}_{1:T}))\] \[=\underbrace{\mathbb{E}_{\tau\sim q(\tau)}[\sum_{t}^{T}r(\mathbf{ s}_{t},\mathbf{a}_{t})+\mathcal{H}(q(\cdot|\mathbf{s}_{t}))]}_{\text{ maximum entropy objective}}+D_{\mathrm{KL}}(q(\mathbf{a}_{t}|\mathbf{s}_{t})\parallel P( \mathbf{a}_{t}|\mathbf{s}_{t},\mathcal{E}_{1:T}))\]

where \(\mathcal{L}(q,P)=\mathbb{E}_{q}[\log\frac{P}{q}]\) is the Evidence Lower Bound (ELBO) [28]. The maximum entropy objective arises naturally as the environment dynamics in \(P(\tau,\mathcal{E})\) and \(q(\tau)\) cancel out. Under this formulation, the soft policy iteration theorem [19] has an elegant Expectation-Maximization (EM) algorithm [28] interpretation: the E-step corresponds to the policy evaluation of the maximum entropy objective \(\mathcal{L}(q^{[k]},P)\); while the M-step corresponds to the policy improvement of the \(D_{\mathrm{KL}}\) term \(q^{[k+1]}=\operatorname*{arg\,max}_{q}D_{\mathrm{KL}}(q^{[k]}(\tau)\parallel P (\tau\mid\mathcal{E}))\). Thus, soft policy iteration is an exact inference if both EM steps can be performed exactly.

**Theorem 1** (Convergence Theorem for Soft Policy Iteration).: _Let \(\tau\) be the latent variable and \(\mathcal{E}\) be the observed variable. Define the variational distribution \(q(\tau)\) and the log-likelihood \(\log P(\mathcal{E})\). Let \(M:q^{[k]}\to q^{[k+1]}\) represent the mapping defined by the EM steps inference update, so that \(q^{[k+1]}=M(q^{[k]})\). The likelihood function increases at each iteration of the variational inference algorithm until convergence conditions are satisfied._

Proof.: See A.1. 

### The Option Framework

In conventional SMDP-based Option Framework [47], an option is a triple \((\mathbb{I}_{o},\pi_{o},\beta_{o})\in\mathcal{O}\), where \(\mathcal{O}\) denotes the option set; \(o\in\mathbb{O}=\{1,2,\ldots,K\}\) is a positive integer index which denotes the \(o\)-th triple where \(K\) is the number of options; \(\mathbb{I}_{o}\) is an initiation set indicating where the option can be initiated; \(\pi_{o}=P_{o}(\mathbf{a}|\mathbf{s}):\mathbb{A}\times\mathbb{S}\to[0,1]\) is the action policy of the \(o\)th option; \(\beta_{o}=P_{o}(\mathbf{b}=1|\mathbf{s}):\mathbb{S}\to[0,1]\) where \(\mathbf{b}\in\{0,1\}\) is a _termination function_. For clarity, we use \(P_{o}(\mathbf{b}=1|\mathbf{s})\) instead of \(\beta_{o}\) which is widely used in previous option literatures (e.g., Sutton et al. [47], Bacon et al. [4]). A _master policy_\(\pi(\mathbf{o}|\mathbf{s})=P(\mathbf{o}|\mathbf{s})\) where \(\mathbf{o}\in\mathbb{O}\) is used to sample which option will be executed. Therefore, the dynamics (stochastic process) of the option framework is written as:

\[P(\tau)=P(\mathbf{s}_{0},\mathbf{o}_{0}) \prod_{t=1}^{\infty}P(\mathbf{s}_{t}|\mathbf{s}_{t-1},\mathbf{a}_{ t-1})P_{o_{t}}(\mathbf{a}_{t}|\mathbf{s}_{t})\] \[[P_{o_{t-1}}(\mathbf{b}_{t}=0|\mathbf{s}_{t})\mathbf{1}_{\mathbf{ o}_{t}=o_{t-1}}+P_{o_{t-1}}(\mathbf{b}_{t}=1|\mathbf{s}_{t})P(\mathbf{o}_{t}| \mathbf{s}_{t})],\] (1)where \(\tau=\{\mathbf{s}_{0},\mathbf{0}_{0},\mathbf{a}_{0},\mathbf{s}_{1},\mathbf{o}_{1}, \mathbf{a}_{1},\ldots\}\) denotes the trajectory of the option framework. \(\mathbf{1}\) is an indicator function and is only true when \(\mathbf{o}_{t}=o_{t-1}\) (notice that \(o_{t-1}\) is the realization at \(\mathbf{o}_{t-1}\)). Therefore, under this formulation the option framework is defined as a Semi-Markov process since the dependency on an activated option \(o\) can cross a variable amount of time [47]. Due to the nature of SMDP assumption, conventional option framework is unstable and computationally expensive to optimize. Li et al. [34; 35] proposed the Hidden Temporal Markovian Decision Process (HiT-MDP):

\[P(\tau)=P(\mathbf{s}_{0},\mathbf{o}_{0})\prod_{t=1}^{\infty}P(\mathbf{s}_{t}| \mathbf{s}_{t-1},\mathbf{a}_{t-1})P(\mathbf{a}_{t}|\mathbf{s}_{t},\mathbf{o}_{ t})P(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1})\] (2)

and theoretically proved that the option-induced HiT-MDP is homomorphically equivalent to the conventional SMDP-based option framework. Following RL conventions, we use \(\pi^{A}=P(\mathbf{a}_{t}|\mathbf{s}_{t},\mathbf{o}_{t})\) to denote the action policy and \(\pi^{O}=P(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1})\) to denote the option policy respectively. In HiT-MDPs, options can be viewed as latent variables with a temporal structure \(P(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1})\), enabling options to be represented as dense latent embeddings rather than traditional option triples. They demonstrated that learning options as embeddings on HiT-MDPs offers significant advantages in performance, scalability, and stability by reducing variance. However, their work only derived an on-policy policy gradient algorithm for learning options on HiT-MDPs. In this work, we extend their approach to an off-policy algorithm under the variational inference framework, enhancing exploration and sample efficiency.

## 3 Methodology

In this section, we introduce the Variational Markovian Option Critic (VMOC) algorithm by extending the variational policy iteration (Theorem 1) to the option framework. In Section 3.1, we reformulate the optimal option trajectory and the variational distribution as probabilistic graphical models (PGMs), propose the corresponding variational objective, and present a provable exact inference procedure for these objectives in tabular settings. Section 3.2 extends this result by introducing VMOC, a practical off-policy option learning algorithm that uses neural networks as function approximators and proves the convergence of VMOC under approximate inference settings. Our approach differs from previous works [19; 33; 34] by leveraging structured variational inference directly, providing a more concise pathway to both theoretical results and practical algorithms.

### PGM Formulations of The Option Framework

Formulating complex problems as probabilistic graphical models (PGMs) offers a consistent and flexible framework for deriving principled objectives, analyzing convergence, and devising practical algorithms. In this section, we first formulate the optimal trajectory of the conventional SMDP-based option framework (Eq. 1) as a PGM. We then use the HiT-MDPs as the variational distribution to approximate this optimal trajectory. With these PGMs, we can straightforwardly derive the variational objective, where maximum entropy terms arise naturally. This approach allows us to develop a stable algorithm for learning diversified options and preventing degeneracy. Specifically, we follow [31; 28]

by introducing the concept of "Optimality" [48] into the conventional SMDP-based option framework (Equation equation 1). This allows us to define the probability of an option trajectory being optimal

Figure 1: PGMs of the option framework.

as a probabilistic graphical model (PGM), as illustrated in Figure 1 (a):

\[P(\tau,\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1:T}) =P(\mathbf{s}_{0},\mathbf{o}_{0})\prod_{t=1}^{T}P(\mathbf{s}_{t+1}| \mathbf{s}_{t},\mathbf{a}_{t})P(\mathcal{E}^{A}_{t}=1|\mathbf{s}_{t},\mathbf{a }_{t})P(\mathcal{E}^{O}_{t}=1|\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t}, \mathbf{o}_{t-1})P(\mathbf{o}_{t})P(\mathbf{a}_{t})\] \[\propto\underbrace{P(\mathbf{s}_{0})\prod_{t=1}^{T}P(\mathbf{s}_ {t+1}|\mathbf{s}_{t},\mathbf{a}_{t})}_{\text{Environment Dynamics}}\underbrace{ \prod_{t=1}^{T}P(\mathcal{E}^{A}_{t}=1|\mathbf{s}_{t},\mathbf{a}_{t})P( \mathcal{E}^{O}_{t}=1|\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t},\mathbf{o} _{t-1})}_{\text{Optimality Likelihood}},\] (3)

where \(\mathcal{E}\in\{0,1\}\) are observable binary "optimal random variables" [31], \(\tau=\{\mathbf{s}_{0},\mathbf{o}_{0},\mathbf{a}_{0},\mathbf{s}_{1}\ldots\}\) denotes the trajectory of the option framework. The agent is _optimal_ at time step \(t\) when \(P(\mathcal{E}^{A}_{t}=1|\mathbf{s}_{t},\mathbf{a}_{t})\) and \(P(\mathcal{E}^{O}_{t}=1|\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t},\mathbf{ o}_{t-1})\). We will use \(\mathcal{E}\) instead of \(\mathcal{E}=1\) in the following text to avoid cluttered notations. To simplify the derivation, priors \(P(\mathbf{o})\) and \(P(\mathbf{a})\) can be assumed to be uniform distributions without loss of generality [31]. Note that Eq. 3 shares the same environment dynamics with Eq. 1 and Eq. 2. With the optimal random variables \(\mathcal{E}^{O}\) and \(\mathcal{E}^{A}\), the likelihood of a state-action \(\{\mathbf{s}_{t},\mathbf{a}_{t}\}\) pair that is optimal is defined as:

\[P(\mathcal{E}^{A}_{t}|\mathbf{s}_{t},\mathbf{a}_{t})=\exp(r(\mathbf{s}_{t}, \mathbf{a}_{t})),\] (4)

as this specific design facilitates recovering the value function at the latter structural variational inference stage. Based on the same motivation, the likelihood of an option-state-action \(\{\mathbf{o}_{t},\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t-1}\}\) pair that is optimal is defined as,

\[P(\mathcal{E}^{O}_{t}|\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t},\mathbf{o} _{t-1})=\exp(f(\mathbf{o}_{t},\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t-1})),\] (5)

where \(f(\cdot)\) is an arbitrary non-positive function which measures the preferable of selecting an option given state-action pair \([\mathbf{s}_{t},\mathbf{a}_{t}]\) and the previous executed option \(\mathbf{o}_{t-1}\). In this work, we choose \(f\) to be the mutual-information \(f=I[\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t-1}]\) as a fact that when the uniform prior assumption of \(P(\mathbf{o})\) is relaxed the optimization introduces a mutual-information as a regularizer [35].

As explained in Section 2.1, direct optimization of Eq. 3 results in optimistic policies that assumes a degree of control over the dynamics. We correct this risk-seeking behavior [31] through approximating the optimal trajectory \(P(\tau)\) with the variational distribution:

\[q(\tau)=P(\mathbf{s}_{0},\mathbf{o}_{0})\prod_{t=1}^{T-1}P(\mathbf{s}_{t+1}| \mathbf{s}_{t},\mathbf{a}_{t})q(\mathbf{a}_{t}|\mathbf{s}_{t},\mathbf{o}_{t}) q(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1})\] (6)

where the initial distribution \(P(\mathbf{s}_{0},\mathbf{o}_{0})\) and transition distribution \(P(\mathbf{s}_{t+1}\mid\mathbf{s}_{t},\mathbf{a}_{t})\) is set to be the true environment dynamics from \(P(\tau)\). The variational distribution turns out to be the HiT-MDP, where the action policy \(q(\mathbf{a}_{t}\mid\mathbf{s}_{t})\) and the option policy \(q(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1})\) are used to approximate the optimal policy \(P(\mathbf{a}_{t}|\mathbf{s}_{t},\mathbf{o}_{t},\mathcal{E}^{A}_{1:T})\) and \(P(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1},\mathcal{E}^{O}_{1:T})\). The Evidence Lower Bound (ELBO) [28] of the log-likelihood optimal trajectory (Eq. 3) can be derived as (see A.3):

\[\mathcal{L}(q(\tau),P(\tau,\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_ {1:T})) =\mathbb{E}_{q(\tau)}[\log P(\tau,\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_ {1:T})-\log q(\tau)]\] \[=\mathbb{E}_{q(\tau)}[r(\mathbf{s}_{t},\mathbf{a}_{t})+f(\cdot) \neg\log q(\mathbf{a}_{t}|\mathbf{s}_{t},\mathbf{o}_{t})-\log q(\mathbf{o}_{ t}|\mathbf{s}_{t},\mathbf{o}_{t-1})]\] \[=\mathbb{E}_{q(\tau)}\left[r(\mathbf{s}_{t},\mathbf{a}_{t})+f( \cdot)+\mathcal{H}[\pi^{A}]+\mathcal{H}[\pi^{O}]\right]\] (7)

where line 2 is substituting Eq. 3 and Eq. 6 into the ELBO. As a result, the maximum entropy objective naturally arises in Eq. 7. Optimizing the ELBO not only seeks high-reward options but also maximizes entropy across the space, promoting extensive exploration and maintaining high diversity.

Given the ELBO, we now define soft value functions of the option framework following the Bellman Backup Functions along the trajectory \(q(\tau)\) as follow:

\[Q^{soft}_{O}[\mathbf{s}_{t},\mathbf{o}_{t}] =f(\cdot)+\mathbb{E}_{\mathbf{a}_{t}\sim\pi^{A}}\left[Q^{soft}_{A }[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]\right]+H[\pi^{A}],\] (8) \[Q^{soft}_{A}[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}] =r(s,a)+\mathbb{E}_{\mathbf{s}_{t+1}\sim P(\mathbf{s}_{t+1}| \mathbf{s}_{t},\mathbf{a}_{t})}\left[\mathbb{E}_{\mathbf{o}_{t+1}\sim\pi^{O}} \left[Q^{soft}_{O}[\mathbf{s}_{t+1},\mathbf{o}_{t+1}]\right]+H[\pi^{O}]\right]\] (9)

Assuming policies \(\pi^{A},\pi^{O}\in\Pi\) where \(\Pi\) is an arbitrary feasible set, under a tabular setting where the inference on \(\mathcal{L}\) can be done exactly, we have the following theorem holds:

**Theorem 2** (Soft Option Policy Iteration Theorem).: _Repeated optimizing \(\mathcal{L}\) and \(D_{\mathrm{KL}}\) defined in Eq. 10 from any \(\pi^{A}_{0},\pi^{O}_{0}\in\Pi\) converges to optimal policies \(\pi^{A*},\pi^{O*}\) such that \(Q^{soft*}_{O}[\mathbf{s}_{t},\mathbf{o}_{t}]\geq Q^{soft}_{O}[\mathbf{s}_{t}, \mathbf{o}_{t}]\) and \(Q^{soft*}_{A}[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]\geq Q^{soft}_{A}[ \mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]\), for all \(\pi^{A}_{0},\pi^{O}_{0}\in\Pi\) and \((\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t})\in\mathcal{S}\times\mathcal{A}\times \mathcal{O}\), assuming under tabular settings where \(|\mathcal{S}|<\infty,\ |\mathcal{O}|<\infty,\ |\mathcal{A}|<\infty\)._

Proof.: See Appendix A.2. 

Theorem 2 guarantees finding the optimal solution only when the inference can be done exactly under tabular settings. However, real-world applications often involve large continuous domains and employ neural networks as function approximators. In these cases, inference procedures can only be done approximately. This necessitate a practical approximation algorithm which we present below.

### Variational Markovian Option Critic Algorithm

Formulating complex problems as probabilistic graphical models (PGMs) allowing us to leverage established methods from PGM literature to address the associated inference and learning challenges in real-world applications. To this end, we utilizes the structured variational inference treatment for optimizing the log-likelihood of optimal trajectory and prove its convergence under approximate inference settings. Specifically, using the variational distribution \(q(\tau)\) (Eq. 6) as an approximator, the ELBO can be derived as (see Appendix A.3):

\[\mathcal{L}(q(\tau),P(\tau,\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1:T}))=-D_{ \mathrm{KL}}(q(\tau)||P(\tau|\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1:T}))+ \log P(\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1:T})\] (10)

where \(D_{\mathrm{KL}}\) is the KL-divergence between the trajectory following variational policies \(q(\tau)\) and optimal policies \(P(\tau|\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1:T})\). Under the structural variational inference [28] perspective, convergence to the optimal policy can be achieved by optimizing the ELBO with respect to the the variational policy repeatedly:

**Theorem 3** (Convergence Theorem for Variational Markovian Option Policy Iteration).: _Let \(\tau\) be the latent variable and \(\mathcal{E}^{A},\mathcal{E}^{O}\) be the ground-truth optimality variables. Define the variational distribution \(q(\tau)\) and the true log-likelihood of optimality \(\log P(\mathcal{E}^{A},\mathcal{E}^{O})\). iterates according to the update rule \(q^{k+1}=\operatorname*{arg\,max}_{q}\mathcal{L}(q(\tau),P(\tau,\mathcal{E}^{A }_{1:T},\mathcal{E}^{O}_{1:T}))\) converges to the maximum value bounded by the true log-likelihood of optimality._

Proof.: See Appendix A.4. 

We further implements a practical algorithm, the Variational Markovian Option Policy Iteration. Let \(\tau\) be the latent variable and \(\mathcal{E}^{A},\mathcal{E}^{O}\) be the ground-truth optimality variables. Define the variational distribution \(q(\tau)\) and the true log-likelihood of optimality \(\log P(\mathcal{E}^{A},\mathcal{E}^{O})\). iterates according to the update rule \(q^{k+1}=\operatorname*{arg\,max}_{q}\mathcal{L}(q(\tau),P(\tau,\mathcal{E}^{A }_{1:T},\mathcal{E}^{O}_{1:T}))\) converges to the maximum value bounded by the true log-likelihood of optimality.

Proof.: See Appendix A.4. 

We further implements a practical algorithm, the Variational Markovian Option Critic (VMOC) algorithm, which is suitable for complex continuous domains. Specifically, we employ parameterized neural networks as function approximators for both the Q-functions (\(Q^{soft}_{\psi^{A}}\), \(Q^{soft}_{\psi^{A}}\)) and the policies (\(\pi_{\theta^{A}}\), \(\pi_{\theta^{O}}\)). Instead of running evaluation and improvement to full convergence using Theorem 2, we can optimize the variational distribution by taking stochastic gradient descent following Theorem 3 with respect to the ELBO (Eq. 7) directly. Share the same motivation with Haarnoja et al. [19] of reducing the variance during the optimization procedure, we derive an option critic framework by optimizing the maximum entropy objectives between the action Eq. 9 and the option Eq. 8 alternatively. The Bellman residual for the action critic is:

\[J_{Q^{A}}(\psi^{A}_{i})=\mathbb{E}_{(\mathbf{s}_{t},\mathbf{o}_{t}, \mathbf{a}_{t},\mathbf{s}_{t+1})\sim D}\bigg{[}\bigg{(}\min_{i=1,2}Q_{\psi^{A} _{i}}(\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t})-\] \[\big{(}r(\mathbf{s}_{t},\mathbf{a}_{t})+\mathbb{E}_{\mathbf{o}_{t+ 1}\sim\pi^{O}}\left[Q^{soft}_{O}[\mathbf{s}_{t+1},\mathbf{o}_{t+1}]\right]+ \alpha^{O}H[\pi^{O}]\big{)}\bigg{)}^{2}\bigg{]}\]

where \(\alpha^{O}\) is the temperature hyper-parameter and the expectation over option random variable \(\mathbb{E}_{\mathbf{o}_{t+1}\sim\pi^{O}}\) can be evaluated exactly since \(\pi^{O}\) is a discrete distribution. The Bellman residual for the option critic is:

\[J_{Q^{O}}(\psi^{O}_{i})=\mathbb{E}_{(\mathbf{s}_{t},\mathbf{o}_{t})\sim D} \bigg{[}\bigg{(}\min_{i=1,2}Q^{O}_{\psi^{O}_{i}}(\mathbf{s}_{t},\mathbf{o}_{t} )-\] \[\big{(}f(\cdot)+\mathbb{E}_{\mathbf{a}_{t}\sim\pi^{A}}\left[Q^{ soft}_{A}[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]-\alpha^{A}\log q( \mathbf{a}_{t}|\mathbf{s}_{t},\mathbf{o}_{t})\right]\big{)}\bigg{)}^{2}\bigg{]}\]\(\alpha^{A}\) is the temperature hyper-parameter. Unlike \(\mathbb{E}_{\mathbf{o}_{t+1\sim\pi^{O}}}\) can be trivially evaluated, evaluating \(\mathbb{E}_{\mathbf{a}_{t}\sim\pi^{A}}\) is typically intractable. Therefore, in implementation we use \(\mathbf{a}_{t}\) sampled from the replay buffer to estimate the expectation over \(\pi^{A}\).

Following Theorem3, the policy gradients can be derived by directly taking gradient with respect to the ELBOs defined for the action Eq.9 and the option Eq.8 policies respectively. The action policy objective is given by:

\[J_{\pi^{A}}(\theta^{A})=-\mathbb{E}_{(\mathbf{s}_{t},\mathbf{o}_{t})\sim D} \left[\min_{i=1,2}Q_{\psi_{i}^{A}}(\mathbf{s}_{t},\mathbf{o}_{t},\tilde{ \mathbf{a}}_{t})-\alpha^{A}\log q(\tilde{\mathbf{a}}_{t}|\mathbf{s}_{t}, \mathbf{o}_{t})\right],\ \tilde{\mathbf{a}}_{t}\sim q(\cdot|\mathbf{s}_{t},\mathbf{o}_{t})\]

where in practice the action policy is often sampled by using the re-parameterization trick introduced in [19]. The option objective is given by:

\[J_{\pi^{O}}(\theta^{O})=-\mathbb{E}_{(\mathbf{s}_{t},\mathbf{o}_{t-1})\sim D }\left[\min_{i=1,2}Q_{\psi_{i}^{O}}(\mathbf{s}_{t},\mathbf{o}_{t})+\alpha^{O }\mathcal{H}[\pi^{O}]\right]\]

The variational distribution \(q(\tau)\) defined in Eq.6 allows us to learn options as embeddings [34; 35] with a learnable embedding matrix \(\mathbf{W}\in\mathbb{R}^{\texttt{num\_options}\times\texttt{embedding\_ dim}}\). Under this setting, the embedding matrix \(\mathbf{W}\) can be absorbed into the parameter vector \(\theta^{O}\). This integration into VMOC ensures that options are represented as embeddings without any additional complications, thereby enhancing the expressiveness and scalability of the model.

The temperature hyper-parameters can also be adjusted by minimizing the following objective:

\[J(\alpha^{A})=-\mathbb{E}_{\tilde{\mathbf{a}}_{t}\sim\pi^{A}}\left[\alpha^{A }(\log\pi^{A}(\tilde{\mathbf{a}}_{t}\mid\mathbf{s}_{t},\mathbf{o}_{t})+ \overline{\mathcal{H}})\right]\]

for the action policy temperature \(\alpha^{A}\), where \(\overline{\mathcal{H}}\) is a target entropy. Similarly, the option policy temperature \(\alpha^{O}\) can be adjusted by:

\[J(\alpha^{O})=-\mathbb{E}_{\mathbf{o}_{t}\sim\pi^{O}}\left[\alpha^{O}(\log\pi ^{O}(\mathbf{o}_{t}\mid\mathbf{s}_{t},\mathbf{o}_{t-1})+\overline{\mathcal{H }})\right]\]

where \(\overline{\mathcal{H}}\) is also a target entropy for the option policy. In both cases, the temperatures \(\alpha^{A}\) and \(\alpha^{O}\) are updated using gradient descent, ensuring that the entropy regularization terms dynamically adapt to maintain a desired level of exploration. This approach aligns with the methodology proposed in SAC [19]. By adjusting the temperature parameters, the VMOC algorithm ensures a balanced trade-off between exploration and exploitation, which is crucial for achieving optimal performance in complex continuous control tasks. We summarize the VMOC algorithm in AppendixB.

## 4 Experiments

In this section, we design experiments on the challenging single task OpenAI Gym MuJoCo [7] environments (10 environments) to test Variational Markovian Option Critic (VMOC)'s performance over other option variants and non-option baselines.

For VMOC in all environments, we fix the temperature rate for both \(\alpha^{O}\) and \(\alpha^{A}\) to \(0.05\); we add an exploration noise \(\mathcal{N}(\mu=0,\sigma=0.2)\) during exploration. For all baselines, we follow DAC [52]'s open source implementations and compare our algorithm with six baselines, five of which are option variants, _i.e._, MOPG [35], DAC+PPO, AHP+PPO [32], IOPG [45], PPOC [27], OC [4] and PPO [41]. All baselines' parameters used by DAC remain unchanged over 1 million environment steps to converge. Figures are plotted following DAC's style: curves are averaged over 10 independent runs and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations. All experiments are run on an Intel(r) Core(tm) i9-9900X CPU @ 3.50GHz with a single thread and process. Our implementation details are summarized in AppendixC. For a fair comparison, we follow option literature conventions and use four options in all implementations. Our code is available in supplemental materials.

## 5 Experiments

We evaluate the performance of VMOC against six option-based baselines (MOPG [35], DAC+PPO [52], AHP+PPO [32], IOPG [45], PPOC [27], and OC [4]) as well as the hierarchy-freePPO algorithm [41]. Previous studies [27; 45; 20; 52] have suggested that option-based algorithms do not exhibit significant advantages over hierarchy-free algorithms in single-task environments. Nonetheless, our results demonstrate that VMOC significantly outperforms all baselines in terms of episodic return, convergence speed, step variance, and variance across 10 runs, as illustrated in Figure 2. The only exception is the relatively simple InvertedDoublePendulum environment, which

Notably, VMOC exhibits superior performance on the Humanoid-v2 and HumanoidStandup-v2 environments. These environments are characterized by a large state space (\(\mathcal{S}\in\mathbb{R}^{376}\)) and action space (\(\mathcal{A}\in\mathbb{R}^{17}\)), whereas other environments typically have state dimensions less than 20 and action dimensions less than 5. The enhanced performance of VMOC in these environments can be attributed to its maximum entropy capability: in large state-action spaces, the agent must maximize rewards while exploring a diverse set of state-action pairs. Maximum likelihood methods tend to quickly saturate with early rewarding observations, leading to the selection of low-entropy options that converge to local optima.

A particularly relevant comparison is with the Markovian Option Policy Gradient (MOPG) [35], as both VMOC and MOPG are developed based on HiT-MDPs and employ option embeddings. Despite being derived under the maximum entropy framework, MOPG utilizes an on-policy gradient descent approach. Our experimental results show that VMOC's performance surpasses that of MOPG, highlighting the limitations of on-policy methods, which suffer from shortsighted rollout lengths and quickly saturate to early high-reward observations. In contrast, VMOC's variational off-policy approach effectively utilizes the maximum entropy framework by ensuring better exploration and stability across the learning process. Additionally, the off-policy nature of VMOC allows it to reuse samples from a replay buffer, enhancing sample efficiency and promoting greater diversity in the learned policies. This capability leads to more robust learning, as the algorithm can leverage a broader range of experiences to improve policy optimization.

## 6 Related Work

The VMOC incorporates three key ingredients: the option framework, a structural variational inference based off-policy algorithm and latent variable policies. We review prior works that draw

Figure 2: Experiments on Mujoco Environments. Curves are averaged over 10 independent runs with different random seeds and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations.

on some of these ideas in this section. The options framework [47] offers a promising approach for discovering and reusing temporal abstractions, with options representing temporally abstract skills. Conventional option frameworks [39], typically developed under the maximum likelihood (MLE) framework with few constraints on options behavior, often suffer from the option degradation problem [32, 4]. This problem occurs when options quickly saturate with early rewarding observations, causing a single option to dominate the entire policy, or when options switch every timestep, maximizing policy at the expense of skill reuse across tasks. On-policy option learning algorithms [4, 3, 52, 34, 35] aim to maximize expected return by adjusting policy parameters to increase the likelihood of high-reward option trajectories, which often leads to focusing on low-entropy options. Several techniques [20, 21, 23] have been proposed to enhance on-policy algorithms with entropy-like extrinsic rewards as regularizers, but these often result in biased optimal trajectories. In contrast, the maximum entropy term in VMOC arises naturally within the variational framework and provably converges to the optimal trajectory.

Although several off-policy option learning algorithms have been proposed [10, 43, 45, 50], these typically focus on improving sample efficiency by leveraging the control as inference framework. Recent works [45] aim to enhance sample efficiency by inferring and marginalizing over options, allowing all options to be learned simultaneously. Wulfmeier et al. [50] propose off-policy learning of all options across every experience in hindsight, further boosting sample efficiency. However, these approaches generally lack constraints on options behavior. A closely related work [33] also derives a variational approach under the option framework; however, it is based on probabilistic graphical model that we believe are incorrect, potentially leading to convergence issues. Additionally, our algorithm enables learning options as latent embeddings, a feature not present in their approach.

Recently, several studies have extended the maximum entropy reinforcement learning framework to discover skills by incorporating additional latent variables. One class of methods [22, 17] maintains latent variables constant over the duration of an episode, providing a time-correlated exploration signal. Other works [19, 51] focus on discovering multi-level action abstractions that are suitable for repurposing by promoting skill distinguishability, but they do not incorporate temporal abstractions. Studies such as [38, 1, 8] aim to discover temporally abstract skills essential for exploration, but they predefine their temporal resolution. In contrast, VMOC learns temporal abstractions as embeddings in an end-to-end data-driven approach with minimal prior knowledge encoded in the framework.

## 7 Conclusion

In this paper, we have introduced the Variational Markovian Option Critic (VMOC), a novel off-policy algorithm designed to address the challenges of ineffective exploration, sample inefficiency, and computational complexity inherent in the conventional option framework for hierarchical reinforcement learning. By integrating a variational inference framework, VMOC leverages maximum entropy as intrinsic rewards to promote the discovery of diverse and effective options. Additionally, by employing low-cost option embeddings instead of traditional, computationally expensive option triples, VMOC enhances both scalability and expressiveness. Extensive experiments in challenging Mujoco environments demonstrate that VMOC significantly outperforms existing on-policy and off-policy option variants, validating its effectiveness in learning coherent and diverse option sets suitable for complex tasks. This work advances the field of hierarchical reinforcement learning by providing a robust, scalable, and efficient method for learning temporally extended actions.

## 8 Limitations

Due to limited computing resources, we did not conduct an ablation study of VMOC. Additionally, the temperature parameter was fixed in our experiments, whereas an automatically tuned parameter could potentially enhance performance (see SAC [19]). While our baselines focus on option variants, a thorough comparison to other off-policy algorithms is also worth investigating. It is particularly important to explore whether VMOC exhibits performance improvements in scalability when the number of option embeddings is significantly increased. These investigations are left for future work.

## References

* Ajay et al. [2020] Ajay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum, O. Opal: Offline primitive discovery for accelerating offline reinforcement learning. _arXiv preprint arXiv:2010.13611_, 2020.
* Araujo and Grupen [1996] Araujo, E. G. and Grupen, R. A. Learning control composition in a complex environment. In _Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior_, pp. 333-342, 1996.
* Bacon [2018] Bacon, P.-L. _Temporal Representation Learning_. PhD thesis, McGill University Libraries, 2018.
* Bacon et al. [2017] Bacon, P.-L., Harb, J., and Precup, D. The option-critic architecture. In _Thirty-First AAAI Conference on Artificial Intelligence_, 2017.
* Bertsekas and Tsitsiklis [1996] Bertsekas, D. and Tsitsiklis, J. N. _Neuro-dynamic programming_. Athena Scientific, 1996.
* Brockett [1993] Brockett, R. W. Hybrid models for motion control systems. In _Essays on Control: Perspectives in the Theory and its Applications_, pp. 29-53. Springer, 1993.
* Brockman et al. [2016] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Co-Reyes et al. [2018] Co-Reyes, J., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In _International conference on machine learning_, pp. 1009-1018. PMLR, 2018.
* Colombetti et al. [1996] Colombetti, M., Dorigo, M., and Borghi, G. Behavior analysis and training-a methodology for behavior engineering. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 26(3):365-380, 1996.
* Daniel et al. [2016] Daniel, C., Van Hoof, H., Peters, J., and Neumann, G. Probabilistic inference for determining options in reinforcement learning. _Machine Learning_, 104(2-3):337-357, 2016.
* Dayan and Hinton [1993] Dayan, P. and Hinton, G. E. Feudal reinforcement learning. _Advances in Neural Information Processing Systems_, pp. 271-278, 1993.
* Dietterich [2000] Dietterich, T. G. Hierarchical reinforcement learning with the maxq value function decomposition. _Journal of Artificial Intelligence Research_, 13:227-303, 2000.
* Eysenbach et al. [2018] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills without a reward function. _arXiv preprint arXiv:1802.06070_, 2018.
* Fujimoto et al. [2018] Fujimoto, S., Van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. _arXiv preprint arXiv:1802.09477_, 2018.
* Goyal et al. [2019] Goyal, A., Islam, R., Strouse, D., Ahmed, Z., Botvinick, M., Larochelle, H., Bengio, Y., and Levine, S. Infobot: Transfer and exploration via the information bottleneck. _arXiv preprint arXiv:1901.10902_, 2019.
* Guo et al. [2017] Guo, Z., Thomas, P. S., and Brunskill, E. Using options and covariance testing for long horizon off-policy policy evaluation. In _Advances in Neural Information Processing Systems_, pp. 2492-2501, 2017.
* Gupta et al. [2019] Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. _arXiv preprint arXiv:1910.11956_, 2019.
* Haarnoja et al. [2017] Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Reinforcement learning with deep energy-based policies. In _International Conference on Machine Learning_, pp. 1352-1361. PMLR, 2017.
* Haarnoja et al. [2018] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. _arXiv preprint arXiv:1801.01290_, 2018.

* [20] Harb, J., Bacon, P.-L., Klisarov, M., and Precup, D. When waiting is not an option: Learning options with a deliberation cost. In _Thirty-Second AAAI Conference on Artificial Intelligence_, 2018.
* [21] Harutyunyan, A., Dabney, W., Borsa, D., Heess, N., Munos, R., and Precup, D. The termination critic. _arXiv preprint arXiv:1902.09996_, 2019.
* [22] Hausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. Learning an embedding space for transferable robot skills. In _International Conference on Learning Representations_, 2018.
* [23] Kamat, A. and Precup, D. Diversity-enriched option-critic. _arXiv_, 2020.
* [24] Khetarpal, K. and Precup, D. Learning options with interest functions. In _Proceedings of the 32nd AAAI Conference on Artificial Intelligence_, pp. 1-2, 2019.
* [25] Khetarpal, K., Klisarov, M., Chevalier-Boisvert, M., Bacon, P.-L., and Precup, D. Options of interest: Temporal abstraction with interest functions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pp. 4,444-4,451, 2020.
* [26] Klissarov, M. and Precup, D. Flexible option learning. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 4632-4646. Curran Associates, 2021.
* [27] Klissarov, M., Bacon, P.-L., Harb, J., and Precup, D. Learnings options end-to-end for continuous action tasks. _arXiv preprint arXiv:1712.00004_, 2017.
* [28] Koller, D. and Friedman, N. _Probabilistic graphical models: principles and techniques_. MIT press, 2009.
* [29] Kolobov, A., Weld, D. S., et al. Discovering hidden structure in factored mdps. _Artificial Intelligence_, 189:19-47, 2012.
* [30] Konidaris, G. and Barto, A. G. Skill discovery in continuous reinforcement learning domains using skill chaining. In _Advances in neural information processing systems_, pp. 1015-1023, 2009.
* [31] Levine, S. Reinforcement learning and control as probabilistic inference: Tutorial and review. _arXiv preprint arXiv:1805.00909_, 2018.
* [32] Levy, K. Y. and Shimkin, N. Unified inter and intra options learning using policy gradient methods. In _European Workshop on Reinforcement Learning_, pp. 153-164. Springer, 2011.
* [33] Li, C., Ma, X., Zhang, C., Yang, J., Xia, L., and Zhao, Q. Soac: The soft option actor-critic architecture. _arXiv preprint arXiv:2006.14363_, 2020.
* [34] Li, C., Song, D., and Tao, D. The skill-action architecture: Learning abstract action embeddings for reinforcement learning. 2020.
* [35] Li, C., Song, D., and Tao, D. Hit-mdp: learning the smdp option framework on mdps with hidden temporal embeddings. In _The Eleventh International Conference on Learning Representations_, 2022.
* [36] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.
* [37] Osa, T., Tangkaratt, V., and Sugiyama, M. Hierarchical reinforcement learning via advantage-weighted information maximization. _arXiv preprint arXiv:1901.01365_, 2019.
* [38] Pertsch, K., Rybkin, O., Ebert, F., Finn, C., Jayaraman, D., and Levine, S. Long-horizon visual planning with goal-conditioned hierarchical predictors. _NeurIPS_, 2020.
* [39] Precup, D. _Temporal abstraction in reinforcement learning_. University of Massachusetts Amherst, 2000.

* Schulman et al. [2017] Schulman, J., Chen, X., and Abbeel, P. Equivalence between policy gradients and soft q-learning. _arXiv preprint arXiv:1704.06440_, 2017.
* Schulman et al. [2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Sharma et al. [2019] Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. Dynamics-aware unsupervised discovery of skills. _arXiv preprint arXiv:1907.01657_, 2019.
* Shiarlis et al. [2018] Shiarlis, K., Wulfmeier, M., Salter, S., Whiteson, S., and Posner, I. Taco: Learning task decomposition via temporal alignment for control. In _International Conference on Machine Learning_, pp. 4654-4663. PMLR, 2018.
* Silver et al. [2016] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. _Nature_, 529(7587):484-489, 2016.
* Smith et al. [2018] Smith, M., Hoof, H., and Pineau, J. An inference-based policy gradient method for learning options. In _International Conference on Machine Learning_, pp. 4,703-4,712, 2018.
* Sutton and Barto [2018] Sutton, R. S. and Barto, A. G. _Reinforcement learning: An introduction_. MIT press, 2018.
* Sutton et al. [1999] Sutton, R. S., Precup, D., and Singh, S. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. _Artificial Intelligence_, 112(1-2):181-211, 1999.
* Todorov [2006] Todorov, E. Linearly-solvable markov decision problems. _Advances in neural information processing systems_, 19, 2006.
* Todorov et al. [2012] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pp. 5026-5033. IEEE, 2012.
* Wulfmeier et al. [2020] Wulfmeier, M., Rao, D., Hafner, R., Lampe, T., Abdolmaleki, A., Hertweck, T., Neunert, M., Tirumala, D., Siegel, N., Heess, N., et al. Data-efficient hindsight off-policy option learning. _arXiv preprint arXiv:2007.15588_, 2020.
* Zhang et al. [2022] Zhang, D., Courville, A., Bengio, Y., Zheng, Q., Zhang, A., and Chen, R. T. Latent state marginalization as a low-cost approach for improving exploration. _arXiv preprint arXiv:2210.00999_, 2022.
* Zhang and Whiteson [2019] Zhang, S. and Whiteson, S. DAC: The double actor-critic architecture for learning options. In _Advances in Neural Information Processing Systems_, pp. 2,012-2,022, 2019.
* Ziebart et al. [2010] Ziebart, B. D., Bagnell, J. A., and Dey, A. K. Modeling interaction via the principle of maximum causal entropy. In _ICML_, 2010.

## Appendix A Proofs

### Theorem 1

Theorem 1 (Convergence Theorem for Structured Variational Policy Iteration). _Let \(\tau\) be the latent variable and \(\mathcal{E}\) be the observed variable. Define the variational distribution \(q(\tau)\) and the log-likelihood \(\log P(\mathcal{E})\). Let \(M:q^{[k]}\to q^{[k+1]}\) represent the mapping defined by the EM steps inference update, so that \(q^{[k+1]}=M(q^{[k]})\). The likelihood function increases at each iteration of the variational inference algorithm until the conditions for equality are satisfied and a fixed point of the iteration is reached:_

\[\log P(\mathcal{E}\mid q^{[k+1]})\geq\log P(\mathcal{E}\mid q^{[k]}),\text{ with equality if and only if}\]

\[\mathcal{L}(q^{[k+1]},P)=\mathcal{L}(q^{[k]},P)\]

_and_

\[D_{\text{KL}}(q^{[k+1]}(\tau)\parallel P(\tau\mid\mathcal{E}))=D_{\text{KL}} (q^{[k]}(\tau)\parallel P(\tau\mid\mathcal{E})).\]Proof.: Let \(\tau\) be the latent variable and \(\mathcal{E}\) be the observed variable. Define the evidence lower bound (ELBO) as \(\mathcal{L}(q,P)\) and the Kullback-Leibler divergence as \(\text{D}_{\text{KL}}(q\parallel P)\), where \(q(\tau)\) approximates the posterior distribution and \(P(\mathcal{E}\mid\tau)\) is the likelihood.

The log-likelihood function \(\log P(\mathcal{E})\) can be decomposed as:

\[\log P(\mathcal{E})=\mathcal{L}(q,P)+\text{D}_{\text{KL}}(q(\tau)\parallel P (\tau\mid\mathcal{E})),\]

where

\[\mathcal{L}(q,P)=\mathbb{E}_{q(\tau)}\left[\log P(\mathcal{E},\tau)-\log q( \tau)\right]\]

and

\[\text{D}_{\text{KL}}(q(\tau)\parallel P(\tau\mid\mathcal{E}))=\mathbb{E}_{q( \tau)}\left[\log\frac{q(\tau)}{P(\tau\mid\mathcal{E})}\right].\]

Let \(M:q^{[k]}\to q^{[k+1]}\) represent the mapping defined by the variational inference update, so that \(q^{[k+1]}=M(q^{[k]})\). If \(q^{*}\) is a variational distribution that maximizes the ELBO, so that \(\log P(\mathcal{E}\mid q^{*})\geq\log P(\mathcal{E}\mid q)\) for all \(q\), then \(\log P(\mathcal{E}\mid M(q^{*}))=\log P(\mathcal{E}\mid q^{*})\). In other words, the maximizing distributions are fixed points of the variational inference algorithm. Since the likelihood function is bounded (for distributions of practical interest), the sequence of variational distributions \(q^{[0]},q^{[1]},\ldots,q^{[k]}\) yields a bounded nondecreasing sequence \(\log P(\mathcal{E}\mid q^{[0]})\leq\log P(\mathcal{E}\mid q^{[1]})\leq\cdots \leq\log P(\mathcal{E}\mid q^{[k]})\leq\log P(\mathcal{E}\mid q^{[k]})\) which must converge as \(k\rightarrow\infty\).

### Theorem 2

Theorem 2 (Soft Option Policy Iteration Theorem).: _Repeated optimizing \(\mathcal{L}\) and \(D_{\text{KL}}\) defined in Eq. 10 from any \(\pi_{0}^{A},\pi_{0}^{O}\in\Pi\) converges to optimal policies \(\pi^{A*},\pi^{O*}\) such that \(Q_{O}^{soft*}[\mathbf{s}_{t},\mathbf{o}_{t}]\geq Q_{O}^{soft}[\mathbf{s}_{t}, \mathbf{o}_{t}]\) and \(Q_{A}^{soft*}[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]\geq Q_{A}^{soft}[ \mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]\), for all \(\pi_{0}^{A},\pi_{0}^{O}\in\Pi\) and \((\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t})\in\mathcal{S}\times\mathcal{A} \times\mathcal{O}\), assuming \(|\mathcal{S}|<\infty,\;|\mathcal{O}|<\infty,\;|\mathcal{A}|<\infty\)._

Proof.: Define the entropy augmented reward as \(r^{soft}(\mathbf{s}_{t},\mathbf{a}_{t})=r(\mathbf{s}_{t},\mathbf{a}_{t})+ \mathcal{H}[\pi^{A}]\) and \(f^{soft}(\mathbf{o}_{t},\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t-1})=f( \mathbf{o}_{t},\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t-1})+\mathcal{H}[ \pi^{O}]\) and rewrite Bellman Backup functions as,

\[Q_{O}[\mathbf{s}_{t},\mathbf{o}_{t}] =f^{soft}(\cdot)+\mathbb{E}_{\mathbf{a}_{t}\sim\pi^{A}}\left[Q_{A }[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]\right],\] \[Q_{A}[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}] =r^{soft}(s,a)+\mathbb{E}_{\mathbf{s}_{t+1}\sim P(\mathbf{s}_{t+1} |\mathbf{s}_{t},\mathbf{a}_{t})}\left[\mathbb{E}_{\mathbf{o}_{t+1}\sim\pi^{O} }\left[Q_{O}[\mathbf{s}_{t+1},\mathbf{o}_{t+1}]\right]\right]\]

We start with proving the convergence of soft option policy evaluation. As with the standard Q-function and value function, we can relate the Q-function at a future state via a _Bellman Operator_\(\mathcal{T}^{soft}\). The option-action value function satisfies the Bellman Operator \(\mathcal{T}^{soft}\)

\[\mathcal{T}^{soft}Q_{A}[\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}] =\mathbb{E}[G_{t}|\mathbf{s}_{t},\mathbf{o}_{t},\mathbf{a}_{t}]\] \[=r^{soft}(s,a)+\gamma\sum_{\mathbf{s}_{t+1}}P(\mathbf{s}_{t+1}| \mathbf{s}_{t},\mathbf{a}_{t})Q_{O}[\mathbf{s}_{t+1},\mathbf{o}_{t}],\]

As with the standard convergence results for policy evaluation [46], by the definition of \(\mathcal{T}^{soft}\) (Eq. 11) the option-action value function \(Q_{A}^{\pi_{A}}\) is a fixed point.

To prove the \(\mathcal{T}^{soft}\) is a contraction, define a norm on \(V\)-values functions \(V\) and \(U\)

\[\|V-U\|_{\infty}\triangleq\max_{\bar{s}\in S}|V(\bar{s})-U(\bar{s})|.\] (11)

where \(\bar{s}=\{s,o\}\).

By recurssively apply the Hidden Temporal Bellman Operator \(\mathcal{T}^{soft}\), we have:\[Q_{O}[\mathbf{s}_{t},\mathbf{o}_{t-1}] =\mathbb{E}[G_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1}]=\sum_{\mathbf{o} _{t}}P(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1})Q_{O}[\mathbf{s}_{t}, \mathbf{o}_{t}]\] \[=\sum_{\mathbf{o}_{t}}P(\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_ {t-1})\sum_{\mathbf{a}_{t}}P(\mathbf{a}_{t}|\mathbf{s}_{t},\mathbf{o}_{t}) \bigg{[}r(s,a)+\gamma\sum_{\mathbf{s}_{t+1}}P(\mathbf{s}_{t+1}|\mathbf{s}_{t}, \mathbf{a}_{t})Q_{O}[\mathbf{s}_{t+1},\mathbf{o}_{t}]\bigg{]}\] \[=r(s,a)+\gamma\sum_{\mathbf{o}_{t},\mathbf{s}_{t+1}}P(\mathbf{s}_ {t+1},\mathbf{o}_{t}|\mathbf{s}_{t},\mathbf{o}_{t-1})Q_{O}[\mathbf{s}_{t+1}, \mathbf{o}_{t}]\] \[=r(s,a)+\gamma E_{\mathbf{s}_{t+1},\mathbf{o}_{t}}\bigg{[}Q_{O}[ \mathbf{s}_{t+1},\mathbf{o}_{t}]\bigg{]}\] (12)

Therefore, by applying Eq. 12 to \(V\) and \(U\) we have:

\[\|T^{\pi}V-T^{\pi}U\|_{\infty}\] \[=\gamma\max_{\tilde{s}\in S}E_{\mathbf{s}_{t+1},\mathbf{o}_{t}} \bigg{[}Q_{O}[\mathbf{s}_{t+1},\mathbf{o}_{t}]-U[\mathbf{s}_{t+1},\mathbf{o}_{ t}]\bigg{]}\] \[\leq\gamma\max_{\tilde{s}\in S}E_{\mathbf{s}_{t+1},\mathbf{o}_{t} }\bigg{[}\gamma\max_{\tilde{s}\in S}\bigg{|}Q_{O}[\mathbf{s}_{t+1},\mathbf{o}_ {t}]-U[\mathbf{s}_{t+1},\mathbf{o}_{t}]\bigg{|}\bigg{]}\] \[\leq\gamma\max_{\tilde{s}\in S}|V[\tilde{s}]-U[\tilde{s}]|\] \[=\gamma\|V-U\|_{\infty}\] (13)

Therefore, \(\mathcal{T}^{soft}\) is a contraction. By the fixed point theorem, assuming that throughout our computation the \(Q_{A}[\cdot,\cdot]\) and \(Q_{O}[\cdot]\) are bounded and \(\mathbb{A}<\infty\), the sequence \(Q_{A}^{k}\) defined by \(Q_{A}^{k+1}=\mathcal{T}^{soft}Q_{A}^{k}\) will converge to the option-action value function \(Q_{A}^{\pi_{A}}\) as \(k\to\infty\).

The convergence results of and the Soft Option Policy Improvement Theorem then follows conventional Soft Policy Improvement Theorem 1. Consequently, the Soft Option Policy Iteration Theorem follows directly from these results.

### Derivation of Eq. 10

\[\mathcal{L}(q(\tau),P(\tau,\mathcal{E}_{1:T}^{A},\mathcal{E}_{1:T} ^{O})) =\mathbb{E}_{q(\tau)}[\log P(\tau,\mathcal{E}_{1:T}^{A},\mathcal{E }_{1:T}^{O})-\log q(\tau)]\] \[=\mathbb{E}_{q(\tau)}[\log P(\tau|\mathcal{E}_{1:T}^{A},\mathcal{E }_{1:T}^{O})+\log P(\mathcal{E}_{1:T}^{A},\mathcal{E}_{1:T}^{O})-\log q(\tau)]\] \[=\mathbb{E}_{q(\tau)}[\log P(\tau|\mathcal{E}_{1:T}^{A},\mathcal{E }_{1:T}^{O})-\log q(\tau)]+\mathbb{E}_{q(\tau)}\log P(\mathcal{E}_{1:T}^{A}, \mathcal{E}_{1:T}^{O})\] \[=\mathbb{E}_{q(\tau)}[\frac{\log P(\tau|\mathcal{E}_{1:T}^{A}, \mathcal{E}_{1:T}^{O})}{\log q(\tau)}]+\log P(\mathcal{E}_{1:T}^{A},\mathcal{E }_{1:T}^{O})\] \[=-D_{\mathrm{KL}}(\log q(\tau)\parallel\log P(\tau|\mathcal{E}_{1: T}^{A},\mathcal{E}_{1:T}^{O}))+\log P(\mathcal{E}_{1:T}^{A},\mathcal{E}_{1:T}^{O})\]

### Theorem 3

**Theorem 3** (Convergence Theorem for Variational Markovian Option Policy Iteration).: _Let \(\tau\) be the latent variable and \(\mathcal{E}^{A},\mathcal{E}^{O}\) be the ground-truth optimality variables. Define the variational distribution \(q(\tau)\) and the true log-likelihood of optimality \(\log P(\mathcal{E}^{A},\mathcal{E}^{O})\). iterates according to the update rule \(q^{k+1}=\operatorname*{arg\,max}_{q}\mathcal{L}(q(\tau),P(\tau,\mathcal{E}_{1: T}^{A},\mathcal{E}_{1:T}^{O}))\) converges to the maximum value bounded by the data log-likelihood._Proof.: The objective is to maximize the ELBO with respect to the policy \(q\). Formally, this can be written as:

\[q^{k+1}=\arg\max_{q}\mathcal{L}(q,P).\]

Suppose we \(q\) is a neural network function approximator, assuming the continuity and differentiability of \(q\) with respect to its parameters. Using stochastic gradient descent (SGD) to optimize the parameters guarantees that the ELBO increases, such that \(\mathcal{L}(q^{k+1},P)\geq\mathcal{L}(q^{k},P)\).

Rearranging Eq. 10 we get:

\[D_{\mathrm{KL}}(q^{k+1}(\tau)||P(\tau|\mathcal{E}^{A}_{1:T}, \mathcal{E}^{O}_{1:T})) =-L(q^{k+1}(\tau),P(\tau,\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1 :T}))+\log P(\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1:T})\] \[\leq-L(q^{k}(\tau),P(\tau,\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{ 1:T}))+\log P(\mathcal{E}^{A}_{1:T},\mathcal{E}^{O}_{1:T})\] \[=D_{\mathrm{KL}}(q^{k}(\tau)||P(\tau|\mathcal{E}^{A}_{1:T}, \mathcal{E}^{O}_{1:T}))\]

Thus, each SGD update not only potentially increases the ELBO but also decreases the KL divergence, moving \(q\) closer to \(P\). Given the properties of SGD and assuming appropriate learning rates and sufficiently expressive neural network architectures, the sequence \(\{q^{k}\}\) converges to a policy \(q^{*}\) that minimizes the KL divergence to the true posterior. 

## Appendix B VMOC Algorithm

```
1:Initialize parameter vectors \(\psi^{A}\), \(\psi^{O}\), \(\theta^{O}\), \(\theta^{A}\)
2:for each epoch do
3: Collect trajectories \(\{\mathbf{o}_{t-1},\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{o}_{t}\}\) into the replay buffer
4:for each gradient step do
5: Update \(Q^{soft}_{\psi^{A}_{i}}\): \(\psi^{A}_{i}\leftarrow\psi^{A}_{i}-\eta_{Q^{A}}\nabla J_{Q^{soft}_{\psi^{A}_{i}}}\) for \(i\in\{1,2\}\)
6: Update \(Q^{soft}_{\psi^{O}_{i}}\): \(\psi^{O}_{i}\leftarrow\psi^{O}_{i}-\eta_{Q^{O}}\nabla J_{Q^{soft}_{\psi^{O}_{i} }}\) for \(i\in\{1,2\}\)
7: Update \(\pi^{o}_{\theta^{O}}\): \(\theta^{O}\leftarrow\theta^{O}-\eta_{\pi^{O}}\nabla J_{\pi^{O}}\)
8: Update \(\pi^{A}_{\theta^{A}}\): \(\theta^{A}\leftarrow\theta^{A}-\eta_{\pi^{A}}\nabla J_{\pi^{A}}\)
9: Update target networks: \(\bar{\psi}^{A}\leftarrow\sigma\psi^{A}+(1-\sigma)\bar{\psi}^{A}\), \(\bar{\psi}^{O}\leftarrow\sigma\psi^{O}+(1-\sigma)\bar{\psi}^{O}\)
10: Update temperature factors: \(\alpha^{O}\leftarrow\alpha^{O}-\eta_{\alpha^{O}}\nabla J_{\alpha^{O}}\), \(\alpha^{A}\leftarrow\alpha^{A}-\eta_{\alpha^{A}}\nabla J_{\alpha^{A}}\)
11:endfor
12:endfor ```

**Algorithm 1** VMOC Algorithm

## Appendix C Implementation Details

### Hyperparameters

In this section we summarize our implementation details. For a fair comparison, all baselines: MOPG [35], DAC+PPO [52], AHP+PPO [32], PPOC [27], OC [4] and PPO [41] are from DAC's open source Github repo: https://github.com/ShangtonGZhang/DeepRL/tree/DAC. Hyperparameters used in DAC [52] for all these baselines are kept unchanged.

**VMOC Network Architecture:** We use Pytorch to build neural networks. Specifically, for option embeddings, we use an embedding matrix \(\bm{W}_{S}\in\mathbb{R}^{4\times 40}\) which has \(4\) options (\(4\) rows) and an embedding size of \(40\) (\(40\) columns). For layer normalization we use Pytorch's built-in function LayerNorm 2. For Feed Forward Networks (FNN), we use a 2 layer FNN with ReLu function as activation function with input size of state-size, hidden size of \([256,256]\), and output size of action-dim neurons. For Linear layer, we use built-in Linear function3 to map FFN's outputs to \(4\) dimension.

Each dimension acts like a logit for each skill and is used as density in Categorical distribution4. For both action policy and critic module, FFNs are of the same size as the one used in the skill policy.

Footnote 4: https://github.com/pytorch/pytorch/blob/master/torch/distributions/categorical.py

**Preprocessing:** States are normalized by a running estimation of mean and std.

**Hyperparameters for all on-policy option variants:** For a fair comparison, we use exactly the same parameters of PPO as DAC. Specifically:

* Optimizer: Adam with \(\epsilon=10^{-5}\) and an initial learning rate \(3\times 10^{-4}\)
* Discount ratio \(\gamma\): \(0.99\)
* GAE coefficient: \(0.95\)
* Gradient clip by norm: \(0.5\)
* Rollout length: \(2048\) environment steps
* Optimization epochs: \(10\)
* Optimization batch size: \(64\)
* Action probability ratio clip: \(0.2\)

**Computing Infrastructure:** We conducted our experiments on an Intel(r) Core(tm) i9-9900X CPU @ 3.50GHz with a single thread and process with PyTorch.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately reflect the claims and findings of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations of the study are discussed in the discussion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper provides a full derivation of assumptions and proofs of the theoretical result (convergence of the evidence lower bound)

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes. Our code is provided in supplementary materials. Full details of the experimental setup, model architectures are provided. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provides open access to the code and data. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Justification: All details are provided in the main content and the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All gym env experiments are run with 10 different random seeds. Performance are reported by 1 sigma shaded area over all 10 runs. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Computational details are provided in the Appendix. Guidelines: The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research was conducted in accordance with the NeurIPs Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work in the paper has no potential for societal impacts. Guidelines: The answer NA means that there is no societal impact of the work performed.
11. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
12. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The only applicable assets are the code which are credited and distributed under a Creative Commons Attribution License. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: New assets include the code required to run the experiments described in the paper. Documentation is provided along with the code. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.