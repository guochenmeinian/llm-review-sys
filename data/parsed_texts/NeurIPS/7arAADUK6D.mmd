# Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration

Yichong Huang\({}^{\dagger}\), Xiaocheng Feng\({}^{\ddagger}\)\({}^{\ddagger}\)\({}^{\tt{less}}\), Baohang Li\({}^{\dagger}\), Yang Xiang\({}^{\ddagger}\), Hui Wang\({}^{\ddagger}\)

**Ting Liu\({}^{\dagger}\), Bing Qin\({}^{\dagger}\)\({}^{\ddagger}\)**

\({}^{\dagger}\)Harbin Institute of Technology

\({}^{\ddagger}\) Peng Cheng Laboratory

{ychuang,xcfeng,baohangli,tliu,qinb}@ir.hit.edu.cn

{xiangy,wangh06}@ir.hit.edu.cn

###### Abstract

Large language models (LLMs) exhibit complementary strengths in various tasks, motivating the research of LLM ensembling. However, existing work focuses on training an extra reward model or fusion model to select or combine all candidate answers, posing a great challenge to the generalization on unseen data distributions. Besides, prior methods use textual responses as communication media, ignoring the valuable information in the internal representations. In this work, we propose a training-free ensemble framework DeePEn, fusing the informative probability distributions yielded by different LLMs at each decoding step. Unfortunately, the vocabulary discrepancy between heterogeneous LLMs directly makes averaging the distributions unfeasible due to the token misalignment. To address this challenge, DeePEn maps the probability distribution of each model from its own probability space to a universal _relative space_ based on the relative representation theory, and performs aggregation. Next, we devise a search-based inverse transformation to transform the aggregated result back to the probability space of one of the ensembling LLMs (main model), in order to determine the next token. We conduct extensive experiments on ensembles of different number of LLMs, ensembles of LLMs with different architectures, and ensembles between the LLM and the specialist model. Experimental results show that (i) DeePEn achieves consistent improvements across six benchmarks covering subject examination, reasoning, and knowledge, (ii) a well-performing specialist model can benefit from a less effective LLM through distribution fusion, and (iii) DeePEn has complementary strengths with other ensemble methods such as voting1.

Footnote 1: Our code is available at: [https://github.com/OrangeInSouth/DeePEn](https://github.com/OrangeInSouth/DeePEn)

## 1 Introduction

With the scaling of model capacities and data volumes, generative large language models (LLMs) have shown impressive language understanding and generation abilities, shedding light for artificial general intelligence [35, 22, 13, 28]. Due to diversities of data sources, model architectures, and training recipes, LLMs have different strengths and weaknesses in various tasks and cases. Therefore, recent research has explored the ensemble of LLMs to exploit the complementary potential [15, 19].

Existing methods can be categorized into selection-based and fusion-based ensembling. Selection-based ensembling selects the best candidate answer from all individual LLMs' answers using an additionally trained reward model [15, 31, 25, 19]. Fusion-based ensembling combines all candidate answers using a trained fusion model [15]. However, these approaches inevitably face significantchallenges in generalizing to unseen data distributions and base models. Besides, prior methods enable collaboration via conveying the textual responses between LLMs while ignoring the rich information (_e.g.,_ confidence and alternative answers) in the internal representations.

An ideal solution to this issue is to apply the well-established technology of prediction fusion. [36; 24; 7; 10]. For LLM ensemble, prediction fusion works at each decoding step, averaging the probability distributions from different LLMs to determine the next token. It could not only directly apply to the ensemble of any LLMs without extra parameter training, making it more general, but leverages the informative internal representations (_i.e.,_ probability distributions) as communication media. Unfortunately, the vocabulary discrepancy between different LLMs makes it unfeasible to average the distributions due to token misalignment.

In this work, we tackle this key challenge by drawing upon the cross-model invariance of relative representation, which represents each token using the embedding similarities of this token to a set of anchor tokens [21]. Specifically, we propose an ensemble framework **DeeP**EN (**DeeP**arallel **E**nsemble), enabling distribution fusion for heterogeneous LLMs. **DeeP**EN transforms the probability distribution from the heterogeneous probability space to a homogeneous relative space, using a matrix formed by the relative representation of all tokens. Next, **DeeP**EN aggregates the relative representations of all probability distributions in the relative space, coordinating the decision on the next token. Finally, the result of aggregation is transformed back to the probability space of the main model using a search-based inverse transformation to determine the next token.

We conduct extensive experiments ranging from 2-model to 9-model ensembles, covering ensembles of models with parameters ranging from 6B to 70B, ensembles of dense and sparse models, and the ensemble of LLMs with specialist models. Experimental results on six widely-used benchmarks demonstrate that compared to baselines, **DeeP**EN achieves consistent improvements across all benchmarks. It is also discovered that **DeeP**EN has complementary strengths when combined with other ensemble methods.

## 2 Theoretical Analysis

We first introduce relative representation and then illustrate the theoretical support for our method.

### Relative Representation

Previous study discovers that despite the misalignment between latent spaces of different neural networks, the embedding similarity between samples do not change across models [21; 11; 23]. Specifically, Moschella et al. [21] propose relative representation, which represents each sample \(x^{(i)}\) by the embedding similarities to a set of anchor samples \(\mathbb{A}\) (\(x^{(i)}\) and \(\mathbb{A}\) are identically distributed):

\[\mathbf{r}_{x^{(i)}}=(cos(e_{x^{(i)}},e_{a^{(1)}}),...,cos(e_{x^{(i)}},e_{a^{( |\mathbb{A}|)}})), \tag{1}\]

where \(e_{(*)}\) denotes the embedding of samples, also is absolute representation.

It is empirically evidenced that relative representations possess cross-model invariance, _i.e.,_ the relative representation of the same sample keeps invariant across different models, which lays the theoretical foundation for our work to fuse heterogeneous probability distributions.

### Theoretical Support for **DeeP**en

Average probability distribution has been widely evidenced to effectively improve the predictive performance in the filed of image and text [2; 10]. For generative language models, as we understand, the underlying mechanism is to interpolate different output semantics represented by the probability distributions. However, for LLM ensemble, vocabulary discrepancy isolates these output semantics in semantic spaces with different basis vectors, making the interpolation infeasible. To tackle this challenge, we aim to enable the cross-model alignment for output semantics, _i.e.,_ find a transformation to map the output semantics into a universal space. To this effect, we propose to represent the output semantics with the convex combination of relative representations of all tokens where the weight is the probability assigned to the token.

Definition of output semantics in relative space.Formally, given the absolute representation of the output semantics \(\mathbf{p}\) and the relative representation matrix \(R\in\mathbb{R}^{|V|\times|A|}\) where \(V\) is the vocabulary and \(A\subseteq V\) is the anchor token set. The \(i\)-th row of \(R\) is the relative representation of word \(w^{(i)}\):

\[R[i]=(cos(e_{w^{(i)}},e_{a^{(1)}}),...,cos(e_{w^{(i)}},e_{a^{(|A|)}})), \tag{2}\]

and the relative representation of the output semantics \(\mathbf{p}\) is defined as: \(\mathbf{r}=\mathbf{p}\cdot R\).

Model-invariance of relative representation of output semantic.Next, we illustrate why this representation scheme could align the output semantics isolated in heterogeneous absolute spaces. First, considering two LLMs \(\theta_{A}\) and \(\theta_{B}\) with the same vocabulary (_e.g.,_ LLaMA2-7B and LLaMA2-13B). When expressing the same output semantic, these models output the same probability distribution (_i.e.,_ absolute representation) \(\mathbf{p}_{A}\) and \(\mathbf{p}_{B}\). Besides, they have the same (highly similar in practice) relative representation matrix due the vocabulary consistency and cross-model invariance of relative representation. Therefore, the relative representations of output semantics are also identical:

\[\mathbf{r}_{A}=\mathbf{p}_{A}\cdot R_{A}=\mathbf{p}_{B}\cdot R_{B}=\mathbf{r}_ {B}. \tag{3}\]

Then, let's consider a language model \(\theta_{C}\) with a different vocabulary (_e.g.,_ Mistral). Based on the fact that different LLMs typically share mass tokens in their vocabularies (SSA), the vocabulary of model \(\theta_{C}\) is identical to adding and removing partial tokens to the vocabulary of \(\theta_{B}\), which leads to \(\mathbf{p}_{B}\not\cong\mathbf{p}_{C}\) and \(R_{B}\not\cong R_{C}\). However, in our study, we discover that this change to the vocabulary has not incurred significant influence on the relative representation of the unchanged tokens (_i.e.,_ the common tokens between \(\theta_{B}\) and \(\theta_{C}\)), as shown in Fig. 1. Therefore, we make the reasonable assumption that the local change in the vocabulary could hardly influence the relative space.

## 3 Methodology

In this section, we first introduce the overall process of our ensemble framework DeepPen and then describe the three parts of DeepPen in detail.

### Overview

We illustrate the process of DeepPen in Fig. 2. Given \(N\) models to ensemble, DeepPen first constructs their transformation matrices (_i.e.,_ relative representation matrices) mapping the probability distributions from the heterogeneous absolute spaces into the relative space (SS3.2). At each decoding step, all models perform prediction and output \(N\) probability distributions. These distributions are mapped into the relative space and aggregated (SS3.3). Finally, the aggregation result is transformed back into the absolute space of the main model, in order to determine the next token (SS3.4).

Figure 1: Visualizations for relative representations between models with the same vocabulary and between models with different vocabularies. PCA and K-means clustering are applied only for visualization. Different colors indicate different clusters of samples (word embeddings). The red block indicates the representation of tokens that only appear in Mistral’s vocabulary. Relative representation consistency is obtained by calculating the cosine similarity between the relative representations of the same token in different models.

### Construction of Relative Transformation

Given \(N\) models to ensemble, DeePen first finds out the intersection of vocabularies of all models, _i.e.,_ common token set \(C\), and samples a subset or uses the full set of common tokens as the anchor token set \(A\subseteq C\). Next, for each model, DeePen calculates embedding similarities of each token to the anchor words, obtaining the relative representation matrix \(R\) (as shown in Eq.2). Finally, to overcome the relative representation degeneration of outlier words, which will be introduced later, we perform normalization on the relative representation of all tokens by a softmax operation so that it becomes a probability distribution. We denote the normalized representation matrix \(\hat{R}\):

\[\hat{R}[i]=softmax(R[i]). \tag{4}\]

Anchor Selection.The choice of anchor tokens is crucial for the relative representation capability. Previous research discovers that the capability improves as the number of anchor words increases [21]. Therefore, we employ the full set of common words between LLMs as the anchor words. It is also empirically proved that this method performs more stablely on downstream tasks (SS5.2).

Normalization of relative representation matrix.In DeePen, the relative representation of each token is normalized by the softmax operation to avoid the relative representation degeneration of outlier words, which are referred to as words that are far away from other words (including the anchors) and become distinguishable in relative space since for being zero vectors. The softmax operation effectively resolves this problem by making each relative representation a probabilistic distribution instead of a zero vector.

### Aggregation in Relative Space

At each decoding step, once each model \(\theta_{i}\) outputs the probability distribution \(\textbf{p}_{i}\), DeePen transforms \(\textbf{p}_{i}\) into the relative representation \(\textbf{r}_{i}\) using the normalized relative representation matrix: \(\textbf{r}_{i}=\textbf{p}_{i}\cdot\hat{R}_{i}\), and aggregate all relative representations to obtain the aggregated relative representation:

\[\overline{\textbf{r}}=\sum_{i=1}^{N}\alpha_{i}\times\textbf{r}_{i}, \tag{5}\]

where \(\alpha_{i}\) is the collaboration weight of model \(\theta_{i}\).

Collaboration Weight.As our work focuses on enabling the distribution fusion of heterogeneous LLMs instead of finding the optimal collaboration weights, we follow the most common practice to uniformly aggregate the distributions (\(\alpha=1/N\), \(N\) is the number of models), which is named **DeePen-Avg.** Besides, we also adopt a simple and effective method of deducing weights, **DeePen-Adapt**, which heuristically sets a larger value to the model with a better performance on the development set: \(\alpha_{i}=s_{i}/\sum_{j}s_{j}\), where \(s_{i}=Acc(\theta_{i},\mathcal{D}^{dev})-\epsilon\), \(Acc(\cdot,\cdot)\) indicates the average accuracy of model \(\theta_{i}\) on the development set, and \(\epsilon\) indicates the chance level on the evaluation task. Specifically, \(\epsilon=0\) on the free-form generation tasks and \(\epsilon=1/K\) on the \(K\)-choice tasks.

Figure 2: Overview of DeePen. The relative representation matrix of each LLM is directly derived by calculating the embedding similarities between each token with the anchor tokens.

### Inverse Transformation of Relative Representations

To decide the next token according to the aggregated relative representation, DeepEn aims to transform it from the relative space back to the absolute space of the main model, which is empirically selected with the best-performing model on the development set. To enable this inverse transformation, we adopt a search-based strategy, finding out the absolute representation whose relative representation is identical to the aggregated relative representation. This search problem is formulated as:

\[\mathbf{\overline{p}}_{i}=\operatorname*{arg\,min}_{\mathbf{p}_{i}\in\mathbb{P} _{i}}\ell(\mathbf{p}_{i}\times\hat{R},\ \overline{r}), \tag{6}\]

where \(\mathbb{P}_{i}\) denotes the absolute space of model \(\theta_{i}\), and \(\ell(\cdot)\) is the loss function to measure the distance between relative representations. In this work, we adopt the KL-divergence due to its convergence.

This search is iteratively conducted under the guidance of the gradient of the loss in Eq.6 with respect to the absolute representation \(\mathbf{p}_{i}\). Specifically, we initialize the start point of searching \(\mathbf{p}_{i}^{(0)}\) with the main model's original absolute representation, and update it as:

\[\mathbf{p}_{i}^{(t+1)}=\mathbf{p}_{i}^{(t)}-\eta\times\frac{\partial\ell}{ \partial\mathbf{p}_{i}^{(t)}},t\in[0,T] \tag{7}\]

where \(\eta\) is an important hyperparameter named the relative ensemble learning rate, and \(T\) is the iterations number named relative ensemble learning steps. Finally, we use the updated absolute representation \(\mathbf{p}_{i}^{(T)}\) to determine the emitted token.

## 4 Experiments

### Experimental Setup

Benchmarks.We mainly conduct experiments on six benchmarks, which can be categorized into:

* **Comprehensive Examination:** (1) MMLU (5-shot) [12], which covers 57 subjects that humans learn, and (2) ARC-C (0-shot) [5], collected from standardized natural science tests.
* **Reasoning Capabilities:** (1) GSM8K [6] (4-shot), which is a dataset of high quality problems at the grade school math level, and (2) PIQA [3] (0-shot), which is a commonsense reasoning dataset.
* **Knowledge Capacities:** (1) TriviaQA (5-shot) [16], collected by Trivia enthusiast authored, and (2) NQ (5-shot) [18], which is a QA corpus consists of queries issued to the Google search engine.

Evaluation.For all benchmarks, we follow the test scripts of OpenCompass leaderboard. Specifically, on the multiple-choice tasks (MMLU, ARC-C, and PIQA), the option with the highest likelihood is selected to calculate the accuracy. On the free-form generation tasks (GSM8K, TriviaQA and NQ), we calculate the exact match (EM) accuracy.

Individual models.As ensemble learning typically works on models with comparable performance [24; 34], we select six well-performing LLMs whose performance are closely matched: LLaMA-2-13B [29], Mistral-7B-v0.1 [13], InternLM-20B [26], Yi-6B [1], Skywork-13B-base [32], and Tigerbot-13b-base-v2 [4]. To achieve better ensemble performance, we conduct experiments on the ensemble of the top-2 models and the top-4 models for each benchmark. Besides, we also consider ensembling various number of models (SS4.3) and ensembling more diverse models (SS5.1).

Hyperparameters.In this work, we select all of the common tokens between LLMs as the anchor tokens to build the relative spaces, _i.e.,_\(A=C\) (SS5.2). For the inverse transformation of relative representations, we search the optimal relative learning rate (\(\eta\) in Eq. 7) from 0.05 to 0.30 with an interval of 0.05. We empirically set the number of relative ensemble learning steps \(T=5\) (SS5.3).

Comparative methods.We compare DeepEn with (1) **MinED**[30; 9], which maps the probability distributions of heterogeneous LLMs to the distribution of the main model via aligning tokens in different vocabularies with edit distance, and (2) **LLM-Blender**[15], which comprises a reward model PairRanker to score each response of LLMs and a fusion model GenFuser to fusecandidate responses. In this work, we we only adopt the PairRanker since GenFuser suffers from serious over-generation under our training-free setting. In the ensemble of more than two models, we introduce two additional ensemble methods: (3) **Voting**, which selects the choice favored by most models on the tasks with outputs limited to a fixed set, and (4) **MBR**[8; 17], which selects the answer with the highest textual similarity to other candidate answers. The implementation details of baselines are illustrated in SSB.

### Main Results

The main results are shown in Tab. 1, from which we have drawn the following observations:

(1) DeePEn achieves consistent improvements over the individual models.These results prove that our DeePEn successfully enables collaboration between heterogeneous LLMs via aggregating their probability distributions in the relative space. Specifically, DeePen-Avg achieves improvements of +1.43(MMLU)\(\sim\)+2.72(PIQA) on the ensemble of top-2 models, and +1.00(PIQA)\(\sim\)+2.89(ARC-C) on the ensemble of top-4 model. DeePen-Adapt gains improvements of +1.44(TriviaQA)\(\sim\)+3.34(ARC-C) on the ensemble of top-4 models.

(2) DeePEn shows better stability than baselines.As shown, LLM-Blender struggles to achieve improvements under the training-free setting. MinED shows unstable performance across different benchmarks. For example, MinED leads to performance drops of -35.40 on the GSM8K benchmark under the top-2 models ensemble setting and -2.70 on the TriviaQA, indicating the limitation of using textual similarity to align tokens in heterogeneous vocabularies. Through case studies, it is revealed that this method of aligning tokens with edit distance disturbs the decoding and produces incomplete words (demonstrated in SS7). Instead, DeePen-Avg achieves consistent improvements and surpasses all baselines in 7/12 settings.

(3) DeePEn has complementary strengths with other ensemble methods.Voting achieves a significant improvement on the mathematical reasoning GSM8K, showing the effectiveness of

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & \multicolumn{2}{c}{**Examination**} & \multicolumn{2}{c}{**Reasoning**} & \multicolumn{2}{c}{**Knowledge**} \\ \cline{2-7}  & **MMLU** & **ARC-C** & **GSM8K** & **PIQA** & **TriviaQA** & **NQ** \\ \hline \hline \multicolumn{7}{c}{_Individual Models_} \\ \hline LLaMA2-13B & 55.07 & 59.32 & 29.80 & 59.68 & 74.32 & 28.67 \\ InternLM-20B & 59.94 & 75.81 & 53.83 & 64.78 & 66.88 & 26.09 \\ Skywork-13B & 61.16 & 66.50 & 53.90 & 74.04 & 58.65 & 19.75 \\ Tigerbot-13B & 51.95 & 57.44 & 48.82 & 68.28 & 66.22 & 22.71 \\ Mistral-7B & 62.13 & 73.33 & 47.50 & 65.61 & 73.18 & 27.62 \\ Yi-6B & 63.25 & 73.33 & 37.91 & 76.15 & 59.02 & 18.98 \\ \hline \hline \multicolumn{7}{c}{_Top-2 Ensemble_} \\ \hline LLM-Blender & 63.85 (+0.60) & 75.73 ( - 0.08) & 54.89 (+0.99) & 78.31 (+2.16) & 74.10 ( - 0.22) & 28.61 ( - 0.06) \\ MinED & 65.04 (+1.79) & 77.35 (+1.54) & 18.50 (-35.40) & 78.98 (+2.83) & 72.30 ( - 2.02) & 28.45 ( - 0.22) \\
**DeePEn-Avg** & 64.68 (+1.43) & 77.52 (+1.71) & 55.42 (+1.52) & 78.87 (+2.72) & 75.90 (+1.58) & 30.17 (+1.50) \\
**DeePEn-Adapt** & 65.01 (+1.76) & 77.52 (+1.71) & 55.65 (+1.75) & 79.37 (+3.22) & 76.08 (+1.76) & 30.69 (+2.02) \\ \hline \hline \multicolumn{7}{c}{_Top-4 Ensemble_} \\ \hline LLM-Blender & 61.44 ( - 1.81) & 71.03 ( - 4.78) & 43.37(-10.53) & 71.16 ( - 4.99) & 67.87 ( - 6.45) & 24.18 ( - 4.49) \\ Voting & 64.88 (+1.63) & 78.41 (+2.60) & 63.15 (+9.25) & 76.82 (+0.67) & — & — \\ MBR & — & — & 62.09 (+8.26) & — & 74.32 (+0.00) & 30.28 (+1.61) \\ MinED & 65.61 (+2.36) & 78.68 (+2.87) & 56.56 (+2.66) & 77.87 (+1.72) & 71.62 ( - 2.70) & 29.50 (+0.83) \\
**DeePEn-Avg** & 65.09 (+1.84) & 78.70 (+2.89) & 56.18 (+2.28) & 77.15 (+1.00) & 75.74 ( +1.42) & 31.55 ( +2.88) \\
**DeePEn-Adapt** & 65.25 (+2.00) & 79.15 (+3.34) & 56.25 (+2.35) & 78.59 (+2.44) & 75.76 (+1.44) & 31.77 ( +3.10) \\ \(\dagger\)+Voting/MBR & 65.40 (+2.15) & 79.44 (+3.63) & 65.25 (+11.35) & 77.37 (+1.22) & 75.65 (+1.33) & 32.11 (+3.44) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Main results. The best individual model is highlighted in \(\overline{\text{red}}\), and the best ensemble method is highlighted in \(\overline{\text{green}}\), except for the results of the combined method (i.e., the last row). The top-4 models on each benchmark are underlined. ‘—’ indicates that the method does not apply to the task.

reasoning with multiple paths. To evidence the complementary strength of DeepPen with Voting, we combine both methods. On the TriviaQA and NQ, Voting is replaced with MBR. As shown that the combination of both methods gains a further improvement over Voting (63.15\(\rightarrow\)65.25).

(4) Collaboration with more worse-performing LLMs is a double-edged sword.The ensemble performance of DeepPen-Avg with top-4 models surpasses that with top-2 models on 4 benchmarks, but falls short on 2 benchmarks. This is reasonable because incorporating the 3rd and 4th ranked LLMs enhances complementary strengths but also causes the interference with the top-2 models.

### Results on Different Numbers of Models

Next, we illustrate the effectiveness of DeepPen on the ensemble of more models on the MMLU, PIQA, and NQ. We add Nanbeige-16B into the ensemble on all three benchmarks, and add LLaMA2-70B and Mixtral-8\(\times\)7B on the PIQA due to their comparable performance. As illustrated in Fig. 3, the ensemble performance increases first and then decreases with the joining of more models in descending order of performance. And the ensemble performance peaks in the top-4 or top-5 models across three benchmarks.

## 5 Analysis

To deeply understand DeepPen, we first evaluate its performance on the ensemble learning of model sets with diverse architectures, abilities, and performance gaps. Next, we conduct a series of analyses on the reverse transformation process of relative representations.

### Results of Ensembling Diverse Models

Ensemble of the dense model and the sparse model.We first evaluate our method on the ensemble learning of the dense model and the sparse MoE model on the challenge reasoning tasks. Specifically, we use the widely-used large-scale dense model LLaMA2-70B [29] and the popular sparse MoE model Mixtral-8\(\times\)7B [14] as the base models. As the results shown in Tab. 2, our DeepPen achieves improvements of +1.60 and +3.22 on the GSM8K and PIQA datasets, even though the base models have achieved a high level of performance.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **GSM8K** & **PIQA** \\ \hline LLaMA2-70B & 63.84 & 71.27 \\ Mixtral-8\(\times\)7B & 65.73 & 71.88 \\ LLM-Biender & 64.52 (-1.21) & 74.54 (+2.66) \\ MinED & 67.10 (+1.37) & 75.65 (+3.77) \\ \(\overline{\text{DeepPen}}\) & 67.33 (+1.60) & 75.10 (+3.22) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ensemble learning of the _dense_ large language model LLaMA2-70B and the _specialist_ translator model NLLB on the _sparse_ MoE model Mixtral-8\(\times\)7B.

Figure 3: Test set results of ensemble learning on various number of models. Individual models are arranged in descending order of their performance on the development set, and sequentially incorporated into the ensemble. \(\hat{\Delta}\) indicates the largest improvement achieved by DeepPen.

Ensemble of the generalist model and the specialist model.To investigate the effectiveness of DeePen on the ensemble of the generalist model and the specialist model for the specific task, we conduct experiments on the machine translation task using the ensemble of the large language model LLaMA2 and the machine translation model NLLB [27], which is a well-known open-source multilingual translator. We adopt the widely-used machine translation benchmark Flores-2002. As the results in Tab. 3 illustrated, DeePen achieves better translation performance leveraging the diverse translation knowledge in the generalist LLM and the specialist translator.

Footnote 2: [https://github.com/facebookresearch/flores](https://github.com/facebookresearch/flores)

Ensemble of models with different performance gaps.To assess the stability of DeePen regarding to the performance gap of base models, we conduct an experiment on the ensemble of model pairs with increasing performance gaps. As the result demonstrated in Tab. 5, the performance of ensemble learning between a well-performing model (the rank-first model)with a worse-performing model could achieve improvements or slightly lag behind the well-performing model.

### Analysis on Relative Transformation

Effect of anchor selection.We demonstrate the impact of different numbers of anchor words through experiments with the top-2 ensemble models on the MMLU and ARC-C datasets. As shown in Fig. 4, an increased number of anchor words can improve performance for LLMs in downstream tasks, and selecting the full set of common words as anchors provides better performance.

Effect of normalization on relative representation matrix.To demonstrate the importance of normalization on the relative representation matrix to the ensemble performance (SS3.2), we conduct an ablation analysis. The result is shown in Tab. 4, the ensemble struggles to achieve improvements due to the ineffective representation of outlier words, _i.e.,_ words distant to other words. The proportion of outlier words can be derived from the distribution of distance to nearest neighbor words, which is illustrated in Fig. 8. As illustrated, a remarkable proportion (\(>30\%\)) of words are distant from other words, _i.e.,_ cosine similarity to its nearest neighbor word is less than 0.3. Through the normalization operation, the output semantics that intend to emit outlier words could be prevented from becoming zero vectors by relative transformation.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{2}{c}{**MMLU-Dev**} & \multicolumn{2}{c}{**TriviaQA-Dev**} \\ \cline{2-5}  & ACC & \(\Delta\) & ACC & \(\Delta\) \\ \hline
**Baseline** & 61.19 & – & 72.74 & – \\
**DeePEN** & 63.61 & +2.42 & 74.79 & +2.05 \\   w/o. **Rel-Norm** & 60.73 & -0.46 & 72.95 & +0.21 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation study of normalization on the relative representation matrix to the ensembling performance on the development sets. **Baseline** refers to as the best single model on each benchmark. **DeePen** refers the performance of ensembling top-2 models in the benchmark.

Figure 4: Effect of the number of anchor words. The **x-axis** indicates the number of anchor words randomly sampled from the common words for 4 times.

Figure 5: 2-model ensemble of the top-1 model (LLaMA2-13B) with different models on the NQ benchmark, respectively.

### Analysis of Reverse Transformation

To better understand the reverse transformation process (SS3.4) transforming the relative representation back to the absolute space of the main model, we further analyze each component of this process.

Analysis of relative ensemble learning rates.As shown in Tab. 5, the performance of DeePEn is sensitive to the value of relative ensemble learning rate (\(\eta\)), which is abbreviated by RELR. This observation motivates us to measure the generality of this hyperparameter. Specifically, we illustrate the cross-distribution performance of the searched optimal value of \(\eta\) in Tab. 9. As observed, the optimal value of RELR varies across different datasets, which suggests that the inverse transformation from relative space to absolute space requires adaptive mapping schemes.

Effect of iteration steps in relative ensemble learning.To give a deep view of the dynamics of the inverse transformation in DeePEn, we report the performance change along with different numbers of relative ensemble learning steps (\(T\)). Besides, the dynamics of loss of relative ensemble learning (\(\eta\) in Eq. 6)is also reported. As shown in Fig. 9, on the one hand, more steps of relative ensemble learning significantly lead to lower losses. However, the loss is hard to reach zero, _i.e.,_ under-fitting. On the other hand, increasing the number of steps of relative ensemble learning will cause the performance to increase first and then decrease. The reason behind the performance drop could be that in the early stage of optimization, the focus of optimization is on updating the high-probability tokens. In the later stage of optimization, since the probabilities of all words will be adjusted equally, the high-probability tokens will be interfered with the high-probability ones, thus affecting the performance. Therefore, it is recommended to set a modest value of step number (_e.g.,_\(T=5\)).

## 6 Related Work

Selection-based ensemble._Rerank_ is an intuitive solution to utilize multi-model strengths. Jiang et al. [15] take the first step towards LLM ensemble, training a reward model PairRanker for pairwise comparison on candidate outputs. To overcome the huge computation costs of multi-LLM inference, several works have explored to train a _router_ to predict the best-performing model out of a fixed set of LLMs for the given input [31, 25, 19].

Fusion-based ensemble.Towards a synergy between LLMs, Jiang et al. [15] propose GenFuser, trained to combine multiple candidate answers. Different from these training-dependent ensemble methods which pose a great challenge to the generalizability of the reward model or fusion model, our DeePEn is completely training-free, making it more general. Similar to our method, MinED also aims to tackle the vocabulary discrepancy via aligning the tokens in different vocabularies based on edit distance [30, 9]. Unfortunately, this textual similarity-based method exhibits unstable performance and produces abnormal text for LLM ensemble (Tab. 7).

There are several contemporaneous works related to our work. Xu et al. [33] propose EVA to tackle vocabulary discrepancy by learning token alignment between different vocabularies with the assistance of overlapping tokens. Our DeePEn eliminates this training process via directly aligning tokens with the relative representation (more discussion is illustrated in SSB). Mavromatis et al. [20] explore adaptive collaboration weights at test time by harnessing the perplexity on the input prompt. We emphasize that this work is complementary to our work.

## 7 Conclusion

In this work, we propose a training-free LLM ensembling framework DeePEn, which addresses the vocabulary discrepancy when fusing the probability distributions of heterogeneous LLMs. Experimental results on six widely-used benchmarks demonstrate that DeePEn exhibits more stable

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**RELR** (\(\eta\)) & **0.05** & **0.10** & **0.15** & **0.20** & **0.25** & **0.30** \\ \hline
**MMLU** & \(\pm\)2.42 & \(+\)1.57 & \(+\)1.77 & \(+\)1.96 & \(+\)1.31 & \(+\)1.31 \\ \hline
**TriviaQA** & \(+\)1.31 & \(\pm\)2.05 & \(+\)1.63 & \(+\)1.94 & \(+\)1.82 & \(+\)1.26 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Sensitivity analysis of relative ensemble learning rate (**RELR**). We report the improvements of ensembling top-2 models over the best individual models.

performance than baseline methods and has complementary strengths with other ensemble methods such as Voting. We believe our work can inspire further research on the LLMs collaboration, model reuse, and knowledge distillation. In the future, we aim to explore more effective adaptive collaboration schemes to leverage the complementary strengths between different LLMs.

## Acknowledgements

Xiaocheng Feng is the corresponding author of this work. We thank the anonymous reviewers for their insightful comments. This work was supported by the National Natural Science Foundation of China (NSFC) (U22B2059, grant62276078), the Key R&D Program of Heilongjiang via grant 2022ZX01A32, the International Cooperation Project of PCL, PCL2022D01 and the Fundamental Research Funds for the Central Universities (Grant No.HIT.OCEF.2023018).

## References

* [1]S. Ali (2024) Yi: open foundation models by 01.ai. External Links: Link Cited by: SS1.
* [2]Z. Allen-Zhu and Y. Li (2020) Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. External Links: Link Cited by: SS1.
* [3]Y. Bisk, R. Zellers, R. Le bras, J. Gao, and Y. Choi (2020-05) Piqua: reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence34 (05), pp. 7432-7439. External Links: Link Cited by: SS1.
* [4]Y. Chen, W. Cai, L. Wu, X. Li, Z. Xin, and C. Fu (2023) Tigerbot: an open multilingual multitask llm. External Links: Link Cited by: SS1.
* [5]P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord (2018) Think you have solved question answering? try arc, the ai2 reasoning challenge. External Links: Link Cited by: SS1.
* [6]K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman (2021) Training verifiers to solve math word problems. External Links: Link Cited by: SS1.
* [7]X. Dong, Z. Yu, W. Cao, Y. Shi, and Q. Ma (2020) A survey on ensemble learning. Frontiers of Computer Science14, pp. 241-258. External Links: Link Cited by: SS1.
* [8]M. Freitag, B. Ghorbani, and P. Fernandes (2023-12) Epsilon sampling rocks: investigating sampling strategies for minimum Bayes risk decoding for machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, pp. 9198-9209. External Links: Link, Document Cited by: SS1.
* [9]Y. Fu, H. Peng, L. Ou, A. Sabharwal, and T. Khot (2022-05) Specializing smaller language models towards multi-step reasoning. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 202, pp. 10421-10430. External Links: Link Cited by: SS1.
* [10]E. Garmash and C. Monz (2016-11) Ensemble learning for multi-source neural machine translation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [11]B. He and M. Ozay (2022) Feature kernel distillation. In International Conference on Learning Representations, External Links: Link Cited by: SS1.
* [12]B. He and M. Ozay (2022) Feature kernel distillation. In International Conference on Learning Representations, External Links: Link Cited by: SS1.
* [13]Y. He, H. Peng, L. Ou, A. Sabharwal, and T. Khot (2022-05) Specializing smaller language models towards multi-step reasoning. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 202, pp. 10421-10430. External Links: Link Cited by: SS1.
* [14]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [15]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [16]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [17]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [18]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [19]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [20]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [21]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [22]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [23]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [24]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [25]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [26]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [27]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [28]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [29]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [30]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [31]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [32]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [33]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [34]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [35]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [36]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [37]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [38]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [39]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [40]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [41]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [42]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan, pp. 1409-1418. External Links: Link Cited by: SS1.
* [43]Y. He, H. Peng, L. Ou, and A. Sabharwal (2020-11) Deep learning for multi-source neural machine translation. In Proceedings of COLING 2020, the 26th International Conference on * Hendrycks et al. [2021] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=d7KBjm13GmQ](https://openreview.net/forum?id=d7KBjm13GmQ).
* Jiang et al. [2023] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.
* Jiang et al. [2024] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, and et al. Mistral of experts, 2024.
* Jiang et al. [2023] D. Jiang, X. Ren, and B. Y. Lin. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 14165-14178, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.792. URL [https://aclanthology.org/2023.acl-long.792](https://aclanthology.org/2023.acl-long.792).
* Joshi et al. [2017] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).
* May 7 2004. Association for Computational Linguistics. URL [https://aclanthology.org/N04-1022](https://aclanthology.org/N04-1022).
* Kwiatkowski et al. [2019] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026).
* Lu et al. [2023] K. Lu, H. Yuan, R. Lin, J. Lin, Z. Yuan, C. Zhou, and J. Zhou. Routing to the expert: Efficient reward-guided ensemble of large language models, 2023.
* Mavromatis et al. [2024] C. Mavromatis, P. Karypis, and G. Karypis. Pack of llms: Model fusion at test-time via perplexity optimization, 2024.
* Moschella et al. [2023] L. Moschella, V. Maiorca, M. Fumero, A. Norelli, F. Locatello, and E. Rodola. Relative representations enable zero-shot latent space communication. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=SrC-nwieGJ](https://openreview.net/forum?id=SrC-nwieGJ).
* OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
* Park et al. [2019] W. Park, D. Kim, Y. Lu, and M. Cho. Relational knowledge distillation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 3967-3976, 2019.
* Sagi and Rokach [2018] O. Sagi and L. Rokach. Ensemble learning: A survey. _WIREs Data Mining and Knowledge Discovery_, 8(4):e1249, 2018. doi: [https://doi.org/10.1002/widm.1249](https://doi.org/10.1002/widm.1249). URL [https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1249](https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1249).
* Shnitzer et al. [2024] T. Shnitzer, A. Ou, M. Silva, K. Soule, Y. Sun, J. Solomon, N. Thompson, and M. Yurochkin. Large language model routing with benchmark datasets, 2024. URL [https://openreview.net/forum?id=LyNsMNNLjY](https://openreview.net/forum?id=LyNsMNNLjY).
* Team [2023] I. Team. Interlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InterLM/InternLM-techreport](https://github.com/InterLM/InternLM-techreport), 2023.

* [27] N. Team. No language left behind: Scaling human-centered machine translation, 2022. URL [https://arxiv.org/abs/2207.04672](https://arxiv.org/abs/2207.04672).
* [28] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023.
* [29] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, and et al. Llama 2: Open foundation and fine-tuned chat models, 2023.
* [30] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi. Knowledge fusion of large language models. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=jib8k12qcz](https://openreview.net/forum?id=jib8k12qcz).
* [31] H. Wang, F. M. Polo, Y. Sun, S. Kundu, E. Xing, and M. Yurochkin. Fusing models with complementary expertise. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=PhMrGMIRL](https://openreview.net/forum?id=PhMrGMIRL).
* [32] T. Wei, L. Zhao, L. Zhang, B. Zhu, L. Wang, and at el. Skywork: A more open bilingual foundation model, 2023.
* [33] Y. Xu, J. Lu, and J. Zhang. Bridging the gap between different vocabularies for llm ensemble, 2024.
* [34] C. Zhang and Y. Ma. _Ensemble Machine Learning: Methods and Applications_. Springer Publishing Company, Incorporated, 2012. ISBN 1441993258.
* [35] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R. Wen. A survey of large language models, 2023.
* [36] Z.-H. Zhou. _Ensemble methods: foundations and algorithms_. CRC press, 2012.

## Appendix A Statistics of Common Tokens across different LLMs

We count the number of common tokens shared among different LLM vocabularies and present the results in Fig. 6. It is observed that a large number of common words (>20k) exist across the different vocabularies. We also count the number of common tokens in all six LLMs and find that there are a total of 18k common tokens, enabling DeePEn to be applied to the ensemble learning of a large number of models.

## Appendix B Details of Baselines

LLM-Blender.(1) the selection-based ensemble method PairRanker Jiang et al. [15], which is a reward model to score each response of LLMs and (2) the fusion-based ensemble method GenFuser Jiang et al. [15], which is a generative model to fuse multiple candidate responses. Both models are trained on the constructed instruction tuning dataset MixInstruct. In our experiments, as GenFuser struggles to generate responses following the expected format, we only adopt PairRanker.

Voting.For tasks with outputs limited to a fixed set (_i.e.,_ MMLU, ARC-C, PIQA, GSM8K benchmarks), we adopt the Voting method on the ensemble learning of more than 2 models. Concretely, we count each candidate answer's occurrences and select the most frequent as the final output. In the event of a tie, the main model's answer is used as the final output.

Mbr.For generation tasks, we implement the MBR [8, 17] method, which selects the answer with the highest lexical similarity to other candidate answers. To measure this similarity, we experimented with the edit distance and chrF3 metrics, ultimately choosing chrF due to its superior performance.

MinED.To bridge the gap between different vocabularies in LLM ensemble, MinED apply the Minimum Edit Distance (MinED) approach to align tokens across different vocabularies, _e.g.,_ "get" to "gets". However, this textual similarity-based mapping method could disturb the text generation process and produce incomplete words.

Eva.Recently, Xu et al. [33] propose EVA to tackle the vocabulary discrepancy by learning mappings between the vocabularies of different LLMs with the assistance of overlapping tokens. We have tried to re-implement their method with the released code. However, we encounter a technical problem in that EVA only supports the ensemble learning between LLMs with the same embedding dimension. This is caused by the limitation of tool of vecmap4, which is used to learn the token alignment.

Figure 6: Statistics of common words across different vocabularies.

## Appendix C Additional Experiments

### Choice of main model.

In the process of inverse transformation, DeePen maps the relative aggregated representation to the absolute space of the main model. Ideally, we expected the results of inverse transformation to keep invariant with the choice of the main model. However, this objective is hard to achieve due to the underfitting observed in the search process. Therefore, we illustrate the performance variance of choosing different main models in Tab 6. As the results shown on ARC-C, changing the main model from the first-ranked Mistral-7B to the second-rank Yi-6B, the ensemble performance is decreased slightly from \(77.73\) to \(77.55\). Interestingly, changing the main model from the rank-1 Yi-6B to the rank-2 Mistral-7B on **MMLU**, the performance is actually improved from \(63.63\) to \(64.46\), which indicates that Mistral-7B benefits more than Yi-6B from collaboration. Even so, choosing different main models does not significantly affects the ensemble performance.

### Comparison to Vanilla Prediction Average

To compare our DeePen with vanilla prediction average, we conduct an experiment for ensembling two LLMs with the same vocabulary and comparable performance on MMLU, _i.e._, LLaMA2-7B and LLaMA1-13B. As shown in Tab. 7, the performance of DeePen is comparable, even better than, that of the vanilla prediction average. Theoretically, the performance of the vanilla prediction average is the performance upper-bound of DeePen. The reason that DeePen could ever the vanilla one on MMLU is the under-fitting in the inverse transformation process, which leads to the weights to aggregate the output semantics of different models not being a uniform distribution (_i.e.,_\((0.5,0.5)\)).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Models**} & \multicolumn{2}{c}{**MMLU-Dev**} & \multicolumn{2}{c}{**ARC-C-Dev**} \\ \cline{2-5}  & Indiv & DeePen & Indiv & DeePen \\ \hline Yi-6B & 61.19 & **63.61** (+2.42) & 72.72 & **77.55** (+4.83) \\ Mistral-7B & 60.80 & **64.46** (+3.66) & 73.88 & **77.73** (+3.85) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance of DeePen with choosing different main models on the development sets. Indiv refers to as individual models. The result of DeePen indicates the performance of using the model of this row as the main model.

Figure 7: Analysis of the generation process of MinED. To illustrate the problematic generation process of MinED, we list the top-10 high-probability tokens in the probability distribution of the assistant model and their aligned token.

For example, in Tab. 7, the weights for LLaMA1 and LLaMA2 could be \((0.6,0.4)\), where the weight of the main model is larger than the other model.

### Latency Analysis

To accomplish the fusion of heterogeneous distributions, DeePEn first maps the distributions into the relative space and adopts the search-based inverse transformation to map the aggregated relative representation back to the main model's probability distribution, which incurs an extra latency. This latency is mainly caused by the inverse transformation process, which requires \(T\)-round search. To demonstrate this latency, we report the token-level inference latency of ensembling two LLMs (Mixtral-8\(\times\)7b and LLaMA2-70B). This experiment is conducted on 8 A100 GPUs. All of our experiments can be re-implemented on 8 A100 GPUs. As shown in Tab. 8, DeePEn causes +17% token-level inference latency. However, in practice, this latency could be greatly decreased since all individual models intend to emit the same token in 90% decoding steps. In these steps, we could skip the fusion process and use the consistently agreed token as the next token. In total, DeePEn actually incurs less than 2% sentence-level inference latency.

## Appendix D Limitations

As illustrated in Tab. 1, collaboration with more LLMs can sometimes lead to a performance drop caused by interference from lower-performing models. This issue limits the ensemble performance of our current method, even though we have explored setting different collaboration weights for each model on each benchmark (DeePEn-Adapt). An ideal solution would be to set adaptive collaboration weights at the sample level, or even the token level, for each LLM, which remains a significant

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & \multicolumn{2}{c}{**MMLU-Dev**} & \multicolumn{2}{c}{**MMLU-Test**} \\ \cline{2-6}
**Models** & \multicolumn{2}{c}{Indiv Vanill DeePEn} & \multicolumn{2}{c}{Indiv Vanill DeePEn} \\ \hline LLaMA1-13B & 43.26 & 45.48 & 44.37 & 43.70 & 45.01 & 44.22 \\ LLaMA2-7B & 42.28 & **45.94** & 42.99 & 45.31 & **45.31** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison to vanilla prediction average (Vanill) on the ensemble of LLMs with the same vocabulary.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & \multicolumn{1}{c}{**Baseline**} & \multicolumn{1}{c}{\(T=1\)} & \multicolumn{1}{c}{\(T=3\)} & \multicolumn{1}{c}{\(T=5\)} & \multicolumn{1}{c}{\(T=10\)} \\ \hline
**Inference Latency** & 0.19s & 0.20s & 0.21s & 0.22s & 0.24s \\
**Relative Change** & 0\% & +7\% & +11\% & +17\% & +29\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: Inference Latency of DeePEn with different search steps \(T\).

Figure 8: Distance distribution to nearest neighbor words. The distance is measured by calculating the cosine similarity between words.

challenge. Despite this, our work represents an important step towards the distribution fusion of LLMs.

\begin{table}
\begin{tabular}{l|c|c|c||c|c} \hline \hline  & \multicolumn{1}{c}{**Baseline**} & \multicolumn{1}{c}{**TrivaQA**} & \multicolumn{1}{c}{**NQ**} & \multicolumn{1}{c}{**ARC-C**} & \multicolumn{1}{c}{**MMLU**} \\ \hline TriviaQA & 73.42 & 75.59 & 75.41 & 75.56 & 75.44 \\ NQ & 29.11 & 30.55 & 30.65 & 30.42 & 30.69 \\ ARC-C & 60.29 & 69.32 & 72.31 & 74.19 & 73.76 \\ MMLU & 54.06 & 59.97 & 61.04 & 61.94 & 61.42 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Cross-distribution validation of relative ensemble learning rate (\(\eta\)). We report the performance of ensembling LLaMA2-13B and Mistral-7B. Each row indicates the test set used to evaluate performance. Each column indicates the development set used to search the optimal value of \(\eta\).

Figure 9: Effect of different number of relative ensemble learning steps.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: This paper proposes DEEPEN, a training-free ensemble framework that enables collaboration between large language models with heterogeneous vocabularies by averaging their probability distributions. The key innovation is transforming the probability distributions from each model's vocabulary space into a shared "relative representation" space based on embedding similarities to anchor tokens. The aggregated relative representation is then mapped back to the vocabulary space of one LLM to determine the generated token. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of our work in SSD. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The method proposed in this work is based on the cross-model invariance of relative representation, which is described in the 'Introduction'. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the information of our experiments is illustrated in SS4.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code will be submitted. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the test details are described in SS4.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Since our work does not involve with model training, we have not conducted the error statistics. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As mentioned in SSC.3, all of our experiments are conducted on 8 A100 GPUs. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our work conforms the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our research motivates the research of prediction fusion and model fusion via tackling the vocabulary discrepancy. There is no negative societal impacts of the work. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer:[Yes] Justification: All resource used in our work are from open source community. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. ** If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not involved Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.