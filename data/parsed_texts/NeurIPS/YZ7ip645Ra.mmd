# Structured Prediction

with Stronger Consistency Guarantees

 Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu &Mehrvar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com &Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

We present an extensive study of surrogate losses for structured prediction supported by _\(\mathcal{H}\)-consistency bounds_. These are recently introduced guarantees that are more relevant to learning than Bayes-consistency, since they are not asymptotic and since they take into account the hypothesis set \(\mathcal{H}\) used. We first show that no non-trivial \(\mathcal{H}\)-consistency bound can be derived for widely used surrogate structured prediction losses. We then define several new families of surrogate losses, including _structured comp-sum losses_ and _structured constrained losses_, for which we prove \(\mathcal{H}\)-consistency bounds and thus Bayes-consistency. These loss functions readily lead to new structured prediction algorithms with stronger theoretical guarantees, based on their minimization. We describe efficient algorithms for minimizing several of these surrogate losses, including a new _structured logistic loss_.

## 1 Introduction

In most applications, the output labels of learning problems have some structure that is crucial to consider. This includes natural language processing applications, where the output may be a sentence, a sequence of parts-of-speech tags, a parse tree, or a dependency graph. It also includes image annotation, image segmentation, computer vision, video annotation, object recognition, motion estimation, computational photography, bioinformatics, and many other important applications.

Several algorithms have been designed in the past for structured prediction tasks, including Conditional Random Fields (CRFs) [11, 10], StructSVMs [23], Maximum-Margin Markov Networks (M3N) [26], kernel-regression-based algorithms [13, 14], Voted CRF and StructBoost [13, 15, 16], search-based methods [1, 12, 14, 15, 17, 18] and a variety of deep learning techniques [21, 13, 14, 15], see Appendix A for a more comprehensive list of references and discussion.

Structured prediction tasks inherently involve a natural loss function based on substructures, which could be the Hamming loss, the \(n\)-gram loss, the edit-distance loss, or some other sequence similarity-based loss or task-specific structured loss. Many of the algorithms previously mentioned overlook this inherent structured loss by simply minimizing the cross-entropy loss. In contrast, the surrogate loss functions minimized by algorithms such as CRFs [11, 12], M3N [26], StructSVMs [23] or Voted CRF and StructBoost [13] do take into account the natural structured loss of the task. But are these structured prediction loss functions consistent? What guarantees can we rely on when minimizing them over a restricted hypothesis set that does not include all measurable functions? Can we derive non-asymptotic guarantees?This paper deals precisely with these theoretical problems in structured prediction.

**Previous work.** We include a detailed discussion on consistency in structured prediction in Appendix A. Here, we briefly discuss previous work by Osokin et al. (2017). To our knowledge, this is one of the only studies proving Bayes-consistency for a family of loss functions in structured prediction (see also (Nowak et al., 2020) and other related references in Appendix A). The surrogate losses the authors proposed are the following _quadratic losses_ (see also (Zhang, 2004)) defined for any function \(h\) mapping \(\mathcal{X}\times\mathcal{Y}\) to \(\mathbb{R}\) and any loss function \(\ell\) between output labels by

\[\forall(x,y)\in\mathcal{X}\times\mathcal{Y},\quad\mathsf{L}^{\mathrm{quad}}(h,x,y)=\sum_{y^{\prime}\in\mathcal{Y}}[\ell(y^{\prime},y)+h(x,y^{\prime})]^{2}.\] (1)

However, the authors only consider the hypothesis set of linear scoring functions. Moreover, the feature vector in their setting only depends on the input \(x\) and ignores the label \(y\). In many applications such as natural language prediction, however, it is critical to allow for features that depend both on the input sequence and the output sequence, parse tree, or dependency graph. Finally, in this formulation, the structured prediction problem is cast as a regression problem. Thus, as shown below, the loss function derived is non-standard, even in the binary classification case, where \(\ell=\ell_{0-1}\) is the zero-one loss and \(\mathcal{Y}=\{y_{1},y_{2}\}\). In this simple case, \(\mathsf{L}^{\mathrm{quad}}(h,x,y_{1})\) can be expressed as

\[\mathsf{L}^{\mathrm{quad}}(h,x,y_{1})=\sum_{y^{\prime}\in\mathcal{Y}}[\ell_{0 -1}(y^{\prime},y_{1})+h(x,y^{\prime})]^{2}=h(x,y_{1})^{2}+(1+h(x,y_{2}))^{2}.\] (2)

This is not a typical formulation since it incorporates the magnitude of individual scores. In contrast, in standard binary classification scenario, only the difference between scores matters.

**Structure of the paper.** We present an extensive study of surrogate losses for structured prediction supported by \(\mathcal{H}\)_-consistency bounds_. These are recently introduced guarantees that are more relevant to learning than Bayes-consistency, since they are not asymptotic and since they take into account the hypothesis set \(\mathcal{H}\) used. We first show that no non-trivial \(\mathcal{H}\)-consistency bound or even Bayes-consistency can be derived for widely used surrogate structured prediction losses (Section 3). We then define several new families of surrogate losses, including _structured comp-sum losses_ (Section 4) and _structured constrained losses_ (Section 5), for which we prove \(\mathcal{H}\)-consistency bounds and thus Bayes-consistency. These loss functions readily lead to new structured prediction algorithms with stronger theoretical guarantees, based on their minimization. We also describe efficient gradient computation algorithms for several of these surrogate losses, including a new _structured logistic loss_ (Section 6).

## 2 Preliminaries

**Learning scenario.** We consider the standard structured prediction scenario with the input space \(\mathcal{X}\) and output space \(\mathcal{Y}=\{1,\dots,n\}\). The output space may be discrete objects with overlapping structures, such as sequences, images, graphs, parse trees, lists, or others. We assume that the output can be decomposed into \(l\) substructures. The substructures could represent words or tokens for example, or other subsequences along a sequence, resulting in the decomposition of the output space \(\mathcal{Y}\) as \(\mathcal{Y}=\mathcal{Y}_{1}\times\dots\times\mathcal{Y}_{l}\). Here, each \(\mathcal{Y}_{j}\) represents the set of possible labels or classes that can be assigned to the \(j\)-th substructure.

**Scoring function.** Structured prediction is typically formulated via _scoring functions_ that map \(\mathcal{X}\times\mathcal{Y}\) to \(\mathbb{R}\), which assign a score to each possible class \(y\in\mathcal{Y}\). Let \(\mathcal{H}\) be a family of such scoring functions. For any \(h\in\mathcal{H}\), we denote by \(\mathsf{h}(x)\) its prediction for the input \(x\in\mathcal{X}\), which is the output \(y\in\mathcal{Y}\) that maximizes the score \(h(x,y)\), that is, \(\mathsf{h}(x)=\operatorname*{argmax}_{y\in\mathcal{Y}}h(x,y)\), with a fixed deterministic strategy to break ties in selecting the label with the highest score. For simplicity, we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. We denote by \(\mathcal{H}_{\mathrm{all}}\) the family of all measurable scoring functions.

**Generalization error and target loss.** Given a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\mathcal{Y}\) and a loss function \(\mathsf{L}\colon\mathcal{H}\times\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\), the _generalization error_ of a hypothesis \(h\in\mathcal{H}\) and the _best-in-class generalization error_ are defined as follows:

\[\mathcal{R}_{\mathsf{L}}(h)=\operatorname*{\mathbb{E}}_{(x,y)\sim\mathcal{D}}[ \mathsf{L}(h,x,y)]\quad\text{ and }\quad\mathcal{R}^{*}_{\mathsf{L},\mathcal{H}}=\inf_{h\in\mathcal{H}} \mathcal{R}_{\mathsf{L}}(h).\]

[MISSING_PAGE_FAIL:3]

Theorem 3.2], the minimizability gaps vanish \(\mathcal{M}_{\mathsf{L}}(\mathcal{H}_{\mathsf{all}})=0\) for the family of all measurable functions. More generally, the minimizability gaps vanish when the best-in-class error coincides with the Bayes-error, that is, \(\mathcal{R}^{*}_{\ell}(\mathcal{H})=\mathcal{R}^{*}_{\ell}(\mathcal{H}_{ \mathsf{all}})\)[Awasthi et al., 2022b, Mao et al., 2023h].

The following result characterizes the best-in-class conditional error and the conditional regret for a target loss \(\mathsf{L}\), which will be helpful for proving \(\mathcal{H}\)-consistency bounds in structured prediction. We denote by \(\mathsf{H}(x)\) the set of all possible predictions on a input \(x\) generated by hypotheses in \(\mathcal{H}\): \(\mathsf{H}(x)=\{\mathsf{h}(x)\colon h\in\mathcal{H}\}\). The proof is given in Appendix B.

**Lemma 3**.: _The best-in-class conditional error and the conditional regret for a target loss \(\mathsf{L}\) in structured prediction can be expressed as follows:_

\[\mathcal{C}^{*}_{\mathsf{L},\mathcal{H}}(x) =\min_{y^{\prime}\in\mathsf{H}(x)}\sum_{y\in\mathfrak{y}}p(x,y) \ell(y^{\prime},y)\] \[\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x) =\sum_{y\in\mathfrak{y}}p(x,y)\ell(\mathsf{h}(x),y)-\min_{y^{ \prime}\in\mathsf{H}(x)}\sum_{y\in\mathfrak{y}}p(x,y)\ell(y^{\prime},y).\]

## 3 Structured max losses

In this section, we examine the loss functions associated to several prominent structured prediction algorithms. We show that, while they are natural, none of them is Bayes-consistent, which implies that they cannot be supported by \(\mathcal{H}\)-consistency bounds either. More generally, we consider the following family of surrogate loss functions proposed in [Cortes, Kuznetsov, Mohri, and Yang, 2016], which we refer to as _structured max losses_:

\[\forall(x,y)\in\mathcal{X}\times\mathfrak{y},\quad\mathsf{L}^{\max}(h,x,y)= \max_{y^{\prime}\neq y}\Phi_{\ell(y^{\prime},y)}(h(x,y)-h(x,y^{\prime})),\] (5)

where \(\Phi_{u}\colon\mathbb{R}\to\mathbb{R}_{+}\) is an upper bound on \(v\mapsto u\mathds{1}_{v\leq 0}\) for any \(u\in\mathbb{R}_{+}\). In this formulation, different choices of \(\Phi_{u}\) can lead to different structured prediction algorithms. Specifically, as shown by Cortes et al. (2016), the following choices of \(\Phi_{u}(v)\) recover many well-known algorithms:

* \(\Phi_{u}(v)=\max(0,u(1-v))\): _StructSVM_[Tsochantaridis et al., 2005b].
* \(\Phi_{u}(v)=\max(0,u-v)\): _Max-Margin Markov Networks (M3N)_[Taskar et al., 2003b].
* \(\Phi_{u}(v)=\log(1+e^{u-v})\): _Conditional Random Field (CRF)_[Lafferty et al., 2001b].
* \(\Phi_{u}(v)=ue^{-v}\): _StructBoost_[Cortes et al., 2016].

The following gives a general negative result for \(\mathsf{L}^{\max}\) that holds under broad assumptions.

**Theorem 4** (**Negative results of \(\mathsf{L}^{\max}\))**.: _Assume that \(n>2\) and that \(\Phi_{u}(v)\) is convex and non-increasing for \(u=1\). Then, the max structured loss \(\mathsf{L}^{\max}\) is not Bayes-consistent._

The proof is included in Appendix C. It is straightforward to see that the assumption of Theorem 4 holds for all the choices of \(\Phi_{u}\) listed above. Thus, the theorem rules out consistency guarantees for any of the loss functions associated to the structured prediction algorithms mentioned above: StructSVM, M3N, CRF, Structboost. Furthermore, Theorem 4 provides negative results for a broad and generalized family of loss functions, collectively referred to as structured max loss. This extends the scope of existing research, as previous works had only addressed the inconsistency of specific instances within the structured max loss category, such as that of M3N [Osokin et al., 2017, Ciliberto et al., 2016, Nowak et al., 2020].

## 4 Structured comp-sum losses

In this section, we first analyze the Voted CRF loss function, which incorporates the auxiliary loss function \(\ell\) in the CRF loss and which has been used in several previous studies. Next, we introduce a new family of loss functions for structured predictions that we prove to admit strong consistency guarantees.

### Voted Conditional Random Field (VCRF)

We first study a family of surrogate losses called _Voted Conditional Random Field (VCRF)_, which corresponds to the structured prediction algorithm defined in (Cortes et al., 2016):

\[\forall(x,y)\in\mathcal{X}\times\mathcal{Y},\,\mathsf{L}^{\textsc{VCRF}}(h,x,y )=-\log\!\left[\frac{e^{h(x,y)}}{\sum_{y^{\prime}\in\mathcal{Y}}e^{h(x,y^{ \prime})}+\ell(y,y^{\prime})}\right]=\log\!\left[\sum_{y^{\prime}\in\mathcal{ Y}}e^{\ell(y,y^{\prime})+h(x,y^{\prime})-h(x,y)}\right].\]

This loss function has also been presented as the softmax margin (Gimpel and Smith, 2010) or the reward-augmented maximum likelihood (Norouzi et al., 2016). It can be viewed as the _softmax variant_ of the M3N loss. Indeed, the loss function for M3N can be written as follows:

\[\mathsf{L}(h,x,y)=\max_{y^{\prime}}\max(0,\ell(y^{\prime},y)+h(x,y^{\prime})- h(x,y)).\] (6)

If we replace the maximum function with the softmax, we obtain

\[\mathsf{L}(h,x,y)=\log\!\left[\sum_{y^{\prime}\in\mathcal{Y}}e^{\max\left(0, \ell(y^{\prime},y)+h(x,y^{\prime})-h(x,y)\right)}\right]=\log\!\left[\sum_{y^ {\prime}\in\mathcal{Y}}\max\!\left(1,e^{\ell(y^{\prime},y)+h(x,y^{\prime})-h( x,y)}\right)\right]\!.\] (7)

Next, we show that, as with the loss function for M3N, the VCRF loss function \(\mathsf{L}^{\textsc{VCRF}}\) is inconsistent.

**Theorem 5** (Negative result of \(\mathsf{L}^{\textsc{VCRF}}\)).: _The Voted Conditional Random Field \(\mathsf{L}^{\textsc{VCRF}}\) is not Bayes-consistent._

The proof is included in Appendix D. The key observation in the proof is that the conditional error of VCRF loss function can be reduced to a specific form when the target loss function \(\mathsf{L}\) decouples, which can lead to a different Bayes classifier from that of the target loss function.

To the best of our knowledge, no prior studies in the literature have explored the consistency of the VCRF loss formulation. The most closely related discussions center around a specialized instance of the multi-class logistic loss (also referred to as Conditional Random Field in that context), in which \(\ell(y^{\prime},y)\) disappears within the framework of the Voted Conditional Random Field. The previous works by Osokin et al. (2017); Ciliberto et al. (2016); Nowak et al. (2020) point out that the multi-class logistic loss cannot be consistent in structured prediction due to the absence of the target loss function within its formulation. Instead, our result shows that, even when integrating the target loss \(\ell(y^{\prime},y)\) within its formulation, the Voted Conditional Random Field cannot be consistent.

Along with Theorem 4, these results rule out consistency guarantees for commonly used surrogate loss functions in structured prediction.

### Structured comp-sum loss functions

In this section, we define a family of new loss functions for structured prediction that are not only Bayes-consistent but also supported by \(\mathcal{H}\)-consistency bounds. These are loss functions that can be viewed as the generalization to structured prediction of loss functions defined via a composition and a sum, and that have been referred to as _comp-sum losses_ in (Mao et al., 2023). Thus, we will refer to them as _structured comp-sum losses_. They are defined as follows:

\[\forall(x,y)\in\mathcal{X}\times\mathcal{Y},\quad\mathsf{L}^{\textsc{comp}}(h, x,y)=\sum_{y^{\prime}\in\mathcal{Y}}\bar{\ell}(y^{\prime},y)\Phi_{1}\!\left(\sum_{y^{ \prime\prime}\in\mathcal{Y}}\Phi_{2}(h(x,y^{\prime\prime})-h(x,y^{\prime})) \right),\] (8)

where \(\bar{\ell}(y^{\prime},y)=1-\ell(y^{\prime},y)\), \(\Phi_{1}\!:\!\mathbb{R}_{+}\to\mathbb{R}_{+}\) is a non-decreasing auxiliary function and \(\Phi_{2}\!:\!\mathbb{R}\to\mathbb{R}_{+}\) a non-decreasing auxiliary function. This formulation (8) can also be viewed as a weighted comp-sum loss, if we interpret \(\bar{\ell}(\cdot,y)\) as a weight vector.

Specifically, we can choose \(\Phi_{2}(v)=e^{v}\) and \(\Phi_{1}(v)=\log(v)\), \(\Phi_{1}(v)=v-1\), \(\Phi_{1}(v)=\frac{1}{\alpha}\big{(}1-\frac{1}{v^{\alpha}}\big{)},\alpha\in(0,1)\) and \(\Phi_{1}(v)=1-\frac{1}{v}\), which leads to new surrogate losses for structured prediction defined in Table 1. These surrogate losses are novel strict generalization of their counterparts in the standard multi-class classification case where \(\ell=\ell_{0-1}\). More precisely, when \(\ell=\ell_{0-1}\), \(\l_{\log}^{\textsc{comp}}\) coincides with the _logistic loss_(Verhulst, 1838, 1845; Berkson, 1944, 1951); \(\l_{\mathrm{exp}}^{\textsc{comp}}\) coincides with the _sum-exponential loss_(Weston and Watkins, 1998; Awasthi et al., 2022); \(\l_{\mathrm{exp}}^{\textsc{comp}}\) coincides with thegeneralized cross-entropy loss_(Zhang and Sabuncu, 2018); and \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{mae}}\) coincides with the _mean absolute error loss_(Ghosh et al., 2017).

We will show that these structured comp-sum losses benefit from \(\mathcal{H}\)-consistency bounds in structured prediction, when \(\mathcal{H}\) is a _symmetric_ and _complete_ hypothesis set. A hypothesis set \(\mathcal{H}\) is _symmetric_ if there exists a family \(\mathcal{F}\) of real-valued functions such that \(\{[h(x,1),\ldots,h(x,n)]\colon h\in\mathcal{H}\}=\{[f_{1}(x),\ldots,f_{n}(x)] \colon f_{1},\ldots,f_{n}\in\mathcal{F}\}\) for any \(x\in\mathcal{X}\). Thus, the choice of the scoring functions does not depend on the order of the categories in \(\mathcal{Y}\). A hypothesis set \(\mathcal{H}\) is _complete_ if it can generate scores that span \(\mathbb{R}\), that is, \(\{h(x,y)\colon h\in\mathcal{H}\}=\mathbb{R}\) for any \((x,y)\in\mathcal{X}\times\mathcal{Y}\). As shown by Awasthi et al. (2022b) and Mao et al. (2023), these assumptions are general and hold for common hypothesis sets used in practice, such as the family of linear hypotheses and that of multi-layer feed-forward neural networks, and of course that of all measurable functions.

**Theorem 6** (\(\mathcal{H}\)-consistency bound of \(\mathsf{L}^{\mathrm{comp}}\)).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any target loss \(\ell\), any hypothesis \(h\in\mathcal{H}\) and any distribution, we have_

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\leq \Gamma\big{(}\mathcal{R}_{\mathsf{L}^{\mathrm{comp}}}(h)-\mathcal{R}_{\mathsf{ L}^{\mathrm{comp}},\mathcal{H}}^{*}+\mathcal{M}_{\mathsf{L}^{\mathrm{comp}}, \mathcal{H}}\big{)}-\mathcal{M}_{\mathsf{L},\mathcal{H}},\] (9)

_where \(\Gamma(t)=2\sqrt{t}\) when \(\mathsf{L}^{\mathrm{comp}}=\mathsf{L}^{\mathrm{comp}}_{\mathrm{log}}\) or \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}\); \(\Gamma(t)=2\sqrt{n^{\alpha}t}\) when \(\mathsf{L}^{\mathrm{comp}}=\mathsf{L}^{\mathrm{comp}}_{\mathrm{gce}}\); and \(\Gamma(t)=nt\) when \(\mathsf{L}^{\mathrm{comp}}=\mathsf{L}^{\mathrm{comp}}_{\mathrm{mae}}\)._

Theorem 6 represents a consolidated result for the four structured comp-sum losses, with the proofs for each being presented separately in Appendix E. The key step of the proof is to upper bound the conditional regret of the target loss (Lemma 3) by that of a surrogate loss. To achieve this, we upper bound the best-in-class conditional error by the conditional error of a carefully selected hypothesis \(\overline{h}_{\mu}\in\overline{\mathcal{H}}\). The resulting softmax \(\overline{\mathcal{S}}_{\mu}\) of this hypothesis only differs from the original softmax \(\mathcal{S}\) corresponding to \(\overline{h}\) on two labels. Theorem 6 admits as special cases the \(\mathcal{H}\)-consistency bounds of Mao et al. (2023) given for standard multi-class classification (\(\ell=\ell_{0-1}\)) and significantly extends them to the general structured prediction scenario.

Let us emphasize that our proof technique is novel and distinct from the approach used in (Mao et al., 2023), which only applies to the special case where \(\ell\) is the zero-one loss and cannot be generalized to any target loss \(\ell\). In their proof, the authors choose \(\overline{h}_{\mu}\) based on individual scores \(\overline{h}(x,y)\), rather than the softmax. Consequently, when \(\ell\neq\ell_{0-1}\), as is common in structured prediction, the resulting optimization problem of \(\mu\) can be very intricate and a closed-form expression of the optimization solution cannot be derived. However, our new proof method overcomes this limitation. By viewing the softmax of hypothesis as a unit and introducing a pseudo-conditional distribution \(\overline{q}\), we are able to solve a simple constrained optimization problem on \(\mu\) within structured prediction scenario.

By Steinwart (2007, Theorem 3.2), the minimizability gaps \(\mathcal{M}_{\mathsf{L}^{\mathrm{comp}},\mathcal{H}}\) and \(\mathcal{M}_{\mathsf{L},\mathcal{H}}\) vanish for the family of all measurable functions. Therefore, when \(\mathcal{H}=\mathcal{H}_{\mathrm{all}}\), the \(\mathcal{H}\)-consistency bounds provided in Theorem 6 imply the Bayes-consistency of these structured comp-sum losses.

**Corollary 7**.: _The structured comp-sum loss \(\mathsf{L}^{\mathrm{comp}}\) is Bayes-consistent for \(\mathsf{L}^{\mathrm{comp}}=\mathsf{L}^{\mathrm{comp}}_{\mathrm{log}}\), \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}\), \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{gce}}\), and \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{mae}}\)._

In fact, Theorem 6 provides stronger quantitative bounds than Bayes-consistency when the minimizability gaps vanish, which suggests that if the estimation error of the structured comp-sum loss \(\mathcal{R}_{\mathsf{L}^{\mathrm{comp}}}(h)-\mathcal{R}_{\mathsf{L}^{\mathrm{ comp}},\mathcal{H}}^{*}\) is reduced to \(\epsilon\), the estimation error of the target loss \(\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\) is upper bounded by \(2\sqrt{\epsilon}\) for structured logistic loss and structured sum-exponential loss, \(2\sqrt{n^{\alpha}\,\epsilon}\) for structured generalized cross-entropy loss, and \(n\,\epsilon\) for structured mean absolute error loss.

\begin{table}
\begin{tabular}{l l l} \hline \hline \(\Phi_{1}(v)\) & Name & Formulation \\ \hline \(\log(v)\) & Structured logistic loss & \(\mathsf{L}^{\mathrm{comp}}_{\log}=-\sum_{y^{\prime}\neq y}\overline{\ell}(y^{ \prime},y)\log\left[\frac{e^{h(x,y^{\prime})}}{\sum_{y^{\prime\prime}\neq y}e^{h (x,y^{\prime\prime})}}\right]\). \\ \(v-1\) & Structured sum-exponential loss & \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}=\sum_{y^{\prime}\neq y}\overline{\ell}(y ^{\prime},y)\sum_{y^{\prime\prime}\neq y^{\prime\prime}}e^{h(x,y^{\prime\prime}) -h(x,y^{\prime})}\) \\ \(\frac{1}{\alpha}\big{[}1-\frac{1}{v^{\alpha}}\big{]}\) & Structured generalized cross-entropy loss & \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{gce}}=\sum_{y^{\prime}\neq y}\overline{\ell}(y ^{\prime},y)\frac{1}{\alpha}\Big{[}1-\Big{[}\frac{e^{h(x,y^{\prime})}}{\sum_{y^{ \prime\prime}\neq y}e^{h(x,y^{\prime\prime})}}\Big{]}^{\alpha}\Big{]}\) \\ \(1-\frac{1}{v}\) & Structured mean absolute error loss & \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{mae}}=\sum_{y^{\prime}\neq y}\overline{\ell}(y ^{\prime},y)\Big{[}1-\frac{e^{h(x,y^{\prime})}}{\sum_{y^{\prime\prime}\neq y}e^{h (x,y^{\prime\prime})}}\Big{]}\). \\ \hline \hline \end{tabular}
\end{table}
Table 1: A new family of surrogate losses for structured prediction: structured comp-sum losses.

## 5 Structured constrained loss functions

In this section, we introduce another new family of surrogate losses for structured prediction that we prove to admit \(\mathcal{H}\)-consistency bounds. We will present a novel generalization of the _constrained losses_(Lee et al., 2004; Awasthi et al., 2022b) to structured prediction. Thus, we refer to them as _structured constrained losses_ and define them as follows:

\[\forall\left(x,y\right)\in\mathcal{X}\times\mathcal{Y},\quad\mathsf{L}^{ \mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}(h,x,y)=\sum_{y^{\prime}\in\mathcal{ Y}}\Phi_{\ell(y^{\prime},y)}(-h(x,y^{\prime})),\] (10)

with the constraint that \(\sum_{y\in\mathcal{Y}}h(x,y)=0\) and \(\Phi_{u^{\prime}}\mathbb{R}\rightarrow\mathbb{R}_{+}\) is an upper bound on \(v\mapsto u\mathds{1}_{v\geq 0}\) for any \(u\in\mathbb{R}_{+}\). In standard constrained loss formulation, a single-variable function \(\Phi(v)\) that defines a margin-based loss is used. In (10), the single-variable function \(\Phi(v)\) is generalized to being a function of two variables \(\Phi_{u}(v)\), which depends on both the target loss and the scores, to accommodate the structured prediction scenario. Specifically, we can choose \(\Phi_{u}(v)=ue^{-v}\), \(\Phi_{u}(v)=u\max\{0,1-v\}^{2}\), \(\Phi_{u}(v)=u\max\{0,1-v\}\), \(\Phi_{u}(v)=u\min\{\max\{0,1-v/\rho\},1\}\), which lead to new surrogate losses for structured prediction defined in Table 2. These surrogate losses are novel generalization of their corresponding counterparts (Lee et al., 2004; Awasthi et al., 2022b) in standard multi-class classification, where \(\ell=\ell_{0-1}\). As with structured comp-sum losses, we will show that these structured constrained losses benefit from \(\mathcal{H}\)-consistency bounds in structured prediction as well, for any symmetric and complete hypothesis set \(\mathcal{H}\).

**Theorem 8** (\(\mathcal{H}\)**-consistency bound of \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}\)).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any target loss \(\ell\), hypothesis \(h\in\mathcal{H}\) and any distribution, we have_

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\leq \Gamma\big{(}\mathcal{R}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d }}}(h)-\mathcal{R}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}, \mathcal{H}}^{*}+\mathcal{M}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n} \mathrm{d}},\mathcal{H}}\big{)}-\mathcal{M}_{\mathsf{L},\mathcal{H}}.\] (11)

_where \(\Gamma(t)=2\sqrt{\ell_{\max}t}\) when \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}=\mathsf{L}^{\mathrm{ c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{exp}}\); \(\Gamma(t)=2\sqrt{t}\) when \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}=\mathsf{L}^{\mathrm{ c}\mathrm{t}\mathrm{c}\mathrm{n}\mathrm{d}}_{\mathrm{sign}-\mathrm{h}\mathrm{e}}\); and \(\Gamma(t)=t\) when \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}=\mathsf{L}^{\mathrm{ c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{h}\mathrm{e}}\) or \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{p}}\)._

The proof is included in Appendix F. As for Theorem 8, the key part of the proof is to upper bound the conditional regret of the target loss (Lemma 3) by that of a surrogate loss. Here too, we introduce a pseudo-conditional distribution \(q\), which can be viewed as a weighted distribution of the original one, \(p(x)\), with weights given by the target loss function. Then, we upper bound the best-in-class conditional error by the conditional error of a carefully selected hypothesis \(\overline{h}_{\mu}\in\overline{\mathcal{H}}\).

As shown by Steinwart (2007, Theorem 3.2), for the family of all measurable functions, the minimizability gaps vanish: \(\mathcal{M}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}},\mathcal{H}} =0\) and \(\mathcal{M}_{\mathsf{L},\mathcal{H}}=0\). Therefore, when \(\mathcal{H}=\mathcal{H}_{\mathrm{all}}\), the \(\mathcal{H}\)-consistency bounds provided in Theorem 6 imply the Bayes-consistency of these structured constrained losses.

**Corollary 9**.: _The structured constrained loss \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}\) is Bayes-consistent for \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}=\mathsf{L}^{\mathrm{ c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{exp}}\), \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{sign}-\mathrm{h} \mathrm{e}}\), \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{h}\mathrm{e}}\), and \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\rho}\)._

As with the cases of structured comp-sum losses, Theorem 8 provides in fact stronger quantitative bounds than Bayes-consistency. They show that that if the estimation error of the structured constrained loss \(\mathcal{R}_{\mathsf{L}^{\mathrm{comp}}}(h)-\mathcal{R}_{\mathsf{L}^{\mathrm{ comp}},\mathcal{H}}^{*}\) is reduced to \(\epsilon\), the estimation error of the target loss \(\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\) is upper bounded by \(2\sqrt{\ell_{\max}\epsilon}\) for \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{exp}}\), \(2\sqrt{\epsilon}\) for \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{sign}-\mathrm{h} \mathrm{e}}\) and \(\epsilon\) for \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{h}\mathrm{e}}\) and \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\rho}\).

It is important to note that we can upper bound the minimizability gap by the approximation error, or finer terms depending on the magnitude of the parameter space as in (Mao et al., 2023h). Furthermore, our \(\mathcal{H}\)-consistency bounds (Theorems 6 and 8) can be used to derive finite sample learning bounds for a hypothesis set \(\mathcal{H}\). These bounds depend on the Rademacher complexity of the hypothesis set and the loss function, as well as an upper bound on the minimizability gap for the surrogate loss.

\begin{table}
\begin{tabular}{l l l} \hline \hline \(\Phi_{u}(v)\) & Name & Formulation (\(\sum_{\mathrm{seq}}h(x,y)=0\)) \\ \hline \(ue^{-v}\) & Structured constrained loss & \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{exp}}=\sum_{y^{ \prime}\in\mathcal{Y}}\ell(y^{\prime},y)\max\{0,1-h(x,y^{\prime})\}^{2}\) \\ \(u\max\{0,1-v\}^{2}\) & Structured constrained squared-hing loss & \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{exp}}=\sum_{y^{ \prime}\in\mathcal{Y}}\ell(y^{\prime},y)\max\{0,1-h(x,y^{\prime})\}\) \\ \(u\max\{0,1-v\}\) & Structured constrained hinge loss & \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\mathrm{h}\mathrm{e}}=\sum_{y^{ \prime}\in\mathcal{Y}}\ell(y^{\prime},y)\max\{0,1-h(x,y^{\prime})\}\) \\ \(u\min\{\max\{0,1-\frac{v}{\rho}\},1\}\) & Structured constrained \(\rho\)-margin loss & \(\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{n}\mathrm{d}}_{\rho}=\sum_{y^{ \prime}\in\mathcal{Y}}\ell(y^{\prime},y)\min\{\max\{0,1-\frac{h(x,y^{\prime})}{ \rho}\},1\}\). \\ \hline \hline \end{tabular}
\end{table}
Table 2: A new family of surrogate losses for structured prediction: structured constrained losses.

## 6 Optimization of \(\mathsf{L}_{\log}^{\mathrm{comp}}\) and \(\mathsf{L}_{\exp}^{\mathrm{comp}}\)

In this section, we show that the gradient of the structured logistic loss \(\mathsf{L}_{\log}^{\mathrm{comp}}\) can be computed efficiently at any point \((x_{i},y_{i})\) and therefore that this loss function is both supported by \(\mathcal{H}\)-consistency bounds and is of practical use. We similarly show that for \(\mathsf{L}_{\exp}^{\mathrm{comp}}\) in Appendix G.2.

Fix the labeled pair \((x_{i},y_{i})\) and \(h\in\mathcal{H}\). Observe that \(\mathsf{L}_{\log}^{\mathrm{comp}}(h,x_{i},y_{i})\) can be equivalently rewritten as follows:

\[\mathsf{L}_{\log}^{\mathrm{comp}}(h,x_{i},y_{i}) =\sum_{y^{\prime}\in\mathcal{Y}}\overline{\ell}(y^{\prime},y_{i}) \log\!\left[\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h(x_{i},y^{\prime\prime}) -h(x_{i},y^{\prime})}\right]\] \[=-\sum_{y^{\prime}\in\mathcal{Y}}\overline{\ell}(y^{\prime},y_{i} )h(x_{i},y^{\prime})+\left[\sum_{y^{\prime}\in\mathcal{Y}}\overline{\ell}(y^{ \prime},y_{i})\right]\log\!\left[\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h(x_ {i},y^{\prime\prime})}\right]\] \[=-\sum_{y^{\prime}\in\mathcal{Y}}\overline{\ell}(y^{\prime},y_{i} )h(x_{i},y^{\prime})+\overline{\ell}_{i}\log Z_{h,i},\]

where \(\overline{\ell}_{i}=\sum_{y^{\prime}\in\mathcal{Y}}\overline{\ell}(y^{\prime},y_{i})\), and \(Z_{h,i}=\sum_{y^{\prime}\in\mathcal{Y}}e^{h(x_{i},y)}\). Note that \(\overline{\ell}_{i}\) does not depend on \(h\) and can be pre-computed. Modulo normalization, this quantity is the average _similarity_ of \(y_{i}\) to \(\mathcal{Y}\), if we interpret \(\overline{\ell}=1-\ell\) as a similarity. While \(\mathcal{Y}\) may be very large, this can be often computed straightforwardly for most loss functions \(\ell\). For example, for the Hamming loss, for sequences of length \(l\), we have

\[\frac{1}{|\mathcal{Y}|}\overline{\ell}_{i}=\frac{1}{l}\operatorname{\mathbb{E} }\!\left[\sum_{k=1}^{l}(1-\mathds{1}_{y^{\prime}_{k}+y_{i,k}})\right]=\frac{1 }{l}\sum_{k=1}^{l}\operatorname{\mathbb{E}}\!\left[\mathds{1}_{y^{\prime}_{k} =y_{i,k}}\right]=\frac{1}{l}\sum_{k=1}^{l}\frac{1}{2}=\frac{1}{2}.\]

Thus, in this case, \(\overline{\ell}_{i}\) does not depend on \(i\) and is a universal constant. Similarly, \(\overline{\ell}_{i}\) can be shown to be a constant for many other losses.

**Hypothesis set.** For the remaining of this section, to simplify the presentation, we will consider the hypothesis set of linear functions \(\mathcal{H}=\big{\{}x\mapsto\mathbf{w}\cdot\Psi(x,y)\mathbf{:w}\in\mathbb{R}^ {d}\big{\}}\), where \(\Psi\) is a feature mapping from \(\mathcal{X}\times\mathcal{Y}\) to \(\mathbb{R}^{d}\). Note that a number of classical structured prediction algorithms adopt the same linear hypothesis set: StructSVM (Tschantaridis et al., 2005b), Max-Margin Markov Networks (M3N) (Taskar et al., 2003b), Conditional Random Field (CRF) (Lafferty et al., 2001b), Voted Conditional Random Field (VCRF) (Cortes et al., 2016). Our algorithms can also be incorporated into standard procedures for training neural network architectures (see (Cortes et al., 2018), Appendix B).

**Markovian features.** We will also assume Markovian features, as is common in structured prediction. Features used in practice often admit this property. Furthermore, in the absence of any such assumption, it is known that learning and inference in general are intractable. We will largely adopt here the definitions and notation from (Cortes et al., 2016) and will consider the common case where \(\mathcal{Y}\) is a set of sequences of length \(l\) over a finite alphabet \(\Delta\) of size \(r\). Other structured problems can be treated in similar ways. We will denote by \(\varepsilon\) the empty string and for any sequence \(y=(y_{1},\ldots,y_{l})\in\mathcal{Y}\), we will denote by \(y^{s^{\prime}}_{s}=(y_{s},\ldots,y_{s^{\prime}})\) the substring of \(y\) starting at index \(s\) and ending at \(s^{\prime}\). For convenience, for \(s\leq 0\), we define \(y_{s}\) by \(y_{s}=\varepsilon\).

We will assume that the feature vector \(\Psi\) admits a _Markovian property of order \(q\)_, that is it can be decomposed as follows for any \((x,y)\in\mathcal{X}\times\mathcal{Y}\):

\[\Psi(x,y)=\sum_{s=1}^{l}\psi(x,y^{s}_{s-q+1},s).\] (12)

for some position-dependent feature vector function \(\psi\) defined over \(\mathcal{X}\times\Delta^{q}\times[l]\). We note that we can write \(\Psi=\sum_{k=1}^{p}\tilde{\Psi}_{k}\) with \(\tilde{\Psi}_{k}=(0,\ldots,\Psi_{k},\ldots,0)\). In the following, abusing the notation, we will simply write \(\Psi_{k}\) instead of \(\tilde{\Psi}_{k}\). Each \(\Psi_{k}\) corresponds to a Markovian feature vector based only on \(k\)-grams, \(p\) is the largest \(k\). Thus, for any \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\), we have

\[\Psi(\mathbf{x},y)=\sum_{k=1}^{p}\Psi_{k}(x,y).\] (13)For any \(k\in[1,p]\), let \(\psi_{k}\) denote the position-dependent feature vector function corresponding to \(\Psi_{k}\). Also, for any \(x\in\mathcal{X}\) and \(y\in\Delta^{l}\), define \(\widetilde{\psi}\) by \(\widetilde{\psi}(x,y^{s}_{s-p+1},s)=\sum_{k=1}^{p}\psi_{k}(x,y^{s}_{s-k+1},s)\). Observe then that we can write

\[\Psi(x,y)=\sum_{k=1}^{p}\Psi_{k}(x,y)=\sum_{k=1}^{p}\sum_{s=1}^{l}\psi_{k}(x,y^ {s}_{s-k+1},s)=\sum_{s=1}^{l}\sum_{k=1}^{p}\psi_{k}(x,y^{s}_{s-k+1},s)=\sum_{s= 1}^{l}\widetilde{\psi}(x,y^{s}_{s-p+1},s).\]

**Gradient computation.** Adopting the shorthand \(\mathbf{w}\) for \(h\), we can rewrite the loss at \((x_{i},y_{i})\) as:

\[\mathsf{L}_{\log}^{\mathrm{comp}}(\mathbf{w},x_{i},y_{i})=-\mathbf{w}\cdot \left[\sum_{y^{\prime}\in\mathbb{J}}\overline{\ell}(y^{\prime},y_{i})\Psi(x_{ i},y^{\prime})\right]+\overline{\ell}_{i}\log Z_{\mathbf{w},i}.\]

Thus, the gradient of \(\mathsf{L}_{\log}^{\mathrm{comp}}\) at any \(\mathbf{w}\in\mathbb{R}^{d}\) is given by

\[\nabla\mathsf{L}_{\log}^{\mathrm{comp}}(\mathbf{w}) =-\sum_{y^{\prime}\in\mathbb{J}}\overline{\ell}(y^{\prime},y_{i} )\Psi(x_{i},y^{\prime})+\overline{\ell}_{i}\sum_{y^{\prime}\in\mathbb{J}} \frac{e^{\mathbf{w}\cdot\Psi(x_{i},y)}}{\sum_{y^{\prime}\in\mathbb{J}}e^{ \mathbf{w}\cdot\Psi(x_{i},y^{\prime})}}\Psi(x_{i},y)\] \[=-\sum_{y^{\prime}\in\mathbb{J}}\overline{\ell}(y^{\prime},y_{i} )\Psi(x_{i},y^{\prime})+\overline{\ell}_{i}\mathop{\mathbb{E}}_{y\to \mathbf{w}}[\Psi(x_{i},y)],\]

where \(\mathsf{q}_{\mathbf{w}}\) is defined for all \(y\in\mathbb{J}\) by \(\mathsf{q}_{\mathbf{w}}(y)=\frac{e^{\mathbf{w}\cdot\Psi(x_{i},y)}}{Z_{\mathbf{ w}}}\) with \(Z_{\mathbf{w}}=\sum_{y\in\mathbb{J}}e^{\mathbf{w}\cdot\Psi(x_{i},y)}\). Note that the sum defining these terms is over a number of sequences \(y\) that is exponential in \(r\) and that the computation appears to be therefore challenging. The following lemma gives the expression of the gradient of \(\mathsf{L}_{\log}^{\mathrm{comp}}\) and helps identify the most computationally challenging terms.

**Lemma 10**.: _For any \(\mathbf{w}\in\mathbb{R}^{d}\), the gradient of \(\mathsf{L}_{\log}^{\mathrm{comp}}\) can be expressed as follows:_

\[\nabla\mathsf{L}_{\log}^{\mathrm{comp}}(\mathbf{w})=\sum_{s=1}^{l}\sum_{ \mathbf{z}\in\Delta^{p}}\Bigl{[}\overline{\ell}_{i}\mathsf{Q}_{\mathbf{w}}( \mathbf{z},s)-\mathsf{L}(\mathbf{z},s)\Bigr{]}\widetilde{\psi}(x_{i},\mathbf{ z},s),\]

where \(\mathsf{Q}_{\mathbf{w}}(\mathbf{z},s)=\sum_{y:y^{s}_{s-p+1}=\mathbf{z}} \mathsf{q}_{\mathbf{w}}(y)\) and \(\mathsf{L}(\mathbf{z},s)=\sum_{y:y^{s}_{s-p+1}=\mathbf{z}}\overline{\ell}(y, y_{i})\).

Proof.: Using the decomposition of the feature vector, we can write:

\[\sum_{y\in\mathbb{J}}\overline{\ell}(y,y_{i})\Psi(x_{i},y)=\sum_{ y\in\Delta^{l}}\overline{\ell}(y,y_{i})\sum_{s=1}^{l}\widetilde{\psi}(x_{i},y^{s}_{s- p+1},s)=\sum_{s=1}^{l}\sum_{\mathbf{z}\in\Delta^{p}}\Biggl{[}\sum_{y:y^{s}_{s- p+1}=\mathbf{z}}\overline{\ell}(y,y_{i})\Biggr{]}\widetilde{\psi}(x_{i}, \mathbf{z},s)\] \[\mathop{\mathbb{E}}_{y\to\mathbf{q}_{\mathbf{w}}}[\Psi(x_{i},y)]= \sum_{y\in\Delta^{l}}\mathsf{q}_{\mathbf{w}}(y)\sum_{s=1}^{l} \widetilde{\psi}(x_{i},y^{s}_{s-p+1},s)=\sum_{s=1}^{l}\sum_{\mathbf{z}\in \Delta^{p}}\Biggl{[}\sum_{y:y^{s}_{s-p+1}=\mathbf{z}}\mathsf{q}_{\mathbf{w}}( y)\Biggr{]}\widetilde{\psi}(x_{i},\mathbf{z},s).\]

This completes the proof. 

In light of this result, the bottleneck in the gradient computation is the evaluation of \(\mathsf{Q}_{\mathbf{w}}(\mathbf{z},s)\) and \(\mathsf{L}(\mathbf{z},s)\) for all \(s\in[l]\) and \(\mathbf{z}\in\Delta^{p}\). In previous work [11, 10], it was shown that the quantities \(\mathsf{Q}_{\mathbf{w}}(\mathbf{z},s)\) can be determined efficiently, all together, by running two single-source shortest-distance algorithms over the \((+,\times)\) semiring on an appropriate weighted finite automaton (WFA). The overall time complexity of the computation of all quantities \(\mathsf{Q}_{\mathbf{w}}(\mathbf{z},s)\), \(\mathbf{z}\in\Delta^{p}\) and \(s\in[l]\), is then in \(O(lr^{p})\), where \(r=|\Delta|\).

We now analyze the computation of \(\mathsf{L}(\mathbf{z},s)\) for a fixed \(\mathbf{z}\in\Delta^{p}\) and \(s\in[l]\). Note that, unlike \(\mathsf{Q}_{\mathbf{w}}(\mathbf{z},s)\), this term does not depend on \(\mathbf{w}\) and can therefore be computed once and for all, before any gradient computation. The sum defining \(\mathsf{L}(\mathbf{z},s)\) is over all sequences \(y\) that admit the substring \(\mathbf{z}\) at position \(s\).

**Rational losses.** In Appendix G.1, we also give an efficient algorithm for the computation of the quantities \(\mathsf{L}(\mathbf{z},s)\) in the case of Markovian losses. Here, we present an efficient algorithm for their computation in the important case of _rational losses_. This is a general family of loss functions based on rational kernels [11] that includes, in particular, \(n\)-gram losses, which can be defined for a pair of sequences \((y,y^{\prime})\) as the negative inner product of the vectors of \(n\)-gram counts of \(y\) and \(y^{\prime}\).

[MISSING_PAGE_FAIL:10]

## References

* Ahmad et al. (2023) T. E. Ahmad, L. Brogat-Motte, P. Laforgue, and F. d'Alche Buc. Sketch in, sketch out: Accelerating both learning and inference for structured prediction with kernels. _arXiv preprint arXiv:2302.10128_, 2023.
* Allauzen and Mohri (2003) C. Allauzen and M. Mohri. An efficient pre-determinization algorithm. In O. H. Ibarra and Z. Dang, editors, _Implementation and Application of Automata, 8th International Conference, CIAA 2003, Santa Barbara, California, USA, July 16-18, 2003, Proceedings_, volume 2759 of _Lecture Notes in Computer Science_, pages 83-95. Springer, 2003.
* Allauzen and Mohri (2004) C. Allauzen and M. Mohri. An optimal pre-determinization algorithm for weighted transducers. _Theor. Comput. Sci._, 328(1-2):3-18, 2004.
* Awasthi et al. (2021a) P. Awasthi, N. Frank, A. Mao, M. Mohri, and Y. Zhong. Calibration and consistency of adversarial surrogate losses. In _Advances in Neural Information Processing Systems_, 2021a.
* Awasthi et al. (2021b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. A finer calibration analysis for adversarial robustness. _arXiv preprint arXiv:2105.01550_, 2021b.
* Awasthi et al. (2022a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. H-consistency bounds for surrogate loss minimizers. In _International Conference on Machine Learning_, 2022a.
* Awasthi et al. (2022b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Multi-class H-consistency bounds. In _Advances in neural information processing systems_, 2022b.
* Awasthi et al. (2023a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for adversarial robustness. In _International Conference on Artificial Intelligence and Statistics_, pages 10077-10094, 2023a.
* Awasthi et al. (2023b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. DC-programming for neural network optimizations. _Journal of Global Optimization_, 2023b.
* Belanger and McCallum (2016) D. Belanger and A. McCallum. Structured prediction energy networks. In _International Conference on Machine Learning_, pages 983-992, 2016.
* Belanger et al. (2017) D. Belanger, B. Yang, and A. McCallum. End-to-end learning for structured prediction energy networks. In _International Conference on Machine Learning_, pages 429-439, 2017.
* Belharbi et al. (2018) S. Belharbi, R. Herault, C. Chatelain, and S. Adam. Deep neural networks regularization for structured output prediction. _Neurocomputing_, 281:169-177, 2018.
* Berkson (1944) J. Berkson. Application of the logistic function to bio-assay. _Journal of the American Statistical Association_, 39:357---365, 1944.
* Berkson (1951) J. Berkson. Why I prefer logits to probits. _Biometrics_, 7(4):327---339, 1951.
* Blondel (2019) M. Blondel. Structured prediction with projection oracles. In _Advances in neural information processing systems_, 2019.
* Brogat-Motte et al. (2020) L. Brogat-Motte, A. Rudi, C. Brouard, J. Rousu, and F. d'Alche Buc. Learning output embeddings in structured prediction. _arXiv preprint arXiv:2007.14703_, 2020.
* Cabannes et al. (2021) V. A. Cabannes, F. Bach, and A. Rudi. Fast rates for structured prediction. In _Conference on Learning Theory_, pages 823-865, 2021.
* Cabannes et al. (2020) V. Cabannes, A. Rudi, and F. Bach. Structured prediction with partial labelling through the infimum loss. In _International Conference on Machine Learning_, pages 1230-1239, 2020.
* Chang et al. (2015) K. Chang, A. Krishnamurthy, A. Agarwal, H. Daume III, and J. Langford. Learning to search better than your teacher. In _Proceedings of ICML_, 2015.
* Chen et al. (2014) L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. _arXiv preprint arXiv:1412.7062_, 2014.
* Chen et al. (2015)L.-C. Chen, A. Schwing, A. Yuille, and R. Urtasun. Learning deep structured models. In _International Conference on Machine Learning_, pages 1785-1794, 2015.
* Choi et al. [2016] H. Choi, O. Meshi, and N. Srebro. Fast and scalable structural svm with slack rescaling. In _Artificial Intelligence and Statistics_, pages 667-675, 2016.
* Ciliberto et al. [2016] C. Ciliberto, L. Rosasco, and A. Rudi. A consistent regularization approach for structured prediction. In _Advances in neural information processing systems_, 2016.
* Ciliberto et al. [2019] C. Ciliberto, F. Bach, and A. Rudi. Localized structured prediction. In _Advances in Neural Information Processing Systems_, 2019.
* Ciliberto et al. [2020] C. Ciliberto, L. Rosasco, and A. Rudi. A general framework for consistent structured prediction with implicit loss embeddings. _The Journal of Machine Learning Research_, 21(1):3852-3918, 2020.
* Collobert et al. [2011] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. _Journal of machine learning research_, 12(ARTICLE):2493-2537, 2011.
* Corro [2023] C. Corro. On the inconsistency of separable losses for structured prediction. _arXiv preprint arXiv:2301.10810_, 2023.
* Cortes et al. [2004] C. Cortes, P. Haffner, and M. Mohri. Rational kernels: Theory and algorithms. _J. Mach. Learn. Res._, 5:1035-1062, 2004.
* Cortes et al. [2005] C. Cortes, M. Mohri, and J. Weston. A general regression technique for learning transductions. In L. D. Raedt and S. Wrobel, editors, _Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August 7-11, 2005_, volume 119 of _ACM International Conference Proceeding Series_, pages 153-160. ACM, 2005.
* Cortes et al. [2007] C. Cortes, M. Mohri, and J. Weston. A General Regression Framework for Learning String-to-String Mappings. In _Predicting Structured Data_. MIT Press, 2007.
* Cortes et al. [2014a] C. Cortes, V. Kuznetsov, and M. Mohri. Ensemble methods for structured prediction. In _Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014_, volume 32 of _JMLR Workshop and Conference Proceedings_, pages 1134-1142. JMLR.org, 2014a.
* Cortes et al. [2014b] C. Cortes, V. Kuznetsov, and M. Mohri. Learning ensembles of structured prediction rules. In _Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers_, pages 1-12. The Association for Computer Linguistics, 2014b.
* Cortes et al. [2015] C. Cortes, V. Kuznetsov, M. Mohri, and M. K. Warmuth. On-line learning algorithms for path experts with non-additive losses. In _Proceedings of COLT_, 2015.
* Cortes et al. [2016] C. Cortes, V. Kuznetsov, M. Mohri, and S. Yang. Structured prediction theory based on factor graph complexity. In _Advances in Neural Information Processing Systems_, 2016.
* Cortes et al. [2018] C. Cortes, V. Kuznetsov, M. Mohri, D. Storcheus, and S. Yang. Efficient gradient computation for structured output learning with rational and tropical losses. In _Advances in Neural Information Processing Systems_, 2018.
* Daume III et al. [2009] H. Daume III, J. Langford, and D. Marcu. Search-based structured prediction. _Machine Learning_, 75(3):297-325, 2009.
* Domke [2013] J. Domke. Learning graphical model parameters with approximate marginal inference. _IEEE transactions on pattern analysis and machine intelligence_, 35(10):2454-2467, 2013.
* Doppa et al. [2014] J. R. Doppa, A. Fern, and P. Tadepalli. Structured prediction via output space search. _JMLR_, 15(1):1317-1350, 2014.
* Dragone et al. [2021] P. Dragone, S. Teso, and A. Passerini. Neuro-symbolic constraint programming for structured prediction. _arXiv preprint arXiv:2103.17232_, 2021.
* D'Amico et al. [2015]S. Edunov, M. Ott, M. Auli, D. Grangier, and M. Ranzato. Classical structured prediction losses for sequence to sequence learning. _arXiv preprint arXiv:1711.04956_, 2017.
* Finocchiaro et al. [2019] J. Finocchiaro, R. Frongillo, and B. Waggoner. An embedding framework for consistent polyhedral surrogates. In _Advances in neural information processing systems_, 2019.
* Ghosh et al. [2017] A. Ghosh, H. Kumar, and P. S. Sastry. Robust loss functions under label noise for deep neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, 2017.
* Gimpel and Smith [2010] K. Gimpel and N. A. Smith. Softmax-margin crfs: Training log-linear models with cost functions. In _Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics_, pages 733-736, 2010.
* Graber and Schwing [2019] C. Graber and A. Schwing. Graph structured prediction energy networks. In _Advances in Neural Information Processing Systems_, 2019.
* Graber et al. [2018] C. Graber, O. Meshi, and A. Schwing. Deep structured prediction with nonlinear output transformations. In _Advances in Neural Information Processing Systems_, 2018.
* Gygli et al. [2017] M. Gygli, M. Norouzi, and A. Angelova. Deep value networks learn to evaluate and iteratively refine structured outputs. In _International Conference on Machine Learning_, pages 1341-1351, 2017.
* Hershey et al. [2014] J. R. Hershey, J. L. Roux, and F. Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. _arXiv preprint arXiv:1409.2574_, 2014.
* Huang et al. [2015] Z. Huang, W. Xu, and K. Yu. Bidirectional lstm-crf models for sequence tagging. _arXiv preprint arXiv:1508.01991_, 2015.
* Jaderberg et al. [2014] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Deep structured output learning for unconstrained text recognition. _arXiv preprint arXiv:1412.5903_, 2014.
* Jang et al. [2023] H. Jang, S. Mo, and S. Ahn. Diffusion probabilistic models for graph-structured prediction. _arXiv preprint arXiv:2302.10506_, 2023.
* Jiang et al. [2022] N. Jiang, M. Zhang, W.-J. van Hoeve, and Y. Xue. Constraint reasoning embedded structured prediction. _Journal of Machine Learning Research_, 23(345):1-40, 2022.
* Jurafsky and Martin [2009] D. Jurafsky and J. H. Martin. _Speech and Language Processing (2nd Edition)_. Prentice-Hall, Inc., 2009.
* Kim et al. [2017] Y. Kim, C. Denton, L. Hoang, and A. M. Rush. Structured attention networks. In _International Conference on Learning Representations_, 2017.
* Kunisch and Pock [2013] K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. _SIAM Journal on Imaging Sciences_, 6(2):938-983, 2013.
* Lafferty et al. [2001a] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In _Proceedings of ICML_, 2001a.
* Lafferty et al. [2001b] J. D. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In _International Conference on Machine Learning_, pages 282-289, 2001b.
* Lam et al. [2015] M. Lam, J. R. Doppa, S. Todorovic, and T. G. Dietterich. \(\mathcal{H}\)c-search for structured prediction in computer vision. In _CVPR_, 2015.
* Larsson et al. [2018] M. Larsson, A. Arnab, S. Zheng, P. Torr, and F. Kahl. Revisiting deep structured models for pixel-level labeling with gradient-based inference. _SIAM Journal on Imaging Sciences_, 11(4):2610-2628, 2018.
* Lee et al. [2004] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data. _Journal of the American Statistical Association_, 99(465):67-81, 2004.
* Li and Zemel [2014] Y. Li and R. Zemel. Mean-field networks. _arXiv preprint arXiv:1410.5884_, 2014.
* Liu et al. [2015]T. Liu, Y. Jiang, N. Monath, R. Cotterell, and M. Sachan. Autoregressive structured prediction with language models. _arXiv preprint arXiv:2210.14698_, 2022.
* Long et al. (2015) J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3431-3440, 2015.
* Lu and Huang (2020) Y. Lu and B. Huang. Structured output learning with conditional generative flows. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 5005-5012, 2020.
* Lucchi et al. (2013) A. Lucchi, L. Yunpeng, and P. Fua. Learning for structured prediction using approximate subgradient descent with working sets. In _Proceedings of CVPR_, 2013.
* Manning and Schutze (1999) C. D. Manning and H. Schutze. _Foundations of Statistical Natural Language Processing_. The MIT Press, Cambridge, Massachusetts, 1999.
* Mao et al. (2023a) A. Mao, C. Mohri, M. Mohri, and Y. Zhong. Two-stage learning to defer with multiple experts. In _Advances in neural information processing systems_, 2023a.
* Mao et al. (2023b) A. Mao, M. Mohri, and Y. Zhong. H-consistency bounds: Characterization and extensions. In _Advances in Neural Information Processing Systems_, 2023b.
* Mao et al. (2023c) A. Mao, M. Mohri, and Y. Zhong. Principled approaches for learning to defer with multiple experts. _arXiv preprint arXiv:2310.14774_, 2023c.
* Mao et al. (2023d) A. Mao, M. Mohri, and Y. Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. _arXiv preprint arXiv:2310.14772_, 2023d.
* Mao et al. (2023e) A. Mao, M. Mohri, and Y. Zhong. H-consistency bounds for pairwise misranking loss surrogates. In _International conference on Machine learning_, 2023e.
* Mao et al. (2023f) A. Mao, M. Mohri, and Y. Zhong. Ranking with abstention. In _ICML 2023 Workshop The Many Faces of Preference-Based Learning_, 2023f.
* Mao et al. (2023g) A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for score-based multi-class abstention. _arXiv preprint arXiv:2310.14770_, 2023g.
* Mao et al. (2023h) A. Mao, M. Mohri, and Y. Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In _International Conference on Machine Learning_, 2023h.
* Meister et al. (2020) C. Meister, T. Vieira, and R. Cotterell. Best-first beam search. _Transactions of the Association for Computational Linguistics_, 8:795-809, 2020.
* Mohri et al. (2023) C. Mohri, D. Andor, E. Choi, M. Collins, A. Mao, and Y. Zhong. Learning to reject with a fixed predictor: Application to decontextualization. _arXiv preprint arXiv:2301.09044_, 2023.
* Mohri (1997) M. Mohri. Finite-state transducers in language and speech processing. _Computational Linguistics_, 23(2):269-311, 1997.
* Mohri (2000) M. Mohri. Generic \(\epsilon\)-removal algorithm for weighted automata. In S. Yu and A. Paun, editors, _Implementation and Application of Automata, 5th International Conference, CIAA 2000, London, Ontario, Canada, July 24-25, 2000, Revised Papers_, volume 2088 of _Lecture Notes in Computer Science_, pages 230-242. Springer, 2000.
* Mohri (2002a) M. Mohri. Semiring Frameworks and Algorithms for Shortest-Distance Problems. _Journal of Automata, Languages and Combinatorics_, 7(3):321-350, 2002a.
* Mohri (2002b) M. Mohri. Generic \(\epsilon\)-removal and input \(\epsilon\)-normalization algorithms for weighted transducers. _Int. J. Found. Comput. Sci._, 13(1):129-143, 2002b.
* Mohri (2009) M. Mohri. Weighted automata algorithms. In _Handbook of Weighted Automata_, pages 213-254. Springer, 2009.
* Mohri et al. (2012c)M. Mohri and M. Riley. Weighted determinization and minimization for large vocabulary speech recognition. In G. Kokkinakis, N. Fakotakis, and E. Dermatas, editors, _Fifth European Conference on Speech Communication and Technology, EUROSPEECH 1997, Rhodes, Greece, September 22-25, 1997_, pages 131-134. ISCA, 1997.
* Mohri et al. [2018] M. Mohri, A. Rostamizadeh, and A. Talwalkar. _Foundations of Machine Learning_. MIT Press, second edition, 2018.
* Nadeau and Sekine [2007] D. Nadeau and S. Sekine. A survey of named entity recognition and classification. _Linguisticae Investigationes_, 30(1):3-26, January 2007.
* Norouzi et al. [2016] M. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuurmans, et al. Reward augmented maximum likelihood for neural structured prediction. In _Advances In Neural Information Processing Systems_, 2016.
* Nowak et al. [2019] A. Nowak, F. Bach, and A. Rudi. Sharp analysis of learning with discrete losses. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1920-1929, 2019.
* Nowak et al. [2020] A. Nowak, F. Bach, and A. Rudi. Consistent structured prediction with max-min margin markov networks. In _International Conference on Machine Learning_, pages 7381-7391, 2020.
* Nowak et al. [2022] A. Nowak, A. Rudi, and F. Bach. On the consistency of max-margin losses. In _International Conference on Artificial Intelligence and Statistics_, pages 4612-4633, 2022.
* Nowak-Vila et al. [2019] A. Nowak-Vila, F. Bach, and A. Rudi. A general theory for structured prediction with smooth convex surrogates. _arXiv preprint arXiv:1902.01958_, 2019.
* Osokin et al. [2017] A. Osokin, F. Bach, and S. Lacoste-Julien. On structured prediction theory with calibrated convex surrogate losses. In _Advances in Neural Information Processing Systems_, 2017.
* Pan et al. [2020] P. Pan, P. Liu, Y. Yan, T. Yang, and Y. Yang. Adversarial localized energy network for structured prediction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 5347-5354, 2020.
* Patel et al. [2022] D. Patel, P. Dangati, J.-Y. Lee, M. Boratko, and A. McCallum. Modeling label space interactions in multi-label classification using box embeddings. In _International Conference on Learning Representations_, 2022.
* Pillutla et al. [2018] V. K. Pillutla, V. Roulet, S. M. Kakade, and Z. Harchaoui. A smoother way to train structured prediction models. In _Advances in Neural Information Processing Systems_, 2018.
* Ross et al. [2011] S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of AISTATS_, 2011.
* Schwing and Urtasun [2015] A. G. Schwing and R. Urtasun. Fully connected deep structured networks. _arXiv preprint arXiv:1503.02351_, 2015.
* Steinwart [2007] I. Steinwart. How to compare different loss functions and their risks. _Constructive Approximation_, 26(2):225-287, 2007.
* Stoyanov et al. [2011] V. Stoyanov, A. Ropson, and J. Eisner. Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 725-733, 2011.
* Sutskever et al. [2014] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In _Advances in neural information processing systems_, 2014.
* Tai et al. [2015] K. S. Tai, R. Socher, and C. D. Manning. Improved semantic representations from tree-structured long short-term memory networks. _arXiv preprint arXiv:1503.00075_, 2015.
* Taskar et al. [2003a] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In _NIPS_, 2003a.
* Taskar et al. [2003b] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In _Advances in neural information processing systems_, 2003b.
* Taskar et al. [2003c]I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. _JMLR_, 6:1453-1484, Dec. 2005a.
* Tsochantaridis et al. (2005b) I. Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun, and Y. Singer. Large margin methods for structured and interdependent output variables. _Journal of machine learning research_, 6(9), 2005b.
* Tu and Gimpel (2018) L. Tu and K. Gimpel. Learning approximate inference networks for structured prediction. _arXiv preprint arXiv:1803.03376_, 2018.
* Tu and Gimpel (2019) L. Tu and K. Gimpel. Benchmarking approximate inference methods for neural structured prediction. _arXiv preprint arXiv:1904.01138_, 2019.
* Tu et al. (2019) L. Tu, R. Y. Pang, and K. Gimpel. Improving joint training of inference networks and structured prediction energy networks. _arXiv preprint arXiv:1911.02891_, 2019.
* Verhulst (1838) P. F. Verhulst. Notice sur la loi que la population suit dans son accroissement. _Correspondance mathematique et physique_, 10:113---121, 1838.
* Verhulst (1845) P. F. Verhulst. Recherches mathematiques sur la loi d'accroissement de la population. _Nouveaux Memoires de l'Academie Royale des Sciences et Belles-Lettres de Bruxelles_, 18:1---42, 1845.
* Vinyals et al. (2015a) O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language. In _Proceedings of NIPS_, 2015a.
* Vinyals et al. (2015b) O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In _Proceedings of CVPR_, 2015b.
* Wang et al. (2016) S. Wang, S. Fidler, and R. Urtasun. Proximal deep structured models. _Advances in Neural Information Processing Systems_, 2016.
* Weston and Watkins (1998) J. Weston and C. Watkins. Multi-class support vector machines. Technical report, Citeseer, 1998.
* Wu et al. (2016) Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Lukasz Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. _CoRR_, abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.
* Zhang et al. (2008) D. Zhang, L. Sun, and W. Li. A structured prediction approach for statistical machine translation. In _Proceedings of IJCNLP_, 2008.
* Zhang (2004) T. Zhang. Statistical analysis of some multi-category large margin classification methods. _Journal of Machine Learning Research_, 5(Oct):1225-1251, 2004.
* Zhang and Sabuncu (2018) Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In _Advances in neural information processing systems_, 2018.
* Zheng et al. (2023) C. Zheng, G. Wu, F. Bao, Y. Cao, C. Li, and J. Zhu. Revisiting discriminative vs. generative classifiers: Theory and implications. _arXiv preprint arXiv:2302.02334_, 2023.
* Zheng and Pronobis (2019) K. Zheng and A. Pronobis. From pixels to buildings: End-to-end probabilistic deep networks for large-scale semantic mapping. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 3511-3518, 2019.
* Zheng et al. (2015) S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional random fields as recurrent neural networks. In _Proceedings of the IEEE international conference on computer vision_, pages 1529-1537, 2015.

###### Contents of Appendix

* A Related work
* B Proof of Lemma 3
* C Proofs for structured max losses
* D Proofs for Voted Conditional Random Field
* E Proofs for structured comp-sum losses
* E.1 Structured logistic loss
* E.2 Structured sum-exponential loss
* E.3 Structured generalized cross-entropy loss
* E.4 Structured mean absolute error loss
* F Proofs for structured constrained losses
* F.1 Structured constrained exponential loss
* F.2 Structured constrained squared-hinge loss
* F.3 Structured constrained hinge loss
* F.4 Structured constrained \(\rho\)-margin loss
* G Efficient gradient computation and inference
* G.1 Efficient gradient computation for \(\mathsf{L}_{\log}^{\,\mathrm{comp}}\)
* G.2 Efficient gradient computation for \(\mathsf{L}_{\mathrm{exp}}^{\,\mathrm{comp}}\)
* G.3 Efficient Inference
Related work

**Structured prediction and neural networks.** A variety of deep learning techniques have been used for structured prediction tasks, including a unified neural network architecture for natural language processing [Collobert et al., 2011], energy model-based structured prediction including structured prediction energy networks (SPENs) and various inference methods [Belanger and McCallum, 2016, Tu and Gimpel, 2018, Larsson et al., 2018, Tu and Gimpel, 2019, Tu et al., 2019, Graber and Schwing, 2019, Pan et al., 2020], attention networks incorporating richer structural distributions [Kim et al., 2017], tree-structured long short-term memory (LSTM) networks [Tai et al., 2015], sequence to sequence learning [Sutskever et al., 2014, Edunov et al., 2017], memory-reduced variant of best-first beam search [Meister et al., 2020], end-to-end learning for SPENs [Belanger et al., 2017], conditional generative flow [Lu and Huang, 2020], proximal methods [Wang et al., 2016], CRF using deep features [Jaderberg et al., 2014, Huang et al., 2015, Chen et al., 2014, Schwing and Urtasun, 2015, Chen et al., 2015], non-iterative feed-forward predictors [Stoyanov et al., 2011, Domke, 2013, Kunisch and Pock, 2013, Hershey et al., 2014, Li and Zemel, 2014, Belharbi et al., 2018, Zheng et al., 2015], deep value network [Gygli et al., 2017], fully convolutional networks [Long et al., 2015], constraint reasoning tool [Dragone et al., 2021, Jiang et al., 2022], multi-label box model[Patel et al., 2022], probabilistic deep networks[Zheng and Pronobis, 2019, Jang et al., 2023], autoregressive methods [Liu et al., 2022], nonlinear output transformations and embedding [Graber et al., 2018, Brogat-Motte et al., 2020], smoothing methods [Pillutla et al., 2018] and structural training [Choi et al., 2016, Ahmad et al., 2023]. Let us also mention ensemble algorithms for structured prediction algorithms [Cortes, Kuznetsov, and Mohri, 2014a,b], which can be used to combine several algorithms for this problem.

**Consistency in structured prediction.** Here, we discuss in detail previous work on consistency in structured prediction [Osokin et al., 2017, Ciliberto et al., 2016, Blondel, 2019, Nowak et al., 2020, 2022, Ciliberto et al., 2019, Nowak-Vila et al., 2019, Nowak et al., 2019, Cabannes et al., 2021, Cabannes et al., 2020, Corro, 2023].

Osokin et al. [2017], Nowak et al. [2020] and Nowak et al. [2022] pointed out that the max-margin Markov networks (M3N), or more generally structural SVMs may not be Bayes-consistent. Instead, Osokin et al. [2017] proposed the first Bayes-consistent surrogate loss in the structured prediction setting, the quadratic surrogate (QS) loss. A general theory of QS was further developed in [Nowak-Vila et al., 2019, Nowak et al., 2019]. However, as pointed out in Section 1, the quadratic surrogate loss formulation casts the structured prediction problem as a regression problem and is not a typical formulation even in the binary classification case.

Nowak et al. [2020] proposed a consistent method called _max-min margin Markov networks (M4N)_ derived from first principles for binary SVM. However, this method is restricted to SVM-type loss functions. Instead, we propose broad families of surrogate losses, which can be naturally derived from common multi-class losses including the logistic loss.

Nowak et al. [2022] addressed the inconsistency of Max-Margin loss in structured prediction by introducing the notion of Restricted-Max-Margin, where the maximization is performed over a subset of the original domain. Their method is based on an implicit embedding [Finocchiaro et al., 2019]; a general framework for structured prediction has been further developed by Ciliberto et al. [2020]. However, these methods are only applied to polyhedral-type surrogates which are not as smooth as the logistic loss. Thus, the resulting surrogate losses may not be favorable from the optimization point of view. Instead, our novel families of surrogate losses are very general and can be smooth, including a new structured logistic loss, for which we describe efficient gradient computation algorithms.

Ciliberto et al. [2016] focused on a least squares surrogate loss function and corresponding framework. In this framework, the structured prediction problem is cast as a regression problem. They derived a regularization approach to structured prediction from the least squares surrogate loss and proved the Bayes-consistency of that approach. Ciliberto et al. [2019] focused on a local structure-adapted framework for structured prediction. They proposed a novel structured prediction algorithm that adaptively leverages locality in the learning problem. Ciliberto et al. [2020] developed a general framework for structured prediction based on implicit embedding. Their methods lead to polyhedral-type surrogates losses that benefit from Bayes-consistency.

On the other hand, our work presents an extensive study of surrogate losses for structured prediction supported by \(\mathcal{H}\)-consistency bounds. Different from the surrogate loss studied in the previous work,the formulations of our proposed surrogate losses including structured comp-sum losses and structured constrained losses are completely novel and do not cast structured prediction problems as a regression problem. Furthermore, we prove stronger consistency guarantees that imply Bayes-consistency for these new proposed families of surrogate loss.

Other related work on structured prediction includes: projection-based losses for structured prediction [Blondel, 2019]; fast convergence rates for general structured prediction problems [Cabannes et al., 2021]; a unified framework for dealing with partial labelling [Cabannes et al., 2020]; and an analysis of the inconsistency of separable negative log-likelihood losses for structured prediction [Corro, 2023].

## Appendix B Proof of Lemma 3

**Lemma 3**.: _The best-in-class conditional error and the conditional regret for a target loss \(\mathsf{L}\) in structured prediction can be expressed as follows:_

\[\mathcal{C}_{\mathsf{L},\mathcal{H}}^{\mathsf{*}}(x) =\min_{y^{\prime}\in\mathsf{H}(x)}\sum_{y\in\mathcal{Y}}p(x,y) \ell(y^{\prime},y)\] \[\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x) =\sum_{y\in\mathcal{Y}}p(x,y)\ell(\mathsf{h}(x),y)-\min_{y^{\prime }\in\mathsf{H}(x)}\sum_{y^{\prime}\in\mathcal{Y}}p(x,y)\ell(y^{\prime},y).\]

Proof.: By the definition, the conditional \(\mathsf{L}\)-risk can be expressed as follows:

\[\mathcal{C}_{\mathsf{L}}(h,x)=\sum_{y\in\mathcal{Y}}p(x,y)\ell(\mathsf{h}(x), y).\] (15)

Since \(\{\mathsf{h}(x):h\in\mathcal{H}\}=\mathsf{H}(x)\), the best-in-class conditional error can be expressed as follows:

\[\mathcal{C}_{\mathsf{L},\mathcal{H}}^{\mathsf{*}}(x)=\min_{y^{\prime}\in \mathsf{H}(x)}\sum_{y\in\mathcal{Y}}p(x,y)\ell(y^{\prime},y),\]

which proves the first part of the lemma. By the definition,

\[\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x)=\mathcal{C}_{\mathsf{L}}(h,x) -\mathcal{C}_{\mathsf{L},\mathcal{H}}^{\mathsf{*}}(x)=\sum_{y\in\mathcal{Y}}p (x,y)\ell(\mathsf{h}(x),y)-\min_{y^{\prime}\in\mathsf{H}(x)}\sum_{y\in \mathcal{Y}}p(x,y)\ell(y^{\prime},y).\]

## Appendix C Proofs for structured max losses

**Theorem 4** (**Negative results of \(\mathsf{L}^{\max}\)**).: _Assume that \(n>2\) and that \(\Phi_{u}(v)\) is convex and non-increasing for \(u=1\). Then, the max structured loss \(\mathsf{L}^{\max}\) is not Bayes-consistent._

Proof.: For the structured max loss \(\mathsf{L}^{\max}\), the conditional \(\mathsf{L}^{\max}\)-risk can be expressed as follows:

\[\mathcal{C}_{\mathsf{L}^{\max}}(h,x)=\sum_{y\in\mathcal{Y}}p(x,y)\max_{y^{ \prime}\ast y}\Phi_{\ell(y^{\prime},y)}(h(x,y)-h(x,y^{\prime})).\]

Take \(\ell(y^{\prime},y)=\mathds{1}_{y\ast y^{\prime}}\) to be the zero-one loss. Since \(\ell(y^{\prime},y)=1\) for any \(y\neq y^{\prime}\), the conditional \(\mathsf{L}^{\max}\)-risk can be reformulated as follows:

\[\mathcal{C}_{\mathsf{L}^{\max}}(h,x)=\sum_{y\in\mathcal{Y}}p(x,y)\max_{y^{ \prime}\ast y}\Phi_{1}(h(x,y)-h(x,y^{\prime})).\]

Consider the distribution that supports on a singleton domain \(\{x\}\). Take \(y_{1}\neq y_{2}\in\mathcal{Y}\) such that \(y_{1}\neq n\) and \(y_{2}\neq n\). We define the conditional distribution as \(p(x,y_{1})=p(x,y_{2})=\frac{1}{2}\) and \(p(x,y)=0\) for other \(y\in\mathcal{Y}\). Then, by using the fact that \(\Phi_{1}(v)\) is convex and non-increasing, we obtain

\[\mathcal{R}_{\mathsf{L}^{\max}}(h)=\mathcal{C}_{\mathsf{L}^{\max} }(h,x) =\frac{1}{2}\max_{y^{\prime}\ast y_{1}}\Phi_{1}(h(x,y_{1})-h(x,y^{ \prime}))+\frac{1}{2}\max_{y^{\prime}\ast y_{2}}\Phi_{1}(h(x,y_{2})-h(x,y^{ \prime}))\] \[=\frac{1}{2}\Phi_{1}\bigg{(}h(x,y_{1})-\max_{y^{\prime}\ast y_{1}} h(x,y^{\prime})\bigg{)}+\frac{1}{2}\Phi_{1}\bigg{(}h(x,y_{2})-\max_{y^{\prime} \ast y_{2}}h(x,y^{\prime})\bigg{)}\] \[\geq\Phi_{1}\bigg{(}\frac{1}{2}h(x,y_{1})-\frac{1}{2}\max_{y^{ \prime}\ast y_{2}}h(x,y^{\prime})+\frac{1}{2}h(x,y_{2})-\frac{1}{2}\max_{y^{ \prime}\ast y_{1}}h(x,y^{\prime})\bigg{)}\] \[\geq\Phi_{1}(0) (\Phi_{1}(v)\text{ is non-increasing})\]where the equality can be achieved by \(h^{*}\in\mathcal{H}\), defined as \(h^{*}(x,1)=h^{*}(x,2)=\ldots=h^{*}(x,n)\). Therefore, \(h^{*}\) is a Bayes classifier of the structured max loss. Note that \(\mathsf{h}^{*}(x)=n\). However, by Lemma 3, in such a case, the Bayes classifier \(h^{*}_{\ell}\) of the target loss satisfies that

\[\mathsf{h}^{*}_{\ell}(x)=\operatorname*{argmin}_{y^{\prime}\in\mathcal{H}} \sum_{y\in\mathcal{H}}p(x,y)\ell(y^{\prime},y)=\operatorname*{argmin}_{y^{ \prime}\in\mathcal{H}}(p(x,y_{1})\mathds{1}_{y^{\prime}xy_{1}}+p(x,y_{2}) \mathds{1}_{y^{\prime}xy_{2}})=y_{1}\text{ or }y_{2}.\]

Thus, we obtain \(\mathsf{h}^{*}\neq\mathsf{h}^{*}_{\ell}\). Therefore, \(\mathsf{L}^{\max}\) is not Bayes-consistent. 

## Appendix D Proofs for Voted Conditional Random Field

**Theorem 5** (Negative result of \(\mathsf{L}^{\textsc{vcbf}}\)).: _The Voted Conditional Random Field \(\mathsf{L}^{\textsc{vcbf}}\) is not Bayes-consistent._

Proof.: For the Voted Conditional Random Field \(\mathsf{L}^{\textsc{vcbf}}\), the conditional \(\mathsf{L}^{\textsc{vcbf}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\mathsf{L}^{\textsc{vcbf}}}(h,x)=\sum_{y\in\mathcal{H}}p(x,y) \log\Biggl{(}\sum_{y^{\prime}\in\mathcal{H}}e^{\ell(y,y^{\prime})+h(x,y^{ \prime})-h(x,y)}\Biggr{)}.\]

Consider the distribution that supports on a singleton domain \(\{x\}\). Note that \(\mathcal{R}_{\mathsf{L}^{\textsc{vcbf}}}=\mathcal{C}_{\mathsf{L}^{\textsc{vcbf}}}\) is convex with respect to \(h(x,y),y=1,\ldots,n\). To find the global minimum, we will differentiate \(\mathcal{C}_{\mathsf{L}^{\textsc{vcbf}}}\) with respect to \(h(x,y)\) for any \(y\in\mathcal{V}\) and setting the derivatives to zero. Thus, we obtain for any \(y\in\mathcal{V}\),

\[p(x,y)\frac{-\sum_{y^{\prime}\neq y}e^{\ell(y,y^{\prime})+h(x,y^{\prime})-h(x, y)}}{\sum_{y^{\prime}\in\mathcal{V}}e^{\ell(y,y^{\prime})+h(x,y^{\prime})-h(x,y)}}+ \sum_{y^{\prime}\neq y}p(x,y^{\prime})\frac{e^{\ell(y^{\prime},y)+h(x,y)-h(x, y^{\prime})}}{\sum_{y^{\prime\prime}\in\mathcal{V}}e^{\ell(y^{\prime},y^{\prime \prime})+h(x,y^{\prime\prime})-h(x,y^{\prime})}}=0.\] (16)

Using the fact that \(\sum_{y^{\prime}\neq y}e^{\ell(y,y^{\prime})+h(x,y^{\prime})-h(x,y)}=\sum_{y^{ \prime}\in\mathcal{V}}e^{\ell(y,y^{\prime})+h(x,y^{\prime})-h(x,y)}-e^{\ell(y,y )+h(x,y)-h(x,y)}\) to further simplify the LHS of (16), we obtain for any \(y\in\mathcal{V}\),

\[p(x,y)=\sum_{y^{\prime}\in\mathcal{V}}p(x,y^{\prime})\frac{e^{\ell(y^{\prime},y)+h(x,y)-h(x,y^{\prime})}}{\sum_{y^{\prime\prime}\in\mathcal{V}}e^{\ell(y^{ \prime},y^{\prime\prime})+h(x,y^{\prime\prime})-h(x,y^{\prime})}}=\sum_{y^{ \prime}\in\mathcal{V}}p(x,y^{\prime})\frac{e^{\ell(y^{\prime},y^{\prime})+h(x,y)}}{\sum_{y^{\prime\prime}\in\mathcal{V}}e^{\ell(y^{\prime},y^{\prime\prime}) +h(x,y^{\prime\prime})}}.\] (17)

Consider a target loss function \(\mathsf{L}\) such that \(e^{\ell(y,y^{\prime})}=\Phi_{y}\Phi_{y^{\prime}}\), that is \(\ell(y,y^{\prime})=\log(\Phi_{y})+\log(\Phi_{y^{\prime}})\), where \(\Phi_{y}\) is a function mapping from \(\mathcal{V}\) to \(\mathbb{R}_{+}\). For this special choice of the target loss function, the expression of \(\ell(y,y^{\prime})\) decouple and (17) can be simplified to

\[p(x,y)=\sum_{y^{\prime}\in\mathcal{V}}p(x,y^{\prime})\frac{\Phi_{y}\Phi_{y^{ \prime}}e^{h(x,y)}}{\sum_{y^{\prime\prime}\in\mathcal{V}}\Phi_{y^{\prime}}\Phi _{y^{\prime\prime}}e^{h(x,y^{\prime\prime})}}=\frac{\Phi_{y}e^{h(x,y)}}{\sum_{y ^{\prime}\in\mathcal{V}}\Phi_{y^{\prime}}e^{h(x,y^{\prime})}}.\] (18)

Therefore, for the Bayes classifier \(h^{*}\) of Voted Conditional Random Field, by (18), we have

\[\frac{\Phi_{y}e^{h^{*}(x,1)}}{p(x,1)}=\ldots=\frac{\Phi_{y}e^{h^{*}(x,n)}}{p(x,n)}\]

which implies that

\[\mathsf{h}^{*}(x)=\operatorname*{argmax}_{y^{\prime}\in\mathcal{V}}h^{*}(x,y^{ \prime})=\operatorname*{argmax}_{y^{\prime}\in\mathcal{V}}e^{h^{*}(x,y^{\prime })}=\operatorname*{argmax}_{y^{\prime}\in\mathcal{V}}\frac{p(x,y^{\prime})}{ \Phi_{y^{\prime}}}.\]

However, by Lemma 3, in such a case, the Bayes classifier \(h^{*}_{\ell}\) of the target loss satisfies that

\[\mathsf{h}^{*}_{\ell}(x)=\operatorname*{argmin}_{y^{\prime}\in\mathcal{V}}\sum_ {y\in\mathcal{V}}p(x,y)\ell(y^{\prime},y)=\operatorname*{argmin}_{y^{\prime} \in\mathcal{V}}\sum_{y\in\mathcal{V}}p(x,y)\bigl{(}\log(\Phi_{y})+\log(\Phi_{y ^{\prime}})\bigr{)}=\operatorname*{argmin}_{y^{\prime}\in\mathcal{V}}\Phi_{y^{ \prime}}.\]

Thus, we obtain \(\mathsf{h}^{*}\neq\mathsf{h}^{*}_{\ell}\) in general. Indeed, consider the case where \(p(x,y)=\frac{\Phi_{y}^{2}}{\sum_{h=1}^{n}\Phi_{k}^{2}},y\in\mathcal{V}\). Then, \(\mathsf{h}^{*}(x)=\operatorname*{argmax}_{y^{\prime}\in\mathcal{V}}\frac{\Phi_{y ^{\prime}}}{\sum_{h=1}^{n}\Phi_{k}^{2}}=\operatorname*{argmax}_{y^{\prime}\in \mathcal{V}}\Phi_{y^{\prime}}\neq\operatorname*{argmin}_{y^{\prime}\in \mathcal{V}}\Phi_{y^{\prime}}=\mathsf{h}^{*}_{\ell}(x)\) when \(\{\Phi_{y}:y\in\mathcal{V}\}\) are not equal. Therefore, \(\mathsf{L}^{\textsc{vcbf}}\) is not Bayes-consistent.

Proofs for structured comp-sum losses

### Structured logistic loss

**Theorem 11** (\(\mathcal{H}\)**-consistency bound of \(\mathsf{L}_{\log}^{\mathrm{comp}}\))**.: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any target loss \(\ell\), hypothesis \(h\in\mathcal{H}\) and any distribution,_

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\leq 2 \Big{(}\mathcal{R}_{\mathsf{L}_{\log}^{\mathrm{comp}}}(h)-\mathcal{R}_{ \mathsf{L}_{\log}^{\mathrm{comp}},\mathcal{H}}^{*_{\mathrm{comp}}}+\mathcal{M} _{\mathsf{L}_{\log}^{\mathrm{comp}},\mathcal{H}}\Big{)}^{\frac{1}{2}}- \mathcal{M}_{\mathsf{L},\mathcal{H}}.\] (19)

Proof.: For the comp-sum structured loss \(\mathsf{L}_{\log}^{\mathrm{comp}}\), the conditional \(\mathsf{L}_{\log}^{\mathrm{comp}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\mathsf{L}_{\log}^{\mathrm{comp}}}(h,x) =-\sum_{y\in\mathcal{Y}}p(x,y)\sum_{y^{\prime}\in\mathcal{Y}} \overline{\ell}(y^{\prime},y)\log\!\left(\frac{e^{h(x,y^{\prime})}}{\sum_{y^{ \prime\prime}\in\mathcal{Y}}e^{h(x,y^{\prime\prime})}}\right)\] \[=-\sum_{y^{\prime}\in\mathcal{Y}}\log\!\left(\frac{e^{h(x,y^{ \prime})}}{\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h(x,y^{\prime\prime})}} \right)\sum_{y\in\mathcal{Y}}p(x,y)\overline{\ell}(y^{\prime},y)\] \[=-\sum_{y^{\prime}\in\mathcal{Y}}\log(\mathcal{S}(x,y^{\prime})) \overline{q}(x,y^{\prime}),\]

where we denote by \(\overline{q}(x,y^{\prime})=\sum_{y\in\mathcal{Y}}p(x,y)\overline{\ell}(y^{ \prime},y)\in[0,1]\) and \(\mathcal{S}(x,y)=\frac{e^{h(x,y)}}{\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h( x,y^{\prime\prime})}}\in[0,1]\) with the constraint that \(\sum_{y\in\mathcal{Y}}\mathcal{S}(x,y)=1\). Let \(y_{\min}=\operatorname{argmin}_{y^{\prime\prime}\in\mathcal{Y}}p(x,y)\ell(y^ {\prime},y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. For any \(h\in\mathcal{H}\) such that \(\mathsf{h}(x)\neq y_{\min}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\left\{\overline{h}_{\mu}:\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x, \mathsf{h}(x))]\right\}\subset\mathcal{H}\) such that \(\overline{\mathcal{S}}_{\mu}(x,\cdot)=\frac{e^{h_{\mu}(x,\cdot)}}{\sum_{y^{ \prime}\in\mathcal{Y}}e^{h_{\mu}(x,y^{\prime})}}\) take the following values:

\[\overline{\mathcal{S}}_{\mu}(x,y)=\begin{cases}\mathcal{S}(x,y)&\text{if }y \not\in\{y_{\min},\mathsf{h}(x)\}\\ \mathcal{S}(x,y_{\min})+\mu&\text{if }y=\mathsf{h}(x)\\ \mathcal{S}(x,\mathsf{h}(x))-\mu&\text{if }y=y_{\min}.\end{cases}\] (20)

Note that \(\overline{\mathcal{S}}_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}\overline{\mathcal{S}}_{\mu}(x,y)=\sum_{y\in\mathcal{Y}} \mathcal{S}(x,y)=1,\;\forall\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x, \mathsf{h}(x))].\]

By (20) and using the fact that \(\mathsf{H}(x)=\mathcal{Y}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{C}_{\mathsf{L}^{\mathrm{comp}},\mathcal{H}}(h,x) =\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(h,x)-\mathcal{C}_{\mathsf{ L}^{\mathrm{comp}}}^{*}(\mathcal{H},x)\] \[\geq\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(h,x)-\inf_{\mu\in \mathbb{R}}\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(\overline{h}_{\mu},x)\] \[=\sup_{\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x,\mathsf{h}(x) )]}\left\{\overline{q}(x,y_{\min})[-\log(\mathcal{S}(x,y_{\min}))+\log( \mathcal{S}(x,\mathsf{h}(x))-\mu)\right]\] \[\qquad+\overline{q}(x,\mathsf{h}(x))[-\log(\mathcal{S}(x,\mathsf{ h}(x)))+\log(\mathcal{S}(x,y_{\min})+\mu)]\right\}.\]

Differentiating with respect to \(\mu\) yields the optimal value \(\mu^{*}=\frac{\overline{q}(x,\mathsf{h}(x))\mathcal{S}(x,\mathsf{h}(x))- \overline{q}(x,y_{\min})\mathcal{S}(x,y_{\min})}{\overline{q}(x,y_{\min})+ \overline{q}(x,\mathsf{h}(x))}\). Plugging in that value gives:

\[\Delta\mathcal{C}_{\mathsf{L}^{\mathrm{comp}},\mathcal{H}}(h,x) \geq\overline{q}(x,y_{\min})\log\frac{(\mathcal{S}(x,\mathsf{h}(x))+ \mathcal{S}(x,y_{\min}))\overline{q}(x,y_{\min})}{\mathcal{S}(x,y_{\min})( \overline{q}(x,y_{\min})+\overline{q}(x,\mathsf{h}(x)))}\] \[\qquad+\overline{q}(x,\mathsf{h}(x))\log\frac{(\mathcal{S}(x, \mathsf{h}(x))+\mathcal{S}(x,y_{\min}))\overline{q}(x,\mathsf{h}(x))}{ \overline{\mathcal{S}}(x,\mathsf{h}(x))(\overline{q}(x,y_{\min})+\overline{q}( x,\mathsf{h}(x)))}.\]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_EMPTY:23]

Proof.: For the comp-sum structured loss \(\mathsf{L}_{\mathrm{gce}}^{\mathrm{comp}}\), the conditional \(\mathsf{L}_{\mathrm{gce}}^{\mathrm{comp}}\)-risk can be expressed as follows:

\[\begin{split}\mathcal{C}_{\mathsf{L}_{\mathrm{gce}}^{\mathrm{comp}} }(h,x)&=\sum_{y\in\mathcal{Y}}p(x,y)\,\sum_{y^{\prime}\in \mathcal{Y}}\overline{\ell}(y^{\prime},y)\frac{1}{\alpha}\Bigg{(}1-\Bigg{(} \frac{e^{h(x,y^{\prime})}}{\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h(x,y^{ \prime\prime})}}\Bigg{)}^{\alpha}\Bigg{)}\\ &=\frac{1}{\alpha}\sum_{y^{\prime}\in\mathcal{Y}}\Bigg{(}1-\Bigg{(} \frac{e^{h(x,y^{\prime})}}{\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h(x,y^{ \prime\prime})}}\Bigg{)}^{\alpha}\Bigg{)}\sum_{y\in\mathcal{Y}}p(x,y)\overline{ \ell}(y^{\prime},y)\\ &=\frac{1}{\alpha}\sum_{y^{\prime}\in\mathcal{Y}}(1-\mathcal{S}(x,y^{\prime})^{\alpha})\overline{q}(x,y^{\prime}),\end{split}\]

where we denote by \(\overline{q}(x,y^{\prime})=\sum_{y\in\mathcal{Y}}p(x,y)\overline{\ell}(y^{ \prime},y)\in[0,1]\) and \(\mathcal{S}(x,y)=\frac{e^{h(x,y)}}{\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h( x,y^{\prime\prime})}}\in[0,1]\) with the constraint that \(\sum_{y\in\mathcal{Y}}\mathcal{S}(x,y)=1\). Let \(y_{\min}=\operatorname*{argmin}_{y^{\prime}\in\mathcal{Y}}\sum_{y\in\mathcal{ Y}}p(x,y)\ell(y^{\prime},y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. For any \(h\in\mathcal{H}\) such that \(\mathsf{h}(x)\neq y_{\min}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\left\{\overline{h}_{\mu}:\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x, \mathsf{h}(x))]\right\}\subset\mathcal{H}\) such that \(\overline{\mathcal{S}}_{\mu}\big{(}x,\cdot\big{)}=\frac{e^{h_{\mu}(x,\cdot)} }{\sum_{y^{\prime}\in\mathcal{Y}}e^{h_{\mu}(x,y^{\prime})}}\) take the following values:

\[\overline{\mathcal{S}}_{\mu}(x,y)=\begin{cases}\mathcal{S}(x,y)&\text{if }y \not\in\{y_{\min},\mathsf{h}(x)\}\\ \mathcal{S}(x,y_{\min})+\mu&\text{if }y=\mathsf{h}(x)\\ \mathcal{S}(x,\mathsf{h}(x))-\mu&\text{if }y=y_{\min}.\end{cases}\] (24)

Note that \(\overline{\mathcal{S}}_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}\overline{\mathcal{S}}_{\mu}(x,y)=\sum_{y\in\mathcal{Y}} \mathcal{S}(x,y)=1,\;\forall\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x, \mathsf{h}(x))].\]

By (24) and using the fact that \(\mathsf{H}(x)=\mathcal{Y}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\begin{split}&\Delta\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}, \mathcal{H}}(h,x)\\ &=\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(h,x)-\mathcal{C}_{ \mathsf{L}^{\mathrm{comp}}}^{*}(\mathcal{H},x)\\ &\geq\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(h,x)-\inf_{\mu\in \mathcal{H}}\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(\overline{h}_{\mu},x)\\ &=\frac{1}{\alpha}\sup_{\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x, \mathsf{h}(x))]}\left\{\overline{q}(x,y_{\min})\big{[}-\mathcal{S}(x,y_{\min} )^{\alpha}+\left(\mathcal{S}(x,\mathsf{h}(x))-\mu\right)^{\alpha}\right]\\ &\qquad+\overline{q}(x,\mathsf{h}(x))\big{[}-\mathcal{S}(x, \mathsf{h}(x))^{\alpha}+\left(\mathcal{S}(x,y_{\min})+\mu\right)^{\alpha}\big{]} \Bigg{\}}\\ &=\frac{1}{\alpha}(\mathcal{S}(x,\mathsf{h}(x))+\mathcal{S}(x,y_{ \min}))^{\alpha}\Big{(}\overline{q}(x,y_{\min})^{\frac{1}{1-\alpha}}+\overline{ q}(x,\mathsf{h}(x))^{\frac{1}{1-\alpha}}\Big{)}^{1-\alpha}\\ &\qquad-\frac{1}{\alpha}\overline{q}(x,y_{\min})\mathcal{S}(x,y_{ \min})^{\alpha}-\frac{1}{\alpha}\overline{q}(x,\mathsf{h}(x))\mathcal{S}(x, \mathsf{h}(x))^{\alpha}\end{split}\]

(differentiating with respect to \(\mu\) to optimize, optimum \(\mu^{*}=\frac{\overline{q}(x,\mathsf{h}(x))\frac{1}{1-\alpha}\mathcal{S}(x, \mathsf{h}(x))-\overline{q}(x,y_{\min})\frac{1}{1-\alpha}\mathcal{S}(x,y_{\min} )}{\overline{q}(x,y_{\min})\frac{1}{1-\alpha}\mathcal{S}(x,y_{\min})}\)

\[\geq\frac{1}{\alpha n^{\alpha}}\bigg{[}2^{\alpha}\Big{(}\overline{q}(x,y_{\min}) ^{\frac{1}{1-\alpha}}+\overline{q}(x,\mathsf{h}(x))^{\frac{1}{1-\alpha}} \Big{)}^{1-\alpha}-\overline{q}(x,y_{\min})-\overline{q}(x,\mathsf{h}(x))\bigg{]}\]

(differentiating with respect to \(\mathcal{S}\) to minimize, minimum is attained when \(\mathcal{S}(x,\mathsf{h}(x))=\mathcal{S}(x,y_{\min})=\frac{1}{n}\))

\[\begin{split}&\geq\frac{\left(\overline{q}(x,\mathsf{h}(x))- \overline{q}(x,y_{\min})\right)^{2}}{4n^{\alpha}}\ \ \left(\left(\frac{\frac{1}{1-\alpha}+b\frac{1}{1-\alpha}}{2}\right)^{1-\alpha}- \frac{a+b}{2}\geq\frac{\alpha}{4}(a-b)^{2},\forall a,b\in[0,1],\,0\leq a+b\leq 1 \right)\\ &=\frac{\left(\sum_{y\in\mathcal{Y}}p(x,y)\ell(\mathsf{h}(x),y)- \sum_{y\in\mathcal{Y}}p(x,y)\ell(y_{\min},y)\right)^{2}}{4n^{\alpha}}\\ &=\frac{\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x)^{2}}{4n^{ \alpha}}.\end{split}\] (by Lemma 3 and \(\mathsf{H}(x)=\mathcal{Y}\))Since the function \(t\mapsto\frac{t^{2}}{4n^{\alpha}}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\frac{\big{(}\mathcal{R}_{\text{L}}(h)-\mathcal{R}_{\text{L}, \mathcal{H}}^{*}+\mathcal{M}_{\text{L},\mathcal{H}}\big{)}^{2}}{4n^{\alpha}} =\frac{\big{(}\mathbb{E}_{X}\big{[}\Delta\mathcal{C}_{\text{L}, \mathcal{H}}(h,x)\big{]}\big{)}^{2}}{4n^{\alpha}}\] \[\leq\mathbb{E}\bigg{[}\frac{\Delta\mathcal{C}_{\text{L},\mathcal{ H}}(h,x)^{2}}{4n^{\alpha}}\bigg{]}\] \[\leq\mathbb{E}\big{[}\Delta\mathcal{C}_{\text{L}^{\text{comp}}, \mathcal{H}}(h,x)\big{]}\] \[=\mathcal{R}_{\text{L}^{\text{comp}}}(h)-\mathcal{R}_{\text{L}^{ \text{comp}},\mathcal{H}}^{*}+\mathcal{M}_{\text{L}^{\text{comp}},\mathcal{H}},\]

which leads to

\[\mathcal{R}_{\text{L}}(h)-\mathcal{R}_{\text{L},\mathcal{H}}^{*}\leq 2n^{ \frac{\alpha}{2}}\Big{(}\mathcal{R}_{\text{L}^{\text{comp}}_{\text{ge}}}(h)- \mathcal{R}_{\text{L}^{\text{comp}}_{\text{ge}},\mathcal{H}}^{*}+\mathcal{M}_ {\text{L}^{\text{comp}}_{\text{ge}},\mathcal{H}}\Big{)}^{\frac{1}{2}}- \mathcal{M}_{\text{L},\mathcal{H}}.\]

### Structured mean absolute error loss

**Theorem 14** (\(\mathcal{H}\)-consistency bound of \(\mathsf{L}^{\text{comp}}_{\text{mae}}\)).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any target loss \(\ell\), hypothesis \(h\in\mathcal{H}\) and any distribution,_

\[\mathcal{R}_{\text{L}}(h)-\mathcal{R}_{\text{L},\mathcal{H}}^{*}\leq n\big{(} \mathcal{R}_{\text{L}^{\text{comp}}_{\text{mae}}}(h)-\mathcal{R}_{\text{L}^{ \text{comp}}_{\text{mae}},\mathcal{H}}^{*}+\mathcal{M}_{\text{L}^{\text{comp }}_{\text{mae}},\mathcal{H}}\big{)}-\mathcal{M}_{\text{L},\mathcal{H}}.\] (25)

Proof.: For the comp-sum structured loss \(\mathsf{L}^{\text{comp}}_{\text{mae}}\), the conditional \(\mathsf{L}^{\text{comp}}_{\text{mae}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\text{L}^{\text{comp}}_{\text{mae}}}(h,x) =\sum_{y\in\mathcal{Y}}p(x,y)\sum_{y^{\prime}\in\mathcal{Y}} \overline{\ell}(y^{\prime},y)\bigg{(}1-\frac{e^{h(x,y^{\prime})}}{\sum_{y^{ \prime\prime}\in\mathcal{Y}}e^{h(x,y^{\prime\prime})}}\bigg{)}\] \[=\sum_{y^{\prime}\in\mathcal{Y}}\bigg{(}1-\frac{e^{h(x,y^{\prime })}}{\sum_{y^{\prime\prime}\in\mathcal{Y}}e^{h(x,y^{\prime\prime})}}\sum_{y \in\mathcal{Y}}p(x,y)\overline{\ell}(y^{\prime},y)\] \[=\sum_{y^{\prime}\in\mathcal{Y}}\big{(}1-\mathcal{S}(x,y^{\prime })\big{)}\overline{q}(x,y^{\prime}),\]

where we denote by \(\overline{q}(x,y^{\prime})=\sum_{y\in\mathcal{Y}}p(x,y)\overline{\ell}(y^{ \prime},y)\in[0,1]\) and \(\mathcal{S}(x,y)=\frac{e^{h(x,y)}}{\sum_{y^{\prime\prime}\in\mathcal{Y}}p^{ h(x,y^{\prime\prime})}}\in[0,1]\) with the constraint that \(\sum_{y\in\mathcal{Y}}\mathcal{S}(x,y)=1\). Let \(y_{\min}=\operatorname*{argmin}_{y^{\prime}\in\mathcal{Y}}\sum_{y\in\mathcal{Y }}p(x,y)\ell(y^{\prime},y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. For any \(h\in\mathcal{H}\) such that \(\mathcal{h}(x)\neq y_{\min}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\big{\{}\overline{h}_{\mu}:\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x,\text {h}(x))]\big{\}}\subset\mathcal{H}\) such that \(\overline{\mathcal{S}}_{\mu}(x,\cdot)=\frac{e^{h_{\mu}(x,\cdot)}}{\sum_{y^{ \prime}\in\mathcal{Y}}e^{h_{\mu}(x,y^{\prime})}}\) take the following values:

\[\overline{\mathcal{S}}_{\mu}(x,y)=\begin{cases}\mathcal{S}(x,y)&\text{if }y\notin\{y_{\min},\text{h}(x)\}\\ \mathcal{S}(x,y_{\min})+\mu&\text{if }y=\text{h}(x)\\ \mathcal{S}(x,\text{h}(x))-\mu&\text{if }y=y_{\min}.\end{cases}\] (26)

Note that \(\overline{\mathcal{S}}_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}\overline{\mathcal{S}}_{\mu}(x,y)=\sum_{y\in\mathcal{Y}} \mathcal{S}(x,y)=1,\,\forall\mu\in[-\mathcal{S}(x,y_{\min}),\mathcal{S}(x, \text{h}(x))].\]By (26) and using the fact that \(\mathsf{H}(x)=\mathcal{Y}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{C}_{\mathsf{L}^{\mathrm{comp}},\mathcal{H}}(h,x)\] \[=\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(h,x)-\mathcal{C}_{ \mathsf{L}^{\mathrm{comp}}}^{*}(\mathcal{H},x)\] \[\geq\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(h,x)-\inf_{\mu\in \mathcal{H}}\mathcal{C}_{\mathsf{L}^{\mathrm{comp}}}(\overline{h}_{\mu},x)\] \[=\sup_{\mu\in\{-\mathcal{S}(x,y_{\min}),\mathcal{S}(x,\mathsf{h} (x))\}}\left\{\overline{q}(x,y_{\min})[-\mathcal{S}(x,y_{\min})+\mathcal{S}(x, \mathsf{h}(x))-\mu]\right.\] \[\qquad+\overline{q}(x,\mathsf{h}(x))[-\mathcal{S}(x,\mathsf{h}( x))+\mathcal{S}(x,y_{\min})+\mu]\right\}\] \[=\overline{q}(x,y_{\min})\mathcal{S}(x,\mathsf{h}(x))-\overline{ q}(x,\mathsf{h}(x))\mathcal{S}(x,\mathsf{h}(x))\] (differentiating with respect to \[\mu\] to optimize, optimum \[\mu^{*}=-\mathcal{S}(x,y_{\min})\] ) \[\geq\frac{1}{n}(\overline{q}(x,y_{\min})-\overline{q}(x,\mathsf{h }(x)))\] (differentiating with respect to \[\mathcal{S}\] to minimize, minimum is attained when \[\mathcal{S}(x,\mathsf{h}(x))=\frac{1}{n}\] \[=\frac{\sum_{y\in\mathcal{Y}}p(x,y)\ell(\mathsf{h}(x),y)-\sum_{y \in\mathcal{Y}}p(x,y)\ell(y_{\min},y)}{n}\] \[=\frac{\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x)}{n}\] (by Lemma 3 and \[\mathsf{H}(x)=\mathcal{Y}\] )

Therefore, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\frac{\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L}, \mathcal{H}}^{*}+\mathcal{M}_{\mathsf{L},\mathcal{H}}}{n} =\frac{\mathbb{E}_{X}\big{[}\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x)\big{]}}{n}\] \[=\mathbb{E}_{X}\big{[}\Delta\mathcal{C}_{\mathsf{L}^{\mathrm{ comp}},\mathcal{H}}(h,x)\big{]}\] \[=\mathcal{R}_{\mathsf{L}^{\mathrm{comp}}}(h)-\mathcal{R}_{ \mathsf{L}^{\mathrm{comp}},\mathcal{H}}^{*}+\mathcal{M}_{\mathsf{L}^{\mathrm{ comp}},\mathcal{H}},\]

which leads to

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\leq n \big{(}\mathcal{R}_{\mathsf{L}^{\mathrm{comp}}_{\mathrm{max}}}(h)-\mathcal{R}_ {\mathsf{L}^{\mathrm{comp}}_{\mathrm{max}},\mathcal{H}}^{*}+\mathcal{M}_{ \mathsf{L}^{\mathrm{comp}}_{\mathrm{max}},\mathcal{H}}\big{)}-\mathcal{M}_{ \mathsf{L},\mathcal{H}}.\]

## Appendix F Proofs for structured constrained losses

### Structured constrained exponential loss

**Theorem 15** (\(\mathcal{H}\)-consistency bound of \(\mathsf{L}^{\mathrm{crstnd}}_{\mathrm{exp}}\)).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any target loss \(\ell\), hypothesis \(h\in\mathcal{H}\) and any distribution,_

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\leq 2 \sqrt{\ell_{\max}}\Big{(}\mathcal{R}_{\mathsf{L}^{\mathrm{crstnd}}_{ \mathrm{exp}}}(h)-\mathcal{R}_{\mathsf{L}^{\mathrm{crstnd}}_{\mathrm{exp}}, \mathcal{H}}^{*}+\mathcal{M}_{\mathsf{L}^{\mathrm{crstnd}}_{\mathrm{exp}}, \mathcal{H}}\Big{)}^{\frac{1}{2}}-\mathcal{M}_{\mathsf{L},\mathcal{H}}.\] (27)

Proof.: Denote by \(q(x,y^{\prime})=\sum_{y\in\mathcal{Y}}p(x,y)\ell(y^{\prime},y)\in[0,\ell_{ \max}]\). For the constrained structured loss \(\mathsf{L}^{\mathrm{crstnd}}_{\mathrm{exp}}\), the conditional \(\mathsf{L}^{\mathrm{crstnd}}_{\mathrm{exp}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\mathsf{L}^{\mathrm{crstnd}}_{\mathrm{exp}}}(h,x)=\sum_{y\in \mathcal{Y}}p(x,y)\sum_{y^{\prime}\in\mathcal{Y}}\ell(y,y^{\prime})e^{h(x,y^ {\prime})}=\sum_{y^{\prime}\in\mathcal{Y}}e^{h(x,y^{\prime})}q(x,y^{\prime}).\]

Let \(y_{\min}=\operatorname*{argmin}_{y^{\prime}\in\mathcal{Y}}\sum_{y\in\mathcal{ Y}}p(x,y)\ell(y^{\prime},y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. For any \(h\in\mathcal{H}\) such that \(\mathsf{h}(x)\neq y_{\min}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\left\{\overline{h}_{\mu}:\mu\in\mathbb{R}\right\}\subset\mathcal{H}\) such that \(h_{\mu}(x,\cdot)\) take the following values:

\[\overline{h}_{\mu}(x,y)=\begin{cases}h(x,y)&\text{if }y\not\in\{y_{\min}, \mathsf{h}(x)\}\\ h(x,y_{\min})+\mu&\text{if }y=\mathsf{h}(x)\\ h(x,\mathsf{h}(x))-\mu&\text{if }y=y_{\min}.\end{cases}\] (28)

[MISSING_PAGE_EMPTY:27]

Proof.: Denote by \(q(x,y^{\prime})=\sum_{y\in\mathcal{Y}}p(x,y)\ell(y^{\prime},y)\in[0,\ell_{\max}]\). For the constrained structured loss \(\mathsf{L}^{\mathrm{cstnd}}_{\mathrm{sq-hinge}}\), the conditional \(\mathsf{L}^{\mathrm{cstnd}}_{\mathrm{sq-hinge}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\mathsf{L}^{\mathrm{cstnd}}_{\mathrm{sq-hinge}}}(h,x) =\sum_{y\in\mathcal{Y}}p(x,y)\sum_{y^{\prime}\in\mathcal{Y}}\ell(y^{ \prime},y)\max\{0,1+h(x,y^{\prime})\}^{2}\] \[=\sum_{y^{\prime}\in\mathcal{Y}}\max\{0,1+h(x,y^{\prime})\}^{2}q( x,y^{\prime}).\]

Let \(y_{\min}=\operatorname*{argmin}_{y^{\prime}\in\mathcal{Y}}\sum_{y\in \mathcal{Y}}p(x,y)\ell(y^{\prime},y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. For any \(h\in\mathcal{H}\) such that \(\mathsf{h}(x)\neq y_{\min}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\{\overline{h}_{\mu}:\mu\in\mathbb{R}\}\subset\mathcal{H}\) such that \(h_{\mu}(x,\cdot)\) take the following values:

\[\overline{h}_{\mu}(x,y)=\begin{cases}h(x,y)&\text{if }y\not\in\{y_{\min}, \mathsf{h}(x)\}\\ h(x,y_{\min})+\mu&\text{if }y=\mathsf{h}(x)\\ h(x,\mathsf{h}(x))-\mu&\text{if }y=y_{\min}.\end{cases}\] (30)

Note that the hypotheses \(\overline{h}_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}\overline{h}_{\mu}(x,y)=\sum_{y\in\mathcal{Y}}h(x,y)=0, \;\forall\mu\in\mathbb{R}.\]

Since \(\sum_{y\in\mathcal{Y}}h(x,y)=0\), there must be non-negative scores. By definition of \(\mathsf{h}(x)\) as a maximizer, we must thus have \(h(x,\mathsf{h}(x))\geq 0\). By (30) and using the fact that \(\mathsf{H}(x)=\mathcal{Y}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{C}_{\mathsf{L}^{\mathrm{cstnd}},\mathcal{H}}(h,x)\] \[=\mathcal{C}_{\mathsf{L}^{\mathrm{cstnd}}}(h,x)-\mathcal{C}_{ \mathsf{L}^{\mathrm{cstnd}}}^{*}(\mathcal{H},x)\] \[\geq\mathcal{C}_{\mathsf{L}^{\mathrm{cstnd}}}(h,x)-\inf_{\mu\in \mathbb{R}}\mathcal{C}_{\mathsf{L}^{\mathrm{cstnd}}}(\overline{h}_{\mu},x)\] \[=\sup_{\mu\in\mathbb{R}}\bigg{\{}q(x,y_{\min})\Big{(}\max\{0,1+h( x,y_{\min})\}^{2}-\max\{0,1+h(x,\mathsf{h}(x))-\mu\}^{2}\Big{)}\] \[+q(x,\mathsf{h}(x))\Big{(}\max\{0,1+h(x,\mathsf{h}(x))\}^{2}-\max \{0,1+h(x,y_{\min})+\mu\}^{2}\Big{)}\bigg{\}}\] \[\geq(1+h(x,\mathsf{h}(x)))^{2}(q(x,y_{\min})-q(x,\mathsf{h}(x)))^ {2}\quad\text{ (differentiating with respect to $\mu$ to optimize)}\] \[\geq(q(x,\mathsf{h}(x))-q(x,y_{\min}))^{2}\] ( \[h(x,\mathsf{h}(x))\geq 0\] ) \[=\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x)^{2}.\] (by Lemma 3 and \[\mathsf{H}(x)=\mathcal{Y}\] )

Since the function \(t\mapsto t^{2}\) is convex, by Jensen's inequality, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\big{(}\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}^{*}_{\mathsf{L}, \mathcal{H}}+\mathcal{M}_{\mathsf{L},\mathcal{H}}\big{)}^{2} =\left(\operatorname*{\mathbb{E}}[\Delta\mathcal{C}_{\mathsf{L}, \mathcal{H}}(h,x)]\right)^{2}\] \[\leq\operatorname*{\mathbb{E}}[\Delta\mathcal{C}_{\mathsf{L}, \mathcal{H}}(h,x)^{2}]\] \[\leq\operatorname*{\mathbb{E}}[\Delta\mathcal{C}_{\mathsf{L}^{ \mathrm{cstnd}},\mathcal{H}}(h,x)]\] \[=\mathcal{R}_{\mathsf{L}^{\mathrm{cstnd}}}(h)-\mathcal{R}^{*}_{ \mathsf{L}^{\mathrm{cstnd}},\mathcal{H}}+\mathcal{M}_{\mathsf{L}^{\mathrm{ cstnd}},\mathcal{H}},\]

which leads to

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}^{*}_{\mathsf{L},\mathcal{H}}\leq\left( \mathcal{R}_{\mathsf{L}^{\mathrm{cstnd}}_{\mathrm{sq-hinge}}}(h)-\mathcal{R}^{*}_ {\mathsf{L}^{\mathrm{cstnd}}_{\mathrm{sq-hinge}},\mathcal{H}}+\mathcal{M}_{ \mathsf{L}^{\mathrm{cstnd}}_{\mathrm{sq-hinge}},\mathcal{H}}\right)^{\frac{1}{2} }-\mathcal{M}_{\mathsf{L},\mathcal{H}}.\]

[MISSING_PAGE_FAIL:29]

### Structured constrained \(\rho\)-margin loss

**Theorem 18** (\(\mathcal{H}\)-consistency bound of \(\mathsf{L}_{\rho}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}\mathrm{a}\mathrm{t}}\)).: _Assume that \(\mathcal{H}\) is symmetric and complete. Then, for any target loss \(\ell\), hypothesis \(h\in\mathcal{H}\) and any distribution,_

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\leq \mathcal{R}_{\mathsf{L}_{\rho}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}}}(h)- \mathcal{R}_{\mathsf{L}_{\rho}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}}, \mathcal{H}}^{*}+\mathcal{M}_{\mathsf{L}_{\rho}^{\mathrm{c}\mathrm{t}\mathrm{a} \mathrm{t}},\mathcal{H}}-\mathcal{M}_{\mathsf{L},\mathcal{H}}.\] (33)

Proof.: Denote by \(q(x,y^{\prime})=\sum_{y\in\mathcal{Y}}p(x,y)\ell(y^{\prime},y)\in[0,\ell_{ \mathrm{max}}]\). For the constrained structured loss \(\mathsf{L}_{\rho}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}\mathrm{a}}\), the conditional \(\mathsf{L}_{\rho}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}}\)-risk can be expressed as follows:

\[\mathcal{C}_{\mathsf{L}_{\rho}^{\mathrm{c}\mathrm{t}\mathrm{a} \mathrm{t}}}(h,x) =\sum_{y\in\mathcal{Y}}p(x,y)\sum_{y^{\prime}\in\mathcal{Y}}\ell(y^ {\prime},y)\max\{0,1+h(x,y^{\prime})\}\] \[=\sum_{y^{\prime}\in\mathcal{Y}}\max\{0,1+h(x,y^{\prime})\}q(x,y^ {\prime}).\]

Let \(y_{\mathrm{min}}=\operatorname{argmin}_{y^{\prime}\in\mathcal{Y}}\sum_{y\in \mathcal{Y}}p(x,y)\ell(y^{\prime},y)\), where we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. For any \(h\in\mathcal{H}\) such that \(\mathsf{h}(x)\neq y_{\mathrm{min}}\) and \(x\in\mathcal{X}\), by the symmetry and completeness of \(\mathcal{H}\), we can always find a family of hypotheses \(\left\{\overline{h}_{\mu}:\mu\in\mathbb{R}\right\}\subset\mathcal{H}\) such that \(h_{\mu}(x,\cdot)\) take the following values:

\[\overline{h}_{\mu}(x,y)=\begin{cases}h(x,y)&\text{if }y\not\in\{y_{\mathrm{min}}, \mathsf{h}(x)\}\\ h(x,y_{\mathrm{min}})+\mu&\text{if }y=\mathsf{h}(x)\\ h(x,\mathsf{h}(x))-\mu&\text{if }y=y_{\mathrm{min}}.\end{cases}\] (34)

Note that the hypotheses \(\overline{h}_{\mu}\) satisfies the constraint:

\[\sum_{y\in\mathcal{Y}}\overline{h}_{\mu}(x,y)=\sum_{y\in\mathcal{Y}}h(x,y)=0, \;\forall\mu\in\mathbb{R}.\]

Since \(\sum_{y\in\mathcal{Y}}h(x,y)=0\), there must be non-negative scores. By definition of \(\mathsf{h}(x)\) as a maximizer, we must thus have \(h(x,\mathsf{h}(x))\geq 0\). By (34) and using the fact that \(\mathsf{H}(x)=\mathcal{Y}\) when \(\mathcal{H}\) is symmetric, we obtain

\[\Delta\mathcal{C}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a} \mathrm{t}\mathrm{a}\mathrm{t}},\mathcal{H}}(h,x)\] \[=\mathcal{C}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t }}}(h,x)-\mathcal{C}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}}}^{*} (\mathcal{H},x)\] \[\geq\mathcal{C}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a} \mathrm{t}}}(h,x)-\inf_{\mu\in\mathbb{R}}\mathcal{C}_{\mathsf{L}^{\mathrm{c} \mathrm{t}\mathrm{a}\mathrm{t}}}(\overline{h}_{\mu},x)\] \[=\sup_{\mu\in\mathbb{R}}\left\{q(x,y_{\mathrm{min}})\bigg{(} \min\bigg{\{}\max\bigg{\{}0,1+\frac{h(x,y_{\mathrm{min}})}{\rho}\bigg{\}},1 \bigg{\}}-\min\bigg{\{}\max\bigg{\{}0,1+\frac{h(x,\mathsf{h}(x))-\mu}{\rho} \bigg{\}},1\bigg{\}}\bigg{)}\right.\] \[+q(x,\mathsf{h}(x))\bigg{(}\min\bigg{\{}\max\bigg{\{}0,1+\frac{h( x,\mathsf{h}(x))}{\rho}\bigg{\}},1\bigg{\}}-\min\bigg{\{}\max\bigg{\{}0,1+\frac{h(x,y_{ \mathrm{min}})+\mu}{\rho}\bigg{\}},1\bigg{\}}\bigg{)}\right\}\] \[\geq q(x,\mathsf{h}(x))-q(x,y_{\mathrm{min}})\] (differentiating with respect to \[\mu\] to optimize) \[=\sum_{y\in\mathcal{Y}}p(x,y)\ell(\mathsf{h}(x),y)-\sum_{y\in \mathcal{Y}}p(x,y)\ell(y_{\mathrm{min}},y)\] \[=\Delta\mathcal{C}_{\mathsf{L},\mathcal{H}}(h,x).\] (by Lemma 3 and \(\mathsf{H}(x)=\mathcal{Y}\))

Therefore, we obtain for any hypothesis \(h\in\mathcal{H}\) and any distribution,

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}+ \mathcal{M}_{\mathsf{L},\mathcal{H}} =\operatorname*{\mathbb{E}}_{X}\big{[}\Delta\mathcal{C}_{\mathsf{L}, \mathcal{H}}(h,x)\big{]}\] \[\leq\operatorname*{\mathbb{E}}_{X}\big{[}\Delta\mathcal{C}_{ \mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}\mathrm{a}\mathrm{t}}, \mathcal{H}}(h,x)\big{]}\] \[=\mathcal{R}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t} \mathrm{a}\mathrm{t}}}(h)-\mathcal{R}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a} \mathrm{t}\mathrm{t}\mathrm{a}\mathrm{t}},\mathcal{H}}^{*}+\mathcal{M}_{\mathsf{ L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}\mathrm{a}\mathrm{t}}, \mathcal{H}},\]

which leads to

\[\mathcal{R}_{\mathsf{L}}(h)-\mathcal{R}_{\mathsf{L},\mathcal{H}}^{*}\leq\mathcal{R} _{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}}}(h)-\mathcal{R}_{ \mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}\mathrm{a}\mathrm{t}}, \mathcal{H}}^{*}+\mathcal{M}_{\mathsf{L}^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t} \mathrm{a}\mathrm{t}},\mathcal{H}}-\mathcal{M}_{\mathsf{L},\mathcal{H}}.\]Efficient gradient computation and inference

Here, we describe efficient algorithms for the computation of the gradients for the loss functions \(\mathsf{L}_{\log}^{\mathrm{comp}}\) and \(\mathsf{L}_{\exp}^{\mathrm{comp}}\). We also briefly discuss an efficient algorithm for inference.

### Efficient gradient computation for \(\mathsf{L}_{\log}^{\mathrm{comp}}\)

We first present an efficient algorithm for the computation of the quantities \(\mathsf{L}(\mathbf{z},s)\) in the important case of rational losses, next in the case of Markovian losses.

**Rational losses.**_Rational losses_ form a general family of loss functions based on rational kernels (Cortes et al., 2004) that includes, in particular, \(n\)-gram losses, which can be defined for a pair of sequences \((y,y^{\prime})\) as the negative inner product of the vectors of \(n\)-gram counts of \(y\) and \(y^{\prime}\).

Our algorithm bears some similarity to that of Cortes et al. (2018) for the computation of the gradient of the VCRF loss function. It is however distinct because the structured prediction loss function we are considering and our definition of rational loss are both different. We will adopt a similar notation and terminology. Recall that for any sequence \(y\), we denote by \(y_{i}\) the symbol in its \(i\)th position and by \(y_{i}^{j}=y_{i}y_{i+1}\cdots y_{j}\) the substring of \(y\) starting at position \(i\) and ending at \(j\). We denote by \(\mathsf{E}_{\mathcal{A}}\) the set of transitions of a WFA \(\mathcal{A}\).

Let \(\mathcal{U}\) be a weighted finite-state transducer (WFST) over the \((+,\times)\) semiring over the reals, with \(\Delta\) as both the input and output alphabet. Then, we define the rational loss associated to \(\mathcal{U}\) for all \(y,y^{\prime}\in\Delta^{*}\) by \(\overline{\ell}(y,y^{\prime})=\mathcal{U}(y,y^{\prime})\).

Let \(\overline{\mathcal{Y}}\) denote a WFA over the \((+,\times)\) semiring accepting the set of all sequences of length \(l\) with weight one and let \(\mathcal{Y}_{i}\) denote the WFA accepting only \(y_{i}\) with weight one. Then, by definition, the weighted transducer \(\overline{\mathcal{Y}}\circ\mathcal{U}\circ\mathcal{Y}_{i}\) obtained by composition maps each sequence \(y\) in \(\Delta^{l}\) to \(y_{i}\) with weight \(\mathcal{U}(y,y_{i})\). The WFA \(\Pi_{1}(\overline{\mathcal{Y}}\circ\mathcal{U}\circ\mathcal{Y}_{i})\) derived from that transducer by projection on the input (that is by removing output labels) is associating to each sequence \(y\) weight \(\mathcal{U}(y,y_{i})\). WFA \(\overline{\mathcal{Y}}\) for \(\Delta=\{a,b\}\) and \(l=3\), We use weighted determinization (Mohri, 1997) to compute an equivalent deterministic WFA denote \(\mathcal{M}\). As shown by Cortes et al. (2015)[Theorem 3], \(\mathcal{M}\) can be computed in polynomial time. \(\mathcal{M}\) admits a unique path labeled with any sequence \(y\in\Delta^{l}\) and the weight of that path is \(\mathcal{U}(y,y_{i})\). The weight of that accepting path is obtained by multiplying the weights of its transitions and that of the final state.

We now define a deterministic \(p\)-gram WFA \(\mathcal{N}\) that accepts all sequences \(y\in\Delta^{l}\) with each of its states \((\mathbf{z}^{\prime},s)\) encoding a \((p-1)\)-gram \(\mathbf{z}^{\prime}\) read to reach it and the position \(s\) in the sequence \(y\) at which it is reached. The transitions of \(\mathcal{N}\) are therefore defined as follows with weight one:

\[\mathsf{E}_{\mathcal{N}}=\Big{\{}\Big{(}\big{(}y_{s-p+1}^{s-1},s-1\big{)},a,1,\big{(}y_{s-p+2}^{s-1}a,s\big{)}\Big{)}\colon y\in\Delta^{l},a\in\Delta,s\in [l]\Big{\}}.\]

The initial state is \((\epsilon,0)\) and the final states are those with the second element of the pair (the position) being \(l\). Note that, by construction, \(\mathcal{N}\) is deterministic. Then, the composition (or intersection) WFA \(\mathcal{N}\circ\mathcal{M}\) still associates the same weight as \(\mathcal{M}\) to each input string \(y\in\Delta^{l}\). However, the states in that composition help us compute \(\mathsf{L}(\mathbf{z},s)\). In particular, for any \(\mathbf{z}\in\Delta^{p}\) and \(s\in[l]\), let \(\mathsf{E}(\mathbf{z},s)\) be the set of transitions of \(\mathcal{N}\circ\mathcal{M}\) constructed by pairing the transition \(((\mathbf{z}_{1}^{p-1},s-1),z_{p},\omega(\mathbf{z},s),(\mathbf{z}_{2}^{p},s))\) in \(\mathcal{N}\) with a transition \((q_{\mathcal{M}},z_{p},\omega,q_{\mathcal{M}}^{\prime})\) in \(\mathcal{M}\). They admit the following form:

\[\mathsf{E}(\mathbf{z},s)= \Big{\{}\big{(}(q_{\mathcal{N}},q_{\mathcal{M}}),z_{p},\omega, \big{(}q_{\mathcal{N}}^{\prime},q_{\mathcal{M}}^{\prime}\big{)}\big{)}\in \mathsf{E}_{\mathcal{N}\circ\mathcal{M}}\colon q_{\mathcal{N}}=(\mathbf{z}_{1}^ {p-1},s-1)\Big{\}}.\] (35)

The WFA \(\mathcal{N}\circ\mathcal{M}\) is deterministic as a composition of two deterministic WFAs. Thus, there is a unique path labeled with a sequence \(y\in\Delta^{l}\) in \(\mathcal{N}\circ\mathcal{M}\) and \(y\) admits the substring \(\mathbf{z}\) ending at position

Figure 4: Illustration of the WFA \(\mathcal{N}\) for \(\Delta=\{a,b\}\), \(p=2\) and \(l=2\).

\(s\) iff that path goes through a transition in \(\mathsf{E}(\mathbf{z},s)\) when reaching position \(s\). Therefore, to compute \(\mathsf{L}(\mathbf{z},s)\), it suffices for us to compute the sum of the weights of all paths in \(\mathcal{N}\circ\mathcal{M}\) going through a transition in \(\mathsf{E}(\mathbf{z},s)\). This can be done straightforwardly using the forward-backward algorithm or two single-source shortest-distance algorithm over the \((+,\times)\) semiring (Mohri, 2002a), one from the initial state, the other one from the final states. Since \(\mathcal{N}\circ\mathcal{M}\) is acyclic and admits \(O(l|\Delta|^{p})\) transitions, we can compute all the quantities \(\mathsf{L}(\mathbf{z},s)\), \(s\in[l]\) and \(\mathbf{z}\in\Delta^{p}\), in time \(O(l|\Delta|^{p})\).

**Markovian loss.** We consider adopting a Markovian assumption, which is commonly adopted in natural language processing (Manning and Schutze, 1999). We will assume that \(\overline{\ell}\) can be decomposed as follows for all \(y,y^{\prime}\in\Delta^{l}\): \(\overline{\ell}(y,y^{\prime})=\prod_{t=1}^{l}\overline{\ell}_{t}(y^{t}_{t-p+ 1},y^{\prime})\). Thus, we can write:

\[\mathsf{L}(\mathbf{z},s)=\sum_{y:y^{\prime}_{s-p+1}=\mathbf{z}}\prod_{t=1}^{ l}\overline{\ell}_{t}(y^{t}_{t-p+1},y_{i}).\]

To efficiently compute \(\mathsf{L}(\mathbf{z},s)\), we will use a WFA representation similar to the one used by Cortes et al. (2016, 2018) and, for convenience, will adopt a similar notation. \(\mathsf{L}(\mathbf{z},s)\) coincides with a flow computation in a WFA \(\mathcal{A}\) that we now define. \(\mathcal{A}\) has the following set of states:

\[Q_{\mathcal{A}}=\Big{\{}(y^{t}_{t-p+1},t)\colon y\in\Delta^{l},t=0,\ldots,l \Big{\}},\]

with \(\mathsf{l}_{\mathcal{A}}=(\varepsilon,0)\) its single initial state, \(\mathcal{F}_{\mathcal{A}}=\{(y^{l}_{l-p+1},l)\colon y\in\Delta^{l}\}\) its set of final states, and a transition from state \((y^{t-1}_{t-p+1},t-1)\) to state \((y^{t-1}_{t-p+2},b,t)\) with label \(b\) and weight \(\omega(y^{t-1}_{t-p+1},b,t)=\overline{\ell}_{t}(y^{t-1}_{t-p+1}b,y_{i})\), that is the following set of transitions:

\[\mathsf{E}_{\mathcal{A}}=\Big{\{}\Big{(}(y^{t-1}_{t-p+1},t-1),b,\omega(y^{t-1 }_{t-p+1},b,t),(y^{t-1}_{t-p+2}b,t)\Big{)}\colon y\in\Delta^{l},b\in\Delta,t \in[l]\Big{\}}.\]

By construction, \(\mathcal{A}\) is deterministic. The weight of a path in \(\mathcal{A}\) is obtained by multiplying the weights of its constituent transitions. In view of that, \(\mathsf{L}(\mathbf{z},s)\) can be seen as the sum of the weights of all paths in \(\mathcal{A}\) going through the transition from state \((\mathbf{z}^{p-1}_{1},s-1)\) to \((\mathbf{z}^{p}_{2},s)\) with label \(z_{p}\).

For any state \((y^{t}_{t-p+1},t)\in Q_{\mathcal{A}}\), we denote by \(\alpha((y^{t}_{t-p+1},t))\) the sum of the weights of all paths in \(\mathcal{A}\) from the initial state \(\mathsf{l}_{\mathcal{A}}\) to \((y^{t}_{t-p+1},t)\) and by \(\beta((y^{t}_{t-p+1},t))\) the sum of the weights of all paths from \((y^{t}_{t-p+1},t)\) to a final state. Then, \(\mathsf{L}(\mathbf{z},s)\) is given by

\[\mathsf{L}(\mathbf{z},s)=\alpha\big{(}(\mathbf{z}^{p-1}_{1},s-1)\big{)}\times \omega(\mathbf{z},s)\times\beta\big{(}(\mathbf{z}^{p}_{2},s)\big{)}.\]

Since \(\mathcal{A}\) is acyclic, \(\alpha\) and \(\beta\) can be computed for all states in linear time in the size of \(\mathcal{A}\) using a single-source shortest-distance algorithm over the \((+,\times)\) semiring or the so-called forward-backward algorithm. Thus, since \(\mathcal{A}\) admits \(O(l|\Delta|^{p})\) transitions, we can also compute all quantities \(\mathsf{L}(\mathbf{z},s)\), \(s\in[l]\) and \(\mathbf{z}\in\Delta^{p}\), in time \(O(lr^{p})\).

### Efficient gradient computation for \(\mathsf{L}^{\,\mathrm{comp}}_{\mathrm{exp}}\)

In this section, we provide a brief overview of the gradient computation for \(\mathsf{L}^{\,\mathrm{comp}}_{\mathrm{exp}}\), which is similar to the approach used for \(\mathsf{L}^{\,\mathrm{comp}}_{\mathrm{log}}\).

Note that the loss \(\mathsf{L}^{\,\mathrm{comp}}_{\mathrm{exp}}\) on a given point \((x_{i},y_{i})\) can be expressed as follows:

\[\mathsf{L}^{\,\mathrm{comp}}_{\mathrm{exp}} =\sum_{y^{\prime}\in\Delta^{l}}\overline{\ell}(y^{\prime},y_{i}) \sum_{y^{\prime\prime}a^{\prime}}e^{h(x_{i},y^{\prime\prime})-h(x_{i},y^{ \prime})}\] \[=\sum_{y^{\prime}\in\Delta^{l}}\overline{\ell}(y^{\prime},y_{i}) \sum_{y^{\prime\prime}\in\Delta^{l}}e^{h(x_{i},y^{\prime\prime})-h(x_{i},y^{ \prime})}-\sum_{y^{\prime}\in\Delta^{l}}\overline{\ell}(y^{\prime},y_{i})\] \[=\left[\sum_{y^{\prime\prime}\in\Delta^{l}}e^{h(x_{i},y^{\prime \prime})}\right]\sum_{y^{\prime}\in\Delta^{l}}\overline{\ell}(y^{\prime},y_{i}) e^{-h(x_{i},y^{\prime})}-\sum_{y^{\prime}\in\Delta^{l}}\overline{\ell}(y^{ \prime},y_{i})\] \[=\left[\sum_{y^{\prime\prime}\in\Delta^{l}}e^{\mathbf{w}\cdot\Psi( x_{i},y^{\prime\prime})}\right]\sum_{y^{\prime}\in\Delta^{l}}\overline{\ell}(y^{ \prime},y_{i})e^{-\mathbf{w}\cdot\Psi(x_{i},y^{\prime})}-\sum_{y^{\prime}\in \Delta^{l}}\overline{\ell}(y^{\prime},y_{i}).\]The gradient of \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}\) is therefore given by

\[\begin{split}\nabla\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}(\mathbf{w })=&\left[\sum_{y^{\prime\prime}\in\Delta^{l}}e^{\mathbf{w}\cdot \Psi(x_{i},y^{\prime\prime})}\Psi(x_{i},y^{\prime\prime})\right]\sum_{y^{ \prime}\in\Delta^{l}}\overline{\ell}(y^{\prime},y_{i})e^{-\mathbf{w}\cdot \Psi(x_{i},y^{\prime})}\\ &-\left[\sum_{y^{\prime\prime}\in\Delta^{l}}e^{\mathbf{w}\cdot \Psi(x_{i},y^{\prime\prime})}\right]\sum_{y^{\prime}\in\Delta^{l}}\overline{ \ell}(y^{\prime},y_{i})e^{-\mathbf{w}\cdot\Psi(x_{i},y^{\prime})}\Psi(x_{i},y^ {\prime}).\end{split}\] (36)

An efficient computation of these terms is not straightforward since the summations run over an exponential number of sequences for \(y\). However, we will leverage the Markovian property of the features to design an efficient computation. This approach is similar to what we demonstrated earlier for \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{log}}\). We start with identifying the most computationally challenging terms by rewriting the expression of the gradient of \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}\) in the following lemma.

**Lemma 19**.: _For any \(\mathbf{w}\in\mathbb{R}^{d}\), the gradient of \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}\) can be expressed as follows:_

\[\nabla\mathsf{L}^{\mathrm{comp}}_{\mathrm{exp}}(\mathbf{w})=\sum_{s=1}^{l}\sum_ {\mathbf{z}\in\Delta^{p}}[\mathsf{N}_{\mathbf{w}}\mathsf{Q}^{\prime}_{ \mathbf{w}}(\mathbf{z},s)-Z_{\mathbf{w}}\mathsf{C}_{\mathbf{w}}(\mathbf{z},s) ]\widetilde{\psi}(x_{i},\mathbf{z},s),\]

where \(\mathsf{Q}^{\prime}_{\mathbf{w}}(\mathbf{z},s)=\sum_{y:y^{\prime}_{s-p+1}= \mathbf{z}}e^{\mathbf{w}\cdot\Psi(x_{i},y)}\), \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)=\sum_{y:y^{\prime}_{s-p+1}=\mathbf{z}} \overline{\ell}(y,y_{i})e^{-\mathbf{w}\cdot\Psi(x_{i},y)}\) and \(\mathsf{N}_{\mathbf{w}}=\sum_{y\in\Delta^{l}}\overline{\ell}(y,y_{i})e^{- \mathbf{w}\cdot\Psi(x_{i},y)}\).

Proof.: Using the decomposition of the feature vector, we can write:

\[\begin{split}\sum_{y\in\Delta^{l}}e^{\mathbf{w}\cdot\Psi(x_{i}, y)}\Psi(x_{i},y)&=\sum_{y\in\Delta^{l}}e^{\mathbf{w}\cdot\Psi(x_{i}, y)}\sum_{s=1}^{l}\widetilde{\psi}(x_{i},y_{s-p+1}^{s},s)\\ &=\sum_{s=1}^{l}\sum_{\mathbf{z}\in\Delta^{p}}\Biggl{[}\sum_{y^{ \prime}_{s-p+1}=\mathbf{z}}e^{\mathbf{w}\cdot\Psi(x_{i},y)}\Biggr{]}\widetilde {\psi}(x_{i},\mathbf{z},s)\\ \sum_{y\in\Delta^{l}}\overline{\ell}(y,y_{i})e^{-\mathbf{w}\cdot \Psi(x_{i},y)}\Psi(x_{i},y)&=\sum_{y\in\Delta^{l}}\overline{ \ell}(y,y_{i})e^{-\mathbf{w}\cdot\Psi(x_{i},y)}\sum_{s=1}^{l}\widetilde{\psi}( x_{i},y_{s-p+1}^{s},s)\\ &=\sum_{s=1}^{l}\sum_{\mathbf{z}\in\Delta^{p}}\Biggl{[}\sum_{y^{ \prime}_{s-p+1}=\mathbf{z}}\overline{\ell}(y,y_{i})e^{-\mathbf{w}\cdot\Psi(x _{i},y)}\Biggr{]}\widetilde{\psi}(x_{i},\mathbf{z},s).\end{split}\]

This completes the proof. 

It was shown by Cortes et al. (2016, 2018) that all of the quantities \(\mathsf{Q}^{\prime}_{\mathbf{w}}(\mathbf{z},s)\) for \(\mathbf{z}\in\Delta^{p}\) and \(s\in[l]\) and \(Z_{\mathbf{w}}\) can be computed efficiently in time \(O(lr^{p})\), where \(r=|\Delta|\). Thus, the remaining bottleneck in the gradient computation suggested by Lemma 19 is the evaluation of the quantities \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\) for \(\mathbf{z}\in\Delta^{p}\) and \(s\in[l]\) and \(\mathsf{N}_{\mathbf{w}}\). As with the loss function \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{log}}\) discussed in the previous section, we will analyze the computation of these quantities first in the case of rational losses, next in that of Markovian loss.

**Rational losses.** We will adopt the same notation as in the case of the \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{log}}\) loss with the same definition of a _rational loss_: \(\ell\) is a rational loss if there exists a WFST over the \((+,\times)\) semiring over the reals with \(\Delta\) as both the input and output alphabet such that for all \(y,y^{\prime}\in\Delta^{*}\), we have \(\overline{\ell}(y,y^{\prime})=\mathcal{U}(y,y^{\prime})\).

Our algorithm is also somewhat similar to the one described for the \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{log}}\) loss or that of Cortes et al. (2018) for the computation of the gradient of the VCRF loss function. There are, however, several differences here too because the quantities computed and thus the automata operations required are distinct.

Exactly as in the case of \(\mathsf{L}^{\mathrm{comp}}_{\mathrm{log}}\) loss, we first define a deterministic WFA \(\mathcal{M}\) over the \((+,\times)\) semiring that can be computed in polynomial time and that admits a unique path labeled with any sequence \(y\in\Delta^{l}\), whose weight is \(\mathcal{U}(y,y_{i})\). Next, as in [12], we define a deterministic WFA \(\mathcal{A}\) such that

\[\mathcal{A}(y)=e^{-\mathbf{w}\cdot\Psi(x_{i},y)}=\prod_{s=1}^{l}e^{-\mathbf{w} \cdot\overline{\psi}(x_{i},y_{s-p+1}^{s},s)}.\]

The set of states \(\mathsf{Q}_{\mathcal{A}}\) of \(\mathcal{A}\) are defined as \(\mathsf{Q}_{\mathcal{A}}=\Big{\{}(y_{s-p+1}^{s},s)\colon y\in\Delta^{l},s=0, \ldots,l\Big{\}}\), with \(\mathsf{l}_{\mathcal{A}}=(\varepsilon,0)\) its single initial state, \(\mathcal{F}_{\mathcal{A}}=\{(y_{l-p+1}^{l},l)\colon y\in\Delta^{l}\}\) its set of final states, and with a transition from state \((y_{s-p+1}^{s-1},s-1)\) to state \((y_{s-p+2}^{s-1},a,s)\) with label \(a\) and weight, that is, the following set of transitions:

\[\mathsf{E}_{\mathcal{A}}=\Big{\{}\Big{(}(y_{s-p+1}^{s-1},s-1),a,e^{-\mathbf{w }\cdot\overline{\psi}(x_{i},y_{s-p+1}^{s-1}a,s)},(y_{s-p+2}^{s-1}a,s)\Big{)} \colon y\in\Delta^{l},a\in\Delta,s\in[l]\Big{\}}.\]

Then, by definition of composition or intersection [13], the WFA \((\mathcal{M}\circ\mathcal{A})\) is deterministic and admits a unique path labeled with any given \(y\in\Delta^{l}\) whose weight is \((\mathcal{M}\circ\mathcal{A})(y)=\mathcal{M}(y)\cdot\mathcal{A}(y)=\overline{ \ell}(y,y_{i})e^{-\mathbf{w}\cdot\Psi(x_{i},y)}\).

Now, \(\mathsf{N}_{\mathbf{w}}\) coincides with the sum of the weights of all accepted paths in this WFA. Thus, since \((\mathcal{M}\circ\mathcal{A})\) is acyclic, it can be computed in time linear in the size of \((\mathcal{M}\circ\mathcal{A})\), that is its number of transitions. For any \(s\in[l]\) and \(\mathbf{z}\in\Delta^{p}\), \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\) is the sum of the weights of all paths in \((\mathcal{M}\circ\mathcal{A})\) labeled with a sequence \(y\) admitting \(\mathbf{z}\) as a substring ending at position \(s\). The states in the composition \((\mathcal{M}\circ\mathcal{A})\) help us compute \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\). As in the case of the \(\mathsf{L}_{\mathrm{Log}}^{\mathrm{comp}}\) loss, for any \(\mathbf{z}\in\Delta^{p}\) and \(s\in[l]\), we define \(\mathsf{E}(\mathbf{z},s)\) as the set of transitions of \((\mathcal{M}\circ\mathcal{A})\) constructed by pairing a transition \((q_{\mathcal{M}},z_{p},\omega_{\mathcal{M}},q_{\mathcal{M}}^{\prime})\) in \(\mathcal{M}\) with the transition \((\big{(}\mathbf{z}_{1}^{p-1},s-1\big{)},z_{p},\omega(\mathbf{z},s),(\mathbf{z }_{2}^{p},s))\) in \(\mathcal{A}\). They admit the following form:

\[\mathsf{E}(\mathbf{z},s)= \Big{\{}\big{(}(q_{\mathcal{M}},q_{\mathcal{A}}),z_{p},\omega_{ \mathcal{M}}\cdot\omega(\mathbf{z},s),(q_{\mathcal{M}}^{\prime},q_{\mathcal{A }}^{\prime})\big{)}\in\mathsf{E}_{\mathcal{M}\circ\mathcal{A}}\colon q_{ \mathcal{A}}=\big{(}\mathbf{z}_{1}^{p-1},s-1\big{)}\Big{\}}.\] (37)

The WFA \((\mathcal{M}\circ\mathcal{A})\) is deterministic as a composition of two deterministic WFAs. Thus, there is a unique path labeled with a sequence \(y\in\Delta^{l}\) in \((\mathcal{M}\circ\mathcal{A})\) and \(y\) admits the substring \(\mathbf{z}\) ending at position \(s\) iff that path goes through a transition in \(\mathsf{E}(\mathbf{z},s)\) when reaching position \(s\). Therefore, to compute \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\), it suffices for us to compute the sum of the weights of all paths in \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\) going through a transition in \(\mathsf{E}(\mathbf{z},s)\). This can be done straightforwardly using the forward-backward algorithm or two single-source shortest-distance algorithm over the \((+,\times)\) semiring [13], one from the initial state, the other one from the final states. Since \((\mathcal{M}\circ\mathcal{A})\) is acyclic and admits \(O(l|\Delta|^{p})\) transitions, we can compute all the quantities \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\), \(s\in[l]\) and \(\mathbf{z}\in\Delta^{p}\), in time \(O(l|\Delta|^{p})\).

**Markovian loss.** Here, we adopt the Markovian assumption and assume that \(\overline{\ell}\) can be decomposed as follows for all \(y,y^{\prime}\in\Delta^{l}\colon\overline{\ell}(y,y^{\prime})=\prod_{t=1}^{l} \overline{\ell}_{t}(y_{t-p+1}^{t},y^{\prime})\). Thus, the quantity \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\) can be written as:

\[\mathsf{C}_{\mathbf{w}}(\mathbf{z},s) =\sum_{y:y_{s-p+1}^{s}\approx\mathbf{z}}\prod_{t=1}^{l}\overline {\ell}_{t}(y_{t-p+1}^{t},y_{i})e^{-\mathbf{w}\cdot\overline{\sum}_{k=1}^{l} \overline{\psi}(x_{i},y_{k-p+1}^{k},k)}\] \[=\sum_{y:y_{s-p+1}^{s}\approx\mathbf{z}}\prod_{t=1}^{l}\overline {\ell}_{t}(y_{t-p+1}^{t},y_{i})\prod_{k=1}^{l}e^{-\mathbf{w}\cdot\overline{ \psi}(x_{i},y_{k-p+1}^{t},k)}\] \[=\sum_{y:y_{s-p+1}^{s}\approx\mathbf{z}}\prod_{t=1}^{l}\overline {\ell}_{t}(y_{t-p+1}^{t},y_{i})e^{-\mathbf{w}\cdot\overline{\psi}(x_{i},y_{t-p+1 }^{t},t)}.\]

Then, we can proceed as in the Markovian loss case for the loss function \(\mathsf{L}_{\mathrm{log}}^{\mathrm{comp}}\) except that instead of the WFA \(\mathcal{A}\) used there, we define here a similar WFA \(\mathcal{A}^{\prime}\). The only difference is that the weight \(\omega(y_{t-p+1}^{t-1},b,t)=\overline{\ell}_{t}(y_{t-p+1}^{t-1}b,y_{i})\) for the WFA \(\mathcal{A}\) is replaced with \(\omega^{\prime}(y_{t-p+1}^{t-1}b,t)=\overline{\ell}_{t}(y_{t-p+1}^{t-1}b,y_{i}) e^{-\mathbf{w}\cdot\overline{\psi}(x_{i},y_{i-p+1}^{t-1}b,t)}\). With the same argument, we can compute all quantities \(\mathsf{C}_{\mathbf{w}}(\mathbf{z},s)\), \(s\in[l]\) and \(\mathbf{z}\in\Delta^{p}\), in time \(O(lr^{p})\). The quantity \(\mathsf{N}_{\mathbf{w}}\) can also be efficiently computed in time \(O(lr^{p})\) since it is the sum of the weights of all paths in \(\mathcal{A}^{\prime}\).

### Efficient Inference

We focused on the problem of efficient computation of the gradient. Inference is also a key problem in structured prediction since the label with a highest score must be determined out of an exponentially large set of possible ones. However, for the linear hypotheses considered in the previous sections, this problem can be efficiently tackled since it can be cast as a shortest-distance problem in a directed acyclic graph, as in (Cortes, Kuznetsov, Mohri, and Yang, 2016).

More generally, an efficient gradient computation, efficient inference and other related algorithms can benefit from standard weighted automata and transducer optimization algorithms such as \(\epsilon\)-removal (Mohri, 2000, 2002b) and determinization (Mohri, 1997; Mohri and Riley, 1997; Allauzen and Mohri, 2003, 2004) (see also the survey chapter (Mohri, 2009)).