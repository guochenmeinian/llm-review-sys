# Galopa: Graph Transport Learning

with Optimal Plan Alignment

 Yejiang Wang\({}^{1,2}\) Yuhai Zhao\({}^{1,2,\dagger}\) Zhengkui Wang\({}^{3}\) Ling Li\({}^{1,2}\)

\({}^{1}\) School of Computer Science and Engineering, Northeastern University, China

\({}^{2}\) Key Laboratory of Intelligent Computing in Medical Image

of Ministry of Education, Northeastern University, China

\({}^{3}\) InfoComm Technology Cluster, Singapore Institute of Technology, Singapore

wangyejiang@stumail.neu.edu.cn, zhaoyuhai@mail.neu.edu.cn, zhengkui.wang@singaporetech.edu.sg, lilingneu@gmail.com

 Corresponding author.

###### Abstract

Self-supervised learning on graphs aims to learn graph representations in an unsupervised manner. While graph contrastive learning (GCL - relying on graph augmentation for creating perturbation views of anchor graphs and maximizing/minimizing similarity for positive/negative pairs) is a popular self-supervised method, it faces challenges in finding label-invariant augmented graphs and determining the exact extent of similarity between sample pairs to be achieved. In this work, we propose an alternative self-supervised solution that (i) goes beyond the label invariance assumption without distinguishing between positive/negative samples, (ii) can calibrate the encoder for preserving not only the structural information inside the graph, but the matching information between different graphs, (iii) learns isometric embeddings that preserve the distance between graphs, a by-product of our objective. Motivated by optimal transport theory, this scheme relies on an observation that the optimal transport plans between node representations at the output space, which measure the matching probability between two distributions, should be consistent with the plans between the corresponding graphs at the input space. The experimental findings include: (i) The plan alignment strategy significantly outperforms the counterpart using the transport distance; (ii) The proposed model shows superior performance using only node attributes as calibration signals, without relying on edge information; (iii) Our model maintains robust results even under high perturbation rates; (iv) Extensive experiments on various benchmarks validate the effectiveness of the proposed method.

## 1 Introduction

Self-supervised graph learning involves learning representations of real-world graph data without the need for human supervision. Graph contrastive learning (GCL) [6, 16, 71] has been identified as one of the most successful graph self-supervised learning approaches, with its key components consisting of graph augmentation and contrastive learning. The former creates perturbation views of anchor graphs via various augmenting techniques, while the latter maximizes the similarity for two augmentations (positive pairs) of the same anchor and minimizes the similarity for those of two different anchors (negative pairs). The effectiveness of contrastive learning is dependent on the assumption that the augmented operations preserve the nature of data points and ensure that the augmented samples have consistent labels with the original ones.

However, graph data structures are discrete and their properties may vary significantly even with slight perturbations, which makes it much more challenging to design reasonable augmentations that guarantee the label-invariant assumption for graphs, in contrast to images or text. Additionally, the concept of "maximum similarity" in contrastive learning is difficult to measure since it is vague and lacks a clear indication of how much similarity should be maximized (or minimized) for a given pair of positive (or negative) views.

To address these challenges, an intuitive solution is to introduce the concept of distance from the input space, whereby the distance between the learned embeddings is forced to be equal to the distance between the corresponding input graphs. However, this requirement is challenging to achieve as the input objects (graphs) and output representations (vectors) are two distinct concepts, making it difficult to agree on their distance metrics. For instance, comparing the graph edit distance between graphs [15] and the Euclidean distance between vectors directly is inconceivable.

In response to the aforementioned challenges, in this paper, we propose a novel self-supervised learning method that seeks to align optimal transport plans [59] from graph space to node representation space, instead of transport distance [44]. This method is referred to as Graph Transport Learning (GTL), and it exploits the key concept in optimal transport (OT) theory, which aims to identify an optimal match (i.e., transport plan) between two data distributions (e.g., node sets of graphs) that minimizes the matching cost (i.e., transport distance). As shown in Figure 1, the transportation plan \(\pi\) explicitly determines how to match particles from the source distribution \(\mu\) to the target distribution \(\nu\). Our approach involves several key steps. First, we obtain two graph views by augmenting a graph and generating node embeddings using a backbone model, such as Graph Neural Network (GNNs) [25]. Two optimal transport plans can be computed from the graph space and the representation space, respectively. To accurately capture the matching relationships in the original graph space, we enforce the backbone to learn representations that exhibit consistency with the optimal transport plans between the corresponding source graphs in the input space. This is achieved by minimizing the discrepancy between the two plans. Unlike distance-based approaches, the transport plan alignment shows several advantages: (i) _Direct comparability_: Plans with the same dimension can be compared directly, regardless of differences between the graph and representation spaces. The value of the transport plan is dimensionless, representing the joint probability of two particles; (ii) _Accurate match relationship_: For discrete objects, the transport plan retains more accurate matching relation between data compared to distance; (iii) _Label-variant_: Importantly, our self-supervised model does not require differentiating between positive and negative samples after graph augmentation. This is due to the availability of corrective information from the input space, which eliminates the need for a label-invariant assumption. Notably, our experimental findings in Section 7 demonstrate that our model maintains robust performance even under high perturbation rates, such as when 80\(\%\) of both edges and node attributes are destroyed during graph augmentation. In summary, we make the following contributions:

* We propose a novel paradigm for self-supervised graph learning based on optimal plan alignment, Galopa, which offers a distinct objective compared to contrastive learning methods. This approach eliminates the need for a label-invariant assumption.
* By constraining the discrepancy between the transport plans, we introduce a new loss to enable the sharing of the exact matching information from the graphs space to the representation space.
* Multiple comprehensive experiments, including distance v.s plan, node feature v.s edge, robustness test and comparison with state-of-the-art methods demonstrate remarkable performance of Galopa.

## 2 Related Work

**Self-supervised Learning on Graphs.** Graph self-supervised learning has been a promising paradigm for learning useful representations of unlabeled graph data. The node embedding methods aim to learn a low-dimensional vector embedding of vertex preserving different kinds of order proximity via factorization [3], random walk [17] or deep learning [8; 61]. Recently, graph contrastive learning has achieved unprecedented success [6; 28; 55; 64], where the goal is to ensure representations have high similarity between positive views of a graph and high dissimilarity between negative views. A common way of contrastive objective is contrasting views at the node level. For the

Figure 1: An illustration of discrete optimal transport plan.

representations (positive views) \(\mathbf{h}_{i}^{1}\) and \(\mathbf{h}_{i}^{2}\) of same node \(i\) in two augmented graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\), the pairwise contrastive loss can be defined as

\[\mathcal{L}_{\text{contrast}}=\sum_{i}\log\left(\frac{e^{\mathcal{D}(\mathbf{h}_ {i}^{1},\mathbf{h}_{i}^{2})/\tau}}{e^{\mathcal{D}(\mathbf{h}_{i}^{1},\mathbf{h }_{i}^{2})/\tau}+\sum_{k\neq i}e^{\mathcal{D}(\mathbf{h}_{i}^{1},\mathbf{h}_{k }^{2})/\tau}}\right)\] (1)

where \(\mathcal{D}\) is a discriminator that maps two views to an agreement (similarity) score, such as the inner product. \(\tau\) denotes the temperature. Obviously, this type of loss strongly relies on the label invariance assumption. In other words, it needs to know (or assume) beforehand that the two views are positive/negative samples, which is challenging for discrete graph structures. For example, there exist some graphs, such as molecular graphs, whose labels are very sensitive to perturbation/corruption. Some studies have explored the label-invariant augmentation [29; 31; 74], but such augmentations require very careful design and adjustment, and sometimes limit the power of graph augmentation. More recently, [22] utilizes the graph edit distance to train graph discriminator to predict whether a graph is an original graph or a perturbed one. However, this model still requires positive and negative samples and relies on the assumption of label invariance.

**Optimal Transport.** Optimal transport (OT) [59] is a mathematical tool for aligning probability distributions has received much attention in the machine learning community in many applications, e.g., computer vision [4; 51], generative adversarial network [2; 7; 30], domain adaptation [18]. OT aims to derive a transport plan between a source and a target distribution, such that the transport cost is minimized. [34] proposes the Gromov-Wasserstein (GW) distance between metrics defined within each space rather than between samples across different spaces, which has been used as a distance between graphs in several applications [56; 60; 67]. However, these studies can only perform graph classification on datasets with multiple graphs and cannot be applied to network analysis, such as node representation classification, where only one network is available. In this work, we combine optimal transport problem with graph neural network to form a novel self-supervised learning paradigm that allows both graph and network learning. See Appendix A for more details.

## 3 Background

Notations.Let \(\mathcal{G}=(\mathcal{V},\bm{A},\mu)\) be a graph of \(n\) nodes where \(\mathcal{V}\) denotes the set of nodes, \(\bm{A}\in\{0,1\}^{n\times n}\) is the adjacency matrix. \(\mu\in\mathbb{R}^{n}\) is the empirical distribution of nodes in \(\mathcal{G}\). Generally, \(\mu\) is a uniform discrete probability distribution. When the node label (or attribute) is available, we represent the node feature matrix as \(\bm{X}\in\mathbb{R}^{n\times d}\), where \(d\) denotes the dimension of node attributes. \(\bm{X}^{i}\) represents the \(i\)-row of \(\bm{X}\). Let \(\llbracket n\rrbracket=\{1,\cdots,n\}\), \(\llbracket n\rrbracket^{2}=\llbracket n\rrbracket\times\llbracket n\rrbracket\) and \(\langle\cdot,\cdot\rangle\) denotes the inner product for matrices. We denote \(\otimes\) the tensor-matrix multiplication. \(\mathbf{1}_{n}\) represents the vector with ones as all the \(n\) elements. \(|\cdot|\) denotes absolute value.

Plan and Optimal Transport.The optimal transport (OT) problem is pioneered by Monge [35] in order to seek the most cost-effective _transport plan_ that transforms the mass of a pile of sand into another one. In particular, it studies how to find an optimal coupling or optimal plan \(\pi\) for transforming the distribution \(\mu\) to \(\nu\) with minimum total transport cost (i.e., optimal transport distance), where the element of \(\pi\) describes the probability of moving mass from one position to another. In this work, we mainly focus on the discrete case. Given two sets of features \(\bm{X}_{1}=\{\bm{X}_{1}^{i}\}_{i=1}^{n}\) and \(\bm{X}_{2}=\{\bm{X}_{2}^{j}\}_{j=1}^{m}\), where \(n\) and \(m\) are are the number of features, respectively. \(\mu\in\mathbb{R}^{n}\) and \(\nu\in\mathbb{R}^{m}\) are the probability distributions of the entities in the two sets, respectively. The formulation of the OT distance is

\[\mathcal{W}(\bm{X}_{1},\bm{X}_{2})=\min_{\pi\in\Pi(\mu,\nu)}\sum_{i\in \llbracket n\rrbracket}\sum_{j\in\llbracket m\rrbracket}c_{\mathcal{X}}(\bm{X} _{1}^{i},\bm{X}_{2}^{j})\cdot\pi_{ij}=\min_{\pi\in\Pi(\mu,\nu)}\left\langle \bm{K}(\bm{X}_{1},\bm{X}_{2}),\pi\right\rangle\] (2)

where

\[\Pi(\mu,\nu)=\{\pi\in\mathbb{R}^{n\times m}\mid\pi\mathbf{1}_{m}=\mu,\mathbf{ 1}_{n}\pi=\nu\}\] (3)

denotes all the joint distributions \(\pi\) with marginals \(\mu\) and \(\nu\). \(\bm{K}(\bm{X}_{1},\bm{X}_{2})_{ij}=c_{\mathcal{X}}(\bm{X}_{1}^{i},\bm{X}_{2}^{j})\) is the cost (work) of moving \(\bm{X}_{1}^{i}\) to \(\bm{X}_{2}^{j}\), the cosine distance between \(\bm{X}_{1}^{i}\) and \(\bm{X}_{2}^{i}\) is a popular choice. The \(\pi\in\mathbb{R}^{n\times m}\) is called as **transport plan**. This distance is also known as the Wasserstein distance.

Optimal Transport for Graphs.The Wasserstein problem requires the two distributions of point sets to lie in the same space. But for graphs, it is difficult to measure the cost between two nodes on different graphs without node label (attribute). Even if the cost between nodes could be calculated, the Wasserstein distance cannot take the edge information into account. To compare distributions that are not necessarily in the same space, [34] defines Gromov-Wasserstein distance between two graphs without node label \(\mathcal{G}_{1}=(\bm{A}_{1},\mu)\) and \(\mathcal{G}_{2}=(\bm{A}_{2},\nu)\) as follow

\[\mathcal{W}_{\text{GW}}(\mathcal{G}_{1},\mathcal{G}_{2})=\min_{\pi_{\mathcal{G }}\in\Pi(\mu,\nu)}\sum_{i,k\in\llbracket n\rrbracket^{2}}\sum_{j,l\in \llbracket m\rrbracket^{2}}c_{\mathcal{A}}(\bm{A}_{1}^{ik},\bm{A}_{2}^{jl}) \cdot\pi_{ik}\pi_{jl}=\min_{\pi\in\Pi(\mu,\nu)}\langle\bm{L}(\bm{A}_{1},\bm{A}_ {2})\otimes\pi,\pi\rangle\] (4)

where \(\bm{L}(\bm{A}_{1},\bm{A}_{2})\) is 4-D tensor and \(\bm{L}(\bm{A}_{1},\bm{A}_{2})_{ijkl}=c_{\mathcal{A}}(\bm{A}_{1}^{ik},\bm{A}_{2 }^{jl})\). The cost function \(c_{\mathcal{A}}\) is commonly defined as \(c_{\mathcal{A}}(\bm{A}_{1}^{ik},\bm{A}_{2}^{jl})=|\bm{A}_{1}^{ik}-\bm{A}_{2}^ {jl}|\).

Consider two graphs with node attributes \(\mathcal{G}_{1}=(\bm{A}_{1},\bm{X}_{1},\mu)\) and \(\mathcal{G}_{2}=(\bm{A}_{2},\bm{X}_{2},\nu)\) where \(\bm{A}_{1}\in\mathbb{R}^{n\times n}\) and \(\bm{A}_{2}\in\mathbb{R}^{m\times m}\) denote their adjacency matrices, \(\bm{X}_{1}\in\mathbb{R}^{n\times d}\) and \(\bm{X}_{2}\in\mathbb{R}^{m\times d}\) are feature matrices. The fused Gromov-Wasserstein distance [56] between graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) can be defined as

\[\mathcal{W}_{\text{FGW}}(\mathcal{G}_{1},\mathcal{G}_{2})=\min_{\pi_{\mathcal{ G}}\in\Pi(\mu,\nu)}\ \sigma\sum_{ij}c_{\mathcal{X}}(\bm{X}_{1}^{i},\bm{X}_{2}^{j})\cdot\pi_{ij}^{ \mathcal{G}}+(1-\sigma)\sum_{ijkl}c_{\mathcal{A}}(\bm{A}_{1}^{ik},\bm{A}_{2}^ {jl})\cdot\pi_{ij}^{\mathcal{G}}\pi_{kl}^{\mathcal{G}}\] (5)

which is equivalent to

\[\min_{\pi_{\mathcal{G}}\in\Pi(\mu,\nu)}\ \langle\sigma\bm{K}(\bm{X}_{1},\bm{X}_ {2})+(1-\sigma)\bm{L}(\bm{A}_{1},\bm{A}_{2})\otimes\pi_{\mathcal{G}},\ \pi_{\mathcal{G}}\rangle\] (6)

where \(\bm{K}(\bm{X}_{1},\bm{X}_{2})_{ij}=c_{\mathcal{X}}(\bm{X}_{1}^{i},\bm{X}_{2}^ {j})\), \(\bm{L}(\bm{A}_{1},\bm{A}_{2})_{ijkl}=c_{\mathcal{A}}(A_{1}^{ik},A_{2}^{jl})\) and \(\sigma\in[0;1]\) denotes a trade-off parameter. Obviously, this definition is a fusion of Equations (2) and (4).

## 4 Graph Transport Alignment

As analyzed in the previous section, the recent graph contrastive techniques are deeply plagued by positive and negative sample generation, since graph properties could become completely different even with slight perturbations. To design a universal self-supervision scheme, we are motivated to calibrate the similarity between different representations in the output space using the matching signal between corresponding graphs from the input space. In particular, we first perturb the given graph to obtain two different graphs (e.g., a perturbed graph and the original one) and generate the node embeddings of the two graphs using the backbone model (e.g., GNNs). After computing the optimal transport plans for the graphs and the sets of node representations respectively (Section 4.1), we take the discrepancy between the two plans as the loss to calibrate the backbone for obtaining representation with rich geometry awareness and interpretable correspondences (Section 4.2). We compare the proposed graph self-supervised learning paradigm with graph contrastive learning in Section 4.3. The framework can be found in Figure 2.

### Optimal Transport Plan

In general, it is challenging to define two similarity metrics (e.g., distances), which can be directly compared, in two different spaces. This is especially the case for graphs and vectors, two distinct objects by nature. Fortunately, optimal transport theory offers a glimmer of hope, transportation plan, for such comparison. In this section, we present objectives that aim at finding the optimal plan for two graphs (or sets of vectors) to minimize the transport cost. For the graph, a natural idea is to jointly take into account both node attributes and explicit topology information (i.e., edges) in the transportation plan. In this work, we leverage [56] to present an objective function for calculating the optimal transport plan which integrates the edge structure and the feature information on nodes.

Figure 2: A framework of graph transport self-supervised learning.

Specifically, the fused optimal transport plan \(\pi_{\mathcal{G}}^{*}\in\mathbb{R}^{n\times m}\) between two node-attribute graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) can be defined as

\[\pi_{\mathcal{G}}^{*}=\operatorname*{argmin}_{\pi_{\mathcal{G}}\in\Pi(\mu,\nu)} \ \langle\sigma\bm{K}(\bm{X}_{1},\bm{X}_{2})+(1-\sigma)\bm{L}(\bm{A}_{1},\bm{A}_{2 })\otimes\pi_{\mathcal{G}},\ \pi_{\mathcal{G}}\rangle\] (7)

with the fused Gromov-Wasserstein distance (6). By tuning the parameter \(\sigma\) we can control the bias of the learned optimal plan between node attributes and edge structure. Intuitively we might think that combining more edge information could greatly benefit the expressiveness power of the model. However, we observe that the performance of the proposed method does not degrade significantly in the absence of edge information, and even increases rather than decreases on some datasets. It will be explained in detail later.

For the node representations of graph encoded by the backbone model (e.g., GNNs), we can either use Equation (2) directly to calculate the optimal plan \(\pi_{\mathcal{Z}}^{*}\in\mathbb{R}^{n\times m}\) or set \(\sigma=1\) in Equation (7) as

\[\pi_{\mathcal{Z}}^{*}(\bm{Z}_{1},\bm{Z}_{2})=\operatorname*{argmin}_{\pi_{ \mathcal{Z}}\in\Pi(\mu,\nu)}\ \langle\bm{J}(\bm{Z}_{1},\bm{Z}_{2}),\ \pi_{\mathcal{Z}}\rangle\] (8)

where \(\bm{J}(\bm{Z}_{1},\bm{Z}_{2})_{ij}=c_{\mathcal{Z}}(\bm{Z}_{1}^{i},\bm{Z}_{2}^ {j})\), \(\bm{Z}_{1}\) and \(\bm{Z}_{2}\) denote the node representations corresponding to \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\), respectively.

### Optimal Transport Alignment

The previous section establishes the foundation that comparing the similarity metrics defined on graph space and vector space can be reduced into comparing the two optimal transport plans from these spaces, each of which can be solved using Equations (7) or (2). This is a valid comparison because the optimal transport plan \(\pi\) acts as a _probabilistic matching_ of two distributions, while the two plan matrices have the same dimensions (i.e., \(n\times m\)).

Naturally, the encoder may succeed in obtaining a _good_ representations for a graph if it learns the node embeddings that not only retain its structural information inside the graph, but also capture matching information with other graphs. This motivates us to force the encoder to preserve the matching relationship in the graph space by aligning the plan between the two graphs with the plan of their corresponding node representations. We define a match alignment loss by minimizing the discrepancy between the two transport plans as follows

\[\mathcal{L}_{\text{match}}=\Delta\Big{(}\pi_{\mathcal{G}}^{*},\,\pi_{ \mathcal{Z}}^{*}\big{(}\bm{Z}_{1},\bm{Z}_{2}\big{)}\Big{)}\] (9)

where the discrepancy function \(\Delta(\cdot,\cdot)\) can be any commonly used metric, e.g., the Frobenius-norm \(\|\cdot-\cdot\|_{F}\) or the divergence \(D(\cdot\|\cdot)\).

In addition, to guide the encoder to learn a representation retaining structural information inside the graph, we also calibrate the cost matrix \(\bm{J}(\bm{Z}_{1},\bm{Z}_{2})\), which implies the implicit structure relationships between nodes, in the representation space as follow

\[\mathcal{L}_{\text{(im)strc}}=\Delta\Big{(}\sigma\bm{K}\big{(}\bm{X}_{1},\bm{ X}_{2}\big{)}+(1-\sigma)\bm{L}\big{(}\bm{A}_{1},\bm{A}_{2}\big{)}\otimes\pi_{ \mathcal{G}}^{*},\,\bm{J}\big{(}\bm{Z}_{1},\bm{Z}_{2}\big{)}\Big{)}\] (10)

To understand the concept of _'implicit structure'_ intuitively, here we consider the one-dimensional case. As shown in Figure 3, given a point set \(\mathcal{P}=\{\bm{Z}_{1}^{1},\cdots,\bm{Z}_{1}^{n}\}\), the location of each of its points is fixed. If the position of a point (yellow) \(\bm{Z}_{2}^{j}\) is unknown but the transportation cost from this point to all points (blue) in the point set \(\mathcal{P}\) is known, then the location of this point is determined with respect to the entire point set \(\mathcal{P}\). The same holds true for another point \(\bm{Z}_{2}^{l}\). Thus the relative position relationship between points \(\bm{Z}_{2}^{j}\) and \(\bm{Z}_{2}^{l}\) can be captured implicitly by the transport costs matrix \(\bm{J}(\bm{Z}_{1},\bm{Z}_{2})\).

To this end, we define the overall graph transport alignment loss as

\[\mathcal{L}_{\text{GALORA}}=\mathcal{L}_{\text{match}}+\rho\mathcal{L}_{\text{ (im)strc}}\] (11)

where \(\rho\) is the trade-off parameter.

Figure 3: An illustration of implicit structure in one-dimension.

### Compare with Graph Contrastive Learning

Although our objective function is different from the contrastive loss as shown in Equation (1), we find that the algorithmic philosophy of both is very similar. Here we analyze at the node level. Given a graph \(\mathcal{G}\), the perturbation graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are obtained by augmenting \(\mathcal{G}\). If the attribute and context of node \(i\) in \(\mathcal{G}\) are corrupted in a similar way and obtained two node views in \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\). The cost between these two views is quite small (almost zero) and thus the optimal transport plan yields a high probability of matching between these two node views. With Equations (9) and (10), the model Galopa calibrates the matching probability and cost of the corresponding node representations in the output space, which is actually making the representations of two similar nodes similar enough. And for two different nodes \(i\) and \(k\) of \(\mathcal{G}\), the cost between their corresponding node views in graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) is relatively larger. This leads to the opposite correction, i.e., making the representation of two dissimilar nodes sufficiently dissimilar.

From the contrastive learning perspective, the above process is not inconsistent with its goals. But there is a fundamental difference between graph transport alignment and graph contrastive learning: Galopa directly utilizes calibration signal from the graph space. Precisely because of this signal, we do not have to distinguish between positive and negative samples, like _Maxwell's demon_.

### Complexity

The time complexity of the model Galopa is mainly influenced by the optimization process of Equations (7) and (8). To optimize Equation (7), which contains the fused Gromov-Wasserstein term, we utilize a conditional gradient (CG) solver [21]. This solver necessitates the computation of a gradient with a nearly cubic time complexity at each iteration concerning the size of the graph, i.e., the number of nodes. On the other hand, Equation (8) with the Wasserstein term can be optimized using the Sinkhorn-Knopp algorithm [12], which is highly time-efficient with a nearly square complexity.

## 5 Plan or Distance?

Thanks to the alignment of the plan and cost for the representation, as a byproduct, we find that the OT distance between the optimal node representations \(\bm{Z}^{*}\) in Equation (11) is equal to the distance between its corresponding graphs. This is due to the two losses (10) and (9) constrain the optimal node representation to satisfy \(\bm{J}(\bm{Z}_{1}^{*},\bm{Z}_{2}^{*})=\sigma\bm{K}(\bm{X}_{1},\bm{X}_{2})+(1- \sigma)\bm{L}(\bm{A}_{1},\bm{A}_{2})\otimes\pi_{\mathcal{G}}^{*}\) and \(\pi_{\mathcal{Z}}^{*}(\bm{Z}_{1}^{*},\bm{Z}_{2}^{*})=\pi_{\mathcal{G}}^{*}\), respectively. This means that our losses can prompt the encoder to learn an isometric embedding that preserves the distance between graphs, which is one of the pursuits of the general representation model. Hence, we became interested in the question of who is more important, distance or plan? How would the model perform if we drop the alignment of the plan and cost but instead optimize the distance directly? In this section, we assess and rationalize the role of the plan for graph structure data in our self-supervised framework. To compare the possible performance gap between distance and plan, instead of directly optimizing the plan, we construct a new loss with distance as following

\[\mathcal{L}_{\text{dist}}=|\mathcal{W}_{\mathcal{G}}(\mathcal{G}_{1},\mathcal{ G}_{2})-\mathcal{W}(\bm{Z}_{1},\bm{Z}_{2})|\] (12)

where \(|\cdot|\) denotes absolute value, and \(\mathcal{W}_{\mathcal{G}}(\mathcal{G}_{1},\mathcal{G}_{2})=\min_{\pi\in\Pi(\bm {h}_{1},\bm{h}_{2})}\sigma\sum_{ij}c_{\mathcal{X}}(\bm{X}_{1}^{i},\bm{X}_{2}^ {j})\cdot\pi_{ij}^{\mathcal{G}}+(1-\sigma)\sum_{ijkl}c_{\mathcal{A}}(A_{1}^{ ik},A_{2}^{jl})\cdot\pi_{ik}^{\mathcal{G}}\pi_{jl}^{\mathcal{G}}\).

We evaluate the performance of using the pretraining representations on 2 social network datasets, CORA and CITESEER [25] for node classification, and 2 graph classification data PROTEINS and

Figure 4: Plan versus distance. Comparing mean graph/node classification accuracy between transport alignment loss and distance loss on 4 datasets.

MUTAG from TUDataset [36] for graph classification. See Section 8 for detailed experimental configurations. Figure 4 reports the averaged node/graph classification accuracy results over the node/graph-level datasets. The results suggest that the model using the plan as an objective significantly outperforms the counterpart models using the distance. Although the experimental result may lead to'surprise', it demonstrates that the plan is closer to the essence than the distance, for discrete structured data. The optimal transport formulation Equation (5) contains both matching and implicit structural information. If only the final distance is retained instead of capturing the two types of information separately, the learned representation may fail to align properly with the input element. Because the optimal transport plan for the discrete OT problem is not unique in general and the optimal distance may correspond to several plans.

## 6 Node Attributes or Edges?

Among the information encoded in a graph, the structure and node attributes are two crucial elements for representation learning. The basic requirement of the encoder is to preserve the topology structures and capture the vertex feature of graphs. Thus a problem is encountered in our self-supervised learning paradigm: if we try to calibrate the representations learned by the backbone encoder, which is more important, the edge structure or the node attributes? In other words, if the calibration signal from the input space contains only node attribute information and completely ignores the explicit edge connectivity, will the performance of the proposed model deteriorate significantly? The answer seems obvious--it should be. But the case seems to be different. Let's take a look at the experiment below.

As in the previous section, we performed the comparative experiments on four datasets CORA, CITESEER, PROTEINS, and MUTAG. Here we first consider the normal case of our loss function (i.e., \(\rho\neq 0\)). We set the value of the parameter \(\sigma\) to adjust the bias of node attributes or edge connections for the plan in the graph space. If \(\sigma=1\), the model takes into account only node attributes in the transportation plans. When \(\sigma=0\), it integrates explicit edge information while completely ignoring the node feature. Figure 5 reports the results with different \(\sigma\) and shows a surprising outcome: With only node attributes for the calculation of the plan, the model achieves outstanding performance on all the datasets, even optimal on some data. However, if we set \(\rho=0\) to remove the implicit structure constraint term \(\mathcal{L}_{\text{(im)}\text{strc}}\), the performance of the model suddenly deteriorates dramatically.

This verifies that the constraint \(\mathcal{L}_{\text{(im)}\text{strc}}\) is necessary and the implicit structural information it captures does calibrate the encoder even in the case of missing explicit edge connection. It therefore inspires self-supervised graph learning: it may not be able to tell that edge information does not contribute to correction for significant performance gains, but it is perfectly feasible to use only node attributes as a calibration supervisory signal for backbone model. This is a practical and valuable finding since the number of edges in a real-world graph dataset is much more than the number of nodes. The time and space complexity of the model can be reduced greatly if only the node information is used in the calculation of the correction signal while ignoring the edge connections.

## 7 Are the Transport Alignment Free from Positive/Negative Samples?

As shown in Equation (11), the proposed objective function does not distinguish whether the two different graphs/nodes are positive (negative) samples or not. Here, we want to show that the graph transport alignment strategy can be independent of the label invariance assumption. We conduct experiments below to see how different levels of perturbation affect the performance of Galopa. When augmenting the graph, we fixed one of the augmentations as NoAug and the other augmentation

Figure 5: The mean graph/node classification accuracy on 4 datasets under different values of parameter \(\sigma\).

requires a hyper-parameter "_aug ratio_" that controls the portion of node attributes/edge that are selected for perturbing. Note that different augmentation strategies can be combined. Since the computation of the optimal transport plan in this paper involves node attributes and edge, we perform two augmentation policies, edge perturbation and feature masking, with different augmentation rates on four datasets (i.e., CORA, CITESEER, MUTAG, and PROTEINS) as shown in Figure 6.

From Figure 6 we find that the performance of our model does not change much even when the original graph is perturbed heavily. For example, even if eighty percent of the edges are removed while eighty percent of the node attributes are destroyed, the model's performance on the dataset CITESEER (0.742) is almost equal to the optimal result (0.743). Fluctuations of only zero point five to two percent are also observed on other data sets, such as one point three in PROTEINS data. Hence, it validates that the alignment of optimal transport between the source space and target space is indeed free from the label-invariant assumption.

## 8 Comparison with the State-of-the-art Methods

In this section, we compare our proposed self-supervised pre-training framework, Galopa, with state-of-the-art methods in the settings of unsupervised learning on graph/node classification. More results can be found in the Appendix.

### Experimental Setup

**Datasets.** We analyze the quality of representations learned by Galopa on node and graph classification benchmarks. For node classification, we evaluate the performance of using the pretraining representations on 7 benchmark graph datasets, namely, CORA, CITESEER, PUBMED [25] and Wiki-CS, Amazon-Computers, Amazon-Photo, and Coauthor-CS [47]. For graph classification, we follow GraphCl[72] to perform evaluations on 6 graph classification data NCI1, PROTEINS, DD, MUTAG, COLLAB, and IMDB-B from TUDataset [36].

**Baselines.** For node-level tasks, we adopt three types of baselines: 1) _Supervised learning methods_, including Mlp and Gcn[25]; 2) _Graph embedding methods_, including DeepWalk[42] and

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Model** & **CORA** & **CITESEER** & **PUBMED** & **WikCS** & **Ame-Comp.** & **Ame-Photo** & **Coauthor-CS** & **Average** \\ \hline Mlp & 47.92 \(\pm\) 0.41 & 49.31 \(\pm\) 0.26 & 69.14 \(\pm\) 0.34 & 71.98 \(\pm\) 0.42 & 73.81 \(\pm\) 0.21 & 78.53 \(\pm\) 0.32 & 90.37 \(\pm\) 0.19 & 68.72 \(\pm\) 0.31 \\ Gcn & 81.54 \(\pm\) 0.68 & 70.73 \(\pm\) 0.65 & 79.16 \(\pm\) 0.25 & 93.02 \(\pm\) 0.11 & 86.51 \(\pm\) 0.54 & 92.42 \(\pm\) 0.22 & 93.03 \(\pm\) 0.31 & 85.20 \(\pm\) 0.39 \\ \hline DeepWalk & 70.72 \(\pm\) 0.63 & 51.39 \(\pm\) 0.41 & 73.27 \(\pm\) 0.86 & 74.42 \(\pm\) 0.13 & 85.68 \(\pm\) 0.07 & 89.40 \(\pm\) 0.11 & 84.61 \(\pm\) 0.22 & 75.64 \(\pm\) 0.35 \\ NonDeepXe & 71.08 \(\pm\) 0.91 & 47.34 \(\pm\) 0.84 & 66.23 \(\pm\) 0.95 & 71.76 \(\pm\) 0.14 & 84.41 \(\pm\) 0.14 & 89.68 \(\pm\) 0.19 & 85.16 \(\pm\) 0.04 & 73.67 \(\pm\) 0.46 \\ \hline Gae & 71.49 \(\pm\) 0.41 & 65.83 \(\pm\) 0.40 & 72.23 \(\pm\) 0.71 & 73.97 \(\pm\) 0.16 & 85.27 \(\pm\) 0.19 & 91.62 \(\pm\) 0.13 & 90.01 \(\pm\) 0.71 & 78.63 \(\pm\) 0.39 \\ Vga & 77.31 \(\pm\) 1.02 & 67.41 \(\pm\) 0.24 & 78.55 \(\pm\) 0.62 & 75.56 \(\pm\) 0.28 & 86.40 \(\pm\) 0.22 & 92.16 \(\pm\) 0.12 & 92.13 \(\pm\) 0.16 & 80.97 \(\pm\) 0.38 \\ Ddi & 82.34 \(\pm\) 0.71 & 78.13 \(\pm\) 0.54 & 76.78 \(\pm\) 0.31 & 75.37 \(\pm\) 0.13 & 84.01 \(\pm\) 0.52 & 91.62 \(\pm\) 0.42 & 92.16 \(\pm\) 0.62 & 82.02 \(\pm\) 0.46 \\ Gmi & 82.39 \(\pm\) 0.65 & 71.72 \(\pm\) 0.15 & 79.34 \(\pm\) 1.04 & 74.87 \(\pm\) 0.13 & 82.18 \(\pm\) 0.27 & 90.68 \(\pm\) 0.18 & OOM & \\ MyGkl & 83.45 \(\pm\) 0.68 & 73.28 \(\pm\) 0.48 & 80.09 \(\pm\) 0.62 & 77.51 \(\pm\) 0.06 & 87.53 \(\pm\) 0.12 & 91.74 \(\pm\) 0.08 & 92.11 \(\pm\) 0.14 & 83.67 \(\pm\) 0.31 \\ Grace & 81.92 \(\pm\) 0.89 & 71.21 \(\pm\) 0.64 & 80.54 \(\pm\) 0.36 & 78.19 \(\pm\) 0.10 & 86.35 \(\pm\) 0.44 & 92.15 \(\pm\) 0.25 & 92.91 \(\pm\) 0.20 & 83.32 \(\pm\) 0.41 \\ Gca & 82.38 \(\pm\) 0.47 & 71.51 \(\pm\) 0.32 & 80.89 \(\pm\) 0.28 & 78.29 \(\pm\) 0.36 & 87.88 \(\pm\) 0.26 & 92.33 \(\pm\) 0.68 & 92.64 \(\pm\) 0.34 & 83.70 \(\pm\) 0.39 \\ Bari & 81.30 \(\pm\) 0.54 & 72.06 \(\pm\) 0.63 & 80.52 \(\pm\) 0.30 & 76.13 \(\pm\) 0.18 & **89.09 \(\pm\) 0.51** & 92.15 \(\pm\) 0.32 & 92.33 \(\pm\) 0.39 & 83.37 \(\pm\) 0.41 \\ \hline Galopa & **84.21 \(\pm\) 0.30** & **74.34 \(\pm\) 0.18** & **84.57 \(\pm\) 0.34** & **81.23 \(\pm\) 0.19** & 88.65 \(\pm\) 0.11 & **92.77 \(\pm\) 0.40** & **93.04 \(\pm\) 0.25** & **85.54 \(\pm\) 0.25** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Mean node classification accuracy (%) for supervised and unsupervised models. The highest performance of unsupervised models is highlighted in **boldface**. OOM indicates Out-Of-Memory.

Figure 6: The mean graph/node classification accuracy when contrasting with different perturbation rates under 4 datasets. Fix one of the augmentations as NoAug and the other augmentation be the combination of edge perturbation and feature masking. Darker colors indicate better performance.

Node2Vec [17]; 3) _Graph contrastive learning methods_, including Gae, Vgae [24], Dgi [58], Gmi [41], MvGrl [19], Grace [77], Gca [78], and Bgrl [55]. For graph-level task, we evaluate the performance of Galopa in terms of the linear classification accuracy and compare it with 1) two _supervised learning methods_, including Gcn [25] and Gin [69]; 2) seven _kernel-based methods_, including SP [5], GK [49], WL [50], WLPM [39], FGW [56], Ddg [70], and Mlg [26]; 3) three _unsupervised methods_, including Node2Vec [17], Sub2Vec [1], Graph2Vec [37]; 4) five recent SOTA _self-supervised learning methods_ based on contrastive learning, including InfoGraph [52], GraphCl [72], Ad-Gcl [53], Joav2 [73], Rgcl [28] and SimGrace [66].

**Protocol.** We follow the standard evaluation protocol of previous state-of-the-art graph self-supervised learning approaches at the graph and node levels, respectively. Specifically, for graph classification, we report the mean 10-fold cross-validation accuracy after 5 runs followed by a linear SVM. The linear SVM is trained by applying cross-validation on training data folds and the best mean accuracy is reported. For node classification, we report the mean accuracy on the test set after 50 runs of training followed by a linear neural network model. For the graphs (nodes) datasets, we randomly split the data, where 80%/10%/10% (10%/10%/80%) of graphs (nodes) are selected for the training, validation, and test set, respectively.

**Implementation Details.** In the experiments, we use the Adam optimizer [23] with learning rate is tuned in \(\{0.0001,0.001,0.01\}\). The optimization routine and the convergence analysis are summarized in Appendix B. We conduct the experiment with the trade-off parameter \(\rho\) and \(\sigma\), the parameter \(C\) of Svm, batch size in the sets \(\{10^{-3},10^{-2},\ldots,10^{2},10^{3}\}\), \(\{0,0.1,\ldots,0.9,1\}\), \(\{10^{-3},\ldots,10^{3}\}\), \(\{16,64,128,256,512\}\), respectively. To perform graph augmentation, we use 4 types of operations: Edge Perturbation, Feature Masking, Node Dropping, and Graph Sampling. Our model is implemented with Pytorch Geometric [13] and Deep Graph Library [63].

### Performance Comparison

**Performance under Node-level.** Table 1 reports the averaged results over the node-level datasets. Comparing the results in Table 1, we have the following major observations. The proposed method outperforms the state-of-the-art self-supervised models significantly and even exceeds the supervised models on several datasets. For example, on PUBMED, Galopa achieves 84.57% accuracy, which is a 3.68% relative improvement over previous state-of-the-art unsupervised algorithms. When compared to supervised baselines, it outperforms strong supervised baselines: On CORA, CITESEER and PUBMED benchmarks we observe 2.67%, 3.61% and 5.41% relative improvement over Gcn, respectively. On Coauthor-CS, the proposed unsupervised method shows competitive performance compared to the supervised models. Tabel 1 lists the average accuracy of 7 benchmark datasets, from which Galopa achieves the best performance as well. For example, our proposed Galopa

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Model** & **PROTEINS** & **DD** & **MUTAG** & **NC11** & **COLLAB** & **IMDB-B** & **Average** \\ \hline Gcn & 74.92 \(\pm\) 0.33 & 76.24 \(\pm\) 0.14 & 85.63 \(\pm\) 0.24 & 80.20 \(\pm\) 0.14 & 79.01 \(\pm\) 0.18 & 70.45 \(\pm\) 0.37 & 77.74 \(\pm\) 0.23 \\ Gin & 76.28 \(\pm\) 0.28 & 78.91 \(\pm\) 0.13 & 89.47 \(\pm\) 0.16 & 82.75 \(\pm\) 0.19 & 80.23 \(\pm\) 0.19 & 73.70 \(\pm\) 0.60 & 80.22 \(\pm\) 0.25 \\ \hline SP & 75.07 \(\pm\) 0.54 & -jd & 85.25 \(\pm\) 0.24 & 73.53 \(\pm\) 0.16 & — & 55.62 \(\pm\) 0.02 & — \\ GK & 71.67 \(\pm\) 0.55 & 78.53 \(\pm\) 0.03 & 81.71 \(\pm\) 0.21 & 66.06 \(\pm\) 0.12 & 71.81 \(\pm\) 0.31 & 65.93 \(\pm\) 0.10 & 72.61 \(\pm\) 0.22 \\ WL & 72.92 \(\pm\) 0.56 & 79.78 \(\pm\) 0.36 & 80.76 \(\pm\) 0.30 & 80.01 \(\pm\) 0.50 & 69.30 \(\pm\) 0.42 & 72.30 \(\pm\) 0.44 & 75.84 \(\pm\) 0.43 \\ WLPM & — & 78.79 \(\pm\) 0.38 & 87.13 \(\pm\) 0.42 & **86.32 \(\pm\) 0.19** & — & — & — \\ FGW & 74.50 \(\pm\) 0.23 & — & 88.34 \(\pm\) 0.12 & 86.24 \(\pm\) 0.31 & — & 62.97 \(\pm\) 0.24 & — \\ DgK & 73.21 \(\pm\) 0.61 & 74.79 \(\pm\) 0.32 & 87.51 \(\pm\) 0.65 & 79.98 \(\pm\) 0.36 & 64.43 \(\pm\) 0.48 & 67.09 \(\pm\) 0.37 & 74.50 \(\pm\) 0.46 \\ Mlg & 41.23 \(\pm\) 0.27 & \textgreater{}1d & 87.94 \(\pm\) 0.16 & \textgreater{}1d & \textgreater{}1d & 66.67 \(\pm\) 0.30 & — \\ \hline Node2Vec & 57.58 \(\pm\) 0.36 & — & 72.62 \(\pm\) 1.02 & 54.93 \(\pm\) 0.16 & 56.12 \(\pm\) 0.02 & 50.25 \(\pm\) 0.09 & — \\ Sub2Vec & 53.06 \(\pm\) 0.56 & 54.33 \(\pm\) 0.24 & 61.17 \(\pm\) 1.59 & 52.82 \(\pm\) 0.15 & 55.26 \(\pm\) 0.15 & 55.34 \(\pm\) 0.15 & 55.33 \(\pm\) 0.47 \\ Graph2Vec & 73.33 \(\pm\) 0.21 & 79.32 \(\pm\) 0.29 & 83.28 \(\pm\) 0.93 & 73.21 \(\pm\) 0.18 & 71.10 \(\pm\) 0.54 & 71.16 \(\pm\) 0.05 & 75.23 \(\pm\) 0.36 \\ \hline InfoGraph & 74.44 \(\pm\) 0.31 & 72.85 \(\pm\) 1.78 & 89.01 \(\pm\) 1.13 & 76.20 \(\pm\) 1.06 & 70.05 \(\pm\) 1.13 & **73.03 \(\pm\) 0.87** & 75.93 \(\pm\) 1.04 \\ Graph2Vec & 74.39 \(\pm\) 0.45 & 78.62 \(\pm\) 0.40 & 86.80 \(\pm\) 1.34 & 77.87 \(\pm\) 0.41 & 73.16 \(\pm\) 1.15 & 71.14 \(\pm\) 0.44 & 76.69 \(\pm\) 0.69 \\ ad-GCL & 73.28 \(\pm\) 0.46 & 75.79 \(\pm\) 0.87 & 88.74 \(\pm\) 1.85 & 73.91 \(\pm\) 0.77 & 72.02 \(\pm\) 0.56 & 70.21 \(\pm\) 0.68 & 75.65 \(\pm\) 0.86 \\ Joa0v2 & 74.13 \(\pm\) 0.51 & 77.32 \(\pm\) 0.29 & 87.17 \(\pm\) 1.09 & 78.40 \(\pm\) 0.17 & 69.19 \(\pm\) 0.16 & 70.37 \(\pm\) 0.37 & 76.09 \(\pm\) 0.43 \\ Rgcl & 75.03 \(\pm\) 0.43 & 78.86 \(\pm\) 0.48 & 87.66 \(\pm\) 1.01 & 78.14 \(\pm\) 1.08 & 70.92 \(\pm\) 0.65 & 71.85 \(\pm\) 0.84 & 77.07 \(\pm\) 0.74 \\ SimGrace & 75.23 \(\pm\) 0.19 & 77.45 \(\pm\) 1.03 & 89.27 \(\pm\) 1.39 & 79.10 \(\pm\) 0.25 & 71.37 \(\pm\) 0.44 & 71.45 \(\pm\) 0.29 & 77.31 \(\pm\) 0.59 \\ \hline Galopa & **76.93 \(\pm\) 0.18** & **83.87 \(\pm\) 0.42** & **91.11 \(\pm\) 1.27** & 77.86 \(\pm\) 0.36 & **73.20 \(\pm\) 0.37** & 70.72 \(\outperforms the unsupervised SOTA baseline SimGrace by 1.84% on average, and even outperforms supervised GCN by 0.34%. These results further validate that calibrating the backbone model by optimal transport alignment can produce expressive and generalizable representations.

**Performance under Graph-level.** In this section, we examine whether the proposed Galopa performs better than state-of-the-art methods at graph-level datasets. The results of supervised learning baselines and unsupervised methods are reported in Table 2. The results shown in Table 2 suggest that Galopa achieves state-of-the-art results with respect to unsupervised models. For example, on DD it achieves 83.87% accuracy, a 4.09% relative improvement over the previous state-of-the-art baselines. For kernel methods, our approach achieves better performance on most datasets. When compared to supervised baselines individually, our model outperforms Gcn in 4 out of 6 datasets and outperforms Gin in 3 out of 6 datasets, e.g., a 1.64% relative improvement on Gin for the NCI1 dataset. Our approach outperforms the state-of-the-art graph contrastive learning approaches. For example, compared to SimGrace, which is one of the best SOTA methods, Galopa has a relative improvement of 1.63% on average across all datasets. Galopa outperforms GraphCl and InfoGraph with a relative improvement of 2.25% and 3.01% on average, respectively. To summarize, our newly proposed Galopa for graph self-supervised has achieved SOTA performance.

## Discussion

**Conclusion.** In this paper, we investigated the self-supervised graph learning problem, addressing the challenges posed by label-invariant issues in contrasting graph learning. Unlike existing methods that adopt contrastive or distance-based regularization approaches, we propose a novel paradigm based on optimal transport for self-supervised graph learning. Our approach focuses on aligning optimal transport plans between the graph space and the representation space. By aligning the transport plans, our method enforces the backbone model to learn representations that precisely preserve the matching relationships in the original graph space. Our observations reveal several noteworthy findings: (i) The optimal transport plan serves as a more informative calibration signal for the encoder compared to the transport distance, capturing essential characteristics; (ii) It is feasible to utilize only node attributes as a correction signal for the backbone model, without relying on edge information; (iii) Our proposed graph self-supervised model eliminates the need to distinguish between positive and negative samples and overcomes the label-invariant assumption; Furthermore, extensive experiments conducted on multiple benchmark datasets demonstrate the state-of-the-art performance of our proposed framework in terms of generalizability and robustness.

**Limitations and Future Work.** Although the transport plan opens the door for direct communication between the input graph space and the output representation space, it also becomes a computational bottleneck for the model to some extent due to the limitation of optimal transport computation complexity. To reduce the time complexity, we can utilize the properties of the proposed model and/or the scaling optimal transport techniques that can reduce the time complexity from cubic to square or even to linear, we provide 4 ways to do this below: (i) Unlike general OT settings, where the two graphs are typically quite different and the matching relationship between them is completely unknown, the difference between the original and augmented graphs in Galopa is quite small and the matching relations for subgraph components except with different part (i.e., complementary set of difference part) is known. This means that we can utilize the _matching prior_ to reduce the computational cost. Hence, we can split the difference part with its neighborhood from the two graphs and compute the optimal transport plan only for that part. Since the percentage of that part is very small, it can greatly reduce the time complexity; (ii) According to the observation in Section 6, we can avoid the cubic complexity of optimizing GW by using only the node attributes for computing the optimal plan in graph space, while retaining similar performance with near-square time complexity; (iii) Alternatively, we can reduce the computational cost by utilizing sparsity [27] or graph partitioning [10, 67]. In particular, we can employ the most recent work on linear optimal transport [9, 38], which computes FGW term and/or Wasserstein term in linear time; (iv) We have the option to combine the aforementioned methods. For instance, by merging insights from the first point, a significant portion of subgraph pairs acquired via graph partitioning in the third point turns out to be identical. This realization can further pare down the complexity of partitioning methods.

As the main goal of this paper is to propose an alternative self-supervised graph learning paradigm beyond the label-invariant assumption that accurately links/communicates the input and output spaces, we leave the scalability issue as our future work.

## Acknowledgments and Disclosure of Funding

We thank Edouard Pauwels and Samuel Vaiter for their valuable work and for proving the convergence of the derivatives of Sinkhorn-Knopp. We also thank the anonymous reviewers for their constructive suggestions. This project was in part supported by the following projects: the National Natural Science Foundation of China (No.62032013, No.92267206); Singapore Institute of Technology Ignition Grant (No.R-IE2-A405-0001).

## References

* [1] Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B Aditya Prakash. Sub2vec: Feature learning for subgraphs. In _Pacific-Asia Conference on KDDM_, pages 170-182. Springer, 2018.
* [2] Jonas Adler and Sebastian Lunz. Banach wasserstein gan. _Advances in Neural Information Processing Systems_, 31, 2018.
* [3] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. _Advances in Neural Information Processing Systems_, 14, 2001.
* [4] Nicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation using lagrangian mass transport. In _Proceedings of the 2011 SIGGRAPH Asia Conference_, pages 1-12, 2011.
* [5] Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In _IEEE International Conference on Data Mining_, pages 8 pp.-. IEEE, 2005.
* [6] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. LightGCL: Simple yet effective graph contrastive learning for recommendation. In _International Conference on Learning Representations_, 2023.
* [7] Jiezhang Cao, Langyuan Mo, Yifan Zhang, Kui Jia, Chunhua Shen, and Mingkui Tan. Multi-marginal wasserstein gan. _Advances in Neural Information Processing Systems_, 32, 2019.
* [8] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, number 1, 2016.
* [9] Yidong Chen, Chen Li, and Zhonghua Lu. Computing wasserstein-p distance between images with linear cost. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 519-528, 2022.
* [10] Samir Chowdhury, David Miller, and Tom Needham. Quantized gromov-wasserstein. In _Machine Learning and Knowledge Discovery in Databases_, pages 811-827. Springer, 2021.
* [11] Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. _Advances in Neural Information Processing Systems_, 30, 2017.
* [12] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in Neural Information Processing Systems_, 26, 2013.
* [13] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [14] Ji Gao, Xiao Huang, and Jundong Li. Unsupervised graph alignment with wasserstein distance discriminator. In _Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 426-435, 2021.
* [15] Xinbo Gao, Bing Xiao, Dacheng Tao, and Xuelong Li. A survey of graph edit distance. _Pattern Analysis and Applications_, 13:113-129, 2010.
* [16] Amur Ghose, Yingxue Zhang, Jianye Hao, and Mark Coates. Spectral augmentations for graph contrastive learning. _arXiv:2302.02909_, 2023.
* [17] Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In _Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 855-864, 2016.
* [18] Xiang Gu, Yucheng Yang, Wei Zeng, Jian Sun, and Zongben Xu. Keypoint-guided optimal transport with applications in heterogeneous domain adaptation. _Advances in Neural Information Processing Systems_, 35:14972-14985, 2022.

* [19] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In _Proceedings of the International Conference on Machine Learning_, pages 4116-4126. PMLR, 2020.
* [20] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in Neural Information Processing Systems_, 33:22118-22133, 2020.
* [21] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In _Proceedings of the International Conference on Machine Learning_, pages 427-435. PMLR, 2013.
* [22] Dongki Kim, Jinheon Baek, and Sung Ju Hwang. Graph self-supervised learning with accurate discrepancy learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [24] Thomas Kipf and Max Welling. Variational graph auto-encoders. _arXiv:1611.07308_, 2016.
* [25] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv:1609.02907_, 2016.
* [26] Risi Kondor and Horace Pan. The multiscale laplacian graph kernel. _Advances in Neural Information Processing Systems_, 29, 2016.
* [27] Mengyu Li, Jun Yu, Hongteng Xu, and Cheng Meng. Efficient approximation of gromov-wasserstein distance using importance sparsification. _Journal of Computational and Graphical Statistics_, pages 1-12, 2023.
* [28] Sihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning. In _Proceedings of the International Conference on Machine Learning_, pages 13052-13065, 2022.
* [29] Lu Lin, Jinghui Chen, and Hongning Wang. Spectral augmentation for self-supervised learning on graphs. _arXiv preprint arXiv:2210.00643_, 2022.
* [30] Huidong Liu, Xianfeng Gu, and Dimitris Samaras. Wasserstein gan with quadratic transport cost. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4832-4841, 2019.
* [31] Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. Revisiting graph contrastive learning from the perspective of graph spectrum. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [32] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Revisiting heterophily for graph neural networks. _Advances in Neural Information Processing Systems_, 35:1362-1375, 2022.
* [33] Facundo Memoli. Spectral gromov-wasserstein distances for shape matching. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops_, pages 256-263. IEEE, 2009.
* [34] Facundo Memoli. Gromov-wasserstein distances and the metric approach to object matching. _Foundations of Computational Mathematics_, 11:417-487, 2011.
* [35] Gaspard Monge. Memoire sur la theorie des deblais et des remblais. _Mem. Math. Phys. Acad. Royale Sci._, pages 666-704, 1781.
* [36] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv:2007.08663_, 2020.
* [37] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. Graph2vec: Learning distributed representations of graphs. _arXiv:1707.05005_, 2017.
* [38] Dai Hai Nguyen and Koji Tsuda. On a linear fused gromov-wasserstein distance for graph structured data. _Pattern Recognition_, 138:109351, 2023.
* [39] Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching node embeddings for graph similarity. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.

* [40] Edouard Pauwels and Samuel Vaiter. The derivatives of sinkhorn-knopp converge. _SIAM Journal on Optimization_, 33(3):1494-1517, 2023.
* [41] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph representation learning via graphical mutual information maximization. In _Proceedings of The Web Conference 2020_, pages 259-270, 2020.
* [42] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In _Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 701-710, 2014.
* [43] Gabriel Peyre, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and distance matrices. In _Proceedings of the International Conference on Machine Learning_, pages 2664-2672. PMLR, 2016.
* [44] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [45] Meyer Scetbon and Marco Cuturi. Linear time sinkhorn divergences using positive features. _Advances in Neural Information Processing Systems_, 33:13468-13480, 2020.
* [46] Thibault Sejourne, Francois-Xavier Vialard, and Gabriel Peyre. The unbalanced gromov wasserstein distance: Conic formulation and relaxation. _Advances in Neural Information Processing Systems_, 34:8766-8779, 2021.
* [47] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls of graph neural network evaluation. _arXiv:1811.05868_, 2018.
* [48] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation learning for domain adaptation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [49] Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efficient graphlet kernels for large graph comparison. In _Artificial Intelligence and Statistics_, pages 488-495. PMLR, 2009.
* [50] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. _Journal of Machine Learning Research_, 12(9), 2011.
* [51] Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. _ACM Transactions on Graphics_, 34(4):1-11, 2015.
* [52] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. _arXiv:1908.01000_, 2019.
* [53] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. _Advances in Neural Information Processing Systems_, 34:15920-15933, 2021.
* [54] Jianheng Tang, Weiqi Zhang, Jiajin Li, Kangfei Zhao, Fugee Tsung, and Jia Li. Robust attributed graph alignment via joint structure learning and optimal transport. _arXiv preprint arXiv:2301.12721_, 2023.
* [55] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer, Remi Munos, Petar Velickovic, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In _International Conference on Learning Representations_, 2022.
* [56] Vayer Titouan, Nicolas Courty, Romain Tavenard, and Remi Flamary. Optimal transport for structured data with application on graphs. In _Proceedings of the International Conference on Machine Learning_, pages 6275-6284. PMLR, 2019.
* [57] Vayer Titouan, Ievgen Redko, Remi Flamary, and Nicolas Courty. Co-optimal transport. _Advances in Neural Information Processing Systems_, 33:17559-17570, 2020.
* [58] Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. _International Conference on Learning Representations_, 2019.
* [59] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.

* [60] Cedric Vincent-Cuez, Remi Flamary, Marco Corneli, Titouan Vayer, and Nicolas Courty. Semi-relaxed gromov-wasserstein divergence and applications on graphs. In _International Conference on Learning Representations_, 2022.
* [61] Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In _Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1225-1234, 2016.
* [62] Haonan Wang, Jieyu Zhang, Qi Zhu, and Wei Huang. Can single-pass contrastive learning work for both homophilic and heterophilic graph? _arXiv preprint arXiv:2211.10890_, 2022.
* [63] Minjie Yu Wang. Deep graph library: Towards efficient and scalable deep learning on graphs. In _International Conference on Learning Representations Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* [64] Yejiang Wang, Yuhai Zhao, Zhengkui Wang, and Meixia Wang. Robust self-supervised multi-instance learning with structure awareness. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 10218-10225, 2023.
* [65] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In _International Conference on Machine Learning_, pages 6861-6871. PMLR, 2019.
* [66] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. In _Proceedings of the ACM Web Conference 2022_, pages 1070-1079, 2022.
* [67] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph partitioning and matching. _Advances in Neural Information Processing Systems_, 32, 2019.
* [68] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning for graph matching and node embedding. In _Proceedings of the International Conference on Machine Learning_, pages 6932-6941. PMLR, 2019.
* [69] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv:1810.00826_, 2018.
* [70] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1365-1374, 2015.
* [71] Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogcl: Automated graph contrastive learning via learnable view generators. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8892-8900, 2022.
* [72] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. _Advances in Neural Information Processing Systems_, 33:5812-5823, 2020.
* [73] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In Marina Meila and Tong Zhang, editors, _Proceedings of the International Conference on Machine Learning_, pages 12121-12132. PMLR, 18-24 Jul 2021.
* [74] Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. Label-invariant augmentation for semi-supervised graph classification. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [75] Yuhai Zhao, Yejiang Wang, Zhengkui Wang, and Chengqi Zhang. Multi-graph multi-label learning with dual-granularity labeling. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2327-2337, 2021.
* [76] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks for graphs with heterophily: A survey. _arXiv preprint arXiv:2202.07082_, 2022.
* [77] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. _arXiv:2006.04131_, 2020.
* [78] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In _Proceedings of the Web Conference 2021_, pages 2069-2080, 2021.