# The Minimax Rate of HSIC Estimation for

Translation-Invariant Kernels

 Florian Kalinke

Institute for Program Structures and Data Organization

Karlsruhe Institute of Technology

Karlsruhe, Germany

florian.kalinke@kit.edu

&Zoltan Szabo

Department of Statistics

London School of Economics

London, UK

z.szabo@lse.ac.uk

###### Abstract

Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of \(M\geq 2\) random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on \(\mathbb{R}^{d}\) for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is \(\mathcal{O}\big{(}n^{-1/2}\big{)}\). Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystrom-based one) on \(\mathbb{R}^{d}\).

## 1 Introduction

Kernel methods (Steinwart and Christmann, 2008; Berlinet and Thomas-Agnan, 2004; Saitoh and Sawano, 2016) allow embedding probability measures into reproducing kernel Hilbert spaces (RKHS; (Aronszajn, 1950)) by use of a positive definite function, the _kernel function_. This approach has gained considerable attention over the last 20 years. Such embeddings induce the so-called maximum mean discrepancy (MMD; (Smola et al., 2007; Gretton et al., 2012)), which quantifies the discrepancy of two probability measures by considering the RKHS norm of the distance of their respective embeddings. MMD is a metric on the space of probability distributions if the kernel is characteristic (Fukumizu et al., 2008; Sriperumbudur et al., 2010). MMD is also an integral probability metric (Zolotarev, 1983; Muller, 1997) where the underlying function class is chosen to be the unit ball in the corresponding RKHS.

MMD allows for the quantification of dependence by considering the distance between the embedding of a joint distribution and that of the product of its marginals. This construction gives rise to the so-called Hilbert-Schmidt independence criterion (HSIC; (Gretton et al., 2005)), which is also equal to the RKHS norm of the centered cross-covariance operator. In fact, one of the most widely-used independence measures in statistics, distance covariance (Szekely et al., 2007; Szekely and Rizzo, 2009; Lyons, 2013), was shown to be equivalent to HSIC (Sejdinovic et al., 2013) when the latter is specialized to \(M=2\) components; Sheng and Sriperumbudur (2023) proved a similar result for the conditional case. For \(M>2\) components (Quadrianto et al., 2009; Sejdinovic et al., 2013; Pfister et al., 2018), universality (Steinwart, 2001; Micchelli et al., 2006; Carmeli et al., 2010; Sriperumbudur et al., 2011) of the kernels \((k_{m})_{m=1}^{M}\) (on the respective domains) underlying HSIC guarantees that this measure captures independence (Szabo and Sriperumbudur, 2018). In the case of \(M=2\), characteristic \((k_{m})_{m=1}^{2}\) suffice (Lyons, 2013).

HSIC has been deployed successfully in numerous contexts, including independence testing in batch (Gretton et al., 2008; Wehbe and Ramdas, 2015; Bilodeau and Nangue, 2017; Gorecki et al., 2018; Pfister et al., 2018; Albert et al., 2022; Shekhar et al., 2023) and streaming (Podkopaev et al., 2023) settings, feature selection (Camps-Valls et al., 2010; Song et al., 2012; Yamada et al., 2014; Wang et al., 2022) with applications in biomarker detection (Climente-Gonzalez et al., 2019) and wind power prediction (Bouche et al., 2023), clustering (Song et al., 2007; Climente-Gonzalez et al., 2019), and causal discovery (Mooij et al., 2016; Pfister et al., 2018; Chakraborty and Zhang, 2019; Scholkopf et al., 2021; Kalinke and Szabo, 2023). In addition, HSIC has recently found successful applications in sensitivity analysis (Veiga, 2015; Freitas Gustavo et al., 2023; Fellmann et al., 2024; Herrando-Perez and Saltre, 2024), in the context of uncertainty quantification (Stenger et al., 2020), for the analysis of data augmentation methods for brain tumor detection (Anaya-Isaza and Mera-Jimenez, 2022), and that of multimodal neural networks trained on neuroimaging data (Fedorov et al., 2024).

Many estimators for HSIC exist. The classical ones rely on U-statistics or V-statistics (Gretton et al., 2005; Quadrianto et al., 2009; Pfister et al., 2018) and are known to converge at a rate of \(\mathcal{O}_{P}\left(n^{-1/2}\right)\). In fact, the V-statistic-based estimators are obtained by replacing the population kernel mean embedding with its empirical counterpart; estimating the mean embedding can be carried out at a speed \(\mathcal{O}_{P}\left(n^{-1/2}\right)\)(Smola et al., 2007; Theorem 2), which implies that HSIC can be estimated at the same rate. Existing approximations such as Nystrom HSIC (Kalinke and Szabo, 2023), also achieve this rate under the assumption of an appropriate rate of decay of the effective dimension. While all of these upper bounds match asymptotically, it is not known whether HSIC can be estimated at a faster rate, that is, whether the upper bound of \(\mathcal{O}_{P}\left(n^{-1/2}\right)\) is optimal in the minimax sense, or if designing estimators achieving better rates is possible. Lower bounds for the related MMD are known (Tolstikhin et al., 2016), but the existing analysis considers radial kernels and relies on independent Gaussian distributions. Radial kernels are a special case of the more general class of translation-invariant kernels that we consider.1 The reliance on independent Gaussian distributions renders the analysis of Tolstikhin et al. (2016) inapplicable for HSIC estimation. We tackle both of these severe restrictions in the present article.

Footnote 1: The family of radial kernels encompasses, for example, Gaussians, mixtures of Gaussians, inverse multi-quadratics, and Matern kernels; the Laplace kernel is translation-invariant but not radial (with respect to the traditionally-chosen Euclidean norm \(\left\|\cdot\right\|_{\mathbb{R}^{d}}\)).

We make the following **contributions**.

* We establish the minimax lower bound \(\mathcal{O}\left(n^{-1/2}\right)\) of HSIC estimation with \(M\geq 2\) components on \(\mathbb{R}^{d}\) with continuous bounded translation-invariant characteristic kernels. As this lower bound matches the known upper bounds of the existing "classical" U-statistic and V-statistic-based estimators, and that of the Nystrom HSIC estimator, our result settles their minimax optimality.
* Specifically, our result also implies the minimax lower bound of \(\mathcal{O}\left(n^{-1/2}\right)\) for the estimation of the cross-covariance operator, which can be further specialized to get back the minimax result (Zhou et al., 2019, Theorem 5) on the estimation of the covariance operator.

The paper is structured as follows. Notations are introduced in Section 2. Section 3 is dedicated to our main result on the minimax rate of HSIC estimation on \(\mathbb{R}^{d}\), with proof presented in Section 4. An auxiliary result on the Kullback-Leibler divergence is shown in Appendix A.

## 2 Notations

In this section, we introduce a few notations \(\mathbb{N}_{>0}\), \([M]\), \(\mathbf{I}_{n}\), \(\mathbf{0}_{n}\), \(\mathbf{1}_{n}\), \(\mathbf{A}^{\mathsf{T}}\), \(\left\langle\mathbf{v},\mathbf{w}\right\rangle\), \(\left\|\mathbf{v}\right\|_{\mathbb{R}^{d}}\), \(\mathrm{bdiag}\left(\mathbf{M}_{1},\ldots,\mathbf{M}_{N}\right)\), \(\left|\mathbf{A}\right|\), \(\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\), \(\psi_{\mathrm{P}}\), \(\mathrm{KL}(\mathbb{P}\|\mathbb{Q})\), \(L^{2}\left(\mathbb{R}^{d},\Lambda\right)\), \(\left\|f\right\|_{L^{2}\left(\mathbb{R}^{d},\Lambda\right)}\), \(\mathrm{supp}(\Lambda)\), \(\mathcal{H}_{k}\), \(\phi_{k}\), \(k\), \(\mu_{k}\), \(\mathrm{MMD}_{k}\), \(\otimes_{m=1}^{M}\mathcal{H}_{k_{m}}\), \(\otimes_{m=1}^{M}k_{m}\), \(\mathbb{P}_{m}\), \(\otimes_{m=1}^{M}\mathbb{P}_{m}\), \(\mathcal{O}_{P}\left(r_{n}\right)\), \(\mathcal{O}_{On}(a_{n})\), \(a_{n}\asymp b_{n}\), \(\mathrm{HSIC}_{k}\), and \(C_{X}\). Throughout the paper we consider random variables, probability measures, and kernels on \(\mathbb{R}^{d}\).

For \(M\in\mathbb{N}_{>0}:=\{1,2,\ldots\}\), let \([M]:=\{1,\ldots,M\}\). Denote by \(\mathbf{I}_{n}\) the \(n\times n\)-sized identity matrix and by \(\mathbf{0}_{n}=(0,\ldots,0)^{\mathsf{T}}\in\mathbb{R}^{n}\) (resp. \(\mathbf{1}_{n}=(1,\ldots,1)^{\mathsf{T}}\in\mathbb{R}^{n}\)) a column vector of zeros (resp. ones). The transpose of a matrix \(\mathbf{A}\in\mathbb{R}^{d_{1}\times d_{2}}\) is written as \(\mathbf{A}^{\mathsf{T}}\in\mathbb{R}^{d_{2}\times d_{1}}\). For \(\mathbf{v},\mathbf{w}\in\mathbb{R}^{d}\), \(\left\langle\mathbf{v},\mathbf{w}\right\rangle=\mathbf{v}^{\mathsf{T}} \mathbf{w}\) stands for their Euclidean inner product; \(\left\|\mathbf{v}\right\|_{\mathbb{R}^{d}}=\sqrt{\left\langle\mathbf{v}, \mathbf{v}\right\rangle}\) is the associated Euclidean norm.

\(\mathrm{bdiag}\left(\mathbf{M}_{1},\ldots,\mathbf{M}_{N}\right)\) forms a block-diagonal matrix from its arguments \(\left(\mathbf{M}_{n}\right)_{n=1}^{N}\) (\(\mathbf{M}_{n}\in\mathbb{R}^{d_{n}\times d_{n}}\), \(n\in[N]\)) and \(|\mathbf{A}|\) denotes the determinant of a matrix \(\mathbf{A}\in\mathbb{R}^{d\times d}\).

The set of Borel probability measures on \(\mathbb{R}^{d}\) is denoted by \(\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\). For a random variable \(X\sim\mathbb{P}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\), we denote its characteristic function by \(\psi_{\mathbb{P}}(\boldsymbol{\omega})=\mathbb{E}_{X\sim\mathbb{P}}\left[e^{ i\langle\boldsymbol{\omega},X\rangle}\right]\) with \(\boldsymbol{\omega}\in\mathbb{R}^{d}\) and \(i=\sqrt{-1}\). Let \(\mathbb{P},\mathbb{Q}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\), assume that \(\mathbb{P}\) is absolutely continuous w.r.t. \(\mathbb{Q}\), and let \(\frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}}\) denote the corresponding Radon-Nikodym derivative (of \(\mathbb{P}\) w.r.t. \(\mathbb{Q}\)). Then, the Kullback-Leibler divergence of \(\mathbb{P}\) and \(\mathbb{Q}\) is defined as \(\mathrm{KL}(\mathbb{P}||\mathbb{Q}):=\int_{\mathbb{R}^{d}}\log\left(\frac{ \mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}}(\mathbf{x})\right)\mathrm{d} \mathbb{P}(\mathbf{x})\). Given a measure space \(\left(\mathbb{R}^{d},\mathcal{B}\left(\mathbb{R}^{d}\right),\Lambda\right)\), we denote by \(L^{2}(\mathbb{R}^{d},\Lambda):=L^{2}\left(\mathbb{R}^{d},\mathcal{B}\left( \mathbb{R}^{d}\right),\Lambda\right)\) the Hilbert space of (equivalence classes of) measurable functions \(f:\left(\mathbb{R}^{d},\mathcal{B}\left(\mathbb{R}^{d}\right)\right)\to \left(\mathbb{R},\mathcal{B}\left(\mathbb{R}\right)\right)\) for which \(\|f\|_{L^{2}(\mathbb{R}^{d},\Lambda)}^{2}:=\int_{\mathbb{R}^{d}}|f(\mathbf{x} )|^{2}\mathrm{d}\Lambda(\mathbf{x})<\infty\). The support of a probability measure \(\Lambda\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\) denoted by \(\mathrm{supp}(\Lambda)\) is the subset of \(\mathbb{R}^{d}\) for which every open neighborhood of \(\mathbf{x}\in\mathbb{R}^{d}\) has positive measure (Cohn, 2013, p. 207).

A function \(k:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) is called a kernel if there exists a Hilbert space \(\mathcal{H}\) and a feature map \(\phi:\mathbb{R}^{d}\to\mathcal{H}\) such that \(k(\mathbf{x},\mathbf{x}^{\prime})=\left\langle\phi(\mathbf{x}),\phi(\mathbf{x }^{\prime})\right\rangle_{\mathcal{H}}\) for all \(\mathbf{x},\mathbf{x}^{\prime}\in\mathbb{R}^{d}\). A Hilbert space of functions \(h:\mathbb{R}^{d}\to\mathbb{R}\) is an RKHS \(\mathcal{H}_{k}\) associated to a kernel \(k:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) if \(k(\cdot,\mathbf{x})\in\mathcal{H}_{k}\) and \(\left\langle h,k(\cdot,\mathbf{x})\right\rangle_{\mathcal{H}_{k}}=h(\mathbf{ x})\) for all \(\mathbf{x}\in\mathbb{R}^{d}\) and \(h\in\mathcal{H}_{k}\).2 In this work, we assume all kernels to be measurable and bounded.3 The function \(\phi_{k}(\mathbf{x}):=k(\cdot,\mathbf{x})\) is the canonical feature map, and \(k(\mathbf{x},\mathbf{x}^{\prime})=\left\langle k(\cdot,\mathbf{x}),k(\cdot, \mathbf{x}^{\prime})\right\rangle_{\mathcal{H}_{k}}=\left\langle\phi_{k}( \mathbf{x}),\phi_{k}(\mathbf{x}^{\prime})\right\rangle_{\mathcal{H}_{k}}\) for all \(\mathbf{x},\mathbf{x}^{\prime}\in\mathbb{R}^{d}\). A function \(\kappa:\mathbb{R}^{d}\to\mathbb{R}\) is called positive definite if \(\sum_{i,j\in[n]}c_{i}c_{j}\kappa(\mathbf{x}_{i}-\mathbf{x}_{j})\geq 0\) for all \(n\in\mathbb{N}_{>0}\), \(\mathbf{c}=\left(c_{i}\right)_{i=1}^{n}\in\mathbb{R}^{n}\), and \(\{\mathbf{x}_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d}\). A kernel \(k:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) is said to be translation-invariant if there exists a positive definite function \(\kappa:\mathbb{R}^{d}\to\mathbb{R}\) such that \(k(\mathbf{x},\mathbf{x}^{\prime})=\kappa(\mathbf{x}-\mathbf{x}^{\prime})\) for all \(\mathbf{x},\mathbf{x}^{\prime}\in\mathbb{R}^{d}\). By Bochner's theorem (Wendland, 2005, Theorem 6.6) (recalled in Theorem B.1) for a continuous bounded translation-invariant kernel \(k:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) there exists a finite non-negative Borel measure \(\Lambda_{k}\) such that

Footnote 2: For fixed \(\mathbf{x}\in\mathbb{R}^{d}\), the function \(k(\cdot,\mathbf{x}):\mathbb{R}^{d}\to\mathbb{R}\) means \(\mathbf{x}^{\prime}\mapsto k(\mathbf{x}^{\prime},\mathbf{x})\).

Footnote 3: Boundedness of the kernel, that is, \(\sup_{\mathbf{x},\mathbf{x}^{\prime}\in\mathbb{R}^{d}}k(\mathbf{x},\mathbf{x}^ {\prime})<\infty\), implies boundedness of the feature map, that is, \(\sup_{\mathbf{x}\in\mathbb{R}^{d}}\left\|\phi_{k}(\mathbf{x})\right\|_{ \mathcal{H}_{k}}<\infty\) (and vice versa); it is also equivalent to \(\sup_{\mathbf{x}\in\mathbb{R}^{d}}k(\mathbf{x},\mathbf{x})<\infty\).

\[k(\mathbf{x},\mathbf{y})=\int_{\mathbb{R}^{d}}e^{-i\left(\mathbf{x}-\mathbf{y}, \boldsymbol{\omega}\right)}\mathrm{d}\Lambda_{k}(\boldsymbol{\omega})\] (1)

for all \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\). The (kernel) mean embedding of a probability measure \(\mathbb{P}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\) is

\[\mu_{k}(\mathbb{P})=\int_{\mathbb{R}^{d}}\phi_{k}(\mathbf{x})\mathrm{d} \mathbb{P}(\mathbf{x})\in\mathcal{H}_{k},\]

where the integral is meant in Bochner's sense (Diestel and Uhl, 1977, Chapter II.2); the boundedness of \(k\) ensures that it is well-defined. For \(\mathbb{P},\mathbb{Q}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\) one can define the (semi-)metric called maximum mean discrepancy (Smola et al., 2007, Gretton et al., 2012) as

\[\mathrm{MMD}_{k}(\mathbb{P},\mathbb{Q})=\|\mu_{k}(\mathbb{P})-\mu_{k}(\mathbb{Q}) \|_{\mathcal{H}_{k}}\,.\]

If the mean embedding \(\mu_{k}\) is injective, MMD is a metric and the kernel \(k\) is called characteristic (Fukumizu et al., 2008, Sriperumbudur et al., 2010, Szabo and Sriperumbudur, 2018).

Let \(\mathbb{R}^{d}=\times_{m=1}^{M}\mathbb{R}^{d_{m}}\) (\(d=\sum_{m=1}^{M}d_{m}\)) and assume that each domain \(\mathbb{R}^{d_{m}}\) is equipped with a kernel \(k_{m}:\mathbb{R}^{d_{m}}\times\mathbb{R}^{d_{m}}\to\mathbb{R}\) with associated RKHS \(\mathcal{H}_{k_{m}}\) (\(m\in[M]\)). The tensor product Hilbert space of \(\left(\mathcal{H}_{k_{m}}\right)_{m=1}^{M}\) is denoted by \(\otimes_{m=1}^{M}\mathcal{H}_{k_{m}}\); it is an RKHS (Berlinet and Thomas-Agnan, 2004, Theorem 13) with the tensor product kernel \(k=\otimes_{m=1}^{M}k_{m}:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) defined by

\[k\left((\mathbf{x}_{m})_{m=1}^{M},(\mathbf{x}_{m}^{\prime})_{m=1}^{M}\right)= \prod_{m\in[M]}k_{m}(\mathbf{x}_{m},\mathbf{x}_{m}^{\prime})\quad\text{for all}\quad\mathbf{x}_{m},\mathbf{x}_{m}^{\prime}\in\mathbb{R}^{d_{m}},\,m\in[M].\]

The kernel \(k\) has the canonical feature map \(\phi_{k}\left((\mathbf{x}_{m})_{m=1}^{M}\right)=\otimes_{m=1}^{M}\phi_{kwith joint distribution \(\mathbb{P}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\) and marginal distributions \(\mathbb{P}_{m}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d_{m}}\right)\) (\(m\in[M]\); \(d=\sum_{m=1}^{M}d_{m}\)). We write \(\otimes_{m=1}^{M}\mathbb{P}_{m}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\) for the product of measures \(\mathbb{P}_{m}\) (\(m\in[M]\)). Specifically, \(\mathbb{P}^{n}:=\otimes_{i=1}^{n}\mathbb{P}\in\mathcal{M}_{1}^{+}\left(\left( \mathbb{R}^{d}\right)^{n}\right)\) denotes the \(n\)-fold product of \(\mathbb{P}\). For a sequence of real-valued random variables \(\left(X_{n}\right)_{n=1}^{\infty}\) and a sequence \(\left(r_{n}\right)_{n=1}^{\infty}\) (\(r_{n}>0\) for all \(n\)), \(X_{n}=\mathcal{O}_{P}\left(r_{n}\right)\) denotes that \(\frac{X_{n}}{r_{n}}\) is bounded in probability. For positive sequences \(\left(a_{n}\right)_{n=1}^{\infty}\) and \(\left(b_{n}\right)_{n=1}^{\infty}\), \(b_{n}=\mathcal{O}(a_{n})\) if there exist constants \(C>0\) and \(n_{0}\in\mathbb{N}_{>0}\) such that \(b_{n}\leq Ca_{n}\) for all \(n\geq n_{0}\); \(a_{n}\asymp b_{n}\) if \(a_{n}=\mathcal{O}\left(b_{n}\right)\) and \(b_{n}=\mathcal{O}\left(a_{n}\right)\). One can define our quantity of interest, the Hilbert-Schmidt independence criterion (HSIC; (Gretton et al., 2005; Quadiantno et al., 2009; Pfister et al., 2018; Szabo and Sriperumbudur, 2018)), as

\[\mathrm{HSIC}_{k}(\mathbb{P}) =\mathrm{MMD}_{k}\left(\mathbb{P},\otimes_{m=1}^{M}\mathbb{P}_{m} \right)=\left\|C_{X}\right\|_{\mathcal{H}_{k}},\] \[C_{X} =\mu_{k}(\mathbb{P})-\mu_{k}\left(\otimes_{m=1}^{M}\mathbb{P}_{m }\right)\in\mathcal{H}_{k},\] (2)

and \(C_{X}\) denotes the centered cross-covariance operator.

## 3 Results

This section is dedicated to our results: The minimax lower bound for the estimation of \(\mathrm{HSIC}_{k}(\mathbb{P})\), where \(k\) is a product of continuous bounded translation-invariant characteristic kernels is given in Theorem 1(ii). For the specific case where \(k\) is a product of Gaussian kernels (stated in Theorem 1(i)), the constant in the lower bound is made explicit. Theorem 1(ii) also helps to establish a lower bound on the estimation of the cross-covariance operator (Corollary 1).

Before presenting our results, we recall the framework of minimax estimation (Tsybakov, 2009) adapted to our setting. Let \(\hat{F}_{n}\) denote any estimator of \(\mathrm{HSIC}_{k}(\mathbb{P})\) based on \(n\) i.i.d. samples from \(\mathbb{P}\). A sequence \(\left(\xi_{n}\right)_{n=1}^{\infty}\) (\(\xi_{n}>0\) for all \(n\)) is said to be a lower bound of HSIC estimation w.r.t. a class \(\mathcal{P}\) of Borel probability measures on \(\mathbb{R}^{d}\) if there exists a constant \(c>0\) such that

\[\inf_{\hat{F}_{n}}\sup_{\mathbb{P}\in\mathcal{P}}\mathbb{P}^{n}\left\{\xi_{n}^ {-1}\left|\mathrm{HSIC}_{k}(\mathbb{P})-\hat{F}_{n}\right|\geq c\right\}>0.\] (3)

If a specific estimator of HSIC \(\tilde{F}_{n}\) has an upper bound that matches \(\left(\xi_{n}\right)_{n=1}^{\infty}\) up to constants, that is,

\[\left|\mathrm{HSIC}_{k}(\mathbb{P})-\tilde{F}_{n}\right|=\mathcal{O}_{P}\left( \xi_{n}\right),\] (4)

then \(\tilde{F}_{n}\) is called minimax optimal.

We use Le Cam's method (Le Cam, 1973; Tsybakov, 2009) (recalled in Theorem B.5) to obtain bounds as in (3); estimators of HSIC achieving the bounds in (4) with \(\xi_{n}=n^{-1/2}\) are quoted in the introduction. The key to the application of the method is to show that there exist \(\alpha>0\) and \(n_{0}\in\mathbb{N}_{>0}\) such that for all \(n\geq n_{0}\) one can find an adversarial pair of distributions \(\left(\mathbb{P}_{\theta_{0}},\mathbb{P}_{\theta_{1}}\right)=\left(\mathbb{P}_ {\theta_{0}}(n),\mathbb{P}_{\theta_{1}}(n)\right)\in\mathcal{P}\times\mathcal{P}\) and \(s_{n}>0\) for which

1. \(\mathrm{KL}\left(\mathbb{P}_{\theta_{1}}^{n}\|\mathbb{P}_{\theta_{0}}^{n}\right)\leq\alpha\), in other words, the corresponding \(n\)-fold product measures must be similar in the sense of Kullback-Leibler divergence, but
2. \(\left|\mathrm{HSIC}_{k}(\mathbb{P}_{\theta_{1}})-\mathrm{HSIC}_{k}(\mathbb{P} _{\theta_{0}})\right|\geq 2s_{n}\), that is, their corresponding values of HSIC must be dissimilar.

In this case, \(\inf_{\hat{F}_{n}}\sup_{\mathbb{P}\in\mathcal{P}}\mathbb{P}^{n}\left\{\left| \mathrm{HSIC}_{k}(\mathbb{P})-\hat{F}_{n}\right|\geq s_{n}\right\}\geq\max \left(\frac{e^{-\alpha}}{4},\frac{1-\sqrt{\alpha/2}}{2}\right)\) for all \(n\geq n_{0}\); hence to establish the minimax optimality of existing estimators w.r.t. their known upper bounds, it is sufficient to find adversarial pairs \(\left\{\left(\mathbb{P}_{\theta_{0}}(n),\mathbb{P}_{\theta_{1}}(n)\right)\right\}_ {n\geq n_{0}}\) that satisfy 1. for some positive constant \(\alpha\) and also fulfill 2. with \(s_{n}\asymp n^{-1/2}\).

The proof of the first part of our statement relies on the following Lemma 1 which yields the analytical value of \(\mathrm{HSIC}_{k}\left(\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})\right)\), where \(k=\otimes_{m=1}^{M}k_{m}\) is the product of Gaussian kernels \(k_{m}\) (\(m\in[M]\)) and \(\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})\) denotes the multivariate normal distribution with mean \(\boldsymbol{\mu}\in\mathbb{R}^{d}\) and covariance matrix \(\boldsymbol{\Sigma}\in\mathbb{R}^{d\times d}\).

**Lemma 1** (Analytical value of HSIC for the Gaussian setting).: _Let us consider the Gaussian kernel \(k(\mathbf{x},\mathbf{y})=e^{-\frac{\gamma}{2}\left\|\mathbf{x}-\mathbf{y}\right\|_ {2^{d}}^{2}}\) (\(\gamma>0\), \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\)) and Gaussian random variable \(X=(X_{m})_{m=1}^{M}\sim\mathcal{N}(\mathbf{m},\mathbf{\Sigma})=:\mathbb{P}\), where \(X_{m}\in\mathbb{R}^{d_{m}}\) (\(m\in[M]\)), \(\mathbf{m}=(\mathbf{m}_{m})_{m=1}^{M}\in\mathbb{R}^{d}\), \(\mathbf{\Sigma}=[\mathbf{\Sigma}_{i,j}]_{i,j\in[M]}\in\mathbb{R}^{d\times d}\), \(\mathbf{\Sigma}_{i,j}\in\mathbb{R}^{d_{i}\times d_{j}}\), and \(d=\sum_{m\in[M]}d_{m}\). In this case, with \(\mathbf{\Sigma}_{1}=\mathbf{\Sigma}\) and \(\mathbf{\Sigma}_{2}=\mathrm{bdiag}(\mathbf{\Sigma}_{1,1},\ldots,\mathbf{ \Sigma}_{M,M})\), we have_

\[\mathrm{HSIC}_{k}^{2}(\mathbb{P})=\frac{1}{\left|2\gamma\mathbf{\Sigma}_{1}+ \mathbf{I}_{d}\right|^{\frac{1}{2}}}+\frac{1}{\left|2\gamma\mathbf{\Sigma}_{2} +\mathbf{I}_{d}\right|^{\frac{1}{2}}}-\frac{2}{\left|\gamma\mathbf{\Sigma}_{1 }+\gamma\mathbf{\Sigma}_{2}+\mathbf{I}_{d}\right|^{\frac{1}{2}}}.\]

In this work, we focus on continuous bounded translation-invariant kernels, which are fully characterized by Bochner's theorem (Wendland, 2005, Theorem 6.6); the theorem states that a function on \(\mathbb{R}^{d}\) is positive definite if and only if it is the Fourier transform of a finite nonnegative measure.4 We use this description to obtain our main result, which is as follows.

Footnote 4: We note that for many translation-invariant kernels, the corresponding spectral measures are known (Sripe-rumbudur et al., 2010, Table 2).

**Theorem 1** (Lower bound for HSIC estimation on \(\mathbb{R}^{d}\)).: _Let \(\mathcal{P}\) be a class of Borel probability measures over \(\mathbb{R}^{d}\) containing the \(d\)-dimensional Gaussian distributions. Let \(d=\sum_{m\in[M]}d_{m}\) and \(\hat{F}_{n}\) denote any estimator of \(\mathrm{HSIC}_{k}(\mathbb{P})\) with \(n\geq 2=:n_{0}\) i.i.d. samples from \(\mathbb{P}\in\mathcal{P}\). Assume further that \(k=\otimes_{m=1}^{M}k_{m}\) where either, for \(m\in[M]\),_

1. _the kernels_ \(k_{m}:\mathbb{R}^{d_{m}}\times\mathbb{R}^{d_{m}}\to\mathbb{R}\) _are Gaussian with common bandwidth parameter_ \(\gamma>0\) _defined by_ \((\mathbf{x}_{m},\mathbf{x}_{m}^{\prime})\mapsto e^{-\frac{\gamma}{2}\left\| \mathbf{x}_{m}-\mathbf{x}_{m}^{\prime}\right\|_{\mathbb{R}_{d_{m}}}^{2}}\)__(_\(\mathbf{x}_{m},\mathbf{x}_{m}^{\prime}\in\mathbb{R}^{d_{m}}\)_), or_
2. _the kernels_ \(k_{m}:\mathbb{R}^{d_{m}}\times\mathbb{R}^{d_{m}}\to\mathbb{R}\) _are continuous bounded translation-invariant characteristic kernels._

_Then, for any \(n\geq n_{0}\), it holds that_

\[\inf_{\hat{F}_{n}}\sup_{\mathbb{P}\in\mathcal{P}}\mathbb{P}^{n}\left\{\left| \mathrm{HSIC}_{k}\left(\mathbb{P}\right)-\hat{F}_{n}\right|\geq\frac{c}{\sqrt {n}}\right\}\geq\frac{1-\sqrt{\frac{5}{8}}}{2},\]

_with (i) the constant \(c=\frac{\gamma}{2(2\gamma+1)^{\frac{d}{4}+1}}>0\) (depending on \(\gamma\) and \(d\) only) in the first case, or (ii) some constant \(c>0\) in the second case._

We note that while Theorem 1(ii) applies to the more general class of translation-invariant kernels, we include Theorem 1(i) as it makes the constant \(c\) explicit.

The following corollary allows to recover the recent lower bound on the estimation of the covariance operator by Zhou et al. (2019, Theorem 5) as a special case that we detail in Remark 1(e).

**Corollary 1** (Lower bound on cross-covariance operator estimation).: _In the setting of Theorem 1(ii), let \(\hat{F}_{n}\) denote any estimator of the centered cross-covariance operator \(C_{X}\in\mathcal{H}_{k}\) defined in (2) with \(n\geq 2=:n_{0}\) i.i.d. samples from \(\mathbb{P}\in\mathcal{P}\). Then, for any \(n\geq n_{0}\), it holds that_

\[\inf_{\hat{F}_{n}}\sup_{\mathbb{P}\in\mathcal{P}}\mathbb{P}^{n}\left\{\left\|C_ {X}-\hat{F}_{n}\right\|_{\mathcal{H}_{k}}\geq\frac{c}{\sqrt{n}}\right\}\geq \frac{1-\sqrt{\frac{5}{8}}}{2},\]

_for some constant \(c>0\)._

**Remark 1**.:
1. _[leftmargin=*]_
2. _Validness of HSIC. Though generally the characteristic property of_ \((k_{m})_{m=1}^{M}\)_-s is not enough_ _(Szabo and Sriperumbudur, 2018, Example 2)_ _for_ \(M>2\) _to ensure the_ \(\mathcal{I}\)_-characteristic property of_ \(k=\otimes_{m=1}^{M}k_{m}\) _(in other words, that_ \(\mathrm{HSIC}_{k}(\mathbb{P})=0\) _iff._ \(\mathbb{P}=\otimes_{m=1}^{M}\mathbb{P}_{m}\)_), on_ \(\mathbb{R}^{d}\) _under the imposed continuous bounded translation-invariant assumption (i)_ \(k\) _being characteristic, (ii)_ \(k\) _being_ \(\mathcal{I}\)_-characteristic, and (iii)_ \((k_{m})_{m=1}^{M}\)_-s being characteristic are equivalent (Theorem_ B.4_)._
3. _Minimax optimality of existing HSIC estimators. The lower bounds in Theorem_ 1 _asymptotically match the known upper bounds of the U-statistic and V-statistic-based estimators of_ \(\xi_{n}=n^{-1/2}\)_The Nystrom-based HSIC estimator achieves the same rate under an appropriate decay of the eigenspectrum of the respective covariance operator. Hence, Theorem 1 implies the optimality of these estimators on \(\mathbb{R}^{d}\) with continuous bounded translation-invariant characteristic kernels in the minimax sense._
3. _Difference compared to Tolstikhin et al. (2016) (minimax MMD estimation). We note that a lower bound for the related \(\text{MMD}_{k}\) exists. However, the adversarial distribution pair \((\mathbb{P}_{\theta_{1}},\mathbb{P}_{\theta_{0}})\) constructed by Tolstikhin et al. (2016, Theorem 1) to obtain the lower bound on MMD estimation has a product structure which implies that \(|\mathrm{HSIC}_{k}(\mathbb{P}_{\theta_{1}})-\mathrm{HSIC}_{k}(\mathbb{P}_{ \theta_{0}})|=0\) and hence it is not applicable in our case of HSIC; Tolstikhin et al. (2016, Theorem 2) with radial kernels has the same restriction._
4. _Difference compared to Tolstikhin et al. (2017) (minimax mean embedding estimation). The estimation of the mean embedding \(\mu_{k}(\mathbb{P})\) is known to have a minimax rate of \(\mathcal{O}\left(n^{-1/2}\right)\). But, this rate does not imply an optimal lower bound for the estimation of MMD as is evident from the two works (Tolstikhin et al., 2016, 2017). The same conclusion holds for HSIC estimation._
5. _Difference compared to Zhou et al. (2019) (minimax covariance operator estimation). For the related problem of estimating the centered covariance operator_ \[C_{XX}=\int_{\mathbb{R}^{d}}\left(\phi_{k}(x)-\mu_{k}(\mathbb{P})\right) \otimes\left(\phi_{k}(x)-\mu_{k}(\mathbb{P})\right)\mathrm{d}\mathbb{P}(x) \in\mathcal{H}_{k}\otimes\mathcal{H}_{k},\]

Zhou et al. (2019, Theorem 5) give the lower bound

\[\inf_{\tilde{F}_{n}}\sup_{\mathbb{P}\in\mathcal{P}}\mathbb{P}^{n}\left\{\left\| C_{XX}-\tilde{F}_{n}\right\|_{\mathcal{H}_{k}\otimes\mathcal{H}_{k}}\geq \frac{c}{\sqrt{n}}\right\}\geq 1/8\]

_in the same setting as in Theorem 1(ii), where \(\tilde{F}_{n}\) is any estimator of the centered covariance \(C_{XX}\), and \(c\) is a positive constant. By noting that the centered covariance is the centered cross-covariance of a random variable with itself, Corollary 1 recovers their result._

The next section contains our proofs.

## 4 Proofs

This section is dedicated to our proofs. We present the proof of Lemma 1 in Section 4.1, that of Theorem 1 in Section 4.2, and that of Corollary 1 in Section 4.3.

### Proof of Lemma 1

As

\[\mathrm{HSIC}_{k}^{2}(\mathbb{P}) =\mathrm{MMD}_{k}^{2}(\mathbb{P},\mathbb{Q})=\left\|\mu_{k}( \mathbb{P})-\mu_{k}(\mathbb{Q})\right\|_{\mathcal{H}_{k}}^{2}\] \[=\langle\mu_{k}(\mathbb{P}),\mu_{k}(\mathbb{P})\rangle_{\mathcal{ H}_{k}}+\langle\mu_{k}(\mathbb{Q}),\mu_{k}(\mathbb{Q})\rangle_{\mathcal{H}_{k}}-2 \langle\mu_{k}(\mathbb{P}),\mu_{k}(\mathbb{Q})\rangle_{\mathcal{H}_{k}}\]

with \(\mathbb{Q}=\otimes_{m=1}^{M}\mathbb{P}_{m}=\mathcal{N}(\mathbf{m},\mathrm{bdiag }(\mathbf{\Sigma}_{1,1},\dots,\mathbf{\Sigma}_{M,M}))\), \(\mathbb{P}_{m}=\mathcal{N}(\mathbf{m}_{m},\mathbf{\Sigma}_{m,m})\), it is sufficient to be able to compute \(\langle\mu_{k}(\mathbb{P}),\mu_{k}(\mathbb{Q})\rangle_{\mathcal{H}_{k}}\)-type quantities with \(\mathbb{P}=\mathcal{N}(\mathbf{m}_{1},\mathbf{\Sigma}_{1})\) and \(\mathbb{Q}=\mathcal{N}(\mathbf{m}_{2},\mathbf{\Sigma}_{2})\). One can show (Muandet et al., 2011, Table 1) that \(\langle\mu_{k}(\mathbb{P}),\mu_{k}(\mathbb{Q})\rangle_{\mathcal{H}_{k}}=\frac{ e^{-\frac{1}{2}(\mathbf{m}_{1}-\mathbf{m}_{2})^{T}\left(\mathbf{x}_{1}+\mathbf{ z}_{2}+\gamma^{-1}\mathbf{1}_{d}\right)^{-1}(\mathbf{m}_{1}-\mathbf{m}_{2})}}{ |\gamma\mathbf{\Sigma}_{1}+\gamma\mathbf{\Sigma}_{2}+\mathbf{1}_{d}|^{\frac{1} {2}}}\). Using this fact and that \(\mathbf{m}=\mathbf{m}_{1}=\mathbf{m}_{2}\), the result follows.

### Proof of Theorem 1

The setup and the upper bound on \(\mathrm{KL}(\mathbb{P}_{\theta_{1}}^{n}||\mathbb{P}_{\theta_{0}}^{n})\) agree for (i) and (ii) but the methods that we use to lower bound \(|\mathrm{HSIC}_{k}(\mathbb{P}_{\theta_{1}})-\mathrm{HSIC}_{k}(\mathbb{P}_{ \theta_{0}})|\) differ. We structure the proof accordingly and present the overlapping part before we branch out into (i) and (ii). Both parts of the statement rely on Le Cam's method, which we state as Theorem B.5 for self-completeness.

To construct the adversarial pair, we consider a class \(\mathcal{G}\) of Gaussian distributions over \(\mathbb{R}^{d}\) such that every element \(\mathcal{N}\big{(}\boldsymbol{\mu},\boldsymbol{\Sigma}\big{)}\in\mathcal{G}\), with

\[\boldsymbol{\Sigma}=\boldsymbol{\Sigma}(i,j,\rho)=\begin{bmatrix}1&\cdots&0&0& \cdots&0\\ \vdots&\ddots&\vdots&\vdots&&\vdots\\ 0&\cdots&1&\rho&\cdots&0\\ 0&\cdots&\rho&1&\cdots&0\\ \vdots&&\vdots&\vdots&\ddots&\vdots\\ 0&\cdots&0&0&\cdots&1\end{bmatrix}\in\mathbb{R}^{d\times d},\] (5)

and (fixed) \(i=d_{1}\), \(j=d_{1}+1\), \(\rho\in(-1,1)\). In other words, \(\boldsymbol{\Sigma}\) is essentially the \(d\)-dimensional matrix \(\boldsymbol{I}_{d}\) except for the \((i,j)\) and \((j,i)\) entry; both entries are identical to \(\rho\), and they specify the correlation of the respective coordinates. This family of distributions is indexed by a tuple \((\boldsymbol{\mu},\rho)\in\mathbb{R}^{d}\times(-1,1)=:\mathcal{A}\) and, for \(a\in\mathcal{A}\), we write \(\mathbb{P}_{a}\) for the associated distribution. To bring ourselves into the setting of Theorem B.5, we fix \(n\in\mathbb{N}_{>0}\), choose \(\mathcal{X}=\left(\mathbb{R}^{d}\right)^{n}\), set \(\Theta=\{\theta_{a}:=\mathrm{HSIC}_{k}(\mathbb{P}_{a})\,:\,a\in\mathcal{A}\},\mathcal{P}_{\Theta}=\{\mathbb{P}_{a}^{n}\,:\,a\in\mathcal{A}\}=\{\mathbb{P}_ {a}^{n}\,:\,\theta_{a}\in\Theta\}\), and use the metric \((x,y)\mapsto|x-y|\) for \(x,y\in\mathbb{R}\). Hence, the data \(D\sim\mathbb{P}_{\theta}\in\mathcal{P}_{\Theta}\). For brevity, let \(F:\mathcal{A}\to\mathbb{R}\) stand for \(a\mapsto\mathrm{HSIC}_{k}(\mathbb{P}_{a})\), and let \(\hat{F}_{n}\) stand for the corresponding estimator based on \(n\) samples.

As \(\mathcal{G}\subseteq\mathcal{P}\), it holds for every positive \(s\) that

\[\sup_{\mathbb{P}\in\mathcal{P}}\mathbb{P}^{n}\left\{\left|\mathrm{HSIC}_{k} \left(\mathbb{P}\right)-\hat{F}_{n}\right|\geq s\right\}\geq\sup_{\mathbb{P} \in\mathcal{G}}\mathbb{P}^{n}\left\{\left|\mathrm{HSIC}_{k}\left(\mathbb{P} \right)-\hat{F}_{n}\right|\geq s\right\}.\]

Let \(\mathbb{P}_{\theta_{0}}=\mathcal{N}\left(\boldsymbol{\mu}_{0},\boldsymbol{ \Sigma}_{0}\right)\) and \(\mathbb{P}_{\theta_{1}}=\mathcal{N}\left(\boldsymbol{\mu}_{1},\boldsymbol{ \Sigma}_{1}\right)\) with

\[\boldsymbol{\mu}_{0} =\boldsymbol{0}_{d}\in\mathbb{R}^{d}, \boldsymbol{\Sigma}_{0} =\boldsymbol{\Sigma}(d_{1},d_{1}+1,0)=\mathbf{I}_{d}\in\mathbb{ R}^{d\times d},\] \[\boldsymbol{\mu}_{1} =\frac{1}{\sqrt{d}n}\mathbf{1}_{d}\in\mathbb{R}^{d}, \boldsymbol{\Sigma}_{1} =\boldsymbol{\Sigma}(d_{1},d_{1}+1,\rho_{n})\in\mathbb{R}^{d \times d},\]

where \(\rho_{n}\in(-1,1)\) will be chosen appropriately later.5 We now proceed to upper bound \(\mathrm{KL}\left(\mathbb{P}_{\theta_{1}}^{n}||\mathbb{P}_{\theta_{0}}^{n}\right)\) and lower bound \(|F(\theta_{1})-F(\theta_{0})|\).

Footnote 5: Notice the dependence of \(\mathbb{P}_{\theta_{1}}\) on \(n\).

Upper bound for KL divergenceLemma A.1 implies that with \(\rho_{n}^{2}=\frac{1}{n}\), one has the bound \(\mathrm{KL}\left(\mathbb{P}_{\theta_{1}}^{n}||\mathbb{P}_{\theta_{0}}^{n}\right) \leq\alpha:=\frac{5}{4}\) for \(n\geq 2=:n_{0}\).

Lower bound (i): Gaussian kernels.Recall that the considered kernel is \(k(\mathbf{x},\mathbf{y})=e^{-\frac{\gamma}{2}\left\|\mathbf{x}-\mathbf{y} \right\|_{\mathbb{R}^{d}}^{2}}\) (\(\gamma>0\)). The idea of the proof is as follows.

1. We express \(|F(\theta_{1})-F(\theta_{0})|\) in closed form as a function of \(\gamma\), \(\rho_{n}\), and \(d\).
2. Using the analytical form obtained in the 1st step, we construct the lower bound.

This is what we detail next.

* **Analytical form of \(|F(\theta_{1})-F(\theta_{0})|\)**: Using the fact that \(\mathrm{HSIC}_{k}(\mathbb{P}_{\theta_{0}})=0\), we have that \[\left|F\left(\theta_{1}\right)-\underbrace{F\left(\theta_{0} \right)}_{=0}\right|^{2}=F^{2}\left(\theta_{1}\right)=\mathrm{HSIC}_{k}^{2} \left(\mathbb{P}_{\theta_{1}}\right)=\mathrm{MMD}_{k}^{2}\left(\mathcal{N} \left(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1}\right),\mathcal{N}\left( \boldsymbol{\mu}_{1},\mathbf{I}_{d}\right)\right)\] \[=\underbrace{\left\langle\mu_{k}\left(\mathcal{N}\left( \boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1}\right)\right),\mu_{k}\left( \mathcal{N}\left(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1}\right)\right) \right\rangle_{\mathcal{H}_{k}}}_{(i)}+\underbrace{\left\langle\mu_{k}\left( \mathcal{N}\left(\boldsymbol{\mu}_{1},\mathbf{I}_{d}\right)\right),\mu_{k} \left(\mathcal{N}\left(\boldsymbol{\mu}_{1},\mathbf{I}_{d}\right)\right)\right\rangle _{\mathcal{H}_{k}}}_{(ii)}\] \[\quad-2\underbrace{\left\langle\mu_{k}\left(\mathcal{N}\left( \boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1}\right)\right),\mu_{k}\left( \mathcal{N}\left(\boldsymbol{\mu}_{1},\mathbf{I}_{d}\right)\right)\right\rangle _{\mathcal{H}_{k}}}_{(iii)},\]which we compute term-by-term with Lemma 1, and obtain

\[(i) =\left|2\gamma\mathbf{\Sigma}_{1}+\mathbf{I}_{d}\right|^{-1/2}=\left[ \left(2\gamma+1\right)^{d-2}\left(\left(2\gamma+1\right)^{2}-\left(2\gamma\rho_ {n}\right)^{2}\right)\right]^{-1/2},\] \[(ii) =\left|2\gamma\mathbf{I}_{d}+\mathbf{I}_{d}\right|^{-1/2}=\left[ \left(2\gamma+1\right)^{d}\right]^{-1/2},\] \[(iii) =\left|\gamma\mathbf{\Sigma}_{1}+\gamma\mathbf{I}_{d}+\mathbf{I }_{d}\right|^{-1/2}=\left[\left(2\gamma+1\right)^{d-2}\left(\left(2\gamma+1 \right)^{2}-\left(\gamma\rho_{n}\right)^{2}\right)\right]^{-1/2}.\]

Combining (i), (ii), and (iii) yields that

\[\mathrm{HSIC}_{k}^{2}\left(\mathbb{P}_{\theta_{1}}\right) =(i)+(ii)-2(iii)\] \[=\left[\left(2\gamma+1\right)^{d-2}\left(\left(2\gamma+1\right)^ {2}-\left(2\gamma\rho_{n}\right)^{2}\right)\right]^{-1/2}+\left[\left(2\gamma +1\right)^{d}\right]^{-1/2}\] \[\quad-2\left[\left(2\gamma+1\right)^{d-2}\left(\left(2\gamma+1 \right)^{2}-\left(\gamma\rho_{n}\right)^{2}\right)\right]^{-1/2}.\]
* **Lower bound on \(\left|F(\theta_{1})-F(\theta_{0})\right|\)**: Next, we show that there exists \(c>0\) such that for any \(n\in\mathbb{N}_{>0}\) it holds that \(\mathrm{HSIC}_{k}^{2}\left(\mathbb{P}_{\theta_{1}}\right)\geq\frac{c}{n}\). For \(0<x<\left(1+\frac{1}{2\gamma}\right)^{2}\), let us consider the function \[f_{c}(x) =\left[\left(2\gamma+1\right)^{d-2}\left(\left(2\gamma+1\right)^ {2}-4\gamma^{2}x\right)\right]^{-1/2}+\left[\left(2\gamma+1\right)^{d}\right] ^{-1/2}\] \[\quad-2\left[\left(2\gamma+1\right)^{d-2}\left(\left(2\gamma+1 \right)^{2}-\gamma^{2}x\right)\right]^{-1/2}-cx\] \[=\left[z^{d-2}\left(z^{2}-4\gamma^{2}x\right)\right]^{-1/2}+ \left(z^{d}\right)^{-1/2}-2\left[z^{d-2}\left(z^{2}-\gamma^{2}x\right)\right] ^{-1/2}-cx,\] with the shorthand \(z:=2\gamma+1\).6 With this notation, \(f_{c}(1/n)=\mathrm{HSIC}_{k}^{2}\left(\mathbb{P}_{\theta_{1}}\right)-c/n\); our aim is to determine \(c>0\) such that \(f_{c}(1/n)\geq 0\) for any positive integer \(n\). To achieve this goal, notice that \(f_{c}(0)=0\), and Footnote 6: Notice that \(\left(2\gamma+1\right)^{2}-\gamma^{2}x>\left(2\gamma+1\right)^{2}-4\gamma^{2}x\), and \(\left(2\gamma+1\right)^{2}-4\gamma^{2}x>0\Leftrightarrow x<\left(1+\frac{1}{2 \gamma}\right)^{2}\) for a positive \(x\); hence the imposed assumption on \(x\) ensures that the function \(f_{c}\) is well-defined. \[f_{c}^{\prime}(x) =\frac{2\gamma^{2}z^{d-2}}{\left[z^{d-2}\left(z^{2}-4x\gamma^{2} \right)\right]^{3/2}}-\frac{\gamma^{2}z^{d-2}}{\left[z^{d-2}\left(z^{2}-x \gamma^{2}\right)\right]^{3/2}}-c\] \[>\frac{2\gamma^{2}z^{d-2}}{\left[z^{d-2}\left(z^{2}-x\gamma^{2} \right)\right]^{3/2}}-c=\frac{\gamma^{2}z^{d-2}}{\left[z^{d-2}\left(z^{2}-x \gamma^{2}\right)\right]^{3/2}}-c=\frac{\gamma^{2}z^{d-2}}{\left[z^{d-2}\left( z^{2}-x\gamma^{2}\right)\right]^{3/2}}-c\] \[>\frac{\gamma^{2}z^{d-2}}{\left(z^{d-2}z^{2}\right)^{3/2}}-c= \frac{\gamma^{2}}{z^{2}\sqrt{z^{d}}}-c=\frac{\gamma^{2}}{\left(2\gamma+1 \right)^{2}\sqrt{\left(2\gamma+1\right)^{d}}}-c.\] Choosing now \(c=\frac{\gamma^{2}}{\left(2\gamma+1\right)^{2}\sqrt{\left(2\gamma+1\right)^{ d}}}>0\), we have \(f_{c}^{\prime}(x)\geq 0\), so \(f\) is a nondecreasing function. Note that \(f_{c}(1/n)=\mathrm{HSIC}_{k}^{2}\left(\mathbb{P}_{\theta_{1}}\right)-c/n\geq 0\), with \(x=1/n\) and \(\left(1+\frac{1}{2\gamma}\right)^{-2}<1\leq n<\infty\). By taking the positive square root, this means that \[\mathrm{HSIC}_{k}\left(\mathbb{P}_{\theta_{1}}\right)\geq\frac{\gamma}{\left(2 \gamma+1\right)\left(\left(2\gamma+1\right)^{d}\right)^{1/4}\sqrt{n}}=:2s\] holds for \(n\geq 1\), implying that \(\left|F(\theta_{1})-F(\theta_{0})\right|\geq 2s>0\). We conclude the proof by Theorem B.5 using that \(\alpha=\frac{5}{4}\) and \(\max\left(\frac{e^{-\frac{5}{4}}}{4},\frac{1-\sqrt{\frac{5}{8}}}{2}\right)= \frac{1-\sqrt{\frac{5}{8}}}{2}\).

Lower bound (ii): translation-invariant kernels.Let \(\Lambda_{k}\) denote the spectral measure associated to the kernel \(k\) according to (1). Using the fact that \(\mathrm{HSIC}_{k}(\mathbb{P}_{\theta_{0}})=0\), we have for \(\left|F(\theta_{1})-F(\theta_{0})\right|\) that

\[\left|F\left(\theta_{1}\right)-\underbrace{F\left(\theta_{0} \right)}_{=0}\right|^{2}=F^{2}\left(\theta_{1}\right)=\mathrm{HSIC}_{k}^{2} \left(\mathbb{P}_{\theta_{1}}\right)=\mathrm{MMD}_{k}^{2}\left(\mathcal{N} \left(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1}\right),\mathcal{N}\left( \boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{0}\right)\right)\] \[\stackrel{{(ii)}}{{=}}\left\|\psi_{\mathcal{N} \left(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1}\right)}-\psi_{\mathcal{N} \left(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{0}\right)}\right\|_{L^{2}( \mathbb{R}^{d},\Lambda_{k})}^{2}\] \[\stackrel{{(ii)}}{{=}}\int_{\mathbb{R}^{d}}\left|e ^{i\left(\boldsymbol{\mu}_{1},\boldsymbol{\omega}\right)-\frac{1}{2}\left( \boldsymbol{\omega},\boldsymbol{\Sigma}_{1}\boldsymbol{\omega}\right)}-e^{i \left(\boldsymbol{\mu}_{1},\boldsymbol{\omega}\right)-\frac{1}{2}\left( \boldsymbol{\omega},\boldsymbol{\Sigma}_{0}\boldsymbol{\omega}\right)} \right|^{2}\mathrm{d}\Lambda_{k}(\boldsymbol{\omega})\] \[\stackrel{{(iii)}}{{\geq}}\int_{A}\left|e^{-\frac{1}{2} \left(\boldsymbol{\omega},\boldsymbol{\Sigma}_{1}\boldsymbol{\omega}\right)}-e ^{-\frac{1}{2}\left(\boldsymbol{\omega},\boldsymbol{\Sigma}_{0}\boldsymbol{ \omega}\right)}\right|^{2}\mathrm{d}\Lambda_{k}(\boldsymbol{\omega})\stackrel{{ (iv)}}{{\geq}}\rho_{n}\underbrace{\int_{A}\left[h_{\boldsymbol{ \omega}}^{\prime}(0)\right]^{2}\mathrm{d}\Lambda_{k}(\boldsymbol{\omega})} \stackrel{{(v)}}{{=:(2s)^{2}>0}},\]

where \((i)\) holds by Sriperumbudur et al. (2010, Corollary 4(i)) (recalled in Theorem B.2). \((ii)\) follows from the analytical form \(\psi_{\mathcal{N}\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right)}( \mathbf{t})=e^{i\left(\boldsymbol{\mu},\boldsymbol{t}\right)-\frac{1}{2}\left( \boldsymbol{\upsilon},\boldsymbol{\Sigma}\boldsymbol{\mathrm{t}}\right)}\) of the characteristic function of a multivariate normal distribution \(\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})\). For \((iii)\), we define the non-empty open set

\[A=\left\{\boldsymbol{\omega}=(\omega_{1},\ldots,\omega_{d})^{\mathsf{T}}\in \mathbb{R}^{d}\,:\,\omega_{d_{1}}\omega_{d_{1}+1}<0\right\}\subset\mathbb{R}^ {d},\]

and use that the integration of a non-negative function over a subset yields a lower bound. In \((iv)\), fix \(\boldsymbol{\omega}\in A\) and let

\[h_{\boldsymbol{\omega}}:\rho\in[0,1]\mapsto e^{-\frac{1}{2}\left(\boldsymbol{ \omega},\boldsymbol{\Sigma}(d_{1},d_{1}+1,\rho)\boldsymbol{\omega}\right)}\in( 0,1].\]

Note that \(h_{\boldsymbol{\omega}}(\rho)=e^{-\frac{1}{2}\left(\boldsymbol{\omega}^{ \mathsf{T}}\boldsymbol{\omega}+2\rho\omega_{d_{1}}\omega_{d_{1}+1}\right)}\); \(h_{\boldsymbol{\omega}}\) is continuous on \([0,1]\) and differentiable on \((0,1)\). Hence for any \(\rho\in(0,1)\), by the mean value theorem, there exists \(\tilde{\rho}\in(0,1)\) such that

\[h_{\boldsymbol{\omega}}(\rho)-h_{\boldsymbol{\omega}}(0)=\rho h_{\boldsymbol{ \omega}}^{\prime}(\tilde{\rho})\geq\rho\min_{c\in[0,1]}h_{\boldsymbol{\omega} }^{\prime}(c).\]

We have the first and second derivatives

\[h_{\boldsymbol{\omega}}^{\prime}(c)=-\omega_{d_{1}}\omega_{d_{1}+1}e^{-\frac{ 1}{2}\left(\boldsymbol{\omega}^{\mathsf{T}}\boldsymbol{\omega}+2c\omega_{d_{1} }\omega_{d_{1}+1}\right)},\quad h_{\boldsymbol{\omega}}^{\prime\prime}(c)= \omega_{d_{1}}^{2}\omega_{d_{1}+1}^{2}e^{-\frac{1}{2}\left(\boldsymbol{\omega} ^{\mathsf{T}}\boldsymbol{\omega}+2c\omega_{d_{1}}\omega_{d_{1}+1}\right)}>0,\]

which implies that \(c\mapsto h_{\boldsymbol{\omega}}^{\prime}(c)\) is a strictly increasing function of \(c\) and that it attains its minimum at \(c=0\), that is,

\[h_{\boldsymbol{\omega}}(\rho)-h_{\boldsymbol{\omega}}(0)\geq\rho h_{ \boldsymbol{\omega}}^{\prime}(0)>0,\]

where the 2nd inequality holds by \(\rho>0\) and \(\boldsymbol{\omega}\in A\). This shows that

\[\left[h_{\boldsymbol{\omega}}(\rho)-h_{\boldsymbol{\omega}}(0)\right]^{2}\geq \left[\rho h_{\boldsymbol{\omega}}^{\prime}(0)\right]^{2},\]

and the monotonicity of integration gives \((iv)\). For \((v)\), we note that the kernel \(k=\otimes_{n=1}^{M}k_{m}\) is characteristic (Szabo and Sriperumbudur, 2018, Theorem 4) (recalled in Theorem B.4) as the \((k_{m})_{m=1}^{M}\)-s are characteristic. Thus, \(\mathrm{supp}\left(\Lambda_{k}\right)=\mathbb{R}^{d}\) (see Sriperumbudur et al. (2010, Theorem 9); recalled in Theorem B.3), implying that \(\Lambda_{k}(A)>0\). \((v)\) follows from the positivity of \(h_{\boldsymbol{\omega}}^{\prime}(0)\) (for any \(\boldsymbol{\omega}\in A\)), from the fact that the integral of a positive function on a set with positive measure is positive, and from our choice of \(\rho_{n}=n^{-1/2}\).

Now, by taking the positive square root, we have

\[\left|F\left(\theta_{1}\right)-F\left(\theta_{0}\right)\right|\geq\frac{2c}{ \sqrt{n}}=:2s.\] (11)

We conclude by the application of Theorem B.5 using that \(\alpha=\frac{5}{4}\) and \(\max\left(\frac{e^{-\frac{5}{4}}}{4},\frac{1-\sqrt{\frac{5}{4}}}{2}\right)= \frac{1-\sqrt{\frac{5}{4}}}{2}\).

### Proof of Corollary 1

We use the same argument as in the beginning of the proof of Theorem 1 in Section 4.2 but adjust the setting in which we apply Theorem B.5. Specifically, we now let \(\Theta=\{\theta_{a}:=C_{X_{a}}\,:\,X_{a}\sim\mathbb{P}_{a},\,a\in\mathcal{A}\}\) with \(C_{X}\) defined as in (2) be the set of covariance operators, use the metric \((x,y)\mapsto\left\|x-y\right\|_{\mathcal{H}_{k}}\) for \(x,y\in\mathcal{H}_{k}\), and keep the remaining part of the setup the same. Hence, it remains to lower bound \(\left\|C_{X_{\theta_{1}}}-C_{X_{\theta_{0}}}\right\|_{\mathcal{H}_{k}}\). By using that HSIC is the RKHS norm of the cross-covariance operator, we obtain that

\[\left\|C_{X_{\theta_{1}}}-C_{X_{\theta_{0}}}\right\|_{\mathcal{H}_{k}}\overset{ (i)}{\geq}\left|\begin{array}{c}\left\|C_{X_{\theta_{1}}}\right\|_{\mathcal{ H}_{k}}-\underbrace{\left\|C_{X_{\theta_{0}}}\right\|_{\mathcal{H}_{k}}}_{= \mathrm{HSIC}_{k}\left(\mathbb{P}_{\theta_{1}}\right)}\Big{|}=\left|F(\theta _{1})-F(\theta_{0})\right|\overset{(ii)}{\geq}2s=\frac{2c}{\sqrt{n}},\]

where \((i)\) holds by the reverse triangle inequality, \(F\) is defined as in Section 4.2, and \((ii)\) is guaranteed by (11) for \(c>0\). We conclude as in the proof of Theorem 1(ii) to obtain the stated result.

## Acknowledgments and Disclosure of Funding

This work was supported by the German Research Foundation (DFG) Research Training Group GRK 2153: Energy Status Data -- Informatics Methods for its Collection, Analysis and Exploitation, and by the pilot program Core-Informatics of the Helmholtz Association (HGF).

## References

* Albert et al. (2022) Melisande Albert, Beatrice Laurent, Amandine Marrel, and Anouar Meynaoui. Adaptive test of independence based on HSIC measures. _The Annals of Statistics_, 50(2):858-879, 2022.
* Anaya-Isaza and Mera-Jimenez (2022) Andres Anaya-Isaza and Leonel Mera-Jimenez. Data augmentation and transfer learning for brain tumor detection in magnetic resonance imaging. _IEEE Access_, 10:23217-23233, 2022.
* Aronszajn (1950) Nachman Aronszajn. Theory of reproducing kernels. _Transactions of the American Mathematical Society_, 68:337-404, 1950.
* Berlinet and Thomas-Agnan (2004) Alain Berlinet and Christine Thomas-Agnan. _Reproducing Kernel Hilbert Spaces in Probability and Statistics_. Kluwer, 2004.
* Bilodeau and Nangue (2017) Martin Bilodeau and Aurelien Guetsop Nangue. Tests of mutual or serial independence of random vectors with applications. _Journal of Machine Learning Research_, 18:1-40, 2017.
* Bouche et al. (2023) Dimitri Bouche, Remi Flamary, Florence d'Alche Buc, Riwal Plougonven, Marianne Clausel, Jordi Badosa, and Philippe Drobinski. Wind power predictions from nowcasts to 4-hour forecasts: a learning approach with variable selection. _Renewable Energy_, 211:938-947, 2023.
* Camps-Valls et al. (2010) Gustavo Camps-Valls, Joris M. Mooij, and Bernhard Scholkopf. Remote sensing feature selection by kernel dependence measures. _IEEE Geoscience and Remote Sensing Letters_, 7(3):587-591, 2010.
* Carmeli et al. (2010) Claudio Carmeli, Ernesto De Vito, Alessandro Toigo, and Veronica Umanita. Vector valued reproducing kernel Hilbert spaces and universality. _Analysis and Applications_, 8:19-61, 2010.
* Chakraborty and Zhang (2019) Shubhadeep Chakraborty and Xianyang Zhang. Distance metrics for measuring joint dependence with application to causal inference. _Journal of the American Statistical Association_, 114(528):1638-1650, 2019.
* Climente-Gonzalez et al. (2019) Hector Climente-Gonzalez, Chloe-Agathe Azencott, Samuel Kaski, and Makoto Yamada. Block HSIC Lasso: model-free biomarker detection for ultra-high dimensional data. _Bioinformatics_, 35(14):i427-i435, 2019.
* Cohn (2013) Donald L. Cohn. _Measure Theory_. Birkhauser/Springer, second edition, 2013.
* Diestel and Uhl (1977) Joseph Diestel and John Jerry Uhl. _Vector Measures_. American Mathematical Society. Providence, 1977.
* Diestel et al. (2014)John Duchi. Derivations for linear algebra and optimization. _Berkeley, California_, 3(1):2325-5870, 2007.
* Fedorov et al. (2024) Alex Fedorov, Eloy Geenjaar, Lei Wu, Tristan Sylvain, Thomas P DeRamus, Margaux Luck, Maria Misiura, Girish Mittapalle, R Devon Hjelm, Sergey M Plis, et al. Self-supervised multimodal learning for group inferences from MRI data: Discovering disorder-relevant brain regions and multimodal links. _NeuroImage_, 285:120485, 2024.
* Fellmann et al. (2024) Noe Fellmann, Christophette Blanchet-Scalliet, Celine Helbert, Adrien Spagnol, and Delphine Sinoquet. Kernel-based sensitivity analysis for (excursion) sets. _Technometrics_, 2024.
* Gustavo et al. (2023) Michael Freitas Gustavo, Matti Hellstrom, and Toon Verstraelen. Sensitivity analysis for ReaxFF reparametrization using the Hilbert-Schmidt independence criterion. _Journal of Chemical Theory and Computation_, 19(9):2557-2573, 2023.
* Fukumizu et al. (2008) Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Scholkopf. Kernel measures of conditional dependence. In _Advances in Neural Information Processing Systems (NIPS)_, pages 498-496, 2008.
* Gorecki et al. (2018) Tomasz Gorecki, Miroslaw Krzysko, and Waldemar Wolynski. Independence test and canonical correlation analysis based on the alignment between kernel matrices for multivariate functional data. _Artificial Intelligence Review_, pages 1-25, 2018.
* Gretton et al. (2005) Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In _Algorithmic Learning Theory (ALT)_, pages 63-78, 2005.
* Gretton et al. (2008) Arthur Gretton, Kenji Fukumizu, Choon Hui Teo, Le Song, Bernhard Scholkopf, and Alexander Smola. A kernel statistical test of independence. In _Advances in Neural Information Processing Systems (NIPS)_, pages 585-592, 2008.
* Gretton et al. (2012) Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _Journal of Machine Learning Research_, 13(25):723-773, 2012.
* Herrando-Perez and Saltre (2024) Salvador Herrando-Perez and Frederik Saltre. Estimating extinction time using radiocarbon dates. _Quaternary Geochronology_, 79:101489, 2024.
* Kalinke and Szabo (2023) Florian Kalinke and Zoltan Szabo. Nystrom M-Hilbert-Schmidt independence criterion. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, pages 1005-1015, 2023.
* Le Cam (1973) Lucien Le Cam. Convergence of estimates under dimensionality restrictions. _The Annals of Statistics_, 1:38-53, 1973.
* Lyons (2013) Russell Lyons. Distance covariance in metric spaces. _The Annals of Probability_, 41:3284-3305, 2013.
* Micchelli et al. (2006) Charles Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. _Journal of Machine Learning Research_, 7:2651-2667, 2006.
* Mooij et al. (2016) Joris Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Scholkopf. Distinguishing cause from effect using observational data: Methods and benchmarks. _Journal of Machine Learning Research_, 17:1-102, 2016.
* Muandet et al. (2011) Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Scholkopf. Learning from distributions via support measure machines. In _Advances in Neural Information Processing Systems (NIPS)_, pages 10-18, 2011.
* Muller (1997) Alfred Muller. Integral probability metrics and their generating classes of functions. _Advances in Applied Probability_, 29:429-443, 1997.
* Pfister et al. (2018) Niklas Pfister, Peter Buhlmann, Bernhard Scholkopf, and Jonas Peters. Kernel-based tests for joint independence. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 80(1):5-31, 2018.
* Pfister et al. (2019)* Podkopaev et al. (2023) Aleksandr Podkopaev, Patrick Blobaum, Shiva Kasiviswanathan, and Aaditya Ramdas. Sequential kernelized independence testing. In _International Conference on Machine Learning (ICML)_, pages 27957-27993, 2023.
* Quadrianto et al. (2009) Novi Quadrianto, Le Song, and Alex Smola. Kernelized sorting. In _Advances in Neural Information Processing Systems (NIPS)_, pages 1289-1296, 2009.
* Saitoh and Sawano (2016) Saburou Saitoh and Yoshihiro Sawano. _Theory of Reproducing Kernels and Applications_. Springer Singapore, 2016.
* Scholkopf et al. (2021) Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* Sejdinovic et al. (2013a) Dino Sejdinovic, Arthur Gretton, and Wicher Bergsma. A kernel test for three-variable interactions. In _Advances in Neural Information Processing Systems (NIPS)_, pages 1124-1132, 2013a.
* Sejdinovic et al. (2013b) Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-based and RKHS-based statistics in hypothesis testing. _Annals of Statistics_, 41:2263-2291, 2013b.
* Shekhar et al. (2023) Shubhanshu Shekhar, Ilmun Kim, and Aaditya Ramdas. A permutation-free kernel independence test. _Journal of Machine Learning Research_, 24(369):1-68, 2023.
* Sheng and Sriperumbudur (2023) Tianhong Sheng and Bharath K. Sriperumbudur. On distance and kernel measures of conditional independence. _Journal of Machine Learning Research_, 24(7):1-16, 2023.
* Smola et al. (2007) Alexander Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A Hilbert space embedding for distributions. In _Algorithmic Learning Theory (ALT)_, pages 13-31, 2007.
* Song et al. (2007) Le Song, Alexander J. Smola, Arthur Gretton, and Karsten M. Borgwardt. A dependence maximization view of clustering. In _International Conference on Machine Learning (ICML)_, pages 815-822, 2007.
* Song et al. (2012) Le Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection via dependence maximization. _Journal of Machine Learning Research_, 13(1):1393-1434, 2012.
* Sriperumbudur et al. (2010) Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Scholkopf, and Gert Lanckriet. Hilbert space embeddings and metrics on probability measures. _Journal of Machine Learning Research_, 11:1517-1561, 2010.
* Sriperumbudur et al. (2011) Bharath Sriperumbudur, Kenji Fukumizu, and Gert Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. _Journal of Machine Learning Research_, 12:2389-2410, 2011.
* Steinwart (2001) Ingo Steinwart. On the influence of the kernel on the consistency of support vector machines. _Journal of Machine Learning Research_, 6(3):67-93, 2001.
* Steinwart and Christmann (2008) Ingo Steinwart and Andreas Christmann. _Support Vector Machines_. Springer, 2008.
* Stenger et al. (2020) Jerome Stenger, Fabrice Gamboa, Merlin Keller, and Bertrand Iooss. Optimal uncertainty quantification of a risk measurement from a thermal-hydraulic code using canonical moments. _International Journal for Uncertainty Quantification_, 10(1), 2020.
* Szabo and Sriperumbudur (2018) Zoltan Szabo and Bharath K. Sriperumbudur. Characteristic and universal tensor product kernels. _Journal of Machine Learning Research_, 18(233):1-29, 2018.
* Szekely and Rizzo (2009) Gabor J. Szekely and Maria L. Rizzo. Brownian distance covariance. _The Annals of Applied Statistics_, 3:1236-1265, 2009.
* Szekely et al. (2007) Gabor J. Szekely, Maria L. Rizzo, and Nail K. Bakirov. Measuring and testing dependence by correlation of distances. _The Annals of Statistics_, 35:2769-2794, 2007.
* Tolstikhin et al. (2016) Ilya Tolstikhin, Bharath Sriperumbudur, and Bernhard Scholkopf. Minimax estimation of maximal mean discrepancy with radial kernels. In _Advances in Neural Information Processing Systems (NIPS)_, pages 1930-1938, 2016.
* Tolstikhin et al. (2017)Ilya Tolstikhin, Bharath Sriperumbudur, and Krikamol Muandet. Minimax estimation of kernel mean embeddings. _Journal of Machine Learning Research_, 18:1-47, 2017.
* Tsybakov [2009] Alexandre B. Tsybakov. _Introduction to Nonparametric Estimation_. Springer, 2009.
* De Veiga [2015] Sebastien De Veiga. Global sensitivity analysis with dependence measures. _Journal of Statistical Computation and Simulation_, 85(7):1283-1305, 2015.
* Wang et al. [2022] Andi Wang, Juan Du, Xi Zhang, and Jianjun Shi. Ranking features to promote diversity: An approach based on sparse distance correlation. _Technometrics_, 64(3):384-395, 2022.
* Wehbe and Ramdas [2015] Leila Wehbe and Aaditya Ramdas. Nonparametric independence testing for small sample sizes. In _International Joint Conference on Artificial Intelligence (IJCAI)_, pages 3777-3783, 2015.
* Wendland [2005] Holger Wendland. _Scattered data approximation_. Cambridge University Press, 2005.
* Yamada et al. [2014] Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, and Masashi Sugiyama. High-dimensional feature selection by feature-wise kernelized Lasso. _Neural Computation_, 26(1):185-207, 2014.
* Zhou et al. [2019] Yang Zhou, Di-Rong Chen, and Wei Huang. A class of optimal estimators for the covariance operator in reproducing kernel Hilbert spaces. _Journal of Multivariate Analysis_, 169:166-178, 2019.
* Zolotarev [1983] V. Zolotarev. Probability metrics. _Theory of Probability and its Applications_, 28:278-302, 1983.

Auxiliary Result

In this section, we collect an auxiliary result. Lemma A.1 presents an upper bound on the Kullback-Leibler divergence between multivariate normal distributions.

**Lemma A.1** (Upper bound on \(\operatorname{KL}\) divergence).: _Let \(d=\sum_{m=1}^{M}d_{m}\), with \(d_{m}\in\mathbb{N}_{>0}\) (\(m\in[M]\)). Fix \(i\in[d_{1}]\). Let \(j=i+1\), \(\mathbb{P}_{\theta_{0}}=\mathcal{N}(\mathbf{0}_{d},\mathbf{I}_{d})\), and \(\mathbb{P}_{\theta_{1}}=\mathcal{N}(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_ {1})\), with \(\boldsymbol{\mu}_{1}=\frac{1}{\sqrt{d}n}\mathbf{1}_{d}\in\mathbb{R}^{d}\), and \(\boldsymbol{\Sigma}_{1}=\boldsymbol{\Sigma}(i,j,\rho_{n})\in\mathbb{R}^{d\times d}\) defined as in (5) (\(\rho_{n}\in(0,1)\)). Then, for \(2\leq n\in\mathbb{N}\),_

\[\operatorname{KL}(\mathbb{P}_{\theta_{1}}^{n}||\mathbb{P}_{\theta_{0}}^{n}) \leq\frac{1}{2n}+\frac{n}{2}\frac{\rho_{n}^{2}}{1-\rho_{n}^{2}}.\]

_In particular, for \(\rho_{n}^{2}=1/n\), it holds that \(\operatorname{KL}(\mathbb{P}_{\theta_{1}}^{n}||\mathbb{P}_{\theta_{0}}^{n}) \leq\frac{5}{4}\)._

Proof.: With \(\boldsymbol{\mu}_{0}=\mathbf{0}_{d}\) and \(\boldsymbol{\Sigma}_{0}=\mathbf{I}_{d}\), we obtain that

\[\operatorname{KL}(\mathbb{P}_{\theta_{1}}^{n}||\mathbb{P}_{\theta _{0}}^{n}) \stackrel{{(a)}}{{=}}\sum_{i\in[n]}\operatorname{KL}( \mathbb{P}_{\theta_{1}}||\mathbb{P}_{\theta_{0}})\] \[\stackrel{{(b)}}{{=}}\frac{n}{2}\left[\operatorname {tr}(\boldsymbol{\Sigma}_{0}^{-1}\boldsymbol{\Sigma}_{1})+(\boldsymbol{\mu}_ {0}-\boldsymbol{\mu}_{1})^{\mathsf{T}}\boldsymbol{\Sigma}_{0}^{-1}( \boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1})-d+\ln\left(\frac{|\boldsymbol{ \Sigma}_{0}|}{|\boldsymbol{\Sigma}_{1}|}\right)\right]\] \[=\frac{n}{2}\left[\underbrace{\operatorname{tr}(\boldsymbol{ \Sigma}_{1})}_{=d}+\underbrace{\|\boldsymbol{\mu}_{1}\|_{\mathbb{R}^{d}}^{2}}_{= \frac{1}{n^{2}}}-d+\ln\left(\begin{array}{c}1\\ \frac{|\boldsymbol{\Sigma}_{1}|}{1-\rho_{n}^{2}}\end{array}\right)\right]\] \[=\frac{1}{2n}+\frac{n}{2}\ln\left(\frac{1}{1-\rho_{n}^{2}}\right) \stackrel{{(d)}}{{\leq}}\frac{1}{2n}+\frac{n}{2}\frac{\rho_{n}^{2} }{1-\rho_{n}^{2}}\stackrel{{(e)}}{{\leq}}\frac{5}{4},\]

where (a) is implied by Lemma B.1, (b) follows from Lemma B.2, (c) follows from the definition of the determinant, (d) is the consequence of the inequality \(\ln(x)\leq x-1\) holding for \(x>0\), and (e) holds for \(n\geq 2\) and \(\rho_{n}^{2}=1/n\) as

\[\frac{n}{2}\underbrace{\frac{1/n}{1-1/n}}_{\frac{1}{n-1}}\leq 1\iff \frac{n}{2}\frac{1}{n-1}\leq 1\iff n\leq 2(n-1)\iff n\geq 2,\]

and in this case (for \(n\geq 2\)) one has that \(\frac{1}{2n}\leq\frac{1}{4}\). 

## Appendix B External Theorems

For self-completeness, we include the external statements that we use. The well-known result by Bochner, stated in Theorem B.1, completely characterizes continuous bounded translation-invariant kernels. Theorem B.2 allows expressing \(\operatorname{MMD}\) with continuous bounded translation-invariant kernels in terms of characteristic functions, and Theorem B.3 gives an equivalent condition for a continuous bounded translation-invariant kernel to be characteristic. Theorem B.4 connects characteristic kernels to characteristic product kernels and to \(\mathcal{I}\)-characteristic product kernels on \(\mathbb{R}^{d}\) (we include only the part relevant to our paper for brevity). We recall Le Cam's method in Theorem B.5 and collect results on the Kullback-Leibler divergence in Lemma B.1 and Lemma B.2.

**Theorem B.1** (Bochner; Theorem 6.6; Wendland [2005]).: _A continuous function \(\kappa:\mathbb{R}^{d}\to\mathbb{R}\) is positive definite if and only if it is the Fourier transform of a finite nonnegative Borel measure \(\Lambda\) on \(\mathbb{R}^{d}\), that is,_

\[\kappa(\mathbf{x})=\int_{\mathbb{R}^{d}}e^{-i(\mathbf{x},\boldsymbol{\omega})} \operatorname{d}\Lambda(\boldsymbol{\omega}),\quad\text{for all }\mathbf{x}\in\mathbb{R}^{d}.\]

**Theorem B.2** (Corollary 4(i); Sriperumbudur et al. [2010]).: _Let \(k:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) be a continuous bounded translation-invariant kernel. Then, for any \(\mathbb{P},\mathbb{Q}\in\mathcal{M}_{1}^{+}\left(\mathbb{R}^{d}\right)\),_

\[\operatorname{MMD}_{k}^{2}(\mathbb{P},\mathbb{Q})=\|\psi_{\mathbb{P}}-\psi_{ \mathbb{Q}}\|_{L^{2}(\mathbb{R}^{d},\Lambda_{k})}^{2}\,,\]

_with \(\psi_{\mathbb{P}}\) and \(\psi_{\mathbb{Q}}\) being the characteristic functions of \(\mathbb{P}\) and \(\mathbb{Q}\), respectively, and \(\Lambda_{k}\) defined in (1)._

**Theorem B.3** (Theorem 9; Sriperumbudur et al. (2010)).: _Suppose \(k:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) is a continuous bounded translation-invariant kernel. Then \(k\) is characteristic if and only if \(\operatorname{supp}(\Lambda_{k})=\mathbb{R}^{d}\), with \(\Lambda_{k}\) defined as in (1)._

**Theorem B.4** (Theorem 4; Szabo and Sriperumbudur (2018)).: _Suppose \(k_{m}:\mathbb{R}^{d_{m}}\times\mathbb{R}^{d_{m}}\to\mathbb{R}\) is continuous bounded and translation-invariant kernel for all \(m\in[M]\). Then the following statements are equivalent:_

1. \((k_{m})_{m=1}^{M}\)_-s are characteristic;_
2. \(\otimes_{m=1}^{M}k_{m}\) _is characteristic;_
3. \(\otimes_{m=1}^{M}k_{m}\) _is_ \(\mathcal{I}\)_-characteristic._

The next statement follows directly from Tsybakov (2009, Eq. (2.9)) and Tsybakov (2009, Theorem 2.2).

**Theorem B.5** (Theorem 2.2; Tsybakov (2009)).: _Let \(\mathcal{X}\) be a measurable space, \((\Theta,d)\) is a semi-metric space, and \(\mathcal{P}_{\Theta}=\{\mathbb{P}_{\theta}:\theta\in\Theta\}\) is a class of probability measures on \(\mathcal{X}\) indexed by \(\Theta\). We observe data \(D\sim\mathbb{P}_{\theta}\in\mathcal{P}_{\Theta}\) with some unknown parameter \(\theta\). The goal is to estimate \(\theta\). Let \(\hat{\theta}=\hat{\theta}(D)\) be an estimator of \(\theta\) based on \(D\). Assume that there exist \(\theta_{0},\theta_{1}\in\Theta\) such that \(d(\theta_{0},\theta_{1})\geq 2s>0\) and \(\operatorname{KL}(\mathbb{P}_{\theta_{1}}||\mathbb{P}_{\theta_{0}})\leq\alpha<\infty\) for \(\alpha>0\). Then_

\[\inf_{\hat{\theta}}\sup_{\theta\in\Theta}\mathbb{P}_{\theta}\left(d\left( \hat{\theta},\theta\right)\geq s\right)\geq\max\left(\frac{e^{-\alpha}}{4}, \frac{1-\sqrt{\alpha/2}}{2}\right).\]

We have the following property of the Kullback-Leibler divergence for product measures (Tsybakov, 2009, p. 85).

**Lemma B.1** (KL divergence of product measures).: _Let \(\mathbb{P}=\otimes_{i=1}^{n}\mathbb{P}_{i}\) and \(\mathbb{Q}=\otimes_{i=1}^{n}\mathbb{Q}_{i}\). Then_

\[\operatorname{KL}(\mathbb{P}||\mathbb{Q})=\sum_{i\in[n]}\operatorname{KL}( \mathbb{P}_{i}||\mathbb{Q}_{i}).\]

The following lemma (Duchi, 2007, p. 13) shows that the Kullback-Leibler divergence of multivariate Gaussians can be computed in closed form.

**Lemma B.2** (KL divergence of Gaussians).: _The KL divergence of two normal distributions \(\mathcal{N}(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1})\) and \(\mathcal{N}(\boldsymbol{\mu}_{0},\boldsymbol{\Sigma}_{0})\) on \(\mathbb{R}^{d}\) is_

\[\operatorname{KL}(\mathcal{N}(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{1})|| \mathcal{N}(\boldsymbol{\mu}_{0},\boldsymbol{\Sigma}_{0}))=\frac{\operatorname {tr}(\boldsymbol{\Sigma}_{0}^{-1}\boldsymbol{\Sigma}_{1})+(\boldsymbol{\mu}_{ 0}-\boldsymbol{\mu}_{1})^{\mathsf{T}}\boldsymbol{\Sigma}_{0}^{-1}(\boldsymbol {\mu}_{0}-\boldsymbol{\mu}_{1})-d+\ln\left(\frac{|\boldsymbol{\Sigma}_{0}|}{| \boldsymbol{\Sigma}_{1}|}\right)}{2}.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction motivate and summarize the theoretical results that we establish in the article. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We make the assumptions explicit in all of our results. Remark 1(a) further elaborates the characteristic assumption. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide explicit proofs to all of our claims. External results that we use are referenced and additionally included as part of the supplement to ensure self-completeness. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not contain experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research presented in this work conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work presents basic research without any tangible societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals(or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.