[MISSING_PAGE_FAIL:1]

2022, Pan et al., 2023, Wan et al., 2024) and conflicts in embedded knowledge (Elazar et al., 2021, Lu et al., 2024). Retrieved conflicts arise during the inference stage when newly retrieved information contradicts the model's parametric memory, while embedded conflicts occur during the training stage due to discrepancies within the training text itself. To construct conflict-related datasets for controlled experiments, previous works primarily utilize word-level substitution (Longre et al., 2021, Zhou et al., 2023, Wang et al., 2023) or language model generation methods (Tan et al., 2024) to craft conflicts. Xie et al. (2024) combines these two approaches, eliciting parametric memory from LLMs and constructs more coherent and convincing conflict pairs. However, all these existing studies solely explored the conflicts between the embedded knowledge of LLMs and the retrieved contextual knowledge, leaving other conflict scenarios like the conflict within the models' encoded knowledge and the interplay between different conflict forms under-explored.

To fill in the existing research gap, we construct the ConflictBank, the first comprehensive benchmark for analyzing the models' behavior by simulating the knowledge conflicts encountered in the pre-training and inference stages. In the ConflictBank, we meticulously design three main conflict scenarios, i.e., inaccurate information (misinformation conflict) (Du et al., 2022, Pan et al., 2023, Zhou et al., 2024), knowledge changes over time (temporal conflict) (Lazaridou et al., 2021, Su et al., 2022), and the polysemic nature of language (semantic conflict) (Ansell et al., 2021, Sevgili et al., 2022). Specifically, we collect **2,863,205** claims from Wikidata and generate the evidence with the revised conflict claims to create a total of **7,453,853** claim-evidence pairs. Additionally, we construct **553,117** QA pairs for investigating the model behavior when facing conflicts. Unlike the previous datasets, ConflictBank can be employed to systematically evaluate the effects of knowledge conflict in retrieved knowledge, embedded knowledge, and their interactions. Based on the ConflictBank, we conduct pilot experiments on twelve LLMs across four model series and provide insights into their behaviors under different conflict scenarios. Our main contributions are summarized below:

* We present ConflictBank, the first comprehensive benchmark for knowledge conflicts, including 7M claim-evidence pairs and 553k QA pairs. Our benchmark covers three conflict causes in the real-world scenario, including misinformation, temporal, and semantic conflicts.
* ConflictBank can be utilized to conduct a series of experiments about knowledge conflicts, including conflicts in retrieved knowledge, embedded knowledge, and their interplay.
* We conduct in-depth pilot experiments on twelve LLMs across four model series and provide comprehensive analyses about model scales, conflict causes, and conflict types.
* To make the ConflictBank accessible for future research, we release a Python package that automates data loading, baseline evaluation, and training. Additionally, we have open-sourced all the models used in our analysis. We hope that ConflictBank could facilitate comprehensive studies on different conflict scenarios and contribute to the advancement of more reliable and trustworthy language models.

## 2 ConflictBank Benchmark

### Knowledge Conflicts Causes

Knowledge conflict within datasets can significantly diminish a model's accuracy, reliability, and trustworthiness (Longpre et al., 2021, Wang et al., 2023, Xie et al., 2023, Xu et al., 2024). In this paper, we identify and investigate three prevalent knowledge conflict causes:

* **Type 1: Misinformation Conflict** emerges from the presence of incorrect or misleading information in datasets, resulting in considerable confusion and misinterpretation (Schuster et al., 2021). This conflict usually occurs during data collection, introducing false narratives or misleading facts into the model and diminishing its factual accuracy.
* **Type 2: Temporal Conflict** occurs when knowledge changes or evolves over time (Lazaridou et al., 2021, Su et al., 2022, Huang et al., 2024, 2024). As new knowledge emerges, previous knowledge becomes outdated or obsolete, leading to inconsistencies regarding the same entity.
* **Type 3: Semantic Conflict** arises when words with multiple meanings cause ambiguity in interpretation (Sevgili et al., 2022). These conflicts stem from the polysemic nature of language, leading to misunderstandings as the same word conveys different meanings in different contexts.

### Extracting Facts from Wikidata

We utilize the Wikidata (Vrandei and Krotzsch, 2014) dump of April 02, 2024, as a massive and comprehensiveensive (Longpre et al., 2021; Pan et al., 2023; Xie et al., 2024) knowledge source to extract and construct facts. The information is structured by transforming knowledge triples and qualifiers into a quintuplet format: \((s,r,o,s_{d},o_{d})\), where \(s\) is the subject, \(r\) is the relation, \(o\) is the object, \(s_{d}\) is the description of \(s\), and \(o_{d}\) is the description of \(o\). To filter out overlapping and contradictory knowledge within Wikidata, knowledge triples with the same \((s,r)\) pair are selected only once. As relationship types and their representations are key factors for factual knowledge memorization (Mallen et al., 2023), we focus on the top 100 most frequent relations to ensure sufficient coverage, transforming \((s,r,o)\) into claims using templates for each relation. The used templates are shown in Table 4.

### Constructing Knowledge Conflict Claims

Based on the extracted knowledge triples, we substitute the entity with a same-type entity to construct conflict claims (Longpre et al., 2021). Specifically, we use the following strategies for three conflict types: (1) Misinformation conflicts simulate conflicts involving misinformation, such as fake news and rumors that contradict reality. We generate these conflicts by replacing \(o\) with \(o^{\prime}\) in \((s,r,o,s_{d},o_{d})\), where \(o^{\prime}\) is selected from other quintuplets sharing the same relation \(r\); (2) Temporal conflicts capture the discrepancies arising from changes in knowledge, highlighting the tension between outdated and newly updated information. To prevent conflicts with the LLMs updated parametric knowledge (Lazaridou et al., 2021), we incorporate a future time span into the claim, resulting in \((s,r,o^{\prime},s_{d},o_{d},T_{s},T_{e})\), where \(T_{s}\) and \(T_{e}\) represent the start and the end timestamps, respectively; (3) Semantic conflicts depict situations where identical words convey entirely different meanings. To simulate such polysemous situations, we generate an additional description for the conflicting subject \(s^{\prime}_{d}\) based on \((s,r,o^{\prime},s_{d})\)3. The final modification is \((s,r,o^{\prime},s^{\prime}_{d},o_{d})\).

Footnote 3: We use the LLAMA-3-70B-Instruct model for all evidence and description generation: https://huggingface.co/meta-llama/Meta-llama-3-70B-Instruct.

### Generating Diverse Evidence Texts

Previous works have proven the evidence generated from the powerful generative LLMs is more coherent compared to word-level editing methods (Xie et al., 2024). Therefore, we adopt LLMs to produce corresponding evidence for each claim. Since the description provides additional information that helps the model generate more accurate and relevant evidence (Shao et al., 2023), we utilize \((s_{d},o_{d})\) as part of the prompt to make up supporting evidence for the claim. To account

Figure 1: The pipeline of ConflictBank construction. (1) We extract facts from Wikidata and (2) transform them into conflict claims based on different causes, then (3) employ LLM to generate evidence in three text styles, and finally (4) apply three processes to control data quality: feature filtering, fact-evidence entailment checking, and conflict confirmation between evidence.

for the diversity of texts in each practical field in our taxonomy, we produce three types of textual styles: Wikipedia, book, and news. The textual styles of the generated evidence are finely controlled through corresponding prompts. We exhibit prompts and examples in Appendix E.

### Controlling Data Quality

In order to avoid ambiguity caused by the sparsity of knowledge bases and harvest a high-quality dataset, we perform the following steps to strictly clean the generated data. We provide detailed descriptions of each validation and training step, the running time for each processing step, and the resulting data quantities in Appendix B.

**Feature filtering:** Since LLMs proactively refuse to answer questions when they lack knowledge (Yang et al., 2023), the previous steps are inevitable to involve the response "refusal to answer" when fabricating the fictional information. Such responses cannot serve as credible evidence for specific conflict claims, thereby reducing the quality of the dataset. To address this, we manually identify and extract common features in these refusal responses, e.g., "I apologize" and "I can't." (Chen et al., 2021) and filter out all model-generated content containing these features.

**Fact-evidence entailment checking:** A gold piece of evidence should exhibit a strong correlation with its corresponding claim and effectively support it. To achieve this, we employ the state-of-the-art NLI model DeBERTa-V24 to rigorously assess the relationship between the generated evidence and the claims. We retain samples demonstrating strong fact-evidence entailment to enhance the authenticity of the conflicts within the dataset (Xie et al., 2024).

Footnote 4: https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli.

**Conflict confirmation between evidence:** We verify that each type of conflict evidence contradicts the default evidence. Specifically, we utilize SBERT5 to compute embeddings for all evidence and train a conflict confirmation classifier on the existing conflict dataset (Xie et al., 2024). This classifier evaluates whether two pieces of evidence conflict. To ensure reliability, we manually evaluate 200 random examples and observe over 95% accuracy. We filter out all evidence pairs identified as non-conflicting by the classifier to ensure dataset quality.

Footnote 5: https://huggingface.co/sentence-transformers/all-mpnet-base-v2.

### Synthesizing Question-Answer Pairs

After the steps above, we manually sample 200 data pairs and employ 5 volunteers to evaluate them. The results indicate that our constructed dataset effectively reflects real-world conflict scenarios. Detailed information about this process is provided in Appendix B.5. Subsequently, we construct

Figure 2: Examples and data composition in ConflictBank. Evidence is styled as either Wikipedia, book, or news. Each question includes one default and three types of conflict evidence. ConflictBank contains 7,453,853 claim-evidence pairs and 553,117 QA pairs.

questions for the object by substituting the object in the claim. The model will be required to answer questions based on the provided evidence. Each question is constructed with four options. The first option is the default answer from the original claim. The second is the object we used for substitution when constructing the corresponding conflicting claim. The remaining two are similar but unrelated choices. We search for objects with the same quintuplet structure as o' but do not appear in the default claim or conflict claim to serve as these two options. We provide an example about _"Anne Hathaway"_ and our data composition in Figure 2.

## 3 Experiments

### Experimental Setup

ModelsTo explore the behavior of LLMs when encountering knowledge conflicts, we perform a comprehensive evaluation on 12 LLMs ranging from 0.5B to 70B. This evaluation covers the following four model series: Gemma (2B, 7B) [Team et al., 2024], LLaMA2 (7B, 13B, 70B) [Touvron et al., 2023], LLaMA3 (7B, 70B), Qwen1.5 (0.5B, 4B, 7B, 14B, 72B) [Bai et al., 2023a]. To investigate the impact of internal knowledge conflicts within the parametric memory, we continue pre-training three representative LLMs, including Qwen1.5-4B, Mistral-7B, and LLaMA3-8B.

Evaluation MetricsTo minimize randomness and facilitate evaluation [Hendrycks et al., 2021], we append the question to the input prompt and conclude with the "Answer:". The model then generates probabilities for the tokens "(A)", "(B)", "(C)" and "(D)". The option with the highest probability is selected as the predicted answer. Following Gekhman et al. [2024], we identify QA pairs that models can correctly answer, both with and without additional default evidence, to represent the model's internal knowledge. We then examine how conflicts affect these QA pairs to understand their impact on model behavior. We measure the Original Answer Ratio (OAR) and the Counter Answer Ratio (CAR) to assess model performance [Longpre et al., 2021]. The Memorization Ratio (\(M_{R}\)) is then used to quantify how often LLMs rely on their parametric memory:

\[M_{R}=\frac{OAR}{OAR+CAR}\] (1)

where higher memorization ratios indicate greater reliance on parametric memory, while lower ratios suggest more adoption of the constructed conflicting knowledge.

### Conflicts in Retrieved Knowledge

In this section, we examine LLMs' behavior in two retrieved knowledge conflict scenarios: (1) models are presented solely with external evidence that contradicts their parametric memory, and (2) models are given two pieces of external evidence, one matching their parametric knowledge and one conflicting with it6. We report the \(M_{R}\) to illustrate the performance of different models, varying in series and parameter size when faced with conflicting evidence pairs. The results of these two settings are shown in Figure 3 and Figure 4, respectively. We draw the following observations:

Footnote 6: The order of evidence is randomized in all experiments to avoid any influence of sequence on the results.

**LLMs are highly receptive to external evidence and often prefer evidence consistent with their internal beliefs.** As shown in Figure 3, all models exhibit memorization ratios below 50%, indicating that models are highly receptive to external evidence when it is the only evidence available, even when it conflicts with their parametric memory. However, as shown in Figure 4, all LLMs demonstrate significantly higher memorization ratios (over 50%) when parametric memory is also provided as evidence. These two above findings are consistent with previous work [Xie et al., 2024], confirming that LLMs are easily deceived by disinformation and indicate strong _confirmation bias_ when facing multiple pieces of conflicting evidence. We also conduct extensive experiments using one of the most advanced closed-source models (i.e., GPT-4o), by investigating the model's behavior in the same two scenarios. The results align with our findings above on open-source models, indicating that the representative closed-sourced model GPT-4 is also sensitive to semantic conflict. The results are shown in Table 6.

**LLMs are more sensitive to temporal and semantic conflicts.** In Figure 3, we observe that all models exhibit a lower \(M_{R}\) in temporal and semantic conflicts compared to misinformation conflicts,indicating higher sensitivity to these types of external conflicting knowledge. A similar trend is evident in models with larger parameter sizes when facing two pieces of conflicts in Figure 4. For example, in the LLAMA2-70B model, the \(M_{R}\) for temporal and semantic conflicts is lower than those for misinformation conflicts (i.e., 7.77% & 3.73% v.s. 9.45%). These findings suggest that implicit conflicts, which seem reasonable and closely related to the model's internal knowledge, cause more confusion than explicit factual errors.

**Larger models are more susceptible to conflicting knowledge.** In Figure 4, we observe a decrease in \(M_{R}\) as the parameter size increases within the same model series. For instance, in the LLAMA2 series, the \(M_{R}\) consistently decreases as the parameters increase from 7B to 13B to 70B, with reductions of 5.12% and 6.37% in an average of three conflict causes, respectively. These observations suggest that larger models exhibit increased susceptibility to conflicting knowledge, and this susceptibility becomes more pronounced as model size increases. Figure 5 further shows that models are susceptible to the order of evidence, with larger models tending to favor later pieces. For example, in the LLAMA2 series, when conflicting memory is placed later in the sequence, the 7B model has a \(M_{R}\) of 56.44%, while the 70B model's \(M_{R}\) drops to 38.97%. This phenomenon highlights the importance of considering evidence order to mitigate the impact of conflicting knowledge in future retrieval-augmented models.

### Conflicts in Embedded Knowledge

Benefiting from the extensive data in our benchmark, we provide the opportunity to investigate the impact of internal knowledge conflicts on model performance. We mix default evidence and conflict

Figure 4: Memorization ratio (\(M_{R}\)) of different LLMs under three types of conflict evidence when given two pieces of external evidence. Within the same model series, models with larger parameter sizes exhibit lower \(M_{R}\) compared to their smaller counterparts.

Figure 5: Memorization ratio (\(M_{R}\)) of LLMs with different evidence orders. The legend indicates the type of evidence closest to the question. We report the average \(M_{R}\) for the three conflict causes.

Figure 3: Memorization ratio (\(M_{R}\)) of different LLMs under three types of conflict evidence when presented with solely contradictory external evidence. All series consistently show the lowest \(M_{R}\) for semantic conflicts, with higher values for the other two conflict types across all model sizes.

evidence in 2:1, 1:1, and 2:3, along with a control setup with no conflict evidence (i.e., 1:0). We randomly select evidence from three different conflict types to ensure diversity in conflict sources.

We inject conflicting knowledge into the models by continually pre-training the foundational models and maintaining consistency by training each experimental setup on 1.06B tokens. We report the Original Answer Ratio (OAR) for each model to analyze the impact of internal knowledge conflicts on model behaviors. As shown in Figure 6, our findings highlight two key points:

**Internal knowledge can be easily impacted by the introduction of conflicting data.** LLMs demonstrate a noticeable decline in the ratio of default answers when internal conflicts are introduced. This degradation is particularly significant in LLAMA2-7B and Mistral-7B. Even with only one-third (2:1 ratio of default to conflicting data) of the data being conflicting, these models exhibit substantial declines in OAR, with decreases of 41.5% and 37.6%, respectively. Moreover, as the amount of conflicting knowledge increases, the models' performance further deteriorates. These observations indicate that the introduction of conflicting data negatively affects the models' internal knowledge, and the greater the amount of conflicting data, the worse the models' performance.

**Embedded knowledge conflicts do not affect the model's ability to follow external evidence.** However, when we prepend the original default evidence to the question, we find that models maintain their original performance, choosing the default answer regardless of the amount of conflicting data introduced. This indicates that injected conflicts affect only the model's internal knowledge but do not impact its ability to follow external evidence. This suggests that leveraging retrieved or tool-assisted methods to access correct and relevant knowledge can effectively mitigate the adverse effects of internal conflicts on model performance. Based on this finding, we further explore the model's performance when external conflicts are presented in Section 3.4.

Figure 6: Original Answer Ratio (OAR) of LLMs with varying proportions of conflicts embedded in the model’s parametric memory. “Without evidence” represents the model answering without any evidence, while “with default evidence” represents the model answering with the original evidence prepended to the question. We report the average OAR for the three conflict causes.

Figure 7: Counter Answer Ratio (CAR) of Qwen1.5-4B with internal conflicts and external conflicting evidence. The left picture shows the model with one conflict evidence. The middle and right pictures show the model with one conflict and one default evidence, with the middle picture having a randomized order and the right picture placing the conflict evidence at last. We report the average CAR for the three conflict causes.

### Interplay among the Conflicts

In this section, we aim to investigate the interaction between different types of knowledge conflicts. It is crucial to understand the relationship between the internal knowledge inconsistency of the model and its behavior in response to the context. Following the setup in Section 3.3, we conduct the experiments on the Qwen1.5-4B and use the Counter Answer Ratio (CAR) to measure the model's preference for answering the substituted object. The results are shown in Figure 7. Our observations are summarized as follows:

**Model relies more on retrieved knowledge for answering.** When conflicts are present in the embedded knowledge, the model increasingly relies on externally retrieved knowledge for answers. In the single conflict evidence scenario, the model's dependency on conflict evidence is evident, with the OAR increasing from 97.5% to 98.36% when the ratio of conflict data is 2:1. As the proportion of internal conflict data rises, the model consistently follows the external preference, maintaining an OAR around 98%, as shown in the left part of Figure 7.

In the multiple evidence scenario, the model similarly exhibits a firm reliance on external knowledge. Additionally, we observe that the model shows a higher preference for the evidence closer to the question than scenarios without internal conflicts. As shown in the right part of Figure 7, the model prefers the later-positioned conflict evidence more when internal conflicts are present (77.52% vs. 82.08%), this phenomenon we also observed in Section 3.2 when no internal conflicts were present. However, with internal conflicts, the model shows an even stronger preference for the external knowledge closer to the question. This indicates that the order of evidence during continued pre-training is crucial for conflicting knowledge handling due to the model's increased reliance on external information.

### Detailed Description Can Make the LLMs' Objectives More Explicit

In this section, we aim to explore whether refining questions can encourage the model to exhibit desired behavior when encountering conflicts. Specifically, we incorporate descriptions of temporal and semantic conflicts within the questions. For temporal conflict scenarios, we add specific years to the questions. For semantic conflict scenarios, we include detailed descriptions of the subjects. We conduct experiments to observe the model's behavior with and without internal conflicts when provided with two pieces of external conflicting evidence. We analyze the impact of including these descriptions on the model's performance in both scenarios.

The results are shown in Figure 8. We observe that LLMs, whether with or without internal conflicts, exhibit a significant decrease in \(M_{R}\) when provided with external knowledge containing descriptions compared to without them. Take LLaMA3-70B as an example, the \(M_{R}\) drops from 67.52% to 8.31% when descriptions are included. Furthermore, when internal conflicts are presented, adding descriptions also makes the LLMs' objectives more explicit. For example, when the internal conflict ratio is 2:1, the \(M_{R}\) of Qwen1.5-4B decreases from 55.37% to 41.80% with the inclusion of descriptions. This suggests that the more specific and detailed the text, the more likely the LLM

Figure 8: Memorization Ratio (\(M_{R}\)) of LLMs with or without a description in the question. “w/ des.” and “w/o des.” indicate the presence or absence of conflict context descriptions. The left image shows no internal conflicts, while the right shows the Qwen1.5-4B model with internal conflicts. We report the average \(M_{R}\) for the three causes of conflict.

is to trust the external knowledge, and designing detailed instructions can effectively improve the faithfulness of LLMs.

## 4 Related Work

**Taxonomy of knowledge conflicts** Knowledge conflicts are mainly divided into two types: retrieved knowledge conflicts and embedded knowledge conflicts. Retrieved conflicts occur when the model's internal knowledge conflicts with externally retrieved information, commonly in retrieval-augmented generation (RAG) and tool-augmented scenarios (Zhang and Choi, 2021; Li et al., 2023; Peng et al., 2023; Kasai et al., 2024). Embedded conflicts arise from conflicting parametric knowledge within LLMs, increasing uncertainty during knowledge-intensive tasks and undermining trustworthiness (Chang and Bergen, 2023; Chen et al., 2023; Raj et al., 2023; Rabinovich et al., 2023; Raj et al., 2023; Bartsch et al., 2023). Currently, most research focuses on retrieved conflicts. Our work extends this by investigating both types and their interactions.

**The Causes of Knowledge Conflicts** With the rapid expansion of diverse knowledge sources, the risk of misinformation generated by LLMs has increased, posing challenges for detection (Chen and Shu, 2023; Bengio et al., 2024; Wang et al., 2023; Solaiman et al., 2023; Goldstein et al., 2023; Ferrara, 2024). Therefore, misinformation is the main focus in previous work as a cause of knowledge conflicts (Hsu et al., 2021; Ko et al., 2022; Li et al., 2023). However, many factors contribute to knowledge conflicts in real-world scenarios, such as knowledge update (Lazaridou et al., 2021) and the multiple meanings of words (Sevgili et al., 2022). In ConflictBank, we construct conflicts from three causes to provide a more comprehensive analysis.

**Knowledge Conflicts Datasets** To construct conflict-related datasets, previous works have primarily adopted two methods, including entity-level substitution (Longpre et al., 2021; Chen et al., 2022; Si et al., 2023; Wang et al., 2023) and generative approaches using LLMs (Ying et al., 2024; Xu et al., 2024; Tan et al., 2024). Recent datasets combined these two methods to create more coherent conflicting pairs (Xie et al., 2024), providing insights into the causes and behaviors of LLMs when encountering conflicts (Aggarwal et al., 2021; Chen et al., 2021). However, these datasets primarily focus on conflicts in retrieved knowledge, with few addressing internal conflicts within parametric memory and more complex scenarios.

## 5 Conclusion

We develop ConflictBank, a novel and comprehensive dataset for studying the effect of knowledge conflicts from misinformation, temporal updates, and semantic variations. For each of the knowledge conflict source, we utilize LLMs to generate three styles of texts to maximize the dataset diversity. In summary, ConflictBank is a large diverse dataset consists of 553K QA pairs and 7M knowledge conflict evidence in high quality. The QA pairs could be used for model evaluations, and the evidence could be utilized for simulating the conflicts encountered in the LLM pre-training and the inference phases. With ConflictBank, we conduct pilot experiments to investigate LLMs' behaviors under three common conflict scenarios, including the embedded knowledge conflict in pre-training, the retrieved knowledge conflict when inference, and the interplay between the above two conflicts. We believe ConflictBank could be used in broad applications, and help analyze and build trustworthy large language models.

## Acknowledgement

We want to thank all the anonymous reviewers for their valuable comments. This work was supported by the National Science Foundation of China (NSFC No. 62206194), the Priority Academic Program Development of Jiangsu Higher Education Institutions, the Natural Science Foundation of Jiangsu Province, China (Grant No. BK20220488), and Young Elite Scientists Sponsorship Program by CAST (2023QNRC001).

## References

* [1]J. Wei, M. Bosma, V. Zhao, K. Guu, A. Wei Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le (2022) Finetuned language models are zero-shot learners. In International Conference on Learning Representations, External Links: Link Cited by: SS1, SS2.
* [2]Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie (2023) A survey on evaluation of large language models. External Links: Link Cited by: SS1, SS2.
* [3]Z. Su, J. Li, J. Zhang, T. Zhu, X. Qu, P. Zhou, Y. Bowen, Y. Cheng, et al. (2024) Living in the moment: can large language models grasp co-temporal reasoning?. arXiv preprint arXiv:2406.09072. Cited by: SS1, SS2.
* [4]Z. Su, J. Zhang, T. Zhu, X. Qu, J. Li, M. Zhang, and Y. Cheng (2024) Timo: towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192. Cited by: SS1, SS2.
* [5]M. Jin, Q. Yu, D. Shu, H. Zhao, W. Hua, Y. Meng, Y. Zhang, and M. Du (2024-06) The impact of reasoning step length on large language models. In Findings of the Association for Computational Linguistics ACL 2024, B. R., T. J. Lee, A. Martins, and V. Srikumar (Eds.), Bangkok, Thailand and virtual meeting, pp. 1830-1842. External Links: Link, Document Cited by: SS1, SS2.
* [6]Z. Lu, J. Tian, W. Wei, X. Qu, Y. Cheng, D. Chen, et al. (2024) Mitigating boundary ambiguity and inherent bias for text classification in the era of large language models. arXiv preprint arXiv:2406.07001. Cited by: SS1, SS2.
* [7]Z. Lu, C. Fan, W. Wei, X. Qu, D. Chen, and Y. Cheng (2024-06) Dynamic integration of modular expertise in model merging. arXiv preprint arXiv:2406.15479. Cited by: SS1, SS2.
* [8]C. Fan, Z. Lu, W. Wei, J. Tian, X. Qu, D. Chen, and Y. Cheng (2024) On giant's shoulders: effortless weak to strong by dynamic logits fusion. arXiv preprint arXiv:2406.15480. Cited by: SS1, SS2.
* [9]T. Zhu, X. Qu, D. Dong, J. Ruan, J. Tong, C. He, and Y. Cheng (2024) Llama-moe: building mixture-of-experts from llama with continual pre-training. arXiv preprint arXiv:2406.16554. Cited by: SS1, SS2.
* [10]S. Longpre, K. Perisetla, A. Chen, N. Ramesh, C. DuBois, and S. Singh (2021-10) Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, November 2021, pp. 7052-7063. External Links: Link, Document Cited by: SS1, SS2.
* [11]Y. Wang, S. Feng, H. Wang, W. Shi, V. Balachandran, T. He, and Y. Tsvetkov (2023) Resolving knowledge conflicts in large language models. arXiv preprint arXiv:2310.00935. Cited by: SS1, SS2.
* [12]J. Chen, H. Lin, X. Han, and L. Sun (2023) Benchmarking large language models in retrieval-augmented generation. External Links: Link Cited by: SS1, SS2.
* [13]R. Xu, B. S. Lin, S. Yang, T. Zhang, W. Shi, T. Zhang, Z. Fang, W. Xu, and H. Qiu (2024) The earth is flat because...: investigating llms' belief towards misinformation via persuasive conversation. External Links: Link Cited by: SS1, SS2.

[MISSING_PAGE_POST]

Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. _arXiv preprint arXiv:2406.06007_, 2024b.
* Xia et al. (2024c) Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language models. _arXiv preprint arXiv:2410.13085_, 2024c.
* Pan et al. (2023) Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. On the risk of misinformation pollution with large language models, 2023.
* Xie et al. (2024) Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn cloth: Revealing the behavior of large language models in knowledge conflicts. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=auKAUJZMO6.
* Wan et al. (2024) Alexander Wan, Eric Wallace, and Dan Klein. What evidence do language models find convincing? _arXiv preprint arXiv:2402.11782_, 2024.
* Xie et al. (2023) Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn cloth: Unraveling the behavior of large language models in knowledge conflicts. _arXiv preprint arXiv:2305.13300_, 2023.
* Hsu et al. (2021) Cheng Hsu, Cheng-Te Li, Diego Saez-Trumper, and Yi-Zhan Hsu. Wikicontradiction: Detecting self-contradiction articles on wikipedia. In _2021 IEEE International Conference on Big Data (Big Data)_, pages 427-436. IEEE, 2021.
* Ko et al. (2022) Miyoung Ko, Ingyu Seong, Hwaran Lee, Joonsuk Park, Minsuk Chang, and Minjoon Seo. Claimdiff: Comparing and contrasting claims on contentious issues. _arXiv preprint arXiv:2205.12221_, 2022.
* Pan et al. (2023a) Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang Wang. Attacking open-domain question answering by injecting misinformation. _arXiv preprint arXiv:2110.07803_, 2023a.
* Elazar et al. (2021a) Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schutze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. _Transactions of the Association for Computational Linguistics_, 9:1012-1031, 2021a. doi: 10.1162/tacl_a_00410. URL https://aclanthology.org/2021.tacl-1.60.
* Zhou et al. (2023) Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 14544-14556, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.968. URL https://aclanthology.org/2023.findings-emnlp.968.
* Tan et al. (2024) Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. Blinded by generated contexts: How language models merge generated and retrieved contexts for open-domain qa? _arXiv preprint arXiv:2401.11911_, 2024.
* Du et al. (2022) Yibing Du, Antoine Bosselut, and Christopher D Manning. Synthetic disinformation attacks on automated fact verification systems. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 10581-10589, 2022.
* Zhou et al. (2024) Zihao Zhou, Quifeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, and Kaizhu Huang. Mathattack: Attacking large language models towards math solving ability. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 19750-19758, 2024.
* Lazaridou et al. (2021) Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing temporal generalization in neural language models. _Advances in Neural Information Processing Systems_, 34:29348-29363, 2021.
* Zhang et al. (2020)Zhaochen Su, Zecheng Tang, Xinyan Guan, Lijun Wu, Min Zhang, and Juntao Li. Improving temporal generalization of pre-trained language models with lexical semantic change. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 6380-6393, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.428. URL https://aclanthology.org/2022.emnlp-main.428.
* Ansell et al. (2021) Alan Ansell, Felipe Bravo-Marquez, and Bernhard Pfahringer. PolyLM: Learning about polysemy through language modeling. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pages 563-574, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.45. URL https://aclanthology.org/2021.eacl-main.45.
* Sevgili et al. (2022a) Ozge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko, and Chris Biemann. Neural entity linking: Assurvey of models based on deep learning. _Semantic Web_, 13(3):527-570, April 2022a. ISSN 1570-0844. doi: 10.3233/sw-222986. URL http://dx.doi.org/10.3233/SW-222986.
* Xu et al. (2024b) Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. Knowledge conflicts for llms: A survey. _arXiv preprint arXiv:2403.08319_, 2024b.
* Schuster et al. (2021) Tal Schuster, Adam Fisch, and Regina Barzilay. Get your vitamin C! robust fact verification with contrastive evidence. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 624-643, Online, June 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.naacl-main.52.
* Huang et al. (2024a) Rikui Huang, Wei Wei, Xiaoye Qu, Wenfeng Xie, Xianling Mao, and Dangyang Chen. Joint multi-facts reasoning network for complex temporal question answering over knowledge graph, 2024a. URL https://arxiv.org/abs/2401.02212.
* Huang et al. (2024b) Rikui Huang, Wei Wei, Xiaoye Qu, Shengzhe Zhang, Dangyang Chen, and Yu Cheng. Confidence is not timeless: Modeling temporal validity for rule-based temporal knowledge graph forecasting. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 10783-10794, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.580.
* Sevgili et al. (2020) Ozge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko, and Chris Biemann. Neural entity linking: A survey of models based on deep learning. _Semantic Web_, 13(3):527-570, 2022b.
* Vrandei and Krotzsch (2014) Denny Vrandei and Markus Krotzsch. Wikidata: A free collaborative knowledge base. _Communications of the ACM_, 57:78-85, 2014. URL http://cacm.acm.org/magazines/2014/10/178785-wikidata/fulltext.
* Mallen et al. (2023) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 9802-9822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https://aclanthology.org/2023.acl-long.546.
* Shao et al. (2023) Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. In _International Conference on Machine Learning_, pages 30706-30775. PMLR, 2023.
* Yang et al. (2023) Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. Alignment for honesty, 2023.

* Chen et al. (2021) Wenhu Chen, Xinyi Wang, and William Yang Wang. A dataset for answering time-sensitive questions, 2021.
* Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. _arXiv preprint arXiv:2403.08295_, 2024.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Bai et al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023a.
* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* Gekhman et al. (2024) Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge encourage hallucinations?, 2024.
* Zhang and Choi (2021) Michael JQ Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into qa. _arXiv preprint arXiv:2109.06157_, 2021.
* Li et al. (2023) Jierui Li, Vipul Raheja, and Dhruv Kumar. Contradoc: understanding self-contradictions in documents with large language models. _arXiv preprint arXiv:2311.09182_, 2023.
* Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback, 2023.
* Kasai et al. (2024) Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, Kentaro Inui, et al. Realtime qa: What's the answer right now? _Advances in Neural Information Processing Systems_, 36, 2024.
* Chang and Bergen (2023) Tyler A. Chang and Benjamin K. Bergen. Language model behavior: A comprehensive survey, 2023.
* Chen et al. (2023b) Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, and Yanghua Xiao. Say what you mean! large language models speak too positively about negative commonsense knowledge, 2023b.
* Raj et al. (2023a) Harsh Raj, Vipul Gupta, Domenic Rosati, and Subhabrata Majumdar. Semantic consistency for assuring reliability of large language models, 2023a.
* Rabinovich et al. (2023) Ella Rabinovich, Samuel Ackerman, Oma Raz, Eitan Farchi, and Aeret Anaby-Tavor. Predicting question-answering performance of large language models through semantic consistency, 2023.
* Raj et al. (2023b) Harsh Raj, Domenic Rosati, and Subhabrata Majumdar. Measuring reliability of large language models through semantic consistency, 2023b.
* Bartsch et al. (2023) Henning Bartsch, Ole Jorgensen, Domenic Rosati, Jason Hoelscher-Obermaier, and Jacob Pfau. Self-consistency of large language models under ambiguity, 2023.
* Chen and Shu (2023) Canyu Chen and Kai Shu. Combating misinformation in the age of llms: Opportunities and challenges, 2023.
* Chen et al. (2023)Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atlm Gune Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, and Soren Mindermann. Managing extreme ai risks amid rapid progress. _Science_, 384(6698):842845, May 2024. ISSN 1095-9203. doi: 10.1126/science.adn0117. URL http://dx.doi.org/10.1126/science.adn0117.
* Wang et al. (2023) Cunxiang Wang, Haofei Yu, and Yue Zhang. RFiD: Towards rational fusion-in-decoder for open-domain question answering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Findings of the Association for Computational Linguistics: ACL 2023_, pages 2473-2481, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.155. URL https://aclanthology.org/2023.findings-acl.155.
* Solaiman et al. (2023) Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daume III au2, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, and Apostol Vassilev. Evaluating the social impact of generative ai systems in systems and society, 2023.
* Goldstein et al. (2023) Josh A. Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. Generative language models and automated influence operations: Emerging threats and potential mitigations, 2023.
* Ferrara (2024) Emilio Ferrara. Genai against humanity: nefarious applications of generative artificial intelligence and large language models. _Journal of Computational Social Science_, February 2024. ISSN 2432-2725. doi: 10.1007/s42001-024-00250-1. URL http://dx.doi.org/10.1007/s42001-024-00250-1.
* Chen et al. (2022) Hung-Ting Chen, Michael Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 2292-2307, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.146. URL https://aclanthology.org/2022.emnlp-main.146.
* Si et al. (2023) Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=98p5x51L5af.
* Ying et al. (2024) Jiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long Cui, and Yongbin Liu. Intuitive or dependent? investigating llms' behavior style to conflicting prompts, 2024.
* Aggarwal et al. (2021) Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. Explanations for CommonsenseQA: New Dataset and Models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 3050-3065, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.238. URL https://aclanthology.org/2021.acl-long.238.
* Ying et al. (2023) Jiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long Cui, and Yongbin Liu. Intuitive or dependent? investigating llms' robustness to conflicting prompts. _arXiv preprint arXiv:2309.17415_, 2023.
* Xu et al. (2023) Rongwu Xu, Brian S Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu. The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation. _arXiv preprint arXiv:2312.09085_, 2023.
* Elazar et al. (2021b) Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schutze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. _Transactions of the Association for Computational Linguistics_, 9:1012-1031, 2021b.

* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Ainslie et al. (2023) Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. _arXiv preprint arXiv:2305.13245_, 2023.
* Bai et al. (2023b) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023b.
* Zheng et al. (2024) Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. _arXiv preprint arXiv:2403.13372_, 2024. URL http://arxiv.org/abs/2403.13372.
* Su et al. (2023) Zhaochen Su, Juntao Li, Zikang Zhang, Zihan Zhou, and Min Zhang. Efficient continue training of temporal language model with structural information. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 6315-6329, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.418. URL https://aclanthology.org/2023.findings-emnlp.418.
* Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. {ZeRO-Offload}: Democratizing {Billion-Scale} model training. In _2021 USENIX Annual Technical Conference (USENIX ATC 21)_, pages 551-564, 2021.
* Dao (2023) Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.

Discussion

### Code Access

We have uploaded our datasets to Hugging Face. The claim and evidence conflict pairs can be found at https://huggingface.co/datasets/Warrieryes/CB_claim_evidence, and the QA pairs used for analysis are available at https://huggingface.co/datasets/Warrieryes/CB_qa. We have documented all code (including the code to preprocess the data, create, train, and evaluate the baseline models and metrics) in an openly-available GitHub repository: https://github.com/zhaochen0110/conflictbank.

### Motivation

In essence, our work aims to provide a large-scale, diverse, and realistic benchmark to study knowledge conflicts in LLMs. Our motivation stems from exploring how retrieved and embedded knowledge conflicts impact model behavior and reliability across various scenarios. To align our dataset distribution and research with real-world situations, we construct conflicts from three different causes, including misinformation, temporal discrepancies, and semantic divergences. Our benchmark allows for an equitable comparison of different conflict effects on models, addressing the limitations of existing datasets that often focus narrowly on specific conflict types. Ultimately, by analyzing the results of our dataset, we aim to offer a detailed and nuanced understanding of how models handle conflict information, guiding the development of more robust and trustworthy language models in real-world scenarios.

### Limitation

Our approach uses generative methods to efficiently construct a large number of conflict pairs, a widely adopted technique in current research (Xie et al., 2024). Although conflict pairs may be extracted from pre-training corpora, the vast amount of data makes it challenging to efficiently identify and extract a significant number of conflicts. In future work, we will explore more methods for constructing conflict pairs to verify the robustness of our dataset.

### Ethics Statement

In this paper, we created a comprehensive benchmark ConflictBank for analyzing knowledge conflicts. The dataset is constructed based on Wikidata, which is under the public domain7. Therefore, we can adapt these data to construct our dataset. We will also release our data under the same license. The scope of our dataset is purely for scientific research. However, the contexts from the model outputs that may be considered offensive. Adopting such content is not a decision of the authors, and all content does not reflect the views of the authors of this paper.

Footnote 7: https://www.wikidata.org/wiki/Wikidata:License

## Appendix B Dataset Details

We exhibit a complete example of our proposed ConflictBank in Table 1.

### Construction Process of ConflictBank

To clarify our data construction process, we present detailed pseudocode in Algorithm 1. The input consists of a set of facts \(F\), where each fact is represented as \((s,r,o)\), and a dictionary \(D\) mapping each \((s,r)\) pair to an object \(o\). The goal is to generate a new set of conflict facts \(R\).

We begin with an empty set \(R\) for storing conflict facts and an empty set \(used_{pairs}\) to track processed \((s,r)\) pairs. We then iterate through each fact in \(F\). For each fact \(f_{i}\) represented as \((s_{i},r_{i},o_{i})\), we check if the pair \((s_{i},r_{i})\) is already in \(used_{pairs}\). If it is, we skip to the next fact; if not, we add it to \(used_{pairs}\). Next, we iterate through \(F\) again. For each fact \(f_{j}\) represented as \((s_{j},r_{j},o_{j})\), we check if it shares the same \(r\) with \(f_{i}\), has a different object, and does not correspond to \((s_{i},s_{j})\) in any other 

\begin{table}
\begin{tabular}{l l} \hline
**Relation** & P166 \\ \hline
**Subject** & Anne Hathaway \\ \hline
**Subject Description** & American actress \\ \hline
**Semantic Description** & American writer \\ \hline
**Object** & Primetime Emmy Award \\ \hline
**Object Description** & Academy of Television Arts \& Sciences accolade \\ \hline
**Replaced Object** & Hugo Award \\ \hline
**Replaced Description** & set of awards given annually for the best science fiction of the previous year \\ \hline
**Default Claim** & Anne Hathaway received Primetime Emmy Award. \\ \hline
**Category** & Wikipedia \\ \hline  & Anne Jacqueline Hathaway (born November 12, 1982) is an American actress. Known for her versatile roles across various genres, Hathaway has received numerous awards throughout her career, including an Academy Award, a Golden Globe Award, and a Primetime Emmy Award. Her notable films include: The Princess Diaries, "Les Miserables," and "The Devil Wears Prada," \\  & Primetime Emmy Award: The Primetime Emmy Award is an accolade bestowed by the Academy \\
**Evidence**\(\dagger\) & of Television Arts \& Sciences (ATES) in recognition of excellence in American primetime \\  & television programming. Founded in 1949, the award signifies excellence in television. \\  & Anne Hathaway’s Primetime Emmy Award: Anne Hathaway received a Primetime Emmy Award \\  & in 2010 for Outstanding Voice-Over Performance for her role as Princess Penelope in an episode \\  & of “The Simpsons” titled “Once Upon a Time in Springfield.” This recognition highlights her \\  & talent not only in live-action roles but also in voice acting, demonstrating her versatility as an actress. \\ \hline
**Misinformation claim** & Anne Hathaway received Nobel Prize. \\ \hline
**Category** & News \\ \hline  & In a groundbreaking announcement, actress Anne Hathaway has been awarded the 2024 Nobel \\  & Peace Prize for her extensive humanitarian work, marking the first time an actor has received \\  & the prestigious honor. Hathaway was recognized for her leadership in EmpowerED, an initiative \\  & she co-founded in 2018 to improve global education access, especially for girls in underserved regions. \\  & The Nobel Committee praised Hathaway commitment, noting that EmpowerED has positively \\  & impacted over 15 million students across 27 countries. This award is a victory for women and \\  & children everywhere, Hathaway said in a statement. Education is a fundamental right. \\  & UN Secretary-General Antonio Gueterres lauded Hathaways efforts, calling her a global leader in \\  & the fight for equality. Hollywood peers, including Emily Blunt and Oprah Winfrey, expressed their \\  & Hathaway will receive her Nobel Prize in December, further cementing her legacy as both an artist and activist. \\ \hline
**Temporal Claim** & Anne Hathaway received Hugo Award in 20 April, 2033. \\ \hline
**Time span** & 20 April, 2033 \\ \hline
**Category** & Books \\ \hline  & In the year 2033, the world stood in new as Anne Hathaway, the acclaimed American actress, stroke \\  & gracefully onto the grand stage of the Nobel Prize ceremony. The sun had set in Stockholm, casting \\  & golden hues over the city on April 20th, when the unexpected was announced: Anne Hathaway, known \\  & for her versatility in film and her captivating performances, had received the Nobel Prize. \\  & It wasn for her acting, as many assumed, but for her remarkable contributions to global humanitarian \\  & efforts. Over the past decade, Hathaway had quietly spearheaded initiatives addressing climate change, \\  & poverty alleviation, and mental health awareness. Her collaboration with scientists and policy makers on \\  & cutting-edge environmental technologies made her a driving force for change. As she stood before the \\  & crowd, a symbol of hope, Hathaways achievement represented a convergence of arts, activism, and science \\ \hline
**Semantic Claim** & Anne Hathaway received Hugo Award. \\ \hline
**Category** & Books \\ \hline  & The early morning light filtered through the lab windows, casting a soft glow on the cluttered desks. \\  & Anne Hathaway stood in the center, her white coat flocked with faint traces of chalk from countless \\  & formulas schibbled on the board behind her. She held a vial, her steady hands a testament to the hours \\  & she’d spent perfecting her experiment. To most, the mixture of mathematical theory and genetic \\  & engineering she pioneered seemed too complex, but to Anne, it was a thrilling challenge. \\  & Her breakthrough came unexpectedly elegant discovery that unified chaotic gene sequencing with \\  & predictive AI models. As her colleagues gathered around, mururring in ave, Anne feit a rare quiet \\  & satisfaction. Months later, in Stockholm, her name would be called, joining the ranks of other laureates \\  & for the Nobel Prize in Chemistry. The stage was vast, but her thoughts lingered on the long nights in that \\  & modest lab, where ambition met possibility. \\ \hline
**Question** & Which award d'd Anne Hathaway receive? \\ \hline
**Options** & A. Hugo Award, B. Primetime Emmy Award, C. PEN/Faulkner Award for Fiction, D. uncertain \\ \hline
**Default Option** & B. Primetime Emmy Award \\ \hline
**Replace Option** & A. Hugo Award \\ \hline \end{tabular}
\end{table}
Table 1: A complete example in the ConflictBank benchmark. Entries marked with \(\dagger\) indicate data generated by generative models.

fact. If all conditions are satisfied, we create a new conflict fact \((s_{i},r_{i},o_{j})\) and add it to \(R\). Finally, we return the set \(R\), which contains the generated conflict facts.

The algorithm ensures that the replacement entity doesn't appear in any other \((s,r)\) pair, preventing incorrect conflicts and enhancing dataset quality. For instance, when processing "Anne Hathaway," we check that the "Hugo Award" is not linked to any actual "Anne Hathaway" entries. This also means excluding entities like "George R.R. Martin," who has won both a Hugo and an Emmy. If we find that "George R.R. Martin" is associated with the "Hugo Award," we skip using the Emmy Award for conflicts since its already listed in another entry. This way, the algorithm maintains the uniqueness of each replacement entity and avoids conflicts with existing relationships.

### Comparison of ConflictBank and Prior Datasets

In Table 2, we show the detailed comparison of our ConflictBank benchmark and prior knowledge conflict datasets. Our dataset is the first to include three main causes of conflict and can be used to evaluate the effects of knowledge conflict on retrieved knowledge, embedded knowledge, and their interactions.

### Running time

Table 3 shows the running time and data volumn after each step for ConflictBank.

\begin{table}
\begin{tabular}{c c c c c|c} \hline \hline \multirow{2}{*}{**Dataset**} & \multicolumn{2}{c}{**Type**} & \multicolumn{3}{c|}{**Causes**} & \multirow{2}{*}{**Sample**} \\ \cline{2-2} \cline{5-6}  & **CM** & **IC** & **IM** & **Misinformation** & **Temporal** & **Semantic** \\ \hline Xie et al. (2023) & ✓ & & ✓ & & 20,091 \\ KC (2023a) & ✓ & & ✓ & & 9,803 \\ KRE (2023) & ✓ & & ✓ & & 11,684 \\ Farn (2023) & ✓ & & ✓ & & 1,952 \\ Tan et al. (2024) & ✓ & & ✓ & & 14,923 \\ WikiContradiction (2021) & ✓ & & ✓ & & 2,210 \\ ClaimDiff (2022) & ✓ & ✓ & ✓ & & 2,941 \\ Pan et al. (2023a) & ✓ & ✓ & & & 52,189 \\ ContraDoc (2023) & ✓ & & ✓ & & 449 \\ ConflictingQA (2024) & ✓ & & ✓ & & 238 \\ ParaRel (2021b) & & ✓ & ✓ & & 328 \\ \hline ConflictBank & ✓ & ✓ & ✓ & ✓ & ✓ & 7,453,853 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Analysis of the exisitng conflict datasets.

### ConflictBank Templates

The templates that we used to create ConflictBank is shown in Table 4.

### Human Evaluation

In this section, we recruited five volunteers to evaluate the entailment between claims and generated evidence and the contradiction between default and conflict evidence. Each volunteer assessed a sample of 200 randomly selected examples to ensure the quality and reliability of our dataset. They were tasked with two main evaluations:

* **Entailment Check:** Determining whether the generated evidence logically supports the corresponding claim.
* **Conflict Verification:** Ensuring that the default and conflict evidence are contradictory.

The human evaluation results showed a high level of accuracy in our data generation process. Out of the 200 examples assessed, only one example was found to be ambiguously conflicting. As shown in Table 5, although the model generated evidence for the misinformation claim _"Daniel Rousse worked for Technical University of Liberec"_, it also included information about his work at _"Ecole de technologie superieure"_ due to existing knowledge within the model. Despite this, the overall conflict remained unaffected, so we retained this type of generated evidence.

This indicates that our confirmation classifier and NLI model effectively ensure the integrity of the conflict pairs in our dataset. These evaluations confirm the robustness of our dataset and its suitability for studying knowledge conflicts in LLMs.

## Appendix C Experimental Details

### Chosen Models

We perform comprehensive experiments on 12 representative large language models, covering four series. Below is the detailed description:

1. [leftmargin=*]
2. **Gemma**[Team et al., 2024] leverages transformer-based networks with enhanced attention mechanisms and optimized layer normalization, as well as fine-tuning with domain-specific pre-training and rigorous hyperparameter tuning inspired by Gemini family [Team et al., 2023] to ensure high performance. We select models with 2B and 7B parameters for our analysis.
3. **LLAMA2**[Touvron et al., 2023] is a popular open-source foundation model, trained on 2T tokens with efficient grouped-query attention (GQA) [Ainslie et al., 2023]. For our analysis, we choose models with 7B, 13B, and 70B parameters.
4. **LLAMA3** builds on LLAMA2 with further architectural enhancements and larger datasets, pushing the boundaries of open-source foundation models. It is trained on over 15T tokens collected from public sources. Models with 7B and 70B parameters are selected for our analysis.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline
**\#** & **Step** & **Time** & **\# gpus** & **Default** & **Misinformation** & **Temporal** & **Semantic** \\ \hline  & Input: Total Comments & - & - & 2,863,205 & 2,863,205 & 2,863,205 & 2,863,205 \\ \hline
1 & Claim Construction & - & - & 2,863,205 & 2,863,205 & 2,863,205 & 2,863,205 \\ \hline
2 & Evidence Generation & 120 hours & 16 & 2,863,205 & 2,863,205 & 2,863,205 & 2,863,205 \\ \hline
3 & Feature Filtering & 15 min & - & 2687972 & 2601469 & 2535547 & 2657879 \\ \hline
4 & NLI Checking & 4 hours & 4 & 1,991,218 & 1878340 & 1650026 & 1934269 \\ \hline
5 & Conflict Confirmation & 3 hours & 4 & 553,117 & 553,117 & 553,117 & 553,117 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Running time of each processing step and the amount of data afterwards. We retain all data that passes the NLI entailment check as claim-evidence pairs. All claim-evidence pairs that pass the conflict confirmation and encompass all types are used to construct the corresponding QA pairs.

\begin{table}
\begin{tabular}{l|c|c} \hline
**Relation id** & **Statement template** & **Question template** \\ \hline P108 & \(\langle\)subject-worked for coobject, & Which person or organization did \(\langle\)subject-worked for? \\ P69 & \(\langle\)subject-attended antibody-object- & Which educational institution did \(\langle\)subject-atom? \\ P54 & \(\langle\)subject-pays for coobject- & Which sports team does \(\langle\)subject-pepresent or represent? \\ P26 & \(\langle\)subject-pays is married to \(\langle\)subject-pays? \\ P39 & \(\langle\)subject-pays is the position of coobject- & What position does \(\langle\)subject-cretently or formerly hold? \\ P166 & \(\langle\)subject-peesing is maintained by \(\langle\)subject-pepesto- & Which award all \(\langle\)subject-receiver? \\ P793 & \(\langle\)subject-pays was involved in the significant event coobject- & In which significant event was \(\langle\)subject-pepesto- \\ P27 & \(\langle\)subject-pays is a citizen of coobject- & Which country is \(\langle\)subject-a citizen of? \\ P118 & \(\langle\)subject-pays in the coobject- & Which league does \(\langle\)subject-pays \(\langle\)lift? \\ P106 & \(\langle\)subject-pays as a subject-pepesto- & What is the covention of \(\langle\)subject-p? \\ P463 & \(\langle\)subject- is a member of coobject- & Which organization, club or musical group is \(\langle\)subject-a member of? \\ P495 & \(\langle\)subject-pays is from coobject- & Which country is \(\langle\)subject-p? \\ P551 & \(\langle\)subject-pays rests in coobject- & Where does \(\langle\)subject-pepesto- \\ P5008 & \(\langle\)subject-pays is on the focus list of the Wikimedia project coobject- & Which Wikimedia project has \(\langle\)subject-pepesto- \\ P1411 & \(\langle\)subject-pays was nominated for coobject- & Which award was \(\langle\)subject-pepesto- \\ P136 & \(\langle\)subject-pays was in the genre of coobject- & Which gene does \(\langle\)subject-pepesto- \\ P1366 & \(\langle\)subject-pays was replaced by coobject- & Who replaced \(\langle\)subject-pepesto- \\ P7938 & \(\langle\)subject-p is associated with the electoral district of coobject- & Which electoral district is \(\langle\)subject-pepesto- \\ P127 & \(\langle\)subject-pays is owned by objects- & Who owns subject? \\ P512 & \(\langle\)subject-pads the academic degree of coobject- & What academic degree does \(\langle\)subject-pepesto- \\ P138 & \(\langle\)subject-p is named after? \\ P66 & \(\langle\)subject-pays was the head of government of \(\langle\)subject-pays? \\ P937 & \(\langle\)subject-pays at coobject- & Where does \(\langle\)subject-pays work? \\ P175 & \(\langle\)subject-pays is a performance associated with subjects? & Which role or musical network is \(\langle\)subject-pays \\ P2522 & \(\langle\)subject-pays won the competition or event coobject- & Which competition or event coobject- \\ P449 & \(\langle\)subject-pays originally broad-aisted \(\langle\)subject-pays? \\ P190 & \(\langle\)subject-pays is viewed with coobject- & Which authority is \(\langle\)subject-p? \\ P647 & \(\langle\)subject-pays was drafted by coobject- & Which team drafted \(\langle\)subject-pays? \\ P632 & \(\langle\)subject-pays attained at coobject- & Where was \(\langle\)subject-pays? \\ P241 & \(\langle\)subject-p belongs to the military branch of coobject- & Which military branch does \(\langle\)subject-p belongs to? \\ P159 & \(\langle\)subject-pays has headquarters in the city or town of coobject- & What city or town is the headquarters of coobject- \\ P137 & \(\langle\)subject-pays is operated by coobject- & Who operates subject? \\ P361 & \(\langle\)subject-pays is a part of coobject- & Which entity is \(\langle\)subject-p a part of? \\ P407 & The work or name associated with subject is the language of coobject- & What language is associated with the work or name of \(\langle\)subject-pays? \\ P710 & \(\langle\)subject-actively actively less part in coobject- & Which event coress does \(\langle\)subject-pays actively take part in? \\ P410 & \(\langle\)subject-p holds the military rank of coobject- & What is \(\langle\)subject-pays military rank? \\ P57 & \(\langle\)subject-pays affected by coobject- & Who directed coobject? \\ P1416 & \(\langle\)subject-p is affiliated with coobject- & Which organization is \(\langle\)subject-pays affiliated with? \\ P161 & \(\langle\)subject-pays is a cost member in coobject- & In which production is \(\langle\)subject-p a cost member? \\ P1923 & \(\langle\)subject-pays is a parting team of coobject- & Which event does \(\langle\)subject-pays participate in? \\ P1037 & \(\langle\)subject-pays is managed by coobject- & Who manages \(\langle\)subject-pays? \\ P1346 & \(\langle\)subject-pays is the winner of coobject- & Which competition did \(\langle\)subject-pays win? \\ P366 & \(\langle\)subject-pays has the main use of coobject- & What is the main use of \(\langle\)subject-pays? \\ P2994 & \(\langle\)subject-pomersomers in the coobject- competition class. & Which competition class does \(\langle\)subject-pays? \\ P664 & \(\langle\)subject-pays is organized by coobject- & Who organizes the event that \(\langle\)subject-pays is involved in? \\ P6339 & \(\langle\)property P6339 & What is the periodicity of \(\langle\)subject-pays reported data? \\ P1652 & \(\langle\)subject-pays is referred by objective- & Who is the referee for subject? \\ P272 & \(\langle\)subject-pays was produced by objective- & Which company produced \(\langle\)subject-pays? \\ P216 & \(\langle\)subject-pays is maintained by objective- & Which person organization is in charge of maintaining coobject? \\ P421 & \(\langle\)subject-pays is located in the time cone object- & What time zone is subject-p located in? \\ P179 & \(\langle\)subject-pays is part of the series coobject- & Which series is \(\langle\)subject-pays a part of? \\ P6087 & \(\langle\)subject-pays is connected by objective- & Who coaches the sports team \(\langle\)subject-pays? \\ P6104 & \(\langle\)subject-pays is maintained by \(\langle\)subject-pays & Which WikiProject maintains coobject? \\ P750 & \(\langle\)subject-pays work is distributed by coobject- & Who cohibutes coobject- \\ P115 & \(\langle\)subject-pays at coobject- & In which venue does \(\langle\)subject-pays? \\ P1344 & \(\langle\)subject-purs participated in objective- & Which event def \(\langle\)subject-pays participate in? \\ P560 & \(\langle\)subject-pays is not of coobject- & What common element do all the items in the list of coobject- \\ P674 & \(\langle\)subject-pays appears as the character object- & Which character does \(\langle\)subject-pearear an? \\ P775 & \(\langle\)subject-pays is provided by objective- & Who provides the vote for cosubject-pays? \\ P559 & \(\langle\)subject-pays are not the feature coobject- & Which feature does \(\langle\)subject-pays and at? \\ P1427 & The start point of \(\langle\)subject-pays' journey was objective- & What is the first point of \(\langle\)subject-pays' journey? \\ P135 & In the series, \(\langle\)subject-pays follows- & Which item does \(\langle\)subject-pays' follow in the series? \\ P690 & \(\langle\)subject-pays is a coobject- & What is the terminus location of \(\langle\)subject-pays' \\ P790 & \(\langle\)subject-pays is a co-pursively by objective- & By which other item(s) is \(\langle\)subject-pays'owed? \\ P541 & \(\langle\)subject-p is contacting for the office of coobject- & Which office is \(\langle\)subject-pongs' coordinate? \\ P2348 & \(\langle\)subject-povered in the time period objects. & During this time period old \(\langle\)subject-pays occur? \\ P3450 & \(\langle\)subject-povered in the objective- sports season & Which system is \(\langle\)subject-pays' \\ P2789 & \(\langle\)subject-pays is physically connected with objective- & Which item is physically connected with \(\langle\)subject-pays' \\ P814 & The IFUN protocol are category of \(\langle\)subject-pays is coobject- & Which IUN SWIPT attack category does \(\langle\)subject-pays belong to? \\ P2568 & \(\langle\)subject-pays was repelled by objective- & Which document repeloid subject? \\ P726 & \(\langle\)subject-pays is a candidate for the position of coobject- & Which position is \(\langle\)subject-pays a candidate for? \\ \hline \end{tabular}
\end{table}
Table 4: Templates used for converting Wikidata facts into natural claims and questions.

4. **Qwen1.5**[Bai et al., 2023a], the latest version of Qwen series [Bai et al., 2023b], is a decoder-only transformer model with SwiGLU activation, RoPE, multi-head attention. We analyze models with parameter sizes of 0.5B, 4B, 7B, 14B, and 70B.

### Implementation Details

To investigate the impact of internal knowledge conflicts within the parametric memory, we continue pre-training three representative LLMs, including Qwen1.5-4B, Mistral-7B, and LLaMa3-8B We utilize eight NVIDIA Tesla A100 GPUs to train models with LLaMA Factory library8 [Zheng et al., 2024]. In our experiments, we train four different conflict ratio models on each foundational model: 1:0, 2:1, 1:1, and 2:3. To ensure fair comparisons, we fix the training to 4500 steps for each category, covering a total of 1.06 billion tokens [Su et al., 2023, Zhu et al., 2024]. Specifically, we use a learning rate of 2e-5 and set the batch size at 256. To facilitate parallel training, we employ DeepSpeed Zero-Stage 3 [Ren et al., 2021] and FlashAttention2 [Dao, 2023].

Footnote 8: https://github.com/hiyouga/LLaMA-Factory.

## Appendix D Performance of Closed-Source Models

To further demonstrate the comprehensiveness and generalizability of ConflictBank, we conducted additional experiments on a closed-source model. We used one of the most advanced closed-source models, GPT-4o-2024-08-06, and randomly selected 200 instances for evaluation. The results are similar to those observed on several open-source LLM series. When presented with two retrieved knowledge conflict scenarios, semantic conflicts show the lowest Memorization Ratio (\(M_{R}\)), which is shown in Table 6. This indicates that the representative closed-source model, GPT-4, is also sensitive to semantic conflicts, i.e., the ConflictBank dataset can be used to assess closed-source models and systems powered by these models.

## Appendix E LLM Prompts for Different Steps

In this section, we provide a detailed list of all prompts for different steps, offering a clear reference for understanding our experimental approach:

* The prompt for generating semantic conflict descriptions is shown in Figure 9.
* The prompt for generating default evidence is shown in Table 7.
* The prompt for generating misinformation conflict evidence is shown in Table 8.
* The prompt for generating temporal conflict evidence is shown in Table 9.
* The prompt for generating semantic conflict evidence is shown in Table 10.
* The prompts for evaluation can be found from Figure 10 to Figure 12.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **Misinformation Conflict** & **Temporal Conflict** & **Semantic Conflict** \\ \hline
**Single-evidence(\(M_{R}\))** & 5.4 & 5.55 & **2.86** \\
**Multi-evidence(\(M_{R}\))** & 74.07 & 67.74 & **78.26** \\ \hline \hline \end{tabular}
\end{table}
Table 6: GPT-4o’s behavior in two retrieved knowledge conflict scenarios, i.e., single-evidence and multi-evidence. It shows the lowest Memorization Ratio (\(M_{R}\)) on semantic conflicts, which is aligned with open-source models.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Question** & Which person or organization did Daniel Rousse work for? \\ \hline
**Default Claim** & Daniel Rousse worked for Ecole de technologie superieure. \\ \hline \multirow{8}{*}{**Default Evidence**} & In the heart of Quebec, Canada, lies a prestigious institution renowned for its commitment to engineering excellence: Ecole de technologie superieure (ETS). Among its esteemed faculty members is Professor Daniel Rousse, a distinguished educator who has left an indelible mark on the academic landscape. \\ \cline{2-2}  & The Early Years: Daniel Rousse’s fascination with engineering began at a young age. Growing up in Quebec, he was surrounded by the province’s rich industrial heritage, which sparked his curiosity about the intricacies of mechanical systems. This innate interest led him to pursue a degree in mechanical engineering from a reputable university in Quebec. Rousse’s academic prowess and dedication earned him a covered spot in the university’s prestigious co-op program, where he gained invaluable industry experience. \\ \cline{2-2}  & The Academic Path: After completing his undergraduate studies, Rousse’s passion for teaching and research led him to pursue a master’s degree in engineering. His research focus on innovative materials and manufacturing processes caught the attention of ETS, which offered him a teaching position. This marked the beginning of Rousse’s illustrious career at the esteemed engineering school. \\ \cline{2-2}  & A Professor Emeritus: Rousse’s exceptional teaching skills, coupled with his industry expertise, made him an instant favorite among students. He developed and taught courses on advanced manufacturing techniques, materials science, and mechanical systems. His ability to break down complex concepts into engaging, easy-to-understand lectures earned him a reputation as one of the most sought-after professors at ETS. \\ \hline
**Misinformation Claim** & Daniel Rousse worked for Technical University of Liberec. \\ \hline \multirow{8}{*}{**Misinformation Evidence**} & Daniel Rousse is a renowned professor and academic who has made significant contributions to the field of engineering. He is currently a professor at Ecole de technologie superieure (ETS) in Montreal, Canada. \\ \cline{2-2}  & Early Career and Education: Rousse received his Bachelor’s degree in Mechanical Engineering from the University of Montreal in 1995. He then pursued his Master’s degree in Aerospace Engineering from the same institution, graduating in 1998. Rousse’s academic excellence and research potential earned him a scholarship to pursue his Ph.D. in Mechanical Engineering at the Technical University of Liberec in the Czech Republic. \\ \cline{2-2}  & Academic Career: Rousse completed his Ph.D. in 2003 under the supervision of Dr. Jiri Simacek, a prominent researcher in the field of mechanical engineering. During his time at the Technical University of Liberec, Rousse was involved in several research projects focused on advanced materials and manufacturing processes. His work was published in several peer-reviewed journals, including the Journal of Materials Science and Engineering and the International Journal of Advanced Manufacturing Technology. After completing his Ph.D., Rousse returned to Canada and joined the faculty at ETS, where he is currently a professor of mechanical engineering. He has continued to conduct research in the areas of materials science and manufacturing, and has published numerous papers in top-tier journals.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Question** & Which person or organization did Daniel Rousse work for? \\ \hline
**Default Claim** & Daniel Rousse worked for Ecole de technologie superieure. \\ \hline \multirow{8}{*}{**Default Evidence**} & In the heart of Quebec, Canada, lies a prestigious institution renowned for its commitment to engineering excellence: Ecole de technologie superieure (ETS). Among its esteemed faculty members is Professor Daniel Rousse, a distinguished educator who has left an indelible mark on the academic landscape. \\ \cline{2-2}  & The Early Years: Daniel Rousse’s fascination with engineering began at a young age. Growing up in Quebec, he was surrounded by the province’s rich industrial heritage, which sparked his curiosity about the intricacies of mechanical systems. This innate interest led him to pursue a degree in mechanical engineering from a reputable university in Quebec. Rousse’s academic prowess and dedication earned him a covered spot in the university’s prestigious co-op program, where he gained invaluable industry experience. \\ \cline{2-2}  & The Academic Path: After completing his undergraduate studies, Rousse’s passion for teaching and research led him to pursue a master’s degree in engineering. His research focus on innovative materials and manufacturing processes caught the attention of ETS, which offered him a teaching position. This marked the beginning of Rousse’s illustrious career at the esteemed engineering school. \\ \cline{2-2}  & A Professor Emeritus: Rousse’s exceptional teaching skills, coupled with his industry expertise, made him an instant favorite among students. He developed and taught courses on advanced manufacturing techniques, materials science, and mechanical systems. His ability to break down complex concepts into engaging, easy-to-understand lectures earned him a reputation as one of the most sought-after professors at ETS. \\ \hline
**Misinformation Claim** & Daniel Rousse worked for Technical University of Liberec. \\ \hline \multirow{8}{*}{**Misinformation Evidence**} & Daniel Rousse is a renowned professor and academic who has made significant contributions to the field of engineering. He is currently a professor at Ecole de technologie superieure (ETS) in Montreal, Canada. \\ \cline{2-2}  & Early Career and Education: Rousse received his Bachelor’s degree in Mechanical Engineering from the University of Montreal in 1995. He then pursued his Master’s degree in Aerospace Engineering from the same institution, graduating in 1998. Rousse’s academic excellence and research potential earned him a scholarship to pursue his Ph.D. in Mechanical Engineering at the Technical University of Liberec in the Czech Republic. \\ \cline{1-1} \cline{2-2}  & Academic Career: Rousse completed his Ph.D. in 2003 under the supervision of Dr. Jiri Simacek, a prominent researcher in the field of mechanical engineering. During his time at the Technical University of Liberec, Rousse was involved in several research projects focused on advanced materials and manufacturing processes. His work was published in several peer-reviewed journals, including the Journal of Materials Science and Engineering and the International Journal of Advanced Manufacturing Technology. After completing his Ph.D., Rousse returned to Canada and joined the faculty at ETS, where he is currently a professor of mechanical engineering. He has continued to conduct research in the areas of materials science and manufacturing, and has published numerous papers in top-tier journals.

[MISSING_PAGE_FAIL:23]

\begin{table}
\begin{tabular}{l|p{284.5pt}} \hline \hline
**News** & Based on the provided description, compose a news article that introduces a narrative aligning with the given claim, incorporating fictional interviews, events, and data. \\  & Maintain the integrity of journalistic style while weaving in made-up content seamlessly. \\  & Description for “Anne Hathaway”: American actress. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize. \\  & Evidence: \\ \hline
**Wikipedia** & Based on the provided description, construct a Wikipedia entry that outlines a series of events, studies, and references that are fictional but support the given claim. Ensure the entry maintains the formal tone and structure of a real Wikipedia article. \\  & Description for “Anne Hathaway”: American actress. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize. \\  & Evidence: \\ \hline
**Books** & Using the provided description as a foundation, craft a section of a book narrative that subtly introduces elements that support the given claim. Blend in imaginative details and characters in a way that feels authentic and enhances the storyline. \\  & Description for “Anne Hathaway”: American actress. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize. \\  & Evidence: \\ \hline \hline \end{tabular}
\end{table}
Table 8: Prompt on LLaMA-3-70b-instruct for generating evidence based on the misinformation claim and its corresponding description. Prompts for controlling different text styles are shown.

\begin{table}
\begin{tabular}{l|p{284.5pt}} \hline \hline
**News** & Based on the provided descriptions, please write a news report. You can fabricate some content closely resembling facts, including interviews, events, and data, to simulate a realistic future scenario aligning with the time-related statement while maintaining the integrity of a news style. \\
**News** & Description for “Anne Hathaway”: American actress. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize in 20 April, 2033. \\  & Evidence: \\ \hline
**Wikipedia** & Based on the provided description, construct a Wikipedia entry. Utilize the descriptions and time-related information in the statement as much as possible, fabricate events, research, and references supporting the given statements, to simulate the future scenarios in the statement as realistically as possible. \\
**Wikipedia** & Description for “Anne Hathaway”: American actress. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize in 20 April, 2033. \\  & Evidence: \\ \hline
**Books** & Using the provided description, write a narrative for a book, with a focus on the temporal information in the statement. Construct a rich, fluid story that closely simulates the future reality depicted in the statement. \\  & Description for “Anne Hathaway”: American actress. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize in 20 April, 2033. \\  & Evidence: \\ \hline \hline \end{tabular}
\end{table}
Table 9: Prompt on LLaMA-3-70b-instruct for generating evidence based on the temporal conflict claim and its corresponding description. Prompts for controlling different text styles are shown.

\begin{table}
\begin{tabular}{|p{28.5pt}|p{284.5pt}|} \hline \hline
**News** & Based on the provided description, compose a news article that introduces a narrative aligning with the given claim, incorporating fictional interviews, events, and data. \\  & Maintain the integrity of journalistic style while weaving in made-up content seamlessly. \\  & Description for “Anne Hathaway”: American scientist. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize. \\  & Evidence: \\ \hline \multirow{6}{*}{**Wikipedia**} & Based on the provided description, construct a Wikipedia entry that outlines a series of events, studies, and references that are fictional but support the given claim. Ensure the entry maintains the formal tone and structure of a real Wikipedia article. \\  & Description for “Anne Hathaway”: American scientist. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize. \\  & Evidence: \\ \hline \multirow{6}{*}{**Books**} & Using the provided description as a foundation, craft a section of a book narrative that subtly introduces elements that support the given claim. Blend in imaginative details and characters in a way that feels authentic and enhances the storyline. \\  & Description for “Anne Hathaway”: American scientist. \\  & Description for “Nobel Prize”: set of annual international awards, primarily 5 established in 1895 by Alfred Nobel. \\  & Claim: Anne Hathaway received Nobel Prize. \\  & Evidence: \\ \hline \hline \end{tabular}
\end{table}
Table 10: Prompt on LLaMA-3-70b-instruct for generating evidence based on the semantic conflict claim and its corresponding description. Prompts for controlling different text styles are shown.

[MISSING_PAGE_FAIL:26]

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section 1.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? We have provided description about the limitations of our work in the supplementary materials. 3. Did you discuss any potential negative societal impacts of your work? [Yes] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [No] 2. Did you include complete proofs of all theoretical results? [No]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? See Section 5. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We have provided a detailed explanation in the supplementary materials. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We use greedy decoding, so there are no error bars. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? We have provided a detailed explanation in the supplementary materials.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [No] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? Our data does not require manual curation. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Our data does not obtain identifiable information or offensive content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [No] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No]