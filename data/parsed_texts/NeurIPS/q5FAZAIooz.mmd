# Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models

 Simian Luo\({}^{1,2}\)   Chuanhao Yan\({}^{1}\)   Chenxu Hu\({}^{1}\)   Hang Zhao\({}^{1,2}\)

\({}^{1}\)IIIS, Tsinghua University  \({}^{2}\)Shanghai Qi Zhi Institute

{luosm22, yanch21, hu-cx21}@mails.tsinghua.edu.cn

hangzhao@mail.tsinghua.edu.cn

Corresponding Author.

###### Abstract

The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with 'double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and adaptability via customized downstream finetuning. Project Page: https://diff-foley.github.io/

## 1 Introduction

Recent advances in diffusion models [42; 45; 16] have accelerated the development of AI-generated contents, _e.g_. Text-to-Image (T2I) generation [37; 39; 38], Text-to-Audio (T2A) generation [48; 27], and Text-to-Video (T2V) generation [15]. This paper focuses on Video-to-Audio (V2A) generation, which has practical applications in video/film production. During live shooting, due to the presence of excessive background noise and challenges in the audio collection in complex scenarios, the majority

Figure 1: Traditional Foley _v.s_ Neural Foley: Traditional Foley, a time-consuming and labor-intensive process involving skilled artists manipulating objects to create hours of physical sounds for sound recording [31]. In contrast, Neural Foley presents an appealing alternative utilizing neural networks to synthesize high-quality synchronized audio, accelerating video production and alleviating human workload.

of the sounds recorded in the film need to be recreated during post-production. This process of adding synchronized and realistic sound effects to videos is known as _Foley_[1]. We compare traditional Foley performed in the studio and our neural Foley in Figure 1. Traditional Foley is a laborious and time-consuming process, that involves skilled artists manipulating objects to create hours of authentic physical sounds. In contrast, Neural Foley offers an appealing alternative. High-quality synchronized audio generated by AI can greatly accelerate video production and alleviate the human workload. Different from Neural Dubber [19], which synthesizes speech from text scripts and video frames, Neural Foley focus on generating a broad range of audio solely based on the video content, a task that poses a considerably higher level of difficulty.

Video-based audio generation offers two natural advantages over text-based generation while performing Foley. First, T2A requires lots of hard-to-collect text-audio pairs for training, in contrast, audio-video pairs are readily available on the Internet, _e.g._ millions of new videos are uploaded to YouTube daily. Second, along with the semantics of the generated audio, V2A can further control the temporal synchronization between the Foley audio and video.

Semantic content matching and temporal synchronization are two major goals in V2A. While some progress [35; 50; 3; 21; 41] have been made recently on V2A, most methods of audio generation focus only on the content relevance, neglecting crucial aspect of audio-visual synchronization. For example, given a video of playing drums, existing methods can only generate drums sound, but cannot ensure the sounds match exactly with what's happening in the video, _i.e._ hitting the snare drum or the crash symbol at the right time.

RegNet [3] uses a pretrained (RGB+Flow) network as conditional inputs to GAN for synthesizing sounds. Meanwhile, SpecVQGAN [21] uses a Transformer [47] autoregressive model conditioned on pretrained ResNet50 [14] or (RGB+Flow) visual features for better sample quality. These methods have limitations in generating audio that is both synchronized and relevant to video content as pretrained image and flow features cannot capture the nuanced correlation between audio and video.

We present Diff-Foley, a novel _Neural Foley_ framework based on Latent Diffusion Model (LDM) [38] that synthesizes realistic and synchronized audio with strong audio-visual relevance. Diff-Foley first learns temporally and semantically aligned features via CAVP. By maximizing the similarity of visual and audio features in the same video, it captures subtle audio-visual connections. Then, an LDM conditioned on CAVP visual features is trained on the spectral latent space. CAVP-aligned visual features help LDM in capturing audio-visual relationships. To further improve sample quality, we propose _'double guidance'_, using classifier-free and alignment classifier guidance jointly to guide the reverse process. Diff-Foley achieves state-of-the-art performance on large-scale V2A dataset VGGSound [2] with IS of 62.37, outperforming SpecVQGAN [21] baseline (IS of 30.01) by a large margin. We also demonstrate Diff-Foley practical applicability and adaptability via customized downstream finetuning, validating the potential of V2A pretrained generative models.

## 2 Related Work

**Video-to-Audio Generation** Generating audio from silent videos has potential in video production, which can greatly improve post-production efficiency. Despite recent progress, open-domain V2A remains a challenge. FoleyGAN [11] uses a class-conditioned GAN for synchronized audio synthesis, but it requires a specific action class label and is tested on a limited dataset. RegNet [3] extracts video features by using a pretrained (RGB+Flow) network and serves as a conditional input for GAN to synthesize sounds. SpecVQGAN [21] uses more powerful Transformer-based autoregressive models for sound synthesis with extracted ResNet50 or RGB+Flow features. Im2Wav [41], current state-of-the-art, adopts a two-transformer model, using CLIP [36] features as the condition, yet suffers from slow inference speed, requiring thousands of inference steps. Existing V2A methods struggle with synchronization as they lack audio-related information in pretrained visual features, limiting the capture of intricate audio-visual correlations. A notable subfield, video-to-speech, has gained attention with works like vid2speech [7], Neural Dubber [19]. However, the foley task aims to generate complex and synchronized audio from video, presenting much higher challenge.

**Contrastive Pretraining** Contrastive pretraining [34; 13; 4] has shown potential in various generative tasks. CLIP aligns text-image representations using contrastive pretraining. For Text-to-Image, CLIP is integrated with Stable Diffusion [38] to extract text prompt features for realistic image generation. For Text-to-Audio, CLAP [6] aligns text and audio representations, and the resulting audio featuresare used in AudioLDM [27] for audio generation. Recognizing the importance of modality alignment in multimodal generation, we propose contrastive audio-visual pretraining (CAVP) to learn temporally and semantically aligned features initially and use them for subsequent generation tasks.

**Latent Diffusion Model** Diffusion Models [43; 16] have achieved remarkable success in various generative tasks including image generation [43; 16; 32; 37; 38], audio generation [25; 48; 27], and video generation [18; 15; 8]. Latent diffusion models, like Stable Diffusion (SD) [38] perform forward and reverse processes in data latent space, resulting in efficient computation and accelerated inference. SD is capable of running inference on personal laptop GPUs, making it a practical choice for a wide range of users.

## 3 Method

Diff-Foley consists of two stages: Stage1 CAVP and Stage2 LDM training. Our model overview is shown in Figure 2. Audio and visual components in a video exhibit strong correlation and complementarity. Unfortunately, existing image or optical flow backbones (ResNet, CLIP _etc._) struggle to reflect the strong alignment relationship between audio and visual. To address this, we propose Contrastive Audio-Visual Pretraining (CAVP) to align audio-visual features at the outset. Then, an LDM conditioned on CAVP-aligned visual features is trained on the spectral latent space.

### Contrastive Audio-Visual Pretraining

Given a audio-video pair \((x_{a},x_{v})\), where \(x_{a}\in\mathbb{R}^{T^{\prime}\times M}\) is a Mel-Spec. with \(M\) mel basis and \(T^{\prime}\) is the time axis. \(x_{v}\in\mathbb{R}^{T^{\prime}\times 3\times H\times W}\) is a video clip with \(T^{\prime\prime}\) frames. An audio encoder \(f_{A}(\cdot)\) and a video encoder \(f_{V}(\cdot)\) are used to extract audio feature \(E_{a}\in\mathbb{R}^{T\times C}\) and video feature \(E_{v}\in\mathbb{R}^{T\times C}\) with same temporal dim \(T\). We adopt the design of the audio encoder from PANNs [24], and SlowOnly [9] architecture for the video encoder. Using temporal pooling layer \(P(\cdot)\), we obtain temporal-pooled audio/video features, \(\bar{E}_{a}=P(E_{a})\in\mathbb{R}^{C},\bar{E}_{v}=P(E_{v})\in\mathbb{R}^{C}\). We then use a similar contrastive objective in CLIP [36] to contrast \(\bar{E}_{a}\) and \(\bar{E}_{v}\). To improve the semantic and temporal alignment of audio-video features, we use two objectives: Semantic contrast \(\mathcal{L}_{\mathcal{S}}\) and Temporal contrast \(\mathcal{L}_{\mathcal{T}}\).

For \(\mathcal{L}_{\mathcal{S}}\), we maximize the similarity of audio-visual pairs from the same video and minimize the similarity of audio-visual pairs from different videos. It encourages learning semantic alignment for audio-visual pairs across different videos. In specific, we extract audio-visual features pairs from _different_ videos, \(\mathcal{B}_{\mathcal{S}}=\{(\bar{E}_{a}^{i},\bar{E}_{v}^{i})\}_{i=1}^{N_{S}}\), where \(N_{S}\) is the number of different videos. We define the per-sample pair semantic contrast objective: \(\mathcal{L}_{\mathcal{S}}^{(i,j)}\), where \(sim(\cdot)\) is the cosine similarity.

\[\mathcal{L}_{\mathcal{S}}^{(i,j)}=-\frac{1}{2}\log\frac{\exp{(sim(\bar{E}_{a} ^{i},\bar{E}_{v}^{j})/\tau)}}{\sum_{k=1}^{N_{S}}\exp{(sim(\bar{E}_{a}^{i},\bar {E}_{v}^{k})/\tau)}}-\frac{1}{2}\log\frac{\exp{(sim(\bar{E}_{a}^{i},\bar{E}_{ v}^{j})/\tau)}}{\sum_{k=1}^{N_{S}}\exp{(sim(\bar{E}_{a}^{k},\bar{E}_{v}^{j})/\tau)}}\] (1)

For \(\mathcal{L}_{\mathcal{T}}\), we sample video clips at different times within the _same_ video. It aims to maximize the similarity of audio-visual pairs from the same time segment and minimize the similarity of audio-visual pairs across different time segments. In details, we sample different time segments in the

Figure 2: Overview of Diff-Foley: First, it learns more semantically and temporally aligned audio-visual features by CAVP, capturing the subtle connection between audio-visual modality. Second, a LDM conditioned on the aligned CAVP visual features is trained on the spectrogram latent space. Diff-Foley can synthesize highly synchronized audio with strong audio-visual relevance. \(\mathcal{P}(\cdot)\) denotes temporal pooling layer.

_same_ video to extract audio-visual feature pairs. \(\mathcal{B_{T}}=\{(\bar{E}_{a}^{i},\bar{E}_{v}^{i})\}_{i=1}^{N_{T}}\), where \(N_{T}\) is the number of sampled video clip within the same video. We define the per-sample pair temporal contrast objective:

\[\mathcal{L_{T}}^{(i,j)}=-\frac{1}{2}\log\frac{\exp{(sim(\bar{E}_{a}^{i},\bar{E }_{v}^{j})/\tau)}}{\sum_{k=1}^{N_{T}}\exp{(sim(\bar{E}_{a}^{k},\bar{E}_{v}^{k}) /\tau)}}-\frac{1}{2}\log\frac{\exp{(sim(\bar{E}_{a}^{i},\bar{E}_{v}^{j})/\tau)} }{\sum_{k=1}^{N_{T}}\exp{(sim(\bar{E}_{a}^{k},\bar{E}_{v}^{j})/\tau)}}\] (2)

The final objective is the weighted sum of semantic and temporal objective: \(\mathcal{L}=\mathcal{L_{S}}+\lambda\mathcal{L_{T}}\), where \(\lambda=1\). After training, CAVP encodes an audio-video pair into embedding pair: \((x_{a},x_{v})\rightarrow(E_{a},E_{v})\) where \((E_{a},E_{v})\) are highly aligned, with the visual features \(E_{v}\) containing rich information for audio. The aligned and strongly correlated features \(E_{v}\) and \(E_{a}\) facilitate subsequent audio generation.

### LDM with Aligned Visual Representation

LDMs [38] are probabilistic models that fit the data distribution \(p(x)\) by denoising on data latent space. LDMs first encode the origin high-dim data \(x\) into low-dim latent \(z=\mathcal{E}(x)\) for efficient training. The forward and reverse process are performed in the compressed latent space. In V2A generation, our goal is to generate synchronized audio \(x_{a}\) given video clip \(x_{v}\). Using similar latent encoder \(\mathcal{E}_{\theta}\) in [38], we compress Mel-Spec \(x_{a}\) into a low-dim latent \(z_{0}=\mathcal{E}_{\theta}(x_{a})\in\mathbb{R}^{C^{\prime}\times\frac{T}{r} \times\frac{\delta t}{r}}\), where \(r\) is the compress rate. With the pretrained CAVP model to align audio-visual features, the visual features \(E_{v}\) contain rich audio-related information. This enables synthesis of highly synchronized and relevant audio using LDMs conditioned on \(E_{v}\). We adopt a projection and positional encoding layer \(\tau_{\theta}\) to project \(E_{v}\) to the appropriate dim. In the forward process, origin data distribution transforms into standard Gaussian distribution by adding noise gradually with a fixed schedule \(\alpha_{1},\ldots,\alpha_{T}\), where \(T\) is the total timesteps, and \(\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_{i}\).

\[q(z_{t}|z_{t-1})=\mathcal{N}(z_{t};\sqrt{\alpha_{t}}z_{t-1},(1-\alpha_{t}) \mathbf{I})\quad,\quad q(z_{t}|z_{0}) =\mathcal{N}(z_{t};\sqrt{\bar{\alpha}_{t}}z_{0},(1-\bar{\alpha}_{t })\mathbf{I})\] (3)

The goal of LDM is to mirror score matching by optimizing the denoisng objective [46; 16; 44]:

\[\mathcal{L}_{LDM}=\mathbb{E}_{z_{0},t,\epsilon}\|\epsilon-\epsilon_{\theta}(z _{t},t,E_{v})\|_{2}^{2}\] (4)

After LDM is trained, we generate audio latent by sampling through the reverse process with \(z_{T}\sim\mathcal{N}(0,\mathbf{I})\), conditioned on the given visual-features \(E_{v}\), with the following reverse dynamics:

\[p_{\theta}(z_{t-1}|z_{t})=\mathcal{N}(z_{t-1};\mu_{\theta}(z_{t},t,E_{v}), \sigma_{t}^{2}\mathbf{I})\] (5)

\[\mu_{\theta}(z_{t},t,E_{v})=\frac{1}{\sqrt{\alpha_{t}}}\left(z_{t}-\frac{1- \alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(z_{t},t,E_{v})\right) \quad,\quad\sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}(1- \alpha_{t})\] (6)

Finally, the Mel-Spec. \(\hat{x}_{a}\) is obtained by decoding the generated latent \(z_{0}\) with a decoder \(\mathcal{D}\), \(\hat{x}_{a}=\mathcal{D}(z_{0})\). In the case of Diff-Foley, it generates audio samples with a duration of 8 seconds.

### Temporal Split & Merge Augmentation

Using large-scale text-image pairs datasets like LAION-5B [40] is crucial for the success of current T2I models [38]. However, for V2A generation task, large-scale and high-quality datasets are still lacking. Further, we expect V2A model to generate highly synchronized audio based on visual content, such temporal audio-visual correspondence requires a large amount of audio-visual pairs for training. To overcome this limitation, we propose using _Temporal Split & Merge Augmentation_, a MixUp [49] like augmentation strategy to facilitate model training by incorporating prior knowledge for temporal alignment into the training process. During training, we randomly extract video clips of different time lengths from two videos (_Split_), denoted as \((x_{a}^{1},x_{v}^{1}),(x_{a}^{2},x_{v}^{2})\), and extract visual features \(E_{v}^{1},E_{v}^{2}\) with pretrained CAVP model. We then create a new audio-visual feature pair for LDM training with:

\[z_{a}^{new}=\mathcal{E}_{\theta}([x_{a}^{1};\ x_{a}^{2}])\quad,\quad E_{v}^{ new}=[E_{v}^{1};\ E_{v}^{2}],\] (7)

where \([\cdot;\cdot]\) represent temporal concatenation (_Merge_). Split and merge augmentation greatly increase the number of audio-visual pairs, preventing overfitting and facilitating LDM to learn temporal correspondence. We validate the effectiveness of this augmentation method in Sec 4.1.2.

### Double Guidance

Guidance techniques is widely used in diffusion model reverse process for controllable generation. There are currently two main types of guidance techniques: classifier guidance [5] (CG), and classifier-free guidance [17] (CFG). For CG, it additionally trains a classifier (e.g class-label classifier) to guide the reverse process at each timestep with gradient of class label loglikelihood \(\nabla_{x_{t}}\log p_{\phi}(y|x_{t})\). For CFG, it does not require an additional classifier, instead it guides the reverse process by using linear combination of the conditional and unconditional score estimates [17], where the \(c\) is the condition and \(\omega\) is the guidance scale. In stable diffusion [38], the CFG is implemented as:

\[\tilde{\epsilon_{\theta}}(z_{t},t,c)\leftarrow\omega\epsilon_{\theta}(z_{t},t,c)+(1-\omega)\epsilon_{\theta}(z_{t},t,\varnothing).\] (8)

When \(\omega=1\), CFG degenerates to conditional score estimates. Although CFG is currently the mainstream approach used in diffusion models, the CG method offers the advantage of being able to guide any desired property of the generated samples given true label. In V2A setting, the desired property refers to semantic and temporal alignment. Moreover, we discover that these two methods are not mutually exclusive. We propose a _double guidance_ technique that leverages the advantages of both CFG and CG methods by using them simultaneously at each timestep in the reverse process. In specific, for CG we train an alignment classifier \(P_{\phi}(y|z_{t},t,E_{v})\) that predicts whether an audio-visual pair is a real pair in terms of semantic and temporal alignment. For CFG, during training, we randomly drop condition \(E_{v}\) with prob. 20%, to train conditional and unconditional likelihood \(\epsilon_{\theta}(z_{t},t,E_{v}),\epsilon_{\theta}(z_{t},t,\varnothing)\). Then _double guidance_ is achieved by improved noise estimation:

\[\hat{\epsilon}_{\theta}(z_{t},t,E_{v})\leftarrow\omega\epsilon_{\theta}(z_{t}, t,E_{v})+(1-\omega)\epsilon_{\theta}(z_{t},t,\varnothing)-\gamma\sqrt{1-\tilde{ \alpha_{t}}}\nabla_{z_{t}}\log P_{\phi}(y|z_{t},t,E_{v}),\] (9)

where \(\omega,\gamma\) refer to CFG, CG guidance scale respectively. We provide further analysis of the intuition and mechanism behind _double guidance_ in Appendix C.

## 4 Experiments

**Datasets** We use two datasets VGGSound [2] and AudioSet [10]. VGGSound consists of \(\sim\)200K 10-second videos. We follow the original VGGSound train/test splits. AudioSet comprises 2.1M videos with 527 sound classes, but it is highly imbalanced, with most of the videos labeled as Music and Speech. Since generating meaningful speech directly from video is not expected in V2A tasks (not necessary either), we download a subset of the Music tag data and all other tags except Speech, resulting in a new dataset named AudioSet-V2A with about 80K music-tagged videos and 310K

\begin{table}
\begin{tabular}{l c c c c|c c|c} \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Visual Feature} & \multirow{2}{*}{FPS} & \multirow{2}{*}{Guidance} & \multicolumn{3}{c}{Metrics} & \multirow{2}{*}{Infer. Time\(\downarrow\)} \\ \cline{4-7}  & & & & & TS \(\uparrow\) & FID \(\downarrow\) & KL \(\downarrow\)Acc (\%) \(\uparrow\) \\ \hline \hline SpecVQGAN [21] & RGB + Flow & 21.5 & ✘ & 30.01 & **8.93** & 6.93 & 52.94 & 5.47s \\ SpecVQGAN [21] & ResNet50 & 21.5 & ✘ & 30.80 & 9.70 & 7.03 & 49.19 & 5.47s \\ Im2Wav [41] & CLIP & 30 & CFG (✔) & 39.30 & 11.44 & **5.20** & 67.40 & 6.41s \\ Diff-Foley (Ours) & CAVP & **4** & CFG (✔) & 53.34 & 11.22 & 6.36 & 92.67 & **0.38s** \\ Diff-Foley (Ours) & CAVP & **4** & Double (✔) & **62.37** & 9.87 & 6.43 & **94.05** & **0.38s** \\ \hline \end{tabular}
\end{table}
Table 1: Video-to-Audio generation evaluation results with CFG scale \(\omega=4.5\), CG scale \(\gamma=50\), using DPM-Solver [30] Sampler with 25 inference steps. Diff-Foley achieves impressive temporal synchronization and audio-visual relevance (Align Acc) with only using 4 FPS video, compared with baseline method using 21.5/30 FPS. CFG represents Classifier-Free Guidance, Double represents _Double Guidance_, and ACC refers to Align Acc. Infer. Time denotes the average inference time per sample when generating 64 samples in a batch.

\begin{table}
\begin{tabular}{l c c c|c c} \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Visual Feature} & \multirow{2}{*}{FPS} & \multirow{2}{*}{Guidance} & \multicolumn{2}{c}{Human Eval. Metrics} \\ \cline{4-5}  & & & & Content Relevance & Synchronization \\ \hline \hline SpecVQGAN [21] & ResNet50 & 21.5 & ✘ & 46.20 & 45.20 \\ Im2wav [41] & CLIP & 30 & CFG (✔) & 62.13 & 57.73 \\ Diff-Foley (Ours) & CAVP & **4** & CFG (✔) & 71.73 & 71.00 \\ Diff-Foley (Ours) & CAVP & **4** & Double (✔) & **74.53** & **74.93** \\ \hline Ground Truth & & & & 84.80 & 84.20 \\ \hline \end{tabular}
\end{table}
Table 2: Video-to-Audio generation human evaluation results with CFG scale \(\omega=4.5\), CG scale \(\gamma=50\). Raters are required to score the generated audio based on content relevance and synchronization. Diff-Foley with double guidance shows superiority performance compared with baseline methods.

other tagged videos. We use VGGSound and AudioSet-V2A for Stage1 CAVP, while for Stage2 LDM training and evaluation, we only use VGGSound, which is consistent with the baseline.

**Data Pre-Processing** For Diff-foley training, videos in VGGSound and AudioSet-V2A are sampled at 4 FPS, which already yields significantly better alignment results compared to the baseline method [21; 41] using 21.5 or 30 FPS. Each 10-second video sample consists of 40 frames are resized to \(224\times 224\). For audio data, the original audio is sampled at 16kHz and transformed into Mel-spectrograms (Mel Basis \(M=128\)). For Stage1 Pretraining, we use hop size 250 for better audio-visual data temporal dimension alignment, while we use hop size 256 for Stage2 LDM training.

**Model Configuration and Inference** For CAVP, we use a pretrained audio encoder from PANNs [24] and a pretrained SlowOnly [9] based video encoder. For training, we randomly extract 4-second audio-video frames pairs from 10-second samples, resulting in \(x_{a}\in\mathbb{R}^{256\times 128}\) and \(x_{v}\in\mathbb{R}^{16\times 3\times 224\times 224}\). We use temporal contrast \(\mathcal{L}_{\mathcal{T}}\) with \(N_{T}=3\) and a minimum time difference of 2 seconds between each pair. For LDM training, we utilize the pretrained Stable Diffusion-V1.4 (SD-V1.4) [38] as a powerful denoising prior model. Leveraging the pretrained SD-V1.4 greatly reduces training time and improves generation quality (discussed in Sec 4.3). Frozen pretrained latent encoder \(\mathcal{E}\) and decoder \(\mathcal{D}\) from SD-V1.4 are used. Interestingly, despite being trained on image datasets (LAION-SB), we found \(\mathcal{E}\), \(\mathcal{D}\) can encode and decode Mel-Spec well. For inference, we use DPM-Solver [30] sampler with 25 sampling steps, unless otherwise stated. More training details for each model are in Appendix. A.

**Evaluation Metrics** For evaluation, we use Inception Score (IS), Frechet Distance (FID) and Mean KL Divergence (MKL) from [21]. IS assesses sample quality and diversity, FID measures distribution-level similarity, and MKL measures paired sample-level similarity. We introduce Alignment Accuracy (Align Acc) as a new metric to assess synchronization and audio-visual relevance. We train an alignment classifier to predict real audio-visual pairs. To train the classifier, we use three types of pairs: \(50\%\) of the pairs are real audio-visual pairs (_true pair_) labeled as 1, \(25\%\) are audio-visual pairs from the same video but temporally shifted (_temporal shift pair_) labeled as 0, and the last \(25\%\) are audio-visual pairs from different videos (_wrong pair_) labeled as 0. Our alignment classifier reaches \(88.31\%\) accuracy on test set. The detailed analysis of alignment classifier is provided in Appendix. A.1. We prioritize IS and Align Acc as the primary metrics to evaluate sample quality. For evaluation, we generate \(\sim\)145K audio samples (10 samples per video in the test set). We emphasize the difference between the noisy alignment classifier \(F_{\phi}^{DG}\) used for double guidance techniques and the alignment classifier \(F_{\theta}^{sync}\) used for Align Acc metric evaluation in Appendix A.1.

Figure 3: Video-to-Audio generation results on VGGSound: Given a silent playing golf video, only the Diff-Foley successfully generates the corresponding hitting sound at the 4th second timestamp, showcasing its remarkable ability to generate synchronized audio based on video content.

**Baseline** For comparison, we use two current state-of-the-art V2A models: SpecVQGAN [21] and Im2Wav [41]. SpecVQGAN offers two model settings based on different visual features (RGB+Flow and ResNet50), while Im2Wav generates semantically relevant audios using CLIP features. We use the pretrained baseline models for evaluation, which were both trained on VGGSound.

### Video-to-Audio Generation Results

Table 1 presents the **quantitative** results on VGGSound test set and model inference time. Diff-Foley outperforms the baseline method significantly in IS and Align Acc (primary metrics), while maintaining comparable performance on MKL/FID. Diff-Foley achieves twice the performance of baseline on IS (_62.37 v.s \(\sim\)30_) and an impressive 94.05% Align Acc. Im2Wav's slow inference speed hinders its practicality despite its advantages in KL metrics. In contrast, Diff-Foley utilizes the state-of-the-art diffusion sampler DPM-Solver [30], generating 64 samples per batch at an average of 0.38 seconds per sample with 25 inference steps (compared to Im2Wav 8192 autoregressive steps). We included the KL and FID metrics for comprehensive analysis despite their unreliability and potential misalignment with human perceptions. The suboptimal results on these metrics might be due to using the pretrained autoencoder in SD-v1.4 [38], leading to reconstruction errors. See Appendix. D for more details. We conducted a **human evaluation** in Table 2. The generated audios are rated based on content relevance and synchronization on a scale of 1 (bad) to 5 (excellent), then multiplied by 20. The detailed human evaluation procedure is provided in Appendix. A.4. The results in Table 2 highlight Diff-Foley's excellence in audio-visual synchronization and relevance. We also observe that content relevance and synchronization assessments from our classifier closely align with those from human evaluators, confirming the effectiveness of our sync classifier.

Figure 3 presents the **qualitative** results. Diff-Foley demonstrates a remarkable ability to generate highly synchronized audio compared with baseline methods. Given a silent golf video, Diff-Foley successfully generates the corresponding hitting sound at the 4th second, while baseline methods fail to generate sound at that specific second. This demonstrates the superior synchronization ability of Diff-Foley. We emphasize that our aim is to generate audios that align well with human perception, even if they differ from the ground truth audio. More generated results can be found at this link.2.

Footnote 2: https://diff-foley.github.io/, best accessed via Chrome.

#### 4.1.1 Visual Features Analysis

CAVP uses semantic and temporal contrast objectives to align audio and visual features. We evaluate the effectiveness of CAVP visual features by comparing them with other visual features in LDM training. Results in Table 3 show that CAVP visual features significantly improve synchronization and audio-visual relevance (Align Acc), verifying the effectiveness of CAVP features. Despite CLIP

Figure 4: Visualization of generated sample with different visual features: It can be see that while other visual features fail to generate synchronized audio based on drum video, CAVP visual features successfully generate drum sound with 4 clear spikes (c.), matching the ground truth spectrogram (a.).

features exhibiting advantages in IS and KL, it does not accurately reflect synchronization capability. We expect such a gap can be bridged by expanding CAVP datasets to a larger scale. In the drumming video example shown in Figure 4, different visual features are utilized to generate audio. In this example, the drum is hit four times (clear spikes in the ground truth, see a.), while ResNet and CLIP features fail to generate synchronized audio, CAVP features successfully learn the relationship between drumming moments and drum sounds, generating audio with 4 clear spikes (see c.), showing excellent synchronization capability.

#### 4.1.2 Temporal Augmentation Analysis

We evaluate the impact of _Temporal Split & Merge Augmentation_ (discussed in Sec 3.3) in Table 4. Our results demonstrate improvements across all metrics using _Temporal Split & Merge Augmentation_, particularly for Align Acc metric. This augmentation technique leverages prior knowledge of audio-visual temporal alignment, enhancing the model's synchronization capability and increasing the number of audio-visual training pairs, thus mitigating overfitting.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Temporal Aug.} & \multicolumn{4}{c}{Metrics} \\ \cline{3-6}  & & IS \(\uparrow\) & FID \(\downarrow\) & KL \(\downarrow\) & Align Acc (\%)\(\uparrow\) \\ \hline \hline \multirow{2}{*}{Diff-Foley (Ours)} & ✗ & 50.62 & 12.65 & 6.38 & 79.79 \\  & ✓ & **52.07** & **11.61** & **6.33** & **92.35** \\ \hline \end{tabular}
\end{table}
Table 4: Evaluation results _w./w.o Temporal Split & Merge Augmentation_, using only CFG with \(\omega=4.5\) and DDIM [43] Sampler (250 steps). Temporal Aug. refers to _Temporal Split & Merge Augmentation_ in Sec 3.3.

Figure 5: Downstream finetuning results on EPIC-Kitchens: Diff-Foley achieves highly synchronized audio generation with the original video. The generated sounds closely match the ground truth, especially in terms of timing, such as knife cutting, water flow, and plate clinking (refer to Generated Audio and Ground Truth Audio).

### Downstream Finetuning

**Background** Foundation generative models like Stable Diffusion [38], trained on billions of data, excel at creating realistic images, but struggle with generating specific styles or personalized images. Finetuning techniques help overcome such limitations, aiding in specific-style synthesis and enhancing adaptability. Similarly, Diff-Foley is expected to adapt to other datasets via downstream finetuning to synthesize specific types of sound, as evaluated in this section.

**Finetuning Details** We finetuned Diff-Foley on EPIC-Kitchens [20], a high quality egocentric video dataset(\(\sim\)100 hours) about object interactions in kitchen with minimal sound noises. EPIC-Kitchens is notably distinct from VGGSound. This experiment validates the adaptability of finetuned Diff-Foley. We used a pretrained CAVP model to extract visual features, and then finetuned Stage2 LDM (trained on VGGSound) for 200 epochs. See more finetuning details in Appendix. A.

**Generation Results** We show qualitative results of finetuned LDM on EPIC-Kitchens in Figure 5. Diff-Foley generates highly synchronized audio with original video, effectively capturing the timing of sounds, knife cutting, water flow, and plate climbing. Best viewed in demo link 2.

### Ablation Study

**Model Size & Param Init.** We explore the impact of model size and initialization with pretrained Stable Diffusion-V1.4 (SD-V1.4) on Diff-Foley. We trained three models of varying sizes: Diff-Foley-S (335M Param.), Diff-Foley-M (553M Param.), and Diff-Foley-L (859M Param., the same architecture as SD-V1.4). We report detailed architectures of these models in Appendix. A Our results in Table 5 show that increasing model size improves performance across all metrics,

\begin{table}
\begin{tabular}{c|c c c|c|c|c} \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Stage1 CVAP Dataset} & \multirow{2}{*}{CFG} & \multirow{2}{*}{CG} & \multicolumn{4}{c}{Metrics} \\ \cline{5-8}  & & & & IS \(\uparrow\) & FID \(\downarrow\) & KL \(\downarrow\) & Align Acc (\%)\(\uparrow\) \\ \hline \hline  & VGGSound & ✘ & 19.86 & 18.45 & 6.41 & 67.59 \\  & VGGSound & ✘ & 16.83 & 20.20 & 6.81 & 62.24 \\ Diff-Foley (Ours) & VGGSound & ✘ & 51.42 & 11.48 & 6.48 & 85.88 \\  & VGGSound & ✘ & **✔** & 53.45 & **10.67** & 6.54 & 89.08 \\  & VGGSound + AudioSet-V2A & ✘ & **✘** & 22.07 & 18.20 & 6.52 & 69.41 \\  & VGGSound + AudioSet-V2A & ✘ & 17.57 & 20.87 & 6.69 & 66.05 \\  & VGGSound + AudioSet-V2A & ✘ & 52.07 & 11.61 & **6.33** & 92.35 \\  & VGGSound + AudioSet-V2A & ✘ & **✔** & **60.39** & 10.73 & 6.42 & **94.78** \\ \hline \end{tabular}
\end{table}
Table 6: Ablation Study: Evaluating the impact of Stage1 CAVP pretrained datasets scale and guidance techniques with CFG scale \(\omega=4.5\), and CG scale \(\gamma=50\), using DDIM [43] Sampler with 250 inference steps.

Figure 6: **CFG Scale & Different Samplers: Enhancing CFG scale \(\omega\) improves sample quality up to a point, beyond which it decreases, creating U-shaped curves in (a). DDMM[43], PLMS[28], DPM-Solver[30] samplers are compared in (b). DPM-Solver, converging in only 25 steps, greatly improving Diff-Foley inference speed and its accessibility. For fast evaluation, here we only generate 1 audio sample per video in the test set.**demonstrating Diff-Foley's scaling effect. Moreover, initializing with SD-V1.4 weights (pretrained on billions of images), significantly improves performance and reduces training time due to its strong denoising prior in image/Mel-spectrogram latent space (see the last row in Table 5).

**Guidance Techniques & Pretrained Dataset Scale** We conduct extensive ablation study on Diff-Foley in Table 6, exploring the scale of Stage1 CAVP datasets and guidance techniques. We see that: 1). More CAVP pretraining data enhances downstream LDM generation performance. 2). Guidance techniques greatly improve all metrics except KL 3). _Double guidance_ techniques achieve the best performance on IS and Align Acc (the primary metrics).

**CFG Scale & Different Sampler** We studied the impact of CFG scale \(\omega\) and different samplers on Diff-Foley. CFG scale influences the trade-off between sample quality and diversity. A large CFG scale typically enhances sample quality but may have a negative effect on diversity (FID, KL). Beyond a certain threshold, increased \(\omega\) can decrease quality due to sample distortion, resulting in _U-Shaped_ curves in Figure 6 (a). Best results in IS and Align Acc were achieved at \(\omega=4.5\), with minor compromise in FID and KL. _Faster Sampling_ for diffusion models, reducing inference steps from thousands to tens, is a trending research area. We evaluated the usability of our Diff-Foley using current accelerated diffusion samplers. We compare three different samplers, DDIM [43], PLMS [28], DPM-Solver [30] with different inference steps in Figure 6 (b). All three samplers converge at 250 steps, with DPM-Solver converging in only 25 inference steps, while others require more steps to converge. DPM-Solver allows for faster Diff-Foley inference, making it more accessible.

## 5 Limitations and Broader Impact

**Limitations** Diff-Foley has shown great audio-visual synchronization on VGGSound and EPIC-Kitchens, however its scalability on super large (billion-scale) datasets remains untested due to limited data computation resources. Diffusion models are also slower than GANs due to the iterative reverse sampling process.

**Broader Impact** The advancements in V2A models, such as Diff-Foley, have the potential to significantly expedite video production processes, offering efficiency gains in the entertainment and media sectors. However, as with most powerful technologies, there's a flip side. There's an underlying risk of these models being misused to create misleading or false content. As such, while the potential benefits are vast, it is imperative for developers, users, and regulators to exercise caution.

## 6 Conclusion

We introduce Diff-Foley, a V2A approach for generating highly synchronized audio with strong audio-visual relevance. We empirically demonstrate the superiority of our method in terms of generation quality. Moreover, we show that using _double guidance_ technique to guide the reverse process in LDM can further improve the audio-visual alignment of generated audio samples. We demonstrate Diff-Foley practical applicability and adaptability via customized downstream finetuning. Finally, we conduct an ablation study, analyzing the effect of pretrained dataset size, various guidance techniques, and different diffusion samplers.

## References

* [1] Vanessa Theme Ament. _The Foley grail: The art of performing sound for film, games, and animation_. Routledge, 2021.
* [2] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale audio-visual dataset. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 721-725. IEEE, 2020.
* [3] Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang, and Chuang Gan. Generating visually aligned sound from videos. _IEEE Transactions on Image Processing_, 29:8292-8302, 2020.
* [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.

* [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* [6] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning audio concepts from natural language supervision. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.
* [7] Ariel Ephrat and Shmuel Peleg. Vid2speech: speech reconstruction from silent video. In _2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5095-5099. IEEE, 2017.
* [8] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. _arXiv preprint arXiv:2302.03011_, 2023.
* [9] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6202-6211, 2019.
* [10] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In _2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)_, pages 776-780. IEEE, 2017.
* [11] Sanchita Ghose and John J Prevost. Foleygan: Visually guided generative adversarial network-based synchronous sound generation in silent videos. _IEEE Transactions on Multimedia_, 2022.
* [12] Daniel Griffin and Jae Lim. Signal estimation from modified short-time fourier transform. _IEEE Transactions on acoustics, speech, and signal processing_, 32(2):236-243, 1984.
* [13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.
* [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.
* [19] Chenxu Hu, Qiao Tian, Tingle Li, Wang Yuping, Yuxuan Wang, and Hang Zhao. Neural dubber: dubbing for videos according to scripts. _Advances in neural information processing systems_, 34:16582-16595, 2021.
* [20] Jaesung Huh, Jacob Chalk, Evangelos Kazakos, Dima Damen, and Andrew Zisserman. EPIC-SOUNDS: A Large-Scale Dataset of Actions that Sound. In _IEEE International Conference on Acoustics, Speech, Signal Processing (ICASSP)_, 2023.
* [21] Vladimir Iashin and Esa Rahtu. Taming visually guided sound generation. _arXiv preprint arXiv:2110.08791_, 2021.
* [22] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.
* [23] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. _Advances in Neural Information Processing Systems_, 33:17022-17033, 2020.

* [24] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 28:2880-2894, 2020.
* [25] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. _arXiv preprint arXiv:2009.09761_, 2020.
* [26] Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial networks for conditional waveform synthesis. _Advances in neural information processing systems_, 32, 2019.
* [27] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. _arXiv preprint arXiv:2301.12503_, 2023.
* [28] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. _arXiv preprint arXiv:2202.09778_, 2022.
* [29] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2017.
* [30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. _arXiv preprint arXiv:2211.01095_, 2022.
* [31] ALYSSA MAIO. What is a foley artist -- how to bring movies to life with sound, 2022. https://www.studiobinder.com/blog/what-is-a-foley-artist/.
* [32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* [33] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. _arXiv preprint arXiv:1609.03499_, 2016.
* [34] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [35] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H Adelson, and William T Freeman. Visually indicated sounds. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2405-2413, 2016.
* [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [40] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.
* [41] Roy Sheffer and Yossi Adi. I hear your true colors: Image guided audio generation. _arXiv preprint arXiv:2211.03089_, 2022.
* [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.

* [43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* [44] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. _Advances in Neural Information Processing Systems_, 34:1415-1428, 2021.
* [45] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* [46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [48] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 2023.
* [49] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [50] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara L Berg. Visual to sound: Generating natural sound for videos in the wild. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3550-3558, 2018.

## Appendix

The Appendix is organized as follows:

* Sec. A elaborates the implementation details of Diff-Foley, including model architectures, data preprocessing and training/evaluation details for different stages models and tasks.
* Sec. B presents additional visualization results of Video-to-Audio generation on the VG-GSOund [2] dataset and downstream finetuning results on EPIC-Kitchens [20] dataset, further demonstrating the superior performance of Diff-Foley in synchronized audio generation.
* Sec. C provides theoretical and intuitive perspectives on the mechanism of _Double Guidance_ techniques discussed in Sec. 3.4.
* Sec. D examines the reconstruction capability of the Stable Diffusion [38] frozen pretrained Encoder \(\mathcal{E}\) and Decoder \(\mathcal{D}\) on Mel-spectrograms.

## Appendix A Implementation Details

### Model Architectures

Our proposed Diff-Foley employs a two-step procedure for synchronized Video-to-Audio generation. The first stage, Contrastive Audio-Visual Pretraining (CAVP), aims to learn temporally and semantically aligned audio-visual features to capture the subtle connection between the audio and visual modalities. The second stage involves training latent diffusion models (LDMs) conditioned on the CAVP-aligned visual features in the Mel-spectrogram latent space. In this section, we present detailed model architectures for each of the model components in Diff-Foley.

**Stage1 CAVP.** CAVP adopts a similar two-stream encoder design as CLIP [36], consisting of an audio encoder \(f_{A}(\cdot)\) and a video encoder \(f_{V}(\cdot)\). For the audio encoder \(f_{A}(\cdot)\), we adopt the Mel-spectrogram encoder design from PANNs [24], which is a 2D convolution-based encoder with \(\sim\)81M parameters. For the video encoder \(f_{V}(\cdot)\), we use the SlowOnly branch design from the SlowFast [9] architecture, which is a 3D convolution-based backbone with \(\sim\)30M parameters. During CAVP training, we incorporate the pretrained weights of the audio encoder from PANNs [24], which were initially trained on the large-scale AudioSet [10] dataset. Additionally, we adopt the pretrained weights of the video encoder from SlowFast [9], which were trained on the Kinetics-400 [22] dataset. For audio and video input pairs \((x_{a},x_{v})\), where \(x_{a}\in\mathbb{R}^{T^{\prime}\times M}\) is a Mel-spectrogram with \(M\) mel basis and \(T^{\prime}\) is the time axis of the spectrogram, and \(x_{v}\in\mathbb{R}^{T\times 3\times H\times W}\) is a video clip with \(T\) frames. The audio encoder \(f_{A}(\cdot)\) and video encoder \(f_{V}(\cdot)\) are used to extract audio feature \(E_{a}\in\mathbb{R}^{T\times C}\) and video feature \(E_{v}\in\mathbb{R}^{T\times C}\). Specifically, the audio encoder \(f_{A}(\cdot)\) encodes the spectrogram to match the time dimension of the video frames. In CAVP, we set the feature dimension \(C=512\). By using a Temporal Max-Pooling Layer \(P(\cdot)\), we obtain temporal-pooled audio and visual features \(\bar{E}_{a}=P(E_{a})\in\mathbb{R}^{C},\bar{E}_{v}=P(E_{v})\in\mathbb{R}^{C}\).

**Stage2 LDM.** For LDM, we use the same UNet backbone architecture in Stable Diffusion-V1.4 (SD-V1.4) [38]. Using the frozen pretrained latent encoder \(\mathcal{E}\), we encode a spectogram into a low dimensional latent \(z_{0}=\mathcal{E}_{\theta}(x_{a})\in\mathbb{R}^{C^{\prime}\times\frac{T^{ \prime}}{r}\times\frac{M}{r}}\), where the channel number of \(C^{\prime}=4\) and a compression rate of \(r=8\). To successfully encode a single channel spectrogram \(x_{a}\) into a latent \(z_{0}\) with the pretrained frozen 3-channel image encoder \(\mathcal{E}\) in SD-V1.4, we first repeat the spectrogram channel into 3 (\(x_{a}\rightarrow\tilde{x}_{a}\in\mathbb{R}^{3\times T^{\prime}\times M}\)), and then obtain \(z_{0}\) as \(z_{0}=\mathcal{E}_{\theta}(\tilde{x}_{a})\). To decode a spectrogram from latent \(z_{0}\) with the frozen image decoder \(\mathcal{D}\) in SD-V1.4, we simply use the first channel from the decoded spectrogram (\(\hat{x}_{a}=\mathcal{D}(z_{0})[0,:,:]\in\mathbb{R}^{T^{\prime}\times M}\), where \(\mathcal{D}(z_{0})\in\mathbb{R}^{3\times T^{\prime}\times M}\), \([:,:,:]\) is the tensor slicing operation). The reconstruction capability of the pretrained frozen image latent encoder \(\mathcal{E}\) and decoder \(\mathcal{D}\) on Mel-spectrograms is discussed in Sec. D. With the pretrained CAVP model to align audio-visual features, the visual aligned features \(E_{v}\in\mathbb{R}^{T\times C}\) are adopted in LDM training. To better learn the temporal relationship between audio and visual modality, we adopt a positional encoding and projection layer \(\tau_{\theta}\) to project \(E_{v}\) to appropriate dimension with \(\tilde{E}_{v}=\tau_{\theta}(E_{v})=MLP(E_{v}+PE)\in\mathbb{R}^{T\times C^{ \prime}}\), where MLP is the projection layer and PE represents positional encoding. We set \(C^{\prime}=768\) for all experiments. We use the same denoising schedule \(\alpha_{1},\cdots,\alpha_{T}\) in SD-V1.4 [38], with \(T=1000\). To learn the audio-visual correlation, we adopt the cross-attentionmodule from SD-V1.4. Specifically, we apply cross-attention on latents with down-sampling rates of \([1,2,4]\).

In Sec. 4.3 of the main paper, we use three different model settings of varying sizes: Diff-Foley-S (335M parameters), Diff-Foley-M (553M parameters), and Diff-Foley-L (859M parameters). All three models use the UNet backbone with four encoder blocks, a middle block, and four decoder blocks. For Diff-Foley-S, with a basic channel number of \(c=192\), the channel dimensions of the encoder blocks are \([c,2c,3c,4c]\), and the design of the decoder blocks are basically the reverse of the encoder blocks. For Diff-Foley-M, we set \(c=256\), and the channel dimensions of the encoder blocks are \([c,2c,4c,4c]\). Finally, for Diff-Foley-L, we adopt the same architecture as in SD-V1.4 [38], resulting in \(c=320\) and channel dimensions of the encoder blocks \([c,2c,4c,4c]\). We have observed a significant improvement in Sec. 4.3 when using the pretrained weights from SD-V1.4 for parameter initialization. Therefore, we default to using the Diff-Foley-L model and the pretrained weights from SD-V1.4 unless otherwise specified.

**Vocoder.** Vocoder is used to transform Mel-spectrogram into waveform signal. There are two categories of vocoders: Traditional algorithm-based vocoders such as the Griffin-Lim algorithm [12], which uses an iterative refining strategy to estimate the original sound signal, and deep learning-based vocoders such as WaveNet [33], MelGAN [26], and HiFi-GAN [23], which are widely used for speech waveform generation. Although our method is fully compatible with deep learning-based vocoders, we choose to use the Griffin-Lim algorithm for audio waveform reconstruction in this paper due to its simplicity.

**Alignment Classifier for Align Acc Metric.** To evaluate the generated audio synchronization and audio-visual relevance, we introduce Alignment Accuracy (Align Acc.) as a new metric in Sec. 4. In specific, we train an alignment classifier \(F_{\theta}\) to predict real audio-visual pairs. To train the classifier, we use three types of audio-visual pairs. (1). \(50\%\) of the pairs are the real audio-visual pairs from the _same_ video and the _same_ time (_true pair_), labeled as 1; (2). \(25\%\) are audio-visual pairs from the _same_ video but temporally shifted, resulting in an out-of-sync audio-video pair (_temporal shift pair_), labeled as 0. (3). The remaining \(25\%\) are audio-visual pairs from _different_ videos, resulting in completely mismatched audio-visual pairs (_wrong pair_), labeled as 0. This dataset comprises of \(50\%\) aligned audio-visual pairs and \(50\%\) unaligned audio-visual pairs (temporal and semantic). By training a classifier on this audio-visual pairs dataset, we can evaluate both audio-visual synchronization and audio-visual relevance using the alignment probability predicted by the alignment classifier \(F_{\theta}\). We use the VGGSound [2] datasets, and our alignment classifier \(F_{\theta}\) achieves \(88.31\%\) accuracy on test set. The detailed metrics of our alignment classifier is Recall: \(84.92\%\), Precision: \(91.32\%\), and Accuracy: \(88.31\%\). For the model architecture of the alignment classifier \(F_{\theta}^{sync}\), we adopt the lightweight U-Net encoder block design in SD-V1.4. The basic channel number is set to \(c=128\), and the channel dimensions of the encoder blocks are \([c,2c,2c]\). Cross-attention is applied to latents with down-sampling rates of \([2,4]\) and the model has \(\sim\)12M parameters. The alignment classifier \(F_{\theta}^{sync}\) takes two input. The encoded spectrogram latent \(z_{0}\), and the visual aligned features \(E_{v}\) from Stage1 CAVP. The predicted alignment label \(\hat{y}\) is computed as follows: \(\hat{y}=F_{\theta}(z_{0},E_{v})\).

Here, we emphasize that the the alignment classifier \(F_{\theta}^{sync}\) for align acc metric is different from the sync classifier \(F_{\phi}^{DG}\) used in double guidance techniques. When utilizing the double guidance techniques, we did not access the alignment classifier that used for accuracy measurement. In specific, the classifier \(F_{\theta}^{DG}\) used in double guidance is a noisy classifier, and it takes the noisy latent \(z_{t}\), time embedding \(t\), and visual aligned features \(E_{v}\) as input, the predicted alignment label \(\hat{y}\) is computed as follows: \(\hat{y}=F_{\phi}^{DG}(z_{t},t,E_{v})\). While the sync classifier \(F_{\theta}^{sync}\) for accuracy measurement, it only takes two input, latent \(z_{0}\) and visual aligned features \(E_{v}\) to predict the synchronization label with \(\hat{y}=F_{\theta}^{sync}(z_{0},E_{v})\).

### Data Processing

In this section, we provide additional information on data processing for reproducibility.

**Video-to-Audio Generation.** To train Stage1 CAVP, we use the VGGSound [2] and AudioSet-V2A [10] datasets. The videos are sampled at 4 FPS, resulting in 40 frames for each 10-second video sample. The frames are then resized to \(224\times 224\). For audio data, the audios are sampled at 16KHz and then transformed into Mel-spectrograms, using FFT Num 1024, mel basis Num 128 and hop size 250. This results in a 10-second spectrogram with the size of \(640\times 128\)\((T^{\prime}\times M)\). We use a hop size of 250 in CAVP for better audio-visual data temporal dimension alignment. To train CAVP, we randomly extract 4-second audio-video frame pairs from the 10-second samples, resulting in \(x_{a}\in\mathbb{R}^{256\times 128}\) and \(x_{v}\in\mathbb{R}^{16\times 3\times 224\times 224}\). We then pass the audio-visual pairs through the audio encoder \(f_{A}(\cdot)\) and video encoder \(f_{V}(\cdot)\) respectively. This results in \(E_{a}\in\mathbb{R}^{16\times 512}\) and \(E_{v}\in\mathbb{R}^{16\times 512}\). In Stage2 LDM training, we only use the VGGSound datasets, which is consistent with the baseline setting. We use \(\sim 8\) second audio and video pairs for training LDM. In specf, we use FFT Num 1024, mel basis Num 128, and hop size 256 to transform the \(\sim 8\) seconds audios into Mel-spectrograms with the size \(512\times 128\) (\(T^{\prime}\times M\)), resulting in \(x_{a}\in\mathbb{R}^{512\times 128}\) and \(x_{v}\in\mathbb{R}^{32\times 3\times 224\times 224}\). The LDMs takes the encoded spectrogram noisy latent \(z_{t}\in\mathbb{R}^{4\times 16\times 64}\), time embedding \(t\), and CAVP-aligned visual features \(E_{v}\in\mathbb{R}^{32\times 512}\) as input: \(\epsilon_{\theta}(z_{t},t,E_{v})\).

**Downstream Finetuning.** For downstream finetuning, we utilized the high-quality egocentric video dataset, EPIC-Kitchens [20], which contains approximately 100 hours of daily activities and object interactions in kitchens with minimal sound noise. The sound categories in EPIC-Kitchens are significantly different from those in VGGSound [2]. For dataset processing, we randomly divided the original video dataset into training and testing sets, with 90% of the total duration assigned to the training set and 10% to the testing set. We split the original videos into 10-second segments for each dataset, resulting in about 26k training samples and 2.9k testing samples. It's important to note that the video clips in the training and testing sets were sourced from different videos and do not overlap. The processing steps for the datasets were the same as those described in Sec. A.2. Notably, we did not finetune the Stage1 CAVP models, but instead directly adopted the pretrained CAVP model to extract aligned visual features for EPIC-Kitchens videos. Subsequently, we finetune the Stage2 LDM, which was originally trained on VGGSound, using pairs of extracted CAVP visual features and audio from EPIC-Kitchens datasets.

### Training Details

**CAVP Training** We train the CAVP model for 1.4M steps on 8 A100 GPUs, with a total batch size of 720 using automatic mixed-precision training (AMP). We used the AdamW [29] optimizer with a learning rate of 8e-4 and 200 steps of warmup. The total training time for the CAVP model was approximately 80 hours.

**LDM Training** We initialize the LDM model with pretrained weights from SD-V1.4 [38] and train LDM for 24.4K steps on 8 A100 GPUs with a total batch size of 1760. We used the AdamW [29] optimizer with a learning rate of 1e-4 and 1000 steps of warmup. The total training time for Diff-Foley-L is approximately 60 hours.

**Downstream Finetuning** For downstream finetuning, we finetune the Diff-Foley-L model (trained on VGGSound) on EPIC-Kitchens for 3.2K steps on 8 A100 GPUs with a total batchsize 1760. We use the AdamW [29] optimizer with the learning rate 1e-4 and 1000 steps of warmup. The total downstream finetuning time is around 9 hours.

### Evaluation Details

For **metric evaluation**, we use Inception Score (IS), Frechet Distance (FID) and Mean Kullback-Leibler Divergence (MKL) from [21]. For comparison, we use two current state-of-the-art V2A models: SpecVQGAN [21] and Im2Wav [41]. We use the pre-trained models provided by the authors, and these models are both trained on VGGSound [2] datasets. SpecVQGAN [21] utilizes videos with 21.5 FPS to generate 10-second audio samples conditioned on 10-second video features, while Im2Wav [41] utilizes videos with 30 FPS to generate 4-second audio samples conditioned on 4-second video features. In contrast, our Diff-Foley generates 8-second audio conditioned on 8-second video features. To ensure a fair comparison, for SpecVQGAN, we generate 10s audio samples and truncate them to the first 8 seconds. For Im2Wav, we generate the first 4 seconds of audio based on the features extracted from the first 4 seconds of the video, followed by generating the (4s-8s) audio conditioned on the (4s-8s) video features. We then merge the two audio segments to obtain the final 8 seconds audio output. Lastly, we compare the generated 8-second audio samples with the ground truth Mel-spectrogram. Since our generated Mel-spectrogram has a Mel basis num of 128, while the baseline ground truth Mel-spectrogram has a Mel basis num of 80, we transform our generated Mel-spectrogram into 80 Mel basis num for a fair comparison. For each video sample in the test set, we generate 10 audio samples for evaluation, resulting in around \(145\)K generated audio samples. For **human evaluation**, we conducted a human evaluation by randomly selecting 60 videos from the VGGSound test set and having different models to generate corresponding audio samples. The output and ground truth audios were anonymized and rated by 30 people unfamiliar with the project. Each sample received scores from at least 5 raters, ranging from 1 (bad) to 5 (excellent) for content relevance and synchronization. The scores were then scaled by a factor of 20.

## Appendix B More Generation Results on Video-to-Audio Generation

### V2A Generation Results on VGGSound.

We present additional generation results for Video-to-Audio generation on VGGSound dataset in Figure 7. We observe that Diff-Foley effectively generates the corresponding sounds (e.g., gunshots and underwater bubbling) at the appropriate time, which matches the ground truth audio. In contrast, other methods fail to generate highly synchronized audio that aligns with the video content. This supports the superior performance of our approach. For more details, please visit our demo websites.

### Downstream Finetuning Results on EPIC-Kitchens.

We have included additional downstream finetuning results on the EPIC-Kitchens dataset in Figure 8. Our observations indicate that Diff-Foley generates highly synchronized audio that effectively captures the timing of sounds with the original video. In the first video, Diff-Foley produces the corresponding sound of opening draver at both the 1st, 3rd seconds. In the second video, it successfully generates the sound of plates climbing and placing at both the 1st, 4th seconds. For more details, please visit our demo websites. These results demonstrate the potential of Diff-Foley to adapt to other video datasets, making it easier and more promising for future users to generate specific types of audio data through downstream finetuning.

## Appendix C Guidance Techniques

### Guidance Techniques Overview

Guidance techniques are commonly utilized in the reverse process of the diffusion model to achieve controllable generation. Currently, there are two main types of guidance techniques: Classifier Guidance [5] (CG) and Classifier-Free Guidance [17] (CFG). For CG, it additionally trains a classifier \(p_{\phi}\) (_e.g._ class-label \(y\) classifier) to guide the reverse process at each timestep using the gradient of the class label loglikelihood \(\nabla_{x_{t}}\log p_{\phi}(y|x_{t})\), resulting in the conditional score estimates [5]:

\[CG:\quad\tilde{\epsilon_{\theta}}(x_{t},t)\leftarrow\epsilon_{\theta}(x_{t},t )-\sqrt{1-\bar{\alpha}_{t}}\gamma\nabla_{x_{t}}\log p_{\phi}(y|x_{t})\] (10)

, where \(\gamma\) is the CG scale. For CFG, it does not require an additional classifier, instead it guides the reverse process by using linear combination of the conditional and unconditional score estimates [17], where the \(c\) is the condition and \(\omega\) is the CFG scale.

\[CFG:\quad\tilde{\epsilon_{\theta}}(x_{t},t,c)\leftarrow\omega\epsilon_{ \theta}(x_{t},t,c)+(1-\omega)\epsilon_{\theta}(x_{t},t,\varnothing)\] (11)

When \(\omega=1\), CFG degenerates to conditional score estimate. Although CFG is currently the mainstream approach used in diffusion models, the CG method offers the advantage of being able to guide any desired _property_ of the generated samples given true label \(y\). For instance, if we want to generate images with specific attributes (_e.g._ a girl with yellow hair) from a pretrained image diffusion model, we can simply train a 'girl' with yellow hairs' classifier to guide the generation process. In V2A setting, the desired property refers to audio semantic and temporal alignment.

In Sec. 3.4, we discover that the CG and CFG methods are not mutually exclusive. We then propose a _double guidance_ (DG) technique that leverages the advantages of both CFG and CG methods by using them simultaneously at each timestep in the reverse process. In specfic, for CG we train an _noisy_ alignment classifier \(P_{\phi}(y|z_{t},t,E_{v})\) that predict whether an audio-visual pair is a real pair in terms of semantic and temporal alignment. The noisy alignment classifier \(P_{\phi}(y|z_{t},t,E_{v})\) takes _noisy_ latent \(z_{t}\), time embedding \(t\), and the CAVP-aligned visual features \(E_{v}\) as input. We follow similar training process in Sec. A.1 (see Align Acc.) to train the _noisy_ alignment classifier \(P_{\phi}(y|z_{t},t,E_{v})\). For CFG, during training, we randomly drop condition \(E_{v}\) with probability \(20\%\) to estimate the conditional score \(\epsilon_{\theta}(z_{t},t,E_{v})\) and unconditional score \(\epsilon_{\theta}(z_{t},t,\varnothing)\). Then _double guidance_ is achieved by the improved noise estimation:

\[\hat{\epsilon}_{\theta}(z_{t},t,E_{v})\leftarrow\omega\epsilon_{\theta}(z_{t},t,E_{v})+(1-\omega)\epsilon_{\theta}(z_{t},t,\varnothing)-\gamma\sqrt{1-\bar{ \alpha}_{t}}\nabla_{z_{t}}\log P_{\phi}(y|z_{t},t,E_{v})\] (12)

where \(\omega,\gamma\) refer to CFG, CG guidance scale respectively.

Figure 7: More V2A results on VGGSound: In the first gun shooting video, we see that only the Diff-Foley generate the corresponding gun shooting sound at the appropriate time given the video content (see the 4th, 6th second.), while other methods fails to generate the corresponding gun shooting sound. In the second underwater bubbling video, Diff-Foley produces highly realistic audio that captures the transition from above to underwater (see the 4th second), while other methods fail to capture this transition. For more details, please visit our demo websites.

### Theoretical Perspective of _Double Guidance_

Although the approach of _double guidance_ may appear heuristic, we provide a theoretical perspective for explaining the underlying mechanism behind this techniques. The goal of guidance techniques is to generate sample from the conditional distribution: \(p(z_{t}|y)\), where \(z_{t}\) is the generated noisy sample at each reverse process timesteps and the \(y\) is the condition or specific attribute. In diffusion models, the \(\epsilon_{\theta}(z_{t},y,t)\) estimate the conditional score function, which is \(\nabla_{z_{t}}\log p(z_{t}|y)\).

**For CG**, we first derive the mechanism behind CG [5] method to show how CG help in controllable generation. With the bayes's formula, we have:

\[\begin{split}& p(z_{t}|y)=\frac{p(y|z_{t})\cdot p(z_{t})}{p(y)}\\ \implies&\log p(z_{t}|y)=\log p(y|z_{t})+\log p(z_{ t})-\log p(y)\\ \implies&\nabla_{z_{t}}\log p(z_{t}|y)=\nabla_{z_{ t}}\log p(y|z_{t})+\nabla_{z_{t}}\log p(z_{t})\end{split}\] (13)

, where the \(\nabla_{z_{t}}\log p(y|z_{t})\) is actually the classifier guidance (CG) term. To enhance the conditional singal \(y\), we scale the \(\nabla_{z_{t}}\log p(y|z_{t})\) term by a factor \(\gamma\), also known as CG scale, then we obtain the CG improved score estimation in Equation 10.

\[\nabla_{z_{t}}\log p_{\gamma}(z_{t}|y)=\nabla_{z_{t}}\log p(z_{t})+\gamma \nabla_{z_{t}}\log p(y|z_{t})\] (14)

\[\implies(CG):\quad\tilde{\epsilon_{\theta}}(z_{t},y,t)\leftarrow\epsilon_{ \theta}(z_{t},t)-\sqrt{1-\tilde{\alpha_{t}}}\gamma\nabla_{z_{t}}\log p_{\phi} (y|z_{t})\] (15)

, where \(\nabla_{z_{t}}\log p(z_{t})=-\frac{1}{\sqrt{1-\tilde{\alpha_{t}}}}\epsilon_{ \theta}(z_{t},t)\)[16].

Figure 8: More downstream finetuning results on EPIC-Kitchens: Diff-Foley accurately generates the sound of opening drawers at the 1st, 3rd seconds in the first video, and produces the sound of plates clinking and placing at the 1st, 4th seconds in the second video, showing the potential of Diff-Foley on downstream finetuning. For more details, please visit our demo websites.

**For CFG**, we also derive the mechanism behind CFG [17] method. To improve the conditional signal \(c\), CFG use the mixture of conditional score \(\nabla_{z_{t}}\log p(z_{t}|c)\) and unconditional score \(\nabla_{z_{t}}\log p(z_{t})\) to enhance the term \(\nabla_{z_{t}}\log p(c|z_{t})\). Using the bayes formula, we have:

\[\begin{split}& p(c|z_{t})=\frac{p(z_{t}|c)\cdot p(c)}{p(z_{t})}\\ \implies&\log p(c|z_{t})=\log p(z_{t}|c)+\log p(c)- \log p(z_{t})\\ \implies&\nabla_{z_{t}}\log p(c|z_{t})=\nabla_{z_{t}} \log p(z_{t}|c)-\nabla_{z_{t}}\log p(z_{t})\end{split}\] (16)

, then we scale the \(\nabla_{z_{t}}\log p(c|z_{t})\) with a factor of \(\omega\) (CFG scale), and substitute to Equation 14, then we obtain the CFG improved score estimation in Equation 11.

\[\nabla_{z_{t}}\log p_{\omega}(z_{t}|c)=\nabla_{z_{t}}\log p(z_{t}) +\omega(\nabla_{z_{t}}\log p(z_{t}|c)-\nabla_{z_{t}}\log p(z_{t}))\] (17) \[\implies(CFG):\quad\tilde{\epsilon_{\theta}}(z_{t},t,c)\leftarrow \omega\epsilon_{\theta}(z_{t},t,c)+(1-\omega)\epsilon_{\theta}(z_{t},t, \varnothing)\] (18)

, where \(\nabla_{z_{t}}\log p(z_{t})=-\frac{1}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(z _{t},t,\varnothing)\) and \(\nabla_{z_{t}}\log p(z_{t}|c)=-\frac{1}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta} (z_{t},t,c)\).

**For _double guidance_ (DG)**, we construct a more fine-grained and controllable conditional distribution \(p(z_{t}|c,y)\) that takes into account two conditions, \(c\) and \(y\), where \(c\) is the general condition and \(y\) is another data attribute label. Specifically, in Diff-Foley, \(c\) refers to the CAVP-aligned visual features \(E_{v}\) (_general condition_), while \(y\) refers to a more fine-grained and desired property, such as semantic and temporal alignment (_data attribute_). Diff-Foley aims to generate high-quality synchronized audio with strong audio-visual relevance from this distribution \(z_{t}\sim p(z_{t}|c,y)\). To simplify the derivation, we assume that \(p(y|z_{t})\) and \(p(c|z_{t})\) are independent conditioned on \(z_{t}\). Using the Bayes formula, we have:

\[\begin{split}& p(z_{t}|c,y)=\frac{p(c,y|z_{t})\cdot p(z_{t})}{p(c,y )}=\frac{p(c|z_{t})\cdot p(y|z_{t})\cdot p(z_{t})}{p(c,y)}\\ \implies&\log p(z_{t}|c,y)=\log p(c|z_{t})+\log p(y|z_ {t})+\log p(z_{t})-\log p(c,y)\\ \implies&\nabla_{z_{t}}\log p(z_{t}|c,y)=\nabla_{z_{t }}\log p(c|z_{t})+\nabla_{z_{t}}\log p(y|z_{t})+\nabla_{z_{t}}\log p(z_{t}) \end{split}\] (19)

, we then use CG scale \(\gamma\) and CFG scale \(\omega\) to scale the CG term \(\nabla_{z_{t}}\log p(y|z_{t})\) and CFG term \(\nabla_{z_{t}}\log p(c|z_{t})\) respectively:

\[\begin{split}\nabla_{z_{t}}\log p_{\omega,\gamma}(z_{t}|c,y)& =\omega\nabla_{z_{t}}\log p(c|z_{t})+\gamma\nabla_{z_{t}}\log p(y|z _{t})+\nabla_{z_{t}}\log p(z_{t})\\ &=\omega\left(\nabla_{z_{t}}\log p(z_{t}|c)-\nabla_{z_{t}}\log p( z_{t})\right)+\gamma\nabla_{z_{t}}\log p(y|z_{t})+\nabla_{z_{t}}\log p(z_{t})\\ &=\omega\nabla_{z_{t}}\log p(z_{t}|c)+(1-\omega)\nabla_{z_{t}} \log p(z_{t})+\gamma\nabla_{z_{t}}\log p(y|z_{t})\end{split}\] (20)

Finally, we obtain the _double guidance_ (DG) improved score estimation in Equation 12.

\[\begin{split}(DG):\quad\hat{\epsilon}_{\theta}(z_{t},t,E_{v}) \leftarrow\omega\epsilon_{\theta}(z_{t},t,E_{v})+(1-\omega)\epsilon_{\theta} (z_{t},t,\varnothing)-\gamma\sqrt{1-\hat{\alpha}_{t}}\nabla_{z_{t}}\log P_{ \phi}(y|z_{t},t,E_{v})\end{split}\] (21)

, where \(c\) is the CAVP-aligned visual features \(E_{v}\), \(y\) is the semantic and temporal alignment label, \(\omega\) is CFG scale and \(\gamma\) is the CG scale.

### Intuition Perspective of _Double Guidance_

We offer another intuitive perspective to explain the technique of _double guidance_. In practice, the CFG [17] method tends to outperform the CG [5] method. This is because the classifier in the CG method may learn shortcuts for classification, leading to good classification results, but when its loglikelihood gradient is used to guide the reverse process, it can generate incorrect samples. In contrast, the _double guidance_ technique addresses this issue by leveraging both the CFG and CG terms. The CFG term provides a robust and reliable noise direction for each timestep in the reverse process, while the CG term refines the CFG term's direction towards the desired data attribute, resulting in better sample quality. The effectiveness of the _double guidance_ technique is validated by the evaluation results in main paper Table. 1, 6.

### The Effect of Guidance Techniques

We present the generated results using _double guidance_ techniques in Figure 9. The first row showcases the ground truth Mel-spectrogram and video frames, depicting a man smashing a window. Subsequent rows show the generated Mel-spectrogram results, arranged from left to right with progressively larger CFG scales \(\omega\) and from top to bottom with increasingly larger CG scales \(\gamma\). Moving from left to right, we observed a significant improvement in audio quality and synchronization with increasing CFG scale \(\omega\). Additionally, with increasing CG scale \(\gamma\), the generation results of the original CFG scales were progressively refined, leading to higher levels of audio-visual alignment in the generated results.

Figure 9: Results of _double guidance_ techniques: The first rows shows the Ground Truth Mel-spectrogram and video frames. The subsequent rows exhibit generated results with progrssively larger CFG scales \(\omega\) and CG scales \(\gamma\).

[MISSING_PAGE_FAIL:22]