# Probabilistic Active Few-Shot Learning in

Vision-Language Models

 Anton Baumann\({}^{1}\)1 Marcus Klasson\({}^{2}\) Rui Li\({}^{2}\) Arno Solin\({}^{2}\) Martin Trapp\({}^{2}\)

\({}^{1}\)Technical University of Munich \({}^{2}\)Aalto University

Work done during an internship at Aalto University.

Footnote 1: footnotemark:

###### Abstract

Pre-trained vision-language models (VLMs) have shown to be an useful model class for zero- and few-shot learning tasks. In this work, we investigate probabilistic active few-shot learning in VLMs by leveraging post-hoc uncertainty estimation and targeted support set selection. To equip VLMs with a notion of uncertainty on the target task, we utilize a Laplace approximation to the posterior of the VLM and derive a Gaussian approximation to the distribution over the cosine similarities. Further, we propose a simple adaptive target region selection based on k-nearest neighbour search and evaluate on a series of selection strategies from the Bayesian experimental design literature. Our experiments on standard benchmarks show that leveraging epistemic uncertainties leads to improved performance and that further improvements can be obtained by targeting the selection towards the query region.

## 1 Introduction

The rise of foundation models [4, 6, 9, 30] has led to their increasing adoption in downstream tasks where data is scarce [16, 42]. Moreover, in many real-world settings it is imperative that predictions are reliable and that sources of uncertainties are captured and incorporated to avoid failure modes. The paradigm of _active few-shot learning_ (or _active fine-tuning_) [1, 17, 40] aims to tackle the challenge of actively selecting a support set (training set for adaption) that is most informative for the downstream task. However, classical approaches, _e.g._, from the coreset literature [36] or information theory [14], typically do not incorporate all sources of uncertainties into their metric of informativeness. Recent works in Bayesian active learning [15] aim to address this issue by performing selection of support set candidates based on their effect on the epistemic uncertainty of the model [11] or the predictive distribution [3]. Moreover, progress in Bayesian deep learning [29] has resulted in methods that can efficiently estimate epistemic uncertainties in a post-hoc manner [23, 8], making them particularly attractive for active few-shot learning of large scale models.

In this work, we investigate probabilistic active few-shot learning for vision-language models (VLMs) and show benefits of incorporating uncertainties in the support set selection process as well as targeting the selection towards the query region. For this, we propose an uncertainty estimation-based approach by leveraging a Laplace approximation [23] to the posterior of a pre-trained CLIP [30] model. We derive a Gaussian approximation to the distribution over cosine similarities between the image and text embeddings, and investigate different scoring mechanisms for the support set candidate selection. In addition, we propose a simple adaptive target region selection based on \(k\)-nearest neighbour (\(k\)-NN) search. In our experiments, we evaluate two few-shot classification settings _(i)_ support set selection from a large cross-domain training data source and _(ii)_ selection from the training set. We find improved performance over naive selection for uncertainty-based selection methods and further improvements when the selection is based on an adaptive target region.

Fig. 1 illustrates the setting we are considering in this work: Given a pre-trained VLM, we aim to predict labels for a query set of images of a novel downstream task. The VLM agent \(\mathcal{M}_{0}\) is asked to first estimate its uncertainty over the predictions on the query set, where the difficulty of the prediction is proportional to the predictive uncertainty. To avoid failure modes, the agent can select a small number of labelled support set candidates \(\mathcal{S}\) from a large data source and use them to update its internal state. Finally, the updated model \(\mathcal{M}_{1}\) is used to predict the labels for the query set.

Our main contributions are the following: _(i)_ We propose a post-hoc method for obtaining a distribution over the cosine similarities from a pre-trained VLM without needing architecture changes or further training. _(ii)_ We apply our method in active learning and assess various scoring mechanisms for support set selection. _(iii)_ We show on benchmark data sets that accounting for epistemic uncertainties improves performance and that targeted candidate selection results in further improvements.

## 2 Methods

We denote vectors by bold lower-case letters (_e.g._, \(\bm{x},\bm{a}\)) and use bold upper-case letters for matrices (_e.g._, \(\bm{X},\bm{P}\)). Further, sets are denoted in upper-case calligraphic letters (_e.g._, \(\mathcal{D},\mathcal{I}\)) and model parameters or hyper-parameters are denoted using Greek letters (_e.g._, \(\alpha,\bm{\theta}\)). In particular, let \(\bm{x}_{i}\in\mathbb{R}^{p_{\text{Plot}}}\) and \(\bm{y}_{j}\in\mathbb{R}^{p_{\text{Plot}}}\) denote the \(i^{\text{th}}\) image and \(j^{\text{th}}\) text description, respectively. Further, we use \(\phi:\mathbb{R}^{p_{\text{Broo}}}\to\mathbb{R}^{d_{\text{Broo}}}\) and \(\psi:\mathbb{R}^{p_{\text{Plot}}}\to\mathbb{R}^{d_{\text{Plot}}}\) to denote the image and text encoders of the VLM, where \(p_{\text{Info}}\) and \(p_{\text{Trxt}}\) denote the respective input dimensionality and \(d_{\text{Info}}\), \(d_{\text{Trxt}}\) is the dimensionality of the respective feature space. The embeddings are projected into a joint space, given as \(\bm{g}=\bm{P}\phi(\bm{x})\) and \(\bm{h}=\bm{Q}\psi(\bm{y})\), using linear projections denoted by \(\bm{P}\in\mathbb{R}^{d\times d_{\text{Broo}}}\) and \(\bm{Q}\in\mathbb{R}^{d\times d_{\text{Trxt}}}\), respectively.

VLMs (_e.g._, [30]) are typically trained by minimizing the InfoNCE loss [28], which is the sum of two cross-entropy terms, one for each relational direction--image to text (Img\(\to\)Txt) or text to image (Img\(\leftarrow\)Txt). The loss is given as \(\mathcal{L}(\bm{X},\bm{Y})=\nicefrac{{1}}{{2}}\mathcal{L}_{\text{CE}}^{ \text{Info}\leftarrow\text{Txt}}(\bm{X},\bm{Y})+\nicefrac{{1}}{{2}}\mathcal{L} _{\text{CE}}^{\text{Info}\leftarrow\text{Txt}}(\bm{X},\bm{Y})\) with cross-entropy loss terms defined over the cosine similarities between the embeddings, _i.e._,

\[\mathcal{L}_{\text{CE}}^{\text{Info}\leftarrow\text{Txt}}(\bm{X},\bm{Y})=\sum _{i=1}^{n}-\log\left(\frac{\exp(\hat{\bm{g}}_{i}^{\top}\hat{\bm{h}}_{i})}{ \sum_{j=1}^{n}\exp(\hat{\bm{g}}_{i}^{\top}\hat{\bm{h}}_{j})}\right),\] (1)

where \(\hat{\bm{g}}\) and \(\hat{\bm{h}}\) are the unit-length normalized embeddings. For further details see App. B.1.

In this work, we utilize post-hoc uncertainty estimation based on the Laplace approximation [23] to estimate uncertainties over the model parameters. This approach has found increasing application in contemporary deep learning (_e.g._, [8; 20; 25]) and uses a Gaussian approximation to the posterior distribution. Utilising a Laplace approximation allows us to induce uncertainty over the feature embeddings of both encoders and results in a distribution over cosine similarities, which in turn enables quantifying model uncertainties in a principled manner. Fig. 2 illustrates the propagation of uncertainties in our setup by estimating uncertainties over the projection matrices.

Laplace approximationOne of the main computational challenges associated with the Laplace approximation is related to the estimation of the Hessian matrix of the log joint w.r.t. the model parameters. Since a naive approach is computationally impractical in the case of VLMs, we chose to estimate the Kronecker-factored Generalized Gauss-Newton (GGN) approximation [33; 24]. Moreover, we apply the Laplace approximation only for the projection matrices \(\bm{P}\) and \(\bm{Q}\) of the image and text encoders. Hence, resulting in GGN approximations \(\mathrm{GGN}_{\text{Img}}\) and \(\mathrm{GGN}_{\text{Txt}}\) given in form of their Kronecker factors, see App. C.1 for details.

However, naively applying Laplace approximations in VLMs is challenging as the contrastive loss entangles \(\bm{P}\) and \(\bm{Q}\), which further complicates the estimation of the Hessian. These models are

Figure 1: Illustration of the setting.

also typically trained with mini-batch sizes of around \(30k\) samples. In order to compute the GGN approximations in VLMs, we simplify the contrastive loss \(\mathcal{L}\) used for pre-training by assuming independence between \(\bm{P}\) and \(\bm{Q}\). Specifically, we treat each of the two loss terms independent and consider only \(\mathcal{L}_{\mathrm{CE}}^{\mathrm{Moo}\to\mathrm{TXT}}\) for the image encoder and \(\mathcal{L}_{\mathrm{CE}}^{\mathrm{Moo}\to\mathrm{TXT}}\) for the text encoder in the Laplace approximation. Hence, dropping interactions between the image and text encoders in the Laplace approximation. Lastly, we use an incremental computation of the Kronecker factors to account for large mini-batch sizes. Further details and derivations are given in App. C.1.

Distribution over cosine similaritiesAs the Laplace approximation uses a Gaussian approximation, the feature embeddings are distributed according to another Gaussian distribution. Specifically, the distribution over embedding vectors \(\bm{g}\) (or \(\bm{h}\)) for a datum \(\bm{x}\) (or \(\bm{y}\)) can be expressed as follows due to linearity, _i.e._,

\[\mathcal{N}\left(\bm{g},\left(\phi(\bm{x})^{\top}\bm{A}_{\mathrm{Mo}}^{-1}\phi (\bm{x})\right)\bm{B}_{\mathrm{IMG}}^{-1}\right)\quad\text{and}\quad\mathcal{ N}\left(\bm{h},\left(\psi(\bm{y})^{\top}\bm{A}_{\mathrm{TXT}}^{-1}\psi(\bm{y}) \right)\bm{B}_{\mathrm{TXT}}^{-1}\right),\] (2)

where \(\bm{A}\) and \(\bm{B}\) denote the Kronecker factors of the GNN approximation of the Hessian matrix, respectively. Unfortunately, the distribution over cosine similarities is in general not Gaussian. However, by assuming independence between the elements of \(\bm{g}\) and \(\bm{h}\) and in the limit of \(d\to\infty\) we can approximate the distribution over cosine similarities to be Gaussian distributed. We find this approximation to work well in practice, while not accurately capturing the skewness of the distributions. A detailed derivation and empirical results on the approximation quality are given in App. C.2.

Targeted support set selectionLet \(\mathcal{X}_{\text{test}}=\{\bm{x}_{i}^{*}\}\) with \(\bm{x}_{i}^{*}\sim p(\bm{x}^{*})\) be a set of unseen test data (query set) with unknown class labels. We aim to find a set \(\{(\bm{x}_{j},\bm{y}_{j})\}_{j}^{m}\) of support candidates of cardinality \(m\) with \(\bm{x}_{j},\bm{y}_{j}\sim p(\bm{x}_{j},\bm{y}_{j})\) such that we reduce uncertainty over the class labels of \(\mathcal{X}_{\text{test}}\). To approach this problem, we target the selection process towards the predictive distribution of the query set. In particular, we propose to use a \(k\)-nearest neighbours selection in the joint space to pre-select support set candidates based on the Wasserstein distance between the distributions over image embeddings. After pre-selection, we quantify the information gain of the support set candidates either using the entropy over the predictive distribution, the expected predictive information gain (EPIG, [3]), or the BALD score [15]. Doing so adaptively targets the candidate search for the the support set towards the predictive distribution of the query set and reduces the computational complexity of the selection process. Further details on the selection process and the score functions are given in App. D.

## 3 Experiments

To evaluate our approach for probabilistic active few-show learning, we conducted experiments using pre-trained OpenCLIP models from Hugging Face [18]. We estimated the Laplace approximations of the OpenCLIP model with ViT-Base backbone and ViT-Huge backbone [10] using a randomly sampled subset from the Laion-400M data set [35]. Further details are given in App. E.

For probabilistic active few-shot learning with VLMs we consider the task of image classification and present results on the Flowers102 [27], Food101 [5], CIFAR-100 [21], ImageNet-R [13], EuroSAT

Figure 2: Illustration of uncertainty propagation in VLMs: We estimate uncertainties over the projection matrices of both encoders using a Laplace approximation, which induces distributions over the feature projections. We then approximate the distribution over cosine similarities by a Gaussian.

[12], and the Office-Home [39] data sets. To assess the performance of the proposal, we investigated the following questions: _(i)_ Do approaches that account for epistemic uncertainties improve performance? _(ii)_ What is the effect of targeting the support set candidates towards the query region? _(iii)_ How does the model capacity affect the performance of the proposed approach?

To address these questions, we performed support set selection from all training domains available in the Office-Home data set and evaluated on the test set (query set) of each domain independently. In Fig. 3 we compare the performance of targeted entropy-based support set selection, random selection, random selection with targeted support set region, and the best performing (according to the validation loss) acquisition function that incorporates epistemic uncertainties. We find that incorporating epistemic uncertainties improves the few-shot learning performance in most cases and generally outperforms random selection. Further, we observe that targeted support set selection improves the performance as indicated by the performance gap between naive random selection and targeted random selection and that the model capacity can have a substantial impact on the performance gains across all approaches. A listing of the results using the negative log-predictive density are given in App. E.2.

Single-domain FinetuningIn App. E.2, we show results for single-domain finetuning on standard benchmark data sets (_e.g._ CIFAR-100, Imagenet-R, Flowers102, etc.) using the different support set selection methods with the OpenCLIP model. The selection methods using the epistemic uncertainty (BALD and EPIG) perform better or on par with the Targeted Maximum Entropy across the different subset sizes and data sets, which demonstrates the benefits of using our proposed uncertainty estimates for support set selection.

## 4 Discussion and Conclusion

In this work, we have introduced a probabilistic active few-shot learning approach for VLMs. Our approach leverages a Laplace approximation to the posterior of the projection layers of the VLM to estimate epistemic uncertainties. We have further introduced an adaptive targeted support set candidate selection based on \(k\)-NN selection using the Wasserstein distance between the distributions over image embeddings in the joint space. To assess the performance of probabilistic active few-shot learning in VLMs, we have conducted two sets of experiments, one in the cross-domain setting on the Office-Home data set and one in the single-domain setting on standard benchmark data sets. We found that incorporating epistemic uncertainties improves the few-shot learning performance in most cases and generally outperforms random selection. Moreover, targeting the selection process towards the query region provides further improvements in all cases.

Figure 3: Results on the Office-Home data set with support set selection from all training domains. We observe that incorporating epistemic uncertainties (—) improves over entropy based targeted selection (—) in most of the cases and outperforms naïve random selection (- - -) and random selection with targeted support set candidates (—). Shaded regions indicate the std over 5 runs.

ReproducibilityThe code for the experiments is available at: https://aaltoml.github.io/BayesVLM/.

AS and RL acknowledge funding from the Research Council of Finland (grant number 339730). MT acknowledges funding from the Research Council of Finland (grant number 347279). MK acknowledge funding from the Finnish Center for Artificial Intelligence (FCAI). We acknowledge CSC - IT Center for Science, Finland, for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through CSC. We acknowledge the computational resources provided by the Aalto Science-IT project.

## References

* [1] Jihwan Bang, Sumyeong Ahn, and Jae-Gil Lee. Active prompt learning in vision language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 27004-27014, 2024.
* [2] Shane Barratt. A matrix gaussian distribution, 2018.
* [3] Freddie Bickford Smith, Andreas Kirsch, Sebastian Farquhar, Yarin Gal, Adam Foster, and Tom Rainforth. Prediction-oriented Bayesian active learning. In _International Conference on Artificial Intelligence and Statistics_, 2023.
* [4] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* mining discriminative components with random forests. In _European Conference on Computer Vision_, 2014.
* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [7] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. Probabilistic embeddings for cross-modal retrieval. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8415-8424, 2021.
* [8] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace redux-effortless bayesian deep learning. _Advances in Neural Information Processing Systems_, 34:20089-20103, 2021.
* [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021.
* [11] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In _International conference on machine learning_, pages 1183-1192. PMLR, 2017.
* [12] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 12(7):2217-2226, 2019.
* [13] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_. IEEE, 2021.

* [14] Alex Holub, Pietro Perona, and Michael C Burl. Entropy-based active learning for object recognition. In _2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops_, pages 1-8. IEEE, 2008.
* [15] Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Mate Lengyel. Bayesian active learning for classification and preference learning. _arXiv preprint arXiv:1112.5745_, 2011.
* [16] Shell Xu Hu, Da Li, Jan Stuhmer, Minyoung Kim, and Timothy M Hospedales. Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9068-9077, 2022.
* [17] Jonas Hubotter, Bhavya Sukhija, Lenart Treven, Yarden As, and Andreas Krause. Active few-shot fine-tuning. _arXiv preprint arXiv:2402.15441_, 2024.
* [18] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021.
* [19] Yatai Ji, Junjie Wang, Yuan Gong, Lin Zhang, Yanru Zhu, Hongfa Wang, Jiaxing Zhang, Tetsuya Sakai, and Yujiu Yang. Map: Multimodal uncertainty-aware vision-language pre-training model. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 23262-23271, 2023.
* [20] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, fixes overconfidence in relu networks. In _International conference on machine learning_, pages 5436-5446. PMLR, 2020.
* [21] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
* [22] Hao Li, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Haonan Zhang, and Gongfu Li. A differentiable semantic metric approximation in probabilistic embedding for cross-modal retrieval. _Advances in Neural Information Processing Systems_, 35:11934-11946, 2022.
* [23] David JC MacKay. Information-based objective functions for active data selection. _Neural computation_, 4(4):590-604, 1992.
* [24] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In _International conference on machine learning_, pages 2408-2417. PMLR, 2015.
* [25] Lassi Meronen, Martin Trapp, Andrea Pilzer, Le Yang, and Arno Solin. Fixing overconfidence in dynamic neural networks. In _IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 2680-2690, 2024.
* [26] Kimia Nadjahi, Alain Durmus, Pierre E Jacob, Roland Badeau, and Umut Simsekli. Fast approximation of the sliced-wasserstein distance using concentration of random projections. _Advances in Neural Information Processing Systems_, 34:12411-12424, 2021.
* [27] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _Indian Conference on Computer Vision, Graphics and Image Processing_, 2008.
* [28] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [29] Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Jose Miguel Hernandez-Lobato, et al. Position: Bayesian deep learning is needed in the age of large-scale ai. In _nternational Conference on Machine Learning_, 2024.

* [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [31] Carl Edward Rasmussen and Christopher K. I. Williams. _Gaussian Processes for Machine Learning_. The MIT Press, 2006.
* [32] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. _ACM computing surveys (CSUR)_, 54(9):1-40, 2021.
* [33] Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks. In _International conference on learning representations_, 2018.
* [34] Subhankar Roy, Martin Trapp, Andrea Pilzer, Juho Kannala, Nicu Sebe, Elisa Ricci, and Arno Solin. Uncertainty-guided source-free domain adaptation. In _European conference on computer vision_, pages 537-555. Springer, 2022.
* [35] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs, 2021.
* [36] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In _International Conference on Learning Representations_, 2018.
* [37] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-language models. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_. IEEE, 2023.
* [38] Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano Mancini, and Zeynep Akata. Probvlm: Probabilistic adapter for frozen vison-language models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1899-1910, 2023.
* [39] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5018-5027, 2017.
* [40] Yichen Xie, Han Lu, Junchi Yan, Xiaokang Yang, Masayoshi Tomizuka, and Wei Zhan. Active finetuning: Exploiting annotation budget in the pretraining-finetuning paradigm. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 23715-23724, 2023.
* [41] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.
* [42] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15211-15222, 2023.

## Appendix A Related Work

### Active Learning

The active learning setting [32] entails an agent learning a task from an unlabelled dataset, while simultaneously determining which data points to label for maximal benefit to the target task. The learner uses an acquisition function to base its sample selection on that should quantify how beneficial (or informative) this sample will be to learn from for the target task. There exist various acquisition functions, _e.g._, (i) entropy-based which aims to minimize the expected entropy after observing data points [14], and (ii) core-set based methods which are trained to minimize the generalization error between the unlabelled and labelled sets and use clustering for selection [36]. Uncertainty-based acquisition functions have been explored to select data points that will mostly reduce the epistemic uncertainty in the model, _e.g._, Bayesian Active Learning by Disagreement (BALD) score [11; 15]. More recently, the expected predictive information gain (EPIG) [3] was proposed to measure the information gain in the space of predictions rather than parameters. We experiment with the mentioned uncertainty-based acquisition functions combined with our probabilistic embeddings for targeted data selection in VLM finetuning.

### Probabilistic Vision-Language Models

Several works are aiming to extend VLMs to produce predictive uncertainty estimates for various downstream tasks, _e.g._, cross-modal retrieval [7; 22] and visual-question answering [19]. These methods learn probabilistic embeddings on each modality by estimating probability distributions from the network. However, this approach requires training the networks from scratch, which limits their applicability to pretrained VLMs (_e.g._ CLIP). To this end, Upadhyay et al. [38] proposed a post-hoc method called ProbVLM that learns probabilistic embeddings from finetuned adapters on a frozen VLM backbone. Similar to this work, they also apply their method to the active learning task and use the uncertainty estimates for selecting informative subsets of training data for finetuning. However, ProbVLM requires finetuning the probabilistic embeddings on a proxy task, while our method can be applied directly on the pretrained model.

## Appendix B Preliminaries

This section provides a brief overview of the background concepts relevant to this work.

### Vision-Language Models

In this work, we consider vision-language models (VLM) learned using the contrastive learning objective known as InfoNCE. In particular, let \(\bm{x}_{i}\in\mathbb{R}^{\text{{P}}_{\text{{P}}{\text{{P}}{\text{{P}}{\text{{ P}}{\text{{P}}}}}}}}\) and \(\bm{y}_{j}\in\mathbb{R}^{\text{{P}}_{\text{{P}}{\text{{P}}{\text{{P}}{\text{{ P}}}}}}}\) denote the \(i\)th image and \(j\)th text description, respectively. Further, we use \(\phi:\mathbb{R}^{\text{{P}}_{\text{{P}}{\text{{P}}{\text{{P}}}}}}\to\mathbb{R} ^{d_{\text{{P}}{\text{{P}}{\text{{P}}}}}}\) and \(\psi:\mathbb{R}^{\text{{P}}_{\text{{P}}{\text{{P}}{\text{{P}}}}}}\to\mathbb{R} ^{d_{\text{{P}}{\text{{P}}{\text{{P}}}}}}\) to denote the image and text encoders of the VLM, where \(p_{\text{{M}}{\text{{P}}}}\) and \(p_{\text{{P}}{\text{{P}}{\text{{P}}}}}\) denote the respective input dimensionalities and \(d_{\text{{I}}{\text{{P}}{\text{{P}}}}}\), \(d_{\text{{P}}{\text{{P}}{\text{{P}}}}}\) is the dimensionality of the respective feature space.

To project the embeddings into a joint space, we assume a linear projection layer for both the image and the text encoder denoted by \(\bm{P}\in\mathbb{R}^{d\times d_{\text{{hoc}}}}\) and \(\bm{Q}\in\mathbb{R}^{d\times d_{\text{{Txt}}}}\), respectively. The embeddings in the joint space are then given as \(\bm{g}_{i}=\bm{P}\phi(\bm{x}_{i})\) and \(\bm{h}_{j}=\bm{Q}\psi(\bm{y}_{j})\) and we use hat notation to denote the unit-length normalized embeddings, _e.g._, \(\hat{\bm{g}}_{i}=\frac{\bm{P}\phi(\bm{x}_{i})}{\|\bm{P}\phi(\bm{x}_{i})\|}\).

VLM models (_e.g._, [30]) are typically trained by minimizing the InfoNCE loss, which is given as the sum of two cross-entropy terms, one for each relational direction - image to text (\(\textsc{Img}\rightarrow\textsc{Txt}\)) or text to image (\(\textsc{Img}\leftarrow\textsc{Txt}\)). Specifically, the InfoNCE loss is given as \(\mathcal{L}(\bm{X},\bm{Y})=\nicefrac{{1}}{{2}}\mathcal{L}_{\textsc{CE}}^{ \textsc{Img}\rightarrow\textsc{Txt}}(\bm{X},\bm{Y})+\nicefrac{{1}}{{2}} \mathcal{L}_{\textsc{CE}}^{\textsc{Img}\leftarrow\textsc{Txt}}(\bm{X},\bm{Y})\) with cross-entropy loss terms given as:

\[\mathcal{L}_{\textsc{CE}}^{\textsc{Img}\rightarrow\textsc{Txt}}(\bm{X},\bm{Y }) =\sum_{i=1}^{n}-\log\left(\frac{\exp(\hat{\bm{g}}_{i}^{\top}\hat{\bm{h}}_{i}) }{\sum_{j=1}^{n}\exp(\hat{\bm{g}}_{i}^{\top}\hat{\bm{h}}_{j})}\right)\] (3) \[\mathcal{L}_{\textsc{CE}}^{\textsc{Img}\leftarrow\textsc{Txt}}( \bm{X},\bm{Y}) =\sum_{i=1}^{n}-\log\left(\frac{\exp(\hat{\bm{h}}_{i}^{\top}\hat{ \bm{g}}_{i})}{\sum_{j=1}^{n}\exp(\hat{\bm{h}}_{i}^{\top}\hat{\bm{g}}_{j})} \right).\] (4)

For further details we refer the reader to [30, 41]

### Bayesian Deep Learning

We will briefly review concepts on Bayesian deep learning relevant to this work. Given a dataset \(\mathcal{D}=\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{n}\) and a probabilistic models with likelihood function \(p(\bm{y}\mid\bm{x},\bm{\theta})\) and prior distribution \(p(\bm{\theta})\), we aim to estimate the posterior distribution \(p(\bm{\theta}\mid\mathcal{D})\) of the model parameters \(\bm{\theta}\) given the training data \(\mathcal{D}\). In the context of deep learning, exact inference of the posterior distribution is at least NP-hard in most settings and only becomes tractable if \(p(\bm{\theta}\mid\mathcal{D})\) constitute sufficient structure [23]. Henceforth, we consider approximate Bayesian inference using the Laplace approximation [23] in this work, which has gained increasing popularity in the community (_e.g._, [33, 8, 25, 34]) as a post-hoc techniques to estimate epistemic uncertainties.

The Laplace approximation uses a second-order Taylor expansion of the log-joint around the maximum-a-posteriori (MAP) estimate \(\bm{\theta}_{\textsc{MAP}}\). The resulting distribution is then approximated with an un-normalised Gaussian density, which in turn results in an approximate posterior distribution given by a Gaussian distribution located at the MAP estimate, _i.e._, \(p(\bm{\theta}\mid\mathcal{D})\approx\mathcal{N}(\bm{\theta}\mid\bm{\theta}_{ \textsc{MAP}},\bm{\Sigma})\). Resulting from the Taylor expansion, the covariance is given by the inverse Hessian at the MAP, _i.e._, \(\bm{\Sigma}=(-\nabla^{2}\log p(\bm{\theta},\mathcal{D})|_{\bm{\theta}=\bm{ \theta}_{\textsc{MAP}}})^{-1}\). Predictions are then made based on the posterior predictive distribution \(p(\bm{y}\mid\bm{x},\mathcal{D})=\int p(\bm{y}\mid\bm{x},\bm{\theta})p(\bm{ \theta}\mid\mathcal{D})\mathrm{d}\bm{\theta}\), which is typically performed by Monte Carlo sampling in case of non-linear likelihoods functions, _e.g._, classification settings. We refer to [8] for a detailed review of the topic.

## Appendix C Derivations

This section provides detailed derivations of the equations presented in the main text.

### Laplace Approximation

To obtain the Laplace approximation to the VLM, we first assume independence between \(\bm{P}\) and \(\bm{Q}\). The resulting GGN approximations \(\mathrm{GGN}_{\textsc{Img}}\) and \(\mathrm{GGN}_{\textsc{Txt}}\) are then given in form of their Kronecker factors \(\bm{A}\) and \(\bm{B}\), _i.e._,

\[\mathrm{GGN}_{\textsc{Img}}\approx\quad\underbrace{\left[\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\phi(\bm{x}_{i})\phi(\bm{x}_{i})^{\top}\right]}_{=\bm{A}_{\textsc {Img}}}\otimes\underbrace{\left[\frac{1}{\sqrt{n}}\sum_{i=1}^{n}J_{\textsc{Img}}( \bm{x}_{i})^{\top}\bm{\Lambda}_{\textsc{Img}}\,J_{\textsc{Img}}(\bm{x}_{i}) \right]}_{=\bm{B}_{\textsc{Img}}},\] (5)

where \(J_{\textsc{Img}}(\bm{x}_{i})=\frac{\partial\bm{\theta}_{i}^{\top}\bm{H}}{ \partial\bm{g}_{i}}\) and

\[\mathrm{GGN}_{\textsc{Txt}}\approx\quad\underbrace{\left[\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\psi(\bm{y}_{i})\psi(\bm{y}_{i})^{\top}\right]}_{=\bm{A}_{\textsc {Txt}}}\otimes\underbrace{\left[\frac{1}{\sqrt{n}}\sum_{i=1}^{n}J_{\textsc{Txt} }(\bm{x}_{i})^{\top}\bm{\Lambda}_{\textsc{Txt}}\,J_{\textsc{Txt}}(\bm{x}_{i}) \right]}_{=\bm{B}_{\textsc{Txt}}},\] (6)where \(n\) is the number image-text pairs in the training set.

We further incorporate the prior precision \(\lambda\) into the GGN approximation by adding the prior precision to the diagonal of the GGN Hessian, _i.e._,

\[\mathrm{GGN_{IMG}} \approx\tau\left(\bm{A}_{\text{IMG}}\otimes\bm{B}_{\text{IMG}} \right)+\lambda\bm{I}\] (7) \[\approx\left(\sqrt{\tau}\bm{A}_{\text{IMG}}+\sqrt{\lambda}\bm{I} \right)\otimes\left(\sqrt{\tau}\bm{B}_{\text{IMG}}+\sqrt{\lambda}\bm{I}\right).\] (8)

In our experiments, we set \(\tau=0.75\) for the ViT-Base model and \(\tau=0.3\) for the ViT-Huge model and obtain the prior precision \(\lambda\) through marginal likelihood maximization.

#### c.1.1 Obtaining the Posterior Predictive Distribution

For conciseness, we denote the posterior precision matrices associated with the image encoder as \(\bm{A}_{\text{IMG}}\) and \(\bm{B}_{\text{IMG}}\). We have obtained the posterior distribution over the image projection matrix \(\bm{P}\) represented as \(\mathcal{N}(\mathrm{vec}(\bm{P});\mathrm{vec}(\bm{P}_{\text{MAP}}),\mathrm{ GGN}_{\text{IMG}}^{-1})\). Given that \(\mathrm{GGN}_{\text{IMG}}^{-1}\) is formulated using the Kronecker product of the inverses of these matrices, _i.e._, \(\bm{A}_{\text{IMG}}^{-1}\otimes\bm{B}_{\text{IMG}}^{-1}\), we proceed to express the posterior predictive distribution as a matrix normal distribution \(\mathcal{MN}(\bm{P};\bm{P}_{\text{MAP}},\bm{B}_{\text{IMG}}^{-1},\bm{A}_{ \text{IMG}}^{-1})\) as referenced in [2]:

\[\bm{P} \sim\mathcal{MN}(\bm{P}_{\text{MAP}},\bm{B}_{\text{IMG}}^{-1},\bm {A}_{\text{IMG}}^{-1})\] (9) \[\implies\bm{g}=\bm{P}\phi(\bm{x})\sim\mathcal{MN}(\bm{P}_{\text{ MAP}}\phi(\bm{x}),\bm{B}_{\text{IMG}}^{-1},\phi(\bm{x})^{\top}\bm{A}_{\text{IMG}}^{-1} \phi(\bm{x}))\] (10) \[\implies\bm{g}\sim\mathcal{N}(\bm{P}_{\text{MAP}}\bm{a},\left( \phi(\bm{x})^{\top}\bm{A}_{\text{IMG}}^{-1}\phi(\bm{x})\right)\bm{B}_{\text{IMG }}^{-1})\] (11)

#### c.1.2 Online Laplace Approximation

For the EPIG score, we update our Laplace approximation online after each data point is added to the support set. Given the current Laplace approximation of the posterior over the image projection matrix \(\bm{P}\) we update the posterior distribution as follows:

\[\bm{P}_{t+1} =\bm{P}_{t}-\gamma\nabla_{\bm{P}}\mathcal{L}_{\text{CE}}^{\text{ ImG}\to\mathrm{Txt}}(\bm{x}^{*},\bm{Y})\] (12) \[\bm{A}_{\text{IMG},t+1} =\bm{A}_{\text{IMG},t}+\beta\phi(\bm{x}^{*})\phi(\bm{x}^{*})^{\top}\] (13) \[\bm{B}_{\text{IMG},t+1} =\bm{B}_{\text{IMG},t}+\beta J_{\text{IMG}}(\bm{x}^{*})^{\top} \bm{\Lambda}_{\text{IMG}}J_{\text{IMG}}(\bm{x}^{*})\] (14)

From the updated \(\bm{A}_{\text{IMG},t+1}\) and \(\bm{B}_{\text{IMG},t+1}\) we obtain the updated GGN approximation of the Hessian matrix:

\[\mathrm{GGN}_{\text{IMG},t+1}\approx\left(\sqrt{\tau}\bm{A}_{\text{IMG},t+1}+ \sqrt{\lambda}\bm{I}\right)\otimes\left(\sqrt{\tau}\bm{B}_{\text{IMG},t+1}+ \sqrt{\lambda}\bm{I}\right)\] (15)

After each update, we optimize for the prior precision \(\lambda\) by maximizing the marginal likelihood. For our experiments, we set the learning rates \(\gamma=10^{-3}\) and \(\beta=1\).

#### c.1.3 Jacobians for the GGN Approximation

In the following we derive the Jacobians \(J_{\text{IMG}}(\bm{x}_{i})\) and \(J_{\text{Txt}}(\bm{y}_{i})\) used in the Kronecker-factored Generalized Gauss-Newton (GGN) approximation of the Hessian matrices. Let \(\hat{\bm{g}}_{i}\) and \(\hat{\bm{h}}_{j}\) denote the normalized image and text embedding, respectively. With some misuse of notation, let \(\hat{\bm{H}}\) denote the matrix of normalized text embeddings with \(\hat{\bm{h}}_{j}\) as its columns, and \(\hat{\bm{G}}\) the matrix of normalized image embeddings with \(\hat{\bm{g}}_{i}\) as its columns. Then, for the InfoNCE likelihood, which depends on the dot product between the normalized embedding in the batch, we compute the Jacobian for the image encoder as follows:

\[J_{\text{IMG}}(\bm{x}_{i})^{\top} =\frac{\partial\hat{\bm{H}}^{\top}\hat{\bm{g}}_{i}}{\partial\bm{ g}_{i}}=\hat{\bm{H}}^{\top}\frac{\partial}{\partial\bm{g}_{i}}\frac{\bm{g}_{i}}{ \|\bm{g}_{i}\|}=\hat{\bm{H}}^{\top}\frac{\|\bm{g}_{i}\|-\bm{g}_{i}\frac{\partial \|\bm{g}_{i}\|}{\partial\bm{g}_{i}}\|}{\|\bm{g}_{i}\|^{2}}=\hat{\bm{H}}^{\top} \frac{\|\bm{g}_{i}\|-\frac{\bm{g}_{i}\bm{g}_{i}^{\top}}{\|\bm{g}_{i}\|}}{\|\bm{g }_{i}\|^{2}}\] (16) \[=\hat{\bm{H}}^{\top}\left(\frac{\bm{1}}{\|\bm{g}_{i}\|}-\frac{\bm{ g}_{i}\bm{g}_{i}^{\top}}{\|\bm{g}_{i}\|^{3}}\right)\] (17)

Analogously, we obtain the Jacobian for the text encoder as:

\[J_{\text{Txt}}(\bm{y}_{i})^{\top}=\hat{\bm{G}}^{\top}\left(\frac{\bm{1}}{\|\bm{h} _{i}\|}-\frac{\bm{h}_{i}\bm{h}_{i}^{\top}}{\|\bm{h}_{i}\|^{3}}\right)\] (18)

#### c.1.4 Likelihood Hessian for the GGN Approximation

The zero-shot classifier induced by CLIP computes unnormalized logits for each class \(c\), represented by \(\hat{\bm{g}}_{i}^{\top}\hat{\bm{h}}_{c}=:f_{c}\). By applying the softmax function, we calculate the probabilities for each class \(c\) as \(\pi_{c}=\frac{\exp(f_{c})}{\sum_{e^{\prime}}\exp(f_{e^{\prime}})}\). The likelihood Hessian of the cross-entropy loss for this classifier is represented by:

\[\Lambda_{\mathrm{IMG}}=\mathrm{diag}(\bm{\pi})-\bm{\pi}\bm{\pi}^{\top}\] (19)

Similarly, the likelihood Hessian for the text encoder follows analogous principles in the text-to-image direction. For a more detailed derivation of the likelihood Hessian, we refer to [31]. Rearranging terms in the analytical expression for \(J_{\mathrm{IMG}}^{\top}\bm{\Lambda}_{\mathrm{IMG}}J_{\mathrm{IMG}}\) facilitates space-efficient computation of the GGN approximation.

### Distribution over Cosine Similarities

For the derivation of the distribution over cosine similarities, first recall the definition of the cosine similarity between two vectors, \(\bm{g}\) and \(\bm{h}\), which is given as \(\mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})=\frac{\bm{g}^{\top}\bm{h}}{\|\bm{g} \|\|\bm{h}\|}\). Now, with some abuse of notation, let \(\bm{g}\) and \(\bm{h}\) denote random vectors for the image and text embeddings, respectively. Further, let us assume that their distribution follows a Gaussian distribution with mean \(\bm{\mu}_{\bm{g}}=(\mu_{\bm{g},1},\ldots,\mu_{\bm{g},d})\) and \(\bm{\mu}_{\bm{h}}=(\mu_{\bm{h},1},\ldots,\mu_{\bm{h},d})\) and diagonal covariance structure, _i.e._, \(\bm{\Sigma}_{\bm{g}}=\mathrm{diag}(\sigma_{\bm{g},1}^{2},\ldots,\sigma_{\bm{ g},d}^{2})\) and \(\bm{\Sigma}_{\bm{h}}=\mathrm{diag}(\sigma_{\bm{h},1}^{2},\ldots,\sigma_{\bm{h},d}^ {2})\).

Then the expected value of the cosine similarity is:

\[\mathbb{E}[\mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})] =\frac{\mathbb{E}[\bm{g}^{\top}\bm{h}]}{\mathbb{E}[\|\bm{g}\|] \mathbb{E}[\|\bm{h}\|]}\] (20) \[=\frac{\sum_{i}^{d}\mu_{\bm{g},i}\mu_{\bm{h},i}}{\mathbb{E}[\|\bm {g}\|]\mathbb{E}[\|\bm{h}\|]}.\] (21)

Note that computing \(\mathbb{E}[\|\bm{x}\|]\) is intractable, and we therefore bound the expected value by application of the triangle inequality, _i.e._,

\[\mathbb{E}[\|\bm{x}\|]\leq\sqrt{\sum_{i}\mu_{\bm{x},i}^{2}+\sigma_{\bm{x},i}^ {2}}\,,\] (22)

where we use the fact that \(\mathbb{E}[x^{2}]=\mu_{x}^{2}+\sigma_{x}^{2}\). Consequently, we obtain an approximation to the expected value of the cosine similarity given by:

\[\mathbb{E}[\mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})]\approx\frac{\sum_{i}^{d }\mu_{\bm{g},i}\mu_{\bm{h},i}}{\sqrt{\sum_{i}\mu_{\bm{g},i}^{2}+\sigma_{\bm{g},i}^{2}}\sqrt{\sum_{i}\mu_{\bm{h},i}^{2}+\sigma_{\bm{h},i}^{2}}}.\] (23)

Next, we will derive the second moment (variance) of the cosine similarity of two random vectors. First note that the variance can be written as the difference of two expectations, _i.e._,

\[\mathbb{V}\mathrm{ar}[\mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})]=\mathbb{E}[ \mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})^{2}]-\mathbb{E}[\mathrm{S}_{\mathrm{ COS}}(\bm{g},\bm{h})]^{2},\] (24)

where the second expection corresponds to:

\[\mathbb{E}[\mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})]^{2}\approx\frac{(\sum_{ i}^{d}\mu_{\bm{g},i}\mu_{\bm{h},i})^{2}}{\sum_{i}\mu_{\bm{g},i}^{2}+\sigma_{\bm{g},i}^{2}\sum_{i}\mu_{\bm{h},i}^{2}+\sigma_{\bm{h},i}^{2}}.\] (25)

Next we can obtain \(\mathbb{E}[\mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})^{2}]\) for which we will use the fact that \(\mathbb{E}[x^{2}]=\mu_{x}^{2}+\sigma_{x}^{2}\) again, _i.e._,

\[\mathbb{E}[\mathrm{S}_{\mathrm{COS}}(\bm{g},\bm{h})^{2}]=\frac{\mathbb{E}[( \bm{g}^{\top}\bm{h})^{2}]}{\sum_{i}\mu_{\bm{g},i}^{2}+\sigma_{\bm{g},i}^{2} \sum_{i}\mu_{\bm{h},i}^{2}+\sigma_{\bm{h},i}^{2}}\] (26)

where

\[\mathbb{E}[(\bm{g}^{\top}\bm{h})^{2}] =\sum_{i}\sum_{j}\mu_{\bm{g},i}\mu_{\bm{h},i}\mu_{\bm{g},j}\mu_{\bm {h},j}\] (27) \[+\sum_{i}\sigma_{\bm{g},i}^{2}\mu_{\bm{h},i}^{2}+\mu_{\bm{g},i}^{ 2}\sigma_{\bm{h},i}^{2}+\sigma_{\bm{g},i}^{2}\sigma_{\bm{h},i}^{2}.\] (28)Henceforth, we obtain for the variance:

\[\mathbb{V}_{\mathrm{ar}}[\mathrm{S}_{\mathrm{cos}}(\bm{g},\bm{h})]=\frac{\sum_{i }\sigma_{\bm{g},i}^{2}(\sigma_{\bm{h},i}^{2}+\mu_{\bm{h},i}^{2})+\sigma_{\bm{h},i}^{2}\mu_{\bm{g},i}^{2}}{\sum_{i}\mu_{\bm{g},i}^{2}+\sigma_{\bm{g},i}^{2} \sum_{i}\mu_{\bm{h},i}^{2}+\sigma_{\bm{h},i}^{2}}.\] (29)

To empirically assess the approximation quality, we compared the approximation to a kernel density estimate (KDE) over Monte Carlo samples. In particular, we generated \(500\) samples for the image and text feature distributions for a given input. For the resulting samples, we then computed the respective cosine similarity for each pair and performed kernel density estimation with Gaussian kernel and lengthscale of \(0.3\) on the similarity scores. We added increasing shifts to the distribution mean to evaluate the change in the approximation quality under varying cosine similarity values. Fig. 4 illustrates the approximation quality compared to a Monte Carlo simulation for image-text pairs with increasing distance between their feature projection means.

## Appendix D Details on Support Set Selection

This section provides further details on the support set selection strategies used in this work.

### k-Nearest Selection

Active learning acquisition functions like Maximum Entropy Selection or BALD are often applied to the training set, lacking consideration of the target distribution and resulting in unrepresentative selections. To address this, we propose the following heuristic: we greedily acquire a maximally informative intermediate set \(\mathcal{S}^{*}\subseteq\mathcal{X}_{\text{test}}\) from the test set, followed by selecting training data points in the vicinity of the intermediate set \(\mathcal{S}^{*}\). In case of deterministic embeddings one can use the cosine similarity or Euclidean distance for this purpose. However, as the embeddings are probabilistic in our setting, a point-wise comparison is not possible. Henceforth, we propose to compute the Wasserstein distance between the distributions of the embeddings of the test set and the training set, and select the training samples with minimal Wasserstein distance to the test set. For multivariate Gaussian distributions, the Wasserstein distance can be computed in closed form and is given as:

\[W_{2}^{2}\left(\mathcal{N}(\bm{\mu}_{1},\bm{\Sigma}_{1}),\mathcal{N}(\bm{\mu}_ {2},\bm{\Sigma}_{2})\right)=\|\bm{\mu}_{1}-\bm{\mu}_{2}\|_{2}^{2}+\mathrm{tr} \left(\bm{\Sigma}_{1}+\bm{\Sigma}_{2}-2(\bm{\Sigma}_{1}^{1/2}\bm{\Sigma}_{2} \bm{\Sigma}_{1}^{1/2})^{1/2}\right)\] (30)

where \(\|\cdot\|_{2}\) denotes the Euclidean norm, \(\mathrm{Tr}(\cdot)\) is the trace operator, and \(\bm{\Sigma}^{1/2}\) is the matrix square root of \(\bm{\Sigma}\). As computing the Wasserstein distance exactly is computationally and memory intensive, we approximate it by ignoring the correlation terms between the dimensions of the embeddings

Figure 4: Approximation quality of the Gaussian approximation (- - -) to the distribution over cosine similarities compared to KDE over samples (—) for image-text pairs with increasing Euclidean distance between their feature projection means (\(\bm{\mu_{g}},\bm{\mu_{h}}\)).

resulting in the Wasserstein distance for univariate Gaussian distributions. We aim to explore more sophisticated approximations, _e.g._, using the sliced Wasserstein distance [26], in future work. Based on this distance, we select the training samples closest to the test set in the joint embedding space, resulting in:

\[\mathcal{S}=\bigcup_{\bm{g}^{*}\in\mathcal{S}^{*}}\mathcal{N}_{k}(\bm{g}^{*}, \mathcal{X}_{\text{train}}),\] (31)

with \(\mathcal{N}_{k}(\bm{g}^{*},\mathcal{X}_{\text{train}})\) denoting the set of \(k\)-nearest neighbours of \(\bm{g}^{*}\) in the training set \(\mathcal{X}_{\text{train}}\) according to the Wasserstein distance over the distributions of the normalized image embeddings. To ensure that we select \(k\) distinct training samples for each test sample, we perform an iterative search in which we discard the already selected training samples and iteratively increase the search radius until \(k\) distinct samples are found. This process is illustrated in Fig. 5.

### Acquisition Functions

Naive RandomFor the _naive random_ acquisition function, we randomly sample \(m\) data points from the train set \(\mathcal{X}_{\text{train}}\) to form the support set \(\mathcal{S}_{\text{ID}}\).

Targeted RandomFor the _targeted random_ acquisition function, we randomly sample \(m\) data points from the test set \(\mathcal{X}_{\text{test}}\) to form a intermediate support set \(\mathcal{S}^{*}\). According to App. D.1, we then

Figure 5: Illustration of the nearest neighbour based support set selection for adaptive targeted selection. The circles \(\boxplus\) show test data points with uncertainty scores depicted through their colours: high, medium, low. For each test datum we find the \(k=1\) nearest neighbour from the support set candidates \(\bm{\mathsf{x}}\). If the \(k=1\) nearest neighbour is already selected, we increase \(k\) for those with occupied neighbours and choose the second nearest neighbour, _i.e._, \(k=2\). This recursion continues until every test datum has a selected support set candidate. The selected candidates are shown by coloured circles. Note that in case of the blue test datum, the closest support set candidate has already been chosen by the yellow and hence the second closes candidate is selected in the second stage.

Figure 6: Illustration of targeted support set selection. We aim to select an informative support set that reduces the uncertainty over the predictions on the query set \(\boxplus\). Only focusing on the epistemic uncertainties would not lead to a good selection as we would select uninformative support set candidates \(\bm{\mathsf{x}}\) with high epistemic uncertainty. Hence, we target the selection process.

select the nearest neighbours to \(\mathcal{S}^{*}\) from the training set \(\mathcal{X}_{\text{train}}\) based on the cosine similarity of the normalized image embeddings to form the support set \(\mathcal{S}_{\text{t-ID}}\).

Targeted Maximum EntropyFor the _entropy_ acquisition function, we compute the predictive entropy \(\mathcal{H}(y_{i}^{*}\mid\bm{x}_{i}^{*})\) for each data point \(\bm{x}_{i}^{*}\in\mathcal{X}_{\text{test}}\) and select the \(m\) data points with the highest entropy. We use the predictive entropy on the MAP estimate of the model parameters to estimate the predictive entropy of the model:

\[\mathcal{H}\left(y\mid\bm{x},\bm{\theta}_{\mathrm{MAP}}\right)=-\sum_{c=1}^{C }p(y=c\mid\bm{x},\bm{\theta}_{\mathrm{MAP}})\log p(y=c\mid\bm{x},\bm{\theta}_{ \mathrm{MAP}})\] (32)

According to App. D.1, we then select the most similar data points from \(\mathcal{X}_{\text{train}}\) to form the support set \(\mathcal{S}_{\text{t-entropy}}\).

BALDWe compute the BALD score [15] for each data point in \(\mathcal{X}_{\text{train}}\) and select the \(m\) data points with the highest score. The score is approximated using nested Monte Carlo sampling as in [15].

\[\mathrm{BALD}(\bm{x}) =\mathbb{E}_{p(y|\bm{x})}\left[\mathcal{H}\left(p(\bm{\theta}) \right)-\mathcal{H}\left(p(\bm{\theta}\mid\bm{x},y)\right)\right]\] (33) \[=\mathbb{E}_{p(\bm{\theta}|\mathcal{D})}\left[\mathcal{H}\left(p( y\mid\bm{x},\bm{\theta})\right)-\mathcal{H}\left(p(y\mid\bm{x},\mathcal{D}) \right)\right]\] (34)

Targeted BALDWe compute the BALD score (Eq. (34)) for each data point \(\bm{x}_{i}^{*}\in\mathcal{X}_{\text{test}}\) and select the \(m\) data points with the highest score. According to App. D.1, we then select the most similar data points from \(\mathcal{X}_{\text{train}}\) to form the support set \(\mathcal{S}_{\text{t-BALD}}\).

EPIGThe Expected Predictive Information Gain (EPIG) score [3] calculates the expected mutual information between the model parameters and the predictive distribution resulting from the acquisition of a training data point. This method is specifically designed to target relevant information, eliminating the need for a k-nearest neighbor search typically used in other acquisition functions. The EPIG score is given by

\[\mathrm{EPIG}(\bm{x}) =\mathbb{E}_{p_{*}(\bm{x}^{*})p_{\Theta}(y|\bm{x})}\left( \mathcal{H}\left(p_{\phi}(y^{*}\mid\bm{x}^{*})\right)-\mathcal{H}\left(p_{ \phi}(y^{*}\mid\bm{x}^{*},x,y)\right)\right)\] (35) \[=\mathbb{E}_{p_{*}(\bm{x}^{*})}\left[\mathrm{D}_{\mathrm{KL}} \left(p_{\phi}(y,y^{*}\mid\bm{x},\bm{x}^{*})\,\|\,p_{\phi}(y\mid\bm{x})p_{ \phi}(y^{*}\mid\bm{x}^{*})\right)\right]\] (36) \[=\mathbb{E}_{p_{*}(\bm{x}^{*})}\left[\sum_{y\in\mathcal{Y}}\sum_ {y^{*}\in\mathcal{Y}}p_{\phi}(y,y^{*}\mid\bm{x},\bm{x}^{*})\log\frac{p_{\phi}( y,y^{*}\mid\bm{x},\bm{x}^{*})}{p_{\phi}(y\mid\bm{x})p_{\phi}(y^{*}\mid\bm{x}^{*})}\right]\] (37)

and can be approximated using Monte Carlo sampling. For the EPIG selection we perform online updates to the model weights using the online Laplace as described in App. C.1.2.

## Appendix E Experiments

### Experimental Details

In our experiments we used the a pre-trained CLIP model [30] as the vision-language model with a ViT-Base and ViT-Huge backbone. We estimated the Hessians separately for the CLIP image and text encoders using the pre-training dataset Laion-400M [35]. For this estimation, we randomly sampled a subset of 3 million data points for the CLIP model with a ViT-Base backbone and 0.5 million data points for the CLIP model with a ViT-Huge backbone. The pre-training dataset was filtered to exclude NSFW content. For the Laplace approximation, we used the GGN approximation of the Hessian matrices as described in Sec. 2 and estimated the covariance matrices \(\bm{A}\) and \(\bm{B}\) for the image and text encoders. We use a grid search to find the Hessian scaling \(\tau\) and learned the optimal prior precision by maximizing the marginal likelihood of the training data. The grid for the Hessian scale was set to \(\tau\in\{0.3,0.35,0.4,0.45,0.5\}\) for the ViT-Base model and \(\tau\in\{0.6,0.65,0.7,0.75,0.8\}\) for the ViT-Huge model.

For the _Office-Home_ and _Flowers_ data sets, we used the pre-defined splits provided by the original authors. For _EuroSAT_, we utilized the splits provided by [37]. For _ImageNet-R_, we divided the provided training set into a training and validation set with a validation ratio of \(0.25\) and used the provided test set as is. Similarly, for the _Food_ and _CIFAR-10/100_ data sets, we split the training set into a training and validation set with a validation ratio of \(0.2\) and used the provided test set without modifications.

[MISSING_PAGE_FAIL:15]

Figure 7: Accuracy and negative log-probability density (NLPD) over subset sizes of the support set across different data sets and subset selection methods using the OpenCLIP huge model variant. Results for random are averaged over 5 seeds.

Figure 8: Accuracy and negative log-probability density (NLPD) over subset sizes of the support set across different data sets and subset selection methods using the OpenCLIP base model variant. Results for random are averaged over 5 seeds.

Figure 10: Illustration of the distribution over cosine similarities, depicting mean and variance, for varying image and text perturbations. We can observe that the mean cosine similarity decreases with increasing perturbation, while the variance increases, indicating that the distribution over cosine similarities captures model uncertainties in out-of-distribution settings.

Figure 9: Results on the Office-Home data set with support set selection from all training domains. We depict the performance of the best performing acquisition function incorporating epistemic uncertainties (), entropy based selection with targeted support set region (), naive random selection (), and random selection with targeted support set candidates ().