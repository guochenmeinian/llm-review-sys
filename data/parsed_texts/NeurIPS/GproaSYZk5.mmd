# Universal In-Context Approximation

By Prompting Fully Recurrent Models

 Aleksandar Petrov, Tom A. Lamb, Alasdair Paren, Philip H.S. Torr, Adel Bibi

Department of Engineering Science

University of Oxford

aleks@robots.ox.ac.uk

###### Abstract

Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions. Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator. While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism. Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve as universal in-context approximators. To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures. LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks. We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation.

## 1 Introduction

Until recently, solving a task with machine learning required training or fine-tuning a model on a dataset matching the task at hand. However, large foundation models exhibit the ability to solve new tasks without being specifically fine-tuned or trained for them: often it is sufficient to simply prompt them in the right way. Prompting has been especially successful because of _in-context learning_: the ability to modify the model's behavior with information provided within the input sequence, without changing the underlying model parameters (Brown et al., 2020). Yet, we know little about the theoretical properties of prompting. It is not even clear if there are limits to what can be achieved with prompting or, conversely, whether it is possible to prompt your way into any behaviour or task.

This can be framed as a universal approximation question. Classically, universal approximation results show how a class of tractable functions, such as neural networks, approximates another class of concept functions, e.g., all continuous functions on a bounded domain, with arbitrary accuracy. This is often done by showing that one can choose _model parameters_ that approximate the target function. However, in-context learning poses a different challenge as the model parameters are _fixed_. Instead, a part of the input (the prompt) is modified to cause the model to approximate the target function. Hence, we define universal _in-context_ approximation to be the property that there exist fixed weights such that the resulting model can be prompted to approximate any function from a concept class. Understanding whether a model can be a universal _in-context_ approximator is especially important as most commercial models are accessible exclusively via a prompting interface (La Malfa et al., 2023).

In-context learning has been almost exclusively studied in conjunction with the transformer architecture (Vaswani et al., 2017). This is likely because in-context abilities appear once the models are large enough (Wei et al., 2021) and most large models have been transformer-based. On the subject of universal in-context approximation, Wang et al. (2023) were first to show that a transformer possesses this property by discretising and memorising all possible functions in the model weights. Memorisation is not needed, though, and even small transformers can be universal approximators when prompted Petrov et al. (2024). Both results, however, critically depend on the attention mechanism of the transformer architecture (Bahdanau et al., 2015).

Still, generative models are not restricted to attention-based architectures: there are the "classic" recurrent neural networks (RNNs, Amari, 1972), long short-term memory models (LSTMs, Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs, Cho et al., 2014). Recently, Linear RNN models (also known as state-space models or SSMs) were proposed as a scalable alternative to the transformer architecture (Orvieto et al., 2023; Fu et al., 2023) and have started to outperform similarly-sized transformers when multiplicative gating is added (Gu and Dao, 2023; De et al., 2024; Botev et al., 2024). Furthermore, despite in-context learning being associated with the transformer, recent empirical results show in-context learning in SSMs, RNNs, LSTMs and even convolutional models (Xie et al., 2022; Akyurek et al., 2024; Lee et al., 2024).

Yet, despite their ability to be in-context learners, there is little known about the theoretical properties of these fully recurrent architectures. As these architectures become more and more widely used, understanding their in-context approximation abilities is increasingly more important for their safety, security and alignment. We show that, in fact, many of these architectures, similarly to transformers, can be universal in-context approximators. Concretely, our contributions are as follows:

1. We develop _Linear State Recurrent Language_ (LSRL): a programming language that compiles to different fully recurrent models. Programming in LSRL is akin to "thinking like a recurrent model". LSRL programs can then be implemented exactly as model weights.
2. Using LSRL, we construct Linear RNN models that can be prompted to act as any token-to-token function over finite token sequences, or to approximate any continuous function. These results also hold for RNNs, LSTMs, GRUs and Hawk/Griffin models (De et al., 2024).
3. We present constructions with and without multiplicative gating. However, we observe that the constructions without these gates depend on numerically unstable conditional logic.
4. Nevertheless, we show that multiplicative gates lead to more compact and numerically stable models, making it more likely that universal in-context approximation properties arise in models utilising them, such as LSTMs, GRUs and the latest generation of Linear RNNs.

## 2 Preliminaries

Fully recurrent architectures.In this work, we focus exclusively on fully recurrent neural network architectures. Recurrent models operate over sequences. Concretely, consider an input sequence \((\bm{x}_{1},\dots,\bm{x}_{N})\) with \(\bm{x}_{t}\in\mathcal{X}\), \(\mathcal{X}\) being some input space. We will refer to the elements of the input sequence as _tokens_ even if they are real-valued vectors. A recurrent model \(g:\mathcal{X}^{\star}\rightarrow\mathcal{Y}\) maps a sequence of inputs to an output in some output space \(\mathcal{Y}\). These models are always causal, namely:

\[\bm{y}_{t}=g(\bm{x}_{1},\dots,\bm{x}_{t}).\] (1)

We will abuse the notation and refer to \((\bm{y}_{1},...,\bm{y}_{t}){=}(g(\bm{x}_{1}),...,g(\bm{x}_{1},...,\bm{x}_{t}))\) as simply \(g(\bm{x}_{1},...,\bm{x}_{t})\). We will also separate the input sequence into a query \((\bm{q}_{1},...,\bm{q}_{n})\) and a prompt \((\bm{p}_{1},...,\bm{p}_{N})\). The prompt specifies the target function \(f\) that we approximate while the query designates the input at which we evaluate it. Contrary to the typical setting, we will place the query before the prompt.1

Footnote 1: That is necessitated by the limited capacity of the state variables. As the model is fixed, in order to increase the precision of the approximation, we can only increase the prompt length. If the prompt is before the query, it would have to be compressed into a fixed-size state, limiting the approximation precision even with increased prompt lengths. But if the query has a fixed size, it can be stored in a fixed-size state variable exactly.

There are various neural network architectures that fall under the general framework of Eq. (1). The quintessential one is the RNN. It processes inputs one by one with only a non-linear state being passed from one time step to the other. A model \(g\) can thus be stacked RNN layers, each one being:

\[\bm{s}_{t} =\sigma(\bm{A}\bm{s}_{t-1}+\bm{B}\bm{x}_{t}+\bm{b}),\] (Classic RNN) \[\bm{y}_{t} =\phi(\bm{s}_{t}),\] (2)with \(\bm{A},\bm{B},\bm{b}\) and the initial state value \(\bm{s}_{0}\) being model parameters, \(\sigma\) a non-linear activation function and \(\phi\) a multi-layer perceptron (MLP) with \(\mathtt{ReLU}\) activations. We assume that \(\sigma\) is always a \(\mathtt{ReLU}\) to keep the analysis simpler. The non-linearity in the state update can make the model difficult to train (vanishing and exploding gradients, Bengio et al., 1994). Therefore, Linear RNNs have been proposed as regularizing the eigenvalues of \(\bm{A}\) can stabilise the training dynamics (Orvieto et al., 2023). Linear RNNs also admit a convolutional representation, making them trainable in parallel (Gu et al., 2021; Fu et al., 2023). Linear RNNs drop the non-linearity from the state update in Eq. (2):

\[\bm{s}_{t} =\bm{A}\bm{s}_{t-1}+\bm{B}\bm{x}_{t}+\bm{b},\] (Linear RNN) \[\bm{y}_{t} =\phi(\bm{s}_{t}).\] (3)

The fully linear state updates do not affect the expressivity of the models, as non-linear activations are nevertheless present in the MLP layers \(\phi\) between the linear state update layers (Wang and Xue, 2023; Boyd and Chua, 1985). The state-of-the-art Linear RNN models also utilise some form of multiplicative gating (Gu and Dao, 2023; De et al., 2024; Botev et al., 2024). While specific implementations can differ, we can abstract it as the following Gated Linear RNN architecture:

\[\bm{s}_{t} =\bm{A}\bm{s}_{t-1}+\bm{B}\bm{x}_{t}+\bm{b},\] (Gated Linear RNN) \[\bm{y}_{t} =\gamma(\bm{x}_{t})\odot\phi(\bm{s}_{t}),\] (4)

with \(\gamma\) being another MLP and \(\odot\) being the element-wise multiplication operation (Hadamard product). Eq. (4) encompasses a range of recently proposed models. For example, one can show that any model consisting of \(L\) stacked Gated Linear RNN layers, with \(\gamma\) and \(\phi\) with \(k\) layers, can be represented as a \(L(k{+}2)\)-layer Hawk or Griffin model (De et al., 2024). The conversions are described in detail in App. E. We can similarly add multiplicative gating to the classic RNN architecture:

\[\bm{s}_{t} =\sigma(\bm{A}\bm{s}_{t-1}+\bm{B}\bm{x}_{t}+\bm{b}),\] (5) \[\bm{y}_{t} =\gamma(\bm{x}_{t})\odot\phi(\bm{s}_{t}),\]

Eq. (5) may appear unusual but it is related to the well-known GRU (Cho et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997) architectures. Same as the case with Griffin/Hawk, any Gated RNN can be represented as a \(L(k{+}2)\)-layer GRU or LSTM model (details in Apps. C and D). As a result, if there exists a Gated RNN model that is a universal in-context approximator (which we later show to be the case), then there also exist GRU and LSTM models with the same property.

Theoretical understanding of in-context learning.Beyond the question of universal in-context approximation, there have been attempts to theoretically understand in-context learning from various perspectives. The ability to learn linear functions and perform optimization in-context has been extensively explored in the context of linear regression (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023; Fu et al., 2023; Zhang et al., 2023; Ahn et al., 2023), kernel regression (Han et al., 2023) and dynamical systems (Li et al., 2023). Furthermore, studies have explored how in-context learning identifies and applies the appropriate pretraining skill (Xie et al., 2022; Coda-Forno et al., 2023; Bai et al., 2023). It has also been shown that transformers can construct internal learning objectives and optimize them during the forward pass (von Oswald et al., 2023; Dai et al., 2023). However, these studies almost exclusively focus on the transformer architecture, and the applicability of their findings to fully recurrent models remains unclear.

Approximation theory.Let \(\mathcal{X}\) and \(\mathcal{Y}\) be normed vector spaces. Take a set of functions \(\mathcal{C}\subseteq\mathcal{Y}^{\mathcal{X}}\) from \(\mathcal{X}\) to \(\mathcal{Y}\) called a _concept space_. Take also a set of nicely behaved functions \(\mathcal{H}\subset\mathcal{Y}^{\mathcal{X}}\), called _hypothesis space_. \(\mathcal{H}\) could be any set that we have tools to construct and analyse, e.g., all polynomials or all neural networks of a particular architectural type. Approximation theory is concerned with how well functions in \(\mathcal{H}\) approximate functions in \(\mathcal{C}\). We say that \(\mathcal{H}\)_universally approximates_\(\mathcal{C}\) over a compact domain \(\mathcal{D}\) (or that \(\mathcal{H}\)_is dense in \(\mathcal{C}\)_) if for every \(f{\in}\mathcal{C}\) and \(\epsilon{>}0\) there exist a \(h{\in}\mathcal{H}\) such that \(\sup_{\bm{x}\in\mathcal{D}}|f(\bm{x}){\text{-}}h(\bm{x})|{\leq}\epsilon\). There is a long history of studying the concept class of continuous functions and hypothesis classes of single hidden layer neural networks (Cybenko, 1989; Barron, 1993) or deeper models (Hornik et al., 1989; Telgarsky, 2015). The concept class of sequence-to-sequence functions has been shown to be universally approximated with the hypothesis classes of transformers (Yun et al., 2019), RNNs (Schafer and Zimmermann, 2006) and Linear RNNs (Wang and Xue, 2023).

The hypothesis spaces in this work are different. The model is fixed and only the prompt part of the input is changed, i.e., all learnable parameters are in the prompt. Take a recurrent model \(g\) as in Eq. (1) with _fixed_ model parameters and a query length \(n\). The hypothesis class is all functions that result by calling \(g\) on the user query followed by the prompt and taking the last \(n^{\prime}\) outputs:

\[\mathcal{H}_{g}^{\mathcal{D}^{n}}=\{(\bm{q}_{1},\dots,\bm{q}_{n})\mapsto g(\bm{ q}_{1},\dots,\bm{q}_{n},\bm{p}_{1},\dots,\bm{p}_{N})[\neg n^{\prime}\,;]\mid \forall\bm{p}_{i}\in\mathcal{D},N>0\}.\] (6)The domain \(\mathcal{D}\) of \(\bm{p}_{i}\) and \(\bm{q}_{i}\) can be continuous embeddings in \(\mathbb{R}^{d}\) or discrete tokens \(\mathcal{V}=\{1,...,V\}\). Note that each \(h{\in}\mathcal{H}_{g}\) is identified by a prompt \((\bm{p}_{1},...,\bm{p}_{N})\) but is a function with domain all possible queries \((\bm{q}_{1},...,\bm{q}_{n})\). Therefore, finding a hypothesis \(h{\in}\mathcal{H}_{g}\) that approximates a target function \(f\) is equivalent to finding the prompt of that hypothesis. The approximation properties of \(\mathcal{H}_{g}\) in Eq. (6) depend on the architecture of \(g\), as well as its specific parameters.

We study the recurrent architectures in Eqs. (2) to (5) and their ability to approximate continuous functions over real-valued vectors and to represent discrete maps over tokens (which corresponds to how language models are used in practice). We consider the following classes of functions. \(\mathcal{C}^{\text{vec}}{=}(\mathbb{R}^{d_{\text{adv}}})^{[0,1]^{d_{n}}}\) contains all continuous functions from the unit hypercube to \(\mathbb{R}^{d_{\text{adv}}}\), while \(\mathcal{C}^{\text{tok}}{=}\{h{\in}(\mathcal{V}^{l})^{\mathcal{V}^{l}}\mid h \text{ causal}\}\) all causal functions from \(l\) tokens to \(l\) tokens. The hypothesis classes are \(\mathcal{H}^{\text{vec}}(g)\) corresponding to Eq. (6) with \(D{=}[0,1]^{d_{n}},n{=}n^{\prime}{=}1\) and \(g\) some _fixed_ model of one of the four architectures in Eqs. (2) to (5), and \(\mathcal{H}^{\text{tok}}(g)\) with \(D{=}\mathcal{V}\) and \(n{=}n^{\prime}{=}l\).

## 3 Linear State Recurrent Language (LSRL)

We can construct the weights for universal in-context models with the architectures in Eqs. (2) to (5) by hand but this is labour-intensive, error-prone, difficult to interpret, and the specific weights would be architecture-dependent. Working at such a low level of abstraction can also obfuscate common mechanisms and design patterns, making it more difficult to appreciate both the capabilities and the constraints of fully recurrent architectures. Instead, we propose a new programming language: _Linear State Recurrent Language_ (LSRL).2 LSRL programs compile to the four architectures in Eqs. (2) to (5). Conversely, any Linear RNN can be represented as an LSRL program, making LSRL a versatile tool for studying the capabilities of recurrent models. Later, in Secs. 4 to 6 we make use of LSRL to develop programs that are universal approximators for \(\mathcal{C}^{\text{vec}}\) and \(\mathcal{C}^{\text{tok}}\), thus showing that all four architectures can be universal in-context approximators.

Footnote 2: Our implementation of LSRL is available at https://github.com/AleksandarPetrov/LSRL

LSRL syntax.An LSRL program specifies how a single element is processed and how the recurrent states are updated for the next element. LSRL programs always start with an \(\mathbf{Input}(\bm{x})=\bm{x}\) and an \(\bm{x}\) of a fixed dimension. Only one Input can be declared in a program. Linear layers and \(\mathsf{RelUs}\) are also supported: \(\mathsf{Lin}[\bm{A},\bm{b}](\bm{x}):=\bm{A}\bm{x}+\bm{b}\), \(\mathsf{RelU}(\bm{x}):=\max(\bm{0},\bm{x})\). The unique component of LSRL, however, is its \(\mathsf{LinState}\) operation implementing the linear state update in Linear RNNs (Eq. (3)): \(\mathsf{LinState}[\bm{A},\bm{B},\bm{b},\bm{s}_{0}](\bm{x}_{t}):=\bm{A}\bm{s}_{t -1}+\bm{B}\bm{x}_{t}+\bm{b}\), where the state \(\bm{s}_{t-1}\) is the output of the call this node at step \(t-1\). \(\mathsf{LinState}\) is the only way information can be passed from previous tokens to the current one. We also provide a \(\mathsf{Concat}\) operation that combines

Figure 1: **Compilation of an LSRL program to a Linear RNN.** An example of a simple LSRL program that takes a sequence of 0s and 1s as an input and outputs 1 if there have been more 1s than 0s and 0 otherwise. The LSRL compiler follows the rules in App. A to simplify the computation DAG into a path graph. The resulting path graph can be represented as a Linear RNN with one layer.

variables: \(\texttt{Concat}(\bm{x},\bm{y}):=(\bm{x}_{1},...,\bm{x}_{|\bm{x}|},\bm{y}_{1},..,\bm{y}_{|\bm{y}|})\). Finally, to support gating architectures we also implement a rudimentary Multi operation that splits its input into two sub-arrays and returns their element-wise multiplication: \(\texttt{Multi}(\bm{x}):=\bm{x}\texttt{\{\{}:|\bm{x}|/_{2}\texttt{\}}}\odot \bm{x}\texttt{\{\{}:|\bm{x}|/_{2}\texttt{\}}}\odot\bm{x}\texttt{\{\{}:|\bm{x }|/_{2}\texttt{\}}}:\). Naturally, Multi requires that \(\bm{x}\) has even length. These six operations can be composed into a direct acyclic graph (DAG) with a single source node (the Input variable) and a single sink node (marked with a return statement). Such a program operates over a single token \(\bm{x}_{t}\) passed to Input, while a recurrent model needs to operate over sequences. Thus, we wrap the program into a ForEach loop that passes each element individually for the DAG to output a variable denoted by a return clause. Each element is processed by the exact same program, with the only difference being that the state of the LinState variables is changing between iterations. You can see an example of a small LSRL program in Fig. 1.

Expressiveness limitations.ForEach does not behave like the typical for loop: only the states are accessible between iterations, i.e., you cannot use the output of a linear layer at step \(t\) in any computation at step \(t+1\). Furthermore, as the program is a DAG and only states of LinState nodes are passed between iterations, variables computed in latter operations of a previous time step are not accessible as inputs in earlier layers (with respect to the topological sorting of the computation graph). This leads to a key programming paradigm in LSRL: a LinState update cannot depend non-linearly on its own state. That includes it depending on a variable that depends on the LinState itself and conditional updates to the state. Such a dependency would break the DAG property of the program.3 This poses serious limitations on what algorithms can be expressed in a Linear RNN and makes programming them challenging. Still, in Sec. 4 we show how carefully constructing state updates and auxiliary variables can nevertheless allow to program some limited conditional behaviours.

Footnote 3: For example, we cannot implement an operation that adds one to the state and squares it at each time step: \(s_{t+1}=(s_{t}+1)^{2}\) or an operation that performs conditional assignment \(s_{t+1}=0\) if \((s_{t}>5)\) else \(s_{t}\).

Complication.Any LSRL program without Multi nodes can be compiled to a Linear RNN (Eq. (3)) or to a Gated Linear RNN (Eq. (4)). If the program has Multi nodes, then it cannot be compiled to a Linear RNN as the multiplicative gating cannot be implemented exactly. However, it can be compiled to a Gated Linear RNN. To compile an LSRL program to a Linear (Gated) RNN, we first parse the program to build a computation graph. This is a DAG with a single source (the Input node) and a single sink (the return statement of the ForEach loop). At the same time, a Linear (Gated) RNN can be represented as a path graph (no branching) with the six basic operations as nodes. Therefore, the compilation step needs to transform this DAG into a path graph. We achieve that by iterativly collapsing the first branching point into a single node. The exact rules that achieve that are described in App. A. Later, in Sec. 6, we will show how any Linear (Gated) RNN can be converted into a _non-linear_ (Gated) RNN, hence, how we can compile LSRL programs to these architectures as well.

Syntactic sugar.To make programming easier, we define several convenience functions. For instance, we can Slice variables \(\bm{x}[l{:}u]\) via sparse Lin layers. We can also sum variables and element-wise multiplication with scalars (implemented as Lin layers). For logical operations we also need step functions which can be approximated with RelUs: \(\texttt{f\_step}[\mu](\bm{x}):=\texttt{ReLU}(\mu\bm{x})-\mu\texttt{ReLU}(\bm{x }-\nicefrac{{1}}{{\mu}})\), where \(\mu\) is a positive constant controlling the quality of the approximation. We can also approximate bump functions (1 between \(l\) and \(u\) and 0 otherwise): \(\texttt{f\_bump}[\bm{l},\bm{u},\mu](\bm{x}):=\texttt{f\_step}[\mu](\bm{x}-\bm{ l})-\texttt{f\_step}[\mu](\bm{x}-\bm{u})\). Similarly, we can approximate conjunction (\(\texttt{f\_and}\)), disjunction (\(\texttt{f\_or}\)), negation (\(\texttt{f\_not}\)), and comparison operators (\(\texttt{f\_larger}\) and \(\texttt{f\_smaller}\)). See App. F for the definitions.

Critically, we need also a conditional operator that assigns a value \(\texttt{t}(\bm{x})\) if a certain condition is met and another value \(\texttt{f}(\bm{x})\) otherwise. One way to implement this is:

\[\begin{split}\texttt{f\_ifelse}[\texttt{cond},\texttt{t}, \texttt{f},\lambda](\bm{x})&:=\texttt{ReLU}(\neg\lambda\, \texttt{cond}(\bm{x})\texttt{+f}(\bm{x}))+\texttt{ReLU}(\neg\lambda\,\texttt{ f\_not}(\texttt{cond}(\bm{x}))\texttt{+t}(\bm{x}))\\ &-\texttt{ReLU}(\neg\lambda\,\texttt{cond}(\bm{x})\texttt{-f}( \bm{x}))-\texttt{ReLU}(\neg\lambda\,\texttt{f\_not}(\texttt{cond}(\bm{x})) \texttt{-t}(\bm{x})),\end{split}\] (7)

where \(\lambda\) is a constant that is larger than any absolute value that \(\texttt{t}(\bm{x})\) and \(\texttt{f}(\bm{x})\) can attain. This construction, however, is not numerically stable and we will study alternatives in Sec. 5. We provide both numerical (SciPy.sparse, Virtanen et al.2020) and symbolic (SymPy, Meurer et al.2017) backends with the second being crucial for programs that are not numerically stable.

Prior work on encoding algorithms in model weights.A similar approach to developing a programming language that compiles to model weights was already done for the transformer architecture with the RASP language (Weiss et al., 2021) and the Tracr compiler (Lindner et al., 2023). They were predominantly created as a tool for interpretability research. In a sense, RASP is to a transformer as LSRL is to a (Linear) (Gated) RNN. Hence, can be used to develop benchmarks for interpretability methods for fully-recurrent architectures. However, while RASP can only express a subset of transformer models, LSRL is isomorphic to the set of all (Gated) Linear RNNs (though not to the non-linear ones). That means that any (Gated) Linear RNN can be represented and analysed as an LSRL program and vice versa. Hence, the limitations of what you can express in LSRL are also limitations of what a Linear (Gated) RNN can do. Namely: (_i_) we cannot have exact multiplicative interactions between inputs without multiplicative gates, and (_ii_) we cannot have state variable updates depending non-linearly on their previous iterations or in any way on a variable that depends on them.

Figure 2: **Intuition behind the LSRL program for universal in-context approximation for continuous functions in List. 1. Our target function \(f\) has input dimension \(d_{\text{in}}=2\) and output dimension \(d_{\text{out}}=1\). Each input dimension is split into two parts, hence \(\delta=\nicefrac{{1}}{{2}}\). We illustrated an example input sequence of length 5: one for the query and four for the prompt tokens corresponding to each of the discretisation cells. The query \((q_{1},q_{2})\) falls in the cell corresponding to the third prompt token. We show how the two LinState variables in the program are updated after each step. Most notably, how the state holding the output y is updated after \(\bm{p}_{3}\) is processed.**

## 4 Universal In-Context Approximation with Linear RNNs

We proceed with building LSRL programs that are universal in-context approximators: one for approximating continuous functions (\(\mathcal{C}^{\text{vec}}\)), and one for maps between token sequences (\(\mathcal{C}^{\text{tok}}\)).

### Approximating continuous functions in \(\mathcal{C}^{\text{vec}}\)

The idea behind the approximation for continuous functions is to discretise the domain into a grid and approximate the function as constant in each cell of the grid. This technique is commonly used for showing universal approximation using the step activation function (Blum and Li, 1991; Scarselli and Tsoi, 1998). However, it is not obvious how to implement this approach in-context when information across input tokens can be only combined linearly. Consider a target function \(f:[0,1]^{d_{\text{in}}}{\rightarrow}[0,1]^{d_{\text{out}}}\) and a discretization step \(\delta\). Our approach is to describe the value of \(f\) in each of the discretization cells as a single prompt token. For the cell with lower bounds \(l_{1},\ldots,l_{d_{\text{in}}}\) and their respective upper bounds \(l_{1}{+}\delta,...,l_{d_{\text{in}}}{+}\delta\), the corresponding prompt token is a \((d_{\text{in}}{+}d_{\text{out}}{+}1)\)-dimensional vector:

\[\bm{p}=[\delta,l_{1},\ldots,l_{d_{\text{in}}},\bm{\bar{y}}_{1},\ldots\bm{\bar {y}}_{d_{\text{out}}}]^{\top},\] (8)

where \(\bm{\bar{y}}\) is the value of \(f\) at the centre of that cell: \(\bm{\bar{y}}=f(l_{1}{+}\nicefrac{{\delta}}{{2}},...,l_{d_{\text{in}}}{+} \nicefrac{{\delta}}{{2}})\). Each prompt token describes the size of the cell (the discretisation step \(\delta\)), its starting lower bound, and the value of the target function at the centre of the cell. Thus, \(\left\lceil\nicefrac{{1}}{{\delta}}\right\rceil^{d_{\text{in}}}\) such tokens, one for each cell, are sufficient to describe the piece-wise constant approximation of \(f\). A query \(\bm{q}^{\prime}\in[0,1]^{d_{\text{in}}}\) can fall in only one of the cells. We pad it with zeros and encode it as the first input element: \(\bm{q}=\left[\bm{q}^{\prime\top},\bm{0}_{d_{\text{out}}}^{\top}\right]^{\top}\), followed by the prompt. Our program will extract and save \(\bm{q}^{\prime}\) to a state and then process the prompt tokens one at a time until it finds the one whose cell contains \(\bm{q}^{\prime}\). The target function value for this cell will be added to an accumulator state. If the current cell does not contain \(\bm{q}^{\prime}\), then \(0\) is instead added.Hence, the accumulator's final value corresponds to the value of \(f\) at the centre of the cell containing \(\bm{q}^{\prime}\). The full LSRL program is provided in Lst. 1 and an illustration for \(d_{\text{in}}=2,d_{\text{out}}=1,\delta=\nicefrac{{1}}{{2}}\) is shown in Fig. 2. The prompt length required to approximate an \(L\)-Lipschitz function \(f\) (w.r.t. the \(\ell_{2}\) norm) to precision \(\epsilon\) is \(N=(\nicefrac{{2\epsilon}}{{L}}/\nicefrac{{\mathcal{M}_{\text{in}}}}{{2}})^{- d_{\text{in}}}=\mathcal{O}(\epsilon^{\epsilon^{d_{\text{in}}}})\) (see App. B for the proof). Asymptotically, this is as good as one can hope without further assumptions on the target function. This is also better than the best known result for the same problem for transformers: \(\mathcal{O}(\epsilon^{\text{-10-14}d_{\text{in}}{+}d_{\text{in}}^{2}})\) in Petrov et al. 2024.

### Approximating functions over token sequences in \(\bm{C}^{\text{tok}}\)

Sec. 4.1 focused on continuous functions but recurrent architectures are often used to model natural language whose domain is tokens. Thus, we also look at modelling maps over a discrete domain. Any function from \(n\) tokens to \(n\) tokens taking values in \(\mathcal{V}=\{1,\ldots,V\}\) can be represented as a dictionary whose keys and values are in \(\mathcal{V}^{n}\). Therefore, a simple way to represent this function in-context is to first provide the \(n\) tokens corresponding to the query and then a sequence of \(2n\) tokens corresponding to key and value pairs (see Fig. 3 for an illustration of the setup). The model stores the

Figure 3: **Intuition behind the LSRL program for universal in-context approximation for discrete functions in Lst. 2. Our keys and values have length \(n{=}3\) and represent countries and capitals, e.g., \(\mathsf{AUStria}{\rightarrow}\mathsf{VIEnna}\), \(\mathsf{BULgaria}{\rightarrow}\mathsf{SOFia}\), and so on. The query is \(\mathsf{CAN}\) for Canada and the final \(n\) outputs are \(\mathsf{OTT}\) (Ottawa). We show the values of some of the variables in Lst. 2 at each step, with the \(\mathsf{LinState}\) variables being marked with arrows. For cleaner presentation we are tokenizing letters as \(\emptyset{\rightarrow}7\), \(1{\rightarrow}\mathsf{A}\), \(2{\rightarrow}\mathsf{B}\), etc. Vertical separators are for illustration purposes only.**query in a state and processes the key-value pairs one by one by comparing the key (the first \(n\) tokens) with the query. If they match, then the value (the next \(n\) tokens) is copied into a state that keeps it and repeatedly outputs it. This continues until the end of the prompt, at which point the last \(n\) outputted tokens will be the value corresponding to the key matching the query. This is essentially a dictionary lookup. However, as shown in Lst. 2, implementing dictionary lookup in a linear recurrent model is much less straightforward than executing dict[key] in a general-purpose programming language.

Lst. 2 can appear daunting at first so we would like to clarify the non-trivial aspects. First, we need to count how far we are into every set of \(n\) or \(2n\) tokens. This can be done with \(\mathrm{mod}\,n\) and \(\mathrm{mod}\,2n\) operations but implementing modulo for arbitrary large inputs is not possible with ReLU MLPs (Ziyin et al., 2020). Therefore, we implement this with LinState as f_modulo_counter which has a unit-length state that is rotated \(\nicefrac{{1}}{{n}}\) or \(\nicefrac{{1}}{{2n}}\) revolutions per iteration, with the angle corresponding to the modulo value (App. F.7). Second, we need to do dynamic indexing to copy the \(i\)-th input in a subsequence to the \(i\)-th element of a state and vice-versa. Dynamic indexing, however, cannot be succinctly represented in a Linear RNN. We work around this with temporary variables that are non-zero only at the \(i\)-th coordinates (see Lines 16, 17, 19, 20, 32 to 35, 37 and 38). Finally, in order to compare whether all \(n\) elements in the query and the key match, we need to remember whether the previous \(n\) pairs were matching. As RNNs do not have attention, we implement this short-term memory buffer as a LinState with a shift matrix (Line 23).

## 5 Stable Universal In-Context Approximation with Gated Linear RNNs

The ReLU-based conditional operator is not numerically stable.The LSRL programs in Lsts. 1 and 2 for approximating functions in respectively \(\mathcal{C}^{\text{vec}}\) and \(\mathcal{C}^{\text{tok}}\) rely on the f_ifelse conditional assignment operator in Eq. (7) in order to implement different behaviours depending on whether we are processing the query or specific parts of the prompt. This operator is not numerically stable. The first term in Eq. (7) relies on \(\mathsf{cond}(\bm{x})\) being exactly zero if the condition is not met. In this way, multiplying it with \(-\lambda\) would be 0 and \(\mathsf{f}(\bm{x})\) would be returned. However, if \(\mathsf{cond}(\bm{x})\) is not identically 0 but has a small positive value, then \(-\lambda\mathsf{cond}(\bm{x})\) can "overpower" \(\mathsf{f}(\bm{x})\) resulting in the ReLU output being 0. In our experience, this is not a problem when processing inputs through the LSRL program step-by-step. However, de-branching the DAG into a path graph --which is necessary in order to uncover the equivalent Linear RNN-- appears to introduce such numerical instabilities which occasionally result in wrong outputs as conditional assignments will be 0 when they should not. This problem is more prominent in Lst. 2 which is longer (more debranching steps) and has more f_ifelse operations: it gets most tokens wrong because of that instability (see _Original, No noise_ in Fig. 4). To this end, we support LSRL with a symbolic backend (based on SymPy) that performs the debranching steps exactly. Using it, both programs always produce the correct output.

This numerical instability highlights a critical practical limitations of the universal approximation results in Sec. 4: if the models are not numerically stable, it is unlikely that they occur in practice by training models using gradient descent. This section shows how to improve the numerical stability of Eq. (7) and obtain more realistic recurrent models that are universal approximators in-context.

Removing unnecessary terms in Eq. (7).Eq. (7) has 4 separate ReLU terms. The first two handle the cases when \(\mathsf{t}(\bm{x})\) and \(\mathsf{f}(\bm{x})\) are positive and the second two when they are negative. Therefore, if we know that one or both of these will always be non-negative, we can drop the corresponding terms. Additionally, if \(\mathsf{f}(\bm{x})\) is always \(0\), then the first and third terms can be safely dropped. Similarly, the second and fourth are unnecessary if \(\mathsf{f}(\bm{x})\equiv 0\). All f_ifelse in Lsts. 1 and 2 fall in this case and hence can be simplified. We will refer to this f_ifelse implementation that is aware of the attainable values of \(\mathsf{t}(\bm{x})\) and \(\mathsf{f}(\bm{x})\) as optimized. As it reduces the number of numerically unstable ReLU operations in the model, we expect that it will improve the stability of the compiled models. We experimented with adding various levels of noise to the non-zero model parameters, and, as the results in Fig. 4 show, optimized is indeed more numerically robust than original.

Step-based implementation.We can get rid of the input sensitivity of Eq. (7) using f_step:

\[\begin{split}\texttt{f\_ifelse}[\mathsf{cond},\mathsf{t}, \mathsf{f},\lambda](\bm{x})&:=\texttt{ReLU}(-\lambda\star \lambda\texttt{f\_step}(\nicefrac{{1}}{{2}}\!-\!\mathsf{cond}(\bm{x}))\!+ \!\mathsf{f}(\bm{x}))+\texttt{ReLU}(-\lambda\star\lambda\texttt{f\_step}( \mathsf{cond}(\bm{x})-\nicefrac{{1}}{{2}})\!+\!\mathsf{t}(\bm{x}))\\ &-\texttt{ReLU}(-\lambda\star\lambda\texttt{f\_step}(\nicefrac{{ 1}}{{2}}\!-\!\mathsf{cond}(\bm{x}))\!-\!\mathsf{f}(\bm{x}))-\texttt{ReLU}(- \lambda\star\lambda\texttt{f\_step}(\mathsf{cond}(\bm{x})-\nicefrac{{1}}{{2}} )\!-\!\mathsf{t}(\bm{x}))).\end{split}\] (9)

We can also apply the optimisation strategy here. While this implementation is robust to noise in the input it appears to be more sensitive to parameter noise, as shown in Fig. 4.

Figure 4: **Robustness of the various f_ifelse implementations to model parameter noise.** We show how the performance of the two universal approximation programs in Lsts. 1 and 2 deteriorates as we add Gaussian noise of various magnitudes to the non-zero weights of the resulting compiled models. As expected, the original f_ifelse implementation in Eq. (7) exhibits numerical precision errors at the lowest noise magnitude. For the token sequence case, numerical precision errors are present in all samples even in the no-noise setting. Hence, the original f_ifelse implementation is less numerically robust while the implementations with multiplicative gating are the most robust. For Lst. 1 (approximating \(\mathcal{C}^{\text{vec}}\)) we report the Euclidean distance between the target function value and the estimated one over 10 queries for 25 target functions. For Lst. 2 we report the percentage of wrong token predictions over 5 queries for 25 dictionary maps. Lower values are better in both cases.

Numerically stable f_ifelse with multiplicative gates.Removing the unused ReLU terms in the original f_ifelse reduces the opportunities for numerical precision issues to creep in but does not solve the underlying problem. The multiplicative gating present in the Linear Gated RNN (Eq.4) and Gated RNN models (Eq.5) can help via implementing a numerically stable conditional operator:

\[\text{f\_ifelse}[\texttt{cond},\texttt{t},\texttt{f}](\bm{x}):=\texttt{cond}( \bm{x})\odot\texttt{t}(\bm{x})+\texttt{f\_not}(\texttt{cond}(\bm{x}))\odot \texttt{f}(\bm{x}),\] (10)

where the element-wise product is implemented in LSRL with Concat and Multi. We will refer to the implementation of f_ifelse in Eq.10 as multiplicative. Similarly to original implementation of f_ifelse in Eq.7, we can drop the \(\texttt{t}(\bm{x})\) and \(\texttt{f}(\bm{x})\) term if they are equal to zero (multiplicative optimized). If \(\texttt{cond}(\bm{x})\) is not exactly zero, \(\texttt{cond}(\bm{x})\odot\texttt{t}(\bm{x})\) will result in a small error to the output but, in contrast to the original implementation, is not going to cause a discontinuity in the output of the operation. Therefore, Eq.10 should be more robust to numerical precision issues than Eq.7. Fig.4 shows that this is the case in practice with Lsts.1 and 2 being more robust to parameter noise when using multiplicative gates compared to the ReLU-based implementations. Therefore, Linear Gated RNNs (Eq.4) --to which models with multiplicative gates can be compiled-- are more likely than Linear RNNs (Eq.3) to exhibit universal approximation properties in practice.

## 6 Universal In-context Approximation with Non-linear (Gated) RNNs

Secs.4 and 5 showed how universal approximation of continuous and token-to-token functions can be implemented in LSRL and compiled to respectively Linear RNNs and Linear Gated RNNs. This section aims to address the situation with _non-linear_ state updates, that is, the cases of classic and gated RNNs (Eqs.2 and 5). Concretely, we show how every _linear_ (Gated) RNN can be converted to a _non-linear_ (Gated) RNN. The key idea is that the ReLU applied to the state updates in the non-linear architectures is an identity operation if its inputs are positive. Hence, we can split the states in positive and negative components, flip the sign of the negative component, pass them separately through the ReLU--which will act as an identity as all elements will be non-negative-- and then fuse the positive and negative components back together in the \(\bm{A}\) matrix at the next time step:

\[\begin{array}{l}\bm{s}_{t}=\bm{A}\bm{s}_{t-1}+\bm{B}\bm{x}_{t}+\bm{b}\\ \bm{y}_{t}=\phi(\bm{s}_{t}).\end{array}\quad\equiv\quad\begin{array}{l}\begin{bmatrix} \bm{s}_{t}^{+}\\ \bm{s}_{t}^{-}\end{bmatrix}=\texttt{ReLU}\left(\begin{bmatrix}\bm{A}&-\bm{A} \\ -\bm{A}&\bm{A}\end{bmatrix}\begin{bmatrix}\bm{s}_{t}^{+}\\ \bm{s}_{t}^{-}\end{bmatrix}+\begin{bmatrix}\bm{B}\\ -\bm{B}\end{bmatrix}\bm{x}_{t}+\begin{bmatrix}\bm{b}\\ -\bm{b}\end{bmatrix}\right)\\ \bm{y}_{t}=\phi\left(\begin{bmatrix}\bm{I}&-\bm{I}\end{bmatrix}\begin{bmatrix} \bm{s}_{t}^{+}\\ \bm{s}_{t}^{-}\end{bmatrix}\right).\end{array}\] (11)

Using Eq.11 we can compile any LSRL program to an RNN (Eq.2) or a Gated RNN (Eq.5). This includes Lsts.1 and 2. Hence, RNNs and Gated RNNs can be universal in-context approximators for continuous and token-to-token functions. As any Gated RNN can be represented as a GRU model (App. C) or an LSTM (App. D), these models are too universal in-context approximators.

## 7 Discussion and Conclusions

We developed LSRL: a programming language for specifying programs expressible with recurrent neural architectures. We then used LSRL to show that various architectures --from the humble RNN to the state-of-the-art Linear Gated RNNs-- can all be universal approximators _in-context_.

Safety and security implications.If a model can be prompted to approximate any function, then preventing it from exhibiting undesirable behaviours (i.e., alignment) might be impossible. Therefore, it is important to further study the safety and security implications of these properties.

Limitations.In this work we provide _constructive existence results_: that is, we show that there can exist models with various recurrent architectures that are universal in-context approximators. However, the present theory is not sufficient to analyse whether _a given model_ has this property. That is a much more difficult question that would require a very different approach. We also assume no restrictions on the \(\bm{A}\) matrix in the state update equations. However, many state-of-the-art models impose structural constraints on \(\bm{A}\) (e.g., it being diagonal) for the sake of fast training and inference (Gu et al., 2020, 2021; Gupta et al., 2022). It is not directly obvious whether such structural restrictions would affect the universal in-context approximation abilities of these architectures. In practice, however, the compiled matrices are very sparse and often diagonal. Therefore, it is highly likely that our results translate to models with structural restrictions.

## Acknowledgements

We would like to thank Simon Schug for pointing us to relevant works on fully recurrent models. We are also extremely grateful to Juuso Haavisto for his insight on building compilers. This work is supported by a UKRI grant Turing AI Fellowship (EP/W002981/1) and the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (EP/S024050/1). Adel Bibi acknowledges funding from the KAUST Office of Sponsored Research (OSR-CRG2021-4648) and support from Google Cloud through the Google Gemma 2 Academic Program GCP Credit Award.

## References

* Ahn et al. (2023) Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. 2023. Transformers learn to implement preconditioned gradient descent for in-context learning. In _Advances in Neural Information Processing Systems_.
* Akyurek et al. (2022) Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? Investigations with linear models. In _The Eleventh International Conference on Learning Representations_.
* Akyurek et al. (2024) Ekin Akyurek, Bailin Wang, Yoon Kim, and Jacob Andreas. 2024. In-context language learning: Arhitectures and algorithms. _arXiv preprint arXiv:2401.12973_.
* Amari (1972) Shun-ichi Amari. 1972. Learning patterns and pattern sequences by self-organizing nets of threshold elements. _IEEE Transactions on Computers_, C-21(11):1197-1206.
* Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In _International Conference on Learning Representations_.
* Bai et al. (2023) Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. 2023. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In _Advances in neural information processing systems_.
* Barron (1993) Andrew R Barron. 1993. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information Theory_, 39(3):930-945.
* Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_.
* Bengio et al. (1994) Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. _IEEE Transactions on Neural Networks_, 5(2):157-166.
* Blum and Li (1991) Edward K Blum and Leong Kwan Li. 1991. Approximation theory and feedforward networks. _Neural networks_, 4(4):511-515.
* Botev et al. (2024) Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, et al. 2024. RecurrentGemma: Moving past transformers for efficient open language models. _arXiv preprint arXiv:2404.07839_.
* Boyd and Chua (1985) Stephen Boyd and Leon Chua. 1985. Fading memory and the problem of approximating nonlinear operators with Volterra series. _IEEE Transactions on Circuits and Systems_, 32(11):1150-1161.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_.
* Cho et al. (2014) Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_.
* Coda-Forno et al. (2023) Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, and Eric Schulz. 2023. Meta-in-context learning in large language models. In _Advances in Neural Information Processing Systems_, pages 65189-65201.
* Coda-Forno et al. (2020)George Cybenko. 1989. Approximation by superpositions of a sigmoidal function. _Mathematics of control, signals and systems_, 2(4):303-314.
* Dai et al. (2023) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can GPT learn in-context? Language models secretly perform gradient descent as meta-optimizers. In _Findings of the Association for Computational Linguistics: ACL 2023_.
* De et al. (2024) Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. 2024. Griffin: Mixing gated linear recurrences with local attention for efficient language models. _arXiv preprint arXiv:2402.19427_.
* Fu et al. (2023a) Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. 2023a. Hungry Huppy: Towards language modeling with state space models. In _International Conference on Learning Representations_.
* Fu et al. (2023b) Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. 2023b. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. _arXiv preprint arXiv:2310.17086_.
* Garg et al. (2022) Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. 2022. What can transformers learn in-context? A case study of simple function classes. In _Advances in Neural Information Processing Systems_.
* Gers et al. (2000) Felix A Gers, Jurgen Schmidhuber, and Fred Cummins. 2000. Learning to forget: Continual prediction with lstm. _Neural computation_, 12(10):2451-2471.
* Gu and Dao (2023) Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_.
* Gu et al. (2020) Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. 2020. HiPPO: Recurrent memory with optimal polynomial projections. In _Advances in Neural Information Processing Systems_.
* Gu et al. (2021) Albert Gu, Karan Goel, and Christopher Re. 2021. Efficiently modeling long sequences with structured state spaces. In _International Conference on Learning Representations_.
* Gupta et al. (2022) Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal state spaces are as effective as structured state spaces. In _Advances in Neural Information Processing Systems_.
* Han et al. (2023) Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. 2023. In-context learning of large language models explained as kernel regression. _arXiv preprint arXiv:2305.12766_.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. _Neural Computation_, 9(8):1735-1780.
* Hornik et al. (1989) Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366.
* La Malfa et al. (2023) Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Raza Nazar, Anthony G. Cohn, Nigel Shadbolt, and Michael Wooldridge. 2023. Language Models as a Service: Overview of a new paradigm and its challenges. _arXiv preprint arXiv:2309.16573_.
* Lee et al. (2024) Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. 2024. Exploring the relationship between model architecture and in-context learning ability. In _International Conference on Learning Representations_.
* Li et al. (2023) Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023. Transformers as algorithms: Generalization and stability in in-context learning. In _International Conference on Machine Learning_.
* Lindner et al. (2023) David Lindner, Janos Kramar, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir Mikulik. 2023. Tracr: Compiled transformers as a laboratory for interpretability. In _Advances in Neural Information Processing Systems_.
* Li et al. (2023)Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondrej Certik, Sergey B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Stepan Roucka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Scopatz. 2017. SymPy: Symbolic computing in Python. _PeerJ Computer Science_, 3.
* Orvieto et al. (2023) Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. 2023. Resurrecting recurrent neural networks for long sequences. In _International Conference on Machine Learning_.
* Petrov and Tsoi (2024) Aleksandar Petrov, Philip HS Torr, and Adel Bibi. 2024. Prompting a pretrained transformer can be a universal approximator. In _International Conference on Machine Learning_.
* Scarselli and Tsoi (1998) Franco Scarselli and Ah Chung Tsoi. 1998. Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results. _Neural networks_, 11(1):15-37.
* Schafer and Zimmermann (2006) Anton Maximilian Schafer and Hans Georg Zimmermann. 2006. Recurrent neural networks are universal approximators. In _Artificial Neural Networks-ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16_, pages 632-640. Springer.
* Shazeer (2019) Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. _arXiv preprint arXiv:1911.02150_.
* Telgarsky (2015) Matus Telgarsky. 2015. Representation benefits of deep feedforward networks. _arXiv preprint arXiv:1509.08101_.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Advances in Neural Information Processing Systems_.
* Virtanen et al. (2020) Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental algorithms for scientific computing in Python. _Nature Methods_, 17:261-272.
* Oswald et al. (2023a) Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023a. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_.
* Oswald et al. (2023b) Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, and Joao Sacramento. 2023b. Uncovering mesa-optimization algorithms in transformers. _arXiv preprint arXiv:2309.05858_.
* Wang and Xue (2023) Shida Wang and Beichen Xue. 2023. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. In _Advances in Neural Information Processing Systems_.
* Wang et al. (2023) Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. 2023. Universality and limitations of prompt tuning. _Advances in Neural Information Processing Systems_.
* Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_.
* Weiss et al. (2021) Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021. Thinking like transformers. In _International Conference on Machine Learning_.
* Weckesser et al. (2020)Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit Bayesian inference. In _International Conference on Learning Representations_.
* Yun et al. (2019) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. 2019. Are transformers universal approximators of sequence-to-sequence functions? In _International Conference on Learning Representations_.
* Zhang et al. (2023) Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. 2023. Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_.
* Ziyin et al. (2020) Liu Ziyin, Tilman Hartwig, and Masahito Ueda. 2020. Neural networks fail to learn periodic functions and how to fix it. In _Advances in Neural Information Processing Systems_.

[MISSING_PAGE_FAIL:15]

Case 5A: Only Lin nodes and they are all Slices.This is one of the more challenging cases. While the Slice nodes are simply Lin nodes with special structure, we cannot treat them like standard Lin nodes (see Case 5B). While we can merge them into a single Lin node, we will then need further Slices to extract the relevant subspaces for the subsequent nodes. Therefore, we would be simply replacing Slice nodes with Slice nodes. Instead, we use the observation that Slice nodes can be fused with subsequent Lin and LinState nodes and can be pushed after ReLU and Concat nodes. Therefore we treat each subsequent node differently, depending on its type:

* If there are Multi nodes after any of the Slice nodes, they can all be fused into a single Lin node followed by a single Multi node.
* If there are Lin or LinState nodes after any of the Slice nodes, the Slices can be fused with the \(\bm{A}\) matrix of the Lin nodes and the \(\bm{B}\) matrix of the LinState nodes. This uses the fact that composing linear functions results in a linear function.
* If there is a ReLU after a Slice node, their position can be switched without changing the nodes. That is because ReLU commutes with linear operations with \(\bm{b}=\bm{0}\) and \(\bm{A}\) with non-negative eigenvalues as is the case for Slice nodes.
* If there is a Concat node after a Slice node, we can similarly push the Slice as a new Lin node after the Concat.

This step does not reduce the number of branching nodes but prepares the graph for a removal, with the specific case depending on the remaining nodes.

Case 5B: Only Lin nodes and they are not all Slices.We can combine them into a single Lin node and then add Slices to extract the relevant subspaces for the subsequent nodes. These Slices can then be pushed into the next operations using Case 5A.

Case 6: Both LinState nodes and other nodes.If both LinState nodes and other nodes are present, we can pass through the other variables with dummy LinState variables using zero matrices for \(\bm{A}\) and identities for \(\bm{B}\). Then, Case 2 can be used to fuse all the LinState variables together.

Case 7A: Only Lin and ReLU nodes where all Lin nodes are followed by only one node which is a ReLU.If we add Lin bypasses to the ReLUs we will have only Lin nodes left. Each one of them would be followed by a ReLU. Hence, Case 5B can be first applied, followed by Case 3.

Case 7B: Only Lin and ReLU nodes where some Lin nodes are not followed by only one node which is a ReLU.In this case we cannot apply the above strategy. Instead, we fuse the ReLUs by placing ReLU-based bypasses before the Lin nodes. We do this in a similar spirit to Eq. (11), by splitting the positive and negative components and treating them separately. See App. F.6 for the LSRL implementation. Our DAG will then be in Case 7A first, then Case 5B, and, finally, in Case 3.

Case 8: Only Lin and Concat nodes.We add Lin bypasses for the Concat nodes which can then be merged using Case 5B and then Case 5A.

Case 9: Only ReLU and Concat nodes.Same strategy as for Case 8 but with ReLU bypasses.

Case 10: Only Lin, ReLU or Concat nodes.We introduce ReLU bypasses to all Concat nodes and to the Lin branches which are not immediately followed by a ReLU. This will be followed by applying Case 5B and then Case 3.

The above 13 cases cover all possible branching configurations. After repeated application, they reduce any DAG corresponding to an LSRL program to a path graph that can be compiled to one of the recurrent models in Sec. 2.

## Appendix B Error Bound on the Approximation Scheme for Continuous Functions

In Sec. 4.1 we outlined a strategy to perform universal in-context approximation for continuous functions with Linear RNNs. The full program is in Lst. 1 and an illustration of the scheme is presented in Fig. 2. In Sec. 4.1 we claimed that the prompt length required to approximate an \(L\)-Lipschitz function \(f\) (w.r.t. the \(\ell_{2}\) norm) to precision \(\epsilon\) is \(N=\nicefrac{{(2\epsilon/L\sqrt{d_{\text{in}}})^{d_{\text{in}}}}}{{2}}=\mathcal{O} (\epsilon^{\cdot d_{\text{in}}})\). The present appendix offers the formal proof of this claim.

The program in Lst. 1 approximates the value of a function \(\bm{y}=f(\bm{q})\) with the value \(\bar{\bm{y}}\) at the centre \(\bm{c}\) of the cell that contains \(\bm{q}\). Therefore, the error of our approximation is the maximum difference between \(f(\bm{q})\) and \(f(\bm{c})\): \(\|f(\bm{q})-f(\bm{c})\|_{2}\). First, as the length of each side of the cell is \(\delta\), that means that \(\|\bm{q}-\bm{c}\|_{\infty}\leq\nicefrac{{\delta}}{{2}}\). Thus, \(\|\bm{q}-\bm{c}\|_{2}\leq\nicefrac{{\sqrt{d_{\text{in}}}}}{{2}}\). Therefore, thanks to \(f\) being \(L\)-Lipschitz we get:

\[\|f(\bm{q})-f(\bm{c})\|_{2}\leq\frac{\delta L\sqrt{d_{\text{in}}}}{2}.\]

If we want to upper bound this approximation error by \(\epsilon\), we need to have \(\delta\) small enough:

\[\delta\leq\frac{2\epsilon}{L\sqrt{d_{\text{in}}}}.\]

Finally, as the number of cells we need to cover the whole domain is \(N=(\nicefrac{{1}}{{\delta}})^{d_{\text{in}}}\), this corresponds to us needing sufficiently long prompt:

\[N\geq\left(\frac{1}{\delta}\right)^{d_{\text{in}}}\geq\left(\frac{L\sqrt{d_{ \text{in}}}}{2\epsilon}\right)^{d_{\text{in}}}.\]

Therefore, if we want our approximation to have error at most \(\epsilon\) anywhere in the domain, we need a prompt of length at least \(\nicefrac{{\left(L\sqrt{d_{\text{in}}}/2\epsilon\right)^{d_{\text{in}}}}}{{2 \epsilon}}\).

## Appendix C Gated RNNs are GRU models

A GRU layer (Cho et al., 2014) with input \(\bm{a}_{t}\in\mathbb{R}^{d_{\text{in}}}\) and hidden state \(\bm{h}_{t-1}\in\mathbb{R}^{d_{\text{hidden}}}\), and output \(\bm{h}_{t}\in\mathbb{R}^{d_{\text{hidden}}}\) can be described as follows:

\[\bm{z}_{t} =\texttt{Sigmoid}(\bm{W}_{z}\bm{a}_{t}+\bm{U}_{z}\bm{h}_{t-1}+ \bm{b}_{z}), \text{(update gate vector)}\] (12) \[\bm{r}_{t} =\texttt{Sigmoid}(\bm{W}_{r}\bm{a}_{t}+\bm{U}_{r}\bm{h}_{t-1}+ \bm{b}_{r}), \text{(reset gate vector)}\] (13) \[\hat{\bm{h}}_{t} =\texttt{tanh}(\bm{W}_{h}\bm{a}_{t}+\bm{U}_{h}(\bm{r}_{t}\odot \bm{h}_{t-1})+\bm{b}_{h}), \text{(candidate activation vector)}\] (14) \[\bm{h}_{t} =(1-\bm{z}_{t})\odot\bm{h}_{t-1}+\bm{z}_{t}\odot\hat{\bm{h}}_{t}, \text{(output vector)}\] (15)

In this section, we show a conversion of a single Gated RNN layer (Eq. (5)) to \(k+2\) GRU layers. Here, \(k\) is the number of layers in the \(\gamma\) and \(h\) MLPs in Eq. (5). We first show that a single GRU layer can be used to compute the updated state \(\bm{s}_{t}\) and the output of the first layer of \(\gamma\) when applied to \(\bm{x}_{t}\). Then, every pair of single layers of \(\gamma(\bm{x}_{t})\) and \(\phi(\bm{s}_{t})\) can be represented as an individual GRU layer. Finally, a single layer can be used to compute the element-wise multiplication \(\gamma(\bm{x}_{t})\odot\phi(\bm{s}_{t})\). For simplicity, we assume the \(\texttt{Sigmoid}\) and \(\texttt{tanh}\) nonlinearities are replaced by \(\texttt{ReLUs}\). If not, they can each be approximated with MLPs and hence also with additional GRU layers. Additionally, for convenience we will assume \(d_{\text{in}}=d_{\text{hidden}}\).

### Representing the state update as a GRU layer

For this layer we set \(\bm{b}_{z}=\bm{1}\), \(\bm{W}_{z}=\bm{0}\), \(\bm{U}_{z}=\bm{0}\) giving \(\bm{z}_{t}=\bm{1}\). Similarly, we set \(\bm{b}_{r}=\bm{1}\), \(\bm{W}_{r}=\bm{0}\), \(\bm{U}_{r}=\bm{0}\) giving \(\bm{r}_{t}=\bm{1}\). Thus, Eq. (14) reduces to:

\[\hat{\bm{h}}_{t}=\sigma(\bm{W}_{h}\bm{a}_{t}+\bm{U}_{h}\bm{h}_{t-1}+\bm{b}_{h}),\] (16)

Setting \(\bm{a}_{t}=\begin{bmatrix}\bm{0}\\ \bm{x}_{t}\end{bmatrix}\), where \(\bm{x}_{t}\in\mathbb{R}^{d_{\text{in}}/2}\), \(\bm{h}_{t-1}=\begin{bmatrix}\bm{s}_{t-1}\\ \bm{0}\end{bmatrix}\), where \(\bm{s}_{t-1}\in\mathbb{R}^{d_{\text{hidden}}/2}\), \(\bm{W}_{h}=\begin{bmatrix}\bm{0}&\bm{B}\\ \bm{0}&\bm{I}\end{bmatrix}\), \(\bm{U}_{h}=\begin{bmatrix}\bm{A}&\bm{0}\\ \bm{0}&\bm{0}\end{bmatrix}\), \(\bm{b}_{h}=\begin{bmatrix}\bm{b}\\ -\bm{k}_{lb}\end{bmatrix}\), where \(\bm{k}_{lb}\) is a vector where every element in \(\bm{k}\) is a lower bound on \(\bm{x}_{t}\). results in Eq. (15) becoming:

\[\bm{h}_{t}=\sigma\left(\begin{bmatrix}\bm{0}&\bm{B}\\ \bm{0}&\bm{I}\end{bmatrix}\begin{bmatrix}\bm{0}\\ \bm{x}_{t}\end{bmatrix}+\begin{bmatrix}\bm{A}&\bm{0}\\ \bm{0}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{s}_{t-1}\\ \bm{0}\end{bmatrix}+\begin{bmatrix}\bm{b}\\ -\bm{k}_{lb}\end{bmatrix}\right)=\begin{bmatrix}\sigma(\bm{A}\bm{s}_{t-1}+\bm{B} \bm{x}_{t}+\bm{b})\\ \sigma(\bm{x}_{t}-\bm{k}_{lb})\end{bmatrix}=\begin{bmatrix}\sigma(\bm{s}_{t})\\ \bm{x}_{t}-\bm{k}_{lb}\end{bmatrix}.\] (17)

_Note: if we do not want to assume a compact domain for \(\bm{x}_{t}\), it would be possible to use the same trick as in Equation (11) rather than subtracting \(\bm{k}\) in this layer and adding in the next. However, we omit this approach for clarity of presentation._

### Representing each MLP layer as a GRU layer

In these layers, similarly to the recurrent layer, we set \(\bm{b}_{z}=\bm{1}\), \(\bm{W}_{z}=\bm{0}\), \(\bm{U}_{z}=\bm{0}\) giving \(\bm{z}_{t}=\bm{1}\). In the same way, we set \(\bm{b}_{r}=\bm{1}\), \(\bm{W}_{r}=\bm{0}\), \(\bm{U}_{r}=\bm{0}\) giving \(\bm{r}_{t}=\bm{1}\). Here, however, we set \(\bm{W}_{h}=\begin{bmatrix}\bm{W}_{h_{i}}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\), \(\bm{U}_{h}=\bm{0}\) and \(\bm{b}_{h}=\begin{bmatrix}\bm{b}_{h_{i}}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\), except for the first of such layer where \(\bm{b}_{h}=\begin{bmatrix}\bm{b}_{h_{i}}\\ \bm{b}_{\gamma_{i}}+\bm{W}_{\gamma_{i}}\bm{k}_{lb}\end{bmatrix}\). Thus, for an input \(\bm{a}_{t}=\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\) the layer output (Eq. (15)) for layer \(i\) is:

\[\bm{h}_{t}=\sigma\left(\begin{bmatrix}\bm{W}_{\phi_{i}}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}+\begin{bmatrix}\bm{b}_{\phi_{i}}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\right)=\begin{bmatrix}\phi_{i}(\bm{a}_{1,t}) \\ \gamma_{i}(\bm{a}_{1,t})\end{bmatrix}.\] (18)

Here, \(\phi_{i}\) and \(\gamma_{i}\) are the \(i\)-th layers (including the ReLU) of respectively \(\phi\) and \(\gamma\) in Eq. (5).

### Representing the multiplicative gating with a single GRU layer

The only thing left is to model the element-wise multiplication of the outputs of \(\phi\) and \(\gamma\) in Eq. (5). We do this using a GRU layer with \(\bm{b}_{z}=\bm{0}\), \(\bm{W}_{z}=\bm{0}\), \(\bm{U}_{z}=\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{0}&\bm{I}\end{bmatrix}\). We set \(\bm{b}_{r}=\bm{0}\), \(\bm{W}_{r}=\bm{0}\), \(\bm{U}_{r}=\bm{0}\) giving \(\bm{r}_{t}=\bm{0}\). We also set \(\bm{b}_{h}=\bm{0}\), \(\bm{W}_{h}=\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{I}&\bm{0}\end{bmatrix}\), \(\bm{U}_{h}=\bm{0}\). Thus, for an input \(\bm{a}_{t}=\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\), the output \(\bm{h}_{t}\) (Eq. (15)) of this GRU layer becomes:

\[\bm{h}_{t}=\sigma\left(\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{I}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\right)\odot\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{0}&\bm{I}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}=\begin{bmatrix}\bm{0}\\ \sigma(\bm{a}_{1,t})\odot\bm{a}_{2,t}\end{bmatrix}.\] (19)

If \(\bm{a}_{t}\) is the output of a GRU layer constructed as in Eq. (18) (as is in our case), then it must be non-negative. This is due to the ReLU application in Eq. (18). Hence, the application of another ReLU to \(\bm{a}_{1,t}\) in Eq. (19) can be safely removed as ReLU is idempotent and Eq. (19) simplifies to

\[\bm{h}_{t}=\begin{bmatrix}\bm{0}\\ \bm{a}_{1,t}\odot\bm{a}_{2,t}\end{bmatrix}.\] (20)

Thus, this construction computes element-wise multiplication of \(\bm{a}_{1,t}\) and \(\bm{a}_{2,t}\).

### Composing the operations to model a single Gated RNN layer

In order to represent Eq. (5), we use one GRU layer for the recurrence (as described in App. C.1), followed by \(k\) GRU layers modelling a pair of the \(k\) MLP layers of \(\phi\) and \(\gamma\) (App. C.2), completed with a single mixing layer (App. C.3). This stack of \(k+2\) layers models exactly the Gated RNN layer (Eq. (5)):

\[\bm{s}_{t} =\sigma\left(\bm{A}\begin{bmatrix}\bm{0}\\ \bm{s}_{t-1}\end{bmatrix}+\bm{B}\begin{bmatrix}\bm{x}_{t}\\ \bm{0}\end{bmatrix}+\bm{b}\right)\] \[\bm{y}_{t} =\begin{bmatrix}\bm{0}\\ \gamma(\bm{x}_{t})\odot\phi(\bm{s}_{t})\end{bmatrix},\]

With this, we have shown that any Gated RNN (Eq. (5)) can be expressed as a GRU-based model. Hence, the two universal approximation programs in Lsts. 1 and 2 can be implemented also in GRU-based models. Thus, the GRU architecture can also be a universal in-context approximator.

## Appendix D Gated RNNs are LSTMs

A single LSTM layer (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) with input \(\bm{a}_{t}\in\mathbb{R}^{d_{in}}\), hidden state \(\bm{h}_{t-1}\in\mathbb{R}^{d_{\text{id}}}\), candidate memory cell \(\tilde{\bm{c}}_{t}\in\mathbb{R}^{d_{\text{id}}}\), memory cell \(\bm{c}_{t}\in\mathbb{R}^{d_{\text{id}}}\) and layeroutput \(\bm{h}_{t}\in\mathbb{R}^{d_{\text{histome}}}\) can be expressed as:

\[\bm{f}_{t} =\texttt{Sigmoid}(\bm{W}_{f}\bm{a}_{t}+\bm{U}_{f}\bm{h}_{t-1}+\bm{ b}_{f}), \text{(forget gate vector)}\] (21) \[\bm{i}_{t} =\texttt{Sigmoid}(\bm{W}_{i}\bm{a}_{t}+\bm{U}_{i}\bm{h}_{t-1}+ \bm{b}_{i}), \text{(input gate vector)}\] (22) \[\bm{o}_{t} =\texttt{Sigmoid}(\bm{W}_{o}\bm{a}_{t}+\bm{U}_{o}\bm{h}_{t-1}+ \bm{b}_{o}), \text{(output gate vector)}\] (23) \[\tilde{\bm{c}}_{t} =\texttt{tanh}(\bm{W}_{c}\bm{a}_{t}+\bm{U}_{c}\bm{h}_{t-1}+\bm{ b}_{c}), \text{(candidate cell vector)}\] (24) \[\bm{c}_{t} =\bm{f}_{t}\odot\bm{c}_{t-1}+\bm{i}_{t}\odot\tilde{\bm{c}}_{t}, \text{(memory cell vector)}\] (25) \[\bm{h}_{t} =\bm{o}_{t}\odot\texttt{tanh}(\bm{c}_{t}), \text{(output vector)}\] (26)

where \(\bm{h}_{0}=\bm{0}\) and \(\bm{c}_{0}=\bm{0}\).

In a way analogous to App. C, we show that a single layer of a gated RNN (Eq. (5)) can be expressed using \(k+2\) LSTM layers, where \(k\) is the maximum depth of either of the MLP networks \(\phi\) or \(\gamma\). We again follow the setup of replacing all Sigmoid and tanh activation functions with ReLU activations which we denote \(\sigma\) and we again assume that \(d_{\text{in}}=d_{\text{hidden}}\). The set up follows the same structure as in App. C. First, we show that the non-linear state update computing \(\bm{s}_{t}\) can be expressed as a single LSTM layer. We then show that we can represent the layers in MLP networks \(\gamma(\bm{x}_{t})\) and \(\phi(\bm{s}_{t})\) using single LSTM layers. Finally, a single layer can compute the Hadamard product between \(\gamma(\bm{x}_{t})\) and \(\phi(\bm{s}_{t})\). Therefore, any Gated RNN with ReLU activations can be expressed as a LSTM with ReLU activations.

For clarity of the exposition, we once again assume that our inputs belong to a compact domain \(\mathcal{X}\) of real vectors. This implies that the set is bounded and, in particular, that we can find a vector \(\bm{k}_{lb}\) such that \(\bm{k}_{lb,i}\leq(\bm{x}_{t})_{i}\) for \(i\in[d_{\text{in}}]\) for all \(\bm{x}_{t}\in\mathcal{X}\). In other words, we have \((\bm{x}_{t}-\bm{k}_{lb})_{i}\geq 0\) for for \(i\in 1,\dots,d_{\text{in}}\). We will make use of this fact several times when dealing with ReLU activations.

### Representing the state update as an LSTM layer

We first represent the non-linear state update in Eq. (5) using a single layer of an LSTM. In particular, we set \(\bm{W}_{f}=\bm{0}\), \(\bm{U}_{f}=\bm{0}\) and \(\bm{b}_{f}=\bm{0}\) so that \(\bm{f}_{t}=\bm{0}\). We also set \(\bm{W}_{i}=\bm{0}\), \(\bm{U}_{i}=\bm{0}\), \(\bm{b}_{i}=\bm{1}\) and \(\bm{W}_{c}=\bm{0}\), \(\bm{U}_{c}=\bm{0}\), \(\bm{b}_{c}=\bm{1}\). This results in \(\bm{i}_{t}=\bm{1}\) and \(\bm{\tilde{c}}_{t}=\bm{1}\). We see from this that the LSTM layer with these weight settings reduces to

\[\bm{h}_{t}=\bm{o}_{t}=\sigma(\bm{W}_{o}\bm{a}_{t}+\bm{U}_{o}\bm{h}_{t-1}+\bm{ b}_{o}).\] (27)

We now set \(\bm{a}_{t}=\begin{bmatrix}\bm{0}\\ \bm{x}_{t}\end{bmatrix}\), where \(\bm{x}_{t}\in\mathbb{R}^{d_{\text{histome}}/2}\), \(\bm{h}_{t-1}=\begin{bmatrix}\bm{s}_{t-1}\\ \bm{0}\end{bmatrix}\), where \(\bm{s}_{t-1}\in\mathbb{R}^{d_{\text{histome}}/2}\), \(\bm{W}_{o}=\begin{bmatrix}\bm{0}&\bm{B}\\ \bm{0}&\bm{I}\end{bmatrix}\), \(\bm{U}_{o}=\begin{bmatrix}\bm{A}&\bm{0}\\ \bm{0}&\bm{0}\end{bmatrix}\), \(\bm{b}_{o}=\begin{bmatrix}\bm{b}\\ -\bm{k}_{lb}\end{bmatrix}\) so that

\[\bm{h}_{t}=\sigma\left(\begin{bmatrix}\bm{0}&\bm{B}\\ \bm{0}&\bm{I}\end{bmatrix}\begin{bmatrix}\bm{0}\\ \bm{x}_{t}\end{bmatrix}+\begin{bmatrix}\bm{A}&\bm{0}\\ \bm{0}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{s}_{t-1}\\ \bm{0}\end{bmatrix}+\begin{bmatrix}\bm{b}\\ -\bm{k}_{lb}\end{bmatrix}\right)=\begin{bmatrix}\sigma(\bm{A}\bm{s}_{t-1}+\bm{ B}\bm{x}_{t}+\bm{b})\\ \sigma(\bm{x}_{t}-\bm{k}_{lb})\end{bmatrix}=\begin{bmatrix}\bm{s}_{t}\\ \bm{x}_{t}-\bm{k}_{lb}\end{bmatrix}.\] (28)

### Representing each MLP layer as an LSTM layer

Now we want to use an LSTM layers to model the MLP layers of both \(\gamma\) and \(\phi\) simultaneously. We set \(\bm{W}_{f}=\bm{0}\), \(\bm{U}_{f}=\bm{0}\), \(\bm{b}_{f}=\bm{0}\) and \(\bm{W}_{i}=\bm{0}\), \(\bm{U}_{i}=\bm{0}\), \(\bm{b}_{i}=\bm{1}\) and \(\bm{W}_{c}=\bm{0}\), \(\bm{U}_{c}=\bm{0}\), \(\bm{b}_{c}=\bm{1}\) as before. We make a change for these LSTM layers by setting \(\bm{W}_{o}=\begin{bmatrix}\bm{W}_{\phi_{i}}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\), \(\bm{U}_{o}=\bm{0}\) and \(\bm{b}_{o}=\begin{bmatrix}\bm{b}_{\phi_{i}}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\), except for the first layer where \(\bm{b}_{\phi}=\begin{bmatrix}\bm{b}_{\phi_{i}}\\ \bm{b}_{\gamma_{1}}+\bm{W}_{\gamma_{1}}\bm{k}\end{bmatrix}\). Thus, for an input \(\bm{a}_{t}=\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\) the layer output is:

\[\bm{h}_{t}=\sigma\left(\begin{bmatrix}\bm{W}_{\phi_{i}}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}+\begin{bmatrix}\bm{b}_{\phi_{i}}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\right)=\begin{bmatrix}\phi_{i}(\bm{a}_{1,t})\\ \gamma_{i}(\bm{a}_{2,t})\end{bmatrix}.\] (29)

Here, \(\phi_{i}\) and \(\gamma_{i}\) again refer to the \(i\)-th layers (including the ReLU) of respectively \(\phi\) and \(\gamma\) in Eq. (5). Note that, without a loss of generality, if we have that \(\phi\) has \(m\) layers whereas \(\gamma\) has \(k\) with \(m<k\), then we can also model this by simply adding additional layers to model additional layers for \(\gamma\) whilst simply passing on \(\phi\) unchanged. Specifically, we set set the weights to ensure that \(\bm{f}_{t}=0\) and that \(\dot{\bm{i}}_{t}\) and \(\tilde{\bm{c}}_{t}\) are \(\bm{1}\) so that \(\bm{h}_{t}=\bm{o}_{t}\). The input to this layer for \(i>k\) is then given as \(\bm{a}_{t}=\begin{bmatrix}\phi(\bm{s}_{t})\\ \bm{a}_{2,t}\end{bmatrix}\). The we set the weights to compute \(\bm{o}_{t}\) as

\[\bm{o}_{t}=\sigma\left(\begin{bmatrix}\bm{I}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{t}}\end{bmatrix}\begin{bmatrix}\phi(\bm{s}_{t})\\ \bm{a}_{2,t}\end{bmatrix}+\begin{bmatrix}\bm{0}\\ \bm{b}_{\phi_{i}}\end{bmatrix}\right)=\begin{bmatrix}\phi(\bm{s}_{t})\\ \gamma_{i}(\bm{a}_{2,t})\end{bmatrix}.\] (30)

### Representing the multiplicative gating with an LSTM layer

Finally, we model the element-wise multiplication of the outputs of \(\phi\) and \(\gamma\) in Eq. (5). To do this we set the weights of the input gate and candidate cell vectors for the final layers of of \(\gamma\) and \(\phi\) to be as follows:

\[\dot{\bm{i}}_{t}=\sigma\left(\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{I}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}+\begin{bmatrix}\bm{0}\\ \bm{0}\end{bmatrix}\right)=\begin{bmatrix}\bm{0}\\ \bm{a}_{1,t}\end{bmatrix}\] (31)

and

\[\tilde{\bm{c}}=\sigma\left(\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{0}&\bm{I}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}+\begin{bmatrix}\bm{0}\\ \bm{0}\end{bmatrix}\right)=\begin{bmatrix}\bm{0}\\ \bm{a}_{2,t}\end{bmatrix}.\] (32)

Then by setting \(\bm{W}_{f}=\bm{0}\), \(\bm{U}_{f}=\bm{0}\), \(\bm{b}_{f}=\bm{0}\) and \(\bm{W}_{o}=\bm{0}\), \(\bm{U}_{o}=\bm{0}\), \(\bm{b}_{o}=\bm{1}\) to force \(\bm{f}_{t}=\bm{0}\) and \(\bm{o}_{t}=\bm{1}\), we get

\[\bm{y}_{t}=\sigma(\bm{c}_{t})=\begin{bmatrix}\sigma(\bm{0}\odot\bm{0})\\ \sigma(\bm{a}_{1,t}\odot\bm{a}_{2,t})\end{bmatrix}=\begin{bmatrix}\bm{0}\\ \sigma(\bm{a}_{1,t}\odot\bm{a}_{2,t})\end{bmatrix}.\] (33)

### Composing the operations to model a single Gated RNN layer

To model the gated RNN described in Eq. (5), we again follow the same lines as described in App. C. In particular, we use one LSTM layer for the recurrent state updated as described in App. D.1. We then stack \(k\) LSTM layers as described in App. D.2 to model the \(k\) MLP layers of \(\phi\) and \(\gamma\) in parallel. We then use one final layer to both give the final MLP layer of \(\phi\) and \(\gamma\) and to compute their Hadamard product as set out in App. D.3 in order to match the output of the gated RNN in Eq. (5). Now, since we are working with \(\sigma=\text{ReLU}\), both \(\gamma(\bm{x}_{t})\) and \(\phi(\bm{s}_{t})\) are positive and therefore so is their product. Hence, applying \(\sigma\) to the product components in Eq. (33) leaves the the components invariant. Therefore, we output is

\[\bm{y}_{t}=\begin{bmatrix}\bm{0}\\ \gamma(\bm{x}_{t})\odot\phi(\bm{s}_{t})\end{bmatrix},\] (34)

as required.

Hence, we have shown that a single layer of a gated RNN as described by Eq. (5) can be represented using \(k+2\) LSTM layers where \(k\) is the maximum depth of \(\phi\) and \(\gamma\). Therefore, once again, the two universal approximation programs in Lsts. 1 and 2 can also be implemented for LSTMs. Hence, LSTM models are also universal approximators in the sense described in Sec. 4.

## Appendix E Gated Linear RNNs are Hawk/Griffin Models

A single residual block of a Hawk/Griffin model (De et al., 2024) consists of two components, a recurrent block for temporal mixing which makes use of a one-dimensional temporal convolution, as well as real-gated linear recurrent unit (RG-LRU) and a gated MLP block. Specifically, we consider an input \(\bm{a}_{t}\in\mathbb{R}^{d_{\text{in}}}\), inputs to the blocks of dimensions \(d_{\text{in}}\) and outputs from each block of dimensions \(d_{\text{in}}\). Within blocks, all vectors have dimensionality \(d_{\text{hidden}}=Ed_{\text{in}}\), where \(E\) is denotes an expansion factor. Below, we formally describe the form of the recurrent and gated MLP blocks which are the two main components making up the residual blocks used for Hawk and Griffin.

**Recurrent block**. The recurrent block consists of two branches. The first applies a one-dimensional temporal convolution followed by a RG-LRU. The second branch simply performs a linear transformation followed by a non-linearity, i.e. applies a single layer of an MLP.

Consider the first branch of the recurrent block with an input \(\bm{a}_{t}\). The one-dimensional temporal convolution can be written as:

\[\bm{a}_{t}^{\prime} =\bm{W}_{a}\bm{a}_{t},\] (35) \[\bm{g}_{t} =\mathsf{GeLU}(\bm{W}_{g}\bm{a}_{t}+\bm{b}_{g}),\] (36) \[\bm{M}_{t} =\left[\bm{a}_{t-(d_{\text{conv}}-1)}^{\prime},\dots,\bm{a}_{t-2} ^{\prime},\bm{a}_{t-1}^{\prime},\bm{a}_{t}^{\prime}\right],\] (37) \[\bm{z}_{t} =\sum_{i=0}^{d_{\text{conv}}-1}\bm{W}_{M}[i]\bm{M}_{t}[t-i]\ \ +\bm{b}_{\text{conv}}\qquad\qquad\text{(convolution with window size $d_{\text{conv}}$)},\] (38)

where \(\bm{b}_{\text{conv}}\) is a bias vector and \(\bm{W}_{M}=\left[\tilde{\bm{B}},\tilde{\bm{A}}\tilde{\bm{B}},\tilde{\bm{A}}^ {2}\tilde{\bm{B}},\cdots,\tilde{\bm{A}}^{t}\tilde{\bm{B}},\cdots\right]\) is the convolutional kernel for the one-dimensional temporal convolution.

The output of this convolution is then fed into a RG-LRU. We can write this down concretely using as an input \(\bm{z}_{t}\) from the one-dimensional convolution and with recurrent state \(\bm{h}_{t}\in\mathbb{R}^{d_{\text{conv}}}\):

\[\bm{r}_{t} =\mathsf{Sigmoid}(\bm{W}_{r}\bm{z}_{t}+\bm{b}_{r}),\] (39) \[\bm{i}_{t} =\mathsf{Sigmoid}(\bm{W}_{i}\bm{z}_{t}+\bm{b}_{i}),\] (40) \[a =\mathsf{Sigmoid}(\Lambda), (\Lambda\text{ a learnable parameter})\] (41) \[\bm{a}_{t} =a^{\bm{c}\bm{r}_{t}}, (c=8\text{ fixed scalar constant})\] (42) \[\bm{h}_{t} =\bm{a}_{t}\odot\bm{h}_{t-1}+\sqrt{1-\bm{a}_{t}^{2}}\odot(\bm{i} _{t}\odot\bm{z}_{t}).\] (43)

Now consider the second branch of the recurrent block. This performs a linear transformation followed by a non-linear activation:

\[\bm{g}_{t}=\mathsf{GeLU}(\bm{W}_{g}\bm{a}_{t}+\bm{b}_{g}).\] (44)

To get the final output of the recurrent block, we multiply the components of the vectors computed from each branch within the recurrent block and then perform a non-linear transformation:

\[\bm{h}_{t}^{\prime} =\bm{g}_{t}\odot\bm{h}_{t},\] (45) \[\bm{o}_{t} =\bm{W}_{o}\bm{h}_{t}^{\prime}+\bm{b}_{o}.\] (46)

**Gated MLP block**. After passing through the recurrent block, we pass the output \(\bm{o}_{t}\) into a gated MLP block. Again we have two branches, the first where we linearly transform the input to this block

\[\bm{e}_{t}=\bm{W}_{e}\bm{o}_{t}+\bm{b}_{e},\] (47)

and the second performs a single layer MLP transformation as

\[\bm{f}_{t}=\mathsf{GeLU}(\bm{W}_{f}\bm{o}_{t}+\bm{b}_{f}).\] (48)

These are then combined through a Hadamard product and linear transformation as

\[\bm{e}_{t}^{\prime} =\bm{e}_{t}\odot\bm{f}_{t},\] (49) \[\bm{m}_{t} =\bm{W}_{m}\bm{e}_{t}^{\prime}+\bm{b}_{m}.\] (50)

We then have that the vector \(\bm{m}_{t}\) acts as the output of the residual block given the input \(\bm{a}_{t}\).

**Distinction between the Griffin and Hawk models.** Hawk is the more simple of the two architectures proposed in (De et al., 2024). Here, residual blocks using the recurrent block described above are simply stacked on top of each other to form the Hawk architecture. Griffin, on the other hand, mixes recurrent blocks and local attention. In particular, two residual blocks with recurrent blocks are followed by one residual block using local MQA attention (Beltagy et al., 2020; Shazeer, 2019).

**Simplifying Assumptions**. We again follow the setup of replacing all \(\mathsf{Sigmoid}\) and \(\mathsf{tanh}\) activation functions with \(\mathsf{ReLU}\) activations which we denote \(\sigma\). Furthermore, we assume for simplicity that \(d_{\text{in}}=d_{\text{hidden}}\) by choosing \(E=1\). Moreover, the Hawk and Griffin architecture contains residual connections and normalising layers which we omit.4 We again assume compactness of the input domain \(\mathcal{X}\) and denote a vector of finite values \(\bm{k}_{lb}\), such that \(\bm{k}_{lb,i}\leq(\bm{x}_{t})_{i}\) for \(i\in[d_{\text{in}}]\) and all \(\bm{x}_{t}\in\mathcal{X}\), just as before. Finally, we assume that \(d_{\text{conv}}=T\) where \(T\) is the maximum sequence length.

### Representing the state update using a recurrent block

Starting with the input to the Hawk model, which we denote \(\bm{a}_{t}\), we define this to be a function of the input to the Gated RNN \(\bm{x}_{t}\) as \(\bm{a}_{t}=\begin{bmatrix}\bm{0}\\ \bm{x}_{t}\end{bmatrix}\). First, we set \(\bm{W}_{a}=\bm{I}\) so that \(\bm{a}^{\prime}_{t}=\bm{a}_{t}\). Next we choose matrices \(\tilde{\bm{A}}=\begin{bmatrix}\bm{0}&\bm{A}\\ \bm{0}&\bm{0}\end{bmatrix}\) and \(\tilde{\bm{B}}=\begin{bmatrix}\bm{0}&\tilde{\bm{B}}\\ \bm{0}&\bm{0}\end{bmatrix}\) which we then use, with a convolutional window size of \(d_{\text{conv}}=T\) to form the convolutional kernel \(\bm{W}_{M}=\begin{bmatrix}\tilde{\bm{B}},\tilde{\bm{A}}\tilde{\bm{B}},\tilde{ \bm{A}}^{2}\tilde{\bm{B}},\cdots,\tilde{\bm{A}}^{t}\tilde{\bm{B}},\cdots\end{bmatrix}\). Setting the convolutional bias as \(\bm{b}_{\text{conv}}=\begin{bmatrix}\bm{0}\\ \bm{1}\end{bmatrix}\) gives

\[\bm{z}_{t} =\sum_{i=0}^{t-1}\bm{W}_{M}[i]\bm{M}_{t}[t-i]\ \ +\bm{b}_{\text{conv}},\] (51) \[=\tilde{\bm{B}}\bm{a}_{t}+\tilde{\bm{A}}\tilde{\bm{B}}\bm{a}_{t-1 }+\cdots+\tilde{\bm{A}}^{t-1}\tilde{\bm{B}}\bm{a}_{1}+\begin{bmatrix}\bm{0}\\ \bm{1}\end{bmatrix}\] (52) \[=\begin{bmatrix}\bm{s}_{t}\\ \bm{1}\end{bmatrix}.\] (53)

Now, we pass \(\bm{z}_{t}\) through the RG-LRU. We set \(\Lambda=0\) so that \(\bm{a}_{t}=0\). We also define \(\bm{W}_{i}=\bm{0}\) and \(\bm{b}_{i}=\bm{1}\) so that \(\bm{i}_{t}=\bm{1}\). This gives us \(\bm{h}_{t}=\bm{z}_{t}\), so that we pass the output of the one-dimensional convolution through he RG-LRU.

Next, let's focus on the second branch. Making use of the lower bound \(\bm{k}_{lb}\) on the domain \(\mathcal{X}\), we set \(\bm{W}_{g}=\bm{I}\) and \(\bm{b}_{g}=\begin{bmatrix}\bm{1}\\ -\bm{k}_{lb}\end{bmatrix}\) so that

\[\bm{g}_{t}=\sigma\left(\bm{I}\begin{bmatrix}\bm{0}\\ \bm{x}_{t}\end{bmatrix}+\begin{bmatrix}\bm{1}\\ -\bm{k}_{lb}\end{bmatrix}\right)=\begin{bmatrix}\sigma(\bm{1})\\ \sigma(\bm{x}_{t}-\bm{k}_{lb})\end{bmatrix}=\begin{bmatrix}\bm{1}\\ \bm{x}_{t}-\bm{k}_{lb}\end{bmatrix},\] (54)

where we used that \((\bm{x}_{t}-\bm{k}_{lb})_{i}\geq\bm{0}\) for every \(i\). Combining the two branches gives

\[\bm{h}^{\prime}_{t}=\begin{bmatrix}\bm{1}\\ \bm{x}_{t}-\bm{k}_{lb}\end{bmatrix}\odot\begin{bmatrix}\bm{s}_{t}\\ \bm{1}\end{bmatrix}=\begin{bmatrix}\bm{s}_{t}\\ \bm{x}_{t}-\bm{k}_{lb}\end{bmatrix}.\] (55)

We finally get the output of the recurrent block by defining \(\bm{W}_{o}=\bm{I}\) and \(\bm{b}_{0}=\begin{bmatrix}\bm{0}\\ \bm{k}_{lb}\end{bmatrix}\) so that

\[\bm{o}_{t}=\begin{bmatrix}\bm{s}_{t}\\ \bm{x}_{t}\end{bmatrix}.\] (56)

### Representing the identity function using a recurrent block

We now show that we can pass an input unchanged through a recurrent block. Assume that the input to the recurrent block is \(\bm{a}_{t}=\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\) with \(\bm{W}_{a}=\bm{I}\) so that \(\bm{a}^{\prime}_{t}=\bm{a}_{t}\). Then we define matrices \(\tilde{\bm{A}}=\bm{0}\) and \(\tilde{\bm{B}}=\bm{I}\) which we then use to form the convolutional kernel \(\bm{W}_{M}=\begin{bmatrix}\tilde{\bm{B}},\tilde{\bm{A}}\tilde{\bm{B}},\tilde{ \bm{A}}^{2}\tilde{\bm{B}},\cdots,\tilde{\bm{A}}^{t}\tilde{\bm{B}},\cdots \end{bmatrix}\). Finally, setting the convolutional bias as \(\bm{b}_{\text{conv}}=\bm{0}\) results in \(\bm{z}_{t}=\bm{a}_{t}\). From here, we can again set \(\Lambda=0\), \(\bm{W}_{i}=\bm{0}\) and \(\bm{b}_{i}=\bm{1}\) so that \(\bm{h}_{t}=\bm{z}_{t}\). Looking at the second branch and setting \(\bm{W}_{g}=0\) and \(\bm{b}_{g}=\bm{1}\) so that \(\bm{h}^{\prime}_{t}=\bm{h}_{t}\). Finally, we can simply output the input to the recurrent block by setting \(\bm{W}_{o}=\bm{I}\) and \(\bm{b}_{o}=\bm{0}\) so that \(\bm{o}_{t}=\bm{h}_{t}\) which means that \(\bm{o}_{t}=\bm{a}_{t}\).

### Representing each MLP layer as a gated MLP block

We can represent the MLP layers of the networks \(\phi(\bm{s}_{t})\) and \(\gamma(\bm{x}_{t})\) as described in Eq. (4) using Gated MLP blocks. We again denote the \(i\)-th layer of \(\phi\) and \(\gamma\) as \(\phi_{i}\) and \(\gamma_{i}\). Assume that the input to the gated MLP block is \(\bm{a}_{t}=\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\). Then, on the first purely linear branch, let us define \(\bm{W}_{e}=\bm{I}\) and\(\bm{b}_{e}=\bm{1}\) so that \(\bm{e}_{t}=\bm{1}\). On the second non-linear branch, we can define \(\bm{W}_{f}=\begin{bmatrix}\bm{W}_{\phi_{i}}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\) and \(\bm{b}_{f}=\begin{bmatrix}\bm{b}_{\phi_{i}}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\). This results in

\[\bm{f}_{t}=\sigma\left(\begin{bmatrix}\bm{W}_{\phi_{i}}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}+\begin{bmatrix}\bm{b}_{\phi_{i}}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\right)=\begin{bmatrix}\phi_{i}(\bm{a}_{1,t}) \\ \gamma_{i}(\bm{a}_{2,t})\end{bmatrix}.\] (57)

Due to our setting of \(\bm{e}_{t}\), we get \(\bm{e}_{t}^{\prime}=\bm{f}_{t}\). Further, defining \(\bm{W}_{m}=\bm{I}\) and \(\bm{b}_{m}=\bm{0}\) makes the output of the MLP block be

\[\bm{m}_{t}=\begin{bmatrix}\phi_{i}(\bm{a}_{1,t})\\ \gamma_{i}(\bm{a}_{2,t})\end{bmatrix}.\] (58)

**Emulating the layers of only one the two networks.** Suppose without loss of generality (WLOG) that \(\phi\) has \(m\) layers and \(\gamma\) has \(n\) layers where \(m<n\). Suppose also that our input to the MLP block is \(\bm{a}_{t}=\begin{bmatrix}\phi(\bm{x}_{t})\\ \bm{a}_{2,t}\end{bmatrix}\). Again, on the first purely linear branch, let us define \(\bm{W}_{e}=\bm{I}\) and \(\bm{b}_{e}=\bm{1}\) so that \(\bm{e}_{t}=\bm{1}\). Now we modify the weights on the second non-linear branch by defining \(\bm{W}_{f}=\begin{bmatrix}\bm{I}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\) and \(\bm{b}_{f}=\begin{bmatrix}\bm{0}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\). This gives us

\[\bm{f}_{t}=\sigma\left(\begin{bmatrix}\bm{I}&\bm{0}\\ \bm{0}&\bm{W}_{\gamma_{i}}\end{bmatrix}\begin{bmatrix}\phi(\bm{x}_{t})\\ \bm{a}_{2,t}\end{bmatrix}+\begin{bmatrix}\bm{0}\\ \bm{b}_{\gamma_{i}}\end{bmatrix}\right)=\begin{bmatrix}\sigma(\phi(\bm{x}_{t}) )\\ \gamma_{i}(\bm{a}_{2,t})\end{bmatrix}=\begin{bmatrix}\phi(\bm{x}_{t})\\ \gamma_{i}(\bm{a}_{2,t})\end{bmatrix},\] (59)

where we have used that since \(\phi(\bm{x}_{t})\) is a ReLU network whose final activation is a ReLU, we have that \(\phi(\bm{x}_{t})=\sigma(\phi(\bm{x}_{t}))\). Hence, if our networks have different depths and we have fully emulated one of the networks, we can continue to emulate the remaining layers of the other network while keeping the fully emulated network fixed and unchanged.

### Representing the identify function using a gated MLP block

In this section we show that we can represent an identity function using a gated MLP block. This can be simply done by setting \(\bm{W}_{f}=\bm{0},\bm{b}_{f}=\bm{1}\), \(\bm{W}_{e}=\bm{I},\bm{b}_{e}=\bm{0}\), \(\bm{W}_{m}=\bm{I}\) and \(\bm{b}_{m}=\bm{0}\). This then gives us that for an input \(\bm{a}_{t}=\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\) to the gated MLP block, the output of the gated MLP block is \(\bm{m}_{t}=\bm{a}_{t}\). Thus, we pass the input through the gated MLP unchanged.

### Representing multiplicative gating with a gated MLP block

The final thing we need to do is to compute an element-wise product of two vectors in order to match the output in Eq. (4). In other words, to match the \(\phi(\bm{x}_{t})\odot\gamma(\bm{s}_{t})\) operation.

Again, assume that the input to the gated MLP block is \(\bm{a}_{t}=\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}\). Working with the first linear branch, we define \(\bm{W}_{e}=\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{I}&\bm{0}\end{bmatrix}\) and \(\bm{b}_{e}=\bm{0}\), so that

\[\bm{e}_{t}=\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{I}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{a}_{1,t}\\ \bm{a}_{2,t}\end{bmatrix}+\bm{0}=\begin{bmatrix}\bm{0}\\ \bm{a}_{1,t}\end{bmatrix}.\] (60)

Next, we define \(\bm{W}_{f}=\bm{I}\) and \(\bm{b}_{e}=\bm{0}\) so that

\[\bm{f}_{t}=\begin{bmatrix}\sigma(\bm{a}_{1,t})\\ \sigma(\bm{a}_{2,t})\end{bmatrix}.\] (61)

Setting \(\bm{W}_{m}=\bm{I}\) and \(\bm{b}_{m}=\bm{0}\) gives the output of the gated MLP as

\[\bm{m}_{t}=\begin{bmatrix}\bm{0}\\ \bm{a}_{1,t}\odot\sigma(\bm{a}_{2,t})\end{bmatrix}.\] (62)

### Composing the operations to model a single gated linear-RNN layer

Now that we have all the individual layers, we can combine them so that we can use a Hawk model to emulate a single Gated RNN layer.

First we start by taking the input of the form \(\bm{a}_{t}=\begin{bmatrix}\bm{0}\\ \bm{x}_{t}\end{bmatrix}\). We use a residual block that consists of a recurrent block computing the state update as descried in App. E.1 and then a gated MLP block that computes the identity function as demonstrated in App. E.4. This gives an output from this first recurrent block as \(\bm{o}_{t}=\begin{bmatrix}\bm{s}_{t}\\ \bm{x}_{t}\end{bmatrix}\).

Next, we emulate the MLP layers of the networks \(\phi\) and \(\gamma\) in parallel. Suppose WLOG that \(\phi\) and \(\gamma\) have \(m\) and \(n\) MLP layers respectively, where \(m\leq n\). We stack \(m\) residual blocks using recurrent blocks that implement the identity function as described in App. E.2 followed by MLP blocks that apply the MLP layers of \(\phi\) and \(\gamma\) as described in App. E.3. Stacking \(m\) such residual blocks results in the output \(\bm{m}_{t}=\begin{bmatrix}\gamma_{m}(\bm{s}_{t})\\ \phi(\bm{x}_{t})\end{bmatrix}\), where we can fully emulate the shallower network \(\phi(\bm{x}_{t})\).

Now, for the remaining \(k-m\) layers for the network \(\gamma(\bm{x}_{t})\), we stack residual blocks with recurrent blocks implementing the identity function as described in App. E.2 and MLP blocks that leave \(\phi(\bm{x}_{t})\) unchanged whilst applying the additional layers needed to emulate \(\gamma(\bm{s}_{t})\) as described at the end of App. E.3. After stacking \(k-m\) additional residual layers in this fashion, the output of the final residual block will now be \(\bm{m}_{t}=\begin{bmatrix}\gamma(\bm{s}_{t})\\ \phi(\bm{x}_{t})\end{bmatrix}\), which fully reconstructs the MLP networks \(\gamma\) and \(\phi\).

Finally, we utilise a residual block with a recurrent block that implements the identity function as described in App. E.2 followed by a gated MLP block that applies multiplicative gating as described in App. E.5. This then gives as an output of this final residual block \(\bm{m}_{t}=\begin{bmatrix}\bm{0}\\ \gamma(\bm{s}_{t})\odot\sigma(\phi(\bm{x}_{t}))\end{bmatrix}\).

Since \(\phi(\bm{x}_{t})\) is a MLP network with the final activation function being a ReLU activation, we have that \(\sigma(\phi(\bm{x}_{t}))=\phi(\bm{x}_{t})\), giving the required final output from the stacked block of residual blocks as

\[\bm{m}_{t}=\begin{bmatrix}\bm{0}\\ \gamma(\bm{s}_{t})\odot\phi(\bm{x}_{t})\end{bmatrix}.\] (63)

Hence, we have shown that a single layer of a gated RNN as described by Eq. (5) can be represented using \(k+2\) Hawk residual blocks where \(k\) is the maximum depth of \(\phi\) and \(\gamma\). Once again, the two universal approximation programs in Lsts. 1 and 2 can also be applied to Hawk models as they can represent Gated Linear RNNs. Therefore, Hawk models are also universal approximators in the sense described in Sec. 4.

**Gated Linear-RNNs are Griffin models too.** The above argument extends to the Griffin architecture which uses stacks of two residual blocks with recurrent blocks followed by a residual block with attention. The only thing that changes is that for every third residual block, which in our argument will be used to compute the MLP layers of \(\phi\) and \(\gamma\) in parallel, the recurrent block is now replaced with a local MQA block.

We can set the key query and values matrices to implement the identity function which is to act input to the block. Hence, as a corollary of the above argument, we can also show that the universal approximation programs in Lsts. 1 and 2 can also be implemented as Griffin models. Therefore, Griffin models can also be universal approximators in the sense described in Sec. 4.

## Appendix F Definitions for some helper functions in LSRL

### f_not

This is a convenience function that creates a NOT function block. It assumes that \(x\) is 0 or 1. Works with scalar and vector-valued inputs. With vector-valued inputs, it acts element-wise.

``` not_x = 1 - x

### f_and

This is a convenience function that creates an AND function block. It assumes that \(x\) and \(y\) are 0 or 1. Works with scalar and vector-valued inputs. With vector-valued inputs, it acts element-wise. \(\mathtt{mu}\) is the approximation parameter \(\mu\) for f_step as described in Sec. 3.

```
1and_x_y=RelU(f_step(x,m)+f_step(y,m)-1) ```

### f_or

This is a convenience function that creates an OR function block. It assumes that \(x\) and \(y\) are 0 or 1. Works with scalar and vector-valued inputs. With vector-valued inputs, it acts element-wise. \(\mathtt{mu}\) is the approximation parameter \(\mu\) for f_step as described in Sec. 3.

```
1or_x_y=f_step(x+y,m=m) ```

### f_smaller

This is a convenience function that a less than comparison block. Works with scalar and vector-valued inputs. With vector-valued inputs, it acts element-wise. \(\mathtt{mu}\) is the approximation parameter \(\mu\) for f_step as described in Sec. 3.

```
1assler_x_y=f_step(y-x,m=m) ```

### f_larger

This is a convenience function that a more than comparison block. Works with scalar and vector-valued inputs. With vector-valued inputs, it acts element-wise. \(\mathtt{mu}\) is the approximation parameter \(\mu\) for f_step as described in Sec. 3.

```
1larger_x_y=f_step(x-y,m=m) ```

### f_relu_identity

Identity operation using ReLUs. This is useful for debranching when some of the branches have ReLUs but the other don't. We can add this as a bypass for the ones that do not and can then merge the ReLUs together (see App. A for details).

```
1positive_part=ReLUs(
2negative_part=ReLUs(
3Linear)
4inputx,
5A=1+eye(x,dim),
6bereors(x,dim,1),
7)
8)
9both=Concat([positive_part,negative_part])
10relu_identity=Linear(
11input+both,
12A=Bstack(eye(x,dim),-1+eye(x,dim)),
13b=zeros(x,dim,1),
14} ```

### f_modulo_counter

Computes the \(x\mod\mathtt{divisor}\) where \(x\) is a counter starting from zero. The idea is that we rotate a unit vector so that it makes a full revolution every divisor rotations. dummy_input can be any variable, we use it only to construct a constant.

```
1angle=2+pi/divisor
2#=[[cos(angle),sin(angle)],[sin(angle),cos(angle)]]
3unit_vector=[C1],[0]
4#weifrotate,thenoutputsoifwewantthefirstoutputtobe#weedtohavetheinit_stateonestepbeforethat
5init_state=R.inv()#unit_vector6#istitotesa2|vector1/divisorrevolutionsatattime
*cverver<thisdate{
*input<dummy_input,
*AR.
*0b=zeros(2,dummy_input,dim),
*init_state=init_state,
*}
*newnewneedtoextractthepositionoftthecyler
*extraction_matrix=*stack(*(R*1*unit_vector).T)foriinrange(divisor)))
*indicator=linear
*input<cyler,
*A=extraction_matrix,
*b=zeros(divisor,1)
*}
*gthedotproductwiththerowofextractor_matrixcorrespondingtothecurrentpositionoftthecyleris1
*thedotproductwiththesecondhighesticsics(angle)
*gthetususecanthresholdat1<cos(angle/){0getsomehotencodingoftthecurrentpositionoftthecyler
*one_hot=f_larger(indicator,cos(angle/2))
*andtogetanintegervaluewweedomefinallinearlayer
*mod_value=Linear{
*one_hot,
*A=[1foriinrange(divisor)]],
*b=zeros(1,1)
*}

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and the introduction clearly state all the contributions of the paper and clearly differentiate the theoretical results which hold in general and the empirical phenomena that we observe, which may not generalize to all settings. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses the limitations of the present work. There is a dedicated _Limitations_ section in Sec. 7 that addresses the fact that we only provide constructive existence results but not necessary and sufficient conditions for universal in-context approximation to arise. We also highlight that our results might not hold to models with structural constraints on their parameters. Moreover, we have a dedicated section (Sec. 5) which addresses some of the limitations of constructing universal in-context approximators with fully recurrent architectures in practice. This section proposes solutions and demonstrates that they result in more numerically stable models which are more likely to occur in practice. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The paper has two main theoretical results: the constructions of universal in-context approximators for continuous and for discrete functions. Both results are presented as LSRL programs which compile to the architectures considered in this work. Furthermore, these programs have been implemented in Python, their correctness has been tested and they are available in the supplementary materials. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: There are two experimental aspects to this work. First, there is the implementation of LSRL and the two universal approximation programs in Lsts. 1 and 2. The most critical aspect of implementing LSRL is the debranching algorithm which is described in detail in App. A. Additionally, the two programs are described in full in their corresponding listings. We also provide Python implementation for the LSRL compiler and the two programs. Second, there is the study of how affected by parameter noise are the different implementations of the conditional assignment operator f_ifelse which was presented in Sec. 6. The details of this experiment are described in Fig. 4 and we also provide the code with which we did the experiment and our plots. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.

* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We are providing code that includes an implementation of LSRL, the two universal in-context approximation programs in Lsts. 1 and 2 and everything needed to reproduce the experiments in this work. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The experiments in our work are based on _constructed_ models rather than _trained_ models. Therefore, considerations such as dataset, optimizers and hyperparameters do not apply. Guidelines: * The answer NA means that the paper does not include experiments.

* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance*
* Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: For this work, uncertainty quantification could only make sense in the context of Fig. 4. However, the behaviour we observe, especially for the continuous case, is bimodal. As bimodal distributions cannot be properly captured with error bars we decided against using them. Furthermore, we are studying whether a phenomenon occurs, rather than quantifying it. Therefore, we decided to instead use a strip plot instead as it explicitly shows all our results unabridged, clearly indicates the bimodal nature of the results, and distinctly showcases the noise robustness trends of the different approaches we consider. Guidelines:
* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
* **Experiments Compute Resources*
* Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Our experiments were ran on a single machine and using only CPU compute. Therefore, the compute required is negligible for the contemporary machine learning standards. Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
* **Code Of Ethics**Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This is a theoretical work with no human participants, datasets, or potential societal impact or harmful consequences. Therefore, the present work has no moral or ethical relevance or implications. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
* **Broader Impacts*
* Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: As mentioned above, this is a theoretical work which establishes theoretical properties of mathematical objects that are already used in practice. However, we do discuss the implications of our findings, namely that if models are universal in-context approximators, then it might be difficult to ensure that they are aligned and cannot be misused. Nevertheless, we only show that this is a property already present in existing models, and hence our work does not introduce new attack or misuse vectors. On the contrary, we hope that us highlighting this issues will help the community to develop safer and more secure generative AI systems. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
* **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We release no data or models. Guidelines:* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets beyond common Python libraries. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The only new asset arising from this work is the LSRL code base which we have ensured to be well-documented, accompanied by unit and integration tests, and with illustrative Jupyter notebooks. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]Justification: There were no human participants involved in any part of this work. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This is a purely theoretical work and as such no IRB approval or equivalent was necessary or appropriate. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.