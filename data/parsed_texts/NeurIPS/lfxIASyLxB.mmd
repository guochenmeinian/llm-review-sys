# In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness

 Liam Collins

Chandra Family Department of ECE

The University of Texas at Austin

liamc@utexas.edu

&Advait Parulekar

Chandra Family Department of ECE

The University of Texas at Austin

advaitp@utexas.edu

&Aryan Mokhtari

Chandra Family Department of ECE

The University of Texas at Austin

mokhtari@austin.utexas.edu

&Sujay Sanghavi

Chandra Family Department of ECE

The University of Texas at Austin

sanghavi@mail.utexas.edu

&Sanjay Shakkottai

Chandra Family Department of ECE

The University of Texas at Austin

sanjay.shakkottai@utexas.edu

Co-first authors, listed in alphabetical order.

###### Abstract

A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such, that learner must adapt to the context without additional training. We explore the role of _softmax_ attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.

## 1 Introduction

One of the most compelling behaviors of pretrained transformers is their ability to perform _in-context learning (ICL)_[1]: determining how to solve an unseen task simply by making a forward pass on input context tokens. Arguably the most critical innovation enabling ICL is the self-attention mechanism [2], which maps each token in an input sequence to a new token using information from all other tokens. A key design choice in this self-attention architecture is of the activation function that controls how much "attention" a token pays to other tokens. _Softmax_-activated self-attention (i.e. softmax attention) is most commonly, and successfully, used in practice [1, 3, 4, 5, 6].

A natural approach to explain ICL adopted by the literature is to equate it with classical machine learning algorithms, primarily variants of gradient descent (GD). Several works have shown thatwhen the ICL tasks are _linear_ regressions and the activation in the attention unit is identity (referred to as _linear_ attention), transformers that implement preconditioned GD during ICL are global optima of the pretraining loss, which is the population loss on ICL tasks [7, 8, 9]. In particular, the prediction of such transformers with \(l\) linear attention layers equals the prediction of a regressor trained with \(l\) preconditioned GD steps on the context examples. However, since these analyses are limited to linear attention and tasks, they do not explain the widespread success of _softmax_ attention at ICL.

More recent work [10] extends these results by showing that for general regression tasks and any attention activation that is a kernel, ICL equates to training a kernel regressor via functional GD in the Reproducing Kernel Hilbert Space (RKHS) induced by the activation. However, this functional GD yields generalization guarantees only when the activation kernel is _identical_ to a kernel that generates the labels, which does not apply to the softmax activation, as it is not a kernel. Further, like the aforementioned studies of the linear setting [7, 8, 9], this analysis only shows that pretraining leads to learning the _covariate_ distribution, while the activation implicitly encodes the _label_ distribution needed for accurate predictions. Thus, this line of work has not explained the very fundamental question of what _softmax_ attention learns during pretraining that enables it to perform ICL on a wide variety of downstream tasks. Motivated by this gap in the literature, we ask the following question.

_How does softmax attention learn to perform ICL?_

To answer this question, we study general settings in which pretraining and evaluation ICL tasks are regressions that share only _Lipschitzness_ and _label noise variance_. Specifically, the rate at which their ground-truth labels change along particular directions in the input space, and the variance in the label noise, is similar across tasks. In such settings, we observe that softmax attention acts as a nearest neighbors regressor with an _attention window_ - i.e. neighborhood of points around the query that strongly influence, or "attend to", the prediction - that adapts to the pretraining tasks. Specifically, our main result is as follows:

**Main Claim:** Softmax attention performs ICL by calibrating its _attention window_ to the _Lipschitzness_ and _label noise variance_ of the pretraining tasks.

While this does not contradict the line of work showing that ICL manifests via a "meta-learned" gradient-based algorithm, we show in a general setting that a simpler mechanism can explain the capabilities of a widely accepted model of ICL.

Figure 1: **Top Row:** The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. **Middle Row:** Attention weights – which determine the attention window – as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. **Bottom Row:** ICL error versus number of context samples for the three settings. **Adapting to function Lipschitzness leads softmax attention to achieve small error**. Please see Remark 2.1 and Appendix J for further discussion and details.

**Outline.** We substantiate the above claim via two streams of analysis. To our knowledge, these are the first results showing that softmax attention pretrained on ICL tasks recovers shared structure among the tasks that facilitates ICL on downstream tasks.

**(1) Attention window _scale_ adapts to Lipschitzness and noise variance - Section 3.** We prove that the pretraining-optimal softmax attention estimator scales its attention window _inversely with the task Lipschitzness_ and _jointly with the noise level_ to optimally trade-off bias and variance in its prediction (Theorem 3.4). This requires tight upper and lower bounds on the pretraining ICL loss. While the upper bounds (Lemma C.8) hold for all \(L\)-Lipschitz tasks, the lower bounds (Lemma C.9) are more challenging and require considering specific classes of tasks. We consider two classes of generalized linear models (GLMs), and obtain lower bounds via novel concentrations for particular functionals on the distribution of the attention weights for tokens distributed on the hypersphere (Corollary G.5).

**(2) Attention window _directions_ adapt to direction-wise Lipschitzness - Section 4.** We prove that when the target function class consists of linear functions that share a common low-dimensional structure, the optimal softmax attention weight matrix from pretraining projects the data onto this subspace (Theorem 4.4). In other words, softmax attention learns to zero-out the zero-Lipschitzness directions in the ambient data space, and thereby reduces the effective dimension of ICL. We prove this via a careful symmetry-based argument to characterize a particular gradient of the ICL loss as positive (Lemmas H.3 and H.4).

**Tightness of results.** Our results highlight the importance of shared Lipschitzness across training and test, as well as the critical role of the softmax activation, to ICL. We show that softmax attention pretrained on the setting from Section 3 in-context learns _any_ downstream task with _similar Lipschitzness_ to the pretraining tasks, while changing _only the Lipschitzness_ of the evaluation tasks degrades performance (Theorem 3.5) - implying _learning Lipschitzness is both sufficient_ and _necessary for generalization_. Further, to emphasize the _necessity of the softmax_, we show that the minimum ICL loss achievable by linear attention exceeds that achieved by pretrained softmax attention (Theorem 3.6). We verify all of these results with empirical simulations (Section 3.2 and Appendix J).

**Notations.** We use (upper-, lower-)case boldface for (matrices, vectors), respectively. We denote the (identity, zero) matrix in \(\mathbb{R}^{d\times d}\) as (\(\mathbf{I}_{d}\), \(\mathbf{0}_{d\times d}\)), respectively, the set of column-orthonormal matrices in \(\mathbb{R}^{d\times k}\) as \(\mathbb{O}^{d\times k}\), and the (column space, 2-norm) of a matrix \(\mathbf{B}\) as (\(\text{col}(\mathbf{B})\), \(\|\mathbf{B}\|\)), respectively. We indicate the unit hypersphere in \(\mathbb{R}^{d}\) by \(\mathbb{S}^{d-1}\) and the uniform distribution over \(\mathbb{S}^{d-1}\) as \(\mathcal{U}^{d}\). We use asymptotic notation (\(\mathcal{O}\), \(\Omega\)) to hide constants that depend only on the dimension \(d\).

### Additional Related Work

Numerous recent works have _constructed_ transformers that can implement GD and other machine learning algorithms during ICL [11; 12; 13; 14; 15], but it is unclear whether _pretraining_ leads to such transformers. [16] and [13] provide generalization bounds for ICL via tools from algorithmic stability and uniform concentration, respectively. [17] investigate the pretraining statistical complexity of learning a Bayes-optimal predictor for ICL on linear tasks with linear attention. [18; 19; 20] study the role of the pretraining data distribution, rather than the learning model, in facilitating ICL. [21] studies the dynamics of a softmax attention unit trained with GD on ICL tasks, but this analysis considers only linear tasks and orthogonal inputs. The connection between ICL with softmax attention and non-parametric regression has been noticed by other works that analyze the ICL performance of a softmax-like kernel regressor [22] and aim to improve upon softmax attention [23; 24; 25; 26; 27] rather than explain what it learns during pretraining. Please see Appendix A for further discussion of the large body of related works studying the theory of transformers, ICL and kernel regression.

## 2 Preliminaries

**In-Context Learning (ICL) regression tasks.** We study ICL in the regression setting popularized by [28], wherein each task is a regression problem in \(\mathbb{R}^{d}\). The context for task \(t\) consists of a set of \(n\) feature vectors paired with noisy labels \(\{\bm{x}_{i}^{(t)},f^{(t)}(\bm{x}_{i}^{(t)})+\epsilon_{i}^{(t)}\}_{i=1}^{n}\), where \(f^{(t)}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) generates the ground-truth labels for task \(t\) and \(\epsilon_{i}^{(t)}\) is label noise. Given this context, the model solves the task if it accurately predicts the label of a query \(\bm{x}_{n+1}^{(t)}\). During pretraining, the modelobserves many such tasks. Then, it is evaluated on a new task with context \(\{\bm{x}_{i},f^{(*)}(\bm{x}_{i}^{(*)})+\epsilon_{i}^{(*)}\}_{i=1}^{n}\) and query \(\bm{x}_{n+1}^{(*)}\). We emphasize that the model is trained only on the pretraining tasks, not the evaluation context. Unlike traditional supervised learning, which would involve training on the context \(\{\bm{x}_{i},f^{(*)}(\bm{x}_{i}^{(*)})+\epsilon_{i}^{(*)}\}_{i=1}^{n}\) in order to predict \(f^{(*)}(\bm{x}_{n+1}^{(*)})\), ICL happens _entirely in a forward pass_, so there is no training using labels from \(f^{(*)}\). Our inquiry focuses on how ICL is facilitated by the softmax activation in the self-attention unit, which we introduce next.

**The Softmax Attention Unit.** We consider a single softmax attention head \(H_{SA}(\cdot;\bm{\theta}):\mathbb{R}^{(d+1)\times(n+1)}\rightarrow\mathbb{R}^ {(d+1)\times(n+1)}\) parameterized by \(\bm{\theta}:=(\mathbf{W}_{K},\mathbf{W}_{Q},\mathbf{W}_{V})\), where \(\mathbf{W}_{K},\mathbf{W}_{Q},\mathbf{W}_{V}\in\mathbb{R}^{(d+1)\times(d+1)}\) are known as key, query, and value weight matrices, respectively. Intuitively, for a sequence of tokens \(\mathbf{Z}=[\bm{z}_{1},\dots,\bm{z}_{n+1}]\in\bm{z}^{(d+1)\times(n+1)}\), the attention layer creates a "hash map" where the key-value pairs come from key and value embeddings of the input tokens, \(\{\mathbf{W}_{K}\,\bm{z}_{i}:\mathbf{W}_{V}\,\bm{z}_{i}\}\). Each token \(\bm{z}_{i}\) is interpreted as a query \(\mathbf{W}_{Q}\,\bm{z}_{i}\), and during a pass through the attention layer, this query is matched with the keys \(\{\mathbf{W}_{K}\,\bm{z}_{j}\}_{j}\) to return an average over the associated values \(\{\mathbf{W}_{V}\,\bm{z}_{j}\}_{j}\) with a weight determined by the quality of the match (proportional to \(e^{(\mathbf{W}_{K}\,\bm{z}_{j})^{\top}(\mathbf{W}_{Q}\,\bm{z}_{i})}\)). Specifically, \(H_{SA}(\mathbf{Z};\bm{\theta})=[h_{SA}(\bm{z}_{1},\mathbf{Z};\bm{\theta}), \cdots,h_{SA}(\bm{z}_{n+1},\mathbf{Z};\bm{\theta})]\), where

\[h_{SA}(\bm{z}_{i},\mathbf{Z};\bm{\theta})=\frac{\sum_{j=1}^{n} \left(\mathbf{W}_{V}\bm{z}_{j}\right)\ e^{(\mathbf{W}_{K}\bm{z}_{j})^{\top}( \mathbf{W}_{Q}\bm{z}_{i})}}{\sum_{i=1}^{n}e^{(\mathbf{W}_{K}\bm{z}_{j})^{\top} (\mathbf{W}_{Q}\bm{z}_{i})}}\in\mathbb{R}^{d+1}.\] (ATTN)

With slight abuse of notation, we denote \(h_{SA}(\bm{z}_{j})=h_{SA}(\bm{z}_{j},\mathbf{Z};\bm{\theta})\) when it is not ambiguous. To study how this architecture enables ICL, we follow [28] to formalize ICL as a regression problem. Below we define the tokenization, pretraining objective and evaluation task.

**Tokenization for regression.** The learning model encounters token sequences of the form

\[\mathbf{Z}:=\begin{bmatrix}\bm{x}_{1}&\bm{x}_{2}&\dots&\bm{x}_{n }&\bm{x}_{n+1}\\ f(\bm{x}_{1})+\epsilon_{1}&f(\bm{x}_{2})+\epsilon_{1}&\dots&f(\bm{x}_{n})+ \epsilon_{n}&0\end{bmatrix}\in\mathbb{R}^{(d+1)\times(n+1)},\] (1)

where the ground-truth labelling function \(f\) maps from \(\mathbb{R}^{d}\) to \(\mathbb{R}\) and belongs to some class \(\mathcal{F}\), each \(\epsilon_{i}\) is mean-zero noise, and the \(i\)-th input feature vector \(\bm{x}_{i}\in\mathbb{R}^{d}\) is jointly embedded in the same token with its noisy label \(f(\bm{x}_{i})+\epsilon_{i}\in\mathbb{R}\). We denote this token \(\bm{z}_{i}\). The ICL task is to accurately predict this label given the \(n\) context tokens \(\{(\bm{x}_{i},f(\bm{x}_{i})+\epsilon_{i})\}_{i=1}^{n}\), where \(f\) may vary across sequences. The prediction for the label of the \((n+1)\)-th feature vector is the \((d+1)\)-th element of \(h_{SA}(\bm{z}_{n+1})\)[10], denoted \(h_{SA}(\bm{z}_{n+1})_{d+1}\). Ultimately, the goal is to learn weight matrices such that \(h_{SA}(\bm{z}_{n+1})_{d+1}\) is likely to approximate the \((n+1)\)-th label on a random sequence \(\mathbf{Z}\).

**Pretraining protocol.** We study what softmax attention learns when its weight matrices are _pre-trained_ using sequences of the form of (1). These sequences are randomly generated as follows:

\[f\sim D(\mathcal{F}),\quad\bm{x}_{1},\dots,\bm{x}_{n+1}\stackrel{{ \mathrm{i.i.d}}}{{\sim}}D_{\bm{x}}^{\otimes(n+1)},\quad\epsilon_{1}, \dots,\epsilon_{n}\stackrel{{\mathrm{i.i.d}}}{{\sim}}D_{\epsilon}^ {\otimes(n+1)}\] (2)

where \(D(\mathcal{F})\) is a distribution over functions in \(\mathcal{F}\), \(D_{\bm{x}}\) is a distribution over \(\mathbb{R}^{d}\), and \(D_{\epsilon}\) is a distribution over \(\mathbb{R}\) with mean zero and variance \(\sigma^{2}\). The token embedding sequence \(\mathbf{Z}\) is then constructed as in (1). Given this generative model, the pretraining loss of the parameters \(\bm{\theta}=(\mathbf{W}_{Q},\mathbf{W}_{K},\mathbf{W}_{V})\) is the expected squared difference between the prediction of softmax attention and the ground-truth label of the \((n+1)\)-th input feature vector in each sequence, namely

\[\widetilde{\mathcal{L}}(\bm{\theta}):=\mathbb{E}_{f,\{\bm{x}_{i}\}_{i},\{ \epsilon_{i}\}_{i}}\left(h_{SA}(\bm{z}_{n+1})_{d+1}-f(\bm{x}_{n+1})\right)^{2}.\] (3)

We next reparameterize the attention weights to make (3) more interpretable. For the last column of \(\mathbf{W}_{V}\), we show in Appendix B that any minimizer of (3) in the settings we consider must have the first \(d\) elements of this last column equal to zero. We follow [7; 9; 10] by setting the first \(n\) columns of \(\mathbf{W}_{V}\) to zero. As in [10], we fix the \((d+1,d+1)\)-th element of \(\mathbf{W}_{V}\), here as 1 for simplicity. In the same vein, we follow [7; 10] by setting the \((d+1)\)-th row and column of \(\mathbf{W}_{K}\) and \(\mathbf{W}_{Q}\) equal to zero. To summarize, the reparameterized weights are:

\[\mathbf{W}_{V}=\begin{bmatrix}\mathbf{0}_{d\times d}&\mathbf{0}_{d \times 1}\\ \mathbf{0}_{1\times d}&1\end{bmatrix},\quad\mathbf{W}_{K}=\begin{bmatrix}\mathbf{ M}_{K}&\mathbf{0}_{d\times 1}\\ \mathbf{0}_{1\times d}&0\end{bmatrix},\quad\mathbf{W}_{Q}=\begin{bmatrix}\mathbf{ M}_{Q}&\mathbf{0}_{d\times 1}\\ \mathbf{0}_{1\times d}&0\end{bmatrix}\] (4)where \(\mathbf{M}_{K},\mathbf{M}_{Q}\in\mathbb{R}^{d\times d}\). Now, since our goal is to reveal properties of minimizers of the pretraining loss, rather than study the dynamics of optimizing the loss, without loss of generality we can define \(\mathbf{M}:=\mathbf{M}_{K}^{\top}\mathbf{M}_{Q}\) and re-define the pretraining loss (3) as a function of \(\mathbf{M}\). Doing so yields:

\[\mathcal{L}(\mathbf{M}):=\mathbb{E}_{f,\{\bm{x}_{i}\}_{i},\{\bm{x}_{i}\}_{i}} \left(\frac{\sum_{i=1}^{n}(f(\bm{x}_{i})+\epsilon_{i})\ e^{\bm{x}_{i}^{\top} \mathbf{M}\,\bm{x}_{n+1}}}{\sum_{i=1}^{n}e^{\bm{x}_{i}^{\top}\mathbf{M}\,\bm{x }_{n+1}}}-f(\bm{x}_{n+1})\right)^{2}.\] (ICL)

**Interpretation of the pretraining loss.** The loss (ICL) clarifies how softmax attention can be interpreted as a nearest neighbors regressor. When \(\bm{x}_{i}^{\top}\mathbf{M}\,\bm{x}_{n+1}\) is a proxy for the distance between \(\bm{x}_{i}\) and \(\bm{x}_{n+1}\) (which we formally show in Section 3 as happening under reasonable assumptions), the softmax attention prediction is a convex combination of the noisy labels with weights determined by the closeness of \(\bm{x}_{i}\) to \(\bm{x}_{n+1}\), such that the labels of points closer to \(\bm{x}_{n+1}\) have larger weight. Moreover, the decay in weights on points further from \(\bm{x}_{n+1}\) is exponential and controlled by \(\mathbf{M}\), which effectively defines a neighborhood, or attention window, of points around \(\bm{x}_{n+1}\) whose labels have non-trivial weight. More formally, we can think of the attention window defined for a query \(\bm{x}_{n+1}\) as the set \(\texttt{AttnWindow}(\bm{x}_{n+1};\mathbf{M}):=\{\bm{x}:\bm{x}^{\top}\mathbf{M }\,\bm{x}_{n+1}=\Omega(1)\}\). As we have observed in Figure 1, our key insight is that pretrained \(\mathbf{M}\) **scales this attention window with the Lipschitzness of the function class.** Generally speaking, larger \(\mathbf{M}\) entails averaging over a smaller window and incurring less bias due to the function values of distant tokens in the estimate, while smaller \(\mathbf{M}\) entails averaging over a larger window, resulting in larger bias due to distant token labels, but a smaller noise variance. Figure 2 further depicts this tradeoff.

**Connection to non-parametric estimation and the Nadaraya-Watson estimator.** A nonparametric estimation technique to interpolate between known values of a function is to use a kernel estimator. The Nadaraya-Watson (NW) estimator [29; 30; 31] is one such estimator, and interpolates the data as

\[f_{NW}(\bm{x}_{n+1})=\sum_{i}\frac{K(x_{n+1},x_{i})f(x_{i})}{\sum_{j}K(x_{n+1},\,x_{j})}\]

where \(K(r)=e^{-r^{2}/h}\) for some bandwidth \(h\). In Section B.1 we show that optimizing the pretraining loss (ICL) reduces to meta-learning the bandwidth of an NW estimator on a distribution of pretraining tasks. However, to our knowledge, the literature has not determined the optimal bandwidth for the kernel, as there has been no analysis of non-asymptotic lower bounds on the loss, which we need to characterize the optimal solution. A close work to ours is [32], which considers regression on general \(L\)-Lipschitz tasks, but this analysis provides only a tight upper bound on the loss.

**Remark 2.1** (Extreme cases).: _Consider the following two settings._

_(i) Constant functions._ _If each of the functions the attention unit sees in pretraining is constant, as in the_ _Left_ _column of Figure_ 1_, it is best to consider an infinite attention window, that is, take_ \(\mathbf{M}=\mathbf{0}_{d\times d}\) _as this results in a uniform average over all the noisy token labels._

_(ii) Rapidly changing functions._ _If the pretraining functions change rapidly, as in the_ _Right_ _column of Figure_ 1_, attending to a distant token might only corrupt the estimate at the target. For example suppose the input tokens are used to construct Voronoi cells on the surface of the hypersphere, and the label for a new token in a cell is the label of the token used to construct that cell. The optimal estimator attends only to the single nearest token since this incurs error only from label noise._

**Remark 2.2** (Softmax advantage).: _To further highlight the utility of the softmax, we compare with linear attention [7; 9; 11], whose estimator can be written as \(h_{LA}(\bm{x})=\sum_{i}(f(\bm{x}_{i})+\epsilon_{i})\,\bm{x}_{i}^{\top}\mathbf{M }\,\bm{x}\), up to a universal scaling due to the value embedding. This is again a weighted combination of labels, but one that does not allow for adapting an attention window - any scaling of \(\mathbf{M}\) does not change the relative weights placed on each label - unlike softmax attention. Please see Figure 1 (**Middle Row**) for a comparison of the weights used in the different estimators._

## 3 Pretraining Learns Scale of Attention Window

One of our observations of the attention estimator \(h_{SA}\) is that it computes a nearest neighbours regression. We hypothesize that the role of pretraining is to select a neighbourhood within which to select tokens for use in the estimator. In this section we characterize the radius of this neighborhood.

**Definition 3.1** (Lipschitzness).: _A function \(f:\mathcal{X}\to\mathbb{R}\) has Lipschitzness \(L\) if \(L\) is the smallest number satisfying \(f(\bm{x})-f(\bm{x}^{\prime})\leq L\|\,\bm{x}-\bm{x}^{\prime}\,\|\) for all \((\bm{x},\bm{x}^{\prime})\in\mathcal{X}^{2}\)._

The general requirement for the function classes to which our results apply is that the class should be invariant to isometries, each function should be Lipschitz, and the function value at two points should be less correlated as those points get further. These are written formally in Assumption B.4. To be concrete, we work with the following two function classes that satisfy these assumptions (this is shown in Lemmas C.3 and C.7) to derive explicit bounds.

**Definition 3.2** (Affine and ReLU Function Classes).: _The function classes \(\mathcal{F}_{L}^{\text{aff}}\) and \(\mathcal{F}_{L}^{+}\) are respectively defined as:_

\[\mathcal{F}_{L}^{\text{aff}} :=\{f:f(\bm{x})=l\ \mathbf{w}^{\top}\,\bm{x}+b,\ \mathbf{w}\in\mathbb{S}^{d-1},b,l\in[-L,L]\},\] \[\mathcal{F}_{L}^{+} :=\{f:f(\bm{x})=l_{1}(\mathbf{w}^{\top}\,\bm{x})_{+}+l_{2}(- \mathbf{w}^{\top}\,\bm{x})_{+}+b,\ \mathbf{w}\in\mathbb{S}^{d-1},(b,l_{1},l_{2})\in[-L,L]^{2}\}.\]

\(D(\mathcal{F}_{L}^{\text{off}}),D(\mathcal{F}_{L}^{+})\) _are induced by drawing \(\mathbf{w}\sim\bm{\Sigma}\bm{\Omega}\mathcal{U}^{d}\) and \(b,l,l_{1},l_{2}\overset{i.i.d.}{\sim}\text{Unif}([-L,L])\) for some \(\bm{\Sigma}\succ\bm{0}_{d\times d}\). Note that the max Lipschitzness of any function in these classes is \(L\), and \((z)_{+}:=\max(z,0)\)._

Next, we make the following assumption, similar to [7], on the covariate distribution.

**Assumption 3.3** (Covariate Distribution).: _The covariate distribution satisfies \(D_{\bm{x}}=\bm{\Sigma}^{-1}\mathcal{U}^{d}\)._

Now we are ready to state our main theorem that characterizes minimizers of (ICL).

**Theorem 3.4**.: _Let Assumption 3.3 hold and tasks \(f\) be drawn from **(Case 1)**\(D(\mathcal{F}_{L}^{\text{aff}})\) or **(Case 2)**\(D(\mathcal{F}_{L}^{+})\). For \(n=\Omega(1)\) and \(\Omega(n^{-d/2})\leq\sigma^{2}\leq\mathcal{O}(nL^{2})\), any minimizer of the pretraining loss (ICL) satisfies2\(\mathbf{M}^{*}=w_{KQ}\bm{\Sigma}\), where for \(\Lambda:=\frac{nL^{2}}{\sigma^{2}}\), \(\alpha:=\frac{1}{d+4}\) and \(\beta:=\frac{1}{d+2}\):_

Footnote 2: We further show in Appendix B that \(\mathbf{M}^{*}=w_{KQ}\bm{\Sigma}\) for scalar \(w_{KQ}\) holds for a broad family of rotationally-invariant function classes.

\[\text{(Case 1)}\ \ \Omega\left(\Lambda^{\alpha}\right)\leq|w_{KQ}|\leq \mathcal{O}\left(\Lambda^{\frac{2\alpha}{\gamma-\beta}}\right),\quad\text{( Case 2)}\ \ \Omega\left(\Lambda^{\beta}\right)\leq|w_{KQ}|\leq\mathcal{O}\left(\Lambda^{2 \beta}\right).\]

Theorem 3.4 shows that optimizing the pretraining population loss in Equation (ICL) leads to attention key-query parameters that scale with the Lipschitzness of the function class, as well as the noise level and number of in-context samples. These bounds align with our observations from Figures 1 and 2 that softmax attention selects an attention window that shrinks with the function class Lipschitzness, recalling that larger \(w_{KQ}\) results in a smaller window. Further, the dependencies of the bounds on \(\sigma^{2}\) and \(n\) are also intuitive, since larger noise should encourage wider averaging to average out the noise, and larger \(n\) should encourage a smaller window since more samples makes it more likely that there are samples very close to the query. To our knowledge, this is the _first result showing that softmax attention learns properties of the task distribution during pretraining that facilitate ICL._

**Learning Lipschitzness is critical to generalization.** We next give the following generalization result for downstream tasks.

**Theorem 3.5**.: _Suppose softmax attention is first pretrained on tasks drawn from \(D(\mathcal{F}_{L}^{+})\) and then tested on an arbitrary \(L-\)Lipschitz task, then the loss on the new task is upper bounded as \(\mathcal{L}\leq\mathcal{O}(n^{2})\)._

Figure 2: From **left to right**, as we **shrink the attention window** (shaded in blue), the estimator has **lower bias** (the expected value of the estimate, depicted in purple, is closer to the ground-truth label, depicted by the white circle) but **larger variance** (shaded in tan).

\(\mathcal{O}(\frac{L^{2}}{\Lambda^{\beta}})\). _Furthermore, if the new task is instead drawn from \(D(\mathcal{F}^{+}_{L^{\prime}})\), the loss is lower bounded as \(\mathcal{L}\geq\Omega(\frac{L^{\prime^{2}}}{\Lambda^{2\beta}})\) for \(L^{\prime}>L\) and \(\mathcal{L}\geq\Omega(\frac{\Lambda^{\beta d/2}}{n})\) for \(L^{\prime}<L\)._

Theorem 3.5 shows that pretraining on \(D(\mathcal{F}^{+}_{L})\) yields a model that can in-context learn downstream tasks _if and only if_ they have similar Lipschitzness as \(L\). Thus, learning Lipschitzness is _both sufficient and necessary_ for ICL. If the evaluation task Lipschitzness is much larger than that seen in pretraining, the pretrained model will give highly biased estimates. Conversely, if the evaluation Lipschitzness is much lower, the pretrained model will not optimally average the label noise.

**Necessity of Softmax.** To further emphasize the importance of the softmax in Theorem 3.4, we next study the performance of an analogous model with the softmax removed. We consider _linear self-attention_[7; 9; 11], which replaces the softmax activation with an identity operation. In particular, in the in-context regression setting we study, the prediction of \(f(\bm{x}_{n+1})\) by linear attention and the corresponding pretraining loss are given by:

\[h_{LA}(\bm{x}_{n+1}) :=\sum_{i=1}^{n}(f(\bm{x}_{i})+\epsilon_{i})\bm{x}_{i}^{\top} \operatorname{\mathbf{M}}\bm{x}_{n+1},\] \[\mathcal{L}_{\text{LA}}(\operatorname{\mathbf{M}}) :=\mathbb{E}_{f,\{\bm{x}_{i}\}_{i},\{\epsilon_{i}\}_{i}}\left(h_{ LA}(\bm{x}_{n+1})-f(\bm{x}_{n+1})\right)^{2}.\]

As discussed in Remark 2.1, \(h_{LA}(\bm{x}_{n+1})\) cannot adapt an attention window to the problem setting. We show below that this leads it to large ICL loss when tasks are drawn from \(D(\mathcal{F}^{+}_{L})\).

**Theorem 3.6** (Lower Bound for Linear Attention).: _Consider pretraining on \(\mathcal{L}_{\text{LA}}\) with tasks \(f\) drawn from \(D(\mathcal{F}^{+}_{L})\) and covariates drawn from \(\mathcal{U}^{d}\). Then for all \(\operatorname{\mathbf{M}}\in\mathbb{R}^{d\times d}\), \(\mathcal{L}_{LA}(\operatorname{\mathbf{M}})=\Omega(L^{2})\)._

This lower bound on \(\mathcal{L}_{LA}\) is strictly larger than the upper bound on \(\mathcal{L}\) from Theorem 3.5, up to factors in \(d\), as long as \(\frac{\sigma^{2}}{n}\leq 1\), which holds in all reasonable cases. Please see Appendix F for the proof.

### Proof Sketch

To highlight the key insights of our analysis, in this section we consider a modification of the softmax attention that exhibits important properties of the original. Note that this approximation is for illustration only; the above results use the original softmax attention - see Appendices C, D, E. For now, consider a function class \(\mathcal{F}_{L}:=\{f:f(\bm{x})=L\mathbf{w}^{\top}\bm{x},\ \mathbf{w}\in \mathbb{S}^{d-1}\}\) of linear functions.

**(Temporary) modification of the softmax attention.** Rather than averaging over every token with a weight that decays exponentially with distance, we consider a modification which _uniformly_ averages all tokens within a distance specified by \(w_{KQ}=\|\operatorname{\mathbf{M}}\|\). From Lemma B.5, without loss of generality (WLOG) we can consider \(\operatorname{\mathbf{M}}=w_{KQ}\mathbf{I}_{d}\). This means that, ignoring normalization, the weight assigned to \(f(\bm{x}_{i})\) by the true soft-max attention is \(e^{-w_{KQ}\|\ \bm{x}-\bm{x}_{i}\ \|^{2}}\). That is, for all \(\bm{x}_{i}\) satisfying \(\|\bm{x}-\bm{x}_{i}\ \|<\nicefrac{{1}}{{\sqrt{w_{KQ}}}}\), the assigned weights within a constant factor of each other. Meanwhile, for \(\bm{x}_{i}\) satisfying \(\|\bm{x}-\bm{x}_{i}\ \|=\nicefrac{{c}}{{\sqrt{w_{KQ}}}}\) for \(c>1\), the weights are \(e^{-c}\), decaying exponentially in \(c\). This motivates us to consider a "modified softmax attention" given by \(h_{MSA}(\bm{x}):=\sum_{i}\frac{f(\bm{x}_{i})\bm{1}_{i}}{\sum_{j}\bm{1}_{j}},\) where \(\bm{1}_{j}:=\bm{1}\{\|\bm{x}-\bm{x}_{j}\ \|<\nicefrac{{1}}{{\sqrt{w_{KQ}}}}\}\).

**The In-context Loss.** The pretraining loss in Equation ICL can be decomposed as:

\[\mathcal{L}(w_{KQ}\mathbf{I}_{d})=\underbrace{\mathbb{E}_{f,\{\bm{x}_{i}\}_{i}} \left(\sum_{j}\frac{(f(\bm{x}_{n+1})-f(\bm{x}_{j}))\mathbbm{1}_{j}}{\sum_{j} \mathbbm{1}_{j}}\right)^{2}}_{=:\mathcal{L}_{\text{\tiny signal}}(w_{KQ})}^{2} +\underbrace{\mathbb{E}_{\{\bm{x}_{i}\}_{i},\{\bm{\epsilon}_{i}\}_{i}}\left( \sum_{i}\frac{\bm{\epsilon}_{i}\mathbbm{1}_{i}}{\sum_{j}\mathbbm{1}_{j}}\right) ^{2}}_{=:\mathcal{L}_{\text{\tiny noise}}(w_{KQ})}.\]

We first upper and lower bound each of these terms separately, starting with \(\mathcal{L}_{\text{signal}}(w_{KQ})\).

**Noiseless Estimator Bias.** (Please see Appendix C) This term is the squared difference between an unweighted average of the token labels within a radius of \(\bm{x}\), and the true label. Take \(w_{KQ}=\Omega(1)\). Then for large \(d\), most of the points \(\bm{x}_{i}\) satisfying \(\|\bm{x}-\bm{x}_{i}\|\leq\nicefrac{{1}}{{\sqrt{w_{KQ}}}}\) lie on the boundary of the cap, that is, \(\|\bm{x}-\bm{x}_{i}\|<\nicefrac{{1}}{{\sqrt{w_{KQ}}}}\implies\|\bm{x}-\bm{x}_ {i}\|\approx\nicefrac{{1}}{{\sqrt{w_{KQ}}}}\). This motivates us to approximate the set of points \(\bm{x}_{i}\) satisfying the above as coming from a uniform distribution over just the boundary of the cap. The center of mass of a ring of radius \(\nicefrac{{1}}{{\sqrt{w_{KQ}}}}\) embedded on the surface of a hyper-sphere, is \(\mathcal{O}(\nicefrac{{1}}{{w_{KQ}}})\) from the boundary of a sphere, so the squared bias is \(\Theta(\nicefrac{{L^{2}}}{{w_{KQ}^{2}}})\).

**Noise.** (Please see Appendix D for details) Since the noise is independent across tokens, we can write \(\mathcal{L}_{\text{noise}}(w_{KQ})=\frac{\sigma^{2}}{\sum_{j}\mathbbm{1}_{j}}\), which is related to the number of tokens found within a \(\nicefrac{{1}}{{\sqrt{w_{KQ}}}}\) radius of \(\bm{x}\). In Lemma G.1, we derive bounds for the measure of this region. For now, we replace the sum in the denominator with its expectation. We can bound \(\frac{\sum_{j}\mathbbm{1}_{j}}{\mathbbm{1}_{j}}=\Theta\big{(}w_{KQ}^{\frac{d}{ 2}}/n\big{)}\) as long as \(w_{KQ}\lesssim n^{2/d}\).

**Combining the \(\mathcal{L}_{\text{signal}}\) and \(\mathcal{L}_{\text{noise}}\) terms.** (Please see Appendix E for details) Overall, we have \(\mathcal{L}=\mathcal{L}_{\text{signal}}+\mathcal{L}_{\text{noise}}\) with \(\mathcal{L}_{\text{signal}}=\Theta\big{(}L^{2}/\nicefrac{{w_{KQ}}}{{w_{KQ}}} \big{)}\) and \(\mathcal{L}_{\text{noise}}=\Theta\big{(}w_{KQ}^{\frac{d}{2}}\nicefrac{{\sigma^ {2}}}{{n}}\big{)}\). Minimizing this sum reveals that the optimal \(w_{KQ}\) satisfies \(w_{KQ}=\Theta\big{(}(\nicefrac{{mL^{2}}}{{\sigma^{2}}})^{\frac{2}{d^{2}}}\big{)}\).

### Experiments

We next empirically verify our intuitions and results regarding learning the scale of the attention window. In all cases we use the Adam optimizer with one task sampled per round, use the noise distribution \(D_{\epsilon}=\mathcal{N}(0,\sigma^{2})\), and run \(10\) trials and plot means and standard deviations over these 10 trials. Please see Appendix J for full details as well as additional results.

**Ablations over \(L\), \(\sigma\) and \(n\).** We verify whether the relationship between the attention window scale - i.e. \(\|\mathbf{M}\|^{-1}\) - and \(L\), \(\sigma\) and \(n\) matches our bounds in Theorem 3.4 for the case when tasks are drawn from \(D(\mathcal{F}_{L}^{+})\) and the covariates are drawn from \(\mathcal{U}^{d}\), as well as whether these relationships generalize to additional function classes and covariate distributions. We train on tasks drawn from \(D(\mathcal{F}_{L}^{+})\) and \(D(\mathcal{F}_{L}^{\cos})\), where \(\mathcal{F}_{L}^{\cos}:=\{f:f(\bm{x})=\cos\big{(}L\mathbf{w}^{\top}\bm{x}\big{)},\ \mathbf{w}\in\mathbb{S}^{d-1}\}\) and \(D(\mathcal{F}_{L}^{\cos})\) is induced by sampling \(\mathbf{w}\sim\mathcal{U}^{d}\). In all cases we set \(d=5\), and use \((L,\sigma,n)=(1,0.01,20)\) if not ablating over these parameters, and vary only one of \(\{L,\sigma,n\}\) and no other hyperparameters within each plot.

**Attention window scales inversely with \(L\).** Figure 3 shows that \(\|\mathbf{M}\|\) increases with \(L\) in various settings. In Figure 3(Left, Middle-Left), tasks are drawn from \(D(\mathcal{F}_{L}^{+})\), and in Figure 3(Middle-Right, Right), they are drawn \(D(\mathcal{F}_{L}^{\cos})\). In Figure 3(Left, Middle-Right), each \(\bm{x}_{i}\) is drawn from \(\mathcal{U}^{d}\), whereas in Figure 3(Middle-Left, Right), each \(\bm{x}_{i}\) is drawn from a non-isotropic distribution \(\tilde{\mathcal{U}}^{d}\) on \(\mathbb{S}^{d-1}\) defined as follows. First, let \(\mathbf{S}_{d}:=\text{diag}([1,\dots,d])\in\mathbb{R}^{d\times d}\), then \(\bm{x}\sim\tilde{\mathcal{U}}^{d}\) is generated by sampling \(\hat{\bm{x}}\sim\mathcal{N}(\bm{0}_{d},\mathbf{I}_{d})\), then computing \(\bm{x}=\frac{\mathbf{S}_{d}^{1/2}\hat{\bm{x}}}{\|\mathbf{S}_{d}^{1/2}\hat{\bm{ x}}\|}\). Although larger \(L\) implies larger \(\|\nabla_{\mathbf{x}}f(\bm{x})\|\) on average across \(f\), it is not clear that it implies larger \(\|\nabla_{\mathbf{M}_{K}}\mathcal{L}(\mathbf{W}_{K}^{\top}\mathbf{W}_{Q})\|\) nor \(\|\nabla_{\mathbf{M}_{Q}}\mathcal{L}(\mathbf{W}_{K}^{\top}\mathbf{W}_{Q})\|\), so it is surprising that larger \(L\) implies larger pretrained \(\mathbf{M}\) (although it is consistent with our results).

**Attention window scales with \(\sigma\), inversely with \(n\).** Figure 4 shows that the dependence of \(\|\mathbf{M}\|\) on \(\sigma\) and \(n\) also aligns with Theorem 3.4. As expected, \(\|\mathbf{M}\|\) increases slower during pretraining for larger \(\sigma\) (shown in Figures 4(Left, Middle-Left)), since more noise encourages more averaging over a larger window to cancel out the noise. Likewise, \(\|\mathbf{M}\|\) increases faster during pretraining for larger \(n\) (shown in Figures 4(Middle-Right, Right)), since larger \(n\) increases the likelihood that there is a highly informative sample in a small attention window. Here always the covariate distribution is \(\mathcal{U}^{d}\).

**Learning new tasks in-context.** An implication of our work is that for the function classes we consider, the **softmax attention estimator does not adapt to the function class beyond its Lipschitzness**. We have already seen in Figures 3 and 4 that the growth of \(\|\mathbf{M}\|\) during pretraining is similar across different function classes with the same Lipschitzness, as long as \(\sigma\) and \(n\) are fixed. Here we verify the conclusion from Theorem 3.5 that for fixed \(n\) and \(\sigma\), the necessary and sufficient condition for downstream generalization, measured by small ICL error, is that the pretraining and downstream tasks have similar Lipschitzness. Figure 5 supports this conclusion. Here we set \(d=5,n=200,\sigma=0.01\) and draw each \(\bm{x}_{i}\) i.i.d. from \(\mathcal{U}^{d}\). In Figure 5(Left, Middle-Left, Middle-Right), we train three attention units on tasks drawn from the 1-Lipschitz affine (\(D(\mathcal{F}_{1}^{\text{aff}})\)), ReLU (\(D(\mathcal{F}_{1}^{+})\)), and cosine (\(D(\mathcal{F}_{1}^{\text{cos}})\)) task distributions. Each plot shows the test ICL error on tasks drawn from a distribution in \(\{D(\mathcal{F}_{1}^{\text{aff}}),D(\mathcal{F}_{1}^{+}),D(\mathcal{F}_{1}^{ \text{cos}})\}\). Performance is similar regardless of the pairing of pretraining and test distributions, as the Lipschitzness is the same in all cases, demonstrating that **pretraining on tasks with appropriate Lipschitzness is sufficient for generalization**.

Moreover, Figure 5(Right) shows that when the Lipschitzness of the pretraining tasks does _not_ match that of the test tasks, ICL performance degrades sharply, even when the tasks otherwise share similar structure. Here the test task distribution is \(D(\mathcal{F}_{1}^{\text{cos}})\), and the pretraining task distributions are \(D(\mathcal{F}_{1}^{\text{aff}})\), \(D(\mathcal{F}_{1}^{\text{cos}})\), and \(D(\mathcal{F}_{1}^{\text{cos}})\). The only pretraining distribution that leads to downstream generalization is \(D(\mathcal{F}_{1}^{\text{aff}})\) since its Lipschitzness matches that of the downstream tasks, despite the fact that it is not a distribution over cosine functions, unlike the other distributions. Thus, these results lend credence to the idea that in addition to being sufficient, **pretraining on tasks with appropriate Lipschitzness is necessary for generalization**.

## 4 Softmax Attention Learns Direction of Attention Window

Thus far, we have considered distributions over tasks that treat the value of the input data in all directions within the ambient space as equally relevant to its label. However, in practice the ambient dimension of the input data is often much larger than its information content - the labels may change very little with many features of the data, meaning that such features are spurious. This is generally true of embedded language tokens, whose embedding dimension is typically far larger than the minimum dimension required to store them (logarithmic in the vocabulary size) [1]. Motivated by this, we define a notion of "direction-wise Lipschitzness" of a function class to allow for analyzing classes that may depend on some directions within the ambient input data space more than others.

**Definition 4.1** (Direction-wise Lipschitzness of Function Class).: _The Lipschitzness of a function class \(\mathcal{F}\) with domain \(\mathcal{X}\subseteq\mathbb{R}^{d}\) in the direction \(\mathbf{w}\in\mathbb{S}^{d-1}\) is defined as as the largest Lipschitz constant of all functions in \(\mathcal{F}\) over the domain \(\mathcal{X}\) projected onto \(\mathbf{w}\), that is:_

\[\text{Lip}_{\mathbf{w}}(\mathcal{F},\mathcal{X}):=\inf_{L\in\mathbb{R}}\{L:f( \mathbf{w}\mathbf{w}^{\top}\,\bm{x})-f(\mathbf{w}\mathbf{w}^{\top}\,\bm{x}^{ \prime})\leq L|\mathbf{w}^{\top}\,\bm{x}-\mathbf{w}^{\top}\,\bm{x}^{\prime}\, |\,\,\,\,\forall\,\,(\bm{x},\bm{x}^{\prime})\in\mathcal{X}^{2},f\in\mathcal{ F}\}.\]

Using this definition, we analyze function classes consisting of linear functions with parameters lying in a subspace of \(\mathbb{R}^{d}\), as follows:

**Definition 4.2** (Low-rank Linear Function Class).: _The function class \(\mathcal{F}_{\mathbf{B}}^{\text{lin}}\) is defined as \(\mathcal{F}_{\mathbf{B}}^{\text{lin}}:=\{f:f(\bm{x})=\mathbf{a}^{\top}\mathbf{B }^{\top}\,\bm{x},\,\,\mathbf{a}\in\mathbb{R}^{k}\}\), and \(D(\mathcal{F}_{\mathbf{B}}^{\text{lin}})\) is induced by drawing \(\mathbf{a}\sim\mathcal{U}^{k}\)._

where \(\mathbf{B}\in\mathbb{O}^{d\times k}\) is a column-wise orthonormal matrix. Since our motivation is settings with low-dimensional structure, we can think of \(k\ll d\). Let \(\mathbf{B}_{\perp}\in\mathbb{O}^{d\times(d-k)}\) denote a matrix whose columns

Figure 4: Spectral norm of \(\mathbf{M}\) during pretraining on tasks drawn from \(D(\mathcal{F}_{1}^{+})\) in **Left, Middle-Right** and \(D(\mathcal{F}_{1}^{\text{cos}})\) in **Middle-Left, Right**. **Left, Middle-Left** show ablations over the noise standard deviation \(\sigma\) and **Middle-Right, Right** show ablations over the number of context samples \(n\).

form an orthonormal basis for the subspace perpendicular to \(\text{col}(\mathbf{B})\), and note that the Lipschitzness of \(\mathcal{F}_{\mathbf{B}}^{\text{lin}}\) in the direction \(\mathbf{w}\) is \(L\) if \(\mathbf{w}\in\text{col}(\mathbf{B})\) and 0 if \(\mathbf{w}\in\text{col}(\mathbf{B}_{\perp})\). Observe that any function in \(\mathcal{F}_{\mathbf{B}}^{\text{lin}}\) can be learned by projecting the input onto the non-zero Lipschitzness directions, i.e. \(\text{col}(\mathbf{B})\), then solving a \(k\ll d\)-dimensional regression. To formally study whether softmax attention recovers \(\text{col}(\mathbf{B})\), we assume the covariates are generated as follows.

**Assumption 4.3** (Covariate Distribution).: _There are fixed constants \(c_{\mathbf{u}}\neq 0\) and \(-\infty<c_{\mathbf{v}}<\infty\) s.t. sampling \(\bm{x}_{i}\sim D_{\bm{x}}\) is equivalent to \(\bm{x}_{i}=c_{\mathbf{u}}\mathbf{B}\mathbf{u}_{i}+c_{\mathbf{v}}\mathbf{B}_{ \perp}\mathbf{v}_{i}\) where \(\mathbf{u}_{i}\sim\mathcal{U}^{k}\) and \(\mathbf{v}_{i}\sim\mathcal{U}^{d-k}\)._

Assumption 4.3 entails that the data is generated by latent variables \(\mathbf{u}_{i}\) and \(\mathbf{v}_{i}\) that determine label-relevant and spurious features. This may be interpreted as a continuous analogue of dictionary learning models studied in feature learning works [33; 34]. We require no finite upper bound on \(|c_{\mathbf{v}}|\) nor \(\frac{1}{|c_{\mathbf{u}}|}\), so the data may be dominated by spurious features.

**Theorem 4.4**.: _Let \(\mathbf{B}\in\mathbb{O}^{d\times k}\) and consider the pretraining population loss (ICL) with \(f\sim D(\mathcal{F}_{\mathbf{B}}^{\text{lin}})\). Suppose Assumption 4.3 holds, as well as at least one of two cases: (Case 1) \(\sigma=0\), or (Case 2) \(n=2\). Then among all \(\mathbf{M}\in\mathcal{M}:=\{\mathbf{M}\in\mathbb{R}^{d\times d}:\mathbf{M}= \mathbf{M}^{\top},\|\mathbf{B}^{\top}\mathbf{M}\mathbf{B}\|\leq\frac{1}{c_{ \mathbf{u}}^{2}}\}\), the minimizer of the pretraining population loss (ICL) is \(\mathbf{M}^{\star}=c\mathbf{B}\mathbf{B}^{\top}\) for some \(c\in(0,\frac{1}{c_{\mathbf{u}}^{2}}]\)._

Theorem 4.4 shows that softmax attention can achieve dimensionality reduction during ICL on any downstream task that has non-zero Lipschitzness only in \(\text{col}(\mathbf{B})\) by removing the zero-Lipschitzness features while pretraining on \(\mathcal{F}_{\mathbf{B}}^{\text{lin}}\). Removing the zero-Lipschitzness features entails that the nearest neighbor prediction of pretrained softmax attention uses a neighborhood, i.e. attention window, defined strictly by projections of the input onto \(\text{col}(\mathbf{B})\). To our knowledge, this is the _first result showing that softmax attention pretrained on ICL tasks recovers a shared low-dimensional structure among the tasks_. Please see Appendix J for empirical results verifying that softmax attention indeed recovers low-dimensional structure, even for tasks consisting of (nonlinear) generalized linear models.

## 5 Conclusion

We have presented, to our knowledge, the first results showing that softmax attention learns shared structure among pretraining tasks that facilitates downstream ICL. Moreover, we have provided empirical evidence suggesting that our conclusions about what softmax attention learns during pretraining generalize to function classes beyond those considered in our analysis.

**Limitations and Future Work. 1.** The model we use in this work is an attempt to understand a phenomenon that emerges in LLMs, which is that the output of the model can be 'primed' with some examples provided in the context that resembles few-shot learning, even though they are only trained on next token prediction. Establishing a mathematical framework for this remains an interesting question. **2.** We consider the output of a single layer of attention. Studying the nature of the solution when this is iterated over multiple trained layers is an interesting future prospect.

Figure 5: **Left, Middle-Left, Middle-Right: The test error for softmax attention as it is trained on the distributions over 1-Lipschitz affine, ReLU, and cosine function (\(D(\mathcal{F}_{1}^{\text{aff}})\), \(D(\mathcal{F}_{1}^{+})\), and \(D(\mathcal{F}_{1}^{\text{cos}})\), respectively), where the error is evaluated at each pretraining iteration on 5 tasks drawn from the distributions over the 1-Lipschitz (affine, ReLU, cosine) function classes in (Left, Middle-Left, Middle-Right), respectively. Right: The test error evaluated on tasks drawn from \(D(\mathcal{F}_{1}^{\text{cos}})\) for three softmax attention trained on tasks drawn from \(D(\mathcal{F}_{1}^{\text{aff}})\), \(D(\mathcal{F}_{0.1}^{\text{cos}})\), and \(D(\mathcal{F}_{10}^{\text{cos}})\), respectively.**

## 6 Acknowledgments

This work was supported in part by NSF Grants 2127697, 2019844, 2107037, and 2112471, ARO Grant W911NF2110226, ONR Grant N00014-19-1-2566, the Machine Learning Lab (MLL) at UT Austin, and the Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program.

## References

* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Chowdhery et al. [2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.
* Min et al. [2022] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2022.
* Rae et al. [2021] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021.
* Thoppilan et al. [2022] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_, 2022.
* Ahn et al. [2023] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning, 2023.
* Mahankali et al. [2023] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. _arXiv preprint arXiv:2307.03576_, 2023.
* Zhang et al. [2023] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_, 2023.
* Cheng et al. [2023] Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to learn non-linear functions in context. _arXiv preprint arXiv:2312.06528_, 2023.
* Oswald et al. [2023] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Akyurek et al. [2022] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* Bai et al. [2023] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection, 2023.

* [14] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. _arXiv preprint arXiv:2310.17086_, 2023.
* [15] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. _arXiv preprint arXiv:2301.13196_, 2023.
* [16] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In _International Conference on Machine Learning_, pages 19565-19594. PMLR, 2023.
* [17] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? _arXiv preprint arXiv:2310.08391_, 2023.
* [18] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.
* [19] Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. _arXiv preprint arXiv:2301.11916_, 2023.
* [20] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. _arXiv preprint arXiv:2305.19420_, 2023.
* [21] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. _arXiv preprint arXiv:2310.05249_, 2023.
* [22] Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained as kernel regression. _arXiv preprint arXiv:2305.12766_, 2023.
* [23] Yingyi Chen, Qinghua Tao, Francesco Tonin, and Johan AK Suykens. Primal-attention: Self-attention through asymmetric kernel svd in primal representation. _arXiv preprint arXiv:2305.19798_, 2023.
* [24] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: a unified understanding of transformer's attention via the lens of kernel. _arXiv preprint arXiv:1908.11775_, 2019.
* [25] Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho. Fourier-former: Transformer meets generalized fourier integral theorem. _Advances in Neural Information Processing Systems_, 35:29319-29335, 2022.
* [26] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Designing robust transformers using robust kernel density estimation. _arXiv preprint arXiv:2210.05794_, 2022.
* [27] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression, 2023.
* [28] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* [29] Kathryn A Prewitt. A distribution-free theory of nonparametric regression. laszlo gyorfi, michael kohler, adam krzyzak, and harro walk. _Journal of the American Statistical Association_, 98(464):1084-1084, 2003.
* [30] Elizbar A Nadaraya. On estimating regression. _Theory of Probability & Its Applications_, 9(1):141-142, 1964.

* [31] Geoffrey S Watson. Smooth regression analysis. _Sankhya: The Indian Journal of Statistics, Series A_, pages 359-372, 1964.
* [32] Samuele Tosatto, Riad Akrour, and Jan Peters. An upper bound of the bias of nadaraya-watson kernel regression under lipschitz assumptions. _Stats_, 4(1):1-17, 2021.
* [33] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In _International Conference on Machine Learning_, pages 11112-11122. PMLR, 2021.
* [34] Zhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. _arXiv preprint arXiv:2206.01717_, 2022.
* [35] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. _arXiv preprint arXiv:2306.15063_, 2023.
* [36] Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. _arXiv preprint arXiv:2309.05858_, 2023.
* [37] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. _arXiv preprint arXiv:2212.10559_, 2022.
* [38] Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? _arXiv preprint arXiv:2310.08540_, 2023.
* [39] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* [40] Asher Trockman and J. Zico Kolter. Mimetic initialization of self-attention layers, 2023.
* [41] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. _arXiv preprint arXiv:2310.00535_, 2023.
* [42] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase, 2023.
* [43] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. _arXiv preprint arXiv:2303.04245_, 2023.
* [44] Samy Jelassi, Michael E. Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure, 2022.
* [45] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. _arXiv preprint arXiv:2302.06015_, 2023.
* [46] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. _arXiv preprint arXiv:2308.16898_, 2023.
* [47] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism, 2023.
* [48] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. _arXiv preprint arXiv:2310.08566_, 2023.

* [49] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. _arXiv preprint arXiv:2306.02896_, 2023.
* [50] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. _arXiv preprint arXiv:2310.10616_, 2023.
* [51] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* [52] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. _arXiv preprint arXiv:2210.10749_, 2022.
* [53] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a study through the random features lens. _arXiv preprint arXiv:2307.11353_, 2023.
* [54] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. _The Journal of Machine Learning Research_, 22(1):3463-3497, 2021.
* [55] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* [56] Satwik Bhattacharya, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. _arXiv preprint arXiv:2009.11264_, 2020.
* [57] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. _arXiv preprint arXiv:2106.03764_, 2021.
* [58] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022.
* [59] Zhao Song, Guangyi Xu, and Junze Yin. The expressibility of polynomial based attention scheme, 2023.
* [60] Kevin Christian Wibisono and Yixin Wang. On the role of unstructured training data in transformers' in-context learning capabilities. In _NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning_, 2023.
* [61] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. _arXiv preprint arXiv:2310.11685_, 2023.
* [62] James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of attention, 2020.
* [63] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention, 2020.
* [64] Herbert Robbins. A remark on stirling's formula. _The American Mathematical Monthly_, 62(1):26-29, 1955.
* [65] Brian Knaeble. Variations on the projective central limit theorem. _https://arxiv.org/pdf/0904.1048.pdf_, 2015.
* [66] Elliott H Lieb and Michael Loss. _Analysis_, volume 14. American Mathematical Soc., 2001.
* [67] G.H. Hardy, J.E. Littlewood, and G. Polya. _Inequalities_. Cambridge Mathematical Library. Cambridge University Press, 1952.

###### Contents

* 1 Introduction
	* 1.1 Additional Related Work
* 2 Preliminaries
* 3 Pretraining Learns Scale of Attention Window
	* 3.1 Proof Sketch
	* 3.2 Experiments
* 4 Softmax Attention Learns Direction of Attention Window
* 5 Conclusion
* 6 Acknowledgments
* A Additional Related Work
* B Preliminaries
* B.1 Rewriting the Loss
* C The Signal Term
* C.1 Affine functions
* C.2 ReLU-based functions
* D Bounds on Noise Variance
* E Optimizing the Loss
* E.1 Generalization Bounds
* F Lower Bound for Linear Attention
* G Bounds for \(g_{p}(r)\)
* G.1 Bounds on Spherical Caps
* G.2 Bounds on \(g_{p}(r)\)
* H Attention Window Captures Appropriate Directions
* H.1 Proof Sketch
* H.2 Full Proof
* I Additional Lemmas
* J Additional Experiments and Details
* J.1 Low-Rank Experiments

Additional Related Work

**Empirical study of ICL.** Several works have studied ICL of linear tasks in the framework introduced by [28], and demonstrated that pretrained transformers can mimic the behavior of gradient descent [11, 12, 13, 28], Newton's method [14], and certain algorithm selection approaches [13, 16]. [35] studied the same linear setting with the goal of understanding the role of pretraining task diversity, while [36] argued via experiments on general auto-regressive tasks that ICL implicitly constructs a learning objective and optimizes it within one forward pass. Other empirical works have both directly supported [37] and contradicted [38] the hypothesis that ICL is a gradient-based optimization algorithm via experiments on real ICL tasks, while [39] empirically concluded that induction heads with softmax attention are the key mechanism that enables ICL in transformers. Lastly, outside of the context of ICL, [40] noticed that the attention parameter matrices of trained transformers are often close to scaled identities in practice, consistent with our findings on the importance of learning a scale to softmax attention training.

**Transformer training dynamics.**[21] and [41] studied the dynamics of softmax attention trained with gradient descent, but assumed orthonormal input features and either linear tasks [21] or that the softmax normalization is a fixed constant [41]. [42] proved that softmax attention with diagonal weight matrices incrementally learns features during gradient-based training. Other work has shown that trained transformers can learn topic structure [43], spatial structure [44], visual features [45] and support vectors [46, 47] in specific settings disjoint from ICL.

**Expressivity of transformers.** Multiple works have shown that transformers with linear [11, 36], ReLU [13, 14, 48], and softmax [12, 15] attention are expressive enough to implement general-purpose machine learning algorithms during ICL, including gradient descent. A series of works have shown the existence of transformers that recover sparse functions of the input data [49, 50, 51, 52]. [53] studied the statistical complexity the learning capabilities of attention with random weights. More broadly, [54, 55, 56, 57, 58, 59] have analyzed various aspects of the expressivity of transformers.

**Other studies of softmax attention.**[60] hypothesized that the role of the softmax in attention is to facilitate a mixture-of-experts algorithm amenable to unstructured training data. [27] formulated a softmax regression problem and analyzed the convergence of a stylized algorithm to solve it. [22] showed that in a setting with ICL regression tasks a la [28], a kernel regressor akin to softmax attention with \(\mathbf{M}\) equal to the inverse covariance of \(\mathbf{x}\) converges to the Bayes posterior for a new ICL task - in this setting the conditional distribution of the label given the query and \(n\) labelled context samples - polynomially with the number of context samples, but did not study what softmax attention learns during pretraining. [61] also compared softmax and linear attention, but focused on softmax's greater capacity to separate data from two classes. [62] and [63] investigate the Lipschitz constant of attention rather than what attention learns.

**Non-parametric regression.** Our results imply that pretraining softmax attention reduces to the problem of meta-learning the bandwidth of a Nadaraya-Watson estimator with a Gaussian kernel. However, to our knowledge, the non-parametric regression literature has not addressed this problem. The closest work is [32], which only upper bounds the noiseless loss, and only in the limit \(n\rightarrow\infty\), whereas our result characterizes the optimal bandwidth, which requires upper and lower bounds on the noisy loss.

## Appendix B Preliminaries

We first justify our claim that the first \(d\) rows of the last column of \(\mathbf{W}_{V}\) can be set to \(\mathbf{0}_{d}\) for any optimal choice of parameters.

**Lemma B.1**.: _If under the function distribution, a function \(f\) is equally likely as likely as \(-f\), then any optimal solution to \(\mathcal{L}(\mathbf{W}_{V},\mathbf{W}_{K},\mathbf{W}_{Q})\) in 3 satisfies \(\mathbf{W}_{V}=\begin{pmatrix}\mathbf{0}_{d\times d}&\mathbf{0}_{d\times 1} \\ \mathbf{0}_{1\times d}&c\end{pmatrix}\)._Proof.: For readability we write \(\beta_{i}=e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x}_{n+1}\,\|^{2}}\sum_{j}e^{-w_{KQ}\|\,\bm{x }_{j}-\bm{x}_{n+1}\,\|^{2}}\) Suppose \(\mathbf{W}_{V}=\begin{pmatrix}\mathbf{0}_{d\times d}&\mathbf{v}\\ \mathbf{0}_{1\times d}&c\end{pmatrix}\) was optimal, then the loss can be written

\[\mathcal{L}=\mathbb{E}_{f,\{\bm{x}_{i}\}}\left[\left(\sum_{i}c\left(f(\bm{x}_{ i})+\epsilon_{i}\right)\beta_{i}+\sum_{i}\mathbf{v}^{\top}\,\bm{x}_{i}\,\beta_{i}-f( \bm{x}_{n+1})\right)^{2}\right].\]

But because \(f\) and \(-f\) are equally likely, and because the noise is also symmetric about 0, we can write this as

\[\mathcal{L} =\frac{1}{2}\,\mathbb{E}_{f,\{\bm{x}_{i}\},\{\epsilon_{i}\}} \left[\left(\sum_{i}c\left(f(\bm{x}_{i})+\epsilon_{i}\right)\beta_{i}+\sum_{i }\mathbf{v}^{\top}\,\bm{x}_{i}\,\beta_{i}-f(\bm{x}_{n+1})\right)^{2}\right]\] \[\quad+\frac{1}{2}\,\mathbb{E}_{f,\{\bm{x}_{i}\},\{\epsilon_{i}\}} \left[\left(\sum_{i}c\left((-f)(\bm{x}_{i})-\epsilon_{i}\right)\beta_{i}+\sum _{i}\mathbf{v}^{\top}\,\bm{x}_{i}\,\beta_{i}-(-f)(\bm{x}_{n+1})\right)^{2}\right]\]

We can couple the noise \(\{\epsilon_{i}\}\) and the data \(\{\bm{x}_{i}\}\) in the two summands above to write this as

\[\mathbb{E}\left[(A+B+C)^{2}+(-A+B-C)^{2}\right],\]

where \(A=\sum_{i}cf(\bm{x}_{i})\beta_{i}-f(\bm{x})=-(\sum_{i}c(-f)(\bm{x}_{i})\beta_ {i})\), \(B=\sum_{i}\mathbf{v}^{\top}\,\bm{x}_{i}\,\beta_{i}\), and \(C=\sum_{i}c\epsilon_{i}\beta_{i}\). We can set \(B=0\) simply by setting \(\mathbf{v}=\overline{\mathbf{0}}_{d\times 1}\), and this has loss

\[\mathcal{L} =\mathbb{E}_{f,\{\bm{x}_{i}\}}\left[\left(\sum_{i}c\left(f(\bm{x} _{i})+\epsilon_{i}\right)\beta_{i}-f(\bm{x}_{n+1})\right)^{2}\right]\] \[=\frac{1}{2}\left(\mathbb{E}\left[(A+C)^{2}+(-A-C)^{2}\right] \right)\leq\frac{1}{2}\left(\mathbb{E}\left[(A+B+C)^{2}+(-A+B-C)^{2}\right]\right)\]

In all of the distributions over functions we consider for pretraining, \(f\) is equally likely as \(-f\), so without loss of generality we set all elements of \(\mathbf{W}_{V}\) besides the \((d+1,d+1)\)-th to \(0\). For simplicity, we set the \((d+1,d+1)\)-th element to 1.

**Assumption B.2** (Covariate Distribution).: _For each token \(\bm{x}\), first we draw \(\tilde{\bm{x}}\) as \(\tilde{\bm{x}}\sim\mathcal{U}^{d}\). Then \(\bm{x}\) is constructed as \(\bm{x}=\bm{\Sigma}^{1/2}\tilde{\bm{x}}\)._

**Definition B.3** (Linear and 2-ReLU Function Classes).: _The function classes \(\mathcal{F}_{L}^{\text{in}}\) and \(\mathcal{F}_{L}^{+}\) are respectively defined as:_

\[\mathcal{F}_{L}^{\text{in}} :=\{f_{\mathbf{w}}:f_{\mathbf{w}}(\bm{x})=l\mathbf{w}^{\top}\,\bm{ x}+b,\,\,\mathbf{w}\in\mathbb{S}^{d-1},\,\,l\in[-L,L]\},\] (5) \[\mathcal{F}_{L}^{+} :=\{f_{\mathbf{w}}:f_{\mathbf{w}}(\bm{x})=l_{1}\,\text{ReLU}( \mathbf{w}^{\top}\,\bm{x})+l_{2}\,\text{ReLU}(-\mathbf{w}^{\top}\,\bm{x})+b,\, \,\mathbf{w}\in\mathbb{S}^{d-1}\}.\] (6)

\(D(\mathcal{F}_{L}^{\text{in}}),D(\mathcal{F}_{L}^{+})\) _are induced by drawing \(\mathbf{w}\sim\mathcal{N}(\mathbf{0},\bm{\Sigma}^{-1})\) and \(b,l,l_{1},l_{2}\sim\text{Unif}([-L,L])\). We say that these classes are \(L-\)Lipschitz, because the maximum Lipschitz constant for any function in the class is \(L\)._

Note that because \(\|\bm{\Sigma}^{-1/2}\,\bm{x}_{i}\,\|=1\) always, we have

\[2\,\bm{x}_{i}\,\mathbf{M}\,\bm{x}_{n+1}\] \[=\|\bm{\Sigma}^{-1/2}\,\bm{x}_{i}\,\|^{2}+\|\bm{\Sigma}^{1/2} \mathbf{M}\bm{\Sigma}^{1/2}\bm{\Sigma}^{-1/2}\,\bm{x}_{n+1}\,\|^{2}-\|\bm{ \Sigma}^{-1/2}\,\bm{x}_{i}-\bm{\Sigma}^{1/2}\mathbf{M}\bm{\Sigma}^{1/2}\bm{ \Sigma}^{-1/2}\,\bm{x}_{n+1}\,\|^{2}.\]

Let \(\mathbf{M}^{\prime}=\bm{\Sigma}^{1/2}\mathbf{M}\bm{\Sigma}^{1/2}\). This means the attention estimator can be rewritten as

\[h_{SA}(\bm{x}):=\sum_{i}\frac{f(\bm{x}_{i})e^{\bm{x}_{i}^{\top}\,\mathbf{M}\, \bm{x}_{n+1}}}{\sum_{j}e^{\bm{x}_{j}^{\top}\,\mathbf{M}\,\bm{x}_{n+1}}}=\sum_{i }\frac{f(\bm{x}_{i})e^{-\|\bm{\Sigma}^{-1/2}\,\bm{x}_{i}\,-\mathbf{M}^{\prime} \bm{\Sigma}^{-1/2}\,\bm{x}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\bm{\Sigma}^{-1/2}\, \bm{x}_{j}\,-\mathbf{M}^{\prime}\bm{\Sigma}^{-1/2}\,\bm{x}_{n+1}\,\|^{2}}}\] (7)

So the attention a token \(\bm{x}_{n+1}\) places on another \(\bm{x}_{i}\) is related to the distance between

\(\mathbf{M}^{\prime}\bm{\Sigma}^{-1/2}\,\bm{x}_{n+1}\) and \(\bm{\Sigma}^{-1/2}\,\bm{x}_{i}\). It is natural to suppose under some symmetry conditions that \(\mathbf{M}^{\prime}\) is best chosen to be a scaled identity matrix so that the attention actually relates to a distance between tokens. Below we discus sufficient conditions for this.

**Assumption B.4**.: _The function class \(\mathcal{F}\) and distribution \(D(\mathcal{F})\) satisfy_

1. \(|f(\bm{x})-f(\bm{y})|\leq L\|\,\bm{x}-\bm{y}\,\|_{\bm{\Sigma}^{-1}}\,\forall\, \bm{x},\bm{y}\in\mathcal{X}^{2},f\in\mathcal{F}\)__
2. \(\mathbb{E}_{f\sim D(\mathcal{F})}\left[f(\bm{x})f(\bm{y})\right]=\rho(\bm{x}^{ \top}\,\bm{y})\,\forall\,\bm{x},\bm{y}\in\mathcal{X}^{2},\) _for some monotonically increasing_ \(\rho\)_._
3. _For any isometry_ \(\phi\) _preserving the unit sphere, and_ \(f\in\mathcal{F}\)_, we have_ \(f\circ\phi\in\mathcal{F}\)_._

**Lemma B.5**.: _Under Assumption B.4, any minimizer of Equation ICL satisfies \(\mathbf{M}^{*}=w_{KQ}\bm{\Sigma}^{-1}\) for some scalar \(w_{KQ}\geq 0\)._

Proof.: Let \(\{\bm{y}_{i}\}=\{\bm{\Sigma}^{-1/2}\,\bm{x}_{i}\}\). Suppose \(\mathbf{M}\,\bm{y}_{n+1}\neq c\,\bm{y}_{n+1}\) for any \(c>0\) for some \(\bm{y}_{n+1}\). Take \(c_{\bm{y}_{n+1}}=\|\mathbf{M}\,\bm{y}_{n+1}\|\) and \(\bm{y}^{\prime}_{n+1}=\frac{\mathbf{M}\,\bm{y}_{n+1}}{c_{\bm{y}_{n+1}}}\) (the projection of \(\bm{y}\) onto the sphere). Consider a function \(\omega:\mathbb{R}^{d}\to\mathbb{R}^{d}\) satisfying \(\omega(\bm{y}_{n+1})=c_{\bm{y}_{n+1}}\,\bm{y}_{n+1}\). Note that this need not be linear. Let \(\phi\) denote a rotation that sends \(\bm{y}^{\prime}_{n+1}\) to \(\bm{y}_{n+1}\).

We show that \(\mathcal{L}(\mathbf{M})>\mathcal{L}(\omega)\), that is, it is favorable to _not_ rotate \(\bm{y}_{n+1}\). We have

\[\mathcal{L}(\mathbf{M}) =\mathbb{E}_{f,\bm{y}_{n+1},\{\bm{y}_{i}\}}\left[\left(f(\bm{y}_ {n+1})-\frac{\sum_{i}f(\bm{y}_{i})e^{-\|\,\bm{y}_{i}-\mathbf{M}\,\bm{y}_{n+1} \,\|^{2}}}{\sum_{j}e^{-\|\,\bm{y}_{j}-\mathbf{M}\,\bm{y}_{n+1}\,\|^{2}}}\right) ^{2}\right]\] \[=\mathbb{E}_{f,\bm{y}_{n+1},\{\bm{y}_{i}\}}\,f(\bm{y}_{n+1})^{2} +\mathbb{E}_{f,\bm{y}_{n+1},\{\bm{y}_{i}\}}\left[\left(\frac{\sum_{i}f(\bm{y} _{i})e^{-\|\,\bm{y}_{i}-\mathbf{M}\,\bm{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\,\bm{ y}_{j}-\mathbf{M}\,\bm{y}_{n+1}\,\|^{2}}}\right)^{2}\right]\] \[\qquad\quad-2\,\mathbb{E}_{f,\bm{y}_{n+1},\{\bm{y}_{i}\}}\left[ \sum_{i}\frac{f(\bm{y}_{n+1})f(\bm{y}_{i})e^{-\|\,\bm{y}_{i}-\mathbf{M}\,\bm{y} _{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\,\bm{y}_{j}-\mathbf{M}\,\bm{y}_{n+1}\,\|^{2}}}\right]\]

Lets compare this with the loss of \(\omega\). For a depiction of this, please see Figure 6

\[\mathcal{L}(\omega) =\mathbb{E}_{f,\bm{y}_{n+1},\{\bm{y}_{i}\}}\left[\left(f(\bm{y}_ {n+1})-\frac{\sum_{i}f(\bm{y}_{i})e^{-\|\,\bm{y}_{i}-\omega(\bm{y}_{n+1})\|^{2} }}{\sum_{j}e^{-\|\,\bm{y}_{j}-\omega(\bm{y}_{n+1})\|^{2}}}\right)^{2}\right]\] \[=\mathbb{E}_{f,\bm{y}_{n+1},\{\bm{y}_{i}\}}\,f(\bm{y}_{n+1})^{2}+ \mathbb{E}_{f,\bm{y}_{n+1},\{\bm{y}_{i}\}}\left[\left(\frac{\sum_{i}f(\bm{y}_ {i})e^{-\|\,\bm{y}_{i}-\omega(\bm{y}_{n+1})\|^{2}}}{\sum_{j}e^{-\|\,\bm{y}_{j} -\omega(\bm{y}_{n+1})\|^{2}}}\right)^{2}\right]\]

Figure 6: Comparison between using \(\mathbf{M}\) and \(\omega\) in Lemma B.5. Here we denote \(\bm{y}:=\bm{y}_{n+1}\). Under the attention induced by \(\mathbf{M}\), the center of attention for \(\bm{y}\) is actually \(\bm{y}^{\prime}\), and the attention weights are depicted by the light orange shading. Under the attention induced by \(\omega\), the center of attention for \(\bm{y}\) is \(\bm{y}\) and the weights are depicted by the light blue shading. Naturally, using the blue shaded attention should lead to a better estimate of \(f(\mathbf{y})\) under mild regularity conditions.

\[-2\,\mathbb{E}_{f,\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}\left[ \sum_{i}\frac{f(\boldsymbol{y}_{n+1})f(\boldsymbol{y}_{i})e^{-\|\,\boldsymbol{y }_{i}-\omega(\boldsymbol{y}_{n+1})\|^{2}}}{\sum_{j}e^{-\|\,\boldsymbol{y}_{j} -\omega(\boldsymbol{y}_{n+1})\|^{2}}}\right]\]

There are three terms to compare. The first in each is identical. The second is also the same:

\[\mathbb{E}_{f,\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}\left[ \left(\frac{\sum_{i}f(\boldsymbol{y}_{i})e^{-\|\,\boldsymbol{y}_{i}-\text{M} \,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\,\boldsymbol{y}_{j}-\text{M} \,\boldsymbol{y}_{n+1}\,\|^{2}}}\right)^{2}\right]\] \[=\mathbb{E}_{\boldsymbol{y}_{n+1}}\mathbb{E}_{f,\{\boldsymbol{y} _{i}\}}\left[\left(\frac{\sum_{i}f(\boldsymbol{y}_{i})e^{-\|\,\boldsymbol{y}_ {i}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\,\boldsymbol{y}_{ j}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}\right)^{2}\right]\] \[=\mathbb{E}_{\boldsymbol{y}_{n+1}}\mathbb{E}_{f,\{\boldsymbol{y} _{i}\}}\left[\left(\frac{\sum_{i}f(\boldsymbol{y}_{i})e^{-\|\,\boldsymbol{y}_ {i}-c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\| \,\boldsymbol{y}_{j}-c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1}\,\|^{2}} }\right)^{2}\right]\] \[=\mathbb{E}_{\boldsymbol{y}_{n+1}}\mathbb{E}_{f,\{\boldsymbol{y} _{i}\}}\left[\left(\frac{\sum_{i}f(\boldsymbol{y}_{i})e^{-\|\,\boldsymbol{y}_ {i}-c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\| \,\boldsymbol{y}_{j}-c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1}\,\|^{2}} }\right)^{2}\right]\] rotational symmetry of \[\{\boldsymbol{y}_{i}\},\boldsymbol{y}_{n+1}\] \[=\mathbb{E}_{\boldsymbol{y}_{n+1}}\mathbb{E}_{f,\{\boldsymbol{y} _{i}\}}\left[\left(\frac{\sum_{i}f(\boldsymbol{y}_{i})e^{-\|\,\boldsymbol{y}_ {i}-c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\| \,\boldsymbol{y}_{j}-c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1}\,\|^{2}} }\right)^{2}\right]\] rotational symmetry of \[\{\boldsymbol{y}_{i}\}\]

The third takes some more work. For any choice of \(\{\boldsymbol{y}_{i}\}\), let

\[\alpha_{\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}(\boldsymbol{y}_{*})= \frac{e^{-\|\,\boldsymbol{y}_{n+1}-\boldsymbol{y}_{*}\,\|^{2}}}{e^{-\|\, \boldsymbol{y}_{n+1}-\boldsymbol{y}_{*}\,\|^{2}}+\sum_{j}e^{-\|\,\boldsymbol {y}_{n+1}-\boldsymbol{y}_{i}\,\|^{2}}}.\]

We see that \(\alpha_{\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}(\boldsymbol{y}_{*})\) varies monotonically with \(\boldsymbol{y}_{n+1}^{\top}\,\boldsymbol{y}_{*}\) for all \(\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}\). That is,

\[\boldsymbol{y}_{*}^{\top}\,\boldsymbol{y}_{n+1}>\boldsymbol{y}_{*}^{\prime \top}\,\boldsymbol{y}_{n+1}\implies\alpha_{\boldsymbol{y}_{n+1},\{ \boldsymbol{y}_{i}\}}(\boldsymbol{y}_{*})>\alpha_{\boldsymbol{y}_{n+1},\{ \boldsymbol{y}_{i}\}}(\boldsymbol{y}_{*}^{\prime}),\]

\[\mathbb{E}_{f,\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}\left[ \sum_{i}\frac{f(\boldsymbol{y}_{n+1})f(\boldsymbol{y}_{i})e^{-\|\,\boldsymbol{y }_{i}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\,\boldsymbol{y}_ {j}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}\right]\] \[=\mathbb{E}_{\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}\left[ \sum_{i}\frac{\mathbb{E}_{f}\left[f(\boldsymbol{y}_{n+1})f(\boldsymbol{y}_{i}) \right]e^{-\|\,\boldsymbol{y}_{i}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{ \sum_{j}e^{-\|\,\boldsymbol{y}_{j}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}\right]\] \[=\mathbb{E}_{\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}\left[ \sum_{i}\frac{\rho(\boldsymbol{y}_{n+1}^{\top}\,\boldsymbol{y}_{i})e^{-\|\, \boldsymbol{y}_{i}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\| \,\boldsymbol{y}_{j}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}\right]\] \[=\mathbb{E}_{\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}\left[ \sum_{i}\frac{\rho(\boldsymbol{y}_{n+1}^{\top}\,\boldsymbol{y}_{i})e^{-\|\, \boldsymbol{y}_{i}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\, \boldsymbol{y}_{j}-\text{M}\,\boldsymbol{y}_{n+1}\,\|^{2}}}\right]\] \[=n\,\mathbb{E}_{\boldsymbol{y}_{n+1},\boldsymbol{y}_{*},\{ \boldsymbol{y}_{i}\}_{i=[n-1]}}\left[\rho(\boldsymbol{y}_{n+1}^{\top}\, \boldsymbol{y}_{*})\alpha_{\boldsymbol{\text{M}}\,\boldsymbol{y}_{n+1},\{ \boldsymbol{y}_{i}\}}(\boldsymbol{y}_{*})\right)\right]\] \[=n\,\mathbb{E}_{\boldsymbol{y}_{n+1},\boldsymbol{y}_{*},\{ \boldsymbol{y}_{i}\}_{i=[n-1]}}\left[\rho(\boldsymbol{y}_{n+1}^{\top}\, \boldsymbol{y}_{*})\alpha_{c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}^{\prime},\{ \boldsymbol{y}_{i}\}}(\boldsymbol{y}_{*}))\right]\] \[=n\,\mathbb{E}_{\boldsymbol{y}_{n+1},\boldsymbol{y}_{*},\{ \boldsymbol{y}_{i}\}_{i=[n-1]}}\left[\rho(\boldsymbol{y}_{n+1}^{\top}\, \boldsymbol{y}_{*})\alpha_{c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1},\{ \boldsymbol{y}_{i}\}}(\phi^{-1}(\boldsymbol{y}_{*}))\right)\right]\] \[=n\,\mathbb{E}_{\boldsymbol{y}_{n+1},\boldsymbol{y}_{*},\{ \boldsymbol{y}_{i}\}_{i=[n-1]}}\left[\rho(\boldsymbol{y}_{n+1}^{\top}\, \boldsymbol{y}_{*})\alpha_{c_{\boldsymbol{y}_{n+1}}\,\boldsymbol{y}_{n+1},\{ \boldsymbol{y}_{i}\}}(\phi^{-1}(\boldsymbol{y}_{*}))\right)\right]\]

Similarly, we have

\[\mathbb{E}_{f,\boldsymbol{y}_{n+1},\{\boldsymbol{y}_{i}\}}\left[ \sum_{i}\frac{f(\boldsymbol{y}_{n+1})f(\boldsymbol{y}_{i})e^{-\|\, \boldsymbol{y}_{i}-\omega(\boldsymbol{y}_{n+1})\|^{2}}}{\sum_{j}e^{-\|\, \boldsymbol{y}_{j}-\omega(\boldsymbol{y}_{n+1})\|^{2}}}\right]\]\[=\mathbb{E}_{\bm{y}_{n+1},\{\bm{y}_{*}\}}\left[\sum_{i}\frac{\mathbb{E} _{f}\left[f(\bm{y}_{n+1})f(\bm{y}_{i})\right]e^{-\|\,\bm{y}_{i}-c_{\bm{y}_{n+1}} \,\bm{y}_{n+1}\,\|^{2}}}{\sum_{j}e^{-\|\,\bm{y}_{j}-c_{\bm{y}_{n+1}}\,\bm{y}_{ n+1}\,\|^{2}}}\right]\] \[=\mathbb{E}_{\bm{y}_{n+1},\{\bm{y}_{i}\}}\left[\sum_{i}\frac{\rho (\bm{y}_{n+1}^{\top}\,\bm{y}_{i})e^{-\|\,\bm{y}_{i}-c_{\bm{y}_{n+1}}\,\bm{y}_{ n+1}\,\|^{2}}}{\sum_{j}e^{-\|\,\bm{y}_{j}-c_{\bm{y}_{n+1}}\,\bm{y}_{n+1}\,\|^{2}}}\right]\] \[=n\,\mathbb{E}_{\bm{y}_{n+1},\bm{y}_{*},\{\bm{y}_{i}\}_{i=[n-1]}} \left[\frac{\rho(\bm{y}_{n+1}^{\top}\,\bm{y}_{*})e^{-\|\,\bm{y}_{*}-c_{\bm{y}_ {n+1}}\,\bm{y}_{n+1}\,\|^{2}}}{e^{-\|\,\bm{y}_{*}-c_{\bm{y}_{n+1}}\,\bm{y}_{n+1} \,\|^{2}}+\sum_{j}e^{-\|\,\bm{y}_{j}-c_{\bm{y}_{n+1}}\,\bm{y}_{n+1}\,\|^{2}}}\right]\] \[=n\,\mathbb{E}_{\bm{y}_{n+1},\bm{y}_{*},\{\bm{y}_{i}\}_{i=[n-1]}} \left[\rho(\bm{y}_{n+1}^{\top}\,\bm{y}_{*})\alpha_{c_{\bm{y}_{n+1}}\,\bm{y}_{ n+1},\{\bm{y}_{i}\}}(\bm{y}_{*}))\right]\]

Critically, for a given \(\bm{y}_{n+1}\), \(\alpha_{\bm{y},\{\bm{y}_{i}\}}(\bm{y}_{*})\) can be re-parameterized as

\(\alpha_{\bm{y}_{n+1};\{\bm{y}_{i}\}}(\bm{y}_{*})=\alpha^{\prime}_{\{\bm{y}_{i }\}}(\bm{y}_{*}-\bm{y}_{n+1})\) where \(\alpha^{\prime}_{\{\bm{y}_{i}\}}\) is symmetric about \(0\) and decreasing. Similarly, \(\rho(\bm{y}_{n+1}^{\top}\,\bm{y}_{*})\) can be re-parameterized as \(\rho(\bm{y}_{n+1}^{\top}\,\bm{y}_{*})=\rho^{\prime}(\bm{y}_{*}-\bm{y}_{n+1})\) where \(\alpha^{\prime},\rho^{\prime}\) are symmetric decreasing rearrangement (that is, the set of points \(\bm{z}\) such that \(\rho(\bm{x})>r\) is a ball about the origin). From Lemma I.2 we then have

\[\mathbb{E}_{\bm{y}_{n+1}}\,\mathbb{E}\,\bm{y}_{*},\{\bm{y}_{i}\}_ {i=[n-1]}\left[\rho(\bm{y}_{n+1}^{\top}\,\bm{y}_{*})\alpha_{c_{\bm{y}_{n+1}}\, \bm{y}_{n+1},\{\bm{y}_{i}\}}(\phi^{-1}(\bm{y}_{*}))\right]\] \[=\mathbb{E}_{\bm{y}_{n+1}}\,\mathbb{E}\,\bm{y}_{*},\{\bm{y}_{i}\}_ {i=[n-1]}\left[\rho^{\prime}(\|\,\bm{y}_{n+1}-\bm{y}_{*}\,\|)\alpha_{\{\bm{y}_{ i}\}}(\|\,\bm{y}_{n+1}-\phi^{-1}\,\bm{y}_{*}\,\|)\right]\] \[<\mathbb{E}_{\bm{y}_{n+1}}\,\mathbb{E}\,\bm{y}_{*},\{\bm{y}_{i}\}_ {i=[n-1]}\left[\rho^{\prime}(\|\,\bm{y}_{n+1}-\bm{y}_{*}\,\|)\alpha_{\{\bm{y}_{ i}\}}(\|\,\bm{y}_{n+1}-\bm{y}_{*}\,\|)\right]\] \[=\mathbb{E}_{\bm{y}_{n+1}}\,\mathbb{E}\,\bm{y}_{*},\{\bm{y}_{i}\}_ {i=[n-1]}\left[\rho(\bm{y}_{n+1}^{\top}\,\bm{y}_{*})\alpha_{c_{\bm{y}_{n+1}}\, \bm{y}_{n+1},\{\bm{y}_{i}\}}(\bm{y}_{*})\right]\]

So \(\mathcal{L}(\omega)<\mathcal{L}(\mathbf{M})\). Let

\[q(c_{\bm{y}_{n+1}})=\mathbb{E}_{f,\{\bm{y}_{i}\}}\left[\left(f(\bm{y}_{n+1})- \frac{\sum_{i}f(\bm{y}_{i})e^{-\|\,\bm{y}_{i}-c_{\bm{y}_{n+1}}\,\bm{y}_{n+1}\, \|^{2}}}{\sum_{j}e^{-\|\,\bm{y}_{j}-c_{\bm{y}_{n+1}}\,\bm{y}_{n+1}\,\|^{2}}} \right)^{2}\right].\]

Observe that \(\mathcal{L}(\omega)=\mathbb{E}_{\bm{y}_{n+1}}\,q(c_{\bm{y}_{n+1}})\). We might as well set \(\omega\) to be such that \(c_{\bm{y}_{n+1}}\) is the same for all \(\bm{y}_{n+1}\) and a minimizer of \(q\), so we have \(\omega(\bm{y}_{n+1})=c\,\bm{y}_{n+1}\) for all \(\bm{y}_{n+1}\) which implies \(\omega=c\mathbf{I}_{d}\) for some \(c\). Because the optimal \(\mathbf{M}^{\prime}\) is identity, the corresponding optimal \(\mathbf{M}\) is \(\bm{\Sigma}^{-1}\). 

### Rewriting the Loss

As a result of this, we can take \(\mathbf{M}=w_{KQ}\bm{\Sigma}^{-1}\) and write the attention estimator as

\[h_{SA}(\bm{x})=\sum_{i}\frac{f(\bm{x}_{i})e^{-w_{KQ}\|\bm{\Sigma}^{-1/2}\,\bm{x}_ {i}-\bm{\Sigma}^{-1/2}\,\bm{x}_{n+1}\,\|^{2}}}{\sum_{j}e^{-w_{KQ}\|\bm{\Sigma}^{- 1/2}\,\bm{x}_{j}-\bm{\Sigma}^{-1/2}\,\bm{x}_{n+1}\,\|^{2}}}\] (8)

This allows us to make the transformation \(\mathcal{X}\to\bm{\Sigma}^{-1/2}\,\mathcal{X}\). This has the effect of making both the data covariance and the induced function class covariance equal to the identity. Essentially, WLOG we will henceforth consider \(\bm{\Sigma}=\mathbf{I}_{d}\). Henceforth, the estimator will be taken to be

\[h_{SA}(\bm{x})=\sum_{i}\frac{f(\bm{x}_{i})e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x}_{n+1}\, \|^{2}}}{\sum_{j}e^{-w_{KQ}\|\,\bm{x}_{j}-\bm{x}_{n+1}\,\|^{2}}}\] (9)

and the loss will be parameterized by \(w_{KQ}\) as

\[\mathcal{L}(w_{KQ})=\mathbb{E}_{f,\{\bm{x}_{i}\}}\left[\left(\sum_{i}\frac{f( \bm{x}_{i})+\epsilon_{i})\,e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x}_{n+1}\,\|^{2}}}{ \sum_{j}e^{-w_{KQ}\|\,\bm{x}_{j}-\bm{x}_{n+1}\,\|^{2}}}-f(\bm{x}_{n+1})\right)^{2} \right].\]

Because the noise \(\epsilon_{i}\) is independent of everything else, we can decompose this into two terms, a signal term and a noise term as follows

\[\mathcal{L}(w_{KQ})=\underbrace{\mathbb{E}_{f,\{\bm{x}_{i}\}} \left[\left(\sum_{i}\frac{\left(f(\bm{x}_{n+1})-f(\bm{x}_{i})\right)e^{-w_{KQ} \|\,\bm{x}_{i}-\bm{x}_{n+1}\,\|^{2}}}{\sum_{j}e^{-w_{KQ}\|\,\bm{x}_{j}-\bm{x}_{ n+1}\,\|^{2}}}\right)^{2}\right]}_{\mathcal{L}_{\text{sput}}(w_{KQ})}\]\[+\underbrace{\mathbb{E}_{f,\{\bm{x}_{i}\}}\left[\left(\sum_{i}\frac{ \epsilon_{i}e^{-w_{KQ}}\|\,\bm{x}_{i}-\bm{x}_{n+1}\,\|^{2}}{\sum_{j}e^{-w_{KQ}} \|\,\bm{x}_{j}-\bm{x}_{n+1}\,\|^{2}}-f(\bm{x}_{n+1})\right)^{2}\right]}_{ \mathcal{L}_{\text{\tiny{asist}}}(w_{KQ})}\]

We bound the first term in Appendix C and the second in Appendix D. A useful function that we bound in Lemma G.4 and Corrolary G.5 in Appendix G is

\[g_{p}(r)=\sum_{i=1}^{n}\|\,\bm{x}_{i}-\bm{x}\,\|^{p}e^{-r\|\,\bm{x}_{i}^{\top}- \bm{x}^{2}\,\|}.\]

We will use this function, particularly for \(p=0\) and \(1\).

## Appendix C The Signal Term

The purpose of this section of the Appendix is to obtain upper and lower bounds on \(\mathcal{L}_{\text{signal}}(w_{KQ})\). Because we work with two different distributions over functions, and because the bounds depend on the distributions, we will make the distribution explicit in the argument to the function

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}))=\mathbb{E}_{f,\{\bm{x}\}} \left(f(\bm{x}_{i})-\sum_{i}\frac{f(\bm{x}_{i})e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x} _{n+1}\,\|^{2}}}{\sum_{j}e^{-w_{KQ}\|\,\bm{x}_{j}-\bm{x}_{n+1}\,\|^{2}}}\right) ^{2}\]

As a reminder, we consider the following two distributions over functions. Please see section B.1 to see why we have set the covariance of \(\mathbf{w}\) to be identity.

**Definition C.1** (Affine and 2-ReLU Function Classes).: _The function classes \(\mathcal{F}_{L}^{\text{aff}}\) and \(\mathcal{F}_{L}^{+}\) are respectively defined as:_

\[\mathcal{F}_{L}^{\text{aff}} :=\{f:f(\bm{x})=l\mathbf{w}^{\top}\,\bm{x}+b,\ \mathbf{w}\in\mathbb{S}^{d-1}\},\] \[\mathcal{F}_{L}^{+} :=\{f:f(\bm{x})=l_{1}\,\text{ReLU}(\mathbf{w}^{\top}\,\bm{x})+l_{ 2}\,\text{ReLU}(-\mathbf{w}^{\top}\,\bm{x})+b,\ \mathbf{w}\in\mathbb{S}^{d-1}\}.\]

\(D(\mathcal{F}_{L}^{\text{aff}}),D(\mathcal{F}_{L}^{+})\) _are induced by taking \(\mathbf{w}\sim\mathcal{U}^{d}\), \(b,l,l_{1},l_{2}\sim\text{Unif}[-L,L]\)._

First we have the following trivial bound on \(\mathcal{L}_{\text{signal}}(w_{KQ})\).

**Lemma C.2**.: _For all \(w_{KQ}\) we have \(\mathcal{L}_{\text{signal}}(w_{KQ})\leq 4L^{2}\)._

Proof.: We have \(\mathcal{L}_{\text{signal}}(w_{KQ})\leq\mathbb{E}\left[\left(\sum\frac{f(\bm {x}_{i})-f(\bm{x}_{n+1})\gamma_{i}}{\sum_{j}\gamma_{i}}\right)^{2}\right]\) for some positive \(\{\gamma_{i}\}\). By Lipschitz-ness, \(f(\bm{x}_{i})-f(\bm{x}_{n+1})\leq L\|\,\bm{x}_{i}-\bm{x}_{n+1}\,\|\leq 2L\). 

### Affine functions

Here we consider the affine function class \(\mathcal{F}_{L}^{\text{aff}}\). First, we note that this class satisfies Assumption B.4.

**Lemma C.3**.: _The affine class \(\mathcal{F}_{L}^{\text{aff}}\) in Definition 3.2 satisfies Assumption B.4._

Proof.:
1. We have \(|f(\bm{x})-f(\bm{y})|=|\mathbf{w}^{\top}(\bm{x}-\bm{y})|\leq\|\mathbf{w}|\|\, \bm{x}-\bm{y}\,\|\) by Cauchy-Schwarz.
2. Because \(b\) is independent of \(w\), we have \[\mathbb{E}_{f}\left[f(\bm{x})f(\bm{y})\right]=\mathbb{E}_{\mathbf{w}}\left[l^ {2}\,\bm{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}+b^{2}\right]=\mathbb{E }\,l^{2}\frac{\mathbb{E}_{\mathbf{w}}\,\|\mathbf{w}\|^{2}}{d}\,\bm{x}^{\top}\, \bm{y}+\frac{L^{2}}{3}.\]
3. \(\mathbf{w}\) is isotropic, so \(\phi(\mathbf{w})\) is also supported by the distribution on \(\mathbf{w}\).

**Lemma C.4**.: _For affine functions, the signal term is upper bounded as_

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{off}}))\leq\begin{cases} L^{2}\,\mathcal{O}\left(\frac{1}{w_{KQ}^{2}}+\frac{w_{KQ}^{\frac{d}{2}-1}}{n}+ \frac{1}{n}\right)&w_{KQ}\geq\frac{d+\sqrt{d}}{2}\\ 4L^{2}&w_{KQ}<\frac{d+\sqrt{d}}{2}\end{cases}\]

Proof.: In the interest of readability, we will denote \(\bm{x}_{n+1}\) as \(\bm{x}\). Consider \(\tilde{\bm{x}}\) such that \(\tilde{\bm{x}}=\sum_{i}\bm{x}_{i}\,\frac{e^{-2w_{KQ}\,\bm{x}_{i}^{\top}\,\bm{x }}}{\sum_{j}e^{-2w_{KQ}\,\bm{x}_{i}^{\top}\,\bm{x}}}\). Then our loss is given by \(\mathbb{E}\left[l^{2}\mathbf{w}^{\top}(\bm{x}-\tilde{\bm{x}})\right]^{2}\). First, since \(\mathbf{w}\) is independent of \(\bm{x},\{\bm{x}_{i}\}\), we have \(\mathbb{E}\,l^{2}\left(\mathbf{w}^{\top}(\bm{x}-\tilde{\bm{x}})\right)^{2}= \mathbb{E}\,l^{2}\mathbf{w}\mathbf{w}^{\top}(\bm{x}-\tilde{\bm{x}})(\bm{x}- \tilde{\bm{x}})^{\top}\), Now \(\mathbf{w}\) has a uniformly randomly chosen direction, so its covariance is a multiple of the identity. We have \(\mathbb{E}\,\text{Tr}(\mathbf{w}\mathbf{w}\mathbf{w}^{\top})=\mathbb{E}\,\| \mathbf{w}\|^{2}=\frac{L^{2}}{3}\), so \(\mathbb{E}\,l^{2}\mathbf{w}\mathbf{w}^{\top}=\frac{L^{2}}{3d}\mathbf{I}_{d}\). Continuing, \(\mathbb{E}\left(\mathbf{w}^{\top}(\bm{x}-\tilde{\bm{x}})\right)^{2}=\frac{L^ {2}}{3d}\,\mathbb{E}\,\|\,\bm{x}-\tilde{\bm{x}}\|^{2}\). Take any \(\bm{x}^{\prime}\perp\bm{x}\), we have

\[\mathbb{E}\,\tilde{\bm{x}}^{\top}\,\bm{x}^{\prime} =\mathbb{E}\sum_{i}\bm{x}_{i}^{\top}\,\bm{x}^{\prime}\,\frac{e^{ -2w_{KQ}\,\bm{x}_{i}^{\top}\,\bm{x}}}{\sum_{j}e^{-2w_{KQ}\,\bm{x}_{i}^{\top}\, \bm{x}}}\] \[=\mathbb{E}\sum_{i}\mathbb{E}[\bm{x}_{i}^{\top}\,\bm{x}^{\prime} \,|\,\bm{x}_{i}^{\top}]\frac{e^{-2w_{KQ}\,\bm{x}_{i}^{\top}\,\bm{x}}}{\sum_{j }e^{-2w_{KQ}\,\bm{x}_{i}^{\top}\,\bm{x}}}=0\quad\text{ iterated expectation and symmetry}\] Decomposing \(\tilde{\bm{x}}\) into an orthogonal and a parallel component, we have \(\mathbb{E}\,\|\,\bm{x}-\tilde{\bm{x}}\|^{2}=\mathbb{E}\,\|\,\bm{x}-\bm{x}\, \bm{x}^{\top}\,\tilde{\bm{x}}-\bm{x}^{\prime}\,{\bm{x}^{\prime}}^{\top}\, \tilde{\bm{x}}\|^{2}\) for some \(\bm{x}^{\prime}\perp\bm{x}\) with \(\|\,\bm{x}^{\prime}\,\|=1\). But

\[\mathbb{E}\,\|\,\bm{x}-\bm{x}\,\bm{x}^{\top}\,\tilde{\bm{x}}-\bm{ x}^{\prime}\,{\bm{x}^{\prime}}^{\top}\,\tilde{\bm{x}}\|^{2}\] \[=\mathbb{E}\,\|\,\bm{x}(1-\bm{x}^{\top}\,\tilde{\bm{x}})\|^{2}+ \mathbb{E}\,\|\,\bm{x}^{\prime}\,{\bm{x}^{\prime}}^{\top}\,\tilde{\bm{x}}\|^{ 2}-2\,\mathbb{E}\,\bm{x}(1-\bm{x}^{\top}\,\tilde{\bm{x}})\tilde{\bm{x}}^{\top} \,\bm{x}^{\prime}\,{\bm{x}^{\prime}}^{\top}\] \[=\mathbb{E}\,\|\,\bm{x}(1-\bm{x}^{\top}\,\tilde{\bm{x}})\|^{2}+ \mathbb{E}\,\|\,\bm{x}^{\prime}\,{\bm{x}^{\prime}}^{\top}\,\tilde{\bm{x}}\|^{ 2}\qquad\quad\because\bm{x}^{\top}\,\bm{x}^{\prime}=0\implies 2\,\mathbb{E}\,\bm{x}(1-\bm{x}^{\top}\, \tilde{\bm{x}})\tilde{\bm{x}}^{\top}\,\bm{x}^{\prime}\,{\bm{x}^{\prime}}^{\top }=0\] (10)

**Case 1:**\(w_{KQ}\geq\frac{d+\sqrt{d}}{2}\).

Consider first the term \(\mathbb{E}\,\|\,\bm{x}(1-\bm{x}^{\top}\,\tilde{\bm{x}})\|^{2}=\mathbb{E}(1-\bm{ x}^{\top}\,\tilde{\bm{x}})^{2}\). Here we have with probability \(1-\frac{1}{n}\)

\[1-\bm{x}^{\top}\,\tilde{\bm{x}}=\frac{\sum(1-\bm{x}^{\top}\,\bm{ x}_{i})e^{-w_{KQ}\,\|\,\bm{x}-\bm{x}_{i}\,\|^{2}}}{\sum e^{-w_{KQ}\,\|\,\bm{x}-\bm{x}_{i}\,\|^{2} }}=\frac{g_{2}(w_{KQ})}{2g_{0}(w_{KQ})}\] \[\leq\frac{\overline{C_{b}}n\left(\frac{1}{w_{KQ}}\right)^{\frac{ d}{2}+1}}{2\underline{C_{b}}n\left(\frac{1}{w_{KQ}}\right)^{\frac{d}{2}}}\leq\frac{ \overline{C_{b}}}{\underline{C_{b}}}\frac{1}{w_{KQ}}\] Corollary G.5 (11)

The other term \(\mathbb{E}\,\|\,\bm{x}^{\prime}\,{\bm{x}^{\prime}}^{\top}\,\tilde{\bm{x}}\|^{2}= \mathbb{E}(\bm{x}^{\prime\top}\,\tilde{\bm{x}})^{2}\) is the component of the bias in the direction orthogonal to \(\bm{x}\).

\[(\bm{x}^{\prime\top}\,\tilde{\bm{x}})^{2}=\left(\frac{\sum_{i}\bm{x }^{\prime\top}\,\bm{x}_{i}\,e^{-w_{KQ}\,\|\,\bm{x}_{i}-\bm{x}\,\|^{2}}}{\sum_{i} e^{-w_{KQ}\,\|\,\bm{x}_{i}-\bm{x}\,\|^{2}}}\right)^{2}\] \[\leq\left(\frac{\sum_{i}\bm{x}^{\prime\top}\,\bm{x}_{i}\,e^{-w_{KQ} \,\|\,\bm{x}_{i}-\bm{x}\,\|^{2}}}{\sum_{i}e^{-w_{KQ}\,\|\,\bm{x}_{i}-\bm{x}\,\|^{2} }}\right)^{2}\] \[\leq\left(\frac{\sum_{i}\bm{x}^{\prime\top}\,\bm{x}_{i}\,e^{-w_{KQ} \,\|\,\bm{x}_{i}-\bm{x}\,\|^{2}}}{\sum_{i}e^{-w_{KQ}\,\|\,\bm{x}_{i}-\bm{x}\,\|^{2} }}\right)^{2}\] \[\leq\frac{\sum_{i}\left(1-(\bm{x}^{\top}\,\bm{x}_{i})^{2}\right)e^{ -2w_{KQ}\,\|\,\bm{x}_{i}-\bm{x}\,\|^{2}}}{\left(\sum_{i}e^{-w_{KQ}\,\|\,\bm{x}_{i}- \bm{x}\,\|^{2}}\right)^{2}}\] Popoviciu's Variance inequality \[\leq\frac{\sum_{i}2\left(1-\bm{x}^{\top}\,\bm{x}_{i}\right)e^{-2w_{KQ}\,\|\,\bm{x} _{i}-\bm{x}\,\|^{2}}}{\left(\sum_{i}e^{-w_{KQ}\,\|\,\bm{x}_{i}-\bm{x}\,\|^{2} }\right)^{2}}\]\[\leq\frac{\sum_{i}\|\,\bm{x}_{i}-\bm{x}\,\|^{2}e^{-2w_{KQ}\|\,\bm{x}_{i}-\bm{x}\,\| ^{2}}}{\left(\sum_{i}e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x}\,\|^{2}}\right)^{2}}=\frac{g _{2}(2w_{KQ})}{g_{0}^{2}(w_{KQ})}\]

With probability \(1-\frac{1}{n}\), when \(w_{KQ}\geq d+\sqrt{d}\) we have

\[\frac{g_{2}(2w_{KQ})}{g_{0}(w_{KQ})^{2}}\leq\frac{\overline{c_{g}}n\left(\frac{ 1}{2w_{KQ}}\right)^{\frac{d}{2}+1}}{\left(\frac{c_{g}}{n}\left(\frac{1}{w_{KQ} }\right)^{\frac{d}{2}}\right)^{2}}\leq\frac{\overline{c_{g}}w_{KQ}^{\frac{d}{ 2}-1}}{\underline{c_{g}}^{2}2^{\frac{d}{2}+1}n}\] (12)

Putting together Equations 11 and 12, we have with probability \(1-\frac{1}{n}\),

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}))\leq\mathcal{O}\left( \frac{L^{2}}{3d}\left(\frac{1}{w_{KQ}}+\frac{w_{KQ}^{\frac{d}{2}-1}}{n}\right) \right).\]

The signal bias is upper bounded by \(4L^{2}\) always (Lemma C.2). The overall upper-bound on the expectation is

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}))\leq\mathcal{O}\left( \frac{L^{2}}{3d}\left(\frac{1}{w_{KQ}}+\frac{w_{KQ}^{\frac{d}{2}-1}}{n}+4 \right)\right).\]

**Case 2:**\(w_{KQ}<\frac{d+\sqrt{d}}{2}\). We always have \(\mathcal{L}(w_{KQ})\leq 4L^{2}\) from Lemma C.2. 

**Lemma C.5**.: _For affine functions, the signal term is lower bounded as_

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{aff}}))\geq\begin{cases} \Omega\left(\frac{L^{2}}{w_{KQ}^{k}}\right)&w_{KQ}>\frac{d+\sqrt{d}}{2}\\ \Omega\left(1\right)&w_{KQ}<\frac{d+\sqrt{d}}{2}\end{cases}.\]

Proof.: Similar to Equation (10), for \(\tilde{\bm{x}}=\sum_{i}\bm{x}_{i}\,\frac{e^{-2w_{KQ}\bm{x}_{i}^{\top}\bm{x}}} {\sum_{j}e^{-2w_{KQ}\bm{x}_{i}^{\top}\bm{x}}}\), we have

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{aff}}))\geq\frac{ L^{2}}{3d}\,\mathbb{E}\,\|\,\bm{x}(1-\bm{x}^{\top}\tilde{\bm{x}})\|^{2}= \frac{L^{2}}{3d}\,\mathbb{E}(1-\bm{x}^{\top}\tilde{\bm{x}})^{2}\]

Now consider the term \(1-\bm{x}^{\top}\tilde{\bm{x}}\). We have

\[\frac{\sum(1-\bm{x}^{\top}\bm{x}_{i})e^{-w_{KQ}\|\,\bm{x}-\bm{x}_{i}\,\|^{2}}} {\sum e^{-w_{KQ}\|\,\bm{x}-\bm{x}_{i}\,\|^{2}}}\geq\frac{g_{2}(w_{KQ})}{2g_{0} (w_{KQ})}\]

**Case 1:**\(w_{KQ}\geq\frac{d+\sqrt{d}}{2}\). Here we have from Corollary G.5, with probability \(1-1/n\)

\[\frac{\sum(1-\bm{x}^{\top}\bm{x}_{i})e^{-w_{KQ}\|\,\bm{x}-\bm{x}_{i}\,\|^{2}} }{\sum e^{-w_{KQ}\|\,\bm{x}-\bm{x}_{i}\,\|^{2}}}\geq\frac{C_{b}n\left(\frac{1} {w_{KQ}}\right)^{\frac{d}{2}+1}}{2\overline{C_{b}}n\left(\frac{1}{w_{KQ}} \right)^{\frac{d}{2}}}\geq\frac{C_{b}}{2\overline{C_{b}}}\frac{1}{w_{KQ}}.\]

With probability \(1/n\leq\frac{1}{2}\) the lowest we can have is \(\mathcal{L}_{\text{signal}}(w_{KQ})=0\), so overall we have

\[\mathcal{L}_{\text{signal}}(w_{KQ})\geq\frac{L^{2}}{24d}\left(\frac{C_{b}}{ \overline{C_{b}}}\frac{1}{w_{KQ}}\right)^{2}\]

**Case 2:**\(\frac{d+\sqrt{d}}{4}\leq w_{KQ}\leq\frac{d+\sqrt{d}}{2}\). From Corollary G.5, with probability \(1-\frac{1}{n}\)

\[\frac{\sum(1-\bm{x}^{\top}\bm{x}_{i})e^{-w_{KQ}\|\,\bm{x}-\bm{x}_{i}\,\|^{2}}} {\sum e^{-w_{KQ}\|\,\bm{x}-\bm{x}_{i}\,\|^{2}}}\geq\frac{\underline{C_{b}}n \left(\frac{1}{w_{KQ}}\right)^{\frac{d}{2}+1}}{2\overline{C_{b}}ne^{-2w_{KQ}} }\geq\frac{\underline{C_{b}}}{2\overline{C_{b}}}\frac{e^{2w_{KQ}}}{w_{KQ}^{ \frac{d}{2}+1}}.\]With probability \(1/n\leq\frac{1}{2}\) the lowest we can have is \(\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{aff}}))=0\), so overall we have

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{aff}}))\geq\frac{L^ {2}}{24d}\left(\frac{C_{b}}{\overline{C}_{b}}\frac{e^{2w_{KQ}}}{w_{KQ}^{\frac{ \delta}{\delta}+1}}\right)^{2}\]

**Case 3:**\(\frac{d+\sqrt{d}}{4}>w_{KQ}\). From Corollary G.5, with probability \(1-\frac{1}{n}\)

\[\frac{\sum(1-\boldsymbol{x}^{\top}\,\boldsymbol{x}_{i})e^{-w_{KQ} \|\,\boldsymbol{x}-\boldsymbol{x}_{i}\,\|^{2}}}{\sum e^{-w_{KQ}\|\,\boldsymbol {x}-\boldsymbol{x}_{i}\,\|^{2}}}\geq\frac{C_{b}ne^{-4w_{KQ}}}{2\overline{C_{ b}}ne^{-2w_{KQ}}}\geq\frac{C_{b}}{2\overline{C_{b}}}e^{-2w_{KQ}}.\]

With probability \(1/n\leq\frac{1}{2}\) the lowest we can have is \(\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{aff}}))=0\), so overall we have

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{aff}}))\geq\frac{ L^{2}}{24d}\left(\frac{C_{b}}{\overline{C}_{b}}e^{-2w_{KQ}}\right)^{2}\]

**Corollary C.6**.: _Combining the above, we have_

\[L^{2}\,\mathcal{O}\left(\frac{1}{\left(w_{KQ}+1\right)^{2}}\right)\leq \mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{aff}}))\leq L^{2} \,\mathcal{O}\left(\frac{1}{w_{KQ}^{2}}+\frac{w_{KQ}^{\frac{\delta}{\delta}-1} }{n}+\frac{1}{n}\right).\] (13)

We can now perturb these bounds in the case ofthe ReLU-based function class \(\mathcal{F}_{L}^{+}\).

### ReLU-based functions

Consider the function class

\[\mathcal{F}_{L}^{+}=\{l_{1}\text{ReLU}(\mathbf{w}^{\top}\,\boldsymbol{x})+l_ {2}\text{ReLU}(-\mathbf{w}^{\top}\,\boldsymbol{x})+b:\mathbf{w}\in\mathbb{S} ^{d-1},b,l_{1},l_{2}\in[-L,L]\},\]

where \(\text{ReLU}(z):=(z)_{+}:=\max(z,0)\). Consider a distributions on \(\mathcal{F}_{L}^{+}\), namely \(D(\mathcal{F}_{L}^{+})\). Let \(D(\mathcal{F}_{L}^{+})\) be induced by \(\mathbf{w}\sim\mathcal{U}^{d},b,l_{1},l_{2}\sim\text{Unif}[-L,L]\). That is, a vector \(\mathbf{w}\) is drawn uniformly on the unit hypersphere. Then two norms are selected, \(l_{1},l_{2}\), and the overall function is given by

\[f_{\mathbf{w},l_{1},l_{2}}(\boldsymbol{x})=l_{1}\text{ReLU}(\mathbf{w}^{\top }\,\boldsymbol{x})+l_{2}\text{ReLU}(-\mathbf{w}^{\top}\,\boldsymbol{x})+b,\]

so that it follows one affine rule in one halfspace, and another affine rule in the opposite halfspace. Please see section B.1 to see why we have set the covariance of \(\mathbf{w}\) to be identity.

**Lemma C.7**.: _The class \(\mathcal{F}_{L}^{+}\) and distribution \(D(\mathcal{F}_{L}^{+})\) defined above satisfy Assumption B.4._

Proof.:
1. Each function is defined as being piece-wise \(L\)-Lipschitz, and it is continuous, so it is also \(L-\)Lipschitz overall.
2. With probability \(1-2\frac{\arccos(\boldsymbol{x}^{\top}\,\boldsymbol{y})}{\pi}\) the points \(\boldsymbol{x}\) and \(\boldsymbol{y}\) are such that \((\mathbf{w}^{\top}\,\boldsymbol{x})(\mathbf{w}^{\top}\,\boldsymbol{y})<0\) (that is, they are on opposite sides of the hyperplane defining the two pieces of the ReLU). Because the bias \(b\) is independent of the other parameters, we have as in the proof of Lemma C.3 \[\mathbb{E}_{f}\left[f(\boldsymbol{x})f(\boldsymbol{y})\right]=\frac {L^{2}}{3}+\mathbb{E}_{\mathbf{w}}\left[l_{1}^{2}\,\boldsymbol{x}^{\top}\, \mathbf{w}\mathbf{w}^{\top}\,\boldsymbol{y}\right](\mathbf{w}^{\top}\, \boldsymbol{x})(\mathbf{w}^{\top}\,\boldsymbol{y})\geq 0\right]\mathbb{P}[( \mathbf{w}^{\top}\,\boldsymbol{x})(\mathbf{w}^{\top}\,\boldsymbol{y })\geq 0]\] \[\qquad+\mathbb{E}_{\mathbf{w}}\left[l_{1}l_{2}\,\boldsymbol{x}^{ \top}\,\mathbf{w}\mathbf{w}^{\top}\,\boldsymbol{y}\right](\mathbf{w}^{\top}\, \boldsymbol{x})(\mathbf{w}^{\top}\,\boldsymbol{y})<0]\,\mathbb{P}[( \mathbf{w}^{\top}\,\boldsymbol{x})(\mathbf{w}^{\top}\,\boldsymbol{y})<0]\] \[=\frac{L^{2}}{3}+\mathbb{E}_{\mathbf{w}}\left[l_{1}^{2}\, \boldsymbol{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\boldsymbol{y}\right] \boldsymbol{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\boldsymbol{y}>0]\left(2 \frac{\arccos\bigl{(}\boldsymbol{x}^{\top}\,\boldsymbol{y}\bigr{)}}{\pi} \right)\ \ \because l_{1}\perp l_{2}\] Let \(\overline{\boldsymbol{x}}=\frac{\boldsymbol{x}}{\parallel\boldsymbol{x}\parallel}\) for any vector \(\boldsymbol{x}\). Consider a re-parameterization of the pair \((\boldsymbol{x},\boldsymbol{y})\) as \(\xi_{\theta}(\boldsymbol{x},\boldsymbol{y})\to(\overline{\boldsymbol{x}+ \boldsymbol{y}},\overline{\boldsymbol{x}-\boldsymbol{y}})\). Because \(\boldsymbol{x}\) and \(\boldsymbol{y}\) are on the unit sphere, this is a bijection as \[\xi_{\theta}^{-1}(\boldsymbol{x},\boldsymbol{y})=\left(\frac{1+\theta}{2}\, \boldsymbol{x}+\frac{1-\theta}{2}\,\boldsymbol{y},\frac{1+\theta}{2}\, \boldsymbol{x}-\frac{1-\theta}{2}\,\boldsymbol{y}\right).\]That is, for any \(\bm{x},\bm{y}\), \(\bm{\xi}_{\bm{x}^{\top}}^{-1}\bm{y}(\bm{\xi}_{\bm{x}^{\top}}\bm{y}(\bm{x},\bm{y}) )=(\bm{x},\bm{y})\). The push-forward of \(\xi\) is also uniform, that is for \(\bm{x},\bm{y}\) satisfying \(\bm{x}^{\top}\bm{y}=\theta\), \(\xi_{\theta}(\bm{x},\bm{y})\) is distributed as \(\mathcal{U}^{d}\times\mathcal{U}^{d-1}\). For any \(\bm{x},\bm{y}\), let \(\xi_{\theta}^{-1}(\bm{x},\bm{y})=(\bm{x}_{\theta},\bm{y}_{\theta})\). Then we have \(\mathbb{E}_{f}\left[f(\bm{x}_{\theta})f(\bm{y}_{\theta})\right]\) is a decreasing function of \(\theta\). Finally, for \(\theta\leq\theta^{\prime}\), \(L^{2}\,\bm{x}_{\theta}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}_{\theta}>L ^{2}\,\bm{x}_{\theta^{\prime}}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}_{ \theta^{\prime}}\) so \(\bm{x}_{\theta}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}_{\theta}<0\implies \bm{x}_{\theta^{\prime}}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}_{\theta^ {\prime}}<0\). The product of two positive increasing functions is itself non-increasing. Since we have both \(\mathbb{E}_{\mathbf{w}}\left[L^{2}\,\bm{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top} \,\bm{y}\right|\bm{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}>0\right]\) and \(\frac{2\arccos(\bm{x}^{\top}\,\bm{y})}{\pi}\) are increasing functions of \(\bm{x}^{\top}\,\bm{y}\), we also have \[\mathbb{E}_{\mathbf{w}}\left[L^{2}\,\bm{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top} \,\bm{y}\right|\bm{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}>0\right] \left(\frac{2\arccos(\bm{x}^{\top}\,\bm{y})}{\pi}\right)\] is an increasing function of \(\bm{x}^{\top}\,\bm{y}\) since \(\mathbb{E}_{\mathbf{w}}\left[L^{2}\,\bm{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top} \,\bm{y}\right|\bm{x}^{\top}\,\mathbf{w}\mathbf{w}^{\top}\,\bm{y}>0\right]\geq 0\) and \(\left(\frac{2\arccos(\bm{x}^{\top}\,\bm{y})}{\pi}\right)\geq 0\).
3. \(\mathbf{w}\) is distributed uniformly on the hypersphere, so \(\phi(\mathbf{w})\) is also also distributed uniformly on the hypersphere for any isometry \(\phi\) that preserves the origin.

**Lemma C.8**.: _The signal term is upper bounded as_

Proof.: We have

\[\mathcal{L}_{\text{signal}}(w_{KQ};D) =\mathbb{E}_{f,\{\bm{x}_{i}\}}\left(\frac{\sum_{i}\left(f(\bm{x} _{i})-f(\bm{x}_{n})\right)e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x}_{n}\,\|^{2}}}{\sum_ {i}e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x}_{n}\,\|^{2}}}\right)^{2}\] \[\leq\mathbb{E}_{f,\{\bm{x}_{i}\}}\left(\frac{\sum_{i}L\|\,\bm{x} _{i}-\bm{x}_{n}\,\|e^{-w_{KQ}\|\,\bm{x}_{i}-\bm{x}_{n}\,\|^{2}}}{\sum_{i}e^{-w _{KQ}\|\,\bm{x}_{i}-\bm{x}_{n}\,\|^{2}}}\right)^{2}\] \[\leq\left(L\frac{g_{1}(w_{KQ})}{g_{0}(w_{KQ})}\right)^{2}\]

With probability \(1-\frac{1}{n}\), when \(w_{KQ}\geq\frac{d+\sqrt{d}}{2}\) we have

\[\frac{g_{1}(w_{KQ})}{g_{0}(w_{KQ})}\leq\frac{\overline{C_{b}}n\left(\frac{1}{w _{KQ}}\right)^{\frac{d+1}{2}}}{\underline{C_{b}}n\left(\frac{1}{w_{KQ}}\right) ^{\frac{d}{2}}}\leq\frac{\overline{C_{b}}}{\underline{C_{b}}}\left(\frac{1}{w _{KQ}}\right)^{\frac{1}{2}}\]

We always have \(\mathcal{L}_{\text{signal}}(w_{KQ})\leq 4L^{2}\) from Lemma C.2. So the overall upper bound is

\[\mathcal{L}_{\text{signal}}(w_{KQ};D)\leq L^{2}\left(\frac{1}{w_{KQ}}+\frac{4 }{n}\right)\]

For \(w_{KQ}\geq\frac{d+\sqrt{d}}{2}\), as before, we always have \(\mathcal{L}_{\text{signal}}(w_{KQ};D)\leq 4L^{2}\). 

**Lemma C.9**.: _The signal term is lower bounded as_

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{+}))\geq\mathcal{L}_{ \text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\text{off}}))/2\]

Proof.: Again for readability we will write \(\bm{x}_{n+1}\) as \(\bm{x}\). For any \(f\in\mathcal{F}_{L}^{+}\) let \(f_{\bm{x},\text{aff}}\) denote the corresponding affine function that is equal to \(f\) in the halfspace containing \(\bm{x}\), that is if \(f(\bm{x}^{\prime})=l_{1}\text{ReLU}(\mathbf{w}^{\top}\,\bm{x}^{\prime})+l_{2} \text{ReLU}(-\mathbf{w}^{\top}\,\bm{x}^{\prime})+b\), and WLOG \(\mathbf{w}^{\top}\,\bm{x}^{\prime}>0\), then \(f_{\bm{x},\text{aff}}(\bm{x}^{\prime})=l_{1}\mathbf{w}^{\top}\,\bm{x}^{\prime}+b\). Note that \(f_{\bm{x},\text{aff}}\) comes from a \(\mathbf{w}\) selected from the unit sphere and \(b,l\in[-L,L]\) exactly as \(D(\mathcal{F}_{L})\), so it is actually statistically indistinguishable from a sample from \(D(\mathcal{F}_{L}^{\text{aff}})\), the distribution over affine functions in Definition 3.2 (and the object of Lemma C.5). The error of the nonlinear estimator can be written as

\[\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left[\left(\sum_{i}f(\bm{x}_{i})\gamma_{i} -f(\bm{x}_{n})\right)^{2}\right]\]

where \(\gamma_{i}=\frac{e^{-w_{KQ}\|\bm{x}-\bm{x}_{i}\|_{\bm{x}}^{2}-1}}{\sum_{j}e^{- w_{KQ}\|\bm{x}-\bm{x}_{j}\|_{\bm{x}}^{2}}}\) Let us compare the two errors due to the two functions. Let \(A=\{i:(\bm{x}_{i}^{\top}\mathbf{w})(\bm{x}^{\top}\mathbf{w})<0\}\) denote the set of points on the opposite side to \(\bm{x}\) of the hyperplane defining the function.

\[\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}^{\top}))\] \[=\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left[\left(\sum_{i}f(\bm{x }_{i})\gamma_{i}-f(\bm{x})\right)^{2}\right]\] \[=\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left[\left(\sum f_{\bm{x},\text{aff}}(\bm{x}_{i})\gamma_{i}+\sum_{i\in A}\left(f(\bm{x}_{i})-f_{\bm{x},\text{aff}}(\bm{x}_{i})\right)\gamma_{i}-f(\bm{x})\right)^{2}\right]\] \[=\mathbb{E}_{\bm{x},\{\bm{x}_{i}\}}\left[\left(\sum_{i}f_{\bm{x},\text{aff}}(\bm{x}_{i})\gamma_{i}-f_{\bm{x},\text{aff}}(\bm{x})\right)^{2} \right]+\mathbb{E}_{f}\left[\left(\sum_{i\in A}f_{\bm{x},\text{aff}}(\bm{x}_{i })\gamma_{i}\right)^{2}\right]\] \[\geq\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left[\left(\sum f_{\bm{x },\text{aff}}(\bm{x}_{i})\gamma_{i}-f_{\bm{x},\text{aff}}(\bm{x})\right)^{2} \right]+\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left(\sum_{i\in A}f_{\bm{x},\text{ aff}}(\bm{x}_{i})\gamma_{i}\right)^{2}\] \[\qquad\qquad\qquad-2\sqrt{\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}} \left[\left(\sum f_{\bm{x},\text{aff}}(\bm{x}_{i})\gamma_{i}-f_{\bm{x},\text{ aff}}(\bm{x})\right)^{2}\right]\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left[ \left(\sum_{i\in A}f_{\bm{x},\text{aff}}(\bm{x}_{i})\gamma_{i}\right)^{2} \right]}\] \[\qquad\qquad\qquad+\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left(\sum _{i\in A}f(\bm{x}_{i})\gamma_{i}\right)\right)^{2}\]

Here the third equality holds because \(f(\bm{x}_{i})\) is independent of \(f_{\bm{x},\text{aff}}(\bm{x}_{j})\) if \(i\in A,j\not\in A\).

Let \(q=\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left(\sum_{i\in A}f(\bm{x}_{i})\gamma_{ i}\right)^{2}=\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left(\sum_{i\in A}f_{\bm{x},\text{aff}}(\bm{x}_{i})\gamma_{i}\right)\bigr{)}^{2}\). Then from the above we have

\[\mathbb{E}_{f,\bm{x},\{\bm{x}_{i}\}}\left[\left(\sum_{i}f(\bm{x}_{i})\gamma_{i }-f(\bm{x})\right)^{2}\right]\geq(\mathcal{L}_{\text{signal}}(w_{KQ};D( \mathcal{F}_{L}))(w_{KQ})-q)^{2}+q^{2},\]

which has minimum at \(q=\mathcal{L}_{\text{signal}}(w_{KQ};D(\mathcal{F}_{L}))/2\), completing the proof. 

## Appendix D Bounds on Noise Variance

In this section we obtain upper and lower bounds on the variance of the estimator due to label noise. There are three relevant parameters: \(d\), the ambient dimension of the data; \(w_{KQ}\), the scaling induced by the attention layer; and \(n\), the number of tokens. Recall that the noise term is

\[\mathcal{L}_{\text{noise}}(w_{KQ})=\mathbb{E}_{f,\{\boldsymbol{x}_{i}\}}\left[ \left(\sum_{i}\frac{\epsilon_{i}e^{-w_{KQ}\|\,\boldsymbol{x}_{i}-\boldsymbol{x }_{n+1}\,\|^{2}}}{\sum_{j}e^{-w_{KQ}\|\,\boldsymbol{x}_{j}-\boldsymbol{x}_{n+1} \,\|^{2}}}\right)^{2}\right]\]

Because the \(\epsilon_{i}\) are independent, this can further be simplified as

\[\mathcal{L}_{\text{noise}}(w_{KQ})=\sigma^{2}\,\mathbb{E}_{\{ \boldsymbol{x}_{i}\}}\left[\sum_{i}\frac{e^{-2w_{KQ}\|\,\boldsymbol{x}_{i}- \boldsymbol{x}_{n+1}\,\|^{2}}}{\left(\sum_{j}e^{-w_{KQ}\|\,\boldsymbol{x}_{j}- \boldsymbol{x}_{n+1}\,\|^{2}}\right)^{2}}\right]\]

**Lemma D.1**.: _The noise term is bounded for \(d+\sqrt{d}\leq w_{KQ}\leq\left(\frac{n}{45\sqrt{d}\log n}\right)^{\frac{2}{d}}\) as_

\[\Omega\left(\frac{\sigma^{2}w_{KQ}^{\frac{2}{d}}}{n}\right)\leq\mathcal{L}_{ \text{noise}}(w_{KQ})\leq\mathcal{O}\left(\frac{\sigma^{2}\left(1+w_{KQ}^{ \frac{2}{d}}\right)}{n}\right).\]

Proof.: We have

\[\mathcal{L}_{\text{noise}}(w_{KQ})=\sigma^{2}\,\mathbb{E}\left[\sum_{i}\frac{e ^{-2w_{KQ}\|\,\boldsymbol{x}_{i}-\boldsymbol{x}_{n}\,\|^{2}}}{\left(\sum_{j}e ^{-w_{KQ}\|\,\boldsymbol{x}_{j}-\boldsymbol{x}_{n}\,\|^{2}}\right)^{2}}\right] =\sigma^{2}\,\mathbb{E}\left[\frac{g_{0}(2w_{KQ})}{g_{0}(w_{KQ})^{2}}\right].\]

Using Lemma G.5, we have with probability at least \(1-\frac{1}{n}\)

\[\frac{g_{0}(2w_{KQ})}{g_{0}(w_{KQ})^{2}}\leq\frac{\overline{c_{n}}n\left( \frac{1}{w_{2KQ}}\right)^{\frac{d}{2}}}{\left(\underline{c_{n}}n\left(\frac{ 1}{w_{KQ}}\right)^{\frac{d}{2}}\right)}\leq\frac{\overline{c_{n}}}{\underline {c_{n}}^{2}}\frac{w_{KQ}^{\frac{d}{2}}}{n}\]

and similarly

\[\frac{g_{0}(2w_{KQ})}{g_{0}(w_{KQ})^{2}}\geq\frac{\underline{c_{n}}n\left( \frac{1}{w_{2KQ}}\right)^{\frac{d}{2}}}{\left(\overline{c_{n}}n\left(\frac{ 1}{w_{KQ}}\right)^{\frac{d}{2}}\right)}\leq\frac{\underline{c_{n}}}{\underline {c_{n}}^{2}}\frac{w_{KQ}^{\frac{d}{2}}}{n}\]

Finally, in the worst case, we have \(0\leq\mathcal{L}_{\text{noise}}(w_{KQ})\leq 1\). 

Finally, we show that the noise term is monotonic in \(w_{KQ}\).

**Lemma D.2**.: \(\mathcal{L}_{\text{noise}}(w)>\mathcal{L}_{\text{noise}}(w^{\prime}) \iff w>w^{\prime}\)__

Proof.: Let \(a_{i}=e^{-w^{\prime}\|x_{i}-x_{n+1}\|^{2}},b_{i}=e^{-\left(w-w^{\prime}\right) \|x_{i}-x_{n+1}\|^{2}}\). The result follows from Lemma I.3 because \(\{a_{i}\}\) and \(\{b_{i}\}\) satisfy \(a_{i}>a_{j}\iff b_{i}>b_{j}\iff\|x_{i}-x_{n+1}\|<\|x_{j}-x_{n+1}\|\). 

## Appendix E Optimizing the Loss

For the nonlinear function class \(\mathcal{F}_{L}^{+}\), we have the following.

**Theorem E.1**.: _Suppose the functions seen in pretraining are drawn from \(D(\mathcal{F}_{L}^{+})\) as in Definition 3.2, the covariates are drawn as Assumption 3.3, \(n=\Omega\left(\frac{L\log n}{\sigma}\right)^{d}\) and \(n^{\frac{2}{d+2}}=\Omega(1)\), then the optimal \(\mathbf{M}\) satisfies_

\[\mathbf{M}=w_{KQ}\mathbf{I}_{d}\] (14)

_where \(w_{KQ}\) satisfies_

\[\Omega\left(\left(nL^{2}\right)^{\frac{1}{d+2}}\right)\leq w_{KQ}\leq\mathcal{ O}\left(\left(nL^{2}\right)^{\frac{2}{d+2}}\right).\] (15)Proof.: We consider three regions in which the optimal value could potentially lie and see that only the third region is viable.

**Case 1.**\(w_{KQ}\leq d+\sqrt{d}\): In this case, the signal term lower bounds the optimal loss by Lemma C.5 as \(\Omega(1)\).

**Case 2.**\(w_{KQ}>\Omega\left(\frac{n}{\log n}\right)^{\frac{2}{d}}\). In this case, the noise term lower bounds the optimal loss. From Lemma D.2 we know that the noise term is non-decreasing in \(w_{KQ}\) so in the range \(w_{KQ}>\Omega(\left(\frac{n}{\log n}\right)^{\frac{2}{d}}\) is lower bounded by \(\mathcal{L}_{\text{noise}}(w_{KQ})\) at \(w_{KQ}=\Omega(\left(\frac{n}{\log n}\right)^{\frac{2}{d}}\), which is \(\Omega\left(\frac{\sigma^{2}}{\log n}\right)\).

**Case 3.**\(d+\sqrt{d}\leq w_{KQ}\leq\Omega\left(\frac{n}{\log n}\right)^{\frac{2}{d}}\) By combining Lemmas C.8, C.9, and D.1, we obtain the following overall bound on the loss:

\[\underline{c}\left(\frac{L^{2}}{\left(w_{KQ}+1\right)^{2}}+\frac{\sigma^{2}w _{KQ}^{\frac{d}{d}}}{n}\right)\leq\mathcal{L}(w_{KQ})\leq\overline{c}\left( \frac{L^{2}}{w_{KQ}}+\sigma^{2}\frac{w_{KQ}^{\frac{d}{d}}}{n}+\frac{\sigma^{2 }+L^{2}}{n}\right)\]

for some constants \(\overline{c},\underline{c}\) that only depend on \(d\). In the range \(w_{KQ}\geq d+\sqrt{d}\), we have \(w_{KQ}>1\) and \(w_{KQ}\leq n\), so the upper bound can be relaxed as \(\mathcal{L}_{\text{noise}}(w_{KQ})\leq 2\overline{c}\left(\frac{L^{2}}{n}+ \frac{\sigma^{2}w_{KQ}^{\frac{d}{d}}}{n}\right)\), which is minimized at \(w_{KQ}=\left(\frac{nL^{2}}{\sigma^{2}d}\right)^{\frac{2}{d+2}}\). Here it is upper bounded by \(4\overline{c}\left(\frac{dL^{2}\sigma^{2}}{n}\right)^{\frac{2}{d+2}}\). We note first of all that for large enough \(n\) (as long as \(n=\Omega\left(\frac{\sigma\log n}{L}\right)^{d}\) and \(n^{\frac{2}{d+2}}=\Omega(1)\)) this is lower than the lower bounds we got in **Case 1** and **Case 2**, so this is indeed the region of global optimal solution. From Lemma C.9 we have \(\mathcal{L}_{\text{noise}}(w_{KQ})\geq\frac{L^{2}}{w_{KQ}^{2}}+\sigma^{2} \frac{w_{KQ}^{\frac{d}{d}}}{n}\geq\frac{L^{2}}{w_{KQ}^{2}}\) which gives

\[\underline{c}\frac{L^{2}}{w_{KQ}^{2}}\leq\mathcal{L}_{\text{noise} }(w_{KQ})\leq 4\overline{c}L^{2}\left(\frac{\sigma^{2}d}{nL^{2}}\right)^{ \frac{2}{d+2}}\] \[\implies\left(\frac{nL^{2}}{d\sigma^{2}}\right)^{\frac{1}{d+2}} \sqrt{\frac{c}{4\overline{c}}}\leq w_{KQ}\]

for the upper bound, we similarly also have \(\mathcal{L}_{\text{noise}}(w_{KQ})\geq\frac{L^{2}}{w_{KQ}^{2}}+\sigma^{2} \frac{w_{KQ}^{\frac{d}{d}}}{n}\geq\sigma^{2}\frac{w_{KQ}^{\frac{d}{d}}}{n}\) which gives

\[4\overline{c}\left(\frac{dL^{d}\sigma^{2}}{n}\right)^{\frac{2}{d+2}}\geq \sigma^{2}\frac{w_{KQ}^{\frac{d}{d}}}{n}\]

Figure 7: **Left: Rough upper and lower bounds for the bias term (shaded region), along with the noise variance (gray). Right: Overall upper and lower bound for the in-context loss. The horizontal dashed line establishes an upper bound for the optimal loss, while the vertical dashed lines establish lower and upper bounds for the parameter \(w_{KQ}\) that can attain the optimal loss.**

\[\implies w_{KQ}\leq\left(\frac{nL^{2}}{\sigma^{2}}\right)^{\frac{2}{d+2}}\left(4 \begin{matrix}\overline{\widetilde{c}}\\ \overline{c}\end{matrix}d^{\frac{2}{d+2}}\right)^{\frac{2}{d}}\]

Of course, for this to not be vacuous we need

\[\left(\frac{nL^{2}}{\sigma^{2}}\right)^{\frac{2}{d+2}}\left(4\begin{matrix} \overline{\widetilde{c}}\\ \overline{c}\end{matrix}d^{\frac{2}{d+2}}\right)^{\frac{2}{d}}\leq\left(\frac{ 1}{45\sqrt{d}}\frac{n}{\log n}\right)^{\frac{2}{d}}.\]

We will again hide constants that depend only on \(d\) and write this as

\[c_{1}\left(\frac{nL^{2}}{\sigma^{2}}\right)^{\frac{2}{d+2}}\leq c_{2}\left( \frac{n}{\log n}\right)^{\frac{2}{d}}\]

which is true as long as \(n>\left(\frac{L\log n}{\sigma}\right)^{d}\). 

For the affine function class \(\mathcal{F}_{L}^{\text{aff}}\), we have the following

**Theorem E.2**.: _If the functions seen in pretraining are drawn from \(D(\mathcal{F}_{L}^{\text{aff}})\) as in Definition 3.2, and the noise variance \(\sigma^{2}\) and Liphscitz constant \(L\) satisfies \(n\geq\left(\frac{L\log^{2}n}{\sigma}\right)^{d+2}\), and \(n^{\frac{2}{d}}\geq\Omega(1)\), and the covariates are drawn as Assumption 3.3, the optimal \(\mathbf{M}\) satisfies_

\[\mathbf{M}=w_{KQ}\mathbf{I}_{d}\] (16)

_where \(w_{KQ}\) satisfies_

\[\Omega\left((nL^{2})^{\frac{1}{d+4}}\right)\leq w_{KQ}\leq\mathcal{O}\left((nL ^{2})^{\frac{2(d+2)}{d(d+4)}}\right).\] (17)

Proof.: Again we work with three cases.

**Case 1.**\(w_{KQ}\leq d+\sqrt{d}\). Again in this case we have a lower bound to the signal term of \(\Omega(1)\).

**Case 2.**\(w_{KQ}\geq\Omega\left(\frac{n}{\log n}\right)^{\frac{2}{d}}\). Again we have a lower bound of \(\Omega\left(\frac{\sigma^{2}}{\log n}\right)\)

**Case 3.**\(d+\sqrt{d}\leq w_{KQ}\leq\Omega\left(\frac{n}{\log n}\right)^{\frac{2}{d}}\) Combining Lemmas C.4, C.5, D.1 is

\[\underline{c}\left(\frac{L^{2}}{(w_{KQ}+1)^{2}}+\sigma^{2}\frac{w_{KQ}^{ \frac{d}{d}}}{n}\right)\leq\mathcal{L}(w_{KQ})\leq\overline{c}\left(\frac{L^ {2}}{w_{KQ}^{2}}+\sigma^{2}\frac{w_{KQ}^{\frac{d}{d}}}{n}+L^{2}\frac{w_{KQ}^{ \frac{d}{d}-1}}{n}+\frac{L^{2}+\sigma^{2}}{n}\right)\]

We will minimize the upper bound. First suppose \(\frac{L^{2}}{\sigma^{2}}\geq w_{KQ}\) for the \(w_{KQ}\) that minimizes the upper bound. Then we have

\[\mathcal{L}(w_{KQ})\leq\overline{c}\left(\frac{L^{2}}{w_{KQ}^{2}}+\frac{ \sigma^{2}}{n}+2L^{2}\frac{w_{KQ}^{\frac{d}{d}-1}}{n}\right)\]

This upper bound is minimized at \(w_{KQ}=n^{\frac{2}{d+2}}\). However, this contradicts the constraint that \(w_{KQ}\leq\frac{L^{2}}{\sigma^{2}}\), when \(n^{\frac{2}{d+2}}\geq\frac{L^{2}}{\sigma^{2}}\), as we assume. So we have \(w_{KQ}\geq\frac{L^{2}}{\sigma^{2}}\) for the minimizer. This means the upper bound is no more than

\[\mathcal{L}(w_{KQ})\leq\overline{c}\left(\frac{L^{2}}{w_{KQ}^{2}}+\sigma^{2} \frac{2w_{KQ}^{\frac{d}{d}}}{n}+\frac{\sigma^{2}+L^{2}}{n}\right)\]

This upper bound is minimized at \(w_{KQ}=\left(\frac{nL^{2}}{\sigma^{2}d}\right)^{\frac{2}{d+4}}\) where it is upper bounded by

\[\mathcal{L}_{\text{noise}}(w_{KQ})\leq 4L^{2}\overline{c}\left(\frac{\sigma^{2}d}{nL ^{2}}\right)^{\frac{2}{d+4}}+\frac{L^{2}}{n}\leq 5L^{2}\overline{c}\left(\frac{ \sigma^{2}d}{nL^{2}}\right)^{\frac{2}{d+4}}.\]whenever \(n\geq\frac{L^{2}}{\sigma^{2}}\). We see that

\[\mathcal{L}(w_{KQ})\geq \underline{c}\left(\frac{L^{2}}{w_{KQ}^{2}}+\sigma^{2}\frac{w_{KQ}^ {\frac{d}{4}}}{n}\right)\geq\underline{c}\frac{L^{2}}{w_{KQ}^{2}}\] \[\implies \underline{c}\frac{L^{2}}{w_{KQ}^{2}}\leq 5L^{2}\overline{c} \left(\frac{\sigma^{2}d}{nL^{2}}\right)^{\frac{2}{4+4}}\] \[\implies \left(\frac{nL^{2}}{\sigma^{2}}\right)^{\frac{1}{d+4}}\sqrt{ \frac{\underline{c}}{5\overline{c}}}\left(\frac{1}{d}\right)^{\frac{1}{d+4}} \leq w_{KQ}\]

for the upper bound, we similarly also have

\[\mathcal{L}(w_{KQ})\geq \underline{c}\left(\frac{L^{2}}{w_{KQ}^{2}}+\sigma^{2}\frac{w_{KQ }^{\frac{d}{4}}}{n}\right)\geq\underline{c}\sigma^{2}\frac{w_{KQ}^{\frac{d}{ 4}}}{n}\] \[\implies w_{KQ}\leq\left(\frac{nL^{2}}{\sigma^{2}}\right)^{\frac{2(d +2)}{d(4+4)}}\left(5\frac{\overline{c}}{\underline{c}}d^{\frac{2}{4+4}}\right) ^{\frac{2}{d}}\]

Of course, for this to not be vacuous we need

\[\left(\frac{nL^{2}}{\sigma^{2}}\right)^{\frac{2(d+2)}{d(d+4)}}\left(5\frac{ \overline{c}}{\underline{c}}d^{\frac{2}{4+4}}\right)^{\frac{2}{d}}\leq\left( \frac{1}{45\sqrt{d}}\frac{n}{\log n}\right)^{\frac{2}{d}}.\]

We will again hide constants that depend only on \(d\) and write this as

\[c_{1}\left(\frac{nL^{2}}{\sigma^{2}}\right)^{\frac{2(d+2)}{d(d+4)}}\leq c_{2} \left(\frac{n}{\log n}\right)^{\frac{2}{d}}\]

which again is true as long as \(n=\Omega\left(\frac{L\log^{2}n}{\sigma}\right)^{d+2}\) 

### Generalization Bounds

We conclude this section with a proof of the generalization error on a new \(L-\)Lipschitz task.

**Theorem E.3**.: _Suppose our attention is first pretrained on tasks drawn from \(D(\mathcal{F}_{L}^{+})\) and then tested on an arbitrary \(L-\)Lipschitz task, then the loss on the new task is upper bounded as \(\mathcal{L}\leq\mathcal{O}\left(\frac{L^{2}}{\Lambda^{\beta}}\right)\). Furthermore, if the new task is instead drawn from \(D(\mathcal{F}_{L^{\prime}}^{+})\), the loss is lower bounded as \(\mathcal{L}\geq\min\{\Omega(\frac{L^{\prime 2}}{\Lambda^{2\beta}}),\Omega(\frac{ \Lambda^{3d/2}}{n})\}\)_

Proof.: We know from Theorem E.2 that \(\Omega(\Lambda^{\beta})\leq w_{KQ}\leq\mathcal{O}(\Lambda^{2\beta})\). The upper bound for \(\mathcal{L}(w_{KQ})\), which is \(\mathcal{O}(\frac{L^{2}}{w_{KQ}}+\frac{w_{KQ}^{\frac{d}{4}}}{n})\), is a convex function for \(d\geq 2\), so in any range it attains its maximum value at the extreme points. We can check the cases to see that this is \(\mathcal{O}(\max\{\frac{L^{2}}{\Lambda^{\beta}}+\frac{\Lambda^{\beta\beta/2}} {n},\frac{L^{2}}{\Lambda^{2\beta}}+\frac{\Lambda^{\beta\beta}}{n}\})=\mathcal{ O}(\frac{L^{2}}{\Lambda^{\beta}}+\frac{\Lambda^{\beta\beta}}{n}+\frac{L^{2}}{ \Lambda^{2\beta}}+\frac{\Lambda^{\beta\beta}}{n})=\mathcal{O}(\frac{L^{2}}{ \Lambda^{\beta}})\) for large enough \(n\).

Now consider testing on a new task from \(D(F_{L^{\prime}}^{+})\). The ICL loss for \(\Omega\left(\Lambda^{\beta}\right)\leq w_{KQ}\leq\mathcal{O}\left(\Lambda^{2 \beta}\right)\) is bounded below as \(\Omega(\frac{L^{\prime 2}}{\Lambda^{2\beta}})\) and \(\Omega(\frac{\Lambda^{\beta d/2}}{n})\). 

The implication of this is that if \(L^{\prime}\gg L\), the error scales as \(\left(L^{\prime}\right)^{2}\) rather than \(\left(L^{\prime}\right)^{\frac{2d}{d+2}}\) while for \(L^{\prime}\ll L\), the error is lower bounded by a constant.

[MISSING_PAGE_EMPTY:31]

for \(p=0,1/2,1\). For this, we will need high probability upper and lower bounds on the number of points in a spherical cap under a uniform distribution over the hypersphere. Consider \(n\) points \(\{\bm{x}_{i}\}\) drawn uniformly from \(\sigma_{d-1}\), the uniform measure over \(S_{d-1}\), the \(d-\)dimensional hypersphere. The measure of the \(\epsilon-\) spherical cap around \(\bm{x}\in S_{d-1}\), \(C(\epsilon,\bm{x})=\{\bm{x}^{\prime}:\bm{x^{\prime}}^{\intercal}\bm{x}>1- \epsilon\}\) is denoted by \(\sigma_{\epsilon}\).

### Bounds on Spherical Caps

**Lemma G.1**.: _The area of the spherical cap \(C(\epsilon)\), \(\sigma_{\epsilon}\) is bounded as_

\[\frac{(2\epsilon-\epsilon^{2})^{\frac{d-1}{2}}}{\sqrt{2d\pi}}\leq\sigma_{ \epsilon}\leq(2\epsilon-\epsilon^{2})^{\frac{d}{2}}\leq(2\epsilon)^{\frac{d- 1}{2}}\,e^{-\epsilon d/4}\]

Proof.: We derive a lower bound as follows. We replace the surface area of a spherical cap in \(S_{d-1}\) with a \(d-1\) dimensional ball of the same boundary. Let \(V_{d}\) denote the volume of a \(d\) dimensional ball (that is, \(V_{3}(r)=\frac{4}{3}\pi r^{3}\)), and let \(A_{d}\) denote the surface area of a \(d\) dimensional sphere (so \(A_{3}(a)=4\pi r^{2}\)). It is known that

\[V_{d}(r)=\frac{\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2}+1)}r^{d},\text{ and }A_{d}(r)=\frac{2\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2})}r^{d-1}.\]

Then we have

\[\sigma_{\epsilon} \geq\frac{V_{d-1}\left((1-(1-\epsilon)^{2})^{\frac{1}{2}}\right) }{A_{d}(1)}\] \[=\frac{(1-(1-\epsilon)^{2})^{\frac{d-1}{2}}}{2\sqrt{\pi}}\frac{ \Gamma(\frac{d}{2})}{\Gamma(\frac{d+1}{2})}\] \[\geq\frac{(1-(1-\epsilon)^{2})^{\frac{d}{2}}}{\sqrt{d\pi}}\] Lemma G.6 \[=\frac{(2\epsilon-\epsilon^{2})^{\frac{d-1}{2}}}{\sqrt{2d\pi}}\]

The upper bound is similar. This time we replace the cap with the surface of a hemisphere with the same boundary. We have

\[\sigma_{\epsilon}\leq\frac{A_{d}\left((1-(1-\epsilon)^{2})^{\frac{1}{2}} \right)}{2A_{d}(1)}=\frac{(1-(1-\epsilon)^{2})^{\frac{d-1}{2}}}{2}\leq(2 \epsilon-\epsilon^{2})^{\frac{d-1}{2}}\]

Figure 8: The surface area of the purple hemisphere is used to upper bound the surface area of \(C(\frac{i}{r})\), while the _volume_ of the green hypersphere is used as a lower bound. Points in the orange region are \(S_{i+1}\setminus S_{i}\), and their count is \(N_{i+1}-N_{i}\).

We will also need upper and lower bounds on a discretized version of the incomplete gamma function.

**Definition G.2**.: _Denote by \(\gamma(d,\alpha,m)\) the expression \(\gamma(d,\alpha,m)=\sum_{i=1}^{m}i^{d}e^{-\alpha i}\)._

We have the following

**Lemma G.3**.: _For \(d>5,1\leq\alpha\leq 2,\) the incomplete Gamma function is bounded as_

\[\begin{cases}m^{d}e^{-\alpha m-1/2}\leq\gamma(d,\alpha,m)\leq m^{d+1}e^{- \alpha m-1/2}&m<d+\sqrt{d}\\ \frac{\Gamma(d+1)}{2\alpha^{d+1}}\leq\gamma(d,\alpha,m)\leq\frac{2\Gamma(d+1)} {\alpha^{d+1}}&m\geq d+\sqrt{d}\end{cases}\]

Proof.: We compare with the Gamma function

\[\Gamma(d+1)=\int_{0}^{\infty}t^{d}e^{-t}dt.\]

Note that \(\int_{0}^{\infty}t^{d}e^{-\alpha t}dt=\frac{1}{\alpha^{d+1}}\int_{0}^{\infty}t ^{d}e^{-t}dt=\frac{1}{\alpha^{d+1}}\Gamma(d+1)\). Because the function \(t^{d}e^{-\alpha t}\) is uni-modal with maximum \(\left(\frac{d}{ae}\right)^{d}\), we have from Lemma I.1

\[\sum_{i=1}^{m}i^{d}e^{-\alpha i}+\left(\frac{d}{\alpha e}\right)^{d}+\sum_{i= m}^{\infty}i^{d}e^{-\alpha i}\geq\int_{0}^{\infty}t^{d}e^{-\alpha t}dt=\frac{1}{ \alpha^{d+1}}\Gamma(d+1)\]

Now suppose \(m\geq\frac{d+\sqrt{d}}{\alpha}\). Then we have

\[\sum_{i=m}^{\infty}i^{d}e^{-\alpha i} \leq\sum_{i=\frac{d+\sqrt{d}}{\alpha}}^{\infty}i^{d}e^{-\alpha i}\] \[=\sum_{i=\frac{d+\sqrt{d}}{\alpha}}^{\infty}\left(\frac{d+\sqrt{d }}{\alpha}\right)^{d}e^{-(d+\sqrt{d})}\prod_{j=0}^{i-\frac{d+\sqrt{d}}{\alpha }}\left[\frac{1}{e^{\alpha}}\left(\frac{\frac{d+\sqrt{d}}{\alpha}+j+1}{\frac{d +\sqrt{d}}{\alpha}+j}\right)^{d}\right]\] \[\leq\sum_{i=\frac{d+\sqrt{d}}{\alpha}}^{\infty}\left(\frac{d+ \sqrt{d}}{\alpha}\right)^{d}e^{-(d+\sqrt{d})}\prod_{j=0}^{i-\frac{d+\sqrt{d}}{ \alpha}}\left[\frac{1}{e^{\alpha}}\left(\frac{\frac{d+\sqrt{d}}{\alpha}+1}{ \frac{d+\sqrt{d}}{\alpha}}\right)^{d}\right]\] \[\leq\sum_{i=\frac{d+\sqrt{d}}{\alpha}}^{\infty}\left(\frac{d+ \sqrt{d}}{\alpha}\right)^{d}e^{-(d+\sqrt{d})}\left(e^{-\frac{\alpha\sqrt{d}}{d +\sqrt{d}}}\right)^{i-\frac{d+\sqrt{d}}{\alpha}}\] \[=\left(\frac{d+\sqrt{d}}{\alpha}\right)^{d}e^{-(d+\sqrt{d})} \frac{1}{1-e^{-\alpha\sqrt{d}/(d+\sqrt{d})}}\leq\left(\frac{d}{\alpha e}\right) ^{d}\frac{2\sqrt{d}}{\alpha}\]

the first inequality follows because \(\frac{d+\sqrt{d}}{\alpha}\leq m\), the second follows because \(\frac{2d+1}{2d}\geq\frac{2d+j+1}{2d+j}\), the last follows because \(\left(1+\frac{\sqrt{d}}{d\alpha}\right)^{d}\leq e^{\frac{\sqrt{d}}{\alpha}}\) and \(\frac{1}{1-e^{d-x}}\leq 2x\) for \(x\leq 2\). Over all, we have

\[\sum_{i=1}^{m}i^{d}e^{-i}+\left(2\frac{\sqrt{d}}{\alpha}+1\right)\left(\frac{ d}{\alpha e}\right)^{d}\geq\int_{0}^{\infty}t^{d}e^{-t}dt=\frac{\Gamma(d+1)}{ \alpha^{d+1}}\]

While for the upper bound we have

\[\sum_{i=1}^{m}i^{d}e^{-\alpha i}-\left(\frac{d}{\alpha e}\right)^{d}\leq\int_{ 0}^{\infty}t^{d}e^{-\alpha t}dt=\frac{\Gamma(d+1)}{\alpha^{d+1}}\]

Finally, we use Lemma G.3, specifically that \(\left(\frac{d}{\alpha e}\right)^{d}\leq\frac{1}{\alpha^{d+1}}\sqrt{2\pi d} \left(\frac{d}{e}\right)^{d}\leq\frac{\Gamma(d+1)}{\alpha^{d+1}}\) to yield the desired result.

For \(m<\frac{d+\sqrt{d}}{\alpha}\), we have from Lemma G.7 that \(m^{d}e^{-\alpha m}\geq\frac{1}{\sqrt{e}}i^{d}e^{-\alpha i}\) so

\[\sum_{i=0}^{m}i^{d}e^{-\alpha i}\geq m^{d}e^{-\alpha m-\frac{1}{2}}\]

and

\[\sum_{i=0}^{m}i^{d}e^{-\alpha i}\leq m^{d+1}e^{-\alpha m-\frac{1}{2}}\]

### Bounds on \(g_{p}(r)\)

**Lemma G.4**.: _Suppose \(\{\bm{x}_{i}\}\) are drawn independently and uniformly from the unit hypersphere. For \(\frac{n}{\log n}\geq 45\sqrt{d}r^{\frac{d}{2}},n>5,d>2,p\leq 2\), we have \(g_{p}(r)=\sum_{i=1}^{n}\|\,\bm{x}_{i}-\bm{x}\,\|^{p}e^{-r\|\,\bm{x}_{i}^{\top}- \bm{x}\,\|^{2}}\) satisfies_

\[(1-e^{\frac{p}{2}-2})\frac{n2^{\frac{p}{2}}}{\sqrt{8e^{4}\pi d}} \left(\frac{1}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\gamma(\frac{d}{2}+\frac{p} {2},2,r)\leq g_{p}(r)\leq 3n\left(\frac{2}{r}\right)^{\frac{d}{2}+\frac{p}{2}} \gamma(\frac{d}{2}+\frac{p}{2},2,r)\]

_with probability at least \(1-\frac{1}{2n}\)_

Proof.: For \(0\leq i\leq r\) let \(N_{i}\) denote the number, and \(S_{i}\) denote the set, of points satisfying \(1-\frac{i}{r}\leq\bm{x}_{i}^{\top}\,\bm{x}\iff\|\,\bm{x}_{i}-\bm{x}\,\|\leq \left(\frac{2i}{r}\right)^{\frac{1}{2}}\). Also denote by \(N_{-1}\) the points satisfying \(\bm{x}_{i}^{\top}\,\bm{x}<0\), and let \(S_{-1}\) denote this set. Note that

\[g_{p}(r) =\sum_{i=0}^{n}\|\,\bm{x}_{i}^{\top}-\bm{x}\,\|^{p}e^{-r\|\,\bm{x }_{i}^{\top}-\bm{x}\,\|^{2}}\] \[=\sum_{i=0}^{r-1}\sum_{j\in S_{i+1}\setminus S_{i}}\|\,\bm{x}_{i}^ {\top}-\bm{x}\,\|^{p}e^{-r\|\,\bm{x}_{j}^{\top}-\bm{x}\,\|^{2}}+\sum_{j\in S_{- 1}}\|\,\bm{x}_{i}^{\top}-\bm{x}\,\|^{p}e^{-r\|\,\bm{x}_{j}^{\top}-\bm{x}\,\|^{2}}\] \[\leq\sum_{i=0}^{r-1}\left(\frac{2(i+1)}{r}\right)^{\frac{p}{2}}e^ {-2i}\left(N_{i+1}-N_{i}\right)+2^{p}e^{-2r}N_{-1}\]

Similarly,

\[h(r)\geq\sum_{i=0}^{r-1}\left(\frac{2i}{r}\right)^{\frac{p}{2}}e^ {-2(i+1)}\left(N_{i+1}-N_{i}\right)\]

Note that because \(N_{i}>0\),

\[\sum_{i=0}^{r-1}\left(\frac{2(i+1)}{r}\right)^{\frac{p}{2}}N_{i +1}e^{-2i}\geq\sum_{i=0}^{r-1}\left(\frac{2(i+1)}{r}\right)^{\frac{p}{2}} \left(N_{i+1}-N_{i}\right)e^{-2i}\]

And similarly,

\[\sum_{i=0}^{r-1}\left(\frac{2i}{r}\right)^{\frac{p}{2}}N_{i+1}e^ {-2i} =\sum_{i=1}^{r-1}\left(\frac{2i}{r}\right)^{\frac{p}{2}}\sum_{j=0} ^{i}\left(N_{j+1}-N_{j}\right)e^{-2i} \because i=0\implies\frac{2i}{r}=0\] \[=\sum_{j=1}^{r-1}\left(N_{j+1}-N_{j}\right)\sum_{i=j}^{r-1}\left( \frac{2i}{r}\right)^{\frac{p}{2}}e^{-2i}\] \[\leq\sum_{j=1}^{r-1}\left(N_{j+1}-N_{j}\right)\sum_{i=j}^{\infty} \left(\frac{2i}{r}\right)^{\frac{p}{2}}e^{-2i}\]\[\leq\sum_{j=1}^{r-1}\left(N_{j+1}-N_{j}\right)\sum_{i=j}^{\infty} \left(\frac{2j}{r}\right)^{\frac{p}{2}}e^{-2j}\left(\frac{\left(\frac{j+1}{j} \right)^{\frac{p}{2}}}{e^{2}}\right)^{i-j}\quad\because i<j\left(\frac{j+1}{j} \right)^{i-j}\] \[\leq\sum_{j=1}^{r-1}\left(N_{j+1}-N_{j}\right)\sum_{i=j}^{\infty} \left(\frac{2j}{r}\right)^{\frac{p}{2}}e^{-2j}\left(e^{\frac{p}{2j}-2}\right)^ {i-j}\quad\because 1+x\leq e^{x}\] \[\leq\sum_{j=1}^{r-1}\left(N_{j+1}-N_{j}\right)\sum_{i=j}^{\infty} \left(\frac{2j}{r}\right)^{\frac{p}{2}}e^{-2j}\left(e^{\frac{p}{2}-2}\right)^ {i-j}\quad\because j\geq 1\] \[\leq\sum_{j=1}^{r-1}\left(N_{j+1}-N_{j}\right)\left(\frac{2j}{r} \right)^{\frac{p}{2}}e^{-2j}\frac{1}{1-e^{\frac{p}{2}-2}}\quad\because p<4\]

and so

\[\left(1-e^{\frac{p}{2}-2}\right)\sum_{i=0}^{r-1}\left(\frac{2i}{r}\right)^{ \frac{p}{2}}N_{i+1}e^{-2(i+1)}\leq\sum_{j=0}^{r-1}\left(N_{j+1}-N_{j}\right) \left(\frac{2i}{r}\right)^{\frac{p}{2}}e^{-2(i+1)}\]

By a Chernoff bound for Binomial random variables, we have with probability \(1-\frac{r}{n^{2}}\):

\[N_{i}=n\sigma_{\frac{i}{r}}\leq n\sigma_{\frac{i}{r}}+\sqrt{6n\log n\sigma_{ \frac{i}{r}}}\leq 2n\sigma_{\frac{i}{r}}\;\forall r\]

and

\[N_{i}=n\sigma_{\frac{i}{r}}\geq n\sigma_{\frac{i}{r}}-\sqrt{4n\log n\sigma_{ \frac{i}{r}}}\leq\frac{1}{2}n\sigma_{\frac{i}{r}}\]

Whenever

\[n\sigma_{\frac{i}{r}}\geq 16\log n\;\forall i\leftarrow\frac{1}{\sqrt{2\pi d }}\left(\frac{1}{r}\right)^{\frac{d}{2}}\geq\frac{16\log n}{n}\]

and

\[N_{-1}\leq n\]

Over all we have with probability \(1-\frac{r}{n^{2}}\)

\[h(r) \leq\sum_{i=0}^{r-1}\left(\frac{2(i+1)}{r}\right)^{\frac{p}{2}}N_ {i+1}e^{-2i}+2^{p}N_{-1}e^{-2r}\] \[\leq n\sum_{i=0}^{r-1}2e^{-2i}\left(\frac{2(i+1)}{r}\right)^{ \frac{d}{2}+\frac{p}{2}}+2^{p}e^{-2r}n\] \[=2ne^{2}\left(\frac{2}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\sum_{i =1}^{r}i^{\frac{d}{2}+\frac{p}{2}}e^{-2i}+2^{p}e^{-2r}n\] \[=2ne^{2}\left(\frac{2}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\gamma (\frac{d}{2}+\frac{p}{2},2,r)+2^{p}e^{-2r}n\] Definition G.2

We always have for \(p\leq 2\)

\[2ne^{2}\left(\frac{2}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\gamma (\frac{d}{2}+\frac{p}{2},2,r)\geq 2^{p}e^{-2r}n\] \[\leftarrow\left(\frac{d}{2}\right)^{\frac{d}{2}}e^{2r}2^{-p}\geq r ^{\frac{d+p}{2}}\]

So at last, we have

\[g_{p}(r)\leq 16n\left(\frac{2}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\gamma (\frac{d}{2}+\frac{p}{2},2,r)\]We obtain a lower bound in the same way.

\[h(r) \geq(1-e^{\frac{p}{2}-2})\sum_{i=0}^{r-1}\left(\frac{2i}{r}\right)^{ \frac{p}{2}}e^{-2(i+1)}\frac{n}{2\sqrt{2\pi d}}\left(\frac{i}{r}\right)^{\frac{d }{2}}\] \[\geq(1-e^{\frac{p}{2}-2})\frac{n2^{\frac{p}{2}}}{\sqrt{8e^{4}\pi d }}\left(\frac{1}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\sum_{i=0}^{r-1}e^{-2i}i^{ \frac{d}{2}+\frac{p}{2}}\] \[\geq(1-e^{\frac{p}{2}-2})\frac{n2^{\frac{p}{2}}}{\sqrt{8e^{4}\pi d }}\left(\frac{1}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\gamma(\frac{d}{2}+\frac{p }{2},2,r)\]

\[(1-e^{\frac{p}{2}-2})\frac{n2^{\frac{p}{2}}}{\sqrt{8e^{4}\pi d}}\left(\frac{1} {r}\right)^{\frac{d}{2}+\frac{p}{2}}\gamma(\frac{d}{2}+\frac{p}{2},2,r)\leq h (r)\leq 3n\left(\frac{2}{r}\right)^{\frac{d}{2}+\frac{p}{2}}\gamma(\frac{d}{ 2}+\frac{p}{2},2,r)\]

with probability \(1-\frac{r}{n^{2}}\geq 1-\frac{1}{2n}\) when \(\frac{n}{\log n}\geq 45\sqrt{d}r^{\frac{d}{2}}\) 

It will be useful to simplify this bound in regimes that we are interested in

**Corollary G.5**.: _Suppose \(\{\bm{x}_{i}\}\) are drawn independently and uniformly from the unit hypersphere. For \(\frac{n}{\log n}\geq 45\sqrt{d}r^{\frac{d}{2}},n>5\), \(p\leq 2\leq d\), we have \(g_{p}(r)=\sum_{i=1}^{n}\|\bm{x}_{i}-\bm{x}\|^{p}e^{-r\|\bm{x}_{i}^{\top}-\bm{x }\|^{2}}\) satisfies with probability \(1-\frac{1}{2n}\)_

\[\begin{cases}g_{p}(r)=\Theta\left(\frac{n}{r^{\frac{d}{2}+p}}\right)&r\geq \frac{d+\sqrt{d}}{2}\\ g_{p}(r)=\Theta\left(ne^{-2r}\right)&r<\frac{d+\sqrt{d}}{2}\end{cases}\]

The following bounds are known for the Gamma function.

**Lemma G.6**.: _The Gamma function satisfies_

1. \(\sqrt{2\pi d}\left(\frac{d}{e}\right)^{d}\leq\Gamma(d+1)\leq e\sqrt{2\pi d} \left(\frac{d}{e}\right)^{d}\)__
2. \(\frac{\Gamma(x+\frac{1}{2})}{\Gamma(x+1)}\geq\frac{1}{\sqrt{x+0.5}}\)__

Proof.:
1. Please see [64].
2. Please see [65].

**Lemma G.7**.: _The following inequality holds:_

\[\left(1+\frac{1}{\sqrt{d}}\right)^{d}e^{-\sqrt{d}}\geq e^{-\frac{1}{2}}\] (20)

Proof.: Take the logarithm of both sides, we have that this is equivalent to

\[d\log\left(1+\frac{1}{\sqrt{d}}\right)\geq\sqrt{d}-\frac{1}{2}\]

A Taylor series expansion of \(\log(1+x)\) demonstrates that \(\log\!\left(1+\frac{1}{\sqrt{d}}\right)=\sum_{i}(-1)^{i+1}\frac{1}{i\sqrt{d}}\). For \(d>1\), these terms are decreasing in absolute value beyond \(i=2\), so we can upper bound the log with just the first two terms: \(\log\!\left(1+\frac{1}{\sqrt{d}}\right)\geq\frac{1}{\sqrt{d}}-\frac{1}{2d}\). 

## Appendix H Attention Window Captures Appropriate Directions

In this section we prove Theorem 4.4, which entails showing that if the Lipschitzness of the function class is zero in some directions, one-layer self-attention learns to ignore these directions when the function class consists of linear functions. First, we give a brief sketch of the proof.

### Proof Sketch

We briefly sketch the proof of Theorem 4.4. WLOG we write \(\mathbf{M}=\mathbf{BF}+\mathbf{B}_{\perp}\mathbf{G}\) where \(\mathbf{F}:=\mathbf{B}^{\top}\mathbf{M}\) and \(\mathbf{G}:=\mathbf{B}_{\perp}^{\top}\mathbf{M}\). Lemma H.2 leverages the rotational symmetry of \(\mathcal{F}_{\mathbf{B}}\) in \(\operatorname{col}(\mathbf{B})\) to show that the loss is minimized over \((\mathbf{F},\mathbf{G})\) at \((\mathbf{F},\mathbf{G})=(c\mathbf{B}^{\top},c^{\prime}\mathbf{B}_{\perp})\) for some constants \(c,c^{\prime}\). It remains to show that \(\mathcal{L}(c\mathbf{B}\mathbf{B}^{\top}+c^{\prime}\mathbf{B}_{\perp}\mathbf{ B}_{\perp}^{\top})>\mathcal{L}(c\mathbf{B}\mathbf{B}^{\top})\) whenever \(c^{\prime}\) is nonzero. Intuitively, if the attention estimator incorporates the closeness of \(\mathbf{B}_{\perp}^{\top}\,\boldsymbol{x}_{i}\) and \(\mathbf{B}_{\perp}^{\top}\,\boldsymbol{x}_{n+1}\) into its weighting scheme via nonzero \(\mathbf{Q}\), this may improperly up- or down-weight \(f(\boldsymbol{x}_{i})\), since projections of \(\boldsymbol{x}_{i}\) onto \(\operatorname{col}(\mathbf{B}_{\perp})\) do not carry any information about the closeness of \(f(\boldsymbol{x}_{i})\) and \(f(\boldsymbol{x}_{n+1})\).

Using this intuition, we show that for any fixed \(c^{\prime}\) and \(\{\mathbf{v}_{i}\}_{i}\) such that \(c^{\prime}\mathbf{v}_{i}^{\top}\mathbf{v}_{n+1}\neq\mathbf{v}_{i^{\prime}}^{ \top}\mathbf{Q}\mathbf{v}_{n+1}\) for some \(i,i^{\prime}\), the attention estimator improperly up-weights \(f(\boldsymbol{x}_{1})\), where \(1\in\operatorname{arg\,max}_{i}c^{\prime}\mathbf{v}_{i}^{\top}\mathbf{v}_{n+1}\) WLOG. In particular, the version of the pretraining population loss (ICL) with expectation over \(\mathbf{a}\), \(\{\mathbf{u}_{i}\}_{i}\) and \(\{\epsilon_{i}\}_{i}\) is reduced by reducing \(c^{\prime}\mathbf{v}_{1}^{\top}\mathbf{v}_{n+1}\). The only way to ensure all \(\{c^{\prime}\mathbf{v}_{i}^{\top}\mathbf{v}_{n+1}\}_{i}\) are equal for all instances of \(\{\mathbf{v}_{i}\}_{i}\) is to set \(c^{\prime}=0\), so this \(c^{\prime}\) must be optimal.

To show that reducing \(c^{\prime}\mathbf{v}_{i}^{\top}\mathbf{v}_{n+1}\) reduces the loss with fixed \(\{\mathbf{v}_{i}\}_{i}\), we define \(\alpha_{i}:=e^{c_{u}c^{\prime}\mathbf{v}_{i}^{\top}\mathbf{v}_{n+1}}\) for all \(i\in[n]\) and show the loss' partial derivative with respect to \(\alpha_{1}\) is positive, i.e.

\[\frac{\partial}{\partial\alpha_{1}}\left(\tilde{\mathcal{L}}(c,\{\alpha_{i}\} _{i}):=\mathbb{E}_{\mathbf{a},\{\mathbf{u}_{i}\}_{i,\{\epsilon_{i}\}_{i}}} \left[\left(\frac{\sum_{i=1}^{n}(\mathbf{a}^{\top}\mathbf{u}_{i}-\mathbf{a}^{ \top}\mathbf{u}_{n+1}+\epsilon_{i})e^{c\alpha_{u}^{\prime}\mathbf{u}_{i}^{ \top}\mathbf{u}_{i}}\alpha_{i}}{\sum_{i=1}^{n}e^{c\alpha_{u}^{\prime}\mathbf{ u}_{i}^{\top}\mathbf{u}_{n+1}}\alpha_{i}}\right)^{2}\right]\right)>0.\] (21)

This requires a careful symmetry-based argument as the expectation over \(\{\mathbf{u}_{i}\}_{i}\) cannot be evaluated in closed-form. To overcome this, we fix all \(\mathbf{u}_{i}\) but \(\mathbf{u}_{1}\) and one other \(\mathbf{u}_{i^{\prime}}\neq\mathbf{u}_{n+1}\) with \(\alpha_{i^{\prime}}<\alpha_{1}\). We show the expectation over \((\mathbf{u}_{1},\mathbf{u}_{i^{\prime}})\) can be written as an integral over \((\mathbf{y}_{1},\mathbf{y}_{2})\in\mathbb{S}^{k-1}\times\mathbb{S}^{k-1}\) of a sum of the derivatives at each of the four assignments of \((\mathbf{u}_{1},\mathbf{u}_{i^{\prime}})\) to \((\mathbf{y}_{1},\mathbf{y}_{2})\), and show that this sum is always positive. Intuitively, any "bad" assignment for which increasing \(\alpha_{1}\) reduces the loss is outweighed by the other assignments, which favor smaller \(\alpha_{1}\). For example, if \(\mathbf{y}_{1}=\mathbf{u}_{n+1}\neq\mathbf{y}_{2}\), and \(\mathbf{u}_{1}=\mathbf{y}_{1}\) and \(\mathbf{u}_{i^{\prime}}=\mathbf{y}_{2}\), we observe from (21) that increasing \(\alpha_{1}\) can reduce the loss. However, the cumulative increase in the loss on the other three assignments due to increasing \(\alpha_{1}\) is always greater.

### Full Proof

We now prove Theorem 4.4 in full detail.

**Lemma H.1**.: _For any \(\mathbf{u}\in\mathbb{S}^{k-1}\) and \(\alpha_{1},\ldots,\alpha_{n}\) such that \(\min_{i}\alpha_{i}>0\), and any \(c_{a},c_{u}\in\mathbb{R}\setminus\{0\}\), define_

\[J(c) :=c_{a}^{2}c_{u}^{2}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}} \left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top}( \mathbf{u}_{j}-\mathbf{u})e^{c_{u}^{2}c\mathbf{u}_{i}^{\top}\mathbf{u}+c_{u }^{2}c\mathbf{u}_{j}^{\top}\mathbf{u}}\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}e ^{c_{u}^{2}c\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\] \[\quad+\sigma^{2}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}}\left[ \frac{\sum_{i=1}^{n}e^{2c_{u}^{2}c\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}^{2 }}{(\sum_{i=1}^{n}e^{c_{u}^{2}c\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\]

_Then for any \(\delta>0\), \(0\notin\operatorname{arg\,min}_{0\leq c\leq\delta}J(c)\)._

Proof.: We show that there exists some arbitrarily small \(\epsilon>0\) such that \(J(\epsilon)<J(0)\) by showing \(\frac{dJ(c)}{dc}\big{|}_{c=0}<0\). We have

\[\frac{dJ(c)}{dc}\] \[=2c_{u}^{4}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}}\left[\sum_ {i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}(\mathbf{u}_{i}- \mathbf{u})^{\top}(\mathbf{u}_{i^{\prime}}-\mathbf{u})(\mathbf{u}_{i}^{\top} \mathbf{u}-\mathbf{u}_{i^{\prime\prime}}^{\top}\mathbf{u})\frac{e^{c_{u}^{2}c( \mathbf{u}_{i}+\mathbf{u}_{i^{\prime}}+\mathbf{u}_{i^{\prime\prime}})^{\top} \mathbf{u}}\alpha_{i}\alpha_{i^{\prime\prime}}\alpha_{i^{\prime\prime}}}{(\sum_{i=1} ^{n}e^{c_{u}^{2}c\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{3}}\right]\] \[\quad+2\sigma^{2}c_{u}^{2}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}} \Bigg{[}\sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}( \mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{i^{\prime}}^{\top}\mathbf{u})\frac{e ^{c_{u}^{2}c(2\mathbf{u}_{i}+\mathbf{u}_{i^{\prime}}+\mathbf{u}_{i^{\prime\prime}})^{ \top}\mathbf{u}}\alpha_{i}^{2}\alpha_{i^{\prime}}\alpha_{i^{\prime\prime}}}{( \sum_{i=1}^{n}e^{c_{u}^{2}c\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{4}} \Bigg{]}\]

Setting \(c=0\) results in

\[\frac{dJ(c)}{dc}\bigg{|}_{c=0}=\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n} \alpha_{i})^{3}}\sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n} \mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}}\left[(\mathbf{\[\begin{split}&+\frac{2\sigma^{2}c_{u}^{2}}{(\sum_{i=1}^{n}\alpha_{i})^{ 4}}\sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\mathbb{E} _{\{\mathbf{u}_{i}\}_{i\in[n]}}\Big{[}(\mathbf{u}_{i}^{\top}\mathbf{u}- \mathbf{u}_{i}^{\top}\mathbf{u})\alpha_{i}^{2}\alpha_{i^{\prime}}\alpha_{i^{ \prime\prime}}\Big{]}\\ &=\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}}\sum _{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\mathbb{E}_{\{ \mathbf{u}_{i}\}_{i\in[n]}}\left[(\mathbf{u}_{i}-\mathbf{u})^{\top}(\mathbf{u }_{i^{\prime}}-\mathbf{u})(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{i^{ \prime\prime}}^{\top}\mathbf{u})\alpha_{i}\alpha_{i^{\prime}}\alpha_{i^{ \prime\prime}}\right]\\ &=\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}}\sum _{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\mathbb{E}_{\{ \mathbf{u}_{i}\}_{i\in[n]}}\left[(\mathbf{u}_{i}^{\top}\mathbf{u}_{i^{\prime }}+1\right)(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{i^{\prime\prime}}^{ \top}\mathbf{u})\alpha_{i}\alpha_{i^{\prime}}\alpha_{i^{\prime\prime}}\right] \\ &\quad-\frac{2c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}}\sum_{i=1} ^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\mathbb{E}_{\{ \mathbf{u}_{i}\}_{i\in[n]}}\left[(\mathbf{u}^{\top}\mathbf{u}_{i^{\prime}}+ \mathbf{u}_{i}^{\top}\mathbf{u})(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{ i^{\prime\prime}}^{\top}\mathbf{u})\alpha_{i}\alpha_{i^{\prime}}\alpha_{i^{ \prime\prime}}\right]\\ &=-\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}}\sum _{i^{\prime}=1}^{n}\alpha_{i^{\prime}}\\ &\quad\times\left(\sum_{i=1}^{n}\sum_{i^{\prime\prime}=1}^{n} \mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}}\left[\mathbf{u}^{\top}\mathbf{u}_{i ^{\prime}}\mathbf{u}_{i}^{\top}\mathbf{u}\alpha_{i}\alpha_{i^{\prime\prime}} \right]-\sum_{i=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\mathbb{E}_{\{\mathbf{u}_{ i}\}_{i\in[n]}}\left[\mathbf{u}^{\top}\mathbf{u}_{i^{\prime}}\mathbf{u} _{i^{\prime\prime}}^{\top}\mathbf{u}\alpha_{i}\alpha_{i^{\prime\prime}}\right] \right)\\ &\quad-\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}} \sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\mathbb{E}_{ \{\mathbf{u}_{i}\}_{i\in[n]}}\left[\mathbf{u}_{i}^{\top}\mathbf{u}(\mathbf{u} _{i}^{\top}\mathbf{u}-\mathbf{u}_{i^{\prime\prime}}^{\top}\mathbf{u})\alpha_ {i}\alpha_{i^{\prime}}\alpha_{i^{\prime\prime}}\right]\\ &=-\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}}\sum _{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\mathbb{E}_{\{ \mathbf{u}_{i}\}_{i\in[n]}}\left[\mathbf{u}_{i}^{\top}\mathbf{u}(\mathbf{u} _{i}^{\top}\mathbf{u}-\mathbf{u}_{i^{\prime\prime}}^{\top}\mathbf{u})\alpha_ {i}\alpha_{i^{\prime}}\alpha_{i^{\prime\prime}}\right]\\ &=-\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}}\sum _{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\alpha_{i} \alpha_{i^{\prime}}\alpha_{i^{\prime\prime}}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i \in[n]}}\left[\mathbf{u}^{\top}\mathbf{u}_{i}\mathbf{u}_{i}^{\top}\mathbf{u} \right]\\ &\quad+\frac{2c_{a}^{2}c_{u}^{4}}{(\sum_{i=1}^{n}\alpha_{i})^{3}} \sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\sum_{i^{\prime\prime}=1}^{n}\alpha_{i} \alpha_{i^{\prime}}\alpha_{i^{\prime\prime}}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i \in[n]}}\left[\mathbf{u}^{\top}\mathbf{u}_{i}\mathbf{u}_{i^{\prime\prime}}^{ \top}\mathbf{u}\right]\\ &=-\frac{2c_{a}^{2}c_{u}^{4}}{k}+\frac{2c_{u}^{4}}{(\sum_{i=1}^{n} \alpha_{i})^{3}}\sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\alpha_{i}^{2}\alpha_{i^{ \prime}}\\ &=-\frac{2c_{a}^{2}c_{u}^{4}}{k}\left(1-\frac{\sum_{i=1}^{n} \alpha_{i}^{2}}{(\sum_{i=1}^{n}\alpha_{i})^{2}}\right)\\ &<0\end{split}\] (26)

where (22) follows since \(\mathbb{E}[\mathbf{u}_{i}]=\mathbf{0}_{k}\), (23) similarly follows since odd moments of uniform random variables on the hypersphere are zero, (24) follows by the i.i.d.-ness of the \(\mathbf{u}_{i}\)'s, (25) follows since \(\mathbb{E}[\mathbf{u}_{i}\mathbf{u}_{i}^{\top}]=\frac{1}{k}\mathbf{I}_{k}\) and \(\mathbf{u}^{\top}\mathbf{u}=1\), and (26) follows since \(\min_{i}\alpha_{i}>0\). This completes the proof. 

**Lemma H.2**.: _Consider any \(\mathbf{B}\in\mathbb{O}^{d\times k}\) and resulting function class \(\mathcal{F}_{\mathbf{B}}^{\text{lin}}\). Consider the training population loss \(\mathcal{L}\) defined in (1CL), and tasks drawn from \(D(\mathcal{F}_{\mathbf{B}}^{\text{lin}})\) such that \(\mathbb{E}_{\mathbf{a}}[\mathbf{a}\mathbf{a}^{\top}]=c_{a}^{2}\mathbf{I}_{k}\) for some \(c_{a}\neq 0\) and let \(\mathbf{M}:=\mathbf{M}_{K}^{\top}\mathbf{M}_{Q}\) be optimized over the domain \(\mathcal{M}_{\hat{c}}:=\{\mathbf{M}\in\mathbb{R}^{d\times d}:\mathbf{M}=\mathbf{M}^{ \top},\|\mathbf{B}^{\top}\mathbf{M}\mathbf{B}\|_{2}\leq\frac{\hat{c}}{c_{a}^{2}}\}\) for any \(\hat{c}>0\). Then any_

\[\mathbf{M}^{*}\in\arg\min_{\mathbf{M}\in\mathcal{M}_{\hat{c}}}\mathcal{L}(\mathbf{M})\] (27)

_satisfies \(\mathbf{M}^{*}=c_{1}^{*}\mathbf{B}\mathbf{B}^{T}+c_{2}^{*}\mathbf{B}_{\perp} \mathbf{B}_{\perp}^{\top}\) for some \(c_{1}^{*}:|c_{1}^{*}|\in(0,\frac{\hat{c}}{c_{a}^{2}}]\)._

Proof.: Without loss of generality (WLOG), we can decompose \(\mathbf{M}=\mathbf{BF}+\mathbf{B}_{\perp}\mathbf{G}\) where \(\mathbf{F}:=\mathbf{B}^{\top}\mathbf{M}\) and \(\mathbf{G}:=\mathbf{B}_{\perp}^{\top}\mathbf{M}\). Recall that for each \(i\in[n+1]\), \(\mathbf{x}_{i}=c_{u}\mathbf{Bu}_{i}+c_{v}\mathbf{B}_{\perp}\mathbf{v}_{i}\). Thus, for each \(i\in[n]\)we have

\[e^{\mathbf{x}_{i}^{\top}\mathbf{M}\mathbf{x}_{n+1}} =e^{\mathbf{x}_{i}^{\top}\mathbf{B}\mathbf{F}\mathbf{x}_{n+1}}e^{ \mathbf{x}_{i}^{\top}\mathbf{B}_{\perp}\mathbf{G}\mathbf{x}_{n+1}}\] \[=e^{c_{u}\mathbf{u}_{i}^{\top}\mathbf{F}\mathbf{x}_{n+1}}e^{c_{v} \mathbf{v}_{i}^{\top}\mathbf{G}\mathbf{x}_{n+1}}\] \[=e^{c_{u}\mathbf{u}_{i}^{\top}\mathbf{F}\mathbf{x}_{n+1}}\alpha_{i}\] (28)

where, for each \(i\in[n]\), \(\alpha_{i}:=e^{c_{v}\mathbf{v}_{i}^{\top}\mathbf{G}\mathbf{x}_{n+1}}\). For ease of notation, denote \(\mathbf{x}=\mathbf{x}_{n+1}\).

We start by expanding the square and using the linearity of the expectation to re-write the population loss as:

\[\mathcal{L}(\mathbf{M})\] \[=\mathbb{E}_{\mathbf{a},\mathbf{x},\{\mathbf{x}_{i}\}_{i\in[n]}, \{\epsilon_{i}\}_{i\in[n]}}\] \[=c_{u}^{\top}\mathbb{E}_{\mathbf{a},\mathbf{x},\{\mathbf{u}_{i} \},\{\mathbf{v}_{i}\}_{i\in[n]}}\] \[\quad\left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{a}^{\top} \mathbf{u}_{i}-\mathbf{a}^{\top}\mathbf{u})(\mathbf{a}^{\top}\mathbf{u}_{j}- \mathbf{a}^{\top}\mathbf{u})e^{c_{u}\mathbf{u}_{i}^{\top}\mathbf{F}\mathbf{x}+c _{u}\mathbf{u}_{j}^{\top}\mathbf{F}\mathbf{x}}\alpha_{i}\alpha_{j}}{(\sum_{i=1} ^{n}e^{c_{u}\mathbf{u}_{i}^{\top}\mathbf{F}\mathbf{x}}\alpha_{i})^{2}}\right]\] \[\quad\quad+\sigma^{2}\mathbb{E}_{\mathbf{u},\{\mathbf{u}_{i}\}, \{\alpha_{i}\}_{i\in[n]}}\{e_{i}\}_{i\in[n]}\left[\frac{\sum_{i=1}^{n}e^{2c_{v }\mathbf{u}_{i}^{\top}\mathbf{F}\mathbf{x}}\alpha_{i}^{2}}{(\sum_{i=1}^{n}e^{ c_{u}\mathbf{u}_{i}^{\top}\mathbf{F}\mathbf{x}}\alpha_{i})^{2}}\right]\] \[=\mathbb{E}_{\mathbf{x}}\Bigg{[}\underbrace{c_{a}^{2}c_{u}^{2} \mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}_{i\in[n]}}\left[\frac{\sum_ {i=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{u})e^{c_{ u}\mathbf{u}_{i}^{\top}\mathbf{F}\mathbf{x}+c_{u}\mathbf{u}_{j}^{\top} \mathbf{F}\mathbf{x}}\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u}\mathbf{u}_{i }^{\top}\mathbf{F}\mathbf{x}}\alpha_{i})^{2}}\right]}_{=:\tilde{\mathcal{L}}_{ \text{input}}(\mathbf{M},\mathbf{x})}\] \[\quad\quad+\underbrace{\sigma^{2}\mathbb{E}_{\{\mathbf{u}_{i}\}, \{\mathbf{v}_{i}\}_{i\in[n]}}\left[\frac{\sum_{i=1}^{n}e^{2c_{v}\mathbf{u}_{i} ^{\top}\mathbf{F}\mathbf{x}}\alpha_{i}^{2}}{(\sum_{i=1}^{n}e^{c_{u}\mathbf{u}_ {i}^{\top}\mathbf{F}\mathbf{x}}\alpha_{i})^{2}}\right]}_{=:\tilde{\mathcal{L}}_ {\text{input}}(\mathbf{M},\mathbf{x})}\] (29)

WLOG we can write \(\mathbf{F}\mathbf{x}=\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}\|\mathbf{F} \mathbf{x}\|_{2}\) for some rotation matrix \(\mathbf{R}(\mathbf{F}\mathbf{x})\in\mathbb{O}^{k\times k}\). Denote \(C_{1}(\mathbf{F}\mathbf{x}):=\|\mathbf{F}\mathbf{x}\|_{2}\). Then we have

\[\tilde{\mathcal{L}}_{\text{signal}}(\mathbf{M},\mathbf{x})\] \[=c_{a}^{2}c_{u}^{2}\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i }\}}\left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top} (\mathbf{u}_{j}-\mathbf{u})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^{ \top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}+c_{u}C_{1}(\mathbf{F}\mathbf{ x})\mathbf{u}_{j}^{\top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}}\alpha_{i} \alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^{ \top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}}\alpha_{i})^{2}}\right]\] \[=c_{a}^{2}c_{u}^{2}\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i} \}}\left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top} \mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{R}(\mathbf{F}\mathbf{x})^{\top}( \mathbf{u}_{j}-\mathbf{u})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^{ \top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}+c_{u}C_{1}(\mathbf{F}\mathbf{ x})\mathbf{u}_{j}^{\top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}}\alpha_{i} \alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^{ \top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}}\alpha_{i})^{2}}\right]\] (30) \[=c_{a}^{2}c_{u}^{2}\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i} \}}\Bigg{[}\frac{1}{(\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}}\alpha_{i})^{2}}\] \[\quad\quad\quad\quad\quad\times\sum_{i=1}^{n}\sum_{j=1}^{n}\left(( \mathbf{R}(\mathbf{F}\mathbf{x})^{\top}\mathbf{u}_{i}-\mathbf{R}(\mathbf{F} \mathbf{x})^{\top}\mathbf{u})^{\top}(\mathbf{R}(\mathbf{F}\mathbf{x})^{\top} \mathbf{u}_{j}-\mathbf{R}(\mathbf{F}\mathbf{x})^{\top}\mathbf{u})\right.\] \[\quad\quad\quad\quad\quad\times e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{u}+c_{u}C_{1}( \mathbf{F}\mathbf{x})\mathbf{u}_{j}^{\top}\mathbf{R}(\mathbf{F}\mathbf{x}) \mathbf{u}}\alpha_{i}\alpha_{j}\right)\Bigg{]}\] \[=c_{a}^{2}c_{u}^{2}\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i} \}}\left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{R}(\mathbf{F} \mathbf{x})^{\top}\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{R}(\mathbf{F} \mathbf{x})^{\top}\mathbf{u})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^{ \top}\mathbf{u}+c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{j}^{\top}\mathbf{u}} \alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\] (31)where (30) follows since \(\mathbf{R}(\mathbf{F}\mathbf{x})\mathbf{R}(\mathbf{F}\mathbf{x})^{\top}=\mathbf{I}_ {k}\) and (31) follows since the distribution of \(\mathbf{u}_{i}\) is the same as the distribution of \(\mathbf{R}(\mathbf{F}\mathbf{x})^{\top}\mathbf{u}_{i}\) for any rotation \(\mathbf{R}(\mathbf{F}\mathbf{x})^{\top}\). Define

\[g(\mathbf{F},\mathbf{u},\mathbf{v}):=\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{ v}_{i}\}}\left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{R}( \mathbf{F}\mathbf{x})^{\top}\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{R}( \mathbf{F}\mathbf{x})^{\top}\mathbf{u})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}+c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{ j}^{\top}\mathbf{u}}\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u}C_{1}( \mathbf{F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\]

for any \(\mathbf{F}\in\mathbb{R}^{k\times d}\). We have \(\mathcal{L}_{\text{signal}}(\mathbf{M})=c_{a}^{2}c_{u}^{2}\mathbb{E}_{\mathbf{u },\mathbf{v}}[g(\mathbf{F},\mathbf{u},\mathbf{v})]\), and Note that if \(\mathbf{F}^{\prime}=c\mathbf{B}^{\top}\), then \(\mathbf{R}_{\mathbf{F}^{\prime}\mathbf{x}}=\mathbf{I}_{k}\) and \(C_{1}(\mathbf{F}^{\prime}\mathbf{x})=c_{u}c\). Thus,

\[g(\mathbf{F},\mathbf{u},\mathbf{v})-g(\frac{C_{1}(\mathbf{F} \mathbf{x})}{c_{u}}\mathbf{B}^{\top},\mathbf{u},\mathbf{v})\] \[=\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{ \sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{R}(\mathbf{F}\mathbf{x})^{ \top}\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{R}(\mathbf{F}\mathbf{x})^{\top }\mathbf{u})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u} +c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{j}^{\top}\mathbf{u}}\alpha_{i} \alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^ {\top}\mathbf{u}}\alpha_{i})^{2}}\right]\] \[=\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{ \sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{i}^{ \top}\mathbf{R}(\mathbf{F}\mathbf{x})^{\top}\mathbf{u})e^{c_{u}C_{1}(\mathbf{ F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u}C_{1}( \mathbf{F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\] \[=2\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{ \sum_{i=1}^{n}(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{i}^{\top}\mathbf{R} (\mathbf{F}\mathbf{x})^{\top}\mathbf{u})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}{\sum_{i=1}^{n}e^{c_{u}C_{1}( \mathbf{F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}\right]\] \[=2\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{ \sum_{i=1}^{n}\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{i}^{\top}\mathbf{R} (\mathbf{F}\mathbf{x})^{\top}\mathbf{u})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}{\sum_{i=1}^{n}e^{c_{u}C_{1}( \mathbf{F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}\right]\] \[=2(\mathbf{u}^{\top}-\mathbf{u}^{\top}\mathbf{R}(\mathbf{F} \mathbf{x}))\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{ \sum_{i=1}^{n}\mathbf{u}_{i}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x})\mathbf{u}_{i}^{ \top}\mathbf{u}}\alpha_{i}}{\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}\right]\] (32)

Define \(\hat{\mathbf{u}}\coloneqq\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}} \left[\frac{\sum_{i=1}^{n}\mathbf{u}_{i}e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}}\alpha_{i}}{\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{F} \mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}\right]\) and WLOG write \(\mathbf{u}_{i}=\mathbf{p}_{\mathbf{u}_{i}}+\mathbf{q}_{\mathbf{u}_{i}}\), where \(\mathbf{p}_{\mathbf{u}_{i}}\coloneqq\mathbf{u}\mathbf{u}^{\top}\mathbf{u}_{i}\) and \(\mathbf{q}_{\mathbf{u}_{i}}\coloneqq(\mathbf{I}_{k}-\mathbf{u}\mathbf{u}^{\top} )\mathbf{u}_{i}\). Note that for any \(\mathbf{u}_{i}=\mathbf{p}_{\mathbf{u}_{i}}+\mathbf{q}_{\mathbf{u}_{i}}\), \(\mathbf{u}_{i}^{\prime}:=\mathbf{p}_{\mathbf{u}_{i}}-\mathbf{q}_{\mathbf{u}_{i}}\) occurs with equal probability, and flipping \(\mathbf{q}_{\mathbf{u}_{i}}\) does not change any exponent or \(\alpha_{i}\) in (32). Thus

\[\hat{\mathbf{u}} =\mathbb{E}_{\{(\mathbf{p}_{\mathbf{u}_{i}},\mathbf{q}_{\mathbf{u}_{ i}})\}_{i\in[n]},\{\mathbf{v}_{i}\}}\left[\frac{\sum_{i=1}^{n}(\mathbf{p}_{ \mathbf{u}_{i}}+\mathbf{q}_{\mathbf{u}_{i}})e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}{\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{ F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}\right]\] \[=\mathbb{E}_{\{\mathbf{p}_{\mathbf{u}_{i}}\}_{i},\{\mathbf{v}_{i}\}} \left[\frac{\sum_{i=1}^{n}\mathbf{p}_{\mathbf{u}_{i}}e^{c_{u}C_{1}(\mathbf{F} \mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}{\sum_{i=1}^{n}e^{c_{u}C_{1}( \mathbf{F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}\right]\] (33) \[=\tilde{c}\ \mathbf{u}\]

where \(\tilde{c}:=\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{\sum_{i=1 }^{n}\mathbf{u}_{i}^{\top}\mathbf{u}\ e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}{\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{ F}\mathbf{x})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}\right]\). Note that for any \(\mathbf{u}_{i}\), \(-\mathbf{u}_{i}\) occurs with equal probability, so

\[\tilde{c} =\sum_{i=1}^{n}\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}} \left[\frac{\mathbf{u}^{\top}\mathbf{u}\ e^{c_{u}C_{1}(\mathbf{F}\mathbf{x}) \mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}}{\sum_{j=\[\mathcal{L}(\mathbf{M}) =\mathbb{E}_{\mathbf{x}}[\tilde{\mathcal{L}}_{\text{signal}}(\mathbf{ BF}+\mathbf{B}_{\perp}\mathbf{G},\mathbf{x})+\tilde{\mathcal{L}}_{\text{noise}}( \mathbf{BF}+\mathbf{B}_{\perp}\mathbf{G},\mathbf{x})]\] \[\geq\mathbb{E}_{\mathbf{u},\mathbf{v}}\Bigg{[}c_{a}^{2}c_{i}^{2} \mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{\sum_{i=1}^{n} \sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{u})e^ {c_{u}C_{1}(\mathbf{Fx})\mathbf{u}_{i}^{\top}\mathbf{u}+c_{u}C_{1}(\mathbf{Fx })\mathbf{u}_{j}^{\top}\mathbf{u}}\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}e^{c_{u }C_{1}(\mathbf{Fx})\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\] \[\qquad\qquad+\sigma^{2}\mathbb{E}_{\{\mathbf{u}_{i}\},\{\mathbf{ v}_{i}\}}\left[\frac{\sum_{i=1}^{n}e^{2c_{u}C_{1}(\mathbf{Fx})\mathbf{u}_{i}^{\top} \mathbf{u}}\alpha_{i}^{2}}{(\sum_{i=1}^{n}e^{c_{u}C_{1}(\mathbf{Fx})\mathbf{u} _{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\Bigg{]}\] (37)

where (37) is strict if \(\mathbf{R}(\mathbf{Fx})\mathbf{u}\neq\mathbf{u}\) for some \(\mathbf{u},\mathbf{v}\), which is equivalent to saying that \(\mathbf{F}\notin\{c^{\prime}\mathbf{B}^{\top},c^{\prime}>0\}\).

Next, recall that we have defined \(\alpha_{i}\coloneqq e^{c_{v}\mathbf{v}_{i}^{\top}\mathbf{Gx}}\). Using a similar argument as earlier, by the rotational invariance of the \(\mathbf{v}_{i}\)'s, for any fixed \(\mathbf{x}\), we can write \(\alpha_{i}=e^{c_{v}C_{2}(\mathbf{Gx})\mathbf{v}_{i}^{\top}\mathbf{e}_{1}}\) where \(C_{2}(\mathbf{Gx})\coloneqq\|\mathbf{Gx}\|_{2}\) and \(\mathbf{e}_{1}\) is the first standard basis vector.

Next, for \(c_{1},c_{2}\geq 0\) and some fixed \(\mathbf{u},\mathbf{v}\), define

\[H(\mathbf{u},\mathbf{v},c_{1},c_{2})\coloneqq c_{a}^{2}c_{i}^{2}\mathbb{E}_{\{ \mathbf{u}_{i}\},\{\mathbf{v}_{i}\}}\left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}( \mathbf{u}_{i}-\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{u})e^{c_{u}c_{1} \mathbf{u}_{i}^{\top}\mathbf{u}+c_{u}c_{1}\mathbf{u}_{j}^{\top}\mathbf{u}}e^ {c_{u}c_{2}\mathbf{v}_{i}^{\top}\mathbf{e}_{1}+c_{v}c_{2}\mathbf{v}_{j}^{\top }\mathbf{e}_{1}}}{(\sum_{i=1}^{n}e^{c_{u}c_{1}\mathbf{u}_{i}^{\top}\mathbf{u} }e^{c_{u}c_{2}\mathbf{v}_{i}^{\top}\mathbf{e}_{1}})^{2}}\right]\]\[\frac{\partial H_{\text{signal}}(\mathbf{u},\boldsymbol{\alpha})}{ \partial\alpha_{1}}\] \[=\mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}}\left[\frac{\partial\ \sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top}(\mathbf{u}_{j}- \mathbf{u})e^{c\mathbf{u}_{i}^{\top}\mathbf{u}+c\mathbf{u}_{j}^{\top}\mathbf{u }}\mathbf{\alpha}_{i}\alpha_{j}}{(\sum_{i=1}^{n}e^{c\mathbf{u}_{i}^{\top} \mathbf{u}}\mathbf{\alpha}_{i})^{2}}\right]\] \[=2\mathbb{E}_{\{\mathbf{u}_{i}\}_{i}}\left[\frac{(\sum_{i=1}^{n}e ^{c\mathbf{u}_{i}^{\top}\mathbf{u}}\mathbf{\alpha}_{i})^{2}\left(\sum_{j=2}^{n }(\mathbf{u}_{1}-\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{u})e^{c\mathbf{u}_ {i}^{\top}\mathbf{u}+c\mathbf{u}_{j}^{\top}\mathbf{u}}\mathbf{\alpha}_{j}+ \|\mathbf{u}_{1}-\mathbf{u}\|_{2}^{2}e^{2c\mathbf{u}_{i}^{\top}\mathbf{u}} \mathbf{\alpha}_{1}\right)}{(\sum_{i=1}^{n}e^{c\mathbf{u}_{i}^{\top}\mathbf{u }}\mathbf{\alpha}_{i})^{4}}\right]\] \[\quad-2\mathbb{E}_{\{\mathbf{u}_{i}\}_{i}}\left[\frac{(\sum_{i=1} ^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{u })e^{c\mathbf{u}_{i}^{\top}\mathbf{u}+c\mathbf{u}_{j}^{\top}\mathbf{u}} \mathbf{\alpha}_{i}\alpha_{j})(\sum_{i=1}^{n}e^{c\mathbf{u}_{i}^{\top}\mathbf{ u}}\mathbf{\alpha}_{i})e^{c\mathbf{u}_{i}^{\top}\mathbf{u}}}{(\sum_{i=1}^{n}e^{c \mathbf{u}_{i}^{\top}\mathbf{u}}\mathbf{\alpha}_{i})^{4}}\right]\] \[=2\mathbb{E}_{\{\mathbf{u}_{i}\}_{i}}\left[\frac{(\sum_{i=1}^{n}e ^{c\mathbf{u}_{i}^{\top}\mathbf{u}}\mathbf{\alpha}_{i})\left(\sum_{j=1}^{n}( \mathbf{u}_{1}-\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{u})e^{c\mathbf{u}_{i }^{\top}\mathbf{u}+c\mathbf{u}_{j}^{\top}\mathbf{u}}\mathbf{\alpha}_{j}\right) }{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}} \mathbf{\alpha}_{i^{\prime}})^{3}}\right]\] \[\quad-2\mathbb{E}_{\{\mathbf{u}_{i}\}_{i}}\left[\frac{(\sum_{i=1} ^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top}(\mathbf{u}_{j}-\mathbf{u })e^{c\mathbf{u}_{i}^{\top}\mathbf{u}+c\mathbf{u}_{j}^{\top}\mathbf{u}} \mathbf{\alpha}_{i}\alpha_{j})e^{c\mathbf{u}_{i}^{\top}\mathbf{u}}}{(\sum_{i^ {\prime}=1}^{n}e^{c\mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}}\mathbf{\alpha}_{i ^{\prime}})^{3}}\right]\] \[=2\sum_{i=2}^{n}\sum_{j=1}^{n}S_{i,j}\] (41)

where

\[S_{i,j}\coloneqq\alpha_{i}\alpha_{j}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i ^{\prime}\in[n]}}\left[\frac{(\mathbf{u}_{1}-\mathbf{u}_{i})^{\top}(\mathbf{u }_{j}-\mathbf{u})e^{c(\mathbf{u}_{i}^{\top}\mathbf{u}+\mathbf{u}_{i}^{\top} \mathbf{u}+\mathbf{u}_{j}^{\top}\mathbf{u})}}{(\sum_{i^{\prime}=1}^{n}e^{c \mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}}\mathbf{\alpha}_{i^{\prime}})^{3}} \right].\]

Note that terms with \(i=1\) do not appear in (41). We analyze \(S_{i,1}+S_{i,i}\) and each \(S_{i,j}\), \(j\notin\{1,i\}\) separately, and will ultimately show that each of these terms is positive. We start with the latter case as it is easier to handle. For \(j\notin\{1,i\}\), we have

\[S_{i,j}=\alpha_{i}\alpha_{j}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime} \in[n]}}\left[\frac{(\mathbf{u}_{1}-\mathbf{u}_{i})^{\top}(\mathbf{u}_{j}- \mathbf{u})e^{c(\mathbf{u}_{i}^{\top}\mathbf{u}+\mathbf{u}_{i}^{\top}\mathbf{u }+\mathbf{u}_{j}^{\top}\mathbf{u})}}{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{ i^{\prime}}^{\top}\mathbf{u}}\mathbf{\alpha}_{i^{\prime}})^{3}}\right]\]\[=\alpha_{i}\alpha_{j}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime }\in[n]}}\left[\frac{(\mathbf{u}_{1}-\mathbf{u}_{i})^{\top}\mathbf{u}_{i^{\top} }(\mathbf{u}_{j}-\mathbf{u})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{i }^{\top}\mathbf{u}+\mathbf{u}_{j}^{\top}\mathbf{u})}}{(\sum_{i^{\prime}=1}^{n} e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[\quad+\alpha_{i}\alpha_{j}\underbrace{\mathbb{E}_{\{\mathbf{u}_{i ^{\prime}}\}_{i^{\prime}\in[n]}}\left[\frac{\mathbf{u}_{1}^{\top}(\mathbf{I}_{ k}-\mathbf{u}_{i}^{\top})(\mathbf{u}_{j}-\mathbf{u})e^{c(\mathbf{u}_{1}^{\top} \mathbf{u}+\mathbf{u}_{i}^{\top}\mathbf{u}+\mathbf{u}_{j}^{\top}\mathbf{u})} }{(\sum_{i^{\prime}=1}^{n}e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha _{i^{\prime}})^{3}}\right]}_{=0}\] \[\quad-\alpha_{i}\alpha_{j}\underbrace{\mathbb{E}_{\{\mathbf{u}_{i ^{\prime}}\}_{i^{\prime}\in[n]}}\left[\frac{\mathbf{u}_{i}^{\top}(\mathbf{I}_{ k}-\mathbf{u}_{i}^{\top})(\mathbf{u}_{j}-\mathbf{u})e^{c(\mathbf{u}_{1}^{\top} \mathbf{u}+\mathbf{u}_{i}^{\top}\mathbf{u}+\mathbf{u}_{j}^{\top}\mathbf{u})} }{(\sum_{i^{\prime}=1}^{n}e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha _{i^{\prime}})^{3}}\right]}_{=0}\] (42) \[=\alpha_{i}\alpha_{j}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{(\mathbf{u}_{1}^{\top}\mathbf{u}-\mathbf{u}_{i}^{ \top})(\mathbf{u}_{j}^{\top}\mathbf{u}-1)e^{c(\mathbf{u}_{1}^{\top}\mathbf{u} +\mathbf{u}_{i}^{\top}\mathbf{u}+\mathbf{u}_{j}^{\top}\mathbf{u})}}{(\sum_{i^ {\prime}=1}^{n}e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime }})^{3}}\right]\]

where the latter two terms in (42) are zero by the same argument as in (33): flipping the component of either \(\mathbf{u}_{1}\) or \(\mathbf{u}_{i}\) perpendicular to \(\mathbf{u}\) does not change any of the values in any exponent, and each flip occurs with equal probability. Next, note that if \(\alpha_{i}=\alpha_{1}\),

\[\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime}\in[n]}}\left[\frac{ \mathbf{u}_{1}^{\top}\mathbf{u}(\mathbf{u}_{j}^{\top}\mathbf{u}-1)e^{c( \mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{i}^{\top}\mathbf{u}+\mathbf{u}_{j }^{\top}\mathbf{u})}}{(\sum_{i^{\prime}=1}^{n}e^{\mathbf{cu}_{i^{\prime}}^{ \top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]=\mathbb{E}_{\{\mathbf{u}_{i^{ \prime}}\}_{i^{\prime}\in[n]}}\left[\frac{\mathbf{u}_{i}^{\top}\mathbf{u}( \mathbf{u}_{j}^{\top}\mathbf{u}-1)e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+ \mathbf{u}_{i}^{\top}\mathbf{u}+\mathbf{u}_{j}^{\top}\mathbf{u})}}{(\sum_{i^ {\prime}=1}^{n}e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime }})^{3}}\right]\]

thus \(S_{i,j}=0\). Otherwise, \(\alpha_{i}<\alpha_{1}\) by definition of \(\alpha_{1}\), and there must be some such \(\alpha_{i}\), since if not, there would be some \(c^{\prime}\in\mathbb{R}_{+}\) such that \(\boldsymbol{\alpha}=c^{\prime}\boldsymbol{\alpha}^{*}\). For the case \(\alpha_{i}<\alpha_{1}\), we use a symmetry argument to show that \(S_{i,j}>0\).

First we define additional notations. Let \(\bar{U}_{1,i}\coloneqq\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime}\in[n]\setminus\{1,i\}}\), and for any \((a,b)\in[-1,1]^{2}\), define

\[f_{a,b}(\bar{U}_{1,i})\coloneqq\frac{(a-b)(\mathbf{u}_{j}^{\top}\mathbf{u}-1)e ^{c(a+b+\mathbf{u}_{j}^{\top}\mathbf{u})}}{(e^{ca}\alpha_{1}+e^{cb}\alpha_{ i}+\sum_{i^{\prime}\neq 1,i}e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}.\]

In particular, for any \(a\in[-1,1]\), define \(p_{a}\coloneqq\mathbb{P}_{\mathbf{u}_{1}}[\mathbf{u}_{1}^{\top}\mathbf{u}=a]\). Since \(\mathbf{u}_{1}\) and \(\mathbf{u}_{i}\) are i.i.d., we have \(\mathbb{P}_{\mathbf{u}_{1},\mathbf{u}_{i}}[\mathbf{u}_{1}^{\top}\mathbf{u}=a, \mathbf{u}_{i}^{\top}\mathbf{u}=b]=\mathbb{P}_{\mathbf{u}_{1},\mathbf{u}_{i}}[ \mathbf{u}_{1}^{\top}\mathbf{u}=b,\mathbf{u}_{i}^{\top}\mathbf{u}=a]=p_{a}p_{b}\) for any \((a,b)\in[-1,1]^{2}\) Thus, by the law of total expectation we have

\[S_{i,j} =\alpha_{i}\alpha_{j}\mathbb{E}_{\bar{U}_{1,i}}\left[\int_{-1}^{ 1}\int_{-1}^{1}f_{a,b}(\bar{U}_{1,i})p_{a}p_{b}\ da\ db\right]\] \[=\frac{\alpha_{i}\alpha_{j}}{2}\mathbb{E}_{\bar{U}_{1,i}}\left[ \int_{-1}^{1}\int_{-1}^{1}(f_{a,b}(\bar{U}_{1,i})+f_{b,a}(\bar{U}_{1,i}))p_{a}p_{ b}\ da\ db\right]\] (43)

Next we show that for any instance of \(a,b\) and \(\bar{U}_{1,i}\), \(f_{a,b}(\bar{U}_{1,i})+f_{b,a}(\bar{U}_{1,i})\) is positive. We have:

\[f_{a,b}(\bar{U}_{1,i})+f_{b,a}(\bar{U}_{1,i})\] \[=(a-b)(\mathbf{u}_{j}^{\top}\mathbf{u}-1)e^{c(a+b+\mathbf{u}_{j}^{ \top}\mathbf{u})}\] \[\qquad\times\left(\frac{1}{(e^{ca}\alpha_{1}+e^{cb}\alpha_{i}+\sum_ {i^{\prime}\neq 1,i}e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}}) ^{3}}-\frac{1}{(e^{cb}\alpha_{1}+e^{ca}\alpha_{i}+\sum_{i^{\prime}\neq 1,i}e^{\mathbf{cu}_{i^{\prime}}^{\top} \mathbf{u}}\alpha_{i^{\prime}})^{3}}\right)\] \[\geq 0\]

with equality only if \(a=b\) or \(\mathbf{u}_{j}=\mathbf{u}\), since \(\mathbf{u}_{j}^{\top}\mathbf{u}\leq 1\) with equality only if \(\mathbf{u}_{j}=\mathbf{u}\), and

\[a>b\iff(e^{ca}\alpha_{1}+e^{cb}\alpha_{i}+\sum_{i^{\prime}\neq 1,i}e^{\mathbf{cu}_{i^{ \prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}>(e^{cb}\alpha_{1}+e^{ca} \alpha_{i}+\sum_{i^{\prime}\neq 1,i}e^{\mathbf{cu}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime }})^{3}\] (44)

due to \(\alpha_{1}>\alpha_{i}\) and \(\alpha_{i^{\prime}}>0\) for all \(i^{\prime}\). So we have \(S_{i,j}>0\).

Next we analyze \(S_{i,1}+S_{i,i}\). In these cases we cannot immediately drop the components of \(\mathbf{u}_{1}\) and \(\mathbf{u}_{i}\) that are perpendicular to \(\mathbf{u}\). We have:

\[S_{i,1}+S_{i,i}=\alpha_{i}\alpha_{1}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\\[\qquad+\alpha_{i}^{2}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime} \in[n]}}\left[\frac{(\mathbf{u}_{1}-\mathbf{u}_{i})^{\top}(\mathbf{u}_{i}- \mathbf{u})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{i}^{\top}\mathbf{u })}e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{2}u_{i}^{\top}\mathbf{u})}}{( \sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^ {\prime}})^{3}}\right]\] \[\qquad+\alpha_{i}^{2}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{(\mathbf{u}_{1}-\mathbf{u}_{i})^{\top}(\mathbf{u}_ {i}-\mathbf{u})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{2}u_{i}^{\top} \mathbf{u})}}{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{\prime}}^{\top} \mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[\qquad+\alpha_{i}^{2}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{(\mathbf{u}_{1}^{\top}\mathbf{u}_{1}-\mathbf{u}_{1}^ {\top}\mathbf{u}+\mathbf{u}_{i}^{\top}\mathbf{u})e^{c(\mathbf{u}_{1}^{\top} \mathbf{u}+\mathbf{2}u_{i}^{\top}\mathbf{u})}}{(\sum_{i^{\prime}=1}^{n}e^{c \mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[\qquad+\alpha_{i}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{(\mathbf{u}_{i}^{\top}\mathbf{u}_{1}-1-\mathbf{u}_{1 }^{\top}\mathbf{u}+\mathbf{u}_{i}^{\top}\mathbf{u})}{(\sum_{i^{\prime}=1}^{n} e^{c\mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[\qquad+\alpha_{i}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{1}^{ \top}\mathbf{u})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{i}^{\top} \mathbf{u})}(e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{1}+e^{c\mathbf{u}_{1 }^{\top}\mathbf{u}}\alpha_{i})}{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{ \prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\]

Now we can split \(\mathbf{u}_{i}^{\top}\mathbf{u}_{1}\) into the product of the components of \(\mathbf{u}_{i}\), \(\mathbf{u}_{1}\) in the direction \(\mathbf{u}\) and the product of their components in the perpendicular subspace as before. Doing so yields

\[S_{i,1}+S_{i,i} =\alpha_{i}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime} \in[n]}}\left[\frac{(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{1}^{\top} \mathbf{u})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{1}^{\top}\mathbf{u })}(e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{1}+e^{c\mathbf{u}_{1}^{\top} \mathbf{u}}\alpha_{i})}{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{\prime}}^{ \top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[\qquad+\alpha_{i}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{(1-\mathbf{u}_{i}^{\top}\mathbf{u}\mathbf{u}^{\top} \mathbf{u}_{1})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{1}^{\top} \mathbf{u})}(e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{1}-e^{c\mathbf{u}_{1 }^{\top}\mathbf{u}}\alpha_{i})}{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{ \prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[\qquad-\alpha_{i}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{\mathbf{u}_{i}^{\top}(\mathbf{I}_{k}-\mathbf{u} \mathbf{u}^{\top})\mathbf{u}_{1}e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u} _{1}^{\top}\mathbf{u})}(e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{1}-e^{c \mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{i})}{(\sum_{i^{\prime}=1}^{n}e^{c \mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[=\alpha_{i}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime}\in[n ]}}\left[\frac{(\mathbf{u}_{i}^{\top}\mathbf{u}-\mathbf{u}_{1}^{\top} \mathbf{u})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{1}^{\top}\mathbf{u })}(e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{1}+e^{c\mathbf{u}_{1}^{\top} \mathbf{u}}\alpha_{i})}{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{\prime}}^{ \top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] \[\qquad+\alpha_{i}\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{ \prime}\in[n]}}\left[\frac{(1-\mathbf{u}_{i}^{\top}\mathbf{u}\mathbf{u}^{\top} \mathbf{u}_{1})e^{c(\mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{1}^{\top} \mathbf{u})}(e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{1}-e^{c\mathbf{u}_{1 }^{\top}\mathbf{u}}\alpha_{i})}{(\sum_{i^{\prime}=1}^{n}e^{c\mathbf{u}_{i^{ \prime}}^{\top}\mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\] (45)

We argue similarly as in the previous case, except that here we must include additional terms.

\[S_{i,1}+S_{i,i}\] \[=\frac{\alpha_{i}}{2}\mathbb{E}_{\bar{U}_{1,i}}\left[\int_{-1}^{1} \int_{-1}^{1}(g_{a,b}(\bar{U}_{1,i})+g_{b,a}(\bar{U}_{1,i}))p_{a}p_{b}\ da\ db\right]\] \[=\frac{\alpha_{i}}{2}\mathbb{E}_{\bar{U}_{1,i}}\left[\int_{-1}^{1} \int_{-1}^{1}G_{a,b}(\bar{U}_{1,i})p_{a}p_{b}\ da\ db\right]\] (46)

where

\[G_{a,b}(\bar{U}_{1,i})\coloneqq g_{a,b}(\bar{U}_{1,i})+g_{b,a}(\bar{U}_{1,i})\] (47)

We show that for any \((a,b)\in[-1,1]^{2}\) and any \(\bar{U}_{1,i}\), \(G_{a,b}(\bar{U}_{1,i})\) is positive, which implies that \(S_{i,1}+S_{i,i}\) is positive by (46).

First, note that if \(b=a\) for any \(a\in[-1,1]\) and \(\bar{U}_{1,i}\), we have

\[g_{a,a}(\bar{U}_{1,i})=\mathbb{E}_{\{\mathbf{u}_{i^{\prime}}\}_{i^{\prime}\in[n]}} \left[\frac{(1-a^{2})e^{3ca}(\alpha_{1}-\alpha_{i})}{((\alpha_{1}+\alpha_{i})e^{ c\alpha_{i}}+\sum_{i^{\prime}\in[n]\setminus\{1,i\}}e^{c\mathbf{u}_{i^{\prime}}^{\top} \mathbf{u}}\alpha_{i^{\prime}})^{3}}\right]\geq 0\] (48)since each term inside the expectation is nonnegative, as \(a^{2}\leq 1\) and \(\alpha_{1}>\alpha_{i}\). Note that this implies \(G_{a,b}\geq 0\) when \(a=b\), so WLOG we consider \(b\neq a\) for the remainder of the proof. Now we focus on showing (61). Throughout, we will make use of the notation

\[d_{a,b}\coloneqq e^{ca}\alpha_{1}+e^{cb}\alpha_{i}+\sum_{i^{\prime}\in[n] \setminus\{1,i\}}e^{\alpha\mathbf{u}_{i^{\prime}}^{\top}\mathbf{u}}\alpha_{ i^{\prime}}\] (49)

which represents the cube root of the denominator in all terms when \(\mathbf{u}_{1}^{\top}\mathbf{u}=a\) and \(\mathbf{u}_{i}^{\top}\mathbf{u}=b\), and

\[\gamma_{a,b}\coloneqq 1-ab+a-b.\]

Using this notation, we can rewrite

\[g_{a,b}(\bar{U}_{1,i})=e^{c(a+b)}\frac{e^{ca}\gamma_{b,a}\alpha_{1}-e^{cb} \gamma_{a,b}\alpha_{i}}{d_{a,b}^{3}}\] (50)

Therefore,

\[g_{a,b}(\bar{U}_{1,i})+g_{b,a}(\bar{U}_{1,i})\] \[=e^{c(a+b)}\frac{e^{ca}\gamma_{b,a}\alpha_{1}-e^{cb}\gamma_{a,b} \alpha_{i}}{d_{a,b}^{3}}+e^{c(a+b)}\frac{e^{cb}\gamma_{a,b}\alpha_{1}-e^{ca} \gamma_{b,a}\alpha_{i}}{d_{b,a}^{3}}\] \[=e^{c(a+b)}d_{a,b}^{-3}d_{b,a}^{-3}\Big{(}\alpha_{1}\left(e^{ca} \gamma_{b,a}d_{b,a}^{3}+e^{cb}\gamma_{a,b}d_{a,b}^{3}\right)-\alpha_{i}\left(e ^{ca}\gamma_{b,a}d_{a,b}^{3}+e^{cb}\gamma_{a,b}d_{b,a}^{3}\right)\Big{)}\]

Note that \(e^{c(a+b)}d_{a,b}^{-3}d_{b,a}^{-3}>0\), so it remains to show that the term inside the parentheses is positive. This term can be rearranged as:

\[\alpha_{1}\left(e^{ca}\gamma_{b,a}d_{b,a}^{3}+e^{cb}\gamma_{a,b}d _{a,b}^{3}\right)-\alpha_{i}\left(e^{ca}\gamma_{b,a}d_{a,b}^{3}+e^{cb}\gamma_{ a,b}d_{b,a}^{3}\right)\] \[=(\alpha_{1}-\alpha_{i})\left(e^{ca}\gamma_{b,a}d_{b,a}^{3}+e^{cb }\gamma_{a,b}d_{a,b}^{3}\right)\] \[\quad+\alpha_{i}\left(e^{ca}\gamma_{b,a}d_{b,a}^{3}+e^{cb}\gamma _{a,b}d_{a,b}^{3}-e^{ca}\gamma_{b,a}d_{a,b}^{3}-e^{cb}\gamma_{a,b}d_{b,a}^{3}\right)\] \[=\underbrace{(\alpha_{1}-\alpha_{i})\left(e^{ca}\gamma_{b,a}d_{b, a}^{3}+e^{cb}\gamma_{a,b}d_{a,b}^{3}\right)}_{=:T_{1}}+\underbrace{\alpha_{i} \left(d_{b,a}^{3}-d_{a,b}^{3}\right)\left(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a, b}\right)}_{=:T_{2}}\] (51)

First we show that \(T_{1}\) is positive by analyzing \(\gamma_{a,b}\) and \(\gamma_{b,a}\). For any \((a,b)\in[-1,1]^{2}\) such that \(a\neq b\),

\[\frac{\partial}{\partial b}(\gamma_{a,b})=\frac{\partial}{\partial b}(1-ab+a-b )=-1-a\leq 0\] (52)

with equality holding if and only if \(a=-1\). If \(a=-1\), we have \(\gamma_{a,b}=1+b-1-b=0\) for all \(b\in[-1,1]\). Otherwise, (52) shows that \(\gamma_{a,b}\) is strictly decreasing with \(b\), so it is minimized over \(b\in[-1,1]\) at \(b=1\). When \(b=1\), we have \(\gamma_{a,b}=1-a+a-1=0\) for all \(a\). So, \(\gamma_{a,b}\geq 0\) with equality holding if and only if \(a=-1\) or \(b=1\). Note that by symmetry, this implies \(\gamma_{b,a}\geq 0\) with equality holding if and only if \(a=1\) or \(b=-1\). So, we can have both \(\gamma_{a,b}=0\) and \(\gamma_{b,a}=0\) if and only if \(a=b=-1\) or \(a=b=1\). However, we have \(a\neq b\), so at least one of \(\gamma_{a,b}\) and \(\gamma_{b,a}\) are strictly positive, and \(T_{1}\) is strictly positive (using also that \(\alpha_{1}>\alpha_{i}\)).

We next show that \(T_{2}\) is positive. Observe that

\[d_{b,a}^{3}-d_{a,b}^{3}>0\iff b>a\] (53)

since \(\alpha_{1}>\alpha_{i},\) so it remains to show

\[b>a\iff e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}>0.\] (54)

where

\[e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}=e^{ca}(1-ab-a+b)-e^{cb}(1-ab+a-b).\] (55)

We first show the forward direction, namely \(b>a\implies e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}>0\).

Note that if \(b=a\), \(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}=0\). So, if we can show that for any fixed \(a\), \(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}\) is increasing with \(b\) as long as \(b\geq a\), then we will have \(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}>0\) for \(b>a\). To show \(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}\) is increasing, we take its partial derivative with respect to \(b\):

\[\frac{\partial}{\partial b}\left(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}\right) =e^{ca}(1-a)+e^{cb}(1+a+cb-ca-c+cab)\] (56)

We would like to show that the RHS of (56) is nonnegative. To do so, we show that its partial derivative with respect to \(a\) is positive, so it achieves minimum value at \(a=-1\), at which point the value is positive. We have:

\[\frac{\partial}{\partial a}\left(\frac{\partial}{\partial b} \left(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}\right)\right) =e^{ca}(c-ca-1)+e^{cb}(1-c+cb)\] \[=q(b)-q(a)\] (57)

where \(q(x)\coloneqq e^{cx}(1+cx-c)\). Note that \(q(x)\) is monotonically increasing in \(x\in[-1,1]\); to see this, observe that

\[\frac{\partial}{\partial x}q(x)=e^{cx}(1+cx-c)c+e^{cx}c=e^{cx}(2+cx-c)c\geq 0\] (58)

where the inequality follows since \(c\in(0,2]\) and \(x\in[-1,1]\). Therefore, since \(b>a\), we have \(q(b)-q(a)\geq 0\) and \(\frac{\partial}{\partial a}\left(\frac{\partial}{\partial b}\left(e^{ca} \gamma_{b,a}-e^{cb}\gamma_{a,b}\right)\right)\geq 0\) from (57). As a result, \(\frac{\partial}{\partial b}\left(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}\right)\) achieves minimum value at \(a=-1\). At this point, using (56) we have

\[\frac{\partial}{\partial b}\left(e^{ca}\gamma_{b,a}-e^{cb}\gamma_ {a,b}\right) =2e^{-c}+e^{cb}(cb+c-c-cb)\] \[=2e^{-c}\] \[>0\]

This implies that the minimum value of \(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}\) over \(b\in[a,1]\) is achieved at \(b=a\), and we know this value is zero, so we have that \(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}>0\) when \(b-a\).

To show the backward direction of (54), namely \(e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}>0\implies b>a\), note that the converse, namely \(a>b\implies e^{ca}\gamma_{b,a}-e^{cb}\gamma_{a,b}<0\), follows by the same argument as above with \(a\) and \(b\) swapped. Therefore, we have \(T_{2}>0\) as desired. 

**Lemma H.4**.: _Consider any \(\bm{\alpha}\coloneqq[\alpha_{1},\alpha_{2}]\) such that \(\alpha_{1}>\alpha_{2}>0\). Further, let \(c\in(0,1]\). Define_

\[H_{\text{noise}}(\mathbf{u},\bm{\alpha})\coloneqq\mathbb{E}_{\mathbf{u}_{1}, \mathbf{u}_{2}}\left[\frac{e^{2cu_{1}^{\top}}\mathbf{u}\alpha_{1}^{2}+e^{2cu_ {2}^{\top}}\mathbf{u}\alpha_{2}^{2}}{(e^{cu_{1}^{\top}}\mathbf{u}\alpha_{1}+ e^{cu_{2}^{\top}}\mathbf{u}\alpha_{2})^{2}}\right].\]

_Then_

\[\frac{\partial H_{\text{noise}}(\mathbf{u},\bm{\alpha})}{\partial\alpha_{1}}>0\]

Proof.: We have

\[H_{\text{noise}}(\mathbf{u},\bm{\alpha})\coloneqq\mathbb{E}_{\mathbf{u}_{1}, \mathbf{u}_{2}}\left[\frac{e^{2cu_{1}^{\top}}\mathbf{u}\alpha_{1}^{2}+e^{2cu_ {2}^{\top}}\mathbf{u}\alpha_{2}^{2}}{(e^{cu_{1}^{\top}}\mathbf{u}\alpha_{1}+ e^{cu_{2}^{\top}}\mathbf{u}\alpha_{2})^{2}}\right]\]

Since \(n=2\), we have

\[\frac{\partial H_{\text{noise}}(\mathbf{u},\bm{\alpha})}{ \partial\alpha_{1}} =\mathbb{E}_{\mathbf{u}_{1},\mathbf{u}_{2}}\left[\frac{\partial}{ \partial\alpha_{1}}\frac{e^{2cu_{1}^{\top}}\mathbf{u}\alpha_{1}^{2}+e^{2cu_ {2}^{\top}}\mathbf{u}\alpha_{2}^{2}}{(e^{cu_{1}^{\top}}\mathbf{u}\alpha_{1}+ e^{cu_{2}^{\top}}\mathbf{u}\alpha_{2})^{2}}\right]\] \[=\mathbb{E}_{\mathbf{u}_{1},\mathbf{u}_{2}}\left[\frac{2e^{2cu_{1 }^{\top}}\mathbf{u}\alpha_{1}(e^{cu_{1}^{\top}}\mathbf{u}\alpha_{1}+e^{cu_{2 }^{\top}}\mathbf{u}\alpha_{2})^{2}}{(e^{cu_{1}^{\top}}\mathbf{u}\alpha_{1}+e^{ cu_{2}^{\top}}\mathbf{u}\alpha_{2})^{4}}\right]\] \[\quad-\mathbb{E}_{\mathbf{u}_{1},\mathbf{u}_{2}}\left[\frac{2(e^{ cu_{1}^{\top}}\mathbf{u}\alpha_{1}+e^{cu_{2}^{\top}}\mathbf{u}\alpha_{2})e^{cu_{1}^{ \top}}\mathbf{u}(e^{2cu_{1}^{\top}}\mathbf{u}\alpha_{1}^{2}+e^{2cu_{1}^{\top}} \mathbf{u}\alpha_{2}^{2})}{(e^{cu_{1}^{\top}}\mathbf{u}\alpha_{1}+e^{cu_{2}^{ \top}}\mathbf{u}\alpha_{2})^{4}}\right]\]\[=2\alpha_{2}\mathbb{E}_{\mathbf{u}_{1},\mathbf{u}_{2}}\left[\frac{e^{c( \mathbf{u}_{1}^{\top}\mathbf{u}+\mathbf{u}_{2}^{\top}\mathbf{u})}(e^{c\mathbf{u}_ {1}^{\top}\mathbf{u}}\alpha_{1}-e^{c\mathbf{u}_{2}^{\top}\mathbf{u}}\alpha_{2})}{ (e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha_{1}+e^{c\mathbf{u}_{2}^{\top} \mathbf{u}}\alpha_{2})^{3}}\right]\]

Define \(N\coloneqq\mathbb{E}_{\mathbf{u}_{1},\mathbf{u}_{2}}\left[\frac{e^{c(\mathbf{u} _{1}^{\top}u_{1}^{\top}\mathbf{u})}(e^{c\mathbf{u}_{1}^{\top}\mathbf{u}}\alpha _{1}-e^{c\mathbf{u}_{2}^{\top}\mathbf{u}}\alpha_{2})}{(e^{c\mathbf{u}_{1}^{ \top}\mathbf{u}}\alpha_{1}+e^{c\mathbf{u}_{2}^{\top}\mathbf{u}}\alpha_{2})^{ 3}}\right]\), and

\[d_{a,b} \coloneqq e^{ca}\alpha_{1}+e^{cb}\alpha_{2}\] \[h_{a,b} \coloneqq e^{c(a+b)}\frac{e^{ca}\alpha_{1}-e^{cb}\alpha_{i}}{d_{ a,b}^{3}},\]

Now, we have

\[N =\int_{-1}^{1}\int_{-1}^{1}h_{a,b}p_{a}p_{b}\ da\ db\] \[=\frac{1}{2}\int_{-1}^{1}\int_{-1}^{1}(h_{a,b}+h_{b,a})p_{a}p_{b} \ da\ db\] \[=\frac{1}{2}\int_{-1}^{1}\int_{-1}^{1}(h_{a,b}+h_{b,a})p_{a}p_{b} \chi\{a\neq b\}\ da\ db+\frac{1}{2}\int_{-1}^{1}\int_{-1}^{1}(h_{a,b}+h_{b,a})p _{a}p_{b}\chi\{a=b\}\ da\ db\] \[=\frac{1}{2}\int_{-1}^{1}\int_{-1}^{1}(h_{a,b}+h_{b,a})p_{a}p_{b} \chi\{a\neq b\}\ da\ db+\frac{1}{2}\int_{-1}^{1}h_{a,a}p_{a}^{2}\ da+\frac{1}{2 }\int_{-1}^{1}h_{b,b}p_{b}^{2}\ db\] \[=\frac{1}{2}\int_{-1}^{1}\int_{-1}^{1}(h_{a,b}+h_{b,a})p_{a}p_{b} \chi\{a\neq b\}\ da\ db+\frac{1}{4}\int_{-1}^{1}\int_{-1}^{1}h_{a,a}p_{a}^{2} \ da\ db\] \[\qquad+\frac{1}{4}\int_{-1}^{1}\int_{-1}^{1}h_{b,b}p_{b}^{2}\ da\ db\] \[=\frac{1}{2}\int_{-1}^{1}\int_{-1}^{1}(h_{a,b}+h_{b,a})p_{a}p_{b} \chi\{a\neq b\}\ da\ db+\frac{1}{4}\int_{-1}^{1}\int_{-1}^{1}(h_{a,a}p_{a}^{2} +h_{b,b}p_{b}^{2})\ da\ db\] \[=\frac{1}{2}\int_{-1}^{1}\int_{-1}^{1}H_{a,b}\ da\ db\] (59)

where

\[H_{a,b}\coloneqq p_{a}p_{b}(h_{a,b}+h_{b,a})+\frac{p_{a}^{2}}{2}h_{a,a}+ \frac{p_{b}^{2}}{2}h_{b,b}\] (60)

We will show that for any \((a,b)\in[-1,1]^{2}\) and \((p_{a},p_{b})\in[0,1]^{2}\), \(H_{a,b}\) is positive, which implies that \(N_{i}\) is positive by (59). To do this, assuming \(h_{a,a}\) is nonnegative for any \(a\), it is sufficient to show

\[\tilde{H}_{a,b}\coloneqq h_{a,b}+h_{b,a}+\sqrt{h_{a,a}h_{b,b}}>0,\] (61)

since this implies \(h_{a,b}+h_{b,a}>-\sqrt{h_{a,a}h_{b,b}}\) and thus, from (60),

\[H_{a,b} >-p_{a}p_{b}\sqrt{h_{a,a}h_{b,b}}+\frac{p_{a}^{2}}{2}h_{a,a}+ \frac{p_{b}^{2}}{2}h_{b,b}\] \[=\left(p_{a}\sqrt{\frac{h_{a,a}}{2}}-p_{b}\sqrt{\frac{h_{b,b}}{2}} \right)^{2}\] \[\geq 0\] (62)

Before showing (61), we need to confirm that \(h_{a,a}\) is not negative for all \(a\in[-1,1]\). We have

\[h_{a,a}=\frac{e^{3ca}(\alpha_{1}-\alpha_{2})}{d_{a,a}^{3}}\geq 0\] (63)since each term inside the expectation is nonnegative, as \(\alpha_{1}>\alpha_{2}\). Note that this implies \(H_{a,b}\geq 0\) when \(a=b\), so WLOG we consider \(a>b\) for the remainder of the proof.

Note that

\[h_{a,a}h_{b,b}=\frac{e^{3c(a+b)}(\alpha_{1}-\alpha_{i})^{2}}{e^{3c(a+b)}(\alpha_ {1}+\alpha_{2})^{6}}=\frac{(\alpha_{1}-\alpha_{2})^{2}}{(\alpha_{1}+\alpha_{2} )^{6}}\] (64)

Using this, we have

\[\tilde{H}_{a,b} =h_{a,b}+h_{b,a}+\sqrt{h_{a,a}h_{b,b}}\] \[=\frac{e^{2ca+cb}\alpha_{1}-e^{2cb+ca}\alpha_{2}}{d_{a,b}^{3}}+ \frac{e^{2cb+ca}\alpha_{1}-e^{2ca+cb}\alpha_{2}}{d_{b,a}^{3}}+\frac{\alpha_{1} -\alpha_{2}}{(\alpha_{1}+\alpha_{2})^{3}}\] \[=d_{a,b}^{-3}d_{b,a}^{-3}e^{(c+b)}(\alpha_{1}+\alpha_{2})^{3}\] \[\qquad\times\Big{(}(\underbrace{(e^{ca}\alpha_{1}-e^{cb}\alpha_ {2})d_{b,a}^{3}(\alpha_{1}+\alpha_{2})^{3}+(e^{cb}\alpha_{1}-e^{ca}\alpha_{2}) d_{a,b}^{3}(\alpha_{1}+\alpha_{2})^{3}}_{=:P}\] \[\qquad\qquad\underbrace{+e^{-c(a+b)}d_{a,b}^{3}d_{b,a}^{3}(\alpha _{1}-\alpha_{2})}_{=:P}\Big{)}\] (65)

To show that \(\tilde{H}_{a,b}\) is positive, we need to show that \(P\) is positive. Without loss of generality we can consider \(\alpha_{1}=1\) and \(\alpha_{2}\in(0,1)\) by dividing the numerator and denominator of \(H_{\text{noise}}\) by \(\alpha_{1}^{2}\). Thus, for the remainder of the proof we treat \(\alpha_{1}\) as 1 and write \(\alpha:=\alpha_{2}\) for ease of notation. Using this notation we can expand \(P\) as follows:

\[P =(e^{ca}-e^{cb}\alpha)d_{b,a}^{3}(1+\alpha)^{3}+(e^{cb}-e^{ca} \alpha)d_{a,b}^{3}(1+\alpha)^{3}+e^{-c(a+b)}d_{a,b}^{3}d_{b,a}^{3}(1-\alpha)\] \[=(e^{ca}-e^{cb}\alpha)(e^{cb}+e^{ca}\alpha)^{3}(1+\alpha)^{3}+(e^ {cb}-e^{ca}\alpha)(e^{ca}+e^{cb}\alpha)^{3}(1+\alpha)^{3}\] \[\qquad+e^{-c(a+b)}(e^{ca}+e^{cb}\alpha)^{3}(e^{cb}+e^{ca}\alpha)^ {3}(1-\alpha)\] \[=(e^{5ca-cb}+e^{5cb-ca})\left(\alpha^{3}(1-\alpha)\right)\] \[\qquad+(e^{4ca}+e^{4cb})\left(-\alpha-5\alpha^{3}+5\alpha^{4}+ \alpha^{6}\right)\] \[\qquad+(e^{3ca+cb}+e^{3cb+ca})\left(1+6\alpha+10\alpha^{3}-10 \alpha^{4}-6\alpha^{6}-\alpha^{7}\right)\] \[\qquad+e^{2ca+2cb}\left(1+5\alpha+27\alpha^{2}+3\alpha^{3}-3 \alpha^{4}-27\alpha^{5}-5\alpha^{6}-\alpha^{7}\right)\] \[=(1-\alpha)\times\left((e^{5ca-cb}+e^{5cb-ca})\alpha^{3}\right.\] \[\qquad+(e^{4ca}+e^{4cb})\left(-\alpha-\alpha^{2}-6\alpha^{3}- \alpha^{4}-\alpha^{5}\right)\] \[\qquad+(e^{3ca+cb}+e^{3cb+ca})\left(1+7\alpha+7\alpha^{2}+17 \alpha^{3}+7\alpha^{4}+7\alpha^{5}+\alpha^{6}\right)\] \[\qquad+e^{2ca+2cb}\left(1+6\alpha+33\alpha^{2}+36\alpha^{3}+33 \alpha^{4}+6\alpha^{5}+\alpha^{6}\right)\Bigg{)}\]

Recall that \(1-\alpha>0\), so we need to show that the sum of the remaining terms is positive. These terms can be written as a polynomial in \(y\coloneqq e^{c(a-b)}\) as follows:

\[P(1-\alpha)^{-1}e^{ca-5cb} =y^{6}\alpha^{3}\] \[\qquad+y^{5}\left(-\alpha-\alpha^{2}-6\alpha^{3}-\alpha^{4}- \alpha^{5}\right)\] \[\qquad+y^{4}\left(1+7\alpha+7\alpha^{2}+17\alpha^{3}+7\alpha^{4} +7\alpha^{5}+\alpha^{6}\right)\] \[\qquad+y^{3}\left(1+6\alpha+33\alpha^{2}+36\alpha^{3}+33\alpha^{ 4}+6\alpha^{5}+\alpha^{6}\right)\] \[\qquad+y^{2}\left(1+7\alpha+7\alpha^{2}+17\alpha^{3}+7\alpha^{4} +7\alpha^{5}+\alpha^{6}\right)\] \[\qquad+y\left(-\alpha-\alpha^{2}-6\alpha^{3}-\alpha^{4}-\alpha^{5}\right)\] \[\qquad+\alpha^{3}\] (66)We know that \(y^{6}>y^{5}>\cdots>1\) since \(a>b\). We also have that \(\alpha<1\). Using these facts we next show that the sum of the third and smaller-order terms in the RHS of (66) is positive.

\[(*):=y^{3}\left(1+6\alpha+33\alpha^{2}+36\alpha^{3}+33\alpha^{4}+6 \alpha^{5}+\alpha^{6}\right)\] \[\qquad+y^{2}\left(1+7\alpha+7\alpha^{2}+17\alpha^{3}+7\alpha^{4}+ 7\alpha^{5}+\alpha^{6}\right)\] \[\qquad+y\left(-\alpha-\alpha^{2}-6\alpha^{3}-\alpha^{4}-\alpha^{ 5}\right)\] \[\qquad+\alpha^{3}\] \[>y\left(1+6\alpha+33\alpha^{2}+36\alpha^{3}+33\alpha^{4}+6 \alpha^{5}+\alpha^{6}\right)\] \[\qquad+y\left(1+7\alpha+7\alpha^{2}+17\alpha^{3}+7\alpha^{4}+7 \alpha^{5}+\alpha^{6}\right)\] \[\qquad+y\left(-\alpha-\alpha^{2}-6\alpha^{3}-\alpha^{4}-\alpha^ {5}\right)\] \[\qquad+\alpha^{3}\] \[>y\left(2+12\alpha+39\alpha^{2}+47\alpha^{3}+39\alpha^{4}+12 \alpha^{5}+1\alpha^{6}\right)\] \[>0\]

Next we show that the sum of the sixth-, fifth-, and fourth-order terms is positive. Let \(a_{6}\coloneqq\alpha^{3}\), \(a_{5}\coloneqq\alpha+\alpha^{2}+6\alpha^{3}+\alpha^{4}+\alpha^{5}\), and \(a_{4}\coloneqq 1+7\alpha+7\alpha^{2}+17\alpha^{3}+7\alpha^{4}+7\alpha^{5}+\alpha^{6}\), so the sum of the sixth-, fifth-, and fourth-order terms is \(y^{6}a_{6}-y^{5}a_{5}+y^{4}a_{4}\). Note that \(32a_{6}<a_{4}\) since \(\alpha<1\), and

\[a_{5}-4a_{6} =\alpha+\alpha^{2}+2\alpha^{3}+\alpha^{4}+\alpha^{5}\] \[=\frac{1}{7.5}\left(7.5\alpha+7.5\alpha^{2}+15\alpha^{3}+7.5 \alpha^{4}+7.5\alpha^{5}\right)\] \[<\frac{1}{7.5}\left(1+7\alpha+7\alpha^{2}+17\alpha^{3}+7\alpha^{4} +7\alpha^{5}+\alpha^{6}\right)\] \[=\frac{a_{4}}{7.5}\] (67)

thus \(a_{5}<\frac{a_{4}}{7.5}+4a_{6}.\) Also, \(y=e^{c(a-b)}\leq e^{2}<7.5\) since \(c\leq 1\). Therefore,

\[y^{6}a_{6}-y^{5}a_{5}+y^{4}a_{4} =y^{4}\left(y^{2}a_{6}-ya_{5}+a_{4}\right)\] \[>y^{4}\left(y^{2}a_{6}-4ya_{6}-y\frac{a_{4}}{7.5}+a_{4}\right)\] \[>y^{4}\left(y^{2}a_{6}-4ya_{6}+a_{4}\underbrace{\left(1-\frac{y}{ 7.5}\right)}_{>0\text{ since }y<7.5}\right)\] \[>y^{4}\left(y^{2}a_{6}-4ya_{6}+32a_{6}\left(1-\frac{y}{7.5}\right)\right)\] \[=y^{4}a_{6}\left(y^{2}-\frac{62}{7.5}y+32\right)\] \[>y^{4}a_{6}\left(-\frac{1}{4}\left(\frac{62}{7.5}\right)^{2}+32\right)\] (68) \[>0\] (69)

where (68) follows by minimizing the terms inside the parentheses over \(y\). Thus, we have \(\tilde{H}_{a,b}>0\), which completes the proof. 

Now we can finally prove Theorem 4.4. We prove a slightly stronger result, formally stated as follows.

**Theorem H.5**.: _Consider any \(\mathbf{B}\in\mathbb{O}^{d\times k}\) and the corresponding function class \(\mathcal{F}_{\mathbf{B}}^{\text{lin}}\) as defined in (4.2). Suppose tasks are drawn from \(D(\mathcal{F}_{\mathbf{B}}^{\text{lin}})\) and Assumption 4.3 holds. Recall the pretraining population loss:_

\[\mathcal{L}(\mathbf{M})=\mathbb{E}_{f,\{\mathbf{x}_{i}\}_{i\in[n+1]},\{e_{i}\}_ {i\in[n]}}\left[\left(\frac{\sum_{i=1}^{n}(f(\mathbf{x}_{i})-f(\mathbf{x}_{n+1 })+\epsilon_{i})e^{\mathbf{x}_{i}^{\top}\mathbf{M}\mathbf{x}_{n+1}}}{\sum_{i= 1}^{n}e^{\mathbf{x}_{i}^{\top}\mathbf{M}\mathbf{x}_{n+1}}}\right)^{2}\right].\] (70)

[MISSING_PAGE_EMPTY:50]

which implies that \(\tilde{c}=0\) is the unique argument that achieves the minimal value of \(\mathcal{L}(c_{p}\mathbf{B}\mathbf{B}^{\top}+\tilde{c}\mathbf{B}_{\perp}\mathbf{B }_{\perp}^{\top})\) over \(\tilde{c}\in\mathbb{R}\) (and this value is \(\mathbb{E}_{\mathbf{u}}\left[H(\mathbf{u},\boldsymbol{\alpha}^{*})\right]\)).

Proving \((i)\) is trivial as it can be easily checked that \(H(\mathbf{u},\boldsymbol{\alpha})=H(\mathbf{u},c^{\prime}\boldsymbol{\alpha})\) for all \(\mathbf{u}\in\mathbb{S}^{d-1}\), \(\boldsymbol{\alpha}\in\mathbb{R}_{+}^{n}\), and \(c^{\prime}\in\mathbb{R}_{+}\).

Proving \((ii)\) is more involved. Consider any \(\boldsymbol{\alpha}\neq c^{\prime}\boldsymbol{\alpha}^{*}\) for any \(c^{\prime}\in\mathbb{R}_{+}\). WLOG let \(1\in\arg\max_{i}\alpha_{i}\). We show that the partial derivative of \(H(\mathbf{u},\boldsymbol{\alpha})\) with respect to \(\alpha_{1}\) is strictly positive, which means that \(H(\mathbf{u},\boldsymbol{\alpha})\) can be reduced by reducing \(\alpha_{1}\) by some \(\epsilon>0\). We can repeat this argument, repeatedly reducing \(\max_{i}\alpha_{i}\) at each step and thereby reducing the loss, until we reach an \(\boldsymbol{\alpha}^{\prime}\) satisfying \(\boldsymbol{\alpha}^{\prime}=c^{\prime}\boldsymbol{\alpha}^{*}\). Since the loss is reduced at each step, we have that \(H(\mathbf{u},\boldsymbol{\alpha})>H(\mathbf{u},\boldsymbol{\alpha}^{*})\).

To show that the partial derivative of \(H(\mathbf{u},\boldsymbol{\alpha})\) with respect to \(\alpha_{1}\) is strictly positive, we decompose \(\frac{\partial H(\mathbf{u},\boldsymbol{\alpha})}{\partial\alpha_{1}}=\frac{ \partial H_{\text{signal}}(\mathbf{u},\boldsymbol{\alpha})}{\partial\alpha_{1} }+\frac{\partial H_{\text{noise}}(\mathbf{u},\boldsymbol{\alpha})}{\partial \alpha_{1}}\), where

\[H_{\text{signal}}(\mathbf{u},\boldsymbol{\alpha}) :=c_{a}^{2}c_{u}^{2}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}} \left[\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}(\mathbf{u}_{i}-\mathbf{u})^{\top}( \mathbf{u}_{j}-\mathbf{u})e^{c\mathbf{u}_{i}^{\top}\mathbf{u}+c\mathbf{u}_{j} ^{\top}\mathbf{u}}\alpha_{i}\alpha_{j}}{(\sum_{i=1}^{n}e^{c\mathbf{u}_{i}^{ \top}\mathbf{u}}\alpha_{i})^{2}}\right]\] \[H_{\text{noise}}(\mathbf{u},\boldsymbol{\alpha}) :=\sigma^{2}\mathbb{E}_{\{\mathbf{u}_{i}\}_{i\in[n]}}\left[\frac{ \sum_{i=1}^{n}e^{2c\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i}^{2}}{(\sum_{i=1 }^{n}e^{c\mathbf{u}_{i}^{\top}\mathbf{u}}\alpha_{i})^{2}}\right]\]

By Lemma H.3, we have \(\frac{\partial H_{\text{signal}}(\mathbf{u},\boldsymbol{\alpha})}{\partial \alpha_{1}}>0\). If \(\sigma=0\) we are done, otherwise we have \(n=2\) and \(\frac{\partial H_{\text{noise}}(\mathbf{u},\boldsymbol{\alpha})}{\partial \alpha_{1}}>0\) by Lemma H.4. This completes the proof. 

## Appendix I Additional Lemmas

**Lemma I.1**.: _Consider a continuous unimodal function \(f\). Then we have_

\[\sum_{i=0}^{\infty}f(i)-\max f\leq\int_{0}^{\infty}f(t)dt\leq\sum_{i=1}^{ \infty}f(i)+\max f\]

Proof.: Let \(T\) denote the point that achieves the maximum of \(f\). Then we know that \(f(t)\geq f(\lfloor t\rfloor)\) for \(t<T\), while \(f(t)\geq f(\lceil t\rceil)\) for \(t>T\). This means \(\int_{i-1}^{i}f(t)dt\leq f(i)\leq\int_{i}^{i+1}f(t)dt\) for \(t\leq\lfloor T\rfloor\) and \(\int_{i-1}^{i}f(t)dt\geq f(i)\geq\int_{i}^{i+1}f(t)dt\) for \(t\geq\lceil T\rceil\) So

\[\sum_{i=0}^{\infty}f(i) =\sum_{i=0}^{\lfloor T\rfloor}f(i)+\sum_{i=\lceil T\rceil}^{ \infty}f(i)\] \[\leq\sum_{i=0}^{\lfloor T\rfloor}\int_{i}^{i+1}f(t)dt+\sum_{ \lceil T\rceil}^{\infty}\int_{i-1}^{i}f(t)dt\] \[\leq\sum_{i=0}^{\infty}\int_{i}^{i+1}f(t)dt+\int_{\lfloor T \rfloor}^{\lceil T\rceil}f(t)dt\] \[\leq\int_{0}^{\infty}f(t)dt+\max f\]

Similarly we have

\[\sum_{i=1}^{\infty}f(i) =\sum_{i=1}^{\lfloor T\rfloor}f(i)+\sum_{i=\lceil T\rceil}^{ \infty}f(i)\] \[\leq\sum_{i=1}^{\lfloor T\rfloor}\int_{i-1}^{i}f(t)dt+\sum_{ \lceil T\rceil}^{\infty}\int_{i}^{i+1}f(t)dt\]\[\leq\sum_{i=1}^{\infty}\int_{i-1}^{i}f(t)dt-\int_{\lfloor T\rfloor}^{ \lceil T\rceil}f(t)dt\] \[\leq\int_{0}^{\infty}f(t)dt-\max f\]

**Lemma I.2**.: _If \(f\) and \(g\) are nonnegative measurable real functions, then_

\[\int f(x)g(x)dx\leq\int f^{*}(x)g^{*}(x)dx\]

_where \(f^{*},g^{*}\) are the symmetric decreasing rearrangements of \(f\) and \(g\)._

Proof.: Please see [66] or [67]. 

**Lemma I.3**.: _Suppose \(\{a_{i}\},\{b_{i}\}\) are sorted the same way, \(a_{i}>a_{j}\iff b_{i}>b_{j}\). Then we have_

\[\frac{\sum a_{i}^{2}}{\left(\sum a_{i}\right)^{2}}<\frac{\sum a_{i}^{2}b_{i}^{ 2}}{\left(\sum a_{i}b_{i}\right)^{2}}.\]

Proof.: Cross multiplying and expanding, we have

\[(\sum a_{i}^{2})(\sum a_{i}b_{i})^{2}<(\sum a_{i}^{2}b_{i}^{2})( \sum a_{i})^{2}\] \[\iff \sum_{i,j,k}a_{i}b_{i}a_{j}b_{j}a_{k}^{2}<\sum_{i,j,k}a_{i}^{2}b_ {i}^{2}a_{j}a_{k}\] \[\iff \frac{1}{3}\sum_{i,j,k}a_{i}b_{i}a_{j}b_{j}a_{k}^{2}+a_{j}b_{j}a_ {k}b_{k}a_{i}^{2}+a_{k}b_{k}a_{i}b_{i}a_{j}^{2}<\frac{1}{3}\sum_{i,j,k}a_{i}^{2 }b_{i}^{2}a_{j}a_{k}+a_{j}^{2}b_{j}^{2}a_{k}a_{i}+a_{k}^{2}b_{k}^{2}a_{i}a_{j}\] \[\iff \frac{1}{3}\sum_{i,j,k}a_{i}^{2}b_{i}^{2}a_{j}a_{k}+a_{j}^{2}b_{j} ^{2}a_{k}a_{i}+a_{k}^{2}b_{k}^{2}a_{i}a_{j}-(a_{i}b_{i}a_{j}b_{j}a_{k}^{2}+a_{ j}b_{j}a_{k}b_{k}a_{i}^{2}+a_{k}b_{k}a_{i}b_{i}a_{j}^{2})>0\] \[\iff \frac{1}{3}\sum_{i,j,k}a_{i}a_{j}a_{k}\left(a_{i}b_{i}^{2}+a_{j}b _{j}^{2}+a_{k}b_{k}^{2}-a_{i}b_{j}b_{k}-a_{j}b_{k}b_{i}-a_{k}b_{i}b_{j}\right)>0\]

The last of which follows from the rearrangement inequality [67]. 

## Appendix J Additional Experiments and Details

All experiments were run in Google Colab in a CPU runtime. We used a random seed of 0 in all cases. All training was executed in PyTorch with the Adam optimizer. We tuned learning rates in \(\{10^{-3},10^{-2},10^{-1}\}\) separately for linear and softmax attention, and we initialized \(\mathbf{M}_{K}\) and \(\mathbf{M}_{Q}\) by setting each to \(0.001\mathbf{I}_{d}\), and tie the weights of \(\mathbf{M}_{K}\) and \(\mathbf{M}_{Q}\) to speed up training.

**Figure 1.** The upper row depicts our functions, which increase in Lipschitzness from left to right. The black curve depicts the ground truth, while the gray dots depict the noisy training samples. The shaded region represents the attention window. The middle row depicts the attention weights for softmax and linear attention. We remark that the softmax is able to adapt to the Lipschitzness while linear is not. The bottom row depicts the ICL error as a function of the context length \(n\) for Linear and ReLU pretraining using Linear and Softmax attention. That is, at each iteration, a context is drawn from a non-linear regression (defined below) consisting of a randomly phase shifted cosine function. The ICL task is to predict the function value at a randomly chosen query on the unit circle. Each point in the plot depicts the ICL error of a pretrained attention unit (using softmax (blue) or linear (orange) activation) at the end of \(15000\) iterations with learning rate \(10^{-3}\). We use \(d=2\) and a distribution \(D(\mathcal{F}_{\nu,\text{hills}})\). Here we define

\[\mathcal{F}_{\nu,\text{hills}}=\{\nu\cos\left(\theta-b\right)\}\]

and a distribution \(D(\mathcal{F}_{\nu,\text{hills}})\) is induced by drawing \(b\) uniformly from \([-\pi,\pi]\). We use \(\nu=0,1.5,6\) for the left, middle and right plots in the bottom row, respectively.

**Figures 3, 4, 5.** In all cases, we use an exponentially decaying learning rate schedule with factor 0.999. In Figures 3 and 5 we use initial learning rate 0.1 and in Figure 4 we use an initial learning rate 0.01. Moreover, in all cases besides those with varying \(n\) in Figure 4, we compute gradients with respect to the ICL loss evaluated on \(N\coloneqq\lfloor\sqrt{n}\rfloor\) query samples per task (that is, each context input to the attention unit has \(n+N\) samples, of which \(n\) are labeled, and the other \(N\) labels are inferred). When \(n\) varies in Figure 4, we use \(N=1\). In Figure 5 we show smoothed test ICL errors with smoothing rate 0.01.

### Low-Rank Experiments

Due to our results in Section 3 showing that softmax attention can learn an appropriate attention window scale when pretrained on nonlinear tasks, we hypothesize that it can also learn the appropriate _directions_ during pretraining on nonlinear tasks. To test this, we consider tasks drawn from low-rank versions of affine, quadratic and cosine function classes, in particular: \(\mathcal{F}_{\mathbf{B}}^{\text{aff}}:=\{f:f(\bm{x})=\mathbf{a}^{\top} \mathbf{B}^{\top}\,\bm{x}+2,\mathbf{a}\in\mathbb{S}^{k-1}\}\), \(\mathcal{F}_{\mathbf{B}}^{2}:=\{f:f(\bm{x})=(\mathbf{a}^{\top}\mathbf{B}^{ \top}\,\bm{x})^{2},\mathbf{a}\in\mathbb{S}^{k-1}\}\) and \(\mathcal{F}_{\mathbf{B}}^{\text{cos}}:=\{f:f(\bm{x})=\cos(4\mathbf{a}^{\top} \mathbf{B}^{\top}\,\bm{x}),\mathbf{a}\in\mathbb{S}^{k-1}\}\). Each task distribution \(D(\mathcal{F}_{\mathbf{B}}^{\text{aff}}),D(\mathcal{F}_{\mathbf{B}}^{2}),D( \mathcal{F}_{\mathbf{B}}^{\text{cos}})\) is induced by drawing \(\mathbf{a}\sim\mathcal{U}^{k}\). We train \(\mathbf{M}_{K}\) and \(\mathbf{M}_{Q}\) with Adam with learning rate tuned separately for softmax and linear attention. We set \(d=10\), \(k=2\), \(n=50\), and \(\sigma=0.01\). We draw \(\{\bm{x}_{i}\}_{i=1}^{n+1}\) i.i.d. from a non-uniform distribution on \(\mathbb{S}^{d-1}\) for each task, and draw one task per training iteration. We draw \(\mathbf{B}\) randomly at the start of each trial, and repeat each trial 5 times and plots means and standard deviations over the 5 trials. We capture the extent to which the learned \(\mathbf{M}=\mathbf{M}_{K}^{\top}\mathbf{M}_{Q}\) recovers \(\text{col}(\mathbf{B})\) via the metric \(\rho(\mathbf{M},\mathbf{B}):=\frac{\|\mathbf{B}_{i}^{\top}\mathbf{M}\mathbf{B} _{i}\|_{2}}{\sigma_{\min}(\mathbf{B}^{\top}\mathbf{M}\mathbf{B})}\), where \(\sigma_{\min}(\mathbf{A})\) is the minimum singular value of \(\mathbf{A}\). For test error, we compute the average squared error on 500 random tasks drawn from the same distribution as the (pre)training tasks. Please see Appendix J for more details.

We randomly generate \(\mathbf{B}\) on each trial by first sampling each element of \(\hat{\mathbf{B}}\) i.i.d. from the standard normal distribution, then take its QR decomposition to obtain \(\mathbf{B}\). To draw the covariates, we draw a random matrix \(\tilde{\mathbf{J}}\in\mathbb{R}^{d\times d}\) by sampling each element i.i.d. from the standard normal distribution. Then, we compute \(\mathbf{J}=(\tilde{\mathbf{J}}^{\top}\tilde{\mathbf{J}})^{1/2}\). Then we draw \(\tilde{\bm{x}}_{i}\sim\mathcal{N}(\mathbf{0}_{d},\mathbf{I}_{d})\) and set \(\bm{x}_{i}=\frac{\mathbf{J}\tilde{\bm{x}}_{i}}{\|\mathbf{J}\tilde{\bm{x}}_{i} \|}\).

**Results.** Figure 9 shows that softmax attention recovers the low-rank structure when tasks are drawn from each of the three function classes, which leads to test error improving with the quality of the learned subspace. In contrast, linear attention does not learn any meaningful structure in these cases.

Figure 9: Representation learning error \((\rho(\mathbf{M},\mathbf{B}))\) and test ICL error (mean squared error) during pretraining softmax and linear attention on tasks from **Left:**\(\mathcal{F}_{\mathbf{B}}^{\text{aff}}\), **Center:**\(\mathcal{F}_{\mathbf{B}}^{2}\), and **Right:**\(\mathcal{F}_{\mathbf{B}}^{\text{cos}}\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract is consistent with our introduction. In the introduction, we point to the places in the paper in which we substantiate our claims. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provide a discussion in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Assumptions are specified before all theorem statements. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide details in Sections 3.2 and J. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Please see supplementary material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide details in Sections 3.2 and J. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Please see results in Sections 3.2 and J. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please see Section J. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: we have read the Code of Ethics and ensured conformity. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is primarily an analysis of an already existing algorithm. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines: [The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.