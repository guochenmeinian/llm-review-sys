# Fast Tree-Field Integrators: From Low Displacement

Rank to Topological Transformers

 Krzysztof Choromanski\({}^{1,2}\)

equal contribution

Arijit Sehanobish\({}^{3,}\)

equal contribution

Somnath Basu Roy Chowdhury\({}^{4,}\)

Han Lin\({}^{4,}\)

Avinava Dubey\({}^{5,}\)

Equal contribution

Tamas Sarlos\({}^{5}\)

Snigdha Chaturvedi\({}^{4}\)

\({}^{1}\) Google DeepMind, \({}^{2}\) Columbia University, \({}^{3}\) Independent, \({}^{4}\) UNC Chapel Hill, \({}^{5}\) Google Research.

###### Abstract

We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular _low displacement rank_) for integrating tensor fields defined on weighted trees. Several applications of the resulting _fast tree-field integrators_ (FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) _Topological Transformers_ (TTs) [15, 16] for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as **three** extra learnable parameters per Transformer layer, leading to **1.0-1.5%+** accuracy gains. Importantly, most of FTFIs are **exact** methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide **5.7-13x** speedups. We also provide an extensive theoretical analysis of our methods.

## 1 Introduction

Matrix-vector multiplication remains a key computational block of virtually all modern machine learning (ML) algorithms. For this reason, decades of research have been dedicated towards making this fundamental operation more efficient. One approach to achieve this goal is through efficient hardware design, e.g., using modern GPU and TPU accelerators [1, 14, 15]. The alternative method involves developing algorithms for efficient matrix-vector multiplication by leveraging either (1) sparse matrices [20, 17], or (2) structured dense matrices [21, 16]. These algorithms can be applied in modern neural network systems, where weights are pruned to encourage sparsity [1] or they can be parameterized with structured matrices [18].

In this work, we aim to accelerate multiplications with a large class of matrices, that we refer to as _\(f\)-distance matrices_, which play an important role in several ML algorithms. Consider a matrix \(\mathbf{M}_{f}^{\mathrm{G}}=[f(\mathrm{dist}(i,j))]_{i,j=1,\ldots,N}\in\mathbb{ R}^{N\times N}\), where \(\mathrm{dist}(i,j)\) stands for the shortest-path distance between the \(i\)-th and \(j\)-th vertex of an undirected graph \(\mathrm{G}=(\mathrm{V},\mathrm{E},\mathrm{W})\). Here \(\mathrm{V}=\{1,...,N\}\) stands for the set of vertices (nodes), \(\mathrm{E}\) denotes the set of edges, \(\mathrm{W}:\mathrm{E}\rightarrow\mathbb{R}_{+}\) maps them to their positive weights, and \(f:\mathbb{R}\rightarrow\mathbb{R}\). We call \(\mathbf{M}_{f}^{\mathrm{G}}\) a _\(f\)-distance matrix in \(\mathrm{G}\)_. Note that if \(f(x)\stackrel{{\mathrm{def}}}{{=}}x\), then \(\mathbf{M}_{f}^{\mathrm{G}}\) is the Shortest Path Kernel matrix.

The product \(\mathbf{M}_{f}^{\mathrm{G}}\mathbf{x}\) (where \(\mathbf{x}\in\mathbb{R}^{N}\)) represents a scalar field on \(\mathrm{V}\) obtained by discretely integrating the field defined by \(\mathbf{x}\). In this integration, a new field value at a vertex \(v\) is calculated by averaging the old field values at all vertices \(u\), weighted according to the function \(f(\mathrm{dist}(v,u))\). This integration canbe extended to general tensor fields by replacing vector \(\mathbf{x}\in\mathbb{R}^{N}\) with a tensor \(\mathbf{X}\in\mathbb{R}^{N\times d_{1}\times d_{2}\times\cdots}\):

\[\mathbf{M}_{f}^{\mathrm{G}}\mathbf{X}[i]=\sum_{j\in\mathrm{V}(\mathrm{G})}f( \operatorname{dist}(i,j))\mathbf{X}[j] \tag{1}\]

We refer to the above procedure as the \(f\)-integration of a field \(\mathbf{X}\) on \(\mathrm{G}\). We will use the terms _graph field integration_ (GFI) and _multiplication with \(f\)-distance matrices_ interchangeably throughout the paper. When the graph, \(\mathrm{G}\), is a tree, we call this procedure (Eq. 1) _tree field integration_. Next, we highlight several applications that rely on multiplications with \(f\)-distance matrices, \(\mathbf{M}_{f}^{\mathrm{G}}\).

1. **Interpolation on manifolds:** This task involves predicting unseen values on a manifold from a set of known values. For example, predicting the velocities of all points on a flag with known velocities for a few points (Praff et al., 2021). For a discretized manifold, the interpolated values can be obtained using a weighted average using graph field integration (Eq. 1).
2. **Optimal Transport (OT):** A popular method used to solve the entropic OT problem (Peyre and Cuturi, 2019) is the Sinkhorn algorithm (Eckstein and Nutz, 2022). Sinkhorn relies on multiplications with _cost matrices_, which are special cases of \(f\)-distance matrices for metric spaces induced by shortest-path distances in graphs. This can be efficiently solved using graph field integration.
3. **Topological Transformers (TTs):** Topological Transformers (Choromanski et al., 2022) are extensions of traditional Transformers (Vaswani et al., 2017) for graph inputs. TTs modify the 1-D relative positional encoding (RPE) using "mask matrices", which are \(f\)-distance matrices. We show how these matrices can be efficiently integrated into the attention mechanism (Sec. 4.4).

In the above applications, apart from the graph field integration step, the bottleneck lies in the process of explicitly materializing the \(f\)-distance matrix. Naively performing the integration in Eq 1 consists of two steps: **(a)** computing the \(f\)-distance matrix, \(\mathbf{M}_{f}^{\mathrm{G}}\), which requires \(O(N^{3})\) time in the worst case (which we call _preprocessing_), and **(b)** performing the multiplication takes \(O(N^{2})\) time. This is prohibitively expensive while using large graphs.

In this paper, we introduce a new class of fast polylog-linear algorithms for graph field integration that uses low displacement rank (LDR) matrices (Thomas et al., 2018; Chandrasekaran et al., 2018). To summarize, our primary contributions are given below:

1. We provide the first **exact** polylog-linear multiplication algorithms called **Fast Tree-Field Integrators** (FTFIs), for general weighted trees and a rich class of maps \(f\), including rational, trigonometric, exponential and exponentiated quadratic functions (Sec. 3.2).
2. We show how Fast Tree-Field Integrators can be applied to support fast computations on general graphs by approximating graph metrics with tree metrics (Sec. 4).
3. We show that FTFs are **5.7-10x** faster than baseline graph field integration methods for large-scale graphs (Sec. 4.1 and 4.2).
4. We showcase the efficacy of FTFs in several applications including graph classification (Sec. 4.2), interpolation on meshes (Sec. 4.2), and Topological Vision Transformers (TVTs) (Sec. 4.4). For TVTs, we propose new relative position encoding (RPE) masking mechanisms by introducing only **three** extra learnable parameters, which leads to **1.0-1.5%** accuracy gains. We provide an exhaustive evaluation on Vision Performers (**25** models on multiple datasets). Some of our best models use exponentiated quadratic functions \(f\), which has not been applied in this context before.

For completeness, we also propose approximate FTFI extensions via _Non-Uniform FFT_ (NU-FFT) (Kircheis et al., 2023) and random Fourier features (RFFs) (Rahimi and Recht, 2007) (Sec. A.2).

## 2 Related work

Efficient graph field integration (Eq. 1) has been studied by prior works for different classes of matrices. For example, Al-Mohy and Higham (2011) considered exponentiated adjacency matrix-vector multiplication, Spielman and Teng (2012) targeted symmetric diagonally dominant matrices (e.g., Laplacian), Arrigo et al. (2018) analyzed matrices that are power series of random walk kernels. In contrast to these approaches, Saad and Schultz (1986) proposed general iterative methods for solving certain linear systems using Arnoldi's iterations. However, These iterative methods can suffer from convergence issues. Williams (2007) showed that it is possible to pre-process any boolean matrix to achieve sub-quadratic matrix-vector multiplication.

The general problem of computing the action of a matrix on a vector, where the matrix is the graph kernel, in sub-quadratic time is intractable, except for a few special cases (Al-Mohy and Higham, 2011; Choromanski et al., 2023). In this work, we embed the graph \(G\) under consideration in a tree (replacing the graph metric by the underlying _tree metric_). Then, we leverage the tree structure to approximate the action of the kernel on a given vector by providing **exact** integration on a tree.

Previous works (Bartal et al., 2022, 2019; Abraham et al., 2008; Bartal, 1998) have used the theory of _tree metrics_ (TMs) in several applications in mathematics and computer science. TMs are widely used to embed a complex metric space (e.g., a Riemannian manifold) into a more tractable one, while approximately preserving (all or most of the) pairwise distances. They find applications in distributed & online algorithms (Khan et al., 2008; Bubeck et al., 2018), biology (Mossel, 2007), vision, robotics (Athitsos and Sclaroff, 2003), and ML (e.g., metric spaces' regression (Gottlieb et al., 2011)).

Tree metrics for fast matrix multiplication:Applying tree metrics (TM) to compute approximate \(\mathbf{M}_{f}^{G}\) is a natural approach to scale up matrix multiplications. If a TM approximates the metric space well, then the derived embeddings should have low distortion. However, in the worst-case scenario, this is not true for deterministic _tree embeddings_. A natural alternative is to sample trees from probabilistic distributions, which are shown to provide logarithmic distortion in expectation (Fakcharoenphol et al., 2004; Bartal et al., 2022). This can be further improved to constant distortion for certain classes of metrics, e.g., celebrated _snowflake metrics_(Leeb, 2016). For graph metrics defined by shortest-path distances, there exist spanning trees providing constant average distortion (over all pairs of nodes). These spanning trees can be constructed as _near minimum weight spanning trees_(Bartal et al., 2016). Unfortunately, explicit application of _any_ tree metric still requires \(O(N^{2})\) time (impractical for large \(N\)) to: **(1)** compute all shortest-path distances via the breadth-first-search algorithm (BFS), even if sub-quadratic methods were used to construct a tree (e.g. minimum spanning tree), **(2)** store the matrix, and **(3)** perform matrix-vector multiplications. We provide more details about work related to graph field integration in Appendix B.

## 3 Fast Tree-Field Integrators (FTFI)

In this section, we present our approach for performing efficient field integration on a tree, which we call _fast tree field integrator_. We begin by introducing the concept of integrator trees (ITs), which is a

Figure 1: Pictorial representation of the IntegratorTree (see: Sec 3.1) data structure for the nine-vertex input tree \(\mathcal{T}\) on the left. Numbers in blue next to the input tree denote the weights of its edges. Leaves of the IntegratorTree object represent \(f\)-transformed (element-wise) distance matrices: \(\mathbf{D}_{0},\mathbf{D}_{1},\mathbf{D}_{2},\mathbf{D}_{3}\) for sub-trees induced by vertex-sets: \(\{1,2,4\},\{1,3,0\},\{5,7,8\}\) and {5,6,0} respectively. Different _levels_ correspond to different distances from the pivot point.

specialized decomposition of a tree using the theory of _balanced separators_ (Sec 3.1). Subsequently, we leverage these integrator trees to execute efficient integration on a tree via a _divide-and-conquer algorithm_ (Sec 3.2).

### IntegratorTrees (ITs) - preliminaries

To support fast integration for various tensor fields \(\mathbf{X}\in\mathbb{R}^{N\times d_{1}\times\ldots\times d_{s}}\) defined on a given input tree \(\mathcal{T}\), we first design a special data structure that we refer to as an _IntegratorTree_ (IT). An object of this type is constructed only once per \(\mathcal{T}\), regardless of the number of tensor fields used. An IT is a rooted binary tree. To avoid confusion, we will refer to its vertices as _nodes_, reserving term _vertices_ for those of \(\mathcal{T}\). Each node of IT corresponds to the induced sub-tree \(\mathcal{ST}\) of \(\mathcal{T}\). For every non-leaf node corresponding to some \(\mathcal{ST}\), a _pivot_ point \(p\) along with two sub-trees: \(\mathcal{ST}_{\mathrm{left}}\) and \(\mathcal{ST}_{\mathrm{right}}\) are constructed. The following needs to be satisfied:

* \(|\mathcal{ST}_{x}|\geq\frac{|\mathcal{ST}|}{4}\) for \(x\in\{\mathrm{left}\}\), \(\mathrm{right}\),
* \(\mathcal{ST}_{x}\cap\mathcal{ST}_{y}=\{p\}\) (\(|\cdot|\) denotes the number of vertices).

The next lemma shows that every tree \(\mathcal{K}\) with \(|\mathcal{K}|\geq 6\) has the above decomposition and it can be efficiently found.

**Lemma 3.1** (**Pivoting**).: _If \(\mathcal{K}\) is a tree with \(|\mathcal{K}|\geq 6\), then \(\mathcal{K}\) admits a decomposition (\(\mathcal{K}_{\mathrm{left}},\mathcal{K}_{\mathrm{right}},p\)) given above and it can be constructed in **linear** time._

The algorithmic proof is provided in Appendix A.1 and uses standard tools from the theory of balanced separators.

The _left child_ of the non-leaf node for \(\mathcal{ST}\) corresponds to \(\mathcal{ST}_{\mathrm{left}}\) and the _right child_ to \(\mathcal{ST}_{\mathrm{right}}\). In addition to these two pointers, a non-leaf node also contains eight extra fields, partitioned into two groups, one corresponding to its left child and one to its right children. The fields corresponding to the left child are as follows:

* **Left-ids:** an array of the ids (in \(\mathcal{T}\)) of those vertices that are in \(\mathcal{ST}_{\mathrm{left}}\), mapping the ids of vertices in \(\mathcal{ST}_{\mathrm{left}}\) to the original ids in \(\mathcal{T}\) (each sub-tree uses consecutive numbers from \(0\) as ids locally).
* **Left-d:** an array of different shortest-path **d**istances from the pivot point to the vertices in \(\mathcal{ST}_{\mathrm{left}}\).
* **Left-id-d:** an array mapping the ids of vertices (in \(\mathcal{ST}_{\mathrm{left}}\)) to the indices in left-d of their corresponding distances from the pivot point.
* **Left-s:** a corresponding array of the ordered sub-sets of ids (in \(\mathcal{ST}_{\mathrm{left}}\)) of vertices within a particular distance from the pivot point.

Fields corresponding to the right child are defined similarly. The leaf nodes of the IT consist only of the \(f\)-transformed (element-wise) distance matrices \(\mathbf{D}\) for their corresponding sub-trees (see: Fig 1). In principle, the leaf nodes of IT correspond to sub-trees with less than \(t=6\) vertices each. In practice, we choose higher \(t\), for more efficient integration (see: discussion in Sec. 4.1).

Time & space complexity of constructing ITs:From what we have said so far, it is clear that an IT can be constructed by applying _breadth first search_ (BFS) and the linear algorithmic procedure for constructing the decomposition from Lemma 3.1. Note that every vertex of the input tree appears in the logarithmic number of nodes in the IT since the size of the sub-tree is at most \(\frac{3}{4}\times\) the size of its parent in IT. We conclude that IT for the given input tree \(\mathcal{T}\) can be computed in \(O(N\log(N))\) time, where \(N\) stands for the number of vertices \(|\mathcal{T}|\) of \(\mathcal{T}\).

### Integrating with IntegratorTrees

We are ready to explain how ITs allow us to efficiently integrate any given tensor field \(\mathbf{X}\in\mathbb{R}^{N\times d_{1}\times\ldots\times d_{s}}\) defined on \(\mathcal{T}\) for a wide class of function \(f:\mathbb{R}\to\mathbb{R}\). We will apply a _divide-and-conquer_ strategy.

We start in the root node of IT. If that node is a leaf then the \(f\)-transformed distance matrix is stored and can be directly used for matrix-tensor multiplication. If this node is not a leaf, then it encodes the decomposition \((\mathcal{T}_{\mathrm{left}},\mathcal{T}_{\mathrm{right}},p)\). Take some \(v\in\mathrm{V}(\mathcal{T}_{\mathrm{left}})\). Note that the value \(\mathbf{M}_{f}^{\mathrm{G}}\mathbf{X}[v]\) of the new field in \(v\) after \(f\)-integration is given as follows for \(\mathcal{W}=\mathrm{V}(\mathcal{T}_{\mathrm{right}})\backslash\{p\}\):

\[\underbrace{\sum_{j\in\mathcal{V}(\mathcal{T}_{\mathrm{left}})}f(\mathrm{dist}( v,j))\mathbf{X}[j]}_{\mathrm{F}_{\mathrm{norm}}(v)}+\underbrace{\sum_{j\in \mathcal{W}}f(\mathrm{dist}(v,j))\mathbf{X}[j]}_{\mathrm{F}_{\mathrm{norm}}(v) }. \tag{2}\]

To compute the new values of the field for nodes \(v\in\mathcal{T}_{\mathrm{left}}\), one needs to:

1. Compute the contribution to it from \(\mathcal{T}_{\mathrm{left}}\) (\(\mathrm{F}_{\mathrm{inner}}(v)\)-terms). This can be done simply by applying Eq. 2 recursively for \(\mathcal{T}_{\mathrm{left}}\), which means traversing to the left child of the root.
2. Add the so-called _cross-terms_ contributions coming from the vertices of \(\mathcal{W}\) (\(\mathrm{F}_{\mathrm{cross}}(v)\)-terms).

The key observation is that the latter (cross-term) contributions can be retrieved simply by computing \(\mathbf{C}\mathbf{X}^{\prime}\), where: (1) \(\mathbf{C}\in\mathbb{R}^{k\times l}\) with \(k\) and \(l\) being the sizes of the node's left-d and right-d arrays respectively. \(\mathbf{C}(i,j)=f(\text{left-d}[i]+\text{right-d}[j])\), and (2) Let \(b_{j}\stackrel{{\mathrm{def}}}{{=}}|\text{right-s}[j]|\) where \(|\cdot|\) refers to the size of the subset. Then \(\mathbf{X}^{\prime}\in\mathbb{R}^{l\times d_{1}\times\ldots\times d_{s}}\) is defined as follows:

\[\mathbf{X}^{\prime}[j]\stackrel{{\mathrm{def}}}{{=}}\sum_{z=0}^{b _{j}-1}\mathbf{X}[\text{right-ids}[\text{right-s}[j][z]]]. \tag{3}\]

Given the structure of IT, tensor \(\mathbf{X}^{\prime}\) can be computed in linear time. Note that the following holds:

\[\mathrm{F}_{\mathrm{cross}}(v)=(\mathbf{C}\mathbf{X}^{\prime})[\tau(v)]-f( \text{left-d}[\tau(v)])\mathbf{X}^{\prime}[0], \tag{4}\]

where \(\tau(v)=\text{left-d-d}[v]\). Analogous analysis can be derived for \(v\in\mathcal{T}_{\mathrm{right}}\), with matrix \(\mathbf{C}^{\top}\) replacing \(\mathbf{C}\). Thus the overall time complexity of the cross-terms computations is determined by the algorithm for matrix-tensor multiplications with matrices \(\mathbf{C}\) and \(\mathbf{C}^{\top}\).

2.1 The case for structured matrices: multiplications with \(\mathbf{C},\mathbf{C}^{\top}\) and cordiality

Matrices \(\mathbf{C},\mathbf{C}^{\top}\) are of the form: \([f(x_{i}+y_{j})]_{i=1,\ldots,a}^{j=1,\ldots,b}\) for some sequences \(X=(x_{i})_{i=1}^{a}\), \(Y=(y_{j})_{j=1}^{b}\) and \(a,b\in\mathbb{N}_{+}\).

**Definition 3.2** (cordial functions).: A function \(f:\mathbb{R}\to\mathbb{R}\) is _\(d\)-cordial_ (or: _cordial_ if \(d\) is not specified), if there exists \(d\in\mathbb{N}\) such that matrix-vector multiplication with a matrix \(\mathbf{M}=[f(x_{i}+y_{j})]_{i=1,\ldots,a}^{j=1,\ldots,b}\) can be conducted in time \(O((a+b)\log^{d}(a+b))\) for every \((x_{i})_{i=1}^{a}\), \((y_{j})_{j=1}^{b}\).

Next, we demonstrate the importance of cordial functions in our FTFI framework.

**Lemma 3.3** (\(f\)-integration with cordial functions).: _If \(f\) is \(d\)-cordial then \(f\)-integration for the general weighted tree of \(N\) vertices can be conducted in time \(O(N\log^{d+1}(N))\)._

Proof.: Denote by \(T(N)\) time complexity for running FTFI on the \(N\)-vertex tree. We have the following recursive formula for \(T\), where \(\frac{1}{4}\leq c\leq\frac{3}{4}\):

\[T(N)\leq T(cN)+T((1-c)N)+O(N\log^{d}(N)) \tag{5}\]

This is implied by the fact that: (1) the size of each sub-tree is at most \(\frac{3}{4}\times\) the size of its parent, (2) the computation across left and right children is dominated by multiplications with matrices \(\mathbf{C}\) and \(\mathbf{C}^{\top}\). The solution of this recursion leads to the statement. 

Next, we show some practical implications of Lemma 3.3, where tree weights are **completely arbitrary**. Additional results are given in Sec. A.2.3.

Rational functions:We claim that every rational \(f\) is \((2+\epsilon)\)-cordial for any \(\epsilon>0\). We will use Lemma 1 from [10] stating that: given any set of \(b\) rational functions \(R_{j}(x)=\frac{P_{j}(x)}{Q_{j}(x)}\) and \(\{x_{i}\}_{i=1}^{a}\), one can compute the \(a\) values \(\sum_{j=1}^{b}R_{j}(x_{i})\) in time \(O((a+b)\log^{2}(b)\log(\log(b)))\) (by applying FFT). For a given vector \(\mathbf{v}\in\mathbb{R}^{b}\), it thus suffices to define: \(R_{j}(x)=v_{j}f(x+y_{j})\) and that lemma can be applied to efficiently compute \(\mathbf{M}\mathbf{v}\). We conclude that for any \(\epsilon>0\), \(f\)-integration can be conducted in \(O(N\log^{3+\epsilon}(N))\) time for \(N\)-vertex weighted trees and any rational \(f:\mathbb{R}\to\mathbb{R}\) (see also: Sec. 4.3, Sec. 4.2, Sec. 4.4).

Polynomial functions:The above result on rational functions clearly applies also to polynomial \(f\), but here we can do better. We show that \(f\) is \(0\)-cordial. Assume that \(f(x)=\sum_{t=0}^{B}a_{t}x^{t}\). We have: \(\mathbf{M}=\sum_{t=0}^{B}\sum_{l=0}^{t}a_{t}\binom{t}{\mathbf{M}_{t,t-l}}\), where matrix \(\mathbf{M}_{u,v}\in\mathbb{R}^{a\times b}\) is defined as an outer-product of two vectors: \((x_{1}^{u},...,x_{a}^{u})\in\mathbb{R}^{a}\) and \((y_{1}^{v},...,y_{b}^{v})\in\mathbb{R}^{b}\). Thus each \(\mathbf{M}_{u,v}\) supports linear matrix-vector multiplication (via associativity property). The proof is completed, since \(B\) is a constant. We conclude that \(f\)-integration can be conducted in \(O(N\log(N))\) time for \(N\)-vertex weighted trees and any polynomial \(f:\mathbb{R}\rightarrow\mathbb{R}\) (see: Fig. 2 and Fig 9).

Exponential functions:Take \(f(x)=\exp(\lambda x)\). Then \(\mathbf{M}\) is an outer-product of two vectors: \((\exp(\lambda x_{i}))_{i=1}^{a}\in\mathbb{R}^{a}\) and \((\exp(\lambda y_{j}))_{j=1}^{b}\in\mathbb{R}^{b}\). The remaining analysis and conclusion is thus the same as for the polynomial case (see also: Sec. 4.4).

Function:\(f(x)=\frac{\exp(\lambda x)}{x+c}\): (c) is a constant) We claim that \(f\) is \(2\)-cordial. In that setting, matrix \(\mathbf{M}\) satisfies: \(\mathbf{M}(i,j)=\frac{\exp(\lambda x_{i})\exp(\lambda y_{j})}{(x_{i}+\frac{i}{ 2})+(y_{j}+\frac{i}{2})}\) and thus is a _Cauchy-like_ LDR, supporting fast \(O(N\log^{2}(N))\) matrix-vector multiplication (Victor Y. Pan, 2000). We conclude that \(f\)-integration can be conducted in \(O(N\log^{3}(N))\) time for \(N\)-vertex weighted trees and \(f(x)=\frac{\exp(\lambda x)}{x+c}\) (see: Fig. 2).

Functions \(f(x)=\exp(ux^{2}+vx+w)\) and trees with positive rational weights:Now matrix \(\mathbf{M}\) can be re-written as \(\mathbf{M}=\exp(w)\mathbf{D}_{1}\mathbf{V}\mathbf{D}_{2}\), where \(\mathbf{D}_{1}\in\mathbb{R}^{a\times a}\) and \(\mathbf{D}_{2}\in\mathbb{R}^{b\times b}\) are diagonal, with diagonal entries given by sequences \(\{\exp(ux_{i}^{2}+vx_{i})\}_{i=1}^{a}\) and \(\{\exp(uy_{j}^{2}+vy_{j})\}_{j=1}^{b}\) respectively, and furthermore \(\mathbf{V}\) is the _generalized Vandermonde matrix_ (GVM) (using arbitrary nonnegative integers as exponents). It is defined as: \(\mathbf{V}(i,j)=r_{i}^{s_{j}}\), where \(r_{i}=\exp(\frac{2ux_{i}}{q})\) and \(s_{j}=y_{j}q\in\mathbb{N}\). As in the previous case, the embedding trick can be applied, but we will use it only for columns. That effectively leads to the completion of the set of exponents \(\{s_{j}\}\) to the set of consecutive integers starting from \(0\) and a regular Vandermonde matrix, that supports \(O(N\log^{2}(N))\) matrix-vector multiplication, replacing GVM. The benefit of this embedding, as compared to the previous one, is that even though it still increases the number of columns by a multiplicative factor of \(p\), the number of rows does not change. Therefore, for \(p\gg\log(N)\), substantial computational speedups are achieved (see: Sec. 4.4).

## 4 Experiments

In this section, we outline the experimental setup and report the performance of FTFI across various settings. For all the experiments, we only consider minimum spanning tree (MST) as an approximation of our graph. Specifically, we design experiments to answer these research questions:

Figure 2: Pictorial representations of the main concepts behind efficient matrix-vector multiplications \(\mathbf{M}\mathbf{v}\) with \(\mathbf{M}\in\mathbb{R}^{5\times 4}\), for the polynomial \(f\) and \(f(x)=\frac{\exp(\lambda x)}{x+c}\). In the polynomial case, \(\mathbf{M}\) is re-written as a sum of low-rank outer-product matrices corresponding to terms of different degrees (e.g., constant, linear, quadratic, etc.). Matrix associativity property is applied for efficient calculations (dotted-border blocks indicating the order of computations). In the second case, \(\mathbf{M}\) is high-rank, but the so-called _low displacement rank operator_\(\Delta_{D_{1},D_{2}}:\mathbf{X}\rightarrow\mathbf{D}_{1}\mathbf{M}-\mathbf{M} \mathbf{D}_{2}\) for diagonal \(\mathbf{D}_{1},\mathbf{D}_{2}\) can be applied to make it a low-rank outer-product matrix. The multiplication with \(\mathbf{M}\) can be efficiently performed using the theory of LDR matrices (Thomas et al., 2018).

* **How efficient are FTFIs for tree field integration?**
* **How does the approximation quality of FTFI compare to other integration algorithms?**
* **How can we further improve the approximation quality in FTFI?**
* **How can we use FTFI in real-world large-scale settings?**

### Runtime Efficiency of FTFI

The main goal of this experiment is to evaluate the speedups obtained by FTFI as compared to brute-force tree field integrator (BTFI) i.e. the explicit calculation of Eq 1 on a tree. We consider two classes of graphs: **(a)**_synthetic_, obtained from a path-graph by adding random edges and **(b)**_mesh graphs_ from Thingi10K [Zhou and Jacobson, 2016] dataset. For BTFI, we compute the MST and then integrate a random scalar field \(\mathbf{X}\) on the vertices of the MST. Since BTFI & FTFI are numerically equivalent, we report the pre-processing time and integration as a function of vertex count (\(N\)) in Fig. 3. We observe that FTFI achieves up to **13x** speedups for 20K-vertex meshes and **5.7x+** for synthetic graphs with over 10K vertices compared to BTFI.

### Approximation Quality of FTFI

**Interpolation on meshes.** We compare the efficiency of FTFI with baselines on the _normal vector prediction task_. Every node of the considered mesh \(\mathrm{G}\) with a vertex-set \(\mathrm{V}\), is associated with a location \(\mathbf{x}_{i}\in\mathbb{R}^{3}\) and a vertex normal \(\mathbf{F}_{i}\in\mathbb{R}^{3}\). For each mesh, we randomly select a subset \(\mathrm{V}^{\prime}\subseteq\mathrm{V}\) with \(|\mathrm{V}^{\prime}|=0.8|\mathrm{V}|\) and mask out their vertex normals (set as zero vectors). The interpolation task involves predicting the vertex normals of each masked node \(i\in\mathrm{V}^{\prime}\) as: \(\mathbf{F}_{i}=\sum_{j\in\mathrm{V}\setminus\mathrm{V}^{\prime}}\mathrm{K}_{j }(i,j)\mathbf{F}_{j},\) where \(\mathrm{K}_{f}(w,v)=f(\mathrm{dist}(w,v))\), with \(\mathrm{dist}(w,v)\) being the shortest path distance between node \(w\) and \(v\), and \(f\) is a rational function \(f(x)=1/(1+\lambda x^{2})\). We perform a grid search to set hyperparameter \(\lambda\) for each mesh and report the result with the highest cosine similarity between predicted and ground truth vertex normals, averaged over all the nodes. We run tests on **40 meshes** of the 3D-printed objects with a wide range of sizes from the Thingi10K dataset (details in Appendix D.3). We compare FTFI with BTFI, low-distortion tree-based algorithms such as Bartal Trees [Bartal, 1996] and FRT trees [Fakcharoenphol et al., 2004a] alongside the state-of-the-art method for graph-field integration, the Separator Factorization (SF) algorithm [Choromanski et al., 2023]. We also compare against the baseline BGFI which entails explicitly materializing the kernel matrix of \(\mathrm{G}\) and then performing matrix tensor multiplication with a tensor field \(\mathbf{F}\) defined by the \(\mathbf{F}_{i}\)'s.

Preprocessing involves building specific tree structures (FRT, Bartal), calculating the kernel matrices (BGFI, BTFI), or creating specialized data structures (SF, FTFI) for efficient later use. The first two plots in Fig. 4 shows the pre-processing time and cosine similarity for various algorithms applied to meshes of different sizes. FTFI is the fastest in terms of pre-processing time and achieves competitive performance in terms of cosine similarity (between predicted and actual vertex normals) when compared with the SF algorithm while being numerically equivalent to BTFI. FTFI is a few orders of magnitude faster than BTFI and the tree-based methods while maintaining accuracy.

**Graph classification.** Graph kernels have been widely used for graph classification tasks in previous works [Kriege et al., 2020, Nikolentzos et al., 2021]. We compare the classification results obtained using the approximate kernel from FTFI with those from the exact SP kernel. In this setting, we use the Shortest Path (SP) kernel, \(f(\mathrm{dist}(i,j))\). We perform experiments on a wide range of bioinformatics and social networks datasets like D&D, Mutag, Reddit, Imdb, among others. We follow [de Lara and Pineau, 2018] and construct the graph feature for both kernels by using the smallest \(k\) eigenvalues (\(k\) is a hyperparameter). This feature set is then used for classification, using

Figure 3: Runtime comparison of FTFI with BTFI as a function of the number of vertices, \(N\). **Left:** Synthetic graphs. **Right**: Mesh-graphs from Thingi10K. The speed is not necessarily monotonic in \(N\) as it depends on the distribution of lengths of the shortest paths. For each graph, 10 experiments were run (std. shown via dotted lines).

a random forest classifier. We observe that FTFI achieves significant speed improvements while achieving similar accuracy compared to its brute-force counterpart, BGFI (see Fig. 5). We provide more details about the experimental setup and baselines Appendix D.4. We also report additional experiments on meshes and point clouds in Appendix D.1.

### Improving approximation quality with learnable \(f\)-distance matrices

We propose to further improve the approximation quality of FTFI by learning a \(f\)-distance matrix on metrics derived from the MST. As an application, we choose _general graph metrics_, where our goal is to learn the shortest-path distance \(d_{v,w}\) between a given pair of nodes \((v,w)\) in a graph. Given a \(f\)-distance matrix and tree-derived metric \(\widehat{d}_{v,w}\) the objective is to learn a mapping to minimize

\[\mathbb{E}_{(v,w)\in\mathcal{D}}\left[\left(d_{v,w}-f^{a_{0},...,a_{t}}_{b_{0},...,b_{s}}(\widehat{d}_{v,w})\right)^{2}\right]. \tag{6}\]

Rather than using a fixed \(f\), we parameterize and train it. We consider rational function \(f\):

\[f^{a_{0},...,a_{t}}_{b_{0},...,b_{s}}(x)=\frac{a_{0}+a_{1}x+...+a_{t}x^{t}}{b_ {0}+b_{1}x+...+b_{s}x^{s}}, \tag{7}\]

where \(a_{0},...,a_{t},b_{0},...,b_{s}\in\mathbb{R}\) are trainable parameters.

Figure 4: Speed (pre-processing time) and accuracy (cosine similarity) comparison of the FTFI and other baselines for vertex normal prediction on meshes. Cosine similarity of BFFI and FTFI almost overlaps. The last two figures are qualitative examples showcasing the tradeoff between cosine similarity and preprocessing time for meshes of sizes 3K and 5K nodes respectively.

Figure 5: Trade-off plot comparing graph classification accuracy and feature processing time for the classifiers using FTFI and BGFI. FTFI achieves similar accuracy as BGFI while significantly reducing fp time across most datasets. We report the reduction in FTFI’s processing time (\(\pm\)x%) compared to BGFI using a dotted line.

Training dataset \(\mathcal{D}\). For a graph \(\mathrm{G}\), we randomly sample vertices. The training dataset consists of tuples of the form: \((v,w,d_{v,w},\widehat{d}_{v,w})\in\mathcal{D}\), where \(v,w\) are randomly sampled vertices. Each data point can be constructed in time \(O(N\log(N))\), or even \(O(N)\) if weights are in \(\mathbb{N}\)[Thorup, 1997].

Final evaluation. To evaluate the quality of the approximation, we compute the relative Frobenius norm error: \(\epsilon=\frac{\|\mathbf{M}_{\mathrm{T}}^{T}-\mathbf{M}_{\mathrm{id}}^{G}\|_{ \mathrm{F}}}{\|\mathbf{M}_{\mathrm{id}}^{G}\|_{\mathrm{F}}}\), where \(\|\cdot\|_{\mathrm{F}}\) stands for the _Frobenius norm_, \(\mathrm{T}\) is a tree for a given graph \(\mathrm{G}\) and \(\mathrm{id}\) is an identity function (see: our notation from Sec. 1). It quantifies how closely the distance matrix of \(\mathrm{G}\) is approximated by the \(f\)-distance matrix of \(\mathrm{T}\). Computing \(\epsilon\) is expensive and our training does not rely on it. Our empirical results show that the relative error, \(\epsilon\), can be substantially improved by using the light-weight MSE training loss (defined in Eq. 6).

We report the evaluation error for these experiments in Fig. 6 (with additional results in Fig. 8 in the Appendix). We observe that a rational function with quadratic numerator and denominator provides strong performance across different graphs. We notice that increasing the training set to \(>100\) data points does not have a substantial impact on the final error. Estimating the coefficients of \(f\) provides approximation improvements across all graphs in as few as **40 training steps**.

These above results show that tree-based estimators are expressive enough to emulate integration on arbitrary graphs. This expressive power can be further enhanced by pairing them with "nonlinear" functions \(f\). Thus, they explain why the presented techniques are relevant for general graphs.

### Large Scale Transformer Experiments using FTFI

For large-scale applications of FTFI, we select Topological Vision Transformers (TopViT), [Choromanski et al., 2022], and leverage it for efficient incorporation of masking within ViTs. We provide detailed description of masked Transformers in Appendix C.

**Topological Vision Transformers with trees :** We propose an extension to TopViT that seamlessly integrates FTFI. In this extension, we model the mask matrix as an \(f\)-distance matrix (with learnable \(f\)) defined on the minimum spanning tree (MST) obtained from the 2D grid graph image encoding, where vertices correspond to different patches. We parameterize \(f\) as \(f_{g}^{t}\stackrel{{\mathrm{def}}}{{=}}g(\sum_{i=0}^{t}a_{t}x^{t})\). We use the linear attention mechanism introduced in Performers [Choromanski et al., 2021], where the attention kernel is written as: \(\mathrm{K}(\mathbf{q},\mathbf{k})=\phi(\mathbf{q})^{\top}\phi(\mathbf{k})\) for a deterministic \(\phi:\mathbb{R}^{d_{QK}}\rightarrow\mathbb{R}\), applied element-wise. We experiment with different values of hyperparameters \(g\), \(t\), \(\phi\) and cross-heads parameter sharing strategies as shown in Table 1 (synced indicates that RPE-parameters are shared across different attention heads).

We run experiments on ImageNet and Places365 datasets using ViT-B/16 (see Table 1). For all the kernels, our variants beat the baselines. For \(\phi(x)=x^{4}\), the best variant applies an exponentiated quadratic function, for which we apply Vandermonde matrices (see: discussion in Sec. 3.2.1). Our

Figure 6: **Left: Relative Frobenius norm error as a function of the number of training iterations for different sizes \(n\) and learnable quadratic \(f\). Middle: Comparison of the training of different rational functions \(f\) with num:d defining the degree of the numerator and den:d, the degree of the denominator for the synthetic graph obtained from a path on \(N=800\) by adding 600 random edges and assigning random weights taken from \((0,1)\). Right: constructed similarly, but for a sampled mesh graphs from Thingi10k dataset.**

best variant across all kernels (**78.79%**) provides **2%** accuracy gains over the best baseline (**76.76%**). In the synced setting, we use only **three** extra learnable parameters per layer (shared in all attention heads across all layers) and obtain **1-1.5%** accuracy gains. In the asynced setting, we use a small set of **36** extra learnable parameters per layer (3 extra parameters per head). Overall, we observe that FTFI improves the approximation quality within Transformers with a minimal number of parameters. We provide additional discussions on the ViT results for ImageNet in Appendix D.5.1 and for Places365 in Appendix D.5.2.

Additional results on the I-Naturalist dataset, where we outperform various low-rank attention baselines, are provided in Appendix D.5.3.

Larger Transformer models:We scale our experiments to run on the larger ViT-L architectures and evaluate on ImageNet. In this setting, we use RPE mechanism with \(g=\exp\) and \(t=1\) (that provided strong performance in previous experiments) and asynced strategy. We observe that FTFI provides **7%** accuracy improvement (see: Fig. 7).

Further results on Video Transformer (ViViT) [1] are provided in Appendix D.6. We also provide additional experiments including Gromov-Wasserstein distance computation [20] (see Sec. D.2), along with code pointers (Appendix D).

## 5 Conclusion

We provided a new class of algorithms for fast and exact integration of tensor fields defined on weighted trees, relying on the theory of structured (in particular low displacement rank) matrices. We showed how those algorithms can be applied for accurate integration on general graphs, in particular via their minimum weight spanning trees. We presented several applications of the presented methods, from graph classification and interpolation on meshes, through graph metric approximation to Topological Vision Transformers. Our methods provide significant (5-13x) speedups while maintaining the quality of their exact counterparts.

## 6 Author Contributions

KC conceived the idea behind FTFI, proved the theoretical results, implemented FTFI algorithm and ran the vision experiments in this paper. AS integrated the FTFI algorithm in the GW style algorithms and ran some graph and point cloud classification tasks. SBRC ran graph classification experiments as well as experiments on the CUBES dataset. HL ran the experiments on the meshes. AD helped develop methods, and along with TS and SC acted as senior advisors for the project. All authors contributed to the writing of the manuscript.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{} & \multicolumn{4}{c}{**ImageNet**} & \multicolumn{4}{c}{**Place365**} \\ \multicolumn{1}{c}{} & \multicolumn{4}{c}{\(\phi\!=\!\pi\!-\!x^{2}\)} & \multicolumn{4}{c}{\(\phi\!=\!x\!-\!x^{4}\)} & \multicolumn{4}{c}{\(\phi\!:=\!\pi\!-\!x^{4}\)} & \multicolumn{4}{c}{\(\phi\!:=\!\exp\)} & \multicolumn{4}{c}{\(\phi\!:=\!\exp\)} \\ \cline{2-14} \multicolumn{1}{c}{**word**} & \multicolumn{1}{c}{\(\mathbf{\theta}\)} & \multicolumn{1}{c}{**Acc.(\%)**} & \multicolumn{1}{c}{**ynced**} & \multicolumn{1}{c}{**g**} & \multicolumn{1}{c}{**Acc.(\%)**} & \multicolumn{1}{c}{**synced**} & \multicolumn{1}{c}{**g**} & \multicolumn{1}{c}{**t**} & \multicolumn{1}{c}{**Acc.(\%)**} & \multicolumn{1}{c}{**synced**} & \multicolumn{1}{c}{**g**} & \multicolumn{1}{c}{**t**} & \multicolumn{1}{c}{**Acc.(\%)**} \\ \hline \hline Na & NA & NA & 76.23 & NA & NA & NA & 75.03 & NA & NA & NA & 76.37 & NA & NA & NA & NA & 76.76 & NA & NA & NA & 54.80 \\ ✓ & \(\exp\) & \(1\) & 77.28 & ✓ & \(\exp\) & 1 & 76.66 & ✓ & \(\exp\) & 1 & 77.84 & ✗ & \(\exp\) & 1 & **78.79** & ✗ & \(\exp\) & 1 & 56.69 \\ ✓ & \(\exp\) & \(2\) & 76.60 & ✓ & \(\exp\) & 2 & 75.91 & ✓ & \(\exp\) & 2 & 77.23 & ✗ & \(

## References

* Abadi et al. (2016) Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In Kimberly Keeton and Timothy Roscoe, editors, _12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016_, pages 265-283. USENIX Association, 2016. URL [https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi](https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi).
* Abraham et al. (2008) Ittai Abraham, Yair Bartal, and Ofer Neiman. Nearly tight low stretch spanning trees. In _49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA_, pages 781-790. IEEE Computer Society, 2008. doi: 10.1109/FOCS.2008.62. URL [https://doi.org/10.1109/FOCS.2008.62](https://doi.org/10.1109/FOCS.2008.62).
* Acebron (2019) Juan A. Acebron. A monte carlo method for computing the action of a matrix exponential on a vector, 2019.
* Acebron et al. (2019) Juan A. Acebron, Jose R. Herrero, and Jose Monteiro. A highly parallel algorithm for computing the action of a matrix exponential on a vector based on a multilevel monte carlo method, 2019.
* Al-Mohy and Higham (2010) Awad H. Al-Mohy and Nicholas J. Higham. A new scaling and squaring algorithm for the matrix exponential. _SIAM Journal on Matrix Analysis and Applications_, 31(3):970-989, 2010. doi: 10.1137/09074721X. URL [https://doi.org/10.1137/09074721X](https://doi.org/10.1137/09074721X).
* Al-Mohy and Higham (2011) Awad H. Al-Mohy and Nicholas J. Higham. Computing the action of the matrix exponential, with an application to exponential integrators. _SIAM Journal on Scientific Computing_, 33(2):488-511, 2011. doi: 10.1137/100788860. URL [https://doi.org/10.1137/100788860](https://doi.org/10.1137/100788860).
* Arnab et al. (2021) Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6836-6846, 2021.
* Arnoldi (1951) Walter E. Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem. _Quarterly of Applied Mathematics_, 9:17-29, 1951. URL [https://api.semanticscholar.org/CorpusID:115852469](https://api.semanticscholar.org/CorpusID:115852469).
* Arrigo et al. (2018) Francesca Arrigo, Peter Grindrod, Desmond J. Higham, and Vanni Noferini. On the exponential generating function for non-backtracking walks. _Linear Algebra and its Applications_, 556:381-399, 2018. ISSN 0024-3795. doi: [https://doi.org/10.1016/j.laa.2018.07.010](https://doi.org/10.1016/j.laa.2018.07.010). URL [https://www.sciencedirect.com/science/article/pii/S0024379518303288](https://www.sciencedirect.com/science/article/pii/S0024379518303288).
* Athitsos and Sclaroff (2003) Vassilis Athitsos and Stan Sclaroff. Database indexing methods for 3d hand pose estimation. In Antonio Camurri and Gualtiero Volpe, editors, _Gesture-Based Communication in Human-Computer Interaction, 5th International Gesture Workshop, GW 2003, Genova, Italy, April 15-17, 2003, Selected Revised Papers_, volume 2915 of _Lecture Notes in Computer Science_, pages 288-299. Springer, 2003. doi: 10.1007/978-3-540-24598-8_27. URL [https://doi.org/10.1007/978-3-540-24598-8_27](https://doi.org/10.1007/978-3-540-24598-8_27).
* Auckenthaler et al. (2010) T. Auckenthaler, M. Bader, T. Huckle, A. Sporl, and K. Waldherr. Matrix exponentials and parallel prefix computation in a quantum control problem. _Parallel Computing_, 36(5-6):359-369, May 2010. ISSN 0167-8191. doi: 10.1016/j.parco.2010.01.006.
* Bartal (1996) Yair Bartal. Probabilistic approximation of metric spaces and its algorithmic applications. In _Proceedings of 37th Conference on Foundations of Computer Science_, pages 184-193. IEEE, 1996.
* Bartal (1998) Yair Bartal. On approximating arbitrary matrices by tree metrics. In Jeffrey Scott Vitter, editor, _Proceedings of the Thirtieth Annual ACM Symposium on the Theory of Computing, Dallas, Texas, USA, May 23-26, 1998_, pages 161-168. ACM, 1998. doi: 10.1145/276698.276725. URL [https://doi.org/10.1145/276698.276725](https://doi.org/10.1145/276698.276725).
* Bartal et al. (2018)Yair Bartal, Arnold Filtser, and Ofer Neiman. On notions of distortion and an almost minimum spanning tree with constant average distortion. In Robert Krauthgamer, editor, _Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2016, Arlington, VA, USA, January 10-12, 2016_, pages 873-882. SIAM, 2016. doi: 10.1137/1.9781611974331.CH62. URL [https://doi.org/10.1137/1.9781611974331.ch62](https://doi.org/10.1137/1.9781611974331.ch62).
* Bartal et al. (2019) Yair Bartal, Arnold Filtser, and Ofer Neiman. On notions of distortion and an almost minimum spanning tree with constant average distortion. _J. Comput. Syst. Sci._, 105:116-129, 2019. doi: 10.1016/J.JCSS.2019.04.006. URL [https://doi.org/10.1016/j.jcss.2019.04.006](https://doi.org/10.1016/j.jcss.2019.04.006).
* Bartal et al. (2022) Yair Bartal, Ora Nova Fandina, and Ofer Neiman. Covering metric spaces by few trees. _J. Comput. Syst. Sci._, 130:26-42, 2022. doi: 10.1016/J.JCSS.2022.06.001. URL [https://doi.org/10.1016/j.jcss.2022.06.001](https://doi.org/10.1016/j.jcss.2022.06.001).
* Beniamini et al. (2020) Gal Beniamini, Nathan Cheng, Olga Holtz, Elaye Karstadt, and Oded Schwartz. Sparsifying the operators of fast matrix multiplication algorithms, 2020.
* Benzi et al. (2017) Michele Benzi, Thomas M. Evans, Steven P. Hamilton, Massimiliano Lupo Pasini, and Stuart R. Slattery. Analysis of monte carlo accelerated iterative methods for sparse linear systems. _Numerical Linear Algebra with Applications_, 24, 2017. URL [https://api.semanticscholar.org/CorpusID:6970134](https://api.semanticscholar.org/CorpusID:6970134).
* Blalock et al. (2020) Davis W. Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John V. Guttag. What is the state of neural network pruning? _CoRR_, abs/2003.03033, 2020. URL [https://arxiv.org/abs/2003.03033](https://arxiv.org/abs/2003.03033).
* Blelloch et al. (2011) Guy E. Blelloch, Anupam Gupta, Ioannis Koutis, Gary L. Miller, Richard Peng, and Kanat Tangwongsan. Near linear-work parallel sdd solvers, low-diameter decomposition, and low-stretch subgraphs, 2011.
* Boman et al. (2008) Erik Boman, Bruce Hendrickson, and Stephen Vavasis. Solving elliptic finite element systems in near-linear time with support preconditioners, 2008.
* Brent (2010) Richard P. Brent. Stability of fast algorithms for structured linear systems. _CoRR_, abs/1005.0671, 2010. URL [http://arxiv.org/abs/1005.0671](http://arxiv.org/abs/1005.0671).
* Bubeck et al. (2018) Sebastien Bubeck, Michael B. Cohen, Yin Tat Lee, James R. Lee, and Aleksander Madry. k-server via multiscale entropic regularization. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors, _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018_, pages 3-16. ACM, 2018. doi: 10.1145/3188745.3188798. URL [https://doi.org/10.1145/3188745.3188798](https://doi.org/10.1145/3188745.3188798).
* Cabello (2022) Sergio Cabello. Computing the inverse geodesic length in planar graphs and graphs of bounded treewidth. _ACM Trans. Algorithms_, 18(2):14:1-14:26, 2022. doi: 10.1145/3501303. URL [https://doi.org/10.1145/3501303](https://doi.org/10.1145/3501303).
* Chandrasekaran et al. (2018) Shivkumar Chandrasekaran, Nithin Govindarajan, and Abhejit Rajagopal. Fast algorithms for displacement and low-rank structured matrices. In Manuel Kauers, Alexey Ovchinnikov, and Eric Schost, editors, _Proceedings of the 2018 ACM on International Symposium on Symbolic and Algebraic Computation, ISSAC 2018, New York, NY, USA, July 16-19, 2018_, pages 17-22. ACM, 2018. doi: 10.1145/3208976.3209025. URL [https://doi.org/10.1145/3208976.3209025](https://doi.org/10.1145/3208976.3209025).
* Choromanski et al. (2022) Krzysztof Choromanski, Han Lin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish, Valerii Likhonserstov, Jack Parker-Holder, Tamas Sarlos, Adrian Weller, and Thomas Weingarten. From block-toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 3962-3983. PMLR, 2022. URL [https://proceedings.mlr.press/v162/choromanski22a.html](https://proceedings.mlr.press/v162/choromanski22a.html).
* Choromanski et al. (2019) Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In _9thInternational Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL [https://openreview.net/forum?id=Ua6zuk0WRH](https://openreview.net/forum?id=Ua6zuk0WRH).
* Choromanski et al. (2023) Krzysztof Marcin Choromanski, Arijit Sehanobish, Han Lin, Yunfan Zhao, Eli Berger, Tetiana Parshakova, Alvin Pan, David Watkins, Tianyi Zhang, Valerii Likhosherstov, Somnath Basu Roy Chowdhury, Kumar Avinava Dubey, Deepali Jain, Tamas Sarlos, Snigdha Chaturvedi, and Adrian Weller. Efficient graph field integrators meet point clouds. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 5978-6004. PMLR, 2023. URL [https://proceedings.mlr.press/v2020/choromanski23b.html](https://proceedings.mlr.press/v2020/choromanski23b.html).
* Christiano et al. (2010) Paul Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel A. Spielman, and Shang-Hua Teng. Electrical flows, laplacian systems, and faster approximation of maximum flow in undirected graphs, 2010.
* Cygan et al. (2015) Marek Cygan, Fedor V. Fomin, Lukasz Kowalik, Daniel Lokshtanov, Daniel Marx, Marcin Pilipczuk, Michal Pilipczuk, and Saket Saurabh. _Parameterized Algorithms_. Springer, 2015. ISBN 978-3-319-21274-6. doi: 10.1007/978-3-319-21275-3. URL [https://doi.org/10.1007/978-3-319-21275-3](https://doi.org/10.1007/978-3-319-21275-3).
* Daitch and Spielman (2008) Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized flow via interior point algorithms, 2008.
* de Lara and Pineau (2018) Nathan de Lara and Edouard Pineau. A simple baseline algorithm for graph classification, 2018.
* Drineas and Kannan (2001) P. Drineas and R. Kannan. Fast monte-carlo algorithms for approximate matrix multiplication. In _Proceedings 42nd IEEE Symposium on Foundations of Computer Science_, pages 452-459, 2001. doi: 10.1109/SFCS.2001.959921.
* Drineas et al. (2006) Petros Drineas, Ravindran Kannan, and Michael Mahoney. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. _SIAM J. Comput._, 36:132-157, 01 2006. doi: 10.1137/S0097539704442684.
* Eckstein and Nutz (2022) Stephan Eckstein and Marcel Nutz. Quantitative stability of regularized optimal transport and convergence of sinkhorn's algorithm. _SIAM J. Math. Anal._, 54(6):5922-5948, 2022. doi: 10.1137/21M145505X. URL [https://doi.org/10.1137/21m145505x](https://doi.org/10.1137/21m145505x).
* Elkin et al. (2005) Michael Elkin, Yuval Emek, Daniel A. Spielman, and Shang-Hua Teng. Lower-stretch spanning trees, 2005.
* Erica et al. (2020) Federico Erica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. In _Proceedings of the 8th International Conference on Learning Representations (ICLR)_, 2020.
* Fakcharoenphol et al. (2004a) Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. _Journal of Computer and System Sciences_, 69(3):485-497, 2004a.
* Fakcharoenphol et al. (2004b) Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. _J. Comput. Syst. Sci._, 69(3):485-497, 2004b. doi: 10.1016/J.JCSS.2004.04.011. URL [https://doi.org/10.1016/j.jcss.2004.04.011](https://doi.org/10.1016/j.jcss.2004.04.011).
* Flamary et al. (2021) Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Leo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8, 2021. URL [http://jmlr.org/papers/v22/20-451.html](http://jmlr.org/papers/v22/20-451.html).
* Gottlieb et al. (2011) Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient regression in metric spaces via approximate lipschitz extension. _CoRR_, abs/1111.4470, 2011. URL [http://arxiv.org/abs/1111.4470](http://arxiv.org/abs/1111.4470).
* Gottlieb et al. (2011)Leslie Greengard and June-Yub Lee. Accelerating the nonuniform fast fourier transform. _SIAM Rev._, 46(3):443-454, 2004. doi: 10.1137/S003614450343200X. URL [https://doi.org/10.1137/S003614450343200X](https://doi.org/10.1137/S003614450343200X).
* Hanocka et al. (2019) Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. Meshcnn: a network with an edge. _ACM Transactions on Graphics (ToG)_, 38(4):1-12, 2019.
* Kay et al. (2017) Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.
* Khan et al. (2008) Maleq Khan, Fabian Kuhn, Dahlia Malkhi, Gopal Pandurangan, and Kunal Talwar. Efficient distributed approximation algorithms via probabilistic tree embeddings. In Rida A. Bazzi and Boaz Patt-Shamir, editors, _Proceedings of the Twenty-Seventh Annual ACM Symposium on Principles of Distributed Computing, PODC 2008, Toronto, Canada, August 18-21, 2008_, pages 263-272. ACM, 2008. doi: 10.1145/1400751.1400787. URL [https://doi.org/10.1145/1400751.1400787](https://doi.org/10.1145/1400751.1400787).
* Kircheis et al. (2023) Melanie Kircheis, Daniel Potts, and Manfred Tasche. Nonuniform fast fourier transforms with nonequispaced spatial and frequency data and fast sinc transforms. _Numer. Algorithms_, 92(4):2307-2339, 2023. doi: 10.1007/S11075-022-01389-6. URL [https://doi.org/10.1007/s11075-022-01389-6](https://doi.org/10.1007/s11075-022-01389-6).
* Kloster and Gleich (2023) Kyle Kloster and David F. Gleich. A nearly-sublinear method for approximating a column of the matrix exponential for matrices from large, sparse networks. In _Algorithms and Models for the Web Graph: 10th International Workshop, WAW 2013, Cambridge, MA, USA, December 14-15, 2013, Proceedings_, page 68-79, Berlin, Heidelberg, 2023. Springer-Verlag. ISBN 978-3-319-03535-2. doi: 10.1007/978-3-319-03536-9_6. URL [https://doi.org/10.1007/978-3-319-03536-9_6](https://doi.org/10.1007/978-3-319-03536-9_6).
* Koutis and Miller (2007) Ioannis Koutis and Gary L. Miller. A linear work, o(n1/6) time, parallel algorithm for solving planar laplacians. In _ACM-SIAM Symposium on Discrete Algorithms_, 2007. URL [https://api.semanticscholar.org/CorpusID:9556271](https://api.semanticscholar.org/CorpusID:9556271).
* Koutis and Miller (2008) Ioannis Koutis and Gary L. Miller. Graph partitioning into isolated, high conductance clusters: theory, computation and applications to preconditioning. In _ACM Symposium on Parallelism in Algorithms and Architectures_, 2008. URL [https://api.semanticscholar.org/CorpusID:2152215](https://api.semanticscholar.org/CorpusID:2152215).
* Koutis et al. (2010) Ioannis Koutis, Gary L. Miller, and Richard Peng. Approaching optimality for solving sdd systems, 2010.
* Koutis et al. (2011a) Ioannis Koutis, Gary Miller, and Richard Peng. A nearly-mlogn time solver for sdd linear systems, 2011a.
* Koutis et al. (2011b) Ioannis Koutis, Gary L. Miller, and David Tolliver. Combinatorial preconditioners and multilevel solvers for problems in computer vision and image processing. _Comput. Vis. Image Underst._, 115(12):1638-1646, dec 2011b. ISSN 1077-3142. doi: 10.1016/j.cviu.2011.05.013. URL [https://doi.org/10.1016/j.cviu.2011.05.013](https://doi.org/10.1016/j.cviu.2011.05.013).
* Koutis et al. (2012) Ioannis Koutis, Gary L. Miller, and Richard Peng. A fast solver for a class of linear systems. _Commun. ACM_, 55(10):99-107, oct 2012. ISSN 0001-0782. doi: 10.1145/2347736.2347759. URL [https://doi.org/10.1145/2347736.2347759](https://doi.org/10.1145/2347736.2347759).
* Kriege et al. (2020) Nils M. Kriege, Fredrik D. Johansson, and Christopher Morris. A survey on graph kernels. _Applied Network Science_, 5(1), January 2020. ISSN 2364-8228. doi: 10.1007/s41109-019-0195-3. URL [http://dx.doi.org/10.1007/s41109-019-0195-3](http://dx.doi.org/10.1007/s41109-019-0195-3).
* Leeb (2016) William Leeb. Approximating snowflake metrics by trees. _Applied and Computational Harmonic Analysis_, 45, 12 2016. doi: 10.1016/j.acha.2016.10.002.
* Luo et al. (2021) Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 22795-22807, 2021.
* Liu et al. (2018)Bruce M. Maggs, Gary L. Miller, Ojas D. Parekh, Ramamoorthi Ravi, Sha-Sha Leung, and Maverick Woo. Solving symmetric diagonally-dominant systems by preconditioning. 2003. URL [https://api.semanticscholar.org/CorpusID:1593940](https://api.semanticscholar.org/CorpusID:1593940).
* Martinsson [2019] Per-Gunnar Martinsson. Randomized methods for matrix computations, 2019.
* Memoli [2011] Facundo Memoli. Gromov-wasserstein distances and the metric approach to object matching. _Foundations of computational mathematics_, 11:417-487, 2011.
* Moler and Van Loan [2003] Cleve Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later. _SIAM Review_, 45(1):3-49, 2003. doi: 10.1137/S00361445024180. URL [https://doi.org/10.1137/S00361445024180](https://doi.org/10.1137/S00361445024180).
* Moore [2011] Gerald Moore. Orthogonal polynomial expansions for the matrix exponential. _Linear Algebra and its Applications_, 435(3):537-559, 2011. ISSN 0024-3795. doi: [https://doi.org/10.1016/j.laa.2010.09.021](https://doi.org/10.1016/j.laa.2010.09.021). URL [https://www.sciencedirect.com/science/article/pii/S0024379510004842](https://www.sciencedirect.com/science/article/pii/S0024379510004842). Special Issue: Dedication to Pete Stewart on the occasion of his 70th birthday.
* Morris et al. [2020] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In _ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)_, 2020. URL www.graphlearning.io.
* Mossel [2007] Elchanan Mossel. Distorted metrics on trees and phylogenetic forests. _IEEE ACM Trans. Comput. Biol. Bioinform._, 4(1):108-116, 2007. doi: 10.1109/TCBB.2007.1010. URL [https://doi.org/10.1109/TCBB.2007.1010](https://doi.org/10.1109/TCBB.2007.1010).
* Nikolentzos et al. [2021] Giannis Nikolentzos, Giannis Siglidis, and Michalis Vazirgiannis. Graph kernels: A survey. _Journal of Artificial Intelligence Research_, 72:943-1027, November 2021. ISSN 1076-9757. doi: 10.1613/jair.1.13225. URL [http://dx.doi.org/10.1613/jair.1.13225](http://dx.doi.org/10.1613/jair.1.13225).
* Peyre and Cuturi [2019] Gabriel Peyre and Marco Cuturi. Computational optimal transport. _Found. Trends Mach. Learn._, 11(5-6):355-607, 2019. doi: 10.1561/22000000073. URL [https://doi.org/10.1561/22000000073](https://doi.org/10.1561/22000000073).
* Pfaff et al. [2021] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning mesh-based simulation with graph networks. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL [https://openreview.net/forum?id=roNqYLO_XP](https://openreview.net/forum?id=roNqYLO_XP).
* Rahimi and Recht [2007] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, _Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007_, pages 1177-1184. Curran Associates, Inc., 2007.
* Saad and Schultz [1986] Youcef Saad and Martin H. Schultz. Gmres: A generalized minimal residual algorithm for solving nonsymmetric linear systems. _SIAM Journal on Scientific and Statistical Computing_, 7(3):856-869, 1986. doi: 10.1137/0907058. URL [https://doi.org/10.1137/0907058](https://doi.org/10.1137/0907058).
* Shewchuk [1994] Jonathan R Shewchuk. An introduction to the conjugate gradient method without the agonizing pain. Technical report, USA, 1994.
* Sainath et al. [2015] Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 3088-3096, 2015. URL [https://proceedings.neurips.cc/paper/2015/hash/851300ee84c2b80ed40f51ed26d8866fc-Abstract.html](https://proceedings.neurips.cc/paper/2015/hash/851300ee84c2b80ed40f51ed26d8866fc-Abstract.html).
* Spielman and Teng [2008] Daniel A. Spielman and Shang-Hua Teng. A local clustering algorithm for massive graphs and its application to nearly-linear time graph partitioning, 2008.
* Spielman and Teng [2010] Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs, 2010.
* Spielman and Teng [2008]Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems, 2012.
* Thomas et al. (2018) Anna T. Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher Re. Learning compressed transforms with low displacement rank. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 9066-9078, 2018.
* Thorup (1997) Mikkel Thorup. Undirected single source shortest path in linear time. In _38th Annual Symposium on Foundations of Computer Science, FOCS '97, Miami Beach, Florida, USA, October 19-22, 1997_, pages 12-21. IEEE Computer Society, 1997. doi: 10.1109/SFCS.1997.646088. URL [https://doi.org/10.1109/SFCS.1997.646088](https://doi.org/10.1109/SFCS.1997.646088).
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017.
* Vayer et al. (2018) Titouan Vayer, Laetitia Chapel, Remi Flamary, Romain Tavenard, and Nicolas Courty. Fused gromov-wasserstein distance for structured objects: theoretical foundations and mathematical properties. _CoRR_, abs/1811.02834, 2018. URL [http://arxiv.org/abs/1811.02834](http://arxiv.org/abs/1811.02834).
* Yin (2000) Ailong Zheng Victor Y. Pan. Superfast algorithms for cauchy-like matrix computations and extensions. _Linear Algebra and its Applications_, 310(1-3):83-108, 2000. URL [https://www.sciencedirect.com/science/article/pii/S0024379500000410](https://www.sciencedirect.com/science/article/pii/S0024379500000410).
* Wang (2021) Ziheng Wang. Sparsednn: Fast sparse deep learning inference on cpus. _CoRR_, abs/2101.07948, 2021. URL [https://arxiv.org/abs/2101.07948](https://arxiv.org/abs/2101.07948).
* Williams (2007) Ryan Williams. Matrix-vector multiplication in sub-quadratic time: (some preprocessing required). In _Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '07, page 995-1001, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245.
* Wu et al. (2015) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes, 2015.
* Yu et al. (2020) Fuxun Yu, Zirui Xu, Tong Shen, Dimitrios Stamoulis, Longfei Shangguan, Di Wang, Rishi Madhok, Chunshui Zhao, Xin Li, Nikolaos Karianakis, Dimitrios Lymberopoulos, Ang Li, Chenchen Liu, Yiran Chen, and Xiang Chen. Towards latency-aware DNN optimization with GPU runtime analysis and tail effect elimination. _CoRR_, abs/2011.03897, 2020. URL [https://arxiv.org/abs/2011.03897](https://arxiv.org/abs/2011.03897).
* Yu et al. (2022) Fuxun Yu, Di Wang, Longfei Shangguan, Minjia Zhang, Chenchen Liu, and Xiang Chen. A survey of multi-tenant deep learning inference on GPU. _CoRR_, abs/2203.09040, 2022. doi: 10.48550/ARXIV.2203.09040. URL [https://doi.org/10.48550/arXiv.2203.09040](https://doi.org/10.48550/arXiv.2203.09040).
* Zhou and Jacobson (2016) Qingnan Zhou and Alec Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. _arXiv preprint arXiv:1605.04797_, 2016.

Theoretical results

In this section, we provide proofs of all theoretical results in the paper.

### Proof of Lemma 3.1

Proof.: We will apply Lemma 7.19 from [Cygan et al., 2015] (that we provide also below for reader's convenience) and its algorithmic proof. We refer to Cygan et al. [2015] for a definition of the related graph terms.

**Lemma A.1**.: _Assume that \(\mathrm{G}\) is a graph of treewidth at most \(k\), and consider a nonnegative weight function \(\mathbf{w}:\mathrm{V}(\mathrm{G})\rightarrow\mathbb{R}_{\geq 0}\) on the vertices of \(\mathrm{G}\). Then in \(\mathrm{G}\) there exists a \(\frac{1}{2}\)-balanced separator \(X\) of size at most \(k+1\)._

Note first that for each rooted tree, we can compute the size of each of its rooted sub-trees (and store it in the root of the sub-tree) in the linear time, simply by applying dynamic programming. We can now apply the above lemma for the tree \(\mathrm{G}=\mathcal{K}\) with the weight function that assigns weight \(w=1.0\) for each vertex. By following its algorithmic proof (and using breadth first search for tree exploration), we can obtain a node \(p\) and sub-trees \(T_{1},...,T_{l}\) rooted in vertices connected with \(p\), with the following properties:

* \(\mathrm{V}(T_{1})\cup...\cup\mathrm{V}(T_{l})\cup\{p\}=\mathrm{V}(\mathcal{K})\),
* \(|\mathrm{V}(T_{i})|\leq\frac{1}{2}|\mathrm{V}(\mathcal{K})|\) for \(i=1,...,l\) and where \(||\) stands for the set size.

We then choose the first index \(k\) such that \(|\mathrm{V}(T_{1})|+...+|\mathrm{V}(T_{k})|\geq\frac{3}{4}|\mathrm{V}(\mathcal{ K})|\). Note that such an index \(k\) exists and \(k>1\) because of the above and the fact that our tree has at least six vertices. We define as \(\mathcal{K}_{\mathrm{left}}\) a sub-tree of \(\mathcal{K}\) induced by the set: \(\mathrm{V}(T_{1})\cup...\mathrm{V}(T_{k-1})\cup\{p\}\) and by \(\mathcal{K}_{\mathrm{right}}\) a sub-tree of \(\mathcal{K}\) induced by the set: \(\mathrm{V}(T_{k})\cup...\mathrm{V}(T_{l})\cup\{p\}\). Note that the triple \((\mathcal{K}_{\mathrm{left}},\mathcal{K}_{\mathrm{right}},p)\) satisfies the requirements of Lemma 3.1. That completes the proof. 

### Fast Approximate Tree-Field Integrators

If matrices \(\mathbf{M}=[f(x_{i}+y_{j})]_{i=1,...,a}^{j=1,...,b}\) from Sec. 3.2.1 do not support fast matrix-vector multiplication, the question arises whether fast approximate procedures can be applied.

#### a.2.1 Random Fourier Features (RFFs)

Assume that the Fourier Transform (FT) of \(f\) exists and denote it by \(\tau:\mathbb{C}\rightarrow\mathbb{C}\). Note that \(f\) is the inverse FT of \(\tau\) and can be re-written as \(f(z)=\int_{\mathbb{R}}\exp(2\pi\mathbf{i}\omega z)\tau(\omega)d\omega\). Therefore, the following holds:

\[f(x_{i}+y_{j})=\int_{\mathbb{R}}\exp(2\pi\mathbf{i}\omega x_{i})\exp(2\pi \mathbf{i}\omega y_{j})\tau(\omega)d\omega. \tag{8}\]

We conclude that for any probabilistic distribution \(\mathcal{P}\) on \(\mathbb{R}\) with pdf \(p\), \(f(x_{i}+y_{j})\) can be re-written as: \(f(x_{i}+y_{j})=\mathbb{E}[\mu(x_{i})^{\top}\mu(y_{j})]\), where random \(\mu:\mathbb{R}\rightarrow\mathbb{R}^{m}\) is given as: \(\mu(t)^{\top}=\frac{1}{\sqrt{m}}\left(\sqrt{\frac{\tau(\omega_{l})}{p(\omega_ {l})}}\exp(2\pi\mathbf{i}\omega_{l}t)\right)_{l=1}^{m}\) for \(\omega_{1},...,\omega_{m}\sim\mathcal{P}\) and \(m\in\mathbb{N}_{+}\). Thus matrix \(\mathbf{M}\) can be unbiasedly approximated as: \(\mathbf{M}\approx\mathbf{U}\mathbf{W}^{\top}\) for \(\mathbf{U}\in\mathbb{R}^{a\times m}\), \(\mathbf{W}\in\mathbb{R}^{b\times m}\) with rows given by \(\mu(x_{1})^{\top},...,\mu(x_{a})^{\top}\) and \(\mu(y_{1})^{\top},...,\mu(y_{b})^{\top}\) respectively. Matrix-vector product \(\mathbf{M}\mathbf{v}\) can be then unbiasedly approximated as \(\mathbf{U}(\mathbf{W}^{\top}\mathbf{v})\) and computed in time \(O((a+b)m)\). For \(m\ll\frac{ab}{a+b}\), substantial computational gains are obtained. In particular, if \(m=O(\log^{d}(a+b))\), the approximate \(f\)-integration is conducted in time \(O(N\log^{d+1}(N))\). Note that \(m\) controls estimator's variance, thus decreasing \(m\) increases the error.

#### a.2.2 Non-Uniform FFT (NU-FFT)

We will now propose a closely-related method, that relies on the non-uniform FFT (NU-FFT).2Denote: \(\mathbf{g}=\mathbf{Mv}\) for a given \(\mathbf{v}=(v_{1},...,v_{b})^{\top}\in\mathbb{R}^{b}\). Define: \(g(x)=\int_{R}f(x-z)P(z)dz\), where \(P\) is given as: \(P(z)=\sum_{j=1}^{b}v_{j}\delta(z-z_{j})\), and furthermore: (1) \(\delta\) is a _delta-Dirac_ function, (2) \(z_{j}=-y_{j}\). Our goal is to efficiently evaluate function \(g\) in points: \(\{x_{1},...,x_{a}\}\).

Assume that the inverse FT of \(g\) exists and denote it by \(\eta:\mathbb{C}\rightarrow\mathbb{C}\). Note that \(g\) is the FT of \(\eta\) and can be written as: \(g(x)=\int_{\mathbb{R}}\eta(\omega)\exp(-2\pi\mathbf{i}\omega x)d\omega\). Since \(g\) is also a convolution of \(f\) and \(P\), \(\eta\) is a product of the inverse FTs: \(\rho\) and \(R\) respectively. Therefore, we can write: \(g(x)=\int_{\mathbb{R}}\rho(\omega)R(\omega)\exp(-2\pi\mathbf{i}\omega x)d\omega\), where \(R(\omega)=\sum_{j=1}^{b}v_{j}\exp(2\pi\mathbf{i}\omega z_{j})\). Now, function \(g\) can be evaluated for \(\{x_{1},\ldots,x_{a}\}\) as follows: (1) a quadrature method is applied to obtain points: \(\omega_{1},...,\omega_{r}\) (and corresponding weights) for the approximate computation of the integral defining \(g\), (2) the NU-FFT is applied to compute \(R(\omega)\) simultaneously in those points in polylog-linear time, (3) given pre-computed \((\rho(\omega_{i})R(\omega_{i}))_{i=1}^{r}\) (and the quadrature weights), NU-FFT is applied again to compute quadrature-based approximation of \(g\).

The \(f\)-integration process applying this method runs in polylog-linear time since the computation of \(\mathbf{g}=\mathbf{Mv}\) takes polylog-linear time. A prominent application is \(f\) given as: \(f(x)=\frac{\sin(x)}{x}\), with \(\rho\) being a renormalized indicator of belonging to interval \([-0.5,0.5]\). In this setting, the integral defining \(g\) is thus limited to \([-0.5,0.5]\). Interestingly, for \(f(x)=\frac{\sin(x)}{x}\) we can also apply methods from Sec. 3.2.1 (see: our discussion below on the trigonometric case).

#### a.2.3 Additional implications of Lemma 3.3

Products of exponentials and polynomials:Note that a Hadamard (element-wise) product of two outer-product matrices is itself an outer-product matrix. Using the analysis from the polynomial and exponential cases, we conclude that \(\mathbf{M}\) is a sum of a constant number of terms, each being an outer-product matrix. Thus the same conclusion follows.

The case of the trigonometric \(f\):If \(f(x)=\cos(x)\) then it can be re-written as: \(f(x)=\frac{\exp(\mathbf{i}x)+\exp(-\mathbf{i}x)}{2}\). Observe that the cordiality property is preserved under linear combination of the finite number of cordial functions. We can thus conclude that analogous results as the above for \(f(x)=\exp(\lambda x)\) can be derived for \(f(x)=\cos(x)\). That is also the case for \(f(x)=\sin(x)\) that can be re-written as: \(f(x)=\frac{\exp(\mathbf{i}x)-\exp(-\mathbf{i}x)}{2\mathbf{i}}\). In both cases, we extend the domain from \(\mathbb{R}\) to \(\mathbb{C}\), but this does not affect the analysis.

So far we have not put any restrictions on the tree weights. If we restrict all weights to be the same (without loss of generality, equal to one), then the problem becomes easier. In this case for any function \(f\), matrices \(\mathbf{C}\) and \(\mathbf{C}^{\top}\) are Hankel [1] (constant on each anti-diagonal and belonging to LDR class). Then, matrix-vector multiplication can be done in \(O((a+b)\log(a+b))\). The analysis from the proof of Lemma 3.3 for \(d=1\) can be repeated. We conclude that \(f\)-integration can be conducted in \(O(N\log^{2}(N))\) time for \(N\)-vertex unweighted trees and any \(f:\mathbb{R}\rightarrow\mathbb{R}\). This was already proven in [1].

Trees with positive rational weights:Assume that tree weights take values of the form: \(\{\frac{\epsilon}{q}:e\in\{1,...,p\}\}\) for some \(p,q\in\mathbb{N}_{+}\). Then, matrices \(\mathbf{C}\) and \(\mathbf{C}^{\top}\) do not need to be Hankel, but can be embedded into Hankel matrices with rows/columns corresponding to distances \(\frac{l}{q}\) from the pivot, where \(l=\{0,...,mp\}\) and \(\frac{mp}{q}\) is the largest distance between a vertex and the pivot. Tensor \(\mathbf{X}\) can also be padded into a larger one with extra rows/columns (corresponding to unrealized distances) set to zero. If \(p\) is constant, the asymptotic time complexity remains the same as in the previous case, but the algorithm might not be practical since the number of rows and columns grows by a multiplicative factor of \(p\). For certain non-cordial \(f\), the algorithm can be modified for potential gains.

## Appendix B Additional Related Work

In this section we provide additional related works. One of the methods to tackle this problem is via iterative methods [11] like Arnoldi iteration [1], Conjugate Gradient [10] and the celebrated Spielman-Teng algorithm [14] for symmetric diagonally dominant (SDD) matrices. There are a number of extensions and variations of the above methods [1, 15, 16], Christiano et al., 20, Koutis and Miller, 2007; Spielman and Teng, 2008; Daitch and Spielman, 2008; Koutis and Miller, 2008).They mainly take into account the structure of the matrix (SDD) (Koutis et al., 2010, 2011a, 2012), embedding of a graph into low stretch spanning trees (Elkin et al., 2005), graph sparsification (Spielman and Teng, 2010) and the choice of a good _pre-conditioner_(Maggs et al., 2003; Koutis et al., 2011b). We want to emphasize that the research on low stretch trees for general graphs is orthogonal to the main topic of this work. In our manuscript, we show in particular how to conduct efficient integration on arbitrary trees. Thus our work can be naturally combined with those algorithms to leverage all the above low stretch tree constructions for a better approximation of the graph's metric.

The other class of method comes from the celebrated work of (Al-Mohy and Higham, 2011) and there are a number of extensions of this work (Kloster and Gleich, 2023; Al-Mohy and Higham, 2010; Moore, 2011; Moler and Van Loan, 2003; Auckenthaler et al., 2010).

Another class of methods is via sampling, where one samples a subset of a large matrix, which is then used to approximate the matrix-vector multiplication (i.e. Monte Carlo sampling) methods (Drineas et al., 2006; Drineas and Kannan, 2001; Acebron, 2019; Acebron et al., 2019; Benzi et al., 2017; Martinsson, 2019).

We note that none of these methods are directly applicable in our cases as our \(f\)-matrix is neither Hermitian or SDD. The randomized algorithms are harder to use in the setting of training of a neural network. Moreover our method is _exact_ on _trees_, where all the above methods are approximations.

## Appendix C Topological Transformers

```
Query/key matrices: \(\mathbf{Q},\mathbf{K}\in\mathbb{R}^{L\times d_{QK}}\), value matrix \(\mathbf{V}\in\mathbb{R}^{L\times d}\), mask \(\mathbf{M}\in\mathbb{R}^{L\times L}\), procedure \(\mathrm{FastMult}_{\mathbf{M}}:\mathbb{R}^{L}\rightarrow\mathbb{R}^{L}\) calculating \(\mathbf{M}\mathbf{x}\) (or its approximation) for the input \(\mathbf{x}\in\mathbb{R}^{L}\), kernel feature map: \(\phi:\mathbb{R}^{d_{QK}}\rightarrow\mathbb{R}^{m}\). \(\mathrm{vec}(\cdot)\) denotes vectorization. Output:Masked low-rank attention embeddings using \(\phi\).
1. Compute matrices \(\mathbf{V}^{1}\in\mathbb{R}^{L\times(md)}\), \(\mathbf{V}^{2}\in\mathbb{R}^{L\times m}\) with rows defined as: \(\mathbf{V}^{1}_{i:}=\mathrm{vec}(\phi(\mathbf{k}^{\top}_{i})\mathbf{v}_{i})\), \(\mathbf{V}^{2}_{i:}=\phi(\mathbf{k}^{\top}_{i})^{\top}\), where \(\mathbf{k}_{i}\)/\(\mathbf{v}_{i}\) stands for the ith row of \(\mathbf{K}\)/\(\mathbf{V}\).
2. \(\tilde{\mathbf{D}}^{1}:=[\mathrm{FastMult}_{\mathbf{M}}(\mathbf{V}^{1}_{:1}),...,\mathrm{FastMult}_{\mathbf{M}}(\mathbf{V}^{1}_{:md})]\in\mathbb{R}^{L \times md}\), \(\tilde{\mathbf{D}}^{2}:=[\mathrm{FastMult}_{\mathbf{M}}(\mathbf{V}^{2}_{:1}),...,\mathrm{FastMult}_{\mathbf{M}}(\mathbf{V}^{2}_{:m})]\in\mathbb{R}^{L\times m}\) for \(\mathbf{V}^{1/2}_{:i}\) denoting ith column of \(\mathbf{V}^{1/2}\).
3. Output the embedding \(\mathbf{r}_{i}\) of the ith tokens as: \(\mathbf{r}_{i}=\frac{\phi(\mathbf{q}^{\top}_{i})^{\top}\mathrm{dvec}(\tilde{ \mathbf{D}}^{1}_{:i})}{\phi(\mathbf{q}^{\top}_{i})^{\top}(\tilde{\mathbf{D}}^{ 2}_{:i})^{\top}}\), where \(\mathbf{q}_{i}\) is the ith row of \(\mathbf{Q}\) and \(\mathrm{dvec}(\cdot)\) devectorizes its input back to \(\mathbb{R}^{m\times d}\).
```

**Algorithm 1** General Efficient Low-Rank Masked Attention from Choromanski et al. (2022)

We now recall the formulation of general masked transformers.

Let us denote by \(L\) the number of input tokens. The attention used in a regular Transformer linearly projects their representations into three learnable matrices \(\mathbf{Q},\mathbf{K}\in\mathbb{R}^{L\times d_{QK}}\), \(\mathbf{V}\in\mathbb{R}^{L\times d}\) called _queries_, _keys_ and _values_ respectively.

**Definition C.1** (general masked attention).: _General masked attention is given by the following equation, where \(\mathbf{M}\in\mathbb{R}^{L\times L}\) is the mask matrix, and \(\mathbf{A}\in\mathbb{R}^{L\times L}\) is the so-called masked attention matrix (MAM): which is defined as:_

\[\mathrm{Att}_{\mathrm{K}}(\mathbf{Q},\mathbf{K},\mathbf{V},\mathbf{M})=\mathbf{ D}^{-1}\mathbf{A}\mathbf{V}, \tag{9}\]

_where \(\odot\) denotes the element-wise (Hadamard) matrix product, \(\mathrm{K}:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}\) is some kernel function and \(\mathrm{K}(\mathbf{Q},\mathbf{K})\) is a kernel matrix defined as: \(\mathrm{K}(\mathbf{Q},\mathbf{K})_{i,j}=\mathrm{K}(\mathbf{q}^{\top}_{i}, \mathbf{k}^{\top}_{j})\) for the \(ith\) row \(\mathbf{q}_{i}\) of \(\mathbf{Q}\) and the jth row \(\mathbf{k}_{j}\) of \(\mathbf{K}\) respectively. We call \(\mathbf{A}^{\prime}=\mathrm{K}(\mathbf{Q},\mathbf{K})\) the unmasked attention matrix (UAM). Note that when \(\mathrm{K}\) is the softmax function, we recover the well-known attention mechanism in Transformers._

Here \(\mathbf{1}_{L}\) is the all-ones vector of length \(L\), and \(\mathrm{diag}(\cdot)\) is a diagonal matrix with the input vector as the diagonal. The time complexity of computing (9) is \(O(L^{2}d)\).

If the kernel \(\mathrm{K}\) admits (at least in expectation) a dot-product decomposition, i.e. \(\mathrm{K}(\mathbf{x},\mathbf{y})=\mathbb{E}[\phi(\mathbf{x})^{\top}\phi(\mathbf{y})]\) for some mapping: \(\phi:\mathbb{R}^{d_{QK}}\rightarrow\mathbb{R}^{m}\) (and some \(m>0\)). \(\phi(\mathbf{u})\) is called a _(random) feature map_ (RFM) for \(\mathbf{u}\in\mathbb{R}^{d}\). For \(\mathbf{Q}^{\prime},\mathbf{K}^{\prime}\in\mathbb{R}^{L\times m}\) with rows given as \(\phi(\mathbf{q}_{i}^{\top})^{\top}\) and \(\phi(\mathbf{k}_{i}^{\top})^{\top}\) respectively, RFM-based kernel linearization leads directly to the efficient unmasked attention mechanism of the form:

\[\widehat{\mathrm{Att}_{\mathrm{K}}}(\mathbf{Q},\mathbf{K},\mathbf{ V}) =\widehat{\mathbf{D}}^{-1}(\mathbf{Q}^{\prime}((\mathbf{K}^{\prime })^{\top}\mathbf{V})), \tag{10}\] \[\widehat{\mathbf{D}} =\mathrm{diag}(\mathbf{Q}^{\prime}((\mathbf{K}^{\prime})^{\top} \mathbf{1}_{L})).\]

Here \(\widehat{\mathrm{Att}_{\mathrm{K}}}\) stands for the approximate attention and brackets indicate the order of computations. Such a mechanism is characterized by time complexity \(O(Lmd)\) as opposed to \(O(L^{2}d)\) for regular attention. If \(m\ll L\), computational gains are obtained.

The central question in (Choromanski et al., 2022) was how to incorporate the masking in the linear attention as above. Note that in this case \(\mathbf{A}^{\prime}\) is never materialized. Building on the work of (Luo et al., 2021), the authors (Choromanski et al., 2022) propose a general algorithm that efficiently implements masked linear attention.

In this work, we use different mappings \(\phi\) (see Table 1). Our key contribution in this work is to propose a novel mask matrix \(\mathbf{M}\) and the implementation of a fast matrix multiplication by \(\mathbf{M}\). The above result then allows us to construct novel classes of Topological Transformers.

## Appendix D Experimental Details and Additional Experiments

In this section, we provide additional details regarding the experimental setup and present additional results from our experiments. Our code is available at [https://github.com/brcsomnath/FastTreeIntegrator](https://github.com/brcsomnath/FastTreeIntegrator). Specifically, we provide there the code for: (1) our algorithm leveraging IntegratorTree data structure (depicted in Fig 1), (2) adaptation to the Gromov-Wasserstein-type computation, (3) graph classification and (4) experiments on interpolation on meshes.

### Additional experiments for graph metric approximation with \(f\)-distance matrices

We present additional results for the training loss, relative Frobenius Norm Error (\(\epsilon\)), for more samples from the Thingi10K dataset (to complement the results in Fig. 6). In Fig. 9, we observe that in most cases having rational functions with higher polynomial degrees results in lower training loss.

We also perform similar experiments for graph classification on the CUBES dataset Hanocka et al. (2019). Specifically, we investigate how the polynomial degree affects the graph classification performance in Fig. 9 (left). We observe that increasing the polynomial degree improves the classification accuracy up to a certain degree. For the same dataset, we also compute the training loss for different polynomial degrees in Fig. 9 (right). Similarly, we observe that higher-degree rational functions achieve lower training loss for fitting the polynomial coefficients.

Moreover, we benchmark FTFI on ModelNet10 (Wu et al., 2015), a dataset for 3D Point Cloud (PC) classification. For each PC, we create an \(\epsilon\)-neighborhood-graph and use FTFI for graph classification The Shortest Path kernel achieves an accuracy of \(39.6\%\), whereas our FTFI with the degree-2

Figure 8: Relative Frobenius norm error as a function of the number of training iterations for different sizes \(n\) and learnable quadratic \(f\). We report the results for 3 mesh graphs from Thingi10k.

polynomial improves the accuracy to \(44.2\)% (\(10\)% relative improvement over the baseline), similar to the observation in 9.

### Integration of FTFI into GW-style algorithms

Wasserstein distance has found many uses in ML, particularly due to it's principled approach to compare probability distributions. Gromov Wasserstein Memoli (2011) discrepancy is an extension of Wasserstein distance to graph structured data, with a lot of downstream applications like graph clustering and classification. Inspired by the work of (Choromanski et al., 2023), we follow the exact same procedure in the integration of FTFI in the conditional gradient algorithm. The FTFI can be injected seamlessly in place of the Fast Matrix Multiplication (FMM) algorithms in Algorithm 2 and Algorithm 3 (see (Choromanski et al., 2023)).

Our method GW-FTFI run consistently \(2\)-\(6\)x faster than the baseline methods using the Shortest Path kernel, with _no drop_ in accuracy in computing the associated costs (Figure 10). The plots shown are obtained by averaging over \(10\) seeds and random trees of various sizes. For the baseline experiments, we use the implementation from the POT library (Flamary et al., 2021).

### Interpolation on Meshes

In this section, we present implementation details for the mesh interpolation experiments in Section 4.2. All experiments were run on a computer with an i9-12900k CPU and 64GB memory.

In the vertex normal prediction task in Section 4.2, we choose 40 meshes for 3D-printed objects with a wide range of size from the Thingi10K (Zhou and Jacobson, 2016) dataset with the File IDs:

Figure 10: Comparison of field integration time between GW and FTFI-GW. We observe that FTFI achieves significant computation time gain over the baseline.

Figure 9: **Left**: Variation in FTFI performance with different \(f\)-distance functions on the CUBES dataset. We use general rational functions (GRF) of varying polynomial degrees. GRF(\(i\)) indicates a rational function of the \(i\)-th degree. We observe a general trend of accuracy increase with function complexity up to a certain degree. The coefficients of the GRF were learnt using a few graph instances. **Right**: We show the training loss curves for estimating the coefficients of the rational function, \(f\), for samples in the CUBES dataset. We report the training loss for rational functions with varying polynomial degrees. We observe that the training loss is lower when we use rational functions with high-degree polynomials.

[60246, 85580, 40179, 964933, 1624039, 91657, 79183, 82407, 40172, 65414, 90431, 74449, 73464, 230349, 40171, 61193, 77938, 375276, 39463, 110793, 368622, 37326, 42435, 1514901, 65282, 116878, 550964, 409624, 101902, 73410, 87602, 255172, 98480, 57140, 285606, 96123, 203289, 87601, 409629, 37384, 57084]

For both our FTFI and the baseline BFFI methods, we do a grid-search over the hyper-parameter \(\lambda\) for each mesh and report the pre-processing time associated with the hyper-parameter(s) that give(s) us the best cosine similarity.

### Additional Details on Graph Classification

We conduct graph classification experiments on a wide range of benchmark datasets. We report the dataset statistics for the graph classification datasets in Table 2. More details about the datasets are available in Morris et al. (2020). To evaluate the performance of the different kernels, we employ the framework proposed by (Errica et al., 2020). In particular, 10-fold cross-validation is used to obtain an estimate of the generalization performance of our method and the baseline method. We repeat this cross validation experiment 5 times to get a robust estimation and report the standard deviation for each setup.

To obtain graph features, we follow the approach presented in (de Lara and Pineau, 2018). In this setting, we obtain the \(k\)-smallest eigenvalues from the approximated kernel from FTFI and forward these features to a random forest classifier for classification. For BGFI, we perform the same process obtaining the \(k\)-smallest eigenvalues from the exact shortest kernel. FTFI achieves similar performance to the BGFI while being significantly faster. We tune the hyperparameter \(k\) independently for each method.

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline  & & & Avg. & Avg. & \# Node & \# Node \\ Datasets & \# Graphs & \# Labels & \# Nodes & \# Edges & Labels & Attributes \\ \hline Mutag & 188 & 2 & 17.93 & 19.79 & 7 & - \\ Ptc-Mr & 344 & 2 & 14.29 & 14.69 & 19 & - \\ Enzymes & 600 & 6 & 32.63 & 62.14 & 3 & 18 \\ Proteins & 1113 & 2 & 39.06 & 72.82 & 3 & 1 \\ D\&D & 1178 & 2 & 284.32 & 715.66 & 82 & - \\ Imdb Binary & 1000 & 2 & 19.77 & 96.53 & - & - \\ Imdb Multi & 1500 & 3 & 13.0 & 65.94 & - & - \\ NCI1 & 4110 & 2 & 29.87 & 32.30 & 37 & - \\ Collab & 5000 & 3 & 74.49 & 2457.78 & - & - \\ Reddit Binary & 2000 & 2 & 429.63 & 497.75 & - & - \\ Reddit Multi-5k & 4999 & 5 & 508.52 & 594.87 & - & - \\ Reddit Multi-12k & 11929 & 11 & 391.41 & 456.89 & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 2: Statistics of the graph classification datasets used in this paper.

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline  & & \multicolumn{6}{c}{Datasets} \\ \cline{2-7}  & IMDB & IMDB & Reddit & Reddit & Reddit & Reddit \\ Algorithm & Binary & Multi & Binary & Multi-5K & Multi-12K & Collab \\ \hline BGFI & 5.6 & 7.6 & 3371.9 & 6267.6 & 8086.3 & 209.1 \\ FTFI & 4.3 & 4.7 & 338.2 & 755.3 & 1959.5 & 232.4 \\ \hline Improvement & +23.2\% & +38.2\% & +90.0\% & +88.0\% & +75.8\% & -11.1\% \\ \hline \hline \multicolumn{7}{c}{Datasets} \\ \cline{2-7} Algorithm & Mutag & Enzymes & NCI1 & Ptc-Mr & D\&D & Proteins \\ \hline BGFI & 0.88 & 3.68 & 32.8 & 0.89 & 715.4 & 14.9 \\ FTFI & 0.37 & 4.39 & 20.2 & 0.93 & 325.3 & 18.6 \\ \hline Improvement & +58.0\% & -19.3\% & +38.4\% & -4.5\% & +54.5\% & -24.8\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Feature processing time of FTFI compared to exact shortest path kernel computation. We observe that FTFI achieves significant speedups up to 90% reduction in processing time. All times are reported in seconds (s).

In Table 4, we report the results for a wide range of baselines and compare FTFI. We observe that FTFI achieves competitive performance among various strong kernel-based classification baseline approaches. Note that FTFI results are not directly comparable with other approaches, as FTFI constructs an intra-graph kernel while other methods use inter-graph kernels. Despite the aforementioned considerations, we contend that positioning our results within the broader framework of alternative methodologies demonstrates that FTFI remains a compelling approach, owing to its speed and comparable classification accuracy.

### Additional details on experiments for Topological transformers

In this subsection, we provide additional training details for our image classification tasks. Table 5 and table 6 present the architectural as well as the training details.

We train the ViT models starting from their pretrained checkpoint (pretrained on ImageNet-21k). We replace the dense attention in ViT by the Performer attention (see Equation 10). We use Algorithm 1 to efficiently incorporate the mask matrix \(\mathbf{M}\) in the attention mechanism.

#### d.5.1 ImageNet

We have already provided comparison with SOTA efficient-attention methods: low-rank attention Transformers in Sec 4.4, quality-wise. On standard ImageNet benchmark, our best Transformer with FTFI provide 78.15\(\%\) accuracy, as compared to 76.37\(\%\) of the best low-rank -attention variant (obtained by testing three different linear variants). That gives 1.78\(\%\) accuracy improvement with only 3 extra trainable parameters per head (36 extra trainable parameters per layer). We have also run the experiments with cosFormer. It achieved 76.3\(\%\) accuracy (consistent with what is reported in the literature, see [8]), lower than both: our method and the best tested low-rank attention variant. The RF-Gate-Gaussian achieved 76.35\(\%\) accuracy, which is is still lower than both: FTFI and the best tested low-rank attention variant.

#### d.5.2 Places365

We have also conducted tests on another challenging dataset: Places365. In the paper, we report 1.71\(\%\) accuracy improvement over low-rank attention Transformer (56.51\(\%\) accuracy vs 54.8\(\%\) accuracy). For the rebuttal, we also run the experiment with cosFormer which achieved 55.4\(\%\) accuracy (consistent with what is reported in the literature, see: [8]). This is still 0.93\(\%\) behind our method. The RF-Gate-Gaussian achieved accuracy 55.1\(\%\), lower than this of cosFormer.

#### d.5.3 I-naturalist 2017

I-naturalist is yet another challenging dataset, with 10K classes, diverse image quality and significant class imbalance. Transformer with FTFI provides 1\(\%\) accuracy improvement over its regular low-rank attention counterpart and the cosFormer. Furthermore, FTFI achieved 0.8\(\%\) improvement over RF-Gate-Gaussian. The convergence of the FTFI variant is 20-23\(\%\) faster than this of its regular low-rank attention counterpart, the cosFormer and RF-Gate-Gaussian.

### Video Vision Transformer

ViViT ([17]) is a novel architecture that adapts the Vision Transformer (ViT) for video processing. It efficiently handles the spatiotemporal dimensions of video data by factorizing the input and applying attention mechanisms across both space and time. This allows ViViT to capture complex motion patterns and long-range dependencies in videos.

Applying FTFI with a topological masking mechanism to the ViViT architecture (factorized Transformer model variant, trained from scratch, as described in Arnab et al. [2021]) results in a \(\mathbf{0.8}\%\) absolute improvement on the Kinetics dataset ([13]). The experimental setup follows Arnab et al. [2021]. To the best of our knowledge, this is the first application of Topological Transformers to video data.

[MISSING_PAGE_FAIL:24]

## Appendix E Broader Impact

We do believe that the potential impact of this work is significant, as providing both: (a) theoretical advancements in structural graph theory as well as (b) practical applications in (1) designing computationally efficient Transformers leveraging topological inductive priors, (2) graph classification and (3) interpolation on manifolds. The core problem of fast multiplication with \(f\)-distance matrices plays an important role in various fields: physical sciences, chemistry, and network sciences. Our main contributions are algorithmic, with no clear negative side effects. While used in the context of Transformers, they should be though applied cautiously due to the nontrivial carbon emission footprint associated with training large Transformer models.

## Appendix F Limitations

Currently, FTFI can be applied on general graphs via certain classes of trees defined on these graphs (e.g. spanning trees), with low-distortion trees being more preferable. It would be interesting to see whether the main concepts used in the FTFI algorithm (such as the theory of balanced separators) can be directly incorporated into efficient and exact algorithms operating on general graphs (or general sparse graphs that appear in most machine learning applications). Determining general conditions on the classes of graphs and functions \(f\) under consideration that are sufficient for exact sub-quadratic time integration is yet another important problem for future work.

\begin{table}
\begin{tabular}{l r} \hline \hline Parameter & Value \\ \hline Activation layer & gelu \\ Dropout prob & \(0.1\) \\ Attention dropout prob & \(0.1\) \\ Optimizer & Adam \\ Learning rate & \(10^{-3}\) \\ Batch Size & \(4096\) \\ Compute resources & \(8\times 8\) TPUv3 \\ Number of Epochs & \(300\) \\ Warmup & \(10\)K \\ weight decay & \(0.1\) \\ learning schedule & cosine decay \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters for Topological Transformer experiments

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline Model & Heads & Layers & Hidden Dim. & MLP Dim. & Params & Patch Size \\ \hline ViT-Base & 12 & 12 & 768 & 3072 & 86M & 16 \\ ViT-Large (16) & 24 & 16 & 1024 & 4096 & 307M & 16 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters for the different ViT models used in this paper

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We give detailed explanations of our contributions in the introduction (page 2). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are clearly explained in Appendix F Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We introduce the notion of our algorithm Fast Tree Field Integrator in section 3. We describe the main algorithm in detail and introduce the technical (theoretical) results. The proofs of these results can be found in Appendix A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Training details to replicate each experiment are in the Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the code as well as details to run our experiments in Appendix D. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details are presented in Sec 4 and Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments in the paper except for the ones using large Transformer models have been run multiple times using various random seeds and we report the relevant statistics. The experiments using Transformers are too expensive to run multiple times as the experiments are run on a huge dataset like ImageNet. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report the compute resources used in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: All authors have reviewed the NeurIPS code of ethics and the research conform to the code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The broader impacts of our work is detailed in Appendix E. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper is theoretical in nature and we are not releasing any new models or data. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have properly cited all the papers that introduced various algorithms and data that are used in this work. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We release the code for the main algorithm. The usage is detailed in the anonymous github repo. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not conduct any research that involves crowd sourcing or with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowd sourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.