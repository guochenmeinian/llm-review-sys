# An Optimal Structured Zeroth-order Algorithm for Non-smooth Optimization

 Marco Rando

MaLGa-DIBRIS, University of Genova, IT (marco.rando@edu.unige.it, lorenzo.rosasco@unige.it).

Cesare Molinari

MaLGa - DIMA, University of Genova, Italy (molinari@dima.unige.it, silvia.villa@unige.it).

Silvia Villa

Istituto Italiano di Tecnologia, Genova, Italy and CBMM - MIT, Cambridge, MA, USA

Lorenzo Rosasco

Istituto Italiano di Tecnologia, Genova, Italy and CBMM - MIT, Cambridge, MA, USA

###### Abstract

Finite-difference methods are a class of algorithms designed to solve black-box optimization problems by approximating a gradient of the target function on a set of directions. In black-box optimization, the non-smooth setting is particularly relevant since, in practice, differentiability and smoothness assumptions cannot be verified. To cope with nonsmoothness, several authors use a smooth approximation of the target function and show that finite difference methods approximate its gradient. Recently, it has been proved that imposing a structure in the directions allows improving performance. However, only the smooth setting was considered. To close this gap, we introduce and analyze O-ZD, the first structured finite-difference algorithm for non-smooth black-box optimization. Our method exploits a smooth approximation of the target function and we prove that it approximates its gradient on a subset of random _orthogonal_ directions. We analyze the convergence of O-ZD under different assumptions. For non-smooth convex functions, we obtain the optimal complexity. In the non-smooth non-convex setting, we characterize the number of iterations needed to bound the expected norm of the smoothed gradient. For smooth functions, our analysis recovers existing results for structured zeroth-order methods for the convex case and extends them to the non-convex setting. We conclude with numerical simulations where assumptions are satisfied, observing that our algorithm has very good practical performances.

## 1 Introduction

Black-box optimization problems are a class of problems for which only values of the target function are available and no first-order information is provided. Typically, these problems arise when the evaluation of the objective function is based on a simulation and no analytical form of the gradient is accessible or its explicit calculation is too expensive [14, 45, 36, 35].

To face these problems, different methods that do not require first-order information have been proposed - see for instance [39, 47, 19, 26, 20, 30, 33] and references therein. These techniques are called derivative-free methods and a wide class of these is the class of finite-difference algorithms [31, 39, 17]. These iterative procedures mimic first-order optimization strategies by replacing the gradient of the objective function with an approximation built through finite differences in random directions.

Two types of finite-difference methods can be identified: unstructured and structured ones, depending on the way in which the random directions are generated. In the former, directions are sampled i.i.d. from some distribution [39, 17, 10, 46] while in the latter, directions have to satisfy some structural constraints, e.g. orthogonality [32, 42]. Several authors [32, 42, 5, 12] theoretically and empiricallyobserved that imposing orthogonality among the directions provides better performance than using unstructured directions. Intuitively, imposing orthogonality allows us to avoid cases in which the gradient approximation is built using similar or redundant directions.

Notably, methods based on structured finite differences have been analyzed only for smooth and convex (or specific non-convex) target functions. This represents a strong limitation, since smoothness and convexity are in practice hardly verifiable, due to the nature of black-box optimization problems. The aim of this paper is the analysis of structured finite difference algorithms dropping smoothness and convexity assumptions.

For unstructured finite-difference methods, a common way to face nonsmoothness consists in introducing a smooth approximation of the target function, also known as "smoothing" [6; 16] and using it as a surrogate of the target. Although the gradient of the smoothing is not computable, different authors showed that for certain types of smoothing the unstructured finite-difference approximation provides an unbiased estimation of such a gradient - see, for example, [39; 46; 18; 22; 21; 24]. This key observation allows to prove that unstructured finite-difference methods approximate a solution in different nonsmooth settings [17; 39; 46; 18; 22; 34].

For structured finite-difference methods, the analysis in the nonsmooth setting is not available. A key step which is missing is the proof of the fact that the surrogate of the gradient built with structured directions is an estimation of the gradient of a suitable smoothing.

In this paper, we close the gap, and introduce O-ZD, a structured finite-difference algorithm in the non-smooth setting. The algorithm builds an approximation of the gradient of a smoothed target using a set of \(\ell\leq d\) orthogonal directions. We analyze the procedure proving that our finite-difference surrogate is an unbiased estimation of the gradient of a smoothing. We provide convergence rates for non-smooth convex functions with optimal dependence on the dimension [17] and rates for the non-smooth non-convex setting (for which lower bounds are not known [34]). Moreover, for non-smooth convex functions we provide the first proof of convergence of the iterates for structured finite differences in this setting. For smooth convex functions, we recover the standard results for structured zeroth-order methods [32; 42]. We conclude with numerical illustrations. To the best of our knowledge, this is the first work on nonsmooth finite-difference method with structured directions.

The paper is organized as follows. In Section 2, we introduce the problem and the algorithm. In Section 3, we state and discuss the main results. In Section 4 we provide some experiments and in 5 some final remarks.

## 2 Problem Setting & Algorithm

Given a function \(f:\mathbb{R}^{d}\to\mathbb{R}\) and assuming that \(f\) has at least a minimizer in \(\mathbb{R}^{d}\), we consider the problem to find

\[x^{*}\in\operatorname*{arg\,min}_{x\in\mathbb{R}^{d}}f(x).\] (1)

In particular, we consider the non-smooth setting where \(f\) might be non-differentiable. To solve problem (1), we propose a zeroth-order algorithm, namely an iterative procedure that uses only function values. At every iteration \(k\in\mathbb{N}\), a first-order information of \(f\) is approximated with finite-differences using a set of \(\ell\leq d\) random orthogonal directions \((p_{k}^{(i)})_{i=1}^{\ell}\). Such orthogonal directions are represented as rotations (and reflections) of the first \(\ell\) vectors of the canonical basis \((e_{i})_{i=1}^{\ell}\). We set \(p_{k}^{(i)}=G_{k}e_{i}\), where \(G_{k}\) belongs to the orthogonal group defined as

\[O(d):=\{G\in\mathbb{R}^{d\times d}\,|\,\det G\neq 0\,\wedge\,G^{-1}=G^{\intercal}\}.\]

Methods for generating orthogonal matrices are discussed in Appendix D. Given \(G\in O(d)\), \(0<\ell\leq d\) and \(h>0\), we consider the following central finite-difference surrogate of the gradient information

\[g_{(G,h)}(x)=\frac{d}{\ell}\sum_{i=1}^{\ell}\frac{f(x+hGe_{i})-f(x-hGe_{i})}{ 2h}Ge_{i}.\] (2)

Then, we introduce the following algorithm.

Starting from an initial guess \(x_{0}\in\mathbb{R}^{d}\), at every iteration \(k\in\mathbb{N}\), the algorithm samples an orthogonal matrix \(G_{k}\) i.i.d. from the orthogonal group \(O(d)\) and computes a surrogate \(g_{(G_{k},h_{k})}\) of the gradient at the current iterate \(x_{k}\). Then, it computes \(x_{k+1}\) by moving in the opposite direction of \(g_{(G_{k},h_{k})}(x_{k})\). This approach belongs to the class of _structured_ finite-difference methods, where a bunch of orthogonal directions is used to approximate a gradient of the target function [32, 42]. Such algorithms have been proposed and analyzed only for smooth functions. To cope with this limitation, and extend the analysis to the nonsmooth setting, we exploit a smoothing technique. For a fixed a probability measure \(\rho\) on \(\mathbb{R}^{d}\) and a positive parameter \(h>0\) called _smoothing parameter_, we define the following _smooth_ surrogate of \(f\)

\[f_{h,\rho}(x):=\int f(x+hu)\,d\rho(u).\] (3)

As shown in [6], \(f_{h,\rho}\) is differentiable even when \(f\) is not. In the literature, different authors have used this strategy to face non-smooth zeroth-order optimization with random finite-difference methods [18, 39, 49, 22] fixing specific smoothing distribution, but no one applied and analyze it for structured methods.

In this work, \(\rho\) is the uniform distribution over the \(\ell_{2}\) unit ball \(\mathbb{B}^{d}\), defining the smooth surrogate

\[f_{h}(x)=\frac{1}{\text{vol}(\mathbb{B}^{d})}\int_{\mathbb{B}^{d}}f(x+hu)\,du,\] (4)

where \(\text{vol}(\mathbb{B}^{d})\) denotes the volume of \(\mathbb{B}^{d}\). One of our main contributions is the following Lemma which shows that the surrogate proposed in (2) is an unbiased estimator of the gradient of the smoothing in (4).

**Lemma 1** (Smoothing Lemma).: _Given a probability space \((\Omega,\mathcal{F},\mathbb{P})\), let \(G:\Omega\to O(d)\) be a random variable where \(O(d)\) is the orthogonal group endowed with the Borel \(\sigma\)-algebra. Assume that the probability distribution of \(G\) is the (normalized) Haar measure. Let \(h>0\) and let \(g\) be the gradient surrogate defined in eq. (2). Then,_

\[(\forall x\in\mathbb{R}^{d})\qquad\mathbb{E}_{G}[g_{(G,h)}(x)]=\nabla f_{h}(x),\]

_where \(f_{h}\) is the smooth approximation of the target function \(f\) defined in eq. (4)._

The proof of Lemma 1 is provided in Appendix A.1.

**Remark 1**.: _Note that Lemma 1 holds also using \(g^{F}_{(G,h)}\) or \(g^{S}_{(G,h)}\) defined as_

\[g^{F}_{(G,h)}(x):=\frac{d}{\ell}\sum_{i=1}^{\ell}\frac{f(x+hGe_{i})-f(x)}{h} Ge_{i}\qquad\text{and}\qquad g^{S}_{(G,h)}(x):=\frac{d}{\ell}\sum_{i=1}^{\ell} \frac{f(x+hGe_{i})}{h}Ge_{i},\]

_since \(\mathbb{E}_{G}[g_{(G,h)}(x)]=\mathbb{E}_{G}[g^{F}_{(G,h)}(x)]=\mathbb{E}_{G}[ g^{S}_{(G,h)}(x)]\). Despite these two estimators being computationally cheaper than the proposed one, we use central finite differences since they allow us to derive a better bound for \(\mathbb{E}_{G}[\|g_{(G,h)}(x)\|^{2}]\) as observed in [46] for the case \(\ell=1\)._

Thanks to Lemma 1, we can interpret each step of Algorithm 1 as a Stochastic Gradient Descent (SGD) on the smoothed function \(f_{h_{k}}\). But the analysis of the proposed algorithm does not follow from the SGD one for two reasons. First, the smoothing parameter \(h_{k}\) changes along the iterations; second (and more importantly), the set of minimizers of \(f_{h}\) and \(f\) are different in general. However, we will take advantage of the fact that \(f_{h}\) can be seen as an approximation of \(f\). The relationship between \(f\) and its approximation \(f_{h}\) depends on the properties of \(f\) - see Proposition 1 in Appendix A.

Our main contributions are the theoretical and numerical analysis of Algorithm 1 under different choices of the free parameters \(\alpha_{k},h_{k}\), namely the stepsize and the smoothing parameter. To the best of our knowledge, Algorithm 1 is the first structured zeroth-order method for non-smooth optimization.

### Related Work

In practice, the advantage of the use of structured directions in finite-difference methods has been observed in several applications [12] and motivated their study. In [5], the authors theoretically and empirically observed that structured directions provide a better approximation of the gradient with respect to unstructured (Gaussian and spherical) ones. Next, we review the most related works, showing the differences with our algorithm.

Unstructured Finite-differences.Most of existing works focused on the theoretical analysis of methods using a single direction to approximate the gradient - see e.g. [39; 46; 22; 45; 18; 24; 34; 17; 10]. The main results existing so far analyze the convergence of the function values in expectation. They provide convergence rates in terms of the number of function evaluations and explicitly characterize the dependence on the dimension of the ambient space.

**Smooth setting:** In [39; 17], a finite difference algorithm with a single direction is analyzed. Rates on the expected function values are shown. In [39] the dependence on the dimension in the rates is not optimal. In [17], both single and multiple direction cases are analyzed and lower bounds are derived. However, only the convex setting is considered. In [24], both convex and non-convex settings are analyzed. They obtain optimal dependence on dimension in the complexity for convex functions. However, only the single-direction case is considered and only the smooth setting is analyzed. Comparing the result with our rates, Algorithm 1 achieves the same dependence on the dimension in the complexity taking \(\ell\) as a fraction of \(d\). Note that, by parallelizing the computation of the function evaluations, we can get a better result.

**Non-smooth setting:** In [39; 17] the non-smooth setting is also analyzed. However, only the single direction case has been considered and both algorithms do not achieve the lower bound. More precisely, for convex functions, a complexity of \(\mathcal{O}(d^{2}\varepsilon^{-2})\) is achieved by [39] and \(\mathcal{O}(d\log(d)\varepsilon^{-2})\) by [17] while our algorithm gets the optimal dependence. Moreover, note that in [17] the strategy adopted (also called "double smoothing") requires tuning one more sequence of parameters, which is a challenging problem in practice, and only the convex setting is considered. In [39], also the non-convex setting is analyzed by bounding the expected norm of the smoothed gradient. However, they obtain a complexity of \(\mathcal{O}(d^{3}\varepsilon^{-2}h^{-1})\) while our algorithm gets a better dependence on the dimension. In [46], the optimal complexity is obtained for convex functions. However, the author does not analyze the non-convex setting. Moreover, note that, despite the complexity in terms of function evaluations being the same, our algorithm gets a better complexity in terms of the number of iterations since it uses multiple directions (and this is an advantage if we can parallelize the function evaluations). Furthermore, note that the method proposed in [46] can be seen as the special case of Algorithm 1 with \(\ell=1\). In [34] the single direction case is analyzed only in the non-convex setting. The dependence on the dimension of the complexity in the number of function evaluations achieved matches our result in this setting (again, in the number of iterations our method obtains a better dependence).

Structured Finite-difference.In [32; 42], authors analyze structured finite differences in both deterministic and stochastic settings. However, only the smooth convex setting is considered. In [12] orthogonal matrices are used to build directions but no analysis is provided. In [25], finite-difference with coordinate directions is analyzed. At every iteration, \(d+1\) function evaluations are performed to compute the estimator and only the smooth setting is considered.

## 3 Main Results

In this section, we analyze Algorithm 1 considering both non-smooth and smooth problems. We present the rates obtained by our method for convex and non-convex settings and compare them with those obtained by state-of-the-art methods. Proofs are provided in Appendix B. In the following, we call _complexity in the number of iterations / function evaluations_, respectively, the number of iterations / function evaluations required to achieve an accuracy \(\varepsilon\in(0,1)\).

### Non-smooth Convex Setting

In this section, we provide the main results for non-smooth convex functions. In particular, we will assume that the target function is convex and satisfy the following hypothesis.

**Assumption 1** (\(L_{0}\)-Lipschitz continuous).: _The function \(f\) is \(L_{0}\)-Lipschitz continuous; i.e., for some \(L_{0}>0\),_

\[(\forall x,y\in\mathbb{R}^{d})\qquad|f(x)-f(y)|\leq L_{0}\|x-y\|.\]

Note that this assumption implies that also \(f_{h}\) is \(L_{0}\)-Lipschitz continuous - see Proposition 1. Moreover, to analyze the algorithm, we will consider the following parameter setting.

**Assumption 2** (Zeroth-order non-smooth convergence conditions).: _The step-size sequence \(\alpha_{k}\) and the sequence of smoothing parameters \(h_{k}\) satisfy the following conditions:_

\[\alpha_{k}\not\in\ell^{1},\qquad\alpha_{k}^{2}\in\ell^{1}\qquad\text{and} \qquad\alpha_{k}h_{k}\in\ell^{1}.\]

The assumption above is required to guarantee convergence to a solution. In particular, the first two conditions are common for subgradient method and stochastic gradient descent, while the third condition was already used in structured zeroth-order methods [32, 42] and links the decay of the smoothing parameter with the stepsize's one. An example of \(\alpha_{k},h_{k}\) that satisfy Assumption 2 is \(\alpha_{k}=k^{-\theta}\) and \(h_{k}=k^{-\rho}\) with \(\theta\in(1/2,1)\) and \(\rho\) s.t. \(\theta+\rho>1\).

We state now the main theorem for non-smooth convex functions.

**Theorem 1** (Non-smooth convex).: _Under Assumption 1, assume that \(f\) is convex and let \((x_{k})_{k\in\mathbb{N}}\) be a sequence generated by Algorithm 1. For every \(k\in\mathbb{N}\), denote \(A_{k}=\sum_{i=0}^{k}\alpha_{i}\) and set \(\bar{x}_{k}:=\sum_{i=0}^{k}\alpha_{i}x_{i}/A_{k}\). Then_

\[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq S_{k}/A_{k}\qquad\text{with}\qquad S_{k} :=\frac{\|x_{0}-x^{*}\|^{2}}{2}+c\frac{dL_{0}^{2}}{\ell}\sum_{i=0}^{k}\alpha_ {i}^{2}+L_{0}\sum_{i=0}^{k}\alpha_{i}h_{i},\]

_where \(c>0\) is a constant independent of the dimension and \(x^{*}\) is any solution in \(\arg\min f\). Moreover, under Assumption 2, we have_

\[\lim_{k\to+\infty}f(x_{k})=\min f\quad\text{a.s},\]

_and that there exists a random variable \(x^{*}\) taking values in \(\arg\min f\) s.t. \(x_{k}\to x^{*}\) a.s._

In the next corollary, we derive explicit rates for specific choices of the parameters.

**Corollary 1**.: _Under the assumptions of Theorem 1, let \(x^{*}\in\arg\min f\). Then, the following hold:_

* _Let_ \(\theta\in(1/2,1)\) _and_ \(\rho\in\mathbb{R}\) _such that_ \(\theta+\rho>1\)_. For every_ \(k\in\mathbb{N}\)_, let_ \(\alpha_{k}=\alpha(k+1)^{-\theta}\) _and_ \(h_{k}=h(k+1)^{-\rho}\) _with_ \(\alpha>0\) _and_ \(h>0\)_. Then_ \[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{C}{\alpha k^{1-\theta}}+o\Big{(} \frac{1}{k^{1-\theta}}\Big{)},\] _for some constant_ \(C\) _provided in the proof._
* _For every_ \(k\in\mathbb{N}\)_, let_ \(\alpha_{k}=\alpha\) _and_ \(h_{k}=h\) _with_ \(\alpha,h>0\)_. Then_ \[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{\|x_{0}-x^{*}\|^{2}}{2\alpha k}+ \frac{cdL_{0}^{2}}{\ell}\alpha+L_{0}h,\] _where_ \(c\) _is a constant independent of the dimension._
* _Fix an accuracy_ \(\varepsilon\in(0,1)\) _and let_ \(K\geq 8(cL_{0}^{2}\|x_{0}-x^{*}\|^{2})(d/\ell)\varepsilon^{-2}\)_. Set_ \(\alpha_{k}=\sqrt{\frac{\ell}{d}}\frac{\|x_{0}-x^{*}\|}{\sqrt{2cKL_{0}}}\)_, and_ \(h_{k}=h\leq\varepsilon/(2L_{0})\) _for every_ \(k\leq K\)_. Then_ \[\mathbb{E}[f(\bar{x}_{K})-\min f]\leq\varepsilon\] _and the complexity in terms of number of iterations is_ \(\mathcal{O}((d/\ell)\varepsilon^{-2})\)_._

Discussion.The bound in Theorem 1 depends on the initialization and on an additional quantity that can be interpreted as an approximation error. The latter is composed of two parts. The first one is generated by the approximation of the gradient of the smoothed function; while the second, involving \(h_{k}\), is generated by the smoothing. Since the rate depends on \(1/\sum_{i=0}^{k}\alpha_{i}\), we would like to choose the stepsize as large as possible. However, to get convergence, we need to make the approximation errorsvanish sufficiently fast. To guarantee this, as we can observe from Theorem 1, we need to impose some conditions on the stepsize \(\alpha_{k}\) and on the smoothing parameter \(h_{k}\) (i.e. Assumption 2), that will slow down the decay of the first term. In Corollary 1, we provide two choices of parameters: the first one satisfies Assumption 2 and ensures convergence; the second one corresponds to constant stepsize and smoothing parameter. For the first choice, we recover the rate of the subgradient method in terms of \(k\). In particular, for \(\theta\) approaching \(1/2\), the convergence rate is arbitrarily close to \(O(k^{-1/2})\) and is similar to the one derived in (17, Theorem 2). The dependence on the dimension depends on the choice of the constant \(\alpha\). The optimal dependence is obtained with the choice \(\alpha=\sqrt{\ell/d}\). Indeed, in that case, the complexity in the number of iterations is of the order \(\mathcal{O}((d/\ell)\varepsilon^{-2})\), which is better than the one achieved by (17, Theorem 2) and (39). Note that also (46, Corollary 1) and (22, Theorem 2.4) obtain a worse complexity in terms of the number of iterations (since they use a single direction), but the same complexity in the number of function evaluations. Clearly, since multiple directions are used, a single iteration of O-ZD will be more expensive than one iteration of (46, 22). However, our algorithm is more efficient if the \(\ell\) function evaluations required at each iteration can be parallelized. On the more theoretical side, we observe that the advantage of multiple orthogonal directions is in the tighter bounds for the variance of the estimator, namely for \(\mathbb{E}[\|g_{(G_{k},h_{k})}(x_{k})\|^{2}]\) - see (46, Lemma 5) and Lemma 4.

### Non-smooth Non-convex Setting

To analyze the non-convex setting, following (39), we provide a bound on the averaged expected square norm of the gradient of the smoothed target. In particular, we use the following notation:

\[\eta_{k}^{(h)}:=\left(\sum_{i=0}^{k}\alpha_{i}\,\mathbb{E}[\|\nabla f_{h}(x_{i })\|^{2}]\right)/A_{k},\qquad\qquad\text{where}\ \ A_{k}:=\sum_{i=0}^{k}\alpha_{i}.\]

Next, we state the main theorem for the non-convex non-smooth setting.

**Theorem 2** (Non-smooth non-convex).: _Under Assumption 1, let \((x_{k})_{k\in\mathbb{N}}\) be a sequence generated by Algorithm 1 with, for every \(k\in\mathbb{N}\), \(h_{k}=h\) for some \(h>0\). Then_

\[\eta_{k}^{(h)}\leq S_{k}/A_{k}\quad\text{with}\quad S_{k}:=f_{h}(x_{0})-\min f +c\frac{L_{0}^{3}d\sqrt{d}}{\ell}\sum_{i=0}^{k}\frac{\alpha_{i}^{2}}{h}.\]

In the next corollary, we derive the rates for specific choices of the parameters.

**Corollary 2**.: _Under the assumptions of Theorem 2, the following statements hold._

* _If_ \(\alpha_{k}=\alpha(k+1)^{-\theta}\) _with_ \(\alpha>0\) _and_ \(\theta\in(1/2,1)\)_, then_ \[\eta_{k}^{(h)}\leq C\frac{f_{h}(x_{0})-\min f}{\alpha k^{1-\theta}}+o\Big{(} \frac{1}{k^{1-\theta}}\Big{)},\] _where_ \(C\) _is a constant independent of the dimension._
* _If_ \(\alpha_{k}=\alpha\) _with_ \(\alpha>0\) _for every_ \(k\in\mathbb{N}\)_, then_ \[\eta_{k}^{(h)}\leq\frac{f_{h}(x_{0})-\min f}{\alpha k}+\frac{cL_{0}^{3}d\sqrt{ d}}{\ell h}\alpha,\] _where_ \(c\) _is a constant independent of the dimension._
* _Let_ \(\varepsilon\in(0,1)\)_, let_ \(K\geq 4(f_{h}(x_{0})-\min f)cL_{0}^{3}d\sqrt{d}\varepsilon^{-2}/(\ell h)\) _and choose_ \(\alpha=\sqrt{\frac{(f_{h}(x_{0})-\min f)\ell h}{KcL_{0}^{3}d\sqrt{d}}}\)_. Then we have that_ \(\eta_{K}^{(h)}\leq\varepsilon\)_. Thus, the number of function evaluations required to get a precision_ \(\eta_{k}^{h}\leq\varepsilon\) _is of the order_ \(\mathcal{O}(d\sqrt{d}h^{-1}\varepsilon^{-2})\)_._

Relying on the results in (34), we show that this is related to a precise notion of approximate stationarity. To do so, we need to introduce a definition of subdifferential which is suitable to this setting. As shown in (34) the Clarke subdifferential is not the right notion, and the approximate Goldstein subdifferential should be used instead.

**Definition 1** (Goldstein subdifferential and stationary point).: _Under Assumption 1, let \(x\in\mathbb{R}^{d}\) and \(h>0\). The \(h\)-Goldstein subdifferential of \(f\) at \(x\) is \(\partial_{h}f(x):=\text{conv}(\cup_{y\in\mathbb{B}^{d}_{k}(x)}\partial f(y))\) where \(\partial f\) is the Clarke subdifferential [13] and \(\mathbb{B}^{d}_{h}(x)\) is the ball centered in \(x\) with radius \(h\). For \(\varepsilon\in(0,1)\), a point \(x\in\mathbb{R}^{d}\) is a \((h,\varepsilon)\)-Goldstein stationary point for the function \(f\) if \(\min\{\|g\|\,|\,g\in\partial_{h}f(x)\}\leq\varepsilon\)._

**Corollary 3**.: _Under the same assumptions of Theorem 2, fix \(K\in\mathbb{N}\) and let \(I\) be a random variable taking values in \(\{0,\ldots,K-1\}\) such that, for all \(i\), \(\mathbb{P}[I=i]=\alpha_{i}/A_{K-1}\). Let also \(S_{k}\) be defined as in Theorem 2. Then_

\[\mathbb{E}_{I}\left[\min\{\|\eta\|^{2}\,:\,\eta\in\partial f_{h}(x_{I})\} \right]\leq S_{K}/A_{K}.\]

_In the setting of Corollary 2 (iii), we have also that \(\mathbb{E}_{I}\left[\min\{\|\eta\|^{2}\,:\,\eta\in\partial f_{h}(x_{I})\} \right]\leq\varepsilon\)._

Discussion.In Theorem 2, we fix the smoothing of the target, i.e. we consider \(h_{k}\) constant, and we analyze the non-smooth non-convex setting providing a rate on the expected norm of the smoothed gradient. The resulting bound is composed of two parts. The first part is very natural, and due to the functional value at the initialization. The second part is the approximation error. Recall that Assumption 1 holds, and therefore \(f_{h}\leq f+L_{0}h\) due to Proposition 1. This suggests taking \(h\) as small as possible in order to reduce the gap between \(f_{h}\) and \(f\). However, taking \(h\) too small would make the approximation error very big. In our analysis, we consider the case with \(h\) constant. Moreover, as for the convex case, the speed of the rate depends on \(A_{k}\) and so we would like to take the stepsize as large as possible. But to control the approximation error, we need to assume \(\alpha_{k}^{2}\in\ell^{1}\). In Corollary 2, we consider two choices of stepsize. The first choice satisfies the property of \(\alpha_{k}^{2}\in\ell^{1}\), while the second one analyzes the case of constant step-size. Comparing our rate to the one in [39] we see that we obtain a better dependence on the dimension in the complexity, both in terms of iterations and function evaluations. Our results match the one of [34, Theorem 3.2] in terms of rate and in terms of function evaluations. We get a better dependence on the dimension in the number of iterations. Note again that, despite the complexity in terms of the number of function evaluations being the same, the possibility of parallelization for the function evaluations yields a better result for our method. As for the convex setting, we have a tighter upper-bound on the variance of the estimator of the smoothed gradient with respect to the dimension - see [34, Lemma D.1]. Goldstein stationarity has been used to assess the approximate stationarity for first-order methods as well, see [15]. The latter work shows that a cutting plane algorithm achieves a rate of \(\mathcal{O}(d\varepsilon^{-3})\) for Lipschitz functions.

### Smooth Convex setting

We consider now the smooth setting, i.e. we assume that the target function satisfies the following hypothesis.

**Assumption 3** (\(L_{1}\)-Smooth).: _The function \(f\) is \(L_{1}\)-smooth; i.e. the function \(f\) is differentiable and, for some \(L_{1}>0\),_

\[(\forall x,y\in\mathbb{R}^{d})\qquad\|\nabla f(x)-\nabla f(y)\|\leq L_{1}\|x- y\|.\]

This is the standard assumption for analyzing first-order methods and has been used in many other works in the literature for zeroth-order algorithms - see e.g. [39, 17]. As shown in previous works, if \(f\) satisfies Assumption 3 then also \(f_{h}\) satisfies it - see Proposition 1. We will consider also the following assumptions on the stepsize and the smoothing in order to guarantee convergence.

**Assumption 4** (Smooth zeroth-order convergence conditions).: _The stepsize sequence \((\alpha_{k})_{k\in\mathbb{N}}\) and the smoothing sequence \((h_{k})_{k\in\mathbb{N}}\) satisfy the following conditions:_

\[\alpha_{k}\not\in\ell^{1}\quad\text{and}\quad\alpha_{k}h_{k}\in\ell^{1}.\]

_Moreover, \(\alpha_{k}\leq\bar{\alpha}<\ell/dL_{1}\) for every \(k\in\mathbb{N}\)._

Note that this is a weaker version of Assumption 2. Next, we state the main theorem for convex smooth functions.

**Theorem 3** (Smooth convex).: _Under Assumptions 3 and 4, let \((x_{k})_{k\in\mathbb{N}}\) be a sequence generated by Algorithm 1 and \(x^{*}\in\arg\min f\). For every \(k\in\mathbb{N}\), set \(A_{k}=\sum_{i=0}^{k}\alpha_{i}\) and \(\bar{x}_{k}=\sum\limits_{i=0}^{k}\alpha_{i}x_{i}/A_{k}\). Then, for every \(k\in\mathbb{N}\),_

\[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{D_{k}}{A_{k}}\quad\text{with} \quad D_{k}:=\frac{\ell\Delta+d\bar{\alpha}}{2\ell\Delta}\Big{(}S_{k}+\sum \limits_{i=0}^{k}\rho_{i}\Big{(}\sqrt{S_{i}}+\sum\limits_{j=0}^{i}\rho_{j} \Big{)}\Big{)},\]_where \(S_{k}:=\|x_{0}-x^{*}\|^{2}+\sum_{i=0}^{k}\frac{L_{1}^{2}d^{2}}{2\ell}\alpha_{i}^{2 }h_{i}^{2},\;\rho_{k}:=L_{1}d\alpha_{k}h_{k},\text{and}\;\Delta:=\left(\frac{1}{ L_{1}}-\frac{d}{\ell}\bar{\alpha}\right)\)._

**Corollary 4**.: _Under the same Assumptions of Theorem 3, the following hold._

1. _If for every_ \(k\in\mathbb{N}\) _we set_ \(\alpha_{k}=\alpha>0\) _and_ \(h_{k}=h(k+1)^{-\theta}\) _for_ \(h>0\) _and_ \(\theta>1\)_, then_ \[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{C}{\alpha k},\] _where_ \(C\) _is a constant provided in the proof. Moreover, if_ \(\alpha<\ell/(2dL_{1})\)_,_ \(\lim_{k\to\infty}f(x_{k})=\min f\) _a.s. and there exists a random variable_ \(\hat{x}\) _taking values in_ \(\arg\min f\) _s.t._ \(x_{k}\to\hat{x}\) _a.s._
2. _If for every_ \(k\in\mathbb{N}\) _we set_ \(\alpha_{k}=\alpha>0\) _and_ \(0<h_{k}\leq h\)_, then_ \[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{C_{1}}{k}+C_{2}\alpha h+C_{3}\alpha ^{2}h^{2}\sqrt{k}+C_{4}\alpha^{2}h^{2}k,\] _where_ \(C_{1},C_{2},C_{3}\) _and_ \(C_{4}\) _are non-negative constants._

Discussion.As in the previous cases, the bound in Theorem 3 is composed by two terms: the error due to the initialization and the one due to the approximation. An important difference with the results in the non-smooth setting is that every term in the approximation error is decreasing with respect to the smoothing parameter \(h_{k}\). This allows obtaining convergence also with the constant step-size scheme, taking \(h_{k}\in\ell^{1}\). In Corollary 4 (i), we recover the result of [32, Theorem 5.4] with a specific choice of parameters \(\alpha,h\) (up to constants). The complexity depends on the choice of \(\alpha\). Note that by Assumption 4, \(\alpha<\ell/(L_{1}d)\) thus the dependence on the dimension in the rate will be at least \(d/\ell\). In particular, taking \(\alpha=\ell/(2dL_{1})\), we obtain the optimal complexity of \(\mathcal{O}(d\varepsilon^{-1})\) in terms of function evaluations. This result has a better dependence on the dimension than [39]. In Corollary 4 (ii), the dependence on the dimension in the complexity depends on the choice of \(\alpha\) and \(h\). Moreover, the rate obtained is equal (up to constants) to the rate obtained in [32] in the same setting, i.e. \(\mathcal{O}(1/k)\) (in which we hide the dependence on \(d\) and \(\ell\)). As for [32], for the first setting we can prove the almost sure convergence of the iterates.

### Smooth Non-Convex setting

To analyze the smooth non-convex setting, we introduce the following notation:

\[(\forall k\in\mathbb{R}^{d})\qquad A_{k}:=\sum_{i=0}^{k}\alpha_{i},\qquad \eta_{k}:=\left(\sum_{i=0}^{k}\alpha_{i}\,\mathbb{E}[\|\nabla f(x_{i})\|^{2}] \right)/A_{k}.\]

Note that, in comparison with the quantity defined in Section 3.2, here \(\eta_{k}\) is related to the exact objective function \(f\) and not to its smoothed version \(f_{h}\). Next, we state the main result for smooth non-convex functions.

**Theorem 4** (Smooth non-convex).: _Suppose that Assumption 3 holds and assume that, for every \(k\in\mathbb{N}\), \(\alpha_{k}\leq\bar{\alpha}<\ell/(2dL_{1})\). Let \((x_{k})_{k\in\mathbb{N}}\) be a sequence generated by Algorithm 1. Then_

\[\eta_{k}\leq\frac{1}{\Delta A_{k}}\Bigg{(}f(x_{0})-\min f+\frac{L_{1}^{2}d^{2} }{8}\sum_{i=0}^{k}\alpha_{i}h_{i}^{2}+\frac{L_{1}^{3}d^{2}}{4\ell}\sum_{i=0}^{ k}\alpha_{i}^{2}h_{i}^{2}\Bigg{)},\qquad\Delta:=\Big{(}\frac{1}{2}-\frac{L_{1}d}{ \ell}\bar{\alpha}\Big{)}.\]

**Corollary 5**.: _Under the assumptions of Theorem 4, the following hold._

1. _If_ \(\alpha_{k}=\alpha\leq\bar{\alpha}\) _and_ \(h_{k}=hk^{-\theta}\) _with_ \(h>0\) _and_ \(\theta>1\)_, then_ \[\eta_{k}\leq\left[\frac{f(x_{0})-\min f}{\Delta\alpha}+\frac{C_{1}d^{2}h^{2}} {\Delta}+\frac{C_{2}\alpha h^{2}d^{2}}{\Delta\ell}\right]\cdot\frac{1}{k},\] _where_ \(C_{1}\) _and_ \(C_{2}\) _are constants provided in the proof._
2. _If_ \(\alpha_{k}=\alpha\leq\bar{\alpha}\) _and_ \(h_{k}=h>0\)_, then_ \[\eta_{k}\leq\frac{f(x_{0})-\min f}{\Delta\alpha k}+\frac{C_{1}d^{2}h^{2}}{ \Delta}+\frac{C_{2}\alpha h^{2}d^{2}}{\Delta\ell},\] _where_ \(C_{1}\) _and_ \(C_{2}\) _are constants provided in the proof._Discussion.As for the convex case, every term in the approximation error depends on the smoothing parameter \(h_{k}\). In Corollary 5 (i), we take constant step-size and \(h_{k}\in\ell^{1}\). With this choice of parameters, we get a rate of \(\mathcal{O}(1/k)\) which matches with the result obtained by [39]. The dependence on the dimension depends on the choice of \(\alpha\) and \(h\). Note that \(\alpha<\ell/(2dL_{1})\), thus taking \(h=\mathcal{O}(1/d)\), in the rate we get a dependence on the dimension of \(d/\ell\). Taking for instance \(\alpha=\ell/(3dL_{1})\) and \(h=\mathcal{O}(1/d)\), we get a complexity of \(\mathcal{O}(d\varepsilon^{-1})\) in terms of function evaluations.

## 4 Numerical Results

In this section, we provide some numerical experiments to assess the performances of our algorithm. We consider two target functions: a convex smooth one and a convex non-smooth one. Details on target functions and parameters of the algorithms are reported in Appendix C. To report our findings, we run the experiments \(10\) times and provide the mean and standard deviation of the results.

How to choose the number of directions?In these experiments, we set a fixed budget of \(4000\) function evaluations and we consider \(d=50\). We investigate how the performance of Algorithm 1 changes as the value of \(\ell\) increases. In Figure 1, we observe the mean sequence \(f(x_{k})-f(x^{*})\) after each function evaluation. If \(\ell>1\), then the target function values are repeated \(2\ell\) times, since we need to perform \(2\ell\) function evaluations to do one iteration. For a sufficiently large budget, increasing the number of directions \(\ell\) leads to better results compared to using a single direction in both smooth and non-smooth settings.

Comparison with finite-difference methods.Now, we compare Algorithm 1 with other finite-difference methods. More precisely, we consider finite differences with single (and multiple) Gaussian (and spherical) directions. The budget of function evaluations is \(1000\) and the ambient dimension is \(d=10\). For multiple direction methods, we fix the number of directions \(\ell=d\). Further experiments are provided in Appendix F.

Figure 1: From left to right, function values per function evaluation in optimizing smooth and non-smooth target functions with different numbers of directions.

Figure 2: From left to right, function values per function evaluation in optimizing smooth and non-smooth convex functions with different finite-difference algorithms.

In Figure 2, we plot the sequence \(f(x_{k})-f(x^{*})\) with respect to the number of function evaluations. While in terms of rates and complexity the different algorithms are the same, Algorithm 1 shows better performances than random directions approaches, and we believe this is due to the use of structured (i.e. orthogonal) directions. Indeed, orthogonal directions yield a better approximation of first-order information with respect to other methods. The practical advantages of structured directions were already observed in [32, 42, 5, 12] and these experiments confirm that the good practical behavior holds even in the nonsmooth setting.

## 5 Conclusion

We introduced and analyzed O-ZD a zeroth-order algorithm for non-smooth zeroth-order optimization. We analyzed the algorithm and derived rates for non-smooth and smooth functions. This work opens different research directions. An interesting one would be the introduction of a learning procedure for the orthogonal directions. Such an approach could have significant practical applications.

## Acknowledgments and Disclosure of Funding

This project has been supported by the TraDE-OPT project, which received funding from the European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No 861137. L. R. and M. R. acknowledge the financial support of the European Research Council (grant SLING 819789), the AFOSR projects FA9550-18-1-7009 (European Office of Aerospace Research and Development), the EU H2020-MSCA-RISE project NoMADS - DLV-777826, and the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. S. V. and L. R. acknowledge the support of the AFOSR project FA8655-22-1-7034. The research by S. V. and C. M. has been supported by the MIUR Excellence Department Project awarded to Dipartimento di Matematica, Universita di Genova, CUP D33C23001110001. S. V. and C. M. are members of the Gruppo Nazionale per l'Analisi Matematica, la Probabilita e le loro Applicazioni (GNAMPA) of the Istituto Nazionale di Alta Matematica (INdAM). This work represents only the view of the authors. The European Commission and the other organizations are not responsible for any use that may be made of the information it contains.

## References

* [1] Hans Wilhelm Alt. _Linear Functional Analysis: An Application-Oriented Introduction_. Springer London, London, 2016.
* [2] T. W. Anderson, I. Olkin, and L. G. Underhill. Generation of random orthogonal matrices. _SIAM Journal on Scientific and Statistical Computing_, 8(4):625-629, 1987.
* [3] J.B. Baillon and G. Haddad. Quelques proprietes des operateurs angle-bornes etn-cycliquement monotones. _Israel Journal of Mathematics_, 26(2):137-150, Jun 1977.
* [4] A. Barvinok. Approximating orthogonal matrices by permutation matrices. _Pure and applied mathematics quarterly_, 2, 11 2005.
* [5] A. S. Berahas, L. Cao, K. Choromanski, and K. Scheinberg. A theoretical and empirical comparison of gradient approximations in derivative-free optimization. _Foundations of Computational Mathematics_, 22(2):507-560, Apr 2022.
* [6] D. P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals. _Journal of Optimization Theory and Applications_, 12(2):218-231, Aug 1973.
* [7] A. Bjorck. Numerics of gram-schmidt orthogonalization. _Linear Algebra and its Applications_, 197-198:297-316, 1994.
* [8] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized hadamard transform. _SIAM Journal on Matrix Analysis and Applications_, 34(3):1301-1340, 2013.

* [9] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gael Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In _ECML PKDD Workshop: Languages for Data Mining and Machine Learning_, pages 108-122, 2013.
* [10] R. Chen and S. Wild. Randomized derivative-free optimization of noisy convex functions. _arXiv preprint arXiv:1507.03332_, 2015.
* [11] K. Choromanski, M. Rowland, W. Chen, and A. Weller. Unifying orthogonal monte carlo methods. In _International Conference on Machine Learning_, pages 1203-1212. PMLR, 2019.
* [12] K. Choromanski, M. Rowland, V. Sindhwani, R. Turner, and A. Weller. Structured evolution with compact architectures for scalable policy optimization. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 970-978. PMLR, 10-15 Jul 2018.
* [13] F. H. Clarke. _Optimization and Nonsmooth Analysis_. Society for Industrial and Applied Mathematics, 1990.
* [14] A. R. Conn, K. Scheinberg, and L. N. Vicente. _Introduction to Derivative-Free Optimization_. Society for Industrial and Applied Mathematics, 2009.
* [15] D. Davis, D. Drusvyatskiy, Y. T. Lee, S. Padmanabhan, and G. Ye. A gradient sampling method with complexity guarantees for lipschitz functions in high and low dimensions. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 6692-6703. Curran Associates, Inc., 2022.
* [16] J. C. Duchi, P. L. Bartlett, L. Peter, and M. J. Wainwright. Randomized smoothing for stochastic optimization. _SIAM Journal on Optimization_, 22(2):674-701, 2012.
* [17] J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. _IEEE Transactions on Information Theory_, 61(5):2788-2806, 2015.
* [18] A. Flaxman, A. Tauman Kalai, and B. McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In _SODA '05 Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms_, pages 385-394, January 2005.
* [19] M. Fornasier, T. Klock, and K. Riedl. Consensus-based optimization methods converge globally in mean-field law. _arXiv 2103.15130_, 2021.
* [20] P. I. Frazier. _Bayesian Optimization_, chapter 11, pages 255-278. INFORMS, 2018.
* [21] X. Gao, B. Jiang, and S. Zhang. On the information-adaptive variants of the admm: An iteration complexity perspective. _Journal of Scientific Computing_, 76(1):327-363, Jul 2018.
* [22] A. Gasnikov, A. Novitskii, V. Novitskii, F. Abdukhakimov, D. Kamzolov, A. Beznosikov, M. Takac, P. Dvurechensky, and B. Gu. The power of first-order smooth optimization for black-box non-smooth problems. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 7241-7265. PMLR, 17-23 Jul 2022.
* [23] A. Genz. Methods for generating random orthogonal matrices. In _Monte-Carlo and Quasi-Monte Carlo Methods 1998: Proceedings of a Conference held at the Claremont Graduate University, Claremont, California, USA, June 22-26, 1998_, pages 199-213. Springer, 2000.
* [24] S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* [25] G. N. Grapiglia. Worst-case evaluation complexity of a derivative-free quadratic regularization method. _Optimization Letters_, Feb 2023.

* [26] N. Hansen. _The CMA Evolution Strategy: A Comparing Review_, pages 75-102. Springer Berlin Heidelberg, Berlin, Heidelberg, 2006.
* [27] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. Fernandez del Rio, M. Wiebe, P. Peterson, P. Gerard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357-362, September 2020.
* [28] A. Hedayat and W. D. Wallis. Hadamard matrices and their applications. _The Annals of Statistics_, 6(6):1184-1238, 1978.
* [29] J. D. Hunter. Matplotlib: A 2d graphics environment. _Computing in Science & Engineering_, 9(3):90-95, 2007.
* [30] N. K. Jain, U. Nangia, and J. Jain. A review of particle swarm optimization. _Journal of The Institution of Engineers (India): Series B_, 99(4):407-411, Aug 2018.
* [31] J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. _Annals of Mathematical Statistics_, 23:462-466, 1952.
* [32] D. Kozak, C. Molinari, L. Rosasco, L. Tenorio, and S. Villa. Zeroth order optimization with orthogonal random directions, 2021.
* [33] R. M. Lewis, V. Torczon, and M. W. Trosset. Direct search methods: then and now. _Journal of Computational and Applied Mathematics_, 124(1):191-207, 2000. Numerical Analysis 2000. Vol. IV: Optimization and Nonlinear Equations.
* [34] T. Lin, Z. Zheng, and M. Jordan. Gradient-free methods for deterministic and stochastic nonsmooth nonconvex optimization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 26160-26175. Curran Associates, Inc., 2022.
* [35] S. Liu, P. Y. Chen, B. Kailkhura, G. Zhang, A. O. Hero III, and P. K. Varshney. A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications. _IEEE Signal Processing Magazine_, 37(5):43-54, 2020.
* [36] H. Mania, A. Guy, and B. Recht. Simple random search of static linear policies is competitive for reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [37] P. Mattila. _Geometry of Sets and Measures in Euclidean Spaces: Fractals and Rectifiability_. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 1995.
* [38] F. Mezzadri. How to generate random matrices from the classical compact groups. _arXiv preprint math-ph/0609050_, 2006.
* [39] Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. _Foundations of Computational Mathematics_, 17:527-566, 2017.
* 597, 1967.
* [41] B. T. Polyak. Introduction to optimization. _Optimization Software Inc., Publications Division, New York_, 1:32, 1987.
* [42] M. Rando, C. Molinari, S. Villa, and L. Rosasco. Stochastic zeroth order descent with structured directions, 2022.
* [43] H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales and some applications. In J. S. Rustagi, editor, _Optimizing Methods in Statistics_, pages 233-257. Academic Press, 1971.

* [44] C. Rusu and L. Rosasco. Fast approximation of orthogonal matrices and application to pca. _Signal Processing_, 194:108451, 2022.
* [45] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning, 2017.
* [46] O. Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. _The Journal of Machine Learning Research_, 18(1):1703-1713, 2017.
* [47] C. Totzeck. _Trends in Consensus-Based Optimization_, pages 201-226. Springer International Publishing, Cham, 2022.
* [48] T. Trogdon. On spectral and numerical properties of random butterfly matrices. _Applied Mathematics Letters_, 95:48-58, 2019.
* [49] F. Yousefian, A. Nedic, and U. V. Shanbhag. On stochastic gradient and subgradient methods with adaptive steplength sequences. _Automatica_, 48(1):56-67, 2012.

Auxiliary Results

In this appendix, we state and collect lemmas and propositions required to prove the main results.

Notation.In the following sections, we denote with \(\mathcal{F}_{k}\) the filtration \(\sigma(G_{1},\cdots,G_{k-1})\). Moreover, to simplify the notation, we define \(g_{k}\) as the gradient surrogate in eq.(2) at time-step \(k\) i.e. \(g_{k}:=g_{(G_{k},h_{k})}(x_{k})\) and \(g(\cdot):=g_{(G,h)}(\cdot)\) for an arbitrary \(G\in O(d)\) and \(h>0\). We denote the normalized Haar measure [37] by \(\mu\). We define the unit ball \(\mathbb{B}^{d}\) and the unit sphere \(\mathbb{S}^{d-1}\) as follow

\[\mathbb{B}^{d}:=\{v\in\mathbb{R}^{d}\,|\,\|v\|\leq 1\}\qquad\text{and}\qquad \mathbb{S}^{d-1}:=\{v\in\mathbb{R}^{d}\,|\,\|v\|=1\}.\]

We denote by \(\sigma\) and \(\sigma_{N}\) the spherical measure and the normalized spherical measure on \(\mathbb{S}^{d-1}\), respectively. Moreover, we denote with \(I_{d,\ell}\in\mathbb{R}^{d\times\ell}\) the (truncated) identity matrix.

**Lemma 2**.: _Let \(\beta(\mathbb{S}^{d-1})\) be the surface area of \(\mathbb{S}^{d-1}\) and let \(I\in\mathbb{R}^{d\times d}\) be the identity matrix. Then,_

\[\int_{\mathbb{S}^{d-1}}vv^{\intercal}\,\,d\sigma(v)=\frac{\beta(\mathbb{S}^{d -1})}{d}I.\]

Proof.: This result is proved in [21, Lemma 7.3, point (b)]. 

**Lemma 3**.: _Let \(\phi:\mathbb{R}^{d}\to\mathbb{R}\) be a \(L\)-Lipschitz function. If \(u\) is uniformly distributed on \(\mathbb{S}^{d-1}\), then_

\[(\mathbb{E}[\phi(u)-\mathbb{E}[\phi(u)]])^{2}\leq c\frac{L^{2}}{d},\]

_for some numerical constant \(c>0\)._

Proof.: The proof follows the same line as [46, Lemma 9]. 

### Smoothing Lemma & Properties

In this appendix, we provide the proof of the Smoothing Lemma (i.e. Lemma 1).

Proof of Smoothing Lemma.By eq. (2),

\[\mathbb{E}_{G}[g_{(G,h)}(x)]=\frac{d}{\ell}\sum_{i=1}^{\ell}\int_{O(d)}\frac{ f(x+hGe_{i})-f(x-hGe_{i})}{2h}Ge_{i}\,d\mu(G).\]

By [37, Theorem 3.7],

\[\mathbb{E}_{G}[g_{(G,h)}(x)]=\frac{d}{2\ell h}\sum_{i=1}^{\ell}\int_{\mathbb{ S}^{d-1}}(f(x+hv^{(i)})-f(x-hv^{(i)}))v^{(i)}\,d\sigma_{N}(v^{(i)}).\]

Since \(v^{(i)}\) is uniformly distributed on the sphere, which is symmetric with respect to the origin, we have

\[\mathbb{E}_{G}[g_{(G,h)}(x)]=\frac{d}{\ell h}\sum_{i=1}^{\ell}\int_{\mathbb{ S}^{d-1}}f(x+hv^{(i)})v^{(i)}\,d\sigma_{N}(v^{(i)}).\]

As a consequence of Stokes' Theorem (details in [18, Lemma 1] and [1, Theorem A8.8]), we get

\[\mathbb{E}[g_{(G,h)}(x)]=\frac{1}{\ell}\sum_{i=1}^{\ell}\nabla f_{h}(x)\qquad \text{with}\qquad f_{h}(x):=\frac{1}{\text{vol}(\mathbb{B}^{d})}\int_{\mathbb{ B}^{d}}f(x+hu)\,du.\]

Rearranging terms, we get the claim. 

**Proposition 1** (Smoothing properties).: _Let \(f_{h}\) be the smooth approximation of \(f\) defined in eq. (4). Then the following hold: If \(f\) is convex then \(f_{h}\) is convex and, for every \(x\in\mathbb{R}^{d}\),_

\[f(x)\leq f_{h}(x).\]_If \(f\) is \(L_{0}\)-Lipschitz continuous - i.e. \(\forall x,y\in\mathbb{R}^{d}\), \(|f(x)-f(y)|\leq L_{0}\|x-y\|\), then \(f_{h}\) is \(L_{0}\)-Lipschitz continuous, differentiable and for every \(x,y\in\mathbb{R}^{d}\)_

\[\|\nabla f_{h}(x)-\nabla f_{h}(y)\|\leq\frac{L_{0}\sqrt{d}}{h}\|x-y\|\quad\text {and}\quad f_{h}(x)\leq f(x)+L_{0}h.\]

_If \(f\) is \(L_{1}\)-smooth - i.e. \(f\) is differentiable and \(\forall x,y\in\mathbb{R}^{d}\), \(\|\nabla f(x)-\nabla f(y)\|\leq L_{1}\|x-y\|\) then \(f_{h}\) is \(L_{1}\)-smooth and for every \(x\in\mathbb{R}^{d}\),_

\[\|\nabla f_{h}(x)-\nabla f(x)\|\leq\frac{hdL_{1}}{2}\quad\text{and}\quad f_{h }(x)\leq f(x)+\frac{L_{1}}{2}h^{2}.\]

Proof.: These are standard results proposed and proved in different works - see for example [16, Lemma 8],[21, Proposition 7.5],[34, Proposition 2.2],[49]. 

**Lemma 4** (Approximation Error).: _Let \(g(\cdot)\) be the surrogate defined in eq. (2) for arbitrary \(h>0\) and \(G\in O(d)\). Then the following hold:_

* _If_ \(f\) _is_ \(L_{0}\)_-Lipschitz (see Assumption_ 1_), then, for every_ \(x\in\mathbb{R}^{d}\)_,_ \[\mathbb{E}_{G}[\|g(x)\|^{2}]\leq 2c\frac{dL_{0}^{2}}{\ell},\] _where_ \(c\) _is a numerical constant._
* _If_ \(f\) _is_ \(L_{1}\)_-smooth (see Assumption_ 3_), then, for every_ \(x\in\mathbb{R}^{d}\)_,_ \[E_{G}[\|g(x)\|^{2}]\leq\frac{2d}{\ell}\|\nabla f(x)\|^{2}+\frac{L_{1}^{2}d^{2} }{2\ell}h^{2}.\]

Proof.: Note that, since directions are orthogonal, we have

\[\mathbb{E}_{G}[\|g(x)\|^{2}]=\frac{d^{2}}{4\ell^{2}h^{2}}\sum_{i=1}^{\ell} \mathbb{E}_{G}[(f(x+hGe_{i})-f(x-hGe_{i}))^{2}\|Ge_{i}\|^{2}].\]

By [37, Theorem 3.7],

\[\mathbb{E}_{G}[\|g(x)\|^{2}]=\frac{d^{2}}{4\ell^{2}h^{2}}\sum_{i=1}^{\ell} \mathbb{E}_{v_{i}}[(f(x+hv^{(i)})-f(x-hv^{(i)}))^{2}\|v^{(i)}\|^{2}],\] (5)

where each \(v^{(i)}\) is uniformly distributed on \(\mathbb{S}^{d-1}\).

\((i)\): Set \(\gamma=\mathbb{E}_{v^{(i)}}[f(x+hv^{(i)})]\) for every \(i\) (this expectation does not depend on \(i\)). Then

\[\mathbb{E}_{G}[\|g(x)\|^{2}] =\frac{d^{2}}{4\ell^{2}h^{2}}\sum_{i=1}^{\ell}\mathbb{E}_{v^{(i)} }[(f(x+hv^{(i)})-f(x-hv^{(i)})+\gamma-\gamma)^{2}\|v^{(i)}\|^{2}]\] \[=\frac{d^{2}}{4\ell^{2}h^{2}}\sum_{i=1}^{\ell}\mathbb{E}_{v^{(i)} }[((f(x+hv^{(i)})-\gamma)-(f(x-hv^{(i)})-\gamma))^{2}\|v^{(i)}\|^{2}]\] \[\leq\frac{d^{2}}{2\ell^{2}h^{2}}\sum_{i=1}^{\ell}\mathbb{E}_{v^{( i)}}[((f(x+hv^{(i)})-\gamma)^{2}+(f(x-hv^{(i)})-\gamma)^{2})\|v^{(i)}\|^{2}]\] \[=\frac{d^{2}}{2\ell^{2}h^{2}}\sum_{i=1}^{\ell}\Big{[}\,\mathbb{E} _{v^{(i)}}[(f(x+hv^{(i)})-\gamma)^{2}\|v^{(i)}\|^{2}]\] \[+\mathbb{E}_{v^{(i)}}[(f(x-hv^{(i)})-\gamma)^{2}\|v^{(i)}\|^{2}] \Big{]}.\]

Since \(v^{(i)}\) is uniformly distributed on \(\mathbb{S}^{d-1}\), it satisfies \(\|v^{(i)}\|^{2}=1\) and by symmetry we have

\[\mathbb{E}_{G}[\|g(x)\|^{2}]\leq\frac{d^{2}}{\ell^{2}h^{2}}\sum_{i=1}^{\ell} \mathbb{E}_{v^{(i)}}[(f(x+hv^{(i)})-\gamma)^{2}].\]The definition of \(\gamma\) yields

\[\mathbb{E}_{G}[\|g(x)\|^{2}] \leq\frac{d^{2}}{\ell^{2}h^{2}}\sum_{i=1}^{\ell}\mathbb{E}_{v^{(i)} }[((f(x+hv^{(i)})-\gamma)^{2}]\] \[=\frac{d^{2}}{\ell^{2}h^{2}}\sum_{i=1}^{\ell}\mathbb{E}_{v^{(i)} }[(f(x+hv^{(i)})-\mathbb{E}_{v^{(i)}}[f(x+hv^{(i)})])^{2}].\]

The claim follows by Lemma 3 and the fact that \(f(x+hv^{(i)})\) is \(hL_{0}\)-Lipschitz continuous w.r.t to \(v^{(i)}\).

\((ii)\): Equation (5) yields

\[\mathbb{E}_{G}[\|g(x)\|^{2}] =\frac{d^{2}}{4\ell^{2}h^{2}}\sum_{i=1}^{\ell}\mathbb{E}_{v^{(i)} }[(f(x+hv^{(i)})-f(x-hv^{(i)})-f(x)+f(x))^{2}\|v^{(i)}\|^{2}]\] \[\leq\frac{d^{2}}{2\ell^{2}h^{2}}\sum_{i=1}^{\ell}\Big{[}\,\mathbb{ E}_{v^{(i)}}[(f(x+hv^{(i)})-f(x))^{2}\|v^{(i)}\|^{2}]\] \[+\mathbb{E}_{v^{(i)}}[(f(x-hv^{(i)})-f(x))^{2}\|v^{(i)}\|^{2}] \Big{]}\] \[=\frac{d^{2}}{\ell^{2}h^{2}}\sum_{i=1}^{\ell}\mathbb{E}_{v^{(i)} }[(f(x+hv^{(i)})-f(x))^{2}],\]

where the last equation follows by symmetry. Adding and subtracting \(\left\langle\nabla f(x),hv^{(i)}\right\rangle\) we derive

\[\mathbb{E}_{G}[\|g(x)\|^{2}]\] \[\leq\frac{2d^{2}}{\ell^{2}h^{2}}\sum_{i=1}^{\ell}\Bigg{(}\,\mathbb{ E}_{v^{(i)}}\left[\Big{(}f(x+hv^{(i)})-f(x)-\left\langle\nabla f(x),hv^{(i)} \right\rangle\Big{)}^{2}\right]\] \[+\mathbb{E}_{v^{(i)}}\left[\Big{(}\left\langle\nabla f(x),hv^{(i) }\right\rangle\Big{)}^{2}\right]\Bigg{)}.\]

Denote by \(\beta(\mathbb{S}^{d-1})\) the surface area of \(\mathbb{S}^{d-1}\). The Descent Lemma [41] implies

\[\mathbb{E}_{G}[\|g(x)\|^{2}]\] \[=\frac{L_{1}^{2}d^{2}}{2\ell}h^{2}+\frac{2d^{2}}{\ell^{2}\beta( \mathbb{S}^{d-1})}\sum_{i=1}^{\ell}\int_{\mathbb{S}^{d-1}}\nabla f(x)^{\intercal }v^{(i)}v^{(i)\intercal}\nabla f(x)\,d\sigma(v).\]

By Lemma 2, we get the claim. Indeed,

\[\mathbb{E}_{G}[\|g(x)\|^{2}] \leq\frac{L_{1}^{2}d^{2}}{2\ell}h^{2}+\frac{2d^{2}}{\ell^{2} \beta(\mathbb{S}^{d-1})}\sum_{i=1}^{\ell}\Big{(}\frac{\beta(\mathbb{S}^{d-1}) }{d}\|\nabla f(x)\|^{2}\Big{)}\] \[=\frac{2d}{\ell}\|\nabla f(x)\|^{2}+\frac{L_{1}^{2}d^{2}}{2\ell} h^{2}.\]

### Auxiliary results and proofs for the nonsmooth setting, convex, and nonconvex.

In this subsection, for every \(k\), we will denote by \(\mathcal{F}_{k}\) the \(\sigma\)-algebra \(\sigma(G_{0},\ldots,G_{k-1})\).

**Lemma 5**.: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) be a lower semi-continuous function and denote with \(S=\arg\min f\) and \(f^{*}=\min f\). Then,_

\[\left\{\begin{array}{ll}\text{(A)}&\forall x^{*}\in S,\;\exists\lim_{k}\|x_{k} -x^{*}\|\\ \text{(B)}&\liminf_{k}f(x_{k})=f^{*}\end{array}\right.\implies\exists x_{ \infty}\in S\quad\text{s.t.}\quad x_{k}\to x_{\infty}.\]

Proof.: Since (B) holds, we have that exists \((x_{k_{j}})_{j\in\mathbb{N}}\) subsequence of \((x_{k})_{k\in\mathbb{N}}\) such that \(f(x_{k_{j}})\to f^{*}\). Since \(S\neq\emptyset\) and (A) we have that

\[\exists x^{*}\in S\quad\text{and}\quad\exists\lim_{k}\|x_{k}-x^{*}\|.\]

Thus, the sequence \((x_{k})_{k\in\mathbb{N}}\) is bounded and, therefore, also \((x_{k_{j}})_{j\in\mathbb{N}}\) is bounded. Taking a convergent subsequence \((x_{k_{j_{n}}})_{n\in\mathbb{N}}\) of \((x_{k_{j}})_{j\in\mathbb{N}}\), we have that exists \(x_{\infty}\) s.t.

\[x_{k_{j_{n}}}\to x_{\infty}.\]

Since \(f\) is assumed to be lower semi-continuous, we have that

\[f(x_{\infty})\leq\liminf_{n}f(x_{k_{j_{n}}})=f^{*}=\lim_{j}f(x_{k_{j}}).\]

Thus, we have that \(x_{\infty}\in S\) which implies, by (A), that

\[\exists\lim_{k}\|x_{k}-x_{\infty}\|\quad\text{and}\quad\lim_{n}\|x_{k_{j_{n}} }-x_{\infty}\|=0.\]

Hence, since \(x_{k_{j_{n}}}\) is a subsequence of \(x_{k}\),

\[\lim_{k}\|x_{k}-x_{\infty}\|=0,\]

and, therefore, \(x_{k}\to x_{\infty}\in S\). 

**Lemma 6** (Convergence: convex non-smooth).: _Assume that \(f\) is convex and \(L_{0}\) Lipschitz continuous. Let \((x_{k})_{k\in\mathbb{N}}\) be the sequence generated by Algorithm 1 and let \(x^{*}\in\arg\min f\). Then, for every \(k\in\mathbb{N}\), the following inequality holds:_

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^{2}+2\alpha_ {k}(f(x_{k})-f(x^{*}))\leq 2c\frac{L_{0}^{2}d}{\ell}\alpha_{k}^{2}+2L_{0} \alpha_{k}h_{k},\]

_where \(c\) is some non-negative constant independent from the dimension. Moreover, if the stepsizes satisfy Assumption 2, we have_

\[\lim_{k\to+\infty}f(x_{k})=f(x^{*})\quad\text{a.s,}\]

_and there exists a random variable \(\hat{x}\) taking values in in \(\arg\min f\) such that \(x_{k}\to\hat{x}\) a.s._

Proof.: Let \(k\in\mathbb{N}\). By Algorithm 1,

\[\|x_{k+1}-x^{*}\|^{2}-\|x_{k}-x^{*}\|^{2}=\alpha_{k}^{2}\|g_{k}\|^{2}-2\alpha_ {k}\left\langle g_{k},x_{k}-x^{*}\right\rangle.\] (6)

Since \(f_{h_{k}}\) is convex by Proposition 1 and \(\mathbb{E}[g_{k}|\mathcal{F}_{k}]=\nabla f_{h_{k}}(x_{k})\) (see Lemma 1), we have

\[-\left\langle\nabla f_{h_{k}}(x_{k}),x_{k}-x^{*}\right\rangle\leq f_{h_{k}}(x ^{*})-f_{h_{k}}(x_{k}).\]

Thus, taking the conditional expectation with respect to \(\mathcal{F}_{k}\), by Lemma 4, we get,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^{2}\leq \underbrace{2c\frac{L_{0}^{2}d}{\ell}\alpha_{k}^{2}}_{=:C_{k}}-2\alpha_{k}(f_ {h_{k}}(x_{k})-f_{h_{k}}(x^{*})).\]

Then, by Proposition 1,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^{2}\leq C_{k }-2\alpha_{k}(f(x_{k})-f(x^{*}))+2L_{0}\alpha_{k}h_{k}.\]

Next suppose that Assumption 2 holds. Rearranging the terms,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^{2}+2\alpha_ {k}(f(x_{k})-f(x^{*}))\leq C_{k}+2L_{0}\alpha_{k}h_{k},\]

with \(C_{k}\in\ell^{1}\) and \(\alpha_{k}h_{k}\in\ell^{1}\). Therefore, Robbins-Siegmund Theorem [43] implies that \((\|x_{k}-x^{*}\|)_{k\in\mathbb{N}}\) is a.s. convergent, \(\alpha_{k}(f(x_{k})-f(x^{*}))\in\ell^{1}\) a.s. and thus, since \(\alpha_{k}\not\in\ell^{1}\),

\[\liminf_{k\to\infty}f(x_{k})=f(x^{*})\quad\text{a.s.}\] (7)

We derive from [32, Lemma 9.9] and Lemma 5 that there exists a random variable \(\hat{x}\) taking values in \(\arg\min f\) such that \(x_{k}\to\hat{x}\) a.s. Finally, continuity of \(f\) yields that \(\lim_{k}f(x_{k})=f(x_{*})\) a.s.

In the next Lemma, to derive bounds on function values, we study the sequence \((f_{h_{k}}(x_{k+1})-f_{h_{k}}(x_{k}))_{k\in\mathbb{N}}\). It is the difference between the smoothed function at iteration \(k\) evaluated at \(x_{k}\) and at \(x_{k+1}\). It corresponds to the function value decrease between the iterations \(k+1\) and \(k\) if \(h_{k}\) is constant.

**Lemma 7** (Function Value decrease: nonconvex non-smooth setting).: _Under Assumption 1, let \((x_{k})_{k\in\mathbb{N}}\) be the sequence generated by Algorithm 1. Then,_

\[\mathbb{E}[f_{h_{k}}(x_{k+1})|\mathcal{F}_{k}]-f_{h_{k}}(x_{k})\leq-\alpha_{k} \|\nabla f_{h_{k}}(x_{k})\|^{2}+c\frac{L_{0}^{3}d\sqrt{d}}{\ell}\frac{\alpha_{ k}^{2}}{h_{k}},\]

_where \(c\) is a numerical constant._

Proof.: By Lemma 1, we have that \(f_{h_{k}}\) is \(L_{0}\sqrt{d}/h_{k}\)-smooth. Thus, by the Descent Lemma [41],

\[f_{h_{k}}(x_{k+1})-f_{h_{k}}(x_{k})\leq-\alpha_{k}\left\langle\nabla f_{h_{k}} (x_{k}),g_{k}\right\rangle+\frac{L_{0}\sqrt{d}}{2h_{k}}\alpha_{k}^{2}\|g_{k} \|^{2}.\]

Taking the conditional expectation with respect to \(\mathcal{F}_{k}\),

\[\mathbb{E}[f_{h_{k}}(x_{k+1})|\mathcal{F}_{k}]-f_{h_{k}}(x_{k})\leq-\alpha_{k }\|\nabla f_{h_{k}}(x_{k})\|^{2}+\frac{L_{0}\sqrt{d}}{2h_{k}}\alpha_{k}^{2} \,\mathbb{E}[\|g_{k}\|^{2}|\mathcal{F}_{k}].\] (8)

The claim follows from Lemma 4. 

### Auxiliary results for smooth setting.

**Lemma 8** (Function value decrease: convex smooth setting).: _Under Assumption 3, let \((x_{k})_{k\in\mathbb{N}}\) be the sequence generated by Algorithm 1. Then the following holds:_

\[\mathbb{E}[f(x_{k+1})|\mathcal{F}_{k}]-f(x_{k})\leq-\alpha_{k}\Big{(}\frac{1} {2}-\frac{L_{1}d}{\ell}\alpha_{k}\Big{)}\|\nabla f(x_{k})\|^{2}+\frac{L_{1}^{ 2}d^{2}\alpha_{k}h_{k}^{2}}{8}+\frac{L_{1}^{3}d^{2}}{4\ell}\alpha_{k}^{2}h_{k }^{2}.\]

Proof.: By the Descent Lemma [41] and Algorithm 1,

\[f(x_{k+1})-f(x_{k})\leq-\alpha_{k}\left\langle\nabla f(x_{k}),g_{k}\right\rangle +\frac{L_{1}}{2}\alpha_{k}^{2}\|g_{k}\|^{2}.\]

Taking the conditional expectation and by Lemma 4,

\[\mathbb{E}[f(x_{k+1})|\mathcal{F}_{k}]-f(x_{k})\leq-\alpha_{k}\left\langle \nabla f(x_{k}),\nabla f_{h_{k}}(x_{k})\right\rangle+\frac{L_{1}}{2}\alpha_{k }^{2}\Big{[}\frac{2d}{\ell}\|\nabla f(x_{k})\|^{2}+\frac{L_{1}^{2}d^{2}}{2 \ell}h_{k}^{2}\Big{]}.\]

Adding and subtracting \(\nabla f(x_{k})\),

\[\mathbb{E}[f(x_{k+1})|\mathcal{F}_{k}]-f(x_{k}) \leq-\alpha_{k}\left\langle\nabla f(x_{k}),\nabla f_{h_{k}}(x_{k })-\nabla f(x_{k})\right\rangle-\alpha_{k}\|\nabla f(x_{k})\|^{2}\] \[+\frac{L_{1}}{2}\alpha_{k}^{2}\Big{[}\frac{2d}{\ell}\|\nabla f( x_{k})\|^{2}+\frac{L_{1}^{2}d^{2}}{2\ell}h_{k}^{2}\Big{]}.\]

By Cauchy-Schwarz inequality and Proposition 1,

\[\mathbb{E}[f(x_{k+1})|\mathcal{F}_{k}]-f(x_{k}) \leq\alpha_{k}\Big{(}\frac{L_{1}d}{2}h_{k}\Big{)}\|\nabla f(x_{k} )\|-\alpha_{k}\|\nabla f(x_{k})\|^{2}\] \[+\frac{L_{1}}{2}\alpha_{k}^{2}\Big{[}\frac{2d}{\ell}\|\nabla f(x_ {k})\|^{2}+\frac{L_{1}^{2}d^{2}}{2\ell}h_{k}^{2}\Big{]}.\]

By Young's inequality,

\[\mathbb{E}[f(x_{k+1})|\mathcal{F}_{k}]-f(x_{k}) \leq\frac{L_{1}^{2}d^{2}\alpha_{k}h_{k}^{2}}{8}+\frac{\alpha_{k} }{2}\|\nabla f(x_{k})\|^{2}-\alpha_{k}\|\nabla f(x_{k})\|^{2}\] \[+\frac{L_{1}}{2}\alpha_{k}^{2}\Big{[}\frac{2d}{\ell}\|\nabla f(x _{k})\|^{2}+\frac{L_{1}^{2}d^{2}}{2\ell}h_{k}^{2}\Big{]}.\] \[=-\alpha_{k}\Big{(}\frac{1}{2}-\frac{L_{1}d}{\ell}\alpha_{k}\Big{)} \|\nabla f(x_{k})\|^{2}+\frac{L_{1}^{2}d^{2}\alpha_{k}h_{k}^{2}}{8}+\frac{L_{1} ^{3}d^{2}}{4\ell}\alpha_{k}^{2}h_{k}^{2}.\]

This concludes the proof.

**Lemma 9** (Convergence in smooth setting).: _Let \((x_{k})_{k\in\mathbb{N}}\) be the sequence generated by Algorithm 1 and let \(x^{*}\in\operatorname*{arg\,min}_{x\in\mathbb{R}^{d}}f(x)\). Then, under Assumption 3, the following inequality holds_

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}|-\|x_{k}-x^{*}\|^ {2} \leq\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}+\frac{L_{ 1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{2}\] \[\quad+L_{1}d\alpha_{k}h_{k}\|x_{k}-x^{*}\|-2\alpha_{k}\left\langle \nabla f(x_{k}),x_{k}-x^{*}\right\rangle.\]

_Moreover, if \(f\) is convex, Assumption 4 holds and \(\alpha_{k}\leq\bar{\alpha}<\ell/(2dL_{1})\). Then_

* \((\alpha_{k}\|\nabla f(x_{k})\|^{2})_{k\in\mathbb{N}}\in\ell^{1}\) _a.s._
* \((\|x_{k}-x^{*}\|)_{k\in\mathbb{N}}\) _is a.s. convergent._
* \((\alpha_{k}(f(x_{k})-f(x^{*})))_{k\in\mathbb{N}}\in\ell^{1}\) _a.s._
* _there exists a random variable_ \(\hat{x}\) _taking values in_ \(\operatorname*{arg\,min}f\) _such that_ \(x_{k}\to\hat{x}\) _a.s. and_ \(\lim_{k\to\infty}f(x_{k})=\min f.\)__

Proof.: We have

\[\|x_{k+1}-x^{*}\|^{2}-\|x_{k}-x^{*}\|^{2}=\alpha_{k}^{2}\|g_{k}\|^{2}-2\alpha_ {k}\left\langle g_{k},x_{k}-x^{*}\right\rangle.\]

Taking the conditional expectation,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^{2}=\alpha_{ k}^{2}\,\mathbb{E}[\|g_{k}\|^{2}|\mathcal{F}_{k}]-2\alpha_{k}\left\langle\nabla f _{h_{k}}(x_{k}),x_{k}-x^{*}\right\rangle.\]

For every \(k\), set \(u_{k}=\|x_{k}-x^{*}\|\). By Lemma 4,

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-u_{k}^{2}\leq\frac{2d}{\ell}\alpha_{k} ^{2}\|\nabla f(x_{k})\|^{2}+\frac{L_{1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{ 2}-2\alpha_{k}\left\langle\nabla f_{h_{k}}(x_{k}),x_{k}-x^{*}\right\rangle.\]

Note that

\[-2\alpha_{k}\left\langle\nabla f_{h_{k}}(x_{k}),x_{k}-x^{*}\right\rangle=2 \alpha_{k}\left\langle\nabla f_{h_{k}}(x_{k})-\nabla f(x_{k}),x^{*}-x_{k} \right\rangle-2\alpha_{k}\left\langle\nabla f(x_{k}),x_{k}-x^{*}\right\rangle.\]

Thus,

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-u_{k}^{2} \leq\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}+\frac{L _{1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{2}\] \[\quad+2\alpha_{k}\left\langle\nabla f_{h_{k}}(x_{k})-\nabla f(x_ {k}),x^{*}-x_{k}\right\rangle-2\alpha_{k}\left\langle\nabla f(x_{k}),x_{k}-x ^{*}\right\rangle.\]

By the Cauchy-Schwarz inequality,

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-u_{k}^{2} \leq\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}+\frac{ L_{1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{2}\] \[\quad+2\alpha_{k}\|\nabla f_{h_{k}}(x_{k})-\nabla f(x_{k})\|u_{k} -2\alpha_{k}\left\langle\nabla f(x_{k}),x_{k}-x^{*}\right\rangle.\]

The first claim follows from Proposition 1. By Proposition 1 and Young's inequality with parameter \(\tau_{k}=\alpha_{k}h_{k}\), we get

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-u_{k}^{2} \leq\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}+\frac{L _{1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{2}\] (9) \[\quad+\frac{L_{1}d}{2\tau_{k}}\alpha_{k}^{2}h_{k}^{2}+\frac{L_{1} d\tau_{k}}{2}u_{k}^{2}-2\alpha_{k}\left\langle\nabla f(x_{k}),x_{k}-x^{*} \right\rangle.\] \[=\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}+\frac{L_{1}^ {2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{2}\] \[\quad+\frac{L_{1}d}{2}\alpha_{k}h_{k}+\frac{L_{1}d}{2}\alpha_{k}h _{k}u_{k}^{2}\] \[\quad-2\alpha_{k}\left\langle\nabla f(x_{k}),x_{k}-x^{*}\right\rangle.\]

Since \(f\) is convex, by Baillon-Haddad Theorem [3], we derive that

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-u_{k}^{2} \leq-2\Big{(}\frac{1}{L_{1}}-\frac{d}{\ell}\alpha_{k}\Big{)} \alpha_{k}\|\nabla f(x_{k})\|^{2}+\frac{L_{1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{ k}^{2}\] \[\quad+\frac{L_{1}d}{2}\alpha_{k}h_{k}+\frac{L_{1}d}{2}\alpha_{k}h _{k}u_{k}^{2}.\]By Assumption 4,

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-u_{k}^{2} \leq-2\underbrace{\Big{(}\frac{1}{L_{1}}-\frac{d}{\ell}\bar{\alpha} \Big{)}}_{=:\Delta}\alpha_{k}\|\nabla f(x_{k})\|^{2}+\underbrace{\frac{L_{1}d} {2}\alpha_{k}h_{k}}_{=:\rho_{k}}u_{k}^{2}\] \[+\underbrace{\frac{L_{1}d}{2}\alpha_{k}h_{k}+\frac{L_{1}^{2}d^{2} }{2\ell}\alpha_{k}^{2}h_{k}^{2}}_{=:C_{k}}.\]

Note that \(\Delta>0\). Thus, rearranging the terms

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-(1+\rho_{k})u_{k}^{2}+2\Delta\alpha_{k }\|\nabla f(x_{k})\|^{2}\leq C_{k}.\]

Since \(\rho_{k},C_{k}\in\ell^{1}\) by Assumption 4, Robbins-Siegmund Theorem [43] ensures that \((u_{k}^{2})_{k\in\mathbb{N}}\) is convergent and \((\alpha_{k}\|\nabla f(x_{k})\|^{2})_{k\in\mathbb{N}}\in\ell^{1}\) a.s. Since \(f\) is convex, it follows from (9) that

\[\mathbb{E}[u_{k+1}^{2}|\mathcal{F}_{k}]-(1+\rho_{k})u_{k}^{2}\leq\frac{2d}{ \ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}-2\alpha_{k}(f(x_{k})-f(x^{*}))+C_{ k}.\]

Robbins-Siegmund Theorem [43] implies that \((\alpha_{k}(f(x_{k})-f(x^{*})))_{k\in\mathbb{N}}\in\ell^{1}\) a.s. Assumption 4 implies that \(\alpha_{k}\not\in\ell^{1}\) therefore

\[\liminf_{k}f(x_{k})-f(x^{*})=0\text{ a.s.}\] (10)

By Lemma 8 and Assumption 4, we have that the sequence \(\mathbb{E}[f(x_{k+1})-f(x^{*})|\mathcal{F}_{k}]-(f(x_{k})-f(x^{*}))\) is upper-bounded by a sequence in \(\ell^{1}\). Thus, by Robbins-Siegmund Theorem [43], \(\lim_{k}(f(x_{k})-f(x^{*}))\) exists a.s. Then, it follows from (10) that

\[\lim_{k\to\infty}f(x_{k})=f(x^{*})\quad a.s.\]

Moreover, as we saw before, \((\|x_{k}-x^{*}\|)_{k\in\mathbb{N}}\) is convergent a.s. for every \(x^{*}\in\arg\min f\). Then, by Opial's Lemma [40], there exists a random variable \(\hat{x}\) taking values in \(\arg\min f\) such that \(x_{k}\to\hat{x}\) a.s. 

**Lemma 10** (Gradient bound: convex smooth setting).: _Suppose that Assumptions 3 and 4 hold, and assume \(f\) to be convex. Let \((x_{k})_{k\in\mathbb{N}}\) be the sequence generated by Algorithm 1. Then, for every \(k\in\mathbb{N}\) and every \(x^{*}\in\arg\min f\),_

\[\sum_{i=0}^{k}\alpha_{i}\,\mathbb{E}[\|\nabla f(x_{i})\|^{2}]\leq\frac{1}{2 \Delta}\Big{(}S_{k}+\sum_{i=0}^{k}\rho_{i}\sqrt{\mathbb{E}[\|x_{i}-x^{*}\|^{2} ]}\Big{)},\]

_and_

\[\sqrt{\mathbb{E}[\|x_{k}-x^{*}\|^{2}]}\leq\sqrt{S_{k-1}}+\sum_{i=0}^{k}\rho_ {i},\]

_where_

\[\Delta :=\Big{(}\frac{1}{L_{1}}-\frac{d}{\ell}\bar{\alpha}\Big{)}\,, \quad S_{k}:=\|x_{0}-x^{*}\|+\sum_{i=0}^{k}C_{i}\] \[C_{k} :=\frac{L_{1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{2}\quad\text{ and}\quad\rho_{k}:=L_{1}d\alpha_{k}h_{k}.\]

Proof.: By Lemma 9 we derive

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\| ^{2} \leq\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}+C_{k}\] \[+\rho_{k}\|x_{k}-x^{*}\|-2\alpha_{k}\left\langle\nabla f(x_{k}), x_{k}-x^{*}\right\rangle.\]

By Baillon-Haddad Theorem and Assumption 4,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\| ^{2} \leq-2\Delta\alpha_{k}\|\nabla f(x_{k})\|^{2}+C_{k}+\rho_{k}\|x_{ k}-x^{*}\|.\]Let \(u_{k}:=\sqrt{\mathbb{E}[\|x_{k}-x^{*}\|^{2}]}\). Taking the full expectation, by Jensen inequality we have

\[u_{k+1}^{2}-u_{k}^{2}\leq-2\Delta\alpha_{k}\,\mathbb{E}[\|\nabla f(x_{k})\|^{2}] +\rho_{k}u_{k}+C_{k}.\]

Summing the previous inequality from \(i=0,\cdots,k\), we get

\[u_{k+1}^{2}+2\Delta\sum\limits_{i=0}^{k}\alpha_{i}\,\mathbb{E}[\|\nabla f(x_{i })\|^{2}]\leq\underbrace{u_{0}^{2}+\sum\limits_{i=0}^{k}C_{i}}_{=:S_{k}}+\sum \limits_{i=0}^{k}\rho_{i}u_{i}.\] (11)

Since \(u_{k}\) is non-negative, the first claim of the lemma follows. Since \(\Delta>0\), \(\rho_{k}\geq 0\), \(S_{k}\) is non decreasing, and \(S_{k}\geq u_{0}^{2}\) in (11), then

\[u_{k+1}^{2}\leq S_{k}+\sum\limits_{i=0}^{k}\rho_{i}u_{i}.\]

Thus, the (discrete) Bihari's Lemma [32, Lemma 9.8] yields

\[u_{k+1}\leq\frac{1}{2}\sum\limits_{i=0}^{k}\rho_{i}+\left[S_{k}+\left(\frac{1 }{2}\sum\limits_{i=0}^{k}\rho_{i}\right)^{2}\right]^{1/2}\leq\sqrt{S_{k}}+ \sum\limits_{i=0}^{k}\rho_{i},\]

concluding the proof. 

## Appendix B Proofs of Main Results

### Proof of Theorem 1

By Lemma 6,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^{2}+2\alpha_ {k}(f(x_{k})-f(x^{*}))\leq 2c\frac{L_{0}^{2}d}{\ell}\alpha_{k}^{2}+2L_{0} \alpha_{k}h_{k}.\]

Rearranging the terms, taking the full expectation, and summing the first \(k\) iterations

\[\sum\limits_{i=0}^{k}\alpha_{i}\,\mathbb{E}[(f(x_{i})-f(x^{*}))]\leq\frac{\|x _{0}-x^{*}\|^{2}}{2}+c\frac{dL_{0}^{2}}{\ell}\sum\limits_{i=0}^{k}\alpha_{i} ^{2}+L_{0}\sum\limits_{i=0}^{k}\alpha_{i}h_{i}.\]

Let \(\bar{x}_{k}:=\sum\limits_{i=0}^{k}\alpha_{i}x_{i}/(\sum\limits_{i=0}^{k}\alpha _{i})\). Dividing by \(\sum\limits_{i=0}^{k}\alpha_{i}\) and observing that by convexity we have

\[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{\sum\limits_{i=0}^{k}\alpha_{i}\, \mathbb{E}[(f(x_{i})-f(x^{*}))]}{\sum\limits_{i=0}^{k}\alpha_{i}},\]

we get the first claim. Under Assumption 2, the second claim holds by Lemma 6.

### Proof of Corollary 1

By Theorem 1,

\[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{1}{\sum\limits_{i=0}^{k}\alpha_{i} }\Bigg{(}\frac{\|x_{0}-x^{*}\|^{2}}{2}+c\frac{dL_{0}^{2}}{\ell}\sum\limits_{i= 0}^{k}\alpha_{i}^{2}+L_{0}\sum\limits_{i=0}^{k}\alpha_{i}h_{i}\Bigg{)}.\]

Replacing \(\alpha_{k}\) and \(h_{k}\) with the sequences in the statement,

\[\mathbb{E}[f(\bar{x}_{k})-f(x^{*})]\leq\frac{C_{1}}{\alpha k^{1-\theta}}+\frac {C_{2}}{k^{\rho}}h+\frac{d}{\ell}\frac{C_{3}}{k^{\theta}}\alpha,\]with

\[C_{1}:=\frac{(1-\theta)\|x_{0}-x^{*}\|^{2}}{2},\qquad C_{2}:=\frac{L_{0}(1-\theta) }{(1-\theta-\rho)}\quad\text{and}\quad C_{3}:=\frac{cL_{0}^{2}(1-\theta)}{(1-2 \theta)}.\]

The second point of the corollary can be proved replacing \(\alpha_{k}=\alpha\) and \(h_{k}=h\). Now, to prove the third point, fix \(\varepsilon\in(0,1)\). Since we want \(\mathbb{E}[f(\bar{x}_{k})-f(x^{*})]\leq\varepsilon\), we impose

\[\frac{\|x_{0}-x^{*}\|^{2}}{2\alpha k}+\frac{cdL_{0}^{2}}{\ell}\alpha+L_{0}h \leq\varepsilon.\]

Choosing \(h_{k}=h\leq\frac{\varepsilon}{2L_{0}}\), to get the previous inequality it is sufficient to impose

\[\frac{\|x_{0}-x^{*}\|^{2}}{2\alpha k}+\frac{cdL_{0}^{2}}{\ell}\alpha\leq\frac {\varepsilon}{2}.\]

We fix a priori a number of iterations \(K\) and we minimize the left handside with respect to \(\alpha\), obtaining

\[\alpha=\sqrt{\frac{\ell}{d}}\frac{\|x_{0}-x^{*}\|}{\sqrt{2cK}L_{0}}.\]

Thus, for \(h_{k}=h\leq\frac{\varepsilon}{2L_{0}}\), \(\alpha\) as above and

\[K\geq\frac{8\|x_{0}-x^{*}\|^{2}L_{0}^{2}cd}{\ell\varepsilon^{2}},\]

we have \(\mathbb{E}[f(\bar{x}_{k})-f(x^{*})]\leq\varepsilon\). Note that, since the computation of the surrogate requires \(2\ell\) function evaluations, to ensure an error of \(\varepsilon\) we need to perform a number of function evaluations of the order

\[\mathcal{O}(d\varepsilon^{-2}).\]

This concludes the proof.

### Proof of Theorem 2

By Lemma 7,

\[\mathbb{E}[f_{h}(x_{k+1})|\mathcal{F}_{k}]-f_{h}(x_{k})\leq-\alpha_{k}\|\nabla f _{h}(x_{k})\|^{2}+c\frac{L_{0}^{3}d\sqrt{d}}{\ell}\frac{\alpha_{k}^{2}}{h}.\]

Taking the full expectation and rearranging the terms,

\[\alpha_{k}\,\mathbb{E}[\|\nabla f_{h}(x_{k})\|^{2}]\leq\mathbb{E}[f_{h}(x_{k} )-f_{h}(x_{k+1})]+c\frac{L_{0}^{3}d\sqrt{d}}{\ell}\frac{\alpha_{k}^{2}}{h}.\]

Next sum from \(i=0\) to \(i=k\). By definition of \(f_{h}\), we have \(f_{h}(x)\geq\min f\) for every \(x\in\mathbb{R}^{d}\), thus,

\[\sum_{i=0}^{k}\alpha_{i}\,\mathbb{E}[\|\nabla f_{h}(x_{i})\|^{2}]\leq\mathbb{ E}[f_{h}(x_{0})-\min f]+c\frac{L_{0}^{3}d\sqrt{d}}{\ell}\sum_{i=0}^{k}\frac{ \alpha_{i}^{2}}{h}.\] (12)

The claim follows.

### Proof of Corollaries 2 and 3

By Theorem 2,

\[\eta_{k}^{(h)}\leq\Big{(}(f_{h}(x_{0})-f(x^{*}))+c\frac{L_{0}^{3}d\sqrt{d}}{ \ell}\sum_{i=0}^{k}\frac{\alpha_{i}^{2}}{h}\Big{)}/\Big{(}\sum_{i=0}^{k}\alpha _{i}\Big{)}.\]

Due to the choice of \(\alpha_{k}=\alpha(k+1)^{-\theta}\) with \(\theta\in(1/2,1)\) and \(\alpha>0\), we get

\[\eta_{k}^{(h)}\leq\frac{C_{1}}{\alpha(k+1)^{1-\theta}}+\frac{C_{2}d\sqrt{d} \alpha}{\ell h}\frac{1}{(k+1)^{\theta}},\]where

\[C_{1}:=\|x_{0}-x^{*}\|^{2}(1-\theta)\quad\text{and}\quad C_{2}:=\frac{cL_{0}^{3}(1 -\theta)}{(1-2\theta)}.\]

If we choose \(\alpha_{k}=\alpha\), we derive

\[\eta_{k}^{(h)}\leq\frac{f_{h}(x_{0})-\min f}{\alpha k}+\frac{cL_{0}^{3}d\sqrt{ d}\alpha}{\ell h}.\] (13)

If we fix a priori a number of iteration \(K\) and we minimize the right handside with respect to \(\alpha\), we get

\[\hat{\alpha}=\sqrt{\frac{(f_{h}(x_{0})-f(x^{*}))\ell h}{KcL_{0}^{3}d\sqrt{d}}}.\]

Let \(\varepsilon\in(0,1)\). Choosing \(\alpha=\hat{\alpha}\), we get \(\eta_{K}^{(h)}\leq\varepsilon\) for

\[K\geq 4\frac{(f_{h}(x_{0})-f(x^{*}))cL_{0}^{3}d\sqrt{d}}{\ell h}\varepsilon^{-2}.\] (14)

This concludes the proof of Corollary 2. To prove Corollary 3, we fix a maximum number of iterations \(K\in\mathbb{N}\) and consider the random variable \(I\) of the statement. Let \(\partial_{h}f\) be the \(h\)-Goldstein subdifferential defined in Definition 1. It follows from [34, Theorem 3.1] that \(\nabla f_{h}(x_{I})\in\partial_{h}f(x_{I})\) almost surely, therefore

\[\mathbb{E}_{I}\min[\|\eta\|^{2}\,:\,\eta\in\partial_{h}f(x_{I})]\leq\mathbb{E }_{I}\,\mathbb{E}[\|\nabla f_{h}(x_{I})\|^{2}].\]

In addition, Theorem 2 yields

\[\mathbb{E}_{I}\,\mathbb{E}_{G}[\|\nabla f_{h}(x_{I})\|^{2}] = \bigg{(}\sum_{j=0}^{K-1}\alpha_{j}\,\mathbb{E}_{G}[\|\nabla f_{h} (x_{j})\|^{2}]\bigg{)}/\sum_{j=0}^{K-1}\alpha_{j}\] \[\leq \mathbb{E}[f_{h}(x_{0})-\min f]+c\frac{L_{0}^{3}d\sqrt{d}}{\ell} \sum_{i=0}^{k}\frac{\alpha_{i}^{2}}{h}.\]

Thus,

\[\mathbb{E}_{I}[\|\eta\|^{2}\,:\,\eta\in\partial_{h}f(x_{I})]\leq\mathbb{E}_{I }\,\mathbb{E}[\|\nabla f_{h}(x_{I})\|^{2}]=\eta_{k}^{(h)}.\]

Hence, for \(\alpha=\bar{\alpha}\) and \(K\) chosen s.t. inequality (14) holds, we have

\[\mathbb{E}_{I}[\|\eta\|^{2}\,:\,\eta\in\partial_{h}f(x_{I})]\leq\varepsilon.\]

This concludes the proof.

### Proof of Theorem 3

By Lemma 9,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^ {2} \leq\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}+2\alpha _{k}\,\langle\nabla f(x_{k}),x^{*}-x_{k}\rangle\] \[+\underbrace{L_{1}d\alpha_{k}h_{k}}_{=:\rho_{k}}\|x^{*}-x_{k}\|+ \underbrace{\frac{L_{1}^{2}d^{2}}{2\ell}\alpha_{k}^{2}h_{k}^{2}}_{=:C_{k}}.\]

By convexity,

\[\mathbb{E}[\|x_{k+1}-x^{*}\|^{2}|\mathcal{F}_{k}]-\|x_{k}-x^{*}\|^ {2} \leq\frac{2d}{\ell}\alpha_{k}^{2}\|\nabla f(x_{k})\|^{2}-2\alpha _{k}(f(x_{k})-f(x^{*}))\] \[+\rho_{k}\|x^{*}-x_{k}\|+C_{k}.\]

Rearranging the terms and taking the full expectation,

\[2\,\mathbb{E}[\alpha_{k}(f(x_{k})-f(x^{*}))] \leq\mathbb{E}[\|x_{k}-x^{*}\|^{2}-\|x_{k+1}-x^{*}\|^{2}]+\frac{2 d}{\ell}\alpha_{k}^{2}\,\mathbb{E}[\|\nabla f(x_{k})\|^{2}]\] \[+\rho_{k}\,\mathbb{E}[\|x^{*}-x_{k}\|]+C_{k}.\]Since \(\mathbb{E}[\|x^{*}-x_{k}\|]=\mathbb{E}[\sqrt{\|x^{*}-x_{k}\|^{2}}]\), Jensen's inequality implies that

\[2\,\mathbb{E}[\alpha_{k}(f(x_{k})-f(x^{*}))] \leq\mathbb{E}[\|x_{k}-x^{*}\|^{2}-\|x_{k+1}-x^{*}\|^{2}]+\frac{2d }{\ell}\alpha_{k}^{2}\,\mathbb{E}[\|\nabla f(x_{k})\|^{2}]\] \[+\rho_{k}\sqrt{\mathbb{E}[\|x^{*}-x_{k}\|^{2}]}+C_{k}.\]

Denoting with \(u_{k}=\mathbb{E}[\|x_{k}-x^{*}\|^{2}]\) and taking the sum from \(i=0\) to \(i=k\),

\[2\sum\limits_{i=0}^{k}\alpha_{i}\,\mathbb{E}[f(x_{i})-f(x^{*})] \leq\underbrace{u_{0}^{2}+\sum\limits_{i=0}^{k}C_{i}}_{=:S_{k}} +\frac{2d}{\ell}\sum\limits_{i=0}^{k}\alpha_{i}^{2}\,\mathbb{E}[\|\nabla f(x_ {i})\|^{2}]+\sum\limits_{i=0}^{k}\rho_{i}u_{i}\] \[\leq S_{k}+\frac{2d}{\ell}\bar{\alpha}\sum\limits_{i=0}^{k} \alpha_{i}\,\mathbb{E}[\|\nabla f(x_{i})\|^{2}]+\sum\limits_{i=0}^{k}\rho_{i} u_{i},\]

where the last inequality holds by Assumption 4. Let \(\Delta:=(1/L_{1}-(d/\ell)\bar{\alpha})\). By Lemma 10, we have

\[\sum\limits_{i=0}^{k}\alpha_{i}\,\mathbb{E}[f(x_{i})-f(x^{*})] \leq\frac{1}{2}\Big{(}S_{k}+\frac{d\bar{\alpha}}{\Delta\ell} \Big{[}S_{k}+\sum\limits_{i=0}^{k}\rho_{i}u_{i}\Big{]}+\sum\limits_{i=0}^{k} \rho_{i}u_{i}\Big{)}\] \[=\frac{\ell\Delta+d\bar{\alpha}}{2\ell\Delta}\Big{(}S_{k}+\sum \limits_{i=0}^{k}\rho_{i}u_{i}\Big{)}\] \[\leq\frac{\ell\Delta+d\bar{\alpha}}{2\ell\Delta}\Big{(}S_{k}+\sum \limits_{i=0}^{k}\rho_{i}(\sqrt{S_{i}}+\sum\limits_{j=0}^{i}\rho_{j})\Big{)}.\]

Let \(\bar{x}_{k}:=\sum\limits_{i=0}^{k}\alpha_{i}x_{i}/(\sum\limits_{i=0}^{k} \alpha_{i})\). Dividing both sides by \(\sum\limits_{i=0}^{k}\alpha_{i}\), convexity yields

\[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{\sum\limits_{i=0}^{k}\alpha_{i}\, \mathbb{E}[(f(x_{i})-f(x^{*}))]}{\sum\limits_{i=0}^{k}\alpha_{i}}.\]

### Proof of Corollary 4

In this proof, we use the same notation as the one in the proof of Theorem 3. By the choices of the parameters, we have

\[\sum\limits_{i=0}^{k}\rho_{i}\leq C_{1}d\alpha h\quad\text{with}\quad C_{1}:= \frac{L_{1}\theta}{\theta-1},\]

\[S_{k}\leq\|x_{0}-x^{*}\|^{2}+C_{2}\frac{d^{2}}{\ell}\alpha^{2}h^{2}\quad\text{ with}\quad C_{2}:=\frac{L_{1}^{2}\theta}{2\theta-1}.\]

Thus, using these inequalities in Theorem 3, we get

\[D_{k} \leq\frac{\ell\Delta+d\bar{\alpha}}{2\ell\Delta}\Big{(}\|x_{0}- x^{*}\|^{2}+C_{2}\frac{d^{2}\alpha^{2}h^{2}}{\ell}+\sqrt{\|x_{0}-x^{*}\|^{2}}C_{3}d \alpha h\] \[+C_{4}\frac{d\alpha h}{\sqrt{\ell}}+C_{5}d^{2}\alpha^{2}h^{2} \Big{)},\]

with

\[C_{3}:=\frac{L_{1}^{2}\theta}{\theta-1},\quad C_{4}:=\frac{L_{1}^{2}}{\sqrt{ 2}},\quad C_{5}:=\frac{L_{1}^{2}\theta}{(\theta-1)^{2}}.\]

Dividing by \(\sum\limits_{i=0}^{k}\alpha_{i}\), we get

\[\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\frac{C}{\alpha k}.\]Note that by Assumption 4, \(\alpha<\ell/(dL_{1})\), thus \(1/\alpha>(dL_{1})/\ell\). The algorithm performs \(2\ell\) function evaluations at each iteration. Thus, to guarantee \(\mathbb{E}[f(\bar{x}_{k})-\min f]\leq\varepsilon\) for \(\varepsilon\in(0,1)\), the algorithm has to perform a number of function evaluations in the order of

\[\mathcal{O}(d\varepsilon^{-1}).\]

Assuming, instead, \(\alpha_{k}\leq\bar{\alpha}<\ell/(2dL_{1})\), by Lemma 9 we get the last claim; i.e, there exists a random variable \(\hat{x}\) taking values in \(\arg\min f\) s.t. \(x_{k}\to\hat{x}\) a.s.

### Proof of Theorem 4

Set \(C_{1}=(dL_{1})/2\). It follows from Lemma 8 that

\[\mathbb{E}[f(x_{k+1})|\mathcal{F}_{k}]-f(x_{k})\leq-\Big{(}\frac{1}{2}-\frac{ L_{1}d}{\ell}\bar{\alpha}\Big{)}\alpha_{k}\|\nabla f(x_{k})\|^{2}+\frac{C_{1}^{2} \alpha_{k}h_{k}^{2}}{2}+\frac{L_{1}^{3}d^{2}}{4\ell}\alpha_{k}^{2}h_{k}^{2}.\]

Taking the full expectation and rearranging the terms, and recalling the definition of \(\Delta\),

\[\Delta\alpha_{k}\,\mathbb{E}[\|\nabla f(x_{k})\|^{2}]\leq\mathbb{E}[f(x_{k})- f(x_{k+1})]+\frac{C_{1}^{2}\alpha_{k}h_{k}^{2}}{2}+\frac{L_{1}^{3}d^{2}}{4 \ell}\alpha_{k}^{2}h_{k}^{2}.\]

Summing for \(i=0,\cdots,k\) and observing that \(\min f\leq f(x)\) for every \(x\),

\[\Delta\sum_{i=0}^{k}\alpha_{i}\,\mathbb{E}[\|\nabla f(x_{i})\|^{2}]\leq f(x_{0 })-\min f+\sum_{i=0}^{k}\frac{C_{1}^{2}\alpha_{i}h_{i}^{2}}{2}+\frac{L_{1}^{3} d^{2}}{4\ell}\sum_{i=0}^{k}\alpha_{i}^{2}h_{i}^{2}.\]

Divinding by \(\Delta\sum\limits_{i=0}^{k}\alpha_{i}\) we get the claim.

### Proof of Corollary 5

\((i)\): From the choice of \(\alpha_{k}\) and \(h_{k}\), we have

\[\sum_{i=0}^{k}\alpha_{i}h_{i}^{2}\leq\frac{2\theta\alpha h^{2}}{2\theta-1} \qquad\sum_{i=0}^{k}\alpha_{i}^{2}h_{i}^{2}\leq\frac{2\theta\alpha^{2}h^{2}}{ 2\theta-1}.\]

It follows from Theorem 4 that

\[\eta_{k}\leq\frac{1}{\Delta\alpha k}\Bigg{(}f(x_{0})-\min f+C_{1}d^{2}\alpha h ^{2}+\frac{C_{2}\alpha^{2}h^{2}d^{2}}{\ell}\Bigg{)},\]

with \(C_{1}=\frac{L_{1}^{2}\theta}{4(2\theta-1)}\) and \(C_{2}=\frac{L_{1}^{3}\theta}{2(2\theta-1)}\).

\((ii)\): It follows directly from Theorem 4 taking into account that

\[\sum_{i=0}^{k}\alpha_{i}h_{i}^{2}=k\alpha h^{2},\qquad\sum_{i=0}^{k}\alpha_{i} ^{2}h_{i}^{2}=k\alpha^{2}h^{2},\]

and setting \(C_{1}=L_{1}^{2}/8\) and \(C_{2}=L_{1}^{3}/4\).

## Appendix C Experimental Details

In this appendix, we report details on the experiments performed. We implemented every script in Python3 (version 3.9.11) and used numpy (version 1.22.2) [27] and matplotlib (version 3.5.1) [29] libraries.

Machine used to perform the experiments.In the following table, we describe the features of the machine used to perform the experiments in Section 4.

\begin{table}
\begin{tabular}{l l} \hline \hline Feature & \\ \hline OS & Debian GNU/Linux 11 \\ CPU(s) & 4 x Intel(R) Core(TM) i7-1165G7 11th Gen @ 2.80GHz \\ CPU Core(s) & 4 \\ RAM & 8 GB \\ \hline \hline \end{tabular}
\end{table}
Table 1: Machine used to perform the experimentsTarget Functions.We considered two synthetic target functions: a convex smooth function \(f_{1}\) and a convex non-smooth function \(f_{2}\) defined as follows

\[(\text{Convex Smooth})\quad f_{1}(x) :=\frac{1}{2}\|Ax\|^{2}\quad\text{with}\quad A\in\mathbb{R}^{d \times d}\] \[(\text{Convex Non-smooth})\quad f_{2}(x) :=\|x-\bar{v}\|_{1}\]

where \(A\) is a random Gaussian matrix (i.e. \(A_{i,j}\sim\mathcal{N}(0,1)\)) and \(\bar{v}:=[0,1,\cdots,d-1]^{\intercal}\).

Choice of the number of directions.We report here the details of the first experiment of Section 4. For these experiments, we consider \(d=50\) and we use, for the smooth convex case, the following parameters

\[\alpha_{k}=0.99\frac{\ell}{dL_{1}}\qquad\text{and}\qquad h_{k}=\frac{10^{-5}} {k+1}.\]

The constant \(L_{1}\) is computed as the maximum eigenvalue of the matrix \(A^{\intercal}A\). Note that this parameter choice satisfies Assumption 4. For the non-smooth target, we used

\[\alpha_{k}=\sqrt{\frac{\ell}{d}}k^{-1/2-10^{-5}}\qquad\text{and}\qquad h_{k}= \frac{10^{-7}}{k+1}.\]

Note that this parameter configuration satisfies Assumption 2. The maximum number of function evaluations considered is \(4000\). The direction matrices \(G_{k}\) are generated with the QR method - see Appendix D.

Comparison with Finite-difference methods.In Section 4, we compare finite-difference method with different choice of directions. In order to make a fair comparison we consider only central finite-differences. However, note that Algorithm 1 can be modified (in practice) considering computationally cheaper gradient estimators - see Remark 1. For these experiments, we consider \(d=10\) and \(\ell=d\) for methods with multiple directions. The maximum number of function evaluations is \(1000\) for both smooth and non-smooth targets and the direction matrices \(G_{k}\) for Algorithm 1 are generated with the QR method - see Appendix D. To solve the smooth problem we consider the following parameter choice for every method

\[\alpha_{k}=c\frac{\ell}{dL_{1}}\qquad\text{and}\qquad h_{k}=\frac{10^{-7}}{d^ {2}(k+1)},\]

where \(L_{1}\) is computed taking the maximum eigenvalue of \(A^{\intercal}A\). For Algorithm 1 and finite-difference with single and multiple spherical directions \(c=0.99\) while it is equal to \(c=0.11\) for finite-difference with single and multiple Gaussian directions. We made this choice since for finite-difference methods with Gaussian directions we observed divergence for larger choices of \(c\) - see Figure 3.

For the non-smooth convex target, we considered the following parameter choice

\[\alpha_{k}=c\frac{\ell}{d}k^{-1/2-10^{-5}}\qquad\text{and}\qquad h_{k}=\frac{ 1}{d^{2}(k+1)}.\]

Figure 3: From left to right, comparison of finite-difference methods for smooth convex target with \(c=0.99\) and \(c=0.2\) for methods with Gaussian directions.

For every method, we selected \(c=0.65\) except for the method with multiple Gaussian directions in which we selected \(c=0.08\) since it provided better performances - see Figure 4.

## Appendix D Techniques to Generate Orthogonal Direction Matrices

In the literature, different algorithms were proposed to generate orthogonal matrices - see for instance [23, 38, 11, 28, 7, 2, 4, 44, 8] and references therein. Such methods can be used to generate the direction matrices \(G_{k}\) required for the iteration proposed in Algorithm (1). In this appendix, we briefly discuss three of them.

QR factorization.As observed in [32, 42], a way to generate orthogonal consists in generating a random Gaussian matrix \(A\in\mathbb{R}^{d\times d}\) with \(A_{i,j}\sim\mathcal{N}(0,1)\) and perform the QR factorization i.e. \(A=QR\). Then, the direction matrix is the truncation of the \(Q\) matrix i.e. \(QI_{d,\ell}\).

Householder Reflection.To obtain a direction matrix, we can use a Householder reflector. This can be done by sampling a vector \(v\) from the unit sphere \(\mathbb{S}^{d-1}\). The direction matrix \(G\) is defined as a Householder reflector, given by

\[G:=I-2vv^{\intercal},\]

with \(I\in\mathbb{R}^{d\times d}\) identity matrix. To obtain the desired matrix, we compute the product of \(G\) with \(I_{d,\ell}\), i.e., we take the first \(\ell\) columns. The (truncated) identity matrix can be generated and stored offline (note that since it is very sparse, it can be stored using a sparse format (e.g. the COO format proposed in scikit-learn library[9]). In this way, we can save resources in high-dimensional settings. In order to quantify the time-cost of this procedure, we compared the time of generating this kind of matrix with random matrices with different dimensions. For this experiment, we consider the \(\ell=d\) case i.e. the most expensive. Matrices are computed in CPU and the details of the machine used are described in Appendix C. We report the mean and standard deviation of the time using \(500\) repetitions. In Figure 5, we compare the time-cost of generating orthogonal matrices with this procedure against generating random matrices while in Table 2 we report the mean and standard deviation of the results.

Figure 4: From left to right and up to down, comparison of finite difference method with different directions and different values of \(c\) for multiple Gaussian directions. The values of \(c\) considered are the following \([0.085,0.089,0.09,0.1,0.2,0.3,0.5,0.65]\)

In Figure 5, we can observe that using this strategy we can limit the cost of generating random orthogonal matrices. In particular, for dimensions larger than \(32\), our method is faster than random gaussian and spherical directions.

Moreover, if more computational resources are available, we can build \(m>1\) Householder reflectors \(G_{1},\cdots,G_{m}\) using \(m\) random vectors \(v_{1},\cdots,v_{m}\) sampled i.i.d from \(\mathbb{S}^{d-1}\) and define the direction matrix as

\[G_{1}G_{2}\cdots G_{m}I_{d,\ell}.\]

It is important to note that when \(m=d\), this procedure is equivalent to using the QR factorization.

Haar Butterfly matrices.We can build orthogonal matrices using Butterfly matrices [48]. Let \(G^{(0)}:=[1]\), we can build an orthogonal matrix of dimension \(d=2^{n}\) with the following recursion

\[G^{(n)}=\begin{bmatrix}\cos(\theta_{n})G^{(n-1)}&\sin(\theta_{n})G^{(n-1)}\\ -\sin(\theta_{n})G^{(n-1)}&\cos(\theta_{n})G^{(n-1)}\end{bmatrix}\]

where \(\theta_{n}\) is sampled uniformly in \([0,2\pi]\). Then we compute \(GI_{d,\ell}\) (we take the first \(\ell\) columns). The construction of Haar butterfly matrices is faster than previous methods because it only requires simple operations. However, this procedure allows to build only matrices with \(d=2^{n}\) for \(n\geq 0\). In literature, different methods were proposed to cope with this limitation e.g. [23].

\begin{table}
\begin{tabular}{l l l l} \hline \hline \(d\) & Random Gaussian & Random Spherical & Householder \\ \hline \(2\) & \(9.27\times 10^{-7}\pm 7.96\times 10^{-7}\) & \(5.49\times 10^{-6}\pm 2.05\times 10^{-6}\) & \(9.32\times 10^{-6}\pm 3.34\times 10^{-6}\) \\ \(4\) & \(1.30\times 10^{-6}\pm 7.21\times 10^{-7}\) & \(6.56\times 10^{-6}\pm 2.63\times 10^{-6}\) & \(1.12\times 10^{-5}\pm 5.79\times 10^{-6}\) \\ \(8\) & \(2.18\times 10^{-6}\pm 6.06\times 10^{-7}\) & \(8.01\times 10^{-6}\pm 5.32\times 10^{-6}\) & \(1.11\times 10^{-5}\pm 5.20\times 10^{-6}\) \\ \(8\) & \(2.18\times 10^{-6}\pm 6.06\times 10^{-7}\) & \(8.01\times 10^{-6}\pm 5.32\times 10^{-6}\) & \(1.11\times 10^{-5}\pm 5.20\times 10^{-6}\) \\ \(16\) & \(5.69\times 10^{-6}\pm 1.61\times 10^{-6}\) & \(1.15\times 10^{-5}\pm 4.10\times 10^{-6}\) & \(1.18\times 10^{-5}\pm 7.20\times 10^{-6}\) \\ \(32\) & \(1.78\times 10^{-5}\pm 6.42\times 10^{-6}\) & \(2.49\times 10^{-5}\pm 1.33\times 10^{-5}\) & \(1.16\times 10^{-5}\pm 7.25\times 10^{-6}\) \\ \(64\) & \(6.58\times 10^{-5}\pm 7.03\times 10^{-6}\) & \(7.74\times 10^{-5}\pm 1.95\times 10^{-5}\) & \(1.62\times 10^{-5}\pm 3.79\times 10^{-6}\) \\ \(128\) & \(2.73\times 10^{-4}\pm 2.37\times 10^{-5}\) & \(2.98\times 10^{-4}\pm 2.45\times 10^{-5}\) & \(3.32\times 10^{-5}\pm 4.02\times 10^{-6}\) \\ \(256\) & \(1.26\times 10^{-3}\pm 2.79\times 10^{-5}\) & \(1.36\times 10^{-3}\pm 2.90\times 10^{-5}\) & \(1.20\times 10^{-4}\pm 1.04\times 10^{-4}\) \\ \(512\) & \(5.50\times 10^{-3}\pm 1.63\times 10^{-4}\) & \(5.91\times 10^{-3}\pm 1.22\times 10^{-4}\) & \(1.22\times 10^{-3}\pm 3.82\times 10^{-4}\) \\ \(1024\) & \(2.16\times 10^{-2}\pm 6.92\times 10^{-4}\) & \(2.41\times 10^{-2}\pm 7.35\times 10^{-4}\) & \(4.83\times 10^{-3}\pm 2.26\times 10^{-3}\) \\ \(2048\) & \(8.92\times 10^{-2}\pm 8.19\times 10^{-2}\) & \(1.04\times 10^{-1}\pm 1.03\times 10^{-1}\) & \(2.40\times 10^{-2}\pm 3.87\times 10^{-2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of the time-cost (seconds) of generating random and orthogonal matrices with different dimensions

Figure 5: Time comparison in CPU of different methods to generate direction matrices.

Limitations

In this appendix, we discuss the main practical limitations of Algorithm 1. Like all finite-difference methods with multiple directions, O-ZD requires multiple function evaluations to execute a single step. In many practical applications, function evaluations can be time-consuming, leading to the use of a small number of directions \(\ell\). This may result in poor performance as observed in numerical experiments. As for the subgradient method, in O-ZD the step size significantly affects performance, and tuning it can be challenging. To address this limitation, an adaptive stepsize selection method could be proposed. Furthermore, decreasing the sequence \(h_{k}\) too quickly can lead to numerical instability, as noted in [42].

## Appendix F Other Experiments

We performed other experiments in minimizing convex functions. We considered the targets defined in Table 3 and, for each experiment, we reported the mean and standard deviation using \(20\) repetitions.

In Table 3, we define the function used for the experiments. In particular:

\begin{table}
\begin{tabular}{l l l l} \hline \hline Name & Definition & \(d\) & \(\ell\) \\ \hline Sparse Group Lasso & \(f(x):=\sum\limits_{i=1}^{p}\|x^{(\beta_{i})}\|\) & & \(50\) & \(25\) \\ Huber Loss & \(f(x):=\left\{\begin{array}{ll}0.5\|x\|_{2}^{2}&\|x\|_{2}\leq\delta\\ \delta\|x\|_{2}-0.5\delta^{2}&\text{otherwise}\end{array}\right.\) & for \(\delta>0\) & \(50\) & \(25\) \\ Elastic Net & \(f(x):=\alpha\|x\|_{1}+0.5\beta\|x\|_{2}^{2}\) & & \(50\) & \(25\) \\ L1 & \(f(x):=\|x\|_{1}\) & & \(50\) & \(25\) \\ Infinity Norm & \(f(x):=\|x\|_{\infty}\) & & \(50\) & \(20\) \\ Total Variation & \(f(x):=\|x\|_{\text{TV}}\) & & \(50\) & \(25\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Functions used and relative dimension and number of directions considered.

Figure 6: Function values per function evaluation in optimizing functions with different algorithms.

* Sparse Group Lasso: \(p\) is set to \(3\) and given an \(x\in\mathbb{R}^{d}\), \(x^{(\beta_{i})}\) is a vector obtained by taking \(3\) entries of \(x\).
* Huber Loss: \(\delta\) is set to \(0.5\).
* Elastic Net: \(\alpha,\beta\) are set to \(0.5\).