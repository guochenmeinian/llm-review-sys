Improved Guarantees for Fully Dynamic \(k\)-Center Clustering with Outliers in General Metric Spaces

 Leyla Biabani

Eindhoven University of Technology

Eindhoven, The Netherlands

l.biabani@tue.nl

&Annika Hennes

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

annika.hennes@hhu.de

Denise La Gordt Dillie

Eindhoven University of Technology

Eindhoven, The Netherlands

lagordtdilliedenise@gmail.com

&Morteza Monemizadeh

Eindhoven University of Technology

Eindhoven, The Netherlands

m.monemizadeh@tue.nl

Melanie Schmidt

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

mschmidt@hhu.de

###### Abstract

The metric \(k\)-center clustering problem with \(z\) outliers, also known as \((k,z)\)-center clustering, involves clustering a given point set \(P\) in a metric space \((M,d)\) using at most \(k\) balls, minimizing the maximum ball radius while excluding up to \(z\) points from the clustering. This problem holds fundamental significance in various domains, such as machine learning, data mining, and database systems.

This paper addresses the fully dynamic version of the problem, where the point set undergoes continuous updates (insertions and deletions) over time. The objective is to maintain an approximate \((k,z)\)-center clustering with efficient update times. We propose a novel fully dynamic algorithm that maintains a \((4+\epsilon)\)-approximate solution to the \((k,z)\)-center clustering problem that covers all but at most \((1+\epsilon)z\) points at any time in the sequence with probability \(1-k/\epsilon^{\Omega(\log k)}\). The algorithm achieves an expected amortized update time of \(\mathcal{O}(\epsilon^{-3}k^{6}\log(k)\log(\Delta))\), and is applicable to general metric spaces. Our dynamic algorithm presents a significant improvement over the recent dynamic \((14+\epsilon)\)-approximation algorithm by Chan, Lattanzi, Sozio, and Wang [5] for this problem.

## 1 Introduction

Clustering problems and algorithms play an important role across a multitude of fields, helping researchers and practitioners in the analysis of data and identification of patterns. These techniques find extensive application in diverse domains, including machine learning, where they help in categorizing and understanding complex datasets. In data mining, clustering methods are utilized to uncover hidden structures and relationships within large datasets, facilitating better decision-making and insight generation. Moreover, in image and signal processing, clustering algorithms assist in segmenting and classifying data, enabling tasks such as image recognition and signal denoising.

In bioinformatics, clustering techniques are essential for organizing biological data and identifying patterns in genetic sequences, protein structures, and gene expression profiles. Similarly, in anomaly detection, clustering methods are employed to identify unusual or unexpected patterns in data, which may indicate potential anomalies or security breaches. Furthermore, in social network analysis, clustering algorithms help in understanding the structure and dynamics of social networks by identifying communities and influential nodes.

The \(k\)-center problem is known as one of the fundamental clustering problems. Given a set of points \(P\) in a metric space and a number \(k\), the aim of the \(k\)-center problem is to find \(k\) centers such that the maximum distance between any point and its closest center is minimized. This can also be equivalently formulated as finding a minimum radius \(r\) and centers \(c_{1},\ldots,c_{k}\) such that the balls \(\cup_{i=1}^{k}\mathcal{B}_{P}(c_{i},r)\) cover the point set. The \(k\)-center problem can be \(2\)-approximated, and this is the best possible approximation guarantee [13]. In the last decade, the focus has shifted to analyzing the problem under various complications that arise in applications.

One line of research is to study the \(k\)-center problem (and other clustering problems) in different computational models like _streaming_ or for _dynamic_ point sets. In the first case, points arrive sequentially, and only a summary can be stored in memory. In the second case, the point set is maintained by insertion queries and deletion queries for single points, and algorithms have to update their solution after any such query.

Another line of research is to study clustering under constraints. For example, _capacitated_ clustering is very popular, i.e., limiting the number of points per cluster. However, lower bounds on cluster sizes have also been studied in the context of anonymity, and newer works have also considered constraints that model societal concerns like fair or diverse composition of clusters. In this paper, we study clustering _with outliers_. Formulated as a constraint, the \(k\)-center problem with outliers allows \(k+z\) centers, but \(z\) of these have to be singletons, meaning no point may be assigned to them. We can intuitively formulate it with balls as follows: The \(k\)-center problem with outliers asks to find a minimum \(r\) and \(k\) centers \(c_{1},\ldots,c_{k}\) such that the balls \(\cup_{i=1}^{k}\mathcal{B}_{P}(c_{i},r)\) cover all but \(z\) points.

Solving clustering problems in the presence of outliers is a crucial task due to the common occurrence of measurement errors or other sources of significant deviation from the rest of the data in real-world datasets. Ignoring outliers can severely distort the results of clustering algorithms, leading to inaccurate groupings. To address this challenge, a common approach is to solve a clustering problem while excluding up to \(z\) data points considered as outliers.

The first approximation algorithm for the \(k\)-center problem with outliers is due to Charikar et al. [8]. The challenge when designing approximation algorithms in the presence of outliers is that one needs to show that _enough_ points are covered by balls of bounded sizes around the approximate centers. It is not necessary to identify the outliers of an optimal solution _exactly_ as long as the number of uncovered points remains small enough. Due to this, [8] and follow-up papers use _charging_ arguments. Points covered by the solution of the algorithm are mapped to points in optimum clusters, and it is then shown that the number of uncharged points is small enough.

We explore the \(k\)-center clustering problem with \(z\) outliers within the fully dynamic model, where the point set experiences continuous updates through insertions and deletions over time. Quite some work on this problem has been done for inputs from metric spaces with _bounded doubling dimension_[2, 3, 19]. This setting allows for geometric data structures that allow easier navigation through the data set and also bounding of the number of changes that can occur due to a query by dimension-dependent volume arguments.

We study the general metric setting. General metric spaces represent a key objective for clustering algorithms due to their broad range of distance functions, ensuring applicability to any data type. Chan et al. recently showed that in general metric spaces, any dynamic \(\mathcal{O}(1)\)-approximation algorithm for \(k\)-center clustering excluding at most \(z\) outliers has an amortized update time of \(\Omega(z)\)[5]. For real-world applications, the fraction of outliers of the data could be arbitrarily large. Therefore, we allow for \((1+\epsilon)z\) outliers to be excluded to avoid the dependence on \(z\) of the update time.

The only previous work in this setting is the algorithm by Chan et al. [5], which returns a solution with at most \((1+\epsilon)z\) outliers with probability at least \(1-\delta\), for \(0<\delta\leq\frac{1}{k}\) and \(\epsilon>0\). It works by maintaining a clustering with \(t:=k\cdot\lceil\log_{1+\delta}\frac{k}{\delta}\rceil\) clusters of radius \(2r\) and using this to derive a final clustering with \(k\) clusters and radius \(14r\). They maintain such a clustering for all \(r\in\Gamma:=\{(1+\tau)^{i}:d_{\text{min}}\leq(1+\tau)^{i}\leq(1+\tau)\cdot d_{ \text{max}},i\in\mathbb{N}\}\), with \(d_{\text{min}}\) and \(d_{\text{max}}\) the minimum and maximum distances, respectively, between any two points ever inserted. It is then shown that there exists an instance \(r\in\Gamma\) that will approximate the optimal radius to within a factor \((14+\tau)\)while allowing for \((1+\epsilon)z\) outliers, with probability at least \(1-\delta\). The amortized time per update is \(\mathcal{O}(|\Gamma|\cdot\frac{k^{2}}{\epsilon^{2}}\log^{2}\frac{1}{\delta})\), with \(|\Gamma|=(\log\frac{d_{\mathrm{max}}}{d_{\mathrm{min}}})/\tau\). The total memory requirement is \(\mathcal{O}(|\Gamma|\cdot|n|)\) where \(n\) is the number of points in the current set.

### Our contribution

We introduce a novel fully dynamic \((4+\epsilon)\)-approximation algorithm designed to maintain a \(k\)-center clustering while allowing for at most \((1+\epsilon)z\) outliers at any time in the sequence.

The expected amortized update time is \(\mathcal{O}(\epsilon^{-3}k^{6}\log(k)\log(\Delta))\) per operation (insertion or deletion). This is independent of \(z\) and of the number of points currently present in the sequence. This characteristic makes the algorithm applicable to real-world scenarios.

Notice that our data structure is continually storing an actual solution, so we can produce this solution at any time and do not need to specify additional query times (like, for example [9; 2; 18]).

Our main technical contribution is a novel combination of the sampling-based level data structure by Chan et al. [5] and the original greedy strategy by Charikar et al. [8] that enables us to achieve a much improved approximation guarantee compared to [5].

**Theorem 1.1**.: _Let \((M,d)\) be a metric space and \(\epsilon>0\) be an accuracy parameter. The spread ratio \(\Delta=\frac{d_{\mathrm{max}}}{d_{\mathrm{min}}}\) of all points ever inserted is assumed to be bounded. There exists a randomized fully dynamic algorithm that maintains a \(k\)-center solution that allows up to \((1+\epsilon)z\) many outliers on the current set of points. At every point in time, the current clustering is a \((4+\epsilon)\)-approximation to an optimal solution for the \((k,z)\)-center problem with high probability. Upon insertion or deletion of a point, the data structure is updated in amortized update time \(\mathcal{O}(\epsilon^{-3}k^{6}\log(k)\log(\Delta))\)._

To achieve this, we make use of a data structure that is described in Section 2.1 and maintained by the respective algorithms handling updates to the point set. The algorithm handling insertions is specified in Section 2.2, and the deletion algorithm is described in Section 2.3. Similar to the approach by Chan et al. [5], we maintain a hierarchical structure consisting of levels, each containing one cluster. Unlike their work, however, we only maintain \(\leq k\) levels. For the correct radius guess, the union of these levels directly gives the desired \((4+\epsilon)\)-approximation. If a level violates certain properties, this and all higher levels are reclustered. As opposed to [5], we do not need to recluster every time a center gets deleted but only when a cluster does not contain enough points anymore. The algorithm handling the reclustering is described in Section 2.4.

Our analysis of the approximation ratio follows a similar argument as the proof of the 3-approximation for static \(k\)-center with \(z\) outliers as given by Charikar et al. [8] and works by charging points from chosen clusters to points in optimal clusters. We follow the same iterative charging method but extend their approach by maintaining a set of artificial outliers in order to adjust the argument to our algorithm specifically.

Note that we assume that a center can be any point from the underlying metric space, if the point was present in the data set once. Requiring that centers can only be placed at currently active points, we can achieve a 6-approximation. We formalize this in Lemma G.1.

### Further Related work

The \(k\)-center problem is very well understood; two \(2\)-approximation algorithms for it are known [13; 15] and a matching lower bound is also known [16]. The \(k\)-center problem with outliers was first studied and \(3\)-approximated in [8]. In 2016, [4] gave a \(2\)-approximation for this problem, but the algorithm is rather complex and less amenable to practical implementation. Charikar et al. [7] developed an elegant algorithm to maintain an \(8\)-approximation for the vanilla \(k\)-center problem in the streaming model. Streaming can be seen as a dynamic insertion-only model. McCutchen and Khuller [17] improved this algorithm to a \((2+\epsilon)\)-approximation and also extended it to a \((4+\epsilon)\)-approximation for \(k\)-center with outliers. The \(k\)-center problem with outliers has also been studied in the sliding window model [10; 18] that also bears some similarity with our setting. For bounded doubling dimension, [18] gave a \((3+\epsilon)\)-approximation, but the algorithm for the general metric case is only stated as a \(\mathcal{O}(1)\)-approximation.

The fully dynamic \(k\)-center problem without outliers in general metrics was studied by [1; 6; 11], and the best-known result is a \((2+\epsilon)\)-approximation with an amortized update time of \(\mathcal{O}(k\operatorname{polylog}(n,\Delta))\) by Bateni et al. [1]. The same paper also shows that any algorithm that provably satisfies a non-trivial approximation guarantee needs \(\Omega(nk)\) queries to the distance function, i.e., the amortized update time is in \(\Omega(k)\), making their algorithm close to tight. The fully dynamic \(k\)-center problem without outliers was also studied for metrics with bounded doubling dimension [14; 19; 12] and in Euclidean space [20].

### Preliminaries

In the preliminaries section, we provide an introduction to the \((k,z)\)-center problem and discuss the dynamic model used in our paper.

We study the \(k\)-center clustering problem with \(z\) outliers, formally defined as follows.

**Definition 1.2** (\((k,z)\)-center clustering).: Let \(P\) be a point set in a metric space \((M,d)\) and let \(k,z\in\mathbb{N}\) be two parameters. The goal is to compute a set \(C\subseteq M\) of size at most \(k\), such that the maximum distance of all but at most \(z\) points to their nearest \(c\in C\) is minimized. That is, find \(C\) such that \(\min_{Z\subseteq P,|Z|\leq z}\max_{x\in P\setminus Z}\min_{c\in C}d(x,c)\), with \(|C|\leq k\).

We define \(d_{\text{min}}\) and \(d_{\text{max}}\) as the minimum and maximum distances, respectively, between any two points ever inserted. The ratio \(\Delta=\frac{d_{\text{max}}}{d_{\text{min}}}\) is referred to as the spread ratio, and is assumed to be bounded. Let \(r_{\text{OPT}}\) be the optimal radius for the \((k,z)\)-center clustering problem of a point set \(P\). We also define the ball \(\mathcal{B}_{P}(c,r)=\{p\in P:d(c,p)\leq r\}\) to be the set of points in \(P\) that are within distance \(r\) from \(c\). If \(P\) is clear from the context, we may drop \(P\) from the definition \(\mathcal{B}_{P}(c,r)\). For a non-negative integer \(m\), we denote \(\{1,\cdots,m\}\) by \([m]\).

In the version of the \((k,z)\)-center clustering problem that we consider, we allow \((1+\epsilon)z\) outliers instead of strictly \(z\). Furthermore, we require centers of clusters to be in the metric space \((M,d)\), but they need not be currently present in \(P\). This is referred to as the non-discrete version of the problem. Lemma G.1 in the Appendix shows how our data structure can also support the discrete version of the problem and provides a \(6\)-approximation solution for it.

Fully dynamic model.We consider the \((k,z)\)-center clustering problem in the _fully dynamic model_ against an _adaptive adversary_. In this model, we start with an empty point set, \(P=\emptyset\), and process a sequence of operations determined by the adversary. We assume the adversary does not know the random bits chosen by our algorithm; however, it can observe the algorithm's output and adapt its responses in real time (unlike an _oblivious adversary_, which fixes a sequence of operations in advance). Each operation can be either an insertion, where a point from the metric space \((M,d)\) is added to \(P\), or a deletion, where a point currently in \(P\) is removed. We assume only points currently in \(P\) may be deleted. Let \(P^{t}\) represent the point set \(P\) after \(t\) operations. In other words, \(P^{t}\) consists of all points in \((M,d)\) that have been inserted but not deleted after \(t\) operations.

## 2 Algorithm

To explain the main ideas of our algorithm, we start by describing an iterative offline algorithm, which gives a \((4+\epsilon)\) approximation for the \((k,z)\)-center clustering problem.

Offline algorithm.We start with a point set \(P_{1}=P\), \(\epsilon>0\) and parameters \(k,z,r\in\mathbb{N}\). Here, \(r\) is a fixed guess for the optimal radius. For each iteration \(i\), we sample a set of \(S_{i}\subset P_{i}\) of \(|S_{i}|=\psi\epsilon^{-1}k^{2}\log k\) points with replacement, uniformly at random. Here, \(\psi\geq 6\beta\) is a constant, where \(\beta>\alpha\geq 1\) are constants. We find a point \(c_{i}\in S\), such that \(\mathcal{B}(c_{i},2r)\) covers at least \(\phi\) points from \(P_{i}\), where \(\phi\) is some threshold to be defined later. We then create a new cluster \(C_{i}=\mathcal{B}_{P_{i}}(c_{i},4r)\), let \(P_{i+1}=P_{i}\backslash C_{i}\), and continue to the next iteration. We stop once \(k\) clusters have been created or \(P_{i}=\emptyset\). Let \(\lambda\) be a random variable representing the number of iterations we complete, where \(\lambda\) can be at most \(k\). After we have done \(\lambda\) iterations we have computed clusters \(C_{1},C_{2},...,C_{\lambda}\) with corresponding centers \(c_{1},c_{2},...,c_{\lambda}\). The points that are not covered by the union of these clusters will be the set of outliers that our algorithm reports. If the set of outliers is at most \((1+\epsilon)z\) points, we can report a solution for the \((k,z)\)-center clustering problem. This offline algorithm will be used as a sub-routine for the fully dynamic algorithm, which will be explained in Section 2.4. The definition of \(\phi\) and the pseudocode of the algorithm will also be given in that section.

Guesses for unknown \(r_{\text{OPT}}\).Since the optimal \(r\) is usually not known, we can run the algorithm for all \(r\in\mathcal{R}=\{(1+\epsilon)^{i}:d_{\text{min}}\leq(1+\epsilon)^{i}\leq(1+ \epsilon)\cdot d_{\text{max}},i\in\mathbb{N}\}\). We then choose the smallest \(r\in\mathcal{R}\) such that all but at most \((1+\epsilon)z\) points are covered. We will show that this gives a \((4+\epsilon)\)-approximation.

Leveling.We can visualize the output of this algorithm as (at most) \(\lambda+1\) levels, where the first \(\lambda\leq k\) levels each represent a cluster, and the last level represents the outliers (See Figure 1). More precisely, level \(i\) represents cluster \(C_{i}\) with center \(c_{i}\), which was created in the \(i^{th}\) iteration. Note that the construction of level \(i\) only considers the points in \(P_{i}\), which does not include points in the lower levels \(\{1,2,...,i-1\}\).

The main idea for the fully dynamic algorithm is to maintain each level \(i\) for \(i\in[\lambda+1]\) under an arbitrary number of insertions and deletions by updating the cluster \(C_{i}\) when necessary. In the rest of the paper, we use subscript \(i\) to refer to the level and superscript \(t\) to refer to the time. For example, \(P_{i}^{t}\) refers to the points present in level \(i\) at time \(t\), and \(r_{\text{OPT}}^{t}\) refers to the optimal radius at time \(t\). If the level or time is clear from the context, these may be omitted. For instance, if we define a situation at time \(t^{\prime}\), we will not repeat the superscript \(t^{\prime}\) on every term.

### Data structure and invariants

We will construct a clustering \(C_{1},\ldots,C_{\lambda}\) of the current set of points \(P^{t}\) that we store in the data structure \(\mathcal{D}_{r}\) introduced by [6]. If \(t\) is clear from the context, we will just write \(P\) for \(P^{t}\). As we do not know the current optimal radius, we will maintain one data structure \(\mathcal{D}_{r}\) for every choice of \(r\in\mathcal{R}=\{(1+\epsilon)^{i}:d_{\text{min}}\leq(1+\epsilon)^{i}\leq d_{ \text{max}},i\in\mathbb{N}\}\). Every data structure consists of up to \(\lambda\leq k\) cluster-levels, with each level \(i\) containing a cluster \(C_{i}\). In level \(i\), we keep track of the set of the points \(P_{i}\) currently not covered, i. e., \(P_{i}=P\setminus\bigcup_{j<i}C_{j}\). Level \(\lambda+1\) contains the outliers, i.e., \(\mathcal{Z}_{r}=P\setminus\cup_{i\leq\lambda}C_{i}\). We denote \(n_{i}=|P_{i}|\). By \(\mathcal{B}(p,r)\) we denote the ball with radius \(r\) centered at \(p\), and \(\mathcal{B}_{A}(p,r)=\mathcal{B}(p,r)\cap A\) for \(A\subseteq P\).

Let \(\alpha\geq 1\). The data structure \(\mathcal{D}_{r}\) consists of the following components:

1. A list \(\mathcal{F}_{r}=\{c_{1},c_{2},...,c_{\lambda}\}\) of \(\lambda\leq k\) centers.
2. A list \(\mathcal{L}_{r}=\{C_{1},C_{2},...,C_{\lambda}\}\) of clusters with \(C_{i}=\mathcal{B}_{P_{i}}(c_{i},4r)\).
3. A set \(\mathcal{Z}_{r}=P\setminus\big{(}\cup_{i\in[\lambda]}C_{i}\big{)}\) of outliers such that for all \(i\in[\lambda]\) and \(x\in\mathcal{Z}_{r}\), \(d(x,c_{i})>4r\).

Let \(\alpha\geq 1\) be a constant. We have the following invariants for \(\mathcal{D}_{r}\).

1. **Level invariant:** for all \(i\), \(P_{i+1}=P_{i}\backslash C_{i}\), with \(C_{i}=\mathcal{B}_{P_{i}}(c_{i},4r)\).
2. **Dense invariant:** for all \(c_{i}\in\mathcal{F}_{r},|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\min\Big{(}z+1, \frac{n_{i}-z}{k-i+1}-\frac{cz}{\alpha k}\Big{)}\).

Figure 1: The \(\lambda\) levels constructed by our offline algorithm.

### Insertion

When a new point \(p\) is inserted at time \(t\), Procedure 3 in the appendix is executed. As input, we have \(\mathcal{D}_{r}\) at time \(t\), \(p\in(M,d)\), \(\epsilon>0\) and \(z,k\in\mathbb{N}\). First, we check whether \(p\) is inside one of the existing clusters, in which case we can add \(p\) to such a cluster. Otherwise, we add \(p\) to \(\mathcal{Z}_{r}\). Next, we check if the dense invariant is maintained after the insertion of the new point. Generally, the dense invariant can be broken in two ways:

* If the insertion of a new point results in more than \((1+\epsilon)z\) outliers, then Lemma 3.7 shows that the dense invariant is no longer valid at some level \(i\).
* If the addition of a point \(p\) to a cluster \(C_{i}\) causes the dense invariant to be violated at some lower level \(j<i\), that occurs because \(n_{j}^{t+1}=n_{j}^{t}+1\).

If the dense invariant is broken for some level \(i\), we recluster levels \(i,...,\lambda\) with \(\lambda\leq k\) by invoking Procedure 5 as a sub-routine. Observe that if there are multiple levels \(i\) where the dense invariant is broken, we choose the lowest one.

### Deletion

For the deletion of a point \(p\) at time \(t\), Procedure 4 in the appendix is executed. The input is \(\mathcal{D}_{r}\) at time \(t\), \(p\in(M,d)\), \(\epsilon>0\) and \(z,k\in\mathbb{N}\). First we check if \(p\) is an outlier, in which case we remove \(p\) from \(\mathcal{Z}_{r}\). If \(p\) is either a center or a point in a cluster, we find cluster \(C_{i}\) which contains \(p\). If cluster \(C_{i}=\mathcal{B}(c_{i},4r)\) contains at least \(\min\left(z+1,\frac{|P_{i}|-z}{k-i+1}-\frac{\epsilon z}{\alpha k}\right)\) points, we do not re-cluster and simply remove \(p\) from \(C_{i}\). Note that the underlying point set exists in the metric space \((M,d)\). If the center \(c_{i}\) of a cluster \(C_{i}\) is deleted, provided that the dense invariant remains valid, we can continue to utilize \(c_{i}\) as the center of cluster \(C_{i}\), given our knowledge that the point \(c_{i}\) is located within the metric space \((M,d)\). In Lemma G.1, we explain how we can still obtain a 6-approximation if centers have to come from the current point set. If \(|C_{i}|<\min\left(z+1,\frac{|P_{i}|-z}{k-i+1}-\frac{\epsilon z}{\alpha k}\right)\) after the deletion of \(p\), then it also follows that \(|\mathcal{B}(c_{i},2r)\cap P_{i}|<\min\left(z+1,\frac{|P_{i}|-z}{k-i+1}-\frac{ \epsilon z}{\alpha k}\right)\), i.e., the dense invariant is violated on this level. In this case, we want to redistribute the points in \(P_{i}\) such that the levels \(i,\ldots,\lambda\) fulfill the dense invariant. Deletion of a point in level \(i\) does not violate the invariants at levels \(1,\ldots,i-1\). We re-cluster the points in \((\cup_{i\leq j\leq k}C_{j})\cup\mathcal{Z}_{r}\) using Procedure 5. We finish by updating \(\mathcal{D}_{r}\).

### Clustering sub-routine

The clustering sub-routine is the offline algorithm that was described at the beginning of Section 2. The pseudocode of this sub-routine is shown in Procedure 5. We use it to iteratively build the levels of data structure \(\mathcal{D}_{r}\). Two cases are distinguished based on whether \(z\) is small or large compared to the number of points in level \(i\). In line 4, \(\psi\) is a constant with \(\psi\geq 6\beta\), where \(\beta>\alpha\), and \(\alpha\) is the constant used in the dense invariant. The threshold \(\phi\) that was introduced in Section 2 is different depending on the size of \(z\) compared to \(n_{i}\). If \(z+1\leq\frac{n_{i}-z}{4(k-i+1)}\), then \(\phi=\frac{n_{i}-z}{2(k-i+1)}\) and if \(z+1>\frac{n_{i}-z}{4(k-i+1)}\), then \(\phi=\frac{n_{i}-z}{k-i+1}-\frac{\epsilon z}{\beta k}\). If we find multiple points \(p^{*}\) in line 6 or 12 we choose one arbitrarily.

### All aspects combined

The first step will be to initialize all \(\mathcal{D}_{r}\) such that \(\mathcal{F}_{r},\mathcal{L}_{r},\mathcal{Z}_{r}=\emptyset\). Then, the algorithm waits for an insertion or deletion operation. If a point is inserted, Procedure 3 is executed, and if a point is deleted, Procedure 4 is executed. The algorithm is ran simultaneously for all \(r\in\mathcal{R}=\{(1+\epsilon)^{i}:d_{\min}\leq(1+\epsilon)^{i}\leq(1+\epsilon )\cdot d_{\max},i\in\mathbb{N}\}\).

In the next sections, it will be shown that a \((4+\epsilon)\)-approximation is given by the clustering \(\mathcal{L}_{r}\) with \(r\in\mathcal{R}\) the smallest \(r\) for which \(|\mathcal{Z}_{r}|\leq(1+\epsilon)z\).

## 3 Analysis

We begin our analysis of the dynamic algorithm by introducing the concept of a dense cluster and specifying the criteria for a valid solution.

**Definition 3.1** (Dense cluster).: A cluster \(\mathcal{B}_{P_{i}}(c_{i},4r)\) is dense with respect to point set \(P_{i}\) if it satisfies the dense invariant. That is, \(|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\min\left(z+1,\frac{n_{i}-z}{k-i+1}-\frac{ \epsilon z}{\alpha k}\right)\).

Observe that for the dense invariant, we consider the ball \(\mathcal{B}_{P_{i}}(c_{i},2r)\) with a radius of \(2r\). However, a cluster (as seen in line 18 of Procedure5) corresponds to the points within the ball \(\mathcal{B}_{P_{i}}(c_{i},4r)\), which has a radius of \(4r\).

**Definition 3.2** (Valid solution).: A solution \(\mathcal{B}_{P_{i}}(c_{i},4r)\cup\dots\cup\mathcal{B}_{P_{i}}(c_{\lambda},4r)\) with \(\lambda\leq k\) for point set \(P_{1}\) is valid if it covers all but at most \((1+\epsilon)z\) points from \(P_{1}\). We refer to each cluster \(\mathcal{B}_{P_{j}}(c_{j},4r)\) for \(j\in\{i,\dots,\lambda\}\) of a valid solution as a valid cluster.

**Lemma 3.3** (Running time offline algorithm).: _The running time of the offline algorithm, shown in Procedure 5, is \(\mathcal{O}(n\epsilon^{-1}k^{3}\log(k))\), where \(n=|P|\)._

Proof.: In each iteration of the while-loop, we need to compute \(|\mathcal{B}(p,2r)|\) for all \(p\in S_{i}\). Since \(|S_{i}|=\mathcal{O}(\epsilon^{-1}k^{2}\log k)\), this takes \(\mathcal{O}(n\epsilon^{-1}k^{2}\log k)\) time. There can be at most \(k\) iterations of the while-loop and hence the total running time is \(\mathcal{O}(n\epsilon^{-1}k^{3}\log k)\). 

### Maintaining invariants

In Supplementary Section B, we prove the following three lemmas. Lemmas 3.4 and 3.5 show that the dense and level invariants are maintained during the insertion or deletion of a point, respectively. Additionally, Lemma 3.6 shows that both invariants are maintained when invoking Procedure 5 as a subroutine upon the insertion or deletion of an arbitrary point with high probability if \(r\geq r_{\text{OPT}}\). The case where \(r<r_{\text{OPT}}\) is considered in Lemmas E.1 and E.2.

**Lemma 3.4** (Procedure 3 maintains invariants).: _Assume that at time \(t\), we have point set \(P^{t}\), data structure \(\mathcal{D}_{r}=(\mathcal{F}_{r},\mathcal{L}_{r},\mathcal{Z}_{r})\). We assume that the level and dense invariants hold at time \(t\) and \(r\geq r_{\text{OPT}}^{t+1}\). At the start of time \(t+1\), we insert point \(p\) using Procedure 3. After the insertion, the level and dense invariants still hold with probability 1 if Procedure 5 was not called and with the probability of at least \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\), where \(\Psi\geq 1\) if Procedure 5 was not called._

**Lemma 3.5** (Procedure 4 maintains invariants).: _Assume that at time \(t\), we have point set \(P^{t}\), instance \(\mathcal{D}_{r}=(\mathcal{F}_{r},\mathcal{L}_{r},\mathcal{Z}_{r})\), parameters \(k,z\in\mathbb{N}\) and \(\epsilon>0\). We assume that the level and dense invariants hold at time \(t\) and \(r\geq r_{\text{OPT}}^{t+1}\). At the start of time \(t+1\), we delete an arbitrary point \(p\) using Procedure 4. After the deletion, the level and dense invariants hold with probability 1 if Procedure 5 was not called, and with probability \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\) with \(\Psi\geq 1\) if Procedure 5 was called._

**Lemma 3.6** (Procedure 5 maintains invariants with high probability).: _Suppose the level and dense invariants hold for all levels \(j<i\) and we call Procedure 5 on \(P_{i}\) as the result of an insertion or deletion. Let \(\lambda\leq k\) be a random variable representing the number of levels we have after completing Procedure 5. If \(r\geq r_{\text{OPT}}\), Procedure 5 maintains the level and dense invariants for all levels \(j\) with \(i\leq j\leq\lambda\) with probability at least \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\), with \(\Psi\geq 1\)._

### Approximation guarantee

Next, we prove that if the invariants hold, our data structure \(\mathcal{D}_{r}=(\mathcal{F}_{r},\mathcal{L}_{r},\mathcal{Z}_{r})\) contains a valid solution to the \(k\)-center with \(z\) outliers problem. Furthermore, if \(r<(1+\epsilon)r_{\text{OPT}}\), \(\mathcal{D}_{r}\) provides a \((4+\epsilon)\)-approximation. Without loss of generality, for simplicity we assume that \(\mathcal{D}_{r}\) contains \(k\) levels. The proof of Lemma 3.7 follows the structure of the proof given in [8]. This proof uses the greediness of the algorithm to argue that the balls in the solution cover enough points to charge to. Here, we have to modify the proof to work with the dense invariant.

**Lemma 3.7** (Approximation guarantee).: _Let \(P\) be the current point set. Assume that the level invariant and the dense invariant hold for all levels \(i\leq k\). Then we have the following guarantees:_

1. _Valid solution: If_ \(r_{\text{OPT}}\leq r\)_, then_ \(\mathcal{B}(c_{1},4r)\cup\dots\cup\mathcal{B}(c_{k},4r)\) _covers all but at most_ \((1+\frac{\epsilon}{\alpha})z\) _outliers in_ \(P\)_._
2. _Approximate solution: If_ \(r_{\text{OPT}}\leq r<(1+\epsilon)r_{\text{OPT}}\)_, then_ \(\mathcal{B}(c_{1},4r)\cup\dots\cup\mathcal{B}(c_{k},4r)\) _gives a_ \((4+\epsilon)\)_-approximation for the_ \(k\)_-center with_ \(z\) _outliers problem on_ \(P\)Proof.: Recall that the level invariant states that for all \(i\), we have \(P_{i+1}=P_{i}\backslash\mathcal{B}(c_{i},4r)\) and the dense invariant states that for all \(i\), we have that \(|\mathcal{B}(c_{i},2r)\cap P_{i}|\geq\min(z+1,\frac{n_{i}-z}{k-i+1}-\frac{cz}{ \alpha k})\).

Assume that in the optimal solution, we have balls \(O_{1},\ldots,O_{k}\) with radius \(\leq r_{\text{OPT}}\). The union of these balls covers all but \(z\) points in \(P\). In order to prove Part 1, we aim to charge all but at most \(\frac{c}{\alpha}z\) points of the optimal solution to points in our solution \(\mathcal{B}(c_{1},4r)\cup\ldots\cup\mathcal{B}(c_{k},4r)\). We prove this by induction, and Part 2 will then follow easily. In order to construct the charging argument, we need to argue that there are enough points in our solution to charge the points in the optimal balls. To this end, we construct modified optimal balls \(O^{\prime}_{1},\ldots,O^{\prime}_{k}\), where \(O^{\prime}_{i}\subseteq O_{i}\) for every \(i\leq k\).

For the base case, we have \(O^{\prime}_{1}=O_{1},\cdots,O^{\prime}_{k}=O_{k}\). We will show that we can order the modified balls in such a way, that at the end of time step \(i\), all but at most \(\frac{cz}{\alpha k}\cdot i\) points from the first \(i\) modified balls are charged to distinct points in \(\mathcal{B}(c_{1},4r)\cup\ldots\cup\mathcal{B}(c_{i},4r)\). This will allow us to prove that our solution covers at least as many points as the optimal solution.

Assume that all but at most \(\frac{cz}{\alpha k}\cdot(i-1)\) points in the first \(i-1\) modified balls \(O^{\prime}_{1}\cup O^{\prime}_{2}\cup\ldots\cup O^{\prime}_{i-1}\) have been charged to distinct points in \(\mathcal{B}(c_{1},4r)\cup\ldots\cup\mathcal{B}(c_{i-1},4r)\) and consider iteration \(i\). We distinguish two cases, namely if \(\mathcal{B}(c_{1},2r)\cup\ldots\cup\mathcal{B}(c_{i},2r)\) intersects one of the remaining modified balls, or if it does not. The charging argument for each case proceeds as follows:

**Case 1.**  Case 1 is when \(\mathcal{B}(c_{1},2r)\cup\ldots\cup\mathcal{B}(c_{i},2r)\) intersects a remaining modified ball, call this ball \(O^{\prime}_{i}\). Note that \(O^{\prime}_{i}\) will be covered entirely by \(\mathcal{B}(c_{1},4r)\cup\ldots\cup\mathcal{B}(c_{i},4r)\), since \(r\geq r_{\text{OPT}}\). Hence, we charge the points of \(O^{\prime}_{i}\) to themselves and mark these points as covered. (See case 1 in Figure 2.) We call this charging _rule_\(I\). Since the modified balls are disjoint 1, any point can be charged only once (to itself) by this rule. Next, we update \(O^{\prime}_{1},O^{\prime}_{2},\ldots,O^{\prime}_{k}\).

Footnote 1: In fact, original balls \(O_{1},\cdots,O_{k}\) can overlap, but we can make them disjoint by assigning any point that is inside multiple balls to just one of them arbitrarily.

We maintain two sets of credit points that we save and may use for future charging purposes. First, the set \(\mathcal{Z}^{c}_{i}=\mathcal{B}(c_{i},4r)\backslash(O^{\prime}_{1}\cup\ldots \cup O^{\prime}_{k})\) which is the set of points covered by \(\mathcal{B}(c_{i},4r)\) that are not covered by \(O^{\prime}_{1}\cup O^{\prime}_{2}\cup\ldots\cup O^{\prime}_{k}\). We let \(z^{c}_{i}=|\mathcal{Z}^{c}_{i}|\) be the number of such points. In Figure 2, points \(p,q\) are such points. Observe that no point is charged to points in \(Z^{c}_{i}\), allowing us to use them later. We refer to these points as _credit points_.

There may also be previous modified balls \(O^{\prime}_{j}\), with \(j<i\) that were considered in case 2 and are still present in \(P_{i}\). More specifically, let \(Z^{d}_{i}\) be the set of (there may exist) points in \(O^{\prime}_{j}\cap\mathcal{B}(c_{j},4r)\) that have been charged to distinct points in \(\mathcal{B}(c_{j},4r)\) (See the discussion of case 2, below). For example, points \(a,b,c\) in case 2 of Figure 2. Let \(z^{d}_{i}=|Z^{d}_{i}|\) be the number of such points. Since no points

Figure 2: Visualization of _charging rules_ I and II. For case 1, we see that \(\mathcal{B}(c_{i},2r)\) and \(O_{i}\) intersect. As a result, \(\mathcal{B}(c_{i},4r)\) covers all points in \(O_{i}\). Points \(p,q\) are in set \(Z^{c}_{i}\). For case 2, the balls \(\mathcal{B}(c_{1},2r)\) and \(\mathcal{B}(c_{2},2r)\) do not intersect the optimal cluster \(O^{\prime}_{i}\). The crossed points in \(O^{\prime}_{2}\) are charged to black squared in \(\mathcal{B}(c_{2},2r)\). The circle points in \(O^{\prime}_{2}\) are not charged to any point. Points \(a,b,c\) are in \(Z^{d}_{i}\).

are charged to points in \(Z^{d}_{i}\), we save them as _credit points_ for future charging purposes. We now update \(O^{\prime}_{1},O^{\prime}_{2},\ldots,O^{\prime}_{k}\) as follows: \(O^{\prime}_{1},\ldots,O^{\prime}_{i}\) stay the same and we define \(z^{c}_{i}+z^{d}_{i}\) artificial outliers in \((O^{\prime}_{i+1}\cup\ldots\cup O^{\prime}_{k})\cap P_{i+1}\).

Case 2.Case 2 occurs if \(\mathcal{B}(c_{1},2r)\cup\ldots\cup\mathcal{B}(c_{i},2r)\) does not intersect any of the remaining modified balls. See Figure 2. Let \(O^{\prime}_{i}\) be one of the remaining modified balls covering \(\leq\frac{n_{i}-z}{k-i+1}\) points. Note that for finding \(O^{\prime}_{i}\) we do not count points that have been defined as artificial outliers, since these artificial outliers are already covered by balls of radius \(4r\) in previous levels and we do not cover them by the remaining modified balls. We prove in Lemma 3.8 that such a ball \(O^{\prime}_{i}\) exists.

Using the assumption of this lemma that the dense invariant holds, we show in Lemma 3.9 that \(|\mathcal{B}(c_{i},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{\epsilon z}{\alpha k}\). In this way, we find an upper bound for the number of points in \(O^{\prime}_{i}\) and a lower bound for the number of points in \(\mathcal{B}(c_{i},2r)\). Now, we charge all but at most \(\frac{\epsilon z}{\alpha k}\) points of \(O^{\prime}_{i}\) to the points of \(\mathcal{B}(c_{i},2r)\) and mark these points as charged. We call this charging _rule II_. Note the difference between charging argument in Case 1 and 2. In Case 1, we charge the points in \(O^{\prime}_{i}\) to themselves, but in Case 2, we charge them to points covered by \(\mathcal{B}(c_{i},2r)\). Recall that \(\mathcal{B}(c_{i},2r)\) and \(O^{\prime}_{i}\) are disjoint.

Observe that points in balls \(O^{\prime}_{i+1},\cdots,O^{\prime}_{k}\) will not be charged to points in \(\mathcal{B}(c_{i},2r)\). Indeed, points in balls \(O^{\prime}_{i+1},\cdots,O^{\prime}_{k}\) are charged to the balls that our algorithms find either using rule I or rule II. However, this will not happen based on rule I since \(\mathcal{B}(c_{i},2r)\) is disjoint from all balls \(O^{\prime}_{i+1},\cdots,O^{\prime}_{k}\). Moreover, rule II cannot also be applied since among balls \(O^{\prime}_{i},O^{\prime}_{i+1},\cdots,O^{\prime}_{k}\), we already charged the points in \(O^{\prime}_{i}\) to \(\mathcal{B}(c_{i},2r)\) and points in \(O^{\prime}_{i+1},\cdots,O^{\prime}_{k}\) will be charged to points in balls \(\mathcal{B}(c_{i+1},2r),\cdots,\mathcal{B}(c_{\lambda},2r)\).

Next, we consider how we update \(O^{\prime}_{1},O^{\prime}_{2},\ldots,O^{\prime}_{k}\). Similar to case 1, we define the size of two sets of credits points. First, the variable \(z^{c}_{i}=|\mathcal{B}(c_{i},4r)\backslash(O^{\prime}_{1}\cup\ldots\cup O^{ \prime}_{k})|\) that corresponds to the number of points covered by \(\mathcal{B}(c_{i},4r)\) that are not covered by \(O^{\prime}_{1}\cup O^{\prime}_{2}\cup\ldots\cup O^{\prime}_{k}\). There are \(z^{c}_{i}-|O^{\prime}_{i}|\) points from \(\mathcal{B}(c_{i},4r)\) that are free (i.e., no point is charged to) and can still be charged. We consider these points as credits that we save and may use for future charging purposes.

There may also be previous modified balls \(O^{\prime}_{j}\), with \(j<i\) that were considered in case 2 and are still present in \(P_{i}\). More specifically, let \(Z^{d}_{i}\) be the set of (there may exist) points in \(O^{\prime}_{j}\cap\mathcal{B}(c_{j},4r)\) that have been charged to distinct points in \(\mathcal{B}(c_{j},4r)\). For example, points \(a,b,c\) in case 2 of Figure 2. Let \(z^{d}_{i}=|Z^{d}_{i}|\) be the number of such points. Since no points are charged to points in \(Z^{d}_{i}\), we save them as _credit points_ for future charging purposes. We now update \(O^{\prime}_{1},O^{\prime}_{2},\ldots,O^{\prime}_{k}\) as follows: \(O^{\prime}_{1},O^{\prime}_{2},\ldots,O^{\prime}_{i}\) stays the same and we define \((z^{c}_{i}-|O^{\prime}_{i}|)+z^{d}_{i}\) artificial outliers in \((O^{\prime}_{i+1}\cup\ldots\cup O^{\prime}_{k})\cap P_{i+1}\).

Now, for both cases, we apply charging rule I to any points in the remaining modified balls \(O^{\prime}_{i+1}\cup\ldots\cup O^{\prime}_{k}\) that are inside \(\mathcal{B}(c_{i},4r)\). These points are then marked as covered. See Figure 3 for an example. Note that these points will not be in \(P_{i+1}=P_{i}\backslash\mathcal{B}(c_{i},4r)\). We assumed that \(O_{1}\cup O_{2}\cup\ldots\cup O_{k}\) covers all but at most \(z\) points. After the charging has taken place and the modified clustering \(O^{\prime}_{1}\cup O^{\prime}_{2}\cup\ldots\cup O^{\prime}_{k}\) has been constructed, we have that \(|O^{\prime}_{1}\cup O^{\prime}_{2}\cup\ldots\cup O^{\prime}_{k}|=n-z-\sum_{j \text{ in case }1}(z^{c}_{j}+z^{d}_{j})-\sum_{j\text{ in case }2}(z^{c}_{j}-|O^{\prime}_{j}|+z^{d}_{j})\).

Note that \(|O^{\prime}_{j}|\) refers to the number of points covered by the modified ball \(O^{\prime}_{j}\) at the time of iteration \(j\). Then, by the way we have charged \(O^{\prime}_{1}\cup\ldots\cup O^{\prime}_{k}\) to our solution \(\mathcal{B}(c_{1},4r)\cup\ldots\cup\mathcal{B}(c_{k},4r)\), we obtain \(|\mathcal{B}(c_{1},4r)\cup\ldots\cup\mathcal{B}(c_{k},4r)|\geq|O^{\prime}_{1} \cup O^{\prime}_{2}\cup\ldots\cup O^{\prime}_{k}|-\frac{\epsilon z}{\alpha}+ \sum_{j\text{ in case }1}(z^{c}_{j}+z^{d}_{j})\)

\(+\sum_{j\text{ in case }2}(z^{c}_{j}-|O^{\prime}_{j}|+z^{d}_{j})=n-(1+\frac{ \epsilon}{\alpha})z\). This concludes the proof of Part 2 of the lemma. It follows easily that when \(r_{\text{OPT}}\leq r<(1+\epsilon)r_{\text{OPT}}\), Part 2 also holds, which completes the proof of this lemma. 

**Lemma 3.8**.: _Let \(i\) be an iteration of the charging argument above such that we are in case 2. This means that \(\mathcal{B}(c_{1},2r)\cup\ldots\cup\mathcal{B}(c_{i},2r)\) does not intersect any of the remaining modified balls. Then, there must be a remaining modified ball covering \(\leq\frac{n_{i}-z}{k-i+1}\) points._

For the proof of Lemma 3.8, see Supplementary Section C.

**Lemma 3.9** (Coverage of \(B(c_{i},2r)\)).: _When we are in case 2 of the charging argument for some iteration \(i\), we must have that \(|\mathcal{B}(c_{i},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{cz}{\alpha k}\)._

For the proof of Lemma 3.9, see Supplementary Section C.

### Duration of dense clusters

After running Procedure 5 as a subroutine, we know that each cluster \(C_{i}\) covers at least \(\phi\) points. More specifically, for each level \(i\), if \(z+1\leq\frac{n_{i}-z}{4(k-i+1)}\), then \(|\mathcal{B}(c_{i},2r)|\geq\frac{n_{i}-z}{2(k-i+1)}\) and if \(z+1>\frac{n_{i}-z}{4(k-i+1)}\), then \(|\mathcal{B}(c_{i},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{cz}{\beta k}\). The following two lemmas show that when \(C_{i}\) satisfies one of these two constraints, it will be dense for a significant number of arbitrary operations. This will lead to the amortized update time being independent of \(n\). The remaining lemmas are split into two cases, corresponding to the two cases in Procedure 5.

**Lemma 3.10** (Duration of dense cluster for \(z\) is small).: _Assume that we are currently at time \(t\). Let us consider a level \(i\) in which \(z+1\leq\frac{n_{i}^{t}-z}{4(k-i+1)}\). Let \(p=arg\,max_{p^{\prime}\in S_{i}}|\mathcal{B}_{P_{i}}(p^{\prime},2r)|\), and \(\mathcal{B}_{\max}=\mathcal{B}_{P_{i}}(p,2r)\). Assume that \(\frac{n_{i}^{t}-z}{2(k-i+1)}\leq|\mathcal{B}_{\text{max}}|\). Then, we can add \(\mathcal{B}_{P_{i}}(p,4r)\) as a cluster in our solution, and this cluster will be dense until time \(t^{\prime}=t+t^{*}\), with \(t^{*}\geq\frac{n_{i}^{t}-z}{4(k-i+1)}\)._

For the proof of Lemma 3.10, see Supplementary Section D.

**Lemma 3.11** (Duration of dense cluster for \(z\) is large).: _Assume that we are currently at time \(t\). Let \(z+1>\frac{n_{i}^{t}-z}{4(k-i+1)}\) for some level \(i\). Let \(p=arg\,max_{p^{\prime}\in S_{i}}|\mathcal{B}_{P_{i}}(p^{\prime},2r)|\), and \(\mathcal{B}_{\max}=\mathcal{B}_{P_{i}}(p,2r)\). Assume that \(\frac{n_{i}^{t}-z}{k-i+1}-\frac{cz}{\beta k}\leq|\mathcal{B}_{\text{max}}|\). Then, we can add \(\mathcal{B}_{P_{i}}(p,4r)\) as a cluster in our solution, and this cluster will be dense until time \(t^{\prime}=t+t^{*}\), with \(t^{*}=\Omega(\frac{cz}{k})\)._

For the proof of Lemma 3.11, see Supplementary Section D.

### Small radius guesses

It has not yet been considered what will happen if Procedure 5 fails on some level \(i\). That is, when there exists no \(p\in S_{i}\) such that \(\mathcal{B}_{P_{i}}(p,2r)\) covers sufficiently many points. Lemmas E.1 and E.2 show that if Procedure 5 fails at some level, then with high probability the guess of the optimal radius for that specific instance is too small (\(r<r_{\text{OPT}}\)). In this case, we postpone the construction of that level to a time \(t+t^{*}\), referred to as \(t^{\prime}_{r}\) in the algorithm.

### Computing update time

Finally, we prove the update time of our algorithm. Lemma 3.4, Lemma 3.5 and Lemma 3.6 prove that the algorithms maintain the invariants with high probability. Together with Lemma 3.7 and Lemma F.1 this implies Theorem 1.1.

### Robustness to adversarial inputs

Our dynamic algorithm is robust against an adaptive adversary. Specifically, we do not assume that the adversary has predetermined the sequence of updates in advance, as with an oblivious adversary. Instead, the adversary can query the insertion and deletion updates in an online manner, with knowledge of our solution.

We only use randomization to generate the sample set \(S_{i}\) in Procedure 5, which is then used to construct the levels. This randomness affects only the probability of failure in this procedure. After the insertion or deletion updates, we do not need to reconstruct a level until it no longer satisfies the invariants. The guarantees we provide for how long a level can remain valid are all in the worst case; therefore, they hold even when an adaptive adversary chooses the insertion and deletion operations online.

### Acknowledgements

The authors would like to thank the anonymous reviewers for their insightful comments. Annika Hennes' and Melanie Schmidt's research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project number 456558332.

## References

* [1] MohammadHossein Bateni, Hossein Esfandiari, Hendrik Fichtenberger, Monika Henzinger, Rajesh Jayaram, Vahab Mirrokni, and Andreas Wiese. Optimal fully dynamic \(k\)-center clustering for adaptive and oblivious adversaries. In _Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2677-2727. Society for Industrial and Applied Mathematics, 2023.
* [2] Leyla Biabani, Annika Hennes, Morteza Monemizadeh, and Melanie Schmidt. Faster query times for fully dynamic \(k\)-center clustering with outliers. _Advances in Neural Information Processing Systems_, 36, 2024.
* [3] Matteo Ceccarello, Andrea Pietracaprina, and Geppino Pucci. Solving k-center clustering (with outliers) in mapreduce and streaming, almost as accurately as sequentially. _Proc. VLDB Endow._, 12(7):766-778, 2019.
* [4] Deeparnab Chakrabarty, Prachi Goyal, and Ravishankar Krishnaswamy. The non-uniform \(k\)-center problem. In _Proceedings of the 43rd International Colloquium on Automata, Languages, and Programming (ICALP)_, volume 55 of _LIPIcs. Leibniz Int. Proc. Inform._, pages Art. No. 67, 15. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2016.
* [5] T-H Hubert Chan, Silvio Lattanzi, Mauro Sozio, and Bo Wang. Fully dynamic k-center clustering with outliers. _Algorithmica_, 86(1):171-193, 2024.
* [6] TH Hubert Chan, Arnaud Guergin, and Mauro Sozio. Fully dynamic k-center clustering. In _Proceedings of the 2018 World Wide Web Conference_, pages 579-587, Lyon, France, 2018. International World Wide Web Conferences Steering Committee.
* [7] Moses Charikar, Chandra Chekuri, Tomas Feder, and Rajeev Motwani. Incremental clustering and dynamic information retrieval. In _Proceedings of the twenty-ninth annual ACM symposium on Theory of computing_, pages 626-635, 1997.
* [8] Moses Charikar, Samir Khuller, David M. Mount, and Giri Narasimhan. Algorithms for facility location problems with outliers. In S. Rao Kosaraju, editor, _Proceedings of the Twelfth Annual Symposium on Discrete Algorithms, January 7-9, 2001, Washington, DC, USA_, pages 642-651. ACM/SIAM, 2001.
* [9] Mark de Berg, Leyla Biabani, and Morteza Monemizadeh. \(k\)-center clustering with outliers in the MPC and streaming model. _CoRR_, abs/2302.12811, 2023.
* Leibniz-Zentrum fur Informatik, 2021.
* [11] Hendrik Fichtenberger, Monika Henzinger, and Andreas Wiese. On fully dynamic constant-factor approximation algorithms for clustering problems. _CoRR_, abs/2112.07217, 2021.
* [12] Jinxiang Gan and Mordecai J Golin. Fully dynamic k-center in low dimensions via approximate furthest neighbors. In _2024 Symposium on Simplicity in Algorithms (SOSA)_, pages 269-278. SIAM, 2024.
* [13] Teofilo F Gonzalez. Clustering to minimize the maximum intercluster distance. _Theoretical computer science_, 38:293-306, 1985.

* [14] Gramoz Goranci, Monika Henzinger, Dariusz Leniowski, Christian Schulz, and Alexander Svozil. Fully dynamic \(k\)-center clustering in low dimensional metrics. In _2021 Proceedings of the Workshop on Algorithm Engineering and Experiments (ALENEX)_, pages 143-153. SIAM, 2021.
* [15] Dorit S Hochbaum and David B Shmoys. A unified approach to approximation algorithms for bottleneck problems. _Journal of the ACM (JACM)_, 33(3):533-550, 1986.
* [16] Wen-Lian Hsu and George L Nemhauser. Easy and hard bottleneck location problems. _Discrete Applied Mathematics_, 1(3):209-215, 1979.
* [17] Richard Matthew McCutchen and Samir Khuller. Streaming algorithms for \(k\)-center clustering with outliers and with anonymity. In _Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques_, pages 165-178. Springer, 2008.
* [18] Paolo Pellizzoni, Andrea Pietracaprina, and Geppino Pucci. k-center clustering with outliers in sliding windows. _Algorithms_, 15(2):52, 2022.
* [19] Paolo Pellizzoni, Andrea Pietracaprina, and Geppino Pucci. Fully dynamic clustering and diversity maximization in doubling metrics. In _Algorithms and Data Structures Symposium_, pages 620-636. Springer, 2023.
* [20] Melanie Schmidt and Christian Sohler. Fully dynamic hierarchical diameter \(k\)-clustering and \(k\)-center. _arXiv preprint arXiv:1908.02645_, 2019.

[MISSING_PAGE_FAIL:13]

```
1:\(\lambda\leftarrow|\mathcal{F}_{r}|\)
2:if\(p\in\mathcal{Z}_{r}\)then
3: remove \(p\) from \(\mathcal{Z}_{r}\)
4:else
5: Let \(C_{i}\) be the cluster containing \(p\), where \(i\in[\lambda]\)
6:\(P_{i}\leftarrow(\cup_{i\leq j\leq\lambda}C_{j})\cup\mathcal{Z}_{r}\)
7:if\(\mathcal{B}(c_{i},2r)\) covers \(>\min\left(z+1,\frac{|P_{i}|-z}{k-i+1}-\frac{cz}{\alpha k}\right)\) from \(P_{i}\)then
8: remove \(p\) from \(C_{i}\)
9:else
10: OfflineCluster\((P_{i},i,r)\)
11:endif
12:endif ```

**Procedure 4**\(\textsc{Delete}(p,r)\)

```
1:\(t^{\prime}_{r}\leftarrow-1\)
2:while\(i\leq k\) and \(P_{i}\neq\emptyset\)do
3:\(n_{i}\leftarrow|P_{i}|\)
4: Pick a uniform sample \(S_{i}\) of \(\psi\epsilon^{-1}k^{2}\log k\) points from \(P_{i}\)
5:if\(z+1\leq\frac{n_{i}-z}{4(k-i+1)}\)then
6: Find a point \(p^{*}\in S_{i}\) such that \(|\mathcal{B}_{P_{i}}(p^{*},2r)|\geq\frac{n_{i}-z}{2(k-i+1)}\)
7:if there does not exist such a \(p^{*}\)then
8:\(t^{\prime}_{r}\gets t+\frac{n_{i}-z}{2(k-i+2)}\) // See Lemma E.1
9: break
10:endif
11:elseif\(z+1>\frac{n_{i}-z}{4(k-i+1)}\)then
12: Find a point \(p^{*}\in S_{i}\) such that \(|\mathcal{B}_{P_{i}}(p^{*},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{cz}{\beta k}\)
13:if there does not exist such a \(p^{*}\)then
14:\(t^{\prime}_{r}\gets t+\frac{cz}{2\beta k}\) // See Lemma E.2
15: break
16:endif
17:endif
18:\(c_{i}\gets p^{*},C_{i}\leftarrow\mathcal{B}(p^{*},4r)\), and \(P_{i+1}\gets P_{i}\backslash C_{i}\)
19:\(i\gets i+1\)
20:endwhile
21:\(\mathcal{F}_{r}\leftarrow\{c_{1},c_{2},...,c_{i-1}\},\;\mathcal{L}_{r}\leftarrow \{C_{1},C_{2},...,C_{i-1}\}\;,\mathcal{Z}_{r}\gets P_{i}\) ```

**Procedure 5**\(\textsc{OfflineCluster}(P_{i},i,r)\)

## Appendix B Missing proofs Section 3.1

**Lemma 3.4** (Procedure 3 maintains invariants).: _Assume that at time \(t\), we have point set \(P^{t}\), data structure \(\mathcal{D}_{r}=(\mathcal{F}_{r},\mathcal{L}_{r},\mathcal{Z}_{r})\). We assume that the level and dense invariants hold at time \(t\) and \(r\geq r_{\mathrm{OPT}}^{t+1}\). At the start of time \(t+1\), we insert point \(p\) using Procedure 3. After the insertion, the level and dense invariants still hold with probability 1 if Procedure 5 was not called and with the probability of at least \(1-\frac{2(k-i+1)}{e^{\frac{\pi}{\theta}\log k}}\), where \(\Psi\geq 1\) if Procedure 5 was not called._

Proof.: If we enter the case in line 2, the new point \(p\) will be added to an existing cluster \(C_{i}\). Then, we have that \(p\in P_{j}\) for all \(j\leq i\) and \(p\notin P_{j}\) for all \(j>i\). If we enter the case in line 4, the new point \(p\) is added as an outlier. This is the highest level, so we have that \(p\in P_{i}\) for all \(i\). The level invariant is maintained by definition in these cases. If we enter the case in line 9 and recluster levels \(i,\ldots,k\), we recluster the points in \(P_{i}\) as defined in line 7, and hence, none of the levels \(j<i\) are affected. Then by Lemma 3.6, the level invariant is maintained for all levels. The dense invariant is maintained because of the check we do in line 9 for all levels. Furthermore, we choose the lowest level \(i\) where the dense invariant does not hold and recluster from this level upwards. Hence, the dense invariant will hold for all levels \(j<i\). We call Procedure 5 on \(P_{i}\), so by Lemma 3.6, the dense invariant is maintained for levels \(j\geq i\) with probability \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\), with \(\Psi\geq 1\). 

**Lemma 3.5** (Procedure 4 maintains invariants).: _Assume that at time \(t\), we have point set \(P^{t}\), instance \(\mathcal{D}_{r}=(\mathcal{F}_{r},\mathcal{L}_{r},\mathcal{Z}_{r})\), parameters \(k,z\in\mathbb{N}\) and \(\epsilon>0\). We assume that the level and dense invariants hold at time \(t\) and \(r\geq r_{\mathrm{OPT}}^{t+1}\). At the start of time \(t+1\), we delete an arbitrary point \(p\) using Procedure 4. After the deletion, the level and dense invariants hold with probability \(1\) if Procedure 5 was not called, and with probability \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\) with \(\Psi\geq 1\) if Procedure 5 was called._

Proof.: If we enter the case in line 2, where \(p\) is an outlier, it follows easily that the level and dense invariants are maintained. In the second case, starting in line 4, \(p\) is in some cluster \(C_{i}\), either as a center or as another point. If \(C_{i}\) still covers sufficiently many points after the deletion of \(p\) as described in line 7, it follows easily that the level invariant is maintained. For the dense invariant, note that for all levels \(j<i\), \(n_{j}\) and with this \(\frac{n_{j}-z}{k-j+1}-\frac{\epsilon z}{\alpha k}\) can only decrease. Furthermore, for all levels \(j>i\), \(n_{j}\) remains unchanged. This observation, combined with the fact that for any \(i\neq j\), \(|C_{j}|\) is unchanged, means that the dense invariant still holds for all levels.

If \(C_{i}\) is no longer dense after deleting \(p\), and we enter the case in line 9, the clusters in levels \(j<i\) remain unchanged. The level invariant will be maintained since \(P_{j}\) is updated to \(P_{j}\backslash\{p\}\) for all \(j<i\). The dense invariant is maintained in levels \(j<i\) by the same reasoning as the previous case. On the remaining points, Procedure 5 is called in line 10. Using Lemma 3.6, the level and dense invariants are maintained for the remaining levels with probability at least \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\), where \(\Psi\geq 1\). Crucial for maintaining the level invariant is line 6. This line ensures that we do not consider the points in levels \(j<i\) when constructing level \(i\) and any higher levels. 

**Lemma 3.6** (Procedure 5 maintains invariants with high probability).: _Suppose the level and dense invariants hold for all levels \(j<i\) and we call Procedure 5 on \(P_{i}\) as the result of an insertion or deletion. Let \(\lambda\leq k\) be a random variable representing the number of levels we have after completing Procedure 5. If \(r\geq r_{\mathrm{OPT}}\), Procedure 5 maintains the level and dense invariants for all levels \(j\) with \(i\leq j\leq\lambda\) with probability at least \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\), with \(\Psi\geq 1\)._

Proof.: By the while-loop structure in combination with line 18, the level invariant is maintained for all levels \(j\geq i\). For each newly constructed level \(j\geq i\), there are two cases within the while-loop.

For the first case where \(z+1\leq\frac{n_{j}-z}{4(k-j+1)}\), we find a ball \(\mathcal{B}_{P_{j}}(p^{*},2r)\) that covers \(\geq\frac{n_{j}-z}{2(k-j+1)}\) points with probability at least \(1-\frac{2}{e^{\Psi\log k}}\) if \(r\geq r_{\mathrm{OPT}}\). For the proof of this, we refer to Lemma E.1.

Then, the dense invariant holds for any such level since

\[\frac{n_{j}-z}{2(k-j+1)}\geq\frac{n_{j}-z}{4(k-j+1)}\geq z+1\geq\min\left(z+1,\frac{n_{j}-z}{k-j+1}-\frac{\epsilon z}{\alpha k}\right).\] (1)

In the second case where \(z+1>\frac{n_{j}-z}{4(k-j+1)}\), we find a ball \(\mathcal{B}_{P_{j}}(p^{*},2r)\) that covers \(\geq\frac{n_{j}-z}{k-j+1}-\frac{\epsilon z}{\beta k}\) points with probability \(1-\frac{2}{e^{\Psi\log k}}\) if \(r\geq r_{\mathrm{OPT}}\). For the proof of this, we refer to Lemma E.2. Since \(\beta>\alpha\) (see Section 2.4), the dense invariant also holds for every level in this case. Then, the probability that the dense invariant holds for all levels \(j\) with \(i\leq j\leq\lambda\) is at least \(1-\frac{2(k-i+1)}{e^{\Psi\log k}}\). This is because \(\lambda-i+1\leq k-i+1\) is the number of new levels we constructed during Procedure 5. 

## Appendix C Missing proofs Section 3.2

**Lemma 3.8**.: _Let \(i\) be an iteration of the charging argument above such that we are in case 2. This means that \(\mathcal{B}(c_{1},2r)\cup\ldots\cup\mathcal{B}(c_{i},2r)\) does not intersect any of the remaining modified balls. Then, there must be a remaining modified ball covering \(\leq\frac{n_{i}-z}{k-i+1}\) points._

Proof.: We prove this using strong induction on iterations in case 2. First, let us consider the base case. In the base case, we are in iteration \(i\) which is the first iteration to be in case 2 of the charging argument. There have been \(i-1\) iterations before \(i\) which were charged according to case 1. For any \(j<i\), define \(x_{j}\) to be the number of artificial outliers covered by \(\mathcal{B}(c_{j},4r)\). Then, for the remaining modified balls we have that:

\[|O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}|=n_{i}-\left(z-\sum_{j=1}^{i-1}(z_{ j}^{c}-x_{j})\right)-\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})=n_{i}-z\] (2)

This is because we assume that there are exactly \(z\) points outside our optimal solution \(O_{1}\cup\ldots\cup O_{k}\). These \(z\) points are also outside \(O^{\prime}_{1}\cup\ldots\cup O^{\prime}_{k}\) since for \(1\leq i\leq k\), \(O^{\prime}_{i}\subseteq O_{i}\). Of those \(z\) points, \((z-\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j}))\) have not yet been seen in an iteration. Hence, these points are definitely not covered by \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\). Furthermore, we know that \(\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\) artificial outliers have been defined in \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\) in total. This is because all iterations \(j<i\) were in case 1, and any artificial outliers present in \(O^{\prime}_{j}\) will be propagated to \((O^{\prime}_{j+1}\cup\ldots\cup O^{\prime}_{k})\cap P_{j+1}\), as these are included in \(z_{j}^{c}\). We subtract \(x_{j}\) because these artificial outliers have already been counted by another \(z_{j}^{c}\), and should not be counted again when computing the total amount of artificial outliers. Note that \(z_{j}^{d}=0\) for all iterations \(j<i\) as there were no previous iterations before \(j\) in case 2. Since at most \(n_{i}-z\) points are covered by \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\), there must be at least one of the modified optimal ball which covers \(\leq\frac{n_{i}-z}{k-i+1}\). This concludes the proof of the base case.

Now, consider the inductive step. We are in an iteration \(i\) that is in case 2. There have been \(i-1\) previous iterations, of which an arbitrary number has been charged by case 2. Assume that for any such iteration \(j<i\) that was in case 2, we found an optimal ball \(O^{\prime}_{j}\) that was covering at most \(\frac{n_{j}-z}{k-j+1}\) points. At the time of iteration \(j\), the points of ball \(O^{\prime}_{j}\) were charged to distinct points in \(\mathcal{B}(c_{j},2r)\), but not necessarily covered. In the iterations between \(j\) and \(i\), however, more points of the ball \(O^{\prime}_{j}\) may have been covered. Define \(O^{\prime}_{j,\text{unc}}\) as the points of \(O^{\prime}_{j}\) that are still uncovered at the time of iteration \(i\). Now, for the remaining modified balls, we should have that:

\[|O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}| =n_{i}-\left(z-\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\right)-\sum_{j<i \text{ in case 1}}(z_{j}^{c}+z_{j}^{d}-x_{j})\] \[-\sum_{j<i\text{ in case 2}}(z_{j}^{c}+z_{j}^{d}-|O^{\prime}_{j}| -x_{j})-\sum_{j<i\text{ in case 2}}|O^{\prime}_{j,\text{unc}}|\] (3)

Similar to the base case, \((z-\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j}))\) points will definitely be outside \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\). In the iterations before \(j<i\) in case 1, \((z_{j}^{c}+z_{j}^{d})\) points are stored as a credit. We subtract \(x_{j}\) from this since we don't want to recount outliers from previous iterations. In each iteration \(j<i\) that was in case 2, \((z_{j}^{c}+z_{j}^{d}-|O^{\prime}_{j}|)\) are stored as a credit. Even though \(B(c_{j},2r)\) is disjoint from the remaining modified balls in this case, \(B(c_{j},2r)\) can still cover some artificial outliers present in the remaining modified balls. We do not want to recount these for the total amount of outliers in \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\), and hence we subtract \(x_{j}\). Lastly, since the modified balls are disjoint, any points of \(O^{\prime}_{j}\) with \(j<i\) and iteration \(j\) in case 2 that are still in the point set \(n_{i}\) are not covered by \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\). We can rewrite

Figure 3: An example to illustrate the extra charging we do at the end of both cases 1 and 2. We are in iteration \(i\), and the ball \(\mathcal{B}(c_{i},4r)\) covers two points from ball \(O^{\prime}_{j}\), with \(j>i\). The two points, shown in red, will be charged to themselves as in charging _rule I_ and marked as covered.

this as follows:

\[|O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}|=n_{i}-z-\left(\sum_{j<i}z_{j}^{d}+ \sum_{j<i\:\text{in case 2}}|O^{\prime}_{j,\text{unc}}|\right)+\sum_{j<i\:\text{ in case 2}}|O^{\prime}_{j}|=n_{i}-z\] (4)

This is because by the definition of \(z_{j}^{d}\), \((\sum_{j<i}z_{j}^{d}+\sum_{j<i\:\text{in case 2}}|O^{\prime}_{j,\text{unc}}|)= \sum_{j<i\:\text{in case 2}}|O^{\prime}_{j}|\).

Since at most \(n_{i}-z\) points are covered by \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\), there must be at least one of the remaining modified balls which covers \(\leq\frac{n_{i}-z}{k-i+1}\). This concludes the proof. 

**Lemma 3.9** (Coverage of \(B(c_{i},2r)\)).: _When we are in case 2 of the charging argument for some iteration \(i\), we must have that \(|\mathcal{B}(c_{i},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{ez}{\alpha k}\)._

Proof.: We prove the statement by induction on iterations \(i\leq k\) in case 2 of the charging argument. For the case case, iteration \(i\) is the first iteration in case 2. This means that for all previous iterations \(j<i\), \(O^{\prime}_{j}\) has been charged and covered by charging rule I. Hence, \(\mathcal{B}(c_{i},2r)\) cannot intersect any such \(O^{\prime}_{j}\) since the points in \(O^{\prime}_{j}\) are not in \(P_{i}\). We know that \(\mathcal{B}(c_{i},2r)\) also does not intersect any of the remaining modified balls by our assumption that iteration \(i\) is in case 2. Hence, \(\mathcal{B}(c_{i},2r)\) does not intersect \(O^{\prime}_{1}\cup\ldots\cup O^{\prime}_{k}\). The ball \(\mathcal{B}(c_{i},2r)\) can cover \(x\leq\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\) artificial outliers, where \(x\) is the total number of artificial outliers present in \(O^{\prime}_{i}\cup\ldots\cup O^{\prime}_{k}\). The ball \(\mathcal{B}(c_{i},2r)\) can cover at most \(z-\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\) other points, since there are exactly \(z\) points outside \(O_{1}\cup\ldots\cup O_{k}\), of which \(\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\) have been seen in a previous ball \(\mathcal{B}(c_{j},4r)\), for \(j<i\). Hence, \(|\mathcal{B}(c_{i},2r)|\leq z\). So, in order to satisfy the dense invariant, \(|\mathcal{B}(c_{i},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{ez}{\alpha k}\).

Now consider the induction case. We are in an arbitrary iteration \(i\) in case 2, and there have been an arbitrary number of iterations \(j<i\) in case 2. For iterations \(j<i\) in case 2, we assume that \(|\mathcal{B}(c_{j},2r)|\geq\frac{n_{j}-z}{k-j+1}-\frac{ez}{\alpha k}\). Even though \(\mathcal{B}(c_{i},2r)\) is disjoint from the remaining modified balls, it can cover \(x\) artificial outliers and \(z-\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\) other points. For \(x\) we have that:

\[x\leq\sum_{j<i\:\text{in case 1}}(z_{j}^{c}+z_{j}^{d}-x_{j})+\sum_{j<i\:\text{ in case 2}}(z_{j}^{c}+z_{j}^{d}-|O^{\prime}_{j}|-x_{j})\] (5)

\(\sum_{j<i\:\text{in case 1}}(z_{j}^{c}+z_{j}^{d}-x_{j})\) are the total number of artificial outliers from iterations in case 1, and \(\sum_{j<i\:\text{in case 2}}(z_{j}^{c}+z_{j}^{d}-|O^{\prime}_{j}|-x_{j})\) are the total number of artificial outliers from case 2. Then, for the total amount of points in \(\mathcal{B}(c_{i},2r)\), we have:

\[|\mathcal{B}(c_{i},2r)|\leq z+\sum_{j<i}z_{j}^{d}-\sum_{j<i\:\text{in case 2}}|O^{ \prime}_{j}|\leq z\] (6)

This is because by the definition of \(z_{j}^{d}\), we have that \(\sum_{j<i}z_{j}^{d}\leq\sum_{j<i\:\text{in case 2}}|O^{\prime}_{j}|\). So, in order to satisfy the dense invariant, \(|\mathcal{B}(c_{i},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{ez}{\alpha k}\). This concludes the proof. 

## Appendix D Missing proofs Section 3.3

**Lemma 3.10** (Duration of dense cluster for \(z\) is small).: _Assume that we are currently at time \(t\). Let us consider a level \(i\) in which \(z+1\leq\frac{n_{i}^{t}-z}{4(k-i+1)}\). Let \(p=\arg max_{p^{\prime}\in S_{i}}|\mathcal{B}_{P_{i}}(p^{\prime},2r)|\), and \(\mathcal{B}_{\max}=\mathcal{B}_{P_{i}}(p,2r)\). Assume that \(\frac{n_{i}^{t}-z}{2(k-i+1)}\leq|\mathcal{B}_{\text{max}}|\). Then, we can add \(\mathcal{B}_{P_{i}}(p,4r)\) as a cluster in our solution, and this cluster will be dense until time \(t^{\prime}=t+t^{*}\), with \(t^{*}\geq\frac{n_{i}^{t}-z}{4(k-i+1)}\)._

Proof.: The subscript of \(n_{i}\) will be omitted for simplicity. At time \(t\), we know that \(\frac{n^{t}-z}{2(k-i+1)}\leq|\mathcal{B}_{\max}|\). The resulting cluster \(\mathcal{B}_{P_{i}}(p,4r)\) will be dense as long as \(\mathcal{B}_{\max}\) covers more than \(\min\left(z+1,\frac{n^{t^{\prime}}-z}{k-i+1}-\frac{ez}{\alpha k}\right)\) points at time \(t^{\prime}\). Hence, the dense invariant will not be broken as long as \(\mathcal{B}_{\text{max}}\) is greater than \(z+1\). Let \(t^{\prime}\) be the time at which \(|\mathcal{B}_{\text{max}}|<z+1\). We can lower bound the time \(t^{*}\) between \(t\) and \(t^{\prime}\) using the change in size of \(\mathcal{B}_{\text{max}}\). Then, we get the following lower bound:

\[t^{*}\geq\frac{n^{t}-z}{2(k-i+1)}-z\] (7)

Then, since \(z+1\leq\frac{n^{t}-z}{4(k-i+1)}\) by our assumption, we have:

\[t^{*}>\frac{n^{t}-z}{2(k-i+1)}-\frac{n^{t}-z}{4(k-i+1)}\geq\frac{n^{t}-z}{4(k- i+1)}\] (8)

This concludes the proof. 

**Lemma 3.11** (Duration of dense cluster for \(z\) is large).: _Assume that we are currently at time \(t\). Let \(z+1>\frac{n_{i}^{t}-z}{4(k-i+1)}\) for some level \(i\). Let \(p=arg\max_{p^{\prime}\in S_{i}}|\mathcal{B}_{P_{i}}(p^{\prime},2r)|\), and \(\mathcal{B}_{\max}=\mathcal{B}_{P_{i}}(p,2r)\). Assume that \(\frac{n^{t}-z}{k-i+1}-\frac{\epsilon z}{\beta k}\leq|\mathcal{B}_{\text{max}}|\). Then, we can add \(\mathcal{B}_{P_{i}}(p,4r)\) as a cluster in our solution, and this cluster will be dense until time \(t^{\prime}=t+t^{*}\), with \(t^{*}=\Omega(\frac{\epsilon z}{k})\)._

Proof.: The subscript of \(n_{i}\) will be omitted for simplicity. At time \(t\), we know that \(\frac{n^{t}-z}{k-i+1}-\frac{\epsilon z}{\beta k}\leq|\mathcal{B}_{\text{max}}|\). The resulting cluster \(\mathcal{B}_{P_{i}}(p,4r)\) will be dense as long as \(\mathcal{B}_{\text{max}}\) covers more than \(\min\left(z+1,\frac{n^{t^{\prime}}-z}{k-i+1}-\frac{\epsilon z}{\alpha k}\right)\) points at time \(t^{\prime}\), with \(\alpha\) a fixed constant smaller than \(\beta\). Hence, the dense invariant will not be broken as long as \(\mathcal{B}_{\text{max}}\) covers at least \(\frac{n^{t^{\prime}}-z}{k-i+1}-\frac{\epsilon z}{\alpha k}\) points. Let \(t^{\prime}\) be the time at which \(|\mathcal{B}_{\text{max}}|<\frac{n^{t^{\prime}}-z}{k-i+1}-\frac{\epsilon z}{ \alpha k}\). We can lower bound \(t^{*}\) by examining the change in size of \(\mathcal{B}_{\text{max}}\). Using that \(n^{t^{\prime}}\leq n^{t}+t^{*}\), we have:

\[t^{*} \geq(\frac{n^{t}-z}{k-i+1}-\frac{\epsilon z}{\beta k})-(\frac{n^{ t^{\prime}}-z}{k-i+1}-\frac{\epsilon z}{\alpha k})\] \[\geq\frac{n^{t}-z}{k-i+1}-\frac{n^{t}+t^{*}-z}{k-i+1}-\frac{ \epsilon z}{\beta k}+\frac{\epsilon z}{\alpha k}=\frac{-t^{*}}{k-i+1}-\frac{ \epsilon z}{\beta k}+\frac{\epsilon z}{\alpha k}\] (9)

Now, solving for \(t^{*}\) gives:

\[t^{*}\geq\frac{\epsilon z(k-i+1)}{k(k-i+2)}(\frac{1}{\alpha}-\frac{1}{\beta} )\geq\frac{\epsilon z}{2k}(\frac{1}{\alpha}-\frac{1}{\beta})\] (10)

Since \(\alpha\) and \(\beta\) are two fixed constants such that \(\beta>\alpha\), this gives \(t^{*}=\Omega(\frac{\epsilon z}{k})\). This concludes the proof. 

## Appendix E Missing proofs Section 3.4

**Lemma E.1** (Radius guess is small for \(z\) is small).: _Assume that we are currently at time \(t\). Let us consider a level \(i\) for which \(z+1\leq\frac{n_{i}^{t}-z}{4(k-i+1)}\). Let \(p=arg\max_{p^{\prime}\in S_{i}}|\mathcal{B}_{P_{i}}(p^{\prime},2r)|\), where \(S_{i}\) is the sample chosen in Algorithm 5, and \(\mathcal{B}_{\max}=\mathcal{B}_{P_{i}}(p,2r)\). Assume that \(|\mathcal{B}_{\text{max}}|<\frac{n_{i}^{t}-z}{2(k-i+1)}\). Then, with probability at least \(1-\frac{2}{e^{\Psi\log k}}\), for \(\Psi\geq 1\), we have that \(r<r_{\text{OPT}}\) and until time \(t^{\prime}=t+t^{*}\) we do not need to consider the instance for \(r\), with \(t^{*}\geq\frac{n_{i}^{t}-z}{2(k-i+2)}\)._

Proof.: Let \(O_{1},\ldots,O_{k}\) be the optimal balls at time \(t\). Consider the same charging argument used in Lemma 3.7, where we charged one of the optimal balls to \(\mathcal{B}_{P_{i}}(c_{i},4r)\) in each level \(i\). Using Equation 4 and the fact that \(O_{i}^{\prime}\subseteq O_{i}\) for each of the remaining optimal balls, we know that \(|O_{i}\cup\ldots\cup O_{k}|\geq n_{i}^{t}-z\). Then, there must be at least one of the remaining optimal balls covering \(\geq\frac{n_{i}^{t}-z}{k-i+1}\) points. Let \(O_{i}\) be the largest such ball. Using the Chernoff bound, we show that with high probability, one of the sampled points \(p\in S_{i}\) is in \(O_{i}\). Define independent random variables \(X_{1},\ldots,X_{|O_{i}|}\), one for each point in \(O_{i}\). Each \(X_{j}\), corresponding to point \(j\in O_{i}\), will be 1 if and 0 otherwise. Define \(X=\sum_{j=1}^{|O_{i}|}X_{j}\). We know that \(\mathbf{E}[X_{j}]=\frac{|S_{i}|}{n_{i}^{t}}\). Then, using linearity of expectation, we find:

\[\mathbf{E}[X]=\frac{|S_{i}|\cdot|O_{i}|}{n_{i}^{t}}\geq\frac{|S_{i}|}{n_{i}^{t }}\cdot\frac{n_{i}^{t}-z}{k-i+1}\geq\frac{|S_{i}|\cdot(n_{i}^{t}-z)}{n_{i}^{t} \cdot k}\geq\frac{|S_{i}|}{k}(1-\frac{z}{n_{i}^{t}})\] (11)

The condition \(z+1\leq\frac{n_{i}^{t}-z}{4(k-i+1)}\) implies \(z\leq\frac{n_{i}^{t}}{4(k-i+1)}\leq\frac{n_{i}^{t}}{4}\). Then, since \(|S_{i}|=\psi\epsilon^{-1}k^{2}\log k\), it follows that \(\frac{|S_{i}|}{k}(1-\frac{z}{n_{i}^{t}})\geq\frac{3}{4}\psi\epsilon^{-1}k\log k\). Given that \(\psi\geq 6\beta\) and \(\beta>\alpha\geq 1\), this is at least \(3\epsilon^{-1}k\log k\geq 3\Psi\log k\) for \(\Psi\geq 1\).

Now, using the Chernoff bound, we find:

\[\Pr\left[|X-\mathbf{E}[X]|\geq\mathbf{E}[X]|\right]\leq 2e^{-\frac{ \mathbf{E}[X]}{3}}\leq\frac{1}{e^{\Psi\log k}},\] (12)

where \(\Psi\geq 1\).

Hence, with probability of at least \(1-\frac{2}{e^{\Psi\log k}}\), there will be at least 1 point from \(O_{i}\) in \(S_{i}\). Let us call this point \(p\). If \(r\geq r_{\text{OPT}}\), \(\mathcal{B}_{P_{i}}(p,2r)\) would cover all points of \(O_{i}\) since \(p\in O_{i}\) and \(O_{i}\) has radius \(\leq r_{\text{OPT}}\). However, since \(|\mathcal{B}_{\text{max}}|<\frac{n_{i}^{t}-z}{2(k-i+1)}\), we must have that \(r<r_{\text{OPT}}\).

Left to prove is that until time \(t^{\prime}=t+t^{*}\) we do not need this instance of \(r\), where \(t^{*}\geq\frac{n_{i}^{t}-z}{2(k-i+2)}\). To this end, let \(\mathcal{B}_{\text{max}}^{t}\) be \(\mathcal{B}_{\text{max}}\) at time \(t\). Consider the situation at time \(t\). We know that \(|\mathcal{B}_{\text{max}}^{t}|<\frac{n_{i}^{t}-z}{2(k-i+1)}\).

Let \(t^{\prime}\) be some time after \(t\) such that \(|\mathcal{B}_{\text{max}}^{t^{\prime}}|\geq\frac{n_{i}^{t^{\prime}}-z}{k-i+1}\). Hence, the instance for \(r\) becomes valid at time \(t^{\prime}\). We want to derive a lower bound for \(t^{*}=t^{\prime}-t\) to complete the proof. By examining the change in the size of \(\mathcal{B}_{\text{max}}\), we derive the following lower bound:

\[t^{*}\geq\frac{n_{i}^{t^{\prime}}-z}{k-i+1}-\frac{n_{i}^{t}-z}{2(k-i+1)}\] (13)

Using \(n_{i}^{t^{\prime}}\geq n_{i}^{t}-t^{*}\) and subsequently solving for \(t^{*}\) we find that:

\[t^{*}\geq\frac{n_{i}^{t}-z}{2(k-i+2)}\] (14)

This completes the proof. 

**Lemma E.2** (Radius guess is small for \(z\) is large).: _Assume that we are currently at time \(t\). Let us consider a level \(i\) in which \(z+1>\frac{n_{i}^{t}-z}{4(k-i+1)}\). Let \(p=arg\max_{p^{\prime}\in S_{i}}|\mathcal{B}_{P_{i}}(p^{\prime},2r)|\), where \(S_{i}\) is the sample chosen in Algorithm 5, and \(\mathcal{B}_{\text{max}}=\mathcal{B}_{P_{i}}(p,2r)\). Assume that \(|\mathcal{B}_{\text{max}}|<\frac{n_{i}^{t}-z}{k-i+1}-\frac{\epsilon z}{\beta k}\). Then, with probability at least \(1-\frac{2}{e^{\Psi\log k}}\), for \(\Psi\geq 1\), we have that \(r<r_{\text{OPT}}\) and until time \(t^{\prime}=t+t^{*}\) we do not need to consider the instance for \(r\), with \(t^{*}\geq\frac{\epsilon z}{2\beta k}\)._

Proof.: Let \(O_{1},\ldots,O_{k}\) be the optimal balls at time \(t\). As in Lemma E.1, let \(O_{i}\) be the largest remaining optimal ball covering \(\geq\frac{n_{i}^{t}-z}{k-i+1}\) points. Using the Chernoff bound, we show that with high probability, one of the sampled points \(p\in S_{i}\) is in \(O_{i}\). Define independent random variables \(X_{1},\ldots,X_{|O_{i}|}\), one for each point in \(O_{i}\). Each \(X_{j}\), corresponding to point \(j\in O_{i}\), will be 1 if \(j\in S_{i}\) and 0 otherwise. Define \(X=\sum_{j=1}^{|O_{i}|}X_{j}\). We know that \(\mathbf{E}[X_{j}]=\frac{|S_{i}|}{n_{i}^{t}}\). Then, using linearity of expectation, we find

\[\mathbf{E}[X]=\frac{|S_{i}|\cdot|O_{i}|}{n_{i}^{t}}\geq\frac{|S_{i}|}{n_{i}^{t }}\cdot\frac{n_{i}^{t}-z}{k-i+1}.\] (15)

Without loss of generality, we can assume \(\frac{n_{i}^{t}-z}{k-i+1}-\frac{\epsilon z}{\beta k}\geq 1\), otherwise there would not need to be any points in the ball \(\mathcal{B}_{P_{i}}(p,2r)\) to satisfy the dense invariant, and level \(i\) would be trivial. Using this, Equation (15) can be simplified as follows:

\[\mathbf{E}[X]\geq\frac{|S_{i}|}{n_{i}^{t}}\cdot\frac{n_{i}^{t}-z}{k-i+1}\geq \frac{|S_{i}|}{n_{i}^{t}}\cdot\frac{\epsilon z}{\beta k}\] (16)We make a case distinction. Either \(z\geq\frac{n_{i}^{t}}{2}\). In this case, Equation (15) further simplifies to

\[\mathbf{E}[X]\geq\frac{\epsilon|S_{i}|}{2\beta k}.\] (17)

Recall that \(|S_{i}|=\psi\epsilon^{-1}k^{2}\log k\). Given that \(\psi\geq 6\beta\) and \(\beta>\alpha\geq 1\), we have

\[\mathbf{E}[X]\geq\frac{1}{2\beta}\psi k\log k\geq 3k\log k\geq 3\Psi\log k\] (18)

for some \(\Psi\geq 1\).

In the other case \(z<\frac{n_{i}^{t}}{2}\), Equation (15) simplifies to

\[\mathbf{E}[X]\geq\frac{|S_{i}|}{n_{i}^{t}}\cdot\frac{n_{i}^{t}/2}{k-i+1}\geq \frac{|S_{i}|}{2k}\geq\frac{1}{2}\psi\epsilon^{-1}k\log k\geq 3\Psi\log k\] (19)

for some \(\Psi\geq 1\).

In both cases, using the Chernoff bound, we find that with probability at least \(1-\frac{2}{e\psi\log k}\), there will be at least one point \(p\) from \(O_{i}\) in \(X_{i}\). If \(r\geq r_{\mathrm{OPT}}\), \(\mathcal{B}_{P_{i}}(p,2r)\) would cover all points of \(O_{i}\) since \(p\in O_{i}\) and \(O_{i}\) has radius \(\leq r_{\mathrm{OPT}}\). However, since \(|\mathcal{B}_{\max}|<\frac{n_{i}^{t}-z}{k-i+1}-\frac{\epsilon z}{\beta k}< \frac{n_{i}^{t}-z}{k-i+1}\leq|O_{i}|\), we must have that \(r<r_{\mathrm{OPT}}\).

Left to prove is that until time \(t^{\prime}=t+t^{*}\), with \(t^{*}=\Omega(\frac{\epsilon z}{k})\), we do not need this instance of \(r\). To this end, let \(\mathcal{B}_{\max}^{t}\) be \(\mathcal{B}_{\max}\) at time \(t\). Consider the situation at time \(t\). We know that \(|\mathcal{B}_{\max}^{t}|<\frac{n_{i}^{t}-z}{k-i+1}-\frac{\epsilon z}{\beta k}\).

Let \(t^{\prime}\) be some time after \(t\) such that \(|\mathcal{B}_{\max}^{t^{\prime}}|\geq\frac{n_{i}^{t^{\prime}}-z}{k-i+1}\). The cluster \(\mathcal{B}_{P_{i}}(p,2r)\) in level \(i\) is now covering sufficiently many points. We want to derive a lower bound for \(t^{*}=t^{\prime}-t\) to complete the proof. By examining the change in the size of \(\mathcal{B}_{\max}\) and using that \(n_{i}^{t^{\prime}}\geq n_{i}^{t}-t^{*}\), we derive

\[t^{*} \geq\frac{n_{i}^{t^{\prime}}-z}{k-i+1}-(\frac{n_{i}^{t}-z}{k-i+1} -\frac{\epsilon z}{\beta k})\] (20) \[=\frac{n_{i}^{t}-t^{*}-z}{k-i+1}-\frac{n_{i}^{t}-z}{k-i+1}+\frac {\epsilon z}{\beta k}\] (21) \[=\frac{-t^{*}}{k-i+1}+\frac{\epsilon z}{\beta k}\] (22)

as a lower bound for \(t^{*}\). Solving for \(t^{*}\) gives

\[t^{*}\geq\frac{\epsilon z(k-i+1)}{\beta k(k-i+2)}\geq\frac{\epsilon z}{2\beta k}.\] (23)

Since \(\beta\) is a constant, this is in \(\Omega(\frac{\epsilon z}{k})\). This concludes the proof. 

## Appendix F Missing proofs from Section 3.5

**Lemma F.1**.: _The amortized update time of our dynamic algorithm is \(\mathcal{O}(\epsilon^{-3}k^{6}\log(k)\log(\Delta))\)._

Proof.: Let us fix an arbitrary time \(t\) and assume that at time \(t\), there are \(\lambda\leq k\) clusters. Let time \(t^{\prime}=t+t^{*}\) be the time at which we need to invoke Procedure 5 on an arbitrary level \(i\leq k\), due to the dense invariant being violated. We have two cases:

* \(z+1\leq\frac{n_{i}^{t}-z}{4(k-i+1)}\): For this case, Lemma 3.10 shows the following: If we find a cluster such that \(|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\frac{n_{i}^{t}-z}{2(k-i+1)}\) at time \(t\), then this cluster will remain dense for \(t^{*}\geq\frac{n_{i}^{t}-z}{4(k-i+1)}\) update operations (either insert or delete).
* \(z+1>\frac{n_{i}^{t}-z}{4(k-i+1)}\): For this case, Lemma 3.11 proves that if we find a cluster such that \(|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\frac{n_{i}^{t}-z}{k-i+1}-\frac{\epsilon z} {\beta k}\) at time \(t\), then this cluster will remain dense for the next \(t^{*}\geq\frac{\epsilon z}{2k}(\frac{1}{\alpha}-\frac{1}{\beta})\) time steps. Asymptotically, \(t^{*}=\Omega(\frac{\epsilon z}{k})\). By substituting the lower bound of \(z\) in this formula, we obtain \(t^{*}=\Omega(\frac{\epsilon n_{i}^{t}}{k^{2}})\).

Observe that in both cases, we have a worst-case guarantee for the number of time steps during which the cluster \(\mathcal{B}_{P_{i}}(c_{i},2r)\) remains dense. This, in turn, allows us to achieve a worst-case amortized update time, rather than the weaker notion of expected amortized update time. We analyze the update time of each case separately.

Let us start with the first case which is \(z+1\leq\frac{n_{i}^{t}-z}{4(k-i+1)}\) and suppose that we find a cluster such that \(|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\frac{n_{i}^{t}-z}{2(k-i+1)}\) at time \(t\). We know that \(t^{*}\geq\frac{n_{i}^{t}-z}{4(k-i+1)}\). The cost of reclustering levels \(i,\ldots,k\) according to Lemma 3.3 is \(\mathcal{O}({n_{i}^{t^{\prime}}}\epsilon^{-1}\cdot k^{3}\log k)\), with \(n_{i}^{t^{\prime}}\leq n_{i}^{t}+t^{*}\). Then, the amortized update time of an arbitrary update operation is \(\mathcal{O}(\frac{n_{i}^{t}}{t^{*}}(n_{i}^{t}+t^{*})\epsilon^{-1}k^{3}\log k )=\mathcal{O}(\frac{n_{i}^{t}}{t^{*}}\epsilon^{-1}k^{3}\log k)\). Since \(z\leq\frac{n_{i}^{t}}{4(k-i+1)}\), we obtain \(\frac{n_{i}^{t}}{t^{*}}\leq n_{i}^{t}\cdot\frac{4(k-i+1)}{n_{i}^{t}-z}\leq 4 k\cdot\frac{n_{i}^{t}}{n_{i}^{t}-z}\leq 4k\cdot\frac{n_{i}^{t}}{n_{i}^{t}(1 -\frac{1}{4(k-i+1)})}=4k\cdot\frac{4(k-i+1)}{4(k-i+1)-1}=\mathcal{O}(k)\enspace.\) Thus, \(\mathcal{O}(\frac{n_{i}^{t}}{t^{*}}\epsilon^{-1}k^{3}\log k)=\mathcal{O}( \epsilon^{-1}k^{4}\log k)\).

Now, we consider the second case. Using a similar analysis as in the first case, we have \(t^{*}=\Omega(\frac{cn_{i}^{t}}{k^{2}})\). Then, the amortized update time of an arbitrary update operation is \(\mathcal{O}(\frac{n_{i}^{t}}{t^{*}}\epsilon^{-1}k^{3}\log k)=\mathcal{O}( \epsilon^{-2}k^{5}\log k)\).

After reclustering levels \(i,\ldots,k\), there is no longer a lower bound for \(t^{*}\) for levels \(j<i\). Thus, it could happen that such a level \(j\) needs to be reclustered soon after time \(t^{\prime}\). Since \(j<k\), this leads to an extra factor \(k\) such that the final amortized cost is \(\mathcal{O}(\epsilon^{-2}k^{6}\log k)\).

Finally, we need to consider the case that offline clustering fails for a level \(i\). That is, we fail to find a ball covering sufficiently many points in Procedure 5. If offline clustering fails on some level \(i\) in case 1, meaning we fail to find a ball such that \(|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\frac{n_{i}^{t}-z}{2(k-i+1)}\), then Lemma E.1 proves that with probability \(1-\frac{1}{e^{2(k\log k)}}\), the guess \(r\) is small (i.e., \(r<r_{\text{OPT}}\)) and indeed, remains small and we do not need to consider this guess until a time \(t^{\prime}=t+t^{*}\) where \(t^{*}\geq\frac{n_{i}^{t}-z}{2(k-i+2)}\).

At time \(t^{\prime}\), we recluster levels \(i,\ldots,k\). Then, since we are in the first case, we have \(z\leq\frac{n_{i}^{t}}{4(k-i+1)}\). Thus, the amortized update time of an arbitrary update operation in this case is \(\mathcal{O}(\frac{1}{t^{*}}(n_{i}^{t}+t^{*})\epsilon^{-1}k^{3}\log k)=\mathcal{ O}(\frac{n_{i}^{t}}{\ell^{4}}\epsilon^{-1}k^{3}\log k)=\mathcal{O}(\epsilon^{-1}k^{4} \log k)\) using Lemma 3.3.

Next, we consider the situation when the offline clustering fails in case 2. Specifically, this occurs when we fail to find a ball such that \(|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\frac{n_{i}-z}{k-i+1}-\frac{cz}{\beta k}\). According to Lemma E.2, the guess \(r\) is small (i.e., \(r<r_{\text{OPT}}\)) with probability \(1-\frac{1}{e^{2(\log k)}}\) and indeed, remains small and we do not need to consider this guess until time \(t^{\prime}=t+t^{*}\) where \(t^{\prime}=\Omega(\frac{cz}{k})\). This leads to an amortized update time of \(\mathcal{O}(\epsilon^{-2}k^{5}\log k)\). As mentioned above, the running time needs to be multiplied by an additional factor \(k\) to account for the possible need to recluster lower levels.

Finally, we need to consider \(\mathcal{O}(\frac{\log\Delta}{\log(1+\epsilon)})\) guesses for the optimal radii. For small \(\epsilon\), this is in \(\mathcal{O}(\frac{\log\Delta}{\epsilon})\). Thus, we derive the final amortized update time of \(\mathcal{O}(\epsilon^{-3}k^{6}\log k\log\Delta)\). 

## Appendix G How to support discrete \(k\)-center clustering with outliers

**Lemma G.1**.: _Let \((M,d)\) be a metric space and \(\epsilon>0\) be an accuracy parameter. The spread ratio \(\Delta=\frac{d_{\max}}{d_{\min}}\) of all points ever inserted is assumed to be bounded. There exists a randomized fully dynamic algorithm that maintains a discrete \(k\)-center solution that allows up to \((1+\epsilon)z\) many outliers on the current set of points. At every point in time \(t\), the current clustering with centers \(c_{1},\ldots,c_{\lambda}\) is a \((4+\epsilon)\)-approximation to an optimal solution for the \((k,z)\)-center problem with high probability and \(c_{i}\in P^{t}\) for all \(i\leq\lambda\). Upon insertion or deletion of a point, the data structure is updated in amortized update time \(\mathcal{O}(\epsilon^{-3}k^{6}\log(k)\log(\Delta))\)._

Proof.: Let \(r\) and \(i\) be fixed, and \(c_{i}\) be the center of cluster \(C_{i}=\mathcal{B}_{P_{i}}(c_{i},4r)\). As long as \(c_{i}\) is not deleted, we report \(c_{i}\) as the \(i\)-th center. Note that any feasible solution for the discrete version is also a feasible solution for the non-discrete version. Therefore, the optimal radius for the discrete version is not smaller than the optimal radius for the non-discrete version.

After the deletion of the point \(c_{i}\), we consider two cases: \(\min\left(z+1,\frac{n_{i}-z}{k-i+1}-\frac{cz}{\alpha k}\right)\leq 0\) or \(\min\left(z+1,\frac{n_{i}-z}{k-i+1}-\frac{cz}{\alpha k}\right)>0\). If \(\min\left(z+1,\frac{n_{i}-z}{k-i+1}-\frac{cz}{\alpha k}\right)\leq 0\), then \(n_{i}<(1+\varepsilon)z\) as \(i\geq 1\) and \(\alpha>1\). Hence, we can report all the \(n_{i}\) points as outliers and stop. We next consider the second case. The dense invariant states that \(|\mathcal{B}_{P_{i}}(c_{i},2r)|\geq\min(z+1,\frac{n_{i}-z}{k-i+1}-\frac{cz}{ \alpha k})\). Therefore, \(|\mathcal{B}_{P_{i}}(c_{i},2r)|>0\) holds in the second case, and there exists a point \(\hat{p}\in\mathcal{B}_{P_{i}}(c_{i},2r)\). Then we report an arbitrary point \(\hat{p}\in\mathcal{B}_{P_{i}}(c_{i},2r)\) after the deletion of \(c_{i}\). Note that we do not replace \(c_{i}\) by \(\hat{p}\) in our data structure, and \(c_{i}\) does not change in our data structure as long as the \(i\)-th level is not reconstructed. The point \(\hat{p}\) is just the center that we report for the discrete version.

We next prove the \(6\)-approximation guarantee. To do this, we show that \(C_{i}\subseteq\mathcal{B}(\hat{p},6r)\). This implies that any feasible solution with radius \(4r\) for the non-discrete version can be used to report a feasible solution for the discrete version. Also, see Figure 4 for a visual representation. Let \(q\in C_{i}\). We show that \(q\in\mathcal{B}(\hat{p},6r)\). Since \(q\in C_{i}=\mathcal{B}(c_{i},4r)\), we have \(d(q,c_{i})\leq 4r\). Moreover, \(d(c_{i},\hat{p})\leq 2r\) since \(\hat{p}\in\mathcal{B}(c_{i},2r)\). Then by the triangle inequality, we have

\[d(q,\hat{p})\leq d(q,c_{i})+d(c_{i},\hat{p})\leq 4r+2r=6r.\]

To report a solution for the discrete version, it is enough to keep \(\mathcal{B}(c_{i},2r)\cap P_{i}\). Note that our data structure already stores \(C_{i}\cap P_{i}\), and since \(\mathcal{B}(c_{i},2r)\cap P_{i}\subseteq C_{i}\cap P_{i}\), the space complexity and time complexity remain the same. 

Figure 4: Illustration of the claims proof. Part (i): before deletion of \(c_{i}\), we report \(c_{i}\) as the center. Part (ii): after deletion of \(c_{i}\), we report an arbitrary point \(\hat{p}\in\mathcal{B}(c_{i},2r)\) as the center. Then, \(\mathcal{B}(\hat{p},6r)\) can cover all the points in the cluster \(C_{i}\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The problem setting stated at the beginning of the abstract summarizes this paper's scope as formalized in Section 1.3. The main statements made in the abstract can be found in Theorem 1.1 in Section 1.1, which is a summarization of Lemma 3.7 in Section 3.2 and Lemma F.1 in Section 3.5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: As stated in the abstract, Section 1.1, Section 1.3 and the result Lemma 3.7, our algorithm yields a bicriteria approximation. This means that we approximate the objective function and also allow a slight violation of the outlier side constraint while comparing with an optimal solution that fulfills the side constraint exactly. Our results hold with high probability, which is stated formally in statement Lemma 3.6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The problem setting alongside assumptions are stated in Section 1.3. This paper's main statements are Lemma 3.7 and Lemma F.1. Lemma 3.8 and Lemma 3.9 are used to prove Lemma 3.7. It uses the assumption that the level and dense invariants hold. Lemma 3.6, Lemma 3.4 and Lemma 3.5 show that running our procedures maintains these invariants. Lemma F.1 uses Lemma 3.3, Lemma 3.11, Lemma 3.10, Lemma E.1, Lemma E.2. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not contain experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not contain experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not contain experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not contain experiments. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not contain experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: As our paper is of theoretical nature, most of the points addressed in the Code of Ethics do not apply. Further, we could not identify any foreseeable dangers or harms directly caused by the utilization of our results. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no foreseeable direct societal impact of the work performed.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.