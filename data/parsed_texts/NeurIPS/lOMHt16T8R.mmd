# PaCE: Parsimonious Concept Engineering

for Large Language Models

Jinqi Luo &Tianjiao Ding\({}^{\star}\)&Kwan Ho Ryan Chan\({}^{\star}\)&Darshan Thaker\({}^{\star}\)

Aditya Chattopadhyay&Chris Callison-Burch\({}^{\star}\)&Rene Vidal\({}^{\star}\)

\({}^{\star}\)University of Pennsylvania &Johns Hopkins University

{jinqiluo,tjding}@upenn.edu

Equal contribution.

###### Abstract

Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable outputs via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose **P**arsimonious **C**ncept **E**ngineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components. By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities. Our collected dataset for concept representations is available at https://github.com/peterljq/Parsimonious-Concept-Engineering.

## 1 Introduction

Large Language Models (LLMs) are useful for tasks as far ranging as question answering [65, 77], symbolic reasoning [25, 56], multi-modal synthesis [40, 44, 86], and medical diagnosis [85]. LLMs are typically pre-trained on a broad collection of textual corpora with the next-token prediction objective [54, 70], enabling them to generate human-like text. An important aspect of deploying pre-trained LLMs for real-world applications is preventing undesirable responses such as toxic language, hallucinations, and biased information through alignment methods, which aim to make AI systems behave in line with human intentions and values [28]. A common alignment approach is tuning LLMs with human feedback [55, 62] for better instruction-following capabilities. However, after such aligning, undesirable and harmful content can still be elicited from LLMs. For example, jailbreaking can produce hate speech and aggression [22, 32], stress-testing shows hallucinatory responses such as illogical statements [87], and various kinds of biases are not fully removed from LLM responses [19]. This emphasizes the need for further development of aligned LLMs.

Overall, alignment methods can largely be categorized into: parameter fine-tuning, prompt engineering, and activation engineering. _Parameter fine-tuning_ methods, such as low-rank adaptation[26] and knowledge editing [14; 74], involve updating the model parameters using datasets of input-response pairs [75]. Unfortunately, such computations over large datasets are often costly. Furthermore, whenever a new category of undesirable behaviors is identified or a new group of customers is acquired, the LLM supplier has to incur the cost of data creation and fine-tuning again. _Prompt engineering_ attempts to manipulate the LLM's reasoning with carefully designed instruction prompts [78; 80; 81]. However, effective instructions are commonly obtained through empirical trial-and-error, with no guarantee of coverage across tasks of different domains. Notably, recent works show that the instruction itself can be lengthy [38] or contain human errors [10; 61].

_Activation engineering_, i.e., algorithms that modify the latent _activations_ of LLMs, has emerged to alleviate high-cost and poor coverage of tasks. Recent work has shown that certain directions in the activation space of LLMs are associated with semantic concepts (c.f. SS2.1). Thus, given an input prompt at inference time, modifying its neural activations towards or away from these directions controls the semantics of the model response. For example, methods based on Vector Addition (VecAdd) [37; 43; 67; 68; 69; 72; 88] directly add multiples of a concept direction to a neural activation, while those based on Orthogonal Projection (OrthoProj) [23; 88] subtract from a neural activation its orthogonal projection onto a concept direction. Nonetheless, these methods face two major challenges. First, these methods inadequately model the geometry of the activation space, as we will detail in SS2.2. Hence, they tend to either remove benign concepts, harming linguistic capability; or insufficiently remove undesirable concepts, thereby failing the alignment task. Second, for each alignment task, these methods typically only remove a single concept direction from the input activation vector, while there may be multiple concepts related to the alignment task.

To address these challenges, we propose Parsimonious Concept Engineering (PaCE), an activation engineering framework for alignment that i) enforces alignment goals effectively and efficiently, ii) retains linguistic capability, and iii) adapts to new alignment goals without costly parameter fine-tuning. PaCE consists of two stages: (1) Concept Construction and Partition, and (2) Activation Decomposition and Intervention (Figure 3). We summarize the procedure of PaCE below and highlight our contributions in bold.

* _Concept Dictionary Construction and Partition (SS3.2)_: Since existing works only provide a limited number of concept directions, **we collect a large concept dictionary, PaCE-1M, that consists of 40,000 concept directions extracted from over 1,200,000 context sentences.** In particular, for each concept in the Brown Corpus [18], we use a knowledge-driven GPT [35; 44; 65] to propose contextual scenarios to describe the concept, and extract concept directions in the representation (activation) space [88] from the context sentences. This is done only once offline. Further, given any alignment task, we instruct a GPT to automatically partition the concept directions in the dictionary into benign and undesirable directions, which is done once per task offline.
* _Activation Decomposition and Intervention (SS3.3)_: At inference time, given any user input prompt, **we decompose the activations as a sparse linear combination of concept directions using sparse coding techniques**. Notably, this allows for an efficient and accurate estimate of both undesirable and benign components in the activations, which is overlooked in previous activation engineering methods. By removing the undesirable components from the activations, we reorient the behavior of LLMs toward alignment goals, while maintaining their linguistic capability.

We evaluate PaCE on alignment tasks including response detoxification, faithfulness enhancement, and sentiment revising (SS4). **We show that PaCE achieves state-of-the-art performance on these tasks, while retaining its linguistic capability at a comparable level.** We further shed insights on the concept directions of PaCE-1M: concept directions tend to form clusters with directions from each cluster corresponding to similar semantics, and decomposing an activation reveals its semantics.

## 2 Basics of Latent Space Engineering

As motivated above, in this paper we are interested in controlling LLMs by leveraging structures in their latent space. We begin by reviewing some basic properties of the latent space in SS2.1. This lays the foundation for previous methods on latent space intervention in SS2.2 as well as our method in SS3.

Figure 1: Our framework PaCE achieves alignment goals by sparse coding and adjusting vectors in the activation space of the LLM Decoder Layer (DCL).

### The Latent Space and Its Linear Controllability

Let \(\mathcal{Z}\subset\mathbb{R}^{d}\) denote a _latent space_ whose elements can be mapped into text. That is, there exists a (surjective) decoder \(g:\mathcal{Z}\to\mathcal{T}\) where \(\mathcal{T}\) is some set of texts. For ease of notation, we follow the convention and use \(\bm{z}_{\text{some text}}\in\mathcal{Z}\) to denote an element in the pre-image \(g^{-1}(\text{`some text'})\).

**Linear Controllability.** Consider the word pairs ('France', 'Paris') and ('Japan', 'Tokyo') - the latter is the capital of the former. It is natural to wonder if their latent codes have such correspondence. In various settings as we will review, there is approximately a _linear_ relation: there exists a \(\bm{v}_{\text{capital}}\in\mathbb{R}^{d}\), such that \(\bm{z}_{\text{France}}+c\cdot\bm{v}_{\text{capital}}\approx\bm{z}_{\text{Paris}}\) for some control strength \(c>0\), and \(\bm{z}_{\text{Japan}}+c^{\prime}\cdot\bm{v}_{\text{capital}}\approx\bm{z}_{ \text{Tokyo}}\) for some \(c^{\prime}>0\). Beyond this example, prior works seem to support the existence of a set of _concept directions_\(\mathcal{V}\subset\mathbb{R}^{d}\) that linearly relate pairs of latent codes2. Note, however, that the notion of _linear controllability_ is different from the notion _linear or affine combination_ in linear algebra in that there may be only one choice of \(c\) such that \(\bm{z}+c\bm{v}\in\mathcal{Z}\).

Footnote 2: \(\bm{v}_{\text{capital}}\in\mathcal{V}\) typically can not be decoded by \(g\) to obtain the text ‘capital’, as opposed to elements in \(\mathcal{Z}\).

_Remark 1_ (\(\mathcal{Z}=\text{Word Embeddings}\)).: A classic setting where linear controllability shows up is that of _word embeddings_. Here, \(\mathcal{T}\) is the vocabulary (say, the set of English words), \(\mathcal{Z}\) contains some vectors in \(\mathbb{R}^{d}\), and \(g\) is a bijection between \(\mathcal{Z}\) and \(\mathcal{T}\). In the seminal work of Mikolov et al. [50], the authors observe that word embeddings learned by recurrent neural networks approximately enjoy relations such as \(\bm{z}_{\text{king}}-\bm{z}_{\text{man}}+\bm{z}_{\text{woman}}\approx\bm{z}_{ \text{queen}}\), where one can view \(\bm{z}_{\text{woman}}-\bm{z}_{\text{man}}\) as the concept direction \(\bm{v}\in\mathcal{V}\) and \(c=1\) as the control strength. This observation is later extended to word embeddings of various networks and learning objectives such as word2vec [49], Skip-Grams [34, 48], GloVe [59], and Swivel [63]. On the theoretical front, a fruitful line of research has been devoted to understanding the emergence of such properties in word embeddings [1, 2, 3, 17, 21, 52].

_Remark 2_ (\(\mathcal{Z}=\text{Neural Activations}\)).: Modern neural architectures such as transformers have significantly boosted the linguistic performance of language models. Much of their success is attributed to the attention mechanism, which incorporates long-range context into the neural activations in transformers. This has motivated people to take \(\mathcal{Z}\) as certain hidden states in transformers3, and search for concept directions \(\mathcal{V}\) in \(\mathcal{Z}\). An interesting line of works has supported the empirical existence of \(\mathcal{V}\): [6, 46] find directions that indicate truthful output, [68] finds directions for sentiments, [88] finds directions for emotions and honesty, and [53] finds directions for current player tile in a synthetic board game model. Interestingly, [30, 57, 71] further offer theoretical models, under which the linear controllability shows up provably in the latent space of LLMs.

Footnote 3: A variety of choices of layers have been explored in the literature; see, e.g., [67] for a comparison.

### Controlling Language Models via Latent Space Engineering

The above findings have supported the development of practical methods to control the behavior of language models. As we will see, a key challenge there is to decide the correct control strength.

**Vector Addition.** The work of [37, 43, 53, 67, 68, 72, 88] proposes to add or subtract multiples of a concept direction from the latent code. For example, to remove hatred from \(\bm{z}\), one performs

\[\bm{z}\mapsto\bm{z}-\hat{c}\cdot\bm{v}_{\text{hatred}},\] (VecAdd)

where \(\hat{c}>0\) is a parameter of the control strength. In principle, as each input prompt may contain a different 'extent' of the concept to be removed, \(\hat{c}\) should depend on both the prompt and the concept. Thus, in practice, one either tunes \(\hat{c}\) per input prompt and concept, which is laborious, or one fixes

Figure 2: To remove a concept direction ‘red’ from the latent code ‘red apple’ (left), prior works use i) orthogonal projection (middle right, (OrthoProj)), which may remove extra directions, or ii) vector addition (right, (VecAdd)), where it is hard to pick the edit strength \(c\). Instead, PaCE explicitly models the concept dictionary in the latent space and use oblique projection (middle left).

a \(\hat{c}\), which is sub-optimal. Indeed, this has been observed by the work [72]: In their Table 10, the optimal coefficients \(\hat{c}\) are markedly different across the examples; see also their 'discussion' section.

**Orthogonal Projection.** The work of [5] proposed to remove gender bias in _word embeddings_ by projecting the embeddings onto the orthogonal complement to a gender direction \(\bm{v}_{\text{gender}}\):

\[\bm{z}\mapsto\Pi_{\text{span}(\bm{v}_{\text{polar}})^{\perp}}\bm{z}=\bm{z}-\Pi_ {\text{span}(\bm{v}_{\text{polar}})}\bm{z}.\] (OrthoProj)

Here, for any \(\bm{w}\in\mathbb{R}^{d}\), \(\text{span}(\bm{w})\) is the linear subspace spanned by \(\bm{w}\), and for any linear subspace \(\mathcal{S}\subset\mathbb{R}^{d}\), \(\Pi_{S}\) denotes the ortho-projector onto \(\mathcal{S}\). Such an idea is later applied to _neural activations_ of LLMs [23; 88]. Applying orthogonal projection to remove concept directions from latent codes may be reasonable: if directions corresponding to different concepts are orthogonal, then orthogonal projection only removes the gender direction while leaving the others intact. That being said, there are often more concept directions presented, and they are not orthogonal. For example, [29] shows that causally related concepts only exhibit _partial_ orthogonality for their directions.

To sum up, numerous attempts have been made to control the behavior of language models. However, existing methods either have a control strength parameter that is hard to tune or may remove extra concept directions. As we will see in the next section, these issues can be resolved by the proposed PaCE framework, which explicitly models the geometry of a large concept dictionary.

## 3 Our Method: Parsimonious Concept Engineering

### Activation Intervention via Overcomplete Oblique Projection

Can we efficiently remove one or more target concept directions from a given latent activation without affecting other concept directions present? To address this problem, our key insight is to model as many concept directions as possible, and then decompose the activation to estimate its components along these directions. Figure 2 presents an idealized visual example. Here, one is given a latent activation meaning'red apple', and the goal is to remove the'red' direction from the activation (left). As illustrated, orthogonal projection and vector addition tend to fail (middle right and right), as we discussed in SS2.2. In contrast, by decomposing the activation along the concept directions of'red' and 'apple', one can safely remove the component along'red' without affecting that along 'apple' (middle left). This is related to the idea of _oblique projection_, which gives the name of this section.

That said, several challenges remain to be addressed. As motivated above, to accurately model semantic concepts, one needs to collect as many concept directions in the latent space as possible. Since existing works only provide a limited number of concept directions (as reviewed in Remark 2), we contribute by collecting a large dictionary of concept directions, which we will discuss in SS3.2. Moreover, oblique projection is well-defined only when the concept directions are linearly independent, while concept directions are often dependent (as we show in SS4.3) so the decomposition is not unique. SS3.3 discusses our choice of decomposition algorithm to address this difficulty.

### Knowledge-Driven Concept Dictionary

**Concept Dictionary Construction.** We take the top 40,000 words from the Brown Corpus [18] ranked by word frequency [4] as the concept collection \(T\). For each concept \(t_{i}\in T\), we prompt GPT-4 to generate around \(30\) pieces of contextual stimuli \(s_{i}=\{s_{i}^{1},s_{i}^{2},\cdots,s_{i}^{30},\cdots\}\) that are scenarios

Figure 3: Pipeline of PaCE has several major steps: Step 1 collects concept vectors and constructs the concept dictionary, Step 2 decomposes the activation vector of the given input by sparse coding to get concept coefficients, and Step 3 performs editing on the concepts towards reoriented response.

describing the concept. To enhance the diversity of the concept stimuli, we retrieve knowledge from Wikipedia [35; 44; 65] (as we detail in Appendix B.4) to augment the prompt of stimulus synthesis. Samples of concepts and their stimuli are shown in Figure 4 and Appendix Figure 12. For each concept \(t_{i}\), we extract a direction \(\bm{v}_{i}^{\ell}\) from the activations of its contextual stimuli at the \(\ell\)-th decoder layer of the LLM [88], which gives a dictionary \(\bm{D}^{\ell}\in\mathbb{R}^{d\times n}\) per layer (detailed in Appendix B.2).

**Task-Driven Dictionary Partition.** Given an alignment task, we further instruct GPT-4 as a concept partitioner to classify whether a concept needs to be removed from the input representation. To take detoxification as an example, the concept 'harmful' is highly correlated to the toxic response (hence needs removal) while benign concepts such 'bird' and 'laptop' will remain. That is, the instructed GPT-4 partitions the concepts into undesirable and benign to the alignment tasks. The full prompting templates of concept synthesis and partitioning are shown in Appendix E. In the next sub-section, we describe the notations and usages of the annotated concept dictionary.

### Overcomplete Oblique Projection via Sparse Coding

Now that we have a dictionary \(\bm{D}=[\bm{v}_{1},\dots,\bm{v}_{n}]\in\mathbb{R}^{d\times n}\) of \(n\) concepts directions4, where each \(\bm{v}_{i}\) is a concept direction of known semantic meaning. Given a latent activation \(\bm{z}^{\text{in}}\) coming from the user input, how can we control it via oblique projection?

Footnote 4: For notational simplicity, we discuss sparse coding for a single \(\bm{D}\); Algorithm 2 deals with multiple layers.

**Oblique Projection.** The general paradigm of oblique projection can be stated as follows.

* _Step \(1\)-Decomposition:_ Find \(c_{1}^{\text{in}},\dots,c_{n}^{\text{in}}\in\mathbb{R}\) such that \(\bm{z}^{\text{in}}=c_{1}^{\text{in}}\bm{v}_{1}+\dots+c_{n}^{\text{in}}\bm{v}_{ n}+\bm{r}^{\text{in}}\) by solving \[\bm{c}^{\text{in}}\in\operatorname*{argmin}_{c}\frac{1}{2}\|\bm{z}^{\text{in}}- \bm{D}\bm{c}\|_{2}^{2}+\Omega(\bm{c}),\] (1) where \(\Omega(\bm{c})\) is a sparsity-promoting regularizer that we will discuss soon. Then, each coefficient \(c_{i}^{\text{in}}\) for \(i\in\{1,\dots,n\}\) can be viewed as how much the concept represented by \(\bm{v}_{i}\) is present in \(\bm{z}^{\text{in}}\), and \(\bm{r}^{\text{in}}\) is the residual that is not explained by \(\bm{D}\).
* _Step 2-Intervention:_ Obtain the controlled coefficients \(c_{1}^{\text{ctrl}},\dots,c_{n}^{\text{ctrl}}\in\mathbb{R}\), where \(c_{i}^{\text{ctrl}}\) is set to \(c_{i}^{\text{in}}\) if the concept of \(\bm{v}_{i}\) is benign to the control task and \(0\) if undesirable (which has been decided offline in SS3.2). Then, synthesize a new latent code using the modified coefficients and the residual by taking \(\bm{z}^{\text{ctrl}}=c_{1}^{\text{ctrl}}\bm{v}_{1}+\dots+c_{n}^{\text{ctrl}} \bm{v}_{n}+\bm{r}^{\text{in}}\).

The synthesized \(\bm{z}^{\text{ctrl}}\) will replace \(\bm{z}^{\text{in}}\) to be passed on to the next layer of the neural network5.

Footnote 5: While in sparse coding one typically removes the residual \(\bm{r}^{\text{in}}\) for denoising purpose, here \(\bm{r}^{\text{in}}\) may contain useful information (e.g., grammar) not captured by the dictionary, so we simply keep it in the synthesized \(\bm{z}^{\text{ctrl}}\).

_Remark 3_ ((OrthoProj, \(\text{VecAdd}\)) = Special Cases of Oblique Projection).: If one restricts \(\bm{D}\) to contain only the undesirable concept directions (i.e., the ones to be removed from the latent code), and further takes \(\Omega(\cdot)\) to be a constant function, it can be shown that oblique projection reduces to the special case of orthogonal projection (OrthoProj). On the other hand, if \(\bm{D}\) contains only one undesirable concept direction, and \(\Omega(\cdot)\) is \(\lambda\|\cdot\|_{2}^{2}\) for some regularization strength \(\lambda\in\mathbb{R}\), then oblique projection recovers vector addition (VecAdd), by setting \(\lambda\) equal to \(\hat{c}\) in (VecAdd). We provide proofs in Appendix B.1. As we will see next, our method differs from these two in having a larger dictionary and a sparsity-promoting regularizer.

**Overcomplete Oblique Projection.** As mentioned in SS3.1, when the concept directions are linearly independent, then there is a unique decomposition of the latent code along the concept directions. However, often the concept directions can be dependent or nearly so, leading to infinitely many decompositions or numerical issues. To address this issue, we leverage the idea of _sparse coding_: natural signals are typically generated from sparse linear combinations of dictionary atoms, and

Figure 4: Examples of the constructed concepts and their partition for the detoxification task sampled from our PaCE-1M.

[MISSING_PAGE_FAIL:6]

representations. Finally, Table 3 shows the contribution of design choices in PaCE, and Figure 6 shows the effect of the dictionary size on the performance. We observe clear improvement after each design choice is progressively added. Appendix B.5 includes the details of these ablation studies.

**Linguistic Capability.** To validate that the detoxified representations of PaCE are still effective on general linguistic capability, we also evaluate the responses by N-gram fluency and perplexity. Furthermore, we apply PaCE to detoxify MMLU questions (which are naturally unharmful) to show that the detoxification will not significantly degrade the LLM's reasoning capability. We observe that the MMLU response accuracy of PaCE is the highest among all activation manipulation baselines.

**Efficiency.** Table 2 shows that PaCE is more time-efficient compared to the OrthoProj which also projects the concept vector onto the input vector. PaCE sees a three times speed improvement in average time per response and a two times improvement over average time per word when compared to OrthoProj. While PaCE is computationally slower than VecAdd, we argue the performance gain in a majority of the categories is a benefit that outweighs this particular shortcoming.

**Solvers.** Figure 7 additionally evaluates Orthogonal Matching Pursuit (OMP) [7, 58], a fast greedy solver for the activation decomposition. OMP iteratively adds to the support the concept that has maximum coherence with the unexplained residual and updates the residual by solving the least square using the new support. It stops when a pre-defined maximum size \(k\) of support is reached. Intuitively, the \(k\) is the number of non-zero elements in the solved coefficients. We observe from the table that one can choose improvements in computational speed at the cost of safety performance.

\begin{table}
\begin{tabular}{l l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{\begin{tabular}{c} Target \\ Model \\ \end{tabular} } & \multirow{2}{*}{Method} & \multicolumn{6}{c}{Safety (\%, \(\uparrow\))} & \multicolumn{6}{c}{Linguistic Capability} \\ \cline{3-14}  & & & PS & PG & EM & IA & MH & OF & PH & PP & UB & \begin{tabular}{c} Fluency \\ (\(\uparrow\)) \\ \end{tabular} & \begin{tabular}{c} Perplexity \\ (\(\downarrow\)) \\ \end{tabular} & \begin{tabular}{c} PMLU \\ (\%, \(\uparrow\)) \\ \end{tabular} \\ \hline \multirow{6}{*}{\begin{tabular}{c} _Safety_ \\ \end{tabular} } & Vanilla [70] & 17.6 & 19.5 & 10.1 & 7.79 & 11.3 & 17.2 & 22.6 & 11.8 & 17.2 & 7.70 & 3.51 & **43.4** \\  & Prompting [70] & **82.5** & 47.3 & 57.8 & 65.2 & 75.1 & 54.8 & 72.0 & 72.4 & 56.1 & 7.50 & **3.04** & 15.4 \\  & VecAdd [67, 72, 88] & 50.9 & **58.9** & **59.0** & 53.9 & 66.1 & 55.0 & 60.7 & 61.7 & 66.4 & 6.58 & 7.58 & 29.0 \\  & OrthoProj [23, 88] & 50.7 & 57.9 & 50.2 & 47.5 & 67.0 & 50.1 & 74.9 & 65.7 & 66.4 & 7.46 & 3.73 & 34.1 \\  & PaCE (Ours) & 69.6 & 46.2 & 58.2 & **75.3** & **94.2** & **62.3** & **80.8** & **72.8** & **88.3** & **8.07** & 3.52 & 37.1 \\ \hline \multirow{6}{*}{
\begin{tabular}{c} _Safety_ \\ \end{tabular} } & Vanilla [70] & 8.01 & 23.7 & 13.6 & 19.8 & 18.3 & 21.6 & 13.6 & 14.0 & 16.7 & **7.66** & 2.48 & **54.9** \\  & Prompting [70] & 35.8 & 68.3 & 59.3 & 52.5 & 73.5 & 23.4 & 78.0 & 71.1 & 66.5 & 7.63 & **2.22** & 52.1 \\ \cline{1-1}  & VecAdd [67, 72, 88] & 76.6 & 71.4 & 70.0 & 64.3 & 87.2 & 66.9 & 47.4 & 74.5 & 71.1 & 7.46 & 2.75 & 51.6 \\ \cline{1-1}  & OrthoProj [23, 88] & 51.1 & 82.6 & 50.6 & 72.4 & 52.3 & 58.0 & 51.4 & 65.1 & 75.5 & 2.29 & 2.88 & 52.9 \\ \cline{1-1}  & PaCE (Ours) & **93.7** & **97.9** & **97.7** & **94.9** & **98.9** & **96.6** & **99.3** & **90.8** & **98.9** & 7.52 & 2.85 & 54.1 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in **bold** and the second best is underlined.

Figure 5: An example of jailbreaking LLaMA2-7B-Chat and detoxification by PaCE. PaCE successfully detoxifies the response while maintaining the instruction-following capability.

Figure 6: The detoxification performances for LLaMA2-13B w.r.t. the dictionary size.

### Improving Faithfulness and Removing Negative Sentiment

We evaluate the framework based on the response's faithfulness and sentiment when input prompts requests for information involving biographical facts or minority social groups. Faithfulness reflects the level of factuality in the generation, and sentiment describes the emotional tone behind the generation. In short, we find PaCE effective in improving the faithfulness and removing negative sentiment in LLMs' outputs. We describe the setup, metrics and method below.

**Setup.**_Faithfulness_: We use the FactScore suite and the fact evaluator for faithful biography generation [51]. The suite is divided into labeled and unlabeled subsets used in different sections of the original paper. Our table reports the Labeled Score (LS), the total number of Labeled Atomic Facts (LAF), the Unlabeled Score (US), and the total number of unlabeled Atomic Facts (LAF). _Sentiment_: We use the HolisticBias suite [66] and hate speech evaluator [64] to measure the sentiment of the response to underrepresented descriptors. The reported numbers are the average of non-negative sentiment scores for underrepresented groups categorized by Gender (GN), Occupation (OC), and Nationality (NT). During the sentiment revising, the concept setups for all approaches follow the detoxification setup. For the faithfulness experiments, PaCE removes the top 50 undesirable (hallucinatory) concepts ranked by the partitioner. The Prompting approach instructs the LLM not to output sentences relevant to these top concepts. The VecAdd and OrthoProj operate on the concept vector of 'fabrication'.

**Results.** Our results are shown in Table 4. For both 7B and 13B models, PaCE achieves more factual responses and improves the sentiment according to most metrics. For linguistic performance, our method ranks right after the Vanilla method for the larger 13B model, and achieves comparable

\begin{table}
\begin{tabular}{l c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{\begin{tabular}{c} Time per \\ Respons \\ \end{tabular} } & \multicolumn{2}{c}{\begin{tabular}{c} Time per \\ Token \\ \end{tabular} } & \multicolumn{2}{c}{\begin{tabular}{c} Time per \\ Response \\ \end{tabular} } \\ \hline Vanilla & 12.4 & 0.041 & 20.7 & 0.076 \\ VecAdd & 16.3 & 0.062 & 29.1 & 0.109 \\ OrthoProj & 143.7 & 0.514 & 221.6 & 0.780 \\ PaCE (Ours) & 44.8 & 0.119 & 50.3 & 0.149 \\ \hline \hline \end{tabular} \begin{tabular}{c} \begin{tabular}{c} Safety \\ (\%, \(\uparrow\)) \\ \end{tabular} & 
\begin{tabular}{c} Fluency \\ (\(\uparrow\)) \\ \end{tabular} \\ \hline PaCE (LLaMA2-7B-Chat) & 50.2 & 7.26 \\ + Decomposition on \(10^{4}\) Concepts & 57.6 & 7.58 \\ + Clustering of Concepts & 62.3 & 7.63 \\ + Concept Partitioner & 65.1 & 7.70 \\ + Removal of Top \(50\) Concepts & 76.5 & 8.07 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Computation time (in seconds) evaluation Table 3: Ablation study for PaCE on the for PaCE and representation manipulation baselines. detoxifying LLaMA2-7B. Starting from a reported numbers are the average of non-negative sentiment scores for underrepresented groups categorized by Gender (GN), Occupation (OC), and Nationality (NT). During the sentiment revising, the concept setups for all approaches follow the detoxification setup. For the faithfulness experiments, PaCE removes the top 50 undesirable (hallucinatory) concepts ranked by the partitioner. The Prompting approach instructs the LLM not to output sentences relevant to these top concepts. The VecAdd and OrthoProj operate on the concept vector of ‘fabrication’.

Figure 7: Ablation study for solvers. We observe that greedy solvers can improve computational speed at the cost of safety performance.

results for LLaMA2-7B. Overall, we argue PaCE is an effective method for improving faithfulness and sentiment revising.

### Representation Space Sampled by PaCE-1M

Our collected dataset of conceptual representations enables us to investigate the geometry and potential applications of the representation (activation) space.

**Concept Clustering and Retrieval.** Here we explore the semantic structure of the activation space of the LLaMA2-13B-Chat by visualizing the first 10,000 concepts from the PaCE-1M dataset. We apply a dimensionality reduction method UMAP [47] on the concept vectors and visualize the first two dimensions in Figure 8. Concept vectors with similar semantics appear to be close to each other: e.g., in Figure 8 (1), concepts such as 'college', 'university', 'Academy', and 'Institute' are related to Education and they are close in the UMAP space. Notably, concepts of different semantics are clearly separated: those related to Education, Countries/States, Cities, Food and Clothing, and Positive Emotions respectively form distinct clusters. In particular, while concepts relevant to geography are closely clustered in Figure 8 (2), we observe a clear boundary between concepts related to Countries/States and those to Cities. These semantic structures indicate that the activation space sampled by our PaCE-1M dataset can capture and organize semantic information of the concepts, enabling further analysis and manipulations in PaCE. Figure 10 further reports the concept retrieval by evaluating the distance between a target concept with other concept vectors in the activation space. We observe organizational structure from the concept clusters based on their semantics. For instance, vectors for the concept 'affection' and 'friendship', are geometrically close and semantically relevant to the concept 'love.' Zooming out, such semantic structures are observed throughout the activation spaces of LLaMA2, and we conjecture they generalize to those in other LLMs. We provide more details of clustering and retrieval in Appendix C.2 and Appendix C.3.

## 5 Discussion

We provide discussions on the monosemanticity of concepts and connections among different alignment paradigms in this section. We also argue how PaCE handles context-dependent concepts.

### Polysemy of Words

While VecAdd and OrthoProj may be affected by the polysemy of words, PaCE's overcomplete dictionary allows accurate analysis of the target representation through sparse decomposition. Table 1 and Table 4 show that PaCE outperforms OrthoProj and VecAdd on linguistic metrics. We attribute the high helpfulness of PaCE to the large-scale dictionary with sparse coding, explained as follows.

**Comprehensive Coverage.** Since the dictionary is large, concepts with single and clear semantics are involved. E.g., if the stimuli of 'kill' may have different meanings, there exist other more polarized concept vectors such as'murder' (more harmful) and'spend' (more benign).

**Parsimony of Solution.** Sparse coding aims to choose the fewest concepts to reconstruct the latent representation (i.e., parsimony). For the sake of argument, assuming the sentence is about 'killing time' and the vanilla LLM has the correct semantic understanding of its benignness, the latent representation of the whole sentence will be closer to concepts such as'spend' and 'time' rather than string-matching to 'kill' (which in your setup could have mixed harmful and benign senses). As the sparse coding of the target representation promotes the parsimonious selection of concepts with monosemantics, it helps to represent benign contexts correctly without assigning significant weights to ambiguous terms like 'kill'.

### Different Alignment Paradigms

As mentioned in SS1, beyond activation engineering, there are other alignment paradigms such as Supervised Fine-Tuning (SFT) [26], Reinforcement Learning from Human Feedback (RLHF) [55], and Knowledge Engineering (KE) [14; 74]. We clarify the main advantages of PaCE over them.

**Training-Free.** RLHF, SFT, and KE all need to tune the parameters of LLM, which potentially degrade the well-structured priors of the pre-trained LLM. Taking a step back, even if LoRA is adopted for these paradigms, the training/tuning incurs significant computation and memory costs. PaCE does not modify the parameters of LLM and requires no training. It better preserves the priors of LLM, provides a low-resource alignment solution, and retains the general linguistic capabilities.

**Interpretable and Adaptive.** The solved coefficients of PaCE are an accurate interpretation of how a user input's representation is composed in the concept space. Also, when a new alignment goal is set, RLHF, SFT, and KE need to collect sufficient task samples and tune the LLM on the new dataset. In contrast, PaCE just needs to run the concept partitioner through PaCE-1M, which is expected to be much faster and more convenient.

### Context-dependent Concepts

The structured activation space of LLMs and the large-scale concept dictionary of PaCE help to handle the influence of the context for a concept in the target prompt. As the LLM scales up, its capability to capture and utilize contextual information grows with the help of attention modules. The activation space, as already used for many representation manipulation methods, is expected to convey the underlying semantic information of concepts in the sentence (context). That is, the space hosting concept vectors is not collapsed, and it is structured to distinguish different concepts. The representation (activation) to be steered at inference time encodes the context and conveys the semantics of a concept based on the context. Then, since our overcomplete concept dictionary in PaCE widely covers concepts of various categories, the sparse coding on this dictionary will effectively analyze the target representation as the linear combination of these concepts.

## 6 Conclusion

In this paper, we present PaCE, an activation engineering framework designed for aligning LLMs by effectively and efficiently addressing undesirable representations while retaining linguistic capabilities. By constructing a large-scale concept dictionary and leveraging sparse coding for activation decomposition, PaCE opens up new research avenues for training-free LLM alignment. Our experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising demonstrate that PaCE achieves state-of-the-art performance compared to existing representation manipulation approaches. PaCE not only ensures alignment with less cost but also adapts to evolving alignment goals without significantly compromising the LLM's linguistic proficiency. We open-source the PaCE-1M dataset to facilitate future research and practical applications of LLM alignment, and will release the source code soon. We further elaborate on the potential limitations, societal impacts, and future works of PaCE in Appendix B.7.

## Acknowledgments and Disclosure of Funding

This research was supported by ARO MURI W911NF-17-1-0304, DARPA GARD HR001119S0026, DARPA RED HR00112090132, ODNI IARPA HIATUS #2022-22072200005, the NSF grant 2031985, Simons Foundation MoDL 135615, a gift from AWS AI to Penn Engineering's ASSET Center for Trustworthy AI, and NSF Graduate Research Fellowship #DGE2139757. We would like to thank Liangzu Peng, Hancheng Min, Bowen Li, Xinyu Yang, and Fengrui Tian for their suggestions in the presentation and experiments. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, NSF, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.

## References

* [1] Carl Allen and Timothy Hospedales. Analogies Explained: Towards Understanding Word Embeddings. _arXiv preprint arXiv:1901.09813_, 2019.
* [2] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear Algebraic Structure of Word Senses, with Applications to Polysemy. In _TACL_, 2018.
* [3] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A Latent Variable Model Approach to PMI-based Word Embeddings. _arXiv preprint arXiv:1502.03520_, 2019.
* [4] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with python: analyzing text with the natural language toolkit. _" O'Reilly Media, Inc."_, 2009.
* [5] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. In _NeurIPS_, 2016.
* [6] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering Latent Knowledge in Language Models Without Supervision. _arXiv preprint arXiv:2212.03827_, 2024.
* [7] T. Tony Cai and Lie Wang. Orthogonal matching pursuit for sparse signal recovery with noise. In _IEEE Transactions on Information Theory_, 2011.
* [8] Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloff. Self-conditioning pre-trained language models. In _ICML_, 2022.
* [9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In _NeurIPS_, 2023.
* [10] Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language models ask better questions for themselves. _arXiv preprint arXiv:2311.04205_, 2023.
* [11] Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, and Benjamin D Haeffele. Unsupervised manifold linearizing and clustering. In _ICCV_, 2023.
* [12] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models: An algorithmic survey. _arXiv preprint arXiv:2312.00678_, 2024.
* [13] Michael Elad. _Sparse and redundant representations: from theory to applications in signal and image processing_. Springer, 2010.
* [14] Ronen Eldan and Mark Russinovich. Who's harry potter? approximate unlearning in lms. _arXiv preprint arXiv:2310.02238_, 2023.
* [15] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy Models of Superposition. _arXiv preprint arXiv:2209.10652_, 2022.
* [16] Ehsan Elhamifar and Rene Vidal. Block-sparse recovery via convex optimization. In _IEEE Transactions on Signal Processing_, 2012.

* [17] Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. Towards Understanding Linear Word Analogies. _arXiv preprint arXiv:1810.04882_, 2019.
* [18] W. Nelson Francis and Henry Kucera. Computational analysis of present-day american english. Brown University Press, 1967.
* [19] Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: A survey. _arXiv preprint arXiv:2309.00770_, 2023.
* [20] Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman. Finding alignments between interpretable causal variables and distributed neural representations. In _Proceedings of the Third Conference on Causal Learning and Reasoning_, 2024.
* Zipf + Uniform = Vector Additivity. In _ACL_, 2017.
* [22] Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramer, and Milad Nasr. Query-based adversarial prompt generation. _arXiv preprint arXiv:2402.12329_, 2024.
* [23] John Hewitt, John Thickstun, Christopher D. Manning, and Percy Liang. Backpack language models. In _ACL_, 2023.
* [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.
* [25] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Jake Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. _arXiv preprint arXiv:2306.03901_, 2023.
* [26] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _ICLR_, 2022.
* [27] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. _arXiv preprint arXiv:2112.09118_, 2021.
* [28] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, and Wen Gao. Ai alignment: A comprehensive survey. _arXiv preprint arXiv:2310.19852_, 2024.
* [29] Yibo Jiang, Bryon Aragam, and Victor Veitch. Uncovering Meanings of Embeddings via Partial Orthogonality. _arXiv preprint arXiv:2310.17611_, 2023.
* [30] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch. On the Origins of Linear Representations in Large Language Models. _arXiv preprint arXiv:2403.03867_, 2024.
* [31] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with GPUs. In _IEEE Transactions on Big Data_, 2019.
* [32] Nikhil Kandpal, Matthew Jagielski, Florian Tramer, and Nicholas Carlini. Backdoor attacks for in-context learning with language models. _arXiv preprint arXiv:2307.14692_, 2023.
* [33] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In _ICCV_, 2023.
* [34] Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations. In _CNLL_, 2014.
* [35] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In _NeurIPS_, 2020.
* [36] Jose Lezama, Qiang Qiu, Pablo Muse, and Guillermo Sapiro. Ole: Orthogonal low-rank embedding-a plug and play geometric loss for deep learning. In _CVPR_, 2018.
* [37] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In _ICLR_, 2023.

* [38] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. _arXiv preprint arXiv:2404.02060_, 2024.
* [39] Zonglin Li, Chong You, Srinadh Bhojanapalli, Dialing Li, Ankit Singh Rawat, Sashank J. Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar. The lazy neuron phenomenon: On emergence of activation sparsity in transformers. In _ICLR_, 2023.
* [40] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. _arXiv preprint arXiv:2305.13655_, 2023.
* [41] Renjie Liao, Alex Schwing, Richard Zemel, and Raquel Urtasun. Learning deep parsimonious representations. _NeurIPS_, 29, 2016.
* [42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [43] Sheng Liu, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. _arXiv preprint arXiv:2311.06668_, 2023.
* [44] Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, and Rene Vidal. Knowledge pursuit prompting for zero-shot multimodal synthesis. _arXiv preprint arXiv:2311.17898_, 2023.
* [45] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In _CVPR_, 2023.
* [46] Samuel Marks and Max Tegmark. The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets. _arXiv preprint arXiv:2310.06824_, 2023.
* [47] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. _arXiv preprint arXiv:1802.03426_, 2020.
* [48] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. _arXiv preprint arXiv:1301.3781_, 2013.
* [49] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. _arXiv preprint arXiv:1310.4546_, 2013.
* [50] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In _NAACL HLT_, 2013.
* [51] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In _EMNLP_, 2023.
* [52] Masahiro Naito, Sho Yokoi, Geewook Kim, and Hidetoshi Shimodaira. Revisiting Additive Compositionality: AND, OR and NOT Operations with Word Embeddings. _arXiv preprint arXiv:2105.08585_, 2022.
* [53] Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent Linear Representations in World Models of Self-Supervised Sequence Models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, _ACL BlackboxNLP Workshop_, 2023.
* [54] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [55] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* [56] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. In _EMNLP_, 2023.
* [57] Kiho Park, Yo Joong Choe, and Victor Veitch. The Linear Representation Hypothesis and the Geometry of Large Language Models. _arXiv preprint arXiv:2311.03658_, 2023.
* [58] Y. C. Pati, Ramin Rezaiiifar, and P. S. Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. 1993.

* [59] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In _EMNLP_, 2014.
* [60] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2021.
* [61] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. A systematic survey of prompt engineering in large language models: Techniques and applications. _arXiv preprint arXiv:2402.07927_, 2024.
* [62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [63] Noam Shazeer, Ryan Doherty, Colin Evans, and Chris Waterson. Swivel: Improving Embeddings by Noticing What's Missing. _arXiv preprint arXiv:1602.02215_, 2016.
* [64] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. In _EMNLP-IJCNLP_, 2019.
* [65] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models. _arXiv preprint arXiv: 2301.12652_, 2023.
* [66] Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. "I'm sorry to hear that": Finding new biases in language models with a holistic descriptor dataset. In _EMNLP_, 2022.
* [67] Nishant Subramani, Nivedita Suresh, and Matthew Peters. Extracting Latent Steering Vectors from Pretrained Language Models. In _ACL Findings_, 2022.
* [68] Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear Representations of Sentiment in Large Language Models. _arXiv preprint arXiv:2310.15154_, 2023.
* [69] Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. Function vectors in large language models. In _ICLR_, 2024.
* [70] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. _arXiv preprint arXiv:2307.09288_, 2023.
* [71] Matthew Trager, Pramuditha Perera, Luca Zancato, Alessandro Achille, Parminder Bhatia, and Stefano Soatto. Linear spaces of meanings: compositional structures in vision-language models. In _ICCV_, 2023.
* [72] Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. _arXiv preprint arXiv:2308.10248v3_, 2023.
* [73] Rene Vidal, Yi Ma, and Shankar Sastry. _Generalized Principal Component Analysis_. Interdisciplinary Applied Mathematics. Springer New York, 2016.
* [74] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifing large language models via knowledge editing. _arXiv preprint arXiv:2403.14472_, 2024.
* [75] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. _arXiv preprint arXiv:2307.12966_, 2023.
* [76] Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for (score-based) text-controlled generative models. In _NeurIPS_, 2023.

* [77] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. In _TMLR_, 2022.
* [78] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022.
* [79] John Wright and Yi Ma. _High-dimensional data analysis with low-dimensional models: Principles, computation, and applications_. Cambridge University Press, 2022.
* [80] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In _ICLR_, 2024.
* [81] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In _NeurIPS_, 2023.
* [82] Chong You, Chun Guang Li, Daniel P Robinson, and Rene Vidal. Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering. In _CVPR_, 2016.
* [83] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin Haeffele, and Yi Ma. White-box transformers via sparse rate reduction. _NeurIPS_, 2024.
* [84] Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann LeCun. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. _arXiv preprint arXiv:2103.15949_, 2023.
* [85] Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex R. Dalal, Jennifer L. Kim, Michael Moor, Kevin Alexander, Euan Ashley, Jack Boyd, Kathleen Boyd, Karen Hirsch, Curt Langlotz, Joanna Nelson, and William Hiesinger. Almanac: Retrieval-augmented language models for clinical medicine. _arXiv preprint arXiv:2303.01229_, 2023.
* [86] Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and Xin Wang. Controllable text-to-image generation with gpt-4. _arXiv preprint arXiv:2305.18583_, 2023.
* [87] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the ai ocean: A survey on hallucination in large language models. _arXiv preprint arXiv:2309.01219_, 2023.
* [88] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, Zico Kolter, and Dan Hendrycks. Representation engineering: A top-down approach to ai transparency. _arXiv preprint arXiv:2310.01405_, 2023.
* [89] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

**Supplementary Material**

## Appendix A Structure of The Appendix

The appendix is structured as follows:

Appendix B describes details of our PaCE framework, including proofs of propositions and a comprehensive explanation of the framework's algorithm.

Appendix C elaborates on the PaCE-1M dataset, demonstrating the structure of the dataset with explorations of subspace clustering to analyze the dataset.

Appendix D presents textual results, including visualizations of baseline comparisons and samples of concept clusters.

Appendix E shows the instruction templates used for GPT-4 to synthesize and partition concepts.

## Appendix B Details of PaCE Framework

This section validates the propositions of the PaCE framework discussed in SS3.3, followed by descriptions of how to extract representations and the algorithm of the whole procedures of PaCE.

### Proofs of Oblique Projection Recovers Vector Addition and Orthogonal Projection

**Proposition 1**.: _Let \(\bm{D}\in\mathbb{R}^{d\times n}\) be a dictionary matrix and \(\bm{z}\in\mathbb{R}^{d}\) a latent code. Then, any solution \(\bm{c}^{*}\) of the optimization problem_

\[\min_{\bm{c}}\lVert\bm{z}-\bm{D}\bm{c}\rVert_{2}^{2}\] (3)

_satisfies \(\bm{D}\bm{c}^{*}=\Pi_{\mathrm{range}(\bm{D})}\bm{z}\). Therefore, the map \(\bm{z}\mapsto\bm{z}-\bm{D}\bm{c}^{*}(\bm{z})\) is the same as \(\bm{z}\mapsto\bm{z}-\Pi_{\mathrm{range}(\bm{D})}\bm{z}=\bm{z}\mapsto\Pi_{ \mathrm{range}(\bm{D})^{\perp}\bm{z}}\) in (OrthoProj)._

Proof.: Note that \(\bm{I}=\Pi_{\mathrm{range}(\bm{D})}+\Pi_{\mathrm{range}(\bm{D})^{\perp}}\). Therefore, the objective of (3) can be written as

\[\lVert\bm{z}-\bm{D}\bm{c}\rVert_{2}^{2} =\left\lVert\Pi_{\mathrm{range}(\bm{D})^{\perp}}\bm{z}+\Pi_{ \mathrm{range}(\bm{D})}\bm{z}-\bm{D}\bm{c}\right\rVert_{2}^{2}\] \[=\left\lVert\Pi_{\mathrm{range}(\bm{D})^{\perp}}\bm{z}\right\rVert _{2}^{2}+\left\lVert\Pi_{\mathrm{range}(\bm{D})}\bm{z}-\bm{D}\bm{c}\right\rVert _{2}^{2}+2\langle\Pi_{\mathrm{range}(\bm{D})^{\perp}}\bm{z},\Pi_{\mathrm{range} (\bm{D})}\bm{z}-\bm{D}\bm{c}\rangle,\]

where \(\langle\cdot,\cdot\rangle\) is the Euclidean inner product of \(\mathbb{R}^{d}\). The first term is constant with respect to \(\bm{c}\), so it can be omitted. Further, since any ortho-projector (in particular \(\Pi_{\mathrm{range}(\bm{D})^{\perp}}\)) is self-adjoint, we have

\[\langle\Pi_{\mathrm{range}(\bm{D})^{\perp}}\bm{z},\Pi_{\mathrm{range}(\bm{D})} \bm{z}-\bm{D}\bm{c}\rangle=\langle\bm{z},\Pi_{\mathrm{range}(\bm{D})^{\perp}} \big{(}\Pi_{\mathrm{range}(\bm{D})}\bm{z}-\bm{D}\bm{c}\big{)}\rangle=0.\]

Therefore, problem (3) is equivalent to optimizing

\[\left\lVert\Pi_{\mathrm{range}(\bm{D})}\bm{z}-\bm{D}\bm{c}\right\rVert_{2}^{2},\]

which is lower bounded by \(0\). This lower bound is realizable since \(\Pi_{\mathrm{range}(\bm{D})}\bm{z}\in\mathrm{range}(\bm{D})\). Thus, any minimizer \(\bm{c}^{*}\) must realize this lower bound, meaning \(\Pi_{\mathrm{range}(\bm{D})}\bm{z}=\bm{D}\bm{c}\). So we are done. 

**Proposition 2**.: _Let \(\bm{D}\) contain only one concept direction \(\bm{v}\in\mathbb{R}^{d}\). Let \(\bm{z}\in\mathbb{R}^{d}\) be a latent code, and \(\lambda>-1\) a regularization strength. Then, the solution \(c^{*}\in\mathbb{R}\) of the optimization problem_

\[\min_{\bm{c}}\lVert\bm{z}-\bm{D}\bm{c}\rVert_{2}^{2}+\lambda\lVert\bm{c} \rVert_{2}^{2}\ \Leftrightarrow\ \min_{c}\lVert\bm{z}-c\bm{v}\rVert_{2}^{2}+\lambda c^{2}\] (4)

_is given by \(c^{*}=\frac{\langle\bm{z},\bm{v}\rangle}{\lambda+1}\). Therefore, the map \(\bm{z}\mapsto\bm{z}-\bm{D}\bm{c}^{*}(\bm{z})\) recovers (VecAdd): the former is the same as \(\bm{z}\mapsto\bm{z}-\eta_{\lambda}\bm{v}_{+}\), where one can set any \(\eta_{\lambda}>0\) by properly choosing \(\lambda>-1\), and \(\bm{v}_{+}\) is defined as \(\bm{v}\) if \(\langle\bm{v},\bm{z}\rangle>0\) and \(-\bm{v}\) otherwise._

Proof.: Note that the objective of (4) is simply a univariate quadratic function of \(c\):

\[\lVert\bm{z}\rVert_{2}^{2}-2\langle\bm{z},\bm{v}\rangle c+(\lambda+1)c^{2}.\]This has a unique minimizer \(c^{*}=\frac{\langle\bm{z},\bm{v}\rangle}{\lambda+1}\) since \(\lambda+1>0\) by assumption. To prove the second part of the proposition, note that

\[\bm{z}-\bm{D}\bm{c}^{*}(\bm{z})=\bm{z}-c^{*}(\bm{z})\bm{v}=\bm{z}-\frac{\langle \bm{z},\bm{v}\rangle}{\lambda+1}\bm{v}=\bm{z}-\frac{|\langle\bm{z},\bm{v} \rangle|}{\lambda+1}\cdot(\bm{v}\operatorname{sign}(\langle\bm{z},\bm{v} \rangle)).\] (5)

Define \(\eta_{\lambda}:=\frac{|\langle\bm{z},\bm{v}\rangle|}{\lambda+1}\) and \(\bm{v}_{+}:=\bm{v}\operatorname{sign}(\langle\bm{z},\bm{v}\rangle)\). One can see that by varying \(\lambda\in(-1,+\infty)\), \(\eta_{\lambda}\) can take any value in \((0,\infty)\). This concludes the proof. 

### Extracting Concept Directions and Constructing Dictionary

Recall from SS3.2 that for each concept \(t_{i}\), we have collected a set of context stimuli (i.e., sentences that describe \(t_{i}\)) \(s_{i}=\{s_{i}^{0},s_{i}^{1},\cdots,s_{i}^{N_{*}}\}\). This totals \(40,000\) concepts and more than \(1,200,000\) context stimuli.

To obtain a vector for each concept, we follow the _representation reading_ algorithm [88] to map the concept to the hidden states of LLM decoder layers. We describe the algorithm here for completeness. Each context sentence \(s_{i}^{j}\) together with the concept \(t_{i}\) is first plugged into a pre-defined prompt template, producing \(\bar{s}_{i}^{j}\).

```  Consider the <concept \(t_{i}\)> in the following scenario:  Scenario: <stimulus \(s_{j}^{j}\)>  Answer: ```

**Algorithm 3**Parsimonious Concept Engineering (PaCE)

For any prompt \(p\), denote by \(f^{\ell}(p)\) the activation of the last token at the \(l\)-th layer of the LLM when the input is \(p\). Then, to extract a vector for concept \(t_{i}\), one looks at the activations of pairs of stimuli

\[X_{i}^{\ell}:=\Big{\{}\Pi_{\mathbb{S}^{d-1}}\Big{(}f^{\ell}(\bar{s}_{i}^{j})- f^{\ell}(\bar{s}_{i^{\prime}}^{j^{\prime}})\Big{)}:\forall i^{\prime}\neq i,\quad\forall j,j^{\prime}\Big{\}},\] (6)

where \(\Pi_{\mathbb{S}^{d-1}}(\cdot)\) is the projection onto the unit sphere, used to normalize the difference vectors. In practice, the work [88] uses a downsampled subset of \(X_{i}^{\ell}\) rather than the entire \(X_{i}^{\ell}\). We obtain the direction \(\bm{v}_{i}^{\ell}\) of concept \(i\) at layer \(\ell\) by applying PCA on the set \(X_{i}^{\ell}\), and taking the first principal direction; note that \(\big{\|}\bm{v}_{i}^{\ell}\big{\|}_{2}=1\). Then, we construct the dictionary \(\bm{D}^{\ell}=[\bm{v}_{1}^{\ell},\dots,\bm{v}_{n}^{\ell}]\in\mathbb{R}^{d \times n}\) of layer \(\ell\), and doing this for all layers gives \(\{\bm{D}^{\ell}\}_{\ell=1}^{L}\) as used in Algorithm 2.

### Full Procedure of PaCE

Algorithm 3 shows the full procedure of PaCE from textual prompt suites to reoriented LLM responses towards the desired behavior.

**Input:** Pre-trained LLM with \(L\) decoder layers (DCL) to decompose, input prompt suit \(P\)

For each concept \(t_{i}\in T:\)\(\triangleright\) SS3.2: Concept Dictionary Extraction (Done Once)

Instrued knowledge-driven GPT to generate context stimuli \(s_{i}=\{s_{i}^{1},\cdots,s_{i}^{N_{*}}\}\)

 Extract the concept vector \(\bm{v}_{i}=\operatorname{RepReading}(t_{i},s_{i})\)\(\triangleright\) Appendix B.2

Construct the concept dictionaries \(\{\bm{D}^{\ell}\}_{\ell=1}^{L}\) from concept vectors \(\{\bm{v}\}_{i=1}^{N_{*}}\).

For each concept \(t_{i}\in T:\)\(\triangleright\) SS3.2: Concept Ranking (Per Task)

Instruct the concept partitioner to give a partition score \(\operatorname{Partitioner}(t_{i})\) for the task

Take the index of top-scored concepts from the partition of undesirable concepts as the index set \(I\)

For each input prompt \(p_{i}\in P\):\(\triangleright\) SS3.3: Activation Intervention (Per Prompt)

 Embed the prompt \(p_{i}\) to the token space \(\bm{E}_{i}\)

 For each next token \(j\) to generated: \(\bm{e}_{i}^{j}=\operatorname{Algorithm2}(\bm{E}_{i})\)\(\triangleright\) Intervention by ObliqProj

 Append the generated token \(\bm{e}_{i}^{j}\) to \(\bm{E}_{i}\)

 Map the final embedding \(\bm{E}_{i}\) to response \(r_{i}\).

**Output:** The response suite \(R=\{r_{1},r_{2},\cdots,r_{N_{*}}\}\).

[MISSING_PAGE_FAIL:18]

Safety score and response time increase as the dictionary size increases. We observe that the safety performance does not increase too much after the dictionary size changes from 9000 to 10000. This validates our experiment choice of the dictionary size in this interval.

Figure 11 shows that the regularization with \(\tau=0.95\) yields the best safety performance among the five choices. Pure ridge regression (\(\tau=0\)) and pure lasso regression (\(\tau=1\)) do not perform as well as the mixed regularization strategy.

### Providing Dictionary and Scores to Target LLM

For clarity, we elaborate on more details of how PaCE uses the concept dictionary. In our paper, dictionaries are a collection of concept vectors and are frozen for representation decomposition. First, the LLM takes an input prompt (e.g., malicious requests). Then an activation engineering framework [72, 88] extracts the activations at each decoder block of the transformer. Such extraction results in a vector corresponding to the input prompt, which can then be modified for steering in different ways. For PaCE, the steering has two main stages. Stage 1 pre-computes the large-scale concept dictionary offline and the partition (i.e., scores) of which concepts represent benign/harmful concepts. Stage 2 extracts the representation of an input prompt at inference time and uses sparse coding to decompose this as the linear combination of atoms in our frozen dictionary. We then modify this linear combination by removing undesirable components and proceeding with inference in the LLM with the detoxified representation.

### Limitations, Societal Impacts, and Future Works

While our framework shows promising results, there exist potential limitations and several directions worth further exploration to address them.

**Parsimonious Concept Representation.** In this paper, we follow the current practice (SS2.2) to represent a concept by a single vector. Nonetheless, several alternatives could be explored. Results on linear polysemy [2, 15, 84] suggest that a concept might be better represented by multiple vectors or low-dimensional linear subspaces, each corresponding to different semantic meanings. A concept vector may also be sparse, i.e., having a few non-zero entries: the work of [8, 20] identifies some expert neurons in LLMs associated with each concept, and the authors of [39] observe that some layer in a transformer block manifests very sparse activation across all depth levels of various transformer architectures for different tasks. Inspired by how parsimonious structures can be used to accelerate the inference of LLMs [12], controlling the LLMs could also be made faster.

**Controlling Generative Models.** The principles behind latent space control via oblique projection could be adapted to other generative models, such as score-based diffusion models for images [24, 60] or videos [33, 45], and visual language models [9, 42]. Recent literature [76] combines orthogonal projection and vector addition in the diffusion score space to achieve controlled generation, suggesting potential for cross-modal applications of our approach. Finally, the work of [11, 36, 83] aims to learn encoders that, by design, promote the activations to lie in a union of low-dimensional subspaces, and applying our framework for controlled generation would be of interest.

We acknowledge the societal impacts of our approach. The jailbreak prompts could be offensive to certain readers, LLM responses may still inherit biases present in the pre-extracted concept dictionaries, and automatic concept partitioning could unintentionally result in contentious annotations that are misunderstood across different cultures. Further research into context-aware online concept partitioning and more diverse dataset collection could enhance the inclusivity of PaCE.

## Appendix C Details of PaCE-1M Dataset

This section shows more details on the collected concept representation dataset PaCE-1M, and explores subspace clustering on the sampled representation space. We provide the full dataset at https://github.com/peterljq/Parsimonious-Concept-Engineering with instructions on how to read the dataset.

Figure 11: Ablation study for the regularization \(\tau\).

### Stimulus Visualization

Recall that given a concept, a concept stimulus aims to capture the general semantics of the concept under different contexts. In other words, it provides different interpretation of the same concept. Figure 12 shows extensive examples of the curated concepts and their corresponding concept stimuli in our PaCE-1M dataset.

### Subspace Clustering on Concept Vectors

In this visualization, we aim to reveal the structures of the concept vectors by applying an algorithm called _subspace clustering_, which can be used to find clusters when the data lie close to a union of linear subspaces. Here we describe the setup and results of subspace clustering on the concepts vectors extracted on LLaMA-2-13b model for simplicity, but the same can be done for other sized models.

**Data.** Recall that we are using a subset of size \(10,000\) of all the concept vectors. Since we use the activation space of \(19\) layers, each of dimension \(5120\), each concept \(t_{i}\) maps to a vector \(\bm{v}_{i}^{\text{all}}:=[\bm{v}_{i}^{\top},\dots,\bm{v}_{i}^{19\top}]^{\top} \in\mathbb{R}^{19\cdot 5120}\). Since this is high dimensional, it is standard to apply linear dimensionality reduction to the concept vectors. Specifically, we perform Singular Value Decomposition (SVD) on the \(10,000\) vectors, and retained the first \(\hat{d}\) principal components such that \(95\%\) of the energy was retained. That is, \(\hat{d}\) equals to the smallest \(d^{\prime}\) such that

\[\frac{\sum_{i=d^{\prime}+1}^{19\times 5120}\sigma_{i}^{2}}{\sum_{i=1}^{19\times 5 120}\sigma_{i}^{2}}<0.95\]

holds, which results in \(\hat{d}=1712\). We observe that most projected vectors have their \(\ell^{2}\) norm close to \(19\). This is expected, since i) \(\left\lVert\bm{v}_{i}^{\text{eff}}\right\rVert_{2}=1\), so \(\left\lVert\bm{v}_{i}^{\text{all}}\right\rVert_{2}=19\), ii) the linear dimensionality reduction preserves most of the energy.

**Algorithm.** We apply Elastic Net Subspace Clustering (EnSC) [82] on the preprocessed vectors to obtain \(200\) clusters. The parameters of EnSC is set to \(\tau=1\) and \(\gamma=100\).

**Results.** Figure 15 shows the affinity matrix learned by EnSC on the concept directions. The rows and columns of the matrix are sorted by cluster assignment. Notably, it can be seen that the affinity exhibits a block-diagonal structure, suggesting a good clustering of the concept vectors; that is, the points from different clusters are separated, while points from the same cluster are close. The obtained clusters are visualized in Appendix D.2.

### Computing Pair-wise Similarity Among Concept Vectors

One of the motivations for this work is that concept vectors need not be orthogonal, therefore applying (OrthoProj) would remove extra concept vectors, harming the linguistic capability of LLMs (SS2.2).

We follow the same data pre-processing as in Appendix C.2 to obtain \(10,000\) dimensionality-reduced concept vectors in \(\mathbb{R}^{1712}\). We further normalize these vectors via a division by \(19\) so that each of them has its \(\ell^{2}\) close to \(1\) (see the discussion in Appendix C.2). The similarity between two processed concept vectors is simply defined as their inner product followed by the absolute value. This is a good approximation of cosine similarity, as the vectors have their \(\ell^{2}\) norm close to \(1\). Note that the cosine similarity is a better measure than Euclidean distance in this case, since in extracting the concept vectors (Appendix B.2), the principal directions have sign ambiguities.

## Appendix D Textual Results

This section presents the textual results generated using PaCE. It includes detailed detoxification comparisons with baseline models and analyses of the emergent clusters from the dataset.

### Baseline Responses and Additional Benchmark

Figure 13 shows the full response version of the Figure 5. Figure 14 shows an additional example of the jailbreaking and detoxification. We observe that PaCE outperforms in detoxification performance

[MISSING_PAGE_FAIL:21]

## Appendix A

Figure 12: Examples of concepts and their stimuli in the collected PaCE-1M dataset. Our broad collection of concepts enables PaCE to accurately decompose a task input and modify the representation towards desired behaviors.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Cluster ID** & **Topic** & **Concepts** \\ \hline \multirow{4}{*}{10} & \multirow{4}{*}{10} & \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & increasing, improvement, equipped, reform, \\  & & improving, strengthen, boost, shaping, \\  & Enhancement & gaining, modernization, strengthening, broadening, \\  & & supplementary, polish, fortified, intensification \\ \hline \multirow{4}{*}{14} & \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & look, seen, read, actual, sight, looks, seeing, observed, \\  & Observation / & vision, views, composed, visual, sees, visible, witness, \\  & Vision & spectacle, glimpse, sights, witnessed, Seeing, observing, \\  & & manifestations, viewing, observes, actuality, sighted, eyed \\ \hline \multirow{4}{*}{21} & \multirow{4}{*}{Expense} & cost, spent, rates, price, budget, spend, payment, expense, \\  & & bills, charges, expensive, spending, afford, waste, \\  & & fees, cheap, rent, commodities, overhead, costly, mileage, \\  & & discount, expenditure, incurred, spends, fare, calories \\ \hline \multirow{4}{*}{44} & \multirow{4}{*}{Name} & John, James, Mike, Jones, Richard, Joseph, Alfred, David, Charlie, Anne, Rachel, Linda, Kate, Paul, Susan, Andy, Harold, Dave, Johnny, Mrya, Shayne, Billy, Eileen, Arlene, Johine, Owen, \\  & & Alec, Theresa, Pete, Spencer, Elaine, Deegan, Bridget, Lilian \\  & & Keith, Allen, Pamela, Paula, Meredith, Andrei, Lizzie, Angie, \\  & & Nadine, Anthony, Claire, Jerry, Roger, Ryan, Katie, Juanita, \\  & & Eugenia, Daniel, Joan, Diane, Lester, Sally, Bryan, Garry, Joel, Chris, \\  & & Jimmy, Maria, Vince, Julie, Bernard, Larry, Wendell, Angelo, \\  & & Judy, Francesca, Jenny, Patricia, Nicholas, Anna, Aaron, Marcus, Nikita \\ \hline \multirow{4}{*}{81} & \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & system, program, data, programs, technical, electronic, \\  & & system, program, data, programs, technical, electronic, \\  & & model, engineering, Assembly, electronics, intelligent, \\  & & code, computed, mechanics, circuit, technological, \\  & & System & codes, generator, python, computer, functioning, terminal, \\  & & architecture, generated, bits, hardware, Autocoder, computing, \\  & & Technology, architectural, Engineering, generate, gadgets \\ \hline \multirow{4}{*}{97} & \multirow{4}{*}{Animal} & horse, cattle, dogs, snake, chicken, fish, bird, \\  & & snakes, herd, sheep, cats, bears, bees, lion, cows, \\  & & anaconda, flies, rabbit, elephants, poultry, oxen, mice, \\  & & Bears, Phoenix, duck, oysters, buffalo, turtle, deer, \\  & & bumblebees, elephant, antelope, lambs, pony \\ \hline \multirow{4}{*}{98} & \multirow{4}{*}{
\begin{tabular}{} \end{tabular} } & chief, king, captain, owner, Prince, colony, \\  & Royalty / & sovereign, royal, queen, kingdom, crown, ordinance, \\  & Leadership & empire, Imperial, crowned, lord, emperor, piston, \\  & & royalty, knight \\ \hline \multirow{4}{*}{107} & \multirow{4}{*}{Relationship} & family, friend, neighborhood, relative, neighbor, \\  & & brothers, Cousin, sister, partner, friendship, allies, \\ \cline{1-1}  & & neighboring, colleagues, relatives, mate, companion, \\ \cline{1-1}  & & partners, associates, sisters, buddy, brother, \\ \cline{1-1}  & & subordinates, colleague, peers, companions, twins \\ \hline \multirow{4}{*}{129} & \multirow{4}{*}{Food and Drinks} & food, dinner, coffee, wine, breakfast, drinking, liquor, \\ \cline{1-1}  & & lunch, beer, supper, eating, meals, cocktail, \\ \cline{1-1}  & & cook, wines, luncheon, whisky, drink, dish, diet, \\ \cline{1-1}  & & whiskey, candy, cake, champagne, cereal, \\ \cline{1-1}  & & alcohol, perfume, dinners, chocolate, Cologne, salad, \\ \cline{1-1}  & & cheese, steak, recipe, sandwich, dessert, Supper, brandy \\ \hline \multirow{4}{*}{197} & \multirow{4}{*}{Income} & income, wage, wages, salary, yield, profit, surplus, \\ \cline{1-1}  & & profits, wealth, revenue, earnings, compensation, \\ \cline{1-1}  & & earn, reward, proceeds, earning, waged, currency, salaries \\ \hline \hline \end{tabular}
\end{table}
Table 6: Sampled concept clusters in the representation space and their corresponding topics.

Figure 16: The zoom-in view of the sampled clusters in the representation (activation) space (Figure 8).

Figure 15: The affinity matrix learned by Elastic Net Subspace Clustering (EnSC) on the concept vectors, which gathers the concepts into \(200\) clusters. The rows and columns of the matrix are sorted by cluster assignment. Table 6 further shows samples of these concept clusters and their topics.

[MISSING_PAGE_EMPTY:27]

* [16] D. D. Kingma and J. Ba. Adam: "A fast method for stochastic optimization." _Journal of Machine Learning Research_, 2014.

[MISSING_PAGE_POST]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction describe the research scope, background, motivation, our approach, and the contributions in detail. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Appendix B.7, we discuss the current limitations, potential future directions, and societal impacts. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: In SS2, SS3, and Appendix B.1, we elaborate on our proof with justifications.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In SS4, Appendix B, and Appendix C, the implementation details, framework procedures, and the visualization of the dataset are discussed in detail. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our collected concept representation dataset at https://github.com/peterljq/Parsimonious-Concept-Engineering with instructions on how to read the dataset. Also, SS4 and Appendix C show the details of the dataset. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In SS4, Appendix B, and Appendix C, the implementation details and experiment procedures are well discussed. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments are executed at least five times with various seeds to take the mean value. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The Appendix B.4 describes our computation resources and Table 2 compares the computation time of our PaCE with other baselines. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We make sure that the actions in this submission are aligning with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The Appendix B.7 elaborates on societal impacts and potential future directions. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: Our experimented LLMs (LLaMA2-Chat and GPT-4) are all instruction-tuned for dialogue uses. When automatically collecting the PaCE-1M dataset, we develop post-processing modules to ensure the synthesis of GPT-4 does not conflict with our protocols. Also, note that the motivation of our paper is to promote the trustworthy use of LLMs. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: we have cited the open-source assets that we are using for PaCE. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

Answer: [Yes] Justification: In SS3.2 and Appendix C, we visualize the structure and templates of samples of our dataset. Also, we provide documentation of how to extract and read our dataset on the anonymous access site.

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

Answer: [NA] Justification: The research in this paper does not involve third-party volunteers as human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA] Justification: The research in this paper does not involve third-party volunteers as human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.