# Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents

Wenlong Huang\({}^{1}\), Fei Xia\({}^{2}\), Dhruv Shah\({}^{3}\), Danny Driess\({}^{2}\), Andy Zeng\({}^{2}\),

**Yao Lu\({}^{2}\), Pete Florence\({}^{2}\), Igor Mordatch\({}^{2}\), Sergey Levine\({}^{2,3}\), Karol Hausman\({}^{2}\), Brian Ichter\({}^{2}\)**

\({}^{1}\)Stanford University, \({}^{2}\)Google Deepmind, \({}^{3}\)UC Berkeley

grounded-decoding.github.io

Work done as an intern at Google.

###### Abstract

Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is _both_ likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models.

## 1 Introduction

Recent works have demonstrated robots that are increasingly proficient at understanding and acting upon natural language, whether through planning or conditioned policies. Complementing such progress, the field of natural language processing has recently seen large language models (LLMs) become ubiquitously used as pre-trained or few-shot prompted models, due to their impressive few-shot performance and vast knowledge-base. These LLMs have efficiently learned from web-scale data through autoregressively modeling the probability distribution over text tokens and thus generate text. However, the nature of this process is such that applying such models to embodied settings remains a challenge. They have not interacted with their environment, lack observability of non-language observation modalities (e.g., images), and may not know what is safe or possible for a particular embodiment.

Determining how to execute long-horizon behaviors based on high-level verbal commands is one particular area of robotics where the rich semantic knowledge in large language models can be especially useful. This problem combines elements of semantic reasoning and planning: the robotmust understand the instruction, determine the steps needed to fulfill it, and also determine how to sequence those steps appropriately given its capabilities and the current state of the environment. However, this is not a problem that can be solved purely with semantics, as it requires sufficient grounding to understand how the task should be performed _in context_ - for example, in the example in Figure 1, the language model alone has no way of knowing which block to pick up because this requires knowledge of which blocks are present, and also what manipulations the robot is capable of performing on them. Thus, although a language model can assign probabilities for how likely various steps are to correspond to the desired task _semantically_, the constraints of the planning problem must also enter into the process. These constraints could themselves be represented as probabilities that mirror the token probabilities generated by a language model, reflecting their applicability to the current environment rather than their semantic likelihood. We can frame this as a problem similar to probabilistic filtering: decode a sequence (i.e., a task description) that both has a high probability under the language model and a high probability under a grounded model that predicts how applicable this sequence is to the current scene.

Herein, we present **Grounded Decoding (GD)**, a scalable, general approach to planning with LLMs embodied domains. Grounded Decoding jointly decodes the token probability of an LLM and token probabilities from token-conditioned, robotic functions, such as affordance functions capturing the abilities of a robot given its embodiment, safety functions, or more. By guiding the LLM directly at its output, Grounded Decoding enables a general and flexible family of planning algorithms that combines LLM's strength of _long-horizon_ and _semantic_ reasoning and grounded models' strength of _local_ and _embodiment_ grounding.

Our contributions are as followed: 1) we present a robot-centric formulation for decoding language models to perform long-horizon robotic tasks with token-conditioned grounded models, 2) we demonstrate techniques for learning such grounded models, serving different purposes such as affordances and safety requirements, and 3) we show empirical evidence, across three simulation and real-world domains, that the proposed method performs strongly on a wide range of tasks while also significantly outperforming prior methods in efficiency.

## 2 Related Work

**Guided Decoding for Language Models.** Decoding strategies for large language models is an active area of research within natural language processing [77; 87; 25; 85; 38]. A number of recent works have focused on developing decoding heuristics for natural text generation [49; 35; 48; 18; 25; 6; 36]. Another line of works use external classifiers for maximizing certain language-space utilities when decoding language models [71; 92; 26; 39; 37; 21; 38; 4; 12; 23]. Most closely related to our work are classifier-guided decoding methods developed for offline domains, such as image captioning [78; 74] and task-oriented dialog [72; 83]. However, extensions to embodied domains, which we investigate exclusively in this work, remain non-trivial because grounding in embodied domains is bounded by the abilities of the agent and by environment state transition as the agent actively interacts with the environment.

Figure 1: Grounded Decoding solves robotic tasks by taking an instruction as input and selecting tokens that have high probability under a Large Language Model (LLM) and a set of Grounded Models (GM). Thus, it leverages the open-vocabulary and semantic knowledge of LLMs while being grounded in the environment and in the robot’s capabilities. Furthermore, the whole process does not require expensive fine-tuning of the LLM.

**Embodied and Multimodal Language Models.** Training language models to understand embodiment is an active area of research. Training multimodal models can enable some degree of embodied reasoning, such as understanding images and videos [9; 40; 76; 3]. Directly fine-tuning language models to output actions has also been investigated [75; 55; 66]. Lastly, training downstream models on language model embeddings shows promise [46; 52; 24; 94; 60; 41]. In this work, we investigate leveraging large frozen language models for embodied applications [29; 2; 96; 8; 64; 30; 42; 70; 47; 27; 58; 73; 44; 43; 14; 16; 82; 93; 89; 45; 56], with grounded models to provide domain-specific grounding during decoding process.

**Comparison to SayCan.** The most closely related work to our work is SayCan [2]. SayCan uses a large language model and a value function to select robotic skills among a constrained set of primitives. This constrained set of primitives enables SayCan to use the so-called "scoring-mode" of the LLM to get the probability of a skill being useful to a high-level instruction. This requirement to consider only a fixed and enumerated set of primitives limits the applicability of SayCan in scenarios with many possible skills, such as open vocabulary or combinatorial tasks. Grounded Decoding on the other hand jointly decodes the LLM and the grounded model at the token level, allowing for expressive decoding with an open vocabulary. Furthermore, SayCan considers only grounding functions derived from RL-trained value functions for affordance grounding functions, while Grounded Decoding explores many types of grounding functions to propose a broad family of algorithms.

**Task and Motion Planning.** Task and motion planning [33] seeks to solve high-level instructions via sequencing tasks in dynamically feasible manner. Research within this area generally focuses on symbolic planning [19] or optimization-based [79] approaches. Machine learning has increasingly been used to accelerate planning and enable new domains [91; 61; 69; 20; 17; 28; 90; 31; 1; 65; 51; 86; 88; 15]. However, planning constraints are often explicitly specified for TAMP methods. In contrast, we specify constraints as (learned) probabilities, which are baked into the decoding process and provided by domain-specific grounded models.

## 3 Grounded Decoding

### LLMs and Grounding Models

**Large Language Models.** LLMs are trained to predict the probability \(p(W)\) of a text sequence \(W\), represented as a sequence of tokens \(W=w_{1:N}=(w_{1},\ldots,w_{N})\). The tokens are elements of a fixed vocabulary \(\mathcal{W}\). Typical neural architectures factorize the joint probability into \(p(W)=\prod_{n=1}^{N}p_{\text{LLM}}(w_{n}|w_{1:n-1})\), where \(p_{\text{LLM}}\) is predicted by a transformer network [81]. Given \(p_{\text{LLM}}\), generating a text consisting of \(N\)-many tokens, the so-called decoding process, can be seen as the optimization problem \(\arg\max_{w_{1:N}\in\mathcal{W}}\prod_{n=1}^{N}p_{\text{LLM}}(w_{n}|w_{1:n-1})\), which in practice is solved, e.g., using greedy search, beam search, or sampling strategies. To further ensure the LLM is solving a desired task, one typically starts with a given text, the so-called _prefix_ or _prompt_, that describes the task, and then the LLM completes this task in its decoding process.

Figure 2: Overview of **Grounded Decoding**. Given a _free-form_ language instruction, a language model and grounded models jointly decide the next candidate token to be decoded by combining their respective likelihood. Language model proposes likely tokens that produce goal-directed and coherent long-horizon behaviors, while grounded models connect them to the physical scene, through a flexible composition of multiple objective functions from multiple modalities, such as affordance, preferences, and safety.

**Grounding Functions.** We use the concept of grounding functions, \(p_{\text{G}}(w_{1:n}|s)\), which seek to model a probability of tokens \(w_{1:n}\) given (potentially non-textual) state \(s\in\mathcal{S}\). This state is intended to capture the embodiment of the robot and the environment, which may be an image, proprioception of the robot, or the environment state. Thus the grounding function models probabilities relevant to the robot embodiment and environment, such as whether the tokens are possible for the robot to execute given the state (affordances), or other values like safety, cost, or user preferences.

### Problem formulation.

Given an instruction in language \(\ell\), we look at the problem of using an LLM to decode a language plan \(w_{1:N}\), which is typically done by finding the most likely tokens under the probability distribution predicted by the LLM, \(p_{\text{LLM}}(w_{1:N}|\ell)\), with \(\ell\) being the prefix. However, based on the instruction \(\ell\) as the prefix alone, the LLM can easily generate text that is not grounded in the physical state of the environment, rendering such plans useless in the real world. In order to _ground_ the language model in an actual physical embodiment, we propose _Grounded Decoding_ (GD): The main idea of GD is to guide the generation of token sequences with _grounding function(s)_ that are conditioned on the embodiment of the system.

Formally, let \(s\in\mathcal{S}\) denote a representation of the state of the world. Then, GD attempts to generate text that is consistent with both the instruction \(\ell\)_and_ the physical state \(s\):

\[w_{1:N}^{*}=\arg\max_{w_{1:N},w_{n}\in\mathcal{W}}p_{\text{GD}}(w_{1:N}|s,\ell)\] (1)

To leverage the Internet-scale knowledge of LLMs, we factorize \(p_{\text{GD}}(w_{1:N}|s,\ell)\) as follows 2:

Footnote 2: We make three assumptions for this derivation: 1) \(s\) and \(\ell\) are marginally independent (Line 3), 2) \(s\) and \(\ell\) are conditionally independent given \(w_{1:N}\) (Line 5), and 3) \(p(w_{1:N})\) is uniform over responses (Line 6).

\[p_{\text{GD}}(w_{1:N}|s,\ell) =\frac{p(s,\ell|w_{1:N})\;p(w_{1:N})}{p(s,\ell)}\] (2) \[=\frac{p(s|w_{1:N})p(\ell|w_{1:N})p(w_{1:N})}{p(s,\ell)}\] (3) \[=\frac{p(w_{1:N}|\ell)p(\ell)p(w_{1:N}|s)p(s)p(w_{1:N})}{p(s, \ell)p(w_{1:N})p(w_{1:N})}\] (4) \[\propto\frac{p(w_{1:N}|\ell)}{p(w_{1:N})}p(w_{1:N}|s)\] (5) \[\propto p(w_{1:N}|\ell)p(w_{1:N}|s).\] (6)

To decode autoregressively with the formulation, we factorize above into token decoding:

\[p_{\text{GD}}(w_{1:N}|s,\ell)\propto\prod_{n=1}^{N}p_{\text{LLM}}(w_{n}|w_{1: n-1},\ell)\;p_{\text{G}}(w_{1:n}|s).\] (7)

The first term, \(p_{\text{LLM}}(w_{n}|w_{1:n-1},\ell)\), can be modeled as the probability of the LLM predicting the token for the given instruction \(\ell\) appended previously decoded tokens \(w_{1:n-1}\) without the state \(s\) as input. The second term, \(p_{\text{G}}(w_{1:n}|s)\), is the grounding function that is only conditioned on the state \(s\) and judges whether the generated text \(w_{1:n}\) is consistent with the physical state. The core idea behind this factorization is that LLMs exhibit long-term planning capabilities, while the grounding function guides the planning of the LLM to be possible in the concrete embodied physical world without needing to be informed or capable of reasoning over the long-horizon instruction.

### Grounded Decoding

This work investigates grounded decoding exclusively in the context of task planning for embodied agents. Figure 2 visualizes a single step of the simplest greedy search form of GD, and accompanying pseudo-code can be found in Algorithm 1. Given a high-level language instruction and history of executed actions, GD proceeds through a process similar to probabilistic filtering by selecting tokens iteratively that have high probability under the language model and the grounded model. Aftereach token is selected, it is appended to the prefix. The process continues until a token in the terminal set \(\mathcal{W}_{\text{perm}}\) is selected, which could be a period sign "." indicating the end of a single-step skill (e.g., pick-and-place). Then the command \(w_{1:i}\) is sent to a language-conditioned policy \(\pi(a|s,w_{1:i})\) that executes the action \(a\) conditioned on the environment state \(s\). Crucially, this grounding function must accept partial commands to enable grounding during decoding.3 Additionally, we note that GD, in its essence, provides a grounded scoring function; thus, it can be easily extended to any search methods such as beam search, top-k sampling, etc.

Footnote 3: As an example, an affordance ground function for a skill “pick up _object_”, should emit a high probability for “pick” and “pick up” if any object is able to be picked and collapse to only feasible objects only once the _object_ token is decoded.

```
1:Given: state \(s\), instruction \(\ell\), terminal set \(\mathcal{W}_{\text{perm}}\)
2:Initialize:\(w=\{\}\), \(n=0\)
3:while\(w_{n}\notin\ \mathcal{W}_{\text{perm}}\)do
4:\(n=n+1\)
5:\(w_{n}=\operatorname*{arg\,max}_{w_{n}\in\mathcal{W}}p_{\text{LLM}}(w_{n}|w_{1: n-1},\ell)\ p_{\text{G}}(w_{1:n}|s)\)
6:endwhile
7:Return:\(w\) ```

**Algorithm 1** Grounded Decoding (GD) w/ Greedy Search

### Techniques for Obtaining Grounded Models

Unlike language tasks, where a single model is capable of performing general semantic reasoning, a singular grounded model remains an open problem. Indeed, each domain may impose varied environmental and embodiment constraints. Despite these challenges, we present several techniques for obtaining grounded models that can be leveraged in GD's formulation, and validate them in three domains in Section 4.

**Token-Conditioned Value Functions.** Assuming a robot acts with action \(a\) according to policy \(\pi(a|s,w_{1:n})\), that aims to maximize certain a utility and that the utility captures a task objective, a natural choice that can provide "grounding score" is the action-value function \(Q(s,a|w_{1:n})\) as it necessarily captures the embodiment of the robot. Additional objectives, such as task constraints, can also be encapsulated in \(Q(s,a|w_{1:n})\) to ensure grounding. Note that unlike the formulation proposed in [2], \(w_{1:n}\) cannot be restricted to a fixed repertoire of token sequences. In practice, to obtain a \(Q(s,a|w_{1:n})\) that satisfies the requirements, one can train multi-task language-conditioned agents, either through reinforcement learning (Section 4.2) or supervised learning (Section 4.1).

**Multimodal Foundation Models.** A general choice to ground LLMs is through using multimodal foundation models, such as CLIP [57] or open-vocabulary object detectors [22; 34; 50]. Although these models can connect language to other grounded modalities (e.g., vision), they often lack the capability for complex or long-horizon reasoning, and they do not consider embodiment constraints. As a result, to leverage them in the decoding process, they need to constrained to where they are the most applicable rather than always decoding jointly. To this end, we use a prompt-based technique that allows LLMs to choose when to jointly decode (Section 4.3), which we find to be effective in most cases.4.

Footnote 4: Emerging multimodal language models [16; 53] provide strong baselines, but they similarly cannot serve general-purpose grounding functions because they are not conditioned on embodiment, except for cases where embodiment data from each individual domain can be used to finetune the LLM [16].

**Rule-based Methods.** Another source of grounding may come from features \(x=\phi(w_{1:n})\) designed with expert domain knowledge, which can then be used to map \(w_{1:n}\) to a "grounding score" using parametric or non-parametric functions \(f(x)\). Such techniques may be most applicable when interpretability and enforcing hard constraints are required, such as safety-critical settings, or when data are scarce, such as cases involving preferences of individual users (as shown in Section 4.1).

### Comparisons to Prompt-based Methods

One alternative approach for grounding is including scene information as part of the prompt (e.g., object detection results [96]), which complements the grounding method proposed in this work.

However, we note that prompting is often insufficient for grounding, as information about the scene and about the capabilities of the robot may not always be succinctly described in the prompt. Such examples include 1) in a block stacking task, a block that has been stacked on cannot be picked, 2) in a navigation task, to open a door, one must have a key and that door must be reachable, and 3) in a mobile manipulation domain, an object may be visible but is out of reach of the manipulator. Therefore, Grounded Decoding is a more general and flexible grounding method that injects _continuous probabilities_ during decoding, which may even come from grounding functions from _other modalities_ (e.g., vision).

## 4 Experiments

### Long-Horizon Tabletop Manipulation

Herein we experiment with a simulated tabletop manipulation environment based on RAVENS [95]. We create a custom set of 20 tasks, with 10 seen tasks and 10 unseen tasks. Seen tasks are used for training (for supervised baseline) or for few-shot prompting. They are grouped by following categories. Detailed breakdown can be found in Appendix A.2.

i. **Letters**: Rearranging alphabetical letters ("sort the letters in alphabetical order"). ii. **Blocks & Bowls**: Rearranging or combining blocks and bowls ("put blocks in matching bowls"). iii. **Box Packing**: Sorting food items and utensils into boxes in accordance with safety constraints and user preferences ("Can you pack the picnic box for me?").

Given only high-level language instructions and top-down visual observation of the environment, Grounded Decoding decodes a sequence of text tokens representing the step command to be executed. Note that because GD generated grounded _free-form_ actions, it does not require each step to strictly map to a repertoire of skill as in [29; 2]. After a complete command is generated, it is executed via a pre-trained multi-task language-conditioned CLIPort [67]. An example rollout is shown in Figure 4.To demonstrate the techniques proposed in Section 3.4 to obtain grounding functions, we study the composition of following grounding functions (overall grounding score is calculated as \(p_{\text{G}}=\prod_{i=1}^{n}p_{i}\)) depending on the task categories. Refer to the Appendix A.2 for details.

Figure 3: Example rollouts and likelihood of representative tokens under Grounded Decoding objective in three distinct domains: simulated tabletop rearrangement **(top)**, Minigrid 2D Maze (**middle**), and real-world kitchen mobile manipulation (**bottom**). Each domain uses different prompts, grounded models, and low-level primitives. The GD formulation is shared across the domains, decoding a pre-trained langauge model with respect to domain-specific grounded models to decompose a _open-ended_ instruction into actionable steps.

[MISSING_PAGE_FAIL:7]

Affordance Grounding Function.Following the recipe from Section 3.3, we train token-conditioned affordance function to be used in GD. The difference is that the grounding function here is the value function from the goal-conditioned policy that is trained with PPO [62] instead of from demonstrations as in CLIPort [67]. The policy performs short-horizon skills such as "Go to red key" or "Open the door" and are conditioned on CLIP embeddings of the skill and an image of the scene. Accordingly, the goal-conditioned value function evaluates the feasibility given the current observation and the (partially) decoded skill.

**Baselines.** We compare the two variants of GD - with greedy and beam search - with 1) a solitary PPO policy [62], 2) a hierarchical RL algorithm which plans over the low-level skills, and 3) a hierarchical method that uses an ungrounded language model for planning [29].

**Analysis.** Table 2 reports the success rate, averaged across 100 episodes of randomly initialized environments. The "flat" RL agent performs poorly in all but the simplest environments, owing to difficulties with understanding the high-level instruction and reasoning over long horizons (often over 100 steps). Planning over low-level skills using hierarchical RL [5] improves this performance, since the high-level decision-making problem is greatly simplified. However, the high-level RL agent still needs to reason over low-level (textual) skills by understanding their underlying capabilities and stitching them together. Using the planning capabilities of large language models to reason over textual skills significantly boosts this performance [29], since the language model can inherit the strong reasoning capabilities from its training data. This tends to be insufficient in challenging environments, however, since the number of _potentially viable_ skills may be very large and the LLM has no information about the robot's observations. GD can leverage the learned affordance function (in this case, the goal-conditioned value function) to inform the language model's plans, enabling successful long-horizon reasoning. We further find that beam search improves performance modestly, particularly in long-horizon tasks.

### Mobile Manipulation in a Physical Kitchen

Our last environment is a kitchen robot in the real world, and we follow the same implementations of the mobile manipulation platform and skills in SayCan [2]. We perform instruction following tasks, as in [2]. An example task is "Bring an apple", for which the robot needs to plan and execute a sequence of "1. Find an apple, 2. Pick up the apple, 3. Bring it to you. 4. Put it down, 5. Done". We split the tasks into two categories. _Unambiguous_ means the instruction explicitly contains the object of interest, and _Ambiguous_ means the instruction does not contain the object name. For example, when human asks "bring me the fruit", the robot needs to first determine available fruits. We assume all necessary objects are in the field of view. More details can be found in Appendix A.4.

**Grounded Decoding with Chain-of-thought.** We demonstrate using multimodal foundation models for Grounded Decoding, as proposed in Section 3.4. In particular, we use open-vocabulary object detector owl-vit [50]. Note that because these off-the-shelf models are not trained on robot domain data, we find that it works best by constraining their influence on decoding. We achieve this by making a slight modification to the SayCan algorithm [2]: before generating action plans, we prompt the LLM to generate _visually-grounded_ chain-of-thought [84] by giving LLM the option of when to enable grounded decoding and disable grounded decoding, as visualized in Fig. 5. Specifically,

Figure 4: Greedy decoding rollout with GD, where key decoded tokens are shown (_yellow_, _purple_, _red_, _yellow_). Combined scores are normalized to the maximum for visual clarity; others are normalized to their sum.

LMs can be prompted to generate a left bracket to start decoding jointly with grounded models and a right bracket to revert to ungrounded decoding. After chain-of-thought, we use SayCan scoring mode for decoding the action plans.

**Analysis.** Table 3 shows that GD recovers similar performance on _Unambiguous_ tasks, and gain 25% in planning performance on _Ambiguous_ tasks. This shows that GD with multimodal foundation models can effectively use _visually-grounded_ chain-of-thought to disambiguate abstract tasks.

## 5 Analysis

### Comparison to SayCan

In this section, we directly compare GD to SayCan [2], which is related to our method in that both combine language model knowledge and grounded model knowledge (discussed in more detail in Section 2). However, SayCan uses the language model to score all pre-specified options, rendering it inefficient at dealing with large or combinatorial action spaces. In contrast, GD computation considers all possible language token in the autoregressive decoding process, which is _independent_ of the size of the action space. Results shown in Table 4 demonstrate that GD is two orders of magnitude more efficient on our tasks, with comparable performance. Furthermore, by decoding at the most basic functioning unit of language, GD's formulation allows open-vocabulary grounding beyond just affordances, e.g. safety, preferences, and multimodal embeddings such as CLIP.

### Breakdown of Failure Reasons

Because all hierarchical approaches share an imperfect low-level policy for step execution, the results reported in Table 1 are compounded with both planning failures and low-level policy failure. In Figure 6, we provide failure breakdown analysis for Grounded Decoding and associated baselines. Note that the CLIPort baselines are solitary methods that do not use a planner, so the failures are solely composed of policy failures. As shown in Figure 6, while all planning-based methods use the same underlying low-level policy, Grounded Decoding significantly reduces planning failure by being able to incorporate grounded scene information into the decoding process. Moreover, we observe that despite the shared affordance function across beam and greedy search, the beam search variant performs stronger by being aware of the full-length single-step instructions during decoding.

### Grounded Action Manifold

A central goal of this work is to investigate the integration of grounded information into language model decoding to output instructions actionable by a policy. To investigate this, we use a t-SNE [80] plot to illustrate the extent to which grounded models help narrow down the search space for language models. Specifically, we first enumerate all meaningful instructions in the tabletop domain, such as "pick x and place it on y," which are represented as dots in the figure. We then compute the affordance values with respect to four different scenes, where each color represents one scene. Finally, we group the dots using t-SNE and BERT embeddings [13]. Figure 7 shows that grounded

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **GD (Greedy)** & **GD (Beam)** & **SayCan** \\ \hline
**Success Rate** & 50\% & 60\% & **64\%** \\
**Token Count** & **1x** & 4.3x & 113.7x \\ \hline \hline \end{tabular}
\end{table}
Table 4: By avoiding full enumeration of skills, GD is more efficient than SayCan while staying performant.

Figure 5: Example prompt and rollout in real-world kitchen environment.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{2}{c}{**GD**} & \multicolumn{2}{c}{**SayCan**} \\ \cline{2-4}
**Tasks** & **Planning** & **Execution** & **Planning** & **Execution** \\ \hline
**Unambiguous** & 85\% & 57\% & 85\% & 57\% \\
**Ambiguous** & 58\% & 44\% & 33\% & 25\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Success rates in kitchen environment.

models can effectively identify achievable skills to produce an actionable manifold within the language space and that this grounding is required, as language alone does not perfectly group actionable skills. It is worth noting that while we provide manual enumeration of all possible skills for practical analysis, the full language space is much larger. This highlights the even more pronounced narrowing of the search in the language space.

## 6 Conclusions, Limitations, & Future Works

We presented Grounded Decoding (GD), an approach for leveraging the knowledge and capabilities of large language models in embodied settings through grounding functions, which model the probabilities of tokens given an embodiment. GD resembles probabilistic filtering, by decoding tokens that have high probabilities under the language model _and_ under grounded model(s). By guiding the LLM's decoding directly at its output, GD is a general, flexible, and expressive approach to embodied tasks. This is demonstrated on three embodied domains, showing GD is capable of solving complex, long-horizon tasks.

Though quite general and flexible, GD has a few limitations. First, although we present several techniques for obtaining grounding functions in different domains, it remains a question whether a capable and general grounding function can be obtained. We hope that recent progress in large-scale robotics models (e.g. [7] and [59]) can remove this bottleneck, and note that the flexibility of GD allows such progress to be straightforwardly leveraged. Second, prompt engineering is often required to steer LLMs to the desired action space (e.g., likely action verbs, likely present objects). Finally, while not requiring additional training, the joint decoding may be limiting compared to a single model capable of both grounding and language reasoning [3; 9; 16].

This work presented a family of algorithms for grounding LLMs in embodiment, for which there are many avenues for future work. The flexibility of the approach enables many other grounding functions and ways to integrate grounding. Furthermore, the development and integration of a foundation model for grounding would improve performance significantly. Finally, though GD's probabilistic filtering-based approach is quite general, fusing grounding information to the language model _after_ each token decoding may be limiting and future works can investigate how such grounding can be elegantly integrated _during_ decoding.

#### Acknowledgments

The authors would like to acknowledge Pierre Sermanet, Carolina Parada, Jie Tan, Yevgen Chebotar, Vincent Vanhoucke, and Dorsa Sadigh for their feedback and contributions. This work is supported in part by OpenAI academic access program, granted to Wenlong Huang.

Figure 6: Failure breakdown on tabletop domain. GD achieves lowest planning failure among planning-based methods, among which beam search variant performs the best.

Figure 7: Visualization of actions colored by affordance values in different scenes. Every dot represents a possible action in the tabletop domain, where the majority of the actions are infeasible. We show how grounded models can identify the feasible actions for specific scenes. Notably, these actions are not always clustered in language space, requiring the grounding function to determine what action to perform.

## References

* [1] Christopher Agia, Toki Migimatsu, Jiajun Wu, and Jeannette Bohg. Stap: Sequencing task-agnostic policies. _arXiv preprint arXiv:2210.12250_, 2022.
* [2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quaimbao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can and not as i say: Grounding language in robotic affordances. In _arXiv preprint arXiv:2204.01691_, 2022.
* [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _arXiv preprint arXiv:2204.14198_, 2022.
* [4] Ashutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan. Generating more interesting responses in neural conversation models with distributional constraints. _arXiv preprint arXiv:1809.01215_, 2018.
* [5] Andrew G. Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. _Discrete Event Dynamic Systems_, 13(1-2):41-77, jan 2003.
* [6] Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R Varshney. Mirostat: A neural text decoding algorithm that directly controls perplexity. _arXiv preprint arXiv:2007.14966_, 2020.
* [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. _arXiv preprint arXiv:2212.06817_, 2022.
* [8] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S Ryoo, Austin Stone, and Daniel Kappler. Open-vocabulary queryable scene representations for real world planning. _arXiv preprint arXiv:2209.09874_, 2022.
* [9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022.
* [10] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for gymnasium, 2018.
* [11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [12] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. _arXiv preprint arXiv:1912.02164_, 2019.
* [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [14] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with large language models for object rearrangement. _arXiv preprint arXiv:2303.06247_, 2023.
* [15] Danny Driess, Ozgur Oguz, Jung-Su Ha, and Marc Toussaint. Deep visual heuristics: Learning feasibility of mixed-integer programs for manipulation planning. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9563-9569. IEEE, 2020.

* [16] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.
* [17] Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planning and reinforcement learning. _Advances in Neural Information Processing Systems_, 2019.
* [18] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. _arXiv preprint arXiv:1805.04833_, 2018.
* [19] Richard E Fikes and Nils J Nilsson. Strips: A new approach to the application of theorem proving to problem solving. _Artificial intelligence_, 1971.
* [20] Caelan Reed Garrett, Chris Paxton, Tomas Lozano-Perez, Leslie Pack Kaelbling, and Dieter Fox. Online replanning in belief space for partially observable task and motion problems. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, 2020.
* [21] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. Hafez: an interactive poetry generation system. In _Proceedings of ACL 2017, System Demonstrations_, pages 43-48, 2017.
* [22] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. _arXiv preprint arXiv:2104.13921_, 2021.
* [23] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. _arXiv preprint arXiv:2203.09509_, 2022.
* [24] Felix Hill, Sona Mokra, Nathaniel Wong, and Tim Harley. Human instruction-following with deep reinforcement learning via transfer-learning from text. _arXiv preprint arXiv:2005.09382_, 2020.
* [25] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. _arXiv preprint arXiv:1904.09751_, 2019.
* [26] Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. Learning to write with cooperative discriminators. _arXiv preprint arXiv:1805.06087_, 2018.
* [27] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. _arXiv preprint arXiv:2210.05714_, 2022.
* [28] De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, and Juan Carlos Niebles. Neural task graphs: Generalizing to unseen tasks from a single video demonstration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019.
* [29] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_. PMLR, 2022.
* [30] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In _arXiv preprint arXiv:2207.05608_, 2022.
* [31] Brian Ichter, Pierre Sermanet, and Corey Lynch. Broadly-exploring, local-policy trees for long-horizon task planning. _Conference on Robot Learning (CoRL)_, 2021.
* [32] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In _Conference on Robot Learning_, pages 991-1002. PMLR, 2021.

* [33] Leslie Pack Kaelbling and Tomas Lozano-Perez. Hierarchical planning in the now. In _Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence_, 2010.
* [34] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1780-1790, 2021.
* [35] Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Dragomir Radev, Yejin Choi, and Noah A Smith. Beam decoding with controlled patience. _arXiv preprint arXiv:2204.05424_, 2022.
* [36] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. _arXiv preprint arXiv:1909.05858_, 2019.
* [37] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazreen Fatema Rajani. Gedi: Generative discriminator guided sequence generation. _arXiv preprint arXiv:2009.06367_, 2020.
* [38] Remi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar, Jean-Baptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, and Oriol Vinyals. Machine translation decoding beyond beam search. _arXiv preprint arXiv:2104.05336_, 2021.
* [39] Jiwei Li, Will Monroe, and Dan Jurafsky. Learning to decode for future success. _arXiv preprint arXiv:1701.06549_, 2017.
* [40] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. _arXiv preprint arXiv:1908.03557_, 2019.
* [41] Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin Akyurek, Antonio Torralba, Jacob Andreas, and Igor Mordatch. Pre-trained language models for interactive decision-making. _arXiv preprint arXiv:2202.01771_, 2022.
* [42] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. _arXiv preprint arXiv:2209.07753_, 2022.
* [43] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: From natural language instructions to feasible plans. _arXiv preprint arXiv:2303.12153_, 2023.
* [44] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. _arXiv preprint arXiv:2304.11477_, 2023.
* [45] Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, and William Yang Wang. Multimodal procedural planning via dual text-image prompting. _arXiv preprint arXiv:2305.01795_, 2023.
* [46] Corey Lynch and Pierre Sermanet. Grounding language in play. 2020.
* [47] Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with visual affordances over unstructured data. _arXiv preprint arXiv:2210.01911_, 2022.
* [48] Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Typical decoding for natural language generation. _arXiv preprint arXiv:2202.00666_, 2022.
* [49] Clara Meister, Tim Vieira, and Ryan Cotterell. If beam search is the answer, what was the question? _arXiv preprint arXiv:2010.02650_, 2020.
* [50] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection with vision transformers. _arXiv preprint arXiv:2205.06230_, 2022.

* Nair et al. [2018] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. In _Advances in Neural Information Processing Systems_, 2018.
* Nair et al. [2021] Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, and Chelsea Finn. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In _Conference on Robot Learning_, pages 1303-1315. PMLR, 2021.
* Gpt-4 technical report [2023] OpenAI. Gpt-4 technical report. _arXiv_, 2023.
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* Pashevich et al. [2021] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navigation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021.
* Patel et al. [2023] Dhruvesh Patel, Hamid Eghbalzadeh, Nitin Kamra, Michael Louis Iuzzolino, Unnat Jain, and Ruta Desai. Pretrained language models as visual planners for human assistance. _arXiv preprint arXiv:2304.09179_, 2023.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 18-24 Jul 2021.
* Raman et al. [2022] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. _arXiv preprint arXiv:2211.09935_, 2022.
* Reed et al. [2022] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. _arXiv preprint arXiv:2205.06175_, 2022.
* Reid et al. [2022] Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help offline reinforcement learning. _arXiv preprint arXiv:2201.12122_, 2022.
* Savinov et al. [2018] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. _arXiv preprint arXiv:1803.00653_, 2018.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Scialom et al. [2020] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Discriminative adversarial search for abstractive summarization. In _International Conference on Machine Learning_, pages 8555-8564. PMLR, 2020.
* Shah et al. [2022] Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. _arXiv preprint arXiv:2207.04429_, 2022.
* Shah et al. [2022] Dhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander Toshev, Sergey Levine, and Brian Ichter. Value function spaces: Skill-centric state abstractions for long-horizon reasoning. _ICLR_, 2022.
* Sharma et al. [2021] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent language. _arXiv preprint arXiv:2110.01517_, 2021.
* Shridhar et al. [2022] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _Conference on Robot Learning_, pages 894-906. PMLR, 2022.
* Shridhar et al. [2022] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. In _Proceedings of the 6th Conference on Robot Learning (CoRL)_, 2022.

* [69] Tom Silver, Rohan Chitnis, Nishanth Kumar, Willie McClinton, Tomas Lozano-Perez, Leslie Pack Kaelbling, and Joshua Tenenbaum. Inventing relational state and action abstractions for effective and efficient bilevel planning. _arXiv preprint arXiv:2203.09634_, 2022.
* [70] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. _arXiv preprint arXiv:2209.11302_, 2022.
* [71] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. _arXiv preprint arXiv:2206.11871_, 2022.
* [72] Charlie Snell, Sherry Yang, Justin Fu, Yi Su, and Sergey Levine. Context-aware language modeling for goal-oriented dialogue systems. _arXiv preprint arXiv:2204.10198_, 2022.
* [73] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. _arXiv preprint arXiv:2212.04088_, 2022.
* [74] Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel Collier. Language models can see: plugging visual controls in text generation. _arXiv preprint arXiv:2205.02655_, 2022.
* [75] Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. Embodied bert: A transformer model for embodied, language-guided visual task completion. _arXiv preprint arXiv:2108.04927_, 2021.
* [76] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019.
* [77] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. _Advances in neural information processing systems_, 27, 2014.
* [78] Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zero-shot image-to-text generation for visual-semantic arithmetic. _arXiv preprint arXiv:2111.14447_, 2021.
* [79] Marc Toussaint. Logic-geometric programming: An optimization-based approach to combined task and motion planning. In _Twenty-Fourth International Joint Conference on Artificial Intelligence_, 2015.
* [80] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* [81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [82] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. _2023_, 2023.
* [83] Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine. Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning. _arXiv preprint arXiv:2204.08426_, 2022.
* [84] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* [85] Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. Consistency of a recurrent language model with respect to incomplete decoding. _arXiv preprint arXiv:2002.02492_, 2020.
* [86] Bohan Wu, Suraj Nair, Li Fei-Fei, and Chelsea Finn. Example-driven model-based reinforcement learning for solving long-horizon visuomotor tasks. _arXiv preprint arXiv:2109.10312_, 2021.

* [87] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. _arXiv preprint arXiv:1609.08144_, 2016.
* [88] Fei Xia, Chengshu Li, Roberto Martin-Martin, Or Litany, Alexander Toshev, and Silvio Savarese. Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, 2021.
* [89] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. Translating natural language to planning goals with large-language models. _arXiv preprint arXiv:2302.05128_, 2023.
* [90] Danfei Xu, Ajay Mandlekar, Roberto Martin-Martin, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Deep affordance foresight: Planning through what can be done in the future. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 6206-6213. IEEE, 2021.
* [91] Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Neural task programming: Learning to generalize across hierarchical tasks. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, 2018.
* [92] Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators. _arXiv preprint arXiv:2104.05218_, 2021.
* [93] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. _arXiv preprint arXiv:2303.16563_, 2023.
* [94] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. _Advances in Neural Information Processing Systems_, 2021.
* [95] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging the visual world for robotic manipulation. _Conference on Robot Learning (CoRL)_, 2020.
* [96] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. _arXiv preprint arXiv:2204.00598_, 2022.

Appendix

### Grounded Decoding Implementation Details

We study three different implementations of Grounded Decoding for each of the experimental domains. While each instantiation applied Grounded Decoding to long-horizon planning and behavior synthesis, different components including language models and grounded models are used in each domain, as seen in Table 5. Grounded models used in these domains include Affordance Functions (AF), Safety Functions (S), Preference Functions (P), and Open-Vocabulary Object Detectors (D).

### Implementation Details of Simulated Tabletop Rearrangement

#### a.2.1 Tasks

There are a total of 20 tasks (templates of language instructions), listed in Table 6, grouped into three task category: Letters, Blocks&Bowls, and Box Packing. Three categories share a total of 57 objects. For Letters category, the goals are to rearrange the alphabetical letter objects such that they satisfy certain orders specified by the language instructions. At the beginning of each episode, task-relevant objects and a set of 1 to 3 randomly-sampled distractor objects (except for the Letters category) are initialized at random positions on the tabletop with fixed orientations. A minimum 15cm distance is enforced between any two objects to avoid collision and penetration at initialization. To allow for automatic evaluations, a binary reward function is defined for each task using ground-truth state of the objects. Furthermore, we implement scripted policies for each task to collect demonstrations for training the CLIPort baseline. For certain tasks, we also randomize the attributes mentioned in the given instructions, which can be found below:

i. \(\langle\text{word}\rangle\): hi, world, left, right, top, down, love, you

ii. \(\langle\text{corner/side}\rangle\): left side, top left corner, top side, top right corner, bottom side, bottom left corner

#### a.2.2 Low-level Primitives

We use CLIPort [67] as the low-level primitive that can be invoked by the LLM planner, as it shows promising results of generalization across free-form language instructions. Additionally, since the policy predicts per-pixel affordance, it can be repurposed to serve as grounded models for planning for long-horizon tasks, which we leverage in this work. The single primitive policy is trained on 50,000 pre-collected demonstrations, across 10 training tasks, where each demonstration contains 1) language instruction of the format "pick up [x] and place it on [y]", 2) top-down RGB-D observation of the current scene, 3) expert pick location expressed as pixel coordinates, and 4) expert place location expressed as pixel coordinates. The expert actions are obtained by accessing ground-truth object pose in the simulator. We further apply substring augmentation during training as we find it helps with performance on partial commands (see Section

#### a.2.3 Language Model

We use InstructGPT [54] (text-davinci-002), accessed through OpenAI API.

#### a.2.4 CLIPort Baseline

As CLIPort [67] already takes as input a natural language instruction and is capable of directly outputting robot actions, it bears the question whether we need a high-level planner for completing

\begin{table}
\begin{tabular}{c l l l} \hline \hline  & **Tabletop Rearrangement (Sim)** & **MiniGrid 2D Maze (Sim)** & **Kitchen Mobile Manipulation (Real)** \\ \hline
**LLM** & InstructGPT [54] & InstructGPT & InstructGPT + PaLM[11] \\ \hline
**Primitives** & CLIPort [67] & PPO [62] & RT-1 [7] \\ \hline
**Grounded Models** & AF + S + P & AF & D \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison between different versions of GD implemented in three different environments.

long-horizon tasks. To this end, we additionally train two variants of multi-task CLIPort policy on 10 of the total 20 tasks as baselines (see Table 6 for the train/test split). One variant, which we referred as "CLIPort (Short)", is trained only on single-step pick-and-place instructions of the format "pick up [x] and place it on [y]" on the 10 training tasks. The decomposed pick-and-place instructions are obtained from scripted planners. At evaluation time, the policy is fed in only the high-level instructions without any planners. The other variant, which we referred as "CLIPort (Long)", is trained on the high-level instructions from the 10 training tasks (without decomposition from scripted planners). Similarly, at evaluation time, it is fed in only the high-level instructions and evaluated on both seen and unseen instructions. Both variants are trained on 50,000 demonstrations, similar to the Grounded Decoding primitive. The goal of these baselines is to evaluate whether solitary language-conditioned policies can perform well on long-horizon tasks and generalize to new task instructions. Note that the CLIPort baselines are different from the primitive used in Grounded Decoding, although they share the same architecture.

#### a.2.5 Full Experimental Results in Simulated Tabletop Domain

Below we show the full list of tasks and the full experimental results in the simulated tabletop domain. Each entry is the average success rate across 20 rollouts. The tasks with blue-colored background are seen tasks and the tasks with orange-colored background are the unseen tasks. Seen tasks may be used for training for supervised baselines (CLIPort) or may be used in the prompt for methods using language model planner. Note that for the "Box Packing" task category, although all tasks were seen in training or the prompts, we enforce additional safety and preference constraints for evaluation only at test time.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & & \multicolumn{2}{c}{**CLIPort**} & \multicolumn{2}{c}{**+LLM**} & \multicolumn{2}{c}{**+Grounded Decoding**} \\ \cline{2-7} Tasks & \(p_{\mathrm{G}}\) & Short & Long & Ungrounded & Greedy & Beam \\ \hline \(\mathrm{Letters}\) & & & & & & \\ Put the letters in alphabetical order from left to right & AF & 5\% & 20\% & 10\% & 20\% & **40\%** \\ Spell as much of _word_ as you can & AF & 10\% & 60\% & 30\% & 60\% & **65\%** \\ Separate the vowels from the remaining letters to the bottom side & AF & 5\% & 40\% & 20\% & 50\% & **65\%** \\ Put the letters in reverse alphabetical order from left to right & AF & 15\% & 10\% & 15\% & **25\%** & **25\%** \\ Correctly spell out a sport using the present letters & AF & 10\% & 10\% & 5\% & **30\%** & **30\%** \\ Sort the geometrically symmetrical letters to the bottom side & AF & 5\% & 10\% & 15\% & 35\% & **50\%** \\ Separate the consonants from the remaining letters to the bottom side & AF & 0\% & 0\% & **25\%** & **25\%** & **25\%** \\ Sort the letters less than “D” according to ASCII to the bottom side & AF & 0\% & 20\% & 35\% & 70\% & **75\%** \\ \hline \(\mathrm{Blocks}\ \&\ Bowls\) & & & & & & \\ Stack all the blocks & AF & 5\% & **90\%** & 30\% & 75\% & **90\%** \\ Put all the blocks on the _comcertifide_ & AF & 0\% & 65\% & 50\% & 45\% & **70\%** \\ Put all the blocks in the bowls with matching colors & AF & 0\% & 30\% & 25\% & 60\% & **70\%** \\ Put the blocks in the bowls with mismatched colors & AF & 25\% & 30\% & 45\% & 30\% & **55\%** \\ Put all the blocks in different corners & AF & 0\% & 5\% & 40\% & 50\% & **60\%** \\ Stack only the blocks of cool colors & AF & 5\% & 5\% & 20\% & **70\%** & **70\%** \\ Stack only the blocks of warm colors & AF & 0\% & 10\% & 15\% & **45\%** & 35\% \\ Sort the primary color blocks to the left side & AF & 0\% & 0\% & 20\% & 25\% & **30\%** \\ \hline \(\mathrm{Box}\ \mathrm{Packing}^{\ast}\) & & & & & & \\ Pick the objects into the brown box & AF + S & 20\% & 40\% & 5\% & **100\%** & 90\% \\ Pack the objects into the boxes & AF + S & 10\% & 20\% & 5\% & **75\%** & 70\% \\ \(\Gamma\)d like some snacks on the right side & AF + P & 15\% & 20\% & 15\% & 40\% & **55\%** \\ Pack me a picnic box & AF + S + P & 15\% & 30\% & 20\% & **100\%** & 95\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: Full Experimental Results in Simulated Tabletop Rearrangement Tasks. The tasks with blue-colored background are seen tasks and the tasks with orange-colored background are the unseen tasks. *Box Packing tasks are all seen during training, but safety and preference requirements are only enforced during evaluation.

### Implementation Details of Minigrid 2D Maze

#### a.3.1 Environment Setup

We use the open-source gym-minigrid suite of environments to evaluate our method with one simple change -- instead of the default observation space which is a \(7\times 7\) egocentric window, our agent has access to _entire grid_ -- that allows us to simplify the tasks by removing partial observability [65].

#### a.3.2 Tasks

The tasks are grouped in three categories (please see Table 7 for example instructions):

1. **Easy**: Simple tasks where the horizon is short (10-30 steps) and fully described by the textual instruction, e.g. OpenDoors and PutNext. The short horizon makes them relatively easy for a wide range of HRL algorithms. The instructions for these tasks generally spell out each individual skill, making them particularly easy for high-level planners based on language modeling.
2. **Medium**: Combination of short and long horizon tasks (up to 80 steps) with step-by-step textual instructions, e.g. LockedRoom. While being significantly longer, these tasks also tend to have instructions that spell out the low-level tasks (see Table 7).
3. **Hard**: Complex, long horizon instructions (over 100 steps) with short, ambiguous instructions that necessitate multi-step reasoning and efficient exploration, e.g. MultiRoom and BlockedUnlock. In addition to being long-horizon, the instructions in this case tend to be ambiguous and under-specified, e.g. "traverse through the rooms to get to the goal", which does not provide enough context for any _blind_ planning agent.

#### a.3.3 Language Model

We use InstructGPT [54] (text-davinci-002), accessed through OpenAI API. The prompts used can be found in Section A.5.

We found the prompts to be generally sufficient for solving the "seen" tasks, as well as "unseen" tasks, i.e. tasks that do not have an example in the context. Empirically, we did not find any improvements by including more then 3 example tasks in the prompt -- we hypothesize that this is likely due to the shared low-level primitives across tasks. For all Minigrid experiments presented in this paper, we used the prompt shown in Section A.5.

#### a.3.4 Low-level Primitives

To train low-level primitives, we train an RL agent to solve a wide range of short-horizon sub-tasks (under 10 steps) that are shared across the various Minigrid tasks -- go to <obj>, pick up <obj>, drop <obj>, open <obj>. Rather than training individual skills for each of them [65], we train a single multi-task policy that is conditioned on the CLIP embeddings [57] of the task strings. This scheme allows some robustness to synonyms and ambiguous task specifications, and has been widely used in learning language-grounded policies [68, 32].

We train these primitives using PPO [62], as recommended by the environment developers [10]. Each of these skills are trained with a sparse outcome reward (+1 if a trajectory is successful, 0

\begin{table}
\begin{tabular}{c l l} \hline \hline
**Difficulty** & **Task Name** & **Example Instruction** \\ \hline Easy & OpenDoors & open door blue, then open door red \\  & PutNext & move the red ball next to the green box \\ \hline Medium & LockedRoom & get the red key from the purple room, open the red door and go to the goal \\ \hline Hard & MultiRoom & traverse the rooms to get to the goal \\  & BlockedUnlock & pick up the blue box \\ \hline \hline \end{tabular}
\end{table}
Table 7: Example Instructions in Minigridotherwise). In addition to these low-level skills, we perform a form of hindsight relabeling where "substrings" of the task strings are masked to allow generalization to partial strings, e.g. "go to red" may be interpreted as "go to red key" or "go to red door", and our masking strategy allows the multi-task policy to execute tasks specified by partially complete strings, if necessary.

#### a.3.5 Additional Qualitative Results

Figure 8: Minigrid Domain

### Implementation Details of Real-World Mobile Manipulation

#### a.4.1 Tasks

#### a.4.2 Language Model

For planning, we use PaLM [11], a 540B parameter language model trained on a large datasets that include high-quality web documents, books, Wikipedia, conversations, and GitHub code. Before planning, we use InstructGPT [54] (text-davinci-002), accessed through OpenAI API. to generate the (grounded) chain of thought.

We used square bracket to indicate grounded decoding, as illustrated in Fig. 5. The prompts are shown in Listing 3.

#### a.4.3 Low-level Primitives

We use a combination of learned and scripted control policies for navigation and manipulation, following the implementation described in SayCan [2] and RT-1 [7]. The manipulation policies for the picking action are learned using Behavior Cloning (BC) on \(68000\) demonstrations and \(12000\) autonomous successes that were collected over the course of 11 months using a fleet of 10 robots. The demonstrations are collected by teleoperators using VR headset controllers to track the motion of their hand, which is then mapped onto the robot's end-effector pose. The navigation policies are scripted, based on a ground-truth map as well as a learned perception module for collision avoidance and planning. The placing actions follow pre-computed motions only when preceded by a navigation policy. The Value Functions used by SayCan for affordance grounding are provided by the \(Q\)-networks of trained RL agents; we follow the RL training setup described in [2].

#### a.4.4 Open-Vocabulary Detector Grounding Function

We use owl-vit [50] as our grounding model. It takes in an image and a natural language query, and returns a list of bounding boxes with scores. We take the maximum score a the grounding function.

\begin{table}
\begin{tabular}{|p{227.6pt}|} \hline
**Instruction** \\ \hline put an energy bar and water bottle on the table \\ bring me a lime soda and a bag of chips \\ Can you throw away the apple and bring me a coke \\ bring me a 7up can and a tea \\ move an multigrain chips to the table and an apple to the far counter \\ move the lime soda, the sponge, and the water bottle to the table \\ bring me an apple, a coke, and water bottle \\ \hline \end{tabular}
\end{table}
Table 8: **List of unambiguous SayCan instructions**

\begin{table}
\begin{tabular}{|p{227.6pt}|} \hline
**Instruction** \\ \hline I want to wipe some spill. \\ Bring me a fruit \\ Bring me a snack \\ Bring me a bag of chips \\ Bring me a bag of snack \\ Bring me a bag of chips and something to wipe a spill \\ Bring me a bag of chips and something to drink \\ Bring me a bag of chips and a soda \\ Human: I want a soda that is not coke, and a fruit. \\ I want a fruit and a soda \\ \hline \end{tabular}
\end{table}
Table 9: **List of ambiguous SayCan instructions**More examples of object detection as a grounding function can be found in Fig. 9.

Figure 9: Additional examples of using open-vocabulary object detection as a grounding function in Real-World Kitchen Mobile Manipulation Domain.

### Prompts

``` Task:Pack all letter objects on the brown box Step 1: pick up the e and place it on the brown box Step 2: pick up the g and place it on the brown box Step 3: done ```

``` Task:Put the letters on the tables in alphabetical order Step 1: pick up the c and place it on the bottom left side Step 2: pick up the d and place it on the right of c Step 3: pick up the i and place it on the right of d Step 4: pick up the l and place it on the right of i Step 5: pick up the w and place it on the right of l Step 6: done ```

``` Task:Spell as much of "blue" as you can Step 1: pick up the l and place it on the bottom left side Step 2: pick up the the u and place it on the right of l Step 3: pick up the the e and place it on the right of u Step 4: done ```

``` Task:Separate the vowels from the remaining letters Step 1: pick up the i and place it on the bottom side Step 2: pick up the o and place it on the bottom side Step 3: done ```

``` Task:Stack all the blocks Step 1: pick up the brown block and place it on the pink block Step 2: pick up the cyan block and place it on the brown block Step 3: pick up the orange block and place it on the cyan block Step 4: pick up the gray block and place it on the orange block Step 5: done ```

``` Task:Put all the blocks on the bottom left corner Step 1: pick up the white block and place it on the bottom left corner Step 2: pick up the yellow block and place it on the bottom left corner Step 3: pick up the green block and place it on the bottom left corner Step 4: pick up the blue block and place it on the bottom left corner Step 5: pick up the purple block and place it on the bottom left corner Step 6: done ```

``` Task:Put all the blocks in the bowls with matching colors Step 1: pick up the cyan block and place it on the cyan bowl Step 2: pick up the purple block and place it on the purple bowl Step 3: pick up the brown block and place it on the brown bowl Step 4: pick up the pink block and place it on the pink bowl Step 5: done ```

``` Task:Pack the items into any box Step 1: pick up the donut stick and place it on the red box Step 2: pick up the pepsi and place it on the brown box Step 3: pick up the peach and place it on the brown box Step 4: pick up the strawberry and place it on the red box Step 5: done ```

``` Task:Pack the items on the table into the brown box Step 1: pick up the knife and place it on the brown box Step 2: pick up the plan and place it on the brown box Step 3: pick up the pepsi and place it on the brown box Step 4: pick up the cupcake and place it on the brown box Step 5: done ```

``` Task:Pack the items on the table into the brown box Step 1: pick up the i and place it on the brown box Step 2: pick up the green block and place it on the brown box Step 3: pick up the l and place it on the brown box Step 4: done ```

``` Task:Can you put some snacks on the right side for me? Step 1: pick up the plum and place it on the right side Step 2: done ```

``` Task:Can you pack my picnic box for m? Step 1: pick up the orange and place it on the picnic box Step 2: pick up the diet popsi and place it on the picnic box Step 3: pick up the knife and place it on the picnic box Step 4: done ```
Tou are a 2D maze-solving agent with access to a variety of low-level skills such as picking up or dropping objects, navigating to doors/keys/boxes, and opening/closing doors. Here are some example tasks:

Task: get the green key from the purple room, unlock the green door and go to the goal Step 1: go to the purple door and open it Step 2: go to the green key Step 3: pick up the key Step 4: go to the green door and open it Step 5: go to the goal.

Task: pick up the purple box Step 1: go to the green obstacle Step 2: pick up the obstacle Step 3: place the obstacle Step 4: go to the blue key Step 5: pick up the blue key Step 6: go to the blue door and open it Step 8: drop the blue key Step 7: go to the purple box Step 8: pick up the purple box.

Task: traverse the rooms to get to the goal Step 1: go to the purple door and open it Step 2: go to the green door and open it Step 3: go to the purple door and open it Step 4: go to the green door and open it Step 5: go to the green door and open it Step 6: go to the goal.

Now your turn.

The following objects are in the scene: Pup, apple, banana, mango, tea, multigrain chips, kettle chips, jalapen chips, rice chips, coke, graprefruit soda, popsi, redbull, energy bar, lime soda, sponge, paper towel, and water bottle. The following locations are in the scene: close counter, far counter, table, trash, bowl. The robot will always put object name in brackets [].

Robot: I am a robot that can bring objects to you. Human: I am hungry. Robot thought: I will find the [multigrain chips]. Robot plan: I. Find the multigrain chips 2. Pick up the multigrain chips 3. Bring it to you 4. Put it down 5. Dono

Robot: I am a robot that can bring objects to you. Human: Throw away the fruit. Robot thought: I will find the [ mango] and move it to the trash. Robot plan: I. Find the mango 2. Pick up the mango 3. Go to the trash 4. Put it down 5. Dono

Robot: I am a robot that can bring objects to you. Human: (inject instruction). Robot thought: