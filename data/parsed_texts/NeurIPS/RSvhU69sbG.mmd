# MathPile : A Billion-Token-Scale Pre-training Corpus for Math

 Zengzhi Wang\({}^{1,3,4}\) Xuefeng Li\({}^{1,4}\) Rui Xia\({}^{3}\) Pengfei Liu\({}^{1,2,4}\)

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Shanghai Artificial Intelligence Laboratory

\({}^{3}\)Nanjing University of Science and Technology \({}^{4}\)Generative AI Research Lab (GAIR)

zzwang.nlp@gmail.com pengfei@sjtu.edu.cn

 Corresponding author

###### Abstract

High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MathPile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of "_less is more_", firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MathPile to boost language models' mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at https://github.com/GAIR-NLP/MathPile/).

## 1 Introduction

High-quality, diverse pre-training corpora form the cornerstone for developing powerful foundation models, enabling AI assistants like ChatGPT [47] to exhibit balanced competencies across a broad spectrum of tasks [11]. In this work, our concern centers on mathematical reasoning capabilities within foundational language models [13, 5, _inter alia_], for which can potentially boost the application in education tools, automated problem solving, data analysis, etc., thereby improving user experience. To facilitate this, we are not directly building a model, but rather focusing on a more fundamental aspect: _creating a high-quality and diverse pre-training corpus tailored for the math domain_, namely MathPile. Specifically, our work is significantly different from the previous work in the following characteristics (cf. Table 1 for comparison):

**Math-centric**. Previous open-sourced pre-training corpora have typically focused on general domains, such as Pile [20], RedPajama [59] and Dolma [2]. Others have concentrated on multilingual aspects or programming languages, such as ROOTS [32] and The Stack [30], respectively. However, a notable absence in these offerings is a corpus specifically tailoring for mathematics. While there exist some corpora designed for training or continually improving math-specific

Figure 1: Key features of MathPile.

language models, such as Minerva's mathematical training dataset [35] and OpenAI's MathMix [37], these are not open-sourced. Note that a recent work concurrent with ours, OpenWebMath [49], although math-centric, is solely sourced from web pages. We will discuss the comparison with it later. Recognizing this gap, we aim to democratize access to high-quality mathematical corpus, fostering inclusive advancements in language models' mathematical reasoning.

**Diversity**. While Hendrycks et al. [26] introduced AMPS, a problem set ranging from elementary mathematics to multivariable calculus (K-12 level) for pre-training purposes, it lacks content at the college-level and more challenging competition-level mathematics, focusing instead on a supervised dataset rather than an extensive corpus. The ProofPile corpus, introduced by Azerbavey et al. [4], aims to improve autoformalization and formal proving capabilities in models, yet its scope is confined to formal proving, not covering the broader mathematical domain from K-12 to postgraduate level. Concurrently with our work, Paster et al. [49] propose the OpenWebMath corpus, featuring a corpus composed of mathematical web pages. However, our corpus goes beyond web pages, integrating high-quality mathematics textbooks, lecture notes, scientific papers from arXiv in the field of mathematics, and carefully selected content from StackExchange, ProofWiki, and Wikipedia among others, which positions our corpus as a richer and more diverse mathematical resource for language models.

**High-Quality**. Recent studies have increasingly highlighted the detrimental effects of low-quality and repeated content in pre-training corpora on model training, as evidenced in various works [1, 42, 33, 27, 41]. The importance of high-quality datasets has thus come to the fore. It has been shown that properly filtered and deduplicated web data can yield models as equally powerful as those trained on curated, high-quality corpora [51]. This similar practice has been recently adopted in several notable studies [12, 2, 60]. A notable example is the 1.3 billion-parameter code-focused model pre-trained on synthetically generated textbooks and filtered web pages, a project that broke existing scaling laws although did not open source its data [24]. It's important to emphasize that quality of the corpus is far more significant than its quantity. For instance, OpenAI's MathMix comprises only 1.5 billion tokens. In this work, we diligently adhere to the principle of _less is more_, as outlined in Zhou et al. [71]. To achieve a high-quality corpus, Unlike other approaches that uniformly process all data, we have conducted specialized preprocessing and prefiltering for each data source before global data processing (including language identification, filtering, cleaning, and deduplication). We're dedicated to refining and optimizing our corpus, making a distinctive contribution to the field.

**Data Documentation**. Auditing large-scale pre-training corpora is essential for identifying content characteristics, intended uses, and potential biases, despite challenges due to their size [7, 21, 44]. However, many such corpora are released without proper documentation [45]. Recent audits of certain pre-training corpora have uncovered issues such as irrelevant content [42, 31, 19], copyright infringement [6], and inclusion of test sets for downstream tasks [1, 18], highlighting the need for detailed data sheets and transparent documentation. To this end, following previous efforts to enhance corpora transparency, we have provided a dataset sheet for our MathPile (see Table 6). Throughout our extensive data processing workflow, numerous documents were annotated for quality, such as language identification scores and the ratio of symbols to words (as exemplified in Figure 12). These quality annotations enable future users to apply their specific filters based on these scores. Additionally, we have conducted extensive deduplication for this corpus and performed data contamination detection with downstream benchmark test sets, removing any duplicated samples identified (cf. SS 3.4). Interestingly, we have also discovered a significant number of questions from downstream test sets in OpenWebMath (cf. SS 3.4). This underscores the importance of meticulous data documentation. We plan to release different versions to facilitate future use. See Appendix C for examples.

Additionally, we conducted continual pre-training experiments on MathPile and found that it generally enhances the performance of language models across various mathematical reasoning benchmarks, with an average improvement of up to 5% (cf. SS 4.2). In conclusion, we hope to facilitate the growth of the field of AI for mathematics by contributing this specialized, high-quality, diverse corpus focused on the mathematical domain while maintaining utmost transparency about the data for practitioners.

## 2 The Collection of Corpora

In order to construct MathPile, we gather data from a variety of sources, which also includes a component of manual collection. We provide an ethics statement regarding copyright in Appendix B.

Mathematical TextbooksTextbooks, covering mathematical concepts, exercises, and solutions, are valuable for _educational purposes_ for both humans and machines. Recent studies, even though not focused on math, support this view with synthesized textbooks [24, 36]. To collect these genuine and high-quality textbooks, we began by conducting extensive manual searches across the internet, seeking open-source and freely accessible mathematics-related textbook websites. Afterwards, we proceeded to download these PDF files, resulting in a collection of 38 K-12 level textbooks, along with 369 college-level mathematics textbooks that cover a wide range of subjects including linear algebra, probability theory, calculus, and optimization. In addition to these textbooks, we also included 467 college course handouts and lecture notes, which tend to be more concise compared to full-length textbooks. Subsequently, we employed the Mathpix API2 to parse the PDFs into markdown format. Then, we meticulously cleaned up extraneous elements such as parsed image URLs, preface sections, table of contents, acknowledge sections, index sections, and consecutive empty lines within the parsed content, resulting in a total of 874 documents.

Footnote 2: https://mathpix.com/ocr

We also refined high-quality mathematics-related synthetic textbooks from OpenPhi Project.3 It is an open-source counterpart to the Phi work [24]. While the underlying model and generation process differ, the output encompasses a broad spectrum of subjects, extending beyond programming. To isolate mathematics-related documents, we employed a straightforward criterion: the presence of the symbol "$" combined with common mathematical expressions like "\(\backslash\{\texttt{mathbf}^{\text{\tiny{2}}}\}\)" and "\(\backslash\{\texttt{frac}}\)". While "$\(\lessapprox\)" alone is not always reliable, combining it with these symbols improves accuracy based on manual verification. This approach yielded 3,889 documents from an initial pool of 124,493. As the volume of pre-training data escalates, the synthesis of high-quality data becomes increasingly crucial. More advanced filtering methods and mathematical corpora synthesis are left for future exploration.

Footnote 3: https://huggingface.co/open-phi

**Mathematical Papers from ArXiv** ArXiv offers a free distribution service and serves as an open-source archive housing millions of scientific papers. It also provides invaluable training data for numerous powerful language models [61, 59, _inter alia_]. In our endeavor to collect mathematical papers from ArXiv, we identify 50 sub-subjects spanning Mathematics, Computer Science, Statistics, Physics, Quantitative Finance and Economics. Our process involved filtering ArXiv's metadata4 to focus on the chosen subjects (cf. Table 7), followed by accessing the source LaTex files (if available). We exclusively retained the LaTex files and consolidated multiple files based on their respective order as indicated by commands such as "\(\texttt{include}\)" and "\(\texttt{input}\)" within the main LaTex file of each paper. Subsequently, we undertook extensive transformations to enhance data clarity and consistency, including removing comments, reverting macros, omitting figures but keeping captions, excluding acknowledgements and references, condensing empty lines, replacing some formatting commands, substituting titles, and preserving only the main body content (cf. SS D for more details). Finally, we compiled 347,945 meticulously cleaned LaTex documents (around 8.5 billion tokens), with each document corresponding to a single paper.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Datasets** & **Open** & **Target Domain** & **\# Textbooks** & **Has Synth** & **Data Location** & **\# Textbooks** & **Source** \\ \hline Minuvu & ✗ & Corpus & General Math & ✗ & ✗ & ✓ & 38.58 & arXiv. Web \\ \hline MathMix & ✗ & Corpus + PS & General Math & 7 & ✓ & ✓ & 1.58 & 7 \\ \hline ProofFile & ✓ & Corpus & Theorem Proving & 7 & ✗ & ✗ & 8.38 & arXiv. Textbooks, Lib., StaszErchang, \\  & & & & & & & & \\ \hline OpenWebMath & ✓ & Corpus & General Math & ✗ & ✗ & ✗ & 14.78 & Web \\ \hline DM-Mathematics & ✓ & PS & Math Competition & ✗ & ✓ & - & 4.48 & Synthesis \\ \hline AMPS & ✓ & PS & Math Competition & ✗ & ✓ & ✗ & 0.78 & Khan Academy, Synthesis \\ \hline MathFile (Ours) & ✓ & Corpus & General Math & 3,979 & ✓ & ✓ & 9.58 & arXiv. Textbooks, StaszErchang, \\  & & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: The comparison of MathPile with other mathematical Corpora, where PS denotes the problem set type. For non-open-sourced corpora, details are inferred from literature, with unknowns marked as “?”. Token counts may vary by tokenizer; we use statistics from each dataset’s report and the GPTNeoX-20B tokenizer [9] for our corpus. DM-Mathematics is from Saxton et al. [56]. “Minerva” refers to its dataset. ProofFile-2 [5], encompassing OpenWebMath and others, is excluded from this comparison.

Mathematical Entries in Wikipedia Wikipedia is one the largest and most popular free online encyclopedias, offering information on a wide range of topics, including history, science, technology, culture, and more. This extensive knowledge has proven to be highly beneficial for numerous natural language processing tasks [34, _inter alia_] and pre-trained language models [17, 61, _inter alia_]. To collect mathematical entries from Wikipedia, we downloaded the mathematics-focused (without pictures) dump of Wikipedia in English for the month of August 2023. We extracted the HTML documents from the dump using the library libzim, resulting in approximately 106,900 documents. Subsequently, we converted these HTML documents into markdown format using the html2text library5 while removing the hyperlinks following the practice of LLaMA [61]. We retained the alternative text content but excluded image (often in SVG format) paths. Additionally, we eliminated extra newlines within paragraphs and condensed more than three consecutive empty lines to two using regular expressions. Further refinement involved the removal of boilerplate content at the bottom of the pages, typically denoted with phrases like "This article is issued from Wikipedia. The text is...". In the end, our efforts yielded a collection of 106,881 mathematical Wikipedia entries, about 0.8 billion tokens.

Footnote 5: We later found that the html2text library resulted in the LaTeX display issue in the cleaned documents (cf. Figure 11). Switching to another library Resiliparse with DOM parsing resolved this issue, ensuring correct LaTeX display.

Entries from ProofWikiProofWiki, an online compendium of mathematical proofs, has been instrumental in advancing the fields of autoformalization and formal proof proving, as evidenced by NaturalProofs [65] and ProofPile. We sourced data from the ProofWiki dump dated April 9, 2022 (provided by the Internet Archive), mirroring the preprocessing approach employed by NaturalProofs, which was based on the version from November 12, 2020. Specifically, this involved leveraging the BeautifulSoup library to parse all Wiki pages followed by the extraction of raw text content using the wikitextparser library. This process yielded a substantial collection of mathematical content, totaling about 7.6 million tokens, comprising 10,328 definitions and 13,511 theorem-proof pairs. To facilitate better data organization, we formatted the definitions using the "definition" environment, and the theorem-proof pairs within the "section" environment with their respective titles serving as the section headings, similar to ProofPile.

Mathematical Discussions on StackExchangeStackExchange, renowned for its network of community-powered question-and-answering websites, spans a wide array of topics, each concentrated on a particular topic. Its high-quality data trove has significantly contributed to the development of various language models [61, 71, _inter alia_]. In our study, we identify eleven sites within this network, including five dedicated to mathematics (such as Mathematics and MathOverflow) and six others in closely related fields like Physics (cf. Table 8). Our data collection process began with downloading the site dumps from August 2023 (provided by the Internet Archive). We only retained the essential components in the posts, namely questions and answers (also associated meta information). To convert HTML documents to raw text, we utilized the BeautifulSoup library, coupled with a meticulous removal of invalid XML characters. We then systematically paired questions and their respective answers. Each question typically garners multiple responses, each with its own score and in some cases, an endorsement as the accepted answer by the questioner. To guarantee quality, we applied a quality threshold (i.e., 5) for filtering. Questions underwent filtering based on the threshold, whereas answers were assessed by either the threshold or the score of the accepted answer, whichever was lower. Unanswered questions scoring at least 10 were preserved for potential future use. This rigorous process resulted in a rich collection of data, comprising 267,919 questions, 435,129 answers, and 3,418 unanswered questions, totaling about 254 million tokens.

Mathematical Web Pages from Common CrawlCommon Crawl, an archive of web data since 2007, is crucial for training advanced language models like GPT-3 [10] and LLaMA. Our work targets extracting math web pages from SlimPajama [12], a cleaned and deduplicated version of RedPajama, focusing on its CommonCrawl and C4 subsets. Eschewing the common approach of using neutral network-based filtering, we opt for heuristic rule-based methods. Our procedure began with the creation of TF-IDF features, derived from our curated high-quality textbooks. During this process, we removed the stop words, limited the features to a maximum of 10,000, and employed white space tokenization. Upon the observation of the resulting vocabulary, we identified 11 commonly used LaTex commands, integral to mathematical expressions. We utilize these commands as a basis for a hard match within each document. A document is classified as mathematical if it contains any of these commands along with the symbol "$5", typically indicative of a mathematical document. Thisrule-based approach, though simplistic, proved to be highly effective, especially given the vast size of the Common Crawl corpus. We also experimented with more intricate dense embedding-based methods to identify mathematical documents, but these resulted in poor recall. Our efforts resulted in the compilation of a substantial collection of mathematical web pages: 4,307 documents from SlimPajama-C4 and 72,137 documents from SlimPajama-CommonCrawl, totaling approximately 633 million tokens. We acknowledge the potential for more efficient methods to sift mathematical documents from Common Crawl snapshots, an area we plan to explore in future work.

## 3 Global Data Processing

After conducting specific data preprocessing for each data source during the data collection process, we globally engage in three critical steps: language identification, filtering, and deduplication, to ensure the quality of the entire corpus, as shown in Figure 2.

### Language Identification

To filter non-English documents, we utilized the fastText language identifier, which was trained on Wikipedia, Tatoeba, and SETimes [29; 23]. A common practice is to classify a document as its respective language if the score exceeds 0.5, a threshold also employed by CCNet [66]. However, during the application of this practice, we encountered a considerable number of false positives--cases where documents were erroneously filtered as non-English when, in fact, they were written in English but contained a substantial amount of mathematical symbols. We attribute this issue to the domain gap between the fastText training datasets and the mathematical content. To enhance non-English document filtering, we set customized score thresholds for each data source. Specifically, Wikipedia and StackExchange thresholds were set at 0.1, arXiv at 0.3, and Common Crawl at 0.5. No thresholds were applied to ProofWiki and Textbooks due to manual verification ensuring English content. This refinement removed about 8,400 documents, totaling 231 million tokens.

### Data Cleaning and Filtering

Despite thorough preprocessing, some documents, especially from sources like Wikipedia and Common Crawl, lack quality for language modeling due to brevity or automated content. Existing filtering methods [55; 54; 41; 51; 12], while detailed, risk excluding valuable documents in our math-focused corpus if directly applying them as-is. To address this issue, we developed a unique set of cleaning and filtering heuristic rules, specifically crafted for the mathematical domain and drawing from past studies. These rules are aimed at removing meaningless lines (such as boilerplate content) and documents. Specifically, we (1) detect lines containing "lorem ipsum" and filter them out if the resulting line is less than 5 characters; (2) detect lines containing "javascript" that also include "enable", "disable" or "browser" and are under 200 characters, and filter them; (3) filter lines containing fewer than 10 words that include keywords like "Log in", "sign-in", "read more...", or "items in cart."; (4) filter documents if the ratio of uppercase words exceeds 40%; (5) filter lines that end with "..." if they constitute more than 30% of the entire document; (6) filter documents if the

Figure 2: The creation process of MathPile. We additionally perform data contamination detection on benchmark test sets (cf. § 3.4). We visualize its component ratios by document counts (Right).

ratio of non-alphabetic words surpasses 80%; (7) exclude documents with an average English word length outside the range of (3, 10); (8) discard documents that lack at least two common stop words such as "the", "be" "to" "of" "and" "that" or "have"; (9) filter out documents if the ratio of ellipses (...) to words exceeds 0.5 (e.g., progress bars); (10) remove documents where 90% of lines start with bullet points; (11) filter documents including less than 200 characters after removing spaces and punctuation marks.

These meticulously crafted rules enabled us to curate a high-quality mathematical corpus. They also facilitated the assignment of quality annotations to each document from Wikipedia and Common Crawl. These annotations provide researchers and developers with the flexibility to filter the data according to their criteria, catering to specific needs (as shown in Figure 12). This process resulted in filtering approximately 1,100 documents, removing 17 million tokens.

### Data Deduplication

Given that our corpus originates from diverse sources, it is inevitable that there will be repetitions both within and across these sources. Deduplication is vital for training efficiency and reducing data memorization, addressing both exact and near-duplicates [33]. We utilized the MinHash LSH algorithm [22] built on the implementation of text-dup [46] and Lee et al. [33], to process large-scale corpora efficiently. Specifically, our process involved splitting each document using whitespace and constructing 5-grams, applying the "shal" hash function, and configuring 450 buckets with 20 minhashes each, totaling 9,000 minhashes per document, as per RefinedWeb's guidelines [51].

During the deduplication process within each source, we encountered numerous exact and near-duplicate documents across various sources: 304 in arXiv, 623 in Common Crawl, 83,716 in Wikipedia, 783 in textbooks (primarily synthetic), and 144 duplicate questions in StackExchange. Despite finding many near-duplicates in ProofWiki, they were differentiated as unique lemmas, proofs, or definitions, leading us to retain these entries (cf. Table 13). Manual review revealed significant duplication in Wikipedia due to collecting multiple historical document versions and in StackExchange from reposts across different forms (e.g., Math and MathOverflow) for broader visibility (cf. Table 16). We provide near-duplicate examples from each data source in Table 11-16. Cross-source deduplication revealed minimal overlap, with a single StackExchange question duplicated in Common Crawl, which was removed. This eliminated around 714 million tokens.

Note that we also experimented with using suffix arrays [43] to eliminate exact match sequences within documents. However, it tended to remove common phrases like "Questions: ". While it can effectively remove some templated content, it also disrupts the contextual integrity of our corpus. Consequently, we decided against employing this in order to preserve the context of our data.

### Data Contamination Detection

As pre-training corpora grow, encountering data contamination becomes inevitable, where evaluation examples are found in the training set. Traditionally, post-hoc analysis, employing n-gram overlap, assesses contamination levels (e.g., GPT-2 [53], GPT-3 [10], FLAN [63], LLaMA-2 [62]). We advocate for early contamination detection during dataset creation to prevent irreversible damage as delaying exacerbates issues (c.f., previous study [30]). Here, we utilize popular mathematical reasoning benchmarks, namely GSM8K [16], MATH [26], MMLU-STEM [25], AGIEval-SAT-MATH [70], MathQA [3] and AQuA [39] to detect data contamination.

To detect data contamination, we aggregated questions and answers from benchmark tests into a reference set, considering only questions for MMLU, AGIEval, MathQA and AQuA due to its multiple-choice format. Intuitively, math problem solutions often involve diverse reasoning steps, making questions easier to detect for contamination in pre-training data due to their more fixed nature. We utilized line-level exact match detection, dividing documents into lines, hashing each with MD5 (taking the first 64 bits and the line itself to form sets), and applied this to both our corpus and the test sets. If a test set line and its hash match exactly with our dataset, it's marked as contamination.

\begin{table}
\begin{tabular}{c|c c} \hline \hline
**Corpus** & **MATH** & **MMLU-STEM** \\ \hline Ours & 23 & 2 \\ OpenWebMath & 195 & 65 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Benchmark test set occurrences in pre-training corpora, with numbers representing minimum occurrences, given potential undetected duplicates.

After our detection process, we found 23 questions from MATH and 2 from MMLU-STEM in our corpus (see Table 2), with no accompanying answers. No contamination was detected in other benchmarks. These duplicates mainly originated from StackExchange, Textbooks, and Common Crawl (see Table 17 and Table 18 for examples). Notably, questions from AMC mathematics competition books, also used in the MATH benchmark, were identified in Textbooks. We extended our analysis to OpenWebMath, uncovering more duplicate questions from MATH and MMLU (cf. Table 19), although many were repeats. This aligns with similar findings by Azerbayev et al. [5]. These instances highlight the importance of vigilance in creating pre-training corpora to avoid undermining downstream benchmarks. We removed all detected exact matches to mitigate data contamination, resulting in MathPile corpus.

## 4 Data Analysis

### Statistics

We present detailed statistical information for each component of MathPile in Table 3, such as the number of documents and the count of tokens. Following our meticulous and comprehensive data collection and processing process, we obtain 29GB of high-quality and diverse math-centric corpus, encompassing around 9.5 billion tokens, from an initial volume of 2.2TB of raw data (cf. Figure 2). Compositionally, arXiv constitutes the largest portion of MathPile, while Textbooks represent the smallest share but are of exceptionally high quality.

We analyze the document length (in terms of token numbers) and their respective proportions from each source within MathPile, which is visualized in Figure 3. Intuitively, if the data from each source contains a higher amount of near-duplicates or machine-generated content, the distribution of documents of similar lengths becomes more prevalent, leading to a less smooth distribution curve. Figure 3 shows that, thanks to our thorough and rigorous processing, the document length distribution in MathPile is relatively smooth across different sources. Note that ProofWiki, due to its fixed format of definitions, lemmas, and proofs, naturally contains shorter content, resulting in a distribution with many similar lengths. We can also observe that, on average, the documents from arXiv and Textbooks tend to be lengthier, while those from ProofWiki and StackExchange are generally shorter.

### Continual Pre-training Experiments

We chose Mistral-7B-v0.1 [28] (the state-of-the-art open-source model at the time) for continual pre-training. We segmented packed text into chunks with a window size of 4,096 and continued

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline
**Components** & **Size (MB)** & **\# Documents** & **\# Tokens** & **max(\# Tokens)** & **min (\# Tokens)** & **ave (\# Tokens)** \\ \hline Textbooks & 644 & 3,979 & 187,194,060 & 1,634,015 & 256 & 47,046 \\ Wikipedia & 274 & 22,795 & 59,990,005 & 109,282 & 56 & 2,632 \\ ProofWiki & 23 & 23,839 & 7,608,526 & 6,762 & 25 & 319 \\ CommonCrawl & 2,560 & 75,142 & 615,371,126 & 367,558 & 57 & 8,189 \\ StackExchange & 1,331 & 433,751 & 253,021,062 & 125,475 & 28 & 583 \\ arXiv & 24,576 & 343,830 & 8,324,324,917 & 4,156,454 & 20 & 24,211 \\ \hline Total & 29,408 & 903,336 & 9,447,509,696 & - & - & 10,458 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The components and data statistics of MathPile.

Figure 3: Document length distribution (log-scale).

pre-training for 3 epochs with a global batch size of 1024. We employ a cosine learning rate schedule with a maximum learning rate of 1e-5 and 1% warmup steps. All experiments were conducted on NVIDIA A100 8*80GB GPUs. For evaluation, we employ a range of benchmarks - GSM8K, MATH, MMLU-MATH, AGIEval-SAT-MATH, MathQA, AQuA - to assess varying levels of mathematical reasoning abilities, comparing all models using the same few-shot prompting with greedy decoding.

**The Effectiveness of MathPile** We further pre-trained Mistral-7B-v0.1 on several subsets, respectively. As shown in Table 4, overall, continual pre-training on the subsets generally enhances performance across diverse math benchmarks, albeit to varying degrees. There are exceptions, such as the lack of improvement on GSM8K after training on StackExchange; we suspect this is due to community users rarely asking basic arithmetic questions on StackExchange. Continual pre-training on arXiv leds to a slight performance boost on GSM8K and MMLU-MATH, but a degradation on MATH, SAT-MATH, and MathQA. We attribute this performance degradation to the disparity between the math knowledge present in arXiv papers and that required for the downstream benchmarks. We also conducted pre-training on a collection of Textbooks, Wikipedia, StackExchange, and CC. Experimental results indicate improved performance on GSM8K and MATH, but not on other benchmarks. Due to limited computational resources,6 we did not extensively experiment with the entire dataset or combine data from MathPile's subsets and existing general corpora, leaving these valuable aspects for future work. Note that we also report some evaluation results on general language benchmarks provided in Appendix G.

Footnote 6: Pre-training 10 billion tokens for 1 epoch requires approximately 1,760 NVIDIA A100 GPU hours, making us keen to partner with well-resourced corporations to gain deeper insights in the future.

Furthermore, we also conducted continual pre-training on some existing corpora listed in Table 1 for comparison, including AMPS, DM-Mathematics and a random subset of OpenWebMath, cleansed of data leakage, in volumes approximately equal to that of Textbooks. Surprisingly, pre-training directly with these synthetic datasets degraded model performance. We attribute this to the narrow, monotonous structure of AMPS and DM-Mathematics problem sets, making them unsuitable for standalone pre-training; such datasets generally yield better results when combined with broader corpora for pre-training [67]. Additionally, the OpenWebMath subset produced even less improvement than the same or smaller scale subsets of MathPile, such as Textbooks and Wikipedia (cf. Table 4), likely due to a need for more tokens to show substantial gains. These results underscore the superior quality of our data.

**The Effectiveness of Data Processing Pipeline** We utilized the Wikipedia subset as a testbed to evaluate our data processing pipeline. We distinguished between raw Wikipedia, which is collected but not globally processed, and cleaned Wikipedia, which has undergone global data processing. Additionally, we performed an ablation study on LaTeX display issues in Wikipedia (cf. Figure 11), attributed to HTML-to-text conversion tools, by comparing documents with problematic and correct LaTeX displays. Following previous settings, we executed continual pre-training on these datasets.

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline
**Models** & **GSM8K** & **MATH** & **SAT-** & **MMLU-** & **MathQA** & **AQuA** \\ \hline Mistral-7B-v0.1 & 47.38 & 10.08 & 47.27 & 44.92 & 23.51 & 27.95 \\ \hline + Textbooks (0.56B) & **48.97** & **12.10** & **56.36** & **48.93** & **30.38** & **33.07** \\ + Wikipedia (0.18B) & **49.96** & **9.96** & **53.63** & **47.16** & **28.97** & **35.43** \\ + StackExchange (0.87B) & 43.05 & **11.64** & **47.27** & 43.31 & **27.67** & **30.70** \\ + Common Crawp (1.83B) & 45.36 & **9.88** & **50.45** & **45.37** & **25.79** & **31.88** \\ + arXiv (NAV) & 47.91 & 7.50 & 42.72 & **46.34** & 18.05 & 27.53 \\ + Textbooks, Wikipedia, StackEx, CC (4B) & **49.88** & **11.70** & **43.18** & 43.75 & 23.24 & 25.19 \\ \hline + AMPS (1B) & 0.08 & 0.82 & 3.18 & 0.47 & 10.99 & 8.27 \\ + DM Mathematics (5B) & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ + Sampled OpenWebMath (0.59B) & 43.21 & 7.86 & **47.72** & **47.52** & 21.80 & 24.80 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on each subset of MathPile and sampled OpenWebMath. The numbers in parentheses represent the number of tokens trained. **Bold** results denote improvements over the original Mistral.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline
**Models** & **Global Data** & **Fix Latex** & **GSM8K** & **MATH** & **SAT-** & **MMLU-** & **MathQA** & **AQuA** \\ \hline Mistral-v0.1.7B & - & - & 47.38 & 10.08 & 47.27 & 44.92 & 23.51 & 27.95 \\ \hline + Sample raw Wikipedia (0.55B) & ✗ & ✗ & 41.92 & 6.28 & 20.90 & 23.70 & 24.72 & 24.01 \\ + Full raw Wikipedia (2.18B) & ✗ & ✗ & 23.20 & 4.48 & 13.64 & 25.59 & 27.04 & 23.62 \\ + Full cleaned but LaTeX issued Wikipedia (0.23B) & ✓ & ✗ & 47.15 & 8.58 & 46.81 & 42.92 & 21.00 & 31.88 \\ \hline + Full cleaned Wikipedia (0.18B) & ✓ & ✓ & **49.96** & 9.96 & **53.63** & **47.16** & **28.97** & **35.43** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablation study on data processing pipeline and LaTeX display issue resolution Results in Table 5 indicate that skipping our pipeline notably reduces Mistral's mathematical reasoning abilities, unaffected by increased training size (i.e., 2.18B). Furthermore, correct LaTeX display in documents is vital for enhancing reasoning capabilities, as shown by the last two rows of Table 5. These findings underscore our pipeline's effectiveness and shed light on the superior importance of data quality over quantity, even in the continual pre-training phase.

## 5 Related Work

Pre-training Corpora for Language ModelsIn language modeling, early models like GPT [52] and BERT [17] are trained on resources such as Books [73] and Wikipedia. Later models like GPT-2 [53] and T5 [55] expand training to include web data from Reddit (WebText) and Common Crawl (C4). GPT-3 [10] enlarges its corpus to 300 billion tokens, combining Common Crawl, WebText, Books, and Wikipedia. Pile [20] introduces a diverse collection of 22 datasets for large-scale pre-training. The Gopher project [54] compiles a 10.5TB corpus, and PaLM [14] is built from a 780 billion-token corpus, both closed-source. BLOOM [57] uses the ROOTS dataset [32] for multilingual pre-training. The Stack Koetetkov et al. [30] provides a 3.1 TB code dataset. LLaMA [61] utilizes various data sources but doesn't release its corpus, unlike RedPajama [59] and its de-duplicated version SlimPajama [12]. RefinedWeb shows web-only corpora can rival curated ones [51]. Recent models like GPT-4 [48], Mistral-7B [28] and the lastest Gemini [58] have refrained from open-sourcing data. Constructing diverse, high-quality pre-training corpora is crucial for narrowing the performance gap with closed-source models, reflecting our work's aim.

Pre-training Benchmarks and Corpora for Mathematical ReasoningThe challenge of endowing models with human-like mathematical reasoning has attracted significant interest from the machine learning and natural language processing communities. To evaluate models' mathematical capabilities, several benchmark datasets have been developed, including AQuA [38], DM-Mathematics [56], SVAMP [50], GSM8K [16], and MATH [26], which cover a range of complexities from basic arithmetic to competition-level mathematics. Additionally, benchmarks like NaturalProofs [65] focus on theorem-proving capabilities, while the STEM subset of MMLU [25] evaluates understanding across multiple tasks in science, technology, engineering, and mathematics. To improve models' mathematical reasoning, pre-training corpora like AMPS [26] (despite a large-scale synthetic exercise set), ProofPile [4], and OpenWebMath [49] have been introduced, targeting various levels of mathematical problem-solving and theorem proving. Unlike Google's Minerva [35] and OpenAI's MathMix [37], which are not public, our work focuses on creating a high-quality and diverse mathematical corpus from diverse sources to fill existing gaps.

## 6 Conclusion and Limitations

In this work, we present MathPile, a specialized corpus centered around mathematics, characterized by its diversity and high quality. Throughout its development, we meticulously source and gather data, applying a rigorous and math-specific pipeline. This pipeline encompasses various stages such as preprocessing, prefiltering, language identification, cleaning and filtering, and deduplication, all aimed at maintaining the high quality of the corpus. We also conduct data contamination detection to remove duplicates from popular mathematical reasoning benchmark test sets, crucial for ensuring their integrity and effectiveness, an aspect often overlooked in other similar works. We aim for our MathPile to enhance mathematical reasoning in language models, whether used alone or in conjunction with other datasets, to promote broader applications.

This dataset also has some limitations. Many detailed decisions in its creation were made empirically, which may not always be optimal, and verifying decisions directly can be challenging. Moreover, the data scale is insufficient for training extra-large models; subsets like the common crawl could be expanded. Furthermore, the dataset is focused primarily on English, highlighting the need to construct high-quality datasets for other languages. Future research could also explore data mixing [40] and model-based pre-training corpus refinement [68; 72] to enhance dataset quality and model performance.

## Acknowledgments and Disclosure of Funding

This work was partially funded by the National Natural Science Foundation of China (62476168), Shanghai Artificial Intelligence Laboratory.

## References

* Allamanis [2019] Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In _Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software_, Onward! 2019, pp. 143-153, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450369954. doi: 10.1145/3359591.3359735. URL https://doi.org/10.1145/3359591.3359735.
* datasets at hugging face. https://huggingface.co/datasets/allenai/dolma, Aug 2023.
* Amini et al. [2019] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pp. 2357-2367. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1245. URL https://doi.org/10.18653/v1/n19-1245.
* Azerbayev et al. [2023] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. _CoRR_, abs/2302.12433, 2023. doi: 10.48550/ARXIV.2302.12433. URL https://doi.org/10.48550/arXiv.2302.12433.
* Azerbayev et al. [2023] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. _CoRR_, abs/2310.10631, 2023. doi: 10.48550/ARXIV.2310.10631. URL https://doi.org/10.48550/arXiv.2310.10631.
* Bandy and Vincent [2021] Jack Bandy and Nicholas Vincent. Addressing "documentation debt" in machine learning research: A retrospective datasheet for bookcorpus. _CoRR_, abs/2105.05241, 2021. URL https://arxiv.org/abs/2105.05241.
* Bender and Friedman [2018] Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. _Transactions of the Association for Computational Linguistics_, 6:587-604, 2018. doi: 10.1162/tacl_a_00041. URL https://aclanthology.org/Q18-1041.
* Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pp. 7432-7439, 2020.
* Workshop on Challenges & Perspectives in Creating Large Language Models_, pp. 95-136, virtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.
* Brown et al. [2022] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shotlearners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 20. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
* Bubeck et al. [2023] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. _CoRR_, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712. URL https://doi.org/10.48550/arXiv.2303.12712.
* cerebras. http://tinyurl.com/slimpajama, Jun 2023.
* Chern et al. [2023] Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu. Generative ai for math: Abel. https://github.com/GAIR-NLP/abel, 2023.
* Chowdhery et al. [2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanulayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _J. Mach. Learn. Res._, 24:240:1-240:113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.
* Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reitichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _CoRR_, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
* Dodge et al. [2021] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 1286-1305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emlp-main.98. URL https://aclanthology.org/2021.emnlp-main.98.
* Elazar et al. [2023] Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, and Jesse Dodge. What's in my big data? _CoRR_, abs/2310.20707, 2023. doi: 10.48550/ARXIV.2310.20707. URL https://doi.org/10.48550/arXiv.2310.20707.

* Gao et al. [2021] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. _CoRR_, abs/2101.00027, 2021. URL https://arxiv.org/abs/2101.00027.
* Gebru et al. [2021] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume III, and Kate Crawford. Datasheets for datasets. _Commun. ACM_, 64(12):86-92, nov 2021. ISSN 0001-0782. doi: 10.1145/3458723. URL https://doi.org/10.1145/3458723.
* Gionis et al. [1999] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search in high dimensions via hashing. In Malcolm P. Atkinson, Maria E. Orlowska, Patrick Valduriez, Stanley B. Zdonik, and Michael L. Brodie (eds.), _VLDB'99, Proceedings of 25th International Conference on Very Large Data Bases, September 7-10, 1999, Edinburgh, Scotland, UK_, pp. 518-529. Morgan Kaufmann, 1999. URL http://www.vldb.org/conf/1999/P49.pdf.
* Grave et al. [2018] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word vectors for 157 languages. In Nicoletta Calzolari, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga (eds.), _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1550.
* Gunasekar et al. [2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. _CoRR_, abs/2306.11644, 2023. doi: 10.48550/ARXIV.2306.11644. URL https://doi.org/10.48550/arXiv.2306.11644.
* Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjml3GmQ.
* Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual_, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd@db773eb2dc1ba0178361-Abstract-round2.html.
* Hernandez et al. [2022] Danny Hernandez, Tom B. Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Benjamin Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish. Scaling laws and interpretability of learning from repeated data. _CoRR_, abs/2205.10487, 2022. doi: 10.48550/ARXIV.2205.10487. URL https://doi.org/10.48550/arXiv.2205.10487.
* Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. _CoRR_, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https://doi.org/10.48550/arXiv.2310.06825.
* Joulin et al. [2017] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Mirella Lapata, Phil Blunsom, and Alexander Koller (eds.), _Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers_, pp. 427-431, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://aclanthology.org/E17-2068.

* [30] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 TB of permissively licensed source code. _CoRR_, abs/2211.15533, 2022. doi: 10.48550/ARXIV.2211.15533. URL https://doi.org/10.48550/arXiv.2211.15533.
* [31] Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzi-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoit Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias Muller, Andre Muller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyaken, Jamshidbels Mirzakhalov, Tapiwanashe Matangir, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisana de Silva, Saking Cabuk Ball, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled multilingual datasets. _Transactions of the Association for Computational Linguistics_, 10:50-72, 2022. doi: 10.1162/tacl_a_00447. URL https://aclanthology.org/2022.tacl-1.4.
* [32] Hugo Laurencon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen, Jorg Frohberg, Mario Sasko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Loubna Ben Allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Mariam Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Munoz, Jian Zhu, Daniel van Strien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itzair Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Vilotte Lepercq, Suzana Ilic, Margaret Mitchell, Alexandra Sasha Lucioni, and Yacine Jernite. The bigscience ROOTS corpus: A 1.6tb composite multilingual dataset. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ce9e2e3de2372a4b93355b7f3dc0bd-Abstract-Datasets_and_Benchmarks.html.
* [33] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 8424-8445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.acl-long.577.
* [34] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html.
* [35] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fd9953c3c4fe191-Abstract-Conference.html.
* [36] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need II: phi-1.5 technical report. _CoRR_, abs/2309.05463, 2023. doi: 10.48550/ARXIV.2309.05463. URL https://doi.org/10.48550/arXiv.2309.05463.
* [37] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step.

CoRR_, abs/2305.20050, 2023. doi: 10.48550/ARXIV.2305.20050. URL https://doi.org/10.48550/arXiv.2305.20050.
* Ling et al. [2017] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.
* Ling et al. [2017] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Regina Barzilay and Min-Yen Kan (eds.), _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.
* Liu et al. [2024] Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. _CoRR_, abs/2407.01492, 2024. doi: 10.48550/ARXIV.2407.01492. URL https://doi.org/10.48550/arXiv.2407.01492.
* Longpre et al. [2023] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. _CoRR_, abs/2305.13169, 2023. doi: 10.48550/ARXIV.2305.13169. URL https://doi.org/10.48550/arXiv.2305.13169.
* Luccioni and Viviano [2021] Alexandra Luccioni and Joseph Viviano. What's in the box? an analysis of undesirable content in the Common Crawl corpus. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pp. 182-189, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.24. URL https://aclanthology.org/2021.acl-short.24.
* Manber and Myers [1993] Udi Manber and Eugene W. Myers. Suffix arrays: A new method for on-line string searches. _SIAM J. Comput._, 22(5):935-948, 1993. doi: 10.1137/0222058. URL https://doi.org/10.1137/0222058.
* McMillan-Major et al. [2023] Angelina McMillan-Major, Emily M. Bender, and Batya Friedman. Data statements: From technical concept to community practice. _ACM J. Responsib. Comput._, may 2023. doi: 10.1145/3594737. URL https://doi.org/10.1145/3594737. Just Accepted.
* Mitchell et al. [2022] Margaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina McMillan-Major, Ezinwanne Ozoani, Nazneen Rajani, Tristan Thrush, Yacine Jernite, and Douwe Kiela. Measuring data. _CoRR_, abs/2212.05129, 2022. doi: 10.48550/ARXIV.2212.05129. URL https://doi.org/10.48550/arXiv.2212.05129.
* Mou et al. [2023] Chenghao Mou, Chris Ha, Kenneth Enevoldsen, and Peiyuan Liu. Chenghaomou/text-dedup: Reference snapshot, September 2023. URL https://doi.org/10.5281/zenodo.8364980.
* OpenAI [2022] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, Nov 2022.
* OpenAI [2023] OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.
* Paster et al. [2023] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. _CoRR_, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL https://doi.org/10.48550/arXiv.2310.06786.
* Patel et al. [2021] Arkil Patel, Satwik Bhattachirsha, and Navin Goyal. Are NLP models really able to solve simple math word problems? In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.

* [51] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. _CoRR_, abs/2306.01116, 2023. doi: 10.48550/ARXIV.2306.01116. URL https://doi.org/10.48550/arXiv.2306.01116.
* [52] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [53] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
* [54] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cypien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. _CoRR_, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446.
* [55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020. URL http://jmlr.org/papers/v21/20-074.html.
* [56] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=Hlg8Fi8F5K.
* [57] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamachi, Thomas Wang, Benoit Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatuji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurencon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. _CoRR_, abs/2211.05100, 2022. doi: 10.48550/ARXIV.2211.05100. URL https://doi.org/10.48550/arXiv.2211.05100.
* [58] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [59] Together. Redpajama, a project to create leading open-source models, starts by reproducing llama training dataset of over 1.2 trillion tokens. https://www.together.ai/blog/redpajama, Apr 2023.

* [60] Together. Redpajama-data-v2: An open dataset with 30 trillion tokens for training large language models. https://www.together.ai/blog/redpajama-data-v2, Oct 2023.
* [61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.
* [62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.
* [63] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL https://openreview.net/forum?id=gEZr6CozdqR.
* [64] Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. _arXiv preprint arXiv:1707.06209_, 2017.
* [65] Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hanna Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual_, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/d9d4f495e875a2e075a1a4a6e1b977f-Abstract-round1.html.
* [66] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pp. 4003-4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494.
* [67] Yiheng Xu, Hongjin SU, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. Lemur: Harmonizing natural language and code for language agents. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=hWhw5mtKRh.
* [68] Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. _arXiv preprint arXiv:2406.06046_, 2024.
* [69] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_, 2019.
* [70] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. _CoRR_, abs/2304.06364, 2023. doi: 10.48550/ARXIV.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.

* Zhou et al. [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment. _CoRR_, abs/2305.11206, 2023. doi: 10.48550/ARXIV.2305.11206. URL https://doi.org/10.48550/arXiv.2305.11206.
* Zhou et al. [2024] Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example: Lifting pre-training data quality like experts at scale. _arXiv preprint arXiv:2409.17115_, 2024.
* Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015_, pp. 19-27. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.11. URL https://doi.org/10.1109/ICCV.2015.11.

**Appendix**

A MathPile Datasheet

B Ethics Statement

C Examples of MathPile

D Details for Corpus Collection and Processing

E Example for Quality Annotation

F Examples of Duplicates Encountered in the Deduplication Process

G Evaluation of Continal Pre-trained Models on General Langauge Benchmarks

## Appendix A MathPile Datasheet

**Are relationships between individual instances made explicit?**

**Are there recommended data splits?**

**Are there any errors, sources of noise, or redundancies in the dataset?**

**Is the dataset self-contained, or does it link to or otherwise rely on external resources?**

**Does the dataset contain data that might be considered confidential?**

**Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?**

**Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?**

**Collection**

**How was the data associated with each instance acquired?**

**How was the data as-**

Our data is primarily sourced from the arXiv website and the Internet Archive. The CommonCrawl data originates from SlimPajama. The textbooks included are manually collected, with quality checks performed on publicly available textbooks from various internet sources.

**What mechanisms or procedures were used to collect the data?**

**If the dataset is a sample from a larger set, what was the sampling strategy?**

**Who was involved in the data collection process and how were they compensated?**

**Over what timeframe was the data collected?**

**Wero any ethical review processes conducted?**

**Preprocessing**

**Was any preprocessing/-cleaning/labeling of the data done?**

Yes, during our data collection phase, we conducted extensive filtering and cleansing procedures, detailed in SS 2. After the completion of data collection, we conducted further steps including language identification, additional cleaning and filtering, deduplication, and leakage detection in benchmark datasets. Subsequently, we removed any contaminated examples identified through this process. See SS 3 for details.

**Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data?**

**Is the software that was used to preprocess/clean/label the data available?**

**Uses**

**Has the dataset been used for any tasks already?**

**Is there a repository that links to any or all papers or systems that use the dataset?**

Yes, this data has been used to develop mathematical language models.

**Is there a repository that links to any or all papers or systems that use the dataset?**

Yes, this data has been used to develop mathematical language models.

**Is there a repository that links to any or all papers: (1) JiuZhang 3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models. (2) Task Oriented In-Domain Data Augmentation. (3) Great Memory, Shallow Reasoning: Limits of \(k\)NN-LMs. (4) BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts. (5) SciDFM: A Large Language Model with Mixture-of-Experts for Science. (6) MIND: Math Informed syNthetic Dialogues for Pretraining LLMs and so on.

**What (other) tasks could the dataset be used for?**

**Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?**

**Are there tasks for which the dataset should not be used?**

**Distribution**

**Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?**

**How will the dataset will be distributed?**

**When will the dataset be distributed?**

**The MathPile will be available after this paper is made public.**

**Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?**

\begin{table}
\begin{tabular}{p{113.8pt}|p{113.8pt}}
**Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?**

\begin{tabular}{p{113.8pt}|p{113.8pt}}
**Have any third parties imposed IP-based or other restrictions on the data associated with the instances?** & Not to our knowledge. \\ \hline
**Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?** & Not to our knowledge. \\ \hline
**Who will be supporting/hosting/maintaining the dataset?** & MathPile will be hosted on the HuggingFace Hub. \\ \hline
**How can the owner/curator/manager of the dataset be contacted?** & stefanpengfei@gmail.com zzwang.nlp@gmail.com \\ \hline
**Is there an erratum?** & No. \\ \hline
**Will the dataset be updated?** & Yes, it is currently a work in progress and updates are ongoing. \\ \hline
**If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?** & No. \\ \hline \end{tabular}
\end{table}
Table 6: Datasheet for MathPile, following Gebru et al. [21].

Ethics Statement

In the collection and creation of MathPile, we strictly adhered to all copyright and licensing requirements of the data sources. Specifically, we gathered a large amount of data from the internet, including mathematical textbooks, web pages, and community Q&A content, ensuring that the use of these data complies with the original licensing terms. Wikipedia, ProofWiki, and StackExchange are licensed under CC BY-SA (2.5, 3.0 or 4.0). Textbooks and arXiv are licensed under CC BY 4.0, CC BY-SA 4.0, CC BY-NC-SA 4.0 and others. Common Crawl follows the Common Crawl Foundation Terms of Use and C4 license. The final open-source MathPile dataset is released under the CC BY-NC-SA 4.0 license. If the source data's license is more restrictive than CC BY-NC-SA 4.0, we adopt the stricter license.

However, during the collection of some data, such as publicly available and open-source textbooks, we did not obtain explicit consent from each author. We recognize that this may involve potential copyright issues. Therefore, we have implemented the following measures to mitigate and manage these risks:

1. **Strict Selection of Data Sources**: We prioritize selecting data sources that are clearly marked with open licenses or public domain status, avoiding the use of content explicitly marked as copyright-protected or prohibited from distribution.
2. **Adherence to Fair Use Principles**: When using copyrighted and non-commercially licensed content, we adhere to the principles of fair use, aiming to promote scientific research and educational purposes rather than commercial purposes, thereby not affecting the market value of the original content.
3. **Acceptance of Feedback from Users and Content Authors**: We welcome feedback from data users and authors at any time to request the removal or modification of their data.

MathPile has been carefully curated and processed to minimize any potential ethical concerns. We also explicitly state that if any data owner objects to the use of their data, we are willing to take appropriate action immediately, including removing the relevant data. Through these measures, we strive to ensure the diversity and richness of the collected data while complying with relevant copyright and licensing regulations, thereby reducing potential legal risks. We bear full responsibility for any potential violations of rights or licensing issues that may arise from this dataset.

## Appendix C Examples of MathPile

We provide some illustrative examples from each source in MathPile, as shown in Figure 4 to Figure 10.

## Appendix A document from MathPile-CommonCrawl

Text:

Are there optimizers where it is possible to specify ordinal ranking of parameters?

Assume that \(f\) is smooth (\(n\)-th order differentiable in each of the parameters).

An approach I often use when applying unconstrained optimisation algorithms to constrained problems is to transform the parameter space such that the constraints cannot be violated.

Of course this results in \(\theta_{1}^{\,4}\,\geq\,\theta_{2}^{\,4}\,\geq\,\theta_{3}^{\,*}\) which isn't quite what you asked for. To get a strict ranking you'll need to bump \(x_{1}\,-\,x_{2}^{\,2}\) and \(x_{1}\,-\,x_{2}^{\,2}\,-\,x_{3}^{\,2}\) down at the last digit of precision.

Thus spike a k.k.thns spike a k.

These variants of your constraints are linear, so provided that your function \(f\) is well-behaved (smooth, easy to calculate, easy to compute derivatives, derivatives are well-conditioned, etc.), any constrained optimization solver should be able to solve your problem without issue.

Not the answer you're looking for? Browse other questions tagged optimization constrained-optimization or ask your own question.

Does the amount of correlation of model parameters matter for nonlinear optimizers?

Optimization of a blackbox function with an equality constraint?

...

Subset: CommonCrawl

meta:

language_detection_score: 0.8670,

char_num_after_normalized: 926,

contain_st_least_two_stop_words: True,

ellipsis_line_ratio: 0.0,

idx: 383668,

lines_start_with_bullet_point_ratio: 0.0,

men_length_of_alpha_words: 5.0870,

non_alphabetical_char_ratio: 0.0,

symbols_to_words_ratio: 0.0,

uppercase_word_ratio: 0.0060,

...

Figure 4: An example Common Crawl document in MathPile
## Appendix A document from MathPile -Wikipedia

Text:

# Inner Automorphism

In abstract algebra, an **inner automorphism** is an automorphism of a group, ring, or algebra given by the conjugation action of a fixed element, called the _conjugating element_. They can be realized via simple operations from within the group itself, hence the adjective "inner". These inner automorphisms form a subgroup of the automorphism group, and the quotient of the automorphism group by this subgroup is defined as the outer automorphism group.

# Definition

If \(G\) is a group and \(g\) is an element of \(G\) (alternatively, if \(G\) is a ring, and \(g\) is a unit), then the function

\[\varphi_{g}:\,G \to G\] \[\varphi_{g}(x):\,=g^{-1}xg\]

is called **(right) conjugation by \(g\)** (see also conjugacy class). This function is an endomorphism of \(G\): for all \(x_{1},x_{2}\in G\),

\[\varphi_{g}(x_{1}x_{2})=g^{-1}x_{1}x_{2}g=(g^{-1}x_{1}g)(g^{-1}x_{2}g)=\varphi_ {g}(x_{1})\varphi_{g}(x_{2}),\]

where the second equality is given by the insertion of the identity between \(x_{1}\) and \(x_{2}\). Furthermore, it has a left and right inverse, namely \(\varphi_{g-1}\). Thus, \(\varphi_{g}\) is bijective, and so an isomorphism of \(G\) with itself, i.e., an automorphism. An **inner automorphism** is any automorphism that arises from conjugation.[1] When discussing right conjugation, the expression \(g^{-1}xg\) is often denoted exponentially by \(x^{g}\). This notation is used because composition of conjugations satisfies the identity: \((x^{g_{1}})^{g_{2}}=x^{g_{1}}g^{2}\) for all \(g_{1},g_{2}\in G\). This shows that right conjugation gives a right action of \(G\) on itself.

#W Inner and Outer Automorphism Groups

The composition of two inner automorphisms is again an inner automorphism, and with this operation, the collection of all inner automorphisms of \(G\) is a group, the inner automorphism group of \(G\) denoted \(\text{Im}(G)\).

\(\text{Im}(G)\) is a normal subgroup of the full automorphism group \(\text{Aut}(G)\) of \(G\). The outer automorphism group, \(\text{Out}(G)\), is the quotient group

\[\text{Out}(G)=\frac{\text{Aut}(G)}{\text{Im}(G)}.\]

The outer automorphism group measures, in a sense, how many automorphisms of \(G\) are not inner. Every non-inner automorphism yields a non-trivial element of \(\text{Out}(G)\), but different non-inner automorphisms may yield the same element of \(\text{Out}(G)\).

Saying that conjugation of \(x\) by \(\alpha\) leaves \(x\) unchanged is equivalent to saying that \(a\) and \(x\) commute:

\[a^{-1}xa=x\iff xa=ax.\]

Therefore, the existence and number of inner automorphisms that are not the identity mapping is a kind of measure of the failure of the commutative law in the group (or ring).

An automorphism of a group \(G\) is inner if and only if it extends to every group containing \(G\).[2]

--

Subset: Wikipedia

meta:

language_detection_score: 0.7236, char_num_after_normalized: 5794, contain_it_least_two_stop_words: True, ellipsis_line_ratio: 0.0.

lines_start_with_bullet_point_ratio: 0.0, mean_length_of_split_words: 4.2245, minrepre: text/html,

page_index: 48171, page_path: \(\lambda\)/Inner_automorphism, page_title: Inner automorphism, non_alphabetical_char_index: 0.1422, symbols_to_words_ratio: 0.0, uppercase_word_ratio: 0.0871,...

Figure 5: An example Wikipedia document in MathPile

Figure 6: An example textbook document in MathPile

Figure 8: An example ProofWiki (definition) document in MathPile

Figure 7: An example ProofWiki (a theorem and its proof) document in MathPile

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_EMPTY:32]

**Ozet :**_In algebraic topology we often encounter chain complexes with extra multiplicative structure. For example, the cochain complex of a topological space has what is called the \(E_{\infty}\)-algebra structure which comes from the cup product. In this talk I present an idea for studying such chain complexes, \(E_{\infty}\) differential graded algebras (\(E_{\infty}\) DGAs), using stable homotopy theory. Namely, I discuss new equivalences between \(E_{\infty}\) DGAS that are defined using commutative ring spectra._

**ring spectra are equivalent. Quasi-isomorphic \(E_{\infty}\) DGAs are \(E_{\infty}\) topologically equivalent. However, the examples I am going to present show that the opposite is not true; there are \(E_{\infty}\) DGAs that are \(E_{\infty}\) topologically equivalent but not quasi-isomorphic. This says that between \(E_{\infty}\) DGAs, we have three equivalences than just the quasi-isomorphisms. I also discuss interaction of \(E_{\infty}\) topological equivalences with the Dyer-Lashof operations and cases where \(E_{\infty}\)topological equivalences and quasi-isomorphisms agree._

**Universite de la Saskatchewan, 1 - 4 juin 2015 www.smc.math.ca/2015f**

Comite d'organisation

Financement etudiants

Minisymposia invites

Minisymposia libres

Conferences libres

Horaire - Minisymposa invites

Open Problems

Graphs and matrices

Responsable et president: Shaun Fallat et Karen Meagher (University of Regina)

_WAYNE BARRETT, Brigham Young University_

_The Fielder Vector and Tree Decompositions of Graphs [PDF]_

_In the 1970's Fiedler initiated a study of the second smallest eigenvalue of the Laplacian matrix \(L\) of a graph and the corresponding eigenvector(s). These "Fiedler" vectors have become spectacularly successful in revealing properties of the associated graph. A tree decomposition \(T\) of a graph \(G=(V,E)\) is an associated tree whose nodes are subsets of \(V\) and whose edge set respects the structure of \(G\). Tree decompositions have been used in the analysis of complex networks. This talk reports on an algorithm developed by students at BYU for obtaining a tree decomposition by means of Fiedler vector(s) of \(G\)._

_..._

_Graphs that have a weighted adjacency matrix with spectrum \(\{\lambda_{1}^{n-2},\lambda_{2}^{2}\}\) [PDF]_

_In this talk I will characterize the graphs which have an edge weighted adjacency matrix belonging to the class of \(n\times n\) involutions with spectrum equal to \(\{\lambda_{1}^{n-2},\lambda_{2}^{2}\}\) for some \(\lambda_{1}\) and some \(\lambda_{2}\). The connected graphs turn out to be the cographs constructed as the join of at least two unions of pairs of complete graphs, and possibly joined with one other complete graph._

**Ozet :**_In algebraic topology we often encounter chain complexes with extra multiplicative structure. For example, the cochain complex of a topological space has what is called the \(E_{\infty}\)-algebra structure which comes from the cup product. In this talk I present an idea for studying such chain complexes, \(E_{\infty}\) differential graded algebras (\(E_{\infty}\) DGAs), using stable homotopy theory. Namely, I discuss new equivalences between \(E_{\infty}\) DGAs that are defined using commutative ring spectra._**We say \(E_{\infty}\) DGAs are \(E_{\infty}\) topologically equivalent when the corresponding commutative ring spectra are equivalent. Quasi-isomorphic \(E_{\infty}\) DGAs are \(E_{\infty}\) topologically equivalent. However, the examples I am going to present show that the opposite is not true; there are \(E_{\infty}\) DGAs that are \(E_{\infty}\) topologically equivalent but not quasi-isomorphic. This says that between \(E_{\infty}\) DGAs, we have more equivalences than just the quasi-isomorphisms. I also discuss interaction of \(E_{\infty}\) topological equivalences with the Dyer-Lashof operations and cases where \(E_{\infty}\) topological equivalences and quasi-isomorphisms agree._

**University of Saskatchewan, June 1 - 4, 2015 www.smc.math.ca/2015**

**Invited Minisymposia**

**Contributed Talks**

**Graphs and matrices**

**Organizer and Chair: Shaun Fallat and Karen Meagher (University of Regina)**

_WAYNE BARRETT, Brigham Young University_

_The Fielder Vector and Tree Decompositions of Graphs [PDF]_

_In the 1970's Fiedler initiated a study of the second smallest eigenvalue of the Laplacian matrix \(L\) of a graph and the corresponding eigenvector(s). These "Fiedler" vectors have become spectacularly successful in revealing properties of the associated graph. A tree decomposition \(T\) of a graph \(G=(V,E)\) is an associated tree whose nodes are subsets of \(V\) and whose edge set respects the structure of \(G\). Tree decompositions have been used in the analysis of complex networks. This talk reports on an algorithm developed by students at BYU for obtaining a tree decomposition by means of Fiedler vector(s) of \(G\)._

_..._

_Graphs that have a weighted adjacency matrix with spectrum \(\{\lambda_{1}^{n-2},\lambda_{2}^{2}\}\) [PDF]_

_In this talk I will characterize the graphs which have an edge weighted adjacency matrix belonging to the class of \(n\times n\) involutions with spectrum equal to \(\{\lambda_{1}^{n-2},\lambda_{2}^{2}\}\) for some \(\lambda_{1}\) and some \(\lambda_{2}\). The connected graphs turn out to be the cographs constructed as the join of at least two unions of pairs of complete graphs, and possibly joined with one other complete graph._

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Universite de la Saskatchewan, 1 - 4 juin 2015** & **University of Saskatchewan, June 1 - 4, 2015 www.smc.math.ca/2015** \\
**Fontain d’organisation** & **Invited Minisymposia** \\
**Minisymposia invites** & **Contributed Talks** \\
**Minisymposia libres** & **Graphs and matrices** \\
**Conferences libres** & **Organizer and Chair: Shaun Fallat and Karen Meagher (University of Regina)** \\
**Open Problems** & **WAYNE BARRETT, Brigham Young University** \\
**Graphs and matrices** & **The Fielder Vector and Tree Decompositions of Graphs [PDF]** \\
**(University of Regina)** & **In the 1970’s Fiedler initiated a study of the second smallest eigenvalue of the Laplacian matrix \(L\) of a graph and the corresponding eigenvector(s). These ”Fiedler” vectors have become spectacularly successful in revealing properties of the associated graph. A tree decomposition \(T\) of a graph \(G=(V,E)\) is an associated tree whose nodes are subsets of \(V\) and whose edge set respects the structure of \(G\). Tree decompositions have been used in the analysis of complex networks. This talk reports on an algorithm developed by students at BYU for obtaining a tree decomposition by means of Fiedler vector(s) of \(G\)._

_..._

_Graphs that have a weighted adjacency matrix with spectrum \(\{\lambda_{1}^{n-2},\lambda_{2}^{2}\}\) [PDF]_

_In this talk I will characterize the graphs which have an edge weighted adjacency matrix belonging to the class of \(n\times n\) involutions with spectrum equal to \(\{\lambda_{1}^{n-2},\lambda_{2}^{2}\}\) for some \(\lambda_{1}\) and some \(\lambda_{2}\). The connected graphs turn out to be the cographs constructed as the join of at least two unions of pairs of complete graphs, and possibly joined with one other complete graph._

\end{table}
Table 11: Near-duplication matches found in CommonCrawl by MinHash LSH deduplication (in italics).

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_EMPTY:35]

[MISSING_PAGE_FAIL:36]

[MISSING_PAGE_FAIL:37]

**This was originally posted on mathoverflow, but it seems it's more appropriate to post here.**

_Let \(B\) be a paracompact space with the property that any (topological) vector bundle \(E\to B\) is trivial. What are some non-trivial examples of such spaces, and are there any interesting properties that characterize them?_

_For simple known examples we of course have contractible spaces, as well as the 3-sphere \(S^{3}\). This one follows from the fact that its rank \(n\) vector bundles are classified by \(\pi_{3}(BO(n))=\pi_{2}(O(n))=0\). I'm primarily interested in the case where \(B\) is a closed manifold. Do we know any other such examples?_

_There is this nice answer to a MSE question which talks about using the Whitehead tower of the appropriate classifying space to determine whether a bundle is trivial or not. This seems like a nice tool (of which I am not familiar with) to approaching this problem. As a secondary question, could I ask for some insight/references to this approach? EDIT Now that we know from the answer all the examples for closed 3-manifolds (integral homology spheres). I guess I can now update the question to the case of higher odd dimensions. Does there exist a higher dimensional example?_

**This is a copy of my question on MSE (https://math.stackexchange.com/questions/3372432) because this forum seems better suited for historical questions:**

_In 1985, Gosper used the not-yet-proven formula by Ramanijan_

\[\frac{1}{\pi}=\frac{2\sqrt{2}}{99^{2}}\cdot\sum_{n=0}^{\infty}\frac{(4n)!}{(n!)^{4}}\cdot\frac{26390n+1103}{396^{4n}}\]

_to compute \(17\cdot 10^{6}\) digits of \(\pi\), at that time a new world record. Here (https://www.cs.princeton.edu/courses/archive/fall98/ cs126/refs/pi-ref.txt) it reads: There were a few interesting things about Gosper's computation. First, when he decided to use that particular formula, there was no proof that it actually converged to pi! Ramanijan never gave the math behind his work, and the Borweins had not yet been able to prove it, because there was some very heavy math that needed to be worked through. It appears that Ramanijan simply observed the equations were converging to the 1103 in the formula, and then assumed it must actually be 1103. (Ramanujan was not known for rigor in his math, or for providing any proofs or intermediate math in his formulas.) The math of the Borwein's proof was such that after he had computed 10 million digits, and verified them against a known calculation, his computation became part of the proof. Basically it was like, if you have two integers differing by less than one, then they have to be the same integer. Now my historical question: Who was the first to prove this formula? Was it Gosper because he added the last piece of the proof, or was it the Borweins, afterwards? And was Gosper aware of this proof when he did his computation?_

_Let \(B\) be a paracompact space with the property that any (topological) vector bundle \(E\to B\) is trivial. What are some non-trivial examples of such spaces, and are there any interesting properties that characterize them?_

_For simple known examples we of course have contractible spaces, as well as the 3-sphere \(S^{3}\). This one follows from the fact that its rank \(n\) vector bundles are classified by \(\pi_{3}(BO(n))=\pi_{2}(O(n))=0\). I'm primarily interested in the case where \(B\) is a closed manifold. Do we know any other such examples?_

_There is this nice answer to a MSE question which talks about using the Whitehead tower of the appropriate classifying space to determine whether a bundle is trivial or not. This seems like a nice tool (of which I am not familiar with) to approaching this problem. As a secondary question, could I ask for some insight/references to this approach? EDIT Now that we know from the answers all the examples for closed 3-manifolds, I guess I can now update the question to the case of higher odd dimensions. Does there exist a higher dimensional example?_

_In 1985, Gosper used the not-yet-proven formula by Ramanijan_

\[\frac{1}{\pi}=\frac{2\sqrt{2}}{99^{2}}\cdot\sum_{n=0}^{\infty}\frac{(4n)!}{(n!)^{4}}\cdot\frac{26390n+1103}{99^{4n}}\]

_to compute \(17\cdot 10^{6}\) digits of \(\pi\), at that time a new world record. Here (https://www.cs.princeton.edu/courses/archive/fall98/ cs126/refs/pi-ref.txt) it reads: There were a few interesting things about Gosper's computation. First, when he decided to use that particular formula, there was no proof that it actually converged to pi! Ramanijan never gave the math behind his work, and the Borweins had not yet been able to prove it, because there was some very heavy math that needed to be worked through. It appears that Ramanijan simply observed the equations were converging to the 1103 in the formula, and then assumed it must actually be 1103. (Ramanujan was not known for rigor in his math, or for providing any proofs or intermediate math in his formulas.) The math of the Borwein's proof was such that after he had computed 10 million digits, and verified them against a known calculation, his computation became part of the proof. Basically it was like, if you have two integers differing by less than one, then they have to be the same integer. Now my historical question: Who was the first to prove this formula? Was it Gosper because he added the last piece of the proof, or was it the Borweins, afterwards? And was Gosper aware of this proof when he did his computation?_

\begin{table}
\begin{tabular}{p{142.3pt}} \hline \hline
**This was originally posted on mathoverflow, but it seems it’s more appropriate to post here.** \\ _Let \(B\) be a paracompact space with the property that any (topological) vector bundle \(E\to B\) is trivial. What are some non-trivial examples of such spaces, and are there any interesting properties that characterize them?_

_For simple known examples we of course have contractible spaces, as well as the 3-sphere \(S^{3}\). This one follows from the fact that its rank \(n\) vector bundles are classified by \(\pi_{3}(BO(n))=\pi_{2}(O(n))=0\). I’m primarily interested in the case where \(B\) is a closed manifold. Do we know any other such examples?_

_There is this nice answer to a MSE question which talks about using the Whitehead tower of the appropriate classifying space to determine whether a bundle is trivial or not. This seems like a nice tool (of which I am not familiar with) to approaching this problem. As a secondary question, could I ask for some insight/references to this approach? EDIT Now that we know from the answers all the examples for closed 3-manifolds, I guess I can now update the question to the case of higher odd dimensions. Does there exist a higher dimensional example?_

\begin{tabular}{p{142.3pt}} \hline \hline
**This is a copy of my question on MSE (https://math.stackexchange.com/questions/3372432) because this forum seems better suited for historical questions:**

_In 1985, Gosper used the not-yet-proven formula by Ramanijan_

\[\frac{1}{\pi}=\frac{2\sqrt{2}}{99^{2}}\cdot\sum_{n=0}^{\infty}\frac{(4n)!}{(n!)^{4}}\cdot\frac{26390n+1103}{99^{4n}}\]

_to compute \(17\cdot 10^{6}\) digits of \(\pi\), at that time a new world record. Here (https://www.cs.princeton.edu/courses/archive/fall98/ cs126/refs/pi-ref.txt) it reads: There were a few interesting things about Gosper's computation. First, when he decided to use that particular formula, there was no proof that it actually converged to pi! Ramanijan never gave the math behind his work, and the Borweins had not yet been able to prove it, because there was some very heavy math that needed to be worked through. It appears that Ramanijan simply observed the equations were converging to the 1103 in the formula, and then assumed it must actually be 1103. (Ramanujan was not known for rigor in his math, or for providing any proofs or intermediate math in his formulas.) The math of the Borwein's proof was such that after he had computed 10 million digits, and verified them against a known calculation, his computation became part of the proof. Basically it was like, if you have two integers differing by less than one, then they have to be the same integer. Now my historical question: Who was the first to prove this formula? Was it Gosper because he added the last piece of the proof, or was it the Borweins, afterwards? And was Gosper aware of this proof when he did his computation?_

\end{table}
Table 16: Near-duplication matches found in StackExchange by MinHash LSH deduplication (in italics).

\begin{table}
\begin{tabular}{l} \hline _Coin A is flipped three times and coin \(B\) is flipped four times. What is the probability that the number of heads obtained from flipping the two fair coins is the same?_ \\ Video Solution \\ Answer: \\
**\#** Problem 3.2.2 (AMC 10) \\ Two tour guides are leading six tourists. The guides decide to split up. Each tourist must choose one of the guides, but with the stipulation that each guide must take at least one tourist. How many different groupings of guides and tourists are possible? \\ _....... One morning each member of Angela’s family drunk an 8-ounce mixture of coffee with milk. The amounts of coffee and milk varied from cup to cup, but were never zero. Angela drunk a quarter of the total amount of milk and a sixth of the total amount of coffee. How many people are in the family?_ \\ Answer: \\
**\#** Problem 20.2.15 (AMC 12) \\ The state income tax where Kristin lives is levied at the rate of \(p\%\) of the first \(\$28000\) of annual income plus \((p+2)\%\) of any amount above \(\$28000\). Kristin noticed that the state income tax she paid amounted to \((p+0.25)\%\) of her annual income. What was her annual income? \\ Answer: \\ _....... Find the least positive integer \(k\) for which the equation \(\lfloor\frac{2002}{n}\rfloor=k\) has no integer solutions for \(n\). (The notation \(\lfloor x\rfloor\) means the greatest integer less than or equal to \(x\).)_ \\ Answer: \\
**\#** Problem 40.1.9 (AIME) \\ Find the number of positive integers \(n\) less than 1000 for which there exists a positive real number \(x\) such that \(n=x\lfloor x\rfloor\).”, ”, ’Note: \(\lfloor x\rfloor\) is the greatest integer less than or equal to \(x\).’ \\ _....... What is the sum of the roots of \(z^{12}=64\) that have a positive real part?_ \\ Answer: \\
**\#** Problem 45.8.13 (AMC 12) \\ The complex numbers \(z\) and \(w\) satisfy \(z^{13}=w,w^{11}=z\), and the imaginary part of \(z\) is \(\sin\frac{m\pi}{n}\), for relatively prime positive integers \(m\) and \(n\) with \(m<n\). Find \(n\).’ \\ Answer: \\ _......._ \\ \end{tabular}
\end{table}
Table 17: Exact match examples from the test set of MATH benchmark found in Textbooks by line-level exact match deduplication (in _italics_).

\begin{table}
\begin{tabular}{l} \hline _Let \(x\) and \(y\) be real numbers satisfying \(x^{4}y^{5}+y^{4}x^{5}=810\) and \(x^{3}y^{6}+y^{3}x^{6}=945\). Evaluate \(2x^{3}+(xy)^{3}+2y^{3}\)._ \\ \hline _Let \(x_{1}<x_{2}<x_{3}\) be the three real roots of the equation \(\sqrt{2014x^{3}}-4029x^{2}+2=0\). Find \(x_{2}(x_{1}+x_{3})\)._ \\ _Let \(m\) be the largest real solution to the equation_ \\ \(\frac{3}{x-3}+\frac{5}{x-5}+\frac{17}{x-17}+\frac{19}{x-19}=x^{2}-11x-4\) \\ \hline _There are positive integers_ \(a\)_,_ \(b\)_, and_ \(c\) _such that_ \(m=a+\sqrt{b+\sqrt{c}}\)_. Find_ \(a+b+c\)_._ \\ _Let_ \(f(x)=x^{4}+ax^{3}+bx^{2}+cx+d\)_. If_ \(f(-1)=-1\)_,_ \(f(2)=-4\)_,_ \(f(-3)=-9\)_, and_ \(f(4)=-16\)_. Find_ \(f(1)\)_._ \\ _Solve in positive integers_ \(x^{2}-4xy+5y^{2}=169\)_._ \\ _Solve in integers the question_ \(x+y=x^{2}-xy+y^{2}\)_._ \\ _Solve in integers_ \(\frac{x+y}{x^{2}-xy+y^{2}}=\frac{3}{7}\) \\ _Prove the product of_ \(4\) _consecutive positive integers is a perfect square minus_ \(1\)_._ \\ _For any arithmetic sequence whose terms are all positive integers, show that if one term is a perfect square, this sequence must have infinite number of terms which are perfect squares._ \\ _Prove there exist infinite number of positive integer_ \(a\) _such that for any positive integer_ \(n\)_,_ \(n^{4}+a\) _is not a prime number._ \\...... \\ _The real root of the equation \(8x^{3}-3x^{2}-3x-1=0\) can be written in the form \(\frac{8\alpha+\frac{8\beta+1}{c}}{c}\), where \(a\), \(b\), and \(c\) are posit five integers. Find \(a+b+c\)_._ \\ _Find the number of positive integers_ \(m\) _for which there exist nonnegative integers_ \(x_{0}\)_,_ \(x_{1}\) _,...,_\(x_{2011}\) _such that_ \\ \(m^{x_{0}}=\sum_{k=1}^{2011}m^{x_{k}}\)_._ \\ _Suppose_ \(x\) _is in the interval_ \([0,\frac{\pi}{2}]\) _and_ \(\log_{24\sin x}(24\cos x)=\frac{3}{2}\)_. Find_ \(24\cot^{2}x\)_._ \\ _Let_ \(P(x)\) _be a quadratic polynomial with real coefficients satisfying_ \(x^{2}-2x+2\leq P(x)\leq 2x^{2}-4x+3\) _for all real numbers_ \(x\)_, and suppose_ \(P(11)=181\)_. Find_ \(P(16)\)_._ \\ _Let_ \((a,b,c)\) _be the real solution of the system of equations_ \(x^{3}-xyz=2\)_,_ \(y^{3}-xyz=6\)_,_ \(z^{3}-xyz=20\)_. The greatest possible value of_ \(a^{3}+b^{3}+c^{3}\) _can be written in the form_ \(\frac{m}{n}\)_, where_ \(m\) _and_ \(n\) _are relatively prime positive integers. Find_ \(m+n\)_._ \\ _Find the smallest positive integer_ \(n\) _with the property that the polynomial_ \(x^{4}-nx+63\) _can be written as a product of two nonconstant polynomials with integer coefficients._ \\ _The zeros of the function_ \(f(x)=x^{2}-ax+2a\) _are integers. What is the sum of the possible values of_ \(a\)_?_ \\ _Let_ \(a\)_,_ \(b\)_, and_ \(c\) _be three distinct one-digit numbers. What is the maximum value of the sum of the roots of the equation_ \((x-a)(x-b)+(x-b)(x-c)=0\)_?_ \\ _At the theater children get in for half price. The price for_ \(5\) _adult tickets and_ \(4\) _child tickets is_ \(24.50\)_. How much would_ \(8\) _adult tickets and_ \(6\) _child tickets cost?_ \\ _The quadratic equation_ \(x^{2}+px+2p=0\) _has solutions_ \(x=a\) _and_ \(x=b\)_. If the quadratic equation_ \(x^{2}+cx+d=0\) _has solutions_ \(x=a+2\) _and_ \(x=b+2\)_, what is the value of_ \(d\)_?_ \\ _......._ \\ _Find the smallest positive integer_ \(n\) _with the property that the polynomial_ \(x^{4}-nx+63\) _can be written as a product of two nonconstant polynomials with integer coefficients._ \\ _The zeros of the function_ \(f(x)=x^{2}-ax+2a\) _are integers. What is the sum of the possible values of_ \(a\)_?_ \\ _Let_ \(a\)_,_ \(b\)_, and_ \(c\) _be three distinct one-digit numbers. What is the maximum value of the sum of the roots of the equation_ \((x-a)(x-b)+(x-b)(x-c)=0\)_?_ \\ _At the theater children get in for half price. The price for_ \(5\) _adult tickets and_ \(4\) _child tickets is_ \(24.50\)_. How much would_ \(8\) _adult tickets and_ \(6\) _child tickets cost?_ \\ _The quadratic equation_ \(x^{2}+px+2p=0\) _has solutions_ \(x=a\) _and_ \(x=b\)_. If the quadratic equation_ \(x^{2}+cx+d=0\) _has solutions_ \(x=a+2\) _and_ \(x=b+2\)_, what is the value of_ \(d\)_?_ \\ _PolynomialAndEquation Root Delta SpecialEquation Function NumberTheoryBasic IndeterminateEquation SqueezeMethod Pythagore anTripletFormula TrigIdentity Inequality LogicalAndReasoning_ \\ _AMC10/12 AIME IMO_ \\ _US International_ \\ _With Solutions_ \\ _(c)_ _2009 - 2023 Math All Star_ \\...... \\ \hline \end{tabular}
\end{table}
Table 18: Exact match examples from the test set of MATH benchmark found in CommonCrawl by line-level exact match deduplication (in italics). In these examples, we only observe repeated questions from MATH, but do not identify duplicate answers.

\begin{table}
\begin{tabular}{l} \hline \hline _The sum of an infinite geometric series is a positive number_ \(S\)_, and the second term in the series is_ 1_. What is the smallest possible value of_ \(S\)? \\ \hline \hline
**(A)**\(\frac{1+\sqrt{S}}{2}\) **(B)**\(2\) **(C)**\(\sqrt{5}\) **(D)**\(3\) **(E)**\(4\) \\ \hline \end{tabular}
\end{table}
Table 19: Exact match examples from the test set of MATH benchmark (upper) and MMLU-STEM (bottom) found in OpenWebMath by line-level exact match deduplication (in _italics_). In these examples, we only observe repeated questions, but do not identify duplicate answers.

Evaluation of Continal Pre-trained Models on General Language Benchmarks

Does continual pre-training on MathPile lead to improvements in general language benchmarks? To explore this, we evaluated some continual pre-trained models on MathPile (in Table 4) in several representative benchmarks including PIQA [8], ARC-Easy [15], ARC-Challenge [15], SciQ [64] and HellaSwag [69]. For these evaluations, we used the infrastructure provided by EleutherAI's lm-evaluation-harness, 7 and following common practices, we report the accuracy (acc norm) metric.

Footnote 7: https://github.com/EleutherAI/lm-evaluation-harness

One important point that needs to be emphasised is that when enhancing a model's capabilities in a specific domain, it is typically necessary to mix the new domain-specific training data with the original training data, which helps to prevent catastrophic forgetting. In current experiments, we conducted continual pre-training exclusively on math-specific data, which means that we did not necessarily expect improvements in general language modeling benchmarks and, in some cases, a regression could occur. As shown in Table 20, after continual pre-training on MathPile subsets, the model's general language modeling abilities did not experience significant degradation. In fact, there were some improvements on certain benchmarks, though some metrics did see slight declines.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Models** & **PiQA** & **ARC-Challenge** & **ARC Easy** & **SciQ** & **Hallswag** \\ \hline \hline Mistral-7B & 81.93 & 53.75 & 79.58 & 94.0 & 81.05 \\ + Textbooks & 80.14 & 52.73 & 79.92 & 95.6 & 81.15 \\ + Wikipedia & 80.57 & 56.48 & 79.71 & 94.8 & 82.07 \\ + Stackexchange & 80.41 & 49.66 & 75.38 & 90.5 & 82.87 \\ \hline \hline \end{tabular}
\end{table}
Table 20: Performance of continual pre-trained models on general language benchmarks

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [No] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [No] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]