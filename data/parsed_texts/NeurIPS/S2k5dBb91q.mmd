# Transportability for Bandits

with Data from Different Environments

 Alexis Bellot, Alan Malek, Silvia Chiappa

Google DeepMind

London, UK

abellot@google.com

###### Abstract

A unifying theme in the design of intelligent agents is to efficiently optimize a policy based on what prior knowledge of the problem is available and what actions can be taken to learn more about it. Bandits are a canonical instance of this task that has been intensely studied in the literature. Most methods, however, typically rely solely on an agent's experimentation in a single environment (or multiple closely related environments). In this paper, we relax this assumption and consider the design of bandit algorithms from a combination of batch data and qualitative assumptions about the relatedness across different environments, represented in the form of causal models. In particular, we show that it is possible to exploit invariances across environments, wherever they may occur in the underlying causal model, to consistently improve learning. The resulting bandit algorithm has a sub-linear regret bound with an explicit dependency on a term that captures how informative related environments are for the task at hand; and may have substantially lower regret than experimentation-only bandit instances.

## 1 Introduction

Multi-armed bandits (MABs) constitute one of the most widely used frameworks for modeling decision-making under uncertainty. In this framework, an agent repeatedly takes actions in an environment with the goal of optimizing a desired objective, such as efficiently inferring the action with highest reward or maximizing cumulative rewards in the long run [34]. As in most reinforcement learning problems, there is a substantial amount of exploration involved while the agent learns about reward distributions under different available actions. This process can be costly in many applications; from an ethical perspective, for example, physicians may not risk compromising their patient's health with unknown treatments. It is therefore important to be efficient with experimentation while learning an optimal policy. In the literature, _structured_ bandit instances can help navigate the exploration-exploitation trade-off effectively, for example with assumptions on the functional association between action and reward that facilitate estimation such as linear bandits [1, 15, 19] and causal bandits [23, 36, 28, 27, 24, 26, 12, 29, 7].

An alternative approach to alleviate the cost of active experimentation is to consider leveraging prior data or prior experimentation in related environments to inform an agent's decision-making, which leads to the _hybrid learning_ paradigm. The expectation (or rather hope) is that informative prior data or prior experimentation can serve to narrow down reward distributions and _warm start_ the MAB so as to converge to optimal actions faster and ultimately achieve higher cumulative reward. Current methods can be categorized into multi-task learning [41, 40, 13] and meta-learning [10, 42, 4, 20, 33, 30]. The former aims to solve a prescribed set of related bandit tasks with shared structure, _e.g._ multiple player scenarios with similar reward distributions. The latter considers an arbitrary number of bandit problems whose parameters are sampled \(i.i.d.\) from a meta-prior thatcan be inferred as the agent experiments across the different related tasks1. However, both families of methods assume a relatively restricted class of potential changes across environments and rely explicitly on agent experimentation across all environments for learning. If discrepancies across environments are more general, naively leveraging prior data does not necessarily lead to more informative reward distributions or efficiency improvements in a new environment.

Footnote 1: A more extensive review of related work is given in Appendix A.

As a concrete example, consider a learning scenario in which historical data is available for the design of a clinical trial2 aiming to determine the optimal level of a hypertension treatment \(X\) for Alzheimer's disease \(Y\)3. Alzheimer's aetiology is complex but it is well established that a patient's age \(Z\) and blood pressure \(W\) contribute to the development of the disease, and so do a number of (typically) unobserved factors, _e.g._, physical activity levels, socio-economic status, diet patterns, etc. [37] (encoded with a bi-directed dashed edge). Such data can be useful but has to be handled with care, especially if we suspect the clinical trial population to differ in several aspects from that recorded in historical data. We would expect, for example, age distributions \(P(z)\) to differ. Fig. 1 graphically describes this scenario. A naive approach, ignoring the differences across populations, can be sub-optimal. Take an instance where variables \(X,Z,W,Y,U\in\{0,1\}\); their values are decided by functions: \(X\gets U,W\gets X,Y\gets Z\oplus(W\cdot U)\); \(\oplus\) represents the exclusive-or operator; \(U\) is independently distributed in \(\{0,1\}\) but older individuals are historically over-represented \(P(Z=1)=0.6\) in comparison to the clinical trial population \(P(Z=1)=0.4\). Historical data suggests that the better policy involves a lower dosage \(X=0\), as \(\mathbb{E}[Y\mid do(X=0)]=0.6>0.5=\mathbb{E}[Y\mid do(X=1)]\), which is the opposite of what is optimal for the clinical trial population.

Footnote 2: [17] refer to adaptive clinical trials as the “chief practical motivation [for the design of bandit algorithms]”.

Footnote 3: For this example, \(Y\) a measured biomarker of Alzheimer’s disease that acts as a designated reward variable to be maximized.

This example shows that differences across environments may be complex, subtle, and non-trivially influence optimal decision-making. Even when one is able to perfectly estimate reward distributions from historical data, the induced policy can still be sub-optimal depending on the location and magnitude of the changes expected across environments. In general, reward distributions will not straightforwardly extrapolate across different environments. In this paper, we attempt to capture this (structural) uncertainty through a causal lens. In the causality literature, this problem appears under the rubric of _transportability theory_[32; 3; 11]; several criteria, algorithms, and estimation methods have been developed for identifying when and how a causal effect can be computed across environments. Our task, in the bandit setting, is to define a learning agent that optimally exploits prior data with knowledge of the potential discrepancies across domains, for _any_ given graph. Prior work [43] has considered specific instances, _i.e._ environments defined by the Bow and Instrumental Variables graphs, but a general approach applicable to arbitrary graphs and arbitrary differences across environments is still missing. Our approach is Bayesian, and involves posterior sampling of reward distributions defined by a parameterization informed by the underlying causal structure. Our contribution is to develop a novel bandit algorithm that achieves sub-linear cumulative regret with an explicit dependency on the entropy of an inferred prior, a quantity that implicitly captures the relatedness between environments. The significance of this result is that it guarantees consistent improvements on performance over methods not leveraging prior data. To the best of our knowledge, this is one of the first general attempts to consistently use prior data from related environments in general decision-making scenarios in which causal dependencies can be established.

### Preliminaries

We use capital and small letters to denote random variables and their values respectively, _e.g._\(X\) and \(x\), and bold capital and small letters to denote sets of variables and their values, _e.g._\(\bm{X}\) and \(\bm{x}\). The domain of variable \(X\) is indicated with \(\Omega_{X}\).

A environment's data generating mechanism is described by a _structural causal model_ (SCM) [31, Definition 7.1.1]. A SCM \(M\) is a tuple \(\langle\bm{V},\bm{U},\mathcal{F},P(\bm{U})\rangle\), where \(\bm{V}\) is a set of endogenous (observed) variables, \(\bm{U}\) is a set of exogenous latent variables, and \(\mathcal{F}=\{f_{V}\}_{V\bm{e}\bm{V}}\) is a set of functions

Figure 1: Diagram encoding causal structure and differences across environments.

such that \(f_{V}\) determines values of \(V\) taking as argument variables \(\bm{Pa}_{V}\subseteq\bm{V}\) and \(\bm{U}_{V}\subseteq\bm{U}\), _i.e._\(V\gets f_{V}(\bm{Pa}_{V},\bm{U}_{V})\). Values of \(\bm{U}\) are drawn from an exogenous distribution \(P(\bm{u})\). We assume the model to be recursive, _i.e._ that there are no cyclic dependencies among the variables, such as to define a distribution \(P(\bm{V})\) over endogenous variables \(\bm{V}\). An intervention or action by an agent on a subset \(\bm{X}\subset\bm{V}\), denoted by \(do(\bm{x})\), is an operation that fixed values of \(\bm{X}\) to constants \(\bm{x}\), replacing the functions \(\{f_{X}:X\in\bm{X}\}\) that would normally determine their values. Let \(M_{\bm{x}}\) denote the model induced by action \(do(\bm{x})\). Accordingly, \(M_{\bm{x}}\) induces a corresponding interventional distribution over \(\bm{V}\), denoted \(P(\bm{V}_{\bm{x}}):=P(\bm{V}\mid do(\bm{x}))\). We will consistently use \(X,Y\in\bm{V}\) as designated action and reward variables, respectively.

Causal graphs \(\mathcal{G}=(\bm{V},\mathcal{E})\) describe the functional associations in an underlying SCM \(M\). In particular, we draw a _directed edge_ between two variables \(V\to W\in\mathcal{E}\) if \(V\) appears as an argument of \(f_{W}\) in \(M\), and a _bi-directed dashed edge_ between two variables \(V\leftarrow\)\(\cdots\)\(W\in\mathcal{E}\) if \(\bm{U}_{V}\cap\bm{U}_{W}\neq\emptyset\), _i.e._\(V\) and \(W\) share an unobserved _confounder_. We will use standard family conventions for graphical relationships, _e.g._ parents \(pa(\bm{X})_{\mathcal{G}}:=\cup_{X\in\bm{X}}pa(X)_{\mathcal{G}}\) of a set of nodes \(\bm{X}\subseteq\bm{V}\) are all nodes in \(\mathcal{G}\) with directed edges into elements of \(\bm{X}\). Its capitalized version \(Pa\) includes the argument as well, _e.g._\(Pa(\bm{X})_{\mathcal{G}}:=pa(\bm{X})_{\mathcal{G}}\bigcup\bm{X}\). We will make use a special clustering of the nodes in \(\bm{V}\) called \(c\)-components [39]: two nodes are in the same \(c\)-component \(\bm{C}\subseteq\bm{V}\) if and only if they are connected by a bi-directed path. \(c\)-components form a partition over exogenous variables: a \(c\)-component \(\bm{C}\subseteq\bm{V}\) is said to cover an exogenous variable \(U\) if \(U\in\bigcup_{V\in\bm{C}}\bm{U}_{V}\). We denote with \(\bm{C}_{U}\) the \(c\)-component covering \(U\). As an example, the diagram in Fig. 1 has \(c\)-components \(\{X,Y\}\), \(\{W\}\), and \(\{Z\}\); and \(\bm{C}_{U_{XY}}=\{X,Y\}\), \(\bm{C}_{U_{W}}=\{W\}\), and \(\bm{C}_{U_{Z}}=\{Z\}\). We refer the reader to [31, Chapter 7] for a more detailed review of SCMs.

## 2 Bandits with Transportability

From the agent's perspective, the point of departure with respect to conventional bandit instances is that in addition to the ability to take actions in a deployment environment \(\pi^{*}\), the agent has access to data from one or more related environments \(\pi^{a},\pi^{b},\ldots\), each characterized by SCMs \(M^{a},M^{b},\ldots\). We assume that all environments have the same scope, _i.e._ the same sets \(\bm{V}\) and \(\bm{U}\), but may differ in _any_ other aspect. In the transportability literature [3, 32], such structural differences between environments are called domain discrepancies and can be encoded in selection diagrams.

**Definition 1** (Domain Discrepancy).: _Let \(\pi^{a}\) and \(\pi^{b}\) be two domains with SCMs \(M^{a}\) and \(M^{b}\). There exists a domain discrepancy between \(\pi^{a}\) and \(\pi^{b}\) if \(f_{V}^{a}\neq f_{V}^{b}\) or \(P^{a}(\bm{U}_{V})\neq P^{b}(\bm{U}_{V})\) for some \(V\in\bm{V}\)._

**Definition 2** (Selection diagram).: _Given domain discrepancy set \(\Delta^{a,b}:=\{V\in\bm{V}:f_{V}^{a}\neq f_{V}^{b}\text{ or }P^{a}(\bm{U}_{V})\neq P^{b}(\bm{U}_{V})\}\) between two domains \(\pi^{a}\) and \(\pi^{b}\) and a causal graph \(\mathcal{G}^{a}=(\bm{V},\mathcal{E})\), let \(\bm{S}=\{S_{V}:V\in\Delta^{a,b}\}\) be called selection nodes. The graph \(\mathcal{G}^{a,b}=(\bm{V}\cup\bm{S},\mathcal{E}\cup\{S_{V}\to V\}_{S_{V}\in \bm{S}})\) is called selection diagram._

Selection nodes indicate where structural discrepancies between two environments might take place. The absence of a selection node pointing to a variable represents the assumption that the causal mechanism responsible for assigning values to that variable is identical in both environments. In the clinical trial example, Fig. 1 shows a selection diagram comparing historical and clinical trial environments, denoted \(\pi^{*},\pi^{a}\) respectively; the presence of selection node \(S_{Z}\) indicates a potential difference in the assignment of \(Z\), _i.e._, either \(f_{Z}^{*}\neq f_{Z}^{a}\) and / or \(P^{*}(\bm{u}_{W})\neq P^{a}(\bm{u}_{W})\). On the other hand, the absence of _e.g._ selection node \(S_{Y}\) indicates the assumption \(f_{Y}^{*}=f_{Y}^{a}\) and \(P^{*}(\bm{u}_{Y})=P^{a}(\bm{u}_{Y})\). With this formalism, the task is to leverage data from related environments in a consistent and efficient manner.

**Definition 3** (Bandits with Transportability).: _Let \(\pi^{*}\) denote the deployment environment in which the agent acts. Given samples from \(P^{a}(\bm{V}),P^{b}(\bm{V}),\ldots\) and selection diagrams \(\mathcal{G}^{*,a},\mathcal{G}^{*,b},\ldots\), in each round \(t=1,\ldots,T\) the agent takes an action \(x^{(t)}\) and observes a sample from \(P^{*}(\bm{V}_{x^{(t)}})\), adjusting its actions to minimize (expected) cumulative regret in \(\pi^{*}\),_

\[\mathbb{E}_{P^{*}}R_{T}:=\sum_{t=1}^{T}\mathbb{E}_{P^{*}}Y_{\tilde{x}}-\mathbb{E }_{P^{*}}Y_{x^{(t)}},\] (1)

_that compares the optimal intervention \(\tilde{x}=\text{arg max}_{x\in\Omega_{X}}\mathbb{E}_{P^{*}}Y_{x}\) with the agent's chosen intervention in each round._Quantities such as \(\mathbb{E}_{P^{*}}Y_{x}\) or \(P^{*}(y_{x})\) are called _transportability queries_ and their estimation with prior data, underlying structural assumptions and their use within active experimentation schemes will be the focus of this paper.

### Informative priors for bandits

Before experimentation takes place there is a degree of _unidentifiability_ of reward distributions \(P^{*}(y_{x})\) depending on causal assumptions and discrepancies between environments. For instance, revisiting the clinical trial example, if age distributions are allowed to vary arbitrarily across environments, values of \(P^{*}(y_{x})\) will similarly vary and thus involve a degree of uncertainty4. This unidentifiability feature is relevant even without major discrepancies across domains as \(P^{*}(y_{x})\) may still be unidentifiable in the presence of unobserved confounders. The Bow graph in Fig. 2 (ignoring the selection node) is a common example.

Footnote 4: Formally, we say there exists multiple SCMs \(M_{1},M_{2}\in\mathcal{M}\) with functional dependencies defined by \(\mathcal{G}^{*}\), consistent with prior data and selection diagrams, such that \(P_{M_{1}}(y_{x})\neq P_{M_{2}}(y_{x})\).

One may be tempted to conclude that prior data is rarely useful. However, even under multiple discrepancies across environments, causal effects \(P^{*}(y_{x})\) are rarely completely unconstrained. In general, causal effects lie in a non-trivial interval \([a,b],0<a\leq b<1\). For instance, for the graph \(\mathcal{G}^{*,*}\) in Fig. 2\(P^{*}(y_{x})\) can be shown to be contained in \(\big{[}P^{a}(x,y),P^{a}(x,y)+1-P^{a}(x)\big{]}\). In particular, with a probabilistic or Bayesian interpretation of unknown quantities in a SCM, that is with an explicit probability measure over SCMs \(M\in\mathcal{M}(\mathcal{G}^{*})\)5, one can define a _distribution_ of reward probabilities \(P^{*}(y_{x})\) (or expected rewards) that honestly captures prior uncertainty in their values. For example, given prior samples \(\bar{\bm{v}}^{a}=\{\bm{v}^{a}_{(j)}:j=1,\ldots,500\}\) independently drawn from a source distribution \(P^{a}(\bm{V})\) (and a particular prior model over SCMs, to be discussed in Sec. 2.2), a posterior density over \(\mathbb{E}_{P*}Y_{x}\) can be evaluated and sampled from, see Fig. 2 for an illustration, where theoretical bounds are shown with vertical lines.

Footnote 5: \(\mathcal{M}(\mathcal{G}^{*})\) stands for the set of SCMs whose functional dependencies are given by \(\mathcal{G}^{*}\).

From this perspective, the problem of doing inference on reward distributions given prior data and knowledge of structural discrepancies can be formulated as an optimization problem, that is, evaluate,

\[P_{M\sim\mathcal{M}(\mathcal{G}^{*})}\Big{(}\,\mathbb{E}_{P_{M}}\big{[}Y_{x} \big{]}\;\;|\;\;\bar{\bm{v}}\,\Big{)},\quad\text{such that}\quad\forall V\notin \Delta^{*,i}:f^{*}_{V}=f^{i}_{V},P_{\pi^{*}}(\bm{u}_{V})=P_{\pi^{i}}(\bm{u}_{V }),\] (2)

where \(\bar{\bm{v}}:=\{\bar{\bm{v}}^{a},\bar{\bm{v}}^{b},\ldots\},\bar{\bm{v}}^{i}=\{ \bm{v}^{i}_{(j)}:j=1,\ldots,n_{i}\}\) is a set of \(n_{i}\) independent samples from \(P^{i}(\bm{V})\). In words, the task is to evaluate a distribution over expected rewards under intervention over all deployment domains \(\pi^{*}\) compatible with our knowledge prior to experimenting in \(\pi^{*}\).

It remains a question, however, how to define a model and parameterization of SCMs. Remember that only prior data and selection diagrams are assumed to be available to the researcher; any choices on the distribution of exogenous variables \(P(\bm{U})\) or functional form of deterministic structural assignments \(\mathcal{F}\) represent untestable assumptions that are difficult to justify in practice. Going forward we will restrict ourselves to SCMs with _discrete_ endogenous variables (while exogenous variables may be arbitrarily defined, _e.g._ continuously-valued with arbitrary probability density functions).

### General parameterization of reward distributions

Systems of discrete observables have the distinctiveness of involving a finite number of probabilities of the form \(P^{*}(y_{x})\), _i.e._ one for each combination \((x,y)\). Reward distributions \(P^{*}(y_{x})\) in any underlying SCM \(M\), however complex \(P(\bm{U})\) and \(\mathcal{F}\) may be, can logically be equivalently expressed by a corresponding _discrete_ SCM \(N\) in which \(P(\bm{U})\) is discrete and \(\mathcal{F}\) is a discrete mapping between finite spaces [44; 6]. This observation is interesting because it allows us to consistently and uniquely parameterize \(P(\bm{U})\) and \(\mathcal{F}\), without untestable choices on their form6.

Footnote 6: A similar reasoning does not apply for continuous endogenous variables that would require continuous exogenous variables and therefore a (untestable) choice of parametric family for all variables.

Figure 2: Bow graph and posterior reward density.

**Corollary 1** (Proposition 2.7. [44]).: _For any causal graph \(\mathcal{G}\), let \(M\) be an arbitrary SCM compatible with \(\mathcal{G}\). For any sets \(\bm{Y},\bm{X}\subset\bm{V}\), the interventional distribution \(P(\bm{y_{x}})\) could be parameterized as_

\[\sum_{\bm{v}\setminus\{\bm{x}\cup\bm{y}\}}\sum_{\begin{subarray}{c}u=1,\ldots, d_{U},\\ U\in\bm{U}\end{subarray}}\prod_{V\in\bm{V}}\mathbbm{1}\{\xi_{V}^{(\bm{p_{\bm{u} }}_{V},\bm{u}_{V})}=v\}\prod_{U\in\bm{U}}\theta_{u},\] (3)

_where \(\theta_{u}:=P(U=u)\) defines exogenous probabilities of discrete variables \(U\in\bm{U}\) with cardinality \(d_{U}=\prod_{V\in\mathbf{C}_{U}}\left|\Omega_{\bm{P_{\bm{a}}}(V)}\right|\); and each \(\xi_{V}^{(\bm{p_{\bm{a}}}_{V},\bm{u}_{V})}\) is a deterministic mapping between finite domains \(\Omega_{\bm{p_{\bm{a}}}_{V}}\times\Omega_{\bm{U}_{V}}\mapsto\Omega_{V}\)._

For example, \(P^{*}(y_{x})\) in Fig. 1 can be parameterized by

\[\sum_{w,z,u_{z},u_{w},u_{xy}}\mathbbm{1}\{\xi_{Y}^{(w,z,u_{xy})}=y\}\mathbbm{1 }\{\xi_{W}^{(x,u_{w})}=w\}\mathbbm{1}\{\xi_{Z}^{(u_{z})}=z\}\theta_{u_{xy}} \theta_{u_{z}}\theta_{u_{w}},\] (4)

where, assuming \(X,Y,Z,W\) are binary, \(\theta_{u_{z}}\) is a discrete distribution over a finite domain \(\{1,2\}\) since \(|\Omega_{U_{z}}|=|\Omega_{Pa(Z)}|=|\Omega_{Z}|=2\), \(\theta_{u_{w}}\) is a distribution over \(\{1,\ldots,4\}\) since \(|\Omega_{U_{W}}|=|\Omega_{Pa(W)}|=|\Omega_{X}|\cdot|\Omega_{Z}|=4\), and \(\theta_{u_{xy}}\) is distribution over \(\{1,\ldots,32\}\) since \(|\Omega_{U_{X}Y}|=|\Omega_{Pa(X)}|\cdot|\Omega_{Pa(Y)}|=|\Omega_{Z}|\cdot| \Omega_{X}|\cdot|\Omega_{W}|\cdot|\Omega_{Z}|\cdot|\Omega_{Y}|=32\). Cocl. 1 guarantees that for any value of \(P^{*}(y_{x})\) induced by an arbitrary SCM \(M\) there exists a combination of parameters in Eq. (4) that reaches that exact same value. In other words, this parameterization is sufficiently expressive to encode _any_ underlying reward distribution.

Parameters that define reward distributions are specific to the deployment environment \(\pi^{*}\). The key observation, however, is that some of them can be inferred with prior data whenever there exists an invariance in \(P(\bm{U})\) or \(\mathcal{F}\) across environments as there exists a one-to-one relationship between model parameters and structural features of the underlying SCM. For example, given Fig. 1, the absence of a selection node into \(Y\) implies that both functional assignments and exogenous probabilities of \(Y\) agree across environments, that is \(\xi_{Y}^{*}=\xi_{Y}^{*},\theta_{u_{Y}}^{*}=\theta_{u_{Y}}^{*}\). Both may thus be approximated with prior data which in turn constraints or informs \(P^{*}(y_{x})\) even if other parameters in its expression in Eq. (4) remain unknown. The result is a non-trivial distribution over reward probabilities that may be used to warm-start bandit algorithms, even before any experimentation takes place.

### Bandit algorithms

To exploit non-trivial parameter distribution given prior data, a bandit algorithm can be designed to choose actions in proportion to the probability that an intervention leads to highest reward, also known as posterior or Thompson sampling [38; 2]. Specifically, at a particular round \(t\) of experimentation in the deployment domain \(\pi^{*}\), _posterior_ parameter distributions \(P\left(\bm{\xi},\bm{\theta}\mid\bar{\bm{v}},\bm{v}_{x^{(1)}},\ldots,\bm{v}_{ x^{(t-1)}}\right)\) can be evaluated to exactly capture uncertainty given both prior and experimental data up to round \(t\). Action \(x^{(t)}\) is then chosen according to the one that gives highest reward, _i.e._ arg max \({}_{x}\)\(\mathbb{E}_{P^{*}}\left[Y_{x}\mid\bm{\xi}^{(t)},\bm{\theta}^{(t)}\right]\), where \((\bm{\xi}^{(t)},\bm{\theta}^{(t)})\) is an independent draw from its posterior distribution. In other words, the agentperforms natural Bayesian updates based on both the data available in source environments and its own experimentation as interventional samples \(\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t-1)}}\) become available, matching the intuition of most other Thompson sampling bandits in the literature. The full algorithm, called Thompson sampling with Transportability (tTS), is given in Alg. 1.

## 3 Regret guarantees conditional on prior data

We define information-theoretic regret bounds that aim to capture the exploration-exploitation trade-off for Alg. 1 when prior information allows it to infer parts of the environment before experimentation takes place.

Performance in MABs is, to a large extent, intimately related with the agent's uncertainty about which action is optimal, represented by a random variable \(\tilde{X}:\Omega_{\bm{\xi}}\times\Omega_{\bm{\theta}}\mapsto\Omega_{X}\) where \(\Omega_{\bm{\xi}}\times\Omega_{\bm{\theta}}\) defines the space of all models \(\mathcal{M}(\mathcal{G}^{*})\) consistent with our knowledge of the deployment environment \(\pi^{*}\). For example, \(P_{M\sim\mathcal{M}(\mathcal{G}^{*})}(\tilde{X}=x)=P_{M\sim\mathcal{M}( \mathcal{G}^{*})}(\mathbb{E}_{P_{M}}[Y_{x}]>\mathbb{E}_{P_{M}}[Y_{x^{\prime}}],\forall x^{\prime}\in\Omega_{X}\backslash\{x\})\) where \(P_{M\sim\mathcal{M}(\mathcal{G}^{*})}\) is a probability mass function defined over \(\mathcal{M}(\mathcal{G}^{*})\). It is reasonable to assume that one would only choose actions with large regret when it can reduce the uncertainty in \(\tilde{X}\) substantially. Following [35, Sec. 5], we define a scalar \(\Gamma_{t}\)8 such that the per-round regret can be bounded by information gain,

Footnote 8: \(\Gamma_{t}\) is called the information ratio and quantifies the trade-off between incurring low regret and gaining information about the optimal action. \(\Gamma_{t}\) can always be upper-bounded by \(|\Omega_{X}|/2\)[35, Sec. 5]. We provide a derivation of a bound for \(\Gamma_{t}\) for a specific set of environments as an example in Appendix C.3.

\[\mathbb{E}[Y_{\tilde{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\leq \Gamma_{t}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}\left(\tilde{X}; \bm{V}_{X^{(t)}}\right)},\] (5)

where \(\bar{\bm{V}}_{t}:=\{\bm{V}_{X^{(1)}},\ldots,\bm{V}_{X^{(t-1)}}\}\) denotes the agent's history of interactions with \(\pi^{*}\) up to round \(t\), and \(I_{P}(X,Y):=\mathcal{D}_{KL}(P(X,Y)\|P(X)P(Y))\) denotes the filtered mutual information defined based on \(P\) (where \(\mathcal{D}_{KL}\) is the Kullback-Leibler divergence). Expectations, unless otherwise stated, are taken with respect to all random quantities. The following proposition, extended from [35, Prop. 1], shows that the Bayesian regret of an agent acting according to Alg. 1 is sub-linear with a dependency on the entropy of the optimal action \(\tilde{X}\).

**Proposition 1**.: _Let \(R_{T}\) denote the regret incurred by following Thompson sampling (Alg. 1). For any \(T\in\mathbb{N}\) and \(\Gamma\geq\Gamma_{t}\), then_

\[\mathbb{E}[R_{T}\mid\bar{\bm{v}}]\leq\Gamma\sqrt{\mathcal{H}(\tilde{X}\mid \bar{\bm{v}})T},\] (6)

_where \(\mathcal{H}(\tilde{X}\mid\bar{\bm{v}})\) is the conditional entropy of \(\tilde{X}\) given \(\bar{\bm{v}}\)._

Proofs are given in Appendix C.

This bound is interesting because it cleanly relates the regret with the uncertainty about the optimal action conditioned on prior data. On one extreme, if data from source environments fully characterizes the optimal action, the entropy equals 0, and no further experimentation is required; on the other extreme, if data from source environments have no relationship with the target query, the entropy equals \(\log(|\Omega_{X}|)\), and the bound reverts to conventional worst-case guarantees [35]. The entropy of the optimal action is often not sufficient to capture the information from \(\bar{\bm{v}}\) as \(\tilde{X}\) may still have a uniform distribution even though posterior distributions over \((\bm{\theta},\bm{\xi})\) have tightened. In other words, there is additional structure among different reward distributions that is not captured by the entropy of the optimal action. Such a setting can be analyzed with a different assumption on the per round regret that explicitly considers model parameters \((\bm{\theta},\bm{\xi})\) to quantify information gain,

\[\mathbb{E}[Y_{\tilde{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\leq \Gamma_{t}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}\left(\bm{\theta}, \bm{\xi};\bm{V}_{X^{(t)}}\right)}+\epsilon_{t}.\] (7)

where \(\epsilon_{t}>0\) is an additional slack term. Accordingly, the following proposition provides an alternative bound using a conditional analogue of [25, Prop. 2].

**Proposition 2**.: _Let \(R_{T}\) denote the regret incurred following the policy defined by Alg. 1. For any \(T\in\mathbb{N}\), if Eq. (7) holds with \(\Gamma\geq\Gamma_{t}\) for all \(t\),_

\[\mathbb{E}[R_{T}\mid\bar{\bm{v}}]\leq\Gamma\sqrt{TI_{P(\cdot\mid\bar{\bm{v}} )}\left(\bm{\theta},\bm{\xi};\bm{V}_{X^{(1)}},\ldots,\bm{V}_{X^{(T)}}\right)}+ \sum_{t=1}^{T}\mathbb{E}[\epsilon_{t}].\] (8)This proposition shows that if prior data allows the agent to concentrate \((\bm{\theta},\bm{\xi})\) around some value, additional experimentation does not provide much more information and the regret should be small. In principle, it is possible to get precise per-round regret values by inferring values for \(\Gamma_{t}\) and \(\epsilon_{t}\) through the construction of concentration inequalities for the reward variable, as done by Lu et al. in [25, Lem. 3]. We adapt this result using conditional information-theoretic quantities in the following proposition.

**Proposition 3**.: _Fix \(\delta>0\) and choose \(\Gamma_{t}\) such that \(\big{|}Y_{x}-\mathbb{E}[Y_{x}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\big{|}\leq \frac{\Gamma_{t}}{2}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(\bm{ \theta},\bm{\xi};Y_{x})}\) for all \(x\in\Omega_{X}\) simultaneously with probability greater than \(1-\delta\). Then Alg. 1 chooses actions \(X^{(t)}\) that satisfy_

\[\mathbb{E}[Y_{\hat{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\leq \Gamma_{t}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(\bm{\theta},\bm{ \xi};\bm{V}_{X^{(t)}})}+\delta B,\] (9)

_where \(B\geq 0\) is such that \(\sup_{y,y^{\prime}\in\Omega_{Y}}y-y^{\prime}\leq B\)._

## 4 Posterior approximations

This section describes a tractable algorithm to evaluate posterior distributions of the form \(P(\bm{\xi},\bm{\theta}\mid\bar{\bm{v}},\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t-1 )}})\) and its posterior updates when new data is collected. Priors on \(\bm{\xi},\bm{\theta}\) may be defined such as to induce tractable conditional distributions that may be used within a Gibbs sampling framework. The Gibbs sampler starts with some initial value for all latent quantities \((\bm{U},\bm{\xi},\bm{\theta})\) in our target expected reward

\[\mathbb{E}_{P}\bm{*}[Y_{x}]=\sum_{y\in\Omega_{Y}}yP^{\bm{*}}(Y_{x}=y)=\sum_{ \bm{v}\setminus\{x\}}y\sum_{\begin{subarray}{c}u=1,\ldots,d_{U},\\ U\in\bm{U}\end{subarray}}\prod_{V\in\bm{V}\setminus X}\mathbbm{1}\{\xi_{V}^{( \bm{pa}_{V},\bm{u}_{V})}=v\}\prod_{U\in\bm{U}}\theta_{u},\] (10)

and samples each one iteratively using their conditional distributions, each parameter conditioned on the current values of the remaining terms in the parameter vector and the available data [16]. As mentioned, what data point carries information about which parameters depends on the structural differences between environments.

**Prior.** For every \(V\in\bm{V},\forall p\bm{a}_{V},\bm{u}_{V}\), the functional assignment parameters \(\xi_{V}^{(\bm{pa}_{V},\bm{u}_{V})}\) are drawn uniformly in the discrete domain \(\Omega_{V}\). For every \(U\in\bm{U}\), exogenous probabilities \(\bm{\theta}_{U}\) with dimension \(d_{U}=\prod_{V\in\mathbf{C}_{U}}\big{|}\Omega_{P_{a}(V)}\big{|}\) are drawn from a prior Dirichlet distribution (here chosen for conjugacy with the categorical distribution of \(\bm{U}\)), \(\bm{\theta}_{U}=(\theta_{1},\ldots,\theta_{d_{U}})\sim Dir(\alpha_{1},\ldots, \alpha_{d_{U}})\), with hyperparameters \(\alpha_{1},\ldots,\alpha_{d_{U}}\).

**Posterior.** In a particular round \(t\), the Gibbs sampler iterates over the following steps.

1. _Sample_ \(\bm{u}\). We start by sampling a corresponding exogenous latent variable for each observed sample \(\bm{v}^{(n)}\in(\bar{\bm{v}},\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t-1)}}),n=1, \ldots,t-1+\sum_{i}n_{i}\). Let \((\bar{\bm{u}},\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})\) denote the corresponding set of samples of \(\bm{U}\). Exogenous variables \(\bm{U}^{(n)}\) are mutually independent given \(\bm{V}^{(n)},\bm{\xi},\bm{\theta}\) and thus we can sample each separately using the conditional \[P(\bm{u}^{(n)}\mid\bm{v}^{(n)},\bm{\xi},\bm{\theta})\propto P(\bm{u}^{(n)}, \bm{v}^{(n)}\mid\bm{\xi},\bm{\theta})=\prod_{V\in\bm{V}}\mathbbm{1}\{\xi_{V}^{ (\bm{pa}_{V}^{(n)},\bm{u}_{V}^{(n)})}=v^{(n)}\}\prod_{U\in\bm{U}}\theta_{u}.\] (11)
2. _Sample_ \(\bm{\xi}\). Similarly, for fixed \(p\bm{a}_{V},\bm{u}_{V}\), parameters \(\xi_{V}^{(\bm{pa}_{V},\bm{u}_{V})}\) are mutually independent given \(\bar{\bm{v}},\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t-1)}},\bar{\bm{u}},\bm{u}_{x ^{(1)}},\ldots,\bm{u}_{x^{(t-1)}},\bm{\theta}\). As mentioned, each parameter is updated with the subset of \((\bar{\bm{v}},\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t-1)}},\bar{\bm{u}},\bm{u}_{x ^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})\) associated with environments in which the functional assignment of \(V\) is invariant across source and deployment environments. As they represent a mapping between variables, its conditional distribution is given by \(P(\xi_{V}^{(\bm{pa}_{V},\bm{u}_{V})}=v\mid\bar{\bm{v}},\bm{v}_{x^{(1)}},\ldots, \bm{v}_{x^{(t-1)}},\bar{\bm{u}},\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})=1\) if there exists a (relevant) sample \((v^{(n)},\bm{pa}_{V}^{(n)},\bm{u}_{V}^{(n)})\) that fixes the mapping \(\bm{pa}_{V}^{(n)},\bm{u}_{V}^{(n)}\mapsto v^{(n)}\). Otherwise, \(P(\xi_{V}^{(\bm{pa}_{V},\bm{u}_{V})}=v\mid\bar{\bm{v}},\bm{v}_{x^{(1)}}, \ldots,\bm{v}_{x^{(t-1)}},\bar{\bm{u}},\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})\) is sampled uniformly from a discrete distribution over \(\Omega_{V}\).
3. _Sample_ \(\bm{\theta}\). Fix \(U\in\bm{U}\). By conjugacy of Dirichlet distributions with the categorical distribution, its conditional distribution given all other quantities is given by a Dirichlet distribution \(\bar{\bm{v}},\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t-1)}},\bar{\bm{u}},\bm{u}_{x^{(1) }},\ldots,\bm{u}_{x^{(t-1)}}\sim Dir\left(\beta_{1},\ldots,\beta_{d_{U}}\right)\) where \(\beta_{j}:=\alpha_{j}+\sum_{n}\mathbbm{1}\{u^{(n)}=u_{j}\}\), and, similarly, \(n\) iterates over the samples \((\bar{\bm{u}},\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})\) associated with the subset of environments in which exogenous probabilities match the deployment environment.

This procedure eventually forms a Markov chain with the invariant distribution \(P(\bm{u},\bm{\xi},\bm{\theta}\mid\bar{\bm{v}},\bar{\bm{v}}_{x^{(1)}},\ldots, \bar{\bm{v}}_{x^{(t-1)}})\). We plug in one of these samples \(\bm{\xi},\bm{\theta}\sim P(\bm{\xi},\bm{\theta}\mid\bar{\bm{v}},\bar{\bm{v}}_{ x^{(1)}},\ldots,\bar{\bm{v}}_{x^{(t-1)}})\) into Eq. (10) for different \(x\) to choose the next action \(x^{(t)}\). This sample initializes the chain for \(P(\bm{u},\bm{\xi},\bm{\theta}\mid\bar{\bm{v}},\bar{\bm{v}}_{x^{(1)}},\ldots, \bar{\bm{v}}_{x^{(t)}})\) in round \(t\) of Alg. 1.

So far, we have described algorithms, approximations, and regret guarantees that pre-suppose the correct specification of causal and selection diagrams across multiple domains. Some degree of misspecification, however, can be tolerated without voiding guarantees on performance improvements. We discuss more details in Appendix B.1.

## 5 Experiments

We evaluate the proposed approach on several synthetic scenarios inspired by the literature on clinical trials and advertising. We compare Thompson sampling with additional data sources (tTS, Alg. 1) with Thompson sampling with uninformative priors (TS) [38], a KL-UCB [9] algorithm with uninformative priors (UCB), and as a baseline also include the algorithm that chooses actions uniformly at random (Uniform)9. For all algorithms, we measure their regrets \(R_{T}\), averaged over \(10\) repetitions. Details on all data generating mechanisms and a discussion on mis-specification and limitations of the proposed approach can be found in Appendix D and Appendix B, respectively.

Footnote 9: In contrast to related work, causal bandit algorithms are designed for _single_ environments with _multiple_ intervention targets, meta-learning bandit algorithms require _experimentation_ in multiple related environments, while in the setting considered in this paper, data from source environments are given as a batch. Both sets of methods thus focus on a different class of problems. See Appendix A for more details.

**Experiment 1.** We start by evaluating the usefulness of prior data by comparing learned distributions of expected reward with and without access to prior data. We consider a bandit problem with action, reward and contextual variables \(X,Y,Z\in\{0,1\}\), respectively, characterized by Fig. 2(a) in which 1000 prior data samples are given from an environment \(\pi^{a}\) that differs in the causal assignment of \(Z\) in comparison with the deployment environment \(\pi^{*}\). Specifically, with this model,

\[P^{*}(y_{x})=\sum_{z,u_{z},u_{xy}}\mathbbm{1}\{\xi_{Y}^{\{x,z,u_{xy}\}}=y\} \mathbbm{1}\{\xi_{Z}^{\{u_{x}\}}=z\}\theta_{u_{z}}\theta_{u_{xy}},\] (12)

where \((\xi_{Y},\theta_{u_{xy}})\) are invariant across environments while \((\xi_{Z},\theta_{u_{x}})\) are specific to the deployment environment. We start by considering Fig. 2(b) that gives samples from \(\mathbb{E}_{P^{*}}[Y_{x}]\mid\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t)}}\) as a function of experimentation rounds \(t\), that is without making use of prior data \(\bar{\bm{v}}\). Distributions of expected rewards under action \(x=0\) and \(x=1\) overlap substantially until round \(t=300\) at which point \(x=1\) is inferred to lead to higher expected rewards. Fig. 2(c) gives a similar plot with the exception that the left part of the plot gives prior samples \(\mathbb{E}_{P^{*}}[Y_{x}]\mid\bar{\bm{v}}\) illustrating the shape of the expected reward distribution learned from prior data only. In particular, we observe \(\mathbb{E}_{P^{*}}[Y_{x=1}]\mid\bar{\bm{v}}\) concentrated in the interval \([0.3,0.9]\) and \(\mathbb{E}_{P^{*}}[Y_{x=0}]\mid\bar{\bm{v}}\) concentrated in the interval \([0.1,0.8]\). The

Figure 3: Performance figures related to Experiment 1.

agent starts experimentation at round \(t=0\) with this prior and from then onward expected reward samples are drawn from \(\mathbb{E}_{P*}\!\left[Y_{x}\right]\mid\bar{v},\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^ {(t)}}\) as a function of experimentation round \(t\). The bandit algorithm with prior data is remarkably more efficient, being able to determine \(x=1\) as the superior action after only 80 rounds of experimentation. Overall, prior data leads the bandit algorithm to pull the optimal arm in \(99\%\) of time versus \(93\%\) of the time without prior data.

**Experiment 2.** We revisit our introductory example to quantify the benefit of leveraging historical patient data from various hospitals. In this example, the objective is to infer the optimal level of hypertension medication \(X\). We are given a choice among 5 different levels, _i.e._\(|\Omega_{X}|=5\), and wish to increase the probability of the presence of a beneficial biomarker \(Y\), _i.e._\(|\Omega_{Y}|=2\), in the clinical trial population \(\pi\)* given that we have prior observational data in a different hospital \(\pi^{a}\). The selection diagram describing this causal protocol is given in Fig. 1. Regret comparisons for all algorithms are given in Fig. 4 (LHS). We observe a significant gain in performance by tTS that chooses the optimal intervention in \(67\%\) of the rounds in contrast with \(35\%\) of the rounds for TS, and \(28\%\) of the rounds for UCB (and \(17\%\) for an algorithm choosing interventions at random).

We use this example also to illustrate empirically the dependence between regret and prior entropy shown in Prop. 1 (RHS). For this, we consider different prior beta distributions for \(P*(W=1\mid x)\), specifically with increasing standard deviations around the true value of \(P*(W=1\mid x)\) for each \(x\in\Omega_{X}\). A larger standard deviations implies a less informative prior and higher entropy of the random variable \(\tilde{X}\) that denotes the optimal action. The entropy of \(\tilde{X}\) takes values in the interval \([0.2,1.4]\) for assumed Beta priors for \(P*(W=1\mid x)\) with standard deviations in the interval \([0.001,0.1]\). Fig. 4 (RHS) demonstrates empirically the influence of the entropy of \(\tilde{X}\) on the expected cumulative regret given in Prop. 1. In particular, narrower, more informative priors lead to better regret.

**Experiment 3.** This example considers an advertiser seeking to optimize which ads to show visitors on a particular website. For each visitor, we choose one out of a collection of 6 ads \(X\), \(|\Omega_{X}|=6\), some of which will be more engaging than others, to ultimately optimize whether a user clicks \(Y\in\{0,1\}\). Each ad has some theoretical but unknown click-through-rate \(P*(y_{x})\).

In this example, we assume access to 500 data points from an ad recommendation system used on a different website, _i.e._ a different environment \(\pi^{a}\). There, the effect of an add \(X\) on the number of clicks \(Y\) is confounded by the user's age \(A\), (here categorized into old and young such that \(\Omega_{A}=\{0,1\}\) and \(|\Omega_{A}|=2\)) and the user's product preferences level \(W\), which interacts with current ad-recommendation system through a user's browsing history \(Z\) and location (not observed and therefore represented with a bi-directed edge between \(W\) and \(X\)). Moreover, in this example, the relationship between \(W\) and \(Y\) is itself confounded by unobserved factors. The population visiting the website of interest \(\pi\)*, where the MAB will be deployed, is known to agree on all causal components with \(\pi^{a}\) except on the distribution of age \(A\). This causal protocol as well as regret comparisons for this example are shown in Fig. 5. We observe noticeable improvements in regret with prior data and knowledge of structural differences as tTS substantially improves over algorithms agnostic of prior data.

## 6 Conclusions

This paper investigated the problem of improving the efficiency of multi-armed bandits using batch data from related environments. As source environments might differ, some knowledge of structure and (potential) discrepancies are necessary to extrapolate consistently. This paper demonstrated that knowledge of selection diagrams that encode causal influence as well as potential discrepancies

Figure 4: Performance figures related to Experiment 2.

Figure 5: Performance figures related to Experiment 3.

across source and target environments, without, however, an explicit specification of functional form and distributions, is sufficient to consistently define an informative prior over reward distributions using data from arbitrary environments. The resulting algorithm guarantees improvements in regret in comparison to algorithms agnostic of prior data. To our knowledge, this serves as one of the first principled approaches to consistently leverage prior data in the context of bandits and we hope it can pave the way for developing more general transfer learning methods in reinforcement learning.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* [2] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In _International Conference on Machine Learning_, pages 127-135. PMLR, 2013.
* [3] Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. _Proceedings of the National Academy of Sciences_, 113(27):7345-7352, 2016.
* [4] Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the prior in bandits. _Advances in Neural Information Processing Systems_, 34:28029-28041, 2021.
* [5] Alexis Bellot and Elias Bareinboim. Partial transportability for domain generalization. _Technical Report R-88, Causal AI Lab, Columbia University_, 2022.
* [6] Alexis Bellot, Junzhe Zhang, and Elias Bareinboim. Scores for learning discrete causal graphs with unobserved confounders. _Technical Report R-83, Causal AI Lab, Columbia University_, 2022.
* [7] Blair Bilodeau, Linbo Wang, and Daniel M Roy. Adaptively exploiting d-separators with causal bandits. _arXiv preprint arXiv:2202.05100_, 2022.
* [8] Leon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. _Journal of Machine Learning Research_, 14(11), 2013.
* [9] Olivier Cappe, Aurelien Garivier, Odalric-Ambrym Maillard, Remi Munos, and Gilles Stoltz. Kullback-Leibler upper confidence bounds for optimal sequential allocation. _The Annals of Statistics_, pages 1516-1541, 2013.
* [10] Leonardo Cella and Massimiliano Pontil. Multi-task and meta-learning with sparse linear bandits. In _Uncertainty in Artificial Intelligence_, pages 1692-1702. PMLR, 2021.
* [11] Juan Correa and Elias Bareinboim. General transportability of soft interventions: Completeness results. _Advances in Neural Information Processing Systems_, 33:10902-10912, 2020.
* [12] Arnoud AWM de Kroon, Danielle Belgrave, and Joris M Mooij. Causal discovery for causal bandits utilizing separating sets. _arXiv preprint arXiv:2009.07916_, 2020.
* [13] Aniket Anand Deshmukh, Urun Dogan, and Clay Scott. Multi-task learning for contextual bandits. _Advances in neural information processing systems_, 30, 2017.
* [14] Alun Evans, Veikko Salomaa, Sangita Kulathinal, Kjell Asplund, Francois Cambien, Marco Ferrario, Markus Perola, Leena Peltonen, Denis Shields, Hugh Tunstall-Pedoe, et al. Morgan (an international pooling of cardiovascular cohorts). _International journal of epidemiology_, 34(1):21-27, 2005.
* [15] Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The generalized linear case. _Advances in Neural Information Processing Systems_, 23, 2010.
* [16] Alan E Gelfand. Gibbs sampling. _Journal of the American statistical Association_, 95(452):1300-1304, 2000.

* [17] John C Gittins and David M Jones. A dynamic allocation index for the discounted multiarmed bandit problem. _Biometrika_, 66(3):561-565, 1979.
* [18] Limor Gultchin, Virginia Aglietti, Alexis Bellot, and Silvia Chiappa. Functional causal bayesian optimization. _arXiv preprint arXiv:2306.06409_, 2023.
* [19] Botao Hao, Tor Lattimore, and Mengdi Wang. High-dimensional sparse linear bandits. _Advances in Neural Information Processing Systems_, 33:10753-10763, 2020.
* [20] Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical bayesian bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 7724-7741. PMLR, 2022.
* [21] Nathan Kallus and Angela Zhou. Confounding-robust policy improvement. _Advances in neural information processing systems_, 31, 2018.
* [22] Juha Karvanen. Study design in causal models. _Scandinavian Journal of Statistics_, 42(2):361-377, 2015.
* [23] Finnian Lattimore, Tor Lattimore, and Mark D Reid. Causal bandits: Learning good interventions via causal inference. _Advances in Neural Information Processing Systems_, 29, 2016.
* [24] Sanghack Lee and Elias Bareinboim. Structural causal bandits with non-manipulable variables. In _AAAI Conference on Artificial Intelligence_, volume 33, pages 4164-4172, 2019.
* [25] Xiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for reinforcement learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [26] Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Causal bandits with unknown graph structure. _arXiv preprint arXiv:2106.02988_, 2021.
* [27] Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, and William Yan. Regret analysis of bandit problems with causal background knowledge. In _Conference on Uncertainty in Artificial Intelligence_, pages 141-150. PMLR, 2020.
* [28] Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, and Zhenyu Yan. Regret analysis of causal bandit problems. _arXiv preprint arXiv:1910.04938_, 2019.
* [29] Alan Malek and Silvia Chiappa. Asymptotically best causal effect identification with multi-armed bandits. _Advances in Neural Information Processing Systems_, 34:21960-21971, 2021.
* [30] Hyejin Park, Seiyun Shin, Kwang-Sung Jun, and Jungseul Ok. Transfer learning in bandits with latent continuity. In _2021 IEEE International Symposium on Information Theory (ISIT)_, pages 1463-1468. IEEE, 2021.
* [31] Judea Pearl. _Causality_. Cambridge University Press, 2009.
* [32] Judea Pearl and Elias Bareinboim. Transportability of causal and statistical relations: A formal approach. In _Twenty-fifth AAAI conference on artificial intelligence_, 2011.
* [33] Amit Peleg, Naama Pearl, and Ron Meir. Metalearning linear bandits by prior update. In _International Conference on Artificial Intelligence and Statistics_, pages 2885-2926. PMLR, 2022.
* [34] Herbert Robbins. Some aspects of the sequential design of experiments. _Bulletin of the American Mathematical Society_, 58(5):527-535, 1952.
* [35] Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling. _Journal of Machine Learning Research_, 17(1):2442-2471, 2016.
* [36] Rajat Sen, Karthikeyan Shanmugam, Alexandros G Dimakis, and Sanjay Shakkottai. Identifying best interventions through online importance sampling. In _International Conference on Machine Learning_, pages 3057-3066. PMLR, 2017.

* [37] Ingmar Skoog and Deborah Gustafson. Update on hypertension and alzheimer's disease. _Neurological Research_, 28(6):605-611, 2006.
* [38] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294, 1933.
* [39] Jin Tian and Judea Pearl. _A general identification condition for causal effects_. eScholarship, University of California, 2002.
* [40] Zhi Wang, Chicheng Zhang, and Kamalika Chaudhuri. Thompson sampling for robust transfer in multi-task bandits. _arXiv preprint arXiv:2206.08556_, 2022.
* [41] Zhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel Riek, and Kamalika Chaudhuri. Multitask bandit learning through heterogeneous feedback aggregation. In _International Conference on Artificial Intelligence and Statistics_, pages 1531-1539. PMLR, 2021.
* [42] Kan Xu and Hamsa Bastani. Learning across bandits in high dimension via robust statistics. _arXiv preprint arXiv:2112.14233_, 2021.
* [43] Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandit: a causal approach. In _Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems_, pages 1778-1780, 2017.
* [44] Junzhe Zhang, Jin Tian, and Elias Bareinboim. Partial counterfactual identification from observational and experimental data. _arXiv preprint arXiv:2110.05690_, 2021.

**Appendix for "Transportability for Bandits with Data from Different Environments"**

This Appendix includes

* Related work in Appendix A.
* Discussion on limitations and analysis of misspecification in Appendix B.
* Proofs in Appendix C.
* Details on the data generating mechanisms Appendix D.
* Details on the Gibbs sampler in Appendix E.

## Appendix A Related work

In the bandit literature, problems of transfer learning in which existing experience and knowledge is used to improve the performance of an agent appear under the rubric of multi-task learning and meta-learning. Typically, in the context of multi-task learning the agent aims to solve a prescribed set of related bandit tasks with shared structure. For example, [41] consider the setting in which multiple players interact in an environment with the property that each player has slightly different associated reward distributions. [40] extend this approach to the Thompson sampling algorithm with an assumption that reward distribution between players are close but not identical. Similar ideas can be found in the contextual case in which, _e.g._[13] propose to leverage similarities in contexts for different arms and improve prediction of reward distributions from contexts. In the context of meta-learning, the agent is designed to work well on an arbitrary number of tasks from a common environment (_i.e._ sampled from a prescribed distribution), relying on already completed tasks from the same environment. For example, [4, 20, 33] assume a hierarchical bandit structure in which parameters governing multiple bandit instances are sampled \(i.i.d.\) from a meta-prior. The authors demonstrate that updates on the meta-prior from one instance can benefit other instances and that regret bounds over a sequence of instances can be established. [30] adopt a similar setting to infer a Lipschitz continuity constant which can be used to derive scale free regret bounds. Others, _e.g._[10, 42], have assumed that structural parameters of each instance can be decomposed into a shared component and an instance-specific component, and learned with high-dimensional regularization schemes.

Existing methods adopt a relatively wide range of assumptions on the "relatedness" of different bandit instances or players that include, as surveyed above, the existence of a meta-distribution across bandit instances, assumptions on the pairwise similarities in the reward distributions of different players, assumptions on the decomposition of bandit parameters, and assumptions on the distribution of contextual variables and their association with rewards. The environment and causal structure typically remain invariant across bandit instances, and only small and specific set of changes in distribution are allowed across bandit instances.

The proposed approach aims to tackle a more general setting in which we allow for arbitrary changes in the causal mechanisms underlying each environment as long as their location can be established and encoded in selection diagrams, _i.e._ graphs that describe the difference in structure across domains without constraining their form. In the causality literature, these analyses fall under the transportability umbrella. Several authors have demonstrated the power of this approach to identify causal effects, predict counterfactual distributions and, more generally, make inference across different environments [3, 11, 32, 5]. In the bandit literature, there is also an extensive literature on various ways of exploiting knowledge of a causal graph for a single environment, known as causal bandits [23, 36, 28, 27, 24, 26, 12, 29, 7, 18]. Within this literature the causal graph specifies a dependency structure between different actions that allows an agent to identify redundant actions and ultimately more efficient exploration, improving regret bounds by a multiplicative factor. Practical examples of the use of causal graphs (within decision-making problems) are prevalent in the contexts of clinical trials, healthcare, and advertising. We note, in particular, the examples in Figs. 2, 3, 4, in [22] that encode the design of case-cohort studies and clinical trials used in the MORGAM study [14] using causal graphs for the estimation of causal effects. Separately, [21] consider theInternational Stroke Trial and the known causal associations between relevant features for policy optimization. In the context of advertising, [8] describes the use of causal methods with several detailed examples. Specifically, Figs. 3, 4, and 6 in [8] show examples of causal graphs that may be defined for particular computational advertising applications. See also [5] for a bandit algorithm leveraging the computational advertising graphs in [8], and[36, Sec. 5.2] for similar causal treatments also in the context of advertising. So far, however, all bandit algorithms exploiting causal knowledge have been studied in the "single environment" setting without access to prior data. Variations among existing proposals are due to the class of graphs or the extent of knowledge of the graph, _e.g._ with or without knowledge of the full causal graph, with or without unobserved confounding, etc. One may interpret the proposed approach as extending the causal bandit formalism to problems in which data from multiple different environments is available prior to an agent experimenting in a target environment, all of which are, as in the causal bandit literature, described by underlying causal models.

[MISSING_PAGE_FAIL:15]

causal assignment of \(Z\) across environments, and will consider inference of expected rewards under various degrees of discrepancy, _e.g._ one may explicitly assume that either \(P^{*}(z)\) is arbitrary or for example assume \(P^{*}(z)\in[a,b]\). We illustrate various such scenarios in Fig. 6. Each panel, from right to left increases allows for a greater level of uncertainty that results in a correspondingly wider distribution of expected reward prior to experimentation. The corresponding regret of bandit algorithms that incorporate these bounds on \(Z\) in \(P^{*}\) to warm start the distribution of expected regret prior to experimentation are given in Fig. 8. We observe that the wider the prior the higher the corresponding regret, which illustrates how degrees of prior knowledge influence regret.

Using the same reasoning, it holds that if instead the researcher misses edges or incorrectly assigns priors on the causal mechanisms to be expected in the target domain, prior inference will be incorrect in general. For example, if distributions of expected reward in Fig. 6 do not cover the true expected reward, then a bandit algorithm will need to correct for such a biased prior and we can expect cumulative rewards to be lower than those of an algorithm that ignores prior data and structure. The extent to which the corresponding regret of a bandit algorithm incorrectly making assumptions on structure and discrepancies across environments will depend on the underlying causal graph and functional associations between variables.

Most of the discussion in the main body of this paper has assumed knowledge of a discrepancy between environments without considering its magnitude, although the previous example shows that knowledge of bounds or distributions of some of the variables that differ in the target domain can be incorporated. It is worth noting that other kinds of domain knowledge may be available, such as in the functional form of an association in the target domain, specific magnitudes of differences expressed within such a functional form, etc. It would be an interesting task to consider how to use that information optimally.

### Run time evaluations

Reward distributions \(P^{*}(y_{x})\) induced by any underlying SCM can be expressed by a corresponding discrete SCM in which \(P(\bm{U})\) is discrete and \(\mathcal{F}\) is a discrete mapping between finite spaces. The complexity of the corresponding parameterization depends on the underlying selection diagram and dimensionality of observed variables.

In particular, in a given iteration of the Gibbs sampler, posterior updates are done for each parameter separately so that computational time is proportional to the parameter count, approximately, which in turn is determined by the cardinality of variables as well as the structure of the graph. For a fixed graph, assuming that each update requires a small constant amount of time to compute, we could therefore establish analytically how computational time scales with the cardinality of variables. For arbitrary graphs, an analytical result is in general more involved as the parameter count increases differently depending on the local structure of each variable. As an example for illustration, consider the graph in Fig. 8(a) where \(X\) is an action variable, \(Z\) is a contextual variable, and \(Y\) is a reward variable. Following the parameterization in Corol. 1, the cardinality of parameters is defined as follows: \(|\bm{\theta}_{U}|=|\Omega_{X}|\cdot|\Omega_{Z}|\cdot|\Omega_{Y}|\), \(|\bm{\xi}_{X}|=|\Omega_{X}|\cdot|\Omega_{Z}|\cdot|\Omega_{Y}|\), \(|\bm{\xi}_{Z}|=|\Omega_{X}|\), \(|\bm{\xi}_{Y}|=|\Omega_{Z}|\cdot|\Omega_{Y}|\cdot|\Omega_{Z}|\).

Figure 8: Regret under misspecification.

Figure 9: Run time experiments.

We would expect run time to increase linearly with the cardinality of variables \(X,Y\) and to increase "slower than quadratically" with the cardinality of \(Z\). Fig. 8(b) gives run time evaluations of the proposed bandit algorithm using \(1000\) rounds of experimentation as a function of the number of actions. Fig. 8(c) evaluates the influence of the size of prior data on run time by plotting the time required for drawing \(1000\) samples from the prior \(P(\cdot\mid\bar{\bm{v}})\) with increasing prior data size (in addition to 1000 samples Gibbs samples that are discarded as burn-in). For both Figs. 8(b) and 8(c) we observe run time to scale approximately linearly with the cardinality of the action space and sample size.

Theoretical results

### Preliminaries

Counterfactuals and optimal actionsWe have defined in Sec. 1.1 the fact that intervening in an SCM results in a new model that differs in the causal mechanisms subject to interventions. In this section, we extend this notion to consider models derived from interventions that replace a causal mechanism with another function, not necessarily a constant. This gives consistent definitions for counterfactual random variables of the form \(Y_{X}\) that appear in Sec. 3.

**Definition 4** (Intervened model).: _Let \(\mathcal{M}\) be an SCM, \(\hat{\bm{U}}\subseteq\bm{U}\), \(X\in\bm{V}\), and \(\hat{X}:\hat{\bm{U}}\to\Omega_{X}\) a function. Then, \(\mathcal{M}_{\hat{X}}\), called the intervened model of \(\mathcal{M}\) subject to \(\hat{X}\), is identical to \(\mathcal{M}\), except that the function \(f_{X}\) is replaced with the function \(\hat{X}\)._

Uncertainty in \(\mathcal{M}_{\hat{X}}\) is similarly encoded by the distribution \(P(\bm{U})\) which when averaged over leads to random variables \(Y_{\hat{X}}\) for \(Y\in\bm{V}\) that describes the random variable \(Y\) under a model \(\mathcal{M}_{\hat{X}}\). This definition is tightly related to that of potential outcome as define in Sec. 1.1, but the former explicitly allows for interventions that do not necessarily fix the variable \(X\) to a constant value. Probabilities in this model can be computed using the chain rule

\[P(Y_{X}=y)=\int_{\Omega_{X}}P(Y_{x}=y\mid X=x)P(X=x)dx.\] (13)

And similarly expectations are given by

\[\mathbb{E}_{P}\big{[}Y_{X}\big{]}=\int_{\Omega_{Y}}\int_{\Omega_{X}}yP(Y_{x}=y \mid X=x)P(X=x)dxdy.\] (14)

In turn, which action is optimal is represented by a random variable \(\tilde{X}:\Omega_{\bm{\xi}}\times\Omega_{\bm{\theta}}\mapsto\Omega_{X}\) where \(\Omega_{\bm{\xi}}\times\Omega_{\bm{\theta}}\) defines the space of possible models \(M:=M(\bm{\xi},\bm{\theta})\in\mathcal{M}(\mathcal{G}_{\pi^{\bm{\pi}}})\) that parameterize the deployment environment \(\pi^{\bm{\ast}}\). For example, \(P_{M\sim\mathcal{M}(\mathcal{G}_{\pi^{\bm{\ast}}})}(\tilde{X}=x)=P_{M\sim \mathcal{M}(\mathcal{G}_{\pi^{\bm{\ast}}})}(\mathbb{E}_{P_{M}}\big{[}Y_{x} \big{]}>\mathbb{E}_{P_{M}}\big{[}Y_{x^{\prime}}\big{]},\forall x^{\prime}\in \Omega_{X}\backslash\{x\})\) where \(P_{M\sim\mathcal{M}(\mathcal{G}_{\pi^{\bm{\ast}}})}\) is a probability mass function defined over the space of possible models \(\mathcal{M}(\mathcal{G}_{\pi^{\bm{\ast}}})\).

Information Theory DefinitionsThis section provides a definition of all the information-theoretic terms used throughout and shows that they can be generalized to mixed quantities that include discrete and continuous random variables in a consistent manner. For a discrete random variable \(X\in\Omega_{X}\), the Shannon entropy and its conditional counter parts are defined as

\[\mathcal{H}(X) :=-\sum_{x\in\Omega_{X}}P(X=x)\log P(X=x),\] \[\mathcal{H}(X\mid Z=z) :=-\sum_{x\in\Omega_{X}}P(X=x\mid Z=z)\log P(X=x\mid Z=z)\text{ and,}\] \[\mathcal{H}(X\mid Z) :=\sum_{z\in\Omega_{Z}}\mathcal{H}(X\mid Z=z)P(Z=z).\]

With a slight abuse of notation, we will define the _filtered entropy_ as

\[\mathcal{H}(X\mid\bar{\bm{v}}):=-\sum_{x\in\mathcal{X}}P(X=x\mid\bar{\bm{v}}) \log P(X=x\mid\bar{\bm{v}}),\]

where \(\bar{\bm{v}}\) is a dataset.

Given two probability measures \(P\) and \(Q\), where \(P\) is absolutely continuous w.r.t. \(Q\) so that the Radon-Nikodym derivative \(\frac{dP}{dQ}\) is well defined, the Kullback-Leibler divergence is

\[\mathcal{D}_{KL}(P\|Q):=\int\log\frac{dP}{dQ}dP,\]

which allows the standard definition of _mutual information_:

\[I_{P}(X;Y):=\mathcal{D}_{KL}(P(X,Y)\|P(X)P(Y)),\]where the subscript \(P\) that denotes the probability distribution used is sometimes omitted if unambiguous. Equivalently, we can define the mutual information and _conditional mutual information_ by

\[I(X;Y)=\mathcal{H}(X)-\mathcal{H}(X\mid Y)\text{ and }I(X;Y\mid Z)=\mathcal{H}(X \mid Z)-\mathcal{H}(X\mid Y,Z),\]

and we will also need the _filtered mutual information_,

\[I_{P(\cdot\mid Z=z)}(X;Y)\coloneqq\mathcal{D}_{KL}(P(X,Y\mid Z=z)\|P(X\mid z)P (Y\mid Z=z)),\]

where we will typically evaluate \(I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(X;Y)\coloneqq D(P(X,Y\mid\bar{ \bm{V}}_{t},\bar{\bm{v}})\|P(X\mid\bar{\bm{V}}_{t},\bar{\bm{v}})P(Y\mid\bar{ \bm{V}}_{t},\bar{\bm{v}}))\) where recall \(\bar{\bm{V}}_{t}:=\{\bm{V}_{X^{(1)}},\ldots,\bm{V}_{X^{(t-1)}}\}\) denotes the random variables associated with the agent's history of interactions with \(\pi^{*}\) up to round \(t\). The conditional mutual information is related to the filtered mutual information by an expectation: \(\mathbb{E}[I_{P(\cdot\mid Z)}(X;Y)]=I(X;Y\mid Z)\), with respect to \(P(Z)\). We will frequently use this fact to show that

\[\mathbb{E}[I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(X;Y)]=I_{P(\cdot\mid \bar{\bm{v}})}(X;Y\mid\bar{\bm{V}}_{t}),\]

where the expectation is taken with respect to \(P(\bar{\bm{V}}_{t}\mid\bar{\bm{v}})\). Perhaps the most important property of mutual information will be the chain rule,

\[I(X;Z_{1},\ldots,Z_{m})=I(X;Z_{1})+I(X;Z_{2}\mid Z_{1})+\ldots+I(X;Z_{m}\mid Z _{1},\ldots,Z_{m-1}).\]

We will need to evaluate \(I(\bm{\theta},\bm{\xi};Z)\), where \(Z\) is a discrete random variable, _e.g._\(\bar{\bm{v}}_{t}\) or \(\bm{V}^{(t)}\), \(\bm{\theta}\) is continuous, and \(\bm{\xi}\) is discrete. We can evaluate this mutual information by writing

\[I(\bm{\theta},\bm{\xi};Z) =I(Z;\bm{\theta},\bm{\xi})\] \[=I(Z;\bm{\theta})+I(Z;\bm{\xi}\mid\bm{\theta})\] \[=\mathcal{H}(Z)-\mathcal{H}(Z\mid\bm{\theta})+\mathcal{H}(Z\mid \bm{\theta})-\mathcal{H}(Z\mid\bm{\xi},\bm{\theta}),\]

which are all well defined.

### Proofs

For readability, we restate all propositions.

**Proposition 1** (restated).: _Let \(R_{T}\) denote the regret incurred by following Thompson sampling (Alg. 1). For any \(T\in\mathbb{N}\) and \(\Gamma\geq\Gamma_{t}\), then_

\[\mathbb{E}[R_{T}\mid\bar{\bm{v}}]\leq\Gamma\sqrt{\mathcal{H}(\tilde{X}\mid \bar{\bm{v}})T},\]

_where \(\mathcal{H}(\tilde{X}\mid\bar{\bm{v}})\) is the conditional entropy of \(\tilde{X}\) given \(\bar{\bm{v}}\)._

Proof.: This analysis extends [35, Prop. 1] to account for the case in which prior data is conditioned upon. We first bound the expected regret by using the law of total expectations and introducing the mutual information between the optimal action and the action observation tuple,

\[\mathbb{E}[R_{T}\mid\bar{\bm{v}}] =\mathbb{E}\left[\sum_{t=1}^{T}Y_{\tilde{X}}-Y_{X^{(t)}}\mid\bar{ \bm{v}}\right]\] (15) \[=\mathbb{E}\sum_{t=1}^{T}\mathbb{E}\left[Y_{\tilde{X}}-Y_{X^{(t)} }\mid\bar{\bm{V}}_{t},\bar{\bm{v}}\right]\] (16) \[=\mathbb{E}\sum_{t=1}^{T}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t}, \bar{\bm{v}})}(\tilde{X};\bm{V}_{X^{(t)}})}\frac{\mathbb{E}\left[Y_{\tilde{X}}- Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}\right]}{\sqrt{I_{P(\cdot \mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(\tilde{X};\bm{V}_{X^{(t)}})}}.\] (17)

By using the KL divergence definition of mutual information and using the fact that Thompson sampling precisely chooses actions according the their probability of being optimal, it can be shown that,

\[\mathbb{E}\left[Y_{\tilde{X}}-Y_{X^{(t)}}\right] =\sum_{x\in\Omega_{X}}P(\tilde{X}=x)\mathbb{E}[Y_{x}\mid\tilde{X} =x]-\sum_{x\in\Omega_{X}}P(X=x)\mathbb{E}[Y_{x}]\] \[=\sum_{x\in\Omega_{X}}P(\tilde{X}=x)(\mathbb{E}[Y_{x}\mid\tilde{X} =x]-\mathbb{E}[Y_{x}]),\]where \(P(X=x)=P(\tilde{X}=x)\) by definition of the policy used by Thompson sampling. Let the information ratio \(\Gamma_{t}\) is defined as an upperbound on the ratio

\[\frac{\mathbb{E}\left[Y_{\tilde{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}} \right]}{\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(\tilde{X};\bm{V}_{ X^{(t)}})}}\leq\Gamma_{t}.\]

By writing \(\Gamma\geq\Gamma_{t},\forall t\), applying this bound yields

\[\mathbb{E}[R_{T}\mid\bar{\bm{v}}] =\mathbb{E}\sum_{t=1}^{T}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar {\bm{v}})}(\tilde{X};\bm{V}_{X^{(t)}})}\frac{\mathbb{E}\left[Y_{\tilde{X}}-Y_{ X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}\right]}{\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t}, \bar{\bm{v}})}(\tilde{X};\bm{V}_{X^{(t)}})}}\] \[\leq\mathbb{E}\sum_{t=1}^{T}\Gamma_{t}\sqrt{I_{P(\cdot\mid\bar{ \bm{V}}_{t},\bar{\bm{v}})}(\tilde{X};\bm{V}_{X^{(t)}})}\] \[\leq\Gamma\mathbb{E}\sum_{t=1}^{T}\sqrt{I_{P(\cdot\mid\bar{\bm{V} }_{t},\bar{\bm{v}})}(\tilde{X};\bm{V}_{X^{(t)}})}\] \[\leq\Gamma\sqrt{T\mathcal{H}(\tilde{X}\mid\bar{\bm{v}})},\]

where the second to last inequality follows from the Cauchy-Schwartz inequality and the last inequality follows from the fact that,

\[\mathbb{E}\sum_{t=1}^{T}I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v }})}(\tilde{X};\bm{V}_{X^{(t)}}) =\sum_{t=1}^{T}I_{P(\cdot\mid\bar{\bm{v}})}(\tilde{X};\bm{V}_{X^{ (t)}}\mid\bar{\bm{V}}_{t})\] (18) \[=I_{P(\cdot\mid\bar{\bm{v}})}(\tilde{X};(\bm{V}_{X^{(T)}},\dots, \bm{V}_{X^{(1)}}))\] (19) \[=\mathcal{H}(\tilde{X}\mid\bar{\bm{v}})-\mathcal{H}(\tilde{X}\mid \bm{V}_{X^{(T)}},\dots,\bm{V}_{X^{(1)}},\bar{\bm{v}})\] (20) \[\leq\mathcal{H}(\tilde{X}\mid\bar{\bm{v}}),\] (21)

by the chain rule for the mutual information and given the non-negativity of the entropy. 

**Proposition 2** (restated).: _Let \(R_{T}\) denote the regret incurred following the policy defined by Alg. 1. For any \(T\in\mathbb{N}\), if Eq. (7) holds with \(\Gamma\geq\Gamma_{t}\) for all \(t\),_

\[\mathbb{E}[R_{T}\mid\bar{\bm{v}}]\leq\Gamma\sqrt{TI_{P(\cdot\mid\bar{\bm{v}})} \left(\bm{\theta},\bm{\xi};\bm{V}_{X^{(1)}},\dots,\bm{V}_{X^{(T)}}\right)}+ \sum_{t=1}^{T}\mathbb{E}[\epsilon_{t}].\]

Proof.: This proof uses a different characterization of the per round regret to explicitly consider the influence of model parameters \((\bm{\theta},\bm{\xi})\) on information gain. Assume instead that there exists \(\Gamma_{t}\) such that

\[\mathbb{E}[Y_{\tilde{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\leq \Gamma_{t}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}\left(\bm{\theta},\bm{\xi};\bm{V}_{X^{(t)}}\right)}+\epsilon_{t},\] (22)where \(\epsilon_{t}>0\) is an additional slack term. The proof strategy follows that of [25, Prop. 2] where unconditional regret bounds were shown. Given Eq. (22) the following derivation holds,

\[\mathbb{E}[R_{T}\mid\bar{\bm{v}}]=\mathbb{E}\left[\sum_{t=1}^{T}Y_{ \bar{X}}-Y_{X^{(t)}}\mid\bar{\bm{v}}\right]\] by definition \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}\left[Y_{\bar{X}}-Y_{X^ {(t)}}\mid\bar{\bm{v}},\bar{\bm{V}}_{t}\right]\right]\] by the law of iterated expectations \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\Gamma_{t}\sqrt{I_{P(\cdot\mid \bar{\bm{V}}_{t},\bar{\bm{v}})}\left(\bm{\theta},\bm{\xi};\bm{V}_{X^{(t)}} \right)}+\epsilon_{t}\right]\] by definition of \[\Gamma_{t}\] and \[\epsilon_{t}\] \[\leq\Gamma\sqrt{T\sum_{t=1}^{T}\mathbb{E}\left[I_{P(\cdot\mid \bar{\bm{V}}_{t},\bar{\bm{v}})}\left(\bm{\theta},\bm{\xi};\bm{V}_{X^{(t)}} \right)\right]}+\mathbb{E}\left[\sum_{t}\epsilon_{t}\right]\] by Cauchy-Schwarz's inequality \[=\Gamma\sqrt{T\sum_{t=1}^{T}I_{P(\cdot\mid\bar{\bm{v}})}\left( \bm{\theta},\bm{\xi};\bm{V}_{X^{(t)}}\mid\bar{\bm{V}}_{t}\right)}+\mathbb{E} \left[\sum_{t}\epsilon_{t}\right]\] by definition of the expectation of \[I_{P}\] \[=\Gamma\sqrt{TI_{P(\cdot\mid\bar{\bm{v}})}\left(\bm{\theta},\bm{ \xi};\bm{V}_{X^{(1)}},\ldots,\bm{V}_{X^{(T)}}\right)}+\mathbb{E}\left[\sum_{t} \epsilon_{t}\right]\] by the chain rule for mutual information.

**Proposition 3** (restated).: _Fix \(\delta>0\) and choose \(\Gamma_{t}\) such that \(\left|Y_{x}-\mathbb{E}[Y_{x}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\right|\leq \frac{\Gamma_{t}}{2}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(\bm{ \theta},\bm{\xi};Y_{x})}\) for all \(x\in\Omega_{X}\) simultaneously with probability greater than \(1-\delta\). Then Alg. 1 chooses actions \(X^{(t)}\) that satisfy_

\[\mathbb{E}[Y_{\bar{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\leq\Gamma _{t}\sqrt{I_{P(\cdot\mid\bar{\bm{V}}_{t},\bar{\bm{v}})}(\bm{\theta},\bm{\xi}; \bm{V}_{X^{(t)}})}+\delta B,\]

_where \(B\geq 0\) is such that \(\sup_{y,y^{\prime}\in\Omega_{Y}}y-y^{\prime}\leq B\)._

Proof.: This proposition extends [25, Lem. 3] to account for the case in which prior data is conditioned upon. Thompson sampling, by definition, samples an action according to its probability of being optimal with the current distribution of parameter values,

\[\mathbb{E}[Y_{\bar{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]=\mathbb{ E}\left[\mathbb{E}_{P_{\bar{X}}}[Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]- \mathbb{E}_{P_{\pi}\ast}[Y_{X^{(t)}}\mid\bar{\bm{V}}_{t},\bar{\bm{v}}]\right],\]

where \(\hat{M}:=M(\hat{\bm{\xi}},\hat{\bm{\theta}})\) defines the SCM that is obtained with the current sample \(\hat{\bm{\xi}},\hat{\bm{\theta}}\sim P(\bm{\xi},\bm{\theta}\mid\bar{\bm{v}}_{ t},\bar{\bm{v}})\) from the posterior distribution at round \(t\), while \(M_{\pi}\ast\) refers to the true underlying SCM of the environment. Expectations with respect to the distributions \(P_{\hat{M}}\) and \(P_{\pi}\ast\) are conditioned on a specific model of the environment. Define,

\[\mathcal{E}:=\left\{(\bm{\xi},\bm{\theta})\in\Omega_{\bm{\xi}}\times\Omega_{ \bm{\theta}}:|Y_{x,M(\bm{\xi},\bm{\theta})}-\mathbb{E}_{M\sim P_{\mathcal{M}( \mathcal{G}_{\pi}\ast)}}[Y_{x,M}]|\leq\Gamma_{t}\sqrt{I_{P(\cdot\mid\bar{\bm{V} }_{t},\bar{\bm{v}})}(\bm{\theta},\bm{\xi};Y_{x})},\forall x\in\Omega_{X}\right\},\]

where \(Y_{x,M(\bm{\xi},\bm{\theta})}\) refers to the random variable \(Y\) in SCM \(M(\bm{\xi},\bm{\theta})\) in which we intervene to set \(X\) to \(x\), and \(Y_{x,M}\) refers to the random variable \(Y\) in SCM \(M\) in which we set \(X\) to \(x\).

[MISSING_PAGE_FAIL:22]

\(x\neq x^{\prime}\). Bounds on the information ratio have also been shown for linear bandits [35]. With our parameterization of \(P_{\pi}\ast(y_{x})\) the probability of exogeneous variables encoded in \(\bm{\theta}\) is shared while \(\bm{\xi}\) is not, thus some information about reward distributions is expected to be gained on all arms in each MAB round, and \(1/2<\Gamma_{t}<|\Omega_{X}|/2\).

In the following we will quantify the amount of sharing between reward distributions and compute a corresponding a smaller upperbound for the information ratio for a specific graph in which the computation is tractable following the proof strategy of [35, Proposition 5]. This illustrates more precisely the regret gains that can be expected from having a shared parameterization for reward distributions.

We consider the selection diagram with domain-specific confounding given in Fig. 10 and the computation of \(P^{\ast}(y\mid do(x))=\sum_{z\in\Omega_{Z}}P^{a}(y\mid x,z)P^{\ast}(z)\) given knowledge of \(P^{a}(x,y,z)\). This graph is interesting because we may directly parameterize the conditional distributions involved in \(P^{\ast}(y\mid do(x))\) instead of considering its underlying SCM. Let \(Y\) be binary for simplicity (all arguments hold also more generally). For the purposes of this discussion, we will assume that \(P^{a}(y\mid x,z)\) can be approximated arbitrarily well from sufficient prior data. It holds that

\[P^{\ast}(y\mid do(x)) =\sum_{z\in\Omega_{Z}}P^{a}(y\mid x,z)P^{\ast}(z)\] (24) \[=\bm{\phi}(x)^{T}\bm{p},\] (25)

where \(\bm{\phi}(x)\in\mathbb{R}^{|\Omega_{Z}|}\) is a column vector with \(i\)-th entry \(P_{\pi^{\ast}}(Y=1\mid x,z=i)\) and \(\bm{p}\in\mathbb{R}^{|\Omega_{Z}|}\) is a column vector with \(i\)-th entry \(P(Z=i)\). The latter has some distribution in the interval \([0,1]\) and in particular we denote its mean by \(\mu:=\mathbb{E}[\bm{p}]\) and its mean conditioned on the optimal action being \(x_{j}\) by \(\mu_{j}:=\mathbb{E}[\bm{p}\mid\tilde{X}=x_{j}]\). Recall that \(\tilde{X}\) denotes the optimal action.

Let \(\alpha_{i}:=P(\tilde{X}=x_{i})\) and define \(M\in\mathbb{R}^{|\Omega_{X}|\times|\Omega_{X}|}\) by its \((i,j)\)-th entry,

\[M_{i,j} :=\sqrt{\alpha_{i}\alpha_{j}}\left(\mathbb{E}[Y_{x_{i}}\mid\tilde {X}=x_{j}]-\mathbb{E}[Y_{x_{i}}]\right)\] (26) \[=\sqrt{\alpha_{i}\alpha_{j}}\left(\bm{\phi}(x_{i})^{T}\mathbb{E} [\bm{p}\mid\tilde{X}=x_{j}]-\bm{\phi}(x_{i})^{T}\mathbb{E}[\bm{p}]\right)\] (27) \[=\sqrt{\alpha_{i}\alpha_{j}}\left(\mathbb{E}[\bm{p}\mid\tilde{X} =x_{j}]-\mathbb{E}[\bm{p}]\right)^{T}\bm{\phi}(x_{i}).\] (28)

All expectations are taken with respect to \(P(\cdot\mid\tilde{\bm{V}}_{t},\bar{\bm{v}})\). \(M\) can therefore be written as a product of two matrices of rank \(|\Omega_{X}|\). With this definition, it was shown by [35, Proposition 5] that,

\[\mathbb{E}\left[Y_{\tilde{X}}-Y_{X^{(t)}}\mid\bar{\bm{V}}_{t}, \bar{\bm{v}}\right]^{2}=\text{Trace}(M),\] (29)

and that,

\[I_{P(\cdot|\bar{\bm{V}}_{t},\bar{\bm{v}})}(\tilde{X};\bm{V}_{X^{ (t)}})\geqslant 2\|M\|_{F}^{2},\] (30)

leading to the fact that the information ratio,

\[\Gamma_{t}:=\frac{\mathbb{E}\left[Y_{\tilde{X}}-Y_{X^{(t)}}\mid \bar{\bm{V}}_{t},\bar{\bm{v}}\right]^{2}}{I_{P(\cdot|\bar{\bm{V}}_{t},\bar{\bm {v}})}(\tilde{X};(X,Y_{X}))}\leqslant\frac{\text{Rank}(M)}{2}=\frac{|\Omega_{ Z}|}{2}.\] (31)

Now, \(|\Omega_{Z}|<<|\Omega_{X}|\) in certain applications which shows that the information ratio may be smaller than the worst-case value of \(|\Omega_{X}|/2\) due to sharing of \(P^{\ast}(z)\) across different actions \(x\).

Details on the data generating mechanisms

Bow graph with domain-specific confounding.To generate source data we choose an SCM \(M^{a}\) compatible with the graph specified as follows: \(P(u_{Z}),P(u_{X},P(u_{Y},P(u_{XY})\) are given by independent Gaussian distributions with mean 0 and variance 1, and each observation \((z,x,y)\) is generated from \((u_{Z},u_{X},u_{Y},u_{XY})\) using the structural assignments: \(z\leftarrow\mathbbm{1}\{u_{Z}<1\},x\leftarrow\mathbbm{1}\{z+u_{XY}-u_{X}>0\}, y\leftarrow\mathbbm{1}\{x-0.5z+2u_{XY}>0\}\).

The deployment domain \(\pi\)* is given by a different SCM \(M^{*}\) that varies in the causal mechanism relating to \(Z\). It is given by the following SCM: \(P(u_{Z}),P(u_{X},P(u_{Y},P(u_{XY})\) are given by independent Gaussian distributions with mean 0 and variance 1, and each observation \((z,x,y)\) is generated from \((u_{Z}),u_{X},u_{Y},u_{XY})\) using the structural assignments: \(z\leftarrow\mathbbm{1}\{u_{Z}<0\},x\leftarrow\mathbbm{1}\{z+u_{XY}-u_{X}>0\}, y\leftarrow\mathbbm{1}\{x-0.5z+2u_{XY}>0\}\).

Hypertension example.To generate data from the patient population \(\pi^{a}\) we choose an SCM \(M^{a}\) compatible with the graph specified as follows: \(P(u_{Z}),P(u_{X},P(u_{W},P(u_{XY})\) are given by independent Gaussian distributions with mean 0 and variance 1, and each observation \((z,x,w,y)\) is generated from \((u_{Z}),u_{X},u_{Y},u_{XY})\) using the structural assignments: \(z\leftarrow\mathbbm{1}\{u_{Z}>0\},x\leftarrow\text{int}\{0.5z+\mathbbm{1}\{u_{ XY}>0\}}+2\mathbbm{1}\{u_{XY}>0\}-\mathbbm{1}\{u_{X}>0\}>0\}+2\mathbbm{1}\{u_{X}>1 \}+1\},w\leftarrow\mathbbm{1}\{0.3x-u_{W}-0.8>0\},y\leftarrow\mathbbm{1}\{w-0. 5z+u_{XY}>0\}\). int\(\{\cdot\}\) stands for the integer part of the content of the brackets.

The domain \(\pi\)* in which the MAB is deployed is given by a different SCM \(M^{*}\) that varies in the causal mechanism relating to \(Z\) and \(W\) with respect to \(M^{b}\), and varies in \(W\) with respect to \(M^{a}\). It is defined by: \(P(u_{Z}),P(u_{X},P(u_{W},P(u_{XY})\) are given by independent Gaussian distributions with mean 0 and variance 1, and each observation \((z,x,w,a,y)\) is generated from \((u_{Z}),u_{X},u_{Y},u_{XY})\) using the structural assignments: \(z\leftarrow\mathbbm{1}\{u_{Z}>0\},x\leftarrow\text{int}\{0.5z+\mathbbm{1}\{u_ {XY}>0\}}+2\mathbbm{1}\{u_{XY}>0\}-\mathbbm{1}\{u_{X}>0\}+2\mathbbm{1}\{u_{X}> 1\}+1\},w\leftarrow\mathbbm{1}\{0.2x-u_{W}-1.8>0\},y\leftarrow\mathbbm{1}\{w-0.5z+u_{XY}>0\}\).

Digital advertising example.This example considers data from two domains. Data from the source domain \(\pi^{a}\) is given by a SCM \(M^{a}\) compatible with the graph that is specified as follows: \(P(u_{Z}),P(u_{X},P(u_{W},P(u_{XW},P(u_{WY},P(u_{A})\) are given by independent Gaussian distributions with mean 0 and variance 1, and each observation \((z,x,w,a,y)\) is generated from a sample \(u_{Z},u_{X},u_{W},u_{XW},u_{WY},u_{A}\) using the structural assignments: \(w\leftarrow\mathbbm{1}\{u_{XW}+u_{WY}>0\},z\leftarrow\mathbbm{1}\{u_{Z}+w>0\},a\leftarrow\mathbbm{1}\{u_{A}>0\},x\leftarrow\text{int}\{0.5z-0.5a+\mathbbm{1 }\{u_{XW}>0\}+2\mathbbm{1}\{u_{XW}>0.5\}-\mathbbm{1}\{u_{X}>0\}+1\},y\leftarrow \mathbbm{1}\{0.2x-0.5a+u_{WY}-1>0\}\).

The domain \(\pi\)* in which the MAB is deployed is given by a different SCM \(M\)* that varies in the causal mechanism relating to \(A\) with respect to \(M^{a}\). It is defined by: \(P(u_{Z}),P(u_{X},P(u_{W},P(u_{XW},P(u_{WY},P(u_{A})\) are given by independent Gaussian distributions with mean 0 and variance 1, and each observation \((z,x,w,a,y)\) is generated from a sample \(u_{Z},u_{X},u_{W},u_{XW},u_{WY},u_{A}\) using the structural assignments: \(w\leftarrow\mathbbm{1}\{u_{XW}+u_{WY}>0\},z\leftarrow\mathbbm{1}\{u_{Z}+w>0\},a\leftarrow\mathbbm{1}\{u_{A}>0.5\},x\leftarrow\text{int}\{0.5z-0.5a+\mathbbm{1 }\{u_{XW}>0\}+2\mathbbm{1}\{u_{XW}>0.5\}-\mathbbm{1}\{u_{X}>0\}+1\},y\leftarrow \mathbbm{1}\{0.2x-0.5a+u_{WY}-1>0\}\).

Gibbs sampling

This section gives the derivation of all conditionals using our parameterization of causal effects for the bow graph with confounding example.

The query of interest is given by \(P^{*}(y_{x}=1)\) which is first approximated using source data \(\bar{\bm{v}}\) and then updated using interventional data \(\bm{v}_{x^{(1)}},\bm{v}_{x^{(2)}},\dots\) collected by the MAB in the deployment environment \(\pi^{*}\).

Its parameterization, following Corol. 1, is given by

\[P^{*}(y_{x}=1)=\sum_{u_{xy},u_{z}}\mathbbm{1}\{\xi_{Y}^{(x,z,u_{xy})}=y\} \mathbbm{1}\{\xi_{Z}^{(u_{z})}=z\}\theta_{u_{xy}}\theta_{u_{z}},\] (32)

where \(\xi_{Y}^{(x,z,u_{xy})}\) and \(\theta_{u_{xy}}\) parameters are shared across source and deployment environments, while \(\mathbbm{1}\{\xi_{Z}^{(u_{z})}=z\}\) and \(\theta_{u_{z}}\) is specific to the deployment environment. We start by approximating the posterior of all relevant parameters, that is \(\xi_{Y}^{(x,z,u_{xy})}\) and \(\theta_{u_{xy}}\), given \(\bar{\bm{v}}\), before interacting with the deployment environment \(\pi^{*}\). Note that prior data \(\bar{\bm{v}}\) is not relevant for estimating \(\mathbbm{1}\{\xi_{Z}^{(u_{z})}=z\}\) and \(\theta_{u_{z}}\) and thus we maintain uniform prior distributions over these parameters at this stage. In the following, we give the derivation of the complete conditionals over \(\xi_{Y}^{(x,z,u_{xy})}\) and \(\theta_{u_{xy}}\).

1. Sampling from \(P(\bar{u}_{xy},\bar{u}_{z}\mid\bar{\bm{v}},\bm{\xi},\bm{\theta})\). Let \(\bar{u}_{xy}=\{u_{xy}^{(n)},n=1,\dots,N\}\) and \(\bar{u}_{z}=\{u_{z}^{(n)},n=1,\dots,N\}\) denote \(N\) independent samples of \(U_{xy}\) and \(U_{z}\) respectively, one corresponding to each observation \(\bm{v}^{(n)}=(x^{(n)},y^{(n)},z^{(n)})\), where is the number of prior data samples. The complete conditional can be derived following the functional dependencies in the underlying SCM given by the causal graph, \[P(u_{xy}^{(n)},u_{z}^{(n)}\mid\bar{\bm{v}},\bm{\xi},\bm{\theta})=P (u_{xy}^{(n)},u_{z}^{(n)}\mid\bm{v}^{(n)},\bm{\xi},\bm{\theta})\propto P(u_{ xy}^{(n)},u_{z}^{(n)},\bm{v}^{(n)}\mid\bm{\xi},\bm{\theta})\] \[=P(y^{(n)}\mid x^{(n)},z^{(n)},u_{xy}^{(n)})P(x^{(n)}\mid z^{(n)},u_{xy}^{(n)})P((z^{(n)}\mid u_{z}^{(n)})P(u_{z}^{(n)})P(u_{xy}^{(n)})\] \[=\mathbbm{1}\{\xi_{Y}^{(x^{(n)},z^{(n)},u_{xy})}=y^{(n)}\} \mathbbm{1}\{\xi_{X}^{(z^{(n)},u_{xy})}=x^{(n)}\}\mathbbm{1}\{\xi_{Z}^{(u_{z} ^{(n)})}=(z^{(n)})\}\theta_{u_{xy}^{(n)}}\theta_{u_{z}^{(n)}},\] where we have replaced the probabilities with the corresponding parameters that are used to define them.
2. Sampling from \(P(\xi_{Y}^{(x,z,u_{xy})}\mid\bar{\bm{v}},\bar{\bm{u}},\bm{\theta})\). Similarly, for fixed \(x,z,u_{xy}\), parameter \(\xi_{Y}^{(x,z,u_{xy})}\) is mutually independent of any other parameter in \(\bm{\xi}\) given \(\bar{\bm{v}},\bar{\bm{u}},\bm{\theta}\) and can be sampled separately. Recall that by definition of the underlying SCM \(\xi_{Y}^{(x,z,u_{xy})}\) represent a deterministic mapping between inputs \(x,z,u_{xy}\) and output \(y\in\Omega_{Y}\). The value \(\xi_{Y}^{(x,z,u_{xy})}\in\Omega_{Y}\) is therefore implicitly determined by the current values \(\bar{\bm{v}},\bar{\bm{u}}\): if there exists a tuple \((x^{(n)}=x,z^{(n)}=z,u_{xy}^{(n)}=u_{xy},y^{(n)}=y)\) for some \(n=1\dots,N\), then by definition \(\xi_{Y}^{(x,z,u_{xy})}:=y\) with probability 1. If no such tuple exist, then the distribution of \(\xi_{Y}^{(x,z,u_{xy})}\) remains uniform over its domain \(\Omega_{Y}\) as none of the data points carries information as to the mapping \((x,z,u_{xy})\mapsto y\).
3. Sampling from \(P(\theta_{u_{xy}}\mid\bar{\bm{v}},\bar{\bm{u}},\bm{\theta})\). The conditional distribution over \(\theta_{u_{xy}}\) given \(\bar{\bm{v}},\bar{\bm{u}}\) is given by a Dirichlet distribution following the conjugacy of it with regard to categorical distributions of \(U_{xy}\) and can be updated by adding the counts of each outcome \(\bar{u}_{xy}\) of \(U_{xy}\) in the current sample to the corresponding prior concentration parameters of the Dirichlet prior distribution.

This process eventually forms a chain of samples from the correct posterior distribution of each parameter. At this stage, the MAB is deployed in \(\pi^{*}\) and all parameters may be updated both with prior data \(\bar{\bm{v}}\) as well as with additional samples \(\bm{v}_{x}\) collected online in every round of experimentation. Offline and online data points are different in kind, and contribute to updating parameters differently. For shared parameters, both types of data may be used while for parameters specific to \(\pi^{*}\) only newly collected interventional data samples will be relevant in posterior computations.

\(\xi_{Y}^{(x,z,u_{xy})}\) is updated using the posterior \(P(\xi_{Y}^{(x,z,u_{xy})}\mid\bar{\bm{v}},\bm{v}_{x^{(1)}},\dots,\bm{v}_{x^{(t-1 )}},\bar{\bm{u}},\bm{u}_{x^{(1)}},\dots,\bm{u}_{x^{(t-1)}})\), while \(\xi_{Z}^{(u_{z})}\) is updated using the posterior \(P(\xi_{Z}^{(u_{z})}\mid\bar{\bm{v}},\bm{v}_{x^{(1)}},\dots,\bm{v}_{x^{(t-1)}}, \bar{\bm{u}},\bm{u}_{x^{(1)}},\dots,\bm{u}_{x^{(t-1)}})=\)\(P(\xi_{Z}^{(u_{z})}\mid\bm{v}_{x^{(1)}},\ldots,\bm{v}_{x^{(t-1)}},\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})\) using the same intuition as described above. Similarly, \(\theta_{u_{x_{y}}}\) is updated using the posterior \(P(\theta_{u_{x_{y}}}\mid\bar{\bm{u}},\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})\) while \(\theta_{u_{x}}\) is updated using the posterior \(P(\theta_{u_{z}}\mid\bar{\bm{u}},\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})=P (\theta_{u_{z}}\mid\bm{u}_{x^{(1)}},\ldots,\bm{u}_{x^{(t-1)}})\).