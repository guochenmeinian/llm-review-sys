# GEO-Bench:

Toward Foundation Models for Earth Monitoring

 Alexandre Lacoste\({}^{*}{}^{1}\)

Hannah Kemer\({}^{4}\)

Bjorn Lutgens\({}^{5}\)

Jeremy Irvin\({}^{3}\)

David Dao\({}^{6}\)

Hamed Alemohammad\({}^{7}\)

Alexandre Drouin\({}^{1,8}\)

Mehmet Gunturkun\({}^{1}\)

Gabriel Huang\({}^{1,9}\)

David Vazquez\({}^{1}\)

Dava Newman\({}^{5}\)

Yoshua Bengio\({}^{8,9}\)

Stefano Ermon\({}^{3}\)

Xiao Xiang Zhu\({}^{2}\)

\({}^{1}\) ServiceNow Research \({}^{2}\) Technical University of Munich \({}^{3}\) Stanford University \({}^{4}\) Arizona State University \({}^{5}\) MIT \({}^{6}\) ETH Zurich \({}^{7}\) Clark University \({}^{8}\) Mila-Quebec \({}^{9}\) University of Montreal

###### Abstract

Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. Such models, recently coined _foundation models_, have been transformational to the field of natural language processing. Variants have also been proposed for image data, but their applicability to remote sensing tasks is limited. To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation. We accompany this benchmark with a robust methodology for evaluating models and reporting aggregated results to enable a reliable assessment of progress. Finally, we report results for 20 baselines to gain information about the performance of existing models. We believe that this benchmark will be a driver of progress across a variety of Earth monitoring tasks.

## 1 Introduction

Earth monitoring with machine learning-based methods plays an increasing role in climate change mitigation and adaptation as well as climate science [57]. Related applications include methane source detection [61, 16], forest carbon quantification [44], extreme weather prediction [49], and crop monitoring [34, 14]. Across many of these applications, pre-trained models (e.g., a ResNet trained on ImageNet) have been used to increase generalisation performance. Improvement of the pre-trained models has been shown to reduce the need for large labelled datasets in some contexts [11] and can improve model generalisation outside of the training distribution [28]. Recent studies exploring the scaling of such pre-trained models found that increasing the size of an unsupervised (or weakly supervised) dataset as well as properly scaling the model led to an even greater increase in performance under various metrics [33, 55].

While the training of such large-scale models is usually reserved for industrial research groups with very large computer clusters, the publication of pre-trained models creates vast opportunities for the entire research and technology community (including communities of domain experts outside of machine learning). These large pre-trained models were recently coined as _foundation models_[6] as they might serve as foundations for sub-fields of machine learning. Specifically, the publication of large pre-trained models like BERT [15] and GPT-3 [7] led to a paradigm shift in the field of natural language processing (NLP). This inspired a similar shift in the field of computer vision with the release of models like CLIP [55] and DINO [9]. While CLIP performs well on various types of vision tasks, it still under-performson Earth monitoring tasks [55]. This is not surprising as it is trained mainly on RGB images taken from a ground perspective at a single point in time.

While there are many similarities between Earth observation datasets and typical ML image datasets, there are also many important differences to consider when designing effective ML models. Earth observation images are taken from an overhead rather than ground perspective, usually from a fixed distance from the Earth's surface (defined by a satellite's orbit). The satellite revisits provide a temporal axis that is sometimes irregular (e.g., a few times per year) or regular (e.g., every five days) with cloud coverage causing spurious occlusions. Images are acquired with sensors containing multiple spectral bands (e.g., thirteen for Sentinel-2), or even with different kinds of sensors, e.g., synthetic aperture radar (SAR), which can penetrate cloud coverage. Moreover, the GPS coordinates and timestamp of each acquisition offer the opportunity to combine data from multiple sources, e.g., weather data, semantic maps, and elevation. This leads to a rich multi-modal signal with potentially missing information that can be inferred from other elements of the signal. There are currently petabytes of accessible satellite datasets containing images of the Earth under various modalities from the present day to as far back as the 1960s. Distilling this large amount of information into pre-trained models of various sizes offers the opportunity to redistribute this information and make it accessible to various labs for increasing the performances on a large range of downstream tasks.

The fundamental goal of these large pre-trained models is to improve generalization performance on downstream tasks. Hence, to support the machine learning community in producing better pre-trained models, it is crucial to provide a benchmark with a wide variety of downstream tasks, covering a range of modalities and dataset shapes that are likely to be encountered in practice. At the moment, existing works on pre-training models from earth observations e.g., [13; 46; 69], evaluate on different sets of downstream tasks, making it impossible to directly compare performance. Moreover, the set of tasks is often narrow in terms of diversity and the statistical methodologies do not adequately report the uncertainties in the evaluation.

The present work aims to fill this void by providing a wide range of tasks across various countries with various modalities of sensors. Also, the transformed versions of the datasets are smaller than their original form, and all results can be replicated on single GPUs. This increases accessibility to research labs with limited resources and reduces overall energy consumption. Our proposed benchmark, GEO-Bench1, is composed of six image classification and six semantic segmentation tasks, which were curated by domain experts to ensure their diversity and relevance toward sustainable development. We expect this contribution to:

Footnote 1: https://zenodo.org/communities/geo-bench

* Stimulate and facilitate the development of foundation models for Earth monitoring
* Provide a systematic way of measuring the quality of models for better scientific progress
* Provide insights into which pre-trained models work best
* Potentially reduces negative impacts of foundation models through an open evaluation procedure.

In what follows, we start by discussing sources of data that can serve to train foundation models for earth monitoring (Sec. 2). We then present the details of GEO-Bench (Sec. 3) and how it can be used for the evaluation of foundation models (Sec. 4). Further, we review existing benchmark datasets for earth monitoring and discuss why GEO-Bench is complementary (Sec. 5). Finally, we present an extensive set of experiments, showing the performance of 20 state-of-the-art models on the benchmark to lay down reference points and to gain valuable information on existing pre-trained models (Sec. 6).

## 2 Remote sensing data for self-supervision

The development of foundation models does not typically rely on a specific dataset for the pre-training phase. The choice of data source is part of the design of the model, e.g., a very large corpus of text from the internet [50] or pairs of text associated with images from the web [55]. As such, we do not provide data for training foundation models with this benchmark. However, for completeness, we outline potential sources of Earth observation data that could be used for pre-training foundation models.

Multispectral images with revisits Satellite data sources such as Sentinel-2 [20; 23] and Landsat 8 [66] provide images in multiple spectral bands with periodic revisits. This yields a four-dimensional array of structured data (longitude, latitude, wavelength, time) which can be used to perform various forms of self-supervision, e.g., predicting adjacent tiles [30] or contrasting the different seasons for the same region [46].

Other sensors Synthetic Aperture Radar (SAR) and terrain elevation are also frequently available and can be matched to other sources of data through geolocalisation [54]. Such data are complementary to optical spectral bands and may encourage the model to learn higher-level semantic representations.

Semantic data Through georeferencing, text-based data such as Wikipedia articles can be linked to satellite images [67]. It is also possible to join content from non-image data layers like OpenStreetMap [39]. By predicting or contrasting information from these sources, the model may learn useful and transferable semantic representations.

## 3 GEO-Bench

GEO-Bench is composed of 6 classification tasks and 6 segmentation tasks. Detailed characteristics are presented in Table 1, examples are depicted in Figure 2 and 3, and the spatial coverage on the world map is presented in Figure 8 (supplementary material). In what follows, we describe the procedure for collecting and transforming the datasets.

### Design Principles

GEO-Bench was established by modifying and gathering geospatial datasets, adhering to principles that secure accessibility, usability, and effective model performance assessment across tasks.

Ease of Use A fundamental goal was to create an accessible, simple-to-use benchmark, and a compact dataset assortment with code for loading the data in a consistent schema. A key aim was to harmonize data to reduce the engineering work needed to tailor pre-trained architectures, while maintaining sensor type and resolution diversity.

Sector Experts and Steering Committee To align GEO-Bench with practical use-cases, we assembled a team of six sector experts from fields such as forestry and climate science. A steering committee of respected scientists guides high-level benchmark decisions, assuring relevance and impact.

Diversity of Modalities The objective is to evaluate model adaptability to varied geospatial sensors. Thus, the benchmark encompasses multispectral, SAR, hyperspectral, elevation, and cloud probability modalities, with spatial resolutions from 0.1 to 30 m/pixel.

Diversity of Tasks We ventured beyond image classification, incorporating object detection and semantic segmentation. To maintain _ease of use_, detection and counting tasks were transformed into semantic segmentation. This led to two task sets: six image classification tasks, and six semantic segmentation tasks [25; 38].

Original Train, Validation, and Test Splits Original dataset splits were preserved when available; otherwise, we generated validation and test sets from the train set while ensuring no spatial overlap.

Permissive License Most datasets needed to be adapted from their original form to satisfy the above criteria and be included in the benchmark. Hence, we include only datasets with permissive licenses.

Figure 1: Foundation models encapsulate multimodal data streams through self-supervised training. The trained models can then be fine-tuned for a variety of climate-related remote sensing tasks. Image sources: quantification [44], detection [32], generation [43], counting [36], segmentation [75], and multi-class classification [51].

### Dataset Transformations

To produce a benchmark that complies with the design choices of Section 3.1, we applied the following transformations to each dataset. The procedure that was used to download and transform each dataset is fully documented and open-sourced in the GEO-Bench GitHub repository2.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{**Classification**} & \multicolumn{1}{c}{Image Size} & \# Classes & Train & Val & Test & \# Bands & RGB res & Sensors & Cite & License \\ \hline m-biggerather & 120 x 120 & 43 & 20000 & 1000 & 1000 & 12 & 10.0 & Sentinel-2 & [40] & CDLA-P-1.0 \\ m-sq2sat & 32 x 32 & 17 & 19992 & 986 & 986 & 18 & 10.0 & Sentinel-2 & [70] & CC-BY-4.0 \\ m-brick-klin & 64 x 64 & 2 & 15063 & 999 & 999 & 13 & 10.0 & Sentinel-2 & [77] & CC-BY-S4.0 \\ m-foresnet & 332 x 332 & 12 & 6464 & 989 & 993 & 6 & 15.0 & Landsat-8 & [20] & CC-BY-4.0 \\ m-cursat & 64 x 64 & 10 & 2000 & 1000 & 1000 & 13 & 10.0 & Sentinel-2 & [77] & MIT \\ m-pq4ger & 320 x 320 & 2 & 11814 & 999 & 999 & 3 & 0.1 & RGB & [40] & MIT \\ \hline \multicolumn{1}{c}{**Segmentation**} & \multicolumn{1}{c}{} & & & & & & & & & & \\ \hline Name & Image Size & \# Classes & Train & Val & Test & \# Bands & RGB res & Sensors & Cite & License \\ \hline m-pq4-seq & 320 x 320 & 2 & 3000 & 403 & 403 & 3 & 0.1 & RGB & [40] & MIT \\ m-chesupake-landcover & 256 x 256 & 7 & 3000 & 1000 & 1000 & 4 & 1.0 & RGB & [40] & CDLA-P-1.0 \\ m-cashve-plantation & 256 x 256 & 7 & 1150 & 400 & 50 & 13 & 10.0 & Sentinel-2 & [74] & CC-BY-4.0 \\ m-S\(\times\)-crop-type & 256 x 256 & 10 & 3000 & 1000 & 1000 & 13 & 10.0 & Sentinel-2 & [74] & CC-BY-4.0 \\ m-m-cattle & 500 x 500 & 2 & 524 & 66 & 65 & 3 & 0.1 & RGB & [40] & CC-BY-4.0 \\ m-NeonTree & 400 x 400 & 2 & 270 & 94 & 93 & 5 & 0.1 & RGB & [71] & CC 1.0 \\ \multicolumn{1}{c}{} & & & & & & & & & & & \\ \multicolumn{1}{c}{} & & & & & & & & & & & & \\ \multicolumn{1}{c}{} & & & & & & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: **GEO-Bench: Characteristics of datasets in the benchmark. Since datasets are _modified_, we prepend their name with “m-” to distinguish them from the original dataset.**

Figure 3: Representative samples of the **segmentation benchmark**.

Figure 2: Representative samples of the **classification benchmark**.

**Subsampling Large Datasets** To be more representative of typical downstream tasks, where data is usually scarce, datasets larger than \(20000\) samples were randomly subsampled. Avoiding large downstream tasks also comes with other benefits:

* In Appendix A, we show that larger downstream datasets can decrease the ability to discriminate between two models that are similar in performance.
* Downstream tasks with very large training sets will not usually benefit from pre-training3. Hence they are less useful for our evaluation purpose. Footnote 3: From Bayes rule, we know that the influence of the prior (pre-trained model) decreases as the size of the training data increases.
* A smaller benchmark is faster to download, yields results quicker and requires less energy for computation.
* We can increase the variety of experiments and the number of seeds to improve the knowledge gained from experiments.

**Removing Class Imbalance** We randomly subsampled large classes to have near-uniform class sizes across datasets. This was done to prevent users of the benchmark from increasing their score by using clever class imbalance techniques instead of making progress on better pre-trained models. While good performance on highly imbalanced (long tail of classes) datasets would be a desired property of a pre-trained model, we have not found a good dataset containing a large number of classes.

## 4 Using The Benchmark

**Fine Tuning** In the self-supervised learning literature, it is common to use the pre-trained model to encode a fixed representation of each image in the dataset and learn to classify images based on this representation [30]. While this works relatively well, this method highly depends on the pre-training task as it may not learn to encode information that is important for the downstream task [65; 53]. In practice, fine-tuning the pre-trained model often mitigates this issue and is known to frequently yield a much higher generalization performance than a model trained from random weights [46; 11]. Since this is more representative of practical usage, we encourage users of the benchmark to report the results of fine-tuned models. On the other hand, we do not discourage users from also reporting results with fixed backbones (pre-trained weights) as this can provide valuable information about the pre-trained model. In all cases, we ask users to report their fine-tuning methodology with enough details for reproducibility.

**Hyperparameter Tuning** Deep learning algorithms often require the adjustment of hyperparameters, especially when an architecture is fine-tuned on a small dataset. For this reason, we recommend adjusting hyperparameters, but within a maximum budget of 16 trials per task4. Early stopping based on validation metrics is also recommended.

Footnote 4: While 16 is fairly small, we believe it’s enough to adjust sensitive hyperparameters such as learning rate. Also, this favours models that are less sensitive to hyperparameter tuning.

**Data Augmentation** Data augmentation plays a crucial role in the training of deep learning models, especially with small training datasets. Hence, we consider it to be part of the fine-tuning process. As a guideline, we propose limiting the augmentations to \(90^{\circ}\) rotations and vertical and horizontal flips5. On the other hand, we also encourage users to study what are the best data augmentations for remote sensing as this could lead to useful findings for practitioners and the benchmark is well-suited for evaluating such findings.

Footnote 5: Random crop and resize are also common in vision, but in remote sensing, this reduces the spatial resolution, which is often crucial for high performances.

**Toolbox** To facilitate the usage of the benchmark, we provide a collection of tools for various parts of the experimental pipeline as part of the open-sourced codebase6. This includes tools for loading datasets and visualising results. We also provide tools based on PyTorch-Lightning [24] to facilitate model training.

Footnote 6: https://github.com/ServiceNow/geo-bench

### Reporting Results

For reliable and comparable results across different publications, we recommend that users follow this procedure to report results. The aim is to report results on individual tasks as well as aggregated across all tasks, with reliable confidence intervals (inspired by [2]). Code is provided to generate figures based on raw results.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_EMPTY:7]

### Protocol

For each model, we replaced the last layer with a randomly initialised layer of the appropriate shape for the task at hand. We use different learning rates for the last layer (which starts from random weights) and for the backbone (which starts from pre-trained weights). The best learning rates were selected using the highest accuracy or Intersection over Union (IoU) on the validation set over 16 trials 8. After choosing the hyperparameters, we repeated the training for 10 seeds. To minimize overfitting, we selected the best time step using accuracy (or IoU) on the validation set and we reported the test metrics at the chosen time step. We use AdamW [42] to train convolution architectures and SGD to train transformer architectures.

Footnote 8: The range of selected learning rates is different for each model and is selected based on early experiments, see appendix for details.

### Classification

#### 6.2.1 Baselines Naming Schema

Each baseline name starts with the corresponding architecture: **ResNet18 and ResNet50:** standard ResNet architectures [26]; **ConvNeXt-B:** the base architecture of ConvNeXt [41]; **ViT-T and ViT-S:** ViT architectures [19] of size tiny and small respectively; **SwinV2-T:** a SwinV2-tiny architecture [40];

Then, keywords provide details about the training procedure: **SeCo:** a ResNet50 model trained on Sentinel 2 data with temporal contrastive loss across seasons [46]; **MoCo-S2 and DINO-S2:** model trained with self-supervision on Sentinel data [70] (RGB and Multispectral pre-trained weights); **Rnd:** weights are randomly initialised; **timm:** pre-trained weights are obtained from the timm library, usually from training on ImageNet; **+R-Multi:** we manually augment an RGB architecture by randomly initialising the weights of the missing channels in the 1st layer; **multi:** the pre-trained model has multispectral channels.

#### 6.2.2 Comparing Baselines on RGB only

In Figure 4, we report bootstrapped IQM of the normalized accuracy (Sec 4.1) for the six datasets of the classification benchmark, as well as aggregated results9. In this first experiment, all models can only see the RGB channels.

Footnote 9: We note that the variance of the results represents the uncertainty of the mean (IQM) which is significantly smaller than the variance of the raw seeds presented in Figure 9 in Appendix.

These results offer valuable information across 10 common baselines in the literature. We denote the outstanding performance of ConvNext and SwinV2 compared to other models. It is by a large margin the best models in aggregated results and almost systematically outperforms all models on all datasets. We can also observe the large difference between Scratch ResNet18 and ResNet18 on all datasets. This highlights the importance of using a pre-trained model. Also, perhaps disappointingly, the existing model pre-trained on remote sensing data does not exhibit any improvement compared to their timm pre-trained weights, i.e., ResNet18-MoCo-S2, ResNet50-MoCo-S2, and ResNet50-SeCo-S2 are all comparable to ResNet18 on the aggregated performance. On the other hand, in Section 6.2.4, we see that ResNet50-MoCo-S2-multi can leverage multispectral data to slightly surpass ResNet50-timm.

Another insight that can be gained from these results is how useful a dataset is at discriminating baselines, i.e., a dataset where most baselines perform equally would have limited utility in our benchmark. To this end, we had to discard GeoLiteClef 2022 [12] as all models were performing equally badly10. m-eurosat also offers limited discriminativity as most models obtain very high accuracy (see Figure 9). To make this dataset harder, we subsample down to 2000 training samples. We can now see that smaller models tend to perform better on this dataset, but the discriminativity remains fairly low.

Footnote 10: We suspect this dataset to have high aleatoric uncertainties.

#### 6.2.3 Accuracy vs training set size

As part of the benchmark, we also provide official subsets of the training sets with train ratios of (0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1) 11.

Figure 5 depicts a different perspective on the models. First, we can observe the noise due to the hyperparameter selection process that is not accounted for by repeating 10 seeds with fixed hyperparameters. Also, we see that ConvNeXt often becomes better than SwinV2 as the training set decreases. This coincides with the common observations that transformer architectures tend to be more data-hungry, but also tend to outperform convolution architectures in the high data regime [18]. We note also, that ConvNeXt-B-timm only requires 2% of the training set to obtain aggregated performances comparable to that of ResNet18-Rnd. This impressive factor of 50x on data efficiency highlights the importance of developing new architectures and new pre-training methods. Finally, we can observe an increase in the discriminativity of the datasets as the training set decreases, specifically for m-eurosat, when the task becomes more difficult, the strong baselines stand out even more. The discriminativity of datasets is further studied in Section A.5.

#### 6.2.4 Leveraging Multispectral Information

We now study the effect of leveraging multispectral information during the pre-training phase and during the fine-tuning phase. We do so by fixing the backbone to either ResNet50 (Fig. 6) or ViT-S (Fig. 7) and exploring various weight initialisation schema. Since we could only find pre-trained models for Sentinel-2, we limit this experiment to the four datasets satisfying this criterion.

We found that using a model pre-trained on RGB-only (timm pre-trained) and augmenting the architecture by randomly initialising the weights of the missing channels in the first layer (+RMulti) does not lead to systematic improvement. Moreover, the fine-tuning time is largely extended since we have to wait until the newly initialised weights on the first layer fully converge. On the other hand, the ResNet50 pre-trained on Sentinel-2 using DINO or MoCo [70] leads to a modest performance increase on average. When looking at ViT-S (Fig. 7), incorporating multi-spectral only leads to a systematic performance decrease.

### Segmentation

We defer experiments on the Segmentation benchmark to Appendix A.3, where we provide experiments on six baselines (ResNet18, ResNet50, ResNet101) \(\times\) (U-Net, DeepLabV3) with pre-trained weights provided by the timm library. While ResNet101-DeepLabV3 performs best in aggregate, it still underperforms on some datasets.

### Resource Usage

See Appendix A.6 for detailed resource usage of each algorithm evaluated in this section. We report the number of parameters, memory usage, the time required for a forward pass, and the convergence time for fine-tuning on downstream tasks. While memory footprint can increase by a factor of 4x for a model like SwinV2 and ConvNeXt-B compared to ResNet50, their forward pass is only twice as slow.

## 7 Conclusion

We developed a new benchmark for evaluating pre-trained models on remote sensing downstream tasks. This involves adapting a variety of remote sensing datasets to a more conventional machine learning pipeline and providing code for fine-tuning and evaluating individual tasks. We expect that this benchmark will stimulate the development of new foundation models that could lead to better generalization on a variety of earth monitoring downstream tasks and could open up opportunities for new applications.

LimitationsOur benchmark does not extensively evaluate all desired features of a pre-trained model for earth monitoring. For example, it does not evaluate its ability to fine-tune temporal data nor perform fusion with other types of data such as text or weather. The spatial coverage of the benchmark covers most continents and improves coverage over individual datasets. However, the spatial coverage could still be largely improved to include a much wider range of countries and biomes. Finally, as pre-trained models become stronger, they will get closer to the theoretical limit of generalization performance, i.e. approaching the aleatoric uncertainty of the dataset. Under such a regime, we expect a bigger overlap between error bars when comparing 2 different models.

## References

* [1] Diab Abuaiadah and Alexander Switzer. Remote sensing dataset for detecting cows from high resolution aerial images. 2022.

* Agarwal et al. [2021] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in neural information processing systems_, 34:29304-29320, 2021.
* Alemohammad [2021] Hamed Alemohammad. The case for open-access ML-ready geospatial training data. In _International Geoscience and Remote Sensing Symposium_. IEEE, 2021.
* Bellemare et al. [2013] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013.
* Bender et al. [2021] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 610-623, 2021.
* Bommasani et al. [2021] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* Brown et al. [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.
* Burke et al. [2021] Marshall Burke, Anne Driscoll, David B Lobell, and Stefano Ermon. Using satellite imagery to understand and promote sustainable development. _Science_, 371(6535), 2021.
* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. _arXiv preprint arXiv:2104.14294_, 2021.
* Chen et al. [2017] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* Cole et al. [2020] Elijah Cole, Benjamin Deneu, Titouan Lorieul, Maximilien Servajean, Christophe Botella, Dan Morris, Nebojsa Jojic, Pierre Bonnet, and Alexis Joly. The goififeclef 2020 dataset. _arXiv preprint arXiv:2004.04192_, 2020.
* Cong et al. [2022] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall Burke, David B Lobell, and Stefano Ermon. Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery. _arXiv preprint arXiv:2207.08051_, 2022.
* Dado et al. [2020] Walter T Dado, Jillian M Deines, Rinkal Patel, Sang-Zi Liang, and David B Lobell. High-resolution soybean yield mapping across the us midwest using subfield harvester data. _Remote Sensing_, 12(21):3471, 2020.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Dileep et al. [2020] Sonu Dileep, Daniel Zimmerle, J Ross Beveridge, and Timothy Vaughn. Automated identification of oil field features using cnns. 2020.
* Dimitrovski et al. [2023] Ivica Dimitrovski, Ivan Kitanovski, Dragi Kocev, and Nikola Simidjievski. Current trends in deep learning for earth observation: An open-source benchmark arena for image classification. _ISPRS Journal of Photogrammetry and Remote Sensing_, 197:18-35, 2023.
* Dosovitskiy et al. [2010] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arxiv 2020. _arXiv preprint arXiv:2010.11929_, 2010.
* Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* Drusch et al. [2012] Matthias Drusch, Umberto Del Bello, Sebastien Carlier, Olivier Colin, Veronica Fernandez, Ferran Gascon, Bianca Hoersch, Claudia Isola, Paolo Laberniti, Philippe Martimort, et al. Sentinel-2: Esa's optical high-resolution mission for gmes operational services. _Remote sensing of Environment_, 120:25-36, 2012.
* Eforn [1979] B Eforn. Bootstrap methods: another look at the jackknife. _The Annals of Statistics_, 7:1-26, 1979.

* [22] EPA. Greenhouse Gas Emissions: Understanding Global Warming Potentials. Technical report, US Environmental Protection Agency, February 2017.
* [23] ESA. Sentinel-2. Technical report, European Space Agency, Paris, France, 2021.
* [24] William Falcon and The PyTorch Lightning team. PyTorch Lightning, 3 2019.
* [25] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. _IEEE transactions on pattern analysis and machine intelligence_, 32(9):1627-1645, 2010.
* [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [27] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 2019.
* [28] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. _arXiv preprint arXiv:1906.12340_, 2019.
* [29] Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Yu, Sharon Zhou, Kyle Story, Rose Rustowicz, Cooper Elsworth, Kemen Austin, and Andrew Y Ng. Forestnet: Classifying drivers of deforestation in indonesia using deep learning on satellite imagery. _arXiv preprint arXiv:2011.05479_, 2020.
* [30] Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon. Tile2vec: Unsupervised representation learning for spatially distributed data. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 3967-3974, 2019.
* [31] Forrest Johnson, Andrew Wlazlo, Ryan Keys, Viren Desai, Erin Wetherley, Ryan Calvert, and Elena Berman. Airborne methane surveys pay for themselves: An economic case study of increased revenue from emissions control. preprint, Environmental Monitoring, July 2021.
* an ai-driven approach to quantifying methane point-source emission from high-resolution 2-d plume imagery. _ICML Workshop on Tackling Climate Change with AI_, 2021.
* [33] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* [34] Hannah Kerner, Gabriel Tseng, Inbal Becker-Reshef, Catherine Nakalembe, Brian Barker, Blake Munshell, Madhava Paliyam, and Mehdi Hosseini. Rapid response crop maps in data sparse regions. _arXiv preprint arXiv:2006.16866_, 2020.
* [35] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. _arXiv preprint arXiv:1910.09700_, 2019.
* [36] Issam Laradji, Pau Rodriguez, Freddie Kalatizis, David Vazquez, Ross Young, Ed Davey, and Alexandre Lacoste. Counting cows: Tracking illegal cattle ranching from high-resolution satellite imagery. _arXiv preprint arXiv:2011.07369_, 2020.
* [37] Jihyeon Lee, Nina R. Brooks, Fahim Tajwar, Marshall Burke, Stefano Ermon, David B. Lobell, Debashish Biswas, and Stephen P. Luby. Scalable deep learning to identify brick kilns and aid regulatory capacity. _Proceedings of the National Academy of Sciences_, 118(17), 2021.
* [38] Victor Lempitsky and Andrew Zisserman. Learning to count objects in images. _Advances in neural information processing systems_, 23, 2010.
* [39] Haifeng Li, Xin Dou, Chao Tao, Zhixiang Wu, Jie Chen, Jian Peng, Min Deng, and Ling Zhao. Rsi-cb: A large-scale remote sensing image classification benchmark using crowdsourced data. _Sensors_, 20(6):1594, 2020.
* [40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12009-12019, 2022.
* [41] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11976-11986, 2022.

* [42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [43] Bjorn Lutjens, Brandon Leshchinskiy, Christian Requena-Mesa, Farrukh Chishtie, Natalia Diaz-Rodriguez, Oceane Boulais, Aruna Sankaranarayanan, Aaron Pina, Yarin Gal, Chedy Raissi, Alexander Lavin, and Dava Newman. Physically-consistent generative adversarial networks for coastal flood visualization. _ICML Workshop on AI for Modeling Oceans and Climate Change (AIMOCC)_, 2021.
* [44] Bjorn Lutjens, Lucas Liebenwein, and Katharina Kramer. Machine learning-based estimation of forest carbon stocks to increase transparency of forest preservation efforts. _2019 NeurIPS Workshop on Tackling Climate Change with AI (CCAI)_, 2019.
* [45] Lei Ma, Yu Liu, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, and Brian Alan Johnson. Deep learning in remote sensing applications: A meta-analysis and review. _ISPRS journal of photogrammetry and remote sensing_, 152:166-177, 2019.
* [46] Oscar Manas, Alexandre Lacoste, Xavier Giro-i Nieto, David Vazquez, and Pau Rodriguez. Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9414-9423, 2021.
* [47] M Maskey, H Alemohammad, KJ Murphy, and R Ramachandran. Advancing ai for earth science: A data systems perspective. _Eos_, 101, 2020.
* [48] Kevin Mayer, Benjamin Rausch, Marie-Louise Arlt, Gunther Gust, Zhecheng Wang, Dirk Neumann, and Ram Rajagopal. 3d-pv-locator: Large-scale detection of rooftop-mounted photovoltaic systems in 3d. _Applied Energy_, 310:118469, 2022.
* [49] Amy McGovern, Kimberly L. Elmore, David John Gagne, Sue Ellen Haupt, Christopher D. Karstens, Ryan Lagerquist, Travis Smith, and John K. Williams. Using artificial intelligence to improve real-time decision-making for high-impact weather. _Bulletin of the American Meteorological Society_, 98(10), 2017.
* [50] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. Never-ending learning. _Communications of the ACM_, 61(5):103-115, April 2018.
* [51] Cassandra Pallai and Kathryn Wesson. Chesapeake bay program partnership high-resolution land cover classification accuracy assessment methodology, 2017.
* [52] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. _arXiv preprint arXiv:2104.10350_, 2021.
* [53] Otavio AB Penatti, Keiller Nogueira, and Jefersson A Dos Santos. Do deep features generalize from everyday objects to remote sensing and aerial scenes domains? In _Proceedings of the IEEE conference on computer vision and pattern recognition workshops_, pages 44-51, 2015.
* 2020 IEEE International Geoscience and Remote Sensing Symposium_, pages 1030-1033, Waikoloa, HI, USA, September 2020. IEEE.
* [55] Alec Radford, Jong Wood Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. _arXiv preprint arXiv:2103.00020_, 2021.
* [56] Caleb Robinson, Le Hou, Kolya Malkin, Rachel Soobitsky, Jacob Czavlykko, Bistra Dilkina, and Nebojsa Jojic. Large scale high-resolution land cover mapping with multi-resolution data. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 12726-12735, 2019.
* [57] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. Tackling climate change with machine learning. _arXiv preprint arXiv:1906.05433_, 2019.
* [58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical image computing and computer-assisted intervention_, pages 234-241. Springer, 2015.

* [59] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing. 2021.
* [60] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. _Communications of the ACM_, 63(12):54-63, 2020.
* [61] Hao Sheng, Jeremy Irvin, Sasankh Mmukutla, Shawn Zhang, Christopher Cross, Kyle Story, Rose Rustowicz, Cooper Elsworth, Zutao Yang, Mark Omara, et al. Ognet: Towards a global oil and gas infrastructure database using deep learning on remotely sensed imagery. _arXiv preprint arXiv:2011.07227_, 2020.
* [62] Adam J Stewart, Caleb Robinson, Isaac A Corley, Anthony Ortiz, Juan M Lavista Ferres, and Arindam Banerjee. Torchgeo: deep learning with geospatial data. _arXiv preprint arXiv:2111.08872_, 2021.
* [63] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3645-3650, 2019.
* [64] Gencer Sumbul, Arne De Wall, Tristan Kreuziger, Filipe Marcelino, Hugo Costa, Pedro Benevides, Mario Caetano, Begim Demir, and Volker Markl. Bigearthnet-mm: A large-scale, multimodal, multilabel benchmark archive for remote sensing image classification and retrieval [software and data sets]. _IEEE Geoscience and Remote Sensing Magazine_, 9(3):174-180, 2021.
* [65] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? _Advances in Neural Information Processing Systems_, 33:6827-6839, 2020.
* [66] USGS. Landsat 8. Technical report, United States Geological Survey, Reston, Virginia, USA, 2021.
* [67] Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang, Marshall Burke, David Lobell, and Stefano Ermon. Learning to interpret satellite images using wikipedia. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, 2019.
* [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [69] Di Wang, Jing Zhang, Bo Du, Gui-Song Xia, and Dacheng Tao. An empirical study of remote sensing pretraining. _IEEE Transactions on Geoscience and Remote Sensing_, pages 1-1, 2022.
* [70] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad M Albrecht, and Xiao Xiang Zhu. Ssl4eo-s12: A large-scale multi-modal, multi-temporal dataset for self-supervised learning in earth observation.
* [71] Ben G Weinstein, Sarah J Graves, Sergio Marconi, Aditya Singh, Alina Zare, Dylan Stewart, Stephanie A Bohlman, and Ethan P White. A benchmark dataset for canopy crown detection and delineation in co-registered airborne rgb, lidar and hyperspectral imagery from the national ecological observation network. _PLoS computational biology_, 17(7):e1009180, 2021.
* [72] Zhitong Xiong, Fahong Zhang, Yi Wang, Yilei Shi, and Xiao Xiang Zhu. Earthqu: Empowering ai in earth observation. _arXiv preprint arXiv:2210.04936_, 2022.
* [73] Christopher Yeh, Chenlin Meng, Sherrie Wang, Anne Driscoll, Erik Rozi, Patrick Liu, Jihyeon Lee, Marshall Burke, David B Lobell, and Stefano Ermon. Sustainbench: Benchmarks for monitoring the sustainable development goals with machine learning. _arXiv preprint arXiv:2111.04724_, 2021.
* [74] Jin Z., Lin C., Weigl C., Obarowski J., and Hale D. Smallholder cashew plantations in benin, 2021.
* [75] Valentina Zantedeschi, Fabrizio Falasca, Alyson Douglas, Richard Strange, Matt J Kusner, and Duncan Watson-Parris. Cumulo: A dataset for learning cloud classes. _arXiv preprint arXiv:1911.04227_, 2019.
* [76] Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang, Lichao Mou, Hossein Bagheri, Matthias Haberle, Yuansheng Hua, Rong Huang, et al. So2sat lcz42: A benchmark dataset for global local climate zones classification. _arXiv preprint arXiv:1912.12171_, 2019.
* [77] Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and Friedrich Fraundorfer. Deep learning in remote sensing: A comprehensive review and list of resources. _IEEE Geoscience and Remote Sensing Magazine_, 5(4):8-36, 2017.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix C 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.6.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] They are creative common datasets 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] Let's discuss it here. There is no text data and remote sensing data is unlikely to contain PII. The only high-resolution dataset is NeonTree and it is collected over protected forests in the United States.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]