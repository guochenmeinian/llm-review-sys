# MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations

Yubo Ma\({}^{1}\), Yuhang Zang\({}^{2}\), Liangyu Chen\({}^{1}\), Meiqi Chen\({}^{3}\), Yizhu Jiao\({}^{4}\)

**Xinze Li\({}^{1}\), Xinyuan Lu\({}^{5}\), Ziyu Liu\({}^{6}\), Yan Ma\({}^{7}\), Xiaoyi Dong\({}^{2}\), Pan Zhang\({}^{2}\) Liangming Pan\({}^{8}\), Yu-Gang Jiang\({}^{9}\), Jiaqi Wang\({}^{2}\), Yixin Cao\({}^{9*}\), Aixin Sun\({}^{1}\) \({}^{1}\)** S-Lab, Nanyang Technological University, \({}^{2}\) Shanghai AI Laboratory, \({}^{3}\) Peking University \({}^{4}\) University of Illinois Urbana-Champaign, \({}^{5}\) National University of Singapore, \({}^{6}\) Wuhan University \({}^{7}\) Singapore Management University, \({}^{8}\) University of Arizona, \({}^{9}\) Fudan University

Corresponding Authors. Project Page: https://mayubo2333.github.io/MMLongBench-Doc

###### Abstract

Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents **MMLongBench-Doc**, a long-context, multi-modal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (_i.e.,_ page number). Moreover, 33.7% of the questions are _cross-page questions_ requiring evidence across multiple pages. 20.6% of the questions are designed to be _unanswerable_ for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9%, while the second-best, GPT-4V, scores 30.5%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.

## 1 Introduction

Documents are one of the fundamental forms of information preservation and exchange. In each year, tens of millions of documents are created, read, saved, and dispatched [1]. Beyond unstructured pure-text, documents feature both complicated layout structures and information across distinct modalities such as text, table, chart, image, _etc._ Accordingly, the automatic understanding of documents (Document Understanding; DU) stands as a long-standing task in urgent and practical needs.

Recently, a number of LVLMs, both closed-source ones (GPT-4o [2], Gemini-1.5 [3], Claude-3 [4], _etc._) and open-source ones (InternLM-XC2-4KHD [5], InternVL-Chat [6], Otter [7], LLaVA-NeXT [8], CogVLM [9], mPLUG-DocW1 1.5 [10], TextMonkey [11], _etc._) have been developed and presented the great potential to handle documents. Most of them have achieved promising performance on single-page DU datasets like DocVQA [12], ChartQA [13], InfoVQA [14], TAT-DQA [15], _etc._ However, considerable amounts of documents in the real world are long-contextdocuments with tens or even hundreds of pages. The understanding of these lengthy documents brings new challenges for LVLMs from at least two aspects: (1) **Localization**: identify and retrieve information from massive, heterogeneous information (similar to the _needle in a haystack_ task); (2) **Cross-page comprehension**: collect and reason over multi-source information across different pages. These two kinds of abilities are beyond the evaluation scopes of the aforementioned single-page DU datasets. Some recent DU datasets [16; 17; 18] feature multiple-page DU, but almost all their documents are either as short of only several pages or of low information density, making the localization-related questions over-simple. Additionally, few (if any) questions in these datasets necessitate cross-page comprehension. See more detailed related work in Section 2. In summary, there lacks a unified and high-quality benchmark on lengthy documents, leaving the evaluation of long-context DU largely unexplored.

In this paper, we present MMLongBench-Doc, a benchmark designed to evaluate the **M**ulti-**M**odality **Long**-context **D**oument understanding abilities of LVLMs. Towards a comprehensive benchmark, it incorporates lengthy documents from both four existing datasets [13; 17; 18; 19] and other various papers, brochures, _etc._ Consequently, our benchmark includes 135 PDF-formatted documents spanning across 7 diverse domains, with each document averaging 47.5 pages and 21,214.1 textual tokens. Regarding the questions, we employ ten expert-level annotators to (1) edit questions associated with documents from existing datasets to meet our benchmark's standard and (2) create new questions for all collected documents to expand the scale of the benchmark. Then a three-round, semi-automatic reviewing process ensures the benchmark's annotation quality. As a result, MMLongBench-Doc comprises 1,082 human-annotated questions, with 184 sourced from four existing datasets and 898 newly annotated. Being a multi-modal benchmark, the answer to each question requires evidence from one or more of these five in-document sources: _text_, _layout_, _chart_, _table_, and _image_. Questions are categorized into three types based on the number of evidence pages 1, with examples illustrated in Figure 1(a): (1) 494 _single-page_ questions (with one evidence page) mainly to evaluate localization abilities, (2) 365 _cross-page_ questions (with multiple evidence pages) to assess cross-page comprehension, and (3) 223 _wanswerable_ questions (no evidence for answering it, _i.e.,_ no evidence pages) to reduce shortcuts and measure LVLMs' potential hallucinations. Meta-information including evidence pages, sources, and answer formats, is preserved for fine-grained evaluation and analysis. Detailed descriptions of the annotation pipeline and statistics can be found in Section 3.

Footnote 1: Given a document \(D\) and a question \(q\) upon \(D\), We call page \(P\) (in document \(D\)) an _evidence page_ of \(q\) if the answer of \(q\) necessitates one or more pieces of evidence in page \(P\).

We conduct extensive experiments on MMLongBench-Doc to evaluate the long-context DU abilities of 14 LVLMs, including 4 proprietary and 10 open-source ones. Given a document, we screenshot each page and feed all of these PNG-formatted images to LVLMs in an end-to-end approach. For comparison, we also convert the documents to textual format by optical character recognition (OCR) and evaluate another 6 proprietary and 4 open-source 10 LLMs (6 proprietary and

Figure 1: MMLongBench-Doc evaluates understanding abilities of LVLMs on lengthy documents that span tens of pages and incorporate multi-modal elements. Experiments (bottom-right) indicate that most LVLMs struggle, even falling behind LLMs that are fed with only OCR-parsed documents.

4 open-source ones). The results in Figure 1(c) highlight the challenges that current LVLMs face with long-context DU. The best-performing LVLM, GPT-4o, achieves an overall F1 score of only 44.9%, while the second-best LVLM, GPT-4V, scores 30.5%. Moreover, all the remaining LVLMs tested with multi-modal documents performed worse than single-modal LLMs handling lossy, OCR-parsed texts. Specifically, the Gemini-1.5-Pro and Claude-3-Opus present 4.2% and 6.4% absolute decrease when the inputs change from document screenshots to OCR-parsed texts. Regarding open-source models, the best-performing LVLM lags behind the best-performing LLM by 11.7%. These results reveal that long-context DU is a far-from-resolved task for current LVLMs.

## 2 Related Work

**Benchmarks for Document Understanding.** A great amount of datasets have emerged to evaluate the DU capabilities of LVLMs. Many datasets focus exclusively on either a single component (_e.g.,_ table, chart) [13; 15; 21; 22] or a single page [12; 14] from the full documents. Some recent DU datasets [16; 17; 18; 23; 19] attempt to assess multi-page documents, but still exhibit shortcomings in terms of document length (page number), information density (token number) and the construction approaches. Specifically, MP-DocVQA [16] is an extension of DocVQA [12] and inherently absent of both cross-page and unanswerable questions. Annotating from scratch, DUDE [17] includes a small percentage of cross-page questions (2.1%) and unanswerable questions (12.7%). However, due to the relatively short context length (5.3 pages on average) and the use of crowd-sourced annotations, questions in DUDE tend to be less challenging and somewhat less rigorous. SlideVQA features 20-page documents and cross-page questions (12.9%). Nevertheless, the documents in SlideVQA are in slide-deck format and of relatively low information density. Moreover, these cross-page questions are HotpotQA-style [24] created by instantiating entity graphs and co-referencing in-graph entities across multiple pages. The entity graph from a closed document tends to be sparse and has significant shortcuts (see examples in Appendix A.4). These shortcuts sometimes lead to false cross-page questions that actually do not require answer evidence across different pages. The recent FinanceBench [19] features both extremely long-context documents and practical, scalable cross-page questions. However, its documents are exclusively financial reports. Additionally, the reference answers are in open-ended formats, making the expert-level manual evaluation indispensable. The above reasons limit the broader applicability of FinanceBench. To our best knowledge, MMLongBench-Doc is the first comprehensive, qualified, and easy-to-use benchmark on the long-context DU task. More detailed descriptions and comparisons are presented in Table 1.

**Models for Document Understanding.** There are two main branches of models for automatic DU tasks. The first approach employs two-stream, OCR-dependent architectures to separately encode textual information (parsed via OCR) and visual information (images and/or layout structures) [25; 26; 27]. In contrast, the second approach develops OCR-free models that understand documents

\begin{table}
\begin{tabular}{l|c c|c c c c c} \hline \hline \multirow{2}{*}{**Benchmarks**} & \multicolumn{3}{c|}{**Document**} & \multicolumn{3}{c}{**Question type**} & \multicolumn{3}{c}{**Answer Evidence**} \\  & **\#** & **Pages** & **\#** & **Tokens** & **Cross-page (\%)** & **Unans. (\%)** & **Doc. Rel.** & **Source** & **Avg. Position** \\ \hline DocVQA [12] & 1.0 & 151.5 & ✗ & ✗ & ✗ & \# & TXT/L/C/TAB/I & - \\ ChartQA [13]2 & 1.0 & 236.9 & ✗ & ✗ & ✓ & C & - \\ InfovQA [14]2 & 1.2 & 288.0 & ✗ & ✗ & ✗ & L/C/TAB/I & - \\ TXT-DQA [15] & 1.1 & 577.0 & ✗ & ✗ & ✗ & TXT/TAB & - \\ VisualWebBench [21]2 & 1.0 & 452.4 & ✗ & ✗ & ✓ & LAY/I & - \\ PWC [22] & -12* & -7000.0 & ✗ & ✗ & ✗ & TAB & - \\ MP-DocVQA [16] & 8.3 & 2026.6 & ✗ & ✗ & ✗ & TXT/L/C/TAB/I & 6.0 \\ DUDE [17] & 5.7 & 1831.5 & ✓(2.1\%) & ✓(12.7\%) & ✗ & TXT/L/C/TAB/I & 2.5 \\ SlideVQA [18] & 20.0 & 2030.5 & ✓(13.9\%) & ✗ & ✗ & TXT/L/C/TAB/I & 9.1 \\ \hline MMLongBench-Doc & 47.5 & 21214.1 & ✓(33.0\%) & ✓(22.5\%) & ✓ & TXT/L/C/TAB/I & 23.6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between our benchmark and previous DU datasets. **Unans.**: unanswerable question. **TXT/L/C/TAB/I**: pure text/generalized layout/chart/table/image. **Doc. Rel.**: document relevance. Whether document information is indispensable for the answer. **Avg. Position**: the average page index on which the answer evidence is located. *:**-Statistics from [20].

in an end-to-end manner [28; 29]. With the rapid advancement of LVLMs, the latter approach has dominated the current DU solutions. As mentioned above, a range of LVLMs demonstrate promising performance on single-page DU datasets. However, as shown in Section 4, even the most advanced LVLMs fall significantly short of achieving satisfactory performance on our benchmark. It reveals that understanding lengthy documents still poses great challenges to current LVLMs.

Long-context LVLMs and LLMs.Lengthy documents necessitate the use of LVLMs or LLMs with extended context sizes. Several benchmarks [30; 31; 32; 33] and solutions [34; 35; 36; 37] have been proposed to evaluate and develop long-context LLMs. However, there exists limited related work for long-context LVLMs, leaving this area largely unexplored. Until very recently, contemporary studies [38; 39; 40] assess and/or improve LVLMs' multi-image understanding capabilities. Evaluations on both MMLongBench-Doc and these works indicate that current LVLMs are still not fully equipped to handle long-context DU and many other practical tasks that require extensive contextual comprehension.

## 3 MMLongBench-Doc

We design a three-stage annotation pipeline for the construction of our benchmark. The three stages will be introduced in Section 3.1, Section 3.2, and Section 3.3, respectively. We also provide key statistics of our benchmark in Section 3.4.

### Document Collection

As a long-context DU benchmark, the documents shall be of diverse topics and lengthy enough. To this end, we crawl a great amount of documents from various sources. Then we select the lengthy ones from these documents. Specifically, we encompass a diverse array of documents from two approaches. (1) **Existing documents** from four previous datasets: DUDE [17], SlideVQA [18], ChartQA [13], and FinanceBench [19]. (2) **Newly-collected documents** from Arxiv 3, ManualsLib 4 and Google Search 5. Then we (1) filter out the documents with fewer than 15 pages or license restrictions and (2) down-sample documents from DUDE, SlideVQA, and FinanceBench for a more balanced distribution. Detailed descriptions of our selection and processing procedure can be found in Appendix A.1 and Appendix A.2.

Footnote 3: https://arxiv.org

Footnote 4: https://www.manualslib.com

Footnote 5: https://www.google.com.sg

In summary, we collect a total of 135 documents. Among them, 76 documents are from existing datasets and incorporate previously annotated questions (represented as triangles). The remaining 59 documents are newly collected and incorporate no existing questions. We manually categorize them into 7 types: _Research Report_, _Financial Report_, _Academic Paper_, _Brochure_, _Guideline_, _Administration & Industry File_, _Tutorial / Workshop_. We showcase some instances of these documents in Appendix A.3.

### Question and Answer Collection

To serve as a high-quality and comprehensive benchmark, the question annotation of our benchmark adheres to the following standards: (1) All questions shall be neither over-easy nor over-difficult. (2) Questions are not repetitively derived from the same page or the same pattern. (3) The distribution of evidence numbers, evidence sources, and evidence locations for the questions shall be balanced. (4) No questions shall be answered correctly without accessing the relevant documents.

Ten authors serve as expert-level annotators for the question-and-answer collection. All of them are doctors or Ph.D. students proficient in English reading and writing. Before formal annotation, they undergo a training session and pre-annotate three documents for practice. We iteratively review their annotation results and provide personalized feedback until their annotations meet the standards mentioned above. Regarding the formal annotation, we divide 135 documents into 54 batches (each having 2-4 documents) and dispatch these batches to annotators. We then ask the annotators to submit their results in units of batches and set reasonable time intervals for each batch's submission. Wetimely evaluate their annotations after each submission and remind the annotators if their questions in this turn diverge from the standards. It avoids the annotators rushing all assignments in a short time and benefits the annotation quality. We recommend the annotators take 60-90 minutes on each document. Specifically, the annotators shall rapidly read through the whole document in the first 15-30 minutes. For the remaining time, they shall dive deep into specific components to modify existing annotations and/or add new annotations as detailed below.

**Modify Existing Questions.** Documents collected from existing datasets had been annotated with some questions and answers from previous work. However, their crowd-sourcing annotations inevitably make some questions, answers, and other meta information unqualified. Therefore, we edit their annotations before including them as a component of our benchmark.

Specifically, we classify six potential problems in original annotations: _Wrong Answers or Evidence Pages_, _Repetitive Question_, _Ambiguous Question_, _Decontexualization-required Question_, _Low Document-relevant Question_ and _Potential Shortcut_. See detailed explanations and examples about these problems in Appendix A.4. Given an existing document, the annotators are tasked to evaluate each existing question's quality according to whether they have one or more above problems and assign a label from {Retain,Revise,Remove} for each question. Then the annotators would revise the Revise questions to meet our quality criteria and remove the Remove questions. Among all 425 original questions from 76 existing documents, 32.2% of them are revised and 46.1% are removed. We finally collect 211 questions in this procedure. The corresponding GUI is shown in Appendix A.7.

**Add New Questions.** We newly annotate questions on both existing and newly collected documents to expand the questions in our benchmark. Specifically, we ask annotators to add about 3 questions on existing documents, and 6 questions on newly-collected documents. Given most existing questions (even after editing) are single-page ones and sourced from texts, we put more focus on (1) cross-page and unanswerable questions and (2) questions sourced from tables, charts, and images for newly added questions to balance the distribution. We detail the quantitative requirements in Appendix A.5. Associated with questions, annotators also provide reference answers and meta-information (_i.e._, evidence sources, answer format, evidence locations) for all samples. We finalized a collection of 965 samples in this procedure. The corresponding GUI is shown in Appendix A.7.

### Quality Control

Combining the merits of humans and LVLMs, we adopt a three-round, semi-automatic quality control procedure to improve the annotation quality of our benchmark. We detail each round in the following components and leave the discussion of potential bias in Appendix A.6.

**Document-relevant Detection.** Our benchmark is designed to evaluate LVLMs' long-context document understanding abilities. All questions are expected to be unanswerable without access to corresponding documents. To remove low document-relevant questions (_i.e._, questions not relying on documents), we feed each annotated question **WITHM** documents to GPT-4o. A question will be identified as _low document-relevant_ question if GPT-4o correctly predicts under this case. Ultimately, 94 samples are identified as low document-relevant questions and removed in this round.

**Self-reflection.** We draw inspirations from MMBench [41] and leverage LVLMs to reduce the wrongly-annotated samples. Specifically, we feed the remaining questions from the last round **WITH** their documents to GPT-4o. Samples whose model predictions are inconsistent with the reference answers are sent back to corresponding annotators. The annotators are asked to check each question and identify whether the inconsistency is caused by _problematic annotation_ or not. As a result, 13.8% of the samples are identified as problematic annotations. The annotators revise them accordingly.

**Cross-checking.** In parallel, annotators cross-check the annotated samples from other annotators and determine the inconsistency reasons the same as described above. We calculate Cohen's kappa value of their identifications as 0.42 (17.5% inconsistent samples), showing a moderate agreement. Regarding the 17.5% inconsistent samples, two primary authors serve as meta-annotators and make final decisions on them (and if necessary, revise accordingly).

### Dataset Overview and Analysis

The main statistics of MMLongBench-Doc are presented in Table 2. Overall, our benchmark consists of 1,082 questions. These questions are constructed upon 135 lengthy documents across 7document types, with an average of 47.5 pages and 21,214.1 tokens. Please see detailed distributions of these documents in Figure 2. Regarding the questions, there are 494 single-page questions (1 evidence page), 365 cross-page questions (2+ evidence pages), and 223 unanswerable questions (no evidence page). These three types of questions evaluate the LVLMs's long-context DU capabilities from complementary aspects: the localization ability, the cross-page comprehension ability, and the hallucination severity, respectively. For single-page and cross-page questions, their answer evidence is scattered among different context sources (_i.e.,_ text, layout, table, chart, image) and evenly distributed across different locations of the documents (see Table 2, Figure 3 Left and Middle). Also notably, 28.6% of cross-page questions have more than two evidence pages, which further enhances the challenge of our benchmark.

## 4 Evaluation

### Evaluation Protocol

We follow MATHVISTA [56] to conduct a three-step evaluation protocol: _response generation_, _answer extraction_, and _score calculation_. We adopt such a protocol out of three considerations: (1) Current LVLMs are instructed to generate long responses, rather than short-form answers, in conventional settings. (2) The evaluation of long responses, however, remains an open and challenging problem. (3) We focus on the document understanding (not instruction following) abilities of LVLMs.

Specifically, we impose no limitations on _response generation_ stage to encourage LVLMs to answer the questions in a freestyle. Then we propose a unified LLM-based _answer extractor_ (GPT-4o under our setting) to convert their long responses to short-form answers. Finally, we use a rule-based _score calculator_ to evaluate the converted short answers. We report both generalized accuracy and generalized F1 score to balance the answerable (positive) and unanswerable (negative) questions. The used prompt, the high correlation between our automatic _answer extractor_ and human evaluation, and the detailed rules of our _score calculation_ are described in Appendix B.

### Experimental Setup

We evaluate 14 LVLMs on MMLongBench-Doc, including 4 proprietary LVLMs and 10 open-source LVLMs. To purely evaluate LVLMs' long-context DU abilities, we screenshot each page of the PDF-formatted document with 144 DPI and feed all these PNG-formatted images to LVLMs in an end-to-end approach. Notably, all evaluated open-source LVLMs do not support multi-image inputs or present significant performance drops when fed with excessive images (_e.g.,_ more than 10 or 20 images). Therefore, we employ a concatenation strategy that combines all screenshot pages into 1 or 5 images and feeds these concatenated images to open-source LVLMs. Regarding proprietary LVLMs, we adopt the same concatenation strategy and reduce the image number to 20 for Claude-3-Opus to fit its maximum image threshold. For GPT-4o, GPT-4V, and Gemini-1.5-Pro, we directly send all original screenshots to them (_i.e.,_ the image number equals the page number).

For comparison, we also use the Tesseract [42] OCR model to recognize and extract texts from the documents and feed the parsed documents to 10 LLMs, including 6 proprietary and 4 open-source

\begin{table}
\begin{tabular}{l c c|c c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**\#Param**} & \multicolumn{2}{c|}{**Context**} & \multicolumn{4}{c|}{**Evidence Source**} & \multicolumn{4}{c|}{**Evidence Page**} \\  & & \multicolumn{1}{c|}{**Window**} & \multicolumn{1}{c|}{TXT} & \multicolumn{1}{c}{LAY} & \multicolumn{1}{c}{CHA} & \multicolumn{1}{c}{TAB} & \multicolumn{1}{c|}{FIG} & \multicolumn{1}{c|}{SIN} & \multicolumn{1}{c|}{MUL} & \multicolumn{1}{c}{UNA} & \multicolumn{1}{c}{**ACC**} & \multicolumn{1}{c}{**F1**} \\ \hline \multicolumn{10}{c}{_OCP-source Models_} \\ \multicolumn{10}{c}{Chatt-M218k [37]} & 6B & 128k & 23.4 & 12.7 & 9.7 & 10.2 & 12.2 & 18.8 & 11.5 & 18.1 & 16.3 & 14.9 \\ Mistral-Instructv-0.2 [43] & 7B & 32k & 19.9 & 13.4 & 10.2 & 10.1 & 11.0 & 16.9 & 11.3 & 24.1 & 16.4 & 13.8 \\ Mistral-Instructv-0.1 [44] & 8x7B & 32k & 24.2 & 14.8 & 12.5 & 15.0 & 13.7 & 21.3 & 14.1 & 13.1 & 17.0 & 16.9 \\ Mistral-Instructv-0.1 [44] & 8x22B & 64k & 34.2 & 21.3 & 19.5 & 21.3 & 19.2 & 27.7 & 21.9 & 32.4 & 26.9 & 24.7 \\ _Propricency Models_ & & & & & & & & & & & & & \\ QWen-Plus [45] & - & 32k & 17.4 & 15.6 & 7.4 & 7.9 & 8.8 & 14.2 & 10.6 & 42.2 & 18.9 & 13.4 \\ DeepSeeX-V2 [46] & - & 32k & 27.8 & 19.6 & 8.8 & 17.0 & 9.4 & 20.2 & 15.4 & 48.1 & 24.9 & 19.6 \\ Claude-3 Qpus [4] & - & 32k & 30.8 & 30.1 & 16.4 & 24.4 & 16.3 & 32.0 & 18.6 & 30.9 & 26.9 & 24.5 \\ Gemini-1.5-Pro [3] & - & 32k & 29.3 & 15.9 & 12.5 & 17.7 & 11.5 & 21.2 & 16.4 & **73.4** & 31.2 & 24.8 \\ GPT-4 turbo [47] & - & 128k & 36.5 & 21.0 & 20.7 & 24.3 & 17.3 & 28.7 & 23.8 & 31.2 & 27.6 & 25.9 \\ GPT-4o [2] & - & 128k & 41.1 & 23.4 & 28.5 & 38.1 & 22.4 & 35.4 & 29.3 & 18.6 & 30.1 & 30.5 \\ \hline \multicolumn{10}{c}{_Large Visual Language Models (LVLMs)_} \\ \hline \multicolumn{10}{c}{_Open-source, 7-4B Models_} \\ DeepSeeX-V1-Chatt [48] & 7.3B & 4k & 7.2 & 6.5 & 1.6 & 5.2 & 7.6 & 5.2 & 7.0 & 12.8 & 7.4 & 5.4 \\ IdeficSz [49] & 8B & 8k & 9.0 & 10.6 & 4.8 & 4.1 & 8.7 & 7.7 & 7.2 & 5.0 & 7.0 & 6.8 \\ Mintori-Ilama-V2.5 [50, 51] & 8B & 2k & 11.9 & 10.8 & 5.1 & 5.9 & 12.2 & 5.9 & 9.5 & 4.5 & 8.5 & 8.6 \\ InterMLM-X2-4KHD [5] & 8B & 16k & 9.9 & 14.3 & 7.7 & 6.3 & 13.0 & 12.6 & 7.6 & 9.6 & 10.3 & 9.8 \\ mPLUG-DecOwl 1.5 [52] & 8.1B & 4k & 8.2 & 8.4 & 2.0 & 3.4 & 9.9 & 7.4 & 6.4 & 6.2 & 6.9 & 6.3 \\ Qwen-VL-Chatt [53] & 9.6B & 6k & 5.5 & 9.0 & 5.4 & 2.2 & 6.9 & 5.2 & 7.1 & 6.2 & 6.1 & 5.4 \\ Monkey-Chatt [54] & 9.8B & 2k & 6.8 & 7.2 & 3.6 & 6.7 & 9.4 & 6.6 & 6.2 & 6.2 & 6.2 & 5.6 \\ _Oepensource, 5-4B Models_ & & & & & & & & & & & & & \\ CoqVLM2-LMAMA-Chat [9] & 19B & 8k & 3.7 & 2.7 & 6.0 & 3.2 & 6.9 & 3.9 & 5.3 & 3.7 & 4.4 & 4.0 \\ InternVL-Chatt-V1.5 [6] & 26B & 4k & 14.0 & 16.2 & 7.1 & 10.1 & 16.6 & 14.9 & 12.2 & 17.5 & 14.6 & 13.0 \\ EMU2-Chatt [55] & 37B & 2k & 6.1 & 9.7 & 2.6 & 3.8 & 7.7 & 5.7 & 6.1 & 16.5 & 8.3 & 5.5 \\ _Propricency Models_ & & & & & & & & & & & & & \\ Claude-3 Qpus [4] & - & 200k & 24.9 & 24.7 & 14.8 & 13.0 & 17.1 & 25.6 & 13.8 & 7.6 & 17.4 & 18.1 \\ Gemini-1.5-Pro [3] & - & 128k & 21.0 & 17.6 & 6.9 & 14.5 & 15.2 & 21.1 & 11.1 & 69.2 & 28.2 & 20.6 \\ GPT-4V(sion) [47] & - & 128k & 34.4 & 28.3 & 28.2 & 32.4 & 26.8 & 36.4 & 27.0 & 31.2 & 32.4 & 31.2 \\ GPT-4o [2] & - & 128k & **46.3** & **46.0** & **45.3** & **50.0** & **44.1** & **5.45** & **41.5** & 20.2 & **42.8** & **44.9** \\ \hline \multicolumn{10}{c}{_Human Baseline_} \\ \hline Human Experts & - & - & - & - & - & - & - & - & - & - & 65.8 & 66.0 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Evaluation of various models on MMLongBench-Doc.** We report the generalized accuracy of five types of evidence sources including pure text (TXT), layout (LAY), chart (CHA), table (TAB), and image (IMG). We also present the generalized accuracy of questions categorized by the number of evidence pages: single-page (SIN), cross-page (MUL), and unanswerable (UNA) questions. The \(|\)**best** and \(|\)**second-best** performance in each section are highlighted.

ones. Texts exceeding their context lengths are truncated. Notably, as a key component of the classical solution for the DU task, the OCR model can handle most flattened texts and some structured tables in the document. However, it cannot perceive the information from the charts or images. Thus the TXT-formatted, OCR-parsed documents are lossy documents in which the information is not fully preserved. More detailed hyperparameters are introduced in Appendix B.5. Additionally, we also conduct manual evaluation on a subset of our datasets (238 questions from 29 documents) to indicate the difficulty of this task for humans.

### Main Results

We compare the performance of different LVLMs and LLMs in Table 3, reporting their generalized accuracy and F1 scores (shown in the last two columns). Regarding LVLMs, we draw several conclusions as below: (1) The performance demonstrates that long-context DU is still a challenging and unsolved task for current LVLMs. The best-performing LVLM, GPT-4o, merely achieves a 44.9% F1 score. The second best-performing LVLM, GPT-4V, lags behind by over 10% percent and presents a 31.4% F1 score. All other LVLMs only achieve about 20% or even lower F1 scores. (2) Though far from satisfactory, GPT-4o performs much better than all other models (including GPT-4V). Thus we speculate that the multi-modal pre-training paradigm significantly benefits LVLMs' cross-modality understanding capabilities. (3) Proprietary LVLMs perform better than open-source LVLMs by a large margin. We attribute it to the difference of acceptable image numbers: open-source LVLMs only support single-image or several-image inputs, while proprietary LVLMs can be fed with at least 20 images or even more. Given that lengthy documents have tens of even hundreds of pages, it is impractical for open-source LVLMs to accurately perceive the information in the documents from the excessively concatenated images. (4) The performances of different models are highly correlated with their acceptable image numbers and maximum image resolutions. Notably, open-source LVLMs that support high-resolution images (_i.e.,_ InternLM-XC2-4KHD and InternVL-Chat-v1.5) exhibit superior performance compared to those with lower resolution limits.

Surprisingly, LVLMs even demonstrate overall worse performance than LLMs, even LLMs are fed with lossy OCR-parsed documents. Specifically, Gemini-1.5-Pro and Claude-3 Opus have 4.2% and 6.4% absolute F1-score degradations on vision versions. And the best-performing LLM (Mixtral) also surpasses the best-performing LVLM (InternVL-v1.5) by 11.7%. The above results clearly reveal that most current LVLMs are still not proficient in cross-modality, long-context document understandings. It is promising that GPT-4o and GPT-4-turbo achieve better performance when seeing multi-modality PDF documents than parsed text by 14.4% and 5.3% F1-score, respectively. Their performances validate the feasibility, benefit, and necessity of understanding documents in an end-to-end, cross-modality approach. We speculate that the scarce related pre-training corpus (_i.e.,_ extremely multi-image or lengthy documents) hinders the long-context DU capabilities of other LVLMs. We will leave related explorations for future work.

Regarding the human evaluation, we observe 66.0% F1-score from our annotators and a significant performance gap (exceeding 20% in absolute) between the current LVLMs and humans. This gap highlights the challenges of document understanding for LVLMs and the necessity of our benchmark.

### Fine-grained Results.

**Document Type.** As illustrated in Figure 4, LVLMs and LLMs exhibit distinct performance patterns across various document types. Our findings include: (1) All evaluated models demonstrate decent performance on industrial documents, which tend to have more standardized formats and less non-textual information. (2) The GPT series and Mixtral (_i.e.,_ the SoTA open-source LLM) show relatively balanced performance across different document types. In contrast, other models perform significantly worse in specialized domains such as academic papers and financial reports. (3) When equipped with OCR, LLM-based models like GPT-4 and Mixtral achieve comparable or even superior performance on industrial documents, academic papers, and brochures. Conversely, end-to-end LVLMs outperform OCR+LLMs in areas such as tutorials, research reports, and guidelines. We speculate that comprehending these latter document types requires more extensive multi-modal information, from which LVLMs significantly benefit.

**Evidence Source.** We categorize questions based on their evidence sources and present fine-grained results in Figure 4 and Table 3. Our observations reveal that only GPT-4o exhibits relatively balanced performance across the different sources. Other LVLMs, however, show inferior performance on questions related to charts and/or images compared to those related to text and/or layout. Additionally, LLMs generally demonstrate better or comparable performance to LVLMs on text- and table-related questions but show worse performance on questions involving other elements. This highlights the limitations of OCR (and other PDF parsers) when dealing with charts and images, as well as the gap in OCR capabilities between LVLMs and pure-text LLMs.

**Evidence Position.** We also examine how the evidence locations (_i.e.,_ the page indexes where the answer evidence is found) affect model performance. The results shown in Figure 5 reinforce that MMLongBenchDoc poses significant challenges for current models, at least partially due to the extended length of the documents. Almost all models (except InternVL-v1.5) exhibit their best performance on questions derived from the initial pages, while their performance declines progressively as the page index increases. Interestingly, two proprietary models, Gemini-Pro-1.5 and Claude-3-Opus, experience particularly sharp declines in performance.

**Number of Evidence Page.** We observe a consistent trend that all models achieve higher scores on single-page questions than cross-page questions. It reveals that gathering and reasoning over all necessary information across different pages is not trivial for current LVLMs and LLMs. More interestingly, evaluated LVLMs behave differently on unanswerable questions. GPT-4o and Claude-3 Opus adopt more aggressive strategies and usually tend to provide some answers. It makes their answers more likely helpful, but also increases the risk of hallucination and unfaithfulness (see their scores on unanswerable questions are much lower than answerable questions). On the contrary, Gemini-1.5-Pro, DeepSeek-VL-Chat, and EMU2-Chat are much more cautious and tend to refuse to answer questions about which they are uncertain. It makes their answers safer but less helpful (with large amounts of responses like _I don't know_).

## 5 Analysis & Discussion

### Oracle Setting

We conduct additional experiments to explore to what extent the challenges of MMLongBenchDoc are caused by the long-context lengths of documents. Specifically, we feed 820 answerable questions along with their oracle evidence pages (instead of the whole documents) to three representative LVLMs and show results in Figure 6. On one hand, it indicates that long-context length is a

Figure 4: Fine-grained results on various document types and evidence sources.

Figure 5: Relationships between evidence positions and model performances.

significantly challenging factor for document understanding. Compared with the oracle-page setting, lengthy documents lead to more than 20% absolute performance degradation on Gemini-1.5-Pro and InternLM-XC2-4KHD. Regarding the single-page questions, the performance difference even achieves up to 30%. On the other hand, the overall performance achieves only about 40% and 30% for Gemini-1.5-Pro and InternLM-XC2-4KHD even under oracle-page setting. And the improvement for GPT-4o is much less (about 10%). It demonstrates that the development of long-context LVLMs can largely facilitate, though still can not fully solve, the long-context DU task.

### Error Analysis

We further conduct error analysis to understand the bottleneck of current LVLMs in a qualitative approach. Specifically, we randomly select 72 error predictions from GPT-4o's responses and manually check their error reasons. These errors are categorized into seven types: _Perceptual Error_, _Irrelevant Answer_, _Incomplete Evidence_, _Hallucinated Evidence_, _Extractor Error_, _Reasoning Error_ and _Knowledge Lacking_. The distribution of these errors is illustrated in Figure 7. It indicates that most errors come from the model's hallucination (_i.e.,_ wrong explanations and answers to unanswerable questions) and perceptual errors (mainly in visual contexts). Additionally, GPT-4o sometimes misunderstands the intent of questions and provides irrelevant responses. The errors caused by collecting incomplete evidence (for cross-page questions) are also unignorable. The descriptions and examples of these error types are detailed in Appendix C.1.

## 6 Conclusion

In this work, we present MMLongBench-Doc to evaluate the long-context DU capabilities of LVLMs. Extensive experiments on 14 LVLMs (and 10 LLMs for comparison) reveal that the understanding of lengthy documents poses great challenges to current LVLMs. Even though the performance of GPT-4o proves the benefit of end-to-end, multi-modality perception for DU tasks, most LVLMs struggle on long visual contexts (_i.e.,_ extremely multiple images) and show inferior performance compared to OCR+LLM pipelines. We hope that the construction of our benchmark could push forward the development of more powerful LVLMs on lengthy document understanding.

## Acknowledgements

This study is supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). This work is also supported by Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201).

Figure 6: Performance comparisons between normal setting (feeding models with the whole documents) and oracle setting (feeding models only with the evidence pages) among three LVLMs.

Figure 7: Error distribution

## References

* [1] Lutz Bornmann and Rudiger Mutz. Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. _Journal of the Association for Information Science and Technology_, 66, 2014.
* [2] Open AI. Hello gpt-4o, 2024.
* [3] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.
* [4] Anthropic. Introducing the next generation of claude, 2024.
* [5] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-Xcomposer2-4KHD: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. _ArXiv preprint_, abs/2404.06512, 2024.
* [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. _ArXiv preprint_, abs/2404.16821, 2024.
* [7] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning, 2023.
* [8] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. LLAVA-NeXT: Stronger l lms supercharge multimodal capabilities in the wild, 2024.
* [9] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. CogVLM: Visual expert for pretrained language models, 2023.
* [10] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, 2024.
* [11] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document, 2024.
* [12] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: A dataset for vqa on document images. _2021 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 2199-2208, 2020.
* [13] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 2263-2279, Dublin, Ireland, 2022. Association for Computational Linguistics.
* [14] Minesh Mathew, Viraj Bagal, Ruben Perez Tito, Dimosthenis Karatzas, Ernest Valveny, and C.V. Jawahar. Infographicvqa. _2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 2582-2591, 2021.
* [15] Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 4857-4866, 2022.
* [16] Ruben Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multi-page docvqa, 2023.
* [17] Jordy Van Landeghem, Ruben Perez Tito, Lukasz Borchmann, Michal Pietruszka, Pawel J'oziak, Rafal Powalski, Dawid Jurkiewicz, Mickael Coustaty, Bertrand Ackaert, Ernest Valveny, Matthew B. Blasenko, Sien Moens, and Tomasz Stanislawek. Document understanding dataset and evaluation (DUDE). In _ICCV_, 2023.
* [18] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. SlideVQA: A dataset for document visual question answering on multiple images. In _AAAI_, 2023.
* [19] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. FinanceBench: A new benchmark for financial question answering, 2023.

* [20] Lukasz Borchmann, Michal Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Michal Turski, Karolina Szyndler, and Filip Gralinski. Due: End-to-end document understanding benchmark. In _NeurIPS Datasets and Benchmarks_, 2021.
* [21] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.
* [22] Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, and Robert Stojnic. AxCell: Automatic extraction of results from machine learning papers. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 8580-8594, Online, 2020. Association for Computational Linguistics.
* [23] Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, David Seunghyun Yoon, Ryan A. Rossi, and Franck Dernoncourt. Pdftriage: Question answering over long, structured documents, 2023.
* [24] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2369-2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
* [25] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, _KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020_, pages 1192-1200. ACM, 2020.
* [26] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 2579-2591, Online, 2021. Association for Computational Linguistics.
* [27] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In _Proceedings of the 30th ACM International Conference on Multimedia_, MM '22, page 4083-4091, New York, NY, USA, 2022. Association for Computing Machinery.
* [28] Gewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Orc-free document understanding transformer. In _European Conference on Computer Vision (ECCV)_, 2022.
* [29] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: screenshot parsing as pretraining for visual language understanding. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* [30] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 12007-12021, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.
* [31] Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023.
* [32] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual, multitask benchmark for long context understanding. _ArXiv preprint_, abs/2308.14508, 2023.
* [33] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. oobench: Extending long context evaluation beyond 100k tokens, 2024.
* [34] Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milo's. Focused transformer: Contrastive training for context scaling. _ArXiv preprint_, abs/2307.03170, 2023.
* [35] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. _ArXiv preprint_, abs/2309.12307, 2023.

* [36] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. _ArXiv preprint_, abs/2309.00071, 2023.
* [37] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign: A recipe for long context alignment of large language models. _ArXiv preprint_, abs/2401.18058, 2024.
* [38] Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. _ArXiv preprint_, abs/2404.18532, 2024.
* [39] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning, 2024.
* [40] Yujie Lu, Xiujun Li, Tsu-Jui Fu, Miguel Eckstein, and William Yang Wang. From text to pixel: Advancing long-context understanding in mllms, 2024.
* [41] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMbench: Is your multi-modal model an all-around player? _ArXiv preprint_, abs/2307.06281, 2023.
* [42] Ray Smith. An overview of the tesseract ocr engine. In _ICDAR_, 2007.
* [43] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023.
* [44] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral of experts, 2024.
* [45] Qwen Team. Introducing qwen1.5, 2024.
* [46] DeepSeek-AI. DeepSeek-V2: A strong, economical, and efficient mixture-of-experts language model, 2024.
* [47] OpenAI. GPT-4 technical report, 2024.
* [48] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. DeepSeek-VL: towards real-world vision-language understanding. _ArXiv preprint_, abs/2403.05525, 2024.
* [49] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024.
* [50] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwan He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. RLAIF-V: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. _ArXiv preprint_, abs/2405.17220, 2024.
* [51] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. LLAVA-UHD: an lmm perceiving any aspect ratio and high-resolution images. _ArXiv preprint_, abs/2403.11703, 2024.
* [52] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mPLUG-DocOwl 1.5: Unified structure learning for ocr-free document understanding. _ArXiv preprint_, abs/2403.12895, 2024.
* [53] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. _ArXiv preprint_, abs/2308.12966, 2023.
* [54] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _ArXiv preprint_, abs/2311.06607, 2023.
* [55] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yuez Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. _ArXiv preprint_, abs/2312.13286, 2023.

* [56] Pan Lu, Hrutik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In _International Conference on Learning Representations (ICLR)_, 2024.
* [57] Tomasz Stanislawek, Filip Grali'nski, Anna Wr'oblewska, Dawid Lipi'nski, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and P. Biecek. Kleister: Key information extraction datasets involving long documents with complex layouts. In _IEEE International Conference on Document Analysis and Recognition_, 2021.
* [58] S. Svetlichnasa. DeepForm: Understand structured documents at scale., 2020.
* [59] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset for form understanding in noisy scanned documents. _2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)_, 2:1-6, 2019.
* [60] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. _2019 International Conference on Document Analysis and Recognition (ICDAR)_, pages 1516-1520, 2019.
* [61] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5376-5384, 2017.
* [62] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In _The IEEE Winter Conference on Applications of Computer Vision (WACV)_, March 2020.
* [63] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. _ArXiv_, abs/2101.11272, 2021.
* [64] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. WebSRC: A dataset for web-based structural reading comprehension. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 4173-4185, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

Benchmark Construction Details

### Existing Document Collection

Although previous datasets contain a relatively small proportion of lengthy documents, their absolute quantity should not be disregarded. Therefore, we compile lengthy documents from various datasets to include them as part of the documents in this benchmark. Specifically, we review and consider 21 previous document understanding (DU) datasets, and ultimately select 4 of them for further document selection. The selection reasons are shown in Table 4. All of these four datasets are licensed under the Creative Commons license (CC-BY) or other open-source licenses. Regarding the 4 selected datasets: DUDE [17], SlideVQA [18], ChartQA [13] and FinanceBench [19], we collect a total of 76 documents and detail our collection procedures as below.

**DUDE:** We first filter all documents over 15 pages in the validation set of the original dataset, resulting in 87 documents. From these, we randomly sample 23 to include as a component of our benchmark documents.

**SlideVQA**: We download slide decks in the test set by following the instructions in the original repository 6. Pursuing lengthy documents, we slightly modified the code to remove the 20-page truncation procedure. Then we randomly select 27 slide decks for our benchmark documents.

Footnote 6: https://github.com/nttmdlab-nlp/SlideVQA

**FinanceBench**: We randomly sample 5 financial reports from the test set.

**ChartQA**: Different from the above three datasets, ChartQA only contains chart screenshots cropped from documents. We take the following steps to recover these original documents: (1) We use the Tesseract OCR model [42] to recognize the text within the charts. (2) We use these texts as keywords to search for related documents on Google Search. (3) We manually identify these documents and remove all those that are less than 15 pages. From the ChartQA test set, we finalize a collection of 53 research reports from the Pew Research Center. We randomly sample 18 of these documents to include as a component of our benchmark documents.

\begin{table}
\begin{tabular}{l|c|c}
**Dataset** & **Selected** & **Comment** \\ \hline DUDE [17] & ✓ & - \\ SlideVQA [18] & ✓ & - \\ ChartQA [13] & ✓ & - \\ FinanceBench [19] & ✓ & - \\ \hline DocVQA [12] & ✗ & Repetitive with some documents/questions in DUDE; Single-page documents only \\ MP-DocVQA [16] & ✗ & Repetitive with some documents/questions in DUDE; Single-page questions only \\ Kleister Charity [57] & ✗ & Repetitive with some documents/questions in DUDE; Over-simple \\ Kleister NDA [57] & ✗ & Repetitive with some documents/questions in DUDE; Over-simple \\ DeepForm [58] & ✗ & Repetitive with some documents/questions in DUDE; Over-simple \\ FUNSD [59] & ✗ & Repetitive with some documents/questions in DUDE; Over-simple \\ SROIE [60] & ✗ & Repetitive with some documents/questions in DUDE; Over-simple \\ Infographics VQA [14] & ✗ & Infographics are not long-context documents \\ TAT-QA [15] & ✗ & Repetitive with some documents/questions in FinanceBench \\ PWC [22] & ✗ & Repetitive with our self-annotated questions from academic papers \\ PaperQA [56] & ✗ & Repetitive with our self-annotated questions from academic papers \\ TextbookQA [61] & ✗ & Low document-relevance; Over-simple \\ PlotQA [62] & ✗ & Repetitive with our self-annotated questions from academic papers and research reports \\ VisualMRC [63] & ✗ & Human performance reached; Website screenshots are not long-context documents \\ WebSRC [64] & ✗ & Human performance reached; Website screenshots are not long-context documents \\ VisualWebBench [21] & ✗ & Human performance reached; Website screenshots are not long-context documents \\ PDFTriage [23] & ✗ & Not publicly available \\ \hline \end{tabular}
\end{table}
Table 4: Comparison of selected and considered datasets for our benchmark.

[MISSING_PAGE_FAIL:16]

Figure 11: Document example about **Financial Report** (only show first 50 pages)

Figure 10: Document example about **Research Report**

Figure 14: Document example about **Brochure**

Figure 12: Document example about **Academic Paper**

Figure 13: Document example about **Guidebook**

### Existing Question Editing

Documents collected from existing datasets had been annotated with some questions and answers. However, their crowd-sourcing annotations inevitably make some questions, answers, and other meta information unqualified. So we conduct a systematic and manual pipeline to edit their annotations. Specifically, we classify six potential problems in original annotations. The definitions and examples of these problems are shown below.

**1. Wrong Answer or Evidence Pages:** The reference answers and/or evidence pages in original datasets are wrongly annotated.

Figure 15: Example of the original annotation with _Wrong Answer or Evidence Pages_.

## 2. Repetitive Question:

Too many questions with the same types (_e.g.,_ key information extraction) occur in a single document (or even on the same page or point).

**Error type:**

Repetitive Question

The above two questions are created repeatedly upon a single point of the document. We call them repetitive questions and drop any one of them.

Figure 16: Example of the original annotation with _Repetitive Question_.

## 3 Ambiguous Question:

The question is ambiguous at the document level (_e.g.,_ the absence of entity, period, exact section or page, _etc._), or too broad to exactly answer.

### Error type:

Ambiguous Question

**Comment:**

The are two main entities occurred in this report: (1) The Limes Residential Home, and (2) Care Quality Commission. There is neither explicit statement nor the implicit coreference about any entity in this question, making it ambiguous.

### Revised Question:

What is the telephone no for The Limes Residential Home?

**Revised Answer:**

01983 873655

Figure 17: Example of the original annotation with _Ambiguous Question_.

## 4 Potential Shortcut:

The resolution of the question does not rely on two entities (across different pages) but only one of them, _i.e.,_ there exists a shortcut for this question.

**Original Question:**

Why did the company which Mamoon Hamid is affiliated with invest $1M in the seed for greenhouse?

**Original Answer:**

Strong conviction in team, market and early customer validation.

**Error Type:**

Potential Shortcut

**Comment:**

The coreference of Adventure Capital circled in white in the left slide, _i.e., the company which Mamond Hamid is affiliated with_, makes no sense for answering this question. It is because that the words circled in blue in the right slide, _i.e., invest $1M in the seed_, is a potentially a strong shortcut for answering this question. Though seemingly relying on the information across two pages, it is still likely a single-page question.

**Revised Question:**

Why did greenhouse invest $1M in the seed for greenhouse?

**Revised Answer:**

Strong conviction in team, market and early customer validation.

Figure 18: Example of the original annotation with _Potential Shortcut_.

## 5 Low Document-relevant Question:

The resolution of the question does not rely on the information from the document. It can be solved by the parametric knowledge in the LVLMs.

Figure 19: Example of the original annotation with _Low Document-relevant Question_.

## 6 Decontextulization-required Question:

The understanding of the question is conditioned on a single page or even a single component of the document.

When dealing with questions categorized under any of these six problem types, annotators are instructed to either revise or remove them. Typically, repetitive questions and those with potential shortcuts are removed. In contrast, wrongly-annotated or decontextualization-required questions are generally revised. For ambiguous and low document-relevant questions, the course of action depends more on the annotators' discretion.

Figure 20: Example of the original annotation with _Decontextulization-required Question_.

### New Question Annotation

We annotate new questions on both existing and newly-collected documents. To ensure a diverse range of questions, we impose limitations on the question distributions categorized by their types (_i.e.,_ single-page, cross-page or unanswerable) and evidence sources (_i.e.,_ table, chart, image). To balance existing questions which are mostly single-page and text-based, we place greater emphasis on cross-page, unanswerable, table-related, chart-related, and image-related questions. The detailed standards are as follows:

### Potential Bias for LVLM-based Quality Checking

As described in Section 3.3, we employ GPT-4o to remove document-agnostic (_i.e.,_ can be correctly answer without documents) samples and review potential wrongly-labeled samples. A reasonable speculation raises that our final benchmark can be biased toward GPT-4o's answers, especially when GPT-4o outperforms others by a large margin. We discuss this potential bias as follows.

We check the effect of GPT-4o's involvement in the quality control step-by-step. Specifically, we compare the performance of samples remained after each step across GPT-4o and two other competitive models (GPT-4V and Gemini-1.5-Pro). We show their results in the table below.

The results illustrate that the potential bias in step 1 (document-relevance detection) actually reduce, rather than increase, the performance gap between GPT4o and other models. It is because that we filter out all samples correctly answered by GPT4o without the access to documents. Under this case, the more significant performance drop of GPT-4V and Gemini-1.5-Pro can only be attributed to their limited document understanding and over-reliance on their internal knowledge. Regarding the step 2 and 3 (self-reflection and cross-checking), we provide inconsistent answers between human annotations and GPT4o's predictions to annotators and ask them to check and revise accordingly. The potential bias of this step does lead to a slight performance bias (1.1% absolute difference at maximum). We believe that such bias is NOT the main cause of GPT4o's significantly best performance. Without the involvement of GPT-4o in the quality control process, GPT-4o still significantly outperforms GPT-4V by 7.9% (43.1% - 35.2%) and Gemini-1.5-Pro by 19.8% (43.1% - 23.3%). Accordingly, all primary conclusions in our paper still hold.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline  & **GPT-4o** & **GPT-4V** & **Gemini-1.5-Pro** \\ \hline No quality control & 43.1\% & 35.2\% & 23.3\% \\ + document-relevance detection & 41.2\% & 31.0\% & 20.5\% \\ + document-relevance detection + self-reflection / cross-checking & 42.7\% & 31.4\% & 20.9\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: Step-wise performance comparison with and without LVLM-based quality checking

\begin{table}
\begin{tabular}{l|c c|c c|c} \hline \hline \multirow{2}{*}{**Document Type**} & \multicolumn{2}{c}{**Evidence Page**} & \multicolumn{2}{c}{**Evidence Source**} & \multirow{2}{*}{**All**} \\  & Cross-page & Unanswerable & Table & Chart & Image & \\ \hline Industrial File & \(\geq 2\) & - & & - & \(\geq 3\) \\ Workshop \& Tutorial & \(\geq 2\) & \(\geq 1\) & —\(\geq 3\) & — & \(\geq 6\) \\ Research Report & \(\geq 3\) & \(\geq 1\) & \(\geq 2\) & \(\geq 2\) & - & \(\geq 5\) \\ Financial Report & \(\geq 5\) & \(\geq 2\) & \(\geq 7\) & - & - & \(\geq 10\) \\ Academic Paper & \(\geq 3\) & \(\geq 1\) & \(\geq 2\) & —\(\geq 3\) & — & \(\geq 6\) \\ Guidebook & \(\geq 3\) & \(\geq 1\) & - & - & \(\geq 4\) & \(\geq 7\) \\ Brochure & \(\geq 2\) & \(\geq 1\) & - & - & \(\geq 3\) & \(\geq 7\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The **minimum** requirements for the number and distribution of questions, categorized by the evidence page numbers and evidence sources. We have set varying requirements for different document types based on their specific characteristics.

### GUI screenshots

We present the screenshots for editing existing questions and annotating new questions (along with their reference answers and meta-data) in Figure 21 and Figure 22 respectively.

### Annotation Cost

This benchmark is annotated by the authors of this paper. Therefore, the data collection does not need compensation. And we count the time cost of our benchmark as below.

**Pre-annotation** (about 45h): the development of annotation interface (10h), the writing of annotation guideline (5h), training session (10h), preliminary annotation and personalized feedback (20h).

**Annotation** (about 150h): It takes about 60-90 minutes for the annotation of each document. And all of the 130 documents take about 150 hours.

**Post-annotation** (about 45h): quality checking (30h), data processing and release preparation (15h).

In summary, our benchmark annotation approximately takes a total of 45+150+45=240 hours (1.36 man months).

Figure 21: GUI screenshot for editing existing questions (along with reference answers and meta-data)

Figure 22: GUI screenshot for annotating new questions (along with reference answers and meta-data)Experimental Details

### Prompt for Response Generation

Listing 1: Prompt used for response generation. The [Document] is in PNG format (page screenshots) for LVLMs, and TXT format for LLMs.

[Document]

Read the above documents and answer this question:

[question]

Please make your answer as concise as possible.

### Prompt for Answer Extraction

Given the question and analysis, you are tasked to extract answers with required formats from the free-form analysis.

- Your extracted answers should be one of the following formats: (1) Integer, (2) Float, (3) String and (4) List. If you find the analysis the question can not be answered from the given documents, type "Not answerable". Exception: If the analysis only tells you that it can not read/understand the images or documents, type "Fail to answer".

- Please make your response as concise as possible. Also note that your response should be formatted as below:

[command=

]

 [command={}] ///

Please read the following example, then extract the answer from the model response and type it at the end of the prompt.

---

Question: List the primary questions asked about the services in this report. Analysis: The primary questions asked about the services in the report for The Limes Residential Home are:

1. Is the services safe?

2. Is the service effective?

3. Is the service caring?

4. Is the service responsive?

5. Is the service well-led?

Extracted answer: ['Is the service safe?', 'Is the service effective', 'Is the serve caring?', 'Is the service responsive?', 'Is the service well-led?'] Answer format: List

---

Question: How many regulations of the HSCA 2008 are breached in all according to this report? Analysis: According to the report, the provider breached 10 Health and Social Care Act 2008 (Regulated Activities) Regulations in total. Here are the specifics:

1. Regulation 13: Safeguarding service users from abuse and improper treatment

2. Regulation 12: Safe care and treatment

3. Regulation 18: Staffing

4. Regulation 11: Need for consent

5. Regulation 10: Dignity and respect

6. Regulation 9: Person-centred care

7. Regulation 17: Good governance

8. Regulation 18 (CQC Registration Regulations 2009): Notification of other incidents

9. Regulation 18: Failure to maintain an accurate and up-to-date care plan 10. Regulation 11: Failure to implement the Mental Capacity Act 2005 code of practice effectively These breaches involve issues concerning staffing, safeguarding, medicines management, dignity and respect, consent, care planning, governance, and failure to notify the CQC of incidents. Extracted answer: 10 Answer format: Integer --- Question: According to the survey that is the percentage of Chinese who are paying more or about the same attention to politics after Trump's election? Analysis: The survey provided does not specify the percentage of Chinese individuals specifically who are paying more or about the same attention to politics after Trump's election. The report focuses primarily on American demographics and does not include specific details about the Chinese population in relation to this question. If you need information about a different demographic or a summary of the findings from the American demographic, I can certainly help with that! Extracted answer: Not answerable Answer format: None --- Question: How many quotations from male respondent over 50 years old are included in this report? Analysis: The image you've provided appears to be a screenshot of a document with multiple charts. However, the text is too small and blurry to read accurately. If you can provide a clearer image or more context, I might be able to help you with your question. Extracted answer: Fail to answer Answer format: None ---

### Rules for Score Calculation

We evaluate the model's responses by scoring the extracted answers against the reference answers. The scorer is rule-based and employs different strategies according to the format of the reference answer. We detail its rules as below:

**String:** We firstly use a series of regular expressions to determine whether the answers require exact matching (_e.g.,_ telephone numbers, email addresses, website addresses, file names, times, dates, _etc._). If an exact match is needed, we perform a straightforward string comparison and score the answer either 0 or 1. Otherwise, we follow previous work [17] and calculate the ANLS (Average Normalized Levenshtein Similarity) with a pre-defined threshold (\(\tau=0.5\)).

**Integer:** We perform an exact match comparison and score the answer either 0 or 1.

**Float:** We view the prediction and reference answers as equal if they fall within a 1% relative tolerance.

**List:** We adopt a relatively strict rule for scoring answers in list format: predictions that do not have the same number of elements as the reference receive a score of 0. For the remaining predictions, as Eq. 1 indicates, we score each element in order and use the minimum element-wise score as the score for the entire list. The element-wise scoring strategies is determined by the formats of elements (_i.e.,_ string, integer or float).

\[\begin{split}\text{pred\_list},\text{ref\_list}=\text{sorted }(\text{pred\_list}),\text{sorted}(\text{ref\_list})\\ \text{Score}(\text{pred\_list},\text{ref\_list})=\text{min}(\\ [\text{Score}(\text{pred},\text{ref})\text{ for pred},\text{ ref in zip}(\text{pred\_list},\text{ ref\_list})]\end{split}\] (1)Evaluation detailed in the Appendix B.4 shows that while this scorer is not perfect, it aligns well with human judgment. We will continue refining these rules to cover more corner cases and enhance their accuracy.

### Human Evaluation on the Automatic Evaluation Pipeline

We conduct human evaluations to assess the performance of our automatic evaluation pipeline, which includes the answer extractor and the score calculator. Specifically, we randomly select 100 questions and review their responses from two representative LVLMs: GPT-4o and Gemini-1.5-Pro. We manually evaluate the correctness of each response and compare the results between human evaluation and automatic evaluation. The performance, as shown in Table 7, indicates a high correlation between human judgment and our automatic pipeline.

### Model Hyperparameters

The hyperparameters of used LVLMs and LLMs in Section 3.3 are detailed in Table 8. The temperature is set as \(0.0\), and the max_new_tokens is set as \(1024\) for all the models. The 'concatenated_images' parameter determines the maximum number of images that can be combined into a single input for LVLMs. By concatenating multiple images, we can meet the minimum context window requirements. The'max_pages' parameter specifies the maximum number of images that can be directly input into the LVLMS without concatenation.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c}{**Inconsistent Evaluation**} \\  & Ans. Extractor & Scorer & Overall \\ \hline GPT-4o & 4 & 2 & 6 \\ Gemini-1.5-Pro & 2 & 2 & 4 \\ \hline \hline \end{tabular}
\end{table}
Table 7: We manually check 100 responses from GPT-4o and Gemini-1.5-Pro, and compare the evaluation results between humans and our automatic pipeline.

\begin{table}
\begin{tabular}{l|c} \hline \hline
**Model** & \multicolumn{2}{c}{**Hyperparameters**} \\ \hline \(\mathit{LLM}\) & \\ \hline ChatGLM-128k & max\_input\_words=60000 \\ Mistral-Instruct-v0.2-7B & max\_input\_words=20000 \\ Mistral-Instruct-v0.1-8x7B & max\_input\_words=20000 \\ Mistral-Instruct-v0.1-8x22B & max\_input\_words=40000 \\ QWen-Plus & max\_input\_words=1600 \\ DeepSeeV-V2 & max\_input\_words=20000 \\ \hline \(\mathit{LVLM}\) & \\ \hline DeepSeeV-VL-Chat & concatenated\_images=5 \\ Qwen-VL-Chat & concatenated\_images=5 \\ Idefes2 & concatenated\_images=5 \\ MiniCPM-Llama-V2.5 & concatenated\_images=2 \\ InternLM-XC2-4KHD & concatenated\_images=2 \\ Monkey-Chat & concatenated\_images=1 \\ CogVLM2-Llama3-Chat & concatenated\_images=1 \\ InternVL-Chat-v1.5 & concatenated\_images=5 \\ EMU2-Chat & concatenated\_images=5 \\ \hline \(\mathit{LLM}\) \& \\ \hline Claude-3 Opus & version=claude-3-opus-20240229, concatenated\_images=20 \\ Gemini-1.5-Pro & max\_pages=120, version=gemini-1.5-pro-latest \\ GPT-4-turbo & max\_pages=120, version=gpt-4-turbo-2024-04-09 \\ GPT-4o & max\_pages=120, version=gpt-4o-2024-05-13 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Model HyperparametersQualitative Study

### Error Analysis

We delve into the analysis of error by GPT-4o to further understand its bottlenecks and potentials on long-context document understanding. We manually check 72 incorrect responses and categorized their error reasons into 7 types. Except for the _Extraction Error_ caused by our automatic evaluation pipeline (see Appendix B.4), we detail and showcase another six reasons as below:

**Perceptual Error:** GPT-4o sometimes struggles to extract or understand visual information from document screenshots. For instance, it misinterprets the axes and colored circles in the charts shown in Figure 23. Additionally, it inaccurately counts the number of green bars in Figure 24. They demonstrate that even the cutting-edge LVLMs still fall short in fundamental perceptual capabilities.

**Incomplete Evidence:** Though GPT-4o has achieved significantly better _global searching abilities_ compared to other models when dealing with lengthy, multi-modal documents, it sometimes still omits certain information. For example, GPT-4o misses one chapter author from Columbia University in the full list (Figure 25). Additionally, it overlooks an app that appears across two pages (Figure 26).

**Hallucinated Evidence:** As stated in Section 3.4, GPT-4o adopts more aggressive strategies and tends to provide more false-positive answers. It sometimes even fabricates non-existent evidence in documents to support its incorrect responses. For example, it references a non-existent page in Figure 27, and fabricates the content of a page in Figure 28. The above examples clearly reveal the importance of further research on LVLMs' hallucination and safety.

**Knowledge Lacking:** Resolving certain questions requires both information from the documents and the parametric knowledge within LVLMs. We have observed error cases stemming from the absence of specific knowledge. For example, GPT-4o overlooks details about the _fixed asset turnover ratio_ and uses the single-point value instead of the average value to calculate this metric (Figure 29). Additionally, it misidentifies buildings at Tsinghua University in Figure 30.

**Reasoning Error:** Though not a primary cause, flawed reasoning based on correctly collected evidence and information from documents can sometimes lead to wrong answers. For example, GPT-4o correctly gathers all data but calculates a relative percentage instead of an absolute percentage in Figure 31. Additionally, as shown in Figure 32, it correctly lists all quizzes but inaccurately counts them in the final step.

**Irrelevant Answer:** GPT-4o sometimes misunderstands the intent of questions and provides irrelevant responses. For instance, in Figure 33, GPT-4o answers about button operations when the question asks about button functions. Similarly, in Figure 34, where the question asks for the MOST discrimination type, GPT-4o summarizes all types instead.

## Perceptual Error: Case 1

**Question**: Among people who age 30-49, what is the difference of percentage value between Americans and German having positive view on their bilateral relationship?

**Response**: The difference in the percentage of people aged 30-49 who have a positive view of the U.S.-Germany relationship between Americans and German is 9% points. According to data on page 5, 40% of Americans aged 30-49 view the relationship positively, while only 31% of Germans in the same age group share this view.

**Error Type: Perceptual Error Analysis:** The model wrongly identifies the percentages of people aged 30-49 who have a positive view of the U.S.-Germany relationship from the chart. It should be 35% and 72%, rather than 31% and 40%.

Figure 23: Error example about _Perceptual Error_

[MISSING_PAGE_EMPTY:32]

## Appendix A

Figure 25: Error example about _Incomplete Evidence_

## Appendix A

Figure 26: Error example about _Incomplete Evidence_

## Appendix A

Figure 27: Error example about _Hallucinated Evidence_

## Appendix A

Figure 28: Error example about _Hallucinated Evidence_

### Knowledge Lacking: Case 1

**Figure 29: Error example about _Knowledge Lacking_

[MISSING_PAGE_EMPTY:38]

Figure 31: Error example about _Reasoning Error_

Figure 32: Error example about _Reasoning Error_

Figure 33: Error example about _Irrelevant Answer_

## Appendix A

Figure 34: Error example about _Irrelevant Answer_

Figure 33: Error example about _Irrelevant Answer_

### Case Study

Figure 35: Case Study. Evidence source: table. The evidence pages are zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.

Figure 36: Case Study. Evidence source: text. The evidence pages are zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.

Figure 37: Case Study. Evidence source: layout. The evidence page is zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.

Figure 38: Case Study. Evidence source: image. The evidence page is zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.

## Appendix D Limitations

MMLongBench-Doc is the first comprehensive benchmark designed to evaluate the long-context document understanding capabilities of LVLMs. While our benchmark addresses significant gaps in the previous datasets, we acknowledge several limitations.

One primary limitation is the scale of the benchmark. Currently, our benchmark includes a test set comprising 135 documents and 1,082 questions. It is much smaller compared to previous datasets. The complexity and difficulty of annotations limit the scale of our benchmark. As a long-context benchmark, our documents average about 50 pages and 20,000 tokens. And most questions require either complicated reasoning or cross-page comprehension. It takes more than one hour for an expert-level annotator to read through a single document, and then edit existing instances and create new instances on this document. Given the purpose of MMLongBench-Doc as an evaluation benchmark, we prioritize annotation quality over quantity. Moreover, the results presented in Sections 3.3 and 3.4 confirm that the scale of our benchmark is sufficient for fine-grained evaluations across different document types, evidence sources, evidence pages, _etc._. Additionally, we plan to expand our benchmark by adding more documents and questions in future iterations.

We roughly categorize these questions into three types, _i.e.,_ single-page, cross-page, and unanswerable questions, based on whether evidence can be found in the documents and the number of evidence pages. However, unlike MMBench [41] or MathVista [56], we provide no further taxonomy to classify some (_e.g.,_ 7 or 20) fine-grained, evaluated reasoning or perception capabilities out of two main reasons: (1) Prior (_i.e.,_ pre-annotation) taxonomy limits the diversity of the questions. Therefore we provide no predefined classifications in our guideline and encourage the expert-level annotators to freely write questions without constraints. (2) The intrinsic complexity of document understanding presents significant challenges for establishing a posterior (_i.e.,_ post-annotation) taxonomy.

While there exist limitations in our benchmark, MMLongBench-Doc surely represents a significant step forward in this field. We would iteratively maintain and refine this benchmark and hope it could push forward the development of long-context document understanding.

## Appendix E Social Impacts

The development and use of MMLongBench-Doc may have potential societal implications. For instance, biased or inaccurate outputs from benchmarked models could perpetuate harmful stereotypes or reinforce existing social inequalities. Additionally, the ability to process and analyze long documents could potentially be used to surveil or monitor individuals' personal information. Developers and users of MMLongBench-Doc benchmark must be aware of these potential consequences and take steps to ensure responsible development and deployment of AI models.

## Appendix F Author Statement

The authors state that all of the previous datasets that we collected are licensed under the Creative Commons license (CC-BY) or other open-source licenses. Using this dataset should abide by the policy of OpenAI. Regarding the newly collected documents, we manually check them to ensure their availability for academic use. Should any authors request the removal of their documents, we will promptly comply.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Appendix D. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See supplemental material E. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] We didn't involve theory in this benchmark. 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] https://mayubo2333.github.io/MMLongBench-Doc 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] See Appendix F. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] https://mayubo2333.github.io/MMLongBench-Doc 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix A.1 and A.2 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]