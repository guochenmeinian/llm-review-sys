# Graph Classification Gaussian Processes

via Hodgelet Spectral Features

 Mathieu Alain\({}^{1,5,\dagger}\) So Takao\({}^{2}\) Bastian Rieck\({}^{3}\) Xiaowen Dong\({}^{4}\) Emmanuel Noutahi\({}^{5}\)

\({}^{1}\)University College London \({}^{2}\)California Institute of Technology \({}^{3}\)University of Fribourg

\({}^{4}\)University of Oxford \({}^{5}\)Valence Labs

\({}^{\dagger}\)mathieu.alain.21@ucl.ac.uk

###### Abstract

The problem of classifying graphs is ubiquitous in machine learning. While it is standard to apply graph neural networks or graph kernel methods, Gaussian processes can be employed by transforming spatial features from the graph domain into spectral features in the Euclidean domain, and using them as the input points of classical kernels. However, this approach currently only takes into account features on vertices, whereas some graph datasets also support features on edges. In this work, we present a Gaussian process-based classification algorithm that can leverage one or both vertex and edges features. Furthermore, we take advantage of the Hodge decomposition to better capture the intricate richness of vertex and edge features, which can be beneficial on diverse tasks.

## 1 Introduction

Classification is omnipresent in machine learning, yet typically assumes data to be Euclidean. Extending this task to non-Euclidean domains, such as graphs, presents challenges due to their irregularity, varying sizes, and multi-site information (e.g. vertices and edges). However, classifying graphs is of critical importance in scientific and industrial applications, being used for instance, to predict properties of molecules, or discovering new drugs [1, 2]. Although graph neural networks [3] are usually the model of choice for such applications, a downside is that they may require large datasets for effective training. Gaussian processes (GP) [4], on the other hand, prove to be a data-efficient and interpretable modelling choice. They do not need separate validation datasets to tune hyperparameters and provide robust uncertainty estimates for predictions. This makes them ideal for small-data scenarios and high-risk decision-making tasks that require reliable uncertainty estimates.

In a recent work, Opolka et al. [5] introduced a GP-based algorithm capable of classifying graphs. Their method relies on tools developed from the graph signal processing literature [6], including the spectral graph Fourier transform [7] and the spectral graph wavelet transform [8]. Specifically, spectral graph methods use spatial graph features to compute graph spectral coefficients, which may be leveraged to generate spectral features in the Euclidean domain. Such spectral representations can be passed as input points to a standard GP [5], subsequently employed for classification via approximate inference [9]. Closely related are techniques developed in the early graph neural network literature, for instance, Spectral Networks [10] and ChebNet [11], which lean on the graph Fourier transform and graph wavelet transform to establish a notion of convolution on graphs. While the approach proposed in Opolka et al. [5] accommodates features living on vertices, it cannot easily take into account features on edges. However, edge information can often be as valuable as vertex information, representing crucial quantities such as flows [12, 13] and chemical bonds [14].

Our paper aims to fill this gap by proposing a novel GP-based classification algorithm that naturally incorporate features on vertices, edges, and more generally, simplices, building on recent work defining GPs on simplicial complexes [15] and cellular complexes [16]. Moreover, we utilise thecelebrated Hodge decomposition on spatial graph features, separating and processing them into three canonical components, each exhibiting distinct properties. This enables the modelling of these components separately, using different kernels. This idea have been used successfully in past works [17; 18; 19; 20; 15], providing greater flexibility. We are not aware that these recent techniques have been applied in the context of graph classification. We demonstrate empirically that these extensions achieve similar or better performance than the method introduced by Opolka et al. [5]. Although we focus on graphs, our approach can be extended easily to higher-order networks, such as simplicial complexes, cellular complexes, and hypergraphs, which can describe polyadic interactions [21; 22; 23; 24], thereby generalising the dyadic interactions found in graphs (see Appendix D). We highlight that our method can be readily adapted for regression by selecting an appropriate likelihood. Finally, although our Hodgelet spectral features are employed in the context of Gaussian processes, they can be readily integrated into other machine learning methods.

## 2 Gaussian Processes for Graph Classification

We assume a dataset containing \(M\) undirected graphs \(\mathcal{G}^{(1)},\ldots,\mathcal{G}^{(M)}\) and labels \(y^{(i)}\in\mathbb{Z}\), for \(1\leq i\leq M\). We assign an orientation to each graph but emphasise that this choice is arbitrary. Let \(\mathcal{V}^{(i)}\) be the vertex set and \(\mathcal{E}^{(i)}\) be the edge set, both finite. For each graph \(\mathcal{G}^{(i)}=(\mathcal{V}^{(i)},\mathcal{E}^{(i)})\), there are \(N_{v}^{(i)}\) vertices, \(N_{e}^{(i)}\) edges, and an incidence matrix \(\bm{B}_{ve}^{(i)}\in\mathbb{Z}^{N_{v}^{(i)}\times N_{e}^{(i)}}\). The latter encodes the incidence between each vertex and edge, and furthermore defines the _graph Laplacian_\(\bm{L}_{v}^{(i)}\coloneqq\bm{B}_{ve}^{(i)}\bm{B}_{ve}^{(i)\top}\in\mathbb{Z}^{N _{v}^{(i)}\times N_{v}^{(i)}}\). By considering \(3\)-cliques, we obtain the edge-to-triangle incidence matrix \(\bm{B}_{ct}^{(i)}\in\mathbb{Z}^{N_{e}^{(i)}\times N_{e}^{(i)}}\), where \(N_{t}^{(i)}\) is the number of triangles in \(\mathcal{G}^{(i)}\). Likewise, we define the _graph Helmholtzian_\(\bm{L}_{e}^{(i)}\coloneqq\bm{B}_{ve}^{(i)\top}\bm{B}_{ve}^{(i)}+\bm{B}_{ct}^{(i) \top}\bm{B}_{ct}^{(i)\top}\in\mathbb{Z}^{N_{e}^{(i)}\times N_{e}^{(i)}}\), which applies to edges rather than vertices. We underline that \(\bm{L}_{v}^{(i)}\) and \(\bm{L}_{e}^{(i)}\) are instances of the _discrete Hodge Laplacian_ (see Appendix D.2). Let \(\mathcal{V}^{(i)}\rightarrow\mathbb{R}^{D_{v}}\) and \(\mathcal{E}^{(i)}\rightarrow\mathbb{R}^{D_{e}}\) be two functions, for \(D_{v},D_{e}\in\mathbb{N}\). By introducing an ordering on vertices and edges, we represent the preceding functions as matrices \(\mathbb{R}^{D_{v}\times N_{v}^{(i)}}\) and \(\mathbb{R}^{D_{e}\times N_{v}^{(i)}}\), respectively, which are understood as vertex and edge feature matrices, containing \(D_{v}\) and \(D_{e}\)_channels_, respectively. We denote vertex features for channel \(1\leq d\leq D_{v}\) by the column vector \(\bm{x}_{vd}^{(i)}\in\mathbb{R}^{N_{v}^{(i)}}\) and edge features for channel \(1\leq d\leq D_{e}\) by the column vector \(\bm{x}_{ed}^{(i)}\in\mathbb{R}^{N_{e}^{(i)}}\). We stress that our approach can adapt to graphs that may have vertex features, edge features, or both.

### Wavelet transforms on graphs

The key idea behind our graph classification algorithm is to convert vertex and edge features from the graph domain into _spectral features_ in the Euclidean domain, enabling standard GP classification. We first consider the eigendecomposition of the graph Laplacian and graph Helmholtzian

\[\bm{L}_{v}^{(i)}=\bm{U}_{v}^{(i)}\bm{\Lambda}_{v}^{(i)\top}\bm{U}_{v}^{(i) \top},\qquad\bm{L}_{e}^{(i)}=\bm{U}_{e}^{(i)}\bm{\Lambda}_{e}^{(i)}\bm{U}_{e }^{(i)\top},\] (1)

and define _graph Fourier coefficients_ as projections of spatial graph features onto the eigenbases,

\[\hat{\bm{x}}_{vd}^{(i)}\coloneqq\bm{U}_{v}^{(i)\top}\bm{x}_{vd}^{(i)}\in \mathbb{R}^{N_{v}^{(i)}},\qquad\hat{\bm{x}}_{ed}^{(i)}\coloneqq\bm{U}_{e}^{(i) \top}\bm{x}_{ed}^{(i)}\in\mathbb{R}^{N_{e}^{(i)}}.\] (2)

We observe that \(\hat{\bm{x}}_{\bullet d}^{(i)}\) reside in the eigenspace of \(\bm{L}_{\bullet}^{(i)}\). Furthermore, they are _invariant_ to vertex and edge ordering, making them sound choices for constructing spectral features, and they are perfectly localised in frequency, capturing _global_ properties of the original features. However, it is often beneficial to also possess spatially localised information, focusing on _local_ properties. A solution is to compute the more flexible _graph wavelet coefficients_[8] (see Appendix B) by modulating Fourier coefficients using a _wavelet filter_ on the eigenvalues \(\bm{\Lambda}_{v}^{(i)}\) and \(\bm{\Lambda}_{e}^{(i)}\), and then perform the inverse _Fourier transform_. A wavelet filter is a combination of a _scaling function_ at a single scale and a _wavelet function_ at multiple scales, offering _multi-scale resolution_. Wavelet functions, \(b:\mathbb{R}\rightarrow\mathbb{R}\) and \(d:\mathbb{R}\rightarrow\mathbb{R}\), operate as _band-pass filters_, and scaling functions, \(a:\mathbb{R}\rightarrow\mathbb{R}\) and \(c:\mathbb{R}\rightarrow\mathbb{R}\), are _low-pass filters_. A wavelet filter captures one perspective of a graph, but to obtain a comprehensive picture, it is essential to employ multiple wavelet filters. Wavelet filters \(j\) are defined by

\[w_{vj}(\lambda) \coloneqq a(\alpha_{j}\lambda)+\sum_{l=1}^{L_{v}}b(\beta_{jl}\lambda), \quad\Theta_{vj}\coloneqq\{\alpha_{j},\beta_{j1},\dots,\beta_{jL_{v}}\},\quad 1 \leqslant j\leqslant W_{v},\] (3) \[w_{ej}(\lambda) \coloneqq c(\gamma_{j}\lambda)+\sum_{l=1}^{L_{v}}d(\delta_{jl} \lambda),\quad\Theta_{ej}=\{\gamma_{j},\delta_{j1},\dots,\delta_{jL_{e}}\}, \quad 1\leqslant j\leqslant W_{e},\] (4)

where \(L_{\bullet}\) is the number of scales, \(W_{\bullet}\) is the number of wavelet filters, and \(\Theta_{\bullet j}\) is a collection of trainable parameters controlling the scaling. Wavelet coefficients are given by

\[\hat{\bm{x}}^{(i)}_{vdj}\coloneqq\bm{U}^{(i)}_{v}w_{vj}\big{(}\bm{\Lambda}^{( i)}_{v}\bm{U}^{(i)\top}_{v}\bm{x}^{(i)}_{vd}\in\mathbb{R}^{N^{(i)}_{\circ}}, \qquad\hat{\bm{x}}^{(i)}_{edj}\coloneqq\bm{U}^{(i)}_{e}w_{ej}\big{(}\bm{ \Lambda}^{(i)}_{e}\bm{)}\bm{U}^{(i)\top}_{e}\bm{x}^{(i)}_{ed}\in\mathbb{R}^{N^ {(i)}_{\circ}}.\] (5)

### Hodge decomposition

The _discrete Hodge decomposition_[25] (see Appendix D.3) states that _spatial graph feature spaces_, i.e. the spaces inhabited by \(\bm{x}^{(i)}_{vd}\) and \(\bm{x}^{(i)}_{ed}\), can each be separated into an orthogonal sum of three subspaces, _exact_, _co-exact_, and _harmonic_, collectively referred to as the _Hodge subspaces_. From this, the eigenbases in (1) can be divided into sub-eigenbases, each spanning a different Hodge subspace,

\[\bm{U}^{(i)}_{v}=\big{[}\bm{U}^{(i)}_{vc}\ \bm{U}^{(i)}_{vh}\big{]}, \qquad\bm{U}^{(i)}_{e}=\big{[}\bm{U}^{(i)}_{ee}\ \bm{U}^{(i)}_{ec}\ \bm{U}^{(i)}_{eh}\big{]}.\] (6)

We observe that \(\bm{U}^{(i)}_{v}\) has only two components. The _co-exact sub-eigenbasis_\(\bm{U}^{(i)}_{vc}\) amounts to the non-zero eigenvectors of \(\bm{L}^{(i)}_{v}\), and the _harmonic sub-eigenbasis_\(\bm{U}^{(i)}_{vh}\) to the zero ones. The _exact_ and co-exact _sub-eigenbases_\(\bm{U}^{(i)}_{ee}\) and \(\bm{U}^{(i)}_{ec}\) are the non-zero eigenvectors of \(\bm{B}^{(i)\top}_{ve}\bm{B}^{(i)}_{ve}\) and \(\bm{B}^{(i)}_{et}\bm{B}^{(i)\top}_{et}\), respectively. Finally, the harmonic sub-eigenbasis \(\bm{U}^{(i)}_{eh}\) comprises the zero eigenvectors of \(\bm{L}^{(i)}_{e}\). For edges, the exact and co-exact components are sometimes termed _gradient_ and _curl_ components, respectively, reminiscent of vector fields. A gradient part is _curl-free_, indicating no _vortices_. A curl part is _divergence-free_, meaning no _sources_ or _sinks_. A harmonic part is curl-free and divergence-free.

### Hodgelet spectral features

We generate graph spectral features from graph wavelet coefficients and then use them in downstream GP classification tasks (see Appendix A). By combining the wavelet transform (5) and the Hodge decomposition (6), we compute the wavelet coefficients \(\hat{\bm{x}}^{(i)}_{vdcj},\hat{\bm{x}}^{(i)}_{vdjh}\) and \(\hat{\bm{x}}^{(i)}_{edje},\hat{\bm{x}}^{(i)}_{edjc},\hat{\bm{x}}^{(i)}_{edjh}\) (see Appendix C for more details). We derive our _Hodgelet spectral features_ by concatenating the 2-norm of the preceding wavelet coefficients across each wavelet filter and channel, resulting in the column vectors \(\bm{v}^{(i)}_{c},\bm{v}^{(i)}_{h}\in\mathbb{R}^{W_{v}D_{v}}\) and \(\bm{e}^{(i)}_{e},\bm{e}^{(i)}_{c},\bm{e}^{(i)}_{h}\in\mathbb{R}^{W_{v}D_{e}}\). These spectral representations, which are invariant to graph isomorphism, are then fed to our additive _Hodgelet kernel_

\[\begin{split}\kappa\big{(}\mathcal{G}^{(i)},\mathcal{G}^{(j)} \big{)}&\coloneqq\kappa_{vc}\big{(}\bm{v}^{(i)}_{c},\bm{v}^{(j)}_ {c}\big{)}+\kappa_{vh}\big{(}\bm{v}^{(i)}_{h},\bm{v}^{(j)}_{h}\big{)}\\ &+\kappa_{ee}\big{(}\bm{e}^{(i)}_{e},\bm{e}^{(j)}_{c}\big{)}+ \kappa_{ec}\big{(}\bm{e}^{(i)}_{c},\bm{e}^{(j)}_{c}\big{)}+\kappa_{eh}\big{(} \bm{e}^{(i)}_{h},\bm{e}^{(j)}_{h}\big{)},\end{split}\] (7)

where \(\kappa_{\bullet\bullet}\) is a standard kernel function, such as the _squared exponential kernel function_. We note that parameters \(\Theta_{\bullet j}\) are optimised jointly with the kernel hyperparameters and that a separate kernel for each part of the Hodge decomposition offers greater flexibility. We highlight that our GP-based classification algorithm supports multi-dimensional spatial graph features and graphs of varying sizes, in contrast to typical graph kernel-based methods [26]. The GP component scales according to the number of graphs, while the eigendecompositions are a one-off cost that can be performed in advance.

## 3 Experiments

The aim of the experiments is two-fold: (1) we validate the added flexibility given by the Hodge decomposition, and (2) we demonstrate that when edge features are present, it is better to work with edges directly rather than converting graphs to line-graphs. The latter point has been observed in previous works [22; 15] and our experiments, across 10 seeds, further validate their conclusions.

### Graph classification benchmarks

Our first experiment compares the method in Opolka et al. [5], which we refer to the wavelet-transform GP (WT-GP), which does not use the Hodge decomposition, to our method, WT-GP-Hodge, which employs the decomposition. In Table 1, we display the results on some standard graph classification benchmark datasets used in Opolka et al. [5]. We observe that on all but one dataset, WT-GP-Hodge improves the classification accuracy. This may be surprising as the Hodge decomposition for vertex features yields only co-exact and harmonic parts, where the harmonic part is constant across the connected components of the graph. This suggests that we can gain accuracy by separating vertex features into a constant bias (harmonic part) and fluctuations around it (co-exact part), using different kernels for each. However, if constant biases do not aid classification (e.g. when classes have vertex feature of similar magnitudes), then we do not expect WT-GP-Hodge to improve over WT-GP.

### Vector field classification

Our second experiment consider the task of classifying noisy vector fields, i.e. predicting whether they are predominantly divergence-free or curl-free. We proceed by generating 100 random vector fields, with half mostly divergence-free (Figure E.1c) and the other half mostly curl-free (Figure E.1d). The generated vector fields are then projected onto the edges of a randomly generated triangular mesh on a square domain with \(N\) vertices (Figure E.2). Finally, we corrupt the edge features with i.i.d. Gaussian noise, resulting in a dataset composed of \(100\) oriented graphs, each containing scalar edge features corresponding to the _net flow_ of the vector field along the edges. Again, we compare our method against the vanilla WT-GP classification method. However, since WT-GP does not take edge features, we first convert graphs to line-graphs before applying it. We refer to it as WT-GP-LG. In Table 2, we display the results of WT-GP-LG and WT-GP-Hodge for various choices of \(N\). We see that WT-GP-Hodge is consistently better, with large improvements as mesh resolution is increased. On the other hand, WT-GP-LG cannot distinguish accurately between divergence-free and curl-free fields, even as the mesh resolution becomes higher. Likely reasons: (1) the Hodge decomposition in WT-GP-Hodge helps to discriminate more clearly between divergence-free and curl-free components, and (2) there are properties that are canonical to edges, such as orientation, which WT-GP-Hodge can handle naturally, whereas WT-GP-LG cannot. In Figure 1, we plot the classification accuracy with varying noise level, which shows robustness of WT-GP-Hodge to noise compared to WT-GP-LG.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & ENZYMES & MUTAG & IMDB-BINARY & IMDB-MULTI & ring-vs-clique & subm \\ \hline WT-GP & 65.00 \(\pm\) 4.94 & 86.73 \(\pm\) 4.18 & **74.20 \(\pm\) 3.87** & 48.73 \(\pm\) 2.76 & 99.5 \(\pm\) 1.5 & 86.42 \(\pm\) 6.92 \\ WT-GP-Hodge & **67.65 \(\pm\) 6.86** & **88.06 \(\pm\) 7.99** & 73.40 \(\pm\) 3.04 & **52.09 \(\pm\) 3.44** & **100.0 \(\pm\) 0.0** & **88.02 \(\pm\) 7.35** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of classification accuracy on several graph classification benchmark datasets.

Figure 1: Accuracy vs. noise level in the vector field classification.

Conclusion and Future Directions

We have presented a GP-based classification algorithm for classifying graphs according to one or both vertex and edge features. By applying the graph wavelet transform to spatial graph features, we have constructed spectral features, providing multi-resolution spectral signatures of the original features, subsequently utilised as input points to a standard GP. Furthermore, by taking the discrete Hodge decomposition, we have shown improvements over the method proposed by Opolka et al. [5], even on graph datasets containing only vertex features, owing to our flexible Hodgelet kernel. Overall, we have demonstrated that our approach effectively improves graph classification tasks by employing a spectral perspective to capture both local and global properties of vertex and edge features. In the future, we intend to explore extensions to higher-order networks, including simplicial complexes, cellular complexes, and hypergraphs, which we briefly outline in Appendix D.

## Acknowledgments

MA is supported by a Mathematical Sciences Doctoral Training Partnership held by Prof. Helen Wilson, funded by the Engineering and Physical Sciences Research Council (EPSRC), under Project Reference EP/W523835/1. ST is supported by a Department of Defense Vannevar Bush Faculty Fellowship held by Prof. Andrew Stuart, and by the SciAI Center, funded by the Office of Naval Research (ONR), under Grant Number N00014-23-1-2729.

## References

* [1] Xiao-Meng Zhang, Li Liang, Lin Liu, and Ming-Jing Tang. Graph neural networks and their current applications in bioinformatics. _Frontiers in Genetics_, 2021.
* [2] Patrick Reiser, Marlen Neubert, Andre Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, and Pascal Friederich. Graph neural networks for materials science and chemistry. _Communications Materials_, 2022.
* [3] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. _IEEE Transactions on Neural Networks_, 2009.
* [4] Carl Edward Rasmussen and Christopher K I Williams. _Gaussian processes for machine learning_. MIT Press, 2005.
* [5] Felix Opolka, Yin-Cong Zhi, Pietro Lio, and Xiaowen Dong. Graph classification gaussian processes via spectral features. In _Conference on Uncertainty in Artificial Intelligence_, 2023.
* [6] Antonio Ortega, Pascal Frossard, Jelena Kovacevic, Jose MF Moura, and Pierre Vandergheynst. Graph signal processing: Overview, challenges, and applications. _Proceedings of the IEEE_, 2018.
* [7] David I Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. _IEEE Signal Processing Magazine_, 2013.
* [8] David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral graph theory. _Applied and Computational Harmonic Analysis_, 2011.
* [9] Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary gaussian process classification. _Journal of Machine Learning Research_, 2008.
* [10] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In _International Conference on Learning Representations_, 2014.

* [11] Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In _Conference on Neural Information Processing Systems_, 2016.
* [12] Junteng Jia, Michael T Schaub, Santiago Segarra, and Austin R Benson. Graph-based semi-supervised and active learning for edge flows. In _International Conference on Knowledge Discovery and Data Mining_, 2019.
* [13] Bulat Kerimov, Vincent Pons, Spyros Pritsis, Riccardo Taormina, and Franz Tscheikner-Gratl. Sensor placement and state estimation in water distribution systems using edge gaussian processes. _Engineering Proceedings_, 2024.
* [14] Mingjian Wen, Samuel M Blau, Evan Walter Clark Spotte-Smith, Shyam Dwaraknath, and Kristin A Persson. Bondnet: a graph neural network for the prediction of bond dissociation energies for charged molecules. _Chemical Science_, 2021.
* [15] Maosheng Yang, Viacheslav Borovitskiy, and Elvin Isufi. Hodge-compositional edge gaussian processes. In _International Conference on Artificial Intelligence and Statistics_, 2024.
* [16] Mathieu Alain, So Takao, Brooks Paige, and Marc P Deisenroth. Gaussian processes on cellular complexes. In _International Conference on Machine Learning_, 2024.
* [17] Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, and Yinyu Ye. Statistical ranking and combinatorial hodge theory. _Mathematical Programming_, 2011.
* [18] Federica Baccini, Filippo Geraci, and Ginestra Bianconi. Weighted simplicial complexes and their representation power of higher-order network data and topology. _Physical Review E_, 2022.
* [19] T Mitchell Roddenberry, Florian Frantzen, Michael T Schaub, and Segarra Santiago. Hodgelets: Localized spectral representations of flows on simplicial complexes. In _International Conference on Acoustics, Speech, and Signal Processing_, 2022.
* [20] Claudio Battiloro, Paolo Di Lorenzo, and Sergio Barbarossa. Topological slepians: Maximally localized representations of signals over simplicial complexes. In _IEEE International Conference on Acoustics, Speech and Signal Processing_, 2023.
* [21] Sergio Barbarossa and Stefania Sardellitti. Topological signal processing over simplicial complexes. _IEEE Transactions on Signal Processing_, 2020.
* [22] Michael T Schaub, Yu Zhub, Jean-Baptiste Sebyc, T Mitchell Roddenberry, and Santiago Segarra. Signal processing on higher-order networks: Livin' on the edge... and beyond. _Signal Processing_, 2021.
* [23] Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson, Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro Lio, Paolo Di Lorenzo, Vasileios Maroulas, Nina Miolane, Farzana Nasrin, Karthikeyan Natesan Ramamurthy, Bastian Rieck, Simone Scardapane, Michael T Schaub, Petar Velickovic, Bei Wang, Yusu Wang, Guo-Wei Wei, and Ghada Zamzmi. Position: Topological deep learning is the new frontier for relational learning. In _International Conference on Machine Learning_, 2024.
* [24] Claudio Battiloro, Lucia Testa, Lorenzo Giusti, Stefania Sardellitti, Paolo Di Lorenzo, and Sergio Barbarossa. Generalized simplicial attention neural networks. _IEEE Transactions on Signal and Information Processing over Networks_, 2024.
* [25] Lek-Heng Lim. Hodge laplacians on graphs. _SIAM Review_, 2020.
* [26] Karsten Borgwardt, Elisabetta Ghisu, Felipe Llinares-Lopez, Leslie O'Bray, and Bastian Rieck. Graph kernels: State-of-the-art and future challenges. _Foundations and Trends in Machine Learning_, 2020.

* [27] Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. _International Conference on Artificial intelligence and Statistics_, 2009.
* [28] James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian Processes for Big Data. In _Uncertainty in Artificial Intelligence_, 2013.
* [29] Manfred Opper and Cedric Archambeau. The variational gaussian approximation revisited. _Neural Computation_, 2009.
* [30] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In _AAAI Conference on Artificial Intelligence_, 2019.
* [31] Ruben Ballester, Ernst Roell, Daniel Bin Schmid, Mathieu Alain, Sergio Escalera, Carles Casacuberta, and Bastian Rieck. Mantra: The manifold triangulations assemblage. _arXiv preprint arXiv:2410.02392_, 2024.
* [32] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20, 2007.
* [33] Keenan Crane. Discrete differential geometry: An applied introduction. _Notices of the AMS, Communication_, 2018.

Gaussian Processes

Gaussian processes are stochastic processes that can be considered as Gaussian distributions extended to _infinite dimensions_, providing a powerful _non-parametric_ technique for modelling distributions over functions.

More precisely, a Gaussian process over \(\mathbb{R}^{d}\) is a random function \(f:\Omega\times\mathbb{R}^{d}\to\mathbb{R}\), characterised by a _mean function_\(\mu:\mathbb{R}^{d}\to\mathbb{R}\) and a _kernel function_\(\kappa:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\), satisfying \(\mu(\bm{x})\coloneqq\mathbb{E}[f(\bm{x})]\) and \(\kappa(\bm{x},\bm{x}^{\prime})\coloneqq\mathrm{Cov}[f(\bm{x}),f(\bm{x}^{\prime })]\), respectively, for any \(\bm{x},\bm{x}^{\prime}\in\mathbb{R}^{d}\). We denote this relation by writing that \(f\sim\mathrm{GP}(\mu,\kappa)\). For any _finite_ collection of _input points_\(\bm{x}_{1},\dots,\bm{x}_{M}\in\mathbb{R}^{d}\), the random vector \(\bm{f}=[f(\bm{x}_{1}),\dots,f(\bm{x}_{M})]^{\top}\sim\mathrm{N}(\bm{\mu},\bm{ K})\) is _jointly_ Gaussian, where \(\bm{\mu}=[\mu(x_{1}),\dots,\mu(x_{M})]^{\top}\) and \((\bm{K})_{ij}=k(\bm{x}_{i},\bm{x}_{j})\).

Gaussian processes are non-parametric in the sense that they define a distribution directly on functions rather than on parameters of a function. This offers the advantage that a Gaussian process can generate functions of increasing sophistication as more data are acquired, unconstrained by a predetermined parametric structure.

For a complete presentation of Gaussian processes, we refer the reader to the book by Rasmussen and Williams [4].

### Bayesian inference paradigm

Gaussian processes are employed in the _Bayesian inference paradigm_:

1. A Gaussian process is defined as a _prior distribution_ over an _unknown function_, capturing our _prior beliefs_ about the possible shapes and behaviour of this function before observing any data.
2. A _likelihood_ is a modelling choice representing how the functions from the prior distribution generate the data. An essential role of the likelihood is to define the assumptions about the _data noise_. By observing a _dataset_, the likelihood assesses the goodness of fit between our prior beliefs and the actual _observed data_.
3. By combining the prior distribution and the likelihood using the celebrated _Bayes' theorem_, a _posterior distribution_ is computed to _update_ our beliefs about the unknown function. For regression, a common assumption is a Gaussian likelihood, leading the posterior distribution to be a Gaussian process again. However, for classification, the likelihood is non-Gaussian, often Bernoulli for _binary classification_ or categorical for _multi-class classification_. This results in a posterior distribution that is _intractable_, meaning that it cannot be expressed in a closed-form. In this case, it is possible to derive an approximate posterior distribution using techniques such as _variational inference_.
4. By conditioning on the dataset, the posterior distribution is employed to derive the _predictive posterior distribution_ for new input points. The mean of this distribution serves as the predictions, while the variance provides a measure of the uncertainty around them.

### Kernel functions

Gaussian process is uniquely defined by a mean function and kernel function, which has the advantage of being simple and _interpretable_. The mean function is often set to zero, i.e. \(\mu(\bm{x})=0\), for every \(\bm{x}\in\mathbb{R}^{d}\), especially after normalising the dataset since it simplifies derivations and proofs. The kernel function is thus considered the most defining component in describing the behaviour of a Gaussian process and measures the similarities between pairs of input points. It is important to note that a kernel function must be a _symmetric_ and _positive semi-definite_ function as it defines the covariance between two input points. Some popular choices are the _square exponential kernel function_ (also called _radial basis function kernel_)

\[\kappa_{\mathrm{se}}(\bm{x},\bm{x}^{\prime})\coloneqq\sigma^{2}\exp\left(- \frac{\|\bm{x}-\bm{x}^{\prime}\|^{2}}{2\ell^{2}}\right),\ \ \ell>0,\] (8)

and the _Matern kernel function_

\[\kappa_{\mathrm{mat}}(\bm{x},\bm{x}^{\prime})\coloneqq\sigma^{2}\frac{2^{1- \nu}}{\Gamma(\nu)}\left(\sqrt{2\nu}\frac{\|\bm{x}-\bm{x}^{\prime}\|}{\ell} \right)^{\nu}K_{\nu}\left(\sqrt{2\nu}\frac{\|\bm{x}-\bm{x}^{\prime}\|}{\ell} \right),\ \ \ell,\nu>0,\] (9)where \(|\cdot|\) is the \(2\)-norm, \(\Gamma\) the _gamma function_, and \(K_{\nu}\) the _modified Bessel function_ of the second kind. We observe that for \(\nu\to\infty\), the Matern kernel function converges to the square exponential kernel function.

The parameters of a kernel function are called _hyperparameters_ and they are typically optimised by _maximum marginal likelihood estimation_.

### Computational cost and memory requirements

When the likelihood is Gaussian, a closed form expression of the posterior exists and computing it requires inverting a \(N\times N\)_Gram matrix_, where \(N\) is the number of training input points. This results to a cubic _computational complexity_\(\mathcal{O}(N^{3})\) and quadratic _memory requirements_\(\mathcal{O}(N^{2})\). Fortunately, sparse Gaussian processes [27] have been devised to alleviate this large computational expense by constructing a smaller training dataset consisting of \(M\)_pseudo input points_, called _inducing points_, where \(M\ll N\). This technique leads to a computational complexity \(\mathcal{O}(NM^{2}+M^{3})\) and memory requirements \(\mathcal{O}(MN)\). The sparse variational Gaussian process in [28] further extends this to non-Gaussian likelihood settings by means of stochastic variational inference (see also [29] for a variational approximation without using inducing points). This can be used to compute approximate posteriors in the setting of classification problems.

## Appendix B Spectral Analysis

Spectral analysis is a set of tools for analysing and manipulating signals according to their constituent _frequencies_. These methods have found numerous applications in disciplines as diverse as telecommunications, physics, chemistry, signal processing, quantitative finance and, of course, machine learning.

Suppose a continuous function \(f:\mathbb{R}\to\mathbb{R}\) representing a signal function. This signal function can be expressed as a superposition of _sinusoidal plane waves_\(e^{i2\pi\xi x}\) at different frequencies \(\xi\),

\[f(x)\coloneqq\int_{-\infty}^{\infty}\hat{f}(\xi)e^{i2\pi\xi x}d\xi,\] (10)

where \(\hat{f}(\xi)\) is the _Fourier transform_ of \(f\),

\[\hat{f}(\xi)\coloneqq\int_{-\infty}^{\infty}f(x)e^{-i2\pi\xi x}dx.\] (11)

The absolute value \(|\hat{f}(\xi)|\) represents the amplitude of \(\xi\), reflecting the strength in the original signal of the sinusoidal plane wave associated to \(\xi\). The transformed function \(\hat{f}\) makes it possible to explore the importance of specific frequencies, revealing insights into patterns, periodicities and trends that may not be apparent from examining the original signal \(f\). We observe that this _integral transform_ does not lose information. Indeed, the function \(f\) can be recovered from (11) using the _inverse Fourier transform_ (10). The collection of sinusoidal plane waves \(\{e^{i2\pi\xi x}\}_{\xi}\) are _orthonormal_ and commonly called the _Fourier basis_.

Fourier transform and Laplacian.There are important connections between the Fourier transform and the Laplacian. The Laplacian \(\Delta f\) is the divergence of the gradient of \(f\),

\[\Delta f\coloneqq\nabla\cdot\nabla f.\] (12)

It turns out that the Fourier basis function \(e^{i2\pi\xi x}\) is the _generalised eigenfunction_ of \(\Delta f\) associated to the eigenvalue \(\lambda=-(2\pi)^{2}|\xi|^{2}\). We note that small eigenvalues correspond to small frequencies, and large eigenvalues to large frequencies. Consequently, the eigenvalues of the Laplacian are good surrogates for the frequencies.

### Graph wavelet transform

The notion of Fourier transform can be adapted to graphs. Let \(\bm{x}\in\mathbb{R}^{N}\) be a column vector representing a discrete signal on the \(N\) vertices of a graph. The eigendecomposition of the _graph Laplacian_ is given by

\[\bm{L}=\bm{U}\bm{\Lambda}\bm{U}^{\top}.\] (13)Similar to the continuous case, the eigenvector matrix \(\bm{U}\) is referred to as the _graph Fourier basis_, and we consider the eigenvalues of \(\bm{L}\) as the _graph frequencies_.

The _graph Fourier transform_\(\bm{U}^{T}\) performs a projection of the signal onto the graph Fourier basis, thus producing a finite number of _Fourier coefficients_

\[\hat{\bm{x}}\coloneqq\bm{U}^{\top}\bm{x}\in\mathbb{R}^{N}.\] (14)

We see that the Fourier coefficients belong to the eigenspace of the graph Laplacian. They provide _complete localisation in terms of frequency_, meaning that every frequency is represented and its contribution to the original signal can be identified, the \(n\)-th component of \(\hat{\bm{x}}\) is associated to the \(n\)-th frequency. By contrast, the original signal offers _complete resolution in space_ (or _time_), making it possible to determine how the signal varies across the vertices. In other words, the \(n\)-th component of \(\bm{x}\) is associated to the \(n\)-th vertex. However, it is often advantageous not to be limited to just one or the other. The _spectral graph wavelet transform_ offers _multi-scale resolution_, allowing a more balanced analysis between the spatial and frequency domains. Instead of sinusoidal plane waves, it employs the more general notion of _wavelets_. A wavelet can be scaled and translated, meaning that it can be tuned to capture localised changes in space. By contrast, a sinusoidal plane wave is uniform and extend across the entire signal.

The idea behind wavelet transform is to derive the _wavelet coefficients_ from the Fourier coefficients by applying a _wavelet filter_\(w:\mathbb{R}\to\mathbb{R}\) on the eigenvalues of the graph Laplacian and then performing the inverse Fourier transform,

\[\hat{\bm{x}}\coloneqq\bm{U}w(\bm{\Lambda})\bm{U}^{\top}\bm{x}.\] (15)

A wavelet filter is a combination of a _scaling function_\(a:\mathbb{R}\to\mathbb{R}\) at the scale \(\alpha\) and a _wavelet function_\(b:\mathbb{R}\to\mathbb{R}\) at \(L\) different scales \(\beta_{l}\),

\[w(\lambda)\coloneqq a(\alpha\lambda)+\sum_{l=1}^{L}b(\beta_{l}\lambda),\qquad \Theta\coloneqq\{\alpha,\beta_{1},\dots,\beta_{L}\}.\] (16)

The wavelet function at each scale represents a scaled and translated variant of a _mother wavelet_. The role of the wavelet function is to serve as a _band-pass filter_, covering medium and high frequencies. The scaling function is a _low-pass filter_, capturing low frequencies.

The reader is referred to Hammond et al. [8] for a more detailed description of the graph wavelet transform.

Mother wavelet.A popular choice of mother wavelets is the _Mexican hat wavelet_, also called _Ricker wavelet_. The Mexican hat wavelet is defined as the negative normalised second derivative of a Gaussian function,

\[b(\lambda)\coloneqq\frac{2}{\sqrt{3\sigma}\pi^{1/4}}\left(1-\left(\frac{ \lambda}{\sigma}\right)^{2}\right)e^{-\frac{\lambda^{2}}{2\sigma^{2}}},\quad \sigma>0.\] (17)

## Appendix C Hodgelet spectral features

Here, we provide further details about the Hodgelet spectral features introduced in Section 2.3. By the Hodge decomposition from Section 2.2 (more details in Appendix D.3), the wavelet coefficients in (5) can be decomposed into exact, co-exact and harmonic terms (note that vertex features have no exact component),

\[\hat{\bm{x}}^{(i)}_{v\!dj}=\Big{[}\hat{\bm{x}}^{(i)}_{v\!dj\,c}\,,\,\hat{\bm{ x}}^{(i)}_{v\!dj\,h}\Big{]}^{\top},\quad\hat{\bm{x}}^{(i)}_{e\!dj\,c}\,,\,\hat{\bm{ x}}^{(i)}_{e\!dj\,h}\,,\,\hat{\bm{x}}^{(i)}_{e\!dj\,h}\Big{]}^{\top},\] (18)

where \(e,c,h\) in the last entry of the subscripts indicate exact, co-exact and harmonic components, respectively. We also recall that the indices \(d,j\) correspond to the channel and filter dimensions, respectively. Concretely, the components are computed as

\[\hat{\bm{x}}^{(i)}_{v\!dj\,\bullet}=\bm{U}^{(i)}_{v\bullet}w_{v\!j\bullet} \big{(}\bm{\Lambda}^{(i)}_{v\bullet}\big{)}\bm{U}^{(i)\top}_{v\bullet}\bm{x}^{ (i)}_{v\!d},\quad\hat{\bm{x}}^{(i)}_{e\!dj\,\bullet}=\bm{U}^{(i)}_{c\bullet}w_ {c\!j\bullet}\big{(}\bm{\Lambda}^{(i)}_{c\bullet}\big{)}\bm{U}^{(i)\top}_{c \bullet}\bm{x}^{(i)}_{ed},\] (19)

which follow from the orthogonality of the decomposition.

Next, we compute the Hodgelet spectral features \(\bm{v}_{\bullet}^{(i)}\in\mathbb{R}^{D_{c}W_{v}}\), \(\bm{e}_{\bullet}^{(i)}\in\mathbb{R}^{D_{c}W_{c}}\), which are defined by a concatenation of the \(2\)-norms, i.e. \(\|\bm{x}\|_{2}^{2}:=x_{1}^{2}+x_{N}^{2}\),

\[\big{(}\bm{v}_{\bullet}^{(i)}\big{)}_{dj}=\big{\|}\bm{x}_{vdj\bullet}^{(i)} \big{\|}_{2},\quad\big{(}\bm{e}_{\bullet}^{(i)}\big{)}_{dj}=\big{\|}\bm{x}_{edj \bullet}^{(i)}\big{\|}_{2},\] (20)

where the column vectors \(\bm{v}_{\bullet}^{(i)}\), \(\bm{e}_{\bullet}^{(i)}\) are indexed by \((d,j)\in\{1,\ldots,D_{\bullet}\}\times\{1,\ldots,W_{\bullet}\}\). We note that the features (20) are invariant under graph isomorphism, due to the following argument.

Permutation invariance.Let us consider the vertex features for a graph \(\mathcal{G}^{(i)}\) and consider a graph isomorphism \(\varphi:\mathcal{V}^{(i)}\rightarrow\mathcal{V}^{(i)^{\prime}}\), where \(\mathcal{G}^{(i)^{\prime}}=(\mathcal{V}^{(i)^{\prime}},\mathcal{E}^{(i)^{ \prime}})\cong\mathcal{G}^{(i)}\). In vector representation (i.e., considering \(\mathcal{V}^{(i)}\cong\mathbb{R}^{N_{v}^{(i)}}\) and \(\mathcal{V}^{(i)^{\prime}}\cong\mathbb{R}^{N_{v}^{(i)}}\) by introducing an ordering on the vertices), the isomorphism is defined via a permutation matrix \(\bm{P}^{(i)}\in\mathbb{R}^{N_{v}^{(i)}\times N_{v}^{(i)}}\), which is orthogonal, that is \(\bm{P}^{(i)\top}\bm{P}^{(i)}=\bm{I}\), and therefore norm-preserving. According to this permutation, one can check the following transformations,

\[\bm{x}_{vd}^{(i)}\overset{\varphi}{\mapsto}\bm{P}^{(i)}\bm{x}_{vd }^{(i)},\] (21) \[\bm{U}_{v}^{(i)}\overset{\varphi}{\mapsto}\bm{P}^{(i)}\bm{U}_{v \bullet}^{(i)\top}\bm{P}^{(i)\top},\] (22) \[\bm{\Lambda}_{v\bullet}^{(i)}\overset{\varphi}{\mapsto}\bm{P}^{ (i)}\bm{\Lambda}_{v\bullet}^{(i)}\bm{P}^{(i)\top}.\] (23)

The last line also implies that \(w_{vj\bullet}(\bm{\Lambda}_{v\bullet}^{(i)})\overset{\varphi}{\mapsto}\bm{P} ^{(i)}w_{vj\bullet}(\bm{\Lambda}_{v\bullet}^{(i)})\bm{P}^{(i)\top}\), since \(w_{vj\bullet}\) is given as a component-wise function. Then, we get

\[\big{(}\bm{v}_{\bullet}^{(i)}\big{)}_{dj}=\big{\|}\bm{x}_{vdj \bullet}^{(i)}\big{\|}_{2}=\big{\|}\bm{U}_{v\bullet}^{(i)}w_{vj\bullet}\big{(} \bm{\Lambda}_{v\bullet}^{(i)}\big{)}\bm{U}_{v\bullet}^{(i)\top}\bm{x}_{vd}^{( i)}\big{\|}_{2}\] (24) \[\overset{\varphi}{\mapsto}\big{\|}\bm{P}^{(i)}\bm{U}_{v\bullet}^ {(i)}\bm{P}^{(i)\top}\big{(}\bm{P}^{(i)}w_{vj\bullet}\big{(}\bm{\Lambda}_{v \bullet}^{(i)}\big{)}\bm{P}^{(i)\top}\big{)}\big{(}\bm{P}^{(i)}\bm{U}_{v\bullet }^{(i)\top}\bm{P}^{(i)\top}\big{)}\bm{P}^{(i)}\bm{x}_{vd}^{(i)}\big{\|}_{2}\] (25) \[=\big{\|}\bm{P}^{(i)}\bm{U}_{v\bullet}^{(i)}\underbrace{\big{(}\bm {P}^{(i)\top}\bm{P}^{(i)}\bm{P}^{(i)}\big{)}}_{=I}\big{\|}\bm{v}_{vj\bullet} \big{(}\bm{\Lambda}_{v\bullet}^{(i)}\big{)}\underbrace{\big{(}\bm{P}^{(i)\top }\bm{P}^{(i)}\big{)}}_{=I}\bm{U}_{v\bullet}^{(i)}\underbrace{\big{(}\bm{P}^{(i )\top}\bm{P}^{(i)}\big{)}}_{=I}\bm{x}_{vd}^{(i)}\big{\|}_{2}\] (26) \[=\big{\|}\bm{P}^{(i)}\bm{U}_{v\bullet}^{(i)}w_{vj\bullet}\big{(} \bm{\Lambda}_{v\bullet}^{(i)}\big{)}\bm{U}_{v\bullet}^{(i)}\bm{x}_{vd}^{(i)} \big{\|}_{2}\] (27) \[=\big{\|}\bm{U}_{v\bullet}^{(i)}w_{vj\bullet}\big{(}\bm{\Lambda}_{ v\bullet}^{(i)}\big{)}\bm{U}_{v\bullet}^{(i)}\bm{x}_{vd}^{(i)}\big{\|}_{2}\] (28) \[=\big{(}\bm{v}_{\bullet}^{(i)}\big{)}_{dj},\] (29)

where we used that \(\bm{P}^{(i)}\) is norm-preserving to go from (27) to (28). Hence, the vertex spectral features \(\bm{v}_{\bullet}^{(i)}\) are invariant under graph isomorphism and by the same argument, one can show that the edge spectral features \(\bm{e}_{\bullet}^{(i)}\) are also invariant. Altogether, the Hodgelet spectral features (20) are well-defined features for graph inputs.

## Appendix D Extension to Higher-Order Networks

Graphs can support signals on their vertices and edges, resulting in a richer structure than tabular data. Common examples are social networks, electrical grids, citation networks, traffic maps, chemical reaction networks, collaboration networks, and molecules. However, graphs do have an important limitation: a graph only permits _dyadic interactions_ between its vertices via its edges. Multiple recent works have tackle this issue by focusing on more general structures, such as _simplicial complexes_, _cellular complexes_ and _hypergraphs_[30, 22, 18, 19, 16, 15, 24, 23, 31]. For a comparative illustration between a graph, simplicial complex and a cellular complex, we refer to Figure D.1.

### Simplicial complexes

Simplicial complexes represent a natural extension of graphs, and generalise the notion of vertices and edges by using _simplices_ or _simplex_ in the singular. In this broader setting, vertices are referred to as \(0\)-simplices, and edges as \(1\)-simplices. A \(2\)-simplex is a triangle formed by three adjacent vertices. We observe that this describes a _triadic interaction_ between vertices, in other words, a relation between three vertices. We can create simplices of arbitrary dimension \(k\geq 0\). However,the first three dimensions are often sufficient. We define a simplicial complex \(\mathcal{S}=(\mathcal{V},\mathcal{E},\mathcal{T})\) has a structure constructed from a vertex set \(\mathcal{V}\), an edge set \(\mathcal{E}\), and a triangle set \(\mathcal{T}\). We say that \(\mathcal{S}\) is a simplicial \(2\)-complex or a simplicial complex of dimension \(2\). Finally, the number of vertices, edges, and triangles are denoted by \(N_{v},N_{e}\) and \(N_{t}\), respectively.

Orientation.So far, our simplicial complexes are _undirected_. However, we can give an _orientation_ to a simplicial complex (see Figure 2). An oriented simplicial \(2\)-complex has an orientation on its edges and triangles, requiring an ordering to be imposed on its vertices. For an edge \(e=\{v_{1},v_{2}\}\), an orientation means assigning a direction, transforming \(e\) into either \((v_{1},v_{2})\) or \((v_{2},v_{1})\). We notice that an edge always has exactly two possible orientations. A triangle \(t=\{v_{1},v_{2},v_{3}\}\) similarly also has two possible orientations, corresponding to the parity of permutations of \(\{v_{1},v_{2},v_{3}\}\). The parity defines two equivalence classes

\[[(v_{1},v_{2},v_{3})] =\{(v_{1},v_{2},v_{3}),(v_{2},v_{3},v_{1}),(v_{3},v_{1},v_{2})\},\] (30) \[=\{(v_{3},v_{2},v_{1}),(v_{2},v_{1},v_{3}),(v_{1},v_{3},v_{2})\},\] (31)

which, at an intuitive level, corresponds to orientations that are either _clockwise_ or _anti-clockwise_. We point out that although an orientation is necessary to express concepts such as edge flows, the choice of orientation is arbitrary. Fixing an orientation is akin to converting an undirected graph into a _directed_ one, although they are fundamentally different since oriented graphs do not permit two edges between the same pair of vertices, whereas directed graphs do.

Cellular complexes.A more general structure is that of cellular complexes. The treatment of cellular complexes is similar to simplicial complexes. For more details, see Alain et al. [16]. We could also easily handle hypergraphs by using a Laplacian on hypergraphs.

### Hodge Laplacians

If one takes a step back, it becomes apparent that graphs and simplicial complexes have a _chain-like_ structure: vertices are connected to edges and edges are connected to triangles. More precisely, we say that vertices have edges as their _upper neighbours_, edges have vertices as their _lower neighbours_ and triangles as their upper neighbours, and triangles have edges as their lower neighbours. This leads us to the definition of two incidence matrices, \(\bm{B}_{ve}\in\mathbb{Z}^{N_{v}\times N_{e}}\) and \(\bm{B}_{et}\in\mathbb{Z}^{N_{e}\times N_{t}}\), sometimes called vertex-to-edge and edge-to-triangle incidence matrices, respectively. While \(\bm{B}_{ve}\) is the incidence matrix between vertices and edges, \(\bm{B}_{et}\) is the incidence matrix between edges and triangles. In a more general setting, these matrices are simply called _boundary matrices_ and they are denoted by \(\bm{B}_{1}\) and \(\bm{B}_{2}\), respectively, since vertices are \(1\)-simplices and edges are \(2\)-simplices. This results in the general definition of a boundary matrix \(\bm{B}_{k}\in\mathbb{Z}^{N_{h}\times N_{k+1}}\), where \(N_{k}\) is the number of \(k\)-simplices. Note that for a simplicial \(K\)-complex, \(\bm{B}_{0}=\bm{0}\) and \(\bm{B}_{k+1}=\bm{0}\). This simply means that \(0\)-simplices have no lower neighbours, and \(k\)-simplices have no upper neighbours. We can define a more general

Figure 1: Graph, simplicial complex, and cellular complex (specifically, a polyhedral complex). A simplicial complex cannot represent arbitrary polygons like the pentagon in (c).

notion of Laplacian, called the _Hodge Laplacian_,

\[\bm{L}_{k}:=\mathbf{B}_{k}^{\top}\mathbf{B}_{k}+\mathbf{B}_{k+1}\mathbf{B}_{k+1}^ {\top}\in\mathbb{Z}^{N_{k}\times N_{k}}.\] (32)

The component \(\mathbf{B}_{k}^{\top}\mathbf{B}_{k}\) is sometimes called the _lower Laplacian_, and \(\mathbf{B}_{k+1}\mathbf{B}_{k+1}^{\top}\), the _upper Laplacian_. We observe in particular that the graph Laplacian is nothing other than \(\bm{L}_{0}\), and the graph Helmholtz is \(\bm{L}_{1}\).

Interested readers are invited to find out more by looking at the concept of _chain complex_.

Cliques.A \(k\)-clique is a subset of \(k\) vertices, such that every \(k\) distinct vertices in the clique are adjacent. Intuitively, \(2\)-cliques are like considering triangles. Edges have no upper neighbours in simplicial \(1\)-complexes, i.e. graphs. Hence, the edge-to-triangle incidence matrix \(\bm{B}_{2}=\bm{0}\). In turn, this implies that the graph Helmholtz is equal to its lower Laplacian, i.e. \(\bm{L}_{1}=\bm{B}_{1}^{\top}\bm{B}_{1}\). Nevertheless, we can consider the triangles of a graph in order to have a non-zero \(\bm{B}_{2}\) (see Figure D.2). We can then have a complete graph Helmholtz, i.e. \(\bm{L}_{1}=\bm{B}_{1}^{\top}\bm{B}_{1}+\bm{B}_{2}\bm{B}_{2}^{\top}\).

### Hodge decomposition

The _Helmholtz decomposition_ is often called the fundamental theorem of vector calculus. This famous theorem states that any 2D vector fields on a Euclidean domain can be expressed as the sum of two orthogonal components: (1) one that intuitively has no sinks or sources, called _divergence-free_ or _solenoidal_, and (2) one that has no vortices, sometimes called _curl-free_ or _irrotational_. This decomposition extends to differential forms on manifolds and is referred to as the _Hodge decomposition_. For Riemannian manifolds, we can identify differential \(1\)-forms with vector fields via the musical isomorphism. This implies that Hodge decomposition can be interpreted as a decomposition of vector fields on Riemannian manifolds. The Hodge decomposition on the \(k\)-simplex feature space \(\mathbb{R}^{N_{k}}\) (for a single channel) leads to the sum of three orthogonal components: _exact_, _co-exact_, and _harmonic_,

\[\mathbb{R}^{N_{k}}=\text{im}(\bm{B}_{k+1})\oplus\text{im}(\bm{B}_{k}^{\top}) \oplus\text{ker}(\bm{L}_{k}),\] (33)

where \(\text{im}(\bm{B}_{k+1})\) is the _exact subspace_, \(\text{im}(\bm{B}_{k}^{\top})\) is the _co-exact subspace_, and \(\text{ker}(\bm{L}_{k})\) is the _harmonic subspace_. Interestingly, we note that the dimension of \(\text{ker}(\bm{L}_{k})\) is equal to the \(k\)-th _Betti number_, which describe the number of \(k\)-dimensional holes. The Hodge decomposition implies the eigendecomposition of the Hodge Laplacian,

\[\bm{L}_{k}=\begin{pmatrix}\bm{U}_{ke}\\ \bm{U}_{ke}\\ \bm{U}_{kh}\end{pmatrix}\begin{pmatrix}\bm{\Lambda}_{ke}&\bm{0}&\bm{0}\\ \bm{0}&\bm{\Lambda}_{ke}&\bm{0}\\ \bm{0}&\bm{0}&\bm{\Lambda}_{kh}\end{pmatrix}\begin{pmatrix}\bm{U}_{ke}\\ \bm{U}_{ke}\\ \bm{U}_{kh}\end{pmatrix}^{\top},\] (34)

where \((\bm{\Lambda}_{ke},\bm{U}_{ke})\) are the non-zero eigenvalues and eigenvectors of \(\bm{B}_{k}^{\top}\bm{B}_{k}\), \((\bm{\Lambda}_{ke},\bm{U}_{ke})\) are the non-zero eigenvalues and eigenvectors of \(\bm{B}_{k+1}\bm{B}_{k+1}^{\top}\), and \((\bm{\Lambda}_{kh},\bm{U}_{kh})\) are the zero eigenvalues and eigenvectors of \(\bm{L}_{k}\). We observe that the _exact eigenbasis_\(\bm{U}_{0e}=\bm{0}\) as \(\bm{B}_{0}=\bm{0}\). Similarly, _co-exact eigenbasis_\(\bm{U}_{k+1e}=\bm{0}\) since \(\bm{B}_{k+1}=\bm{0}\). We also have that \(\bm{U}_{ke}\) spans \(\text{im}(\bm{B}_{k+1})\), \(\bm{U}_{ke}\) spans \(\text{im}(\bm{B}_{k}^{\top})\), and \(\bm{U}_{kh}\) spans \(\text{ker}(\bm{L}_{k})\).

Figure D.2: A graph is augmented by taking its \(2\)-cliques and then assigned an orientation.

Hodgelet kernel.In the same spirit as in Section 2.1, we compute the wavelet transform on the \(k\)-simplex features and then obtain the wavelet spectral features. Finally, we present the _Hodgelet kernel_ on simplicial \(K\)-complexes,

\[\kappa\big{(}\mathcal{S}^{(i)},\mathcal{S}^{(j)}\big{)}\coloneqq \sum_{k=1}^{K}\left(\kappa_{ke}\big{(}\bm{s}_{ke}^{(i)},\bm{s}_{ke}^{(j)}\big{)} +\kappa_{kc}\big{(}\bm{s}_{kc}^{(i)},\bm{s}_{ke}^{(j)}\big{)}+\kappa_{kh}\big{(} \bm{s}_{kh}^{(i)},\bm{s}_{kh}^{(j)}\big{)}\right),\] (35)

where \(\bm{s}_{k\bullet}^{(i)}\) for \(\bullet\in\{e,c,h\}\) are spectral features corresponding to the \(k\)-simplices. This is defined in an analogous way to the construction detailed in Appendix C.

## Appendix E Experiment Details

### Graph Classification Benchmarks

We refer to Opolka et al. [5] for details on all the datasets used in this section.

### Vector Field Classification Details

We provide some details about the vector field classification experiment in Section 3. To generate a random vector field, we first sample a Gaussian process \(f:\Omega\times\mathbb{R}^{2}\to\mathbb{R}^{2}\), and then take its derivatives \(f_{x},f_{y}\) in both spatial components. Noting that a curl-free 2D vector field is always the gradient of a potential, we can sample an arbitrary _curl-free field_ by taking

\[\bm{X}_{\text{curl-free}}\coloneqq\nabla f=[f_{x},f_{y}]^{\top}.\] (36)

Similarly, since a _divergence-free_ 2D vector field is always a _Hamiltonian vector field_, we can choose

\[\bm{X}_{\text{div-free}}\coloneqq\nabla^{\perp}f=[f_{y},-f_{x}]^{\top},\] (37)

in order to sample an arbitrary divergence-free field.

For the vector field \(f\), we used samples of the squared-exponential Gaussian process, sampled using its random feature approximation [32]. We display this in Figure E.1, where we plot the derivatives \(f_{x},f_{y}\), and their combinations to yield divergence-free and curl-free fields.

Our data consist of _mostly divergence-free and curl-free_ vector fields, which are generated by considering the linear combination

\[\bm{X}\coloneqq\lambda\bm{X}_{\text{div-free}}+(1-\lambda)\bm{X}_{\text{curl- free}}+R\epsilon,\] (38)

where \(\lambda\sim U([0.1,0.9])\), \(R>0\) is the noise level, \(\epsilon\sim\mathcal{N}(0,1)\), and \(\bm{X}_{\text{div-free}},\bm{X}_{\text{curl-free}}\) generated randomly. If \(\lambda<0.5\), we say that the vector field \(\bm{X}\) is mostly curl-free, and if \(\lambda>0.5\), we say that it is mostly divergence-free.

Projecting to a simplicial complex.To project a vector field \(\bm{X}:\mathbb{R}^{2}\to\mathbb{R}\) onto the edges of a graph, we employ the _de Rham map_[33], which "discretises" a vector field into edge signals. For a given oriented edge \(e=(v_{0},v_{1})\in\mathcal{E}\) with endpoint coordinates \(\bm{x}_{0}\) and \(\bm{x}_{1}\), we define the projection \(X^{e}\) of \(\bm{X}\) onto \(e\) by the following integral,

\[X^{e}\coloneqq\int_{0}^{1}\bm{X}\big{(}\bm{x}_{0}+t(\bm{x}_{1}- \bm{x}_{0})\big{)}\cdot\hat{\bm{t}}\,\mathrm{d}t,\] (39)

where \(\hat{\bm{t}}:=\frac{\bm{x}_{1}-\bm{x}_{0}}{|\bm{x}_{1}-\bm{x}_{0}|}\) is the unit tangent vector along the edge. Numerically, this can be computed efficiently using numerical quadratures, owing to the fact that the integral is only defined over the interval \([0,1]\). Note that this depends on the ordering of the vertices \(v_{0}\) and \(v_{1}\) characterising the edge - if we flip the order, then the sign of \(X^{e}\) flips. Thus, we require the graph to be oriented in order for the projections \(\{X^{e}\}_{e\in\mathcal{E}}\) to be well-defined. In Figure E.2, we display an example of such a projection onto a regular triangular mesh.

Figure E.1: Illustration of the random vector field data generating process.

Figure E.2: Projection of a continuous divergence-free field onto a regular triangular mesh.