# Investigating Annotator Bias in Large Language Models for Hate Speech Detection

Amit Das1, Zheng Zhang2, Najib Hasan3, Souvika Sarkar3, Fatemeh Jamshidi4, Tathagata Bhattacharya5, Mostafa Rahgouy6, Nilanjana Raychawdhary6, Dongji Feng7, Vinija Jain8,9, Aman Chadha8,9, Mary Sandage6, Lauramarie Pope6, Gerry Dozier6, Cheryl Seals6

1University of North Alabama, 2Murray State University, 3Wichita State University,

4California State Polytechnic University Pomona, 5Auburn University at Montgomery,

5Auburn University, 7Gustavus Adolphus College, 8Stanford University, 9Amazon GenAI

###### Abstract

Data annotation, the practice of assigning descriptive labels to raw data, is pivotal in optimizing the performance of machine learning models. However, it is a resource-intensive process susceptible to biases introduced by annotators. The emergence of sophisticated Large Language Models (LLMs) presents a unique opportunity to modernize and streamline this complex procedure. While existing research extensively evaluates the efficacy of LLMs, as annotators, this paper delves into the biases present in LLMs when annotating hate speech data. Our research contributes to understanding biases in four key categories: gender, race, religion, and disability with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and German-2. Specifically targeting highly vulnerable groups within these categories, we analyze annotator biases. Furthermore, we conduct a comprehensive examination of potential factors contributing to these biases by scrutinizing the annotated data. We introduce our custom hate speech detection dataset, _HateBiasNet_, to conduct this research. Additionally, we perform the same experiments on the ETHOS Mollas et al. (2022) dataset also for comparative analysis. This paper serves as a crucial resource, guiding researchers and practitioners in harnessing the potential of LLMs for data annotation, thereby fostering advancements in this critical field. The _HateBiasNet_ dataset is available here: https://github.com/AmitDasRup123/HateBiasNet

**Content Warning:** This article features hate speech examples that may be disturbing to some readers.

## 1 Introduction

The growing widespread presence of online hate speech presents a critical challenge for maintaining safe and inclusive digital environments. Automated hate speech detection systems, powered by machine learning and large language models (LLMs), have emerged as vital tools to address this issue (Tan et al., 2024). These systems rely heavily on annotated datasets to train and evaluate their performance. However, the process of annotating hate speech is inherently subjective and influenced by the annotators' sociocultural, and personal biases. As a result, these biases can inadvertently be embedded into the datasets and, subsequently, into the detection models, raising concerns about fairness, accuracy, and generalizability. Despite the advancements in LLMs and their capabilities, annotator bias remains an underexplored yet significant factor affecting their performance and reliability in hate speech detection tasks.

LLMs offer a promising pathway toward transforming data annotation practices. Their ability to automate annotation tasks, ensure consistency across vast datasets, and adapt through fine-tuning or prompts tailored to specific domains significantly alleviates challenges inherent in traditionalannotation methodologies, thereby establishing a new standard for achievable outcomes in the realm of NLP.

However, data annotation with humans comes with the risk of annotator biases, both conscious and unconscious, that can significantly impact the downstream applications of AI systems. In this paper, we primarily focus on biases in LLMs for hate speech data annotation. Our paper explores four categories: gender, racial, religious, and disability-based bias. Specifically, we select the target groups that are highly vulnerable within the mentioned four categories and explore the annotator biases. Additionally, we provide a detailed analysis of the possible reasons for these biases by exploring the data being annotated. We compare the results across four LLMs: GPT-3.5, GPT-40, Llama-3.1, and Gemm-2. We also explore why certain biases achieve higher accuracy compared to others.

Understanding the implications of these biases is crucial, as they can perpetuate harmful stereotypes and reinforce discrimination in automated systems. By systematically analyzing the biases present in LLMs, we aim to illuminate the mechanisms through which these biases emerge and manifest in the annotations they produce. This investigation not only highlights the ethical considerations involved in utilizing LLMs for sensitive tasks like hate speech detection but also provides insights for improving model training and annotation strategies to foster more equitable AI outcomes.

Moreover, the stakes are particularly high in the realm of hate speech detection, where the potential for mislabeling or biased labeling can lead to severe consequences, including the marginalization of vulnerable communities and the unjust suppression of free speech. As LLMs become increasingly integrated into content moderation systems, it is essential to ensure that the annotations they produce are not only accurate but also reflective of a fair and balanced perspective. By addressing these biases, we can pave the way for the development of more robust, transparent, and socially responsible AI systems, ultimately contributing to a safer online environment for all users. Serving as a critical guide, this paper aims to steer researchers toward exploring the potential of LLMs for data annotation, thereby facilitating future advancements in this essential domain.

Through rigorous data annotation, prompt engineering, quantitative and qualitative analysis, we aim to answer the following research questions:

**RQ1: Does annotator bias exist in Large Language Models for hate speech detection?**

**RQ2: If it exists, what potential factors contribute to its existence?**

Figure 1: Workflow diagram of our study, illustrating how varying biases can lead to different outcomes when annotating a sample text as hateful. We investigate annotator biases across four categories for hate speech detection using the following LLMs: GPT-3.5, GPT-40, Llama-3.1, and Gemm-2.

### RQ3: How can this problem be effectively mitigated?

To this end, our work makes the following contributions:

[leftmargin=*]
**(The Contributions)**

* Our research demonstrates that annotator bias is present in LLMs used for hate speech detection. This bias arises from the subjective interpretations of annotators, which influence the training data and consequently affect the model's performance. We provide empirical evidence illustrating how such biases skew detection results, leading to potential inaccuracies and unfair outcomes.
* In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts.
* We delve into the underlying factors contributing to bias and propose a potential solution to address this issue. We analyze various aspects to uncover the root causes of bias and present a strategy aimed at mitigating its effects. Through our investigation, we aim to provide valuable insights into understanding and combatting bias in our study.

## 2 Related Work

The advent of LLMs has revolutionized NLP tasks by enabling the development of more sophisticated and context-aware language understanding systems. Models such as BERT (Devlin et al., 2018), and their variants have demonstrated remarkable performance across a wide range of NLP tasks, including text classification, language generation, and question answering. These models leverage pre-training on large corpora followed by fine-tuning on task-specific data, allowing them to capture intricate linguistic patterns and semantic relationships.

Recent research has explored the use of LLMs for data annotation tasks, leveraging their ability to comprehend and generate human-like text. For instance, (Gururangan et al., 2020) proposed a framework for generating natural language explanations for machine learning models, facilitating the annotation of model predictions with interpretable justifications. Similarly, (Raffel et al., 2020) introduced a method for efficiently annotating speech data using GPT-2, demonstrating significant reductions in annotation time compared to traditional manual labeling approaches.

The increasing interest in leveraging Large Language Models as versatile annotators for various natural language tasks has been highlighted in recent research (Kuzman et al., 2023; Zhu et al., 2023; Ziems et al., 2024). (Wang et al., 2021) demonstrated that GPT-3 can significantly decrease labeling costs by up to 96% for both classification and generation tasks. Similarly, (Ding et al., 2023) conducted an assessment of GPT-3's effectiveness in labeling and data augmentation across classification and token-level tasks. Furthermore, empirical evidence suggests that LLMs can surpass crowdsourced annotators in certain classification tasks (Gilardi et al., 2023; He et al., 2023).

The investigation of social biases within Natural Language Processing (NLP) models constitutes a significant area of research. Previous studies have delineated two primary categories of biases and harms: allocational harms and representational harms (Blodgett et al., 2020; Crawford, 2017). Scholars have explored various methodologies to assess and alleviate these biases in both Natural Language Understanding (NLU) (Bolukbasi et al., 2016; Dixon et al., 2018; Zhao et al., 2018; Bordia and Bowman, 2019; Dev et al., 2021; Sun and Peng, 2021) and Natural Language Generation (NLG) tasks (Sheng et al., 2019; Dinan et al., 2019).

Within this body of literature, (Sun and Peng, 2021) proposed utilizing the Odds Ratio (OR) (Szumilas, 2010) as a metric to quantify gender biases, particularly in items exhibiting significant frequency disparities or high salience among genders. (Sheng et al., 2019) assessed biases in NLG model outputs conditioned on specific contextual cues, while (Dhamala et al., 2021) extended this analysis by incorporating real-world prompts extracted from Wikipedia. Several strategies (Sheng et al., 2020; Liu et al., 2021; Cao et al., 2022; Gupta et al., 2022) have been proposed to mitigate biases in NLG models, yet their applicability to closed API-based LLMs, such as ChatGPT, remains uncertain.

Methodologies

### Data Collection and Annotation

The study initiates with the utilization of a hate speech lexicon sourced from Hatebase.org1, comprising terms and expressions identified by online users as indicative of hate speech. Leveraging the Twitter API, we conducted a search for tweets containing lexicon terms, resulting in a corpus of 3003 tweets. Subsequently, three speech-language pathology graduate students were engaged for the purpose of data annotation. These annotators were tasked with categorizing each tweet into one of two classifications: hateful or not hateful. We name this dataset as _HateBiasNet_.

Footnote 1: https://hatebase.org/

Acknowledging the inherent vagueness in prior methodologies for annotating hate speech, as noted by (Schmidt & Wiegand, 2017), which often led to low agreement scores, our study took measures to enhance the clarity and consistency of the an- notation process. To achieve this, all annotators collaboratively formulated and refined annotation guidelines to ensure a shared understanding of hate speech. An explicit definition, accompanied by a detailed explanation, was provided to elucidate the concept further.

Annotators were instructed to consider not only the isolated words within a tweet but also the broader contextual usage of these terms. Emphasis was placed on discerning the intent behind the lan- guage and recognizing that the mere presence of offensive vocabulary did not inherently classify a tweet as hate speech. Each tweet underwent coding by three independent annotators, and the majority decision among them was employed to assign the final label. The annotation details are provided in the appendix.

### Data Annotation by LLMs

We then had our data annotated by the four LLMs. For the annotation, we first provided the annotator details, using direct prompt provided by (Das et al., 2024) for the annotation. One such prompt with the annotator being 'Female' is as follows. Note that [Text] refers to the input text to be annotated.

### Annotator Biases

We used only the highly vulnerable groups on social media and used them as annotators. With expert opinions, we selected six groups from four categories that face the most hateful comments on social media. We then explore the annotator bias in LLM annotation assuming the one annotator to be from one of the six categories, and one annotator not from that category. Figure 1 depicts the workflow diagram of our work. The annotator biases we explored are given in Table 1.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Category** & **Annotator Bias** \\ \hline Gender & Female vs. Not Female \\ \hline Race & Asian vs. Not Asian \\ Race & Black vs. Not Black \\ \hline Religion & Muslim vs. Not Muslim \\ \hline Disability & Mental Disability vs. No Disability \\ Disability & Physical Disability vs. No Disability \\ \hline \hline \end{tabular}
\end{table}
Table 1: Annotator Biases in LLMs explored in this paper. With expert opinions, we selected six groups from four categories that face the most hateful comments on social media. We then explore the annotator bias in LLM annotation assuming the one annotator to be from one of the six categories, and one annotator not from that category.

Results & Discussion

Along with _HateBiasNet_, we explored the same annotator bias on ETHOS (Mollas et al., 2022) dataset. We re-annotated the whole dataset using the four LLMs with the same experimental setup we used for annotating _HateBiasNet_. The analysis of data annotations by the LLMs revealed notable biases on both the datasets across each category. We observed a significant skew in the distribution of annotations towards the categories we used. Table 2 shows the mismatches between different annotator biases both on _HateBiasNet_ and ETHOS dataset while annotating them with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and Genma-2. It is observed that there are significant mismatches in both _HateBiasNet_ and the ETHOS dataset. These findings underscore the presence of subjectivity and ambiguity in the LLM-based annotation process, highlighting the need for standardized guidelines and rigorous quality control measures.

### GPT-3.5

In the context of GPT-3.5, annotator bias arises due to its pretraining on large, publicly available datasets, which embed societal biases, particularly in sensitive areas like hate speech. Since GPT-3.5 lacks alignment optimizations present in later models, it struggles with the nuanced classification of hate speech, leading to inconsistent annotations based on cultural and contextual variations.

An analysis of GPT-3.5 reveals significant differences in how annotators classify offensive language, influenced by their personal and cultural backgrounds. For example, Annotator 1 (Black) displayed leniency towards culturally contextual or ironic language, labeling statements like "The superior race but cannot stand in the sun without literally burning" as 'Not Hateful,' while Annotator 2 (Not Black) labeled the same statements as 'Hateful'. These discrepancies also extended to gender, sexuality, and disability-related content, where Annotator 1 (Female, Muslim, disabled) and Annotator 2 (Not Female, Not Muslim, no disability) showed different sensitivities. Annotator 1 was more tolerant of remarks related to gender and sexuality but more sensitive to offensive terms related to disability, whereas Annotator 2 exhibited opposite tendencies.

These findings highlight how annotators' personal identities, including race, religion, gender, and disability status, influence their interpretation of hate speech, leading to inconsistent classifications in GPT-3.5's output.

### GPT-4o:

In analyzing GPT-4o's performance in hate speech detection, notable biases emerge despite improvements in its alignment and fine-tuning. These biases are linked to the model's broad training data and an overcompensation for ethical considerations, leading to inconsistencies in identifying offensive content, particularly in borderline or culturally specific cases.

The study reveals demographic and cultural disparities in hate speech annotation. For example, Asians consistently label derogatory terms such as 'cripple' or'retard' as hateful due to their cultural and historical context, while non-Asians show more variability in their classifications. Significant differences also arise in how statements about black individuals, nationalities, disabilities, and religions are perceived. For instance, a Black annotator might classify a statement like "black Americans are more in danger than homosexuals" as hateful, while another annotator may not.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**An annotator Bias** & **Mismatch (\%)** **for GPT-3.5** & **Mismatch (\%)** **for GPT-4o** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** \\  & **annotation** & **annotation** & **annotation** & **Llama-3.1 annotation** & **Geman-2 annotation** \\ \hline \multirow{2}{*}{_HateBiasNet_} & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ \\  & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c c c c c} \hline \hline
**An annotator Bias** & **Mismatch (\%)** **for GPT-3.5** & **Mismatch (\%)** **for GPT-4o** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** \\  & **annotation** & **annotation** & **Location-3.1 annotation** & **Geman-2 annotation** & **Location-3.1 annotation** & **Geman-2 annotation** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mismatches between different annotations when annotated by LLMs. It can be seen that for the ETHOS dataset, the biases are significantly reduced for GPT-4o annotation when compared to GPT-3.5, Llama-3.1 and Genma-2 annotation.

[MISSING_PAGE_FAIL:6]

Figure 3: Heatmap of the ETHOS dataset depicting the accuracy of 11 biases across 4 LLMs. The bias ‘Black’ achieved the highest accuracy for both GPT-3.5 and GPT-4o, while ‘Asian’ exhibited the highest accuracy for Llama-3.1. The word cloud of the dataset (Figure 4) suggests that specific keywords may influence annotation results for these LLMs. Notably, GPT-4o and Llama-3.1 consistently outperformed GPT-3.5 and Gemma-2 across all biases. Among the LLMs, GPT-4o’s performance on the ‘Black’ bias stands out as the highest overall accuracy.

Figure 2: Heatmap of the _**HateBiasNet**_ dataset illustrating the accuracy of 11 biases across 4 LLMs. Notably, GPT-3.5, GPT-4o, and Llama-3.1 demonstrate the highest accuracy for the ‘Mental disability’ bias. The word cloud of the dataset (Figure 4) suggests that specific keywords may influence annotation outcomes for these LLMs. Additionally, Llama-3.1 shows the highest accuracy overall for the ‘Mental disability’ bias among the 4 models.

We proceeded to evaluate which bias yields the highest accuracy for data annotation by comparing the annotation results with the original human annotations. The results shown in Table 3 indicate that the Mental Disability bias achieves the highest accuracy on _HateBiasNet_. Additionally, Figure 4 illustrates the word histogram of _HateBiasNet_ after the removal of stopwords. Notably, the most frequently occurring words are'retarded' and 'trailer trash', both of which are closely associated with the mental disability bias.

For the ETHOS dataset, it can be seen that using Black bias gives the best result. Figure 4 shows the word histogram of Ethos dataset after removing the stopwords. It can be seen there is a clear relation between the keywords present in the dataset and the accuracy of data annotation by a particular group. We believe, although annotator bias exists in LLMs for hate speech detection, but selecting the correct prompt for the annotation can help mitigate this problem. Figures 2 and 3 show the heatmaps of the 11 biases across the the 4 LLMs for the _HateBiasNet_ and the Ethos datasets respectively.

## 5 Conclusion

Our research highlights the presence of annotator biases in hate speech detection using both GPT-3.5 and GPT-4o, opening avenues for future investigation. One potential direction involves mitigating these biases by incorporating specific rules into the LLMs while training or prompting annotators to prevent biased outputs. Additionally, exploring broader aspects of the problem statement through enhanced language style or lexical content analyses holds promise.

The advent of LLMs like ChatGPT has introduced novel applications such as data annotation. However, our study underscores the risk of biases emerging when LLMs are directly utilized for annotation tasks. We meticulously assess four types of biases in LLM-assisted hate speech detection, revealing the propagation and amplification of harmful biases in annotations.

Our findings emphasize the need for cautious utilization of AI-assisted data annotation to counteract biases effectively. We advocate for the development of comprehensive policies governing the use of LLMs in real-world scenarios. Furthermore, we call for continued research into identifying and mitigating fairness issues in data annotation with LLMs, as understanding and addressing underlying biases are imperative for reducing potential harms in future LLM research endeavors.

Figure 4: Word histogram (considering only the top 5 words) of (a) _HateBiasNet_ and (b) ETHOS after removing the stopwords.

## References

* Blodgett et al. (2020) Su Lin Blodgett, Solon Barocas, Hal Daume III, and Hanna Wallach. Language (technology) is power: A critical survey of "bias" in NLP. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 5454-5476, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.485. URL https://aclanthology.org/2020.acl-main.485.
* Bolukbasi et al. (2016) Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. _Advances in neural information processing systems_, 29, 2016.
* Bordia and Bowman (2019) Shikha Bordia and Samuel R Bowman. Identifying and reducing gender bias in word-level language models. _arXiv preprint arXiv:1904.03035_, 2019.
* Cao et al. (2022) Yang Trista Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Varun Kumar, Jwala Dhamala, and Aram Galstyan. On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations. _arXiv preprint arXiv:2203.13928_, 2022.
* Crawford (2017) Kate Crawford. The trouble with bias. In _Conference on Neural Information Processing Systems, invited speaker_, 2017.
* Das et al. (2024) Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata Bhattacharya, Nilanjana Raychaowdhary, Mary Sandage, Laurarmarie Pope, Gerry Dozier, and Cheryl Seals. Offlandat: A community based implicit offensive language dataset generated by large language model through prompt engineering. _arXiv preprint arXiv:2403.02472_, 2024.
* Dev et al. (2021) Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jilin Kim, Akihiro Nishi, Nanyun Peng, et al. On measures of biases and harms in nlp. _arXiv preprint arXiv:2108.03362_, 2021.
* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Dhamala et al. (2021) Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pp. 862-872, 2021.
* Dinan et al. (2019) Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston. Queens are powerful too: Mitigating gender bias in dialogue generation. _arXiv preprint arXiv:1911.03842_, 2019.
* Ding et al. (2023) Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. Is GPT-3 a good data annotator? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 11173-11195, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.626. URL https://aclanthology.org/2023.acl-long.626.
* Dixon et al. (2018) Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, pp. 67-73, 2018.
* Ghosh et al. (2022) Soumitra Ghosh, Asif Ekbal, Pushpak Bhattacharyya, Tista Saha, Alka Kumar, and Shikha Srivastava. Sehc: A benchmark setup to identify online hate speech in english. _IEEE Transactions on Computational Social Systems_, 10(2):760-770, 2022.
* Gilardi et al. (2023) Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. _Proceedings of the National Academy of Sciences_, 120(30):e2305016120, 2023.
* Gilardi et al. (2018)Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, and Aram Galstyan. Mitigating gender bias in distilled language models via counterfactual role reversal. _arXiv preprint arXiv:2203.12574_, 2022.
* Gururangan et al. (2020) Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don't stop pretraining: Adapt language models to domains and tasks. _arXiv preprint arXiv:2004.10964_, 2020.
* He et al. (2023) Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen, et al. Annollm: Making large language models to be better crowdsourced annotators. _arXiv preprint arXiv:2303.16854_, 2023.
* Kuzman et al. (2023) Taja Kuzman, Igor Mozetic, and Nikola Ljubesic. Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification, 2023.
* Liu et al. (2021) Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, and Yejin Choi. Dexerts: Decoding-time controlled text generation with experts and anti-experts. _arXiv preprint arXiv:2105.03023_, 2021.
* Mollas et al. (2022) Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. Ethos: a multi-label hate speech detection dataset. _Complex & Intelligent Systems_, 8(6):4663-4678, 2022.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* Schmidt and Wiegand (2017) Anna Schmidt and Michael Wiegand. A survey on hate speech detection using natural language processing. In _Proceedings of the fifth international workshop on natural language processing for social media_, pp. 1-10, 2017.
* Sheng et al. (2019) Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. _arXiv preprint arXiv:1909.01326_, 2019.
* Sheng et al. (2020) Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. Towards controllable biases in language generation. _arXiv preprint arXiv:2005.00268_, 2020.
* Sun and Peng (2021) Jiao Sun and Nanyun Peng. Men are elected, women are married: Events gender bias on wikipedia. _arXiv preprint arXiv:2106.01601_, 2021.
* Szumilas (2010) Magdalena Szumilas. Explaining odds ratios. _Journal of the Canadian academy of child and adolescent psychiatry_, 19(3):227, 2010.
* Tan et al. (2024) Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. Large language models for data annotation: A survey. _arXiv preprint arXiv:2402.13446_, 2024.
* Wang et al. (2021) Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. Want to reduce labeling cost? GPT-3 can help. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), _Findings of the Association for Computational Linguistics: EMNLP 2021_, pp. 4195-4205, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.354. URL https://aclanthology.org/2021.findings-emnlp.354.
* Zhao et al. (2018) Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. _arXiv preprint arXiv:1804.06876_, 2018.
* Zhu et al. (2023) Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, and Gareth Tyson. Can chatgpt reproduce human-generated labels? a study of social computing tasks. _arXiv preprint arXiv:2304.10145_, 2023.
* Ziems et al. (2024) Caleb Ziems, William Held, Omar Shaikh, Jiao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? _Computational Linguistics_, pp. 1-55, 2024.
* Zhang et al. (2020)Appendix

Here we explain the details of the _HateBiasNet_ dataset creation and the results of the 4 LLMs.

### _HateBiasNet_ Details

Out of 3003 tweets, 2076 were annotated 'Hateful' and remaining 927 tweets were annotated 'Not Hateful' by the student annotators. The annotators achieved an average Cohen's Kappa score of 0.76. We paid each student above the national average per hour for doing the task.

### Student Data Annotation Instructions

1. You will be working with the 'Sample.csv' file.
2. The first column contains the tweets to be annotated. The second column is 'Label'. And the third column is for 'Category'. There is also an additional column, 'Comments', for comments.
3. You will not be changing anything in the first column. These are the data downloaded from X (Twitter).
4. After reading each tweet from the first column, you will be deciding whether a tweet is 'Hateful' or 'Not Hateful', and you will be putting it in the second column (titled 'Label') for the corresponding tweet. So basically, the second column, 'Label', will contain two options: 1. Hateful 2. Not Hateful. You can use the following definition of hate (Ghosh et al., 2022) to decide whether a tweet is hateful or not: Hate: A hate tweet contains a directed insult(s), vulgar language to denigrate a target or words that instigate or support violence. Furthermore, the simple use of offensive language such as slang and slurs on does not automatically result in a tweet of the type hate. Non-Hate: All other tweets that do not fall in the hate category are non-hate tweets.
5. If the 'Label' of a particular tweet is 'Not Hateful', leave the third column ('Category') blank.
6. To decide whether the text is hateful, check specifically two points: 1. The target of the hate and 2. The keywords used in the text.
7. There could be some non-English words in those cases to annotate by understanding the text's overall meaning.
8. The 'Sample Annotation.csv' file contains nearly 50 annotated tweets and the 'List of words.txt' file contains the list of words used to download the tweets. Please check these files before starting the annotation.
9. Lastly, you have an additional column, 'Comments' where you can add any comment if you want to. This section is totally optional.

### Keywords for Downloading Tweets

In this section, we enlist the keywords used for downloading tweets to construct _HateBiasNet_. These keywords were chosen to capture a wide range of offensive and derogatory language, enabling us to compile a comprehensive dataset for studying hate speech patterns on social media. Table 5 presents sample texts annotated by human annotators of various demographic groups and by two versions of a large language model (GPT-3.5 and GPT-4o). The table illustrates the variation and potential biases in annotations. Table 4 presents a selection of sample texts from the dataset accompanied by human-provided annotations.

[MISSING_PAGE_FAIL:12]

Figure 5: Line graph of the _HateBiasNet_ dataset displaying 11 biases across the 4 LLMs. Notably, for GPT-3.5, GPT-4o, and Llama-3.1, the ‘mental disability’ bias achieved the highest accuracy. The word cloud of the dataset (Figure 4) suggests that the presence of specific keywords may influence the annotation outcomes for these three LLMs.

Figure 6: Line graph of the Ethos dataset showing the performance of 11 biases across 4 LLMs. It is evident that the bias ‘Black’ achieved the highest accuracy for GPT-3.5 and GPT-4o, while ‘Asian’ showed the highest accuracy for Llama-3.1. The word cloud of the dataset (see Figure 4) suggests that the presence of specific keywords may influence annotation outcomes for these LLMs. Additionally, it is noticeable that GPT-4o and Llama-3.1 consistently outperformed GPT-3.5 and Gamma-2 across all biases for the Ethos dataset.

[MISSING_PAGE_FAIL:14]

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Text** & **Dataset** & **Annotator** & **Annotation** & **Annotator** & **Annotation** & **LLM** \\ \hline It’s scary how "Technical Debt Criples Companies And Threatens To Stifile Innovation" - learn more in this segment on with vFunction CEO & HateBiasNet & Physical Disability & Hateful No Disability & Not Hateful & GPT-3.5 \\ \hline You’ve wanted to visit for years gimp \& shared your deepest needs online. It’s time. Let’s get started. & HateBiasNet & Mental Disability & Not Hateful & No Disability & Hateful & GPT-3.5 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Sample Texts with annotation biases while annotating by the LLMs.