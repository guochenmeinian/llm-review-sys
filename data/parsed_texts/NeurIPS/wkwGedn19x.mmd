# Scaling White-Box Transformers for Vision

 Jinrui Yang\({}^{*1}\) Xianhang Li\({}^{*1}\) Druv Pai\({}^{2}\)

Yuyin Zhou\({}^{1}\) Yi Ma\({}^{2}\) Yaodong Yu\({}^{12}\) Cihang Xie\({}^{1}\)\({}^{1}\)

\({}^{*}\)equal technique contribution, \({}^{\dagger}\)equal advising

\({}^{1}\)UC Santa Cruz \({}^{2}\)UC Berkeley

###### Abstract

crate, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of crate remains an open question which this paper aims to address. Specifically, we propose crate-\(\alpha\), featuring strategic yet minimal modifications to the sparse coding block in the crate architecture design, and a light training recipe designed to improve the scalability of crate. Through extensive experiments, we demonstrate that crate-\(\alpha\) can effectively scale with larger model sizes and datasets. For example, our crate-\(\alpha\)-B substantially outperforms the prior best crate-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our crate-\(\alpha\)-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned crate models, as we demonstrate through showing that the learned token representations of increasingly larger trained crate-\(\alpha\) models yield increasingly higher-quality unsupervised object segmentation of images. The project page is https://rayjryang.github.io/CRATE-alpha/.

## 1 Introduction

Over the past several years, the Transformer architecture [42] has dominated deep representation learning for natural language processing (NLP), image processing, and visual computing [8, 2, 9, 5, 12]. However, the design of the Transformer architecture and its many variants remains largely empirical and lacks a rigorous mathematical interpretation. This has largely hindered the development of new Transformer variants with improved efficiency or interpretability. The recent white-box Transformer model crate[46] addresses this gap by deriving a simplified Transformer block via unrolled optimization on the so-called _sparse rate reduction_ representation learning objective.

More specifically, layers of the white-box crate architecture are mathematically derived and fully explainable as unrolled gradient descent-like iterations for optimizing the sparse rate reduction. The self-attention blocks of crate explicitly conduct compression via denoising features against learned low-dimensional subspaces, and the MLP block is replaced by an incremental sparsification (via ISTA [1, 11]) of the features. As shown in previous work [47], besides mathematical interpretability, the learned crate models and features also have much better semantic interpretability than conventional transformers, i.e., visualizing features of an image naturally forms a zero-shot image segmentation of that image, even when the model is only trained on classification.

Scaling model size is widely regarded as a pathway to improved performance and emergent properties [44, 40, 41, 14]. Until now, the deployment of crate has been limited to relatively modest scales. The most extensive model described to date is the base model size encompasses 77.6M parameters(create-Large) [46]. This contrasts sharply with standard Vision Transformers (ViTs [9]), which have been effectively scaled to a much larger model size, namely 22B parameters [5].

To this end, this paper provides the first exploration of training crate at different scales for vision, i.e., Tiny, Small, Base, Large, Huge. Detailed model specifications are given in Table 7 of Appendix A.1. To achieve effective scaling, we make two key changes. First, we identify the vanilla ISTA block within crate as a limiting factor that hinders further scaling. To overcome this, we significantly expand the channels, decouple the association matrix, and add a residual connection, resulting in a new model variant -- crate-\(\alpha\). It is worth noting that this architecture change still preserves the mathematical interpretability of the model. Second, we propose an improved training recipe, inspired by previous work [38, 46, 39], for better coping the training with our new crate-\(\alpha\) architecture.

We provide extensive experiments supporting the effective scaling of our crate-\(\alpha\) models. For example, we scale the crate-\(\alpha\) model from Base to Large size for supervised image classification on ImageNet-21K [6], achieving _85.1% top-1 accuracy on ImageNet-1K_ at the Large model size. We further scale the model size from Large to Huge, utilizing vision-language pre-training with contrastive learning on DataComp1B [10], and achieve _a zero-shot top-1 accuracy of 72.3% on ImageNet-1K_ at the Huge model size.1 These results demonstrate the strong scalability of the crate-\(\alpha\) model, shedding light on scaling up mathematically interpretable models for future work.

Footnote 1: Model configurations are detailed in Table 7 (in Appendix A.1).

The main contributions of this paper are threefold:

1. We design three strategic yet minimal modifications for the crate model architecture to unleash its potential. In Figure 1, we reproduce the results of the crate model within our training setup, initially pre-training on ImageNet-21K classification and subsequently fine-tuning on ImageNet-1K classification. Compared to the vanilla crate model that achieves 68.5% top-1 classification accuracy on ImageNet-1K, our crate-\(\alpha\)-B/32 model significantly improves the vanilla crate model by 8%, which clearly demonstrates the benefits of the three modifications to the existing crate model. Moreover, following the settings of the best crate model and changing the image patch size from 32 to 8, our crate-\(\alpha\)-B model attains a top-1 accuracy of 83.2% on ImageNet-1K, exceeding the previous best crate model's score of 79.5% by a significant margin of 3.7%.
2. Through extensive experiments, we show that one can effectively scale crate-\(\alpha\) via model size and data simultaneously. In contrast, when increasing the crate model from Base to Large model size, there is a marginal improvement on top-1 classification accuracy (+0.5%, from 70.8% to 71.3%) on ImageNet-1K, indicating diminished returns [46]. Furthermore, by scaling the training dataset, we achieved a substantial 1.9% improvement in top-1 classification accuracy on ImageNet-1K, increasing from 83.2% to 85.1% when going from crate-\(\alpha\) Base to Large.
3. We further successfully scale crate-\(\alpha\) model from Large to Huge by leveraging vision-language pre-training on DataComp1B. Compared to the Large model, the Huge model (crate-\(\alpha\)-H) achieves a zero-shot top-1 classification accuracy of 72.3% on ImageNet-1K, marking a significant

Figure 1: _(Left)_ We demonstrate how modifications to the components enhance the performance of the crate model. The four models are trained using the same setup: first pre-trained on ImageNet-21K and then fine-tuned on ImageNet-1K. Details are provided in Section 3. _(Right)_. We compare the FLOPs and accuracy on ImageNet-1K of our methods with ViT [9] and CRATE [46]. The values of crate-\(\alpha\) model correspond to those presented in Table 1. A more detailed comparison between crate-\(\alpha\) and ViT is included in Appendix A.2.

scaling gain of 2.5% over the Large model. These results indicate that the crate architecture has the potential to serve as an effective backbone for vision-language foundation models.

#### Related Work

**White-box Transformers.**[46; 45] argued that the quality of a learned representation can be assessed through a unified objective function called the _sparse rate reduction_. Based on this framework, [46; 45] developed a family of transformer-like deep network architectures, named crate, which are mathematically fully interpretable. crate models has been demonstrably effective on various tasks, including vision self-supervised learning and language modeling [26; 45]. Nevertheless, it remains unclear whether crate can scale as effectively as widely used black-box transformers. Previous work [46] suggests that scaling the vanilla crate model can be notably challenging.

**Scaling ViT.** ViT [9] represents the initial successful applications of Transformers to the image domain on a large scale. Many works [12; 31; 33; 32; 5; 37; 21; 22; 29; 18; 49] have deeply explored various ways of scaling ViTs in terms of model size and data size. From the perspective of self-supervision, MAE [12] provides a scalable approach to effectively training a ViT-Huge model using only ImageNet-1K. Following the idea of MAE, [31] further scales both model parameters to billions and data size to billions of images. Additionally, CLIP was the first to successfully scale ViT on a larger data scale (i.e., 400M) using natural language supervision. Based on CLIP, [32; 33] further scale the model size to 18 billion parameters, named EVA-CLIP-18B, achieving consistent performance improvements with the scaling of ViT model size. From the perspective of supervised learning, [49; 5] present a comprehensive analysis of the empirical scaling laws for vision transformers on image classification tasks, sharing some similar conclusions with [15]. [49] suggests that the performance-compute frontier for ViT models, given sufficient training data, tends to follow a saturating power law. More recently, [5] scales up ViT to 22 billion parameters. Scaling up different model architectures is non-trivial. [37; 21; 22] have made many efforts to effectively scale up different architectures. In this paper, due to the lack of study on the scalability of white-box models, we explore key architectural modifications to effectively scale up white-box transformers in the image domain.

## 2 Background and Preliminaries

In this section, we present the background on white-box transformers proposed in [46], including representation learning objectives, unrolled optimization, and model architecture. We first introduce the notation that will be used in the later presentation.

**Notation.** We use notation and problem setup following Yu et al. [46]. We use the matrix-valued random variable \(\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}\) to represent the data, where each \(\bm{x}_{i}\in\mathbb{R}^{D}\) is a "token", such that each data point is a realization of \(\bm{X}\). For instance, \(\bm{X}\) can represent a collection of image patches for an image, and \(\bm{x}_{i}\) is the \(i\)-th image patch. We use \(f\in\mathcal{F}\colon\mathbb{R}^{D\times N}\to\mathbb{R}^{d\times N}\) to denote the mapping induced by the transformer, and we let \(\bm{Z}=f(\bm{X})=[\bm{z}_{1},\dots,\bm{z}_{N}]\in\mathbb{R}^{d\times N}\) denote the features for input data \(\bm{X}\). Specifically, \(\bm{z}_{i}\in\mathbb{R}^{d}\) denotes the feature of the \(i\)-th input token \(\bm{x}_{i}\). The transformer \(f\) consists of multiple, say \(L\), layers, and so can be written as \(f=f^{L}\circ\dots\circ f^{1}\circ f^{\text{pre}}\), where \(f^{\ell}\colon\mathbb{R}^{d\times N}\to\mathbb{R}^{d\times N}\) denotes the \(\ell\)-th layer of the transformer, and the pre-processing layer is denoted by \(f^{\text{pre}}=\mathbb{R}^{D\times N}\to\mathbb{R}^{d\times N}\). The input to the \(\ell\)-th layer \(f^{\ell}\) of the transformer is denoted by \(\bm{Z}^{\ell}=\left[\bm{z}_{1}^{\ell},\dots,\bm{z}_{N}^{\ell}\right]\in \mathbb{R}^{d\times N}\), so that \(f^{\ell}\colon\bm{Z}^{\ell}\mapsto\bm{Z}^{\ell+1}\). In particular, \(\bm{Z}^{1}=f^{\text{pre}}(\bm{X})\in\mathbb{R}^{d\times N}\) denotes the output of the pre-processing layer and the input to the first layer.

### Sparse Rate Reduction

Following the framework proposed in [45], we posit that the goal of representation learning is to learn a feature mapping or _representation_\(f\in\mathcal{F}\colon\mathbb{R}^{D\times N}\to\mathbb{R}^{d\times N}\) that transforms the input data \(\bm{X}\) (which may have a nonlinear, multi-modal, and otherwise complicated distribution) into _structured and compact_ features \(\bm{Z}\), such that the token features lie on a union of low-dimensional subspaces, say with orthonormal bases \(\bm{U}_{[K]}=(\bm{U}_{k})_{k\in[K]}\in(\mathbb{R}^{d\times p})^{K}\). [46] proposes the _Sparse \(\bm{R}\)ate \(\bm{R}\)duction_ (SRR) _objective_ to measure the goodness of such a learned representation:

\[\max_{f\in\mathcal{F}}\,\mathbb{E}_{\bm{Z}=f(\bm{X})}\left[L_{\texttt{srr}}( \bm{Z})\right]=\min_{f\in\mathcal{F}}\,\mathbb{E}_{\bm{Z}=f(\bm{X})}\left[R^{ c}(\bm{Z}\,|\,\bm{U}_{[K]})-R(\bm{Z}\,|\,\bm{U}_{[K]})+\lambda\|\bm{Z}\|_{1} \right],\] (1)where \(\bm{Z}=f(\bm{X})\) denotes the token representation, \(\|\bm{Z}\|_{1}\) denotes the \(\ell^{1}\) norm, and \(R(\bm{Z})\), \(R^{c}(\bm{Z}\mid\bm{U}_{[K]})\) are (estimates for) _rate distortions_[4, 7], defined as:

\[R(\bm{Z})\doteq\frac{1}{2}\log\det\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm{Z}^ {\top}\bm{Z}\right),\qquad R^{c}(\bm{Z}\mid\bm{U}_{[K]})\doteq\sum_{k=1}^{K} R(\bm{U}_{k}^{\top}\bm{Z}).\] (2)

In particular, \(R^{c}(\bm{Z}\mid\bm{U}_{[K]})\) (resp. \(R(\bm{Z})\)) provide closed-form estimates for the number of bits required to encode the sample \(\bm{Z}\) up to precision \(\epsilon>0\), conditioned (resp. unconditioned) on the samples being drawn from the subspaces with bases \(\bm{U}_{[K]}\). Minimizing the term \(R^{c}\) improves the compression of the features \(\bm{Z}\) against the posited model, and maximizing the term \(R\) promotes non-collapsed features. The remaining term \(\lambda\|\bm{Z}\|_{1}\) promotes sparse features. Refer to [45] for more details about the desiderata and objective of representation learning via the rate reduction approach.

### crate: Coding RATE Transformer

**Unrolled optimization.** To optimize the learning objective and learn compact and structured representation, one approach is unrolled optimization [11, 36]: each layer of the deep network implements an iteration of an optimization algorithm on the learning objective. For example, one can design the layer \(f^{\ell}\) such that the forward pass is equivalent to a proximal gradient descent step for optimizing learning objective \(L(\bm{Z})\), i.e., \(\bm{Z}^{\ell+1}=f^{\ell}(\bm{Z}^{\ell})=\texttt{Prox}[\bm{Z}^{\ell}-\eta\cdot \nabla_{\bm{Z}}L(\bm{Z}^{\ell})]\). Here we use \(\eta\) to denote the step size and \(\texttt{Prox}[\cdot]\) to denote the proximal operator [27].

**One layer of the crate model.** We now present the design of each layer of the white-box transformer architecture - Coding RATE Transformer (crate) - proposed in [46]. Each layer of crate contains two blocks: the compression block and the sparsification block. These correspond to a two-step alternating optimization procedure for optimizing the sparse rate reduction objective (1). Specifically, the \(\ell\)-th layer of crate is defined as

\[\bm{Z}^{\ell+1}=f^{\ell}(\bm{Z}^{\ell})=\texttt{ISTA}(\bm{Z}^{\ell+1/2}\mid \bm{D}^{\ell}),\quad\mathrm{where}\quad\bm{Z}^{\ell+1/2}=\bm{Z}^{\ell}+ \texttt{MSSA}(\bm{Z}^{\ell}).\] (3)

**Compression block (MSSA).** The compression block in crate, called **M**ulti-head **S**ubspace **S**elf-**A**ttention block (MSSA), is derived for compressing the token set \(\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{N}]\) by optimizing the compression term \(R^{c}\) (defined Eq. (1)), i.e.,

\[\bm{Z}^{\ell+1/2}=\bm{Z}^{\ell}+\texttt{MSSA}(\bm{Z}^{\ell}\mid\bm{U}_{[K]}^{ \ell})\approx\bm{Z}^{\ell}-\kappa\nabla_{\bm{Z}}R^{c}(\bm{Z}^{\ell}\,|\,\bm{U} _{[K]}^{\ell}),\] (4)

where \(\bm{U}_{[K]}^{\ell}\) denotes the (local) signal model at layer \(\ell\), and the MSSA operator is defined as

\[\texttt{MSSA}(\bm{Z}\mid\bm{U}_{[K]})=\frac{\kappa p}{N\epsilon^{2}}\left[\bm {U}_{1}\,\cdots\,\bm{U}_{K}\right]\begin{bmatrix}(\bm{U}_{1}^{\top}\bm{Z}) \operatorname{softmax}((\bm{U}_{1}^{\top}\bm{Z})^{\top}(\bm{U}_{1}^{\top}\bm{Z }))\\ \vdots\\ (\bm{U}_{K}^{\top}\bm{Z})\operatorname{softmax}((\bm{U}_{K}^{\top}\bm{Z})^{ \top}(\bm{U}_{K}^{\top}\bm{Z}))\end{bmatrix}.\] (5)

Compared with the commonly used attention block in transformer [42], where the \(k\)-th attention head is defined as \((\bm{V}_{k}^{\top}\bm{Z})\operatorname{softmax}((\bm{Q}_{k}^{\top}\bm{Z})^{ \top}(\bm{K}_{k}^{\top}\bm{Z}))\), MSSA uses only one matrix to obtain the query, key, and value matrices in the attention: that is, \(\bm{U}_{k}=\bm{Q}_{k}=\bm{K}_{k}=\bm{V}_{k}\).

**Sparse coding block (ISTA).** The Iterative Shrinkage-Thresholding Algorithm (ISTA) block is designed to optimize the sparsity term and the global coding rate term, \(\lambda\|\bm{Z}\|_{0}-R(\bm{Z}\mid\bm{U}_{[K]})\) in (1). [46] shows that an optimization strategy for these terms posits a (complete) incoherent dictionary \(\bm{D}^{\ell}\in\mathbb{R}^{d\times d}\) and takes a proximal gradient descent step towards solving the associated LASSO problem \(\arg\min_{\bm{Z}\geq 0}[\frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}\bm{Z}\|_{2}^{2}+ \lambda\|\bm{Z}\|_{1}]\), obtaining the iteration

\[\bm{Z}^{\ell+1}=\texttt{ISTA}(\bm{Z}^{\ell+1/2}\,|\,\bm{D}^{\ell})=\mathrm{ ReLU}(\bm{Z}^{\ell+1/2}+\eta\,(\bm{D}^{\ell})^{\top}(\bm{Z}^{\ell+1/2}-\bm{D}^{ \ell}\bm{Z}^{\ell+1/2})-\eta\lambda\bm{1}).\] (6)

In particular, the \(\texttt{ISTA}\) block sparsifies the intermediate iterates \(\bm{Z}^{\ell+1/2}\) w.r.t. \(\bm{D}^{\ell}\) to obtain \(\bm{Z}^{\ell+1}\).

## 3 CRATE-\(\alpha\) Model

In this section, we present the crate-\(\alpha\) architecture, which is a variant of crate[46]. As shown in Fig. 1 (_Right_), there is a significant performance gap between the white-box transformer crate-B/16(70.8%) and the vision transformer ViT-B/16 (84.0%) [9]. One possible reason is that the ISTA block applies a complete dictionary \(\bm{D}\in\mathbb{R}^{d\times d}\), which may limit its expressiveness. In contrast, the MLP block in the transformer2 applies two linear transformations \(\bm{W}_{1},\bm{W}_{2}\in\mathbb{R}^{d\times d}\), leading to the MLP block having 8 times more parameters than the ISTA block.

Footnote 2: The MLP block is defined as \(\bm{Z}^{\ell+1}=\bm{Z}^{\ell}+\bm{W}_{2}\sigma(\bm{W}_{1}^{\top}\bm{Z}^{\ell+ 1/2})\), where \(\sigma\) is the nonlinear activation function and \(\bm{Z}^{\ell+1/2}\) denotes the output of the attention block.

Since the ISTA block in CRATE applies a single incremental step to optimize the sparsity objective, applying an orthogonal dictionary can make it ineffective in sparsifying the token representations. Previous work [28] has theoretically demonstrated that overcomplete dictionary learning enjoys a favorable optimization landscape. In this work, we use an overcomplete dictionary in the sparse coding block to promote sparsity in the features. Specifically, instead of using a complete dictionary \(\bm{D}^{\ell}\in\mathbb{R}^{d\times d}\), we use an overcomplete dictionary \(\bm{D}^{\ell}\in\mathbb{R}^{d\times(Cd)}\), where \(C>1\) (a positive integer) is the overcompleteness parameter. Furthermore, we explore two additional modifications to the sparse coding block that lead to improved performance for crate. We now describe the three variants of the sparse coding block that we use in this paper.

**Modification #1: Overparameterized sparse coding block.** For the output of the \(\ell\)-th crate attention block \(\bm{Z}^{\ell+1/2}\), we propose to sparsify the token representations with respect to an overcomplete dictionary \(\bm{D}^{\ell}\in\mathbb{R}^{d\times(Cd)}\) by optimizing the following LASSO problem,

\[\bm{A}^{\ell}\approx\operatorname*{arg\,min}_{\bm{A}\succeq\bm{0}}\Big{[} \frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}\bm{A}\|_{2}^{2}+\lambda\|\bm{A} \|_{1}\Big{]}.\] (7)

To approximately solve (7), we apply two steps of proximal gradient descent, i.e.,

\[\bm{A}_{0}^{\ell}=\bm{0},\qquad\bm{A}_{1}^{\ell}=\texttt{Prox}_{\eta,\lambda }\big{[}\bm{A}_{0}^{\ell};\bm{D}^{\ell},\bm{Z}^{\ell+1/2}\big{]},\qquad\bm{A }_{2}^{\ell}=\texttt{Prox}_{\eta,\lambda}\big{[}\bm{A}_{1}^{\ell};\bm{D}^{ \ell},\bm{Z}^{\ell+1/2}\big{]},\] (8)

where Prox is the proximal operator of the above non-negative LASSO problem (7) and defined as

\[\texttt{Prox}_{\eta,\lambda}[\bm{A};\bm{D},\bm{Z}]=\mathrm{ReLU}(\bm{A}-\eta \bm{D}^{\top}(\bm{D}\bm{A}-\bm{Z})-\eta\lambda\bm{1}).\] (9)

The output of the sparse coding block is defined as

\[\bm{Z}^{\ell+1}=\bm{D}^{\ell}\bm{A}^{\ell},\quad\mathrm{where}\quad\bm{A}^{ \ell}=\bm{A}_{2}^{\ell}\doteq\texttt{ISTA-OC}(\bm{Z}^{\ell+1/2}\mid\bm{D}^{ \ell}).\] (10)

Namely, \(\bm{A}^{\ell}\) is a sparse representation of \(\bm{Z}^{\ell+1/2}\) with respect to the overcomplete dictionary \(\bm{D}^{\ell}\). The original crate ISTA tries to learn a complete dictionary \(\bm{D}\in\mathbb{R}^{d\times d}\) to transform and sparsify the features \(\bm{Z}\). By leveraging more atoms than the ambient dimension, the overcomplete dictionary \(\bm{D}\in\mathbb{R}^{d\times(Cd)}\) can provide a redundant yet expressive codebook to identify the salient sparse

Figure 2: One layer of the crate-\(\alpha\) model architecture. MSSA (**M**ulti-head **S**ubspace **S**elf-**A**ttention, defined in (5)) represents the compression block, and **ODL (**O**vercomplete **D**ictionary **L**earning, defined in (12)) represents the sparse coding block. A more detailed illustration of the modifications is provided in Fig. 6 in the Appendix.

structures underlying \(\bm{Z}\). As shown in Fig. 1, the overcomplete dictionary design leads to \(5.3\%\) improvement compared to the vanilla crate model.

**Modification #2: Decoupled dictionary.** We propose to apply a decoupled dictionary \(\widehat{\bm{D}}^{\ell}\) in the last step (defined in (10) of the sparse coding block, \(\bm{Z}^{\ell+1}=\widehat{\bm{D}}^{\ell}\bm{A}^{\ell}\), where \(\widehat{\bm{D}}^{\ell}\in\mathbb{R}^{d\times(Cd)}\) is a different dictionary compared to \(\bm{D}^{\ell}\). By introducing the decoupled dictionary, we further improve the model performance by \(2.0\%\), as shown in Fig. 1. We denote this mapping from \(\bm{Z}^{\ell+1/2}\) to \(\bm{Z}^{\ell+1}\) as the **O**vercomplete **D**ictionary **L**earning block (ODL), defined as follows:

\[\texttt{ODL}(\bm{Z}^{\ell+1/2}\mid\bm{D}^{\ell},\widehat{\bm{D}}^{\ell}) \doteq\widehat{\bm{D}}^{\ell}\cdot\texttt{ISTA-OC}(\bm{Z}^{\ell+1/2}\mid\bm{ D}^{\ell})=\widehat{\bm{D}}^{\ell}\bm{A}^{\ell}.\] (11)

**Modification #3: Residual connection.** Based on the previous two modifications, we further add a residual connection, obtaining the following modified sparse coding block:

\[\bm{Z}^{\ell+1}=\bm{Z}^{\ell+1/2}+\texttt{ODL}(\bm{Z}^{\ell+1/2}\mid\bm{D}^{ \ell},\widehat{\bm{D}}^{\ell}).\] (12)

An intuitive interpretation of this modified sparse coding block is as follows: instead of directly sparsifying the feature representations \(\bm{Z}\), we first identify the potential sparse patterns present in \(\bm{Z}\) by encoding it over a learned dictionary. Subsequently, we incrementally refine \(\bm{Z}\) by exploiting the sparse codes obtained from the previous encoding step. From Fig. 1, we find that the residual connection leads to a \(0.7\%\) improvement.

To summarize, to effectively scale white-box transformers, we implement three modifications to the vanilla white-box crate model proposed in [46]. Specifically, in our crate-\(\alpha\) model, we introduce a decoupling mechanism, quadruple the dimension of the dictionary (4\(\times\)), and incorporate a residual connection in the sparse coding block.

## 4 Experiments

**Overall.** The experimental section consists of three parts: (1) **Scaling study:** We thoroughly investigate the scaling behaviors of crate-\(\alpha\) from Base to Large size and ultimately to Huge size. (2) **Downstream applications:** To further verify the broader benefits of scaling the crate-\(\alpha\) model, we conduct additional experiments on real-world downstream tasks and present preliminary exploration results of crate-\(\alpha\) on language tasks. (3) **Interpretability:** In addition to scalability, we study the interpretability of crate-\(\alpha\) across different model sizes.

### Dataset and Evaluation

**Scaling Study.** For the transition from Base to Large size, we pre-train our model on ImageNet-21K and fine-tune it on ImageNet-1K via supervised learning. When scaling from Large to Huge, we utilize the DataComp1B [10] dataset within a vision-language pre-training paradigm, allowing us to study the effects of scaling the model to a massive size. For evaluation, we evaluate the zero-shot accuracy of these models on ImageNet-1K.

Figure 3: Training loss curves of crate-\(\alpha\) on ImageNet-21K. (_Left_) Comparing training loss curves across crate-\(\alpha\) with different model sizes. (_Right_) Comparing training loss curves across crate-\(\alpha\)-Large with different patch sizes.

**Downstream Applications.** We include additional experimental results on four downstream datasets (CIFAR-10/100, Oxford Flowers, and Oxford-IIT Pets). We also examine the dense prediction capability of crate-\(\alpha\) by training it on segmentation tasks using the ADE20K dataset [51]. For language tasks, we conduct new experiments with crate-\(\alpha\) using autoregressive training on OpenWebText, following the setup in nanoGPT [16].

**Interpretability**. Following the evaluation setup of crate as outlined in [46], we apply MaskCut [43] to validate and evaluate the rich semantic information captured by our model in a zero-shot setting, including both qualitative and quantitative results.

### Training and Fine-tuning Procedures

**Scaling Study. (1) Base to Large size:** We initially pre-train the crate-\(\alpha\) model on ImageNet-21K and subsequently fine-tune it on ImageNet-1K. During the pre-training phase, we set the learning rate to \(8\times 10^{-4}\), weight decay to 0.1, and batch size to 4096. We apply data augmentation techniques such as Inception crop [35] resized to 224 and random horizontal flipping. In the fine-tuning phase, we adjust the base learning rate to \(1.6\times 10^{-4}\), maintain weight decay at 0.1, and batch size at 4096. We apply label smoothing with a smoothing parameter of 0.1 and apply data augmentation methods including Inception crop, random horizontal flipping, and random augmentation with two transformations (magnitude of 9). For evaluation, we resize the smaller side of an image to 256 while maintaining the original aspect ratio and then crop the central portion to 224\(\times\)224. In both the pre-training and fine-tuning phases, we use the AdamW optimizer [24] and incorporate a warm-up strategy, characterized by a linear increase over 10 epochs. Both the pre-training and fine-tuning are conducted for a total of 91 epochs, utilizing a cosine decay schedule.

**(2) Large to Huge size:** In the pre-training stage, we utilize an image size of 84\(\times\)84, and the maximum token length is 32, with a total of 2.56 billion training samples. During the fine-tuning stage, we increase the image size to 224\(\times\)224 while maintaining the maximum token length at 32, with a 512 million training samples. Here, the key distinction between the pre-training stage and the fine-tuning stage is the image size. A smaller image size results in a faster training speed. In the configurations of crate-\(\alpha\)-CLIPA-B, crate-\(\alpha\)-CLIPA-L, and crate-\(\alpha\)-CLIPA-H, we use the crate-\(\alpha\) model as the vision encoder, and utilize the same pre-trained huge transformer model from CLIPA [18] as the text encoder. For both the pre-training and fine-tuning stages, we freeze the text encoder and only train the vision encoder, i.e., the crate-\(\alpha\) model. As we will show in the later results, this setup effectively demonstrates the scaling behaviors of crate-\(\alpha\) models in the image domain. Detailed hyperparameter settings can be found in Appendix A.

**Downstream Applications.** On four downstream datasets, we follow the training setup from [46]. For the segmentation task, we compare the performance of CRATE and crate-\(\alpha\) on the ADE20K dataset, mainly following the setup of [30] with minor modifications. Our batch size is set to 128, and the total number of training steps is 5000. For the language task, we conduct experiments with crate-\(\alpha\) using autoregressive training on OpenWebText, following the setup in [16]. We compare crate-\(\alpha\) models (57M and 120M) with CRATE and GPT-2, using results from CRATE reported in [45].

\begin{table}
\begin{tabular}{l c|c c} \hline \hline Models (Base) & ImageNet-1K(\%) & Models (Large) & ImageNet-1K(\%) \\ \hline \hline crate-B/16 w/o IN-21K & 70.8\({}^{\ddagger}\) & crate-L/16 w/o IN-21K & 71.3\({}^{\ddagger}\) \\ \hline \hline crate-\(\alpha\)-B/32 & 76.5 & crate-\(\alpha\)-L/32 & 80.2 \\ \hline crate-\(\alpha\)-B/16 & 81.2 & crate-\(\alpha\)-L/14 & 83.9 \\ \hline crate-\(\alpha\)-B/8 & 83.2 & crate-\(\alpha\)-L/8 & 85.1 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Top-1 accuracy of crate-\(\alpha\) on ImageNet-1K with different model scales when pre-trained on ImageNet-21K and then fine-tuned on ImageNet-1K. For comparison, we also list the results from the paper [46] which demonstrate the diminished return from crate base to large, trained only on ImageNet-1K. “IN-21K” refers to ImageNet-21K. (\({}^{\ddagger}\)Results from [46].)

### Results and Analysis

**Scaling the crate-\(\alpha\) Model from Base to Large.** As shown in Table 1, we compare crate-\(\alpha\)-B and crate-\(\alpha\)-L at patch sizes 32, 16, and 8. Firstly, we find our proposed crate-\(\alpha\)-L consistently achieves significant improvements across all patch sizes. Secondly, compared with the results of the vanilla crate (the first row of Table 1), increasing from crate-B to crate-L results in only a 0.5% improvement on ImageNet-1K. This indicates a case of diminishing returns. These findings compellingly highlight that the scalability of crate-\(\alpha\) models significantly outperforms that of the vanilla crate. Meanwhile, the training loss in the pre-training stage is presented in Fig. 3; as the model capacity increases, the trend of the training loss improves predictably. This phenomenon is also described in [9].

**Scaling the crate-\(\alpha\) Model from Large to Huge.** From the results shown in Fig. 4, we find that: (1) crate-\(\alpha\)-CLIPA-L/14 significantly outperforms crate-\(\alpha\)-CLIPA-B/16 by 11.3% and 9.0% in terms of ImageNet-1K zero-shot accuracy during the pre-training and fine-tuning stages, respectively. The substantial benefit suggests that the quality of learned representation may be constrained by the model size. Therefore, increasing the model size effectively leverages larger amounts of data. (2) When continuing to scale up model size, we also observe that crate-\(\alpha\)-CLIP-H/14 continues to benefit from larger training datasets, outperforming crate-\(\alpha\)-CLIP-L/14 by 3.1% and 2.5% in terms of ImageNet-1K zero-shot accuracy during the pre-training and fine-tuning stages, respectively. This demonstrates the strong scalability of the crate-\(\alpha\) model. To explore the performance ceiling, we train a standard ViT-CLIPA-H/14 from scratch and observe improved performance.

**Downstream Applications.** On four downstream datasets, as shown in Table 2, we find that crate-\(\alpha\) consistently outperforms CRATE, with both models pre-trained on IN21K, while crate-\(\alpha\) demonstrates improved performance as model size increases. For the segmentation task, results in Table 3 show that crate-\(\alpha\) consistently outperforms CRATE across all key metrics, with both models pre-trained on IN21K. These findings indicate significant performance gains in vision tasks beyond classification. For the language task, Table 4 shows that crate-\(\alpha\) significantly improves over CRATE in language modeling. Due to limited time and resource constraints, we completed 80% of the total iterations for crate-\(\alpha\)-small and 55% for crate-\(\alpha\)-base, compared to the 600K total iterations used for CRATE. Nevertheless, crate-\(\alpha\) still demonstrated notable improvements.

**Interpretability.** As shown in Fig. 5, we provide the segmentation visualization on COCO val2017 [20] for crate-\(\alpha\), crate, and ViT, respectively. We find that our model preserves and even improves the (semantic) interpretability advantages of crate. Moreover, we summarize quantitative evaluation results on COCO val2017 in Table 6. Interestingly, when scaling up model size for crate-\(\alpha\), the Large model improves over the Base model in terms of object detection and segmentation.

### Compute-efficient Scaling Strategy

We further explore methods to scale models efficiently in terms of computation. Table 1 demonstrates that the crate-\(\alpha\) model scales effectively from the Base model to its larger variants. However, the pre-training computation for the top-performing model, crate-\(\alpha\)-L/8, is resource-intensive on

Figure 4: (_Left_) Comparing training loss curves of crate-\(\alpha\)-CLIPA with different model sizes on DataComp1B. (_Right_) Comparing zero-shot accuracy of crate-\(\alpha\)-B/L/H models and ViT-H on ImageNet-1K.

ImageNet-21K. Inspired by CLIPA [18], we aim to reduce computational demands by using reduced image token sequence lengths, while maintaining the same training setup during the fine-tuning stage. The results are summarized in Table 5.

**Results and analysis.** (1) When fine-tuning with crate-\(\alpha\)-L/14 and using crate-\(\alpha\)-L/32 for pre-training on ImageNet-21K, this approach consumes about 35% of the TPU v3 core-hours required by crate-\(\alpha\)-L/14, yet achieves a promising 83.7% top-1 accuracy on ImageNet-1K, comparable to the 83.9% achieved by crate-\(\alpha\)-L/14; (2) When fine-tuning with crate-\(\alpha\)-L/8 and using crate-\(\alpha\)-L/32 for pre-training, this approach consumes just 15% of the training time required by crate-\(\alpha\)-L/8, yet it still achieves a promising 84.2% top-1 accuracy on ImageNet-1K, compared to 85.1% when using the crate-\(\alpha\)-L/8 model in the pre-training stage; (3) While the total computational cost of crate-\(\alpha\)-L/32 + crate-\(\alpha\)-L/8 is less than that of crate-\(\alpha\)-L/14 + crate-\(\alpha\)-L/14, the performance of the former is slightly better. In summary, we find that this strategy offers a valuable reference for efficiently scaling crate-\(\alpha\) models in the future.

## 5 Discussion

**Limitations.** Although we have used some existing compute-efficient training methods (e.g., CLIPA [18]) and have initiated an exploration into compute-efficient scaling strategies for white-box transformers in Section 4.4, this work still requires a relatively large amount of computational resources, which may not be easily accessible to many researchers.

**Societal impact.** A possible broader implication of this research is the energy consumption needed to conduct the experiments in our scaling study. However, there is growing interest in developing white-box transformers for better interpretability and transparency across a wide range of tasks and domains, including image segmentation [46], self-supervised masked autoencoders [26], and integrated sensing and communications [50], etc. Moreover, our results on the scalability of white

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & GPT-2-base & CRATE-base & CRATE-\(\alpha\)-small & CRATE-\(\alpha\)-base \\ \hline Model size & 124M & 60M & 57M & 120M \\ Cross-entropy validation loss & 2.85 & 3.37 & 3.28 & 3.14 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The comparison between CRATE and CRATE-\(\alpha\) on the NLP task using the OpenWebText dataset.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Dataset & CRATE-B/32 & CRATE-\(\alpha\)-B/32 & CRATE-\(\alpha\)-L/32 & CRATE-\(\alpha\)-B/16 & CRATE-\(\alpha\)-L/14 \\ \hline CIFAR-10 & 97.22 & 98.17 & 98.68 & 98.67 & 99.10 \\ CIFAR-100 & 85.27 & 89.40 & 91.16 & 90.58 & 92.57 \\ Oxford Flowers-102 & 93.90 & 97.77 & 99.01 & 99.27 & 99.56 \\ Oxford-IIIIT-Pets & 80.38 & 88.19 & 90.46 & 92.70 & 93.98 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The performance comparison between CRATE and CRATE-\(\alpha\) across various datasets.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & GPT-2-base & CRATE-base & CRATE-\(\alpha\)-small & CRATE-\(\alpha\)-base \\ \hline Model size & 124M & 60M & 57M & 120M \\ Cross-entropy validation loss & 2.85 & 3.37 & 3.28 & 3.14 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Compute-efficient scaling strategy. To reduce the compute requirements of the pre-training stage, we use a model with a larger patch size. This results in a shorter token length for the same input size. The second and fourth columns indicate the compute requirements for the pre-training and fine-tuning stages, respectively, measured in TPU v3 core-hours. Details are provided in Section 4.4.

box transformers could also shed light on scaling up a broader class of white-box deep neural networks, such as white-box ISTA networks and their variants [11; 34; 3; 48; 17], designed via unrolled optimization. In summary, we believe that our findings and insights could be helpful for developing white-box transformers for a wide range of applications and tasks, benefiting a broad audience interested in building more interpretable and performant deep learning models and further amortizing the pre-training compute costs.

## 6 Conclusion

This paper provides the first exploration of training white-box transformer crate at scale for vision tasks. We introduce both principled architectural changes and improved training recipes to unleash the potential scalability of the crate type architectures. With these modifications, we successfully scale up the crate-\(\alpha\) model along both the dimensions of model size and data size, while preserving, in most cases even improving, the semantic interpretability of the learned white-box transformer models. We believe this work provides valuable insights into scaling up mathematically interpretable deep neural networks, not limited to transformer-like architectures.

## Acknowledgement

This work is supported by a gift from Open Philanthropy, TPU Research Cloud (TRC) program, and Google Cloud Research Credits program.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline  & & \multicolumn{3}{c}{Detection} & \multicolumn{3}{c}{Segmentation} \\ Model & Train & AP\({}_{50}\uparrow\) & AP\({}_{75}\uparrow\) & AP \(\uparrow\) & AP\({}_{50}\uparrow\) & AP\({}_{75}\uparrow\) & AP \(\uparrow\) \\ \hline crate-B/8 [47] & Supervised & 2.9 & 1.0 & 1.3 & 2.2 & 0.7 & 1.0 \\ ViT-B/8 [47] & Supervised & 0.8 & 0.2 & 0.4 & 0.7 & 0.5 & 0.4 \\ \hline crate-\(\alpha\)-B/8 & Supervised & 3.5 & 1.1 & 1.5 & 2.2 & 1.0 & 1.1 \\ crate-\(\alpha\)-L/8 & Supervised & 4.0 & 1.7 & 2.0 & 2.7 & 1.1 & 1.4 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Object detection and fine-grained segmentation via MaskCut on COCO val2017 [20].** We evaluate models of various scales and assess their average precision using COCO’s official evaluation metric. Compared with existing models such as crate and ViT, crate-\(\alpha\) model achieves a notable performance gain. In addition, when scaling crate-\(\alpha\) from base to large, it also exhibits the benefit of scalability.

Figure 5: **Visualization of segmentation on COCO val2017 [20] with MaskCut [43].** (_Top row_) Supervised crate-\(\alpha\) effectively identifies the main objects in the image. Compared with crate (_Middle row_), crate-\(\alpha\) achieves better segmentation performance in terms of boundary. (_Bottom row_) Supervised ViT fails to identify the main objects in most images. We mark failed image with \(\square\).

## References

* Blumensath and Davies [2008] Thomas Blumensath and Mike E Davies. Iterative thresholding for sparse approximations. _Journal of Fourier analysis and Applications_, 14:629-654, 2008.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chen et al. [2018] Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of unfolded ista and its practical weights and thresholds. _Advances in Neural Information Processing Systems_, 31, 2018.
* Cover [1999] Thomas M Cover. _Elements of information theory_. John Wiley & Sons, 1999.
* Dehghani et al. [2023] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In _International Conference on Machine Learning_, pages 7480-7512. PMLR, 2023.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Derksen et al. [2007] Harm Derksen, Yi Ma, Wei Hong, and John Wright. Segmentation of multivariate mixed data via lossy coding and compression. In _Visual Communications and Image Processing 2007_, volume 6508, pages 170-181. SPIE, 2007.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* Gadre et al. [2024] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _Advances in Neural Information Processing Systems_, 36, 2024.
* Gregor and LeCun [2010] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In _Proceedings of the 27th international conference on international conference on machine learning_, pages 399-406, 2010.
* He et al. [2022] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009, 2022.
* Ilharco et al. [2021] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. July 2021.
* Jiang et al. [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* Karpathy [2022] Andrej Karpathy. nanogpt, 2022. URL https://github.com/karpathy/nanoGPT. GitHub repository.

* [17] Mingyang Li, Pengyuan Zhai, Shengbang Tong, Xingjian Gao, Shao-Lun Huang, Zhihui Zhu, Chong You, Yi Ma, et al. Revisiting sparse convolutional model for visual recognition. _Advances in Neural Information Processing Systems_, 35:10492-10504, 2022.
* [18] Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scaling law for clip training. _Advances in Neural Information Processing Systems_, 36, 2024.
* [19] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In _CVPR_, 2023.
* [20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11976-11986, 2022.
* [23] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In _ICLR_, 2017.
* [24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2018.
* [26] Druv Pai, Ziyang Wu, Sam Buchanan, Tianzhe Chu, Yaodong Yu, and Yi Ma. Masked completion via structured diffusion with white-box transformers. In _Conference on Parsimony and Learning (Recent Spotlight Track)_, 2023.
* [27] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. _Foundations and trends(r) in Optimization_, 1(3):127-239, 2014.
* [28] Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Geometric analysis of nonconvex optimization landscapes for overcomplete learning. In _International Conference on Learning Representations_, 2019.
* [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [30] Sucheng Ren, Fangyun Wei, Zheng Zhang, and Han Hu. Tinymim: An empirical study of distilling mim pre-trained models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3687-3697, 2023.
* [31] Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollar, Christoph Feichtenhofer, Ross Girshick, et al. The effectiveness of mae pre-pretraining for billion-scale pretraining. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5484-5494, 2023.
* [32] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023.
* [33] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Eva-clip-18b: Scaling clip to 18 billion parameters. _arXiv preprint arXiv:2402.04252_, 2024.

* [34] Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. Supervised deep sparse coding networks. In _2018 25th IEEE International Conference on Image Processing (ICIP)_, pages 346-350. IEEE, 2018.
* [35] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1-9, 2015.
* [36] Bahareh Tolooshams and Demba Ba. Stable and interpretable unrolled dictionary learning. _arXiv preprint arXiv:2106.00058_, 2021.
* [37] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272, 2021.
* [38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International conference on machine learning_, pages 10347-10357. PMLR, 2021.
* [39] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In _European conference on computer vision_, pages 516-533. Springer, 2022.
* [40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [43] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3124-3134, 2023.
* [44] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022.
* [45] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin D Haeffele, and Yi Ma. White-box transformers via sparse rate reduction: Compression is all there is? _arXiv preprint arXiv:2311.13110_, 2023.
* [46] Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin Haeffele, and Yi Ma. White-box transformers via sparse rate reduction. _Advances in Neural Information Processing Systems_, 36, 2023.
* [47] Yaodong Yu, Tianzhe Chu, Shengbang Tong, Ziyang Wu, Druv Pai, Sam Buchanan, and Yi Ma. Emergence of segmentation with minimalistic white-box transformers. In _Conference on Parsimony and Learning_, pages 72-93. PMLR, 2024.
* [48] John Zarka, Louis Thiry, Tomas Angles, and Stephane Mallat. Deep network classification by scattering and homotopy dictionary learning. _arXiv preprint arXiv:1910.03561_, 2019.
* [49] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12104-12113, 2022.

* [50] Bowen Zhang and Geoffrey Ye Li. White-box 3d-omp-transformer for isac. _arXiv preprint arXiv:2407.02251_, 2024.
* [51] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 633-641, 2017.

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_EMPTY:16]

**Visualization of self-attention maps of crate-\(\alpha\).** We provide visualization of attention maps of crate-\(\alpha\) in Fig. 7.

**Visualization of loss curves.** We visualize the training loss curves of the four models, including crate and its three variants, in Fig. 8. We visualize the training loss curves of crate-\(\alpha\)-Base with different patch sizes in Fig. 9. In Fig. 10, we also visualize the training loss curves of models trained with efficient scaling strategy described in Section 4.4 in the main paper.

Figure 8: Training loss curves of different model architectures (mentioned in Fig. 1 in the main paper) on ImageNet-21K. The patch size is 32 for all four models shown in this figure. (+O: +overcomplete dictionary, +D: +decoupled dictionary, +R: +residual connection.)

Figure 7: We visualize the self-attention maps of the crate-\(\alpha\) Base model using \(8\times 8\) patches trained using classification. Similar to the original CRATE [47], our model also demonstrates the capability to automatically capture the structural information of objects. For each row, the original image is displayed on the left, while the corresponding self-attention maps are shown on the right. The number of self-attention maps corresponds to the number of heads in the crate-\(\alpha\) model.

Figure 10: Comparing training loss curves when using the efficient scaling strategy. The blue curve corresponds to the crate-\(\alpha\)-Large/32 model (in the pre-training stage). After pre-training the crate-\(\alpha\)-Large/32, we further fine-tune it with smaller patch sizes (longer token length), including patch size 14 (orange curve) and patch 8 (green curve).

Figure 9: Comparing training loss curves across crate-\(\alpha\)-Base with different patch sizes.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answ and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly outline the primary contributions and scope of the paper. For specific details, refer to abstract and Sections 1, which align with the claims stated. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We discuss the limitations of the work in section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide details of reproducing the experimental results in Section 4 and the appendix. Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We use public datasets and will also release the code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide details of reproducing the experimental results in Section 4 and the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We evaluate our method on a large public validation dataset, which is representative. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details about the training compute resources in Section 4. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: we follow the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: we discuss them in Section 5. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work has no potential risk for misuse.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All assets used in the paper are properly credited. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: we haven't introduced new assets in this work. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.