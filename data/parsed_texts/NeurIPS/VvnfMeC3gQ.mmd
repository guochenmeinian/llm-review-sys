# RevColV2: Exploring Disentangled Representations in Masked Image Modeling

Qi Han

MEGVII Technology

Beijing, China

hqer@foxmail.com

&Yuxuan Cai

MEGVII Technology

Beijing, China

larryx.tsai@gmail.com

Equal contribution

Xiangyu Zhang

MEGVII Technology

Beijing, China

zhangxiangyu@megvii.com

Equal contributionCorresponding author. This work is supported by The National Key Research and Development Program of China (No. 2017YFA0700800).

###### Abstract

Masked image modeling (MIM) has become a prevalent pre-training setup for vision foundation models and attains promising performance. Despite its success, existing MIM methods discard the decoder network during downstream applications, resulting in inconsistent representations between pre-training and fine-tuning and can hamper downstream task performance. In this paper, we propose a new architecture, RevColV2, which tackles this issue by keeping the entire autoencoder architecture during both pre-training and fine-tuning. The main body of RevColV2 contains bottom-up columns and top-down columns, between which information is reversibly propagated and gradually disentangled. Such design enables our architecture with the nice property: maintaining disentangled low-level and semantic information at the end of the network in MIM pre-training. Our experimental results suggest that a foundation model with decoupled features can achieve competitive performance across multiple downstream vision tasks such as image classification, semantic segmentation and object detection. For example, after intermediate fine-tuning on ImageNet-22K dataset, RevColV2-L attains 88.4% top-1 accuracy on ImageNet-1K classification and 58.6 mIoU on ADE20K semantic segmentation. With extra teacher and large scale dataset, RevColv2-L achieves 62.1 AP\({}_{box}\) on COCO detection and 60.4 mIoU on ADE20K semantic segmentation.

## 1 Introduction

Pre-trained vision foundation models attract more attention in vision community [1; 2; 3; 4]. A key component in pre-training is how to learn generalizable features which meet the demands of various visual applications [5]. Typical series of methods focus on self-supervised learning, such as contrastive learning [6; 7] and masked image modeling (MIM) [8; 9]. The latter obtains promising results recently in most scenarios by learning occlusion invariant features [10], and becomes a commonly used approach in vision pre-training especially for large models [11; 4; 12; 13].

A typical MIM method, masked autoencoders (MAE) [8], employs an encoder to embed the masked images into semantic features and a decoder to reconstruct unseen patches, as shown in Figure 1 (b). Under such pre-training paradigm, features are rich in low-level information in both input and output. The semantic features, which are desired for downstream tasks, are reserved inside network. A common method to utilize such semantic feature is to manually partition the encoder and decoder based on the amount of semantic information in features and discard the decoder during downstream fine-tuning [8; 14; 15; 16]. Even so, discarding parts of the pre-trained network could incur information loss when transferring to downstream visual tasks. Existing works alleviate this defect by utilizing encoder only architectures [9; 17] and jointly modeling the un-masked patches and mask tokens. Even with extra computation cost in pre-training, these methods often show inferior generalization abilities, because the low-level information used for reconstructing images and the semantic information still appears in an entangled form during pre-training, as shown in Figure 1 (c).

In this paper, we tackle this problem in terms of architecture design. Rather than discarding the decoder, we keep the entire autoencoder architecture in both pre-training and fine-tuning. Similar architecture has proven to be successful in masked language modeling [18]. Nevertheless, vision tasks are naturally different from language tasks, due to their inconsistent output space between pre-training and fine-tuning [19; 20]. In order to obtain better transfer ability as well as keep the unified encoder-decoder architecture, it is important to separate the low-level and semantic information during the image reconstruction process in pre-training. RevCol [1] is a pioneer that combines the idea of reversible network [21; 22] and multi-column architecture [23; 24] to learn disentangled representation in label-guided supervised pre-training. A straightforward attempt is directly combining RevCol with the decoder used in MAE to perform MIM pre-training. However, in order to reconstruct raw images, both low-level and semantic information is needed for reasoning the unseen content and detail appearance, resulting in entangled information. This not only harms the downstream tasks but also destroys the disentangled learning objective of RevCol.

As shown in Figure 2, we re-design the architecture of RevCol to fit the MIM pre-training target. The new architecture contains a bottom-up reversible column encoder and a top-down reversible column decoder. The bottom-up columns and top-down columns are totally symmetric with masked images and encoder embedding as input. During MIM pre-training, the raw image reconstruction loss is connected to the end of the last column in decoder. Hence low-level information primarily sinks to the bottom level and semantic information moves upwards to other stages based on lossless propagation, as shown in Figure 1 (d). Between bottom-up columns and top-down columns, reversible connections are added to ensure the disentangle feature learning object of the whole network. Benefited from the disentangled representations, during downstream fine-tuning, features in the last top-down columns can be quickly adapted to various tasks. Thus the unified architecture does not have to discard the decoder network3 and avoids the mixture of low-level and semantic information, fully exploring the pre-training abilities. The re-designed new architecture is named _RevColV2_.

Figure 1: Illustration of the information distribution in different pre-training schemes. (a): low-level information is gradually changed to be semantic in supervised pre-training; (b) and (c): low-level information firstly changes to be semantic then recovers to be low-level or entangled in MAE and BEiT pre-training; (d): low-level and semantic information is gradually disentangled in RevColV2 architecture with MIM pre-training.

We build various RevColV2 models with different number of parameters and computation budgets and evaluate them on downstream ImageNet [25] image recognition, ADE20K [26] semantic segmentation and COCO [27] object detection after MIM pre-training. The experimental results show consistent improvements over RevCol(V1). Compared with other counterparts, RevColV2 achieves comparable performance to state-of-the-art models pre-trained on pure ImageNet-1K dataset, verifying the effectiveness of the new RevColV2 architecture. After intermediately fine-tuning on ImageNet-22K dataset, RevColV2-L achieves 88.4% top-1 accuracy on ImageNet-1K and 58.7 mIoU on ADE20K segmentation. Moreover, we design a new pre-training scheme which joint modeling the masked semantic features in the top-level, as well as the low-level image pixels in the bottom. Under such pre-training scheme and large scale dataset Laion400M[28], RevColv2-L achieves 62.1 AP\({}_{box}\) on COCO detection. In ADE20K segmentation, RevColv2-L reaches 60.4 mIoU with multi-scale test. We also conduct analytical experiments showing that RevColV2 with bottom-up reversible columns and top-down reversible columns can learn disentangled representation in MIM pre-training. This property benefits RevColV2 to share unified architecture between pre-training and fine-tuning, fully exploiting the potential of vision pre-training.

## 2 RevCol V2

In this section, we introduce the newly designed RevColV2, which is a pure isotropic transformer architecture [29]. The core idea in RevColV2, learning disentangled representations during MIM pre-training, is accomplished by the proposed symmetrical reversible encoder-decoder columns and the unified architecture in pre-training and fine-tuning.

### Preliminary

RevCol [1] is a reversible column architecture which learns disentangled representation in supervised learning. The main body of RevCol is composed of multiple subnetworks named columns respectively. Each column contains several basic residual blocks and could be partitioned into four levels with corresponding feature maps. Between each column, reversible connections are introduced to keep lossless information propagation. In Equation 1, we summarize the forward and inverse propagation function in RevCol.

\[\begin{split} Forward&:x_{i}^{l}=\bm{F}_{i}^{l}(x_{i -1}^{l},x_{i+1}^{l-1})+\gamma x_{i}^{l-1}\\ Inverse&:x_{i}^{l-1}=\gamma^{-1}[x_{i}^{l}-\bm{F}_{i} ^{l}(x_{i-1}^{l},x_{i+1}^{l-1})],\end{split}\] (1)

where \(x_{i}^{l}\) denotes feature maps of the \(i\)-th level in \(l\)-th column; \(\bm{F}_{i}^{l}\) denotes an arbitrary non-linear operation in \(i\)-th level, analogous to those residual functions in standard _ResNets_; \(\gamma\) is a reversible operation (_e.g._ channel-wise scaling), whose inverse is denoted by \(\gamma^{-1}\). The \(i\)-th level takes the features of the lower level at the same column \(x_{i-1}^{l}\) and the upper level at the previous column \(x_{i+1}^{l-1}\) as input. These two features are fused together and passed through a stack of building blocks. After that, feature \(x_{i}^{l-1}\) are added with reversible operation \(\gamma\) to get the final output. Given the features within one column, we can compute the features in other columns recursively during forward and backward propagation according to Equation 1. At the back-propagation, we can reconstruct the required activations _on the fly_ from the last column to the first, which means we only need to maintain activations from _one_ column in memory during training.

With this reversible property, RevCol can learn disentangled representations in supervised pre-training: the most semantic information is maintained in the last level according to the class label target, and the irrelevant low-level information is kept in other levels based on lossless propagation. In RevColV2, we inherit the RevCol design paradigm but make some improvements for MIM pre-training.

### Symmetrical Encoder and Decoder with Reversible Columns

Figure 2 gives a sketch of the RevColV2 top-level architecture design. The bottom-up column encoder follows RevCol [1] in macro design. The input image is first split into non-overlapping patches by a patch-embedding module. In MIM pre-training, the image patches are randomly masked and the un-masked patches are input into each bottom-up column. The forward and inverse function still follows Equation 1, but we change the operation inside \(F\). We select hardware-friendly vanilla transformer block [29] which includes pre-LayerNorm, Self-Attention and Feed-Forward network (FFN) as the basic building blocks in each column. The blocks are evenly split into four levels for

easy implementation of downstream tasks. Two input feature maps of each level are first normalized by a LayerNorm module and then summed together. Consider one of the input \(x_{i+1}^{l-1}\) is from the previous column, we add another linear transformation on this inter-column branch to project the input space into current column's, as shown in Figure 2.

The top-down column decoder is symmetric to the bottom-up column encoder. Similar as MAE [8], output features of the last bottom-up column are first normalized and concatenated with learnable mask tokens, then input into the top-down columns. As the information propagation is in the opposite direction, the input features of each level are also opposite to the encoder. Equation 1 becomes:

\[\begin{split} Forward&:x_{i}^{l}=\bm{F}_{i}^{l}(x_{ i+1}^{l},x_{i-1}^{l-1})+\gamma x_{i}^{l-1}\\ Inverse&:x_{i}^{l-1}=\gamma^{-1}[x_{i}^{l}-\bm{F}_{i}^{l }(x_{i+1}^{l},x_{i-1}^{l-1})].\end{split}\] (2)

Two inputs of \(\bm{F}_{i}^{l}\) becomes \(x_{i+1}^{l}\) (feature of the upper level in current column) and \(x_{i-1}^{l-1}\) (feature of the lower level in previous column). For the input (to the highest level) of each top-down column, \(x_{i+1}^{l}\) is actually the last level's output feature of the encoder, which follows the design of image patches input to each bottom-up column. At the lowest level of the last top-down column, a pixel-level loss is add to reconstruct the unseen image patches. Obviously, the top-down column is also reversible. Apart from the input, all other calculations remain the same as the encoder, and that's why we describe the encoder and decoder as fully-symmetrical.

Compared with existing architectures [1, 29], the symmetrical reversible column architecture in MIM pre-training has the following advantages:

* The disentangled feature learning object in RevCol is no longer restricted to supervised learning. Although the image reconstruction loss at the end of decoder requires mostly low-level information, the semantic information can still be maintained in other levels and gradually refined through reversible propagation. Therefore, we can learn disentangled representations in MIM pre-training without labels.
* Low-level and semantic information is retained in both bottom-up columns and top-down columns. Thus there is no need to discard parts of the network during fine-tuning. The bottom-up encoder and top-down decoder can serve as a unified architecture that yields consistent representation during pre-training and fine-tuning.

Figure 2: The pipeline of RevColV2 is a unified architecture between pre-training and fine-tuning. The bottom-up columns inherit the design of RevCol with masked image patches as inputs. In the pre-training, the top-down columns receive the outputs of bottom-up columns along with the mask tokens, and reconstruct unseen raw patches at the lowest level of the last column. In the fine-tuning, the output features of top-down columns are selectively used in downstream tasks, such as classification task using the top level features.

### The Unified Architecture between Pre-training and Fine-tuning

**MIM Pre-training.** As shown in Figure 2, in MIM pre-training, masked image patches are fed into the bottom-up columns and reconstruct unseen patches through top-down columns. At the lowest level of the last top-down column, we use mean-square error (MSE) loss to reconstruct raw images, similar to [8]. As described in Section 2.2, the low-level information is mainly gathered in the bottom levels (yellow regions in Figure 1(d)) with the constraint of MSE loss. Under the constraint of lossless propagation, the semantic information is accordingly decoupled to the top levels (blue regions in Figure 1(d)). Under such circumstances, the top-down column decoder learns not only reconstruction details, but also semantic features, indicating that downstream tasks can directly take advantage of these decoder outputs.

**Joint Pre-training.** Considering the semantic information will be gathered in the top levels during MIM training, we make a further step. We _explicitly_ joint model the masked semantic features in the top-level, as well as the low-level image pixels in the bottom. Specifically, we introduce CLIP [30] as as an extra teacher to model the semantic features and use cosine similarity as loss function. We also keep the pixel reconstruction MSE loss. Thanks to the disentangled feature in the last column, we can apply two different loss separately at the top and bottom. Our method is different from raw pixel reconstruction methods like MAE [8] or mask distillation works like MaskDistill [31], which modeling homogeneous features at the output of the network.

**Downstream Fine-tuning.** Rather than discard the decoder during fine-tuning, we leverage the entire encoder-decoder architecture. We come up with two fine-tuning methods in downstream tasks, according to different characteristics. For classification tasks, which require highly semantic features only, we apply classification heads on top of the last top-down column. Since the whole network is optimized by raw pixel reconstruction pre-training target, low-level information sinks to the bottom level features and semantic information is preserved at the top. For dense prediction tasks, which require of both semantic and low-level information, we take all level's features of the last top-down column, then directly connect them to task oriented dense prediction heads.

Compared with exists autoencoder architectures [8; 14; 32; 33; 34] that have to drop the decoder in downstream applications, the unified RevColV2 architecture has consistent representations during pre-training and fine-tuning, fully taking advantage of vision pre-training. Meanwhile, encoder only architectures [9; 17; 4; 31] often entangle the low-level and semantic information in the final output. However, RevColV2 can learn disentangled representations during pre-training, naturally avoids these problems. Compared with CNN-based backbones, ViT backbones are cumbersome to transfer in dense prediction tasks, often with an adapter like ViT-Adapter [35] or SimpleFPN in ViTDet [36]. Thanks to the multi-level embedding output, RevColV2 is free from any extra feature pyramid structures (eg, FPN [37], BiFPN[38], etc) or adapters. We simply interpolate the output features in the last column to multiple resolutions, then leverage the hierarchical embedding in dense prediction tasks. The simple and effective solution for dense prediction tasks makes the ViT-based RevColV2 easy to transfer, like previous CNN-based backbones.

### Model Variants

We provide two variants of RevColV2 models, **B**ase and **L**arge, as shown in Table 1. The depth of bottom-up columns and top-down columns are fixed to 12 and 4. The number of bottom-up columns and top-down columns is changed from 3 to 4 according to different model sizes. The model parameters include the whole bottom-up columns and top-down columns. However, in the downstream task such as image classification, not all levels are involved in computation (as shown in Figure 2), such the number of parameters is slightly lower (as shown in Table 2). We do not use intermediate supervision for each column as it needs careful tuning.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \#BU columns. & BU depth & \#TD columns. & TD depth & dim & head dim & Params. & FLOPs \\ \hline Base & 3 & 12 & 3 & 4 & 416 & 32 & 101M & 19G \\ Large & 4 & 12 & 4 & 4 & 672 & 56 & 342M & 67G \\ \hline \hline \end{tabular}
\end{table}
Table 1: RevColV2 architecture configurations for different model sizes. FLOPs are measured on classification task with \(224^{2}\) resolution input. BU and TD are short for bottom-up and top-down.

## 3 Experiments

### MIM Pre-training

#### 3.1.1 Pre-training Details

We pre-train RevColV2 on ImageNet-1K [25] dataset. Hyper-parameters generally follow [8]. The mask ratio is set as 75% with random sampling strategy and the reconstruction target is the normalized raw pixel from the original image. We pre-train 1600 epochs for RevColV2 models. The pre-training image size is 224\({}^{2}\) and the pre-training optimization parameters are: batch size 4096, base learning rate 1.5e-4 for 256 batch-size and linear scaled up, AdamW with weight decay 0.05. We do not use stochastic depth strategy in pre-training. More details can be found in supplementary material.

#### 3.1.2 ImageNet Classification

**Setup.** For image classification, we evaluate top-1 accuracy on ImageNet-1K [25]. We initialize weights using MIM pre-trained models, and fine-tune on ImageNet-1K with class label, similar to [8; 12; 9]. To fully exploit the potential of RevColV2, we intermediately fine-tune the models on ImageNet-22K [39] following [34; 12] after MIM pre-training. The intermediate fine-tuning hyper-parameters are almost the same as [12] and shown in supplementary material.

**Results.** Table 2 shows the results on ImageNet-1K classification. The MIM pre-train epochs for RevColV2 is 1600, and other methods are reported with their longest schedules. RevColV2-B achieves **84.7** top-1 accuracy with pure ImageNet-1K data, outperforms ViT [29] architecture pre-trained with MIM [8; 9; 14; 40; 32] by a large margin using only raw pixels as reconstruction target. As a pure transformer isotropic architecture, RevColV2-B also achieves comparable performance with state-of-the-art hierarchical architectures, _i.e_. RevColV2-B reaches higher performance than SwinV2-B[11], HorNet\({}_{\mathrm{GF}}\)-B [44] and other counterparts in Table 2. For large size models, RevColV2-L with **86.3%** top-1 accuracy outperforms ConNeXt V2 [12], MAE [8], and CAE [14] counterparts.

We initialize RevColV2 models with the MIM pre-trained model weights and then use larger dataset ImageNet-22K and supervised training methods to test the scaling up ability. Under this training

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Model & Size & Target & Params & FLOPs & FT \\ \hline \multicolumn{6}{l}{_ImageNet-1K pre-train:_} \\ BEIT-B [9] & \(224^{2}\) & DALL-E & 87M & 18G & 83.2 \\ MAE-B [8] & \(224^{2}\) & Pixel & 87M & 18G & 83.6 \\ CAE-B [14] & \(224^{2}\) & DALL-E & 87M & 18G & 83.9 \\ SdAE-B [40] & \(224^{2}\) & EMA & 87M & 18G & 84.1 \\ MaskFeat-B [32] & \(224^{2}\) & HOG & 87M & 18G & 84.0 \\ ConvNeXt-B [41] & \(224^{2}\) & Pixel & 89M & 15G & 83.8 \\ SimMM-B [17] & \(224^{2}\) & Pixel & 88M & 16G & 84.0 \\ HiViT-B [42] & \(224^{2}\) & Pixel & 66M & 84.2 \\ DeiT III-B [43] & \(224^{2}\) & Label & 87M & 18G & 83.8 \\ HorNet\({}_{\mathrm{GP}}\)-B [44] & \(224^{2}\) & Label & 88M & 16G & 84.3 \\ SwinV2-B [11] & \(224^{2}\) & Label & 88M & 20G & 84.2 \\ ConvNeXt V2-B [12] & \(224^{2}\) & Pixel & 89M & 15G & 84.9 \\ RevCol-B [1] & \(224^{2}\) & Label & 138M & 17G & 84.1 \\ \hline RevColV2-B & \(224^{2}\) & Pixel & 88M & 19G & **84.7** \\ DeiT III-L [43] & \(224^{2}\) & Label & 304M & 62G & 84.9 \\ BEIT II-L [9] & \(224^{2}\) & Pixel & 307M & 62G & 85.2 \\ MAE-L [8] & \(224^{2}\) & Pixel & 307M & 62G & 85.9 \\ CAE-L [14] & \(224^{2}\) & DALL-E & 307M & 62G & 86.2 \\ MaskFeat-L [32] & \(224^{2}\) & HOG & 307M & 62G & 85.7 \\ SimMIM-L [17] & \(224^{2}\) & Pixel & 197M & 35G & 85.4 \\ ConvNeXt V2-L [12] & \(224^{2}\) & Pixel & 198M & 34G & 85.8 \\ RevColV2-L & \(224^{2}\) & Pixel & 327M & 67G & **86.3** \\ \hline \hline \end{tabular}
\end{table}
Table 2: ImageNet-1K classification results for various sizes models. The left table shows the end-to-end fine-tune results after MIM pre-training, and the right table shows the results after intermediate fine-tuning on ImageNet-22K.

schedule, RevColV2-B reaches **87.5%** top-1 accuracy on ImageNet-1K with \(512^{2}\) input resolution. For large size models, the performance of RevColV2 achieves **88.4 %** top-1 accuracy, surpassing hierarchical counterparts of SwinV2 [11] and ConvNeXt V2 [12]. Compared with over RevCol(V1) [1], all sizes RevColV2s show consistent and significant improvements on classification tasks, further illustrating the effectiveness of our design.

#### 3.1.3 Semantic Segmentation

**Setup.** For semantic segmentation tasks, we evaluate RevColV2 backbones on ADE20K benchmarks [26] with _UperNet_[46] and _Mask2Former_[47] framework. We interpolate the position embedding to a fixed input size for both bottom-up and top-down columns. Following previous works [11; 12], we initialize weights using ImageNet-1k classification fine-tuned models. Training hyper-parameters are available in supplementary material.

**Results.** Table 3 shows the head-to-head comparison results on ADE20K semantic segmentation with various backbones and UperNet segmentation head. RevColV2 models gain competitive performance over single-scale and multi-scale mIoU across different pure transformer and CNN architectures. RevCol-B/L pre-trained on ImageNet-1K achieve **52.3** and **54.4** mIoU on ADE20K with \(512^{2}\) input resolution respectively, outperforming other counterparts such as ConvNeXt V2 [12]. After intermediate fine-tuning on ImageNet-22K, RevColV2-B/L reach **53.9** and **55.7** mIoU on ADE20K benchmark with \(640^{2}\) resolution, exceeding other competitors such as Focal transformer [52] and CSwin [53]. With stronger segmentation framework Mask2Former [47], our RevCol-B/L backbones achieve **55.8** and **58.6** mIoU, further demonstrating the effectiveness of RevColV2 backbones.

#### 3.1.4 Object Detection

**Setup.** For object detection and instance segmentation task, we evaluate RevColV2 backbones on COCO [27] dataset with _Mask R-CNN_[55] and _Cascade Mask R-CNN_[56] framework. We follow

\begin{table}
\begin{tabular}{l c c c} \hline \hline Backbone & \multicolumn{1}{c}{AP\({}_{box}\)} & \multicolumn{1}{c}{AP\({}_{mask}\)} & Params \\ \hline _with Mask R-CNN:_ & & & \\ MAE-B [8] & 49.8 & 44.3 & 110M \\ MAE-B† [8] & 51.3 & 45.4 & 122M \\ RevColV2-B & **52.4** & **46.2** & 119M \\ \hline MAE-L [8] & 53.1 & 47.1 & 323M \\ MAE-L† [8] & 53.3 & 47.4 & 330M \\ RevColV2-L & **54.0** & **47.8** & 363M \\ \hline \hline \end{tabular}
\end{table}
Table 4: Object detection and instance segmentation results on COCO dataset. † means using ViT-Adapter [35] networks. † means that model pre-trained on ImageNet-22K dataset and † denotes using additional distillation teacher in pre-training.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Backbone & mIoU (ss.) & mIoU (ms.) & Params \\ \hline _ImageNet-1K:_ & & & \\ MAE-B [8] & 48.1 & N/A & 156M \\ CAE-B [8] & 50.2 & N/A & 156M \\ PeCo-B [48] & 48.5 & N/A & 156M \\ Swin-B [49] & 48.1 & 49.7 & 121M \\ ConvNeXt-B [41] & 49.1 & 49.9 & 122M \\ ConvNeXt V2-B [12] & N/A & 52.1 & 122M \\ InternImage-B [50] & 50.8 & 51.3 & 128M \\ RevCol-B [1] & 49.0 & 50.1 & 122M \\ RevColV2-B & **51.3** & **52.3** & 121M \\ \hline BEIT [9] & 53.3 & N/A & 421M \\ MAE-L [8] & 53.6 & N/A & 421M \\ ConvNeXt V2-L [12] & 53.2 & 53.7 & 235M \\ RevColV2-L & **54.0** & **54.4** & 399M \\ \hline \hline \end{tabular}
\end{table}
Table 3: Semantic segmentation result on ADE20K dataset with UperNet and Mask2Former framework. _M2F_ denotes using Mask2Former framework. We report mIoU with single/multi-scale test.

the setting in [36] that using window attention in RevColV2 backbones. We train models with \(1024^{2}\) resolution crops using large scale jittering augmentations [57].

**Results.** Table 4 left shows the experiment results for RevColV2 backbones compared with MAE [8] baseline using Mask R-CNN detector. For MAE baseline, we reproduce the results using the same training configuration optionally with ViT-Adapter [35]. RevColV2 series achieve **52.4** and **54.0** box AP for base and large models, outperforming MAE series. In Table 4 right, we compare RevColV2 backbones with other architectures using Cascade Mask R-CNN detector. RevColV2-B achieve **55.2** box AP and **47.9** mask AP, outperforming RevCol(V1)-B [1], ViTDet-B [36] and other competitors.

Note that in all dense prediction tasks, we do not use any extra feature pyramid structures (eg, FPN, BiFPN, etc.) or adapters like ViT-Adapters in RevColV2 backbones implementation.

### Joint Pre-training and Data Scaling

#### 3.2.1 Pre-training Details

In joint pre-training, we use OpenCLIP-L as the teacher to represent the semantic features similar to MaskDistill and EVA. Except for the additional teacher, we use a larger dataset Laion400M[28], which contains about 400M unlabeled images in pre-training. Note that we do not use datasets such as COCO, ADE20K, Object365, etc. in pre-training to avoid artificial fitting to specific distribution (this is different from EVA-02 which uses a merged dataset that has overlapped data in the downstream task). We use 800 ImageNet-1k epochs on Laion400M dataset and then 300 epochs on ImageNet-1k dataset during pre-training.

#### 3.2.2 Results

Then we evaluate our model on downstream tasks such as ImageNet1K classification, COCO detection with cascade Mask-RCNN, and ADE20K semantic segmentation with Mask2Former. The newly trained RevColv2-L achieves 87.7% Top-1 accuracy in ImageNet-1k classification with \(224\times 224\) input resolution. The larger dataset and the extra teacher lead to better performance compared with purely IN-1k MIM pre-training (86.3%) and IN-1k MIM + IN-22k intermediate fine-tuning (87.4%). As shown in Table 5, the performance gain is more prominent on dense prediction tasks. RevColv2-L achieves **62.1** box AP and **52.3** mask AP in COCO instance detection and segmentation. In ADE20K segmentation, RevColv2-L reaches **60.4 mIoU** with multi-scale test.

### Analysis

Experiments in this section are based on basic MIM pre-training.

**RevColV2 can learn disentangle representation.** Linear probing is a useful tool to evaluate the sparsity of features. We assume the semantic information tends to be more sparse, while the low-level information is more abundant. So, we evaluate the linear probing accuracy of each level to visualize the distribution of semantic and low-level information. We construct two experiments: 1) RevCol(V1) [1]+MAE [8] baseline in which the RevCol(V1) encoder has 3 columns and MAE decoder has 8 ViT blocks; 2) RevColV2 with 3 bottom-up columns encoder and 3 top-down columns decoder in which each column contains 12 blocks. The number of model parameters is maintained at the same level. Figure 3 shows the evaluation results on ImageNet-1K. The output level of the baseline encoder only reaches 47.2% accuracy, which is far less than the previous level 60.1%. The

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Params} & \multicolumn{3}{c}{Dataset} & \multicolumn{3}{c}{COCO (w/o Obj365)} & \multicolumn{3}{c}{ADE20K} \\ \cline{3-10}  & & Data & teacher & Detector & AP\({}_{box}\) & AP\({}_{mask}\) & Segmenter & mIoU & +ms \\ \hline MaskDistill-L & 0.3 G & ImageNet1K & CLIP & - & - & - & UperNet & 56.5 & - \\ BEiTv2-L & 0.3 G & ImageNet2K & CLIP & - & - & - & UperNet & 57.5 & - \\ \hline EVA-02-L & 0.3 G & merged 33M & EVA01-CLIP Cascade & 62.3 & 53.8 & UperNet & 60.1 & - \\ RevCol-H & 2.1 G & private 168M & semi-labeled & HTC++ & 61.1 & 53.0 & Mask2Former & 60.4 & 61.0 \\ RevColV2-L & 0.3 G & Laion400M+IN-1K & OpenCLIP Cascade & 62.1 & 53.2 & Mask2Former & 59.5 & 60.4 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results of the data scaling. RevColV2-L with larger pre-training data and additional teacher achieves comparable performance than 2.1G parameter RevColV1-H and EVA-02-L.

baseline method will entangle the low-level and semantic information during column propagation, because the MAE decoder needs both low-level and semantic features to reconstruct unseen patches. Conversely, RevColV2 with reversible top-down columns can learn the disentangled representation as described in Section 2. As shown in Figure 3, the last level of encoder and decoder reaches high linear probing accuracy (64.9% and 62.5%), and the accuracy decreases at the lower levels. Thus low-level and semantic information lies in a disentangled manner between the bottom and top levels in the last column.

We also show the linear probing accuracy of various sizes RevColV2 models in supplementary material. The separated semantic information significantly boosts the linear probing accuracy compared to other encoder only methods such as SimMIM [17].

**RevColV2 matters in unified representation fine-tune.** We experimentally verify the effectiveness of RevColV2 with unified architecture between pre-training and fine-tuning. We first compare the bottom-up encoder only variant and the entire autoencoder variant of RevColV2 on ImageNet-1K. The latter has about **0.9%** (83.8% _v.s._ 84.7%) top-1 accuracy gain, showing the top-down columns are critical in downstream fine-tuning. To eliminate the influence of model capacity (#params, and FLOPs), we also build one experiment with the same number of parameters and FLOPs for comparison. The experimental results show the accuracy of this encoder only variant (83.9%) is still lower than the entire auto-encoder variant(84.7%).

Similarly, we conduct experiments for ViT [29] + MAE [8] that without dropping decoder in the fine-tuning stage. The accuracy on ImageNet-1K drops about **0.4%** (from 83.6% to 83.2%). Give more model capacity does not increase the performance, indicating that the vanilla ViT decoder is useless in downstream fine-tuning.

We also make an ablation that only utilize the encoder's pre-trained weights while initializing the decoder weights randomly with RevColV2-B. This variant achieves 84.4% (-0.3%) top-1 accuracy on ImageNet-1K dataset and 50.7 (-0.6) mIoU on ADE20K dataset, with only ImageNet-1K MIM pre-trained encoder weights. These experimental results draw the same conclusion that the pre-trained decoder is necessary for RevColV2.

### Ablation Study

Experiments in this section are based on basic MIM pre-training.

**Depth of top-down decoder columns** is analyzed similar to the decoder depth analysis in MAE [8], as shown in Table 6 (a). In MAE [8], the fine-tuning accuracy does not vary much over the decoder depth. However, the performance of RevColv2 drops with deeper decoder. This is because the top-down column decoder also takes up model parameters as we do not discard decoder during fine-tuning. We keep the total parameter the same, and deeper decoder will lead to shallower (narrower) encoder, which could give rise to degraded performance. This experiment also explains why we use shallower decoder (4 blocks decoder compared to 12 blocks encoder) in our RevColV2 variants.

**Different type of decoder**. The decoder in RevColV2 is a symmetric architecture with encoder, that includes multiple reversible top-down columns. To verify the effectiveness of these top-down columns, we compare this design with other variants: MAE decoder with a stack of ViT blocks; single

Figure 3: Linear probing accuracy of each levels on ImageNet-1K after pre-training 300 epochs. The left is RevCol+MAE baseline, and the right is RevColV2 with reversible multi-column decoder.

column decoder with shortcuts from encoder similar to UNet [58]; top-down multi-columns without shortcuts and reversible connections. Experiment results in Table 6 (c) draw the same conclusion with Section 2 that reversible column decoder achieves the best performance. This is because top-down reversible multi-columns can help feature disentangling while others not, as described in Section 2 and the analysis in Section 3.3.

**Different pre-training schedules**. We also conduct experiments with shorter schedules compared with the default 1600 epochs training, _i.e._ 300/800 epochs. Table 6 (b) shows the fine-tuning top-1 accuracy on ImageNet-1K. The results indicate larger models may need longer iterations (+0.6% from 300 to 1600 epochs for base size model, +1.1% from 300 to 1600 epochs for large size model) in MIM pre-training to fully activate the potential of models.

## 4 Related Works

**Disentangled feature learning.** Disentangled representation refers to separating the factors of variation [5]. Traditional methods [59; 60; 61] mainly focus on generative models to learn disentangled representation. Hinton proposes GLOM [23] to build a general network that learning the part-whole hierarchies. Cai _et al._[1] integrate the idea of GLOM with reversible networks [22; 21] to learn disentangle representation in supervised learning paradigm, namely RevCol. Although RevCol can learn disentangled representation, it needs large-scale labeled data to maintain this property that can not directly benefit from MIM. In this paper, We seek the advances of re-design RevCol with MIM to further enhance the representation abilities while maintaining the decoupled feature learning ability.

**Masked image modeling.** Self-supervised learning has a long history in computer vision research community [62; 63; 64; 65; 66; 7]. The recent method masked image modeling (MIM) erases the masked image patches and then predicts the unseen contents, a representative of which is masked autoencoders [8]. Following MIM pipeline, researchers make efforts on designing different reconstruction targets, such as DALL-E [9; 14], HOG [32], VQGAN [48], frequency [33; 67], to learn occlusion invariant features [10].

Some methods try to use consistent architecture during pre-training and fine-tuning, such as SimMIM [17] which utilizes an encoder only network. As a result, low-level and semantic information is entangled at the end of the network, resulting in degraded performance, especially for the linear probing accuracy. The above methods usually directly adopt vanilla ViT as backbones, ignoring the mutual promotion between architectures and MIM pipelines. Recent works [12; 68] aim to co-design the architecture with MIM, and focus on the CNN architectures [41]. In this paper, we explore the new research direction: learning disentangled representation during MIM. We design a unified architecture between pre-training and fine-tuning based on RevCol [1] to learn disentangled features and naturally avoid the above information mixture problem.

## 5 Conclusion

In this paper, we design a new architecture named RevColV2 which learns disentangled representations during MIM pre-training and keep a unified autoencoder architecture when transferring to downstream tasks. RevColV2 extends the reversible columns network which is previously limited in supervised learning to MIM training, and bridges the gap between pre-training and fine-tuning. These unified architectures improve the performance across various downstream tasks, including image classification, object detection and semantic segmentation, without using additional task specific adapters. By these impressive results, we hope to stimulate more research in learning generalizable features and help foundation model pre-training.

\begin{table}

\end{table}
Table 6: Ablation results on ImageNet-1K. In different configurations of architectures, we keep the same overall FLOPs for fair comparison unless special descriptions.

## References

* [1] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li, and Xiangyu Zhang. Reversible column networks. _arXiv preprint arXiv:2212.11696_, 2022.
* [2] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers. _arXiv preprint arXiv:2210.06423_, 2022.
* [3] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _arXiv preprint arXiv:2208.10442_, 2022.
* [4] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. _arXiv preprint arXiv:2211.07636_, 2022.
* [5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. _IEEE transactions on pattern analysis and machine intelligence_, 35(8):1798-1828, 2013.
* [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [7] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* [8] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* [9] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. _arXiv preprint arXiv:2106.08254_, 2021.
* [10] Xiangwen Kong and Xiangyu Zhang. Understanding masked image modeling via learning occlusion invariant feature. _arXiv preprint arXiv:2208.04164_, 2022.
* [11] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12009-12019, 2022.
* [12] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. _arXiv preprint arXiv:2301.00808_, 2023.
* [13] Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollar, Christoph Feichtenhofer, Ross Girshick, et al. The effectiveness of mae pre-pretraining for billion-scale pretraining. _arXiv preprint arXiv:2303.13496_, 2023.
* [14] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. _arXiv preprint arXiv:2202.03026_, 2022.
* [15] Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. _Advances in neural information processing systems_, 35:35946-35958, 2022.
* [16] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. _arXiv preprint arXiv:2203.12602_, 2022.

* [17] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9653-9663, 2022.
* [18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* [19] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. _arXiv preprint arXiv:2212.02499_, 2022.
* [20] Jia Ning, Chen Li, Zheng Zhang, Zigang Geng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying output space of visual tasks via soft token. _arXiv preprint arXiv:2301.02229_, 2023.
* [21] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. _Advances in neural information processing systems_, 30, 2017.
* [22] Jorn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks. _arXiv preprint arXiv:1802.07088_, 2018.
* [23] Geoffrey Hinton. How to represent part-whole hierarchies in a neural network. _Neural Computation_, pages 1-40, 2022.
* [24] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. _IEEE transactions on pattern analysis and machine intelligence_, 43(10):3349-3364, 2020.
* [25] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [26] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 633-641, 2017.
* [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [28] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* [29] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [31] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. A unified view of masked image modeling. _arXiv preprint arXiv:2210.10615_, 2022.
* [32] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14668-14678, 2022.

* [33] Jiahao Xie, Wei Li, Xiaohang Zhan, Ziwei Liu, Yew Soon Ong, and Chen Change Loy. Masked frequency modeling for self-supervised visual pre-training. _arXiv preprint arXiv:2206.07706_, 2022.
* [34] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. _arXiv preprint arXiv:2208.06366_, 2022.
* [35] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. _arXiv preprint arXiv:2205.08534_, 2022.
* [36] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IX_, pages 280-296. Springer, 2022.
* [37] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2117-2125, 2017.
* [38] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10781-10790, 2020.
* [39] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. _arXiv preprint arXiv:2104.10972_, 2021.
* [40] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Sdae: Self-distillated masked autoencoder. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXX_, pages 108-124. Springer, 2022.
* [41] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11976-11986, 2022.
* [42] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler and more efficient design of hierarchical vision transformer. In _The Eleventh International Conference on Learning Representations_, 2023.
* [43] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In _European Conference on Computer Vision_, pages 516-533. Springer, 2022.
* [44] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. _arXiv preprint arXiv:2207.14284_, 2022.
* [45] Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan, Yukun Zhu, Alan Yuille, Hartwig Adam, and Liang-Chieh Chen. Moat: Alternating mobile convolution and attention brings strong vision models. _arXiv preprint arXiv:2210.01820_, 2022.
* [46] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _Proceedings of the European conference on computer vision (ECCV)_, pages 418-434, 2018.
* [47] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander G Schwing. Mask2former for video instance segmentation. _arXiv preprint arXiv:2112.10764_, 2021.
* [48] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. _arXiv preprint arXiv:2111.12710_, 2021.
* [49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.

* [50] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Intermimage: Exploring large-scale vision foundation models with deformable convolutions. _arXiv preprint arXiv:2211.05778_, 2022.
* [51] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11963-11975, 2022.
* [52] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. _arXiv preprint arXiv:2107.00641_, 2021.
* [53] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12124-12134, 2022.
* [54] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual representation for neon genesis. _arXiv preprint arXiv:2303.11331_, 2023.
* [55] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 2961-2969, 2017.
* [56] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6154-6162, 2018.
* [57] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2918-2928, 2021.
* [58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* [59] Guillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors of variation via generative entangling. _arXiv preprint arXiv:1210.5474_, 2012.
* [60] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In _International Conference on Learning Representations_, 2017.
* [61] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. _Advances in neural information processing systems_, 29, 2016.
* [62] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In _Proceedings of the IEEE international conference on computer vision_, pages 1422-1430, 2015.
* [63] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI_, pages 69-84. Springer, 2016.
* [64] Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and Bharath Hariharan. Learning features by watching objects move. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2701-2710, 2017.
* [65] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. _arXiv preprint arXiv:1803.07728_, 2018.

* [66] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [67] Hao Liu, Xinghua Jiang, Xin Li, Antai Guo, Deqiang Jiang, and Bo Ren. The devil is in the frequency: Geminated gestalt autoencoder for self-supervised visual pre-training. _arXiv preprint arXiv:2204.08227_, 2022.
* [68] Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, and Zehuan Yuan. Designing bert for convolutional networks: Sparse and hierarchical masked modeling. _arXiv preprint arXiv:2301.03580_, 2023.
* [69] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 646-661. Springer, 2016.

Analysis of Speed and Memory cost

Inference.The current model variants of RevColV2 introduce more latency compared with other works of the similar number of parameters and FLOPs, such as ViT. We test the inference latency of variant models in Table 7. As described RevColV1 [1], fragmented memory access takes a large part of latency. In RevColV2, we make some improvements: 1) remove the up-sample and down-sample operation in RevColV1; 2) reduce the number of total blocks; 3) hard-ware friendly architecture without hierarchy. As shown in Table 7, RevColV2 has lower latency than the V1 version during inference, but is still 1.21x higher than ViT. This is because of the large number of building blocks in RevColV2-L (about twice of ViT-L). Though we reduce the total number of blocks, the multi-column RevColv2 still requires at least 12 blocks in each column in the encoder. Shallower column leads to coarse representation which could harm the performance. On the other hand, if we make the ViT model deeper and maintain the same FLOPs, ViT-L-deeper (48 blocks) and RevColV2-L (48 blocks) have similar latency.

We also analyze the impact of batch size. We show throughput (#image/s) under the different batch size of RevColV2-L and ViT-L on a single A100 GPU.The results in Table 8 show that with the increase in batch size, the inference speed gap between RevCol-L and ViT-L is closing because the fragment memory access time can be distributed to each sample. Although the speed of RevColV2 is lower than vanilla ViT, we think it can be solved by advanced techniques.

TrainingWe make some further analysis on the per-training speed and its memory cost, and compare them with the popular used ViT-MAE baseline. We take RevColV2-B and ViT-B for comparisons. We test training speed and memory cost on a single A100 (80GB) x 8 machine, with the same data-loader (implemented for our cluster). We use our own implementation for RevColV2 and the official implementation for MAE. Table 9 shows the training cost with batch size 4096 for one epoch. To speed up training and save memory, we equip RevColV2 with Flash Attention. We only use data parallel in this testing.

Table 9 shows that the vanilla implementation of RevColV2 pre-training has a little slower (249s vs. 220s) than ViT. Equipped with FlashAttn, RevColV2 achieves comparable pre-training cost (211s vs. 220s and 42G vs. 43G). We further analyze the impact of reversible propagation. We test the pre-training cost of the Reversible version of RevColV2 (re-compute the intermediate features during backward according to the last column outputs, rather than the vanilla autograd function in PyTorch. It is the key component of reversible column networks [1]). Results show that RevColV2-B can use extremely few GPU memory (only 18G) during pre-training with a total batch size 4096. This allows RevColV2 can be pre-trained with limited resources, such as RTX3090 GPU.

In addition to the above comparison, the fragmented access of memory can be optimized by some techniques which can be further investigated in further work. Here, we give two ways that may be further studied:

* Kernel fusion. This can reduce the frequent access of the memory caused by a large number of blocks.

\begin{table}
\begin{tabular}{l c c|l c c} \hline \hline Model & FLOPs & Latency & Model & FLOPs & Latency \\ \hline RevCol-L & 39G & 61 ms & RevColV2-L & 67G & 51 ms \\ ViT-L & 62G & 42 ms & ViT-L (48 blocks) & 64G & 52 ms \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results of the inference latency, all models are tested with batch-size 32 on single A100 GPU.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & bs=16 & bs=32 & bs=64 & bs=128 & bs=256 & bs=512 \\ \hline MAE-L [8] & 730 & 754 & 786 & 811 & 820 & 823 \\ RevColV2-L [8] & 432 & 629 & 661 & 697 & 721 & 741 \\ Speedup & 0.591 & 0.834 & 0.841 & 0.859 & 0.879 & 0.900 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The throughput under different inference batch-size of RevColV2.

* Model parallel. Before the calculation of previous columns is finished, parts of the current column can be calculated in parallel. This is the nature of the multi-column network and can be further studied to speed up the inference and training.

## Appendix B More Training Details

This section gives more training details on MIM pre-training and fine-tuning on downstream tasks, such as ImageNet classification, COCO detection, and ADE20K segmentation. For ImageNet experiments, the base learning rate is based on batch size 256.

### Training Details on MIM pre-training.

We use the same setting for different sizes RevCol models on MIM pre-training. The detail hyper-parameters are shown in Table 10. Following exists works [8, 12], we do not use stochastic depth [69] and other regularization strategies in MIM pre-training.

### Details on Image22K intermediate fine-tuning.

We further intermediately fine-tune RevColV2 models on ImageNet-22K dataset. The fine-tuning details is shown in Table 11. The hyper-parameters generally follow [1, 12].

### End-to-end fine-tuning details on ImageNet-1K.

We end-to-end fine-tune RevCol variants on ImageNet-1K after MIM pre-training and intermediately fine-tuning on ImageNet-22K. Table 12 shows the detail training settings after MIM pre-training.

We also show training settings on ImageNet-1K after ImageNet-22K fine-tuning. Table 13 gives the detailed hyper-parameters.

### Details on ADE20K semantic segmentation

For semantic segmentation, we evaluate different backbones on ADE20K dataset. We fine-tune the pre-trained networks on ADE20K with 160,000 iterations. For UperNet framework [46], the learning rate is 4e-5 with batch size 16, using AdamW optimizer. The layer-wise learning rate decay rate is set as 0.65 for both base and large size models. The drop path rate is 0.1. For Mask2Former

\begin{table}
\begin{tabular}{l|l} config & value \\ \hline optimizer & AdamW \\ base learning rate & 1.5e-4 \\ weight decay & 0.05 \\ optimizer momentum & \(\beta_{1},\beta_{2}\)\(=\)\(0.9,0.95\) \\ batch size & 4096 \\ learning rate schedule & cosine decay \\ warmup epochs & 40 \\ training epochs & 1600 \\ augmentation & RandomResizedCrop \\ \end{tabular}
\end{table}
Table 10: MIM Pre-training settings.

\begin{table}
\begin{tabular}{l l r} \hline Model & Time Cost & Memory (each GPU) \\ \hline MAE-B [8] & 220 \(s\)/epoch & 43G \\ RevColV2-B [8] & 249 \(s\)/epoch & 49G \\ RevColV2-B + FlashAttn [8] & 211 \(s\)/epoch & 42G \\ RevColV2-B + FlashAttn + Reversible [8] & 240 \(s\)/epoch & 18G \\ \hline \end{tabular}
\end{table}
Table 9: The training time and memory cost comparison on a single A100 (80GB) x 8 machine.

framework [47], the learning rate is 2e-5 with batch size 16. The drop path rate is set as 0.3 and the layer-wise learning decay rate is 0.9.

### Details on COCO object detection and instance segmentation

For object detection and instance segmentation, we evaluate RevColV2 backbones with Mask R-CNN [55] and Cascade Mask R-CNN [56] detectors. We use ImageNet-1K MIM pre-trained weights as initialization and fine-tune the models with 50 epochs and a batch size of 32, learning rate 1e-4 for Mask R-CNN framework. The large scale jittering data augmentation strategy is used with scale range [0.1, 2.0]. The drop path rates for RevCOIV2 are set as 0.2 (base) and 0.3 (large) and the layer-wise learning rate decay rates are set as 0.9. For Cascade Mask R-CNN framework, we train models with 100 epochs following [36] with large scale jittering augmentation strategy. The learning rate is 1e-4 with batch size 64. We do not use soft-NMS in our experiments.

\begin{table}
\begin{tabular}{l|l} config & value \\ \hline optimizer & AdamW \\ base learning rate & 5e-4 \\ weight decay & 0.05 \\ optimizer momentum & \(\beta_{1},\beta_{2}{=}0.9,0.999\) \\ optimizer momentum & \(\beta_{1},\beta_{2}{=}0.9,0.999\) \\ layer-wise lr decay & 0.8 \\ batch size & 4096 \\ learning rate schedule & cosine decay \\ warmup epochs & 5 \\ training epochs & 90 \\ augmentation & RandAug (9, 0.5) \\ label smoothing & 0.1 \\ mixup & 0.8 \\ cutmix & 1.0 \\ drop path & 0.1 \\ head init & 0.001 \\ ema & 0.9999 \\ \end{tabular}
\end{table}
Table 11: End-to-end IN-22K intermediate fine-tuning settings.

\begin{table}
\begin{tabular}{l|l} config & value \\ \hline optimizer & AdamW \\ base learning rate & 5e-4 \\ weight decay & 0.05 \\ optimizer momentum & \(\beta_{1},\beta_{2}{=}0.9,0.999\) \\ layer-wise lr decay & 0.75 \\ batch size & 1024 \\ learning rate schedule & cosine decay \\ warmup epochs & 5 \\ training epochs & 100 (B), 50 (L) \\ augmentation & RandAug (9, 0.5) \\ label smoothing & 0.1 \\ mixup & 0.8 \\ cutmix & 1.0 \\ drop path & 0.1 \\ head init & 0.001 \\ ema & 0.9999 \\ \end{tabular}
\end{table}
Table 12: End-to-end ImageNet-1K fine-tuning settings

[MISSING_PAGE_FAIL:19]