# BIGOS V2 Benchmark for Polish ASR: Curated Datasets and Tools for Reproducible Evaluation

Michal Junczyk

Adam Mickiewicz University, Poznan, Poland, michal.junczyk@amu.edu.pl Allegro, Poznan, Poland, michal.junczyk@allegro.com

###### Abstract

Speech datasets available in the public domain are often underutilized because of challenges in accessibility and interoperability. To address this, a system to survey, catalog, and curate existing speech datasets was developed, enabling reproducible evaluation of automatic speech recognition (ASR) systems. The system was applied to curate over 24 datasets and evaluate 25 ASR models, with a specific focus on Polish. This research represents the most extensive comparison to date of commercial and free ASR systems for the Polish language, drawing insights from 600 system-model-test set evaluations across 8 analysis scenarios. Curated datasets and benchmark results are available publicly. 1 The evaluation tools are open-sourced to support reproducibility of the benchmark, encourage community-driven improvements, and facilitate adaptation for other languages.2

Footnote 1: BIGOS collection on Hugging Face

Footnote 2: BIGOS ASR eval tools on GitHub

## 1 Introduction

### Background

The Polish language is spoken by more than 50 million people worldwide. The number of available ASR systems and services, as well as speech data resources that support Polish, is systematically growing. However, the community has insufficient resources to methodically evaluate and track progress of ASR (Automatic Speech Recognition) technology for Polish. First, the available data assets are underutilized due to challenges such as accessibility, licensing, and interoperability. Secondly, there is no standardized ASR benchmark dataset. Finally, the tooling to reproduce or systematically extend evaluation scope is missing. As a result Polish ASR systems benchmarks performed so far cover limited number systems and datasets (see Appendix B.1). [15, 27, 28, 41, 17, 10] These limitations may slow the development of new systems and applications, as reliable evaluations and publicly available _leaderboards_ drive research progress and inform the public about the capabilities of AI technology. [22] The international ASR community has recognized the importance of evaluation methodologies for consistent and comparative performance assessments in ASR specifically. [2, 38, 7] and ML field in general [19, 24, 23] This calls for innovations in the management of ASR datasets and evaluation methods. [13, 40]

### Research gap

Current data curation and ASR benchmarking methods for low-resource languages such as Polish exhibit several shortcomings:* **Data utilization:** Speech datasets are often underutilized due to limited awareness or restricted accessibility.
* **Data quality:** Insufficient understanding of test sets can lead to an inaccurate representation of state-of-the-art performance.
* **Evaluation reproducibility:** Limited adoption of common benchmark sets impedes the validation and replication of research results.
* **Evaluation scope:** Ecologically valid ASR evaluations require consideration of a broader range of datasets, systems, and performance metrics to ensure comprehensive assessment.

### Contributions

1. **Benchmark datasets curation:** To address the lack of standardized ASR evaluation resources for Polish, a benchmark dataset was curated from 24 openly available sources.3 Diverse samples of both read4 and spontaneous speech5 are included. Footnote 3: BIGOS datasheets
2. **Benchmark toolchain development:** A benchmark toolchain was developed to ensure consistent ASR evaluation through standardized protocols, with flexible support for incorporating new datasets, systems, and metrics.6 Footnote 4: BIGOS ASR eval tools on GitHub
3. **ASR systems evaluation:** Using the curated dataset, nine ASR systems and twenty-five models, including both commercial and freely available solutions, were evaluated. Variations in performance across different systems, datasets, and speaker demographics were observed. Results are available publicly on the Polish ASR leaderboard.7 Footnote 6: BIGOS ASR eval tools on GitHub
4. **Open resources sharing:** All datasets, tools, and evaluation results are made freely available to the research community. This promotes transparency, reproducibility, and collaboration, allowing researchers to leverage the resources for further Polish ASR development or adapt them to other languages.

## 2 Methodology

### System overview

The system developed for data curation and ASR benchmarking encompasses three main processes:

* **ASR speech datasets survey**: Involves analyzing speech data catalogs and taxonomies, creating a dashboard that summarizes and categorizes existing speech datasets.
* **Curation of ASR benchmark dataset**: Includes processing, formatting, and analyzing datasets to create a standardized set for benchmarking ASR systems. BIGOS (Benchmark Intended Grouping of Open Speech) format was used. [10]
* **Evaluation of ASR systems**: Involves managing the evaluation process, generating results, and presenting performance metrics through a public dashboard for system comparison and analysis.

Figure 1 illustrates the system architecture and the core open tools used for development. The subsequent sections provide a detailed description of the specific processes and tools.

### Survey of datasets

A keyword-based literature review was used to identify and document relevant datasets. [34] The datasets were manually analyzed and annotated. The final methodology included:

1. Conducting keyword searches in relevant sources
2. Manually analyzing and annotating documentation
3. Cross-checking multiple sources for consistency and accuracy
4. Validating and analyzing downloadable datasets
5. Analyzing metadata to derive insights on Polish ASR speech datasets
6. Making the catalog and insights publicly available

The survey sources include language data repositories, scientific community platforms, and public domain documentation. The attributes considered include creator, funding, license, publication date, quality assurance, and content characteristics such as the format of the audio file and the number of speakers. [12] Resulting catalog and survey insights are shared on GitHub8 and Hugging Face.9

Footnote 8: Polish ASR speech data survey on GitHub

Footnote 9: Polish ASR survey on Hugging Face

### Dataset curation

#### 2.3.1 Design considerations

A curated benchmark dataset for Polish ASR systems is intended to have the following features:

* **Task-appropriate:** Relevant and practical for the intended ASR task.
* **Accessible:** Available online under a license allowing the free use and derivative works.
* **Discoverable:** Easy to find and acquire (no registration or other access barriers).
* **Diverse and challenging:** Containing various examples to test the adaptability of the model, as well as complex cases to encourage community participation and minimize the risk of benchmark saturation.
* **Annotated**: With metadata about speakers and recordings allowing nuanced analysis and interpretation of the results.
* **Optimally sized:** Large enough to be representative, but manageable to download and use.
* **Clean yet realistic:** Free of major errors, but noisy enough to represent the complexity of the real world.

Figure 1: Architecture of data curation and ASR evaluation system.

* **Well-documented:** Provided with documentation that is understandable to users without technical skills.
* **Well-explained:** Provided with evaluation baselines and how-to-use script examples.

#### 2.3.2 Leveraging speech data catalog for sourcing open datasets

The Polish ASR speech dataset catalog was used to select datasets for curation. [11] Following criteria were considered:

* Datasets are available online under a license allowing free use for noncommercial purposes.
* Transcriptions are aligned with the recordings.
* Recording sampling rate is at least 8 kHz.
* Audio files are encoded using at least 16 bits per sample.

Twenty-four source datasets were curated as two new datasets: _BIGOS V2_ and _PELCRA for BIGOS_. Named after the Polish dish _bigos_, a traditional cabbage-based stew -- **BIGOS V2** builds upon its predecessor, _BIGOS (Benchmark Intended Grouping of Open Speech)_[10]10 and offers expanded selection of metadata and recordings from the following source corpora:

Footnote 10: BIGOS V1 dataset on Hugging Face

* **The Common Voice dataset (_mozilla-common_voice_15-23_) [3] covers over 60 languages and many underrepresented groups. Available under CC-0 license.
* **The Multilingual LibriSpeech (MLS) dataset (fair-mls-20)** is a large multilingual corpus made by Facebook AI Research (FAIR) [31]. Derived from audiobooks, it covers eight languages, with 44,000 hours of English and 6,000 hours for other languages. The Polish data includes 137 hours from 25 books by 16 speakers. Available under CC-BY license.
* **The Clarin Studio dataset (_clarin-pjatk-studio-15)_ by CLARIN-PL includes 13,802 short utterances (56 hours) from 554 sessions by 317 speakers. Each session has 20-31 audio files, all recorded in a studio for clear audio. Available under CC-BY-SA license.
* **The Clarin Mobile dataset (_clarin-pjatk-mobile-15)_ is a Polish speech corpus of read speech recorded on a telephone. It includes many speakers reading several dozen sentences and words with rare phonemes. Available under CC-BY-SA license.
* **The Jerzy Sas PWR datasets*
* (Politechnika Wroclawska) comprise three legacy sets of recordings available in the public domain:
* Male speaker speech set _(pwr-maleset-unk)_ - single male speaker recordings.
* Utterances containing short words (_pwr-shortwords-unk)_ - single-phoneme conjunctions and prepositions likely to be misrecognized.
* Spoken commands as very important utterances (VIUs) _(pwr-viu-unk)_ - editor control commands and domain-specific utterances.
* **The M-AI Labs Speech corpus (mailabs-19)** created from audiobooks as _MLS_. Intended for training speech recognition and synthesis systems in nine languages, with nearly a thousand hours of audio, including 53.5 hours for Polish. Available under proprietary license.
* **The AZON Read and Spontaneous Speech datasets (_pwr-azon_spont-20, pwr-azon_read-20)_ contain recordings from academic staff in the physical chemistry domain, including both supervised readings and unsupervised spontaneous recordings such as interviews and presentations. Available under a CC-BY-SA license.11 Footnote 11: AZON dataset homepage FLEURS dataset homepage

Compared to predecessor, _BIGOS V2_ contains curated recordings and metadata from the following source corpora:

* **Google FLEURS (_google-fleurs-22)_ is a parallel speech benchmark dataset in 102 languages, based on the FLORes-101 machine translation benchmark. [6] Hosted on Hugging Face12 and available under a CC-BY license.

* **PolyAI Minds14** (_polyai-minds14-21_) is a dataset for training and evaluating intent recognition systems using spoken data. Covers spoken samples in the commercial e-banking domain in 14 language variations. [8] Hosted on Hugging Face13 and available under a CC-BY license. Additionally, _PELCRA for BIGOS_ dataset contains recordings and metadata from the following source corpora: Footnote 13: Minds14 dataset homepage
* **PolEval 22 Diabiz sample** (_ul-diabiz_poleval-22_) was used for a punctuation restoration task in the 2022 PolEval competition. It is a subset of the _DiaBiz homepage14_ dialog corpus of phone-based customer-agent interactions by the PELCRA group of the University of Lodz. Available publicly under CC-BY-SA-NC-ND and curated for Polish ASR systems benchmarking purposes with the consent of the author. Footnote 14: Diabiz dataset homepage
* **SpokesMix15** is a corpus of conversational Polish by the PELCRA group. [26] It includes speech recordings and word-by-word transcriptions with non-speech events. Available under the CC-BY-NC-ND license and curated with permission of the authors. Footnote 15: SpokesMix dataset homepage
* **SpokesBiz16** is a corpus of conversational Polish from the CLARIN-BIZ project, featuring over 650 hours of recordings from nearly 600 speakers. [28] Transcriptions are diarized and manually annotated. Includes eight diverse subsets, e.g. biographical interviews, job interviews, podcasts, and student presentations. Available under the CC-BY-NC-ND license and curated with the authors permission.

Footnote 16: SpokesBiz dataset homepage

Datasheets of curated datasets can be found in Appendices C.9, C.10, C.11, C.12 and Hugging Face.17

Footnote 17: BIGOS datasheets

#### 2.3.3 Curation process

* **Dataset structure curation:*
* Downloading and manually inspecting format and contents
* Creating train/dev/test splits if not available
* Assigning standard IDs to speakers and files
* **Audio file curation:*
* Removal of invalid and duplicated audio files
* Unifying audio format to WAV 16 bits/16 kHz
* Normalizing audio amplitude to -3 dBFS
* Splitting long audio files into shorter segments based on time-alignment annotations
* **Text files (transcripts and metadata) curation:*
* Converting text encoding to UTF8
* Extracting original transcription and removing redundant characters
* Removal of audio files for utterances containing offensive content
* Extracting and unifying metadata contents
* Generating metadata from text and audio content
* Saving in the standard tabular format
* **Dataset distribution*
* Uploading to the HF dataset hub
* Referencing the original license and authors in the README file

The resulting _BIGOS utterance data object_ with a description of the standard metadata fields is available in Table 24 in the Appendix.

### ASR evaluation

#### 2.4.1 System design considerations

Below is an overview of the main design considerations. Established tools and platforms for data management and evaluation were used whenever feasible (see Appendices C.1 and C.2 for details).

* **Metrics**: Support for well-established metrics for ASR evaluation.
* **Extensibility**: Easy integration of new datasets, normalization methods, metrics, and systems.
* **Availability**: Publicly accessible and intuitive presentation of results.
* **Comprehensiveness**: Performance analysis across scenarios, system parameters, and user groups.

#### 2.4.2 Overview of the evaluation process

The process is presented on figure 2. Currently four evaluation metrics are supported: Sentence Error Rate (SER), Word Error Rate (WER), Match Error Rate (MER) and Character Error Rate (CER). [21] The definitions are provided in Appendix C.3. The same pipeline was used to normalize both references and hypotheses. The impact of normalization is discussed in section 3.1 and normalization steps are described in Appendix C.5. Python scripts used for the evaluation are available on GitHub.18

Footnote 18: BIGOS ASR eval tools on GitHub

In total twenty-five models of nine ASR systems were evaluated: Google STT V1 and V2, Azure STT, Whisper local and cloud, AssemblyAI, NeMo, MMS and Wav2Vec2. The references and details are available in the Appendix C.4. The complete list of evaluated systems and models is presented in table 17.

## 3 Evaluation results

Eight evaluation scenarios encompassing several key dimensions are supported. The _All System Variants_ scenario considers different system-model variants across the entire dataset, while _Subset Analysis_ focuses on evaluating specific subsets of the test data. The _System Type Comparison_ scenario contrasts free versus commercial systems, highlighting differences in performance. _Model Size Evaluation_ assesses variants by their respective model sizes, and _Audio Duration Analysis_ provides insight on the best and worst performing systems for different ranges of audio lengths. _Speaking Rate Evaluation_ examines system performance across varying speech rates, while _Speaker Age Group Analysis_ and _Speaker Gender Analysis_ evaluate system variants based on speaker age and gender demographics, respectively.

All benchmark results can be accessed through the public interactive dashboard.19 Users can display the evaluation results for a specific scenario or perform custom analysis for specific datasets, systems, metrics, normalization techniques, and diagram types. The results of selected scenarios are analyzed in the subsequent sections.

Figure 2: ASR evaluation process data flow.

### Impact of normalization on error rates

Table 1 presents the individual and average error rate reductions, measured in percentage points, for each normalization method applied. Corresponding results for the _PELCRA for BIGOS_ dataset can be found in the Appendix C.6 and online.

### Overall accuracy of available ASR systems and models

Figure 3 show the WER box plot for the systems evaluated using the _BIGOS V2 dataset_. The 3 best ASR models in terms of accuracy are _Whisper Large V3_, _Whisper Cloud_ and _Assembly AI best_. Corresponding results for the _PELCRA for BIGOS_ dataset can be found in the Appendix C.6 and online.

### Subset analysis

Figure4 presents performance across subsets of the _BIGOS V2 dataset_, sorted by median WER. The _CommonVoice_ and _PWR_ subsets are the least challenging overall, though the _pwr-viu-unk_ subset shows high WER for many systems. As revealed by manual inspection, this is caused by hallucinations for unnaturally slow speech rates. The most challenging subsets are _pwr-azon_read20_, _pwr-azon_spont20_ and _polyai-minds14-21_, containing specialized terminology, spontaneous speech and varied accents, respectively. These factors contribute to increased difficulty for ASR systems, leading to significant performance variation across different models.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline
**Method** & **SER [p.p.]** & **WER [p.p.]** & **MER [p.p.]** & **CER [p.p.]** & **Average [p.p.]** \\ \hline blanks & -1.79 & 0.00 & 0.00 & -0.85 & -0.66 \\ lowercase & -2.65 & -6.06 & -6.27 & -1.40 & -4.10 \\ punctuation & -1.40 & -7.61 & -7.95 & -1.67 & -4.66 \\ all & -24.90 & -14.63 & -15.22 & -4.04 & -14.70 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Reduction of error rates caused by normalization of references and hypothesis for _BIGOS V2 dataset_

Figure 3: Box plot showing Word Error Rate (WER) distributions for systems evaluated on the _BIGOS V2 dataset_. Lower values indicate better performance, while narrower boxes and whisker ranges demonstrate more consistent performance across the 12 source datasets.

### Comparison of accuracy of commercial and freely available ASR systems

Table 2 compares the WER of commercial and free ASR systems. Commercial systems achieved lower minimum and median WER for BIGOS V2 and PELCRA datasets by approximately 2.5 p.p. and 4.5 p.p., respectively. Furthermore, both commercial and free systems obtained better recognition accuracy for read speech (BIGOS V2) than conversational speech (PELCRA) by approximately 17 and 19 p.p., respectively.

### Accuracy as a function of model size

Figures 4(a) and 5(a) present the relationship between model size and WER for BIGOS and PELCRA datasets, respectively. The figures show that as model size increases, WER generally decreases, indicating improved performance for larger models.

### Accuracy as a function of speech rate

Figures 4(b) and 5(b) illustrate the relationship between WER and speech rate, defined as the average number of words spoken per second.

## 4 Discussion

### Analysis of findings

#### 4.1.1 Impact of normalization

Normalization techniques resulted in significant reductions in error rates for all types of metrics (SER, WER, MER, CER). Applying all methods reduced WER by 15.78 p.p. for the PELCRA dataset and

\begin{table}
\begin{tabular}{l l l r r r r} \hline \hline
**Dataset** & **Speech** & **Systems** & **Med. WER** & **Mean WER** & **Std. WER** & **Min. WER** \\ \hline BIGOS V2 & read & paid & 12.96 & 17.26 & 24.98 & 0.00 \\ BIGOS V2 & read & free & 15.47 & 21.93 & 19.29 & 2.10 \\ PELCRA & spontaneous & paid & 29.90 & 31.34 & 14.72 & 5.27 \\ PELCRA & spontaneous & free & 34.18 & 37.45 & 19.43 & 8.74 \\ \hline \hline \end{tabular}
\end{table}
Table 2: WER statistics for freely available and commercial ASR systems.

Figure 4: Boxplot of Word Error Rate (WER) per subset of _BIGOS V2 dataset_. Each box represents the WER distribution for a subset, with individual ASR systems indicated by unique colors and markers. Lower values indicate better performance.

15.22 p.p. for the _BIGOS V2 dataset_, highlighting the sensitivity of lexical metrics to spelling and formatting variations.

#### 4.1.2 Determining the best systems among free and commercial

Conversational speech (PELCRA) has higher error rates due to its spontaneous nature, with greater variability in style, speed, and pauses. The read speech (BIGOS V2) contains more structured speech, resulting in lower WER.

#### 4.1.3 Impact of model size on WER

Figure 4(a) shows that as the model size increases, the WER generally decreases, with larger models consistently achieving better performance. This trend is clear for models like the _Whisper_ series, although there are significant variations between models of similar sizes, particularly those trained on different datasets, such as _MMS_ and _Wav2vec2_. The _Whisper Large_ models achieve the lowest overall WER, while smaller models such as _Nemo Multilang_ and _Nemo Conformer_ still manage competitive results relative to larger models, demonstrating efficiency.

Figure 5(a) shows that the WER in all models for the PELCRA data set is higher. The trend of decreasing WER with larger models holds for both datasets, but the gains from larger models are more pronounced for conversational, noisy speech from PELCRA dataset, especially for _Whisper Large v1, v2, v3_. While _MMS_ models performed well on BIGOS, they show increased WER for PELCRA, indicating challenges with spontaneous interactions. The efficiency of _Nemo Multilang_ and _Nemo Conformer_ is also notable, though their advantage is reduced for conversational speech.

Figure 5: Example of evaluation scenario results.

Figure 6: Example of evaluation scenario results.

Overall, larger models are particularly beneficial for handling the variability of conversational datasets like PELCRA compared to read speech.

#### 4.1.4 Impact of speech rate on WER

Figures 4(b) and 4(b) show the relationship between Word Error Rate (WER) and speech rate for the BIGOS (read speech) and PELCRA (conversational speech) datasets. In both, WER decreases at moderate rates and rises for extreme speeds.

For BIGOS, WER is lowest between 1-2 words per second, with _assembly_best_ slightly outperforming _whisper_large_v3_. PELCRA shows a broader range up to 8 words per second, with WER lowest between 2-4 words per second, but both models struggle at extreme rates, particularly _whisper_cloud_. Manual inspection revealed a stronger tendency for the _whisper_ model to produce _hallucinations_ in either short recordings with high speech rates or long recordings with slowly pronounced words. Overall, conversational speech presents higher WER due to variability, while moderate speech rates yield optimal performance in both datasets.

### Implications

The developed data curation and evaluation system offers the following benefits for the research community:

* Establishes a standard for evaluating Polish ASR systems, enhancing reproducibility.
* Facilitates better use of datasets, promoting focused research. As of October 30th 2024, _BIGOS V2_ dataset had over 6,500 downloads, while _PELCRA for BIGOS_ had over 1,500 downloads.
* Encourages data sharing and collaboration, improving resources and progress.
* Identifies gaps, such as the need for detailed metadata and semantic metrics, guiding future studies.

Advantages for industry include:

* Informs public about strengths and weaknesses of available ASR system.
* Proposes a standard evaluation procedure to increase evaluation efficiency.
* Showcases the importance of normalization and utilization of metadata for analysis.
* Provides incentive to companies to showcase superior performance on a public benchmark for marketing purposes.

### Limitations and challenges

The reliability of results may be affected if recordings from popular datasets, such as _Common Voice_ and _MLS_, were included in training of the evaluated systems. To address this, new, non-public test recordings should be added to the benchmark dataset. Future research should also include manual transcriptions and annotations to ensure test data quality. Manual and automatic error classification and correction [40] can also be explored. Adding semantically informed metrics could offer additional insights into task-specific accuracy. [37; 35] Incorporating recordings that represent diverse usage conditions and Polish speaker demographics should improve reliability of assessing ASR systems robustness [14] and sociodemographic bias. [2; 1] Lastly, newly released systems and model updates could be systematically evaluated and compared with longitudinal studies in other languages [36].

## 5 Conclusion

The research addresses the issue of limited dataset usage for Polish benchmarking by offering a curated benchmark set derived from 24 publicly available datasets. The evaluation of 9 ASR systems and 25 models revealed notable performance differences between model sizes and speech types. This work improves reproducibility and directs future ASR advancements by providing public access to data catalogs, curated datasets, evaluation tools, and dashboards with comprehensive benchmarking results covering 8 scenarios. Specific methods and tools has potential to be reused for other low-resource languages.

Acknowledgments

The author gratefully acknowledges the original dataset creators for sharing their work openly and allowing its curation in this resource. BibTeX citations for the original authors are provided on curated datasets web pages, and the author hopes that users of datasets curated in _BIGOS_ format will include references to the original sources. 20 The author also extends thanks to anonymous reviewers for their insightful feedback on the first version of this manuscript.

Footnote 20: BIGOS collection on Hugging Face

## References

* [1] Alena Aksenova, Zhehuai Chen, Chung-Cheng Chiu, Daan van Esch, Pavel Golik, Wei Han, Levi King, Bhuvana Ramabhadran, Andrew Rosenberg, Suzan Schwartz, and Gary Wang. Accented Speech Recognition: Benchmarking, Pre-training, and Diverse Data. _arXiv preprint_, 5 2022.
* [2] Alena Aksenova, Daan van Esch, James Flynn, and Pavel Golik. How Might We Create Better Benchmarks for Speech Recognition? In _Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future_, pages 22-34, Stroudsburg, PA, USA, 2021. Association for Computational Linguistics.
* [3] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common Voice: A Massively-Multilingual Speech Corpus. _arXiv preprint_, 12 2019.
* [4] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common Voice: A Massively-Multilingual Speech Corpus. _arXiv preprint_, 12 2019.
* [5] Evelina Bakhtturina, Vitaly Lavrukhin, and Boris Ginsburg. A toolbox for construction and analysis of speech datasets. _arXiv preprint_, 4 2021.
* [6] Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech. _arXiv preprint_, 5 2022.
* [7] Sanchit Gandhi, Patrick von Platen, and Alexander M. Rush. Esb: A benchmark for multi-domain end-to-end speech recognition. _arXiv preprint_, 10 2022.
* [8] Daniela Gerz, Pei-Hao Su, Razvan Kuszto s, Avishek Mondal, Michal Lis, Eshan Singhal, Nikola Mrksic, Tsung-Hsien Wen, and Ivan Vulic. Multilingual and Cross-Lingual Intent Detection from Spoken Data. _arXiv preprint_, 4 2021.
* [9] Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, and Michael Auli. Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. _arXiv preprint_, 4 2021.
* benchmark intended grouping of open speech corpora for polish automatic speech recognition. In Maria Ganzha, Leszek A. Maciaszek, Marcin Paprzycki, and Dominik Slezak, editors, _Proceedings of the 18th Conference on Computer Science and Intelligence Systems, FedCSIS 2023, Warsaw, Poland, September 17-20, 2023_, volume 35 of _Annals of Computer Science and Information Systems_, pages 585-590, 2023.
* [11] Michal Junczyk. Polish ASR Speech Datasets Catalog. https://github.com/goodmike31/pl-asr-speech-data-survey, 2023.
* [12] Michal Junczyk. A survey of Polish ASR speech datasets. _Poznan Studies in Contemporary Linguistics_, 60(1):27-52, 3 2024.

* [13] Seonmin Koo, Chanjun Park, Jinsung Kim, Jaehyung Seo, Sugyeong Eo, Hyeonseok Moon, and Heuiseok Lim. KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 4798-4815, Stroudsburg, PA, USA, 2023. Association for Computational Linguistics.
* [14] Seonmin Koo, Chanjun Park, Jinsung Kim, Jaehyung Seo, Sugyeong Eo, Hyeonseok Moon, and Heuiseok Lim. Toward Practical Automatic Speech Recognition and Post-Processing: a Call for Explainable Error Benchmark Guideline. _arXiv preprint_, 1 2024.
* [15] Danijel Korzinek. Task 5: Automatic speech recognition PolEval 2019 competition. Technical report, Polish-Japanese Academy of Information Technology, 2019.
* [16] Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, and Yang Zhang. QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions. _arXiv preprint_, 10 2019.
* research for Polish language. _Procedia Computer Science_, 225:1134-1141, 2023.
* [18] Quentin Lhoest, Albert del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matussiere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A Community Library for Natural Language Processing. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 175-184, Online and Punta Cana, Dominican Republic, 11 2021. Association for Computational Linguistics.
* [19] Thomas Liao, Rohan Taori, Deborah Raji, and Ludwig Schmidt. Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning. In Joaquin Vanschoren and Sai-Kit Yeung, editors, _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1_, 2021.
* [20] Wes McKinney and others. pandas: a foundational Python library for data analysis and statistics. _Python for high performance and scientific computing_, 14(9):1-9, 2011.
* [21] Andrew Cameron Morris, Viktoria Maier, and Phil Green. From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition. In _Interspeech 2004_, pages 2765-2768, ISCA, 10 2004. ISCA.
* [22] Nathan Lambert. In defense of the open LLM leaderboard, 9 2023.
* [23] Curtis G. Northcutt, Anish Athalye, and Jonas Mueller. Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. _arXiv preprint_, 3 2021.
* [24] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore. PMLB: a large benchmark suite for machine learning evaluation and comparison. _BioData Mining_, 10(1):36, 12 2017.
* [25] Marcin Pacholczyk. Przeglad I porowannie rozwiazan rozpoznawania mowy pod katem rozpoznawania zbioru komend glossowych. In Jolanta Krystsek and Swierniak Andrzej, editors, _Automaryacja procesow dyskretnych: Teoria i zastosowania_, pages 147-164. Politechnika Slaska, 2018.
* [26] Piotr Pezik. Increasing the Accessibility of Time-Aligned Speech Corpora with Spokes Mix. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, 5 2018. European Language Resources Association (ELRA).
* [27] Piotr Pezik and Michal Adamczyk. Automatic Speech Recognition for Polish in 2022. Technical report, University of Lodz, Lodz, 4 2022.

- an Open Corpus of Conversational Polish. _arXiv preprint_, 12 2023.
* an Annotated Corpus of Polish Call Center Dialogs. In Nicoletta Calzolari, Frederic Bechet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Helene Mazo, Jan Odijk, and Stelios Piperidis, editors, _Proceedings of the Thirteenth Language Resources and Evaluation Conference_, pages 723-726, Marseille, France, 6 2022. European Language Resources Association.
* [30] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. Scaling Speech Technology to 1,000+ Languages. _arXiv preprint_, 5 2023.
* [31] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A Large-Scale Multilingual Dataset for Speech Research. In _Interspeech 2020_, pages 2757-2761, ISCA, 10 2020. ISCA.
* [32] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust Speech Recognition via Large-Scale Weak Supervision. _arXiv preprint_, 12 2022.
* [33] Francis McCann Ramirez, Luka Chkhetiani, Andrew Ehrenberg, Robert McHardy, Rami Botros, Yash Khare, Andrea Vanzo, Taufiquzzaman Peyash, Gabriel Oexle, Michael Liang, Ilya Sklyar, Enver Fakhan, Ahmed Etefly, Daniel McCrystal, Sam Flamini, Domenic Donato, and Takuya Yoshioka. Anatomy of Industrial Scale Multilingual ASR. _arXiv preprint_, 4 2024.
* [34] Jennifer Rowley and Frances Slack. Conducting a literature review. _Management Research News_, 27(6):31-39, 6 2004.
* [35] Somnath Roy. Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for End Usability. _arXiv preprint_, 6 2021.
* A Longitudinal Study_, pages 520-529. 2020.
* [37] Espen James Stokke. _Semantic Word Error Rate: A Metric Based on Semantic Distance_. PhD thesis, The University of Bergen, 2023.
* [38] Piotr Szymanski, Piotr Zelasko, Mikolaj Morzy, Adrian Szymczak, Marzena Zyla-Hoppe, Joanna Banaszczak, Lukasz Augustyniak, Jan Mizgajski, and Yishay Carmiel. WER we are and WER we think we are. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 3290-3295, Stroudsburg, PA, USA, 2020. Association for Computational Linguistics.
* [39] Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. _arXiv preprint_, 1 2021.
* [40] Johannes Wirth and Rene Peinl. Automatic Speech Recognition in German: A Detailed Error Analysis. In _2022 IEEE International Conference on Omni-layer Intelligent Systems (COINS)_, pages 1-8. IEEE, 8 2022.
* [41] Marta Zielonka, Wiktor Krasinski, Jakub Nowak, Przemyslaw Roslen, Jan Stopinski, Mateusz Zak, Franciszek Gorski, and Andrzej Czyzewski. A survey of automatic speech recognition deep models performance for Polish medical terms. In _2023 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)_, pages 19-24. IEEE, 9 2023.

## 8 Appendices

Provide additional data, tools' documentation, and other supplementary materials that are relevant but not central to the article's narrative.

### Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Abstract and introduction explicitly describes contributions: Survey of datasets, methodology thereof, curated evaluation datasets process and outcomes, system for ASR evaluation, interactive dashboard with benchmark results. 2. Did you describe the limitations of your work? Limitations include limited representation of Polish speakers, lack of manual transcription verification and unification, limited scope of transcription normalization, lack of support for embedding based metrics, lack of manual analysis for ASR errors, limited availability of recordings with speaker metadata. 3. Did you discuss any potential negative societal impacts of your work? In the limitations section, it is mentioned that the evaluation datasets do not encompass all Polish users or the various conditions under which ASR systems are used. However, the results presented can guide the selection of the best-performing ASR systems for use-cases similar to those in the BIGOS evaluation dataset. For new and particularly high-risk scenarios, such as the medical field or specific demographic group, an independent evaluation on a representative dataset is necessary to accurately assess performance and ensure safe, unbiased operation. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? Presented work follows the ethical guidelines. No PII or protected information about individuals is revealed. The authors of original datasets obtained presents from speech recording contributors. The author of curated dataset obtained consent to use datasets for evaluation dataset curation and evaluation, either directly or based on licensing terms. Research did not include experiments involving human subjects.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Did you include complete proofs of all theoretical results? If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Code, data and instructions how to reproduce results are available on respective publicly available repositories on Hugging Face and GitHub platforms. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? Yes, all authors of existings assets were cited both in submitted article and repositories with curated assets. 2. Did you mention the license of the assets? Yes, license types are mentioned in the respective tables describing source datasets, as well as on repositories hosting curated assets. 3. Did you include any new assets either in the supplemental material or as a URL? Yes Yes, links to meta-corpora resulting from curation of existing assets were provided.

* Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Yes, the consent from the author of PELCRA corpora to curate dataset for open competition and benchmarking purposes is mentioned.
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] Yes, the lack of PII is mentioned, however the inspection if datasets contain potentially offensive content was not performed.
* If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]

## Appendix A Additional information required by organizers

In the Appendix, we provide additional information. This section will often be part of the supplemental material. Please see the call on the NeurIPS website for links to additional guides on dataset publication.

Submission introducing new datasets must include the following in the supplementary materials:

1. Dataset documentation and intended uses. Recommended documentation frameworks include datasheets for datasets, dataset nutrition labels, data statements for NLP, and accountability frameworks.
2. URL to website/platform where the dataset/benchmark can be viewed and downloaded by the reviewers.
3. URL to Croissant metadata record documenting the dataset/benchmark available for viewing and downloading by the reviewers. You can create your Croissant metadata using e.g. the Python library available here: https://github.com/mlcommons/croissant
4. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license.
5. Hosting, licensing, and maintenance plan. The choice of hosting platform is yours, as long as you ensure access to the data (possibly through a curated interface) and will provide the necessary maintenance.

## Appendix B Additional information relevant to submitted article

### Polish ASR benchmarks overview

This section presents ASR benchmarks for the Polish language reported in the public domain as of March 2024:

* BOR (_BOR POLSL PS 18_) [25]
* PolEval 19 ASR challenge (PolEval PJATK 19) [15]
* DiaBiz commercial ASR systems benchmark [29]
* Medical PG [41]
* Medical PS [17]

The benchmarks described in this study, as well as those performed on a previous version of the BIGOS dataset [10], are not included.

Table 3 provides a summary of reported Polish ASR benchmarks, listing each benchmark by year, models evaluated, the best model, and observations on performance.

Table 4 offers an overview of the specific use cases for these benchmarks, detailing primary applications across each evaluation.

Table 5 summarizes benchmarks from 2018 to 2023, including systems, datasets, and metrics utilized. In Table 6, a breakdown is provided of domains, speech types, audio sources, and recording devices across benchmarks.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Benchmark** & **Use Cases** \\ \hline BOR POLSL PS 18 & Voice Control \\ PolEval PJATK 19 & Oration \\ DiaBiz CLARIN Voicelab 22 & Conversations \\ SpokesBiz CLARIN 23 & Conversations, Meetings, Orations \\ Medical UW SOVVA PS 23 & Dictation \\ Medical PG 23 & Dictation \\ \hline \hline \end{tabular}
\end{table}
Table 4: Overview of ASR Use Cases in Polish ASR Benchmarks

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Benchmark** & **Year** & **Models** & **Best** & **Lowest** & **Observations** \\  & & & **Model** & **WER** & \\ \hline BOR & 2018 & ARM, Skrybot, & Google & Google & clean-50\% & Tested systems are not accurate enough for training government agents. \\ PolEval & 2019 & GOLEM, & & 90\% & All systems except ARM-1 are based on Kaldi; all but clarin-pl use GMM models. Fixed systems used in-domain data only. \\ DiaBiz & 2022 & Azure, Google, & Azure, Google & Azure & 10.50\% & Azure achieved the best results (10.51 WER for both channels), followed by Voicelab’s ASR (11.51 WER). Google’s Polish ASR performed worse on the DiaBiz dataset (20.84 WER). Azure outperformed others in 8 of 9 domains, while Voicelab was slightly better for telecommunications customer support dialogs.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Benchmark** & **Year** & **Models** & **Best** & **Lowest** & **Observations** \\  & & & **Model** & **WER** & \\ \hline BOR & 2018 & ARM, Skrybot, & Google & Google & clean-50\% & Tested systems are not accurate enough for training government agents. \\ PolEval & 2019 & GOLEM, & & & 90\% & All systems except ARM-1 are based on Kaldi; all but clarin-pl use GMM models. Fixed systems used in-domain data only. \\  & & & & & & use GMM models. Fixed systems used in-domain data only. \\ DiaBiz & 2022 & Azure, Google, & & & & \\ 
\begin{tabular}{l} Voicelab \\ 22 \\ \end{tabular} & & & & & & & \\ \hline \end{tabular}
\end{table}
Table 3: Reported benchmarks for Polish ASR Systems as of March 2024Table 8 highlights the acoustic conditions and speaker metadata available in these Polish ASR benchmarks.

Table 9 details the metrics used, both automated and human-evaluated, to assess system performance.

Table 10 presents benchmarks from 2018 to 2023 with the number and details of ASR systems evaluated, totaling 19 system-model combinations.

Table 11 shows the frequency of independent evaluations for ASR systems supporting Polish.

Table 12 lists ASR systems supporting Polish that, as of March 2024, lacked publicly reported evaluations. All systems, except _notta.ai_ were included in this benchmark.

Finally, Table 13 categorizes system types, showing the range of commercial, public domain, and community-provided ASR systems benchmarked from 2018 to 2023.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Benchmark** & **Domain** & **Speech Types** & **Audio Sources** & **Audio Devices** \\ \hline BOR POLSL PS 18 & Government & Read & Field Rec. & Lavalier Mic \\ PolEval PIATK 19 & Parliament & Read & Field Rec. & Venue Mic \\ DiaBiz CLARIN 22 & Cust. Support & Spontaneous & Phone calls & Phone \\ SpokesBiz CLARIN 23 & Various & Spontaneous & Podcasts & Various \\ Medical UW PS 23 & Medical & Read & Field Rec. & Lavalier Mic \\ Medical PG 23 & Medical & Read & Field Rec. & Lavalier Mic \\ \hline \hline \end{tabular}
\end{table}
Table 6: Overview of Domains, Speech Types, Audio Sources, and Recording Devices

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Benchmark** & **Audio (hours)** & **Domains** & **Recordings** & **Speakers** \\ \hline BOR POLSL PS 18 & 1 & 1 & 140 & 18 \\ PolEval PIATK 19 & 1 & 1 & 29 & 29 \\ DiaBiz CLARIN 22 & 41 & 7 & 400 & 151 \\ SpokesBiz CLARIN 23 & 52 & 7 & 79 & 79 \\ Medical UW PS 23 & 1 & 1 & 1000 & No Info \\ Medical PG 23 & 1 & 1 & 1200 & 10 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Dataset Size, Number of Domains, Recordings, and Speakers

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Benchmark** & **Acoustic Conditions** & **Speaker Meta-Data** \\ \hline BOR POLSL PS 18 & Mixed & None \\ PolEval PIATK 19 & Mixed & None \\ DiaBiz CLARIN 22 & Mixed & Age, gender, education \\ SpokesBiz CLARIN 23 & Mixed & Age, gender, education \\ Medical UW PS 23 & Clean & Age, gender, region \\ \hline \hline \end{tabular}
\end{table}
Table 8: Acoustic Conditions, Annotations, and Speaker Meta-Data Across Polish ASR Benchmarks

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Benchmark** & **Year** & **Systems** & **Datasets** & **Metrics Auto.** & **Metrics Manual** \\ \hline BOR POLSL PS 18 & 2018 & 3 & 1 & 3 & 0 \\ PolEval PIATK 19 & 2019 & 6 & 1 & 1 & 0 \\ DiaBiz CLARIN Voicelab 22 & 2022 & 3 & 7 & 3 & 0 \\ Medical PG 23 & 2023 & 3 & 1 & 6 & 0 \\ Medical UW PS 23 & 2023 & 3 & 1 & 5 & 3 \\ SpokesBiz CLARIN 23 & 2023 & 1 & 8 & 3 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Summary of Public Domain ASR Benchmarks (2018–2023)

## Appendix C Overview of tools for dataset management and ASR evaluation

### ASR speech datasets management tools

This section describes the most frequently used general or ASR-specific data management tools accessible under open licenses.

* **pandas**[20] is an open-source Python library that provides high-performance data manipulation and analysis tools. The objective of Pandas is to simplify the handling of data structures such as SQL tables, Excel, or text files, spanning from tabular data with different types of columns to time series and labeled matrices. The library gas two core data structures, the Series for one-dimensional data, and the DataFrame for two-dimensional data. _Pandas_ excels in various data operations, such as managing missing data, modifying the size of data structures, aligning data based on labels, and grouping data for analysis. It also simplifies

\begin{table}
\begin{tabular}{l l c} \hline \hline
**Benchmark** & **Evaluated Systems** & **Models Evaluated** \\ \hline BOR POLSL PS 18 & ARM, Skrybot, Google & 3 \\ PolEval PJATK 19 & GOLEM, ARM-1, SGMM2, tri2a, & 6 \\  & clarin-pl-studio, clarin-pl-sejm & \\ DiaBiz CLARIN Voicelab 22 & Azure, Google, Voicelab & 3 \\ SpokesBiz CLARIN 23 & Whisper (large) & 1 \\ Medical UW PS 23 & Azure, Google, Techno & 3 \\ Medical PG 23 & Azure, Google, Whisper (large-v2) & 3 \\ \hline
**Total** & & **19** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Publicly Reported Evaluations of ASR Models for Polish Language

\begin{table}
\begin{tabular}{l l} \hline \hline
**System** & **Benchmarks** \\ \hline azure\_latest & 3 \\ google\_default & 4 \\ skrybot\_default & 1 \\ voicelab\_default & 1 \\ arm\_default & 2 \\ techno\_default & 1 \\ clarin\_studio\_kaldi\_default & 1 \\ clarin\_pl\_sejm\_default & 1 \\ golem\_default & 1 \\ sgmm2\_default & 1 \\ tri2a\_default & 1 \\ whisper\_local\_large-v2 & 2 \\ \hline
**Total** & **19** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Number of Reported Independent Evaluations and Benchmarks per System

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Benchmark** & **Lexicon-Based Metrics** & **Annotation-Based** \\  & & **Metrics** \\ \hline BOR POLSL PS 18 & SRR, WRR & None \\ PolEval PJATK 19 & WER & None \\ DiaBiz CLARIN Voicelab 22 & WER & None \\ SpokesBiz CLARIN 23 & WER, MER, WIL & None \\ Medical UW PS 23 & Accuracy, WER, LED, JWS & Error types \\ Medical PG 23 & WER, MER, WIL, CER, LED, Jaccard & None \\  & distance \\ \hline \hline \end{tabular}
\end{table}
Table 9: Overview of Metrics Employed in Polish ASR System Benchmarksthe conversion of heterogeneous data forms into DataFrame objects, provides easy data slicing, indexing, concatenation, reshaping, and data fields renaming. Available under BSD 3-Clause license.
* **The Hugging Face datasets**[18] is a Python library designed to simplify data handling in ML projects. Its main benefit is the extensive support for public datasets in different formats and languages, which allows users to load the dataset with just one line of code. The library is also compatible with popular ML frameworks like _Numpy_, _Pandas_, _PyTorch_, _TensorFlow_, and _JAX_. _datasets_ library facilitate efficient data preparation thanks to standardized data pre-processing tools that can handle datasets in various file formats. Furthermore, it simplifies the sharing of new datasets using the _HF datasets hub_21. Advanced library functionalities include: Footnote 21: https://huggingface.co/datasets

* handling large datasets beyond RAM capacity through memory-mapping, Footnote 22: https://huggingface.co/datasets
* smart caching to avoid redundant processing, Footnote 23: https://huggingface.co/datasets
* compatibility with different data types, including audio and image Footnote 24: smart caching mode for efficient use of disk space and immediate data iteration.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Benchmark** & **Year** & **System Types** \\ \hline BOR POLSL PS 18 & 2018 & Commercial \\ PolEval PJATK 19 & 2019 & Community Provided \\ DiaBiz CLARIN 22 & 2022 & Commercial \\ SpokesBiz CLARIN 23 & 2023 & Commercial \\ Medical UW PS 23 & 2023 & Commercial \\ Medical PG 23 & 2023 & Commercial + Public Domain \\ \hline \hline \end{tabular}
\end{table}
Table 13: Types of ASR Systems Evaluated in Public Domain ASR Benchmarks (2018–2023)

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**System** & **Model** & **Type** & **License** \\ \hline google\_v2 & long & Commercial & Proprietary \\ google\_v2 & short & Commercial & Proprietary \\ google & latest\_long & Commercial & Proprietary \\ google & latest\_short & Commercial & Proprietary \\ google & command\_and\_search & Commercial & Proprietary \\ whisper\_cloud & whisper-1 & Commercial & Proprietary \\ assembly\_ai & best & Commercial & Proprietary \\ assembly\_ai & nano & Commercial & Proprietary \\ notta.ai & default & Commercial & Proprietary \\ mms & 1b-all & Free & CC-BY-NC \\ mms & 1b-fl102 & Free & CC-BY-NC \\ mms & 1b-l1107 & Free & CC-BY-NC \\ nemo & stt\_pl\_fastconformer\_hybrid\_large\_pc & Free & CC-BY \\ nemo & nemo\_stt\_multilingual\_fastconformer\_ & Free & CC-BY \\ nemo & stt\_pl\_quartznet15x5 & Free & CC-BY \\ whisper\_local & tiny & Free & MIT \\ whisper\_local & base & Free & MIT \\ whisper\_local & small & Free & MIT \\ whisper\_local & medium & Free & MIT \\ whisper\_local & large-v1 & Free & MIT \\ whisper\_local & large-v3 & Free & MIT \\ wav2vec & xls-r-1b-polish & Free & Apache \\ wav2vec & large\_xlsr-53-polish & Free & Apache \\ \hline \hline \end{tabular}
\end{table}
Table 12: ASR Systems Supporting Polish Without Benchmark in the Public Domain as of March 2024* **Speech Data Explorer (SDE)**[5] is a tool for the exploration and analysis of speech datasets.22 SDE was created by the NVIDIA team responsible for the development of the ASR system and the NLP framework Nemo.23 Researchers used SDE to investigate errors and fine-tune the process of constructing a speech dataset using the _forced alignement_ technique. The main features of SDE are: Footnote 22: SDE User Guide

Footnote 23: NVIDIA NeMo ASR toolkit

Footnote 24: https://huggingface.co/docs/transformers/index

* calculating dataset statistics e.g., number of recordings, alphabet, vocabulary, duration-based histograms
* dataset exploration with interactive data-tables for filtering and sorting
* audio data inspection tools e.g., waveforms, spectrograms, audio playback
* transcriptions and hypotheses analysis tools e.g., ASR accuracy metrics, alignments
* audio signal measurements e.g., encoding, amplitude, spectrum

Summary information on tools for the management of ASR speech datasets is provided in Table 14.

### ASR evaluation tools

This section outlines the most commonly used tools for the evaluation of ASR systems, which are available under permissive open-source licenses.

* **sclite**: Developed by the National Institute of Standards and Technologies (NIST), written in C, this tool uses the WER as its primary metric. Its features include speaker-level statistics, identification of commonly misrecognized words, and the ability to count hits, insertions, deletions, and substitutions. It also provides alignment capabilities. The software is available on _GitHub_ and falls under NIST's software license.
* **jiver**: A product of Jitsi, implemented in Python, JIWER calculates WER, along with Character Error Rate (CER), Match Error Rate (MER) and Word Information Lost (WIL) It supports aligning hypothesis and reference, as well as native support for text normalization transformations. The library is hosted on GitHub and released under the Apache 2.0 license.
* **asr-evaluation**: Created by Ben Lambert and also in Python, this tool measures WER, the word recognition rate WRR and the sentence error rate SER. It can handle simple normalization, removal of empty utterances, and calculation of the WER relative to the reference length. In addition, it generates confusion tables. Available on GitHub, _asr-evaluation_ is licensed under Apache 2.0.
* **fstalign**: Developed by Rev and written in Python/C++, _fstalign_ assesses WER and supports multiple input formats such as CTM, NLP, FST, and CSV. It natively supports text normalization and synonym handling and provides detailed error analysis based on metadata (WER tags) in NLP format. This tool is available on _GitHub_ under the Apache 2.0 license.
* **evaluate**: From Hugging Face and built with Python/C++, this tool focuses on WER and is integrated with the Hugging Face _datasets_ and _transformers24_ libraries, enhancing its utility for users in the Hugging Face ecosystem. It can be found on _GitHub_, with an Apache 2.0 license.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Tool** & **Language** & **Features** & **License** \\ \hline Pandas & Python & Supports various data formats, data manipulation and analysis tools & BSD 3-Clause \\ Hugging Face Datasets & Python & Dataloaders for public datasets, large dataset handling, streaming & Apache 2.0 \\ Speech Data Explorer & Python & Dataset stats, audio inspection, transcription analysis, signal measurements & Apache 2.0 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Tools for ASR Dataset Management
* ASR evaluation tool from the NVIDIA's Nemo toolkit, with the following features:
* On-the-fly data augmentation for ASR robustness evaluation.
* Analysis of insertion, deletion, and substitution error rates.
* Reliability assessment across metadata available, e.g. gender, audio length, etc.

Detail information on tools for the evaluation of ASR systems is provided in table 15.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Tool** & **Author** & **Lang.** & **Metric(s)** & **Features** & **License** \\ \hline \hline \multirow{2}{*}{
\begin{tabular}{l} sclite \\ jiwer \\ asr-evaluation \\ fstalign \\ \end{tabular} } & NIST & C & WER & Speaker-level stats, alignments, insertions, deletions & NIST \\  & Jitsi & Python & WER, CER, MER & Alignments, text normalization, CLI support & Apache 2.0 \\  & B. & Python & WER, WRR, WER by length, confusion tables & Apache 2.0 \\  & Lambert & & SER & Supports CTM, NLP, CSV, native normalization & Apache 2.0 \\  & Rev & Python, C++ & WER & Integrates with hfdatasets, transformers & Apache 2.0 \\  & Hugging & Python & WER, CER & Nemo ASR models integration & Apache 2.0 \\  & Face & & & & \\ asr-evaluator & NVidia & Python & WER, CER & Nemo ASR models integration & Apache 2.0 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Tools for ASR Evaluation

### Evaluation metrics

ASR systems predictions were evaluated against target transcriptions using the following metrics:

* **Sentence Error Rate** (SER), which calculates the proportion of sentences that are not perfectly recognized, i.e., sentences that contain at least one error.
* **Word Error Rate** (WER), which is defined as the minimum number of operations (substitutions, insertions, and deletions) required to transform the system output into the reference transcript, divided by the total number of words in the reference. The result is expressed as a percentage. A lower WER indicates a more accurate system. The WER value can be greater than 100%.
* **Match Error Rate** (MER), which calculates the ratio of the total number of errors (substitutions, insertions, and deletions) to the total number of words in the reference and hypothesis (system output) transcripts. Unlike WER, which is normalized by the number of words in the reference, MER is normalized by the total number of words in both the reference and hypothesis. This makes the MER potentially less sensitive to the insertion of incorrect words by the ASR system, offering a different perspective on the accuracy of the system. MER value is equal to or less than 100%.
* **Character Error Rate** (CER), which calculates the minimum number of character-level operations (substitutions, insertions, and deletions) needed to change the system output to the reference transcript, divided by the total number of characters in the reference.

### Evaluated ASR system details

* **Google Cloud Speech-to-Text25** supports more than 125 languages and variants. Google's service offers several useful features, such as noise cancelation, support for streaming, automatic punctuation, and the capability to recognize specific phrases or words when provided with context (e.g., specialized vocabulary or formats for spoken numbers, addresses, years, currencies, etc.). For selected languages, it also provides domain-specific models, multichannel audio support, and filtering of profanity content. Two generations of service are available: v126 and v2.27 For Polish, multiple model variants are available and were evaluated: _v1_default_, _v1_latest_long_, _v1_latest_short_, _v1_command_and_search_, _v2_long_ and _v2_short_. Footnote 25: https://cloud.google.com/speech-to-text
* **Microsoft's Azure Speech Service**28 as of May 2023 supports more than 100 languages and variants. In addition to standard transcription, the Azure Speech Service supports continuous real-time speech recognition and provides robust noise reduction capabilities. It allows users to apply custom models to improve the accuracy of domain-specific terminology. Additional services include text search or analytics on transcribed content, as well as speaker diarization. The _latest default_ model for Polish (dated for January 2023) was used, as no specialized model types support this language. Footnote 28: https://cloud.google.com/speech-to-text/docs/speech-to-text-requests?hl=en
* **Whisper**29 is an ASR system developed by the OpenAI company. It is trained on a large amount of weakly supervised multilingual and multitask data collected from the Internet. [32] According to the literature, Whisper is capable of handling different languages, dialects, and accents, demonstrating strong performance in diverse applications when evaluated on well-known benchmark datasets, e.g. Common Voice. [32] Whisper is available via a web API or as a pre-trained model for local use. Five versions of models of varying sizes are available for free download. The large model is available in 3 versions. Footnote 29: https://azure.microsoft.com/en-us/products/cognitive-services/speech-to-text
* **NVIDIA NeMo** is the ASR system available as part of the Nemo toolkit30. Three models supporting the Polish language are available: _stt_pl_fastconformer_hybrid_large_pc_,_stt_multilingual_fastconformer_hybrid_large_pc_ and _stt_pl_quartznet15x5_. [16] Polish models were fine-tuned from English to Polish on the _Mozilla Common Voice (MCV)_ dataset. [4] The authors report on 14 % WER on the _dev set_ from the Polish MCV dataset. All models are available for free use under a CC-BY-NC license.
* **MMS**: Facebook AI's massive multilingual pre-trained model for speech ("MMS"). It was pre-trained on about 500,000 hours of speech data in more than 1,400 languages. [30] The MMS system supports over 1000 languages and other speech processing tasks such as _Text-to-Speech (TTS)_ generation and _Speech Language Identification (LID)_31. The MMS system is available for free32 under the CC-BY-NC 4.0 license. The following versions of the fine-tuned model of ASR are available: Footnote 31: https://huggingface.co/spaces/mms-meta/MMS

Footnote 32: https://huggingface.co/facebook/mms-1b-all

Footnote 33: wav2vec2 fine-tuned to Polish

Footnote 34: Assembly AI

Footnote 35: OWSM homepage

* 1 billion parameter model fine-tuned on _FLEURS_ Dataset [6] * _1b-l1107_
- 1 billion parameter model fine-tuned _MMS-lab_ Dataset. [30] * _1b-all_
- 1 billion parameter model fine-tuned on _MMS-lab, FLEURS, CommonVoice, MLS_ and_VoxPopuli_ datasets. [4, 30, 31, 39]
* **Wav2Vec** is the automated speech recognition (ASR) system created by Facebook AI. It employs self-supervision to learn from unlabeled training data. Upon its launch in 2020, wav2vec2 exceeded the top semi-supervised approach with only a fraction of labeled training data. [9] Two models fine-tuned for Polish are available on the Hugging Face platform: _xls-r-1b-polish33_ and _large_xlsr-53-polish_. Footnote 33: ESPnet
* **Assembly AI34** provides an advanced automatic speech recognition service supporting multiple languages. Key features include real-time transcription, automatic punctuation, and robust noise cancellation. The service supports domain-specific vocabulary through custom models, filtering of sensitive content and integration with various platforms via a web API. The system is designed to handle diverse accents and dialects, ensuring high accuracy across different use cases. According to the authors, their system "leverages a diverse training Dataset comprising unsupervised (12.5M hours), supervised (188k hours), and pseudo-labeled (1.6M hours) data across four languages". [33] It is also reported that the _Universal-1_ model achieves comparative WER scores to larger and more computationally expensive models, such as Whisper large and Canary-1B. [33]. The amount of training data for Polish is not reported. Footnote 34: ESPnet
* **Open Whisper-style Speech Models35** (OWSM, pronounced as "awesome") are a series of speech foundation models developed by WAVLab at Carnegie Mellon University. They reproduce Whisper-style training using publicly available data and our open-source toolkit ESPnet.36. The authors released data preparation scripts, training and inference code, pre-trained model weights and training logs in order to promote transparency and open science in large-scale speech pre-training. Footnote 35: www2vec2 fine-tuned to Polish

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Size** & **Parameters** & **English-only model** & **Multilingual model** \\ \hline tiny & 39 M & Yes & Yes \\ \hline base & 74 M & Yes & Yes \\ \hline small & 244 M & Yes & Yes \\ \hline medium & 769 M & Yes & Yes \\ \hline large & 1550 M & No & Yes \\ \hline \hline \end{tabular}
\end{table}
Table 16: Model sizes and availability of English-only and Multilingual models.

### Normalization methods

Table 19 contains overview of scope of normalization of each available method.

### Evaluation results - PELCRA

Table 20 shows the impact of specific normalization methods for reduction of error rates for PELCRA dataset. Figure 7 shows a box plot of WER scores for PELCRA dataset.

Figure 8 shows a difference in WER for female and male recordings from PELCRA dataset. Figure 9 shows a standard deviation of WER for recordings originating from speakers in various age groups PELCRA dataset.

### Dataset splits details

Tables 22 and 23 present logic of data splits applied during curation for BIGOS and PELCRA datasets, respectively.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Shortname** & **System** & **Model** \\ \hline assembly\_best & assembly\_ai & best \\ \hline assembly\_nano & assembly\_ai & nano \\ \hline azure\_latest & azure & latest \\ \hline google\_cmd\_search & google & command\_and\_search \\ \hline google\_default & google & default \\ \hline google\_long & google & latest\_long \\ \hline google\_short & google & latest\_short \\ \hline google\_v2\_long & google\_v2 & long \\ \hline google\_v2\_short & google\_v2 & short \\ \hline mms\_all & mms & 1b-all \\ \hline mms\_102 & mms & 1b-fl102 \\ \hline mms\_1107 & mms & 1b-11107 \\ \hline memo\_multilang & nemo & stt\_multilingual\_fastconformer\_hybrid\_large\_pc \\ \hline memo\_pl\_confromer & nemo & stt\_pl\_fastconformer\_hybrid\_large\_pc \\ \hline memo\_pl\_quartznet & nemo & stt\_pl\_quartznet15x5 \\ \hline w2v-53-pl & wav2vec2 & large-xlsr-53-polish \\ \hline w2v-1b-pl & wav2vec2 & xls-r-1b-polish \\ \hline whisper\_cloud & whisper\_cloud & whisper-1 \\ \hline whisper\_base & whisper\_local & base \\ \hline whisper\_large\_v1 & whisper\_local & large-v1 \\ \hline whisper\_large\_v2 & whisper\_local & large-v2 \\ \hline whisper\_large\_v3 & whisper\_local & large-v3 \\ \hline whisper\_medium & whisper\_local & medium \\ \hline whisper\_small & whisper\_local & small \\ \hline whisper\_tiny & whisper\_local & tiny \\ \hline \hline \end{tabular}
\end{table}
Table 17: ASR systems evaluated in the study.

Figure 8: Difference in WER across speaker gender - PELCRA dataset.

Figure 7: Box plot of WER for systems evaluated on the _PELCRA for BIGOS_ dataset.

### Dataset splits details

Table 24 presents metadata fields associated with each individual data item.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Shorntname** & **Usage cost** & **License** \\ \hline assembly\_best & commercial & Proprietary \\ \hline assembly\_nano & commercial & Proprietary \\ \hline azure\_latest & commercial & Proprietary \\ \hline google\_cmd\_search & commercial & Proprietary \\ \hline google\_default & commercial & Proprietary \\ \hline google\_long & commercial & Proprietary \\ \hline google\_short & commercial & Proprietary \\ \hline google\_v2\_long & commercial & Proprietary \\ \hline google\_v2\_short & commercial & Proprietary \\ \hline mms\_all & free & CC-BY-NC \\ \hline mms\_102 & free & CC-BY-NC \\ \hline mms\_1107 & free & CC-BY-NC \\ \hline nemo\_multilang & free & CC-BY \\ \hline nemo\_pl\_confomer & free & CC-BY \\ \hline nemo\_pl\_quartznet & free & CC-BY \\ \hline w2v-53-pl & free & Apache \\ \hline w2v-1b-pl & free & Apache \\ \hline whisper\_cloud & commercial & Proprietary \\ \hline whisper\_base & free & MIT \\ \hline whisper\_large\_v1 & free & MIT \\ \hline whisper\_large\_v2 & free & MIT \\ \hline whisper\_large\_v3 & free & MIT \\ \hline whisper\_medium & free & MIT \\ \hline whisper\_small & free & MIT \\ \hline whisper\_tiny & free & MIT \\ \hline \hline \end{tabular}
\end{table}
Table 19: Methods of normalizing references and hypotheses.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Normalization** & **Scope** \\
**method** & \\ \hline blanks removal & Elimination of superfluous white spaces. \\ \hline lowercasing & Conversion of all characters to lowercase. \\ \hline punctuation removal & Removal of punctuation symbols. \\ \hline lexicon-based & Removal of specific words e.g. fillers “um”, “mhm” etc. Unification of \\ normalization & spelling e.g. Kissindzer -> Kissinger \\ \hline tags removal & Removal of tags e.g. ’trunc’ in PELCRA dataset. \\ \hline \hline \end{tabular}
\end{table}
Table 18: Evaluated ASR systems usage cost and license type.

[MISSING_PAGE_EMPTY:27]

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Subset** & **Original partitioning** & **BIGOS split process** & **Entity for BIGOS split** \\ \hline google-fleurs-22 & train, test, dev & original splits preserved & N/A \\ \hline polyai-minds14-21 & none & pseudorandom & audio file id \\ \hline pjatk-clarin\_mobile-15 & none & pseudorandom & session (speaker id) \\ \hline pjatk-clarin\_studio-15 & none & pseudorandom & session (speaker id) \\ \hline pwr-azon\_read-20 & none & pseudorandom & session (speaker id) \\ \hline pwr-azon\_spont-20 & none & pseudorandom & session (speaker id) \\ \hline fair-mls-20 & train, test, dev & original splits preserved & N/A \\ \hline mozilla-cv15-23 & train, test, dev & original splits preserved & N/A \\ \hline mailabs-corpus\_library-19 & none & pseudorandom & audio file id \\ \hline pwr-maleset-unk & none & pseudorandom & audio file id \\ \hline pwr-shortwords-unk & none & pseudorandom & audio file id \\ \hline pwr-viu-unk & none & pseudorandom & audio file id \\ \hline \hline \end{tabular}
\end{table}
Table 22: Metadata and partitioning of source datasets - _BIGOS V2 dataset_

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**System** & **Twenties** & **Thirties** & **Fourties** & **Sixties** & **Seventies** & **Std. Dev** \\ \hline whisper\_medium & 30.70 & 27.90 & 34.32 & 19.63 & 31.02 & 5.56 \\ assembly\_best & 28.65 & 31.67 & 35.10 & 20.42 & 36.83 & 6.47 \\ whisper\_large\_v3 & 28.54 & 33.88 & 35.09 & 19.23 & 33.85 & 6.59 \\ google\_long & 35.84 & 34.30 & 40.10 & 23.37 & 28.11 & 6.61 \\ whisper\_large\_v1 & 30.60 & 33.39 & 38.57 & 20.62 & 27.44 & 6.70 \\ google\_v2\_long & 36.76 & 36.98 & 41.03 & 23.68 & 30.18 & 6.83 \\ whisper\_cloud & 24.42 & 32.38 & 36.42 & 18.71 & 32.26 & 7.14 \\ google\_short & 30.14 & 30.96 & 46.12 & 25.99 & 32.33 & 7.65 \\ google\_cmd\_search & 45.27 & 46.16 & 55.02 & 32.88 & 41.07 & 8.06 \\ google\_default & 47.15 & 47.50 & 55.18 & 34.47 & 38.51 & 8.16 \\ whisper\_large\_v2 & 24.79 & 37.72 & 33.14 & 15.70 & 29.40 & 8.43 \\ azure\_latest & 42.68 & 38.16 & 41.03 & 23.73 & 25.97 & 8.83 \\ mms\_1107 & 47.43 & 51.72 & 61.20 & 37.36 & 54.86 & 8.90 \\ mms\_102 & 47.48 & 57.60 & 64.74 & 41.31 & 52.47 & 9.03 \\ nemo\_pl\_quartznet & 75.10 & 74.76 & 83.85 & 58.77 & 69.89 & 9.17 \\ mms\_all & 48.30 & 45.20 & 58.40 & 30.17 & 48.33 & 10.19 \\ nemo\_multilang & 38.60 & 43.66 & 53.57 & 24.45 & 42.65 & 10.57 \\ google\_v2\_short & 41.12 & 41.96 & 53.69 & 26.48 & 28.98 & 11.01 \\ w2v-1b-pl & 40.59 & 59.48 & 61.57 & 36.66 & 51.29 & 11.09 \\ w2v-53-pl & 61.85 & 62.14 & 71.46 & 40.05 & 50.87 & 12.08 \\ memo\_pl\_conformer & 45.96 & 52.00 & 64.78 & 30.02 & 50.91 & 12.56 \\ whisper\_base & 54.67 & 48.19 & 71.40 & 35.04 & 62.24 & 13.82 \\ assembly\_nano & 62.85 & 60.88 & 78.18 & 56.22 & 98.55 & 17.30 \\ whisper\_tiny & 88.47 & 88.03 & 80.52 & 45.12 & 70.87 & 17.96 \\ whisper\_small & 40.68 & 32.96 & 83.03 & 24.65 & 39.38 & 22.65 \\ \hline \hline \end{tabular}
\end{table}
Table 21: WER across age groups - _PELCRA for BIGOS_ dataset.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Field name** & **Description** \\ \hline audioname & Standardized unique identifier for each audio recording in the dataset. \\ \hline split & Indicates the dataset split the recording belongs to: train, test or \\ validation. \\ \hline dataset & Source dataset identifier. \\ \hline ref\_orig & The original transcript associated with the audio recording. \\ \hline ref\_spoken & Transcription in the spoken domain format. \\ \hline ref\_written & Transcription in the written domain format. \\ \hline audio & Object for storing audio data in HF datasets format. \\ \hline sampling\_rate & The sampling rate of the audio recording in the dataset. Can be the same as the original or adjusted for standardization. \\ \hline samplingrate\_orig & The original sampling rate of the audio recording. \\ \hline speaker\_id & A unique identifier of the speaker in the recording. \\ \hline audiopath\_bigos & The relative path to the audio file from distributed data archive. \\ \hline audiopath\_local & The absolute path to the extracted audio file, typically in the default hf \\ datasets cache directory. \\ \hline audio\_duration\_samples Recording duration in samples. \\ \hline audio\_duration\_seconds Recording duration in seconds. \\ \hline speaker\_gender & Information about the speaker’s gender in the CommonVoice format. \\ \hline speaker\_age & Information about the speaker’s age in CommonVoice format. \\ \hline speech\_rate\_words & Speech rate expressed in words per seconds. \\ \hline speech\_rate\_chars & Speech rate expressed in characters per seconds. \\ \hline utterance\_length\_words & Length of the utterance in words. \\ \hline utterance\_length\_chars & Length of the utterance in characters. \\ \hline \hline \end{tabular}
\end{table}
Table 23: Metadata and partitioning of source datasets - _PELCRA for BIGOS_ dataset

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Subset** & **Original partitioning** & **BIGOS split process** & **Entity for BIGOS split** \\ \hline ul-diabiz\_polexal-22 & train, test, dev & original splits preserved & N/A \\ \hline ul-spokes\_biz\_bio-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_biz\_int-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_biz\_luz-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_biz\_pod-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_biz\_pres-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_biz\_vc-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_biz\_vv-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_biz\_wyw-23 & none & pseudorandom & recording id \\ \hline ul-spokes\_mix\_emo-18 & none & pseudorandom & recording id \\ \hline ul-spokes\_mix\_luz-18 & none & pseudorandom & recording id \\ \hline ul-spokes\_mix\_parl-18 & none & pseudorandom & recording id \\ \hline \hline \end{tabular}
\end{table}
Table 24: Attributes in the BIGOS utterance data object

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Dataset** & **Codename** & **License** & **Languages** \\ \hline DiaBiz ASR PolEval 22 & ul-diabiz\_poleval-22 & Public domain & monolingual \\ \hline SpokesBiz CBIZ\_BIO & ul-spokes\_biz\_bio-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesBiz CBIZ\_INT & ul-spokes\_biz\_int-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesBiz CBIZ\_LUZ & ul-spokes\_biz\_luz-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesBiz CBIZ\_POD & ul-spokes\_biz\_pod-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesBiz CBIZ\_PRES & ul-spokes\_biz\_pres-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesBiz CBIZ\_VC & ul-spokes\_biz\_vc-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesBiz CBIZ\_VC2 & ul-spokes\_biz\_vc2-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesBiz CBIZ\_WYW & ul-spokes\_biz\_wyw-23 & CC-BY-NC-ND & monolingual \\ \hline SpokesMix PELCRA\_EMO & ul-spokes\_mix\_emo-18 & CC-BY & monolingual \\ \hline SpokesMix PELCRA\_LUZ & ul-spokes\_mix\_luz-18 & CC-BY & monolingual \\ \hline SpokesMix PELCRA\_PARL & ul-spokes\_mix\_parl-18 & CC-BY & monolingual \\ \hline \hline \end{tabular}
\end{table}
Table 26: PELCRA for BIGOS dataset subset license and language coverage.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Dataset** & **Codename** & **License** & **Languages** \\ \hline Clarin Studio & piatk-clarin\_studio-15 & CC-BY & monolingual \\ \hline Clarin Mobile & piatk-clarin\_mobile-15 & CC-BY & monolingual \\ \hline Munich AI Labs LibriVox & mailabs-corpus\_librivox-19 & Proprietary & multilingual \\ \hline Mozilla Common Voice & mozilla-common\_voice\_15-23 & CC-0 & multilingual \\ \hline Multilingual Librispeech & fair-mls-20 & CC-BY & multilingual \\ \hline Azon Read & pwr-azon\_read-20 & CC-BY-SA & monolingual \\ \hline Azon Spontaneous & pwr-azon\_spont-20 & CC-BY-SA & monolingual \\ \hline PWR Male Set & pwr-maleset-unk & Public domain & monolingual \\ \hline PWR Short Words & pwr-shortwords-unk & Public domain & monolingual \\ \hline PWR Very Important Utterances & pwr-viu-unk & Public domain & monolingual \\ \hline Google FLEURS & google-fleurs-22 & CC-BY & multilingual \\ \hline PolyAI Minds14 & polyai-minds14-21 & CC-BY & multilingual \\ \hline \hline \end{tabular}
\end{table}
Table 25: BIGOS V2 dataset subset license and language coverage.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Codename** & **Domain** & **Speech type** & **Interaction type** \\ \hline piatk-clarin\_studio-15 & open domain & read & monolog \\ \hline piatk-clarin\_mobile-15 & open domain & read & monolog \\ \hline mailabs-corpus\_library-19 & audiobook & read & monolog \\ \hline mozilla-common\_voice\_15-23 & open domain & read & monolog \\ \hline fair-mls-20 & audiobook & read & monolog \\ \hline pwr-azon\_read-20 & scientific & read & monolog \\ \hline pwr-azon\_spont-20 & scientific & spontaneous & monolog \\ \hline pwr-maleset-unk & commands & read & monolog \\ \hline pwr-shortwords-unk & commands & read & monolog \\ \hline pwr-viu-unk & commands & read & monolog \\ \hline google-fleurs-22 & wikipedia & read & monolog \\ \hline polyai-minds14-21 & banking & read & monolog \\ \hline \hline \end{tabular}
\end{table}
Table 27: BIGOS V2 dataset subset domains and speech types.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Codename** & **Domain** & **Speech type** & **Interaction type** \\ \hline ul-diabiz\_polexal-22 & customer service & spontaneous & dialog \\ \hline ul-spokes\_biz\_bio-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_biz\_int-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_biz\_luz-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_biz\_pod-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_biz\_pres-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_biz\_vc-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_biz\_vc2-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_biz\_vvp-23 & open domain & spontaneous & dialog \\ \hline ul-spokes\_mix\_emo-18 & open domain & spontaneous & dialog \\ \hline ul-spokes\_mix\_luz-18 & open domain & spontaneous & dialog \\ \hline ul-spokes\_mix\_parl-18 & open domain & spontaneous & monolog \\ \hline \hline \end{tabular}
\end{table}
Table 28: PELCRA for BIGOS dataset subset domains and speech types.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Codename** & **Speech source** & **Acoustic environment** & **Audio device** \\ \hline ul-diabiz\_poleval-22 & volunteers & quiet & telephone \\ \hline ul-spokes\_biz\_bio-23 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_biz\_int-23 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_biz\_luz-23 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_biz\_pool-23 & public speakers & quiet & various \\ \hline ul-spokes\_biz\_pres-23 & public speakers & quiet & various \\ \hline ul-spokes\_biz\_vc-23 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_biz\_vc2-23 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_biz\_vv2-23 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_biz\_wyw-23 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_mix\_emo-18 & volunteers & quiet & lavalier mic \\ \hline ul-spokes\_mix\_luz-18 & volunteers & quiet & lavalier mic \\ \hline \hline \end{tabular}
\end{table}
Table 30: PELCRA for BIGOS subsets speakers, environments, and devices.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Codename** & **Speech source** & **Acoustic environment** & **Audio device** \\ \hline pjatk-clarin\_studio-15 & volunteers & quiet & studio mic \\ \hline pjatk-clarin\_mobile-15 & volunteers & quiet & mobile phone \\ \hline mailabs-corpus\_librox-19 & volunteers & quiet & various \\ \hline mozilla-common\_voice\_15-23 & crowd & various & various \\ \hline fair-mls-20 & volunteers & various & various \\ \hline pwr-azon\_read-20 & volunteers & quiet & studio mic \\ \hline pwr-azon\_spont-20 & public speakers & mixed & lavalier \\ \hline pwr-maleset-unk & volunteers & quiet & studio mic \\ \hline pwr-shortwords-unk & volunteers & quiet & studio mic \\ \hline pwr-viu-unk & volunteers & quiet & studio mic \\ \hline google-fleurs-22 & volunteers & quiet & mobile phone \\ \hline polyai-minds14-21 & crowd & quiet & mobile phone \\ \hline \hline \end{tabular}
\end{table}
Table 29: BIGOS V2 dataset subset speakers, environments, and devices.

\begin{table}
\begin{tabular}{l r r r} \hline \hline
**Subset** & **Size [hours]** & **Samples** & **Speakers** \\ \hline \hline \multicolumn{1}{l}{ul-diabiz\_poleval-22} & 9.83 & 8950 & 170 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_bio-23} & 137.98 & 54917 & 158 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_int-23} & 2.25 & 1109 & 9 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_luz-23} & 74.27 & 41966 & 158 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_pod-23} & 55.00 & 22807 & 113 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_press-23} & 32.25 & 17174 & 55 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_vc-23} & 52.07 & 45272 & 78 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_vc2-23} & 81.04 & 25802 & 84 \\ \hline \multicolumn{1}{l}{ul-spokes\_biz\_wyw-23} & 28.21 & 11357 & 38 \\ \hline \multicolumn{1}{l}{ul-spokes\_mix\_emo-18} & 25.61 & 24329 & 40 \\ \hline \multicolumn{1}{l}{ul-spokes\_mix\_luz-18} & 18.74 & 20919 & 21 \\ \hline \multicolumn{1}{l}{ul-spokes\_mix\_parl-18} & 12.27 & 8656 & 48 \\ \hline \multicolumn{1}{l}{total} & 529.52 & 283258 & 972 \\ \hline \hline \end{tabular}
\end{table}
Table 31: Audio content size metrics for _BIGOS V2 dataset_

\begin{table}
\begin{tabular}{l r r r} \hline \hline
**Subset** & **Size [hours]** & **Samples** & **Speakers** \\ \hline \hline \multicolumn{1}{l}{fair-mls-20} & 107.86 & 26072 & 24 \\ \hline \multicolumn{1}{l}{google-fleurs-22} & 12.07 & 3937 & 3 \\ \hline \multicolumn{1}{l}{mailabs-corpus\_birvox-19} & 32.14 & 14862 & 2 \\ \hline \multicolumn{1}{l}{mozilla-common\_voice\_15-23} & 53.00 & 36910 & 2920 \\ \hline \multicolumn{1}{l}{pjatk-clarin\_mobile-15} & 12.48 & 3495 & 117 \\ \hline \multicolumn{1}{l}{pjatk-clarin\_studio-15} & 56.43 & 13810 & 553 \\ \hline \multicolumn{1}{l}{polyai-minds14-21} & 3.07 & 562 & 3 \\ \hline \multicolumn{1}{l}{pwr-azon\_read-20} & 5.72 & 2788 & 29 \\ \hline \multicolumn{1}{l}{pwr-azon\_spont-20} & 2.14 & 456 & 27 \\ \hline \multicolumn{1}{l}{pwr-maleset-unk} & 6.38 & 4738 & 3 \\ \hline \multicolumn{1}{l}{pwr-shortwords-unk} & 1.43 & 939 & 3 \\ \hline \multicolumn{1}{l}{pwr-viu-unk} & 1.04 & 2703 & 3 \\ \hline \multicolumn{1}{l}{total} & 293.76 & 111272 & 3945 \\ \hline \hline \end{tabular}
\end{table}
Table 32: Audio content size metrics for _PELCRA for BIGOS_ dataset