# Accelerating Diffusion Models with Parallel Sampling: Inference at Sub-Linear Time Complexity

 Haoxuan Chen

ICME

Stanford University

haoxuanc@stanford.edu

&Yinuo Ren

ICME

Stanford University

yinuoren@stanford.edu

&Lexing Ying

Department of Mathematics and ICME

Stanford University

lexing@stanford.edu

&Grant M. Rotskoff

Department of Chemistry and ICME

Stanford University

rotskoff@stanford.edu

Equal contribution, alphabetical order.Corresponding author.

###### Abstract

Diffusion models have become a leading method for generative modeling of both image and scientific data. As these models are costly to train and _evaluate_, reducing the inference cost for diffusion models remains a major goal. Inspired by the recent empirical success in accelerating diffusion models via the parallel sampling technique [1], we propose to divide the sampling process into \(\mathcal{O}(1)\) blocks with parallelizable Picard iterations within each block. Rigorous theoretical analysis reveals that our algorithm achieves \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) overall time complexity, marking _the first implementation with provable sub-linear complexity w.r.t. the data dimension \(d\)_. Our analysis is based on a generalized version of Girsanov's theorem and is compatible with both the SDE and probability flow ODE implementations. Our results shed light on the potential of fast and efficient sampling of high-dimensional data on fast-evolving modern large-memory GPU clusters.

## 1 Introduction

Diffusion and probability flow based models [2, 3, 4, 5, 6, 7, 8, 9, 10, 11] are now state-of-the-art in many fields, such as computer vision and image generation [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], natural language processing [23, 24], audio and video generation [25, 26, 27, 28, 29], optimization [30, 31], sampling and learning of fixed classes of distributions [32, 33, 34, 35, 36, 37, 38, 39, 40, 41], solving high-dimensional partial differential equations [42, 43, 44, 45, 46], and more recently several applications in physical, chemical and biological fields [47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]. For a more comprehensive list of related work, one may refer to the following review papers [64, 65, 66]. While there are already many variants, such as denoising diffusion probabilistic models (DDPMs) [7], score-based generative models (SGMs) [9], diffusion schrodinger bridges [67], stochastic interpolants and flow matching [2, 3, 4], _etc._, the recurring idea is to design a stochastic process that interpolates between the data distribution and some simple distribution, along which _score functions_ or alike are learned by neural network-based estimators, and then perform inference guided by the learned score functions.

Due to the sequential nature of the sampling process, the inference of high-quality samples from diffusion models often requires a large number of iterations and, thus, evaluations of the neural network-based score function, which can be computationally expensive [68]. Efforts have beenmade to accelerate this process by resorting to higher-order or randomized numerical schemes [69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79], augmented dynamics [80], adaptive step sizes [81], operator learning [82], restart sampling [83], self-consistency [84; 85; 86; 87] and knowledge distillation [88; 89; 90]. Recently, several empirical works [1; 91; 92; 93; 94] leverage the Picard iteration and triangular Anderson acceleration to parallelize the sampling procedure of diffusion models and achieve empirical success in large-scale image generation tasks. Some other recent work [95; 96] also combine the parallel sampling technique with the randomized midpoint method [97] to accelerate the inference of diffusion models.

This efficiency issue is closely related to the problem of bounding the required number of steps and evaluations of score functions to approximate an arbitrary data distribution on \(\mathbb{R}^{d}\) to \(\delta\)-accuracy, which has been analyzed extensively in the literature [98; 99; 100; 101; 102; 103; 104; 105]. In terms of the dependency on the dimension \(d\), the current state-of-the-art result for the SDE implementation of diffusion models is \(\widetilde{\mathcal{O}}(d)\)[107], improved from the previous \(\widetilde{\mathcal{O}}(d^{2})\) bound [104]. [111] gives a \(\widetilde{\mathcal{O}}(\sqrt{d})\) bound for the probability flow ODE implementation by considering a predictor-corrector scheme with the underdamped Langevin Monte Carlo (UMLC) algorithm.

In this work, we aim to provide parallelization strategies, rigorous analysis, and theoretical guarantees for accelerating the inference process of diffusion models. The time complexity of previous implementations of diffusion models has been largely hindered by the discretization error, which requires the step size to scale with \(\widetilde{\mathcal{O}}(1/d)\) for the SDE implementation and \(\widetilde{\mathcal{O}}(1/\sqrt{d})\) for the probability flow ODE implementation. We show that the inference process can be first divided into \(\mathcal{O}(1)\) blocks with parallelizable evaluations of the score function within each, and thus reduce the overall time complexity to \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\). We provide **the first implementation of diffusion models with poly-logarithmic complexity**, a significant improvement over the current state-of-the-art polynomial results that sheds light on the potential fast and efficient sampling of high-dimensional distributions with diffusion models on fast-developing memory-efficient modern GPU clusters.

### Contributions

* We propose parallelized inference algorithms for diffusion models in both the SDE and probability flow ODE implementations (PIADM-SDE/ODE) with exponential integrators, a shrinking step size scheme towards the data end, and the early stopping technique;
* We provide a rigorous convergence analysis of PIADM-SDE, showing that our parallelization strategy yields a diffusion model with \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) approximate time complexity;
* We show that our strategy is also compatible with the probability flow ODE implementation, and PIADM-ODE could improve the space complexity from \(\widetilde{\mathcal{O}}(d^{2})\) to \(\widetilde{\mathcal{O}}(d^{3/2})\) while maintaining the poly-logarithmic time complexity.

## 2 Preliminaries

In this section, we briefly recapitulate the framework of score-based diffusion models, define notations, and discuss related work.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Work** & **Implementation** & **Measure** & **Approx. Time Complexity** \\ \hline
[100, Theorem 2] & SDE & \(\mathrm{TV}(p_{0},\widehat{q}_{T})^{2}\) & \(\widetilde{\mathcal{O}}(d\delta^{-1})\) \\
[104, Theorem 2] & SDE & \(D_{\mathrm{KL}}(p_{\eta}\|\widehat{q}_{T-\eta})\) & \(\widetilde{\mathcal{O}}(d^{2}\delta^{-2})\) \\
[107, Corollary 1] & SDE & \(D_{\mathrm{KL}}(p_{\eta}\|\widehat{q}_{T-\eta})\) & \(\widetilde{\mathcal{O}}(d\delta^{-2})\) \\
[111, Theorem 3] & ODE w/UMLC correction & \(\mathrm{TV}(p_{\eta},\widehat{q}_{T-\eta})^{2}\) & \(\widetilde{\mathcal{O}}(\sqrt{d}\delta^{-1})\) \\
**Theorem 3.3** & **SDE w/parallel sampling** & \(\bm{D_{\mathrm{KL}}(p_{\eta}\|\widehat{q}_{T-\eta})}\) & \(\widetilde{\mathcal{O}}(\mathrm{poly}\log(d\delta^{-2}))\) \\
**Theorem 3.5** & **ODE w/parallel sampling** & \(\mathbf{TV}(p_{\eta},\widehat{q}_{T-\eta})^{2}\) & \(\widetilde{\mathcal{O}}(\mathrm{poly}\log(d\delta^{-2}))\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of the approximate time complexity (_cf._ Definition 2.1) of different implementations of diffusion models. \(\eta\) is a small parameter that controls the smooth approximation of the data distribution (_cf._ Section 3.1.1).

### Diffusion Models

In score-based diffusion models, one considers a diffusion process \((\bm{x}_{s})_{s\geq 0}\) in \(\mathbb{R}^{d}\) governed by the following stochastic differential equation (SDE):

\[\mathrm{d}\bm{x}_{s}=\bm{\beta}_{s}(\bm{x}_{s})\mathrm{d}s+\bm{\sigma}_{s} \mathrm{d}\bm{w}_{s},\quad\text{with}\quad\bm{x}_{0}\sim p_{0},\] (2.1)

where \((\bm{w}_{s})_{s\geq 0}\) is a standard Brownian motion, and \(p_{0}\) is the target distribution that we would like to sample from. The distribution of \(\bm{x}_{s}\) is denoted by \(p_{s}\). Once the drift \(\bm{\beta}_{s}(\cdot)\), the diffusion coefficient \(\bm{\sigma}_{s}\), and a sufficiently large time horizon \(T\) are specified, (2.1) also corresponds to a backward process \((\bm{\bar{x}}_{t})_{0\leq t\leq T}\) for another arbitrary diffusion coefficient \((\bm{v}_{s})_{s\geq 0}\)[116]:

\[\mathrm{d}\bm{\bar{x}}_{t}=\left[-\bm{\tilde{\beta}}_{t}(\bm{\bar{x}}_{t})+ \frac{\bm{\tilde{\sigma}}_{t}\bm{\tilde{\sigma}}_{t}^{\top}+\bm{\tilde{v}}_{t }\bm{\tilde{v}}_{t}^{\top}}{2}\nabla\log\tilde{p}_{t}(\bm{\bar{x}}_{t})\right] \mathrm{d}t+\bm{\tilde{v}}_{t}\mathrm{d}\bm{w}_{t},\] (2.2)

where \(\bm{\tilde{\ast}}_{t}\) denotes \({\ast}_{T-t}\), with \(\tilde{p}_{0}=p_{T}\) and \(\tilde{p}_{T}=p_{0}\).

For notational simplicity, we adopt a simple choice of the drift and the diffusion coefficients in what follows: \(\bm{\beta}_{t}(\bm{x})=-\frac{1}{2}\bm{x}\), \(\bm{\sigma}_{t}=\bm{I}_{d}\), and \(\bm{v}=\upsilon\bm{I}_{d}\), under which (2.1) is an Ornstein-Uhlenbeck (OU) process converging exponentially to its stationary distribution, _i.e._\(p_{T}\approx\tilde{p}_{T}:=\mathcal{N}(0,\bm{I}_{d})\), and (2.1) and (2.2) reduce to the following form:

\[\mathrm{d}\bm{x}_{s}=-\frac{1}{2}\bm{x}_{s}\mathrm{d}s+\mathrm{d}\bm{w}_{s}, \quad\text{and}\quad\mathrm{d}\bm{\bar{x}}_{t}=\left[\frac{1}{2}\bm{\bar{x}}_ {t}+\frac{1+\upsilon^{2}}{2}\nabla\log\tilde{p}_{t}(\bm{\bar{x}}_{t})\right] \mathrm{d}t+\upsilon\mathrm{d}\bm{w}_{t}.\] (2.3)

In practice, the score function \(\nabla\tilde{p}_{t}(\bm{\bar{x}}_{t})\) is often estimated by a neural network (NN) \(\bm{s}_{t}^{\theta}(\bm{x}_{t})\), where \(\theta\) represents its parameters, by minimizing the denoising score-matching loss [117; 118]:

\[\mathcal{L}(\theta) :=\mathbb{E}_{\bm{x}_{t}\sim p_{t}}\left[\left\|\nabla\log p_{t}( \bm{x}_{t})-\bm{s}_{t}^{\theta}(\bm{x}_{t})\right\|^{2}\right]\] (2.4) \[=\mathbb{E}_{\bm{x}_{0}\sim p_{0}}\left[\mathbb{E}_{\bm{x}_{t} \sim p_{t|0}(\bm{x}_{t}|\bm{x}_{0})}\left[\left\|\frac{\bm{x}_{t}-\bm{x}_{0}e ^{-t/2}}{1-e^{-t}}-\bm{s}_{t}^{\theta}(\bm{x}_{t})\right\|^{2}\right]\right],\]

and the backward process in (2.3) is approximated by the following SDE thereafter:

\[\mathrm{d}\bm{y}_{t}=\left[\frac{1}{2}\bm{y}_{t}+\frac{1+\upsilon^{2}}{2}\bm {s}_{t}^{\theta}(\bm{y}_{t})\right]\mathrm{d}t+\upsilon\mathrm{d}\bm{w}_{t}, \quad\text{with}\quad\bm{y}_{0}\sim\mathcal{N}(0,\bm{I}_{d}).\] (2.5)

Implementations.Diffusion models admit multiple _implementations_ depending on the choice of the parameter \(\upsilon\) in the backward process (2.2). The SDE implementation with \(\upsilon=1\) is widely used in the literature for its simplicity and efficiency [10], while recent studies [111] claim that the probability flow ODE implementation with \(\upsilon=0\) may exhibit better time complexity. We refer to [111; 119] for theoretical and [120; 121] for empirical comparisons of different implementations.

### Parallel Sampling

Parallel sampling algorithms have been actively explored in the literature, including the parallel tempering method [122; 123; 124] and several recent studies [125; 126; 127]. For diffusion models, the idea of parallel sampling is based on the _Picard iteration_[128; 129] for solving nonlinear ODEs. Suppose we have an ODE \(\mathrm{d}\bm{x}_{t}=\bm{f}_{t}(\bm{x}_{t})\mathrm{d}t\) and we would like to solve it for \(t\in[0,T]\), then the Picard iteration is defined as follows:

\[\bm{x}_{t}^{(0)}\equiv\bm{x}_{0},\quad\text{and}\quad\bm{x}_{t}^{(k+1)}:=\bm{ x}_{0}+\int_{0}^{t}\bm{f}_{s}(\bm{x}_{s}^{(k)})\mathrm{d}s,\quad\text{for}\ k\in[0:K-1].\] (2.6)

Under assumptions on the Lipschitz continuity of \(\bm{f}_{t}\), the Picard iteration converges to the true solution exponentially fast, in the sense that \(\|\|\bm{x}_{t}^{(k)}-\bm{x}_{t}\|\|_{L^{\infty}([0,T])}\leq\delta\) with \(K=\mathcal{O}(\log\delta^{-1})\) iterations. Unlike high-order ODE solvers, the Picard iteration is intrinsically parallelizable: for any \(t\in[0,T]\), the computation of \(\bm{x}_{t}^{(k+1)}\) relies merely on the values of the most recent iteration \(\bm{x}_{t}^{(k)}\). With sufficient computational sources parallelizing the evaluations of \(\bm{f}\), the computational cost of solving the ODE no longer scales with \(T\) but with the number of iterations \(K\).

Recently, this idea has been applied to both the Langevin Monte Carlo (LMC) and the underdamped Langevin Monte Carlo (UMLC) contexts [130]. Roughly speaking, it is proposed to simulate the Langevin diffusion process \(\mathrm{d}\bm{x}_{t}=-\nabla V(\bm{x}_{t})\mathrm{d}t+\mathrm{d}\bm{w}_{t}\) with the following iteration resembling (2.6):

\[\bm{x}_{t}^{(0)}\equiv\bm{x}_{0},\quad\text{and}\quad\bm{x}_{t}^{(k+1)}:=\bm{x }_{0}-\int_{0}^{t}\nabla V(\bm{x}_{t}^{(k)})\mathrm{d}s+\bm{w}_{t},\quad\text{ for }k\in[0:K-1],\] (2.7)

where all iterations share a common Wiener process \((\bm{w}_{t})_{t\geq 0}\).

It is shown that for well-conditioned log-concave distributions, parallelized LMC would achieve an iteration depth of \(K=\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) that matches the indispensable time horizon \(T=\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) to achieve exponential ergodicity (_cf._[130, Theorem 13]). This promises a significant speedup in sampling high-dimensional distributions from the standard LMC of \(T=\widetilde{\mathcal{O}}(d)\) iterations, hindered by the \(o(1/d)\) step size as imposed by the discretization error and now evaded by the parallelization.

### Approximate Time Complexity

A similar situation is expected in diffusion models, where the application bottleneck is largely the inference process with sequential iterations and expensive evaluations of the learned score function \(\bm{s}_{t}^{\theta}(\cdot)\), which is often parametrized by large-scale NNs. Despite several unavoidable costs involving pre- and post-processing, data storage and retrieval, and arithmetic operations, we define the following notion of the _approximate time complexity_ of the inference process of diffusion models:

**Definition 2.1** (Approximate time complexity).: _For a specific implementation of diffusion models (2.5), we define the approximate time complexity of the sampling process as the number of unparallelizable evaluations of the learned NN-based score function \(\bm{s}_{t}^{\theta}(\cdot)\)._

This definition coincides with the notion of _the number of steps required to reach a certain accuracy_ in [104; 100], _iteration complexity_ in [107; 111], _etc._ in the previous theoretical studies of diffusion models. We have adopted this notion in Table 1 for a comparison of the current state-of-the-art results and our bounds in this work. We will use the notion of _space complexity_ likewise to denote the approximate required storage during the inference. Trivially, the space complexity of the sequential implementation is \(\mathcal{O}(d)\). Should no confusion occur, we omit the dependency of the complexities above on the accuracy threshold \(\delta\), _etc._, during our discussion, as we focus on applications of diffusion models to high-dimensional data distributions, following the standard practice in the literature.

## 3 Main Results

Inspired by the acceleration achieved by the parallel sampling technique in LMC and ULMC, we aim to accommodate parallel sampling into the theoretical analysis framework of diffusion models. The benefit of the parallel sampling technique in this scenario has been recently confirmed by up to 14\(\times\) acceleration achieved by the ParaDiGMS algorithm [1] and ParaTAA [92], where several practical compromises are made to mitigate GPU memory constraints and theoretical guarantees are still lacking.

In this section, we will propose **P**arallelized **I**nference **A**lgorithms for **D**iffusion **M**odels with both the **SDE** and probability flow **ODE** implementations, namely the **PIADM-SDE** (Algorithm 1) and **PIADM-ODE** (Algorithm 2), and present theoretical guarantees of our algorithms, including the approximate time complexity and space complexity, for both implementations in Section 3.1 and Section 3.2, respectively. Due to the large number of notations used in the presentation, we give an overview of notations in Appendix A.1 for readers' convenience.

### SDE Implementation

We first focus on the approximation, parallelization strategies, and error analysis of diffusion models with the SDE implementation, _i.e._ the forward and backward process (2.3) and its approximation (2.5) with \(\upsilon=1\). We will show that PIADM-SDE _achieves an \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) approximate time complexity with \(\widetilde{\mathcal{O}}(d^{2})\) space complexity_.

#### 3.1.1 Algorithm

PIADM-SDE is summarized in Algorithm 1 and illustrated in Figure 1. The main idea behind our algorithm is the fact that (2.5) can be efficiently solved by the Picard iteration within a period of \(\mathcal{O}(1)\) length, transferring \(\widetilde{\mathcal{O}}(d)\) sequential computations to a parallelizable iteration of depth \(\widetilde{\mathcal{O}}(\log d)\). In the following, we introduce the numerical discretization scheme of our algorithm and the implementation of the Picard iteration in detail.

Step Size Scheme.In our algorithm, the time horizon \(T\) is first segmented into \(N\) blocks of length \(\left(h_{n}\right)_{n=0}^{N-1}\), with each \(h_{n}\leq h:=T/N=\Omega(1)\), forming a grid \((t_{n})_{n=0}^{N}\) with \(t_{n}=\sum_{j=1}^{n}h_{j}\). For any \(n\in[0:N-1]\), the \(n\)-th block is further discretized into a grid \((\tau_{n,m})_{m=0}^{M_{n}}\) with \(\tau_{n,0}=0\) and \(\tau_{n,M_{n}}=h_{n}\). We denote the step size of the \(m\)-th step in the \(n\)-th block as \(\epsilon_{n,m}=\tau_{n,m+1}-\tau_{n,m}\), and the total number of steps in the \(n\)-th block as \(M_{n}\).

For the first \(N-1\) blocks, we simply use the unique discretization, _i.e._\(h_{n}=h\), \(\epsilon_{n,m}=\epsilon\), and \(M_{n}=M:=h/\epsilon\), for \(n\in[0:N-2]\) and \(m\in[0:M-1]\). Following [104, 107], to curb the potential blow-up of the score function as \(t\to T\), which is shown by [107] for \(0\leq s<t<T\) to be of the order

\[\mathbb{E}\left[\int_{s}^{t}\|\nabla\log\widetilde{p}_{\tau}(\vec{\boldsymbol {x}}_{\tau})-\nabla\log\widetilde{p}_{s}(\vec{\boldsymbol{x}}_{s})\|^{2} \mathrm{d}\tau\right]\lesssim d\left(\frac{t-s}{T-t}\right)^{2},\]

we apply early stopping at time \(t_{N}=T-\eta\), where \(\eta\) is chosen in a way such that the \(\mathcal{O}(\sqrt{\eta})\) 2-Wasserstein distance between \(\widetilde{p}_{T}\) and its smoothed version \(\widetilde{p}_{T-\eta}\) that we aim to sample from alternatively, is tolerable for the downstream tasks. We also impose the exponential decay of the step size towards the data end in the last block. To be specific, we let \(h_{N-1}=h-\delta\), and discretize the interval \([t_{N-1},t_{N}]=[(N-1)h,T-\eta]\) into a grid \((\tau_{N-1,m})_{m=0}^{M_{N-1}}\) with step sizes \((\epsilon_{N-1,m})_{m=0}^{M_{N}-1}\) satisfying

\[\epsilon_{N-1,m}\leq\epsilon\wedge\epsilon\left(h-\tau_{N-1,m+1}\right).\] (3.1)

As shown in Lemma B.7, this exponential decaying step size scheme towards the data end is crucial to bound the discretization error in the last block.

For the simplicity of notations, we introduce the following indexing function: for \(\tau\in[t_{n},t_{n+1}]\), we define \(I_{n}(\tau)\) to be the unique integer such that \(\sum_{j=1}^{I_{n}(\tau)}\epsilon_{n,j}\leq\tau<\sum_{j=1}^{I_{n}(\tau)+1} \epsilon_{n,j}\). We also define

Figure 1: Illustration of PIADM-SDE/ODE. The outer iterations are divided into \(\mathcal{O}(\log d)\) blocks of \(\mathcal{O}(1)\) length. Within each block, the inner iterations are parallelized with \(\widetilde{\mathcal{O}}(d)\) steps for SDE (_cf. Theorem 3.3_), or \(\widetilde{\mathcal{O}}(\sqrt{d})\) for probability flow ODE implementation (_cf. Theorem 3.5_). The overall approximate time complexity is \(KN=\widetilde{\mathcal{O}}(\operatorname{poly}\log d)\). brown, green, blue, and red curves represent the computation graph at \(t=t_{n}+\tau_{n,m}\) for \(m=1,2,M_{n}-1,M_{n}\).

a piecewise function \(g\) such that \(g_{n}(\tau)=\sum_{j=1}^{I_{n}(\tau)}\epsilon_{n,j}\). It is easy to check that under the uniform discretization for \(n\in[1:N-1]\), we have \(I_{n}(\tau)=\lfloor\tau/\epsilon\rfloor\) and \(g_{n}(\tau)=\lfloor\tau/\epsilon\rfloor\epsilon\).

Exponential IntegratorFor each step \(\tau\in[t_{n}+\tau_{n,m},t_{n}+\tau_{n,m+1}]\), we use the following exponential integrator scheme [77], as the numerical discretization of the SDE (2.5):

\[\widehat{\boldsymbol{y}}_{t_{n},\tau_{n,m+1}}=e^{\epsilon_{n,m}/2}\widehat{ \boldsymbol{y}}_{t_{n},\tau_{n,m}}+2\left(e^{\epsilon_{n,m}/2}-1\right) \boldsymbol{s}_{t_{n}+\tau_{n,m}}^{\theta}(\widehat{\boldsymbol{y}}_{t_{n}+ \tau_{n,m}})+\sqrt{e^{\epsilon_{n,m}}-1}\boldsymbol{\xi},\]

where \(\boldsymbol{\xi}\sim\mathcal{N}(0,\boldsymbol{I}_{d})\). Lemma B.3 shows its equivalence to approximating (2.5) as

\[\mathrm{d}\widehat{\boldsymbol{y}}_{t_{n},\tau}=\left[\frac{1}{2}\widehat{ \boldsymbol{y}}_{t_{n},\tau}+\boldsymbol{s}_{t_{n}+\tau_{n,m}}^{\theta}( \widehat{\boldsymbol{y}}_{t_{n},\tau_{n,m}})\right]\mathrm{d}\tau+\mathrm{d} \boldsymbol{w}_{t_{n}+\tau},\quad\text{for }\tau\in[\tau_{n,m},\tau_{n,m+1}].\] (3.2)

**Remark 3.1**.: _One could also implement a straightforward Euler-Maruyama scheme instead of the exponential integrator (3.4), where an additional high-order discretization error term would emerge [104, Theorem 1], which we believe would not affect the overall \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) time complexity with parallel sampling._

Picard Iteration.Within each block, we apply Picard iteration of depth \(K\). As shown by Lemma B.3, the discretized scheme (3.4) implements the following iteration for \(k\in[0:K-1]\):

\[\mathrm{d}\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(k+1)}=\left[\frac{1}{2} \widehat{\boldsymbol{y}}_{t_{n},\tau}^{(k+1)}+\boldsymbol{s}_{t_{n}+g_{n}( \tau)}^{\theta}\Big{(}\widehat{\boldsymbol{y}}_{t_{n},g_{n}(\tau)}^{(k)} \Big{)}\right]\mathrm{d}\tau+\mathrm{d}\boldsymbol{w}_{t_{n}+\tau},\quad \text{for }\tau\in[0,h_{n}].\] (3.3)

We denote the distribution of \(\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(K)}\) by \(\widehat{\boldsymbol{y}}_{t_{n}+\tau}\). As proved in Lemma B.6, the iteration above would converge to (3.2) in each block exponentially fast, which given a sufficiently accurate learned score estimation \(\boldsymbol{s}_{t}^{\theta}\) should be close to the true backward SDE (2.3). One should also notice that the Gaussians \(\boldsymbol{\xi}_{m}\) are only sampled once and used for all iterations.

The parallelization for (3.4) in Algorithm 1 should be understood as that for any \(k\in[0:K-1]\), each \(\boldsymbol{s}_{t_{n}+\tau_{n,j}}^{\theta}(\widehat{\boldsymbol{y}}_{t_{n}, \tau_{n,j}}^{(k)})\) for \(j\in[0:M_{n}]\) is evaluated in parallel, with subsequent floating-point operations comparably negligible, resulting in the overall \(\mathcal{O}(NK)\) approximate time complexity.

#### 3.1.2 Assumptions

Our theoretical analysis will be built on the following mild assumptions on the regularity of the data distribution and the numerical properties of the neural networks:

**Assumption 3.1** (\(L^{2}([0,t_{N}])\)\(\delta\)-accurate learned score).: _The learned NN-based score \(\bm{s}_{t}^{\theta}\) is \(\delta_{2}\)-accurate in the sense of_

\[\mathbb{E}_{\widetilde{p}}\left[\sum_{n=0}^{N-1}\sum_{m=0}^{M_{n}-1}\epsilon_{n, m}\left\|\bm{s}_{t_{n}+\tau_{n,m}}^{\theta}\big{(}\bm{\tilde{x}}_{t_{n}+\tau_{n,m}} \big{)}-\nabla\log\widetilde{p}_{t_{n}+\tau_{n,m}}\big{(}\bm{\tilde{x}}_{t_{n}+ \tau_{n,m}}\big{)}\right\|^{2}\right]\leq\delta_{2}^{2}.\] (3.5)

**Assumption 3.2** (Regular and normalized data distribution).: _The data distribution \(p_{0}\) has finite second moments and is normalized such that \(\mathrm{cov}_{p_{0}}(\bm{x}_{0})=\bm{I}_{d}\)._

**Assumption 3.3** (Bounded and Lipschitz learned NN-based score).: _The learned NN-based score function \(\bm{s}_{t}^{\theta}\) has bounded \(C^{1}\) norm, i.e. \(\||\bm{s}_{t}^{\theta}(\cdot)\||_{L^{\infty}([0,T])}\leq M_{\bm{s}}\) with Lipschitz constant \(L_{\bm{s}}\)._

**Remark 3.2**.: _Assumption 3.1 and the finite moment assumption in Assumption 3.2 are standard assumptions across previous theoretical works on diffusion models [100, 104, 111], while we adopt the normalization Assumption 3.2 from [107] to simplify true score function-related computations (cf. Lemma A.8). Assumption 3.3 can be easily satisfied by truncation, ensuring computational stability. Notice that the exponential integrator, one actually applies Picard iteration to \(e^{-t/2}\bm{s}_{t}^{\theta}\), a relaxation of Assumption 3.1 might be possible, which is left for future work._

#### 3.1.3 Theoretical Guarantees

The following theorem summarizes our theoretical analysis for PIADM-SDE (Algorithm 1):

**Theorem 3.3** (Theoretical Guarantees for PIADM-SDE).: _Under Assumptions 3.1, 3.2, and 3.3, given the following choices of the order of the parameters_

\[T=\mathcal{O}(\log(d\delta^{-2})),\quad h=\Theta(1),\quad N= \mathcal{O}\left(\log(d\delta^{-2})\right),\] \[\epsilon=\Theta\left(d^{-1}\delta^{2}\log^{-1}(d\delta^{-2}) \right),\quad M=\mathcal{O}\left(d\delta^{-2}\log(d\delta^{-2})\right),\quad K =\widetilde{\mathcal{O}}(\log(d\delta^{-2})),\]

_and let \(L_{\bm{s}}^{2}h_{n}e^{\frac{7}{2}h_{n}}\ll 1\), \(\delta_{2}\lesssim\delta\), \(T\lesssim\log\eta^{-1}\), the distribution \(\widehat{q}_{t_{N}}\) that PIADM-SDE (Algorithm 1) generates samples from satisfies the following error bound:_

\[D_{\mathrm{KL}}(p_{\eta}\|\widehat{q}_{t_{N}})\lesssim de^{-T}+d\epsilon T+ \delta_{2}^{2}+dTe^{-K}\lesssim\delta^{2},\]

_with a total of \(KN=\widetilde{\mathcal{O}}\left(\log^{2}(d\delta^{-2})\right)\) approximate time complexity and \(dM=\widetilde{\mathcal{O}}\left(d^{2}\delta^{-2}\right)\) space complexity for parallailzable \(\delta\)-accurate score function computations._

**Remark 3.4**.: _We would like to make the following remarks on the result above:_

* _The acceleration from_ \(\widetilde{\mathcal{O}}(d)\) _to_ \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) _is at the cost of a trade-off with extra memory cost of_ \(M=\widetilde{\mathcal{O}}(d)\) _for computing and updating_ \(\{\bm{s}_{t_{n}+\tau_{n,j}}^{\theta}(\widehat{\bm{y}}_{t_{n},\tau_{n,m}}^{(k)} )\}_{m\in[0:M_{n}]}\) _simultaneously during each Picard iteration;_
* _Compared with log-concave sampling_ _[_130_]__,_ \(M\) _being of order_ \(\widetilde{\mathcal{O}}(d)\) _instead of_ \(\widetilde{\mathcal{O}}(\sqrt{d})\) _therein is partly due to the time independence of the score function_ \(\nabla\log p(\cdot)\) _in general sampling tasks. Besides, the scaling_ \(M=\widetilde{\mathcal{O}}(d)\) _agrees with the current state-of-the-art dependency_ _[_107_]_ _for the SDE implementation of diffusion models;_
* _As mentioned above, the scale of the step size_ \(\epsilon\) _within one block is still confined to_ \(\Theta(1/M)=\widetilde{\Theta}\left(1/d\right)\)_. The block length_ \(h\)_, despite being required to be small compared to_ \(1/L_{\bm{s}}\)_, is of order_ \(\Theta(1)\)_, resulting in only_ \(\Theta(\log d)\) _blocks and thus_ \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) _total iterations._

#### 3.1.4 Proof Sketch

The detailed proof of Theorem 3.3 is deferred to Section B. The pipeline of the proof is to (a) first decompose the error \(D_{\mathrm{KL}}(\widetilde{p}_{t_{N}}\|\widehat{q}_{t_{N}})\) into blockwise errors using the chain rule of KL divergence; (b) bound the error in each block by invoking Girsanov's theorem; (c) sum up the errors in all blocks.

The key technical challenge lies in Step (b). Different from all previous theoretical works [100, 104, 111], the Picard iteration in our algorithm generates \(K\) paths recursively in each block using the learned score \(\bm{s}_{t}^{\theta}\). And therefore the final path \((\widehat{\bm{y}}_{t_{n},\tau}^{(K)})_{\tau\in[0,h_{n}]}\) depends on all previous paths \((\widehat{\bm{y}}_{t_{n},\tau}^{(k)})_{\tau\in[0,h_{n}]}\) for \(k\in[0:K-1]\), ruling out a direct change of measure argument from thenaive application of Girsanov's theorem. To this end, we need a more sophisticated mathematical framework of stochastic processes, as given in Appendix A.2. We define the measurable space \((\Omega,\mathcal{F})\) with filtrations \((\mathcal{F}_{t})_{t\geq 0}\) to specify the probability measures on \((\Omega,\mathcal{F})\) of each Wiener process, and resort to one of the most general forms of Girsanov's theorem ([131, Theorem 8.6.6]). For example, in the \(n\)-th block, we apply the following change of measure procedure:

1. Let \(q|_{\mathcal{F}_{t_{n}}}\) be the measure where \(\bm{w}_{t}(\omega)\) is the shared Wiener process in the Picard iteration (3.3) for any \(k\in[0:K-1]\);
2. Define another process \(\mathrm{d}\widetilde{\bm{w}}_{t_{n}+\tau}(\omega)=\mathrm{d}\bm{w}_{t_{n}+\tau }(\omega)+\bm{\delta}_{t_{n}}(\tau,\omega)d\tau\), where \[\bm{\delta}_{t_{n}}(\tau,\omega):=\bm{s}^{\theta}_{t_{n}+g_{n}(\tau)}(\widetilde {\bm{y}}^{(K-1)}_{t_{n},g_{n}(\tau)}(\omega))-\nabla\log\widetilde{p}_{t_{n}+ \tau}(\widetilde{\bm{y}}^{(K)}_{t_{n}+\tau}(\omega));\]
3. Invoke Girsanov's theorem, which yields that the Radon-Nikodym derivative of the measure \(\widetilde{p}|_{\mathcal{F}_{t_{n}}}\) with respect to \(q|_{\mathcal{F}_{t_{n}}}\) satisfies \[\log\frac{\mathrm{d}\widetilde{p}|_{\mathcal{F}_{t_{n}}}}{\mathrm{d}q|_{ \mathcal{F}_{t_{n}}}}(\omega)=-\int_{0}^{h_{n}}\bm{\delta}_{t_{n}}(\tau, \omega)^{\top}\mathrm{d}\bm{w}_{t_{n}+\tau}(\omega)-\frac{1}{2}\int_{0}^{h_{n} }\|\bm{\delta}_{t_{n}}(\tau,\omega)\|^{2}\mathrm{d}\tau;\]
4. Conclude that \((\widetilde{\bm{w}}_{t_{n}+\tau})_{\tau\geq 0}\) is a Wiener process under the measure \(\widetilde{p}|_{\mathcal{F}_{t_{n}}}\) and thus (3.3) at iteration \(K\) satisfies the following SDE: \[\mathrm{d}\widetilde{\bm{y}}^{(K)}_{t_{n},\tau}(\omega)=\left[\frac{1}{2} \widetilde{\bm{y}}^{(K)}_{t_{n},\tau}(\omega)+\nabla\log\widetilde{p}_{t_{n}+ \tau}\big{(}\widetilde{\bm{y}}^{(K)}_{t_{n},\tau}(\omega)\big{)}\right] \mathrm{d}\tau+\mathrm{d}\widetilde{\bm{w}}_{t_{n}+\tau}(\omega),\] _i.e._ the true backward SDE (2.3) with the true score function for \(\tau\in[t_{n},t_{n+1}]\).

One should notice that this change of measure argument will cause an additional term in the bound of the discrepancy between the first iteration \(\widetilde{\bm{y}}^{(1)}_{t_{n},\tau}\) and the initial condition \(\widetilde{\bm{y}}^{(0)}_{t_{n},\tau}\) in Lemma B.5. However, due to the exponential convergence of the Picard iteration, this term does not affect the overall error bound.

### Probability Flow ODE Implementation

In this section, we will show that our parallelization strategy is also compatible with the probability ODE implementation of diffusion models, _i.e._ the forward and backward process (2.3) and its approximatation (2.5) with \(v=0\). We will demonstrate that PIADM-ODE (Algorithm 2) further _improves the space complexity from \(\widetilde{\mathcal{O}}(d^{2})\) to \(\widetilde{\mathcal{O}}(d^{3/2})\) while maintaining the same \(\widetilde{\mathcal{O}}(\mathrm{poly}\log d)\) approximate time complexity_.

#### 3.2.1 Algorithm

Due to the space limit, we refer the readers to Section C.1 and Algorithm 2 for the details of our parallelization of the probability flow ODE formulation of diffusion models. PIADM-ODE keeps the discretization scheme detailed in Section 3.1.1 that divides the time horizon \(T\) into \(N\) blocks and uses exponential integrators for all updating rules. Notably, PIADM-ODE has the following distinctions compared with PIADM-SDE (Algorithm 1):

* Instead of applying Picard iteration to the backward SDE as in (3.2), we apply Picard iteration to the probability flow ODE as in (C.3) within each block, which does not require sampling i.i.d. Gaussians to simulate a Wiener process;
* The most significant difference is the adoption of an additional _corrector step_[111] after running the probability flow ODE with Picard iteration within one block. During the corrector step, one augments the state space with a Gaussian that represents the initial momentum and then simulates an underdamped Langevin dynamics for \(\mathcal{O}(1)\) time with the learned NN-based score function at the time of the block end;
* We then further parallelize the underdamped Langevin dynamics in the corrector step so that it can also be accomplished with \(\mathcal{O}(\log d)\) approximate time complexity, as a naive implementation would result in \(\widetilde{\mathcal{O}}(\sqrt{d})\)[130], which is incompatible with our desired poly-logarithmic guarantee.

#### 3.2.2 Assumptions

Due to technicalities specific to this implementation, we need first to modify Assumption 3.1 and add assumption on the Lipschitzness of the true score functions \(\nabla\log p_{t}\), which is a common practice in related literature [104, 111]. Recent work on the probability flow ODE implementation [112, 114] also adopts stronger assumptions compared to the SDE implementation.

**Assumption 3.1'** (\(L^{\infty}([0,t_{N}])\)\(\delta\)-accurate learned score).: _For any \(n\in[0:N-1]\) and \(m\in[0:M_{n}-1]\), the learned NN-based score \(\bm{s}^{\theta}_{t_{n},\tau_{n,m}}\) is \(\delta_{\infty}\)-accurate in the sense of_

\[\mathbb{E}_{\tilde{p}_{t_{n}+\tau_{n,m}}}\left[\left\|\bm{s}^{\theta}_{t_{n}+ \tau_{n,m}}\big{(}\bm{\tilde{x}}_{t_{n}+\tau_{n,m}}\big{)}-\nabla\log\tilde{p} _{t_{n}+\tau_{n,m}}\big{(}\bm{\tilde{x}}_{t_{n}+\tau_{n,m}}\big{)}\right\|^{2} \right]\leq\delta_{\infty}^{2}.\]

**Assumption 3.4** (Bounded and Lipschitz true score).: _The true score function \(\nabla\log p_{t}\) has bounded \(C^{1}\) norm, i.e. \(\||\nabla\log p_{t}(\cdot)\||_{L^{\infty}([0,T])}\leq M_{p}\) with Lipschitz constant \(L_{p}\)._

Further relaxations on Assumption 3.4 to time-dependent assumptions accommodating the blow-up to the data end (_e.g._[103, Assumption 1.5]) are left for further work.

#### 3.2.3 Theoretical Guarantees

Our results for PIADM-ODE are summarized in the following theorem:

**Theorem 3.5** (Theoretical Guarantees for PIADM-ODE).: _Under Assumptions 3.1', 3.2, 3.3, and 3.4, given the following choices of the order of the parameters_

\[T=\mathcal{O}(\log(d\delta^{-2})),\quad h=\Theta(1),\quad N= \mathcal{O}(\log(d\delta^{-2})),\] \[\epsilon=\Theta\left(d^{-1/2}\delta\log^{-1}(d^{-1/2}\delta^{-1}) \right),\quad M=\mathcal{O}(d^{1/2}\delta^{-1}\log(d^{1/2}\delta^{-1})),\quad K =\widetilde{\mathcal{O}}(\log(d\delta^{-2})),\]

_for the outer iteration and_

\[T^{\dagger}=\mathcal{O}(1)\lesssim L_{p}^{-1/2}\wedge L_{\bm{s} }^{-1/2},\quad h^{\dagger}=\Theta(1),\quad N^{\dagger}=\mathcal{O}(1),\] \[\epsilon^{\dagger}=\Theta(d^{-1/2}\delta),\quad M^{\dagger}= \mathcal{O}(d^{1/2}\delta^{-1}),\quad K^{\dagger}=\mathcal{O}(\log(d\delta^{- 2})),\]

_for the inner iteration during the corrector step, and let \(L_{\bm{s}}^{2}h^{2}e^{h}\lor L_{\bm{s}}^{2}{h^{\dagger}}^{2}e^{h^{\dagger}}/\gamma\ll 1\), \(\delta_{\infty}\lesssim\delta\log^{-1}(d\delta^{-2})\), and \(\gamma\gtrsim L_{p}^{1/2}\), then the distribution \(\widehat{q}_{t_{N}}\) that PIADM-ODE (Algorithm 2) generates samples from satisfies the following error bound:_

\[\mathrm{TV}(p_{\eta},\widehat{q}_{t_{N}})^{2}\lesssim de^{-T}+d \epsilon^{2}T^{2}+(T^{2}+N^{2})\delta_{\infty}^{2}+dN^{2}e^{-K}\lesssim\delta^ {2},\]

_with a total of \((K+K^{\dagger}N^{\dagger})N=\widetilde{\mathcal{O}}\left(\log^{2}(d\delta^{-2 })\right)\) approximate time complexity and \(d(M\lor M^{\dagger})=\widetilde{\Theta}\left(d^{3/2}\delta^{-1}\right)\) space complexity for parallalizable \(\delta\)-accurate score function computations._

The reduction of space complexity by the probability flow ODE implementation is intuitively owing to the fact that the probability flow ODE process is a deterministic process in time rather than a stochastic process as in the SDE implementation, getting rid of the \(\mathcal{O}(\epsilon)\) term derived by Ito's symmetry. This allows the discretization error to be bounded with \(\mathcal{O}(\epsilon^{2})\) instead (_cf._ Lemma B.7 and C.5).

#### 3.2.4 Proof Sketch

The details of the proof of Theorem 3.5 are provided in Section C. Along with the complexity benefits the deterministic nature of the probability flow ODE may bring, the analysis is technically more involved than that of Theorem 3.3 and requires an intricate interplay between statistical distances. Several major challenges and our corresponding solutions are summarized below:

* The error of the parallelized algorithm within each block may now only be bounded by 2-Wasserstein distance (_cf._ Theorem C.7) instead of any \(f\)-divergence that admits data processing inequality as in the SDE case by Girsanov's theorem. The additional corrector step exactly handles this issue and would intuitively translate 2-Wasserstein proximity to TV distance proximity (_cf._ Lemma C.18), allowing the decomposition of the overall error into each block;* For the corrector step, the underdamped Langevin dynamics as a second-order dynamics requires only \(\mathcal{O}(\sqrt{d})\) steps to converge, instead of \(\mathcal{O}(d)\) steps in its overdamped counterpart. We then adapt the parallelization technique mentioned in Section 2.2 to conclude that it can be accomplished with \(\mathcal{O}(\log d)\) approximate time complexity (_cf._ Theorem C.17). The error caused by the approximation to the true score and numerical discretization within this step is bounded in KL divergence by invoking Girsanov's theorem(Theorem A.4) as in the proof of Theorem 3.3;
* Different from the SDE case, where the chain rule of KL divergence can easily decouple the initial distribution and the subsequent dynamics, we need several interpolating processes between the implementation and the true backward process in this case. The final guarantee is in TV distance as it connects with the KL divergence via Pinsker's inequality and admits data processing inequality. We refer the readers to Figure 2 for an overview of the proof pipeline, as well as the notations and intuitions of the auxiliary and interpolating processes appearing in the proof.

## 4 Discussion and Conclusion

In this work, we have proposed novel parallelization strategies for the inference of diffusion models in both the SDE and probability flow ODE implementations. Our algorithms, namely PIADM-SDE and PIADM-ODE, are meticulously designed and rigorously proved to achieve \(\widetilde{\mathcal{O}}(\operatorname{poly}\log d)\) approximate time complexity and \(\widetilde{\mathcal{O}}(d^{2})\) and \(\widetilde{\mathcal{O}}(d^{3/2})\) space complexity, respectively, marking the first inference algorithm of diffusion and probability flow based models with sub-linear approximate time complexity. Our algorithm intuitively divides the time horizon into several \(\mathcal{O}(1)\) blocks and applies Picard iteration within each block in parallel, transferring the time complexity into space complexity. Our analysis is built on a sophisticated mathematical framework of stochastic processes and provides deeper insights into the mathematical theory of diffusion models.

Our findings echo and corroborate the recent empirical work [1, 91, 92, 93, 94] that parallel sampling techniques significantly accelerate the inference process of diffusion models. Theoretical exploration of the adaptive block window scheme therein presents an interesting future research potential. Possible future work also includes the investigation of how to apply our parallelization framework to other variants of diffusion models, such as the discrete [132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 23, 142] and multi-marginal [143] formulations. Although we anticipate implementing diffusion models in parallel may introduce engineering challenges, _e.g._ scalability, hardware compatibility, memory bandwidth, _etc._, we believe that our theoretical contributions lay a solid foundation that not only supports but also motivates the empirical development of parallel inference algorithms for diffusion models since advancements continue in GPU power and memory efficiency.

## Acknowledgments and Disclosure of Funding

LY acknowledges the support of the National Science Foundation under Grant No. DMS-2011699 and DMS-2208163. GMR is supported by a Google Research Scholar Award.

## References

* [1] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [2] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.
* [3] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. _arXiv preprint arXiv:2209.15571_, 2022.
* [4] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* [5] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.

* [6] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [8] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. _Advances in neural information processing systems_, 34:1415-1428, 2021.
* [9] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* [10] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [11] Linfeng Zhang, Weinan E, and Lei Wang. Monge-ampere flow for generative modeling. _arXiv preprint arXiv:1809.10188_, 2018.
* [12] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In _International Conference on Machine Learning_, pages 1737-1752. PMLr, 2023.
* [13] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. _arXiv preprint arXiv:2401.14404_, 2024.
* [14] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _Journal of Machine Learning Research_, 23(47):1-33, 2022.
* [15] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. _arXiv preprint arXiv:2401.08740_, 2024.
* [16] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.
* [17] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLr, 2021.
* [18] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* [19] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [20] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. _arXiv preprint arXiv:2111.08005_, 2021.
* [21] Yu Sun, Zihui Wu, Yifan Chen, Berthy T Feng, and Katherine L Bouman. Provable probabilistic imaging using score-based generative priors. _arXiv preprint arXiv:2310.10835_, 2023.
* [22] Xingyu Xu and Yuejie Chi. Provably robust score-based diffusion posterior sampling for plug-and-play image reconstruction. _arXiv preprint arXiv:2403.17042_, 2024.
* [23] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.

* [24] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. _Advances in Neural Information Processing Systems_, 35:4328-4343, 2022.
* [25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _Advances in Neural Information Processing Systems_, 35:8633-8646, 2022.
* [26] Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. _arXiv preprint arXiv:2402.14285_, 2024.
* [27] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. _arXiv preprint arXiv:2103.16091_, 2021.
* [28] Flavio Schneider. Archisound: Audio generation with diffusion. _arXiv preprint arXiv:2301.13267_, 2023.
* [29] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. _Entropy_, 25(10):1469, 2023.
* [30] Zihao Li, Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Yinyu Ye, Minshuo Chen, and Mengdi Wang. Diffusion model for data-driven black-box optimization. _arXiv preprint arXiv:2403.13219_, 2024.
* [31] Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, and Yao Xie. Flow-based distributionally robust optimization. _IEEE Journal on Selected Areas in Information Theory_, 2024.
* [32] Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from mean-field gibbs measures via diffusion processes. _arXiv preprint arXiv:2310.08912_, 2023.
* [33] Sitan Chen, Vasilis Kontonis, and Kulin Shah. Learning general gaussian mixtures with efficient score matching. _arXiv preprint arXiv:2404.18893_, 2024.
* [34] Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 323-334. IEEE, 2022.
* [35] Khashayar Gatmiry, Jonathan Kelner, and Holden Lee. Learning mixtures of gaussians using diffusion models. _arXiv preprint arXiv:2404.18869_, 2024.
* [36] Ye He, Kevin Rojas, and Molei Tao. Zeroth-order sampling methods for non-log-concave distributions: Alleviating metastability by denoising diffusion. _arXiv preprint arXiv:2402.17886_, 2024.
* [37] Xunpeng Huang, Hanze Dong, HAO Yifan, Yian Ma, and Tong Zhang. Reverse diffusion monte carlo. In _The Twelfth International Conference on Learning Representations_, 2023.
* [38] Brice Huang, Andrea Montanari, and Huy Tuan Pham. Sampling from spherical spin glasses in total variation via algorithmic stochastic localization. _arXiv preprint arXiv:2404.15651_, 2024.
* [39] Song Mei and Yuchen Wu. Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models. _arXiv preprint arXiv:2309.11420_, 2023.
* [40] Andrea Montanari. Sampling, diffusions, and stochastic localization. _arXiv preprint arXiv:2305.10690_, 2023.
* [41] Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. _arXiv preprint arXiv:2304.11449_, 2023.
* [42] Nicholas M Boffi and Eric Vanden-Eijnden. Probability flow solution of the fokker-planck equation. _Machine Learning: Science and Technology_, 4(3):035012, 2023.

* [43] Yan Huang and Li Wang. A score-based particle method for homogeneous landau equation. _arXiv preprint arXiv:2405.05187_, 2024.
* [44] Lingxiao Li, Samuel Hurault, and Justin M Solomon. Self-consistent velocity matching of probability flows. _Advances in Neural Information Processing Systems_, 36, 2024.
* [45] Jianfeng Lu, Yue Wu, and Yang Xiang. Score-based transport modeling for mean-field fokker-planck equations. _Journal of Computational Physics_, 503:112859, 2024.
* [46] Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker-planck equations through gradient-log-density estimation. _Entropy_, 22(8):802, 2020.
* [47] Amira Alakhdar, Barnabas Poczos, and Newell Washburn. Diffusion models in de novo drug design. _Journal of Chemical Information and Modeling_, 2024.
* [48] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. _arXiv preprint arXiv:2402.04997_, 2024.
* [49] Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. _arXiv preprint arXiv:2402.05841_, 2024.
* [50] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Ksenia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In _International Conference on Machine Learning_, pages 1276-1301. PMLR, 2023.
* [51] Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex X Lu, Nicolo Fusi, Ava P Amini, and Kevin K Yang. Protein generation with evolutionary diffusion: sequence is all you need. _BioRxiv_, pages 2023-09, 2023.
* [52] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. _Nature_, 620(7976):1089-1100, 2023.
* [53] Jordan Cotler and Semon Rezchikov. Renormalizing diffusion models. _arXiv preprint arXiv:2308.12355_, 2023.
* [54] Florian Furrutter, Gorka Munoz-Gil, and Hans J Briegel. Quantum circuit synthesis with diffusion models. _arXiv preprint arXiv:2311.02041_, 2023.
* [55] Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, and Jianlin Cheng. Diffusion models in bioinformatics and computational biology. _Nature reviews bioengineering_, 2(2):136-154, 2024.
* [56] Artan Sheshmani, Yi-Zhuang You, Baturalp Buyukates, Amir Ziashahabi, and Salman Avestimehr. Renormalization group flow, optimal transport and diffusion-based generative model. _arXiv preprint arXiv:2402.17090_, 2024.
* [57] Luke Triplett and Jianfeng Lu. Diffusion methods for generating transition paths. _arXiv preprint arXiv:2309.10276_, 2023.
* [58] Lingxiao Wang, Gert Aarts, and Kai Zhou. Generative diffusion models for lattice field theory. _arXiv preprint arXiv:2311.03578_, 2023.
* [59] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodifff: A geometric diffusion model for molecular conformation generation. _arXiv preprint arXiv:2203.02923_, 2022.
* [60] Mengjiao Yang, KwangHwan Cho, Amil Merchant, Pieter Abbeel, Dale Schuurmans, Igor Mordatch, and Ekin Dogus Cubuk. Scalable diffusion for materials generation. _arXiv preprint arXiv:2311.09235_, 2023.

* [61] Jiahao Fan, Ziyao Li, Eric Alcaide, Guolin Ke, Huaqing Huang, and Weinan E. Accurate conformation sampling via protein structural diffusion. _Journal of Chemical Information and Modeling_, 2024.
* [62] Jason Yim, Hannes Stark, Gabriele Corso, Bowen Jing, Regina Barzilay, and Tommi S Jaakkola. Diffusion models in protein structure and docking. _Wiley Interdisciplinary Reviews: Computational Molecular Science_, 14(2):e1711, 2024.
* [63] Yuchen Zhu, Tianrong Chen, Evangelos A Theodorou, Xie Chen, and Molei Tao. Quantum state generation with structure-preserving diffusion model. _arXiv preprint arXiv:2404.06336_, 2024.
* [64] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. _ACM Computing Surveys_, 56(4):1-39, 2023.
* [65] Stanley H Chan. Tutorial on diffusion models for imaging and vision. _arXiv preprint arXiv:2403.18103_, 2024.
* [66] Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang. Opportunities and challenges of diffusion models for generative ai. _National Science Review_, page nwae348, 2024.
* [67] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* [68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* [69] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. _Advances in Neural Information Processing Systems_, 35:30150-30166, 2022.
* [70] Gen Li, Yu Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, and Yuxin Chen. Accelerating convergence of score-based diffusion models, provably. _arXiv preprint arXiv:2403.03852_, 2024.
* [71] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. _arXiv preprint arXiv:2202.09778_, 2022.
* [72] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* [73] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. _arXiv preprint arXiv:2211.01095_, 2022.
* [74] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. _Advances in Neural Information Processing Systems_, 36:55502-55542, 2023.
* [75] Zhenyu Zhou, Defang Chen, Can Wang, and Chun Chen. Fast ode-based sampling for diffusion models in around 5 steps. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7777-7786, 2024.
* [76] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* [77] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. _arXiv preprint arXiv:2204.13902_, 2022.
* [78] Saravanan Kandasamy and Dheeraj Nagaraj. The poisson midpoint method for langevin dynamics: Provably efficient discretization for diffusion models. _arXiv preprint arXiv:2405.17068_, 2024.

* [79] Yuchen Wu, Yuxin Chen, and Yuting Wei. Stochastic runge-kutta methods: Provable acceleration of diffusion models. _arXiv preprint arXiv:2410.04760_, 2024.
* [80] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. _arXiv preprint arXiv:2112.07068_, 2021.
* [81] Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. _arXiv preprint arXiv:2105.14080_, 2021.
* [82] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In _International Conference on Machine Learning_, pages 42390-42402. PMLR, 2023.
* [83] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. _Advances in Neural Information Processing Systems_, 36:76806-76838, 2023.
* [84] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. _arXiv preprint arXiv:2403.06807_, 2024.
* [85] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. _arXiv preprint arXiv:2303.01469_, 2023.
* [86] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. _arXiv preprint arXiv:2310.14189_, 2023.
* [87] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. _arXiv preprint arXiv:2410.11081_, 2024.
* [88] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. _arXiv preprint arXiv:2101.02388_, 2021.
* [89] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14297-14306, 2023.
* [90] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. _arXiv preprint arXiv:2202.00512_, 2022.
* [91] Hyungjin Chung, Jeongsol Kim, Sehui Kim, and Jong Chul Ye. Parallel diffusion models of operator and image for blind inverse problems. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6059-6069, 2023.
* [92] Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, and Tsung-Hui Chang. Accelerating parallel sampling of diffusion models. _arXiv preprint arXiv:2402.09970_, 2024.
* [93] Jiezhang Cao, Yue Shi, Kai Zhang, Yulun Zhang, Radu Timofte, and Luc Van Gool. Deep equilibrium diffusion restoration with parallel sampling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2824-2834, 2024.
* [94] Nikil Roashan Selvam, Amil Merchant, and Stefano Ermon. Self-refining diffusion samplers: Enabling parallelization via parareal iterations. _arXiv preprint arXiv:2412.08292_, 2024.
* [95] Shivam Gupta, Linda Cai, and Sitan Chen. Faster diffusion-based sampling with randomized midpoints: Sequential and parallel. _arXiv preprint arXiv:2406.00924_, 2024.
* [96] Gen Li and Yuchen Jiao. Improved convergence rate for diffusion probabilistic models. _arXiv preprint arXiv:2410.13738_, 2024.
* [97] Ruoqi Shen and Yin Tat Lee. The randomized midpoint method for log-concave sampling. _Advances in Neural Information Processing Systems_, 32, 2019.

* [98] Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In _Conference on Learning Theory_, pages 3084-3114. PMLR, 2019.
* [99] Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. _arXiv preprint arXiv:2002.00107_, 2020.
* [100] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. _arXiv preprint arXiv:2209.11215_, 2022.
* [101] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. _Advances in Neural Information Processing Systems_, 35:22870-22882, 2022.
* [102] Francesco Pedrotti, Jan Maas, and Marco Mondelli. Improved convergence of score-based diffusion models via prediction-correction. _arXiv preprint arXiv:2305.14164_, 2023.
* [103] Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. In _International Conference on Machine Learning_, pages 4462-4484. PMLR, 2023.
* [104] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In _International Conference on Machine Learning_, pages 4735-4763. PMLR, 2023.
* [105] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In _International Conference on Algorithmic Learning Theory_, pages 946-985. PMLR, 2023.
* [106] Sokhna Diarra Mbacke and Omar Rivasplata. A note on the convergence of denoising diffusion probabilistic models. _arXiv preprint arXiv:2312.05989_, 2023.
* [107] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. _arXiv preprint arXiv:2308.03686_, 2023.
* [108] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. _arXiv preprint arXiv:2306.09251_, 2023.
* [109] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for diffusion-based generative models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [110] Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. A sharp convergence theory for the probability flow odes of diffusion models. _arXiv preprint arXiv:2408.02320_, 2024.
* [111] Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ode is provably fast. _Advances in Neural Information Processing Systems_, 36, 2024.
* [112] Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in wasserstein distances. _arXiv preprint arXiv:2401.17958_, 2024.
* [113] Yuchen Liang, Peizhong Ju, Yingbin Liang, and Ness Shroff. Non-asymptotic convergence of discrete-time diffusion models: New approach and improved rate. _arXiv preprint arXiv:2402.13901_, 2024.
* [114] Daniel Zhengyu Huang, Jiaoyang Huang, and Zhengjiang Lin. Convergence analysis of probability flow ode for score-based generative models. _arXiv preprint arXiv:2404.09730_, 2024.
* [115] Gen Li and Yuling Yan. \(o(d/t)\) convergence theory for diffusion probabilistic models under minimal assumptions. _arXiv preprint arXiv:2409.18959_, 2024.

* [116] Brian DO Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* [117] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* [118] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [119] Yu Cao, Jingrun Chen, Yixin Luo, and Xiang Zhou. Exploring the optimal choice for generative processes in diffusion models: Ordinary vs stochastic differential equations. _Advances in Neural Information Processing Systems_, 36, 2024.
* [120] Teo Deveney, Jan Stanczuk, Lisa Maria Kreusser, Chris Budd, and Carola-Bibiane Schonlieb. Closing the ode-sde gap in score-based diffusion models through the fokker-planck equation. _arXiv preprint arXiv:2311.15996_, 2023.
* [121] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: Sde beats ode in general diffusion-based image editing. _arXiv preprint arXiv:2311.01410_, 2023.
* [122] Charles J Geyer. Markov chain monte carlo maximum likelihood. In _E. M. Kernamidas, editor, Computing Science and Statistics: Proc. 23rd Symposium on the Interface_, pages 156-163. Interface Foundation of North America, 1991.
* [123] Koji Hukushima and Koji Nemoto. Exchange monte carlo method and application to spin glass simulations. _Journal of the Physical Society of Japan_, 65(6):1604-1608, 1996.
* [124] Faming Liang. Use of sequential structure in simulation from high-dimensional systems. _Physical Review E_, 67(5):056101, 2003.
* [125] Nima Anari, Yizhi Huang, Tianyu Liu, Thuy-Duong Vuong, Brian Xu, and Katherine Yu. Parallel discrete sampling via continuous walks. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 103-116, 2023.
* [126] Holden Lee. Parallelising glauber dynamics. _arXiv preprint arXiv:2307.07131_, 2023.
* [127] Lu Yu and Arnak Dalalyana. Parallelized midpoint randomization for langevin monte carlo. _arXiv preprint arXiv:2402.14434_, 2024.
* [128] Ernest Lindelof. Sur laplication de la methode des approximations successives aux equations differentielles ordinaires du premier ordre. _Comptes rendus hebdomadaires des seances de lAcademie des sciences_, 116(3):454-457, 1894.
* [129] Emile Picard. Sur les methodes d'approximations successives dans la theorie des equations differentielles. _American Journal of Mathematics_, pages 87-100, 1898.
* [130] Nima Anari, Sinho Chewi, and Thuy-Duong Vuong. Fast parallel sampling under isoperimetry. _arXiv preprint arXiv:2401.09016_, 2024.
* [131] Bernt Oksendal. _Stochastic differential equations: an introduction with applications_. Springer Science & Business Media, 2013.
* [132] Emiel Hoogeboom, Alexey A Gritsenko, Jasmin Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. _arXiv preprint arXiv:2110.02037_, 2021.
* [133] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_, 34:12454-12465, 2021.
* [134] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. _Advances in Neural Information Processing Systems_, 35:34532-34545, 2022.

* [135] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. _arXiv preprint arXiv:2211.16750_, 2022.
* [136] Pierre H Richemond, Sander Dieleman, and Arnaud Doucet. Categorical sdes with simplex diffusion. _arXiv preprint arXiv:2210.14784_, 2022.
* [137] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. _arXiv preprint arXiv:2310.16834_, 2023.
* [138] Griffin Floto, Thorsteinn Jonsson, Mihai Nica, Scott Sanner, and Eric Zhengyu Zhu. Diffusion on the probability simplex. _arXiv preprint arXiv:2309.02530_, 2023.
* [139] Javier E Santos, Zachary R Fox, Nicholas Lubbers, and Yen Ting Lin. Blackout diffusion: generative diffusion models in discrete-state spaces. In _International Conference on Machine Learning_, pages 9034-9059. PMLR, 2023.
* [140] Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. From denoising diffusions to denoising markov models. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 86(2):286-301, 2024.
* [141] Hongrui Chen and Lexing Ying. Convergence analysis of discrete diffusion model: Exact implementation through uniformization. _arXiv preprint arXiv:2402.08095_, 2024.
* [142] Yinuo Ren, Haoxuan Chen, Grant M Rotskoff, and Lexing Ying. How discrete and continuous diffusion meet: Comprehensive analysis of discrete diffusion models via a stochastic integral framework. _arXiv preprint arXiv:2410.03601_, 2024.
* [143] Michael S Albergo, Nicholas M Boffi, Michael Lindsey, and Eric Vanden-Eijnden. Multi-marginal generative modeling with stochastic interpolants. _arXiv preprint arXiv:2310.03695_, 2023.
* [144] Jean-Francois Le Gall. _Brownian motion, martingales, and stochastic calculus_. Springer, 2016.
* [145] Arnaud Guillin and Feng-Yu Wang. Degenerate fokker-planck equations: Bismut formula, gradient estimate and harnack inequality. _Journal of Differential Equations_, 253(1):20-40, 2012.

Mathematical Background

In this section, we will summarize used notations and rigorous mathematical framework of Ito processes as necessary in the proofs. We will also present several technical lemmas for later reference.

### Notations

We adopt the following notations throughout the paper:

### Preliminaries

**Theorem A.1** (Properties of \(f\)-divergence).: _Suppose \(p\) and \(q\) are two probability measures on a common measurable space \((\Omega,\mathcal{F})\) with \(p\ll q\). The \(f\)-divergence between \(p\) and \(q\) is defined as_

\[D_{f}(p\|q)=\mathbb{E}_{X\sim q}\left[f\left(\frac{\mathrm{d}p}{\mathrm{d}q} \right)\right],\] (A.1)

_where \(\frac{\mathrm{d}p}{\mathrm{d}q}\) is the Radon-Nikodym derivative of \(p\) with respect to \(q\), and \(f:\mathbb{R}^{+}\to\mathbb{R}\) is a convex function. In particular, \(D_{f}(\cdot\,\|\,\cdot)\) coincides with the Kullback-Leibler (KL) divergence when \(f(x)=x\log x\) and \(D_{f}(\cdot\,\|\,\cdot)=\mathrm{TV}\) coincides with the total variation (TV) distance when \(f(x)=\frac{1}{2}|x-1|\)._

_For the \(f\)-divergence defined above, we have the following properties:_

1. _(Data-processing inequality). Suppose_ \(\mathcal{H}\) _is a sub-_\(\sigma\)_-algebra of_ \(\mathcal{F}\)_, the following inequality holds_ \[D_{f}\left(p|_{\mathcal{H}}\parallel q|_{\mathcal{H}}\right)\leq D_{f}(p\,\|\,q);\] _for any_ \(f\)_-divergence_ \(D_{f}(\cdot\|\,\cdot)\)_._
2. _(Chain rule). Suppose_ \(X\) _is a random variable generating a sub-_\(\sigma\)_-algebra_ \(\mathcal{F}_{X}\) _of_ \(\mathcal{F}\)_, and_ \(p(\cdot|X)\ll q(\cdot|X)\) _holds for any value of_ \(X\)_, then_ \[D_{\mathrm{KL}}(p\|q)=D_{\mathrm{KL}}(p|_{\mathcal{F}_{X}}\|q|_{\mathcal{F}_{X} })+\mathbb{E}_{\mathcal{F}_{X}}[D_{\mathrm{KL}}(p(\cdot|X)\|q(\cdot|X))].\]

\begin{table}
\begin{tabular}{c|l} \hline \hline
**Notation** & **Description** \\ \hline \([a:b]\) & The set \(\{a,a+1,\ldots,b\}\) \\ \(\bm{I}_{d}\) & Identity matrix in \(\mathbb{R}^{d\times d}\) \\ \(\overset{\ast}{\ast}_{t}\) & \(\ast_{T-t}\) \\ \(\overset{\ast}{\ast}\) & Used to denote quantities produced by the algorithm \\ \(\overset{\ast}{\ast}\) & Used to denote quantities related to the auxiliary processes \\ \(\ast^{\dagger}\) & Used to denote quantities related to the corrector step \\ \(\|\cdot\|\) & The Euclidean norm of a vector \\ \(\lesssim\bm{\mathrm{or}}\gtrsim\) & The inequality holds up to a constant factor \\ \(\ll\) & Absolute continuity (for measures)/ much less than (for quantities) \\ \((\bm{x}_{t})_{t\geq 0}\) & The forward process of the diffusion model (2.3) \\ \((\bm{\tilde{x}}_{t})_{t\in[0,T]}\) & The backward process of the diffusion model (2.3) \\ \((\bm{y}_{t})_{t\in[0,T]}\) & The approximate backward process of the diffusion model (2.5) \\ \(\widehat{\bm{y}}^{(k)}_{t_{n},\tau_{n,m}}\) & The approximate value of the approximate process \(\bm{y}_{t}\) at time \(t_{n}+\tau_{n,m}\) \\  & after \(k\) iterations in the \((n+1)\)-th block \\ \(\widehat{\bm{y}}_{t_{n}}\) & The value of the approximate process \(\bm{y}_{t}\) at time \(t_{n}\) \\ \(\widehat{q}_{t_{n}}\) & The distribution of \(\widehat{\bm{y}}_{t_{n}}\) \\ \(\bm{z}_{t_{1}:t_{2}}\) & The path \((\bm{z}_{t})_{t\in[t_{1},t_{2}]}\) of the process \(\bm{z}_{t}\) \\ \(D_{f}(\cdot\,\|\,\cdot)\) & The \(f\)-divergence between two distributions \\ \(D_{\mathrm{KL}}(\cdot\,\|\,\cdot)\) & The KL divergence between two distributions \\ \(TV(\cdot\,,\,\cdot)\) & The total variation distance between two distributions \\ \(W_{2}(\cdot,\cdot)\) & The 2-Wasserstein distance between two distributions \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of notationsIn this paper, we consider a probability space \((\Omega,\mathcal{F},p)\) on which \((\bm{w}_{t}(\omega))_{t\geq 0}\) is a Wiener process in \(\mathbb{R}^{d}\). The Wiener process \((\bm{w}_{t}(\omega))_{t\geq 0}\) generates the filtration \(\{\mathcal{F}_{t}\}_{t\geq 0}\) on the measurable space \((\Omega,\mathcal{F})\). For an Ito process \(\bm{z}_{t}(\omega)\) with the following governing SDE:

\[\mathrm{d}\bm{z}_{t}(\omega)=\bm{\alpha}(t,\omega)\mathrm{d}t+\bm{\Sigma}(t, \omega)\mathrm{d}\bm{w}_{t}(\omega),\]

for any time \(t\), we denote the marginal distribution of \(\bm{z}_{t}\) by \(p_{t}\), _i.e._

\[p_{t}:=p\left(\bm{z}_{t}^{-1}(\cdot)\right),\;\text{where}\;\bm{z}_{t}:\Omega \rightarrow\mathbb{R}^{m},\;\omega\mapsto\bm{z}_{t}(\omega),\]

as well as the _path measure_ of the process \(\bm{z}_{t}\) in the sense of

\[p_{t_{1}:t_{2}}:=p\left(\bm{z}_{t_{1}:t_{2}}^{-1}(\cdot)\right),\;\text{where} \;\bm{z}_{t_{1}:t_{2}}:\Omega\rightarrow\mathcal{C}([t_{1},t_{2}],\mathbb{R}^ {m}),\;\omega\mapsto(\bm{z}_{t}(\omega))_{t\in[t_{1},t_{2}]}.\]

For the sake of simplicity, we define the following class of functions:

**Definition A.2**.: _For any \(0\leq t_{1}<t_{2}\), we define \(\mathcal{V}(t_{1},t_{2})\) as the class of functions \(f(t,\omega):[0,+\infty)\times\Omega\rightarrow\mathbb{R}\) such that_

1. \(f(t,\omega)\) _is_ \(\mathcal{B}\times\mathcal{F}\)_-measurable, where_ \(\mathcal{B}\) _is the Borel_ \(\sigma\)_-algebra on_ \(\mathbb{R}^{d}\)_;_
2. \(f(t,\omega)\) _is_ \(\mathcal{F}_{t}\)_-adapted for all_ \(t\geq 0\)_;_
3. _The following_ Novikov condition _holds_ \[\mathbb{E}\left[\exp\int_{t_{1}}^{t_{2}}f^{2}(t,\omega)\mathrm{d}t\right]<+\infty,\]

_and \(\mathcal{V}=\cap_{t>0}\mathcal{V}(0,t)\). For vectors and matrices, we say it belongs to \(\mathcal{V}^{n}(t,\omega)\) or \(\mathcal{V}^{m\times n}(t,\omega)\) if each component of the vector or each entry of the matrix belongs to \(\mathcal{V}(t,\omega)\)._

**Remark A.3**.: _Novikov's condition appeared in the third requirement is often relaxed to the squared integrability condition in the general definition of Ito processes, which requires_

\[\mathbb{E}\left[\int_{t_{1}}^{t_{2}}f^{2}(t,\omega)\mathrm{d}t\right]<+\infty.\]

_Here, we adopt the more restricted condition in the spirit of its necessity for Girsanov's theorem to hold, as we shall see later._

Similar to previous work [111], here we can avoid checking Novikov's condition throughout our proofs below by using the approximation argument presented in [100]. A review of Girsanov can be found in textbooks like in [131, 144]. We will present the following generalized version of Girsanov's theorem:

**Theorem A.4** (Girsanov's Theorem [131, Theorem 8.6.6]).: _Let \(\bm{\alpha}(t,\omega)\in\mathcal{V}^{m}\), \(\bm{\Sigma}(t,\omega)\in\mathcal{V}^{m\times n}\), and \((\bm{w}_{t}(\omega))_{t\geq 0}\) be a Wiener process on the probability space \((\Omega,\mathcal{F},q)\). For \(t\in[0,T]\), suppose \(\bm{z}_{t}(\omega)\) is an Ito process with the following SDE:_

\[\mathrm{d}\bm{z}_{t}(\omega)=\bm{\alpha}(t,\omega)\mathrm{d}t+\bm{\Sigma}(t, \omega)\mathrm{d}\bm{w}_{t}(\omega),\] (A.2)

_and there exist processes \(\bm{\delta}(t,\omega)\in\mathcal{V}^{n}\) and \(\bm{\beta}(t,\omega)\in\mathcal{V}^{m}\) such that_

1. \(\bm{\Sigma}(t,\omega)\bm{\delta}(t,\omega)=\bm{\alpha}(t,\omega)-\bm{\beta}(t,\omega)\)_;_
2. _The process_ \(M_{t}(\omega)\) _as defined below is a martingale with respect to the filtration_ \(\{\mathcal{F}_{t}\}_{t\geq 0}\) _and probability measure_ \(q\)_:_ \[M_{t}(\omega)=\exp\left(-\int_{0}^{t}\bm{\delta}(s,\omega)^{\top}\mathrm{d}\bm {w}_{s}(\omega)-\frac{1}{2}\int_{0}^{t}\|\bm{\delta}(s,\omega)\|^{2}\mathrm{d}s \right),\]

_then there exists another probability measure \(p\) on \((\Omega,\mathcal{F})\) such that_

1. \(p\ll q\) _with the Radon-Nikodym derivative_ \(\frac{\mathrm{d}p}{\mathrm{d}q}(\omega)=M_{T}(\omega)\)_,_2. _The process_ \(\widetilde{\bm{w}}_{t}(\omega)\) _as defined below is a Wiener process on_ \((\Omega,\mathcal{F},p)\)_:_ \[\widetilde{\bm{w}}_{t}(\omega)=\bm{w}_{t}(\omega)+\int_{0}^{t}\bm{\delta}(s, \omega)\mathrm{d}s,\]
3. _Any continuous path in_ \(\mathcal{C}([t_{1},t_{2}],\mathbb{R}^{m})\) _generated by the process_ \(\bm{z}_{t}\) _satisfies the following SDE under the probability measure_ \(p\)_:_ \[\mathrm{d}\widetilde{\bm{z}}_{t}(\omega)=\bm{\beta}(t,\omega)\mathrm{d}t+\bm{ \Sigma}(t,\omega)\mathrm{d}\widetilde{\bm{w}}_{t}(\omega).\] (A.3)

**Corollary A.5**.: _Suppose the conditions in Theorem A.4 hold, then for any \(t_{1},t_{2}\in[0,T]\) with \(t_{1}<t_{2}\), the path measure of the SDE (A.3) under the probability measure \(p\) in the sense of \(p_{t_{1}:t_{2}}=p\left(\bm{z}_{t_{1}:t_{2}}^{-1}(\cdot)\right)\) is absolutely continuous with respect to the path measure of the SDE (A.2) in the sense of \(q_{t_{1}:t_{2}}=q\left(\bm{z}_{t_{1}:t_{2}}^{-1}(\cdot)\right)\). Moreover, the KL divergence between the two path measures is given by_

\[D_{\mathrm{KL}}(p_{t_{1}:t_{2}}\|q_{t_{1}:t_{2}})=D_{\mathrm{KL}}(p_{t_{1}}\|q _{t_{1}})+\mathbb{E}_{\omega\sim p|_{\mathcal{F}_{t_{1}}}}\left[\frac{1}{2} \int_{t_{1}}^{t_{2}}\|\bm{\delta}(t,\omega)\|^{2}\mathrm{d}t\right]\] (A.4)

Proof.: First, by Theorem A.1, we have

\[D_{\mathrm{KL}}(p_{t_{1}:t_{2}}\|q_{t_{1}:t_{2}})=D_{\mathrm{KL}}(p|_{ \mathcal{F}_{t_{1}}}\|q|_{\mathcal{F}_{t_{1}}})+\mathbb{E}_{\bm{z}\sim p|_{ \mathcal{F}_{t_{1}}}}\left[D_{\mathrm{KL}}\big{(}p(\widetilde{\bm{z}}_{t_{1}: t_{2}}^{-1}(\cdot))|\widetilde{\bm{z}}_{t_{1}}=\widetilde{\bm{z}})\|q( \widetilde{\bm{z}}_{t_{1}:t_{2}}^{-1}(\cdot))|\widetilde{\bm{z}}_{t_{1}}= \widetilde{\bm{z}})\big{)}\right].\]

From Girsanov's theorem (Theorem A.4), we have that the measure \(p|_{\mathcal{F}_{t_{1}}}\) is absolutely continuous with respect to \(q|_{\mathcal{F}_{t_{1}}}\), which allows us to compute the second term above as follows:

\[D_{\mathrm{KL}}\big{(}p(\widetilde{\bm{z}}_{t_{1}:t_{2}}^{-1}( \cdot)|\widetilde{\bm{z}}_{t_{1}}=\widetilde{\bm{z}})\|q(\widetilde{\bm{z}}_{ t_{1}:t_{2}}^{-1}(\cdot)|\widetilde{\bm{z}}_{t_{1}}=\widetilde{\bm{z}})\big{)}\] \[=\mathbb{E}_{\widetilde{\bm{z}}_{t_{1}:t_{2}}}\left[\log\frac{ \mathrm{d}p(\widetilde{\bm{z}}_{t_{1}:t_{2}}^{-1}(\cdot)|\widetilde{\bm{z}}_{t _{1}}=\bm{z})}{\mathrm{d}q(\widetilde{\bm{z}}_{t_{1}:t_{2}}^{-1}(\cdot)| \widetilde{\bm{z}}_{t_{1}}=\bm{z})}\right]=\mathbb{E}_{\omega\sim p|_{ \mathcal{F}_{t_{1}}}}\left[\log\frac{\mathrm{d}p|_{\mathcal{F}_{t_{1}}}}{ \mathrm{d}q|_{\mathcal{F}_{t_{1}}}}\right]\] \[=\mathbb{E}_{\omega\sim p|_{\mathcal{F}_{t_{1}}}}\left[-\int_{t_ {1}}^{t_{2}}\bm{\delta}(t,\omega)^{\top}\mathrm{d}\bm{w}_{t}(\omega)-\frac{1}{ 2}\int_{t_{1}}^{t_{2}}\|\bm{\delta}(t,\omega)\|^{2}\mathrm{d}t\right]\] \[=\mathbb{E}_{\omega\sim p|_{\mathcal{F}_{t_{1}}}}\left[\frac{1}{2} \int_{t_{1}}^{t_{2}}\|\bm{\delta}(t,\omega)\|^{2}\mathrm{d}t\right],\]

and therefore

\[D_{\mathrm{KL}}(p_{t_{1}:t_{2}}\|q_{t_{1}:t_{2}})=D_{\mathrm{KL}}(p_{t_{1}}\|q _{t_{1}})+\mathbb{E}_{\omega\sim p|_{\mathcal{F}_{t_{1}}}}\left[\frac{1}{2} \int_{t_{1}}^{t_{2}}\|\bm{\delta}(t,\omega)\|^{2}\mathrm{d}t\right],\]

which completes the proof. 

### Helper Lemmas

**Lemma A.6** ([107, Lemma 2]).: _For the backward process (2.3), we have for \(0\leq s<t<T\),_

\[\frac{\mathrm{d}}{\mathrm{d}t}\left(\mathbb{E}\left[\|\nabla\log\widetilde{p}_ {t}(\widetilde{\bm{x}}_{t})-\nabla\log\widetilde{p}_{s}(\widetilde{\bm{x}}_{ s})\|^{2}\right]\right)\leq\frac{1}{2}\mathbb{E}\left[\|\nabla\log \widetilde{p}_{s}(\widetilde{\bm{x}}_{s})\|^{2}\right]+\mathbb{E}\left[\| \nabla^{2}\log\widetilde{p}_{t}(\widetilde{\bm{x}}_{t})\|_{F}^{2}\right].\]

**Lemma A.7** ([107, Lemma 3]).: _For the forward process (2.3), we have for \(0\leq t<T\),_

\[\mathbb{E}\left[\nabla\log p_{t}(\bm{x}_{t})\right]\leq d\sigma_{t}^{-2},\text{ and }\mathbb{E}\left[\|\nabla^{2}\log p_{t}(\bm{x}_{t})\|_{F}^{2}\right]\leq d\sigma_{t}^{-4}+2 \frac{\mathrm{d}}{\mathrm{d}t}\left(\sigma_{t}^{-4}\mathbb{E}\left[\operatorname {tr}\bm{\Sigma}_{t}\right]\right),\]

_where the posterior covariance matrix \(\bm{\Sigma}_{t}:=\mathrm{cov}_{p_{0|t}}(\bm{x}_{0})\) and \(\sigma_{t}^{2}=1-e^{-t}\). Moreover, the posterior covariance matrix \(\bm{\Sigma}_{t}\) satisfies_

\[\mathbb{E}\left[\operatorname{tr}\bm{\Sigma}_{t}\right]\lesssim d\wedge d\sigma_{t} ^{2}.\]

**Lemma A.8**.: _For any \(n\in[0:N-1]\) and \(\tau\in[0,h_{n}]\), under the assumption \(\mathrm{cov}_{p_{0}}(\bm{x}_{0})=\bm{I}_{d}\), we have_

\[\mathbb{E}\left[\left\|\bm{\tilde{x}}_{t_{n}}\right\|^{2}\right]\leq 2d,\] (A.5)

_and_

\[\mathbb{E}\left[\left\|\bm{\tilde{x}}_{t_{n}}-\bm{\tilde{x}}_{t_{n}+\tau} \right\|^{2}\right]\leq 3d.\] (A.6)

Proof.: Conditioned on \(\bm{x}_{0}\), we have that

\[\bm{\tilde{x}}_{t_{n}}=\bm{x}_{T-t_{n}}\sim\mathcal{N}\left(e^{-\frac{1}{2}(T -t_{n})}\bm{x}_{0},(1-e^{-(T-t_{n})})\bm{I}_{d}\right),\]

and

\[\bm{\tilde{x}}_{t_{n}+\tau}=\bm{x}_{T-t_{n}-\tau}\sim\mathcal{N}\left(e^{- \frac{1}{2}(T-t_{n}-\tau)}\bm{x}_{0},(1-e^{-(T-t_{n}-\tau)})\bm{I}_{d}\right)\]

for any \(\tau\in[0,h_{n}]\). Therefore, we have

\[\mathbb{E}\left[\left\|\bm{\tilde{x}}_{t_{n}}\right\|^{2}\right] =\mathbb{E}\left[\mathbb{E}\left[\left\|\bm{x}_{T-t_{n}}\right\| ^{2}\middle|\bm{x}_{0}\right]\right]\] \[\leq\mathbb{E}\left[\mathbb{E}\left[\left\|\bm{x}_{T-t_{n}}-e^{- \frac{1}{2}(T-t_{n})}\bm{x}_{0}\right\|^{2}\middle|\bm{x}_{0}\right]+\left\|e^ {-\frac{1}{2}(T-t_{n})}\bm{x}_{0}\right\|^{2}\right]\] \[\leq d(1-e^{-(T-t_{n})})+e^{-(T-t_{n})}\mathbb{E}[\left\|\bm{x}_{ 0}\right\|^{2}]\leq 2d.\]

Taking the difference between them then implies that for any \(\tau\in[0,h_{n}]\),

\[\mathbb{E}\left[\left\|\bm{\tilde{x}}_{t_{n}}-\bm{\tilde{x}}_{t_{n }+\tau}\right\|^{2}\right] =\mathbb{E}\left[\mathbb{E}\left[\left\|\bm{x}_{T-t_{n}}-\bm{x}_ {T-t_{n}-\tau}\right\|^{2}\middle|\bm{x}_{0}\right]\right]\] \[\leq d(2-e^{-(T-t_{n})}-e^{-(T-t_{n}-\tau)})\] \[+\left(e^{-\frac{1}{2}(T-t_{n})}-e^{-\frac{1}{2}(T-t_{n}-\tau)} \right)^{2}\mathbb{E}[\left\|\bm{x}_{0}\right\|^{2}]\] \[\leq 2d+e^{-(T-t_{n}-\tau)}(1-e^{-\frac{1}{2}\tau})^{2}\mathbb{E} [\left\|\bm{x}_{0}\right\|^{2}]\leq 3d.\]

**Lemma A.9** (Lemma 9 in [104]).: _For \(\widehat{q}_{0}\sim\mathcal{N}(0,\bm{I}_{d})\) and \(\vec{p}_{0}=p_{T}\) is the distribution of the solution to the forward process (2.3), we have_

\[\mathrm{TV}(\vec{p}_{0},\widehat{q}_{0})^{2}\leq D_{\mathrm{KL}}(\vec{p}_{0} \|\widehat{q}_{0})\lesssim de^{-T}.\]

## Appendix B Details of SDE Implementation

In this section, we will present the missing proofs for Theorem 3.3. For readers' convenience, we reiterate the backward process (2.3)

\[\mathrm{d}\bm{\tilde{x}}_{t}=\left[\frac{1}{2}\bm{\tilde{x}}_{t}+\nabla\log \vec{p}_{t}(\bm{\tilde{x}}_{t})\right]\mathrm{d}t+\mathrm{d}\bm{w}_{t},\quad \text{with}\quad\bm{\tilde{x}}_{0}\sim p_{T},\] (B.1)

and its approximate version (2.5) with the learned score function

\[\mathrm{d}\bm{y}_{t}=\left[\frac{1}{2}\bm{y}_{t}+\bm{s}_{t}^{\theta}(\bm{y}_{ t})\right]\mathrm{d}t+\mathrm{d}\bm{w}_{t},\quad\text{with}\quad\bm{y}_{0}\sim \mathcal{N}(0,\bm{I}_{d}).\]

The filtration \(\mathcal{F}_{t}\) refers to the filtration of the SDE (B.1) up to time \(t\).

### Auxiliary Process

We would like first to consider the errors that Algorithm 1 may cause within one block of update. To this end, we consider the following auxiliary process for \(\tau\in[0,h_{n}]\) conditioned on the filtration \(\mathcal{F}_{t_{n}}\) at time \(t_{n}\):

**Definition B.1** (Auxiliary Process).: _For any \(n\in[0:N-1]\), we define the auxiliary process \((\widehat{\boldsymbol{y}}^{(k)}_{t_{n},\tau})_{\tau\in[0,h_{n}]}\) as the solution to the following SDE recursively for \(k\in[0:K-1]\):_

\[\mathrm{d}\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}(\omega)=\bigg{[}\frac{ 1}{2}\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}(\omega)+\boldsymbol{s}^{ \theta}_{t_{n}+g_{n}(\tau)}\Big{(}\widehat{\boldsymbol{y}}^{(k)}_{t_{n},g_{n}( \tau)}(\omega)\Big{)}\bigg{]}\mathrm{d}\tau+\mathrm{d}\boldsymbol{w}_{t_{n}+ \tau}(\omega),\] (B.2)

_with the initial condition_

\[\widehat{\boldsymbol{y}}^{(0)}_{t_{n},\tau}(\omega)\equiv\widehat{\boldsymbol {y}}_{t_{n}}(\omega)\text{ for }\tau\in[0,h_{n}],\quad\text{and}\quad\widehat{ \boldsymbol{y}}^{(k)}_{t_{n},0}(\omega)\equiv\widehat{\boldsymbol{y}}_{t_{n}} (\omega)\text{ for }k\in[1:K]\] (B.3)

_where \(\widehat{\boldsymbol{y}}_{t_{n}}(\omega)=\widehat{\boldsymbol{y}}^{(K)}_{t_{n- 1},\tau_{n-1},M_{n-1}}(\omega)\) if \(n\in[1:N-1]\) and \(\widehat{\boldsymbol{y}}_{t_{0}}(\omega)\sim\mathcal{N}(0,\boldsymbol{I}_{d})\)._

The iteration should be perceived as a deterministic procedure to each event \(\omega\in\Omega\), _i.e._ each realization of the Wiener process \((\boldsymbol{w}_{t})_{t\geq 0}\). The following lemma clarifies this fact and proves the well-definedness and parallelability of the iteration in (B.2).

**Lemma B.2**.: _The auxiliary process \((\widehat{\boldsymbol{y}}^{(k)}_{t_{n},\tau}(\omega))_{\tau\in[0,h_{n}]}\) is \(\mathcal{F}_{t_{n}+\tau}\)-adapted for any \(k\in[0:K]\) and \(n\in[0:N-1]\)._

Proof.: Since the initialization \(\widehat{\boldsymbol{y}}^{(0)}_{t_{n},\tau}(\omega)\equiv\widehat{\boldsymbol {y}}_{t_{n}}(\omega)\) for \(\tau\in[0,h_{n}]\), where \(\widehat{\boldsymbol{y}}_{t_{n}}(\omega)\) is \(\mathcal{F}_{t_{n}}\)-adapted, it is obvious that \(\widehat{\boldsymbol{y}}^{(0)}_{t_{n},\tau}(\omega)\) is \(\mathcal{F}_{t_{n}+\tau}\)-adapted. Now suppose that \((\widehat{\boldsymbol{y}}^{(k)}_{t_{n},\tau}(\omega))_{\tau\in[0,h_{n}]}\) is \(\mathcal{F}_{t_{n}+\tau}\)-adapted, since \(g_{n}(\tau)\leq\tau\), we have the following Ito integral well-defined and \(\mathcal{F}_{t_{n}+\tau}\)-adapted:

\[\int_{0}^{\tau}\boldsymbol{s}^{\theta}_{t_{n}+g_{n}(\tau^{\prime})}\Big{(} \widehat{\boldsymbol{y}}^{(k)}_{t_{n},g_{n}(\tau^{\prime})}(\omega)\Big{)} \mathrm{d}\tau^{\prime},\]

and therefore (B.2) has a unique strong solution \((\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}(\omega))_{\tau\in[0,h_{n}]}\) that is also \(\mathcal{F}_{t_{n}+\tau}\)-adapted. The lemma follows by induction. 

**Lemma B.3** (Equivalence between (3.4) and (B.2)).: _For any \(n\in[0:N-1]\), the update rule (3.4) in Algorithm 1 is equivalent to the exact solution of the auxiliary process (B.2) for any \(k\in[0:K-1]\) and \(\tau\in[0,h_{n}]\)._

Proof.: The dependency on \(\omega\) will be omitted in the proof below.

Rewriting (B.2) and multiplying \(e^{-\frac{\tau}{2}}\) on both sides yield

\[\mathrm{d}\left[e^{-\frac{\tau}{2}}\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n}, \tau}\right] =e^{-\frac{\tau}{2}}\left[\mathrm{d}\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}-\frac{1}{2}\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}\mathrm{d} \tau\right]=e^{-\frac{\tau}{2}}\left[\boldsymbol{s}^{\theta}_{t_{n}+g_{n}(\tau )}\Big{(}\widehat{\boldsymbol{y}}^{(k)}_{t_{n},g_{n}(\tau)}\Big{)}\mathrm{d} \tau+\mathrm{d}\boldsymbol{w}_{t_{n}+\tau}\right].\]

Integrating on both sides from \(0\) to \(\tau\) implies

\[e^{-\frac{\tau}{2}}\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau }-\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},0} =\int_{0}^{\tau}e^{-\frac{\tau^{\prime}}{2}}\Bigg{(}\boldsymbol{s} ^{\theta}_{t_{n}+g_{n}(\tau^{\prime})}\Big{(}\widehat{\boldsymbol{y}}^{(k)}_{t _{n},g_{n}(\tau)}\Big{)}\mathrm{d}\tau^{\prime}+\mathrm{d}\boldsymbol{w}_{t_{n} +\tau^{\prime}}\Bigg{)}\] \[=\sum_{m=0}^{M_{n}}\int_{\tau\wedge t_{n,m}}^{\tau\wedge\tau_{n,m+1} }e^{-\frac{\tau^{\prime}}{2}}\boldsymbol{s}^{\theta}_{t_{n}+\tau_{n,m}}\Big{(} \widehat{\boldsymbol{y}}^{(k)}_{t_{n},\tau_{n,m}}\Big{)}\mathrm{d}\tau^{\prime} +\int_{0}^{\tau}e^{-\frac{\tau^{\prime}}{2}}\mathrm{d}\boldsymbol{w}_{t_{n}+ \tau^{\prime}}\] \[=\sum_{m=0}^{M_{n}}2\left(e^{-\frac{\tau\wedge\tau_{n,m}}{2}}-e^{- \frac{\tau\wedge\tau_{n,m+1}}{2}}\right)\boldsymbol{s}^{\theta}_{t_{n}+\tau_{n,j}}\Big{(}\widehat{\boldsymbol{y}}^{(k)}_{t_{n},\tau_{n,m}}\Big{)}+\int_{0}^{ \tau}e^{-\frac{\tau^{\prime}}{2}}\mathrm{d}\boldsymbol{w}_{t_{n}+\tau^{\prime}},\]

and then multiplying \(e^{\frac{\tau}{2}}\) on both sides above yields

\[\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau} =e^{\frac{\tau}{2}}\widehat{\boldsymbol{y}}^{(k+1)}_{t_{n},0}+\sum _{m=0}^{M_{n}}2\left(e^{\frac{\tau\wedge\tau_{n,m+1}-\tau\wedge\tau_{n,m}}{2}}-1 \right)e^{\frac{\alpha\vee(\tau-\tau_{n,m+1})}{2}}\boldsymbol{s}^{\theta}_{t_{n} +\tau_{n,m}}\Big{(}\widehat{\boldsymbol{y}}^{(k)}_{t_{n},\tau_{n,m}}\Big{)}\] \[+\sum_{m=0}^{M_{n}}\int_{\tau\wedge\tau_{n,m}}^{\tau\wedge\tau_{n,m+ 1}}e^{\frac{\tau-\tau^{\prime}}{2}}\mathrm{d}\boldsymbol{w}_{t_{n}+\tau^{ \prime}},\]where, by Ito isometry, we have

\[\int_{\tau\wedge\tau_{n,m}}^{\tau\wedge\tau_{n,m+1}}e^{\frac{\tau-\tau^{\prime}}{ 2}}\mathrm{d}\bm{w}_{t_{n}+\tau^{\prime}}\sim\mathcal{N}\left(\bm{0},\left(e^{ \tau\wedge\tau_{n,m+1}-\tau\wedge\tau_{n,m}}-1\right)e^{0\vee(\tau-\tau_{n,m+1 })}\bm{I}_{d}\right)\]

for \(\tau>\tau_{n,m}\) and equals to \(\bm{0}\) otherwise. Plugging in \(\tau=\tau_{j,m}\) gives us (3.4), as desired. 

### Errors within Block

We shall invoke Girsanov's theorem (Theorem A.4) in the procedure as detailed below:

1. Setting (A.2) in Theorem A.4 as the auxiliary process (B.2) at iteration \(K\), where \(\bm{w}_{t}(\omega)\) is a Wiener process under the measure \(q|_{\mathcal{F}_{t_{n}}}\);
2. Defining another process \(\widetilde{\bm{w}}_{t_{n}+\tau}(\omega)\) governed by the following SDE: \[\mathrm{d}\widetilde{\bm{w}}_{t_{n}+\tau}(\omega)=\mathrm{d}\bm{w}_{t_{n}+\tau }(\omega)+\bm{\delta}_{t_{n}}(\tau,\omega)d\tau,\] where \[\bm{\delta}_{t_{n}}(\tau,\omega):=\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta}(\widehat {\bm{y}}_{t_{n},g_{n}(\tau)}^{(K-1)}(\omega))-\nabla\log\overline{p}_{t_{n}+ \tau}(\widehat{\bm{y}}_{t_{n}+\tau}^{(K)}(\omega)),\] (B.4) and computing the Radon-Nikodym derivative of the measure \(\overline{p}|_{\mathcal{F}_{t_{n}}}\) with respect to \(q|_{\mathcal{F}_{t_{n}}}\) as \[\frac{\mathrm{d}\overline{p}|_{\mathcal{F}_{t_{n}}}}{\mathrm{d}q|_{\mathcal{F }_{t_{n}}}}(\omega):=\exp\left(-\int_{0}^{h_{n}}\bm{\delta}_{t_{n}}(\tau, \omega)^{\top}\mathrm{d}\bm{w}_{t_{n}+\tau}(\omega)-\frac{1}{2}\int_{0}^{h_{n} }\|\bm{\delta}_{t_{n}}(\tau,\omega)\|^{2}\mathrm{d}\tau\right),\]
3. Concluding that (B.2) at iteration \(K\) under the measure \(q|_{\mathcal{F}_{t_{n}}}\) satisfies the following SDE: \[\mathrm{d}\widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)=\left[\frac{1}{2} \widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)+\nabla\log\overline{p}_{t_{n}+ \tau}\big{(}\widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)\big{)}\right]\mathrm{d }\tau+\mathrm{d}\widetilde{\bm{w}}_{t_{n}+\tau}(\omega),\] with \((\widetilde{\bm{w}}_{t_{n}+\tau})_{\tau\geq 0}\) being a Wiener process under the measure \(\overline{p}|_{\mathcal{F}_{t_{n}}}\). If we replace \(\widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)\) by \(\widetilde{\bm{x}}_{t_{n}+\tau}(\omega)\), one should notice (B.5) is immediately the original backward SDE (2.3) with the true score function on \(t\in[t_{n},t_{n+1}]\): \[\mathrm{d}\widetilde{\bm{x}}_{t_{n}+\tau}(\omega)=\left[\frac{1}{2}\widetilde {\bm{x}}_{t_{n}+\tau}(\omega)+\nabla\log\overline{p}_{t_{n}+\tau}(\widetilde{ \bm{x}}_{t_{n}+\tau}(\omega))\right]\mathrm{d}\tau+\mathrm{d}\widetilde{\bm{w}} _{t_{n}+\tau}(\omega).\] (B.5)

**Remark B.4**.: _The applicability of Girsanov's theorem here relies on the \(\mathcal{F}_{\tau}\)-adaptivity of \(\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta}\Big{(}\widehat{\bm{y}}_{t_{n},g_{n}(\tau) }^{(K-1)}(\omega)\Big{)}\) established by Lemma B.2. One should notice the change of measure procedure above depends on the number of iterations \(K\), and different \(K\) would lead to different transform (B.4)._

Then Corollary A.5 provides the following computation

\[\begin{split} D_{\mathrm{KL}}(\overline{p}_{t_{n+1}}\|\widehat{ q}_{t_{n+1}})&\leq D_{\mathrm{KL}}(\overline{p}_{t_{n}:t_{n+1}}\| \widehat{q}_{t_{n}:t_{n+1}})\\ &=D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+ \mathbb{E}_{\omega\sim q|_{\mathcal{F}_{t_{n}}}}\left[\frac{1}{2}\int_{0}^{h_{n }}\|\bm{\delta}_{t_{n}}(\tau,\omega)\|^{2}\mathrm{d}\tau\right],\end{split}\] (B.6)where the first inequality is by the data-processing inequality (Theorem A.1). Now, the problem remaining is to bound the discrepancy quantified by

\[\begin{split}&\quad\int_{0}^{h_{n}}\|\boldsymbol{\delta}_{t_{n}}( \tau,\omega)\|^{2}\mathrm{d}\tau\\ =&\,\int_{0}^{h_{n}}\left\|\boldsymbol{s}_{t_{n}+g_{ n}(\tau)}^{\theta}(\widehat{\boldsymbol{y}}_{t_{n},g_{n}(\tau)}^{(K-1)}(\omega))- \nabla\log\widetilde{p}_{t_{n}+\tau}(\widehat{\boldsymbol{y}}_{t_{n},\tau}^{( K)}(\omega))\right\|^{2}\mathrm{d}\tau\\ \leq& 3\Bigg{(}\underbrace{\int_{0}^{h_{n}}\left\| \nabla\log\widetilde{p}_{t_{n}+g_{n}(\tau)}\big{(}\widehat{\boldsymbol{y}}_{t _{n},g_{n}(\tau)}^{(K)}(\omega)\big{)}-\nabla\log\widetilde{p}_{t_{n}+\tau} \big{(}\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(K)}(\omega)\big{)}\right\|^{2} \mathrm{d}\tau}_{:=A_{t_{n}}(\omega)}\\ &\quad+\underbrace{\int_{0}^{h_{n}}\left\|\boldsymbol{s}_{t_{n}+g _{n}(\tau)}^{\theta}\big{(}\widehat{\boldsymbol{y}}_{t_{n},g_{n}(\tau)}^{(K)} (\omega)\big{)}-\nabla\log\widetilde{p}_{t_{n}+g_{n}(\tau)}\big{(}\widehat{ \boldsymbol{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)\big{)}\right\|^{2}\mathrm{d }\tau}_{:=B_{t_{n}}(\omega)}\\ &\quad+\int_{0}^{h_{n}}\left\|\boldsymbol{s}_{t_{n}+g_{n}(\tau)}^{ \theta}\big{(}\widehat{\boldsymbol{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega) \big{)}-\boldsymbol{s}_{t_{n}+g_{n}(\tau)}^{\theta}\big{(}\widehat{ \boldsymbol{y}}_{t_{n},g_{n}(\tau)}^{(K-1)}(\omega)\big{)}\right\|^{2}\mathrm{d }\tau\Bigg{)}.\end{split}\] (B.7)

Before we continue our proof, we would like first to provide the following lemma bounding the behavior of the auxiliary process (B.2) when \(k=0\) for \(\tau\in[0,h_{n}]\).

**Lemma B.5**.: _For any \(n\in[0:N-1]\), suppose the initialization \(\widehat{\boldsymbol{y}}_{t_{n}}\) in (B.3) of the auxiliary process (B.2) follows the distribution of \(\widetilde{\boldsymbol{x}}_{t_{n}}\sim\widetilde{p}_{t_{n}}\), then the following estimate holds_

\[\begin{split}&\quad\sup_{\tau\in[0,h_{n}]}\mathbb{E}_{\omega \sim\widetilde{p}|_{\mathcal{F}_{t_{n}}}}\left[\|\widehat{\boldsymbol{y}}_{t_ {n},\tau}^{(1)}(\omega)-\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(0)}(\omega) \|^{2}\right]\\ \leq&\,h_{n}e^{\frac{7}{2}h_{n}}\left(M_{\boldsymbol {s}}^{2}+2d\right)+3e^{\frac{7}{2}h_{n}}\mathbb{E}_{\omega\sim\widetilde{p}|_{ \mathcal{F}_{t_{n}}}}\left[A_{t_{n}}(\omega)+B_{t_{n}}(\omega)\right]\\ +&\,3e^{\frac{7}{2}h_{n}}h_{n}L_{\boldsymbol{s}}^{2 }\sup_{\tau\in[0,h_{n}]}\mathbb{E}_{\omega\sim\widetilde{p}|_{\mathcal{F}_{t _{n}}}}\left[\left\|\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(K)}(\omega)- \widehat{\boldsymbol{y}}_{t_{n},\tau}^{(K-1)}(\omega)\right\|^{2}\right].\end{split}\] (B.8)

Proof.: Let \(\boldsymbol{z}_{t_{n},\tau}=\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(1)}- \widehat{\boldsymbol{y}}_{t_{n},\tau}^{(0)}\). For \(k=0\), we can rewrite (B.2) as

\[\mathrm{d}\boldsymbol{z}_{t_{n},\tau}=\Bigg{[}\frac{1}{2}\left(\boldsymbol{z} _{t_{n},\tau}+\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(0)}\right)+\boldsymbol{ s}_{t_{n}+g_{n}(\tau)}^{\theta}\Big{(}\widehat{\boldsymbol{y}}_{t_{n},g_{n}( \tau)}^{(0)}\Big{)}\Bigg{]}\mathrm{d}\tau+\mathrm{d}\boldsymbol{w}_{t_{n}+ \tau},\]

By applying Ito's lemma and plugging in the expression of \(\boldsymbol{w}_{t_{n}+\tau}\) given by Theorem A.4, we have

\[\begin{split}\mathrm{d}\|\boldsymbol{z}_{t_{n},\tau}\|^{2}=& \Bigg{[}\|\boldsymbol{z}_{t_{n},\tau}\|^{2}+\boldsymbol{z}_{t_{n},\tau}^{\top} \widehat{\boldsymbol{y}}_{t_{n},\tau}^{(0)}+2\boldsymbol{z}_{t_{n},\tau}^{ \top}\boldsymbol{s}_{t_{n}+g_{n}(\tau)}^{\prime}\Big{(}\widehat{\boldsymbol{y }}_{t_{n},g_{n}(\tau)}^{(0)}\Big{)}+d\Bigg{]}\mathrm{d}\tau\\ &+2\boldsymbol{z}_{t_{n},\tau}^{\top}\big{(}\boldsymbol{\mathrm{ d}}\widetilde{\boldsymbol{w}}_{t_{n}+\tau}(\omega)-\boldsymbol{\delta}_{t_{n}}(\tau, \omega)d\tau\big{)},\end{split}\] (B.9)

By integrating from \(0\) to \(\tau\) and taking the expectation on both sides of (B.9), we obtain that

\[\begin{split}&\quad\mathbb{E}_{\omega\sim\widetilde{p}|_{\mathcal{F}_{ t_{n}}}}\left[\|\boldsymbol{z}_{t_{n},\tau}\|^{2}\right]\\ =&\,\mathbb{E}_{\omega\sim\widetilde{p}|_{\mathcal{F} _{t_{n}}}}\left[\int_{0}^{\tau}\Bigg{(}\|\boldsymbol{z}_{t_{n},\tau^{\prime}} \|^{2}+\boldsymbol{z}_{t_{n},\tau^{\prime}}^{\top}\widehat{\boldsymbol{y}}_{t_ {n},\tau^{\prime}}^{(0)}+2\boldsymbol{z}_{t_{n},\tau^{\prime}}^{\top} \boldsymbol{s}_{t_{n}+g_{n}(\tau^{\prime})}^{\prime}\Big{(}\widehat{\boldsymbol{y }}_{t_{n},g_{n}(\tau^{\prime})}^{(0)}\Big{)}+d\right)\!\mathrm{d}\tau^{\prime} \right]\\ &+2\mathbb{E}_{\omega\sim\widetilde{p}|_{\mathcal{F}_{t_{n}}}}\left[ \int_{0}^{\tau}\boldsymbol{z}_{t_{n},\tau^{\prime}}^{\top}\Big{(}\mathrm{d} \widetilde{\boldsymbol{w}}_{t_{n}+\tau^{\prime}}(\omega)-\boldsymbol{\delta}_{t_{n} }(\tau^{\prime},\omega)\mathrm{d}\tau^{\prime}\Big{)}\right],\end{split}\]

and by AM-GM, we further have

\[\begin{split}&\quad\mathbb{E}_{\omega\sim\widetilde{p}|_{\mathcal{F}_{ t_{n}}}}\left[\|\boldsymbol{z}_{t_{n},\tau}\|^{2}\right]\\ \leq&\,\mathbb{E}_{\omega\sim\widetilde{p}|_{ \mathcal{F}_{t_{n}}}}\left[\int_{0}^{\tau}\left[\frac{7}{2}\|\boldsymbol{z}_{t_ {n},\tau^{\prime}}\|^{2}+\frac{1}{2}\left\|\widehat{\boldsymbol{y}}_{t_{n}, \tau^{\prime}}^{(0)}\right\|^{2}+\left\|\boldsymbol{s}_{t_{n}+g_{n}(\tau^{ \prime})}^{\theta}\Big{(}\widehat{\boldsymbol{y}}_{t_{n},g_{n}(\tau^{\prime})}^{ (0)}\Big{)}\right\|^{2}+d+\|\boldsymbol{\delta}_{t_{n}}(\tau,\omega)\|^{2} \right]\!\mathrm{d}\tau^{\prime}\right]\\ \leq&\,\int_{0}^{\tau}\mathbb{E}_{\omega\sim\widetilde{p}|_{ \mathcal{F}_{t_{n}}}}\left[\frac{7}{2}\|\boldsymbol{z}_{t_{n},\tau^{\prime}}\|^{2}+ \|\boldsymbol{\delta}_{t_{n}}(\tau,\omega)\|^{2}\right]\mathrm{d}\tau^{\prime} +\left(\frac{1}{2}\mathbb{E}\left[\left\|\widehat{\boldsymbol{y}}_{t_{n},\tau}^{(0)} \right\|^{2}\right]+M_{\boldsymbol{s}}^{2}+d\right)\tau,\end{split}\]where \(\bm{\delta}_{t_{n}}(\tau,\omega)\) is defined in (B.4). Similar to (B.7), we may use triangle inequality to upper bound \(\|\bm{\delta}_{t_{n}}(\tau,\omega)\|^{2}\), which implies that for any \(\tau\in[0,h_{n}]\)

\[\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[\| \bm{z}_{t_{n},\tau}\|^{2}\right]\] \[\leq\frac{7}{2}\int_{0}^{\tau}\mathbb{E}_{\omega\sim\bar{p}\mid_{ \mathcal{F}_{t_{n}}}}\left[\|\bm{z}_{t_{n},\tau^{\prime}}\|^{2}\right]\mathrm{ d}\tau^{\prime}+\left(\frac{1}{2}\mathbb{E}\left[\left\|\widehat{\bm{y}}_{t_{n}, \tau}^{(0)}\right\|^{2}\right]+M_{\bm{s}}^{2}+d\right)\tau\] \[+3\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[ \int_{0}^{\tau}\left\|\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta}(\widehat{\bm{y}}_{t _{n},g_{n}(\tau)}^{(K-1)}(\omega))-\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta}( \widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega))\right\|^{2}\mathrm{d}\tau ^{\prime}\right]\] \[+3\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[ \int_{0}^{\tau}\left\|\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta}(\widehat{\bm{y}}_{t _{n},g_{n}(\tau)}^{(K)}(\omega))-\nabla\log\bar{p}_{t_{n},g_{n}(\tau)}(\widehat {\bm{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega))\right\|^{2}\mathrm{d}\tau^{\prime}\right]\] \[+3\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[ \int_{0}^{\tau}\left\|\nabla\log\bar{p}_{t_{n}+g_{n}(\tau)}(\widehat{\bm{y}}_{ t_{n},g_{n}(\tau)}^{(K)}(\omega))-\nabla\log\bar{p}_{t_{n}+\tau}(\widehat{\bm{y}}_{ t_{n}+\tau}^{(K)}(\omega))\right\|^{2}\mathrm{d}\tau^{\prime}\right]\] \[\leq\frac{7}{2}\int_{0}^{\tau}\mathbb{E}_{\omega\sim\bar{p}\mid_{ \mathcal{F}_{t_{n}}}}\left[\|\bm{z}_{t_{n},\tau^{\prime}}\|^{2}\right]\mathrm{ d}\tau^{\prime}+\left(\frac{1}{2}\mathbb{E}\left[\left\|\widehat{\bm{y}}_{t_{n}, \tau}^{(0)}\right\|^{2}\right]+M_{\bm{s}}^{2}+d\right)\tau\] \[+3L_{\bm{s}}^{2}\int_{0}^{\tau}\mathbb{E}_{\omega\sim\bar{p}\mid _{\mathcal{F}_{t_{n}}}}\left[\left\|\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{\prime })}^{(K)}(\omega)-\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(K-1)}(\omega) \right\|^{2}\right]\mathrm{d}\tau^{\prime}+3\mathbb{E}_{\omega\sim\bar{p}\mid _{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}(\omega)+B_{t_{n}}(\omega)\right],\]

where in the second inequality above, we have used the fact that \(s_{t}^{\theta}(\cdot)\) is \(L_{\bm{s}}\)-Lipschitz for any \(t\). By Gronwall's inequality, we have that for any \(\tau\in[0,h_{n}]\)

\[\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[\| \bm{z}_{t_{n},\tau}\|^{2}\right] \leq e^{\frac{7}{2}\tau}\left[\left(\frac{1}{2}\mathbb{E}\left[ \left\|\widehat{\bm{y}}_{t_{n},\tau}^{(0)}\right\|^{2}\right]+M_{\bm{s}}^{2}+d \right)\tau\right]\] \[+3e^{\frac{7}{2}\tau}\mathbb{E}_{\omega\sim\bar{p}\mid_{ \mathcal{F}_{t_{n}}}}\left[A_{t_{n}}(\omega)+B_{t_{n}}(\omega)\right]\] (B.10) \[+3e^{\frac{7}{2}\tau}L_{\bm{s}}^{2}\int_{0}^{\tau}\mathbb{E}_{ \omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[\left\|\widehat{\bm{y}}_{t_{ n},g_{n}(\tau^{\prime})}^{(K)}(\omega)-\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(K-1)}( \omega)\right\|^{2}\right]\mathrm{d}\tau^{\prime}.\]

By assumption, \(\widehat{\bm{y}}_{t_{n},\tau}^{(0)}=\widehat{\bm{y}}_{t_{n}}\) follows the distribution of \(\bar{\bm{x}}_{t_{n}}\sim\widetilde{\bm{y}}_{t_{n}}\), which allows us to bound the second moment of \(\widehat{\bm{y}}_{t_{n}}\) for any \(n\in[0:N]\) by Lemma A.8:

\[\mathbb{E}\left[\|\widehat{\bm{y}}_{t_{n}}\|^{2}\right]=\mathbb{E}\left[\| \bar{\bm{x}}_{t_{n}}\|^{2}\right]\leq 2d.\]

Substituting (A.5) into (B.10) then yields that for any \(\tau\in[0,h_{n}]\)

\[\mathbb{E}_{\omega\sim q\mid_{\mathcal{F}_{t_{n}}}}\left[\|\bm{z}_ {t_{n},\tau}\|^{2}\right] \leq\tau e^{\frac{7}{2}\tau}\left(M_{\bm{s}}^{2}+2d\right)+3e^{ \frac{7}{2}\tau}\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[A_ {t_{n}}(\omega)+B_{t_{n}}(\omega)\right]\] \[+3\tau e^{\frac{7}{2}\tau}L_{\bm{s}}^{2}\sup_{\tau^{\prime}\in[0,h _{n}]}\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[\left\| \widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{ \prime}}^{(K-1)}(\omega)\right\|^{2}\right].\]

Taking supremum with respect to \(\tau\in[0,h_{n}]\) on both sides above completes our proof. 

As utilized in the proof of the existence of solutions of SDEs, the following lemma demonstrates the exponential convergence of the iteration defined in (B.2).

**Lemma B.6** (Exponential convergence of Picard iteration in PIADM-SDE).: _For any \(n\in[0,N]\), suppose the initialization \(\widehat{\bm{y}}_{t_{n}}\) in (B.3) of the auxiliary process (B.2) follows the distribution of \(\bar{\bm{x}}_{t_{n}}\sim\widetilde{p}_{t_{n}}\), then the two ending terms \(\widehat{\bm{y}}_{t_{n},\tau}^{(K)}\) and \(\widehat{\bm{y}}_{t_{n},\tau}^{(K-1)}\) of the sequence \(\{\widehat{\bm{y}}_{t_{n},\tau}^{(k)}\}_{k\in[0:K-1]}\) satisfy the following exponential convergence rate_

\[\sup_{\tau\in[0,h_{n}]}\mathbb{E}_{\omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}} \left[\left\|\widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)-\widehat{\bm{y}}_{t_{n}, \tau}^{(K-1)}(\omega)\right\|^{2}_{2}\right]\] \[\leq\frac{\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\right)^{K-1}he^{ \frac{7}{2}h_{n}}\left(M_{\bm{s}}^{2}+2d\right)}{1-3\left(L_{\bm{s}}^{2}h_{n}e^ {2h_{n}}\right)^{K-1}e^{\frac{7}{2}h_{n}}h_{n}L_{\bm{s}}^{2}}+\frac{3\left(L_{ \bm{s}}^{2}h_{n}e^{2h_{n}}\right)^{K-1}e^{\frac{7}{2}h_{n}}\mathbb{E}_{ \omega\sim\bar{p}\mid_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}(\omega)+B_{t_{n}}( \omega)\right]}{1-3\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\right)^{K-1}e^{\frac{7}{2} h_{n}}h_{n}L_{\bm{s}}^{2}}.\]

Proof.: For each \(\omega\in\Omega\) conditioned on the filtration \(\mathcal{Fwe have

\[\mathrm{d}\left(\widehat{\bm{y}}_{t_{n},\tau}^{(k+1)}(\omega)- \widehat{\bm{y}}_{t_{n},\tau}^{(k)}(\omega)\right)\] \[= \left[\frac{1}{2}\left(\widehat{\bm{y}}_{t_{n},\tau}^{(k+1)}( \omega)-\widehat{\bm{y}}_{t_{n},\tau}^{(k)}(\omega)\right)+\bm{s}_{t_{n}+g_{n} (\tau)}^{\theta}\big{(}\widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(k)}(\omega) \big{)}-\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta}\big{(}\widehat{\bm{y}}_{t_{n},g_{n} (\tau)}^{(k-1)}(\omega)\big{)}\right]\mathrm{d}\tau,\]

where the diffusion term \(\mathrm{d}\bm{w}_{t_{n}+\tau}\) cancels each other out. Now we may use the formula above to compute derivative \(\frac{\mathrm{d}}{\mathrm{d}\tau^{\prime}}\left\|\widehat{\bm{y}}_{t_{n},\tau ^{\prime}}^{(k+1)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k)}(\omega )\right\|^{2}\) explicitly, integrate it from \(\tau^{\prime}=0\) to \(\tau\), and obtain the following inequality

\[\left\|\widehat{\bm{y}}_{t_{n},\tau}^{(k+1)}(\omega)-\widehat{\bm {y}}_{t_{n},\tau}^{(k)}(\omega)\right\|^{2}\] \[= \int_{0}^{\tau}2\left(\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k+ 1)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k)}(\omega)\right)^{\top} \left(\bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{\theta}\big{(}\widehat{\bm{y}}_{t_{ n},g_{n}(\tau^{\prime})}^{(k)}(\omega)\big{)}-\bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{ \theta}\big{(}\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(k-1)}(\omega) \big{)}\right)\mathrm{d}\tau^{\prime}\] \[+ \int_{0}^{\tau}\left\|\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k+ 1)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k)}(\omega)\right\|^{2} \mathrm{d}\tau^{\prime}\] \[\leq 2\int_{0}^{\tau}\left\|\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^ {(k+1)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k)}(\omega)\right\|^ {2}\mathrm{d}\tau^{\prime}\] \[+ \int_{0}^{\tau}\left\|\bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{\theta }\big{(}\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(k)}(\omega)\big{)}- \bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{\theta}\big{(}\widehat{\bm{y}}_{t_{n},g_{ n}(\tau^{\prime})}^{(k-1)}(\omega)\big{)}\right\|^{2}\mathrm{d}\tau^{\prime}\] \[\leq 2\int_{0}^{\tau}\left\|\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^ {(k+1)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k)}(\omega)\right\|^ {2}\mathrm{d}\tau^{\prime}+L_{\bm{s}}^{2}\int_{0}^{\tau}\left\|\widehat{\bm{y }}_{t_{n},g_{n}(\tau^{\prime})}^{(k)}(\omega)-\widehat{\bm{y}}_{t_{n},g_{n}( \tau^{\prime})}^{(k-1)}(\omega)\right\|^{2}\mathrm{d}\tau^{\prime}.\]

By Gronwall's inequality, we have

\[\left\|\widehat{\bm{y}}_{t_{n},\tau}^{(k+1)}(\omega)-\widehat{\bm{y}}_{t_{n}, \tau}^{(k)}(\omega)\right\|^{2}\leq L_{\bm{s}}^{2}e^{2\tau}\int_{0}^{\tau} \left\|\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(k)}(\omega)-\widehat{ \bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(k-1)}(\omega)\right\|^{2}\mathrm{d} \tau^{\prime}.\] (B.12)

Taking expectation on both sides above further implies that for any \(\tau\in[0,h_{n}]\),

\[\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[\left\| \widehat{\bm{y}}_{t_{n},\tau}^{(k+1)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau}^{(k )}(\omega)\right\|^{2}\right]\] \[\leq L_{\bm{s}}^{2}e^{2\tau}\int_{0}^{\tau}\mathbb{E}_{\omega\sim \bar{p}|_{\mathcal{F}_{t_{n}}}}\left[\left\|\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{ \prime})}^{(k)}(\omega)-\widehat{\bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(k-1)}( \omega)\right\|^{2}\right]\mathrm{d}\tau^{\prime}\] (B.13) \[\leq L_{\bm{s}}^{2}\tau e^{2\tau}\sup_{\tau^{\prime}\in[0,\tau]} \mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[\left\|\widehat{\bm {y}}_{t_{n},\tau^{\prime}}^{(k)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{ \prime}}^{(k-1)}(\omega)\right\|^{2}\right].\]

Furthermore, we take supremum over \(\tau\in[0,h_{n}]\) on both sides above and iterate (B.12) over \(k\in\mathbb{N}\), which indicates

\[\sup_{\tau\in[0,h_{n}]}\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_ {t_{n}}}}\left[\left\|\widehat{\bm{y}}_{t_{n},\tau}^{(k+1)}(\omega)-\widehat{\bm {y}}_{t_{n},\tau}^{(k)}(\omega)\right\|^{2}\right]\] \[\leq L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\sup_{\tau\in[0,h_{n}]}\mathbb{E }_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[\left\|\widehat{\bm{y}}_{t_{n}, \tau}^{(k)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau^{\prime}}^{(k-1)}(\omega) \right\|^{2}\right]\] \[\leq\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\right)^{k}\sup_{\tau\in[ 0,h_{n}]}\mathbb{E}\left[\left\|\widehat{\bm{y}}_{t_{n},\tau}^{(1)}(\omega)- \widehat{\bm{y}}_{t_{n},\tau}^{(0)}(\omega)\right\|^{2}\right]\] (B.14) \[\leq\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\right)^{k}he^{\frac{7}{2}h_ {n}}\left(M_{\bm{s}}^{2}+2d\right)+3\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\right)^ {k}e^{\frac{7}{2}h_{n}}\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[A_ {t_{n}}(\omega)+B_{t_{n}}(\omega)\right]\] \[+3\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\right)^{k}e^{\frac{7}{2}h_{n}}h _{n}L_{\bm{s}}^{2}\sup_{\tau\in[0,h_{n}]}\mathbb{E}_{\omega\sim\bar{p}|_{ \mathcal{F}_{t_{n}}}}\left[\left\|\widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)- \widehat{\bm{y}}_{t_{n},\tau}^{(K-1)}(\omega)\right\|^{2}\right],\]where the last inequality follows from Lemma B.5. By rearranging the inequality above, setting \(k=K-1\) and using the assumption that \(L_{s}^{2}h_{n}e^{2h_{n}}\ll 1\), we obtain

\[\sup_{\tau\in[0,h_{n}]}\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_ {t_{n}}}}\left[\left\|\widetilde{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)-\widetilde{ \bm{y}}_{t_{n},\tau}^{(K-1)}(\omega)\right\|^{2}\right]\] \[\leq \frac{\left(L_{s}^{2}h_{n}e^{2h_{n}}\right)^{K-1}he^{\frac{7}{2}h_ {n}}\left(M_{s}^{2}+2d\right)+3\left(L_{s}^{2}h_{n}e^{2h_{n}}\right)^{K-1}e^{ \frac{7}{2}h_{n}}\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{ t_{n}}(\omega)+B_{t_{n}}(\omega)\right]}{1-3\left(L_{s}^{2}h_{n}e^{2h_{n}} \right)^{K-1}e^{\frac{7}{2}h_{n}}h_{n}L_{s}^{2}},\] (B.15)

as desired. 

The following lemma from [107] bounds the expectation of the term \(A_{t_{n}}(\omega)\) in (B.7):

**Lemma B.7** ([107, Section 3.1]).: _We have_

\[\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}(\omega) \right]\lesssim\epsilon d{h_{n}},\quad\text{for }n\in[0:N-2],\text{ and }\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{N-1}}( \omega)\right]\lesssim\epsilon d\log\eta^{-1},\]

_where \(\eta\) is the parameter for early stopping._

Proof.: Notice that

\[\mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n} }(\omega)\right]\] \[= \mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[\int_{ 0}^{h_{n}}\left\|\nabla\log\overline{p}_{t_{n}+g_{n}(\tau)}\big{(}\widetilde{ \bm{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)\big{)}-\nabla\log\overline{p}_{t_{n} +\tau}\big{(}\widetilde{\bm{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)\big{)} \right\|^{2}\mathrm{d}\tau\right]\] \[= \mathbb{E}_{\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[\sum_{ m=0}^{M_{n}}\int_{\tau_{n,m}}^{\tau_{n,m+1}}\left\|\nabla\log\overline{p}_{t_{n}+ \tau_{n,m}}\big{(}\widetilde{\bm{y}}_{t_{n},\tau_{n,m}}^{(K)}(\omega)\big{)}- \nabla\log\overline{p}_{t_{n}+\tau}\big{(}\widetilde{\bm{y}}_{t_{n},\tau}^{(K )}(\omega)\big{)}\right\|^{2}\mathrm{d}\tau\right],\] \[= \sum_{m=0}^{M_{n}}\int_{\tau_{n,m}}^{\tau_{n,m+1}}\mathbb{E}_{ \omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}}\left[\left\|\nabla\log\overline{p}_ {t_{n}+\tau_{n,m}}\big{(}\widetilde{\bm{x}}_{t_{n}+\tau}(\omega)\big{)}- \nabla\log\overline{p}_{t_{n}+\tau}\big{(}\widetilde{\bm{x}}_{t_{n}+\tau}( \omega)\big{)}\right\|^{2}\right]\mathrm{d}\tau,\]

where for the last equality, we use the fact that the process \(\widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)\) follows the backward SDE with the true score function under the measure \(\bar{p}\). In the following, we drop the superscript \(\omega\sim\bar{p}|_{\mathcal{F}_{t_{n}}}\) of the expectation for simplicity.

By Lemma A.6 and A.7, we have

\[\mathbb{E}\left[\left\|\nabla\log\overline{p}_{t_{n}+\tau_{n,m}} \big{(}\widetilde{\bm{x}}_{t_{n}+\tau}(\omega)\big{)}-\nabla\log\overline{p}_ {t_{n}+\tau}\big{(}\widetilde{\bm{x}}_{t_{n}+\tau}(\omega)\big{)}\right\|^{2}\right]\] \[\leq \int_{0}^{\tau}\left(\frac{1}{2}\mathbb{E}\left[\|\nabla\log \overline{p}_{t_{n}+\tau_{n,m}}\big{(}\widetilde{\bm{x}}_{t_{n}+\tau_{n,m}}( \omega)\big{)}\|^{2}\right]+\mathbb{E}\left[\|\nabla^{2}\log\overline{p}_{t_{n }+\tau^{\prime}}\big{(}\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}(\omega)\big{)} \|_{F}^{2}\right]\right)\mathrm{d}\tau^{\prime}\] \[\leq \int_{0}^{\tau}\left(\frac{1}{2}d\widetilde{\sigma}_{\tau^{\prime} }^{-2}+d\widetilde{\sigma}_{\tau^{\prime}}^{-4}\right)\mathrm{d}\tau^{\prime}+ \left(\widetilde{\sigma}_{t_{n}+\tau_{n,m}}^{-4}\mathbb{E}\left[\operatorname{tr }\tilde{\bm{\Sigma}}_{t_{n}+\tau_{n,m}}\right]-\widetilde{\sigma}_{t_{n}+\tau}^ {-4}\mathbb{E}\left[\operatorname{tr}\tilde{\bm{\Sigma}}_{t_{n}+\tau}\right] \right),\]

Now noticing that

\[\widetilde{\sigma}_{t}^{2}=\sigma_{T-t}^{2}\lesssim T-t,\]

we further have

\[\int_{\tau_{n,m}}^{\tau_{n,m+1}}\mathbb{E}\left[\left\|\nabla \log\overline{p}_{t_{n}+\tau_{n,m}}\big{(}\widetilde{\bm{x}}_{t_{n}+\tau}( \omega)\big{)}-\nabla\log\overline{p}_{t_{n}+\tau}\big{(}\widetilde{\bm{x}}_{t_{ n}+\tau}(\omega)\big{)}\right\|^{2}\right]\mathrm{d}\tau\] \[\lesssim \int_{\tau_{n,m}}^{\tau_{n,m+1}}\int_{0}^{\tau^{\prime}}\frac{d}{( T-t_{n}-\tau_{n,m+1})^{2}}\mathrm{d}\tau^{\prime}\mathrm{d}\tau+\frac{\epsilon_{n,m} \left(\mathbb{E}\left[\operatorname{tr}\tilde{\bm{\Sigma}}_{t_{n}+\tau_{n,m}} \right]-\mathbb{E}\left[\operatorname{tr}\tilde{\bm{\Sigma}}_{t_{n}+\tau_{n,m+1}} \right]\right)}{(T-t_{n}-\tau_{n,m})^{2}}\] \[\lesssim d\frac{\epsilon_{n,m}^{2}}{(T-t_{n}-\tau_{n,m+1})^{2}}+\frac{ \epsilon\left(\mathbb{E}\left[\operatorname{tr}\tilde{\bm{\Sigma}}_{t_{n}+\tau_{n,m}} \right]-\mathbb{E}\left[\operatorname{tr}\tilde{\bm{\Sigma}}_{t_{n}+\tau_{n,m+1}} \right]\right)}{T-t_{n}-\tau_{n,m}},\]and thus

\[\sum_{m=0}^{M_{n}}\int_{\tau_{n,m}}^{\tau_{n,m+1}}\mathbb{E}\left[ \left\|\nabla\log\overline{p}_{t_{n}+\tau_{n,m}}\big{(}\widetilde{\bm{x}}_{t_{n}+ \tau}(\omega)\big{)}-\nabla\log\overline{p}_{t_{n}+\tau}\big{(}\widetilde{\bm{x }}_{t_{n}+\tau}(\omega)\big{)}\right\|^{2}\right]\mathrm{d}\tau\] \[\lesssim d\sum_{m=0}^{M_{n}}\frac{\epsilon_{n,m}^{2}}{(T-t_{n}-\tau_{n, m+1})^{2}}+\sum_{m=0}^{M_{n}}\frac{\epsilon}{T-t_{n}-\tau_{n,m}}\left(\mathbb{E} \left[\operatorname{tr}\widetilde{\bm{\Sigma}}_{t_{n}+\tau_{n,m}}\right]- \mathbb{E}\left[\operatorname{tr}\widetilde{\bm{\Sigma}}_{t_{n}+\tau_{n,m+1}} \right]\right)\] \[\leq d\epsilon^{2}M_{n}+\frac{\epsilon\mathbb{E}\left[\operatorname{ tr}\widetilde{\bm{\Sigma}}_{t_{n}+\tau_{n,0}}\right]}{T-t_{n}-\tau_{n,0}}+\sum_{m=0}^{M_{n}} \frac{\epsilon\epsilon_{n,m}\mathbb{E}\left[\operatorname{tr}\widetilde{\bm{ \Sigma}}_{t_{n}+\tau_{n,m}}\right]}{(T-t_{n}-\tau_{n,m+1})(T-t_{n}-\tau_{n,m})}\] \[\leq d\epsilon^{2}M_{n}+\epsilon d+d\epsilon^{2}M_{n}\lesssim d \epsilon^{2}M_{n}.\]

For \(n\in[0,N-2]\), we have \(M_{n}\epsilon=h_{n}\) and thus \(\mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}( \omega)\right]\lesssim\epsilon dh_{n}\), and for \(n=N-1\), we have

\[M_{N}\lesssim\int_{\eta}^{h}\frac{1}{\epsilon\tau}\mathrm{d}\tau=\log\eta^{-1 }\epsilon^{-1}\]

and thus \(\mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{N-1}}( \omega)\right]\lesssim\epsilon^{2}dM_{n}\lesssim\epsilon d\log\eta^{-1}\). 

### Overall Error Bound

Proof of Theorem 3.3.: We first continue the computation in (B.6) and (B.7):

\[D_{\mathrm{KL}}(\overline{p}_{t_{n+1}}\|\widehat{q}_{t_{n+1}}) \leq D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+\mathbb{E}_{ \omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[1\frac{1}{2}\int_{0}^{h_{n }}\|\bm{\delta}_{t_{n}}(\tau,\omega)\|^{2}\mathrm{d}\tau\right]\] \[\leq D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+3 \mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}( \omega)+B_{t_{n}}(\omega)\right]\] \[+ 3\mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[ \int_{0}^{h_{n}}\left\|\bm{\delta}_{t_{n}+g_{n}(\tau)}^{\theta}\big{(} \widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)\big{)}-\bm{s}_{t_{n}+g_{n} (\tau)}^{\theta}\big{(}\widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(K-1)}(\omega) \big{)}\right\|^{2}\mathrm{d}\tau\right]\] \[\leq D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+3 \mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}( \omega)+B_{t_{n}}(\omega)+L_{\bm{s}}^{2}\int_{0}^{h_{n}}\left\|\widehat{\bm{y }}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)-\widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(K -1)}(\omega)\right\|^{2}\mathrm{d}\tau\right]\] \[\leq D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+3 \mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}( \omega)+B_{t_{n}}(\omega)+h_{n}L_{\bm{s}}^{2}\sup_{\tau\in[0,h_{n}]}\left\| \widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)-\widehat{\bm{y}}_{t_{n},\tau}^{(K -1)}(\omega)\right\|^{2}\right].\]

Then plugging in the result of Lemma B.6, we have

\[D_{\mathrm{KL}}(\overline{p}_{t_{n+1}}\|\widehat{q}_{t_{n+1}})\] \[\leq D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+3 \mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}( \omega)+B_{t_{n}}(\omega)\right]+3h_{n}L_{\bm{s}}^{2}\frac{\left(L_{\bm{s}}^{2} h_{n}e^{2h_{n}}\right)^{K-1}he^{\frac{1}{2}h_{n}}\left(M_{\bm{s}}^{2}+2d \right)}{1-3\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}}\right)^{K-1}e^{\frac{1}{2}h_{n }}h_{n}L_{\bm{s}}^{2}}\] \[+ h_{n}L_{\bm{s}}^{2}\frac{9\left(L_{\bm{s}}^{2}h_{n}e^{2h_{n}} \right)^{K-1}e^{\frac{7}{2}h_{n}}\mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F} _{t_{n}}}}\left[A_{t_{n}}(\omega)+B_{t_{n}}(\omega)\right]}{1-3\left(L_{\bm{s}}^ {2}h_{n}e^{2h_{n}}\right)^{K-1}e^{\frac{7}{2}h_{n}}h_{n}L_{\bm{s}}^{2}}\] \[\lesssim D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+\frac{1 +e^{-K}h_{n}e^{h_{n}}}{1-e^{-K}h_{n}e^{h_{n}}}\mathbb{E}_{\omega\sim\overline{p} |_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}(\omega)+B_{t_{n}}(\omega)\right]+e^{-K}h _{n}^{2}e^{h_{n}}d\] \[\lesssim D_{\mathrm{KL}}(\overline{p}_{t_{n}}\|\widehat{q}_{t_{n}})+ \mathbb{E}_{\omega\sim\overline{p}|_{\mathcal{F}_{t_{n}}}}\left[A_{t_{n}}( \omega)+B_{t_{n}}(\omega)\right]+e^{-K}h_{n}^{2}e^{h_{n}}d,\]

where we used the assumption that \(L_{\bm{s}}^{2}h_{n}e^{\frac{7}{2}h_{n}}\ll 1\).

The term \(\sum_{n=0}^{N-1}\mathbb{E}_{\omega\sim\tilde{p}|_{Y_{t_{n}}}}[B_{t_{n}}(\omega)]\) is bounded by Assumption 3.1 as

\[\sum_{n=0}^{N-1}\mathbb{E}_{\omega\sim\tilde{p}|_{Y_{t_{n}}}}[B_{t_{ n}}(\omega)]\] \[\leq \mathbb{E}_{\omega\sim\tilde{p}|_{Y_{t_{n}}}}\left[\sum_{n=0}^{N-1 }\int_{0}^{h_{n}}\left\|\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta}\big{(}\widehat{\bm {y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)\big{)}-\nabla\log\tilde{p}_{t_{n}+g_{n} (\tau)}\big{(}\widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(K)}(\omega)\big{)}\right\|^ {2}\mathrm{d}\tau\right]\] \[= \mathbb{E}_{\omega\sim\tilde{p}|_{Y_{t_{n}}}}\left[\sum_{n=0}^{N- 1}\sum_{m=0}^{M_{n}-1}\epsilon_{n,m}\left\|\bm{s}_{t_{n}+\tau_{n,m}}^{\theta} \big{(}\widehat{\bm{y}}_{t_{n},\tau_{n,m}}^{(K)}(\omega)\big{)}-\nabla\log \tilde{p}_{t_{n}+\tau_{n,m}}\big{(}\widehat{\bm{y}}_{t_{n},\tau_{n,m}}^{(K)}( \omega)\big{)}\right\|^{2}\right]\] \[= \mathbb{E}_{\omega\sim\tilde{p}|_{Y_{t_{n}}}}\left[\sum_{n=0}^{N- 1}\sum_{m=0}^{M_{n}-1}\epsilon_{n,m}\left\|\bm{s}_{t_{n}+\tau_{n,m}}^{\theta} \big{(}\widetilde{\bm{x}}_{t_{n}+\tau}(\omega)\big{)}-\nabla\log\tilde{p}_{t_{ n}+\tau_{n,m}}\big{(}\widetilde{\bm{x}}_{t_{n}+\tau}(\omega)\big{)}\right\|^{2} \right]\leq\delta_{2}^{2},\]

where the last equality is because the process \(\widehat{\bm{y}}_{t_{n},\tau}^{(K)}(\omega)\) under measure \(\tilde{p}\) follows the backward SDE (B.5).

Thus, by Theorem A.1 and plugging in the iteration relations above

\[D_{\mathrm{KL}}(p_{\eta}\|\widehat{\bm{q}}_{t_{N}})=D_{\mathrm{ KL}}(\tilde{p}_{t_{N}}\|\widehat{\bm{q}}_{t_{N}})\] \[\leq D_{\mathrm{KL}}(\tilde{p}_{0}\|\widehat{\bm{q}}_{0})+\sum_{n=0} ^{N-1}\left(\mathbb{E}_{\omega\sim\tilde{p}|_{Y_{t_{n}}}}\left[A_{t_{n}}( \omega)+B_{t_{n}}(\omega)\right]+e^{-K}h_{n}^{2}e^{h_{n}}d\right)\] \[\leq D_{\mathrm{KL}}(\tilde{p}_{0}\|\widehat{\bm{q}}_{0})+\sum_{n=0} ^{N-2}\epsilon dh_{n}+\epsilon d\log\eta^{-1}+\sum_{n=0}^{N-1}\mathbb{E}_{ \omega\sim\tilde{p}|_{Y_{t_{n}}}}[B_{t_{n}}(\omega)]+e^{-K}h_{n}^{2}e^{h_{n}}dN\] \[\leq de^{-T}+\epsilon d(T+\log\eta^{-1})+\delta_{2}^{2}+e^{-K}dT\leq de ^{-T}+\epsilon dT+\delta^{2}+e^{-K}dT,\]

as \(T\gtrsim\log\eta^{-1}\), \(h_{n}\lesssim 1\), and \(\delta_{2}\lesssim\delta\), and then it is straightforward to see that the following choices of parameters

\[T=\mathcal{O}(\log(d\delta^{-2})),\quad h=\Theta(1),\quad N= \mathcal{O}\left(\log(d\delta^{-2})\right),\] \[\epsilon=\Theta\left(d^{-1}\delta^{2}\log^{-1}(d\delta^{-2}) \right),\quad M=\mathcal{O}\left(d\delta^{-2}\log(d\delta^{-2})\right),\] \[K=\widetilde{\mathcal{O}}(\log(d\delta^{-2})),\]

would yield an overall error of \(\mathcal{O}(\delta^{2})\). 

## Appendix C Details of Probability Flow ODE Implementation

In this section, we provide the details of the parallelized algorithm for the probability flow ODE formulation of diffusion models. We first introduce the algorithm and define the necessary notations, then discuss the error analysis during the predictor and corrector steps, respectively, and finally provide the proof of Theorem 3.5.

### Algorithm

In the parallelized inference algorithm for diffusion models in the probability flow ODE formulation, we adopt the same discretization scheme as in Section 3.1.1 and the exponential integrator for all updating rules. For each block, we first run a _predictor step_, which consists of running the probability flow ODE in parallel. Then we run a _corrector step_, which runs an underdamped Langevin dynamics in parallel to correct the distribution of the samples. The algorithm is summarized In Algorithm 2.

Parallelized Predictor StepThe parallelization strategies in the predictor step are similar to those in the SDE algorithm (Algorithm 1). The only difference here is that instead of applying Picard iteration to the backward SDE as in (3.2), we apply Picard iteration to the probability flow ODE as in (C.3), which does not require i.i.d. samples from standard Gaussian distribution. As shown in Lemma C.3, the update rule in the predictor step (C.1) in Algorithm 2 is equivalent to running ```
0:\(\widehat{\bm{y}}_{0}\sim\widehat{q}_{0}=\mathcal{N}(0,\bm{I}_{d})\), a discretization scheme (\(T\), \((h_{n})_{n=1}^{N}\) and \((\tau_{n,m})_{n\in[1:N],m\in[0:M_{n}]}\)) satisfying (3.1), parameters for the corrector step (\(T^{\dagger}\),\(N^{\dagger}\), \(h^{\dagger}\), \(M^{\dagger}\), \(\epsilon^{\dagger}\)), the depth of iteration \(K\) and \(K^{\dagger}\), the learned NN-based score \(\bm{s}_{t}^{\theta}(\cdot)\).
0: A sample \(\widehat{\bm{y}}_{T}\sim\widehat{q}_{T}\approx\widehat{p}_{T}\).
1for\(n=0\)to\(N-1\)do
2\(\triangleright\)PredictorStep (Section C.2)
3\(\widehat{\bm{y}}_{t_{n},\tau_{n},\tau_{n}}^{(0)}\leftarrow\widehat{\bm{y}}_{t_ {n}}\) for \(m\in[0:M_{n}]\);
4for\(k=1\)to\(K\)do
5\(\widehat{\bm{y}}_{t_{n},0}^{(k)}\leftarrow\widehat{\bm{y}}_{t_{n}}\);
6for\(m=1\)to\(M_{n}\)in paralleldo
7\(\widehat{\bm{y}}_{t_{n},\tau_{n},\tau_{n}}^{(k)}\leftarrow\)\(\frac{1}{2}e^{\frac{\tau_{n}m}{2}}\widehat{\bm{y}}_{t_{n},0}^{(k-1)}\) \[+\frac{1}{2}\sum_{j=0}^{m-1}e^{\frac{\tau_{n,m}-\tau_{n,j+1}}{2}} \left(e^{\epsilon_{n,j}}-1\right)\bm{s}_{t_{n}+\tau_{n,j}}^{\theta}(\widehat{ \bm{y}}_{t_{n},\tau_{n,j}}^{(k-1)})\]
8
9 end for
10\(\triangleright\)CorrectorStep (Section C.3)
11\(\widehat{\bm{u}}_{t_{n},0}^{(0)}\leftarrow\widehat{\bm{y}}_{t_{n},h_{n}}^{(K)}\) and \(\widehat{\bm{v}}_{t_{n},0}^{(0)}\sim\mathcal{N}(0,\bm{I}_{d})\);
12for\(n^{\dagger}=0\)to\(N^{\dagger}-1\)do
13\((\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},m^{\epsilon\dagger}}^{(0)}, \widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},m^{\epsilon\dagger}}^{(0)}) \leftarrow(\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger}},\widehat{\bm{v}}_{ t_{n},n^{\dagger}h^{\dagger}})\) for \(m^{\dagger}\in[0:M^{\dagger}]\);
14for\(k^{\dagger}=1\)to\(K\)do
15\((\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},0}^{(k^{\dagger})},\widehat{ \bm{v}}_{t_{n},n^{\dagger}h^{\dagger},0}^{(k^{\dagger})})\leftarrow(\widehat{ \bm{u}}_{t_{n},n^{\dagger}h^{\dagger}},\widehat{\bm{v}}_{t_{n},n^{\dagger}h^{ \dagger}})\);
16\(\bm{\xi}_{j^{\dagger}}\sim\mathcal{N}\left(0,2\gamma(1+\gamma^{-2})(1-e^{-\gamma \epsilon^{\dagger}})^{2}e^{-2\gamma((M^{\dagger}-j^{\dagger}+1)\epsilon^{ \dagger})}\bm{I}_{d}\right)\) for \(j^{\dagger}\in[0:M^{\dagger}]\);
17for\(m^{\dagger}=1\)to\(M^{\dagger}\)in paralleldo
18\(\begin{bmatrix}\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},m^{\epsilon \dagger}}^{(k^{\dagger})}\\ \widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},m^{\epsilon\dagger}}^{(k^{ \dagger})}\end{bmatrix}\leftarrow\bm{G}(m^{\dagger}\epsilon^{\dagger})\begin{bmatrix} \widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},0}^{(k^{\dagger}-1)}\\ \widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},0}^{(k^{\dagger}-1)}\\ \widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},0}^{(k^{\dagger}-1)}\end{bmatrix}\) \[+\sum_{j^{\dagger}=0}^{m-1}\bm{G}((m^{\dagger}-j^{\dagger}-1) \epsilon^{\dagger})\left(\bm{I}_{d}-\bm{G}(\epsilon^{\dagger})\right)\begin{bmatrix} \bm{0}\\ \bm{s}_{t_{n+1}}^{\theta}(\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},j^{ \dagger}\epsilon^{\dagger}}^{(k^{\dagger}-1)})\end{bmatrix}\] (C.2)
19\(+\sum_{j^{\dagger}=0}^{m-1}\bm{G}((m^{\dagger}-j^{\dagger}-1) \epsilon^{\dagger})\begin{bmatrix}\bm{0}\\ \bm{\xi}_{j^{\dagger}}\end{bmatrix}\);
20
21 end for
22
23 end for
24\((\widehat{\bm{u}}_{t_{n},(n^{\dagger}+1)h^{\dagger}},\widehat{\bm{v}}_{t_{n},(n ^{\dagger}+1)h^{\dagger}})\leftarrow(\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{ \dagger},h^{\dagger}}^{(K^{\dagger})},\widehat{\bm{v}}_{t_{n},n^{\dagger}h^{ \dagger},h^{\dagger}}^{(K^{\dagger})})\);
25
26 end for
27\(\widehat{\bm{y}}_{t_{n+1}}\leftarrow\widehat{\bm{u}}_{t_{n},T^{\dagger}}\);
28

[MISSING_PAGE_POST]

1841
1851
186
187
1888
1999
1900
1917
1989
2000
219
21901
2203
22191
22204
223
2334
2435
2467
2478
2489
259
2601
2719
2828
2829
2930
2940
2951
2962
2972
29841
2985
29763

299864

300641
30657
30767
30788
30888
3088

309989
4099
5099
6099
7099

610000

62909

700000

631000

6409100

7000000

6410000

6510000

665100
661000

670000
67100
67100
681000

690000
69100
61000
the auxiliary predictor process (C.3). The auxiliary predictor process takes in the result from the previous corrector step (or the initialization if \(n=0\)) and outputs \(\widehat{\bm{y}}_{t_{n},h_{n}}^{(K)}\) as the initialization for the next corrector step.

Parallelized Corrector StepThe parallelization of the underdamped Langevin dynamics is similar to that mentioned in Section 2.2. Given a sample resulting from the predictor step, we initialize the auxiliary corrector process (Definition C.8) which is an underdamped Langevin dynamics with the initialization \(\widehat{\bm{u}}_{t_{n},0}=\bm{y}_{t_{n},h_{n}}^{(K)}\) and the augmented variable \(\widehat{\bm{v}}_{t_{n},0}\sim\mathcal{N}(0,\bm{I}_{d})\) representing the momentum.

We run the underdamped Langevin dynamics for time \(T^{\dagger}\), which is set to be of order \(\Omega(1)\) so that it is large enough to correct the distribution of the samples (_cf._ Lemma C.18) while being comparably short to ensure numerical stability (_cf._ Theorem C.17). Following a similar strategy as in Section 2.2 and in Algorithm 1, we further divide the time horizon \(T^{\dagger}\) into \(N^{\dagger}\) blocks with step size \(h^{\dagger}\), and for each block the block length \(h^{\dagger}\) into \(M^{\dagger}\) steps with step size \(\epsilon^{\dagger}\). Within each block, we run the underdamped Langevin dynamics in parallel for \(K^{\dagger}\) iterations. As shown in Lemma C.9, the update rule in the corrector step (C.2) in Algorithm 2 is equivalent to running the auxiliary corrector process (C.11).

In the following subsections, we proceed to provide theoretical guarantees for the algorithm.

### Parallelized Predictor Step

**Definition C.1** (Auxiliary Predictor Process).: _For any \(n\in[0:N-1]\), we define the auxiliary predictor process \((\widehat{\bm{y}}_{t_{n},\tau}^{(k)})_{\tau\in[0,h_{n}]}\) as the solution to the following ODE recursively for \(k\in[0:K-1]\):_

\[\mathrm{d}\widehat{\bm{y}}_{t_{n},\tau}^{(k+1)}=\Bigg{[}\frac{1}{2}\widehat{ \bm{y}}_{t_{n},\tau}^{(k+1)}+\frac{1}{2}\bm{s}_{t_{n}+g_{n}(\tau)}^{\theta} \Big{(}\widehat{\bm{y}}_{t_{n},g_{n}(\tau)}^{(k)}\Big{)}\Bigg{]}\mathrm{d}\tau,\] (C.3)

_with the initial condition_

\[\widehat{\bm{y}}_{t_{n},\tau}^{(0)}\equiv\widehat{\bm{y}}_{t_{n}} \text{ for }\tau\in[0,h_{n}],\quad\text{and}\quad\bm{y}_{t_{n},0}^{(k)}\equiv\widehat{ \bm{y}}_{t_{n}}\text{ for }k\in[1:K]\] (C.4)

_where \(\widehat{\bm{y}}_{t_{n}}=\widehat{\bm{u}}_{t_{n-1},N^{\dagger}h^{\dagger}}\) if \(n\in[1:N-1]\) and \(\widehat{\bm{y}}_{t_{0}}\sim\mathcal{N}(0,\bm{I}_{d})\). We will also denote the probability distribution of \(\widehat{\bm{y}}_{t_{n},\tau}^{(K)}\) as \(\widehat{q}_{t_{n},\tau}\)._

Figure 2: Illustration of the proof pipeline of Theorem 3.5 for PIADM-ODE within the \(n\)-th block.

**Definition C.2** (Interpolating Process).: _For any \(n\in[0:N-1]\), we define the interpolating process \((\widetilde{\boldsymbol{y}}^{(k)}_{t_{n},\tau})_{\tau\in[0,h_{n}]}\) as the solution to the following ODE recursively for \(k\in[0:K-1]\):_

\[\mathrm{d}\widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}=\Bigg{[}\frac{1}{2} \widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}+\frac{1}{2}\boldsymbol{s}^{ \theta}_{t_{n}+g_{n}(\tau)}\Big{(}\widetilde{\boldsymbol{y}}^{(k)}_{t_{n},g_{ n}(\tau)}\Big{)}\Bigg{]}\mathrm{d}\tau,\] (C.5)

_with initial condition_

\[\widetilde{\boldsymbol{y}}^{(0)}_{t_{n},\tau}\equiv\widetilde{\boldsymbol{y}} ^{(0)}_{t_{n},0}\quad\text{for }\tau\in[0,h_{n}],\text{ and }\widetilde{\boldsymbol{y}}^{(k)}_{t_{n},0}\equiv \widetilde{\boldsymbol{y}}^{(0)}_{t_{n},0}\quad\text{for }k\in[1:K],\]

_where \(\widetilde{\boldsymbol{y}}^{(0)}_{t_{n},0}\sim\widetilde{p}_{t_{n}}\). We will also denote the probability distribution of \(\widetilde{\boldsymbol{y}}^{(K)}_{t_{n},\tau}\) as \(\widetilde{q}_{t_{n},\tau}\)._

Similar to the equivalence between (3.4) and (B.2), we have the following lemma:

**Lemma C.3** (Equivalence between (C.1) and (C.3)).: _For any \(n\in[0:N-1]\), the update rule (C.1) in Algorithm 2 is equivalent to the exact solution of (C.3) for any \(k\in[0:K-1]\) and \(\tau\in[0,h_{n}]\)._

Proof.: Rewriting (C.3) and multiplying \(e^{-\frac{\tau}{2}}\) on both sides yield

\[\mathrm{d}\left[e^{-\frac{\tau}{2}}\widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n}, \tau}\right]=e^{-\frac{\tau}{2}}\left[\mathrm{d}\widetilde{\boldsymbol{y}}^{( k+1)}_{t_{n},\tau}-\frac{1}{2}\widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n}, \tau}\mathrm{d}\tau\right]=\frac{e^{-\frac{\tau}{2}}}{2}\boldsymbol{s}^{ \theta}_{t_{n}+g_{n}(\tau)}\Big{(}\widetilde{\boldsymbol{y}}^{(k)}_{t_{n},g_ {n}(\tau)}\Big{)}\mathrm{d}\tau\]

Integrating on both sides from \(0\) to \(\tau\) implies

\[e^{-\frac{\tau}{2}}\widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n}, \tau}-\widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n},0}=\int_{0}^{\tau}\frac{e^{- \frac{\tau}{2}}}{2}\boldsymbol{s}^{\theta}_{t_{n}+g_{n}(\tau^{\prime})}\Big{(} \widetilde{\boldsymbol{y}}^{(k)}_{t_{n},g_{n}(\tau^{\prime})}\Big{)}\mathrm{d} \tau^{\prime}\] \[= \frac{1}{2}\sum_{m=0}^{M_{n}}\int_{\tau\wedge t_{n},m}^{\tau\wedge \tau_{n,m+1}}e^{-\frac{\tau^{\prime}}{2}}\boldsymbol{s}^{\theta}_{t_{n}+\tau_ {n,m}}\Big{(}\boldsymbol{y}^{(k)}_{t_{n},\tau_{n,m}}\Big{)}\mathrm{d}\tau^{\prime}\] \[= \sum_{m=0}^{M_{n}}\Big{(}e^{-\frac{\tau\wedge\tau_{n,m}}{2}}-e^{- \frac{\tau\wedge\tau_{n,m+1}}{2}}\Big{)}\,\boldsymbol{s}^{\theta}_{t_{n}+\tau_ {n,m}}\Big{(}\widetilde{\boldsymbol{y}}^{(k)}_{t_{n},\tau_{n,m}}\Big{)},\]

and then multiplying \(e^{\frac{\tau}{2}}\) on both sides above yields

\[\widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}=e^{\frac{\tau}{2}}\widetilde{ \boldsymbol{y}}^{(k+1)}_{t_{n},0}+\sum_{m=0}^{M_{n}}\Big{(}e^{\frac{\tau\wedge \tau_{n,m}+1-\tau\wedge\tau_{n,m}}{2}}-1\Big{)}\,e^{\frac{0\vee(\tau-\tau_{n,m+1})}{2}}\boldsymbol{s}^{\theta}_{t_{n}+\tau_{n,m}}\Big{(}\widetilde{ \boldsymbol{y}}^{(k)}_{t_{n},\tau_{n,m}}\Big{)}.\]

Plugging in \(\tau=\tau_{n,m}\) gives us (C.1), as desired. 

**Lemma C.4** (Error between the interpolating process and the true process).: _Under the Picard iteration, we have that the ending process \(\{\widetilde{\boldsymbol{y}}^{(K)}_{t_{n},\tau}\}_{\tau\in[0,h_{n}]}\) satisfies the following exponential convergence rate_

\[\sup_{\tau\in[0,h_{n}]}\mathbb{E}\left[\left\|\widetilde{\boldsymbol{y}}^{(K)}_ {t_{n},\tau}-\widetilde{\boldsymbol{x}}_{t_{n}+\tau}\right\|^{2}\right]\leq 3d \left(\frac{h_{n}^{2}e^{h_{n}+\frac{3}{2}}L_{\boldsymbol{s}}^{2}}{2}\right)^{K} +\frac{e^{h_{n}+\frac{3}{2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\frac{3}{2}}L_{ \boldsymbol{s}}^{2}/2}\Big{(}h_{n}\delta_{\infty}^{2}+\mathbb{E}[D_{t_{n}}] \Big{)},\]

_where_

\[D_{t_{n}}:=\int_{0}^{h_{n}}\left\|\boldsymbol{s}^{\theta}_{t_{n}+g_{n}(\tau^{ \prime})}\big{(}\widetilde{\boldsymbol{x}}_{t_{n}+g_{n}(\tau^{\prime})}\big{)} -\boldsymbol{s}^{\theta}_{t_{n}+\tau^{\prime}}\big{(}\widetilde{\boldsymbol{x} }_{t_{n}+\tau^{\prime}}\big{)}\right\|^{2}\mathrm{d}\tau^{\prime}.\]

Proof.: Recall that the backward true process \(\{\widetilde{\boldsymbol{x}}_{t_{n}+\tau}\}_{\tau\in[0,h_{n}]}\) satisfies the following backward SDE within one block

\[\mathrm{d}\widetilde{\boldsymbol{x}}_{t_{n}+\tau}=\left[\frac{1}{2} \widetilde{\boldsymbol{x}}_{t_{n}+\tau}+\frac{1}{2}\nabla\log\widetilde{p}_{t_ {n}+\tau}(\widetilde{\boldsymbol{x}}_{t_{n}+\tau})\right]\mathrm{d}\tau.\] (C.6)

By subtracting (C.6) from (C.5), we obtain that

\[\frac{\mathrm{d}}{\mathrm{d}\tau}\left(\widetilde{\boldsymbol{y}} ^{(k+1)}_{t_{n},\tau}-\widetilde{\boldsymbol{x}}_{t_{n}+\tau}\right) =\frac{1}{2}\left[\widetilde{\boldsymbol{y}}^{(k+1)}_{t_{n},\tau}- \widetilde{\boldsymbol{x}}_{t_{n}+\tau}\right]\] (C.7) \[+\frac{1}{2}\left[\boldsymbol{s}^{\theta}_{t_{n}+g_{n}(\tau)} \big{(}\widetilde{\boldsymbol{y}}^{(k)}_{t_{n},g_{n}(\tau)}\big{)}- \boldsymbol{s}^{\theta}_{t_{n}+g_{n}(\tau)}(\widetilde{\boldsymbol{x}}_{t_{n}+g _{n}(\tau)})\right]\] \[+\frac{1}{2}\left[\boldsymbol{s}^{\theta}_{t_{n}+g_{n}(\tau)}( \widetilde{\boldsymbol{x}}_{t_{n}+g_{n}(\tau)})-\nabla\log\widetilde{p}_{t_{n}+g _{n}(\tau)}(\widetilde{\boldsymbol{x}}_{t_{n}+g_{n}(\tau)})\right]\] \[+\frac{1}{2}\left[\nabla\log\widetilde{p}_{t_{n}+g_{n}(\tau)}( \widetilde{\boldsymbol{x}}_{t_{n}+g_{n}(\tau)})-\nabla\log\widetilde{p}_{t_{n}+ \tau}(\widetilde{\boldsymbol{x}}_{t_{n}+\tau})\right].\]Then by

\[\mathrm{d}\left\|\widetilde{\bm{y}}_{t_{n},\tau^{\prime}}^{(k+1)}-\widetilde{\bm{ x}}_{t_{n}+\tau^{\prime}}\right\|^{2}=2\left(\widetilde{\bm{y}}_{t_{n},\tau^{ \prime}}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right)^{\top}\mathrm{d }\left(\widetilde{\bm{y}}_{t_{n},\tau^{\prime}}^{(k+1)}-\widetilde{\bm{x}}_{t_ {n}+\tau^{\prime}}\right),\]

and integrating for \(\tau^{\prime}\in[0,h_{n}]\), we have

\[\begin{split}&\left\|\widetilde{\bm{y}}_{t_{n},\tau}^{(k+1)}- \widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2}\\ =&\int_{0}^{\tau}\left(\widetilde{\bm{y}}_{t_{n}, \tau^{\prime}}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right)^{\top} \left(\bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{\theta}\big{(}\widetilde{\bm{y}}_{t _{n},g_{n}(\tau^{\prime})}^{(k)}\big{)}-\bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{ \theta}(\widetilde{\bm{x}}_{t_{n}+g_{n}(\tau^{\prime})})\right)\mathrm{d}\tau^ {\prime}\\ +&\int_{0}^{\tau}\left(\widetilde{\bm{y}}_{t_{n}, \tau^{\prime}}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right)^{\top} \left(\delta_{t_{n}+g_{n}(\tau^{\prime})}^{\theta}(\widetilde{\bm{x}}_{t_{n}+g_ {n}(\tau^{\prime})})-\nabla\log\widetilde{p}_{t_{n}+g_{n}(\tau^{\prime})}( \widetilde{\bm{x}}_{t_{n}+g_{n}(\tau^{\prime})})\right)\mathrm{d}\tau^{\prime }\\ +&\int_{0}^{\tau}\left(\widetilde{\bm{y}}_{t_{n}, \tau^{\prime}}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right)^{\top} \left(\nabla\log\widetilde{p}_{t_{n}+g_{n}(\tau^{\prime})}(\widetilde{\bm{x}}_ {t_{n}+g_{n}(\tau^{\prime})})-\nabla\log\widetilde{p}_{t_{n}+\tau^{\prime}}( \widetilde{\bm{x}}_{t_{n}+r^{\prime}})\right)\mathrm{d}\tau^{\prime}\\ +&\int_{0}^{\tau}\left\|\widetilde{\bm{y}}_{t_{n}, \tau^{\prime}}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right\|^{2} \mathrm{d}\tau^{\prime}.\end{split}\]

Using AM-GM inequality and taking expectations on both sides, we further upper bound the summation above as

\[\begin{split}&\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n}, \tau}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2}\right]\\ \leq&\left(1+\frac{3}{2h_{n}}\right)\int_{0}^{\tau} \mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n},\tau^{\prime}}^{(k+1)}- \widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right\|^{2}\right]\mathrm{d}\tau^{ \prime}\\ +&\frac{h_{n}}{2}\int_{0}^{\tau}\mathbb{E}\left[\left\| \bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{\theta}\big{(}\widetilde{\bm{y}}_{t_{n},g _{n}(\tau^{\prime})}^{(k)}\big{)}-\bm{s}_{t_{n}+g_{n}(\tau^{\prime})}^{\theta }\big{(}\widetilde{\bm{x}}_{t_{n}+g_{n}(\tau^{\prime})}\big{)}\right\|^{2} \right]\mathrm{d}\tau^{\prime}\\ +&\frac{h_{n}}{2}\mathbb{E}\bigg{[}\underbrace{ \int_{0}^{\tau}\left\|\nabla\log\widetilde{p}_{t_{n}+g_{n}(\tau^{\prime})}( \widetilde{\bm{x}}_{t_{n}+g_{n}(\tau^{\prime})})-\nabla\log\widetilde{p}_{t_{ n}+\tau^{\prime}}(\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}})\right\|^{2} \mathrm{d}\tau^{\prime}}_{\leq D_{t_{n}}}\bigg{]}\\ \leq&\left(1+\frac{3}{2h_{n}}\right)\int_{0}^{\tau} \mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n},\tau^{\prime}}^{(k+1)}- \widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right\|^{2}\right]\mathrm{d}\tau^{ \prime}+\frac{h_{n}}{2}\left(\tau\delta_{\infty}^{2}+\mathbb{E}\left[D_{t_{n} }\right]\right)\\ +&\frac{L_{\bm{s}}^{2}h_{n}}{2}\int_{0}^{\tau} \mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n},g_{n}(\tau^{\prime})}^{(k)}- \widetilde{\bm{x}}_{t_{n}+g_{n}(\tau^{\prime})}\right\|^{2}\right]\mathrm{d} \tau^{\prime},\end{split}\]

where the last equality is by Assumption 3.1'.

Applying Gronwall's inequality, we have

\[\begin{split}&\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n}, \tau}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2}\right]\\ \leq&\frac{e^{\left(1+\frac{3}{2h_{n}}\right)\tau}L_{ \bm{s}}^{2}h_{n}}{2}\int_{0}^{\tau}\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t _{n},g_{n}(\tau^{\prime})}^{(k)}-\widetilde{\bm{x}}_{t_{n}+g_{n}(\tau^{\prime})} \right\|^{2}\right]\mathrm{d}\tau^{\prime}+\frac{e^{\left(1+\frac{3}{2h_{n}} \right)\tau}h_{n}}{2}\Big{(}\tau\delta_{\infty}^{2}+\mathbb{E}[D_{t_{n}}] \Big{)}\\ \leq&\frac{\tau e^{\left(1+\frac{3}{2h_{n}}\right)\tau} L_{\bm{s}}^{2}h_{n}}{2}\sup_{\tau^{\prime}\in[0,\tau]}\mathbb{E}\left[\left\| \widetilde{\bm{y}}_{t_{n},\tau^{\prime}}^{(k)}-\widetilde{\bm{x}}_{t_{n}+\tau^{ \prime}}\right\|^{2}\right]+\frac{e^{\left(1+\frac{3}{2h_{n}}\right)\tau}h_{n}}{ 2}\Big{(}\tau\delta_{\infty}^{2}+\mathbb{E}[D_{t_{n}}]\Big{)},\end{split}\] (C.8)

and by taking supremum

\[\begin{split}&\sup_{\tau\in[0,h_{n}]}\mathbb{E}\left[\left\| \widetilde{\bm{y}}_{t_{n},\tau}^{(k+1)}-\widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2} \right]\\ \leq&\frac{h_{n}^{2}e^{h_{n}+\frac{3}{2}}L_{\bm{s}}^{2} }{2}\sup_{\tau^{\prime}\in[0,\tau]}\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_ {n},\tau^{\prime}}^{(k)}-\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}}\right\|^{2} \right]+\frac{e^{h_{n}+\frac{3}{2}}h_{n}}{2}\Big{(}h_{n}\delta_{\infty}^{2}+ \mathbb{E}[D_{t_{n}}]\Big{)}\end{split}\] (C.9)Given that constant \(h_{n}\) is sufficiently small, which ensures \(L_{\bm{s}}^{2}h_{n}e^{\frac{5}{2}h_{n}}\ll 1\), iterating the above inequality for \(k\in[0:K-1]\) gives us that

\[\sup_{\tau\in[0,h_{n}]}\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t _{n},\tau}^{(K)}-\widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2}\right]\] \[\leq \left(\frac{h_{n}^{2}e^{h_{n}+\frac{3}{2}}L_{\bm{s}}^{2}}{2} \right)^{K}\sup_{\tau\in[0,h_{n}]}\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t _{n},\tau}^{(0)}-\widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2}\right]+\frac{e^{h _{n}+\frac{3}{2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\frac{3}{2}}L_{\bm{s}}^{2}/2} \Big{(}h_{n}\delta_{\infty}^{2}+\mathbb{E}[D_{t_{n}}]\Big{)},\]

Notice that by Lemma A.8, we have

\[\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n},\tau}^{(0)}-\widetilde{\bm{ x}}_{t_{n}+\tau}\right\|^{2}\right]=\mathbb{E}\left[\left\|\widetilde{\bm{x}}_{t _{n}}-\widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2}\right]\leq 3d,\]

substituting which into (C.9) then gives us that

\[\sup_{\tau\in[0,h_{n}]}\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n},\tau} ^{(K)}-\widetilde{\bm{x}}_{t_{n}+\tau}\right\|^{2}\right]\leq 3d\left(\frac{h_{n}^ {2}e^{h_{n}+\frac{3}{2}}L_{\bm{s}}^{2}}{2}\right)^{K}+\frac{e^{h_{n}+\frac{3} {2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\frac{3}{2}}L_{\bm{s}}^{2}/2}\Big{(}h_{n} \delta_{\infty}^{2}+\mathbb{E}[D_{t_{n}}]\Big{)},\]

as desired. 

Now it remains to bound \(C_{t_{n}}\) and \(D_{t_{n}}\) in Lemma C.4. We first bound \(D_{t_{n}}\) using the following lemma:

**Lemma C.5**.: _For any \(n\in[0:N-1]\), we have that_

\[\mathbb{E}\left[D_{t_{n}}\right]\lesssim d\epsilon^{2}h_{n}.\]

Proof.: For any \(n\in[0:N-2]\), we have \(T-t_{n+1}\gtrsim\mathcal{O}(1)\) and thus by [111, Corollary 1] that

\[\mathbb{E}\left[\left\|\nabla\log\widetilde{p}_{t_{N-1}+\tau_{n,m}}( \widetilde{\bm{x}}_{t_{N-1}+\tau_{n,m}})-\nabla\log\widetilde{p}_{t_{N-1}+\tau ^{\prime}}(\widetilde{\bm{x}}_{t_{N-1}+\tau^{\prime}})\right\|^{2}\right] \lesssim d\epsilon_{n,m}^{2},\]

for any \(\tau^{\prime}\in[\tau_{n,m},\tau_{n,m+1}]\), and thus

\[\mathbb{E}\left[D_{t_{n}}\right]=\int_{0}^{h_{n}}\mathbb{E}\left[ \left\|\nabla\log\widetilde{p}_{t_{n}+g_{n}(\tau^{\prime})}(\widetilde{\bm{x }}_{t_{n}+g_{n}(\tau^{\prime})})-\nabla\log\widetilde{p}_{t_{n}+\tau^{\prime} }(\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}})\right\|^{2}\right]\mathrm{d}\tau^ {\prime}\] \[=\sum_{m=0}^{M_{n}}\int_{\tau_{n,m}}^{\tau_{n,m+1}}\mathbb{E} \left[\left\|\nabla\log\widetilde{p}_{t_{n}+\tau_{n,m}}(\widetilde{\bm{x}}_{t _{n}+\tau_{n,m}})-\nabla\log\widetilde{p}_{t_{n}+\tau^{\prime}}(\widetilde{\bm {x}}_{t_{n}+\tau^{\prime}})\right\|^{2}\right]\mathrm{d}\tau^{\prime}\] \[\lesssim\sum_{m=0}^{M_{n}}d\epsilon_{n,m}^{2}\epsilon_{n,m} \leq d\epsilon^{2}h_{n}.\]

For \(n=N-1\), notice that by the step size schedule (_cf._ Section 3.1.1) and suppose \(\epsilon\leq 1/2\), we have

\[\frac{T-\tau}{2}\leq T-g_{n}(\tau)\leq T-\tau,\]

and then again [111, Corollary 1] states

\[\mathbb{E}\left[\left\|\nabla\log\widetilde{p}_{t_{n}+\epsilon_{n,m}}( \widetilde{\bm{x}}_{t_{n}+\epsilon_{n,m}})-\nabla\log\widetilde{p}_{t_{n}+ \tau^{\prime}}(\widetilde{\bm{x}}_{t_{n}+\tau^{\prime}})\right\|^{2}\right] \lesssim\frac{d\epsilon_{n,m}^{2}}{T-\tau_{n,m}},\]

and thus

\[\mathbb{E}\left[D_{t_{N-1}}\right]=\int_{0}^{h_{N-1}}\mathbb{E} \left[\left\|\nabla\log\widetilde{p}_{t_{N-1}+g_{n}(\tau^{\prime})}( \widetilde{\bm{x}}_{t_{N-1}+g_{n}(\tau^{\prime})})-\nabla\log\widetilde{p}_{t_{ N-1}+\tau^{\prime}}(\widetilde{\bm{x}}_{t_{N-1}+\tau^{\prime}})\right\|^{2} \right]\mathrm{d}\tau\] \[=\sum_{m=0}^{M_{N-1}}\int_{\tau_{n,m}}^{\tau_{n,m+1}}\mathbb{E} \left[\left\|\nabla\log\widetilde{p}_{t_{N-1}+\tau_{n,m}}(\widetilde{\bm{x}}_{t_{ N-1}+g_{n}(\tau^{\prime})})-\nabla\log\widetilde{p}_{t_{N-1}+\tau^{\prime}}( \widetilde{\bm{x}}_{t_{N-1}+\tau^{\prime}})\right\|^{2}\right]\mathrm{d}\tau^{\prime}\] \[\lesssim\sum_{m=0}^{M_{N-1}}\frac{d\epsilon_{n,m}^{2}}{T-\tau_{n,m }}\epsilon_{n,m}\leq\sum_{m=0}^{M_{N-1}}d\epsilon_{n,m}^{2}\epsilon\lesssim \int_{\delta_{\infty}}^{T-t_{N-1}}d\tau\mathrm{d}\tau\lesssim d\epsilon^{2}h_{N-1}.\]

**Remark C.6**.: _The above lemma is able to achieve a better dependency on \(\epsilon\) compared to Lemma B.7, because the backward process \((\bm{\bar{x}}_{t})_{t\in[0,T]}\) is now a deterministic process in the probability flow ODE formulation, instead of a stochastic process as in the SDE formulation as in Lemma B.7. Thus, intuitively applying Cauchy-Schwarz rather than Ito symmetry gives us a \(\mathcal{O}(\epsilon^{2})\)-dependency rather than \(\mathcal{O}(\epsilon)\)-dependency._

**Theorem C.7**.: _Under Assumptions 3.1', 3.2, 3.3, and 3.4, then the distribution \(\widetilde{q}_{t_{n},h_{n}}\) that the parallelized predictor step generates samples from satisfies the following error bound:_

\[W_{2}(\widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}})^{2}\lesssim de^{-K}+ h_{n}^{2}\delta_{\infty}^{2}+d\epsilon^{2}h_{n}^{2},\]

_for \(n\in[0:N-1]\)._

Proof.: By the definition of 2-Wasserstein distance, we have for any coupling of \(\widetilde{\bm{y}}_{t_{n},h_{n}}^{(K)}\) and \(\bm{\bar{x}}_{t_{n}+h_{n}}\),

\[W_{2}(\widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}})^{2}\leq\mathbb{E} \left[\left\|\widetilde{\bm{y}}_{t_{n},h_{n}}^{(K)}-\bm{\bar{x}}_{t_{n}+h_{n}} \right\|^{2}\right],\]

and therefore

\[W_{2}(\widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}})^{2} \leq\mathbb{E}\left[\left\|\widetilde{\bm{y}}_{t_{n},h_{n}}^{(K)}-\bm{\bar{x} }_{t_{n}+h_{n}}\right\|^{2}\right]\leq\sup_{\tau\in[0,h_{n}]}\mathbb{E}\left[ \left\|\widetilde{\bm{y}}_{t_{n},\tau}^{(K)}-\bm{\bar{x}}_{t_{n}+\tau}\right\| ^{2}\right]\] \[\leq 3d\left(\frac{h_{n}^{2}e^{h_{n}+\frac{3}{2}}L_{\bm{s}}^{2}}{2} \right)^{K}+\frac{e^{h_{n}+\frac{3}{2}}h_{n}/2}{1-h_{n}^{2}e^{h_{n}+\frac{3}{2 }}L_{\bm{s}}^{2}/2}\Big{(}h_{n}\delta_{\infty}^{2}+\mathbb{E}[D_{t_{n}}]\Big{)}\] \[\lesssim de^{-K}+h_{n}^{2}\delta_{\infty}^{2}+d\epsilon^{2}h_{n}^{2},\]

where for the second to last inequality we used Lemma C.4, the last inequality is due to Lemma C.5 and the assumption \(h_{n}^{2}e^{h_{n}}L_{\bm{s}}^{2}\ll 1\). 

### Parallelized Corrector Step

After each predictor step, we run the corrector step for \(\mathcal{O}(1)\) time to reduce the error. Particularly, we apply the Parallelized underdamped Langevin dynamics algorithm [130] to the corrector step, which yields \(\mathcal{O}(1)\) approximate time complexity compared to the ordinary implementation of the ULMC dynamics as in [111]. In the following, we will drop the dependency on \(\omega\) for notational simplicity, and we refer readers to Appendix A.2 and B.2 to review the change of measure arguments and the application of Girsanov's theorem A.4. We will also use a general notation \(*^{\dagger}\) to distinguish the time in the backward process and the inner time in the corrector step of the \(n\)-th block.

We first define the true underdamped Langevin dynamics \((\bm{u}_{t_{n},t^{\dagger}},\bm{v}_{t_{n},t^{\dagger}})_{t\geq 0}\):

\[\begin{cases}\mathrm{d}\bm{u}_{t_{n},t^{\dagger}}=\bm{v}_{t_{n},t^{\dagger}} \mathrm{d}t^{\dagger}\\ \mathrm{d}\bm{v}_{t_{n},t^{\dagger}}=-\gamma\bm{v}_{t_{n},t^{\dagger}}\mathrm{ d}t^{\dagger}-\nabla\log\widetilde{p}_{t_{n+1}}(\bm{u}_{t_{n},t^{\dagger}}) \mathrm{d}t^{\dagger}+\sqrt{2\gamma}\mathrm{d}\bm{b}_{t_{n},t^{\dagger}},\end{cases}\] (C.10)

with initial condition \(\bm{u}_{t_{n},0}\equiv\widetilde{\bm{y}}_{t_{n},h_{n}}^{(K^{\dagger})}\) from the predictor step and \(\bm{v}_{t_{n},0}\sim\mathcal{N}(0,\bm{I}_{d})\), where \((\bm{b}_{t_{n},t^{\dagger}})_{t\geq 0}\) is a Wiener process. We may also write the system of SDEs above in the following matrix form:

\[\mathrm{d}\begin{bmatrix}\bm{u}_{t_{n},t^{\dagger}}\\ \bm{v}_{t_{n},t^{\dagger}}\end{bmatrix}=\begin{bmatrix}\bm{0}&\bm{I}_{d}\\ \bm{0}&-\gamma\bm{I}_{d}\end{bmatrix}\begin{bmatrix}\bm{u}_{t_{n},t^{\dagger}} \\ \bm{v}_{t_{n},t^{\dagger}}\end{bmatrix}-\begin{bmatrix}\bm{0}&\bm{0}\\ \nabla\log\widetilde{p}_{t_{n+1}}(\bm{u}_{t_{n},t^{\dagger}})\end{bmatrix} \right]\mathrm{d}t^{\dagger}+\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{0}&\sqrt{2\gamma}\bm{I}_{d}\end{bmatrix}\mathrm{d}\begin{bmatrix}\bm{b}_{t_ {n},t^{\dagger}}^{\prime}\\ \bm{b}_{t_{n},t^{\dagger}}\end{bmatrix}.\]

We run this underdamped Langevin dynamics until the pre-determined time horizon \(T^{\dagger}\). We also define the joint probability distribution of \((\bm{u}_{t_{n},t^{\dagger}},\bm{v}_{t_{n},t^{\dagger}})\) at time \(t\) as \(\pi_{t_{n},t^{\dagger}}(\bm{u}_{t_{n},t^{\dagger}},\bm{v}_{t_{n},t^{\dagger}})\) and its marginal on \(\bm{u}_{t_{n},t^{\dagger}}\) as \(\pi_{t_{n},t^{\dagger}}^{\bm{u}}(\bm{u}_{t_{n},t^{\dagger}})\).

Similar to the parallelizing strategy in Section 3.1.1, we discretize the time interval \([0,T^{\dagger}]\) into \(N^{\dagger}\) blocks with length \(h^{\dagger}=T^{\dagger}/N^{\dagger}\). Within the \(n\)-th block, we further divide the block \([n^{\dagger}h^{\dagger},(n+1)h^{\dagger}]\) into \(M^{\dagger}\) steps, each with step size \(\epsilon^{\dagger}=h^{\dagger}/M^{\dagger}\).

**Definition C.8** (Auxiliary correct process).: _For any \(n^{\dagger}\in[0:N^{\dagger}-1]\), we define the auxiliary corrector process \((\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})})_ {\tau^{\dagger}\in[0,h^{\dagger}]}\) as the solution to the following SDE recursively for \(k^{\dagger}\in[0:K^{\dagger}-1]\):_

\[\begin{cases}\mathrm{d}\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(k^{\dagger})}=\widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^ {\dagger}}^{(k+1)}\mathrm{d}\tau^{\dagger},\\ \mathrm{d}\widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+ 1)}=-\widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)} \mathrm{d}\tau^{\dagger}-\bm{s}_{t_{n+1}}\big{(}\widehat{\bm{u}}_{t_{n},n^{ \dagger}h^{\dagger},g_{n}(\tau^{\dagger})}^{(k^{\dagger})}\big{)}\mathrm{d} \tau^{\dagger}+\sqrt{2\gamma}\mathrm{d}\bm{b}_{t_{n},n^{\dagger}h^{\dagger}+ \tau^{\dagger}}\end{cases}\] (C.11)

_with the initial condition_

\[\begin{cases}\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{ (0)}\equiv\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger}}&\text{for }\tau^{ \dagger}\in[0,h^{\dagger}],\text{ and }\begin{cases}\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}} ^{(k^{\dagger})}\equiv\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger}}&\text{ for }k\in[1:K^{\dagger}],\\ \widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})} \equiv\widehat{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger}}&\text{for }k\in[1:K^{\dagger}],\end{cases}\] (C.12)

_where_

\[\widehat{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger}}:=\widehat{\bm{u}}_{t_{n},(n^{ \dagger}-1)h^{\dagger},h^{\dagger}}^{(K^{\dagger})},\quad\widehat{\bm{v}}_{t _{n},n^{\dagger}h^{\dagger}}:=\widehat{\bm{v}}_{t_{n},(n^{\dagger}-1)h^{ \dagger},h^{\dagger}}^{(K^{\dagger})}\]

_for \(n^{\dagger}\in[1:N^{\dagger}-1]\), and_

\[\widehat{\bm{u}}_{t_{n},0}=\bm{y}_{t_{n},h_{n}}^{(K)},\quad\widehat{\bm{v}}_{ t_{n},0}\sim\mathcal{N}(0,\bm{I}_{d}).\]

_We define the joint probability distribution of \((\widehat{\bm{u}}_{t_{n},t^{\dagger}},\widehat{\bm{v}}_{t_{n},t^{\dagger}})\) at time \(t\) as \(\widehat{\pi}_{t_{n},t^{\dagger}}(\widehat{\bm{u}}_{t_{n},t^{\dagger}}, \widehat{\bm{v}}_{t_{n},t^{\dagger}})\) and its marginal on \(\widehat{\bm{u}}_{t_{n},t^{\dagger}}\) as \(\widehat{\pi}_{t_{n},t^{\dagger}}^{(\widehat{\bm{u}}}(\widehat{\bm{u}}_{t_{n},t^{\dagger}})\). We will also denote the resulting probability distribution of \(\widehat{\pi}_{t_{n},T^{\dagger}}^{\widetilde{\bm{u}}}\) as \(\widehat{q}_{t_{n+1}}\)._

**Lemma C.9** (Equivalence between (C.2) and (C.11)).: _For any \(n^{\dagger}\in[0:N^{\dagger}-1]\), the update rule in Algorithm 2 is equivalent to the exact solution of the auxiliary process (C.11) for any \(k^{\dagger}\in[0:K^{\dagger}-1]\) and \(\tau^{\dagger}\in[0,h^{\dagger}]\)._

Proof.: Without loss of generality, we will prove the lemma for \(m^{\dagger}=M^{\dagger}\). The proof for \(m^{\dagger}\in[0:M^{\dagger}-1]\) can be done similarly.

We first rewrite (C.2) into the matrix form:

\[\begin{split}\mathrm{d}\begin{bmatrix}\widetilde{\bm{u}}_{t_{n},n^ {\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})}\\ \widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger} )}\end{bmatrix}&=\begin{bmatrix}\bm{0}&\bm{I}_{d}\\ \bm{0}&-\gamma\bm{I}_{d}\end{bmatrix}\begin{bmatrix}\widetilde{\bm{u}}_{t_{n},n^ {\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})}\\ \widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger} )}\end{bmatrix}-\begin{bmatrix}\bm{0}\\ \bm{s}_{t_{n+1}}\big{(}\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},g_{n}( \tau^{\dagger})}^{(k^{\dagger})}\big{)}\end{bmatrix}\mathrm{d}\tau^{\dagger} \\ &\quad+\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{0}&\sqrt{2\gamma}\bm{I}_{d}\end{bmatrix}\mathrm{d}\begin{bmatrix}\bm{b}_{t_{n},n^{\prime}h^{\prime}h^{\prime}+\tau^{\dagger}}^{(k^{\prime})}\\ \bm{b}_{t_{n},n^{\prime}h^{\prime}+\tau^{\dagger}}\end{bmatrix}.\end{split}\] (C.13)

Define the time-dependent matrix \(\bm{G}(\cdot)\) as

\[\bm{G}(t^{\dagger}):=\begin{bmatrix}\bm{I}_{d}&\frac{1-e^{-\tau t^{\dagger}}}{ \gamma}\bm{I}_{d}\\ \bm{0}&e^{-\gamma t^{\dagger}}\bm{I}_{d}\end{bmatrix}=\exp\left(\begin{pmatrix} \bm{0}&\bm{I}_{d}\\ \bm{0}&-\gamma\bm{I}_{d}\end{pmatrix}t^{\dagger}\right),\] (C.14)

satisfying that

\[\frac{\mathrm{d}}{\mathrm{d}t^{\dagger}}\bm{G}(t^{\dagger})=\begin{bmatrix}\bm{0 }&\bm{I}_{d}\\ \bm{0}&-\gamma\bm{I}_{d}\end{bmatrix}\bm{G}(t^{\dagger})=\bm{G}(t^{\dagger}) \begin{bmatrix}\bm{0}&\bm{I}_{d}\\ \bm{0}&-\gamma\bm{I}_{d}\end{bmatrix}.\]

Now we multiply \(\bm{G}(-\tau^{\dagger})\) on both sides of (C.13) to obtain:

\[\begin{split}\mathrm{d}\begin{pmatrix}\bm{G}(-\tau^{\dagger}) \begin{bmatrix}\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger}h^{\dagger},\tau^ {\dagger}}^{(k^{\dagger})}\\ \widetilde{\bm{v}}_{t_{n},n^{\prime}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})} \end{bmatrix}\end{split}=&-\bm{G}(-\tau^{\dagger})\begin{bmatrix} \bm{0}\\ \bm{s}_{t_{n+1}}\big{(}\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},g_{n}(\tau^ {\dagger})}^{(k^{\dagger})}\big{)}\end{bmatrix}\mathrm{d}\tau^{\dagger}\\ &+\bm{G}(-\tau^{\dagger})\begin{bmatrix}\bm{0}&\bm{0}\\ \bm{0}&\sqrt{2\gamma}\bm{I}_{d}\end{bmatrix}\mathrm{d}\begin{bmatrix}\bm{b}_{t_{n},n^{\prime}h^{\dagger}+\tau^{\dagger}}^{(k^{\prime})}\\ \bm{b}_{t_{n},n^{\prime}h^{\dagger}+\tau^{\dagger}}\end{bmatrix}.\end{split}\]

[MISSING_PAGE_FAIL:38]

_We define the joint probability distribution of \((\widetilde{\bm{u}}_{t_{n},t^{\dagger}},\widetilde{\bm{v}}_{t_{n},t^{\dagger}})\) at time \(t\) as \(\widetilde{\pi}_{t_{n},t^{\dagger}}(\widetilde{\bm{u}}_{t_{n},t^{\dagger}}, \widetilde{\bm{v}}_{t_{n},t^{\dagger}})\) and its marginal on \(\widetilde{\bm{u}}_{t_{n},t^{\dagger}}\) as \(\widetilde{\pi}_{t_{n},t^{\dagger}}^{\widetilde{\bm{u}}}(\widetilde{\bm{u}}_{ t_{n},t^{\dagger}})\)._

We invoke Girsanov's theorem (Theorem A.4) again by the following procedure

1. Setting (A.2) as the auxiliary process (C.15) at iteration \(K^{\dagger}\), where \(\bm{b}_{t_{n},t^{\dagger}}(\omega)\) is a Wiener process under the measure \(Q\);
2. Defining another process \(\widetilde{\bm{b}}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}\) governed by the following SDE: \[\mathrm{d}\widetilde{\bm{b}}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}= \mathrm{d}\bm{b}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}-\bm{\phi}_{t_{ n},n^{\dagger}h^{\dagger}}(\tau^{\dagger})\mathrm{d}\tau^{\dagger},\] (C.17) where \[\bm{\phi}_{t_{n},n^{\dagger}h^{\dagger}}(\tau^{\dagger})=\frac{1}{\sqrt{2 \gamma}}\left(\bm{s}_{t_{n+1}}(\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger },\lfloor\frac{\tau^{\dagger}}{\epsilon^{\dagger}}\rfloor}^{(K^{\dagger}-1)} )-\nabla\log\widetilde{p}_{t_{n+1}}(\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{ \dagger},\tau^{\dagger}}^{(K^{\dagger})})\right)\] (C.18) and computing the Radon-Nikodym derivative of the measure \(P\) with respect to \(Q\) as \[\frac{\mathrm{d}P}{\mathrm{d}Q}=\exp\left(\int_{0}^{h^{\dagger}}\bm{\phi}_{t_{n },n^{\dagger}h^{\dagger}}(\tau^{\dagger})^{\top}\mathrm{d}\bm{b}_{t_{n},n^{ \dagger}h^{\dagger}+\tau^{\dagger}}-\frac{1}{2}\int_{0}^{h^{\dagger}}\|\bm{ \phi}_{nh}(\tau^{\dagger})\|^{2}\mathrm{d}\tau^{\dagger}\right);\] (C.19)
3. Concluding that (C.15) at iteration \(K^{\dagger}\) under the measure \(Q\) satisfies the following SDE: \[\begin{cases}\mathrm{d}\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(K^{\dagger})}=\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(K^{\dagger})}\mathrm{d}\tau^{\dagger}\\ \mathrm{d}\widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{( K^{\dagger})}=-\gamma\widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(K^{\dagger})}\mathrm{d}\tau^{\dagger}-\nabla\log\widetilde{p}_{t_ {n+1}}(\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger })})\mathrm{d}\tau^{\dagger}+\sqrt{2\gamma}\mathrm{d}\widetilde{\bm{b}}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}},\end{cases}\] (C.20) with \((\widetilde{\bm{b}}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}})_{\tau^{ \dagger}\geq 0}\) being a Wiener process under the measure \(P\). If we replace \((\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})}, \widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})})\) by \((\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}},\bm{v}_{t_{n},n^{\dagger }h^{\dagger}+\tau^{\dagger}})\), one should notice (C.20) is immediately the original backward SDE (C.10) with the true score function on \(t\in[n^{\dagger}h^{\dagger},(n+1)h^{\dagger}]\): \[\begin{cases}\mathrm{d}\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}= \bm{v}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}\mathrm{d}\tau^{\dagger}\\ \mathrm{d}\bm{v}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}=-\gamma\bm{v}_{t _{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}\mathrm{d}\tau^{\dagger}-\nabla\log \widetilde{p}_{t_{n+1}}(\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}) \mathrm{d}\tau^{\dagger}+\sqrt{2\gamma}\mathrm{d}\widetilde{\bm{b}}_{t_{n},n^{ \dagger}h^{\dagger}+\tau^{\dagger}}.\end{cases}\] (C.21) We further define the joint probability distribution of \((\bm{u}_{t_{n},t^{\dagger}},\bm{v}_{t_{n},t^{\dagger}})\) at time \(t\) as \(\pi_{t_{n},t^{\dagger}}(\bm{u}_{t_{n},t^{\dagger}},\bm{v}_{t_{n},t^{\dagger}})\) and its marginal on \(\bm{u}_{t_{n},t^{\dagger}}\) as \(\pi_{t_{n},t^{\dagger}}^{\bm{u}}(\bm{u}_{t_{n},t^{\dagger}})\).

**Remark C.11**.: _The application of Girsanov's theorem A.4 is by writing the system of SDEs in the matrix form._

**Definition C.12** (Stationary process).: _Under the \(P\)-measure that is defined by the Radon-Nikodym derivative (C.19), we may define a stationary underdamped Langevin process for \(n^{\dagger}\in[0:N^{\dagger}-1]\) and \(\tau^{\dagger}\in[0,h^{\dagger}]\) as_

\[\begin{cases}\mathrm{d}\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}= \bm{v}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\mathrm{d}\tau^{ \dagger},\\ \mathrm{d}\bm{v}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}=-\gamma\bm{v}_ {t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\mathrm{d}\tau^{\dagger}-\nabla \log\widetilde{p}_{t_{n+1}}(\bm{u}_{t_{n},t^{\dagger}}^{*}(\bm{u}_{t_{n},t^{\dagger}} ^{*})\mathrm{d}\tau^{\dagger}+\sqrt{2\gamma}\mathrm{d}\widetilde{\bm{b}}_{t_{n},n^{ \dagger}h^{\dagger}+\tau^{\dagger}},\end{cases}\] (C.22)

_with the initial condition \(\bm{u}_{t_{n},n^{\dagger}h^{\dagger}}^{*}\sim\widetilde{p}_{t_{n+1}}\) and \(\bm{v}_{t_{n},n^{\dagger}h^{\dagger}}^{*}\sim\mathcal{N}(0,\bm{I}_{d})\). We define the joint probability distribution of \((\bm{u}_{t_{n},t^{\dagger}}^{*},\bm{v}_{t_{n},t^{\dagger}}^{*})\) at time \(t\) as \(\pi_{t_{n},t^{\dagger}}^{*}(\bm{u}_{t_{n},t^{\dagger}}^{*},\bm{v}_{t_{n},t^{ \dagger}}^{*})\) and its marginal on \(\bm{u}_{t_{n},t^{\dagger}}^{*}\) as \(\pi_{t_{n},t^{\dagger}}^{*,\bm{u}_{t_{n},t^{\dagger}}^{*}}(\bm{u}_{t_{n},t^{ \dagger}}^{*})\)._

Thus, from Corollary A.5, we have that

\[\begin{split}& D_{\mathrm{KL}}(\pi_{t_{n},n^{\dagger}h^{\dagger}}\| \widetilde{\pi}_{t_{n},n^{\dagger}h^{\dagger}})\\ \leq& D_{\mathrm{KL}}(\pi_{t_{n},(n-1)h^{\dagger}}\| \widetilde{\pi}_{t_{n},(n-1)h^{\dagger}})+\sum_{n=0}^{N^{\dagger}-1}D_{\mathrm{KL}}( \pi_{t_{n},n^{\dagger}h^{\dagger};(n+1)h^{\dagger}}\|\widetilde{\pi}_{t_{n},n^ {\dagger}h^{\dagger};(n+1)h^{\dagger}})\\ \leq& D_{\mathrm{KL}}(\pi_{t_{n},(n-1)h^{\dagger}}\| \widetilde{\pi}_{t_{n},(n-1)h^{\dagger}})\\ +&\frac{1}{4\gamma}\mathbb{E}_{P}\left[\int_{0}^{h^{ \dagger}}\left\|\bm{s}_{t_{n+1}}(\widetilde{\bm{u}}_{t_{n},By triangle inequality, we have

\[\int_{0}^{h^{\dagger}}\left\|\bm{s}_{t_{n+1}}(\widetilde{\bm{u}}_{t_{n },n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{\dagger}} \rfloor e^{\dagger}}^{(K^{\dagger}-1)})-\nabla\log\bar{p}_{t_{n+1}}(\widetilde{ \bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})})\right\|^ {2}\mathrm{d}\tau^{\dagger}\] (C.24) \[\leq 5\int_{0}^{h^{\dagger}}\left\|\bm{s}_{t_{n+1}}(\widetilde{\bm{u} }_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger}-1)}-\bm{s}_{t_{n+1}}(\widetilde{\bm {u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger})})\right\|^{2}\mathrm{d}\tau^{\dagger}\] \[+5\int_{0}^{h^{\dagger}}\left\|\bm{s}_{t_{n+1}}(\widetilde{\bm{u} }_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger})})-\nabla\log\bar{p}_{t_{n+1}}( \widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger} )})\right\|^{2}\mathrm{d}\tau^{\dagger}\] \[+5\int_{0}^{h^{\dagger}}\left\|\nabla\log\bar{p}_{t_{n+1}}(\bm{u} _{t_{n},n^{\dagger}h^{\dagger}+\lfloor\frac{\tau^{\dagger}}{\varepsilon^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger})})-\nabla\log\bar{p}_{t_{n+1}}(\bm{ u}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})})\right\|^{2} \mathrm{d}\tau^{\dagger}\] \[+5\int_{0}^{h^{\dagger}}\left\|\nabla\log\bar{p}_{t_{n+1}}(\bm{u} _{t_{n},n^{\dagger}h^{\dagger}+\lfloor\frac{\tau^{\dagger}}{\varepsilon^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger})}-\nabla\log\bar{p}_{t_{n+1}}(\bm{ u}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})})\right\|^{2} \mathrm{d}\tau^{\dagger}\] \[\leq 5L_{\bm{s}}^{2}\int_{0}^{h^{\dagger}}\left\|\widetilde{\bm{u} }_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger}-1)}-\widetilde{\bm{u}}_{t_{n},n^{ \dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{\dagger}} \rfloor e^{\dagger}}^{(K^{\dagger})}\right\|^{2}\mathrm{d}\tau^{\dagger}\] \[+5\underbrace{\left(L_{\bm{s}}^{2}\int_{0}^{h^{\dagger}}\left\| \widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{ \varepsilon^{\dagger}}\rfloor e^{\dagger}}^{(K^{\dagger})}-\bm{u}_{t_{n},n^{ \dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{\dagger}} \rfloor e^{\dagger}}^{(K^{\dagger})}\right\|^{2}\mathrm{d}\tau^{\dagger}+L_{p}^{2 }\int_{0}^{h^{\dagger}}\left\|\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger}, \tau^{\dagger}}^{(K^{\dagger})}-\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{ \dagger}}^{*}\right\|^{2}\mathrm{d}\tau^{\dagger}\right)}_{:=E_{t_{n},n^{ \dagger}h^{\dagger}}}\] \[+5h^{\dagger}\delta_{\infty}^{2}+5L_{p}^{2}\underbrace{\int_{0}^{h^ {\dagger}}\left\|\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\lfloor\frac{\tau^{ \dagger}}{\varepsilon^{\dagger}}\rfloor e^{\dagger}}^{*}-\bm{u}_{t_{n},n^{ \dagger}h^{\dagger}+\tau^{\dagger}}^{*}\right\|^{2}\mathrm{d}\tau^{\dagger}}_{: =F_{t_{n},n^{\dagger}h^{\dagger}}},\]

where we used the Lipschitz continuity of the learned score function (Assumption 3.3) and the true score function (Assumption 3.4), and the \(\delta_{\infty}\)-accuracy of the learned score function at each time step (Assumption 3.1').

Now we proceed to bound the terms in the error decomposition (C.24). We first bound the \(F_{t_{n},n^{\dagger}h^{\dagger}}\) term by the following lemma:

**Lemma C.13**.: _For any \(n\in[0:N-1]\) and \(\tau^{\dagger}\in[0,h^{\dagger}]\), we have_

\[\mathbb{E}_{P}\left[\left\|\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\lfloor\frac{ \tau^{\dagger}}{\varepsilon^{\dagger}}\rfloor e^{\dagger}}^{*}-\bm{u}_{t_{n},n^ {\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\right\|^{2}\right]\leq d{\epsilon^{ \dagger}}^{2},\]

_and therefore_

\[\mathbb{E}_{P}\left[F_{t_{n},n^{\dagger}h^{\dagger}}\right]\leq dh^{\dagger}{ \epsilon^{\dagger}}^{2}.\]

Proof.: By the definition of \((\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*},\bm{v}_{t_{n},n^{ \dagger}h^{\dagger}+\tau^{\dagger}}^{*})\) as the stationary underdamped Langevin dynamics (C.22), we have

\[\mathbb{E}_{P}\left[\left\|\bm{u}_{t_{n},n^{\dagger}h^{\dagger}+ \lfloor\frac{\tau^{\dagger}}{\varepsilon^{\dagger}}\rfloor e^{\dagger}}^{*}- \bm{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\right\|^{2}\right] =\mathbb{E}_{P}\left[\left\|\int_{\lfloor\frac{\tau^{\dagger}}{ \varepsilon^{\dagger}}\rfloor e^{\dagger}}^{\tau^{\dagger}}\bm{v}_{t_{n},n^{ \dagger}h^{\dagger}+\tau^{\dagger}}^{*}\mathrm{d}\tau^{\dagger}\right\|^{2}\right]\] \[\leq{\epsilon^{\dagger}}\int_{\lfloor\frac{\tau^{\dagger}}{ \varepsilon^{\dagger}}\rfloor e^{\dagger}}^{\tau^{\dagger}}\mathbb{E}_{P}\left[ \left\|\bm{v}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\right\|^{2} \right]\mathrm{d}\tau^{\dagger}{}^{\prime}\leq d{\epsilon^{\dagger}}^{2},\]

where the first inequality follows from Cauchy-Schwarz inequality and the last inequality is by the fact that

\[\bm{v}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\sim\mathcal{N}(0,\bm{I}_{ d}),\quad\text{for any }{\tau^{\dagger}}^{\prime}\in[0,h^{\dagger}].\]Consequently, we have

\[\mathbb{E}_{P}\left[F_{t_{n},n^{\dagger}h^{\dagger}}\right]=\int_{0}^{h^{\dagger}} \mathbb{E}_{P}\left[\left\|\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}+\lfloor \frac{\tau^{\dagger}}{\epsilon^{\dagger}}\rfloor^{\epsilon^{\dagger}}}^{*}- \boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\right\|^{2} \right]\mathrm{d}\tau^{\dagger}\leq dh^{\dagger}{\epsilon^{\dagger}}^{2}.\]

The term \(E_{t_{n},n^{\dagger}h^{\dagger}}\) can be bounded with the following lemma:

**Lemma C.14**.: _For any \(n^{\dagger}\in[0:N^{\dagger}-1]\), suppose that \(\gamma\lesssim L_{p}^{-1/2}\) and \(T^{\dagger}\lesssim L_{p}^{-1/2}\), then we have the following inequality for any \(\tau^{\dagger}\in[0,h^{\dagger}]\)_

\[\mathbb{E}_{P}\left[\left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{ \dagger},\tau^{\dagger}}^{(K^{\dagger})}-\boldsymbol{u}_{t_{n},n^{\dagger}h^ {\dagger}+\tau^{\dagger}}^{*}\right\|^{2}\right]\lesssim W_{2}^{2}(\widetilde {q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}}),\]

_and therefore_

\[\mathbb{E}_{P}\left[E_{t_{n},n^{\dagger}h^{\dagger}}\right]\lesssim h^{ \dagger}(L_{\boldsymbol{s}}^{2}+L_{p}^{2})W_{2}^{2}(\widetilde{q}_{t_{n},h_{n} },\widetilde{p}_{t_{n+1}}).\]

Proof.: Recall that under the measure \(P\), \(\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^ {\dagger})}\) follows the dynamics of \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}\) (C.21) for \(\tau^{\dagger}\in[0,h^{\dagger}]\), which coincides with that of \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\). As the only difference between the two processes \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}\) and \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\) is the initial condition, we can invoke Lemma 10 proved in [111] to deduce that

\[\mathbb{E}_{P}\left[\left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{ \dagger},\tau^{\dagger}}^{(K^{\dagger})}-\boldsymbol{u}_{t_{n},n^{\dagger}h^{ \dagger}+\tau^{\dagger}}^{*}\right\|^{2}\right]\lesssim W_{2}^{2}(\pi_{t_{n},n ^{\dagger}h^{\dagger}},\widetilde{p}_{t_{n+1}}),\]

where the assumption that \(\gamma\lesssim L_{p}^{-1/2}\) and \(T^{\dagger}\lesssim L_{p}^{-1/2}\) is required.

Now notice that \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{*}\) and \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}\) also follow the same dynamics with the true score function for \(\tau^{\dagger}\in[0,n^{\dagger}h^{\dagger}]\), for any coupling of \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}}^{*}\) and \(\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}}\), we have

\[W_{2}^{2}(\pi_{t_{n},n^{\dagger}h^{\dagger}},\widetilde{p}_{t_{n +1}})\leq\mathbb{E}\left[\|\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}}- \boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}}^{*}\|^{2}\right]\] \[\leq W_{2}^{2}(\pi_{t_{n},0},\widetilde{p}_{t_{n+1}})=W_{2}^{2}( \widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}}),\]

where the last equality is again by [111, Lemma 10].

Therefore, we have

\[\mathbb{E}_{P}\left[E_{t_{n},n^{\dagger}h^{\dagger}}\right]\] \[= \int_{0}^{h^{\dagger}}\mathbb{E}_{P}\left[L_{\boldsymbol{s}}^{2} \left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor \frac{\tau^{\dagger}}{\epsilon^{\dagger}}\rfloor^{\epsilon^{\dagger}}}^{(K^{ \dagger})}-\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}+\lfloor\frac{\tau^{ \dagger}}{\epsilon^{\dagger}}\rfloor^{\epsilon}}^{*}\right\|^{2}+L_{p}^{2} \left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^ {(K^{\dagger})}-\boldsymbol{u}_{t_{n},n^{\dagger}h^{\dagger}+\tau^{\dagger}}^{* }\right\|^{2}\right]\mathrm{d}\tau^{\dagger}\] \[\leq h^{\dagger}(L_{\boldsymbol{s}}^{2}+L_{p}^{2})W_{2}^{2}( \widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}}).\]

Now, we provide lemmas that are used to bound the first term in (C.24).

**Lemma C.15**.: _For any \(n^{\dagger}\in[0:N^{\dagger}-1]\), we have the following estimate:_

\[\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\left\| \widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(1)}- \widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(0)} \right\|^{2}\right]\] \[\leq \frac{5L_{\boldsymbol{s}}^{2}h^{\dagger}e^{(3+\gamma)h^{\dagger}}} {2\gamma}\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\left\| \widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{ \dagger}-1)}-\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(K^{\dagger})}\right\|^{2}\right]\] \[+ \frac{5h^{\dagger}e^{(3+\gamma)h^{\dagger}}}{2\gamma}\mathbb{E}_{P} \left[E_{t_{n},n^{\dagger}h^{\dagger}}+h^{\dagger}\delta_{\infty}^{2}+L_{p}^{2}F_{ t_{n},n^{\dagger}h^{\dagger}}\right]+h^{\dagger}e^{(3+\gamma)h^{\dagger}} \left(3\gamma d+M_{\boldsymbol{s}}^{2}\right)+h^{\dagger}e^{2h^{\dagger}}d.\]Proof.: Let \(\bm{\mu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}:=\widetilde{\bm{u}}_{t_ {n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(1)}-\widetilde{\bm{u}}_{t_{n,n} \n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)}\) and \(\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}:=\widetilde{\bm{v} }_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(1)}-\widetilde{\bm{v} }_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)}\). Then for \(k=0\), we may rewrite (C.15) as follows

\[\begin{cases}\mathrm{d}\bm{\mu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}=\left(\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}+ \widetilde{\bm{v}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)} \right)\mathrm{d}\tau^{\dagger}\\ \mathrm{d}\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}=-\gamma (\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}+\widetilde{\bm{v} }_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)})\mathrm{d}\tau^{ \dagger}-\bm{s}_{t_{n+1}}(\widetilde{\bm{u}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)})\mathrm{d}\tau^{\dagger}+\sqrt{2\gamma}\mathrm{d}\bm{b}_ {t_{n,n}\n^{\dagger}\hbar^{\dagger}+\tau^{\dagger}}\end{cases}\] (C.25)

On the one hand, by using the first equation in (C.25), we may compute the derivative

\[\frac{\mathrm{d}}{\mathrm{d}\tau^{\dagger}}\left\|\bm{\mu}_{t_{n,n}\n^{\dagger} \hbar^{\dagger},\tau^{\dagger}}\right\|^{2}=2\bm{\mu}_{t_{n}\n^{\dagger}\hbar^{ \dagger},\tau^{\dagger}}^{\top}\left(\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{ \dagger},\tau^{\dagger}}+\widetilde{\bm{v}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)}\right)\]

and integrate it for \({\tau^{\dagger}}^{\prime}\in[0,\tau^{\dagger}]\), which yields

\[\left\|\bm{\mu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger }}\right\|^{2}=2\int_{0}^{\tau^{\dagger}}\bm{\mu}_{t_{n,n}\n^{\dagger}\hbar^{ \dagger},\tau^{\dagger}}^{\top}(\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger}, \tau^{\dagger}}+\widetilde{\bm{v}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}^{(0)})\mathrm{d}{\tau^{\dagger}}^{\prime}\] \[\leq 2\int_{0}^{\tau^{\dagger}}\left\|\bm{\mu}_{t_{n,n}\n^{\dagger} \hbar^{\dagger},\tau^{\dagger}}\right\|^{2}\mathrm{d}{\tau^{\dagger}}^{\prime}+ \int_{0}^{\tau^{\dagger}}\left\|\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger}, \tau^{\dagger}}\right\|^{2}\mathrm{d}{\tau^{\dagger}}^{\prime}+\int_{0}^{\tau ^{\dagger}}\left\|\widetilde{\bm{v}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}^{(0)}\right\|^{2}\mathrm{d}{\tau^{\dagger}}^{\prime}.\]

Applying Gronwall's inequality, we have

\[\left\|\bm{\mu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}\right\|^{2} \leq e^{2\tau^{\dagger}}\left(\int_{0}^{\tau^{\dagger}}\left\|\bm{\nu}_{t_{n,n }\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}\right\|^{2}\mathrm{d}{\tau^{ \dagger}}^{\prime}+\int_{0}^{\tau^{\dagger}}\left\|\widetilde{\bm{v}}_{t_{n,n} \n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)}\right\|^{2}\mathrm{d}{\tau^ {\dagger}}^{\prime}\right).\]

We then take expectation with respect to the path measure \(P\) and then the supremum with respect to \(\tau^{\dagger}\in[0,h^{\dagger}]\), implying that

\[\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\left\| \bm{\mu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}\right\|^{2}\right]\] \[\leq \sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\left(e^{2\tau^{\dagger}} \int_{0}^{\tau^{\dagger}}\mathbb{E}_{P}\left[\left\|\bm{\nu}_{t_{n,n}\n^{ \dagger}\hbar^{\dagger},\tau^{\dagger}}\right\|^{2}\right]\mathrm{d}{\tau^{ \dagger}}^{\prime}+e^{2\tau^{\dagger}}\int_{0}^{\tau^{\dagger}}\mathbb{E}_{P} \left[\left\|\widetilde{\bm{v}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}\right\|^{2}\right]\mathrm{d}{\tau^{\dagger}}^{\prime}\right)\] \[\leq h^{\dagger}e^{2h^{\dagger}}\sup_{\tau^{\dagger}\in[0,h^{ \dagger}]}\mathbb{E}_{P}\left[\left\|\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{ \dagger},\tau^{\dagger}}\right\|^{2}\right]+h^{\dagger}e^{2h^{\dagger}}d.\]

On the other hand, by applying Ito's lemma and plugging in the expression of \(\bm{b}_{t_{n,n}\n^{\dagger}\hbar^{\dagger}+\tau^{\dagger}}\) given by (C.17), we have

\[\mathrm{d}\|\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}\|^{2}\] \[= -\left[2\gamma\|\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}\|^{2}+2\gamma\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}^{\top}\widetilde{\bm{v}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{ \dagger}}^{(0)}+2\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{ \top}\bm{s}_{t_{n+1}}\Big{(}\widetilde{\bm{u}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger}, \tau^{\dagger}}^{(0)}\Big{)}-2\gamma d\right]\mathrm{d}\tau^{\dagger}\] \[+2\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{\top} \sqrt{2\gamma}\big{(}\mathrm{d}\widetilde{\bm{b}}_{t_{n,n}\n^{\dagger}\hbar^{ \dagger}+\tau^{\dagger}}+\bm{\phi}_{t_{n,n}\n^{\dagger}\hbar^{\dagger}}(\tau^{ \dagger})\mathrm{d}\tau^{\dagger}\big{)},\] (C.27)

Then similarly, we may compute the derivative of \(\|\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}\|^{2}\), integrate it for \(\tau^{\dagger}\in[0,h^{\dagger}]\), and take the supremum with respect to \(\tau^{\dagger}\) to obtain

\[\mathbb{E}_{P}\left[\|\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger}, \tau^{\dagger}}\|^{2}\right]\] \[= \mathbb{E}_{P}\left[-\int_{0}^{\tau^{\dagger}}\left(2\gamma\|\bm{\nu} _{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}\|^{2}+2\gamma\bm{\nu}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{\top}\widetilde{\bm{v}}_{t_{n,n} \n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)}-2\gamma d\right)\mathrm{d}\tau^{ \dagger}\right]\] \[+\mathbb{E}_{P}\left[-\int_{0}^{\tau^{\dagger}}2\bm{\nu}_{t_{n,n} \n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{\top}\bm{s}_{t_{n+1}}\Big{(} \widetilde{\bm{u}}_{t_{n,n}\n^{\dagger}\hbar^{\dagger},\tau^{\dagger}}^{(0)} \Big{}\mathrm{d}{\tau^{\dagger}}^{\prime}\Big{]}\] \[+2\sqrt{2\gamma}\mathbb{E}_{P}\left[By Ito's lemma, this equals to

\[\mathbb{E}_{P}\left[\left\|\boldsymbol{\nu}_{t_{n},n^{\dagger}h^{ \dagger},\tau^{\dagger}}\right\|^{2}\right]\] \[= \mathbb{E}_{P}\left[-\int_{0}^{\tau^{\dagger}}\left(2\gamma\| \boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}\|^{2}+2\gamma \boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{\top}\widetilde {\boldsymbol{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(0)}-2\gamma d \right)\mathrm{d}\tau^{\dagger}{}^{\prime}\right]\] \[+\mathbb{E}_{P}\left[-\int_{0}^{\tau^{\dagger}}2\boldsymbol{\nu}_ {t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{\top}\boldsymbol{s}_{t_{n+1}} \left(\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^ {(0)}\right)+2\sqrt{2\gamma}\boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger}, \tau^{\dagger}}^{\top}\boldsymbol{\phi}_{t_{n},n^{\dagger}h^{\dagger}}(\tau^ {\dagger}{}^{\prime})\mathrm{d}\tau^{\dagger}{}^{\prime}\right].\]

Applying AM-GM gives

\[\mathbb{E}_{P}\left[||\boldsymbol{\nu}_{t_{n},n^{\dagger}h^{ \dagger},\tau^{\dagger}}||^{2}\right]\] \[\leq \int_{0}^{\tau^{\dagger}}\mathbb{E}_{P}\left[(1+\gamma)|| \boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}||^{2}+\| \boldsymbol{\phi}_{t_{n},n^{\dagger}h^{\dagger}}(\tau^{\dagger}{}^{\prime})\| ^{2}\right]\mathrm{d}\tau^{\dagger}{}^{\prime}\] \[+ \int_{0}^{\tau^{\dagger}}\mathbb{E}_{P}\left[\gamma\left\| \widetilde{\boldsymbol{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(0)} \right\|^{2}+\left\|\boldsymbol{s}_{t_{n+1}}\left(\widetilde{\boldsymbol{u}}_{ t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(0)}\right)\right\|^{2}+2 \gamma d\right]\mathrm{d}\tau^{\dagger}{}^{\prime}\] \[\leq \int_{0}^{\tau^{\dagger}}\mathbb{E}_{P}\left[(1+\gamma)|| \boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}|^{2}+\| \boldsymbol{\phi}_{t_{n},n^{\dagger}h^{\dagger}}(\tau^{\dagger}{}^{\prime})\| ^{2}\right]\mathrm{d}\tau^{\dagger}{}^{\prime}+\left(\gamma\mathbb{E}\left[ \left\|\widetilde{\boldsymbol{v}}_{t_{n},n^{\dagger}h^{\dagger},0}^{(0)}\right\| ^{2}\right]+M_{\boldsymbol{s}}^{2}+2\gamma d\right)\tau^{\dagger}\] \[= (1+\gamma)\int_{0}^{\tau^{\dagger}}\mathbb{E}_{P}\left[\| \boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}\|^{2}\right] \mathrm{d}\tau^{\dagger}{}^{\prime}+\int_{0}^{\tau^{\dagger}}\mathbb{E}_{P} \left[\|\boldsymbol{\phi}_{t_{n},n^{\dagger}h^{\dagger}}(\tau^{\dagger}{}^{ \prime})\|^{2}\right]\mathrm{d}\tau^{\dagger}{}^{\prime}+\tau^{\dagger}\left(3 \gamma d+M_{\boldsymbol{s}}^{2}\right),\]

where in the last equality, we used the initialization of the auxiliary corrector process \(\widetilde{\boldsymbol{v}}_{t_{n},n^{\dagger}h^{\dagger},0}^{(0)}\sim\mathcal{ N}(0,\boldsymbol{I}_{d})\).

Again, we apply Gronwall's inequality to the above inequality and take the supremum with respect to \(\tau^{\dagger}\in[0,h^{\dagger}]\) to obtain

\[\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\| \boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}\|^{2}\right]\] (C.28) \[\leq e^{(1+\gamma)h^{\dagger}}\int_{0}^{h^{\dagger}}\mathbb{E}_{P} \left[\|\boldsymbol{\phi}_{t_{n},n^{\dagger}h^{\dagger}}(\tau^{\dagger})\|^{2} \right]\mathrm{d}\tau^{\dagger}+h^{\dagger}e^{(1+\gamma)h^{\dagger}}\left(3 \gamma d+M_{\boldsymbol{s}}^{2}\right)\] \[\leq \frac{e^{(1+\gamma)h^{\dagger}}}{2\gamma}\mathbb{E}_{P}\left[\int_ {0}^{h^{\dagger}}\left\|\boldsymbol{s}_{t_{n+1}}(\widetilde{\boldsymbol{u}}_{ t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\tau^{\dagger}}\rfloor e^{ \dagger}}^{(K^{\dagger}-1)})-\nabla\log\widetilde{p}_{t_{n+1}}(\widetilde{ \boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})}) \right\|^{2}\mathrm{d}\tau^{\dagger}\right]\] \[+h^{\dagger}e^{(1+\gamma)h^{\dagger}}\left(3\gamma d+M_{ \boldsymbol{s}}^{2}\right),\]

and for the difference term within the expectation, we decompose it again by the triangle inequality in (C.24), _i.e._

\[\int_{0}^{h^{\dagger}}\left\|\boldsymbol{s}_{t_{n+1}}(\widetilde{ \boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{ \tau^{\dagger}}\rfloor e^{\dagger}}^{(K^{\dagger}-1)})-\nabla\log\widetilde{p}_{t _{n+1}}(\widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}} ^{(K^{\dagger})})\right\|^{2}\mathrm{d}\tau^{\dagger}\] \[\leq 5L_{\boldsymbol{s}}^{2}\int_{0}^{h^{\dagger}}\left\|\widetilde{ \boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\tau^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger}-1)}-\widetilde{\boldsymbol{u}}_{t_{n },n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\tau^{\dagger}}\rfloor e^{ \dagger}}^{(K^{\dagger})}\right\|^{2}\mathrm{d}\tau^{\dagger}+5L_{t_{n},n^{ \dagger}h^{\dagger}}+5h^{\dagger}\delta_{\infty}^{2}+5L_{p}^{2}F_{t_{n},n^{\dagger}h ^{\dagger}},\]

to obtain that

\[\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\| \boldsymbol{\nu}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}}\|^{2}\right]\] \[\leq \frac{5L_{\boldsymbol{s}}^{2}e^{(1+\gamma)h^{\dagger}}}{2\gamma} \mathbb{E}_{P}\left[\int_{0}^{h^{\dagger}}\left\|\widetilde{\boldsymbol{u}}_{t_{n },n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\tau^{\dagger}}\rfloor e^{\dagger}}^{(K^{\dagger}-1)}-\widetilde{ \boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\tau^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger})}\right\|^{2}\mathrm{d}\tau^{ \dagger}\right]\] \[+\frac{5e^{(1+\gamma)h^{\dagger}}}{2\gamma}\mathbb{E}_{P}\left[E _{t_{n},n^{\dagger}h^{\dagger}}+h^{\dagger}\delta_{\infty}^{2}+L_{p}^{2}F_{t_{n},n^ {\dagger}h^{\dagger}}\right]+h^{\dagger}e^{(1+\gamma)h^{\dagger}}\left(3\gamma d+ M_{\boldsymbol{s}}^{2}\right)\] \[\leq \frac{5L_{\boldsymbol{s}}^{2}e^{(1+\gamma)h^{\dagger}}}{2\gamma}h^{ \dagger}\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\left\| \widetilde{\boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{ \dagger}}{\tau^{\dagger}}\rfloor e^{\dagger}}^{(K^{\dagger}-1)}-\widetilde{ \boldsymbol{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{\tau^{ \dagger}}\rfloor e^{\dagger}}^{(K^{\dagger})}\right\|^{2}\right]\] \[+\frac{5e^{(1+\gamma)h^{\dagger}}}{2\gamma}\mathbb{E}_{P}\left[E _{t_{n},n^{\dagger}h^{\dagger}}+h^{\dagger}\delta_{\infty}^{2}+L_{p}^{2}F_{t_{n}

substituting which into (C.26) completes our proof of this Lemma. 

**Lemma C.16** (Exponential convergence of Picard iteration in the corrector step of PIADM-ODE).: _For any \(n^{\dagger}\in[0,N^{\dagger}-1]\), then the two ending terms \(\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})}\) and \(\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})}\) of the sequence \(\{\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})} \}_{k^{\dagger}\in[0:K^{\dagger}-1]}\) satisfy the following exponential convergence rate_

\[\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\left\| \widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger})}- \widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(K^{\dagger}-1)} \right\|^{2}\right]\] \[\leq C_{K^{\dagger}}\left(\frac{5h^{\dagger}e^{(3+\gamma)h^{\dagger }}}{2\gamma}\mathbb{E}_{P}\left[E_{t_{n},n^{\dagger}h^{\dagger}}+h^{\dagger} \delta_{\infty}^{2}+L_{p}^{2}F_{t_{n},n^{\dagger}h^{\dagger}}\right]+{h^{ \dagger}}^{2}e^{(3+\gamma)h^{\dagger}}\left(3\gamma d+M_{\bm{s}}^{2}\right)+h^ {\dagger}e^{2h^{\dagger}}d\right),\] (C.29)

_where the coefficient_

\[C_{K^{\dagger}}=\left(\frac{L_{\bm{s}}^{2}{h^{\dagger}}^{2}e^{h^{\dagger}}}{2 \gamma}\right)^{K^{\dagger}-1}\left/\left(1-\frac{5L_{\bm{s}}^{2}h^{\dagger}e^ {(3+\gamma)h^{\dagger}}}{2\gamma}\left(\frac{L_{\bm{s}}^{2}{h^{\dagger}}^{2}e^ {h^{\dagger}}}{2\gamma}\right)^{K^{\dagger}-1}\right).\right.\]

Proof.: We subtract the dynamics of \(\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}\) and \(\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k)}\) in (C.15) to obtain

\[\mathrm{d}\left(\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+ 1)}-\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})} \right)=\left(\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1) }-\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})} \right)\mathrm{d}\tau^{\dagger}.\]

Then, we use the formula above to compute the derivative

\[\frac{\mathrm{d}}{\mathrm{d}\tau^{\dagger}}\left\|\widetilde{\bm{u}}_{n^{ \dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{\bm{u}}_{n^{\dagger}h^{ \dagger},\tau^{\dagger}}^{(k^{\dagger})}\right\|^{2}=2\left(\widetilde{\bm{u} }_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{\bm{u}}_{n^{ \dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})}\right)^{\top}\left( \widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{ \bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})}\right)\]

and integrate for \({\tau^{\dagger}}^{\prime}\in[0,\tau^{\dagger}]\) to obtain

\[\left\|\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^ {(k+1)}-\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{ \dagger})}\right\|^{2}\] \[= 2\int_{0}^{\tau^{\dagger}}\left(\widetilde{\bm{u}}_{n^{\dagger}h^ {\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger}, \tau^{\dagger}}^{(k^{\dagger})}\right)^{\top}\left(\widetilde{\bm{v}}_{n^{ \dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{\bm{v}}_{n^{\dagger}h^ {\dagger},\tau^{\dagger}}^{(k^{\dagger})}\right)\mathrm{d}{\tau^{\dagger}}^{\prime}\] \[\leq \int_{0}^{\tau^{\dagger}}\left\|\widetilde{\bm{u}}_{n^{\dagger}h^ {\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger}, \tau^{\dagger}}^{(k^{\dagger})}\right\|^{2}\mathrm{d}{\tau^{\dagger}}^{\prime}+ \int_{0}^{\tau^{\dagger}}\left\|\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger}, \tau^{\dagger}}^{(k+1)}-\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(k^{\dagger})}\right\|^{2}\mathrm{d}{\tau^{\dagger}}^{\prime}\]

Applying Gronwall's inequality gives us that

\[\left\|\widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}- \widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})} \right\|^{2}\leq e^{\tau^{\dagger}}\int_{0}^{\tau^{\dagger}}\left\|\widetilde{ \bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{\bm{v}}_{n^ {\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})}\right\|^{2}\mathrm{d}{ \tau^{\dagger}}^{\prime}\]

and taking the supremum with respect to \(\tau^{\dagger}\in[0,h^{\dagger}]\) on both sides above implies

\[\sup_{\tau^{\dagger}\in[0,h^{\dagger}]}\mathbb{E}_{P}\left[\left\| \widetilde{\bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{ \bm{u}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})}\right\|^{2} \right]\leq h^{\dagger}e^{h^{\dagger}}\sup_{\tau^{\dagger}\in[0,h^{\dagger}]} \mathbb{E}_{P}\left[\left\|\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(k+1)}-\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^ {\dagger})}\right\|^{2}\right].\] (C.30)

We then apply a similar argument for \(\widetilde{\bm{v}}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k+1)}-\widetilde{\bm{v }}_{n^{\dagger}h^{\dagger},\tau^{\dagger}}^{(k^{\dagger})}\) as well

\[\mathrm{d}\left(\widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^ {\dagger}}^{(k+1)}-\widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(k^{\dagger})}\right)\] \[= -\gamma\left(\widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^ {\dagger}}^{(k+1)}-\widetilde{\bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{ \dagger}}^{(k^{\dagger})}\right)\mathrm{d}\tau^{\dagger}-\left(\bm{s}_{t_{n+ 1}}(\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{ \dagger}}{c^{\dagger}}\rfloor e^{t}})-\bm{s}_{t_{n+1}}(\widetilde{\bm{u}}_{t _{n},n^{\dagger}h^{\dagger},\lfloor\frac{\tau^{\dagger}}{c^{\dagger}}\rfloor e^{ t}}^{(k-1)})\right)\mathrm{d}\tau^{\dagger},\]integrate which for \(\tau^{\uparrow}\in[0,\tau^{\uparrow}]\) to obtain

\[\left\|\widetilde{\boldsymbol{v}}_{n^{\uparrow}h^{\uparrow},\tau^{ \uparrow}}^{(k+1)}-\widetilde{\boldsymbol{v}}_{n^{\uparrow}h^{\uparrow},\tau ^{\uparrow}}^{(k^{\uparrow})}\right\|^{2}\] \[= -\int_{0}^{\tau^{\uparrow}}2\gamma\left\|\widetilde{ \boldsymbol{v}}_{n^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k+1)}- \widetilde{\boldsymbol{v}}_{n^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k^{ \uparrow})}\right\|^{2}\mathrm{d}\tau^{\uparrow}\] \[- 2\int_{0}^{\tau^{\uparrow}}\left(\widetilde{\boldsymbol{v}}_{n^{ \uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k+1)}-\widetilde{\boldsymbol{v}}_{n ^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k^{\uparrow})}\right)^{\top} \left(\boldsymbol{s}_{t_{n+1}}(\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow} h^{\uparrow},\lfloor\frac{\tau^{\uparrow}}{\epsilon^{\uparrow}}\rfloor \epsilon^{\uparrow}}^{(k^{\uparrow})})-\boldsymbol{s}_{t_{n+1}}(\widetilde{ \boldsymbol{u}}_{t_{n},n^{\uparrow}h^{\uparrow},\lfloor\frac{\tau^{\uparrow }}{\epsilon^{\uparrow}}\rfloor\epsilon^{\uparrow}}^{(k-1)})\right)\mathrm{d} {\tau^{\uparrow}}^{\prime}\] \[\leq \frac{1}{2\gamma}\int_{0}^{\tau^{\uparrow}}\left\|\boldsymbol{s} _{t_{n+1}}(\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow}h^{\uparrow},\lfloor \frac{\tau^{\uparrow}}{\epsilon^{\uparrow}}\rfloor\epsilon^{\uparrow}}^{(k^{ \uparrow})})-\boldsymbol{s}_{t_{n+1}}(\widetilde{\boldsymbol{u}}_{t_{n},n^{ \uparrow}h^{\uparrow},\lfloor\frac{\tau^{\uparrow}}{\epsilon^{\uparrow}} \rfloor\epsilon^{\uparrow}}^{(k-1)})\right\|^{2}\mathrm{d}\tau^{\uparrow}\] \[\leq \frac{L_{\boldsymbol{s}}^{2}}{2\gamma}\int_{0}^{\tau^{\uparrow}} \left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow}h^{\uparrow},\lfloor \frac{\tau^{\downarrow}}{\epsilon^{\uparrow}}\rfloor\epsilon^{\uparrow}}^{(k^ {\uparrow})}-\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow}h^{\uparrow}, \lfloor\frac{\tau^{\uparrow}}{\epsilon^{\uparrow}}\rfloor\epsilon^{\uparrow}} ^{(k-1)}\right\|^{2}\mathrm{d}\tau^{\uparrow}.\]

And then taking the supremum with respect to \(\tau^{\uparrow}\in[0,h^{\uparrow}]\) on both sides above implies

\[\sup_{\tau^{\uparrow}\in[0,h^{\uparrow}]}\mathbb{E}_{P}\left[\left\|\widetilde {\boldsymbol{v}}_{n^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k+1)}- \widetilde{\boldsymbol{v}}_{n^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k^{ \uparrow})}\right\|^{2}\right]\leq\frac{h^{\uparrow}L_{\boldsymbol{s}}^{2}}{2 \gamma}\sup_{\tau^{\uparrow}\in[0,h^{\uparrow}]}\mathbb{E}_{P}\left[\left\| \widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{( k^{\uparrow})}-\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow}h^{\uparrow}, \tau^{\uparrow}}^{(k-1)}\right\|^{2}\right]\] (C.31)

Substituting (C.31) into (C.30) and iterating over \(k\in[1:K^{\uparrow}-1]\), we obtain that

\[\sup_{\tau^{\uparrow}\in[0,h^{\uparrow}]}\mathbb{E}_{P}\left[\left\| \widetilde{\boldsymbol{u}}_{n^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k^{ \uparrow})}-\widetilde{\boldsymbol{u}}_{n^{\uparrow}h^{\uparrow},\tau^{ \uparrow}}^{(k^{\uparrow}-1)}\right\|^{2}\right]\leq\frac{L_{\boldsymbol{s}}^{2 }{h^{\uparrow}}^{2}e^{h^{\uparrow}}}{2\gamma}\sup_{\tau^{\uparrow}\in[0,h^{ \uparrow}]}\mathbb{E}_{P}\left[\left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{ \uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k^{\uparrow}-1)}-\widetilde{ \boldsymbol{u}}_{t_{n},n^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(k^{\uparrow}- 2)}\right\|^{2}\right]\] \[\leq \left(\frac{L_{\boldsymbol{s}}^{2}{h^{\uparrow}}^{2}e^{h^{\uparrow }}}{2\gamma}\right)^{K^{\uparrow}-1}\sup_{\tau^{\uparrow}\in[0,h^{\uparrow}]} \mathbb{E}_{P}\left[\left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow}h^{ \uparrow},\tau^{\uparrow}}^{(1)}-\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow} h^{\uparrow},\tau^{\uparrow}}^{(0)}\right\|^{2}\right]\] \[\leq \left(\frac{L_{\boldsymbol{s}}^{2}{h^{\uparrow}}^{2}e^{h^{\uparrow }}}{2\gamma}\right)^{K^{\uparrow}-1}\frac{5h^{\uparrow}e^{(3+\gamma)h^{ \uparrow}}}{2\gamma}\mathbb{E}_{P}\left[E_{t_{n},n^{\uparrow}h^{\uparrow}}+h^{ \uparrow}\delta_{\infty}^{2}+L_{p}^{2}F_{t_{n},n^{\uparrow}h^{\uparrow}}\right]\] \[+ \left(\frac{L_{\boldsymbol{s}}^{2}{h^{\uparrow}}^{2}e^{h^{\uparrow }}}{2\gamma}\right)^{K^{\uparrow}-1}\left({h^{\uparrow}}^{2}e^{(3+\gamma)h^{ \uparrow}}\left(3\gamma d+M_{\boldsymbol{s}}^{2}\right)+h^{\uparrow}e^{2h^{ \uparrow}}d\right)\] \[+ \left(\frac{L_{\boldsymbol{s}}^{2}{h^{\uparrow}}^{2}e^{h^{\uparrow }}}{2\gamma}\right)^{K^{\uparrow}-1}\frac{5L_{\boldsymbol{s}}^{2}h^{\uparrow}e^{ (3+\gamma)h^{\uparrow}}}{2\gamma}\sup_{\tau^{\uparrow}\in[0,h^{\uparrow}]} \mathbb{E}_{P}\left[\left\|\widetilde{\boldsymbol{u}}_{t_{n},n^{\uparrow}h^{ \uparrow},\tau^{\uparrow}}^{(K^{\uparrow}-1)}-\widetilde{\boldsymbol{u}}_{t_{n},n ^{\uparrow}h^{\uparrow},\tau^{\uparrow}}^{(K^{\uparrow})}\right\|^{2}\right],\]

where we plug in the results from Lemma C.15 in the last inequality. Rearranging the inequality above completes our proof. 

**Theorem C.17**.: _Under Assumptions 3.1', 3.2, 3.3, and 3.4, given the following choices of the order of the parameters_

\[T^{\dagger}=\mathcal{O}(1),\quad N^{\dagger}=\mathcal{O}(1),\quad h^{\dagger}= \Theta(1)\]

\[M^{\dagger}=\Theta(d^{1/2}\delta^{-1}),\quad\epsilon^{\dagger}=\Theta(d^{-1/2} \delta),\quad K^{\dagger}=\mathcal{O}(\log(d\delta^{-2}))\]

_and let_

\[\frac{L_{\boldsymbol{s}}^{2}{h^{\uparrow}}^{2}e^{h^{\dagger}}}{2\gamma}\ll 1,\quad\gamma\lesssim L_{p}^{-1/2},\quad T^{\dagger}\lesssim L_{p}^{-1/2} \wedge L_{\boldsymbol{s}}^{-1/2},\quad\delta_{\infty}\lesssim\delta\]

_then the distribution \(\widetilde{\boldsymbol{\tau}}_{t_{n},T^{\dagger}}\) satisfies the following error bound:_

\[D_{\mathrm{KL}}(\pi_{t_{n},T^{\dagger}}\|\widetilde{\boldsymbol{ \tau}}_{t_{n},T^{\dagger}})\lesssim T^{\dagger}W_{2}^{2}(\widetilde{q}_{t_{n},h_{n}}, \widetilde{p}_{t_{n+1}})+T^{\dagger}\delta_{\infty}^{2}+dT^{\dagger}{ \epsilon^{\uparrow}}^{2}+e^{-K^{\uparrow}}T^{\dagger}h^{\dagger}d\] \[\lesssim W_{2}^{2}(\widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}})+ \delta^{2},\]

_with a total of \(K^{\dagger}N^{\dagger}=\mathcal{O}\left(\log(d\delta^{-2})\right)\) approximate time complexity and \(M=\Theta\left(d^{1/2}\delta^{-2}\right)\) space complexity for parallallizable \(\delta\)-accurate score function computations._Proof.: Now, we continue the computation by plugging the decomposition in (C.24) and all the error bounds derived above into the equation. First for the last term in (C.23)

\[\mathbb{E}_{P}\left[\int_{0}^{h^{\dagger}}\left\|\bm{s}_{t_{n+1}}( \widetilde{\bm{u}}_{t_{n,n}\dagger^{\dagger}\mathbb{I},\lfloor\frac{\tau^{ \dagger}}{\varepsilon^{\dagger}}\rfloor\varepsilon^{\dagger}}^{(K^{\dagger}-1 )})-\nabla\log\widetilde{p}_{t_{n+1}}(\widetilde{\bm{u}}_{t_{n,n}\dagger^{ \dagger}\mathbb{I},\tau^{\dagger}\tau^{\dagger}}^{(K^{\dagger})})\right\|^{2} \mathrm{d}\tau^{\dagger}\right]\] \[\leq 5L_{\bm{s}}^{2}h^{\dagger}\sup_{\tau^{\dagger}\in[0,h^{\dagger} ]}\mathbb{E}_{P}\left[\left\|\widetilde{\bm{u}}_{t_{n,n}\dagger^{\dagger} \mathbb{I},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{\dagger}}\rfloor \varepsilon^{\dagger}}^{(K^{\dagger}-1)}-\widetilde{\bm{u}}_{t_{n,n}\dagger^{ \dagger}\mathbb{I},\lfloor\frac{\tau^{\dagger}}{\varepsilon^{\dagger}} \rfloor\varepsilon^{\dagger}}^{(K^{\dagger})}\right\|^{2}]+5\mathbb{E}_{P} \left[E_{t_{n,n}\dagger^{\dagger}\mathbb{I}^{\dagger}}+h^{\dagger}\delta_{ \infty}^{2}+L_{p}^{2}F_{t_{n,n}\dagger^{\dagger}\mathbb{I}^{\dagger}}\right]\] \[\leq 5\left(1+L_{\bm{s}}^{2}h^{\dagger}C_{K^{\dagger}}\left(h^{ \dagger 2}e^{(3+\gamma)h^{\dagger}}\left(3\gamma d+M_{\bm{s}}^{2}\right)+h^{ \dagger}e^{2h^{\dagger}}d\right)\right.\]

where the last inequality is by Lemma C.16. We further substitute Lemma C.14 and C.13 into (C.23) to obtain

\[D_{\mathrm{KL}}(\pi_{t_{n,n}\dagger^{\dagger}h^{\dagger}}\| \widetilde{\bm{\pi}}_{t_{n,n}\dagger^{\dagger}\mathbb{I}})\] \[\leq D_{\mathrm{KL}}(\pi_{t_{n,(n-1)h^{\dagger}}}\|\widetilde{\bm{ \pi}}_{t_{n,(n-1)h^{\dagger}}})+5\frac{2\gamma+L_{\bm{s}}^{2}C_{K^{\dagger}}5 \hbar^{\dagger 2}e^{(3+\gamma)h^{\dagger}}}{4\gamma^{2}}\mathbb{E}_{P}\left[E_{t_{n,n} \dagger^{\dagger}h^{\dagger}}+h^{\dagger}\delta_{\infty}^{2}+L_{p}^{2}F_{t_{n,n}\dagger^{\dagger}h^{\dagger}}\right]\] \[+ \frac{5L_{\bm{s}}^{2}h^{\dagger}C_{K^{\dagger}}}{2\gamma}\left(h^ {\dagger 2}e^{(3+\gamma)h^{\dagger}}\left(3\gamma d+M_{\bm{s}}^{2}\right)+h^{ \dagger}e^{2h^{\dagger}}d\right)\] \[\lesssim D_{\mathrm{KL}}(\pi_{t_{n,(n-1)h^{\dagger}}}\|\widetilde{\bm{ \pi}}_{t_{n,(n-1)h^{\dagger}}})\] \[+ 5\frac{2\gamma+L_{\bm{s}}^{2}C_{K^{\dagger}}5\hbar^{\dagger 2}e^{(3+ \gamma)h^{\dagger}}}{4\gamma^{2}}\left(h^{\dagger}(L_{\bm{s}}^{2}+L_{p}^{2})W _{2}^{2}(\widetilde{q}_{t_{n,h_{n}}},\widetilde{p}_{t_{n+1}})+h^{\dagger} \delta_{\infty}^{2}+dh^{\dagger}\epsilon^{\dagger 2}\right)\] \[+ \frac{5L_{\bm{s}}^{2}h^{\dagger}C_{K^{\dagger}}}{2\gamma}\left(h^ {\dagger 2}e^{(3+\gamma)h^{\dagger}}\left(3\gamma d+M_{\bm{s}}^{2}\right)+h^{ \dagger}e^{2h^{\dagger}}d\right)\] \[\lesssim D_{\mathrm{KL}}(\pi_{t_{n,(n-1)h^{\dagger}}}\|\widetilde{\bm{ \pi}}_{t_{n,(n-1)h^{\dagger}}})+h^{\dagger}W_{2}^{2}(\widetilde{q}_{t_{n,h_{n }}},\widetilde{p}_{t_{n+1}})+h^{\dagger}\delta_{\infty}^{2}+dh^{\dagger} \epsilon^{\dagger 2}+e^{-K^{\dagger}}h^{\dagger 2}d,\]

and then sum over \(n\) to obtain

\[D_{\mathrm{KL}}(\pi_{t_{n,T}\dagger}\|\widetilde{\bm{\pi}}_{t_{n,T}\dagger})=D_{\mathrm{KL}}(\pi_{t_{n,N}\dagger^{\dagger}\mathbb{I}}\| \widetilde{\bm{\pi}}_{t_{n,N}\dagger^{\dagger}\mathbb{I}})\] \[\lesssim D_{\mathrm{KL}}(\pi_{t_{n,0}}\|\widetilde{\bm{\pi}}_{t_{n,0}})+N ^{\dagger}h^{\dagger}W_{2}^{2}(\widetilde{q}_{t_{n,h_{n}}},\widetilde{p}_{t_{n+1 }})+N^{\dagger}h^{\dagger}\delta_{\infty}^{2}+dN^{\dagger}h^{\dagger}\epsilon^{ \dagger 2}+e^{-K^{\dagger}}N^{\dagger}h^{\dagger 2}d\] \[= T^{\dagger}W_{2}^{2}(\widetilde{q}_{t_{n,h_{n}}},\widetilde{p}_{t_ {n+1}})+T^{\dagger}\delta_{\infty}^{2}+dT^{\dagger}\epsilon^{\dagger 2}+e^{-K^{\dagger}}T^{\dagger}h^{ \dagger}d.\]

Then, it is straightforward to see that when the following order of the parameters holds

\[T^{\dagger}=\mathcal{O}(1),\quad h^{\dagger}=\Theta(1),\quad N^{ \dagger}=\mathcal{O}(1),\] \[\epsilon^{\dagger}=\Theta(d^{-1/2}\delta),\quad M^{\dagger}= \mathcal{O}(d^{1/2}\delta^{-1}),\quad K^{\dagger}=\mathcal{O}(\log(d\delta^{-2 }))\]

and \(\delta_{\infty}\leq\delta\), we have

\[D_{\mathrm{KL}}(\pi_{t_{n,T}\dagger}\|\widetilde{\bm{\pi}}_{t_{n,T}\dagger})\lesssim W_{2}^{2}(\widetilde{q}_{t_{n,h_{n}}},\widetilde{p}_{t_{n+ 1}})+\delta^{2}.\]

**Lemma C.18**.: _Suppose \(T^{\dagger}\lesssim L_{p}^{-1/2}\), then we have_

\[\mathrm{TV}(\pi_{t_{n,T}\dagger},\widetilde{p}_{t_{n+1}})\leq\sqrt{D_{\mathrm{KL }}(\pi_{t_{n,T}\dagger}\|\widetilde{p}_{t_{n+1}})}\lesssim\frac{1}{L_{p}^{\frac {1}{\lambda}}(T^{\dagger})^{\frac{3}{2}}}W_{2}(\pi_{t_{n,0}},\widetilde{p}_{t_{n +1}})\lesssim W_{2}(\widetilde{q}_{t_{n,h_{n}}},\widetilde{p}_{t_{n+1}}).\]

Proof.: A complete proof of the Lemma above is presented in [111, Lemma 9], which is derived based on [145, Corollary 4.7 (1) ].

### Overall Error Bound

We are now ready to prove Theorem 3.5.

Proof of Theorem 3.5.: Notice that the interpolating corrector process \((\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}},\widetilde{\bm {v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}})\) is constructed to follow the same dynamics as the auxiliary corrector process \((\widetilde{\bm{u}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}},\widetilde{ \bm{v}}_{t_{n},n^{\dagger}h^{\dagger},\tau^{\dagger}})\) in the corrector step. Therefore, we have by data processing inequality that

\[\mathrm{TV}(\widehat{\pi}_{t_{n},T^{\dagger}}^{\widehat{\alpha}},\widetilde{ \pi}_{t_{n},T^{\dagger}}^{\widetilde{\alpha}})\leq\mathrm{TV}(\widehat{\pi}_{t _{n},0}^{\widehat{\alpha}},\widetilde{\pi}_{t_{n},0}^{\widetilde{\alpha}})= \mathrm{TV}(\widehat{q}_{t_{n},h_{n}},\widetilde{q}_{t_{n},h_{n}}),\] (C.32)

and again, since the interpolating predictor process \(\widetilde{\bm{y}}_{t_{n},n^{\dagger}h^{\dagger}}\) is constructed to follow the same dynamics as the auxiliary predictor process \(\widetilde{\bm{y}}_{t_{n},n^{\dagger}h^{\dagger}}\) in the predictor step, we further have by data processing inequality that

\[\mathrm{TV}(\widehat{q}_{t_{n},h_{n}},\widetilde{q}_{t_{n},h_{n}})\leq \mathrm{TV}(\widehat{q}_{t_{n},0},\widetilde{q}_{t_{n},0})=\mathrm{TV}( \widehat{q}_{t_{n}},\widetilde{p}_{t_{n}}).\] (C.33)

Furthermore, applying triangle inequality, Pinsker's inequality along with Theorem C.17 and Theorem C.7 proved above, we may upper bound the second term above as follows

\[\mathrm{TV}(\pi_{t_{n},T^{\dagger}},\widetilde{\pi}_{t_{n},T^{\dagger}})^{2} \lesssim D_{\mathrm{KL}}(\pi_{t_{n},T^{\dagger}}\|\widetilde{\pi}_{t_{n},T^{ \dagger}})\lesssim W_{2}^{2}(\widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1 }})+\delta^{2}\] (C.34)

Summarizing the above inequalities, we have

\[\mathrm{TV}(\widetilde{\pi}_{t_{n},T^{\dagger}}^{\widehat{\bm{u} }},\widetilde{p}_{t_{n+1}})^{2} =\mathrm{TV}(\widetilde{\pi}_{t_{n},T^{\dagger}}^{\widetilde{\bm{ u}}},\pi_{t_{n},T^{\dagger}}^{*,\bm{u}^{*}})^{2}\] (C.35) \[\leq\mathrm{TV}(\widetilde{\pi}_{t_{n},T^{\dagger}}^{\widetilde{ \bm{u}}},\pi_{t_{n},T^{\dagger}})^{2}+\mathrm{TV}(\pi_{t_{n},T^{\dagger}}^{ \bm{u}},\pi_{t_{n},T^{\dagger}}^{*,\bm{u}^{*}})^{2}\] \[\leq\mathrm{TV}(\widetilde{\pi}_{t_{n},T^{\dagger}},\pi_{t_{n},T^ {\dagger}})^{2}+\mathrm{TV}(\pi_{t_{n},T^{\dagger}},\pi_{t_{n},T^{\dagger}}^{ *})^{2}\] \[\lesssim W_{2}^{2}(\widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1 }})+\delta^{2}+W_{2}^{2}(\widetilde{q}_{t_{n},h_{n}},\widetilde{p}_{t_{n+1}})\] \[\lesssim de^{-K}+h_{n}^{2}\delta_{\infty}^{2}+d\epsilon^{2}h_{n}^ {2}+\delta^{2},\]

where the second last inequality is deduced from Theorem C.17 and Lemma C.18 and the last inequality is derived via Theorem C.7. Therefore, for any \(n\in[0:N-1]\), applying triangle inequality along with data processing inequality (_cf._ Theorem A.1) yields

\[\begin{split}\mathrm{TV}(\widehat{q}_{t_{n+1}},\widetilde{p}_{t _{n+1}})&=\mathrm{TV}(\widehat{\pi}_{t_{n},T^{\dagger}}^{ \widehat{\bm{u}}},\widetilde{p}_{t_{n+1}})\\ &\leq\mathrm{TV}(\widehat{\pi}_{t_{n},T^{\dagger}}^{\widehat{\bm{ u}}},\widetilde{\pi}_{t_{n},T^{\dagger}}^{\widehat{\bm{u}}})+\mathrm{TV}( \widetilde{\pi}_{t_{n},T^{\dagger}}^{\widehat{\bm{u}}},\widetilde{p}_{t_{n+1}} )\\ &\leq\mathrm{TV}(q_{t_{n}},\widetilde{p}_{t_{n}})+d^{1/2}e^{-K/2} +h_{n}\delta_{\infty}+d^{1/2}\epsilon h_{n}+\delta.\end{split}\] (C.36)

where the last inequality is derived by plugging in (C.32), (C.33) and (C.36). Applying Lemma A.9 and summing the inequalities above further give us that

\[\begin{split}\mathrm{TV}(\widehat{q}_{t_{N}},p_{\eta})& =\mathrm{TV}(\widehat{q}_{t_{N}},\widetilde{p}_{t_{N}})\\ &\lesssim\mathrm{TV}(\widehat{q}_{0},\widetilde{p}_{0})+\sum_{n=0} ^{N-1}\left(d^{1/2}e^{-\frac{K}{2}}+h_{n}\delta+d^{1/2}\epsilon h_{n}+\delta \right)\\ &\lesssim d^{1/2}e^{-T/2}+Nd^{1/2}e^{-K/2}+T\delta_{\infty}+d^{1/ 2}\epsilon T+\delta N.\end{split}\] (C.37)

By setting the parameters

\[T=\mathcal{O}(\log(d\delta^{-2})),\quad h=\Theta(1),\quad N=\mathcal{O}(\log(d \delta^{-2})),\]

\[\epsilon=\Theta\left(d^{-1/2}\delta\log^{-1}(d^{-1/2}\delta^{-1})\right),\quad M =\mathcal{O}(d^{1/2}\delta^{-1}\log(d^{1/2}\delta^{-1})),\quad K=\widetilde{ \mathcal{O}}(\log(d\delta^{-2})),\]

and letting \(\delta_{\infty}\lesssim\delta T^{-1}\lesssim\delta\log^{-1}(d\delta^{-2})\), we finally obtained the upper bound

\[\mathrm{TV}(\widehat{q}_{t_{N}},p_{\eta})^{2}\lesssim de^{-T}+N^{2}de^{-K}+ \delta^{2}+d\epsilon^{2}T^{2}\leq\delta^{2}\]

as desired.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have carefully reviewed the abstract and introduction to ensure that they accurately reflect the paper's contributions and scope.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of our work in Section 4 (Discussion and Conclusion).
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full set of assumptions and complete proofs for all theoretical results in the paper.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: As a theoretical paper, we do not include experiments.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: As a theoretical paper, we do not include experiments.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: As a theoretical paper, we do not include experiments.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: As a theoretical paper, we do not include experiments.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: As a theoretical paper, we do not include experiments.

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and have ensured that our research conforms to it.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: As a theoretical paper, we do not expect direct societal impacts of our work.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: As a theoretical paper, we do not release data or models that have a high risk for misuse.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: As a theoretical paper, we do not use existing assets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: As a theoretical paper, we do not introduce new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: As a theoretical paper, we do not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: As a theoretical paper, we do not involve crowdsourcing nor research with human subjects.