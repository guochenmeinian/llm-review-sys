# Isometric Quotient Variational Auto-Encoders for Structure-Preserving Representation Learning

 In Huh\({}^{1,*}\), Changwook Jeong\({}^{2,\dagger}\), Jae Myung Choe\({}^{1}\), Young-Gu Kim\({}^{1}\), Dae Sin Kim\({}^{1}\)

\({}^{1}\)CSE Team, Innovation Center, Samsung Electronics

\({}^{2}\)Graduate School of Semiconductor Materials and Devices Engineering, UNIST

\({}^{*}\)in.huh@samsung.com \({}^{\dagger}\)changwook.jeong@unist.ac.kr

###### Abstract

We study structure-preserving low-dimensional representation of a data manifold embedded in a high-dimensional observation space based on variational auto-encoders (VAEs). We approach this by decomposing the data manifold \(\mathcal{M}\) as \(\mathcal{M}=\mathcal{M}/G\times G\), where \(G\) and \(\mathcal{M}/G\) are a group of symmetry transformations and a quotient space of \(\mathcal{M}\) up to \(G\), respectively. From this perspective, we define the structure-preserving representation of such a manifold as a latent space \(\mathcal{Z}\) which is isometrically isomorphic (i.e., distance-preserving) to the quotient space \(\mathcal{M}/G\) rather \(\mathcal{M}\) (i.e., symmetry-preserving). To this end, we propose a novel auto-encoding framework, named _isometric quotient VAEs (IQVAEs)_, that can extract the quotient space from observations and learn the Riemannian isometry of the extracted quotient in an unsupervised manner. Empirical proof-of-concept experiments reveal that the proposed method can find a meaningful representation of the learned data and outperform other competitors for downstream tasks.

## 1 Introduction

There has been a common consensus that natural image datasets form a low-dimensional manifold \(\mathcal{M}\) embedded in a high-dimensional observation space \(\mathbb{R}^{d}\), i.e., \(\text{dim}(\mathcal{M})\ll d\)[6]. From this perspective, a good neural network is a mapping function that can recognize the underlying structure of the data manifold well [25]. A question that arises is what structures should be represented from the data.

For unsupervised learning task, especially in generative modeling, if we suppose the data manifold to be a Riemannian one, then preserving geometric structures in the representation space is a key consideration [1, 8, 25]. To make things more explicit, if \(\mathcal{Z}\) is a latent representation of \(\mathcal{M}\), then \(\mathcal{Z}\) should be isometrically isomorphic to \(\mathcal{M}\) in the sense of the Riemannian isometry. This means that an infinitesimal distance on the data manifold should be preserved in the latent space as well, promoting geodesic distances in the latent space that accurately reflect those in the data space. Recently, several papers [8, 40, 25] have suggested using the concept of Riemannian isometry for deep generative models, such as variational auto-encoders (VAEs) [22] and generative adversarial networks (GANs) [14], to obtain more meaningful and relevant representations. These concepts have been applied to improve latent space interpolations [30] or clustering [45] of data manifolds.

However, the Riemannian isometry does not tell the whole story of the data structure, particularly in the case of vision datasets. One common property of natural images is that there are symmetry transformations that preserve the semantic meaning of the given image. For example, if one applies a rotational transformation to a certain image, then its semantic meaning, e.g., its label, does not change. In this situation, what one really wants to do is represent the inherent geometry of the manifold, i.e., the geometry up to such symmetry transformations [29].

The concept of the inherent geometry can be formalized by using the notion of the principal bundle \(\mathcal{M}=\mathcal{M}/G\times G\), a fiber bundle that consists of the group \(G\) of symmetry transformations and a quotient space \(\mathcal{M}/G\) as the fiber and base spaces, respectively [16; 26]. In this case, the inherent geometry of \(\mathcal{M}\) up to \(G\) is indeed determined by its quotient \(\mathcal{M}/G\) solely; the quotient formulates an invariant structure of the data, thus a measure on \(\mathcal{M}/G\) gives a measure on \(\mathcal{M}\) that is invariant to the actions of \(G\). Therefore, one should find the Riemannian isometry of \(\mathcal{M}/G\) rather than \(\mathcal{M}\) for a geometric-meaningful representation of the dataset (see Figure 1). Nevertheless, to the best of our knowledge, there has been a lack of studies, at least in the field of generative models, that integrate both the concepts of the quotient space and its Riemannian geometry.

In this paper, we propose a novel auto-encoding framework, named _isometric quotient VAEs (IQVAEs)_, that can extract the quotient space from observations and learn its Riemannian isometry in an unsupervised manner, allowing for a correct representation of the data manifold and accurate measurement of distances between samples from the learned representation. The proposed IQVAE model can be applied to a variety of practical tasks such as downstream clustering and out-of-distribution (OoD) detection. We evaluate the model's performance using three datasets: rotated MNIST [44], mixed-type wafer defect maps (MixedWM38) [42] and cervical cancer cell images (SIPaKMeD) [32]. In summary, our contribution is threefold:

* We propose a novel approach for structure-preserving representation learning by viewing a data manifold as a principal bundle, and formulate it as an unsupervised learning task that finds a Riemannian isometry for the quotient of the data manifold.
* We introduce a practical method, called the IQVAE, which can learn such a structure-preserving representation in an unsupervised manner by using the auto-encoding framework.
* We demonstrate through experimental evaluations on various datasets that our proposed method can find useful representations and outperform other competing methods for downstream tasks such as classification, clustering, and OoD detection.

## 2 Related Works

### _G_-Invariance and Quotient Space Learning

Quotient space learning is highly related to the notion of the \(G\)-invariance. Data augmentation is the most common approach to deal with the \(G\)-invariance of label distributions [18]. To achieve such a symmetry more explicitly, various \(G\)-invariant and \(G\)-equivariant architectures have been proposed; [10] has proposed \(G\)-convolution beyond the conventional translational equivariance of convolutional neural networks. [12] has generalized it further for Lie groups. \(G\)-orbit pooling, proposed by [23], can be applied to both arbitrary discrete groups and continuous groups by discretizing them. Some research has attempted to discover the underlying invariances within data [4; 46; 31; 35]. These approaches can be integrated as a major component of auto-encoders to represent the quotient symmetry of the data manifold, as we will discuss later in Section 3.3.

More relevant to our work, [37] has proposed using the consistency regularization for VAEs to encourage consistency in the latent variables of two identical images with different poses. It can be considered as the unsupervised learning version of the data augmentation and does not guarantee the explicit \(G\)-invariance of the regularized latent space. [5] has disentangled image content from pose variables, e.g., rotations or translations under the VAE framework. However, it has some limitations,such as the requirement of using a specific decoder, the spatial transformer network [20]. It also has a problem that it cannot handle arbitrary groups beyond the rotation and translation. [11] has dealt with more complicated SO(3) group under the VAE framework. [29] has proposed the quotient auto-encoder (QAE) that can learn the quotient space of observations with respect to a given arbitrary group, extending early works in quotient images [34; 41]. However, it is a deterministic model and therefore cannot generate new samples easily. In addition, all the above-mentioned studies have not considered the Riemannian geometry of the learned latent representation.

### Rimeannian Geometry of Generative Models

Recent studies have proposed to view the latent space of generative models as a Riemannian manifold [1; 9; 7]. In this perspective, the metric tensor of the latent space is given by a pullback metric induced from the observation space through the generator function. In line with this theoretical perspective, the flat manifold VAE (FMVAE) [8] and its variant [25; 15] have been proposed to regularize the generator function to be a Riemannian isometry by matching the pullback metric with the Euclidean one. The regularized latent space preserves geodesic distances of the observation space, making it a more geometrically meaningful representation that can be useful. However, the FMVAE and its variants do not take into account the underlying group symmetry, which can lead to inaccurate estimation of distances between data points. For example, they could estimate a non-zero distance between two identical images with different rotational angles, which is unsuitable for clustering tasks.

## 3 Method

Suppose \(\mathbf{x}\in\mathcal{X}\subset\mathbb{R}^{d}\) is an observation where \(\mathcal{X}\) is an extrinsic view of \(\mathcal{M}\), i.e., the data manifold is realized via a mapping \(\pi:\mathcal{M}\rightarrow\mathbb{R}^{d}\) such that \(\mathcal{X}=\pi(\mathcal{M})\), and \(\mathbf{z}\in\mathcal{Z}\subset\mathbb{R}^{n}\) is its latent representation where \(n\ll d\). In addition, suppose \(G\) to be a group (of symmetry transformations) that its group elements \(g\in G\) naturally act on \(\mathcal{X}\) by a left group action \(\alpha:G\times\mathcal{X}\rightarrow\mathcal{X}\). We will denote \(\alpha(g,\mathbf{x})\) as \(g*\mathbf{x}\). Typical examples of \(G\) include the special orthogonal group SO(2), a group of 2-dimensional rotations that can also naturally act on \(d\)-dimensional images by rotating them.

### Auto-Encoders

The auto-encoder framework consists of two parameterized neural networks, an encoder \(\mu_{\theta}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}\) and a decoder \(\mu_{\phi}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{d}\). Vanilla auto-encoders find a low-dimensional compression of observations by minimizing the following reconstruction loss:

\[\mathbb{E}_{\mathbf{x}\sim p_{\mathcal{X}}(\mathbf{x})}\big{[}\mathcal{L}_{ \text{AE}}(\theta,\phi;\mathbf{x})\big{]}=\mathbb{E}_{\mathbf{x}\sim p_{ \mathcal{X}}(\mathbf{x})}\big{[}\|\mathbf{x}-\mu_{\phi}\circ\mu_{\theta}( \mathbf{x})\|_{2}^{2}\big{]},\] (1)

where \(p_{\mathcal{X}}(\mathbf{x})\) is the data distribution. The expectation over \(p_{\mathcal{X}}(\mathbf{x})\) can be computed via Monte Carlo (MC) estimation with finite samples \(\mathbf{X}=\{\mathbf{x}_{i}\}_{i=1}^{N}\) as \(\mathbb{E}_{p_{\mathcal{X}}(\mathbf{x})}\big{[}\mathcal{L}_{\text{AE}}(\theta,\phi;\mathbf{x})\big{]}\approx\ (1/N)\sum_{i=1}^{N}\|\mathbf{x}_{i}-\mu_{\phi}\circ\mu_{ \theta}(\mathbf{x}_{i})\|_{2}^{2}\). Unless otherwise mentioned, we will omit the expectation over \(p_{\mathcal{X}}(\mathbf{x})\) for brevity.

Although sufficiently deep auto-encoders can provide effective latent compression \(\mu_{\theta}(\mathbf{X})=\mathbf{Z}\subset\mathbb{R}^{n}\) of high-dimensional data, they tend to overfit and the learned latent manifolds are often inaccurate [25], i.e., they cannot consider the underlying geometry of data. This can lead to problems such as incorrect latent interpolation or poor performance in downstream tasks, in addition to the absence of the ability to generate new samples.

### Variational Auto-Encoders

The VAE [22] is a stochastic auto-encoding architecture belonging to the families of deep generative models. Contrary to vanilla auto-encoders, the VAE framework consists of two parameterized distributions \(q_{\theta}(\mathbf{z}|\mathbf{x})\) and \(p_{\phi}(\mathbf{x}|\mathbf{z})\) where the former and the latter are variational posterior and likelihood, respectively. VAEs try to maximize the marginal log-likelihood of observations by optimizing the following evidence lower bound:

\[\log p_{\phi}(\mathbf{x})\geq\mathbb{E}_{\mathbf{z}\sim q_{\theta}(\mathbf{z }|\mathbf{x})}\big{[}\log p_{\phi}(\mathbf{x}|\mathbf{z})\big{]}-D_{\text{KL} }\big{(}q_{\theta}(\mathbf{z}|\mathbf{x})\|p_{\mathcal{Z}}(\mathbf{z})\big{)} \triangleq-\mathcal{L}_{\text{VAE}}(\theta,\phi;\mathbf{x}),\] (2)

where \(p_{\mathcal{Z}}(\mathbf{z})\) is the latent prior and \(D_{\text{KL}}(\cdot,\cdot)\) is the Kullback-Leibler (KL) divergence. \(p_{\mathcal{Z}}(\mathbf{z})\) is often given by a normal distribution \(\mathcal{N}(\mathbf{z}|\mathbf{0},\mathbf{I}_{n})\) where \(\mathbf{0}\) is a zero vector and \(\mathbf{I}_{n}\) is \(n\times n\) identity matrix.

For vanilla VAEs, \(q_{\theta}(\mathbf{z}|\mathbf{x})\) is chosen as a multivariate Gaussian distribution with a diagonal covariance matrix \(\mathcal{N}(\mathbf{z}|\mu_{\theta}(\mathbf{x}),\mathrm{diag}[\sigma_{\theta}^{2 }(\mathbf{x})])\) and represented by a neural network encoder \((\mu_{\theta},\sigma_{\theta}):\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}\times \mathbb{R}^{n}_{+}\). A latent variable is sampled by using the reparameterization trick \(\mathbf{z}=\mu_{\theta}(\mathbf{x})+\sigma_{\theta}(\mathbf{x})\odot\epsilon\) where \(\epsilon\sim\mathcal{N}(\epsilon|\mathbf{0},\mathbf{I}_{n})\). Although \(p_{\phi}(\mathbf{x}|\mathbf{z})\) is chosen depending on the modeling of the data, it is often taken as a simple distribution such as a Gaussian with fixed variance, \(\mathcal{N}(\mathbf{x}|\mu_{\phi}(\mathbf{z}),\beta\mathbf{I}_{d})\), represented by a neural network decoder \(\mu_{\phi}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{d}\). In this case, the VAE objective (2) can be seen as a regularized version of (1) given by a sum of the stochastic auto-encoding reconstruction term \(\mathcal{L}_{\text{AE}}(\theta,\phi;\mathbf{x},\epsilon)=\|\mathbf{x}-\mu_{ \phi}(\mu_{\theta}(\mathbf{x})+\sigma_{\theta}(\mathbf{x})\odot\epsilon)\|_{2 }^{2}\) and \(\beta\)-weighted KL divergence \(\mathcal{L}_{\text{KL}}(\theta;\mathbf{x},\beta)=\beta D_{\text{KL}}( \mathcal{N}(\mathbf{z}|\mu_{\theta}(\mathbf{x}),\mathrm{diag}[\sigma_{\theta }^{2}(\mathbf{x})])\|\mathcal{N}(\mathbf{z}|\mathbf{0},\mathbf{I}_{n}))\) as follows:

\[\mathcal{L}_{\text{VAE}}(\theta,\phi;\mathbf{x},\beta)=\mathbb{E}_{\epsilon \sim\mathcal{N}(\epsilon)}\big{[}\mathcal{L}_{\text{AE}}(\theta,\phi;\mathbf{x },\epsilon)\big{]}+\mathcal{L}_{\text{KL}}(\theta;\mathbf{x},\beta).\] (3)

Practically, the expectation over \(\mathcal{N}(\epsilon)\) term is estimated via a single-point MC when the mini-batch size is sufficiently large [22]. When \(\beta\to 0\) and \(\sigma_{\theta}^{2}\to 0\), VAEs reduce to vanilla auto-encoders.

By minimizing (3), VAEs can learn the probabilistic process on the smooth latent space [13] that can easily generate plausible new samples by sampling a latent variable and decoding it, i.e., \(\mu_{\phi}(\mathcal{Z})\approx\mathcal{X}\). However, VAEs, like vanilla auto-encoders, do not ensure that the learned latent representation preserves the crucial information about the symmetry and geometry of the data manifold.

### Quotient Auto-Encoders

QAE [29] is a modification of the deterministic auto-encoding framework that can find the quotient of the observation set \(\mathrm{X}/G\) by replacing (1) as the following quotient reconstruction loss:

\[\mathcal{L}_{\text{QAE}}(\theta,\phi;\mathbf{x},G)=\inf_{g\in G}\|g*\mathbf{ x}-\mu_{\phi}\circ\mu_{\theta}(\mathbf{x})\|_{2}^{2}.\] (4)

As shown in (4), QAEs aim to minimize the set distance between the auto-encoded sample \(\mu_{\phi}\circ\mu_{\theta}(\mathbf{x})\) and \(G\)-orbit \(G*\mathbf{x}=\{g*\mathbf{x}|g\in G\}\) of the given \(\mathbf{x}\) rather than \(\mathbf{x}\) itself. It is noteworthy that \(G\)-orbit loses the information of \(G\) for a given \(\mathbf{x}\), which is the case of the natural quotient for \(\mathbf{x}\) up to \(G\). Thus, QAE can learn the quotient space of observations directly: for the ideal case of QAEs, i.e., when \(\mathcal{L}_{\text{QAE}}(\theta,\phi;\mathbf{x},G)\to 0\), the following property holds:

\[\mu_{\phi}\circ\mu_{\theta}(\mathbf{x})=\mu_{\phi}\circ\mu_{\theta}(\mathbf{x} =g*\mathbf{x})\triangleq\hat{\mathbf{x}},\quad\forall g\in G,\]

which means that the image \(\mu_{\phi}\circ\mu_{\theta}(\mathbf{X})\triangleq\hat{\mathbf{X}}\) does not contain any information on \(g\). In addition, QAEs typically adopt \(G\)-invariant architectures such as \(G\)-orbit pooling [23] (see Section B of Supplementary Material (SM) for details) for the encoder part to explicitly guarantee the following \(G\)-invariant latent representation:

\[\mu_{\theta}^{G}(\mathbf{x})=\mu_{\theta}^{G}(g*\mathbf{x})\triangleq\hat{ \mathbf{z}},\quad\forall g\in G,\]

where \(\mu_{\theta}^{G}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}\) is the \(G\)-invariant encoder. It is worth mentioning that these encoder architectures are unable to learn a meaningful representation of the data when using a vanilla auto-encoding framework, as the reconstruction loss (1) cannot be reduced enough; on the other hand, QAEs can use the explicitly \(G\)-invariant encoding architectures easily by taking advantage of the infimum in (4). This allows for both \(\hat{\mathbf{X}}\) and \(\mu_{\theta}^{G}(\mathbf{X})\triangleq\hat{\mathbf{Z}}\) to possess the invariant property up to \(G\).

### Isometric Quotient Variational Auto-Encoders

Although QAEs with \(G\)-invariant encoders can extract the underlying symmetry of observations, they still have the following drawbacks. First, QAEs are deterministic and may overfit to limited observations, similar to vanilla auto-encoders. Second, the Riemannian geometry of the extracted quotient manifold may not be preserved in the latent space. To address these issues, we present a stochastic version of QAEs called QVAEs, and further improve them with a novel framework for isometric learning called IQVAEs.

QVAEs.Inspired by the connection among (1), (3), and (4), we propose the stochastic version of QAEs (QVAEs) which minimizes the following stochastic quotient objective:

\[\mathcal{L}_{\text{QVAE}}(\theta,\phi;\mathbf{x},\beta,G)\triangleq\mathbb{E}_{ \epsilon\sim\mathcal{N}(\epsilon)}\big{[}\mathcal{L}_{\text{QAE}}(\theta,\phi; \mathbf{x},\epsilon,G)\big{]}+\mathcal{L}_{\text{KL}}(\theta;\mathbf{x}, \beta,G),\] (5)

[MISSING_PAGE_FAIL:5]

the following regularization should be added to (5) to make the decoder be a Riemannian isometry between the latent representation \(\hat{\mathcal{Z}}\) and quotient \(\hat{\mathcal{X}}\):

\[\mathcal{L}_{\text{ISO}}(\theta,\phi;\mathbf{x},\mathbf{H}_{\hat{ \mathcal{Z}}},\lambda)=\lambda\mathbb{E}_{\hat{\mathbf{z}}\sim q_{\theta}( \hat{\mathbf{z}}|\mathbf{x})}\|\mathbf{J}_{\mu_{\phi}}(\hat{\mathbf{z}}) \mathbf{J}_{\mu_{\phi}}(\hat{\mathbf{z}})-\mathbf{H}_{\hat{\mathcal{Z}}}(\hat{ \mathbf{z}})\|_{F},\] (6)

where \(\mathbf{J}_{\mu_{\phi}}(\hat{\mathbf{z}})\) is Jacobian of \(\mu_{\phi}\) at \(\hat{\mathbf{z}}\), \(\lambda\) is a hyper-parameter, and \(\mathbf{H}_{\hat{\mathcal{Z}}}(\hat{\mathbf{z}})\) is a metric tensor of \(\hat{\mathcal{Z}}\).

There are two things that should be clarified for practical computation of (6). The first aspect to consider is the selection of the Riemannian metric \(\mathbf{H}_{\hat{\mathcal{Z}}}(\hat{\mathbf{z}})\). A commonly used and particularly useful choice of \(\mathbf{H}_{\hat{\mathcal{Z}}}(\hat{\mathbf{z}})\) is the scaled Euclidean metric in \(\mathbb{R}^{n}\), i.e., \(\mathbf{H}_{\hat{\mathcal{Z}}}(\hat{\mathbf{z}})=\mathbf{C}_{n}=c^{2}\mathbf{ I}_{n}\) where \(c\) is a constant. This is because it is the case that using the Euclidean distance1 on the latent representation \(d_{\hat{\mathcal{Z}}}(\hat{\mathbf{z}}_{0},\hat{\mathbf{z}}_{1})=\sqrt{( \hat{\mathbf{z}}_{1}-\hat{\mathbf{z}}_{0})^{\text{T}}\mathbf{I}_{n}(\hat{ \mathbf{z}}_{1}-\hat{\mathbf{z}}_{0})}\) can preserve the geodesic distance on the quotient manifold \(d_{\hat{\mathcal{X}}}(\hat{\mathbf{x}}_{0},\hat{\mathbf{x}}_{1})\) up to \(c\). The constant \(c\) can be regarded as either a pre-defined hyper-parameter or a learnable one. More generally, one can use any parameterized symmetric positive-definite matrix \(\mathbf{C}_{n}\) for \(\mathbf{H}_{\hat{\mathcal{Z}}}(\hat{\mathbf{z}})\). Note that such matrices can be achieved from an arbitrary learnable parameterized \(n\times n\) matrix \(\mathbf{M}_{n}\) by using the Cholesky decomposition as follows:

Footnote 1: We assume the straight path between \(\hat{\mathbf{z}}_{0}\) and \(\hat{\mathbf{z}}_{1}\) is an on-manifold path for every pair \(\hat{\mathbf{z}}_{0},\hat{\mathbf{z}}_{1}\in\hat{\mathcal{Z}}\).

\[\mathbf{C}_{n}=\mathbf{L}_{n}(\mathbf{L}_{n})^{\text{T}},\quad \mathbf{L}_{n}=\text{lower}[\mathbf{M}_{n}]-\text{diag}[\mathbf{M}_{n}]+ \text{diag}[\mathbf{M}_{n}]^{2}+\varepsilon^{2}\mathbf{I}_{n},\] (7)

where \(\mathbf{L}_{n}\) is a real lower triangular matrix with all positive diagonal entries and \(\varepsilon\) is a small non-zero constant. In this case, using the Mahalanobis distance on the latent space \(d_{\hat{\mathcal{Z}}}(\hat{\mathbf{z}}_{0},\hat{\mathbf{z}}_{1})=\sqrt{(\hat{ \mathbf{z}}_{1}-\hat{\mathbf{z}}_{0})^{\text{T}}\mathbf{C}_{n}(\hat{\mathbf{z }}_{1}-\hat{\mathbf{z}}_{0})}\) preserves the geodesic distance of the quotient space of the observation manifold \(d_{\hat{\mathcal{X}}}(\hat{\mathbf{x}}_{0},\hat{\mathbf{x}}_{1})\).

The second aspect to consider is the sampling from \(q_{\theta}(\hat{\mathbf{z}}|\mathbf{x})\). It can be considered as Gaussian vicinal distribution for finite observation samples that smoothly fills the latent space where data is missing. In addition to that, following [8], we use the mix-up vicinal distribution [39] to effectively regularize the entire space of interest to be isometric. As a result, the tractable form of (6) is equal to2:

Footnote 2: In (8), we temporarily retrieve the expectation on observations to make things more explicit.

\[\lambda\mathbb{E}_{\mathbf{x}_{i,j}\sim p_{X},\epsilon_{i,j}\sim \mathcal{N},\alpha\sim[0,1]}\|\mathbf{J}_{\mu_{\phi}}^{\text{T}}(f_{\theta}^{ \alpha}(\mathbf{x}_{i,j},\epsilon_{i,j}))\mathbf{J}_{\mu_{\phi}}(f_{\theta}^{ \alpha}(\mathbf{x}_{i,j},\epsilon_{i,j}))-\mathbf{C}_{n}\|_{F},\] (8)

where \(f_{\theta}^{\alpha}(\mathbf{x}_{i,j},\epsilon_{i,j})=(1-\alpha)\hat{\mathbf{z} }_{i}+\alpha\hat{\mathbf{z}}_{j}\) is the latent mix-up for \(\hat{\mathbf{z}}_{i,j}=\mu_{\theta}^{G}(\mathbf{x}_{i,j})+\sigma_{\theta}^{G} (\mathbf{x}_{i,j})\odot\epsilon_{i,j}\). Practically, it is computed by sampling a mini-batch of latent variables, i.e., \(\{\hat{\mathbf{z}}_{i}\}_{i=1}^{N}=\{\mu_{\theta}^{G}(\mathbf{x}_{i})+\sigma_{ \theta}^{G}(\mathbf{x}_{i})\odot\epsilon_{i}\}_{i=1}^{N}\), shuffling it for \(\{\hat{\mathbf{z}}_{j}\}_{j=1}^{N}=\texttt{shuffle}(\{\hat{\mathbf{z}}_{j}\}_ {i=1}^{N})\), and mixing-up them.

Iqvae.We define the IQVAE as a class of QVAEs whose objective function is given by the sum of the variational quotient objective \(\mathcal{L}_{\text{QVAE}}\) (5) and Riemannian isometry \(\mathcal{L}_{\text{ISO}}\) (8) as follows:

\[\mathcal{L}_{\text{IQVAE}}(\theta,\phi;\mathbf{x},\beta,\lambda,G)\triangleq \mathcal{L}_{\text{QVAE}}(\theta,\phi;\mathbf{x},\beta,G)+\mathcal{L}_{\text{ ISO}}(\theta,\phi,\mathbf{C}_{n};\mathbf{x},\lambda).\] (9)

The proposed IQVAE optimization procedure is summarized in Algorithm 1 (see Section A of SM for additional tips). In the IQVAE, we typically set \(\mathbf{C}_{n}=c^{2}\mathbf{I}_{n}\). For clarity, we will refer to the IQVAE with (7) as IQVAE-M, to distinguish it from the basic version.

``` Input: data \(\{\mathbf{x}_{i}\}_{i=1}^{N}\), hyper-parameters (\(\beta,\lambda\)), group \(G\), \(G\)-invariant encoders (\(\mu_{\theta}^{G},\sigma_{\theta}^{G}\)), decoder \(\mu_{\phi}\) Initialize \(\theta\), \(\phi\), \(\mathbf{C}_{n}\) while training do  Sample \(\{\alpha_{i}\sim[0,1]\}^{N}\), \(\{\epsilon_{i}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\}_{i=1}^{N}\) Compute \(\{\mu_{\theta}^{G},\sigma_{\theta}^{G}\}_{i=1}^{N}=\{\mu_{\theta}^{G}(\mathbf{x}_{i })\,\sigma_{\theta}^{G}(\mathbf{x}_{i})\}_{i=1}^{N}\) Sample \(\{\mathbf{z}_{i}\}_{i=1}^{N}=\{\mu_{\theta}^{G}+\sigma_{\theta}^{G}\odot \epsilon_{i}\}_{i=1}^{N}\) Shuffle \(\{\mathbf{z}_{i}\}_{i=1}^{N}=\texttt{shuffle}(\{\mathbf{z}_{i}\}_{i=1}^{N})\)  Augment \(\{\hat{\mathbf{z}}_{i}\}_{i=1}^{N}=\{(1-\alpha_{i})\mathbf{z}_{i}+\alpha_{i} \mathbf{z}_{j}\}_{i=j=1}^{i=j=N}\) Compute \(\mathcal{L}_{\text{QAE}}\equiv\sum_{i=1}^{N}\min_{j\in G}\|g\ast\mathbf{x}_{i }-\mu_{\phi}(\mathbf{z}_{i})\|_{2}^{2}\) Compute \(\mathcal{L}_{\text{KL}}=\sum_{i=1}^{N}D_{\text{KL}}(\mathcal{N}(\mu_{\theta}^{G},\text{diag}[\sigma_{\theta}^{G}]^{2})\|\mathcal{N}(\mathbf{0},\mathbf{I}_{n}))\) Compute \(\{\mathbf{J}_{\mu_{\phi}}^{\text{T}}\}_{i=1}^{N}=\{\mathbf{J}_{\mu_{\phi}}( \tilde{\mathbf{z}}_{i})\}_{i=1}^{N}\) Compute \(\{\mathbf{J}_{\mu_{\phi}}^{\text{T}}\}_{i=1}^{N}=\{\mathbf{J}_{\mu_{\phi}}( \tilde{\mathbf{z}}_{i})\}_{i=1}^{N}\) Compute \(\mathcal{L}_{\text{ISO}}=\sum_{i=1}^{N}\|(\mathbf{J}_{\mu_{\phi}}^{\text{T}}) ^{\text{T}}\mathbf{J}_{\mu_{\phi}}^{\text{T}}-\mathbf{C}_{n}\|_{F}\) Optimize \((\mathcal{L}_{\text{QAE}}+\beta\mathcal{L}_{\text{KL}}+\lambda\mathcal{L}_{ \text{ISO}})/N\) w.r.t \(\theta,\phi\) endwhile ```

**Algorithm 1** IQVAE

## 4 Experiments

We compared our proposed QVAE and IQVAE with six different competitors: the auto-encoder (AE), VAE, \(\beta\)-VAE [19

[MISSING_PAGE_FAIL:7]

While the QVAE shows a more convincing interpolation compared to the VAE, it also shows abrupt changes at \(t=7\) and \(t=8\). The IQVAE shows the smoothest interpolation between digits 1 and 7 compared to the other models owing to the isometry regularization. The smooth interpolation of the IQVAE is also quantitatively confirmed in Figure 5 (b) which compares the geometric volume measures along each linear path of the QVAE and IQVAE. The volume measure is computed via \(\sqrt{\det\mathbf{J}_{\mu_{\phi}}^{\mathrm{T}}(\mathbf{z}(t))\mathbf{J}_{\mu_{ \phi}}(\mathbf{z}(t))}\) and can be viewed as a local infinitesimal volume ratio between the latent and observation spaces. The volume measure of the QVAE shows a clear peak near \(t=8\), while that of the IQVAE is nearly constant.

Evidence for isometry.To demonstrate that the learned decoding function of IQVAEs up-holds the desired Riemannian isometry, we evaluated the condition numbers of the pull-back metric tensors and the Riemannian volume measures for all test samples across models. The condition number is defined as the ratio \(\lambda_{\text{max}}/\lambda_{\text{min}}\), where \(\lambda_{\text{max}}\) and \(\lambda_{\text{min}}\) respectively represent the maximum and minimum eigenvalues of the pull-back metric, denoted as \(\mathbf{J}_{\mu_{\phi}}^{\mathrm{T}}(\mathbf{z})\mathbf{J}_{\mu_{\phi}}( \mathbf{z})\). All computed volume measures are normalized by their average values. Therefore, condition numbers and normalized volume measures approaching 1.0, accompanied by minimal variances, indicate that the learned latent representation is isometrically isomorphic to Euclidean space. In other words, the learned pull-back metric aligns with the Euclidean metric. As shown in Figure 6, the IQVAE presents condition numbers and volume measures close to 1.0, exhibiting trivial variances compared to other models.

Sample efficiency comparison.Table 2 compares the sample efficiency of IQVAE and the most comparable baseline, QAE, in terms of classification accuracy. The results demonstrate the effectiveness of the proposed method as IQVAE shows more robust performance with respect to variations in the training sample size.

Test-time orbit augmentation.The current QVAE and IQVAE require more computational cost than vanilla VAE due to the \(G\)-orbit pooling encoders, which augment the number of observations \(|G|\) times larger by expanding a data point \(\mathbf{x}\) as a \(G\)-orbit \(\bar{G}*\mathbf{x}\). It might be a bottleneck, especially when training the proposed model with larger-scale datasets. To resolve this issue, we suggest the test-time orbit augmentation strategy, which involves using a coarse step size when discretizing the group parameters (e.g., rotation angle) during training, and then switching to a finer step size during the inference phase. Table 3 compares wall-clock training

\begin{table}
\begin{tabular}{c c c} \hline \hline Sample size & QAE + SVM & IQVAE + SVM \\ \hline
5,000 & 83.9\(\pm\)0.7 & 87.6\(\pm\)0.6 \\
10,000 & 85.5\(\pm\)0.8 & 89.8\(\pm\)1.0 \\
60,000 & 91.8\(\pm\)0.6 & 92.9\(\pm\)0.3 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Classification test accuracies for \(2\pi\)-rotated MNIST with varying training sample sizes.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & Time [s] & \(k\)-means & GMM \\ \hline VAE & 1.12 & 11.4\(\pm\)1.2 & 14.7\(\pm\)1.6 \\ QVAE (36-36) & 10.17 & 53.9\(\pm\)7.2 & 66.3\(\pm\)6.5 \\ QVAE (12-12) & 4.06 & 50.8\(\pm\)2.6 & 59.2\(\pm\)1.5 \\ QVAE (12-36) & 4.06 & 54.6\(\pm\)2.8 & 63.8\(\pm\)3.9 \\ IQVAE (36-36) & 13.22 & 70.9\(\pm\)2.9 & 72.9\(\pm\)1.5 \\ IQVAE (12-12) & 6.11 & 59.6\(\pm\)2.7 & 60.9\(\pm\)1.8 \\ IQVAE (12-36) & 6.11 & 64.3\(\pm\)2.1 & 65.0\(\pm\)2.1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Training wall-clock time per epoch and clustering ARIs of VAEs, QVAEs, and IQVAEs. We used a single V100 32GB GPU.

Figure 5: (a) Linear interpolations between digits 1 and 7 for the VAE, QVAE, and IQVAE. (b) The geometric volume measures along each linear path of the QVAE and IQVAE.

Figure 6: (left) Condition numbers and (right) volume measures for the \(2\pi\)-rotated MNIST for the VAE, QVAE, and IQVAE.

[MISSING_PAGE_FAIL:9]

class-conditional mean latent vector of learned in-distribution sets, in terms of the Mahalanobis distance [24] (see Section G of SM for details). Table 5 shows the comparison of competing models using standard metrics (AUCROC and AUPRC) for threshold-based OoD detection, demonstrating that the proposed IQVAE excels in the OoD tasks as it accurately learns the data structure.

### SIPaKMeD

The SIPaKMeD dataset includes 4,049 single-cell Pap smear images for cervical cancer diagnosis and is split into 5 classes [32] (see Section C of SM for examples). We assume O(2) symmetry for the SIPaKMeD dataset. We resized the original SIPaKMeD images to size 32 \(\times\) 32 for better usability. In accordance with the original data splitting, 3,549 samples were used for training and 500 samples for testing.

IQVAE-M.As we did with rotated MNIST, we conducted the same experiment on the SIPaKMeD dataset. Furthermore, we also assessed the proposed IQVAE-M that incorporates a learnable flexible Riemannian metric as described in (7) and utilizes the latent Mahalanobis distance when computing the radial basis function (RBF) kernel \(K:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}\) of SVMs as \(K(\mathbf{z}_{0},\mathbf{z}_{1})=\exp\left(-(\mathbf{z}_{1}-\mathbf{z}_{0})^ {T}\mathbf{C}_{n}(\mathbf{z}_{1}-\mathbf{z}_{0})\right)\). To quantitatively analyze the learned representations, we present the downstream task performance of the competing models on the SIPaKMeD dataset in Table 6 (see Section E of SM for UMAP visualizations). As shown in Table 6, the proposed methods exhibit superior performance compared to other models4.

Footnote 4: When using the Euclidean RBF (\(\mathbf{C}_{n}=\mathbf{I}_{n}\)) for SVMs of the IQVAE-M, the accuracy decreases to 81.9; it shows the usefulness of the learned metric and the corresponding Mahalanobis RBF.

## 5 Conclusion and Limitation

We have proposed and demonstrated the effectiveness of IQVAE, a simple yet effective approach that maintains symmetry and geometry in data. However, our work has two main limitations as follows.

Predefined groups of symmetry transformations.We assume the group structure of a given dataset is known in advance. This group invariance is represented using the quotient auto-encoding framework with a Riemannian isometry. However, several recent papers have delved into learning an unknown group directly from data. For example, [35] presents a novel neural network that identifies bispectral invariants. [31] employs Lie algebra to find underlying Lie group invariances. [46] tackles the disentanglement challenge with VAEs, aiming to learn the latent space in terms of one-parameter subgroups of Lie groups. Integrating these methods with our IQVAEs learning approach could be a promising direction for future research.

Euclidean metric tensors.We focus on cases where the metric tensor of the observation space is Euclidean. If the intrinsic dimensionality of the image manifold is significantly lower than that of the ambient observation space, the Euclidean metric of the observation space can well serve as a Riemannian metric for the intrinsic geometry of the image manifold. This reasoning supports our use of the pullback metric as a metric tensor for the latent space. However, when constructing a latent representation that effectively addresses a specific task, a specialized metric tensor might be more appropriate than the standard Euclidean metric of the ambient space. For instance, when tasks involve capturing subtle variations in a local image patch, the pullback metric derived from the entire observation dimensions may not provide the most efficient geometry. In such cases, a specialized metric structure better suited for capturing these local variations should be considered. In this context, a task-specific metric can be determined using prior knowledge about the tasks [27]. Alternatively, IQVAEs can semi-supervisedly learn this specialized metric with minimal labeled data, leveraging metric learning concepts [3]. This approach holds significant potential for future research, as it allows the model to tailor its representation to the specific requirements of the task, thereby improving overall performance.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Method & \(k\)-means & GMM & SVM & RF \\ \hline AE & 33.4\(\pm\)0.4 & 35.6\(\pm\)1.0 & 78.6\(\pm\)0.5 & 76.9\(\pm\)1.1 \\ VAE & 33.9\(\pm\)1.5 & 34.0\(\pm\)1.7 & 79.4\(\pm\)1.0 & 76.5\(\pm\)2.1 \\ \(\beta\)-VAE & 34.4\(\pm\)1.5 & 33.7\(\pm\)1.1 & 79.1\(\pm\)1.0 & 76.6\(\pm\)1.3 \\ CRVAE & 24.9\(\pm\)3.6 & 31.9\(\pm\)1.3 & 77.1\(\pm\)0.7 & 80.6\(\pm\)1.1 \\ QAE & 40.9\(\pm\)1.9 & 41.1\(\pm\)1.9 & 80.5\(\pm\)9.9 & 81.6\(\pm\)0.7 \\ FMVAE & 39.6\(\pm\)0.3 & 37.9\(\pm\)1.1 & 80.0\(\pm\)0.8 & 75.7\(\pm\)1.2 \\ QVAE & 40.6\(\pm\)3.2 & 40.6\(\pm\)2.5 & 82.2\(\pm\)0.8 & **81.7\(\pm\)0.9** \\ IQVAE & 39.3\(\pm\)0.4 & 44.7\(\pm\)2.0 & 82.7\(\pm\)0.3 & 80.9\(\pm\)0.7 \\ IQVAE-M & **43.0\(\pm\)1.1** & **46.5\(\pm\)2.9** & **84.3\(\pm\)0.7** & **81.6\(\pm\)1.8** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Clustering ARIs and classification test accuracies for SIPaKMeD (repeated five times).

## Acknowledgments and Disclosure of Funding

Changwook Jeong sincerely acknowledges the support from Samsung Electronics Co., Ltd (IO221027-03271-01, IO230220-05060-01) and is further supported by the Technology Innovation Program (20020803, RS-2023-00231956) and Korea Institute for Advancement of Technology (KIAT) grant (P0023703), both funded by the MOTIE, Korea (1415180307, 1415187475), the National Research Foundation of Korea (NRF) grant (RS-2023-00257666) funded by the MSIT, Korea, the IC Design Education Center (IDEC), Korea, and the research project funds (1.220125.01, 1.230063.01) and supercomputing center of UNIST.

## References

* [1] Georgios Arvanitidis, Lars Kai Hansen, and Soren Hauberg. Latent space oddity: on the curvature of deep generative models. In _International Conference on Learning Representations_, 2018.
* [2] Georgios Arvanitidis, Soren Hauberg, and Bernhard Scholkopf. Geometrically enriched latent spaces. In _International Conference on Artificial Intelligence and Statistics_, pages 631-639. PMLR, 2021.
* [3] Aurelien Bellet, Amaury Habrard, and Marc Sebban. _Metric learning_. Springer, 2022.
* [4] Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. _Advances in Neural Information Processing Systems_, 33:17605-17616, 2020.
* [5] Tristan Bepler, Ellen Zhong, Kotaro Kelley, Edward Brignole, and Bonnie Berger. Explicitly disentangling image content from translation and rotation with spatial-VAE. _Advances in Neural Information Processing Systems_, 32, 2019.
* [6] Pratik Prabhanjan Brahma, Dapeng Wu, and Yiyuan She. Why deep learning works: A manifold disentanglement perspective. _IEEE Transactions on Neural Networks and Learning Systems_, 27(10):1997-2008, 2015.
* [7] Clement Chadebec and Stephanie Allassonnniere. A geometric perspective on variational autoencoders. _arXiv preprint arXiv:2209.07370_, 2022.
* [8] Nutan Chen, Alexej Klushyn, Francesco Ferroni, Justin Bayer, and Patrick Van Der Smagt. Learning flat latent manifolds with VAEs. In _International Conference on Machine Learning_, pages 1587-1596. PMLR, 2020.
* [9] Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, and Patrick Smagt. Metrics for deep generative models. In _International Conference on Artificial Intelligence and Statistics_, pages 1540-1550. PMLR, 2018.
* [10] Taco Cohen and Max Welling. Group equivariant convolutional networks. In _International Conference on Machine Learning_, pages 2990-2999. PMLR, 2016.
* [11] Luca Falorsi, Pim De Haan, Tim R Davidson, Nicola De Cao, Maurice Weiler, Patrick Forre, and Taco S Cohen. Explorations in homeomorphic variational auto-encoding. In _ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models_, 2021.
* [12] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to Lie groups on arbitrary continuous data. In _International Conference on Machine Learning_, pages 3165-3176. PMLR, 2020.
* [13] Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf. From variational to deterministic autoencoders. In _International Conference on Learning Representations_, 2020.
* [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.

* [15] Amos Gropp, Matan Atzmon, and Yaron Lipman. Isometric autoencoders. _arXiv preprint arXiv:2006.09289_, 2020.
* [16] Jihun Ham and Daniel D Lee. Separating pose and expression in face images: a manifold learning approach. _Neural Information Processing-Letters and Reviews_, 11(4):91-100, 2007.
* [17] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector machines. _IEEE Intelligent Systems and their applications_, 13(4):18-28, 1998.
* [18] A Hernandez-Garcia, P Konig, and TC Kietzmann. Learning robust visual representations using data augmentation invariance. In _2019 Conference on Cognitive Computational Neuroscience_, pages 44-47. [Sl: snl], 2019.
* [19] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. \(\beta\)-VAE: Learning basic visual concepts with a constrained variational framework. In _International Conference on Learning Representations_, 2017.
* [20] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. _Advances in Neural Information Processing Systems_, 28, 2015.
* [21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [22] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [23] Dmitry Laptev, Nikolay Savinov, Joachim M Buhmann, and Marc Pollefeys. Ti-pooling: transformation-invariant pooling for feature learning in convolutional neural networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 289-297, 2016.
* [24] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _Advances in neural information processing systems_, 31, 2018.
* [25] Yonghyeon Lee, Sangwoong Yoon, Minjun Son, and Frank C Park. Regularized autoencoders for isometric representation learning. In _International Conference on Learning Representations_, 2022.
* [26] Jongwoo Lim, Jeffrey Ho, Ming-Hsuan Yang, Kuang-chih Lee, and David Kriegman. Image clustering with metric, local linear structure, and affine symmetry. In _European Conference on Computer Vision_, pages 456-468. Springer, 2004.
* [27] Laura Manduchi, Kieran Chin-Cheong, Holger Michel, Sven Wellmann, and Julia Vogt. Deep conditional Gaussian mixture model for constrained clustering. _Advances in Neural Information Processing Systems_, 34:11303-11314, 2021.
* [28] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. UMAP: Uniform manifold approximation and projection. _Journal of Open Source Software_, 3(29):861, 2018.
* [29] Eloi Mehr, Andre Lieutier, Fernando Sanchez Bermudez, Vincent Guitteny, Nicolas Thome, and Matthieu Cord. Manifold learning in quotient spaces. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 9165-9174, 2018.
* [30] Mike Yan Michelis and Quentin Becker. On linear interpolation in the latent space of deep generative models. In _ICLR 2021 Workshop on Geometrical and Topological Representation Learning_, 2021.
* [31] Artem Moskalev, Anna Sepliarskaia, Ivan Sosnovik, and Arnold Smeulders. LieGG: Studying learned Lie group generators. _Advances in Neural Information Processing Systems_, 35:25212-25223, 2022.

* [32] Marina E Plissiti, Panagiotis Dimitrakopoulos, Giorgos Sfikas, Christophoros Nikou, O Krikoni, and Antonia Charchanti. SIPaKMeD: A new dataset for feature and image based classification of normal and pathological cervical cells in Pap smear images. In _2018 25th IEEE International Conference on Image Processing (ICIP)_, pages 3144-3148. IEEE, 2018.
* [33] Douglas A Reynolds. Gaussian mixture models. _Encyclopedia of Biometrics_, 741(659-663), 2009.
* [34] Tammy Riklin-Ravi and Amnon Shashua. The quotient image: Class based recognition and synthesis under varying illumination conditions. In _Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)_, volume 2, pages 566-571. IEEE, 1999.
* [35] Sophia Sanborn, Christian A Shewmake, Bruno Olshausen, and Christopher J Hillar. Bispectral neural networks. In _International Conference on Learning Representations_, 2023.
* [36] Hang Shao, Abhishek Kumar, and P Thomas Fletcher. The Riemannian geometry of deep generative models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops_, pages 315-323, 2018.
* [37] Samarth Sinha and Adji Bousso Dieng. Consistency regularization for variational auto-encoders. _Advances in Neural Information Processing Systems_, 34:12943-12954, 2021.
* [38] Douglas Steinley. Properties of the Hubert-Arable adjusted Rand index. _Psychological methods_, 9(3):386, 2004.
* [39] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In _International Conference on Machine Learning_, pages 6438-6447. PMLR, 2019.
* [40] Binxu Wang and Carlos R Ponce. A geometric analysis of deep generative image models and its applications. In _International Conference on Learning Representations_, 2021.
* [41] Haitao Wang, Stan Z Li, and Yangsheng Wang. Generalized quotient image. In _Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004._, volume 2, pages II-II. IEEE, 2004.
* [42] Junliang Wang, Chuqiao Xu, Zhengliang Yang, Jie Zhang, and Xiaoou Li. Deformable convolutional networks for efficient mixed-type wafer defect pattern recognition. _IEEE Transactions on Semiconductor Manufacturing_, 33(4):587-596, 2020.
* [43] Rui Wang and Nan Chen. Wafer map defect pattern recognition using rotation-invariant features. _IEEE Transactions on Semiconductor Manufacturing_, 32(4):596-604, 2019.
* [44] Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant CNNs. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 849-858, 2018.
* [45] Tao Yang, Georgios Arvanitidis, Dongmei Fu, Xiaogang Li, and Soren Hauberg. Geodesic clustering in deep generative models. _arXiv preprint arXiv:1809.04747_, 2018.
* [46] Xinqi Zhu, Chang Xu, and Dacheng Tao. Commutative Lie group VAE for disentanglement learning. In _International Conference on Machine Learning_, pages 12924-12934. PMLR, 2021.