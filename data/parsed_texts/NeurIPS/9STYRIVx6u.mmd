# Convergence of Mean-field Langevin dynamics:

Time-space discretization, stochastic gradient,

and variance reduction

 Taiji Suzuki\({}^{1,2}\),  Denny Wu\({}^{3,4}\),  Atsushi Nitanda\({}^{2,5}\)

\({}^{1}\)University of Tokyo, \({}^{2}\)RIKEN AIP, \({}^{3}\)New York University,

\({}^{4}\)Flatiron Institute, \({}^{5}\)Kyushu Institute of Technology

taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu, nitanda@ai.kyutech.ac.jp

###### Abstract

The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide a general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient. To demonstrate the wide applicability of our framework, we establish quantitative convergence rate guarantees to the regularized global optimal solution for \((i)\) a wide range of learning problems such as mean-field neural network and MMD minimization, and \((ii)\) different gradient estimators including SGD and SVRG. Despite the generality of our results, we achieve an improved convergence rate in both the SGD and SVRG settings when specialized to the standard Langevin dynamics.

## 1 Introduction

In this work we consider the mean-field Langevin dynamics (MFLD) given by the following McKean-Vlasov stochastic differential equation:

\[\mathrm{d}X_{t}=-\nabla\frac{\delta F(\mu_{t})}{\delta\mu}(X_{t})\mathrm{d}t+ \sqrt{2\lambda}\mathrm{d}W_{t},\] (1)

where \(\mu_{t}=\mathrm{Law}(X_{t})\), \(F:\mathcal{P}_{2}(\mathbb{R}^{d})\rightarrow\mathbb{R}\) is a convex functional, \(W_{t}\) is the \(d\)-dimensional standard Brownian motion, and \(\frac{\delta F}{\delta\mu}\) denotes the first-variation of \(F\). Importantly, MFLD is the Wasserstein gradient flow that minimizes an entropy-regularized convex functional as follows:

\[\min_{\mu\in\mathcal{P}_{2}}\{F(\mu)+\lambda\mathrm{Ent}(\mu)\}.\] (2)

While the above objective can also be solved using other methods such as double-loop algorithms based on iterative linearization (Nitanda et al., 2020, 2023; Oko et al., 2022), MFLD remains attractive due to its simple structure and connection to neural network optimization. Specifically, the learning of two-layer neural networks can be lifted into an infinite-dimensional optimization problem in the space of measures (i.e., the _mean-field limit_), for which the convexity of loss function can be exploited to show the global convergence of gradient-based optimization (Nitanda and Suzuki, 2017; Mei et al., 2018; Chizat and Bach, 2018; Rotskoff and Vanden-Eijnden, 2018; Sirignano and Spiliopoulos, 2020). Under this viewpoint, MFLD Eq. (1) corresponds to the continuous-time limit of the _noisy_ gradient descent update on an infinite-width neural network, where the injected Gaussian noise encourages "exploration" and facilities global convergence (Mei et al., 2018; Hu et al., 2019).

Quantitative analysis of MFLD.Most existing convergence results of neural networks in the mean-field regime are _qualitative_ in nature, that is, they do not characterize the rate of convergence nor the discretization error. A noticeable exception is the recent analysis of MFLD by Nitanda et al. (2022); Chizat (2022), where the authors proved exponential convergence to the optimal solution of Eq. (2) under a _logarithmic Sobolev inequality_ (LSI) that can be verified in various settings including regularized empirical risk minimization using neural networks. This being said, there is still a large gap between the ideal MFLD analyzed in prior works and a feasible algorithm. In practice, we parameterize \(\mu\) as a mixture of \(N\) particles \((X^{i})_{i=1}^{N}\) -- this corresponds to a neural network with \(N\) neurons, and perform a discrete-time update: at time step \(k\), the update to the \(i\)-th particle is given as

\[X^{i}_{k+1}=X^{i}_{k}-\eta_{k}\tilde{\nabla}\frac{\delta F(\mu_{k})}{\delta\mu} (X^{i}_{k})+\sqrt{2\lambda\eta_{k}}\xi^{i}_{k},\] (3)

where \(\eta_{k}\) is the step size at the \(k\)-th iteration, \(\xi^{i}_{k}\) is an i.i.d. standard Gaussian vector, and \(\tilde{\nabla}\frac{\delta F(\mu)}{\delta\mu}\) represents a potentially inexact (e.g., stochastic) gradient.

Comparing Eq. (1) and Eq. (3), we observe the following discrepancies between the ideal MFLD and the implementable noisy particle gradient descent algorithm.

1. **Particle approximation.**\(\mu\) is entirely represented by a finite set of particles: \(\mu_{k}=\frac{1}{N}\sum_{i=1}^{N}\delta_{X^{i}_{k}}\).
2. **Time discretization.** We employ discrete gradient descent update as opposed to gradient flow.
3. **Stochastic gradient.** In many practical settings, it is computationally prohibitive to obtain the exact gradient update, and hence it is preferable to adopt a stochastic estimate of the gradient.

The control of finite-particle error (point \((i)\)) is referred to as _propagation of chaos_(Sznitman, 1991) (see also Lacker (2021) and references therein). In the context of mean-field neural networks, discretization error bounds in prior works usually grow _exponentially in time_(Mei et al., 2018; Javanmard et al., 2019; De Bortoli et al., 2020), unless one introduces additional assumptions on the dynamics that are difficult to verify (Chen et al., 2020). Consequently, convergence guarantee in the continuous limit cannot be transferred to the finite-particle setting unless the time horizon is very short (e.g., Abbe et al. (2022)), which limits the applicability of the theory.

Very recently, Chen et al. (2022); Suzuki et al. (2023) established a _uniform-in-time_ propagation of chaos for MFLD, i.e., the "distance" between the \(N\)-particle system and the infinite-particle limit is of order \(\mathcal{O}(1/N)\) for all \(t>0\). While this represents a significant step towards an optimization theory for practical finite-width neural networks in the mean-field regime, these results assumed the continuous-time limit and access to exact gradient, thus cannot cover points \((ii)\) and \((iii)\).

In contrast, for the standard gradient Langevin dynamics (LD) without the mean-field interactions (which is a special case of MFLD Eq. (1) by setting \(F\) to be a _linear_ functional), the time discretization is well-understood (see e.g. Dalalyan (2014); Vempala and Wibisono (2019); Chewi et al. (2021)), and its stochastic gradient variant (Welling and Teh, 2011; Ma et al., 2015), including ones that employ variance-reduced gradient estimators (Dubey et al., 2016; Zou et al., 2018; Kinoshita and Suzuki, 2022), have also been extensively studied.

The gap between convergence analyses of LD and MFLD motivates us to ask the following question.

_Can we develop a complete non-asymptotic convergence theory for MFLD that takes into account points \((i)\) - \((iii)\), and provide further refinement over existing results when specialized to LD?_

### Our Contributions

We present a unifying framework to establish uniform-in-time convergence guarantees for the mean-field Langevin dynamics under time and space discretization, simultaneously addressing points \((i)\)-\((iii)\). The convergence rate is exponential up to an error that vanishes when the step size and stochastic gradient variance tend to \(0\), and the number of particles \(N\) tends to infinity. Moreover, our proof is based on an LSI condition analogous to Nitanda et al. (2022); Chizat (2022), which is satisfied in a wide range of regularized risk minimization problems. The advantages of our analysis is summarized as follows.

* Our framework provides a unified treatment of different gradient estimators. Concretely, we establish convergence rate of MFLD with stochastic gradient and stochastic variance reduced gradient (Johnson and Zhang, 2013). While it is far from trivial to derive a tight bound on the stochastic gradient approximation error because it requires evaluating the correlation between the randomness of each gradient and the updated parameter distribution, we are able to show that a stochastic gradient effectively improves the computational complexity. Noticeably, despite the fact that our theorem simultaneously handles a wider class of \(F\), when specialized to standard Langevin dynamics (i.e., when \(F\) is linear), we recover state-of-the-art convergence rates for LD; moreover, by introducing an additional mild assumption on the smoothness of the objective, our analysis can significantly improve upon existing convergence guarantees.
* Our analysis greatly extends the recent works of Chen et al. (2022); Suzuki et al. (2023), in that our propagation of chaos result covers the discrete time setting while the discretization error can still be controlled uniformly over time, i.e., the finite particle approximation error does not blow up as \(t\) increases. Noticeably, we do not impose weak interaction / large noise conditions that are common in the literature (e.g., Delarue and Tse (2021); Lacker and Flem (2022)); instead, our theorem remains valid for _any_ regularization strength.

## 2 Problem Setting

Consider the set of probability measure \(\mathcal{P}\) on \(\mathbb{R}^{d}\) where the Borel \(\sigma\)-algebra is equipped. Our goal is to find a probability measure \(\mu\in\mathcal{P}\) that approximately minimizes the objective given by

\[F(\mu)=U(\mu)+\mathbb{E}_{\mu}[r],\]

where \(U:\mathcal{P}\rightarrow\mathbb{R}\) is a (convex) loss function, and \(r:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a regularization term. Let \(\mathcal{P}_{2}\) be the set of probability measures with the finite second moment. In the following, we consider the setting where \(F(\mu)\leq C(1+\mathbb{E}_{\mu}[\|X\|^{2}])\), and focus on \(\mathcal{P}_{2}\) so that \(F\) is well-defined.

As previously mentioned, an important application of this minimization problem is the learning of two-layer neural network in the _mean-field regime_. Suppose that \(h_{x}(\cdot)\) is a neuron with a parameter \(x\in\mathbb{R}^{d}\), e.g., \(h_{x}(z)=\sigma(w^{\top}z+b)\), where \(w\in\mathbb{R}^{d-1},b\in\mathbb{R}\), and \(x=(w,v)\). The mean-field neural network corresponding to a probability measure \(\mu\in\mathcal{P}\) can be written as \(f_{\mu}(\cdot)=\int h_{x}(\cdot)\mu(\mathrm{d}x)\). Given training data \((z_{i},y_{i})_{i=1}^{n}\in\mathbb{R}^{d-1}\times\mathbb{R}\), we may define the empirical risk of \(f_{\mu}\) as \(U(\mu)=\frac{1}{n}\sum_{i=1}^{n}\ell(f_{\mu}(z_{i}),y_{i})\) for a loss function \(\ell:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}\). Then, the objective \(F\) becomes

\[F(\mu)=\frac{1}{n}\sum_{i=1}^{n}\ell(f_{\mu}(z_{i}),y_{i})+\lambda_{1}\int\|x \|^{2}\mathrm{d}\mu(x),\]

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Method (authors) & \# of particles & Total complexity & Single loop & Mean-field \\ \hline PDA* & & & & \\ (Nintada et al., 2021) & \(\epsilon^{-2}\log(n)\) & \(G_{\epsilon}\epsilon^{-1}\) & \(\times\) & ✓ \\ \hline P-SDCA & & & & \\ (Oke et al., 2022) & & & & \\ \hline GLD & & & & \\ (Vempala and Wibisono, 2019) & & & & \\ \hline SVRG-LD & & & & \\ (Kinoshita and Suzuki, 2022) & & & & \\ \hline F-MFLD **(ours)** & \(\epsilon^{-1}\) & \(nE_{*}\frac{\log(\epsilon^{-1})}{(\lambda\alpha)}\) & ✓ & ✓ \\ \hline SGD-MFLD* **(ours)** & \(\epsilon^{-1}\) & \(\epsilon^{-1}E_{*}\frac{\log(\epsilon^{-1})}{(\lambda\alpha)}\) & ✓ & ✓ \\ \hline SGD-MFLD* (ii) **(ours)** & \(\epsilon^{-1}\) & \(\epsilon^{-1}(1+\sqrt{\lambda E_{*}})\frac{\log(\epsilon^{-1})}{(\lambda \alpha)^{2}}\) & ✓ & ✓ \\ \hline SVRG-MFLD **(ours)** & \(\epsilon^{-1}\) & \(\sqrt{n}E_{*}\frac{\log(\epsilon^{-1})}{(\lambda\alpha)}+n\) & ✓ & ✓ \\ \hline SVRG-MFLD (ii) **(ours)** & \(\epsilon^{-1}\) & \((n^{1/3}E_{*}+\sqrt{n}\lambda^{1/4}E_{*}^{3/4})\frac{\log(\epsilon^{-1})}{( \lambda\alpha)}+n\) & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of computational complexity to optimize an entropy-regularized finite-sum objective up to excess objective value \(\epsilon\), in terms of dataset size \(n\), entropy regularization \(\lambda\), and LSI constant \(\alpha\). Label * indicates the _online_ setting, and the unlabeled methods are tailored to the _finite-sum_ setting. “Mean-field” indicates the presence of particle interactions. “Single loop” indicates whether the algorithm requires an inner-loop MCMC sampling sub-routine at every step. “(ii)” indicates convergence rate under additional smoothness condition (Assumption 4), where \(E_{*}=\frac{\tilde{L}^{2}}{\alpha\epsilon}+\frac{\tilde{L}}{\sqrt{\lambda \alpha\epsilon}}\). For double-loop algorithms (PDA and P-SDCA), \(G^{*}\) is the number of gradient evaluations required for MCMC sampling; for example, for MALA (Metropolis-adjusted Langevin algorithm) \(G_{\epsilon}=O(n\alpha^{-5/2}\log(1/\epsilon)^{3/2})\), and for LMC (Langevin Monte Carlo) \(G_{\epsilon}=O(n(\alpha\epsilon)^{-2}\log(\epsilon))\).

where the regularization term is \(r(x)=\lambda_{1}\|x\|^{2}\). Note that the same objective can be defined for expected risk minimization. We defer additional examples to the last of this section.

One effective way to solve the above objective is the _mean-field Langevin dynamics_ (MFLD), which optimizes \(F\) via a noisy gradient descent update. To define MFLD, we need to introduce the first-variation of the functional \(F\).

**Definition 1**.: _Let \(G:\mathcal{P}_{2}\to\mathbb{R}\). The first-variation \(\frac{\delta G}{\delta\mu}\) of a functional \(G:\mathcal{P}_{2}\to\mathbb{R}\) at \(\mu\in\mathcal{P}_{2}\) is defined as a continuous functional \(\mathcal{P}_{2}\times\mathbb{R}^{d}\to\mathbb{R}\) that satisfies \(\lim_{\epsilon\to 0}\frac{G(\epsilon\nu+(1-\epsilon)\mu)}{\epsilon}=\int \frac{\delta G}{\delta\mu}(\mu)(x)\mathrm{d}(\nu-\mu)\) for any \(\nu\in\mathcal{P}_{2}\). If there exists such a functional \(\frac{\delta G}{\delta\mu}\), we say \(G\) admits a first-variation at \(\mu\), or simply \(G\) is differentiable at \(\mu\)._

To avoid the ambiguity of \(\frac{\delta G}{\delta\mu}\) up to constant shift, we follow the convention of imposing \(\int\frac{\delta G}{\delta\mu}(\mu)\mathrm{d}\mu=0\). Using the first-variation of \(F\), the MFLD is given by the following stochastic differential equation:

\[\mathrm{d}X_{t}=-\nabla\frac{\delta F(\mu_{t})}{\delta\mu}(X_{t})\mathrm{d}t+ \sqrt{2\lambda}\mathrm{d}W_{t},\quad\mu_{t}=\mathrm{Law}(X_{t}),\] (4)

where \(X_{0}\sim\mu_{0}\), \(\mathrm{Law}(X)\) denotes the distribution of the random variable \(X\) and \((W_{t})_{t\geq 0}\) is the \(d\)-dimensional standard Brownian motion. Readers may refer to Huang et al. (2021) for the existence and uniqueness of the solution. MFLD is an instance of _distribution-dependent_ SDE because the drift term \(\frac{\delta F(\mu_{t})}{\delta\mu}(\cdot)\) depends on the distribution \(\mu_{t}\) of the current solution \(X_{t}\)(Kahn and Harris, 1951; Kac, 1956; McKean, 1966). It is known that the MFLD is a Wasserstein gradient flow to minimize the following _entropy-regularized_ objective (Mei et al., 2018; Hu et al., 2019; Nitanda et al., 2022; Chizat, 2022):

\[\mathscr{F}(\mu)=F(\mu)+\lambda\mathrm{Ent}(\mu),\] (5)

where \(\mathrm{Ent}(\mu)=-\int\log(\mathrm{d}\mu(z)/\mathrm{d}z)\mathrm{d}\mu(z)\) is the negative entropy of \(\mu\).

Reduction to standard Langevin dynamics.Note that the MFLD reduces to the standard gradient Langevin dynamics (LD) when \(F\) is a linear functional, that is, there exists \(V\) such that \(F(\mu)=\int V(x)\mathrm{d}\mu(x)\). In this case, \(\frac{\delta F}{\delta\mu}=V\) for any \(\mu\) and the MFLD Eq. (4) simplifies to

\[\mathrm{d}X_{t}=-\nabla V(X_{t})\mathrm{d}t+\sqrt{2\lambda}\mathrm{d}W_{t}.\]

This is exactly the gradient Langevin dynamics for optimizing \(V\) or sampling from \(\mu\propto\exp(-V/\lambda)\).

### Some Applications of MFLD

Here, we introduce a few examples that can be approximately solved via MFLD.

_Example 1_ (Two-layer neural network in mean-field regime.).: Let \(h_{x}(z)\) be a neuron with a parameter \(x\in\mathbb{R}^{d}\), e.g., \(h_{x}(z)=\tanh(r\sigma(w^{\top}x))\), \(h_{x}(z)=\tanh(r)\sigma(w^{\top}x)\) for \(x=(r,w)\), or simply \(h_{x}(z)=\sigma(x^{\top}z)\). Then the learning of mean-field neural network \(f_{\mu}(\cdot)=\int h_{x}(\cdot)\mu(\mathrm{d}x)\) via minimizing the empirical risk \(U(\mu)=\frac{1}{n}\sum_{i=1}^{n}\ell(f_{\mu}(z_{i}),y_{i})\) with a convex loss (e.g., the logistic loss \(\ell(f,y)=\log(1+\exp(-yf))\), or the squared loss \(\ell(f,y)=(f-y)^{2}\)) falls into our framework.

_Example 2_ (Density estimation via MMD minimization).: For a positive definite kernel \(k\), the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) between two probability measures \(\mu\) and \(\nu\) is defined as \(\mathrm{MMD}^{2}(\mu,\nu):=\int(k(x,x)-2k(x,y)+k(y,y))\mathrm{d}\mu(x)\mathrm{d }\nu(y)\). We perform nonparametric density estimation by fitting a Gaussian mixture model \(f_{\mu}(z)=\int g_{x}(z)\mathrm{d}\mu(x)\), where \(g_{x}\) is the Gaussian density with mean \(x\) and a given variance \(\sigma^{2}>0\). The mixture model is learned by minimizing \(\mathrm{MMD}^{2}(f_{\mu},p^{*})\) where \(p^{*}\) is the target distribution. If we observe a set of training data \((z_{i})_{i=1}^{n}\) from \(p^{*}\), then the empirical version of MMD is one suitable loss function \(U(\mu)\) given as

\[\widehat{\mathrm{MMD}}^{2}(\mu):=\!\!\int\!\big{[}\iint\!g_{x}(z)g_{x^{\prime }}(z^{\prime})k(z,z^{\prime})\mathrm{d}z\mathrm{d}z^{\prime}\big{]}\!\mathrm{d}( \mu\!\times\!\mu)(x,x^{\prime})\!-\!2\!\int\!\big{(}\frac{1}{n}\!\sum\limits_{ i=1}^{n}\!\!\int\!g_{x}(z)k(z,z_{i})\mathrm{d}z\big{)}\!\mathrm{d}\mu(x),\]

Note that we may also choose to directly fit the particles to the data (instead of a Gaussian mixture model), that is, we use a Dirac measure for each particle, as in Chizat (2022, Section 5.2) and Arbel et al. (2019). Here we state the Gaussian parameterization for the purpose of density estimation.

_Example 3_ (Kernel Stein discrepancy minimization).: In settings where we have access to the target distribution through the score function (e.g., sampling from a posterior distribution in Bayesian inference), we may employ the kernel Stein discrepancy (KSD) as the discrepancy measure (Chwialkowski et al., 2016; Liu et al., 2016). Suppose that we can compute \(s(z)=\nabla\log(\mu^{*})(z)\), then for a positive definite kernel \(k\), we define the _Stein kernel_ as

\[W_{\mu^{*}}(z,z^{\prime}):=s(z)^{\top}s(z^{\prime})k(z,z^{\prime})+s(z)^{\top} \nabla_{z^{\prime}}k(z,z^{\prime})+\nabla_{z}^{\top}k(z,z^{\prime})s(z^{\prime })+\nabla_{z^{\prime}}^{\top}\nabla_{z}k(z,z^{\prime}).\]

We take \(U(\mu)\) as the KSD between \(\mu\) and \(\mu^{*}\) defined as \(\mathrm{KSD}(\mu)=\int\int W_{\mu^{*}}(z,z^{\prime})\mathrm{d}\mu(z)\mathrm{d }\mu(z^{\prime})\). By minimizing this objective via MFLD, we attain kernel Stein variational inference with convergence guarantees. This can be seen as a "Langevin" version of KSD descent in Korba et al. (2021).

**Remark 1**.: _In the above examples, we introduce additional regularization terms in the objective Eq. (5) to establish the convergence rate of MFLD, in exchange for a slight optimization bias. Note that these added regularizations often have a statistical benefit due to the smoothing effect._

### Practical implementations of MFLD

Although the convergence of MFLD (Eq. (4)) has been studied in prior works (Hu et al., 2019; Nitanda et al., 2022; Chizat, 2022), there is a large gap between the ideal dynamics and a practical implementable algorithm. Specifically, we need to consider \((i)\) the finite-particle approximation, \((ii)\) the time discretization, and \((iii)\) stochastic gradient.

To this end, we consider the following space- and time-discretized version of the MFLD with stochastic gradient update. For a finite set of particles \(\mathscr{X}=(X^{i})_{i=1}^{N}\subset\mathbb{R}^{d}\), we define its corresponding empirical distribution as \(\mu_{\mathscr{X}}=\frac{1}{N}\sum_{i=1}^{N}\delta_{X_{i}}\). Let \(\mathscr{R}_{k}=(X^{i}_{k})_{i=1}^{N}\subset\mathbb{R}^{d}\) be \(N\) particles at the \(k\)-th update, and define \(\mu_{k}=\mu_{\mathscr{X}_{k}}\) as a finite particle approximation of the population counterpart. Starting from \(X^{i}_{0}\sim\mu_{0}\), we update \(\mathscr{R}_{k}\) as,

\[X^{i}_{k+1}=X^{i}_{k}-\eta_{k}v^{i}_{k}+\sqrt{2\lambda\eta_{k}}\xi^{i}_{k},\] (6)

where \(\eta_{k}>0\) is the step size, \(\xi^{i}_{k}\) is an i.i.d. standard normal random variable \(\xi^{i}_{k}\sim N(0,I)\), and \(v^{i}_{k}=v^{i}_{k}(\mathscr{X}_{0:k},\omega^{i}_{k})\) is a stochastic approximation of \(\nabla\frac{\delta F(\mu_{k})}{\delta\mu}(X^{i}_{k})\) where \(\omega_{k}=(\omega^{i}_{k})_{i=1}^{N}\) is a random variable generating the randomness of stochastic gradient. \(v^{i}_{k}\) can depend on the history \(\mathscr{X}_{0:k}=(\mathscr{X}_{0},\ldots,\mathscr{X}_{k})\), and \(\mathbb{E}_{\omega^{i}_{k}}[v^{i}_{k}|\mathscr{X}_{0:k}]=\nabla\frac{\delta F (\mu_{k})}{\delta\mu}(X^{i}_{k})\). We analyze three versions of \(v^{i}_{k}\).

(1) Full gradient: F-MFLD.If we have access to the exact gradient, we may compute

\[v^{i}_{k}=\nabla\frac{\delta F(\mu_{k})}{\delta\mu}(X^{i}_{k}).\]

(2) Stochastic gradient: SGD-MFLD.Suppose the loss function \(U\) is given by an expectation as \(U(\mu)=\mathbb{E}_{Z\sim P_{Z}}[\ell(\mu,Z)],\) where \(Z\in\mathcal{Z}\) is a random observation obeying a distribution \(P_{Z}\) and \(\ell:\mathcal{P}\times\mathcal{Z}\to\mathbb{R}\) is a loss function. In this setting, we construct the stochastic gradient as

\[v^{i}_{k}=\frac{1}{B}\sum_{j=1}^{B}\nabla\frac{\delta\ell(\mu_{k},z^{(j)}_{k})} {\delta\mu}(X^{i}_{k})+\nabla r(X^{i}_{k}),\]

where \((z^{(j)}_{k})_{j=1}^{B}\) is a mini-batch of size \(B\) generated from \(P_{Z}\) in an i.i.d. manner.

(3) Stochastic variance reduced gradient: SVRG-MFLD.Suppose that the loss function \(U\) is given by a finite sum of loss functions \(\ell_{i}\): \(U(\mu)=\frac{1}{n}\sum_{i=1}^{n}\ell_{i}(\mu),\) which corresponds to the empirical risk in a usual machine learning setting. Then, the variance reduced stochastic gradient (SVRG) (Johnson and Zhang, 2013) is defined as follows:

1. When \(k\equiv 0\bmod m\), where \(m\) is the update frequency, we set \(\dot{\mathscr{X}}=(\dot{X}^{i})_{i=1}^{n}=(X^{i}_{k})_{i=1}^{n}\) as the anchor point and refresh the stochastic gradient as \(v^{i}_{k}=\nabla\frac{\delta F(\mu_{k})}{\delta\mu}(X^{i}_{k})\).
2. When \(k\not\equiv 0\bmod m\), we use the anchor point to construct a control variate and compute \[v^{i}_{k}=\frac{1}{B}\sum_{j\in I_{k}}\nabla\left(\frac{\delta\ell_{j}(\mu_{ \mathscr{X}_{k}})}{\delta\mu}(X^{i}_{k})+r(X^{i}_{k})-\frac{\delta\ell_{j}(\mu_{ \mathscr{X}})}{\delta\mu}(\dot{X}^{i})+\frac{\delta U(\mu_{\mathscr{X}})}{ \delta\mu}(\dot{X}^{i})\right),\] where \(I_{k}\) is a uniformly drawn random subset of \(\{1,\ldots,n\}\) with size \(B\) without duplication.

Main Assumptions and Theoretical Tools

For our convergence analysis, we make the following assumptions which are inherited from prior works in the literature (Nitanda et al., 2020, 2022; Chizat, 2022; Oko et al., 2022; Chen et al., 2022).

**Assumption 1**.: _The loss function \(U\) and the regularization term \(r\) are convex. Specifically,_

1. \(U:\mathcal{P}\to\mathbb{R}\) _is a convex functional on_ \(\mathcal{P}\)_, that is,_ \(U(\theta\mu+(1-\theta)\nu)\leq\theta U(\mu)+(1-\theta)U(\nu)\) _for any_ \(\theta\in[0,1]\) _and_ \(\mu,\nu\in\mathcal{P}\)_. Moreover,_ \(U\) _admits a first-variation at any_ \(\mu\in\mathcal{P}_{2}\)_._
2. \(r(\cdot)\) _is twice differentiable and convex, and there exist constants_ \(\lambda_{1},\lambda_{2}>0\) _and_ \(c_{r}>0\) _such that_ \(\lambda_{1}I\preceq\nabla\nabla^{\top}r(x)\preceq\lambda_{2}I\)_,_ \(x^{\top}\nabla r(x)\geq\lambda_{1}\|x\|^{2}\)_, and_ \(0\leq r(x)\leq\lambda_{2}(c_{r}+\|x\|^{2})\) _for any_ \(x\in\mathbb{R}^{d}\)_, and_ \(\nabla r(0)=0\)_._

**Assumption 2**.: _There exists \(L>0\) such that \(\left\|\nabla\frac{dU(\mu)}{\delta\mu}(x)-\nabla\frac{dU(\mu^{\prime})}{ \delta\mu}(x^{\prime})\right\|\leq L(W_{2}(\mu,\mu^{\prime})+\|x-x^{\prime}\|)\) and \(\left|\frac{\delta^{2}U(\mu)}{\delta\mu^{2}}(x,x^{\prime})\right|\leq L(1+c_{ L}(\|x\|^{2}+\|x^{\prime}\|^{2}))\) for any \(\mu,\mu^{\prime}\in\mathcal{P}_{2}\) and \(x,x^{\prime}\in\mathbb{R}^{d}\). Also, there exists \(R>0\) such that \(\|\nabla\frac{dU(\mu)}{\delta\mu}(x)\|\leq R\) for any \(\mu\in\mathcal{P}\) and \(x\in\mathbb{R}^{d}\)._

Verification of this assumption in the three examples (Examples 1, 2 and 3) is given in Appendix A in the supplementary material. We remark that the assumption on the second order variation is only required to derive the discretization error corresponding to the uniform-in-time propagation of chaos (Lemma 8 in Appendix E.4). Under these assumptions, Proposition 2.5 of Hu et al. (2019) yields that \(\mathscr{F}\) has a unique minimizer \(\mu^{*}\) in \(\mathcal{P}\) and \(\mu^{*}\) is absolutely continuous with respect to the Lebesgue measure. Moreover, \(\mu^{*}\) satisfies the self-consistent condition: \(\mu^{*}(X)\propto\exp\left(-\frac{1}{\lambda}\frac{\delta F(\mu^{*})}{\delta \mu}(X)\right).\)

### Proximal Gibbs Measure & Logarithmic Sobolev inequality

The aforementioned self-consistent relation motivates us to introduce the _proximal Gibbs distribution_(Nitanda et al., 2022; Chizat, 2022) whose density is given by

\[p_{\mathscr{X}}(X)\propto\exp\left(-\frac{1}{\lambda}\frac{\delta F(\mu_{ \mathscr{X}})}{\delta\mu}(X)\right),\]

where \(\mathscr{X}=(X^{i})_{i=1}^{N}\subset\mathbb{R}^{d}\) is a set of \(N\) particles and \(X\in\mathbb{R}^{d}\). As we will see, the convergence of MFLD heavily depends on a _logarithmic Sobolev inequality_ (LSI) on the proximal Gibbs measure.

**Definition 2** (Logarithmic Sobolev inequality).: _Let \(\mu\) be a probability measure on \((\mathbb{R}^{d},\mathcal{B}(\mathbb{R}^{d}))\). \(\mu\) satisfies the LSI with constant \(\alpha>0\) if for any smooth function \(\phi:\mathbb{R}^{d}\to\mathbb{R}\) with \(\mathbb{E}_{\mu}[\phi^{2}]<\infty\), we have \(\mathbb{E}_{\mu}[\phi^{2}\log(\phi^{2})]-\mathbb{E}_{\mu}[\phi^{2}]\log( \mathbb{E}_{\mu}[\phi^{2}])\leq\frac{2}{\alpha}\mathbb{E}_{\mu}[\|\nabla\phi \|_{2}^{2}].\)_

This is equivalent to the condition that the KL divergence from \(\mu\) is bounded by the Fisher divergence: \(\int\log(\mathrm{d}\nu/\mathrm{d}\mu)\mathrm{d}\nu\leq\frac{1}{2\alpha}\int\| \nabla\log(\mathrm{d}\nu/\mathrm{d}\mu)\|^{2}\mathrm{d}\nu,\) for any \(\nu\in\mathcal{P}\) which is absolutely continuous with respect to \(\mu\). Our analysis requires that the proximal Gibbs distribution satisfies the LSI as follows.

**Assumption 3**.: \(\mu^{*}\) _and \(p_{\mathscr{X}}\) satisfy the LSI with \(\alpha>0\) for any set of particles \(\mathscr{X}=(X^{i})_{i=1}^{N}\subset\mathbb{R}^{d}\)._

Verification of LSI.The LSI of proximal Gibbs measure can be established via standard perturbation criteria. For \(U(\mu)\) with bounded first-variation, we may apply the classical Bakry-Emery and Holley-Stroock arguments (Bakry and Emery, 1985a; Holley and Stroock, 1987) (see also Corollary 5.7.2 and 5.1.7 of Bakry et al. (2014)). Whereas for Lipschitz perturbations, we employ Miclo's trick (Bardet et al., 2018) or the more recent perturbation results in Cattiaux and Guillin (2022).

**Theorem 1**.: _Under Assumptions 1 and 2, \(\mu^{*}\) and \(p_{\mathscr{X}}\) satisfy the log-Sobolev inequality with_

\[\alpha\geq\frac{\lambda_{1}}{2\lambda}\exp\!\left(\!-\frac{4R^{2}}{\lambda_{1} \lambda}\sqrt{2d/\pi}\right)\vee\left\{\!\frac{4\lambda}{\lambda_{1}}+e^{ \frac{R^{2}}{2\lambda_{1}}\!\left(\frac{R}{\lambda_{1}}\!+\!\sqrt{\frac{2\lambda }{\lambda_{1}}}\right)^{2}}\left[2\!+\!d\!+\!\frac{d}{2}\log\left(\frac{\lambda _{2}}{\lambda_{1}}\right)\!+\!4\frac{R^{2}}{\lambda_{1}\lambda}\right]\right\}^ {-1}.\]

_Furthermore, if \(\|\frac{\delta U(\mu)}{\delta\mu}\|_{\infty}\leq R\) is satisfied for any \(\mu\in\mathcal{P}_{2}\), then \(\mu^{*}\) and \(p_{\mathscr{X}}\) satisfy the LSI with \(\alpha\geq\frac{\lambda_{1}}{\lambda}\exp\left(-\frac{4R}{\lambda}\right)\)._

See Lemma 5 for the proof of the first assertion, and Section A.1 of Nitanda et al. (2022) or Proposition 5.1 of Chizat (2022) for the proof of the second assertion.

Main Result: Convergence Analysis

In this section, we present our the convergence rate analysis of the discretized dynamics Eq. (6). To derive the convergence rate, we need to evaluate the errors induced by three approximations: (i) time discretization, (ii) particle approximation, and (iii) stochastic gradient.

### General Recipe for Discretization Error Control

Note that for the finite-particle setting, the entropy term does not make sense because the negative entropy is not well-defined for a discrete empirical measure. Instead, we consider the distribution of \(N\) particles, that is, let \(\mu^{(N)}\in\mathcal{P}^{(N)}\) be a distribution of \(N\) particles \(\mathscr{X}=(X^{i})_{i=1}^{N}\) where \(\mathcal{P}^{(N)}\) is the set of probability measures on \((\mathbb{R}^{d\times N},\mathcal{B}(\mathbb{R}^{d\times N}))\). Similarly, we introduce the following objective on \(\mathcal{P}^{(N)}\):

\[\mathscr{F}^{N}(\mu^{(N)})=N\mathbb{E}_{\mathscr{X}\sim\mu^{(N)}}[F(\mu_{ \mathscr{X}})]+\lambda\mathrm{Ent}(\mu^{(N)}).\]

One can easily verify that if \(\mu^{(N)}\) is a product measure of \(\mu\in\mathcal{P}\), then \(\mathscr{F}^{N}(\mu^{(N)})\geq N\mathscr{F}(\mu)\) by the convexity of \(F\). _Propagation of chaos_(Sznitman, 1991) refers to the phenomenon that, as the number of particles \(N\) increases, the particles behave as if they are independent; in other words, the joint distribution of the \(N\) particles becomes "close" to the product measure. Consequently, the minimum of \(\mathscr{F}^{N}(\mu^{(N)})\) (which can be obtained by the particle-approximated MFLD) is close to \(N\mathscr{F}(\mu^{*})\). Specifically, it has been shown in Chen et al. (2022) that

\[0\leq\inf_{\mu^{(N)}\in\mathcal{P}^{(N)}}\frac{1}{N}\mathscr{F}^{N}(\mu^{(N)} )-\mathscr{F}(\mu^{*})\leq\frac{C_{\lambda}}{N},\] (7)

for some constant \(C_{\lambda}>0\) (see Section E for more details). Importantly, if we consider the Wasserstein gradient flow on \(\mathcal{P}^{(N)}\), the convergence rate of which depends on the logarithmic Sobolev inequality (Assumption 3), we need to ensure that the LSI constant does not deteriorate as \(N\) increases. Fortunately, the propagation of chaos and the _tensorization_ of LSI entail that the LSI constant with respect to the objective \(\mathscr{F}^{N}\) can be uniformly bounded over all choices of \(N\) (see Eq. (11) in the appendix).

To deal with the time discretization error, we build upon the one-step interpolation argument from Vempala and Wibisono (2019) which analyzed the vanilla gradient Langevin dynamics (see also Nitanda et al. (2022) for its application to the infinite-particle MFLD).

Bounding the error induced by the stochastic gradient approximation is also challenging because the objective is defined on the space of probability measures, and thus techniques in finite-dimensional settings cannot be utilized in a straightforward manner. Roughly speaking, this error is characterized by the variance of \(v_{k}^{i}\): \(\sigma^{2}_{v,k}:=\mathbb{E}_{\omega_{k},\mathscr{X}_{0:k}}[\|v_{k}^{i}- \nabla\frac{\delta F(\mu_{k})}{\delta\mu}\|^{2}]\)1. In addition, to obtain a refined evaluation, we also incorporate the following smoothness assumption.

Footnote 1: After the acceptance of this manuscript, we noticed that by replacing \(\sigma^{2}_{v,k}\) with the square of a conditional expectation \(\mathbb{E}_{\mathscr{X}_{0:k+1}}[\|\mathbb{E}_{\omega_{k}|\mathscr{X}_{0:k+1} }[v_{k}^{i}-\nabla\frac{\delta F(\mu_{k})}{\delta\mu}\|^{2}]\), we can obtain a refined bound as in Das et al. (2023) suggested by anonymous reviewer Y8on. We defer such refined analysis to future work.

**Assumption 4**.: \(v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})\) _and \(D_{m}^{i}F(\mathscr{X}_{k})=\nabla\frac{\delta F(\mu_{k})}{\delta\mu}(X_{k}^{ i})\) are differentiable w.r.t. \((X_{k}^{j})_{j=1}^{N}\) and, for either \(G(\mathscr{X}_{k})=v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})\) or \(G(\mathscr{X}_{k})=D_{m}^{i}F(\mathscr{X}_{k})\) as a function of \(\mathscr{X}_{k}\), it is satisfied that \(\|G(\mathscr{X}_{k}^{\prime})-G(\mathscr{X}_{k})-\sum_{j=1}^{N}\nabla_{X_{k}^ {j}}^{\top}G(\mathscr{X}_{k})\cdot(X_{k}^{\prime j}-X_{k}^{j})\|\leq Q\frac{ \sum_{j\neq i}\|X_{k}^{\prime j}-X_{k}^{j}\|^{2}+N\|X_{k}^{\prime i}-X_{k}^{i} \|^{2}}{2N}\) for some \(Q>0\) and \(\mathscr{X}_{k}=(X_{k}^{j})_{j=1}^{N}\) and \(\mathscr{X}_{k}^{\prime}=(X_{k}^{\prime j})_{j=1}^{N}\). We also assume \(\mathbb{E}_{\omega_{k}}\big{[}\|\nabla_{X_{k}^{j}}v_{k}^{i}(\mathscr{X}_{0:k}, \omega_{k}^{i})^{\top}-\nabla_{X_{k}^{j}}D_{m}^{i}F(\mathscr{X}_{k})^{\top} \big{]}_{\mathrm{op}}^{2}\big{]}\leq\frac{1+\delta_{i,j}N^{2}}{N^{2}}\tilde{ \sigma}_{v,k}^{2}\), and \(\|v_{k}^{i}((\mathscr{X}_{k},\mathscr{X}_{0:k-1}),\omega_{k}^{i})-v_{k}^{i}(( \mathscr{X}_{k}^{\prime},\mathscr{X}_{0:k-1}),\omega_{k}^{i})\|\leq L(W_{2}( \mu_{\mathscr{X}_{k}},\mu_{\mathscr{X}_{k}^{\prime}})+\|X_{k}^{i}-X_{k}^{\prime i }\|)\)._

Note that this assumption is satisfied if the gradient is twice differentiable and the second derivative is bounded. The factor \(N\) appears because the contribution of each particle is \(O(1/N)\) unless \(i=j\). In the following, we present both the basic convergence result under Assumption 2, and the improved rate under the additional Assumption 4.

Taking these factors into consideration, we can evaluate the decrease in the objective value after one-step update. Let \(\mu_{k}^{(N)}\in\mathcal{P}^{(N)}\) be the distribution of \(\mathscr{X}_{k}\) conditioned on the sequence \(\omega_{0:k}=(\omega_{k^{\prime}})_{k^{\prime}=0}^{k}\). Define

\[\bar{R}^{2}:=\mathbb{E}[\|X_{0}^{i}\|^{2}]+\frac{1}{\lambda_{1}}\left[\left( \frac{\lambda_{1}}{4\lambda_{2}}+\frac{1}{\lambda_{1}}\right)(R^{2}+\lambda_ {2}c_{r})+\lambda d\right],\ \ \ \delta_{\eta}:=C_{1}\bar{L}^{2}(\eta^{2}+\lambda\eta),\]

where \(C_{1}=8[R^{2}+\lambda_{2}(c_{r}+\bar{R}^{2})+d]\) and \(\bar{L}=L+\lambda_{2}\).2

Footnote 2: The constants are not optimized (e.g., \(O(\frac{1}{\lambda_{1}})\) in \(\bar{R}\) can be improved) since we prioritize for clean presentation.

**Theorem 2**.: _Under Assumptions 1, 2 and 3, if \(\eta_{k}\leq\lambda_{1}/(4\lambda_{2})\), we have_

_where \(\Upsilon_{k}=4\eta_{k}\delta_{\eta_{k}}+\left(R+\lambda_{2}\bar{R}\right)(1+ \sqrt{\lambda/\eta_{k}})\left(\eta_{k}^{2}\tilde{\sigma}_{v,k}\sigma_{v,k}+Q \eta_{k}^{3}\sigma_{v,k}^{2}\right)+\eta_{k}^{2}(L+\lambda_{2})^{2}\sigma_{v,k} ^{2}\) with Assumption 4, and \(\Upsilon_{k}=\sigma_{v,k}^{2}\eta_{k}\) without this additional assumption; the expectation is taken with respect to the randomness \((\omega_{k^{\prime}})_{k^{\prime}=1}^{k}=((\omega_{k^{\prime}}^{i})_{i=1}^{N} )_{k^{\prime}=1}^{k}\) of the stochastic gradient; and \(C_{\lambda}=2\lambda L\alpha(1+2c_{L}\bar{R}^{2})+2\lambda^{2}L^{2}\bar{R}^{2}\)._

The proof can be found in Appendix B. We remark that to derive the bound for \(\Upsilon_{k}=\sigma_{v,k}^{2}\eta_{k}\) is relatively straightforward, but to derive a tighter bound with Assumption 4 is technically challenging, because we need to evaluate how the next-step distribution \(\mu_{k+1}^{(N)}\) is correlated with the stochastic gradient \(v_{k}^{i}\). Evaluating such correlations is non-trivial because the randomness is induced not only by \(\omega_{k}\) but also by the Gaussian noise. Thanks to this general result, we only need to evaluate the variance \(\sigma_{v,k}\) and \(\tilde{\sigma}_{v,k}\) for each method to obtain the specific convergence rate.

Conversion to a Wasserstein distance bound.As a consequence of the bound on \(\frac{1}{N}\mathscr{F}^{N}(\mu_{k}^{(N)})-\mathscr{F}(\mu^{*})\), we can control the Wasserstein distance between \(\mu_{k}^{(N)}\) and \(\mu^{*N}\), where \(\mu^{*N}\in\mathcal{P}^{(N)}\) is the (\(N\)-times) product measure of \(\mu^{*}\). Let \(W_{2}(\mu,\nu)\) be the 2-Wasserstein distance between \(\mu\) and \(\nu\), then

\[W_{2}^{2}(\mu_{k}^{(N)},\mu^{*N})\leq\frac{2}{\lambda\alpha}(\mathscr{F}^{N}( \mu_{k}^{(N)})-N\mathscr{F}(\mu^{*})),\]

under Assumptions 1 and 3 (see Lemma 3 in the Appendix). Hence, if \(\frac{1}{N}\mathscr{F}^{N}(\mu_{k}^{(N)})-\mathscr{F}(\mu^{*})\) is small, the particles \((X_{k}^{i})_{i=1}^{N}\) behaves like an i.i.d. sample from \(\mu^{*}\). As an example, for the mean-field neural network setting, if \(|h_{x}(z)-h_{x^{\prime}}(z)|\leq L\|x-x^{\prime}\|\)\((\forall x,x^{\prime}\in\mathbb{R}^{d})\) and \(V_{\mu^{*}}=\int(f_{\mu^{*}}(z)-h_{x}(z))^{2}\mathrm{d}\mu^{*}(x)<\infty\) for a fixed \(z\), then Lemma 4 in the appendix yields that

\[\mathbb{E}_{\mathscr{X}_{k}\sim\mu_{k}^{(N)}}[(f_{\mu_{\mathscr{X}_{k}}}(z)-f _{\mu^{*}}(z))^{2}]\leq\frac{2L^{2}}{N}W_{2}^{2}(\mu_{k}^{(N)},\mu^{*N})+\frac {2}{N}V_{\mu^{*}},\]

which also gives \(\mathbb{E}_{\mathscr{X}_{k}\sim\mu_{k}^{(N)}}[(f_{\mu_{\mathscr{X}_{k}}}(z)-f _{\mu^{*}}(z))^{2}]\leq\frac{4L^{2}}{\lambda\alpha}\left(N^{-1}\mathscr{F}^{N} (\mu_{k}^{(N)})-\mathscr{F}(\mu^{*})\right)+\frac{2}{N}V_{\mu^{*}}\). This allows us to monitor the convergence of the finite-width neural network to the optimal solution \(\mu^{*}\) in terms of the model output (up to \(1/N\) error).

### F-MFLD and SGD-MFLD

Here, we present the convergence rate for F-MFLD and SGD-MFLD simultaneously. F-MFLD can be seen as a special case of SGD-MFLD where the variance \(\sigma_{v,k}^{2}=0\). We specialize the previous assumptions to the stochastic gradient setting as follows.

**Assumption 5**.:
1. \(\sup_{x\in\mathbb{R}^{d}}\|\nabla\frac{\delta\ell(\mu,z)}{\delta\mu}(x)\|\leq R\) _for all_ \(\mu\in\mathcal{P}\) _and_ \(z\in\mathcal{Z}\)_._
2. \(\sup_{x}\|\nabla_{x}\nabla_{x}^{\top}\frac{\delta\ell(\mu,z)}{\delta\mu}(x)\|_ {\mathrm{op}},\ \sup_{x,x^{\prime}}\|\nabla_{x}\nabla_{x^{\prime}}^{\top}\frac{ \delta^{2}\ell(\mu,z)}{\delta^{2}\mu}(x,x^{\prime})\|_{\mathrm{op}}\leq R\)_._

Note that point \((i)\) is required to bound the variance \(\sigma_{v,k}^{2}\) (i.e., \(\sigma_{v,k}^{2}\leq R^{2}/B\)), whereas point \((ii)\) corresponds to the additional Assumption 4 required for the improved convergence rate. Let \(\Delta_{0}:=\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{0}^{(N)})]-\mathscr{F}( \mu^{*})\). We have the following evaluation of the objective. Note that we can recover the evaluation for F-MFLD by formally setting \(B=\infty\) so that \(\sigma_{v,k}^{2}=0\) and \(\tilde{\sigma}_{v,k}^{2}=0\).

**Theorem 3**.: _Suppose that \(\eta_{k}=\eta\)\((\forall k\in\mathbb{N}_{0})\) and \(\lambda\alpha\eta\leq 1/4\) and \(\eta\leq\lambda_{1}/(4\lambda_{2})\). Under Assumptions 1, 2, 3 and 5, it holds that_

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*})\leq \exp\left(-\lambda\alpha\eta k/2\right)\Delta_{0}+\frac{4}{\lambda\alpha} \bar{L}^{2}C_{1}\left(\lambda\eta+\eta^{2}\right)+\frac{4}{\lambda\alpha\eta} \bar{\Upsilon}+\frac{4C_{\lambda}}{\lambda\alpha N},\] (8)

_where \(\bar{\Upsilon}=4\eta\delta_{\eta}+\left[R+\lambda_{2}\bar{R}+(L+\lambda_{2})^{ 2}\right](1+\sqrt{\frac{\lambda}{\eta}})\eta^{2}\frac{R^{2}}{B}+\left(R+ \lambda_{2}\bar{R}\right)R(1+\sqrt{\frac{\lambda}{\eta}})\eta^{3}\frac{R^{2}} {B}\) under Assumption 5-(ii), and \(\bar{\Upsilon}=\frac{R^{2}}{B}\eta\) without this additional assumption._

The proof is given in Appendix C. This can be seen as a mean-field generalization of Vempala and Wibisono (2019) which provides a convergence rate of discrete time vanilla GLD with respect to the KL divergence. Indeed, their derived rate \(O(\exp(-\lambda\alpha\eta k)\Delta_{0}+\frac{\eta}{\alpha\lambda})\) is consistent to ours, since Eq. (26) in the Appendix implies that the objective \(\frac{1}{\lambda}(\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]- \mathscr{F}(\mu^{*}))\) upper bounds the KL divergence between \(\mu_{k}^{(N)}\) and \(\mu^{*N}\). Moreover, our result also handles the stochastic approximation which can give better total computational complexity even in the vanilla GLD setting as shown below.

For a given \(\epsilon>0\), if we take

\[\eta\leq\frac{\alpha\epsilon}{40\bar{L}^{2}C_{1}}\wedge\frac{1}{\bar{L}}\sqrt {\frac{\lambda\alpha\epsilon}{40C_{1}}}\wedge 1,\;\;k\geq\frac{2}{\lambda\alpha\eta} \log(2\Delta_{0}/\epsilon),\]

with \(B\geq 4\left[(1+R)(R+\lambda_{2}\bar{R})+(L+\lambda_{2})^{2}\right]R^{2}(\eta +\sqrt{\eta\lambda})/(\lambda\alpha\epsilon)\) under Assumption 5-(ii) and \(B\geq 4R^{2}/(\lambda\alpha\epsilon)\) without such assumption, then the right hand side of (8) can be bounded as \(\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*})= \epsilon+\frac{4C_{\lambda}}{\lambda\alpha N}\). Hence we achieve \(\epsilon+O(1/N)\) error with iteration complexity:

\[k=O\left(\frac{\bar{L}^{2}}{\alpha\epsilon}+\frac{\bar{L}}{\sqrt{\lambda \alpha\epsilon}}\right)\frac{1}{\lambda\alpha}\log(\epsilon^{-1}).\] (9)

If we neglect \(O(1/\sqrt{\lambda\alpha\epsilon})\) as a second order term, then the above can be simplified as \(O\big{(}\frac{\bar{L}^{2}}{\lambda\alpha^{2}}\frac{\log(\epsilon^{-1})}{ \epsilon}\big{)}\).

Noticeably, the mini-batch size \(B\) can be significantly reduced under the additional smoothness condition Assumption 5-(ii) (indeed, we have \(O(\eta+\sqrt{\eta\lambda})\) factor reduction). Recall that when the objective is an empirical risk, the gradient complexity per iteration required by F-MFLD is \(O(n)\). Hence, the total complexity of F-MFLD is \(O(nk)\) where \(k\) is given in Eq. (9). Comparing this with SGD-MFLD with mini-batch size \(B\), we see that SGD-MFLD has better total computational complexity (\(Bk\)) when \(n>B\). In particular, if \(B=\Omega((\eta+\sqrt{\eta\lambda})/(\lambda\alpha\epsilon))\geq\Omega(\lambda^ {-1}+\sqrt{(\epsilon\lambda\alpha)^{-1}})\) which yields the objective value of order \(O(\epsilon)\), SGD-MFLD achieves \(O(n/B)=O(n(\lambda\wedge\sqrt{\epsilon\lambda\alpha}))\) times smaller total complexity. For example, when \(\lambda=\alpha\epsilon=1/\sqrt{n}\), a \(O(\sqrt{n})\)-factor reduction of total complexity can be achieved by SGD-MFLD, which is a significant improvement.

### Svrg-Mfld

Now we present the convergence rate of SVRG-MFLD in the fixed step size setting where \(\eta_{k}=\eta\)\((\forall k)\). Instead of Assumption 5, we introduce the following two assumptions.

**Assumption 6**.:
1. _For any_ \(i\in[n]\)_, it holds that_ \(\left\|\nabla\frac{\delta\ell_{i}(\mu)}{\delta\mu}(x)-\nabla\frac{\delta\ell_{i} (\mu^{\prime})}{\delta\mu}(x^{\prime})\right\|\leq L(W_{2}(\mu,\mu^{\prime})+ \|x-x^{\prime}\|)\) _for any_ \(\mu,\mu^{\prime}\in\mathcal{P}_{2}\) _and_ \(x,x^{\prime}\in\mathbb{R}^{d}\)_, and_ \(\sup_{x\in\mathbb{R}^{d}}\|\nabla\frac{\delta\ell_{i}(\mu)}{\delta\mu}(x)\|\leq R\) _for any_ \(\mu\in\mathcal{P}\)_._
2. _Additionally, we have_ \(\sup_{x}\|\nabla_{x}\nabla_{x}^{\top}\frac{\delta\ell_{i}(\mu)}{\delta\mu}(x) \|_{\mathrm{op}},\sup_{x,x^{\prime}}\|\nabla_{x}\nabla_{x^{\prime}}^{\top} \frac{\delta^{2}\ell_{i}(\mu)}{\delta^{2}\mu}(x,x^{\prime})\|_{\mathrm{op}} \leq R\)_._

Here again, point (i) is required to bound \(\sigma_{v,k}^{2}\) and point (ii) yields Assumption 4. We have the following computational complexity bound for SVRG-MFLD.

**Theorem 4**.: _Suppose that \(\lambda\alpha\eta\leq 1/4\) and \(\eta\leq\lambda_{1}/(4\lambda_{2})\). Let \(\Xi=\frac{n-B}{B(n-1)}\). Then, under Assumptions 1, 2, 3 and 6, we have the same error bound as Eq. (8) with different \(\bar{\Upsilon}\):_

\[\bar{\Upsilon}=4\eta\delta_{\eta}+\left(1+\sqrt{\frac{\lambda}{\eta}}\right) \left\{\left(R+\lambda_{2}\bar{R}\right)\eta^{2}\sqrt{C_{1}\Xi L^{2}m(\eta^{2}+ \eta\lambda)R^{2}\Xi}+\left[\left(R+\lambda_{2}\bar{R}\right)R\eta^{3}+(L+ \lambda_{2})^{2}\eta^{2}\right]\right\},\]

_under Assumption 6-(ii), and \(\bar{\Upsilon}=C_{1}\Xi L^{2}m\eta^{2}(\eta+\lambda)\) without Assumption 6-(ii)._The proof is given in Appendix D. Therefore, to achieve \(\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*})\leq O( \epsilon)+\frac{4C_{N}}{\lambda\alpha N}\) for a given \(\epsilon>0\), it suffices to set

\[\eta=\frac{\alpha\epsilon}{40L^{2}C_{1}}\wedge\frac{\sqrt{\lambda\alpha \epsilon}}{40L\sqrt{C_{1}}},\,\,k=\frac{2\log(2\Delta_{0}/\epsilon)}{\lambda \alpha\eta}=O\left(\frac{\bar{L}^{2}}{\alpha\epsilon}+\frac{\bar{L}}{\sqrt{ \lambda\alpha\epsilon}}\right)\frac{\log(\epsilon^{-1})}{(\lambda\alpha)},\]

with \(B\geq\left[\sqrt{m}\frac{(\eta+\sqrt{\eta/\lambda})^{2}}{\eta\lambda}\lor m \frac{(\eta+\sqrt{\eta/\lambda})^{3}}{\eta\lambda}\right]\wedge n\). In this setting, the total gradient complexity can be bounded as

\[Bk+\frac{nk}{m}+n\lesssim\max\left\{n^{\frac{1}{3}}\left(1+\sqrt{\frac{\eta}{ \lambda}}\right)^{\frac{4}{4}},\sqrt{n}(\eta\lambda)^{\frac{1}{4}}\left(1+ \sqrt{\frac{\eta}{\lambda}}\right)^{\frac{3}{2}}\right\}\frac{1}{\eta}\frac{ \log(\epsilon^{-1})}{\lambda\alpha}+n,\]

where \(m=\Omega(n/B)=\Omega([n^{2/3}(1+\sqrt{\eta/\lambda})^{-4/3}\wedge\sqrt{n}(1+ \sqrt{\eta/\lambda})^{-3/2}(\sqrt{\eta\lambda})^{-1/2})]\lor 1)\) and \(B=O\left(\left[n^{\frac{1}{3}}\left(1+\sqrt{\frac{\eta}{\lambda}}\right)^{ \frac{4}{3}}\vee\sqrt{n}(\eta\lambda)^{\frac{1}{4}}\left(1+\sqrt{\frac{\eta}{ \lambda}}\right)^{\frac{3}{2}}\right]\wedge n\right)\). When \(\epsilon\) is small, the first term in the right hand side becomes the main term which is \(\max\{n^{1/3},\sqrt{n}(\alpha\epsilon\lambda)^{1/4}\}\frac{\log(\epsilon^{-1}) }{\lambda\alpha^{2}\epsilon}\). Therefore, comparing with the full batch gradient method (F-MFLD), SVRG-MFLD achieves at least \(\min\left\{n^{\frac{2}{3}},\sqrt{n}(\alpha\epsilon\lambda)^{-\frac{1}{4}}\right\}\) times better total computational complexity when \(\eta\leq\lambda\). Note that even without the additional Assumption 6-(ii), we still obtain a \(\sqrt{n}\)-factor improvement (see Appendix D). This indicates that variance reduction is indeed effective to improve the computational complexity, especially in a large sample size setting.

Finally, we compare the convergence rate in Theorem 4 (setting \(F\) to be linear) against prior analysis of standard gradient Langevin dynamics (LD) under LSI. To our knowledge, the current best convergence rate of LD in terms of KL divergence was given in Kinoshita and Suzuki (2022), which yields a \(O\left(\left(n+\frac{\sqrt{n}}{\epsilon}\right)\frac{\log(\epsilon^{-1})}{( \lambda\alpha)^{2}}\right)\) iteration complexity to achieve \(\epsilon\) error. This corresponds to our analysis under Assumption 6-(i) (without (ii)) (see Appendix D). Note that in this setting, our bound recovers their rate even though our analysis is generalized to nonlinear mean-field functionals (the \(O(\lambda)\)-factor discrepancy is due to the difference in the objective - their bound considers the KL divergence while our objective corresponds to \(\lambda\) times the KL divergence). Furthermore, our analysis gives an even faster convergence rate under an additional mild assumption (Assumption 6-(ii)).

## 5 Conclusion

We gave a unified theoretical framework to bound the optimization error of the _single-loop_ mean-field Langevin dynamics (MFLD) that is applicable to the finite-particle, discrete-time, and stochastic gradient algorithm. Our analysis is general enough to cover several important learning problems such as the optimization of mean-field neural networks, density estimation via MMD minimization, and variational inference via KSD minimization. We considered three versions of the algorithms (F-MFLD, SGD-MFLD, SGLD-MFLD); and despite the fact that our analysis deals with a more general setting (mean-field interactions), we are able to recover and even improve existing convergence guarantees when specialized to the standard gradient Langevin dynamics.

## Acknowledgements

TS was partially supported by JSPS KAKENHI (20H00576) and JST CREST (JPMJCR2115). AN was partially supported by JSPS KAKENHI (22H03650). We thank the anonymous reviewers for their careful reading of our manuscript and their insightful comments.

## References

* Abbe et al. (2022) E. Abbe, E. B. Adsera, and T. Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* Arbel et al. (2019) M. Arbel, A. Korba, A. SALIM, and A. Gretton. Maximum mean discrepancy gradient flow. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/944a5ae3483ed5c1e10bbccb7942a279-Paper.pdf.
* Bakry and Emery (1985) D. Bakry and M. Emery. Diffusions hypercontractives. In J. Azema and M. Yor, editors, _Seminaire de Probabilites XIX 1983/84_, pages 177-206, Berlin, Heidelberg, 1985a. Springer Berlin Heidelberg. ISBN 978-3-540-39397-9.
* Bakry and Emery (1985b) D. Bakry and M. Emery. Diffusions hypercontractives in sem. probab. xix lnm 1123, 1985b.
* Bakry et al. (2014) D. Bakry, I. Gentil, M. Ledoux, et al. _Analysis and geometry of Markov diffusion operators_, volume 103. Springer, 2014.
* 353, 2018. doi: 10.3150/16-BEJ879. URL https://doi.org/10.3150/16-BEJ879.
* Cattiaux and Guillin (2014) P. Cattiaux and A. Guillin. _Semi Log-Concave Markov Diffusions_, pages 231-292. Springer International Publishing, Cham, 2014.
* 2321, 2022. doi: 10.3150/21-BEJ1419. URL https://doi.org/10.3150/21-BEJ1419.
* Chen et al. (2022) F. Chen, Z. Ren, and S. Wang. Uniform-in-time propagation of chaos for mean field langevin dynamics. _arXiv preprint arXiv:2212.03050v1_, 2022.
* Chen et al. (2020) Z. Chen, G. M. Rotskoff, J. Bruna, and E. Vanden-Eijnden. A dynamical central limit theorem for shallow neural networks. _arXiv preprint arXiv:2008.09623_, 2020.
* Chewi et al. (2021) S. Chewi, M. A. Erdogdu, M. B. Li, R. Shen, and M. Zhang. Analysis of langevin monte carlo from poincar\(\backslash\)'e to log-sobolev. _arXiv preprint arXiv:2112.12662_, 2021.
* Chizat (2022) L. Chizat. Mean-field langevin dynamics : Exponential convergence and annealing. _Transactions on Machine Learning Research_, 2022. URL https://openreview.net/forum?id=BDqzLHlgEm.
* Chizat and Bach (2018) L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. In _Advances in Neural Information Processing Systems 31_, pages 3040-3050, 2018.
* Chwialkowski et al. (2016) K. Chwialkowski, H. Strathmann, and A. Gretton. A kernel test of goodness of fit. In M. F. Balcan and K. Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 2606-2615, New York, New York, USA, 20-22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/chwialkowski16.html.
* Dalalyan (2014) A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. _arXiv preprint arXiv:1412.7392_, 2014.
* Das et al. (2023) A. Das, D. M. Nagaraj, and A. Raj. Utilising the clt structure in stochastic gradient based sampling : Improved analysis and faster algorithms. In G. Neu and L. Rosasco, editors, _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 4072-4129. PMLR, 12-15 Jul 2023. URL https://proceedings.mlr.press/v195/das23b.html.
* De Bortoli et al. (2020) V. De Bortoli, A. Durmus, X. Fontaine, and U. Simsekli. Quantitative propagation of chaos for sgd in wide neural networks. _Advances in Neural Information Processing Systems_, 33:278-288, 2020.
* Delarue and Tse (2021) F. Delarue and A. Tse. Uniform in time weak propagation of chaos on the torus. _arXiv preprint arXiv:2104.14973_, 2021.
* Delarue et al. (2020)K. A. Dubey, S. J. Reddi, S. A. Williamson, B. Poczos, A. J. Smola, and E. P. Xing. Variance reduction in stochastic gradient langevin dynamics. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/9b698eb3105bd82528f23dc052dedfc0-Paper.pdf.
* Gretton et al. (2006) A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel method for the two-sample-problem. In B. Scholkopf, J. Platt, and T. Hoffman, editors, _Advances in Neural Information Processing Systems_, volume 19. MIT Press, 2006. URL https://proceedings.neurips.cc/paper/2006/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Paper.pdf.
* Holley and Stroock (1987) R. Holley and D. Stroock. Logarithmic sobolev inequalities and stochastic ising models. _Journal of statistical physics_, 46(5-6):1159-1194, 1987.
* Hu et al. (2019) K. Hu, Z. Ren, D. Siska, and L. Szpruch. Mean-field langevin dynamics and energy landscape of neural networks. _arXiv preprint arXiv:1905.07769_, 2019.
* Huang et al. (2021) X. Huang, P. Ren, and F.-Y. Wang. Distribution dependent stochastic differential equations. _Frontiers of Mathematics in China_, 16(2):257-301, 2021.
* Javanmard et al. (2019) A. Javanmard, M. Mondelli, and A. Montanari. Analysis of a two-layer neural network via displacement convexity. _arXiv preprint arXiv:1901.01375_, 2019.
* Johnson and Zhang (2013) R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/acld209c0cc5e5d1c6e28598e8c0be8-Paper.pdf.
* Kac (1956) M. Kac. Foundations of kinetic theory. In _Proceedings of The third Berkeley symposium on mathematical statistics and probability_, volume 3, pages 171-197, 1956.
* Kahn and Harris (1951) H. Kahn and T. E. Harris. Estimation of particle transmission by random sampling. _National Bureau of Standards applied mathematics series_, 12:27-30, 1951.
* Kinoshita and Suzuki (2022) Y. Kinoshita and T. Suzuki. Improved convergence rate of stochastic gradient langevin dynamics with variance reduction and its application to optimization. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=Sj22__iiNX-.
* Korba et al. (2021) A. Korba, P.-C. Aubin-Frankowski, S. Majewski, and P. Ablin. Kernel stein discrepancy descent. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 5719-5730. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/korba21a.html.
* Lacker (2021) D. Lacker. Hierarchies, entropy, and quantitative propagation of chaos for mean field diffusions. _arXiv preprint arXiv:2105.02983_, 2021.
* Lacker and Flem (2022) D. Lacker and L. L. Flem. Sharp uniform-in-time propagation of chaos. _arXiv preprint arXiv:2205.12047_, 2022.
* Liu et al. (2016) Q. Liu, J. Lee, and M. Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In M. F. Balcan and K. Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 276-284, New York, New York, USA, 20-22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/liub16.html.
* Ma et al. (2015) Y.-A. Ma, T. Chen, and E. Fox. A complete recipe for stochastic gradient mcmc. _Advances in neural information processing systems_, 28, 2015.
* McKean (1966) H. P. McKean. A class of markov processes associated with nonlinear parabolic equations. _Proceedings of the National Academy of Sciences_, 56(6):1907-1911, 1966.
* McKean (1966)S. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* Mischler [2019] S. Mischler. An introduction to evolution PDEs, Chapter 0: On the Gronwall lemma, 2019. URL https://www.ceremade.dauphine.fr/~mischler/Enseignements/M2evol2018/chap0.pdf.
* Nitanda and Suzuki [2017] A. Nitanda and T. Suzuki. Stochastic particle gradient descent for infinite ensembles. _arXiv preprint arXiv:1712.05438_, 2017.
* Nitanda et al. [2020] A. Nitanda, D. Wu, and T. Suzuki. Particle dual averaging: Optimization of mean field neural networks with global convergence rate analysis, 2020.
* Nitanda et al. [2021] A. Nitanda, D. Wu, and T. Suzuki. Particle dual averaging: Optimization of mean field neural network with global convergence rate analysis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 19608-19621. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/a34e1ddbb4d329167f50992ba59fe45a-Paper.pdf.
* Nitanda et al. [2022] A. Nitanda, D. Wu, and T. Suzuki. Convex analysis of the mean field langevin dynamics. In G. Camps-Valls, F. J. R. Ruiz, and I. Valera, editors, _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 9741-9757. PMLR, 28-30 Mar 2022.
* Nitanda et al. [2023] A. Nitanda, K. Oko, D. Wu, N. Takenouchi, and T. Suzuki. Primal and dual analysis of entropic fictitious play for finite-sum problems. _arXiv preprint arXiv:2303.02957_, 2023.
* Oko et al. [2022] K. Oko, T. Suzuki, A. Nitanda, and D. Wu. Particle stochastic dual coordinate ascent: Exponential convergent algorithm for mean field neural network optimization. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=PQQp7AJwz3.
* Otto and Villani [2000] F. Otto and C. Villani. Generalization of an inequality by talagrand and links with the logarithmic sobolev inequality. _Journal of Functional Analysis_, 173(2):361-400, 2000.
* Rotskoff and Vanden-Eijnden [2018] G. M. Rotskoff and E. Vanden-Eijnden. Trainability and accuracy of neural networks: An interacting particle system approach. _arXiv preprint arXiv:1805.00915_, 2018.
* Sirignano and Spiliopoulos [2020] J. Sirignano and K. Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. _Stochastic Processes and their Applications_, 130(3):1820-1852, 2020.
* Suzuki et al. [2023] T. Suzuki, A. Nitanda, and D. Wu. Uniform-in-time propagation of chaos for the mean field gradient langevin dynamics. In _Submitted to The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=_JScUk6TBUn.
* Sznitman [1991] A.-S. Sznitman. Topics in propagation of chaos. In _Ecole d'ete de probabilites de Saint-Flour XIX-1989_, pages 165-251. Springer, 1991.
* Vempala and Wibisono [2019] S. Vempala and A. Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. In _Advances in Neural Information Processing Systems_, pages 8094-8106, 2019.
* Welling and Teh [2011] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688, 2011.
* Zou et al. [2018] D. Zou, P. Xu, and Q. Gu. Subsampled stochastic variance-reduced gradient langevin dynamics. In _International Conference on Uncertainty in Artificial Intelligence_, 2018.

## Table of Contents

* 1 Introduction
	* 1.1 Our Contributions
* 2 Problem Setting
	* 2.1 Some Applications of MFLD
	* 2.2 Practical implementations of MFLD
* 3 Main Assumptions and Theoretical Tools
	* 3.1 Proximal Gibbs Measure & Logarithmic Sobolev inequality
* 4 Main Result: Convergence Analysis
	* 4.1 General Recipe for Discretization Error Control
	* 4.2 F-MFLD and SGD-MFLD
	* 4.3 SVRG-MFLD
* 5 Conclusion
* A Verification of Assumptions
* B Proof of Theorem 2
* B.1 Evaluation of Objective Decrease
* B.2 Stochastic Gradient Error (Term \(C\))
* B.2.1 Analysis under Assumption 4
* B.3 Putting Things Together
* C Proof of Theorem 3: F-MFLD and SGD-MFLD
* D Proof of Theorem 4: SVRG-MFLD
* E Auxiliary lemmas
* E.1 Properties of the MFLD Iterates
* E.2 Wasserstein Distance Bound
* E.3 Logarithmic Sobolev Inequality
* E.4 Uniform Log-Sobolev Inequality

## Appendix A Verification of Assumptions

Assumption 2 can be satisfied in the following settings for the three examples presented in Section 2.1.

1. **mean-field neural network.** The neurons \(h_{x}(\cdot)\) and their gradients are bounded (e.g., tanh activation), and the first derivative of the loss is Lipschitz continuous (e.g., squared loss, logistic loss). That is, there exists \(C>0\) such that \(\sup_{x}\sup_{x}|h_{x}(z)|\leq C\), \(\sup_{z}\sup_{x}\|\nabla_{x}h_{x}(z)\|\leq C\) and \(\|\nabla h(x)-\nabla h(x^{\prime})\|\leq C\|x-x^{\prime}\|\) for all \(x,x^{\prime}\in\mathbb{R}^{d}\), and \(\sup_{y,z}\sup_{\mu\in\mathcal{P}}|\partial_{f}\ell(f_{\mu}(z),y)|\leq C\), \(|\partial_{f}\ell(f_{\mu}(z),y)-\partial_{f}\ell(f_{\mu^{\prime}}(z),y)|\leq C |f_{\mu}(z)-f_{\mu^{\prime}}(z)|\) and \(|\partial_{f}^{2}\ell(f_{\mu}(z),y)|\leq C\) for all \((y,z)\) and \(\mu,\mu^{\prime}\in\mathcal{P}\).
2. **MMD minimization.** The kernel \(k\) is smooth and has light tail, e.g., the Gaussian RBF kernel.
3. **KSD minimization.** The kernel \(k\) has a light tail such that \(\sup_{z,z^{\prime}}\max\{|W_{\mu^{*}}(z,z^{\prime})|,\)\(\|\nabla_{z}W_{\mu^{*}}(z,z^{\prime})\|,\)\(\|\nabla_{z}\nabla_{z}^{\top}W_{\mu^{*}}(z,z^{\prime})\|_{\mathrm{op}}\}\leq C\); for example, \(k(z,z^{\prime})=\exp\big{(}-\frac{\|z\|^{2}}{2\sigma_{1}^{2}}-\frac{\|z^{\prime }\|^{2}}{2\sigma_{1}^{2}}-\frac{\|z-z^{\prime}\|^{2}}{2\sigma_{2}^{2}}\big{)}\), and \(\max\{\|\nabla\log(\mu^{*}(z))\|,\|\nabla^{\otimes 2}\log(\mu^{*}(z))\|_{ \mathrm{op}},\)\(\|\nabla^{\otimes 3}\log(\mu^{*}(z))\|_{\mathrm{op}}\}\leq C(1+\|z\|)\).

## Appendix B Proof of Theorem 2

This section gives the proof of Theorem 2. For notation simplicity, we write \(\eta\) to indicate \(\eta_{k}\). Recall that for \(\mathscr{X}=(X^{i})_{i=1}^{N}\), the proximal Gibbs distribution \(\mu_{\mathscr{X}}\) is defined by

\[p_{\mathscr{X}^{\prime}}(X)\propto\exp\left(-\frac{1}{\lambda}\frac{\delta F( \mu_{\mathscr{X}^{\prime}})}{\delta\mu}(X)\right).\]

We also define another version of the proximal Gibbs distribution corresponding to \(\mathscr{F}^{N}\) as

\[p^{(N)}(\mathscr{X})\propto\exp\left(-\frac{N}{\lambda}F(\mu_{\mathscr{X}}) \right).\]

### Evaluation of Objective Decrease

By the same argument as Chen et al. (2022), we have that

\[-\frac{1}{\lambda}\nabla\frac{\delta F(\mu_{\mathscr{X}})}{\delta\mu}(X^{i}) =\nabla\log(p_{\mathscr{X}}(X_{i}))=\nabla_{i}\log(p^{(N)}(\mathscr{X}))=- \frac{N}{\lambda}\nabla_{i}F(\mu_{\mathscr{X}}),\]

where \(\nabla_{i}\) is the partial derivative with respect to \(X_{i}\). Therefore, we have

\[\nabla_{i}\frac{\delta\mathscr{F}^{N}(\mu^{(N)})}{\delta\mu}( \mathscr{X}) =N\nabla_{i}F(\mu_{\mathscr{X}})+\lambda\nabla_{i}\log(\mu^{(N)}( \mathscr{X}))\] \[=\nabla\frac{\delta F(\mu_{\mathscr{X}})}{\delta\mu}(X_{i})+ \lambda\nabla_{i}\log(\mu^{(N)}(\mathscr{X}))\] \[=-\lambda\nabla_{i}\log(p^{(N)}(\mathscr{X}))+\lambda\nabla_{i} \log(\mu^{(N)}(\mathscr{X})).\] (10)

Remembering that \(\mathscr{X}_{k}=(X_{k}^{i})_{i=1}^{N}\) is updated by \(X_{k+1}^{i}=X_{k}^{i}-\eta v_{k}^{i}+\sqrt{2\lambda\eta}\xi_{k}^{i}\), the solutions \(X_{k}^{i}\) and \(X_{k+1}^{i}\) can be interpolated by the following continuous time dynamics:

\[\widetilde{\mathscr{X}_{0}}=\mathscr{X}_{k},\] \[\mathrm{d}\widetilde{X}_{t}^{i}=-v_{k}^{i}\mathrm{d}t+\sqrt{2 \lambda}\mathrm{d}W_{t}^{i},\]

for \(0\leq t\leq\eta\). Then, \(\widetilde{\mathscr{X}_{\eta}}\) obeys the same distribution as \(\mathscr{X}_{k+1}\). Let \(\tilde{\mu}_{t}^{(N)}\) be the law of \(\widetilde{\mathscr{X}_{t}}\). The Fokker-Planck equation of the dynamics yields that

\[\frac{\mathrm{d}\tilde{\mu}_{t}^{(N)}}{\mathrm{d}t}(\mathscr{X}|\mathscr{X}_{ 0:k},\omega_{0:k})=\sum_{i=1}^{N}\nabla_{i}\cdot\Big{(}\tilde{\mu}_{t}^{(N)}( \mathscr{X}|\mathscr{X}_{0:k},\omega_{0:k})v_{k}^{i}\Big{)}+\lambda\sum_{i=1}^ {n}\Delta_{i}\tilde{\mu}_{t}^{(N)}(\mathscr{X}|\mathscr{X}_{0:k},\omega_{0:k}).\]Hence, by taking expectation with respect to \(\mathscr{X}_{0:k}\), it holds that

\[\frac{\mathrm{d}\tilde{\mu}_{t}^{(N)}}{\mathrm{d}t}(\mathscr{X})\] \[=\sum_{i=1}^{N}\lambda\nabla_{i}\cdot\left(\tilde{\mu}_{t}^{(N)}( \mathscr{X})\left(\nabla_{i}\log\left(\frac{\tilde{\mu}_{t}^{(N)}}{p^{(N)}}( \mathscr{X})\right)\right)\right)\] \[\quad+\sum_{i=1}^{N}\nabla_{i}\cdot\left\{\tilde{\mu}_{t}^{(N)}( \mathscr{X})\left(\mathbb{E}_{\mathscr{X}_{0:k}|\bar{\mathscr{X}}_{t}}\left[v _{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})\ \right|\ \bar{\mathscr{X}}_{t}=\mathscr{X},\omega_{k} \right]-\nabla\frac{\delta F(\mu_{\bar{\mathscr{X}}_{t}})}{\delta\mu}(X^{i}) \right)\right\},\]

where we omitted the influence of \(\omega_{0:k}\) to \(\tilde{\mu}_{t}^{(N)}\), which should be written as \(\tilde{\mu}_{t}^{(N)}(\mathscr{X}|\omega_{0:k})\) in a more precise manner. Combining this and Eq. (10) yields the following decomposition,

\[\frac{\mathrm{d}\mathscr{F}^{N}(\tilde{\mu}_{t}^{(N)})}{\mathrm{d }t}\] \[\leq -\underbrace{\frac{3\lambda^{2}}{4}\sum_{i=1}^{N}\mathbb{E}_{ \mathscr{X}\sim\tilde{\mu}_{t}^{(N)}}\left[\left\|\nabla_{i}\log\left(\frac{ \tilde{\mu}_{t}^{(N)}}{p^{(N)}}(\mathscr{X})\right)\right\|^{2}\right]}_{=:A}\] \[+\sum_{i=1}^{N}\underbrace{\mathbb{E}_{\widetilde{\mathscr{X}}_{t },\widetilde{\mathscr{X}}_{0}}\left[\left\|\nabla_{i}\frac{\delta F(\tilde{\mu }_{0}^{(N)})}{\delta\mu}(\widetilde{X}_{0}^{i})-\nabla_{i}\frac{\delta F(\tilde {\mu}_{t}^{(N)})}{\delta\mu}(\widetilde{X}_{t}^{i})\right\|^{2}\right]}_{=:B}\] \[-\sum_{i=1}^{N}\lambda\underbrace{\mathbb{E}_{\widetilde{\mathscr{ X}}_{t},\widetilde{\mathscr{X}}_{0}}\left[\left\langle\nabla_{i}\log\left( \frac{\tilde{\mu}_{t}^{(N)}}{p^{(N)}}(\widetilde{\mathscr{X}}_{t})\right),v_{k }^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})-\nabla\frac{\delta F(\mu_{\widetilde{ \mathscr{X}}_{0}})}{\delta\mu}(\widetilde{X}_{0}^{i})\right\rangle\right]}_{=:C}.\]

**Evaluation of term \(A\):** By the _leave-one-out argument_ in the proof of Theorem 2.1 of Chen et al. (2022), the first term of the right hand side can be upper bounded by

\[-\frac{3\lambda^{2}}{4N}\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim \tilde{\mu}_{t}^{(N)}}\left[\left\|\nabla_{i}\log\left(\frac{\tilde{\mu}_{t}^{( N)}}{p^{(N)}}(\mathscr{X})\right)\right\|^{2}\right]\] \[\leq-\frac{\lambda^{2}}{4N}\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X} \sim\tilde{\mu}_{t}^{(N)}}\left[\left\|\nabla_{i}\log\left(\frac{\tilde{\mu}_ {t}^{(N)}}{p^{(N)}}(\mathscr{X})\right)\right\|^{2}\right]\] \[-\frac{\lambda\alpha}{2}\left(\frac{1}{N}\mathscr{F}^{N}(\tilde{ \mu}_{t}^{(N)})-\mathscr{F}(\mu^{*})\right)+\frac{C_{\lambda}}{N},\] (11)

with a constant \(C_{\lambda}\). The proof is given in Lemma 7 for completeness, where we see that the inequality holds with \(C_{\lambda}=2\lambda L\alpha(1+2c_{L}\bar{R}^{2})+2\lambda^{2}L^{2}\bar{R}^{2}\).

**Evaluation of term \(B\):** By Lemma 2, we have

\[\mathbb{E}_{\widetilde{\mathscr{X}}_{t},\widetilde{\mathscr{X}}_{0}}\left[ \left\|\nabla\frac{\delta F(\tilde{\mu}_{0}^{(N)})}{\delta\mu}(\widetilde{X}_ {0}^{i})-\nabla\frac{\delta F(\tilde{\mu}_{t}^{(N)})}{\delta\mu}(\widetilde{X }_{t}^{i})\right\|^{2}\right]\leq\delta_{\eta_{k}},\]

for \(\delta_{\eta_{k}}=C_{1}L^{2}(\eta_{k}^{2}+\lambda\eta_{k})=O(L^{2}(\eta_{k}^{2 }+\eta_{k}\lambda))\).

### Stochastic Gradient Error (Term \(C\))

Now we evaluate the final term C. First, we derive a bound without Assumption 4. By the Cauchy-Schwarz inequality, we have that

\[\lambda\mathbb{E}_{\widetilde{\mathscr{X}}_{t},\widetilde{\mathscr{X}}_{0}} \left[\left\langle\nabla_{i}\log\left(\frac{\tilde{\mu}_{t}^{(N)}}{p^{(N)}}( \widetilde{\mathscr{X}}_{t})\right),v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i} )-\nabla\frac{\delta F(\mu_{\widetilde{\mathscr{X}}_{0}})}{\delta\mu}( \widetilde{X}_{0}^{i})\right\rangle\right]\]\[\leq \frac{\lambda^{2}}{4}\mathbb{E}_{\widetilde{\mathscr{X}_{t}}, \widetilde{\mathscr{X}_{0}}}\left[\left\|\nabla_{i}\log\left(\frac{\tilde{\mu}_{t }^{(N)}}{p^{(N)}}(\widetilde{\mathscr{X}_{t}})\right)\right\|^{2}\right]+ \mathbb{E}_{\widetilde{\mathscr{X}_{t}},\widetilde{\mathscr{X}_{0}}}\left[ \left\|v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})-\nabla\frac{\delta F(\mu_ {\widetilde{\mathscr{X}_{0}}})}{\delta\mu}(\widetilde{X}_{0}^{i})\right\|^{2}\right]\] \[\leq \frac{\lambda^{2}}{4}\mathbb{E}_{\widetilde{\mathscr{X}_{t}}, \widetilde{\mathscr{X}_{0}}}\left[\left\|\nabla_{i}\log\left(\frac{\tilde{\mu}_ {t}^{(N)}}{p^{(N)}}(\widetilde{\mathscr{X}_{t}})\right)\right\|^{2}\right]+ \sigma_{v,k}^{2}.\]

#### b.2.1 Analysis under Assumption 4

Next, we derive a tighter result under the additional Assumption 4. We decompose term \(C\) as follows:

\[\mathbb{E}_{\widetilde{\mathscr{X}_{t}},\widetilde{\mathscr{X}_{0 }}}\left[\left\langle\nabla_{i}\log\left(\frac{\tilde{\mu}_{t}^{(N)}}{p^{(N)}} (\widetilde{\mathscr{X}_{t}})\right),v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{ i})-\nabla\frac{\delta F(\mu_{\widetilde{\mathscr{X}_{0}}})}{\delta\mu}( \widetilde{X}_{0}^{i})\right\rangle\right]\] \[= \mathbb{E}_{\widetilde{\mathscr{X}_{t}},\widetilde{\mathscr{X}_{0 }}}\left[\left\langle\nabla_{i}\log\left(\tilde{\mu}_{t}^{(N)}(\widetilde{ \mathscr{X}_{t}})\right),v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})-\nabla \frac{\delta F(\mu_{\widetilde{\mathscr{X}_{0}}})}{\delta\mu}(\widetilde{X}_{ 0}^{i})\right\rangle\right]\] \[-\mathbb{E}_{\widetilde{\mathscr{X}_{t}},\widetilde{\mathscr{X}_ {0}}}\left[\left\langle\nabla_{i}\log\left(p^{(N)}(\widetilde{\mathscr{X}_{t}}) \right),v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})-\nabla\frac{\delta F(\mu_{ \widetilde{\mathscr{X}_{0}}})}{\delta\mu}(\widetilde{X}_{0}^{i})\right\rangle \right].\]

Since \(\nabla_{i}\log(\tilde{\mu}_{t}^{(N)})=\nabla_{i}\tilde{\mu}_{t}^{(N)}/\tilde{ \mu}_{t}^{(N)}\), this is equivalent to

\[\mathbb{E}_{\widetilde{\mathscr{X}_{t}},\widetilde{\mathscr{X}_{0 }}}\!\!\left[\!\left\langle\!\left(\tilde{\mu}_{t}^{(N)}(\widetilde{\mathscr{ X}_{t}})^{-1}\nabla_{i}\tilde{\mu}_{t}^{(N)}(\widetilde{\mathscr{X}_{t}}) \!-\!\nabla_{i}\log(p^{(N)})(\widetilde{\mathscr{X}_{t}})\right)\!\!,\!v_{k}^{ i}(\mathscr{X}_{0:k},\omega_{k}^{i})\!-\!\nabla\frac{\delta F(\mu_{\widetilde{ \mathscr{X}_{0}}})}{\delta\mu}(\widetilde{X}_{0}^{i})\right\rangle\!\right]\] \[= \underbrace{\int\mathbb{E}_{\widetilde{\mathscr{X}_{0}}| \widetilde{\mathscr{X}_{t}}}\left[\left\langle\int\nabla_{i}\tilde{\mu}_{t}^{(N )}(\widetilde{\mathscr{X}_{t}}|\mathscr{X}_{0:k}^{\prime})\mu^{(N)}(\mathrm{d} \mathscr{X}_{0:k}^{\prime}),v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})- \nabla\frac{\delta F(\mu_{\widetilde{\mathscr{X}_{t}}})}{\delta\mu}(X_{k}^{i} )\right\rangle\right]\mathrm{d}\widetilde{\mathscr{X}_{t}}}_{C\text{-(I)}}\] \[-\underbrace{\mathbb{E}_{\widetilde{\mathscr{X}_{0}},\widetilde{ \mathscr{X}_{t}}}\left[\left\langle\nabla_{i}\log(p^{(N)})(\widetilde{\mathscr{ X}_{t}}),v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})-\nabla\frac{\delta F(\mu_{ \widetilde{\mathscr{X}_{0}}})}{\delta\mu}(\widetilde{X}_{0}^{i})\right\rangle \right]}_{C\text{-(II)}},\] (12)

where \(\mathscr{X}_{0:k}^{\prime}\) is an independent copy of \(\mathscr{X}_{0:k}^{\prime}\).

_Evaluation of \(C\)-(I):_

(1) The first term (\(C\)-(I)) of the right hand side of Eq. (12) can be evaluated as

\[\int\mathbb{E}_{\widetilde{\mathscr{X}_{0}}|\widetilde{\mathscr{X} _{t}}}\left[\left\langle\int\nabla_{i}\tilde{\mu}_{t}^{(N)}(\widetilde{ \mathscr{X}_{t}}|\mathscr{X}_{0:k}^{\prime})\mu^{(N)}(\mathrm{d}\mathscr{X}_{0:k }^{\prime}),v_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^{i})-\nabla\frac{\delta F(\mu_ {\mathscr{X}_{t}})}{\delta\mu}(X_{k}^{i})\right\rangle\right]\mathrm{d} \widetilde{\mathscr{X}_{t}}\] \[= \int\mathbb{E}_{\widetilde{\mathscr{X}_{0}}|\widetilde{\mathscr{X} _{t}}}\!\!\left[\!\left\langle\int\nabla_{i}\tilde{\mu}_{t}^{(N)}(\widetilde{ \mathscr{X}_{t}}|\mathscr{X}_{0:k}^{\prime})\mu^{(N)}(\mathrm{d}\mathscr{X}_{0:k }^{\prime}),v_{k}^{i}((\widetilde{\mathscr{X}_{t}},\mathscr{X}_{0:k-1}),\omega _{k}^{i})\!-\!\nabla\frac{\delta F(\mu_{\widetilde{\mathscr{X}_{t}}})}{\delta\mu}( \widetilde{X}_{t}^{i})\right\rangle\!\right]\mathrm{d}\widetilde{\mathscr{X}_{t}}\] \[+\int\mathbb{E}_{\widetilde{\mathscr{X}_{0}}|\widetilde{\mathscr{X} _{t}}}\left[\left\langle\int\nabla_{i}\tilde{\mu}_{t}^{(N)}(\widetilde{ \mathscr{X}_{t}}|\mathscr{X}_{0:k}^{\prime})\mathrm{d}\mu^{(N)}(\mathscr{X}_{0:k }^{\prime}),\right.\right.\] \[\left.\left.\hskip 14.226378ptv_{k}^{i}(\mathscr{X}_{0:k},\omega_{k}^ {i})-\nabla\frac{\delta F(\mu_{\mathscr{X}_{k}})}{\delta\mu}(X_{k}^{i})-v_{k}^{ i}((\widetilde{\mathscr{X}_{t}},\mathscr{X}_{0:k-1}),\omega_{k}^{i})+\nabla\frac{\delta F(\mu_{ \widetilde{\mathscr{X}_{t}}})}{\delta\mu}(\widetilde{X}_{t}^{i})\right\rangle \right]\mathrm{d}\widetilde{\mathscr{X}_{t}},\] (13)

where \((\widetilde{\mathscr{X}_{t}},\mathscr{X}_{0:k-1})=(\mathscr{X}_{0},\mathscr{X}_{1}, \ldots,\mathscr{X}_{k-1},\widetilde{\mathscr{X}_{t}})\) which is obtained by replacing \(\mathscr{X}_{k}\) of \(\mathscr{X}_{0:k}\) to \(\widetilde{\mathscr{X}_{t}}\). First, we evaluate the first term in the right hand side of Eq. (13). Let

\[D_{m}^{i}F(\mathscr{X}):=\frac{\delta F(\mu_{\mathscr{X}})}{\delta\mu}(X^{i}),\]

for \(\mathscr{X}=(X^{i})_{i=1}^{N}\), and let \(D_{m}F(\mathscr{X})=(D_{m}^{i}F(\mathscr{X}))_{i=1}^{N}\). Let

\[\Delta v^{i}:=v_{k}^{i}(\mathscr{X}_{0:k}^{\prime},\omega_{k}^{i})-D_{m}^{i}F( \mathscr{X}_{k}^{\prime}),\]and let \(\Delta v=(\Delta v^{i})_{i=1}^{N}\). We also define

\[Z_{i}=(\sqrt{2\lambda t})^{-1}[\widetilde{X}_{t}^{i}-(X_{k}^{\prime i}-tv_{k}^{i }(\mathscr{X}_{0:k}^{\prime},\omega_{k}^{i}))].\]

Then \(Z_{i}\sim N(0,1)\) conditioned by \(\mathscr{X}_{0:k}^{\prime}\) and is independent of \(\omega_{k}^{i}\). We also define

\[v_{k}^{i}(\mathscr{\hat{X}},\omega_{k}^{i}):=\mathbb{E}_{\mathscr{X}_{0:k-1}| \mathscr{\hat{X}},\omega_{1:k}}[v_{k}^{i}((\mathscr{\hat{X}},\mathscr{X}_{0:k- 1}),\omega_{k}^{i})],\]

where the expectation is taken conditioned by \(\omega_{1:k}\) but we omit \(\omega_{1:k-1}\) from the left hand side for the simplicity of notation. Since the density of the conditional distribution \(\tilde{\mu}_{t}^{(N)}(\widetilde{\mathscr{X}_{t}}|\mathscr{X}_{0:k}^{\prime})\) is proportional to \(\exp\left(-\sum_{i=1}^{N}\frac{\|\widetilde{X}_{t}^{i}-(X_{k}^{\prime i}-tv_{k }^{i}(\mathscr{X}_{0:k}^{\prime},\omega_{k}^{i}))\|^{2}}{2(2\lambda t)}\right)\), it holds that, under second order differentiability of \(v_{k}^{i}\) and \(D_{m}F\),

\[\int\left\langle\int-\frac{\widetilde{X}_{t}^{i}-(X_{k}^{\prime i }-tv_{k}^{i}(\mathscr{X}_{0:k}^{\prime},\omega_{k}^{i}))}{2\lambda t}\tilde{ \mu}_{t}^{(N)}(\widetilde{\mathscr{X}_{t}}|\mathscr{X}_{0:k}^{\prime})\mu^{(N )}(\mathrm{d}\mathscr{X}_{0:k}^{\prime}),\right.\] \[\left.v_{k}^{i}(\widetilde{\mathscr{X}_{t}},\omega_{k}^{i})- \nabla\frac{\delta F(\mu_{\overline{x}_{t}^{i}})}{\delta\mu}(\widetilde{X}_{t }^{i})\right\rangle\mathrm{d}\widetilde{\mathscr{X}_{t}}\] \[= \mathbb{E}_{Z,\mathscr{X}_{0:k}^{\prime}}\left[\left\langle-\frac {Z_{i}}{\sqrt{2\lambda t}},v_{k}^{i}(\mathscr{X}_{k}^{\prime}-tD_{m}F( \mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z-t\Delta v,\omega_{k}^{i})-\right.\right.\] \[\left.\left.D_{m}^{i}F(\mathscr{X}_{k}^{\prime}-tD_{m}F( \mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z-t\Delta v)\right\rangle\right]\] \[= \mathbb{E}_{Z,\mathscr{X}_{0:k}^{\prime}}\left[\left\langle-\frac {Z_{i}}{\sqrt{2\lambda t}},\right.\right.\] \[\left.v_{k}^{i}(\mathscr{X}_{k}^{\prime}-tD_{m}F(\mathscr{X}_{k}^ {\prime})+\sqrt{2\lambda t}Z,\omega_{k}^{i})-D_{m}^{i}F(\mathscr{X}_{k}^{ \prime}-tD_{m}F(\mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z)\right.\] \[\left.+v_{k}^{i}(\mathscr{X}_{k}^{\prime}-tv_{k}(\mathscr{X}_{0:k }^{\prime},\omega_{k})+\sqrt{2\lambda t}Z,\omega_{k}^{i})-v_{k}^{i}(\mathscr{ X}_{k}^{\prime}-tD_{m}F(\mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z,\omega_{k}^{i})\right.\] \[\left.-D_{m}^{i}F(\mathscr{X}_{k}^{\prime}-tv_{k}(\mathscr{X}_{0:k }^{\prime},\omega_{k})+\sqrt{2\lambda t}Z)+D_{m}^{i}F(\mathscr{X}_{k}^{\prime }-tD_{m}F(\mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z)\right\rangle\right].\] (14)

By Assumption 4, we can evaluate

\[\mathbb{E}_{\omega_{k}}\Big{[}\Big{\|}v_{k}^{i}(\mathscr{X}_{k}^{ \prime}-tv_{k}(\mathscr{X}_{0:k}^{\prime},\omega_{k})+\sqrt{2\lambda t}Z, \omega_{k}^{i})-v_{k}^{i}(\mathscr{X}_{k}^{\prime}-tD_{m}F(\mathscr{X}_{k}^{ \prime})+\sqrt{2\lambda t}Z,\omega_{k}^{i})\] \[-D_{m}^{i}F(\mathscr{X}_{k}^{\prime}-tv_{k}(\mathscr{X}_{0:k}^{ \prime},\omega_{k})+\sqrt{2\lambda t}Z)+D_{m}^{i}F(\mathscr{X}_{k}^{\prime}- tD_{m}F(\mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z)\Big{\|}\Big{]}\] \[\leq \mathbb{E}_{\mathscr{X}_{0:k-1}^{\prime\prime},\omega_{k}}\Big{[} \Big{\|}v_{k}^{i}((\mathscr{X}_{k}^{\prime}-tv_{k}(\mathscr{X}_{0:k}^{\prime}, \omega_{k})+\sqrt{2\lambda t}Z,\mathscr{X}_{0:k-1}^{\prime\prime}),\omega_{k}^ {i})\] \[-v_{k}^{i}((\mathscr{X}_{k}^{\prime}-tD_{m}F(\mathscr{X}_{k}^{ \prime})+\sqrt{2\lambda t}Z,\mathscr{X}_{0:k-1}^{\prime\prime}),\omega_{k}^{i})\] \[-D_{m}^{i}F(\mathscr{X}_{k}^{\prime}-tv_{k}(\mathscr{X}_{0:k}^{ \prime},\omega_{k})+\sqrt{2\lambda t}Z)+D_{m}^{i}F(\mathscr{X}_{k}^{\prime}- tD_{m}F(\mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z)\Big{\|}\Big{]}\] \[\leq \mathbb{E}_{\mathscr{X}_{0:k-1}^{\prime\prime},\omega_{k}}\left[ \sum_{j=1}^{N}\Big{\|}\nabla_{j}^{\top}v_{k}^{i}((\mathscr{X}_{k}^{\prime}-tD_{m}F( \mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z,\mathscr{X}_{0:k-1}^{\prime\prime }),\omega_{k}^{i})\right.\] \[\left.-\nabla_{j}^{\top}D_{m}^{i}F((\mathscr{X}_{k}^{\prime}-tD_{m }F(\mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z,\mathscr{X}_{0:k-1}^{\prime \prime}))\Big{\|}_{\mathrm{op}}\|t\Delta v_{j}\|\right.\] \[\left.+Qt^{2}\left(\frac{1}{N}\sum_{j=1}^{N}\|\Delta v_{j}\|^{2}+ \|\Delta v_{i}\|^{2}\right)\right],\]

where Jensen's inequality is used in the first inequality. By using Assumption 4 again, the first term in the right hand side can be evaluated as

\[\sum_{j=1}^{N}\mathbb{E}_{\mathscr{X}_{0:k-1}^{\prime\prime},\omega_{k}}\left[ \frac{a_{j}}{2}\|\nabla_{j}^{\top}v_{k}^{i}((\mathscr{X}_{k}^{\prime}-tD_{m}F( \mathscr{X}_{k}^{\prime})+\sqrt{2\lambda t}Z,\mathscr{X}_{0:k-1}^{\prime\prime }),\omega_{k}^{i})\right.\]\[\leq \frac{\lambda}{4}\mathbb{E}\left[\left\|\nabla_{i}\log\left(\tilde{ \mu}_{t}^{(N)}(\widetilde{\mathscr{X}_{t}})/p^{(N)}(\widetilde{\mathscr{X}_{t} })\right\|^{2}\right]\right.\] \[\leq \frac{\lambda}{4}\mathbb{E}\left[\left\|\nabla_{i}\log\left(\tilde {\mu}_{t}^{(N)}(\widetilde{\mathscr{X}_{t}})/p^{(N)}(\widetilde{\mathscr{X}_{ t}})\right\|^{2}\right]\right.\] \[\left.+\frac{1}{\lambda}\int\mathbb{E}_{\widetilde{\mathscr{X}_{ 0}},\widetilde{\mathscr{X}_{t}}}\left[\left\|v_{k}^{i}((\mathscr{X}_{k}, \mathscr{X}_{0:k-1}),\omega_{k}^{i})-\nabla\frac{\delta F(\mu_{\mathscr{X}_{k} })}{\delta\mu}(X_{k}^{i})-v_{k}^{i}((\widetilde{\mathscr{X}_{t}},\mathscr{X}_ {0:k-1}),\omega_{k}^{i})+\nabla\frac{\delta F(\mu_{\widetilde{\mathscr{X}_{t} }^{\prime}})}{\delta\mu}(\widetilde{X}_{t}^{i})\right\|^{2}\right],\]

where we used Jensen's inequality. By Lemma 2 (the same bound applies to \(v_{k}^{i}\) by the same proof), we see that the second term in the right hand side is bounded by

\[\frac{2}{\lambda}\delta_{\eta_{k}}.\] (18)

Finally, we evaluate the second term in the right hand side of Eq. (17). Note that

\[\mathbb{E}_{\widetilde{\mathscr{X}_{0}},\widetilde{\mathscr{X}_{t}}}\left[ \left\langle\nabla_{i}\log\left(p^{(N)}(\widetilde{\mathscr{X}_{t}})\right),\right.\right.\]

[MISSING_PAGE_EMPTY:20]

Moreover, Assumption 2 and Lemma 1 yield that

\[\mathbb{E}\left[\left\|\nabla_{i}\log\left(p^{(N)}(\widetilde{ \mathcal{X}_{0}})\right)\right\|\right] \leq\frac{1}{\lambda}\left(\sup_{\mu\in\mathcal{P},x\in\mathbb{R}^ {d}}\left\|\nabla\frac{\delta U(\mu)}{\delta\mu}(x)\right\|+\mathbb{E}[\|\nabla r (\widetilde{X}_{0}^{i})\|]\right)\] \[\leq\frac{1}{\lambda}\left(R+\mathbb{E}[\|\nabla r(\widetilde{X} _{0}^{i})-r(0)+r(0)\|]\right)\] \[\leq\frac{1}{\lambda}\left(R+\lambda_{2}\mathbb{E}[\|\widetilde{X }_{0}^{i}\|]\right)\] \[\leq\frac{1}{\lambda}\left(R+\lambda_{2}\sqrt{\mathbb{E}[\| \widetilde{X}_{0}^{i}\|^{2}]}\right)\] \[\leq\frac{1}{\lambda}\left(R+\lambda_{2}\bar{R}\right),\]

where the second and third inequalities are due to Assumption 2 and the last inequality is by Lemma 1. Then, the right hand side of Eq. (19) can be bounded by

\[\frac{1}{\lambda}\left(R+\lambda_{2}\bar{R}\right)\left(2t\tilde{ \sigma}_{v,k}\sigma_{v,k}+2Qt^{2}\sigma_{v,k}^{2}\right).\]

where we used Assumption 2 and Lemma 1. Hence, the second term of the right hand side of Eq. (13) (which is same as Eq. (17)) can be bounded by

\[\frac{4}{\lambda}\delta_{\eta_{k}}+\frac{1}{\lambda}\left(R+ \lambda_{2}\bar{R}\right)\left(2t\tilde{\sigma}_{v,k}\sigma_{v,k}+2Qt^{2} \sigma_{v,k}^{2}\right).\]

(3) By summing up the results in (1) and (2), we have that

\[C\text{-(I)}\leq \frac{\lambda}{4}\mathbb{E}\left[\left\|\nabla_{i}\log\left( \tilde{\mu}_{t}^{(N)}(\widetilde{\mathcal{X}_{t}})/p^{(N)}(\widetilde{\mathcal{ X}_{t}})\right\|\right^{2}\right]\] \[+4\delta_{\eta_{k}}+\left(R+\lambda_{2}\bar{R}\right)\left(1+ \sqrt{\frac{\lambda}{t}}\right)\left(2\frac{t}{\lambda}\tilde{\sigma}_{v,k} \sigma_{v,k}+2\frac{Qt^{2}}{\lambda}\sigma_{v,k}^{2}\right).\]

_Evaluation of \(C\text{-(II)}\):_

Next, we evaluate the term \(C\text{-(II)}\). Here, let \(\hat{\mathcal{X}_{t}}=(\hat{X}_{t}^{i})_{i=1}^{n}\) be the following stochastic process:

\[\hat{\mathcal{X}_{0}}=\mathcal{X}_{k},\] \[\mathrm{d}\hat{X}_{t}^{i}=-\nabla\frac{\delta F(\mu_{k})}{\delta \mu}(X_{k}^{i})\mathrm{d}t+\sqrt{2\lambda}\mathrm{d}W_{t}^{i},\]

for \(0\leq t\leq\eta\), where \((W_{t}^{i})_{t}\) is the same Brownian motion as that drives \(\widetilde{X}_{t}^{i}\). By definition, we notice that

\[\mathrm{d}(\widetilde{X}_{t}^{i}-\hat{X}_{t}^{i})=\left(\nabla \frac{\delta F(\mu_{k})}{\delta\mu}(X_{k}^{i})-v_{k}^{i}\right)\mathrm{d}t,\]

which yields that

\[\widetilde{X}_{t}^{i}-\hat{X}_{t}^{i}=t\left(\nabla\frac{\delta F (\mu_{k})}{\delta\mu}(X_{k}^{i})-v_{k}^{i}\right).\] (20)

By subtracting and adding the derivative at \(\hat{\mathcal{X}_{t}}\) from \(\nabla_{i}\log(p^{(N)}(\widetilde{\mathcal{X}_{t}}))\), we have

\[\nabla_{i}\log\left(p^{(N)}(\widetilde{\mathcal{X}_{t}})\right)= -\frac{1}{\lambda}\nabla\frac{\delta F(\mu_{\widetilde{\mathcal{ X}_{t}}})}{\delta\mu}(\widetilde{X}_{t}^{i})\] \[= -\frac{1}{\lambda}\left(\nabla\frac{\delta F(\mu_{\widetilde{ \mathcal{X}_{t}}})}{\delta\mu}(\widetilde{X}_{t}^{i})-\nabla\frac{\delta F(\mu _{\hat{\mathcal{X}_{t}}})}{\delta\mu}(\hat{X}_{t}^{i})\right)-\frac{1}{\lambda} \nabla\frac{\delta F(\mu_{\hat{\mathcal{X}_{t}}})}{\delta\mu}(\hat{X}_{t}^{i}).\]

By Assumptions 1 and 2, the first two terms in the right hand side can be bounded as

\[\left\|\nabla\frac{\delta F(\mu_{\widetilde{\mathcal{X}_{t}}})}{\delta\mu}( \widetilde{X}_{t}^{i})-\nabla\frac{\delta F(\mu_{\hat{\mathcal{X}_{t}}})}{ \delta\mu}(\hat{X}_{t}^{i})\right\|\]\[= \left\|\nabla\frac{\delta U(\mu_{\vec{x}_{t}^{i}})}{\delta\mu}( \widetilde{X}_{t}^{i})+\nabla r(\widetilde{X}_{t}^{i})-\nabla\frac{\delta U(\mu_{ \hat{\mathcal{X}}_{t}^{i}})}{\delta\mu}(\hat{X}_{t}^{i})-\nabla r(\hat{X}_{t}^{ i})\right\|\] \[\leq L(W_{2}(\mu_{\vec{x}_{t}},\mu_{\hat{\mathcal{X}}_{t}^{i}})+\| \widetilde{X}_{t}^{i}-\hat{X}_{t}^{i}\|)+\lambda_{2}\|\widetilde{X}_{t}^{i}- \hat{X}_{t}^{i}\|\] \[\leq t(L+\lambda_{2})\left(\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left\|v_{ k}^{i}-\nabla\frac{\delta F(\mu_{k})}{\delta\mu}(X_{k}^{i})\right\|^{2}}+\left\|v_{ k}^{i}-\nabla\frac{\delta F(\mu_{k})}{\delta\mu}(X_{k}^{i})\right\|\right),\] (21)

where the first inequality follows from

\[\|\nabla r(x)-\nabla r(x^{\prime})\| =\left\|\int_{0}^{1}[\nabla\nabla^{\top}r(\theta x+(1-t)x)](x-x^{ \prime})\mathrm{d}\theta\right\|\] \[\leq\lambda_{2}\int_{0}^{1}\|x-x^{\prime}\|\mathrm{d}\theta= \lambda_{2}\|x-x^{\prime}\|\] (22)

by Assumption 1 and we used Eq. (20) in the last inequality. Therefore, by noticing \(\widetilde{\mathcal{X}_{0}}=\mathcal{X}_{k}\), we can obtain the following evaluation:

\[\sum_{i=1}^{N}C\text{-}\text{(II)}\] \[= \sum_{i=1}^{N}\left|\mathbb{E}_{v_{k}^{i}}\left[\mathbb{E}_{ \widetilde{\mathcal{X}}_{t},\mathcal{X}_{0:k}}\left[\left\langle\nabla_{i} \log\left(p^{(N)}(\widetilde{\mathcal{X}_{t}})\right),v_{k}^{i}(\mathcal{X}_{0: k})-\nabla\frac{\delta F(\mu_{\widetilde{\mathcal{X}_{0}}^{i}})}{\delta\mu}( \widetilde{X}_{0}^{i})\right)\right]\right|\right|\] \[\leq \frac{2(L+\lambda_{2})^{2}}{\lambda}t\sum_{i=1}^{N}\mathbb{E}_{( v_{k}^{1})_{i=1}^{N},\mathcal{X}_{0:k}}\left[\left\|v_{k}^{i}(\mathcal{X}_{0:k})- \nabla\frac{\delta F(\mu_{\widetilde{\mathcal{X}_{0}}^{i}})}{\delta\mu}( \widetilde{X}_{0}^{i})\right\|^{2}\right]\] \[\leq tN\frac{2(L+\lambda_{2})^{2}}{\lambda}\sigma_{v,k}^{2}.\]

_Combining the bounds on \(C\)-(I) and \(C\)-(II):_

\[\sum_{i=1}^{N}\lambda C\leq \lambda\sum_{i=1}^{N}(C\text{-}\text{(I)}+C\text{-}\text{(II)})\] \[\leq \frac{\lambda^{2}}{4}\sum_{i=1}^{N}\mathbb{E}\left[\left\|\nabla_{ i}\log\left(\tilde{\mu}_{t}^{(N)}(\widetilde{\mathcal{X}_{t}})/p^{(N)}( \widetilde{\mathcal{X}_{t}})\right\|^{2}\right]\right.\] \[\left.+N\left\{4\delta_{\eta_{k}}+\left(R+\lambda_{2}\bar{R} \right)\left(1+\sqrt{\frac{\lambda}{t}}\right)\left(2t\tilde{\sigma}_{v,k} \sigma_{v,k}+2Qt^{2}\sigma_{v,k}^{2}\right)+t[2(L+\lambda_{2})^{2}\sigma_{v,k }^{2}]\right\}.\]

### Putting Things Together

By Gronwall lemma (see Mischler (2019), for example), we arrive at

\[\frac{1}{N}\mathbb{E}_{\omega_{0:k}}[\mathscr{F}^{N}(\mu_{k+1}^{ (N)})]-\mathscr{F}(\mu^{*})\] \[\leq\exp(-\lambda\alpha\eta_{k}/2)\left(\frac{1}{N}\mathbb{E}_{ \omega_{0:k-1}}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*})\right)+ \eta_{k}\left(\delta_{\eta_{k}}+\frac{C_{\lambda}}{N}\right)+\sigma_{v,k}^{2} \eta_{k}.\]

In addition, if Assumption 4 is satisfied, we have

\[\frac{1}{N}\mathbb{E}_{\omega_{0:k}}[\mathscr{F}^{N}(\mu_{k+1}^{( N)})]-\mathscr{F}(\mu^{*})\] \[\leq\exp(-\lambda\alpha\eta_{k}/2)\left(\frac{1}{N}\mathbb{E}_{ \omega_{0:k-1}}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*})\right)+ \eta_{k}\left(\delta_{\eta_{k}}+\frac{C_{\lambda}}{N}\right)\] \[\quad+\left[4\eta_{k}\delta_{\eta_{k}}+\left(R+\lambda_{2}\bar{R} \right)\left(1+\sqrt{\frac{\lambda}{\eta_{k}}}\right)\left(\eta_{k}^{2} \tilde{\sigma}_{v,k}\sigma_{v,k}+Q\eta_{k}^{3}\sigma_{v,k}^{2}\right)+\eta_{k}^ {2}(L+\lambda_{2})^{2}\sigma_{v,k}^{2}\right].\]Proof of Theorem 3: F-MFLD and SGD-MFLD

Applying the Gronwall lemma (Mischler, 2019) with Theorem 2, we have that

\[\frac{1}{N}\mathbb{E}_{\omega_{0:k-1}}[\mathscr{F}^{N}(\mu_{k}^{(N)} )]-\mathscr{F}(\mu^{*})\] \[\leq\exp\left(-\lambda\alpha\sum_{j=0}^{k-1}\eta_{j}/2\right) \Delta_{0}+\sum_{i=0}^{k-1}\exp\left(-\lambda\alpha\sum_{j=i}^{k-1}\eta_{j}/2 \right)\left[\Upsilon_{i}+\eta_{i}\left(\delta_{\eta_{i}}+\frac{C_{\lambda}}{N }\right)\right].\] (23)

Here, remember that Assumption 5 yields that

\[\sigma_{v,k}^{2}\leq\frac{R^{2}}{B},\ \ \tilde{\sigma}_{v,k}^{2}\leq\frac{R^{2}} {B}.\]

Under Assumption 5, we can easily check that Assumption 4 holds with \(Q=R\). When \(\eta_{k}=\eta\) for all \(k\geq 0\), we have a uniform upper bound of \(\Upsilon_{k}\) as

\[\Upsilon_{k}\leq\begin{cases}4\eta\delta_{\eta}+\left[R+\lambda_{2}\bar{R}+(L +\lambda_{2})^{2}\right]\left(1+\sqrt{\frac{\lambda}{\eta}}\right)\eta^{2} \frac{R^{2}}{B}&\\ \quad+\left(R+\lambda_{2}\bar{R}\right)R\left(1+\sqrt{\frac{\lambda}{\eta}} \right)\eta^{3}\frac{R^{2}}{B},&\text{with Assumption \ref{Assumption:F-1}-(ii)},\\ \frac{R^{2}}{B}\eta,&\text{without Assumption \ref{Assumption:F-1}-(ii)}.\end{cases}\]

We denote the right hand side as \(\bar{\Upsilon}\). Then we have

\[\frac{1}{N}\mathbb{E}_{\omega_{0:k-1}}[\mathscr{F}^{N}(\mu_{k}^{( N)})]-\mathscr{F}(\mu^{*})\] \[\leq\exp\left(-\lambda\alpha\eta k/2\right)\Delta_{0}+\sum_{i=0}^ {k-1}\exp\left(-\lambda\alpha(k-1-i)\eta/2\right)\left[\bar{\Upsilon}+\eta \left(\delta_{\eta}+\frac{C_{\lambda}}{N}\right)\right]\] \[\leq\exp\left(-\lambda\alpha\eta k/2\right)\Delta_{0}+\frac{1- \exp(-\lambda\alpha k\eta/2)}{1-\exp(-\lambda\alpha\eta/2)}\left[\bar{\Upsilon }+\eta\left(\delta_{\eta}+\frac{C_{\lambda}}{N}\right)\right]\] \[\leq\exp\left(-\lambda\alpha\eta k/2\right)\Delta_{0}+\frac{4}{ \lambda\alpha\eta}\left[\bar{\Upsilon}+\eta\left(\delta_{\eta}+\frac{C_{ \lambda}}{N}\right)\right]\quad\quad(\because\lambda\alpha\eta/2\leq 1/2)\] \[=\exp\left(-\lambda\alpha\eta k/2\right)\Delta_{0}+\frac{4}{ \lambda\alpha}\bar{L}^{2}C_{1}\left(\lambda\eta+\eta^{2}\right)+\frac{4}{ \lambda\alpha\eta}\bar{\Upsilon}+\frac{4C_{\lambda}}{\lambda\alpha N}.\]

Then, by taking

\[k\geq\frac{2}{\lambda\alpha\eta}\log(\Delta_{0}/\epsilon),\]

then the right hand side can be bounded as

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}( \mu^{*})\] \[\leq\epsilon+\frac{4}{\lambda\alpha}\bar{L}^{2}C_{1}\left( \lambda\eta+\eta^{2}\right)+\frac{4}{\lambda\alpha\eta}\bar{\Upsilon}+\frac{4C _{\lambda}}{\lambda\alpha N}.\]

(1) Hence, without Assumption 5-(ii), if we take

\[\eta\leq\frac{\lambda\alpha\epsilon}{8\bar{L}^{2}}\left(C_{1}\lambda\right)^{ -1}\wedge\frac{1}{8\bar{L}}\sqrt{\frac{2\lambda\alpha\epsilon}{C_{1}}},\ \ B\geq 4R^{2}/(\lambda\alpha\epsilon)\]

then the right hand side can be bounded as

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*})\leq 3 \epsilon+\frac{4C_{\lambda}}{\lambda\alpha N}.\]

We can easily check that the number of iteration \(k\) satisfies

\[k=O\left(\frac{\bar{L}^{2}}{\lambda\alpha\epsilon}\lambda+\frac{\bar{L}}{\sqrt {\lambda\alpha\epsilon}}\right)\frac{1}{\lambda\alpha}\log(\epsilon^{-1}),\]in this setting.

(2) On the other hand, under Assumption 5-(ii), if we take

\[\eta\leq\frac{\lambda\alpha\epsilon}{40\bar{L}^{2}}\left(C_{1}\lambda \right)^{-1}\wedge\frac{1}{\bar{L}}\sqrt{\frac{\lambda\alpha\epsilon}{40C_{1}}} \wedge 1,\] \[B\geq 4\left[(1+R)(R+\lambda_{2}\bar{R})+(L+\lambda_{2})^{2} \right]R^{2}(\eta+\sqrt{\eta\lambda})/(\lambda\alpha\epsilon),\]

then the right hand side can be bounded as

\[\frac{1}{\bar{N}}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{* })\leq 3\epsilon+\frac{4C_{\lambda}}{\lambda\alpha N}.\]

We can again check that it suffices to take the number of iteration \(k\) as

\[k=O\left(\frac{\bar{L}^{2}}{\lambda\alpha\epsilon}\lambda+\frac{\bar{L}}{ \sqrt{\lambda\alpha\epsilon}}\right)\frac{1}{\lambda\alpha}\log(\epsilon^{-1}),\]

to achieve the large accuracy.

## Appendix D Proof of Theorem 4: SVRG-MFLD

The standard argument of variance yields

\[\sigma_{v,k}^{2}\leq\max_{1\leq i\leq N}\frac{n-B}{B(n-1)}\frac{1}{n}\sum_{j= 1}^{n}\mathbb{E}[\|\Delta_{k}^{i,j}\|^{2}],\]

where \(\Delta_{k}^{i,j}=\nabla\frac{\delta\ell_{j}(\mu_{k}^{(N)})}{\delta\mu}(X_{k}^ {i})-\nabla\frac{\delta U(\mu_{k}^{(N)})}{\delta\mu}(X_{k}^{i})-\nabla\frac{ \delta\ell_{i}(\mu_{i}^{(N)})}{\delta\mu}(X_{s}^{i})+\nabla\frac{\delta U(\mu_ {i}^{(N)})}{\delta\mu}(X_{s}^{i})\). Here, Assumption 2 and 6 give that

\[\|\Delta_{k}^{i,j}\|^{2}\leq 4(L^{2}W_{2}^{2}(\mu_{k}^{(N)},\mu_{s}^{(N)})+ L^{2}\|X_{k}^{i}-X_{s}^{i}\|^{2})\leq 4L^{2}\left(\frac{1}{N}\sum_{j^{\prime}=1}^ {N}\|X_{k}^{j^{\prime}}-X_{s}^{j^{\prime}}\|^{2}+\|X_{k}^{i}-X_{s}^{i}\|^{2} \right).\]

Taking the expectation, we have

\[\frac{1}{n}\sum_{j=1}^{n}\mathbb{E}[\|\Delta_{k}^{i,j}\|^{2}] \leq 4L^{2}\frac{1}{n}\sum_{j=1}^{n}\sum_{l=s}^{k-1}(\eta^{2} \mathbb{E}[\|v_{l}^{i}\|^{2}]+2\eta\lambda d)\] \[\leq C_{1}L^{2}\underbrace{(k-s)}_{\leq m}(\eta^{2}+\eta\lambda),\]

where we used that \(\mathbb{E}[\|v_{k}^{i}\|^{2}]\leq 2(R^{2}+\lambda_{2}(c_{r}+\bar{R}^{2}))\leq C_{1}\) by Eq. (24) with Lemma 1. Hence, we have that

\[\sigma_{v,k}^{2}\leq C_{1}\Xi L^{2}m(\eta^{2}+\eta\lambda),\]

where \(\Xi=\frac{n-B}{B(n-1)}\).

On the other hand, we have

\[\tilde{\sigma}_{v,k}^{2}\leq\frac{n-B}{B(n-1)}R^{2}=\Xi R^{2},\]

by the same argument with SGD-MFLD. We have a uniform upper bound of \(\Upsilon_{k}\) as

\[\Upsilon_{k}\leq\begin{cases}4\eta\delta_{\eta}+\left(R+\lambda_{2}\bar{R} \right)\left(1+\sqrt{\frac{\lambda}{\eta}}\right)\eta^{2}\sqrt{C_{1}\Xi L^{2 }m(\eta^{2}+\eta\lambda)}\sqrt{R^{2}\Xi}\\ \quad+\left[(R+\lambda_{2}\bar{R})R\eta^{3}+(L+\lambda_{2})^{2}\eta^{2}\right] \left(1+\sqrt{\frac{\lambda}{\eta}}\right)C_{1}\Xi L^{2}m(\eta^{2}+\eta \lambda),&\text{with Assumption \ref{Assumption:2}-(ii)},\\ \eta C_{1}\Xi L^{2}m(\eta^{2}+\eta\lambda),&\text{without Assumption \ref{Assumption:2}-(ii)}.\end{cases}\]

We again let the right hand side be \(\tilde{\Upsilon}\). Then, we have that

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*})\]\[\leq\exp(-\lambda\alpha\eta/2)\left(\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}( \mu_{k-1}^{(N)})]-\mathscr{F}(\mu^{*})\right)+\tilde{\Upsilon}+\eta\left(\delta_ {\eta}+\frac{C_{\lambda}}{N}\right).\]

Then, by the Gronwall's lemma yields that

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F} (\mu^{*})\] \[\leq\exp(-\lambda\alpha\eta k/2)\left(\frac{1}{N}\mathbb{E}[ \mathscr{F}^{N}(\mu_{0}^{(N)})]-\mathscr{F}(\mu^{*})\right)+\sum_{l=1}^{k} \exp(-\lambda\alpha\eta(k-l)/2)\left[\tilde{\Upsilon}+\eta\left(\delta_{\eta}+ \frac{C_{\lambda}}{N}\right)\right]\] \[\leq\exp(-\lambda\alpha\eta k/2)\left(\frac{1}{N}\mathbb{E}[ \mathscr{F}^{N}(\mu_{0}^{(N)})]-\mathscr{F}(\mu^{*})\right)+\frac{1-\exp(- \lambda\alpha\eta k/2)}{1-\exp(-\lambda\alpha\eta/2)}\left[\tilde{\Upsilon}+ \eta\left(\delta_{\eta}+\frac{C_{\lambda}}{N}\right)\right]\] \[\leq\exp(-\lambda\alpha\eta k/2)\left(\frac{1}{N}\mathbb{E}[ \mathscr{F}^{N}(\mu_{0}^{(N)})]-\mathscr{F}(\mu^{*})\right)+\frac{4}{\lambda \alpha\eta}\tilde{\Upsilon}+\frac{4}{\alpha\lambda}\left(\delta_{\eta}+\frac{ C_{\lambda}}{N}\right),\]

where we used \(\lambda\alpha\eta/2\leq 1/2\) in the last inequality.

(1) Without Assumption 6-(ii), we have

\[\frac{1}{\lambda\alpha\eta}\tilde{\Upsilon}=\frac{C_{1}}{\lambda \alpha\eta}\Xi L^{2}m\eta^{2}(\eta+\lambda)=\frac{C_{1}}{\lambda\alpha}L^{2}m \eta(\eta+\lambda)\frac{(n-B)}{B(n-1)},\]

we obtain that

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{ F}(\mu^{*}) \leq\epsilon+\frac{4C_{1}\bar{L}^{2}(\eta^{2}+\lambda\eta)+4C_{ \lambda}/N}{\lambda\alpha}+\frac{4C_{1}}{\lambda\alpha}L^{2}m\eta(\eta+ \lambda)\frac{(n-B)}{B(n-1)}\] \[=\epsilon+\frac{4C_{1}\bar{L}^{2}(\eta^{2}+\lambda\eta)(1+\frac{ (n-B)}{B(n-1)}m)}{\lambda\alpha}+\frac{4C_{\lambda}}{\lambda\alpha N}\]

when

\[k\gtrsim\frac{1}{\lambda\alpha\eta}\log(\epsilon^{-1}),\]

and \(\lambda\alpha\eta/2\leq 1/2\). In particular, if we set \(B\geq m\) and

\[\eta=\frac{\alpha\epsilon}{4L^{2}C_{1}}\wedge\frac{\sqrt{\lambda \alpha\epsilon}}{4\bar{L}\sqrt{C_{1}}},\]

we have

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{ F}(\mu^{*})\leq\epsilon+\frac{4C_{\lambda}}{\alpha\lambda N},\]

with the iteration complexity and the total gradient computation complexity:

\[k\lesssim\left(\frac{\bar{L}^{2}}{\alpha\epsilon}+\frac{\bar{L} }{\sqrt{\lambda\alpha\epsilon}}\right)\frac{1}{(\lambda\alpha)}\log(\epsilon^ {-1}),\] \[Bk+\frac{nk}{m}+n\lesssim\sqrt{n}k+n\lesssim\sqrt{n}\left(\frac{ \bar{L}^{2}}{\alpha\epsilon}+\frac{\bar{L}}{\sqrt{\lambda\alpha\epsilon}} \right)\frac{1}{(\lambda\alpha)}\log(\epsilon^{-1})+n,\]

where \(m=B=\sqrt{n}\).

(2) With Assumption 6-(ii), we have

\[\frac{1}{\lambda\alpha\eta}\tilde{\Upsilon}=\frac{4}{\lambda \alpha}\delta_{\eta}+\frac{1}{\lambda\alpha}O\left(\eta\sqrt{m(\eta^{2}+ \lambda\eta)}+\eta m(\eta^{2}+\lambda\eta)\right)\left(1+\sqrt{\frac{\lambda} {\eta}}\right)\frac{(n-B)}{B(n-1)}.\]

Then, we obtain that

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{ F}(\mu^{*})\] \[\leq\epsilon+\frac{20C_{1}\bar{L}^{2}(\eta^{2}+\lambda\eta)+4C_{ \lambda}/N}{\lambda\alpha}+\frac{1}{\lambda\alpha}O\left(\eta\sqrt{m(\eta^{2}+ \lambda\eta)}+\eta m(\eta^{2}+\lambda\eta)\right)\left(1+\sqrt{\frac{\lambda} {\eta}}\right)\frac{(n-B)}{B(n-1)},\]when

\[k\gtrsim\frac{1}{\lambda\alpha\eta}\log(\epsilon^{-1}),\]

and \(\lambda\alpha\eta/2\leq 1/2\). In particular, if we set \(B\geq\left[\sqrt{m}\frac{(\eta+\sqrt{\eta/\lambda})^{2}}{\eta\lambda}\lor m \frac{(\eta+\sqrt{\eta/\lambda})^{3}}{\eta\lambda}\right]\wedge n\) and

\[\eta=\frac{\alpha\epsilon}{40\bar{L}^{2}C_{1}}\wedge\frac{\sqrt{\lambda\alpha \epsilon}}{\bar{L}\sqrt{40C_{1}}},\]

we have

\[\frac{1}{N}\mathbb{E}[\mathscr{F}^{N}(\mu_{k}^{(N)})]-\mathscr{F}(\mu^{*}) \leq O(\epsilon)+\frac{4C_{\lambda}}{\alpha\lambda N},\]

with the iteration complexity and the total gradient computation complexity:

\[k\lesssim\left(\frac{\bar{L}^{2}}{\alpha\epsilon}+\frac{\bar{L} }{\sqrt{\lambda\alpha\epsilon}}\right)\frac{1}{(\lambda\alpha)}\log(\epsilon^ {-1}),\] \[Bk+\frac{nk}{m}+n\lesssim\max\left\{n^{1/3}\left(1+\sqrt{\eta/ \lambda}\right)^{4/3},\sqrt{n}\left(1+\sqrt{\eta/\lambda}\right)^{3/2}(\sqrt {\eta\lambda})^{1/2}\right\}k+n\] \[\lesssim\max\left\{n^{1/3}\left(1+\sqrt{\eta/\lambda}\right)^{4/ 3},\sqrt{n}\left(1+\sqrt{\eta/\lambda}\right)^{3/2}(\sqrt{\eta\lambda})^{1/2} \right\}\eta^{-1}\frac{1}{(\lambda\alpha)}\log(\epsilon^{-1})+n,\]

where \(m=\Omega(n/B)=\Omega([n^{2/3}(1+\sqrt{\eta/\lambda})^{-4/3}\wedge\sqrt{n}(1+ \sqrt{\eta/\lambda})^{-3/2}(\sqrt{\eta\lambda})^{-1/2})]\lor 1)\) and \(B\geq\left[n^{1\frac{3}{3}}\left(1+\sqrt{\frac{7}{\lambda}}\right)^{\frac{3}{ 3}}\vee\sqrt{n}(\eta\lambda)^{\frac{1}{4}}\left(1+\sqrt{\frac{7}{\lambda}} \right)^{\frac{3}{2}}\right]\wedge n\).

## Appendix E Auxiliary lemmas

### Properties of the MFLD Iterates

Under Assumption 2 with Assumption 5 for SGD-MFLD or 6 for SVRG-MFLD, we can easily verify that

\[\left\|\nabla\frac{\delta U(\mu_{k})}{\delta\mu}(X_{k}^{i})\right\|\leq R,\ \ \mathbb{E}_{\omega_{k}^{i}|\mathscr{X}_{0 \omega}}[\|v_{k}^{i}\|^{2}]\leq 2(R^{2}+\lambda_{2}(c_{r}+\|X_{k}^{i}\|^{2})).\] (24)

**Lemma 1**.: _Under Assumption 2 with Assumption 5 for SGD-MFLD or Assumption 6 for SVRG-MFLD, if \(\eta\leq\lambda_{1}/(4\lambda_{2})\), we have the following uniform bound of the second moment of \(X_{k}^{i}\):_

\[\mathbb{E}[\|X_{k}^{i}\|^{2}]\leq\mathbb{E}[\|X_{0}^{i}\|^{2}]+\frac{2}{ \lambda_{1}}\left[\left(\frac{\lambda_{1}}{8\lambda_{2}}+\frac{1}{2\lambda_{1 }}\right)(R^{2}+\lambda_{2}c_{r})+\lambda d\right],\]

_for any \(k\geq 1\)._

Proof.: By the update rule of \(X_{k}^{i}\) and Assumption 1, we have

\[\mathbb{E}[\|X_{k+1}^{i}\|^{2}] =\mathbb{E}[\|X_{k}^{i}\|^{2}]-2\mathbb{E}[(X_{k}^{i},\eta v_{k} ^{i}+\sqrt{2\eta\lambda}\xi_{k}^{i})]+\mathbb{E}[\|\eta v_{k}^{i}+\sqrt{2\eta \lambda}\xi_{k}^{i}\|^{2}]\] \[=\mathbb{E}[\|X_{k}^{i}\|^{2}]-2\eta\mathbb{E}\left[\left\langle X _{k}^{i},\nabla\frac{\delta U(\mu_{k})}{\delta\mu}(X_{k}^{i})+\nabla r(X_{k}^ {i})\right\rangle\right]+\mathbb{E}[\eta^{2}\|v_{k}^{i}\|^{2}]+2\eta\lambda d\] \[\leq\mathbb{E}[\|X_{k}^{i}\|^{2}]+2\eta\mathbb{E}\mathbb{E}[\| X_{k}^{i}\|]-2\eta\lambda_{1}\mathbb{E}[\|X_{k}^{i}\|^{2}]+2\eta^{2}(R^{2}+ \lambda_{2}(c_{r}+\|X_{k}^{i}\|^{2}))+2\eta\lambda d\] \[\leq(1-2\eta\lambda_{1}+2\eta^{2}\lambda_{2})\mathbb{E}[\|X_{k }^{i}\|^{2}]+2\eta R\mathbb{E}[\|X_{k}^{i}\|]+2\eta(\eta(R^{2}+\lambda_{2}c_{r} )+\lambda d)\] \[\leq(1-\eta\lambda_{1})\mathbb{E}[\|X_{k}^{i}\|^{2}]+2\eta(\eta(R^ {2}+\lambda_{2}c_{r})+\lambda d+R^{2}/\lambda_{1})\] \[\quad(\cdot\cdot\,2R\mathbb{E}[\|X_{k}^{i}\|]\leq\lambda_{1} \mathbb{E}[\|X_{k}^{i}\|^{2}]/2+2R^{2}/\lambda_{1}).\]

where we used the assumption \(\eta\leq\lambda_{1}/(4\lambda_{2})\). Then, by the Gronwall lemma, it holds that

\[\mathbb{E}[\|X_{k}^{i}\|^{2}] \leq(1-\eta\lambda_{1})^{k}\mathbb{E}[\|X_{0}^{i}\|^{2}]+\frac{1 -(1-\eta\lambda_{1})^{k}}{\eta\lambda_{1}}\eta[\eta(R^{2}+\lambda_{2}c_{r})+ \lambda d+R^{2}/\lambda_{1}]\] \[\leq\mathbb{E}[\|X_{0}^{i}\|^{2}]+\frac{1}{\lambda_{1}}[\eta(R^{2 }+\lambda_{2}c_{r})+\lambda d+R^{2}/(\lambda_{1})].\]

This with the assumption \(\eta\leq\lambda_{1}/(4\lambda_{2})\) yields the assertion.

**Lemma 2**.: _Let \(\bar{R}^{2}:=\mathbb{E}[\|X_{0}^{i}\|^{2}]+\frac{1}{\lambda_{1}}\left[\left(\frac{ \lambda_{1}}{4\lambda_{2}}+\frac{1}{\lambda_{1}}\right)(R^{2}+\lambda_{2}c_{r}) +\lambda d\right]\). Under Assumptions 1 and 2, if \(\eta\leq\lambda_{1}/(4\lambda_{2})\) and we define_

\[\delta_{\eta}=8[R^{2}+\lambda_{2}(c_{r}+\bar{R}^{2})+d]\bar{L}^{2}(\eta^{2}+ \lambda\eta),\]

_then it holds that_

\[\mathbb{E}_{\widetilde{\mathcal{X}}_{t},\widetilde{\mathcal{X}}_{0}}\left[ \left\|\nabla\frac{\delta F(\tilde{\mu}_{0}^{(N)})}{\delta\mu}(\widetilde{X}_ {0}^{i})-\nabla\frac{\delta F(\tilde{\mu}_{t}^{(N)})}{\delta\mu}(\widetilde{X} _{t}^{i})\right\|^{2}\right]\leq\delta_{\eta}.\]

Proof.: The proof is basically a mean field generalization of Nitanda et al. (2022). By Assumptions 1 and 2, the first two terms in the right hand side can be bounded as

\[\left\|\nabla\frac{\delta F(\tilde{\mu}_{0}^{(N)})}{\delta\mu}( \widetilde{X}_{0}^{i})-\nabla\frac{\delta F(\tilde{\mu}_{t}^{(N)})}{\delta\mu} (\widetilde{X}_{t}^{i})\right\|\] \[= \left\|\nabla\frac{\delta U(\tilde{\mu}_{0}^{(N)})}{\delta\mu}( \widetilde{X}_{0}^{i})+\nabla r(\widetilde{X}_{0}^{i})-\nabla\frac{\delta U( \tilde{\mu}_{t}^{(N)})}{\delta\mu}(\widetilde{X}_{t}^{i})-\nabla r(\widetilde{ X}_{t}^{i})\right\|\] \[\leq L(W_{2}(\tilde{\mu}_{0}^{(N)},\tilde{\mu}_{t}^{(N)})+\|\widetilde {X}_{0}^{i}-\widetilde{X}_{t}^{i}\|)+\lambda_{2}\|\widetilde{X}_{t}^{i}- \widetilde{X}_{t}^{i}\|.\] (25)

Therefore, the right hand side can be further bounded as

\[\left\|\nabla\frac{\delta F(\tilde{\mu}_{0}^{(N)})}{\delta\mu}( \widetilde{X}_{0}^{i})-\nabla\frac{\delta F(\tilde{\mu}_{t}^{(N)})}{\delta\mu} (\widetilde{X}_{t}^{i})\right\|^{2}\] \[\leq (L+\lambda_{2})^{2}(W_{2}(\tilde{\mu}_{0}^{(N)},\tilde{\mu}_{t}^{ (N)})+\|\widetilde{X}_{0}^{i}-\widetilde{X}_{t}^{i}\|)^{2}\] \[\leq 2\bar{L}^{2}\frac{1}{N}\sum_{i=1}^{N}\|\widetilde{X}_{0}^{i}- \widetilde{X}_{t}^{i}\|^{2}+2\bar{L}^{2}\|\widetilde{X}_{0}^{i}-\widetilde{X} _{t}^{i}\|^{2}\] \[\leq 2\bar{L}^{2}\frac{1}{N}\sum_{i=1}^{N}\left\|tv_{k}^{i}-\sqrt{2t \lambda}\xi_{k}^{i}\right\|^{2}+2\bar{L}^{2}\|tv_{k}^{i}-\sqrt{2t\lambda}\xi_ {k}^{i}\|^{2}.\]

Then, by taking the expectation, it holds that

\[\mathbb{E}_{\widetilde{\mathcal{X}}_{t},\widetilde{\mathcal{X}}_ {0}}\left[\left\|\nabla\frac{\delta F(\tilde{\mu}_{0}^{(N)})}{\delta\mu}( \widetilde{X}_{0}^{i})-\nabla\frac{\delta F(\tilde{\mu}_{t}^{(N)})}{\delta\mu }(\widetilde{X}_{t}^{i})\right\|^{2}\right]\] \[\leq 2\bar{L}^{2}\frac{1}{N}\sum_{i=1}^{N}(t^{2}\mathbb{E}[\|v_{k}^{ i}\|^{2}]+2t\lambda d)+2\bar{L}^{2}(t^{2}\mathbb{E}[\|v_{k}^{i}\|^{2}]+2t \lambda d)\] \[\leq 4\bar{L}^{2}[t^{2}2(R^{2}+\lambda_{2}(c_{r}+\bar{R}^{2}))+2t \lambda d]\ \ \ \ (\because\text{Lemma \ref{eq:L-2}})\] \[= 8\bar{L}^{2}[t^{2}(R^{2}+\lambda_{2}(c_{r}+\bar{R}^{2}))+t \lambda d]\] \[\leq 8[R^{2}+\lambda_{2}(c_{r}+\bar{R}^{2})+d]\bar{L}^{2}(t^{2}+t \lambda).\]

Then, by noticing \(t\leq\eta\), we obtain the assertion. 

### Wasserstein Distance Bound

Recall that \(W_{2}(\mu,\nu)\) is the 2-Wasserstein distance between \(\mu\) and \(\nu\). We let \(D(\mu,\nu)=\int\log\left(\frac{\nu}{\mu}\right)\mathrm{d}\nu\) be the KL-divergence between \(\mu\) and \(\nu\).

**Lemma 3**.: _Under Assumptions 1 and 3, it holds that_

\[W_{2}^{2}(\mu^{(N)},\mu^{*N})\leq\frac{2}{\lambda\alpha}(\mathcal{F}^{N}(\mu^{( N)})-N\mathcal{F}(\mu^{*})).\]Proof.: By Assumption 3, \(\mu^{*}\) satisfies the LSI condition with a constant \(\alpha>0\). Then, it is known that its tensor product \(\mu^{*N}\) also satisfies the LSI condition with the same constant \(\alpha\) (see, for example, Proposition 5.2.7 of Bakry et al. (2014)). Then, Otto-Villani theorem Otto and Villani (2000) yields the Talagrand's inequality of the tensorized measure \(\mu^{*N}\):

\[W_{2}^{2}(\mu^{(N)},\mu^{*N})\leq\frac{2}{\alpha}D(\mu^{(N)},\mu^{*N}).\]

Moreover, the proof of Theorem 2.11 of Chen et al. (2022) yields that, under Assumption 1, it holds that

\[D(\mu^{(N)},\mu^{*N})\leq\frac{1}{\lambda}(\mathscr{F}^{N}(\mu^{ (N)})-N\mathscr{F}(\mu^{*})).\] (26)

**Lemma 4**.: _Suppose that \(|h_{x}(z)-h_{x^{\prime}}(z)|\leq L\|x-x^{\prime}\|\)\((\forall x,x^{\prime}\in\mathbb{R}^{d})\) and let \(V_{\mu^{*}}:=\mathrm{Var}_{\mu^{*}}(f_{\mu^{*}})=\int(f_{\mu^{*}}(z)-h_{x}(z)) ^{2}\mathrm{d}\mu^{*}(x)\), then it holds that_

\[\mathbb{E}_{\mathscr{X}_{k}\sim\mu_{k}^{(N)}}[(f_{\mu_{\mathscr{X }_{k}}}(z)-f_{\mu^{*}}(z))^{2}]\leq\frac{2L^{2}}{N}W_{2}^{2}(\mu_{k}^{(N)},\mu ^{*N})+\frac{2}{N}V_{\mu^{*}}.\]

Proof.: Consider a coupling \(\gamma\) of \(\mu_{\mathscr{X}_{k}}^{(N)}\) and \(\mu^{*N}\) and let \((\mathscr{X}_{k},\mathscr{X}_{*})=((X_{k}^{i})_{i=1}^{N},(X_{*}^{i})_{i=1}^{N})\) be a random variable obeying the law \(\gamma\). Then,

\[(f_{\mu_{\mathscr{X}_{k}}}(z)-f_{\mu^{*}}(z))^{2} =\left(\frac{1}{N}\sum_{i=1}^{N}h_{X_{k}^{i}}(z)-\int h_{x}(z) \mathrm{d}\mu^{*}(x)\right)^{2}\] \[=\left(\frac{1}{N}\sum_{i=1}^{N}(h_{X_{k}^{i}}(z)-h_{X_{k}^{i}}(z ))+\frac{1}{N}\sum_{i=1}^{N}h_{X_{i}^{i}}(z)-\int h_{x}(z)\mathrm{d}\mu^{*}(x) \right)^{2}\] \[\leq 2\left(\frac{1}{N}\sum_{i=1}^{N}(h_{X_{k}^{i}}(z)-h_{X_{*}^{ i}}(z))\right)^{2}+2\left(\frac{1}{N}\sum_{i=1}^{N}h_{X_{*}^{i}}(z)-\int h_{x}(z) \mathrm{d}\mu^{*}(x)\right)^{2}\] \[\leq 2L^{2}\frac{1}{N}\sum_{i=1}^{N}\|X_{k}^{i}-X_{*}^{i}\|^{2}+2 \left(\frac{1}{N}\sum_{i=1}^{N}h_{X_{*}^{i}}(z)-\int h_{x}(z)\mathrm{d}\mu^{*} (x)\right)^{2}.\]

Then, by taking the expectation of the both side with respect to \((\mathscr{X}_{k},\mathscr{X}_{*})\), we have that

\[\mathbb{E}_{\mathscr{X}_{k}\sim\mu_{k}^{(N)}}[(f_{\mu_{\mathscr{X }_{k}}}(z)-f_{\mu^{*}}(z))^{2}]\] \[\leq 2L^{2}\mathbb{E}_{\gamma}\left[\frac{1}{N}\sum_{i=1}^{N}\|X_ {k}^{i}-X_{*}^{i}\|^{2}\right]+\frac{2}{N}V_{\mu^{*}}.\]

Hence, taking the infimum of the coupling \(\gamma\) yields that

\[\mathbb{E}_{\mathscr{X}_{k}\sim\mu_{k}^{(N)}}[(f_{\mu_{\mathscr{X }_{k}}}(z)-f_{\mu^{*}}(z))^{2}]\leq\frac{2L^{2}}{N}W_{2}^{2}(\mu_{k}^{(N)},\mu ^{*N})+\frac{2}{N}V_{\mu^{*}}.\]

**Remark 2**.: _By the self-consistent condition of \(\mu^{*}\), we can see that \(\mu^{*}\) is sub-Gaussian. Therefore, \(V_{\mu^{*}}<\infty\) is always satisfied._

### Logarithmic Sobolev Inequality

**Lemma 5**.: _Under Assumptions 1 and 2, \(\mu^{*}\) and \(p_{\mathscr{X}}\) satisfy the LSI condition with a constant_

\[\alpha\geq\frac{\lambda_{1}}{2\lambda}\exp\left(-4\frac{R^{2}}{\lambda_{1} \lambda}\sqrt{2d/\pi}\right)\!\!\vee\!\left\{\frac{4\lambda}{\lambda_{1}}+ \left(\frac{R}{\lambda_{1}}+\sqrt{\frac{2\lambda}{\lambda_{1}}}\right)^{2}e^{ \frac{R^{2}}{2\lambda_{1}\lambda}}\left[2+d+\frac{d}{2}\log\left(\frac{\lambda _{2}}{\lambda_{1}}\right)+4\frac{R^{2}}{\lambda_{1}\lambda}\right]\right\}^{-1}.\]

Proof.: In the following, we give two lower bounds of \(\alpha\). By taking the maximum of the two, we obtain the assertion.

(1) By Assumption 2, \(\frac{\delta U(\mu)}{\delta\mu}\) is Lipschitz continuous with the Lipschitz constant \(R\), which implies that \(\frac{1}{\lambda}\frac{\delta U(\mu)}{\delta\mu}\) is \(R/\lambda\)-Lipschitz continuous. Hence, \(\mu^{*}\) and \(p_{\mathscr{X}}\) is a Lipshitz perturbation of \(\nu(x)\propto\exp(-\lambda^{-1}r(x))\). Since \(\lambda^{-1}\nabla\nabla^{\top}r(x)\succeq\frac{\lambda_{1}}{\lambda}I\) by Assumption 1, Miclo's trick (Lemma 2.1 of Bardet et al. (2018)) yields that \(\mu^{*}\) and \(p_{\mathscr{X}}\) satisfy the LSI with a constant

(2) We can easily check that \(V(x)=\frac{r(x)}{\lambda}\) and \(H(x)=\frac{1}{\lambda}\frac{\delta U(\tilde{\mu})}{\delta\mu}(x)\), for appropriately chosen \(\tilde{\mu}\), satisfies the conditions in Lemma 6 with \(c_{1}=\frac{\lambda_{1}}{\lambda}\), \(c_{2}=\frac{\lambda_{2}}{\lambda}\), \(c_{V}=c_{r}\), and \(\bar{L}=\frac{R}{\lambda}\). Hence, \(\mu^{*}\) and \(p_{\mathscr{X}}\) satisfy the LSI with a constant \(\alpha\) such that

\[\alpha\geq\left\{\frac{4\lambda}{\lambda_{1}}+\left(\frac{L}{\lambda_{1}}+ \sqrt{\frac{2\lambda}{\lambda_{1}}}\right)^{2}e^{\frac{R^{2}}{2\lambda_{1} \lambda}}\left[2+d+\frac{d}{2}\log\left(\frac{\lambda_{2}}{\lambda_{1}}\right) +4\frac{R^{2}}{\lambda_{1}\lambda}\right]\right\}^{-1}.\]

**Lemma 6** (Log Sobolev inequality with Lipschitz perturbation).: _Let \(\nu(x)\propto\exp(-V(x))\) where \(\nabla\nabla^{\top}V(x)\succeq c_{1}I\), \(x^{\top}\nabla V(x)\geq c_{1}\|x\|^{2}\) and \(0\leq V(x)\leq c_{2}(c_{V}+\|x\|^{2})\) with \(c_{1},c_{2},c_{V}>0\), and let \(H:\mathbb{R}^{d}\to\mathbb{R}\) is a Lipschitz continuous function with the Lipschitz constant \(\bar{L}\). Suppose that \(\mu\in\mathcal{P}\) is given by \(\mu(x)\propto\exp(-H(x))\nu(x)\). Then, \(\mu\) satisfies the LSI with a constant \(\alpha\) such that_

\[\alpha\geq\left\{\frac{4}{c_{1}}+\left(\frac{\bar{L}}{c_{1}}+\sqrt{\frac{2}{c _{1}}}\right)^{2}e^{\frac{\bar{L}^{2}}{2c_{1}}}\left[2+d+\frac{d}{2}\log\left( \frac{c_{2}}{c_{1}}\right)+4\frac{\bar{L}^{2}}{c_{1}}\right]\right\}^{-1}.\]

Proof.: Since \(\nu\) is a strongly log-concave distribution, the Bakry-Emery argument (Bakry and Emery, 1985b) yields that it satisfies the LSI condition with a constant \(\alpha^{\prime}=c_{1}\) by the assumption \(\nabla\nabla^{\top}V(x)\succeq c_{1}I\).

Next, we evaluate the second moment of \(\nu\) because it is required in the following analysis. Since we know that \(\nu\) is the stationary distribution of the SDE \(\mathrm{d}X_{t}=-\nabla V(X_{t})\mathrm{d}+\sqrt{2}\mathrm{d}W_{t}\), its corresponding infinitesimal generator gives that

\[\frac{\mathrm{d}}{\mathrm{d}t}\mathbb{E}[\|X_{t}\|^{2}]=\mathbb{E}[-2X_{t}^{ \top}\nabla V(X_{t})]+2d\leq-2c_{1}\mathbb{E}[\|X_{t}\|^{2}]+2d.\]

Then, by the Gronwall lemma, we have

\[\mathbb{E}[\|X_{t}\|^{2}] \leq\exp(-2c_{1}t)\mathbb{E}[\|X_{0}\|^{2}]+\int_{0}^{t}\exp(-2c_{ 1}(t-s)))\mathrm{d}s2d\] \[=\exp(-2c_{1}t)\mathbb{E}[\|X_{0}\|^{2}]+(1-\exp(-2c_{1}t))\frac{ d}{c_{1}}.\]

Hence, by taking \(X_{0}=0\) (a.s.), we see that \(\limsup_{t}\mathbb{E}[\|X_{t}\|^{2}]\leq d/c_{1}\) that yields \(\mathbb{E}_{\nu}[\|X^{2}\|]\leq d/c_{1}\).

A distribution \(\tilde{\mu}\) satisfies the Poincare inequality with a constant \(\tilde{\alpha}\) if it holds that

\[\mathbb{E}_{\tilde{\mu}}[(f-\mathbb{E}_{\tilde{\mu}}[f])^{2}]\leq\frac{1}{ \tilde{\alpha}}\mathbb{E}_{\tilde{\mu}}[\|\nabla f\|^{2}],\]for all bounded \(f\) with bounded derivatives. By Example (3) in Section 7.1 of Cattiaux and Guillin (2014), \(\mu\) satisfies the Poincare inequality with a constant \(\tilde{\alpha}\) such that

\[\frac{1}{\tilde{\alpha}}\leq\frac{1}{2}\left(\frac{2\bar{L}}{c_{1}}+\sqrt{ \frac{8}{c_{1}}}\right)^{2}e^{\frac{\bar{L}^{2}}{2c_{1}}}.\]

Here, let \(G(x)=H(x)-H(0)\). We can see that \(\mu(x)\) can be expressed as \(\mu(x)=\frac{\exp(-G(x))\nu(x)}{Z_{G}}\) with a normalizing constant \(Z_{G}>0\). Note that, by the Lipschitz continuity assumption of \(H\), we have

\[|G(x)|\leq\bar{L}\|x\|.\]

Hence, we have the following evaluation of the normalizing constant \(Z_{G}\):

\[Z_{G} =\int\exp(-G(x))\nu(x)\mathrm{d}x\] \[\geq\int\exp\left(-\bar{L}\|x\|-c_{2}(c_{V}+\|x\|^{2})\right) \frac{1}{\sqrt{(2\pi(1/c_{1}))^{d}}}\mathrm{d}x\] \[\geq\int\exp\left(-\frac{3c_{2}}{2}\|x\|^{2}-\frac{\bar{L}^{2}}{2 c_{2}}-c_{2}c_{V}\right)\frac{1}{\sqrt{(2\pi(1/c_{1}))^{d}}}\mathrm{d}x\] \[=\left(\frac{c_{1}}{3c_{2}}\right)^{d/2}\exp\left(-\frac{\bar{L} ^{2}}{2c_{2}}-c_{2}c_{V}\right).\]

Theorem 2.7 of Cattiaux and Guillin (2022) claims that \(\mu\) satisfies the LSI with a constant \(\alpha\) such that

\[\frac{2}{\alpha}\leq\frac{(\beta+1)(1+\theta^{-1})}{\beta}\frac{2}{\alpha^{ \prime}}+\frac{1}{\tilde{\alpha}}(2+\mathbb{E}_{\nu}[G-\log(Z_{G})])+\bar{L}^ {2}\frac{2}{\tilde{\alpha}\alpha^{\prime}}\left(\frac{(1+\theta)(1+\beta)}{4 \beta}+\frac{\beta^{2}}{2}\right),\]

for any \(\beta>0\) and \(\theta>0\). The right hand side can be bounded by

\[\frac{(\beta+1)(1+\theta^{-1})}{\beta}\frac{2}{c_{1}}+\frac{1}{ \tilde{\alpha}}\left(2+\bar{L}\mathbb{E}_{\nu}[\|X\|]+\frac{\bar{L}^{2}}{2c_{2 }}+\frac{d}{2}\log(3c_{2}/c_{1})\right)+\bar{L}^{2}\frac{2}{\tilde{\alpha}c_{ 1}}\left(\frac{(1+\theta)(1+\beta)}{4\beta}+\frac{\beta^{2}}{2}\right)\] \[\leq \frac{(\beta+1)(1+\theta^{-1})}{\beta}\frac{2}{c_{1}}+\frac{1}{ \tilde{\alpha}}\left(2+\bar{L}\sqrt{\frac{d}{c_{1}}}+\frac{\bar{L}^{2}}{2c_{1 }}+\frac{d}{2}\log(3c_{2}/c_{1})\right)+\bar{L}^{2}\frac{2}{\tilde{\alpha}c_{ 1}}\left(\frac{(1+\theta)(1+\beta)}{4\beta}+\frac{\beta^{2}}{2}\right)\] \[\leq \frac{(\beta+1)(1+\theta^{-1})}{\beta}\frac{2}{c_{1}}+\frac{1}{ \tilde{\alpha}}\left(2+\frac{\bar{L}^{2}}{c_{1}}+\frac{d}{2}(\log(3c_{2}/c_{1 })+1)\right)+\bar{L}^{2}\frac{2}{\tilde{\alpha}c_{1}}\left(\frac{(1+\theta)( 1+\beta)}{4\beta}+\frac{\beta^{2}}{2}\right)\] \[\leq \frac{(\beta+1)(1+\theta^{-1})}{\beta}\frac{2}{c_{1}}+\frac{1}{ \tilde{\alpha}}\left[2+\frac{d}{2}(\log(c_{2}/c_{1})+2)+\frac{\bar{L}^{2}}{c_ {1}}\left(1+\frac{(1+\theta)(1+\beta)}{2\beta}+\beta^{2}\right)\right].\]

Hence, if we set \(\beta=1\) and \(\theta=1\), we have

\[\frac{1}{\alpha} \leq 2\frac{2}{c_{1}}+\frac{1}{2\tilde{\alpha}}\left[2+\frac{d}{2}( \log(c_{2}/c_{1})+2)+\frac{\bar{L}^{2}}{c_{1}}\left(1+2+1\right)\right],\] \[\leq\frac{4}{c_{1}}+\left(\frac{\bar{L}}{c_{1}}+\sqrt{\frac{2}{c _{1}}}\right)^{2}e^{\frac{\bar{L}^{2}}{2c_{1}}}\left[2+d+\frac{d}{2}\log\left( \frac{c_{2}}{c_{1}}\right)+4\frac{\bar{L}^{2}}{c_{1}}\right].\]

### Uniform Log-Sobolev Inequality

**Lemma 7** (Uniform log-Sobolev inequality).: _Under the same condition as Theorem 2, it holds that_

\[-\frac{\lambda^{2}}{2N}\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\bar{\mu}_{i}^{ (N)}}\left[\left\|\nabla_{i}\log\left(\frac{\bar{\mu}_{i}^{(N)}}{p^{(N)}}( \mathscr{X})\right)\right\|^{2}\right]\leq-\frac{\lambda\alpha}{2}\left(\frac {1}{N}\mathscr{F}^{N}(\bar{\mu}_{t}^{(N)})-\mathscr{F}(\mu^{*})\right)+\frac{C_{ \lambda}}{N},\] (27)

_with a constant \(C_{\lambda}=2\lambda L\alpha(1+2c_{L}\bar{R}^{2})+2\lambda^{2}L^{2}\bar{R}^{2}\)._Proof.: The derivation is analogous to Chen et al. (2022), but we present the full proof for the sake of the completeness. For \(\mathscr{X}=(X_{i})_{i=1}^{N}\), let \(\mathscr{X}^{-i}:=(X_{j})_{j\neq i}\). Then, it holds that

\[-\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}} \left[\left\|\nabla_{i}\log\left(\frac{\tilde{\mu}_{t}^{(N)}}{p^{(N)}}( \mathscr{X})\right)\right\|^{2}\right]\] \[= -\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}} \left[\left\|\nabla_{i}\log\left(\tilde{\mu}_{t}^{(N)}(\mathscr{X})\right)- \frac{1}{\lambda}\nabla\frac{\delta F(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(X_{i })\right\|^{2}\right]\] \[= -\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}} \left[\left\|\nabla_{i}\log\left(\tilde{\mu}_{t}^{(N)}(\mathscr{X})\right)- \frac{1}{\lambda}\nabla\frac{\delta F(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(X_{i })+\frac{1}{\lambda}\nabla\frac{\delta F(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(X _{i})\right.\] \[\left.\qquad\qquad-\frac{1}{\lambda}\nabla\frac{\delta F(\mu_{ \mathscr{X}})}{\delta\mu}(X_{i})\right\|^{2}\right]\] \[\leq -\frac{1}{2}\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_ {t}^{(N)}}\left[\left\|\nabla_{i}\log\left(\tilde{\mu}_{t}^{(N)}(\mathscr{X}) \right)-\frac{1}{\lambda}\nabla\frac{\delta F(\mu_{\mathscr{X}^{-i}})}{\delta \mu}(X_{i})\right\|^{2}\right]\] \[+\sum_{i=1}^{N}L^{2}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N )}}[W_{2}^{2}(\mu_{\mathscr{X}},\mu_{\mathscr{X}^{-i}})]\] \[= -\frac{1}{2}\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_ {t}^{(N)}}\left[\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}}\left[\left\| \nabla_{i}\log\left(\tilde{\mu}_{t}^{(N)}(\mathscr{X})\right)-\frac{1}{\lambda }\nabla\frac{\delta F(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(X_{i})\right\|^{2} \left|\mathscr{X}^{-i}\right]\right]\] \[+\sum_{i=1}^{N}L^{2}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N )}}[W_{2}^{2}(\mu_{\mathscr{X}},\mu_{\mathscr{X}^{-i}})],\]

where the second inequality is due to the Lipschitz continuity of \(\nabla\frac{\delta U}{\delta\mu}\) in terms of the Wasserstein distance (Assumption 2). Here, the term corresponding to the Wasserstein distance can be upper bounded as

\[\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}}[W_{2}^{2}(\mu_{ \mathscr{X}},\mu_{\mathscr{X}^{-i}})] \leq\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}}\left[\frac {1}{N(N-1)}\sum_{j\neq i}\|X_{j}-X_{i}\|^{2}\right]\] \[\leq\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}}\left[\frac {2}{N(N-1)}\sum_{j\neq i}\|X_{j}\|^{2}+\frac{2}{N}\|X_{i}\|^{2}\right]\] \[\leq\frac{4}{N}\bar{R}^{2}.\]

Denote by \(P_{X_{i}|\mathscr{X}^{-i}}\) the conditional law of \(X_{i}\) conditioned by \(\mathscr{X}^{-i}\) and denote by \(P_{\mathscr{X}^{-i}}\) the marginal law of \(\mathscr{X}^{-i}\) where the joint law of \(\mathscr{X}\) is \(\tilde{\mu}_{t}^{(N)}\). Then, it holds that

\[\nabla_{i}\log(\tilde{\mu}_{t}^{(N)}(\mathscr{X}))=\frac{\nabla_{i} (P_{X_{i}|\mathscr{X}^{-i}}(X_{i})P_{\mathscr{X}^{-i}}(\mathscr{X}^{-i}))}{P_ {X_{i}|\mathscr{X}^{-i}}(X_{i})P_{\mathscr{X}^{-i}}(\mathscr{X}^{-i})}=\frac{ \nabla_{i}P_{X_{i}|\mathscr{X}^{-i}}(X_{i})}{P_{X_{i}|\mathscr{X}^{-i}}(X_{i})} =\nabla_{i}\log(P_{X_{i}|\mathscr{X}^{-i}}(X_{i})).\] \[-\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}^{-i}\sim P_{\mathscr{X}^{- i}}}\left[\mathbb{E}_{X_{i}\sim P_{X_{i}|\mathscr{X}^{-i}}}\left[\left\|\nabla_{i} \log\left(P_{X_{i}|\mathscr{X}^{-i}}(X_{i})\right)-\frac{1}{\lambda}\nabla \frac{\delta F(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(X_{i})\right\|^{2}\left| \mathscr{X}^{-i}\right]\right]\] \[\leq -2\alpha\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{ (N)}}\left[D(p_{\mathscr{X}^{-i}},P_{X_{i}|\mathscr{X}^{-i}})\right],\]by the LSI condition of the proximal Gibbs measure (Assumption 3). Let \(\nu_{r}\) be the distribution with the density \(\nu_{r}\propto\exp(-r(x)/\lambda)\). Note that the proximal Gibbs measure is the minimizer of the linearized objective:

\[p_{\mathscr{X}^{-i}} =\operatorname*{argmin}_{\mu\in\mathcal{P}}\left\{\int\frac{ \delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(X_{i})\mathrm{d}\mu+\mathbb{E}_{ \mu}[r]+\lambda\mathrm{Ent}(\mu)\right\}\] \[=\operatorname*{argmin}_{\mu\in\mathcal{P}}\left\{\int\frac{ \delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(X_{i})\mathrm{d}\mu+\lambda D( \nu_{r},\mu)\right\}.\]

Then, by the optimality of the proximal Gibbs measure, it holds that

\[\lambda D(p_{\mathscr{X}^{-i}},P_{X_{i}|\mathscr{X}^{-i}})\] \[= \int\frac{\delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(x_{i})(P _{X_{i}|\mathscr{X}^{-i}}-p_{\mathscr{X}^{-i}})(\mathrm{d}x_{i})+\lambda D( \nu_{r},P_{X_{i}|\mathscr{X}^{-i}})-\lambda D(\nu_{r},p_{\mathscr{X}^{-i}})\] \[\geq \int\frac{\delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(x_{i})(P _{X_{i}|\mathscr{X}^{-i}}-\mu^{*})(\mathrm{d}x_{i})+\lambda D(\nu_{r},P_{X_{i }|\mathscr{X}^{-i}})-\lambda D(\nu_{r},\mu^{*}).\] (28)

The expectation of the first term of the right hand side can be further evaluated as

\[\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}} \left[\int\frac{\delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(x_{i})(P_{X_{i}| \mathscr{X}^{-i}}-\mu^{*})(\mathrm{d}x_{i})\right]\] \[= \sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}} \left[\int\frac{\delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(x_{i})\delta_{X_{ i}}(\mathrm{d}x_{i})-\int\frac{\delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(x_{i}) \mu^{*}(\mathrm{d}x_{i})\right]\] \[\geq \sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}} \left\{\int\frac{\delta U(\mu_{\mathscr{X}^{)}}}{\delta\mu}(x_{i})\delta_{X_{ i}}(\mathrm{d}x_{i})-\int\frac{\delta U(\mu_{\mathscr{X}^{)}}}{\delta\mu}(x_{i}) \mu^{*}(\mathrm{d}x_{i})\right.\] \[\left.-\frac{2L}{N}\left[2+c_{L}(\|X_{i}\|^{2}+\mathbb{E}_{X\sim \mu^{*}}[\|X\|^{2}]+\|X_{i}\|^{2}+\frac{1}{N-1}\sum_{j\neq i}\|X_{j}\|^{2}) \right]\right\}\] \[\geq N\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}}\left[U(\mu_{ \mathscr{X}})-U(\mu^{*})\right]-4L\left(1+2c_{L}\bar{R}^{2}\right),\]

where the first inequality is due to Lemma 8 and the second inequality is due to Lemma 1 (we also notice that \(\mathbb{E}_{X\sim\mu^{*}}[\|X\|^{2}]\leq\bar{R}^{2}\)). The second term in the right hand side of Eq. (28) can be evaluated as

\[\sum_{i=1}^{N}\mathbb{E}_{\tilde{\mu}_{t}^{(N)}}[D(\nu_{r},P_{X_{ i}|\mathscr{X}^{-i}})] =\sum_{i=1}^{N}\mathbb{E}_{\tilde{\mu}_{t}^{(N)}}[\mathrm{Ent}(P _{X_{i}|\mathscr{X}^{-i}})]-\mathbb{E}_{\tilde{\mu}_{t}^{(N)}}\left[\sum_{i=1} ^{N}\log(\nu_{r}(X_{i}))\right]\] \[\geq\mathrm{Ent}(\tilde{\mu}_{t}^{(N)})-\mathbb{E}_{\tilde{\mu}_{ t}^{(N)}}\left[\sum_{i=1}^{N}\log(\nu_{r}(X_{i}))\right]=D(\nu_{r}^{N},\tilde{ \mu}_{t}^{(N)}),\]

where we used Lemma 3.6 of Chen et al. (2022) in the first inequality. Combining all of them, we arrive at

\[-\sum_{i=1}^{N}\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{t}^{(N)}} \left[\left\|\nabla_{i}\log\left(\frac{\tilde{\mu}_{t}^{(N)}}{p^{(N)}}( \mathscr{X})\right)\right\|^{2}\right]\] \[\leq -\frac{\alpha}{\lambda}\left[N\mathbb{E}_{\mathscr{X}\sim\tilde{ \mu}_{t}^{(N)}}[U(\mu_{\mathscr{X}})]+\lambda D(\nu_{r}^{N},\tilde{\mu}_{t}^{(N )})-N(U(\mu^{*})+\lambda D(\nu_{r},\mu^{*}))-4L\left(1+2c_{L}\bar{R}^{2}\right) \right]+4L^{2}\bar{R}^{2}\]\[= -\frac{\alpha}{\lambda}\left[N\mathbb{E}_{\mathscr{X}\sim\tilde{\mu}_{ i}^{(N)}}[F(\mu_{\mathscr{X}})]+\lambda\text{Ent}(\tilde{\mu}_{t}^{(N)})-N(F(\mu^{*})+ \lambda\text{Ent}(\mu^{*}))-4L\left(1+2c_{L}\bar{R}^{2}\right)\right]+4L^{2} \bar{R}^{2}\] \[= -\frac{\alpha}{\lambda}\left(\mathscr{F}^{N}(\tilde{\mu}_{t}^{(N) })-N\mathscr{F}(\mu^{*})\right)+\frac{4L\alpha}{\lambda}\left(1+2c_{L}\bar{R} ^{2}\right)+4L^{2}\bar{R}^{2}.\]

Then, we have the assertion with

\[C_{\lambda}=2\lambda L\alpha(1+2c_{L}\bar{R}^{2})+2\lambda^{2}L^{2}\bar{R}^{2}.\]

\(\Box\)

**Lemma 8**.: _With the same setting as Lemma 7, it holds that_

\[\left|\frac{\delta U(\mu_{\mathscr{X}})}{\delta\mu}(x)-\frac{\delta U(\mu_{ \mathscr{X}^{-i}})}{\delta\mu}(x)\right|\leq\frac{L}{N}\left[2+c_{L}\Bigg{(}2 \|x\|^{2}+\|X_{i}\|^{2}+\frac{1}{N-1}\sum_{j\neq i}\|X_{j}\|^{2}\Bigg{)}\right]\]

Proof.: Let \(\mu_{\mathscr{X},\theta}:=\theta\mu_{\mathscr{X}}+(1-\theta)\mu_{\mathscr{X} ^{-i}}\). Then, we see that

\[\left|\frac{\delta U(\mu_{\mathscr{X}})}{\delta\mu}(x)-\frac{ \delta U(\mu_{\mathscr{X}^{-i}})}{\delta\mu}(x)\right|= \int_{0}^{1}\left|-\frac{1}{N}\frac{\delta^{2}U(\mu_{\mathscr{X},\theta})}{\delta\mu^{2}}(x,X_{i})+\frac{1}{N(N-1)}\sum_{j\neq i}\frac{\delta^ {2}U(\mu_{\mathscr{X},\theta})}{\delta\mu^{2}}(x,X_{j})\right|\mathrm{d}\theta\] \[\leq \frac{L}{N}\left[2+c_{L}\Bigg{(}2\|x\|^{2}+\|X_{i}\|^{2}+\frac{1} {N-1}\sum_{j\neq i}\|X_{j}\|^{2}\Bigg{)}\right],\]

where we used Assumption 2 (this is the only place where the boundedness of the second order variation of \(U\) is used). Hence, we obtain the assertion.