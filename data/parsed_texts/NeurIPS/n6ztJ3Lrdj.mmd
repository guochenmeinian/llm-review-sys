# Learning with Explanation Constraints

 Rattana Pukdee

Carnegie Mellon University

rpukdee@cs.cmu.edu

&Dylan Sam

Carnegie Mellon University

dylansam@andrew.cmu.edu

&J. Zico Kolter

Carnegie Mellon University

Bosch Center for AI

zkolter@cs.cmu.edu

&Maria-Florina Balcan

Carnegie Mellon University

ninamf@cs.cmu.edu

&Pradeep Ravikumar

Carnegie Mellon University

pkr@cs.cmu.edu

Equal contribution

###### Abstract

As larger deep learning models are hard to interpret, there has been a recent focus on generating explanations of these black-box models. In contrast, we may have apriori explanations of how models should behave. In this paper, we formalize this notion as learning from _explanation constraints_ and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. One may naturally ask, "When would these explanations be helpful?" Our first key contribution addresses this question via a class of models that satisfies these explanation constraints in expectation over new data. We provide a characterization of the benefits of these models (in terms of the reduction of their Rademacher complexities) for a canonical class of explanations given by gradient information in the settings of both linear models and two layer neural networks. In addition, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraints more frequently, when compared to simpler augmented Lagrangian methods to incorporate these explanations. We demonstrate the benefits of our approach over a large array of synthetic and real-world experiments.

## 1 Introduction

There has been a considerable recent focus on generating explanations of complex black-box models so that humans may better understand their decisions. These can take the form of feature importance [31, 35], counterfactuals [31, 35], influential training samples [18, 43], etc. But what if humans were able to provide explanations for how these models should behave? We are interested in the question of how to learn models given such apriori explanations. A recent line of work incorporates explanations as a regularizer, penalizing models that do not exhibit apriori given explanations [33, 32, 15, 36]. For example, Rieger et al. [32] penalize the feature importance of spurious patches on a skin-cancer classification task. These methods lead to models that inherently satisfy "desirable" properties and, thus, are more trustworthy. In addition, some of these empirical results suggest that constraining models via explanations also leads to higher accuracy and robustness to changing test environments. However, there is no theoretical analysis to explain this phenomenon.

We note that such explanations can arise from domain experts and domain knowledge, but also other large "teacher" models that might have been developed for related tasks. An attractive facet of the latter is that we can automatically generate model-based explanations given unlabeled data points. For instance, we can use segmentation models to select the background pixels of images solely onunlabeled data, which we can use in our model training. We thus view incorporating explanation constraints from such teacher models as a form of knowledge distillation into our student models [13].

In this paper, we provide an analytical framework for learning from explanations to reason when and how explanations can improve the model performance. We first provide a mathematical framework for model constraints given explanations. Casting explanations as functionals \(g\) that take in a model \(h\) and input \(x\) (as is standard in explainable AI), we can represent domain knowledge of how models should behave as constraints on the values of such explanations. We can leverage these to then solve a constrained ERM problem where we additionally constrain the model to satisfy these explanation constraints. Since the explanations and constraints are provided on randomly sampled inputs, these constraints are random. Nevertheless, via standard statistical learning theoretic arguments [38], any model that satisfies the set of explanation constraints on the finite sample can be shown to satisfy the constraints in expectation up to some slack with high probability. In our work, we term a model that satisfies explanations constraints in expectation, an _CE model_ (see Definition 1). Then, we can capture the benefit of learning with explanation constraints by analyzing the generalization capabilities of this class of CE models (Theorem 3.2). This analysis builds off of a learning theoretic framework for semi-supervised learning of Balcan and Blum [1, 2]. We remark that if the explanation constraints are arbitrary, it is not possible to reason if a model satisfies the constraints in expectation based on a finite sample. We provide a detailed discussion on when this argument is possible in Appendix B,D. In addition, we note that our work also has a connection with classical approaches in stochastic programming [16, 4] and is worth investigating this relationship further.

Another key contribution of our work is concretely analyzing this framework for a canonical class of explanation constraints given by gradient information for linear models (Theorem 4.1) and two layer neural networks (Theorem 4.2). We focus on gradient constraints as we can represent many different notions of explanations, such as feature importance and ignoring spurious features. These corollaries clearly illustrate that restricting the hypothesis class via explanation constraints can lead to fewer required labeled data. Our results also provide a quantitative measure of the benefits of the explanation constraints in terms of the number of labeled data. We also discuss when learning these explanation constraints makes sense or is possible (i.e., with a finite generalization bound). We note that our framework allows for the explanations to be noisy, and not fully satisfied by even the Bayes optimal classifier. Why then would incorporating explanation constraints help? As our analysis shows, this is by reducing the estimation error (variance) by constraining the hypothesis class, at the expense of approximation error (bias). We defer the question of how to explicitly denoise noisy explanations to future work.

Now that we have provided a learning theoretic framework for these explanation constraints, we next consider the algorithmic question: how do we solve for these explanation-constrained models? In general, these constraints are not necessarily well-behaved and are difficult to optimize. One can use augmented Lagrangian approaches [33, 7], or simply regularized versions of our constrained problems [32] (which however do not in general solve the constrained problems for non-convex parameterizations but is more computationally tractable). We draw from seminal work in posterior regularization [9], which has also been studied in the capacity of model distillation [14], to provide a variational objective. Our objective is composed of two terms; supervised empirical risk and the discrepancy between the current model and the class of CE models. The optimal solution of our objective is also the optimal solution of the constrained problem which is consistent with our theoretical analysis. Our objective naturally incorporates unlabeled data and provides a simple way to

Figure 1: A restricted hypothesis class \(\mathcal{H}_{\phi,\alpha}\) (left). Our algorithmic solution to solve a proposed variational objective in Section 5 (right).

control the trade-off between explanation constraints and the supervised loss (Section 5). We propose a tractable algorithm that iteratively trains a model on the supervised data, and then approximately projects this learnt model onto the class of CE models. Finally, we provide an extensive array of experiments that capture the benefits of learning from explanation constraints. These experiments also demonstrate that the variational approach improves over simpler augmented Lagrangian approaches and can lead to models that indeed satisfy explanations more frequently.

## 2 Related Work

**Explainable AI.** Recent advances in deep learning have led to models that achieve high performance but which are also highly complex [20; 11]. Understanding these complex models is crucial for safe and reliable deployments of these systems in the real-world. One approach to improve our understanding of a model is through explanations. This can take many forms such as feature importance [31; 35; 23; 37], high level concepts [17; 44], counterfactual examples [39; 12; 25], robustness of gradients [41], or influential training samples [18; 43].

In contrast to generating post-hoc explanations of a given model, we aim to learn models given apriori explanations. There has been some recent work along such lines. Koh et al. [19], Zarlenga et al. [45] incorporates explanations within the model architecture by requiring a conceptual bottleneck layer. Ross et al. [33], Rieger et al. [32], Ismail et al. [15], Stacey et al. [36] use explanations to modify the learning procedure for any class of models: they incorporate explanations as a regularizer, penalizing models that do not exhibit apriori given explanations; Ross et al. [33] penalize input gradients, while Rieger et al. [32] penalize a Contextual Decomposition score [26]. Some of these suggest that constraining models via explanations leads to higher accuracies and more robustness to spurious correlation, but do not provide analytical guarantees. On the theoretical front, Li et al. [22] show that models that are easier to explain locally also generalize well. However, Bilodeau et al. [3] show that common feature attribution methods without additional assumptions on the learning algorithm or data distribution do no better than random guessing at inferring counterfactual model behavior.

**Learning Theory.** Our contribution is to provide an analytical framework for learning from explanations that quantify the benefits of explanation constraints. Our analysis is closely related to the framework of learning with side information. Balcan and Blum [2] shows how unlabeled data can help in semi-supervised learning through a notion of compatibility between the data and the target model. This work studies classical notions of side information (e.g., margin, smoothness, and co-training). Subsequent papers have adapted this learning theoretic framework to study the benefits of representation learning [10] and transformation invariance [34]. On the contrary, our paper focuses on the more recent notion of explanations. Rather than focus on the benefits of unlabeled data, we characterize the quality of different explanations. We highlight that constraints here are stochastic, as they depend on data points which differs from deterministic constraints that have been considered in existing literature, such as constraints on the norm of weights (i.e., L2 regularization).

**Self-Training.** Our work can also be connected to the self-training literature [5; 42; 40; 8], where we could view our variational objective as comprising a regularized (potentially simpler) teacher model that encodes these explanation constraints into a student model. Our variational objective (where we use simpler teacher models) is also related to distillation, which has also been studied in terms of gradients [6].

## 3 Learning from Explanation Constraints

Let \(\mathcal{X}\) be the instance space and \(\mathcal{Y}\) be the label space. We focus on binary classification where \(\mathcal{Y}=\{-1,1\}\), but which can be naturally generalized. Let \(\mathcal{D}\) be the joint data distribution over \((\mathcal{X},\mathcal{Y})\) and \(\mathcal{D}_{\mathcal{X}}\) the marginal distribution over \(\mathcal{X}\). For any classifier \(h:\mathcal{X}\rightarrow\mathcal{Y}\), we are interested in its classification error \(\operatorname{err}(h):=\Pr_{(x,y)\sim D}(h(x)\neq y)\), though one could also use other losses to define classification error. Our goal is to learn a classifier with small error from a family of functions \(\mathcal{H}\). In this work, we use the words model and classifier interchangeably. Now, we formalize local explanations as functionals that take in a model and a test input, and output a vector:

**Definition 1** (Explanations).: _Given an instance space \(\mathcal{X}\), model hypothesis class \(\mathcal{H}\), and an explanation functional \(g:\mathcal{H}\times\mathcal{X}\rightarrow\mathbb{R}^{r}\), we say \(g(h,x)\) is an explanation of \(h\) on point \(x\) induced by \(g\)._For simplicity, we consider the setting when \(g\) takes a single data point and model as input, but this can be naturally extended to multiple data points and models. We can combine these explanations with prior knowledge on how explanations should look like at sample points in term of constraints.

**Definition 2** (Explanation Constraint Set).: _For any instance space \(\mathcal{X}\), hypothesis class \(\mathcal{H}\), an explanation functional \(g:\mathcal{H}\times\mathcal{X}\rightarrow\mathbb{R}^{r}\), and a family of constraint sets \(\{C(x)\subseteq\mathbb{R}^{r}\mid x\in\mathcal{X}\}\), we say that \(h\in\mathcal{H}\) satisfies the explanation constraints with respect to \(C\) iff:_

\[g(h,x)\in C(x),\ \forall x\in\mathcal{X}.\]

In our definition, \(C(x)\) represents values that we believe our explanations should take at a point \(x\). For example, "an input gradient of a feature 1 must be larger than feature 2" can be represented by \(g(h,x)=\nabla_{x}h(x)\) and \(\bar{C}(x)=\{(x_{1},\dots,x_{d})\in\mathbb{R}^{d}\mid x_{1}>x_{2}\}\). In practice, human annotators will be able to provide the constraint set \(C(x^{\prime})\) for a random sample \(k\) data points \(S_{E}=\{x_{1}^{\prime},\dots,x_{k}^{\prime}\}\) drawn i.i.d. from \(\mathcal{D}_{\mathcal{X}}\). We then say that any \(h\in\mathcal{H}\)\(S_{E}\)-satisfies the explanation constraints with respect to \(C\) iff \(g(h,x)\in C(x),\ \forall x\in S_{E}\). We note that the constraints depends on random samples \(x_{i}^{\prime}\) and therefore _are random_. To tackle this challenge, we can draw from the standard learning theoretic arguments to reason about probably approximately satisfying the constraints in expectation. Before doing so, we first consider the notion of explanation surrogate losses, which will allow us to generalize the setup above to a form that is amenable to practical estimators.

**Definition 3**.: _(Explanation surrogate loss) An explanation surrogate loss \(\phi:\mathcal{H}\times\mathcal{X}\rightarrow\mathbb{R}\) quantifies how well a model \(h\) satisfies the explanation constraint \(g(h,x)\in C(x)\). For any \(h\in\mathcal{H},x\in\mathcal{X}\):_

1. \(\phi(h,x)\geq 0\)_._
2. _If_ \(g(h,x)\in C(x)\) _then_ \(\phi(h,x)=0\)_._

For example, we could define \(\phi(h,x)=1\{g(h,x)\in C(x)\}\). Given such a surrogate loss, we can substitute the explanation constraint that \(g(h,x)\in C(x)\) with the surrogate \(\phi(h,x)\leq 0\). We now have the machinery to formalize how to reason about the random explanation constraints given a random set of inputs. First, denote the expected explanation loss as \(\phi(h,\mathcal{D}):=\mathbb{E}_{x\sim\mathcal{D}}[\phi(h,x)]\). We are interested in models that satisfy the explanation constraints up to some slack \(\tau\) (i.e. approximately) in expectation. We define a learnability condition of this explanation surrogate loss as EPAC (Explanation Probably Approximately Correct ) learnability.

**Definition 4** (EPAC learnability).: _For any \(\delta\in(0,1),\tau>0\), the sample complexity of \((\delta,\tau)\) - EPAC learning of \(\mathcal{H}\) with respect to a surrogate loss \(\phi\), denoted \(m(\tau,\delta;\mathcal{H},\phi)\) is defined as the smallest \(m\in\mathbb{N}\) for which there exists a learning rule \(\mathcal{A}\) such that every data distribution \(\mathcal{D}_{\mathcal{X}}\) over \(\mathcal{X}\), with probability at least \(1-\delta\) over \(S\sim\mathcal{D}^{m}\),_

\[\phi(\mathcal{A}(S),\mathcal{D})\leq\inf_{h\in\mathcal{H}}\phi(h,\mathcal{D})+\tau.\]

_If no such \(m\) exists, define \(m(\tau,\delta;\mathcal{H},\phi)=\infty\). We say that \(\mathcal{H}\) is EPAC learnable in the agnostic setting with respect to a surrogate loss \(\phi\) if \(\ \forall\delta\in(0,1),\tau>0\), \(m(\tau,\delta;\mathcal{H},\phi)\) is finite._

_Furthermore, for a constant \(\tau\), we denote any model \(h\in\mathcal{H}\) with \(\tau\)-Approximately Correct Explanation where \(\phi(h,\mathcal{D})\leq\tau\), with a \(\tau\) - CE models. We define the class of \(\tau\) - CE models as_

\[\mathcal{H}_{\phi,\mathcal{D},\tau}=\{h\in\mathcal{H}\ :\ \phi(h,\mathcal{D}) \leq\tau\}.\] (1)

We simply use \(\mathcal{H}_{\phi,\tau}\) to denote this class of CE models. From natural statistical learning theoretic arguments, a model that satisfies the random constraints in \(S_{E}\) might also be a CE model.

**Proposition 3.1**.: _Suppose a model \(h\)\(S_{E}\)-satisfies the explanation constraints then_

\[\phi(h,\mathcal{D}_{\mathcal{X}})\leq 2R_{k}(\mathcal{G})+\sqrt{\frac{\ln(4/ \delta)}{2k}},\]

_with probability at least \(1-\delta\), when \(k=|S_{E}|\) and \(\mathcal{G}=\{\phi(h,\cdot)\mid h\in\mathcal{H}\}\)._

We use \(R_{k}(\cdot)\) to denote Rademacher complexity; please see Appendix A where we review this and related concepts. Note that even when \(h\) satisfies the constraints exactly on \(S\), we can only guarantee a bound on the expected surrogate loss \(\phi(h,\mathcal{D}_{\mathcal{X}})\).We can achieve a bound similar to that in Proposition 3.1 via a single and simpler constraint on the empirical expectation \(\frac{1}{|S_{E}|}\sum_{x\in S_{E}}\phi(h,x)\). We can then extend the above proposition to show that if \(\phi(h,S_{E})\leq\tau\), then \(\phi(h,D_{\mathcal{X}})\leq\tau+2R_{k}(\mathcal{G})+\sqrt{\frac{\ln(4/\delta)}{2 k}},\) with probability at least \(1-\delta\). Another advantage of such a constraint is that the explanation constraints could be noisy, or it may be difficult to satisfy them exactly, so \(\tau\) also serves as a slack. The class \(\mathcal{G}\) contains all surrogate losses of any \(h\in\mathcal{H}\). Depending on the explanation constraints, \(\mathcal{G}\) can be extremely large. We remark that the surrogate loss \(\phi\) allows us to reason about satisfying an explanation constraint on a new data point and in expectation. However, for many constraints, \(\phi\) does not have a closed-form or is unknown on an unseen data point. The question of which types of explanation constraints are generalizable may be of independent interest, and we further discuss this in Appendix B and provide further examples of learnable constraints in Appendix D.

**EPAC-ERM Objective.** Let us next discuss _combining_ the two sources of information: the explanation constraints that we set up in the previous section, together with the usual set of labeled training samples \(S=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\) drawn i.i.d. from \(\mathcal{D}\) that informs the empirical risk. Combining these, we get what we call EPAC-ERM objective:

\[\min_{h\in\mathcal{H}}\frac{1}{n}\sum_{i=1}^{n}\ell(h,x_{i},y_{i})\ \ \text{s.t.}\ \ \frac{1}{k}\sum_{i=1}^{k}\phi(h,x_{i}^{\prime})\leq\tau.\] (2)

We provide a learnability condition for a model that achieve both low average error and surrogate loss in Appendix F.

### Generalization Bound

We assume that we are in a doubly agnostic setting. Firstly, we are agnostic in the usual sense that there need be no classifier in the hypothesis class \(\mathcal{H}\) that perfectly labels \((x,y)\); instead, we hope to achieve the best error rate in the hypothesis class, \(h^{*}=\arg\min_{h\in\mathcal{H}}\operatorname{err}_{\mathcal{D}}(h)\). Secondly, we are also agnostic with respect to the explanations, so that the optimal classifier \(h^{*}\) may not satisfy the explanation constraints exactly, so that it incurs nonzero surrogate explanation loss \(\phi(h^{*},D)>0\).

**Theorem 3.2** (Generalization Bound for Agnostic Setting).: _Consider a hypothesis class \(\mathcal{H}\), distribution \(\mathcal{D}\), and explanation loss \(\phi\). Let \(S=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\) be drawn i.i.d. from \(\mathcal{D}\) and \(S_{E}=\{x_{1}^{\prime},\ldots,x_{k}^{\prime}\}\) drawn i.i.d. from \(\mathcal{D}_{\mathcal{X}}\). With probability at least \(1-\delta\), for \(h\in\mathcal{H}\) that minimizes empirical risk \(\operatorname{err}_{S}(h)\) and has \(\phi(h,S_{E})\leq\tau\), we have_

\[\operatorname{err}_{D}(h)\leq\operatorname{err}_{D}(h^{*}_{\tau-\varepsilon_{ k}})+2R_{n}(\mathcal{H}_{\phi,\tau+\varepsilon_{k}})+2\sqrt{\frac{\ln(4/\delta)}{2 n}},\]

\[\varepsilon_{k}=2R_{k}(\mathcal{G})+\sqrt{\frac{\ln(4/\delta)}{2k}},\]

_when \(\mathcal{G}=\{\phi(h,x)\mid h\in\mathcal{H},x\in\mathcal{X}\}\) and \(h^{*}_{\tau}=\arg\min_{h\in\mathcal{H}_{\phi,\tau}}\operatorname{err}_{ \mathcal{D}}(h)\)._

Proof.: The proof largely follows the arguments in Balcan and Blum [2], but we use Rademacher complexity-based deviation bounds instead of VC-entropy. We defer the full proof to Appendix E. 

Our bound suggests that these constraints help with our learning by shrinking the hypothesis class \(\mathcal{H}\) to \(\mathcal{H}_{\phi,\tau+\varepsilon_{k}}\), reducing the required sample complexity. However, there is also a trade-off between reduction and accuracy. In our bound, we compare against the best classifier \(h^{*}_{\tau-\varepsilon_{k}}\in\mathcal{H}_{\phi,\tau-\varepsilon_{k}}\) instead of \(h^{*}\). Since we may have \(\phi(h^{*},\mathcal{D})>0\), if \(\tau\) is too small, we may reduce \(\mathcal{H}\) to a hypothesis class that does not contain any good classifiers. Recall that the generalization bound for standard supervised learning -- in the absence of explanation constraints -- is given by

\[\operatorname{err}_{D}(h)\leq\operatorname{err}_{D}(h^{*})+2R_{n}(\mathcal{H} )+2\sqrt{\frac{\ln(2/\delta)}{2n}}.\]

We can see the difference between this upper bound and the upper bound in Theorem 3.2 here as a possible notion of the goodness of an explanation constraint. We further discuss this in Appendix C.

Gradient Explanations for Particular Hypothesis Classes

In this section, we further quantify the usefulness of explanation constraints on different concrete examples and characterize the Rademacher complexity of the restricted hypothesis classes. In particular, we consider an explanation constraint of a constraint on the input gradient. For example, we may want our model's gradient to be close to that of some \(h^{\prime}\in\mathcal{H}\). This translates to \(g(h,x)=\nabla_{x}h(x)\) and \(C(x)=\{x\in\mathbb{R}^{d}\:|\:\|x-\nabla_{x}h^{\prime}(x)\|\leq\tau\}\) for some \(\tau>0\).

### Gradient Explanations for Linear Models

We now consider the case of a uniform distribution on a sphere, and we use the symmetry of this distribution to derive an upper bound on the Rademacher complexity (full proof to Appendix H).

**Theorem 4.1** (Rademacher complexity of linear models with a gradient constraint, uniform distribution on a sphere).: _Let \(\mathcal{D}_{\mathcal{X}}\) be a uniform distribution on a unit sphere in \(\mathbb{R}^{d}\), let \(\mathcal{H}=\{h:x\mapsto\langle w_{h},x\rangle\mid w_{h}\in\mathbb{R}^{d},||w_{ h}||_{2}\leq B\}\) be a class of linear models with weights bounded by a constant \(B\). Let \(\phi(h,x)=\theta(w_{h},w_{h^{\prime}})\) be a surrogate loss where \(\theta(u,v)\) is an angle between \(u,v\). We have_

\[R_{n}(\mathcal{H}_{\phi,\tau})\leq\frac{B}{\sqrt{n}}\left(\sin(\tau)\cdot p+ \frac{1-p}{2}\right),\]

_where \(p=\operatorname{erf}\left(\frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right)\) and \(\operatorname{erf}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^{2}}dt\) is the standard error function._

The standard upper bound on the Rademacher complexity of linear models is \(\frac{B}{\sqrt{n}}\). Our bound has a nice interpretation; we shrink our bound by a factor of \((\frac{1-p}{2}+\sin(\tau)p)\). We remark that \(d\) increases, we observe that \(p\to 1\), so the term \(\sin(\tau)p\) dominates this factor. As a consequence, we get that our bound is now scaled by \(\sin(\tau)\approx\tau\) and the the Rademacher complexity scales down by a factor of \(\tau\). This implies that given \(n\) labeled data, to achieve a fast rate \(\mathcal{O}(\frac{1}{n})\), we need \(\tau\) to be as good as \(O(\frac{1}{\sqrt{n}})\).

### Gradient Explanations for Two Layer Neural Networks

**Theorem 4.2** (Rademacher complexity of two layer neural networks (\(m\) hidden nodes) with a gradient constraint).: _Let \(\mathcal{X}\) be an instance space and \(\mathcal{D}_{\mathcal{X}}\) be a distribution over \(\mathcal{X}\) with a large enough support. Let \(\mathcal{H}=\{h:x\mapsto\sum_{j=1}^{m}w_{j}\sigma(u_{j}^{\top}x)|w_{j}\in \mathbb{R},u_{j}\in\mathbb{R}^{d},\sum_{j=1}^{m}|w_{j}|\leq B,\|u_{j}\|_{2}=1\}\) be a class of two layer neural networks with a ReLU activation function and bounded weight. Assume that there exists some constant \(C>0\) such that \(\mathbb{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[\|x\|_{2}^{2}]\leq C^{2}\). Consider an explanation loss given by_

\[\phi(h,x)= \|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|_{2}+\infty\cdot 1\{\| \nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|>\tau\}\]

_for some \(\tau>0\). Then, we have that \(R_{n}(\mathcal{H}_{\phi,\tau})\leq\frac{3\pi mC}{\sqrt{n}}\)._

Proof.: (Sketch) The key ingredient is to identify the impact of the gradient constraint and the form of class \(\mathcal{H}_{\phi,\tau}\). We provide an idea when we have \(m=1\) node. We write \(h(x)=w\sigma(u^{\top}x)\) and \(h^{\prime}(x)=w^{\prime}\sigma(u^{\top}x)\). Note that \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)=wu1\{u^{\top}x>0\}-w^{\prime}u^{\prime }1\{(u^{\prime})^{\top}x>0\}\) is

Figure 2: Visualization of the piecewise constant function of \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) when \(h\) is a two layer NNs with 1 node. Background colors represent regions with non-zero value.

a piecewise constant function (Figure 2). Assume that the probability mass of each region is non-negative, our gradient constraint implies that the norm of each region cannot be larger than \(\tau\).

1. If \(u,u^{\prime}\) have different directions, we have 4 regions in \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) and can conclude that \(|w|<\tau,|w^{\prime}|<\tau\).
2. If \(u=u^{\prime}\) have the same direction, we only have 2 regions in \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) and can conclude that \(\|wu-w^{\prime}u^{\prime}\|=|w-w^{\prime}|<\tau\).

The gradient constraint enforces a model to have the same node boundary \((u=u^{\prime})\) with a small weight difference \(|w-w^{\prime}|<\tau\) or that node would have a small weight \(|w|<\tau\). This finding allows us to determine the restricted class \(\mathcal{H}_{\phi,\tau}\), and we can use this to bound the Rademacher complexity accordingly. For full details, see Appendix I. 

We compare this with the standard Rademacher complexity of a two layer neural network [24],

\[R_{n}(\mathcal{H})\leq\frac{2BC}{\sqrt{n}}.\]

We can do better than this standard bound if \(\tau<\frac{2B}{3m}\). One interpretation for this is that we have a budget at most \(\tau\) to change the weight of each node and for total \(m\) nodes, we can change the weight by at most \(\tau m\). We compare this to \(B\) which is an upper bound on the total weight \(\sum_{j=1}^{m}|w_{j}|\leq B\). Therefore, we can do better than a standard bound when we can change the weight by at most two thirds of the average weight \(\frac{2B}{3m}\) for each node. We would like to point out that our bound does not depend on the distribution \(\mathcal{D}\) because we choose a specific explanation loss that guarantees that the gradient constraint holds almost everywhere. Extending to a weaker loss such as \(\phi(h,x)=\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|\) is a future research direction. In contrast, our result for linear models uses a weaker explanation loss and depends on \(\mathcal{D}\) (Theorem H.1). We also assume that there exists \(x\) with a positive probability density at any partition created by \(\nabla_{x}h(x)\). This is not a strong assumption, and it holds for any distribution where the support is the \(\mathbb{R}^{d}\), e.g., Gaussian distributions.

## 5 Algorithms for Learning from Explanation Constraints

Although we have analyzed learning with explanation constraints, algorithms to solve this constrained optimization problem are non-trivial. In this setting, we assume that we have access to \(n\) labeled data \(\{(x_{i},y_{i})\}_{i=1}^{n}\), \(m\) unlabeled data \(\{x_{n+i}\}_{i=1}^{m}\), and \(k\) data with explanations \(\{(x_{n+m+i},\phi(\cdot,x_{n+m+i}))\}_{i=1}^{k}\). We argue that in many cases, \(n\) labeled data are the most expensive to annotate. The \(k\) data points with explanations also have non-trivial cost; they require an expert to provide the annotated explanation or provide a surrogate loss \(\phi\). If the surrogate loss is specified then we can evaluate it on any unlabeled data, otherwise these data points with explanations could be expensive. On the other hand, the \(m\) data points can cheaply be obtained as they are completely unlabeled. We now consider existing approaches to incorporate this explanation information.

**EPAC-ERM:** Recall our EPAC-ERM objective from (2):

\[\min_{h\in\mathcal{H}}\frac{1}{n}\sum_{i=1}^{n}1\{h(x_{i})\neq y_{i}\}\ \text{ s.t. }\ \frac{1}{k}\sum_{j=n+m+1}^{n+m+k}\phi(h,x_{j})\leq\tau\]

for some constant \(\tau\). This constraint in general requires more complex optimization techniques (e.g., running multiple iterations and comparing values of \(\tau\)) to solve algorithmically. We could also consider the case where \(\tau=0\), which would entail the hypotheses satisfy the explanation constraints exactly, which however is in general too strong a constraint with noisy explanations.

**Augmented Lagrangian objectives:**

\[\min_{h\in\mathcal{H}}\frac{1}{n}\sum_{i=1}^{n}1[h(x_{i})\neq y_{i}]+\frac{ \lambda}{k}\sum_{j=n+m+1}^{n+m+k}\phi(h,x_{j})\]

As is done in prior work [32], we can consider an augmented Lagrangian objective. A crucial caveat with this approach is that the explanation surrogate loss is in general a much more complicated functional of the hypothesis than the empirical risk. For instance, it might involve the gradient of the hypothesis when we use gradient-based explanations. Computing the gradients of such a surrogate loss can be more expensive compared to the gradients of the empirical risk. For instance, in our experiments, computing the gradients of the surrogate loss that involves input gradients is 2.5 times slower than that of the empirical risk. With the above objective, however, we need to compute the same number of gradients of both the explanation surrogate loss and the empirical risk. These computational difficulties have arguably made incorporating explanation constraints not as popular as they could be.

### Variational Method

To alleviate these aforementioned computational difficulties, we propose a _new_ variational objective

\[\min_{h\in\mathcal{H}}(1-\lambda)\operatorname*{\mathbb{E}}_{(x,y)\sim \mathcal{D}}\left[\ell(h(x),y)\right]+\lambda\inf_{f\in\mathcal{H}_{\phi,\tau} }\operatorname*{\mathbb{E}}_{x}\left[\ell(h(x),f(x))\right],\]

where \(\ell\) is some loss function and \(t\geq 0\) is some threshold. The first term is the standard expected risk of \(h\) while the second term can be viewed as a projection distance between \(h\) and \(\tau\)-CE models.

It can be seen that the optimal solution of **EPAC-ERM** would also be an optimal solution of our proposed variational objective. The advantage of this formulation however is that it decouples the standard expected risk component from the surrogate risk component. This allows us to solve this objective with the following iterative technique, drawing inspiration from prior work in posterior regularization [9; 14]. More specifically, let \(h_{t}\) be the learned model at time \(t\) and at each timestep \(t\),

1. We project \(h_{t}\) to the class of \(\tau\)-CE models. \[f_{t+1,\phi}= \operatorname*{argmin}_{h\in\mathcal{H}}\frac{1}{m}\sum_{i=n+1}^{n+ m}\ell(h(x_{i}),h_{t}(x_{i}))\quad+\lambda\max\left(0,\,\frac{1}{k}\sum_{i=n+m+1}^{ n+m+k}\phi(h,x_{i})-\tau\right).\] The first term is the difference between \(h_{t}\) and \(f\) on unlabeled data. The second term is the surrogate loss, which we want to be smaller than \(t\). \(\eta\) is a regularization hyperparameter.
2. We calculate \(h_{t+1}\) that minimizes the empirical risk of labeled data and matches pseudolabels from \(f_{t+1,\phi}\) \[h_{t+1,\phi}= \operatorname*{argmin}_{h\in\mathcal{H}}\frac{1}{n}\sum_{i=1}^{n}\ell(h(x _{i}),y_{i})+\frac{1}{m}\sum_{i=n+1}^{n+m}\ell(h(x_{i}),f_{t+1,\phi}(x_{i})).\] Here, the discrepancy between \(h\) and \(f_{t+1,\phi}\) is evaluated on the unlabeled data \(\{x_{j}\}_{j=n+1}^{n+m}\).

The advantage of this decoupling is that we could use a differing number of gradient steps and learning rates for the projection step that involves the complicated surrogate loss when compared to the empirical risk minimization step. Secondly, we can simplify the projection iterate computation by replacing \(\mathcal{H}_{\phi,\tau}\) with a simpler class of teacher models \(\mathcal{F}_{\phi,\tau}\) for greater efficiency. Thus, the decoupled approach to solving the EPAC-ERM objective is in general more computationally convenient.

We initialize this procedure with some model \(h_{0}\). We remark that could see this as a constraint regularized self-training where \(h_{t}\) is a student model and \(f_{t}\) is a teacher model. At each timestep, we project a student model to the closest teacher model that satisfies the constraint. The next student model then learns from both labeled data and pseudo labels from the teacher model. In the standard self-training, we do not have any constraint and we have \(f_{t}=h_{t}\).

## 6 Experiments

We provide both synthetic and real-world experiments to support our theoretical results and clearly illustrate interesting tradeoffs of incorporating explanations. In our experiments, we compare our method against 3 baselines: (1) a standard supervised learning approach, (2) a simple Lagrangian-regularized method (that directly penalizes the surrogate loss \(\phi\)), and (3) self-training, which propagates the predictions of (1) and matches them on unlabeled data. We remark that (2) captures the essence of the method in Ross et al. [33], except there is no \(\ell_{2}\) regularization term.

Our experiments demonstrate that the proposed variational approach is preferable to simple Lagrangian methods and other supervised methods in many cases. In particular, the variational approach leads to a higher accuracy under limited labeled data settings. In addition, our method leadsto models that satisfy the explanation constraints much more frequently than other baselines. We also compare to a Lagrangian-regularized + self-training baseline (first, we use the model (2) to generate pseudolabels for unlabeled data and then train a new model on both labeled and unlabeled data) in Appendix L. We remark that this baseline isn't a standard method in practice and does not fit nicely into a theoretical framework, although it seems to be the most natural approach to using unlabeled data in this procedure. More extensive ablations are deferred to Appendix N, and code to replicate our experiments will be released with the full paper.

### Regression Task with Exact Gradient Information

In our synthetic experiments, we focus on a regression task where we try to learn some model contained in our hypothesis class. Our data is given by \(\mathcal{X}=\mathbb{R}^{d}\), and we try to learn a target function \(h^{*}:\mathcal{X}\rightarrow\mathbb{R}\). Our data distribution is given by \(X\sim\mathcal{N}(0,\sigma^{2}I)\), where \(I\) is a \(d\times d\) identity matrix. We generate \(h^{*}\) by randomly initializing a model in the specific hypothesis class \(\mathcal{H}\). We assume that we have \(n\) labeled data, \(m\) unlabeled data, and \(k\) data with explanations.

We first present a synthetic experiment for learning with a perfect explanation, meaning that \(\phi(h^{*},S)=0\). We consider the case where we have the _exact_ gradient of \(h^{*}\). Here, let \(\mathcal{H}\) be a linear classifier and note that the exact gradient gives us the slope of the linear model, and we only need to learn the bias term. Incorporating these explanation indeed helps as both methods that include explanation constraints (Lagrangian and ours) perform much better (Figure 3).

We also demonstrate incorporating this information for two layer neural networks. We observe a clear difference between the simpler Lagrangian approach and our variational objective (Figure 4 - left). Our method is clearly the best in the setting with limited labeled data and matches the performance of the strong self-training baseline with sufficient labeled data. We note that this is somewhat expected, as these constraints primarily help in the setting with limited labeled data; with enough labeled data, standard PAC bounds suffice for strong performance.

We also analyze how strongly the approaches enforce these explanation constraints on new data points that are seen at test time (Figure 4 - right) for two layer NNs. We observe that our variational objective approaches have input gradients that more closely match the ground-truth target network's

Figure 4: Comparison of MSE on regressing a two layer neural network (left) and \(\ell_{2}\) distance over input gradients as we vary the amount of labeled data \(n\) (right). Left is task performance and right is explanation constraint satisfaction. Results are averaged over 5 seeds. \(m=1000,k=20\).

Figure 3: Comparison of MSE on regressing a linear model. Results are averaged over 5 seeds. \(m=1000.\,k=20\).

input gradients. This demonstrates that, in the case of two layer NNs with gradient explanations, our approach best achieves both good performance and satisfying the constraints. Standard self-training achieves similar performance in terms of MSE but has no notion of satisfying the explanation constraints. The Lagrangian method does not achieve the same level of satisfying these explanations as it is unable to generalize and satisfy these constraints on new data.

### Tasks with Imperfect Explanations

Assuming access to perfect explanations may be unrealistic in practice, so we present experiments when our explanations are imperfect. We present classification tasks (Figure 5) from a weak supervision benchmark [46]. In this setting, we obtain explanations through the approximate gradients of a single weak labeler, as is done in [2]. More explicitly, weak labelers are heuristics designed by domain experts; one example is functions that check for the presence of particular words in a sentence (e.g., checking for the word "delicious" in a Yelp comment, which would indicate positive sentiment). We can then access gradient information from such weak labelers, which gives us a notion of feature importance about particular features in our data. We note that these examples of gradient information are rather _easy_ to obtain, as we only need domain experts to specify simple heuristic functions for a particular task. Once given these functions, we can apply them easily over unlabeled data without requiring any example-level annotations.

We observe that our variational objective achieves better performance than all other baseline approaches on the majority of settings defined by the number of labeled data. We remark that the explanation in this dataset is a noisy gradient explanation along two feature dimensions, yet this still improves upon methods that do not incorporate this explanation constraint. Indeed, our method outperforms the Lagrangian approach, showing the benefits of iterative rounds of self-training over the unlabeled data. In addition to our real-world experiments, we present synthetic experiments with noisy gradients in Appendix K.1.

## 7 Discussion

Our work proposes a new learning theoretic framework that provides insight into how apriori explanations of desired model behavior can benefit the standard machine learning pipeline. The statistical benefits of explanations arise from constraining the hypothesis class: explanation samples serve to better estimate the population explanation constraint, which constrains the hypothesis class. This is to be contrasted with the statistical benefit of labeled samples, which serve to get a better estimate of the population risk. We provide instantiations of our analysis for the canonical class of gradient explanations, which captures many explanations in terms of feature importance. It would be of interest to provide corollaries for other types of explanations in future work. As mentioned before, the generality of our framework has larger implications towards incorporating constraints that are not considered as "standard" explanations. For example, this work can be leveraged to incorporate more general notions of side information and inductive biases. We also discuss the societal impacts of our approach in Appendix O. As a whole, our paper supports using further information (e.g., explanation constraints) in the standard learning setting.

Figure 5: Comparison of accuracy on the YouTube (left) and the Yelp (right) datasets. Here, we let \(m=500,k=150,T=2,\tau=0.0\). Results are averaged over 40 seeds.

Acknowledgements

This work was supported in part by DARPA under cooperative agreement HR00112020003, FA8750-23-2-1015, ONR grant N00014-23-1-2368, NSF grant IIS-1909816, a Bloomberg Data Science PhD fellowship and funding from Bosch Center for Artificial Intelligence and the ARCS Foundation.

## References

* [1] M.-F. Balcan and A. Blum. A pac-style model for learning from labeled and unlabeled data. In _International Conference on Computational Learning Theory_, pages 111-126. Springer, 2005.
* [2] M.-F. Balcan and A. Blum. A discriminative model for semi-supervised learning. _Journal of the ACM (JACM)_, 57(3):1-46, 2010.
* [3] B. Bilodeau, N. Jaques, P. W. Koh, and B. Kim. Impossibility theorems for feature attribution. _arXiv preprint arXiv:2212.11870_, 2022.
* [4] J. R. Birge and F. Louveaux. _Introduction to stochastic programming_. Springer Science & Business Media, 2011.
* [5] O. Chapelle, B. Scholkopf, and A. Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. _IEEE Transactions on Neural Networks_, 20(3):542-542, 2009.
* [6] W. M. Czarnecki, S. Osindero, M. Jaderberg, G. Swirszcz, and R. Pascanu. Sobolev training for neural networks. _Advances in neural information processing systems_, 30, 2017.
* [7] F. Fioretto, P. Van Hentenryck, T. W. Mak, C. Tran, F. Baldo, and M. Lombardi. Lagrangian duality for constrained deep learning. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 118-135. Springer, 2021.
* [8] S. Frei, D. Zou, Z. Chen, and Q. Gu. Self-training converts weak learners to strong learners in mixture models. In _International Conference on Artificial Intelligence and Statistics_, pages 8003-8021. PMLR, 2022.
* [9] K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured latent variable models. _The Journal of Machine Learning Research_, 11:2001-2049, 2010.
* [10] S. Garg and Y. Liang. Functional regularization for representation learning: A unified theoretical perspective. _Advances in Neural Information Processing Systems_, 33:17187-17199, 2020.
* [11] I. Goodfellow, Y. Bengio, and A. Courville. _Deep learning_. MIT press, 2016.
* [12] Y. Goyal, Z. Wu, J. Ernst, D. Batra, D. Parikh, and S. Lee. Counterfactual visual explanations. In _International Conference on Machine Learning_, pages 2376-2384. PMLR, 2019.
* [13] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [14] Z. Hu, X. Ma, Z. Liu, E. Hovy, and E. Xing. Harnessing deep neural networks with logic rules. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2410-2420, 2016.
* [15] A. A. Ismail, H. Corrada Bravo, and S. Feizi. Improving deep learning interpretability by saliency guided training. _Advances in Neural Information Processing Systems_, 34:26726-26739, 2021.
* [16] P. Kall, S. W. Wallace, and P. Kall. _Stochastic programming_, volume 6. Springer, 1994.
* [17] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In _International conference on machine learning_, pages 2668-2677. PMLR, 2018.
* [18] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In _International conference on machine learning_, pages 1885-1894. PMLR, 2017.

* Koh et al. [2020] P. W. Koh, T. Nguyen, Y. S. Tang, S. Mussmann, E. Pierson, B. Kim, and P. Liang. Concept bottleneck models. In _International Conference on Machine Learning_, pages 5338-5348. PMLR, 2020.
* LeCun et al. [2015] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. _nature_, 521(7553):436-444, 2015.
* Ledoux and Talagrand [1991] M. Ledoux and M. Talagrand. _Probability in Banach Spaces: isoperimetry and processes_, volume 23. Springer Science & Business Media, 1991.
* Li et al. [2020] J. Li, V. Nagarajan, G. Plumb, and A. Talwalkar. A learning theoretic perspective on local explainability. In _International Conference on Learning Representations_, 2020.
* Lundberg and Lee [2017] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. _Advances in neural information processing systems_, 30, 2017.
* Ma [2022] T. Ma. Lecture notes from machine learning theory, 2022. URL http://web.stanford.edu/class/stats214/.
* Mothilal et al. [2020] R. K. Mothilal, A. Sharma, and C. Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In _Proceedings of the 2020 conference on fairness, accountability, and transparency_, pages 607-617, 2020.
* Murdoch et al. [2018] W. J. Murdoch, P. J. Liu, and B. Yu. Beyond word importance: Contextual decomposition to extract interactions from lstms. In _International Conference on Learning Representations_, 2018.
* Natarajan et al. [2013] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari. Learning with noisy labels. _Advances in neural information processing systems_, 26, 2013.
* Pukdee et al. [2022] R. Pukdee, D. Sam, P. K. Ravikumar, and N. Balcan. Label propagation with weak supervision. In _The Eleventh International Conference on Learning Representations_, 2022.
* Ratner et al. [2017] A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. Re. Snorkel: Rapid training data creation with weak supervision. In _Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases_, volume 11, page 269. NIH Public Access, 2017.
* Ratner et al. [2016] A. J. Ratner, C. M. De Sa, S. Wu, D. Selsam, and C. Re. Data programming: Creating large training sets, quickly. _Advances in neural information processing systems_, 29, 2016.
* Ribeiro et al. [2016] M. T. Ribeiro, S. Singh, and C. Guestrin. " why should i trust you?" explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144, 2016.
* Rieger et al. [2020] L. Rieger, C. Singh, W. Murdoch, and B. Yu. Interpretations are useful: penalizing explanations to align neural networks with prior knowledge. In _International conference on machine learning_, pages 8116-8126. PMLR, 2020.
* Ross et al. [2017] A. S. Ross, M. C. Hughes, and F. Doshi-Velez. Right for the right reasons: training differentiable models by constraining their explanations. In _Proceedings of the 26th International Joint Conference on Artificial Intelligence_, pages 2662-2670, 2017.
* Shao et al. [2022] H. Shao, O. Montasser, and A. Blum. A theory of PAC learnability under transformation invariances. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=1lWlfNaRkKw.
* Smilkov et al. [2017] D. Smilkov, N. Thorat, B. Kim, F. Viegas, and M. Wattenberg. Smoothgrad: removing noise by adding noise. _ICML Workshop on Visualization for Deep Learning, 2017_, 2017.
* Stacey et al. [2022] J. Stacey, Y. Belinkov, and M. Rei. Supervising model attention with human explanations for robust natural language inference. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 11349-11357, 2022.
* Sundararajan et al. [2017] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In _International conference on machine learning_, pages 3319-3328. PMLR, 2017.

* [38] L. G. Valiant. A theory of the learnable. _Communications of the ACM_, 27(11):1134-1142, 1984.
* [39] S. Wachter, B. Mittelstadt, and C. Russell. Counterfactual explanations without opening the black box: Automated decisions and the gdpr. _Harv. JL & Tech._, 31:841, 2017.
* [40] C. Wei, K. Shen, Y. Chen, and T. Ma. Theoretical analysis of self-training with deep networks on unlabeled data. In _International Conference on Learning Representations_, 2020.
* [41] M. R. Wicker, J. Heo, L. Costabello, and A. Weller. Robust explanation constraints for neural networks. In _The Eleventh International Conference on Learning Representations_, 2022.
* [42] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet classification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10687-10698, 2020.
* [43] C.-K. Yeh, J. Kim, I. E.-H. Yen, and P. K. Ravikumar. Representer point selection for explaining deep neural networks. _Advances in neural information processing systems_, 31, 2018.
* [44] C.-K. Yeh, B. Kim, S. Arik, C.-L. Li, T. Pfister, and P. Ravikumar. On completeness-aware concept-based explanations in deep neural networks. _Advances in Neural Information Processing Systems_, 33:20554-20565, 2020.
* [45] M. E. Zarlenga, P. Barbiero, G. Ciravegna, G. Marra, F. Giannini, M. Diligenti, F. Precioso, S. Melacci, A. Weller, P. Lio, et al. Concept embedding models. In _NeurIPS 2022-36th Conference on Neural Information Processing Systems_, 2022.
* [46] J. Zhang, Y. Yu, Y. Li, Y. Wang, Y. Yang, M. Yang, and A. Ratner. Wrench: A comprehensive benchmark for weak supervision. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.

Uniform Convergence via Rademacher Complexity

A standard tool for providing performance guarantees of supervised learning problems is a generalization bound via uniform convergence. We will first define the Rademacher complexity and its corresponding generalization bound.

**Definition 5**.: _Let \(\mathcal{F}\) be a family of functions mapping \(\mathcal{X}\to\mathbb{R}\). Let \(S=\{x_{1},\ldots,x_{m}\}\) be a set of examples drawn i.i.d. from a distribution \(D_{\mathcal{X}}\). Then, the empirical Rademacher complexity of \(\mathcal{F}\) is defined as_

\[R_{S}(\mathcal{F})=\operatorname*{\mathbb{E}}_{\sigma}\left[\sup_{f\in \mathcal{F}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}f(x_{i})\right)\right]\]

_where \(\sigma_{1},\ldots,\sigma_{m}\) are independent random variables uniformly chosen from \(\{-1,1\}\)._

**Definition 6**.: _Let \(\mathcal{F}\) be a family of functions mapping \(\mathcal{X}\to\mathbb{R}\). Then, the Rademacher complexity of \(\mathcal{F}\) is defined as_

\[R_{n}(\mathcal{F})=\operatorname*{\mathbb{E}}_{S\sim\mathcal{D}_{\mathcal{X}}^ {n}}\left[R_{S}(\mathcal{F})\right].\]

_The Rademacher complexity is the expectation of the empirical Rademacher complexity, over \(n\) samples drawn i.i.d. from the distribution \(\mathcal{D}_{\mathcal{X}}\)._

**Theorem A.1** (Rademacher-based uniform convergence).: _Let \(D_{\mathcal{X}}\) be a distribution over \(\mathcal{X}\), and \(\mathcal{F}\) a family of functions mapping \(\mathcal{X}\to[0,1]\). Let \(S=\{x_{1},\ldots,x_{n}\}\) be a set of samples drawn i.i.d. from \(D_{\mathcal{X}}\), then with probability at least \(1-\delta\) over our draw \(S\),_

\[|\operatorname*{\mathbb{E}}_{\mathcal{D}}[f(x)]-\operatorname*{\mathbb{E}}_{S }[f(x)]|\leq 2R_{n}(\mathcal{F})+\sqrt{\frac{\ln(2/\delta)}{2n}}.\]

_This holds for every function \(f\in\mathcal{F}\), and \(\operatorname*{\mathbb{E}}_{S}[f(x)]\) is expectation over a uniform distribution over \(S\)._

This bound on the empirical Rademacher complexity leads to the standard generalization bound for supervised learning.

**Theorem A.2**.: _For a binary classification setting when \(y\in\{\pm 1\}\) with a zero-one loss, for \(\mathcal{H}\subset\{h:\mathcal{X}\to\{-1,1\}\}\) be a family of binary classifiers, let \(S=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\) is drawn i.i.d. from \(D\) then with probability at least \(1-\delta\), we have_

\[|\mathrm{err}_{\mathcal{D}}(h)-\widehat{\mathrm{err}_{S}}(h)|\leq R_{n}( \mathcal{H})+\sqrt{\frac{\ln(2/\delta)}{2n}},\]

_for every \(h\in\mathcal{H}\) when_

\[\mathrm{err}_{\mathcal{D}}(h)=\Pr_{(x,y)\sim\mathcal{D}}(h(x)\neq y)\]

_and_

\[\widehat{\mathrm{err}_{S}}(h)=\frac{1}{n}\sum_{i=1}^{n}1[h(x_{i})\neq y_{i}]\]

_is the empirical error on \(S\)._

For a linear model with a bounded weights in \(\ell_{2}\) norm, the Rademacher complexity is \(\mathcal{O}(\frac{1}{\sqrt{n}})\). We refer to the proof from Ma [24] for this result.

**Theorem A.3** (Rademacher complexity of a linear model ([24])).: _Let \(\mathcal{X}\) be an instance space in \(\mathbb{R}^{d}\), let \(\mathcal{D}_{\mathcal{X}}\) be a distribution on \(\mathcal{X}\), let \(\mathcal{H}=\{h:x\to\langle w_{h},x\rangle\mid w_{h}\in\mathbb{R}^{d},||w_{h}||_ {2}\leq B\}\) be a class of linear model with weights bounded by some constant \(B>0\) in \(\ell_{2}\) norm. Assume that there exists a constant \(C>0\) such that \(\operatorname*{\mathbb{E}}_{x\sim\mathcal{D}_{\mathcal{X}}}[||x||_{2}^{2}]\leq C ^{2}\). For any \(S=\{x_{1},\ldots,x_{n}\}\) is drawn i.i.d. from \(\mathcal{D}_{\mathcal{X}}\), we have_

\[R_{S}(\mathcal{H})\leq\frac{B}{n}\sqrt{\sum_{i=1}^{n}||x_{i}||_{2}^{2}}\]

_and_

\[R_{n}(\mathcal{H})\leq\frac{BC}{\sqrt{n}}.\]Many of our proofs require the usage of Talgrand's lemma, which we now present.

**Lemma A.4** (Talgrand's Lemma [21]).: _Let \(\phi:\mathbb{R}\rightarrow\mathbb{R}\) be a \(k\)-Lipschitz function. Then for a hypothesis class \(\mathcal{H}=\{h:\mathbb{R}^{d}\rightarrow\mathbb{R}\}\), we have that_

\[R_{S}(\phi\circ\mathcal{H})\leq kR_{s}(\mathcal{H})\]

_where \(\phi\circ\mathcal{H}=\{f:z\mapsto\phi(h(z))|h\in\mathcal{H}\}\)._

## Appendix B Generalizable Constraints

We know that constraints \(C(x)\) capture human knowledge about how explanations at a point \(x\) should behave. For any constraints \(C(x)\) that are known apriori for all \(x\in\mathcal{X}\), we can evaluate whether a model satisfies the constraints at a point \(x\in\mathcal{X}\). This motivates us to discuss the ability of models to generalize from any finite samples \(S_{E}\) to satisfy these constraints over \(\mathcal{X}\) with high probability. Having access to \(C(x)\) is equivalent to knowing how models should behave over _all_ possible data points in terms of explanations, which may be too strong of an assumption. Nevertheless, many forms of human knowledge can be represented by a closed-form function \(C(x)\). For example,

1. An explanation has to take value in a fixed range can be represented by \(C(x)=\Pi_{i=1}^{r}[a_{i},b_{i}],\forall x\in\mathcal{X}\).
2. An explanation has to stay in a ball around \(x\) can be represented by \(C(x)=\{u\in\mathbb{R}^{d}\mid||u-x||_{2}\leq r\}\).
3. An explanation has to stay in a rectangle around \(\frac{x}{3}\) can be represented by \(C(x)=\{u\in\mathbb{R}^{d}\mid\frac{x_{i}}{3}-a_{i}\leq u_{i}\leq\frac{x_{i}}{3} +b_{i},i=1,\ldots,d\}\).

In this case, there always exists a surrogate loss that represents the explanation constraints \(C(x)\); for example, we can set \(\phi(h,x)=1\{g(h,x)\in C(x)\}\). On the other hand, directly specifying explanation constraints through a surrogate loss would also imply that \(C(x)\) is known apriori for all \(x\in\mathcal{X}\). The task of generalization to satisfy the constraint on unseen data is well-defined in this setting. Furthermore, if a surrogate loss \(\phi\) is specified, then we can evaluate \(\phi(h,x)\) on any unlabeled data point without the need for human annotators which is a desirable property.

On the other hand, we usually do not have knowledge over all data points \(x\in\mathcal{X}\); rather, we may only know these explanation constraints over a random sample of \(k\) data points \(S_{E}=\{x_{1}^{\prime},\ldots,x_{k}^{\prime}\}\). If we do not know the constraint set \(C(x)\), it is unclear what satisfying the constraint at an unseen data point \(x\) means. Indeed, without additional assumptions, it may not make sense to think about generalization. For example, if there is no relationship between \(C(x)\) for different values of \(x\), then it is not possible to infer about \(C(x)\) from \(C(x_{i}^{\prime})\) for \(i=1,\ldots,k\). In this case, we could define

\[\phi(h,x)=1\{g(h,x)\in C(x)\}1\{x\in S_{E}\},\]

Figure 6: Illustration of examples of explanation constraints, given from some learnable class \(C(x)\).

where we are only interested in satisfying these explanation constraints over the finite sample \(S_{E}\). For other data points, we have \(\phi(h,x)=0\). This guarantees that any model with low empirical explanation loss would also achieve loss expected explanation loss, although this does not have any particular implication on any notion of generalization to new constraints. Regardless, we note that our explanation constraints still reduce the size of the hypothesis class from \(\mathcal{H}\) to \(\mathcal{H}_{\phi,\tau}\), leading to an improvement in sample complexity.

The more interesting setting, however, is when we make an additional assumption that the true (unknown) surrogate loss \(\phi\) exists and, during training, we only have access to instances of this surrogate loss evaluated on the sample \(\phi(\cdot,x_{i}^{\prime})\). We can apply a uniform convergence argument to achieve

\[\phi(h,\mathcal{D}_{\mathcal{X}})\leq\phi(h,S_{E})+2R_{k}(\mathcal{G})+\sqrt{ \frac{\ln(4/\delta)}{2k}}\]

with probability at least \(1-\delta\) over \(S_{E}\), drawn i.i.d. from \(\mathcal{D}_{\mathcal{X}}\) and \(\mathcal{G}=\{\phi(h,\cdot)|h\in\mathcal{H}\}\), \(k=|S_{E}|\). Although the complexity term \(R_{k}(\mathcal{G})\) is unknown (since \(\phi\) is unknown), we can upper bound this by the complexity of a class of functions \(\Phi\) (e.g., neural networks) that is large enough to well-approximate any \(\phi(h,\cdot)\in\mathcal{G}\), meaning that \(R_{k}(\mathcal{G})\leq R_{k}(\Phi)\). Comparing to the former case when \(C(x)\) is known for all \(x\in\mathcal{X}\) apriori, the generalization bound has a term that increases from \(R_{k}(\mathcal{G})\) to \(R_{k}(\Phi)\), which may require more explanation-annotated data to guarantee generalization to new data points. We note that the simpler constraints lead to a simpler surrogate loss, which in turn implies a less complex upper bound \(\Phi\). This means that simpler constraints are easier to learn.

Nonetheless, this is a more realistic setting when explanation constraints are hard to acquire and we do not have the constraints for all data points in \(\mathcal{X}\). For example, Ross et al. [33] considers an image classification task on MNIST, and imposes an explanation constraint in terms of penalizing the input gradient of the background of images. In essence, the idea is that the background should be less important than the foreground for the classification task. In general, this constraint does not have a closed-form expression, and we do not even have access to the constraint for unseen data points. However, if we assume that a surrogate loss \(\phi(h,\cdot)\) can be well-approximated by two layer neural networks, then our generalization bound allows us to reason about the ability of model to generalize and ignore background features on new data.

## Appendix C Goodness of an explanation constraint

**Definition 7** (Goodness of an explanation constraint).: _For a hypothesis class \(\mathcal{H}\), a distribution \(\mathcal{D}\) and an explanation loss \(\phi\), the goodness of \(\phi\) with respect to a threshold \(\tau\) and \(n\) labeled examples is:_

\[G_{n,\tau}(\phi,\mathcal{H})=(R_{n}(\mathcal{H})-R_{n}(\mathcal{H}_{\phi,\tau} ))+(\mathrm{err}_{\mathcal{D}}(h^{*})-\mathrm{err}_{\mathcal{D}}(h^{*}_{t}))\]

\[h^{*}=\arg\min_{h\in\mathcal{H}}\mathrm{err}_{\mathcal{D}}(h),\quad h^{*}_{\tau }=\arg\min_{h\in\mathcal{H}_{\phi,\tau}}\mathrm{err}_{\mathcal{D}}(h).\]

Here, we assume access to infinite explanation data so that \(\varepsilon_{k}\to 0\). The goodness depends on the number of labeled examples \(n\) and a threshold \(t\). In our definition, a good explanation constraint leads to a reduction in the complexity of \(\mathcal{H}\) while still containing a classifier with low error. This suggests that the benefits from explanation constraints exhibit diminishing returns as \(n\) becomes large. In fact, as \(n\to\infty\), we have \(R_{n}(\mathcal{H})\to 0,R_{n}(\mathcal{H}_{\phi,\tau})\to 0\) which implies \(G_{n}(\phi,\mathcal{H})\to\mathrm{err}_{\mathcal{D}}(h^{*})-\mathrm{err}_{ \mathcal{D}}(h^{*}_{\tau})\leq 0\). On the other hand, explanation constraints help when \(n\) is small. For \(t\) large enough, we expect \(\mathrm{err}_{\mathcal{D}}(h^{*})-\mathrm{err}_{\mathcal{D}}(h^{*}_{\tau})\) to be small, so that our notion of goodness is dominated by the first term: \(R_{n}(\mathcal{H})-R_{n}(\mathcal{H}_{\phi,\tau})\), which has the simple interpretation of reduction in model complexity.

## Appendix D Examples for Generalizable constraints

In this section, we look at the Rademacher complexity of \(\mathcal{G}\) for different explanation constraints to characterize how many samples with explanation constraints are required in order to generalize to satisfying the explanation constraints on unseen data. We remark that this is a different notion of sample complexity; these unlabeled data require annotations of explanation constraints, not standard labels. In practice, this can be easier and less expertise might be necessary if define the surrogate loss \(\phi\) directly. First, we analyze the case where our explanation is given by the gradient of a linear model.

**Proposition D.1** (Learning a gradient constraint for linear models).: _Let \(\mathcal{D}\) be a distribution over \(\mathbb{R}^{d}\). Let \(\mathcal{H}=\{h:x\mapsto\langle w_{h},x\rangle\mid w_{h}\in\mathbb{R}^{d},\|w_{h }\|_{2}\leq B\}\) be a class of linear models that pass through the origin. Let \(\phi(h,x)=\theta(w_{h},w_{h^{\prime}})\) be a surrogate explanation loss. Let \(\mathcal{G}=\{\phi(h,\cdot)\mid h\in\mathcal{H}\}\), then we have_

\[R_{n}(\mathcal{G})\leq\frac{\pi}{2\sqrt{m}}.\]

Proof.: For a linear separator, \(\phi(h,\cdot)\) is a constant function over \(\mathcal{X}\). The Rademacher complexity is given by

\[R_{n}(\mathcal{G}) =\underset{x\sim D}{\mathbb{E}}\left[\underset{\sigma}{\sup} \left[\underset{\phi(h,\cdot)\in\mathcal{G}}{\sup}\left(\frac{1}{m}\sum_{i=1} ^{m}\sigma_{i}\phi(h,x_{i})\right)\right]\right]\] \[=\underset{x\sim D}{\mathbb{E}}\left[\underset{\sigma}{\sup} \left[\underset{h\in\mathcal{H}}{\sup}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_ {i}\right)\theta(w_{h},w_{h^{\prime}})\right]\right]\] \[=\underset{x\sim D}{\mathbb{E}}\left[\underset{\sigma}{\sup} \left[\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}\right)\sup_{h\in\mathcal{H}} \theta(w_{h},w_{h^{\prime}})\right]\right]\] \[=\frac{\pi}{2}\underset{\sigma}{\mathbb{E}}\left[\left|\frac{1}{ m}\sum_{i=1}^{m}\sigma_{i}\right|\right]\] \[\leq\frac{\pi}{2\sqrt{m}}.\]

We compare this with the Rademacher complexity of linear models which is given by \(R_{m}(\mathcal{H})\leq\frac{B}{\sqrt{m}}\). The upper bound does not depend on the upper bound on the weight \(B\). In practice, we know that the gradient of a linear model is constant for any data point. This implies that knowing a gradient of a single point is enough to identify the gradient of the linear model.

We consider another type of explanation constraint that is given by a noisy model. Here, we could observe either a noisy classifier and noisy regressor, and the constraint could be given by having similar outputs to this noisy model. This is reminiscent of learning with noisy labels [27] or weak supervision [30, 29, 28]. In this case, our explanation \(g\) is simply the hypothesis element \(h\) itself, and our constraint is on the values that \(h(x)\) can take. We first analyze this in the classification setting.

**Proposition D.2** (Learning a constraint given by a noisy classifier).: _Let \(\mathcal{D}\) be a distribution over \(\mathbb{R}^{d}\). Consider a binary classification task with \(\mathcal{Y}=\{-1,1\}\). Let \(\mathcal{H}\) be a hypothesis class. Let \(\phi(h,x)=1[h(x)\neq h^{\prime}(x)]\) be a surrogate explanation loss. Let \(\mathcal{G}=\{\phi(h,\cdot)\mid h\in\mathcal{H}\}\), then we have_

\[R_{n}(\mathcal{G})=\frac{1}{2}R_{n}(\mathcal{H}).\]

Proof.: \[R_{n}(\mathcal{G}) =\underset{x\sim D}{\mathbb{E}}\left[\underset{\sigma}{\sup} \left[\underset{\phi(h,\cdot)\in\mathcal{G}}{\sup}\left(\frac{1}{m}\sum_{i=1} ^{m}\sigma_{i}\phi(h,x_{i})\right)\right]\right]\] \[=\underset{x\sim D}{\mathbb{E}}\left[\underset{\sigma}{\sup} \left[\underset{h\in\mathcal{H}}{\sup}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i} (\frac{1-h(x)h^{\prime}(x)}{2})\right)\right]\right]\] \[=\underset{x\sim D}{\mathbb{E}}\left[\underset{\sigma}{\sup} \left[\underset{h\in\mathcal{H}}{\sup}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i} (\frac{h(x)h^{\prime}(x)}{2})\right)\right]\right]\] \[=\underset{x\sim D}{\mathbb{E}}\left[\underset{\sigma}{\sup} \left[\underset{h\in\mathcal{H}}{\sup}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i} (\frac{h(x)}{2})\right)\right]\right]\] \[=\frac{1}{2}R_{n}(\mathcal{H}).\]Here, to learn the restriction of \(\mathcal{G}\) is on the same order of \(R_{n}(\mathcal{H})\). For a given noisy regressor, we observe slightly different upper bound.

**Proposition D.3** (Learning a constraint given by a noisy regressor).: _Let \(\mathcal{D}\) be a distribution over \(\mathbb{R}^{d}\). Consider a regression task with \(\mathcal{Y}=\mathbb{R}\). Let \(\mathcal{H}\) be a hypothesis class that \(\forall h\in\mathcal{H},-h\in\mathcal{H}\). Let \(\phi(h,x)=|h(x)-h^{\prime}(x)|\) be a surrogate explanation loss. Let \(\mathcal{G}=\{\phi(h,\cdot)\mid h\in\mathcal{H}\}\), then we have_

\[R_{n}(\mathcal{G})\leq 2R_{n}(\mathcal{H}).\]

Proof.: \[R_{n}(\mathcal{G}) =\mathop{\mathbb{E}}_{x\sim D}\left[\mathop{\mathbb{E}}_{ \sigma}\left[\sup_{\phi(h,\cdot)\in\mathcal{G}}\left(\frac{1}{m}\sum_{i=1}^{m} \sigma_{i}\phi(h,x_{i})\right)\right]\right]\] \[=\mathop{\mathbb{E}}_{x\sim D}\left[\mathop{\mathbb{E}}_{ \sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}|h( x_{i})-h^{\prime}(x_{i})|\right)\right]\right]\] \[=\mathop{\mathbb{E}}_{x\sim D}\left[\mathop{\mathbb{E}}_{ \sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i} \max(0,h(x_{i})-h^{\prime}(x_{i}))+\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}\max(0,h ^{\prime}(x_{i})-h(x_{i}))\right)\right]\right]\] \[\leq\mathop{\mathbb{E}}_{x\sim D}\left[\mathop{\mathbb{E}}_{ \sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i} \max(0,h(x_{i})-h^{\prime}(x_{i}))\right)\right]\right]+\] \[\leq\mathop{\mathbb{E}}_{x\sim D}\left[\mathop{\mathbb{E}}_{ \sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}(h( x_{i})-h^{\prime}(x_{i}))\right)\right]\right]+\mathop{\mathbb{E}}_{x\sim D}\left[ \mathop{\mathbb{E}}_{\sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1}{m}\sum _{i=1}^{m}\sigma_{i}(h^{\prime}(x_{i})-h(x_{i}))\right)\right]\right],\]

where in the last line, we apply Talgrand's lemma A.4 and note that the max function \(\max(0,h(x))\) is 1-Lipschitz; in the third line, we note that we break up the supremum as both terms by definition of the \(\max\) function are non-negative. Then, noting that we do not optimize over \(h^{\prime}(x)\), we further simplify this as

\[R_{n}(\mathcal{G}) \leq\mathop{\mathbb{E}}_{x\sim D}\left[\mathop{\mathbb{E}}_{ \sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}h( x_{i})\right)\right]\right]+\mathop{\mathbb{E}}_{x\sim D}\left[\mathop{\mathbb{E}}_{ \sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}(-h( x_{i}))\right)\right]\right]\] \[\leq 2R_{n}(\mathcal{H}).\]

As mentioned before, knowing apriori surrogate loss \(\phi\) might be too strong. In practice, we may only have access to the instances \(\phi(\cdot,x_{i})\) on a set of samples \(S=\{x_{1},\ldots,x_{k}\}\). We also consider the case when \(\phi(h,x)=|h(x)-h^{\prime}(x)|\) when \(h^{\prime}\) is unknown and \(h^{\prime}\) belongs to a learnable class \(\mathcal{C}\).

**Proposition D.4** (Learning a constraint given by a noisy regressor from some learnable class \(\mathcal{C}\)).: _Assume \(\mathcal{D}\) is a distribution over \(\mathbb{R}^{d}\). Let \(\mathcal{H}\) and \(\mathcal{D}\) be hypothesis classes. Let \(\phi_{h^{\prime}}(h,x)=|h(x)-h^{\prime}(x)|\) be a surrogate explanation loss of a constraint corresponding to \(h^{\prime}\). Let \(\mathcal{G}_{\mathcal{C}}=\{\phi_{h^{\prime}}(h,\cdot)|h\in\mathcal{H},h^{ \prime}\in\mathcal{C}\}\), then we have_

\[R_{n}(\mathcal{G}_{\mathcal{C}})\leq 2R_{n}(\mathcal{H})+2R_{n}(\mathcal{C}).\]Proof.: \[R_{n}(\mathcal{G}_{\mathcal{C}}) =\operatorname*{\mathbb{E}}_{x\sim D}\left[\operatorname*{\mathbb{E} }_{\sigma}\left[\sup_{\phi(h_{i},\cdot)\in\mathcal{G}_{\mathcal{C}}}\left( \frac{1}{m}\sum_{i=1}^{m}\sigma_{i}\phi(h,x_{i})\right)\right]\right]\] \[=\operatorname*{\mathbb{E}}_{x\sim D}\left[\operatorname*{ \mathbb{E}}_{\sigma}\left[\sup_{\begin{subarray}{c}h\in\mathcal{H}_{i}\\ h^{\prime}\in\mathcal{C}\end{subarray}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma _{i}|h(x_{i})-h^{\prime}(x_{i})|\right)\right]\right]\] \[\leq\operatorname*{\mathbb{E}}_{x\sim D}\left[\operatorname*{ \mathbb{E}}_{\sigma}\left[\sup_{\begin{subarray}{c}h\in\mathcal{H}_{i}\\ h^{\prime}\in\mathcal{C}\end{subarray}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma _{i}\max(0,h(x_{i})-h^{\prime}(x_{i}))\right)\right]\right]\right.\] \[\left.\operatorname*{\mathbb{E}}_{x\sim D}\left[\operatorname*{ \mathbb{E}}_{\sigma}\left[\sup_{\begin{subarray}{c}h\in\mathcal{H}_{i}\\ h^{\prime}\in\mathcal{C}\end{subarray}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma _{i}\max(0,h^{\prime}(x_{i})-h(x_{i}))\right)\right]\right]\right.\] \[\leq\operatorname*{\mathbb{E}}_{x\sim D}\left[\operatorname*{ \mathbb{E}}_{\sigma}\left[\sup_{\begin{subarray}{c}h\in\mathcal{H}_{i}\\ h^{\prime}\in\mathcal{C}\end{subarray}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma _{i}(h(x_{i})-h^{\prime}(x_{i}))\right)\right]\right]\right.\] \[\left.\operatorname*{\mathbb{E}}_{x\sim D}\left[\operatorname*{ \mathbb{E}}_{\sigma}\left[\sup_{\begin{subarray}{c}h\in\mathcal{H}_{i}\\ h^{\prime}\in\mathcal{C}\end{subarray}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma _{i}(h^{\prime}(x_{i})-h(x_{i}))\right)\right]\right]\right.\]

where the lasts line again holds by an application of Talgrand's lemma. In this case, we indeed are optimizing over \(h^{\prime}\), so we get that

\[R_{n}(\mathcal{G}_{\mathcal{C}}) \leq 2\cdot\operatorname*{\mathbb{E}}_{x\sim D}\left[ \operatorname*{\mathbb{E}}_{\sigma}\left[\sup_{h\in\mathcal{H}}\left(\frac{1} {m}\sum_{i=1}^{m}\sigma_{i}(h(x_{i}))\right)\right]\right]+2\cdot\operatorname* {\mathbb{E}}_{x\sim D}\left[\operatorname*{\mathbb{E}}_{\sigma}\left[\sup_{h ^{\prime}\in\mathcal{C}}\left(\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}(h^{\prime}(x _{i}))\right)\right]\right]\] \[=2R_{n}(\mathcal{H})+2R_{n}(\mathcal{C}).\]

We remark that while this value is much larger than that of \(R_{n}(\mathcal{H})\), we only need information about \(\phi(h,x)\) and _not_ the true label. Therefore, in many cases, this is preferable and not as expensive to learn.

## Appendix E Proof of Theorem 3.2

We consider the agnostic setting of Theorem 3.2. Here, we have two notions of deviations; one is deviation in a model's ability to satisfy explanations, and the other is a model's ability to generalize to correctly produce the target function.

Proof.: From Rademacher-based uniform convergence, for any \(h\in\mathcal{H}\), with probability at least \(1-\delta/2\) over \(S_{E}\)

\[|\phi(h,\mathcal{D})-\phi(h,S_{E})|\leq 2R_{k}(\mathcal{G})+\sqrt{\frac{\ln(4/ \delta)}{2k}}=\varepsilon_{k}\]

Therefore, with probability at least \(1-\delta/2\), for any \(h\in\mathcal{H}_{\phi,t-\varepsilon_{k}}\) we also have \(\phi(h,S_{E})\leq t\) and for any \(h\) with \(\phi(h,S_{E})\leq t\), we have \(h\in\mathcal{H}_{\phi,t+\varepsilon_{k}}\). In addition, by a uniform convergence bound, with probability at least \(1-\delta/2\), for any \(h\in\mathcal{H}_{\phi,t+\varepsilon_{k}}\)

\[|\mathrm{err}_{\mathcal{D}}(h)-\mathrm{err}_{S}(h)|\leq R_{n}(\mathcal{H}_{ \phi,t+\varepsilon_{k}})+\sqrt{\frac{\ln(4/\delta)}{2n}}.\]Now, let \(h^{\prime}\) be the minimizer of \(\operatorname{err}_{S}(h)\) given that \(\phi(h,S_{E})\leq t\). By previous results, with probability \(1-\delta\), we have \(h^{\prime}\in\mathcal{H}_{\phi,t+\varepsilon_{k}}\) and

\[\operatorname{err}_{\mathcal{D}}(h^{\prime}) \leq\operatorname{err}_{S}(h^{\prime})+R_{n}(\mathcal{H}_{\phi,t +\varepsilon_{k}})+\sqrt{\frac{\ln(4/\delta)}{2n}}\] \[\leq\operatorname{err}_{S}(h^{*}_{t-\varepsilon_{k}})+R_{n}( \mathcal{H}_{\phi,t+\varepsilon_{k}})+\sqrt{\frac{\ln(4/\delta)}{2n}}\] \[\leq\operatorname{err}_{\mathcal{D}}(h^{*}_{t-\varepsilon_{k}})+2 R_{n}(\mathcal{H}_{\phi,t+\varepsilon_{k}})+2\sqrt{\frac{\ln(4/\delta)}{2n}}.\]

## Appendix F EPAC-PAC learnability

We note that in our definition of EPAC learnability, we are only concerned with whether a model achieves a lower surrogate loss than \(\tau\). However, the objective of minimizing the EPAC-ERM objective is to achieve both low average error and low surrogate loss. We characterize this property as EPAC-PAC learnability.

**Definition 8** (EPAC-PAC learnability).: _For any \(\delta\in(0,1),\tau>0\), the sample complexity of \((\delta,\tau,\gamma)\) - EPAC learning of \(\mathcal{H}\) with respect to a surrogate loss \(\phi\), denoted \(m(\delta,\tau,\gamma;\mathcal{H},\phi)\) is defined as the smallest \(m\in\mathbb{N}\) for which there exists a learning rule \(\mathcal{A}\) such that every data distribution \(\mathcal{D}_{\mathcal{X}}\) over \(\mathcal{X}\), with probability at least \(1-\delta\) over \(S\sim\mathcal{D}^{m}\),_

\[\phi(\mathcal{A}(S),\mathcal{D})\leq\inf_{h\in\mathcal{H}}\phi(h,\mathcal{D})+\tau\]

_and_

\[\operatorname{err}_{\mathcal{D}}(\mathcal{A}(S))\leq\inf_{h\in\mathcal{H}} \operatorname{err}_{\mathcal{D}}(h)+\gamma.\]

_If no such \(m\) exists, define \(m(\delta,\tau,\gamma;\mathcal{H},\phi)=\infty\). We say that \(\mathcal{H}\) is EPAC-PAC learnable in the agnostic setting with respect to a surrogate loss \(\phi\) if \(\forall\delta\in(0,1),\tau>0\), \(m(\delta,\tau,\gamma;\mathcal{H},\phi)\) is finite._

## Appendix G A Generalization Bound in the Realizable Setting

In this section, we assume that we are in the doubly realizable [2] setting where there exists \(h^{*}\in\mathcal{H}\) such that \(\operatorname{err}_{\mathcal{D}}(h^{*})=0\) and \(\phi(h^{*},\mathcal{D})=0\). The optimal classifier \(h^{*}\) lies in \(\mathcal{H}\) and also achieve zero expected explanation loss. In this case, we want to output a hypothesis \(h\) that achieve both zero empirical risk and empirical explanation risk.

**Theorem G.1** (Generalization bound for the doubly realizable setting).: _For a hypothesis class \(\mathcal{H}\), a distribution \(\mathcal{D}\) and an explanation loss \(\phi\). Assume that there exists \(h^{*}\in\mathcal{H}\) that \(\operatorname{err}_{\mathcal{D}}(h^{*})=0\) and \(\phi(h^{*},\mathcal{D})=0\). Let \(S=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\) is drawn i.i.d. from \(\mathcal{D}\) and \(S_{E}=\{x_{1}^{\prime},\ldots,x_{k}^{\prime}\}\) drawn i.i.d. from \(\mathcal{D}_{\mathcal{X}}\). With probability at least \(1-\delta\), for any \(h\in\mathcal{H}\) that \(\operatorname{err}_{S}(h)=0\) and \(\phi(h,S_{E})=0\), we have_

\[\operatorname{err}_{D}(h)\leq R_{n}(\mathcal{H}_{\phi,\varepsilon_{k}})+ \sqrt{\frac{\ln(2/\delta)}{2n}}\]

_when_

\[\varepsilon_{k}=2R_{k}(\mathcal{G})+\sqrt{\frac{\ln(2/\delta)}{2k}}\]

_when \(\mathcal{G}=\{\phi(h,x)\mid h\in\mathcal{H},x\in\mathcal{X}\}\)._

Proof.: We first consider only classifiers than has low empirical explanation loss and then perform standard supervised learning. From Rademacher-based uniform convergence, for any \(h\in\mathcal{H}\), with probability at least \(1-\delta/2\) over \(S_{E}\)

\[\phi(h,\mathcal{D})\leq\phi(h,S_{E})+2R_{k}(\mathcal{G})+\sqrt{\frac{\ln(2/ \delta)}{2k}}\]when \(\mathcal{G}=\{\phi(h,x)\mid h\in\mathcal{H},x\in\mathcal{X}\}\). Therefore, for any \(h\in\mathcal{H}\) with \(\phi(h,S_{E})=0\), we have \(h\in\mathcal{H}_{\phi,\varepsilon_{k}}\) with probability at least \(1-\delta/2\). Now, we can apply the uniform convergence on \(\mathcal{H}_{\phi,\varepsilon_{k}}\). For any \(h\in\mathcal{H}_{\phi,\varepsilon_{k}}\) with \(\mathrm{err}_{S}(h)=0\), with probability at least \(1-\delta/2\), we have

\[\mathrm{err}_{\mathcal{D}}(h)\leq R_{n}(\mathcal{H}_{\phi,\varepsilon_{k}})+ \sqrt{\frac{\ln(2/\delta)}{2n}}.\]

Therefore, for \(h\in\mathcal{H}\) that \(\phi(h,S_{E})=0,\mathrm{err}_{S}(h)=0\), we have our desired guarantee. 

We remark that, since our result relies on the underlying techniques of the Rademacher complexity, our result is on the order of \(O(\frac{1}{\sqrt{n}})\). In the (doubly) realizable setting, this is somewhat loose, and more complicated techniques are required to produce tighter bounds. We leave this as an interesting direction for future work.

## Appendix H Rademacher Complexity of Linear Models with a Gradient Constraint

We calculate the empirical Rademacher complexity of a linear model under a gradient constraint.

**Theorem H.1** (Empirical Rademacher complexity of linear models with a gradient constraint).: _Let \(\mathcal{X}\) be an instance space in \(\mathbb{R}^{d}\), let \(\mathcal{D}_{\mathcal{X}}\) be a distribution on \(\mathcal{X}\), let \(\mathcal{H}=\{\hbar:x\rightarrow\langle w_{h},x\rangle\mid w_{h}\in\mathbb{R}^ {d},||w_{h}||_{2}\leq B\}\) be a class of linear model with weights bounded by some constant \(B>0\) in \(\ell_{2}\) norm. Assume that there exists a constant \(C>0\) such that \(\mathbb{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[\|x\|_{2}^{2}]\leq C^{2}\). Assume that we have an explanation constraint in terms of gradient constraint; we want the gradient of our linear model to be close to the gradient of some linear model \(h^{\prime}\). Let \(\phi(h,x)=\theta(w_{h},w_{h^{\prime}})\) be an explanation surrogate loss when \(\theta(u,v)\) is an angle between \(u,v\). For any \(S=\{x_{1},\ldots,x_{n}\}\) is drawn i.i.d. from \(\mathcal{D}_{\mathcal{X}}\), we have_

\[R_{S}(\mathcal{H}_{\phi,\tau})=\frac{B}{n}\mathbb{E}_{\sigma}\left[\|v\|f(v) \right].\]

_when \(v=\sum_{i=1}^{n}x_{i}\sigma_{i}\) and_

\[f(v)=\begin{cases}1&\text{when }\theta(v,w^{\prime})\leq\tau\\ \cos(\theta(v,w^{\prime})-\tau)&\text{when }\tau\leq\theta(v,w^{\prime})\leq \frac{\pi}{2}+\tau\\ 0&\text{when }\theta(v,w^{\prime})\geq\frac{\pi}{2}+\tau.\end{cases}\]

For the proof, we refer to Appendix H for full proof. We compare this with the standard bound on linear models which is given by

\[R_{S}(\mathcal{H})=\frac{B}{n}\mathbb{E}_{\sigma}[\|v\|].\]

Figure 7: Illustration of different value of a function \(f(v)\).

The benefits of the explanation constraints depend on the underlying data distribution; in the case of linear models with a gradient constraint, this depends on an angle between \(v=\sum_{i=1}^{n}x_{i}\sigma_{i}\) and \(w^{\prime}\). The explanation constraint reduces the term inside the expectation by a factor of \(f(v)\) depending on \(\theta(v,w^{\prime})\). When \(\theta(v,w^{\prime})\leq\tau\) then \(f(v)=1\) which implies that there is no reduction. The value of \(f(v)\) decreases as the angle between \(\theta(v,w^{\prime})\) increases and reaches \(f(v)=0\) when \(\theta(v,w^{\prime})\geq\frac{\pi}{2}+\tau\). When the data is concentrated around the area of \(w^{\prime}\), the possible regions for \(v\) would be close to \(w^{\prime}\) or \(-w^{\prime}\) (Figure 8 (Top)). The value of \(f(v)\) in this region would be either \(1\) or \(0\) and the reduction would be \(\frac{1}{2}\) on average. In essence, this means that the gradient constraint of being close to \(w^{\prime}\) does not actually tell us much information beyond the information from the data distribution. On the other hand, when the data points are nearly orthogonal to \(w^{\prime}\), the possible regions for \(v\) would lead to a small \(f(v)\) (Figure 8 (Bottom)). This can lead to a large reduction in complexity. Intuitively, when the data is nearly orthogonal to \(w^{\prime}\), there are many valid linear models including those not close in angle to \(w^{\prime}\). The constraints allows us to effectively shrink down the class of linear models that are close to \(w^{\prime}\).

Proof.: (Proof of Theorem H.1) Recall that \(\mathcal{H}_{\phi,\tau}=\{h:x\rightarrow\langle w_{h},x\rangle\mid w_{h}\in \mathbb{R}^{d},||w_{h}||_{2}\leq B,\theta(w_{h},w_{h^{\prime}})\leq\tau\}\). For a set of sample \(S\), the empirical Rademacher complexity of \(\mathcal{H}_{\phi,\tau}\) is given by

\[R_{S}(\mathcal{H}_{\phi,\tau}) =\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{h\in\mathcal{H}_{\phi,\tau}}\sum_{i=1}^{n}h(x_{i})\sigma_{i}\right]\] \[=\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{\begin{subarray}{c} \|w_{h}\|_{2}\leq B\\ \theta(w_{h},w_{h^{\prime}})\leq\tau\end{subarray}}\sum_{i=1}^{n}\langle w_{ h},x_{i}\rangle\sigma_{i}\right]\] \[=\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{\begin{subarray}{c} \|w_{h}\|_{2}\leq B\\ \theta(w_{h},w_{h^{\prime}})\leq\tau\end{subarray}}\langle w_{h},\sum_{i=1}^{ n}x_{i}\sigma_{i}\rangle\right].\]

For a vector \(w^{\prime}\in\mathbb{R}^{d}\) with \(\|w^{\prime}\|_{2}=1\), and a vector \(v\in\mathbb{R}^{d}\), we will claim the following,

Figure 8: Benefits of an explanation constraint also depend on the data distribution. We represent data points \(x_{i}\) with red squares (Left). The possible regions for \(v=\sum_{i=1}^{n}x_{i}\sigma_{i}\) are the shaded areas (Right). When the data is highly correlated with \(w^{\prime}\), \(v\) would lie in a region where \(f(v)\) is large (Top) and this implies that the gradient constraints provide less benefits. On the other hand, when the data is almost orthogonal to \(w^{\prime}\), \(v\) would lie in a region with a small value of \(f(v)\) (Bottom) which leads to more benefits from the gradient constraints

1. If \(\theta(v,w^{\prime})\leq\tau\), we have \[\sup_{\begin{subarray}{c}\|w\|_{2}\leq B\\ \theta(w,w^{\prime})\leq\tau\end{subarray}}\langle w,v\rangle=B\|v\|.\]
2. If \(\frac{\pi}{2}+\tau\leq\theta(v,w^{\prime})\leq\pi\), we have \[\sup_{\begin{subarray}{c}\|v\|_{2}\leq B\\ \theta(w,w^{\prime})\leq\tau\end{subarray}}\langle w,v\rangle=0.\]
3. If \(\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau\), we have \[\sup_{\begin{subarray}{c}\|w\|_{2}\leq B\\ \theta(w,w^{\prime})\leq\tau\end{subarray}}\langle w,v\rangle=B\|v\|\cos( \theta(v,w^{\prime})-\tau)\]

For the first claim, we can see that if \(\theta(v,w^{\prime})\leq\tau\), we can pick \(w=\frac{Bv}{\|v\|}\) and achieve the optimum value. For the second claim, we use the fact that \(\theta(\cdot,\cdot)\) satisfies a triangle inequality and for any \(w\) that \(\theta(w,w^{\prime})\leq\tau\), we have

\[\theta(v,w)+\theta(w,w^{\prime})\geq\theta(v,w^{\prime})\] \[\theta(v,w)\geq\theta(v,w^{\prime})-\theta(w,w^{\prime})\] \[\theta(v,w)\geq\frac{\pi}{2}+\tau-\tau=\frac{\pi}{2}.\]

This implies that for any \(w\) that \(\theta(w,w^{\prime})\leq\tau\), we have \(\langle w,v\rangle=\|w\|\|v\|\cos(\theta(v,w))\leq 0\) and the supremum is given by \(0\) where we can set \(\|w\|=0\). For the third claim, we know that \(\langle w,v\rangle\) is maximum when the angle between \(v,w\) is the smallest. From the triangle inequality above, we must have \(\theta(w,w^{\prime})=\tau\) to be the largest possible value so that we have the smallest lower bound \(\theta(v,w)\geq\theta(v,w^{\prime})-\theta(w,w^{\prime})\). In addition, the inequality holds when \(v,w^{\prime},w\) lie on the same plane. Since we do not have further restrictions on \(w\), there exists such \(w\) and we have

\[\sup_{\begin{subarray}{c}\|w\|_{2}\leq B\\ \theta(w,w^{\prime})\leq\tau\end{subarray}}\langle w,v\rangle=B\|v\|\cos( \theta(v,w^{\prime})-\tau)\]

as required. One can calculate a closed form formula for \(w\) by solving a quadratic equation. Let \(w=\frac{B\tilde{w}}{\|v\|}\) when \(\tilde{w}=v+\lambda w^{\prime}\) for some constant \(\lambda>0\) such that \(\theta(w,w^{\prime})=\tau\). With this we have an equation

\[\frac{\langle\tilde{w},w^{\prime}\rangle}{\|\tilde{w}\|} =\cos(\tau)\] \[\frac{\langle v+\lambda w^{\prime},w^{\prime}\rangle}{\|v+ \lambda w^{\prime}\|} =\cos(\tau)\]

Let \(\mu=\langle v,w^{\prime}\rangle\), solving for \(\lambda\), we have

\[\frac{\mu+\lambda}{\sqrt{\|v\|^{2}+2\lambda\mu+\lambda^{2}}} =\cos(\tau)\] \[\mu^{2}+2\mu\lambda+\lambda^{2} =\cos^{2}(\tau)(\|v\|^{2}+2\lambda\mu+\lambda^{2})\] \[\sin^{2}(\tau)\lambda^{2}+2\sin^{2}(\tau)\mu\lambda+\mu^{2}-\cos ^{2}(\tau)\|v\|^{2} =0\] \[\lambda^{2}+2\mu\lambda+\frac{\mu^{2}}{\sin^{2}(\tau)}-\cot^{2}( \tau)\|v\|^{2} =0\]

Solve this quadratic equation, we have

\[\lambda=-\mu\pm\cot(\tau)\sqrt{\|v\|^{2}-\mu^{2}}.\]Since \(\lambda>0\), we have \(\lambda=-\mu+\cot(\tau)\sqrt{\|v\|^{2}-\mu^{2}}\). We have

\[\tilde{w} =v+\lambda w^{\prime}\] \[=v+(-\mu+\cot(\tau)\sqrt{\|v\|^{2}-\mu^{2}})w^{\prime}\] \[=v-\langle v,w^{\prime}\rangle w^{\prime}+cot(\tau)w^{\prime} \sqrt{\|v\|^{2}-\mu^{2}}.\]

With these claims, we have

\[R_{S}(\mathcal{H}_{\phi,\tau}) =\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{\begin{subarray}{c}\| w_{k}\|_{2}\leq B\\ \theta(w_{h},v_{h^{\prime}})\leq\tau\end{subarray}}\langle w_{h},\sum_{i=1}^{n} x_{i}\sigma_{i}\rangle\right]\] \[=\frac{B}{n}\mathbb{E}_{\sigma}\left[\|v\|1\{\theta(v,w^{\prime} )\leq\tau\}+\|v\|1\{\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau\}\cos( \theta(v,w^{\prime})-\tau)\right]\] \[=\frac{B}{n}\mathbb{E}_{\sigma}\left[\|v\|f(v)\right].\]

**Theorem H.2** (Rademacher complexity of linear models with gradient constraint, uniform distribution on a sphere).: _Let \(\mathcal{X}\) be an instance space in \(\mathbb{R}^{d}\), let \(\mathcal{D}_{\mathcal{X}}\) be a uniform distribution on a unit sphere in \(\mathbb{R}^{d}\), let \(\mathcal{H}=\{h:x\rightarrow\langle w_{h},x\rangle\mid w_{h}\in\mathbb{R}^{d},\|\lvert w_{h}\rvert_{2}\leq B\}\) be a class of linear model with weights bounded by some constant \(B>0\) in \(\ell_{2}\) norm. Assume that there exists a constant \(C>0\) such that \(\mathbb{E}_{\kappa\sim\mathcal{D}_{\mathcal{X}}}[||x||_{2}^{2}]\leq C^{2}\). Assume that we have an explanation constraint in terms of gradient constraint; we want the gradient of our linear model to be close to the gradient of some linear model \(h^{\prime}\). Let \(\phi(h,x)=\theta(w_{h},w_{h^{\prime}})\) be an explanation surrogate loss when \(\theta(u,v)\) is an angle between \(u,v\). We have_

\[R_{n}(\mathcal{H}_{\phi,\tau})=\frac{B}{\sqrt{n}}\left(\sin(\tau)\cdot p+\frac {1-p}{2}\right),\]

_where_

\[p=\operatorname{erf}\left(\frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).\]

Proof.: From Theorem H.1, we have that

\[R_{n}(\mathcal{H}_{\phi,\tau}) =\mathbb{E}[R_{S}(\mathcal{H}_{\phi,\tau})]\] \[=\frac{B}{n}\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma} \left[\|v\|1\{\theta(v,w^{\prime})\leq\tau\}+\|v\|1\{\tau\leq\theta(v,w^{ \prime})\leq\frac{\pi}{2}+\tau\}\cos(\theta(v,w^{\prime})-\tau)\right]\right]\] \[=\frac{B}{n}\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma} \left[\|v\|1\{\theta(v,w^{\prime})\leq\frac{\pi}{2}-\tau\}+\|v\|1\{\frac{\pi}{ 2}-\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau\}\cos(\theta(v,w^{\prime })-\tau)\right]\right]\]

when \(v=\sum_{i=1}^{n}x_{i}\sigma_{i}\). Because \(x_{i}\) is drawn uniformly from a unit sphere, in expectation \(\theta(v,w^{\prime})\) has a uniform distribution over \([0,\pi]\) and the distribution \(\|v\|\) for a fixed value of \(\theta(v,w^{\prime})\) are the same for all \(\theta(v,w^{\prime})\in[0,\pi]\). From Trigonometry, we note that

\[\cos(\frac{\pi}{2}-2\tau+a)+\cos(\frac{\pi}{2}-a)=\sin(2\tau-a)+\sin(a)\leq 2 \sin(\tau).\]By the symmetry property and the uniformity of the distribution of \(\theta(v,w^{\prime})\) and \(\|v\|\).

\[\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma}\left[\|v\|1\{ \frac{\pi}{2}-\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau\}\cos(\theta(v, w^{\prime})-\tau)\right]\right]\] \[=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma}\left[\|v\|1\{ 0\leq\theta(v,w^{\prime})\leq 2\tau\}\cos(\frac{\pi}{2}+\theta(v,w^{\prime})- \tau)\right]\right]\] \[=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma}\left[\|v\|1\{ 0\leq\theta(v,w^{\prime})\leq\tau\}\cos(\frac{\pi}{2}+\theta(v,w^{\prime})- \tau)+1\{\tau\leq\theta(v,w^{\prime})\leq 2\tau\}\cos(\frac{\pi}{2}+\theta(v,w^{ \prime})-\tau))\right]\right]\] \[=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma}\left[\|v\|(1 \{0\leq\theta(v,w^{\prime})\leq\tau\}\cos(\frac{\pi}{2}+\theta(v,w^{\prime})- \tau)+1\{0\leq 2\tau-\theta(v,w^{\prime})\leq\tau\}\cos(\frac{\pi}{2}-(2 \tau-\theta(v,w^{\prime}))))\right]\right]\] \[=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma}\left[\|v\|(1 \{0\leq\theta(v,w^{\prime})\leq\tau\}\cos(\frac{\pi}{2}+\theta(v,w^{\prime})- \tau)+1\{0\leq\tilde{\theta}(v,w^{\prime})\leq\tau\}\cos(\frac{\pi}{2}-\tilde{ \theta}(v,w^{\prime})))\right]\right]\] \[=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma}\left[\|v\|(1\{ 0\leq\theta(v,w^{\prime})\leq\tau\}\cos(\frac{\pi}{2}+\theta(v,w^{\prime})- \tau)+1\{0\leq\theta(v,w^{\prime})\leq\tau\}\cos(\frac{\pi}{2}-\theta(v,w^{ \prime})))\right]\right]\] \[\leq\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma}\left[\|v\|1 \{\frac{\pi}{2}-\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau\}\sin(\tau )\right]\right]\]

when \(\tilde{\theta}(v,w^{\prime})=\frac{\pi}{2}-\theta(v,w^{\prime})\). We have

\[R_{n}(\mathcal{H}_{\phi,\tau}) \leq\frac{B}{n}\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma} \left[\|v\|1\{\theta(v,w^{\prime})\leq\frac{\pi}{2}-\tau\}+\|v\|1\{\frac{\pi} {2}-\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau\}\sin(\tau)\right]\right]\] \[=\frac{B}{n}\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\sigma} \left[\|v\|\right]\right]\left(\Pr(\theta(v,w^{\prime})\leq\frac{\pi}{2}- \tau)+\Pr(\frac{\pi}{2}-\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau) \sin(\tau)\right)\]

The last equation follows from the symmetry and uniformity properties. We can bound the first expectation

\[\mathbb{E}_{\mathcal{D}}[\mathbb{E}_{\sigma}\|v\|]] =\mathbb{E}_{\mathcal{D}}[\mathbb{E}_{\sigma}[\sum_{i=1}^{n}x_{i }\sigma_{i}\|]]\] \[\leq\mathbb{E}_{\mathcal{D}}[\sqrt{\mathbb{E}_{\sigma}\|\sum_{i= 1}^{n}x_{i}\sigma_{i}\|^{2}]}]\] \[=\mathbb{E}_{\mathcal{D}}[\sqrt{\mathbb{E}_{\sigma}\sum_{i=1}^{n} \|x_{i}\|^{2}\sigma_{i}^{2}]}]\] \[\leq C\sqrt{n}.\]

Next, we can simply note that, since our data is distributed over a unit sphere, each data has norm no greater than 1. Therefore, we know that \(C=1\) is indeed an upper bound on \(\mathbb{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[||x||_{2}^{2}]\). For the probability term, we note that in expectation \(v\) has the same distribution as a random vector \(u\) drawn uniformly from a unit sphere. We let this be some probability \(p\):

\[p=\Pr\left(\frac{\pi}{2}-\tau\leq\theta(v,w^{\prime})\leq\frac{\pi}{2}+\tau \right)=\Pr\left(|\langle u,w^{\prime}\rangle|\leq\sin(\tau)\right).\]

We know that the projection \(\langle u,w^{\prime}\rangle\sim\mathcal{N}(0,\frac{1}{d})\). Then, we have that \(|\langle u,w^{\prime}\rangle|\) is given by a Folded Normal Distribution, which has a CDF given by

\[\Pr\left(|\langle u,w^{\prime}\rangle|\leq\sin(\tau)\right) =\frac{1}{2}\left[\operatorname{erf}\left(\frac{\sqrt{d}\sin( \tau)}{\sqrt{2}}\right)+\operatorname{erf}\left(\frac{\sqrt{d}\sin(\tau)}{ \sqrt{2}}\right)\right]\] \[=\operatorname{erf}\left(\frac{\sqrt{d}\sin(\tau)}{\sqrt{2}} \right).\]

We then observe that

\[\Pr\left(\theta(v,w^{\prime})\leq\frac{\pi}{2}-\tau\right) =\frac{1}{2}\left(1-\Pr\left(\frac{\pi}{2}-\tau\leq\theta(v,w^{ \prime})\leq\frac{\pi}{2}+\tau\right)\right)\] \[=\frac{1-p}{2}\]Plugging this in yields the following bound

\[R_{n}(\mathcal{H}_{\phi,\tau})=\frac{B}{\sqrt{n}}\left(\sin(\tau)\cdot p+\frac{1- p}{2}\right),\]

where

\[p=\mathrm{erf}\left(\frac{\sqrt{d}\sin(\tau)}{\sqrt{2}}\right).\]

## Appendix I Rademacher Complexity for Two Layer Neural Networks with a Gradient Constraint

Here, we present the full proof of the generalization bound for two layer neural networks with gradient explanations. In our proof, we use two results from Ma [24]. One result is a technical lemma, and the other is a bound on the Rademacher complexity of two layer neural networks.

**Lemma I.1**.: _Consider a set \(S=\{x_{1},...,x_{n}\}\) and a hypothesis class \(\mathcal{F}\subset\{f:\mathbb{R}^{d}\rightarrow\mathbb{R}\}\). If_

\[\sup_{f\in\mathcal{F}}\sum_{i=1}^{n}f(x_{i})\sigma_{i}\geq 0\ \ \text{for any}\ \ \sigma_{i}\in\{\pm 1\},i=1,...,n,\]

_then, we have that_

\[\mathbb{E}_{\sigma}\left[\sup_{f\in\mathcal{F}}|\sum_{i=1}^{n}f(x_{i})\sigma _{i}|\right]\leq 2\mathbb{E}_{\sigma}\left[\sup_{f\in\mathcal{F}}\sum_{i=1}^{n}f( x_{i})\sigma_{i}\right].\]

**Theorem I.2** (Rademacher complexity for two layer neural networks [24]).: _Let \(\mathcal{X}\) be an instance space and \(\mathcal{D}_{\mathcal{X}}\) be a distribution over \(\mathcal{X}\). Let \(\mathcal{H}=\{h:x\mapsto\sum_{i=1}^{m}w_{i}\sigma(u_{i}^{\top}x)|w_{i}\in \mathbb{R},u_{i}\in\mathbb{R}^{d},\sum_{i=1}^{m}|w_{i}|\|u_{i}\|_{2}\leq B\}\) be a class of two layer neural networks with \(m\) hidden nodes with a ReLU activation function \(\sigma(x)=\max(0,x)\). Assume that there exists some constant \(C>0\) such that \(\mathbb{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[\|x\|_{2}^{2}]\leq C^{2}\). Then, for any \(S=\{x_{1},\ldots,x_{n}\}\) is drawn i.i.d. from \(\mathcal{D}_{\mathcal{X}}\), we have that_

\[R_{S}(\mathcal{H})\leq\frac{2B}{n}\sqrt{\sum_{i=1}^{n}||x_{i}||_{2}^{2}}\]

_and_

\[R_{n}(\mathcal{H})\leq\frac{2BC}{\sqrt{n}}.\]

We defer interested readers to [24] for the full proof of this result. Here, the only requirement of the data distribution is that \(\mathbb{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[\|x\|_{2}^{2}]\leq C^{2}\). We now present our result in the setting of two layer neural networks with one hidden node \(m=1\) to provide clearer intuition for the overall proof.

**Theorem I.3** (Rademacher complexity for two layer neural networks (\(m=1\)) with gradient constraints).: _Let \(\mathcal{X}\) be an instance space and \(\mathcal{D}_{\mathcal{X}}\) be a distribution over \(\mathcal{X}\). Let \(\mathcal{H}=\{h:x\mapsto w\sigma(u^{\top}x)|w\in\mathbb{R},u\in\mathbb{R}^{d}, |w|\leq B,\|u\|=1\}\). Without loss of generality, we assume that \(\|u\|=1\). Assume that there exists some constant \(C>0\) such that \(\mathbb{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[\|x\|_{2}^{2}]\leq C^{2}\). Our explanation constraint is given by a constraint on the gradient of our models, where we want the gradient of our learnt model to be close to a particular target function \(h^{\prime}\in\mathcal{H}\). Let this be represented by an explanation loss given by_

\[\phi(h,x)=\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|_{2}+\infty\cdot 1\{\| \nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|>\tau\}\]

_for some \(\tau>0\). Let \(h^{\prime}(x)=w^{\prime}\sigma((u^{\prime})^{\top}x)\) the target function, then we have_

\[R_{n}(\mathcal{H}_{\phi,\tau})\leq\frac{\tau C}{\sqrt{n}} \text{if}\ |w^{\prime}|>\tau,\] \[R_{n}(\mathcal{H}_{\phi,\tau})\leq\frac{3\tau C}{\sqrt{n}} \text{if}\ |w^{\prime}|\leq\tau.\]Proof.: Our choice of \(\phi(h,x)\) guarantees that, for any \(h\in\mathcal{H}_{\phi,\tau}\), we have that \(\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|\leq\tau\) almost everywhere. We note that for \(h(x)=w\sigma(u^{\top}x)\), the gradient is given by \(\nabla_{x}h(x)=wu1\{u^{\top}x>0\}\), which is a piecewise constant function over two regions (i.e., \(u^{\top}x>0,u^{\top}x\leq 0\)), captured by Figure I.

We now consider \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\), and we have 3 possible cases.

**Case 1:**\(\theta(u,u^{\prime})>0\)

This implies that the boundaries of \(\nabla_{x}(h)\) and \(\nabla_{x}h^{\prime}(x)\) are different. Then, we have that \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) is a piecewise constant function with 4 regions, taking on values

\[\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)=\begin{cases}wu-w^{\prime}u^{\prime}& \text{when }u^{\top}x>0,(u^{\prime})^{\top}x>0\\ wu&\text{when }u^{\top}x>0,(u^{\prime})^{\top}x<0\\ -w^{\prime}u^{\prime}&\text{when }u^{\top}x<0,(u^{\prime})^{\top}x>0\\ 0&\text{when }u^{\top}x<0,(u^{\prime})^{\top}x<0\end{cases}\]

If we assume that each region has probability mass greater than 0 then our constraint \(\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|_{2}\leq\tau\) implies that \(|w|=|w|\|u\|\leq\tau,|w^{\prime}|=|w^{\prime}|\|u^{\prime}\|\leq\tau,\|wu-w^{ \prime}u^{\prime}\|\leq\tau\).

**Case 2:**\(\theta(u,u^{\prime})=0\)

This implies that the boundary of \(\nabla_{x}h(x)\) and \(\nabla_{x}h^{\prime}(x)\) are the same. Then, \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) is a piecewise constant over two regions

\[\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)=\begin{cases}wu-w^{\prime}u^{\prime}& \text{when }u^{\top}x>0\\ 0&\text{when }u^{\top}x<0\end{cases}\]

This gives us that \(|w-w^{\prime}|=\|wu-w^{\prime}u^{\prime}\|\leq\tau\).

**Case 3:**\(\theta(u,u^{\prime})=\pi\)

Here, we have that the decision boundaries of \(\nabla_{x}h(x)\) and \(\nabla_{x}h^{\prime}(x)\) are the same but the gradients are non-zero on different sides. Then, \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) is a piecewise constant on two regions

\[\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)=\begin{cases}wu&\text{when }u^{\top}x>0\\ -w^{\prime}u^{\prime}&\text{when }u^{\top}x<0\end{cases}\]

This gives us that \(|w|\leq\tau\) and \(|w^{\prime}|\leq\tau\).

These different cases tell us that the constraint \(\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|\leq\tau\) reduces \(\mathcal{H}\) into a class of models follows either

Figure 9: Visualization of the piecewise constant function of \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) over 4 regions.

[MISSING_PAGE_FAIL:28]

Combining this with the fact that \(\mathbb{E}\left[\|x\|^{2}\right]\leq C^{2}\), we have

\[R_{n}(\mathcal{H}_{\phi,\tau}) =\mathbb{E}[R_{S}(\mathcal{H}_{\phi,\tau})]\] \[\leq\frac{\tau}{n}\mathbb{E}[\sqrt{\sum_{i=1}^{n}\|x_{i}\|^{2}}\] \[\leq\frac{\tau}{n}\sqrt{\mathbb{E}[\sum_{i=1}^{n}\|x_{i}\|^{2}]} \quad\text{(Jensen's inequality)}\] \[\leq\frac{\tau C}{\sqrt{n}}.\]

**Case 2:**\(|w^{\prime}|||u^{\prime}||<\tau\).

We know that \(\mathcal{H}_{\phi,\tau}=\mathcal{H}_{\phi,\tau}^{(1)}\bigcup\mathcal{H}_{\phi,\tau}^{(2)}\) when

\[\mathcal{H}_{\phi,\tau}^{(1)} =\{h\in\mathcal{H}|h:x\to w\sigma(u^{\top}x),u=u^{\prime},|w-w^{ \prime}|<\tau\}\] \[\mathcal{H}_{\phi,\tau}^{(2)} =\{h\in\mathcal{H}|h:x\to w\sigma(u^{\top}x),\|u\|=1,u\neq u^{ \prime},|w|<\tau\}\]

We have

\[R_{S}(\mathcal{H}_{\phi,\tau}) =\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{h\in\mathcal{H}_{\phi,\tau}}\sum_{i=1}^{n}h(x_{i})\sigma_{i}\right]\] \[\leq\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{h\in\mathcal{H}_{ \phi,\tau}^{(1)}}\sum_{i=1}^{n}h(x_{i})\sigma_{i}+\sup_{h\in\mathcal{H}_{\phi,\tau}^{(2)}}\sum_{i=1}^{n}h(x_{i})\sigma_{i}\right]\] \[=R_{S}(\mathcal{H}_{\phi,\tau}^{(1)})+R_{S}(\mathcal{H}_{\phi, \tau}^{(2)})\]

The second line holds as \(\sup_{x\in A\cup B}f(x)\leq\sup_{x\in A}f(x)+\sup_{x\in B}f(x)\) when \(\sup_{x\in A}f(x)\geq 0\) and \(\sup_{x\in B}f(x)\geq 0\). We know that both of these supremums be greater than zero, as we can recover the value of 0 with \(w=0\). From **Case 1**, we know that

\[R_{n}(\mathcal{H}_{\phi,\tau}^{(1)})\leq\frac{\tau C}{\sqrt{n}}.\]

We also note that \(\mathcal{H}_{\phi,\tau}^{(2)}\) is a class of two layer neural networks with weights with norms bounded by \(\tau\). From Theorem 1.2, we have that

\[R_{n}(\mathcal{H}_{\phi,\tau}^{(2)})\leq\frac{2\tau C}{\sqrt{n}}.\]

Therefore, in **Case 2**,

\[R_{n}(\mathcal{H}_{\phi,\tau})\leq\frac{3\tau C}{\sqrt{n}}.\]

as required. 

Now, we consider in the general setting (i.e., no restriction on \(m\)).

**Theorem 1.4** (Rademacher complexity for two layer neural networks with gradient constraints ).: _Let \(\mathcal{X}\) be an instance space and \(\mathcal{D}_{\mathcal{X}}\) be a distribution over \(\mathcal{X}\) with a large enough support. Let \(\mathcal{H}=\{h:x\mapsto\sum_{j=1}^{m}w_{j}\sigma(u_{j}^{\top}x)|w_{j}\in \mathbb{R},u_{j}\in\mathbb{R}^{d},\|u_{j}\|_{2}=1,\sum_{j=1}^{m}|w_{j}|\leq B\}\). Assume that there exists some constant \(C>0\) such that \(\mathbb{E}_{x\sim\mathcal{D}_{\mathcal{X}}}[\|x\|_{2}^{2}]\leq C^{2}\). Our explanation constraint is given by a constraint on the gradient of our models, where we want the gradient of our learnt model to be close to a particular target function \(h^{\prime}\in\mathcal{H}\). Let this be represented by an explanation loss given by_

\[\phi(h,x)=\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|_{2}+\infty\cdot 1\{\| \nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|>\tau\}\]_for some \(\tau>0\). Then, we have that_

\[R_{n}(\mathcal{H}_{\phi,\tau})\leq\frac{3\tau mC}{\sqrt{n}}.\]

_To be precise,_

\[R_{n}(\mathcal{H}_{\phi,\tau})\leq\frac{(2m+q)\tau C}{\sqrt{n}}.\]

_when \(q\) is the number of node \(j\) of \(h^{\prime}\) such that \(|w_{j}^{\prime}|<\tau\)._

We note that this result indeed depends on the number of hidden dimensions \(m\); however, we note that in the general case (Theorem I.2), the value of \(B\) is \(O(m)\) as it is a sum over the values of each hidden node. We now present the proof for the more general version of our theorem.

Proof.: For simplicity, we first assume that any \(h\in\mathcal{H}\) has that \(\|u_{j}\|=1,\forall j\). Consider \(h\in\mathcal{H}\), we write \(h=\sum_{j=1}^{m}w_{j}^{\prime}\sigma((u_{j}^{\prime})^{\top}x)\) and let \(h^{\prime}(x)=\sum_{j=1}^{m}w_{j}^{\prime}\sigma((u_{j}^{\prime})^{\top}x)\) be a function for our gradient constraint. The gradient of a hypothesis \(h\) is given by

\[\nabla_{x}h(x)=\sum_{j=1}^{m}w_{j}u_{j}\cdot 1\{u_{j}^{\top}x>0\},\]

which is a piecewise constant function over at most \(2^{m}\) regions. Then, we consider that

\[\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)=\sum_{j=1}^{m}w_{j}u_{j}\cdot 1\{u_{j}^{ \top}x>0\}-\sum_{j=1}^{m}w_{j}^{\prime}u_{j}^{\prime}\cdot 1\{(u_{j}^{\prime})^{ \top}x>0\},\]

which is a piecewise constant function over at most \(2^{2m}\) regions. We again make an assumption that each of these regions has a non-zero probability mass. Our choice of \(\phi(h,x)\) guarantees that the norm of the gradient in each region is less than \(\tau\). Similar to the case with \(m=1\), we will show that the gradient constraint leads to a class of functions with the same decision boundary or neural networks that have weights with a small norm.

Assume that among \(u_{1},...,u_{m}\) there are \(k\) vectors that have the same direction as \(u_{1}^{\prime},...,u_{m}^{\prime}\). Without loss of generality, let \(u_{j}=u_{j}^{\prime}\) for \(j=1,...,k\). In this case, we have that \(\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\) is a piecewise function over \(2^{2m-k}\) regions. As each region has non-zero probability mass, for each \(j\in\{1,...,k\}\), we know that \(\exists x\) such that

\[u_{j}^{\top}x=(u_{j}^{\prime})^{\top}x>0,\qquad u_{i}^{\top}x<0\text{ for }i\neq j,\qquad(u_{i}^{\prime})^{\top}x<0\text{ for }i\neq j.\]

In other words, we can observe a data point from each region that uniquely defines the value of a particular \(w_{j},u_{j}\). In this case, we have that

\[\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x) =w_{j}u_{j}-w_{j}^{\prime}u_{j}^{\prime}\] \[=(w_{j}-w_{j}^{\prime})u_{j}^{\prime}.\]

From our gradient constraint, we know that \(||\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)||\leq\tau,\forall x\), which implies that \(|w_{j}-w_{j}^{\prime}|\leq\tau\) for \(j=1,...,k\).

On the other hand, for the remaining \(j=k+1,...,m\), we know that there exists \(x\) such that

\[u_{j}^{\top}x>0,\qquad u_{i}^{\top}x<0\text{ for }i\neq j,\qquad(u_{i}^{\prime})^{ \top}x<0\text{ for }i=1,...,m.\]

Then, we have that \(\nabla_{x}h(x)=w_{j}u_{j}\), and our constraint implies that \(|w_{j}||u_{j}||=|w_{j}|\leq\tau\). Similarly, we have that \(|w_{j}^{\prime}||u_{j}^{\prime}||=|w_{j}^{\prime}|<\tau\), for \(j=k+1,...,m\). We can conclude that \(\mathcal{H}_{\phi,\tau}\) is a class of two layer neural networks with \(m\) hidden nodes (assuming \(\|u_{i}\|\) = 1) that for each node \(w_{j}\sigma(u_{j}^{\top}x)\) satisfies

1. There exists \(l\in[m]\) that \(u_{j}=u_{l}^{\prime}\) and \(|w_{j}-w_{l}^{\prime}|<\tau\).

2. \(|w_{j}|<\tau\)

We further note that for a node \(w^{\prime}_{j}\sigma((u^{\prime}_{l})^{\top}x)\) in \(h^{\prime}(x)\) that has that a high weight \(|w^{\prime}_{l}|>\tau\), there must be a node \(w_{j}\sigma(u^{\top}_{j}x)\) in \(h\) with the same boundary \(u_{j}=u_{l}\). Otherwise, there is a contradiction with \(|w^{\prime}_{l}|<\tau\) for all nodes in \(h^{\prime}\) without a node in \(h\) with the same boundary. We can utilize this characterization of the restricted class \(\mathcal{H}_{\phi,\tau}\) to bound the Rademacher complexity of the class. Let

\[\mathcal{H}^{\prime}=\{h:x\mapsto\sum_{j=1}^{m}w^{\prime}_{j}\sigma((u^{\prime }_{j})^{\top}x)a_{j}\mid a_{j}\in\{0,1\}\text{ and for }j\text{ that }|w^{\prime}_{j}|>\tau,a_{j}=1\}.\]

This is a class of two layer neural networks with at most \(m\) nodes such that each node is from \(h^{\prime}\). We also have a condition that if the weight of the \(j\)-th node in \(h^{\prime}\) is greater than \(\tau\), the \(j\)-th node must be present in any member of this class. Let

\[\mathcal{H}^{(\tau)}=\{h:x\mapsto\sum_{j=1}^{m}w_{j}\sigma((u_{j})^{\top}x)a_ {j}\mid w_{j}\in\mathbb{R},u_{j}\in\mathbb{R}^{d},|w_{j}|<\tau,\|u_{j}\|=1\}.\]

be a class of two layer neural networks with \(m\) nodes such that the weight of each node is at most \(\tau\). We claim that for any \(h\in\mathcal{H}_{\phi,\tau}\) there exists \(h_{1}\in\mathcal{H}^{(\tau)},h_{2}\in\mathcal{H}^{(\tau)}\) that \(h=h_{1}+h_{2}\). For any \(h\in\mathcal{H}_{\phi,\tau}\), let \(p_{h}:[m]\rightarrow[m]\cup\{0\}\) be a function that match a node in \(h\) with the node with the same boundary in \(h^{\prime}\). Formally,

\[p_{h}(j)=\begin{cases}l&\text{ when }u_{j}=u^{\prime}_{l}\\ 0&\text{ otherwise.}\end{cases}\]

The function \(p_{h}\) maps \(j\) to \(0\) if there is no node in \(h^{\prime}\) with the same boundary. Let \(w^{\prime}_{0}=0,u^{\prime}_{0}=[0,\ldots,0]\), we can write

\[h(x) =\sum_{j=1}^{m}w_{j}\sigma(u^{\top}_{j}x)\] \[=\sum_{j=1}^{m}w_{j}\sigma(u^{\top}_{j}x)-w^{\prime}_{p_{h}(j)} \sigma((u^{\prime})^{\top}_{p_{h}(j)}x)+w^{\prime}_{p_{h}(j)}\sigma((u^{\prime })^{\top}_{p_{h}(j)}x)\] \[=\underbrace{\sum_{p_{h}(j)\neq 0}(w_{j}-w^{\prime}_{p_{h}(j)}) \sigma((u^{\prime})^{\top}_{p_{h}(j)}x)+\sum_{p_{h}(j)=0}w_{j}\sigma(u^{\top} _{j}x)}_{\in\mathcal{H}^{(\tau)}}+\underbrace{\sum_{p(j)\neq 0}w^{\prime}_{p_{h}(j)} \sigma((u^{\prime})^{\top}_{p_{h}(j)}x)}_{\in\mathcal{H}^{(\tau)}}.\]

The first term is a member of \(\mathcal{H}^{(\tau)}\) because we know that \(|w_{j}-w^{\prime}_{p(j)}|<\tau\) or \(|w_{j}|<\tau\). The second term is also a member of \(\mathcal{H}^{\prime}\) since for any \(l\) that \(|w^{\prime}_{l}|>\tau\), there exists \(j\) that \(p_{h}(j)=l\). Therefore, we can write \(h\) in terms of a sum between a member of \(\mathcal{H}^{\prime}\) and \(\mathcal{H}^{(\tau)}\). This implies that

\[R_{n}(\mathcal{H}_{\phi,\tau})\leq R_{n}(\mathcal{H}^{\prime})+R_{n}(\mathcal{ H}^{(\tau)}).\]

From Theorem I.2, we have that

\[R_{n}(\mathcal{H}^{(\tau)}_{\phi,\tau})\leq\frac{2\tau mC}{\sqrt{n}}.\]Now, we will calculate the Rademacher complexity of \(\mathcal{H}^{\prime}\). For \(S=\{x_{1},\ldots,x_{n}\}\),

\[R_{S}(\mathcal{H}^{\prime}) =\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{h\in\mathcal{H}^{\prime} }\sum_{i=1}^{n}h(x_{i})\sigma_{i}\right]\] \[=\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{h\in\mathcal{H}^{ \prime}}\sum_{i=1}^{n}(\sum_{j=1}^{m}w_{j}^{\prime}\sigma((u_{j}^{\prime})^{ \top}x_{i})a_{j})\sigma_{i}\right]\] \[=\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{h\in\mathcal{H}^{ \prime}}\sum_{i=1}^{n}(\sum_{|w_{j}^{\prime}|<\tau}w_{j}^{\prime}\sigma((u_{j} ^{\prime})^{\top}x_{i})a_{j}+\sum_{|w_{j}^{\prime}|>\tau}w_{j}^{\prime}\sigma( (u_{j}^{\prime})^{\top}x_{i}))\sigma_{i}\right]\] \[=\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{a_{j}\in\{0,1\}}\sum_ {i=1}^{n}\sum_{|w_{j}^{\prime}|<\tau}w_{j}^{\prime}\sigma((u_{j}^{\prime})^{ \top}x_{i})a_{j}\sigma_{i}\right]\] \[=\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{a_{j}\in\{0,1\}}\sum_ {|w_{j}^{\prime}|<\tau}a_{j}(w_{j}^{\prime}\sum_{i=1}^{n}\sigma((u_{j}^{\prime })^{\top}x_{i})\sigma_{i})\right].\]

To achieve the supremum, if \(w_{j}^{\prime}\sum_{i=1}^{n}\sigma((u_{j}^{\prime})^{\top}x_{i})\sigma_{i}>0\) we need to set \(a_{j}=1\), otherwise, we need to set \(a_{j}=0\). Therefore,

\[R_{S}(\mathcal{H}^{\prime}) =\frac{1}{n}\mathbb{E}_{\sigma}\left[\sup_{a_{j}\in\{0,1\}}\sum_ {|w_{j}^{\prime}|<\tau}a_{j}(w_{j}^{\prime}\sum_{i=1}^{n}\sigma((u_{j}^{\prime })^{\top}x_{i})\sigma_{i})\right]\] \[=\frac{1}{n}\mathbb{E}_{\sigma}\left[\sum_{|w_{j}^{\prime}|<\tau }\sigma(w_{j}^{\prime}\sum_{i=1}^{n}\sigma((u_{j}^{\prime})^{\top}x_{i})\sigma _{i})\right]\] \[=\frac{1}{2n}\mathbb{E}_{\sigma}\left[\sum_{|w_{j}^{\prime}|<\tau }(w_{j}^{\prime}\sum_{i=1}^{n}\sigma((u_{j}^{\prime})^{\top}x_{i})\sigma_{i}) +|w_{j}^{\prime}\sum_{i=1}^{n}\sigma((u_{j}^{\prime})^{\top}x_{i})\sigma_{i} \right]\hskip 28.452756pt(\sigma(x)=\frac{x+|x|}{2})\] \[=\frac{1}{2n}\mathbb{E}_{\sigma}\left[\sum_{|w_{j}^{\prime}|<\tau }|w_{j}^{\prime}\sum_{i=1}^{n}\sigma((u_{j}^{\prime})^{\top}x_{i})\sigma_{i}|\right]\] \[\leq\frac{1}{2n}\left(\sum_{|w_{j}^{\prime}|<\tau}|w_{j}^{\prime}| \right)\mathbb{E}_{\sigma}\left[\sup_{\|u\|=1}|\sum_{i=1}^{n}\sigma(u^{\top}x _{i})\sigma_{i}|\right]\] \[\leq\frac{1}{n}\left(\sum_{|w_{j}^{\prime}|<\tau}|w_{j}^{\prime}| \right)\mathbb{E}_{\sigma}\left[\sup_{\|u\|=1}\sum_{i=1}^{n}\sigma(u^{\top}x _{i})\sigma_{i}\right]\] (Lemma _1.1_ ) \[\leq\left(\sum_{|w_{j}^{\prime}|<\tau}|w_{j}^{\prime}|\right) \underbrace{\mathbb{E}_{\sigma}\left[\frac{1}{n}\sup_{\|u\|=1}\sum_{i=1}^{n}u^ {\top}x_{i}\sigma_{i}\right]}_{\text{Empirical Rademacher complexity of a linear model}}\] (Talagrand's Lemma).

From Theorem 4.1, we can conclude that

\[R_{n}(\mathcal{H}^{\prime})\leq\sum_{|w_{j}^{\prime}|<\tau}|w_{j}^{\prime}| \frac{C}{\sqrt{n}}\leq\frac{q\tau C}{\sqrt{n}}\leq\frac{m\tau C}{\sqrt{n}}\]

when \(q\) is the number of nodes \(j\) of \(h^{\prime}\) such that \(|w_{j}^{\prime}|<\tau\). Therefore,\[R_{n}(\mathcal{H}^{\prime})\leq\frac{(2m+q)\tau C}{\sqrt{n}}\leq\frac{3m\tau C}{ \sqrt{n}}.\]

A tighter bound is given by \(\frac{(2m+q)\tau C}{\sqrt{n}}\) when \(q\) is the number of \(w_{j}^{\prime}\) that \(|w_{j}^{\prime}|<\tau\). As \(\tau\to 0\), we also have \(q\to 0\). This implies that we have an upper bound of \(\frac{2m\tau C}{\sqrt{n}}\) if \(\tau\) is small enough. When comparing this to the original bound \(\frac{2BC}{\sqrt{n}}\), we can do much better if \(\tau\ll\frac{B}{m}\). We would like to point out that our bound does not depend on the distribution \(\mathcal{D}\) because we choose a strong explanation loss

\[\phi(h,x)=\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|_{2}+\infty\cdot 1\{\| \nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|>\tau\}\]

which guarantees that \(\|\nabla_{x}h(x)-\nabla_{x}h^{\prime}(x)\|_{2}\leq\tau\) almost everywhere. We also assume that we are in a high-dimensional setting \(d\gg m\) and there exists \(x\) with a positive probability density at any partition created by \(\nabla_{x}h(x)\).

## Appendix J Algorithmic Results for Two Layer Neural Networks with a Gradient Constraint

Now that we have provided generalization bounds for the restricted class of two layer neural networks, we also present an algorithm that can identify the parameters of a two layer neural network (up to a permutation of the weights). In practice, we might solve this via our variational objective or other simpler regularized techniques. However, we also provide a theoretical result for the required amount of data (given some assumptions about the data distribution) and runtime for an algorithm to exactly recover the parameters of these networks under gradient constraints.

We again know that the gradient of two layer neural networks with ReLU activations can be written as

\[\nabla_{x}f_{w,U}(x)=\sum_{i=1}^{m}w_{i}u_{i}\cdot 1\{u_{i}^{T}x>0\},\]

where we consider \(||u_{i}||=1\). Therefore, an exact gradient constraint given of the form of pairs \((x,\nabla_{x}f(x))\) produces a system of equations.

**Proposition J.1**.: _If the values of \(u_{i}\)'s are known, we can identify the parameters \(w_{i}\) with exactly \(m\) fixed samples._

Proof.: We can select \(m\) datapoints, which each achieve value 1 for the indicator value in the gradient of the two layer neural network. This would give us \(m\) equations, which each are of the form

\[\nabla_{x}f_{w,U}(x_{i})=w_{i}u_{i}.\]

Therefore, we can easily solve for the values of \(w_{i}\), given that \(u_{i}\) is known. 

To make this more general, we now consider the case where \(u_{i}\)'s are not known but are at least linearly independent.

**Proposition J.2**.: _Let the \(u_{i}\)'s be linearly independent. Assume that each region of the data (when partitioned by the values of \(u_{i}\)) has non-trivial support \(>p\). Then, with probability \(1-\delta\), we can identify the parameters \(w_{i},u_{i}\) with \(O\left(2^{m}+\frac{m+\log(\frac{1}{\delta})}{\log(\frac{1}{1-p})}\right)\) data points and in \(O(2^{2m})\) time._

Proof.: Let us partition \(\mathcal{X}\) into regions satisfying unique values of the binary vector \((1\{u_{1}^{T}x>0\},...,1\{u_{m}^{T}x>0\})\), which by our assumption each have at least some probability mass \(p\). First, we calculate the probability that we observe one data point with an explanation from each region in this partition. This is equivalent to sampling from a multinomial distribution with probabilities \((p_{1},...,p_{2^{m}})\), where \(p_{i}\geq p,\forall i\). Then,

\[\Pr(\text{observe all regions in $n$ draws}) =1-\Pr(\exists i\text{ s.t. we do not observe region $i$})\] \[=1-2^{m}(1-p)^{n}.\]Setting this as no less than \(1-\delta\) leads to that \(n\geq\frac{m+\log\{\frac{1}{2}\}}{\log\{\frac{1}{1-p}\}}\).

Given \(O(2^{m}+\frac{m+\log\{\frac{1}{2}\}}{\log\{\frac{1}{1-p}\}})\) pairs of data and gradients, we will observe at least one pair from each region of the partition. Then, identifying the values of \(u_{i}\)'s and \(w_{i}\)'s is equivalent to identifying the datapoints that correspond to a value of the binary vector where only one indicator value is 1. These values can be identified in \(O(2^{3m})\) time; the algorithm is given in Algorithm J.1. These results demonstrate that we can indeed learn the parameters (up to a permutation) of a two layer neural network given exact gradient information. 

### Algorithm for Identifying Regions

We first note that identifying the parameters \(u_{i}\)'s and \(w_{i}\)'s of a two layer neural network is equivalent to identifying the values \(\{x_{1},...,x_{m}\}\) from the set \(\{\sum_{x\in C}x|C\in\mathcal{P}(\{x_{1},...,x_{m}\})\}\), where \(\mathcal{P}\) denotes the power set. We also assume that \(x_{1},...,x_{m}\) are linearly independent, so we cannot create \(x_{i}\) from any linear combination of \(x_{j}\)'s with \(j\neq i\). Then, we can identify the set \(\{x_{1},...,x_{m}\}\) as in Algorithm 1. This algorithm runs in \(O(2^{3m})\) time as it iterates through each point in \(M\) and computes the overlapping set \(O\) and resulting updated sum \(S\), which takes \(O(2^{2m})\) time. From the resulting set \(B\), we can exactly compute values \(u_{i}\) and \(w_{i}\) up to a permutation.

```
1:Input: We are given \(M=\{\sum_{x\in C}x|C\in\mathcal{P}(\{x_{1},...,x_{m}\})\}\), with \(\{x_{1},...,x_{m}\}\) linearly independent
2:Output: The set of basis elements \(\{x_{1},...,x_{m}\}\)
3:function
4:\(B=\{\}\), \(S=\{\}\) {Set for basis vectors and set for a current sum of at least 2 elements}
5:for\(x\in M\)do
6:if\(x\in S\)then
7: pass
8:else
9:\(B=B\cup\{x\}\)
10:if\(|B|=2\)then
11:\(S=\{y_{1}+y_{2}\}\), where \(B=\{y_{1},y_{2}\}\)
12:else
13:\(S=S\cup\{y+x|y\in S\}\) {Updating sums from adding \(x\)}
14:endif
15:\(O=B\cap S\) {Computing overlap between current basis and sums}
16:\(B=B\setminus O\) {Removing elements contained in pairwise span}
17:\(S=\{y-y_{o}|y\in S,y_{o}\in O\}\) {Updating sums \(S\) from removing set \(O\)}
18:endif
19:endfor
20:return\(B\)
21:endfunction ```

**Algorithm 1** Algorithm for identifying parameters of a two layer neural network, given exact gradient constraints

## Appendix K Additional Synthetic Experiments

We now present additional synthetic experiments that demonstrate the performance of our approach under settings with imperfect explanations and compare the benefits of using _different types_ of explanations.

### Variational Objective is Better with Noisy Gradient Explanations

Here, we present the remainder of the results from the synthetic regression task of 1, under more settings of noise \(\epsilon\) added to the gradient explanation.

Again, we observe that our method does better than that of the Lagrangian approach and the self-training method. Under high levels of noise, the Lagrangian method does poorly. On the contrary,our method is resistant to this noise and also outperforms self-training significantly in settings with limited labeled data.

### Comparing Different Types of Explanations

Here, we present synthetic results to compare using different types of explanation constraints. We focus on comparing noisy gradients as before, as well as noisy classifiers, which are used in the setting of weak supervision [30]. Here, we generate our noisy classifiers as \(h^{*}(x)+\epsilon\), where \(\epsilon\sim\mathcal{N}(0,\sigma^{2})\). We omit the results of self-training as it does not use any explanations, and we keep the supervised method as a baseline. Here, \(t=0.25\).

We observe different trends in performance as we vary the amount of noise in the noisy gradient or noisy classifier explanations. With any amount of noise and sufficient regularization (\(\lambda\)), this influences the overall performance of the methods that incorporate constraints. With few labeled data, using noisy classifiers helps outperform standard supervised learning. With a larger amount of noise, the results of the methods are consistent with the ones of the noisy gradient.

Figure 10: Comparison of MSE on regressing a two layer neural network with explanations of noisy gradients. \(m=1000,k=20,\lambda=10\). For the iterative methods, \(T=10\). Results are averaged over 5 seeds.

of labeled data, this leads to no benefits (if not worse performance of the Lagrangian approach). However, with the noisy gradient, under small amounts of noise, the restricted class of hypothesis will still capture solutions with low error. Therefore, in this case, we observe that the Lagrangian approach outperforms standard supervised learning in the case with few labeled data and matches it with sufficient labeled data. Our method outperforms or matches both methods across all settings.

We consider another noisy setting, where noise has been added to the weights of a copy of the target two layer neural network. Here, we compare how this information impacts learning from the direct outputs (noisy classifier) or the gradients (noisy gradients) of that noisy copy (Figure 12).

Figure 11: Comparison of MSE on regressing a two layer neural network with explanations as a noisy classifier (top) and noisy gradients (bottom). \(m=1000,k=20\). For the iterative methods, \(T=10\). Results are averaged over 5 seeds. \(\epsilon\) represents the variance of the noise added to the noisy classifier or noisy gradient.

Figure 12: Comparison of MSE on regressing a two layer neural network with explanations as a noisy classifier (top) and noisy gradients (bottom). \(m=1000,k=20\). For the iterative methods, \(T=10\). Results are averaged over 5 seeds. \(\epsilon\) represents the variance of the noise added to the noisy classifier or noisy gradient.

Additional Baselines

We compare against an additional baseline of a Lagrangian-regularized model + self-training on unlabeled data. We again note that this is not a standard method in practice and does not naturally fit into a theoretical framework, although we present it to compare against a method that uses both explanations and unlabeled data.

We observe that our method outperforms this baseline (Figure 13), again especially in the settings with limited labeled data. We observe that although this new method indeed satisfies constraints, when performing only a single round of self-training, it no longer satisfies these constraints as much. Thus, this supports the use of our method to perform multiple rounds of projections onto a set of EPAC models.

## Appendix M Experimental Details

For all of our synthetic and real-world experiments, we use values of \(m=1000,k=20,T=3,\tau=0,\lambda=1\), unless otherwise noted. For our synthetic experiments, we use \(d=100,\sigma^{2}=5\). Our two layer neural networks have hidden dimensions of size 10. They are trained with a learning rate of 0.01 for 50 epochs. We evaluate all networks on a (synthetic) test set of size 2000.

For our real-world data, our two layer neural networks have a hidden dimension of size 10 and are trained with a learning rate of 0.1 (YouTube) and 0.1 (Yelp) for 10 epochs. \(\lambda=0.01\) and gradient values computed by the smoothed approximation in [7 ] has \(c=1\). Test splits are used as follows from the YouTube and Yelp datasets in the WRENCH benchmark [46].

We choose the initialization of our variational algorithm \(h_{0}\) as the standard supervised model, trained using gradient descent.

Figure 13: Comparison of MSE on regressing a linear model (top left) and two layer neural network (top right) with gradient explanations. \(m=1000,k=20\). For the iterative methods, \(T=2\). Results are averaged over 5 seeds. Comparison of classification accuracy on the YouTube dataset (bottom left) and the Yelp dataset (bottom right). \(m=500,k=150\). Results are averaged over 40 seeds.

Ablations

We also perform ablation studies in the same regression setting as Section 6. We vary parameters that determine either the experimental setting or hyperparameters of our algorithms.

### Number of Explanation-annotated Data

First, we vary the value of \(k\) to illustrate the benefits of our method over the existing baselines.

We observe that our variational approach performs much better than a simple augmented Lagrangian method, which in turn does better than supervised learning with sufficiently large values of \(k\). Our approach is always better than the standard supervised approach.

We also provide results for how well these methods satisfy these explanations over varying values of \(k\)

Figure 14: Comparison of MSE on regressing a two layer neural network over different amounts of explanation-annotated data \(k\). \(m=1000\). For the iterative methods, \(T=10\). Results are averaged over 5 seeds.

### Simpler Teacher Models Can Maintain Good Performance

As noted before, we can use _simpler_ teacher models to be regularized into the explanation-constrained subspace. This can lead to overall easier optimization problems, and we synthetically verify the impacts on the overall performance. In this experimental setup, we are regressing a two layer neural network with a hidden dimension size of 100, which is much larger than in our other synthetic experiments. Here, we vary over simpler teacher models by changing their hidden dimension size.

### Number of Unlabeled Data

As a main benefit of our approach is the ability to incorporate large amounts of unlabeled data, we provide a study as we vary the amount of unlabeled data \(m\) that is available. When varying the amount of unlabeled data, we observe that the performance of self-training and our variational objective improves at similar rates.

Figure 15: Comparison of MSE on regressing a two layer neural network over simpler teacher models (hidden dimension). Here, \(k=20,m=1000,T=10\). Results are averaged over 5 seeds.

We observe no major differences as we shrink the hidden dimension size by a small amount. For significantly smaller hidden dimensions (e.g., 2 or 4), we observe a large drop in performance as these simpler teachers can no longer fit the approximate projection onto our class of EPAC models accurately. However, slightly smaller networks (e.g., 6, 8) can fit this projection as well, if not better in some cases. This is a useful finding, meaning that our teacher can be a _smaller model_ and get comparable results, showing that this simpler teacher can help with scalability without much or any drop in performance.

## 6 Conclusion

Figure 16: Comparison of MSE on regressing a two layer neural network over different values of \(m\). \(k=20,T=10\). Results are averaged over 5 seeds.

### Data Dimension

We also provide ablations as we vary the underlying data dimension \(d\). As we increase the dimension \(d\), we observe that the methods seem to achieve similar performance, due to the difficulty in modeling the high-dimensional data. Also, here gradient information is much harder to incorporate, as the input gradient itself is \(d\)-dimensional, so we do not see as much of a benefit of our approach as \(d\) grows.

Figure 17: Comparison of MSE on regressing a two layer neural network over different underlying data dimensions \(d\). \(m=1000,k=20\). For the iterative methods, \(T=10\). Results are averaged over 5 seeds.

### Hyperparameters

First, we compare the different approaches over different values of regularization (\(\lambda\)) towards satisfying the explanation constraints. Here, we compare the augmented Lagrangian approach, the self-training approach, and our variational approach.

We observe that there is not a significant trend as we change the value of \(\lambda\) across the different methods. Since we know that our explanation is perfect (our restricted EPAC class contains the target classifier), increasing the value of \(\lambda\) should help, until this constraint is met.

Next, we compare different hyperparameter settings for our variational approach. Here, we analyze trends as we vary the values of \(T\) (number of iterations) and \(\tau\) (threshold before adding hinge penalty).

Figure 19: Comparison of MSE on regressing a two layer neural network over different values of \(T\) (left) and \(\tau\) (right) in our variational approach. \(m=1000,k=20,\tau=10,T=10\), unless noted otherwise. Results are averaged over 5 seeds.

Figure 18: Comparison of MSE on regressing a two layer neural network over different values of \(\lambda\). \(m=1000,k=20\). For the iterative methods, \(T=10\). Results are averaged over 5 seeds.

We note that the value of \(\tau\) does not significantly impact the performance of our method while increasing values of \(T\) seems to generally benefit performance on this task.

## Appendix O Social Impacts

While our proposed method has the potential to improve performance and efficiency in a variety of applications, our method could introduce new biases or reinforce existing biases in the data used to train the model. For example, if our explanations constraints are poorly specified and reflect biased behavior, this could lead to inaccurate or discriminatory predictions, which could have negative impacts on individuals or groups that are already marginalized. Therefore, it is important to note that these explanation constraints must be properly analyzed and specified to exhibit the desired behavior of our model.