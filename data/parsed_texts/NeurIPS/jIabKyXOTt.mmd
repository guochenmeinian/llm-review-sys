# Sparsity-Agnostic Linear Bandits

with Adaptive Adversaries

 Tianyuan Jin

Department of Electrical and Computer Engineering

National University of Singapore

Singapore

tianyuan@nus.edu.sg

&Kyoungseok Jang

Dipartimento di Informatica

Universita degli Studi di Milano

Milano, Italy

ksajks@gmail.com

&Nicolo Cesa-Bianchi

Universita degli Studi di Milano

Politecnico di Milano

Milano, Italy

nicolo.cesa-bianchi@unimi.it

###### Abstract

We study stochastic linear bandits where, in each round, the learner receives a set of actions (i.e., feature vectors), from which it chooses an element and obtains a stochastic reward. The expected reward is a fixed but unknown linear function of the chosen action. We study _sparse_ regret bounds, that depend on the number \(S\) of non-zero coefficients in the linear reward function. Previous works focused on the case where \(S\) is known, or the action sets satisfy additional assumptions. In this work, we obtain the first sparse regret bounds that hold when \(S\) is unknown and the action sets are adversarially generated. Our techniques combine online to confidence set conversions with a novel randomized model selection approach over a hierarchy of nested confidence sets. When \(S\) is known, our analysis recovers state-of-the-art bounds for adversarial action sets. We also show that a variant of our approach, using Exp3 to dynamically select the confidence sets, can be used to improve the empirical performance of stochastic linear bandits while enjoying a regret bound with optimal dependence on the time horizon.

## 1 Introduction

\(K\)-armed bandits are a basic model of sequential decision-making in which a learner sequentially chooses which arm to pull in a set of \(K\) arms. After each pull, the learner only observes the reward returned by the chosen arm. After \(T\) pulls, the learner must obtain a total reward as close as possible to the reward obtained by always pulling the overall best arm. Linear bandits extend \(K\)-armed bandits to a setting in which arms belong to a \(d\)-dimensional feature space. In each round \(t\) of a linear bandit problem, the learner receives an action set \(\mathcal{A}_{t}\subset\mathbb{R}^{d}\) from the environment, chooses an arm \(A_{t}\in\mathcal{A}_{t}\) based on the past observations, and then receives a reward \(X_{t}\). In this work, we consider the stochastic setting in which rewards are defined by \(X_{t}=\langle\theta_{*},A_{t}\rangle+\varepsilon_{t}\), where \(\theta_{*}\in\mathbb{R}^{d}\) is a fixed latent parameter and \(\varepsilon_{t}\) is zero-mean independent noise. In linear bandits, the learner's goal is to minimize the difference between the total reward obtained by pulling in each round \(t\) the arm \(a\in\mathcal{A}_{t}\) maximizing \(\langle\theta_{*},a\rangle\) and the total reward obtained by the learner.

In stochastic linear bandits, the regret after \(T\) rounds is known to be of order \(d\sqrt{T}\) up to logarithmic factors. The linear dependence on the number \(d\) of features implies that the learner is better off by ignoring features corresponding to negligible components of the latent target vector \(\theta_{*}\). Hence,one would like to design algorithms that depend on the number \(S\ll d\) of relevant features without requiring any preliminary knowledge on \(\theta_{*}\). This is captured by the setting of sparse linear bandits, where \(\theta_{*}\) is assumed to have only \(0<S\leq d\) nonzero components.

In the sparse setting, Lattimore and Szepesvari [17, Section 24.3] show that a regret of \(\Omega\big{(}\sqrt{SdT}\big{)}\) is unavoidable for any algorithm, even with knowledge of \(S\). When \(S\) is known, this lower bound is matched (up to log factors) by an algorithm of Abbasi-Yadkori et al. [2] who, under the same assumptions and for the same algorithm, also prove an instance-dependent regret bound of \(\widetilde{O}\big{(}\frac{Sd}{\Delta}\big{)}\). Here \(\tilde{\Delta}\) is the minimum gap, over all \(T\) rounds, between the expected reward of the optimal arm and that of any suboptimal arm. In this work we focus on the sparsity-agnostic setting, i.e., when \(S\) is unknown. Fewer results are known for this case, and all of them rely on additional assumptions on the action set, or assumptions on the sparsity structure. For example, if the action set is stochastically generated, Oh et al. [22] prove a \(\widetilde{O}\big{(}S\sqrt{T}\big{)}\) sparsity-agnostic regret bound. More recently, Dai et al. [9] showed a sparsity-agnostic bound \(\widetilde{O}\big{(}S^{2}\sqrt{T}+S\sqrt{dT}\big{)}\) when the action set is fixed and equal to the unit sphere. In a model selection setting, Cutkosky et al. [8] prove a \(\widetilde{O}\big{(}S^{2}\sqrt{T}\big{)}\) sparsity-agnostic regret bound for adversarial action sets, but under an additional nestedness assumption: \((\theta_{*})_{i}\neq 0\) for \(i=1,\dots,S\). Surprisingly, no bounds improving on the \(\widetilde{O}\big{(}d\sqrt{T}\big{)}\) regret of the OFUL algorithm [1] in the sparsity-agnostic case are known that avoid additional assumptions on the sparsity structure or on the action set generation.

**Main contributions.** Here is the summary of our main contributions. All the proofs of our results can be found in the appendix.

\(\bullet\) We introduce a randomized sparsity-agnostic linear bandit algorithm, SparseLinUCB, achieving regret \(\widetilde{O}\big{(}S\sqrt{dT}\big{)}\) with no assumptions on the sparsity structure (e.g., nestedness) or on the action set (which may be controlled by an adaptive adversary). When \(S=o(\sqrt{d})\), our bound is strictly better than the OFUL bound \(\widetilde{O}\big{(}d\sqrt{T}\big{)}\).

\(\bullet\) Our analysis of SparseLinUCB simultaneously guarantees an instance-dependent regret bound \(\widetilde{O}\big{(}\max\{d^{2},S^{2}d\}/\Delta\big{)}\), where \(\Delta\) is the smallest suboptimality gap over the \(T\) rounds.

\(\bullet\) If the sparsity level is known, our algorithm recovers the optimal bound \(\widetilde{\Theta}(\sqrt{SdT})\).

\(\bullet\) We also introduce AdaLinUCB, a variant of SparseLinUCB that uses Exp3 to learn the probability distribution over a hierarchy of confidence sets in stochastic linear bandits. Unlike previous works, which only showed a \(\widetilde{O}\big{(}T^{2/3}\big{)}\) regret bound for similar approaches, AdaLinUCB has a \(\widetilde{O}\big{(}\sqrt{T}\big{)}\) regret bound. In experiments on synthetic data, AdaLinUCB performs better than OFUL.

**Technical challenges.** Recall that the arm chosen in each round by OFUL is

\[A_{t}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\langle a,\widehat{\theta }_{t}\rangle+\sqrt{\gamma_{t}}\|a\|_{V_{t-1}^{-1}}\] (1.1)

where \(\widehat{\theta}_{t}\) is the regularized least-squares estimate of \(\theta_{*}\), \(V_{t-1}=I+\sum_{s<t}A_{s}A_{s}^{\top}\) is the regularized covariance matrix of past actions, and \(\sqrt{\gamma_{t}}\) is the radius of the confidence set

\[\left\{\theta\in\mathbb{R}^{d}:\|\theta-\widehat{\theta}_{t-1}\|_{V_{t-1}}^{2} \leq\gamma_{t}\right\}\;.\] (1.2)

The squared radius \(\gamma_{t}=O(d\ln t)\) is such that \(\theta_{*}\) belongs to (1.2) with high probability simultaneously for all \(t\geq 1\). Our approach, instead, builds on the online to confidence set conversion technique of Abbasi-Yadkori et al. [2], where they show how to design a different confidence set for \(\theta_{*}\) based on the predictions of an arbitrary algorithm for online linear regression, such that the squared radius of the confidence set is roughly equal to the regret bound of the algorithm. Using the algorithm SeqSEW for sparse online linear regression [13], whose regret bound is \(O(S\log T)\), they obtain the optimal regret \(\widetilde{O}\big{(}\sqrt{SdT}\big{)}\) for sparse linear bandits. Unfortunately, this result requires knowing \(S\) to properly set the radius of the confidence set. Our strategy SparseLinUCB (Algorithm 1) bypasses this problem by running the online to confidence set conversion technique over a hierarchy of nested confidence sets with radii \(\alpha_{i}=2^{i}\log T\) for \(i=1,\dots,n=\Theta(\log d)\). The framework of Abbasi-Yadkori et al. [2] guarantees that, for any sparsity value \(S\in[d]\), there is a critical radius \(\alpha_{o}=O(S\log T)\) such that, with high probability, \(\theta_{*}\) lies in the set with radius \(\alpha_{i}\) for all \(i\geq o\). SparseLinUCB randomizes the choice of the index \(i\) of the confidence radius \(\alpha_{i}\), used for selecting the action at time \(t\). Ifthe random index \(I_{t}\) is such that \(I_{t}\geq o\), then we can bound the regret incurred at step \(t\) using standard techniques [1, 2]. By choosing \(\mathbb{P}(I_{t}=i)\) proportional to \(2^{-i}\), we make sure that larger confidence sets (delivering suboptimal regret bounds) are chosen with exponentially small probability. If \(I_{t}<o\), then \(\theta_{*}\) is not guaranteed to lie in the confidence set of radius \(\alpha_{I_{t}}\) with high probability. Our main technical contribution is to show that the regret summed over these bad rounds is bounded by \(\widetilde{O}\big{(}\sqrt{SdT/Q}\big{)}\), where \(Q=\mathbb{P}(I_{t}\geq o)\). The proof of this bound requires showing that the regret in a bad round \(t\) (when \(I_{t}<o\)) can be bounded by \(\sqrt{\alpha_{o}}\|A_{t}^{o}\|_{V_{t-1}^{-1}}\). Proving that \(\|A_{t}^{o}\|_{V_{t-1}^{-1}}\) shrinks fast enough uses the fact that \(\det V_{t}\) grows fast enough due to the exploration in the good rounds \(t\) (when \(I_{t}\geq o\)). This is done by a carefully designed peeling technique that partitions \([T]\) in blocks based on the value of \(\det V_{t}\).

To extend our analysis of SparseLinUCB and obtain instance-dependent regret bound, we apply the techniques of Abbasi-Yadkori et al. [1] to show that the regret over the good rounds is bounded by \(\widetilde{O}\big{(}d^{2}/\Delta\big{)}\). The regret over a bad round \(t\) is controlled by \((\alpha_{o}/\Delta)\|A_{t}^{o}\|_{V_{t-1}^{-1}}^{2}\) and--using techniques similar to the instance-independent analysis--we bound the regret summed over all bad rounds with \(\widetilde{O}\big{(}Sd/(Q\Delta)\big{)}\).

Given that SparseLinUCB uses a fixed probability of order \(2^{-i}\) to choose its confidence radius \(\alpha_{i}\), it is tempting to explore adaptive probability assignments, that increase the probability of a confidence set proportionally to the rewards obtained by the actions that were selected based on that set. Algorithm AdaLinUCB (see Algorithm 2) is a variant of SparseLinUCB using Exp3 [4] to assign probabilities to confidence sets. The analysis of AdaLinUCB combines--in a non-trivial way--the analysis of Exp3 (including a forced exploration term \(q\)) with that of SparseLinUCB. Although the resulting regret bound does not improve on OFUL, our algorithm provides a new principled solution to the problem of tuning the radius in (1.1). Experiments show that SparseLinUCB can perform better than OFUL.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Reference & \begin{tabular}{c} Sparsity \\ Agnostic \\ \end{tabular} & 
\begin{tabular}{c} Adaptive \\ Adversary \\ \end{tabular} & Expected Regret & Assumptions \\ \hline Abbasi-Yadkori et al. [1] & ✓ & ✓ & \(\widetilde{O}(\min\left\{d\sqrt{T},d^{2}/\Delta\right\})\) & - \\ \hline Abbasi-Yadkori et al. [2] & ✗ & ✓ & \(\widetilde{O}(\min\{\sqrt{SdT},dS/\Delta\})\) & - \\ \hline Pacchiano et al. [23, 24] & ✓ & ✗ & \(\widetilde{O}(S^{2}\sqrt{T})\) & Nested, i.i.d. actions \\ \hline Cutkosky et al. [8] & ✓ & ✓ & \(\widetilde{O}(S^{2}\sqrt{T})\) & Nested \\ \hline Pacchiano et al. [25] & ✓ & ✓ & \(\widetilde{O}(S^{2}\sqrt{T})\) & Nested \\  & ✓ & ✗ & \(\widetilde{O}(S^{2}d^{2}/\Delta)\) & Nested, i.i.d. actions \\ \hline Lattimore et al. [18] & ✗ & ✗ & \(\widetilde{O}(S\sqrt{T})\) & \(\varepsilon_{t}\in[-1,1]\) \\ \hline Sivakumar et al. [26] & ✗ & ✓ & \(\widetilde{O}(S\sqrt{T})\) & Smoothed adversary \\ \hline Hao et al. [15] & ✗ & ✗ & \(\widetilde{O}(\sqrt{ST})\) & \(\widetilde{\text{{\small{Actions}}}}\) set spans \(\mathbb{R}^{d}\) \\ \hline Oh et al. [22] & ✓ & ✗ & \(\widetilde{O}(S\sqrt{T})\) & Compatibility \\ \hline Dai et al. [9] & ✓ & ✗ & \(\widetilde{O}(S^{2}\sqrt{T}+S\sqrt{dT})\) & Action set is unit sphere \\ \hline Lower bound [17] & ✗ & ✓ & \(\Omega\big{(}\sqrt{SdT}\big{)}\) & - \\ \hline
**This paper** & ✓ & ✓ & \(\widetilde{O}\big{(}\min\left\{S\sqrt{dT},\frac{1}{\Delta}\max\{d^{2},S^{2}d\} \right\}\big{)}\) & - \\
**This paper** & ✗ & ✓ & \(\widetilde{O}\big{(}\sqrt{SdT}\big{)}\) & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with other sparse linear bandit works. \(S\in[d]\) is the sparsity level and \(\Delta\) is the suboptimality gap (3.3). The nested assumption refers to \((\theta_{*})_{i}\neq 0\) for \(i=1,\ldots,S\). The minimum signal and the compatibility condition refer to assumptions on the distribution of the action set and on the smallest value of the non-zero elements in \(\theta_{*}\). Smoothed adversary refers to adversarially selected action sets with added Gaussian noise. The regret bounds listed in [1, 2, 23, 8, 25, 18, 9] are high-probability bounds: with high probability, the regret is of the same order as the bound in the table.

### Additional related work

**Sparse linear bandits.** With the goal of obtaining sparsity-agnostic regret bounds, different types of assumptions on the action set have been considered in the past. Starting from the \(\widetilde{O}(S\sqrt{T})\) regret upper bound of [18], where the action set is assumed fixed and equal to the hypercube, some works considered stochastic action sets and proved regret bounds depending on spectral parameters of the action distribution, such as the minimum eigenvalue of the covariance matrix [9; 15; 16; 20]. Others assumed a stochastic action set with strong properties, such as compatibility conditions or margin conditions [3; 6; 7; 19; 22]. As far as we know, there has been no research on adaptive adversarial action sets after [2].

**Model selection.** Sparse linear bandits can be naturally viewed as a bandit model selection problem. For example, Ghosh et al. [14] establish a regret bound of \(\widetilde{O}(\sqrt{ST}+d^{2}/\alpha^{4.65})\) for a fixed action set, where \(\alpha\) is the minimum absolute value of the nonzero components of \(\theta_{*}\). Quite a bit of work has been devoted to sparse regret bounds in the nested setting. With i.i.d. and fixed-size actions sets, Foster et al. [12] achieve a regret bound of order \(\widetilde{O}\big{(}S^{1/3}T^{2/3}/\gamma^{3}\big{)}\) in the nested setting, where \(\gamma\) is the smallest eigenvalue of the covariance matrix of \(\mathcal{A}_{t}\). Under the same assumption on the action set, Pacchiano et al. [24; 23] obtain a regret bound of \(\widetilde{O}(S^{2}\sqrt{T})\). For adversarial action sets, Cutkosky et al. [8] obtain \(\widetilde{O}(S^{2}\sqrt{T})\) in the nested setting. When actions are sampled i.i.d., Cutkosky et al. [8] and Pacchiano et al. [25] obtain a regret bound of \(\widetilde{O}(S^{2}\sqrt{T})\) for nested settings. They also obtain simultaneous instance-dependent bounds, in particular, Pacchiano et al. [25] achieve \(\widetilde{O}\big{(}(Sd)^{2}/\Delta\big{)}\). Compared to the instance-dependent regret bound, our results are more general, as we allow the action set to be adaptively chosen by an adversary and do not require the nested assumption.

**Parameter tuning.** Although the theoretical analysis of OFUL only holds for \(\gamma_{t}=O(d\log t)\), smaller choices of the radius in (1.2) are known to perform better in practice. Our design of SparseLinUCB and AdaLinUCB borrows ideas from the parameter tuning setting, which is typically addressed using a set of base algorithms and a randomized master algorithm that adaptively changes the probability of selecting each base algorithm [21; 23; 24]. In particular, AdaLinUCB builds on [11], where they show that running Exp3 as the master algorithm over instances of OFUL with different radii has a better empirical performance than Thompson Sampling and UCB. Yet, they only show a regret bound of \(\widetilde{O}\big{(}T^{2/3}\big{)}\) when the action set is drawn i.i.d. in each round (they also prove a bound of order \(\sqrt{T}\), but only under additional assumptions on the best model). This is consistent with the results of Pacchiano et al. [24], who also obtained a regret of the same order using Exp3 as master algorithm.

## 2 Problem definition

In linear bandits, a learner and an adversary interact over \(T\) rounds. In each round \(t=1,\ldots,T\):

1. The adversary chooses an arm set \(\mathcal{A}_{t}\subset\mathbb{R}^{d}\);
2. The learner choose an arm \(A_{t}\in\mathcal{A}_{t}\);
3. The learner obtains a reward \(X_{t}\).

We assume the adversary is adaptive, i.e., \(\mathcal{A}_{t}\) can depend in an arbitrary way on the (possibly randomized) past choices of the learner. The reward in each round \(t\) satisfies

\[X_{t}=\langle A_{t},\theta_{*}\rangle+\varepsilon_{t}\.\] (2.1)

Here \(\theta_{*}\in\mathbb{R}^{d}\) is a fixed and unknown target vector and \(\{\varepsilon_{t}\}_{t\in[T]}\) are independent conditionally 1-subgaussian random variables. We also assume \(\|\theta_{*}\|_{2}\leq 1\) and \(\|a\|_{2}\leq 1\) for all \(a\in\mathcal{A}_{t}\) and all \(t\in[T]\).1 The regret of a strategy over \(T\) rounds is defined as the difference between the reward obtained by the optimal policy, always choosing the best arm in \(\mathcal{A}_{t}\), and the reward obtained by the strategy choosing arm \(A_{t}\in\mathcal{A}_{t}\) for \(t\in[T]\),

Footnote 1: The choice of the constant \(1\) is arbitrary. Choosing different constants would scale our bounds similarly to the scaling of the bounds in [2].

\[R_{T}=\sum_{t=1}^{T}\max_{a\in\mathcal{A}_{t}}\langle a,\theta_{*}\rangle-\sum _{t=1}^{T}\langle A_{t},\theta_{*}\rangle.\]In the sparse setting, we would like to devise strategies whose regret depends on

\[S=\|\theta_{*}\|_{0}=\sum_{i=1}^{d}\mathbb{1}\{\theta_{i}\neq 0\}\]

corresponding to the number of nonzero components of \(\theta_{*}\).

### Online to confidence set conversions

Establishing a confidence set including the target vector with high probability is at the core of linear bandit algorithms, and our approach for designing a sparsity-agnostic algorithm is based on a result by Abbasi-Yadkori et al. [2]. They show how to construct a confidence set for OFUL [1] based on the predictions of a generic algorithm for online linear regression, a sequential decision-making setting defined as follows. For \(t=1,\dots,T\):

1. The adversary privately chooses input \(A_{t}\in\mathbb{R}^{d}\) and outcome \(X_{t}\in\mathbb{R}\);
2. The learner observes \(A_{t}\) and chooses prediction \(\widehat{X}_{t}\in\mathbb{R}\);
3. The adversary reveals \(X_{t}\) and the learner suffers loss \((\widehat{X}_{t}-X_{t})^{2}\).

The learner's goal in online linear regression is to minimize the following notion of regret against any comparator \(\theta\in\mathbb{R}^{d}\)

\[\rho_{T}(\theta)=\sum_{t=1}^{T}(X_{t}-\widehat{X}_{t})^{2}-\sum_{t=1}^{T}(X_{t }-\langle A_{t},\theta\rangle)^{2}.\]

The confidence set proposed by Abbasi-Yadkori et al. [2] is established by the following result.

**Lemma 2.1** (Abbasi-Yadkori et al. [2, Corollary 2]).: _Let \(\delta\in(0,1/4]\) and \(\|\theta_{*}\|_{2}\leq 1\). Assume a sequence \(\big{\{}(A_{t},X_{t})\}_{t\in[T]}\), where \(X_{t}\) satisfies (2.1) for all \(t\in[T]\), is fed to an online linear regression algorithm \(\mathcal{B}\) generating predictions \(\big{\{}\widehat{X}_{t}\big{\}}_{t\in[T]}\). Then \(\mathbb{P}\left(\exists t\in[T]\,:\,\theta_{*}\notin\mathcal{C}_{t}\right)\leq\delta\), where_

\[\mathcal{C}_{t} =\left\{\theta\in\mathbb{R}^{d}:\|\theta\|_{2}^{2}+\sum_{s=1}^{t- 1}(\widehat{X}_{s}-\langle A_{s},\theta\rangle)^{2}\leq\gamma(\delta,\theta_{* })\right\}\] (2.2) \[\gamma(\delta,\theta_{*}) =2+2B_{T}(\theta_{*})+32\log\left(\frac{\sqrt{8}+\sqrt{1+B_{T}( \theta_{*})}}{\delta}\right)\]

_and \(B_{T}(\theta_{*})\) is an upper bound on the regret \(\rho_{T}(\theta_{*})\) of \(\mathcal{B}\). When understood from the context, we will abuse the notation and denote the best radius in hindsight by \(\gamma(\delta):=\gamma(\delta,\theta_{*})\) and the regret bound by \(B_{T}\)._

Gerchinovitz [13] designed an algorithm, SeqSEW, for _sparse_ linear regression that bounds \(\rho_{T}(\theta)\) in terms of \(\|\theta\|_{0}\) simultaneously for all comparators \(\theta\in\mathbb{R}^{d}\). Below here, we state his bound in the formulation of Lattimore and Szepesvari [17].

**Lemma 2.2** (Lattimore and Szepesvari [17, Theorem 23.6]).: _Assume \(\max_{t\in[T]}\|A_{t}\|_{2}\leq 1\) and \(\max_{t\in[T]}|X_{t}|\leq 1\). There exists a universal constant \(c\) such that algorithm SeqSEW achieves, for any \(\theta\in\mathbb{R}^{d}\),_

\[\rho_{T}(\theta)\leq B_{T}(\theta):=c\|\theta\|_{0}\left\{\log(e+T^{1/2})+C_{T }\log\left(1+\frac{\|\theta\|_{1}}{\|\theta\|_{0}}\right)\right\}\]

_where \(C_{T}=2+\log_{2}\log(e+T^{1/2})\)._

Using the confidence set (2.2) with \(\mathcal{B}\) set to SeqSEW, Abbasi-Yadkori et al. [2] achieved the minimax optimal regret bound of \(\widetilde{O}(\sqrt{SdT})\). However, to construct \(\mathcal{C}_{t}\), the learner must know \(\gamma(\delta)\), which depends on the unknown sparsity level \(S=\|\theta_{*}\|_{0}\) through \(B_{T}\).

## 3 A multi-level sparse linear bandit algorithm

In this section, we introduce our main algorithm, SparseLinUCB, whose pseudo-code is shown in Algorithm 1. The algorithm, which runs SeqSEW as base algorithm \(\mathcal{B}\), uses a hierarchy of confidence sets of increasing radius. In each round \(t=1,\ldots,T\), after receiving the action set \(\mathcal{A}_{t}\), the algorithm draws the index \(I_{t}\) of the confidence set for time \(t\) by sampling from the distribution \(\mathbf{q}\in\Delta_{n}:=\{(q_{1},\ldots,q_{n})\in[0,1]^{n}:\sum_{i=1}^{n}q_{i }=1\}\). Then the algorithm plays the action \(A_{t}\) using the confidence set \(\mathcal{C}_{t}^{I_{t}}:=\{\theta\in\mathbb{R}^{d}:\|\theta-\widehat{\theta}_ {t-1}\|_{V_{t-1}}^{2}\leq 2^{I_{t}}\log T\}\) (where a larger \(I_{t}\) implies a larger radius, and thus more exploration). Following the online to confidence set approach, upon receiving the reward \(X_{t}\), the algorithm feeds the pair \((A_{t},X_{t})\) to SeqSEW and uses the prediction \(\widehat{X}_{t}\) to update the regularized least squares estimate \(\widehat{\theta}_{t}\).

Let \(\alpha_{i}=2^{i}\log T\) for all \(i\in\mathbb{N}\) and set \(n\in\mathbb{N}\) as

\[n=\left\lceil\log_{2}\frac{\max_{\theta\in\mathbb{R}^{d}:\|\theta\|_{2}\leq 1 }\gamma(1/T,\theta)}{\log T}\right\rceil\] (3.1)

where \(\gamma(\delta,\theta)\) is defined in Lemma 2.1 for \(\mathcal{B}=\texttt{SeqSEW}\).

One can check that \(n=\Theta(\log d)\) (when \(\|\theta\|_{0}=d\)), which gives \(\alpha_{n}=\Theta(d\log T)\). Our bounds depend on the following quantity, which defines the index of the smallest "safe" confidence set (i.e., the smallest \(i\in[n]\) such that \(\theta_{*}\in\mathcal{C}_{t}^{i}\) for all \(t\in[T]\)),

\[o:=\operatorname*{argmin}_{i\in[n]}\left\{\gamma(1/T)\leq\alpha_{i}\right\}\] (3.2)

The choice of our confidence set (Line 4 in Algorithm 1) is justified by the following result, which implies that \(o\) is safe.

**Lemma 3.1**.: _For \(\mathcal{C}_{t}\) defined in (2.2), we have that \(\mathcal{C}_{t}\subseteq\mathcal{C}_{t}^{o}\) for all \(t\in[T]\)._

As we use SeqSEW as base algorithm \(\mathcal{B}\), \(\alpha_{o}=O(S\log T)\). Our main result is an upper bound on the regret of SparseLinUCB.

**Theorem 3.2**.: _The expected regret of SparseLinUCB run with the number of models \(n\) in (3.1) and a distribution \(\boldsymbol{q}=\{q_{s}\}_{s\in[n]}\) satisfies_

\[\mathbb{E}[R_{T}]=O\left((\log T)\sum_{s\geq o}\sqrt{d2^{s}Tq_{s}}+(\log T) \sqrt{STd/Q}\right)\]

_where \(Q=\sum_{s\geq o}q_{s}\)._

If the sparsity level \(S\) is indeed known, then \(o\) in (3.2) can be computed and we get the following bound, which is tight up to log factors [17].

**Corollary 3.3**.: _Assume that the sparsity level \(S\) is known and choose the number of models \(n>o\) and the distribution \(\{q_{s}\}_{s\in[n]}\) with \(q_{o}=1\), where \(o\) is set as in (3.2). Then, the expected regret of \(\mathtt{SparseLinUCB}\) is \(\mathbb{E}[R_{T}]=O\big{(}\sqrt{SdT}\log T\big{)}\)._

**An instance-dependent bound.**\(\mathtt{SparseLinUCB}\) also enjoys an instance-dependent regret bound comparable to that of OFUL. Let \(\Delta\) be the minimum gap between the optimal arm and any suboptimal arms over all rounds,

\[\Delta=\min_{t\in[T]}\min_{a\in\mathcal{A}_{t}\setminus A_{t}^{*}}\langle A_{t }^{*}-a,\theta_{*}\rangle,\] (3.3)

where \(A_{t}^{*}=\max_{a\in\mathcal{A}_{t}}\langle a,\theta_{*}\rangle\) is the optimal arm for round \(t\).

**Theorem 3.4**.: _The expected regret of \(\mathtt{SparseLinUCB}\) run with the number of models \(n\) in (3.1), a distribution \(\textbf{q}=\{q_{s}\}_{s\in[n]}\) and using \(\mathtt{SeqSEW}\) as base algorithm satisfies_

\[\mathbb{E}[R_{T}]=O\left(\frac{(dS/Q)+d^{2}}{\Delta}(\log T)^{2}\right)\]

_where \(Q=\sum_{s\geq o}q_{s}\)._

**Sparsity-agnostic tuning of randomization.** Next, we look at a specific choice of **q**. Fix \(C\geq 1\) and let

\[q_{s}=\left\{\begin{array}{cc}C^{2}2^{-s}&\text{if }C^{2}2^{-s}<1\\ \kappa&\text{otherwise,}\end{array}\right.\] (3.4)

where \(\kappa>0\) is chosen so to normalize the probabilities. It is easy to verify that for any \(C\geq 1\),

\[\sum_{s\in[n]}\mathbbm{1}\big{\{}C^{2}2^{-s}<1\big{\}}q_{s}\leq 1\]

implying that \(\kappa\) can be chosen in \([0,1]\). Combining Theorem 3.2 and 3.4, we obtain the following corollary providing a hybrid distribution-free and distribution-dependent bound.

**Corollary 3.5**.: _Pick any \(C\geq 1\). Let the number of models \(n\) as in (3.1) and \(\textbf{q}=\{q_{s}\}_{s\in[n]}\) be chosen as in (3.4). Then the expected regret of \(\mathtt{SparseLinUCB}\) is_

\[\mathbb{E}[R_{T}]=\widetilde{O}\left(\min\left\{\max\big{\{}C,S/C\big{\}} \sqrt{dT},\,\frac{\max\big{\{}d^{2},S^{2}d/C^{2}\big{\}}}{\Delta}\right\}\right)\]

For \(C=1\) the above bound is \(\widetilde{O}(S\sqrt{dT})\), which is tight up to the factor \(\sqrt{S}\) due to the lower bound of \(\Omega(\sqrt{SdT})\)[17]. However, as mentioned in Lattimore and Szepesvari [17, Section 23.5], no algorithm can enjoy the regret of \(\widetilde{O}(\sqrt{SdT})\) simultaneously for all possible sparsity levels \(S\). While our worst-case regret bound improves with a smaller \(S\), the problem-dependent regret bound scales at least as \((d^{2}/\Delta)\log T\), which is independent of \(S\). This raises an interesting question: could the problem-dependent bound also benefit from sparsity? Even with a very small probability \(p\) of choosing radius \(\alpha_{n}\), the expected number of steps using \(\alpha_{n}\) would be \(pT\). The results in [2] demonstrate that running the OFUL algorithm with \(\alpha_{n}\) over \(pT\) steps results in a regret of \(\widetilde{O}(d^{2}/\Delta)\). One simple way is to decrease the frequency of selecting radius \(\alpha_{n}\). However, selecting \(\alpha_{n}\) less than \(d^{2}/\Delta^{2}\) times may prevent the algorithm from obtaining a good enough estimate of \(\theta_{*}\) in certain settings.

**Remark 3.6**.: _At first glance, it may seem straightforward to select \(C\) in Corollary 3.5, as setting \(C=\sqrt{d}\) yields a regret of \(\widetilde{O}(d^{2}/\Delta)\) without apparent trade-offs. However, the trade-off lies in balancing the instance-dependent and worst-case regret bounds. Opting for \(C=d\) indeed yields an instance-dependent bound of \(\widetilde{O}(d^{2}/\Delta)\). However, this comes at the expense of the worst-case bound, which remains \(\widetilde{O}(d\sqrt{T})\), negating any advantages derived from the sparsity assumption \(S\ll d\)._

If the sparsity level \(S\) is indeed known, then \(o\) in (3.2) can be computed and we get the following bound.

**Corollary 3.7**.: _Assume that the sparsity level \(S\) is known and choose \(\{q_{s}\}_{s\in[n]}\) with \(q_{o}=1\), where \(o\) is set as in (3.2). Then, the expected regret of \(\mathtt{SparseLinUCB}\) is \(\mathbb{E}[R_{T}]=\widetilde{O}\big{(}\frac{Sd}{\Delta}\big{)}\)._

We note that by setting \(q_{o}=1\) in Theorem 3.4, the regret bound becomes \(\widetilde{O}(d^{2}/\Delta)\). This result, as detailed in Theorem 3.4, arises from the parameter \(q_{n}>0\). However, in this case, \(q_{n}=0\), which allows us to achieve a more favorable regret bound.

```
1:Input:\(T\in\mathbb{N}\), \(\eta>0\), \(q\in(0,1]\)
2:Initialization: Let \(S_{i,0}=0\) for all \(i\in[n]\), \(V_{0}=I\), \(\widehat{\theta}_{0}=(0,\ldots,0)\)
3:for\(t=1,2,\cdots,T\)do
4: Receive action set \(\mathcal{A}_{t}\) and draw a Bernoulli random variable \(Z_{t}\) with \(\mathbb{P}(Z_{t}=1)=q\)
5:if\(Z_{t}=1\)then
6: Choose optimistic action \(A_{t}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\left(\langle a,\widehat{ \theta}_{t-1}\rangle+\|a\|_{V_{t-1}^{-1}}\sqrt{2^{n}\log T}\right)\)
7: Receive reward \(X_{t}\);
8:else
9: Draw \(I_{t}\) from the distribution \(P_{t,i}=\dfrac{\exp\left(\eta S_{t-1,i}\right)}{\sum_{j=1}^{n}\exp\left(\eta S _{t-1,j}\right)}\) for \(i\in[n]\);
10: Choose action \(A_{t}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\left(\langle a,\widehat{ \theta}_{t-1}\rangle+\|a\|_{V_{t-1}^{-1}}\sqrt{2^{I_{t}}\log T}\right)\)
11: Receive reward \(X_{t}\);
12: Compute \(S_{t,j}=S_{t-1,j}-\dfrac{\mathds{1}\{I_{t}=j\}(2-X_{t})/4}{P_{t,j}}\) for \(j\in[n]\);
13:endif
14:\(V_{t}=V_{t-1}+A_{t}A_{t}^{\top}\);
15: Feed \((A_{t},X_{t})\) to SeqSEW and obtain prediction \(\widehat{X}_{t}\);
16: Compute regularized least squares estimate \(\widehat{\theta}_{t}=\operatorname*{argmin}_{\theta\in\mathbb{R}^{d}}\left( \|\theta\|_{2}^{2}+\sum_{s=1}^{t}\left(\widehat{X}_{s}-\langle\theta,A_{s} \rangle\right)^{2}\right)\);
17:endfor ```

**Algorithm 2** AdaLinUCB

## 4 Adaptive model selection for stochastic linear bandits

SparseLinUCB is also designed to handle adaptive adversarial action sets. A crucial parameter of SparseLinUCB is \(\{q_{s}\}_{s\in[n]}\), the distribution from which the radius of the confidence set is drawn. It is a natural question whether there exists an algorithm that adaptively updates this distribution based on the observed rewards. In this section we introduce AdaLinUCB (Algorithm 2), which runs Exp3 to dynamically adjust the distribution used by SparseLinUCB.

AdaLinUCB takes as input a forced exploration term \(q\) and the learning rate \(\eta\) for Exp3. Similarly to SparseLinUCB, AdaLinUCB designs confidence sets of various radii, but its selection method differs in two aspects. First, with probability \(q\), the algorithm performs exploration based on the confidence set with the largest radius. With probability \(1-q\), the algorithm instead draws the action based on Exp3. The distribution \(P_{t}\) used by Exp3 at round \(t\) is based on exponential weights applied to the total estimated loss, denoted by \(S_{t}\) (for technical reasons, we translate losses into rewards). The algorithm then draws \(I_{i}\) from \(P_{t}\) and selects the action \(A_{t}\) based on the confidence set with radius \(2^{I_{t}}\log T\). Finally, reward \(X_{t}\) is observed and the pair \((A_{t},X_{t})\) is fed to SeqSEW. The prediction \(\widehat{X}_{t}\) returned by SeqSEW is used to update the regularized least squares estimate \(\widehat{\theta}_{t}\).

The following theorem states the theoretical regret upper bound of AdaLinUCB.

**Theorem 4.1**.: _If the random independent noise \(\varepsilon_{t}\) in (2.1) satisfies \(\varepsilon_{t}\in[-1,1]\) for all \(t\in[T]\), then the regret of AdaLinUCB run with \(\eta=\sqrt{(\log n)/(Tn)}\) for \(n\) in (3.1) and \(q\in(0,1]\) satisfies_

\[\mathbb{E}[R_{T}] \leq\left(\sqrt{8\alpha_{n}q}+4\sqrt{(2\alpha_{\alpha})/q}\right) \sqrt{dT\log\left(1+\frac{TL^{2}}{d}\right)+1}+O\big{(}\sqrt{nT\log n}\big{)}\] \[=\widetilde{O}\left(\max\left\{\sqrt{dq},\sqrt{S/q}\right\}\sqrt {dT}\right)\.\]

Although AdaLinUCB dynamically adjusts the distribution used by SparseLinUCB and may achieve better empirical performance, its regret bound is no better than that of SparseLinUCB. The issue is that the action chosen by AdaLinUCB in Line 10 does not ensure enough exploration to control the regret. Consequently, the algorithm needs to choose the optimistic action in Line 6 with constant probability \(q\). SparseLinUCB has a similar parameter, \(Q\), that bounds from the above the probability of choosing the optimistic action. The key difference is that \(Q\) can be optimized for an unknown \(S\) by carefully selecting the distribution \(\mathbf{q}=\{q_{s}\}_{s\geq 1}\), whereas the parameter \(q\) does not provide a similar flexibility.

## 5 Model selection experiments

In this section we describe some experiments we performed on synthetic data to verify whether AdaLinUCB could outperform OFUL in a model selection task. We also test the empirical performance of SparseLinUCB on the same data (additional details on all the algorithms and the experimental setting are in Appendix E). The data for our model selection experiments are generated using targets \(\theta_{s}\) with different sparsity levels, as we know that sparsity affects the radius of the optimal confidence set. On the other hand, since no efficient implementation of SeqSEw is known (17, Section 23.5), we cannot implement the online to confidence set approach as described in [2] to capture sparsity. Instead, we run SparseLinUCB and AdaLinUCB with \(\widehat{X}_{t}=X_{t}\) for all \(t\in[T]\), which--due to the form of our confidence sets--amounts to running the algorithms over multiple instances of OFUL with different choices of radius \(\alpha_{i}\) for \(i\in[n]\).

We run SparseLinUCB and AdaLinUCB with \(\alpha_{i}=\alpha_{i,t}=2^{i}\log t\) (a mildly time-dependent choice) for \(i=0,1,\cdots,\log_{2}d\). We also include \(\alpha_{0}=0\) corresponding to the greedy strategy \(A_{t}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\langle a,\widehat{\theta }_{t-1}\rangle\). The suffix _Unif indicates \(\{q_{s}\}_{s\in[n]}\) set to \((\frac{1}{n},\cdots,\frac{1}{n})\). The suffix _Theory indicates \(q_{s}=\Theta(2^{-s})\) for \(s=0,\ldots,n\) as prescribed by (3.4). Finally, we included SparseLinUCB_Known using \(q_{i}=1\{i=o\}\) to test the performance when the optimal index \(o\) (for the given \(S\)) is known in advance (see Corollary 3.3). We run our experiments with a fixed set of random actions, \(\mathcal{A}_{t}=\mathcal{A}\) for all \(t\in[T]\), where \(|\mathcal{A}|=30\) and \(\mathcal{A}\) is a set of vectors drawn i.i.d. from the unit sphere in \(\mathbb{F}^{16}\). The target vector \(\theta_{s}\) is a \(S\)-sparse (\(S=1,2,4,8,16\)) vector whose non-zero coordinates are drawn from the unit sphere in \(\mathbb{R}^{S}\). The noise \(\varepsilon_{t}\) is drawn i.i.d. from the uniform distribution over \([-1,1]\). Each curve is an average over \(20\) repetitions with \(T=10^{4}\) where, in each repetition, we draw fresh instances of \(\mathcal{A}\) and \(\theta_{*}\).

As our implementations are not sparsity-aware, we cannot expect the regret to strongly depend on the sparsity level. Indeed, only the regret of SparseLinUCB_Known (which is tuned to the sparsity \(S\)) is significantly affected by sparsity. The theory-driven choice of \(\{q_{s}\}_{s\in[n]}\) (SparseLinUCB_Theory) performs better than the uniform assignment (SparseLinUCB_Unif), and is in the same ballpark as OFUL. On the other hand, AdaLinUCB_Unif and AdaLinUCB_Theory outperform all the competitors, including OFUL. This provides evidence that using Exp3 for adaptive model selection may significantly boost the empirical performance of stochastic linear bandits.

## 6 Limitations and open problems

Unlike previous works, we prove sparsity-agnostic regret bounds with no assumptions on the action sets or on \(\theta_{*}\) (other than boundedness of \(\|\theta_{*}\|\) and \(\|a\|\) for \(a\in\mathcal{A}_{t}\), which are rather standard assumptions). For AdaLinUCB, however, we do require boundedness of the noise \(\varepsilon_{t}\) (instead of just subgaussianity). We conjecture this requirement could be dropped at the expense of a further \(\log T\) factor in the regret. Finally, for efficiency reasons our implementations are not designed to capture sparsity. Hence our experiments are limited to testing the impact of model selection.

Our work leaves some open problems:

1. Proving a lower bound on the regret of sparse linear bandits when the sparsity level is unknown to the learner would be important. Citing again (17, Section 23.5), no algorithm can enjoy regret \(\widetilde{O}(\sqrt{SdT})\) simultaneously for all sparsity levels \(S\). However, we do not know whether the known lower bound \(\Omega(\sqrt{SdT})\) can be strengthened to \(\Omega(S\sqrt{dT})\) in the agnostic case.
2. Our instance-dependent regret bound is of order \(\widetilde{O}\big{(}\max\{S^{2},d\}\frac{d}{\Delta}\big{)}\). It would be interesting to prove an upper bound that improves on the factor \(d^{2}/\Delta\), or a lower bound showing that \(d^{2}/\Delta\) cannot be improved on.
3. Our bound on the regret of AdaLinUCB looks pessimistic due to the presence of the constant exploration probability \(q\). It would be interesting to prove a bound that more closely reflects the good empirical performance of this algorithm.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their helpful comments. This research is supported by the MUR PRIN grant 2022EKNE5K (Learning in Markets and Society), the FAIR (Future Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRR-PE-AI scheme, the EU Horizon CL4-2022-HUMAN-02 research, innovation action under grant agreement 101120237, project ELIAS (European Lighthouse of AI for Sustainability), a Singapore Ministry of Education AcRF Tier 2 grant (A-8000423-00-00), and the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG-PhD/2021-01-004[T]).

In particular, NCB and KJ acknowledge the financial support from the MUR PRIN grant, the FAIR project, and the ELIAS project. TJ is funded by a Singapore Ministry of Education AcRF Tier 2 grant (A-8000423-00-00) and the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG-PhD/202101-004[T]).

## References

* Abbasi-Yadkori et al. [2011] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Abbasi-Yadkori et al. [2012] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and application to sparse stochastic bandits. In _Artificial Intelligence and Statistics_, pages 1-9. PMLR, 2012.
* Ariu et al. [2022] Kaito Ariu, Kenshi Abe, and Alexandre Proutiere. Thresholded lasso bandit. In _International Conference on Machine Learning_, pages 878-928. PMLR, 2022.
* Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* Bubeck et al. [2012] Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* Chakraborty et al. [2023] Sunrit Chakraborty, Saptarshi Roy, and Ambuj Tewari. Thompson sampling for high-dimensional sparse linear contextual bandits. In _International Conference on Machine Learning_, pages 3979-4008. PMLR, 2023.

Figure 1: Experimental results with different sparsity levels \(S\in\{1,2,4,8,16\}\). In each plot, the \(X\)-axis are time steps in \([1,10^{4}]\) and the \(Y\)-axis is cumulative regret. AL stands for AdaLinUCB and SL stands for SparseLinUCB.

* [7] Yi Chen, Yining Wang, Ethan X Fang, Zhaoran Wang, and Runze Li. Nearly dimension-independent sparse linear bandit over small action spaces via best subset selection. _Journal of the American Statistical Association_, 119(545):246-258, 2024.
* [8] Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, and Manish Purohit. Dynamic balancing for model selection in bandits and rl. In _International Conference on Machine Learning_, pages 2276-2285. PMLR, 2021.
* [9] Yan Dai, Ruosong Wang, and Simon Shaolei Du. Variance-aware sparse linear bandits. In _The Eleventh International Conference on Learning Representations_, 2023.
* [10] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. 2008.
* [11] Qin Ding, Yue Kang, Yi-Wei Liu, Thomas Chun Man Lee, Cho-Jui Hsieh, and James Sharpnack. Syndicated bandits: A framework for auto tuning hyper-parameters in contextual bandit algorithms. _Advances in Neural Information Processing Systems_, 35:1170-1181, 2022.
* [12] Dylan J Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. _Advances in Neural Information Processing Systems_, 32, 2019.
* [13] Sebastien Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression. In _Proceedings of the 24th Annual Conference on Learning Theory_, pages 377-396. JMLR Workshop and Conference Proceedings, 2011.
* [14] Avishek Ghosh, Abishek Sankararaman, and Ramchandran Kannan. Problem-complexity adaptive model selection for stochastic linear bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 1396-1404. PMLR, 2021.
* [15] Botao Hao, Tor Lattimore, and Mengdi Wang. High-dimensional sparse linear bandits. _Advances in Neural Information Processing Systems_, 33:10753-10763, 2020.
* [16] Kyoungseok Jang, Chicheng Zhang, and Kwang-Sung Jun. Popart: Efficient sparse regression and experimental design for optimal sparse linear bandits. _Advances in Neural Information Processing Systems_, 35:2102-2114, 2022.
* [17] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [18] Tor Lattimore, Koby Crammer, and Csaba Szepesvari. Linear multi-resource allocation with semi-bandit feedback. _Advances in Neural Information Processing Systems_, 28, 2015.
* [19] Ke Li, Yun Yang, and Naveen N Narisetty. Regret lower bound and optimal algorithm for high-dimensional contextual linear bandit. _Electronic Journal of Statistics_, 15(2):5652-5695, 2021.
* [20] Wenjie Li, Adarsh Barik, and Jean Honorio. A simple unified framework for high dimensional bandit problems. In _International Conference on Machine Learning_, pages 12619-12655. PMLR, 2022.
* [21] Teodor Vanislavov Marinov and Julian Zimmert. The pareto frontier of model selection for general contextual bandits. _Advances in Neural Information Processing Systems_, 34:17956-17967, 2021.
* [22] Min-Hwan Oh, Garud Iyengar, and Assaf Zeevi. Sparsity-agnostic lasso bandit. In _International Conference on Machine Learning_, pages 8271-8280. PMLR, 2021.
* [23] Aldo Pacchiano, Christoph Dann, Claudio Gentile, and Peter Bartlett. Regret bound balancing and elimination for model selection in bandits and RL. _arXiv preprint arXiv:2012.13045_, 2020.
* [24] Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesvari. Model selection in contextual stochastic bandit problems. _Advances in Neural Information Processing Systems_, 33:10328-10337, 2020.
* [25] Aldo Pacchiano, Christoph Dann, and Claudio Gentile. Best of both worlds model selection. _Advances in Neural Information Processing Systems_, 35:1883-1895, 2022.

* [26] Vidyashankar Sivakumar, Steven Wu, and Arindam Banerjee. Structured linear contextual bandits: A sharp and geometric smoothed analysis. In _International Conference on Machine Learning_, pages 9026-9035. PMLR, 2020.

## Appendix A Notation

In Table 2 we list the most used notations. Next, we recall some definitions that are used throughout this appendix. We have \(V_{t}=I+\sum_{s=1}^{t}A_{s}A_{s}^{\top}\) and

\[\widehat{\theta}_{t}=\operatorname*{argmin}_{\theta\in\mathbb{R}^{d}}\left(\| \theta\|_{2}^{2}+\sum_{s=1}^{t}\big{(}\widehat{X}_{s}-\langle\theta,A_{s} \rangle\big{)}^{2}\right)\]

where \(A_{s}\in\mathcal{A}_{s}\) is the action chosen by the learner at round \(t\). Recall that \(\alpha_{i}=2^{i}\log T\). For \(i\in[n]\),

\[A_{t}^{i}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\max_{\theta\in \mathcal{C}_{t}^{i}}\langle\theta,a\rangle=\operatorname*{argmax}_{a\in \mathcal{A}_{t}}\left(\langle a,\widehat{\theta}_{t-1}\rangle+\|a\|_{V_{t-1}^ {-1}}\sqrt{\alpha_{i}}\right)\]

where

\[\mathcal{C}_{t}^{i}=\left\{\theta\in\mathbb{R}^{d}:\|\theta-\widehat{\theta}_ {t-1}\|_{V_{t-1}}^{2}\leq\alpha_{i}\right\}.\]

Finally, recall definition (2.2) with \(\delta=1/T\),

\[\mathcal{C}_{t}=\left\{\theta\in\mathbb{R}^{d}:\|\theta\|_{2}^{2}+\sum_{s=1}^ {t-1}(\widehat{X}_{s}-\langle A_{s},\theta\rangle)^{2}\leq\gamma(1/T)\right\}.\]

and recall that \(\mathcal{E}=\bigcap_{t\in[T]}\{\theta_{*}\in\mathcal{C}_{t}\}\).

## Appendix B Analysis of SparseLinUCB

**Theorem 3.2**.: _The expected regret of SparseLinUCB run with the number of models \(n\) in (3.1) and a distribution \(\textbf{q}=\{q_{s}\}_{s\in[n]}\) satisfies_

\[\mathbb{E}[R_{T}]=O\left((\log T)\sum_{s\geq o}\sqrt{d^{2s}Tq_{s}}+(\log T) \sqrt{STd/Q}\right)\]

_where \(Q=\sum_{s\geq o}q_{s}\)._

Proof.: Lemma 3.1 implies \(\mathcal{C}_{t}\subset\mathcal{C}_{t}^{o}\). Hence, if \(\mathcal{E}\) is true and \(s<o\), then

\[\langle\theta_{*},A_{t}^{s}\rangle \geq\langle\widehat{\theta}_{t-1},A_{t}^{s}\rangle-\sqrt{\alpha_{ o}}\|A_{t}^{s}\|_{V_{t-1}^{-1}}\] ( \[\theta_{*}\in\mathcal{C}_{t}\subset\mathcal{C}_{t}^{o}\] ) \[=\left(\langle\widehat{\theta}_{t-1},A_{t}^{s}\rangle+\sqrt{ \alpha_{s}}\|A_{t}^{s}\|_{V_{t-1}^{-1}}\right)-(\sqrt{\alpha_{o}}+\sqrt{ \alpha_{s}})\|A_{t}^{s}\|_{V_{t-1}^{-1}}\]

\begin{table}
\begin{tabular}{c l} \hline \hline Symbol & Description \\ \hline \(\mathcal{A}_{t}\) & Action set at time \(t\) \\ \(\theta_{*}\) & True parameter of the linear model \\ \(d\) & Dimension of \(\theta_{*}\) \\ \(S\) & Number of nonzero components in \(\theta_{*}\) \\ \(X_{t}\) & Random reward of pulling arm \(A_{t}\in\mathcal{A}_{t}\) \\ \(\widehat{X}_{t}\) & Prediction of SeeSEW at time \(t\) \\ \(\mathcal{C}_{t}^{s}\) & The confidence set with radius \(2^{s}\log T\) and center \(\widehat{\theta}_{t-1}\) \\ \(I_{t}\) & The index of the confidence set drawn at time \(t\) \\ \(q_{s}\) & The probability \(\mathbb{P}(I_{t}=s)\) of drawing index \(s\) of the confidence set in SparseLinUCB \\ \(o\) & The index of the smallest safe confidence set, defined in (3.2) \\ \(Q\) & The probability \(\mathbb{P}(I_{t}\geq o)=q_{o}+\cdots+q_{n}\) of drawing a safe confidence set in SparseLinUCB \\ \(A_{t}^{s}\) & The optimistic action for \(\mathcal{C}_{t}^{s}\) \\ \(A_{t}^{s}\) & The optimal action in \(\mathcal{A}_{t}\) \\ \(\mathcal{E}\) & Event that \(\theta_{*}\in\mathcal{C}_{t}\) for all \(t\in[T]\) \\ \(\det V\) & Determinant of \(V\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Notation.

\[\geq\left(\langle\widehat{\theta}_{t-1},A^{o}_{t}\rangle+\sqrt{ \alpha_{s}}\|A^{o}_{t}\|_{V^{-1}_{t-1}}\right)-(\sqrt{\alpha_{o}}+\sqrt{\alpha_ {s}})\|A^{s}_{t}\|_{V^{-1}_{t-1}}\quad\text{(maximality of $A^{s}_{t}$ in $C^{s}_{t}$)}\] \[=\langle\widehat{\theta}_{t-1},A^{o}_{t}\rangle+\sqrt{\alpha_{o}} \|A^{o}_{t}\|_{V^{-1}_{t-1}}-(\sqrt{\alpha_{o}}-\sqrt{\alpha_{s}})\|A^{o}_{t} \|_{V^{-1}_{t-1}}-(\sqrt{\alpha_{o}}+\sqrt{\alpha_{s}})\|A^{s}_{t}\|_{V^{-1}_{ t-1}}\] \[\geq\langle\theta_{*},A^{o}_{t}\rangle-(\sqrt{\alpha_{o}}-\sqrt{ \alpha_{s}})\|A^{o}_{t}\|_{V^{-1}_{t-1}}-(\sqrt{\alpha_{o}}+\sqrt{\alpha_{s}}) \|A^{s}_{t}\|_{V^{-1}_{t-1}}\quad\quad\quad(\text{$\theta_{*}\in\mathcal{C}_{ t}\subset\mathcal{C}^{o}_{t}$})\] \[\geq\langle\theta_{*},A^{o}_{t}\rangle-(\sqrt{\alpha_{o}}-\sqrt{ \alpha_{s}})\|A^{o}_{t}\|_{V^{-1}_{t-1}}-(\sqrt{\alpha_{o}}+\sqrt{\alpha_{s}}) \|A^{o}_{t}\|_{V^{-1}_{t-1}}\quad\quad\text{(by Lemma D.3)}\] \[=\langle\theta_{*},A^{o}_{t}\rangle-2\sqrt{\alpha_{o}}\|A^{o}_{t} \|_{V^{-1}_{t-1}}\] \[\geq\langle\theta_{*},A^{s}_{t}\rangle-3\sqrt{\alpha_{o}}\|A^{o}_ {t}\|_{V^{-1}_{t-1}}\] (B.1)

where in the first and the third inequalities, we use the fact that for any \(A\in\mathbb{R}^{d}\),

\[\langle\theta_{*}-\widehat{\theta}_{t-1},A\rangle \leq\|\theta_{*}-\widehat{\theta}_{t-1}\|_{V_{t-1}}\|A\|_{V^{-1}_ {t-1}}, \text{(Cauchy-Schwarz inequality)}\] \[\leq\sqrt{\alpha_{o}}\|A\|_{V^{-1}_{t-1}} \text{(due to $\theta_{*}\in\mathcal{C}^{o}_{t}$)}\]

and for the last inequality,

\[\langle\theta_{*},A^{*}_{t}\rangle \leq\langle\widehat{\theta}_{t-1},A^{o}_{t}\rangle+\sqrt{\alpha_ {0}}\|A^{o}_{t}\|_{V^{-1}_{t-1}} \text{($\theta_{*}\in\mathcal{C}^{o}_{t}$, maximality of $A^{o}_{t}$.)}\]

We can decompose the regret as follows,

\[R_{T} =\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}\}\langle\theta_{*},A^{*}_{ t}-A^{I_{t}}_{t}\rangle+\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle \theta_{*},A^{*}_{t}-A^{I_{t}}_{t}\rangle\] \[=\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle\theta_{*},A^{ *}_{t}-A^{I_{t}}_{t}\rangle+\sum_{t=1}^{T}\mathbb{1}\{I_{t}\geq o,\mathcal{E} \}\langle\theta_{*},A^{*}_{t}-A^{I_{t}}_{t}\rangle+\sum_{t=1}^{T}\mathbb{1}\{I _{t}<o,\mathcal{E}\}\langle\theta_{*},A^{*}_{t}-A^{I_{t}}_{t}\rangle\] \[\leq\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle\theta_{*}, A^{*}_{t}-A^{I_{t}}_{t}\rangle+\sum_{t=1}^{T}\mathbb{1}\{I_{t}\geq o,\mathcal{E} \}\langle\theta_{*},A^{*}_{t}-A^{I_{t}}_{t}\rangle\] \[\qquad+\sum_{t=1}^{T}\mathbb{1}\{I_{t}<o,\mathcal{E}\}\min\left\{2, 3\sqrt{\alpha_{o}}\|A^{o}_{t}\|_{V^{-1}_{t-1}}\right\}\] \[\leq\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle\theta_{*}, A^{*}_{t}-A^{I_{t}}_{t}\rangle+\underbrace{\sum_{t=1}^{T}\mathbb{1}\{I_{t}\geq o,\mathcal{E}\}\langle\theta_{*},A^{*}_{t}-A^{I_{t}}_{t}\rangle}_{\bigtriangleup}+ \underbrace{\sum_{t=1}^{T}\left\{2,3\sqrt{\alpha_{o}}\|A^{o}_{t}\|_{V^{-1}_{t-1 }}\right\}}_{\bigtriangleup}.\] (B.2)

Since \(\mathbb{P}(\mathcal{E}^{c})\leq\delta\leq\frac{1}{T}\), the first sum in the above line is easily bounded,

\[\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle\theta_{*},A^{*}_{t}-A^{I_{t }}_{t}\rangle\leq 2T\mathbb{P}(\mathcal{E}^{c})\leq 2\.\]

**Bounding term \(\clubsuit\).** For each \(s\geq 0\), let

\[\mathcal{T}^{s}=\bigg{\{}t\in[T]:\det(V_{t-1})\in\left[2^{sd},2^{(s+1)d} \right)\bigg{\}}.\]

Note that \(\det(V_{t})\) is monotone increasing w.r.t \(t\). Define \(s^{\prime}=\lceil\log_{2}\det(V_{T})/d\rceil\). Then,

\[[T]=\bigcup_{s=1}^{s^{\prime}}\mathcal{T}^{s}.\]

Therefore,

\[\clubsuit\leq\sum_{s=1}^{s^{\prime}}\sum_{t\in\mathcal{T}^{s}}\min\left\{2,3 \sqrt{\alpha_{o}}\|A^{o}_{t}\|_{V^{-1}_{t-1}}\right\}\,.\]By applying Lemma D.5, we obtain

\[\mathbb{E}[\boldsymbol{\triangle}] =\mathbb{E}\left[\sum_{s=1}^{s^{\prime}}\sum_{t\in\mathcal{T}^{s}} \min\Big{\{}2,3\sqrt{\alpha_{o}}\|A_{t}^{o}\|_{V_{t-1}^{-1}}\Big{\}}\right]\] \[=3\sqrt{\alpha_{o}}\mathbb{E}\left[\sum_{s=1}^{s^{\prime}}\sum_{t \in\mathcal{T}^{s}}\min\Big{\{}1,\|A_{t}^{o}\|_{V_{t-1}^{-1}}\Big{\}}\right]\] ( \[\alpha_{o}\geq 1\] ) \[\leq 3\sqrt{\alpha_{o}}\sqrt{T\cdot\mathbb{E}\left[\sum_{s=1}^{s^{ \prime}}\Bigg{[}\sum_{t\in\mathcal{T}^{s}}\min\bigg{\{}1,\|A_{t}^{o}\|_{V_{t-1} ^{-1}}^{2}\bigg{\}}\right]\Bigg{]}}\] (Cauchy-Schwarz inequality) \[\leq 3\sqrt{\alpha_{o}T}\sqrt{\sum_{s=1}^{s^{\prime}}\left(2d/Q+1/ Q\right)}\] (due to Lemma D.5) \[\leq 3\sqrt{\alpha_{o}T}\sqrt{(2d+1)s^{\prime}/Q}\] \[=3\sqrt{\alpha_{o}T}\sqrt{(2d+1)/Q\lceil\log_{2}\det(V_{T})/d\rceil}\] \[=O\Big{(}\log T\sqrt{STd/Q}\Big{)}.\]

Bounding term \(\boldsymbol{\triangle}\).: Let \(T_{s}=\sum_{t=1}^{T}\mathds{1}\{I_{t}=s\}\).

\[\mathbb{E}[\boldsymbol{\triangle}] =\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{I_{t}\geq o,\mathcal{E }\}\langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle\right]\] \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{I_{t}\geq o, \mathcal{E}\}\cdot\min\bigg{\{}2,\sqrt{\alpha I_{t}}\|A_{t}^{I_{t}}\|_{V_{t-1} ^{-1}}\bigg{\}}\right]\] \[\leq 2\sum_{s\geq o}\mathbb{E}\left[\sqrt{\alpha_{s}T_{s}}\sqrt{ \sum_{t\in\,\lceil T\rceil}\mathds{1}\{I_{t}=s\}\min\bigg{\{}1,\|A_{t}^{s}\|_ {V_{t-1}^{-1}}^{2}\bigg{\}}}\right]\] (Cauchy-Schwarz inequality) \[\leq 2\sum_{s\geq o}\mathbb{E}\left[\sqrt{\alpha_{s}T_{s}}\sqrt{ \sum_{t\in\,\lceil T\rceil}\min\bigg{\{}1,\|A_{t}\|_{V_{t-1}^{-1}}^{2}\bigg{\}}}\right]\] \[\leq 2\sum_{s\geq o}\mathbb{E}\left[\sqrt{\alpha_{s}T_{s}}\sqrt{2 \log\det V_{T}}\right]\] (Lemma D.1) \[\leq 2\sum_{s\geq o}\mathbb{E}\left[\sqrt{\alpha_{s}T_{s}}\right] \cdot O(\sqrt{d\log T})\] (upper bound on \[\det(V_{T})\] ) \[\leq 2\sum_{s\geq o}\sqrt{\alpha_{s}}\sqrt{\mathbb{E}\left[T_{s} \right]}\cdot O(\sqrt{d\log T})\] (Jensen's inequality) \[=O\bigg{(}\sum_{s\geq o}\sqrt{\alpha_{s}dTq_{s}\log T}\bigg{)}\] (because \[\mathbb{P}(I_{t}=s)=q_{s}\].)

Substituting the bounds of \(\boldsymbol{\triangle}\) and \(\boldsymbol{\triangle}\) to (B.2), we have

\[\mathbb{E}[R_{T}]\leq O\bigg{(}\sum_{s\geq o}\sqrt{d\alpha_{s}T\log Tq_{s}}+ \log T\sqrt{SdT/Q}\bigg{)}\]

concluding the proof. 

**Theorem 3.4**.: _The expected regret of \(\mathtt{SparseLinUCB}\) run with the number of models \(n\) in (3.1), a distribution \(\boldsymbol{q}=\{q_{s}\}_{s\in[n]}\) and using \(\mathtt{SeqSEW}\) as base algorithm satisfies_

\[\mathbb{E}[R_{T}]=O\left(\frac{(dS/Q)+d^{2}}{\Delta}(\log T)^{2}\right)\]_where \(Q=\sum_{s\geq o}q_{s}\)._

Proof.: \[R_{T} =\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}\}\langle\theta_{*},A_{t}^{*}- A_{t}^{I_{t}}\rangle+\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle\theta_{*},A_{t}^ {*}-A_{t}^{I_{t}}\rangle\] \[\leq\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle\theta_{*}, A_{t}^{*}-A_{t}^{I_{t}}\rangle+\sum_{t=1}^{T}\mathbb{1}\{I_{t}\geq o,\mathcal{E}\} \frac{\langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle^{2}}{\Delta}\] \[\qquad+\sum_{t=1}^{T}\mathbb{1}\{I_{t}<o,\mathcal{E}\}\frac{ \langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle^{2}}{\Delta}\] (minimality of \[\Delta\] ) \[\leq\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}^{c}\}\langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle+\underbrace{\sum_{t=1}^{T}\mathbb{1}\{I_{t} \geq o,\mathcal{E}\}\frac{\langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle^{2} }{\Delta}}_{\bigtriangleup}\] \[\qquad+9\underbrace{\sum_{t=1}^{T}\mathbb{1}\{I_{t}<o,\mathcal{E} \}\frac{\min\left\{1,\alpha_{o}\|A_{t}^{o}\|_{V_{t-1}^{-1}}^{2}\right\}}{ \Delta}}_{\bigtriangleup}\] (by applying (B.1) to \[\blacktriangle\] )

**Bounding term \(\blacktriangle\).** Let \(s^{\prime}=\lceil\log_{2}\det(V_{T})/d\rceil\). By applying Lemma D.5, we obtain

\[\mathbb{E}[\blacktriangle] \leq\frac{9}{\Delta}\mathbb{E}\left[\sum_{s\geq 0}\sum_{t\in \mathcal{T}^{s}}\min\left\{1,\alpha_{o}\|A_{t}^{o}\|_{V_{t-1}^{-1}}^{2}\right\}\right]\] \[\leq\frac{9\alpha_{o}}{\Delta}\sum_{s=1}^{s^{\prime}}\mathbb{E} \bigg{[}\sum_{t\in\mathcal{T}^{s}}\min\left\{1,\|A_{t}^{o}\|_{V_{t-1}^{-1}}^{2 }\right\}\bigg{]}\] ( \[\alpha_{o}\geq 1\] ) \[\leq\frac{9\alpha_{o}}{\Delta}\sum_{s=1}^{s^{\prime}}\left(2d/Q+ 1/Q\right)\] (due to Lemma D.5 ) \[\leq\frac{27\alpha_{o}ds^{\prime}/Q}{\Delta}\] \[=O\Bigg{(}\frac{dS(\log T)^{2}/Q}{\Delta}\Bigg{)}\] (because \[s^{\prime}=O(\log T)\] )

**Bounding term \(\blacktriangle\).**

\[\mathbb{E}[\blacktriangle] =\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{1}\{I_{t}\geq o,\mathcal{E }\}\frac{\langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle^{2}}{\Delta}\right]\] \[\leq\frac{4}{\Delta}\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{1}\{I_{ t}\geq o,\mathcal{E}\}\cdot\min\left\{1,\alpha_{I_{t}}\|A_{t}^{I_{t}}\|_{V_{t-1}^{-1}}^{2 }\right\}\right]\] \[\leq 4\sum_{s\geq o}\frac{\alpha_{s}}{\Delta}\mathbb{E}\left[\sum_{ t\in\lceil T\rceil}\min\left\{1,\|A_{t}\|_{V_{t-1}^{-1}}^{2}\right\}\right]\] \[\leq 8\sum_{s\geq o}\frac{\alpha_{s}}{\Delta}\cdot\log\det V_{T}\] (due to Lemma D.1 ) \[=O\bigg{(}\frac{d^{2}(\log T)^{2}}{\Delta}\bigg{)}\] (B.3 )

where the first inequality comes from

\[\langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle\leq\max_{\theta\in C_{t-1}^{ I_{t}}}\langle\theta-\theta_{*},A_{t}^{I_{t}}\rangle\] (due to Lemma D.1 )\[\mathbb{E}[R_{T}] =O\left((\log T)\sum_{s\geq o}\sqrt{d2^{s}Tq_{s}}+(\log T)\sqrt{STd/ Q}\right)\] \[\leq O\left((\log T)\sum_{s\in[o,o^{\prime})}\sqrt{d2^{s}T}+(\log T )\sum_{s\in[o^{\prime},n]}\sqrt{d2^{s}Tq_{s}}+(\log T)\sqrt{STd/q_{o^{\prime}}}\right)\] \[\leq\widetilde{O}\left(\sqrt{d2^{o^{\prime}}T}+nC\sqrt{dT}+\sqrt {STd}\right)\]\[=\widetilde{O}\bigg{(}\max\{C,S/C\}\sqrt{dT}\bigg{)}.\] (due to \[C^{2}=\Theta(2^{o^{\prime}})\] and \[2^{o^{\prime}}\geq 2^{o}\geq S\] )

Besides, according to Theorem 3.4,

\[\mathbb{E}[R_{T}] =O\bigg{(}\max\{S/Q,d\}\cdot\frac{d(\log T)^{2}}{\Delta}\bigg{)}\] \[=O\bigg{(}\max\{S/q_{o^{\prime}},d\}\cdot\frac{d(\log T)^{2}}{ \Delta}\bigg{)}\] \[=\widetilde{O}\bigg{(}\frac{\max\{d^{2},S^{2}d/C^{2}\}}{\Delta} \bigg{)}.\]

Therefore,

\[\mathbb{E}[R_{T}]=\widetilde{O}\left(\min\left\{\max\big{\{}C,S/C\big{\}}\sqrt {dT},\;\frac{\max\big{\{}d^{2},S^{2}d/C^{2}\big{\}}}{\Delta}\right\}\right).\]

**Case \(C^{2}\geq 2^{n}\):**\(q_{s}=1/n\) for all \(s\in[n]\). It is easy to verify that \(\mathbb{E}[R_{T}]=\widetilde{O}(d\sqrt{T})\) and \(\mathbb{E}[R_{T}]=\widetilde{O}(d^{2}/\Delta)\). Thus,

\[\mathbb{E}[R_{T}]=\widetilde{O}\left(\min\left\{\max\big{\{}C,S/C\big{\}}\sqrt {dT},\;\frac{\max\big{\{}d^{2},S^{2}d/C^{2}\big{\}}}{\Delta}\right\}\right).\]

**Corollary B.2**.: _Assume that the sparsity level \(S\) is known and choose a distribution \(\boldsymbol{q}=\{q_{s}\}_{s\in[n]}\) with \(q_{o}=1\), where \(o\) is set as in (3.2). Then, the expected regret of SparseLinUCB is \(\mathbb{E}[R_{T}]=\widetilde{O}\big{(}\frac{Sd}{\Delta}\big{)}\)._

Proof.: We follow the same steps as in the proof of Theorem 3.4. The only difference lies in the bounding term \(\spadesuit\) in (B.3). We have

\[\mathbb{E}[\spadesuit] =\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{1}\{I_{t}\geq o,\mathcal{E }\}\frac{\langle\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\rangle^{2}}{\Delta}\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{1}\{\mathcal{E}\}\frac{ \langle\theta_{*},A_{t}^{*}-A_{t}\rangle^{2}}{\Delta}\right]\] \[\leq\frac{4\alpha_{o}}{\Delta}\mathbb{E}\left[\sum_{t\in[T]}\min \left\{1,\|A_{t}\|_{V_{t-1}^{-1}}^{2}\right\}\right]\] \[=O\bigg{(}\frac{Sd(\log T)^{2}}{\Delta}\bigg{)}\] (B.4)

where the second equality is because \(q_{o}=1\) and so \(A_{t}^{I_{t}}=A_{t}^{o}=A_{t}\), and the first inequality comes from

\[\langle\theta_{*},A_{t}^{*}-A_{t}\rangle \leq\max_{\theta\in\mathcal{C}_{t-1}^{o}}\langle\theta-\theta_{*},A_{t}\rangle\] \[\leq\max_{\theta\in\mathcal{C}_{t-1}^{o}}\|\theta-\theta_{*}\|_{V _{t-1}}\|A_{t}^{o}\|_{V_{t-1}^{-1}}\] (Cauchy-Schwarz Inequality) \[\leq 2\sqrt{\alpha_{o}}\|A_{t}\|_{V_{t-1}^{-1}}\]

where the last inequality holds because \(\mathcal{E}\) and \(I_{t}=o\) both hold, and so \(\theta_{*}\in\mathcal{C}_{t-1}\subset\mathcal{C}_{t-1}^{o}\) due to Lemma 3.1. Substituting the bounds of \(\spadesuit\) and \(\spadesuit\) in (B.2), we have

\[\mathbb{E}[R_{T}]=O\bigg{(}\frac{Sd(\log T)^{2}}{\Delta}\bigg{)}.\]

## Appendix C Analysis of AdaLinUCB

**Theorem 4.1**.: _If the random independent noise \(\varepsilon_{t}\) in (2.1) satisfies \(\varepsilon_{t}\in[-1,1]\) for all \(t\in[T]\), then the regret of AdaLinUCB run with \(\eta=\sqrt{(\log n)/(Tn)}\) for \(n\) in (3.1) and \(q\in(0,1]\) satisfies_

\[\mathbb{E}[R_{T}] \leq\left(\sqrt{8\alpha_{n}q}+4\sqrt{(2\alpha_{o})/q}\right) \sqrt{dT\log\left(1+\frac{TL^{2}}{d}\right)+1}+O\big{(}\sqrt{nT\log n}\big{)}\] \[=\widetilde{O}\left(\max\left\{\sqrt{dq},\sqrt{S/q}\right\} \sqrt{dT}\right)\;.\]

Proof.: Let \(\mathcal{T}_{i}\) be the set of time steps where \(Z_{t}=1\) in Line 4 of AdaLinUCB and let \(\mathcal{T}_{2}=[T]\setminus\mathcal{T}_{1}\). For all \(t\in\mathcal{T}_{2}\) the action \(A_{t}\in\mathcal{A}_{t}\) is chosen by Exp3 through an adversarial mapping \(\mu_{t}:[n]\to\mathcal{A}_{t}\) defined in Lines 9-10. The resulting reward \(X_{t}\in[-2,2]\) is fed to Exp3 as a \([0,1]\)-valued loss \(\ell_{t}(I_{t})=(2-X_{t})/4\), where \(\mu_{t}(I_{t})=A_{t}\). Since \(\mathcal{T}_{2}\) is selected through independent coin tosses with fixed bias \(q\), we can apply the Exp3 regret analysis (for losses chosen by a non-oblivious adversary) to bound the regret in \(\mathcal{T}_{2}\) and show that

\[\sum_{t\in\mathcal{T}_{2}}\left\langle A_{t}^{o}-A_{t},\theta_{*} \right\rangle=4\sum_{t\in\mathcal{T}_{2}}\left(\ell_{t}(I_{t})-\ell_{t}(o) \right)=O\big{(}\sqrt{n\log n}\big{)}\] (C.1)

where we used \(\mu_{t}(o)=A_{t}^{o}\) for all \(t\in[T]\).

We have

\[R_{\mathcal{T}_{2}} =\sum_{t\in\mathcal{T}_{2}}\operatorname*{argmax}_{a\in\mathcal{ A}_{t}}\left\langle a-A_{t},\theta_{*}\right\rangle\] \[=\sum_{t\in\mathcal{T}_{2}}\operatorname*{argmax}_{a\in\mathcal{ A}_{t}}\left\langle a-A_{t}^{o}+A_{t}^{o}-A_{t},\theta_{*}\right\rangle\] \[=\underbrace{\sum_{t\in\mathcal{T}_{2}}\operatorname*{argmax}_{a \in\mathcal{A}_{t}}\left\langle a-A_{t}^{o},\theta_{*}\right\rangle}_{\mathrm{ Reg}_{\mathrm{Img}}}+\underbrace{\sum_{t\in\mathcal{T}_{2}}\left\langle A_{t}^{o}-A_{t}, \theta_{*}\right\rangle}_{\mathrm{Reg}_{\mathrm{Exp3}}}.\]

\(\mathrm{Reg}_{\mathrm{Exp3}}\) is bounded in (C.1), hence we only need to bound

\[\mathrm{Reg}_{\mathrm{Img}}=\sum_{t\in\mathcal{T}_{2}}\operatorname*{argmax}_ {a\in\mathcal{A}_{t}}\left\langle a-A_{t}^{o},\theta_{*}\right\rangle.\] (C.2)

Let

\[\widetilde{\theta}_{t}^{i}=\operatorname*{argmax}_{\theta\in C_{t}^{i}}\max_{ a\in\mathcal{A}_{t}}\langle a,\theta\rangle\quad\text{and}\quad A_{t}^{*}= \operatorname*{argmax}_{a\in\mathcal{A}_{t}}\langle a,\theta_{*}\rangle.\]

Recall \(\mathcal{E}\) is the event that \(\theta_{*}\in\mathcal{C}_{t}\) for all \(t\in[T]\). Assume \(\mathcal{E}\) holds. We obtain that for \(t\in[T]\),

\[\left\langle\theta_{*},A_{t}^{*}-A_{t}^{o}\right\rangle \leq\left\langle\widetilde{\theta}_{t}^{o}-\theta_{*},A_{t}^{o}\right\rangle\] (because \[\theta_{*}\in\mathcal{C}_{t}\subseteq\mathcal{C}_{t}^{o}\] ) \[\leq\left\|\widetilde{\theta}_{t}^{o}-\theta_{*}\right\|_{V_{t-1}} \|A_{t}^{o}\|_{V_{t-1}^{-1}}\] (Cauchy-Schwarz inequality) \[\leq\sqrt{\alpha_{o}}\|A_{t}^{o}\|_{V_{t-1}^{-1}},\] (C.3)

where the last inequality is because \(\widetilde{\theta}_{t}^{o}\in\mathcal{C}_{t}^{o}\). Then, assuming \(\mathcal{E}\) holds, we can bound \(\mathrm{Reg}_{\mathrm{Img}}\) as

\[\mathrm{Reg}_{\mathrm{Img}} =\sum_{t\in\mathcal{T}_{2}}\langle\theta_{*},A_{t}^{*}-A_{t}^{o}\rangle\] \[\leq\sum_{t\in\mathcal{T}_{2}}\min\left\{2,\sqrt{\alpha_{o}}\|A_ {t}^{o}\|_{V_{t-1}^{-1}}\right\}\] \[\leq 2\sqrt{\gamma(1/T)}\sum_{t\in\mathcal{T}_{2}}\min\left\{1,\|A_{t }^{o}\|_{V_{t-1}^{-1}}\right\},\] (C.4)where in the first inequality, we use the facts \(\langle\theta_{*},A_{t}^{*}-A_{t}^{o}\rangle\leq\sqrt{\alpha_{o}}\big{\|}A_{t}^{o} \big{\|}_{V_{t-1}^{-1}}\) and \(\langle\theta_{*},A_{t}^{*}-A_{t}^{o}\rangle\leq\langle\theta_{*},A_{t}^{*} \rangle\leq 2\).

From Lemma D.3, we have

\[\sum_{t\in\mathcal{T}_{2}}\min\left\{1,\|A_{t}^{o}\|_{V_{t-1}^{-1}}^{2}\right\} \leq\sum_{t\in\mathcal{T}_{2}}\min\left\{1,\|A_{t}^{n}\|_{V_{t-1}^{-1}}^{2} \right\}.\]

For each \(s\geq 0\), let

\[\mathcal{T}_{2}^{s}=\left\{t\in\mathcal{T}_{2}:\det(V_{t-1})\in\left[2^{ds},2^ {d(s+1)}\right)\right\}\]

so that

\[\mathcal{T}_{2}=\bigcup_{s\geq 0}\mathcal{T}_{2}^{s}.\]

Let \(s^{\prime}=\lceil\log_{2}\det(V_{T})/d\rceil\). We have

\[\mathbb{E}\big{[}\mathrm{Reg}_{\mathrm{Img}}\cdot\mathbb{1}[ \mathcal{E}]\big{]} \leq\mathbb{E}\Bigg{[}2\sqrt{\gamma(1/T)}\sum_{t\in\mathcal{T}_{2}} \min\left\{1,\|A_{t}^{o}\|_{V_{t-1}^{-1}}\right\}\Bigg{]}\] (due to (C.4)) \[\leq 2\sqrt{2\alpha_{o}}\,\mathbb{E}\Bigg{[}\sum_{t\in \mathcal{T}_{2}}\min\left\{1,\|A_{t}^{o}\|_{V_{t-1}^{-1}}\right\}\Bigg{]}\] (due to \[\alpha_{0}\leq 2\gamma(1/T)\] ) \[\leq 2\sqrt{2\alpha_{o}T}\sqrt{\mathbb{E}\Bigg{[}\sum_{t\in \mathcal{T}_{2}}\min\left\{1,\|A_{t}^{o}\|_{V_{t-1}^{-1}}^{2}\right\}\Bigg{]}}\] (Cauchy-Schwarz inequality) \[\leq 2\sqrt{2\alpha_{o}T}\sqrt{\sum_{s=1}^{s^{\prime}}\mathbb{E} \Bigg{[}\sum_{t\in\mathcal{T}_{2}^{s}}\min\left\{1,\|A_{t}^{o}\|_{V_{t-1}^{-1 }}^{2}\right\}\Bigg{]}}\] (where \[d\ln s^{\prime}\leq\log\det(V_{T})\] ) \[\leq 2\sqrt{2\alpha_{o}T}\sqrt{\sum_{s=1}^{s^{\prime}}\mathbb{E} \Bigg{[}\sum_{t\in\mathcal{T}_{2}^{s}}\min\left\{1,\|A_{t}^{n}\|_{V_{t-1}^{-1 }}^{2}\right\}\Bigg{]}}\] (because \[\|A_{t}^{n}\|_{V_{t-1}^{-1}}\geq\|A_{t}^{o}\|_{V_{t-1}^{-1}}\] ) \[\leq 2\sqrt{2\alpha_{o}T}\sqrt{\sum_{s=1}^{s^{\prime}}(2d+1)/q}\] (due to Lemma D.4) \[\leq 2\sqrt{2(2d+1)\alpha_{o}Ts^{\prime}/q}\.\]

To obtain the final results, we have

\[\det(V_{T})=\prod_{i=1}^{d}\lambda_{i}\leq\left(\frac{1}{d}\text{ trace}(V_{T})\right)^{d}\leq\left(1+\frac{TL^{2}}{d}\right)^{d},\] (C.5)

where \(\lambda_{1},\cdots,\lambda_{d}\) are the eigenvalues of \(V_{T}\). Therefore, we have \(s^{\prime}\leq\lceil\log_{2}(1+TL^{2}/d)\rceil\).

\[\mathbb{E}\big{[}\mathrm{Reg}_{\mathrm{Img}}\cdot\mathbb{1}[ \mathcal{E}]\big{]}\leq 2\sqrt{2(2d+1)\alpha_{o}T/q}\cdot\sqrt{s^{\prime}}\] \[\leq 2\sqrt{(6d\alpha_{o}T/q)\lceil\log_{2}(1+TL^{2}/d) \rceil}.\]

By substituting the bounds on \(\mathrm{Reg}_{\mathrm{Img}}\) and \(\mathrm{Reg}_{\mathrm{Exp3}}\) into \(R_{\mathcal{T}_{2}}\), we obtain

\[\mathbb{E}[R_{\mathcal{T}_{2}}] =\mathbb{E}[\mathrm{Reg}_{\mathrm{Exp3}}\mathbb{1}\{\mathcal{E}\}] +\mathbb{E}[\mathrm{Reg}_{\mathrm{Img}}]+T\cdot\mathbb{P}(\mathcal{E}^{c})\] \[\leq 2\sqrt{6d\alpha_{o}T/q}\sqrt{\lceil\log(1+TL^{2}/d) \rceil}+O(\sqrt{nT\log n}),\]

where the last inequality is because \(\mathbb{P}(\mathcal{E}^{c})\leq 1/T\) from Lemma 2.1 with our choice of \(\delta=1/\delta\).

Finally, we bound \(R_{\mathcal{T}_{1}}\). Conditioned on event \(\mathcal{E}\) and using (D.3), \(\forall t\in[T],\theta_{*}\in\mathcal{C}_{t}\subseteq\mathcal{C}_{t}^{0}\subseteq \mathcal{C}_{t}^{n}\). Hence, we can obtain

\[\langle\theta_{*},A_{t}^{*}-A_{t}^{n}\rangle \leq\langle\widetilde{\theta}_{t}^{n}-\theta_{*},A_{t}^{n}\rangle \text{(because $\theta_{*}\in\mathcal{C}_{t}\subseteq\mathcal{C}_{t}^{n}$)}\] \[\leq\|\widetilde{\theta}_{t}^{n}-\theta_{*}\|_{V_{t-1}}\|A_{t}^{ n}\|_{V_{t-1}^{-1}}\text{(Cauchy-Schwarz inequality)}\] \[\leq\sqrt{\alpha_{n}}\|A_{t}^{n}\|_{V_{t-1}^{-1}} \text{(because $\widetilde{\theta}_{t}^{n}\in\mathcal{C}_{t}^{n}$.)}\]

Hence, conditioned on event \(\mathcal{E}\),

\[R_{\mathcal{T}_{1}} \leq\sum_{t\in\mathcal{T}_{1}}\min\left\{2,\langle\theta_{*},A_{ t}^{*}-A_{t}^{n}\rangle\right\}\] \[\leq\sum_{t\in\mathcal{T}_{1}}\min\left\{2,\sqrt{\alpha_{n}}\|A_ {t}^{n}\|_{V_{t-1}^{-1}}\right\}\] (C.6) \[\leq 2\sqrt{\alpha_{n}|\mathcal{T}_{1}|}\sqrt{\sum_{t\in \mathcal{T}_{1}}\min\left\{1,\|A_{t}^{n}\|_{V_{t-1}^{-1}}^{2}\right\}}\text{(Cauchy- Schwarz inequality)}\] \[\leq 2\sqrt{\alpha_{n}|\mathcal{T}_{1}|}\sqrt{\sum_{t\in[T]}\min \left\{1,\|A_{t}\|_{V_{t-1}^{-1}}^{2}\right\}}\] \[\leq 2\sqrt{\alpha_{n}|\mathcal{T}_{1}|}\cdot\sqrt{2\log(\det(V_{T }))},\] (C.7)

where the last inequality is due to Lemma D.1. Therefore, the expected regret for \(t\in\mathcal{T}_{1}\) is

\[\mathbb{E}[R_{\mathcal{T}_{1}}] \leq\mathbb{E}[R_{\mathcal{T}_{1}}\mathbb{1}\{\mathcal{E}\}]+T \cdot\mathbb{P}(\mathcal{E}^{c})\] \[\leq\mathbb{E}\left[2\sqrt{\alpha_{n}|\mathcal{T}_{1}|}\cdot \sqrt{2\log(\det(V_{T}))}\right]+1\] \[\leq\mathbb{E}\left[2\sqrt{\alpha_{n}|\mathcal{T}_{1}|}\cdot \sqrt{2d\log\left(1+\frac{TL^{2}}{d}\right)}\right]+1\] \[=2\sqrt{\alpha_{n}}\mathbb{E}\left[\sqrt{|\mathcal{T}_{1}|} \right]\cdot\sqrt{2d\log\left(1+\frac{TL^{2}}{d}\right)}+1\] \[\leq 2\sqrt{\alpha_{n}Tq}\cdot\sqrt{2d\log\left(1+\frac{TL^{2}}{d }\right)}+1,\] (Jensen's inequality)

where the first inequality is due to (C.7) and \(\mathbb{P}(\mathcal{E}^{c})\leq 1/T\) from Lemma 2.1 and the last inequality is due to the fact that for any \(t\in[T]\), with probability \(q\), \(t\in\mathcal{T}_{1}\). Finally, we obtain

\[\mathbb{E}[R_{T}] =\mathbb{E}[R_{\mathcal{T}_{1}}]+\mathbb{E}[R_{\mathcal{T}_{2}}]\] \[\leq 2\sqrt{\alpha_{n}Tq}\cdot\sqrt{2d\log\left(1+\frac{TL^{2}}{d }\right)}+2\sqrt{6d\alpha_{o}T/q}\sqrt{\lceil\log(1+TL^{2}/d)\rceil}+O(\sqrt{ nT\log n}).\]

Note that \(n=\Theta(\log d)\) and \(\alpha_{o}=\Theta(S\log T)\). We obtain

\[\mathbb{E}[R_{T}]=O\left(\max\left\{\sqrt{qd},\sqrt{\frac{S}{q}} \right\}\cdot\sqrt{dT}\cdot\log T\right),\]

which completes the proof.

## Appendix D Supporting lemmas

**Lemma D.1** (Dani et al. [10]).: _Let \(A_{1},A_{2},\ldots,A_{T}\in\mathbb{R}^{d}\) and \(V_{t}=I+\sum_{s=1}^{t}A_{s}A_{s}^{\top}\) for all \(t\in[T]\). Then_

\[\sum_{t=1}^{T}\min\left\{1,\|A_{t}\|_{V_{t-1}^{-1}}^{2}\right\} \leq 2\log\det(V_{T}).\] (D.1)

_Moreover,_

\[\min\left\{1,\|A_{t}\|_{V_{t-1}^{-1}}^{2}\right\} \leq 2\log\Big{(}1+\|A_{t}\|_{V_{t-1}^{-1}}^{2}\Big{)}=2\log\bigg{(} \frac{\det(V_{t})}{\det(V_{t-1})}\bigg{)}.\]

**Lemma D.2**.: _For \(\mathcal{C}_{t}\) defined in (2.2), we have that \(\mathcal{C}_{t}\subseteq\mathcal{C}_{t}^{o}\) for all \(t\in[T]\)._

Proof.: Recall

\[\mathcal{C}_{t+1}=\left\{\theta\,:\,\|\theta\|_{2}^{2}+\sum_{s=1}^{t}\left( \widehat{X}_{s}-\langle\theta,A_{s}\rangle\right)^{2}\leq\gamma(1/T)\right\}\,.\]

Note that

\[\|\theta\|_{2}^{2}+\sum_{s=1}^{t}\left(\widehat{X}_{s}-\langle \theta,A_{s}\rangle\right)^{2}-\|\widehat{\theta}_{t}\|_{2}^{2}-\sum_{s=1}^{t} \left(\widehat{X}_{s}-\langle\widehat{\theta}_{t},A_{s}\rangle\right)^{2}\] \[=\|\theta\|_{2}^{2}-2\theta^{\top}\left(\sum_{s=1}^{t}\widehat{X }_{s}A_{s}\right)+\theta^{\top}\left(\sum_{s=1}^{t}A_{s}A_{s}^{\top}\right)\theta\] \[\quad-\|\widehat{\theta}_{t}\|_{2}^{2}+2\widehat{\theta}_{t}^{ \top}\left(\sum_{s=1}^{t}\widehat{X}_{s}A_{s}\right)-\widehat{\theta}_{t}^{ \top}\left(\sum_{s=1}^{t}A_{s}A_{s}^{\top}\right)\widehat{\theta}_{t}\] \[=\|\theta\|_{V_{t}}^{2}+2(\widehat{\theta}_{t}-\theta)^{\top}V_{t }\widehat{\theta}_{t}-\|\widehat{\theta}_{t}\|_{V_{t}}^{2}\qquad(V_{t}=I+\sum _{s=1}^{t}A_{s}A_{s}^{\top},\widehat{\theta}_{t}=V_{t}^{-1}\sum_{s=1}^{t}A_{s} \widehat{X}_{s})\] \[=\|\theta\|_{V_{t}}^{2}-2\theta^{\top}V_{t}\widehat{\theta}_{t}+ \|\widehat{\theta}_{t}\|_{V_{t}}^{2}=\|\theta-\widehat{\theta}_{t}\|_{V_{t}} ^{2}\,.\]

Hence, we can express the ellipsoid as

\[\mathcal{C}_{t+1}=\left\{\|\theta-\widehat{\theta}_{t}\|_{V_{t}} ^{2}+\|\widehat{\theta}_{t}\|_{2}^{2}+\sum_{s=1}^{t}\left(\widehat{X}_{s}- \langle\widehat{\theta}_{t},A_{s}\rangle\right)^{2}\leq\gamma(1/T)\right\}\,.\] (D.2)

Therefore, for all \(t\geq 0\),

\[\mathcal{C}_{t+1} =\left\{\theta\,:\,\|\theta-\widehat{\theta}_{t}\|_{V_{t}}^{2}+ \|\widehat{\theta}_{t}\|_{2}^{2}+\sum_{s=1}^{t}\left(\widehat{X}_{s}-\langle \widehat{\theta}_{t},A_{s}\rangle\right)^{2}\leq\gamma(1/T)\right\}\] (due to (D.2)) \[\subseteq\left\{\theta\,:\,\|\theta-\widehat{\theta}_{t}\|_{V_{t}} ^{2}\leq\gamma(1/T)\right\}\] \[\subseteq\left\{\theta\,:\,\|\theta-\widehat{\theta}_{t}\|_{V_{t}} ^{2}\leq\alpha_{o}\right\}\] (by definition of \[\alpha_{0}\] ) \[=\mathcal{C}_{t+1}^{o}\] (D.3)

concluding the proof. 

**Lemma D.3**.: _For any \(1\leq p\leq q\leq n\) and \(t\in[T]\),_

\[\|A_{t}^{p}\|_{V_{t-1}^{-1}}\leq\|A_{t}^{q}\|_{V_{t-1}^{-1}}.\]

Proof.: Note that

\[A_{t}^{p} =\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\max_{\theta\in \mathcal{C}_{t}^{o}}\langle\theta,a\rangle=\operatorname*{argmax}_{a\in \mathcal{A}_{t}}\langle a,\widehat{\theta}_{t-1}\rangle+\sqrt{\alpha_{p}}\|a \|_{V_{t-1}^{-1}},\] \[A_{t}^{q} =\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\max_{\theta\in \mathcal{C}_{t}^{q}}\langle\theta,a\rangle=\operatorname*{argmax}_{a\in \mathcal{A}_{t}}\langle a,\widehat{\theta}_{t-1}\rangle+\sqrt{\alpha_{q}}\|a \|_{V_{t-1}^{-1}}.\]For contradiction, we assume \(\|A_{t}^{q}\|_{V_{t-1}^{-1}}<\|A_{t}^{p}\|_{V_{t-1}^{-1}}\). Since \(\|A_{t}^{q}\|_{V_{t-1}^{-1}}<\|A_{t}^{p}\|_{V_{t-1}^{-1}}\), \(\|A_{t}^{q}\|_{V_{t-1}^{-1}}\neq\|A_{t}^{p}\|_{V_{t-1}^{-1}}\). Besides, according to the definition of \(A_{t}^{p}\) and \(A_{t}^{q}\), we have

\[\langle A_{t}^{q},\widehat{\theta}_{t-1}\rangle+\sqrt{\alpha_{p} }\|A_{t}^{q}\|_{V_{t-1}^{-1}} \leq\langle A_{t}^{p},\widehat{\theta}_{t-1}\rangle+\sqrt{\alpha_{ p}}\|A_{t}^{p}\|_{V_{t-1}^{-1}}\] (D.4) \[<\langle A_{t}^{p},\widehat{\theta}_{t-1}\rangle+\sqrt{\alpha_{q} }\|A_{t}^{p}\|_{V_{t-1}^{-1}}\] (D.5) \[\leq\langle A_{t}^{q},\widehat{\theta}_{t-1}\rangle+\sqrt{\alpha_ {q}}\|A_{t}^{q}\|_{V_{t-1}^{-1}},\] (D.6)

where the first inequality is due to the definition of \(A_{t}^{p}\), the second inequality is due to \(\sqrt{\alpha_{p}}<\sqrt{\alpha_{q}}\), and the last inequality is due to the definition of \(A_{t}^{q}\). From the above results, we further have

\[\langle A_{t}^{q}-A_{t}^{p},\widehat{\theta}_{t-1}\rangle \leq\sqrt{\alpha_{p}}\big{(}\|A_{t}^{p}\|_{V_{t-1}^{-1}}-\|A_{t}^ {q}\|_{V_{t-1}^{-1}}\big{)}\] (Due to (D.4)) \[<\sqrt{\alpha_{q}}\big{(}\|A_{t}^{p}\|_{V_{t-1}^{-1}}-\|A_{t}^{q} \|_{V_{t-1}^{-1}}\big{)}\] (Due to assumption \[\|A_{t}^{q}\|_{V_{t-1}^{-1}}<\|A_{t}^{p}\|_{V_{t-1}^{-1}}\] and \[\alpha_{q}>\alpha_{p}\] ) \[\leq\langle A_{t}^{q}-A_{t}^{p},\widehat{\theta}_{t-1}\rangle.\] (Due to (D.6).)

We obtain a contradiction. Therefore,

\[\|A_{t}^{q}\|_{V_{t-1}^{-1}}\geq\|A_{t}^{p}\|_{V_{t-1}^{-1}}.\] (D.7)

**Lemma D.4**.: \[\mathbb{E}\left[\sum_{t\in\mathcal{T}_{2}^{s}}\min\left\{1,\|A_{t}^{n}\|_{V_{ t-1}^{-1}}^{2}\right\}\right]\leq 2d/q+1/q.\]

Proof.: Let \(\det(V_{t-1})=(q_{t-1})^{d}\) and \(\det(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\top})=(q_{t-1}+x_{t})^{d}\). For each \(s\geq 0\), let

\[\mathcal{T}^{s}=\bigg{\{}t\in[T]:\det(V_{t-1})\in\left[2^{sd},2^{d(s+1)} \right)\bigg{\}}.\]

Since \(\mathcal{T}_{2}^{s}\subseteq\mathcal{T}^{s}\), to show

\[\mathbb{E}\Bigg{[}\sum_{t\in\mathcal{T}_{2}^{s}}\min\left\{1,\|A_{t}^{n}\|_{V_ {t-1}^{-1}}^{2}\right\}\Bigg{]}\leq 2d/q+1/q,\]

we only need to prove

\[\mathbb{E}\Bigg{[}\sum_{t\in\mathcal{T}^{s}}\min\left\{1,\|A_{t}^{n}\|_{V_{t-1 }^{-1}}^{2}\right\}\Bigg{]}\leq 2d/q+1/q.\]

We let \(I_{t}=1\) if the coin tosses in the \(t\)'s round is Head and 0 otherwise. We divide \(\mathcal{T}^{s}\) into two disjoint parts \(\underline{\mathcal{T}^{s}}\) and \(\overline{\mathcal{T}^{s}}\). Specifically,

* for \(\underline{\mathcal{T}^{s}}\), it holds that for \(t\in\underline{\mathcal{T}^{s}}\), \(\det(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\top})\leq 2^{d(s+1)}\).
* for \(\overline{\mathcal{T}^{s}}\), it holds that for \(t\in\overline{\mathcal{T}^{s}}\), \(\det(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\top})>2^{d(s+1)}\).

From definition of \(\underline{\mathcal{T}^{s}}\) and the fact that if \(I_{t}=1\), \(V_{t}=V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\top}\), we have

\[\sum_{t\in\underline{\mathcal{T}^{s}}}x_{t}\cdot I_{t}\leq 2^{s}.\]

From Algorithm 2, with probability \(q\), \(I_{t}=1\). Therefore, if we let \(\{\mathcal{F}_{t}\}_{t\in[T]}\) be the natural filtration of \(\{A_{t},I_{t},X_{t}\}_{t\in[T]}\), we have

\[2^{s}\geq\mathbb{E}\left[\sum_{t\in\underline{\mathcal{T}^{s}}}x_{t}\cdot I_{t}\right]\]\[=\mathbb{E}\left[\sum_{t\in\overline{\mathcal{T}^{s}}}x_{t}\cdot I_{t} \cdot\mathbb{1}\{t\in\underline{\mathcal{T}^{s}}\}\right]\] \[=\sum_{t\in[T]}\mathbb{E}\left[x_{t}\cdot I_{t}\cdot\mathbb{1}\{t \in\underline{\mathcal{T}^{s}}\}\right]\] \[=\sum_{t\in[T]}\mathbb{E}\left[\mathbb{E}\left[x_{t}\cdot I_{t} \cdot\mathbb{1}\{t\in\underline{\mathcal{T}^{s}}\}|\mathcal{F}_{t-1}\right]\right]\] (Law of total expectation) \[=\sum_{t\in[T]}\mathbb{E}\left[x_{t}\cdot\mathbb{1}\{t\in \underline{\mathcal{T}^{s}}\}\cdot\mathbb{E}\left[I_{t}|\mathcal{F}_{t-1} \right]\right]\] ( \[x_{t}\] and \[\mathbb{1}\{t\in\underline{\mathcal{T}^{s}}\}\] are \[\mathcal{F}_{t-1}\] -measurable) \[=\sum_{t\in[T]}\mathbb{E}\left[x_{t}\cdot\mathbb{1}\{t\in \underline{\mathcal{T}^{s}}\}\cdot q\right]\] ( \[I_{t}\] is an independent coin-tossing) \[=q\mathbb{E}\left[\sum_{t\in\underline{\mathcal{T}^{s}}}x_{t}\right]\]

and therefore \(\mathbb{E}\left[\sum_{t\in\underline{\mathcal{T}^{s}}}x_{t}\right]\leq 2^{s}/q\). For \(t\in\underline{\mathcal{T}^{s}}\), we further have

\[\min\left\{1,\|A_{t}^{n}\|_{V_{t-1}^{-1}}^{2}\right\} \leq 2\log\left(\frac{\det(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\top})}{ \det(V_{t-1})}\right)\] (due to Lemma D.1) \[=2d\log\left(1+\frac{x_{t}}{q_{t-1}}\right)\] \[\leq 2d\log\left(1+\frac{x_{t}}{2^{s}}\right)\] (since

\[t\in\mathcal{T}^{s}\]

, \[q_{t-1}\geq 2^{s}\] ) \[\leq\frac{2d\cdot x_{t}}{2^{s}}\] (due to

\[\log(1+x)\leq x\]

 for

\[x>0\]

.)

Therefore,

\[\mathbb{E}\Bigg{[}\sum_{t\in\underline{\mathcal{T}^{s}}}\min\left\{1,\|A_{t} ^{n}\|_{V_{t-1}^{-1}}^{2}\right\}\Bigg{]}\leq\mathbb{E}\Bigg{[}\sum_{t\in \underline{\mathcal{T}^{s}}}\frac{2d\cdot x_{t}}{2^{s}}\Bigg{]}=\frac{2d}{2^{ s}}\mathbb{E}\Bigg{[}\sum_{t\in\underline{\mathcal{T}^{s}}}x_{t}\Bigg{]}\leq 2d/q.\]

From definition of \(\overline{\mathcal{T}^{s}}\), if \(I_{t}=1\) and \(t\in\overline{\mathcal{T}^{s}}\), \(\det(V_{t})>2^{d(s+1)}\). Then, for all \(\tau>t\), \(\tau\notin\mathcal{T}^{s}\). Therefore, there is at most one \(t\in\overline{\mathcal{T}^{s}}\) with \(I_{t}=1\). We obtain

\[\mathbb{E}\Bigg{[}\sum_{t\in\overline{\mathcal{T}^{s}}}\min\left\{1,\|A_{t}^{ n}\|_{V_{t-1}^{-1}}^{2}\right\}\Bigg{]}\leq\mathbb{E}\Bigg{[}\big{|}\overline{ \mathcal{T}^{s}}\big{|}\Bigg{]}\leq 1/q,\]

where the last inequality is because with probability \(q\), \(I_{t}=1\). By combining the bounds for \(t\in\overline{\mathcal{T}^{s}}\) and \(t\in\underline{\mathcal{T}^{s}}\) together, lemma follows. 

**Lemma D.5**.: _Let \(Q=\sum_{s\geq o}q_{s}\). Then,_

\[\mathbb{E}\left[\sum_{t\in\mathcal{T}^{s}}\min\left\{1,\|A_{t}^{n}\|_{V_{t-1} ^{-1}}^{2}\right\}\right]\leq 2d/Q+1/Q.\]

Proof.: Let \(\det(V_{t-1})=(q_{t-1})^{d}\). Let \(x_{t}\) satisfies \(\det(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\top})=(q_{t-1}+x_{t})^{d}\). We let \(I_{t}=\mathbb{1}\{I_{t}\geq o\}\). We divide \(\mathcal{T}^{s}\) into two disjoint parts \(\underline{\mathcal{T}^{s}}\) and \(\overline{\mathcal{T}^{s}}\). Specifically,

* for \(\underline{\mathcal{T}^{s}}\), it holds that for \(t\in\underline{\mathcal{T}^{s}}\), \(\det(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\top})\leq 2^{d(s+1)}\).
* for \(\overline{\mathcal{T}^{s}}\), it holds that for \(t\in\overline{\mathcal{T}^{s}}\), \(\det(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\top})>2^{d(s+1)}\).

Note that for \(I_{t}\geq o\),

\[\log(\det(V_{t}))-\log(\det(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\top}))\] \[= \log(\det(V_{t}))-\log(V_{t-1})-\left(\log(\det(V_{t-1}+A_{t}^{o}(A _{t}^{o})^{\top}))-\log(V_{t-1})\right)\] (due to Lemma D.1) \[= \log(1+\left\|A_{t}^{I_{t}}\right\|_{V_{t-1}^{-1}})-\log(1+\left\| A_{t}^{o}\right\|_{V_{t-1}^{-1}})\] \[\geq 0. \text{(due to Lemma D.3)}\]

Therefore, if we let \(\det(V_{t-1}+A_{t}^{I_{t}}(A_{t}^{I_{t}})^{\top})=(q_{t-1}+x_{t}^{\prime})^{d}\) and \(I_{t}\geq o\), then \(x_{t}^{\prime}\geq x_{t}\). Hence,

\[\sum_{t\in\mathcal{T}^{s}}x_{t}\cdot I_{t}\leq 2^{s}.\]

Note that \(\mathbb{P}(I_{t}\geq o)=\sum_{s\geq o}q_{s}=Q\). Let \(\{\mathcal{F}_{t}\}_{t\in[T]}\) be the natural filtration of \(\{A_{t},I_{t},X_{t}\}_{t\in[T]}\). We have

\[2^{s} \geq\mathbb{E}\left[\sum_{t\in\mathcal{T}^{s}}x_{t}\cdot I_{t}\right]\] \[=\mathbb{E}\left[\sum_{t\in[T]}x_{t}\cdot I_{t}\cdot\mathbb{1}\{t \in\mathcal{T}^{s}\}\right]\] \[=\sum_{t\in[T]}\mathbb{E}\left[x_{t}\cdot I_{t}\cdot\mathbb{1}\{t \in\underline{\mathcal{T}^{s}}\}\right]\] \[=\sum_{t\in[T]}\mathbb{E}\left[\mathbb{E}\left[x_{t}\cdot I_{t} \cdot\mathbb{1}\{t\in\underline{\mathcal{T}^{s}}\}|\mathcal{F}_{t-1}\right]\right]\] (Law of total expectation) \[=\sum_{t\in[T]}\mathbb{E}\left[x_{t}\cdot\mathbb{1}\{t\in \underline{\mathcal{T}^{s}}\}\cdot\mathbb{E}\left[I_{t}|\mathcal{F}_{t-1} \right]\right]\] ( \[x_{t}\] and \[\mathbb{1}\{t\in\underline{\mathcal{T}^{s}}\}\] are \[\mathcal{F}_{t-1}\] -measurable) \[=\sum_{t\in[T]}\mathbb{E}\left[x_{t}\cdot\mathbb{1}\{t\in \underline{\mathcal{T}^{s}}\}\cdot Q\right]\] ( \[I_{t}\] is an independent coin-tossing) \[=Q\mathbb{E}\left[\sum_{t\in\mathcal{T}^{s}}x_{t}\right]\]

and therefore

\[\mathbb{E}\left[\sum_{t\in\underline{\mathcal{T}^{s}}}x_{t}\right] \leq 2^{s}/Q.\]

For \(t\in\underline{\mathcal{T}^{s}}\), we further have

\[\min\left\{1,\left\|A_{t}^{o}\right\|_{V_{t-1}^{-1}}^{2}\right\} \leq\min\left\{1,\left\|A_{t}^{I_{t}}\right\|_{V_{t-1}^{-1}}^{2}\right\}\] (due to Lemma D.1) \[\leq 2\log\left(\frac{\det(V_{t-1}+A_{t}^{I_{t}}(A_{t}^{I_{t}})^{ \top})}{\det(V_{t-1})}\right)\] (due to Lemma D.1) \[=2d\log\left(1+\frac{x_{t}}{q_{t-1}}\right)\] \[\leq 2d\log\left(1+\frac{x_{t}}{2^{s}}\right)\] (since

\[t\in\mathcal{T}^{s}\]

,

\[q_{t-1}\geq 2^{s}\]

) \[\leq\frac{2d\cdot x_{t}}{2^{s}}\] (due to

\[\log(1+x)\leq x\]

 for

\[x>0\]

.)Therefore,

\[\mathbb{E}\Bigg{[}\sum_{t\in\overline{\mathcal{T}^{s}}}\min\left\{1,\|A^{o}_{t}\| _{V^{-1}_{t-1}}^{2}\right\}\Bigg{]}\leq\mathbb{E}\Bigg{[}\sum_{t\in\overline{ \mathcal{T}^{s}}}\frac{2d\cdot x_{t}}{2^{s}}\Bigg{]}=\frac{2d}{2^{s}}\mathbb{E} \Bigg{[}\sum_{t\in\overline{\mathcal{T}^{s}}}x_{t}\Bigg{]}\leq 2d/Q.\]

From definition of \(\overline{\mathcal{T}^{s}}\), if \(I_{t}=1\) and \(t\in\overline{\mathcal{T}^{s}}\), then \(I_{t}\geq o\) and

\[\det(V_{t})=\det(V_{n-1}+A^{I_{t}}_{t}(A^{I_{t}}_{t})^{\top})>\det(V_{n-1}+A^{ o}_{t}(A^{o}_{t})^{\top})>(s+1)^{d}.\]

Then, for all \(\tau>t\), \(\tau\notin\mathcal{T}^{s}\). Therefore, there is at most one \(t\in\overline{\mathcal{T}^{s}}\) with \(I_{t}=1\). We obtain

\[\mathbb{E}\Bigg{[}\sum_{t\in\overline{\mathcal{T}^{s}}}\min\left\{1,\|A^{o}_{ t}\|_{V^{-1}_{t-1}}^{2}\right\}\Bigg{]}\leq\mathbb{E}\bigg{[}\bigg{|}\overline{ \mathcal{T}^{s}}\bigg{|}\bigg{]}\leq 1/Q,\]

where the last inequality is because with probability \(\sum_{s\geq o}q_{s}=Q\), \(I_{t}=1\). By combining the bounds for \(t\in\overline{\mathcal{T}^{s}}\) and \(t\in\underline{\mathcal{T}^{s}}\) together, lemma follows. 

## Appendix E Experimental details

The code used in the experiments can be found in the following repository: https://github.com/jajajang/sparsity_agnostic_model_selection.

### Settings common to all algorithms

* Arm set: \(\mathcal{A}_{t}=\mathcal{A}\) for all \(t\in[T]\). \(\mathcal{A}\subset\mathbb{S}^{d-1}\) (the unit sphere in \(\mathbb{R}^{d}\)) is a set of \(d\)-dimensional vectors drawn independently and uniformly at random from \(\mathbb{S}^{d-1}\), with \(d=16\) and \(|\mathcal{A}|=30\).
* \(\theta_{*}\) is an \(S\)-sparse (\(S=1,2,4,8,16\)) vector generated as follows: before the game starts, draw \((\theta_{*})_{1},\ldots,(\theta_{*})_{S}\sim\mathbb{S}^{S-1}\), and \((\theta_{*})_{k}=0\) for all \(k>S\).
* The noise on rewards: \(\{\varepsilon_{t}\}_{t\in[T]}\) are i.i.d. with \(\xi_{t}\sim\mathrm{Unif}([-1,1])\).
* Number of iterations: \(T=10^{4}\).
* Number of models: \(n=6\).
* Radius of confidence sets: \(\alpha_{0}=0\), and \(\alpha_{i}=2^{i}\log t\) for \(i=1,\cdots,5\).
* Prior distribution \(\{q_{s}\}_{s\in[6]}\)
* For \(\_\)Unif\(,\{q_{s}\}_{s\in[6]}=\left(\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right)\)
* For \(\_\)Theory, \(\{q_{s}\}_{s\in[6]}=\left(\frac{C}{2},\frac{C}{4},\frac{C}{8},\frac{C}{16}, \frac{C}{32},\frac{C}{64}\right)\) where \(C=\frac{63}{64}\) is a normalizing constant.
* Each plot is the result of 20 repetitions for each method. The shade represents the 1-standard deviation bound.
* Type 21HL * CPU: 13th Gen Intel(R) Core(TM) i7-1360P 2.20 GHz * RAM: 32GB
* Computation time: total 1338.38 seconds.

### AdaLinUCB details

* Since we empirically observed that Exp3 provided enough exploration, we aggressively set the forced exploration parameter \(q\) to zero.
* The learning rate of Exp3 was set to \(\eta_{t}=2\sqrt{\frac{\log n}{nt}}\), see [5].
* Given the prior distribution \(\{q_{s}\}_{s\in[6]}\), we set \(P_{t}\) as follows: \[P_{t,s}=\frac{q_{s}\exp\left(\eta_{t}S_{t,s}\right)}{\sum_{j=1}^{n}q_{j}\exp \left(\eta_{t}S_{t,j}\right)}\]

### Ofult details

* We used the log-determinant form of the confidence set based on Abbasi-Yadkori et al. [1, Theorem 2], which gives the choice \[\sqrt{\gamma_{t}}=\sqrt{2\log T+\log\det(V_{t})}+1\] for the parameter \(\gamma_{t}\) in (1.1) when \(\lambda=1\) and \(\delta=1/T\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims presented in the abstract and introduction accurately represent the contributions and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have added a paragraph in the conclusions section discussing the limitations of our work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide the full set of assumptions and complete proofs. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We include our code in our supplementary material and will make the full code public if this work gets accepted. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the details of our experimental setting. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We reported 1-sigma error bars in our plots. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We state all information on the computer resources in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.