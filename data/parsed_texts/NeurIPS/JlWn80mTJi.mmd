# The Implicit Bias of Gradient Descent on Separable Multiclass Data

Hrithik Ravi\({}^{1}\)  Clayton Scott\({}^{1}\)  Daniel Soudry\({}^{2}\)  Yutong Wang\({}^{3}\)

\({}^{1}\)University of Michigan \({}^{2}\)Technion - Israel Institute of Technology

\({}^{3}\)Illinois Institute of Technology

{hrithikr, clayscot}@umich.edu

daniel.soudry@gmail.com

ywang562@iit.edu

###### Abstract

Implicit bias describes the phenomenon where optimization-based training algorithms, without explicit regularization, show a preference for simple estimators even when more complex estimators have equal objective values. Multiple works have developed the theory of implicit bias for binary classification under the assumption that the loss satisfies an _exponential tail property_. However, there is a noticeable gap in analysis for multiclass classification, with only a handful of results which themselves are restricted to the cross-entropy loss. In this work, we employ the framework of Permutation Equivariant and Relative Margin-based (PERM) losses (Wang and Scott, 2024) to introduce a multiclass extension of the exponential tail property. This class of losses includes not only cross-entropy but also other losses. Using this framework, we extend the implicit bias result of Soudry et al. (2018) to multiclass classification. Furthermore, our proof techniques closely mirror those of the binary case, thus illustrating the power of the PERM framework for bridging the binary-multiclass gap.

## 1 Introduction

Overparameterized models such as neural networks have shown state-of-the-art performance in many applications, despite having the potential to overfit. Zhang et al. (2021) demonstrate that this potential is indeed realizable by training real-world models to fit random noise. In recent years, there have been several research efforts that aim to understand the impressive performance of overparametrized models despite this ability to overfit. Both the model architecture and the training algorithms for selecting the weights have been investigated in this regard.

Work on _implicit bias_(Soudry et al., 2018; Ji et al., 2020; Vardi, 2022) has focused on the latter factor. Implicit bias is the hypothesis that gradient-based methods have a built-in preference for models with low-complexity. This hypothesis is perhaps best understood in the setting of (unregularized) empirical risk minimization for learning a linear model under the assumption of linearly separable data. Soudry et al. (2018) showed that in binary classification, implicit bias holds when the loss has the exponential tail property (Soudry et al., 2018, Theorem 3). The same work also demonstrated implicit bias in the multiclass setting for the cross-entropy loss, but implicit bias for a more broadly defined class of losses in the multiclass case is left open. In this work, we extend the notion of the exponential tail property to multiclass losses and prove that the property is sufficient for implicit bias to occur in the multiclass setting. Toward this end, we employ the framework of permutation equivariant and relative margin-based (PERM) losses (Wang and Scott, 2024).

### Contributions

Multiclass extension of the exponential tail property (Definition 2.2)It is unclear how the exponential tail property for binary margin losses should be extended to the multiclass setting. By using the PERM framework, we provide a multiclass extension that generalizes the exponential tail property to multiclass (Definition 2.2 in Section 2.3). We further verify that this property holds for some common losses.

Sufficiency of the exponential tail property for implicit bias (Theorem 3.4)We prove that the proposed multiclass exponential tail property is sufficient for implicit bias. More precisely, we show in Theorem 3.4 that for almost all linearly separable multiclass datasets, given a convex, (\(\beta\)-smooth, strictly decreasing) PERM loss satisfying the exponential tail property in Definition 2.2, gradient descent exhibits directional convergence to the hard-margin multiclass SVM.

### Related Work

Soudry et al. (2018) show that gradient descent, applied to _unregularized_ empirical risk minimization, converges to the hard-margin SVM solution at a slow logarithmic rate, provided the loss satisfies the exponential tail property (defined below). Nacson et al. (2019) improve the convergence rate using a specific step-size schedule. Ji and Telgarsky (2019) extend implicit bias to the setting of _quasi-complete separation_(Candes and Sur, 2020), where the two classes are linearly separated but with a margin of zero. Many works have also considered gradient-based methods beyond gradient descent. For example, Gunasekar et al. (2018) examine the implicit bias effects of mirror descent (Beck and Teboulle, 2003), steepest descent (Boyd and Vandenberghe, 2004), and _adaptive_ gradient descent (Duchi et al., 2011; Kingma and Ba, 2015). Cotter et al. (2012); Clarkson et al. (2012); Ji et al. (2021) study first order methods that are designed specifically to approach the hard-margin SVM as quickly as possible.

Results for the multiclass setting are more scarce, and are _always_ specific to cross-entropy. Soudry et al. (2018) establish implicit bias for cross-entropy loss. Lyu and Li (2019) focus on homogeneous predictors and prove convergence of GD on cross-entropy loss to a KKT point of the margin-maximization problem. Lyu and Li (2019) proves convergence of gradient flow to a generalized max-margin classifier for multiclass classification with cross-entropy loss using homogeneous models.1 In the special case when the model are linear classifiers, the generalized max-margin classifier reduces to the classical hard-margin SVM. Lyu et al. (2021) consider two-layer neural networks and prove convergence of GD on cross-entropy loss to the max-margin solution under an additional assumption on the data, that both \(\mathbf{x}\) and its negative counterpart \(-\mathbf{x}\) must belong to the dataset. Wang et al. (2023) prove that in certain overparameterized regimes, gradient descent on squared loss leads to an equivalent solution to gradient descent on cross-entropy loss.

Footnote 1: Lyu and Li (2019) could be thought of as analyzing losses beyond CE, but the optimization problem would be non-convex so convergence might not be to a global minimum. See Appendix A for a more detailed discussion.

Beyond work establishing (rate of) convergence to the max-margin classifier, there is also a separate line of work (Shamir, 2021; Schliserman and Koren, 2022, 2023) focusing on the _generalization_ aspect of implicit bias. These works examine the binary classification setting, with the exception of Schliserman and Koren (2022) who consider cross-entropy.

### Notations

Let \(K\geq 2\) and \(d\geq 1\) denote the number of classes and feature space dimension, respectively. Let \([K]:=\{1,2,\ldots,K\}\). Vectors are denoted by boldface lowercase letters, e.g., \(\mathbf{v}\in\mathbb{R}^{K}\) whose entries are denoted by \(v_{j}\) for \(j\in[K]\). Likewise, matrices are denoted by boldface uppercase letters, e.g., \(\mathbf{W}\in\mathbb{R}^{d\times K}\). The columns of \(\mathbf{W}\) are denoted \(\mathbf{w}_{1},\ldots,\mathbf{w}_{K}\). By \(\mathbf{0}_{n}\) and \(\mathbf{1}_{n}\) we denote the \(n\)-dimensional vectors of all 0's and all 1's respectively. The \(n\times n\) identity matrix is denoted by \(\mathbf{I}_{n}\).

By \(\|\mathbf{v}\|\) we denote the Euclidean norm of vector \(\mathbf{v}\). \(\|\mathbf{A}\|_{2}\) is the spectral norm of matrix \(\mathbf{A}\). Given two vectors \(\mathbf{w},\mathbf{v}\in\mathbb{R}^{k}\), we write \(\mathbf{w}\succeq\mathbf{v}\) (resp. \(\mathbf{w}\succ\mathbf{v}\)) if \(w_{j}\geq v_{j}\) (resp. \(w_{j}>v_{j}\)) for all \(j\in[k]\); similarly we write \(\mathbf{w}\preceq\mathbf{v}\) (resp. \(\mathbf{w}\prec\mathbf{v}\)) if \(w_{j}\leq v_{j}\) (resp. \(w_{j}<v_{j}\)) for all \(j\in[k]\). On the otherhand, if \(\mathbf{A}\) and \(\mathbf{B}\) are equally-sized _symmetric matrices_, then by \(\mathbf{A}\succeq\mathbf{B}\) (resp. \(\mathbf{A}\preceq\mathbf{B}\)) we mean that \(\mathbf{A}-\mathbf{B}\) (resp. \(\mathbf{B}-\mathbf{A}\)) is positive semi-definite, i.e. \(\mathbf{A}-\mathbf{B}\succeq 0\) (resp. \(\mathbf{B}-\mathbf{A}\succeq 0\)).

A bijection from \([k]\) to itself is called a permutation on \([k]\). Denote by \(\mathtt{Sym}(k)\) the set of all permutations on \([k]\). For each \(\sigma\in\mathtt{Sym}(k)\), let \(\mathbf{S}_{\sigma}\) denote the permutation matrix corresponding to \(\sigma\). In other words, if \(\mathbf{v}\in\mathbb{R}^{k}\) is a vector, then \([\mathbf{S}_{\sigma}\mathbf{v}]_{j}=\mathbf{v}_{\sigma(j)}\).

## 2 Multiclass Loss Functions

In multiclass classification, a classifier is typically represented in terms of a _class-score function_\(f=(f_{1},\ldots,f_{K}):\mathbb{R}^{d}\rightarrow\mathbb{R}^{K}\), which maps an input \(\mathbf{x}\in\mathbb{R}^{d}\) to a vector \(\mathbf{v}:=f(\mathbf{x})\) of class scores. For instance, \(f\) may be a feed-forward neural network and \(\mathbf{v}\) in this context is sometimes referred to as the logits. The label set is \([K]\), and a label is predicted as \(\operatorname*{argmax}_{j}f_{j}(\mathbf{x})\). A \(K\)-ary multiclass loss function is a vector-valued function \(\mathcal{L}=(\mathcal{L}_{1},\ldots,\mathcal{L}_{K}):\mathbb{R}^{K}\rightarrow \mathbb{R}^{K}\) where \(\mathcal{L}_{y}(f(\mathbf{x}))\) is the loss incurred for outputting \(f(\mathbf{x})\) when the ground truth label is \(y\).

In binary classification, a classifier is typically represented using a function \(g:\mathbb{R}^{d}\rightarrow\mathbb{R}\). The label set is \(\{-1,1\}\), and labels are predicted as \(\mathbf{x}\mapsto\operatorname*{sign}(g(\mathbf{x}))\). A _binary margin loss_ is a function of the form \(\psi:\mathbb{R}\rightarrow\mathbb{R}\) where \(\psi(yg(\mathbf{x}))\) is the loss incurred for outputting \(g(\mathbf{x})\) when the ground truth label is \(y\). Margin losses have been central to the development of the theory of binary classification, and the lack of a multiclass counterpart to binary margin losses may have impaired the development of corresponding theory for multiclass classification. To address this issue, Wang and Scott (2024) introduce PERM losses as a bridge between binary and multiclass classification.

### Permutation equivariant and relative margin-based (PERM) losses

Assume the label set is \([K]\). Define 2 the matrix \(\mathbf{D}:=[-\mathbf{I}_{K-1}\quad\mathbf{1}_{K-1}]\in\mathbb{R}^{(K-1) \times K}\). Observe that \(\mathbf{D}\mathbf{v}=(v_{K}-v_{1},v_{K}-v_{2},\ldots,v_{K}-v_{K-1})^{\top}\) for all \(\mathbf{v}\in\mathbb{R}^{K}\).

Footnote 2: Also see (Wang and Scott, 2024, Definition 2).

**Definition 2.1** (PERM loss (Wang and Scott, 2024)).: _Let \(K\geq 2\) be an integer, and \(\mathcal{L}\) be a \(K\)-ary multiclass loss function. We say that \(\mathcal{L}\) is_

1. permutation equivariant _if_ \(\mathcal{L}(\mathbf{S}_{\sigma}\mathbf{v})=\mathbf{S}_{\sigma}\mathcal{L}( \mathbf{v})\) _for all_ \(\mathbf{v}\in\mathbb{R}^{K}\) _and_ \(\sigma\in\mathtt{Sym}(K)\)_,_
2. relative margin-based _if for each_ \(y\in[K]\) _there exists a function_ \(\ell_{y}:\mathbb{R}^{K-1}\rightarrow\mathbb{R}\) _so that_ \(\mathcal{L}_{y}(\mathbf{v})=\ell_{y}(\mathbf{D}\mathbf{v})=\ell_{y}(v_{K}-v_{ 1},v_{K}-v_{2},\ldots,v_{K}-v_{K-1})\)_, for all_ \(\mathbf{v}\in\mathbb{R}^{K}\)_. We refer to the vector-valued function_ \(\ell:=(\ell_{1},\ldots,\ell_{K})\) _as the_ reduced form _of_ \(\mathcal{L}\)_._
3. PERM _if_ \(\mathcal{L}\) _is both permutation equivariant and relative margin-based. In this case, the function_ \(\psi:=\ell_{K}\) _is referred to as the_ template _of_ \(\mathcal{L}\)_._

Wang and Scott (2024) show that PERM losses are characterized by their template \(\psi\). To show this, they introduce the _matrix label code_, an encoding of labels as matrices. Thus, for each \(y\in[K-1]\), let \(\boldsymbol{\Upsilon}_{y}\) be the \((K-1)\times(K-1)\) identity matrix, but with the \(y\)-th column replaced by all \(-1\)'s. For \(y=K\), let \(\boldsymbol{\Upsilon}_{y}\) be the identity matrix. Note that when \(K=2\), this definition reduces to \(\boldsymbol{\Upsilon}_{y}=(-1)^{y}\), the standard encoding of labels in the binary setting. Observe that (after permutation) \(\boldsymbol{\Upsilon}_{y}\mathbf{D}\mathbf{v}=(v_{y}-v_{1},v_{y}-v_{2},\ldots,v_ {y}-v_{K})^{\top}\in\mathbb{R}^{K-1}\), where the \(v_{y}-v_{y}=0\) entry is omitted. Please see Wang and Scott (2024, Lemma B.2) for a simple proof.

**Theorem 2.1** (Wang and Scott (2024)).: _Let \(\mathcal{L}:\mathbb{R}^{K}\rightarrow\mathbb{R}^{K}\) be a PERM loss with template \(\psi\), and let \(v\in\mathbb{R}^{K}\) and \(y\in[K]\) be arbitrary. Then \(\psi\) is a symmetric function. Moreover,_

\[\mathcal{L}_{y}\left(\mathbf{v}\right)=\psi\left(\boldsymbol{\Upsilon}_{y} \mathbf{D}\mathbf{v}\right).\] (1)

_Conversely, let \(\psi:\mathbb{R}^{K-1}\rightarrow\mathbb{R}\) be a symmetric function. Define a multiclass loss function \(\mathcal{L}=(\mathcal{L}_{1},\ldots,\mathcal{L}_{k}):\mathbb{R}^{K}\rightarrow \mathbb{R}^{K}\) according to Eqn. (1). Then \(\mathcal{L}\) is a PERM loss with template \(\psi\)._

Theorem 2.1 shows that a PERM loss is characterized by its template \(\psi\). The right hand side of Eqn. (1) is referred to as the _relative margin form_ of the loss, which extends binary margin losses to multiclass. As noted by Wang and Scott (2024), an advantage of the relative margin form is that it decouples the labels from the predicted scores, which facilitates analysis. Our results below support this understanding.

Many losses in the literature are PERM losses, including the cross-entropy loss whose template is \(\psi(\mathbf{u})=\log(1+\sum_{i=1}^{K-1}\exp(-u_{i}))\), the multiclass exponential loss (Mukherjee and Schapire, 2013) whose template is \(\psi(\mathbf{u})=\sum_{i=1}^{K-1}\exp(-u_{i})\), and the PairLogLoss (Wang et al., 2022) whose template is = \(\psi(\mathbf{u})=\sum_{i=1}^{K-1}\log(1+\exp(-u_{i}))\). See Wang and Scott (2024) for other examples.

### Regularity assumptions on loss functions

Let \(\mathcal{L}\) be a PERM loss with differentiable template \(\psi\). If

\[\frac{\partial\psi}{\partial u_{i}}\left(\mathbf{u}\right)<0,\quad\text{ for all }i\in\{1,2\ldots,K-1\}\text{, }\mathbf{u}\in\mathbb{R}^{K-1}\text{,}\]

i.e., the gradient of the template is entrywise strictly negative, then we say that the PERM loss \(\mathcal{L}\) is _strictly decreasing_. In this case, we write \(\nabla\psi\prec\mathbf{0}\), where \(\mathbf{0}\) is the 0-vector. If the template is differentiable, then it is convex if:

\[\psi(\mathbf{u_{1}})\geq\psi(\mathbf{u_{2}})+\nabla\psi(\mathbf{u_{2}})^{ \top}(\mathbf{u_{1}}-\mathbf{u_{2}}),\quad\text{ for all }\mathbf{u_{1}},\mathbf{u_{2}}\in \mathbb{R}^{K-1}\text{.}\]

If \(\psi\) is twice-differentiable, this is equivalent to saying that the Hessian is positive-semidefinite:

\[\nabla^{2}\psi\left(\mathbf{u}\right)\succeq 0\quad\text{ for all }\mathbf{u}\in\mathbb{R}^{K-1}\text{.}\]

Finally, the template is said to be \(\beta\)-smooth if its gradient is \(\beta\)-Lipschitz:

\[\|\nabla\psi\left(\mathbf{u_{1}}\right)-\nabla\psi\left(\mathbf{u_{2}}\right) \|\leq\beta\|\mathbf{u_{1}}-\mathbf{u_{2}}\|,\quad\text{ for all }\mathbf{u_{1}},\mathbf{u_{2}}\in \mathbb{R}^{K-1}\text{.}\]

If \(\psi\) is twice-differentiable, this is equivalent to saying that the maximum eigenvalue of its Hessian is bounded by \(\beta\):

\[\|\nabla^{2}\psi\left(\mathbf{u}\right)\|_{2}\leq\beta\quad\text{ for all }\mathbf{u}\in\mathbb{R}^{K-1}\text{,}\]

where \(\|\mathbf{A}\|_{2}\) is the spectral norm of matrix \(\mathbf{A}\).

### Multiclass analogue of exponential tail property

In the binary setting, the exponential tail property defined in prior work (Soudry et al. (2018); Nacson et al. (2019); Ji et al. (2020)) is assumed to hold for the negative _derivative_ of the loss. Similarly, in the multiclass setting we are interested in bounding the negative _gradient_ of the PERM loss template.

**Definition 2.2** (Multiclass exponential tail property).: _A multiclass PERM loss with template \(\psi:\mathbb{R}^{K-1}\to\mathbb{R}\) has the exponential tail (ET) property if there exist \(u_{+},u_{-}\in\mathbb{R}\) and positive \(c>0\) such

Figure 1: An illustration of the exponential tail property for the cross entropy/multinomial logistic loss when \(K=3\). _Panel a._ Plot of \(\psi(\mathbf{u})=\log(1+\exp(-u_{1})+\exp(-u_{2}))\), the template for the multinomial logistic loss. Note that the complement of the positive orthant in the domain \(\mathbb{R}^{2}\) is shown in gray. _Panel b. and c._ Plot of the upper bound (shown in black) and lower bounds (red) of \(-\frac{\partial\psi}{\partial u_{1}}\) (blue) respectively. These bounds are from Appendix C.1.3 where \(u_{\pm}=0\) and \(c=1\). Note that the lower bound is valid in the positive orthant, i.e., the red surface is below the blue one there.

_that for all \(i\in[K-1]\) the following holds:_

\[\forall\mathbf{u}\text{ s.t. }\min_{j\in[K-1]}u_{j}>u_{+},\text{ we have }-\frac{\partial\psi}{\partial u_{i}}\left(\mathbf{u}\right)\leq c\exp(-u_{i}), \quad\text{and}\] \[\forall\mathbf{u}\text{ s.t. }\min_{j\in[K-1]}u_{j}>u_{-},\text{ we have }-\frac{\partial\psi}{ \partial u_{i}}\left(\mathbf{u}\right)\geq c\left(1-\sum_{j\in[K-1]}\exp\left( -u_{j}\right)\right)\exp(-u_{i}).\]

**Remark 2.2**.: _We show in Appendix C that cross-entropy (CE), multiclass exponential loss, and PairLogLoss all have this property._

## 3 Main Result

Consider a dataset \(\left\{\left(\mathbf{x}_{n},y_{n}\right)\right\}_{n=1}^{N}\), with \(\mathbf{x}_{n}\in\mathbb{R}^{d}\) and class labels \(y_{n}\in[K]:=\left\{1,\ldots,K\right\}\). The class score function for class \(k\) is \(f_{k}(\mathbf{x})=\mathbf{w}_{k}^{T}\mathbf{x}\). Define \(\mathbf{X}\in\mathbb{R}^{d\times N}\) to be the matrix whose \(n\)th column is \(\mathbf{x}_{n}\). Define \(\mathbf{W}\in\mathbb{R}^{d\times K}\) to be the matrix whose \(k\)th column is \(\mathbf{w}_{k}\). The learning objective is

\[\mathcal{R}\left(\mathbf{W}\right)=\sum_{n=1}^{N}\mathcal{L}_{y_{n}}\left( \mathbf{W}^{\top}\mathbf{x}_{n}\right)\,.\] (2)

From Eqn. 1, if \(\mathcal{L}\) is a PERM loss, then \(\mathcal{L}_{y}\left(\mathbf{v}\right)=\psi\left(\mathbf{\Upsilon}_{y}\mathbf{ D}\mathbf{v}\right)\), and the learning objective becomes

\[\mathcal{R}(\mathbf{W})=\sum_{i=1}^{N}\psi\left(\mathbf{\Upsilon}_{y_{i}} \mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}\right)\,.\] (3)

Up to permuting the entries, \(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}\) is equal to the \((K-1)\)-dimensional vector of relative-margins \(\left[\left(\mathbf{w}_{y_{i}}-\mathbf{w}_{1}\right)^{\top}\mathbf{x}_{i}\right, \left(\mathbf{w}_{y_{i}}-\mathbf{w}_{2}\right)^{\top}\mathbf{x}_{i},\,\ldots, \,\left(\mathbf{w}_{y_{i}}-\mathbf{w}_{K}\right)^{\top}\mathbf{x}_{i}\right]^ {\top}\), where the \(0\)-valued entry \(\left(\mathbf{w}_{y_{i}}-\mathbf{w}_{y_{i}}\right)^{\top}\mathbf{x}_{i}\) is omitted. This follows from Wang and Scott (2024, Lemma B.2).

We are now ready to state our assumptions on the loss:

**Assumption 3.1**.: _The PERM loss's template \(\psi\) is convex, \(\beta\)-smooth, strictly decreasing and non-negative. 34_

Footnote 3: We note that in the binary case the implicit bias result in (Soudry et al., 2018, Theorem 3) does not require the loss to be convex. Closing this binary-multiclass gap is an open question.

Footnote 4: Note that multiclass exponential loss \(\psi\left(\mathbf{u}\right)\) does not have a global smoothness constant. However, we show in Appendix C.2.2 that any learning rate \(\eta<1/\left(B^{2}\mathcal{R}\left(\mathbf{w}(0)\right)\right)\) is sufficient for the gradient descent iterates to achieve local smoothness, where \(B=\sqrt{(2K-2)}\sum_{i=1}^{N}\left\|\mathbf{x}_{i}\right\|\).

**Assumption 3.2**.: _The PERM loss has exponential tail as defined in Definition 2.2._

To optimize Eqn. (2) we employ gradient descent with fixed learning rate \(\eta\). Define \(\mathbf{w}:=\mathsf{vec}(\mathbf{W})\) where \(\mathsf{vec}\) denotes vectorization by column-stacking (See Definition B.1), and let the gradient descent iterate at time \(t\) be \(\mathbf{w}\left(t\right)\). Then:

\[\mathbf{w}\left(t+1\right)=\mathbf{w}\left(t\right)-\eta\nabla\mathcal{R}\left( \mathbf{w}(t)\right).\]

Define the "matrix-version" of the trajectory \(\mathbf{W}(t)\in\mathbb{R}^{d\times K}\) such that \(\mathbf{w}(t)=\mathsf{vec}(\mathbf{W}(t))\). Throughout this work, we frequently work with the risk as a _matrix_-input scalar-output function \(\mathcal{R}(\mathbf{W})\), and as a _vector_-input scalar-output function \(\mathcal{R}(\mathbf{w})\).

These two formulations will each be useful in different situations. For instances, adopting the matrix perspective can facilitate calculation of bounds, e.g., in Section 4.2. On the other hand, the vectorized formulation is easier for defining the Hessian of the risk \(\nabla^{2}\mathcal{R}(\mathbf{w})\). See Appendix B for detail.

We focus on linearly separable datasets:

**Assumption 3.3**.: _The dataset is linearly separable, i.e. there exists \(\mathbf{w}\in\mathbb{R}^{dK}\) such that \(\forall n\in[N],\forall k\in[K]\backslash\{y_{n}\}:\mathbf{w}_{y_{n}}^{\top} \mathbf{x}_{n}\geq\mathbf{w}_{k}^{\top}\mathbf{x}_{n}+1\). Equivalently, there exists \(\mathbf{W}\in\mathbb{R}^{d\times K}\) such that \(\forall n\in[N],\ \mathbf{\Upsilon}_{y_{n}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{n} \succeq\mathbf{1}\)._Finally, let \(\hat{\mathbf{w}}\) be the multiclass hard-margin SVM solution for the linearly separable dataset:

\[\hat{\mathbf{w}}=\operatorname*{argmin}_{\mathbf{w}}\,\tfrac{1}{2}\|\mathbf{w} \|^{2}\;\;\;\text{s.t.}\,\forall n,\forall k\neq y_{n}:\mathbf{w}_{y_{n}}^{ \top}\mathbf{x}_{n}\geq\mathbf{w}_{k}^{\top}\mathbf{x}_{n}+1.\] (4)

Now we state the main result of the paper:

**Theorem 3.4**.: _For any PERM loss satisfying Assumptions 3.1 and 3.2, for all linearly separable datasets such that Assumption 4.1 holds, any sufficiently small learning rate \(0<\eta<2\beta^{-1}\sigma_{\text{max}}^{-2}\left(\mathbf{X}\right)\), and any initialization \(\mathbf{w}(0)\), the iterates of gradient descent will behave as_

\[\mathbf{w}(t)=\hat{\mathbf{w}}\log(t)+\boldsymbol{\rho}(t)\]

_where the norm of the residual, \(\|\boldsymbol{\rho}(t)\|\), is bounded. This implies a directional convergence behavior:_

\[\lim_{t\rightarrow\infty}\frac{\mathbf{w}\left(t\right)}{\|\mathbf{w}\left(t \right)\|}=\frac{\hat{\mathbf{w}}}{\|\hat{\mathbf{w}}\|}.\]

In Appendix I, we show experimental results demonstrating implicit bias towards the hard margin SVM when using the PairLogLoss, in line with Theorem 3.4.

## 4 Proof Sketch

In this section we will overview the proof of the result. Along the way, we prove lemmas that extend to the multiclass setting results from Soudry et al. (2018). The extensions are facilitated by the PERM framework, in particular the relative margin from of the loss.

We adopt the notation of Soudry et al. (2018) where possible throughout this proof. Recalling the notation and definitions from the paper: let us define the standard basis \(\mathbf{e}_{k}\in\mathbb{R}^{K}\) such that \(\left(\mathbf{e}_{k}\right)_{i}=\delta_{ki}\) (where \(\delta\) is the Kronecker-delta function), and the \(d\)-dimension identity matrix \(\mathbf{I}_{d}\). Define \(\mathbf{A}_{k}\in\mathbb{R}^{dk\times d}\) as the Kronecker product between \(\mathbf{e}_{k}\) and \(\mathbf{I}_{d}\), i.e. \(\mathbf{A}_{k}=\mathbf{e}_{k}\otimes\mathbf{I}_{d}\). We can then relate the original \(k^{th}\)-class predictor \(\mathbf{w}_{k}\) to the long column-vector \(\mathbf{w}\) as follows: \(\mathbf{A}_{k}^{\top}\mathbf{w}=\mathbf{w}_{k}\). Next define \(\tilde{\mathbf{x}}_{n,k}:=\left(\mathbf{A}_{y_{n}}-\mathbf{A}_{k}\right) \mathbf{x}_{n}\). Using this notation, the multiclass SVM becomes

\[\operatorname*{argmin}_{\mathbf{w}}\,\tfrac{1}{2}\|\mathbf{w}\|^{2}\;\;\;\; \text{s.t.}\;\;\;\;\forall n,\forall k\neq y_{n}:\mathbf{w}^{\top}\tilde{ \mathbf{x}}_{n,k}\geq 1\] (5)

For each \(k\in[K]\), define \(\mathcal{S}_{k}=\operatorname*{arg\,min}_{n}(\tilde{\mathbf{w}}_{y_{n}}- \tilde{\mathbf{w}}_{k})^{\top}\mathbf{x}_{n}=\{n:(\hat{\mathbf{w}}_{y_{n}}- \tilde{\mathbf{w}}_{k})^{\top}\mathbf{x}_{n}=1\}\), i.e., the \(k^{th}\) class support vectors. From the KKT optimality conditions for Eqn. (5), we have for some dual variables \(\alpha_{n,k}>0\) that

\[\hat{\mathbf{w}}=\sum_{n=1}^{N}\sum_{k=1}^{K}\alpha_{n,k}\tilde{\mathbf{x}}_{ n,k}\mathbbm{1}_{n\in\mathcal{S}_{k}}.\] (6)

Finally, define

\[\mathbf{r}\left(t\right)=\mathbf{w}\left(t\right)-\log\left(t\right)\hat{ \mathbf{w}}-\tilde{\mathbf{w}}\] (7)

where \(\tilde{\mathbf{w}}\) is a solution to

\[\forall k\in[K],\forall n\in\mathcal{S}_{k}:\,\eta\exp\left(-\mathbf{x}_{n}^ {\top}\left(\tilde{\mathbf{w}}_{y_{n}}-\tilde{\mathbf{w}}_{k}\right)\right)= \alpha_{n,k}.\] (8)

In Soudry et al. (2018), the existence of \(\tilde{\mathbf{w}}\) is proven for the binary case for almost all datasets, and assumed in the multiclass case. Here, we also state the existence of \(\tilde{\mathbf{w}}\) as an additional assumption:

**Assumption 4.1**.: _Eqn. 8 has a solution, denoted \(\tilde{\mathbf{w}}\)._

We pose the problem of proving Assumption 4.1 for almost all datasets as a conjecture in Appendix H, where we also show experimentally that on a large number (100 instances for each choice of \(d\in\{2,3,4,5,6\}\) and \(K\in\{3,4,5,6\}\)) of synthetically generated linearly separable datasets, Assumption 4.1 indeed holds.

Note that \(\mathbf{r}(t)=\boldsymbol{\rho}(t)-\tilde{\mathbf{w}}\), and \(\tilde{\mathbf{w}}\) is independent of \(t\), so bounding \(\mathbf{r}(t)\) is equivalent to bounding \(\boldsymbol{\rho}(t)\). Following the same steps as Soudry et al. (2018, Appendix E.3):

\[\|\mathbf{r}\left(t+1\right)\|^{2}-\|\mathbf{r}\left(t\right)\|^{2}=\underbrace {\|\mathbf{r}\left(t+1\right)-\mathbf{r}\left(t\right)\|^{2}}_{\text{First Term}}+2\underbrace{\left(\mathbf{r}\left(t+1 \right)-\mathbf{r}\left(t\right)\right)^{\top}\mathbf{r}\left(t\right)}_{\text{ Second Term}}\] (9)

The high-level approach is to bound the two terms of the above expansion for \(\mathbf{r}(t)\) and then use a telescoping argument to bound \(\mathbf{r}(t)\) for all \(t>0\). Below we provide the main arguments; for a complete proof of the second term's bound, please refer to Appendix F.

### Bounding the First Term

Using \(\log(1+x)\leq x\) for all \(x>0\), we expand the first term as follows:

\[\left\|\mathbf{r}\left(t+1\right)-\mathbf{r}\left(t\right)\right\|^ {2} \leq\eta^{2}\left\|\nabla\mathcal{R}\left(\mathbf{w}\left(t\right) \right)\right\|^{2}+\left\|\hat{\mathbf{w}}\right\|^{2}t^{-2}+2\eta\hat{ \mathbf{w}}^{\top}\nabla\mathcal{R}\left(\mathbf{w}\left(t\right)\right)\log \left(1+t^{-1}\right)\] \[\leq\eta^{2}\left\|\nabla\mathcal{R}\left(\mathbf{w}\left(t \right)\right)\right\|^{2}+\left\|\hat{\mathbf{w}}\right\|^{2}t^{-2}\]

Obtaining the second inequality requires proving that

\[2\eta\hat{\mathbf{w}}^{\top}\nabla\mathcal{R}\left(\mathbf{w}\left(t\right) \right)\log\left(1+t^{-1}\right)\leq 0,\text{or equivalently, }\hat{\mathbf{w}}^{\top}\nabla \mathcal{R}\left(\mathbf{w}\left(t\right)\right)<0\] (10)

We will spend the rest of this subsection going over the complete proof of this inequality.

First we state the following lemma (derived in Appendix B.2) that gives us a useful expression for the gradient of the risk w.r.t. \(\mathbf{W}\):

**Lemma 4.2**.: _For any \(\mathbf{W}\in\mathbb{R}^{d\times K}\), we have that \(\nabla\mathcal{R}(\mathbf{W})=\sum_{i=1}^{N}\mathbf{x}_{i}\nabla\psi\left( \mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}\right)^{ \top}\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\)._

This expression involves weight matrix \(\mathbf{W}\). However the inequality we set out to prove (Eqn. (10)) is in terms of \(\mathbf{w}=\mathsf{vec}(\mathbf{W})\). Throughout our main result proof, these two different forms - weight matrix versus vectorization of that matrix - will each be useful in different situations. Thus, to shuttle back and forth between these forms, the following well-known identity is useful:

**Lemma 4.3**.: _For equally sized matrices \(\mathbf{M}\) and \(\mathbf{N}\), we have \(\mathsf{vec}(\mathbf{M})^{\top}\mathsf{vec}(\mathbf{N})=\mathsf{tr}(\mathbf{ M}^{\top}\mathbf{N})\)._

Now we can prove our inequality of interest, i.e., Eqn. (10).

**Lemma 4.4**.: _(Multiclass generalization of Soudry et al. (2018, Lemma 1)) For any PERM loss that is \(\beta\)-smooth, strictly decreasing, and non-negative, (Assumption 3.1) and Assumption 3.2, and for almost all linearly separable datasets (Assumption 3.3), we have \(\hat{\mathbf{w}}^{\top}\nabla\mathcal{R}(\mathbf{w}(t))<0\)._

Proof.: Define matrix \(\hat{\mathbf{W}}\) such that \(\hat{\mathbf{w}}=\mathsf{vec}(\hat{\mathbf{W}})\). Since \(\mathbf{w}(t)=\mathsf{vec}(\mathbf{W}(t))\), Lemma 4.3 implies

\[\hat{\mathbf{w}}^{\top}\nabla\mathcal{R}(\mathbf{w}(t))=\mathsf{tr}(\hat{ \mathbf{W}}^{\top}\nabla\mathcal{R}(\mathbf{W}(t)))\] (11)

To see how the PERM framework allows for a simple generalization of binary results, we will compare our multiclass proof side-by-side with the binary proof discussed in Soudry et al. (2018, Lemma 1). In the binary case, we have \(\mathcal{R}\left(\mathbf{w}\right)=\sum_{i=1}^{N}\psi\left(y_{i}\mathbf{w}^{ \top}\mathbf{x}_{i}\right)\implies\nabla\mathcal{R}\left(\mathbf{w}\right)= \sum_{i=1}^{N}\psi^{\prime}\left(y_{i}\mathbf{w}^{\top}\mathbf{x}_{i}\right) y_{i}\mathbf{x}_{i}\). Thus \(\hat{\mathbf{w}}^{\top}\nabla\mathcal{R}\left(\mathbf{w}\right)=\sum_{i=1}^{N} \psi^{\prime}\left(y_{i}\mathbf{w}^{\top}\mathbf{x}_{i}\right)y_{i}\hat{ \mathbf{w}}^{\top}\mathbf{x}_{i}\). In the multiclass case, the analogous quantity is \(\mathsf{tr}(\hat{\mathbf{W}}^{\top}\nabla\mathcal{R}(\mathbf{W}(t)))\) which can be computed as

\[\sum_{i=1}^{N}\mathsf{tr}(\hat{\mathbf{W}}^{\top}\mathbf{x}_{i}\nabla\psi \left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i} \right)^{\top}\mathbf{\Upsilon}_{y_{i}}\mathbf{D})=\sum_{i=1}^{N}\nabla\psi \left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i} \right)^{\top}\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\hat{\mathbf{W}}^{\top} \mathbf{x}_{i}.\]

In the multiclass proof we used the risk gradient from Lemma 4.2 as well as the cyclic property of the trace operator. Then we dropped the trace because \(\nabla\psi\left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{ x}_{i}\right)^{\top}\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\hat{\mathbf{W}}^{\top} \mathbf{x}_{i}\) is a scalar (since \(\nabla\psi\left(\cdot\right)\in\mathbb{R}^{K-1},\mathbf{\Upsilon}_{y_{i}} \mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i}\in\mathbb{R}^{K-1}\)). For illustrative purpose, we place the rest of the proof, in both the binary and multiclass setting, side-by-side:

**Binary**: \(\hat{\mathbf{w}}^{\top}\nabla\mathcal{R}(\mathbf{w}(t))\).

Focusing on just the \(i\)-th term of this sum:

\[\psi^{\prime}\left(y_{i}\mathbf{w}(t)^{\top}\mathbf{x}_{i}\right)y_{i}\hat{ \mathbf{w}}^{\top}\mathbf{x}_{i}\]

\(\psi\) is assumed to be strictly decreasing, i.e. \(\psi^{\prime}\left(y_{i}\mathbf{w}(t)^{\top}\mathbf{x}_{i}\right)<0\). The dataset is linearly separable, so \(y_{i}\hat{\mathbf{w}}^{\top}\mathbf{x}_{i}\geq 1\). Thus we obtain a sum (from \(i=1\) to \(N\)) of negative terms.

Thus we see how the PERM framework allows us to essentially mirror the binary proof. In Remark 4.5, we elaborate more on the necessity of the relative margin form here.

Lemma 4.4 directly implies the auxiliary inequality we set out to prove (see Eqn. (10)). Thus we obtain:

\[\left\|\mathbf{r}\left(t+1\right)-\mathbf{r}\left(t\right)\right\|^{2}\leq\eta^{ 2}\left\|\nabla\mathcal{R}\left(\mathbf{w}\left(t\right)\right)\right\|^{2}+ \left\|\hat{\mathbf{w}}\right\|^{2}t^{-2}\] (12)

**Remark 4.5**.: _Let us see what happens to our proof if we just used the general risk form in Eqn. (2) without the PERM framework. First, we need an expression for the gradient of the risk: \(\nabla\mathcal{R}\left(\mathbf{W}\right)=\sum_{i=1}^{N}\mathbf{x}_{i}\nabla \mathcal{R}_{y_{i}}\left(\mathbf{W}^{\top}\mathbf{x_{i}}\right)^{\top}\). Proceeding similarly to the binary case, we focus on just the \(i\)-th term of \(\mathsf{tr}\left(\hat{\mathbf{W}}^{\top}\nabla\mathcal{R}\left(\mathbf{W} \right)\right)\):_

\[\mathsf{tr}\left(\hat{\mathbf{W}}^{\top}\mathbf{x}_{i}\nabla\mathcal{R}_{y_{ i}}\left(\mathbf{W}^{\top}\mathbf{x_{i}}\right)^{\top}\right)=\mathsf{tr}\left( \nabla\mathcal{R}_{y_{i}}\left(\mathbf{W}^{\top}\mathbf{x_{i}}\right)^{\top} \hat{\mathbf{W}}^{\top}\mathbf{x_{i}}\right)=\nabla\mathcal{R}_{y_{i}}\left( \mathbf{W}^{\top}\mathbf{x_{i}}\right)^{\top}\hat{\mathbf{W}}^{\top}\mathbf{ x_{i}}\]

_From here it is not clear how to proceed. The linear separability condition (Assumption 3.3) is not useful anymore- it does not make a statement about the scores in the vector \(\hat{\mathbf{W}}^{\top}\mathbf{x}_{i}\), but rather their relative margins (produced by the multiplication \(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\hat{\mathbf{W}}^{\top}\mathbf{x}_{i}\))._

### Bounding the Second Term

In the previous subsection we established a bound on the first term of Eqn. (9). Here we sketch the main arguments required to bound the second term, i.e. \(\left(\mathbf{r}\left(t+1\right)-\mathbf{r}\left(t\right)\right)^{\top} \mathbf{r}\left(t\right)\). For more details please refer to Appendix F. We state our final bound below as a lemma:

**Lemma 4.6**.: _(Generalization of Soudry et al. (2018, Lemma 2018)) Define \(\theta\) to be the minimum SVM margin across all datapoints and classes, i.e. \(\theta=\min_{k}\left[\min_{\theta\notin\mathcal{S}_{k}}\tilde{\mathbf{x}}_{n, k}^{\top}\hat{\mathbf{w}}\right]>1\). Then_

\[\exists C_{1},C_{2},t_{1}:\,\forall t>t_{1}:\,\left(\mathbf{r}\left(t+1\right) -\mathbf{r}\left(t\right)\right)^{\top}\mathbf{r}\left(t\right)\leq C_{1}t^{- \theta}+C_{2}t^{-2}\,.\] (13)

A remark is in order on the difference of the above result to Soudry et al. (2018, Lemma 20): on a high-level, we are able to generalize the argument of Soudry et al. (2018, Lemma 20) to account for _both binary and multiclass classification_, as well as general PERM ET losses beyond just CE.

We now proceed with the proof sketch. The first step is to rewrite \(\left(\mathbf{r}\left(t+1\right)-\mathbf{r}\left(t\right)\right)^{\top} \mathbf{r}\left(t\right)\) as

\[\left(-\eta\nabla\mathcal{R}\left(\mathbf{w}\left(t\right)\right)-\hat{ \mathbf{w}}\left[\log\left(t+1\right)-\log\left(t\right)\right]\right)^{\top} \mathbf{r}\left(t\right)\qquad\therefore\text{Definition of }\mathbf{r}\left(t\right)\text{ in Equation (\ref{eq:PERM})}\]

\[=\hat{\mathbf{w}}^{\top}\mathbf{r}(t)\left(t^{-1}-\log\left(1+t^{-1}\right) \right)+\mathsf{tr}\left(\left(-\eta\sum_{i=1}^{N}\mathbf{x}_{i}\nabla\psi \left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i} \right)^{\top}\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\right)^{\top}\mathbf{R}(t)\right)\]

\[-t^{-1}\hat{\mathbf{w}}^{\top}\mathbf{r}(t)\qquad\therefore\text{ Expression for }\nabla\mathcal{R}\left(\mathbf{w}\left(t\right)\right)\text{ from Lemma \ref{eq:PERM}}\] (14)

We defer the bound on the first term \(\hat{\mathbf{w}}^{\top}\mathbf{r}(t)\left(t^{-1}-\log\left(1+t^{-1}\right)\right)\) of Equation (14) to the appendix, and instead focus on the second two terms. Using the cyclic property of the trace, the term in the above final line involving the trace can be further simplified as:

\[\sum_{i=1}^{N}-\nabla\psi\left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t )^{\top}\mathbf{x}_{i}\right)^{\top}\mathbf{\Upsilon}_{y_{i}}\mathbf{D} \mathbf{R}(t)^{\top}\mathbf{x}_{i}\] (15)

Note that for each \(i\in[N]\), a summand in Equation (15) is an inner product between two \((K-1)\)-dimensional vectors, i.e., \(-\nabla\psi\left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top} \mathbf{x}_{i}\right)\) and \(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{R}(t)^{\top}\mathbf{x}_{i}\). To proceed, to expand this inner product out as

\[\sum_{i=1}^{N}\sum_{k\in[K]\backslash\{y_{i}\}}[-\nabla\psi\left(\mathbf{ \Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i}\right)]_{k}[ \mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{R}(t)^{\top}\mathbf{x}_{i}]_{k}\] (16)

**Remark 4.7**.: _Here, \([\cdot]_{k}:\mathbb{R}^{K-1}\rightarrow\mathbb{R}\) is defined as the coordinate projection such that \([\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}]_{k}= \tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{w}\). Note that \([\cdot]_{k}\) implicitly depends on \(i\) (the \(\tilde{\mathbf{x}}_{i,y_{i}}\), 0-entry is omitted). But we abuse notation for brevity. Please see Appendix D for a more precise definition._Using Equation (6) and Equation (8) we express the last two terms in Equation (14) as

\[\Big{(}\sum_{i=1}^{N}\sum_{k\in[K]\backslash\{y_{i}\}}\llbracket- \nabla\psi\left(\bm{\Upsilon}_{y_{i}}\mathbf{DW}(t)^{\top}\mathbf{x}_{i}\right) \rrbracket_{k}\llbracket\bm{\Upsilon}_{y_{i}}\mathbf{DR}(t)^{\top}\mathbf{x}_{i }\rrbracket_{k}\Big{)}-t^{-1}\hat{\mathbf{w}}^{\top}\mathbf{r}(t)\] \[=\sum_{i=1}^{N}\sum_{k\in[K]\backslash\{y_{i}\}}\left(\llbracket- \nabla\psi\left(\bm{\Upsilon}_{y_{i}}\mathbf{DW}(t)^{\top}\mathbf{x}_{i}\right) \rrbracket_{k}-t^{-1}\exp(-\tilde{\mathbf{w}}^{\top}\tilde{\mathbf{x}}_{i,k}) \mathbbm{1}_{\{i\in S_{k}\}}\right)\tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{r}(t)\] (17)

Finally, to upper bound the above expression, we consider a single tuple \((i,k)\) case-by-case, depending on the sign of \(\tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{r}(t)\). This is the step where the upper and lower bounds in Definition 2.2 come in. Lemma D.2 in the appendix essentially applies Definition 2.2 to the relative margins to yield

\[\llbracket-\nabla\psi(\bm{\Upsilon}_{y_{i}}\mathbf{DW}(t)^{\top} \mathbf{x}_{i})\rrbracket_{k}\leq\exp(-\tilde{\mathbf{x}}_{i,k}^{\top} \mathbf{w}(t)),\qquad\text{and}\] (18) \[\llbracket-\nabla\psi(\bm{\Upsilon}_{y_{i}}\mathbf{DW}(t)^{\top} \mathbf{x}_{i})\rrbracket_{k}\geq(1-\sum_{r\in[K]\backslash\{y_{i}\}}\exp(- \tilde{\mathbf{x}}_{i,r}^{\top}\mathbf{w}(t)))\exp(-\tilde{\mathbf{x}}_{i,k}^{ \top}\mathbf{w}(t))\] (19)

for all \(k\in\llbracket K\rrbracket\setminus\{y_{i}\}\). We use Definition 2.2's exponential tail bounds by proving that the relative margins \(\tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{w}(t)\) that appear in Lemma D.2 eventually become positive. This is true due to the following lemma (see Appendix E for the proof, which again mirrors the binary case):

**Lemma 4.8**.: _(Multiclass generalization of Soudry et al. (2018, Lemma 1)) Consider any linearly separable dataset, and any PERM loss with template \(\psi\) that is convex, \(\beta\)-smooth, strictly decreasing, and non-negative. For all \(k\in\{1,...,K\}\), let \(\mathbf{w}_{k}(t)\) be the gradient descent iterates at iteration \(t\) for the \(k^{th}\) class. Then \(\forall i\in\{1,...,N\},\forall j\in\{1,...,K\}\backslash\{y_{i}\}:\lim_{t \rightarrow\infty}(\mathbf{w}_{y_{i}}(t)-\mathbf{w}_{j}(t))^{\top}\mathbf{x}_{ i}\rightarrow\infty\)._

This lemma lets us use the exponential tail bounds with any finite \(u_{\pm}\). To conclude, we apply the upper (18) and lower bounds (19) to the summation in Equation (17), and reduce the problem to that of Soudry et al. (2018, Appendix E), thereby proving Lemma 4.6. See our Appendix F for details.

### Tying It All Together

We use the logic of Soudry et al. (2018, Appendix A.2) to conclude the analysis. Define

\[C=\sum_{t=0}^{\infty}\left\|\mathbf{r}\left(t+1\right)-\mathbf{r}\left(t \right)\right\|^{2}\leq\sum_{t=0}^{\infty}\eta^{2}\left\|\nabla\mathcal{R} \left(\mathbf{w}\left(t\right)\right)\right\|^{2}+\left\|\hat{\mathbf{w}} \right\|^{2}t^{-2}.\]

In the latter inequality we used Eqn. (12). Thus, \(C\) is bounded because from Soudry et al. (2018, Lemma 10), we know that \(\sum_{t=0}^{\infty}\left\|\nabla\mathcal{R}\left(\mathbf{w}\left(t\right) \right)\right\|^{2}<\infty\). Here we note that Soudry et al. (2018, Lemma 10) requires the ERM objective \(\mathcal{R}\left(\mathbf{w}\right)\) to be \(\beta^{\prime}\)-smooth for some positive \(\beta^{\prime}\). It is easy to show that if the loss is \(\beta\)-smooth, then \(\mathcal{R}\left(\mathbf{w}\right)\) is \(\beta\sigma_{\text{max}}^{2}\left(\mathbf{X}\right)\)-smooth. This explains the learning rate condition \(\eta<2/\left(\beta\sigma_{\text{max}}^{2}\left(\mathbf{X}\right)\right)\) in our theorem. Also, a \(t^{-p}\) power series converges for any \(p>1\).

Recalling the initial expansion of \(\left\|\mathbf{r}(t+1)\right\|\) from Eqn. (9):

\[\left\|\mathbf{r}\left(t+1\right)\right\|^{2}=\left\|\mathbf{r}\left(t+1 \right)-\mathbf{r}\left(t\right)\right\|^{2}+2\left(\mathbf{r}\left(t+1\right) -\mathbf{r}\left(t\right)\right)^{\top}\mathbf{r}\left(t\right)+\left\| \mathbf{r}\left(t\right)\right\|^{2}.\] (20)

Combining the bounds in Eqn. (12) and Lemma 4.6 into Eqn. (9), we find

\[\left\|\mathbf{r}\left(t\right)\right\|^{2}-\left\|\mathbf{r}\left(t_{1}\right) \right\|^{2}=\sum_{u=t_{1}}^{t-1}\left[\left\|\mathbf{r}\left(u+1\right)\right\| ^{2}-\left\|\mathbf{r}\left(u\right)\right\|^{2}\right]\leq C+2\sum_{u=t_{1}}^ {t-1}\left[C_{1}u^{-\theta}+C_{2}u^{-2}\right].\]

Therefore, \(\left\|\mathbf{r}\left(t\right)\right\|\) is bounded, which proves our main theorem.

## 5 Limitations

Here we describe some of our work's limitations/possible future research directions. We note that these questions have been analyzed for the binary classification setting, but not for multiclass.

Non-ET lossesIn our paper we only analyze multiclass implicit bias for losses with the ET property. Another possible line of future work is to analyze the gradient descent dynamics for non-ET losses. Nacson et al. (2019) and Ji et al. (2020) prove that in the binary setting, ET and well-behaved super-polynomial tailed losses ensure convergence to the maximum-margin direction, while other losses may converge to a different direction with poor margin. Is such a characterization possible in the multiclass setting?

Other gradient-based methodsThis paper only analyzes vanilla gradient descent. Another line of work involves exploring implicit bias effects of other gradient-based methods, such as those characterized in Gunasekar et al. (2018). Nacson et al. (2022) uses similar proof techniques to prove results for SGD, which is prevalent in practice and often generalizes better than vanilla GD ((Amir et al., 2021)).

Non-asymptotic analysisOur result proves that the gradient descent predictors _asymptotically_ do not overfit. However, in the binary classification case, Shamir (2021) goes one step further and proves that for gradient-based methods, throughout the entire training process (not just asymptotically), both the empirical risk and the generalization error decrease at an essentially optimal rate (or remain optimally constant). Does the same phenomenon occur in the multiclass setting?

## 6 Conclusion

We use the permutation equivariant and relative margin-based (PERM) loss framework to provide an multiclass extension of the binary ET property. On a high level, while the binary ET bounds the negative derivative of the loss, our multiclass ET bounds each negative partial derivative of the PERM template \(\psi\). We demonstrate our definition's validity for multinomial logistic loss, multiclass exponential loss, and PairLogLoss. We develop new techniques for analyzing multiclass gradient descent, and apply these to generalize binary implicit bias results to the multiclass setting. Our main result is that for almost all linearly separable multiclass datasets and a suitable ET PERM loss, the gradient descent iterates directionally converge towards the hard-margin multiclass SVM solution.

Our proof techniques in this paper demonstrate the power of the PERM framework to facilitate extensions of known binary results to multiclass settings and provide a unified treatment of both binary and multiclass classification. Thus it is possible that the binary results discussed in the Limitations section can also be extended using the PERM loss framework. In the future we would like to consider more complex settings that have been analyzed primarily for the binary case, such as non-separable data (Ji and Telgarsky (2019)) and two-layer neural nets (Lyu et al. (2021)).

## Acknowledgments and Disclosure of Funding

CS was supported in part by the National Science Foundation under award 2008074, and by the Department of Defense, Defense Threat Reduction Agency under award HDTRA1-20-2-0002. The research of DS was funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. YW was supported in part by the Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship, a Schmidt Futures program.

## References

* Amir et al. (2021) Idan Amir, Tomer Koren, and Roi Livni. Sgd generalizes better than gd (and regularization doesn't help), 2021.
* Beck and Teboulle (2003) Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. _Operations Research Letters_, 31(3):167-175, 2003.
* Boyd and Vandenberghe (2004) Stephen Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge University Press, 2004.
* Boyd et al. (2019)Emmanuel J. Candes and Pragya Sur. The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression. _The Annals of Statistics_, 48(1):27-42, 2020.
* Clarkson et al. (2012) Kenneth L Clarkson, Elad Hazan, and David P Woodruff. Sublinear optimization for machine learning. _Journal of the ACM (JACM)_, 59(5):1-49, 2012.
* Cotter et al. (2012) Andrew Cotter, Shai Shalev-Shwartz, and Nathan Srebro. The kernelized stochastic batch perceptron. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, pages 739-746, 2012.
* Duchi et al. (2011) John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of machine learning research_, 12(7), 2011.
* Gunasekar et al. (2018) Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In _International Conference on Machine Learning_, pages 1832-1841. PMLR, 2018.
* Ji and Telgarsky (2019) Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In _Conference on Learning Theory_, pages 1772-1798. PMLR, 2019.
* Ji et al. (2020) Ziwei Ji, Miroslav Dudik, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In _Conference on Learning Theory_, pages 2109-2136. PMLR, 2020.
* Ji et al. (2021) Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In _International Conference on Machine Learning_, pages 4860-4869. PMLR, 2021.
* Khatri and Rao (1968) CG Khatri and C Radhakrishna Rao. Solutions to some functional equations and their applications to characterization of probability distributions. _Sankhya: The Indian Journal of Statistics, Series A_, pages 167-180, 1968.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations_, volume 6, 2015.
* Liu (1999) Shuangzhe Liu. Matrix results on the khatri-rao and tracy-singh products. _Linear Algebra and its Applications_, 289(1-3):267-277, 1999.
* Lyu and Li (2019) Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. _arXiv preprint arXiv:1906.05890_, 2019.
* Lyu et al. (2021) Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. _Advances in Neural Information Processing Systems_, 34:12978-12991, 2021.
* Magnus and Neudecker (2019) Jan R Magnus and Heinz Neudecker. _Matrix differential calculus with applications in statistics and econometrics_. John Wiley & Sons, 2019.
* Mukherjee and Schapire (2013) Indraneel Mukherjee and Robert E Schapire. A theory of multiclass boosting. _Journal of Machine Learning Research_, 2013.
* Nacson et al. (2019) Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 3420-3428. PMLR, 2019.
* Nacson et al. (2022) Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate, 2022.
* Phelps (2009) Robert R Phelps. _Convex functions, monotone operators and differentiability_, volume 1364. Springer, 2009.
* Schliserman and Koren (2022) Matan Schliserman and Tomer Koren. Stability vs implicit bias of gradient methods on separable data and beyond. In _Conference on Learning Theory_, pages 3380-3394. PMLR, 2022.
* Schapire et al. (2019)Matan Schliserman and Tomer Koren. Tight risk bounds for gradient descent on separable data. _arXiv preprint arXiv:2303.01135_, 2023.
* Shamir [2021] Ohad Shamir. Gradient methods never overfit on separable data. _The Journal of Machine Learning Research_, 22(1):3847-3866, 2021.
* Soudry et al. [2018] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018. See arxiv.org/abs/1710.10345v7 for the most up-to-date version.
* Vardi [2022] Gal Vardi. On the implicit bias in deep-learning algorithms. _arXiv preprint arXiv:2208.12591_, 2022.
* Wang et al. [2023] Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overfitting in multiclass classification: All roads lead to interpolation, 2023.
* Wang et al. [2022] Nan Wang, Zhen Qin, Le Yan, Honglei Zhuang, Xuanhui Wang, Michael Bendersky, and Marc Najork. Rank4class: A ranking formulation for multiclass classification, 2022.
* Wang and Scott [2024] Yutong Wang and Clayton Scott. Unified binary and multiclass margin-based classification. Accepted to _Journal of Machine Learning Research_, arXiv:2311.17778, 2024.
* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.

## Appendix A Discussion of Lyu and Li [2019]

Lyu and Li [2019] allow the class score functions to be linear classifiers, e.g., \(\mathbf{w}_{k}^{\top}\mathbf{x}_{i}\), but also nonlinear, e.g., "cubed" linear classifier \((\mathbf{w}_{k}^{\top}\mathbf{x}_{i})^{3}\). By shifting the cubing operation to the loss, we can view the implicit regularization result of Lyu and Li [2019] as a result for losses beyond the cross entropy. This resulting loss is rather exotic and we are not aware of it being used in the literature; it is interesting nevertheless. However, the optimization problem would become non-convex, so convergence would not necessarily be to a global minimum:

\[\min_{\mathbf{w}}\frac{1}{2}\|\mathbf{w}\|^{2}\quad\text{s.t.}\,\left(\mathbf{ w}_{y_{i}}^{\top}\mathbf{x}_{i}\right)^{3}-\left(\mathbf{w}_{k}^{\top} \mathbf{x}_{i}\right)^{3}\geq 1\text{ for all }i\in[N],j\in[K]\backslash\{y_{i}\}\]

Moreover, the decision region for the \(k\)-th class, i.e., the set of \(\mathbf{x}\in\mathbb{R}^{d}\) such that \((\mathbf{w}_{k}^{\top}\mathbf{x})^{3}>(\mathbf{w}_{j}^{\top}\mathbf{x})^{3}\) for all \(j\neq k\), is an intersection of sets constructed via cubic hypersurfaces.

More precisely, the \(k\)-th decision region can be written as

\[\{\mathbf{x}\in\mathbb{R}^{d}:(\mathbf{w}_{k}^{\top}\mathbf{x})^{3}=\operatorname {argmax}_{j\in|K|}(\mathbf{w}_{j}^{\top}\mathbf{x})^{3}\}=\bigcap_{j\in[K]:j \neq k}\{\mathbf{x}\in\mathbb{R}^{d}:(\mathbf{w}_{k}^{\top}\mathbf{x})^{3}> (\mathbf{w}_{j}^{\top}\mathbf{x})^{3}\}\]

Let us define \(\mathcal{H}_{j}:=\{\mathbf{x}\in\mathbb{R}^{d}:(\mathbf{w}_{k}^{\top}\mathbf{ x})^{3}=(\mathbf{w}_{j}^{\top}\mathbf{x})^{3}\}\). Note that \(\mathcal{H}_{j}\subseteq\mathbb{R}^{d}\) is the zero set of degree \(3\) polynomials with variables in \(\mathbf{x}\), hence, a cubic hypersurface. Now, the set \(\{\mathbf{x}\in\mathbb{R}^{d}:(\mathbf{w}_{k}^{\top}\mathbf{x})^{3}>(\mathbf{ w}_{j}^{\top}\mathbf{x})^{3}\}\) is a subset of the set-theoretic complement of \(\mathcal{H}_{j}\) in \(\mathbb{R}^{d}\). Thus, the decision regions are complicated geometric objects, compared to the classical hard-margin SVM.

## Appendix B Matrix Calculus

This section of the appendix establishes matrix identities that will be useful for us to calculate the gradient/Hessian of the empirical risk objective \(\mathcal{R}(\mathbf{w})\).

**Vector-input scalar-output function**. Suppose \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}\) is a continuously differentiable function. Let \(\mathbf{x}=[x_{1},\cdots,x_{n}]^{\top}\in\mathbb{R}^{n}\) be a vector of variables for differentiation. Define the (column) vector of partial derivatives w.r.t. \(\mathbf{x}\): \(\partial_{\mathbf{x}}:=\left[\frac{\partial}{\partial_{x_{i}}}\right]_{i\in[n]}\). The _gradient_ of \(f\), denoted \(\nabla f\), is the function

\[\nabla f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n},\quad\text{where}\quad\nabla f (\mathbf{x})=\partial_{\mathbf{x}}f(\mathbf{x})=\left[\frac{\partial f}{ \partial x_{1}}(\mathbf{x})\quad\cdots\quad\frac{\partial f}{\partial x_{n}}( \mathbf{x})\right]^{\top}.\] (21)Suppose that \(f\) is twice continuously differentiable. The _Hessian_ of \(f\), denoted \(\nabla^{2}f\), is the function

\[\nabla^{2}f:\mathbb{R}^{n}\to\mathbb{R}^{n\times n},\quad\text{where}\quad\nabla ^{2}f(\mathbf{x})=\left[\tfrac{\partial^{2}f}{\partial x_{i}\partial x_{j}}( \mathbf{x})\right]_{i,j\in[n]}.\] (22)

**Matrix-input scalar-output function**. Let \(f:\mathbb{R}^{m\times n}\to\mathbb{R}\) be a differentiable function. Let \(\mathbf{X}=[x_{ij}]_{i\in[m],j\in[n]}\in\mathbb{R}^{m\times n}\) be an arbitrary matrix. Define the matrix of partial derivatives w.r.t. \(\mathbf{X}\):

\[\partial_{\mathbf{X}}:=\left[\tfrac{\partial}{\partial x_{ij}}\right]_{i\in[m ],j\in[n]}\]

Define the gradient of \(f\), denoted \(\nabla f\), to be the function

\[\nabla f:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n},\quad\text{where}\quad \nabla(f)(\mathbf{X}):=\partial_{\mathbf{X}}f(\mathbf{X})=\left[\tfrac{ \partial}{\partial x_{ij}}f(\mathbf{X})\right]_{i\in[m],j\in[n]}.\] (23)

We do not define the Hessian of a matrix-input scalar-output function \(f:\mathbb{R}^{m\times n}\to\mathbb{R}\). Instead, we will define the Hessian for its _vectorization_\(\mathsf{vec}(f):\mathbb{R}^{mn}\to\mathbb{R}\).

**Definition B.1** (Vectorization operator).: _Let \(\mathsf{vec}\) denote the vectorization operator by stacking the columns of a vector. In other words, if \(\mathbf{A}\in\mathbb{R}^{m\times n}\) is a matrix with columns \(\mathbf{a}_{1},\dots,\mathbf{a}_{n}\in\mathbb{R}^{m}\), then_

\[\mathsf{vec}(\mathbf{A}):=\begin{bmatrix}\mathbf{a}_{1}^{\top}&\mathbf{a}_{2 }^{\top}&\cdots&\mathbf{a}_{n}^{\top}\end{bmatrix}^{\top}.\]

**Definition B.2** (Vectorization of a matrix-input function).: _Let \(f:\mathbb{R}^{m\times n}\to\mathbb{R}\) be a matrix-input function, we define \(\mathsf{vec}(f):\mathbb{R}^{mn}\to\mathbb{R}\) to be the vector-input function such that_

\[f(\mathbf{A})=\mathsf{vec}(f)(\mathsf{vec}(\mathbf{A})).\]

_In particular, if \(f\) is already a vector-input function, then \(\mathsf{vec}(f)=f\)._

See (Magnus and Neudecker, 2019, Ch.5-815). Below in Lemma B.4, we give a convenient formula to calculate the Hessian of \(\mathcal{R}(\mathbf{w})\), the vectorization of \(\mathcal{R}(\mathbf{W})\).

The following relates the vectorization operator with the Kronecker product:

**Lemma B.1**.: _Let \(\mathbf{A}\in\mathbb{R}^{p\times n},\mathbf{B}\in\mathbb{R}^{n\times m}, \mathbf{C}\in\mathbb{R}^{m\times q}\) be matrices. Then_

\[\mathsf{vec}(\mathbf{ABC})=(\mathbf{C}^{\top}\otimes\mathbf{A})\mathsf{vec}( \mathbf{B}).\]

Proof.: This is (Magnus and Neudecker, 2019, Theorem 2.2). 

### Special case of the chain rule for linear functions

**Proposition B.2**.: _Let \(\mathbf{M}\in\mathbb{R}^{m\times n}\) be a matrix. Let \(f:\mathbb{R}^{m}\to\mathbb{R}\) be a continuously differentiable function and define \(g:\mathbb{R}^{n}\to\mathbb{R}\) by \(g(\mathbf{x}):=f(\mathbf{M}\mathbf{x})\). Then_

\[\nabla g(\mathbf{x})=\mathbf{M}^{\top}\nabla f(\mathbf{M}\mathbf{x}),\quad \text{and}\quad\nabla^{2}g(\mathbf{x})=\mathbf{M}^{\top}\nabla^{2}f(\mathbf{M }\mathbf{x})\mathbf{M}.\]

Proof.: See (Magnus and Neudecker, 2019, Ch.9-813) for the first identity and (Magnus and Neudecker, 2019, Ch.10-88) for the second identity. 

The next two results will be referred to as the "gradient formula" and the "Hessian formula", respectively, for the function \(g(\mathbf{X}):=f(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})\).

**Lemma B.3**.: _Let \(f:\mathbb{R}^{p\times q}\to\mathbb{R}\) be a matrix-input scalar-output differentiable function with Jacobian denoted \(\nabla f:\mathbb{R}^{p\times q}\to\mathbb{R}^{p\times q}\). Let \(\mathbf{A}\in\mathbb{R}^{p\times n}\), \(\mathbf{X}\in\mathbb{R}^{m\times n}\), and \(\mathbf{B}\in\mathbb{R}^{m\times q}\). Define a function \(g:\mathbb{R}^{m\times n}\to\mathbb{R}\) by \(g(\mathbf{X}):=f(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})\). Then_

\[\nabla g(\mathbf{X})=\partial_{\mathbf{X}}f(\mathbf{A}\mathbf{X}^{\top} \mathbf{B})=\mathbf{B}\nabla f(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})^{\top} \mathbf{A}.\]

**Lemma B.4**.: _Let \(f:\mathbb{R}^{p}\to\mathbb{R}\) be a vector-input scalar-output twice differentiable function. Let \(\mathbf{A}\in\mathbb{R}^{p\times n},\mathbf{X}\in\mathbb{R}^{m\times n}\) be matrices and \(\mathbf{b}\in\mathbb{R}^{m}\) be a (column) vector. Let \(\mathbf{V}\) be another matrix with the same shape as \(\mathbf{X}\). Let \(\mathbf{x}:=\mathsf{vec}(\mathbf{X})\) and \(\mathbf{v}:=\mathsf{vec}(\mathbf{V})\). Define \(g(\mathbf{X}):=f(\mathbf{A}\mathbf{X}^{\top}\mathbf{b})\) and let \(\overline{g}=\mathsf{vec}(g)\) be the vectorization of \(g\). Then we have the following formula for computing \(\mathbf{v}^{\top}\nabla^{2}\overline{g}(\mathbf{x})\mathbf{v}\):_

\[\mathbf{v}^{\top}\nabla^{2}\overline{g}(\mathbf{x})\mathbf{v}=(\mathbf{A} \mathbf{V}^{\top}\mathbf{b})^{\top}\nabla^{2}f(\mathbf{A}\mathbf{X}^{\top} \mathbf{b})\mathbf{A}\mathbf{V}^{\top}\mathbf{b}^{\top}.\]

### Proof of the gradient formula: Lemma 4.2

In the notation of Section 2.8.1 of the Matrix Cookbook, define matrix \(\mathbf{U}\in\mathbb{R}^{p\times q}\) by \(\mathbf{U}:=\mathbf{A}\mathbf{X}^{\top}\mathbf{B}\). Note that \(\mathbf{U}\) is a function of \(\mathbf{X}\). Then by Eqn. (137) of the Matrix Cookbook, we have for each \((i,j)\in[m]\times[n]\)

\[\frac{\partial}{\partial X_{ij}}f(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})=\frac {\partial}{\partial X_{ij}}f(\mathbf{U})=\operatorname{Tr}\left[\left(\frac{ \partial f(\mathbf{U})}{\partial\mathbf{U}}\right)^{\top}\frac{\partial \mathbf{U}}{\partial X_{ij}}\right]\]

Note that by definition, we have \(\frac{\partial f(\mathbf{U})}{\partial\mathbf{U}}=\nabla f(\mathbf{U})\). Therefore

\[\frac{\partial}{\partial X_{ij}}f(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})= \operatorname{Tr}\left[\nabla f(\mathbf{U})^{\top}\frac{\partial\mathbf{U}}{ \partial X_{ij}}\right]\]

Next, write \(\mathbf{U}=\left[U_{k\ell}\right]_{k\in[p],\ell\in[q]}\) in the "matrix-comprehension" notation. Recall that \(U_{k\ell}\), i.e., the \((k,\ell)\)-th entry of \(\mathbf{U}\), is precisely computed by \(\mathbf{A}[k,:](\mathbf{X}^{\top}\mathbf{B})[:,\ell]=\mathbf{A}[k,:]\mathbf{X }^{\top}\mathbf{B}[:,\ell]\). For each \(k,\ell\in[p]\times[q]\), we have

\[\frac{\partial U_{k\ell}}{\partial X_{ij}}=\frac{\partial(\mathbf{A}[k,:] \mathbf{X}^{\top}\mathbf{B}[:,\ell])}{\partial X_{ij}}\]

where "\([k,:]\)" and " \([:,\ell]\) " denote taking the \(k\)-th row vector and \(\ell\)-th column vector, respectively. Now, by Eqn. (71) of the Matrix Cookbook, we have the following expression of the matrix-partial derivative as an outer product

\[\frac{\partial U_{k\ell}}{\partial\mathbf{X}}=\frac{\partial\mathbf{A}[k,:] \mathbf{X}^{\top}\mathbf{B}[:,\ell]}{\partial\mathbf{X}}=\mathbf{B}[:,\ell] \mathbf{A}[k,:].\]

From this, it follows that computing the entry-wise partial derivative at \(X_{ij}\) is simply obtained by indexing at \((i,j)\), i.e., \(\frac{\partial U_{k\ell}}{\partial X_{ij}}=\mathbf{B}[i,\ell]\mathbf{A}[k,j]= \mathbf{A}[k,j]\mathbf{B}[i,\ell]\) (we emphasize that this is just a product of two scalars). Thus, \(\frac{\partial\mathbf{U}}{\partial X_{ij}}=\mathbf{A}[:,j]\mathbf{B}[i,:]\). Consequently,

\[\frac{\partial}{\partial X_{ij}}f(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})= \operatorname{Tr}\left[\nabla f(\mathbf{U})^{\top}\mathbf{A}[:,j]\mathbf{B}[i,: ]\right]=\mathbf{B}[i,:]\nabla f(\mathbf{U})^{\top}\mathbf{A}[:,j].\]

In other words, \(\frac{\partial}{\partial\mathbf{X}}f(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})= \mathbf{B}\nabla f(\mathbf{U})^{\top}\mathbf{A}\).

For our purposes, we replace \(f\) with \(\psi\), \(\mathbf{A}\) with \(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\), \(\mathbf{X}\) with \(\mathbf{W}\), and \(\mathbf{B}\) with \(\mathbf{x}_{i}\). Thus we obtain

\[\nabla\mathcal{R}\left(\mathbf{W}\right)=\sum_{i=1}^{N}\frac{\partial}{ \partial\mathbf{W}}\psi\left(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^ {\top}\mathbf{x}_{i}\right)=\sum_{i=1}^{N}\mathbf{x}_{i}\nabla\psi\left( \boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}\right)^{ \top}\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\]

as desired. 

### Proof of Hessian formula: Lemma B.4

Our goal is to calculate the Hessian of \(\mathsf{vec}(g)\). First, we note that by definition

\[\mathsf{vec}(g)(\mathbf{x})=\mathsf{vec}(g)(\mathsf{vec}(\mathbf{X}))=\mathsf{ vec}(f)(\mathsf{vec}(\mathbf{A}\mathbf{X}^{\top}\mathbf{b}))\]

Note that the last equality is simply \(\mathsf{vec}(f)(\mathsf{vec}(\mathbf{A}\mathbf{X}^{\top}\mathbf{b}))=f(\mathbf{ A}\mathbf{X}^{\top}\mathbf{b})\), but we work in the more general case of a matrix \(\mathbf{B}\) right now. We will need to simplify \(\mathsf{vec}(\mathbf{A}\mathbf{X}^{\top}\mathbf{b})\). It is more convenient during the first phase of the proof viewing \(\mathbf{b}\) as a \(m\times 1\) matrix and denote it using uppercase letter \(\mathbf{B}\). First, applying Lemma B.1 to \(\mathsf{vec}(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})\), we get

\[\mathsf{vec}(\mathbf{A}\mathbf{X}^{\top}\mathbf{B})=(\mathbf{B}^{\top}\otimes \mathbf{A})\mathsf{vec}(\mathbf{X}^{\top})\]

However, \(\mathsf{vec}(\mathbf{X}^{\top})\neq\mathsf{vec}(\mathbf{X})\) in general. However, these two expressions are related using the _commutation matrix_:

**Definition B.3** (Commutation matrix).: _Define \(\mathbf{K}_{m,n}\) to be the permutation matrix in \(\mathbb{R}^{mn\times mn}\) such that \(\mathbf{K}_{m,n}\mathsf{vec}(\mathbf{A})=\mathsf{vec}(\mathbf{A}^{\top})\) for all matrices \(\mathbf{A}\in\mathbb{R}^{m\times n}\)._See (Magnus and Neudecker, 2019, Ch.3-SS7). Below, we drop the subscripts in Definition B.3 and simply write \(\mathbf{K}:=\mathbf{K}_{m,n}\). Now, we have

\[\mathsf{vec}(\mathbf{AX}^{\top}\mathbf{B})=(\mathbf{B}^{\top}\otimes\mathbf{A}) \mathbf{K}^{\top}\mathsf{vec}(\mathbf{X})=(\mathbf{B}^{\top}\otimes\mathbf{A}) \mathbf{K}^{\top}\mathbf{x}\]

Thus

\[\mathsf{vec}(g)(\mathbf{x})=\mathsf{vec}(g)(\mathsf{vec}(\mathbf{X}))= \mathsf{vec}(f)((\mathbf{B}^{\top}\otimes\mathbf{A})\mathbf{K}^{\top}\mathbf{ x})\]

By Proposition B.2, we have

\[\nabla^{2}\mathsf{vec}(g)(\mathbf{x})\] \[=((\mathbf{B}^{\top}\otimes\mathbf{A})\mathbf{K}^{\top})^{\top} \nabla^{2}\mathsf{vec}(f)((\mathbf{B}^{\top}\otimes\mathbf{A})\mathbf{K}^{ \top}\mathbf{x})(\mathbf{B}^{\top}\otimes\mathbf{A})\mathbf{K}^{\top}\] \[=((\mathbf{B}^{\top}\otimes\mathbf{A})\mathbf{K}^{\top})^{\top} \nabla^{2}\mathsf{vec}(f)(\mathsf{vec}(\mathbf{AX}^{\top}\mathbf{B}))( \mathbf{B}^{\top}\otimes\mathbf{A})\mathbf{K}^{\top}.\]

From this, we see that (recall that \(\mathbf{v}=\mathsf{vec}(\mathbf{V})\))

\[\mathbf{v}^{\top}\nabla^{2}\mathsf{vec}(g)(\mathbf{x})\mathbf{v}=((\mathbf{ B}^{\top}\otimes\mathbf{A})\mathbf{K}^{\top}\mathbf{v})^{\top}\nabla^{2} \mathsf{vec}(f)(\mathsf{vec}(\mathbf{AX}^{\top}\mathbf{B}))(\mathbf{B}^{\top }\otimes\mathbf{A})\mathbf{K}^{\top}\mathbf{v}.\]

Now, by Lemma B.1, we have

\[(\mathbf{B}^{\top}\otimes\mathbf{A})\mathbf{K}^{\top}\mathbf{v}=(\mathbf{B}^ {\top}\otimes\mathbf{A})\mathbf{K}^{\top}\mathsf{vec}(\mathbf{V})=\mathsf{vec }(\mathbf{AV}^{\top}\mathbf{B}).\]

Now, since \(\mathbf{B}=\mathbf{b}\) is just a vector, we have

\[\mathsf{vec}(\mathbf{AV}^{\top}\mathbf{B})=\mathbf{AV}^{\top}\mathbf{b}.\]

and

\[\nabla^{2}\mathsf{vec}(f)(\mathsf{vec}(\mathbf{AX}^{\top}\mathbf{B}))=\nabla ^{2}f(\mathbf{AX}^{\top}\mathbf{b})\]

Putting it all together, we get the desired equality. 

## Appendix C PERM Losses That Satisfy Assumptions 3.1 and 3.2

### Cross-Entropy

By Wang and Scott (2024, Example 1), the cross-entropy loss \(\mathcal{L}_{y}\left(\mathbf{v}\right)=-\log\left(\frac{\exp(v_{y})}{\sum_{k= 1}^{K}\exp(v_{k})}\right)\) has template \(\psi\left(\mathbf{u}\right)=\log\left(1+\sum_{k=1}^{K-1}\exp\left(-u_{k}\right)\right)\). We calculate the partial derivatives:

\[\frac{\partial\psi}{\partial u_{i}}\left(\mathbf{u}\right)\] \[=-\frac{\exp\left(-u_{i}\right)}{1+\sum_{k=1}^{K-1}\exp\left(-u_{ k}\right)}\] (24) \[=-\frac{1}{1+\left(C_{i}+1\right)\exp\left(u_{i}\right)}\quad \text{where }C_{i}=\sum_{k\in[K-1]:k\neq i}\exp\left(-u_{k}\right).\] (25)

#### c.1.1 Convexity

Let us analyze the entries of the Hessian of the template, i.e. \(\nabla^{2}\psi\left(\mathbf{u}\right)\). Let \([\mathbf{A}]_{l,m}\) denote the element of \(\mathbf{A}\) at the \(l\)-th row and \(m\)-th column. We get for all \(i,j\in[K-1]\) where \(j\neq i\):

\[\left[\nabla^{2}\psi\left(\mathbf{u}\right)\right]_{i,i}=\frac{ \partial^{2}\psi\left(\mathbf{u}\right)}{\partial u_{i}^{2}}=\frac{\left(C_{i }+1\right)e^{-u_{i}}}{\left(1+\sum_{k=1}^{K-1}e^{-u_{k}}\right)^{2}}\] \[\left[\nabla^{2}\psi\left(\mathbf{u}\right)\right]_{i,j}=\left[ \nabla^{2}\psi\left(\mathbf{u}\right)\right]_{j,i}=\frac{\partial^{2}\psi \left(\mathbf{u}\right)}{\partial u_{i}u_{j}}=\frac{-e^{-u_{i}-u_{j}}}{\left(1 +\sum_{k=1}^{K-1}e^{-u_{k}}\right)^{2}}\]

From the definition of \(C_{i}\), this implies that:

\[\left[\nabla^{2}\psi\left(\mathbf{u}\right)\right]_{i,i}=\sum_{j\in[K-1],j\neq i }\left|\left[\nabla^{2}\psi\left(\mathbf{u}\right)\right]_{i,j}\right|+\frac{e^ {-u_{i}}}{\left(1+\sum_{k=1}^{K-1}e^{-u_{k}}\right)^{2}}\]

Thus, the Hessian is a symmetric _diagonally dominant_ matrix, and hence is positive semi-definite.

#### c.1.2 \(\beta\)-smoothness

For any diagonally dominant matrix \(\mathbf{B}\), let \(\left|\mathbf{B}\right|\) be the matrix obtained by taking the absolute value of each element of \(\mathbf{B}\), that is:

\[\left[\left|\mathbf{B}\right|\right]_{l,m}=\left|\left|\mathbf{B}\right|_{l,m} \right|\quad\text{for all }l,m.\]

Additionally, let \(\mathsf{diag}(\cdot):\mathbb{R}^{p}\rightarrow\mathbb{R}^{p\times p}\) (for any \(p\in\mathbb{N}\)) be the function that maps a vector to a diagonal matrix in the obvious way.

Then we have the following lemma:

**Lemma C.1**.: _Let \(\mathbf{B}^{\prime}:=\mathsf{diag}\left(\left|\mathbf{B}\right|\mathbf{1}\right)\) where \(\mathbf{1}\) is the appropriately-sized vector of all-1's. Then \(\mathbf{B}\preceq\mathbf{B}^{\prime}\)._

Proof.: This can be proven simply by observing that \(\mathbf{B}^{\prime}-\mathbf{B}\) is symmetric and diagonally dominant (and thus positive semi-definite). 

Lemma C.1 can be directly applied to analyze the Hessian (and eventually bound its maximum eigenvalue). Define \(\mathbf{H}^{\prime}=\mathsf{diag}\left(\left|\nabla^{2}\psi\left(\mathbf{u} \right)\right|\mathbf{1}\right)\). In other words, from Eqn. (25):

\[\left[\mathbf{H}^{\prime}\right]_{i,i} =\sum_{k=1}^{K-1}\left|\nabla^{2}\psi\left(\mathbf{u}\right) \right|_{i,k}=\frac{\left(2C_{i}+1\right)e^{-u_{i}}}{\left(1+\sum_{k=1}^{K-1} e^{-u_{k}}\right)^{2}}\] (26) \[\left[\mathbf{H}^{\prime}\right]_{i,j} =0\]

Thus, by directly applying Lemma C.1, we obtain

\[\nabla^{2}\psi\left(\mathbf{u}\right)\preceq\mathbf{H}^{\prime}\]

So now since \(\mathbf{H}^{\prime}\) is defined to be a diagonal matrix, all that's left to do is bound the diagonal entries by a positive constant. First note that from the definition of \(C_{i}\), it follows that \(1+\sum_{k=1}^{K-1}e^{-u_{k}}=C_{i}+1+e^{-u_{i}}\). Combining this with Eqn. (26), we get:

\[\frac{\left(2C_{i}+1\right)e^{-u_{i}}}{\left(\left(C_{i}+1\right)+e^{-u_{i}} \right)^{2}}=\frac{\left(2C_{i}+1\right)}{\left(\left(C_{i}+1\right)+e^{-u_{i }}\right)\left(\left(C_{i}+1\right)e^{u_{i}}+1\right)}\]

We can find a global minimum of the denominator of the above expression and thus arrive at an upper bound for the expression. Differentiating with respect to \(u_{i}\) and setting to 0 yields a single critical point at \(u_{i}=-\log\left(C_{i}+1\right)\), which produces a value of \(4\left(C_{i}+1\right)\) when substituted in the denominator (this is a global minimum of the denominator expression). Thus, we get

\[\left[\mathbf{H}^{\prime}\right]_{i,i}\leq\frac{\left(2C_{i}+1\right)}{4 \left(C_{i}+1\right)}=\frac{1}{2}-\frac{1}{4\left(C_{i}+1\right)}\]

In the binary i.e. \(K=2\) case, \(C_{i}=0\), so our bound is exactly \(1/4\). However, in the multiclass case (i.e. \(K>2\)), \(C_{i}\) can be arbitrarily large. Setting \(C_{i}=\infty\) yields a final upper bound of \(1/2\).

So our final bound can be summarized as follows:

\[\left\|\nabla^{2}\psi\left(\mathbf{u}\right)\right\|_{2}\leq\left[\mathbf{H}^ {\prime}\right]_{i,i}\leq\begin{cases}1/4&,\,\mathrm{if}\,K=2\\ 1/2&,\,\mathrm{if}\,K>2.\end{cases}\]

Thus, \(\beta=1/4\) for binary cross-entropy (logistic loss), but \(\beta=1/2\) for K-class cross-entropy.

#### c.1.3 Exponential Tail

We claim that for the cross-entropy Definition 2.2 holds with \(u_{\pm}=0\) and \(c=a=1\). We are interested in analyzing the (negative) gradient of the template. From Eqn. 24:

\[-\frac{\partial\psi}{\partial u_{i}}\left(\mathbf{u}\right)=\frac {e^{-u_{i}}}{1+\sum_{k=1}^{K-1}e^{-u_{k}}}\] \[\leq e^{-u_{i}}\] \[\geq e^{-u_{i}}\left(1-\sum_{k=1}^{K-1}e^{-u_{k}}\right)\quad \because\forall x\geq 0,\tfrac{1}{1+x}\geq 1-x\]This proves that the cross-entropy loss satisfies Definition 2.2 with \(u_{\pm}=0\) and \(c=1\).

### Multiclass Exponential Loss [Mukherjee and Schapire, 2013]

The multiclass exponential loss \(\mathcal{L}:\mathbb{R}^{K}\to\mathbb{R}\) can be written as \(\mathcal{L}_{y}(\mathbf{v})=\sum_{k\in[K]:k\neq y}\exp(-(v_{y}-v_{k}))\). Thus, the template function \(\psi:\mathbb{R}^{K-1}\to\mathbb{R}\) can be expressed as \(\psi(\mathbf{u})=\sum_{i\in[K-1]}e^{-u_{i}}\). The partial derivatives of the template are then simply:

\[\frac{\partial}{\partial u_{i}}\psi(\mathbf{u})=-e^{-u_{i}}\quad\text{for all }i\in[K-1].\] (27)

#### c.2.1 Convexity

We have

\[\nabla^{2}\psi(\mathbf{u})=\mathsf{diag}(\exp(-u_{i}):i=1,\dots,K-1).\] (28)

The Hessian is a diagonal matrix with all diagonal entries positive. Hence it is positive definite.

#### c.2.2 \(\beta\)-"smoothness"

Recall the identity derived in Lemma B.4:

\[\mathbf{v}^{\top}\nabla^{2}\overline{g}(\mathbf{x})\mathbf{v}=( \mathbf{AV}^{\top}\mathbf{b})^{\top}\nabla^{2}f(\mathbf{AX}^{\top}\mathbf{b}) \mathbf{AV}^{\top}\mathbf{b}^{\top}.\]

We are interested in the special case where \(\mathbf{X}\leftarrow\mathbf{W}\) is a linear classifier (represented as a matrix) and \(\mathbf{x}\leftarrow\mathsf{vec}(\mathbf{W})=\mathbf{w}\) is its vectorization as in Section 3. Moreover, \(g(\mathbf{X})\) represents the risk \(\mathcal{R}(\mathbf{W})\), viewed as a _matrix-input scalar-output_ function (defined in Appendix B), while \(\overline{g}(\mathbf{x})\) represents the _vectorized_ risk \(\mathcal{R}(\mathbf{w})\), viewed as a _vector-input scalar-output_ function (defined in Appendix B). We will use the formula in Lemma B.4 to calculate \(\mathbf{v}^{\top}\nabla^{2}\mathcal{R}(\mathbf{w})\mathbf{v}\), where we substitute in

\[\mathbf{A}\leftarrow\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\in \mathbb{R}^{(K-1)\times K},\qquad\mathbf{b}\leftarrow\mathbf{x}_{i}\in\mathbb{ R}^{d},\qquad f\leftarrow\psi.\]

where \((\mathbf{x}_{i},y_{i})\) is a training sample and \(\psi\) is the template of a PERM loss. Since \(\nabla^{2}\) is linear (i.e., distributive over additions), we have by Lemma B.4 that

\[\mathbf{v}^{\top}\nabla^{2}\mathcal{R}(\mathbf{w})\mathbf{v}= \mathsf{vec}(\mathbf{V})^{\top}\nabla^{2}\mathcal{R}(\mathbf{w})\mathsf{vec }(\mathbf{V})\] \[=\sum_{i=1}^{N}(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}^{\top} \mathbf{V}\mathbf{x}_{i})^{\top}\nabla^{2}\psi(\boldsymbol{\Upsilon}_{y_{i}} \mathbf{D}^{\top}\mathbf{W}\mathbf{x}_{i})\boldsymbol{\Upsilon}_{y_{i}} \mathbf{D}^{\top}\mathbf{V}\mathbf{x}_{i}.\]

Note that \(\|\mathbf{v}\|=\|\mathsf{vec}(\mathbf{V})\|=\|\mathbf{V}\|_{F}\) by the definitions of the Frobenius norm and vectorization. Thus,

\[\max_{\mathbf{v}\in\mathbb{R}^{dK}:\|\mathbf{v}\|=1}\mathbf{v}^{ \top}\nabla^{2}\mathcal{R}(\mathbf{w})\mathbf{v}=\max_{\mathbf{V}\in\mathbb{ R}^{d\times K}:\|\mathbf{V}\|_{F}=1}\mathsf{vec}(\mathbf{V})^{\top}\nabla^{2} \mathcal{R}(\mathbf{w})\mathsf{vec}(\mathbf{V}).\] (29)

We note that we never defined the Hessian of a _matrix_-input function, i.e., we do not work with \(\nabla^{2}\mathcal{R}(\mathbf{W})\). Combining the two previous identities, we have proven

**Corollary C.2**.: _Let \(\mathcal{R}(\mathbf{W})\) be the risk viewed as a matrix-input scalar-output function defined in Equation (3). Let \(\mathcal{R}(\mathbf{w})\) be the vectorization of \(\mathcal{R}(\mathbf{W})\). Then we have_

\[\max_{\mathbf{v}\in\mathbb{R}^{d\times K}:\|\mathbf{v}\|=1} \mathbf{v}^{\top}\nabla^{2}\mathcal{R}(\mathbf{w})\mathbf{v}\] \[=\max_{\mathbf{V}\in\mathbb{R}^{d\times K}:\|\mathbf{V}\|_{F}=1} \sum_{i=1}^{N}(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}^{\top}\mathbf{V} \mathbf{x}_{i})^{\top}\nabla^{2}\psi(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}^{ \top}\mathbf{W}\mathbf{x}_{i})\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}^{\top} \mathbf{V}\mathbf{x}_{i}.\]

We have

\[\nabla^{2}\psi(\mathbf{u})=\mathsf{diag}(\exp(-u_{i}):i=1,\dots,K-1).\] (30)

Let \(\mathsf{vdiag}(\cdot)\) be the "inverse" of \(\mathsf{diag}(\cdot)\), i.e., \(\mathsf{vdiag}(\cdot)\) takes a diagonal matrix and returns the vector of the diagonal elements.

[MISSING_PAGE_FAIL:18]

Now we will also analyze the Euclidean norm of the gradient, for reasons that will become clear later.

\[\left\|\nabla\mathcal{R}\left(\mathbf{w}\right)\right\|=\left\|\nabla \mathcal{R}\left(\mathbf{W}\right)\right\|_{F}\] \[=\left\|\sum_{i=1}^{N}\mathbf{x}_{i}\nabla\psi\left(\mathbf{ \Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}\right)^{\top} \mathbf{\Upsilon}_{y_{i}}\mathbf{D}\right\|_{F}\] \[\leq\sum_{i=1}^{N}\left\|\mathbf{x}_{i}\nabla\psi\left(\mathbf{ \Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}\right)^{\top} \mathbf{\Upsilon}_{y_{i}}\mathbf{D}\right\|_{F}\quad\because\text{triangle inequality}\] \[\leq\sum_{i=1}^{N}\left\|\mathbf{x}_{i}\right\|_{2}\left\|\nabla \psi\left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i} \right)\right\|_{2}\left\|\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\right\|_{F} \quad\because\|\mathbf{A}\mathbf{B}\|_{F}\leq\|\mathbf{A}\|_{F}\|\mathbf{B}\|_{F}\] \[=\sqrt{(2K-2)}\sum_{i=1}^{N}\left\|\mathbf{x}_{i}\right\|_{2} \left\|\nabla\psi\left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}^{\top} \mathbf{x}_{i}\right)\right\|_{2}\quad\because\text{Lemma C.3}\] \[\leq\sqrt{(2K-2)}\sum_{i=1}^{N}\left\|\mathbf{x}_{i}\right\| \mathcal{R}\left(\mathbf{w}\right)\quad\because\text{for all $a_{j}>0$, $M\geq 1$, $\sqrt{\sum_{i=1}^{M}a_{j}^{2}}\leq\sum_{i=1}^{M}a_{j}$}\]

Gradient descent is a special case of steepest descent with the Euclidean norm (Boyd and Vandenberghe, 2004). Thus, we can apply Gunasekar et al. (2018, Lemmas 11 & 12) to see that \(\nabla\mathcal{R}\left(\mathbf{w}\right)\to 0\) even for multiclass exponential loss. Elaborating on this: these lemmas from Gunasekar et al. (2018) assume a convex risk objective (which we have in the case of multiclass exponential loss). Additionally, they assume that \(\left\|\nabla\mathcal{R}\left(\mathbf{w}\right)\right\|\leq B\mathcal{R}\left( \mathbf{w}\right)\) and \(\left\|\nabla^{2}\mathcal{R}\left(\mathbf{w}\right)\right\|_{2}\leq B^{2} \mathcal{R}\left(\mathbf{w}\right)\). In the above section we prove these exact results with \(B\) as defined in Eqn. 31. Finally, Lemma C.3 below proves that \(\|\mathbf{\Upsilon}_{k}\mathbf{D}\|_{F}=\sqrt{2K-2}\) for all \(k\in[K-1]\).

In conclusion, by Gunasekar et al. (2018, Lemma 11), if our learning rate \(\eta<\frac{1}{B^{2}\mathcal{R}\left(\mathbf{w}\left(0\right)\right)}\), we can use Soudry et al. (2018, Lemma 10).

Finally, we calculate \(\|\mathbf{\Upsilon}_{k}\mathbf{D}\|_{F}\) which was used in several places above:

**Lemma C.3**.: _For each \(k\in[K]\), we have \(\|\mathbf{\Upsilon}_{k}\mathbf{D}\|_{F}=\sqrt{2(K-1)}\)._

Proof.: First, if \(k=K\), then \(\mathbf{\Upsilon}_{K}\) is the identity matrix. In this case, we have \(\mathbf{\Upsilon}_{k}\mathbf{D}=\mathbf{D}=[-\mathbf{I}_{K-1}\quad\mathbf{1}_ {K-1}]\) is the negative \((K-1)\)-by-\((K-1)\) identity matrix concatenated with the all-ones vector. Thus,

\[\|\mathbf{\Upsilon}_{k}\mathbf{D}\|_{F}^{2}=\|\mathbf{D}\|_{F}^{2}=\|\mathbf{ I}_{K-1}\|_{F}^{2}+\|\mathbf{1}_{K-1}\|^{2}=2(K-1)\]

If \(k\neq K\), then

\[\mathbf{\Upsilon}_{k}\mathbf{D}=[-\mathbf{\Upsilon}_{k}\quad\mathbf{\Upsilon} _{k}\mathbf{1}_{K-1}]\]

and so

\[\|\mathbf{\Upsilon}_{k}\mathbf{D}\|_{F}^{2}=\|\mathbf{\Upsilon}_{k}\|_{F}^{2}+ \|\mathbf{\Upsilon}_{k}\mathbf{1}_{K-1}\|^{2}\]

Now, we recall from the definition of \(\mathbf{\Upsilon}_{k}\) (Definition 2.4 of Wang and Scott (2024)) that \(\mathbf{\Upsilon}_{k}\) is obtained by replacing the \(k\)-th column of the identity matrix by the "all-negative-ones" vector. Thus

\[\|\mathbf{\Upsilon}_{k}\|_{F}^{2}=2(K-1)-1,\quad\text{and}\quad\|\mathbf{ \Upsilon}_{k}\mathbf{1}_{K-1}\|_{F}^{2}=\|-\mathbf{e}_{k}\|_{F}^{2}=1.\]

This proves Lemma C.3, as desired. 

#### c.2.3 Exponential Tail

From Eqn. (27), the negative partial derivative of the template is clearly always positive:

\[-\frac{\partial}{\partial u_{i}}\psi(\mathbf{u})=e^{-u_{i}}\geq 0.\]

From the above, it is clear that the upper and lower bounds in Definition 2.2 hold when \(u_{\pm}=0\) and \(c=1\).

### PairLogLoss (Wang et al., 2022)

Recall that the template of the PairLogLoss is \(\psi\left(\mathbf{u}\right)=\sum_{k=1}^{K-1}\log\left(1+\exp\left(-u_{k}\right)\right)\). By elementary calculus, we see that

\[\frac{\partial\psi(u)}{\partial u_{k}}=-\frac{e^{-u_{k}}}{1+e^{-u_{k}}}.\]

#### c.3.1 Convexity

We have

\[\nabla^{2}\psi(\mathbf{u})=\mathsf{diag}\left(e^{u_{i}}/\left(1+e^{u_{i}} \right)^{2}:i=1,\ldots,K-1\right).\]

The Hessian is a diagonal matrix with all diagonal entries positive. Hence it is positive definite.

#### c.3.2 \(\beta\)-smoothness

Notice that the partial derivative of the template is exactly the same expression as the derivative of the logistic loss (i.e. binary cross-entropy). Thus, the exact same proof as logistic loss can be used to prove \(\beta\)-smoothness for the PairLogLoss as well. Thus, from Appendix C.1.2, \(\beta=1/4\) (logistic loss is simply the \(K=2\) case for cross-entropy).

#### c.3.3 Exponential Tail

\[-\frac{\partial\psi(u)}{\partial u_{k}}=\frac{e^{-u_{k}}}{1+e^{-u_{k}}}\leq e ^{-u_{k}}\]

This gives us the desired upper tail. As for the lower tail:

\[-\frac{\partial\psi(u)}{\partial u_{k}}=\frac{e^{-u_{k}}}{1+e^{-u _{k}}}\] \[\geq e^{-u_{k}}\left(1-e^{-u_{k}}\right)\quad\because\frac{1}{1+x} \geq 1-x\text{ for all }x\geq 0\] \[\geq e^{-u_{k}}\left(1-\sum_{i=1}^{K-1}e^{-u_{i}}\right)\]

Thus, PairLogLoss satisfies Definition 2.2 with \(u_{\pm}=0\) and \(c=a=1\).

## Appendix D Pseudo-index

Note that \(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{R}(t)^{\top}\mathbf{x}_{i}\) produces a \((K-1)\)-dimensional vector with entries of the form of \(\left(\mathbf{r}_{y_{i}}(t)-\mathbf{r}_{k}(t)\right)^{\top}\mathbf{x}_{i}, \forall k\in[K]\backslash\{y_{i}\}\). For \(k\in[K]\backslash\{y_{i}\}\), let us represent the corresponding entry of the vector as \(\llbracket\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{R}(t)^{\top}\mathbf{x }_{i}\rrbracket_{k}\). Note that this indexing is not the same as the \(k^{th}\) entry of the vectors, since the \({y_{i}}^{th}\) entry \((\mathbf{r}_{y_{i}}(t)-\mathbf{r}_{y_{i}}(t))^{\top}\mathbf{x}_{i}\) is not present in the vector. Similarly, let us define \(\llbracket-\nabla\psi\left(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}( t)^{\top}\mathbf{x}_{i}\right)\rrbracket_{k}\) to be the corresponding entry of \(-\nabla\psi\).

This section makes this indexing trick rigorous.

**Lemma D.1**.: _Let \(\mathbf{W}\in\mathbb{R}^{d\times K}\) be arbitrary and \(\mathbf{w}:=\mathsf{vec}(\mathbf{W})\) be its vectorization. Let \((\mathbf{x}_{i},y_{i})\) be a training sample. Then there exists a bijection that depends only on \(y_{i}\) that maps the entries of_

\[\boldsymbol{\Upsilon}_{y}\mathbf{D}\mathbf{W}^{\top}\mathbf{x}_{i}\in\mathbb{ R}^{K-1}\]

_to the elements of the set of "\(y_{i}\)-versus-\(k\)" relative margins, i.e., \(\{\widetilde{\mathbf{x}}_{i,k}^{\top}\mathbf{w}\in\mathbb{R}:k\in[K]\setminus \{y_{i}\}\}\)._

The following definition makes the bijection from Lemma D.1 concrete.

**Definition D.1** (Pseudo-index).: _In the situation of Lemma D.1, define \(\llbracket\cdot\rrbracket_{i,k}:\mathbb{R}^{K-1}\to\mathbb{R}\) to be the coordinate projection such that \(\llbracket\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}\mathbf{x}_{i} \rrbracket_{i,k}=\widetilde{\mathbf{x}}_{i,k}^{\top}\mathbf{w}\). In other words, \(\llbracket\cdot\rrbracket_{i,k}\) selects the \(y_{i}\)-versus-\(k\) relative margin. When the sample index \(i\) is clear from context, we drop \(i\) from the subscript and simply write \(\llbracket\cdot\rrbracket_{k}\)._

[MISSING_PAGE_EMPTY:21]

Letting \(\mathbf{u}:=\mathbf{\Upsilon}_{y_{i}}\mathbf{D}^{\top}\mathbf{W}^{\top}\mathbf{x}_{i}\), using Lemma D.1 and Equation (33), we immediately prove Lemma D.2 in the case when \(y=K\). This is because we have

\[\exp(-u_{k})=\exp(-[\mathbf{\Upsilon}_{y_{i}}\mathbf{D}^{\top}\mathbf{W}^{\top} \mathbf{x}_{i}]_{k})=\exp(-\tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{w})\]

and

\[\sum_{r\in[K-1]}\exp\left(-u_{r}\right)=\sum_{r\in[K]\backslash\{y_{i}\}}\exp \left(-u_{r}\right)=\sum_{k\in[K]\backslash\{y_{i}\}}\exp\left(-\tilde{ \mathbf{x}}_{i,k}^{\top}\mathbf{w}\right)\]

When \(y=K\), a similar argument proves Lemma D.2 using Equation (33) for the pseudo-index \([\![\cdot]\!]_{k}\).

## Appendix E Proof of Lemma 4.8

Re-stating the lemma:

Lemma 4.8.: _(Multiclass generalization of Soudry et al. (2018, Lemma 11)) Consider any linearly separable dataset, and any PERM loss with template \(\psi\) that is convex, \(\beta\)-smooth, strictly decreasing, and non-negative. For all \(k\in\{1,...,K\}\), let \(\mathbf{w}_{k}(t)\) be the gradient descent iterates at iteration \(t\) for the \(k^{th}\) class. Then \(\forall i\in\{1,...,N\},\forall j\in\{1,...,K\}\backslash\{y_{i}\}:\lim_{t \rightarrow\infty}(\mathbf{w}_{y_{i}}(t)-\mathbf{w}_{j}(t))^{\top}\mathbf{x}_ {i}\rightarrow\infty\)._

Proof.: We know that \(\lim_{t\rightarrow\infty}\nabla\mathcal{R}\left(\mathbf{w}\left(t\right) \right)=\mathbf{0}\) by Soudry et al. (2018, Lemma 10).

This implies that \(\hat{\mathbf{w}}^{\top}\nabla\mathcal{R}(\mathbf{w}(t))\rightarrow\mathbf{0}\). Following the same steps as in the proof of Lemma 4.4, this is equivalent to saying:

\[\hat{\mathbf{w}}^{\top}\nabla\mathcal{R}(\mathbf{w}(t))=\text{tr}(\nabla\psi \left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i} \right)^{\top}\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\hat{\mathbf{W}}^{\top} \mathbf{x}_{i})\to 0.\]

However, for linearly separable data we know that \(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\hat{\mathbf{W}}^{\top}\mathbf{x}_{i} \succeq\mathbf{1}\) (since \(\hat{\mathbf{W}}\) here is the hard-margin SVM solution). Thus for the above limit to be true, the limit

\[\lim_{t\rightarrow\infty}\nabla\psi\left(\mathbf{\Upsilon}_{y_{i}}\mathbf{D} \mathbf{W}(t)^{\top}\mathbf{x}_{i}\right)=\mathbf{0}\]

must hold. By Proposition G.1, we have

\[\lim_{t\rightarrow\infty}\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{ \top}\mathbf{x}_{i}=\mathbf{\infty}\qquad\forall i\in[N]\]

where \(\mathbf{\infty}\) is the "vector" whose entries are all equal to infinity. This is equivalent to

\[\lim_{t\rightarrow\infty}(\mathbf{w}_{y_{i}}(t)-\mathbf{w}_{j}(t))^{\top} \mathbf{x}_{i}=\infty\qquad\forall i\in[N],\forall j\in[K]\backslash\{y_{i}\}\]

(since \([\![\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i}]\!]_ {j}=(\mathbf{w}_{y_{i}}(t)-\mathbf{w}_{j}(t))^{\top}\mathbf{x}_{i}\)). 

Note that in the binary case, the above "convergence-to-infinity" condition is for a scalar quantity, where the assumption that the loss be strictly decreasing and non-negative suffices. In the multiclass setting, we must ensure that all entries of the vector \(\mathbf{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i}\) converges to infinity. This is a nontrivial result and is addressed by our Proposition G.1.

## Appendix F Proof of Lemma 4.6

Let us first re-state the lemma we want to prove.

Lemma 4.6.: _(Generalization of Soudry et al. (2018, Lemma 20)) Define \(\theta\) to be the minimum SVM margin across all data points and classes, i.e., \(\theta=\min_{k}\left[\min_{n\notin S_{k}}\tilde{\mathbf{x}}_{n,k}^{\top} \hat{\mathbf{w}}\right]>1\). Then:_

\[\exists C_{1},C_{2},t_{1}:\,\forall t>t_{1}:\,\left(\mathbf{r}\left(t+1\right) -\mathbf{r}\left(t\right)\right)^{\top}\mathbf{r}\left(t\right)\leq C_{1}t^{- \theta}+C_{2}t^{-2}\]

[MISSING_PAGE_FAIL:23]

Therefore, the last two terms on the RHS of Equation (37) can be written as

\[\eta\sum_{i=1}^{N}\sum_{k\in[K]\setminus\{y_{i}\}}[\![-\nabla\psi\left( \boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{i} )\right]\!]_{k}[\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{R}(t)^{\top} \mathbf{x}_{i}]\!]_{k}-t^{-1}\hat{\mathbf{w}}^{\top}\mathbf{r}(t)\] \[=\eta\Big{(}\sum_{i=1}^{N}\sum_{k\in[K]\setminus\{y_{i}\}}[\![- \nabla\psi\left(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top} \mathbf{x}_{i}\right)]\!]_{k}[\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{ R}(t)^{\top}\mathbf{x}_{i}]\!]_{k}\] \[\qquad\qquad\qquad\qquad\qquad-t^{-1}\exp(-\tilde{\mathbf{w}}^{ \top}\tilde{\mathbf{x}}_{i,k})\tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{r}(t) \mathbbm{1}_{\{i\in S_{k}\}}\Big{)}\]

Since \(\eta>0\) is constant, we ignore it below and consider only the term inside the parenthesis:

\[\sum_{i=1}^{N}\sum_{k\in[K]\setminus\{y_{i}\}}[\![-\nabla\psi \left(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t)^{\top}\mathbf{x}_{ i}\right)]\!]_{k}[\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{R}(t)^{\top}\mathbf{x}_{i}]\!] \[\qquad\qquad\qquad\qquad\qquad-t^{-1}\exp(-\tilde{\mathbf{w}}^{ \top}\tilde{\mathbf{x}}_{i,k})\tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{r}(t) \mathbbm{1}_{\{i\in S_{k}\}}\] \[\overset{(1)}{=}\sum_{i=1}^{N}\sum_{k\in[K]\setminus\{y_{i}\}} \left([\![-\nabla\psi\left(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D}\mathbf{W}(t )^{\top}\mathbf{x}_{i}\right)]\!]_{k}\right.\] \[\qquad\qquad\qquad\qquad\qquad\left.-t^{-1}\exp(-\tilde{\mathbf{ w}}^{\top}\tilde{\mathbf{x}}_{i,k})\mathbbm{1}_{\{i\in S_{k}\}}\right) \tilde{\mathbf{x}}_{i,k}^{\top}\mathbf{r}(t)\] \[\overset{(2)}{\leq}\sum_{i=1}^{N}\sum_{k\in[K]\setminus\{y_{i}\}} \big{(}\exp\big{(}-\mathbf{w}(t)^{\top}\tilde{\mathbf{x}}_{i,k}\big{)}\] \[\qquad\qquad\qquad\qquad\qquad-t^{-1}\exp(-\tilde{\mathbf{w}}^{ \top}\tilde{\mathbf{x}}_{i,k})\mathbbm{1}_{\{i\in S_{k}\}}\big{)}\tilde{ \mathbf{x}}_{i,k}^{\top}\mathbf{r}(t)\mathbbm{1}_{\{\tilde{\mathbf{x}}_{i,k}^{ \top}\mathbf{r}(t)\geq 0\}}.\] (38)

In (1) we used Lemma D.1, which implies that \(\tilde{\mathbf{x}}_{j,k}^{\top}\mathbf{r}(t)=[\![\boldsymbol{\Upsilon}_{y_{i}} \mathbf{D}\mathbf{R}(t)^{\top}\mathbf{x}_{i}]\!]_{k}\). For (2), from the exponential tail upper/lower bound and Lemma D.2, we have that

\[\exp\left(-\mathbf{w}(t)^{\top}\tilde{\mathbf{x}}_{i,k}\right) \geq[\![-\nabla\psi\left(\boldsymbol{\Upsilon}_{y_{i}}\mathbf{D} \mathbf{W}(t)^{\top}\mathbf{x}_{i}\right)]\!]_{k}\] \[\geq\exp\left(-\mathbf{w}(t)^{\top}\tilde{\mathbf{x}}_{i,k}\right) \Big{(}1-\sum_{k\in[K]\setminus\{y_{i}\}}\exp(-\mathbf{w}(t)^{\top}\tilde{ \mathbf{x}}_{i,k})\Big{)}.\]

We note that Eqn. (38) above is identical to the right hand side of inequality (1) in [11, Eqn. (141)]. Thus, the remainder of the analysis proceeds identically as in Soudry et al. (2018, Lemma 20). 

## Appendix G A structural result on symmetric and convex functions

**Proposition G.1**.: _Let \(\psi:\mathbb{R}^{K-1}\to\mathbb{R}\) be the template of a PERM loss that satisfies our Theorem 3.4. Let \(\mathbf{u}^{t}\in\mathbb{R}^{K-1}\) be any sequence, where \(t=1,2,\ldots\), such that_

\[\lim_{t\to\infty}\nabla\psi(\mathbf{u}^{t})=\mathbf{0}\]

_is the zero vector. Then \(\lim_{t\to\infty}u_{j}^{t}=\infty\) for every \(j\in[K-1]\)._

We prove Proposition G.1 by first proving a structural result (Theorem G.2) concerning symmetric and convex function \(f:\mathbb{R}^{n}\to\mathbb{R}\). The proof of Proposition G.1 will be presented in Appendix G.2 as an application of the structural result, where we take \(f=\psi\), the template of a PERM loss, and \(n=K-1\), number of classes minus one.

Given a vector \(\mathbf{x}\in\mathbb{R}^{n}\) and a real number \(C\in\mathbb{R}\), define \(\mathbf{x}\lor C\in\mathbb{R}^{n}\) to be the vector such that

\[[\mathbf{x}\lor C]_{i}:=\max\{x_{i},C\},\quad\text{for all }i\in[n].\]

In other words, \(\mathbf{x}\lor C\) "boosts" entries of \(\mathbf{x}\) up to \(C\) if those entries are smaller than \(C\). Entries of \(\mathbf{x}\) larger than \(C\) are kept as-is.

Define \(\min(\mathbf{x})=\min_{j\in[n]}x_{j}\) and \(\operatorname{argmin}(\mathbf{x}):=\{i\in[n]:x_{i}=\min(\mathbf{x})\}\). We note the following easy-to-prove properties of the "\(\vee\)" operation:

1. \(\min(\mathbf{x}\lor C)\geq C\) with equality if \(\min(\mathbf{x})\leq C\),
2. \(\operatorname{argmin}(\mathbf{x}\lor C)\supseteq\operatorname{argmin}( \mathbf{x})\).

**Theorem G.2**.: _Suppose that \(f:\mathbb{R}^{n}\to\mathbb{R}\) is a symmetric, convex, and differentiable function. Then for any real number \(C\in\mathbb{R}\) and any \(\mathbf{x}\in\mathbb{R}^{n}\), we have_

\[\tfrac{\partial f}{\partial x_{i}}(\mathbf{x})\leq\tfrac{\partial f}{\partial x _{i}}(\mathbf{x}\lor C),\quad\text{for any }i\in\operatorname{argmin}(\mathbf{x}).\]

Before proceeding with the proof (which is in Appendix G.1), we first introduce some necessary preliminary notations and facts. Given a vector \(\mathbf{x}\in\mathbb{R}^{n}\), we define

\[\mathsf{val}(\mathbf{x}):=\{x_{i}:i=1,\ldots,n\}\]

to be the _set_ of values consignings of the entries of \(\mathbf{x}\). For example, if \(\mathbf{x}\) is the all-ones vector, then \(\mathsf{val}(\mathbf{x})=\{1\}\). Given \(v\in\mathsf{val}(\mathbf{x})\), we let \(\mathsf{idx}(v,\mathbf{x})=\{i\in\mathbf{x}:x_{i}=v\}\) be the set of indices that attains the value \(v\).

**Fact 1**: For a convex and differentiable function \(f:\mathbb{R}^{n}\to\mathbb{R}\), we have that

\[\langle\nabla f(\mathbf{x})-\nabla f(\mathbf{y}),\mathbf{x}-\mathbf{y}\rangle \geq 0,\quad\text{for all }\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}.\] (39)

This is a simple and well-known consequence of convexity. See this stackexchange answer for a short proof. When \(n=1\), Ineq. (39) is the fact that a convex differentiable function has nondecreasing derivative. Ineq. (39) is also a consequence of [20, Theorem 3.24].

**Fact 2**: For a symmetric and differentiable function \(f:\mathbb{R}^{n}\to\mathbb{R}\), we have that

\[\tfrac{\partial f}{\partial x_{i}}(\mathbf{x})=\tfrac{\partial f}{\partial x _{j}}(\mathbf{x}),\quad\text{whenever }x_{i}=x_{j}.\] (40)

This fact follows from the chain rule and the definition of a symmetric function. To be precise, let \(\mathbf{T}:\mathbb{R}^{n}\to\mathbb{R}^{n}\) be the permutation matrix that switches the \(i\) and \(j\)-th coordinate. Then \(f(\mathbf{x})=f(\mathbf{T}\mathbf{x})\) and moreover \(\tfrac{\partial f}{\partial x_{i}}(\mathbf{x})=\tfrac{\partial f}{\partial x _{i}}(\mathbf{T}\mathbf{x})=[\mathbf{T}\nabla f(\mathbf{T}\mathbf{x})]_{i}=[ \nabla f(\mathbf{T}\mathbf{x})]_{j}=[\nabla f(\mathbf{x})]_{j}=\tfrac{\partial f }{\partial x_{j}}(\mathbf{x})\).

### Proof of Theorem G.2

Now, to prove the above theorem, we will use induction on "\(m\)" in the following lemma, which is simply a "stratification" of Theorem G.2 into cases indexed by the "parameter" \(m\):

**Lemma G.3**.: _Suppose that \(f:\mathbb{R}^{n}\to\mathbb{R}\) is a convex, symmetric, and differentiable function. Let \(m\in\{0,1,\ldots,n\}\). Then for any real number \(C\in\mathbb{R}\) and any \(\mathbf{x}\in\mathbb{R}^{n}\) with the property that \(|\{v\in\mathsf{val}(\mathbf{x}):v<C\}|=m\), we have_

\[\tfrac{\partial f}{\partial x_{i}}(\mathbf{x})\leq\tfrac{\partial f}{\partial x _{i}}(\mathbf{x}\lor C),\quad\text{for any }i\in\operatorname{argmin}(\mathbf{x}).\]

Note that if we have proved Lemma G.3 for each \(m\in\{0,1,\ldots,n\}\), then Theorem G.2 holds.

**The base step**: we prove Lemma G.3 when \(m=0\) and \(m=1\). Strictly speaking, the proof-by-induction technique typically only involve _only_ the base case, which would be the \(m=0\) case in this instance. But below, we will see that in the induction step, the \(m=1\) case is helpful.

Note that the \(m=0\) case holds vacuously, since \(\mathbf{x}\lor C=\mathbf{x}\). Below, we focus on the \(m=1\) case, where there exists a unique \(v\in\mathsf{val}(\mathbf{x})\) such that \(v\leq C\). Let \(i\in\operatorname{argmin}(\mathbf{x})\). Note that we have \(\mathsf{idx}(v,\mathbf{x})=\operatorname{argmin}(\mathbf{x})\). Using Equation (39) (Fact 1), we have that

\[\langle\nabla f(\mathbf{x}\lor C)-\nabla f(\mathbf{x}),(\mathbf{x}\lor C)- \mathbf{x}\rangle\geq 0.\]

**Definition G.1**.: _Given any set \(S\subseteq[n]\), we let \(\chi_{S}\in\mathbb{R}^{n}\) denote the characteristic vector on \(S\): \(\chi_{S}\) is the vector whose \(j\)th entry is \(=1\) if \(j\in S\) and \(=0\) otherwise._

By construction, we have

\[(\mathbf{x}\lor C)-\mathbf{x}=(C-v)\chi_{\mathsf{idx}(v,\mathbf{x})}=(C-v) \chi_{\operatorname{argmin}(\mathbf{x})}.\]

The "\(\frac{\partial f}{\partial x_{i}}(\cdot)\)" notation for partial derivatives is a bit cumbersome. Instead, we will write "\([\nabla f(\cdot)]_{i}\)" from now on. By Equation (40) (Fact 2), we have

\[[\nabla f(\mathbf{x})]_{j}=[\nabla f(\mathbf{x})]_{j^{\prime}},\quad\text{for all }j,j^{\prime}\in\mathsf{idx}(v,\mathbf{x})\]

and likewise

\[[\nabla f(\mathbf{x}\lor C)]_{j}=[\nabla f(\mathbf{x}\lor C)]_{j^{\prime}}, \quad\text{for all }j,j^{\prime}\in\mathsf{idx}(v,\mathbf{x}).\]

Thus, by Equation (40), we have

\[\langle\nabla f(\mathbf{x}\lor C)-\nabla f(\mathbf{x}),(\mathbf{x}\lor C)- \mathbf{x}\rangle=|\operatorname{argmin}(\mathbf{x})|\cdot(C-v)([\nabla f( \mathbf{x}\lor C)]_{i}-[\nabla f(\mathbf{x})]_{i}).\]

Now, since \(C>v\) and \(|\operatorname{argmin}(\mathbf{x})|>0\), we must have that \([\nabla f(\mathbf{x}\lor C)]_{i}-[\nabla f(\mathbf{x})]_{i}\geq 0\), as desired. This proves the base step.

**Induction step**: Suppose Lemma G.3 holds for every integer \(m\) where \(0\leq m<n\), we must show that Lemma G.3 also holds for \(m+1\). To this end, let \(\mathbf{x}\in\mathbb{R}^{n}\) and \(C\in\mathbb{R}\) be such that \(|\{v\in\mathsf{val}(\mathbf{x}):v<C\}|=m+1\). Let \(v_{1},\ldots,v_{m+1}\in\mathbb{R}\) be all the elements of \(\{v\in\mathsf{val}(\mathbf{x})\mid v<C\}\) enumerated in increasing order, i.e., \(v_{1}<\cdots<v_{m+1}\).

Note by construction, we have that \(\{v\in\mathsf{val}(\mathbf{x})\mid v<v_{m+1}\}=\{v_{1},\ldots,v_{m}\}\) and so we immediately get that \(|\{v\in\mathsf{val}(\mathbf{x})\mid v<v_{m+1}\}|=m\). By the \(m\)-th case of Lemma G.3 (i.e., the induction hypothesis) using \(v_{m+1}\) as \(C\), we get

\[[\nabla f(\mathbf{x}\lor v_{m+1})]_{i}\geq[\nabla f(\mathbf{x})]_{i}\quad\text {for any }i\in\operatorname{argmin}(\mathbf{x}).\] (41)

Below fix some \(i\in\operatorname{argmin}(\mathbf{x})\) arbitrarily. Let \(\mathbf{x}^{\prime}:=\mathbf{x}\lor v_{m+1}\). We note that by construction, all the entries of \(\mathbf{x}^{\prime}\) that are less than \(C\) are set to equal to \(v_{m+1}\). In other words,

\[\{v\in\mathsf{val}(\mathbf{x}^{\prime}):v<C\}=\{v_{m+1}\}\]

is a singleton set. Thus, by the \(m=1\) case of Lemma G.3 applied to \(\mathbf{x}^{\prime}\), we get that

\[[\nabla f(\mathbf{x}^{\prime}\lor C)]_{i^{\prime}}\geq[\nabla f(\mathbf{x}^{ \prime})]_{i^{\prime}},\quad\text{for any }i^{\prime}\in\operatorname{argmin}(\mathbf{x}^{ \prime}).\]

Since \(\operatorname{argmin}(\mathbf{x}^{\prime})\supseteq\operatorname{argmin}( \mathbf{x})\), we have that \(i\in\operatorname{argmin}(\mathbf{x}^{\prime})\) as well (recall that \(i\) was chosen earlier from \(\operatorname{argmin}(\mathbf{x})\) arbitrarily). Thus the above inequality implies in particular that

\[[\nabla f(\mathbf{x}^{\prime}\lor C)]_{i}\geq[\nabla f(\mathbf{x}^{\prime})]_ {i}=[\nabla f(\mathbf{x}\lor v_{m+1})]_{i}.\]

Combined with Equation (41), we get

\[[\nabla f(\mathbf{x}^{\prime}\lor C)]_{i}\geq[\nabla f(\mathbf{x})]_{i}.\]

Finally, we note that \(\mathbf{x}^{\prime}\lor C=(\mathbf{x}\lor v_{m+1})\lor C=\mathbf{x}\lor C\). Thus, the above implies

\[[\nabla f(\mathbf{x}\lor C)]_{i}\geq[\nabla f(\mathbf{x})]_{i}\quad\text{for any }i\in\operatorname{argmin}(\mathbf{x})\]

since the choice of \(i\in\operatorname{argmin}(\mathbf{x})\) was arbitrary.

### Application to our setting

The condition \(\lim_{t\to\infty}u^{t}_{j}=\infty\) by definition means that for every real number \(M\in\mathbb{R}\), there exists \(T\) such that for all \(t\geq T\) we have \(u^{t}_{j}>M\). Thus, suppose that there exists \(j\in[K-1]\) such that \(\lim_{t\to\infty}u^{t}_{j}\neq\infty\), then there exists a real number \(M\in\mathbb{R}\) such that for all \(T=1,2,\ldots\) there exists some \(t\geq T\) such that \(u^{t}_{j}\leq M\). Passing to a subsequence, we assume that \(u^{t}_{j}\leq M\) (and so \(\min(\mathbf{u}^{t})\leq M\)) for all \(t=1,2,\ldots\). Note that \(\lim_{t\to\infty}\nabla\psi(\mathbf{u}^{t})=\mathbf{0}\) continues to hold.

Below, whenever we say "for all/every \(t\)", we mean "for all/every \(t=1,2,\ldots\)".

## Appendix H On the existence of \(\tilde{\mathbf{w}}\)

The goal of this section is to explain the challenge and the current gap in the proof of the existence of \(\tilde{\mathbf{w}}\) that satisfies the condition in Equation (8) for almost all linearly separable datasets. To this end, recall Equation (4), the hard-margin SVM formulated as a constrained optimization:

\[\hat{\mathbf{w}}=\operatorname*{argmin}_{\mathbf{w}}\frac{1}{2}\|\mathbf{w} \|^{2}\ \ \text{s.t.}\,\forall n,\forall k\neq y_{n}:\mathbf{w}_{y_{n}}^{\top}\mathbf{x}_{ n}\geq\mathbf{w}_{k}^{\top}\mathbf{x}_{n}+1.\] (44)

Moreover, recall that \(\mathcal{S}_{k}\), the set of support vectors for each \(k\in[K]\), is defined by

\[\mathcal{S}_{k}:=\{n:(\hat{\mathbf{w}}_{y_{n}}-\hat{\mathbf{w}}_{k})^{\top} \mathbf{x}_{n}=1\}.\]

The Lagrangian of the objective in Equation (44) is

\[L(\mathbf{w},\boldsymbol{\alpha})=\frac{1}{2}\sum_{r=1}^{K}\left\|\mathbf{w} _{r}\right\|^{2}+\sum_{n=1}^{N}\sum_{r\neq y_{n}}\alpha_{n,r}\left(\mathbf{w} _{y_{n}}-\mathbf{w}_{r}\right)^{\top}\mathbf{x}_{n}\] (45)

where \(\alpha_{n,r}\) are the dual variables. Let \(\delta_{i,j}\) denote the Kronecker delta, i.e., \(\delta_{i,j}=1\) if \(i=j\) and \(\delta_{i,j}=0\) otherwise. Taking the gradient of \(L(\mathbf{w},\boldsymbol{\alpha})\) with respect to \(\mathbf{w}_{k}\), we get

\[\mathbf{w}_{k}+\sum_{n=1}^{N}\sum_{r\neq y_{n}}\alpha_{n,k}\left(\delta_{r,y_ {n}}-\delta_{r,k}\right)\mathbf{x}_{n}=\mathbf{w}_{k}+\sum_{n=1}^{N}\left( \delta_{k,y_{n}}\sum_{r\neq y_{n}}\alpha_{n,r}-\alpha_{n,k}\right)\mathbf{x}_ {n}.\]So the KKT conditions satisfied by a stationary point \(\hat{\mathbf{w}}\) (hence globally optimal for Equation (44)) are

\[\forall k\in[K]:\hat{\mathbf{w}}_{k}=\sum_{n=1}^{N}\left(\alpha_{n,k}- \delta_{k,y_{n}}\sum_{r\neq k}\alpha_{n,r}\right)\mathbf{x}_{n}\] (46) \[\forall k\in[K]:\forall n:\text{one of the following holds}\begin{cases} \alpha_{n,k}\geq 0\ \ \text{and}\ \ (\hat{\mathbf{w}}_{y_{n}}-\hat{\mathbf{w}}_{k})^{\top}\,\mathbf{x}_{n}=1\\ \alpha_{n,k}=0\ \ \text{and}\ \ (\hat{\mathbf{w}}_{y_{n}}-\hat{\mathbf{w}}_{k})^{ \top}\,\mathbf{x}_{n}>1\end{cases}\] (47)

where Eqn. (47) (the second line) above is the complementary slackness condition.

The goal of this section is to prove the following result regarding the existence of \(\tilde{\mathbf{w}}\) that satisfies the condition in Equation (8), which we restate below:

\[\forall k\in[K],\forall n\in\mathcal{S}_{k}:\,\eta\exp\left(-\mathbf{x}_{n}^ {\top}\,(\tilde{\mathbf{w}}_{y_{n}}-\tilde{\mathbf{w}}_{k})\right)=\alpha_{n,k}.\] (48)

**Conjecture H.1**.: _For almost all linearly separable multiclass datasets, Assumption 4.1 holds, i.e., Eqn. (48) has a solution \(\tilde{\mathbf{w}}\)._

Below, we use the word "generically" to mean "for linearly separable datasets outside of a set of Lebesgue measure zero". In order for (48) to have a solution in \(\tilde{\mathbf{w}}\) generically, two conditions need to hold (generically).

**Condition 1**.: \(\alpha_{n,k}>0\) for all \(k\) and \(n\) such that \(n\in\mathcal{S}_{k}:=\{n:(\hat{\mathbf{w}}_{y_{n}}-\hat{\mathbf{w}}_{k})^{\top }\mathbf{x}_{n}=1\}\).

Condition 1 is already nontrivial and a gap in proving the Conjecture, as we will see below. For the sake of explaining Condition 2 below, let us assume Condition 1 holds. Then we can rewrite (48) as

\[\forall k\in[K],\forall n\in\mathcal{S}_{k}:\,\mathbf{x}_{n}^{\top}\,(\tilde{ \mathbf{w}}_{y_{n}}-\tilde{\mathbf{w}}_{k})=\log\left(\frac{\eta}{\alpha_{n,k }}\right).\] (49)

Define the vector \(\mathbf{m}_{n,k}\) obtained by taking the difference between the \(k\)-th and \(y_{n}\)-th elementary basis vector in \(\mathbb{R}^{K}\), i.e.,

\[\mathbf{m}_{n,k}:=\mathbf{e}_{k}-\mathbf{e}_{y_{n}}\in\mathbb{R}^{K}.\]

Then we can further rewrite (49) as

\[\forall k\in[K],\forall n\in\mathcal{S}_{k}:\,(\mathbf{m}_{n,k}\otimes \mathbf{x}_{n})^{\top}\tilde{\mathbf{w}}=\log\left(\frac{\eta}{\alpha_{n,k}} \right).\] (50)

It is more convenient to pool all the class-specific support vectors \(\mathcal{S}_{k}\) into a single set: \(\mathcal{S}\triangleq\left\{(n,k):(\hat{\mathbf{w}}_{y_{n}}-\hat{\mathbf{w}} _{k})^{\top}\,\mathbf{x}_{n}=1\right\}\). For readability, we linearly order the tuples in \(\mathcal{S}\), i.e., we assign to each \((n,k)\in\mathcal{S}\) a unique index \(i\in\{1,...,|\mathcal{S}|\}\). In other words, we define \(n(1),\ldots,n(|\mathcal{S}|)\) and \(k(1),\ldots,k(|\mathcal{S}|)\) such that

\[\mathcal{S}=\{(n(1),k(1)),\,(n(2),k(2)),\,\ldots,\,(n(|\mathcal{S}|),k(| \mathcal{S}|))\}.\]

To reduce notational clutter in the subscript, define \(\overline{\mathbf{x}}_{i}\triangleq\mathbf{x}_{n(i)}\) and \(\overline{\mathbf{m}}_{i}\triangleq\mathbf{m}_{n(i),k(i)}\). Finally, define

\[\overline{\mathbf{M}}\triangleq\left[\overline{\mathbf{m}}_{1},\ldots, \overline{\mathbf{m}}_{|\mathcal{S}|}\right]\mathbb{R}^{K\times|\mathcal{S}|}, \ \overline{\mathbf{X}}\triangleq\left[\overline{\mathbf{x}}_{1},\ldots,\overline{ \mathbf{x}}_{|\mathcal{S}|}\right]\in\mathbb{R}^{d\times|\mathcal{S}|},\ \text{and}\ \mathbf{G} \triangleq(\mathbf{M}\circ\mathbf{X})\in\mathbb{R}^{dK\times|\mathcal{S}|}.\]

with \(\circ\) denoting the Khatri-Rao product, which is, by definition, the matrix obtained by taking the Kronecker product of corresponding columns (Khatri and Rao, 1968). Note that the Khatri-Rao product is only defined for two matrices that have the same number of columns. See Liu (1999) for a reference. We now state

**Condition 2**.: \(\mathrm{rank}(\mathbf{G})=|\mathcal{S}|\) generically.

Note that given Condition 2, Eqn. (50) has a solution in \(\tilde{\mathbf{w}}\), while Condition 1 is necessary for the logarithm in (50) to be valid in the first place.

The challenge in proving Condition 2 in the multiclass case is that the column vectors of \(\overline{\mathbf{X}}\) may have repeats, i.e., it is possible for \(n(i)=n(i^{\prime})\) when \(i\neq i^{\prime}\). It is easy to generate synthetic linearly separable multiclass datasets satisfying this condition. Nonetheless, we observe that even in such a case, the matrix \(\mathbf{G}\) has rank \(|\mathcal{S}|\), i.e., Condition 2 holds. We verify this experimentally in the Python notebook checking_conjecture_in_Appendix_H.ipynb available at

https://github.com/YutongWangML/neurips2024-multiclass-IR-figures

In the binary case, linear classifiers are parametrized simply as a single vector, rather than the more cumbersome one-vector-per-class parametrization. Under the one-vector parametrization, the \(\overline{\mathbf{M}}\) matrix becomes a \(1\)-by-\(|\mathcal{S}|\) matrix consistings of only \(\pm 1\)'s, and \(\mathbf{G}\) reduces to \(\overline{\mathbf{X}}\). Moreover \(\overline{\mathbf{X}}\) has no repeats. Thus, Condition 2 holds trivially. In both the multiclass and binary settings, given Condition 2, the proof for Condition 1 can proceed exactly as in Lemma 12 from Soudry et al. (2018) where their \(\mathbf{X}_{\mathcal{S}}\) is replaced by our \(\mathbf{G}\).

## Appendix I Additional Experiments

We provide additional experimental support for our main theoretical result for the PairLogLoss (Wang et al., 2022). Code for recreating the figures can be found at

https://github.com/YutongWangML/neurips2024-multiclass-IR-figures

The code can be ran on Google Colab with a CPU runtime in under one hour.

Figure 2: Small simulation with \(N=10\), \(d=2\) and \(K=3\). The loss used is the PairLogLoss. _Top row:_ Decision regions of classifiers along the gradient path \(\mathbf{w}(t)\) at \(t=100\), \(1000\), and \(100000\), respectively from left to right. _Bottom row:_ Decision regions of the hard-margin multiclass SVM. Note that most of the progress is made between iterations 100 and 1000.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract clearly introduces the problem considered (implicit bias), identifies a gap in research (few multiclass results, which themselves are only for cross-entropy), and states our contributions (new ET property and implicit bias theorem for new losses). For the sake of brevity we do not state additional assumptions on the loss apart from ET (which we state later in the main text, i.e. smoothness, strictly decreasing, non-negative), because the ET property is a novel contribution and deserves to appear in the abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a separate section where we talk about our work's limitations. We highlight 2 natural questions one can ask: non-ET loss characterization, and non-asymptotic analysis (answering whether overfitting occurs after some _finite_ number of (S)GD timesteps). Guidelines: *

Figure 3: Large simulations with \(N=100\), \(d=10\) and \(K=3\). The loss used is the PairLogLoss. The curves are 10 independent runs with randomly sampled data and random initialization for gradient descent over 100000 iterations. Note that the convergence in direction of the gradient descent iterates to the hard-margin SVM slows down in log-log space.

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All assumptions are stated clearly in bold at the beginning of section 3, and also re-iterated multiple times throughout the paper, The assumption on the learning rate being sufficiently small is mentioned in the theorem statement. Partial proofs are provided in the main text because they highlight salient features of our techniques (namely, simple generalization of binary proof techniques to multiclass). Complete proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We use a simple synthetic setup which can be reproduced easily with Google Colab.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: In Appendices H and I, we include a link to our GitHub repo which contains the complete code to reproduce the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details can be found in the GitHub repository. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our experiments are used to illustrate the main theoretical result, which is of mathematical nature. All experiments support the convergence behavior that we analyzed. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, the experiments can be run with a Google Colab CPU runtime as mentioned in the Appendix. Guidelines:* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]Justification: [NA] Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.