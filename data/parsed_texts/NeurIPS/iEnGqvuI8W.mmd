# Estimating shape distances on neural representations with limited samples

 Dean A. Pospisil\({}^{1}\) **Brett W. Larsen\({}^{2,3,4}\) Sarah E. Harvey\({}^{2,3}\)  Alex H. Williams\({}^{2,3}\) \({}^{1}\)**Princeton University, Princeton, NJ, 08544; dp4846@princeton.edu

\({}^{2}\)New York University, Center for Neural Science, New York, NY, 10003 \({}^{3}\)Flatiron Institute, Center for Computational Neuroscience, New York, NY, 10010 \({}^{4}\)Flatiron Institute, Center for Computational Mathematics, New York, NY, 10010 {brettlarsen, sharvey, awilliams}@flatironinstitute.org

###### Abstract

Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of _shape distance_--a measure of representational dissimilarity proposed by Williams et al. [30]. These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, and we contribute a new estimation method well-suited to practical scientific settings.

## 1 Introduction

Many approaches have been proposed to quantify similarity in neural network representations. Some popular methods include canonical correlations analysis [21], centered kernel alignment [CKA; 13], representational similarity analysis [RSA; 14], and shape metrics [30]. Each of these approaches takes in a set of high-dimensional measurements from two networks--e.g., hidden layer activations or measured biological responses--and outputs a (dis)similarity score. Shape distances additionally satisfy the triangle inequality, thus enabling downstream algorithms for clustering and nearest-neighbor regression that leverage metric space structure [30]. These measures have numerous applications including comparisons of artificial and biological systems [10; 24], comparisons of neural activity across different animal species [15], quantifying how hidden layer activity differs across deep network architectures [18; 19], and many more [see 11, for review]

In many practical settings, these measures must be estimated over a finite set of sampled networks inputs. However, with the noteworthy exception of research on RSA [4; 22; 28], there is little work on quantifying uncertainty (e.g. through confidence intervals) on estimators of representational similarity. This poses a serious obstacle to adoption of these methods, particularly in experimental neuroscience where there is a hard limit on the number of conditions that can be feasibly sampled [23; 29].

We address these concerns in the context of measuring shape distances between neural representations [Fig. 1; 30]. First we obtain analytic upper and lower bounds on the accuracy of typical "plug-in estimates" of shape distance as as a function of the number of samples, \(M\), and the dimension of the representation, \(N\). We then propose a new method-of-moments estimator with an explicit and tunable tradeoff between estimator bias and variance to overcome the limitations of the plug-in estimator.

## 2 Results

We begin by reviewing generalized shape distances and the standard plug-in estimator (extended background can be found in App. A) Based on our theoretical characterization of the plug-in estimator in App. B, we find that plug-in estimates rapidly converge onto their expected value, but the expected error decays moderately slowly (i.e. the estimators have low variance and high bias). We thus introduce a method-of-moments estimator with tunable bias (Sec. 2.2) to overcome these shortcomings. We characterize the behavior of both estimators on synthetic (Sec. 2.3) and neural data (App. D.2). Finally, we discuss the implications of our results in Sec. 3.

### Problem Setting

We consider initially a simple setting where each neural network is a deterministic map (for the stochastic setting, see appendix A.2). A collection of \(K\) neural systems can then be viewed as a set of functions, each denoted \(h_{i}:\mathcal{Z}\mapsto\mathbb{R}^{N}\) for \(i\in\{1,\dots,K\}\). Here, \(\mathcal{Z}\) is a feature space and \(N\) can be interpreted as the number of neurons in each system (e.g. the size of a hidden layer in an artificial network, or the number of recorded neurons in a biological experiment).

Motivated by the shape theory literature [9; 30], we consider estimating the _Procrustes size-and-shape distance_, \(\rho\), and _Riemannian shape distance_, \(\theta\), between neural representations. Let \(h_{i}\) and \(h_{j}\) denote neural systems that are mean-centered and bounded:

\[\mathbb{E}[h_{i}(\bm{z})]=\mathbb{E}[h_{j}(\bm{z})]=\bm{0}\qquad\text{and} \qquad\|h_{i}(\bm{z})\|_{2},\|h_{j}(\bm{z})\|_{2}<B\sqrt{N}\quad\text{almost surely}.\] (2.1)

for some constant \(B>0\). Here, the expectations are taken over \(\bm{z}\sim P\), for some distribution \(P\) over network inputs. The Procrustes and Riemannian shape distances can be defined [App. D in 30]:

\[\rho(h_{i},h_{j}) =\min_{\bm{Q}\in\mathcal{O}(N)}\sqrt{\mathbb{E}[h_{i}(\bm{z})- \bm{Q}h_{j}(\bm{z})]_{2}^{2}}\] (2.2) \[\theta(h_{i},h_{j}) =\min_{\bm{Q}\in\mathcal{O}(N)}\cos^{-1}\left(\frac{\mathbb{E}[h _{i}(\bm{z})^{\mathsf{T}}\bm{Q}h_{j}(\bm{z})]}{\sqrt{\mathbb{E}[h_{i}(\bm{z})^ {\mathsf{T}}h_{i}(\bm{z})]\mathbb{E}[h_{j}(\bm{z})^{\mathsf{T}}h_{j}(\bm{z})]}}\right)\] (2.3)

where \(\mathcal{O}(N)\) denotes the set of \(N\times N\) orthogonal matrices.

Figure 1: **(A)** Classical shape distances [9] can be used to provide a rotation-invariant distance between neural representations [30]. Given two labelled points clouds in \(N\)-dimensional space (_left_ and _middle_), the distance is computed after an optimal orthogonal transformation is chosen to align the point clouds (_right_). In this visual example the point clouds trace out a low-dimensional manifold. **(B)** Heatmap shows the covariances \((\bm{\Sigma}_{ii},\bm{\Sigma}_{jj})\) and cross-covariance \((\bm{\Sigma}_{ij})\) of the 3D representations in panel A. Shape distances can be re-expressed in terms of these quantities (see eq. 2.5, 2.6). **(C)** Our ability to estimate the shape distance is related to \(M\), the number of stimuli. As \(M\) increases (_left_ to _right_) the number of sampled points along the underlying manifold increases, and we are better able to resolve shape differences between the representations.

It is well-known that the optimal orthogonal alignment in eqs. (2.2) and (2.3) can be identified in closed form (App. A). Leveraging this, we can use the covariance and cross-covariance matrices,

\[\bm{\Sigma}_{ii}=\mathbb{E}[h_{i}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}]\ \,\ \ \bm{\Sigma}_{jj}=\mathbb{E}[h_{j}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}]\ \,\ \ \bm{\Sigma}_{ij}=\mathbb{E}[h_{i}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}],\] (2.4)

to reformulate the squared Procrustes distance and cosine shape similarity:

\[\rho^{2}(h_{i},h_{j}) =\operatorname{Tr}[\bm{\Sigma}_{ii}]+\operatorname{Tr}[\bm{ \Sigma}_{jj}]-2\|\bm{\Sigma}_{ij}\|_{*}\] (2.5) \[\cos\theta(h_{i},h_{j}) =\frac{\|\bm{\Sigma}_{ij}\|_{*}}{\sqrt{\operatorname{Tr}[\bm{ \Sigma}_{ii}]\operatorname{Tr}[\bm{\Sigma}_{jj}]}}\] (2.6)

where \(\|\bm{\Sigma}_{ij}\|_{*}\) denotes the nuclear norm (or Shatten 1-norm) of the cross-covariance matrix.

Suppose we are given \(M\) independent and identically distributed network inputs \(\bm{z}_{1},\dots,\bm{z}_{M}\sim P\). We can estimate the generalized shape distances by substituting the empirical covariances:

\[\hat{\bm{\Sigma}}_{ii}=\tfrac{1}{M}\sum\limits_{m=1}^{M}h_{i}(\bm{z}_{m})h_{i }(\bm{z}_{m})^{\mathsf{T}},\ \hat{\bm{\Sigma}}_{jj}=\tfrac{1}{M}\sum\limits_{m=1}^{M}h_{j}(\bm{z}_{m})h_{j }(\bm{z}_{m})^{\mathsf{T}},\ \hat{\bm{\Sigma}}_{ij}=\tfrac{1}{M}\sum\limits_{m=1}^{M}h_{i}(\bm{z}_{m})h_{j }(\bm{z}_{m})^{\mathsf{T}}\] (2.7)

to approximate the true covariances appearing in eqs. (2.5) and (2.6). Thus,

\[\hat{\rho}^{2}(h_{i},h_{j}) =\operatorname{Tr}[\hat{\bm{\Sigma}}_{ii}]+\operatorname{Tr}[ \hat{\bm{\Sigma}}_{jj}]-2\|\hat{\bm{\Sigma}}_{ij}\|_{*}\] (2.8) \[\cos\hat{\theta}(h_{i},h_{j}) =\frac{\|\hat{\bm{\Sigma}}_{ij}\|_{*}}{\sqrt{\operatorname{Tr}[ \hat{\bm{\Sigma}}_{ii}]\operatorname{Tr}[\hat{\bm{\Sigma}}_{jj}]}}\] (2.9)

define plug-in estimators for the squared Procrustes and cosine Riemannian shape distances.

### A new estimator with controllable bias

The plug-in estimator of \(\|\bm{\Sigma}_{ij}\|_{*}\) has low variance but large and slowly decaying bias (see theorems B.2 and B.1). Here we develop an alternative estimator that is nearly unbiased.

First, note that the eigenvalues of \(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}}\) correspond to the squared singular values of \(\bm{\Sigma}_{ij}\). Thus, \(\operatorname{Tr}[(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}})^{1/2}]=\| \bm{\Sigma}_{ij}\|_{*}\), and so we can reduce our problem to estimating the trace of \((\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}})^{1/2}\), which is symmetric. Leveraging ideas from a well-developed literature [1], we proceed to define the \(p^{\text{th}}\) moment of this matrix as:

\[W_{p}=\operatorname{Tr}[(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}})^{p}]= \sum\limits_{n=1}^{N}\lambda_{n}^{p}\] (2.10)

where \(\lambda_{1},\dots,\lambda_{N}\) denote the eigenvalues of \(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}}\). Now, for any function \(f:\mathbb{R}\mapsto\mathbb{R}\) and symmetric matrix \(\bm{S}\) with eigenvalues \(\lambda_{1},\dots,\lambda_{N}\), we define1\(\operatorname{Tr}[f(\bm{S})]=\sum_{i}f(\lambda_{i})\). So long as \(f\) is reasonably well-behaved, we can approximate it using a truncated power series with \(P\) terms. Thus, with \(\bm{S}=\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}}\) and \(f(x)=\sqrt{x}\):

Footnote 1: This is a common convention to extend scalar functions [see e.g. 20, sec. 1.2.6].

\[\|\bm{\Sigma}_{ij}\|_{*}=\operatorname{Tr}[(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^ {\mathsf{T}})^{1/2}]\approx\sum\limits_{n=1}^{N}\sum\limits_{p=0}^{P}\gamma_{p }\lambda_{n}^{p}=\sum\limits_{p=0}^{P}\gamma_{p}\sum\limits_{n=1}^{N}\lambda_ {n}^{p}=\sum\limits_{p=0}^{P}\gamma_{p}W_{p}\] (2.11)

where \(\gamma_{0},\dots,\gamma_{P}\) are scalar coefficients.

In summary, we can estimate \(\|\bm{\Sigma}_{ij}\|_{*}\) by (a) specifying an estimator of the top eigenmoments, \(W_{1},\dots,W_{P}\), and (b) specifying a desired set of scalar coefficients \(\gamma_{0},\dots,\gamma_{P}\). To estimate the eigenmoments, we adapt procedures described by Kong and Valiant [12] to obtain unbiased estimates for each moment, \(\hat{W}_{1},\dots,\hat{W}_{P}\) (see App. C). To select the scalar coefficients, we propose an optimization procedure that trades off between bias and variance in the estimate of \(\|\bm{\Sigma}_{ij}\|_{*}\). Our starting point is the usual bias-variance decomposition:

\[\mathbb{E}\left[\left(\|\bm{\Sigma}_{ij}\|_{*}-\sum_{p}\gamma_{p}\hat{W}_{p} \right)^{2}\right]=\left(\mathbb{E}\left[\|\bm{\Sigma}_{ij}\|_{*}-\sum_{p} \gamma_{p}\hat{W}_{p}\right]\right)^{2}+\mathbb{V}\text{ar}\left[\sum_{p} \gamma_{p}\hat{W}_{p}\right].\] (2.12)Since \(\mathbb{E}[\hat{W}_{p}]=W_{p}=\sum_{n}\lambda_{n}^{p}\), the first term above (i.e. the "bias") simplifies and is upper-bounded:

\[\left(\mathbb{E}\left[\|\bm{\Sigma}_{ij}\|_{*}-\sum_{p}\gamma_{p}\hat{W}_{p} \right]\right)^{2}=\left(\sum_{n}\left(\lambda_{n}^{1/2}-\sum_{p}\gamma_{p} \lambda_{n}^{p}\right)\right)^{2}\leq\max_{0\leq x\leq 1}\left(N\left(x^{1/2}- \sum_{p}\gamma_{p}x^{p}\right)\right)^{2}\]

The inequality follows from replacing each term in the sum over \(n\) with the worst case approximation error of the polynomial expansion (given here as the maximization over \(x\)). Thus, we seek to:

\[\underset{\gamma_{0},\dots,\gamma_{P}}{\text{minimize}}\quad\max_{0\leq x \leq 1}\left(N\left(x^{1/2}-\sum_{p}\gamma_{p}x^{p}\right)\right)^{2}+\sum_{p,p^{ \prime}}\gamma_{p}\gamma_{p^{\prime}}\mathbb{C}\text{Cov}(\hat{W}_{p},\hat{W} _{p^{\prime}}).\] (2.13)

We estimate \(\mathbb{C}\text{Cov}(\hat{W}_{p},\hat{W}_{p^{\prime}})\) by bootstrapping--i.e. the empirical covariance of these statistics across re-sampled datasets where \(\bm{z}_{1:M}\) are sampled with replacement. Given this estimate of covariance, eq. (2.13) can be cast as a convex quadratic program and the maximal bias can be bounded to a user defined limit at the expense of variance (see App. C.2). We use the maximal bias (eq. 2.13, term 1) and variance (eq. 2.13, term 2) to form approximate confidence intervals (see App. C.3).

### Validation on synthetic data

We validate our method-of-moments estimator (section 2.2) on simulated representations jointly sampled from a multivariate normal distribution. We consider estimating the cosine shape similarity, \(\cos\theta\), defined in eq. 2.6. Our estimator of \(\|\bm{\Sigma}_{ij}\|_{*}\) is the principle novelty; thus, it is informative to understand its properties in isolation. To achieve this, in our experiments we use the ground truth covariance of \(\hat{W}_{p}\) (instead of an estimate from a bootstrap) and use the ground truth values of \(\operatorname{Tr}[\bm{\Sigma}_{ii}]\) and \(\operatorname{Tr}[\bm{\Sigma}_{jj}]\). To draw data for our simulations, we set the eigenvalues of the \(\bm{\Sigma}_{ii}\) and the singular values of \(\bm{\Sigma}_{ij}\) to a ground truth nuclear norm and similarity score. To demonstrate the estimators accuracy across the space of orthogonal transformations we apply a random orthogonal rotation matrix to each population's covariance in each new parameter setting.

We first compared the bias of the plug-in estimator to that of the moment-based estimator across a range of ground truth shape similarity values (Fig. 2A). As expected from our intuition discussed in App. B.1, the plug-in estimator (blue line) tends to grossly inflate estimated similarity when ground truth similarity is low (left side of plot). The moment-based estimator (orange line), in contrast, performs reasonably well over the full range of simulations, at the cost of modest increases in estimator variance (blue vs orange error bars).

Next, we fixed the ground truth similarity at 0.2 and studied the effect of sample size, \(M\) (Fig. 2B).The moment estimator (constrained to 5% bias) maintains small bias even with small \(M\), at the cost of high variance (large orange error bars). Increasing \(M\) quickly reduces the variance of the estimator. A similar story emerges when we fix \(M\) and vary the ambient dimension \(N\) (Fig. 2C). As the

Figure 2: Validation of estimator on synthetic data. **(A)** The moment based estimator (orange) compared to plug-in estimator (blue) in simulation with standard deviation bars calculated across simulations. Estimators are evaluated at 20 linearly spaced ground truth similarity score values. **(B)** Effect of increasing sample size when moment estimator is constrained to have a bias less than 5 %. **(C)** Effect of increasing dimensionality. **(D)** Demonstration of conservative confidence intervals that account for variance and maximal bias of moment estimator. We do not include CIs for the plug in estimator (implied by theorem B.1) because for small sample sizes, the theoretical bounds on estimator bias always contain far more than the entire allowable interval (\([0,1]\)).

dimensionality increases, the the plug-in estimator bias quickly explodes. In contrast, the moment estimator (here constrained to 10% bias) has roughly constant bias; however, it's variance grows with \(N\). Thus our estimator bias outperforms the plug-in sample size is low and dimensionality is high.

Finally, an important property of the moment-based estimator is our ability to compute approximate confidence intervals (CI) (see App. C.3). We demonstrate 95% CIs across simulations in Figure 2D (shaded orange region). These CIs are conservative, the true shape score is not within the CI's for only 2.3% of simulations. Results on neural data can be found in App. D.2.

## 3 Discussion

There is a vast literature of papers that utilize or develop measures of representational similarity between neural networks (see 11, for review), and recent works have shown interest in leveraging representational distances that satisfy the triangle inequality [6, 7, 16, 30]. Yet, the statistical properties of these shape distance measures appears understudied. Here, rigorously analyzed of "plug-in" estimates of shape distance in high-dimensional, noisy, and sample-limited regimes. Our analysis showed that these estimates _(a)_ tend to over-estimate representational similarity when the true similarity is small and _(b)_ require a large number of samples, \(M\), to overcome this bias in high dimensional regimes. Theorems B.1 and B.2 provide precise guarantees on the worst-case performance of plug-in estimators, which should guide the design of biological experiments and analyses of their statistical power.

An equally important contribution of our work is to provide a practical method to _(a)_ reduce the bias of plug-in estimators of shape distance, _(b)_ quantify uncertainty in shape distance estimates, and _(c)_ enable practicioners to explicitly trade off estimator bias and variance. When employed on a biological dataset published by Stringer et al. [25], we find that shape similarity estimates are highly uncertain, revealing the challenging nature of the problem in high dimensions and with noisy data. Importantly, this degree of uncertainty is not obvious from the procedures and plug-in estimates advertised by existing work on this subject.

Both theoretical and methodological aspects of our work may be of broader interest beyond the immediate subject of shape distance estimation. We have seen that estimating the nuclear norm of the cross-covariance, \(\|\bm{\Sigma}_{ij}\|_{*}\), is the key challenge in our problem. Estimating the spectrum of cross-covariance matrices is a topic of contemporary interest [2], and further exploring the connections between this problem and shape distance estimation is an intriguing direction. Similarly, the method-of-moments estimator presented in section 2.2 is broadly applicable to generalized trace estimation [1]. While others have used polynomial expansions in this context [17], a key novelty of our approach is the selection of coefficients with a tunable parameter that explicitly trades off estimator bias and variance. A more typical approach would be to choose these coefficients based on a Chebyshev polynomial expansion. While elegant, we believe our procedure for tuning these coefficients will be more relevant to scientific applications where samples are limited (such as neural data) and practitioners desire finer-scale control.

In summary, our work is one of the first to rigorously interrogate the statistical challenges of estimating shape distances in high-dimensional spaces. While shape distances can be well-behaved in certain settings (e.g. in artificial networks where a very large number of inputs can be sampled), our theoretical results and empirical observations underscore the challenging nature of this problem, suggesting the need for carefully designed biological experiments and estimation procedures.

## References

* Adams et al. [2018] R. P. Adams, J. Pennington, M. J. Johnson, J. Smith, Y. Ovadia, B. Patton, and J. Saunderson. Estimating the spectral density of large implicit matrices, 2018.
* 1326, 2023. doi: 10.1214/22-AAP1842. URL https://doi.org/10.1214/22-AAP1842.
* Boyd and Vandenberghe [2004] S. Boyd and L. Vandenberghe. _Convex optimization_. Cambridge University Press, 2004.
* Cai et al. [2018] M. Cai, N. W. Schuck, J. W. Pillow, and Y. Niv. A bayesian method for reducing bias in neural representational similarity analysis. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Paper.pdf.
* Cohen and Welling [2016] T. Cohen and M. Welling. Group equivariant convolutional networks. In M. F. Balcan and K. Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 2990-2999, New York, New York, USA, 20-22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/cohenc16.html.
* Duong et al. [2023] L. R. Duong, J. Zhou, J. Nassar, J. Berman, J. Olielslagers, and A. H. Williams. Representational dissimilarity metric spaces for stochastic neural networks. In _International Conference on Learning Representations_, 2023.
* Giaffar et al. [2023] H. Giaffar, C. R. Buxo, and M. Aoi. The effective number of shared dimensions: A simple method for revealing shared structure between datasets. _bioRxiv_, 2023. doi: 10.1101/2023.07.27.550815. URL https://www.biorxiv.org/content/early/2023/07/28/2023.07.27.550815.
* Gower and Dijksterhuis [2004] J. C. Gower and G. B. Dijksterhuis. _Procrustes problems_, volume 30. OUP Oxford, 2004.
* Kendall et al. [2009] D. G. Kendall, D. Barden, T. K. Carne, and H. Le. _Shape and Shape Theory_. John Wiley & Sons, Sept. 2009.
* Kietzmann et al. [2019] T. C. Kietzmann, C. J. Spoerer, L. K. A. Sorensen, R. M. Cichy, O. Hauk, and N. Kriegeskorte. Recurrence is required to capture the representational dynamics of the human visual system. _Proceedings of the National Academy of Sciences_, 116(43):21854-21863, 2019. doi: 10.1073/pnas.1905544116. URL https://www.pnas.org/doi/abs/10.1073/pnas.1905544116.
* Klabunde et al. [2023] M. Klabunde, T. Schumacher, M. Strohmaier, and F. Lemmerich. Similarity of neural network models: A survey of functional and representational measures, 2023.
* 2247, 2017. doi: 10.1214/16-AOS1525. URL https://doi.org/10.1214/16-AOS1525.
* Kornblith et al. [2019] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In K. Chaudhuri and R. Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 3519-3529. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/kornblith19a.html.
* connecting the branches of systems neuroscience. _Front. Syst. Neurosci._, 2:4, Nov. 2008.
* Kriegeskorte et al. [2008] N. Kriegeskorte, M. Mur, D. A. Ruff, R. Kiani, J. Bodurka, H. Esteky, K. Tanaka, and P. A. Bandettini. Matching categorical object representations in inferior temporal cortex of man and monkey. _Neuron_, 60(6):1126-1141, Dec. 2008.
* Lange et al. [2022] R. D. Lange, D. Kwok, J. Matelsky, X. Wang, D. S. Rolnick, and K. P. Kording. Neural networks as paths through the space of representations. _arXiv preprint arXiv:2206.10999_, 2022.
* Lin et al. [2016] L. Lin, Y. Saad, and C. Yang. Approximating spectral densities of large matrices. _SIAM Review_, 58(1):34-65, 2016. doi: 10.1137/130934283.
* Maheswaranathan et al. [2019] N. Maheswaranathan, A. Williams, M. Golub, S. Ganguli, and D. Sussillo. Universality and individuality in neural dynamics across large populations of recurrent networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/5f5d472067f77b5c88f69f1bcfdale08-Paper.pdf.
* Nguyen et al. [2021] T. Nguyen, M. Raghu, and S. Kornblith. Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. In _International Conference on Learning Representations_, 2021.

* Potters and Bouchaud [2020] M. Potters and J.-P. Bouchaud. _A First Course in Random Matrix Theory: for Physicists, Engineers and Data Scientists_. Cambridge University Press, 2020. doi: 10.1017/9781108768900.
* Raghu et al. [2017] M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Schutt et al. [2021] H. H. Schutt, A. D. Kipnis, J. Diedrichsen, and N. Kriegeskorte. Statistical inference on representational geometries, 2021.
* Shi et al. [2019] J. Shi, E. Shea-Brown, and M. Buice. Comparison against task driven artificial neural networks reveals functional properties in mouse visual cortex. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/748d6b6ed8e13f857ceaa6cfbdca14b8-Paper.pdf.
* Storrs et al. [2021] K. R. Storrs, T. C. Kietzmann, A. Walther, J. Mehrer, and N. Kriegeskorte. Diverse Deep Neural Networks All Predict Human Inferior Temporal Cortex Well, After Training and Fitting. _Journal of Cognitive Neuroscience_, 33(10):2044-2064, 09 2021. ISSN 0898-929X. doi: 10.1162/jocn_a_01755. URL https://doi.org/10.1162/jocn_a_01755.
* Stringer et al. [2019] C. Stringer, M. Pachitariu, N. Steinmetz, M. Carandini, and K. D. Harris. High-dimensional geometry of population responses in visual cortex. _Nature_, 571(7765):361-365, 2019.
* Tropp [2015] J. A. Tropp. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015. ISSN 1935-8237. doi: 10.1561/2200000048. URL http://dx.doi.org/10.1561/2200000048.
* Wainwright [2019] M. J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* Walther et al. [2016] A. Walther, H. Nili, N. Ejaz, A. Alink, N. Kriegeskorte, and J. Diedrichsen. Reliability of dissimilarity measures for multi-voxel pattern analysis. _NeuroImage_, 137:188-200, 2016.
* Williams and Linderman [2021] A. H. Williams and S. W. Linderman. Statistical neuroscience in the single trial limit. _Current Opinion in Neurobiology_, 70:193-205, 2021. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2021.10.008. URL https://www.sciencedirect.com/science/article/pii/S0959438821001203. Computational Neuroscience.
* Williams et al. [2021] A. H. Williams, E. Kunz, S. Kornblith, and S. Linderman. Generalized shape metrics on neural representations. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 4738-4750. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/252a3dbaeb32e7690242ad3b556e626b-Paper.pdf.

Appendix: Background on Generalized Shape Metrics

### Definition of Generalized Shape Metrics

Intuitively, a measure of distance between neural representations should be invariant nuisance symmetries in the neural representation, such as arbitrary permutations over neuron labels [5]. Representational similarity measures are typically designed to be invariant not only to permutations, but also to rotations, reflections, translations, and isotropic scalings in neural firing rate space [13; 14].

We begin by considering a simple setting where each neural network is a deterministic map (for the stochastic setting, see appendix A.2). A collection of \(K\) neural systems can then be viewed as a set of functions, each denoted \(h_{i}:\mathcal{Z}\mapsto\mathbb{R}^{N}\) for \(i\in\{1,\ldots,K\}\). Here, \(\mathcal{Z}\) is a feature space and \(N\) can be interpreted as the number of neurons in each system (e.g. the size of a hidden layer in an artificial network, or the number of recorded neurons in a biological experiment).2

Footnote 2: The assumption that each layer has the same number of neurons is not necessary, and only made for convenience. For networks with dissimilar sizes, we can preprocess by zero-padding the smaller network. If necessary, one could alternatively perform PCA on the larger network to reduce to a common dimension.

Motivated by the shape theory literature [9; 30], we consider estimating the _Procrustes size-and-shape distance_, \(\rho\), and _Riemannian shape distance_, \(\theta\), between neural representations. Let \(h_{i}\) and \(h_{j}\) denote neural systems that are mean-centered and bounded:

\[\mathbb{E}[h_{i}(\bm{z})]=\mathbb{E}[h_{j}(\bm{z})]=\bm{0}\qquad\text{and} \qquad\|h_{i}(\bm{z})\|_{2},\|h_{j}(\bm{z})\|_{2}<B\sqrt{N}\quad\text{almost surely.}\]

for some constant \(B>0\). Here, the expectations are taken over \(\bm{z}\sim P\), for some distribution \(P\) over network inputs. Our assumption that neural population rates are bounded by \(B\sqrt{N}\) can be achieved by assuming that each neuron has a maximum firing rate equal to \(B\). This assumption is common in the literature and reasonable in both artificial networks (since connection weights are finite) and biological networks (since neurons have a maximal firing rate).

The Procrustes and Riemannian shape distances can be defined [App. D in 30]:

\[\rho(h_{i},h_{j}) =\min_{\bm{Q}\in\mathcal{O}(N)}\sqrt{\mathbb{E}\|h_{i}(\bm{z})- \bm{Q}h_{j}(\bm{z})\|_{2}^{2}}\] \[\theta(h_{i},h_{j}) =\min_{\bm{Q}\in\mathcal{O}(N)}\cos^{-1}\left(\frac{\mathbb{E}[h _{i}(\bm{z})^{\mathsf{T}}\bm{Q}h_{j}(\bm{z})]}{\sqrt{\mathbb{E}[h_{i}(\bm{z}) ^{\mathsf{T}}h_{i}(\bm{z})]\mathbb{E}[h_{j}(\bm{z})^{\mathsf{T}}h_{j}(\bm{z} )]}}\right)\]

where \(\mathcal{O}(N)\) denotes the set of \(N\times N\) orthogonal matrices. Again, all expectations are taken over \(\bm{z}\sim P\). Note that different notions of distance arise from different choices of input distribution, \(P\).

To simplify our analysis and exposition, we will focus on estimating the _squared Procrustes distance_, \(\rho^{2}\), and what we call the _cosine shape similarity_, \(\cos\theta\). Thus, we ignore the square root term in eq. (2.2) and the arccosine term in eq. (2.3), but it should be kept in mind that one must apply these nonlinear functions to achieve a proper metric.

Properties of Shape DistanceIt is easy to verify that shape distances are invariant to rotations and reflections: that is, if \(r:\mathbb{R}^{N}\mapsto\mathbb{R}^{N}\) is an orthogonal transformation, then for any function \(h:\mathcal{Z}\mapsto\mathbb{R}^{N}\) representing a neural system we have \(\rho(h,r\circ h)=\theta(h,r\circ h)=0\), where '\(\circ\)' denotes function composition. Furthermore, \(\rho\) and \(\theta\) are proper metrics, meaning that:

\[\rho(h_{i},h_{j})=\rho(h_{j},h_{i})\quad\text{and}\quad\rho(h_{i},h_{j})\leq \rho(h_{i},h_{k})+\rho(h_{k},h_{j})\quad\forall\,i,j,k\in\{1,\ldots,K\},\] (A.1)

and likewise for \(\theta\). These properties are fundamental to rigorously establishing downstream analyses, such as for clustering networks with similar representations [30].

It is well-known that the optimal orthogonal alignment appearing in eqs. (2.2) and (2.3) can be identified in closed form. Leveraging this, we can use the covariance and cross-covariance matrices,

\[\bm{\Sigma}_{ii}=\mathbb{E}[h_{i}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}]\ \,\ \ \bm{\Sigma}_{jj}=\mathbb{E}[h_{j}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}]\ \,\ \ \bm{\Sigma}_{ij}=\mathbb{E}[h_{i}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}],\] (A.2)

to reformulate the squared Procrustes distance and cosine shape similarity:

\[\rho^{2}(h_{i},h_{j}) =\operatorname{Tr}[\bm{\Sigma}_{ii}]+\operatorname{Tr}[\bm{ \Sigma}_{ii}]-2\|\bm{\Sigma}_{ij}\|_{*}\] \[\cos\theta(h_{i},h_{j}) =\frac{\|\bm{\Sigma}_{ij}\|_{*}}{\sqrt{\operatorname{Tr}[\bm{ \Sigma}_{ii}]\operatorname{Tr}[\bm{\Sigma}_{jj}]}}\]where \(\|\bm{\Sigma}_{ij}\|_{*}\) denotes the nuclear norm (or Shatten 1-norm) of the cross-covariance matrix:

\[\|\bm{\Sigma}_{ij}\|_{*}=\sum_{n=1}^{N}s_{n}(\bm{\Sigma}_{ij})\] (A.3)

where \(s_{1}(\bm{M})\geq\cdots\geq s_{N}(\bm{M})\geq 0\) denote the singular values of a matrix \(\bm{M}\). Equations 2.5 and 2.6 are derived in Appendix A.3 to provide the reader with a self-contained narrative.

Plug-in EstimatorsSuppose we are given \(M\) independent and identically distributed network inputs \(\bm{z}_{1},\ldots,\bm{z}_{M}\sim P\). How well can we approximate the shape distances between two networks, as a function of \(M\)? The standard approach, which was previously used in Williams et al. [30], is to use a _plug-in estimator_ in which one computes eqs. (2.2) and (2.3) after identifying the optimal \(\bm{Q}\in\mathcal{O}(N)\). As we show in App. A.4, this is equivalent to estimating the squared Procrustes and cosine Riemannian distances by substituting the empirical covariances:

\[\hat{\bm{\Sigma}}_{ii}=\tfrac{1}{M}\!\!\sum\limits_{m=1}^{M}h_{i}(\bm{z}_{m}) h_{i}(\bm{z}_{m})^{\mathsf{T}},\ \hat{\bm{\Sigma}}_{jj}=\tfrac{1}{M}\!\!\sum\limits_{m=1}^{M}h_{j}(\bm{z}_{m}) h_{j}(\bm{z}_{m})^{\mathsf{T}},\ \hat{\bm{\Sigma}}_{ij}=\tfrac{1}{M}\!\!\sum\limits_{m=1}^{M}h_{i}(\bm{z}_{m})h_ {j}(\bm{z}_{m})^{\mathsf{T}}\] (A.4)

to approximate the true covariances appearing in eqs. (2.5) and (2.6). Thus,

\[\hat{\rho}^{2}(h_{i},h_{j}) =\operatorname{Tr}[\hat{\bm{\Sigma}}_{ii}]+\operatorname{Tr}[ \hat{\bm{\Sigma}}_{ii}]-2\|\hat{\bm{\Sigma}}_{ij}\|_{*}\] \[\cos\hat{\theta}(h_{i},h_{j}) =\frac{\|\hat{\bm{\Sigma}}_{ij}\|_{*}}{\sqrt{\operatorname{Tr}[ \hat{\bm{\Sigma}}_{ii}]\operatorname{Tr}[\hat{\bm{\Sigma}}_{jj}]}}\]

define plug-in estimators for the squared Procrustes and cosine Riemannian shape distances. The empirical behavior of these estimators as a function of \(M\) was only briefly characterized by Williams et al. [30] for a pair of artificial networks trained on CIFAR-10.

### Extension to stochastic networks

Thus far, we have modeled neural networks as deterministic mappings, \(h_{i}:\mathcal{Z}\mapsto\mathbb{R}^{N}\). This assumption is not satisfied in biological data and in many artificial networks (e.g. VAEs). Here, we briefly explain how to extend the estimators to the stochastic setting. In this setting, the response of network \(i\) can be written as \(h_{i}(\bm{z})+\epsilon_{i}(\bm{z})\). As before, \(h_{i}(\bm{z})\) is a deterministic mapping conditioned on a random variable \(\bm{z}\sim P\). The "noise" term \(\epsilon_{i}(\bm{z})\) is a mean-zero random variable that, in addition to inherting the randomness of \(\bm{z}\), captures the stochastic elements of each forward pass through the network (i.e. trial-to-trial variability even when the stimulus is fixed). Importantly, noise contributions are independent and identically distributed for each pass through the network.

Given a second stochastic network with same structure, \(h_{j}(\bm{z})+\epsilon_{j}(\bm{z})\), our goal is to estimate the shape distances eqs. (2.2) and (2.3) as before, effectively ignoring contributions of the "noise" terms \(\epsilon_{i}(\cdot)\) and \(\epsilon_{j}(\cdot)\). Ignoring these terms is not wholly justified, since it is of great interest to quantify how noise varies across networks [6]. Nonetheless, it is useful to develop metrics that isolate the "signal" component of neural representations, and a full development of methods to quantify similarity in noise structure is outside the scope of this paper.

Our basic observation is that it suffices to consider two replicates for each network input. That is, let \(\bm{z}^{\prime}=\bm{z}\) where \(\bm{z}\sim P\). Then, \(\bm{\Sigma}_{ii}=\mathbb{E}[h_{i}(\bm{z})h_{i}(\bm{z}^{\prime})^{\mathsf{T}}]\) which can be approximated by the slightly reformulated plug-in estimator: \(\hat{\bm{\Sigma}}_{ii}=(1/M)\sum_{m}h_{i}(\bm{z}_{m})h_{i}(\bm{z}^{\prime}_{m}) ^{\mathsf{T}}\). Further, since noise is independent across networks, i.e. \(\epsilon_{i}(\bm{z})\perp\!\!\perp\epsilon_{j}(\bm{z})\) for all \(\bm{z}\in\mathcal{Z}\), the cross-covariance estimators, including the method-of-moments estimator described in section 2.2, do not require any modification. Here we provide several relevant derivations for generalized shape metrics. For a more thorough review, we direct the reader to [30] for the foundational results on generalized shape metrics and [6] for the extension to stochastic neural networks.

### Equivalence of eqs. (2.2) and (2.5); eqs. (2.3) and (2.6)

The squared Procrustes can be reformulated in terms of the covariance and cross-covariance matrices as follows:

\[\rho^{2}(h_{i},h_{j}) =\min_{\bm{Q}\in\mathcal{O}(N)}\mathbb{E}\|h_{i}(\bm{z})-\bm{Q}h_ {j}(\bm{z})\|_{2}^{2}\] \[=\mathbb{E}\left[h_{i}(\bm{z})^{\mathsf{T}}h_{i}(\bm{z})\right]+ \mathbb{E}\left[h_{j}(\bm{z})^{\mathsf{T}}h_{j}(\bm{z})\right]-2\max_{\bm{Q} \in\mathcal{O}(N)}\mathbb{E}\left[h_{i}(\bm{z})^{\mathsf{T}}\bm{Q}h_{j}(\bm{z })\right]\] \[=\mathbb{E}\left[\mathrm{Tr}\left[h_{i}(\bm{z})h_{i}(\bm{z})^{ \mathsf{T}}\right]\right]+\mathbb{E}\left[\mathrm{Tr}\left[h_{j}(\bm{z})h_{j} (\bm{z})^{\mathsf{T}}\right]\right]-2\max_{\bm{Q}\in\mathcal{O}(N)}\mathbb{E }\left[\mathrm{Tr}\left[\bm{Q}h_{j}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}\right]\right]\] \[=\mathrm{Tr}\left[\mathbb{E}\left[h_{i}(\bm{z})h_{i}(\bm{z})^{ \mathsf{T}}\right]\right]+\mathrm{Tr}\left[\mathbb{E}\left[h_{j}(\bm{z})h_{j} (\bm{z})^{\mathsf{T}}\right]\right]-2\max_{\bm{Q}\in\mathcal{O}(N)}\mathrm{ Tr}\left[\bm{Q}\mathbb{E}\left[h_{j}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}\right]\right]\] \[=\mathrm{Tr}\left[\mathbb{\Sigma}_{ii}\right]+\mathrm{Tr}\left[ \mathbb{\Sigma}_{jj}\right]-2\max_{\bm{Q}\in\mathcal{O}(N)}\mathrm{Tr}\left[ \bm{Q}\mathbb{\Sigma}_{ij}\right]\] \[=\mathrm{Tr}\left[\mathbb{\Sigma}_{ii}\right]+\mathrm{Tr}\left[ \mathbb{\Sigma}_{jj}\right]-2\|\mathbb{\Sigma}_{ij}\|_{*}\]

Similarly for the cosine Riemannian distance:

\[\cos\theta(h_{i},h_{j}) =\max_{\bm{Q}\in\mathcal{O}(N)}\left(\frac{\mathbb{E}[h_{i}(\bm{z })^{\mathsf{T}}\bm{Q}h_{j}(\bm{z})]}{\sqrt{\mathbb{E}[h_{i}(\bm{z})^{\mathsf{T }}h_{i}(\bm{z})]\mathbb{E}[h_{j}(\bm{z})^{\mathsf{T}}h_{j}(\bm{z})]}}\right)\] \[=\frac{\max_{\bm{Q}\in\mathcal{O}(N)}\mathbb{E}\left[\mathrm{Tr}[ \bm{Q}h_{j}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}]\right]}{\sqrt{\mathbb{E}\left[ \mathrm{Tr}[h_{i}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}]\right]\mathbb{E}\left[ \mathrm{Tr}[h_{j}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}]\right]}}\] \[=\frac{\max_{\bm{Q}\in\mathcal{O}(N)}\mathrm{Tr}\left[\bm{Q} \mathbb{E}[h_{j}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}]\right]}{\sqrt{\mathrm{Tr }\left[\mathbb{E}[h_{i}(\bm{z})h_{i}(\bm{z})^{\mathsf{T}}]\right]\mathrm{Tr} \left[\mathbb{E}[h_{j}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}]\right]}}\] \[=\frac{\max_{\bm{Q}\in\mathcal{O}(N)}\mathrm{Tr}\left[\bm{Q} \mathbb{\Sigma}_{ij}\right]}{\sqrt{\mathrm{Tr}\left[\mathbb{\Sigma}_{ii} \right]\mathrm{Tr}\left[\mathbb{\Sigma}_{jj}\right]}}=\frac{\|\bm{\Sigma}_{ij} \|_{*}}{\sqrt{\mathrm{Tr}\left[\mathbb{\Sigma}_{ii}\right]\mathrm{Tr}\left[ \mathbb{\Sigma}_{jj}\right]}}\]

### Reformulations of the Plug-in Estimator of Procrustes distance

Let \(\bm{z}_{1},\ldots,\bm{z}_{M}\) denote a set of independently and identically distributed samples in the network input space. Then, stack the responses of network \(i\) row-wise into a matrix \(\bm{X}_{i}\in\mathbb{R}^{M\times N}\). Given this set up, a common definition of Procrustes distance is [8]:

\[\min_{\bm{Q}\in\mathcal{O}(N)}\frac{1}{\sqrt{M}}\|\bm{X}_{i}-\bm{X}_{j}\bm{Q} \|_{F}\] (A.5)

Here, we have included a multiplying factor of \(1/\sqrt{M}\) for reasons that will become clear shortly. Aside from this factor, the quantity above is how Williams et al. [30] define the Procrustes distance. Below, we show that the square of this quantity is indeed the plug-in estimator we defined in eq. (2.8) in terms of the empirical covariance matrices:

\[\min_{\bm{Q}\in\mathcal{O}(N)}\frac{1}{M}\|\bm{X}_{i}-\bm{X}_{j}\bm{Q}\|_{F}^ {2} =\min_{\bm{Q}\in\mathcal{O}(N)}\frac{1}{M}\left(\mathrm{Tr}[\bm{X} _{i}^{\mathsf{T}}\bm{X}_{i}]+\mathrm{Tr}[\bm{X}_{j}^{\mathsf{T}}\bm{X}_{j}]-2 \,\mathrm{Tr}[\bm{X}_{i}\bm{X}_{j}^{\mathsf{T}}\bm{Q}]\right)\] \[=\mathrm{Tr}\left[\tfrac{1}{M}\bm{X}_{i}^{\mathsf{T}}\bm{X}_{i} \right]+\mathrm{Tr}\left[\tfrac{1}{M}\bm{X}_{j}^{\mathsf{T}}\bm{X}_{j}\right] -2\max_{\bm{Q}\in\mathcal{O}(N)}\mathrm{Tr}\left[\tfrac{1}{M}\bm{X}_{i}\bm{X} _{j}^{\mathsf{T}}\bm{Q}\right]\] \[=\mathrm{Tr}\left[\hat{\bm{\Sigma}}_{ii}\right]+\mathrm{Tr}\left[ \hat{\bm{\Sigma}}_{jj}\right]-2\max_{\bm{Q}\in\mathcal{O}(N)}\mathrm{Tr} \left[\hat{\bm{\Sigma}}_{ij}\bm{Q}\right]\] \[=\mathrm{Tr}\left[\hat{\bm{\Sigma}}_{ii}\right]+\mathrm{Tr}\left[ \hat{\bm{\Sigma}}_{jj}\right]-2\|\hat{\bm{\Sigma}}_{ij}\|_{*}\] \[=\hat{\rho}^{2}(h_{i},h_{j})\]

## Appendix B Appendix: Plug-in Estimator Theory

Here we provide a number of derivations related to the behavior of the plug-in estimator for generalized shape metrics. These results primarily rely on classic concentration inequalities and results fromrandom matrix theory. For readers interested in further background, we provide pointers to [27] and [26] for the concentration inequalities and [20] for the random matrix theory.

### Summary of Results: Nonasymptotic bounds on the performance of plug-in estimation

First, it is straightforward to estimate \(\mathrm{Tr}[\bm{\Sigma}_{ii}]\) and \(\mathrm{Tr}[\bm{\Sigma}_{jj}]\). Their plug-in estimators are unbiased under our assumptions in eq. (2.1), and they rapidly converge to the correct answer. This is shown in the following lemma, whose proof relies only on classical concentration inequalities.

**Lemma B.1** (App. B.3).: _Under the assumptions in eq. (2.1), with probability at least \(1-\delta\):_

\[\left|\mathrm{Tr}[\bm{\Sigma}_{ii}]-\mathrm{Tr}[\hat{\bm{\Sigma}}_{ii}]\right| \leq BN^{1/2}M^{-1/2}\sqrt{2\log(2/\delta)}\] (B.1)

In contrast, the plug-in estimator for \(\|\bm{\Sigma}_{ij}\|_{*}\) is biased upwards (see appendix B.2) and turns out to converge more slowly. Using the Matrix Bernstein inequality [see 26], we can show:

**Lemma B.2** (App. B.4).: _Under the assumptions in eq. (2.1), for any \(M\) and \(N\):_

\[\mathbb{E}\Big{\|}\|\hat{\bm{\Sigma}}_{ij}\|_{*}-\|\bm{\Sigma}_{ij}\|_{*} \Big{|}<\frac{2B^{2}N^{2}\log(2N)}{3M}+\frac{2B^{2}N^{2}\sqrt{\log(2N)}}{M^{1/ 2}}\] (B.2)

This only upper bounds the expected error. However, the fluctuations around this expectation turn out to be small (see App. B.5), and so we are able to combine lemmas B.1 and B.2 into the following:

**Theorem B.1** (App. B.5).: _Under the assumptions in eq. (2.1), with probability at least \(1-\delta\)_

\[\frac{|\hat{\rho}^{2}-\rho^{2}|}{N}\leq\frac{2B^{2}N\log(2N)}{3M}+\frac{2B^{2} N\sqrt{\log(2N)}}{M^{1/2}}+\bigg{(}\frac{B^{2}}{M^{1/2}}+\frac{2B}{N^{1/2}M^{1/2} }\bigg{)}\sqrt{2\log\bigg{(}\frac{6}{\delta}\bigg{)}}\] (B.3)

Theorem B.1 states a non-asymptotic upper bound on the plug-in estimator's error that holds with high probability. We have expressed this bound on the squared size-and-shape Procrustes distance normalized by \(1/N\), since the raw error, \(|\hat{\rho}-\rho|\), will tend to increase linearly with \(N\) for an uninteresting reason--namely, since the the Procrustes shape distance is comprised of terms like \(\mathrm{Tr}[\bm{\Sigma}_{ii}]\) and \(\mathrm{Tr}[\bm{\Sigma}_{jj}]\). The choice of normalization in theorem B.1 also makes the result more comparable to the cosine shape similiarity (eq. 2.6), which is normalized by a factor, \(\sqrt{\mathrm{Tr}[\bm{\Sigma}_{ii}]\,\mathrm{Tr}[\bm{\Sigma}_{jj}]}\), of order \(N\).

We can gain intuition for theorem B.1 by ignoring logarithmic factors and noticing that the second term dominates. Then, roughly speaking, theorem B.1 says that we can guarantee the plug-in error decreases as a function of \(NM^{-1/2}\). Thus, for any fixed \(N\), we need to increase \(M\) by a factor of 4 to decrease estimation error by a factor of 2. Further, when comparing higher-dimensional neural representations (i.e. higher \(N\)) we need to sample more landmarks--if \(N\) increases by a factor of 2, then \(M\) must be increased by a factor of 4 to compensate.

### Summary of Results: Failure modes of plug-in estimation and a lower bound on performance

Theorem B.1 provides a high probability upper bound on the estimation error. A natural question is whether this upper bound is tight. To investigate, we seek an example where the plug-in estimator performs badly. We intuited that when two neural representations are very far apart in shape space, the plug-in estimator of shape distance should have a large downward bias. This can be understood in two ways. First, from the definitions of \(\rho\) and \(\theta\) in eqs. (2.2) and (2.3), we see that both expressions contain a minimization over \(\bm{Q}\in\mathcal{O}(N)\). For large \(N\) and small \(M\), this high-dimensional orthogonal matrix can be "overfit" to the \(M\) observations resulting in an underestimate of distance. Second, from the alternative formulations in eqs. (2.5) and (2.6), we see that the shape distance is large if the true cross-covariance is "small" as quantified by the nuclear norm. In the extreme case where the singular values of \(\bm{\Sigma}_{ij}\) are all zero, the empirical cross-covariance matrix \((1/M)\sum_{m}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\) will overestimate the nuclear norm, and therefore underestimate the shape distance. This is more severe when \(M\) is small, since there are fewer terms in the sum to "average out" spurious correlations, which are particularly problematic in high dimensions (i.e. when \(N\) is large).

This intuition led us to construct an example where plug-in estimation error approaches the upper bound in theorem B.1. This is summarized in the following result.

**Theorem B.2** (Lower Bound, App. B.6).: _Under the assumptions in eq. (2.1), there exist neural networks and a distribution over inputs such that in the limit that \(N\to\infty\) and \(M\gg N\):_

\[\frac{|\hat{\rho}^{2}-\rho^{2}|}{N}=\frac{16B^{2}}{3\pi}N^{1/2}M^{-1/2}\] (B.4)

Thus, while future work may seek to improve the upper bound in theorem B.1, we cannot hope to improve beyond the lower bound formulated in theorem B.2. If we ignore the logarithmic factors to gain intuition, we observe there is (roughly) a gap of \(N^{1/2}\) between the upper and lower bounds. Thus, it is possible that our analysis in appendix B.1 may be conservative in terms of the ambient dimension--specifically, the lower bound only suggests that \(M\) only needs to be increased two-fold to compensate for a two-fold increase in \(N\). However, in terms of the number of sampled inputs, the rate cannot be improved beyond \(M^{-1/2}\).

### Proof of lemma B.1

Here we show that the plug-in estimate of the total variance \(\operatorname{Tr}[\hat{\bm{\Sigma}}_{ii}]\) converges to the true variance \(\operatorname{Tr}[\bm{\Sigma}_{ii}]\) exponentially fast as \(M\) increases. We begin with some algebraic manipulations:

\[\left|\operatorname{Tr}[\bm{\Sigma}_{ii}-\hat{\bm{\Sigma}}_{ii}]\right| =\left|\operatorname{Tr}\left[\mathbb{E}_{z\sim P}[h_{i}(\bm{z} _{m})h_{i}(\bm{z}_{m})^{\mathsf{T}}]-\tfrac{1}{M}\sum\limits_{m=1}^{M}h_{i}( \bm{z}_{m})h_{i}(\bm{z}_{m})^{\mathsf{T}}\right]\right|\] \[=\left|\mathbb{E}_{z\sim P}\left[\operatorname{Tr}[h_{i}(\bm{z} _{m})h_{i}(\bm{z}_{m})^{\mathsf{T}}]\right]-\tfrac{1}{M}\sum\limits_{m=1}^{M} \operatorname{Tr}[h_{i}(\bm{z}_{m})h_{i}(\bm{z}_{m})^{\mathsf{T}}]\right|\] \[=\left|\mathbb{E}_{z\sim P}\left[\operatorname{Tr}[h_{i}(\bm{z} _{m})^{\mathsf{T}}h_{i}(\bm{z}_{m})]\right]-\tfrac{1}{M}\sum\limits_{m=1}^{M} \operatorname{Tr}[h_{i}(\bm{z}_{m})^{\mathsf{T}}h_{i}(\bm{z}_{m})]\right|\] \[=\left|\mathbb{E}_{z\sim P}\left[h_{i}(\bm{z}_{m})^{\mathsf{T}}h_ {i}(\bm{z}_{m})\right]-\tfrac{1}{M}\sum\limits_{m=1}^{M}h_{i}(\bm{z}_{m})^{ \mathsf{T}}h_{i}(\bm{z}_{m})\right|\]

where we have used the property \(\operatorname{Tr}[\mathbf{x}\mathbf{x}^{\mathsf{T}}]=\mathbf{x}^{\mathsf{T}} \mathbf{x}\) for any column vector \(\mathbf{x}\) in the last two lines.

The main assumption we are going to make is that the neural responses are constrained to an \(\ell_{2}\) ball of radius \(B\sqrt{N}\) or equivalently \(h_{i}(\bm{z}_{m})^{\mathsf{T}}h_{i}(\bm{z}_{m})\leq B^{2}N\) for all stimuli in the support of \(P\). Note that this is a reasonable assumption in both biological (energy constraints) and artificial neural networks (weight decay common).

**Lemma B.3** (Bounded Random Variables are Sub-Gaussian, Wainwright [27] Example 2.4).: _We say that a random variable \(X\) with mean \(\mu\) is sub-Gaussian with parameter \(\sigma\) if:_

\[\mathbb{E}\left[e^{\lambda(X-\mu)}\right]\leq e^{\sigma^{2}\lambda^{2}/2} \quad\text{for all }\lambda\in\mathbb{R}\]

_Intuitively, this means that the tails of \(X\) fall off faster than a Gaussian. Furthermore, if \(X\) is mean zero and supported on the interval \([a,b]\), the \(X\) is sub-Gaussian with parameter \(\sigma=(b-a)/2\)._

Thus our assumption implies that each term with \(\frac{1}{M}h_{i}(\bm{z}_{m})^{\mathsf{T}}h_{i}(\bm{z}_{m})\) is sub-Gaussian with parameter \(\sigma=B\sqrt{N}/M\). We can then immediately apply the Hoeffding bound [27, Proposition 2.5] to obtain:

\[\mathbb{P}\left[\left|\operatorname{Tr}[\bm{\Sigma}_{ii}-\hat{\bm{\Sigma}}_{ii }]\right|\geq t\right]\leq 2\exp\left[-\frac{Mt^{2}}{2B^{2}N}\right]\] (B.5)

Analogously for term (B) we obtain:

\[\mathbb{P}\left[\left|\operatorname{Tr}[\bm{\Sigma}_{jj}-\hat{\bm{\Sigma}}_{ jj}]\right|\geq t\right]\leq 2\exp\left[-\frac{Mt^{2}}{2B^{2}N}\right]\] (B.6)

### Proof of lemma B.2

Our main tool is the matrix Bernstein inequality, given as theorem 6.1.1 in Tropp [26]. We paraphrase a version of the theorem here to keep our narrative self-contained.

**Theorem B.3** (Matrix Bernstein).: _Consider a finite sequence \(\{\bm{S}_{1},\ldots,\bm{S}_{M}\}\) of independent, random \(N\times N\) matrices. Assume that:_

\[\mathbb{E}\big{[}\bm{S}_{m}\big{]}=\bm{0}\quad\text{and}\quad\|\bm{S}_{m}\|_{ \infty}\leq L\quad\text{for each index }m\] (B.7)

_where \(\|\bm{S}_{m}\|_{\infty}=\sup\{\|\bm{S}_{m}\bm{v}\|_{2}\,:\,\|\bm{v}\|_{2}\leq 1\}\) is the matrix operator norm._

_Further, define the variance of the sum \(\sum_{m}\bm{S}_{m}\) as:_

\[V=\big{\|}{\sum_{m}}\mathbb{E}\bm{S}_{m}^{\mathsf{T}}\bm{S}_{m}\big{\|}_{ \infty}=\big{\|}{\sum_{m}}\mathbb{E}\bm{S}_{m}\bm{S}_{m}^{\mathsf{T}}\big{\|}_ {\infty}\] (B.8)

_Then:_

\[\mathbb{E}\big{[}\big{\|}\sum_{m}\bm{S}_{m}\big{\|}_{\infty}\,\big{]}\leq\sqrt {2V\log(2N)}+\frac{L}{3}\log(2N)\] (B.9)

We now turn to the proof of theorem B.1. Define:

\[\bm{S}_{m}=\frac{1}{M}\left(h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}- \bm{\Sigma}_{ij}\right)\] (B.10)

for the sequence of network inputs \(\{\bm{z}_{1},\ldots,\bm{z}_{M}\}\). Notice that:

\[\mathbb{E}\big{[}\bm{S}_{m}\big{]}=\frac{1}{M}\left(\mathbb{E}\left[h_{i}(\bm{ z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\right]-\bm{\Sigma}_{ij}\right)=\frac{1}{M} \left(\bm{\Sigma}_{ij}-\bm{\Sigma}_{ij}\right)=\bm{0}\] (B.11)

Next, due to triangle inequality, we have:

\[\left\|\bm{S}_{m}\right\|_{\infty}=\frac{1}{M}\left\|h_{i}(\bm{z}_{m})h_{j}( \bm{z}_{m})^{\mathsf{T}}-\bm{\Sigma}_{ij}\right\|_{\infty}\leq\frac{1}{M} \underbrace{\big{\|}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\big{\|}_ {\infty}}_{(1)}+\frac{1}{M}\underbrace{\big{\|}\bm{\Sigma}_{ij}\big{\|}_{ \infty}}_{(2)}\] (B.12)

Terms (1) and (2) are each upper bounded by \(B^{2}N\), since for term (1):

\[\big{\|}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\big{\|}_ {\infty} \leq\big{\|}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\bm{v }\big{\|}_{2}\] (for any vector \[\|\bm{v}\|_{2}\leq 1\] ) (B.13) \[=h_{j}(\bm{z}_{m})^{\mathsf{T}}\bm{v}\left\|h_{i}(\bm{z}_{m}) \right\|_{2}\] (B.14) \[\leq\left\|h_{j}(\bm{z}_{m})\right\|_{2}\left\|\bm{v}\right\|_{2} \left\|h_{i}(\bm{z}_{m})\right\|_{2}\] (Cauchy-Schwarz inequality) (B.15) \[\leq B\sqrt{N}\cdot 1\cdot B\sqrt{N}=B^{2}N\] (From assumptions in eq. 2.1 ) (B.16)

And for term (2):

\[\left\|\bm{\Sigma}_{ij}\right\|_{\infty} =\big{\|}\mathbb{E}\,h_{i}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}\big{\|}_ {\infty}\] (B.17) \[\leq\left\|\mathbb{E}\,h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{ T}}\bm{v}\right\|_{2}\] (for any vector \[\|\bm{v}\|_{2}\leq 1\] ) (B.18) \[\leq\mathbb{E}\,\big{\|}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{ \mathsf{T}}\bm{v}\big{\|}_{2}\] (Jensen's inequality) (B.19) \[\leq B^{2}N\] (Repeat the upper bound on term 1) (B.20)

To summarize, we have:

\[\left\|\bm{S}_{m}\right\|_{\infty}\leq\frac{1}{M}\left\|h_{i}(\bm{z}_{m})h_{j} (\bm{z}_{m})^{\mathsf{T}}\right\|_{\infty}+\frac{1}{M}\left\|\bm{\Sigma}_{ij} \right\|_{\infty}\leq\frac{2B^{2}N}{M}\] (B.21)

That is, we have shown that the assumptions of eq. (B.7) are satisfied with \(L=2B^{2}N/M\).

Our next task is to determine an expression for the variance \(V\) defined in eq. (B.8). First, we have:

\[\mathbb{E}\,\bm{S}_{m}^{\mathsf{T}}\bm{S}_{m} =\frac{1}{M^{2}}\mathbb{E}[h_{j}(\bm{z}_{m})h_{i}(\bm{z}_{m})^{ \mathsf{T}}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}+\bm{\Sigma}_{ij}^{ \mathsf{T}}\bm{\Sigma}_{ij}-\bm{\Sigma}_{ij}^{\mathsf{T}}h_{j}(\bm{z}_{m})h_{i}( \bm{z}_{m})^{\mathsf{T}}-h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\bm{ \Sigma}_{ij}]\] \[=\frac{1}{M^{2}}\mathbb{E}[h_{j}(\bm{z}_{m})h_{i}(\bm{z}_{m})^{ \mathsf{T}}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}]+\bm{\Sigma}_{ij}^{ \mathsf{T}}\bm{\Sigma}_{ij}-\bm{\Sigma}_{ij}^{\mathsf{T}}\mathbb{E}[h_{j}(\bm{z} _{m})h_{i}(\bm{z}_{m})^{\mathsf{T}}]-\mathbb{E}[h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m })^{\mathsf{T}}]\bm{\Sigma}_{ij}\] \[=\frac{1}{M^{2}}\mathbb{E}[h_{j}(\bm{z}_{m})h_{i}(\bm{z}_{m})^{ \mathsf{T}}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}]+\bm{\Sigma}_{ij}^{ \mathsf{T}}\bm{\Sigma}_{ij}-\bm{\Sigma}_{ij}^{\mathsf{T}}\bm{\Sigma}_{ij}-\bm{ \Sigma}_{ij}^{\mathsf{T}}\bm{\Sigma}_{ij}\] \[=\frac{1}{M^{2}}\mathbb{E}[h_{j}(\bm{z}_{m})h_{i}(\bm{z}_{m})^{ \mathsf{T}}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}]-\bm{\Sigma}_{ij}^{ \mathsf{T}}\bm{\Sigma}_{ij}\]

[MISSING_PAGE_FAIL:14]

**Lemma B.4** (Bounded Differences Inequality, Wainwright [27] Corollary 2.21).: _Consider a function \(f:\mathbb{R}^{n}\to\mathbb{R}\). The function is said to have the bounded difference property for the \(k\)th coordinate if there exists an \(L_{k}\) for which the following holds:_

\[\max_{X_{1:n}\in\mathbb{R}^{n},X_{k}^{\prime}\in\mathbb{R}}\left|f(X_{1:n})-f(X _{1:k-1},X_{k}^{\prime},X_{k+1:n})\right|\leq L_{k}\]

_Suppose \(f\) satisfies this property with \(L_{1},\ldots,L_{n}\) for each coordinate respectively. Then the following inequality holds:_

\[\mathbb{P}\left[\left|f(X_{1:n})-\mathbb{E}[f(X_{1:n})]\right|\geq t\right] \leq\exp\left[-\frac{2t^{2}}{\sum_{i=1}^{n}L_{i}^{2}}\right]\] (B.23)

We start by applying the reverse triangle inequality:

\[\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ij}\|_{*}\right|\leq\|\bm {\Sigma}_{ij}-\hat{\bm{\Sigma}}_{ij}\|_{*}=\left\|\bm{\Sigma}_{ij}-\frac{1}{M }\underset{m=1}{\overset{M}{\sum}}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{ \mathsf{T}}\right\|_{*}\]

We can bound how much this changes if we change one coordinate of the function, i.e. if \(h_{i}(\bm{z}_{1})^{\mathsf{T}}h_{j}(\bm{z}_{1})\) is replaced by \(h_{i}(\tilde{\bm{z}}_{1})^{\mathsf{T}}h_{j}(\tilde{\bm{z}}_{1})\). The difference is then bounded by:

\[\left\|\bm{\Sigma}_{ij}-\frac{1}{M}\underset{m=1}{\overset{M}{ \sum}}h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\right\|_{*}-\left\|\bm{ \Sigma}_{ij}-\left(\frac{1}{M}\underset{m=1}{\overset{M}{\sum}}h_{i}(\bm{z}_ {m})h_{j}(\bm{z}_{m})^{\mathsf{T}}-\frac{1}{M}h_{i}(\bm{z}_{1})h_{j}(\bm{z}_{1 })^{\mathsf{T}}+\frac{1}{M}h_{i}(\tilde{\bm{z}}_{1})h_{j}(\tilde{\bm{z}}_{1})^ {\mathsf{T}}\right)\right\|_{*}\] \[\leq\frac{1}{M}\left(\left\|h_{i}(\bm{z}_{1})h_{j}(\bm{z}_{1})^{ \mathsf{T}}\right\|_{*}+\left\|h_{i}(\tilde{\bm{z}}_{1})h_{j}(\tilde{\bm{z}}_{ 1})^{\mathsf{T}}\right\|_{*}\right)=\frac{1}{M}\left(\left|h_{i}(\bm{z}_{1})^{ \mathsf{T}}h_{j}(\bm{z}_{1})\right|+\left|h_{i}(\tilde{\bm{z}}_{1})^{\mathsf{ T}}h_{j}(\tilde{\bm{z}}_{1})\right|\right)\]

Finally, we can apply Cauchy-Schwartz and our assumption about the neural activations being bounded to obtain:

\[\frac{1}{M}\left(\left|h_{i}(\bm{z}_{1})^{\mathsf{T}}h_{j}(\bm{z }_{1})\right|+\left|h_{i}(\tilde{\bm{z}}_{1})^{\mathsf{T}}h_{j}(\tilde{\bm{z}}_ {1})\right|\right) \leq\frac{1}{M}\left(\|h_{i}(\bm{z}_{1})\|_{2}\|h_{j}(\bm{z}_{1} )\|_{2}+\|h_{i}(\tilde{\bm{z}}_{1})\|_{2}\|h_{j}(\tilde{\bm{z}}_{1})\|_{2}\right)\] \[\leq\frac{2B^{2}N}{M}\]

Thus we have \(\sum_{i=1}^{M}L_{i}^{2}=\sum_{i=1}^{M}4B^{4}N^{2}/M^{2}=4B^{4}N^{2}/M\), and we can apply the bounded differences inequality to obtain for all \(t\geq 0\):

\[\mathbb{P}\bigg{[}\left|\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ij }\|_{*}\right|-\mathbb{E}\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ ij}\|_{*}\right|\bigg{|}\geq t\right]\leq 2\exp\left[-\frac{Mt^{2}}{2B^{4}N^{2}}\right]\] (B.24)

For the deviation from the expectation to be in the range \([-t,t]\) with probability \(1-\delta\) we require:

\[2\exp\left[-\frac{Mt^{2}}{2B^{4}N^{2}}\right]\leq\delta\]

Solving for \(t\) gives \(t\geq B^{2}NM^{-1/2}\sqrt{2\log\left(2/\delta\right)}\), and thus with probability \(1-\delta\) the following holds:

\[\left|\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ij}\|_{*}\right|- \mathbb{E}\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ij}\|_{*}\right| \right|\leq B^{2}NM^{-1/2}\sqrt{2\log(2/\delta)}\]

To proceed we break this we use a basic identity of the absolute value: if \(|a-b|<c\) then \(a-b<c\) and also \(b-a<c\). Thus, with probability at least \(1-\delta\), we have:

\[\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ij}\|_{*} \leq\mathbb{E}\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_ {ij}\|_{*}\right|+\frac{B^{2}N}{M^{1/2}}\sqrt{2\log(2/\delta)}\] \[\leq\frac{2B^{2}N^{2}}{M^{1/2}}\sqrt{\log(2N)}+\frac{2B^{2}N^{2} }{3M}\log(2N)+\frac{B^{2}N}{M^{1/2}}\sqrt{2\log(2/\delta)}\]And we also have with probability at least \(1-\delta\), we have:

\[\|\hat{\bm{\Sigma}}_{ij}\|_{*}-\|\bm{\Sigma}_{ij}\|_{*} \leq\mathbb{E}\left\|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ ij}\|_{*}\right\|+\frac{B^{2}N}{M^{1/2}}\sqrt{2\log(2/\delta)}\] \[\leq\frac{2B^{2}N^{2}}{M^{1/2}}\sqrt{\log(2N)}+\frac{2B^{2}N^{2}} {3M}\log(2N)+\frac{B^{2}N}{M^{1/2}}\sqrt{2\log(2/\delta)}\]

In the final inequalities above, we have simply plugged in our expectation bound from lemma B.2. The relations above imply that the following holds with probability \(1-\delta\):

\[\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ij}\|_{*}\right|\leq\frac{ 2N^{2}\log(2B^{2}N)}{3M}+\frac{2B^{2}N^{2}\sqrt{\log(2N)}}{M^{1/2}}+\frac{B^{ 2}N}{M^{1/2}}\sqrt{2\log\left(\frac{2}{\delta}\right)}\] (B.25)

To complete the proof we need to combine the above tail bound with lemma B.1. By the triangle inequality we have

\[|\hat{\rho}^{2}-\rho^{2}| =\left|\mathrm{Tr}[\hat{\bm{\Sigma}}_{ii}]+\mathrm{Tr}[\hat{\bm{ \Sigma}}_{jj}]-2\|\hat{\bm{\Sigma}}_{ij}\|_{*}-\mathrm{Tr}[\hat{\bm{\Sigma}}_{ ii}]-\mathrm{Tr}[\hat{\bm{\Sigma}}_{jj}]+2\|\hat{\bm{\Sigma}}_{ij}\|_{*}\right|\] \[=\left|\mathrm{Tr}[\hat{\bm{\Sigma}}_{ii}]-\mathrm{Tr}[\bm{\Sigma }_{ii}]+\mathrm{Tr}[\hat{\bm{\Sigma}}_{jj}]-\mathrm{Tr}[\bm{\Sigma}_{jj}]+2\| \bm{\Sigma}_{ij}\|_{*}-2\|\hat{\bm{\Sigma}}_{ij}\|_{*}\right|\] \[\leq\left|\mathrm{Tr}[\hat{\bm{\Sigma}}_{ii}]-\mathrm{Tr}[\bm{ \Sigma}_{ii}]\right|+\left|\mathrm{Tr}[\hat{\bm{\Sigma}}_{jj}]-\mathrm{Tr}[ \bm{\Sigma}_{jj}]\right|+2\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ ij}\|_{*}\right|\]

Setting \(\delta^{\prime}=\delta/3\) in our results for these three terms yields that the following three inequalities independently hold with probability \(\delta/3\):

\[\left|\mathrm{Tr}[\bm{\Sigma}_{ii}]-\mathrm{Tr}[\hat{\bm{\Sigma} }_{ii}]\right| \geq BN^{1/2}M^{-1/2}\sqrt{2\log(6/\delta)}\] \[\left|\mathrm{Tr}[\bm{\Sigma}_{jj}]-\mathrm{Tr}[\hat{\bm{\Sigma} }_{jj}]\right| \geq BN^{1/2}M^{-1/2}\sqrt{2\log(6/\delta)}\] \[\left|\|\bm{\Sigma}_{ij}\|_{*}-\|\hat{\bm{\Sigma}}_{ij}\|_{*} \right| \geq\frac{2N^{2}\log(2B^{2}N)}{3M}+\frac{2B^{2}N^{2}\sqrt{\log(2N )}}{M^{1/2}}+\frac{B^{2}N}{M^{1/2}}\sqrt{2\log\left(\frac{6}{\delta}\right)}\]

By applying the union bound, we obtain that all three inequalities hold simultaneously with probability \(\leq\delta/3+\delta/3+\delta/3=\delta\). The three reverse inequalities then hold simultaneously with probability greater than or equal to \(1-\delta\). Thus with probability at least \(1-\delta\), the following holds:

\[|\hat{\rho}^{2}-\rho^{2}|\leq\frac{2B^{2}N^{2}\log(2N)}{3M}+\frac{2B^{2}N^{2} \sqrt{\log(2N)}}{M^{1/2}}+\left(\frac{NB^{2}}{M^{1/2}}+\frac{2N^{1/2}B}{M^{1 /2}}\right)\!\!\sqrt{2\log\left(\frac{6}{\delta}\right)}\]

as claimed in theorem B.1.

### Proof of theorem B.2 (Lower Bound on Plug-In Estimator Error)

We derive a lower bound by constructing an explicit example where the plug-in estimator performs badly. Specifically, we consider a scenario where two networks have entirely decorrelated, high-variance representations. To do this, we use _Rademacher random variables_--a random variable \(R\) is called a Rademacher variable if it behaves as follows:

\[R=\begin{cases}+1&\text{with probability }1/2\\ -1&\text{with probability }1/2\end{cases}\] (B.26)

Now, suppose we sample \(M\) network inputs, \(\bm{z}_{1},\dots,\bm{z}_{M}\sim P\), independently. Further, let \(B>0\) be the constant appearing in eq. (2.1). For \(m\in\{1,\dots,M\}\) define

\[X_{m}=\frac{1}{B}h_{i}(\bm{z}_{m})\quad\text{and}\quad Y_{m}=\frac{1}{B}h_{j} (\bm{z}_{m})\] (B.27)Note that \(X_{m}\) and \(Y_{m}\) are \(N\)-dimensional random vectors. Due to eq. (2.1), we have \(\|h_{i}(\bm{z})\|_{2}\leq B\sqrt{N}\) and \(\|h_{j}(\bm{z})\|_{2}\leq B\sqrt{N}\) almost surely. Thus, \(\|X_{m}\|\leq\sqrt{N}\) and \(\|Y_{m}\|\leq\sqrt{N}\) almost surely.

Define \(X=(1/B)h_{i}(\bm{z})\) and \(Y=(1/B)h_{j}(\bm{z})\) for randomly sampled \(\bm{z}\sim P\). The case we will consider is that \(X\) and \(Y\) are each composed of \(N\) independent Rademacher variables. One trivial way to construct this is to suppose each \(\bm{z}\sim P\) is a random vector with \(2N\) elements, all of which are independent Rademacher variables scaled by a factor \(B>0\). Then, let \(h_{i}:\mathbb{R}^{2N}\mapsto\mathbb{R}^{N}\) be the function which extracts the first \(N\) elements of \(\bm{z}\) and let \(h_{j}:\mathbb{R}^{2N}\mapsto\mathbb{R}^{N}\) be the function which extracts the final \(N\) elements.

Thus, we have constructed a setting where \(X_{1},\ldots,X_{M},Y_{1},\ldots,Y_{M}\) are all composed of independent Rademacher variables. In this setting, the squared Procrustes distance is given by:

\[\rho^{2} =\operatorname{Tr}[\bm{\Sigma}_{ii}]+\operatorname{Tr}[\bm{ \Sigma}_{jj}]-2\|\bm{\Sigma}_{ij}\|_{*}\] (B.28) \[=\operatorname{Tr}[\mathbb{E}[h_{i}(\bm{z})h_{i}(\bm{z})^{ \mathsf{T}}]]+\operatorname{Tr}[\mathbb{E}[h_{j}(\bm{z})h_{j}(\bm{z})^{ \mathsf{T}}]]-2\|\mathbb{E}[h_{i}(\bm{z})h_{j}(\bm{z})^{\mathsf{T}}]\|_{*}\] (B.29) \[=B^{2}\cdot\left(\operatorname{Tr}[\mathbb{E}[XX^{\mathsf{T}}]]+ \operatorname{Tr}[\mathbb{E}[YY^{\mathsf{T}}]]-2\|\mathbb{E}[XY^{\mathsf{T}} ]\|_{*}\right)\] (B.30) \[=B^{2}\cdot\left(\mathbb{E}[X^{\mathsf{T}}X]+\mathbb{E}[Y^{ \mathsf{T}}Y]-2\|\mathbb{E}[X]\mathbb{E}[Y^{\mathsf{T}}]\|_{*}\right)\] (B.31) \[=B^{2}\cdot(N+N-0)\] (B.32) \[=2B^{2}N\] (B.33)

where we have used the fact that \(X\) and \(Y\) are independent, mean zero, random vectors to conclude that the cross covariance is an \(N\times N\) matrix filled with zeros. Furthermore, note that \(X_{m}^{\mathsf{T}}X_{m}=N\) and \(Y_{m}^{\mathsf{T}}Y_{m}=N\) almost surely for all \(m\in 1,\ldots,M\) since they are comprised of \(N\) Rademacher variables. Thus, the plug-in estimate of the squared Procrustes distance takes the form:

\[\hat{\rho}^{2} =B^{2}\cdot\left(\operatorname{Tr}[\tfrac{1}{M}\sum_{m}X_{m}X_{m} ^{\mathsf{T}}]+\operatorname{Tr}[\tfrac{1}{M}\sum_{m}Y_{m}Y_{m}^{\mathsf{T}}] -2\|\tfrac{1}{M}\sum_{m}X_{m}Y_{m}^{\mathsf{T}}\|_{*}\right)\] (B.34) \[=B^{2}\cdot\left(\tfrac{1}{M}\sum_{m}X_{m}^{\mathsf{T}}X_{m}+ \tfrac{1}{M}\sum_{m}Y_{m}^{\mathsf{T}}Y_{m}-2\|\tfrac{1}{M}\sum_{m}X_{m}Y_{m}^ {\mathsf{T}}\|_{*}\right)\] (B.35) \[=B^{2}\cdot\left(N+N-2\|\tfrac{1}{M}\sum_{m}X_{m}Y_{m}^{\mathsf{ T}}\|_{*}\right)\] (B.36) \[=2B^{2}N-2B^{2}\|\tfrac{1}{M}\sum_{m}X_{m}Y_{m}^{\mathsf{T}}\|_{*}\] (B.37)

Putting these two results together, we conclude that the absolute error of the plug-in estimator is:

\[|\rho^{2}-\hat{\rho}^{2}|=2B^{2}\|\tfrac{1}{M}\sum_{m}X_{m}Y_{m}^{\mathsf{T}} \|_{*}\] (B.38)

Now, the product of two indepedent Rademacher variables is also a standard Rademacher variable. Thus, each element inside the matrix \((1/M)\sum_{m}X_{m}Y_{m}^{\mathsf{T}}\), is the empirical average of \(M\) independent Rademacher variables. These matrix elements are asymptotically independent in the limit that \(M\to\infty\). Further, the central limit theorem applies in this limit, and thus the distribution of each matrix element approaches a Gaussian distribution \(\mathcal{N}(0,1/M)\).

Such random matrices are well-studied under the name of Ginibre ensembles. In the limit that \(N\to\infty\) and the variance of each matrix element is taken to be \(\sigma^{2}/N\), the density of the singular values takes the following form [see e.g. 20, sec. 3.1.3]:

\[\rho(s)=\frac{\sqrt{4\sigma^{2}-s^{2}}}{\pi\sigma^{2}}\quad s\in(0,2\sigma)\] (B.39)

This is called the quarter circle law since if we look at the density of \(s\) it forms a quarter circle. The nuclear norm of the matrix is \(N\) times the expected value of \(s\) with with respect to the density \(\rho(s)\). Integrating this density, we obtain:

\[\lim_{\begin{subarray}{c}N\to\infty\\ M\gg\mathcal{N}\end{subarray}}\left\|\tfrac{1}{M}\sum_{m}X_{m}Y_{m}^{\mathsf{T }}\right\|_{*} =\frac{N}{\pi\sigma^{2}}\int_{0}^{2\sigma}s\sqrt{4\sigma^{2}-s^{2}}\,ds\] (B.40) \[=\frac{N}{4\pi\sigma^{2}}\left[-\frac{1}{3}(4\sigma^{2}-s^{2})^{3/ 2}\right]_{0}^{2\sigma}\] (B.41) \[=\frac{N}{\pi\sigma^{2}}\left[\frac{1}{3}(4\sigma^{2})^{3/2} \right]=\frac{N}{\pi\sigma^{2}}\left[\frac{8}{3}\sigma^{3}\right]\] (B.42) \[=\frac{8\sigma}{3\pi}N=\frac{8}{3\pi}N^{3/2}M^{-1/2}\] (B.43)Where in the last line we have substituted \(\sigma=\sqrt{N/M}\), which comes from equating \(\sigma^{2}/N\) (the variance in of each matrix element in eq. B.39) with \(1/M\) (the variance given by the average of \(M\) Rademacher variables under the central limit theorem). Note that the analysis above holds asymptotically as \(M,N\rightarrow\infty\) and we keep \(M\gg N\) so that the central limit theorem continues to hold.

Plugging eq. (B.43) into eq. (B.38) and dividing both sides by \(N\) we arrive at the expression appearing in theorem B.2.

## Appendix C Appendix: Method-of-Moments Estimator

### Derivation of method-of-moment estimator

We now turn to constructing our method-of-moments estimator of \(\|\bm{\Sigma}_{ij}\|_{*}=\sum_{n=1}^{N}s_{n}(\bm{\Sigma}_{ij})\), which is required for our novel estimator of the Riemannian shape distance. We can form an unbiased estimator of the matrix \(\bm{\Sigma}_{ij}\) by observing a single random stimuli in the two networks:

\[\bm{\hat{\Sigma}}_{ijm}:=h_{i}(\bm{z}_{m})h_{j}(\bm{z}_{m})^{\mathsf{T}}\in \mathbb{R}^{N\times N},\quad\mathbb{E}[\bm{\hat{\Sigma}}_{ijm}]=\bm{\Sigma}_ {ij}\]

Note that here the randomness comes from the selection of the stimuli, i.e. \(\bm{z}_{m}\sim P\); the output of the network is deterministic. Assuming \(m,m^{\prime}\) are distinct stimuli drawn independently from the distribution \(P\), we then have:

\[\mathbb{E}\left[\bm{\hat{\Sigma}}_{ijm}\bm{\hat{\Sigma}}_{ijm^{\prime}}\right] =\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}}\]

This means we can estimate \(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}}\) by observing a pair of stimuli in both networks.

\[\operatorname{Tr}\left[f(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{ \mathsf{T}})\right] =\sum_{n=1}^{N}f\left(s_{n}^{2}(\bm{\Sigma}_{ij})\right)=\sum_{n =1}^{N}\sum_{p=0}^{\infty}\gamma_{p}s_{n}^{2p}(\bm{\Sigma}_{ij})\] Taylor expansion of \[f(\cdot)\] \[=\sum_{p=0}^{\infty}\gamma_{p}\sum_{n=1}^{N}s_{n}^{2p}(\bm{\Sigma} _{ij})=\sum_{p=0}^{\infty}\gamma_{p}\operatorname{Tr}\left[\left(\bm{\Sigma}_ {ij}\bm{\Sigma}_{ij}^{\mathsf{T}}\right)^{p}\right]\] \[=\sum_{p=0}^{\infty}\gamma_{p}\mathbb{E}\left[\operatorname{Tr} \left[\prod_{\sigma=1}^{p}\bm{\hat{\Sigma}}_{ij(2\sigma-1)}\bm{\hat{\Sigma}}_{ ij(2\sigma)}^{\mathsf{T}}\right]\right]\] Substitute unbiased estimator for \[\left(\bm{\Sigma}_{ij}\bm{\Sigma}_{ij}^{\mathsf{T}}\right)^{p}\] \[\approx\sum_{p=0}^{P}\gamma_{p}\mathbb{E}\left[\operatorname{Tr} \left[\prod_{\sigma=1}^{p}\bm{\hat{\Sigma}}_{ij(2\sigma-1)}\bm{\hat{\Sigma}}_{ ij(2\sigma)}^{\mathsf{T}}\right]\right]\] Approximate with truncated power series

Our estimator for the nuclear norm of \(\bm{\Sigma}_{ij}\) is thus:

\[\widehat{\|\bm{\Sigma}_{ij}\|_{*}}=\sum_{p=0}^{P}\gamma_{p} \operatorname{Tr}\left[\prod_{\sigma=1}^{p}\bm{\hat{\Sigma}}_{ij(2\sigma-1)} \bm{\hat{\Sigma}}_{ij(2\sigma)}^{\mathsf{T}}\right]\] (C.1)

Note that for each element of the product we are considering the estimator based on stimuli \((2\sigma-1)\) and \((2\sigma)\); in total this estimator will use \(2P\) unique stimuli.

### Deriving the Quadratic Program

The optimization problem in eq. (2.13) takes the form:

\[\underset{\bm{\gamma}}{\text{minimize}}\quad\bm{\gamma}^{\mathsf{T}}\bm{A} \bm{\gamma}+N^{2}\left(\max_{x}f^{2}(\bm{\gamma},x)\right)\] (C.2)where \(f(\bm{\gamma},x)=x^{1/2}-\sum_{p}\gamma_{p}x^{p}\),

\[\bm{\gamma}=\begin{bmatrix}\gamma_{1}\\ \vdots\\ \gamma_{p}\end{bmatrix}\in\mathbb{R}^{P},\quad\bm{A}=\begin{bmatrix}\mathbb{C} \text{ov}(\hat{W}_{1},\hat{W}_{1})&\ldots&\mathbb{C}\text{ov}(\hat{W}_{1},\hat {W}_{P})\\ \vdots&&\vdots\\ \mathbb{C}\text{ov}(\hat{W}_{P},\hat{W}_{1})&\ldots&\mathbb{C}\text{ov}(\hat{W} _{P},\hat{W}_{P})\end{bmatrix}\in\mathbb{R}^{P\times P},\] (C.3)

Notice that \(f\) is linear in \(\bm{\gamma}\), and that \(\bm{A}\) is symmetric, positive-definite.

We will reformulate eq. (C.2) in several steps, and ultimately obtain a quadratic program that can be efficiently solved. First, we introduce a new optimization variable \(u\in\mathbb{R}\) whose square is an upper bound on \(f^{2}(\bm{\gamma},x)\) for all \(x\in[0,1]\). Thus, the optimal \(\bm{\gamma}\) for the problem:

\[\underset{\bm{\gamma},u}{\text{minimize}} \bm{\gamma}^{\mathsf{T}}\bm{A}\bm{\gamma}+N^{2}u^{2}\] (C.4) subject to \[u^{2}\geq f^{2}(\bm{\gamma},x)\quad\text{for all }x\in[0,1]\]

coincides to the optimal \(\bm{\gamma}\) solving eq. (C.2). This is essentially an _epigraph reformulation_ of the original problem [see 3, equation 4.11]. Notice that the objective function is quadratic in this reformulation.

Next, we lay down a fine grid of linearly spaced test points \(x_{1},\ldots,x_{T}\in[0,1]\). We can then obtain a good approximation to the solution in eq. (C.4) by solving:

\[\underset{\bm{\gamma},u}{\text{minimize}} \bm{\gamma}^{\mathsf{T}}\bm{A}\bm{\gamma}+N^{2}u^{2}\] (C.5) subject to \[u^{2}\geq f^{2}(\bm{\gamma},x_{t})\quad\text{for all }t\in 1, \ldots,T\]

Of course, increasing \(T\) (the number of test points) improves the approximation arbitrarily well.

Finally, the constraints of the problem can be put into a form that is jointly linear in \(\bm{\gamma}\) and \(u\). First, constraining \(u^{2}\geq f^{2}(\bm{\gamma},x_{t})\) is equivalent to simultaneously constraining \(u\geq f(\bm{\gamma},x_{t})\) and \(u\geq-f(\bm{\gamma},x_{t})\). Then, plugging in the definition of \(f(\bm{\gamma},x_{t})\), and rearranging we have:

\[\underset{\bm{\gamma},u}{\text{minimize}} \bm{\gamma}^{\mathsf{T}}\bm{A}\bm{\gamma}+N^{2}u^{2}\] (C.6) subject to \[u+\sum_{p}\gamma_{p}x_{t}^{p}\geq x_{t}^{1/2}\quad\text{for all }t\in 1, \ldots,T\] \[u-\sum_{p}\gamma_{p}x_{t}^{p}\geq-x_{t}^{1/2}\quad\text{for all }t\in 1, \ldots,T\]

This objective is quadratic and the constraints are linear with respect to the optimized quantitiesa chosen constant (Fig. 3A, extent of blue shaded area centered around true similarity score). The actual maximal bias for a given solution to the program will then be less than or equal to the user defined bias (cyan shaded area within blue). The expected value of the moment estimator stays within the maximal bias, in this case on its bound (orange trace mean and SD across 5,000 simulations). The user defined bias bound remains inactive until it is less than the MSE minimizing solution's bias (blue shaded area completely overlapped by cyan when user defined bias bound is less than 0.2). Variance then begins to increase as higher order \(W_{p}\) terms are weighted more heavily to reduce bias (orange standard deviation bars from simulation increase as cyan region narrows). The expected value of the estimator converges to ground truth as it is constrained by the bias bound (dotted orange line converges to dashed black). The plug-in estimator exceeds the maximal bias of the moment estimator (blue trace outside of cyan shaded area).

Intuition for the method-of-moments estimator can be drawn from example plots of solutions to the power series approximation to the square root (eq. 2.11, Fig. 3B, orange trace approximates black dashed trace) of the squared singular values of \(\Sigma_{1,2}\) (black points all overlapping). Here we have re-scaled the singular values on the vertical axis so that the deviation between the square root and power series approximation is exactly the bias of the moment estimator. In the case where bias is not constrained (associated with left most estimates in panel B) the approximation is poor (dashed-dot orange trace does not match dashed black trace). For these eigenvalues the the deviation is near the worst possible bias (distance from black point dashed dot orange line is nearly as far as any other vertical deviation between the traces), this is why the estimator in panel B sits at the bound of maximal possible bias. On the other hand when the upper bound on bias is very small (far right of B) the approximation is very good (dashed orange overlaps dashed black) because higher order terms are used. Yet this results in very high variance (Fig. 3B).

### Validation on neural data

Here we demonstrate that the estimator performs as expected when applied to noisy non-normal data where covariance of the \(\hat{W}_{p}\) and the denominator of the similarity score must be estimated from data. (Experiments on synthetic data verifying the estimators behave as expected can be found in App. 2.3.) We do so by applying our estimator to neural data: calcium recordings from mouse primary visual cortex in responses to a set of 2,800 natural images repeated twice [25]. We found that our estimator became highly variable when applied to this data in part because of its low SNR and low number of repeat (average SNR \(\approx 0.1\)). We thus select neurons with the highest levels of SNR in each recording to perform our analyses on. To assess variability of the estimates we ran independent simulations from the same distribution by randomly sub sampling stimuli presentations within a recording into 3 disjoint sets.

Figure 3: Control of bias-variance tradeoff with user defined bound on bias. **(A)** Here the moment based estimator is constrained to be within the user defined bias bound (blue region) and to minimize worst case MSE (eq. (2.13)). Maximal bias can be less than the user defined bias (cyan region within blue). As the estimator is constrained to have less bias variance increases (orange trace converges to black dashed as SD bars widen as ). Where simulations become unstable we plot the theoretical expected value (dotted orange). Plug-in estimator is well outside bias bounds of moment estimator thus is more biased than moment estimator (blue trace outside cyan line). **(B)** Example plots of solutions to the quadratic program’s approximation (orange traces) to square root (black dashed trace) of the eigenvalues of \(\Sigma_{1,2}\) (black points). Re-scaling of singular values on vertical axis results in the deviation between the polynomial and the true square root evaluated at the true eigenvalues being exactly the bias of the associated estimates in panel A.

To determine the properties of the bias of our estimator requires comparison to the ground truth value of the similarity score. In the neural data ground truth is unknown. We thus developed two sampling schemes to set the ground truth similarity in the neural data. To set similarity to 0 we measured similarity between different populations of neurons shown different stimuli, thus the two populations responses are independent, thus their cross covariance is 0 so that the similarity score is 0. To set the similarity to 1 we measured similarity between the same population of neurons shown the same stimuli but on different trials, thus the only deviation in their responses is owing to trial-to-trial variability, thus their tuning similarity is 1.

We applied our estimator to populations of neurons (\(N=40\) each) where the ground truth was zero. We found that across recordings the moment estimator correctly indicated the similarity was near 0 (Fig. 4A, orange trace overlaps black dashed) and the confidence intervals always contained the true similarity (light orange contains black dashed). On the other hand the plug-in estimator was upwardly biased (blue above black dashed). Thus the moment based estimator can accurately determine when the similarity is low in noisy neural data whereas the plug-in estimator cannot.

When ground truth similarity was 1, we found the bias of the moment estimator was worse than that of the plug-in (Fig. 4B, blue overlaps black dashed, orange below). This is consistent with our synthetic simulations (see Fig. 2A far right). The CIs always contained the true value but contained nearly the entire possible range of similarity values. Thus while the average estimate is high our confidence intervals are so wide that we do not have much information about the true similarity.

Finally, we assessed the estimators' performance measuring the true similarity between these populations of high SNR neurons (Fig. 4C). Across recordings the moment estimator was near 0.5 but confidence intervals were wide so there is little information about similarity even for the highest SNR neurons (light orange extends from 0 to 1 on vertical axis). The plug-in estimator reports a higher degree of similarity, that we heavily discount given its upward bias. When we included all stimuli (\(M\approx 2800\)) we obtained more accurate estimates, learning that the true similarity is most likely between 0.25 and 0.75 (Fig. 4D). Thus small populations of well-tuned neurons in the same brain region have only intermediate levels of representational similarity. Overall, we find noisy data is a challenging setting for reducing the bias of shape similarity estimates.

### Experimental data from Stringer et al. [25]

Neural activity in mouse primary visual cortex was recorded using a two-photon microscope while mice were free to run on an air-floating ball. Recordings were collected across multiple depth planes at a frequency of 2.5 or 3 Hz, with planes 30-35 \(\mu m\) apart. The field of view of the microscope was selected such that 10,000 neurons could be observed within a retinotopic location on the stimulus display.

All stimuli were presented for 0.5s with a random inter-stimulus interval between 0.3 and 1.1s consisting of a grey-screen. The images used in the experiment were taken from the ImageNet database, which includes categories such as birds, cats, and insects. The researchers manually

Figure 4: Validation of estimator on neural data [25]. **(A)** Comparison of estimators when ground truth similarity of neural data is set to 0. The estimator is applied to three disjoint sets of random stimuli for each recording (\(n=7\)). The estimated maximal bias is plotted in dark orange area and the confidence interval, which includes bias, is plotted in light orange. **(B)** Same simulation as (A) except ground truth similarity is 1.**(C)** Same as (B) except estimation of true similarity. **(D)** Estimation of true similarity on all stimuli. (\(M\approx 2,800\)).

selected images that had a mix of low and high spatial frequencies and that did not consist of more than 50 % uniform background. All images were uniformly contrast-normalized by subtracting the local mean brightness and dividing by the local mean contrast. Each stimulus consisted of a different normalized image from the ImageNet database, with 2,800 different images used in total. The same image was displayed on all three screens, but each screen showed the image at a different rotation. Each of the 2,800 natural image stimuli were displayed twice in a recording in two blocks of the same randomized order.

Calcium movie data was processed using the Suite2p toolbox to estimate spike rates of neurons. Underlying neural activity was estimated using non-negative spike deconvolution (Frierich et. al., 2017). These deconvolved traces were normalized to the mean and standard deviation of their activity during a 30-minute period of grey-screen spontaneous activity. For further detail please see the original study [25]. All analyses done in this paper were performed on the pre-processed data available on figshare (https://figshare.com/articles/Recordings_of_ten_thousand_neurons_in_visual_cortex_in_response_to_2_800_natural_images/6845348).