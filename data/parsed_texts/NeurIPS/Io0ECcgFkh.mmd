# RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation

 Kaiqu Liang\({}^{1}\), Haimin Hu\({}^{2}\), Ryan Liu\({}^{1}\), Thomas L. Griffiths\({}^{1,3}\),

**Jaime Fernandez Fisac\({}^{1,2}\)**

\({}^{1}\)Department of Computer Science, Princeton University

\({}^{2}\)Department of Electrical and Computer Engineering, Princeton University

\({}^{3}\)Department of Psychology, Princeton University

{kl2471,haiminh,ryanliu,tong,jfisac}@princeton.edu

###### Abstract

Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on _immediate_ feedback, which can fail to reflect the true downstream impact of an interaction on users' utility. We demonstrate that this horstighted feedback can, by itself, result in misaligned behaviors like sycophancy and deception, and we propose to alleviate this by refocusing RLHF on _downstream consequences_. Our theoretical analysis reveals that the hindsight gained by simply delaying human feedback mitigates misalignment and improves expected human utility. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods--Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO)--and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.

## 1 Introduction

Aligning artificial intelligence (AI) systems with human values and intentions is crucial to ensuring they behave in ways that are helpful, honest, and trustworthy. A widely-deployed method for achieving this alignment is through human feedback (Leike et al., 2018), with successful applications to, e.g., training AI assistants (Glaese et al., 2022; Touvron et al., 2023; Anthropic, 2023; Achiam et al., 2023). In particular, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Stiennon et al., 2020) leverages human feedback to fine-tune and align foundation models (FMs). While RLHF has shown promise in aligning models with human preferences, it often relies heavily on human perceptions during interactions, which may not accurately reflect the downstream consequences of the service provided. Such myopic feedback can misguide the model's behavior and limit its effectiveness in aligning with human values. For example, human evaluators could misjudge an interaction on the spot, due to limited resources (e.g., partial observability; Casper et al., 2023; Lang et al., 2024) or limited bandwidth (e.g., constraints on time, attention, or care; Pandey et al., 2022; Chmielewski and Kucker, 2020), leading to incomplete or misinformed feedback. A recent theoretical study has suggested that partial observability in RLHF can lead to deceptive model behaviors (Lang et al., 2024). Complementing this analysis, we provide substantial empirical evidence that immediate human feedback frequently misrepresents true utility in everyday interaction settings and, when used as a proxy for it in RLHF fine-tuning, systematically results in misalignment with human goals. This misalignment often manifests as _positive illusion_(fabricating or exaggerating the good and omitting or downplaying the bad), where the model's behavior shifts towards momentarily pleasing the user rather than providing accurate and genuinely helpful advice. Unfortunately, this consistently leads users to make ill-informed decisions whose poor downstream outcomes contrast starkly with their high satisfaction rating at the end of the interaction.

Our central insight is that the utility provided by an AI system to a human user (and similarly its "helpfulness" and "harmlessness", which RLHF evaluators are typically asked to assess), is not generally an intrinsic property of the outputs that it generates, but rather a function of their real-world consequences, brought about by the human user's actions upon consuming said outputs. Evaluators presented with a human-AI interaction without explicit information about its later consequences must either neglect them or implicitly estimate them when providing their assessment. Unfortunately, neither option is suitable for the harder use cases in which human users truly need to rely on AI assistance, precisely the ones in which alignment is crucial, especially as AI capabilities continue to increase.

To address these open challenges, we propose to leverage _hindsight_ as a simple but effective misalignment mitigation mechanism, in which evaluators experience the downstream outcomes of an interaction before being asked for feedback on the model. We provide both theoretical analysis and extensive empirical studies to show the efficacy of hindsight in significantly reducing misalignment of RLHF-trained models. To circumvent the material and ethical difficulties in exposing real people to real consequences, we introduce a novel alignment algorithm called **R**einforcement **L**earning from **H**indsight **S**imulation (**RLHS**), an alternative to RLHF that rapidly simulates human decisions and their downstream outcomes during training, allowing the evaluator to directly assess the long-term impact of the model's outputs rather than relying on an implicit guess of its later outcomes.

Our key finding is that equipping evaluator feedback with the benefit of hindsight--even if this is simulated using imperfect models--can significantly reduce model misalignment with the evaluator's true utility, decreasing the chances of deceptive and misleading outputs. We implement hindsight simulation with both offline and online preference optimization approaches, including direct preference optimization (DPO) (Rafailov et al., 2024) and proximal policy optimization (PPO) (Schulman et al., 2017) and show empirically that it greatly improves alignment in both training paradigms. We also present results from human user studies, in which RLHS consistently improves both users' ground-truth utility and subjective satisfaction, despite being trained with only simulated hindsight feedback. Our comparative findings demonstrate that RLHS significantly outperforms non-hindsight methods--specifically Reinforcement Learning from AI Feedback (RLAIF), which similarly uses AI generation as a proxy for real human feedback, and has been shown to produce results closely resembling that of RLHF (Bai et al., 2022b; Lee et al., 2023).

Figure 1: **RLHF** can incentivize AI systems to provide inaccurate or deceptive information to their users, prioritizing positive on-the-spot feedback and neglecting long-term consequences. For example, a customer may prefer to hear good news while shopping but will ultimately be disappointed (and objectively worse off) if stuck with an ill-informed purchase. The proposed **RLHS** introduces hindsight for human feedback, focusing on evaluations after knowing the outcome. This enables more informed feedback that improves alignment between the AI and the humanâ€™s true utility.

## 2 Alignment Algorithm: RL from Hindsight Simulation

Recent studies have revealed that RLHF can result in misalignment when humans provide feedback based on _partial observations_, rather than the typically assumed--but rarely realistic--full state sequences. This limitation can lead to deceptive or manipulative behaviors in AI systems (Casper et al., 2023; Lang et al., 2024). To address misalignment caused by human uncertainty in RLHF, we propose Reinforcement Learning from Hindsight Simulation (RLHS). The core idea is that by providing humans with information about future outcomes, the learned reward and policy will be significantly better aligned. While the reminder of this paper focuses on the algorithm and results of RLHS, we provide a mathematical formulation of general human-AI alignment problems in Appendix A and prove that the hindsight feedback approaches the oracle one for a sufficiently large hindsight horizon in Appendix B, elucidating the advantage of RLHS over RLHF.

**Hindsight Simulation**. While we have demonstrated theoretically that providing hindsight can mitigate misalignment in RLHF, exposing humans to real consequences can circumvent material and ethical difficulties. To address this, we introduce the concept of _hindsight simulation_--the namesake of our core contribution, RLHS--which allows evaluators, whether human or AI, to make more informed decisions based on simulated outcomes. In practice, hindsight simulation can involve collecting feedback from human evaluators or employ another language model as a proxy to simulate the feedback process. After an evaluator makes a decision based on their interaction with the AI (e.g., purchasing an item), the system provides _groundtruth_ information about the outcome, i.e., the hindsight (e.g., whether the purchased item meets the desired criteria). The evaluator then provides feedback informed by both the decision's outcome and their prior interaction with the model.

This feedback typically takes the form of a rating or binary preference, similar to the feedback used in conventional RLHF. However, unlike the _immediate_ feedback provided solely during an interaction without access to the decision's consequences, feedback obtained through hindsight simulation is more informed as it incorporates long-term outcomes. This aligns with the reasoning presented in Appendix B.1 and demonstrates the potential for improving alignment through simulated hindsight.

We implement this approach with two subroutines: (i) _partial hindsight_, where only a limited set of hindsight information is available to the agent, in a way that more closely matches real-world scenarios, and (ii) _oracle hindsight_, where the agent has access to full set of hindsight information. We hope that through our subsequent empirical study employing both partial and oracle hindsight, we can gain insights into how extending the hindsight step (i.e., revealing additional outcome information to the agent) can improve the alignment performance of the model.

**Illustrative Example: Marketplace Chatbot.** We demonstrate the practical impact of RLHS by applying it to a marketplace AI chatbot. The chatbot's goal is to assist customers in making purchasing decisions by providing recommendations based on available product information. We assume that both customers and the chatbot have access to some public information, such as a list of items and their prices, but customers have their internal preferences, e.g., wanting a TV with 8K resolution, that are unknown to the chatbot. To the best of our knowledge, existing RLHF schemes deployed for training marketplace chatbots (e.g., Amazon, 2024) use customer feedback solely based on the interaction (i.e., if they feel happy about the chatbot's service) but not on the outcome (i.e., if the purchased item has actually met their preferences), potentially causing misalignment.

Our proposed hindsight simulation approach aims to mitigate this issue by deferring the humans' feedback until they have been informed of the outcome of their decisions, e.g., they have received the product and verified that their expectations are (not) met. In hindsight simulation, the simulated customer interacts with the chatbot, makes a purchasing decision, checks the outcome (hindsight) provided by the system, and provides feedback on the customer's satisfaction with the service.

## 3 Experimental Design

### Data Collection

**Preference Data Collection.** Our training data collection process closely follows the standard RLHF data collection pipeline (Stiennon et al., 2020; Ouyang et al., 2022), where feedback is collected based on comparisons between outputs. However, instead of relying on real human feedback, weemployed a strong large language model (LLM) model as a judge to simulate human interactions with the chatbot and provide feedback. For real-world online marketplace chatbots like the Amazon Rufus (Amazon, 2024), human feedback is typically given as a rating at the end of the interaction. However, human users tend to compare their current experience with previous ones when assigning ratings. To capture this behavior, we simulate users comparing services from two different stores and selecting their preferred option, rather than rating each scenario in isolation. This closely aligns with the preference-based data collection method used in prior work (Stiennon et al., 2020; Ouyang et al., 2022), where users provide feedback by comparing responses instead of giving individual ratings.

**Decision-making simulation.** While collecting the preference data, our simulated human (strong model) takes on three roles: interacting with the chatbot, making decisions, and providing feedback. To ensure accurate decision-making and feedback, we adapted the approach in introspective planning (Liang et al., 2024). First, we frame the decision-making problem as a multiple-choice question with four options: (A) Buy option A, (B) Buy option B, (C) Buy option C, or (D) Do not buy anything. We then ask the LLMs to perform Chain-of-Thought reasoning (Wei et al., 2022), querying the next token probabilities to select the best option from \(A,B,C,D\). This approach can reduce the language agent's uncertainty. We apply a similar method for comparing services between two stores.

**Dataset Details.** In our experiments, we used both Llama-2-7B (Touvron et al., 2023) and Llama-3-8B (Dubey et al., 2024) as the AI assistants, and Llama-3.1-70B (Dubey et al., 2024) as the simulated human to interact with the AI assistant and provide feedback. We collected **11,000** preference data points for each AI assistant model, with 10,000 used for training and 1,000 for validation. We also generated a test set of **1,200** examples to evaluate performance across different customer scenarios.

### Experiment Setup

**Environment Details.** In each of our simulated marketplace scenarios there are 10 candidate items, each characterized by 8 features and a price. Each feature can be categorized in two ways: (1) The item either has or lacks a specific feature (e.g., a TV with HDR vs. without HDR), and (2) The feature

Figure 2: **Qualitative results for Llama-2-7b trained with either immediate feedback (RLHF) or partial hindsight (RLHS). The model trained with immediate feedback leads to deception by falsely claiming that both Options A and C meet the customerâ€™s 8K resolution requirement, when in fact, neither does. In contrast, the model trained with partial hindsight truthfully states that none of the available options include 8K resolution.**

[MISSING_PAGE_FAIL:5]

## 4 Simulation Results

**Misalignment between satisfaction rating and real utility.** When using standard RLHF (Ouyang et al., 2022), we observe significant misalignment between user satisfaction ratings and true utility as training progresses (left plot in Figs. 3 and 4). While the satisfaction rating steadily increases, indicating that the language model is learning to deliver responses that receive higher immediate user approval, the true utility shows a sharp decline. This suggests that while the chatbot's responses may appear more polished or helpful in the short term, they are in fact becoming less aligned with the user's true needs or long-term goals. As a result, while users may initially perceive the chatbot's responses as helpful, they are frequently misled and ultimately dissatisfied with their final outcomes. This highlights a fundamental flaw in using standard RLHF with immediate feedback, as it risks optimizing for superficial satisfaction at the expense of true utility.

**Hindsight simulation effectively mitigates misalignment.** As shown in Fig. 3 (left), relying on immediate feedback leads to a steady decline in real utility, ultimately resulting in negative overall utility. In contrast, hindsight simulation consistently improves utility throughout training, eventually achieving positive utility, as in Fig. 3 (middle). It aligns upward trends in both real utility and satisfaction ratings, significantly reducing the gap between them. The qualitative results shown in Fig. 2 further support our claim. When the AI assistant is trained on immediate feedback, it deceptively claims that both Options A and C meet the requirements of the (simulated) customer for 8K resolution, though neither actually does. In contrast, training with partial hindsight leads to truthful responses, acknowledging that none of the options meet the 8K resolution requirement. This shows that while traditional RLHF with immediate feedback may cause misalignment, hindsight simulation mitigates this issue, improving the overall helpfulness and honesty of language agents.

## 5 Human Study

Our human study had three goals: (Goal 1) evaluate the performance of models trained with immediate feedback vs. those trained with hindsight simulation, (Goal 2) assess how hindsight information affects user satisfaction. To achieve these goals, we designed two similar human experiments. Both experiments used Llama-3-8b (Dubey et al., 2024) trained with DPO using either immediate feedback or partial hindsight. We conducted online human experiments via Prolific (Palan and Schitter, 2018), involving 200 participants across 10 scenarios, randomly sampled from a test set of 1,200. For each scenario, 20 participants took part: 10 interacting with each of the RLHF model and the RLHS model.

**Pipeline for evaluating model performance.** The first and second experiments follow the same pipeline but differ in the models used--one is trained with immediate feedback, and the other with partial hindsight simulation--allowing us to compare their performance (Goal 1). Initially, participants are shown a list of available items in a store with hidden features. We specify their requirements for the item (e.g., "must have 8K resolution"). Participants interact with the chatbot to gather information about the products. At each step, they can choose one of the following actions: "ask about the desired feature," "ask about the price", or "ready to make a decision". Pre-generated responses are provided for inquiries. In the second round of interaction, participants may ask about the information they didn't request in the first round. At any point, participants can choose "ready to

Figure 4: **Results on Llama-2-7b trained with DPO.**_Left:_ _Demonstrates the Misalignment of real utility and satisfaction ratings using immediate feedback._Middle:_ _Shows how partial hindsight mitigate the misalignment._Right:_ _Shows the alignment achieved with oracle hindsight._make a decision", at which time they must decide whether to make a purchase decision or opt not to buy. After making their decision, they provide an immediate satisfaction rating.

Hindsight information is then introduced. Buyers learn whether the item meets their requirements (e.g., whether the item has the desired feature) while non-buyers receive no additional information. Participants then provide a second satisfaction rating, referred to as the hindsight rating, which evaluates their long-term satisfaction after considering the hindsight information. This step allows us to assess the impact of hindsight information on user satisfaction (Goal 2). Finally, buyers may keep or return the item, enabling us to quantify the regret rate.

**Statistical Hypothesis Testing.** We conducted experiments to test four hypotheses, using one-tailed and standard t-tests for the first three hypotheses (Fisher, 1970), and Pearson's correlation coefficient for the fourth (Sedgwick, 2012). The one-tailed t-test framework used in Hypotheses 1, 2, and 3 is outlined below. The null hypothesis (\(H_{0}\)) and the alternative hypothesis (\(H_{1}\)) are defined as:

\[H_{0}:\mu_{1}\leq\mu_{2}\] (Group 1 satisfaction is less than or equal to Group 2) \[H_{1}:\mu_{1}>\mu_{2}\] (Group 1 satisfaction is significantly higher than Group 2)

Here, \(\mu_{1}\) and \(\mu_{2}\) represents the mean satisfaction of Group 1 and Group 2, respectively. The two-tailed t-test follows a similar format but tests for any significant difference between the group means.

_Hypothesis 1: Models trained with RLHS lead to a higher long-term user satisfaction rate and lower regret rate than those trained with RLHF using immediate feedback._

We evaluated hindsight ratings for two models: Group 1 (RLHS) and Group 2 (RLHF). The hypothesis test resulted in \(p=4\times 10^{-8}\), well below the significance threshold of 0.001. When reversing the groups for regret rates, the test yielded \(p=5\times 10^{-5}\) again below 0.001.

_Hypothesis 2: Models trained with RLHF using immediate feedback often experience a notable decline in user satisfaction once future outcomes are revealed, while RLHS mitigate this decline._

Group 1 consisted of users interacting with RLHF without hindsight feedback, and Group 2 received hindsight feedback. The hypothesis test gave \(p=4\times 10^{-9}\), confirming a significant decline. To demonstrate RLHS mitigates this decline, we ran a two-tailed t-test comparing immediate and hindsight ratings. The result, \(p=0.90\), showed no significant difference.

_Hypothesis 3: RLHS lead to significantly higher true utility than RLHF._

We assessed the objective performance of the two models by comparing true utility scores for Group 1 (RLHS) and Group 2 (RLHF). The hypothesis test yielded \(p=4\times 10^{-8}\), confirming that RLHS achieves significantly higher true utility than RLHF.

_Hypothesis 4: Models trained with RLHS are more truthful, presenting a strong correlation between their high immediate user satisfaction rate (subjective) and high true utility (objective)._

To evaluate the correlation, we used Pearson's correlation coefficient and tested whether this coefficient was significantly different from zero. The null hypothesis (\(H_{0}\)) assumed no correlation (i.e., \(r=0\)) while the alternative hypothesis (\(H_{1}\)) assumed a non-zero correlation. The test found a significant correlation between immediate ratings and true utility for RLHS (\(p=5\times 10^{-4}\)), while no significant correlation was observed for RLHF (\(p=0.47\)).

Figure 5: The policy trained using the proposed RLHS outperforms that of RLHF in both true utility (_left_) and hindsight rating (_right_). In both plots, each point represents the mean ratio for a scenario, with lines indicating the standard deviation. The identity line is plotted in dashed grey.

**Analysis.** These results validated Hypotheses 1 and 2 with subjective improvements in user satisfaction and regret for RLHS over RLHF, while Hypothesis 3 was verified with the objective improvement in the true utility. We also see from the results a strong alignment between subjective satisfaction and objective utility for the RLHS model, thus validating Hypothesis 4. In addition to the statistical significance tests, we visualize the metrics in Table 2, which shows that training with hindsight simulation (RLHS) achieves a higher long-term satisfaction score (3.71) compared to immediate feedback (RLHF), which only reaches 2.65, supporting Hypothesis 1. Further, RLHF obtained a high immediate rating of 3.74 before hindsight, but it then dropped significantly to 2.65 after the outcome is revealed, thereby validating Hypothesis 2. While both models achieved similar immediate ratings, RLHS achieves a significantly higher true utility (0.43). These results confirm that RLHF can lead to misalignment, whereas RLHS mitigates this, resulting in a more helpful and truthful language agent. We also visualize the utility and rating ratio for each scenario between RLHS and RLHF in Fig. 5.

## 6 Conclusion

In this work, we introduced Reinforcement Learning from Hindsight Simulation (RLHS), an algorithmic framework that mitigates misalignment in RLHF by providing evaluators with future outcome information. We demonstrated that RLHS can significantly improve utility compared to existing RLHF pipelines that rely only on immediate feedback, while maintaining a high user satisfaction rate throughout the human-AI interaction. While our study focused on simulated hindsight with an application to marketplace chatbot, future work should explore incorporating hindsight in RLHF for additional real-world applications with real human evaluators. Further, we see an open opportunity to equip RLHS with other feedback modalities, such as visual cues, which could further enrich the feedback process and improve alignment.

## References

* A. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. (2023)Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.
* A. A. How customers are making more informed shopping decisions with rufus, amazon's generative ai-powered shopping assistant. Note: https://www.aboutamazon.com/news/retail/how-to-use-amazon-rufus Cited by: SS1.
* A. A. Claude (2023)Lattice 2. Note: https://www.anthropic.com/index/claude-2 Cited by: SS1.
* Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. (2022)Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Cited by: SS1.
* Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. (2022)Contributional ai: harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Cited by: SS1.
* Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, et al. (2024)Benchmarking foundation models with language-model-as-an-examiner. Advances in Neural Information Processing Systems36. Cited by: SS1.
* A. Bajcsy and J. F. Fisac (2024)Human-AI safety: a descendant of generative ai and control systems safety. arXiv preprint arXiv:2405.09794. Cited by: SS1.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Model & Immediate rating & Hindsight rating & True utility & Regret rate \\ \hline RLHF & \(3.74_{\pm 0.94}\) & \(2.65_{\pm 1.55}\) & \(-0.16_{\pm 0.87}\) & \(0.64_{\pm 0.48}\) \\ RLHS & \(3.69_{\pm 1.05}\) & \(3.71_{\pm 1.10}\) & \(0.43_{\pm 0.60}\) & \(0.23_{\pm 0.42}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison between RLHF and RLHS models across multiple metrics.While RLHF shows higher immediate satisfaction, RLHS outperforms in hindsight rating, true utility, and regret rate, indicating better long-term alignment with user preferences and reduced regret.

Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. Fine-tuning language models to find agreement among humans with diverse preferences. _Advances in Neural Information Processing Systems_, 35:38176-38189, 2022.
* Bradley and Terry (1952) Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jeremy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillio Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitri Krasheninnikov, Xin Chen, Laurro Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* Chen et al. (2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. _arXiv preprint arXiv:2307.08701_, 2023.
* Chen et al. (2024) Lichang Chen, Chen Zhu, Davit Sosedia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. Odin: Disentangled reward mitigates hacking in rhlf. _arXiv preprint arXiv:2402.07319_, 2024.
* Chmielewski and Kucker (2020) Michael Chmielewski and Sarah C Kucker. An mturk crisis? shifts in data quality and the impact on study results. _Social Psychological and Personality Science_, 11(4):464-473, 2020.
* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. _arXiv preprint arXiv:2407.21783_, 2024.
* Dubois et al. (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsumori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ethayarajh et al. (2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. _arXiv preprint arXiv:2402.01306_, 2024.
* Fernandes et al. (2023) Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andre FT Martins, Graham Neubig, Ankush Garg, Jonathan H Clark, Markus Freitag, and Orhan Firat. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. _arXiv preprint arXiv:2308.07286_, 2023.
* Fisher (1970) Ronald Aylmer Fisher. Statistical methods for research workers. In _Breakthroughs in statistics: Methodology and distribution_, pp. 66-70. Springer, 1970.
* Gao et al. (2023) Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In _International Conference on Machine Learning_, pp. 10835-10866. PMLR, 2023.
* Glaese et al. (2022) Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. _arXiv preprint arXiv:2209.14375_, 2022.
* Gudibande et al. (2023) Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. _arXiv preprint arXiv:2305.15717_, 2023.
* Gudibande et al. (2024)Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable stochastic games. In _AAAI_, volume 4, pp. 709-715, 2004.
* Hong et al. (2022) Joey Hong, Kush Bhatia, and Anca Dragan. On the sensitivity of reward inference to misspecified human models. _arXiv preprint arXiv:2212.04717_, 2022.
* Hsu et al. (2023) Kai-Chieh Hsu, Haimin Hu, and Jaime F Fisac. The safety filter: A unified view of safety-critical control in autonomous systems. _Annual Review of Control, Robotics, and Autonomous Systems_, 7, 2023.
* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Hu et al. (2023) Haimin Hu, Zixu Zhang, Kensuke Nakamura, Andrea Bajcsy, and Jaime Fernandez Fisac. Deception game: Closing the safety-learning loop in interactive robot autonomy. In _7th Annual Conference on Robot Learning_, 2023.
* Lambert et al. (2024) Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. _arXiv preprint arXiv:2403.13787_, 2024.
* Lang et al. (2024) Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, and Scott Emmons. When your ai deceives you: Challenges with partial observability of human evaluators in reward learning. _arXiv preprint arXiv:2402.17747_, 2024.
* Lee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaf: Scaling reinforcement learning from human feedback with ai feedback. _arXiv preprint arXiv:2309.00267_, 2023.
* Leike et al. (2018) Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. _arXiv preprint arXiv:1811.07871_, 2018.
* Li et al. (2023) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. _arXiv preprint arXiv:2308.06259_, 2023a.
* Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023b.
* Liang et al. (2024) Kaiqiu Liang, Zixu Zhang, and Jaime Fernandez Fisac. Introspective planning: Guiding language-enabled agents to refine their own uncertainty. _arXiv preprint arXiv:2402.06529_, 2024.
* Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. _arXiv preprint arXiv:2305.20050_, 2023.
* Lindner and El-Assady (2022) David Lindner and Mennatallah El-Assady. Humans are not boltzmann distributions: Challenges and opportunities for modelling human feedback and interaction in reinforcement learning. _arXiv preprint arXiv:2206.13316_, 2022.
* Luce (1959) R Duncan Luce. _Individual choice behavior_, volume 4. Wiley New York, 1959.
* Luo et al. (2023) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_, 2023.
* Meng et al. (2024) Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. _arXiv preprint arXiv:2405.14734_, 2024.
* Meng et al. (2020)* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* Palan and Schitter (2018) Stefan Palan and Christian Schitter. Prolific. ac--a subject pool for online experiments. _Journal of behavioral and experimental finance_, 17:22-27, 2018.
* Pandey et al. (2022) Rahul Pandey, Hemant Purohit, Carlos Castillo, and Valerie L Shalin. Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-the-loop machine learning. _International Journal of Human-Computer Studies_, 160:102772, 2022.
* Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* Saha et al. (2023) Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. _arXiv preprint arXiv:2310.15123_, 2023.
* Santacroce et al. (2023) Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. Efficient rlhf: Reducing the memory usage of ppo. _arXiv preprint arXiv:2309.00754_, 2023.
* Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. _arXiv preprint arXiv:2206.05802_, 2022.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Sedgwick (2012) Philip Sedgwick. Pearson's correlation coefficient. _Bmj_, 345, 2012.
* Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Wabersich et al. (2023) Kim P Wabersich, Andrew J Taylor, Jason J Choi, Koushil Sreenath, Claire J Tomlin, Aaron D Ames, and Melanie N Zeilinger. Data-Driven Safety Filters: Hamilton-Jacobi Reachability, Control Barrier Functions, and Predictive Methods for Uncertain Systems. _IEEE Control Systems Magazine_, 43(5):137-177, 2023.
* Wang et al. (2023) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. _arXiv preprint arXiv:2309.11235_, 2023.
* Wang et al. (2024) Justin Wang, Haimin Hu, Duy Phuong Nguyen, and Jaime Fernandez Fisac. MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety. In _Algorithmic Foundations of Robotics XVI_, 2024.
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
* Xia et al. (2024) Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. _arXiv preprint arXiv:2402.04333_, 2024.

* Zhao et al. (2023) Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. _arXiv preprint arXiv:2305.10425_, 2023.
* Zhao et al. (2016) Zhibing Zhao, Peter Piech, and Lirong Xia. Learning mixtures of plackett-luce models. In _International Conference on Machine Learning_, pp. 2906-2914. PMLR, 2016.
* Zheng et al. (2023) Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. _arXiv preprint arXiv:2307.04964_, 2023.
* Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.

Background and preliminaries

**Human Decision-Making under Uncertainty.** We consider a decision problem faced by a human entity (e.g., an individual, group, or institution) under predictive uncertainty and imperfect observations. We can model such a problem as a partially observable Markov decision process (POMDP) defined by a tuple \(\mathcal{P}^{H}=(\mathcal{S},\mathcal{A}^{H},\mathcal{O}^{H},\mathcal{T},O^{H},P _{0},r,\gamma,\theta^{H})\), where \(\mathcal{S}\) is the set of relevant world states, \(\mathcal{A}^{H}\) is the set of available actions, \(\mathcal{O}^{H}\) is the human's observation space, \(\mathcal{T}:\mathcal{S}\times\mathcal{A}^{H}\rightarrow\Delta(\mathcal{S})\) is the stochastic transition kernel, \(O^{H}:\mathcal{S}\rightarrow\Delta(\mathcal{O}^{H})\) is the human's observation map, \(P_{0}\in\Delta(\mathcal{S})\) is the initial state distribution, \(r:\mathcal{S}\times\mathcal{A}^{H}\times\Theta^{H}\rightarrow\mathbb{R}\) is the reward function, \(\gamma\in(0,1)\) is the time discount factor, and \(\theta^{H}\in\Theta^{H}\) describes the human's intrinsic preferences. Due to partial observability of the world state \(s\in\mathcal{S}\), the human may maintain an _internal state_\(z^{H}\in\mathcal{Z}^{H}\) (e.g., a belief \(b^{H}\in\Delta(\mathcal{S})\) encoding the human's uncertain knowledge of the world state, although \(z^{H}\) may be thought of as a more general variable that could encode features such as the human's emotional state or attention focus). The human may be modeled as taking actions according to a stochastic policy \(\pi^{H}:\mathcal{Z}^{H}\rightarrow\Delta(\mathcal{A}^{H})\).

**AI-Assisted Human Decision-Making.** When the human consults an AI system (e.g., a FM) to help with their decision problem, we may augment the above problem with the human-AI interaction. The resulting _Assisted POMDP_ is a tuple \(\mathcal{P}^{H}_{=}=(\mathcal{S},\mathcal{A}^{H}\times\mathcal{A}^{H}_{=}, \mathcal{A}^{H}_{=},\mathcal{C}^{H},\mathcal{O}^{H},\mathcal{T},O^{H},O^{ \mathcal{U}^{H}},P_{0},r,\gamma,\theta^{H})\), where \(\mathcal{A}^{H}_{=}\) and \(\mathcal{A}^{H}_{=}\) are the sets of interactive actions available to the human and AI system, \(\mathcal{O}^{\mathcal{U}}\) is the AI's observation space, and \(O^{\mathcal{U}}\) is the AI's observation map \(O^{\mathcal{U}}:\mathcal{S}\rightarrow\Delta(\mathcal{O}^{\mathcal{U}})\). In this model, the AI takes an _advisory_ role: it can respond to a human's interactive action \(a^{H}_{=}\in\mathcal{A}^{H}_{=}\) (e.g., a query through a chat interface) with its own \(\alpha^{AI}_{=}\in\mathcal{A}^{\mathcal{U}}_{=}\) (e.g., a generated text or multimedia output). After one or multiple rounds of such interactions, the human may take a physical action \(a^{H}\in\mathcal{A}^{H}\) to affect the evolution of the world state \(s\). This Assisted POMDP is a special case of a partially observable stochastic game (POSG) (Hansen et al., 2004). In such interactions, the AI's goal is to _influence_ the human's internal state \(z^{H}\) towards maximizing the rewards \(r(s,a^{H};\theta)\) accrued over time. This, however, is made challenging by the AI's fundamental uncertainty about the human's preferences \(\theta^{H}\).

**Reinforcement Learning from Human Feedback (RLHF).** RLHF aims to learn the human's preferences \(\theta^{H}\) from human feedback data, which typically involves three key steps. In **Step 1**, the human is asked to provide feedback on some _state sequences_\(\mathbf{s}=(s_{0},s_{1},\ldots,s_{T})\) (e.g., a human-AI dialogue), with \(s_{t}\in\mathcal{S},\ \forall t=0,1,\ldots,T\). For example, in binary comparison (Christiano et al., 2017), assuming human is a Boltzmann rational decision maker (Luce, 1959), the probability that the human prefers \(\mathbf{s}\) over \(\mathbf{s}^{\prime}\) is \(P_{T}^{\prime}(\mathbf{s}\succ\mathbf{s}^{\prime})=\sigma(\beta(R_{T}(\mathbf{ s})-R_{T}(\mathbf{s}^{\prime})))\), where \(\sigma(\cdot)\) is the sigmoid function, \(\beta>0\) is the inverse temperature parameter, and \(R_{T}(\mathbf{s})=\sum_{t=0}^{T}\gamma^{t}r(s_{t})\) is the _return_ received by state sequence \(\mathbf{s}\). **Step 2** is to fit a reward function \(\hat{r}\) based on a dataset containing state sequences paired with human feedback, aiming for \(\hat{r}\) to resemble \(r\) as closely as possible. **Step 3** is to compute an _AI policy_\(\hat{\pi}:\mathcal{S}\rightarrow\Delta(\mathcal{A}^{H}_{=})\) that maximizes the return based on the estimated reward \(\hat{r}\), i.e., \(\hat{\pi}=\operatorname*{arg\,max}_{\tau}U_{T}(\pi)\), where \(U_{T}(\pi):=\mathbb{E}_{\mathbf{s}\sim p^{\pi}}[\hat{R}_{T}(\mathbf{s})]\) is the _expected utility_ of \(\pi\), and \(p^{\pi}\) is the on-policy distribution of state sequence \(\mathbf{s}\) under \(P_{0}\), \(\mathcal{T}\), and \(\pi\). Due to the lack of an analytical model for \(\mathcal{T}\) and the high-dimensional nature of aligning modern AI models, reinforcement learning (RL) is often used to approximately optimize the policy at scale. Recent studies have revealed that RLHF can lead to misalignment when the human gives feedback based on _partial observations_\(\mathbf{o}^{H}=(\mathcal{O}^{H}_{0},\mathcal{P}^{H}_{1},\ldots,\mathcal{O}^{H}_{T})\) rather than the previously assumed--but rarely realistic--full state sequences, resulting in deceptive or manipulative AI behaviors (Casper et al., 2023; Lang et al., 2024). We argue that RLHF misalignment more generally emerges in settings with significant human uncertainty, whether perceptual, predictive, or a combination of the two.

## Appendix B Alignment Algorithm: RL from Hindsight Simulation

To address the misalignment caused by human uncertainty in RLHF, we introduce RLHS. The key idea is that by providing humans with future outcome information, the learned reward and corresponding policy will be substantially better aligned.

### Providing Hindsight Mitigates Misalignment

Given a predictive model of the human, the AI's decision problem in the Assisted POMDP game \(\mathcal{P}^{H}_{=}\) in Appendix A can be reformulated as a POMDP \(\mathcal{P}^{M}_{=}=(\bar{\mathcal{S}},\mathcal{A}^{H}_{=},\bar{\mathcal{O}}^{ \mathcal{A}},\bar{\mathcal{T}},\bar{O}^{\mathcal{A}},\bar{P}_{0},\bar{r}, \gamma)\), where \(\bar{\mathcal{S}}=\mathcal{S}\times\Theta^{H}\times\mathcal{Z}^{H}\), \(\bar{\mathcal{O}}^{Al}=\mathcal{O}^{\mathcal{U}}\times\mathcal{A}^{H}_{=}\), \(\bar{\mathcal{T}}=(\mathcal{T},\mathcal{T}_{\theta},\mathcal{T}^{H})\), \(\bar{P}_{0}\in\Delta(\bar{\mathcal{S}})\), and \(\bar{r}(s,z^{H},\theta^{H})=\mathbb{E}_{a^{H}\sim\pi^{H}(\cdot|z^{H})}r(s,a^{ H};\theta^{H})\). Here, \(\mathcal{T}^{H}:\mathcal{Z}^{H}\times\mathcal{A}^{Al}_{=}\to\mathcal{Z}^{H}\) is the transition kernel of the human's internal state, modeling how the human's knowledge about the world state is evolved based on new observations and interactions with the AI; we treat \(\theta^{H}\) as a constant for the purposes of this paper, with \(\mathcal{T}_{\theta}\) as the identity map. Finally, \(\pi^{H}:\mathcal{Z}^{H}\to\Delta(\bar{\mathcal{A}}^{H})\), with \(\bar{\mathcal{A}}^{H}:=\mathcal{A}^{H}\times\mathcal{A}^{H}_{=}\). In practice the human model can be a black box (e.g., a web-trained FM).

Due to the complexity of POMDP \(\mathcal{P}^{\mathcal{M}}_{=}\), we aim to solve it approximately using RL with _hindsight_ feedback provided by the evaluator. In the following, we show theoretically that providing human evaluators with hindsight during RLHF generally reduces misalignment and improves utility.

Since the human's utility is inherited from their original decision problem \(\mathcal{P}^{H}\), the expected utility generated by an AI policy \(\pi^{AI}\) is the expected return achieved by the human's course of action. For the purposes of RLHF, we can assume that the human begins taking physical actions after the interaction:

\[U^{H}(\pi^{\mathcal{M}}):=\underset{a^{H}_{=},\sim\pi^{H}(\cdot|z^{H}_{x})}{ \mathbb{E}}\underset{\bar{a}_{=},\sim\pi^{H}(\cdot|z^{H}_{x})}{\mathbb{E}} \underset{\bar{a}_{=},\sim\pi^{H}(\cdot|z_{x},\bar{a}_{=}^{H})}{\mathbb{E}} \left[\sum_{t=T+1}^{\infty}\gamma^{t-T}r(s_{t},a^{H}_{t};\theta^{H})\right],\] (1)

where \(t=0,1,\ldots,T\) is the human-AI interaction phase and \(t=T+1,T+2,\ldots\) is the human acting phase, of which the first \(N\) steps are provided as hindsight information during the RLHF evaluation. The following lemma establishes that, for any two policies \(\pi^{H},\bar{\pi}^{H}\), the difference in finite-hindsight utility estimation becomes an exponentially accurate estimate of the difference in true utility as the hindsight horizon \(N\) increases.

**Lemma 1**.: _Let the finite hindsight utility estimate \(U^{H}_{N}(\pi^{AI})\) be the \(N\)-step truncation of the expected utility sum in equation 1, and let the reward function \(r\) be bounded by \(\underline{r}\leq r(s,a^{H})\leq\bar{r}\) for all \(s\in\mathcal{S}\), \(a^{H}\in\mathcal{A}^{H}\), and \(\theta^{H}\in\Theta^{H}\). Then, for any two policies \(\pi^{H},\bar{\pi}^{H}\),_

\[U^{H}_{N}(\pi^{\mathcal{M}})-U^{H}_{N}(\bar{\pi}^{\mathcal{M}})\in\mathcal{B} \Big{(}U^{H}(\pi^{\mathcal{M}})-U^{H}(\bar{\pi}^{\mathcal{M}}),\frac{\gamma^{ N+1}(\bar{r}-\underline{r})}{1-\gamma}\Big{)}\,.\]

Proof.: The lemma follows directly from bounding the tail of the series from term \(T+N+1\). 

Applying the same logic of this lemma to individual executions and assuming a Boltzmann-rational evaluator like the one discussed in Appendix A (and often considered for theoretical purposes when analyzing RLHF methods), we obtain the following result.

**Theorem 1**.: _Suppose the human evaluator is presented a finite-horizon hindsight of \(N\) steps and makes Boltzmann-rational binary preference choices with inverse temperature \(\beta\). Then the probability that the human prefers a hindsight observation \(\mathbf{o}_{0:T+N}\) over another \(\bar{\mathbf{o}}_{0:T+N}\) from the same initial information state \(P(\mathbf{o}_{0:T+N}\succ\bar{\mathbf{o}}_{0:T+N})\) is within the range_

\[\sigma\left(\beta\Big{(}R_{T}(\mathbf{o}_{0:T+N})-R_{T}(\bar{\mathbf{o}}_{0:T+N })\pm\frac{\gamma^{N+1}(\bar{r}-\underline{r})}{1-\gamma}\Big{)}\right),\]

This ensures that, for a sufficiently large hindsight horizon, the hindsight feedback of a Boltzmann-rational human evaluator can be made arbitrarily close--in probability--to the ideal infinite-horizon oracle feedback. We view this as providing theoretical support for the empirically observed value of hindsight with respect to default RLHF (which corresponds to the degenerate case \(N=0\)).

## Appendix C Related Work

**Reinforcement Learning from Human Feedback.** RLHF is widely used for training language models to align with human preferences and values (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). The classical RLHF pipeline typically involves three stages: supervised fine-tuning (Chen et al., 2023; Taori et al., 2023; Wang et al., 2023; Xia et al., 2024) reward modeling (Gao et al., 2023; Luo et al., 2023; Chen et al., 2024; Lightman et al., 2023; Lambert et al., 2024), and policy optimization (Schulman et al., 2017). PPO (Schulman et al., 2017) is commonly used in the policy optimization phase. However, due to the complexity and optimization challengesof online preference optimization algorithms (Zheng et al., 2023; Santacroce et al., 2023), researchers have been exploring more efficient and simpler offline alternatives without learning the reward model (Rafailov et al., 2024; Meng et al., 2024; Ethayarajh et al., 2024; Zhao et al., 2023). Our approach using hindsight simulation can be applied to both online PPO and offline (DPO) learning algorithms.

**Reinforcement Learning from AI Feedback.** Constitutional AI (Bai et al., 2022b) uses an LLM to provide feedback and refine responses, producing data used to train a fixed reward model. This reward model is then applied in reinforcement learning, a process referred to as RLAIF. The technique of using LLM-as-a-Judge has become a standard method for evaluating model outputs (Dubois et al., 2024; Li et al., 2023; Fernandes et al., 2023; Bai et al., 2024; Saha et al., 2023) and curating data to train reward model (Lee et al., 2023; Chen et al., 2023; Li et al., 2023a). Recent studies have shown that RLAIF performs similarly to RLAIF (Lee et al., 2023). Our approach also utilizes LLMs to provide feedback and uses the preference data to fine-tune our model.

**Challenges of Learning from Human Feedback.** Learning from human feedback presents challenges (Casper et al., 2023). Human evaluators are imperfect (Saunders et al., 2022; Gudibande et al., 2023), making mistakes due to limited time (Chmielewski and Kucker, 2020) or cognitive biases (Pandey et al., 2022). Evaluators may also have conflicting preferences (Bakker et al., 2022). Modeling human preferences is difficult (Zhao et al., 2016; Hong et al., 2022; Lindner and El-Assady, 2022), with models being prone to overoptimization (Gao et al., 2023). Recent studies have found that humans and AI preference models sometimes favor sycophantic responses over truthful ones, leading to misalignment. Due to potential catastrophic consequences caused by a misaligned model in high-stakes applications, learning AI models from human feedback has been increasingly studied as a _safety_ problem (Casper et al., 2023; Bajcsy and Fisac, 2024), for which much inspiration can be drawn from the recent success in guaranteeing safety for embodied AI systems such as robotics through the lens of safety filters (Hsu et al., 2023; Wabersich et al., 2023; Hu et al., 2023; Wang et al., 2024). Our approach can be used within a safety filter framework that monitors and intervenes with a potentially misaligned policy to enable trustworthy human-AI interaction.

## Appendix D Training algorithms.

The initial stage of alignment involves Supervised Fine-Tuning (SFT), where the pre-trained model is adapted to mimic high-quality demonstration data, such as dialogues and summaries. To enhance alignment of the SFT model \(\pi_{\theta}\) with human preferences, previous studies (Ziegler et al., 2019; Ouyang et al., 2022) have implemented the Reinforcement Learning from Human Feedback (RLHF) technique. This approach optimizes the following objective:

\[J_{r}(\pi_{\theta})=\mathbb{E}_{\mathbf{x}\sim p_{\text{fin}},\mathbf{y}\sim \pi_{\theta}}\left[r(\mathbf{x},\mathbf{y})-\beta\log\frac{\pi_{\theta}( \mathbf{y}|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})}\right],\] (2)

where \(r(\mathbf{x},\mathbf{y})\) is the reward function reflecting human preferences, \(\pi_{\theta}\) is a policy model, and \(\pi_{\text{ref}}\) is a reference policy used for regularizing \(\pi_{\theta}\) with Kullback-Leibler divergence. The term \(\beta\) is a regularization parameter to control the degree of regularization.

**Online preference optimization.** When the reward \(r\) is unknown, a reward model \(r_{\phi}\) is derived from human-labeled data. This dataset consists of pairs \((x,y_{w},y_{l})\), with \(y_{w}\) and \(y_{l}\) designated as the preferred and less preferred responses by human evaluators respectively. The preference likelihood, as per the Bradley-Terry model (Bradley and Terry, 1952), is given by:

\[\mathbb{P}(y_{w}>y_{l}\mid x)=\frac{\exp(r_{\phi}(x,y_{w}))}{\exp(r_{\phi}(x,y_ {w}))+\exp(r_{\phi}(x,y_{l}))}\]

To optimize \(r_{\phi}\), we minimize the negative log-likelihood of this model:

\[L_{R}(r_{\phi})=-\mathbb{E}_{(x,y_{w},y)\sim D}\left[\log\sigma\left(r_{\phi}( x,y_{w})-r_{\phi}(x,y_{l})\right)\right]\]

Once \(r_{\phi}\) is fine-tuned, it substitutes the initial reward function \(r\) and is integrated directly into the reinforcement learning framework, enhancing the model's performance through explicit optimization via Proximal Policy Optimization (PPO) (Schulman et al., 2017):

\[\max_{\pi_{\theta}}\mathbb{E}_{(x,y)\sim p_{\nu}}\left[r_{\phi}(x,y)-\beta D_ {KL}(\pi_{\theta}(y\mid x)\|\pi_{\text{ref}}(y\mid x))\right]\]

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

**Analysis:** We provided additional experimental results on Llama-3-8b using PPO and DPO in Fig. 7 and Fig. 8. The results further justifies our claim on misalignment and the effectiveness of hindsight to mitigate the misalignment. We also provided the Likert scale satisfaction ratings for both Llama-2-7b and Llama-3-8b in Fig. 9 and Fig. 10 and conducted additional analysis of the distribution of the ratings in Fig. 11. We observed that models trained with immediate feedback achieve very high satisfaction ratings (predominantly 5), as illustrated in the histogram in Fig. (a)a. However, this comes at the expense of true utility (-0.71), which remains negative and underscores the misalignment issue between satisfaction and true utility. Training with hindsight feedback still maintains a high satisfaction rating while significantly improving true utility, achieving positive values (+0.18), as shown in Fig. (b)b. This indicates that partial hindsight mitigates the misalignment, resulting in more truthful model performance.

**Comparison between online and offline fine-tuning.** We ran both t-test and two-way ANOVA to better understand emergent misalignment and the effectiveness of mitigation through hindsight simulation under online and offline fine-tuning schemes. Results show that PPO with immediate feedback yields significantly lower true utility for the user than DPO (\(p=1.1\times 10^{-4}\) in t-test). In addition, considering the difference between the (normalized) user rating and true utility, we find that _immediate feedback in online RLHF using PPO introduces a larger misalignment gap than offline RLHF using DPO_ (\(p=6.7\times 10^{-5}\) in t-test). Incorporating partial hindsight helps mitigate this misalignment gap across online and offline fine-tuning (\(p=3.1\times 10^{-116}\) in two-way ANOVA test). We also compared online PPO with offline SimPO (Meng et al., 2024) and found that PPO introduces a larger misalignment gap than SimPO (\(p=8.2\times 10^{-5}\) in t-test), with partial hindsight significantly reducing misalignment in SimPO as well (\(p=5\times 10^{-56}\) in t-test).

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Metric**} & \multicolumn{2}{c}{**DPO**} & \multicolumn{2}{c}{**PPO**} & \multicolumn{2}{c}{**SimPO**} \\ \cline{2-7}  & IF & PHS & IF & PHS & IF & PHS \\ \hline Rating \(\uparrow\) & \(0.95_{\pm 0.028}\) & \(0.35_{\pm 0.032}\) & \(0.97_{\pm 0.021}\) & \(0.41_{\pm 0.026}\) & \(0.94_{\pm 0.032}\) & \(0.37_{\pm 0.028}\) \\ True Utility \(\uparrow\) & \(-0.51_{\pm 0.03}\) & \(0.18_{\pm 0.023}\) & \(-0.71_{\pm 0.029}\) & \(0.18_{\pm 0.025}\) & \(-0.49_{\pm 0.044}\) & \(0.16_{\pm 0.032}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance comparison of DPO, PPO, and SimPO models under Immediate Feedback (IF) and Partial Hindsight Simulation (PHS). Average satisfaction ratings and true utility (with standard deviations) are shown. SimPO results are included for comparison between online (PPO) and offline (DPO, SimPO) RLHF approaches.

Figure 11: **Histograms of Likert ratings for Llama-2-7b trained with PPO using immediate feedback (a) and partial hindsight (b). The model trained with immediate feedback achieves high ratings (predominantly 5), but has a negative true utility (-0.71), indicating significant misalignment. In contrast, the model trained with partial hindsight maintains high ratings while achieving high true utility (+0.18), demonstrating better alignment between user ratings and true utility.**

[MISSING_PAGE_EMPTY:20]

Figure 13: **Qualitative results for Llama-3-8b trained with DPO using immediate feedback versus partial hindsight**. The model trained with immediate feedback falsely claims that Option C can play 3D movies, which is incorrect. In contrast, the model trained with partial hindsight accurately states that Option Câ€™s 3D capability is not specified, and recommends Option B, the cheapest option that includes 3D capability.

Figure 14: **Failure case for Llama-2-7b trained with DPO using partial hindsight**. The model trained with immediate feedback deceives about specific features, while the model trained with partial hindsight withholds some information. This reveals shortcomings of partial hindsight, as it does not have observations for all other items. Consequently, it might still encourage the agent to deceive about the price or conceal price information.