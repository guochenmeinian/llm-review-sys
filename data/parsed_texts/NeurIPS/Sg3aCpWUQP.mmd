# Errors-in-variables Frechet Regression

with Low-rank Covariate Approximation

 Dogyoon Song

Department of EECS

University of Michigan

Ann Arbor, MI 48109, USA

dogyoons@umich.edu

Equal contribution. To whom correspondence should be addressed: hankh@uic.edu

&Kyunghee Han

Department of Math, Stat and Comp Sci

University of Illinois at Chicago

Chicago, IL 60607, USA

hankh@uic.edu

Equal contribution. To whom correspondence should be addressed: hankh@uic.edu

###### Abstract

Frechet regression has emerged as a promising approach for regression analysis involving non-Euclidean response variables. However, its practical applicability has been hindered by its reliance on ideal scenarios with abundant and noiseless covariate data. In this paper, we present a novel estimation method that tackles these limitations by leveraging the low-rank structure inherent in the covariate matrix. Our proposed framework combines the concepts of global Frechet regression and principal component regression, aiming to improve the efficiency and accuracy of the regression estimator. By incorporating the low-rank structure, our method enables more effective modeling and estimation, particularly in high-dimensional and errors-in-variables regression settings. We provide a theoretical analysis of the proposed estimator's large-sample properties, including a comprehensive rate analysis of bias, variance, and additional variations due to measurement errors. Furthermore, our numerical experiments provide empirical evidence that supports the theoretical findings, demonstrating the superior performance of our approach. Overall, this work introduces a promising framework for regression analysis of non-Euclidean variables, effectively addressing the challenges associated with limited and noisy covariate data, with potential applications in diverse fields.

## 1 Introduction

Regression analysis is a fundamental statistical methodology to model the relationship between response variables and explanatory variables (covariates). Linear regression, for example, models the (conditional) expected value of the response variable as a linear function of covariates. Regression models enable researchers and analysts to make predictions, gain insights into how input variables influence the outcomes of interest, and validate hypothetical associations between variables in inferential studies. As a result, regression is widely utilized across various scientific domains, including economics, psychology, biology, and engineering [53, 21, 29].

In recent decades, there has been a growing interest in developing statistical methods capable of handling random objects in non-Euclidean spaces. Examples of these include functional data analysis [41], statistical manifold learning [31], statistical network analysis [34], and object-oriented data analysis [39]. In such contexts, the response variable is defined in a metric space that may lack an algebraic structure, making it challenging to apply global, parametric approaches toward regression as in the classical Euclidean setting. To overcome this challenge, (global) Frechet regression, which models the relationship by fitting the (conditional) barycenters of the responses as a function of covariates, has been introduced [40]. Notably, when the Euclidean metric is considered, Frechetregression recovers classical Euclidean regression models. For more details on Frechet regression and its recent developments, we refer readers to [30; 40; 22; 45; 27].

Nevertheless, most existing research on Frechet regression has focused on ideal scenarios characterized by abundant covariate data that are accurately measured and free of noise. In practical applications, however, high-dimensional data often arise, which are also susceptible to measurement errors and other forms of contamination. These errors can stem from various sources, such as unreliable data collection methods (_e.g._, low-resolution probes, subjective self-reports) or imperfect data storage and transmission. The high-dimensionality and the presence of measurement errors in covariates pose critical challenges for statistical inference, as regression analysis based on error-prone covariates may result in incorrect associations between variables, yielding misleading conclusions.

To address these limitations, it is crucial to extend the methodology and analysis of Frechet regression to tackle high-dimensional errors-in-variables problems. In this work, we aim to leverage the low-rank structure in the covariates to enhance the estimation accuracy and computational efficiency of Frechet regression. Specifically, we explore the extension of principal component regression to handle errors-in-variables regression problems with non-Euclidean response variables.

### Contributions

This paper contributes to advancing the (global) Frechet regression of non-Euclidean response variables, with a particular focus on high-dimensional, errors-in-variables regression.

Firstly, we propose a novel framework, called the _regularized (global) Frechet regression_ (Section 3) that combines the ideas from Frechet regression [40] and the principal component regression [32]. This framework effectively utilizes the low-rank structure in the matrix of (Euclidean) covariates by extracting its principal components via low-rank matrix approximation. Our proposed method is straightforward to implement, not requiring any knowledge about the error-generating mechanism.

Furthermore, we provide a comprehensive theoretical analysis (Section 4) in three main theorems to establish the effectiveness of the proposed framework. Firstly, we prove the consistency of the proposed estimator for the true global Frechet regression model (Theorem 1). Secondly, we investigate the convergence rate of the estimator's bias and variance (Theorem 2). Lastly, we derive an upper bound for the distance between the estimates obtained using error-free covariates and those with errors-in-variables covariates (Theorem 3). Collectively, these results demonstrate that our approach effectively addresses model mis-specification and achieves more efficient model estimation by leveraging the low-rank structure of covariates, despite the presence of inherent bias due to unobserved measurement errors.

To validate our theoretical findings, we conduct numerical experiments on synthetic datasets (Section 5). We observe that the proposed method provides more accurate estimates of the regression parameters, especially in high-dimensional settings. Our experimental results emphasize the importance of incorporating the low-rank structure of covariates in Frechet regression, and provide empirical evidence that aligns with our theoretical analysis.

### Related work

**Metric-space-valued variables.** Nonparametric regression models for Riemannian-manifold-valued responses were proposed as a generalization of regression for multivariate outputs by Steinke _et al._[48; 49]. These works provided a foundation for recent developments in regression analysis of non-Euclidean responses. Later, Hein [30] proposed a Nadaraya-Watson-type kernel estimation of regression model for general metric-space-valued outcomes. Since then, statistical properties of regression models for some special classes of metric-space-valued outcomes, such as distribution functions [23; 52; 28] and matrix-valued responses [57; 20], have been investigated. Recently, many researchers have introduced further advances in Frechet regression, including [40; 10; 37; 45]. In this study, we use the global Frechet regression proposed in [40] as the basis for our proposed method.

**Errors-in-variables regression.** Much of earlier work on errors-in-variables (EIV) problems in the statistical literature can be found in [13], which covers the simulation-extrapolation (SIMEX) [16; 11], the attenuation correction method [36], covariate-adjusted model [46; 19], and the deconvolution kernel method [25; 24; 18]. The regression calibration method [47], instrumental variable modeling [12; 43], and the two-phase study design [9; 4] were also proposed when additional data are available for correcting measurement errors. In the high-dimensional modeling literature, regularization methods for recovering the true covariate structure can also be utilized [38; 7; 17]. Despite a diverse body of literature on high-dimensional learning and robust regression modeling, much of it assumes response spaces to be vector spaces endowed with inner products. In this paper, we tackle EIV problems within the Frechet regression framework. While previous works have explored regression analysis in non-Euclidean metric spaces, addressing EIV issues in this context remains uncharted.

**Principal component regression.** The principal component regression (PCR) [32] is a statistical technique that regresses response variables on principal component scores of the covariate matrix. The conventional PCR selects a few principal components as the "new" regressors associated with the first leading eigenvalues to explain the highest proportion of variations observed in the original covariate matrix. In functional data analysis, PCR is known to have a shrinkage effect on the model estimate and produce robust prediction performance in functional regression [42; 33]. Recently, Agarwal _et al._[2] investigated the robustness of PCR in the presence of measurement errors on covariates and the statistical guarantees for learning a good predictive model. Unlike prior statistical analyses of EIV problems that often assume known or estimable noise distributions, PCR leverages inherent low-rank structures in the covariates without requiring a priori knowledge of measurement error distributions. We adopt PCR as a concrete, practical solution to EIV models in non-Euclidean regression, driven by two compelling considerations. Firstly, the prevalence of (approximate) low-rank structures in real-world datasets enhances the practical relevance of our approach. Secondly, we intentionally opt for an approach with minimal assumptions regarding covariate errors to ensure broad applicability.

### Organization

In Section 2, we introduce the notation used throughout the paper, and overview the global Frechet regression framework. Section 3 presents the problem setup, objectives, and our proposed estimator, which we refer to as the regularized Frechet regression (Definition 4). In Section 4, we discuss theoretical guarantees on the regularized Frechet regression method in accurately estimating the global Frechet regression function. Section 5 presents the results of numerical "proof-of-concept" experiments that support the theoretical findings. Finally, we conclude this paper with discussions in Section 6. Due to space constraints, detailed proofs of the theorems as well as additional details and discussions of experiments are provided in the Appendix.

## 2 Preliminaries

### Notation

Let \(\mathbb{N}\) denote the set of positive integers and \(\mathbb{R}\) denote the set of real numbers. Also, let \(\mathbb{R}_{+}:=\{x\in\mathbb{R}:x\geq 0\}\). For \(n\in\mathbb{N}\), we let \([n]\coloneqq\{1,\ldots,n\}\). We mostly use plain letters to denote scalars, vectors, and random variables, but we also use boldface uppercase letters for matrices, and curly letters to denote sets when useful. Note that we may identify a vector with its column matrix representation. For a matrix \(\bm{X}\), we let \(\bm{X}^{-1}\) denote its inverse (if exists) and \(\bm{X}^{\dagger}\) denote the Moore-Penrose pseudoinverse of \(\bm{X}\). Also, we let \(\operatorname{rowsp}\left(\bm{X}\right)\) and \(\operatorname{colsp}\left(\bm{X}\right)\) denote the row and column spaces of \(\bm{X}\), respectively. Furthermore, we let \(\operatorname{spec}\left(\bm{X}\right)\) denote the set of non-zero singular values of \(\bm{X}\), \(\sigma_{i}(\bm{X})\) denote the \(i\)-th largest singular value of \(\bm{X}\), and \(\sigma^{(\lambda)}(\bm{X})\coloneqq\inf\{\sigma_{i}(\bm{X})>\lambda:i\in \mathbb{N}\}\) with the convention \(\inf\emptyset=\infty\). We let \(\bm{1}_{n}=(1,1,\ldots,1)^{\top}\in\mathbb{R}^{d}\) and let \(\mathds{1}\) denote the indicator function. We let \(\|\cdot\|\) denote a norm, and set \(\|\cdot\|=\|\cdot\|_{2}\) (the \(\ell_{2}\)-norm for vectors, and the spectral norm for matrices) by default unless stated otherwise. For a finite set \(\mathcal{D}\), we may identify \(\mathcal{D}\) with its empirical measure \(\nu_{\mathcal{D}}=\frac{1}{|\mathcal{D}|}\sum_{x\in\mathcal{D}}\delta_{x}\), where \(\delta_{x}\) denotes the Dirac measure supported on \(\{x\}\).

Letting \(f,g:\mathbb{R}\to\mathbb{R}\), we write \(f(x)=O(g(x))\) as \(x\to\infty\) if there exist \(M>0\) and \(x_{0}>0\) such that \(|f(x)|\leq M\cdot g(x)\) for all \(x\geq x_{0}\). Likewise, we write \(f(x)=\Omega(g(x))\) if \(g(x)=O(f(x))\). Furthermore, we write \(f(x)=o(g(x))\) as \(x\to\infty\) if \(\lim_{x\to\infty}\frac{f(x)}{g(x)}=0\). For a sequence of random variables \(X_{n}\), and a sequence \(a_{n}\), we write \(X_{n}=O_{p}(a_{n})\) as \(n\to\infty\) if for any \(\varepsilon>0\), there exists \(M\in\mathbb{R}_{+}\) and \(N\in\mathbb{N}\) such that \(P\big{(}\big{|}\frac{X_{n}}{a_{n}}\big{|}>M\big{)}<\varepsilon\) for all \(n\geq N\). Similarly, we write \(X_{n}=o_{p}(a_{n})\) if \(\lim_{n\to\infty}P\big{(}\big{|}\frac{X_{n}}{a_{n}}\big{|}>\varepsilon\big{)}=0\) for all \(\varepsilon>0\).

### Global Frechet regression

Let \((X,Y)\) be a random variable that has a joint distribution \(P_{X,Y}\) supported on \(\mathbb{R}^{p}\times\mathcal{M}\), where \(\mathbb{R}^{p}\) is the \(p\)-dimensional Euclidean space and \(\mathcal{M}=(\mathcal{M},d)\) is a metric space equipped with a distance function \(d:\mathcal{M}\times\mathcal{M}\to\mathbb{R}\). We write the marginal distribution of \(X\) as \(P_{X}\), and the conditional distribution of \(Y\) given \(X\) as \(P_{Y|X}\).

**Definition 1** (Frechet regression function).: _Let \((X,Y)\) be a random element that takes value in \(\mathbb{R}^{p}\times\mathcal{M}\). The Frechet regression function of \(Y\) on \(X\) is a function \(\varphi^{*}:\mathbb{R}^{p}\to\mathcal{M}\) such that_

\[\varphi^{*}(x)=\operatorname*{arg\,min}_{y\in\mathcal{M}}\mathbb{E}\big{[}d^{2 }(Y,y)\,|\,X=x\big{]},\qquad\forall x\in\operatorname{supp}P_{X}\subseteq \mathbb{R}^{p}.\] (1)

We note that \(\varphi^{*}(x)\) is the best predictor of \(Y\) given \(X=x\), as it minimizes the marginal risk \(\mathbb{E}\big{[}d^{2}(Y,\varphi^{*}(X))\big{]}\) under the squared-distance loss. In the literature, \(\varphi^{*}(x)\) is also known as the conditional Frechet mean of \(Y\) given \(X=x\)[26]. It is important to recognize that the existence and uniqueness of the Frechet regression function are closely tied to the geometric characteristics of \(\mathcal{M}\), and are not guaranteed in general [3, 8]. Nonetheless, extensive research has been conducted on the existence and uniqueness of Frechet means in various metric spaces commonly encountered in practical applications. Examples include the unit circle in \(\mathbb{R}^{2}\)[14], Riemannian manifolds [1, 5], Alexandrov spaces with non-positive curvature [51], metric spaces with upper bounded curvature [58], and Wasserstein space [59, 35].

While modeling and estimating the Frechet regression function \(\varphi^{*}\) is often of interest, its global (parametric) modeling may not be straightforward, especially when \(\mathcal{M}\) lacks a useful algebraic structure, such as an inner product. For instance, in classical linear regression analysis with \(\mathcal{M}=\mathbb{R}\), the conditional distribution of \(Y\) given \(X=x\) is normally distributed with a mean of \(\varphi^{*}(x)=\alpha+\beta^{\top}x\) and a fixed variance \(\sigma^{2}\), where \(\alpha\) and \(\beta\) represent the regression coefficients. Similarly, when \(\mathcal{M}\) possesses a linear-algebraic structure, one can specify a class of regression functions that quantifies the association between the expected outcome and covariates in an additive and multiplicative manner. However, the lack of an algebraic structure in general metric spaces may prevent us from characterizing the barycenter \(\varphi^{*}(x)\) in the same way classical regression analysis determines the expected value of outcomes with changing covariates.

To address this challenge, Petersen and Muller [40] recently proposed to exploit algebraic structures in the space of covariates, \(\mathbb{R}^{p}\), instead of \(\mathcal{M}\). Specifically, they consider a weighted Frechet mean as

\[\varphi(x)=\operatorname*{arg\,min}_{y\in\mathcal{M}}\mathbb{E}\big{[}w(X,x) \cdot d^{2}(Y,y)\big{]},\] (2)

where \(w:\mathbb{R}^{p}\times\mathbb{R}^{p}\to\mathbb{R}\) is a weight function such that \(w(\xi,x)\) denotes the influence of \(\xi\) at \(x\). In particular, Petersen and Muller [40] defined the global Frechet regression function with a specific choice of \(w\) as follows.

**Definition 2** (Global Frechet regression function).: _Let \((X,Y)\) be a random variable in \(\mathbb{R}^{p}\times\mathcal{M}\). Let \(\mu=\mathbb{E}(X)\) and \(\Sigma=\operatorname{Var}(X)\). The global Frechet regression function of \(Y\) on \(X\) is a function \(\varphi_{\mathrm{glo}}:\mathbb{R}^{p}\to\mathcal{M}\) such that_

\[\varphi_{\mathrm{glo}}(x)=\operatorname*{arg\,min}_{y\in\mathcal{M}}\mathbb{E} \big{[}w_{\mathrm{glo}}(X,x)\cdot d^{2}(Y,y)\big{]}\] (3)

_where \(w_{\mathrm{glo}}(X,x)=1+(X-\mu)^{\top}\Sigma^{-1}(x-\mu)\)._

When \(\mathcal{M}\) is an inner product space (_e.g._, \(\mathcal{M}=\mathbb{R}\)), the function \(\varphi_{\mathrm{glo}}\) restores the standard linear regression model representation over the domain \(\mathbb{R}^{p}\). For this reason, \(\varphi_{\mathrm{glo}}\) is commonly referred to as the _global Frechet regression model_ for metric-space-valued outcomes [40, 37, 54].

**Remark on Definition 2** One might wonder why the term "global" is used to describe \(\varphi_{\mathrm{glo}}\) as a Frechet regression function. The use of the adjective "global" serves to emphasize its distinction from "local" nonparametric regression methods that interpolate data points. Notably, when \(\mathcal{M}\) is a Hilbert space, \(\varphi_{\mathrm{glo}}\) reduces to the natural linear models. For instance, if \(\mathcal{M}=\mathbb{R}\), then it follows that \(\varphi_{\mathrm{glo}}(x)=\mathbb{E}\big{[}w_{\mathrm{glo}}(X,x)\cdot Y\big{]}= \alpha+\beta^{\top}(x-\mu)\), where \(\alpha=\mathbb{E}[Y]\) and \(\beta=\Sigma^{-1}\cdot\mathbb{E}\big{[}(X-\mu)\cdot Y\big{]}\). These linear models hold uniformly for the evaluation point \(x\). Similarly, in the case of an \(L^{2}\) space equipped with the squared-distance metric \(d^{2}(y,y^{\prime})=\|y-y^{\prime}\|_{2}^{2}\) induced by the \(L^{2}\) norm, \(\varphi_{\mathrm{glo}}\) represents the linear regression model for functional responses. Thus, \(\varphi_{\mathrm{glo}}\) establishes a globally defined model that spans the entire space.

[MISSING_PAGE_FAIL:5]

When \(\mathcal{D}_{n}=\{(X_{i},Y_{i})\in\mathbb{R}^{p}\times\mathcal{M}:i\in[n]\}\) is an IID sample from \(P_{X,Y}\), the \(\lambda\)-regularized estimator \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}\) subsumes the sample-analogue estimator \(\widehat{\varphi}_{\mathcal{D}_{n}}\) in (4) as a special case where \(\lambda=0\).

**Connection to principal component regression**. Here we remark that when \(\mathcal{M}\) is a Euclidean space, the regularized Frechet regression function \(\varphi_{\nu}^{(\lambda)}\) effectively reduces to the principal component regression. Suppose that \(\mathcal{M}=\mathbb{R}\) and \(\mathcal{D}_{n}=\{(x_{i},y_{i})\in\mathbb{R}^{p}\times\mathbb{R}:i\in[n]\}\) is a given dataset. Then \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x)=\overline{y}+\hat{\beta}_{\lambda}^{ \top}\left(x-\mu_{\mathcal{D}_{n}}\right)\) where \(\overline{y}=\frac{1}{n}\sum_{i=1}^{n}y_{i}\) and \(\hat{\beta}_{\lambda}=[\mathsf{SVT}^{(\lambda)}\big{(}\Sigma_{\mathcal{D}_{ n}}\big{)}]^{\dagger}\cdot\big{[}\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\mu_{ \mathcal{D}_{n}})\cdot(y_{i}-\overline{y})\big{]}\). Observe that \(\hat{\beta}_{\lambda}\) is exactly the regression coefficient of principal component regression applied to the centered dataset \(\mathcal{D}_{n}^{\text{cr}}=\{(x_{i}-\mu_{\mathcal{D}_{n}},y_{i}-\overline{y}) :i\in[n]\}\) using \(k\) principal components with \(k=\max_{a\in[p]}\left\{\sigma_{a}(\Sigma_{\mathcal{D}_{n}^{\text{cr}}})\geq \lambda\right\}\).

## 4 Main results

In this section, we investigate properties of \(\varphi_{\nu}^{(\lambda)}\) for \(\lambda\geq 0\), with a focus on two cases: \(\nu=\mathcal{D}_{n}\) and \(\nu=\widetilde{\mathcal{D}}_{n}\), cf. Section 3.1. By denoting the true distribution that generates \((X,Y)\) as \(\nu^{*}\), we can express \(\varphi_{\text{glo}}\) as \(\varphi_{\nu^{*}}^{(0)}\). To analyze the discrepancy between the estimator \(\varphi_{\nu}^{(\lambda)}(x)\) and \(\varphi_{\text{glo}}(x)\), we examine the relationships depicted in the schematic in Figure 1. Our theoretical findings can be summarized as follows: Even in the presence of covariate noises, \(\varphi_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}\) with a suitable \(\lambda>0\) can effectively eliminate the noise in \(Z\) to estimate \(X\), thereby reducing the error in estimating \(\varphi_{\text{glo}}\).

### Model assumptions and examples

We impose the following assumptions for our analysis.

* (Existence) For any probability distribution \(\nu\) and any \(\lambda\in\mathbb{R}_{+}\), the object \(\varphi_{\nu}^{(\lambda)}(x)\) exists (almost surely) and is unique. In particular, \(\inf_{y\in\mathcal{M}:\,d(y,\varphi_{\text{glo}}(x))>\varepsilon}R(y;x)>R( \varphi_{\text{glo}}(x);x)\) for all \(\varepsilon>0\), where \(R(y;x)\coloneqq R_{\nu^{*}}^{(0)}(y;x)\).
* (Growth) There exist \(D_{\text{g}}>0\), \(C_{\text{g}}>0\) and \(\alpha>1\), possibly depending on \(x\), such that for any probability distribution \(\nu\) and any \(\lambda\in\mathbb{R}_{+}\), \[\begin{cases}d\big{(}y,\varphi_{\nu}^{(\lambda)}(x)\big{)}<D_{\text{g}}& \Longrightarrow\quad R_{\nu}^{(\lambda)}(y;x)-R_{\nu}^{(\lambda)}\big{(}\varphi _{\nu}^{(\lambda)}(x);x\big{)}\geq C_{\text{g}}\cdot d\big{(}y,\varphi_{\nu}^ {(\lambda)}(x)\big{)}^{\alpha},\\ d\big{(}y,\varphi_{\nu}^{(\lambda)}(x)\big{)}\geq D_{\text{g}}&\Longrightarrow \quad R_{\nu}^{(\lambda)}(y;x)-R_{\nu}^{(\lambda)}\big{(}\varphi_{\nu}^{( \lambda)}(x);x\big{)}\geq C_{\text{g}}\cdot D_{\text{g}}^{\alpha}.\end{cases}\] (10)
* (Bounded entropy) There exists \(C_{\text{e}}>0\), possibly depending on \(y\), such that \[\limsup_{\delta\to 0}\int_{0}^{1}\sqrt{1+\log\mathfrak{N}\big{(}B_{d} \big{(}y,\delta\big{)},\delta\varepsilon\big{)}}\,\mathrm{d}\varepsilon\leq C_ {\text{e}},\] (11) where \(B_{d}(y,\delta)\coloneqq\{y^{\prime}\in\mathcal{M}:d(y,y^{\prime})\leq\delta\}\) and \(\mathfrak{N}(S,\varepsilon)\) is the \(\varepsilon\)-covering number2 of \(S\).

Figure 1: A schematic for the relationship between the regularized Fr√©chet regression estimators.

Assumption (C0) is common to establish the consistency of an M-estimator [55, Chapter 3.2]; in particular, it ensures the weak convergence of the empirical process \(R_{\mathcal{D}_{n}}^{(\lambda)}\) to the population process \(R_{\nu^{*}}^{(\lambda)}\) implying convergence of their minimizers. Furthermore, the conditions on the curvature (C1) and the covering number (C2) control the behavior of the objectives near the minimum in order to obtain rates of convergence; it is worth mentioning that (C2) corresponds to a (locally) bounded entropy for every \(y\in\mathcal{M}\), while (P1) in [40] requires the same condition only with \(y=\varphi_{\mathrm{glo}}(x)\). These conditions arise from empirical process theory and are also commonly adopted [40, 44, 45].

Here we provide several examples of the space \(\mathcal{M}\), in which the conditions (C0), (C1) and (C2) are satisfied. We verify the conditions in Appendix A; see Propositions 1, 2, 3, and 4.

**Example 1**.: _Let \(\mathcal{M}=(\mathcal{H},d_{\mathrm{HS}})\) be a finite-dimensional Hilbert space \(\mathcal{H}\) equipped with the Hilbert-Schmidt metric \(d_{\mathrm{HS}}(y_{1},y_{2})=\left\langle y_{1}-y_{2},y_{1}-y_{2}\right\rangle^ {1/2}\), e.g., \(\mathcal{M}=(\mathbb{R}^{r},d_{2})\) where \(d_{2}\) is the \(\ell^{2}\)-metric._

**Example 2**.: _Let \(\mathcal{M}\) be \(\mathcal{W}\), the set of probability distributions \(G\) on \(\mathbb{R}\) such that \(\int_{\mathbb{R}}x^{2}\,dG(x)<\infty\), equipped with the Wasserstein metric \(d_{W}\) defined as_

\[d_{W}(G_{1},G_{2})^{2}=\int_{0}^{1}\left(G_{1}^{-1}(t)-G_{2}^{-1}(t)\right)^{2 }\,dt,\]

_where \(G_{1}^{-1}\) and \(G_{2}^{-1}\) are the quantile functions of \(G_{1}\) and \(G_{2}\), respectively. See [40, Section 6]._

**Example 3**.: _Let \(\mathcal{M}=\left\{M\in\mathbb{R}^{r\times r}:M=M^{T},M\succeq 0\text{ and }M_{ii}=1,\forall i\in[r]\right\}\) be the set of correlation matrices of size \(r\), equipped with the Frobenius metric, \(d_{F}(M,M^{\prime})=\|M-M^{\prime}\|_{F}\)._

**Example 4**.: _Let \(\mathcal{M}\) be a (bounded) Riemannian manifold of dimension \(r\), and let \(d_{g}\) be the geodesic distance induced by the Riemannian metric._

### Theorem statements

#### 4.2.1 Noiseless covariate setting

We first verify the consistency of the \(\lambda\)-regularized Frechet regression function as follows.

**Theorem 1** (Consistency).: _Suppose that Assumption (C0) holds. If \(\mathrm{diam}\,(\mathcal{M})<\infty\), then for any \(\lambda\in\mathbb{R}\) such that \(0\leq\lambda<\min\{\sigma_{i}(\Sigma_{\nu^{*}}):\sigma_{i}(\Sigma_{\nu^{*}})>0\}\), and any \(x\in\mathbb{R}^{p}\),_

\[d\big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(\lambda)} (x)\big{)}=o_{P}(1)\qquad\text{ as }\ n\to\infty.\] (12)

If \(\lambda<\sigma^{(0)}(\Sigma_{\nu^{*}})=\min\{\sigma_{i}(\Sigma_{\nu^{*}}): \sigma_{i}(\Sigma_{\nu^{*}})>0\}\), then the regularized estimator \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x)\) effectively reduces to the same as the sample-analog estimator \(\widehat{\varphi}_{\mathcal{D}_{n}}(x)\) in (4) in the limit \(n\to\infty\). Thus, \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x)\) inherits the consistency of \(\widehat{\varphi}_{\mathcal{D}_{n}}\). We provide a detailed proof of Theorem 1 in Appendix B.

In addition to the consistency of \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}\) in the small \(\lambda\) limit, we present an analysis for the convergence rate of \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x)\) in the following theorem.

**Definition 5**.: _The Mahalanobis seminorm of \(x\) induced by a positive semidefinite matrix \(\Sigma\) is \(\|x\|_{\Sigma}\coloneqq\left(x^{\top}\Sigma^{\dagger}x\right)^{1/2}\)._

**Theorem 2** (Rate of convergence).: _Suppose that Assumptions (C0)-(C2) hold. If \(\mathrm{diam}\,(\mathcal{M})<\infty\), then for any \(\lambda\in\mathbb{R}_{+}\) and \(x\in\mathbb{R}^{p}\) such that \(\|x-\mu_{\nu^{*}}\|_{\Sigma_{\nu^{*}}}\leq\frac{C_{\bm{\varepsilon}}D_{\bm{ \varepsilon}}^{\alpha}}{\mathrm{diam}\,(\mathcal{M})^{2\cdot}\sqrt{\mathrm{rank }\,\Sigma_{\nu^{*}}}}\),_

\[d\Big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(0)}(x) \Big{)}=O_{P}\Big{(}\mathfrak{b}_{\lambda}(x)^{\frac{1}{\alpha-1}}+n^{-\frac{1} {2(\alpha-1)}}\Big{)}\quad\text{as }\quad n\to\infty,\] (13)

_where \(\mathfrak{b}_{\lambda}(x)=\mathrm{rank}\left(\Sigma_{\nu^{*}}-\Sigma_{\nu^{*} }^{(\lambda)}\right)^{\frac{1}{2}}\cdot\|x-\mu_{\nu^{*}}\|_{\Sigma_{\nu^{*}}- \Sigma_{\nu^{*}}^{(\lambda)}}\)._

We obtain Theorem 2 by showing a "bias" upper bound \(d\big{(}\varphi_{\nu^{*}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(0)}(x)\big{)}=O \big{(}\mathfrak{b}_{\lambda}(x)^{\frac{1}{\alpha-1}}\big{)}\) and a "variance" bound \(d\big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(\lambda)} (x)\big{)}=O_{P}\big{(}n^{-\frac{1}{2(\alpha-1)}}\big{)}\); see Lemmas 1 and 2 in Appendix C. Here we remark that \(\mathfrak{b}_{\lambda}(x)\) is a monotone non-decreasing function of \(\lambda\), and if \(\lambda<\sigma^{(0)}(\Sigma_{\nu^{*}})\) then \(\mathfrak{b}_{\lambda}(x)=0\). Also, the condition on \(\|x-\mu_{\nu^{*}}\|_{\Sigma_{\nu^{*}}}\) is introduced for a technical reason, and can be removed when \(D_{\mathbf{g}}=\infty\). Note that Condition (C1) holds with \(D_{\mathbf{g}}=\infty\) and \(\alpha=2\) for Examples 1, 2 and 3. Thus, we have \(d\big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(0)}(x)\big{)}= O_{P}\big{(}\mathfrak{b}_{\lambda}(x)+n^{-\frac{1}{2}}\big{)}\) as \(n\to\infty\).

#### 4.2.2 Error-prone covariate setting

Given a set \(\mathcal{D}_{n}=\{(x_{i},y_{i}):i\in[n]\}\), let \(\bm{X}_{\mathcal{D}_{n}}\coloneqq[x_{1}\quad\cdots\quad x_{n}]^{\top}\in\mathbb{ R}^{n\times p}\). We let \(\bm{X}=\bm{X}_{\mathcal{D}_{n}}\) and \(\bm{Z}=\bm{X}_{\widetilde{\mathcal{D}}_{n}}\) for shorthand, and further, we let \(\bm{X}_{\mathrm{ctr}}=\left(\bm{I}_{n}-\frac{1}{n}\bm{1}_{n}\bm{1}_{n}^{\top} \right)\bm{X}\) and \(\bm{Z}_{\mathrm{ctr}}=\left(\bm{I}_{n}-\frac{1}{n}\bm{1}_{n}\bm{1}_{n}^{\top} \right)\bm{Z}\) denote the 'row-centered' matrices.

**Theorem 3** (De-noising covariates).: _Suppose that Assumptions (C0) and (C1) hold. Then there exists a constant \(C>0\) such that for any \(\lambda\in\mathbb{R}_{+}\), if \(x\in\mu_{\mathcal{D}_{n}}+\mathrm{rowsp}\,\bm{X}_{\mathrm{ctr}}\) and_

\[\|x-\mu_{\mathcal{D}_{n}}\|_{\Sigma_{\mathcal{D}_{n}}}\leq\frac{1}{2}\left( \frac{C_{\bm{g}}\cdot D_{\bm{g}}^{\alpha}}{2\,\mathrm{diam}\,(\mathcal{M})} \cdot\frac{\sigma^{(\lambda)}(\bm{X}_{\mathrm{ctr}})\wedge\sigma^{(\lambda)}( \bm{Z}_{\mathrm{ctr}})}{\|\bm{Z}-\bm{X}\|}-1\right),\] (14)

_then_

\[d\left(\varphi_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x),\varphi_{\mathcal{ D}_{n}}^{(\lambda)}(x)\right)\leq C\cdot\left(\frac{\|\bm{Z}-\bm{X}\|}{\sigma^{( \lambda)}(\bm{X}_{\mathrm{ctr}})\wedge\sigma^{(\lambda)}(\bm{Z}_{\mathrm{ctr }})}\cdot\frac{2\cdot\left\|x-\mu_{\mathcal{D}_{n}}\right\|_{\Sigma_{\mathcal{D }_{n}}}+1}{C_{\bm{g}}}\right)^{\frac{1}{\alpha}}.\] (15)

Note that the condition on \(\|x-\mu_{\nu^{*}}\|_{\Sigma_{\nu^{*}}}\) in (14) can be removed when \(D_{\bm{g}}=\infty\). We highlight that the quantity \(\frac{\|\bm{Z}-\bm{X}\|}{\sigma^{(\lambda)}(\bm{X}_{\mathrm{ctr}})\wedge\sigma ^{(\lambda)}(\bm{Z}_{\mathrm{ctr}})}\) acts as the reciprocal of the signal-to-noise ratio. Here, \(\|\bm{Z}-\bm{X}\|\) quantifies the magnitude of the "noise" in the covariates, while \(\min\left\{\sigma^{(\lambda)}(\bm{X}_{\mathrm{ctr}}),\,\sigma^{(\lambda)}(\bm{Z }_{\mathrm{ctr}})\right\}\) measures the strength of the "signal" retained in the \(\lambda\)-SVT of the design matrix. We observe that the error bound (15) increases proportionally to the normalized deviation of \(x\) from the mean, \(\mu_{\mathcal{D}_{n}}\), which is a reasonable outcome. For the complete version of Theorem 3 and its proof, refer to Appendix D.

**Remarks on Theorem 3.** We avoid imposing distributional assumptions on the noise \(\varepsilon=Z-X\), to ensure broad applicability of the result. Also, the inequality (15) is sharp, as there is a worst-case noise instance that attains equality (up to a multiplicative constant). Despite its generality, this upper bound highlights effective error mitigation in specific scenarios. For instance, consider well-balanced, effectively low-rank covariates \(\bm{X}\in\mathbb{R}^{n\times p}\) such that \(|X_{ij}|=\Omega(1)\) for all \(i,j\) and \(\sigma_{1}(\bm{X})\asymp\sigma_{r}(\bm{X})\gg\sigma_{r+1}(\bm{X})\asymp\sigma_ {n\wedge p}(\bm{X})=O(1)\), where \(r\ll n\wedge p\) is the effective rank of \(\bm{X}\). Then \(\sigma_{1}(\bm{X})^{2}\asymp\sigma_{r}(\bm{X})^{2}\asymp\|\bm{X}\|_{r}^{2}/r \gtrsim np/r\). Additionally, if \(\bm{Z}=\bm{X}+\bm{E}\) where \(\bm{E}\) is a random matrix with independent sub-Gaussian rows, then \(\|\bm{Z}-\bm{X}\|\lesssim\sqrt{n}+\sqrt{p}\) with high probability. In the random design scenario where the rows of \(\bm{X}\) and the test point \(x\) are drawn IID from the same distribution, \(\|x-\mu_{D_{n}}\|_{\Sigma}\approx 1\) with high probability. Consequently, the upper bound in (15) is bounded by \(\sqrt{r/p}+\sqrt{r/n}\), which diminishes to 0 when \(r\ll n\wedge p\).

### Proof sketches

**Proof of Theorem 1.** We show that \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)\) weakly converges to \(R_{\nu^{*}}^{(0)}(y;x)\) in the \(\ell^{\infty}(\mathcal{M})\)-sense. According to [55, Theorem 1.5.4], it suffices to show that (1) \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{(0)}(y;x)=o_{p}(1)\) for all \(y\in\mathcal{M}\), and (2) \(R_{\mathcal{D}_{n}}^{(\lambda)}\) is asymptotically equicontinuous in probability.

**Proof of Theorem 2.** We prove upper bounds for the bias and the variance separately.

_To control the bias_ (Appendix C, Lemma 1), we show an upper bound for \(R\big{(}\varphi^{(\lambda)}(x);x\big{)}-R\big{(}\varphi(x);x\big{)}\), and convert it to restrain the distance between the minimizers \(d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}\) using the Growth condition (C1). In this conversion, we employ the "peeling technique" in empirical process theory.

_To control the variance_ (Appendix C, Lemma 2), we follow a similar strategy as in Lemma 1, but with additional technical considerations. Defining the 'fluctuation variable' \(Z_{n}^{(\lambda)}(y;x)\coloneqq R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}} ^{(\lambda)}(y;x)\) parameterized by \(y\in\mathcal{M}\), we derive a probabilistic upper bound for \(R_{\nu^{*}}^{(\lambda)}(\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x);x)-R_{\nu^{*}} ^{(\lambda)}(\varphi_{\nu^{*}}^{(\lambda)}(x);x)\) by establishing a uniform upper bound for \(Z_{n}^{(\lambda)}(y;x)-Z_{n}^{(\lambda)}(\varphi(x);x)\); here, the Entropy condition (C2) is used. Again, we use the Growth condition (C1) and the peeling technique to obtain a probabilistic upper bound for the distance \(d(\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x))\).

**Proof of Theorem 3.** Expressing the difference in the weights \(w_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(y;x)-w_{\mathcal{D}_{n}}^{(\lambda)}(y;x)\) in terms of \(\bm{X}\) and \(\bm{Z}\), we utilize classical matrix perturbation theory to control \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)\), and transform it to an upper bound on the distance \(d(\varphi_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x),\varphi_{\mathcal{D}_{n}}^{( \lambda)}(x))\) using the Growth condition (C1).

Experiments

In this section, we present numerical simulation results to validate and support our theoretical findings. We focus on global Frechet regression analysis for one-dimensional distribution functions (Example 2). These simulations cover various conditions, allowing us to evaluate and compare our methodology's performance with alternative approaches. For a summary of the experimental results, please refer to Figure 2 and Table 1. Further details about simulation settings and additional results are provided in Appendix E.

**Experimental setup.** We consider combinations of \(p\in\{150,300,600\}\) and \(n\in\{100,200,400\}\). The datasets \(\mathcal{D}_{n}=\{(X_{i},Y_{i}):i\in[n]\}\) and \(\widetilde{\mathcal{D}}_{n}=\{(Z_{i},Y_{i}):i\in[n]\}\) are generated as follows. Let \(X_{i}\sim\mathcal{N}_{p}\big{(}\mathbf{0}_{p},\Sigma\big{)}\) be IID multivariate Gaussian with mean \(\mathbf{0}_{p}\) and covariance \(\Sigma\) such that \(\operatorname{spec}\left(\Sigma\right)=\{\kappa_{j}>0:j\in[p]\}\) is an exponentially decreasing sequence such that \(\operatorname{tr}\left(\Sigma\right)=\sum_{j=1}^{p}\kappa_{j}=p\) and \(\kappa_{1}/\kappa_{p}=10^{3}\). Note that \(\sum_{j=1}^{\lfloor p/3\rfloor}\kappa_{j}\big{/}\sum_{j^{\prime}=1}^{p}\kappa _{j^{\prime}}\approx 0.9\), and thus, \(\Sigma\) is effectively low-rank. We generate \(Z_{i}\) following (5) under two scenarios \(\varepsilon_{ij}\overset{ID}{\sim}\mathcal{N}\big{(}0,\sigma_{\varepsilon}^{2} \big{)}\) and \(\varepsilon_{ij}\overset{ID}{\sim}\operatorname{Laplace}\big{(}0,\sigma_{ \varepsilon}\big{)}\), respectively. Lastly, given \(X=x\), let \(Y\) be the distribution function of \(\mathcal{N}\big{(}\mu_{\alpha,\beta}(x)+\eta,\tau^{2}\big{)}\), where (i) \(\mu_{\alpha,\beta}(x)=\alpha+\beta^{\top}x\) with \(\alpha=1\) and \(\beta=p^{-1/2}\cdot\mathbf{1}_{p}\); (ii) \(\eta\sim\mathcal{N}\big{(}0,\sigma_{\eta}^{2}\big{)}\); and (iii) \(\tau^{2}\sim\mathcal{IG}(s_{1},s_{2})\), an inverse gamma distribution with shape \(s_{1}\) and scale \(s_{2}\). We performed \(B=500\) Monte Carlo experiments by drawing \(\mathcal{D}_{n}^{(b)}\) and \(\widetilde{\mathcal{D}}_{n}^{(b)}\) as independent copies of \(\mathcal{D}_{n}\) and \(\widetilde{\mathcal{D}}_{n}\), respectively, for \(b\in[B]\).

**Performance evaluation.** We assess the in-sample and out-of-sample performance of the Frechet regression function estimator by using the mean squared error (MSE) and the mean squared prediction error (MSPE). To this end, we create a "test set" \(\mathcal{D}_{N}^{\text{new}}=\{(X_{i}^{\text{new}},Y_{i}^{\text{new}}):i\in[N]\}\), with \(N=1000\). The MSE and the MSPE are computed as the average of squared metric-distance residuals from the observed responses in the "training set" \(\mathcal{D}_{n}\) and in the "test set" \(\mathcal{D}_{N}^{\text{new}}\)), respectively:

\[\operatorname{MSE}(\varphi_{\nu}^{(\lambda)})=\frac{1}{n}\sum_{i=1}^{n}d_{W} \big{(}Y_{i},\varphi_{\nu}^{(\lambda)}(X_{i})\big{)}^{2}\;\;\text{and}\; \;\operatorname{MSPE}(\varphi_{\nu}^{(\lambda)})=\frac{1}{N}\sum_{i=1}^{N}d_{W }\big{(}Y_{i}^{\text{new}},\varphi_{\nu}^{(\lambda)}(X_{i}^{\text{new}})\big{)} ^{2}.\]

We report the MSE averaged over \(B=500\) random trials: \(\operatorname{MSE}(\varphi_{\nu}^{(\lambda)})=B^{-1}\sum_{b=1}^{B} \operatorname{MSE}(\varphi_{\nu^{(\lambda)}}^{(\lambda)})\), and likewise for MSPE. Furthermore, we evaluate the accuracy and efficiency of the estimator using bias and variance, with detailed definitions deferred to Appendix E.

**Simulation results.** Our numerical study demonstrates that the proposed SVT method consistently improves both estimation and prediction performance, especially in the errors-in-variables setting. Figure 2 highlights how the SVT estimator outperforms the naive errors-in-variables (EIV) estimator, which corresponds to SVT with \(\lambda=0\). The naive EIV suffers from an intrinsic model bias, called the attenuation effect [13], as it regresses responses on error-prone covariates. This leads to a misrepresentation of the association between responses and true covariates, potentially leading to statistical inference based on a mis-specified model.

Remarkably, the SVT estimator achieved a smaller MSPE even compared to the oracle estimator (REF) obtained from the error-free sample. Although the REF estimator had the smallest MSE due to its small bias, we observed its overfitting to the training sample, resulting in poor prediction performance. Notably, even the naive EIV estimator outperformed the REF estimator in MSPE. We believe this is mainly because the true covariate matrix was nearly singular in our simulation setup, causing multicollinearity issues for the REF. In contrast, measurement errors introduced non-ignorable minimum singular values in the EIV covariate matrix, unintentionally mitigating multicollinearity for the naive EIV and causing it to behave like ridge regression.

## 6 Discussion

This paper has addressed errors-in-variables regression of non-Euclidean response variables through the (global) Frechet regression framework enhanced by low-rank approximation of covariates. Specifically, we introduce a novel _regularized (global) Frechet regression_ framework (Section 3), which combines the Frechet regression with principal component regression. We also provide a comprehensive theoretical analysis in three main theorems (Section 4), and validate our theory through numerical experiments on simulated datasets. Moreover, our numerical experiments demonstrate empirical evidence of the effectiveness and superiority of our approach, reinforcing its practical relevance and potential impact in non-Euclidean regression analysis.

We conclude this paper by proposing several promising directions for future research. First, it would be worthwhile to explore the large sample theory for selecting the optimal threshold parameter \(\lambda\) in the proposed SVT method, in order to characterize the theoretical phase transition of the bias-variance trade-off in the regularized (global) Frechet regression. Second, we believe that our framework could be extended to errors-in-variables Frechet regression for response variables in a broader class of metric spaces, e.g., by leveraging the quadruple inequality proposed by Schotz [44; 45]. Lastly, investigating the asymptotic distribution of the proposed SVT estimator would be highly appealing in the statistical literature, as it would enable us to make statistical inferences on the conditional Frechet mean in non-Euclidean spaces [6; 8] with errors-in-variables covariates.

## Acknowledgments and Disclosure of Funding

DS acknowledges support from the National Science Foundation under Grant No. CCF-2212326.

## References

* [1] Bijan Afsari. Riemannian \(L^{p}\) center of mass: Existence, uniqueness, and convexity. _Proceedings of the American Mathematical Society_, 139(2):655-673, 2011.
* [2] Anish Agarwal, Devavrat Shah, Dennis Shen, and Dogyoon Song. On robustness of principal component regression. _Journal of the American Statistical Association_, 116(536):1731-1745, 2021.
* [3] Adil Ahidar-Coutrix, Thibaut Le Gouic, and Quentin Paris. Convergence rates for empirical barycenters in metric spaces: Curvature, convexity and extendable geodesics. _Probability Theory and Related Fields_, 177(1):323-368, 2020.
* [4] Gustavo Amorim, Ran Tao, Sarah Lotspeich, Pamela A Shaw, Thomas Lumley, and Bryan E Shepherd. Two-phase sampling designs for data validation in settings with covariate measurement error and continuous outcome. _Journal of the Royal Statistical Society Series A: Statistics in Society_, 184(4):1368-1389, 2021.
* [5] Marc Arnaudon and Laurent Miclo. Means in complete manifolds: Uniqueness and approximation. _ESAIM: Probability and Statistics_, 18:185-206, 2014.
* [6] Dennis Barden, Huiling Le, and Megan Owen. Central limit theorems for Frechet means in the space of phylogenetic trees. _Electronic Journal of Probability_, 18(25):1-25, 2013.
* [7] Alexandre Belloni, Mathieu Rosenbaum, and Alexandre B Tsybakov. Linear and conic programming estimators in high dimensional errors-in-variables models. _Journal of the Royal Statistical Society. Series B (Statistical Methodology)_, pages 939-956, 2017.
* [8] Rabi Bhattacharya and Lizhen Lin. Omnibus CLTs for Frechet means and nonparametric inference on non-Euclidean spaces. _Proceedings of the American Mathematical Society_, 145(1):413-428, 2017.
* [9] Norman E Breslow and Thomas Lumley. Semiparametric models and two-phase samples: Applications to Cox regression. _IMS Collections_, 9:65-77, 2013.
* [10] Louis Capitaine, Jeremie Bigot, Rodolphe Thiebaut, and Robin Genuer. Frechet random forests for metric space valued regression with non Euclidean predictors. _arXiv preprint arXiv:1906.01741_, 2019.
* [11] Raymond J Carroll, Helmut Kuchenhoff, Frederick Lombard, and Leonard A Stefanski. Asymptotics for the SIMEX estimator in nonlinear measurement error models. _Journal of the American Statistical Association_, 91(433):242-250, 1996.
* [12] Raymond J Carroll, David Ruppert, Ciprian M Crainiceanu, Tor D Tosteson, and Margaret R Karagas. Nonlinear and nonparametric regression and instrumental variables. _Journal of the American Statistical Association_, 99(467):736-750, 2004.
* [13] Raymond J Carroll, David Ruppert, Leonard A Stefanski, and Ciprian M Crainiceanu. _Measurement Error in Nonlinear Models: A Modern Perspective_. Chapman and Hall/CRC, 2006.
* [14] Benjamin Charlier. Necessary and sufficient condition for the existence of a Frechet mean on the circle. _ESAIM: Probability and Statistics_, 17:635-649, 2013.
* [15] Yan Mei Chen, Xiao Shan Chen, and Wen Li. On perturbation bounds for orthogonal projections. _Numerical Algorithms_, 73(2):433-444, 2016.
* [16] John R Cook and Leonard A Stefanski. Simulation-extrapolation estimation in parametric measurement error models. _Journal of the American Statistical Association_, 89(428):1314-1328, 1994.

* [17] Abhirup Datta and Hui Zou. CoCoLasso for high-dimensional error-in-variables regression. _The Annals of Statistics_, 45(6):2400-2426, 2017.
* [18] Aurore Delaigle, Jianqing Fan, and Raymond J Carroll. A design-adaptive local polynomial estimator for the errors-in-variables problem. _Journal of the American Statistical Association_, 104(485):348-359, 2009.
* [19] Aurore Delaigle, Peter Hall, and Wen-xin Zhou. Nonparametric covariate-adjusted regression. _The Annals of Statistics_, 44(5):2190-2220, 2016.
* [20] Shanshan Ding and R Dennis Cook. Matrix variate regressions and envelope models. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 80(2):387-408, 2018.
* [21] David Donoho. 50 years of data science. _Journal of Computational and Graphical Statistics_, 26(4):745-766, 2017.
* [22] Paormita Dubey and Hans-Georg Muller. Functional models for time-varying random objects. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 82(2):275-327, 2020.
* [23] Juan Jose Egozcue, Josep Daunis-I-Estadella, Vera Pawlowsky-Glahn, Karel Hron, and Peter Filzmoser. Simplicial regression. The normal model. _Journal of Applied Probability and Statistics_, 2012.
* [24] Jianqing Fan and Elias Masry. Multivariate regression estimation with errors-in-variables: Asymptotic normality for mixing processes. _Journal of Multivariate Analysis_, 43(2):237-271, 1992.
* [25] Jianqing Fan and Young K Truong. Nonparametric regression with errors in variables. _The Annals of Statistics_, pages 1900-1925, 1993.
* [26] Maurice Frechet. Les elements aleatoires de nature quelconque dans un espace distancie. In _Annales de l'Institut Henri Poincare_, volume 10, pages 215-310, 1948.
* [27] Aritra Ghosal, Wendy Meiring, and Alexander Petersen. Frechet single index models for object response regression. _Electronic Journal of Statistics_, 17(1):1074-1112, 2023.
* [28] Kyunghee Han, Hans-Georg Muller, and Byeong U Park. Additive functional regression for densities as responses. _Journal of the American Statistical Association_, 2019.
* [29] Xuming He, David Madigan,, Bin Yu, and Jon Wellner. Statistics at a crossroads: Who is for the challenge. In _Workshop report: National Science Foundation. https://www. nsf. gov/nps/dms/documents/Statistics_at_a_Crossroads_Workshop_Report_2019. pdf_, 2019.
* [30] Matthias Hein. Robust nonparametric regression with metric-space valued output. _Advances in Neural Information Processing Systems_, 22, 2009.
* [31] Alan J. Izenman. _Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning_. Springer, 2008.
* [32] Ian T Jolliffe. A note on the use of principal components in regression. _Journal of the Royal Statistical Society Series C: Applied Statistics_, 31(3):300-303, 1982.
* [33] Ioannis Kalogridis and Stefan Van Aelst. Robust functional regression based on principal components. _Journal of Multivariate Analysis_, 173:393-415, 2019.
* [34] Eric D. Kolaczyk. _Statistical Analysis of Network Data_. Springer, 2009.
* [35] Thibaut Le Gouic and Jean-Michel Loubes. Existence and consistency of Wasserstein barycenters. _Probability Theory and Related Fields_, 168:901-917, 2017.
* [36] Hua Liang, Wolfgang Hardle, and Raymond J Carroll. Estimation in a semiparametric partially linear errors-in-variables model. _The Annals of Statistics_, 27(5):1519-1535, 1999.
* [37] Zhenhua Lin and Hans-Georg Muller. Total variation regularized Frechet regression for metric-space valued data. _The Annals of Statistics_, 49(6):3510-3533, 2021.

* [38] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. _Advances in Neural Information Processing Systems_, 24, 2011.
* [39] James Stephen Marron and Ian L Dryden. _Object Oriented Data Analysis_. CRC Press, 2021.
* [40] Alexander Petersen and Hans-Georg Muller. Frechet regression for random objects with Euclidean predictors. _The Annals of Statistics_, 47(2):691-719, 2019.
* [41] James O Ramsay and Bernard W Silverman. _Functional Data Analysis_. Springer, 2nd edition, 2005.
* [42] Philip T Reiss and R Todd Ogden. Functional principal component regression and functional partial least squares. _Journal of the American Statistical Association_, 102(479):984-996, 2007.
* [43] Susanne M Schennach. Instrumental variable estimation of nonlinear errors-in-variables models. _Econometrica_, 75(1):201-239, 2007.
* [44] Christof Schotz. Convergence rates for the generalized Frechet mean via the quadruple inequality. _Electronic Journal of Statistics_, 13:4280-4345, 2019.
* [45] Christof Schotz. Nonparametric regression in nonstandard spaces. _Electronic Journal of Statistics_, 16(2):4679-4741, 2022.
* [46] Damla Senturk and Hans-Georg Muller. Covariate-adjusted regression. _Biometrika_, 92(1):75-89, 2005.
* [47] Donna Spiegelman, Aidan McDermott, and Bernard Rosner. Regression calibration method for correcting measurement-error bias in nutritional epidemiology. _The American Journal of Clinical Nutrition_, 65(4):1179S-1186S, 1997.
* [48] Florian Steinke and Matthias Hein. Non-parametric regression between manifolds. _Advances in Neural Information Processing Systems_, 21, 2008.
* [49] Florian Steinke, Matthias Hein, and Bernhard Scholkopf. Nonparametric regression between general Riemannian manifolds. _SIAM Journal on Imaging Sciences_, 3(3):527-563, 2010.
* [50] Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems. _SIAM Review_, 19(4):634-662, 1977.
* [51] Karl-Theodor Sturm. Probability measures on metric spaces of nonpositive curvature. _Heat Kernels and Analysis on Manifolds, Graphs, and Metric Spaces: Lecture Notes from a Quarter Program on Heat Kernels, Random Walks, and Analysis on Manifolds and Graphs_, 338:357, 2003.
* [52] Renata Talska, Alessandra Menafoglio, Jitka Machalova, Karel Hron, and E Fiserova. Compositional regression with functional response. _Computational Statistics & Data Analysis_, 123:66-85, 2018.
* [53] Kristin M Tolle, D Stewart W Tansley, and Anthony JG Hey. The fourth paradigm: Data-intensive scientific discovery [point of view]. _Proceedings of the IEEE_, 99(8):1334-1337, 2011.
* [54] Danielle C Tucker, Yichao Wu, and Hans-Georg Muller. Variable selection for global Frechet regression. _Journal of the American Statistical Association_, pages 1-15, 2021.
* [55] Aad W Vaart and Jon A Wellner. _Weak Convergence and Empirical Processes_. Springer, 2nd edition, 2023.
* [56] Roman Vershynin. _High-dimensional Probability: An Introduction with Applications in Data Science_, volume 47. Cambridge University Press, 2018.
* [57] Cinzia Viroli. On matrix-variate regression analysis. _Journal of Multivariate Analysis_, 111:296-309, 2012.

* [58] Takumi Yokota. Convex functions and barycenter on CAT(1)-spaces of small radii. _Journal of the Mathematical Society of Japan_, 68(3):1297-1323, 2016.
* [59] Y Zemel and V M Panaretos. Frechet means and procrustes analysis in Wasserstein space. _Bernoulli_, 25(2):932-976, 2019.

## Appendix A Verification of the model assumptions

### Additional background

**Definition 6** (\(\varepsilon\)-net and covering number).: _Let \((\mathcal{M},d)\) be a metric space. Let \(S\subseteq T\) be a subset and let \(\varepsilon>0\). A subset \(\mathcal{N}\subseteq S\) is called an \(\varepsilon\)-net of \(S\) if every point in \(S\) is within distance \(\varepsilon\) of some point \(\mathcal{N}\), i.e.,_

\[\forall x\in S,\ \ \exists x_{0}\in\mathcal{N}\text{ such that }d(x,x_{0})\leq\varepsilon.\]

_The \(\varepsilon\)-covering number of \(S\), denoted by \(\mathfrak{N}(S,\varepsilon)\), is the smallest possible cardinality of an \(\varepsilon\)-net of \(S\), i.e.,_

\[\mathfrak{N}(S,\varepsilon)\coloneqq\min\left\{k\in\mathbb{N}:\exists y_{1}, \ldots,y_{k}\in\mathcal{M}\text{ such that }S\subseteq\bigcup_{i=1}^{k}B_{d}(y_{i}, \varepsilon)\right\},\] (16)

_where \(B_{d}(y,\varepsilon)=\{y^{\prime}\in\mathcal{M}:d(y,y^{\prime})\leq\varepsilon\}\) denotes the closed \(\varepsilon\)-ball centered at \(y\in\mathcal{M}\)._

Let \(B_{2}^{r}(0,1)\coloneqq\{x\in\mathbb{R}^{r}:\|x\|_{2}\leq 1\}\) denote the unit \(\ell^{2}\)-norm ball in \(\mathbb{R}^{r}\). It is well known3 that for any \(\varepsilon>0\),

Footnote 3: See [56, Corollary 4.2.13] for example.

\[\left(\frac{1}{\varepsilon}\right)^{r}\leq\mathfrak{N}\big{(}B_{2}^{r}(0,1), \varepsilon\big{)}\leq\left(\frac{2}{\varepsilon}+1\right)^{r}.\] (17)

### Example 1: Euclidean space

**Proposition 1**.: _The space \((\mathcal{H},d_{\mathrm{HS}})\) defined in Example 1 satisfies Assumptions (C0), (C1), and (C2)._

Proof of Proposition 1.: For any probability distribution \(\nu\) and any \(\lambda\in\mathbb{R}_{+}\), let \(y_{\nu}^{(\lambda)}\coloneqq\mathbb{E}_{\nu}\big{[}w_{\nu}^{(\lambda)}(X,x) \cdot Y\big{]}\). Then we observe that

\[R_{\nu}^{(\lambda)}(y;x) =\mathbb{E}_{\nu}\big{[}w_{\nu}^{(\lambda)}(X,x)\cdot d^{2}(Y,y) \big{]}\] \[=\mathbb{E}_{\nu}\left[w_{\nu}^{(\lambda)}(X,x)\cdot\|Y-y_{\nu}^{ \|}\right]+\|y-y_{\nu}^{(\lambda)}\|_{\mathrm{HS}}^{2}\] \[\qquad+2\left\langle\underbrace{\mathbb{E}_{\nu}\left[w_{\nu}^{( \lambda)}(X,x)\cdot\big{(}Y-y_{\nu}^{(\lambda)}\big{)}\right]}_{=0},\,y_{\nu}^ {(\lambda)}-y\right\rangle\] \[=R_{\nu}^{(\lambda)}(y_{\nu}^{(\lambda)};x)+\|y-y_{\nu}^{( \lambda)}\|_{\mathrm{HS}}^{2}.\]

As \(R_{\nu}^{(\lambda)}(y;x)\) is a strictly convex and coercive function, there exists a unique minimizer, \(\varphi_{\nu}^{(\lambda)}\). Thus, Condition (C0) is proved. Furthermore, Condition (C1) is also satisfied with \(D_{\mathbf{g}}=\infty\), \(C_{\mathbf{g}}=1\), and \(\alpha=2\).

Lastly, for any \(y\in\mathcal{H}\) and any \(\varepsilon>0\),

\[\mathfrak{N}\big{(}B_{d_{\mathrm{HS}}}(y,\delta),\delta\varepsilon\big{)}= \mathfrak{N}\big{(}B_{d_{\mathrm{HS}}}(y,1),\varepsilon\big{)}\leq\left(\frac {2}{\varepsilon}+1\right)^{r}\leq C\cdot\varepsilon^{-r}\]

where \(r=\dim\mathcal{H}\) and \(C>1\) is a constant that depends on \(r\) only; see the covering number upper bound in (17). Thus, the integral (11) is bounded as follows:

\[\int_{0}^{1}\sqrt{1+\log\mathfrak{N}\big{(}B_{d}\big{(}\varphi(x ),\delta\big{)},\delta\varepsilon\big{)}}\,\mathrm{d}\varepsilon \leq\int_{0}^{1}\sqrt{1+\log C-r\log\varepsilon}\,\mathrm{d}\varepsilon\] \[\leq\sqrt{1+\log C}+\sqrt{r}\int_{0}^{1}\sqrt{-\log\varepsilon}\, \mathrm{d}\varepsilon\] \[=\sqrt{1+\log C}+\sqrt{r}\int_{0}^{\infty}e^{-t}\sqrt{t}\, \mathrm{d}t\] \[=\sqrt{1+\log C}+\frac{\sqrt{r\pi}}{2}\]using the change of variable \(t=-\log\varepsilon\). Therefore, Assumption (C2) holds with \(C_{\bullet}=\sqrt{1+\log C}+\frac{\sqrt{r\pi}}{2}\).

### Example 2: Set of probability distributions

**Proposition 2**.: _The space \((\mathcal{W},d_{W})\) defined in Example 2 satisfies Assumptions (C0), (C1), and (C2)._

Proof of Proposition 2.: For a probability distribution function \(y\in\mathcal{W}\) defined on \(\mathbb{R}\), let \(\mathcal{Q}=Q(\mathcal{W})\coloneqq\{Q(y):y\in\mathcal{W}\}\) denote the collection of corresponding quantile functions, where \(\big{(}Q(y)\big{)}(u)=y^{-1}(u)\) for \(u\in[0,1]\).

We note that \(f\mapsto\mathbb{E}_{\nu}\big{[}w^{(\lambda)}_{\nu}(X,x)\,\langle Q(Y),f\rangle \big{]}\) is a bounded linear functional defined on \(L^{2}[0,1]\) because \(\mathbb{E}_{\nu}|w^{(\lambda)}_{\nu}(X,x)|^{2}\leq 2+2p\,\|(x-\mu_{\nu})\|_{ \Sigma_{\nu}}^{2}\) implies that \(\mathbb{E}_{\nu}\big{[}w^{(\lambda)}_{\nu}(X,x)|\cdot\|Q(Y)\|_{2}\big{]}\) is bounded. It follows from the Riesz representation theorem that there exists \(f^{(\lambda)}_{x}\in L^{2}[0,1]\) such that

\[\mathbb{E}_{\nu}\big{[}w^{(\lambda)}(X,x)\,\langle Q(Y),g\rangle_{2}\big{]}= \langle f^{(\lambda)}_{x},g\rangle_{2}\] (18)

for all \(g\in L^{2}[0,1]\). Then, we have

\[R^{(\lambda)}_{\nu}(y;x)=\mathbb{E}_{\nu}\big{[}w^{(\lambda)}_{\nu}(X,x)\,\|Q (Y)-f^{(\lambda)}_{x}\|_{2}^{2}\big{]}+\|Q(y)-f^{(\lambda)}_{x}\|_{2}^{2},\] (19)

which yields that

\[\varphi^{(\lambda)}_{\nu}(x)=Q^{-1}\bigg{(}\operatorname*{arg\,min}_{Q\in \mathcal{Q}}\|Q-f^{(\lambda)}_{x}\|_{2}^{2}\bigg{)}.\] (20)

The condition (C0) follows from the convexity of \((\mathcal{Q},\,\|\cdot\|_{2})\). Moreover, the convexity also gives \(\big{\langle}Q(y)-Q(\varphi^{(\lambda)}_{\nu}(x)),f^{(\lambda)}_{x}(x)-Q( \varphi^{(\lambda)}_{\nu}(x))\big{\rangle}_{2}\leq 0\) for all \(y\in\mathcal{W}\), so that

\[R^{(\lambda)}_{\nu}(y;x)-R^{(\lambda)}_{\nu}(\varphi^{(\lambda)} (x);x)\] \[\qquad=\|Q(y)-f^{(\lambda)}_{x}(x)\|_{2}^{2}-\|Q(\varphi^{( \lambda)}_{\nu}(x))-f^{(\lambda)}_{x}(x)\|_{2}^{2}\] \[\qquad=\|Q(y)-Q(\varphi^{(\lambda)}_{\nu}(x))\|_{2}-2\big{\langle} Q(y)-Q(\varphi^{(\lambda)}_{\nu}(x)),f^{(\lambda)}_{x}(x)-Q(\varphi^{(\lambda)}_{ \nu}(x))\big{\rangle}_{2}\] (21) \[\qquad\geq\|Q(y)-Q(\varphi^{(\lambda)}_{\nu}(x))\|_{2}\] \[\qquad=d^{2}_{W}(y,\varphi^{(\lambda)}_{\nu}(x)).\]

Therefore, the condition (C1) holds for any arbitrary constant \(D_{\mathsf{g}}>0\) with \(C_{\mathsf{g}}=1\) and \(\alpha=2\).

Finally, we refer to [40, Proposition 1] to ensure that for any \(\delta>0\) and any \(\varepsilon>0\),

\[\sup_{y\in\mathcal{W}}\log\mathfrak{N}\big{(}B_{d_{W}}(y,\delta),D_{\bullet} \varepsilon\big{)}\leq\sup_{Q\in\mathcal{Q}}\log\mathfrak{N}\big{(}B_{d_{2}} (Q,\delta),\delta\varepsilon\big{)}\leq C\cdot\varepsilon^{-1}\] (22)

holds with an absolute constant \(C>0\). Technically, this fact comes from the covering number bound for a class of uniformly bounded and monotone functions in \(L^{2}\). This confirms that the entropy condition (C2) holds. 

### Example 3: Set of correlation matrices

**Proposition 3**.: _The space \((\mathcal{M},d_{F})\) defined in Example 3 satisfies Assumptions (C0), (C1), and (C2)._

Proof of Proposition 3.: First of all, we note that \(\mathcal{M}\) is a convex subset of \(\mathcal{S}^{r}\coloneqq\{X\in\mathbb{R}^{r\times r}:X=X^{\top}\}\), which is the set of \(r\times r\) symmetric matrices. It is because \(\mathcal{M}=\mathcal{S}^{r}_{\top}\cap H\) where \(\mathcal{S}^{r}_{+}\) denotes the cone of \(r\times r\) positive semidefinite matrices and \(H\coloneqq\{X\in\mathcal{S}^{r}:X_{ii}=1,\;\forall i\in[r]\}\) denotes an affine subspace of \(\mathcal{S}^{r}\), both of which are convex.

Next, we observe that \(\mathcal{S}^{r}\) equipped with the Frobenius metric \(d_{F}\) is isometrically isomorphic to \(\mathbb{R}^{r(r+1)/2}\) equipped with the \(\ell^{2}\)-metric. Hence, \((\mathcal{M},d_{F})\) satisfies Assumptions (C0), (C1), and (C2), inheriting these properties from the ambient space \(\mathcal{S}^{r}\), which is established by Proposition 1. We note that the inheritance of (C0), (C1) relies on the convexity of \(\mathcal{M}\), while (C2) is inherited simply based on the inclusion \(\mathcal{M}\subset\mathcal{S}^{r}\).

### Example 4: Bounded Riemannian manifold

**Proposition 4**.: _The space \((\mathcal{M},d_{g})\) defined in Example 4 satisfies Assumption (C2) provided that the Riemannian metric is equivalent to the ambient Euclidean metric._

_Furthermore, let \(T_{y}\mathcal{M}\) be the tangent space of \(\mathcal{M}\) at \(y\), and \(\operatorname{Exp}_{y}:T_{y}\mathcal{M}\to\mathcal{M}\) be the manifold exponential map at \(y\). Let \(g_{\nu}^{(\lambda)}(u;y,x)\coloneqq R_{\nu}^{(\lambda)}\big{(}\mathbb{E}_{y}(u ),x\big{)}\) for \(u\in T_{y}\mathcal{M}\) If (C0) holds and the Hessian of \(g_{\nu}^{(\lambda)}\big{(}u;\varphi_{\nu}^{(\lambda)}(x),x\big{)}\) is positive definite, then (C1) for some \(D_{\boldsymbol{g}}>0\)._

Proof of Proposition 3.: Since \(\mathcal{M}\) has finite dimension and is bounded, the bounded entropy condition (C2) follows from the metric equivalence.

Suppose that (C0) holds, and let \(\delta>0\) be the injectivity radius at \(\varphi_{\nu}^{(\lambda)}(x)\). Consider \(y\in\mathcal{M}\) such that \(d\big{(}y,\varphi_{\nu}^{(\lambda)}(x)\big{)}<\delta\), and let \(u_{y}=\operatorname{Log}_{\varphi_{\nu}^{(\lambda)}(x)}(y)\). Then we have

\[R_{\nu}^{(\lambda)}\big{(}y;x\big{)}-R_{\nu}^{(\lambda)}\big{(}\varphi_{\nu}^{ (\lambda)}(x);x\big{)}=g_{\nu}^{(\lambda)}\big{(}u_{y};\varphi_{\nu}^{(\lambda )}(x),x\big{)}-g_{\nu}^{(\lambda)}\big{(}0;\varphi_{\nu}^{(\lambda)}(x),x\big{)} =u_{y}^{\top}\,\nabla^{2}g_{\nu}^{(\lambda)}(\bar{u}_{y})\,u_{y}\]

for some \(\bar{u}_{y}\) between \(0\) and \(u_{y}\). Since \(u_{y}^{\top}u_{y}=d\big{(}y,\varphi_{\nu}^{(\lambda)}(x)\big{)}^{2}\) and \(g_{\nu}^{(\lambda)}\) is continuous, the positive definiteness of \(\nabla^{2}g_{\nu}^{(\lambda)}(\bar{u}_{y})\) implies (C1) with \(\alpha=1\). 

## Appendix B Proof of Theorem 1

Proof of Theorem 1.: Recall from Definition 4, cf. (9), that for any probability distribution \(\nu\) on \(\mathbb{R}^{p}\), any \(\lambda\in\mathbb{R}_{+}\), and any \(x\in\mathbb{R}^{p}\), the \(\lambda\)-regularized Frechet regression function evaluated at \(x\) is given as the minimizer of a function \(R_{\nu}^{(\lambda)}\) as

\[\varphi_{\nu}^{(\lambda)}(x)=\operatorname*{arg\,min}_{y\in\mathcal{M}}R_{ \nu}^{(\lambda)}(y;x)\]

where

\[R_{\nu}^{(\lambda)}(y;x) =\mathbb{E}_{(X,Y)\sim\nu}\left[w_{\nu}^{(\lambda)}(X,x)\cdot d^{ 2}(Y,y)\right]\quad\text{and}\] \[w_{\nu}^{(\lambda)}(x^{\prime},x) =1+(x^{\prime}-\mu_{\nu})^{\top}\left[\mathsf{SVT}^{(\lambda)} \big{(}\Sigma_{\nu}\big{)}\right]^{\top}(x-\mu_{\nu}).\]

In this proof, we follow a similar strategy to that in the proof of [40, Theorem 1]. Specifically, it suffices to show \(\sup_{y\in\mathcal{M}}\big{|}R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^ {(0)}(y;x)\big{|}\) converges to zero in probability, due to [55, Corollary 3.2.3]. To this end, we show \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)\) weakly converges to \(R_{\nu^{*}}^{(0)}(y;x)\) in the \(\ell^{\infty}(\mathcal{M})\)-sense, and then apply [55, Theorem 1.3.6]. Again, according to [55, Theorem 1.5.4], this weak convergence can be proved by showing that

* \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{(0)}(y;x)=o_{p}(1)\) for all \(y\in\mathcal{M}\), and
* \(R_{\mathcal{D}_{n}}^{(\lambda)}\) is asymptotically equicontinuous in probability, i.e., for any \(\varepsilon,\eta>0\), there exists \(\delta>0\) such that \[\limsup_{n}P\left(\sup_{y_{1},y_{2}\in\mathcal{M}:\,d(y_{1},y_{2})<\delta} \left|R_{\mathcal{D}_{n}}^{(\lambda)}(y_{1};x)-R_{\mathcal{D}_{n}}^{(\lambda)} (y_{2};x)\right|>\varepsilon\right)<\eta.\]

In what follows, we prove these two statements, (S1) and (S2), thereby completing the proof of Theorem 1.

Step 1: proof of (S1).First of all, we observe that

\[R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{(0)}(y;x)=\underbrace{\Big{(} R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\mathcal{D}_{n}}^{(0)}(y;x)\Big{)}}_{=:T_{1}}+ \underbrace{\Big{(}R_{\mathcal{D}_{n}}^{(0)}(y;x)-R_{\nu^{*}}^{(0)}(y;x)\Big{)} }_{=:T_{2}}.\] (23)

We separately analyze the two terms \(T_{1}\) and \(T_{2}\) below to show \(T_{1}=o_{p}(1)\) and \(T_{2}=o_{p}(1)\) as \(n\to\infty\).

1. \(T_{1}=o_{p}(1)\). Let \(\mathcal{D}_{n}=\{(X_{i},Y_{i}):i\in[n]\}\), and we re-write \[T_{1}=\frac{1}{n}\sum_{i=1}^{n}\left(w_{\mathcal{D}_{n}}^{(\lambda)}(X_{i},x)-w_{ \mathcal{D}_{n}}^{(0)}(X_{i},x)\right)\cdot d^{2}(Y_{i},y).\] Letting \(\widehat{\mu}_{n}=\mu_{\mathcal{D}_{n}}\), \(\widehat{\Sigma}_{n}=\Sigma_{\mathcal{D}_{n}}\), and \(\widehat{\Sigma}_{n}^{(\lambda)}=\texttt{SVT}^{(\lambda)}(\widehat{\Sigma}_{n})\) for shorthand, we observe that \[w_{\mathcal{D}_{n}}^{(\lambda)}(X_{i},x)-w_{\mathcal{D}_{n}}^{(0)}(X_{i},x)= (X_{i}-\widehat{\mu}_{n})^{\top}\left[\widehat{\Sigma}_{n}^{(\lambda),\dagger} -\widehat{\Sigma}_{n}^{\dagger}\right](x-\widehat{\mu}_{n}).\] Let \(\bm{X}=[X_{1}\quad\cdots\quad X_{n}]^{\top}\in\mathbb{R}^{n\times p}\), and note that \(\widehat{\Sigma}_{n}=\frac{1}{n}\big{(}\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{ \top}\big{)}^{\top}\big{(}\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}\big{)}\). Then it follows that \[\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\widehat{\mu}_{n})^{\top}\left[\widehat{\Sigma }_{n}^{(\lambda),\dagger}-\widehat{\Sigma}_{n}^{\dagger}\right]=\frac{1}{n} \bm{1}_{n}^{\top}\big{(}\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}\big{)} \left[\widehat{\Sigma}_{n}^{(\lambda),\dagger}-\widehat{\Sigma}_{n}^{\dagger}\right]\] Consider a singular value decomposition of \(\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}\), namely, \[\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}=\sum_{i=1}^{\min\{n,p\}}s_{i}\cdot u _{i}v_{i}^{\top},\] and observe that \(\widehat{\Sigma}_{n}=\sum_{i=1}^{\min\{n,p\}}s_{i}^{2}\cdot v_{i}v_{i}^{\top}\) is an eigenvalue decomposition of \(\widehat{\Sigma}_{n}\). Letting \(\mathcal{V}_{n}^{(\lambda)}\coloneqq\mathrm{span}\,\left\{v_{i}:i\in[p],\;0<s_ {i}\leq\sqrt{\lambda}\right\}\) be a subspace of \(\mathbb{R}^{p}\) spanned by the eigenvectors of \(\widehat{\Sigma}_{n}\) corresponding to the nonzero eigenvalues no greater than \(\lambda\), we have \[\widehat{\Sigma}_{n}^{(\lambda),\dagger}-\widehat{\Sigma}_{n}^{\dagger} =\sum_{i=1}^{p}\frac{1}{s_{i}^{2}}\cdot\mathds{1}\{s_{i}>\sqrt{ \lambda}\}\cdot v_{i}v_{i}^{\top}-\sum_{i=1}^{p}\frac{1}{s_{i}^{2}}\cdot \mathds{1}\{s_{i}>0\}\cdot v_{i}v_{i}^{\top}\] \[=\sum_{i=1}^{p}\frac{1}{s_{i}^{2}}\cdot\mathds{1}\{0<s_{i}\leq \sqrt{\lambda}\}\cdot v_{i}v_{i}^{\top}\] \[=\widehat{\Sigma}_{n}^{\dagger}\cdot\Pi_{\mathcal{V}_{n}^{( \lambda)}}\] \[=n\cdot\big{(}\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}\big{)}^{ \dagger}\big{(}\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}\big{)}^{\dagger,\top }\cdot\Pi_{\mathcal{V}_{n}^{(\lambda)}}\] (24) where \(\Pi_{\mathcal{V}_{n}^{(\lambda)}}\) denotes the projection matrix onto the subspace \(\mathcal{V}_{n}^{(\lambda)}\). Note that \(\Pi_{\mathcal{V}_{n}^{(\lambda)}}=0\) if and only if \(\min\left\{i\in[p]:0<s_{i}\leq\sqrt{\lambda}\right\}=\emptyset\). Therefore, we have \[T_{1} =\frac{1}{n}\sum_{i=1}^{n}\left(w_{\mathcal{D}_{n}}^{(\lambda)}(X_{ i},x)-w_{\mathcal{D}_{n}}^{(0)}(X_{i},x)\right)\cdot d^{2}(Y_{i},y)\] \[\leq\frac{\mathrm{diam}\,(\mathcal{M})^{2}}{n}\bm{1}_{n}^{\top} \big{(}\bm{X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}\big{)}\left[\widehat{\Sigma} _{n}^{(\lambda),\dagger}-\widehat{\Sigma}_{n}^{\dagger}\right](x-\widehat{\mu }_{n})\] \[=\mathrm{diam}\,(\mathcal{M})^{2}\cdot\bm{1}_{n}^{\top}\big{(}\bm{ X}-\bm{1}_{n}\widehat{\mu}_{n}^{\top}\big{)}^{\dagger,\top}\cdot\Pi_{\mathcal{V}_{n}^{ (\lambda)}}\cdot(x-\widehat{\mu}_{n})\hskip 56.905512pt\because(\ref{eq:M})\] \[=o_{p}(1).\] The last line follows from the fact that \(\sup_{i\in[p]}\left(\sigma_{i}(\widehat{\Sigma}_{n})-\sigma_{i}(\Sigma_{\nu^{ *}})\right)\to 0\) in probability, and thus, \(\Pi_{\mathcal{V}_{n}^{(\lambda)}}\to 0\) in probability.
2. \(T_{2}=o_{p}(1)\).

Letting \(\tilde{R}_{n}(y;x)=\frac{1}{n}\sum_{i=1}^{n}w_{\nu^{*}}^{(0)}(X_{i},x)\cdot d^{2} (Y_{i},y)\), we decompose \(T_{2}\) as follows:

\[T_{2} =R_{\mathcal{D}_{n}}^{(0)}(y;x)-\tilde{R}_{n}(y;x)+\tilde{R}_{n}(y ;x)-R_{\nu^{*}}^{(0)}(y;x)\] \[=\underbrace{\frac{1}{n}\sum_{i=1}^{n}\left\{w_{\mathcal{D}_{n}} ^{(0)}(X_{i},x)-w_{\nu^{*}}^{(0)}(X_{i},x)\right\}\cdot d^{2}(Y_{i},y)}_{=:T_{2A}}\] \[\qquad+\underbrace{\frac{1}{n}\sum_{i=1}^{n}\left\{w_{\nu^{*}}^{ (0)}(X_{i},x)\cdot d^{2}(Y_{i},y)-\mathbb{E}\left[w_{\nu^{*}}^{(0)}(X_{i},x) \cdot d^{2}(Y_{i},y)\right]\right\}}_{=:T_{2B}}\]

Note that \(T_{2B}\) converges to \(0\) in probability by the weak law of large numbers.

Now it remains to show \(T_{2A}=o_{p}(1)\). To this end, we note that

\[w_{\mathcal{D}_{n}}^{(0)}(X_{i},x)-w_{\nu^{*}}^{(0)}(X_{i},x) =V_{n}(x)+X_{i}^{\top}W_{n}(x)\] \[\text{where}\quad\begin{cases}V_{n}(x)=-\widehat{\mu}_{n}^{\top} \widehat{\Sigma}_{n}^{\dagger}(x-\widehat{\mu}_{n})+\mu^{\top}\Sigma^{\dagger} (x-\mu),\\ W_{n}(x)=\widehat{\Sigma}_{n}^{\dagger}(x-\widehat{\mu}_{n})-\Sigma^{\dagger}(x -\mu).\end{cases}\] (25)

Since \(\widehat{\mu}_{n}\) and \(\widehat{\Sigma}_{n}\) respectively converge to \(\mu\) and \(\Sigma\) in probability, it is possible to verify that \(|V_{n}(x)|,\|W_{n}(x)\|\) converge to \(0\) in probability. As a result, \(T_{2}\) also converges to \(0\) in probability.

All in all, we have \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{(0)}(y;x)=o_{p}(1)\), and thus, proved (S1).

Step 2: proof of (S2).For any \(y_{1},y_{2}\in\mathcal{M}\),

\[\left|R_{\mathcal{D}_{n}}^{(\lambda)}(y_{1};x)-R_{\mathcal{D}_{n }}^{(\lambda)}(y_{2};x)\right| =\left|\frac{1}{n}\sum_{i=1}^{n}w_{\mathcal{D}_{n}}^{(\lambda)}(X _{i},x)\cdot\left\{d^{2}(Y_{i},y_{1})-d^{2}(Y_{i},y_{2})\right\}\right|\] \[\leq\frac{1}{n}\sum_{i=1}^{n}\left|w_{\mathcal{D}_{n}}^{(\lambda) }(X_{i},x)\right|\cdot\left|d(Y_{i},y_{1})+d(Y_{i},y_{2})\right|\cdot\left|d( Y_{i},y_{1})-d(Y_{i},y_{2})\right|\] \[\leq 2\operatorname{diam}\left(\mathcal{M}\right)\cdot d(y_{1},y _{2})\cdot\left(\frac{1}{n}\sum_{i=1}^{n}\left|w_{\mathcal{D}_{n}}^{(\lambda) }(X_{i},x)\right|\right)\] \[=O_{p}\left(d(y_{1},y_{2})\right)\]

where the \(O_{p}\) term is independent of \(y_{1},y_{2}\in\mathcal{M}\). Therefore,

\[\sup_{y_{1},y_{2}\in\mathcal{M}:\:d(y_{1},y_{2})<\delta}\left|R_{\mathcal{D}_ {n}}^{(\lambda)}(y_{1};x)-R_{\mathcal{D}_{n}}^{(\lambda)}(y_{2};x)\right|=O_{p }(\delta),\]

which proves (S2).

## Appendix C Proof of Theorem 2

In this section, we prove the two claims in Theorem 2. Specifically, in Section C.1, we present and prove a lemma that controls the bias in the population estimator (Lemma 1), and in Section C.2, we present and prove a lemma that controls the variance of the empirical estimator (Lemma 2).

### Bias in the population estimator

We recall the definition of Mahalanobis seminorm from Definition 5: \(\|x\|_{\Sigma}:=\left(x^{\top}\Sigma^{\dagger}x\right)^{1/2}\).

**Lemma 1**.: _Suppose that Assumptions (C0) and (C1) hold. If_

\[\left\|x-\mu_{\nu^{*}}\right\|_{\Sigma_{\nu^{*}}}\leq\frac{C_{\bm{g}}\cdot D_{ \bm{g}}^{\alpha}}{\operatorname{diam}\left(\mathcal{M}\right)^{2}\cdot\sqrt{ \operatorname{rank}\Sigma_{\nu^{*}}}},\] (26)

_then for any \(\lambda\in\mathbb{R}_{+}\),_

\[d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}\leq 2^{K_{0}}\cdot\mathfrak{ b}_{\lambda}(x)^{\frac{1}{\alpha-1}}=O\Big{(}\mathfrak{b}_{\lambda}(x)^{ \frac{1}{\alpha-1}}\Big{)}\] (27)

_where_

\[K_{0} =\left|\frac{1}{(\alpha-1)\log 2}\cdot\log\left(\frac{4 \operatorname{diam}\left(\mathcal{M}\right)}{C_{\bm{g}}\cdot\big{(}1-2^{-( \alpha-1)}\big{)}}\right)\right|+1\quad\text{and}\] \[\mathfrak{b}_{\lambda}(x) =\sqrt{\operatorname{rank}\big{(}\Sigma_{\nu^{*}}-\Sigma_{\nu^{* }}^{(\lambda)}\big{)}\cdot\left\|x-\mu_{\nu^{*}}\right\|_{\Sigma_{\nu^{*}}- \Sigma_{\nu^{*}}^{(\lambda)}}}\,.\]

Proof of Lemma 1.: For the sake of brevity, we write \(\varphi^{(\lambda)}(x)=\varphi_{\nu^{*}}^{(\lambda)}(x)\) and \(\varphi(x)=\varphi_{\nu^{*}}^{(0)}(x)\) throughout this proof, dropping the subscript \(\nu^{*}\). Likewise, we simply write \(\mu=\mu_{\nu^{*}}\) and \(\Sigma=\Sigma_{\nu^{*}}\).

Step 1: A naive upper bound.Observe that for any \(\lambda\in\mathbb{R}_{+}\), \(x\in\mathbb{R}^{p}\), and \(y\in\mathcal{M}\),

\[\big{|}R(y;x)-R^{(\lambda)}(y;x)\big{|}\] \[\leq\operatorname{diam}\left(\mathcal{M}\right)^{2}\cdot\mathbb{ E}_{\nu^{*}}\left[\left\|X-\mu\right\|_{\Sigma-\Sigma^{(\lambda)}}\right] \cdot\left\|x-\mu\right\|_{\Sigma-\Sigma^{(\lambda)}}\quad\quad\because\text{ Cauchy-Schwarz inequality}\] \[\leq\operatorname{diam}\left(\mathcal{M}\right)^{2}\cdot\left( \mathbb{E}_{\nu^{*}}\left\|X-\mu\right\|_{\Sigma-\Sigma^{(\lambda)}}^{2} \right)^{1/2}\cdot\left\|x-\mu\right\|_{\Sigma-\Sigma^{(\lambda)}}\quad\because \text{Jensen's inequality}\] \[=\operatorname{diam}\left(\mathcal{M}\right)^{2}\cdot\sqrt{ \operatorname{rank}\big{(}\Sigma-\Sigma^{(\lambda)}\big{)}}\cdot\left\|x-\mu \right\|_{\Sigma-\Sigma^{(\lambda)}},\] (28)

where the last inequality follows from \(\mathbb{E}_{\nu^{*}}\left\|X-\mu\right\|_{\Sigma-\Sigma^{(\lambda)}}^{2}= \operatorname{rank}\big{(}\Sigma-\Sigma^{(\lambda)}\big{)}\).

We observe that the upper bound in (28) is monotone non-decreasing with respect to \(\lambda\in\mathbb{R}_{+}\), and it converges to \(0\) as \(\lambda\to 0\). To see this, for any \(\lambda\in\mathbb{R}_{+}\), we let

\[\mathcal{V}^{(\lambda)}\coloneqq\operatorname{span}\,\left\{v_{i}:i\in[p],\;0 <\lambda_{i}\leq\lambda\right\}\]

where \(\Sigma=\sum_{i=1}^{p}\lambda_{i}\cdot v_{i}v_{i}^{\tau}\) is an eigenvdecomposition of \(\Sigma\). Letting \(\Pi_{\mathcal{V}^{(\lambda)}}\) denote the projection matrix onto the subspace \(\mathcal{V}^{(\lambda)}\), we note that \(\Sigma-\Sigma^{(\lambda)}=\Pi_{\mathcal{V}^{(\lambda)}}\Sigma\Pi_{\mathcal{V} ^{(\lambda)}}\), and that \((\Sigma-\Sigma^{(\lambda)})^{\dagger}=\Pi_{\mathcal{V}^{(\lambda)}}\Sigma^{ \dagger}\Pi_{\mathcal{V}^{(\lambda)}}\). Thus, \(\operatorname{rank}\big{(}\Sigma-\Sigma^{(\lambda)}\big{)}=\operatorname{ dim}\mathcal{V}^{(\lambda)}\), and furthermore, we notice that \(\mathcal{V}^{(\lambda)}=\{0\}\) if and only if \(\lambda<\lambda_{\min}\coloneqq\min\{\lambda_{i}:\lambda_{i}>0\}\). Therefore,

\[\lambda<\lambda_{\min}\qquad\implies\qquad R^{(\lambda)}(y;x)-R(y;x)=0 \qquad\implies\qquad\varphi^{(\lambda)}(x)=\varphi(x),\;\;\forall x.\] (29)

The observation (29), together with Assumption (C0), implies that \(d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}=o(1)\) as \(\lambda\to 0\).

Step 2: Controlling risk difference.Next, we move on to determine the order of \(d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}\) -- as a function of \(\mathfrak{b}_{\lambda}(x)\) -- for a fixed \(\lambda\in\mathbb{R}\). We may assume \(\lambda>\lambda_{\min}\) for the proof because the lemma is trivial otherwise, cf. (29). Assuming \(\lambda>\lambda_{\min}\), we may decompose the difference in the population objective at \(\varphi^{(\lambda)}(x)\) and \(\varphi(x)\) as follows:

\[R\big{(}\varphi^{(\lambda)}(x);x\big{)}-R\big{(}\varphi(x);x\big{)} =\underbrace{\Big{\{}R\big{(}\varphi^{(\lambda)}(x);x\big{)}-R^{( \lambda)}\big{(}\varphi^{(\lambda)}(x);x\big{)}+R^{(\lambda)}\big{(}\varphi(x); x\big{)}-R\big{(}\varphi(x);x\big{)}\Big{\}}}_{=:\mathfrak{R}_{1}}\] \[\qquad\qquad-\underbrace{\Big{\{}R^{(\lambda)}\big{(}\varphi(x); x\big{)}-R^{(\lambda)}\big{(}\varphi^{(\lambda)}(x);x\big{)}\Big{\}}}_{=:\mathfrak{R}_{2}}.\]We observe that both \(\mathfrak{R}_{1}\) and \(\mathfrak{R}_{2}\) are non-negative, due to the optimality of \(\varphi(x)\) and \(\varphi^{(\lambda)}(x)\). Then, we obtain an upper bound for \(\mathfrak{R}_{1}\) using a similar argument as in (28). Specifically,

\[R\big{(}\varphi^{(\lambda)}(x);x\big{)}-R\big{(}\varphi(x);x\big{)} \leq\mathfrak{R}_{1}\] \[=\mathbb{E}_{\nu^{*}}\left[\Big{\{}w^{(0)}_{\nu^{*}}(X,x)-w^{( \lambda)}_{\nu^{*}}(X,x)\Big{\}}\cdot\Big{\{}d^{2}\big{(}Y,\varphi^{(\lambda)} (x)\big{)}-d^{2}\big{(}Y,\varphi(x)\big{)}\Big{\}}\right]\] \[\leq 2\operatorname{diam}\big{(}\mathcal{M}\big{)}\cdot\mathfrak{b }_{\lambda}(x)\cdot d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}.\] (30)

Step 3: Converting risk difference to bias.Lastly, we convert the upper bound (30) to an upper bound on the distance \(d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}\) using Assumption (C1). To this end, we begin by confirming that

\[R\big{(}\varphi^{(\lambda)}(x);x\big{)}-R\big{(}\varphi(x);x\big{)} =\mathbb{E}_{\nu^{*}}\left[(X-\mu)^{\top}\cdot\Sigma^{\dagger} \cdot(x-\mu)\cdot\Big{\{}d^{2}\big{(}Y,\varphi^{(\lambda)}(x)\big{)}-d^{2} \big{(}Y,\varphi(x)\big{)}\Big{\}}\right]\] \[\leq\operatorname{diam}\big{(}\mathcal{M}\big{)}^{2}\cdot\Big{(} \mathbb{E}_{\nu^{*}}\left\|X-\mu\right\|_{\Sigma}^{2}\Big{)}^{1/2}\cdot\left\| x-\mu\right\|_{\Sigma}\] \[=\operatorname{diam}\big{(}\mathcal{M}\big{)}^{2}\cdot\sqrt{ \operatorname{rank}\Sigma}\cdot\left\|x-\mu\right\|_{\Sigma}\] \[\leq C_{\mathsf{g}}\cdot D_{\mathsf{g}}^{\alpha}.\]

Thereafter, we choose an arbitrary \(K\in\mathbb{N}\) and \(r\in\mathbb{R}_{+}\) whose values will be determined later in this proof. Then we obtain the following inequality using the so-called peeling technique:

\[\mathds{1}\left\{d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)} >2^{K}\cdot\mathfrak{b}_{\lambda}(x)^{r}\right\}\] \[\quad=\sum_{k=K}^{\infty}\mathds{1}\left\{2^{k}\cdot\mathfrak{b} _{\lambda}(x)^{r}<d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}\leq 2^{k+1} \cdot\mathfrak{b}_{\lambda}(x)^{r}\right\}\] \[\leq\sum_{k=K}^{\infty}\mathds{1}\left\{2^{k}\cdot\mathfrak{b} _{\lambda}(x)^{r}<d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}\leq 2^{k+1} \cdot\mathfrak{b}_{\lambda}(x)^{r}\right\}\] \[\leq\sum_{k=K}^{\infty}\frac{R\big{(}\varphi^{(\lambda)}(x);x \big{)}-R\big{(}\varphi(x);x\big{)}}{C_{\mathsf{g}}\cdot\big{(}2^{k}\cdot \mathfrak{b}_{\lambda}(x)^{r}\big{)}^{\alpha}}\cdot\mathds{1}\left\{d\big{(} \varphi^{(\lambda)}(x),\varphi(x)\big{)}\leq 2^{k+1}\cdot\mathfrak{b}_{\lambda}(x)^{r} \right\}.\quad\because(C1)\] (31)

Moreover, we decompose the numerator in the fraction appearing in the upper bound (31) as follows: Combining (30) with (31), we have

\[\mathds{1}\left\{d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)} >2^{K}\cdot\mathfrak{b}_{\lambda}(x)^{r}\right\}\] \[\quad\leq\sum_{k=K}^{\infty}\frac{2\operatorname{diam}\big{(} \mathcal{M}\big{)}\cdot\mathfrak{b}_{\lambda}(x)\cdot d\big{(}\varphi^{( \lambda)}(x),\varphi(x)\big{)}}{C_{\mathsf{g}}\cdot\big{(}2^{k}\cdot\mathfrak{ b}_{\lambda}(x)^{r}\big{)}^{\alpha}}\cdot\mathds{1}\left\{d\big{(}\varphi^{( \lambda)}(x),\varphi(x)\big{)}\leq 2^{k+1}\cdot\mathfrak{b}_{\lambda}(x)^{r}\right\}\] \[\quad\leq\frac{4\operatorname{diam}\big{(}\mathcal{M}\big{)}}{C_ {\mathsf{g}}}\cdot\mathfrak{b}_{\lambda}(x)^{1-r(\alpha-1)}\sum_{k=K}^{\infty} \frac{1}{2^{k(\alpha-1)}}.\] (32)

Note that \(C\coloneqq\frac{4\operatorname{diam}\mathcal{M}}{C_{\mathsf{g}}}>0\) is a constant independent of \(\lambda\). Let \(r=1/(\alpha-1)\), and observe that the upper bound in (32) becomes smaller than \(1\) for a sufficiently large \(K\). Specifically,

\[K\geq\Bigg{\lfloor}\frac{1}{(\alpha-1)\log 2}\cdot\log\Bigg{(}\frac{4 \operatorname{diam}\big{(}\mathcal{M}\big{)}}{C_{\mathsf{g}}\cdot\big{(}1-2^{-( \alpha-1)}\big{)}}\Bigg{)}\Bigg{\rfloor}+1\quad\implies\quad\frac{4 \operatorname{diam}\big{(}\mathcal{M}\big{)}}{C_{\mathsf{g}}}\cdot\sum_{k=K}^{ \infty}\frac{1}{2^{k(\alpha-1)}}<1.\]

As a result, the inequality "\(d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}>2^{K_{0}}\cdot\mathfrak{b}_{ \lambda}(x)^{r}\)" in the indicator function must be false, and we conclude that

\[d\big{(}\varphi^{(\lambda)}(x),\varphi(x)\big{)}\leq 2^{K_{0}}\cdot\mathfrak{b}_{ \lambda}(x)^{\frac{1}{\alpha-1}}.\]

### Variance of the empirical estimator

**Lemma 2**.: _Suppose that Assumptions (C0), (C1) and (C2) hold. For any \(\lambda\in\mathbb{R}_{+}\) such that \(\lambda\not\in\operatorname{spec}\big{(}\Sigma_{\nu^{*}}\big{)}\), it holds that_

\[d\Big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(\lambda)} (x)\Big{)}=O_{P}\Big{(}n^{-\frac{1}{2(\alpha-1)}}\Big{)}.\]

Proof of Lemma 2.: Recall from the definition of \(\lambda\)-regularized Frechet regression (Definition 4) and (9) that

\[R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)=\frac{1}{n}\sum_{i=1}^{n}w_{\mathcal{D}_{ n}}^{(\lambda)}(X_{i},x)\cdot d^{2}(Y_{i},y)\quad\text{and}\quad R_{\nu^{*}}^{( \lambda)}(y;x)=\mathbb{E}_{(X,Y)\sim\nu^{*}}\left[w_{\nu^{*}}^{(\lambda)}(X,x) \cdot d^{2}(Y,y)\right].\]

Additionally, we define an auxiliary function \(\tilde{R}_{n}(y;x)\) as the "empirical risk with population weight" such that

\[\tilde{R}_{n}(y;x)\coloneqq\frac{1}{n}\sum_{i=1}^{n}w_{\nu^{*}}^{(\lambda)}(X _{i},x)\cdot d^{2}(Y_{i},y).\]

We present the rest of this proof in three steps, outlined as follows. In Step 1, we show the consistency of \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x)\), i.e., \(d\big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(\lambda) }(x)\big{)}=o_{P}(1)\) as \(n\to\infty\). In Step 2, we define the discrepancy variable \(Z_{n}^{(\lambda)}(y;x)\coloneqq R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{* }}^{(\lambda)}(y;x)\) between the finite-sample and the population objectives, cf. (35), and prove a uniform upper bound for \(Z_{n}^{(\lambda)}(y;x)\) that holds in a neighborhood of \(\varphi_{\nu^{*}}^{(\lambda)}(y;x)\). Lastly, in Step 3, we utilize the peeling technique from empirical process theory to obtain the desired rate of convergence.

Step 1: Consistency.We first claim that \(d\big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{*}}^{(\lambda) }(x)\big{)}=o_{P}(1)\) by an argument similar to that used in the proof of Theorem 1. Specifcally, it suffices to show that

* \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{(\lambda)}(y;x)=o_{P}(1)\), and
* \(R_{\mathcal{D}_{n}}^{(\lambda)}(\cdot;x):\mathcal{M}\to\mathbb{R}\) is asymptotically equicontinuous in probability.

Note that we already showed the asymptotic equicontinuity in the proof of Theorem 1; see (S2). Thus, it remains to show the pointwise convergence in probability. To show (S1'), we decompose \(R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{(\lambda)}(y;x)\) as follows.

\[R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{(\lambda)}(y;x) =\big{\{}R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-\tilde{R}_{n}(y;x) \big{\}}+\big{\{}\tilde{R}_{n}(y;x)-R_{\nu^{*}}^{(\lambda)}(y;x)\big{\}}\] \[=\underbrace{\frac{1}{n}\sum_{i=1}^{n}\big{\{}w_{\mathcal{D}_{n}}^ {(\lambda)}(X_{i},x)-w_{\nu^{*}}^{(\lambda)}(X_{i},x)\big{\}}\cdot d^{2}(Y_{i },y)\] \[\qquad+\underbrace{\frac{1}{n}\sum_{i=1}^{n}\Big{(}w_{\nu^{*}}^{( \lambda)}(X_{i},x)\cdot d^{2}(Y_{i},y)-\mathbb{E}_{\nu^{*}}\big{[}w_{\nu^{*}}^ {(\lambda)}(X_{i},x)\cdot d^{2}(Y_{i},y)\big{]}\Big{\}}}_{\coloneqq B_{n}^{( \lambda)}(y;x)}.\]

Next, we show that \(A_{n}^{(\lambda)}(y;x)\) and \(B_{n}^{(\lambda)}(y;x)\) respectively converge to \(0\) in probability.

* Letting \(\widehat{\mu}_{n}=\mu_{\mathcal{D}_{n}}\), \(\widehat{\Sigma}_{n}=\Sigma_{\mathcal{D}_{n}}\), and \(\widehat{\Sigma}_{n}^{(\lambda)}=\texttt{SVT}^{(\lambda)}(\widehat{\Sigma}_{n})\) for shorthand, we can write \[w_{\mathcal{D}_{n}}^{(\lambda)}(X_{i},x)-w_{\nu^{*}}^{(\lambda)}(X_{i},x)=V_ {n}^{(\lambda)}(x)+X_{i}^{\top}W_{n}^{(\lambda)}(x),\] similarly to (25), where \[V_{n}^{(\lambda)}(x) =-\widehat{\mu}_{n}^{\top}\Big{[}\widehat{\Sigma}_{n}^{(\lambda)} \Big{]}^{\dagger}(x-\widehat{\mu}_{n})+\mu^{\top}\Big{[}\Sigma^{(\lambda)} \Big{]}^{\dagger}(x-\mu),\] (33) \[W_{n}^{(\lambda)}(x) =\Big{[}\widehat{\Sigma}_{n}^{(\lambda)}\Big{]}^{\dagger}(x- \widehat{\mu}_{n})-\Big{[}\Sigma^{(\lambda)}\Big{]}^{\dagger}(x-\mu).\]Since \(\|\widehat{\mu}_{n}-\mu\|_{2}=O_{P}(n^{-1/2})\) and \(\|\widehat{\Sigma}_{n}^{(\lambda)}-\Sigma^{(\lambda)}\|=O_{P}(n^{-1/2})\) (if \(\lambda\not\in\operatorname{spec}\Sigma\)) independent of \(\lambda>0\), we also have \(|V_{n}^{(\lambda)}(x)|=O_{P}(n^{-1/2})\) and \(\|W_{n}^{(\lambda)}(x)\|_{2}=O_{P}(n^{-1/2})\). This implies that \(A_{n}^{(\lambda)}(y;x)=o_{P}(1)\).
* Moreover, we note that if \(\|x-\mu\|_{\Sigma}<\infty\), then the random variable \(w_{\nu^{*}}^{(\lambda)}(X,x)\) has finite second moment \[\begin{split}\mathbb{E}_{\nu^{*}}\left[w_{\nu^{*}}^{(\lambda)}(X,x)^{2}\right]&\leq 2\left(1+\mathbb{E}_{\nu^{*}}\left[\left|(X-\mu)^{ \top}\big{[}\Sigma^{(\lambda)}\big{]}^{\dagger}(x-\mu)\right|^{2}\right]\right) \\ &\leq 2\left(1+\mathbb{E}_{\nu^{*}}\left[\left\|X-\mu\right\|_{ \Sigma^{(\lambda)}}^{2}\cdot\left\|x-\mu\right\|_{\Sigma^{(\lambda)}}^{2} \right]\right)\\ &\leq 2\big{\{}1+p\,\|x-\mu\|_{\Sigma}^{2}\big{\}},\end{split}\] (34) regardless of the value of \(\lambda>0\). When \(\operatorname{diam}\left(\mathcal{M}\right)<\infty\), the product \(w_{\nu^{*}}^{(\lambda)}(X,x)\cdot d^{2}(Y,y)\) also has finite second moment. Since \(B_{n}^{(\lambda)}(y;x)\) is the sample mean of IID random variables with mean zero and finite variance, it follows that \[B_{n}^{(\lambda)}(y;x)=O_{P}\left(\sqrt{\frac{\operatorname{Var}\big{[}\,w_{ \nu^{*}}^{(\lambda)}(X_{1},x)\cdot d^{2}(Y_{1},y)\,\big{]}}{n}}\right)=O_{P} \big{(}n^{-1/2}\big{)}.\]

Step 2: Uniform control of the fluctuation in objective discrepancy.For any \(\lambda\in\mathbb{R}_{+}\) and any \((x,y)\in\mathbb{R}^{p}\times\mathcal{M}\), we let \(Z_{n}^{(\lambda)}(y;x)\) denote the random variable defined as

\[Z_{n}^{(\lambda)}(y;x)\coloneqq R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{ *}}^{(\lambda)}(y;x)\] (35)

We observed that

\[\begin{split} Z_{n}^{(\lambda)}&\big{(}y;x\big{)}-Z_ {n}^{(\lambda)}\big{(}\varphi_{\nu^{*}}^{(\lambda)}(x);x\big{)}\\ &=\Big{\{}R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-R_{\nu^{*}}^{( \lambda)}(y;x)\Big{\}}-\Big{\{}R_{\mathcal{D}_{n}}^{(\lambda)}(\varphi_{\nu^{ *}}^{(\lambda)}(x);x)-R_{\nu^{*}}^{(\lambda)}\big{(}\varphi_{\nu^{*}}^{( \lambda)}(x);x)\Big{\}}\\ &=\Big{\{}\Big{\{}R_{\mathcal{D}_{n}}^{(\lambda)}(y;x)-\tilde{R} _{n}(y;x)\Big{\}}-\Big{\{}R_{\mathcal{D}_{n}}^{(\lambda)}(\varphi_{\nu^{*}}^{( \lambda)}(x);x)-\tilde{R}_{n}\big{(}\varphi_{\nu^{*}}^{(\lambda)}(x);x)\Big{\}} \Big{]}\\ &\quad+\Big{[}\Big{\{}\tilde{R}_{n}(y;x)-R_{\nu^{*}}^{(\lambda)}(y ;x)\Big{\}}-\Big{\{}\tilde{R}_{n}\big{(}\varphi_{\nu^{*}}^{(\lambda)}(x);x \big{)}-R_{\nu^{*}}^{(\lambda)}\big{(}\varphi_{\nu^{*}}^{(\lambda)}(x);x)\Big{\}} \Big{]}\\ &=\underbrace{\frac{1}{n}\sum_{i=1}^{n}\Big{\{}w_{\mathcal{D}_{n }}^{(\lambda)}(X_{i},x)-w_{\nu^{*}}^{(\lambda)}(X_{i},x)\Big{\}}\cdot\ell_{i}^ {(\lambda)}(y;x)}_{=:\mathfrak{R}_{n}^{(\lambda)}(y;x)}\\ &\quad+\underbrace{\frac{1}{n}\sum_{i=1}^{n}\Big{(}w_{\nu^{*}}^{( \lambda)}(X_{i},x)\cdot\ell_{i}^{(\lambda)}(y;x)-\mathbb{E}_{\nu^{*}}\left[w_{ \nu^{*}}^{(\lambda)}(X_{i},x)\cdot\ell_{i}^{(\lambda)}(y;x)\right]\Big{)}}_{ =:\mathfrak{R}_{n}^{(\lambda)}(y;x)}\end{split}\] (36)

where \(\ell_{i}^{(\lambda)}(y;x)\coloneqq d^{2}\big{(}Y_{i},y\big{)}-d^{2}\big{(}Y_{i },\varphi_{\nu^{*}}^{(\lambda)}(x)\big{)}\).

Next, we analyze the asymptotic behavior of the two terms, \(\mathfrak{A}_{n}^{(\lambda)}(y;x)\) and \(\mathfrak{B}_{n}^{(\lambda)}(y;x)\). Specifically, we establish upper bounds on their magnitudes that hold uniformly over a \(\delta\)-neighborhood of \(\varphi^{(\lambda)}(x)=\varphi_{\nu^{*}}^{(\lambda)}(x)\), which will be used later in Step 3 of this proof.

* Firstly, we observe that for any \(\delta>0\), \[\sup_{y\in B_{d}\left(\varphi_{\nu^{\star}}^{(\lambda)}(x);\,\delta \right)}\big{|}\mathfrak{R}_{n}^{(\lambda)}(y;x)\big{|}\] \[\qquad\leq\frac{1}{n}\sum_{i=1}^{n}\big{|}w_{\mathcal{D}_{n}}^{( \lambda)}(X_{i},x)-w_{\nu^{\star}}^{(\lambda)}(X_{i},x)\big{|}\cdot\sup_{y\in B _{d}\left(\varphi_{\nu^{\star}}^{(\lambda)}(x);\,\delta\right)}\big{|}d^{2}(Y_ {i},y)-d^{2}(Y_{i},\varphi_{\nu^{\star}}^{(\lambda)}(x))\big{|}\] \[\qquad\leq 2\operatorname{diam}(\mathcal{M})\cdot\left\{\frac{1}{n} \sum_{i=1}^{n}\left\{|V_{n}^{(\lambda)}(x)|+\|X_{i}\|_{2}\,\|W_{n}^{(\lambda) }(x)\|_{2}\right\}\,\right\}\] \[\qquad\qquad\times\sup_{y\in B_{d}\left(\varphi_{\nu^{\star}}^{( \lambda)}(x);\,\delta\right)}d\big{(}y,\varphi_{\nu^{\star}}^{(\lambda)}(x) \big{)}\] \[=O_{P}\left(\delta\cdot n^{-1/2}\right),\] (37) where we used the property of \(V_{n}^{(\lambda)}(x)\) and \(W_{n}^{(\lambda)}(x)\) discussed in the paragraph following (33). Since the stochastic magnitudes of \(V_{n}^{(\lambda)}(x)\) and \(W_{n}^{(\lambda)}(x)\) are independent of \(\delta\), (37) implies that there exists \(C_{1}^{(\lambda)}=C_{1}^{(\lambda)}(x)>0\) such that for any \(\delta>0\), \[\liminf_{n\to\infty}P\left(\sup_{y\in\mathcal{M}}\left\{|\mathfrak{R}_{n}^{( \lambda)}(y;x)|:d\big{(}y,\varphi_{\nu^{\star}}^{(\lambda)}(x)\big{)}<\delta \right\}\leq C_{1}^{(\lambda)}\cdot\delta\cdot n^{-1/2}\right)=1.\] (38) Furthermore, for any \(\gamma,\delta\in\mathbb{R}_{+}\) such that \(0\leq\gamma<\delta\), let \(\mathfrak{E}_{n}^{(\lambda)}(\gamma,\delta;x)\) be defined as an event such that \[\mathfrak{E}_{n}(\gamma,\delta;x)=\left(\sup_{y\in\mathcal{M}}\left\{| \mathfrak{R}_{n}^{(\lambda)}(y;x)|:d\big{(}y,\varphi_{\nu^{\star}}^{(\lambda)} (x)\big{)}\in[\gamma,\delta)\right\}\leq C_{1}^{(\lambda)}\cdot\delta\cdot n^ {-1/2}\right).\] (39) For any \(\gamma\in[0,\delta]\), we have \(\mathfrak{E}_{n}(0,\delta;x)\subseteq\mathfrak{E}_{n}(\gamma,\delta;x)\), and thus, \(\liminf_{n\to\infty}P\big{(}\mathfrak{E}_{n}(\gamma,\delta;x)\big{)}=1\).
* Next, we note that \[\big{|}w_{\nu^{\star}}^{(\lambda)}(X_{i},x)\cdot\ell_{i}^{(\lambda)}(y;x) \big{|}\leq 2\operatorname{diam}(\mathcal{M})\cdot d\big{(}y,\varphi_{\nu^{ \star}}^{(\lambda)}(x)\big{)}\cdot\big{|}w_{\nu^{\star}}^{(\lambda)}(X_{i},x )\big{|}.\] Observe that \(d\big{(}y,\varphi_{\nu^{\star}}^{(\lambda)}(x)\big{)}\leq\operatorname{diam} \left(\mathcal{M}\right)<\infty\) and recall that \(\mathbb{E}_{\nu^{\star}}\left[w_{\nu^{\star}}^{(\lambda)}(X,x)^{2}\right] \leq 2\big{\{}1+p\,\|x-\mu\|_{\Sigma}^{2}\big{\}}\) as shown in Step 1 of this proof, cf. (34). It follows from the uniform entropy condition (C2), Theorem 2.7.11, and Theorem 2.14.2 in [55] that there exists \(D_{\mathfrak{e}}=D_{\mathfrak{e}}(x)>0\) such that for all \(\delta\in[0,D_{\mathfrak{e}})\), \[\mathbb{E}\bigg{[} \sup_{y\in\mathcal{M}}\left\{\big{|}\mathfrak{B}_{n}^{(\lambda)}(y;x )\big{|}:d\big{(}y,\varphi_{\nu^{\star}}^{(\lambda)}(x)\big{)}<\delta\right\} \bigg{]}\] \[\qquad\leq 2\operatorname{diam}(\mathcal{M})\cdot\delta\cdot n ^{-1/2}\sqrt{1+p\,\|x-\mu\|_{\Sigma}^{2}}\int_{0}^{1}\sqrt{1+\log\mathfrak{N} \big{(}B_{d}(\varphi^{(\lambda)}(x);\,\delta),\delta\epsilon\big{)}}\, \mathrm{d}\epsilon\] \[\qquad\leq C_{2}^{(\lambda)}\cdot\delta\cdot n^{-1/2}\] (40) where \(C_{2}^{(\lambda)}=2\left(C_{\mathfrak{e}}+1\right)\cdot\operatorname{diam} \left(\mathcal{M}\right)\cdot\sqrt{1+p\,\|x-\mu\|_{\Sigma}^{2}}\) is independent of \(\delta>0\) and \(n\geq 1\).

Step 3: Concluding the proof.Lastly, we combine the results from Steps 1-2 to show that, for any \(\eta>0\), there exist \(K=K(\eta)>0\) and \(N=N(\eta)\geq 1\) such that \(P\Big{(}d\big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{\star} }^{(\lambda)}(x)\big{)}>2^{K}\,n^{-\beta}\Big{)}<\eta\) for any \(n\geq N\), where \(\beta>0\) is an absolute constant that will be determined later in this proof. We prove this claim using the peeling technique, in a similar manner as we did in the proof of Lemma 1. To avoid cluttered notation, we let \(\Delta(x)=d\big{(}\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x),\varphi_{\nu^{\star} }^{(\lambda)}(x)\big{)}\) in the rest of this proof.

For any fixed \(K\in\mathbb{N}\) and a sufficiently large \(n=n(K)\geq 1\) satisfying \(2^{K}n^{-\beta}<D_{*}\coloneqq D_{\mathfrak{g}}\wedge D_{\mathfrak{e}}\), we observe that

\[P\Big{(}\Delta(x)>2^{K}\,n^{-\beta}\Big{)}=P\Big{(}\Delta(x)\geq D_{*}\Big{)}+P \Big{(}2^{K}\,n^{-\beta}\leq\Delta(x)<D_{*}\Big{)}\] (41)

where we used \(P(A)\leq P(B^{c})+P(A\cap B)\) to get the inequality. As we know that \(P\Big{(}\Delta(x)\geq D_{*}\Big{)}=o(1)\) by Step 1 of this proof, we focus on showing an upper bound for the other term, \(P\big{(}2^{K}\,n^{-\beta}\leq\Delta(x)<D_{*}\big{)}\).

_Step 3-A: Decomposition of \(P\big{(}2^{K}\,n^{-\beta}\leq\Delta(x)<D_{*}\big{)}\)._ For each \(n,k\in\mathbb{N}\), we define

\[\mathfrak{F}_{n,k} =\bigcap_{k^{\prime}=K}^{k}\mathfrak{E}_{n}^{(\lambda)}\big{(}2^ {k^{\prime}}n^{-\beta},\,2^{k^{\prime}+1}n^{-\beta}\wedge D_{*};x\big{)},\] (42) \[\mathfrak{G}_{n,k} =\bigg{(}\bigcap_{k^{\prime}=K}^{k-1}\mathfrak{E}_{n}^{(\lambda )}\big{(}2^{k^{\prime}}n^{-\beta},\,2^{k^{\prime}+1}n^{-\beta}\wedge D_{*};x \big{)}\bigg{)}\cap\mathfrak{E}_{n}^{(\lambda)}\big{(}2^{k}n^{-\beta},\,2^{k+ 1}n^{-\beta}\wedge D_{*};x\big{)}^{c},\]

where we set \(\mathfrak{F}_{n,K-1}\) to be the entire event space so that \(\mathfrak{G}_{n,K}=\big{(}\mathfrak{F}_{n,K}\big{)}^{c}\). It is worth mentioning that \(\mathfrak{G}_{n,k}\) and \(\mathfrak{G}_{n,k^{\prime}}\) are mutually exclusive for any \(k\neq k^{\prime}\geq K\), and we will use this property when concluding the proof in Step 3-C below.

Now, we observe that

\[P\Big{(}2^{K}\,n^{-\beta}\leq\Delta(x)<D_{*}\Big{)}\] (43) \[\qquad\leq P\Big{(}\mathfrak{E}_{n}^{(\lambda)}\big{(}2^{K}n^{- \beta},\,2^{K+1}n^{-\beta}\wedge D_{*};x\big{)}^{c}\Big{)}\] \[\qquad\qquad+P\bigg{(}\Big{(}2^{K}\,n^{-\beta}\leq\Delta(x)<D_{*} \Big{)}\cap\mathfrak{E}_{n}^{(\lambda)}\big{(}2^{k^{\prime}}n^{-\beta},\,2^{ k^{\prime}+1}n^{-\beta}\wedge D_{*};x\big{)}\bigg{)}\] \[\qquad\qquad=P\big{(}\mathfrak{G}_{n,K}\big{)}+P\bigg{(}\Big{(}2 ^{K}\,n^{-\beta}\leq\Delta(x)<D_{*}\Big{)}\cap\mathfrak{F}_{n,K}\bigg{)}\] \[\qquad\qquad+P\bigg{(}\Big{(}2^{K+1}\,n^{-\beta}\leq\Delta(x)<D_{ *}\Big{)}\cap\mathfrak{F}_{n,K}\bigg{)}\]

and that for every \(k\geq K\),

\[P\bigg{(}\Big{(}2^{k+1}\,n^{-\beta}\leq\Delta(x)<D_{*}\Big{)}\cap\mathfrak{F} _{n,k}\bigg{)}\leq P\bigg{(}\Big{(}2^{k+1}\,n^{-\beta}\leq\Delta(x)<D_{*} \Big{)}\cap\mathfrak{F}_{n,k+1}\bigg{)}+P\big{(}\mathfrak{G}_{n,k+1}).\]

As a result, we have

\[P\Big{(}2^{K}\,n^{-\beta}\leq\Delta(x)<D_{*}\Big{)}=\sum_{k=K}^{\infty}P\big{(} \mathfrak{G}_{n,k}\big{)}+\sum_{k=K}^{\infty}\underbrace{P\bigg{(}\Big{(}2^{ k}\,n^{-\beta}\leq\Delta(x)<2^{k+1}\,n^{-\beta}\wedge D_{*}\Big{)}\cap \mathfrak{F}_{n,k}\bigg{)}}_{=:\mathfrak{C}_{n,k}}.\]

[MISSING_PAGE_FAIL:26]

### Useful lemmas

**Definition 7**.: _Let \(n,p\in\mathbb{N}\) and let \(\bm{M}\in\mathbb{R}^{n\times p}\). The row projection matrix for \(\bm{M}\), denoted by \(\Pi_{\bm{M}}^{\mathrm{row}}\in\mathbb{R}^{p\times p}\), is a matrix such that_

\[\Pi_{\bm{M}}^{\mathrm{row}}\coloneqq\bm{M}^{\dagger}\cdot\bm{M}.\] (46)

_and the column projection matrix for \(\bm{M}\), denoted by \(\Pi_{\bm{M}}^{\mathrm{col}}\in\mathbb{R}^{n\times n}\), is a matrix such that_

\[\Pi_{\bm{M}}^{\mathrm{col}}\coloneqq\bm{M}\cdot\bm{M}^{\dagger}.\] (47)

We recall from (6) that for any \(\lambda\in\mathbb{R}_{+}\), the singular value thresholding (SVT) operator \(\texttt{SVT}^{(\lambda)}\) is defined such that

\[\bm{M}=\sum_{i=1}^{\min\{n,p\}}s_{i}\cdot u_{i}v_{i}^{\top}\text{ is a SVD}\qquad\mapsto\qquad\texttt{SVT}^{(\lambda)}(\bm{M})=\sum_{i=1}^{\min\{n,p\}}s_{i} \cdot\mathds{1}\{s_{i}>\lambda\}\cdot u_{i}v_{i}^{\top}.\]

In the rest of this section, we let \(\bm{M}^{(\lambda)}\coloneqq\texttt{SVT}^{(\lambda)}(\bm{M})\) for shorthand.

**Lemma 3** (Properties of the row/column projection matrices).: _Let \(n,p\in\mathbb{N}\), and \(\bm{M}\in\mathbb{R}^{n\times p}\). For any \(\lambda\in\mathbb{R}_{+}\), the following statements are true._

1. \(\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{row}}\) _defines a projection in_ \(\mathbb{R}^{p}\) _and_ \(\operatorname{rank}\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{row}}=\operatorname{ rank}\bm{M}^{(\lambda)}\)_._
2. \(\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{col}}\) _defines a projection in_ \(\mathbb{R}^{n}\) _and_ \(\operatorname{rank}\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{col}}=\operatorname{ rank}\bm{M}^{(\lambda)}\)_._
3. \(\bm{M}\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{row}}\bm{M}^{\dagger}=\Pi_{\bm{M}^{( \lambda)}}^{\mathrm{col}}\) _and_ \(\bm{M}^{\dagger}\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{col}}\bm{M}=\Pi_{\bm{M}^{( \lambda)}}^{\mathrm{row}}\)_._

Proof.: Let \(r=\operatorname{rank}\bm{M}\) and consider a compact singular value decomposition (SVD) of \(\bm{M}\):

\[\bm{M}=\sum_{i=1}^{r}s_{i}\cdot u_{i}v_{i}^{\top}\]

where \(s_{1},\ldots,s_{r}\) are non-zero singular values of \(\bm{M}\). Noticing that

\[\bm{M}^{(\lambda)}=\texttt{SVT}^{(\lambda)}(\bm{M})=\sum_{i=1}^{r}\mathds{1} \{s_{i}>\lambda\}\cdot u_{i}v_{i}^{\top}\]

and that \(\bm{M}^{\dagger}=\sum_{i=1}^{r}s_{i}^{-1}\cdot v_{i}u_{i}^{\top}\), the three conclusions of the lemma follow straightforwardly from the orthonormality of singular vectors.

* \(\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{row}}=\sum_{i=1}^{r}v_{i}v_{i}^{\top}\cdot \mathds{1}\{s_{i}>\lambda\}\) is the projection onto the row space of \(\bm{M}^{(\lambda)}\).
* \(\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{col}}=\sum_{i=1}^{r}u_{i}u_{i}^{\top}\cdot \mathds{1}\{s_{i}>\lambda\}\) is the projection onto the column space of \(\bm{M}^{(\lambda)}\).
* Due to the orthonormality of singular vectors, \[\bm{M}\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{row}}\bm{M}^{\dagger} =\left(\sum_{i=1}^{r}s_{i}\cdot u_{i}v_{i}^{\top}\right)\left( \sum_{i=1}^{r}v_{i}v_{i}^{\top}\cdot\mathds{1}\{s_{i}>\lambda\}\right)\left( \sum_{i=1}^{r}s_{i}^{-1}\cdot v_{i}u_{i}^{\top}\right)\] \[=\sum_{i=1}^{r}u_{i}u_{i}^{\top}\cdot\mathds{1}\{s_{i}>\lambda\}\] \[=\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{col}},\] and likewise, \[\bm{M}^{\dagger}\Pi_{\bm{M}^{(\lambda)}}^{\mathrm{col}}\bm{M}=\Pi_{\bm{M}^{( \lambda)}}^{\mathrm{row}}.\]

In addition, we collect two classical results from matrix perturbation theory and state them as lemmas.

**Lemma 4** ([50, Theorem 3.2]).: _Let \(\bm{X},\bm{Z}\in\mathbb{R}^{n\times p}\). Then the following equation is true:_

\[\bm{Z}^{\dagger}-\bm{X}^{\dagger}=-\bm{Z}^{\dagger}\Pi_{\bm{Z}}^{ \mathrm{col}}(\bm{Z}-\bm{X})\Pi_{\bm{X}}^{\mathrm{row}}\bm{X}^{\dagger}+\bm{Z} ^{\dagger}\Pi_{\bm{Z}}^{\mathrm{col}}{\Pi_{\bm{X}}^{\mathrm{col}}}^{\perp}-\Pi _{\bm{Z}}^{\mathrm{row}}{}^{\perp}\Pi_{\bm{X}}^{\mathrm{row}}\bm{X}^{\dagger}\] (48)

_where \(\Pi_{\bm{X}}^{\mathrm{col}}{}^{\perp}=\bm{I}_{n}-\Pi_{\bm{X}}^{\mathrm{col}}\) and \(\Pi_{\bm{Z}}^{\mathrm{row}}{}^{\perp}=\bm{I}_{p}-\Pi_{\bm{Z}}^{\mathrm{row}}\)._

**Lemma 5** ([15, Theorems 2.4 & 2.5]).: _Let \(\bm{X},\bm{Z}\in\mathbb{R}^{n\times p}\). Then_

\[\left\|\Pi_{\bm{Z}}^{\mathrm{col}}-\Pi_{\bm{X}}^{\mathrm{col}} \right\|\leq\max\left\{\left\|(\bm{Z}-\bm{X})\bm{X}^{\dagger}\right\|,\left\| (\bm{Z}-\bm{X})\bm{Z}^{\dagger}\right\|\right\}.\] (49)

_Moreover, if \(\mathrm{rank}\,\bm{X}=\mathrm{rank}\,\bm{Z}\), then_

\[\left\|\Pi_{\bm{Z}}^{\mathrm{col}}-\Pi_{\bm{X}}^{\mathrm{col}} \right\|\leq\min\left\{\left\|(\bm{Z}-\bm{X})\bm{X}^{\dagger}\right\|,\left\| (\bm{Z}-\bm{X})\bm{Z}^{\dagger}\right\|\right\}.\] (50)

### Stability of the weights under (small) perturbation in covariates

Let \(\mathcal{D}_{n}=\{(x_{i},y_{i})\in\mathbb{R}^{p}\times\mathcal{M}:i\in[n]\}\) and \(\widetilde{\mathcal{D}}_{n}=\{(z_{i},y_{i})\in\mathbb{R}^{p}\times\mathcal{M}:i \in[n]\}\) be two sets in \(\mathbb{R}^{p}\times\mathcal{M}\). We may identify these sets with their empirical distributions. Recall the definition of \(w_{\nu}^{(\lambda)}\) from (9): for any probability measure \(\nu\) on \(\mathbb{R}^{p}\times\mathcal{M}\), any \(\lambda\in\mathbb{R}_{+}\), and any \(x,x^{\prime}\in\mathbb{R}^{p}\),

\[w_{\nu}^{(\lambda)}(x^{\prime},x)=1+(x^{\prime}-\mu_{\nu})^{ \top}\left[\mathtt{SVT}^{(\lambda)}\big{(}\Sigma_{\nu}\big{)}\right]^{\dagger} (x-\mu_{\nu})\]

where \(\mu_{\nu}=\mathbb{E}_{(X,Y)\sim\nu}(X)\) and \(\Sigma_{\nu}=\mathrm{Var}_{(X,Y)\sim\nu}(X)\), cf. (7). We define the _weight vectors_ induced by \(\mathcal{D}_{n}\) and \(\widetilde{\mathcal{D}}_{n}\) as follows: for any \(\lambda\in\mathbb{R}_{+}\) and any \(x\in\mathbb{R}^{p}\),

\[\begin{split}\bar{w}_{\mathcal{D}_{n}}^{(\lambda)}(x)& \coloneqq\left[w_{\mathcal{D}_{n}}^{(\lambda)}(x_{1},x)\quad \cdots\quad w_{\mathcal{D}_{n}}^{(\lambda)}(x_{n},x)\right]\in\mathbb{R}^{n}, \\ \bar{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)& \coloneqq\left[w_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(z_{1},x)\quad \cdots\quad w_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(z_{n},x)\right]\in \mathbb{R}^{n}.\end{split}\] (51)

**Lemma 6** (Stability of weights).: _Let \(\mathcal{D}_{n}=\{(x_{i},y_{i})\in\mathbb{R}^{p}\times\mathcal{M}:i\in[n]\}\) and \(\widetilde{\mathcal{D}}_{n}=\{(z_{i},y_{i})\in\mathbb{R}^{p}\times\mathcal{M}:i \in[n]\}\). Let \(\bm{X}=[x_{1}\quad\cdots\quad x_{n}]^{\top}\in\mathbb{R}^{n\times p}\) and \(\bm{Z}=[z_{1}\quad\cdots\quad z_{n}]^{\top}\in\mathbb{R}^{n\times p}\). For any \(\lambda\in\mathbb{R}_{+}\), if \(x\in\mathbb{R}^{p}\) satisfies \(x-\mu_{\mathcal{D}_{n}}\in\mathrm{rowsp}\left(\bm{X}_{\mathrm{ctr}}\right)\), then_

\[\left\|\bar{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)-\bar{w}_{ \mathcal{D}_{n}}^{(\lambda)}(x)\right\|\leq\frac{\sqrt{n}\cdot\|\bm{Z}-\bm{X} \|}{\min\left\{\sigma^{(\lambda)}(\bm{X}_{\mathrm{ctr}}),\;\sigma^{(\lambda)} (\bm{Z}_{\mathrm{ctr}})\right\}}\cdot\left(2\cdot\left\|x-\mu_{\mathcal{D}_{n }}\right\|_{\Sigma_{\mathcal{D}_{n}}}+1\right)\] (52)

_where \(\bm{X}_{\mathrm{ctr}}=\left(\bm{I}_{n}-\frac{1}{n}\bm{1}_{n}\bm{1}_{n}^{\top} \right)\bm{X}\) and \(\sigma^{(\lambda)}(\bm{X})\coloneqq\inf\{\sigma_{i}(\bm{X})>\lambda:i\in \mathbb{N}\}\) (likewise for \(\bm{Z}\))._

Proof of Lemma 6.: This proof consists of three steps. In Step 1, we express the weight discrepancy \(\bar{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)-\bar{w}_{\mathcal{D}_{n} }^{(\lambda)}(x)\) as a sum of matrix products using projections. In Step 2, we establish upper bounds on the norm of the expression obtained in Step 1. In Step 3, we collect intermediate results together and conclude the proof.

Step 1: Decomposition of the weight discrepancy. First of all, we rewrite \(\bar{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)-\bar{w}_{\mathcal{D}_{n} }^{(\lambda)}(x)\) in a compact matrix representation that is presented in (60) at the end of this step. To this end, we begin by observing that

\[\mu_{\mathcal{D}_{n}}=\frac{1}{n}\bm{X}^{\top}\bm{1}_{n},\qquad \text{and}\qquad\Sigma_{\mathcal{D}_{n}}=\frac{1}{n}\left(\bm{X}-\bm{1}_{n} \mu_{\mathcal{D}_{n}}^{\top}\right)^{\top}\left(\bm{X}-\bm{1}_{n}\mu_{ \mathcal{D}_{n}}^{\top}\right)=\frac{1}{n}\bm{X}_{\mathrm{ctr}}^{\top}\bm{X}_{ \mathrm{ctr}}.\] (53)

For given \(\lambda\in\mathbb{R}_{+}\), we let \(\bm{X}_{\mathrm{ctr}}^{(\lambda)}\coloneqq\mathtt{SVT}^{(\lambda)}(\bm{X}_{ \mathrm{ctr}})\), and observe that

\[\Sigma_{\mathcal{D}_{n}}^{(\lambda)}=\Pi_{\bm{X}_{\mathrm{ctr}}^{( \lambda)}}^{\mathrm{row}}\cdot\left(\frac{1}{n}\bm{X}_{\mathrm{ctr}}^{\top}\bm{X}_ {\mathrm{ctr}}\right)\cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}= \frac{1}{n}\cdot\bm{X}_{\mathrm{ctr}}^{(\lambda)}{}^{\top}\cdot\bm{X}_{\mathrm{ ctr}}^{(\lambda)}.\] (54)

Then it follows that

\[\left[\Sigma_{\mathcal{D}_{n}}^{(\lambda)}\right]^{\dagger}=n\cdot \left[\bm{X}_{\mathrm{ctr}}^{(\lambda)}{}^{\top}\cdot\bm{X}_{\mathrm{ctr}}^{( \lambda)}\right]^{\dagger}=n\cdot\left[\bm{X}_{\mathrm{ctr}}^{(\lambda)}{}^{ \dagger}\cdot\left[\bm{X}_{\mathrm{ctr}}^{(\lambda)}{}^{\top}\right]^{\dagger}=n \cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}\cdot\bm{X}_{ \mathrm{ctr}}^{\dagger}\cdot\left(\bm{X}_{\mathrm{ctr}}^{\top}\right)^{\dagger} \cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}.\]Therefore, we have

\[\begin{split}\vec{w}_{\mathcal{D}_{n}}^{(\lambda)}(x)&= \mathbf{1}_{n}+\left(\bm{X}-\mathbf{1}_{n}\mu_{\mathcal{D}_{n}}^{\top}\right) \cdot\left[\Sigma_{\mathcal{D}_{n}}^{(\lambda)}\right]^{\dagger}\cdot(x-\mu_{ \mathcal{D}_{n}})\\ &=\mathbf{1}_{n}+n\cdot\bm{X}_{\mathrm{ctr}}\cdot\Pi_{\bm{X}_{ \mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}\cdot\bm{X}_{\mathrm{ctr}}^{\top} \cdot\left(\bm{X}_{\mathrm{ctr}}^{\top}\right)^{\dagger}\cdot\Pi_{\bm{X}_{ \mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}\cdot(x-\mu_{\mathcal{D}_{n}})\\ &=\mathbf{1}_{n}+n\cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{ \mathrm{col}}\cdot\left(\bm{X}_{\mathrm{ctr}}^{\top}\right)^{\dagger}\cdot\Pi_ {\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}\cdot(x-\mu_{\mathcal{D}_{n }}),\end{split}\] (55)

where the equality in the last line follows from Lemma 3: \(\bm{X}_{\mathrm{ctr}}\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}} \bm{X}_{\mathrm{ctr}}^{\dagger}=\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{ \mathrm{col}}\).

Likewise, we repeat the above for \(\widetilde{\mathcal{D}}_{n}\) and \(\bm{Z}\) to write

\[\mu_{\widetilde{\mathcal{D}}_{n}}=\frac{1}{n}\bm{Z}^{\top}\mathbf{1}_{n}\qquad \text{ and }\qquad\Sigma_{\widetilde{\mathcal{D}}_{n}}=\frac{1}{n}\bm{Z}_{ \mathrm{ctr}}^{\top}\bm{Z}_{\mathrm{ctr}}.\]

Then, we obtain an expression for \(\vec{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)\) in a similar form to (55), namely,

\[\vec{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)=\mathbf{1}_{n}+n\cdot\Pi_ {\bm{Z}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{col}}\cdot\left(\bm{Z}_{\mathrm{ ctr}}^{\top}\right)^{\dagger}\cdot\Pi_{\bm{Z}_{\mathrm{ctr}}^{(\lambda)}}^{ \mathrm{row}}\cdot(x-\mu_{\widetilde{\mathcal{D}}_{n}}).\] (56)

Thereafter, we define \(c_{x},\tilde{c}_{x}\in\mathbb{R}^{n\times 1}\) so that

\[\begin{split} c_{x}&=\left\|x-\mu_{\mathcal{D}_{n}} \right\|_{\Sigma_{\mathcal{D}_{n}}}=\left(\frac{1}{\sqrt{n}}\bm{X}_{\mathrm{ ctr}}^{\top}\right)^{\dagger}\cdot(x-\mu_{\mathcal{D}_{n}})\qquad\text{ and }\\ \tilde{c}_{x}&=\left\|x-\mu_{\mathcal{D}_{n}} \right\|_{\Sigma_{\mathcal{D}_{n}}}=\left(\frac{1}{\sqrt{n}}\bm{Z}_{\mathrm{ ctr}}^{\top}\right)^{\dagger}\cdot\left(x-\mu_{\widetilde{\mathcal{D}}_{n}} \right).\end{split}\] (57)

Then we observe that for any \(x\in\mathbb{R}^{p}\),

\[n\cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{\mathrm{row}}}^{\mathrm{row}}\cdot(x-\mu_{ \mathcal{D}_{n}})=n\cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}} \cdot\frac{1}{\sqrt{n}}\bm{X}_{\mathrm{ctr}}^{\top}\cdot\left(\frac{1}{\sqrt{n} }\bm{X}_{\mathrm{ctr}}^{\top}\right)^{\dagger}\cdot(x-\mu_{\mathcal{D}_{n}})= \sqrt{n}\cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}\cdot\bm{X}_ {\mathrm{ctr}}^{\top}\cdot c_{x}.\] (58)

Likewise,

\[n\cdot\Pi_{\bm{Z}_{\mathrm{ctr}}^{\mathrm{row}}}^{\mathrm{row}}\cdot\left(x-\mu_ {\widetilde{\mathcal{D}}_{n}}\right)=n\cdot\Pi_{\bm{Z}_{\mathrm{ctr}}^{( \lambda)}}^{\mathrm{row}}\cdot\frac{1}{\sqrt{n}}\bm{Z}_{\mathrm{ctr}}^{\top} \cdot\left(\frac{1}{\sqrt{n}}\bm{Z}_{\mathrm{ctr}}^{\top}\right)^{\dagger} \cdot(x-\mu_{\widetilde{\mathcal{D}}_{n}})=\sqrt{n}\cdot\Pi_{\bm{Z}_{\mathrm{ ctr}}^{\top}}^{\mathrm{row}}\cdot\bm{Z}_{\mathrm{ctr}}^{\top}\cdot\tilde{c}_{x}.\] (59)

Consequently, for any \(x\in\mathbb{R}^{p}\), we obtain from (55) and (56) with aid of (58) and (59) that

\[\begin{split}\vec{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}& (x)-\vec{w}_{\mathcal{D}_{n}}^{(\lambda)}(x)\\ &=\sqrt{n}\cdot\Pi_{\bm{Z}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{ col}}\cdot\left(\bm{Z}_{\mathrm{ctr}}^{\top}\right)^{\dagger}\cdot\Pi_{\bm{Z}_{ \mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}\cdot\bm{Z}_{\mathrm{ctr}}^{\top} \cdot\tilde{c}_{x}-\sqrt{n}\cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{ col}}\cdot\left(\bm{X}_{\mathrm{ctr}}^{\top}\right)^{\dagger}\cdot\Pi_{\bm{X}_{ \mathrm{ctr}}^{(\lambda)}}^{\mathrm{row}}\cdot\bm{X}_{\mathrm{ctr}}^{\top} \cdot c_{x}\\ &=\sqrt{n}\cdot\Pi_{\mathrm{ctr}}^{\mathrm{col}}\cdot\tilde{c}_{x}- \sqrt{n}\cdot\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{col}}\cdot c_{x} \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ddots\text{ Lemma \ref{lem:ctr}}\\ &=\sqrt{n}\cdot\Pi_{\bm{Z}_{\mathrm{ctr}}^{\mathrm{col}}}^{\mathrm{ col}}\cdot(\tilde{c}_{x}-c_{x})+\sqrt{n}\cdot\left(\Pi_{\bm{Z}_{\mathrm{ctr}}^{ (\lambda)}}^{\mathrm{col}}-\Pi_{\bm{X}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{ col}}\right)\cdot c_{x}.\end{split}\] (60)

By triangle inequality, we obtain the following upper bound:

\[\left\|\vec{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)-\vec{w}_{\mathcal{D}_{ n}}^{(\lambda)}(x)\right\|\leq\sqrt{n}\cdot\left\|\Pi_{\bm{Z}_{\mathrm{ctr}}^{( \lambda)}}^{\mathrm{col}}\cdot(\tilde{c}_{x}-c_{x})\right\|+\sqrt{n}\cdot\left\| \left(\Pi_{\bm{Z}_{\mathrm{ctr}}^{(\lambda)}}^{\mathrm{col}}-\Pi_{\bm{X}_{ \mathrm{ctr}}^{(\lambda)}}^{\mathrm{col}}\right)\cdot c_{x}\right\|.\] (61)

Step 2: Upper bounding the norm.Next, we establish separate upper bounds for the two terms in (61).

(1) The first term in (61).First of all, we observe from the definition of \(c_{x}\) and \(\tilde{c}_{x}\), cf. (57), that

\[\begin{split}\tilde{c}_{x}-c_{x}&=\left(\frac{1}{ \sqrt{n}}\bm{Z}_{\mathrm{ctr}}^{\top}\right)^{\dagger}\cdot\left(x-\mu_{ \widetilde{\mathcal{D}}_{n}}\right)-\left(\frac{1}{\sqrt{n}}\bm{X}_{\mathrm{ ctr}}^{\top}\right)^{\dagger}\cdot(x-\mu_{\mathcal{D}_{n}})\\ &=\sqrt{n}\cdot\left(\bm{Z}_{\mathrm{ctr}}^{\top}{}^{\dagger}- \bm{X}_{\mathrm{ctr}}^{\top}{}^{\dagger}\right)\cdot(x-\mu_{\mathcal{D}_{n}})+ \sqrt{n}\cdot\left[\bm{Z}_{\mathrm{ctr}}^{\top}\right]^{\dagger}\cdot\left(\mu_{ \widetilde{\mathcal{D}}_{n}}-\mu_{\mathcal{D}_{n}}\right).\end{split}\]

[MISSING_PAGE_FAIL:30]

### Completing the proof of Theorem 3

Recall that given a set \(\mathcal{D}_{n}=\left\{(x_{i},y_{i}):i\in[n]\right\}\), we let \(\boldsymbol{X}_{\mathcal{D}_{n}}\coloneqq\left[x_{1}\quad\cdots\quad x_{n} \right]^{\top}\in\mathbb{R}^{n\times p}\). In addition, we let

\[\forall y\in\mathcal{M},\;\;\vec{d}_{\mathcal{D}_{n}}^{2}(y)\coloneqq\left[d^{2 }(y_{1},y)\quad\cdots\quad d^{2}(y_{n},y)\right]\in\mathbb{R}^{n}.\] (66)

Recall that we let \(\boldsymbol{X}=\boldsymbol{X}_{\mathcal{D}_{n}}\) and \(\boldsymbol{Z}=\boldsymbol{X}_{\widetilde{\mathcal{D}}_{n}}\) for shorthand, and further, we let \(\boldsymbol{X}_{\mathrm{ctr}}=\left(\boldsymbol{I}_{n}-\frac{1}{n}\boldsymbol{ 1}_{n}\boldsymbol{1}_{n}^{\top}\right)\boldsymbol{X}\) and \(\boldsymbol{Z}_{\mathrm{ctr}}=\left(\boldsymbol{I}_{n}-\frac{1}{n}\boldsymbol {1}_{n}\boldsymbol{1}_{n}^{\top}\right)\boldsymbol{Z}\) denote the 'row-centered' matrices. Here we present and prove the complete version of Theorem 3.

**Theorem 4** (De-noising covariates).: _Suppose that Assumptions (C0) and (C1) hold. For any \(\lambda\in\mathbb{R}_{+}\), if \(x\in\mu_{\mathcal{D}_{n}}+\mathrm{rowsp}\,\boldsymbol{X}_{\mathrm{ctr}}\) and_

\[\|x-\mu_{\mathcal{D}_{n}}\|_{\Sigma_{\mathcal{D}_{n}}}\leq\frac{1}{2}\left( \frac{C_{\boldsymbol{g}}\cdot D_{\boldsymbol{g}}^{\alpha}}{2\,\mathrm{diam} \left(\mathcal{M}\right)}\cdot\frac{\min\left\{\sigma^{(\lambda)}(\boldsymbol {X}_{\mathrm{ctr}}),\;\sigma^{(\lambda)}(\boldsymbol{Z}_{\mathrm{ctr}})\right\} }{\|\boldsymbol{Z}-\boldsymbol{X}\|}-1\right),\] (67)

_then_

\[d\left(\varphi_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x), \varphi_{\mathcal{D}_{n}}^{(\lambda)}(x)\right)\] \[\qquad\leq\left(\frac{\|\boldsymbol{Z}-\boldsymbol{X}\|}{\min \left\{\sigma^{(\lambda)}(\boldsymbol{X}_{\mathrm{ctr}}),\;\sigma^{(\lambda)}( \boldsymbol{Z}_{\mathrm{ctr}})\right\}}\cdot\frac{2\cdot\left\|x-\mu_{ \mathcal{D}_{n}}\right\|_{\Sigma_{\mathcal{D}_{n}}}+1}{C_{\boldsymbol{g}}} \cdot\frac{\left\|\vec{d}_{\mathcal{D}_{n}}^{2}(\tilde{\varphi}_{n})\right\| +\left\|\vec{d}_{\mathcal{D}_{n}}^{2}(\varphi_{n})\right\|}{\sqrt{n}}\right)^ {\frac{1}{\alpha}}.\] (68)

Proof of Theorem 4.: First of all, we recall from (51) that

\[\vec{w}_{\mathcal{D}_{n}}^{(\lambda)}(x)=\left[w_{\mathcal{D}_{n}}^{(\lambda) }(x_{1},x)\quad\cdots\quad w_{\mathcal{D}_{n}}^{(\lambda)}(x_{n},x)\right] \qquad\text{and}\qquad\vec{w}_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)= \left[w_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(z_{1},x)\quad\cdots\quad w_ {\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(z_{n},x)\right].\]

In addition, recall that we let for any \(y\in\mathcal{M}\),

\[\vec{d}_{\mathcal{D}_{n}}^{2}(y)=\left[d^{2}(y_{1},y)\quad\cdots\quad d^{2}(y _{n},y)\right]\in\mathbb{R}^{n}.\]

Thereafter, we observe that for any \(y\in\mathcal{M}\) and any \(x\in\left(\mu_{\mathcal{D}_{n}}+\mathrm{rowsp}\,\boldsymbol{X}_{\mathrm{ctr}}\right)\),

\[\left|R_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(y;x)-R_{ \mathcal{D}_{n}}^{(\lambda)}(y;x)\right| =\frac{1}{n}\left|\sum_{i=1}^{n}\left(w_{\widetilde{\mathcal{D}}_ {n}}^{(\lambda)}(z_{i},x)-w_{\mathcal{D}_{n}}^{(\lambda)}(x_{i},x)\right)\cdot d ^{2}(y_{i},y)\right|\] \[=\frac{1}{n}\left|\left\langle\bar{w}_{\widetilde{\mathcal{D}}_{n} }^{(\lambda)}(x)-\bar{w}_{\mathcal{D}_{n}}^{(\lambda)}(x),\;\vec{d}_{\mathcal{ D}_{n}}^{2}(y)\right\rangle\right|\] \[\stackrel{{(a)}}{{\leq}}\frac{1}{n}\left\|\bar{w}_{ \widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)-\bar{w}_{\mathcal{D}_{n}}^{( \lambda)}(x)\right\|\cdot\left\|\vec{d}_{\mathcal{D}_{n}}^{2}(y)\right\|\] \[\stackrel{{(b)}}{{\leq}}\frac{\|\boldsymbol{Z}- \boldsymbol{X}\|}{\min\left\{\sigma^{(\lambda)}(\boldsymbol{X}_{\mathrm{ctr}}), \;\sigma^{(\lambda)}(\boldsymbol{Z}_{\mathrm{ctr}})\right\}}\cdot\left(2\cdot \left\|x-\mu_{\mathcal{D}_{n}}\right\|_{\Sigma_{\mathcal{D}_{n}}}+1\right) \cdot\frac{\left\|\vec{d}_{\mathcal{D}_{n}}^{2}(y)\right\|}{\sqrt{n}}\] (69)

where (a) is due to Cauchy-Schwarz inequality, and (b) follows from Lemma 6.

Using shorthand notation \(R_{n}=R_{\mathcal{D}_{n}}^{(\lambda)}\), \(\tilde{R}_{n}=R_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}\), \(\varphi_{n}=\varphi_{\mathcal{D}_{n}}^{(\lambda)}(x)\), and \(\tilde{\varphi}_{n}=\varphi_{\widetilde{\mathcal{D}}_{n}}^{(\lambda)}(x)\), we observe that

\[R_{n} (\tilde{\varphi}_{n})-R_{n}(\varphi_{n})\] \[=R_{n}(\tilde{\varphi}_{n})-\tilde{R}_{n}(\tilde{\varphi}_{n})+ \tilde{R}_{n}(\tilde{\varphi}_{n})-R_{n}(\varphi_{n})\] \[\stackrel{{(a)}}{{\leq}}R_{n}(\tilde{\varphi}_{n})- \tilde{R}_{n}(\tilde{\varphi}_{n})+\tilde{R}_{n}(\varphi_{n})-R_{n}(\varphi_{n})\] \[\stackrel{{(b)}}{{\leq}}\frac{\|\boldsymbol{Z}- \boldsymbol{X}\|}{\min\left\{\sigma^{(\lambda)}(\boldsymbol{X}_{\mathrm{ctr}}),\; \sigma^{(\lambda)}(\boldsymbol{Z}_{\mathrm{ctr}})\right\}}\cdot\left(2\cdot\left\|x -\mu_{\mathcal{D}_{n}}\right\|_{\Sigma_{\mathcal{D}_{n}}}+1\right)\cdot\frac{ \left\|\vec{d}_{\mathcal{D}_{n}}^{2}(\tilde{\varphi}_{n})\right\|+\left\|\vec{d} _{\mathcal{D}_{n}}^{2}(\varphi_{n})\right\|}{\sqrt{n}}\] (70)where (a) follows from the optimality of \(\tilde{\varphi}_{n}\), i.e., \(\tilde{R}_{n}(\varphi_{n})\geq\tilde{R}_{n}(\tilde{\varphi}_{n})\), and (b) is due to (69).

Finally, we note that if

\[\|x-\mu_{\mathcal{D}_{n}}\|_{\Sigma_{\mathcal{D}_{n}}}\leq\frac{1}{2}\left( \frac{C_{\mathbf{g}}\cdot D_{\mathbf{g}}^{\alpha}}{2\operatorname{diam}\left( \mathcal{M}\right)}\cdot\frac{\min\left\{\sigma^{(\lambda)}(\boldsymbol{X}_{ \operatorname{ctr}}),\,\sigma^{(\lambda)}(\boldsymbol{Z}_{\operatorname{ctr}}) \right\}}{\|\boldsymbol{Z}-\boldsymbol{X}\|}-1\right),\]

then the upper bound in (70) certifies that \(R_{n}(\tilde{\varphi}_{n})-R_{n}(\varphi_{n})<C_{\mathbf{g}}\cdot D_{\mathbf{g }}^{\alpha}\). Thus, we can use Assumption (C1) to convert the risk bound (70) to derive a distance bound between the minimizers:

\[d\left(\tilde{\varphi}_{n},\varphi_{n}\right)\leq\left(\frac{R_{n}(\tilde{ \varphi}_{n})-R_{n}(\varphi_{n})}{C_{\mathbf{g}}}\right)^{\frac{1}{\alpha}},\]

which completes the proof. 

## Appendix E Further details on the experiments

Experimental setup in Section 5.We consider combinations of \(p\in\{150,300,600\}\) and \(n\in\{100,200,400\}\). The datasets \(\mathcal{D}_{n}=\{(X_{i},Y_{i}):i\in[n]\}\) and \(\widehat{\mathcal{D}}_{n}=\{(Z_{i},Y_{i}):i\in[n]\}\) are generated as follows.

(True covariate \(X\)) Let \(X_{i}\sim\mathcal{N}_{p}\big{(}\mathbf{0}_{p},\Sigma\big{)}\) be IID multivariate Gaussian with mean \(\mathbf{0}_{p}\) and covariance \(\Sigma\) such that \(\operatorname{spec}\left(\Sigma\right)=\{\kappa_{j}>0:j\in[p]\}\) is an exponentially decreasing sequence such that \(\operatorname{tr}\left(\Sigma\right)=\sum_{j=1}^{p}\kappa_{j}=p\). To be specific, for each \(p\), we consider an exponentially decreasing sequence \(1=a_{1}>\cdots>a_{p}=10^{-3}\), and then set \(\kappa_{j}=p\cdot a_{j}/(\sum_{j^{\prime}=1}a_{j^{\prime}})\) for each \(j\in[p]\). Note that \(\sum_{j=1}^{\lfloor p/3\rfloor}\kappa_{j}/\sum_{j^{\prime}=1}^{p}\kappa_{j^{ \prime}}\approx 0.9\), and thus, \(\Sigma\) is effectively low-rank.

(Noisy covariate \(Z\)) For the error-prone covariate \(Z=X+\varepsilon\), we consider two scenarios \(\varepsilon_{j}\stackrel{{ IID}}{{\sim}}\mathcal{N}\big{(}0, \sigma_{\varepsilon}^{2}\big{)}\) and \(\varepsilon_{j}\stackrel{{ IID}}{{\sim}}\operatorname{Laplace} \big{(}0,\sigma_{\varepsilon}\big{)}\). Note that in this setting, we have the signal-to-noise ratio \(\mathbb{E}(\|X\|_{2}^{2})/\mathbb{E}(\|\varepsilon\|_{2}^{2})=1/\sigma_{ \varepsilon}^{2}\). We set \(\sigma_{\varepsilon}^{2}=0.05^{2}\).

(Response \(Y\)) Given \(X=x\), let \(Y\) be the distribution function of \(\mathcal{N}\big{(}\mu_{\alpha,\beta}(x)+\eta,\tau^{2}\big{)}\), where

* \(\mu_{\alpha,\beta}(x)=\alpha+\beta^{\top}x\) with \(\alpha=1\) and \(\beta=p^{-1/2}\cdot\mathbf{1}_{p}\),
* \(\eta\sim\mathcal{N}\big{(}0,\sigma_{\eta}^{2}\big{)}\),
* \(\tau^{2}\sim\mathcal{IG}(s_{1},s_{2})\), an inverse gamma distribution with shape \(s_{1}\) and scale \(s_{2}\).

We note that \(\mathbb{E}(\tau^{2})=\frac{s_{2}}{s_{1}-1}\) and \(\operatorname{Var}(\tau^{2})=\frac{s_{2}^{2}}{(s_{1}-1)^{2}(s_{1}-2)}\). In particular, when \(\tau^{2}=0\), this setting corresponds to the classical linear regression model for scalar responses. We set \(\sigma_{\eta}^{2}=0.5^{2}\), and \((s_{1},s_{2})=(18,17)\). In this setting, we have

* \(\mathbb{E}\big{(}\mu_{\alpha,\beta}(X)\big{)}=1\) and \(\operatorname{Var}\big{(}\mu_{\alpha,\beta}(X)\big{)}=\beta^{\top}\Sigma\beta \approx 1\),
* \(\mathbb{E}(\tau^{2})=1\) and \(\operatorname{Var}(\tau^{2})=0.25^{2}\).

(Tuning parameter \(\lambda\)) For simplicity, we chose a universal threshold value as

\[\hat{\lambda}_{n}=\operatorname*{arg\,min}_{\lambda\in\Lambda}\operatorname{ MSPE}(\varphi_{\widehat{\mathcal{P}}_{n}}^{(\lambda)}),\]

where \(\Lambda\) is a fine grid on \(\big{(}0,\sqrt{\lambda_{1}\cdot p/n}\big{)}\). Then the same threshold \(\hat{\lambda}_{n}\) was used to evaluate \(\operatorname{Bias}^{2}(\varphi_{\widehat{\mathcal{P}}^{(b)}}^{(\lambda)}(x))\), \(\operatorname{Var}(\varphi_{\widehat{\mathcal{P}}^{(b)}}^{(\lambda)}(x))\), and \(\operatorname{MSE}(\varphi_{\widehat{\mathcal{P}}^{(b)}}^{(\lambda)}(x))\) for all \(b=1,\ldots,B\). Therefore, we claim that the performance of the SVT estimator reported in Table 1 has further room for improvement if one substitute \(\hat{\lambda}_{n}^{(b)}=\operatorname*{arg\,min}_{\lambda\in\Lambda} \operatorname{MSPE}(\varphi_{\nu^{(b)}}^{(\lambda)})\) for each Monte Carlo experiment. Although suboptimal results are reported, we note that the proposed SVT outperforms both the oracle estimator and the naive EIV estimator in our simulation study. In practice, one may employ cross-validation for better performance. For the MSPE in Table 1, we reported \(\min_{\lambda\in\Lambda}\operatorname{MSPE}(\varphi_{\widehat{\mathcal{P}}_{n}}^ {(\lambda)})\).

Evaluation metrics: bias and variance.We evaluate the accuracy and efficiency of the Frechet regression function estimator using the bias and the variance. For any given \(x\), we define

\[\mathrm{Bias}_{x}\big{(}\varphi_{\nu}^{(\lambda)}\big{)}\coloneqq d_{W}\Big{(} \overline{\varphi}_{\nu}^{(\lambda)}(x),\,\varphi_{\nu^{\prime}}^{(0)}(x) \Big{)}\quad\text{and}\quad\mathrm{Var}_{x}\big{(}\varphi_{\nu}^{(\lambda)} \big{)}\coloneqq\frac{1}{B}\sum_{b=1}^{B}d_{W}\Big{(}\varphi_{\nu^{(b)}}^{( \lambda)}(x),\,\overline{\varphi}_{\nu}^{(\lambda)}(x)\Big{)}^{2},\]

where \(\nu\in\{\mathcal{D}_{n},\widetilde{\mathcal{D}}_{n}\}\) and \(\overline{\varphi}_{\nu}^{(\lambda)}(x)\coloneqq\arg\min_{y}\sum_{b=1}^{B}d_{W }\big{(}\varphi_{\nu^{(b)}}^{(\lambda)}(x),\,y\big{)}^{2}\) is the Frechet mean of \(\varphi_{\nu^{(1)}}^{(\lambda)}(x),\ldots,\varphi_{\nu^{(B)}}^{(\lambda)}(x)\). Note that these definitions are a generalization of the standard bias and variance of the regression function estimator in Euclidean spaces. We evaluate the global performance of the estimator by considering a fixed set of evaluation points, \(\mathcal{G}_{M}=\{x_{m}:m=1,\ldots,M\}\), and compute

\[\mathrm{Bias}^{2}\big{(}\varphi_{\nu}^{(\lambda)}\big{)}\coloneqq\frac{1}{M} \sum_{m=1}^{M}\mathrm{Bias}_{x_{m}}^{2}\big{(}\varphi_{\nu}^{(\lambda)}\big{)} \quad\text{and}\quad\mathrm{Var}\big{(}\varphi_{\nu}^{(\lambda)}\big{)} \coloneqq\frac{1}{M}\sum_{m=1}^{M}\mathrm{Var}_{x_{m}}\big{(}\varphi_{\nu}^{( \lambda)}\big{)}.\]

In our experiment, we generate the set \(\mathcal{G}_{M}\) by drawing \(x_{1},\ldots,x_{M}\) IID from the same distribution as \(X\), with \(M=500\).

Additional experiment with linear regression modelsWe also conducted an additional experiment for the standard linear regression models with three different metric metrics.

(Model) The linear regression model for \((X,Y)\in\mathbb{R}^{p}\times\mathbb{R}^{d}\) is defined as \(Y=\alpha+X\beta+\eta\) and the covariate is contaminated as \(Z=X+\varepsilon\). We generate \(X\) using the effective low-rank model with a geometrically decaying spectrum and condition number \(10^{3}\) (see Appendix E in the original submission).

(Parameters) Here, we let \(\alpha=\mathbf{1}_{d}+0.1\cdot\bm{g}\), \(\beta=d^{-1/2}\cdot\mathbf{1}_{p\times d}+0.1\cdot\bm{G}\), \(\eta\sim\mathcal{N}(0,0.5^{2}\cdot I_{d})\) and \(\varepsilon\sim\mathcal{N}(0,0.5^{2}\cdot I_{p})\), with \(\bm{g}\), \(\bm{G}\) being standard Gaussians.

Figure 3: Normalized mean squared prediction error (NMSPE) versus threshold \(\lambda\) for vector-valued linear regression with three different metrics for \(\mathcal{Y}\): (a) \(\ell_{1}\)-metric; (b) \(\ell_{2}\)-metric; and (c) \(\ell_{\infty}\)-metric. [**SVT**]: the regularized Frechet regression estimator \(\varphi_{\mathcal{D}_{n}}^{(\lambda)}\) with best NMSE; [**EIV**]: the unregularized Fr√©chet regression estimator \(\varphi_{\mathcal{D}_{n}}^{(0)}\) with errors-in-variables covariates \(Z\); [REF]: the unregularized Fr√©chet regression estimator \(\varphi_{\mathcal{D}_{n}}^{(0)}\) with error-free covariates \(X\) (oracle).