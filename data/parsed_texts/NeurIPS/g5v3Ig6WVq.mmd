# Auslan-Daily: Australian Sign Language Translation

for Daily Communication and News

 Xin Shen\({}^{1}\)   Shaozu Yuan\({}^{2}\)   Hongwei Sheng\({}^{1}\)   Heming Du\({}^{1}\)   Xin Yu\({}^{1}\)

\({}^{1}\)The University of Queensland

\({}^{2}\)JD AI, Beijing, China

x.shen3@uqconnect.edu.au

Corresponding author.

###### Abstract

Sign language translation (SLT) aims to convert a continuous sign language video clip into a spoken language. Considering different geographic regions generally have their own native sign languages, it is valuable to establish corresponding SLT datasets to support related communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale dataset for SLT. To fill this gap, we curate an Australian Sign Language translation dataset, dubbed Auslan-Daily, which is collected from the Auslan educational TV series and Auslan TV programs. The former involves daily communications among multiple signers in the wild, while the latter comprises sign language videos for up-to-date news, weather forecasts, and documentaries. In particular, Auslan-Daily has two main features: (1) the topics are diverse and signed by multiple signers, and (2) the scenes in our dataset are more complex, _e.g._, captured in various environments, gesture interference during multi-signers' interactions and various camera positions. With a collection of more than 45 hours of high-quality Auslan video materials, we invite Auslan experts to align different fine-grained visual and language pairs, including video \(\leftrightarrow\) fingerspelling, video \(\leftrightarrow\) gloss, and video \(\leftrightarrow\) sentence. As a result, Auslan-Daily contains multi-grained annotations that can be utilized to accomplish various fundamental sign language tasks, such as signer detection, sign spotting, fingerspelling detection, isolated sign language recognition, sign language translation and alignment. Moreover, we benchmark results with state-of-the-art models for each task in Auslan-Daily. Experiments indicate that Auslan-Daily is a highly challenging SLT dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide in a broader context. All datasets and benchmarks are available at \(\boldsymbol{\mathsf{\bar{O}}}\) Auslan-Daily.

Figure 1: Diversity of our curated Auslan-Daily dataset. Auslan-Daily involves various topics, signers and diverse environments. In particular, there are multiple persons in a scene and signers appear in different areas of the scene.

## 1 Introduction

Sign language (SL) is the primary way for deaf or people with hearing loss to express themselves. Similar to various spoken languages, sign languages have their vocabularies and grammar [5; 6; 7]. More importantly, diverse geographic regions usually have their native sign languages even though these regions share a commonly spoken language, such as America, Australia and the UK. To eliminate the communication barriers between the deaf and hearing communities, sign language translation (SLT) has been proposed to convert signs into spoken languages [1; 2; 4; 3].

With the emerging deep learning techniques and large-scale sign language datasets, SLT has achieved promising progress recently. As shown in Figure 2, researchers from various countries have constructed their sign language datasets and thus thrust SLT in their respective sign languages, such as American sign language (ASL) [1], British sign language (BSL) [2], Chinese sign language (CSL) [3] and Germany sign language (DGS) [4]. However, to the best of our knowledge, there is no publicly available large-scale Auslan dataset for continuous sign translation2. Meanwhile, according to the Hearing Care Industry Association3, as of June 2015, one in six Australians had hearing loss affecting them and this proportion is expected to increase to one in four by 2050. Due to the societal inclusion and the regional nature of sign languages, Australian sign language (Auslan) datasets are inevitably and urgently needed in order to investigate automatic translation.

Footnote 2: Sign videos are segmented and annotated at the sentence level.

Footnote 3: https://www.hcia.com.au/resources/HCIA.pdf

Footnote 4: https://earlychildhood.qld.gov.au/early-years/sally-possum

Footnote 5: https://view.abc.net.au/show/abc-news-with-auslan

Moreover, existing sign language corpora either are captured in controlled laboratory environments [1; 3; 8; 9; 10; 11] or only contain a single person (the signer) at a certain position in each video clip [2; 4; 12; 13]. In [14], Nunez-Marcos _et al._ point out that the effectiveness of sign language translation can be compromised in multi-individual situations due to the presence of non-signers. Additionally, Yin _et al._[15] emphasise that translating sign language in the wild is even more challenging. Thus, the existing datasets lack sufficient diversity and may not fully reflect the complexity in the wild.

In this work, we aim to construct a large-scale Auslan translation dataset in the wild which contains sufficient high-quality Auslan videos and their English transcriptions. We adopt the first Australian educational TV series for deaf children "Sally and Possum"4, "ABC News with Auslan"5 and several online domain-specific Auslan corpora6 as our data source. Firstly, "Sally and Possum" is aired to help deaf children and their parents to learn Auslan, and it covers various topics of daily life, such as drawing pictures with colourful pigments and asking for advice in diverse environments, including indoor and outdoor scenes (Figure 1, left). Secondly, "ABC News with Auslan" starts broadcasting form 2022 and provides the latest news and information from ABC News every week with Auslan interpreted (Figure 1, middle). Lastly, we aggregate additional content-specific Auslan data from

Figure 2: Comparisons of large-scale sign language translation datasets across diverse geographic regions. (a) How2Sign [1] (American), (b) BOBSL [2] (British), (c) CSL-Daily [3] (Chinese), (d) RWTH-PHOENIX-Weather 2014T [4] (Germany), (e) Our proposed **Auslan-Daily** (the yellow and black bounding-boxes indicate the signer and non-signer in the sign video clip).

[MISSING_PAGE_FAIL:3]

BOBSL [2; 20] and OpenASL [13] are large-scale sign language translation datasets. For these two extensive datasets, experts verification has been confined solely to the validation and test sets, whereas the training set has been annotated via a pre-trained BSL sign alignment model [21] or through "self-generated" time boundaries based on ASL News. How2Sign [1] is the largest American Sign Language (ASL) dataset captured in the lab environment. It contains a variety of annotations, including multi-view information, depth information, pose, and speech. CSL-Daily [3] and DGS Corpus [8] are the extensive datasets of Chinese Sign Language (CSL) and German Sign Language (DGS), respectively. How2Sign, CSL-Daily and DGS Corpus cover diverse topics. The datasets pertinent to Auslan are represented by Auslan Signbank [22] and Auslan Corpus [23; 19; 24]. The Auslan Signbank constitutes a dictionary with approximately 5,500 Auslan glosses, while the Auslan Corpus [23; 19; 24] is primarily confined to exploratory research by linguists and is not entirely accessible to the public. More importantly, the above datasets only consider one person (signer) in each sign video clip, and the signer appears at a controlled certain position in each sign video clip.

However, in the real world, multiple people may perform sign gestures in a scene, and their positions are diverse. Detecting the actual signer and accurately translating their gestural signs within complex environments presents a novel challenge. Factors such as perspective, illumination and the presence of crowds introduce noise and complexity when translating sign language to spoken language. In contrast, we propose Auslan-Daily, the first publicly available real-world Auslan translation dataset. Meanwhile, we enrich the diversity and complexity by collecting source various data.

### Tasks Associated with Sign Language

Currently, there are several tasks for investigating various granularities of sign language, such as fingerspelling (character), gloss (word) and sentence. All the sign language-related tasks aim to foster better sign language understanding and sign language translation development. The sign language-related task, including: (i) **Sign Language Translation (SLT)**[4] task is to translate a sign video clip to the corresponding spoken language. The current SLT models can be divided into three categories, Sign2Gloss2Text, Sign2(Gloss+Text), and Sign2Text [25]. The Sign2Gloss2Text [26] model is a two-stage method, Sign2Gloss and Gloss2Text, respectively. The first stage is sign language recognition, which predicts a gloss sequence from a video, and the second stage translates the predicted gloss into the target natural language. Both Sign2(Gloss+Text) [27; 3; 28; 29; 30] and Sign2Text [31; 32; 33; 34] are end-to-end models. The difference is that Sign2(Gloss+Text) jointly trains the sign language recognition and translation tasks, and uses the gloss information as auxiliary supervision to extract features from videos, thereby improving the translation results. Though Sign2(Gloss+Text) models perform well on existing datasets, obtaining large-scale sign language translation data with continuous glosses annotation is costly and time-consuming. The primary reason is that the process of annotating one hour of continuous sign language video [21] requires an expert, who is proficient in sign language, approximately ten to fifteen hours. Thus, it is hard to apply them to SL datasets that do not contain gloss annotations. For Sign2Text models, they aim to directly convert sign language performed by a single person into target natural language. Furthermore, as the existing datasets are not collected in the wild, previous models might fail to tackle complex scenarios and real-world situations with multiple people; (ii) **Sign Language Alignment**[21; 35] temporally aligns asynchronous subtitles in sign language videos. A proficient alignment model for sign language can mine more sign data for automated translation; (iii) **Isolated Sign Language Recognition**[36; 37; 20] focuses on identifying and understanding individual gestural signs, independent of any surrounding context or sequence of signs; (iv) **Sign Spotting**[38; 39; 40] aims to find accurate locations of the given isolated signs in continuous co-articulated sign language videos; (v) **Fingerspelling Detection**[41; 42; 43] finds the fingerspelling segments' intervals within the clip. Fingerspelling is an important component of Sign Language, in which words are signed letter by letter and (vi) **Active Signer Detection**, also known as Sign Language Detection [44; 45], is identical to the initial stage of Signer Diarisation [46]. It aims to find the signer in the sign video clip.

## 3 Auslan-Daily Dataset

In this section, we describe data collection and cleaning, detail the data labelling procedure and provide statistics of the Auslan-Daily train/test split.

### Data Collection and Cleaning

"Sally and Possum", "ABC News with Auslan" and Auslan corpora from YouTube are public TV programs and open sources8. "Sally and Possum", a premium Australian sign language show for deaf children. To facilitate children learning of Auslan, the production team designs the plots and writes the English scripts. Subsequently, Auslan experts perform sign language based on the script. This show has 6 seasons and 15 episodes per season, which is 22.5 hours long and contains around 20 topics, including daily communication, study skills, and knowledge explanation. Beginning in 2022, "ABC News with Auslan" weekly broadcasts key domestic and international events news and weather forecasts. It is an ongoing TV program, and the current dataset includes 45 news videos as of May 2023. Auslan experts execute a real-time sign language translation (simultaneous interpretation) for deaf or people with hearing loss based on currently broadcast news. However, the English subtitles may leave the following problems [21]: (i) the order of subtitles is not complying between spoken and sign languages, and (ii) the duration of a subtitle varies considerably between signing and speech due to differences in speed and grammar. To enrich the topics of our dataset, we also collect several publicly available high-quality documentaries interpreted with Auslan. These data comprise specific thematic areas, including disaster overviews, preventative measures, and interviews.

Footnote 8: Our dataset follows the copyright **Creative Commons BY-NC-ND 4.0** license ©.

All of the original videos are recorded with standard English dubbing and subtitles. We download subtitles of each whole original video, which are arranged with the format "[Start Time] subtitle [End Time]". The time intervals are marked based on dubbing. Upon spot check, we observe that longer subtitles might extend across multiple temporal intervals, whereas several shorter subtitles tend to appear within a time interval. To procure complete sentence-level subtitles, we conduct the data cleaning operations as follows: (1) for incomplete subtitles, _e.g._, ending with a comma, we connect them with the following subtitles to compose complete sentences and merge their time duration; (2) for several complete subtitles that appear within a time interval, we partition them into multiple independent sentences; (3) for a complete sentence that only contains modal particles, _e.g._, "Oh!" and "Ha ha ha!", we remove them to avoid meaningless translation. As a result, we acquire approximately 29k complete subtitles that require alignment.

### Two-Stage Data Labelling Procedure

To obtain applicable data for sign language translation, we design two stages of the data labelling procedure: (1) aligning video clips and transcriptions and (2) detecting the signer in each aligned video clip. They are imperative as original subtitles often misalign with sign videos, and the position of the signer varies. By analyzing the frequency of individual tokens across all subtitles, we notice a considerable presence of long-tailed words. Due to their sparse occurrence, these words impose challenges for the model and affect the translation results. To investigate this problem, sign language experts annotate the less commonly used glosses in "Sally and Possum". Experts also label the temporal boundaries of significant fingerspelling instances, as fingerspelling is commonly used in the deaf community. Therefore, in the first stage, we engage Auslan experts to synchronise video \(\leftrightarrow\) fingerspelling, video \(\leftrightarrow\) gloss, and video \(\leftrightarrow\) sentence pairs.

Next, we employ Alphapose [16; 17; 18] to track people in each aligned video clip. Then, we record trajectories and pose sequences along with their corresponding IDs. Considering Alphapose

\begin{table}
\begin{tabular}{c|c c c|c c c|c} \hline \hline \multicolumn{1}{c|}{Sub-Dataset} & \multicolumn{2}{c|}{**Auslan-Daily Communication**} & \multicolumn{2}{c|}{**Auslan-Daily News**} & \multicolumn{1}{c}{} \\ \hline Domain/Topic & \multicolumn{4}{c|}{Communication} & \multicolumn{4}{c|}{News \& Documentary} & \multicolumn{1}{c}{} \\ Video Resolution/GPS & 192\(\times\) 1080/92 & 1528\(\times\) 7201920\(\times\) 1080/92 9.97 & \multicolumn{1}{c}{} & & & \\ \hline Split & Train & Dev & Test & Train & Dev & Test & Total \\ \hline Segments & 12.441 & 800 & 800 & 9.665 & 700 & 700 & 25.106 \\ Signers & 49 & 12 & 9 & 18 & 17 & 17 & 67 \\ Frames & 930.321 & 45.369 & 45.711 & 20.027425 & 144.8193 & 142.8993 & 3.381.448 \\ Vocch & 3,064 & 522 & 469 & 12.346 & 2.872 & 2.885 & 13.945 \\ Tot. words & 85.167 & 4,126 & 4,115 & 163.268 & 11.376 & 11.530 & 282.582 \\ Tot. OOVs & & 8 & 1 & 326 & 304 & - \\ Singletons & 1.043 & - & 5,267 & - & - & - \\ Person per clip & 1-11 & 1-8 & 1-8 & 1-8 & 1-8 & 1-7 & 1-10 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Key statistics of Auslan-Daily. Auslan-Daily Communication and Auslan-Daily News are two sub-datasets split from Auslan-Daily. OOV: out-of-vocabulary. Singleton: words that only occur once in the training dataset.

may suffer tracking errors sometimes9, we thus invite annotators to manually check and modify the tracking results by assigning correct IDs to the signers.

Footnote 9: The overall error rate is less than 20 percent of video clips with multiple signers.

To guarantee the annotation quality of our dataset, we conduct a cross-check verification process during each data labelling procedure stage. Specifically, we ask each Auslan annotator as an examiner to cross-check around 5% of annotated video clips provided by another annotator. The video clips are chosen randomly. If the examiner finds more than 10% of annotated videos have obvious errors, a third annotator is invited to review and correct the annotations.

Through the collaborative efforts of five Auslan experts and five annotators, we complete all annotations with approximately 300 work hours. Overall, our Auslan-Daily dataset contains the following **annotations**: (1) temporal boundaries of sign video clips; (2) temporal boundaries of long-tailed glosses; (3) temporal boundaries of partial fingerspellings; (4) pose sequences of signers and non-signers; (5) bounding-boxes of signers; (6) signer identities and (7) English transcriptions. These multi-grained annotations can be further investigated for Australian sign language-related tasks.

### Data Statistics

During the data labelling, experts discard erroneous data, such as subtitles do not have corresponding sign language. As demonstrated in Table 2, there are in total 25,106 video clips encompassing 67 unique signers, with the vocabulary size of 13,945 words. It should be noted that signers in the validation and test sets appear in the training set. As the number of persons ranges from 1 to 10 in a video clip, the distractions, such as gesture interference of multi-persons, are also involved in sign language translation, thus imposing challenges in this task. To verify the robustness of the various models, we also statistics of the sentences within the test set. As shown in Table 3 (Appendix), over 80% of video clips in the test set encompass distinct sentences. Therefore, the robustness of the models can be verified by evaluating on the test set.

After examining the domains of the three data sources, "ABC News with Auslan" and the data from online Auslan corpora are found significantly similar. Thus, we combine the data collected from these two video sources and partition the Auslan-Daily dataset into two sub-datasets: Auslan-Daily Communication and Auslan-Daily News. We randomly split the two sub-datasets data into the training, validation and test sets as shown in Table 2. Each sign video clip has 73 frames and 7.8 words on average for the Auslan-Daily Communication sub-dataset. Each sign video clip has 214 frames and 16.9 words on average for the Auslan-Daily News sub-dataset. The distributions of frames and words in different splits of the sub-datasets are shown in Figure 3 (left).

Moreover, Auslan-Daily has 3,000 (600 classes) long-tailed isolated glosses and 2,000 significant fingerspellings. For the isolated gloss, we select words appearing less than ten times after the natural language corpus post-lemmatization10 in "Sally and Possum". Its main purpose is to enhance translation models to recognise long-tail words through sign language recognition or spotting tasks. Furthermore, considering the pivotal role of fingerspelling in sign language translation, we thus

Figure 3: Distributions of #frames/#words over clips (L) and the topic diversity in Auslan-Daily (R).

annotate Auslan fingerspelling instances. As illustrated in Figure 3 (right), there are 21 types of daily communication and more than 15 types of daily news in our Auslan-Daily dataset.

## 4 Overview of Auslan-Daily Tasks

### Task Definition

**Active Signer Detection** (ASD) [44; 45; 46]: Given trajectories of \(N\) people \(\mathcal{P}=\{p_{i}\}_{1}^{N}\) in a sign video clip \(\mathbb{V}=\{f_{i}\}_{1}^{t}\), the goal of ASD is to find the active signer \(p_{i}\) in the current video clip.

**Sign Spotting** (SS) [38; 39]: Given a sign video clip \(\mathbb{V}\) with \(t\) frames and an isolated gloss \(\mathbb{I}\) clip, the goal of SS is to locate the start frame \(f_{m}\) and end frame \(f_{n}\) of \(I\) in \(\mathbb{V}\).

**Isolated Sign Language Recognition** (ISLR) [36; 37; 20]: Given an isolated gloss \(\mathbb{I}\) clip, ISLR aims to determine the gloss category \(c\) of \(\mathbb{I}\), where \(c\) is pre-defined by a Sign Dictionary.

**Fingerspelling Detection** (FD) [41]: Given a sign video clip \(\mathbb{V}\) with \(t\) frames, the goal of FD is to locate the start frame \(f_{m}\) and end frame \(f_{n}\) of each fingerspelling in \(\mathbb{V}\).

**Sign Language Alignment** (SLA) [21]: Given a whole original video \(\mathcal{V}=\{f_{i}\}_{1}^{T}\) with \(T\) frames and its corresponding M subtitles \(\mathcal{S}=\{s_{i}\}_{1}^{M}\), the goal of SLA is to align each subtitle sentence \(s_{i}\) with the precise begin frame \(f_{i}\) and end frame \(f_{j}\).

**Sign Language Translation** (SLT) [4]: Given a sign video clip \(\mathbb{V}\) with \(t\) frames, the goal of SLT is to translate \(\mathbb{V}\) to an English sentence \(s_{i}\).

### Evaluation Metrics

**BLEU and ROUGE:** BLEU [47] and ROUGE [48] scores are commonly-used evaluation metrics for _Sign Language Translation_[4]. BLEU-n measures the n-gram overlap between the generated text and reference text, and ROUGE-L measures the F1 score based on the longest common subsequences between the generated text and reference text.

**Top-\(K\) Accuracy:** Top-\(K\) classification accuracy measures the number of ground-truth labels within the top \(k\) predicted labels. For _Signer Detection_, \(K=1\) is employed. For _Isolated Sign Language Recognition_, we adopt \(K\) values of 1, 5, and 10.

**IoU:** Intersection over Union (IoU) is defined as the ratio of the intersection and the union of the predicted and actual time intervals of an action within a video [49; 50]. Similar to [41], we employ AP@IOU as a metric for _Fingerspelling Detection_. Following [21; 39], we use F@IOU to evaluate _Sign Language Alignment_ and _Sign Spotting_ tasks.

Figure 4: Overview of Auslan-Daily tasks and provided annotations.

## 5 Auslan-Daily Benchmark

### Video Representation

Pose-based video feature representation:Pose-based representations are robust against background clutters, lighting conditions, and occlusions, while explicitly depicting human hand and limb movements [51; 52; 53]. Several recent studies exploit pose information and achieve state-of-the-art performance in sign language translation-related tasks [9; 54; 55; 36; 56]. Hence, we use the key points extracted from Alphapose[16; 17; 18] as video features to provide benchmark results.

**RGB-based video feature representation:** Several models directly extract features from sign videos, such as CNN-RNN-HMM network [4], S3D [57], and I3D [58]. In the works [20; 36; 31], I3D is used for sign video representation. To better adapt to SL dataset and capture the spatio-temporal information of signs, inspired by [31], we finetune I3D on a word-level sign language recognition dataset and extract sign video features with different window widths and strides.

### Benchmark Results

In this section, we provide benchmark results of sign language translation, alignment, active signer detection, fingerspelling detection, sign spotting and isolated sign recognition tasks on Auslan-Daily.

**Sign Language Translation:** In this task, we employ publicly available gloss-free SLT models, including (1) the RNN-based language translation model (SL-Luong [59]), (2) the Temporal Semantic Pyramid network (TSPNet [31]), (3) the Sign Language Translation Transformer model without glosses (SL-Transf [27]), (4) the multi-modality transfer learning based model (MMTLB [29]) and (5) transformer-based model with the gloss attention mechanism (GASLT [33]). Note that these models are designed to translate sign videos that only contain one single signer. To meet the input requirement of these SLT models, we crop the signer regions based on the ground-truth bounding-boxes of the signers. Since Auslan-Daily videos are captured in diverse scenes, RGB-based representation models may be affected by background clutter and various camera angles. In our single-person SLT experiments, GASLT performs best on the Auslan-Daily Communication subset, while SL-Luong excels on the Auslan-Daily News subset, both using pose points (hands & body) as input. In addition, to shed some light on how existing SLT models perform on Auslan-Daily with multiple persons in each video clip (without cropping acting signers), we directly feed video clips into SLT models. As indicated by Table 3, we observe significant performance degradation. This implies that existing SLT models do not have attention mechanisms to focus on active signers and non-signers can distract SLT. Therefore, we introduce a paradigm of signer detection followed by a translation model, denoted by SD+SLT, in Table 3. It is observed that SD significantly facilitates translation in multi-person scenarios.

**Sign Language Alignment:** We use Subtitle Aligner Transformer (SAT) [21] model to evaluate the sign language alignment task. It employs Transformer [60] to synchronise subtitles with BSL videos and provides a pre-trained model [2]. The results are shown in Table 4. Leveraging the pre-trained

\begin{table}
\begin{tabular}{l|c|c c c c|c c c c c} \hline \hline  & & \multicolumn{4}{c|}{**Auslan-Daily Communication**} & \multicolumn{4}{c}{**Auslan-Daily News**} \\ \hline
**Single-Per. SLT** & Input & R & B1 & B2 & B3 & B4 & R & B1 & B2 & B3 & B4 \\ \hline SL-Luong [59] & **Pose** & **37.27** & **30.15** & **16.26** & 11.67 & 9.45 & 20.65 & **19.84** & **7.81** & **4.59** & **2.81** \\ SL-Luong [59] & RGB & 13.49 & 13.54 & 7.85 & 5.74 & 4.46 & 16.14 & 16.92 & 7.44 & 4.07 & 2.68 \\ SL-Transf [27] & Pose & 35.65 & 31.31 & 16.17 & 11.41 & 9.20 & 20.25 & 21.25 & 6.57 & 3.32 & 2.11 \\ SL-Transf [27] & RGB & 14.97 & 15.25 & 9.05 & 6.51 & 5.20 & 14.93 & 17.64 & 7.41 & 3.98 & 2.52 \\ TSPNet-Joint [31] & RGB & 26.89 & 26.07 & 10.07 & 5.46 & 3.76 & 19.71 & 18.23 & 5.97 & 3.21 & 2.26 \\ MMH [29] & RGB & 17.64 & 18.35 & 14.27 & 9.76 & 6.11 & 18.90 & 19.64 & 5.30 & 3.26 & 2.31 \\ GASLT [33] & Pose & 35.74 & 28.19 & 16.00 & **11.93** & **9.95** & 18.76 & 15.57 & 6.06 & 3.72 & 2.72 \\ GASLT [33] & RGB & 31.46 & 25.05 & 10.18 & 6.25 & 4.73 & **22.01** & 19.54 & 7.45 & 4.41 & 2.56 \\ \hline \hline
**Multi-Per. SLT** & Input & R & B1 & B2 & B3 & B4 & R & B1 & B2 & B3 & B4 \\ \hline SL-Luong [59] & RGB & 14.21 & 13.58 & 6.63 & 3.83 & 2.45 & 14.04 & 15.53 & 6.11 & 3.72 & 2.05 \\ SL-Transf [27] & RGB & 13.53 & 14.45 & 7.58 & 4.48 & 2.86 & 13.68 & 5.86 & 2.72 & 1.55 \\ TSPNet-Joint [31] & RGB & 27.76 & 26.86 & 10.08 & 5.00 & 3.28 & 14.64 & 17.33 & 3.86 & 1.66 & 1.89 \\ MMH [29] & RGB & 20.53 & 16.54 & 11.32 & 6.84 & 4.52 & 17.76 & 16.02 & 4.81 & 2.83 & 1.83 \\ GASLT [33] & RGB & 29.33 & 23.62 & 9.44 & 5.69 & 4.23 & 19.73 & 16.99 & 6.25 & 3.44 & 2.26 \\ \hline
**SD+SLT** & **Pose** & **34.28** & **28.94** & **14.90** & **10.49** & **8.38** & **19.43** & **17.22** & **7.12** & **4.13** & **2.53** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Translation results of Single/Multi-Person SLT gloss-free models on Auslan-Daily.

model provided by [21] and fine-tuning the sign language alignment model on Auslan enhances alignment performance.

**Active Signer Detection:** Active Signal Detection can be considered a binary action recognition problem. Inflated 3D ConvNet (I3D) model [58] is employed as the baseline on the RGB-based model. In addition, Pose-based Temporal Graph Convolution Networks (Pose-TGCN) [36; 61] serve as the pose-based model baseline. As shown in Table 5, Pose-TGCN and I3D achieve similar performance.

**Isolated Sign Language Recognition:** Isolated Sign Recognition is a multi-class action classification task. Similar to ASD, I3D and Pose-TGCN are adopted for the RGB-based and pose-based, respectively. Table 5 illustrates that the pose-based model performs better in complex scenes.

**Fingerspelling Detection:** We evaluate Auslan-Daily fingerspelling detection using the publicly available state-of-the-art fingerspelling detection model [41]. It is a multi-task model that combines pose estimation, recognition and detection tasks to improve the detection results jointly. The performance for the fingerspelling detector achieves 0.33/0.28/0.21 for AP@IoU(0.1/0.3/0.5). Comparing fingerspelling in Auslan-Daily Communication with Auslan-Daily News reveals more complexity and faster speed of fingerspelling in the latter.

**Sign Spotting:** We employ the sign spotting model [39] to evaluate the sign spotting task on the Auslan-Daily sign potting task. This method fusion multi-modal features, including RGB and poses, to obtain sign clip representations. Meanwhile, it introduces an innovative top-k transferring technique during testing to reduce the domain gap between isolated signs and continuous sign language. The baseline of this task is an F1 score of 0.27.

## 6 Discussion and Limitation

**Reasons for Low Performance on Auslan-Daily News Translation:** (1) Table 2 indicates that the dictionary size of Auslan-Daily News is much larger than that of Auslan-Daily Communication and Auslan-Daily News is long-tailed. These impose challenges in translating Auslan from videos to English [12; 2; 13]. (2) The experts commonly use abbreviations for named entities in videos, such as person and organization names. However, the corresponding words in the subtitles are in the full form. (3) Since 'ABC News with Auslan" is a real-time TV program, the experts may summarise and interpret the simultaneous broadcasting news to Auslan, which could lead to missing words. (4) Fingerspelling recognition can be significantly affected by different camera angles and signing speeds, and thus current models struggle to identify each word, especially in news.

**Reasons for Low Performance on Long-Tailed Isolated Sign Language Recognition:** Since we annotate isolated signs which are distributed in the long tail in the vocabulary, the average number of sign videos for each sign is much smaller than that of normal ISLR dataset [36; 37]. Moreover, as our sign videos include diverse signers, environments and camera perspectives, our ISLR videos are more challenging. This scenario is also practical in SLT tasks since not every sign has a large number of corresponding videos. As a result, the ISLR accuracy in Table 5 is low than existing benchmarking results. As demonstrated in Table 5 (Appendix), we also adopt more methods to evaluate ISLR.

**Coreference Resolution:** Coreference occurs when two or more expressions refer to the same person or thing [62]. It is observed that translating Auslan faces the challenge of coreference resolution [15; 63]. Signers frequently employ coreference to nouns to reduce the signing complexity. Consequently, we suggest exploiting context information to address coreference resolution in sign language translation [64; 65; 66; 67].

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline Fine-Tune & Test & **F1\(\bm{\Theta}\).10** & **F1\(\bm{\Theta}\).25** & **F1\(\bm{\Theta}\).50** \\ \hline
**News** & News & 80.59 & 72.30 & 50.08 \\ \hline
**Comm.** & Comm. & 87.58 & 77.41 & 66.33 \\ \hline  & Comm. & 81.28 & 71.81 & 64.27 \\
**Mixed** & News & 85.13 & 77.00 & 53.05 \\  & Mixed & 82.49 & 73.43 & 60.76 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The baseline of Sign Language Alignment (SLA) on Auslan-Daily. _Comm._, _News_ and _Mixed_ refer to two sub-datasets and the total combined dataset, respectively.

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline Task & Model + Feature & **Top-1** & **Top-5** & **Top-10** \\ \hline \multirow{2}{*}{**SD**} & TGCN + B + HS & 88.35 & - & - \\  & ISD + Video & **89.01** & - & - \\ \hline \hline \multirow{2}{*}{**ISLR**} & TGCN + B + HS & **14.82** & **25.37** & **37.32** \\  & ISD + Video & 11.87 & 20.10 & 30.33 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The baseline of Signer Detection (SD) and Isolated Sign Language Recognition (ISLR) on Auslan-Daily. \(B\) and _Hs_ represent Body and Hands keypoints, respectively.

**Cross-Domain Investigation:** The Auslan-Daily dataset is subdivided into Auslan-Daily Communication and Auslan-Daily News. The differences in sources and topics naturally create a domain gap. Auslan-Daily provides a practical dataset for investigating the cross-domain issue inherent in the sign language translation task [68, 69, 70, 55].

**The Volume of Datasets:** Our project aims at an ongoing exploration of machine sign language translation for Auslan. Unlike ASL and BSL, the high-quality data corpora of Auslan are relatively small. Therefore, we will continue incorporating content from "ABC News with Auslan" and other Auslan corpora to enrich Auslan-Daily. We intend to leverage existing Auslan data to provide additional annotations, similar to BOBSL [2]. In other words, we can employ a trained Auslan alignment model for preliminary annotations [21] and then conduct manual verification by experts.

## 7 Conclusion

In this work, we propose the first public available large-scale multi-person Auslan translation dataset with multi-grained annotation, named Auslan-Daily. Moreover, Auslan-Daily includes diverse topics and multiple signers performing in various environments. More importantly, the collected sign conversations are captured in the wild, significantly increasing the challenges of Auslan translation. Extensive experiments demonstrate the validity and challenges of our Auslan-Daily. Thanks to the multi-grained annotations, our dataset can be used for other sign language-related tasks. Furthermore, the presented benchmark results can act as strong baselines for future research. Although Auslan-Daily currently only has English transcriptions, we intend to provide gloss annotations and the benchmark on Continuous Sign Language Recognition (CSLR) in the future to further promote research on Auslan translation.

## Acknowledgement

This research is funded in part by ARC-Discovery grant (DP220100800 to XY), ARC-DECRA grant (DE230100477 to XY) and Google Research Scholar Program. We gratefully thank all the anonymous reviewers and ACs for their constructive comments.

## Broader Impact

Auslan, like many other sign languages, has its distinctive features in semantics and pragmatics. The complexity and diversity of the expressions of Auslan present a significant hurdle in designing vision-language models. In this paper, the challenges posed by Auslan-Daily help stimulate the development of the vision-language community.

Moreover, as a visual language that unfolds in a three-dimensional space, sign language poses perspective-related complexities, such as capturing signs from a side-view perspective, that differ from written and spoken languages. Incorporating sign language into deep learning networks entails addressing these specific challenges. Releasing this dataset is part of open science. This dataset can encourage and support other researchers to conduct research on Auslan. By utilising this dataset, researchers can develop and improve algorithms and applications that understand and generate Auslan. For example, real-time sign language translation systems can be developed, making it easier for deaf individuals to communicate with others and enhancing their social participation and quality of life. This will also contribute to increased awareness and understanding of the Australian deaf community and foster broader social engagement.

Apart from computer science, our dataset will provide new opportunities for interdisciplinary research. For instance, linguists can use it to study the linguistic features and variations of Auslan, social scientists can investigate the culture and social interactions of the Australian deaf community, and psychologists and neuroscientists can explore the cognitive and neural mechanisms underlying sign language processing and learning. More broaderly, the deaf community and sign language users are often overlooked in dataset and technological advancements. This oversight can lead to biases and imbalances within AI models as well. Our releasing this dataset will help bridge that gap and provide necessary data resources for creating more equitable and inclusive AI systems. In the rapidly advancing era of AI, it is of paramount importance to ensure that the needs and inclusion of the deaf community are not overlooked.

## References

* [1] Amanda Cardoso Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres, and Xavier Giro-i-Nieto. How2sign: A large-scale multimodal dataset for continuous american sign language. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 2735-2744. Computer Vision Foundation / IEEE, 2021.
* [2] Samuel Albanie, Gul Varol, Liliane Momeni, Hannah Bull, Triantafyllos Afouras, Himel Chowdhury, Neil Fox, Bencie Woll, Rob Cooper, Andrew McParland, and Andrew Zisserman. Bbc-oxford brittish sign language dataset. _CoRR_, abs/2111.03635, 2021.
* [3] Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and Houqiang Li. Improving sign language translation with monolingual data by sign back-translation. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 1316-1325. Computer Vision Foundation / IEEE, 2021.
* [4] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural sign language translation. In _2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018_, pages 7784-7793. Computer Vision Foundation / IEEE Computer Society, 2018.
* [5] Natasha Abner, Carlo Geraci, Shi Yu, Jessica Lettieri, Justine Mertz, and Anah Salgat. Getting the upper hand on sign language families: Historical analysis and annotation methods. _FEAST. Formal and Experimental Advances in Sign language Theory_, 3:17-29, 2020.
* [6] William C Stokoe Jr. Sign language structure: An outline of the visual communication systems of the american deaf. _Journal of deaf studies and deaf education_, 10(1):3-37, 2005.
* [7] William C Stokoe. Sign language structure. _Annual review of anthropology_, 9(1):365-390, 1980.
* [8] Thomas Hanke, Marc Schulder, Reiner Konrad, and Elena Jahn. Extending the Public DGS Corpus in size and depth. In _Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives_, pages 75-82, Marseille, France, May 2020. European Language Resources Association (ELRA).
* [9] Sang-Ki Ko, Chang Jo Kim, Hyedong Jung, and Choong Sang Cho. Neural sign language translation based on human keypoint estimation. _CoRR_, abs/1811.11436, 2018.
* [10] Ulrich von Agris, Moritz Knorr, and Karl-Friedrich Kraiss. The significance of facial features for automatic sign language recognition. In _8th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2008), Amsterdam, The Netherlands, 17-19 September 2008_, pages 1-6. IEEE Computer Society, 2008.
* [11] Aoxiong Yin, Zhou Zhao, Weike Jin, Meng Zhang, Xingshan Zeng, and Xiaofei He. MLSLT: towards multilingual sign language translation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 5099-5109. IEEE, 2022.
* [12] Necati Cihan Camgoz, Ben Saunders, Guillaume Rochette, Marco Giovanelli, Giacomo Inches, Robin Nachtrab-Ribback, and Richard Bowden. Content4all open research sign language translation datasets. In _16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021, Jodhpur, India, December 15-18, 2021_, pages 1-5. IEEE, 2021.
* [13] Bowen Shi, Diane Brentari, Gregory Shakhnarovich, and Karen Livescu. Open-domain sign language translation learned from online video. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 6365-6379. Association for Computational Linguistics, 2022.
* [14] Adrian Nunez-Marcos, Olatz Perez-de-Vinaspre, and Gorka Labaka. A survey on sign language machine translation. _Expert Syst. Appl._, 213(Part):118993, 2023.
* [15] Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe Alikhani. Including signed languages in natural language processing. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, pages 7347-7360. Association for Computational Linguistics, 2021.

* [16] Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu. Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [17] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. RMPE: Regional multi-person pose estimation. In _ICCV_, 2017.
* [18] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10863-10872, 2019.
* [19] River Tae Smith, Louisa Willoughby, and Trevor Johnston. Integrating Auslan resources into the language data commons of Australia. In _Proceedings of the LREC2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources_, pages 181-186, Marseille, France, June 2022. European Language Resources Association.
* [20] Samuel Albanie, Gul Varol, Liliane Momeni, Triantafyllos Afouras, Joon Son Chung, Neil Fox, and Andrew Zisserman. Bsl-1k: Scaling up co-articulated sign language recognition using mouthing cues. In _European conference on computer vision_, pages 35-53. Springer, 2020.
* [21] Hannah Bull, Triantafyllos Afouras, Gul Varol, Samuel Albanie, Liliane Momeni, and Andrew Zisserman. Aligning subtitles in sign language videos. In _2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021_, pages 11532-11541. IEEE, 2021.
* [22] Steve Cassidy, Onno Crasborn, Henri Nieminen, Wessel Stoop, Micha Hulsbosch, Susan Even, Erwin Komen, and Trevor Johnson. Signbank: Software to support web based dictionaries of sign language. In Nicoletta Calzolari, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, _Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018_. European Language Resources Association (ELRA), 2018.
* [23] Trevor Johnston. From archive to corpus: transcription and annotation in the creation of signed language corpora. In Rachel E. O. Roxas, editor, _Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation, PACLIC 22, Cebu City, Philippines, November 20-22, 2008_, pages 16-29. De La Salle University (DLSU), Manila, Philippines, 2008.
* [24] Trevor Johnston. From archive to corpus: Transcription and annotation in the creation of signed language corpora. _International journal of corpus linguistics_, 15(1):106-131, 2010.
* [25] Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. A simple multi-modality transfer learning baseline for sign language translation. _CoRR_, abs/2203.04287, 2022.
* [26] Kayo Yin and Jesse Read. Better sign language translation with stmc-transformer. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 5975-5989, 2020.
* [27] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Sign language transformers: Joint end-to-end sign language recognition and translation. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020_, pages 10020-10030. Computer Vision Foundation / IEEE, 2020.
* [28] Hao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li. Spatial-temporal multi-cue network for sign language recognition and translation. _IEEE Transactions on Multimedia_, 24:768-779, 2021.
* [29] Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. A simple multi-modality transfer learning baseline for sign language translation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 5110-5120. IEEE, 2022.
* [30] Yutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, Shujie Liu, and Brian Mak. Two-stream network for sign language recognition and translation. In _NeurIPS_, 2022.
* [31] Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Benjamin Swift, Hanna Suominen, and Hongdong Li. Tspnet: Hierarchical feature learning via temporal semantic pyramid for sign language translation. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.

* [32] Jian Zhao, Weizhen Qi, Wengang Zhou, Nan Duan, Ming Zhou, and Houqiang Li. Conditional sentence generation and cross-modal reranking for sign language translation. _IEEE Transactions on Multimedia_, 24:2662-2672, 2021.
* [33] Aoxiong Yin, Tianyun Zhong, Li Tang, Weike Jin, Tao Jin, and Zhou Zhao. Gloss attention for gloss-free sign language translation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2551-2562, 2023.
* [34] Jiangbin Zheng, Yidong Chen, Chong Wu, Xiaodong Shi, and Suhail Muhammad Kamal. Enhancing neural sign language translation by highlighting the facial expression information. _Neurocomputing_, 464:462-472, 2021.
* ECCV 2022
- 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV_, volume 13695 of _Lecture Notes in Computer Science_, pages 671-690. Springer, 2022.
* [36] Dongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li. Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison. In _The IEEE Winter Conference on Applications of Computer Vision_, pages 1459-1469, 2020.
* [37] Hamid Reza Vaezi Joze and Oscar Koller. MS-ASL: A large-scale data set and benchmark for understanding american sign language. In _30th British Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019_, page 100. BMVA Press, 2019.
* ACCV 2020
- 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30
- December 4, 2020, Revised Selected Papers, Part VI_, volume 12627 of _Lecture Notes in Computer Science_, pages 291-308. Springer, 2020.
* ECCV 2022 Workshops
- Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII_, volume 13808 of _Lecture Notes in Computer Science_, pages 271-287. Springer, 2022.
* [40] Gul Varol, Liliane Momeni, Samuel Albanie, Triantafyllos Afouras, and Andrew Zisserman. Scaling up sign spotting through sign language dictionaries. _Int. J. Comput. Vis._, 130(6):1416-1439, 2022.
* [41] Bowen Shi, Diane Brentari, Greg Shakhnarovich, and Karen Livescu. Fingerspelling detection in american sign language. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 4166-4175. Computer Vision Foundation / IEEE, 2021.
* [42] K. R. Prajwal, Hannah Bull, Liliane Momeni, Samuel Albanie, Gul Varol, and Andrew Zisserman. Weakly-supervised fingerspelling recognition in british sign language videos. In _33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022_, page 609. BMVA Press, 2022.
* [43] Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Jonathan Michaux, Diane Brentari, Greg Shakhnarovich, and Karen Livescu. American sign language fingerspelling recognition in the wild. In _2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018_, pages 145-152. IEEE, 2018.
* [44] Mark Borg and Kenneth P Camilleri. Sign language detection "in the wild" with recurrent neural networks. In _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1637-1641. IEEE, 2019.
* [45] Amit Moryossef, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling, and Srini Narayanan. Real-time sign language detection using human pose estimation. In _European Conference on Computer Vision_, pages 237-248. Springer, 2020.
* [46] Samuel Albanie, Gul Varol, Liliane Momeni, Triantafyllos Afouras, Andrew Brown, Chuhan Zhang, Ernesto Coto, Necati Cihan Camgoz, Ben Saunders, Abhishek Dutta, Neil Fox, Richard Bowden, Bencie Woll, and Andrew Zisserman. Seehear: Signer diarisation and a new dataset. In _IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021_, pages 2280-2284. IEEE, 2021.

* [47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* [48] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pages 74-81, 2004.
* [49] Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The THUMOS challenge on action recognition for videos "in the wild". _Comput. Vis. Image Underst._, 155:1-23, 2017.
* [50] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015_, pages 961-970. IEEE Computer Society, 2015.
* [51] Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. Learning to track for spatio-temporal action localization. In _2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015_, pages 3164-3172. IEEE Computer Society, 2015.
* ECCV 2018
- 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part I_, volume 11205 of _Lecture Notes in Computer Science_, pages 106-121. Springer, 2018.
* [53] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, pages 7444-7452. AAAI Press, 2018.
* [54] Hezhen Hu, Weichao Zhao, Wengang Zhou, Yuechen Wang, and Houqiang Li. Signbert: Pre-training of hand-model-aware representation for sign language recognition. In _2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021_, pages 11067-11076. IEEE, 2021.
* [55] Gokul NC, Manideep Ladi, Sumit Negi, Prem Selvaraj, Pratyush Kumar, and Mitesh Khapra. Addressing resource scarcity across sign languages with multilingual pretraining and unified-vocabulary datasets. In _NeurIPS_, 2022.
* 24, 2021_, pages 4353-4361. ACM, 2021.
* [57] Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. A simple multi-modality transfer learning baseline for sign language translation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 5110-5120. IEEE, 2022.
* [58] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In _2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pages 4724-4733. IEEE Computer Society, 2017.
* [59] Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation. In Lluis Marquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015_, pages 1412-1421. The Association for Computational Linguistics, 2015.
* [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [61] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.

* [62] D. Crystal. A dictionary of linguistics and phonetics. 1997.
* [63] Gabrielle Hodge. _Patterns from a signed language corpus: Clause-like units in Auslan (Australian sign language)_. PhD thesis, Macquarie University, 2014.
* [64] Sameen Maruf, Fahimeh Saleh, and Gholamreza Haffari. A survey on document-level neural machine translation: Methods and evaluation. _ACM Comput. Surv._, 54(2):45:1-45:36, 2022.
* [65] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. _CoRR_, abs/2212.02437, 2022.
* [66] Lei Shen, Haolan Zhan, Xin Shen, and Yang Feng. Learning to select context in a hierarchical and global perspective for open-domain dialogue generation. In _IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021_, pages 7438-7442. IEEE, 2021.
* 24, 2021_, pages 4287-4296. ACM, 2021.
* [68] Dongxu Li, Xin Yu, Chenchen Xu, Lars Petersson, and Hongdong Li. Transferring cross-domain knowledge for video sign language recognition. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020_, pages 6204-6213. Computer Vision Foundation / IEEE, 2020.
* [69] Roman Tongi. Application of transfer learning to sign language recognition using an inflated 3d deep convolutional neural network. _CoRR_, abs/2103.05111, 2021.
* [70] Md. Monirul Islam, Md. Rasel Uddin, Md. Nasim AKhtar, and K.M. Rafiqul Alam. Recognizing multiclass static sign language words for deaf and dumb people of banglades based on transfer learning techniques. _Informatics in Medicine Unlocked_, 33:101077, 2022.