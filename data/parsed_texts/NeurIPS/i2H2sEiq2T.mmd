# A Unified Fast Gradient Clipping Framework

for DP-SGD

 Weiwei Kong

Google Research

weiweikong@google.com &Andres Munoz Medina

Google Research

ammedina@google.com

###### Abstract

A well-known numerical bottleneck in the differentially-private stochastic gradient descent (DP-SGD) algorithm is the computation of the gradient norm for each example in a large input batch. When the loss function in DP-SGD consists of an intermediate linear operation, existing methods in the literature have proposed decompositions of gradients that are amenable to fast norm computations. In this paper, we present a framework that generalizes the above approach to arbitrary (possibly nonlinear) intermediate operations. Moreover, we show that for certain operations, such as fully-connected and embedding layer computations, further improvements to the runtime and storage costs of existing decompositions can be deduced using certain components of our framework. Finally, preliminary numerical experiments are given to demonstrate the substantial effects of the aforementioned improvements.

## 1 Introduction

Machine learning models -- more specifically, neural network-based models -- are becoming more popular in industrial applications, in user-facing products, and different scientific fields. The popularity of these models lies on their flexibility and ability to be trained on ever bigger datasets, which may contain personal information about individuals. As these models become bigger and more descriptive, ML practitioners need to ensure that the models, and their black-box interactions, do not reveal information about the data used to train the model. In fact, it has been shown repeatedly [18] that large neural-network models can be used to know if a particular example was used in the training data. Another line of attacks has demonstrated that one can actually reconstruct some training instances with simple interactions with a trained model [3].

The only known robust way of protecting against these attacks is to train models using differential privacy [10; 11]. Using this approach, ML practitioners provide an information theoretic guarantee that ensures that the final model does not depend on any individual example1. To date, the most popular method for training models with differential privacy is the differentially private stochastic gradient descent (DP-SGD) [1] method. In theory, the DP-SGD algorithm requires only minimal changes with respect to its non-private counterpart; one only requires to clip gradients observed in the training process and add some noise proportional to the clipping value. However, in practice, the (naive) gradient clipping step has been shown to increase memory and computational costs in all popular learning platforms (JAX, Tensorflow, and Pytorch). More precisely, for a batch of size \(n\) a naive implementation of DP-SGD requires calculating \(n\) gradients (one for each example in the batch) so they can be clipped. This is in stark contrast with most back-propagation-based training algorithms which calculate a single gradient. This implies a dependency on the runtime and memory in proportion to the batch size used to train the model, and for large models, this cost makes theprospect of training with differential privacy simply not viable. Several papers [8; 17; 21] have also observed that large models need to be trained with large batch sizes to obtain competitive levels of utility, under reasonable amounts of privacy.

In [12], it was recognized that one could clip gradients without actually materializing every example gradient. This technique is referred to as _ghost clipping_. The initial proposal from [12] was specialized to fully connected, feed-forward neural networks that consist of only dense layers. The ghost clipping algorithm was later extended to handle convolution layers [13; 15] and attention layers [14].

However, each of the above solutions required an ad-hoc analysis on the correctness of their implementation. In this work we present a general analysis of the ghost clipping algorithm. To produce our results, we study this algorithm from the lens of linear operator theory. Our paper has three main contributions:

1. We unify years of ad-hoc analysis and interpretations of the ghost clipping algorithm under a single framework.
2. We provide a future-proof way of expanding the ghost-clipping technique to new layers with (possibly) nonlinear dependencies on their weights.
3. We demonstrate that by framing the ghost clipping problem in the language of linear operators, we can obtain a better performance of DP-SGD on models with embedding layers (crucial for personalization models consisting of embeddings of tens of millions of parameters) and fully connected layers with linear bias broadcasting operators.

Additionally, we show in our Appendix how to apply our framework to more complex tranforms/layers such as layer normalization and multi-head attention.

To complement the results of this paper, we open-sourced the general interface of the code using the TensorFlow Keras API2. By introducing an abstract interface, we also expect practitioners to easily extend the ghost clipping algorithm to any type of layer.

Footnote 2: See the repo in https://github.com/google-research/google-research/tree/master/fast_gradient_clipping

## 2 Notation and preliminaries

Throughout the paper, \((\mathcal{W},\langle\cdot,\cdot\rangle)\) denotes a Hilbert space and \(\|\cdot\|\) denotes its induced norm. Examples of \(\mathcal{W}\) are \(\mathbb{R}^{d}\) with the standard dot product and the space of matrices \(\mathcal{W}=\mathbb{R}^{p\times q}\) with the Euclidean (Frobenius) inner product between \(A,B\in\mathcal{W}\) given by \(\langle A,B\rangle=\text{tr}(A^{\top}B)\). For two matrices \(A,B\) we let \(\|A\|\) denote the Frobenius norm of \(A\).

Given two Hilbert spaces \((\mathcal{W},\langle\cdot,\cdot\rangle_{\mathcal{W}})\), \((\mathcal{Y},\langle\cdot,\cdot\rangle_{\mathcal{Y}})\), we denote linear operators between them by italicized letters (\(\mathcal{A}\colon\mathcal{W}\to\mathcal{Y}\)) and \(\mathcal{A}^{*}:\mathcal{Y}\to\mathcal{W}\) to be the _adjoint_ of \(\mathcal{A}\). That is, \(\mathcal{A}^{*}\) is the unique linear operator that satisfies

\[\langle y,\mathcal{A}w\rangle_{\mathcal{Y}}=\left\langle\mathcal{A}^{*}y,w \right\rangle_{\mathcal{W}}\quad\forall w\in\mathcal{W},\quad\forall y\in \mathcal{Y}.\] (1)

Let \((\mathcal{W},\langle\cdot,\cdot\rangle_{\mathcal{W}})\) and \((\mathcal{Y},\langle\cdot,\cdot\rangle_{\mathcal{Y}})\) denote two Hilbert spaces with respective induced norms \(\|\cdot\|_{\mathcal{W}}\) and \(\|\cdot\|_{\mathcal{Y}}\). Moreover, let \(\psi:\mathcal{W}\to\mathcal{Y}\) be an arbitrary function. The _Frechet derivative_ of \(\psi\) at \(w_{0}\in\mathcal{W}\) is given by the unique bounded linear operator \(D\psi(w_{0})\colon\mathcal{W}\to\mathcal{Y}\) satisfying

\[\lim_{\delta\to 0}\frac{\|\psi(w_{0}+\delta)-\psi(w_{0})-D\psi(w_{0})\delta\|_{ \mathcal{Y}}}{\|\delta\|_{\mathcal{W}}}=0.\]

We say \(\psi\) is differentiable if its Frechet derivative exists for all \(w_{0}\in\mathcal{W}\). Throughout this paper we will use two special properties of the Frechet derivative: the chain rule and the existence of gradients. Let \((\mathcal{Z},\langle\cdot,\cdot\rangle_{\mathcal{Z}})\) be another Hilbert space and \(\phi\colon\mathcal{Y}\to\mathcal{Z}\) be given. The _chain rule_ provides us with a simple way to calculate the derivative of the function \(\phi\circ\psi\colon\mathcal{W}\to\mathcal{Z}\), namely,

\[D(\phi\circ\psi)(w_{0})=D\phi(\psi(w_{0}))D\psi(w_{0}).\]

The Frechet derivative of \(\psi\) at \(w_{0}\) with respect to a subset of variables \(u\) is denoted by \(D_{u}\phi(w_{0})\). Finally, \(\nabla\psi(w_{0})\in\mathcal{W}\) denotes the (unique) gradient of a function \(\psi\) at \(w_{0}\), which satisfies

\[D\psi(w_{0})\delta=\left\langle\nabla\psi(w_{0}),\delta\right\rangle_{ \mathcal{W}}\quad\forall\delta\in\mathcal{W}\] (2)

The existence of the gradient is guaranteed by the well-known Riesz-Frechet Representation Theorem [16]. The gradient of \(\psi\) at \(w_{0}\) with respect to a set of variables \(u\) is denoted by \(\nabla_{u}\psi(w_{0})\).

Private Stochastic Gradient DescentWe now turn our attention to the DP-SGD algorithm and specifically, to its instantiation for neural network-based models. Let \(\mathcal{X}\) be an arbitrary space and \(\mathcal{S}=\{x_{1},\ldots,x_{n}\}\subset\mathcal{X}\) be a sample of examples. Let \(\mathcal{W}\) denote a parameter space and \(h\colon\mathcal{X}\times\mathcal{W}\to\mathbb{R}\) denote an arbitrary function. The stochastic gradient descent (SGD) algorithm solves the optimization problem

\[\min_{w\in\mathcal{W}}\frac{1}{n}\sum_{i=1}^{n}h(x_{i},w)\]

by iteratively updating the model paramters using gradients of the loss over a batch \(B\subset\mathcal{S}\) of data.

The celebrated DP-SGD algorithm was introduced by [1] as a simple modification on the SGD algorithm to make it private3. Specifically, the DP-SGD algorithm, given in Algorithm 1, is identical to the SGD algorithm except in two steps:

Footnote 3: When talking about privacy we are referring to the concept of differential privacy. We refer the reader to [11] for a comprehensive guide on differential privacy. In this work we focus on the computational aspects and not the privacy properties of the DP-SGD algorithm

1. The DP-SGD algorithm needs to calculate \(|B|\) (so-called) per-example gradients in order to clip them to have a bounded norm.
2. Adding Gaussian noise \(N(0,C^{2}\sigma^{2}/|B|^{2})\) for some noise multiplier \(\sigma>0\).

Notice that the first step has a prohibitively large cost for large networks. Specifically, in the per example gradient calculation, notice that that the computational and memory usage of a naive implementation4 of DP-SGD algorithm increases as \(\Theta(n_{w}|B|)\), where \(n_{w}\) is the number of parameters in the network. This increase in resources effectively negates the advantages of the backpropagation algorithm. In this paper we show how to run the DP-SGD algorithm with only a small constant increment in both the memory footprint and runtime of traditional SGD.

Footnote 4: Alternatively, one could consider sequentially computing the gradient norms to remove the linear memory scaling in \(|B|\). However, this approach may prohibitively increase the runtime cost in practice.

We will focus on the scenario where the learner is trying to minimize the loss across the space of neural networks. That is, the parameter vector \(w\) is a concatenation of \(k\) parameters \((w_{1},\ldots,w_{k})\) and there exists \(k\) functions \(\phi_{1},\ldots,\phi_{k}\), and a loss function \(\ell\) such that

\[h(x,w)=\ell(x,\phi_{k}(w_{k},\phi_{k-1}(w_{k-1},...,\phi_{1}(w_{1},x))).\]

Note that \(\{\phi_{j}\}\) correspond to the layers of the network and \(\{w_{j}\}\) are the vectors parameterizing these layers.

## 3 Previous work

The DP-SGD algorithm was first introduced by [1]. Due to its simplicity to adapt to standard machine learning frameworks it is now probably the most popular method for training private machine learning models. In the past few years, a large number of papers have been devoted to improving the privacy-utility trade-offs of DP-SGD [4] as well as demonstrating that DP-SGD can be used on multiple tasks [7; 9; 2]. While the main focus on this line of research has been in understanding the privacy-utility trade-offs, some authors did touch on the performance issues of calculating per-example gradients. A simple solution to reduce the memory and computational blow-up of the DP-SGD algorithm is to calculate the gradient with respect to a micro-batch of examples instead. While this reduced the computational cost of running DP-SGD, it came at the cost of decreased quality of the model learned. Some authors [2] got around this performance issues by using some JIT compilation features of JAX. However these authors still observed an increase in the memory use of their private implementations.

In order to speed up the gradient clipping step without sacrificing quality, [12] proposed the ghost clipping technique (see Section 4). ITs general idea is that one can obtain a private gradient estimate \(\widetilde{g}\) without materializing each per example gradient as long as one knows the norm of each per-example gradient. Moreover, the authors show that one can easily calculate these norms by using information already materialized in the forward and backward passes of the back propagation algorithm for training neural networks. The results however were limited to neural networks consisting of only dense layers. The ghost clipping trick was later on applied to train transformers [14] and image classification models which consist of networks with convolution layers [15].

The above works provide ad-hoc proofs that their implementation of the ghost clipping algorithm is correct for their particular layer implementation but do not attempt to extend their results to arbitrary layers. In this work we present a way to extend the ghost clipping technique to arbitrary layers and pinpoint what properties of each layer are crucial for enabling an efficient implementation of the DP-SGD algorithm.

Finally, it is also worth mentioning that paper [6] presents an efficient implementation of the ghost clipping technique in PyTorch for models with fully-connected, embedding, convolution, and normalization layers using variable caching to avoid recomputing certain intermediate parameter gradients. Our implementation also takes advantage of variable caching, but does this implicitly through TensorFlow's GradientTape API5.

Footnote 5: See https://www.tensorflow.org/api_docs/python/tf/GradientTape

## 4 The Ghost Clipping Algorithm

Algorithm 1 suggests that in order to implement DP-SGD one must calculate every per-example gradient, clip it to achieve a bounded norm and then aggregate it to obtain the private gradient estimate \(\widetilde{g}\). The main observation from [12] was that, with knowledge of the norms \(\|g\|_{x}\) one could estimate \(\widetilde{g}\) without explicitly materializing each per example gradient. To do this, the authors define weights \(r_{x}=\min\left\{\hat{C}/\|g_{x}\|,1\right\}\) and a new weighted sum

\[S(w)=\sum_{x\in B}r_{x}h(x,w).\]

Treating the weights \(r_{x}\) as fixed (with respect to the parameters \(w\)), one can use the linearity of the gradient operator to see that \(\nabla S(w)=\widetilde{g}\). Crucially, for neural networks, the gradient of \(S\) can be efficiently calculated using a single back-propagation step. The second contribution of [12] was showing that, for neural networks consisting of dense layers, one can also efficiently calculate \(\|g_{x}\|\) for every \(x\in B\) using only a single forward and backward pass of the back propagation algorithm. That is, the cost of calculating a private gradient is simply twice that of calculating a non-private gradient step.

In the subsections below, we present a general formulation for calculating \(\|g_{x}\|\) in a single forward and backward pass for arbitrary feed-forward neural networks, under the assumption that:

* \(\ell(x,\cdot)\), \(\phi_{1}(\cdot,x)\),..., \(\phi_{k}(\cdot,x)\) are Frechet differentiable for every \(x\in\mathcal{X}\);
* each function \(\phi_{i}(\cdot,x)\) can be decomposed as the composition of at least two Frechet differentiable subfunctions.

### The General Norm Calculation Algorithm

Let \(w=(w_{1},\ldots,w_{k})\) parameterize a neural network. We first note that \(\|\nabla_{w}h(x,w)\|^{2}=\sum_{j=1}^{k}\|\nabla_{w_{k}}h(x,w)\|^{2}\). Hence, we can focus on efficiently calculating the norm of the gradient corresponding to the parameters of each layer. Let us then fix a layer and denote \(\bar{w}\in\bar{\mathcal{W}}\) to be its parameter vector for some restricted space \(\bar{\mathcal{W}}\). Moreover, let the input \(x\) be fixed. We see that, as a function of the layer parameters \(\bar{w}\) only, the loss \(h\) has the form

\[h(x,\bar{w})=\ell_{x}\circ\phi_{x}(\bar{w})\quad\forall x\in\mathcal{X},\] (3)

where \(\phi_{x}\) corresponds to operations performed by the fixed layer and all subsequent layers in the network, and \(\ell_{x}(\cdot)\equiv\ell(x,\cdot)\) corresponds to the loss function, e.g., mean-squared error, for the input example \(x\). The following result, whose proof can be found in the Appendix, gives an expression for the gradient of \(h\) under any decomposition of \(\phi_{x}\).

**Proposition 4.1**.: _Let \(x\in\mathcal{X}\) be fixed and let \((\ell_{x},\phi_{x})\) be a pair of Frechet differentiable functions satisfying (3). Moreover, let \((\psi_{x},Z_{x})\) be a pair of Frechet differentiable functions satisfying \(\phi_{x}=\psi_{x}\circ Z_{x}\), and denote_

\[\mathcal{A}=\mathcal{A}_{x}(\bar{w}):=DZ_{x}(\bar{w}),\quad g=g_{x}(\bar{w}):= \nabla(\ell_{x}\circ\psi_{x})(Z_{x}(\bar{w})).\] (4)

_Then, \(\nabla_{\bar{w}}h(x,\bar{w})=\mathcal{A}^{*}g\)._

The above result gives us an alternative way of computing \(\|\nabla_{\bar{w}}h(x,\bar{w})\|^{2}\), namely, (i) pick a decomposition \((\psi_{x},Z_{x})\) of \(\phi_{x}\) and (ii) compute \(\|\mathcal{A}^{*}g\|^{2}\) in some efficient manner. In later subsections, we provide examples of layers and decompositions where this two-step approach is drastically more efficient than the naive approach of materializing \(\nabla_{\bar{w}}h(x,\bar{w})\) and computing its Euclidean norm.

In view of the decomposition in Proposition 4.1, we present a unified method for computing \(\|\nabla h(x,w)\|\) in Algorithm 2. Specifically, it consists of two loops over the parameters \(w_{i}\). The initial loop is performed to obtain the intermediate outputs \(\zeta_{i,x}\) and some sort of "nice" or efficient representation of the squared-norm functions \(\Omega_{i,x}\). The follow-up loop, also over the parameters \(w_{i}\), computes some necessary intermediate gradients and combines them with the functions \(\Omega_{i,x}\) to obtain the gradient decomposition (and subsequent norm computation) given by Proposition 4.1.

``` Input: Data \(x\in\mathcal{X}\), parameter \(w=(w_{1},\ldots,w_{k})\), and functions \(\{\psi_{x,i}\}_{i=1}^{k}\), \(\{Z_{x,i}\}_{i=1}^{k}\) satisfying \(h(x,\bar{w})=\ell_{x}\circ\psi_{x,i}\circ Z_{x,i}(\bar{w}),\quad i=1,\ldots,k\) for any \(\tilde{w}=(\tilde{w}_{1},\ldots,\tilde{w}_{k})\in\mathcal{W}\), where \(\ell_{x}(\cdot)\equiv\ell(x,\cdot)\). for\(i=1,\ldots,k\)do  Compute \(\zeta_{x,i}:=Z_{x,i}(w_{i})\)  Store a "nice" representation \(\Omega_{x,i}(\cdot)\) of the squared-norm function \(g\mapsto\|[DZ_{x,i}(w_{i})]^{*}(g)\|^{2}\)  for any gradient \(g\) of the function \(\ell_{x}\circ\psi_{x,i}(\cdot)\) endfor for\(i=1,\ldots,k\)do  Compute \(g_{x,i}:=\nabla_{\zeta_{i,x}}(\ell_{x}\circ\psi_{x,i})(\zeta_{x,i})\) and \(\tau_{x,i}:=\Omega_{x,i}(g_{x,i})\) endfor return\((\sum_{i=1}^{k}\tau_{x,i})^{1/2}\) ```

**Algorithm 2** General Gradient Norm Framework

### Efficient Implementation of Algorithm 2

We now describe how Algorithm 2 can be implemented efficiently when \(h(\cdot,\cdot)\) is formed by a neural network \(\mathcal{N}\). First, it is shown in Appendix B.1 that the intermediate quantities \(\{\zeta_{i,x}\}_{i=1}^{k}\) and \(\{g_{i,x}\}_{i=1}^{k}\) can be computed in a single forward and backward pass of \(\mathcal{N}\) (as opposed to naively traversing the network \(\Theta(k)\) times). Second, it is shown in Appendix B.2 that for any \(i\), the batch gradients \(\{\nabla_{\zeta_{i,x}}(\ell_{x}\circ\psi_{x,i})(\zeta_{x,i})\}_{x\in B}\) can be obtained a single (batched) backward pass of \(\mathcal{N}\) if (i) condition (5) holds for every \(x\in B\) and (ii) the gradient \(g_{x,i}\) is replaced by \(\nabla_{\zeta_{x,i}}\{\sum_{x\in B}\ell_{x}\circ\psi_{x,i}(\zeta_{x,i})\}\). Consequently, if we ignore the costs of forming and evaluating \(\Omega_{x,i}(\cdot)\) for each \(x\in B\)and \(i=1,\ldots,k\), then the above facts imply that we can obtain the norms \(\{\|\nabla h(x,w)\|\}_{x\in B}\) using only one additional forward and (batched) backward pass of \(\mathcal{N}\).

## 5 Efficient Squared-Norm Functions

This section presents efficient representations of the functions \(\Omega_{i}(\cdot)\) in Algorithm 2 for some basic layer functions \(\phi_{x}(w)\equiv\phi(w,x)\). Examples involving more complicated layer functions (e.g., layer normalization and multi-head attention) can be found in the Appendix.

Each subsection begins with a precise description of \(\phi_{x}(\cdot)\), presents a decomposition \(\phi(\cdot)=\psi_{x}\circ Z_{x}(\cdot)\), and gives an analysis of the runtime and storage costs of an efficient representation of the function \(\Omega_{x}(\cdot)\equiv\|[DZ_{x}(w)]^{*}(\cdot)\|^{2}\). For the sake of conciseness, the proofs of these decompositions are given in the Appendix.

In addition, we make comparisons of our approach with the naive approach (of computing gradients for each example) and other ghost clipping-like approaches. The results of these comparisons are summarized in Tables 1-2 (see the first paragraphs in Subsections 5.1-5.3 for descriptions of the variables/dimensions).

### Fully-Connected Layers

Given variables \(V\in\mathbb{R}^{p\times q}\) and \(b\in\mathbb{R}^{m}\), a layer input \(U_{x}\in\mathbb{R}^{r\times p}\), an activation function \(\alpha:\mathbb{R}^{r\times q}\mapsto\mathbb{R}^{r\times q}\), and a linear broadcasting operator \(\mathcal{Q}:\mathbb{R}^{m}\mapsto\mathbb{R}^{r\times q}\) satisfying \(m\,|\,rq\), the standard fully-connected layer function \(\phi_{x}(\cdot)\) is given by

\[\phi_{x}(V,b)=\alpha(U_{x}V+\mathcal{Q}b).\] (6)

Typically, \(p\), \(q\), \(r\), and \(m\) are called the _input dimension_, _output dimension_, _channel dimension_, and _bias dimension_, respectively. Usually, it is the case that \(r\ll\min\{p,q\}\) and a common case is \(r=1\).

We now consider the squared-norm function \(\Omega_{x}:\mathbb{R}^{r\times q}\mapsto\mathbb{R}\) generated by the choice of \(Z_{x}(V,b):=U_{x}V+\mathcal{Q}b\). Denoting \(\mathcal{A}\) as in (4), for some \(\bar{w}=(V,b)\), we have that

\[\Omega_{x}(g)=\|\mathcal{A}^{*}g\|^{2}=\|U_{x}^{*}g\|^{2}+\| \mathcal{Q}^{*}g\|^{2}=\langle U_{x}U_{x}^{*},gg^{*}\rangle+\|\mathcal{Q}^{*}g \|^{2},\] (7)

for any \(g\in\mathbb{R}^{r\times q}\). Hence, \(\Omega_{x}(\cdot)\) can be efficiently represented by \(U_{x}U_{x}^{*}\) and an efficient representation of the function \(g\mapsto\|\mathcal{Q}^{*}g\|^{2}\). For an example of the latter, suppose \(\mathcal{Q}\) is the operator that repeats \(b\) in an \(r\)-by-\(q\) matrix row-wise \(\rho:=rq/m\) times. Then, defining the row/column maps

\[\pi(\ell,k):=1+\left\lfloor\frac{(\ell-1)m+(k-1)}{q}\right\rfloor,\quad\xi( \ell,k):=1+\{[(\ell-1)m+(k-1)]\bmod r\},\]

\begin{table}
\begin{tabular}{c|c|c|c}  & Naive & Ghost Clipping & **Ours** \\ \hline Fully-Connected6  & \(\Theta(|B|\{pq+m\})\) & \(\Theta(r^{2}+|B|\{rq\}^{2})\) & \(\Theta(|B|r^{2})\) \\ Embedding7  & \(\Theta(|B|qd)\) & \(O(|B|q^{2})\) & \(\Theta(|B|\{q\log\bar{q}+\bar{q}d\})\) \\ Rank-\(k\) Approx. & \(\Theta(|B|n^{2}k)\) & - & \(\Theta(n^{2}k+|B|)\) \\ \end{tabular}
\end{table}
Table 1: Asymptotic storage costs for computing \(\{\|\nabla h(x,W)\|\}_{x\in B}\). For ghost clipping and our approach, this is the storage cost of representing the squared-norm function \(\Omega_{x}(\cdot)\) on the entire batch \(B\).

\begin{table}
\begin{tabular}{c|c|c|c}  & Naive & Ghost Clipping & **Ours** \\ \hline Fully-Connected6  & \(\Theta(|B|\{pq+m\})\) & \(\Theta(r^{2}\{p+|B|q^{2}\}+m\{rq\}^{2})\) & \(\Theta(r^{2}\{p+|B|q\})\) \\ Embedding7  & \(\Theta(|B|\{pqd\})\) & \(O(|B|q^{2}d)\) & \(\Theta(|B|\{q\log\bar{q}+\bar{q}d\})\) \\ Rank-\(k\) Approx. & \(\Theta(|B|n^{2}k)\) & - & \(\Theta(n^{2}k+|B|)\) \\ \end{tabular}
\end{table}
Table 2: Asymptotic runtime costs for computing \(\{\|\nabla h(x,W)\|\}_{x\in B}\). For ghost clipping and our approach, this includes the time used to generate, represent, and evaluate the squared-norm function \(\Omega_{x}(\cdot)\) on the entire batch \(B\).

[MISSING_PAGE_FAIL:7]

them with the \(\tilde{q}\) nonzero cached values \(\{n_{k}(x)\}_{k=1}^{r}\), which can be done with a \(\Theta(|B|\tilde{q}d)\) runtime cost for a batch \(B\). Consequently, the total runtime (resp. storage) cost for a batch of examples \(B\) is \(\Theta(|B|\{q\log\tilde{q}+\tilde{q}d\})\) (resp. \(\Theta(|B|\tilde{q})\)) using Algorithm 3.

Let us now make a comparison of this decomposition with the naive approach and the ghost clipping approach in [13].

In the naive approach of computing \(\nabla_{W}h(x,W)\) for each \(x\in B\), it is straightforward9 to show that the compute (resp. storage) costs are \(\Theta(|B|qd)\) (resp. \(\Theta(|B|qd)\)).

Footnote 9: Hint: we need to store (and take the norm of) \(q\) embedding vectors in \(\mathbb{R}^{d}\) for each example in the batch \(B\).

In the classic ghost clipping approach, we treat \(\phi_{x}(\cdot)\) as a linear operator, compute \(Y_{\pi(x)}Y_{\pi(x)}^{*}\), and choose the representation \(\Omega_{x}(g)=\langle Y_{\pi(x)}Y_{\pi(x)}^{*},gg^{*}\rangle\). In the worst-case scenario, it is straightforward to see that \(Y_{\pi(x)}Y_{\pi(x)}^{*}\) can be fully dense, e.g., \(\tilde{q}=1\). In this setting, even if \(Y_{\pi(x)}\) is sparsely represented by a small set of \(\tilde{q}\) unique indices, computing \(Y_{\pi(x)}Y_{\pi(x)}^{*}\) still incurs a compute and storage cost of \(O(q^{2})\). Now, since each example gradient \(g\) consists of \(q\) embedding vectors in \(\mathbb{R}^{d}\), for a batch \(B\), the compute (resp. storage) cost of materializing \(gg^{*}\) is \(\Theta(|B|q^{2}d)\) (resp. \(\Theta(|B|q^{2})\)). Combining the above complexities with the \(\Theta(|B|q^{2})\) runtime cost of computing the desired inner products, i.e. (7) with \(U_{x}=Y_{\pi(x)}\) for \(x\in B\), we obtain the complexities in Tables 1-2.

### Low Rank Approximation Layer

Given input matrix \(U_{x}\in\mathbb{R}^{n\times n}\), one way [19; 20] to encourage a rank-\(k\) (or lower) approximation of \(U_{x}\) is to add the intermediate layer transform

\[\phi_{x}(V)=\|U_{x}-VV^{*}\|^{2}+\rho\circ\sigma(VV^{*})\]

for some \(V\in\mathbb{R}^{n\times k}\), where \(\rho(\cdot)\) is a sparsity promoting regularizer (e.g., \(\ell_{1}\) norm, SCAD, MCP) and \(\sigma(\cdot)\) is the function that maps matrices to their singular values. Expanding the norm term, note that the above function can be equivalently (ignoring terms depending solely on \(U_{x}\)) expressed as

\[\phi_{x}(V)=-2\left\langle U_{x},VV^{*}\right\rangle+\mathcal{R}(V),\] (8)

for some function \(\mathcal{R}:\mathbb{R}^{n\times k}\mapsto\mathbb{R}\). Note that \(\mathcal{R}(V)\) does not depend on \(x\) and, hence, its computation does not depend the batch \(B\).

In view of (8), we now consider the squared-norm function \(\Omega_{x}:\mathbb{R}\mapsto\mathbb{R}\) generated by the choice of \(Z_{x}(V)=\langle U_{x},VV^{*}\rangle/2\), where, clearly, one has \(\phi_{x}(V)=-4Z_{x}(V)+\mathcal{R}(V)\). Denoting \(\mathcal{A}\) as in (4), for some \(\bar{w}=V\), it can be shown that

\[\Omega_{x}(g)=\|\mathcal{A}^{*}g\|^{2}=\frac{g^{2}}{4}\|(U_{x}+U_{x}^{*})V\|^ {2},\]

for \(g\in\mathbb{R}\). Hence, \(\Omega_{x}(\cdot)\) can be efficiently represented by the scalar \(\|(U_{x}+U_{x}^{*})V\|^{2}\). It is straightforward to see that computing \(\|(U_{x}+U_{x}^{*})V\|\) requires only a \(\Theta(n^{2}k)\) runtime cost and a \(\Theta(1)\) storage cost. Moreover, evaluation of \(\Omega_{x}(\cdot)\), given \(\|(U_{x}+U_{x}^{*})V\|\), requires only a \(O(1)\) runtime cost.

In the naive approach of computing \(\nabla_{V}h(x,V)\) for each \(x\in B\), it is straightforward to see that the compute (resp. storage) costs is \(\Theta(|B|n^{2}k)\) (resp. \(\Theta(|B|nk)\)) due to the excessive computation (resp. storage) of \(g(U_{x}+U_{x}^{*})V\) for \(x\in B\). The authors are not aware of any ghost clipping-like techniques in the nonlinear setting.

## 6 Numerical Experiments

This section presents numerical experiments that compare our proposed adjoint-based framework (Adjoint) against the naive implementation of DP-SGD (Naive), which computes gradients for each example in a batch, and the classic ghost clipping frameworks (GhostClip) that are described in Subsections 5.1 and 5.2. Specifically, it presents runtimes and memory costs for the gradient norm computation of fully-connected and embedding layers.

Each problem instances was run on a cloud computing platform consisting of (i) 112 Intel(R) Xeon(R) Platinum processors with 28 cores each, (ii) 64 GB of RAM, (iii) Python 3.10.11, and (iv) Tensorflow 2.14. For simplicity, memory usage was measured as the peak amount of heap memory utilized in the run of single gradient norm computation. We also simplify our computations by utilizing batches of size \(|B|=1\) for the first two subsections. The loss function used in our experiment, \(\ell_{x}(\cdot)\), is the mean-squared-error. To reduce the variance of the results in the first two subsections, we repeat each problem instance 20 times and report only the median runtime and memory cost over the repetitions.

### Fully-Connected Layer

Figure 1 presents numerical results for the setting considered in Subsection 5.1, where \(\mathcal{Q}\) is the (linear) broadcasting operator that duplicates the bias \(rq/m\) times to match the dimension of the layer outputs. It specifically plots the effect of the bias dimension \(m\) for various values of the output dimension \(q\). For simplicity, all problem instances fix an input and channel dimension of \(2\) and \(4096\), respectively. Additional experiments, involving the effect of batch size, are given in Appendix F.

The results in Figure 1 demonstrate that the runtime and memory costs of Adjoint are marginal compared to those of GhostClip for fully-connected layers. These results also support the analysis of Subsection 5.1 in that: (i) GhostClip's runtime has a stronger positive dependence on the bias dimension \(m\) than Adjoint's runtime and (iii) GhostClip's runtime has a strong positive dependence on the output dimension \(q\).

### Embedding Layer

Figure 2 presents numerical results for the setting considered in Subsection 5.2. It specifically plots the effect of the number of queries \(q\) for various values of the vocabulary size \(r\). For simplicity, all problem instances fix the embedding dimension to be \(10\) and the embedding indices \(\{\pi_{i}(x)\}_{i=1}^{q}\) are chosen uniformly at randomly from the set \(\{1,\ldots,r\}\).

The results in Figure 2 demonstrate that the runtime and memory costs of Adjoint are marginal compared to those of GhostClip for embedding layers. Moreover, the memory cost graph supports

Figure 1: Runtime and memory cost graphs for fully-connected layer computations with bias dimensions \(m=\{2^{1},2^{2},\ldots,2^{11}\}\) and output dimensions \(q=3,4,5\).

Figure 2: Runtime and memory cost graphs for embedding computations with query sizes \(q=\{1000,2000,\ldots,10000\}\) and vocabulary sizes \(r=5000,7500,10000\).

the analysis in Subsection 5.2 in that GhostClip's memory cost has a significantly stronger positive dependence on the query size \(q\) compared to Adjoint's memory cost.

### Small BERT model

Figure 3 presents numerical results for an end-to-end training run of a small BERT model that consists of dense, embedding, normalization, and multi-head attention sublayers. It specifically plots the effect of the batch size \(|B|\) on the runtime and peak memory usage of training the model. To be more precise, the BERT model is an instance of the TensorFlow BertEncoder model with a vocabulary size of 100, one intermediate transformer layer, and all other parameters set to their default values. Each experiment consists of a single training loop of 50 iterations with uniformly sampled random input data of a query size of 5. The efficient squared norm functions were taken from the descriptions in Section 5 and Appendices E.1 and E.2.

The results in Figure 3 demonstrate that the runtimes and peak memory usages of Adjoint scale better and are comparatively smaller than the corresponding ones for Naive.

## 7 Concluding Remarks

The analysis in Subsection 5.1 may also be applied to more complex layers whose parameter transformations \(\phi_{x}(\cdot)\) primarily involve linear transforms. For example, it is shown in [5, Subsection 2.3] that 2D-convolution layers are equivalent to fully-connected layers when the example image is appropriately transformed to form the matrix \(U_{x}\) in (6) and, hence, our storage and runtime savings for fully-connected layers easily apply to 2D- (and generally \(n\)D-) convolution layers. Another example is the multihead attention layer, whose parameter transforms are simple matrix multiplications (see Appendix E.1 for a derivation).

### Limitations

The proposed framework only applies to layers with at least one differentiable intermediate transformation and models with differentiable losses. Moreover, the framework does not support shared trainable layers.

Figure 3: Runtime and memory cost graphs for training a small BERT model with batch sizes \(|B|=\{100,200,400,800,1600\}\).

## Acknowledgements

The authors acknowledge the generous support from Walid Krichene (Google) and Li Zhang (Microsoft), who have meticulously reviewed the derivation and implementation of the proposed framework.

## References

* Abadi et al. [2016] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* Anil et al. [2021] Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differentially private bert. _arXiv preprint arXiv:2108.01624_, 2021.
* Balle et al. [2022] Borja Balle, Giovanni Cherubin, and Jamie Hayes. Reconstructing training data with informed adversaries. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 1138-1156. IEEE, 2022.
* Bassily et al. [2019] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. _Advances in neural information processing systems_, 32, 2019.
* Bu et al. [2022] Zhiqi Bu, Jialin Mao, and Shiyun Xu. Scalable and efficient training of large convolutional neural networks with differential privacy. _Advances in Neural Information Processing Systems_, 35:38305-38318, 2022.
* Bu et al. [2023] Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Differentially private optimization on large model at small cost. In _International Conference on Machine Learning_, pages 3192-3218. PMLR, 2023.
* Carranza et al. [2023] Aldo Gael Carranza, Rezsa Farahani, Natalia Ponomareva, Alex Kurakin, Matthew Jagielski, and Milad Nasr. Privacy-preserving recommender systems with synthetic query generation using differentially private large language models. _arXiv preprint arXiv:2305.05973_, 2023.
* De et al. [2022] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy differentially private image classification through scale. _arXiv preprint arXiv:2204.13650_, 2022.
* Denison et al. [2022] Carson Denison, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Krishna Giri Narra, Amer Sinha, Avinash Varadarajan, and Chiyuan Zhang. Private ad modeling with dp-sgd. _arXiv preprint arXiv:2211.11896_, 2022.
* Dwork et al. [2006] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer, 2006.
* Dwork et al. [2014] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Goodfellow [2015] Ian Goodfellow. Efficient per-example gradient computations. _arXiv preprint arXiv:1510.01799_, 2015.
* Lee and Kifer [2021] Jaewoo Lee and Daniel Kifer. Scaling up differentially private deep learning with fast per-example gradient clipping. _Proceedings on Privacy Enhancing Technologies_, 2021(1), 2021.
* Li et al. [2021] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. _arXiv preprint arXiv:2110.05679_, 2021.
* Rochette et al. [2019] Gaspar Rochette, Andre Manoel, and Eric W Tramel. Efficient per-example gradient computations in convolutional neural networks. _arXiv preprint arXiv:1912.06015_, 2019.

* [16] Walter Rudin et al. _Principles of mathematical analysis_, volume 3. McGraw-hill New York, 1976.
* [17] Tom Sander, Pierre Stock, and Alexandre Sablayrolles. Tan without a burn: Scaling laws of dp-sgd. In _International Conference on Machine Learning_, pages 29937-29949. PMLR, 2023.
* [18] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2017.
* [19] Fei Wen, Lei Chu, Peilin Liu, and Robert C Qiu. A survey on nonconvex regularization-based sparse and low-rank recovery in signal processing, statistics, and machine learning. _IEEE Access_, 6:69883-69906, 2018.
* [20] Quanning Yao and James Kwok. Efficient learning with a family of nonconvex regularizers by redistributing nonconvexity. In _International Conference on Machine Learning_, pages 2645-2654. PMLR, 2016.
* [21] Yaodong Yu, Maziar Sanjabi, Yi Ma, Kamalika Chaudhuri, and Chuan Guo. Vip: A differentially private foundation model for computer vision. _arXiv preprint arXiv:2306.08842_, 2023.