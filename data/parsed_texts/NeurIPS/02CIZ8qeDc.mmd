# PointAD: Comprehending 3D Anomalies from Points and Pixels for Zero-shot 3D Anomaly Detection

Qihang Zhou\({}^{1}\), Jiangtao Yan\({}^{1}\), Shibo He\({}^{1}\)1, Wenchao Meng\({}^{1}\), Jiming Chen\({}^{1}\)

\({}^{1}\) College of Control Science and Engineering, Zhejiang University

\({}^{1}\){zqhang, jtaoy, s18he, wmengzju, cjm}@zju.edu.cn

Corresponding authors.

###### Abstract

Zero-shot (ZS) 3D anomaly detection is a crucial yet unexplored field that addresses scenarios where target 3D training samples are unavailable due to practical concerns like privacy protection. This paper introduces PointAD, a novel approach that transfers the strong generalization capabilities of CLIP for recognizing 3D anomalies on unseen objects. PointAD provides a unified framework to comprehend 3D anomalies from both points and pixels. In this framework, PointAD renders 3D anomalies into multiple 2D renderings and projects them back into 3D space. To capture the generic anomaly semantics into PointAD, we propose hybrid representation learning that optimizes the learnable text prompts from 3D and 2D through auxiliary point clouds. The collaboration optimization between point and pixel representations jointly facilitates our model to grasp underlying 3D anomaly patterns, contributing to detecting and segmenting anomalies of unseen diverse 3D objects. Through the alignment of 3D and 2D space, our model can directly integrate RGB information, further enhancing the understanding of 3D anomalies in a plug-and-play manner. Extensive experiments show the superiority of PointAD in ZS 3D anomaly detection across diverse unseen objects. Code is available at https://github.com/zqhang/PointAD

## 1 Introduction

Anomaly detection, a significant field within deep learning, has been widely applied to diverse domains, including industrial inspection [2, 3, 32, 36, 37, 44, 15, 24, 55, 6, 19, 63]. While 2D anomaly detection has been extensively studied by exploring RGB information [23, 56, 50, 51, 8, 29], real-world anomalies typically present themselves with abnormal spatial characteristics. Relying solely on RGB information poses challenges in detecting some anomalies in many cases, e.g., when the defect mimics the appearance of the object's background or foreground, as shown in Figure 1(a). The emerging field of 3D anomaly detection aims to unveil these spatial relations indicative of abnormal patterns [22, 4, 45, 9, 54, 13].

However, current 3D anomaly detection methods typically store normal point features during training and identify anomalies by measuring the distance between the test feature and these stored features [22, 54, 13]. They all depend on the assumption that target point clouds are available and entirely normal. This assumption does not hold in various situations when the training samples in the target dataset are inaccessible due to privacy protection (_e.g._, involvement of trade secrets) or the absence of target training data (_e.g._, a new product never seen before) [63]. Figure 1(b) depicts the setting discrepancy between ZS 3D and unsupervised anomaly detection. These methods mentioned above, which detect anomalies by memorizing or reconstructing normal point features, have limitations in generalizing to unseen objects in Figure 1(c). While zero-shot (ZS) anomaly detection has been explored in 2D images [35, 63], ZS 3D anomaly detection remains a research blank. It is a challenging task as ZS3D anomaly detection necessitates the model to detect 3D anomalies across unseen point clouds with diverse class semantics, requiring a robust generalization capacity in the detection model. Recently, Vision-Language Models (VLMs) with their strong generalization capabilities have been applied to various downstream tasks [40; 61; 49; 25; 26]. Particularly, CLIP has demonstrated its strong ZS performance to detect 2D anomalies [35; 24; 63]. Integrating CLIP into the detection model presents a potential solution to the challenging yet unexplored ZS 3D anomaly detection.

In this paper, we propose a unified framework, namely PointAD, to transfer the knowledge of CLIP to detect 3D anomalies in a ZS manner. PointAD comprehends point clouds from both 3D and 2D: **(1)** deriving 2D representations of point clouds via CLIP by rendering them from multiple views, **(2)** understanding 3D representations by projecting 2D representations back to 3D, and **(3)** enhancing 3D comprehension by additional regularization on 2D representations. After grasping point clouds from points and pixels, we propose hybrid representation learning to capture generic normality and abnormality w.r.t. point and pixel information into learnable text prompts [63]. Specifically, since 3D representation manifests its 2D renderings from different views, we treat each representation as one instance and achieve 3D representation aggregation via multiple instance learning (MIL). On this basis, PointAD explicitly aligns the 2D anomalies, rendered from 3D anomalies, to further enhance 3D understanding. We formulate these 2D anomaly recognition tasks from the multi-task learning (MTL) perspective. PointAD collaboratively learns point and pixel representations, promoting the in-depth understanding of underlying abnormal patterns and thus achieving superior ZS normality and abnormality point recognition. Furthermore, benefiting from collaboration optimization, PointAD can directly integrate additional RGB information and perform ZS multimodal 3D (M3D) detection without extra modules and retraining. The main contributions of this paper are summarized as follows:

* To the best of our knowledge, we are the first to investigate the challenging yet valuable ZS 3D anomaly detection domain. We propose to transfer the strong recognition generalization of CLIP to detect and segment 3D anomalies over diverse objects.
* We introduce a novel ZS 3D anomaly detection approach called PointAD, which provides a unified framework to understand 3D anomalies from points and pixels. Hybrid representation learning is proposed to incorporate the generic normality and abnormality semantics into PointAD, enabling a thorough understanding of 3D anomalies.
* PointAD can incorporate 2D RGB information in a plug-and-play manner for testing. In contrast to other methods that require storing RGB information separately, PointAD offers a unified framework to perform ZS M3D anomaly detection directly.
* Extensive experiments are conducted to demonstrate the superiority of our model in detecting and segmenting 3D anomalies, even outperforming some unsupervised SOTA methods that memorize normal features of target objects in certain metrics. We hope that our model will serve as a springboard for future research on ZS 3D and M3D anomaly detection.

## 2 Related Work

3D Anomaly DetectionMVTec3D-AD [4], Eyecandies [5], and Real3D-AD [31] provide the point cloud anomalies and the corresponding 2D-RGB information. MVTec3D-AD bridges the connection between 3D and 2D anomaly detection. 3D-ST [4] uses a teacher net to extract dense local geometric

Figure 1: Motivation of zero-shot 3D anomaly detection. **(a)**: **Top:** The hole on the cookies presents a similar appearance to the background. **Bottom:** Surface damage on the potato is unapparent to the object foreground. In these cases, leveraging RGB information makes it difficult to detect anomalies that imitate the color patterns of the background or foreground. However, effective recognition can be achieved by modeling the point relations within corresponding point clouds. (b) and (c) depicts the setting difference of ZS and unsupervised manner.

descriptors and design a student net to match such descriptors. AST [45] introduces asymmetric teacher and student net to further improve 3D anomaly detection. IMRNet [28] and 3DSR [58] detect 3D anomalies by reconstruction errors. Instead of only using point clouds, BTF [22], M3DM [54; 53], CPFM [7], and SDM [13] integrate point features and RGB pixel features to detect 3D anomalies. While these approaches exhibit commendable performance by storing object-specific normal point and pixel features within the unsupervised learning framework, such paradigms simultaneously limit their generalization capacity to point clouds from unseen objects, which is crucial to detecting anomalies when the target object is unavailable. To the best of our knowledge, no solution addresses this valuable yet challenging problem. To fill this gap, we introduce PointAD, designed to identify unseen anomalies across diverse objects. PointAD extends CLIP to the realm of ZS 3D anomaly detection and shows robust generalization in capturing generic normality and abnormality within point clouds. Furthermore, PointAD serves as a unified framework, allowing seamless integration of point cloud and RGB modality without additional training.

3D Feature ExtractionConventional methods of 3D feature extraction typically employ a point-based network like PointNet [38] or PointNet++[39] to extract 3D features from point clouds. Alternative approaches convert 3D data into a 2D format [48; 18], enabling 2D image backbones to process 3D information. PointCLIP [59] directly projects raw points onto image planes for efficiency, but this approach causes the produced depth map to lack geometric details. Instead, rendering-based methods [48; 21] generate 2D renderings by rendering point clouds, allowing for better preservation of local semantics. CPFM [7] stores normal features of these 2D renderings for unsupervised 3D anomaly detection. In this paper, we apply this rendering strategy to the source samples to capture generic anomaly semantics for recognizing abnormalities in unseen objects.

Prompt LearningInstead of fine-tuning the whole network, prompt learning just optimizes the model to adapt the network to downstream tasks. CoOp [61; 60] introduces global context optimization to update learnable text prompts for few-shot recognition. DenseCLIP [41] extends it to the dense classifications. More recently, AnomalyCLIP [63] proposes object-agnostic prompt learning to capture the generic normality and abnormality for images. Our model first introduces hybrid representation learning for ZS 3D anomaly detection, enabling the detection of anomalies and abnormal regions.

## 3 PointAD

### A Review of CLIP

CLIP, a representative VLM, aligns visual representations to the corresponding textual representations, where an image is classified by comparing the cosine similarity between its visual representation and textual representations of given class-specific text prompts. Specifically, given an image \(x_{i}\) and target class set \(\mathcal{C}\), visual encoders output the global visual representation \(f_{i}\in\mathbb{R}^{d}\) and local visual representations \(f_{i}^{m}\in\mathbb{R}^{h\times w\times d}\), where \(h\), \(w\), and \(d\) are the height, width, and dimension, respectively. Textual representations \(g_{c}\) are encoded by textual encoder \(\mathcal{T}\) with the commonly used text prompt template A photo of a [c], where \(c\in\mathcal{C}\). The probability of \(x_{i}\) belongs to \(c\) can be computed as:

\[P(g_{c},f_{i})=\frac{exp(cos(g_{c},f_{i})/\tau)}{\sum_{c\in\mathcal{C}}exp(cos (g_{c},f_{i}))/\tau)},\] (1)

where \(cos(\cdot,\cdot)\) and \(\tau\) represent the cosine similarity and temperature used in CLIP, respectively. The segmentation \(S_{i(c)}\in\mathbb{R}^{h\times w}\) for class \(c\) can be computed as \(Seg(g_{c},f_{i}^{m})\), where each entry (\(u\),\(v\)) is calculated as \(P(g_{c},f_{i,u,v}^{m})\).

### Overview of PointAD

ZS 3D anomaly detection requires a strong generalization capacity to anomalies on unseen objects with diverse object semantics. In this paper, we propose a unified framework, namely PointAD, to detect and segment 3D anomalies in a ZS manner. In Figure 2, PointAD understands point clouds from both pixel and point perspectives. To make CLIP understand 3D point clouds, we first render point clouds from multiple views and extract the pixel representations of these generated 2D renderings via the visual encoder of CLIP. And then, we derive point representations by projecting these pixel representations back to 3D. Learning generic normality and abnormality is significant in recognizing across-object anomalies. We propose hybrid representative learning, which focuses on global point and pixel abnormality, to optimize normality and abnormality text prompts, enabling PointAD with strong generalization to identify 3D anomalies on diverse objects. Benefiting from the hybrid representation learning, PointAD can directly incorporate 2D RGB information during testing to achieve ZS M3D detection.

### Multi-View Rendering

Multi-view projection is a crucial technology for understanding point clouds from 2D perspectives. Some multi-view projection approaches project point clouds into various depth maps, providing adequate shape information for class recognition [59]. However, in this paper, our objective is to learn both generic global and local anomaly semantics. Depth-map projection lacks sufficient resolution to represent fine-grained anomaly semantics accurately. Hence, we adopt high-precision rendering to preserve the original 3D information offline. Specifically, given an auxiliary dataset of point clouds \(\mathcal{D}_{3d}=\{(x_{i}^{3d},y_{i}^{3d})\}_{i=1}^{N}\), we define the rendering matrix as \(R^{(k)}\) for the \(k\)-\(th\) view, with a total of \(K\) views. We simultaneously render point clouds and point-level ground truths from different views to obtain their corresponding 2D renderings, which is given by \(x_{i}^{(k)}=R^{(k)}(x_{i}^{3d})\) and \(y_{i}^{(k)}=R^{(k)}(y_{i}^{3d})\), where \(x_{i}^{(k)}\in\mathbb{R}^{H\times W}\) and \(y_{i}^{(k)}\in\mathbb{R}^{H\times W}\) respectively represent the \(k\)-\(th\) 2D renderings and corresponding pixel-level ground truth in the \(i\)-\(th\) point cloud. Note that anomaly pixels are marked as 1, and normal pixels are marked as 0.

### Representations for 3D and 2D information

PointAD aims to learn generic anomaly semantics from both 3D and 2D representations, enabling a comprehensive understanding of point and pixel anomaly patterns. For a point cloud \(x_{i}^{3d}\), we first obtain the 2D renderings \(\mathcal{X}_{i}=\{x_{i}^{(k)}\}_{k=1}^{K}\). Then, these renderings are encoded via the vision encoder of CLIP to obtain global 2D representations \(\mathcal{F}_{i}{=}\{f_{i}^{(k)}\}_{k=1}^{K}\), and local 2D representations \(\mathcal{F}_{i}^{m}=\{f_{i}^{m(k)}\}_{k=1}^{K}\). As for point cloud representations, we consider that one point cloud will be projected into multiple 2D renderings. Consequently, global 3D representation \(p_{i}\) and local 3D representations \(p_{i}^{m}\) are expected to include their corresponding 2D representations in each view. Formally, \(p_{i}=\{p_{i}^{(k)}|p_{i}^{(k)}=f_{i}^{(k)}\}_{k=1}^{K}\) and \(p_{i}^{m}=\{p_{i}^{m(k)}|p_{i}^{m(k)}=\{p_{i,j}^{m(k)}\}_{j=1}^{n}\}_{k=1}^{K}\), where \(p_{i,j}^{m(k)}=\{p_{i,j}^{m(k)}=f_{i,u,v}^{m(k)},(u,v)=R^{(k)}(a_{i,j}|b_{i,j}, c_{i,j})\}\) represents the \(j\)-\(th\) point representation of \(i\)-\(th\) point cloud in the \(k\)-\(th\) view, whose 3D coordinate is (\(a_{i,j}\), \(b_{i,j}\), \(c_{i,j}\)). \(R^{\prime(k)}\) is the rendering transformation between the point and pixel representation, derived as \(R^{\prime(k)}=\frac{h}{H}R^{(k)}\).

Figure 2: Framework of PointAD. To transfer the strong generalization of CLIP from 2D to 3D, point clouds and corresponding ground truths are respectively rendered into 2D renderings from multi-view. Then, vision encoder of CLIP extracts the renderings to derive 2D global and local representations. These representations are transformed into global 3D point representations to learn 3D anomaly semantics within point clouds. Finally, we align the normality and abnormality from both point perspectives (multiple instance learning) and pixel perspectives (multiple task learning) and propose a hybrid loss to jointly optimize the text embeddings from the learnable normality and abnormality text prompts, capturing the underlying generic anomaly patterns.

Points at different positions may yield a different number of 2D representations as they are hidden by other points from a specific viewpoint. In this case, we introduce a view-wise visibility mask \(M\), where \(M^{k}_{i,j}\) indicates whether the \(j\)-\(th\) point of the \(i\)-\(th\) point cloud is visible in the \(k\)-\(th\) view. We compare the depth of points projected into the same pixel in the same view and set the corresponding visibility mask to 1 for the point with the minimum depth, and to 0 for the other points. Let \(\mathcal{Q}^{(k)}_{i,u,v}\) denote the depths set of all points that are projected into the same pixel indexed by \((u,v)\) in the \(i\)-\(th\) point cloud in the \(k\)-\(th\) view. \(\mathcal{Q}^{(k)}_{i,u,v}\) and \(M^{k}_{i,j}\) are respectively given as \(\mathcal{Q}^{(k)}_{i,u,v}=\{c_{i,j}\mid R^{\prime(k)}(a_{i,j},b_{i,j},c_{i,j})= (u,v)\}_{j=1}^{n}\) and \(M^{(k)}_{i,j}=\mathbb{I}(i,j,k=\operatorname{argmin}_{i,j,k}\{c_{i,j}\mid c_{i,j}\in\mathcal{Q}^{(k)}_{i,u,v}\})\), where \(\mathbb{I}(\cdot)\) is an indicator function. Local 3D representations of the \(i\)-\(th\) point cloud for the \(k\)-\(th\) view are reformulated as: \(p^{m(k)}_{i}=\{p^{m(k)}_{i,j}*M^{(k)}_{i,j}\}_{j=1}^{n}\).

### Hybrid representation learning

The key of ZS 3D anomaly detection requires the model to capture generic anomaly semantics, rather than relying on specific object semantics. Since CLIP was originally pre-trained to align object semantics, such alignment harms the generalization capacity of CLIP to recognize anomalies on various objects. To adapt CLIP to 3D anomaly detection, we propose a hybrid representation learning, from both 3D and 2D perspectives, to globally and locally optimize textual representations. This enables PointAD to learn more representative text embedding for global anomaly semantics alignment. Following previous work [63; 61], we randomly initialize two learnable text templates \(t_{n}\) and \(t_{a}\), in AnomalyCLIP [63] or CoOp manner [61], to obtain more overall text embeddings \(g_{n}\) and \(g_{a}\) to recognize normality and abnormality, respectively.

\[\begin{array}{ll}t_{n}=[V_{1}]\ldots[V_{E}][object],&t_{n}=[V_{1}]\ldots[V_ {E}][class],\\ \underbrace{t_{a}=[W_{1}]\ldots[W_{E}][damaged][object]}_{\text{PointAD}},&t_{a }=[W_{1}]\ldots[W_{E}][damaged][class],\end{array}\]

where \(V\) and \(W\) are learnable word embeddings, respectively.

MIL-based 3D representation learningTo fully incorporate 3D glocal anomaly semantics into PointAD from point information, we respectively devised two losses to capture 3D global anomalies and local anomaly regions. First, we compute the cosine similarity between the textual representation and its rendering global representations in each view. As point clouds are projected from different views, the resulting renderings in each view reflect certain parts of point clouds. We use view-wise MIL to integrate 2D global representations and then align global labels to capture the global semantics. Formally, the global 3D loss is defined as:

\[L^{global}_{3d}=\tfrac{1}{N}\sum_{i}\text{CrossEntropy}(\tfrac{1}{K}\sum_{f^{ \prime}_{i}\in_{p_{i}}}P(g_{c},f^{(k)}_{i}),\max{(y^{3d}_{i})}).\]

As for local point anomaly semantics, we quantify the cosine similarity between textual representations and local representations of 2D renderings. Since points within point clouds are projected from different views, their projections in each view present part characteristics of themselves. We adopt the pixel-wise MIL to achieve the aggregation of point local representation. The point segmentation can be formulated mathematically as follows:

\[S^{3d}_{i(a)}=\tfrac{1}{K}\sum_{k}Seg(g_{a},p^{m(k)}_{i}),S^{3d}_{i(n)}=\tfrac {1}{K}\sum_{k}Seg(g_{n},p^{m(k)}_{i}).\]

However, deriving such 3D segmentation requires similarity computation for each point. It brings a significant memory burden, with a huge computational complexity of \(O(Knd)\), which is unaffordable for one NVIDIA RTX 3090 24GB GPU. To address this computational challenge, we resort to the rendering correspondence between points (3D space) and their corresponding pixels within each view (2D space). We first can rewrite 3D segmentation from the view perspective as \(S^{3d}_{i(a)}=\tfrac{1}{K}\sum_{k}S^{3d(k)}_{i(a)}\). Then, the \(k\)-\(th\) division of 3D segmentation can be transformed into the 2D counterpart through the rendering projection \(S^{3d(k)}_{i(a)}=(R^{(k)})^{(-1)}S^{2d(k)}_{i(a)}\otimes M^{(k)}_{i}\), where \(\otimes\) is the Hamiltonian product. The \(k\)-\(th\) 2D counterpart can be computed as \(S^{2d(k)}_{i(a)}=\text{Up}(Seg(g_{a},f^{m(k)}_{i}))\), where the operator \(\text{Up}(\cdot)\) represents bilinear interpolation from feature space to 2D space. Finally, we can reformulate the 3D segmentation as follows:

\[S^{3d}_{i(a)}=\tfrac{1}{K}\sum_{k}\Bigl{(}(R^{(k)})^{(-1)}\text{Up}(Seg(g_{a},f^{m(k)}_{i}))\otimes M^{(k)}_{i}\Bigr{)}.\] (2)

From the equation, we can observe that the primary computation can be conducted in the feature space, with a computational complexity of \(O(Khwd)\). This is a substantial overhead reduction compared to \(O(Knd)\) since feature space is much smaller than 3D space, _i.e._, \(h\times w\ll n\). In our experiment, \(h\times w=24\times 24=576\), while \(n=336\times 336=112896\). With this transformation, the entire experiment can be conducted using only a single NVIDIA RTX 3090 24GB GPU. After that, Dice Loss is employed to precisely model the decision boundary of anomaly regions. Let \(I\) represent a full-one matrix of the same size as \(y_{i}^{3d}\). Formally, we define 3D local loss \(L_{3d}^{local}:\)

\[L_{3d}^{local}=\tfrac{1}{N}\sum_{i}\Big{(}\text{Dice}(S_{i(n)}^{3d},I-y_{i}^{3d })+\text{Dice}(S_{i(a)}^{3d},y_{i}^{3d})\Big{)}.\]

MTL-based 2D representation learningWe further improve PointAD point understanding by capturing 2D glocal anomaly semantics into the object-agnostic text prompt template. We treat the anomaly recognition for one rendering from the point cloud as a task. Hence, we formulate the anomaly semantics learning for multiple 2D renderings as MTL. MTL-based 2D representation learning is divided into two parts for respective alignment to 2D global and local anomaly semantics. For 2D global semantics, we use CrossEntropy to quantify the discrepancy between the textual representations and each global 2D representation. Global MTL-based 2D representation learning \(L_{2d}^{global}\) is defined as:

\[L_{2d}^{global}=\tfrac{1}{NK}\sum_{i,k}\text{CrossEntropy}(P(g_{c},f_{i}^{(k)} ),\max{(y_{i}^{(k)})}).\]

Also, we focus on 2D abnormal regions to understand pixel-level anomalies. As the anomaly regions are typically smaller than normal regions, we employ Focal Loss to mitigate the class imbalance besides Dice Loss. Let \(\oplus\) denote the concatenation operation. Local MTL-based 2D representation learning\(L_{2d}^{local}\) is given as follows:

\[L_{2d}^{local}=\tfrac{1}{NK}\sum_{i,k}\text{Focal}(S_{i(n)}^{2d(k)}\oplus S_{ i(a)}^{2d(k)},y_{i}^{(k)})+\text{Dice}(S_{i(n)}^{2d(k)},I-y_{i}^{(k)})+\text{Dice}(S_{i(a)}^{ 2d(k)},y_{i}^{(k)}).\]

### Training and Inference

PointAD detects 3D anomalies from both 3D and 2D perspectives and thus combing these above losses to derive hybrid loss \(L_{hybrid}\). We minimize \(L_{hybrid}\) to incorporate generic anomaly semantics into the text prompt from point and pixel spaces:

\[L_{hybrid}=L_{3d}^{global}+L_{3d}^{local}+L_{2d}^{global}+L_{2d}^{local}.\]

During training, we minimize the hybrid loss \(L_{hybrid}\), where the original parameters of CLIP are frozen to maintain its strong generalization. Since our model provides a unified framework to understand anomaly semantics from point and pixel, it can not only perform **ZS 3D anomaly detection** but also **M3D anomaly detection in a plug-and-play way**. Next, we will introduce the inference process in detail:

ZS 3D/M3D inferenceGiven a point cloud \(x_{i}^{3d}\), we regard the 3D segmentation (See Equ. 2) as the anomaly score map: \(A_{i}^{m}\)=\(G_{\sigma}(S_{i(a)}^{3d})\), where \(G_{\sigma}(\cdot)\) represents the Gaussian filter. The global anomaly score incorporates glocal anomaly semantics and is computed as \(A_{i}^{s}=\frac{1}{2}(\frac{1}{K}\sum_{f_{i}^{(k)}\in\mathcal{F}_{i}}P(g_{c}, f_{i}^{(k)})+\max{(A_{i}^{m})})\). When the RGB counterpart is available for testing, PointAD could directly integrate RGB information by feeding RGB images to 2D branch to derive 2D representations. We project these 2D representations back to 3D branch to respectively compute RGB anomaly score map and anomaly score as \(A_{i}^{m(rgb)}=P(g_{c},f_{i}^{(rgb)})\) and \(A_{i}^{s(rgb)}=G_{\sigma}(S_{i(a)}^{3d(rgb)})\). The final multimodal anomaly score map and anomaly score are defined as \(A_{i}^{m(mod)}=\frac{1}{2}G_{\sigma}\big{(}A_{i}^{m}+A_{i}^{m(rgb)}\big{)}\) and \(A_{i}^{s(mod)}=\frac{1}{2}\left[\frac{1}{2}(A_{i}^{s(rgb)}+A_{i}^{s})+\max{( A_{i}^{m(mod)})}\right]\), respectively.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Detec. & Dataset & MVTC-3D-AD(10) & Eyecaudes(0) & RealD-AD(12) \\ \cline{2-6} level & Mice & \(L_{3d}\) & AUC & AP & AUROC & AP \\ \hline CLIP+R & 61.2 & 85.8 & 66.7 & 69.2 & 68.8 & 72.3 \\  & Cheraghian & 53.6 & 81.7 & 49.5 & 48.1 & 50.3 & 54.4 \\  & PointCLIP V2 & 51.2 & 80.1 & 46.1 & 48.1 & 53.1 & 58.1 \\ G. & PointCLIP V2 & 51.1 & 80.6 & 44.4 & 47.0 & 57.5 & 58.3 \\  & AnomalyCLIP & 56.4 & 33.5 & 57.6 & 59.0 & 55.2 & 71.1 \\  & PointADCOop & **80.9** & **93.9** & 67.8 & **72.8** & **72.9** \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c c c} \hline \hline Metric & P-ARROC & ATP & P-ARROC & ATP \\ \hline CLIP+R & - & 54.4 & 82.1 & - & 39.5 & - \\  & Cheraghian & 88.2 & 57.0 & - & - \\  & PointCLIP V2 & 87.4 & 52.3 & 43.7 & - & 52.9 & - \\ L. & PointCLIP V2 & 87.3 & 52.3 & 44.2 & - & 52.2 & - \\  & AnomalyCLIP & 88.9 & 60.9 & 77.7 & 43.4 & 50.3 & - \\  & PointADCOop & **80.5** & **82.0** & **91.5** & **71.3** & **72.6** \\  & PointAD & **90.5** & **84.4** & **92.1** & **71.3** & **72.8** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparison on ZS 3D anomaly detection in “one-vs-rest” setting.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Detec. & Dataset & MVTC-3D-AD(10) & Eyecaudes(10) \\ \cline{2-5} level & Mice & \(L_{3d}\) & AUC & AP & AUROC & AP \\ \hline CLIP+R & 61.2 & 85.8 & 66.7 & 69.2 & 68.8 & 72.3 \\  & Cheraghian & 53.6 & 81.7 & 49.5 & 48.1 & 50.3 & 54.4 \\  & PointCLIP V2 & 51.2 & 80.1 & 46.1 & 48.1 & 53.1 & 58.1 \\ G. & PointCLIP V2 & 51.1 & 80.6 & 44.4 & 47.0 & 57.5 & 58.3 \\  & AnomalyCLIP & 56.4 & 33.5 & 57.6 & 59.0 & 55.2 & 71.1 \\  & PointADCOop & **80.9** & **93.9** & 67.8 & **72.8** & **72.9** \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c c c} \hline \hline Metric & P-ARROC & ATPRO & P-ARROC & ATPRO \\ \hline CLIP+R & - & 54.4 & 82.1 & - & 39.5 & - \\  & Cheraghian & 88.2 & 57.0 & - & - \\  & PointCLIP V2 & 87.4 & 52.3 & 43.7 & - & 52.9 & - \\  & PointCLIP V2\({}_{2d}\) & 87.3 & 52.3 & 44.2 & - & 52.2 & - \\  & AnomalyCLIP & 88.9 & 60.9 & 77.7 & 43.4 & 50.3 & - \\  & PointADCOop & **80.5** & **82.0** & **91.5** & **71.3** & **72.6** \\  & PointAD & **90.5** & **84.4** & **92.1** & **71.3** & **72.8** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison on ZS M3D anomaly detection in “one-vs-rest” setting.

Experiment

### Experiment Setup

DatasetWe evaluate the performance of ZS 3D anomaly detection on three public datasets including, MVTec3D-AD, Eyecandies and Real3D-AD. MVTec3D-AD, Eyecandies, and Real3D-AD are multi-class datasets and respectively contain 10 classes, 10 classes, and 12 classes. Since these training datasets only contain all normal samples, **we use the common zero-shot setting one-vs-rest, where an object test dataset is used to fine-tune PointAD and assess the ZS anomaly detection for the remaining objects.** We also explore a more challenging setting: **cross-dataset ZS generalization, which requires the detection model to generalize to anomalies on other datasets.** For point cloud anomaly detection, we only use point clouds to detect and localize 3D anomalies. In M3D anomaly detection, the 2D RGB information is utilized only for testing. To comprehensively analyze PointAD, we utilize four metrics to assess its performance in both anomaly detection and segmentation.

### Implementation Details & Baselines

Both point clouds and 2D renderings are resized to 336 \(\times\) 336. We use Open3d library to generate 9 views by rotating point clouds along the X-axis at angles of \(\{-\frac{4}{5}\pi,-\frac{3}{5}\pi,-\frac{2}{5}\pi,-\frac{1}{5}\pi,0,\frac{1}{5 }\pi,\frac{5}{5}\pi,\frac{3}{5}\pi,\frac{4}{5}\pi\}\) for most categories. We circularly set the rendering angles, evenly distributing the angles between \(-\pi\) to \(\pi\). The backbone of PointAD is the pre-trained CLIP model (VIT-L/140336px in open_clip). Following [63], we improve the local visual semantics of vision encoder of CLIP without modifying its parameters. During training, we keep all parameters of CLIP frozen and set the learnable word embeddings in object-agnostic text templates to \(12\). All experiments were conducted on a single NVIDIA RTX 3090 24GB GPU using PyTorch-2.0.0. As there is no work to explore the field of ZS 3D anomaly detection, we make a great effort to provide these comparisons. We apply the original CLIP to our framework for 3D detection, called CLIP + Rendering. Also, we reproduce SOTA 3D recognition works including PointCLIP V2 [64] and Cheraghian [12], and adapt them for ZS 3D anomaly detection. We compare the SOTA 2D anomaly detection approach AnomalyCLIP [63] by fine-tuning it on depth maps. PointAD by default uses object-agnostic text prompts, whereas PointAD-CoOp employs object-aware prompts. Appendix B and C provide more details on implementation and baselines.

### Main Results

We fine-tuned PointAD on three objects on MVTec3D-AD, Eyecandies, and Real3D-AD. Over three runs, the averaged results on **one-vs-rest** and **cross-dataset** settings are reported. We use the metric pairs (I-AUROC% \(\%\uparrow\) and AP% \(\%\uparrow\)) and (P-AUROC% \(\%\uparrow\) and AUPRO% \(\%\uparrow\)) to evaluate the global detection performance, respectively. Details of experimental settings see Appendix A. The best and second-best results in ZS are highlighted in Red and Blue. G. and L. represent 3D global and local anomaly detection. M3D global and local anomaly detection are abbreviated as MG. and ML.

ZS 3D anomaly detectionTable 1 presents the comparison of ZS 3D performance. Compared to the point-based method Cheraghian and the projection-based method PointCLIP V2, PointAD achieves superior performance on ZS 3D anomaly detection over all three datasets. Especially, it outperforms CLIP + Rendering from 61.2% to 82.0% I-AUROC and from 85.8% to 94.2% AP on MVTec3D-AD. In addition, PointAD achieves superior segmentation performance on ZS 3D anomaly detection, improving MVTec3D-AD by a large margin compared to Cheraghian from 88.2%

Figure 3: Visualization on anomaly score maps in ZS 3D anomaly detection. Point clouds of diverse objects are input into PointAD to generate 2D and 3D representations. Each row visualizes the anomaly score maps of 2D renderings from different views, and the final point score maps are also presented. More visualizations are provided in Appendix J.

to 95.5% P-AUROC and from 57.0% to 84.4% AUPRO. This improvement in overall performance is attributed to PointAD adapting CLIP's strong generalization to glocal anomaly semantics through hybrid representation learning. In addition, PointAD advances PointAD-CoOp across all datasets by blocking the class semantics in text prompts [63].

ZS M3D anomaly detectionWe also compare the ZS M3D anomaly detection when RGB information is available for testing. As shown in Table 2, the results indicate that PointAD can integrate additional RGB information and further boost its performance from 82.0% to 86.9% AUROC and from 94.2% to 96.1% AP for global semantics on MVTec3D-AD. Additionally, as for local semantics, the performance improves from 95.5% to 97.2% P-AUROC and from 84.4% to 90.2% AUPRO. A large performance gain is also obtained on Eyecandies and Real3D-AD. While other methods improve their performance in some metrics, they still suffer from performance degradation in other metrics due to inefficient integration of the two modalities. Instead, PointAD achieves overall improvement across all metrics by incorporating explicit joint constraints on both point and pixel information.

Cross-dataset ZS anomaly detectionWe perform the cross-dataset anomaly recognition to further evaluate the zero-shot capacity of PointAD, where we use one object as the auxiliary and test objects with totally different semantics and scenes in another dataset. We compare all baselines that need fine-tuning. From Table 3 and M3D from Table 4, PointAD demonstrates strong cross-dataset generalization performance on Eyeecandies and Real3D-AD, with nearly no obvious performance decay compared to the one-vs-rest setting. The strong transfer ability highlights its robust generalization capabilities in detecting anomalies in objects with unseen semantics and backgrounds.

### Result Analysis

Visualization analysis.To intuitively present the strong generalization capacity of our model to unseen anomalies, we visualize the anomaly score maps of the 3D and corresponding 2D counterparts of PointAD on MVTec3D-AD. As shown in Figure 3, PointAD reveals abnormal spatial relationships of points and further captures the generic point anomaly patterns across diverse objects. And, we also visualize the anomaly score map of corresponding 2D counterparts, where 3D point anomalies are transformed into 2D pixel anomalies. It can be observed that PointAD also has a strong detection ability on such 2D anomalies. The strong representative pixel representations from multiple views facilitate more precise 3D anomaly detection. Quantitative results are provided in Section 5. The strong 3D and 2D detection capabilities of PointAD are from hybrid representation learning, which not only enables PointAD to capture the 3D anomalies but also explicitly constrains 2D representations.

How multimodality makes PointAD accurate.PointAD is a unified framework that can not only capture point anomalies but also handle 2D information in a plug-and-play manner. As shown in Figure 4(a), we visualize M3D results of PointAD on MVTec3D-AD. The surface damage on the potato presents a similar appearance to the object foreground, which makes it difficult to detect this anomaly with RGB information. On the contrary, the point relations for the color stain on foam are the same as those of normal, but they have a clear distinction in the RGB information. PointAD

Figure 4: Visualization comparison between PointAD with hybrid loss and without.

[MISSING_PAGE_FAIL:9]

#### Acknowledgments

This work was supported by NSFC U23A20326 and NSFC 62088101 Autonomous Intelligent Unmanned Systems.

## References

* [1]T. Aota, L. T. Tong, and T. Okatani (2023) Zero-shot versus many-shot: unsupervised texture anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 5564-5572. Cited by: SS1.
* [2]P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger (2019) Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9592-9600. Cited by: SS1.
* [3]P. Bergmann and D. Sattlegger (2023) Anomaly detection in 3d point clouds using deep geometric descriptors. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2613-2623. Cited by: SS1.
* [4]P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger (2020) Uninformed students: student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4183-4192. Cited by: SS1.
* [5]L. Bonfiglioli, M. Toschi, D. Silvestri, N. Fioraio, and D. De Gregorio (2022) The eyecandies dataset for unsupervised multimodal anomaly detection and localization. In Proceedings of the Asian Conference on Computer Vision (ACCV), pp. 3586-3602. Cited by: SS1.
* [6]T. Cao, J. Zhu, and G. Pang (2023) Anomaly detection under distribution shift. arXiv preprint arXiv:2303.13845. Cited by: SS1.
* [7]Y. Cao, X. Xu, and W. Shen (2023) Complementary pseudo multimodal feature for point cloud anomaly detection. arXiv preprint arXiv:2303.13194. Cited by: SS1.
* [8]Y. Cao, X. Xu, J. Zhang, Y. Cheng, X. Huang, G. Pang, and W. Shen (2024) A survey on visual anomaly detection: challenge, approach, and prospect. ArXivabs/2401.16402. Cited by: SS1.
* [9]R. Chen, G. Xie, J. Liu, J. Wang, Z. Luo, J. Wang, and F. Zheng (2023) Easynet: an easy network for 3d industrial anomaly detection. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 7038-7046. Cited by: SS1.
* [10]X. Chen, Y. Han, and J. Zhang (2023) A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&2: 1st place on zero-shot ad and 4th place on few-shot ad. arXiv preprint arXiv:2305.17382. Cited by: SS1.
* [11]Y. Chen, Y. Tian, G. Pang, and G. Carneiro (2022) Deep one-class classification via interpolated gaussian descriptor. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36, pp. 383-392. Cited by: SS1.
* [12]A. Cheraghian, S. Rahman, T. Chowdhury, D. Campbell, and L. Petersson (2022) Zero-shot learning on 3d point cloud objects and beyond. International Journal of Computer Vision130 (10), pp. 2364-2384. Cited by: SS1.
* [13]Y. Chu, C. Liu, T. Hsieh, H. Chen, and T. Liu (2023) Shape-guided dual-memory learning for 3d anomaly detection. In International Conference on Machine Learning, pp. 6185-6194. Cited by: SS1.
* [14]N. Cohen and Y. Hoshen (2020) Sub-image anomaly detection with deep pyramid correspondences. arXiv preprint arXiv:2005.02357. Cited by: SS1.
* [15]H. Deng and X. Li (2022) Anomaly detection via reverse distillation from one-class embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9737-9746. Cited by: SS1.

[MISSING_PAGE_POST]

* [16] Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, and Xingyu Li. Anovel: Adapting vision-language models for unified zero-shot anomaly localization. _arXiv preprint arXiv:2308.15939_, 2023.
* [17] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pre-trained model clip. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, pages 6568-6576, 2022.
* [18] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with a simple and effective baseline. In _International Conference on Machine Learning_, pages 3809-3820. PMLR, 2021.
* [19] Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. Anomallyopt: Detecting industrial anomalies using large vision-language models, 2023.
* [20] Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 98-107, 2022.
* [21] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view transformation network for 3d shape recognition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1-11, 2021.
* [22] Eliahu Horwitz and Yedid Hoshen. Back to the feature: classical 3d features are (almost) all you need for 3d anomaly detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2967-2976, 2023.
* [23] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang. Registration based few-shot anomaly detection. In _European Conference on Computer Vision_, pages 303-319. Springer, 2022.
* [24] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. Winclip: Zero-/few-shot anomaly classification and segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19606-19616, 2023.
* [25] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19113-19122, 2023.
* [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
* [27] Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, and Stephan Mandt. Zero-shot anomaly detection via batch normalization. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [28] Wenqiao Li, Xiaohao Xu, Yao Gu, Bozhong Zheng, Shenghua Gao, and Yingna Wu. Towards scalable 3d anomaly detection and localization: A benchmark via 3d anomaly synthesis and a self-supervised learning network. _2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 22207-22216, 2023.
* [29] Xurui Li, Ziming Huang, Feng Xue, and Yu Zhou. Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. _ArXiv_, abs/2401.16753, 2024.
* [30] Yiting Li, Adam David Goodge, Fayao Liu, and Chuan-Sheng Foo. Promptad: Zero-shot anomaly detection using text prompts. _2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 1082-1091, 2024.
* [31] Jiaqi Liu, Guoyang Xie, ruitao chen, Xinpeng Li, Jinbao Wang, Yong Liu, Chengjie Wang, and Feng Zheng. Real3d-AD: A dataset of point cloud anomaly detection. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.

* [32] Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus-Robert Muller. Explainable deep one-class classification. _arXiv preprint arXiv:2007.01760_, 2020.
* [33] Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus-Robert Muller, and Marius Kloft. Exposing outlier exposure: What can be learned from few, one, and zero outlier images. _arXiv preprint arXiv:2205.11474_, 2022.
* [34] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. _arXiv preprint arXiv:1301.3781_, 2013.
* [35] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-of-distribution detection with vision-language representations. _Advances in Neural Information Processing Systems_, 35:35087-35102, 2022.
* [36] Guansong Pang, Choubo Ding, Chunhua Shen, and Anton van den Hengel. Explainable deep few-shot anomaly detection with deviation networks. _arXiv preprint arXiv:2108.00462_, 2021.
* [37] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. _ACM computing surveys (CSUR)_, 54(2):1-38, 2021.
* [38] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 652-660, 2017.
* [39] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Advances in neural information processing systems_, 30, 2017.
* [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [41] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18082-18091, 2022.
* [42] Tal Reiss, Niv Cohen, Liron Bergman, and Yedid Hoshen. Panda: Adapting pretrained features for anomaly detection and segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2806-2814, 2021.
* [43] Tal Reiss and Yedid Hoshen. Mean-shifted contrastive loss for anomaly detection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 2155-2162, 2023.
* [44] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Scholkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14318-14328, 2022.
* [45] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Asymmetric student-teacher networks for industrial anomaly detection. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 2592-2602, 2023.
* [46] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In _International conference on machine learning_, pages 4393-4402. PMLR, 2018.
* [47] Bernhard Scholkopf, Robert C Williamson, Alexander J Smola, John Shawe-Taylor, John C Platt, et al. Support vector method for novelty detection. In _NIPS_, volume 12, pages 582-588. Citeseer, 1999.

* [48] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In _Proceedings of the IEEE international conference on computer vision_, pages 945-953, 2015.
* [49] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition with limited annotations. _Advances in Neural Information Processing Systems_, 35:30569-30582, 2022.
* [50] Yiyou Sun, Zhenmei Shi, Yingyu Liang, and Yixuan Li. When and how does known class help discover unknown ones? provable understandings through spectral analysis. In _International Conference on Machine Learning_, 2023.
* [51] Yu Tian, Fengbei Liu, Guansong Pang, Yuanhong Chen, Yuyuan Liu, Johan W Verjans, Rajvinder Singh, and Gustavo Carneiro. Self-supervised pseudo multi-class pre-training for unsupervised anomaly detection and segmentation in medical images. _Medical Image Analysis_, page 102930, 2023.
* [52] Yu Tian, Guansong Pang, Fengbei Liu, Yuanhong Chen, Seon Ho Shin, Johan W Verjans, Rajvinder Singh, and Gustavo Carneiro. Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part V 24_, pages 128-140. Springer, 2021.
* [53] Chengjie Wang, Haokun Zhu, Jinlong Peng, Yue Wang, Ran Yi, Yunsheng Wu, Lizhuang Ma, and Jiangning Zhang. M3dm-nr: Rgb-3d noisy-resistant industrial anomaly detection via multimodal denoising. _arXiv preprint arXiv:2406.02263_, 2024.
* [54] Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, and Chengjie Wang. Multimodal industrial anomaly detection via hybrid fusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8032-8041, 2023.
* [55] Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. _arXiv preprint arXiv:2301.12082_, 2023.
* [56] Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. A unified model for multi-class anomaly detection. _Advances in Neural Information Processing Systems_, 35:4571-4584, 2022.
* [57] Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. _arXiv preprint arXiv:2111.07677_, 2021.
* [58] Vitjan Zavrtanik, Matej Kristan, and Danijel Skocaj. Cheating depth: Enhancing 3d surface anomaly detection via depth simulation. _2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 2153-2161, 2023.
* [59] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8552-8562, 2022.
* [60] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16816-16825, 2022.
* [61] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.
* [62] Qihang Zhou, Shibo He, Haoyu Liu, Tao Chen, and Jiming Chen. Pull & push: Leveraging differential knowledge distillation for efficient unsupervised anomaly detection and localization. _IEEE Transactions on Circuits and Systems for Video Technology_, 2022.

* [63] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection, 2023.
* [64] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2639-2650, 2023.

Dataset

DatasetWe evaluate the performance of ZS 3D anomaly detection on three publicly available 3D anomaly detection datasets, MVTec3D-AD, Eyecandies, and Real3D-AD. MVTec3D-AD comprises 4147 point clouds across 10 categories. These objects exhibit diverse object semantics, including bagel, cable gland, carrot, cookie, dowel, foam, peach, potato, rope, and tire. The training dataset comprises 2656 normal point clouds, and the validation dataset comprises 294 normal point clouds. The test dataset includes 948 normal and 249 anomaly point clouds, covering several anomaly types. Point-wise annotations are available for the point clouds. MVTec3D-AD also provides corresponding 2D-RGB image counterparts for the point clouds. We remove the background plane of point clouds in the whole dataset like [22]. Eyecandies also has 10 different classes and provides the corresponding 2D RGB information. Real3D-AD is a recently available dataset, which contains 12 objects. However, it does not provide the RGB information.

Evaluation Setting and MetricSince the training dataset of MVTec3D-AD only contains all normal samples, **we use an object test dataset as the auxiliary dataset to fine-tune PointAD and assess the ZS anomaly detection for the remaining objects.** In particular, we report the average results using different objects as the auxiliary, i.e., carrot, cookie, and dowel for MVTec3D-AD; confetto, LicoriceSandwich, and PeppermitCandy for Eyecandie, seandorse, shell, and starfish for Real3D-AD. Moreover, **we explore more challenging cross-dataset generalization settings, where we use auxiliary data to test all objects of another dataset.** For point cloud anomaly detection, we only use point clouds to detect and localize 3D anomalies in Figure 5(a). In M3D anomaly detection, both point clouds and their 2D-RGB counterparts are utilized, as shown in Figure 5(b). To comprehensively analyze PointAD, we utilize four metrics to assess its anomaly classification and segmentation performance. For anomaly detection, we use the Area Under the Receiver Operating Characteristic Curve (I-AUROC\(\%\uparrow\)) and average precision (AP\(\%\uparrow\)). Regarding anomaly segmentation, we use point-level AUROC (P-AUROC\(\%\uparrow\)) and a restricted metric called AUPRO\(\%(\uparrow)\)[3] to provide a detailed evaluation of subtle anomaly regions.

## Appendix B Implementation Details

Both point clouds and 2D renderings are resized to 336 \(\times\) 336. We use Open3d library2 to generate 9 views by rotating point clouds along the X-axis at angles of

\begin{table}
\begin{tabular}{c c} \hline \hline View number & Rendering angles \\ \hline
1 & 0 \\
3 & \(-\frac{1}{2}\pi,0,\frac{1}{2}\pi\) \\
5 & \(-\frac{2}{3}\pi,-\frac{1}{2}\pi,0,\frac{1}{2}\pi,\frac{2}{3}\pi\) \\
7 & \(-\frac{4}{4}\pi,-\frac{1}{2}\pi,-\frac{1}{4}\pi,0,\frac{1}{4}\pi,\frac{1}{2}\pi, \frac{2}{3}\pi\) \\
9 & \(-\frac{4}{5}\pi,-\frac{1}{2}\pi,-\frac{1}{2}\pi,-\frac{1}{4}\pi,0,\frac{1}{2} \pi,\frac{2}{3}\pi,\frac{1}{2}\pi\) \\
11 & \(-\frac{5}{6}\pi,-\frac{2}{3}\pi,-\frac{1}{2}\pi,-\frac{1}{2}\pi,\frac{1}{6} \pi,0,\frac{1}{6}\pi,\frac{1}{3}\pi,\frac{1}{2}\pi,\frac{2}{3}\pi,\frac{2}{6}\pi\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study on the number of rendering views.

Figure 5: Inference schematic for ZS 3D and M3D anomaly detection.

\(\{-\frac{4}{5}\pi,-\frac{3}{5}\pi,-\frac{2}{5}\pi,-\frac{1}{5}\pi,0,\frac{1}{5}\pi,\frac{2}{5}\pi,\frac{3}{5}\pi,\frac{4}{5}\pi\}\) for most categories. Some categories lose their surface completely because they are not stereo point clouds. Table 7 also gives the specific rendering angles of different view number settings. We set the rendering angles in a circular manner, evenly distributing the angles between \(-\pi\) to \(\pi\). The backbone of PointAD is the pre-trained CLIP model3 (VIT-L/14@336px). Following [63], we improve the local visual semantics of the vision encoder of CLIP without modifying its parameters and introduce learnable tokens in the text encoder. During training, we keep all CLIP parameters frozen and set the learnable word embeddings in object-agnostic text templates to \(12\). We use the Adam optimizer with a learning rate of \(0.001\) to optimize the learnable parameters. The experiment runs for \(15\) epochs with a batch size of \(4\). All experiments were conducted on a single NVIDIA RTX 3090 24GB GPU using PyTorch-2.0.0.

Footnote 3: https://github.com/mlfoundations/open_clip

## Appendix C Baselines

ZS 3D anomaly detection and M3D anomaly detection have not yet been explored. We first make an adaption for the original CLIP for 3D anomaly detection. Then, we reproduce SOTA ZS 3D classification methods (_i.e._, PointCLIP V2 [64] and Cheraghian [12]) and adapt them to our settings. SOTA unsupervised 3D anomaly detection approaches are reported as the performance upper bound. All hyperparameters in these baselines are kept the same. We will present the detailed reproduction as follows:

* CLIP + Rendering is a method, where we apply the original CLIP into our framework for ZS 3D anomaly detection. It uses the same rendering procedure as PointAD. Following [24, 63], we integrate anomaly semantics into CLIP by two class text prompt templates: A photo of a normal [cls] and A photo of an anomalous [cls], where cls denotes the target class name.
* PointCLIP V2 (CVPR 2023) is a SOTA ZS 3D classification method based on CLIP, they project point clouds into depth maps from different views. To adapt PointCLIP V2 into ZS anomaly detection, we replace its original text prompts point cloud of a big [c] with normal text prompts point cloud of a big [c] and abnormal text prompts point cloud of a big damaged [c].
* AnomalyCLIP (ICLR 2024) is a SOTA zero-shot 2D anomaly detection method. AnomalyCLIP introduces object-agnostic learning to capture generic anomaly semantics of images. We adapt AnomalyCLIP in 3D detection by fine-tuning AnomalyCLIP on depth images of corresponding point clouds.
* Cheraghian (IJCV 2022) is an approach for ZS 3D classification without foundation models. They directly extract the point presentations by PointNet and use word2vector [34] to generate the textual embedding of an object. To incorporate the anomaly semantics into Cheraghian, we average the textual embeddings of [c] and damaged. We replace the global representation with dense representations to provide the segmentation to provide the segmentation results.

## Appendix D Related Work

### 2D Anomaly Detection

2D anomaly detection has been studied extensively by leveraging RGB information [47, 46, 14, 52, 42, 11, 62, 43]. Related works can be categorized into two branches: end-to-end and memory-based methods. Representative end-to-end methods exploit knowledge distillation [3, 62, 45] and normalizing flow [20, 57] to model the normal distribution. Instead, memory-based methods [44, 55] store normal features to construct normal prototypes. ZS 2D anomaly detection is proposed to target a challenging problem where training samples are inaccesible [17, 27, 33, 1, 16, 10]. WinCLIP [24] attempts to explore ZS 2D anomaly detection using CLIP. AnomalyCLIP [63] first introduces object-agnostic prompt learning to capture generic normality and abnormality, detecting anomalies across datasets. PromptAD [30] focuses on effectively fusing these embeddings to enhance zero-shot detection performance.

## Appendix E Analysis on Rendering Conditions

Rendering qualityPointAD interprets point clouds through their corresponding 2D renderings, and the quality of these renderings impacts the information that PointAD can extract from the original point clouds. In our manuscript, we used the Open3D Library to render the point clouds, but it does not provide an API for controlling rendering quality. To simulate varying rendering quality, we applied Gaussian blur with different extents \(\sigma\) to the 2D renderings. Sample visualizations are included in Figure 6. Specifically, we conducted experiments on MVTec3D-AD using different blur \(\sigma\) values (i.e., ). Table 8 shows that the detection performance of PointAD diminishes as rendering quality decreases (with increasing sigma). However, the degradation is acceptable even when the renderings are heavily blurred (\(\sigma\) equals 9). In such cases, PointAD still outperforms baselines that use high-quality renderings.

Input ResolutionHere, we study the effect of resolutions of input point clouds. To create low-resolution point clouds, we downsample the entire high-resolution point clouds using Farthest Point Sampling (FPS) with various sampling ratios. This strategy allows us to generate corresponding low-resolution datasets for training and evaluating PointAD. Visualizations of these datasets are provided in Figure 7. We train PointAD using the resulting low-resolution samples and test PointAD on the same resolution. Table 9 demonstrates that PointAD maintains a strong detection capacity for low-resolution point clouds when the downsampling ratios are 20%, 30%, 50%, and 70%. Even at 20% resolution, PointAD still achieves state-of-the-art performance. This indicates that PointAD is generally applicable to point clouds with various resolutions.

Rendering angles and different numbers of viewsPointAD interprets point clouds from their 2D renderings, where rendering angles and numbers collectively determine the amount of information derived. They have different emphases. For rendering angles, the importance lies in the discrepancy between adjacent angles, as this affects the information granularity that PointAD obtains from adjacent views. When the angle discrepancy is fixed, the number of renderings determines the coverage of 3D information in the resulting 2D renderings. To capture all point cloud information, especially abnormal points, it is crucial to ensure comprehensive coverage. Therefore, our approach in selecting rendering angles and the number of renderings is to guarantee that all points in point clouds are adequately represented.

Based on this principle, we conducted experiments to assess the impact of the number of renderings on PointAD's detection performance, circularly rendering point clouds to ensure even coverage of all points. As shown in Table 5, increasing the number of views allows PointAD to gather more detailed information from the 2D renderings, benefiting from smaller angle discrepancies, which improves detection and localization results. However, when the number of views increased from 9 to 11, we observed a performance decline in PointAD, with the I-AUROC for global multimodal detection dropping from 87.4% to 86.4%. This suggests that incorporating too many views could introduce redundant information, resulting in 2D renderings with extensive overlap and excessive

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline blur & \multicolumn{2}{c}{Point detection} & \multicolumn{2}{c}{Multimodal detection} \\ sigurn & Global & Local & Global & Local \\ \hline
**0** & **0.20**(0.92) & **0.95**(0.84) & **0.69**(0.94) & **0.72**(0.90) \\ \hline
1 & 60.1 & 93.5 & 95.2 & 83.2 & 55.5 & 97.0 \\
5 & 70.2 & 92.9 & 95.1 & 82.3 & 96.4 & 97.6 \\
9 & (77.6 & 92.2) & (95.1 & 82.3) & (86.4 & 97.6 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Analysis on the rendering quality. The original setting is highlighted in gray.

Figure 6: Visualization with different rendering quality. A larger \(\sigma\) represents poorer rendering quality.

Figure 7: Visualization with different resolutions. We downsample entire point clouds with different ratios to obtain diverse resolutions.

local detail. This overemphasis on local information can impede global recognition. Hence, the appropriate number of views benefits point understanding from informative views while mitigating the adverse effects of redundant local information. To further explore the impact of the absolute angle, we shift the rendering angles while keeping the angle discrepancy unchanged. The original adjacent angle discrepancy in our paper is \(\frac{1}{5}\pi\). We divide this discrepancy into four parts and perform angle shifts of \(\frac{1}{20}\pi\), \(\frac{2}{20}\pi\), and \(\frac{3}{20}\pi\) to test the impact of varying rendering angles. Table 10 shows that PointAD maintains consistent performance across different rendering angles, demonstrating its robustness to variations in angles different from those used during training.

Rendering lightingFurther exploring the robustness of PointAD under different conditions could enhance its generalized detection performance. We conducted ablation studies to test its sensitivity under different lighting conditions and with occluded point clouds below. To evaluate the impact of rendering lighting, we adjusted the lighting conditions to render point clouds, generating variant datasets with different lighting. We used both stronger and weaker lighting to render point clouds compared to the original dataset, covering a broad lighting range. We denote stronger and the strongest lighting as "+" and "++", and weaker and the weakest lighting as "-" and "-". Visualizations of the resulting samples are presented in Figure 9. The experiments were conducted on MVTec3D-AD, where we tested PointAD, trained on the original dataset, on these lighting variant datasets. Table 11 shows that PointAD can still detect anomalies even with significant discrepancies in rendering lighting, suggesting that PointAD is not sensitive to variations in rendering lighting.

Point occlusionsNext, we evaluated detection performance with occluded point clouds. We occluded point clouds by removing those invisible from a specific rendering angle and then used the same rendering parameters to project the remaining points. During this process, we observed that abnormal regions might be occluded totally. Unlike class classification, where class semantics remain unchanged by point occlusions, removing these anomaly semantics would transform anomaly samples into normal points. Therefore, we selected rendering angles that allow visibility of part or all anomalous regions when the point cloud is an abnormal instance. The occluded point clouds are shown in Figure 9. We then used PointAD, trained on the original dataset, to test the resulting occluded dataset. Table 12 shows that PointAD suffers from performance degradation when the point clouds are occluded. We attribute this to two aspects: 1) Despite this strategy, the occluded

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Point detection} & \multicolumn{2}{c}{Multimodal detection} \\  & Global & Local & Global & Local \\ \hline \hline
**original** & (82.0, 94.2) & (95.5, 84.4) & (86.9, 96.1) & (97.2, 90.2) \\ \hline occlusions & (73.3, 90.6) & (94.3, 80.8) & (83.0, 94.8) & (96.7, 89.5) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Analysis on the rendering angle.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \multirow{2}{*}{Lighting} & \multicolumn{2}{c}{Point detection} & \multicolumn{2}{c}{Multimodal detection} \\  & Global & Local & Global & Local \\ \hline \hline
**+** & (82.0, 94.3) & (95.4, 83.7) & (87.5, 98.9) & (97.2, 90.7) \\
**+** & (82.4, 94.6) & (95.4, 83.8) & (86.1, 95.9) & (97.1, 90.5) \\ \hline \hline
**original** & (82.0, 94.2) & (95.5, 84.0) & (86.9, 95.9) & (97.1, 90.7) \\ \hline
**-** & (82.4, 94.5) & (95.3, 83.4) & (86.4, 95.9) & (97.1, 90.6) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Analysis on the rendering lighting.

[MISSING_PAGE_FAIL:19]

## Appendix G Failure Cases

In this section, we present the failure cases of PointAD, which we attribute to direct multimodality fusion. Since our model uses hybrid loss to incorporate the 3D and 2D anomaly semantics, it performs ZS multimodality 3D anomaly detection in a plug-and-play manner. However, when one modality prediction deviates severely from the ground truth in rare instances, direct fusion may result in an unpromising multimodal score map. As shown in Figure 10, the hole in the cookie is visually similar to the chocolate on cookies, making it challenging to differentiate the hole anomaly via color information alone. Although PointAD can detect the hole based on its abnormal point relations, the RGB score map heavily influences the final multimodal score map. Conversely, the tire presents an inverse situation where RGB can effectively predict the anomalies, but the point score map fails to recognize it. The false detection could arise from unusual point density and distribution. To demonstrate the effect of point density, we randomly select normal regions of point clouds and subsequently increase or decrease the density of these regions through upsampling and downsampling. We provide a qualitative analysis in 11. The visualization shows that PointAD effectively resists noise at reasonable levels. However, when noise levels are extremely low or high, the corresponding regions become excessively sparse or dense. This causes normal regions to appear similar to hole anomalies or squeezed anomalies, leading PointAD to classify these noisy areas as anomalies. Non-parametric score alignment and filter methods could be a potential direction, which we leave for further work.

Robustness to point densityTo investigate the impact of point density differences between the training and test datasets, we train PointAD using high-density point clouds (original dataset) and then test it on the low-density versions of the datasets, downsampled as described in **Input resolution**. Table 16 shows that PointAD can still detect anomalies even when retaining 50% of the points from the original point clouds. However, when more points are removed (30% and 20% sample ratio), PointAD experiences an obvious performance degradation. We attribute the misdetection to the overly sparse point clouds forming holes. Nevertheless, PointAD can still detect anomalies even with

\begin{table}
\begin{tabular}{c|c c c} \hline \hline \begin{tabular}{c} Downsample \\ radio \\ \end{tabular} & \begin{tabular}{c} Point detection \\ Global \\ \end{tabular} & \begin{tabular}{c} Multimodal detection \\ Global \\ \end{tabular} & 
\begin{tabular}{c} Local \\ Local \\ \end{tabular} \\ \hline
1000\% & (020,020,020) & (965,884,846) & (696,961) & (972,920) \\
70\% & (013,940, 949, 92.63) & (056,95, 97) & (068,900) \\
50\% & (79.6, 93.4) & (947,81.8) & (434,95.95) & (056,89.99) \\
30\% & (76.6, 91.8) & (94.7, 97.0) & (93.5, 15.1) & (064,89.59) \\
20\% & (72.7, 90.5) & (94.2, 78.2) & (02.0, 94.6) & (95.2, 88.6) \\ \hline \hline \end{tabular}
\end{table}
Table 16: Effect on point density gap.

Figure 11: The impact of noise level. We randomly downsample and upsample part of normal regions to create different point densities.

Figure 10: Failure case in PointAD.

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline \hline  & Method & \begin{tabular}{c} Candy \\ Case \\ \end{tabular} & \begin{tabular}{c} Chocolate \\ Cooke \\ \end{tabular} & \begin{tabular}{c} Concide \\ Praline \\ \end{tabular} & 
\begin{tabular}{c}

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Candy} & \multicolumn{2}{c}{Cheoclute} & \multicolumn{2}{c}{Cheoclute} & \multicolumn{2}{c}{Gummy} & \multicolumn{2}{c}{Hazelnut} & \multicolumn{2}{c}{Licicorie} & \multicolumn{2}{c}{Lellipp} & \multicolumn{2}{c}{Marsh-} & \multicolumn{2}{c}{Peppermant} & \multirow{2}{*}{Mean} \\  & \multicolumn{2}{c}{Came} & \multicolumn{2}{c}{Cookie} & \multicolumn{2}{c}{Praline} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\ \hline \multirow{2}{*}{MG} & PointCLIPV2\({}_{\alpha}\) & (47.8, 52.7) & (51.9, 55.6) & (31.8, 42.4) & (38.6, 44.9) & (46.0, 49.3) & (32.0, 40.3) & (54.5, 5.4) & (42.7, 30.4) & (58.8, 59.8) & (47.7, 49.8) & (45.2, 48.0) \\  & AnomalyCLIP & (49.5, 52.0) & (31.4, 48.1) & (65.2, 67.3) & (69.5, 70.4) & (41.2, 44.6) & (53.8, 56.3) & (51.2, 55.0) & (60.4, 45.4) & (51.8, 53.3) & (60.6, 33.7) & (56.3, 57.1) \\  & PointCLO-COp & (54.5, 58.5) & (51.6, 59.1) & (72.8, 81.1) & (79.3, 36.4) & (69.1, 76.5) & (62.6, 64.3) & (78.9, 83.6) & (74.4, 67.5) & (67.2, 73.7) & (61.1, 87.0) & (69.1, 72.7) \\  & PointCLO-COp & (51.7, 58.9) & (67.9, 64.7) & (16.8, 84.4) & (79.8, 39.7) & (66.8, 75.0) & (61.3, 61.6) & (61.0, 81.0) & (63.8, 52.3) & (71.8, 71.7) & (63.8, 56.0) & (69.5, 74.3) \\ \hline \multirow{2}{*}{L} & PointCLIPV2\({}_{\alpha}\) & (45.0, -) & (38.4, -) & (49.2, -) & (43.2, -) & (45.0, -) & (55.2, 21.6) & (38.7, -) & (43.4, -) & (42.9, -) & (37.9, -) & (43.9, -) \\  & AnomalyCLIP & (49.5, 58.4) & (76.6, 34.1) & (60.1, 46.7) & (74.7, 42.4) & (80.4, 42.7) & (75.3, 33.1) & (80.7, 40.5) & (80.6, 86.1) & (60.1, 7.4) & (79.4, 45.0) & (79.6, 45.4) \\  & PointCLO-COp & (97.5, 8.5) & (79.4, 74.4) & (62.1, 76.6) & (90.4, 67.3) & (80.6, 69.3) & (84.4, 4.5) & (63.1, 72.7) & (97.8, 86.8) & (66.1, 59.4) & (92.0, 71.7) & (91.8, 70.5) \\ \hline \multirow{2}{*}{ML} & PointCLIPV2\({}_{\alpha}\) & (45.1, -) & (47.2, -) & (48.1, -) & (63.1, -) & (49.7, -) & (54.0, -) & (51.6, -) & (45.2, -) & (33.3, -) & (44.4, -) & (41.9, -) & (47.3, -) \\  & AnomalyCLIP & (59.8, 2.5) & (91.6, 7.1) & (83.6, 62.0) & (84.9, 80.6) & (80.6, 45.4) & (73.6, -) & (89.6, 66.9) & (79.6, 68.0) & (79.5, 42.8) & (49.2, 51.8) & (86.2, 61.3) \\ \hline \multirow{2}{*}{L} & PointCLIPV2\({}_{\alpha}\) & (45.0, -) & (38.4, -) & (49.2, -) & (43.2, -) & (45.0, -) & (55.2, 21.6) & (38.7, -) & (43.4, -) & (42.9, -) & (37.9, -) & (43.9, -) \\  & AnomalyCLIP & (59.8, 4.0) & (76.7, 34.1) & (60.1, 46.7) & (78.4, 42.7) & (80.4, 42.7) & (75.3, 33.1) & (80.7, 40.5) & (80.6, 66.1) & (60.1, 7.4) & (79.4, 45.0) & (79.6, 45.4) \\  & PointCLO-COp & (97.5, 8.5) & (92.4, 74.4) & (92.1, 76.6) & (90.4, 67.3) & (90.6, 67.3) & (84.4, 4.5) & (63.1, 72.7) & (97.8, 86.8) & (66.1, 59.4) & (92.0, 71.7) & (91.8, 70.5) \\ \hline \multirow{2}{*}{ML} & PointCLIPV2\({}_{\alpha}\) & (45.1, -) & (47.2, -) & (48.1, -) & (63.1, -) & (49.7, -) & (54.0, -) & (51.6, -) & (45.2, -) & (33.3, -) & (44.4, -) & (41.9, -) & (47.3, -) \\  & AnomalyCLIP & (59.8, 2.5) & (91.6, 7.1) & (83.6, 62.0) & (84.9, 80.6) & (80.6, 45.4) & (73.6, -) & (89.6, 66.9) & (79.6, 68.0) & (79.5, 42.8) & (49.2, 51.8) & (86.2, 61.3) \\  & PointCLO-COp & (92.0, 7.1) & (96.8, 86.4) & (94.2, 86.2) & (97.9, 92.5) & (87.9, 64.8) & (91.3, 68.6) & (96.2, 83.6) & (96.5, 8.2) & (95.3, 81.3) & (96.1, 86.7) & (94.4, 80.3) \\ \hline \multirow{2}{*}{L} & PointCLIPV2\({}_{\alpha}\) & (50.5, -) & (53.6, -) & (54.2, -) & (47.0, -) & (54.1, -) & (60.7, -) & (59.4, -) & (51.2, -) & (47.1, -) & (55.4, 18.2) & (41.6, -) & (47.5, -) & (51.9, -) \\  & AnomalyCLIP & (50.9, -) & (49.6, -) & (49.8, -) & (50.1, -) & (57.5, -) & (47.9, -) & (48.6, -) & (48.3, -) & (50.2, -) & (50.5, -) & (49.3, -) & (51.0, -) & (50.3, -) \\  & PointCLO-COp & (61.4, -) & (71.2, -) & (64.6, -) & (77.7, -) & (35.0, -) & (54.9, -) & (76.9, -) & (69.3, -)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction precisely reflect the contribution and scope of this paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: we have created a separate "Limitations" section in our paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include theoretical results Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a detailed illustration of our proposed algorithm and baselines in the Appendix C, Appendix B, and Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: we will make our code and dataset available once the paper is accepted. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide full details in Appendix B and Appendix F Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the average results across three runs in Section 4.3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We point out the specific compute resources in Section 4.2 Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our paper obeys the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts of our paper. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: our paper poses no such risks Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: we respect the Licenses for existing assets that we use. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: we will release new assets proposed in our paper once the paper is accepted. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.