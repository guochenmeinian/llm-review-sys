# FedLPA: One-shot Federated Learning with

Layer-Wise Posterior Aggregation

Xiang Liu\({}^{1,\dagger}\), Liangxi Liu\({}^{2,\dagger}\), Feiyang Ye\({}^{3}\), Yunheng Shen\({}^{4}\), Xia Li\({}^{5}\), Linshan Jiang\({}^{1,\ddagger}\), Jialin Li\({}^{1,\ddagger}\)

\({}^{1}\)National University of Singapore, \({}^{2}\)Northeastern University,

\({}^{3}\)University of Technology Sydney, \({}^{4}\)Tsinghua University, \({}^{5}\)ETH Zurich

{liuxiang,lijl}@comp.nus.edu.sg liu.liangx@northeastern.edu feiyang.ye.uts@gmail.com shenyhi9@mails.tsinghua.edu.cn thilixia@gmail.com linshan@nus.edu.sg

\({}^{\dagger}\)Equal Contribution \({}^{\ddagger}\)Correspondence Author

###### Abstract

Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing communication overhead, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with layer-wise posterior aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any private label information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in the practical non-IID scenario, we efficiently infer the posteriors of each layer in each local model using layer-wise Laplace approximation and aggregate them to train the global parameters. Extensive experimental results demonstrate that FedLPA significantly improves learning performance over state-of-the-art methods across several metrics.

## 1 Introduction

Data privacy issues in Deep Learning [1, 2, 3, 4, 5, 6, 7] have grown to be a major global concern [8]. To safeguard data privacy, the conventional federated learning algorithm will use the aggregation methods and follow the data management rules of different institutions, which implies that the distribution of data exhibits variations among clients [8]. In the domain of machine learning, federated learning (FL) [9, 10, 11] has emerged as a prominent paradigm. The fundamental tenet of federated learning revolves around sharing machine learning models derived from decentralized data repositories, as opposed to divulging user raw data. This approach effectively preserves the confidentiality of individual data.

The standard federated learning framework, FedAvg [9, 12], applies local model training. These local models are then aggregated into a global model through parameter averaging. Existing FL algorithms, however, require many communication rounds to effectively train a global model, leading to substantial communication overhead, increased privacy concerns, and higher demand for fault tolerance throughout the rounds. One-shot FL, which reduces client-server communication into a single round as explored by prior work [13, 14, 15], is a promising yet challenging scheme to address these issues. One-shot FL proves particularly practical in scenarios where iterative communication isnot feasible. Moreover, a reduction in communication rounds translates to fewer opportunities for any potential eavesdropping attacks.

While one-shot FL shows promises, existing approaches often grapple with challenges such as inadequate handling of high statistical heterogeneity information [16; 17] or non-independent and non-identically distributed (non-IID) data [18; 19]. Moreover, some prior methods rely on an auxiliary public dataset to achieve satisfactory performance in one-shot FL [13; 14], or even on pre-trained large models [20], which may not be practical [21] in some sensitive scenarios. Additionally, some approaches, such as those [22; 19; 23; 15]), might expose private label information to both local and global models, e.g., the client label distribution, potentially violating General Data Protection Regulation (GDPR) rules. Furthermore, some prior methods [14; 18; 24] require substantial computing resources for dataset distillation, model distillation, or even training a generator capable of generating synthetic data for second-stage training on the server side, making them less practical.

Besides, the performance of one-shot FL often falls short when dealing with non-IID data. Non-IID data biases global updates, reducing the accuracy of the global model and slowing down convergence. In extreme non-IID cases, clients may be required to address distinct classes solely on their side. Several approaches to federated learning are proposed in multi-round settings to tackle this heterogeneity among clients. In the work [25], it allows each client to use a personalized model instead of a shared global model. With the personalized approach, a multi-round framework benefits from joint training while allowing each client to keep its unique model. However, one-shot aggregation on a local model is far from being resolved to address the concern of non-i.i.d data distributions.

In this paper, we introduce a novel one-shot aggregation approach to address these issues, named FedLPA (Federated Learning with Layer-wise Posterior Aggregation). FedLPA infers the posteriors of each layer in each local model using the empirical Fisher information matrix obtained by layer-wise Laplace Approximation. Laplace Approximations are widely used to compute the empirical Fisher information matrix for neural networks, conveying the data statistics in non-i.i.d settings. However, computing empirical Fisher information matrices of multiple local clients and aggregating their Fisher information matrices remains an ongoing challenge [17]. To mitigate it, FedLPA aggregates the posteriors of local models using the accurately computed block-diagonal empirical Fisher information matrices to measure the parameter space. This matrix captures essential parameter correlations and distinguishes itself from prior methods by being non-diagonal and non-low-rank, thereby conveying the statistics of biased local datasets. After that, the global model parameters are aggregated without any need for server-side knowledge distillation [26].

Our extensive experiments verify the efficiency and effectiveness of FedLPA, highlighting that FedLPA markedly enhances the test accuracy when compared to existing one-shot FL baseline approaches across various datasets. Our main contributions are summarized as follows:

* To the best of our knowledge, we are the first to propose an effective one-shot federated learning approach that trains global models using block-diagonal empirical Fisher information matrices. Our approach is data-free without any need for any auxiliary dataset and label information and significantly improves system performance, including negligible communication cost and moderate computing overhead.
* We are the first to train global model parameters via constructing a multi-variate linear objective function and optimizing its quadratic form, which allows us to formulate and solve the problem in a convex form efficiently, which has a linear convergence rate, ensuring good performance.
* We conduct extensive experiments to illustrate the effectiveness of FedLPA. Our approach consistently outperforms the baselines, showcasing substantial improvement across various settings and datasets. Even in some extreme scenarios where label skew is severe, e.g., each client has only one class, we achieve satisfactory results while other existing one-shot federated learning algorithms struggle.

## 2 Background and related works

### Federated learning on non-iid data

Previous work FedAvg [9] first introduced the concept of FL and presented the algorithm, which achieved competitive performance on i.i.d data, in comparison to several centralized techniques. However, it was observed in previous works [27; 28] that the convergence rate and ultimate accuracy of FedAvg on non-IID data distributions were significantly reduced, compared to the results observed with homogeneous data distributions.

Several methods have been developed to enhance performance in federated learning against non-IID data distributions. The SCAFFOLD method [29] leveraged control variates to reduce objective inconsistency in local updates. It estimated the drift of directions in local optimization and global optimization and incorporated this drift into local training to align the local optimization direction with the global optimization. FedNova [30] addressed objective inconsistency while maintaining rapid error convergence through a normalized averaging method. It scaled and normalized the local updates of each client based on the number of local optimization steps. FedProx [31] enhanced the local training process by introducing a global prior in the form of an \(L2\) regularization term within the local objective function. Researchers introduced PFNM [32; 33], a Bayesian probabilistic framework specifically tailored for multilayer perceptrons. PFNM employed a Beta-Bernoulli process (BBP) [34] to aggregate local models, quantifying the degree of alignment between global and local parameters. The framework [17] proposed utilized a multivariate Gaussian product method to construct a global posterior by aggregating local posteriors estimated using an online Laplace approximation. FedPA [16] also applied the Gaussian product method but employed stochastic gradient Markov chain Monte Carlo for approximate inference of local posteriors. DAFL (Data-Free Learning) [35] introduced an innovative framework based on generative adversarial networks. ADI [36] utilized an image synthesis method that leveraged the image distribution to train deep neural networks without real data. The pFedHN method [37] incorporated HyperNetworks [38] to address federated learning applications.

However, all of these methods encountered challenges in the one-shot federated learning setting, as they required aggregating the model by multiple rounds and might be inaccurate due to the omission of critical information, such as posterior joint probabilities between different parameters.

### One-shot federated learning

One-shot Federated Learning (FL) is an emerging and promising research direction characterized by its minimal communication cost. In the first study on one-shot FL [13], the approach involved the aggregation of local models, forming an ensemble to construct the final global model. Subsequently, knowledge distillation using public data was applied in the following step. FedKT [14] brought forward the concept of consistent voting to fortify the ensemble. Recent research endeavors [19; 24] proposed data-free knowledge distillation schemes tailored for one-shot FL. These methods adopted the basic ensemble distillation framework as FedDF [26]. XorMixFL [22] introduced the use of exclusive OR operation (XOR) for encoding and decoding samples in data sharing. It is important to note that XorMixFL assumed the possession of labeled samples from a global class by all clients and the server, which might not align with practical real-world scenarios. A noteworthy innovation of DENSE [24] was its utilization of a generator to create synthetic datasets on the server side, circumventing the need for a public dataset in the distillation process. Co-Boosting [39] improves the ensemble when doing the distillation to improve the performance. FedOV [15] delved into addressing comprehensive label skew cases. FEDCVAE [23] confronted this challenge by transmitting all label distributions from clients to servers. These schemes [22; 14; 19; 24; 23; 15] exposed some client-side private information, leading to additional communication overhead and potential privacy leakage, e.g., FEDCVAE [23] needed all the client label distribution to be transmitted to the server side and FedOV [15] needed the clients to know the labels which were unknown. Instead, MA-Echo [40] adopted a unique approach by emphasizing the addition of norms among layer-wide parameters during the aggregation of local models. The project [41] focused on the theoretic analysis of the error in its approximation method. However, their method grappled with limited experiments and lacked detailed explanations of the approach. FedDISC [20], on the other hand, relied on the pre-trained model CLIP from OpenAI, where their reliance might not always align with practicality or suitability for diverse scenarios.

While some of these techniques are orthogonal to FedLPA and can be integrated with it, it is worth noting that none of the previously mentioned algorithms possess the capability to train global model parameters using empirical Fisher information matrices on extensive experiment settings. Some

[MISSING_PAGE_FAIL:4]

Modern algorithms [45; 46] allow the local training process to obtain an optimal, regarded as the expectation \(\mathbf{\mu}_{k}\) in the above equations. However, \(\tilde{\mathbf{H}}_{k}\) is intractable to compute due to a large number of parameters in modern neural networks. An efficient method is to approximate \(\tilde{\mathbf{H}}_{k}\) using the empirical Fisher information matrix [47].

Inferring the local layer-wise posteriors with the block-diagonal empirical Fisher information matrices

A empirical Fisher \(\tilde{\mathbf{F}}\) is defined as below:

\[\tilde{\mathbf{F}}=\sum_{s\in\mathcal{D}}\left[\nabla\log p(s|\mathbf{\theta}) \nabla\log p(s|\mathbf{\theta})^{\top}\right] \tag{8}\]

where \(p(s|\mathbf{\theta})\) is the likelihood on data point \(s\). It is an approximate of the Fisher information matrix, the empirical Fisher information matrix is equivalent to the expectation of the Hessian of the negative log posterior if assuming \(p(s|\mathbf{\theta})\) is identical for each \(s\in\mathcal{D}\).

Therefore, the local co-variance \(\mathbf{\Sigma}_{k}\) can be approximated by the empirical Fisher \(\tilde{\mathbf{F}}_{k}\)[48; 49].

\[\mathbf{\Sigma}_{k}^{-1}\approx\tilde{\mathbf{F}}_{k}+\lambda\mathbf{I} \tag{9}\]

The works [50; 51; 17] ignore co-relations between different parameters and only consider the self-relations of parameters as computing all co-relations is impossible. Thus, their methods are inaccurate. Detailed discussions and the novelty compared to previous works are in Appendix B.

In order to capture co-relations between different parameters efficiently, previous works [46; 43] estimate a block empirical Fisher information matrix \(\mathbf{F}\) instead of assuming parameters are independent and approximating the co-variance by the diagonal of the empirical Fisher. As pointed out, co-relations inner a layer are much more significant than others [46; 52; 53], while computing the co-relations between different layers brings slight improvement but much more computation [54; 43]. Therefore, assuming parameters are layer-independent is a good trade-off. As a result, the approximated layer-wise empirical Fisher is block-diagonal. For layer \(l\) on client \(k\), its empirical Fisher \(F_{kl}\) is one of the diagonal blocks in the whole empirical Fisher for the local model and is factored into two small matrices as below,

\[\mathbf{\Sigma}_{k_{l}}^{-1}\approx\mathbf{F}_{k_{l}}=\mathbf{A}_{k_{l}}\otimes \mathbf{B}_{k_{l}} \tag{10}\]

where \(\otimes\) is the Kronecker product; \(\mathbf{A}_{k_{l}}=\mathbb{E}\left[\hat{\mathbf{a}}_{k_{l-1}}\hat{\mathbf{a}}_{ k_{l-1}}^{\top}\right]+\pi_{l}\sqrt{\lambda}\mathbf{I}\) and \(\mathbf{B}_{k_{l}}=\mathbb{E}\left[\hat{\mathbf{b}}_{k_{l}}\hat{\mathbf{b}}_{ k_{l}}^{\top}\right]+\frac{1}{\pi_{l}}\sqrt{\lambda}\mathbf{I}\) are two expectation factor matrices over the data samples; \(\hat{\mathbf{a}}_{k_{l}}\) is the activations and \(\hat{\mathbf{b}}_{k_{l}}\) is the gradient of the pre-activations of layer \(l\) on client \(k\), \(\lambda\) is the hyperparameter and \(\pi_{l}\) is a factor minimizing approximation error in \(\mathbf{F}_{k_{l}}\)[46; 49; 55]. \(\mathbf{A}_{k_{l}}\) and \(\mathbf{B}_{k_{l}}\) are symmetric positive definite matrices [45; 46].

We use \(\mathbf{\theta}_{k_{l}}\) to denote the parameter vector of layer \(l\) and \(\mathbf{M}_{k_{l}}=vec^{-1}(\mathbf{\mu}_{k_{l}})\) is the vectorized optimal weight matrix of layer \(l\) on client \(k\). Thus, the resulting local layer-wise posterior approximation is \(\mathbf{\theta}_{k_{l}}\sim\mathcal{N}(\mathbf{\mu}_{k_{l}},\mathbf{F}_{k_{l}}^{-1})\).

### Estimating the global expectation

Given the local posteriors, the global expectation could be aggregated by Eq. 7. With Eq. 10, the \(l\)-th layer's global expectation \(\bar{\mathbf{\mu}}_{l}\) consists of Kronecker products:

\[\bar{\mathbf{\mu}}_{l} =\bar{\mathbf{\Sigma}}_{l}\sum_{k}^{K}\mathbf{\Sigma}_{k_{l}}^{-1}\mathbf{\mu }_{k_{l}}=\bar{\mathbf{\Sigma}}_{l}\sum_{k}^{K}\left(\mathbf{A}_{k_{l}}\otimes \mathbf{B}_{k_{l}}\right)\mathbf{\mu}_{k_{l}} \tag{11}\] \[=\bar{\mathbf{\Sigma}}_{l}\sum_{k}^{K}\mathrm{vec}(\mathbf{B}_{k_{l }}\mathbf{M}_{k_{l}}\mathbf{A}_{k_{l}})=\bar{\mathbf{\Sigma}}_{l}\sum_{k}^{K}\mathbf{ z}_{k_{l}}=\bar{\mathbf{\Sigma}}_{l}\bar{\mathbf{z}}_{l}\]

where \(\bar{\mathbf{z}}_{l}=\sum_{k}^{K}\mathbf{z}_{k_{l}}\) and \(\mathbf{z}_{k_{l}}=\mathrm{vec}(\mathbf{B}_{k_{l}}\mathbf{M}_{k_{l}}\mathbf{A }_{k_{l}})\) is a immediate notations for simplification. For the global expectation, we have \(\bar{\mathbf{\mu}}=\bar{\mathbf{\Sigma}}\cdot\bar{\mathbf{z}}\). The corresponding global co-variance is an inverse of the sum of Kronecker products:

\[\bar{\mathbf{\Sigma}}_{l}=(\sum_{k}^{K}\mathbf{A}_{k_{l}}\otimes\mathbf{B}_{k_{l}})^{-1} \tag{12}\]

As shown in Eq. 11, obtaining the global expectation \(\bar{\mathbf{\mu}}_{l}\) requires calculating the inverse of \(\bar{\mathbf{\Sigma}}_{l}^{-1}\) as Eq. 12, which is unacceptable and the details are in Appendix C. Thus, we propose our method to directly train the parameters of the global model on the server side.

### Train the parameters of the global model

We use \(\mathbb{E}\left[\mathbf{A}\right]\) denotes \(\sum_{k}^{K}(\mathbf{A}_{k})\), \(\mathbb{E}\left[\mathbf{B}\right]\) denotes \(\sum_{k}^{K}(\mathbf{B}_{k})\), \(\mathbb{E}\left[\mathbf{A}\otimes\mathbf{B}\right]\) denotes \(\sum_{k}^{K}(\mathbf{A}_{k}\otimes\mathbf{B}_{k})\). Previous works [46; 49] approximate the expectation of Kronecker products by a Kronecker product of expectations \(\mathbb{E}\left[\mathbf{A}\otimes\mathbf{B}\right]\approx\mathbb{E}\left[ \mathbf{A}\right]\otimes\mathbb{E}\left[\mathbf{B}\right]\) with an assumption of \(\mathbf{A}_{k_{l}}\) and \(\mathbf{B}_{k_{l}}\) are independent, which is called Expectation Approximation (EA). However, it may lead to a biased global expectation. The details are discussed in Appendix D. Instead, we could construct a linear objective after aggregating the approximation of local posteriors via using block-diagonal empirical Fisher information matrices. We denotes \(\bar{\mathbf{M}}\) as the matrix formula of \(\bar{\mathbf{\mu}}=\operatorname{vec}(\bar{\mathbf{M}})\), and the optimal solution of \(f(\bar{\mathbf{\mu}})\) is \(\bar{\mathbf{\mu}}^{*}=\operatorname{vec}(\bar{\mathbf{M}}^{*})\). We construct \(f(\bar{\mathbf{\mu}})\) as a multi-variates linear objective function. When \(\bar{\mathbf{\mu}}=\bar{\mathbf{\mu}}^{*}\) is optimal solution, \(f(\bar{\mathbf{\mu}})=\mathbf{o}\), where \(\mathbf{o}\) is a vector with all zero. Note that

\[\begin{split} f(\bar{\mathbf{\mu}})&=\bar{\mathbf{\Sigma}}^ {-1}\bar{\mathbf{\mu}}-\bar{\mathbf{z}}=\sum_{k}^{K}\operatorname{vec}(\mathbf{B} _{k}\bar{\mathbf{M}}\mathbf{A}_{k})-\bar{\mathbf{z}}\\ &=\operatorname{vec}(\mathbb{E}\left[\mathbf{B}\bar{\mathbf{M}} \mathbf{A}\right])-\bar{\mathbf{z}}\end{split} \tag{13}\]

To obtain the optimal solution, we minimize the following problem to obtain an approximate solution \(\bar{\mathbf{M}}^{*}\) of \(\bar{\mathbf{M}}\):

\[\bar{\mathbf{M}}^{*}=\min_{\bar{\mathbf{M}}}\frac{1}{2}\left\|\sum_{k}^{K} \operatorname{vec}(\mathbf{B}_{k}\bar{\mathbf{M}}\mathbf{A}_{k})-\bar{\mathbf{ z}}\right\|_{2}^{2} \tag{14}\]

The above equation is a quadratic objective, and it can be solved by modern optimization tools efficiently and conveniently. Since the main objective of the above problem is both convex and Lipschitz smooth w.r.t \(\operatorname{vec}(\bar{\mathbf{M}})\), we can use the gradient descent method to solve it with a linear convergence rate. Here, we use automatic differentiation to calculate the gradient w.r.t. \(\bar{\mathbf{M}}\).

```
1:Input: clients \(K\), layers \(L\)
2:Initialize global weight \(\bar{\mathbf{W}}_{l}\) of layer \(l=1,...,L\)
3:clients executes:
4:Initialize local model
5:for k = 1,..., K do
6:\(\{\mathbf{M}_{k_{l}},\mathbf{A}_{k_{l}},\mathbf{B}_{k_{l}}|l=1,...,L\}\leftarrow\) local training
7:endfor
```

**Algorithm 1** FedLPA Global Aggregation

### Overall FedLPA algorithm and discussions

In summary, the proposed algorithm FedLPA follows the same paradigm as the standard one-shot federated learning framework. In FedLPA, the clients locally train their models to get \(\mathbf{M}_{k}\) and calculate the local co-variance over its training dataset using the layer-wise Laplace approximation to compute \(\mathbf{A}_{k},\mathbf{B}_{k}\). Subsequently, each client transmits their local \(\mathbf{A}_{k},\mathbf{B}_{k},\mathbf{M}_{k}\) to the server. Following Algorithm 1, the server aggregates these contributions to obtain the global expectation, as described in Eq. 7, then trains the global model parameters, as outlined in Eq. 14. Thus, the transmitted data between the clients and the server is solely \(\mathbf{A}_{k},\mathbf{B}_{k},\mathbf{M}_{k}\) without any extra auxiliary dataset and label information.

Note that FedLPA can be directly adopted in most common scenarios. For the special case that the neural model has enormous single-layer weight parameters, how to extend our proposed FedLPA is discussed in Appendix E.

### t-SNE observation and discussions

To quickly demonstrate the effectiveness of FedLPA, we show the t-SNE visualization of our FedLPA global model on the MNIST dataset as an example with a biased training data setting among 10 local clients. The experiment details, t-SNE visualizations of the local models and the global models of other algorithms and discussions are in Appendix G.1. As shown in Figure 1, FedLPA generates the global model which can clearly distinguish these classes, meanwhile, the classes are separate.

### Privacy Discussions

FedLPA is intuitively compatible with existing privacy-preserving techniques, such as differential privacy (DP) [56; 57], secure multiparty computation (SMC) [58; 59], and homomorphic encryption (HE) [60; 61; 62]. In Appendix F.1, we propose a naive DP-FedLPA with two different mechanisms to show the compatibility with differential privacy. Meanwhile, we mention that our proposed FedLPA has the same privacy-preserving level as the conventional federated learning algorithms (i.e, FedAvg, FedProx, FedNova and Dense). Compared with FedAvg, we have conducted a detailed analysis from a privacy attack perspective to show that our proposed FedLAP exhibits a security level consistent with FedAvg against several types of privacy attacks, where the details are shown in Appendix F.3. Note that the main focus of FedLPA is to improve the learning performance on the one-shot FL settings, thus, we leave the integration with other privacy-preserving techniques beyond DP as an open problem.

## 4 Experiments

### Experiments settings

**Datasets.** We conduct experiments on MNIST [63], Fashion-MNIST [64], CIFAR-10 [65], and SVHN [66] datasets. In most of the previous works and the most popular benchmark, the majority of their experiments use these datasets and these models. We choose these datasets and models to do the majority of our experiments following these established methods and benchmarks to fairly compare our method with the baselines. We use the data partitioning methods for non-IID settings of the benchmark 1 to simulate different label skews. Specifically, we try two different kinds of partition: 1) #C = \(k\): each client only has data from \(k\) classes. We first assign \(k\) random class IDs for each client. Next, we randomly and equally divide samples of each class to their assigned clients; 2) \(p_{k}\) - Dir(\(\beta\)): for each class, we sample from Dirichlet distribution \(p_{k}\) and distribute \(p_{k,j}\) portion of class \(k\) samples to client \(j\). In this case, smaller \(\beta\) denotes worse skews.

Footnote 1: [https://github.com/Xtra-Computing/NIID-Bench](https://github.com/Xtra-Computing/NIID-Bench)

Here's a brief overview of these datasets. MNIST Dataset: The MNIST dataset comprises binary images of handwritten digits. It consists of 60,000 28x28 training images and 10,000 testing images. FMNIST Dataset: Similar to MNIST, the FMNIST dataset also contains 60,000 28x28 training images and 10,000 testing images. SVHN Dataset: The SVHN dataset includes 73,257 32x32 color training images and 10,000 testing images. CIFAR-10 Dataset: CIFAR-10 consists of 60,000 32x32 color images distributed across ten classes, with each class containing 6,000 images. The input dimensions for MNIST, FMNIST, SVHN, and CIFAR-10 are 784, 784, 3,072, and 3,072, respectively.

Figure 1: t-SNE visualization for our FedLPA global model.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

FedLPA and baselines. Details of the overhead evaluation are referred to Appendix G.6 and G.7. Our observations reveal that FedLPA is slightly slower than FedNova, SCAFFOLD, FedAvg, and FedProx, while much faster than DENSE. FedLPA also has significantly improved the one-shot learning performance of the above four approaches. Similarly, FedLPA performs moderately incremental communication overhead while outperforming other baseline approaches on learning performance, as one-shot FL introduces heavy computation overhead while communication overhead is usually small. It is noteworthy that FedLPA strikes a favorable balance between computation and communication overhead, making it the most promising approach for one-shot FL.

### Extension to multiple rounds

We conduct experiments on MNIST with 10 clients and data partitioning \(p_{k}\) - Dir\((\beta=0.5)\). The results are shown in Figure 2. As DENSE could not support multiple rounds, we compare our methods with FedAvg, FedNova, SCAFFOLD, and FedProx. FedLPA achieves the highest accuracy in the first round, denoting the strongest learning capabilities in a one-shot setting. With the increment in the number of rounds, the performances of FedLPA increase slower than the other baseline approaches. This figure shows that the joint approach (ours (one round) then FedAvg) that utilizes FedLPA in the first round and then adopts other baseline methods may be most promising to save communication and computation resources in the multiple-round federated learning scenario.

### Supplementary experiments

Experiments for privacy concerns, experiments on different local epoch numbers, experiments in extreme settings (the number of clients=5, \(\beta=0.001\)), experiments with more methods, experiments with more complex network structures, experiments with more complex datasets, ablation experiments analyzing the number of approximation iterations of FedLPA can be found in Appendix.

## 5 Conclusions

In this work, we design a novel one-shot FL algorithm FedLPA to better model the global parameters in effective one-shot federated learning. We propose a method that could aggregate the local clients in a layer-wise manner with their posterior approximation via block-diagonal empirical Fisher information matrices, which could effectively capture the accurate statistics of a locally biased dataset. Overall, FedLPA stands out as the most practical and efficient framework that conducts data-free one-shot FL, particularly well-suited for high data heterogeneity in various settings, considering it significantly outperforms other baselines with extensive experiments. Our FedLPA is available in [https://github.com/lebronlambert/FedLPA_NeurIPS2024](https://github.com/lebronlambert/FedLPA_NeurIPS2024).

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline value of \(\lambda\) & 0.01 & 0.001 & 0.0001 \\ \hline \(\beta\)=0.01 & 18.63\(\pm\)0.78 & 21.20\(\pm\)0.67 & 22.50\(\pm\)1.84 \\ \(\beta\)=0.05 & 54.33\(\pm\)0.54 & 54.27\(\pm\)0.38 & 53.30\(\pm\)0.01 \\ \(\beta\)=0.10 & 56.83\(\pm\)0.19 & 53.53\(\pm\)0.06 & 54.60\(\pm\)0.15 \\ \(\beta\)=0.3 & 66.83\(\pm\)0.02 & 68.20\(\pm\)0.04 & 67.53\(\pm\)0.03 \\ \(\beta\)=0.5 & 73.20\(\pm\)0.03 & 73.33\(\pm\)0.06 & 72.17\(\pm\)0.04 \\ \(\beta\)=1.0 & 76.53\(\pm\)0.02 & 76.03\(\pm\)0.05 & 73.47\(\pm\)0.19 \\ \#C=1 & 12.73\(\pm\)0.01 & 13.20\(\pm\)0.02 & 14.17\(\pm\)0.02 \\ \#C=2 & 45.20\(\pm\)0.21 & 46.13\(\pm\)0.15 & 44.80\(\pm\)0.03 \\ \#C=3 & 58.97\(\pm\)0.07 & 57.90\(\pm\)0.06 & 55.60\(\pm\)0.06 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Experimental results of different hyper-parameter \(\lambda\) on FMNIST dataset.

\begin{table}
\begin{tabular}{c|c|c} \hline \hline  & Overall & Overall \\  & Computation (minus) & Communication (MB) \\ \hline FedLPA & 65 & 4.98 \\ FedNova & 50 & 2.47 \\ SCAFFOLD & 50 & 4.94 \\ FedAvg & 50 & 2.47 \\ FedProx & 75 & 2.47 \\ DENSE & 400 & 2.47 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Communication and computation overhead evaluation.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers. The authors would like to thank Yiqun Diao from National Univeristy of Singapore for his valuable comments to improve this work. Dr. Jialin Li is supported by the Singapore Ministry of Education Academic Research Fund Tier 1 (T1 251RES2104) and Tier 2 (MOE-T2EP20222-0016).

## References

* [1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. _nature_, 521(7553):436-444, 2015.
* [2] Jurgen Schmidhuber. Deep learning in neural networks: An overview. _Neural networks_, 61:85-117, 2015.
* [3] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In _International conference on machine learning_, pages 173-182. PMLR, 2016.
* [4] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 60(6):84-90, 2017.
* [5] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, and Sundaraja S Iyengar. A survey on deep learning: Algorithms, techniques, and applications. _ACM Computing Surveys (CSUR)_, 51(5):1-36, 2018.
* [6] Qingchen Zhang, Laurence T Yang, Zhikui Chen, and Peng Li. A survey on deep learning for big data. _Information Fusion_, 42:146-157, 2018.
* [7] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, and Sundaraja S Iyengar. A survey on deep learning: Algorithms, techniques, and applications. _ACM Computing Surveys (CSUR)_, 51(5):1-36, 2018.
* [8] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 10(2):1-19, 2019.
* [9] H Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Federated learning of deep networks using model averaging. _arXiv preprint arXiv:1602.05629_, 2, 2016.
* [10] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [11] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: Vision, hype and reality for data privacy and protection. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [12] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* [13] Neel Guha, Ameet Talwalkar, and Virginia Smith. One-shot federated learning. _arXiv preprint arXiv:1902.11175_, 2019.
* [14] Qinbin Li, Bingsheng He, and Dawn Song. Practical one-shot federated learning for cross-silo setting. _arXiv preprint arXiv:2010.01017_, 2020.
* [15] Yiqun Diao, Qinbin Li, and Bingsheng He. Towards addressing label skews in one-shot federated learning. In _The Eleventh International Conference on Learning Representations_, 2023.

* [16] Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated learning via posterior averaging: A new perspective and practical algorithms. _arXiv preprint arXiv:2010.05273_, 2020.
* [17] Liangxi Liu, Xi Jiang, Feng Zheng, Hong Chen, Guo-Jun Qi, Heng Huang, and Ling Shao. A bayesian federated learning framework with online laplace approximation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* [18] Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. Distilled one-shot federated learning. _arXiv preprint arXiv:2009.07999_, 2020.
* [19] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Jianghe Xu, Shouhong Ding, and Chao Wu. A practical data-free approach to one-shot federated learning with heterogeneity. _arXiv preprint arXiv:2112.12371_, 1, 2021.
* [20] Mingzhao Yang, Shangchao Su, Bin Li, and Xiangyang Xue. Exploring one-shot semi-supervised federated learning with a pre-trained diffusion model. _arXiv preprint arXiv:2305.04063_, 2023.
* [21] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In _International conference on machine learning_, pages 12878-12889. PMLR, 2021.
* [22] MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. _arXiv preprint arXiv:2006.05148_, 2020.
* [23] Clare Elizabeth Heinbaugh, Emilio Luz-Ricca, and Huajie Shao. Data-free one-shot federated learning under very high statistical heterogeneity. In _The Eleventh International Conference on Learning Representations_, 2022.
* [24] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. Dense: Data-free one-shot federated learning. _Advances in Neural Information Processing Systems_, 35:21414-21428, 2022.
* [25] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. _Advances in neural information processing systems_, 30, 2017.
* [26] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. _Advances in Neural Information Processing Systems_, 33:2351-2363, 2020.
* [27] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. _arXiv preprint arXiv:1806.00582_, 2018.
* [28] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. _arXiv preprint arXiv:1907.02189_, 2019.
* [29] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* [30] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. _Advances in neural information processing systems_, 33:7611-7623, 2020.
* [31] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine Learning and Systems_, 2:429-450, 2020.
* [32] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In _International Conference on Machine Learning_, pages 7252-7261. PMLR, 2019.

* [33] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. _arXiv preprint arXiv:2002.06440_, 2020.
* [34] Romain Thibaux and Michael I Jordan. Hierarchical beta processes and the indian buffet process. In _Artificial intelligence and statistics_, pages 564-571. PMLR, 2007.
* [35] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. In _Proceedings of the IEEE international conference on computer vision_, pages 3514-3522, 2019.
* [36] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 8715-8724, 2020.
* [37] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hypernetworks. In _International Conference on Machine Learning_, pages 9489-9502. PMLR, 2021.
* [38] David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron Courville. Bayesian hypernetworks. _arXiv preprint arXiv:1710.04759_, 2017.
* [39] Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, and Bo Han. Enhancing one-shot federated learning through data and ensemble co-boosting. _arXiv preprint arXiv:2402.15070_, 2024.
* [40] Shangchao Su, Bin Li, and Xiangyang Xue. One-shot federated learning without server-side training. _Neural Networks_, 164:203-215, 2023.
* [41] Divyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. Fedfisher: Leveraging fisher information for one-shot federated learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1612-1620. PMLR, 2024.
* [42] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. _IEEE Signal Processing Magazine_, 37(3):50-60, 2020.
* [43] Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks. In _6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings_, volume 6. International Conference on Representation Learning, 2018.
* [44] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace redux-effortless bayesian deep learning. _Advances in Neural Information Processing Systems_, 34:20089-20103, 2021.
* [45] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. _nature_, 323(6088):533-536, 1986.
* [46] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In _International conference on machine learning_, pages 2408-2417. PMLR, 2015.
* [47] Charles F Van Loan. The ubiquitous kronecker product. _Journal of computational and applied mathematics_, 123(1-2):85-100, 2000.
* [48] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In _International conference on machine learning_, pages 2408-2417. PMLR, 2015.
* [49] Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In _International Conference on Machine Learning_, pages 573-582. PMLR, 2016.

* [50] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* [51] Michael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. _Advances in Neural Information Processing Systems_, 35:17703-17716, 2022.
* [52] Frederik Benzing. Gradient descent on neurons and its link to approximate second-order optimization. In _International Conference on Machine Learning_, pages 1817-1853. PMLR, 2022.
* [53] Lin Zhang, Shaohuai Shi, Wei Wang, and Bo Li. Scalable k-fac training for deep neural networks with distributed preconditioning. _IEEE Transactions on Cloud Computing_, 2022.
* [54] James Martens. _Second-order optimization for neural networks_. University of Toronto (Canada), 2016.
* [55] Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep learning. In _International Conference on Machine Learning_, pages 557-565. PMLR, 2017.
* [56] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer, 2006.
* [57] Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and S Yu Philip. Privacy and robustness in federated learning: Attacks and defenses. _IEEE transactions on neural networks and learning systems_, 2022.
* [58] Andrew C Yao. Protocols for secure computations. In _23rd annual symposium on foundations of computer science (sfcs 1982)_, pages 160-164. IEEE, 1982.
* [59] Daniel Demmler, Thomas Schneider, and Michael Zohner. Aby-a framework for efficient mixed-protocol secure two-party computation. In _Network and Distributed System Security Symposium_, 2015.
* [60] Taher ElGamal. A public key cryptosystem and a signature scheme based on discrete logarithms. _IEEE transactions on information theory_, 31(4):469-472, 1985.
* [61] Pascal Paillier. Public-key cryptosystems based on composite degree residuosity classes. In _International conference on the theory and applications of cryptographic techniques_, pages 223-238. Springer, 1999.
* [62] Craig Gentry. Fully homomorphic encryption using ideal lattices. In _Proceedings of the forty-first annual ACM symposium on Theory of computing_, pages 169-178, 2009.
* [63] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [64] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [65] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [66] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
* [67] Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. A principled approach to data valuation for federated learning. _Federated Learning: Privacy and Incentive_, pages 153-167, 2020.

* [68] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. In _2022 IEEE 38th International Conference on Data Engineering (ICDE)_, pages 965-978. IEEE, 2022.
* [69] Hong-You Chen and Wei-Lun Chao. Fedbe: Making bayesian model ensemble applicable to federated learning. _arXiv preprint arXiv:2009.01974_, 2020.
* [70] Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In _International Conference on Machine Learning_, pages 573-582. PMLR, 2016.
* [71] Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep learning. In _International Conference on Machine Learning_, pages 557-565. PMLR, 2017.
* [72] Elaine Angelino, Matthew James Johnson, Ryan P Adams, et al. Patterns of scalable bayesian inference. _Foundations and Trends(r) in Machine Learning_, 9(2-3):119-247, 2016.
* [73] Ward Cheney and David Kincaid. _Numerical mathematics and computing_. International Thomson Publishing, 1998.
* [74] David A Belsley, Edwin Kuh, and Roy E Welsch. _Regression diagnostics: Identifying influential data and sources of collinearity_. John Wiley & Sons, 2005.
* [75] M Hashem Pesaran. _Time series and panel data econometrics_. Oxford University Press, 2015.
* [76] Lloyd N Trefethen and David Bau. _Numerical linear algebra_. SIAM, 2022.
* [77] Jongseok Lee, Matthias Humt, Jianxiang Feng, and Rudolph Triebel. Estimating model uncertainty of neural networks in sparse information form. In _International Conference on Machine Learning_, pages 5702-5713. PMLR, 2020.
* [78] Thomas George, Cesar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in a kronecker factored eigenbasis. _Advances in Neural Information Processing Systems_, 31, 2018.
* [79] Jiankai Sun, Xin Yang, Yuanshun Yao, and Chong Wang. Label leakage and protection from forward embedding in vertical federated learning. _arXiv preprint arXiv:2203.01451_, 2022.
* [80] Aidmar Wainakh, Fabrizio Ventola, Till Mussig, Jens Keim, Carlos Garcia Cordero, Ephraim Zimmer, Tim Grube, Kristian Kersting, and Max Muhlhauser. User-level label leakage from gradients in federated learning. _Proceedings on Privacy Enhancing Technologies_, 2022(2):227-244, 2022.
* [81] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended feature leakage in collaborative learning. In _2019 IEEE symposium on security and privacy (SP)_, pages 691-706. IEEE, 2019.
* [82] Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients-how easy is it to break privacy in federated learning? _Advances in Neural Information Processing Systems_, 33:16937-16947, 2020.
* [83] Cynthia Dwork. Differential privacy. _Encyclopedia of Cryptography and Security_, pages 338-340, 2011.
* [84] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* [85] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level perspective. _arXiv preprint arXiv:1712.07557_, 2017.
* [86] Yiqun Diao, Qinbin Li, and Bingsheng He. Exploiting label skews in federated learning with model concatenation. _arXiv preprint arXiv:2312.06290_, 2023.

* [87] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In _Proceedings of the 22nd ACM SIGSAC conference on computer and communications security_, pages 1322-1333, 2015.
* [88] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under the gan: information leakage from collaborative deep learning. In _Proceedings of the 2017 ACM SIGSAC conference on computer and communications security_, pages 603-618, 2017.
* [89] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_, pages 650-669. IEEE, 2015.
* [90] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. Knock knock, who's there? membership inference on aggregate location data. _arXiv preprint arXiv:1708.06145_, 2017.
* [91] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2017.
* [92] Jamie Hayes, Luca Melis, George Danezis, and E LOGAN De Cristofaro. Membership inference attacks against generative models. _URL https://api. semanticscholar. org/CorpusID_, 202588705, 2018.
* [93] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and Kai Chen. Understanding membership inferences on well-generalized learning models. _arXiv preprint arXiv:1802.04889_, 2018.
* [94] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. Towards demystifying membership inference attacks. _arXiv preprint arXiv:1807.09173_, 2018.
* [95] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [96] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [97] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In _2017 international joint conference on neural networks (IJCNN)_, pages 2921-2926. IEEE, 2017.
* [98] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015.
* [99] Frederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical fisher approximation for natural gradient descent. _Advances in neural information processing systems_, 32, 2019.
* [100] James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In _International Conference on Learning Representations_, 2018.
* [101] Blair Bilodeau, Yanbo Tang, and Alex Stringer. On the tightness of the laplace approximation for statistical inference. _Statistics & Probability Letters_, 198:109839, 2023.

The FedLPA algorithm

The proposed algorithm follows the same paradigm as the standard one-shot federated learning framework. Each client follows the local training procedure as shown in the paper. The global aggregation is illustrated in Algorithm 1.

With the Algorithms, let us assume the dimensionality list of each layer in a fully connected neural network is ([\(s_{0}\), \(s_{1}\), \(s_{2}\),..., \(s_{l}\),..., \(s_{L}\)]), which means the size of the weight \(\mathbf{W}_{k_{l}}\) of layer \(l\) is \(s_{l-1}\)x\(s_{l}\). Consequently, the size of \(\mathbf{A}_{k_{l}}\) for this layer would be \(s_{l-1}\)x\(s_{l-1}\), and the size of \(\mathbf{B}_{k_{l}}\) would be \(s_{l}\)x\(s_{l}\). The size of \(\mathbf{F}_{k_{l}}\) is \((s_{l-1}\)x\(s_{l})\)x(\(s_{l-1}\)x\(s_{l}\)).

Then, we give a concrete example to show the dimensions of different matrices using a fully-connected neural network model with architecture 784-256-64-10 as in Appendix G. Then, the \(\mathbf{M}_{k_{1}}\) is 784x256+256, \(\mathbf{M}_{k_{2}}\) is 256x64+64, \(\mathbf{M}_{k_{3}}\) is 64x10+10. The \(\mathbf{A}_{k_{1}}\) is 785x785, \(\mathbf{A}_{k_{2}}\) is 257x257, \(\mathbf{A}_{k_{3}}\) is 65x65. The \(\mathbf{B}_{k_{1}}\) is 256x256, \(\mathbf{B}_{k_{2}}\) is 64x64, \(\mathbf{B}_{k_{3}}\) is 10x10. Then the \(\mathbf{F}_{k_{1}}\) is (785x785)x(256x256), \(\mathbf{F}_{k_{2}}\) is (257x257)x(64x64), \(\mathbf{F}_{k_{3}}\) is (65x65)x(10x10). The \(\mathbf{F}_{k}\) is (785x785+257x257+65x65)x(256x256+64x64+10x10).

However, in fact, we do not need to combine the \(\mathbf{A}_{k_{l}}\), \(\mathbf{B}_{k_{l}}\), \(\mathbf{F}_{k_{l}}\) into \(\mathbf{A}_{k}\), \(\mathbf{B}_{k}\), \(\mathbf{F}_{k}\). In this paper, we utilize the diagonal block property to compute each block in our method.

## Appendix B Comparison with the previous methods

To the best of our knowledge, we are the first to consider the posterior inference problem in the one-shot scenario. Note that the approach [16] requires a lengthy burn-in period before conducting posterior inference, for instance, 400 rounds, and it updates global model parameters by modifying the covariance-aggregated local models. It means that the algorithm [16] necessarily requires multiple iterations and cannot be used in a one-shot scenario. In contrast, our method FedLPA only requires immediate variational inference after training the local model, ensuring higher flexibility and efficiency in the one-shot scenario.

Besides, in the algorithm [16], obtaining statistical information to compute local covariances is of low rank. In reality, it fails to acquire the posterior of the aggregated model and cannot perform variational inference on the aggregated model. However, our method yields full-rank covariances, and after employing an expectation approximation method for variational inference on the aggregated model, we can achieve a usable global posterior.

In both the domain of natural gradient optimization [48; 70; 16] and modeling output uncertainty in variational inference [71], using the Fisher approximation of the Hessian does not involve the issue of inverting covariance. However, in the context of federated learning, when performing variational inference on the aggregated model, the necessity of inverting covariance becomes unavoidable. To address this problem, we propose a novel algorithm that constructs a quadratic objective function. During aggregation, this algorithm directly trains the aggregated model using local covariances and expectations, thereby circumventing the need for inversion operations.

Note that the previous methods [51; 17] adopt the same core approach that utilizes the online Laplace approximation to obtain diagonal Fisher for model aggregation, in which they conduct experiments on different datasets and published on different venues. We mainly analyze our approach with the comparison of DiagonalFisher [17]. DiagonalFisher assumes independence among parameters, neglecting inter-parameter correlations, resulting in inaccurate posterior approximations. However, strong correlations exist among parameters within each layer, such as matching patterns in convolutional kernels within convolutional networks. This is a crucial factor that cannot be overlooked; otherwise, aggregation of the posterior would result in lower posterior regions, as compared to our method. In complex environments, employing diagonal Fisher for aggregation would prove to be entirely ineffective, whereas our method effectively leverages inter-parameter correlations at each layer, rendering it more robust. To demonstrate, we present results comparing aggregation using diagonal Fisher and our method. We have added experiments using the settings of our paper and an MLP model (784-256-64-10) on the FMNIST dataset with five random seeds for one-shot FL, the client number is 10, and the \(\beta\)=0.01. The results are in Table 6.

In the table, "Initial" denotes whether the client models were initialized using the same parameter values or independently.

When "Initial" is set to "Same", all client models are trained on their respective datasets using identical parameter values for initialization. Consequently, there exists a strong correlation among the local models. Additionally, in this scenario, model aggregation is equivalent to aggregating updates of local models. Although DiagonalFisher performs reasonably well under this condition, our method demonstrates superior performance, exhibiting a 20.29% increase in global test accuracy.

When "Initial" is set to "Different", the models on different clients start training with distinct parameter values. Due to the high heterogeneity of local datasets, there is minimal correlation among local models. In this extreme scenario, DiagonalFisher completely fails, while our method maintains an accuracy of 73.73%, showcasing remarkable robustness.

It is essential to consider the indispensability of parameter correlations, which is why we compute correlations among parameters within layers to ensure the robustness and accuracy of model aggregation.

Now, we discuss some related works which directly utilize K-FAC to approximate the Fisher matrix and make a comparison with our proposed approach FedLPA. The works [48, 70, 71] have provided us with significant inspiration. However, methods like K-FAC do not require computing the inverse of covariance. Nevertheless, in the context of federated learning, the necessity of inverting covariance becomes unavoidable during variational inference on the aggregated model.

Methods like K-FAC assume direct independence among data samples to utilize expectation approximation. They obtain the inverse of Fisher from individual samples and then directly compute the expectation, thereby avoiding inverse operations. However, the expectation approximation inevitably leads to biased results during model aggregation. Detailed analysis can be found in Appendix D.

To address this issue, we propose a novel algorithm that constructs a quadratic objective function. During aggregation, this algorithm directly trains the aggregated model using local covariances and expectations, eliminating the need for inversion operations. This aims to minimize aggregation biases as much as possible.

Here, we provide a comparative analysis of different methods.

FedAvg and FedProx minimize the Kullback-Leibler (KL) divergence between the local and global posteriors: \(\bar{\mathbf{\mu}},\bar{\mathbf{\Sigma}}^{-1}=\min_{\bar{\mathbf{\mu}},\bar{\mathbf{\Sigma}}^{ -1}}KL\left((\sum_{k}^{K}p(\mathbf{\theta}|\mathcal{D}_{k}))|p(\mathbf{\theta}| \mathcal{D})\right)\). SCAFFOLD computes the bias term, and FedNova computes the correction term. None of these four methods consider the correlations between parameters. DENSE and FedOV, on the other hand, employ distillation methods, attempting to extract the distribution of non-iid data among clients through distillation. However, this itself leads to information loss due to dimensionality reduction and introduces additional variance of data.

Although the work [72] also uses the distributed Bayesian inference, however, it focuses on the dataset feature and could not be applied to train the global model parameters.

In conclusion, the reason our approach performs better in this scenario stems from our improved approximation of the global posterior. This approach signifies our novelty in addressing these challenges.

### The efficiency of FedLPA

Although the number of uploaded bits increased per round of FedLPA, it resulted in a significant improvement in the final outcome. Additionally, the increase in transmitted bits enhanced the robustness of the aggregation method. Moreover, as indicated in Table 5 of the paper, we observe only a marginal increase in the amount of communication required.

A fully-connected neural network model with architecture 784-256-64-10, has \(784\cdot 256+256+256\cdot 64+64+64\cdot 10+10=217930\) floating point numbers, which is 6973760 bits or around 0.831

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline Initial & FedAvg & FedProx & SCAFFOLD & DiagonalFisher & FedLPA \\ \hline Same & 42.35\(\pm\)0.16 & 24.80\(\pm\)0.10 & 42.10\(\pm\)0.15 & 56.34\(\pm\)0.34 & 76.63\(\pm\)0.04 \\ Different & 10.00\(\pm\)0.00 & 24.12\(\pm\)0.02 & 10.16\(\pm\)0.70 & 10.51\(\pm\)0.11 & 73.73\(\pm\)0.07 \\ \hline \end{tabular}
\end{table}
Table 6: Experiments with DiagonalFisher using MLP.

MB. For one communication from a client to the server, our approach needs to upload additional \(\mathbf{A}_{k}\) and \(\mathbf{B}_{k}\), which have \(785\cdot 785+256\cdot 257\cdot 257+64\cdot 64+65\cdot 65+10\cdot 10=756231\) floating point numbers. Note, \(\mathbf{A}_{k}\) and \(\mathbf{B}_{k}\) are symmetric matrices, so we only need to upload the upper triangular part of \(\mathbf{A}_{k}\) and \(\mathbf{B}_{k}\), which is around 756231/2 = 378115.5 floating point numbers and 1.442 MB. Therefore, our approach costs 2.272 MB for the directed communication, which is only 1.367 times than DiagonalFisher while DiagonalFisher costs 0.831*2 = 1.662 MB. We show the following Table 7 based on the previous experiment results. When "Initial" is set to "Same", the efficiency of every bit is almost the same. When "Initial" is set to "Different", the efficiency of every bit for our method is much higher than the DiagonalFisher.

## Appendix C Detailed discussion for the time complexity of Eq. 12

A fully-connected neural network model with architecture 784-256-64-10 as an example is shown in Appendix K. We use this example to further explain this question. The size of \(\mathbf{A}_{k_{1}}\) is 785x785 and the size of \(\mathbf{B}_{k_{1}}\) are both 256x256. Then, we need to compute the inverse of the matrix (785x785)x(256x256), which is huge. The time complexity of calculating the inverse of a matrix is \(O(n^{3})\) (n is the dimension of the matrix), which is very slow. The accuracy of calculating it is decided by the condition number of the huge matrix [73, 74, 75, 76]. That's why calculating Eq. 12 is unacceptable, considering the time complexity, the size of the huge matrix and the accuracy.

Further, for example, in the machine learning field, to accelerate the training of the neural network, they use the Newton method. However, using this method, they need to compute the inverse of the Hessian matrix, which is also huge and unacceptable. That is why they introduce the KFAC [46, 49], KFRA [55] and KFLR [55] methods to avoid computing the inverse of the huge Hessian matrix.

In this paper, we avoid computing the inverse of the huge matrix via our method, and the time complexity is linear.

## Appendix D Expectation approximation (EA)

Previous works [46, 49] approximate the expectation of Kronecker products by a Kronecker product of expectations \(\mathbb{E}\left[\mathbf{A}\otimes\mathbf{B}\right]\approx\mathbb{E}\left[ \mathbf{A}\right]\otimes\mathbb{E}\left[\mathbf{B}\right]\) with an assumption of \(\mathbf{A}_{k_{l}}\) and \(\mathbf{B}_{k_{l}}\) are independent, which is called Expectation Approximation (EA).

It is a simple and effective method to approximate the expectation of Kronecker products. As a result, the global co-variance \(\bar{\mathbf{\Sigma}}\) is approximated by:

\[\bar{\mathbf{\Sigma}}_{l}\approx(\sum_{k}^{K}\mathbf{A}_{k_{l}})^{-1}\otimes( \sum_{k}^{K}\mathbf{B}_{k_{l}})^{-1}=\bar{\mathbf{A}}_{l}^{-1}\otimes\bar{ \mathbf{B}}_{l}^{-1} \tag{15}\]

where \(\bar{\mathbf{A}}_{l}=\sum_{k}^{K}\mathbf{A}_{k_{l}}\) and \(\bar{\mathbf{B}}_{l}=\sum_{k}^{K}\mathbf{B}_{k_{l}}\). Denoting \(\bar{\mathbf{Z}}_{l}\) as matrix formula of \(\bar{\mathbf{z}}_{l}=\mathrm{vec}(\bar{\mathbf{Z}}_{l})\), then \(\bar{\mathbf{\mu}}_{l}\) can be computed efficiently as below:

\[\bar{\mathbf{\mu}}_{l}=\bar{\mathbf{\Sigma}}_{l}\cdot\bar{\mathbf{z}}_{l} \approx(\bar{\mathbf{A}}_{l}^{-1}\otimes\bar{\mathbf{B}}_{l}^{-1})\bar{ \mathbf{z}}_{l}=\mathrm{vec}(\bar{\mathbf{B}}_{l}^{-1}\bar{\mathbf{Z}}_{l} \bar{\mathbf{A}}_{l}^{-1}) \tag{16}\]

However, Eq. 16 leads to a biased global expectation. The EA needs the independence assumption, but \(\mathbf{A}_{k_{l}}\) and \(\mathbf{B}_{k_{l}}\) are weakly related in back-propagation. Besides, even if they are independent, Eq. 16 still suffers from approximation error because the clients' number \(K\) is finite and always a small number but statistical independence can only be demonstrated when the sampling number is large enough. Eq. 17 shows the approximation error directly:

\[\begin{split}(\mathbf{A}_{1}+\mathbf{A}_{2})\otimes(\mathbf{B}_{ 1}+\mathbf{B}_{2})&=\mathbf{A}_{1}\otimes\mathbf{B}_{1}+\mathbf{ A}_{2}\otimes\mathbf{B}_{2}\\ &\qquad\qquad\qquad+\mathbf{A}_{1}\otimes\mathbf{B}_{2}+\mathbf{ A}_{2}\otimes\mathbf{B}_{1}\\ &\neq\mathbf{A}_{1}\otimes\mathbf{B}_{1}+\mathbf{A}_{2}\otimes \mathbf{B}_{2}\end{split} \tag{17}\]

\begin{table}
\begin{tabular}{c|c|c} \hline “Initial” Method & FedLPA (Global Test Acc / MB) & DiagonalFisher (Global Test Acc / MB) \\ \hline “Same” & 76.63/(2.272*10) = 3.37 & 56.34/(1.662*10) = 3.39 \\ \hline “Different” & 73.73/(2.272*10) = 3.25 & 10.51/(1.662*10) = 0.63 \\ \hline \end{tabular}
\end{table}
Table 7: Experiments with DiagonalFisher considering efficiency.

Extend FedLPA to the models with enormous single layer weight parameters

This implies a Fisher matrix with a large dimension and it significantly increases communication costs. In such cases, the most intuitive approach is to explore the possibility of dimensionality reduction for its Fisher matrix. A promising approach to enhance the efficiency of our method may employ some low-rank factorization techniques [77]. As described [44], the main idea involves performing an eigendecomposition on the Kronecker factors [78], while preserving only the eigenvectors corresponding to the top k largest eigenvalues. As a result, this approach drastically reduces space complexity, enabling communication costs to be compared favorably with diagonal Fisher matrices.

## Appendix F Privacy discussion of FedLPA

In the FedLPA, \(\mathbf{A}_{k}\) is computed via the activations while \(\mathbf{B}_{k}\) is computed via the linear pre-activations of the layer. We note that \(\mathbf{A}_{k}\), \(\mathbf{B}_{k}\), and \(\mathbf{M}_{k}\) do not carry any label information, thus the transmission of \(\mathbf{A}_{k}\), \(\mathbf{B}_{k}\), and \(\mathbf{M}_{k}\) will not leak any label privacy. As a comparison, FedCAVE, which transmits client label information to the server, requires training in label distribution to do the distillation. Several papers [79; 80] have notified that label privacy, e.g., the concern of label distribution leakage and raw label leakage, is sensitive in federated learning. We believe that it has also been a concern in the one-shot FL scenario.

Besides, our t-SNE illustration in Fig 1 shows the classification capability on the global model, which can separate the classes. However, our figures of the t-SNE illustrations on local models in Appendix G.1 show that for the data belonging to the same class, their t-SNE illustrations are erratically distributed on different local nodes. For instance, for node 2, its training data only has 3 classes while most of the training data locates in class 5. However, it is hard for the server to infer that label distribution since the t-SNE illustration both on node 2 and other nodes also seems irregular.

\(\mathbf{A}_{k}\), \(\mathbf{B}_{k}\), and \(\mathbf{M}_{k}\) are a function of data that may contain privacy-sensitive information of the local training data. However, in this case, our privacy-preserving level is similar to FedAvg, which means that FedLPA has the same privacy-preserving level as the conventional federated learning algorithms (i.e, FedAvg, FedProx, FedNova, and Dense), which are all vulnerable to some privacy attacks (e.g, membership inference [81] or reconstruction attacks [82]). Our approach FedLPA provides more information than FedAvg, However, the additional information we provide is the mean of each sample in each dimension, the mean of squares of each sample in each dimension, and the mean of square gradients. These solely marginally enrich the attack capability of several reconstruction attacks.

FedLPA is intuitively compatible with existing privacy-preserving techniques, such as differential privacy (DP) [56; 57], secure multiparty computation (SMC) [58; 59], and homomorphic encryption (HE) [60; 61; 62]. In Appendix F.1, we propose a naive DP-FedLPA with two different mechanisms to show the compatibility with differential privacy. In Appendix F.2, using iDLG attack [82], we show that our proposed FedLPA has the same privacy-preserving level as the conventional federated learning algorithms (i.e, FedAvg, FedProx, FedNova and Dense). Compared with FedAvg, we have conducted a detailed analysis from a privacy attack perspective to show that our proposed FedLAP exhibits a security level consistent with FedAvg against several types of privacy attacks, where the details are shown in Appendix F.3. Note that the main focus of FedLPA is to improve the learning performance on the one-shot FL settings, thus, we leave the integration with other privacy-preserving techniques beyond DP as an open problem.

### Experiments with differential privacy

We first list the definitions and techniques for differential privacy [83]. (\(\epsilon\)-DP) For \(\epsilon>0\), a randomized function \(f\) provides \(\epsilon\)-differential privacy if, for any datasets \(D,D^{\prime}\) that have only one single record different, for any possible output \(O\),

\[Pr[f(D)\in O]\leq e^{\epsilon}\cdot Pr[f(D^{\prime})\in O] \tag{18}\]

Suppose \(f\) is a function and \(D,D^{\prime}\) have only one record different. The sensitivity of \(f\) is defined as

\[\Delta f=\max_{D,D^{\prime}}\|f(D)-f(D^{\prime})\|_{1} \tag{19}\]Here one record different means a database has one more record than another. We utilize the Laplace mechanism [84] to achieve the \(\epsilon-\)DP.

Laplace Mechanism: For function \(f:\mathcal{D}\to R^{d}\), function:

\[F(D)=f(D)+Lap(0,\Delta f/\epsilon) \tag{20}\]

provides \(\epsilon\)-DP, where \(Lap(0,\Delta f/\epsilon)\) is sampled from Laplace distribution.

Following the differential privacy (DP) mechanisms [56; 85; 57; 86] to protect privacy, we conduct the two mechanisms of DP-FedLPA: (1) adding Laplace random noise to the training data samples, (2) adding Laplace random noise to the parameters to be transmitted. DP is a rigorous and popular privacy metric, which guarantees that the output does not change with a high probability even though an input data record changes. Specifically, since the sensitivity of the data sample distribution after the normalization is 1, we add Laplacian noises with \(\lambda=\frac{1}{\epsilon}\). We set \(\epsilon=\{3,5,8\}\) that provides modest privacy guarantees since normally \(\epsilon\in(1,10)\) is viewed as a suitable choice. We have added the experiments using the same experiment setting in the paper with five random seeds and 10 clients on the FMNIST dataset. Results are shown in Table 8. DP-FedLPA under both mechanisms outperforms FedAvg, which shows that it is compatible with combining our proposed FedLPA with DP to enhance privacy protection levels. Note that the smaller \(\epsilon\) is, the larger noises we add. We find that when the \(\epsilon\) gets smaller, the performance drops simultaneously, while the privacy protection level is increased.

Besides, we have added the experiments using the same experiment setting to show the round results of how many rounds DP-FedAvg needs to achieve the same test performance with the first mechanism. The results in Table 9 show that DP-FedAvg needs about 10 rounds of communication to achieve the same test performance, compared to our one-round FedLPA. Combined with our previous results in Table 5 and Table 8, our FedLPA could save the communication and computation overhead and combine with the DP method to mitigate the potential privacy leakage. Based on the above settings, DP-FedAvg needs at least 3x communication overhead and 5x computation overhead. While DP-FedAvg needs multiple rounds to get similar accuracy, DP-FedAvg maybe vulnerable to more privacy attack methods due to the multiple queries, such as curvature-based privacy attacks.

### Experiments with iDLG attack

We also add experiments with iDLG attack [82] following the link ([https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients/blob/master/iDLG.py](https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients/blob/master/iDLG.py)).

We did the experiments with the setting of the paper [82]: in each single experiment, the client is trained with one random picked image in FMNIST, then we use the iDLG attack to recover the image based on the model from FedAvg and FedLPA. We randomly selected 500 training examples to collect 500 MSEs between the recovered and the original image. The larger the MSE is, the better

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \(\epsilon\) & Partitions & FedAvg & DP-FedLPA (mechanism 1) & DP-FedLPA (mechanism 2) \\ \hline \multirow{3}{*}{8} & \(\beta\)=0.1 & 31.90\(\pm\)0.58 & 50.01\(\pm\)0.07 & 57.15\(\pm\)1.23 \\  & \(\beta\)=0.3 & 44.37\(\pm\)0.05 & 68.30\(\pm\)0.41 & 66.21\(\pm\)0.14 \\  & \(\beta\)=0.5 & 57.92\(\pm\)0.63 & 71.17\(\pm\)0.27 & 73.50\(\pm\)0.06 \\ \hline \multirow{3}{*}{5} & \(\beta\)=0.1 & 28.17\(\pm\)0.16 & 48.51\(\pm\)0.07 & 55.87\(\pm\) 0.88 \\  & \(\beta\)=0.3 & 43.91\(\pm\)0.05 & 67.34\(\pm\)0.92 & 66.02\(\pm\)0.71 \\  & \(\beta\)=0.5 & 57.14\(\pm\)0.63 & 70.89\(\pm\)0.80 & 73.44\(\pm\)0.20 \\ \hline \multirow{3}{*}{3} & \(\beta\)=0.1 & 27.85\(\pm\)0.79 & 48.39\(\pm\)0.07 & 54.31\(\pm\)0.44 \\  & \(\beta\)=0.3 & 42.80\(\pm\)0.05 & 65.08\(\pm\)0.45 & 65.22\(\pm\)0.46 \\ \cline{1-1}  & \(\beta\)=0.5 & 54.80\(\pm\)0.63 & 70.28\(\pm\)1.30 & 72.19\(\pm\)0.62 \\ \hline \end{tabular}
\end{table}
Table 8: Experiments with Differential Privacy using two mechanisms.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \(\beta\)\(\epsilon\) & 8 & 5 & 3 \\ \hline
0.1 & 11 & 11 & 12 \\
0.3 & 11 & 9 & 8 \\
0.5 & 8 & 8 & 7 \\ \hline \end{tabular}
\end{table}
Table 9: Experiments with Differential Privacy for Round Numbers.

the privacy-preserving level for the method. Due to the rebuttal limitation, we cannot show the figure for the cumulative distribution function considering the MSE of the iDLG attack. We provide the results in Table 10 to show MSE considering the percentile for these 500 experiments.

Based on the Table, we could see that from 12.5 to 50.0 percentile, regarding the privacy-preserving aspect, FedLPA behaves better than FedAvg on these samples. However, from 50.0 to 87.5 percentile, FedAvg behaves better than FedLPA on such samples. Thus, no clear evidence exists of which one performs better when referring to the privacy level. Considering the overall 500 data samples, we roughly concluded that FedLPA and FedAvg share a similar privacy level.

### Concrete examples of privacy attack

For privacy attacks, we start by assuming the simplest scenario where each client has only one sample, and the model comprises a single layer, such as a multi-layer perceptron.

Let \(\mathbf{y}=\mathbf{W}*\mathbf{x}\), (\(\mathbf{x}\) is \(n+1\) dimensional, with the last dimension being a unit value 1), \(\mathbf{g}=Df(\mathbf{y})/D\mathbf{x}\) (where \(f\) is the loss function). In this case, \(\mathbf{A}=\mathbf{x}\mathbf{x}^{T},\mathbf{B}=\mathbf{g}\mathbf{g}^{T}\).

In this single-sample scenario, an attacker can directly obtain \(\mathbf{x}\) from the last column of \(\mathbf{A}\). With \(\mathbf{x}\) and \(\mathbf{W}\), the attacker can acquire the model's output. Furthermore, utilizing the Loss and \(\mathbf{g}\), it's possible to get the label information.

FedAvg would also be vulnerable to a reconstruction attack in this scenario, allowing the attacker to obtain sample and label information.

When each client has two samples \((\mathbf{x}_{1}\mathbf{x}_{2}\in Dataset)\), then: \(\mathbf{A}=1/2*\mathbf{x}_{1}*\mathbf{x}_{1}^{T}+1/2*\mathbf{x}_{2}*\mathbf{x }_{2}^{T},\mathbf{B}=1/2*\mathbf{g}_{1}*\mathbf{g}_{1}^{T}+1/2*\mathbf{g}_{2}* \mathbf{g}_{2}^{T}\). The last column \(\mathbf{c}\) of \(\mathbf{A}\) equals \(1/2\mathbf{x}_{1}+1/2\mathbf{x}_{2}\). The diagonal elements \(\mathbf{d}\) of \(\mathbf{A}\) equal \(1/2\mathbf{x}_{1}^{2}+1/2\mathbf{x}_{2}^{2}\). In the case of these two samples, an attacker can utilize the information from \(\mathbf{A}\) and \(\mathbf{B}\) to get the two samples \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\). Using the same methodology, they can also obtain \(\mathbf{g}_{1}\) and \(\mathbf{g}_{2}\). Consequently, the attacker can reverse-engineer the labels as well.

FedAvg could also potentially succumb to a reconstruction attack in this scenario, providing the attacker with sample and label information, although the obtained information might be more ambiguous.

When each client has three or more samples (\(\mathbf{x}\in Dataset\)), \(\mathbf{A}=\mathbb{E}_{\mathbf{x}\in Dataset}(\mathbf{x}*\mathbf{x}^{T}), \mathbf{B}=\mathbb{E}_{\mathbf{x}\in Dataset}(\mathbf{g}*\mathbf{g}^{T})\). In this situation, the last column \(\mathbf{c}\) of \(\mathbf{A}\), \(\mathbf{c}=\mathbb{E}_{\mathbf{x}\in Dataset}(\mathbf{x})\) represents the average of the sample dataset, depicting the projection of the data distribution in the sample space on various coordinate axes. Furthermore, the diagonal elements of \(\mathbf{A}(\mathbb{E}_{\mathbf{x}\in Dataset}(\mathbf{x}*\mathbf{x}^{T}))\) offer the attacker statistical information about this local dataset.

Generally, solely using the statistical information of these datasets cannot reconstruct the entire dataset. Similarly, it's not possible to obtain gradients for the output of each sample, thereby preventing the reconstruction of individual sample labels. The results obtained by using \(\mathbf{c}\) and \(\mathbf{W}\) to gather statistical label information are unreliable.

Additionally, for structures such as CNNs and RNNs/LSTMs, the difficulty of attacks increases due to weight sharing. For CNNs, since convolutional kernels only accept local samples as input, information in \(\mathbf{A}\) encompasses statistical information from all localities of the samples. For RNNs/LSTMs, information in \(\mathbf{A}\) includes statistics of each word vector in a sentence. These network structures make it possible for attackers to fail even in single-sample scenarios. For MLPs, the information contained in the intermediate layer \(\mathbf{A}\) is almost equivalent to the information encoded in the parameters of the BN (Batch Normalization) layer. The mean output of the Batch Normalization (BN) layer is equivalent to the last column of \(\mathbf{A}\), whereas the variances differ between the BN layer and \(\mathbf{A}\)'s diagonal but both contain statistical information related to squared values.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c} \hline Percentile & 12.5 & 25.0 & 37.5 & 50.0 & 62.5 & 75.0 & 87.5 & 100.0 \\ \hline FedLPA & 0.60 & 1.00 & 1.10 & 1.39 & 2.75 & 50.71 & 40736.88 & \textgreater{}=1e9 \\ \hline FedAvg & 0.09 & 0.16 & 0.71 & 1.56 & 26.24 & 950.89 & 54307.91 & \textgreater{}=1e9 \\ \hline \end{tabular}
\end{table}
Table 10: iDLG attack results of FedLPA and FedAvg.

It's worth noting that the parameters acquired by the BN layer using the sliding-window average method are also frequently used during the computation of \(\mathbf{A}\) and \(\mathbf{B}\), as mentioned in the paper [54].

FedAvg provides model parameter values, the average of gradients, and BN layer parameters. Compared to FedAvg, the additional information we offer is actually limited to: the mean of each sample in each dimension, the mean of squares of each sample in each dimension, and the mean of square gradients. Utilizing this information, attacking becomes highly challenging when the number of samples exceeds three. Although we don't rule out the possibility of successful methods in practice due to the data's own correlations, the limitations are significant based on our analysis, and our security level is quite close to that of FedAvg.

We discuss two common attacks here. Inferring class representatives:

i) Model inversion attacks [87] exploit the confidence information provided by machine learning applications or services. Our method does not provide confidence information, nor does it compute the information required for it. Therefore, our method's defense level against these attacks aligns with FedAvg's defense level.

ii) Attacks using GANs to construct class representatives [88] utilize the client-uploaded model as a discriminator and its output as labels to train a generator to generate similar data. The additional statistical information we provide might be used to constrain the distribution of inputs for GANs, specifically their mean values. Since the statistical information of the dataset may contain some common features among samples, it might potentially aid in speeding up the convergence of training GANs but may not significantly enhance the accuracy of generated data after GAN optimization. It's worth noting that if the BN layer parameters uploaded by FedAvg could be used to constrain the statistical information of GANs' inputs, they would be equivalent to the information provided by our method.

Additionally, these attack methods against FedAvg only yield favorable results when class members are similar, meaning the dataset has clear common features that allow the constructed representatives to resemble the training data. When class members are dissimilar, these shared features tend to be confounded, rendering the constraints imposed by the sample mean ineffective, hence not enhancing the effectiveness of GANs attacks.

In summary, our method exhibits a security level consistent with FedAvg against these types of attacks. Even in cases where the BN layer is not required, our method's security is similar to that of FedAvg.

Membership inference attacks against aggregate statistics [87, 88] and Membership inference attacks against ML models [89, 90, 91, 92, 93, 94] aim to infer whether a sample belongs to the training dataset using appropriate prior distributions and statistical data. These attack methods impose specific requirements on the dataset. In such attack scenarios, whether the sample mean information our method can provide is exploitable by the attacker depends on whether this information can reveal the inherent distribution correlations within the dataset. However, for high-dimensional complex data, sample mean information often falls short in achieving this.

The inference attack towards client model is a complex topic. Other inference attack methods and defense mechanisms against them fall outside this paper's scope. It is an interesting topic to explore more robust measures to prevent such breaches in future works.

Therefore, in the case of these attacks we mentioned, our method exhibits the same level of security as FedAvg (since FedAvg requires uploading statistically equivalent information within the BN layer). For scenarios without a BN layer, whether our method reduces security depends on the characteristics of the dataset itself. Real-world data is often high-dimensional and complex, making successful attacks challenging.

## Appendix G Additional experiments

### t-SNE visualization

We conduct experiments using MNIST dataset with a \(\beta\) value of 0.05, training 10 local clients over 200 local epochs with random seed 0. In this biased local dataset setting, local clients could only distinguish a subset of the classes, as illustrated in Figure 3.

Based on seed 0, we partition the training data for the 10 local clients with the following form (label:# of the data) as:

local client #1: {4: 2, 5: 12, 6: 2847, 9: 16}

local client #2: {1: 20, 4: 189, 5: 5349}

local client #3: {0: 669, 1: 476, 2: 67, 6: 15, 7: 6068}

local client #4: {0: 266, 1: 375, 3: 3956, 7: 196, 9: 5932}

local client #5: {0: 4, 1: 418, 2: 5862}

local client #6: {1: 2, 2: 25, 4: 5195, 5: 24, 6: 80, 8: 28}

local client #7: {1: 5034, 2: 3, 4: 22, 5: 6, 6: 2669}

local client #8: {0: 4914}

local client #9: {4: 433, 5: 29, 6: 307, 8: 5373}

local client #10: {0: 70, 1: 417, 2: 1, 3: 2175, 4: 1, 5: 1, 7: 1, 8: 450, 9: 1}

It is worth noting that local client #2 has the training data mostly with label number 5, and as the corresponding t-SNE visualization shows in Figure 2(b), the local train model could mainly cluster the data with label 5 (marked as purple). As data for label 1 (marked as orange) is different from other data with all other labels, some local clients may be able to cluster the data with label 1 with good results. Other local clients, such as local client #3, #4, #6, #7, #9, #10, show the similar results like local client #2.

Figure 4 displays the t-SNE visualization for the global models of FedAvg, FedNova, SCAFFOLD, FedProx, and DENSE using the training data, with the figure legends identical to those in Figure 1. It's evident from Figure 1 that FedLPA outperforms the baselines in classifying the ten classes.

Figure 4: t-SNE visualizations of the baseline approaches on the global model.

Figure 3: t-SNE visualizations of 10 local clients.

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_FAIL:26]

[MISSING_PAGE_FAIL:27]

same as the default. The results in Table 18 show that when the \(\beta\) is smaller than 0.1, our method outperforms Co-Boosting. Thus, with the increment of skewness, FedLPA shows significantly superior results.

In conclusion, our method could be comparable with FedOV and Co-Boosting in some settings, even when they consume more computational resources as shown in Appendix G.7.

### Communication overhead evaluation

Table 5 shows the communication overhead evaluation of a simple CNN with 5 layers on CIFAR-10 dataset. The results are given based on the experiments. In this section, we will give a concrete example to show the details.

The communication bits are the number of bits that are transmitted between a server and a client in a directed communication. It reflects the communication efficiency of federated learning algorithms. Better algorithms should have lower communication bits. The default floating point precision is 32 bits in Pytorch.

**A fully-connected neural network model example:** We use a fully-connected neural network model with architecture 784-256-64-10 as an example to show the calculation, which has \(784\cdot 256+256+256\cdot 64+64+64\cdot 10+10=217930\) floating point numbers, which is \(6973760\) bits or around \(0.831\) MB.

For a single directed communication from a client to the server or vice versa, the cost for FedAvg, FedProx, FedNova, and DENSE is \(0.831\) MB each. SCAFFOLD costs \(1.662\) MB for the same communication, which is double the amount of the others.

For a single communication from a client to the server, our method requires additional upload of \(\mathbf{A}_{k}\) and \(\mathbf{B}_{k}\), which contain \(785\cdot 785+256\cdot 256+257\cdot 257+64\cdot 64+65\cdot 65+10\cdot 10=756231\) floating point numbers in total. Note, as \(\mathbf{A}_{k}\) and \(\mathbf{B}_{k}\) are symmetric matrices, we only need to upload the upper triangular part of them, reducing the total to roughly \(756231/2=378115.5\) floating point numbers as about \(1.442\) MB. Therefore, our approach costs \(2.272\) MB for the one directed communication, which is \(2.734\) times as FedAvg, FedProx, and DENSE, and \(1.367\) times as SCAFFOLD.

**A CNN model example:** We use another example using CNN to show the communication overhead. For example, we have one model, the first layer is nn.Conv2d(1, 6, 5), means there are 3 input channels, 6 output channels, and a 5x5 kernel size; the second layer is nn.Conv2d(6, 8, 5), means there are 6 input channels, 8 output channels, and a 5x5 kernel size.

The parameter count for the first layer is 1x6x5x5+6=156. Note that \(\mathbf{A}\) and \(\mathbf{B}\) are both symmetric matrices. Thus, the additional parameters for \(\mathbf{A}\) and \(\mathbf{B}\) for each kernel would be 5x5, and estimating the covariance for biases without decomposition results in a size of 6x6. Therefore, the additional parameters for this layer are 201 (\(\mathbf{A}_{k_{1}}\)=6x((5x5-5)/2+5)\(,\)\(\mathbf{B}_{k_{1}}\)=6x((5x5-5)/2+5)+(6x6-6)/2+6.

The parameter count for the second layer is 8x6x5x5+8=1208. Therefore, the additional parameters for this layer are 1476 (\(\mathbf{A}_{k_{2}}\)= 6x8x((5x5-5)/2+5)\(,\)\(\mathbf{B}_{k_{2}}\)=6x8x((5x5-5)/2+5)+(8x8-8)/2+8).

These two examples all follow the theory: the communicated parameters \(\mathbf{A}\), \(\mathbf{B}\) and \(\mathbf{M}\) are approximately 2x of the number of all parameters in the model \(\boldsymbol{\theta}\).

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline epoch number & 10 & 20 & 50 & 100 & 200 \\ \hline FedLPA & 47.93\(\pm\)0.89 & 53.37\(\pm\)0.61 & 71.07\(\pm\)0.02 & 71.07\(\pm\)0.35 & 69.63\(\pm\)0.29 \\ \hline FedOV & 71.0\(\pm\)0.25 & 70.27\(\pm\)0.39 & 69.23\(\pm\)0.31 & 65.83\(\pm\)0.23 & 64.50\(\pm\)0.38 \\ \hline \end{tabular}
\end{table}
Table 17: Comparison with FedOV on MNIST with #C=2.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \(\beta\) & 0.01 & 0.05 & 0.1 & 0.3 & 0.5 \\ \hline FedLPA & 21.20\(\pm\)0.67 & 54.27\(\pm\)0.38 & 55.33\(\pm\)0.06 & 68.20\(\pm\)0.04 & 73.33\(\pm\)0.06 \\ \hline Co-Boosting & 17.31\(\pm\)0.24 & 48.97\(\pm\)1.44 & 73.15\(\pm\)1.86 & 83.37\(\pm\)0.44 & 86.21\(\pm\)0.31 \\ \hline \end{tabular}
\end{table}
Table 18: Comparison with Co-Boosting on FMNIST.

[MISSING_PAGE_FAIL:29]

simple-CNN with 10 clients and five random seeds. We do the experiments on EMNIST-mnist and EMNIST-letters. The results are shown in Table 23.

In addition to these, we conduct experiments with Tiny-ImageNet [98] with ResNet-18 with 10 clients and five random seeds. The results are shown in Table 24.

### Ablation experiments analyzing the number of approximation iterations of FedLPA

The proposed method is composed of multiple approximations: 1) empirical Fisher to approximate the Hessian, 2) block-diagonal Fisher matrix instead of full, 3) approximating global model parameter \(\bar{\mathbf{M}}\) with optimization problem in Eq. 14.

1) Empirical Fisher to approximate the Hessian:

Although empirical Fisher has been successfully applied in many methods and yielded good results, discussions concerning the approximation error of empirical Fisher are limited. Fortunately, previous work [99] provides a detailed critical discussion of the empirical Fisher approximation.

**i. Fisher to approximate the Hessian:**

When the loss function represents an exponential family distribution, the Fisher is a well-justified approximation of the Hessian, and its approximation error can be bounded in terms of residuals. The accuracy of this approximation improves as the residuals diminish and is exact when the data is perfectly fitted.

**ii. Empirical Fisher to Fisher:**

It's noted that the Fisher and empirical Fisher coincide near minima of the loss function under two conditions:

A. The model distribution closely approximates the data distribution.

B. A sufficiently large number of samples allows both the Fisher and empirical Fisher to converge to their respective average values in the population.

In practical environments, especially condition 1, might not hold, causing bias between empirical Fisher and Fisher. However, empirical Fisher still contains effective covariance information. In second-order optimization methods, the covariance information in empirical Fisher can adapt to

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline Dataset & Partitions & FedLPA & FedAvg \\ \hline \multirow{3}{*}{EMNIT-mnist (10 classes)} & \(\beta\)=0.1 & 74.23\(\pm\)3.10 & 57.63\(\pm\)2.30 \\  & \(\beta\)=0.3 & 86.55\(\pm\)0.24 & 62.32\(\pm\)1.77 \\  & \(\beta\)=0.5 & 91.75\(\pm\)0.26 & 82.71\(\pm\)0.96 \\ \hline \multirow{3}{*}{EMNIT-letters (37 classes)} & \(\beta\)=0.1 & 26.34\(\pm\)0.71 & 16.22\(\pm\)0.38 \\  & \(\beta\)=0.3 & 31.75\(\pm\)0.03 & 25.51\(\pm\)0.44 \\ \cline{1-1}  & \(\beta\)=0.5 & 33.78\(\pm\)0.14 & 26.34\(\pm\)0.07 \\ \hline \end{tabular}
\end{table}
Table 23: Experimental with EMNIST.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \(\beta\) & FedLPA & FedNova & SCAFFOLD & FedAvg & FedProx & Dense \\ \hline
0.1 & 58.48\(\pm\)1.33 & 28.77\(\pm\)2.03 & 32.45\(\pm\)0.12 & 33.71\(\pm\)0.16 & 31.78\(\pm\)0.40 & 51.76\(\pm\)0.28 \\ \hline
0.3 & 75.98\(\pm\)1.72 & 53.78\(\pm\)0.32 & 55.00\(\pm\)1.07 & 54.61\(\pm\)0.02 & 52.79\(\pm\)0.80 & 70the gradient noise in stochastic optimization. Nevertheless, referencing the work [54], we can use the model's predictive distribution to obtain an unbiased estimate of the true Fisher at the same computational cost as empirical Fisher.

(2) Block-diagonal Fisher matrix to approximate the full one: The work [100] provides a detailed evaluation and testing of using block-diagonal Fisher to approximate the full one. Firstly, Chapter 6.3.1 "Interpretations of this approximation" in the paper [100] indicates that using a block-wise Kronecker-factored Fisher closely approximates the full Fisher. Although there is a bias term, this term approximates zero when there are sufficient samples. Furthermore, the paper examines the approximation quality of block-diagonal Fisher compared with the true Fisher and suggests that block-diagonal Fisher captures the main correlations, while the remaining correlations have a minimal impact on the experimental results.

(3) Besides, we have added some experiments for more ablation studies with our method on the same experiment setting in the paper with five random seeds with 10 clients. We conducted experiments on FMNIST datasets with \(\beta\)=0.1, 0.3 and 0.5. The results are shown in Table 25. We show the experiments analyzing the number of approximation iterations. With the experiment results, we could know that 5000 iterations are enough to get the ideal results. By default, we use 10000 iterations.

We also show that the computation time for the approximation is linear with the number of approximation iterations in the last column of Table 25.

Additionally, it's worth noting that concerning Laplace approximation, the analysis [101] suggests that the error of Laplace approximation is inversely proportional to the input dimension \(n\) with \(O(n^{-1})\). According to this conclusion, it can be inferred that in our method, for each layer of the neural network, the error of Laplace approximation is inversely proportional to its width. When the neural network is infinitely wide, the approximation error tends towards zero.

### Artifact details

We have uploaded the codebase containing all the methods compared in our paper. Setting up the environment is relatively straightforward with the provided readme file. If you refer to the scripts folder, you will find all the bash scripts necessary to reproduce the tables and figures from our experiments.

The experiments.sh script covers the experiments in Table 1, Table 11, Table 12, Table 13, and Table 14. Running these experiments on a single 2080Ti GPU will take approximately 81 days. Specifically, Table 1 itself will take about 35 days.

The experiments_client.sh script covers the experiments in Table 2, requiring approximately 40 days on a single 2080Ti GPU.

The experiments_coor.sh script covers the experiments in Table 4, which can be completed in 2 days. The experiments_dp.sh script covers the experiments in Table 8, requiring approximately 1 day on a single 2080Ti GPU.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Number of iterations & Accuracy(\(\beta\)=0.1) & Accuracy(\(\beta\)=0.3) & Accuracy(\(\beta\)=0.5) & Computation(s) \\ \hline
1000 & 52.81\(\pm\)0.71 & 60.31\(\pm\)0.23 & 72.11\(\pm\)0.57 & 3 \\
5000 & 59.70\(\pm\)0.32 & 68.09\(\pm\)0.30 & 74.27\(\pm\)0.12 & 15 \\
10The experiments_fedavgwith_attack.py and experiments_fedlpawith_attack.py covers the experiments in Table 10 requiring approximately 1 day on a single 2080Ti GPU.

The experiments_extreme_clients.sh script covers the experiments in Table 15 and requires approximately 4 days of GPU processing.

The experiments_extreme.sh script reproduces the experiments in Table 16 and takes about 10 days.

The experiments of Table 17 and Table 18 take about 8 days.

The experiments_emnist.sh script covers the experiments in Table 23 and takes about 1 day.

Running experiments_multiple_round.sh will yield the results as shown in Figure 2, and this process takes about 1 day.

The experiments for Table 6 and Table 7 will take about 1 day. The experiments for Table 25 and Table 3 will also take about 1 day.

To generate the t-SNE visualizations shown in Figure 1, Figure 3, and Figure 4, you can use the experiments.py script with the "alg=tsne" option.

In total, reproducing all the experiment results in this paper will require about 185 days for GPU processing.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]. Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]. Justification: The paper discusses the limitations of the work performed by the authors. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes].

Justification: The paper provides the full set of assumptions and a complete (and correct) proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]. Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]. Justification: The paper provides open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes]. Justification: The paper specifies all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]. Justification: The results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]. Justification: For each experiment, the paper provides sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes]. Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]. Justification: There is no societal impact of the work performed. Guidelines: ** The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.