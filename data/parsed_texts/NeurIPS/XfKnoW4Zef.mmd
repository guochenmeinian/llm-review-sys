# Towards Robust and Expressive Whole-body Human Pose and Shape Estimation

 Hui En Pang1, Zhongang Cai1,2, Lei Yang2, Qingyi Tao2, Zhonghua Wu2,

**Tianwei Zhang1**, **Ziwei Liu1**

1S-Lab, Nanyang Technological University 2SenseTime Research

{huien001, tianwei.zhang, ziwei.liu}@ntu.edu.sg

{caizhongang, yanglei, taoqingyi, uuzhonghu}@sensetime.com

###### Abstract

Whole-body pose and shape estimation aims to jointly predict different behaviors (e.g., pose, hand gesture, facial expression) of the entire human body from a monocular image. Existing methods often exhibit degraded performance under the complexity of in-the-wild scenarios. We argue that the accuracy and reliability of these models are significantly affected by the quality of the predicted _bounding box_, e.g., the scale and alignment of body parts. The natural discrepancy between the ideal bounding box annotations and model detection results is particularly detrimental to the performance of whole-body pose and shape estimation. In this paper, we propose a novel framework RoboSMPLX to enhance the robustness of whole-body pose and shape estimation. RoboSMPLX incorporates three new modules to address the above challenges from three perspectives: **1) Localization Module** enhances the model's awareness of the subject's location and semantics within the image space. **2) Contrastive Feature Extraction Module** encourages the model to be invariant to robust augmentations by incorporating contrastive loss with dedicated positive samples. **3) Pixel Alignment Module** ensures the reprojected mesh from the predicted camera and body model parameters are accurate and pixel-aligned. We perform comprehensive experiments to demonstrate the effectiveness of RoboSMPLX on body, hands, face and whole-body benchmarks. Codebase is available at https://github.com/robosimplx/robosimplx.

## 1 Introduction

Human pose and shape estimation tries to build human body models from monocular RGB images or videos. It has gained widespread attention owing to its extensive applications in various fields, including robotics, computer graphics, and augmented/virtual reality. Early works use various statistical models (e.g., SMPL [31], MANO [43], FLAME [26]) to individually reconstruct different parts, including human body [17, 22, 4, 16, 24, 10, 21, 20], face [9, 8, 12], and hand [28, 3, 63]. Recently, there is a growing interest in whole-body estimation [11, 6, 61, 44, 58], which jointly estimates the pose, hand gestures and facial expressions of the entire human body from the input. Commonly these methods first employ separate sub-networks to extract the features of body, hands and face. These features are then used to predict whole-body 3D joint rotations and other parameters (e.g., body shape, facial expression), which are further combined to generate the whole-body 3D mesh. This is a crucial step towards modeling human behaviors in an efficient and practical manner.

However, achieving accurate and robust whole-body estimation is particularly challenging as it requires precise estimation of each body part and the correct connectivity between them. In particular, due to the smaller sizes of hand and face images, they are typically localized, cropped and resized to higher resolutions before being processed by the relevant sub-network. To tackle the absenceof ground-truth bounding boxes in the real-world scenarios, existing whole-body methods utilize various detection techniques to obtain the crops. The accuracy of the whole-body estimation is highly sensitive to the quality of input crops. Our experiment results in Section 3 show that even minor fluctuations in the scale and alignment of input crops can significantly affect the model performance, indicating a limited ability to localize and extract meaningful features about the subject in the image.

The lack of robustness in existing whole-body pose and shape estimation methods highlights three critical aspects that can be improved upon: 1) accurate localization of the subject and its parts, 2) accurate extraction of useful features, and 3) accurate pixel alignment of outputs. Inspired by these findings, we propose three novel modules, each specifically designed to address a particular goal:

* **Localization Module**. This module implements sparse and dense prediction branches to ensure the model is aware of the location and semantics of the subject's parts in the image. The learned location of the joint positions are helpful in recovering the relative rotations.
* **Contrastive Feature Extraction Module**. This module incorporates a pose- and shape-aware contrastive loss, along with positive samples, to promote better feature extraction under robust augmentations. By minimizing the contrastive loss, the model can produce consistent representations for the same subject, even when presented with different augmentations, making it robust to various transformations and capable of extracting meaningful invariant features.
* **Pixel Alignment Module**. This module applies differentiable rendering to ensure a more precise pixel alignment of the projected mesh, and learn more accurate pose, shape and camera parameters.

By integrating these three modules, we build a more robust and reliable whole-body pose and shape estimation framework, RoboSMPLX. Comprehensive evaluations demonstrate its effectiveness on body, face, hands and whole-body benchmarks.

## 2 Related Works

**Whole-body Mesh Recovery.** Despite significant progress in 3D body-specific [23; 22; 4; 16; 24; 10; 21; 20], hand-specific [28; 3], and face-specific [9] mesh recovery methods, there have been limited attempts to simultaneously recover all those parts. Early studies on whole-body pose and shape estimation primarily fit a 3D human model to 2D or 3D evidence [15; 53; 41; 54], which can be slow and susceptible to noise. Recent studies utilized neural networks to regress the SMPL-X parameters for a whole-body 3D human mesh. The model is composed of separate sub-networks to process body, hand and face, respectively. _One-stage_ methods, e.g., OS-X [27], have the benefit of reduced computational costs and improved communication within part modules for more natural mesh articulation. However, the omission of hand and face experts makes it difficult for the model to leverage the widely available part-specific datasets, thus decreasing the hand and face performance. _Multi-stage_ methods, e.g., ExPose [6], FrankMocap [44], PIXIE [11] and Hand4Whole [35], use different techniques to localize part crops.

Expose [41] and PIXIE [11] localize hand and part crops from the body mesh, making them dependent on the accuracy of body poses. Minor rotation errors accumulated along the kinematic chain may result in deviations in joint locations and thus inaccurate part crops. In contrast, Hand4Whole [35] predicts hand and face bounding boxes using a network leveraging image features and 3D joint heatmaps, but the resulting crops have low resolution. PyMAP-X [11] relies on an off-the-shelf whole-body pose estimation model to obtain crops, which, while more accurate, incurs extra computation. More detailed comparison with PyMAP-X are in Appendix C.

**Robustness in vision tasks.** Efforts to tackle robustness in vision tasks have utilized diverse strategies such as data augmentation, architectural innovations, and training methodologies [25; 56; 42; 30; 51; 60; 2]. AdvMix [51] employs adversarial augmentation and knowledge distillation, challenging models with corrupted images to foster learning from complex samples. Architectural modifications, such as novel heatmap regression [60], have been introduced to mitigate the impact of minor perturbations. HuMoR [42] utilizes a conditional variational autoencoder to capture the dynamics of human movement, thereby achieving generalization across diverse motions and body shapes. Additionally, PoseExaminer [30] employs a multi-agent reinforcement learning system to uncover failure modes inherent in human pose estimation models, highlighting model limitations in real-world scenarios. Complementing these efforts, Robo3D [25] provides a comprehensive benchmark for assessing the robustness of 3D detectors and segmentors in out-of-distribution scenarios.

urthermore, [56] utilize a confidence-guided framework to improve the accuracies of propagated labels. Contrastive learning, as demonstrated by CoKe [2], has also been employed to enhance robustness in keypoint detection, especially in occlusion-prone scenarios.

**Contrastive Learning.** Recently contrastive learning has demonstrated state-of-the-art performance among self-supervised learning (SSL) approaches. This strategy has been applied to 3D hand pose and shape estimation [46; 63]. Sanyal et al. [45] incorporate a novel shape consistency loss for 3D face shape and pose estimation that encourages the face shape parameters to be similar when the identity is the same and different for different people. Choi et al. [5] were the first to apply contrastive learning for 3D human pose and shape estimation. They found that SSL is not useful for this task, as the learned representations could be challenging to embed with high-level human-related information. Khosla et al. [19] proposed supervised contrastive learning for image classification tasks, which incorporates label information during training. Currently there is not attempt to apply this strategy to human pose and shape estimation, where the definition of positive samples is unclear, and data lie in a continuous space. We are the first to overcome these challenges and integrate supervised contrastive learning with whole-body pose and shape estimation.

**Pixel Alignment in Pose and Shape Estimation.** Many studies have been done to learn the subject's location in an image. Some works implicitly supervise the location. They primarily utilize projected meshes by supervising 2D joints regressed from the mesh [17; 23; 22; 4; 16; 24; 10; 21; 20]. Further supervision, such as dense body landmarks, silhouettes, and body part segmentation, is also employed to better align the predictions with the image [55; 37; 40; 49; 59; 57; 10]. Some other works explicitly learn the subject's location. Moon et al. [35] explicitly predict the keypoint locations in the image. Semantic body part segmentation is used as an explicit intermediate representation [41; 37]. PARE [41] employs a renderer to project the ground-truth mesh to the image space, and supervise the predicted part silhouette mask. However, dense part segmentation and differentiable rendering have not been employed in whole-body pose and shape estimation, which will be achieved in our framework.

## 3 Motivation

As discussed in Section 1, existing whole-body pose and shape estimation approaches suffer from the robustness issue, due to the models' sensitivity to the quality of input crops. To investigate the reasons and disclose the influence factors, we conduct a comprehensive evaluation of four state-of-the-art methods: ExPose [6], PIXIE [11], Hand4Whole [35] and OS-X [27]. We opt for a set of ten commonly encountered augmentations and vary their scales within a realistic range (see Appendix A for more details). The augmentations can be classed into three categories (1) _image-variant_ augmentations: they affect the image without altering the objects' 3D poses or positions, such as color jittering; (2) _location-variant_ augmentations: they modify the subject's location without changing its pose, involving operations like translation and scaling; (3) _pose-variant_ augmentations: they simultaneously alter both the 3D pose and location, including rotation.

**Impact of subject localization**. We first reveal that existing models demonstrate high sensitivity to the subject's position, indicating potential difficulties in subject localization. Figure 1 reports the PA-PVE errors of the whole body under different augmentations. We observe that image-variant augmentations (contrast, sharpness, brightness, hue and grayscale) lead to an acceptable range of error rates (approximately in the 50s) and minimal fluctuation (around \(\pm 2\)). In contrast, location

Figure 1: **Wholebody PA-PVE errors under different augmentations (sorted in descending order). The dashed line indicates baseline performance without augmentation.**

variant augmentations altering the subject's position within the frame, such as rotation, scaling, and horizontal or vertical translation, result in substantially higher error magnitudes. This demonstrates the heightened sensitivity of existing models to changes in the subject's position. In Appendix, we provide the results of other metrics and benchmarks in Figures 22 - 23, and visualizations of whole-body estimation under different settings in Figures 25 - 26.

Such position-altering augmentations are common in real-world scenarios, where the subject in the image is often localized using external detection models and control over the quality of crops is less feasible. In practice, to guarantee the visibility of the subject, crops are often made broader, This can lead to significant performance degradation, as errors increase with smaller augmentation scale factors (<1.0) (Figure 1). Besides, horizontal and vertical translations, which correspond to scenarios where the subject is not perfectly centralized or entirely visible within the frame, can further decrease the performance. Similarly, the alignment and scale of these crops also influence the pose and shape estimation systems targeting body, face and hands (Figure 3, more quantitative and qualitative evidence in Appendix M). Whole-body methods bear the additional responsibility of accurately localizing body parts such as hands and face. Inaccurate part crops (Figure 2) can adversely affect the performance of part subnetworks, and further the whole-body estimation.

**Impact of feature extraction**. The deterioration of performance in the face of such variations suggests that the model struggles to extract meaningful features. Under alterations in translation or scale, the subject remains within the image frame, though the proportion of background content may vary. It is difficult for existing methods to effectively disregard irrelevant background elements and extract relevant features related to the subject of interest. To enhance the model's robustness, it is critical to produce consistent features irrespective of various augmentations applied to the image.

**Impact of output pixel alignment**. Pixel alignment is a critical aspect of high model performance. In certain instances, despite having precise subject localization, the model fails to produce properly aligned results (Figure 25 in Appendix). This is often caused by the suboptimal camera parameter estimation. To address this issue, we need to accurately estimate the camera parameters, ensuring the projected mesh is precisely aligned with the ground-truth at the pixel level. Such precision would enhance the effectiveness of the model in producing accurate pose, shape and camera parameter predictions, improving the overall accuracy and reliability of the estimation process.

## 4 RoboSMPLX Framework

We design RoboSMPLX to enhance the robustness of whole-body pose and shape estimation. It provides three specialized modules to address each challenge in Section 3: 1) **Localization Module** (Section 4.2): explicitly learning the location information of the subject and incorporating it into model estimations for pose, shape and camera ; 2) **Contrastive Feature Extraction Module** (Section 4.3): reliably extracting pertinent features under various augmentations, thereby improving the model's generalization ability and robustness to a broader range of real-world scenarios; 3) **Pixel Alignment Module** (Section 4.4): ensuring that the outputs are pixel aligned.

We start with the description of RoboSMPLX architecture with Body, Hand and Face subnetworks (Section 4.1). Each subnetwork is integrated with the **Localization Module** and **Pixel Alignment

Figure 3: **Sensitivity of existing body and hand models to different alignments (left) and scales (right).**

Figure 2: **Crops from (a) ExPose [6] (b) PIXIE [11], (c) Hand4Whole [35] (d) RoboSMPLX.**

**Module**, and applies the **Contrastive Feature Extraction Module** for learning more robust features. Figure 6 shows the Hand subnetwork architecture. The other two subnetworks have the same designs.

### Architecture and Training Details

Figure 4 shows the overall pipeline of RoboSMPLX for whole-body 3D human pose and mesh estimation. The Body subnetwork outputs 3D body joint rotations \(\theta_{b}\in\mathbb{R}^{21\times 3}\), global orientation \(\theta_{bg}\in\mathbb{R}^{3}\), shape parameters \(\beta_{b}\in\mathbb{R}^{10}\), camera parameters \(\pi_{b}\in\mathbb{R}^{3}\), and whole-body joints \(K\in\mathbb{R}^{137\times 3}\). Joints corresponding to the hand and face are used to derive bounding boxes. Subsequently, hand and face images are cropped from a high-resolution image to preserve details. The Hand subnetwork predicts left and right hand 3D finger rotations \(\theta_{b}\in\mathbb{R}^{15\times 3}\). Simultaneously, the Face subnetwork generates 3D jaw rotation \(\theta_{f}\in\mathbb{R}^{3}\) and expression \(\psi_{f}\in\mathbb{R}^{10}\). When training Hand and Face subnetworks with part-specific datasets, additional parameters such as global orientation \(\theta_{fg}\in\mathbb{R}^{3}\), shape \(\beta_{f}\in\mathbb{R}^{50}\), and camera \(\pi_{f}\in\mathbb{R}^{3}\) are estimated. These branches are discarded during whole-body estimation and training. Additional information concerning each subnetwork can be found in Appendix B. Further details regarding the training and inference durations are elaborated upon in Appendix K.

Subnetworks are trained separately, then integrated in a multi-stage manner. Initial whole-body training runs for 20 epochs. The hand and face modules are substituted with the trained Hand and Face subnetworks, followed by 20 epochs of fine-tuning to better unify the knowledge from the Hand and Face subnetworks into the whole-body understanding. Each subnetwork is trained by minimizing the following loss function \(L\):

\[L=\lambda_{3D}L_{3D}+\lambda_{2D}L_{2D}+\lambda_{BM}L_{BM}+\lambda_{proj}L_{ proj}+\lambda_{segm}L_{segm}+\lambda_{con}L_{con}\] (1)

Here \(L_{BM}\) is the L1 distance between the predicted and ground-truth body model parameters. \(L_{3D}\) denotes the L1 distance between 3D keypoints and joints regressed from the body model. \(L_{2D}\) signifies the L1 distance of the ground-truth 2D keypoints to predicted and projected 2D joints. The latter are obtained by projecting the regressed 3D coordinates from the 3D mesh to the image space using the perspective projection [17]. The part segmentation loss \(L_{segm}\) is the cross-entropy loss between \(P_{h,w}\) after softmax and \(P_{h,w}\) averaged over H\(\times\)W elements, following [20]. \(L_{proj}\) refers to the projected segmentation loss, which is the sigmoid loss between the projected mesh and the ground-truth segmentation map. \(L_{con}\) is the contrastive loss described in Section 4.3. For wholebody training, \(L_{box}\) is added to measure the L1 distance between the predicted and actual center and scale of the hands' and face's boxes.

### Localization Module

This module focuses on subject localization by explicitly learning both sparse and dense predictions of the subject within the image. Figure 5 shows an example of the supervision used for each subnetwork.

Figure 4: **Pipeline of our RoboSMPLX framework consisting of Body, Hand and Face subnetworks.**

Figure 5: **Examples of keypoint and part segmentation supervision for Body, Hand and Face subnetworks.**In contrast to prior methods that directly output pose rotations from backbone features, this module aims to make the model explicitly conscious of the subject's location and semantics while predicting pose, shape and camera parameters. It can reduce the model's sensitivity to the variations of the subject's position, caused by minor shifts in the scale and alignment of the bounding box.

As shown in Figure 6, given an image, a convolutional backbone is utilized to extract its feature map \(F\in\mathbb{R}^{512\times 32\times 32}\). Following [35], a 1\(\times\)1 convolutional layer is then used to predict 3D feature maps \(LF\in\mathbb{R}^{32\cdot J\times 32\times 32}\) from \(F\), where \(J\) represents the number of predicted joints with a feature map depth of 32. \(LF\) contains valuable information about the mesh's position in the image and semantics of various parts. It is concatenated with the backbone feature map \(F\) to predict pose \(\theta\in\mathbb{R}^{P}\), shape \(\beta\in\mathbb{R}^{10}\) and camera translation \(\pi\in\mathbb{R}^{3}\), where \(P\) is the number of body parts. Meanwhile, \(LF\) is also used to obtain extra information with two branches: (1) 3D joint coordinates \(K\)\(\in\mathbb{R}^{J\times 3}\) are obtained from \(LF\) using the soft-argmax operation [47] in a differentiable manner. (2) 2D part segmentation maps \(S\in\mathbb{R}^{P+1\times 64\times 64}\) are extracted from \(LF\) with several convolution layers, which model \(P\) part segmentation and 1 background mask. Here, 64 represents the height and width of the feature volume, and each pixel \((h,w)\) stores the likelihood of belonging to a body part \(P\).

Note that learning part segmentation maps and 3D joint coordinates is complementary, as 3D joint coordinates encode depth information that may inform part ordering in segmentation maps. Additionally, joints often reside at the boundaries of part segmentation maps, serving as separators for distinct parts. The Body subnetwork utilizes 24 parts \(P\) and 137 joints \(J\), the Hand subnetwork employs 16 parts \(P\) and 21 joints \(J\), while the Face subnetwork employs 15 parts \(P\) and 73 joints \(J\).

### Contrastive Feature Extraction Module

This module incorporates a pose- and shape-aware contrastive loss, along with positive samples. By minimizing this loss, the model can produce consistent representations for the same subject, even when presented with different augmentations, thus fostering the extraction of meaningful features.

Conventional contrastive learning methods based on SSL (e.g., SimCLR) face challenges in unifying similar pose embeddings and distancing dissimilar ones in human pose and shape estimation tasks. Without labels for guidance, images with similar poses could be misidentified as negative samples and contrasted away, complicating the self-organization of the embeddings in pose space. Figures 9 to 12 in Appendix show their ineffectiveness for the 3D human pose and shape task [5] by visualizing the retrieved samples from the embeddings. The supervised contrastive learning approach by Khosla

Figure 6: **Subnetwork Architecture with three modules.** We use the Hand subnetwork as an example. \(z\) represents normalized \(J_{3D}\) while \(p\) corresponds to the ground-truth of \(z\). Green and red dashed lines refers to contrastive loss for positive and negative samples respectively.

Figure 7: **Augmentations for the Body subnetwork.** Black, blue and red labels represent image-variant, location-variant and pose-variant augmentations, respectively.

et al. [19], though effective for image classification, might not extend well to human pose and shape estimation, which is a high-dimensional regression problem and poses exist in a continuous space rather than well-defined classes.

Our module overcomes the aforementioned issues with two innovations. First, we experiment with three human pose representations \(\bm{z}\) and the corresponding distance functions: (1) A concatenated form of the global orientation and rotational pose; (2) global orientation and rotational pose as separate entities (3) 3D root-aligned joints regressed from the body model, derived from pose and shape inputs. For (1) and (2), we explore relative rotations in two forms: 6D vector and rotation matrix representation. For (3), L1, Smooth L1, and Mean Squared Error (MSE) was used (Table 9).

Second, we investigate ten data augmentations, and classify them into three categories (see Figure 7 for the Body subnetwork, and Figure 29 in Appendix for Hand subnetwork): (1) _image-variant_ augmentations such as color jittering, blur, occlusion and background swapping; (2) _location-variant_ augmentations involving translation and scaling; (3) _pose-variant_ augmentations including rotation and horizontal flipping. Our ablation study in Table 10 shows that augmentations with varied global orientation are detrimental to the model performance. Consequently, we exclude such modifications when constructing positive pairs. Instead, each positive sample is constructed utilizing a random combination of location-variant and color-variant augmentations.

Formally, for a batch of \(N\) images, we construct another \(N\) images by applying augmentation to each sample. For each anchor \(i\), let \(j\) be the corresponding augmented sample. Then \(i\) is contrasted against \(2N-1\) terms (1 positive and \(2N-2\) negatives). The loss takes the following form:

\[\mathcal{L}_{con}=\sum_{i=1}^{N}\left(\tau_{pos}\left(\left|\mathrm{d}\left( \bm{p}_{i},\bm{p}_{j}\right)-\mathrm{d}\left(\bm{z}_{i},\bm{z}_{j}\right) \right|\right)+\tau_{neg}\sum_{k=1}^{2N}\left|\!\varphi_{\left[k\neq i,j \right]}\left(\left|\mathrm{d}\left(\bm{p}_{i},\bm{p}_{k}\right)-\mathrm{d} \left(\bm{z}_{i},\bm{z}_{k}\right)\right|\right)\right)\] (2)

where \(\bm{z}_{i}\), \(\bm{z}_{j}\) and \(\bm{z}_{k}\) denote the predicted pose representations, and \(\bm{p}_{i}\), \(\bm{p}_{j}\) and \(\bm{p}_{k}\) denote the ground-truth pose representations for the anchor, positive and negative samples in the batch. The objective of this loss function is to minimize the distance between the positive pairs and maximize the distance between the negative pairs, in alignment with the pose similarity. Note that unlike traditional approaches where the distance is the same for all negative samples, the pairwise distance \(\mathrm{d}(\bm{p}_{i},\bm{p}_{k})\) varies depending on the pose similarity.

### Pixel Alignment Module

This module employs differentiable rendering to ensure that the projected mesh aligns precisely at the pixel level. The alignment is supervised by the projected mask loss. Attaining a proper alignment between the ground-truth part segmentation and rendered mesh requires the accurate prediction of pose, shape, and camera parameters, which subsequently leads to a more precise estimation process.

## 5 Experiments

**Datasets.** For whole-body training, we employ Human3.6M (H36M) [13], COCO-Wholebody [14] (the whole-body version of MSCOCO [29]) and MPII [1]. The 3D pseudo-ground truths for training are acquired using NeuralAnnot [36]. For hand-specific training, we use FreiHAND [62], Interhand [34] and COCO-Wholebody Hands [14]. For face-specific training, we use FFHQ [18], BUPT [52] and AffectNet [32]. For evaluations specific to 3D body, 3D hand, and 3D face, we utilize 3DPW [50], FreiHAND [62], and Stirling [11], respectively. For the 3D whole-body evaluation, we use EHF [41] and AGORA [39]. Additionally, we present qualitative results on the MSCOCO validation set.

**Metrics.** Mean Per Joint Position Error (MPJPE) and Mean Per-Vertex Position Error (MPVPE) are employed to evaluate the positions of 3D joint and mesh vertices, respectively. Each metric calculates the average 3D joint distance (in _mm_) and 3D mesh vertex distance (in _mm_) between the predicted and ground-truth values after aligning the root joint translation. The pelvis serves as the root joint for whole-body and body, whereas the wrists and neck are utilized as root joints for hands and face. Procrustes Aligned (PA) variants of these metrics, PA-MPJPE and PA-MPVPE, further align with rotation and scale. We report the average errors for the left and right hands as the 3D hand error.

### Benchmarking Results

**Hand Subnetwork.** Table 1 compares the performance of the Hand subnetwork with different hand-only and whole-body methods. Our method outperforms that of our whole-body counterparts when trained with only the FreiHAND dataset (i.e. PIXIE, Hand4Whole, PyMAF) or under mixed datasets (i.e. Hand4Whole \(\dagger\), PyMAF \(\dagger\))1 using an identical backbone. Prior research [33; 48] demonstrated that whole-body methods generally employ a parametric representation of the hand mesh, and are numerically inferior to the non-parametric representation used in recent hand-only methods [33; 28]. Despite such reported gap, RoboSMPLX manages to outperform mesh-based techniques, and achieve comparable results as the state-of-the-art METRO when using the same backbone (HRNet-64). Table 4 compares the estimation errors of the Hand subnetwork in Hand4Whole (current whole-body method with SOTA on hands) and RoboSMPLX under different positional augmentations on the FreiHAND test set. It is clear that RoboSMPLX exhibits much better robustness than Hand4Whole. More visualizations are provided in Figure 20 in Appendix.

Footnote 1: \(\dagger\) denotes training with extra datasets in the following evaluation and tables.

**Body Subnetwork.** Table 2 compares the performance of the Body subnetwork across different methods on the 3DPW test set. We observe the competitiveness of RoboSMPLX in relation to other SMPL-based approaches. Besides, since the performance of various methods may significantly differ based on their backbone initialization, datasets and training strategies [38], we establish a baseline to evaluate the effectiveness of our added modules in Table 12 in Appendix. RoboSMPLX achieves a substantial improvement compared to the baseline.

**Face Subnetwork.** Table 3 compares the performance of the Face subnetwork for different methods on the Stirling3D test set. When training with the same dataset, RoboSMPLX outperforms ExPose. The performance of ExPose declines when training on multiple datasets, while RoboSMPLX can still keep low and consistent errors. Figure 8 in Appendix shows some qualitative results for the in-the-wild scenarios, which demonstrates the high generalization of RoboSMPLX. Table 5 compares the robustness of ExPose and RoboSMPLX under different positional augmentations. We also observe that RoboSMPLX has lower errors with different translation and scaling operations. More visualizations are provided in Figure 21 in Appendix.

**Whole-body Network.** We further provide results of the whole-body network on two benchmarks: EHF val set and AGORA test set in Table 6. On EHF, RoboSMPLX outperforms other full-body approaches, particularly in hand and face performance evaluations, and under different positional augmentations (Table 7). It gives subpar performance on AGORA as the predominant source of

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Method** & PA-PVE \(\downarrow\) & PA-MPPE \(\downarrow\) & F-Scores \(\uparrow\) & \\ \hline
**\begin{tabular}{l} **\# Hand-only** \\ FreiHAND [62] \\ Pose2Mesh [4] \\ IZL-MeshNet [33] \\ METRO (HR64) [28] \\ \end{tabular} & 10.7 - & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.8 & 7.7 & 0.674/0.969 \\ 7.6 & 7.4 & 0.681/0.973 \\ **6.7** & **6.8** & 0.717/0.981 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{
\begin{tabular}{l} \multirow{2}{*}{-} \\ 52.3 \\ \end{tabular} } \\ \cline{1-1}  & & & & & \\ \hline
**\begin{tabular}{l} **\# Whole-body** \\ ExPose [41] \\ Zhou et al. [61] \\ FrankMocap [44] \\ PIXIE [11] \\ Hand4Whole \(\dagger\) [35] \\ HMR (Baseline) [17] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ RooSMPLX \(\dagger\) \\ \end{tabular} & 11.8 & 12.2 & 0.484/0.918 & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.6 \\ 7.6 \\ 7.4 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.8 \\ 8.2 \\ \end{tabular} & 
\begin{tabular}{l} \multirow{2}{*}{-} \\ 96.9 \\ 89.7 \\ 97.9 \\ 94.4 \\ 9.3 \\ \end{tabular} \\ Baseline (ResC80) & 52.4 & 85.2 & 103.6 \\ RooSMPLX (HR85) & 49.8 & 80.8 & 96.7 \\ Baseline (ResC9) & 50.3 & 84.5 & 101.5 \\ RoboSMPLX (HR84) & 48.5 & 80.1 & 95.2 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Evaluation of the Hand subnetwork.**

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & Normal & Transx +0.2x & Transx -0.2x & Transy +0.2y & Transy -0.2y & Scale 1.3x & Scale 0.7x \\ \hline Hand4Whole [35] & 7.47 / 15.0 & 8.51 / 21.58 & 8.38/ 20.36 & 8.74/ 22.51 & 8.48/ 19.85 & 7.73/ 16.44 & 7.78/ 17.00 \\ RoboSMPLX & **7.24** **/ **15.23** & **7.27** **/ **15.62** & **7.36** **/ **15.59** & **7.28** **/ **15.50** & **7.34** **/ **15.50** & **7.49** **/ **15.90** & **7.45** **/ **16.51** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Evaluation of the Body subnetwork.**

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Method** & PA-MPPE \(\downarrow\) & MPPE \(\downarrow\) & PVE \(\downarrow\) \\ \hline
**
\begin{tabular}{l} **\# Whole-body** \\ ExPose [41] \\ Zhou et al. [61] \\ FrankMocap [44] \\ PIXIE [11] \\ Hand4Whole \(\dagger\) [35] \\ HMR (Baseline) [17] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ Baseline (ResC9) \\ Baseline (ResC9) \\ RobustSPLX (HR85) \\ Baseline (ResC9) \\ Baseline (ResC9) \\ Baseline (HR85) \\ Baseline (HR84) & 48.8 & 80.1 & 95.2 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Evaluation of the Body subnetwork.**

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Method** & PA-MPPE \(\downarrow\) & MPPE \(\downarrow\) & PVE \(\downarrow\) \\ \hline
**
\begin{tabular}{l} **\# Head-only** \\ FreiHAND [62] \\ Pose2Mesh [4] \\ IZL-MeshNet [33] \\ METRO (HR64) [28] \\ \end{tabular} & 10.7 - & - & 0.529/0.935 \\
7.8 & 7.7 & 0.674/0.969 \\
7.8 & 7.4 & 0.681/0.973 \\
8.6 & 7.4 & 0.681/0.973 \\ METRO (HR64) [28] & **6.7** & **6.8** & 0.717/0.981 \\ \hline
**\begin{tabular}{l} **\# Whole-body** \\ ExPose [41] \\ Zhou et al. [61] \\ FrankMocap [44] \\ PIXIE [11] \\ Hand4Whole \(\dagger\) [35] \\ HMR (Baseline) [17] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ PyMAF \(\dagger\) [58] \\ RooSMPLX \(\dagger\) [58] \\ \end{tabular} & 11.8 & 12.2 & 0.484/0.918 & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.6 \\ 7.4 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.8 \\ 7.8 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.8 \\ 7.5 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.7 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.7 \\ \end{tabular} & 
\begin{tabular}{l} \multirow{2}{*}{-} \\ 7.7 \\ \end{tabular} \\
**\begin{tabular}{l} 7.8 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.8 \\ \end{tabular} & \begin{tabular}{l} \multirow{2}{*}{-} \\ 7.8 \\ \end{tabular} & 
\begin{tabular}{l} \multirow{2}{*}{-} \\ 7.7 \\ \end{tabular} \\
**\begin{tabular}{l} 7.8 \\ \end{tabular} & 
\begin{tabular}{l} \multirow{2}{*}{-} \\ 7.

error is the misidentification of individuals under intense person-person occlusion. We give detailed investigation in Appendix D.

### Ablation Studies

**Contrastive loss**. We validate prior contrastive SSL methods [63, 46, 5] are not particularly adept at learning useful embeddings for human pose and shape estimation. Figures 9 - 12 in Appendix visualize the retrieved images based on the top-5 embedding similarity. They show that without labels, the model primarily extracts features based on background information instead of pose information. Table 8 shows the estimation errors of top-1 retrieved pose (COCO-train) and query pose (COCO-test) with different methods and contrastive loss functions. We observe that SimCLR has higher mean errors than the supervised training method HMR. These results are aligned with [5] that the representations learned through SSL are not transferable for human pose and shape estimation tasks. RoboSMPLX incorporates contrastive loss and positive samples ("HMR + \(L_{con}\), +ve"), which can produce similar representations under varied augmentations, enhancing its robustness.

Table 9 shows the estimation errors when applying contrastive loss with different representations in Section 4.3: "pose" (a concatenated form of global orientation and rotational pose), "go+pose" (global orientation and rotational pose as separate entities), "keypoint" (3D joints regressed from the body model). We observe that regressed 3D joints are the most effective representation, as they encode both shape and pose information in a normalized space. In contrast, the representation of pose as relative rotation has a detrimental impact on the model performance. Incorporating positive samples ("pose, +ve" and "keypoint, +ve") bolsters contrastive learning, encouraging the model to generate similar representations under varied augmentations. Table 10 compares the model performance with different augmentations. Prior methods [63, 46] employed pose-variant augmentations (e.g., rotation and flipping), which can adversely affect the learning by altering the global orientation, and lead to increased errors ("pose") compared to "baseline". Conversely, color-variant, location-variant and their combination provide an improvement over the baseline, showing these augmentations are helpful.

**Location features.** Table 11 shows the ablation of different modules on the Hand subnetwork (ablation for the Body subnetwork is in Table 12 in Appendix). The baseline model is trained that randomly augments images with a scale factor of 0.2 and bounding box jitter of 0.2. We observe that training using _strongaug_ with a larger scale and jitter factor harms the baseline performance. This is likely due to a domain shift. Hand4Whole [35] employs sampled features from positional pose-guided pooling (PPP) to predict pose parameters while shape and camera parameters only utilize backbone features. Our method focuses on explicitly learning the location and part silhouettes, utilizing sparse and dense supervision methods. This proves advantageous as the location information ("LF") improve the performance of pose and shape estimations, with the reduced joint and vertex errors of the regressed mesh. Moreover,

\begin{table}
\begin{tabular}{l|c c c|c c|c c|c c|c c} \hline \hline \multirow{3}{*}{**Method**} & \multicolumn{4}{c|}{**EHF**} & \multicolumn{4}{c}{**AGORA**} \\ \cline{2-13}  & \multicolumn{3}{c|}{PVE \(\downarrow\)} & \multicolumn{3}{c|}{PA-PVE \(\downarrow\)} & \multicolumn{3}{c|}{PVE \(\downarrow\)} & \multicolumn{3}{c}{N-PVE \(\downarrow\)} \\ \cline{2-13}  & WB & H & F & WB & H & F & WB & B & F & LH/RH & WB & B \\ \hline ExPose [6] & 77.1 & 51.6 & 35 & 54.5 & 12.8 & 5.8 & 217.3 & 151.5 & 51.1 & 74.9/71.3 & 265 & 184.8 \\ PIXIE [11] & 89.2 & 42.8 & 32.7 & 55 & 11.1 & **4.6** & 191.8 & 142.2 & 50.2 & 49.5/49.0 & 233.9 & 173.4 \\ Hand4Whole [35] & 76.8 & 39.8 & 26.1 & 50.3 & 10.8 & 5.8 & 135.5 & 90.2 & 41.6 & 46.3/48.1 & 144.1 & 96.0 \\ OSL (ViT-L) & 70.8 & 53.7 & 26.4 & **48.7** & 15.9 & 6.0 & **122.8** & **80.2** & **36.2** & 45.4/46.1 & **130.6** & **85.3** \\ PyMZF-X (HR48) & **64.9** & **29.7** & 19.7 & 50.2 & 10.2 & 5.5 & 125.7 & 84 & 35 & **44.6/45.6** & 141.2 & 94.4 \\ Ours & 73.7 & 34.9 & **17.8** & 49.7 & **10.0** & **4.6** & 132.3 & 85 & 39.4 & 45.3/46.1 & 138.2 & 91.5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Evaluation of wholebody network on EHF and AGORA test set.**

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Supervision & PA-\(\downarrow\) & MPPIE & PA-\(\downarrow\) & PVE \(\downarrow\) \\ \hline Base (R50) & 8.06 & 16.78 & 7.85 & 16.71 \\ Base (R50) & 8.47 & 17.01 & 8.11 & 16.17 \\ Base (R54) & 7.8 & 15.57 & 7.67 & 15.72 \\ Base (DR54) & \(L_{KS}\) & 7.68 & 15.8 & 7.62 & 16.29 \\ PPP [35] & \(L_{KS}\) & 7.65 & 15.93 & 7.56 & 16.37 \\ LF & \(L_{KS}\) & 7.52 & 15.84 & 7.56 & 16.15 \\ joints & \(L_{KS}\) & 7.86 & 15.92 & 7.75 & 16.24 \\ \hline LF (all) & \(L_{KS}\) & 7.49 & 15.51 & 7.46 & 15.59 \\ LF (all) + \(L_{con}\) & \(L_{KS}\) & 7.48 & 15.01 & 7.32 & 15.29 \\ LF (all) + \(L_{con}\), +ve & \(L_{KS}\) & 7.42 & 14.88 & 7.16 & 14.57 \\ LF (all) & \(L_{KS}\), \(L_{Lip}\), +ve & 7.44 & 14.92 & 7.58 & 15.30 \\ LF (all) & \(L_{KS}\), \(L_{Lip}\), +ve & 7.36 & **14.38** & **7.53** & 15.05 \\
**LF (all) + \(L_{con}\), +ve** & \(L_{KS}\), \(L_{Lip}\), +ve & 7.33 & 14.59 & **7.02** & **14.11** \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Ablation of different modules on Hand subnetwork. Results are trained and evaluated on FreiHAND.**we find that using location features "LF (all)" for predicting shape and camera parameters is also beneficial.

**Pixel alignment.** Tables 11 also shows that incorporating differential rendering and using projected segmentation loss (\(L_{proj}\)) for the mesh in RoboSMPLX helps to achieve lower PVE and MPJPE errors. It facilitates the learning of more precise body model and camera parameters to improve the alignment between the rendered 3D model and 2D image. Notably, metrics such as PVE and MPJPE errors is calculated after root alignment and may not sufficiently reflect the quality of mesh projection onto the image space. To offer a more precise analysis, we evaluate the discrepancies between the projected 2D vertices of the ground-truth and projected meshes. More quantitative and qualitative comparisons can be found in Appendix F.

## 6 Conclusion

In this paper, we introduce a new framework RoboSMPLX to advance the field of whole-body pose and shape estimation. It enhances the whole-body pipeline by learning more precise localization for part crops while ensuring that part subnetworks are robust enough to handle suboptimal part crops and produce reliable outputs. It achieves this goal with three innovations: accurate subject localization by explicitly learning both sparse and dense predictions of the subject, robust feature extraction with supervised contrastive learning, and accurate pixel alignment of outputs with differentiable rendering. Nevertheless, it is important to acknowledge that there are instances in which our framework exhibits limitations, such as (1) inaccurate beta estimation due to out-of-distribution data (children), (2) challenges posed by severe object-occlusion, (3) difficulties arising from person-person occlusion, and (4) the potential for prediction errors in multi-person scenarios, as exemplified by the cases detailed in Appendix G. These challenges represent important avenues for future refinement of our approach.

There are several potential avenues for future research. First, the current approach does not deliberately select negative samples during training. Future work could explore if hard mining by intentionally selecting similar poses in a batch could enhance learning. Second, the careful selection of augmentations is essential. While augmentations that modify the global orientation, such as flipping and rotation, have proven detrimental and are not employed, the effects of individual augmentations and their combinations are not examined. Future research could explore the potential for automatically determining the optimal selection of augmentations to achieve improved performance. Additionally, simplifying the complex framework without sacrificing performance is a beneficial direction for future work. Lastly, considering that videos are a prevalent input format, the integration of video-based estimation can contribute to bolstering model robustness can enhance model robustness, alleviate depth ambiguity, and improve temporal consistency.

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline  & Method & Normal & Transs +0.2x & Transs -0.2x & Transs +0.2y & Transs -0.2y & Scale 1.3x & Scale 0.7x \\ \hline \multirow{4}{*}{Hands} & ExPose [6] & 14.39 & 17.36 & 17.86 & 14.93 & 17.21 & 14.15 & 14.56 \\  & PIXIE [11] & 14.68 & 15.05 & 16.11 & 15.32 & 15.85 & 14.52 & 14.79 \\  & Hand4Whole [35] & 10.83 & 11.15 & 11.34 & 10.50 & 13.70 & 10.77 & 11.25 \\  & OSX [27] & 15.97 & 16.42 & 16.55 & 16.94 & 17.86 & 15.91 & 17.24 \\  & RoboSMPLX & **10.00** & **10.37** & **10.21** & **10.16** & **12.49** & **9.98** & **10.19** \\ \hline \multirow{4}{*}{Face} & ExPose [6] & 6.34 & 10.28 & 6.71 & 8.17 & 6.43 & 6.24 & 6.24 \\  & PIXIE [11] & 5.63 & 6.67 & 6.94 & 6.53 & 6.94 & 5.84 & 5.84 \\  & HandWhole [35] & 5.81 & 5.88 & 5.91 & 5.74 & 5.93 & 5.76 & 5.76 \\  & OSX [27] & 6.09 & 6.03 & 6.09 & 5.83 & 5.96 & 5.92 & 5.92 \\  & RoboSMPLX & **4.65** & **5.10** & **5.38** & **4.75** & **5.30** & **4.77** & **5.22** \\ \hline \multirow{4}{*}{Wholebody} & ExPose [6] & 54.82 & 61.64 & 65.98 & 65.03 & 65.98 & 54.03 & 59.23 \\  & PIXIE [11] & 54.85 & 66.16 & 69.26 & 64.83 & 69.26 & 56.28 & 60.31 \\  & Hand4Whole [35] & 50.37 & 59.10 & 67.85 & 64.64 & 67.85 & 48.10 & 55.28 \\  & OSX [27] & **48.79** & **51.09** & 55.96 & 95.97 & **55.96** & **47.35** & **50.89** \\  & RoboSMPLX & 49.79 & 52.46 & **53.62** & **61.65** & 63.99 & 47.90 & 51.39 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Wholebody, Hand and Face PA-PVE errors under different positional augmentations.**

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline Scale & \multicolumn{1}{c}{Mean} & Std. [3] & Representation & P\({}_{\text{a}}\) & MPJPE\({}_{\text{b}}\) & PA & P\({}_{\text{c}}\) & P\({}_{\text{d}}\) & P\({}_{\text{e}}\) \\ \hline SunCLR (+ pose-variant angle) & 0.227 & 0.0913 & baseline & 7.49 & 15.51 & 7.46 & 15.59 & Augmentation & P\({}_{\text{a}}\) & MPJPE\({}_{\text{b}}\) & PA & P\({}_{\text{c}}\) \\ \hline SunCLR (+ pose-variant angle) & 0.220 & 0.0911 & pose & 8.11 & 15.87 & 7.64 & 16.08 & baseline (to +-eye) & 8.11 & 15.81 & 7.67 & 16.08 \\ SunCLR (+ \(L_{max}\)) & 0.222 & 0.0929 & 0.090 & pose & 7.11 & 14.98 & 7.54 & 14.91 & baseline & **7.42** & 15.01 & **7.18** & 14.91 \\  & SimCLR (+ \(L_{max}\)) & 0.164 & 0.0823 & keypoint & 7.48 & 15.01 & 7.32 & 15.29 & pose & 5.59 & 16.36 & 15.15 & 17.21 \\  & HDR (+ \(L_{max}\)) & 0.124 & **0.08634** & pose, +-eye & 7.45 & 14.94 & 7.20 & **14.47** & location & 7.00 & 13.89 & 7.46 & 15.56 \\  & HDR (+ \(L_{max}\)) & **0.119** & 0.0679 & keypoint, +-eye & **7.31** & **14.62** & **7.18** & 15.01 & cycle +- location & 7.45 & **14.94** & 7.20 & **14.47** \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Ablation of contrastive learning methods and loss.**

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline Method & Normal & Transs +0.2x & Transs -0.2x & Transs +0.2y & Transs -0.2y & Scale 1.3x & Scale 0.7x \\ \hline \multirow{4}{*}{Hands} & ExPose [6] & 14.39 & 17.36 & 17.86 & 14.93 & 17.21 & 14.15 & 14.56 \\  & PIXIE [11] & 14.68 & 15.05 & 16.11 & 15.32 & 15.85 & 14.52 & 14.79 \\  & Hand4Whole [35] & 10.83 & 11.15 & 11.34 & 10.50 & 13.70 & 10.77 & 11.25 \\  & OSX [27] & 15.97 & 16.42 & 16.55 & 16.94 & 17.86 & 15.91 & 17.24 \\  & RoboSMPLX & **10.00** & **10.37** & **10.21** & **10.16** & **12.49** & **9.98** & **10.19** \\ \hline \multirow{4}{*}{Face} & ExPose [6] & 6.34 & 10.28 & 6.71 & 8.17 & 6.43 & 6.24 & 6.24 \\  & PIXIE [11] & 5.63 & 6.67 & 6.94 & 6.53 & 6.94 & 5.84 & 5.84 \\  & HandWhole [35] & 5.81 & 5.88 & 5.91 & 5.74 & 5.93 & 5.76 & 5.76 \\  & OSX [27] & 6.09 & 6.03 & 6.09 & 5.83 & 5.96 & 5.92 & 5.92 \\  & RoboSMPLX & **4.65** & **5.10** & **5.38** & **4.75** & **5.30** & **4.77** & **5.22** \\ \hline \multirow{4}{*}{Wholebody} & ExPose [6] & 54.82 & 61.64 & 65.98 & 65.03 & 65.98 & 54.03 & 59.23 \\  & PIXIE [11] & 54.85 & 66.16 & 69.26 & 64.83 & 69.26 & 56.28 & 60.31 \\ \cline{1

## Acknowledgements

This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-PhD-2023-08-049T). This study is also supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP20221-0012), NTU NAP, and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). We sincerely thank the anonymous reviewers for their valuable comments on this paper.

## References

* Andriluka et al. [2014] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2D human pose estimation: New benchmark and state of the art analysis. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 3686-3693, 2014. ISSN 10636919. doi: 10.1109/CVPR.2014.471.
* 2023 IEEE Winter Conference on Applications of Computer Vision, WACV 2023_, pp. 65-74, 2023. doi: 10.1109/WACV56688.2023.00015.
* Boukhayma et al. [2019] Adnane Boukhayma, Rodrigo De Bem, and Philip H.S. Torr. 3D hand shape and pose from images in the wild. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, 2019-June:10835-10844, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.01110.
* Choi et al. [2020] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 12352 LNCS:769-787, 2020. ISSN 16113349. doi: 10.1007/978-3-030-58571-6_45.
* Choi et al. [2023] Hongsuk Choi, Hyeongjin Nam, Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee. Rethinking Self-Supervised Visual Representation Learning in Pre-training for 3D Human Pose and Shape Estimation. _International Conference on Learning Representations (ICLR)_, pp. 1-18, 2023. URL http://arxiv.org/abs/2303.05370.
* Choutas et al. [2020] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Monocular expressive body regression through body-driven attention. In _European Conference on Computer Vision (ECCV)_, 2020. URL https://expose.is.tue.mpg.de.
* Contributors [2021] MMHuman3D Contributors. Openmmlab 3d human parametric model toolbox and benchmark. https://github.com/open-mmlab/mmhuman3d, 2021.
* Danecek et al. [2022] Radek Danecek, Michael Black, and Timo Bolkart. EMOCA: Emotion Driven Monocular Face Capture and Animation. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, 2022-June:20279-20290, 2022. ISSN 10636919. doi: 10.1109/CVPR52688.2022.01967.
* Deng et al. [2019] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3D face reconstruction with weakly-supervised learning: From single image to image set. _IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops_, 2019-June:285-295, 2019. ISSN 21607516. doi: 10.1109/CVPRW.2019.00038.
* Dwivedi et al. [2021] Sai Kumar Dwivedi, Nikos Athanasiou, Muhammed Kocabas, and Michael J. Black. Learning to Regress Bodies from Images using Differentiable Semantic Rendering. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 11230-11239, 2021. ISBN 9781665428125. doi: 10.1109/iccv48922.2021.01106.
* Feng et al. [2021] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Collaborative regression of expressive bodies using moderation. In _International Conference on 3D Vision (3DV)_, 2021.

* Feng et al. [2021] Yao Feng, Haiwen Feng, Michael J. Black, and Timo Bolkart. Learning an animatable detailed 3D face model from in-the-wild images. In _ACM Transactions on Graphics, (Proc. SIGGRAPH)_, volume 40, 2021. URL https://doi.org/10.1145/3450626.3459936.
* Ionescu et al. [2014] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6M. _Ieee Transactions on Pattern Analysis and Machine intelligence_, pp. 1, 2014. ISSN 01628828. URL http://109.101.234.42/documente/publications/1-82.pdf.
* Jin et al. [2020] Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, and Ping Luo. Whole-Body Human Pose Estimation in the Wild. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 12354 LNCS:196-214, 2020. ISSN 16113349. doi: 10.1007/978-3-030-58545-7_12.
* Joo et al. [2018] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 8320-8329, 2018. ISSN 10636919. doi: 10.1109/CVPR.2018.00868.
* 2021 International Conference on 3D Vision, 3DV 2021_, pp. 42-52, 2021. doi: 10.1109/3DV53792.2021.00015.
* Kanazawa et al. [2018] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-End Recovery of Human Shape and Pose. In _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 7122-7131, 2018. ISBN 9781538664209. doi: 10.1109/CVPR.2018.00744.
* Karras et al. [2021] Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adversarial Networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(12):4217-4228, 2021. ISSN 19393539. doi: 10.1109/TPAMI.2020.2970919.
* Khosla et al. [2020] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _arXiv preprint arXiv:2004.11362_, 2020.
* Kocabas et al. [2021] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, and Michael J. Black. PARE: Part attention regressor for 3D human body estimation. In _Proc. International Conference on Computer Vision (ICCV)_, pp. 11127-11137, October 2021.
* Kocabas et al. [2021] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch, Lea Muller, Otmar Hilliges, and Michael J. Black. SPEC: Seeing people in the wild with an estimated camera. In _Proc. International Conference on Computer Vision (ICCV)_, pp. 11035-11045, October 2021.
* Kolotouros et al. [2019] Nikos Kolotouros, Georgios Pavlakos, Michael Black, and Kostas Daniilidis. Learning to reconstruct 3D human pose and shape via model-fitting in the loop. _Proceedings of the IEEE International Conference on Computer Vision_, 2019-Octob:2252-2261, 2019. ISSN 15505499. doi: 10.1109/ICCV.2019.00234.
* Kolotouros et al. [2019] Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis. Convolutional mesh regression for single-image human shape reconstruction. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, 2019-June:4496-4505, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00463.
* Kolotouros et al. [2021] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, and Kostas Daniilidis. Probabilistic Modeling for Human Mesh Recovery. In _International Conference on Computer Vision (ICCV)_, pp. 11585-11594, 2021. ISBN 9781665428125. doi: 10.1109/iccv48922.2021.01140.
* Kong et al. [2023] Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Robo3d: Towards robust and reliable 3d perception against corruptions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 19994-20006, 2023.

* Li et al. [2017] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4D scans. _ACM Transactions on Graphics, (Proc. SIGGRAPH Asia)_, 36(6):194:1-194:17, 2017. URL https://doi.org/10.1145/3130800.3130813.
* Lin et al. [2023] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. One-stage 3d whole-body mesh recovery with component aware transformer. _arXiv preprint arXiv:2303.16160_, 2023.
* Lin et al. [2021] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-End Human Pose and Mesh Reconstruction with Transformers. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 1954-1963, 2021. ISSN 10636919. doi: 10.1109/CVPR46437.2021.00199.
* Lin et al. [2014] Tsung Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 8693 LNCS(PART 5):740-755, 2014. ISSN 16113349. doi: 10.1007/978-3-319-10602-1_48.
* Liu et al. [2023] Qiaho Liu, Adam Kortylewski, and Alan L Yuille. Poseexaminer: Automated testing of out-of-distribution robustness in human pose and shape estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 672-681, 2023.
* Loper et al. [2015] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. _ACM Trans. Graphics (Proc. SIGGRAPH Asia)_, 34(6):248:1-248:16, October 2015.
* Mollahosseini et al. [2019] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild. _IEEE Transactions on Affective Computing_, 10(1):18-31, 2019. ISSN 19493045. doi: 10.1109/TAFFC.2017.2740923.
* Moon and Lee [2020] Gyeongsik Moon and Kyoung Mu Lee. I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 12352 LNCS:752-768, 2020. ISSN 16113349. doi: 10.1007/978-3-030-58571-6_44.
* Moon et al. [2020] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image. In _European Conference on Computer Vision (ECCV)_, 2020.
* Moon et al. [2022] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Accurate 3d hand pose estimation for whole-body 3d human mesh estimation. In _Computer Vision and Pattern Recognition Workshop (CVPRW)_, 2022.
* Moon et al. [2022] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. NeuralAnnot: Neural Annotator for 3D Human Mesh Training Sets. _IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops_, 2022-June:2298-2306, 2022. ISSN 21607516. doi: 10.1109/CVPRW56347.2022.00256.
* 2018 International Conference on 3D Vision, 3DV 2018_, pp. 484-494, 2018. doi: 10.1109/3DV.2018.00062.
* Pang et al. [2022] Hui En Pang, Zhongang Cai, Lei Yang, Tianwei Zhang, and Ziwei Liu. Benchmarking and analyzing 3d human pose and shape estimation beyond algorithms. In _NeurIPS_, 2022.
* Patel et al. [2021] Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann, Shashank Tripathi, and Michael J. Black. AGORA: Avatars in geography optimized for regression analysis. In _Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)_, June 2021.

* Pavlakos et al. [2018] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas Daniilidis. Learning to Estimate 3D Human Pose and Shape from a Single Color Image. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 459-468, 2018. ISSN 10636919. doi: 10.1109/CVPR.2018.00055.
* Pavlakos et al. [2019] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image. In _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, volume 2019-June, pp. 10967-10977, 2019. ISBN 9781728132938. doi: 10.1109/CVPR.2019.01123.
* Rempe et al. [2021] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J Guibas. Humor: 3d human motion model for robust pose estimation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 11488-11499, 2021.
* Romero et al. [2017] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. _ACM Transactions on Graphics, (Proc. SIGGRAPH Asia)_, 36(6), November 2017.
* Rong et al. [2021] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration. In _Proceedings of the IEEE International Conference on Computer Vision_, volume 2021-Octob, pp. 1749-1759, 2021. ISBN 9781665401913. doi: 10.1109/ICCVW54120.2021.00201.
* Sanyal et al. [2019] Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael Black. Learning to regress 3d face shape and expression from an image without 3d supervision. In _Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* Spurr et al. [2021] Adrian Spurr, Aneesh Dahiya, Xi Wang, Xucong Zhang, and Otmar Hilliges. Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. _Proceedings of the IEEE International Conference on Computer Vision_, pp. 11210-11219, 2021. ISSN 15505499. doi: 10.1109/ICCV48922.2021.01104.
* Sun et al. [2018] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen Wei. Integral human pose regression. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 11210 LNCS:536-553, 2018. ISSN 16113349. doi: 10.1007/978-3-030-01231-1_33.
* Tian et al. [2022] Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang. Recovering 3D Human Mesh from Monocular Images: A Survey. _arXiv preprint arXiv:2203.01923_, pp. 1-20, 2022. URL http://arxiv.org/abs/2203.01923.
* Tung et al. [2017] Hsiao Yu Fish Tung, Hsiao Wei Tung, Ersin Yumer, and Katerina Fragkiadaki. Self-supervised learning of motion capture. _Advances in Neural Information Processing Systems_, 2017-December(Nips):5237-5247, 2017. ISSN 10495258.
* von Marcard et al. [2018] Timo von Marcard, Roberto Henschel, Michael J. Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3D human pose in the wild using IMUs and a moving camera. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 11214 LNCS:614-631, 2018. ISSN 16113349. doi: 10.1007/978-3-030-01249-6_37.
* Wang et al. [2021] Jiahang Wang, Sheng Jin, Wentao Liu, Weizhong Liu, Chen Qian, and Ping Luo. When human pose estimation meets robustness: Adversarial algorithms and benchmarks. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 11850-11859, 2021. ISSN 10636919. doi: 10.1109/CVPR46437.2021.01168.
* Wang and Deng [2020] Mei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware reinforcement learning. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 9319-9328, 2020. ISSN 10636919. doi: 10.1109/CVPR42600.2020.00934.

* Xiang et al. [2019] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, 2019-June:10957-10966, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.01122.
* Xu et al. [2020] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T. Freeman, Rahul Sukthankar, and Cristian Sminchisescu. GHUM GHUML: Generative 3D human shape and articulated pose models. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 6183-6192, 2020. ISSN 10636919. doi: 10.1109/CVPR42600.2020.00622.
* Xu et al. [2019] Yuanlu Xu, Song Chun Zhu, and Tony Tung. DenseRaC: Joint 3D pose and shape estimation by dense render-and-compare. _Proceedings of the IEEE International Conference on Computer Vision_, 2019-Octob:7759-7769, 2019. ISSN 15505499. doi: 10.1109/ICCV.2019.00785.
* Yang et al. [2020] Lei Yang, Qingqiu Huang, Huaiyi Huang, Linning Xu, and Dahua Lin. Learn to propagate reliably on noisy affinity graphs. In _European Conference on Computer Vision_, pp. 447-464. Springer, 2020.
* Zanfir et al. [2020] Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, William T. Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 12351 LNCS:465-481, 2020. ISSN 16113349. doi: 10.1007/978-3-030-58539-6_28.
* Zhang et al. [2023] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards well-aligned full-body model regression from monocular images. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* Zhang et al. [2020] Tianshu Zhang, Buzhen Huang, and Yangang Wang. Object-Occluded Human Shape and Pose Estimation from a Single Color Image. _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, pp. 7374-7383, 2020. ISSN 10636919. doi: 10.1109/CVPR42600.2020.00740.
* Zhang et al. [2022] Yumeng Zhang, Li Chen, Yufeng Liu, Xiaoyan Guo, Wen Zheng, and Junhai Yong. Improving robustness for pose estimation via stable heatmap regression. _Neurocomputing_, 492:322-342, 2022. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2022.04.046. URL https://www.sciencedirect.com/science/article/pii/S0925231222004131.
* Zhou et al. [2021] Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian Theobalt, and Feng Xu. Monocular Real-time Full Body Capture with Inter-part Correlations. In _Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, number 61822111 in 61822111, pp. 4809-4820, 2021. ISBN 9781665445092. doi: 10.1109/CVPR46437.2021.00478.
* Zimmermann et al. [2019] Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max J. Argus, and Thomas Brox. FreiHAND: A dataset for markerless capture of hand pose and shape from single rgb images. _Proceedings of the IEEE International Conference on Computer Vision_, 2019-October:813-822, 2019. ISSN 15505499. doi: 10.1109/ICCV.2019.00090.
* Zimmermann et al. [2021] Christian Zimmermann, Max Argus, and Thomas Brox. Contrastive Representation Learning for Hand Shape Estimation. _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)_, 13024 LNCS:250-264, 2021. ISSN 16113349. doi: 10.1007/978-3-030-92659-5_16.