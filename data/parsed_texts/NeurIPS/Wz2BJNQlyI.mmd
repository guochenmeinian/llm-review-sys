# VisAlign: Dataset for Measuring the Alignment

between AI and Humans in Visual Perception

 Jiyoung Lee\({}^{1}\), Seungho Kim\({}^{1}\), Seunghyun Won\({}^{2}\), Joonseok Lee\({}^{3}\), Marzyeh Ghassemi\({}^{4,5,6}\)

James Thorne\({}^{1}\), Jaeseok Choi\({}^{7}\), O-Kil Kwon\({}^{7}\), Edward Choi\({}^{1}\)

\({}^{1}\)KAIST, \({}^{2}\)Seoul National University Bundang Hospital, \({}^{3}\)Seoul National University,

\({}^{4}\)MIT, \({}^{5}\)University of Toronto, \({}^{6}\)Vector Institute, \({}^{7}\)Kangwon National University Hospital

\({}^{1}\){jiyounglee0523, shokim, thorne, edwardchoi}@kaist.ac.kr

\({}^{2}\)shwon0213@gmail.com, \({}^{3}\)joonseok2010@gmail.com, \({}^{4}\)mghassem@mit.edu

\({}^{7}\){gobibobia, okkwon}@kangwon.ac.kr

###### Abstract

AI alignment refers to models acting towards human-intended goals, preferences, or ethical principles. In this paper, we focus on the models' visual perception alignment with humans, further referred to as _AI-human visual alignment_. Specifically, we propose a new dataset for measuring _AI-human visual alignment_ in terms of image classification. In order to evaluate _AI-human visual alignment_, a dataset should encompass samples with various scenarios and have gold human perception labels. Our dataset consists of three groups of samples, namely _Must-Act_ (_i.e._, Must-Classify), _Must-Abstain_, and _Uncertain_, and further divided into eight categories. All samples have a gold human perception label; even _Uncertain_ (_e.g._, severely blurry) sample labels were obtained via crowd-sourcing. The validity of our dataset is verified by sampling theory, statistical theories related to survey design, and experts in the related fields. Using our dataset, we analyze the visual alignment and reliability of five popular visual perception models and eight abstention methods. Our code and data is available at https://github.com/jiyounglee-0523/VisAlign.

## 1 Introduction

AI alignment [62] seeks to align models to act towards human-intended goals [48, 78], preferences [66, 61], or ethical principles [28]. Misaligned models may show unexpected and unsafe behaviors which can bring about negative outcomes, including loss of human lives [54, 78]. This is particularly true for high-capacity models like deep neural networks, where there is little manual control of feature interaction. In such cases, analyzing the alignment between models and humans can be a proxy measure for safe behavior [45]. In this paper, we particularly focus on alignment in _visual_ perception, referred to as _AI-human visual alignment_, and propose a new dataset for measuring this alignment. Note that recent work in AI-human alignment tends to focus on societal topics with ethical implications, such as racial or gender bias [70, 12, 42]. In this work, however, we use image classification as the target task, which is more fundamental to machine perception but is less contentious.

Image classification presents significant challenges for deployed visual AI systems. When confronted with an image lacking any object from the designated classes, humans typically abstain from making an incorrect decision. In contrast, machine learning models may still generate an output unless they are explicitly trained to abstain from making predictions under certain confidence levels. Similarly, when an image provides imperfect information (_e.g._, due to blurred vision or a dark environment), human decisions tend to waver between a correct prediction and abstention. Conversely, machines oftenmake overconfident predictions [46]. Given this discrepancy between human and model behaviors, we focus on image classification as a foundational starting point.

As AI alignment aims to guide an AI to resemble human behaviors and values for a safe use of AI, _AI-human visual alignment_, being a subcategory of AI alignment, aims to guide the AI to resemble the aforementioned human behaviors in visual perception (_i.e._, abstaining from making incorrect decisions, waering between a correct prediction and abstention) to ensure safety across diverse use cases. Our dataset, VisAlign, encapsulates these behaviors across three distinct groups: _Must-Act_, _Must-Abstain_, and _Uncertain_. _Must-Act_ contains identifiable photo-realistic images that humans can correctly classify (see Figure 1 green box). _Must-Abstain_ includes images that most humans would abstain from classifying due to their lack of photo-realism or because they clearly contain no objects within the target classes (see Figure 1 red box). _Uncertain_ category hosts images that have been cropped or corrupted in diverse ways and at varying intensities (see Figure 1 orange box). For this last group, we provide gold human labels from multiple annotators via crowd-sourcing. In Section 3, we elaborate on requirements that a visual alignment dataset must meet and provide details about our survey design, which has been validated using relevant statistical theories. _Must-Act_ and _Must-Abstain_ have been addressed in previous studies under the purview of robustness [22; 72; 25] and Out-of-Distribution Detection (OOD) [50; 74], respectively. However, most studies overlook _Uncertain_ samples, which are frequently found in real-world scenarios where visual input can continuously vary in aspects such as brightness and resolution. To the best of our knowledge, VisAlign is the first dataset to explore the diverse aspects of visual perception, including _Uncertain_ samples, under the concept of _AI-human visual alignment_. Furthermore, all decisions regarding the construction of VisAlign were based strictly on statistical methods for survey design [64; 9] and expert consultations to maximize the validity of the alignment measure (see Section 3).

We benchmark various image classification methods on our dataset using two different metrics. Firstly, we measure the visual alignment between the gold human label distribution and the model's output distribution using the distance-based method (Section 4.1). Secondly, we evaluate the model's _reliability score_ (Section 4.2). We test models with various architectures, each combined with various ad-hoc abstention functions that endow the model with the ability to abstain. Our findings suggest that current robustness and OOD detection methods cannot be directly applied to _AI-human visual alignment_, thus highlighting the unique challenges posed by our task as compared to conventional ones.

Our contributions can be summarized as follows:

Figure 1: The overview of VisAlign. The example images are given with reference to the class Zebra. _Category 1_. A photo-realistic image of a zebra. _Category 2_. A zebra crossing a road. _Category 3_. A slight noise is added to the Category 1 image. _Category 4_. A picture of a truck. _Category 5_. A head and two limbs of an elephant with the remaining body of a zebra. _Category 6_. A donkey. _Category 7_. A zebra illustrated on a piece of clothing. _Category 8_. Two pictures, one with cropping and the other frosted glass blur, respectively, of a zebra.

* To the best of our knowledge, this is the first work to construct a test benchmark for quantitatively measuring the visual perception alignment between models and humans, referred to as _AI-human visual alignment_, across diverse scenarios (8 categories in total).
* We propose VisAlign, a dataset that captures varied real-world situations and includes gold human labels. The construction of our dataset was carried out meticulously, adhering to statistical methods in survey designs (_i.e_., the number of samples in a dataset [9], intra and inter-consistency in surveys [15], and the required minimum number of participants [64]) and expert consultations.
* We benchmarked visual alignment and reliability on VisAlign using five baseline models and seven popular abstention functions. The results underscore the inadequacy of existing methods in the context of visual alignment and emphasize the need for novel approaches to address this specific task.

## 2 Related Works

Related Datasets.Previous datasets only focus on one aspect or do not have human gold labels. Mazeika et al. [41] focus on subjective interpretations and collected human annotations on emotions (_e.g_., amusement, interest, adoration). Existing corruptions datasets [22; 43; 72] apply slight corruptions to study the robustness of deep neural networks. These works overlook the moderately or severely corrupted images that appear in the real world. Although the dataset by Park et al. [49] applied brightness corruptions on hand X-ray images with multiple severities, they do not have gold human labels. CIFAR10H [52] is a dataset that collects a distribution of soft human labels for CIFAR10 images [30] to represent human perceptual uncertainty. Similarly, Schmarje et al. [65] collected multiple annotations per image. There are three key differences that distinguish our dataset from prior works that focus on uncertainty in object recognition. First, we applied corruption and cropping with different intensities ranging from 1 to 10 to reflect the continuity of uncertainty. As uncertainty is continuous and it is critical to test models on samples where uncertainty may increase in stages. Second, we obtained 134 human annotations per image to obtain numerically robust annotations. Third, while previous dataset include soft labels distributed only among classes, we include soft labels distributed among classes and abstention, which can represent recognizability uncertainty (_i.e_.,, whether an image itself is recognizable or not). Visual perception includes not only object identification (predicting that it is an elephant) but also object recognizability (the object itself is recognizable). In this sense, we cover broader scenarios compared to previous works as we include object recognizability uncertainty in our uncertain category.

Visual Alignment with Humans.Alignment is more broadly studied, including the gap between data collection and model deployment [2], natural language modeling [37], and object similarity [29; 51]. For visual alignment, specifically, previous works [18; 19; 53; 77] use only corrupted or perturbed datasets to compare the humans' and models' decisions. Zhang et al. [76] and Bomatter et al. [5] show that both model and human have better object recognition when given more context information. Both papers provided human-model correlations to describe their relative trends across conditions. However, our study on visual perception alignment is not about following human trends, but about measuring how well the model replicates human perception sample-wise. Geirhos et al. [17] and Bhojanapalli et al. [4] test the robustness of models to perturbations that does not affect the object identity. Peterson et al. [52] only test their models on in-class (_i.e_., Category 1) and out-of-class samples (_i.e_., Category 4 and Category 6) and Schmarje et al. [65] only tested their models on in-class samples (_i.e_., Category 1). In order to thoroughly evaluate visual alignment, models should also be tested under various scenarios with out of distribution properties (_i.e_., Category 5 and Category 7). We prepared VisAlign to include these out of distribution properties, and if needed, generated the samples by ourselves, of which details are in Section 3.2. Furthermore, they showed only accuracy and cross entropy or KL divergence. (which is analogous to KL divergence) of the models. Therefore, they did not test their models on various possible scenarios and did not use proper measurement, as KL divergence is not an optimal choice for visual perception alignment as will be described in Section 4.1. Therefore, although previous works trained their models with the goal of achieving visual perception alignment, none of the works have thoroughly verified how much the models have actually achieved visual perception alignment under diverse situations with an appropriate measurement. In contrast, we quantitatively measured visual perception alignment across various scenarios with multiple human annotations on uncertain images. In addition, we borrowed Hellinger distance to precisely calculate the visual perception alignment after careful consideration of other distance-based metrics. More details of comparison to previous works are in Appendix J

## 3 Dataset Construction

We have carefully considered what conditions must be met in a visual alignment dataset during the process of selecting the classes and the contents of VisAlign. We define four requirements that a visual alignment dataset must satisfy:

* **Requirement 1: Clear Definition of Each Class.** Each class must be distinctly and precisely defined. This criterion proves more challenging to meet than initially anticipated, given that most everyday objects are defined in relatively vague terms and therefore do not lend themselves to rigorous classification. For example, the term "automobile," which is defined by the Cambridge Dictionary as a synonym for "car", is described as "a vehicle with an engine, four wheels, and seats for a few people."1 The phrase "seats for a few people" is ambiguous, and the definition is broad enough to encompass trucks. Despite this, certain parties may contend that "automobile" and "truck" are distinctly separate classes, a view reflected in datasets like CIFAR-10 [30] and STL-10 [8], which treat automobiles and trucks as separate classes. Footnote 1: https://dictionary.cambridge.org/dictionary/english/car
* **Requirement 2: Class Familiarity to Average Individuals.** The classification target (_i.e._, each class) must be known to average people. This is because we employ hundreds of MTurk workers to derive statistically robust ground-truth labels for a subset of images.
* **Requirement 3: Coverage of Diverse and Realistic Scenarios.** Samples must cover a wide range of scenarios that are likely to occur in reality. This includes samples outside of defined classes, out of distributions (_i.e._, Category 5 or 7) and confusing samples where people might not able to recognize or identify. The test will fail to sufficiently evaluate the AI's alignment with human visual perception without this diversity.
* **Requirement 4: Ground Truth Label for Each Sample.** Each sample must have an indisputable or, at the very least, reasonable ground truth. Our dataset's ground truth is human-derived, as we aim to measure the degree of alignment between AI and human visual perception.

### Class Selection

For our dataset to serve as a universal benchmark that any model can be tested on, the classes should have clear definitions so that model developers can easily prepare their models and training strategy. To meet Requirement 1, we cannot choose under-specified class definitions. For example, the class definitions in CIFAR10 [30] can be disputed, as shown in the example of 'automobile' and 'truck' in Requirement 1. MNIST [34] classes cannot be used since numbers are recognized via trivial geometric patterns. After careful consideration, we use the taxonomic classification in biology, which is the meticulous product of decades of effort by countless domain experts to hierarchically distinguish each species as accurately as possible. Following Requirement 2, familiarity is one of the critical criteria since we conducted an MTurk survey to build a subset of our dataset. Therefore, among animal species, we select mammals that are familiar to the average person.

In summary, animal species were selected that 1) can be grouped under one scientific name for clear definitions, 2) are visually distinguishable from other species to avoid multiple correct answers, 3) have characteristic visual features allowing them to be identified by a single image, and 4) are familiar to humans, facilitating participation in our survey. The final 10 classes are _Tiger_, _Rhinoceros_, _Camel_, _Giraffe_, _Elephant_, _Zebra_, _Gorilla_, _Kangaroo_, _Bear_, and _Human_. This selection was revised and verified by two zoologists according to the aforementioned criteria. The scientific names and subspecies for each class can be found in Table 4 of Appendix C.

### Sample Categories

Our dataset, depicted in Figure 1, is partitioned into three groups: _Must-Act_, _Must-Abstain_, and _Uncertain_. To avoid misclassifications due to background objects, all samples exclusively contain one object. The authors manually scrutinized all test samples to ensure this. In line with Requirement 3, these three groups are further subdivided into eight categories to account for as many real-worldscenarios as possible. Each category comprises 100 samples, with the exception of Category 8 comprising 2002, totaling 900 samples. To establish the reliability of the dataset as a valid benchmark, Cronbach's alpha [9] was used, a metric that evaluates the reliability of tests. The dataset was deemed reliable, with a minimum of 100 samples per category. The complete calculation for Cronbach's alpha is detailed in Appendix D.1.

Footnote 2: As category 8 contains a diverse set of croppings and corruptions of varying intensities, we double the number of samples for more reliable evaluation.

* contains clearly identifiable photo-realistic samples belonging to only one of the 10 classes. We intentionally restricted our dataset to photo-realistic samples to avoid ambiguous boundaries between in-class and out-of-class, such as abstract paintings or sculptures (_e.g._,, claiming that a box with four sticks at the bottom and a sinusoidal line on the side is an elephant). Individuals with no visual impairments and familiarity with the 10 mammals can consistently classify these images correctly.
* Category 1: Unaltered samples from the designated classes are included. This category serves as the most basic step required for visual perception alignment. We sourced images from ImageNet1K [60] and images.cv3. Footnote 3: https://images.cv/
* Category 2: Image classification models have been known to sometimes base decisions based on unrelated features, such as the background of an image [25, 57]. We aim to challenge the models by testing them with samples that feature incongruuous backgrounds, _i.e._,, images of animals in environments where they are not commonly seen. Well-aligned models should accurately classify objects regardless of the changes in the background. Samples were generated using Stable Diffusion [59]. Examples of text prompts used for generating samples are provided in the Appendix D.2.
* Category 3: Another case of images that humans can easily identify but models cannot are perturbed images used for adversarial attacks [20, 31]. Well-aligned models would not be influenced by noise or adversarial attacks intentionally designed to deceive them. Here we include Category 1 samples with adversarial perturbation to test such cases. We use Fast Gradient Sign Method (FGSM) [20] to inject adversarial perturbations. The gradients are produced by pre-trained image classifiers available in PyTorch4. Footnote 4: https://pytorch.org/

* are images that qualified individuals always abstain from classifying.
* Category 4: This category includes images that do not belong to any one of VisAlign's 10 mammals. Examples might include other animal species (e.g., birds, cats, dogs), textures (e.g., bubbly, banded), or objects (e.g., truck, inline skate, guitar). This category tests the model's ability to abstain from classifying objects outside its defined scope. Well-aligned models should be able to disregard infinitely diverse objects outside the target classes. The space of Category 4 is inexhaustible; thus, the authors use their best efforts to include as diverse samples as possible to represent this space. Samples were collected from ImageNet1K [60], Describable Textures Dataset [7], and Caltech 10 [14].
* Category 5: While Category 2 tests whether models focus on relevant features of the class definition, it is also important to assess if a model evaluates the object as a whole, rather than focusing on specific portions of a sample. Thus, we included images of creatures that incorporate features from two different animals (e.g., a creature with the head and two limbs of an elephant but the body of a zebra). Recent advances in text-to-image models [55, 56, 63] enable us to rapidly and easily generate images of objects that do not naturally exist. We used Stable Diffusion [59] to create these images. Details of prompts are in Appendix D.2.
* Category 6: An image may contain an object that does not belong to the target class but has features closely resembling those of the target classes. Given the challenging nature of these near-miss cases, we include Category 6, featuring mammals that are biologically close to the 10 target mammals according to scientific taxonomy (e.g., donkeys are close to zebras). The primary purpose of Category 6 is to test the model abstention ability on seemingly similar yet different samples. This category can be considered a more challenging version of Category 4. We have set aside this category as these samples can check the model visual alignment on samples near the natural evolutionary boundary. Samples are collected from ImageNet21K [58].
* Category 7: This category includes images in styles other than photo-realistic (e.g., a drawing of an elephant, a sculpture of a giraffe). Considering that MUST-ACT samples are photo-realistic images confirmed by humans, well-aligned models should be able to discern styles that deviate from photo-realism. The images were collected from DomainNet [50] and ImageNet-R [24].

* includes images that are cropped or corrupted in various styles in different intensities
* Category 8: This category includes images that are either cropped at varying sizes and regions or corrupted using one of the 15 corruption types5. The original samples were collected from ImageNet21K [58]. Well-aligned models should be able to correctly classify slightly corrupted images while abstaining from making decisions on indistinguishably corrupted images. The corruption process follows the approach outlined in ImageNet-C [22], with corruption intensities varying from 1 to 10. Footnote 5: We leveraged open-sourced code available at https://github.com/hendrycks/robustness

### Uncertain Group Label Generation

One challenging aspect of the _Uncertain_ group is the variability of these samples' gold standard labels, which fluctuates depending on corruption types and intensities. For instance, it would be optimal to correctly classify images with slight corruptions when identifiable. However, given a severely darkened image, the object might resemble a tiger, a jaguar, or be entirely unrecognizable. In such scenarios, determining whether a human observer would classify it as a tiger or abstain from decision-making becomes challenging. Therefore, we derive a gold human ratio (_i.e._, the distribution over classes provided by human annotators), rather than assigning one label per image as in _Must-Act_ and _Must-Abstain_, because human perception of an image can vary, and approximating the ratio for each image offers the best test of alignment6. To derive the gold ratio across the 11 classes (10 mammals + abstention), we employ MTurk workers to classify images in the _Uncertain_ group.

Footnote 6: Some might wonder why the machines should settle for aligning with human visual perception, rather than aiming to correctly classify even the most corrupted images (_i.e._ aim for superhuman visual perception). We provide arguments for the necessity of the former in Appendix D.2.2.

Every MTurk worker is asked to classify 35 images, including Category 4 images corrupted with a severity between 1 to 10, with 10 being distractors. This is to minimize MTurk workers' potential biases; _e.g._, a severely dark image can be perceived as anything other than the 10 mammals. After reviewing the task description and image samples for each class, MTurk workers select either one of the 10 mammals or an option labeled "None of the 10 mammals, uncertain, or unrecognizable", which is equivalent to abstention. To ensure the quality of samples, we disregard MTurk results where anything other than abstention was chosen for the distractor images.

In accordance with Requirement 4, we ask 134 individuals per image to estimate the indisputable ground truth distribution within an error bound of 5%, following the survey sampling theory. Proofs are provided in Appendix F. Additionally, we calculate the Fleiss' Kappa [15] to assess two types of consistency among the MTurk workers' answers: intra-annotator and inter-annotator consistency. Intra-annotator consistency measures the consistency of a single worker's responses. To calculate this, we inserted two sets of identical images in random order. If a worker selects the same answers for these identical images, we consider the worker's responses to be consistent. Inter-annotator consistency, on the other hand, measures the agreement among different workers. Our results show an intra-annotator consistency value of \(\kappa=0.91\), indicating almost perfect agreement, and an inter-annotator consistency value of \(\kappa=0.80\), demonstrating substantial agreement. Details on survey instructions, response filtering process, and participant statistics are provided in Appendix F.

### Dataset

We prepare three datasets: the train set, the open test set, and the closed test set. The train set is a subset of ImageNet-21K [58], consisting only of Category 1 samples. By doing so, we ensure the trained models are tested on a variety of unseen categories, reflecting a real-world scenario. For each of our 10 classes, we randomly sample a uniform amount of images from all related ImageNet-21K classes. We collected a total of 1250 images per class, using one-tenth of this data for validation. The creation processes of both the open and closed test sets are identical, as described above. We provide the open test set to allow developers to evaluate their models' visual perception alignment. Developerswishing to evaluate their models on the closed test set can submit their models to us. Table 1 presents a comparison of VisAlign and other datasets in terms of fulfilling the four requirements.

## 4 Metrics

We introduce a distance-based metric to measure _AI-human visual alignment_. Furthermore, we present a reliability score table to explore the correlation between a model's visual perception alignment and model reliability.

### Distance-Based Visual Perception Similarity Metric

We propose a distance-based metric to measure the distance between two multinomial distributions: the human visual distribution and the model output distribution over 11 classes (10 mammals + abstention). We opt for a distance-based metric for two reasons: 1) it does not depend on additional hyperparameters such as abstention threshold, and 2) comparison across all classes, rather than solely on the true class, provides a more accurate measure of visual alignment. For example, consider a _Must-Act_ tiger sample with the gold human label as a one-hot vector for the label _tiger_. Suppose one model outputs a probability of 0.7 for _tiger_ and 0.3 for _abstention_, and another model yields a probability of 0.7 for _tiger_ and 0.1 for _zebra_, _elephant_, and _giraffe_ respectively. These two models differ in visual perception alignment: the former is uncertain between two classes, whereas the latter is indecisive among four classes. If we were to consider only the gold label's probability, both models would yield the same result, which would not accurately represent visual alignment. Hence, we employ a distance-based metric calculated across all 11 classes, as opposed to using the maximum or gold label probability.

Specifically, we employ the Hellinger distance [47] to measure the difference between the two probability distributions as summarized in Eq. 1. Compared to other metrics for comparing two multinomial distributions, Hellinger distance produces smooth distance values even for extreme (_e.g_., one-hot) distributions (unlike KL Divergence [10]) and considers all classes while calculating the distance (unlike Total Variation distance). For instance, given a human visual distribution of [1., 0., 0.] and two model output distributions [0.3, 0., 0.7] and [0.3, 0.4, 0.3], the two output distributions would have the same KL Divergences with the human distribution while they have different Hellinger distances. Hellinger distance accounts not only for the gold label probability but also for the probabilities of all other labels. Additionally, as its range lies between 0 and 1, it provides an intuitive indication of model alignment.

\[h(P,Q)=\frac{1}{\sqrt{2}}\sum_{i}\lVert\sqrt{p}_{i}-\sqrt{q}_{i}\rVert_{2}\] (1)

### Reliability Score with Abstention

We also assess the model's reliability based on its final action. This process involves two steps. First, a model abstains if the abstention probability surpasses an abstention threshold, \(\gamma\); otherwise, it makes a prediction. Next, if a model decides to act, its prediction is one of the 10 mammal classes

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Dataset & Req. 1 & Req. 2 & Req. 3 & Req. 4 \\ \hline ImageNet-C [22] & ✗ & ✗ & \(\triangle\) & ✓ \\ ImageNet-A [25] & ✗ & ✗ & ✗ & ✓ \\ OpenOOD [74] & ✗ & ✗ & \(\triangle\) & ✓ \\ Background Challenge [73] & ✗ & ✗ & ✓ \\ MNIST [34] & ✗ & ✓ & ✗ & ✓ \\ CIFAR10 [30] & ✗ & ✓ & ✗ & ✓ \\ CIFAR10H [52] & ✗ & ✓ & \(\triangle\) & ✓ \\ PLEX [69] & ✗ & ✗ & ✓ & ✓ \\ Park et al. [49] & ✓ & ✗ & \(\triangle\) & ✗ \\ DCDC [65] & ✗ & ✗ & \(\triangle\) & ✓ \\ \hline VisAlign & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: The comparison between VisAlign and other related datasets on the requirements we define. \(\triangle\) indicates that only a subset of our scenarios are covered.

with the highest prediction probability. Table 2 details the reliability scores for each case. We devise separate metrics for _Must-Act_ and _Must-Abstain_ instances. For _Uncertain_ samples, they are treated as _Must-Act_ if the probability of the original label exceeds a threshold \(\lambda\); otherwise, they are treated as _Must-Abstain_. We set an initial \(\lambda\) value at 0.5, but this can be adjusted according to the specific objective. We denote the reliability score as \(RS_{c}(x)\), where \(c\) is the cost of an incorrect prediction. The main criterion for assigning scores is the consequences of the model's decision. The model earns a score of 1 per prediction when it aligns best with human recognition: making a correct prediction in Must-Act and abstaining in Must-Abstain. On the other hand, if the model's decision is erroneous and could potentially result in significant cost--in our case, a wrong prediction--the model receives a score of \(-c\). A score of zero indicates that the prediction is neither beneficial nor detrimental. Original Label Prediction is a special case only applied for Uncertain samples treated as Must-Abstain. In this case, a model correctly classifies a corrupted image that most humans cannot recognize. Although most humans disagree with the model's decision, it does not have a negative impact since it is a correct answer. The total score, \(RS_{c}\), is the summation over all test samples, \(\sum_{i}RS_{c}(x_{i})\).

The proper value of cost \(c\) depends on the industry and the use case. \(c\) can be seen as the "strictness criterion for a reliable model" and can also be interpreted as "how many misclassifications correspond to a single accurate classification." \(c\) can be set as an integer ranging from 0 to the total size of the test set. A value 0 for \(c\) implies a 0% strictness, while the maximum value of \(c\) implies a 100% strictness. This means that even a single mistake would result in a negative score, and abstaining from all decisions on Must-Act samples would be deemed more reliable than making even one incorrect prediction. We designed this metric to enable both absolute and relative reference points. As an absolute reference point, if the final score is at or above 0 (non-negative reliability score), it demonstrates that the model satisfies the user-defined minimum reliability. A relative reference point is between different models; a model with a higher score between two reliability scores is more reliable. In this paper, we set the value of \(c\) as 0, 450, or 900.

## 5 Experiment

### Experiment Settings

We perform experiments with Transformer-based [71], CNN-based [33], and MLP-based models. We use ViT [11] and Swin Transformer [38] for Transformer-based models, and DenseNet [27] and ConvNeXt [39] for CNN-based models. For the MLP-based model, we use MLP-Mixer [68]. All models are trained on our train set and tested on the open test set.

We chose abstention functions that satisfy the following three conditions: 1) must be applicable on any model architecture, 2) do not require OOD or other Must-Abstain samples during training, and 3) do not require a supplementary model. We first calculate the abstention probability using each function, then re-normalize the 10-class prediction probability so that the sum over the 11 classes becomes 1. Since not every function outputs the abstention probability between 0 and 1, we designed a smaller version of the dataset with the identical gather process to test set to use for normalizing the abstention probability.

* Softmax Probability (SP) regards the entropy among the 10 classes as abstention probability.
* Adjusted Softmax Probability (ASP) acts the same as SP, but it applies temperature scaling and adds perturbations to the input image based on the gradients to decrease the softmax score. This method is inspired by ODIN [26].
* Mahalanobis detector (MD) [35] determines abstention probability based on the minimum Mahalanobis distance [40] calculated from each class distribution's mean and variance.

\begin{table}
\begin{tabular}{c c c} \hline \hline Sample Type & Model Action & \(RS_{c}(x)\) \\ \hline \multirow{3}{*}{Must-Act} & Correct Prediction & \(+1\) \\ \cline{2-3}  & Incorrect Prediction & \(-e\) \\ \cline{2-3}  & Abstention & 0 \\ \hline \multirow{3}{*}{Must-Abstain} & Original Label Prediction* & \(0\) \\ \cline{2-3}  & Other Prediction & \(-e\) \\ \cline{1-1} \cline{2-3}  & Abstention & \(+1\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Reliability score table. The optimal outcomes earn a score of 1. Abstention in _Must-Predict_ and Original Label Prediction in _Must-Abstain_ get 0. The worst case receives \(-c\), where \(c\) is the cost value. *Note that the original label prediction can only happen in Uncertain samples that fall under Must-Abstain.

[MISSING_PAGE_FAIL:9]

similar visual alignment performances, predominantly ranging from 0.5 and 0.6. We conjecture the reason comes from that all models are struggling in approximating the overall ratios across 11 classes compared to _Must-Act_ and _Must-Abstain_, where models only need to correctly predict a single class. The difficulty of achieving visual perception alignment in _Uncertain_ suggests that there is room for improvement. KNN [67] has the best visual alignment across all categories on average. This might be because KNN can capture more fine-grained features than other distance-based abstention functions, as it calculates the distance between samples, not clusters. We also compute three reliability scores with \(c\) set to 0 (\(RS_{0}\)), 450 (\(RS_{450}\)), and 900 (\(RS_{900}\)). The resulting ratios of each action type are shown in Appendix G.1. Here, \(c=0\) indicates no negative impacts from incorrect predictions, while \(c=900\) suggests that a single incorrect prediction outweighs the remaining correct predictions. It is worth noting that reliability scores in \(RS_{450}\) and \(RS_{900}\) are mostly negative, suggesting that current models and abstention functions are not perfectly safe to be deployed in the real world. Notably, visual alignment distance is correlated with reliability score as can be seen in Appendix G.2.

Methods based on the minimum distance from each class (MD, KNN, and TAPUDD) generally show a worse visual alignment on _Must-Abstain_ categories. We conjecture that the reason comes from using the shortest distance to in-class clusters. If an embedding contains one clear in-class feature, the distance to the corresponding class would be short, leading the model to make a prediction. On the other hand, methods based on entropy or uncertainty show weak alignment on _Must-Act_ categories. With these methods, the model has to be not only confident that its predicted class is correct but also that the remaining classes are incorrect. Considering the confidence in all classes makes it more challenging for visual alignment in _Must-Act_ categories. An abstention function which takes advantage of both distance-based and probability-based methods is needed to perform well on visual alignment. The distance should be sample-wise to capture the nuanced characteristics of the samples. Overall, our experiments show that no methods perform well across all categories. There is much room for improvement in visual alignment, a field in which our dataset will become an essential tool for benchmarking new methods.

## 6 Conclusion

To the best of our knowledge, this is the first work to construct a test benchmark for quantitatively measuring the visual perception alignment between models and humans, referred to as _AI-human visual alignment_, across diverse scenarios. Our dataset is divided into three main groups and eight categories, each representing unique and essential situations. Our dataset includes gold human labels for each image, with some of these labels collected via MTurk survey. We benchmarked five baseline models and seven popular abstention functions, and our experimental results show that no current methods perform well across all categories. This suggests there is room for improvement in visual alignment. We believe VisAlign can serve as a universal benchmark for testing visual perception alignment and that our work has potential applications in both social and industrial contexts.

Despite our best efforts to construct VisAlign, there are some limitations. First, the number of classes is relatively small compared to other datasets since we collected 134 annotations per image and chose classes that would be familiar to an average human. Note that it is always challenging to collect gold human labels in any domain. For example, in diagnosing chest X-rays, the typical number of diseases is 14. To collect the ground truth labels within a statistical error bound of 5%, one would need to consult at least 107 radiologists. Therefore, more practical solutions are required to measure alignment in specialized domains. Another limitation comes from the nature of uncertainty. We acknowledge that uncertainty is continuous and it is hard to distinguish between clear and uncertain images. Although we put significant effort to include only clear images in Must-Act and Must-Abstain and obtained human annotations on Uncertain images, there is a possibility of corner cases where at least one person disagrees. Furthermore, synthetic corruptions cannot cover all uncertainties arising in the real world. However, uncertainty is too broad to specify and difficult to collect or generate, thus for now we use corruptions. We put our best effort to reflect the continuity of uncertainty by varying corruption intensity from 1 to 10 and include some corruptions that can arise in the real world (_e.g._, pixelation). We detailed further discussions on uncertainty in Appendix I. Also, extending visual alignment to scenarios such as visual illusions may also be introduced. While our dataset focuses on the essential object identification and abstention task under _AI-human visual alignment_, future work can be expanded to potentially contentious but socially engaging topics such as gender or racial bias and other vision tasks such as object detection and segmentation.

## Acknowledgments and Disclosure of Funding

This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No.2019-0-00075, No.2022-0-00984) and National Research Foundation of Korea (NRF) grant (NRF-2020H1D3A2A03100945, NRF-2021H1D3A2A03038607), funded by the Korea government (MSIT). Marzyeh Ghassemi receives support from the Herman L. F. von Helmholtz Career Development Professorship and CIFAR Azrieli Global Scholar award.

## References

* [1] Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. _arXiv preprint arXiv:2106.15831_, 2021.
* [2] Aparna Balagopalan, David Madras, David H Yang, Dylan Hadfield-Menell, Gillian K Hadfield, and Marzyeh Ghassemi. Judging facts, judging norms: Training machine learning models to judge humans requires a modified approach to labeling data. _Science Advances_, 9(19):eabq0701, 2023.
* [3] Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1563-1572, 2016.
* [4] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Dalianing Li, Thomas Unterthiner, and Andreas Veit. Understanding robustness of transformers for image classification. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10231-10241, 2021.
* [5] Philipp Bomatter, Mengmi Zhang, Dimitar Karev, Spandan Madan, Claire Tseng, and Gabriel Kreiman. When pigs fly: Contextual reasoning in synthetic and natural scenes. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 255-264, 2021.
* [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [7] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* [8] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011.
* [9] Lee J Cronbach. Coefficient alpha and the internal structure of tests. _psychometrika_, 16(3):297-334, 1951.
* [10] Imre Csiszar. I-divergence geometry of probability distributions and minimization problems. _The annals of probability_, pages 146-158, 1975.
* [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [12] Mengnan Du, Fan Yang, Na Zou, and Xia Hu. Fairness in deep learning: A computational perspective. _IEEE Intelligent Systems_, 36(4):25-34, 2020.
* [13] Radhika Dua, Seongjun Yang, Yixuan Li, and Edward Choi. Task agnostic and post-hoc unseen distribution detection. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1350-1359, 2023.
* [14] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. _Computer Vision and Pattern Recognition Workshop_, 2004.
* [15] Joseph L Fleiss and Jacob Cohen. The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability. _Educational and psychological measurement_, 33(3):613-619, 1973.
* [16] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _international conference on machine learning_, pages 1050-1059. PMLR, 2016.
* [17] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. _arXiv preprint arXiv:1811.12231_, 2018.
* [18] Robert Geirhos, Kristof Meding, and Felix A Wichmann. Beyond accuracy: quantifying trial-by-trial behaviour of cnns and humans by measuring error consistency. _Advances in Neural Information Processing Systems_, 33:13890-13902, 2020.
* [19] Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Partial success in closing the gap between human and machine vision. _Advances in Neural Information Processing Systems_, 34:23885-23899, 2021.

* [20] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* [21] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* [22] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _Proceedings of the International Conference on Learning Representations_, 2019.
* [23] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. _Advances in neural information processing systems_, 32, 2019.
* [24] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. _ICCV_, 2021.
* [25] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. _CVPR_, 2021.
* [26] Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10951-10960, 2020.
* [27] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [28] Geoffrey Irving and Amanda Askell. Ai safety needs social scientists. _Distill_, 4(2):e14, 2019.
* [29] Kamila M Jozwik, Nikolaus Kriegeskorte, Katherine R Storrs, and Marieke Mur. Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments. _Frontiers in psychology_, 8:1726, 2017.
* [30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [31] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale, 2017.
* [32] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* [33] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. _Neural Computation_, 1(4):541-551, 1989. doi: 10.1162/neco.1989.1.4.541.
* [34] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [35] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _Advances in neural information processing systems_, 31, 2018.
* [36] Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. Magicmix: Semantic mixing with diffusion models. _arXiv preprint arXiv:2210.16056_, 2022.
* [37] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. _arXiv preprint arXiv:2305.20050_, 2023.
* [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11976-11986, 2022.

* [40] Prasanta Chandra Mahalanobis. On the generalised distance in statistics. In _Proceedings of the national Institute of Science of India_, volume 12, pages 49-55, 1936.
* [41] Mantas Mazeika, Eric Tang, Andy Zou, Steven Basart, Jun Shern Chan, Dawn Song, David Forsyth, Jacob Steinhardt, and Dan Hendrycks. How would the viewer feel? estimating wellbeing from video scenarios. _Advances in Neural Information Processing Systems_, 35:18571-18585, 2022.
* [42] Chuizheng Meng, Loc Trinh, Nan Xu, James Enouen, and Yan Liu. Interpretability and fairness evaluation of deep learning models on mimic-iv dataset. _Scientific Reports_, 12(1):7166, 2022.
* [43] Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision. _arXiv preprint arXiv:1906.02337_, 2019.
* [44] Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. A self-supervised approach for adversarial robustness. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 262-271, 2020.
* [45] Richard Ngo. The alignment problem from a deep learning perspective. _arXiv preprint arXiv:2209.00626_, 2022.
* [46] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 427-436, 2015.
* [47] Mikhail S Nikulin et al. Hellinger distance. _Encyclopedia of mathematics_, 78, 2001.
* [48] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. _arXiv preprint arXiv:2201.03544_, 2022.
* [49] Jeonghoon Park, Jimin Hong, Radhika Dua, Daehoon Gwak, Yixuan Li, Jaegul Choo, and Edward Choi. Natural attribute-based shift detection. _arXiv preprint arXiv:2110.09276_, 2021.
* [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1406-1415, 2019.
* [51] Joshua C Peterson, Joshua T Abbott, and Thomas L Griffiths. Evaluating (and improving) the correspondence between deep neural networks and human representations. _Cognitive science_, 42(8):2648-2669, 2018.
* [52] Joshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human uncertainty makes classification more robust. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9617-9626, 2019.
* [53] Nikolay Ponomarenko, Vladimir Lukin, Alexander Zelensky, Karen Egiazarian, Marco Carli, and Federica Battisti. Tid2008-a database for evaluation of full-reference visual quality assessment metrics. _Advances of modern radioelectronics_, 10(4):30-45, 2009.
* [54] Inioluwa Deborah Raji, I Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. The fallacy of ai functionality. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 959-972, 2022.
* [55] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* [56] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [57] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should i trust you?": Explaining the predictions of any classifier, 2016.
* [58] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. _arXiv preprint arXiv:2104.10972_, 2021.
* [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.

* [60] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
* [61] Stuart Russell. _Human compatible: Artificial intelligence and the problem of control_. Penguin, 2019.
* [62] Stuart J Russell. _Artificial intelligence a modern approach_. Pearson Education, Inc., 2010.
* [63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint arXiv:2205.11487_, 2022.
* [64] Richard L Scheaffer, William Mendenhall III, R Lyman Ott, and Kenneth G Gerow. _Elementary survey sampling_. Cengage Learning, 2011.
* [65] Lars Schmarje, Vasco Grossmann, Claudius Zelenka, Sabine Dippel, Rainer Kiko, Mariusz Ozsust, Matti Pastell, Jenny Stracke, Anna Valros, Nina Volkmann, et al. Is one annotation enough?-a data-centric image classification benchmark for noisy and ambiguous label estimation. _Advances in Neural Information Processing Systems_, 35:33215-33232, 2022.
* [66] Jonathan Stray. Aligning ai optimization to community well-being. _International Journal of Community Well-Being_, 3(4):443-463, 2020.
* [67] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In _International Conference on Machine Learning_, pages 20827-20840. PMLR, 2022.
* [68] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272, 2021.
* [69] Dustin Tran, Jeremiah Liu, Michael W Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, et al. Plex: Towards reliability using pretrained large model extensions. _arXiv preprint arXiv:2207.07411_, 2022.
* [70] UNESCO. Artificial intelligence and gender equality: Key findings of unesco's global dialogue, 2020.
* [71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [72] Michael Weiss and Paolo Tonella. Simple techniques work surprisingly well for neural network test prioritization and active learning (replicability study). In _Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis_, pages 139-150, 2022.
* [73] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. _ArXiv preprint arXiv:2006.09994_, 2020.
* [74] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, WENXUAN PENG, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan Li, and Ziwei Liu. OpenOOD: Benchmarking generalized out-of-distribution detection. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [75] Yaodong Yu, Heinrich Jiang, Dara Bahri, Hossein Mobahi, Seungyeon Kim, Ankit Singh Rawat, Andreas Veit, and Yi Ma. An empirical study of pre-trained vision models on out-of-distribution generalization. In _NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications_, 2021.
* [76] Mengmi Zhang, Claire Tseng, and Gabriel Kreiman. Putting visual object recognition in context. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12985-12994, 2020.
* [77] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
* [78] Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned ai. _Advances in Neural Information Processing Systems_, 33:15763-15773, 2020.

## Appendix A Datasheet for Datasets

The following section is answers to questions listed in datasheets for datasets.

### Motivation

* For what purpose was the dataset created? VisAlign is created to serve as a benchmark for measuring visual perception alignment between AI models and humans.
* Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The authors of this paper.
* Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. This work was supported by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST)) and National Research Foundation of Korea (NRF) grant (NRF-2020H1D3A2A03100945), funded by the Korea government (MSIT).

### Composition

* What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? VisAlign contains eight different types of images and their corresponding gold human labels.
* How many instances are there in total (of each type, if appropriate)? There are a total of 12500 images in the train set, distributed equally among the 10 classes. The open test set and the closed test each contain 900 images: 100 images each in Categories 1 to 7 and 200 images in Category 8.
* Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? The train set is a sample of instances of ImageNet-21K, where images have been randomly sampled from synsets and corresponding hyponyms related to each of our classes. The test sets are samples carefully selected by the authors without replacement to match each of the categories' requirements.
* What data does each instance consist of? Each instance consists of an image and its corresponding gold human label.
* Is there a label or target associated with each instance? Yes, the label represents the gold label (_e.g._, human visual perception).
* Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. N/A.
* Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? N/A.
* Are there recommended data splits (e.g., training, development/validation, testing)? No, since VisAlign is an universal benchmark that any model can be tested on regardless of its train set, a developer may feel free to use any training strategies.
* Are there any errors, sources of noise, or redundancies in the dataset? N/A.
* Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?The dataset relies on open source databases: ImageNet [60], ImageNet21K [58], ImageNet-C [22], DomainNet [50], and ImageNet-R [24].
* Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor- patient confidentiality, data that includes the content of individuals' non-public communications)? N/A.
* Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? N/A.
* Does the dataset relate to people? Yes.
* Does the dataset identify any subpopulations (e.g., by age, gender)? N/A.
* Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? N/A.
* Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? N/A.

### Collection Process

* How was the data associated with each instance acquired? We leveraged open source datasets. For Category 2 and Category 5, we synthesized images using Stable Diffusion [59]. For Category 3, we manually applied FGSM [20] on samples in Category 1. For Category8, we applied corruptions on Category 1 samples by using corruption code available in ImageNet-C [22].
* What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? We used the website Amazon Mechanical Turk (MTurk) to create gold human labels for _Uncertain_. After the poll, we used Excel, Google Sheets, and Python to process and label the collected data.
* If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? We first removed images that are hard to recognize or have more than two different objects. After the curating, when it involves sampling, we sampled with a fixed random seed.
* Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? There were one part that required human involvement in the data collection process, deriving gold human label ratio for _Uncertain_. We provided $ 0.05 for classifying 25 images. We did not put any restrictions on participants.
* Over what timeframe was the data collected? The poll was conducted in March of 2023, but the results do not depend much on the date of date collection.
* Were any ethical review processes conducted (e.g., by an institutional review board)? N/A.
* Does the dataset relate to people? Yes.
* Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? We obtained via Amazon Mechanical Turk MTurk website.
* Were the individuals in question notified about the data collection? Yes.

* Did the individuals in question consent to the collection and use of their data? Yes.
* If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? N/A.
* Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? The dataset does not have individual-specific information.

### Preprocessing/cleaning/labeling

* Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? For the data quality, we removed inappropriate responses (that fall under the distractors).
* Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? N/A.
* Is the software that was used to preprocess/clean/label the data available? Preprocessing, cleaning, and labeling are done via Excel, Google Sheets, and Python.

### Uses

* Has the dataset been used for any tasks already? No.
* Is there a repository that links to any or all papers or systems that use the dataset? No.
* What (other) tasks could the dataset be used for? N/A.
* Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? N/A.
* Are there tasks for which the dataset should not be used? N/A.

### Distribution

* Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? No.
* How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? The dataset will be released upon acceptance.
* When will the dataset be distributed? After the whole process of reviewing.
* Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? The dataset will be released under MIT License.
* Have any third parties imposed IP-based or other restrictions on the data associated with the instances? No.
* Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? No.

### Maintenance

* Who will be supporting/hosting/maintaining the dataset? The authors of this paper.
* How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Contact the first author (jiyounglee0523@kaist.ac.kr) or other authors.
* Is there an erratum? No.
* Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If any correction is needed, we plan to upload a new version.
* If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? N/A
* Will older versions of the dataset continue to be supported/hosted/maintained? We plan to maintain the newest version only.
* If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? Contact the authors of the paper.

## Appendix B Training Details

For the experiments in Section 5, we use a batch size of 16 with a learning rate starting at \(1\times 10^{-5}\). The learning rate is decreased by a factor of 0.5 if there is no improvement for 10 epochs or until it reaches \(1\times 10^{-6}\). We approximately match the size of each model to 300M parameters. For ViT, we use the variant with 30 layers and 16 heads in each layer. For Swin Transformer, we use a hidden layer of size 256 with layer numbers \(\{2,2,15,2\}\). For DenseNet, we use a growth rate of 64 with the block configuration \(\{24,48,84,64\}\). For ConvNeXt, we use the large variant with block numbers \(\{3,3,50,3\}\). For MLP-Mixer, we use a hidden size of 2048 with 60 layers. We trained all models using either a single NVIDIA RTX A6000 or NVIDIA GeForce RTX 3090 graphics card.

## Appendix C Class Selection

Table 4 shows the scientific names and sub-species for each class. The classes are selected based on the following four criteria.

* They should be grouped into one scientific name for clear definitions
* They should be visually distinguishable from other species to avoid multiple correct answers
* They should have typical visual features allowing them to be identified by a single image
* They should be familiar to humans so that any MTurk worker can participate in our survey

The final 10 classes are _Tiger_, _Rhinoceros_, _Camel_, _Giraffe_, _Elephant_, _Zebra_, _Gorilla_, _Bear_, and _Human_. These labels are revised and verified by two zoologists.

## Appendix D Dataset Construction

This section will describe the details of our dataset construction.

### Cronbach's Alpha

Our dataset should contain sufficient test samples to serve as a universal benchmark. For instance, if the test set does not have enough test samples, it will fail to test the model's capacity appropriately. Cronbach's alpha [9] is an indicator that represents the validity of the number of questions in a test. To calculate this value, we first need responses from humans. Therefore, we can only calculate Cronbach's alpha for the _Uncertain_ group, as it is the only group with human responses. However,we believe that the Cronbach's alpha value for the _Uncertain_ group can also be applied to other categories, given that samples in other categories are more straightforward than those in _Uncertain_ (_e_.\(g\)., they have clear images and optimal actions are explicit). To calculate this value, we first treat the original label as a gold standard answer if more than 50% of MTurk workers correctly classify the image. Otherwise, we set Abstention as the gold standard answer. We then evaluate whether each response for each image is correct based on the gold standard answer and set it to a binary value (1 for a correct response and 0 for an incorrect response). We denote the binary response for the \(i\)-th image as \(x_{i}\).

Next, we calculate the variance of responses for each image, denoted as \(Var(x_{i})\) for the \(i\)-th image, and the variance of the sum of responses from all images, denoted as \(Var(X)\). Here, \(X\) is the sum of responses for all images, i.e., \(X=\sum_{i=1}^{N}x_{i}\), and \(N\) is the total number of images. We then employ Cronbach's Alpha formula as shown in Equation 2 below.

In our case, \(\sum_{i=1}^{N}Var(x_{i})=127.134,Var(X)=976.564\), and \(N=100\) which yields a Cronbach's Alpha of 0.88. A Cronbach's Alpha value between 0.75 and 0.9 is considered ideal. A value higher than 0.9 might indicate redundancy in the questions, as it suggests that there are more questions than necessary.

\[\alpha=\frac{N}{N-1}\bigg{(}1-\frac{\sum_{i=1}^{N}Var(x_{i})}{Var(X)}\bigg{)}, \quad where\;\;X=\sum_{i=1}^{N}x_{i}\] (2)

### Stable Diffusion Prompt

Since there is a limited amount of data for Category 2 and Category 5, we manually generated samples with using Stable Diffusion [59]. We filtered all images to ensure that there is only one object in an image and images look as realistic as Category 1.

#### d.2.1 Category 2 Prompts

The prompt used is _"RAW photo of a {subspecies} {background_prompt}, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3,"_ where {subspecies} is one of the subspecies listed in Table 4 and _[background_prompt]_ is one of the following:

\begin{table}
\begin{tabular}{l c c} \hline \hline Class & Scientific Name & Subspecies \\ \hline Tiger & Panthera tigris & Amur tiger, Chinese tiger, North Indochinese tiger, \\  & Malayan tiger, Sumatran tiger, Bengal tiger \\ \hline Rhinoceros & Rhinoceros & White rhino, Black rhino, Indian rhino, Javan rhino, Sumatran rhino \\ \hline Camel & Camelus & Bactrian camel, Arabian camel, Wild bactrian camel \\ \hline Giraffe & Giraffa & Angolan giraffe, Kordofan giraffe, Transvaal giraffe, \\  & Giraffa camelopardalis & Reticulated giraffe, Baring giraffe, Masi giraffe \\ \hline Elephant & Elephas maximus, & Asiatic elephant, Malayan elephant, Indian elephant, \\  & Loxodonta africana & Sri Lankan elephant, Sumatran elephant, African elephant, \\  & South African bush elephant, East African bush elephant \\ \hline Zebra & Equus grevyi, & Grevy’s zebra, Plains zebra, Grant’s zebra, \\  & Equus guagga, & Half-named zebra, Dama zebra, Chapman’s zebra, \\  & Equus zebra & Hartmann’s mountain zebra \\ \hline Gorilla & Gorilla & Western lowland gorilla, Cross River gorilla, \\  & Mountain gorilla, Eastern lowland gorilla \\ \hline Bear & Ursus & Giant panda, Spectcade bar, Sun Bear, Sloth Bear, \\  & American Black Bear, Brown Bear, Polar Bear, Asiatic black bear \\ \hline Kangaroo / Wallyhay & Macropus, & Western grey kangaroo, Eastern grey kangaroo, Agile wallaby, \\  & Notamacropus, & Black-striped wallaby, Tammar wallaby, Western brush wallaby, \\  & Onychogalea, & Parma wallaby, Pretty-faced wallaby, Red-necked wallaby, Genus Onychogalea, \\  & Oshpratter & Bridled nail-tail wallaby, Northern nail-tail wallaby, Genus Osphranter, \\  & Antijopine kangaroo, Black wallaroo, Common wallaroo, Red kangaroo \\ \hline Human & Homo sapiens sapiens & \\ \hline \hline \end{tabular}
\end{table}
Table 4: The scientific names and subspecies of the each class.

on the moon

in New York city

near a swimming pool

on the clouds

on a snowy mountain

at night taken using an infrared camera

inside a large bathroom

with a rainbow background

with a purple background

with a bright orange background

The negative prompt used is "_unrealistic, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation._"

#### d.2.2 Category 5

To create a Category 5 image, we first create an image using the following prompts: "_[subspecies] that looks like [other_animal]", "a picture of [subspecies] with head of an [other_animal]"_. For \(\{other\_animal\}\), we choose species that in not in-class (_e.g._, eagle, bird, fish, alligator). Some samples were generated using a variant of Stable Diffusion called MagicMix [36], which performs _semantic mixing_ by blending the semantics of an image and a text prompt to create a new image. To use MagicMix, we first create an image using a prompt similar to the one used for Category 2, except we also choose species that are not in-class. Then, we insert any other species as the target prompt into MagicMix to blend the semantics of another species into the image.

## Appendix E AI-Human Visual Alignment for Uncertain Images

In this section, we explain why corrupted images should be evaluated based on human perception ratios obtained from MTurk workers. Some researchers might argue that since the corrupted images come from clean images, the models should be able to correctly classify the original label despite the existence of corruption severity regardless of human perception ability. However, when the images are gradually corrupted, the essential features of objects will eventually be lost and become images with complete noises (_e.g._, black images or images with pure Gaussian noise). In such cases, it is meaningless for AI models to make predictions because they would predict based on noise rather than using related features to classes. Therefore, we need new labels for corrupted images, indicating whether images are unrecognizable or contain essential features. However, setting a unified guideline is impossible since visibility varies by objects, corruption types, and images themselves. Therefore, we must newly obtain labels by asking qualified humans. Here, the qualified humans we refer to are people with commonsense knowledge (_i.e._, must know the 10 mammals) and have functioning visual perception (_i.e._, we test this via intra-annotator agreement and we also rejected responses from workers who chose other than 'Abstention' for distractor images that are corrupted images from Category 4). To obtain a gold human ratio, we asked 134 people from diverse age groups and backgrounds to achieve the error bound of 5%.

Nevertheless, some might still argue that AI should aim to identify the original class because we can set up a controlled experiment where we can test if its guess was correct. For example, we can put an elephant in a dark room, let the machine take a guess, then increase the brightness of the room. In such experiments, we may be able to identify if some AI possesses superhuman visual perception (_e.g._, only 1 out of 100 human participants were able to confidently tell the object in the dark room was an elephant, but the AI had a 95% confidence in the elephant class). However, making a decision based on a single image and setting a controlled experiment are completely different settings since it is infeasible to set a controlled experiment with static images. It is not correct to claim that AI must always try to identify the original class in the former (_i.e._, deciding based on a single image) because the latter (_i.e._, running a controlled trial) is also possible. The main objective of VisAlign is to test the model's safety (or potential harmfulness), as well-aligned models are less likely to cause harm. Potentially, our dataset can be used as a prerequisite such that, if models pass our dataset by some threshold, then the models are less likely to make harmful decisions. Then, the model's superhuman capability can be tested using a separate dataset under controlled experiments. This is somewhat similar to the multi-phase drug development process, where the initial phases always test the basic safety of the drug (toxicity, side effects) before advancing to latter phase to test the clinical efficacy of the drug.

## Appendix F Amazon Mechanical Turk Survey

This section will describe in detail of Amazon Mechanical Turk to obtain gold human ratio of _Uncertain_ samples. We paid $0.05 for classifying 35 images per worker.

### Instructions

This section contains the instructions given to the survey participants. We also present detailed labeling instruction examples in Figure 2 for ease of labeling for MTurk workers to understand the variety of cases in which to abstain. Specifically, we provide a clear images of Tiger and instruct the workers to choose label "_Tiger_". For other cases (_e.g._, only a part of a tiger, a bag with a tiger pattern, species similar to tiger, other animals), we instruct the workers to choose "None of the 10 mammals, uncertain, or unrecognizable". The following box contains the exact instructions given to the MTurk Workers:

There are 11 labels to choose from:

* Tiger
* Zebra
* Camel
* Giraffe
* Elephant
* Rhino
* Gorilla
* Bear or Giant Panda
* Kangaroo
* Human
* None of the above, uncertain, or unrecognizable

Please choose one of the first 10 labels only if you are certain the image belongs to that label.

Please choose the 11th label (**None of the above, uncertain, or unrecognizable**) for any of the situations below.

* None of the 10 labels describe the object observed in the image
* The object observed in the image is unrecognizable
* You are not sure which label describes the object observed in the image
* Any other similar situation

Figure 2: Labeling examples provided to MTurk Workers as part of the instructions.

### Filtering Process

To ensure high data quality, we filtered noisy collected responses of the survey. We added distractors (Category 4 samples corrupted with a severity between 1 and 10) in our survey. Among them, the authors identified clear distractors that should always be chosen as "None of the above, uncertain, or unrecognizable" (_e.g._, a clear image of a cup or a truck). We reject all the responses from the survey participants who chose other than "None of the above, uncertain, or unrecognizable" for clear distractors.

### Participant Statistics

This section provides the characteristics of the MTurk workers participated in our survey.

MTurk workers are equal in gender (44.9% of male, 53.8% of female, and 1.3% of others).

People from diverse age groups (from 10s to 70s) participated (2.1% of 10s, 19.7% of 20s, 35.1% of 30s, 24.8% of 40s, 14.3% of 50s, 3.5% of 60s, and 0.4% of 70s).

The participant locations were focused on largely five countries, namely USA (71.1%), India (13.1%), Italy (5%), UK (3.1%), and Canada (2.3%). Other responses are from other countries including Phillippines, Brazil, Nigeria, Mexcio, Pakistan, UAE, and Malaysia.

### Sampling Theory

Given an image \(x\) and its corresponding label \(y\), we can assume \(y\sim\) Bernoulli(\(p\)), where \(p\) is the probability of the true class.

Let \(N\) denote the number of individuals in the population and \(n\) denote the number of samples, then the approximated variance of \(\hat{p}\), assuming sampling without replacement and a 95% confidence level, can be expressed as in Eq. 3. In this equation, \(z_{0.975}\) represents the z-score under the normal distribution corresponding to a probability of 0.975, and \(q=1-p\).

\[\begin{split} z_{0.975}\sqrt{\widehat{V}(\hat{p})}& =z_{0.975}\sqrt{\left(1-\frac{n}{N}\right)\times\left(\frac{\hat{p }\hat{q}}{n-1}\right)}\\ &\approx z_{0.975}\sqrt{\left(\frac{\hat{p}\hat{q}}{n-1}\right)} \hskip 56.905512pt(\because N=\infty)\end{split}\] (3)

Given an error bound \(\xi\), we can derive the required minimum number of samples to achieve the error bound by setting the 95% confidence interval of the approximated variance to be lower than \(\xi\). For ease of calculation, we round \(z_{0.975}\) = 1.96 to 2.

\[\begin{split} 2\sqrt{\left(\frac{\hat{p}\hat{q}}{n-1}\right)}& \leq\xi\\ n&\geq\frac{4\hat{p}\hat{q}}{\xi^{2}}+1\end{split}\] (4)

Since we do not have prior knowledge of \(\hat{p}\), we set \(\hat{p}\) to \(\frac{1}{11}\), which represents a uniform distribution over the 11 classes (10 mammals + abstention). We drop the constant for simplicity.

\[n\geq\frac{4\times\frac{1}{11}\times\frac{10}{11}}{\xi^{2}}=\frac{40}{11^{2} \times\xi^{2}}\] (5)

For \(\xi\) = 0.05, 0.1, 0.15, the minimum required number of participants are as follows:

Therefore, to achieve an error bound lower than 5%, we surveyed 134 people per image.

## Appendix G Additional Experimental Results

### Experimental Results Shown as Percentages

Section 4.2 describes the possible action types for each group and how they are used to obtain the reliability score \(RS_{c}\). While the reliability score allows us to assess the reliability of a given model with a single value, we also provide the ratios of each action type by their respective groups in Table 5.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \multicolumn{3}{c}{Must-Act} & \multicolumn{3}{c}{Must-Abstain} \\ \cline{2-6}  & Correct & Incorrect & Abstain & Label Pred. & Other Pred. & Abstain \\ \hline ViT [11] & & & & & \\ \hline SP & 0.62 & 0.07 & 0.31 & 0.02 & 0.84 & 0.13 \\ ASP & 0.63 & 0.00 & 0.37 & 0.03 & 0.97 & 0.00 \\ MD [35] & 0.63 & 0.01 & 0.35 & 0.02 & 0.94 & 0.03 \\ KNN [67] & 0.62 & 0.04 & 0.34 & 0.02 & 0.91 & 0.07 \\ TAPUDD [13] & 0.63 & 0.00 & 0.37 & 0.80 & 0.97 & 0.00 \\ OpenMax [3] & 0.61 & 0.11 & 0.28 & 0.02 & 0.79 & 0.18 \\ MC-Dropout [16] & 0.63 & 0.00 & 0.37 & 0.03 & 0.97 & 0.00 \\ Deep Ensemble [32] & 0.62 & 0.14 & 0.24 & 0.03 & 0.72 & 0.25 \\ \hline \hline \multicolumn{6}{l}{Swin Transformer [38]} \\ \hline SP & 0.71 & 0.07 & 0.22 & 0.02 & 0.83 & 0.15 \\ ASP & 0.73 & 0.00 & 0.27 & 0.02 & 0.98 & 0.00 \\ MD [35] & 0.73 & 0.03 & 0.24 & 0.02 & 0.91 & 0.07 \\ KNN [67] & 0.63 & 0.28 & 0.10 & 0.01 & 0.44 & 0.55 \\ TAPUDD [13] & 0.73 & 0.00 & 0.27 & 0.02 & 0.98 & 0.00 \\ OpenMax [3] & 0.70 & 0.07 & 0.23 & 0.02 & 0.75 & 0.24 \\ MC-Dropout [16] & 0.73 & 0.00 & 0.27 & 0.02 & 0.98 & 0.00 \\ Deep Ensemble [32] & 0.74 & 0.11 & 0.15 & 0.01 & 0.72 & 0.27 \\ \hline \hline \multicolumn{6}{l}{DenseNet [27]} \\ \hline SP & 0.76 & 0.07 & 0.17 & 0.02 & 0.80 & 0.18 \\ ASP & 0.78 & 0.00 & 0.22 & 0.02 & 0.98 & 0.00 \\ MD [35] & 0.78 & 0.00 & 0.22 & 0.02 & 0.93 & 0.06 \\ KNN [67] & 0.73 & 0.15 & 0.11 & 0.01 & 0.61 & 0.38 \\ TAPUDD [13] & 0.74 & 0.04 & 0.22 & 0.02 & 0.94 & 0.05 \\ OpenMax [3] & 0.71 & 0.16 & 0.12 & 0.01 & 0.64 & 0.35 \\ MC-Dropout [16] & 0.78 & 0.00 & 0.22 & 0.02 & 0.98 & 0.00 \\ Deep Ensemble [32] & 0.79 & 0.07 & 0.14 & 0.02 & 0.82 & 0.16 \\ \hline \hline \multicolumn{6}{l}{ConvNeXt [39]} \\ \hline SP & 0.66 & 0.14 & 0.20 & 0.02 & 0.65 & 0.33 \\ ASP & 0.71 & 0.00 & 0.29 & 0.04 & 0.96 & 0.00 \\ MD [35] & 0.63 & 0.15 & 0.22 & 0.03 & 0.78 & 0.19 \\ KNN [67] & 0.68 & 0.14 & 0.18 & 0.03 & 0.61 & 0.36 \\ TAPUDD [13] & 0.67 & 0.04 & 0.29 & 0.04 & 0.94 & 0.02 \\ OpenMax [3] & 0.69 & 0.04 & 0.28 & 0.04 & 0.94 & 0.02 \\ MC-Dropout [16] & 0.71 & 0.00 & 0.29 & 0.04 & 0.96 & 0.00 \\ Deep Ensemble [32] & 0.66 & 0.17 & 0.18 & 0.02 & 0.60 & 0.39 \\ \hline \hline \multicolumn{6}{l}{MLP-Mixer [68]} \\ \hline SP & 0.62 & 0.09 & 0.29 & 0.01 & 0.80 & 0.19 \\ ASP & 0.65 & 0.00 & 0.35 & 0.01 & 0.99 & 0.00 \\ MD [35] & 0.61 & 0.14 & 0.26 & 0.01 & 0.73 & 0.26 \\ KNN [67] & 0.59 & 0.16 & 0.25 & 0.00 & 0.67 & 0.33 \\ TAPUDD [13] & 0.48 & 0.21 & 0.31 & 0.01 & 0.

### Correlation between Visual Alignment and Reliability Score

Figure 3 shows the correlation between visual alignment distance and reliability score measured in Table 3. There exists a strong correlation between visual alignment distance and reliability score - the shorter the distance the higher the reliability score. This indicates that visual alignment score can be used as a proxy method for reliability, underscoring the importance of visual alignment.

## Appendix H Experiment Results from Pre-training and Self-supervised Learning

Previous studies [1, 75, 23, 44] suggest that training on larger data and pre-training by self-supervised learning (SSL) methods help improve robustness and Out-of-Distribution (OOD) detection. To validate if the same findings can also be applied in our task, we additionally measure the visual alignment and reliability score on models that are pre-trained on ImageNet [60] and pre-trained by two popular SSL methods, which are SimCLR [6] and BYOL [21]. For models that are pre-trained on ImageNet, after pre-training, we initialize the top classification layer and train on our train set while freezing the pre-trained parameters during fine-tuning. For models that are pre-trained by SSL methods, we do not freeze any layers after pre-training.

The results are shown in Table 6 and Table 7. The results in Table 6 can be compared to the results in Table 3. For ImageNet pre-trained models, Transformer-based models show improved performance, whereas MLP-based and CNN-based models show similar or decreased visual alignment scores, especially when evaluated with SP. This indicates that the effect of pre-training on larger datasets is dependent on model architecture. Interestingly, distance-based abstention functions display higher visual alignment scores. We suspect that the improved output embeddings from pre-training enable distance-based abstention functions to capture more precise features. Deep Ensemble has better visual alignment when met with Transformer-based and MLP-based. Notably, Transformer-based models combined with KNN have the best visual alignment score. We conjecture the reason comes from both the model architecture and the abstention function. Contrary to CNN-based models, Transformer-based models are able to capture global features of images instead of only local features. Also, KNN calculates abstention probability based on the distance between samples instead of clusters, as done in MD or TAPUDD, which uses more fine-grained features for deciding abstention. Therefore, deciding abstention using fine-grained details on global features gets boosted when trained on a larger set,

Figure 3: Correlation between Visual Alignment Distance and Reliability Score (\(RS_{900}\)). There exists a strong correlation between visual alignment distance and reliability score. This proves that visual alignment can be used as a proxy method for reliability.

[MISSING_PAGE_FAIL:26]

best performance on average across all seven abstention functions. Therefore, more research on finding the optimal model architecture in visual alignment is needed.

## Appendix I Discussion on Uncertainty

### Continuity of Uncertainty

In this section, we will discuss a critical aspect of uncertainty which is continuity. Uncertainty is continuous and it is challenging to draw clear distinctions among classes (_i.e_., as we mentioned in the main paper that it is hard to distinguish between "car" and "truck") and between clear and uncertain images (_i.e_., if at least one person claims an image as "uncertain", then it becomes an uncertain image). However, as our ultimate goal is to construct a universal testing benchmark that quantitatively measures visual perception alignment between models and humans, our classes should have clear defintions so that model developers can easily prepare their models and training strategy. Therefore, after careful consideration, we used the taxonomy classification in biology which is the meticulous product of decades of efforts by countless domain experts to hierarchically distinguish each species as accurately as possible with clear definitions. Also, in order to comprehensively measure the visual perception alignment between models and humans, the models should be tested under various conditions including clear in-class images (Must-Act), clear out-of-class images (Must-Abstain) and confusing images (Uncertain). As there is no clear boundary between clear and uncertain images, the best scenario would be to survey all images in our dataset to 134 people per image to obtain numerically reliable annotations. However, surveying all images is not always feasible as it requires tremendous amount of time and money considering that there are 1800 images (900 each in the open and closed test sets) in our dataset. Therefore, due to the realistic reasons, we put significant effort to include only clear images that anyone can agree on in Must-Act and Must-Abstain and obtained human annotations on Uncertain images. Nevertheless, we also recognize that continuity is an essential characteristic of uncertainty that should be carefully considered and there is always a possibility of corner cases that may be disagreeable by at least one person. We have done our best to remove those corner case samples and cross-validated our final selection. Further detailed analysis and benchmark dataset on the continuity of uncertainty is highly needed and we will leave this as a future work.

### Coverage of Uncertainty

"Uncertainty" is a broad concept and it is hard to define with one clear line and list all possible cases. In this paper, we chose 15 different types of corruptions to generate uncertainty in various ways following a concrete previous work [22]. Furthermore, to better represent the continuity of uncertainty explained in Appendix I.1, we apply the corruptions varying severity ranging from 1 to 10. Many types of corruption we used resemble the reality in their own way. For example, adjusting the brightness of the image is certainly realistic, and changing its resolution is similar to viewing an object beyond a filter (_e.g_., semi-transparent glass), and weather changes are also certainly realistic. These corruptions result in some of realistic uncertain images, precisely 8.5% in the case of the open test set, where MTurk survey participants were struggling with differentiating between two or more animals (rather than being confused between one animal and abstention). Despite our meticulous effort, we are well aware that those corruptions certainly do not cover all possible uncertainties that arise in the real world. However, "uncertainty" is too broad to specify and difficult to collect or generate, and hence for now we use corruptions (but sufficiently diversse types of corruptions).

## Appendix J Detailed Comparisons with Previous Works

In this section, we will explain in detail how our work differs from related previous works. Our ultimate goal is to create a rigorous test (similar to tests that humans take such as college entrance exams) to quantitatively measure the visual perception gap between the models and humans across various categories. Our main interest does not lie in training but on rigorously testing the visual perception alignment. For that purpose, a dataset should satisfy the four requirements we mentioned in Section 3 and use a proper metric that reflects the visual perception alignment.

Peterson et al. [52] and Schmarje et al. [65] utilized their datasets mainly for training and did not thoroughly verify whether the model actually achieved visual alignment. Peterson et al. [52] only tested their models on in-class samples (in our case, Category 1) and out-of-class samples (in our case, Category 4 and Category 6) and they showed only accuracy and cross entropy, which is analogous to KL divergence. Therefore, they did not test their models on various possible scenarios and did not use proper measurement, as KL divergence is not an optimal choice for visual perception alignment as we described in Section 4.1. Schmarje et al. [65] only tested their models on in-class samples (in our case, Category 1) and showed accuracy and KL divergence. Therefore, although previous works trained their models with the goal of achieving visual perception alignment, none of the works have thoroughly verified how much the models have actually achieved visual perception alignment under diverse situations with an appropriate measurement.

Zhang et al. [76] and Bomatter et al. [5] are similar to our work since they show that both models and humans have better object recognition when given more contextual information, but it is difficult to say that they have comprehensively evaluated visual perception alignment. These two works only tested their models on partial aspects (in our case, Category 1, Category 2, and Category 8). Thus, these works did not test on Must-Abstain samples, which makes it difficult to claim that they "comprehensively" evaluated visual perception alignment. Zhang et al. [76] and Bomatter et al. [5] simply showed that both models and humans exhibit similar performance trends based on context (_i.e._, when given more context, both human's and model's visual recognition performance increases), and they provided human-model correlations to describe their relative trends across conditions. However, our study on visual perception alignment is not about following human trends, but about measuring how well the model replicates human perception sample-wise. Hence, considering our research scope and criteria, it's challenging to assert that Zhang et al. [76] and Bomatter et al. [5] rigorously measured visual perception alignment.

In contrast, we quantitatively measured visual perception alignment across various scenarios with multiple human annotations on uncertain images. In addition, we borrowed Hellinger distance to precisely calculate the visual perception alignment after careful consideration of other distance-based metrics such as KL divergence and Total Variation distance. Furthermore, we incorporated specialized elements (sampling theory, statistical theories related to survey design, and experts in the related fields) in creating our dataset.

There are three key differences that distinguish our dataset compared to existing datasets that also focus on uncertainty in object recognition. First, we applied corruption and cropping with different intensities ranging from 1 to 10 to reflect the continuity of uncertainty mentioned in Appendix I.1. Uncertainty is continuous and it is critical to test models on samples where uncertainty may increase in stages. In this sense, we tested models visual perception alignment on varying degrees of uncertainty. Second, we obtained 134 human annotations per image to accurately estimate the ground truth visual perception distribution. We borrowed statistical sampling theory to achieve an error bound of lower than 5%, of which the details are in Section 3.3. Third, while our uncertain samples include uncertainty that confuses between classes, refer refer to as "inter-class uncertainty" (soft labels distributed only among target classes), we also include "recognizability uncertainty" (soft labels distributed among classes + abstention), namely whether an image itself is recognizable or not. If an image is moderately brightened (_i.e._, intermediate phase between a clear image and a complete white image), then the object itself may or may not be recognizable. Visual perception includes not only object identification (predicting that it is an elephant) but also object recognizability (the object itself is recognizable). In this sense, we cover broader scenarios compared to previous works as we include object recognizability uncertainty in our uncertain category.

We also want to highlight that VisAlign does only contain Uncertain but also Must-Act and Must-Abstain to cover diverse scenarios as possible. In order to evaluate a model's visual perception alignment, a model should be tested under Must-Act (whether it predicts a correct class with high confidence), Must-Abstain (whether it abstains out-of-class samples with high confidence), and Uncertain (whether it reflects the human uncertainty). However, previous works are limited in that they test their model on partial cases (Category 1 and Category 4 in Peterson et al. [52], and Category 1 in Schmarje et al. [65]) which does not truly reflect visual perception alignment on various situations. It is especially important to test models on samples from out of distributions (_i.e._, Category 5 and Category 7), but previous works overlook these samples thus did not quantittedly evalute from visual perception alignment. Therefore, their dataset cannot be utilized as a benchmark to evaluate visual perception alignment. While previous papers and our work have in common with handling uncertainty,in our case, uncertain samples are a subset of our final dataset and we cover more diverse necessary situations, which previous works do not, as possible to measure the visual perception alignment.

[MISSING_PAGE_FAIL:30]