# Learning a Neuron by a Shallow ReLU Network:

Dynamics and Implicit Bias for Correlated Inputs

Dmitry Chistikov

University of Warwick

d.chistikov@warwick.ac.uk

&Matthias Englert

University of Warwick

m.englert@warwick.ac.uk

&Ranko Lazic

University of Warwick

r.s.lazic@warwick.ac.uk

Equal contribution.

###### Abstract

We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters. By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets. Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training. We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm. Finally we perform a range of numerical experiments, which corroborate our theoretical findings.

## 1 Introduction

One of the grand challenges for machine learning research is to understand how overparameterised neural networks are able to fit perfectly the training examples and simultaneously to generalise well to unseen data (Zhang, Bengio, Hardt, Recht, and Vinyals, 2021). The double-descent phenomenon (Belkin, Hsu, Ma, and Mandal, 2019), where increasing the neural network capacity beyond the interpolation threshold can eventually reduce the test loss much further than could be achieved around the underparameterised "sweet spot", is a mystery from the standpoint of classical machine learning theory. This has been observed to happen even for training without explicit regularisers.

Implicit bias of gradient-based algorithms.A key hypothesis towards explaining the double-descent phenomenon is that the gradient-based algorithms that are used for training are _implicitly biased_ (or _implicitly regularised_) (Neyshabur, Bhojanapalli, McAllester, and Srebro, 2017) to converge to solutions that in addition to fitting the training examples have certain properties which cause them to generalise well. It has attracted much attention in recent years from the research community, which has made substantial progress in uncovering implicit biases of training algorithms in many important settings (Vardi, 2023). For example, for classification tasks, and for homogeneous networks (which is a wide class that includes ReLU networks provided they contain neither biases at levels deeper than the first nor residual connections), Lyu and Li (2020) and Ji and Telgarsky (2020) established that gradient flow is biased towards maximising the classification margin in parameter space, in the sense that once the training loss gets sufficiently small, the direction of the parameters subsequently converges to a Karush-Kuhn-Tucker point of the margin maximisation problem.

Insights gained in this foundational research direction have not only shed light on overparameterised generalisation, but have been applied to tackle other central problems, such as the susceptibility of networks trained by gradient-based algorithms to adversarial examples (Vardi, Yehudai, and Shamir, 2022) and the possibility of extracting training data from network parameters (Haim, Vardi, Yehudai, Shamir, and Irani, 2022).

Regression tasks and initialisation scale.Showing the implicit bias for regression tasks, where the loss function is commonly mean square, has turned out to be more challenging than for classification tasks, where loss functions typically have exponential tails. A major difference is that, whereas most of the results for classification do not depend on how the network parameters are initialised, the scale of the initialisation has been observed to affect decisively the implicit bias of gradient-based algorithms for regression (Woodworth, Gunasekar, Lee, Moroshko, Savarese, Golan, Soudry, and Srebro, 2020). When it is large so that the training follows the _lazy regime_, we tend to have fast convergence to a global minimum of the loss, however without an implicit bias towards sparsity and with limited generalisation (Jacot, Ged, Simsek, Hongler, and Gabriel, 2021). The focus, albeit at the price of uncertain convergence and lengthier training, has therefore been on the _rich regime_ where the initialisation scale is small.

Considerable advances have been achieved for linear networks. For example, Azulay, Moroshko, Nacson, Woodworth, Srebro, Globerson, and Soudry (2021) and Yun, Krishnan, and Mobahi (2021) proved that gradient flow is biased to minimise the Euclidean norm of the predictor for one-hidden layer linear networks with infinitesimally small initialisation, and that the same holds also for deeper linear networks under an additional assumption on their initialisation. A related extensive line of work is on implicit bias of gradient-based algorithms for matrix factorisation and reconstruction, which has been a fruitful test-bed for regression using multi-layer networks. For example, Gunasekar, Woodworth, Bhojanapalli, Neyshabur, and Srebro (2017) proved that, under a commutativity restriction and starting from a small initialisation, gradient flow is biased to minimise the nuclear norm of the solution matrix; they also conjectured that the restriction can be dropped, which after a number of subsequent works was refuted by Li, Luo, and Lyu (2021), leading to a detailed analysis of both underparameterised and overparameterised regimes by Jin, Li, Lyu, Du, and Lee (2023).

For non-linear networks, such as those with the popular ReLU activation, progress has been difficult. Indeed, Vardi and Shamir (2021) showed that precisely characterising the implicit bias via a non-trivial regularisation function is impossible already for single-neuron one-hidden layer ReLU networks, and Timor, Vardi, and Shamir (2023) showed that gradient flow is not biased towards low-rank parameter matrices for multiple-output ReLU networks already with one hidden layer and small training datasets.

ReLU networks and training dynamics.We suggest that, in order to further substantially our knowledge of convergence, implicit bias, and generalisation for regression tasks using non-linear networks, we need to understand more thoroughly the dynamics throughout the gradient-based training. This is because of the observed strong influence that initialisation has on solutions, but is challenging due to the highly non-convex optimisation landscape. To this end, evidence and intuition were provided by Maennel, Bousquet, and Gelly (2018); Li et al. (2021), and Jacot et al. (2021), who conjectured that, from sufficiently small initialisations, after an initial phase where the neurons get aligned to a number of directions that depend only on the dataset, training causes the parameters to pass close to a sequence of saddle points, during which their rank increases gradually but stays low.

The first comprehensive analysis in this vein was accomplished by Boursier, Pillaud-Vivien, and Flammarion (2022), who focused on orthogonal datasets (which are therefore of cardinality less than or equal to the input dimension), and established that, for one-hidden layer ReLU networks, gradient flow from an infinitesimal initialisation converges to zero loss and is implicitly biased to minimise the Euclidean norm of the network parameters. They also showed that, per sign class of the training labels (positive or negative), minimising the Euclidean norm of the interpolator networks coincides with minimising their rank.

Our contributions.We tackle the main challenge posed by Boursier et al. (2022), namely handling datasets that are not orthogonal. A major obstacle to doing so is that, whereas the analysis of the training dynamics in the orthogonal case made extensive use of an almost complete separation between a turning phase and a growth phase for all hidden neurons, non-orthogonal datasets cause considerably more complex dynamics in which hidden neurons follow training trajectories that simultaneously evolve their directions and norms (Boursier et al., 2022, Appendix A).

To analyse this involved dynamics in a reasonably clean setting, we consider the training of one-hidden layer ReLU networks by gradient flow from a small balanced initialisation on datasets that are labelled by a teacher ReLU neuron with which all the training points are correlated. More precisely, we assume that the angles between the training points and the teacher neuron are less than \(\pi/4\), which implies that all angles between training points are less than \(\pi/2\). The latter restriction has featured per label class in many works in the literature (such as by Phuong and Lampert (2021) and Wang and Pilanci (2022)), and the former is satisfied for example if the training points can be obtained by summing the teacher neuron \(\bm{v}^{*}\) with arbitrary vectors of length less than \(\|\bm{v}^{*}\|/\sqrt{2}\). All our other assumptions are very mild, either satisfied with probability exponentially close to \(1\) by any standard random initialisation, or excluding corner cases of Lebesgue measure zero.

Our contributions can be summarised as follows.

* We provide a detailed **non-asymptotic analysis** of the dynamics of each hidden neuron throughout the training, and show that it applies whenever the initialisation scale \(\lambda\) is below a **precise bound** which is polynomial in the network width \(m\) and exponential in the training dataset cardinality \(n\). Moreover, our analysis applies for any input dimension \(d>1\), for any \(n\geq d\) (otherwise exact learning of the teacher neuron may not be possible), for any \(m\), and without assuming any specific random distribution for the initialisation. In particular, we demonstrate that the role of the overparameterisation in this setting is to ensure that initially at least one hidden neuron with a positive last-layer weight has in its active half-space at least one training point.
* We show that, during a first phase of the training, all active hidden neurons with a positive last-layer weight **get aligned** to a single direction which is positively correlated with all training points, whereas all active hidden neurons with a negative last-layer weight get turned away from all training points so that they deactivate. In contrast to the orthogonal dataset case where the sets of training points that are in the active half-spaces of the neurons are essentially constant during the training, in our correlated setting this first phase in general consists, for each neuron, of a different **sequence of stages** during which the cardinality of the set of training points in its active half-space gradually increases or decreases, respectively.
* We show that, during the rest of the training, the bundle of aligned hidden neurons with their last-layer weights, formed by the end of the first phase, grows and turns as it travels from near the origin to near the teacher neuron, and **does not separate**. To establish the latter property, which is the most involved part of this work, we identify a set in predictor space that depends only on \(\lambda\) and the training dataset, and prove: first, that the trajectory of the bundle **stays inside the set**; and second, that this implies that the directional gradients of the individual neurons are such that the angles between them are non-increasing.
* We prove that, after the training departs from the initial saddle, which takes time logarithmic in \(\lambda\) and linear in \(d\), the gradient satisfies a Polyak-Lojasiewicz inequality and consequently the loss **converges to zero exponentially fast**.
* We prove that, although for any fixed \(\lambda\) the angles in the bundle of active hidden neurons do not in general converge to zero as the training time tends to infinity, if we let \(\lambda\) tend to zero then the networks to which the training converges have a limit: a network of rank \(1\), in which all non-zero hidden neurons are positive scalings of the teacher neuron and have positive last-layer weights. This establishes that gradient flow from an infinitesimal initialisation is **implicitly biased** to select interpolator networks of **minimal rank**. Note also that the limit network is identical in predictor space to the teacher neuron.
* We show that, surprisingly, among all networks with zero loss, there may exist some whose Euclidean norm is smaller than that of any network of rank \(1\). Moreover, we prove that this is the case if and only if a certain condition on angles determined by the training dataset is satisfied. This result might be seen as **refuting the conjecture** of Boursier et al. (2022, section 3.2) that the implicit bias to minimise Euclidean parameter norm holds beyond the orthogonal setting, and adding some weight to the hypothesis of Razin and Cohen (2020). The counterexample networks in our proof have rank \(2\) and make essential use of the ReLU non-linearity.
* We perform numerical experiments that indicate that the training dynamics and the implicit bias we theoretically established occur in practical settings in which some of our assumptions are relaxed. In particular, gradient flow is replaced by gradient descent with a realistic learning rate,the initialisation scales are small but not nearly as small as in the theory, and the angles between the teacher neuron and the training points are distributed around \(\pi/4\).

We further discuss related work, prove all theoretical results, and provide additional material on our experiments, in the appendix.

## 2 Preliminaries

Notation.We write: [\(n\)] for the set \(\{1,\ldots,n\}\), \(\|\bm{v}\|\) for the Euclidean length of a vector \(\bm{v}\), \(\overline{\bm{v}}\coloneqq\bm{v}/\|\bm{v}\|\) for the normalised vector, \(\measuredangle(\bm{v},\bm{v}^{\prime})\coloneqq\arccos(\overline{\bm{v}}^{\top} \overline{\bm{v}}^{\prime})\) for the angle between \(\bm{v}\) and \(\bm{v}^{\prime}\), and \(\operatorname{cone}\{\bm{v}_{1},\ldots,\bm{v}_{n}\}\coloneqq\{\sum_{i=1}^{n} \beta_{i}\bm{v}_{i}\mid\beta_{1},\ldots,\beta_{n}\geq 0\}\) for the cone generated by vectors \(\bm{v}_{1}\),..., \(\bm{v}_{n}\).

One-hidden layer ReLU network.For an input \(\bm{x}\in\mathbb{R}^{d}\), the output of the network is

\[h_{\bm{\theta}}(\bm{x})\coloneqq\sum_{j=1}^{m}a_{j}\,\sigma(\bm{w}_{j}^{\top }\bm{x})\,\]

where \(m\) is the width, the parameters \(\bm{\theta}=(\bm{a},\bm{W})\in\mathbb{R}^{m}\times\mathbb{R}^{m\times d}\) consist of last-layer weights \(\bm{a}=[a_{1},\ldots,a_{m}]\) and hidden-layer weights \(\bm{W}^{\top}=[\bm{w}_{1},\ldots,\bm{w}_{m}]\), and \(\sigma(u)\coloneqq\max\{u,0\}\) is the ReLU function.

Balanced initialisation.For all \(j\in[m]\) let

\[\bm{w}_{j}^{0} \coloneqq\lambda\,\bm{z}_{j} a_{j}^{0} \coloneqq s_{j}\|\bm{w}_{j}^{0}\|\]

where \(\lambda>0\) is the initialisation scale, \(\bm{z}_{j}\in\mathbb{R}^{d}\setminus\{\bm{0}\}\), and \(s_{j}\in\{\pm 1\}\).

A precise upper bound on \(\lambda\) will be stated in Assumption 2.

We regard the initial unscaled hidden-layer weights \(\bm{z}_{j}\) and last-layer signs \(s_{j}\) as given, without assuming any specific random distributions for them. For example, we might have that each \(\bm{z}_{j}\) consists of \(d\) independent centred Gaussians with variance \(\frac{1}{d\,m}\) and each \(s_{j}\) is uniform over \(\{\pm 1\}\).

We consider only initialisations for which the layers are balanced, i.e. \(|a_{j}^{0}|=\|\bm{w}_{j}^{0}\|\) for all \(j\in[m]\). Since more generally each difference \((a_{j}^{t})^{2}-\|\bm{w}_{j}^{t}\|^{2}\) is constant throughout training (Du et al., 2018, Theorem 2.1) and we focus on small initialisation scales that tend to zero, this restriction (which is also present in Boursier et al. (2022)) is minor but simplifies our analysis.

Neuron-labelled correlated inputs.The teacher neuron \(\bm{v}^{*}\in\mathbb{R}^{d}\) and the training dataset \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\subseteq(\mathbb{R}^{d}\setminus\{\bm{0}\})\times \mathbb{R}\) are such that for all \(i\) we have

\[y_{i} =\sigma(\bm{v}^{*\top}\bm{x}_{i}) \measuredangle(\bm{v}^{*},\bm{x}_{i})<\pi/4\.\]

In particular, since the angles between \(\bm{v}^{*}\) and the training points \(\bm{x}_{i}\) are acute, each label \(y_{i}\) is positive.

To apply our results to a network with biases in the hidden layer and to a teacher neuron with a bias, one can work in dimension \(d+1\) and extend the training points to \(\left[\begin{smallmatrix}\bm{x}_{i}\\ 1\end{smallmatrix}\right]\).

Mean square loss gradient flow.For the regression task of learning the teacher neuron by the one-hidden layer ReLU network, we use the standard mean square empirical loss

\[L(\bm{\theta})\coloneqq\tfrac{1}{2n}{\sum_{i=1}^{n}(y_{i}-h_{\bm{\theta}}(\bm{ x}_{i}))^{2}}\.\]

Our theoretical analysis concentrates on training by gradient flow, which from an initialisation as above evolves the network parameters by descending along the gradient of the loss by infinitesimal steps in continuous time (Li et al., 2019). Formally, we consider any parameter trajectory \(\bm{\theta}^{t}\colon[0,\infty)\to\mathbb{R}^{m}\times\mathbb{R}^{m\times d}\) that is absolutely continuous on every compact subinterval, and that satisfies the differential inclusion

\[\mathrm{d}\bm{\theta}^{t}/\mathrm{d}t\in-\partial L(\bm{\theta}^{t})\quad \text{for almost all $t\in[0,\infty)$}\,\]

where \(\partial L\) denotes the Clarke (1975) subdifferential of the loss function (which is locally Lipschitz).

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

sign is negative then it turns to remove from its active half-space all training points that were initially inside. Moreover, those training points cross the activation boundary in the same order as they cross the half-space boundary of the corresponding yardstick trajectory \(\bm{\alpha}_{j}^{t}\), and at approximately the same times (cf. Proposition 2).

**Lemma 3**.: _For all \(j\in J_{+}\cup J_{-}\) there exist unique \(0=t_{j}^{0}<t_{j}^{1}<\ldots<t_{j}^{n_{j}}\) such that for all \(\ell\in[n_{j}]\):_

1. \(I_{s_{j}}(\bm{w}_{j}^{t})=I_{s_{j}}(\bm{z}_{j})\cup\{i_{j}^{1},\ldots,i_{j}^{ \ell-1}\}\) _for all_ \(t\in(t_{j}^{\ell-1},t_{j}^{t})\)_;_
2. \(I_{0}(\bm{w}_{j}^{t})=\emptyset\) _for all_ \(t\in(t_{j}^{\ell-1},t_{j}^{\ell})\)_, and_ \(I_{0}\Big{(}\bm{w}_{j}^{t_{j}^{\ell}}\Big{)}=\{i_{j}^{\ell}\}\)_;_
3. \(|\tau_{j}^{\ell}-t_{j}^{\ell}|\leq\lambda^{1-\big{(}1+\frac{3\ell-1}{3n_{j}} \big{)}\varepsilon}\)_._

The preceding lemma is proved by establishing, for this first phase of the training, non-asymptotic upper bounds on the Euclidean norms of the hidden neurons and hence on the absolute values of the network outputs, and inductively over the stage index \(\ell\), on the distances between the unit-sphere normalisations of \(\bm{\alpha}_{j}^{t}\) and \(\bm{w}_{j}^{t}\). Based on that analysis, we then obtain that each negative-sign hidden neuron does not grow from its initial length and deactivates by time \(T_{0}\coloneqq\max_{j\in J_{+}\cup J_{-}}\tau_{j}^{n_{j}}+1\).

**Lemma 4**.: _For all \(j\in J_{-}\) we have:_

\[\|\bm{w}_{j}^{T_{0}}\|\leq\lambda\|\bm{z}_{j}\| \bm{w}_{j}^{t}=\bm{w}_{j}^{T_{0}}\quad\text{for all $t\geq T_{0}$}\.\]

We also obtain that, up to a later time \(T_{1}\coloneqq\varepsilon\ln(1/\lambda)/\|\bm{\gamma}_{[n]}\|\), each positive-sign hidden neuron: grows but keeps its length below \(2\|\bm{z}_{j}\|\lambda^{1-\varepsilon}\), continues to align to the vector \(\bm{\gamma}_{[n]}\) up to a cosine of at least \(1-\lambda^{\varepsilon}\), and maintains bounded by \(\lambda^{1-3\varepsilon}\) the difference between the logarithm of its length divided by the initialisation scale and the logarithm of the corresponding yardstick vector length.

**Lemma 5**.: _For all \(j\in J_{+}\) we have:_

\[\|\bm{w}_{j}^{T_{1}}\|<2\|\bm{z}_{j}\|\lambda^{1-\varepsilon}\qquad\overline{ \bm{w}}_{j}^{T_{1}^{\top}}\overline{\bm{\gamma}}_{[n]}\geq 1-\lambda^{ \varepsilon}\qquad|\ln\|\bm{\alpha}_{j}^{T_{1}}\|-\ln\|\bm{w}_{j}^{T_{1}}/ \lambda\|\|\leq\lambda^{1-3\varepsilon}\.\]

## 5 Second phase: growth and convergence

We next analyse the gradient flow subsequent to the deactivation of the negative-sign hidden neurons by time \(T_{0}\) and the alignment of the positive-sign ones up to time \(T_{1}\), and establish that the loss converges to zero at a rate which is exponential and does not depend on the initialisation scale \(\lambda\).

**Theorem 6**.: _Under Assumptions 1 and 2, there exists a time \(T_{2}<\ln(1/\lambda)(4+\varepsilon)d\Delta^{2}/\delta^{6}\) such that for all \(t\geq 0\) we have \(L(\bm{\theta}^{T_{2}+t})<0.5\,\Delta^{2}\,\mathrm{e}^{-t\cdot 0.4\,\delta^{4}/ \Delta^{2}}\)._

In particular, for \(\varepsilon=1/4\) and \(\lambda=\Big{(}(m\,n^{n})^{9\Delta^{2}/\delta^{3}}\Big{)}^{-3/\varepsilon}\) (cf. Assumption 2), the first bound in Theorem 6 becomes \(T_{2}<(\ln m+n\ln n)\,d\cdot 17\cdot 27\,\Delta^{4}/\delta^{9}\).

The proof of Theorem 6 is in large part geometric, with a key role played by a set \(\mathcal{S}\coloneqq\mathcal{S}_{1}\cup\cdots\cup\mathcal{S}_{d}\) in predictor space, whose constituent subsets are defined as

\[\mathcal{S}_{\ell}\coloneqq\left\{\bm{v}=\sum_{k=1}^{d}\nu_{k}\bm{u}_{k}\ \left|\ \ \bigwedge_{1\leq k<\ell}\Omega_{k}\ \wedge\ \Phi_{\ell}\ \wedge\bigwedge_{t\leq k<k^{\prime}\leq d}(\Psi_{k,k^{\prime}}^{\downarrow} \wedge\Psi_{k,k^{\prime}}^{\uparrow})\ \wedge\ \Xi\right\}\,\]

where the individual constraints are as follows (here \(\eta_{0}\coloneqq\infty\) so that e.g. \(\frac{\eta_{1}}{2\eta_{0}}=0\)):

\[\Omega_{k} \colon\ 1<\frac{\nu_{k}}{\nu_{k}^{*}}\qquad\Phi_{\ell}\colon\ \frac{\eta_{\ell}}{2\eta_{\ell-1}}<\frac{\nu_{\ell}}{\nu_{\ell}^{*}}\leq 1 \qquad\Psi_{k,k^{\prime}}^{\downarrow} \colon\ \frac{\eta_{k^{\prime}}}{2\eta_{k}}\frac{\nu_{k}}{\nu_{k}^{*}} <\frac{\nu_{k^{\prime}}}{\nu_{k^{\prime}}^{*}}\] \[\Xi \colon\ \overline{\bm{v}}^{\top}\overline{\bm{X}\bm{X}^{\top}(\bm{v} ^{*}-\bm{v})}>\lambda^{\varepsilon/3} \qquad\qquad\qquad\qquad\Psi_{k,k^{\prime}}^{\uparrow} \colon\ \frac{\nu_{k^{\prime}}}{\nu_{k^{\prime}}^{*}}<1-\left(1-\frac{\nu_{k}}{\nu_{k}^{ *}}\right)^{\frac{1}{2}+\frac{\eta_{k^{\prime}}}{2\eta_{k}}}\.\]

Thus \(\mathcal{S}\) is connected, open, and constrained by \(\Xi\) to be within the ellipsoid \(\bm{v}^{\top}\bm{X}\bm{X}^{\top}(\bm{v}^{*}-\bm{v})=0\) which is centred at \(\frac{\bm{v}^{*}}{2}\), with the remaining constraints slicing off further regions by straight or curved boundary surfaces.

In the most complex component of this work, we show that, for all \(t\geq T_{1}\), the trajectory of the sum \(\bm{v}^{t}\coloneqq\sum_{j\in J_{+}}a_{j}^{t}\bm{w}_{j}^{t}\) of the active hidden neurons weighted by the last layer stays inside \(\mathcal{S}\), and the cosines of the angles between the neurons remain above \(1-4\lambda^{\varepsilon}\). This involves proving that each face of the boundary of \(\mathcal{S}\) is repelling for the training dynamics when approached from the inside; we remark that, although that is in general false for the entire boundary of the constraint \(\Xi\), it is in particular true for its remainder after the slicing off by the other constraints. We also show that all points in \(\mathcal{S}\) are positively correlated with all training points, which together with the preceding facts implies that, during this second phase of the training, the network behaves approximately like a linear one-hidden layer one-neuron network. Then, as the cornerstone of the rest of the proof, we show that, for all \(t\geq T_{2}\), the gradient of the loss satisfies a Polyak-Lojasiewicz inequality \(\|\nabla L(\bm{\theta}^{t})\|^{2}>\frac{2\eta_{d}\|\bm{\gamma}_{\eta_{1}}\|} {5\eta_{1}}L(\bm{\theta}^{t})\). Here \(T_{2}\coloneqq\inf\{t\geq T_{1}\mid\nu_{1}^{t}/\nu_{1}^{*}\geq 1/2\}\) is a time by which the network has departed from the initial saddle, more precisely when the first coordinate \(\nu_{1}^{t}\) of the bundle vector \(\bm{v}^{t}\) with respect to the basis consisting of the eigenvectors of the matrix \(\frac{1}{n}\bm{X}\bm{X}^{\top}\) crosses the half-way threshold to the first coordinate \(\nu_{1}^{*}\) of the teacher neuron.

The interior of the ellipsoid in the constraint \(\Xi\) actually consists of all vectors that have an acute angle with the derivative of the training dynamics in predictor space, and the "padding" of \(\lambda^{\varepsilon/3}\) is present because the derivative of the bundle vector \(\bm{v}^{t}\) is "noisy" due to the latter being made up of the approximately aligned neurons. The remaining constraints delimit the subsets \(\mathcal{S}_{1}\),..., \(\mathcal{S}_{d}\) of the set \(\mathcal{S}\), through which the bundle vector \(\bm{v}_{t}\) passes in that order, with each unique "handover" from \(\mathcal{S}_{\ell}\) to \(\mathcal{S}_{\ell+1}\) happening exactly when the corresponding coordinate \(\nu_{\ell}^{t}\) exceeds its target \(\nu_{\ell}^{*}\). The non-linearity of the constraints \(\Psi_{k,k^{\prime}}^{\dagger}\) is needed to ensure the repelling for the training dynamics.

## 6 Implicit bias of gradient flow

Let us denote the set of all balanced networks by

\[\Theta\coloneqq\{(\bm{a},\bm{W})\in\mathbb{R}^{m}\times\mathbb{R}^{m\times d} \;\mid\;\forall j\in\text{[}m\text{]:}\;|a_{j}|=\|\bm{w}_{j}\|\}\]

and the subset in which all non-zero hidden neurons are positive scalings of \(\bm{v}^{*}\), have positive last-layer weights, and have lengths whose squares sum up to \(\|\bm{v}^{*}\|=1\), by

\[\Theta_{\bm{v}^{*}}\coloneqq\{(\bm{a},\bm{W})\in\Theta\;\mid\;\sum_{j=1}^{m} \|\bm{w}_{j}\|^{2}=1\;\wedge\;\forall j\in\text{[}m\text{]:}\;\bm{w}_{j}\neq \bm{0}\;\Rightarrow\;(\overline{\bm{w}}_{j}=\bm{v}^{*}\;\wedge\;a_{j}>0)\}\;.\]

Our main result establishes that, as the initialisation scale \(\lambda\) tends to zero, the networks with zero loss to which the gradient flow converges tend to a network in \(\Theta_{\bm{v}^{*}}\). The explicit subscripts indicate the dependence on \(\lambda\) of the parameter vectors. The proof builds on the preceding results and involves a careful control of accumulations of approximation errors over lengthy time intervals.

**Theorem 7**.: _Under Assumptions 1 and 2, \(L\Big{(}\lim_{t\to\infty}\bm{\theta}_{\lambda}^{t}\Big{)}=0\) and \(\lim_{\lambda\to 0^{+}}\lim_{t\to\infty}\bm{\theta}_{\lambda}^{t}\in\Theta_{\bm{v}^{*}}\)._

## 7 Interpolators with minimum norm

To compare the set \(\Theta_{\bm{v}^{*}}\) of balanced rank-\(1\) interpolator networks with the set of all minimum-norm interpolator networks, in this section we focus on training datasets of cardinality \(d\), we assume the network width is greater than \(1\) (otherwise the rank is necessarily \(1\)), and we exclude the threshold case of Lebesgue measure zero where \(\mathcal{M}=0\). The latter measurement of the training dataset is defined below in terms of angles between the teacher neuron and vectors in any two cones generated by different generators of the dual of the cone of all training points.

Let \([\bm{\chi}_{1},\ldots,\bm{\chi}_{d}]^{\top}\coloneqq\bm{X}^{-1}\) and

\[\mathcal{M}\coloneqq\max\left\{\begin{array}{c|c}\emptyset\subsetneq K \subsetneq\text{[}d\text{]}\\ \left.\begin{array}{l}\wedge\bm{0}\neq\bm{p}\in\operatorname{cone}\{\bm{ \chi}_{k}\mid k\in K\}\\ \wedge\bm{0}\neq\bm{q}\in\operatorname{cone}\{\bm{\chi}_{k}\mid k\notin K\} \end{array}\right\}\;.\end{array}\right.\]

**Assumption 3**.: \(n=d\)_, \(m>1\), and \(\mathcal{M}\neq 0\)._

We obtain that, surprisingly, \(\Theta_{\bm{v}^{*}}\) equals the set of all interpolators with minimum Euclidean norm if \(\mathcal{M}<0\), but otherwise they are disjoint.

**Theorem 8**.: _Under Assumptions 1 and 3:_

1. _if_ \(\mathcal{M}<0\) _then_ \(\Theta_{\bm{v}^{*}}\) _is the set of all global minimisers of_ \(\|\bm{\theta}\|^{2}\) _subject to_ \(L(\bm{\theta})=0\)_;_
2. _if_ \(\mathcal{M}>0\) _then no point in_ \(\Theta_{\bm{v}^{*}}\) _is a global minimiser of_ \(\|\bm{\theta}\|^{2}\) _subject to_ \(L(\bm{\theta})=0\)_._

For each of the two cases, we provide a family of example datasets in the appendix. We remark that a sufficient condition for \(\mathcal{M}<0\) to hold is that the inner product of any two distinct rows \(\bm{\chi}_{k}\) of the inverse of the dataset matrix \(\bm{X}\) is non-positive, i.e. that the inverse of the Gram matrix of the dataset (in our setting this Gram matrix is positive) is a Z-matrix (cf. e.g. Fiedler and Ptak (1962)). Also, if the training points were orthogonal then all the \(\cos\measuredangle(\bm{p},\bm{q})\) terms in the definition of \(\mathcal{M}\) would be zero and consequently we would have \(\mathcal{M}<0\); this is consistent with the result that, per sign class of the training labels in the orthogonal setting, minimising the Euclidean norm of interpolators coincides with minimising their rank (Boursier et al., 2022, Appendix C).

## 8 Experiments

We consider two schemes for generating the training dataset, where \(\mathbb{S}^{d-1}\) is the unit sphere in \(\mathbb{R}^{d}\).

**Centred:** We sample \(\bm{\mu}\) from \(\mathcal{U}(\mathbb{S}^{d-1})\), then sample \(\bm{x}_{1},\ldots,\bm{x}_{d}\) from \(\mathcal{N}(\bm{\mu},\frac{\rho}{d}\bm{I}_{d})\) where \(\rho=1\), and finally set \(\bm{v}^{*}=\bm{\mu}\). This distribution has the property that, in high dimensions, the angles between the teacher neuron \(\bm{v}^{*}\) and the training points \(\bm{x}_{i}\) concentrate around \(\pi/4\). We exclude rare cases where some of these angles exceed \(\pi/2\).

**Uncentred:** This is the same, except that we use \(\rho=\sqrt{2}-1\), sample one extra point \(\bm{x}_{0}\), and finally set \(\bm{v}^{*}=\overline{\bm{x}}_{0}\). Here the angles between \(\bm{v}^{*}\) and \(\bm{x}_{i}\) also concentrate around \(\pi/4\) in high dimensions, but the expected distance between \(\bm{v}^{*}\) and \(\bm{\mu}\) is \(\sqrt{\rho}\).

For each of the two dataset schemes, we train a one-hidden layer ReLU network of width \(m=200\) by gradient descent with learning rate \(0.01\), from a balanced initialisation such that \(\bm{z}_{j}\mathop{\sim}^{\text{i.i.d.}}\mathcal{N}(\bm{0},\frac{1}{d\,m}\bm{I }_{d})\) and \(s_{j}\mathop{\sim}^{\text{i.i.d.}}\mathcal{U}\{\pm 1\}\), and for a range of initialisation scales \(\lambda\) and input dimensions \(d\).2

Footnote 2: We are making code to run the experiments available at https://github.com/englert-m/shallow_ReLU_dynamics.

We present in Figure 1 some results from considering initialisation scales \(\lambda=4^{2},4^{1},\ldots,4^{-12},4^{-13}\) and input dimensions \(d=4,16,64,256,1024\), where we train until the number of iterations

Figure 1: Dependence of the maximum angle between active hidden neurons on the initialisation scale \(\lambda\), for two generation schemes of the training dataset and a range of input dimensions, at the end of the training. Both axes are logarithmic, and each point plotted shows the median over five trials.

reaches \(2\cdot 10^{7}\) or the loss drops below \(10^{-9}\). The plots are in line with Theorem7, showing how the maximum angle between active hidden neurons at the end of the training decreases with \(\lambda\).

Figure2 on the left illustrates the exponentially fast convergence of the training loss (cf. Theorem6), and on the right how the implicit bias can result in good generalisation. The test loss is computed over an input distribution which is different from that of the training points, namely we sample \(64\) test inputs from \(\mathcal{N}(\mathbf{0},\bm{I}_{d})\). These plots are for initialisation scales \(\lambda=4^{-2},4^{-3},\ldots,4^{-7},4^{-8}\).

## 9 Conclusion

We provided a detailed analysis of the dynamics of training a shallow ReLU network by gradient flow from a small initialisation for learning a single neuron which is correlated with the training points, establishing convergence to zero loss and implicit bias to rank minimisation in parameter space. We believe that in particular the geometric insights we obtained in order to deal with the complexities of the multi-stage alignment of hidden neurons followed by the simultaneous evolution of their norms and directions, will be useful to the community in the ongoing quest to understand implicit bias of gradient-based algorithms for regression tasks using non-linear networks.

A major direction for future work is to bridge the gap between, on one hand, our assumption that the angles between the teacher neuron and the training points are less than \(\pi/4\), and the other, the assumption of Boursier et al. (2022) that the training points are orthogonal, while keeping a fine granularity of description. We expect this to be difficult because it seems to require handling bundles of approximately aligned neurons which may have changing sets of training points in their active half-spaces and which may separate during the training. However, it should be straightforward to extend our results to orthogonally separable datasets and two teacher ReLU neurons, where each of the latter has an arbitrary sign, labels one of the two classes of training points, and has angles less than \(\pi/4\) with them; the gradient flow would then pass close to a second saddle point, where the labels of one of the classes have been nearly fitted but the hidden neurons that will fit the labels of the other class are still small. We report on related numerical experiments in the appendix.

We also obtained a condition on the dataset that determines whether rank minimisation and Euclidean norm minimisation for interpolator networks coincide or are distinct. Although this dichotomy remains true if the \(\pi/4\) correlation bound is relaxed to \(\pi/2\), the implicit bias of gradient flow in that extended setting is an open question. Other directions for future work include considering multi-neuron teacher networks, student networks with more than one hidden layer, further non-linear activation functions, and gradient descent instead of gradient flow; also refining the bounds on the initialisation scale and the convergence time.

Figure 2: Evolution of the training loss, and of an outside distribution test loss, during training for an example centred training dataset in dimension \(16\) and width \(200\). The horizontal axes, logarithmic for the training loss and linear for the test loss, show iterations. The vertical axes are logarithmic.

## Acknowledgments and Disclosure of Funding

We acknowledge the Centre for Discrete Mathematics and Its Applications at the University of Warwick for partial support, and the Scientific Computing Research Technology Platform at the University of Warwick for providing the compute cluster on which the experiments presented in this paper were run.

## References

* Azulay et al. (2021) Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake E. Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry. On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent. In _ICML_, pages 468-477, 2021.
* Belkin et al. (2019) Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proc. Natl. Acad. Sci._, 116(32):15849-15854, 2019.
* Bolte et al. (2010) Jerome Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet. Characterizations of Lojasiewicz inequalities: subgradient flows, talweg, convexity. _Trans. Amer. Math. Soc._, 362(6):3319-3363, 2010.
* Boursier and Flammarion (2023) Etienne Boursier and Nicolas Flammarion. Penalising the biases in norm regularisation enforces sparsity. _CoRR_, abs/2303.01353, 2023. Accepted to NeurIPS 2023.
* Boursier et al. (2022) Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow ReLU networks for square loss and orthogonal inputs. In _NeurIPS_, 2022.
* Clarke (1975) Frank H. Clarke. Generalized gradients and applications. _Trans. Amer. Math. Soc._, 205:247-262, 1975.
* Davis et al. (2020) Damek Davis, Dmitriy Drusvyatskiy, Sham M. Kakade, and Jason D. Lee. Stochastic Subgradient Method Converges on Tame Functions. _Found. Comput. Math._, 20(1):119-154, 2020.
* Du et al. (2018) Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced. In _NeurIPS_, pages 382-393, 2018.
* Englert and Lazic (2022) Matthias Englert and Ranko Lazic. Adversarial Reprogramming Revisited. In _NeurIPS_, 2022.
* Ergen and Pilanci (2021) Tolga Ergen and Mert Pilanci. Convex Geometry and Duality of Over-parameterized Neural Networks. _J. Mach. Learn. Res._, 22(212):1-63, 2021.
* Even et al. (2023) Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability. _CoRR_, abs/2302.08982, 2023. Accepted to NeurIPS 2023.
* Fiedler and Ptak (1962) Miroslav Fiedler and Vlastimil Ptak. On matrices with non-positive off-diagonal elements and positive principal minors. _Czechoslovak Mathematical Journal_, 12(3):382-400, 1962.
* Frei et al. (2020) Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic Learning of a Single Neuron with Gradient Descent. In _NeurIPS_, 2020.
* Frei et al. (2023a) Spencer Frei, Gal Vardi, Peter L. Bartlett, and Nathan Srebro. Benign Overfitting in Linear Classifiers and Leaky ReLU Networks from KKT Conditions for Margin Maximization. In _COLT_, pages 3173-3228, 2023a.
* Frei et al. (2023b) Spencer Frei, Gal Vardi, Peter L. Bartlett, and Nathan Srebro. The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks. _CoRR_, abs/2303.01456, 2023b. Accepted to NeurIPS 2023.
* Frei et al. (2023c) Spencer Frei, Gal Vardi, Peter L. Bartlett, Nathan Srebro, and Wei Hu. Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data. In _ICLR_, 2023c.
* Gunasekar et al. (2017) Suriya Gunasekar, Blake E. Woodworth, Srinadhhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit Regularization in Matrix Factorization. In _NeurIPS_, pages 6151-6159, 2017.
* Haim et al. (2022) Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani. Reconstructing Training Data From Trained Neural Networks. In _NeurIPS_, 2022.
* Jacot et al. (2021) Arthur Jacot, Francois Ged, Berlin Simsek, Clement Hongler, and Franck Gabriel. Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization Training, Symmetry, and Sparsity. _CoRR_, abs/2106.15933, 2021.
* Jader et al. (2019)Arnulf Jentzen and Adrian Riekert. Convergence analysis for gradient flows in the training of artificial neural networks with ReLU activation. _J. Math. Anal. Appl._, 517(2):126601, 2023.
* Ji and Telgarsky (2020) Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In _NeurIPS_, 2020.
* Jin et al. (2023) Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon S. Du, and Jason D. Lee. Understanding Incremental Learning of Gradient Descent: A Fine-grained Analysis of Matrix Sensing. In _ICML_, pages 15200-15238, 2023.
* Lee et al. (2022) Sangmin Lee, Byeongsu Sim, and Jong Chul Ye. Magnitude and Angle Dynamics in Training Single ReLU Neurons. _CoRR_, abs/2209.13394, 2022.
* Li et al. (2019) Qianxiao Li, Cheng Tai, and Weinan E. Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations. _J. Mach. Learn. Res._, 20(40):1-47, 2019.
* Li et al. (2021) Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning. In _ICLR_, 2021.
* Lyu and Li (2020) Kaifeng Lyu and Jian Li. Gradient Descent Maximizes the Margin of Homogeneous Neural Networks. In _ICLR_, 2020.
* Lyu et al. (2021) Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias. In _NeurIPS_, pages 12978-12991, 2021.
* Maennel et al. (2018) Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient Descent Quantizes ReLU Network Features. _CoRR_, abs/1803.08367, 2018.
* Melamed et al. (2023) Odelia Melamed, Gilad Yehudai, and Gal Vardi. Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Data Manifolds. _CoRR_, abs/2303.00783, 2023.
* Min et al. (2023) Hancheng Min, Rene Vidal, and Enrique Mallada. Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization. _CoRR_, abs/2307.12851, 2023.
* Neyshabur et al. (2017) Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring Generalization in Deep Learning. In _NeurIPS_, pages 5947-5956, 2017.
* Ongie et al. (2020) Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case. In _ICLR_, 2020.
* Pesme and Flammarion (2023) Scott Pesme and Nicolas Flammarion. Saddle-to-Saddle Dynamics in Diagonal Linear Networks. _CoRR_, abs/2304.00488, 2023. Accepted to NeurIPS 2023.
* Phuong and Lampert (2021) Mary Phuong and Christoph H. Lampert. The inductive bias of ReLU networks on orthogonally separable data. In _ICLR_, 2021.
* Polyak (1963) B.T. Polyak. Gradient methods for the minimisation of functionals. _USSR Computational Mathematics and Mathematical Physics_, 3(4):864-878, 1963.
* Razin and Cohen (2020) Noam Razin and Nadav Cohen. Implicit Regularization in Deep Learning May Not Be Explainable by Norms. In _NeurIPS_, 2020.
* Sarussi et al. (2021) Roei Sarussi, Alon Brutzkus, and Amir Globerson. Towards Understanding Learning in Neural Networks with Linear Teachers. In _ICML_, pages 9313-9322, 2021.
* Savarese et al. (2019) Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm networks look in function space? In _COLT_, pages 2667-2690, 2019.
* Stewart et al. (2023) Lawrence Stewart, Francis Bach, Quentin Berthet, and Jean-Philippe Vert. Regression as Classification: Influence of Task Formulation on Neural Network Features. In _AISTATS_, pages 11563-11582, 2023.
* Timor et al. (2023) Nadav Timor, Gal Vardi, and Ohad Shamir. Implicit Regularization Towards Rank Minimization in ReLU Networks. In _ALT_, pages 1429-1459, 2023.
* Vardi (2023) Gal Vardi. On the Implicit Bias in Deep-Learning Algorithms. _Commun. ACM_, 66(6):86-93, 2023.
* Vardi and Shamir (2021) Gal Vardi and Ohad Shamir. Implicit Regularization in ReLU Networks with the Square Loss. In _COLT_, pages 4224-4258, 2021.
* Vardi et al. (2021) Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a Single Neuron with Bias Using Gradient Descent. In _NeurIPS_, pages 28690-28700, 2021.
* Vardi et al. (2020)* Vardi et al. [2022] Gal Vardi, Gilad Yehudai, and Ohad Shamir. Gradient Methods Provably Converge to Non-Robust Networks. In _NeurIPS_, 2022.
* Wang and Ma [2022] Mingze Wang and Chao Ma. Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks. In _NeurIPS_, 2022.
* Wang and Pilanci [2022] Yifei Wang and Mert Pilanci. The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program. In _ICLR_, 2022.
* Woodworth et al. [2020] Blake E. Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and Rich Regimes in Overparametrized Models. In _COLT_, pages 3635-3673, 2020.
* Xu and Du [2023] Weihang Xu and Simon Du. Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron. In _COLT_, pages 1155-1198, 2023.
* Yehudai and Shamir [2020] Gilad Yehudai and Ohad Shamir. Learning a Single Neuron with Gradient Methods. In _COLT_, pages 3756-3786, 2020.
* Yun et al. [2021] Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural networks. In _ICLR_, 2021.
* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Commun. ACM_, 64(3):107-115, 2021.