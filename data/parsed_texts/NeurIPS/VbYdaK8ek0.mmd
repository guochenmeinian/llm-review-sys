# Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing

 Wei Dong\({}^{1,4}\) Dawei Yan\({}^{1}\) Zhijun Lin\({}^{2}\) Peng Wang\({}^{3}\)

\({}^{1}\)College of Information and Control Engineering,

Xi'an University of Architecture and Technology.

\({}^{2}\)School of Computer Science, Northwestern Polytechnical University.

\({}^{3}\)School of Computer Science and Engineering,

University of Electronic Science and Technology of China.

\({}^{4}\)Xi'an Hypersonic Measurement Technology Co., Ltd.

Corresponding author. Email address: p.wang6@hotmail.com

###### Abstract

The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adaptive adapters. This parameter-sharing strategy in adapter design allows us to further reduce the number of new parameters while maintaining satisfactory performance, thereby offering a promising approach to compress the adaptation cost. We conduct experiments on 24 downstream image classification tasks using various Vision Transformer variants to evaluate our method. The results demonstrate that our approach achieves compelling transfer learning performance with a reduced parameter count. Our code is available at https://github.com/DavidYanAnDe/ARC.

## 1 Introduction

The utilization of large-scale pre-trained models for various downstream tasks has garnered significant interest in the computer vision community [1; 2; 3; 4]. These models continually push the boundaries of downstream task performance while eliminating the need for task-specific model design and training. In early attempts, a commonly adopted transfer learning strategy involved directly fine-tuning a pre-trained model on downstream tasks. However, the full fine-tuning strategy suffers from two major drawbacks: (1) Updating large-scale parameters is prohibitively expensive and typically requires a substantial amount of training data to prevent overfitting. (2) As the sizes of state-of-the-art pre-trained models continue to increase, it becomes impractical and unsustainable to store a distinct set of model weights for each downstream task.

In contrast to fine-tuning the entire pre-trained network, recent research has focused on parameter-efficient model adaptation. The core idea behind this line of work is to keep the majority of pre-trained parameters frozen and only update or introduce a small fraction of task-specific parameters. Several methods fall under this umbrella, including prompt tuning [5; 6], visual adapter [7; 8], and linear feature modulation [9]. These methods have demonstrated competitive or even superior performance compared to full fine-tuning while significantly reducing the adaptation cost. They differ in the design of lightweight adapters and how these adapters interact with the pre-trained parameters. Prompt tuning methods [5; 6] adapt the features of pre-trained Vision Transformers by introducing trainable task-specific tokens into one or multiple attention layers. Visual adapter [7; 8] injects a non-linear lightweight adapter with a bottleneck architecture between layers of the pre-trained model to adjust the feature distribution. Such non-linear adapters are simplified using linear transformations such as shifting and scaling [9] to directly modulate the pre-trained features.

In this work, we not only focus on designing lightweight adapters but also emphasize the importance of adaptation parameter reusability in further compressing the adaptation cost. We adopt a low-rank design for the adapter using a bottleneck operation but propose a novel approach. Unlike other methods that place the adapter in different layers and directly learn different parameters for each adapter to cater to layer-wise variation, we propose sharing the down/up projections in the low-rank adapter across different layers and simply learning low-dimensional re-scaling coefficients to re-compose the linear projections into layer-adaptive adapters. The idea of Adapter Re-Composing (ARC) is motivated by the observation that naturally derived adaptation matrices can exhibit extremely low-rank characteristics, even when not explicitly designed as such. This implies the possibility of using a shared "basis" to re-compose the adapters. Furthermore, we design the low-rank adapters using symmetric down-projection and up-projection matrices, which further reduces the parameter size. Due to their linear nature and careful positioning design, our adapters can be seamlessly integrated into the pre-trained network, as in [7; 9; 10], without adding extra computation during the inference phase.

We evaluate our method on various large Vision Transformer models, including ViT-B [1] and its variants such as ViT-L [1], ViT-H [1], and Swin-B [11], using 24 downstream image classification benchmark datasets. The experimental results demonstrate that our method achieves compelling transfer learning performance while maintaining a smaller parameter size.

The key contributions of this paper are summarized as follows:

* We approach efficient pre-trained model adaptation from a novel perspective by exploring the reusability of adaptation parameters, which goes beyond existing works that primarily focus on the lightweight design of adapter structures.
* We introduce the Adapter Re-Composing (ARC) strategy, which shares bottleneck operation's down-/up-projections across layers and utilizes lower-dimensional re-composing coefficients to create layer-adaptive adapters. This approach enables fewer parameters than prior works.
* Our parameter sharing scheme in the ARC method prevents a linear increase in parameter size with the number of layers, ensuring better scalability, particularly for larger-scale models.
* Through extensive experiments on various Vision Transformer variations and numerous downstream tasks, we show that our method achieves highly competitive transfer learning performance while maintaining a relatively low level of additional parameter.

## 2 Related work

In this section, we present a concise review of the existing literature, focusing on two key areas: (1) Pre-training and fine-tuning, and (2) Parameter-efficient transfer learning.

Pre-training and fine-tuning.Pre-training and fine-tuning, also known as transfer learning [12; 13; 14], is a popular approach that utilizes large-scale datasets [15; 16; 17; 18; 19] to train models for adaptation to different downstream tasks. By extracting knowledge from these datasets and encoding it into parameters, models can be fine-tuned for specific tasks, resulting in improved performance compared to models without pre-training. The availability of large-scale datasets [17] has led to significant advancements in performance and convergence speed for downstream tasks. Scaling up models [20; 21] to handle the growing data volume enhances data utilization and improves the efficiency and robustness of pre-trained models across various data distributions and noise levels. Large models [1; 11; 22] also benefit from self-supervised pre-training [2; 23] using unlabeled data, reducing the cost, duration, and quality issues associated with human annotation. By leveraging this approach, models can effectively extract knowledge from readily available unlabeled data, further enhancing their generalization performance in downstream tasks.

Parameter-efficient transfer learning.The exponential increase in model parameters presents a computational challenge when fine-tuning the entire network on downstream tasks. In the field of NLP, researchers have explored parameter-efficient transfer learning approaches [7; 24; 25; 26; 27; 28] that train a subset of the model or add new modules with fewer parameters while achieving comparable or even superior performance. Inspired by the success of NLP, several notable works [8; 29; 30] have emerged in the computer vision domain. One approach, known as Prompt Tuning [5; 6], addresses the distribution mismatch between pre-training and downstream tasks by learning task-specific tokens. Adapter-like methods [8; 7; 29] insert trainable modules, such as MLPs with activation functions and residual structures, into the network to facilitate transfer learning. LoRA [24] exploits the low-rank update to a large-scale frozen model and introduces a bypass to the original parameter matrix to mimic the fine-tuning of the entire model parameters. SSF [9] introduces lightweight scaling and shift operations to modulate the pre-trained representations. The core concepts of these aforementioned works are visually summarized in Fig. 1.

As an advancement in visual adaptation, ARC addresses the limitations of Prompt Tuning, which requires different prompt designs for different downstream tasks. Moreover, ARC introduces the innovative concept of bottleneck matrix reuse, achieving state-of-the-art performance with minimal adaptation cost comparing to other rank-decomposition strategies. Additionally, ARC employs a linear design for the adapters and inherits the benefits of re-parameterization [9; 10; 24], ensuring that inference does not introduce any additional computational complexity.

## 3 Approach

In this section, we start by providing an introduction to the notations, symbols, and background related to Vision Transformers, which is followed by the presentation of our proposed Adapter Re-Composing (ARC) method. ARC focuses on efficient transfer learning for Vision Transformers by reusing rank-decomposition projections to adaptively compose layer-wise adapters, thereby reducing the size of learnable parameters. Additionally, we discuss the insights gained from our architecture design.

Figure 1: Visual summary of typical parameter-efficient pre-trained model adaptation methods.

### Preliminary

A plain Vision Transformer model contains a patch embedding layer and multiple encoder layers. Given an input image \(\mathbf{X}\in\mathbb{R}^{H\times W\times C}\), the patch embedding layer first splits the image into a sequence of flattened patches \(\mathbf{X}_{\mathrm{patches}}\in\mathbb{R}^{N\times(P^{2}\cdot C)}\), where \((H,W)\) is the resolution of the input image, \((P,P)\) is the resolution of each patch, \(C\) denotes the number of input channels, and \(N=H\cdot W/P^{2}\) is the number of tokens. Subsequently, the image patches are mapped to a \(D\)-dimensional embedding space through a linear projection \(\mathbf{W}\in\mathbb{R}^{(P^{2}\cdot C)\times D}\). A learnable [class] token vector \(\vec{\boldsymbol{x}}_{\mathrm{cls}}\in\mathbb{R}^{D}\) is then prepended to the sequence of \(\mathbf{X}_{\mathrm{patches}}\), and the position embeddings \(\mathbf{X}_{\mathrm{pos}}\in\mathbb{R}^{(N+1)\times D}\) are added to the sequence. The output of the patch embedding layer can be expressed as follows:

\[\mathbf{X}_{\mathrm{emb}}=[\vec{\boldsymbol{x}}_{\mathrm{cls}}^{\mathrm{T}}; \mathbf{X}_{\mathrm{patches}}\mathbf{W}]+\mathbf{X}_{\mathrm{pos}},\] (1)

where \([\cdot;\cdot]\) denotes concatenation operation. This output is then fed into several consecutive encoder layers, each consisting of a Multi-Head Attention (MHA) block and a Feed-Forward Network (FFN) block. LayerNorm (LN) is applied before each block, and residual connections are applied thereafter. The process of \(l\)-th encoder layer is defined as:

\[\begin{split}\mathbf{X}^{(l)\prime}=\mathrm{MHA}(\mathrm{LN}( \mathbf{X}^{(l-1)}))+\mathbf{X}^{(l-1)},\\ \mathbf{X}^{(l)}=\mathrm{FFN}(\mathrm{LN}(\mathbf{X}^{(l)\prime})) +\mathbf{X}^{(l)\prime},\end{split}\] (2)

where \(\mathbf{X}^{(l-1)}\) denotes input tokens in \(l\)-th layer, \(\mathbf{X}^{(l)\prime}\) indicates intermediate representations produced by MHA, and the output of \(l\)-th layer is \(\mathbf{X}^{(l)}\).

In MHA block, each Attention Head (AH) module utilizes the weight matrices \(\mathbf{W}_{q}^{(l)}\in\mathbb{R}^{D^{(l-1)}\times D_{h}^{(l)}}\), \(\mathbf{W}_{k}^{(l)}\in\mathbb{R}^{D^{(l-1)}\times D_{h}^{(l)}}\), and \(\mathbf{W}_{v}^{(l)}\in\mathbb{R}^{D^{(l-1)}\times D_{h}^{(l)}}\) for the _query_, _key_, and _value_ operations, respectively. These operations enable an exclusive attention mechanism on the normalized feature representations \(\mathbf{X}_{\mathrm{norm}}^{(l-1)}=\mathrm{LN}(\mathbf{X}^{(l-1)})\):

\[\mathbf{X}_{h}^{(l)\prime}=\mathrm{AH}_{h}(\mathbf{X}_{\mathrm{norm}}^{(l-1)}) =\mathrm{softmax}(\frac{(\mathbf{X}_{\mathrm{norm}}^{(l-1)}\mathbf{W}_{q}^{( l)})(\mathbf{X}_{\mathrm{norm}}^{(l-1)}\mathbf{W}_{k}^{(l)})^{\mathrm{T}}}{ \sqrt{D_{h}^{(l)}}})\mathbf{X}_{\mathrm{norm}}^{(l-1)}\mathbf{W}_{v}^{(l)},\] (3)

where \(D_{h}^{(l)}=\frac{D^{(l)}}{M}\) is the feature dimensionality of the output representations \(\mathbf{X}_{h}^{(l)\prime}\) for each attention head and \(M\) represents the number of attention heads. The MHA block concatenates multiple \(\{\mathbf{X}_{h}^{(l)\prime}\}\) in sequence and generates the outputs through a linear projection \(\mathbf{W}_{o}^{(l)}\in\mathbb{R}^{(M\cdot D_{h}^{(l)})\times D^{(l)}}\):

\[\mathbf{X}^{(l)\prime}=\mathrm{MHA}(\mathbf{X}_{\mathrm{norm}}^{(l-1)})=[ \mathrm{AH}_{1}(\mathbf{X}_{\mathrm{norm}}^{(l-1)}),\ \cdots,\ \mathrm{AH}_{M}(\mathbf{X}_{\mathrm{norm}}^{(l-1)})]\mathbf{W}_{o}^{(l)}.\] (4)

The FFN block consists of two linear projections with the GELU activation function in between:

\[\mathbf{X}^{(l)}=\mathrm{FFN}(\mathbf{X}_{\mathrm{norm}}^{(l)\prime})= \mathrm{GELU}(\mathbf{X}_{\mathrm{norm}}^{(l)\prime}\mathbf{W}_{1}^{(l)}) \mathbf{W}_{2}^{(l)},\] (5)

where \(\mathbf{W}_{1}^{(l)}\in\mathbb{R}^{D^{(l)}\times 4\cdot D^{(l)}}\) and \(\mathbf{W}_{2}^{(l)}\in\mathbb{R}^{4\cdot D^{(l)}\times D^{(l)}}\) denote two linear projection matrices, and \(\mathbf{X}_{\mathrm{norm}}^{(l)\prime}=\mathrm{LN}(\mathbf{X}^{(l)\prime})\).

### Adapter Re-Composing method

We observe that the majority of existing adaptation methods introduce adapters into various layers and learn separate parameters for adapting the pre-trained model to specific downstream tasks. Previous studies [8; 24] have shown the effectiveness of leveraging the low-rank properties of adapters to fine-tune frozen pre-trained models. Inspired by these findings, we propose a novel approach to create a unified linear space across different adapters to enhance parameter efficiency and adaptation performance.

Architecture.The ARC architecture incorporates a bottleneck operation for adapters, which consists of three key components: a linear down-projection, layer-specific re-scaling coefficients, and a linear up-projection. This architecture is illustrated in Fig. 2. To facilitate the reusability of adaptation matrices, we have developed a sharing and re-composing scheme for the ARC method.

This scheme involves sharing linear projections across layers and learning low-dimensional re-scaling coefficients to re-compose the layer-adaptive adapters. In addition, to enhance parameter compression, we leverage symmetric down-projection and up-projection in a single bottleneck operation:

\[\mathbf{W}_{\mathrm{up}}=(\mathbf{W}_{\mathrm{down}})^{\mathrm{T}},\] (6)

where \(\mathbf{W}_{\mathrm{down}}\in\mathbb{R}^{D\times D^{\prime}}\) and \(\mathbf{W}_{\mathrm{up}}\in\mathbb{R}^{D^{\prime}\times D}\) with \(D^{\prime}\ll D\) denote shared down-projection and up-projection matrices across different layers; \(D^{\prime}\) represents the hidden dimensionality of the projections. To accommodate the variations across different layers, we learn re-scaling coefficients to re-compose the layer-adaptive adaptation matrices. These coefficients are then diagonalized into a diagonal matrix \(\mathbf{C}^{(l)}\in\mathbb{R}^{D^{\prime}\times D^{\prime}}\) specific to each layer \(l\). This diagonal matrix allows for efficient and effective adjustment of the adaptation parameters at each layer. Formally, given an input \(\mathbf{X}_{\mathrm{in}}\in\mathbb{R}^{(N+1)\times D}\), the output of our ARC module is:

\[\mathbf{X}_{\mathrm{out}}=\mathrm{ARC}(\mathbf{X}_{\mathrm{in}})=\mathbf{X}_{ \mathrm{in}}\mathbf{W}_{\mathrm{down}}\mathbf{C}^{(l)}\mathbf{W}_{\mathrm{up} }+\mathbf{X}_{\mathrm{in}}.\] (7)

Unless otherwise specified, we adhere to the default configuration of inserting our ARC modules sequentially before both the MHA and FFN blocks in the Vision Transformer. The influence of adapter positions will be discussed in Section 4.3. Therefore, the Vision Transformer incorporating our ARC modules can be formulated as follows:

\[\mathbf{X}^{(l)\prime}=\mathrm{MHA}(\mathrm{ARC}_{\mathrm{MHA}}( \mathrm{LN}(\mathbf{X}^{(l-1)})))+\mathbf{X}^{(l-1)},\] (8) \[\mathbf{X}^{(l)}=\mathrm{FFN}(\mathrm{ARC}_{\mathrm{FFN}}(\mathrm{ LN}(\mathbf{X}^{(l)\prime})))+\mathbf{X}^{(l)\prime}.\]

Note that \(\mathrm{ARC}_{\mathrm{MHA}}\) and \(\mathrm{ARC}_{\mathrm{FFN}}\) are two independent ARC modules, meaning that the projection matrices of the two modules are not shared between them. During the fine-tuning phase, we exclusively update the learnable parameters of our newly added ARC modules. This involves freezing all original parameters of the pre-trained model and solely focusing on updating the parameters of our ARC.

Inference.Our ARC employs a completely linear transformation so that we can re-parameterize it by fusing the module to the original framework of the pre-trained model. Take the ARC module of FNN as an example, the process can be defined by:

\[\mathbf{X}^{(l)}=\mathrm{GELU}(\mathrm{ARC}_{\mathrm{FFN}}(\mathbf{X}^{(l) \prime})\mathbf{W}_{1}^{(l)})\mathbf{W}_{2}^{(l)},\] (9)

where \(\mathrm{ARC}_{\mathrm{FFN}}(\mathbf{X}^{(l)\prime})\) can be represented by \(\mathbf{X}^{(l)\prime}(\mathbf{W}_{\mathrm{down}}\mathbf{C}^{(l)}\mathbf{W}_ {\mathrm{up}}+\mathbf{I})\) according to Eq. (7) with \(\mathbf{I}\in\mathbb{R}^{D\times D}\) being an identity matrix. Furthermore, we can construct \(\mathbf{W}_{1}^{(l)\prime}\) by:

\[\mathbf{W}_{1}^{(l)\prime}=(\mathbf{W}_{\mathrm{down}}\mathbf{C}^{(l)} \mathbf{W}_{\mathrm{up}}+\mathbf{I})\mathbf{W}_{1}^{(l)}.\] (10)

Figure 2: Illustration of the proposed Adapter Re-Composing Method.

Therefore, we can replace the matrix \(\mathbf{W}_{1}^{(l)}\) by \(\mathbf{W}_{1}^{(l)\prime}\) in the inference stage, thereby avoiding extra computational overheads.

### Insights of architecture design

In this work, we propose an approach that involves adopting a low-rank design for the adapters and sharing the bottleneck projections across layers. Our motivation for adopting this approach stems from the assumption that the layer-wise adapters can be effectively re-composed by re-scaling a small number of base linear projections. To validate this assumption, we conduct an analysis of the singular value distribution of adaptation matrices \(\mathbf{W}_{\mathrm{full}}\in\mathbb{R}^{D\times D}\) learned without the bottleneck operation, which are anticipated to be full-rank. In Fig. 3, we observe that the singular values exhibit extreme sparsity and follow a power-law distribution, with the majority of singular values being close to zero. This indicates that the learned adaptation matrices naturally exhibit low-rank characteristics. Furthermore, the pronounced sparsity of the singular values suggests that the adaptation matrices can be effectively reconstructed using a limited number of basis vectors. These findings provide strong evidence supporting the rationale behind our adaptation parameter-sharing strategy.

## 4 Experiments

In this section, we present the experimental settings, comparison to existing solutions, and ablation studies to unveil the key properties of the proposed method.

### Experimental settings

Datasets.We evaluate the effectiveness of our ARC approach on two sets of visual task adaptation benchmarks, comprising a total of 24 datasets. The list of datasets used for evaluation is provided below:

_FGVC._ We conduct experiments with the default settings in the VPT [6] on a collection of five Fine-Grained Visual Classification (FGVC) datasets, known as FGVC. The FGVC dataset collection includes CUB-200-2011 [31], NABirds [32], Oxford Flowers [33], Stanford Dogs [34], and Stanford Cars [35].

_VTAB-1k._ We also evaluate our ARC method on the VTAB-1k benchmark [36], which consists of 19 diverse visual classification tasks. These tasks are divided into three groups: the _Natural_ group, which contains images captured through standard cameras; the _Specialized_ group, which includes images captured by specialist equipment such as remote sensing and medical imaging; and the _Structured_ group, which comprises synthesized images from simulated environments, such as object counting and 3D depth prediction. Each downstream task in the VTAB-1k benchmark consists of 1000 training examples. Following VPT [6], we set aside 200 samples from the training set as the validation set to

Figure 3: Singular value distribution of adaptation matrices without the bottleneck structure. Two adaptation matrices of both MHA and FFN blocks are fine-tuned on the _DTD_ downstream task. The X-axis represents the singular values, while the Y-axis represents the count of singular values within specific ranges. Complete visualization is available in the appendix.

select hyperparameters. Subsequently, we train the model on the full training data using the selected hyperparameters.

Pre-trained backbone.To evaluate the adaptation capacity of the proposed ARC method, we apply the ARC strategy to two typical types of Vision Transformers: ViT [1] and Swin Transformers [11]. For ViT, we conduct experiments using three different backbone variants with varying model sizes: ViT-**Base**/**Large**/**Huge**. All the backbones are pre-trained on the ImageNet-21K [15] dataset.

Baselines and existing methods.In our comparative analysis, we evaluate the performance of the ARC method against two baselines and several state-of-the-art efficient pre-trained model adaptation methods. The two baselines we consider are: (1) Full Fine-tuning: This baseline involves updating all the parameters of the pre-trained model using the training data of the downstream task. (2) Linear Probing: This baseline focuses on learning a linear classification head on the downstream task while keeping the remaining parameters of the pre-trained model frozen. In addition to the baselines, we compare our ARC method with the following state-of-the-art solutions: (1) Adapter [7]: This method inserts lightweight adaptation operations, consisting of a down-projection, non-linear activation, and an up-projection, into the pre-trained model. (2) Bias [37]: The Bias method fine-tunes only the bias terms of the pre-trained models while keeping the remaining parameters frozen. (3) LoRA [24]: This approach introduces trainable low-rank adaptation matrices into each layer of the Transformer architecture. (4) VPT [6]: The VPT method incorporates extra learnable tokens into the input or all attention layers of the frozen Transformer. (5) SSF [9]: This method adds linear transformation parameters, including scaling and shifting, to modulate the pre-trained features. By comparing the performance of our ARC method with these baselines and state-of-the-art solutions, we aim to demonstrate its superiority in terms of efficiency and effectiveness in pre-trained model adaptation.

Implementation details.To ensure a fair evaluation of the effectiveness of our proposed ARC method, we have opted for a simple training setup without too many additional bells and whistles. Similar to VPT [6], we have used standard data augmentations during the training phase, which include image normalization using ImageNet means and standard deviation, random resize crop to \(224\times 224\) with random horizontal flip for FGVC datasets, and resize to \(224\times 224\) for VTAB-1k. We have used grid search to select hyper-parameters such as the learning rate, weight decay, and batch size, using the validation set of each task, as in VPT [6]. All experiments were conducted using the PyTorch [38] framework on an NVIDIA A40 GPU with 48GB of GPU memory. Further details can be found in the appendix.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline
**Method Dataset** & **CUB-200-2011** & **NAbirds** & **Oxford Flowers** & **S**a** & **S**a** & **Coarse** & **M** & **Params.(M)** \\ \hline \hline Full fine-tuning & 87.3 & 82.7 & 98.8 & 89.4 & 84.3 & 88.5 & 85.98 \\ Linear probing & 85.3 & 75.9 & 97.9 & 86.2 & 51.3 & 79.3 & 0.18 \\ \hline Adapter [7] & 87.1 & 84.3 & 98.5 & 89.8 & 68.6 & 85.7 & 0.41 \\ Bas [42] & 88.4 & 84.2 & 98.8 & 91.2 & 79.4 & 88.4 & 0.28 \\ VPT-Shallow [6] & 86.7 & 78.8 & 98.4 & 90.7 & 68.7 & 84.6 & 0.25 \\ VPT-Deep [6] & **88.5** & 84.2 & 99.0 & 90.2 & 83.6 & 89.1 & 0.58 \\ LaRo [24] & 88.3 & 85.6 & 99.2 & 91.0 & 83.2 & 89.5 & 0.44 \\ SSF [9] & 82.7 & 85.9 & 95.5 & 87.7 & 82.6 & 87.5 & 0.39 \\ SSF [9] & 89.5 & 88.7 & 99.6 & 89.6 & 89.2 & 90.7 & 0.39 \\ \hline ARC\({}_{s}\) & 83.4 & 85.0 & **99.4** & 90.1 & 82.7 & 89.1 & 0.22 \\ ARC & **88.5** & 85.3 & 99.3 & **91.9** & **85.7** & **90.1** & 0.25 \\ ARC* & 89.3 & 85.7 & 99.7 & 89.1 & 89.5 & 90.7 & 0.25 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of ARC with baselines and state-of-the-art efficient adaptation methods on five FGVC datasets. All methods utilize ViT-B/16 pre-trained on ImageNet-21k as the backbone. “SSF*” denotes the performance reported in the original SSF paper [9], which incorporates advanced data augmentations like cutmix [39], mixup [40], and regularization techniques such as label smoothing [41]. To ensure a fair comparison, we reproduced the SSF method using the code provided by [9], while employing the same basic data augmentations as our approach, and we denote SSF’s reported performance as “SSF*” and ARC’s performance augmented with SSF’s data augmentation as “ARC*”. The **bold** font shows the best accuracy of all methods and the underline font shows the second best accuracy.

### Experimental comparisons

In this section, we provide a comprehensive comparison of our ARC method with baseline methods and other state-of-the-art solutions. We evaluate the performance in terms of classification accuracy on downstream tasks as well as the parameter size. The results are summarized in Table 1 and Table 2, with all results obtained using the ViT-B/16 backbone. Based on these comparative results, we make the following observations:

(1) The ARC method demonstrates highly competitive classification accuracy on both sets of visual adaptation datasets, while maintaining a low parameter size. As shown in Table 1 and Table 2, under a fair comparison, ARC achieves the best mean accuracy and outperforms the other methods on the majority of the 24 datasets. This confirms the effectiveness of our proposed pre-trained model adaptation strategy. Furthermore, thanks to the parameter-sharing strategy in ARC, we are able to noticeably reduce the parameter size compared to other rank-decomposition based adapters such as Adapter [7] and LoRA [24]. VPT-Shallow [6] also exhibits parameter efficiency as it only introduces learnable tokens in the input layer. However, this is achieved at the cost of a significant performance sacrifice, resulting in considerably inferior performance compared to VPT-deep [6] and ARC. Another parameter-efficient method, Bias [42], focuses on updating only the bias terms in the pre-trained network, but it also leads to a significant compromise in classification performance on downstream tasks. To further decrease the parameter size, we evaluate ARC\({}_{\text{att}}\), which omits the adapters applied to Feed-Forward Network (FFN) blocks and focuses solely on adapting the Multi-Head Attention (MHA) blocks. This approach achieves nearly a halving of the parameter size while experiencing only a minimal \(1\%\) drop in performance.

(2) In comparison to the efficient adaptation solutions presented in the tables, full fine-tuning yields comparable or even worse classification accuracy across various downstream tasks, despite updating a significantly larger number of parameters. This observation further emphasizes the importance and potential of lightweight adaptation designs. On the other end of the spectrum, linear probing requires minimal parameters but exhibits noticeable performance degradation.

Experiments on larger-scale ViT backbones.In addition to evaluating the ARC method on ViT-B/16 backbone, we also conducted experiments on larger-scale ViT backbones to assess its performance on more computationally demanding models. Specifically, we tested the ARC method on ViT-Large and ViT-Huge backbones. The results, presented in Table 2(a) and Table 2(b), demonstrate that the ARC method maintains its competitive classification accuracy even with larger-scale backbones. It consistently outperforms the baseline methods and achieves comparable or superior performance compared to other state-of-the-art adaptation methods. Furthermore, the parameter size of the ARC method remains noticeably smaller than rank-decomposition based adapters like Adapter [7] and LoRA [24], as well as VPT-deep [6], showcasing its efficiency in terms of parameter utilization. These findings suggest that the ARC method is not only effective on smaller-scale ViT backbones

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{1}{c|}{**Natural**} & \multicolumn{1}{c|}{**S**oulded} & \multicolumn{1}{c|}{**S**oulded} & \multicolumn{1}{c}{**S**oulded} & \multicolumn{1}{c}{**S**oulded} \\ \hline \multicolumn{1}{c|}{**Methods**} & \multicolumn{1}{c|}{**Dataset**} & \multicolumn{1}{c|}{**VPT-B/16**} & \multicolumn{1}{c|}{but also scalable to larger models, making it a promising solution for efficient pre-trained model adaptation across a wide range of backbone sizes.

Experiments on hierarchical Vision Transformers.We extended the ARC method to Swin Transformer [11], a hierarchical Transformer architecture. To accommodate the varying feature dimensionalities in Swin Transformer, we introduced a stage-sharing strategy, enabling parameter sharing within each stage. The results on the VTAB-1k benchmark (Table 4) demonstrate the generalizability of ARC. It achieves competitive transfer learning accuracy and maintains favorable parameter scale, except for the Natural group where ARC performs relatively weaker. These findings highlight ARC's versatility and effectiveness in adapting different transformer architectures, showcasing its potential for practical applications in visual adaptation.

### Ablation studies

In order to gain deeper insights into the ARC method, we performed ablation studies to explore its additional properties. The experiments were conducted on the VTAB-1k benchmark using the pre-trained ViT-B/16 model. The results of these ablation studies are presented in Table 5.

Bottleneck dimensionality.In our ARC method, we adopt the low-rank design for the adapters but with the added feature of sharing the down-/up-projection matrices across layers and learning low-dimensional re-scaling coefficients to re-compose adaptation matrices. In this section, we investigate the impact of the bottleneck dimensionality on adaptation performance. The results are presented in Table 5. We find that a dimensionality of 50 achieves the best balance between transfer learning performance and parameter efficiency. Further reduction to a 10-dimensional space leads to fewer parameters but at the cost of performance degradation. Conversely, higher-dimensional hidden spaces result in inferior performance. These findings validate the effectiveness of our low-rank design, with 50 linear projections providing sufficient flexibility for composing layer-adaptive adapters.

Adapter positioning.By default, our adapters are positioned before the MHA and FFN modules, allowing the adaptation operations to be seamlessly integrated into the pre-trained network during inference without additional inference cost. In this section, we investigate the impact of adapter

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline  & **Natural (7)** & **Specialized (4)** & **Structed (8)** & **Mean Total** & **Params.** \\ \hline Full fine-tuning & 79.1 & 86.2 & 59.7 & 72.4 & 86.8 \\ Linear probing & 73.5 & 80.8 & 33.5 & 58.2 & 0.05 \\ \hline MLP-4 [6] & 70.6 & 80.7 & 31.2 & 57.7 & 4.04 \\ Partial [6] & 73.1 & 81.7 & 35.0 & 58.9 & 12.65 \\ Bias [42] & 74.2 & 80.1 & 42.4 & 62.1 & 0.25 \\ VPT-Shallow [6] & **79.9** & 82.5 & 37.8 & 62.9 & 0.05 \\ VPT-Deep [6] & 76.8 & 84.5 & 53.4 & 67.7 & 0.22 \\ \hline ARC & 79.0 & **86.6** & **59.9** & **72.6** & 0.27 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance comparison on VTAB-1k using Swin-Base pre-trained on ImageNet-21k as backbone. “(-)” denotes the number of tasks in the subgroup. Expanded results are presented in the appendix.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline  & **Natural (7)** & **Specialized (8)** & **Strong Total** & **Params.** \\ \hline Full fine-tuning & 74.7 & 83.8 & 48.1 & 65.4 & 303.4 \\ Linear probing & 70.9 & 69.1 & 25.8 & 31.5 & 0.05 \\ \hline Adaptive [7] & 65.6 & 75.5 & 29.0 & 52.9 & 2.38 \\ Bias [37] & 70.5 & 73.8 & 41.2 & 58.9 & 0.32 \\ VPT-Shallow [6] & 78.7 & 79.9 & 40.6 & 62.9 & 0.15 \\ VPT-Deep [6] & **82.5** & 83.9 & 54.1 & 70.8 & 0.49 \\ LLoRa [24] & 81.4 & 85.0 & **57.3** & 72.0 & 0.74 \\ ARC & 82.2 & **86.4** & **87.3** & **72.5** & 0.18 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance comparison on VTAB-1k using ViT-Large and ViT-Huge pre-trained on ImageNet-21k as backbone. “(-)” denotes the number of tasks in the subgroup. Expanded results are presented in the appendix.

position on pre-trained model adaptation performance using different positioning strategies. The results are presented in Table 4(b). Interestingly, placing the adapters after the MHA and/or FFN modules leads to performance degradation, despite this strategy being commonly adopted in previous works such as[7; 29]. Moreover, using only one type of adapter for either MHA or FFN results in inferior performance compared to using both types of adapters. This suggests that employing both types of adapters allows for more comprehensive adaptation of the pre-trained model to the target task without significantly increasing the parameter count.

Sharing v.s non-sharing adaptation parameters.In ARC, we adopt a parameter-sharing strategy to effectively reduce the number of adaptation parameters. This strategy encompasses two aspects: intra-layer sharing and inter-layer sharing. Through symmetric down-projection and up-projection matrices, we achieve intra-layer sharing, while inter-layer sharing involves sharing projection matrices across different layers. In this section, we investigate the impact of adaptation parameter sharing by conducting experiments with non-sharing or partial sharing designs. "intra + inter*" denotes sharing the bottleneck structure between MHA and FFN. The results presented in Table 4(c) demonstrate that using non-symmetric projection matrices or layer-independent adaptation parameters does not result in performance gains but leads to a noticeable increase in parameters. This validates the effectiveness of our parameter-sharing design.

Adapter insertion.We examine the performance of inserting the proposed adapters in a subset of layers using sequential or parallel insertion strategies. The results in Table 4(d) show that the performance of ARC improves as more layers are inserted. Furthermore, we observe that inserting adapters in the first six layers yields better results compared to inserting them in the last six layers. Additionally, we explore a parallel insertion setting inspired by [24], but the impact is not significantly pronounced. Another notable aspect is that our parameter sharing scheme in the ARC method prevents a linear increase in parameter size with the number of layers, ensuring better scalability, particularly for larger-scale models.

## 5 Limitation

The adaptation parameter sharing scheme in the ARC method is built on the assumption that layers have the same dimensionality. This assumption is crucial as it enables the sharing of down-/up-projection matrices involved in the bottleneck operation across layers, leading to parameter efficiency. However, it is worth exploring strategies to extend this scheme and accommodate dimensionality variation. This research direction holds promise for addressing scenarios where dimensionality varies and further enhancing the flexibility and applicability of the ARC method.

## 6 Conclusions

Our paper introduced the Adapter Re-Composing (ARC) method, which leverages the reusability of adaptation parameters to efficiently adapt pre-trained models. By sharing down-/up-projections in low-rank adapters across layers and learning layer-specific re-scaling coefficients to re-composing layer-adaptive adapters, ARC balances transfer learning performance and adaptation overheads. Extensive experiments demonstrate the compelling performance of our approach with a reduced parameter size. ARC offers a promising solution for efficient pre-trained model adaptation, showcasing the potential of reusing adaptation parameters for competitive results.

\begin{table}

\end{table}
Table 5: Ablation experiments on VTAB-1k benchmark using ViT-B/16 backbone. The table shows average accuracy (“Acc.”) and parameter count (“Params.”) for all downstream datasets.

## References

* [1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," in _International Conference on Learning Representations_, 2020.
* [2] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, "Masked autoencoders are scalable vision learners," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 16 000-16 009.
* [3] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark _et al._, "Learning transferable visual models from natural language supervision," in _International conference on machine learning_. PMLR, 2021, pp. 8748-8763.
* [4] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, "Scaling vision transformers," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 12 104-12 113.
* [5] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, "Conditional prompt learning for vision-language models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 16 816-16 825.
* [6] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, "Visual prompt tuning," in _Proceedings of the European conference on computer vision (ECCV)_. Springer, 2022, pp. 709-727.
* [7] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, "Parameter-efficient transfer learning for nlp," in _International Conference on Machine Learning_. PMLR, 2019, pp. 2790-2799.
* [8] S. Chen, C. Ge, Z. Tong, J. Wang, Y. Song, J. Wang, and P. Luo, "Adaptformer: Adapting vision transformers for scalable visual recognition," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [9] D. Lian, D. Zhou, J. Feng, and X. Wang, "Scaling & shifting your features: A new baseline for efficient model tuning," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [10] G. Luo, M. Huang, Y. Zhou, X. Sun, G. Jiang, Z. Wang, and R. Ji, "Towards efficient visual adaption via structural re-parameterization," _arXiv preprint arXiv:2302.08106_, 2023.
* [11] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 10 012-10 022.
* [12] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, "A comprehensive survey on transfer learning," _Proceedings of the IEEE_, vol. 109, no. 1, pp. 43-76, 2020.
* [13] S. J. Pan and Q. Yang, "A survey on transfer learning," _IEEE Transactions on knowledge and data engineering_, vol. 22, no. 10, pp. 1345-1359, 2010.
* [14] M. Iman, H. R. Arabnia, and K. Rasheed, "A review of deep transfer learning and recent advancements," _Technologies_, vol. 11, no. 2, p. 40, 2023.
* [15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. Ieee, 2009, pp. 248-255.
* [16] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor, "Imagenet-21k pretraining for the masses," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [17] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, "Revisiting unreasonable effectiveness of data in deep learning era," in _Proceedings of the IEEE international conference on computer vision_, 2017, pp. 843-852.

* [18] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. Van Der Maaten, "Exploring the limits of weakly supervised pretraining," in _Proceedings of the European conference on computer vision (ECCV)_, 2018, pp. 181-196.
* [19] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev _et al._, "The kinetics human action video dataset," _arXiv preprint arXiv:1705.06950_, 2017.
* [20] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, "Transformer in transformer," _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 34, pp. 15 908-15 919, 2021.
* [21] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and J. Feng, "Deepvit: Towards deeper vision transformer," _arXiv preprint arXiv:2103.11886_, 2021.
* [22] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, "Language models are few-shot learners," _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 33, pp. 1877-1901, 2020.
* [23] X. Chen, S. Xie, and K. He, "An empirical study of training self-supervised vision transformers," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021, pp. 9640-9649.
* [24] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," _arXiv preprint arXiv:2106.09685_, 2021.
* [25] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021, pp. 3045-3059.
* [26] X. L. Li and P. Liang, "Prefix-tuning: Optimizing continuous prompts for generation," _arXiv preprint arXiv:2101.00190_, 2021.
* [27] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," _ACM Computing Surveys_, vol. 55, no. 9, pp. 1-35, 2023.
* [28] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, "Autoprompt: Eliciting knowledge from language models with automatically generated prompts," _arXiv preprint arXiv:2010.15980_, 2020.
* [29] Y.-L. Sung, J. Cho, and M. Bansal, "Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 5227-5237.
* [30] Y. Zhang, K. Zhou, and Z. Liu, "Neural prompt search," _arXiv preprint arXiv:2206.04673_, 2022.
* [31] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, "The caltech-ucsd birds-200-2011 dataset," 2011.
* [32] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie, "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2015, pp. 595-604.
* [33] M.-E. Nilsback and A. Zisserman, "Automated flower classification over a large number of classes," in _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_. IEEE, 2008, pp. 722-729.
* [34] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li, "Novel dataset for fine-grained image categorization: Stanford dogs," in _Proc. CVPR workshop on fine-grained visual categorization (FGVC)_, vol. 2, no. 1. Citeseer, 2011.

* [35] T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, and L. Fei-Fei, "Fine-grained car detection for visual census estimation," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 31, no. 1, 2017.
* [36] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy _et al._, "A large-scale study of representation learning with the visual task adaptation benchmark," _arXiv preprint arXiv:1910.04867_, 2019.
* [37] E. B. Zaken, Y. Goldberg, and S. Ravfogel, "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, 2022, pp. 1-9.
* [38] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga _et al._, "Pytorch: An imperative style, high-performance deep learning library," _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 32, pp. 8026-8037, 2019.
* [39] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, "Cutmix: Regularization strategy to train strong classifiers with localizable features," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2019, pp. 6023-6032.
* [40] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, "mixup: Beyond empirical risk minimization," _arXiv preprint arXiv:1710.09412_, 2017.
* [41] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, "Rethinking the inception architecture for computer vision," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2016, pp. 2818-2826.
* [42] H. Cai, C. Gan, L. Zhu, and S. Han, "Tinytl: Reduce memory, not parameters for efficient on-device learning," _Advances in Neural Information Processing Systems (NeurIPS)_, vol. 33, pp. 11 285-11 297, 2020.

**Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing**

**Supplementary Materials**

In the supplementary materials involving our work, we demonstrate detailed dataset settings, supplemental insights and analysis, extra experimental details, supplemental experiments, and broader impacts, including:

* A **Detailed descriptions for datasets and implementation**
* B **Insights of architecture design**
* C **Parameter size analysis**
* D **Experimental details on larger-scale and hierarchical ViT backbones**
* E **Experimental details on ablation studies**
* F **Expanded experiments with self-supervised pre-training**
* G **Broader impacts**

Due to the limitation that the file "Supplementary Materials.zip" larger than 100MB cannot be uploaded on OpenReview, the supplementary materials only upload the code for the project. Please refer to the anonymous link https://drive.google.com/file/d/12blHbYF1Jr0u0GeTLI4uII6GHt3CV3I2/view to obtain the complete code, datasets, and models.

Detailed descriptions for datasets and implementation

We describe the details of visual adaptation classification tasks in Table 6 (FGVC) and 7 (VTAB-1k), including the class number and the train/val/test sets.

Table 8 summarizes the detailed configurations we used for experiments. As mentioned in Section 4.1, we utilize grid search to select hyper-parameters such as learning rate, weight decay, batch size, and adapter dropout, using the validation set of each task. Note that we also apply dropout to the middle features produced by our ARC method, which we term as "adapter dropout". Specifically, during the ARC process, we randomly drop partial features before up-projection.

## Appendix B Insights of architecture design

Similar to Fig. 3, we present more visualization results of singular value distribution of adaptation matrices \(\mathbf{W}_{\text{full}}\in\mathbb{R}^{D\times D}\) learned without the bottleneck operation. As shown in Fig. 4, the singular value distribution of adaptation matrices learned on _DTD_ downstream task exhibits a power-law

\begin{table}
\begin{tabular}{l|l|r|r|r|r} \hline \hline
**Dataset** & **Description** & **Classes** & **Train size** & **Val size** & **Test size** \\ \hline CUB-200-2011 [31] & Fine-grained bird species recognition & 200 & 5,394* & 600* & 5,794 \\ NABirds [32] & Fine-grained bird species recognition & 555 & 21,536* & 2,393* & 24,633 \\ Oxford Flowers [33] & Fine-grained flower species recognition & 102 & 1,020 & 1,020 & 6,149 \\ Stanford Dogs [34] & Fine-grained dog species recognition & 120 & 10,800* & 1,200* & 8,580 \\ Stanford Cars [35] & Fine-grained car classificatio & 196 & 7,329* & 815* & 8,041 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Dataset statistics for FGVC. “*” denotes the train/val split of datasets following the dataset setting of VPT models [6].

\begin{table}
\begin{tabular}{l|r|r|r|r} \hline \hline
**Dataset** & **Description** & **Classes** & **Train size** & **Val size** & **Test size** \\ \hline CIFAR-100 & & 100 & & 10,000 \\ Caltech101 & & 102 & & 6,084 \\ DTD & & 47 & & 1,880 \\ Flowers102 & Natural & 102 & 800/1,000 & 200 & 6,149 \\ Pets & & 37 & & 3,669 \\ SVHN & & 10 & & 26,032 \\ Sun397 & & 397 & & 21,750 \\ \hline Patch Camelyon & & 2 & & 32,768 \\ EuroSAT & & 10 & 800/1,000 & 200 & 5,400 \\ Resisc45 & Specialized & 45 & & 6,300 \\ Retinopathy & & 5 & & 42,670 \\ \hline Clevr/count & & 8 & & 15,000 \\ Clevr/distance & & 6 & & 15,000 \\ DMLab & & 6 & & 22,735 \\ KITTI/distance & & 4 & & 711 \\ dSprites/location & Structured & 16 & 800/1,000 & 200 & 73,728 \\ dSprites/orientation & & 16 & & 73,728 \\ SmallNORB/azimuth & & 18 & & 12,150 \\ SmallNORB/elevation & & 9 & & 12,150 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Dataset statistics for VTAB-1k [36].

distribution across various layers in the downstream tasks. This finding provides further support for our research motivation.

## Appendix C Parameter size analysis

To showcase the parameter-efficiency of our ARC method, we compare its parameter size with other popular lightweight adaptation methods (Table 9), including Adapter [7], VPT [6], LoRA [24], and SSF [9]. Adapter [7] adds two linear projections to each encoder layer during fine-tuning, resulting in the introduction of \(2\cdot D\cdot D^{\prime}\cdot L\) learnable parameters, where \(D^{\prime}\) denotes the hidden dimensionality of the linear projections. Furthermore, due to the presence of non-linear activations in Adapter, the additional parameters contribute to supernumerary overhead during the inference phase. VPT [6] incorporates \(m\) prompts into input space, leading to an increase of \(m\cdot D\) parameters for VPT-Shallow and \(m\cdot D\cdot L\) parameters for VPT-Deep. In contrast to Adapter, both LoRA [24] and SSF [9] employ linear adaptation methods without incorporating non-linear functions. This design choice allows them to leverage re-parameterization benefits, thereby mitigating additional computations during inference. Specifically, the adaptation matrix of LoRA, which consists of a down-projection and an up-projection, introduces \(2\cdot w\cdot D\cdot D^{\prime}\cdot L\) learnable parameters, where \(w\) denotes the number of attention matrices undergoing adaptation. SSF inserts linear scaling and shifting coefficients after

\begin{table}
\begin{tabular}{c|c} \hline \hline Optimizer & AdamW \\ Learning Rate & \{0.2, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0001\} \\ Weight Decay & \{0.05, 0.01, 0.005, 0.001, 0\} \\ Batch Size & \{256, 128, 32\} \\ Adapter Dropout & \{0.8, 0.5, 0.1, 0\} \\ Learning Rate Schedule & Cosine Decay \\ Training Epochs & 100 \\ Warmup Epochs & 10 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The implementation details of configurations such as optimizer and hyper-parameters. We select the best hyper-parameters for each download task via using grid search.

Figure 4: Singular value distribution of adaptation matrices without the bottleneck structure. Two adaptation matrices of both MHA and FFN blocks are fine-tuned on the _DTD_ downstream task. The X-axis represents the singular values, while the Y-axis represents the count of singular values within specific ranges.

\(o\) operations, resulting in an addition of \(2\cdot o\cdot D\cdot L\) extra parameters. The proposed ARC method offers additional parameter compression by sharing symmetric projection matrices across different layers. This approach introduces only \(D\cdot D^{\prime}\) parameters. Additionally, we learn low-dimensional re-scaling coefficients and bias terms for each layer, resulting in a total of \((D^{\prime}+D)\cdot L\) additional parameters. Overall, the number of parameters in our default ARC is \(2\cdot((D\cdot D^{\prime})+(D^{\prime}+D)\cdot L)\).

We also compare the parameter size with lightweight adaptation methods on backbones of different scales, as shown in Fig. 5. Our ARCs demonstrate parameter efficiency across various model sizes, comparable to VPT-Shallow [6]. However, the unique advantage of our approach lies in its ability to effectively balance lower overheads and maintain competitive performance. Furthermore, the parameter count of our ARC remains stable even as the model scale increases, showcasing the scalability of our method with minimal additional resource consumption.

Thanks to our adaptation parameter sharing strategy, the ARC method avoids a linear increase in the number of learnable parameters as the number of layers grows. We employ ViT-B as the backbone and integrate adapters into different layers. As shown in Fig.6, in contrast to other adaptation methods, both our ARCs and VPT-Shallow[6] effectively manage parameter growth as the number of inserted layers increases, but only our methods achieve promising performance without significant cost escalation. This highlights the scalability and effectiveness advantages of our ARCs.

Figure 5: The parameter size comparison of lightweight adaptation methods on ViT Backbones of Different Scales. The X-axis represents different adaptation methods, while the Y-axis represents the parameter size in Million (M).

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline
**StageMethod** & **Adapter [7]** & **VPT-Shallow [6]** & **VPT-Deep [6]** & **LoRA [24]** & **SSF [9]** & **ARC** \\ \hline Fine-Tuning & \(2\cdot D\cdot D^{\prime}\cdot L\) & \(m\cdot D\) & \(m\cdot D\cdot L\) & \(2\cdot w\cdot D\cdot D^{\prime}\cdot L\) & \(2\cdot o\cdot D\cdot L\) & \(2\cdot(D\cdot D^{\prime}+(D^{\prime}+D)\cdot L)\) \\ Inference & \(2\cdot D\cdot D^{\prime}\cdot L\) & \(m\cdot D\) & \(m\cdot D\cdot L\) & 0 & 0 & 0 \\ \hline \end{tabular}
\end{table}
Table 9: Comparison of the additional parameter size in both fine-tuning and inference stages with other lightweight adaptation methods.

Figure 6: The parameter size comparison with lightweight adaptation methods with a different number of inserted layers. The X-axis represents different adaptation methods, while the Y-axis represents the parameter size in Million (M).

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

## Appendix G Broader impacts

Efficient usability.Unlike previous approaches, our method incorporates a parameter sharing scheme across different layers of the model, resulting in a significant reduction in the number of parameters that need to be fine-tuned. This approach allows us to maintain competitive performance while achieving parameter efficiency. By maximizing the utilization of large-scale pre-trained models, our ARC methods offer enhanced usability and practicality in various applications.

Environmental-friendly consumption.In addition to the reduction in computational overheads, another significant benefit of our method is the positive impact on carbon emissions reduction and environmental protection. By optimizing the computational efficiency of the model, we minimize the energy consumption required during the training and deployment of the model. This reduction in energy consumption leads to a decrease in carbon emissions, contributing to environmental sustainability. Our method not only delivers improved performance and efficiency but also aligns with the larger goal of mitigating the environmental impact of AI technologies.

Ethical Considerations.Our model focuses on utilizing the representation and generalization capacity obtained from large-scale pre-trained datasets and models. However, it is crucial to acknowledge that if the pre-training datasets contain bias or illegal information, there is a risk of inheriting such issues into our model.

In order to address this concern, it becomes imperative to explore research directions that aim to identify and prevent privacy leakage and correct model bias. This involves developing robust mechanisms to detect and mitigate bias in training data, as well as implementing privacy-preserving techniques to safeguard sensitive information.

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{8}{c}{**Natural**} & \multicolumn{8}{c}{**Spotational**} & \multicolumn{8}{c}{**Structured**} \\ \hline \hline