# VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens

 Zhanpeng Zeng

University of Wisconsin, Madison

zzeng38@wisc.edu

&Cole Hawkins

AWS AI

colehawk@amazon.com

Mingyi Hong

University of Minnesota, Minneapolis & AWS AI

mhong@umn.edu

&Aston Zhang

AWS AI

astonz@amazon.com

Nikolaos Pappas

AWS AI

npappa@amazon.com

&Vikas Singh

University of Wisconsin, Madison

vsingh@biostat.wisc.edu

&Shuai Zheng

AWS AI

shzheng@amazon.com

###### Abstract

Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models with respect to sequence length, dealing with ultra long sequences (e.g., \(>\)16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens, which we call VIP-tokens, are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only efficient (achieving more than \(3\times\) compute efficiency gain compared to baselines on 4K and 16K lengths), but also offers competitive/better performance on a large number of tasks. Further, we show that our algorithm scales to 128K tokens (or more) while consistently offering accuracy improvement. Code is available at https://github.com/mlpen/VCC.

## 1 Introduction

The Transformer [32] is a fundamental/foundational architecture for natural language processing (NLP) and computer vision. It has shown remarkable performance across NLP applications including machine translation [32], language inference [10], and summarization [14]. Transformers have also been successfully applied to various visual recognition tasks and achieve impressive results [11, 4, 41]. Unfortunately, the runtime/memory needs of Transformers involve an unfavorable dependence on the input sequence length, making the use of Transformers for ultra-long sequence applications difficult. Therefore, many studies on Transformers make use of strategies such as truncation to ensure that the input sentence length is at most \(512\), e.g., BERT, T5, and other Transformer-based language models [36, 22, 27]. Unfortunately, such a truncation, and other related strategies, inevitably results in loss of accuracy, the extent of which can vary from one task/dataset to another. Consequently, improving the efficiency for longer input sequence length is a key focus of many proposals. These developments areimportant milestones, and they have reduced the quadratic dependency on sequence lengths to linear [6; 25; 35; 2; 38; 40; 39]. Currently, many Transformer models can process samples with sequence lengths of up to 4K (and even 16K). Very recently, results of newer models being able to handle much longer sequences have appeared [3].

**Rationale.** It is natural to ask whether the ability to process longer sequences is worth the trouble. The short answer is yes. Improved accuracy has been reported on long sequence tasks [2; 38; 14]. So, what is stopping us from harvesting even stronger gains in accuracy by feeding even longer sequences to such models? Models such as Longformer [2] and Big Bird [38] become slow and consume an excessive amount of memory as the sequence length keeps increasing. See Fig. 1 for illustration. Why? The representation update of every token involves computing efficient attention and feed-forward network at each layer. This incurs a linear cost relative to the sequence length and is expensive for sequences much longer than 4K (or 16K) tokens. To endow the models the ability to learn ultra-long range dependency, we need to lower this cost. What we describe in this paper is a concrete step forward - based on certain task-specific assumptions which appear to generally hold, we outline a formulation that works and delivers the expected improvements.

**(1) Focus on what we need for a task: VIP-token centric compression (VCC).** We hypothesize/find that in many tasks where Transformers are effective, only a small subset of tokens, which we refer to as VIP-tokens, are relevant to the final output (and accuracy) of a Transformer. If these tokens had been identified somehow, we could preserve this information in its entirety and only incur a moderate loss in performance. Now, _conditioned_ on _these_ specific VIP-tokens, an aggressive compression on the other _non-VIP-tokens_, can serve to reduce (and often, fully recover) the loss in performance while dramatically decreasing the sequence length. This compression must leverage information regarding the VIP-tokens, with the goal of improving the approximation of the representation of the VIP-tokens. In other words, a high-fidelity approximation of the entire sequence is unnecessary. Once this "selectively compressed" input passes through a Transformer layer, the output sequence is decompressed to the original full sequence allowing the subsequent layers to access the full sequence.

**(2) Specialized data structure for compression/decompression.** A secondary, but important practical issue, is reducing the overhead when compressing/decompressing the input/output sequences internally in the network. Ignoring this problem will impact efficiency. We give a simple but specialized data structure to maintain the hidden states of the intermediate layers, where the compression can be easily accessed from the data structure, and explicit decompression can be avoid by updating the data structure: the sequence is never fully materialized in intermediate layers.

**Practical contributions.** Apart from the algorithmic modules above, we show that despite an aggressive compression of the input sequences, we achieve better/competitive performance on a broad basket of long sequence experiments. Compared to baselines, we get much better runtime/memory efficiency. We show that it is now practical to run **standard** Transformer models on sequences of \(128\)K token lengths, with consistent performance benefits (and **no** complicated architecture changes).

## 2 Preliminaries

We review the Transformer layer, related work on efficient Transformers and define notations/simplifications. **BOLD** uppercase letters denote matrices, **bold** lower case letters denote vectors, and regular lower case letters denote scalars or functions.

**Brief review of the Transformer Model.** Fix \(n\) to be the sequence length and let \(d\) be the embedding dimension. Define an embedding matrix \(\mathbf{X}\in\mathbb{R}^{n\times d}\) which gives the \(n\) feature vector inputs for a Transformer layer. The output of this Transformer layer, \(\mathbf{X}_{new}\), is defined as

\[\mathbf{X}_{new}=\beta(\alpha(\mathbf{X})+\mathbf{X})+\alpha(\mathbf{X})+ \mathbf{X}\] (1)

using \(\alpha(\mathbf{X})\) as shorthand for \(\alpha(\cdot,\cdot,\cdot)\), which is a multi-head attention (MHA) with \(\mathbf{X}\) as input for queries, keys, and values, described shortly. Here, \(\beta(\cdot)\) is a feed-forward network (FFN). Layer

Figure 1: Model efficiency of processing one sequence on a NVIDIA A100 as sequence length increases (note logarithm \(x\) axis).

norms [1] are omitted to reduce clutter. Let the inputs to \(\alpha(\cdot,\cdot,\cdot)\) be \(\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{n\times d}\) for queries, keys, and values. MHA is defined as:

\[\alpha(\mathbf{Q},\mathbf{K},\mathbf{V}):=\text{cat}_{i=1}^{i=g}\left[\text{ softmax}(\mathbf{Q}\mathbf{W}_{Q,i}\mathbf{W}_{K,i}^{\top}\mathbf{K}^{\top}) \mathbf{V}\mathbf{W}_{V,i}\right]\mathbf{W}\] (2)

where \(g\) is the number of attention heads, \(\{\mathbf{W}_{Q,i},\mathbf{W}_{K,i},\mathbf{W}_{V,i}\}\) are trainable projections, and the 'cat' concatenates the outputs of multiple self-attention modules. We omit the biases for notational simplicity. For ease of discussion, let us further simplify the above notation by assuming that \(g=1\), and suppress \(\mathbf{W}_{Q,1},\mathbf{W}_{K,1},\mathbf{W}_{V,1},\mathbf{W}\) as well as the normalization in softmax: they will _still_ be estimated within the model (i.e., this module remains unchanged) but are tangential to the description of our idea. With these simplifications, the \(\alpha(\cdot,\cdot,\cdot)\) can be expressed as:

\[\alpha(\mathbf{Q},\mathbf{K},\mathbf{V}):=\exp(\mathbf{Q}\mathbf{K}^{\top}) \mathbf{V}.\] (3)

Let \(\gamma(\cdot)\) be a placeholder for all heavy computations in the Transformer layer above:

\[\gamma(\mathbf{X}):=\beta(\alpha(\mathbf{X})+\mathbf{X})+\alpha(\mathbf{X}).\] (4)

We can verify that the output of a Transformer block (parameters are suppressed to reduce clutter) is,

\[\mathbf{X}_{new}=\gamma(\mathbf{X})+\mathbf{X}.\] (5)

A Transformer model consists of many such layers: the input of each layer is the output \(\mathbf{X}_{new}\) from the previous layer. Let \(l\) be the number of layers, then the overall complexity is \(\mathcal{O}(ln^{2}d+lnd^{2})\).

**Efficient Transformers.** Many efficient self-attention methods are available to reduce the \(\mathcal{O}(ln^{2}d)\) cost. We list a few models noting that this list is not exhaustive. Performer [6], Random Feature Attention [25], and Nystromformer [35] propose different low rank approximations of the self-attention matrices. Longformer [2] and Big Bird [38] describe global + local sparse attention. Redformer [17] and YOSO [40] exploit locality sensitive hashing for approximating the self-attention matrix. MRA attention [39] gives a multi-resolution approximation of the self-attention matrices. Memorizing Transformers [34] and RMT [3] follow a recurrent design and store the past context in an external memory module. By sequentially processing one segment of input sequences at one time, they avoid blowing up the memory when processing long sequences.

**Efficient Transformers do not scale well to ultra-long sequences**. Existing self-attention mechanisms often reduce the quadratic cost of MHA to linear. But so far, most experiments report sequence lengths of up to 4K, with some exceptions [2, 38, 14]. Beyond 4K, the linear cost (on \(n\)) for both computing efficient attentions and FFN makes the cost prohibitive, especially for large models. For example, although LongT5 [14] can train on sequence lengths of up to 16K tokens with an efficient self-attention and shows promising results for longer sequences, it is slower and needs a sizable amount of compute (for example, see Fig. 1). Similar to efficient self-attention with linear cost attention, [34] and [3] do not try to reduce the linear cost (on sequence length) for FFN, so the computation might still be expensive for processing ultra long sequences. On the other hand, our method seeks to reduce the overall cost (both self-attention and FFN) of processing ultra long sequences and processes the entire sequence simultaneously.

**Other alternatives for sequence compression?** Compressing input sequences for efficiency reasons in Transformers is not a new idea. For example, [8] and [16] propose pyramid Transformer variants that progressively compress the sequence as the layers grow deeper via pooling or core-set selection. [23] proposes adaptively compressing the sequence based on the predicted semantic boundaries within the sequence. [26] proposes compressing the fine-grained past activations to coarser memories. There are **three** key differences with our approach. First, all methods listed above are _task agnostic_. They seek compressed/smaller representations to represent the _original_ sequence well. Our formulation places no emphasis on representing the original sequence, as long as information pertinent to the VIP-tokens is preserved as much as possible. Second, once these methods compress the sequence, the residual information is lost (for the deeper layers or the later time steps). Our entire approach is predicated on avoiding this loss - we maintain access to the full sequence at each layer (via residual connection at least). Lastly, some of these ideas often involve an \(n^{2}\) dependence on the sequence length in the initial stages of their formulation, making long sequence experiments problematic.

## 3 VIP-Token Centric Compression (VCC)

Our main goal is to reduce the dependency on \(n\) (but _not_ by modifying Transformer internals). To do this, we describe a scheme that compresses the input sequence of a Transformer layer and decompresses the output sequence, resulting in a model whose complexity is \(\mathcal{O}(lrd^{2}+lr^{2}d+lr\log(n_{c})d+lrn_{p}d+nd)\). Here, \(r\) is the length of the compressed sequence, \(n_{p}\) is the number of VIP-tokens described shortly, and \(n_{c}\) is the size of non-VIP/remaining tokens. So, we have \(n_{p}+n_{c}=n\) and assume \(n_{p}\ll r\ll n\). (complexity analysis is provided in the Appendix.)

**Parsing the complexity term:** Let us unpack the term to assess its relevance. The first two terms \(\mathcal{O}(lrd^{2}+lr^{2}d)\) pertain to the cost for a Transformer, while the remaining terms are the overhead of compression and decompression. The term \(\mathcal{O}(lr\log(n_{c})d+lrn_{p}d)\) is the overhead of compression and updating our data structure at each layer. The \(\mathcal{O}(nd)\) term corresponds to pre-processing involving converting the hidden states into our data structure and post-processing to recover the hidden states from the data structure. Note that unlike the dependence on \(n\) for vanilla Transformers, this \(\mathcal{O}(nd)\) is incurred only at the input/output stage of the Transformer, but **not** at any intermediate layers.

**High level design choices.** We use the _standard_ Transformer layers with a _standard_ feed-forward network (which results in \(d^{2}\) in the first term) and _standard_ quadratic cost self-attention (which gives the \(r^{2}\) factor in the second term). Why? These choices help isolate the effect of incorporating their efficient counterparts. The proposed algorithm operates on the _input/output of each Transformer layer_ leaving the Transformer module itself unchanged. Therefore, our goals are distinct from the literature investigating efficient self-attentions and efficient feed-forward networks. This is because one can replace these two vanilla modules with _any other_ efficient alternatives to further reduce the \(r^{2}\) and \(d^{2}\) terms directly. Despite these quadratic terms, our approach is faster than baselines (SS4).

We will first describe our general idea, as shown in Fig. 2, which uses VIP-tokens to guide the compression/decompression of the input/output of a Transformer layer so that it only needs to process the compressed sequence (SS3.1, SS3.2). Then, we will discuss an instantiation of the compression process, by adapting a multi-resolution analysis technique (SS3.3). Lastly, we will introduce a data structure which allows more efficient compression/decompression (SS3.4).

### Elevating the Importance of a Few Tokens: VIP-Tokens

Let us start with the simplest compression, which identifies a linear transformation \(\mathbf{S}\in\mathbb{R}^{r\times n}\) which acts on the input, resulting in a smaller representation \(\mathbf{S}\mathbf{X}\in\mathbb{R}^{r\times d}\). Of course, a smaller \(r\) implies that more information about \(\mathbf{X}\) is lost. But we find that in many tasks, only the embedding representations of _a few_ tokens drive the final prediction: we refer to these tokens as _VIP-tokens_.

**Examples of VIP-tokens:** Observe that only the embedding outputs of masked tokens in masked language modeling [10] and the CLS token in sequence classification [10, 11] are/is used for prediction. In question answering, only the questions and possible answers associated with the questions are used for prediction. It is important to note that the masked tokens, CLS tokens, and question tokens are **(1)** defined by the tasks and **(2)**_known_ to the model (although the embedding representation of these tokens are unknown). These VIP-tokens can be viewed as a task or question that is given to the model. The model can process the sequence with a specific goal in mind so that the model can skip/skim less relevant segments. Our general principle involves choosing _a set of tokens_ as the VIP-tokens that **(1)** are important to the specific task goals and **(2)** easily pre-identifiable by the user.

_Caveats._ Not all important tokens can be pre-identified. For example, the tokens in the correct answer span in answer span prediction are also important to the specific goals, but are difficult to pre-identify, so only the question tokens (and not the answer tokens) are used as VIP-tokens. We assume that any other tokens that are relevant for prediction should have high dependency with these VIP-tokens. For example, the answer tokens should have high dependency (in self-attention) to the question tokens.

**VIP-tokens occupy the front seats.** VIP-tokens can occur anywhere within a sequence. But we can re-order the sequence as well as the positional encodings so that VIP-tokens are always at the _head of sequence_ to make analysis/implementation easier. With this layout, let \(\mathds{P}\in\mathbb{R}^{n_{p}\times d}\) be the VIP-tokens and \(\mathbf{C}\in\mathbb{R}^{n_{e}\times d}\) be the non-VIP/remaining tokens, \(\mathbf{X}\) can be expressed as

\[\mathbf{X}=\begin{bmatrix}\mathds{P}\\ \mathbf{C}\end{bmatrix}\] (6)

This is possible since Transformer is permutation invariant when permuting positional encodings (embeddings or IDs) along with tokens. This re-ordering is performed only once for the input of the Transformer model, then the outputs generated by the model are rearranged to their original positions.

Re-ordering makes the analysis, implementation and presentation of our method much clearer and simpler. In fact, placing VIP tokens at the end of the sequence can also serve the same purpose.

From the above discussion, it is clear that one needs to make sure that after compressing the input tokens \(\mathbf{X}\), the VIP-tokens must still stay (more or less) the same, and the compression matrix \(\mathbf{S}\) must be _VIP-token dependent_. We hypothesize that such _VIP-token dependent_ compression matrices require a much smaller dimension \(r\), compared to _VIP-token agnostic_ compression matrices.

### VIP-Token Centric Compression (VCC): An Initial Proposal

For a Transformer layer, let \(\mathbf{X}\) denote its input matrix. Express the output of this layer as follows:

\[\mathbf{X}_{new}=\mathbf{S}^{\dagger}\gamma(\mathbf{S}\mathbf{X})+\mathbf{X}\] (7)

where \(\mathbf{S}\in\mathbb{R}^{r\times n}\) is a _compression_ matrix compressing \(\mathbf{X}\) to a smaller representation and \(\mathbf{S}^{\dagger}\) is the pseudo inverse for _decompression_. With the layout in (6), we can write (7) as

\[\begin{bmatrix}\mathbb{P}_{new}\\ \mathbb{C}_{new}\end{bmatrix}=\mathbf{S}^{\dagger}\gamma\left(\mathbf{S} \begin{bmatrix}\mathbb{P}\\ \mathbb{C}\end{bmatrix}\right)+\begin{bmatrix}\mathbb{P}\\ \mathbb{C}\end{bmatrix}\] (8)

where \(\mathbb{P}_{new}\) and \(\mathbb{C}_{new}\) are the new embeddings for \(\mathbb{P}\) and \(\mathbb{C}\).

**Always reserve seats for VIP-tokens.** What is a useful structure of \(\mathbf{S}\)? Since \(\mathbb{P}_{new}\) is the embedding output for the VIP-tokens \(\mathbb{P}\), we want them to be fully preserved. To achieve this, we impose the following structure on \(\mathbf{S}\) and \(\mathbf{S}^{\dagger}\):

\[\mathbf{S}=\begin{bmatrix}\mathbf{I}_{n_{p}}&0\\ 0&\mathbf{S}_{c}\end{bmatrix}\qquad\mathbf{S}^{\dagger}=\begin{bmatrix} \mathbf{I}_{n_{p}}&0\\ 0&\mathbf{S}_{c}^{\dagger}\end{bmatrix}.\] (9)

The rearrangement simply says that we will avoid compressing \(\mathbb{P}\). But rewriting it in this way helps us easily unpack (8) to check the desired functionality of \(\mathbf{S}_{c}\).

**Prioritize information in VIP-tokens.** Our goal is to ensure \(\mathbb{P}_{new}\) generated from the compressed sequence in (8) will be similar to its counterpart from the uncompressed sequence. Let us check (8) using the compression matrix \(\mathbf{S}\) defined in (9) first. We see that

\[\begin{bmatrix}\mathbf{I}_{n_{p}\times n_{p}}&0\\ 0&\mathbf{S}_{c}^{\dagger}\end{bmatrix}\gamma\left(\begin{bmatrix}\mathbb{P} \\ \mathbf{S}_{c}\mathbb{C}\end{bmatrix}\right)=\begin{bmatrix}\beta(\alpha( \mathbb{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbb{P})+\alpha( \mathbb{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})\\ \mathbf{S}_{c}^{\dagger}\beta(\alpha(\mathbf{S}_{c}\mathbb{C},\mathbf{S} \mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbb{C})+\mathbf{S}_{c}^{ \dagger}\alpha(\mathbf{S}_{c}\mathbb{C},\mathbf{S}\mathbf{X},\mathbf{S} \mathbf{X})\end{bmatrix}.\] (10)

The \(\mathsf{orange}\) color identifies terms where \(\mathbb{P}_{new}\) interacts with other compression-related terms \(\mathbf{C}\) and/or \(\mathbf{S}_{c}\). We primarily care about \(\mathbb{P}_{new}\) in (8), so the first (\(\mathsf{orange}\)) row in (10) is the main concern. We see that \(\mathbb{P}_{new}\) only depends on the compressed \(\mathbf{S}\mathbf{X}\) via \(\alpha(\mathbb{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})\). We can further unpack,

\[\alpha(\mathbb{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})=\exp(\mathbb{P} \mathbf{X}^{\top}\mathbf{S}^{\top})\mathbf{S}\mathbf{X}=\exp(\mathbb{P} \mathbf{P}^{\top})\mathbb{P}+\exp(\mathbb{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{ \top})\mathbf{S}_{c}\mathbb{C}.\] (11)

Again, \(\alpha(\mathbb{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})\) depends on \(\mathbf{C}\) and \(\mathbf{S}_{c}\) via the second (\(\mathsf{orange}\)) term. Normalization in softmax is omitted for simplicity of discussion. This helps us focus on the key term that matters: \(\exp(\mathbb{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{\top})\mathbf{S}_{c}\mathbb{ C}\). As long as the following approximation using \(\mathbf{S}_{c}\) is good

\[\exp(\mathbb{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{\top})\mathbf{S}_{c}\approx \exp(\mathbb{P}\mathbf{C}^{\top}),\] (12)

we will obtain a good approximation of \(\mathbb{P}_{new}\). Our remaining task is to outline a scheme of finding a compression matrix \(\mathbf{S}_{c}\) such that this criterion can be assured.

### A Specific Instantiation via Multi-Resolution Compression

What should be the mechanics of our compression such that (12) holds? In general, to get \(\mathbf{S}_{c}\), we can use any sensible data driven sketching idea which minimizes the error of (12). Doing so efficiently needs a bit of work; we describe the high level idea below and the low-level details are provided in Appendix.

Figure 2: Diagram that illustrates a Transformer layer with VIP-token centric sequence compression.

**High level idea.** Ideally, an efficient scheme for constructing \(\mathbf{S}_{c}\) should operate as follows. If some regions of the sequence \(\mathbf{C}\) have a negligible impact on (12) (via the orange terms above), the procedure should compress the regions aggressively. If other regions are identified to have a higher impact on (12) (again due to the orange terms above), the procedure should scan these regions more carefully for a more delicate compression. This suggests that procedurally a coarse-to-fine strategy may work. For example, multi-resolution analysis does help in approximating self-attention matrices in Transformers [39], but the formulation in [39] cannot be easily written in a form similar to (12), making it incompatible with our design. Nonetheless, we derive an analogous form (details in Appendix) that can be represented in a similar form as (12) and gives a strategy for obtaining \(\mathbf{S}_{c}\mathbf{C}\).

Specifically, let us define a compressed representation (via averaging) of the \(x\)-th \(s\)-length segment of sequence \(\mathbf{C}\): \(\mathbf{c}_{x}^{s}\in\mathbb{R}^{d}\)

\[\mathbf{c}_{x}^{s}:=\frac{1}{s}\sum_{sx-s<i\leq sx}\left[\mathbf{C}\right]_{i}\] (13)

where \(s\in\{k^{0},k^{1},k^{2},\cdots,n_{c}\}\) assuming \(n_{c}\) is a power of \(k\) and \(x\in\{1,2,\cdots,n_{c}/s\}\). \([\cdot]_{i}\) refers to the \(i\)-th row of the input matrix. We fix the increment ratio \(k=2\) for simplicity of discussion. The \(s\) represents the resolution of the approximation: it represents the number of non-VIP token embeddings being averaged into a vector \(\mathbf{c}_{x}^{s}\). Higher \(s\) (e.g., \(s=8\) in \(\mathbf{c}_{1}^{8}\) in Fig. 3) means lower resolution and heavier compression of the corresponding segment. The \(x\) represents the location of the \(s\)-length segment within the sequence \(\mathbf{C}\). In our scheme, we compress the sequence \(\mathbf{C}\) and use a set of \(\mathbf{c}_{x}^{s}\) for some selected \(s\)'s and \(x\)'s as the rows of the compressed \(\mathbf{S}_{c}\mathbf{C}\) as seen in Fig. 3. The sequence \(\mathbf{C}\) is broken into multiple segments of different lengths, then each segment is compressed into a vector \(\mathbf{c}_{x}^{s}\).

Procedurally, as shown in Fig. 3, our scheme starts with the heaviest compression and progressively refines certain segments of \(\mathbf{C}\) guided by the VIP-tokens \(\mathds{P}\). The scheme starts with the heaviest compression that treats \(\mathbf{C}\) as a \(n_{c}\)-length segment and compresses it to a single \(\mathbf{c}_{1}^{n_{c}}\). Then, starting with \(s=n_{c}\) (root node), the procedure **(1)** computes the averaged attention scores between VIP-tokens \(\mathds{P}\) and \(\mathbf{c}_{x}^{s}\)'s for different \(x\)'s (averaged over all attention heads and all VIP-tokens; only one \(\mathbf{c}_{1}^{n_{c}}\) at level \(s=n_{c}\)). We note that the attention scores are obtained by extracting attention matrices from MHA module (2) of the current Transformer layer when using \(\mathds{P}\) as queries and \(\mathbf{c}_{x}^{s}\)'s as keys. Then, it **(2)** splits the \(s\)-length segments corresponding \(\mathbf{c}_{x}^{s}\)'s with higher averaged attention scores (one segment is split in Fig. 3 but we might split more segments, again only one \(\mathbf{c}_{1}^{n_{c}}\) at level \(s=n_{c}\)) into \((s/2)\)-length sub-segments: the corresponding \(\mathbf{c}_{x}^{s/2}\) (13) of each sub-segment is computed for finer representation. Then, at next level for \(s=n_{c}/2\), the same procedure proceeds. This process continues until the sub-segments have length \(1\). We note that this procedure is guided by the VIP-tokens \(\mathds{P}\) and designed to maximally reduce the error of approximating (12). No additional learnable parameters are introduced for this scheme. The technical details of this algorithm are less relevant for our overall approach, but for interested readers, the details are discussed in the Appendix.

**How good is this approximation?** The output \(\mathds{P}_{new}\) (8) is well approximated since the approach preserves the relevant components of \(\mathbf{C}\) that have a high impact on the output \(\mathds{P}_{new}\). Further, if the VIP-tokens \(\mathds{P}\) have high attention weights for some rows of \(\mathbf{C}\), then the corresponding row in \(\mathbf{C}\) will be approximated with higher frequencies (less compressed). So, the output in \(\mathbf{C}_{new}\) (8) for a subset of non-VIP tokens that have a higher dependency with the VIP-tokens will have a better approximation than the others, as desired. This property is useful since some tokens with unknown locations but manifesting a high dependency with the VIP-tokens can also relevant

Figure 4: Proposed data structure \(\mathcal{T}(\mathbf{C})\)

Figure 3: Illustration of multi-resolution compression. \(n_{c}=8\). Purple line: compute attention scores between \(\mathds{P}\) and different segments. Green arrow: segment with higher attention score is split into two sub-segments. Accordingly, \(\mathbf{S}_{c}\mathbf{C}=\begin{bmatrix}\mathbf{c}_{1}^{2}&\mathbf{c}_{3}^{1}& \mathbf{c}_{4}^{1}&\mathbf{c}_{2}^{4}\end{bmatrix}^{\top}\) is constructed.

to the final prediction of a Transformer model in some tasks. The answer in span-based question answering tasks is one example, and our construction ensures that they will be approximated well too.

### Efficient Data Structure for Compression/Decompression

By employing the procedure in SS3.3 illustrated in Fig. 3, we can find the compressed \(\mathbf{S}_{c}\mathbf{C}\) with an \(\mathcal{O}(n_{c}d+rn_{p}d)\) cost at each layer. The main cost \(\mathcal{O}(n_{c}d)\) is due to computing \(\mathbf{c}_{x}^{s}\) defined in (13) for all resolution \(s\) and location \(x\) by using recursive relation from the bottom up:

\[\mathbf{c}_{x}^{2s}=\frac{1}{2}\mathbf{c}_{2x-1}^{s}+\frac{1}{2}\mathbf{c}_{2 x}^{s}\qquad\mathbf{c}_{x}^{1}=\left[\mathbf{C}\right]_{x}\] (14)

We find that these steps could introduce a large overhead. Further, note that if we decompress (apply \(\mathbf{S}^{\dagger}\) to) the output of \(\gamma\) for compressed sequence as in (8), the cost is \(\mathcal{O}(nd)\) since the number of nonzero entries in \(\mathbf{S}^{\dagger}\) is \(n\) (more details in Appendix). As a solution, we now introduce a data structure \(\mathcal{T}(\cdot)\) for storing \(\mathbf{C}\) and \(\mathbf{C}_{new}\), as shown in 4, which enables efficient computation of \(\mathbf{c}_{x}^{s}\) and eliminates explicit decompression. We note that this data structure is only possible due to the specific structure of \(\mathbf{S}_{c}\mathbf{C}\) constructed in SS3.3. Specifically, \(\mathcal{T}(\mathbf{C})\) stores \(\mathbf{c}_{1}^{n_{c}}\) and \(\Delta\mathbf{c}_{x}^{s}\) defined as

\[\Delta\mathbf{c}_{x}^{s}:=\mathbf{c}_{\left[x/2\right]}^{2s}-\mathbf{c}_{x}^{s}\] (15)

for every resolution \(s\neq n_{c}\) and location \(x\). Similarly, \(\mathcal{T}(\mathbf{C}_{new})\) stores \(\left(\mathbf{c}_{new}\right)_{1}^{n_{c}}\) and \(\Delta(\mathbf{c}_{new})_{x}^{s}\) where \(\left(\mathbf{c}_{new}\right)_{x}^{s}\) and \(\Delta(\mathbf{c}_{new})_{x}^{s}\) are defined similar to (13) and (15) but using \(\mathbf{C}_{new}\) instead of \(\mathbf{C}\).

Then, given \(\mathcal{T}(\mathbf{C})\), any \(\mathbf{c}_{x}^{s}\) can be retrieved efficiently in \(\mathcal{O}(\log(n_{c}d))\) cost via recursion:

\[\mathbf{c}_{x}^{s}=\mathbf{c}_{\left[x/2\right]}^{2s}-\Delta\mathbf{c}_{x}^{s} =\mathbf{c}_{\left[x/4\right]}^{4s}-\Delta\mathbf{c}_{\left[x/2\right]}^{2s}- \Delta\mathbf{c}_{x}^{s}=\cdots\] (16)

The only reason we need decompression \(\mathbf{S}^{\dagger}\) is that we need to obtain new representation \(\mathbf{C}_{new}\) (no decompression for \(\mathbb{P}_{new}\) since \(\mathbb{P}\) is uncompressed). Suppose we have \(\mathcal{T}(\mathbf{C}_{new})\), then we have an alternative way of getting \(\mathbf{C}_{new}\) similar to (16) (note \((\mathbf{c}_{new})_{x}^{1}=\left[\mathbf{C}_{new}\right]_{x}\)) without explicit decompression. The key benefit of this data structure is that we can obtain \(\mathcal{T}(\mathbf{C}_{new})\) by changing some nodes in \(\mathcal{T}(\mathbf{C})\). This only needs updating \(\mathcal{O}(r)\) nodes, and each update takes \(\mathcal{O}(d)\) cost.

**An example.** We show a \(\mathcal{T}(\mathbf{C})\) for \(n_{c}=8\) in Fig. 4. Let \(\mathbf{S}_{c}\mathbf{C}=\left[\mathbf{c}_{1}^{2}\quad\mathbf{c}_{3}^{1}\quad \mathbf{c}_{4}^{1}\quad\mathbf{c}_{4}^{2}\right]^{\top}\) as in Fig. 3. Since the segment \(\mathbf{c}_{1}^{2}\) is not split into sub-segments \(\mathbf{c}_{1}^{1}\) and \(\mathbf{c}_{2}^{1}\), we have (details in Appendix):

\[(\mathbf{c}_{new})_{1}^{1}-\mathbf{c}_{1}^{1}=(\mathbf{c}_{new})_{2}^{1}- \mathbf{c}_{2}^{1}=(\mathbf{c}_{new})_{1}^{2}-\mathbf{c}_{1}^{2}\] (17)

By rearranging (17), we can verify that \(\Delta(\mathbf{c}_{new})_{1}^{1},\Delta(\mathbf{c}_{new})_{2}^{1}\) in \(\mathcal{T}(\mathbf{C}_{new})\) stays the same as \(\Delta\mathbf{c}_{1}^{1},\Delta\mathbf{c}_{2}^{1}\) in \(\mathcal{T}(\mathbf{C})\) and thus do not need to be updated:

\[\Delta(\mathbf{c}_{new})_{1}^{1} =(\mathbf{c}_{new})_{1}^{2}-(\mathbf{c}_{new})_{1}^{1}=\mathbf{c} _{1}^{2}-\mathbf{c}_{1}^{1}=\Delta\mathbf{c}_{1}^{1}\] (18) \[\Delta(\mathbf{c}_{new})_{2}^{1} =(\mathbf{c}_{new})_{1}^{2}-(\mathbf{c}_{new})_{2}^{1}=\mathbf{c} _{1}^{2}-\mathbf{c}_{2}^{1}=\Delta\mathbf{c}_{2}^{1}\]

Further, we can verify that only the green nodes in Fig. 4 will be updated. These nodes correspond to the nodes in Fig. 3 that have been traversed. In summary, for each row \(\mathbf{c}_{x}^{s}\) of \(\mathbf{S}_{c}\mathbf{C}\) (a leaf node in Fig. 3), only the node storing \(\Delta(\mathbf{c}_{x})^{s}\) and its ancestor nodes in \(\mathcal{T}(\mathbf{C})\) must be updated, so the total number of nodes (including their ancestors) being updated is \(\mathcal{O}(r)\). Next, we can update the nodes as follows: first, we get representations \((\mathbf{c}_{new})_{1}^{2},(\mathbf{c}_{new})_{1}^{3},(\mathbf{c}_{new})_{1}^{4 },(\mathbf{c}_{new})_{2}^{4}\) by feeding \(\mathbf{S}_{c}\mathbf{C}\) into Transformer layer (details in Appendix). At level \(s=1\), given \((\mathbf{c}_{new})_{1}^{3}\) and \((\mathbf{c}_{new})_{1}^{4}\), we **(1)** compute \((\mathbf{c}_{new})_{2}^{2}\) via (14), and then **(2)** compute \(\Delta(\mathbf{c}_{new})_{1}^{1}\) and \(\Delta(\mathbf{c}_{new})_{1}^{4}\) via (15). The last two values are the new values for \(\Delta\mathbf{c}_{3}^{1}\) and \(\Delta\mathbf{c}_{4}^{1}\) in \(\mathcal{T}(\mathbf{C})\). At level \(s=2\), given \((\mathbf{c}_{new})_{2}^{1}\) and \((\mathbf{c}_{new})_{2}^{2}\) computed at previous level, we apply similar procedure to obtain \((\mathbf{c}_{new})_{1}^{4},\Delta(\mathbf{c}_{new})_{2}^{1},\Delta(\mathbf{c}_{ new})_{2}^{2}\), and the last two values are used to update two nodes in \(\mathcal{T}(\mathbf{C})\). It becomes apparent that each node update takes \(\mathcal{O}(d)\) cost. Putting it together: the complexity of modifying \(\mathcal{T}(\mathbf{C})\) to \(\mathcal{T}(\mathbf{C}_{new})\) is \(\mathcal{O}(rd)\). The detailed algorithm and complexity analysis are described in Appendix.

By maintaining this data structure, we never need to materialize the entire \(\mathbf{C}\) or \(\mathbf{C}_{new}\) in any intermediate layer, but instead we use (16) to construct the rows of \(\mathbf{S}_{c}\mathbf{C}\) and perform updates to \(\mathcal{T}(\mathbf{C})\) to obtain \(\mathbf{C}_{new}\) (represented as \(\mathcal{T}(\mathbf{C}_{new})\)) at each intermediate layer. At the output of a Transformer, \(\mathbf{C}_{new}\) is materialized from \(\mathcal{T}(\mathbf{C}_{new})\) at a \(\mathcal{O}(n_{c}d)\) cost via the recursion (16) from the bottom up.

Experiments

We perform a broad set of experiments to empirically evaluate the performance of our proposed compression. (See hyperparameters/dataset statistics in Appendix.) We evaluate our method on both encoder-only and encoder-decoder architecture types. We compare our method with baselines on a large list of question answering and summarization tasks, where we found long sequences occur most frequently. Then, we study the model performance of scaling to ultra long sequences enabled by our method. Since efficiency is the focus of the efficient baselines and our work, we include runtime efficiency (of a single sequence) in millisecond in each table. (See the procedure for runtime measurement in Appendix.) We also include a discussion on FLOP efficiency in Appendix.

For ease of implementation and hyperparameter selection, we restrict the rows of \(\mathbf{S}_{c}\mathbf{C}\) to have exactly two resolutions for experiments. Specifically, for a pre-defined increment ratio \(k\), we split and refine all segments \(\mathbf{c}_{x}^{s}\) with \(s>k\) to \(k\)-length sub-segments, and select \(h\) (pre-defined) \(k\)-length segments to further split to \(1\)-length sub-segments. So, the rows of \(\mathbf{S}_{c}\mathbf{C}\) would consist of \((n_{c}/k-h)\) of \(\mathbf{c}_{x}^{k}\) and \(hk\) of \(\mathbf{c}_{x}^{1}\) for some \(x\). To simplify the implementation, we only use the proposed compression in the encoder, and use the vanilla computation in the decoder of encoder-decoder models. We note that our method might be applied to the decoder (more details in Appendix).

Further, we found a few layers of standard Transformer layers to pre-process tokens helps the performance. Therefore, in the initial stage of a Transformer, we segment input sequence into multiple \(512\)-length segments. For each segment, we use vanilla computation in the first 4 layers (for base models and 6 layers for larger models) of a Transformer. Then, for the remaining layers, segments are concatenated back into one sequence and processed using our proposed compression. There is _no communication_ among any segments, so the downstream tasks cannot be solved by these first 4 transformer layers alone, and the initial stage is used just for getting a reasonable representation for the compression to operate on.

**Approximation Quality of VIP-Tokens.** We empirical measured the approximation quality of our VIP centric strategy compared to random strategy (the tree growth in Fig. 3 is not guided by VIP-tokens, but is random) and lazy strategy (each k-length segment is compressed to a token). \(\mathds{P}_{new}\) is the approximated representation of VIP tokens computed with compression and let \(\mathds{P}_{new}^{*}\) be the ground truth representation of VIP tokens computed without compression. We measure the relative error (defined as \(||\mathds{P}_{new}^{*}-\mathds{P}_{new}^{*}||_{F}/||\mathds{P}_{new}^{*}||_{F}\)) and correlation coefficient between \(\mathds{P}_{new}\) and \(\mathds{P}_{new}^{*}\). As shown in Tab. 1, we can verify that the proposed procedure indeed improve the approximation quality of VIP-tokens.

**Encoder-Only Models**. For encoder-only architecture, we compare our method with RoBERTa [22] and three strong baselines: Longformer [2], Big Bird [38], and MRA Attention [39]. We first pretrain a RoBERTa model using masked language modeling task, then for each method, we perform continuous pretraining from the RoBERTa checkpoint to expand the positional embeddings to 4K length and adjust model parameters to adapt approximations used in efficient baselines and our method. We verify that our proposed method can be integrated into a pretrained Transformer with some continuous pretraining. But we note that the amount of reduction in log perplexity for our method (\(-0.114\)) during pre-training is much larger than Longformer (\(-0.017\)) and Big Bird (\(-0.025\)) from 50K steps to 250K steps. The continuous pretraining for these baselines might have saturated since only the self-attention is approximated while our method might require more pretraining to adjust the parameters for more aggressive approximation. So, we run a larger scale pretraining for our method; downstream results are in Tab. 2 and Fig. 5, denoted with *. We use HotpotQA [37], QuALITY [24], and WikiHop [33] to assess the language models. HotpotQA is an answer span extraction task, while QuALITY and WikiHop are multi-choice question answering tasks. We set questions and multi-choice answers (for QuALITY and WikiHop) as VIP-tokens.

As shown in Tab. 2, we verify that our method is consistently better compared to Longformer and Big Bird. Our method obtains better accuracy in QuALITY and WikiHop compared to 4K length

\begin{table}
\begin{tabular}{l l l} \hline \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{} \end{tabular} } \\ \hline Random & 0.403 & 0.919 \\ Lazy & 0.528 & 0.869 \\ VIPGener & 0.137 & 0.991 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Approx quality.

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{} \end{tabular} } \\ \hline RoBERTa & base & 512 & 19.9 & 35.1 & 44.9 & 21.2 & 39.0 & 19.6 & 67.6 \\ RoBERTa & base & 4K & 422.3 & 62.2 & 76.1 & 40.2 & 39.5 & 41.4 & 75.2 \\ Big Bird & base & 4K & 297.9 & 59.5 & 32.7 & 30.0 & 85.5 & 29.3 & 74.5 \\ Longformer & base & 4K & 371.9 & 59.9 & 73.6 & 18.0 & 27.3 & 39.0 & 77.4 \\ MRA Attention & base & 4K & 203.5 & 63.4 & 77.0 & 20.5 & 38.7 & 19.2 & 76.1 \\ MRA & base & 4K & 116.6 & 609.7 & 74.6 & 126.4 & 39.6 & 108.0 & 75.9 \\ Ours* & base & 4K & 114.6 & 61.4 & 75.0 & 125.7 & 39.5 & 108.0 & 76.1 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Dev set results for encoder-only models.

[MISSING_PAGE_FAIL:9]

to much longer sequences. We note that NarrativeQA [19] is an ideal testbed as shown in dataset statistics in Appendix. The results are shown in Tab. 4. The left / middle / right values of runtime column are for the entire model / the encoder / the last 8 layers (out of 12 layers) that uses our compression. The performance monotonically increases as sequence length increases. We note that for sequence length 64K, the performance of model with \(k=64\) is lower than the model with \(k=16\). We suspect that since the results are finetuned from the same model that is pretrained with \(k=16\), the large gap between the two different \(k\)'s may have a negative impact on finetuning performance. Nevertheless, the performance is still higher than 32K length models.

**Why focus on 4K - 128K lengths?** We believe that the computation required by standard Transformers for processing shorter sequences is not an efficiency bottleneck. As a result, we do not profile the performance of our method for smaller length sequences, since the standard Transformers are sufficiently fast in this case. Further, while our model can be applied to shorter sequences, we suspect that for shorter sequences, there may be less irrelevant information for VIP-tokens. So compressing the irrelevant information will not offer a meaningful speed up. This is a limitation as the compression works better when there is more compressible information. We have only pushed the sequence lengths to 128K since this length was sufficient to cover a majority of sequence lengths encountered in long sequence tasks (for example, our model is able to process an entire book at once).

## 5 Limitations

Our method assumes that in many tasks, a subset of tokens are disproportionately responsible for the model prediction, the remaining non-VIP-tokens may play a role but are less critical. Our method excels specifically on such tasks by selectively locating relevant information in the sequence for given VIP-tokens. As the experiments show, this choice is effective in many cases but this behavior is not universal. Occasionally, an embedding is pre-computed which must then serve multiple tasks concurrently, e.g., _both_ text retrieval and natural language inference. In this case, if we do not know the tasks beforehand, VIP-token selection cannot be meaningfully performed. Further, VIP-token selection requires some understanding of the tasks. However, we believe that a reasonable selection can be made with some generic knowledge for most tasks or use cases. Please see Appendix for VIP-token selection guidelines.

To reduce the complexity of our implementation, the method is currently setup for the encoder module of the Transformer that assumes full access to the entire sequence. The proposed compression might be extended to approximate the computation in the decoder, but it needs more implementation work, so we leave it as future work. Consequently, the current implementation is less useful for decoder-only models (but in the appendix, we discuss some strategies).

## 6 Conclusions

We propose a VIP-token centric sequence compression method to compress/decompress the input/output sequences of Transformer layers thereby reducing the complexity dependency on the sequence length \(n\) without sacrificing the model accuracy. Our empirical evaluation shows that our method can be directly incorporated into existing pretrained models with some additional training. Also, it often has much higher efficiency compared to baselines with the same sequence length while offering better or competitive model accuracy. For future work, we believe that extending our method to the decoder of the encoder-decoder and decoder-only models will further boost the efficiency of Transformers while maintaining similar model performance.

Acknowledgments.Zeng and Singh were supported in part by funding from the Vilas Board of Trustees and UW-Madison Office of the Vice Chancellor for Research and Graduate Education.

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline Length & Runtime (ms) & \(k\) & \(h\) & EM & F1 \\ \hline
16K & 518.2 / 394.4 / 162.4 & 16 & 90 & 5.9 & 16.6 \\
32K & 946.8 / 671.6 / 2126.0 & 32 & 55 & 6.6 & 17.5 \\
32K & 1027.9 / 751.0 / 298.0 & 16 & 90 & 6.4 & 17.5 \\
64K & 1848.7 / 1177.2 / 254.8 & 64 & 30 & 7.2 & 18.4 \\
64K & 2244.8 / 1574.2 / 559.4 & 16 & 90 & 7.5 & 19.3 \\
128K & 6267.8 / 5125.9 / 1902.2 & 16 & 90 & 8.0 & 19.6 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Dev results of NarrativeQA on base model when scaling sequence length from 16K to 128K.

## References

* [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* [3] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer, 2022.
* [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _European Conference on Computer Vision_, pages 213-229. Springer, 2020.
* [5] Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. SummScreen: A dataset for abstractive screenplay summarization. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8602-8615, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [6] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In _International Conference on Learning Representations (ICLR)_, 2021.
* [7] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)_, 2018.
* [8] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 4271-4282. Curran Associates, Inc., 2020.
* [9] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4599-4610, Online, June 2021. Association for Computational Linguistics.
* [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)_, 2019.
* [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations (ICLR)_, 2021.
* [12] Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 1074-1084, Florence, Italy, July 2019. Association for Computational Linguistics.
* [13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.
* [14] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In _Findings of the Association for Computational Linguistics: NAACL 2022_, pages 724-736, Seattle, United States, July 2022. Association for Computational Linguistics.

* [15] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1419-1436, Online, June 2021. Association for Computational Linguistics.
* [16] Xin Huang, Ashish Khetan, Rene Bidart, and Zohar Karnin. Pyramid-BERT: Reducing complexity via successive core-set based token selection. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8798-8817, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [17] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _International Conference on Learning Representations (ICLR)_, 2020.
* [18] Bryan Klimt and Yiming Yang. The enron corpus: A new dataset for email classification research. In _Proceedings of the 15th European Conference on Machine Learning_, ECML'04, page 217-226, Berlin, Heidelberg, 2004. Springer-Verlag.
* [19] Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. _Transactions of the Association for Computational Linguistics_, 6:317-328, 2018.
* [20] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In _Proceedings of Machine Translation Summit X: Papers_, pages 79-86, Phuket, Thailand, September 13-15 2005.
* [21] Yuta Koreeda and Christopher Manning. ContractNLI: A dataset for document-level natural language inference for contracts. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 1907-1919, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [23] Piotr Nawrot, Jan Chorowski, Adrian Lancucki, and Edoardo M. Ponti. Efficient transformers with dynamic token pooling, 2022.
* [24] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY: Question answering with long input texts, yes! In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5336-5358, Seattle, United States, July 2022. Association for Computational Linguistics.
* [25] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In _International Conference on Learning Representations (ICLR)_, 2021.
* [26] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In _International Conference on Learning Representations_, 2020.
* [27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* [28] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In _International Conference on Learning Representations_, 2019.
* [29] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1073-1083, Vancouver, Canada, July 2017. Association for Computational Linguistics.

* [30] Uri Shaham, Elad Segal, Maor Ivgi, Avira Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized comparison over long language sequences. In _EMNLP_, 2022.
* [31] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In _International Conference on Learning Representations (ICLR)_, 2021.
* [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [33] Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. _Transactions of the Association for Computational Linguistics_, 6:287-302, 2018.
* [34] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In _International Conference on Learning Representations_, 2022.
* [35] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: A nystrom-based algorithm for approximating self-attention. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2021.
* [36] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [37] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2018.
* [38] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [39] Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh. Multi resolution analysis (MRA) for approximate self-attention. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 25955-25972. PMLR, 17-23 Jul 2022.
* [40] Zhanpeng Zeng, Yunyang Xiong, Sathya Ravi, Shailesh Acharya, Glenn M Fung, and Vikas Singh. You only sample (almost) once: Linear cost self-attention via bernoulli sampling. In _International Conference on Machine Learning (ICML)_, 2021.
* [41] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. _arXiv preprint arXiv:2012.15840_, 2020.
* [42] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5905-5921, Online, June 2021. Association for Computational Linguistics.
* [43] Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. Mediasum: A large-scale media interview dataset for dialogue summarization. _arXiv preprint arXiv:2103.06410_, 2021.

## 7 Appendix

### Definition of Notations

We provide a table 5 of notations that are used for more than once so that the readers can refer to their definition easily.

### Details of Multi-Resolution Compression

We describe the omitted technical details of a modified formulation of [39] to construct \(\mathbf{S}_{c}\mathbf{C}\) and corresponding \(\mathbf{S}_{c}\) satisfying good approximation of

\[\exp(\mathbf{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{\top})\mathbf{S}_{c}\approx \exp(\mathbf{P}\mathbf{C}^{\top}).\] (19)

Before diving into the technical details of constructing \(\mathbf{S}_{c}\), we introduce some notations and tools that will be used later. We use \([\cdot]_{i}\) to refer the \(i\)-th entry/row of the input vector/matrix and \([\cdot]_{i,j}\) to refer the \((i,j)\)-th entry of the input matrix. **BOLD** uppercase letters denote matrices, **bold** lower case letters denote vectors, and regular lower case letters denote scalars. Let \(\mathbf{v}_{i}\) be a vector, when we write a matrix of form

\[\begin{bmatrix}\mathbf{v}_{1}&\mathbf{v}_{2}&\cdots&\mathbf{v}_{m}\end{bmatrix},\] (20)

we treat \(\mathbf{v}_{i}\) as a column of the matrix. When we write a matrix of form

\[\begin{bmatrix}\mathbf{v}_{1}\\ \mathbf{v}_{2}\\ \cdots\\ \mathbf{v}_{m}\end{bmatrix},\] (21)

we treat \(\mathbf{v}_{i}\) as a row of the matrix.

\begin{table}
\begin{tabular}{c l} \hline \hline Notation & Description \\ \hline \(l\) & number of layers of a Transformer model \\ \(n\) & number of tokens of an input sequence \\ \(d\) & model embedding dimension \\ \(n_{p}\) & number of VIP-tokens \\ \(n_{e}\) & number of non-VIP/remaining tokens, so \(n_{p}+n_{c}=n\) \\ \(r\) & length of a compressed sequence \\ \hline \(\alpha(\cdot,\cdot,\cdot)\) & multi-head attention taking three inputs for query/key/value embeddings \\ \(\beta(\cdot)\) & two-layer feed-forward network \\ \(\gamma(\cdot)\) & function representing all heavy computation of a Transformer layer \\ \hline \(\mathbf{X}\) & embedding matrix representing a input sequence \\ \(\mathbf{P}\) & embedding matrix representing the VIP-tokens \\ \(\mathbf{C}\) & embedding matrix representing the non-VIP/remaining tokens \\ \(\mathbf{X}_{new}\) & updated embedding matrix of a input sequence, the output of a Transformer layer \\ \(\mathbf{P}_{new}\) & updated embedding matrix representing the VIP-tokens \\ \(\mathbf{C}_{new}\) & updated embedding matrix representing the non-VIP/remaining tokens \\ \hline \(\mathbf{S}\) & compression matrix \\ \(\mathbf{S}_{c}\) & compression submatrix for the non-VIP/remaining tokens \\ \(\mathbf{S}_{c}^{\top}\) & decompression matrix \\ \(\mathbf{S}_{c}^{\top}\) & decompression submatrix for the non-VIP/remaining tokens \\ \hline \(s\) & resolution of approximation, represents the number of non-VIP token embeddings being averaged \\ \(x\) & location of s-length segment with the original sequence \\ \(k\) & increment of resolution, \(s\in\{1,k^{2},k^{3},\cdots,n_{e}\}\) \\ \(h\) & number of \(k\)-length segments are split into 1-length sub-segments, used for experiments \\ \(h_{s}\) & number of \(s\)-length segments are split into shorter sub-segments, used in Appendix discussion \\ \hline \(\mathcal{J}\) & set of components \(\mathbf{b}_{i}^{s}\) that is constructed via Alg. 1, the elements are the rows of \(\mathbf{S}_{c}\) \\ \(\mathbf{b}_{j}^{s}\) & components used for 1-D wavelet transform of scaling \(s\) and translation \(x\) \\ \(\mathbf{e}_{r}^{s}\) & local average of \(x\)-th \(s\)-length segment of sequence \(\mathbf{C}\) \\ \((\mathbf{e}_{new})_{x}^{s}\) & local average of \(x\)-th \(s\)-length segment of sequence \(\mathbf{C}_{new}\) \\ \hline \(\mathcal{T}(\cdot)\) & data structure for storing the input sequence \(\mathbf{C}\) or \(\mathbf{C}_{new}\) \\ \(\Delta\mathbf{e}_{r}^{s}\) & state stored in \(\mathcal{T}(\mathbf{C})\) defined as \(\Delta\mathbf{e}_{r}^{s}:=\mathbf{e}_{r/2}^{s}-\mathbf{e}_{r}^{s}\) \\ \(\Delta(\mathbf{e}_{new})_{x}^{s}\) & state stored in \(\mathcal{T}(\mathbf{C}_{new})\) defined as \(\Delta(\mathbf{e}_{new})_{x}^{s}:=(\mathbf{e}_{new})_{\lceil x/2\rceil}^{2s}-( \mathbf{e}_{new})_{s}^{s}\) \\ \hline \([\cdot]_{i}\) & \(i\)-th entry/row of the input vector/matrix \\ \([\cdot]_{i,j}\) & \((i,j)\)-th entry of the input matrix \\ \hline \hline \end{tabular}
\end{table}
Table 5: Major notations used

#### 7.2.1 Basic Problem Setup

Let \(\mathbf{b}_{x}^{s}\in\mathbb{R}^{n_{c}}\) be a multi-resolution component defined as

\[[\mathbf{b}_{x}^{s}]_{i}:=\begin{cases}\frac{1}{s}&\text{if }sx-s<i\leq sx\\ 0&\text{otherwise}\end{cases}\] (22)

for \(s\in\{k^{0},k^{1},k^{2},\cdots,n_{c}\}\) assuming \(n_{c}\) is a power of \(k\) (we assume \(k=2\) for simplicity). Here, \(s\) and \(x\) represent the scaling and translation (similar to the concepts in wavelet basis) of the component, respectively. Fig. 6 is the visualization of \(\mathbf{b}_{x}^{s}\).

Then, any 1D signal \(\mathbf{f}\in\mathbb{R}^{n_{c}}\) can be represented as a linear combination of \(\mathbf{b}_{x}^{s}\):

\[\mathbf{f}=\sum_{s,x}c_{x}^{s}\mathbf{b}_{x}^{s}\] (23)

where \(c_{x}^{s}\) are the coefficients for the linear combination. For a signal with multi-resolution structure (that is, signal has high frequency in some regions and has low frequency in other regions), we can find an approximation \(\hat{f}^{*}\) that can be expressed as a _sparse_ linear combination where most coefficients are zeros, as shown in Fig. 7.

\[\mathbf{f}\approx\hat{\mathbf{f}}^{*}:=\sum_{\mathbf{b}_{x}^{s}\in\mathcal{J} }c_{x}^{s}\mathbf{b}_{x}^{s}\] (24)

We denote \(\mathcal{J}\) as the set of major components \(\mathbf{b}_{x}^{s}\) corresponding to the large coefficients, that is, \(\mathcal{J}:=\{\mathbf{b}_{x}^{s}\mid|c_{x}^{s}|\text{ being large}\}\). Since the set of all possible \(\mathbf{b}_{x}^{s}\) is an over-complete dictionary, there are multiple possible linear combinations. To reduce the search space of the best set \(\mathcal{J}\), we place a mild restriction on the set \(\mathcal{J}\):

\[\left[\sum_{\mathbf{b}_{x}^{s}\in\mathcal{J}}\mathbf{b}_{x}^{s}\right]_{i} \neq 0\quad\forall i\qquad\quad\langle\mathbf{b}_{x}^{s},\mathbf{b}_{x^{\prime}}^ {s^{\prime}}\rangle=0\quad\forall\mathbf{b}_{x}^{s},\mathbf{b}_{x^{\prime}}^{ s^{\prime}}\in\mathcal{J},\mathbf{b}_{x}^{s}\neq\mathbf{b}_{x^{\prime}}^{s^{ \prime}}\] (25)

The conditions state that each entry of signal \(\mathbf{f}\) is included in the support region of exactly one component in \(\mathcal{J}\). With these tools, we will first describe the approximation when \(\mathcal{J}\) is given, then discuss how the approximation connects the set \(\mathcal{J}\) to our target \(\mathbf{S}_{c}\) and \(\mathbf{S}_{c}\mathbf{C}\). Finally, we will discuss how to construct this \(\mathcal{J}\).

#### 7.2.2 Plugging Our Problem into the Setup

A recent result shows that the self-attention matrix \(\exp(\mathbf{X}\mathbf{X}^{\top})\) has the multi-resolution structure discussed above [39]. Since \(\exp(\mathbf{P}\mathbf{C}^{\top})\) is a sub-matrix of \(\exp(\mathbf{X}\mathbf{X}^{\top})\), we conjecture that the multi-resolution structure also holds in \(\exp(\mathbf{P}\mathbf{C}^{\top})\). As a result, we can find a sparse combination of \(\mathbf{b}_{x}^{s}\) to represent rows of \(\exp(\mathbf{P}\mathbf{C}^{\top})\).

**Claim 7.1**.: _Given the set \(\mathcal{J}\) satisfying restriction (25), we can define an approximation of the \(i\)-th row of \(\exp(\mathbf{P}\mathbf{C}^{\top})\) similar to (24) as illustrated in Fig. 7_

\[\left[\widehat{\exp(\mathbf{P}\mathbf{C}^{\top})}^{*}\right]_{i}:=\sum_{ \mathbf{b}_{x}^{s}\in\mathcal{J}}c_{x}^{s}\mathbf{b}_{x}^{s}\] (26)

Figure 6: Visualization of \(\mathbf{b}_{x}^{s}\) for some scaling \(s\) and translation \(x\). The y axis for different plots are not the same.

Figure 7: An example of approximating an 1D signal using a truncated wavelet transform with components defined in (22). It uses a set \(\mathcal{J}\) of size \(79\) to represent a signal in \(\mathbb{R}^{512}\).

_where \(c_{x}^{s}\) is the optimal solution that minimizes_

\[\left\|\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i}-\left[\widehat{\exp( \mathbf{P}\mathbf{C}^{\top})}^{*}\right]_{i}\right\|_{2}^{2}\] (27)

_Then, the approximation can be written as:_

\[\left[\widehat{\exp(\mathbf{P}\mathbf{C}^{\top})}^{*}\right]_{i,j}=\left\langle \left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i},\mathbf{b}_{x}^{s}\right\rangle\] (28)

_where \(\mathbf{b}_{x}^{s}\in\mathcal{J}\) is the component that is supported on \(j\) (a.k.a. \(\left[\mathbf{b}_{x}^{s}\right]_{j}\neq 0\) and there is exactly one \(\mathbf{b}_{x}^{s}\in\mathcal{J}\) satisfy this condition due to restriction (25))._

Proof.: If \(\mathcal{J}\) is given, let \(\mathbf{B}\in\mathbb{R}^{n_{c}\times|\mathcal{J}|}\) be a matrix whose columns are elements \(\mathbf{b}_{x}^{s}\in\mathcal{J}\) and let \(\mathbf{c}\in\mathbb{R}^{|\mathcal{J}|}\) be a vector whose entries are the corresponding \(c_{x}^{s}\):

\[\mathbf{B} :=\left[\mathbf{b}_{x_{1}}^{s_{1}}\quad\mathbf{b}_{x_{2}}^{s_{2}} \quad\cdots\quad\mathbf{b}_{x_{|\mathcal{J}|}}^{s_{|\mathcal{J}|}}\right]\] (29) \[\mathbf{c} :=\left[c_{x_{1}}^{s_{1}}\quad c_{x_{2}}^{s_{2}}\quad\cdots\quad c _{x_{|\mathcal{J}|}}^{s_{|\mathcal{J}|}}\right]^{\top}\]

then the approximation can be expressed as

\[\left[\widehat{\exp(\mathbf{P}\mathbf{C}^{\top})}^{*}\right]_{i}=\mathbf{b}_ {x}^{s}\in\mathcal{J}_{x}^{s}\mathbf{b}_{x}^{s}=\mathbf{B}\mathbf{c}\] (30)

If we solve for

\[\mathbf{c}:=\arg\min_{\beta}\left\|\left[\exp(\mathbf{P}\mathbf{C}^{\top}) \right]_{i}-\mathbf{B}\beta\right\|\] (31)

then

\[\mathbf{c}=(\mathbf{B}^{\top}\mathbf{B})^{-1}\mathbf{B}^{\top}\left[\exp( \mathbf{P}\mathbf{C}^{\top})\right]_{i}\] (32)

Due to the restriction (25), the columns of \(\mathbf{B}\) are orthogonal, so \(\mathbf{B}^{\top}\mathbf{B}\) is a diagonal matrix:

\[\mathbf{B}^{\top}\mathbf{B}=\begin{bmatrix}1/s_{1}&&\\ &1/s_{2}&&\\ &&\cdots&\\ &&&1/s_{|\mathcal{J}|}\end{bmatrix}\] (33)

We can also write down \(\mathbf{B}^{\top}\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i}\)

\[\mathbf{B}^{\top}\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i}=\begin{bmatrix} \left\langle\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i},\mathbf{b}_{x_ {1}^{s_{1}}}^{s_{1}}\right\rangle\\ \left\langle\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i},\mathbf{b}_{x_ {2}^{s_{2}}}^{s_{2}}\right\rangle\\ \cdots\\ \left\langle\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i},\mathbf{b}_{x_ {|\mathcal{J}|}}^{s_{|\mathcal{J}|}}\right\rangle\end{bmatrix}\] (34)

Putting everything together, we have

\[\mathbf{B}\mathbf{c}=\begin{bmatrix}s_{1}\mathbf{b}_{x_{1}}^{s_{1}}&s_{2} \mathbf{b}_{x_{2}}^{s_{2}}&\cdots&s_{|\mathcal{J}|}\mathbf{b}_{x_{|\mathcal{J }|}}^{s_{|\mathcal{J}|}}\end{bmatrix}\begin{bmatrix}\left\langle\left[\exp( \mathbf{P}\mathbf{C}^{\top})\right]_{i},\mathbf{b}_{x_{1}^{s_{1}}}^{s_{1}} \right\rangle\\ \left\langle\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i},\mathbf{b}_{x_ {2}^{s_{2}}}^{s_{2}}\right\rangle\\ \cdots\\ \left\langle\left[\exp(\mathbf{P}\mathbf{C}^{\top})\right]_{i},\mathbf{b}_{x_ {|\mathcal{J}|}}^{s_{|\mathcal{J}|}}\right\rangle\end{bmatrix}\] (35)

We note that \(s\mathbf{b}_{x}^{s}\) simply re-scale the entry of \(\mathbf{b}_{x}^{s}\) such that any non-zero entry becomes \(1\). Then, let us consider \(j\)-th entry of \(\mathbf{B}\mathbf{c}\). Due to the restriction (25), we have exactly one \(\mathbf{b}_{x}^{s}\in\mathcal{J}\) whose support region contains \(j\), so the \(j\)-th row of the first matrix at the right hand side of (35) contains exactly a \(1\) and the remaining entries are \(0\). Therefore, we have

\[\left[\widehat{\exp(\mathbf{P}\mathbf{C}^{\top})}^{*}\right]_{i,j}=\left[ \mathbf{B}\mathbf{c}\right]_{j}=\left\langle\left[\exp(\mathbf{P}\mathbf{C}^{ \top})\right]_{i},\mathbf{b}_{x}^{s}\right\rangle\] (36)

where \(\mathbf{b}_{x}^{s}\in\mathcal{J}\) is the component that is supported on \(j\), which concludes our proof.

#### 7.2.3 Efficient Approximation

We note that computing (28) for all \(j\) would require access to the entire \(\left[\exp(\mathrm{P}\mathbf{C}^{\top})\right]_{i}\). We exploit the same strategy as described in [39], so the exponential of inner product is used as an approximation to inner product of exponential.

\[\left[\widehat{\exp(\mathrm{P}\mathbf{C}^{\top})}\right]_{i,j}:=\exp(\left\langle \left[\mathrm{P}\mathbf{C}^{\top}\right]_{i},\mathbf{b}_{x}^{s}\right\rangle)\] (37)

We note that \(\left\langle\left[\mathbf{P}\mathbf{C}^{\top}\right]_{i},\mathbf{b}_{x}^{s}\right\rangle\) is the local average of the support region of \(\mathbf{b}_{x}^{s}\), which is also the \(x\)-th \(s\)-length segment of sequence \(\left[\mathbf{P}\mathbf{C}^{\top}\right]_{i}\):

\[\left\langle[\mathrm{P}\mathbf{C}^{\top}]_{i},\mathbf{b}_{x}^{s}\right\rangle =\frac{1}{s}\sum_{\left[\mathbf{b}_{x}^{s}\right]_{j}\neq 0}\left[ \mathrm{P}\mathbf{C}^{\top}\right]_{i,j}\] (38)

By using some arithmetic manipulations, (37) can be efficiently computed

\[\exp(\left\langle[\mathrm{P}\mathbf{C}^{\top}]_{i},\mathbf{b}_{ x}^{s}\right\rangle) =\exp(\frac{1}{s}\sum_{\left[\mathbf{b}_{x}^{s}\right]_{j}\neq 0} \left[\mathrm{P}\mathbf{C}^{\top}\right]_{i,j})=\exp(\frac{1}{s}\sum_{\left( \mathbf{b}_{x}^{s}\right)_{j}\neq 0}\left\langle[\mathrm{P}]_{i}\,,[ \mathbf{C}]_{j}\right\rangle)\] (39) \[=\exp(\left\langle[\mathrm{P}]_{i}\,,\frac{1}{s}\sum_{\left[ \mathbf{b}_{x}^{s}\right]_{j}\neq 0}\left[\mathbf{C}\right]_{j}\right\rangle)=\exp( \left\langle[\mathrm{P}]_{i}\,,\mathbf{c}_{x}^{s}\right\rangle)\]

where \(\mathbf{c}_{x}^{s}\) is defined in the main text:

\[\mathbf{c}_{x}^{s}:=\frac{1}{s}\sum_{sx-s<i\leq sx}\left[\mathbf{C}\right]_{i} =\mathbf{b}_{x}^{s}\mathbf{C}\] (40)

We note that \(\mathbf{c}_{x}^{s}\) is the local average of the \(x\)-th \(s\)-length segment of sequence \(\mathbf{C}\). The \(\mathbf{c}_{x}^{s}\) can be efficiently computed via

\[\mathbf{c}_{x}^{2s}=\frac{1}{2}\mathbf{c}_{2x-1}^{s}+\frac{1}{2}\mathbf{c}_{2 x}^{s}\qquad\mathbf{c}_{x}^{1}=\left[\mathbf{C}\right]_{x}\] (41)

**Claim 7.2**.: _Given the set \(\mathcal{J}\) satisfying restriction (25), let \(\mathbf{S}_{c}\) be be a matrix whose rows are elements \(\mathbf{b}_{x}^{s}\in\mathcal{J}\)_

\[\mathbf{S}_{c}=\begin{bmatrix}\mathbf{b}_{x_{1}}^{s_{1}}\\ \mathbf{b}_{x_{2}}^{s_{2}}\\ \cdot\\ \cdot\\ \mathbf{b}_{x_{|\mathcal{J}|}}^{s_{|\mathcal{J}|}}\end{bmatrix}\qquad\mathbf{D }=\begin{bmatrix}s_{1}&&&\\ &s_{2}&&\\ &&\ldots&\\ &&&s_{|\mathcal{J}|}\end{bmatrix}\] (42)

_Then,_

\[\exp(\mathrm{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{\top})\mathbf{D}\mathbf{S}_{ c}=\widehat{\exp(\mathrm{P}\mathbf{C}^{\top})}\] (43)

_where \(\widehat{\exp(\mathrm{P}\mathbf{C}^{\top})}\) is defined as (37)._

Proof.: Consider \(i\)-th row of \(\exp(\mathrm{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{\top})\),

\[\left[\exp(\mathrm{P}(\mathbf{S}_{c}\mathbf{C})^{\top})\right]_{i} =\exp(\left[\mathrm{P}\right]_{i}(\mathbf{S}_{c}\mathbf{C})^{\top})\] (44) \[=\exp(\left[\mathrm{P}\right]_{i}\left[\mathbf{c}_{x_{1}}^{s_{1}} \quad\mathbf{c}_{x_{2}}^{s_{2}}\quad\cdots\quad\mathbf{c}_{x_{|\mathcal{J}|}}^{s _{|\mathcal{J}|}}\right])\] \[=\left[\exp(\left\langle[\mathrm{P}]_{i}\,,\mathbf{c}_{x_{1}}^{s_ {1}}\right\rangle)\quad\cdots\quad\exp(\left\langle[\mathrm{P}]_{i}\,, \mathbf{c}_{x_{|\mathcal{J}|}}^{s_{|\mathcal{J}|}}\right\rangle)\right]\]

Then, we have

\[\left[\exp(\mathrm{P}(\mathbf{S}_{c}\mathbf{C})^{\top})\mathbf{D}\mathbf{S}_{ c}\right]_{i}=\left[\exp(\left\langle[\mathrm{P}]_{i}\,,\mathbf{c}_{x_{1}}^{s_ {1}}\right\rangle)\quad\cdots\quad\exp(\left\langle[\mathrm{P}]_{i}\,, \mathbf{c}_{x_{|\mathcal{J}|}}^{s_{|\mathcal{J}|}}\right\rangle)\right]\begin{bmatrix} s_{1}\mathbf{b}_{x_{1}}^{s_{1}}\\ \cdots\\ \mathbf{s}_{|\mathcal{J}|}\mathbf{b}_{x_{|\mathcal{J}|}}^{s_{|\mathcal{J}|}} \end{bmatrix}\] (45)

We note that \(s\mathbf{b}_{x}^{s}\) simply re-scales the entry of \(\mathbf{b}_{x}^{s}\) such that any non-zero entry becomes \(1\). Then, let us consider \(j\)-th entry of \(\left[\exp(\mathrm{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{\top})\mathbf{D}\mathbf{S }_{c}\right]_{i}\). Due to the restriction (25), we have exactly one

[MISSING_PAGE_FAIL:18]

The second equality and fourth equality are due to (25) and (39). The approximation error is governed by two components multiply together: attention score between \(\left[\mathbb{P}\right]_{i}\) and the local average \(\mathbf{c}_{x}^{s}\) of the \(x\)-th \(s\)-length segment of sequence \(\mathbf{C}\) and the inner product of \(\left[\mathbb{P}\right]_{i}\) with the amount of deviation of \(\left[\mathbf{C}\right]_{j}\) from its local average \(\mathbf{c}_{x}^{s}\).

When \(s=1\), the deviation is simply zero:

\[\mathbf{c}_{x}^{1}=\left[\mathbf{C}\right]_{x}.\] (52)

It is reasonable to assume that the deviation \(\left[\mathbf{C}\right]_{j}-\mathbf{c}_{x}^{s}\) is smaller if \(s\) is smaller. Therefore, this actually suggests a simple heuristic for selecting \(\mathcal{J}\): when \(\exp(\left\langle\left[\mathbb{P}\right]_{i},\mathbf{c}_{x}^{s}\right\rangle)\) is large, we should approximate the \(x\)-th \(s\)-length segment of \(\mathbf{C}\) with higher resolution (by splitting the segment to shorter sub-segments and using finer approximation). This heuristic describes the selection criteria for one row of \(\exp(\mathbf{PC}^{\top})\), which corresponds to a single VIP-token, for multiple rows of \(\exp(\mathbf{PC}^{\top})\) (for multiple VIP-tokens), we simply use

\[\mu_{x}^{s}=\sum_{i=1}^{n_{p}}\exp(\left\langle\left[\mathbb{P}\right]_{i}, \mathbf{c}_{x}^{s}\right\rangle)\] (53)

as selection criteria since \(\mathcal{J}\) is shared by all VIP-tokens.

The construction of \(\mathcal{J}\) is described in Alg. 1. This algorithm describes the same procedure as the Figure 3 in the main text. The \(\mathbf{b}_{x}^{s}\)'s in \(\mathcal{J}\) are the rows of \(\mathbf{S}_{c}\), and the corresponding \(\mathbf{c}_{x}^{s}\)'s (40) are the rows of \(\mathbf{S}_{c}\mathbf{C}\). The budgets \(h_{2},h_{4},\cdots,h_{n_{c}}\) required by Alg. 1 is used determine the number of components at each resolution that will be added to \(\mathcal{J}\). Specifically, there are \(2h_{2s}-h_{s}\) number of components \(\mathbf{b}_{x}^{s}\) for \(s\neq 1\) based on simple calculations. We can choose budgets such that the final size of \(\mathcal{J}\) is \(r-n_{p}\) to make the length of compressed sequence to be \(r\).

#### 7.2.5 How Good is This Approximation?

At high level, the compression \(\mathbf{S}_{c}\) performs more compression on tokens that are not relevant to the VIP-tokens and less compression to tokens that are important to the VIP-tokens. We will discuss it in more details. Since each row of \(S^{\dagger}\) contain exactly a \(1\) as stated in Claim 7.3, \(S^{\dagger}\) can commute with \(\beta\), so in summary, we can write the approximation of the computation of a Transformer layer as

\[\alpha(\mathbf{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X}) =\exp(\mathbb{P}\mathbf{P}^{\top})\mathbb{P}+\exp(\mathbf{PC}^{ \top}\mathbf{S}_{c}^{\top})\mathbf{D}\mathbf{S}_{c}\mathbf{C}\] (54) \[\mathbf{S}_{c}^{\dagger}\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S }\mathbf{X},\mathbf{S}\mathbf{X}) =\mathbf{S}_{c}^{\dagger}\exp(\mathbf{S}_{c}\mathbf{CP}^{\top}) \mathbb{P}+\mathbf{S}_{c}^{\dagger}\exp(\mathbf{S}_{c}\mathbf{CC}^{\top} \mathbf{S}_{c}^{\top})\mathbf{D}\mathbf{S}_{c}\mathbf{C}\] \[\begin{bmatrix}\mathbb{P}_{new}\\ \mathbf{C}_{new}\end{bmatrix} =\begin{bmatrix}\beta(\alpha(\mathbf{P},\mathbf{S}\mathbf{X}, \mathbf{S}\mathbf{X})+\mathbb{P})+\alpha(\mathbf{P},\mathbf{S}\mathbf{X}, \mathbf{S}\mathbf{X})\\ \beta(\mathbf{S}_{c}^{\dagger}\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S} \mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}^{\dagger}\mathbf{S}_{c} \mathbf{C})+\mathbf{S}_{c}^{\dagger}\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S }\mathbf{X},\mathbf{S}\mathbf{X})\end{bmatrix}+\begin{bmatrix}\mathbb{P}\\ \mathbf{C}\end{bmatrix}\]

Note that \(\mathbf{D}\) is added as discussed in (50).

There are four main approximation components (purple) in (54). Taking the fact that \(\mathbf{D}\mathbf{S}_{c}=(\mathbf{S}_{c}^{\dagger})^{\top}\), all of these approximations are row or column space multi-resolution approximations governed by \(\mathbf{S}_{c}\) matrix. High attention weight implies higher dependency, and the procedure in Alg. 1 refines regions with large attention weights with higher resolutions. Therefore, the token embedding in \(\mathbf{C}\) that have higher dependency to \(\mathbb{P}\) are better approximated. The output \(\mathbb{P}_{new}\) is well approximated by design since the approximation preserves the higher frequency components of the subset of rows of \(\mathbf{C}\) that has high impact on the output \(\mathbf{P}_{ncw}\). Further, the output in \(\mathbf{C}_{new}\) corresponding to the subset of rows of \(\mathbf{C}\) that have higher dependency with the VIP-tokens will have better approximation than the remaining rows of \(\mathbf{C}\). This property addresses the issue that some tokens with unknown locations are also relevant to the final prediction of a Transformer in some tasks. For example, in question answering tasks, candidate answers are usually expected to have large dependency with question tokens (VIP-tokens), so they are approximated well as well. This approximation property is exactly what we need.

#### 7.2.6 Relation to [39] that Inspires Multi-Resolution Compression

Our work and [39] can be viewed as operating at slightly different levels of abstractions. While [39] tries to approximate self-attention computation efficiently, our paper proposes a general framework for performing a VIP-token centric compression on the sequence to efficiently handle extremely long sequences (the self-attention module remains completely unchanged). Our VIP-token centric compression involves a number of steps described in the main text. But one of the key steps involves constructing a compression matrix \(\mathbf{S}_{c}\) which has some desirable properties, namely satisfying (19) which we elaborate further below.

Note that for equation (19), we need a matrix \(\mathbf{S}_{c}\) such that the approximated attention matrix involving \(\mathbf{P}\) and \(\mathbf{C}\) is similar to the true attention matrix involving \(\mathbf{P}\) and \(\mathbf{C}\). This is precisely where the general idea of [39] can be used. But the formulation in [39] cannot be applied directly in its original form since it cannot give us \(\mathbf{S}_{c}\). Why? One reason is that the formulation in [39] cannot be written as matrix form similar to equation (19). This may be a reason why [39] has to use custom CUDA kernels in their implementation. Nonetheless, the properties of [39] are useful. So we derive the analogous form but for 1D instead: this 1D case is expressed as applying a matrix (this is the \(\mathbf{S}_{c}\) we are looking for) to the signal \(\mathbf{C}\).

One bonus of this modification is that it also removes the need for custom CUDA kernels. At a high level, [39] offers a multi-resolution view of the self-attention matrices, and our modified version is best thought of as a similar multi-resolution view of the sequence itself. But we can also substitute in a different means of obtaining \(\mathbf{S}_{c}\) (which could simply be a sketching matrix). Finally, we note that a naive implementation of the resulting modification still requires a \(O(n_{c}d)\) cost due to the computation of \(\mathbf{c}_{x}^{s}\) for all possible scaling \(s\) and translation \(x\). There is a similar cost in [39] (second paragraph in section 4.4 in [39]). The data structure we propose reduces this cost.

### Details of Proposed Data Structure

In section, we describe some omitted technical details of the proposed data structure \(\mathcal{T}(\cdot)\).

3.1 Why \((\mathbf{c}_{new})_{1}^{1}-\mathbf{c}_{1}^{1}=(\mathbf{c}_{new})_{2}^{1}- \mathbf{c}_{2}^{1}=(\mathbf{c}_{new})_{1}^{2}-\mathbf{c}_{1}^{2}\) if \(\mathbf{S}_{c}\mathbf{C}=\begin{bmatrix}\mathbf{c}_{1}^{2}&\mathbf{c}_{3}^{1 }&\mathbf{c}_{4}^{1}&\mathbf{c}_{2}^{1}\end{bmatrix}^{\top}\)?

**Claim 7.4**.: _Given the set \(\mathcal{J}\) satisfying restriction (25), if \(\mathbf{b}_{x}^{s}\in\mathcal{J}\), then \((\mathbf{c}_{new})_{x}^{s}-\mathbf{c}_{x}^{s}=(\mathbf{c}_{new})_{x^{\prime}} ^{s^{\prime}}-\mathbf{c}_{x^{\prime}}^{s^{\prime}}\) for all \(\mathbf{b}_{x^{\prime}}^{s^{\prime}}\) satisfying the support of \(\mathbf{b}_{x^{\prime}}^{s^{\prime}}\) is contained in the support of \(\mathbf{b}_{x}^{s}\) (the \(x^{\prime}\)-th \(s^{\prime}\)-length segment of \(\mathbf{C}\) is a sub-segment of the \(x\)-th \(s\)-length segment of \(\mathbf{C}\))._

Proof.: To simplify the notations a bit, without loss of generality, we assume \(x=1\). Then, for \(i\leq s\), consider \((\mathbf{c}_{new})_{i}^{1}\):

\[\begin{split}(\mathbf{c}_{new})_{i}^{1}=\left[\mathbf{C}_{new} \right]_{i}&=\left[\mathbf{S}_{c}^{\dagger}\beta(\alpha(\mathbf{S}_{ c}\mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c} \mathbf{C})+\mathbf{S}_{c}^{\dagger}\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{ S}\mathbf{X},\mathbf{S}\mathbf{X})\right]_{i}+[\mathbf{C}]_{i}\\ &=\left[\mathbf{S}_{c}^{\dagger}\right]_{i}\beta(\alpha(\mathbf{S }_{c}\mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c} \mathbf{C})+\left[\mathbf{S}_{c}^{\dagger}\right]_{i}\alpha(\mathbf{S}_{c} \mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{c}_{i}^{1}\end{split}\] (55)

By Claim 7.3, \(\mathbf{S}_{c}^{\dagger}=\mathbf{S}_{c}^{\top}\mathbf{D}\) and \(i\)-th row of \(\mathbf{S}_{c}^{\dagger}\) contains exactly a \(1\). The column that contains \(1\) in the \(i\)-th row of \(\mathbf{S}_{c}^{\dagger}\) is exactly \(s\mathbf{b}_{i}^{1}\) since \(i\) is contained in the support of exactly one components in \(\mathcal{J}\) due to the restriction (25). Denote this column index as \(j\), then

\[(\mathbf{c}_{new})_{i}^{1}=\left[\beta(\alpha(\mathbf{S}_{c}\mathbf{C}, \mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C})\right]_{ j}+\left[\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S} \mathbf{X})\right]_{j}+\mathbf{c}_{i}^{1}\] (56)

Note that this holds for all \(i\leq s\). As a result, for \(i,i^{\prime}\leq s\),

\[(\mathbf{c}_{new})_{i}^{1}-\mathbf{c}_{i}^{1}=\left[\beta(\alpha(\mathbf{S}_{ c}\mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C}) \right]_{j}+\left[\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X}, \mathbf{S}\mathbf{X})\right]_{j}=(\mathbf{c}_{new})_{i^{\prime}}^{1}-\mathbf{ c}_{i^{\prime}}^{1}\] (57)Then,

\[\begin{split}(\mathbf{c}_{new})_{\lceil i/2\rceil}^{2}-\mathbf{c}_{ \lceil i/2\rceil}^{2}&=\frac{1}{2}(\mathbf{c}_{new})_{2\lceil i/2 \rceil-1}^{1}+\frac{1}{2}(\mathbf{c}_{new})_{2\lceil i/2\rceil}^{1}-\frac{1}{ 2}\mathbf{c}_{2\lceil i/2\rceil-1}^{1}-\frac{1}{2}\mathbf{c}_{2\lceil i/2 \rceil}^{1}\\ &=\frac{1}{2}((\mathbf{c}_{new})_{2\lceil i/2\rceil-1}^{1}- \mathbf{c}_{2\lceil i/2\rceil-1}^{1})+\frac{1}{2}((\mathbf{c}_{new})_{2\lceil i /2\rceil}^{1}-\mathbf{c}_{2\lceil i/2\rceil}^{1})\\ &=(\mathbf{c}_{new})_{2\lceil i/2\rceil-1}^{1}-\mathbf{c}_{2 \lceil i/2\rceil-1}^{1}\end{split}\] (58)

The rest follows from induction.

3.2 How do we get \((\mathbf{c}_{new})_{1}^{2},(\mathbf{c}_{new})_{3}^{1},(\mathbf{c}_{new})_{4}^{ 1},(\mathbf{c}_{new})_{2}^{4}\)If \(\mathbf{S}_{c}\mathbf{C}=\begin{bmatrix}\mathbf{c}_{1}^{2}&\mathbf{c}_{3}^{1}& \mathbf{c}_{4}^{1}&\mathbf{c}_{2}^{4}\end{bmatrix}^{\top}\mathbf{?}\)

**Claim 7.5**.: _We have_

\[\mathbf{S}_{c}\mathbf{C}_{new}=\beta(\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{ S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C})+\alpha(\mathbf{S}_{c} \mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C}.\] (59)

_And the updated representation \((\mathbf{c}_{new})_{x}^{s}\) of the corresponding \(\mathbf{c}_{x}^{s}\) (a row of \(\mathbf{S}_{c}\mathbf{C}\)) is the corresponding row of \(\mathbf{S}_{c}\mathbf{C}_{new}\)._

Proof.: By definition,

\[\mathbf{C}_{new}=\mathbf{S}_{c}^{\dagger}\beta(\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C})+ \mathbf{S}_{c}^{\dagger}\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X}, \mathbf{S}\mathbf{X})+\mathbf{C}\] (60)

Then,

\[\begin{split}\mathbf{S}_{c}\mathbf{C}_{new}&= \mathbf{S}_{c}\mathbf{S}_{c}^{\dagger}\beta(\alpha(\mathbf{S}_{c}\mathbf{C}, \mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C})+\mathbf{ S}_{c}\mathbf{S}_{c}^{\dagger}\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X}, \mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C}\\ &=\beta(\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X}, \mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C})+\alpha(\mathbf{S}_{c}\mathbf{ C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C}\end{split}\] (61)

Since the \(\mathbf{S}_{c}\) is the same for \(\mathbf{S}_{c}\mathbf{C}_{new}\) and \(\mathbf{S}_{c}\mathbf{C}\), the second statement follows. 

#### 7.3.3 Algorithm for Making \(\mathcal{T}(\mathbf{C})\) into \(\mathcal{T}(\mathbf{C}_{new})\)

In this section, we describe the exact algorithm to update \(\mathcal{T}(\mathbf{C})\) into \(\mathcal{T}(\mathbf{C}_{new})\). The pseudo code is described in Alg. 2 where \(\mathbf{c}_{x}^{s}\) is computed via

\[\mathbf{c}_{x}^{s}=\mathbf{c}_{\lceil x/2\rceil}^{2s}-\Delta\mathbf{c}_{x}^{s }=\mathbf{c}_{\lceil x/4\rceil}^{4s}-\Delta\mathbf{c}_{\lceil x/2\rceil}^{2s} -\Delta\mathbf{c}_{x}^{s}=\cdots\] (63)

We use the term "dirty" in Alg. 2 to indicate the node needs to be handled due to node updates. This term is commonly used in computer cache implementations to indicate that the data of a specific location has been updated and needs to be accounted for.

### Complexity Analysis

In this section, we will discuss the detailed complexity analysis of our proposed method. The overall complexity of our proposed method is \(\mathcal{O}(lrd^{2}+lr^{2}d+lr\log(n_{c})d+lrn_{p}d+nd)\) when using the proposed efficient data structure.

#### 7.4.1 Preparing Input Sequence to \(\mathcal{T}(\mathbf{C})\): \(\mathcal{O}(nd)\)

At the first layer, we need to permute the rows of \(\mathbf{X}\) into \([\mathrm{P};\mathbf{C}]\), which takes \(\mathcal{O}(nd)\) cost. Then, we process \(\mathbf{C}\) into \(\mathcal{T}(\mathbf{C})\). This requires **(1)** computing \(\mathbf{c}_{x}^{s}\) defined in (41). \(\mathbf{c}_{x}^{1}=\left[\mathbf{C}\right]_{x}\), so no compute is needed. With all \(\mathbf{c}_{x}^{1}\) given, computing all \(\mathbf{c}_{x}^{2}\) takes \(\mathcal{O}(n_{c}d/2)\). With all \(\mathbf{c}_{x}^{2}\) given, computing all \(\mathbf{c}_{x}^{4}\) takes \(\mathcal{O}(n_{c}d/4)...\) So, the cost is

\[\mathcal{O}(n_{c}d/2+n_{c}d/4+\cdots+d)=\mathcal{O}(n_{c}d).\] (64)

Then **(2)** computing \(\Delta\mathbf{c}_{x}^{s}\) for all \(s\) and \(x\). Computing each \(\Delta\mathbf{c}_{x}^{s}\) takes \(\mathcal{O}(d)\) when given \(\mathbf{c}_{x}^{s}\) and \(\mathbf{c}_{[x/2]}^{2s}\). The amount of cost is the same as the number of nodes in the tree \(\mathcal{T}(\mathbf{C})\), so the cost is \(\mathcal{O}(n_{c}d)\). Note that \(n_{c}<n\), so the overall complexity of the above operations is \(\mathcal{O}(nd)\).

4.2 Constructing \(\mathcal{J},\mathbf{S}_{c},\mathbf{S}_{c}\): \(\mathcal{O}(lr\log(n_{c})d+lrn_{p}d)\)

We can analyze the complexity of constructing \(\mathcal{J}\) using Algo. 1. There is only one possible \(\mu_{x}^{n_{c}}\). Then for each \(s\), there are \(2h_{s}\) number of \(\mu_{x}^{s/2}\) being computed since there are \(2\) components \(\mathbf{b}_{x}^{s/2}\) for each \(\mathbf{b}_{x^{\prime}}^{s}\). As a result, we need to compute \(\mathcal{O}(1+\sum_{s}2h_{s})\) number of \(\mu_{x}^{s/2}\). When \(\mathbf{c}_{x}^{s/2}\) is given, the cost of computing a \(\mu_{x}^{s/2}\) is \(\mathcal{O}(n_{p}d)\), so the overall cost of constructing \(\mathcal{J}\) is \(\mathcal{O}((1+\sum_{s}2h_{s})n_{p}d)\).

Further, at each \(s\), the size of \(\mathcal{J}\) is increased by \(h_{s}\) since \(h_{s}\) segments are split into \(2h_{s}\) sub-segments, so the size of \(\mathcal{J}\) is \(\mathcal{O}(\sum_{s}h_{s})\). Since \(\mathbf{S}_{c}\in\mathbb{R}^{(r-n_{p})\times n}\) and \(|\mathcal{J}|=r-n_{p}\) as discussed in SS7.2.4, \(\mathcal{O}(r-n_{p})=\mathcal{O}(\sum_{s}h_{s})\). We use \(\mathcal{O}(r)\) for simplicity instead of \(\mathcal{O}(r-n_{p})\). As a result, the overall cost of constructing \(\mathcal{J}\) is \(\mathcal{O}(rn_{p}d)\).

The above cost assumes \(\mathbf{c}_{x}^{s/2}\) is given. If we compute all possible \(\mathbf{c}_{x}^{s/2}\) using (41), the cost will be \(\mathcal{O}(n_{c}d)\) as analyzed in SS7.4.1. However, if we employ the proposed data structure, each \(\mathbf{c}_{x}^{s/2}\) can be retrieved in at most \(\mathcal{O}(\log(n_{c})d)\) by recursively computing (63). Since we need to retrieve \(\mathcal{O}(1+\sum_{s}2h_{s})=\mathcal{O}(r)\) number of \(\mathbf{c}_{x}^{s}\), the complexity of computing necessary \(\mathbf{c}_{x}^{s}\) is \(\mathcal{O}(r\log(n_{c})d)\).

As a result, the complexity of constructing \(\mathcal{J}\) is \(\mathcal{O}(rn_{p}d+r\log(n_{c})d)\) at each layer. When summing the cost over all layers, the complexity is \(\mathcal{O}(lrn_{p}d+lr\log(n_{c})d)\).

By Claim 7.2, the rows of \(\mathbf{S}_{c}\) and \(\mathbf{S}_{c}\mathbf{C}\) are simply the \(\mathbf{b}_{x}^{s}\in\mathcal{J}\) and the corresponding \(\mathbf{c}_{x}^{s}\), which are already computed during the construction of \(\mathcal{J}\), so we essentially can get these \(\mathbf{S}_{c}\) and \(\mathbf{S}_{c}\mathbf{C}\) for free.

#### 7.4.3 Feeding Compressed Sequence into a Transformer Layer: \(\mathcal{O}(lrd^{2}+lr^{2}d)\)

At each layer, we need to compute

\[\begin{bmatrix}\mathrm{P}_{new}\\ \mathbf{S}_{c}\mathbf{C}_{new}\end{bmatrix}=\begin{bmatrix}\beta(\alpha( \mathrm{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathrm{P})+\alpha( \mathrm{P},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathrm{P}\\ \beta(\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X}) +\mathbf{S}_{c}\mathbf{C})+\alpha(\mathbf{S}_{c}\mathbf{C},\mathbf{S}\mathbf{X},\mathbf{S}\mathbf{X})+\mathbf{S}_{c}\mathbf{C}\end{bmatrix}\] (65)

for updating \(\mathcal{T}(\mathbf{C})\). This is the part of a Transformer layer that requires heavy computation. It can be verified that the complexity of a Transformer layer is \(\mathcal{O}(nd^{2}+n^{2}d)\) for a input sequence of length \(n\). Now a compressed sequence of length \(r\) is fed into a Transformer layer, the cost is simply \(\mathcal{O}(rd^{2}+r^{2}d)\). We note that there is an additional re-scaling to plug \(\mathbf{D}\) into \(\exp(\mathrm{P}\mathbf{C}^{\top}\mathbf{S}_{c}^{\top})\mathbf{D}\mathbf{S}_{c}\) during multi-head attention computation discussed in 50. However, the additional cost of applying \(\mathbf{D}\) is \(\mathcal{O}(rd)\), which does not change the complexity. When summing the cost of all layers, the overall complexity is \(\mathcal{O}(lrd^{2}+lr^{2}d)\).

4.4 Updating \(\mathcal{T}(\mathbf{C})\) into \(\mathcal{T}(\mathbf{C}_{new})\): \(\mathcal{O}(lrd)\)

Once (65) is computed, we need to change \(\mathcal{T}(\mathbf{C})\) into \(\mathcal{T}(\mathbf{C}_{new})\). The cost of change \(\mathcal{T}(\mathbf{C})\) into \(\mathcal{T}(\mathbf{C}_{new})\) is \(\mathcal{O}(rd)\) as analyzed in the main text. For more specific analysis, let us take a look at the first three iterations:1. At the first iteration, there are \(\mathcal{O}(2h_{2})\) number of \((\mathbf{c}_{new})_{x}^{1}\) to be computed at the first inner for loop, and there are \(\mathcal{O}(2h_{2})\) number of \(\Delta(\mathbf{c}_{new})_{x}^{1}\) to be updated in the second inner for loop. Additional \(\mathcal{O}(h_{2})\) number of \(\Delta(\mathbf{c}_{new})_{x/2}^{2}\) are masked dirty.
2. At the second iteration, there are \(\mathcal{O}(2h_{4})\) number of \((\mathbf{c}_{new})_{x}^{2}\) to be computed at the first inner for loop, and there are \(\mathcal{O}(2h_{4}+h_{2})\) number of \(\Delta(\mathbf{c}_{new})_{x}^{2}\) to be updated in the second inner for loop. The second term is due to the dirty \(\Delta(\mathbf{c}_{new})_{\lceil x/2\rceil}^{2}\) from the first iteration. Additional \(\mathcal{O}(h_{4}+\frac{h_{2}}{2})\) number of \(\Delta(\mathbf{c}_{new})_{\lceil x/2\rceil}^{4}\) are masked dirty.
3. At the third iteration, there are \(\mathcal{O}(2h_{8})\) number of \((\mathbf{c}_{new})_{x}^{4}\) to be computed at the first inner for loop, and there are \(\mathcal{O}(2h_{8}+h_{4}+\frac{h_{2}}{2})\) number of \(\Delta(\mathbf{c}_{new})_{x}^{4}\) to be updated in the second inner for loop. The second and third term is due to the dirty \(\Delta(\mathbf{c}_{new})_{\lceil x/2\rceil}^{4}\) from the second iteration. Additional \(\mathcal{O}(h_{8}+\frac{h_{4}}{2}+\frac{h_{2}}{4})\) number of \(\Delta(\mathbf{c}_{new})_{\lceil x/2\rceil}^{8}\) are masked dirty.

It becomes apparent that if we sum over the number of computes of \((\mathbf{c}_{new})_{x}^{s}\) and updates of \(\Delta(\mathbf{c}_{new})_{x}^{s}\), the total number is \(\mathcal{O}(\sum_{s}2h_{s}+2\sum_{s}\sum_{j=1}^{\log(s)}\frac{h_{s}}{2^{j}})= \mathcal{O}(\sum_{s}h_{s}+\sum_{s}h_{s})=\mathcal{O}(r)\). Since each compute and update takes \(\mathcal{O}(d)\) cost, the overall complexity of changing \(\mathcal{T}(\mathbf{C})\) into \(\mathcal{T}(\mathbf{C}_{new})\) is \(\mathcal{O}(rd)\). When summing the cost of all layers, the overall complexity is \(\mathcal{O}(lrd)\).

4.5 Materializing \(C_{new}\) from \(\mathcal{T}(\mathbf{C}_{new})\) at the Last Layer: \(\mathcal{O}(nd)\)

At the output of the last layer, we can **(1)** compute all \((\mathbf{c}_{new})_{x}^{n_{c}/2}\) via (63) at a cost of \(\mathcal{O}(2d)\), **(2)** compute \((\mathbf{c}_{new})_{x}^{n_{c}/4}\) via (63) at a cost of \(\mathcal{O}(4d)\)... until all \((\mathbf{c}_{new})_{x}^{1}\) are computed. Then, \(\left[\mathbf{C}_{new}\right]_{x}=c_{x}^{1}\) is materialized from \(\mathcal{T}(\mathbf{C}_{new})\) at a total cost of

\[\mathcal{O}(d+2d+4d+\cdots+n_{c}d)=\mathcal{O}(n_{c}d).\] (66)

Lastly, undoing the permutation so that \(\left[\mathbb{P}_{ncw};\mathbf{C}_{new}\right]\) are re-ordered to the original positions has a complexity of \(\mathcal{O}(nd)\). As a result, the overall complexity is \(\mathcal{O}(nd)\).

#### 7.4.6 Overall Complexity

In summary, the overall complexity of our method is

\[\mathcal{O}(lrd^{2}+lr^{2}d+lr\log(n_{c})d+lrn_{p}d+nd)\] (67)

### Experiments

We run all experiments on NVIDIA A100 GPUs. All code is implemented using the standard PyTorch framework. No custom CUDA kernels are needed. As a result, it can be easily deployed to other platforms or ML frameworks. We will publish all code and checkpoints necessary for reproducibility concurrently with the paper publication.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline  & \multicolumn{3}{c|}{RoBERTa} & \multicolumn{3}{c}{T5} \\ \cline{3-8} Percentile & HotpotQA & QuALITY & WikHop & WikHop & HotpotQA & Qasper & QuALITY & ContractNLI \\ \hline
75th & 1535 & 7603 & 2204 & 2399 / 6 & 1692 / 6 & 7029 / 29 & 7747 / 17 & 2991 / 4 \\
95th & 1928 & 8495 & 3861 & 4206 / 9 & 2129 / 10 & 10920 / 71 & 8603 / 28 & 5061 / 4 \\ \hline \hline  & \multicolumn{3}{c|}{T5} & \multicolumn{3}{c}{T5} \\ \cline{3-8} Percentile & NaratrixQA & CNN/Dailymail & MediaSum & Arxiv & SummScreenFD & GovReport & QMSum & MultiNews \\ \hline
75th & 90482 / 10 & 1242 / 87 & 2621 / 29 & 13477 / 364 & 12119 / 188 & 13304 / 811 & 19988 / 110 & 3032 / 379 \\
95th & 260533 / 18 & 1946 / 130 & 5061 / 64 & 26024 / 759 & 16722 / 330 & 23795 / 983 & 31749 / 162 & 6676 / 468 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Length statistics of each dataset. The values are the percentiles of number of tokens for the specific tokenizers. For T5 tokenizer, the left value of is for sequence lengths of encoder input, and the right value is for sequence lengths of decoder input.

#### 7.5.1 Runtime Measurement

The runtimes presented in the experiment section are measured runtimes of a complete training step (including both forward and backward). For each method, we use the largest batch size that can fit into a 80GB A100 and measure the average latency of 10 steps. Then, the average latency is divided by the batch size to get the estimated runtime for a single instance. Via this procedure, we seek to measure the peak efficiency of each method when the GPU is at the highest possible utilization.

#### 7.5.2 FLOPs

Some might be interested about the FLOP efficiency of each method, so we provide FLOP profiling results for some experiments. The automatic tools that we used for calculating FLOPs is deepspeed's FlopsProfiler. We note that the FLOP profiling tools do not always work and may throw errors or produce incorrect results when the code contains custom CUDA kernels. The results are summarized in Tab. 8 and Tab. 9. \(\times\) in some entries means the profiler throws errors and cannot estimate the FLOPs numbers. Also, the profiler throws errors when we just profile the encoders of encoder-decoder models, so the FLOP numbers in Tab. 9 are for the entire models. As shown in the tables, our method has the lowest FLOPs.

However, we note that FLOP numbers do not always reflect practical latency reductions. Memory bandwidth and latency (which are not captured by FLOP numbers) also play an important role in the overall latency. For example, sparse matrix multiplication (with unstructured sparsity) usually has a much lower FLOP count than dense matrix multiplication. But, the latency reduction is only possible when sparsity is at least 95% or more (depending on the implementation) since sparse matrix multiplication is a memory bandwidth bounded operator.

#### 7.5.3 Pretraining

We use a filtered The Pile dataset [13] for all pretrainings. Since we are using public pretrained tokenizers, we want to enable the distribution of pretraining corpus aligns well with the distribution of corpus used to create the tokenizers. As a result, we use tokens per byte as a proxy for alignment of distributions and filter out PubMed Central, ArXiv, Github, StackExchange, DM Mathematics [28],

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline Method & Size & Length & \multicolumn{3}{c}{HotepepQA} & \multicolumn{3}{c}{QuALITY} & \multicolumn{3}{c}{WikiHop} \\  & & Runtime & EM & F1 & Runtime & Accuracy & Runtime & Accuracy \\ \hline RoBERTa & base & 512 & 19.9 & 35.1 & 44.9 & 21.2 & 39.0 & 19.6 & 67.6 \\ RoBERTa & base & 4k & 422.3 & 62.2 & 76.1 & 40.2 & 39.5 & 414.1 & 75.2 \\ Big Bird & base & 4k & 297.9 & 59.5 & 73.2 & 307.0 & 38.5 & 293.3 & 74.5 \\ Longformer & base & 4k & 371.0 & 59.9 & 73.6 & 368.0 & 27.9 & 369.7 & 74.3 \\ Ours & base & 4k & 114.6 & 60.9 & 74.6 & 126.4 & 39.6 & 108.0 & 75.9 \\ Ours-15QA & base & 4k & 114.6 & 60.7 & 74.1 & 126.4 & 39.4 & 108.0 & 76.1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Dev set results for encoder-only models fitting on HotpotQA, QuALITY, and WikiHop.

\begin{table}
\begin{tabular}{l l l c c c} \hline \hline Method & Size & Length & \multicolumn{3}{c}{WikiHop} \\  & & Runtime & GFLOPs & EM & F1 \\ \hline T5 & base & 512 & 25.7 / 20.5 & 120 & 66.7 & 69.1 \\ T5 & base & 4k & 594.3 / 553.7 & 1449 & 76.2 & 78.1 \\ LongT5 & base & 4k & 207.3 / 233.9 & 948 & 72.7 & 74.8 \\ LSD & base & 4k & 236.6 / 222.9 & \(\times\) & 70.0 & 72.4 \\ Ours & base & 4k & 181.7 / 148.1 & 663 & 76.7 & 78.4 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Dev accuracy and FLOPs for encoder-only models. \({}^{*}\): some calculations are NOT captured by profiler, so value is underestimated.

\begin{table}
\begin{tabular}{l l l c c c} \hline \hline Method & Size & Length & \multicolumn{3}{c}{WikiHop} \\  & & Runtime & GFLOPs & EM & F1 \\ \hline T5 & base & 512 & 25.7 / 20.5 & 120 & 66.7 & 69.1 \\ T5 & base & 4k & 594.3 / 553.7 & 1449 & 76.2 & 78.1 \\ LongT5 & base & 4k & 207.3 / 233.9 & 948 & 72.7 & 74.8 \\ LSD & base & 4k & 236.6 / 222.9 & \(\times\) & 70.0 & 72.4 \\ Ours & base & 4k & 181.7 / 148.1 & 663 & 76.7 & 78.4 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Dev results and FLOPs for encoder-decoder models.

Ubuntu IRC, EuroParl [20], YoutubeSubtitles, and Enron Emails [18] components, which have tokens per byte greater than \(0.3\). Then, the remaining corpus of The Pile dataset is used for pretraining.

For encoder-only models, we pretrain RoBERTa for 750K steps. A batch consists of 8,192 sequences of 512 length. The masking ratio for masked language modeling (MLM) is 15%. Then, 4K length models are continuously pre-trained from the RoBERTa checkpoints for 300k steps. The positional embeddings are extended by duplicating the pretrained 512 positional embedding multiple times. For 4K length RoBERTa, Longformer, Big Bird and MRA Attention, the batch size is 64, and the masking ratio is 15%. With 15% masking ratio, there are roughly 616 masked tokens scattering in the sequences. We find that using 616 scattered masked tokens as VIP tokens for 4,096 length sequences might not be indicative for VIP-token centric compression, so we use masking ratio 7.5% and batch size 128 for our method. The number of masked tokens per sequence is reduced, and the number of total masked token predictions remains the same during pretraining. We note that with larger batch size, the wall clock pretraining runtime for our method is still smaller than baselines. In case that anyone is interested, we also show downstream finetuning on our method pretrained on the same number of tokens but fewer number of masked token predictions in Tab. 7 and Fig. 8, denoted as Ours-150K. The accuracy is consistent with our model pretrained on 300k steps. For the larger scale pretraining denoted with *, we pretrain our method for 250K steps with batch size 512 and masking ratio 7.5%.

For encoder-decoder architecture of our method, we do continuous pretraining from the public checkpoints of T5 for 250K steps with batch size 256 using the masked span prediction. Since each masked span (consists of multiple tokens) is replaced by a single special token, when using masking ratio is 15%, the number of special tokens in a sequence is not too large, we keep masking ratio 15% unchanged.

#### 7.5.4 Downstream Finetuning

The statistics of the sequence lengths of instances in each dataset are summarized in Tab. 6. The hyperparameters of all experiments are summarized in Tab 11. When there are multiple values in an entry, it means we perform a hyperparameter search on these values. The amount of search is determined by the size of datasets. If a dataset is relatively large, we only search the learning rate. If a dataset is small, we include batch size and the number of epochs in search. For all tasks, if the sequence lengths are longer than the model length \(m\), the sequences will be truncated and only the first \(m\) tokens will be used. For encoder-decoder models, we use greedy decoding in sequence generations for simplicity. The maximal decoder output length, specified in Tab. 11, is set such that the maximal length covers the output lengths of more than 99% of instances. When the length of covering 99% of instances is greater than 512, we just set the maximal decoder output length to 512. Additionally, we show one extra experiment on MultiNews [12] in Tab. 10, which is not in the main text due to space limit.

#### 7.5.5 Non-Language Tasks

While the focus of this work is on language tasks, the overall design does not limit its application to other tasks as long as the assumption ("a subset of tokens are disproportionately responsible for

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Method & Size & \# Pharm & Length & \multicolumn{3}{c}{MultiNews} \\  & & & Runtime & R-1 & R-2 & R-L \\ \hline T5 & base & 223M & 512 & 59.2 / 20.5 & 42.5 & 15.3 & 30.0 \\ TS & base & 223M & 4K & 651.2 / 551.8 & 46.4 & 18.2 & 42.6 \\ LongTS & base & 248M & 8K & 721.7 / 550.6 & 46.7 & 18.3 & 42.9 \\ LED & base & 162M & 8K & 535.5 / 454.2 & 46.6 & 17.8 & 42.7 \\ \hline
**Ours** & base & 223M & 3K & 370.7 / 224.6 & 46.4 & 18.1 & 42.7 \\ \hline T5 & large & 738M & 512 & 108.6 / 67.0 & 43.4 & 15.6 & 39.8 \\
**Ours** & **large** & 738M & **8K** & 1103.7 / 561.5 & 48.2 & 19.2 & 44.2 \\ \hline \hline Ours & 30 & 30 & 30 & 3K & 4094.5 / 2096.0 & 48.9 & 19.4 & 44.7 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Dev results for encoder-decoder models on MultiNews.

Figure 8: Model runtime vs WikiHop dev accuracy when using different model specific hyperparameters

[MISSING_PAGE_FAIL:26]

difficult to compress the token that is being generated with neighboring tokens. It is in fact challenging to reduce **the cost of FFN** since every generated token will most likely need a full calculation of FFN when it is being generated.

Having said that, there are still clear opportunities for decoder-only models, which are left as our future work. We briefly describe three possible options to do so. **(1)** We can use the input tokens of the decoder as VIP-tokens to compress the representations of context sequence generated by the encoder before Cross Attention computation to reduce the cost of Cross Attention. **(2)** Auto-regressive decoding operates using Causal Attention at each step. This Causal Attention operation requires memory and computation that is linear in the length of the prefix. We can keep the same Causal Attention VIP-token (the representation of the token currently being generated) and apply our method to compress the representations of the previously generated tokens. This reduces the linear complexity of the Causal Attention operation to sublinear. This is useful for reducing the cost of inference. For training, we can break the sequence into two segments: prefix segment and decoding segment. Then, we can use the proposed compression in prefix segment and vanilla computation in decoding segment. To prevent look ahead to the future tokens, we might only use the first token in the decoding segment as VIP-token. **(3)** In many cases, the current large language models (LLMs) are not used directly to generate an ultra long text. Rather, the requirements for processing ultra longer context manifests due to the need to incorporate user input and previous LLM responses (like ChatGPT) or to incorporate search results (such as New Bing). In these cases, there is a prefix context, and LLMs will generate text based on the user prompt and prefix context. We note that our method can indeed be applied to compress the prefix context, and the user prompt and currently generated tokens will be the VIP-tokens.

### Practical Questions

#### How is the T5's relative positional encoding handled in our method?

To compute the relative positional bias for self-attention in T5, the position indices of queries and keys are needed (to calculate the position distance between each query and key). After applying our method, the input to the T5 block is the compressed sequence (some tokens are compressed while some tokens remain uncompressed). For each uncompressed token, the position index remains unchanged as its true position in the original sequence. For each new token that represents a compressed segment, the position index is the floored average of position indices of tokens in the segment. In this way, we can minimize the amount of modification needed to the internals of the T5 block, and the relative attention bias for the compressed sequence is an approximation of the attention bias for the original sequence.

#### VIP-token selection is task dependent, how to do this selection?

VIP-token selection requires some understanding of the tasks. However, we believe that a reasonable selection can be made with some generic knowledge for most tasks or use cases. For example, for question answering tasks, we just use questions as VIP-tokens. For classification tasks (for example, Long Range Arane benchmark shown in Appendix), we simply use CLS token as the VIP-token since only CLS token is used for final prediction. For masked language modeling, we use the masked tokens as the VIP-tokens since only these masked tokens are used for final prediction. For question answering, the question tokens (and candidate tokens for multi-choice QA) are used as the VIP-tokens since "they **(1)** are important to the specific task goals and **(2)** easily pre-identifiable by the user." In the worst case, if there is no obvious token to be selected, we can prepend some learnable "latent" tokens or certain user commands (such as "summarize" for summarization tasks as we used in our experiments) and use them as VIP-tokens (in fact, CLS tokens can be thought of as these learnable tokens).

#### Why is the performance of our method is better than standard models?

Our method is an approximation of the standard models, which should be inferior to the standard models, but in some cases, the performance of our method is better than standard models. We believe the reason is that the correct inductive bias improves the performance for tasks with limited amounts of data. Our approach is forced to compress irrelevant information and the attention is carried out on the compressed sequences, but in standard model with standard attention, each token has access to the entire sequence, which enables a larger degree of freedom. As a result, more training data might be required for the model to learn the correct pattern or bias.