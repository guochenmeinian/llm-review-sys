Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces

Martin Ryner

Vironova AB, Stockholm, Sweden

Division of Numerical Analysis, Optimization and Systems Theory,

Department of Mathematics,

KTH Royal Institute of Technology, Stockholm, Sweden

martin.ryner@vironova.com, martinrr@kth.se

Jan Kronqvist

Division of Numerical Analysis, Optimization and Systems Theory,

Department of Mathematics,

KTH Royal Institute of Technology, Stockholm, Sweden

jankr@kth.se

Johan Karlsson

Division of Numerical Analysis, Optimization and Systems Theory,

Department of Mathematics,

KTH Royal Institute of Technology, Stockholm, Sweden

johan.karlsson@math.kth.se

###### Abstract

This paper presents a framework for computing the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, where the discrepancy is the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization of the optimal transport problem that finds the assignment between two sets preserving pairwise distances as much as possible. This can be used to quantify the similarity between two formations or shapes, a common problem in AI and machine learning. The problem can be formulated as a Quadratic Assignment Problem (QAP), which is in general computationally intractable even for small problems. Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank. The method scales well with the number of points, and it can be used to find the global solution for large-scale problems with thousands of points. We compare the computational complexity of our approach with state-of-the-art methods on synthetic problems and apply it to a near-symmetrical problem which is of particular interest in computational biology.

## 1 Introduction

Many important applications in machine learning deal with comparing sequences, images, and higher dimensional data, where the data is unstructured and not directly comparable. In physics, chemistry, biology, music, and linguistics, objects with greatly different properties often appear in symmetrical variations characterized by concepts such as isomerisms, chirality, harmonies, and alternations. Understanding, and being able to analyze, these types of variations can be truly critical as some variations in chemicals and biologicals may be toxic or even lethal. The Gromov-Wasserstein framework [15; 16; 17], has shown to be a powerful approach for comparing and matching such data, as it is invariant to translations and rotation. The Gromov-Wasserstein framework has, for example, been successfully applied to domain adaptation [29], graph matching [28], metric alignment [9], single-cell alignment [7], and word embedding [1].

The task of evaluating the Gromov-Wasserstein problem is in general considered to be intractable. Typically, the computational burden grows exponentially with the number of points describing the compared objects. In fact, a Gromov-Wasserstein problem can be formulated as quadratic assignment problem (QAP) [14; 5; 4], which is known to be NP-Hard. Naturally, there has been plenty of research on local and approximate methods for solving Gromov-Wasserstein and QAP problems [20; 23; 22; 27; 2; 24; 25]. However, objects containing symmetries or repeated patterns are particularly challenging for local optimization methods and may lead to significant errors in the estimated discrepancy as matching such objects with local optimization methods may accidentally find the sub-optimal reflections and rotations. The inability to detect such phenomena can have a great impact on the discovery of isomerisms and subsequently attributes of crucial importance.

In this paper, we develop a rigorous method for globally optimizing Gromov-Wasserstein problems by calculating a sequence of iteratively improving upper- and lower bounds. We consider a general class of Gromov-Wasserstein discrepancy problems where the points, representing the objects, belong to a Euclidean space. We show that such Gromov-Wasserstein problems can be formulated exactly as low-rank QAPs. We build upon this low-rank QAP representation to develop an algorithm that scales well with the number of points. The proposed algorithm can be characterized as a so-called cutting plane method [12; 10] where we solve a sequence of relaxed problems that are iteratively strengthened by generating and accumulating valid linear inequality constraints, _i.e.,_ cutting planes. The optimum of the relaxed problem provides a valid lower bound for the optimum of the Gromov-Wasserstein problem in each iteration. By solving a computationally cheap optimal transportation problem [18; 26; 6], we obtain both an upper-bound and a new cutting plane to strengthen the relaxation. We prove convergence for the proposed algorithm, and present a computational study that clearly shows the algorithm's efficiency and that the performance scales well with the number of points.

The main contribution of the paper can be summarized as:

* We identify a general class of Gromov-Wasserstein problems, for point clouds embedded in low dimensional Euclidean spaces, that can be exactly represented as a concave low-rank QAP. In particular, mappings of images fits well within our framework.
* We develop a method for solving this class of Gromov-Wasserstein problems by solving a sequence of alternating sub-problems, which are either low-dimensional or linear.
* We prove that the proposed algorithm converges to a global optimal solution. The algorithm produces an optimality certificate in each iteration, in the form of upper- and lower bounds, which informs us of the potential suboptimality if the algorithm is terminated early.
* We present a numerical study, showing the efficiency of the proposed algorithm by comparing to other global optimization methods. We also illustrate the importance of globally solving Gromov-Wasserstein problems on a problem in computational biology.

In Section 2 we introduce the Gromov-Wasserstein problems and how it can be written as a QAP. In Section 3 we identify a class of Gromov-Wasserstein discrepancy problems that can be written as a concave relaxed QAPs problem, and in Section 4 we present the main methodology and an algorithm for solving this class of problems. Finally, in Section 5 we present numerical results and an application in computational biology.

## 2 The Gromov-Wasserstein discrepancy problem

Let \(x_{1}\ldots,x_{n}\in\mathcal{X}\) and \(y_{1}\ldots,y_{n}\in\mathcal{Y}\) be two sets of points and consider the problem of finding an assignment \(\pi\) between the point sets such that the pairwise distances \(d_{\mathcal{X}}(x_{i},x_{i^{\prime}})\) and \(d_{\mathcal{Y}}(y_{\pi(i)},y_{\pi(i^{\prime})})\) are as close as possible for \(i,i^{\prime}=1,\ldots,n\), where \(d_{\mathcal{X}}\) and \(d_{\mathcal{Y}}\) represents a notion of distance on the sets \(\mathcal{X}\) and \(\mathcal{Y}\), respectively. This can be formulated as the discrete Gromov-Wasserstein discrepancy problem

\[\min_{\Gamma\in P}\ \frac{1}{2}\sum_{i,i^{\prime},j,j^{\prime}=1}^{n}(d_{ \mathcal{X}}(x_{i},x_{i^{\prime}})-d_{\mathcal{Y}}(y_{j},y_{j^{\prime}}))^{2} \Gamma_{i,j}\Gamma_{i^{\prime},j^{\prime}},\] (1)

and where the assignment \(\pi\) is represented by a permutation matrix \(\Gamma\) and \(P\) is the set of all \(n\times n\) permutation matrices. The corresponding relaxed problem, where instead \(\Gamma\) is in the set of doubly stochastic matrices, denoted by \(\overline{P}\), is often referred to as the Gromov-Wasserstein problem [20]. In these formulations we note that

\[\sum_{i,i^{\prime},j,j^{\prime}=1}^{n}(d_{\mathcal{X}}(x_{i},x_{i ^{\prime}})-d_{\mathcal{Y}}(y_{j},y_{j^{\prime}}))^{2}\Gamma_{i,j}\Gamma_{i^{ \prime},j^{\prime}}\] \[=\sum_{i,i^{\prime},j,j^{\prime}=1}^{n}(d_{\mathcal{X}}(x_{i},x_{ i^{\prime}})^{2}-2d_{\mathcal{X}}(x_{i},x_{i^{\prime}})d_{\mathcal{Y}}(y_{j},y_{j^ {\prime}})+d_{\mathcal{Y}}(y_{j},y_{j^{\prime}})^{2})\Gamma_{i,j}\Gamma_{i^{ \prime},j^{\prime}}\] \[=\langle C_{x},C_{x}\rangle-2\langle C_{x}\Gamma,\Gamma C_{y} \rangle+\langle C_{y},C_{y}\rangle\]

where \(C_{x}=[d_{\mathcal{X}}(x_{i},x_{i^{\prime}})]_{i,i^{\prime}=1}^{n}\), \(C_{y}=[d_{\mathcal{Y}}(y_{j},y_{j^{\prime}})]_{j,j^{\prime}=1}^{n}\), and \(\langle\cdot,\cdot\rangle\) denotes the standard (Frobenius) inner product. Since the first and third sums are independent of \(\Gamma\), solving the discrete Gromov-Wasserstein problem (1) is the same as solving a quadratic assignment problem (QAP) on a simplified Koopmans-Beckmann form [5], namely as

\[\min_{\Gamma\in P}\quad-\langle C_{x}\Gamma,\Gamma C_{y}\rangle+ \frac{1}{2}(\langle C_{x},C_{x}\rangle+\langle C_{y},C_{y}\rangle).\] (2)

This problem is in general NP-hard, and the number of variables scales with the number of data points, making (2) computationally intractable for problems of relevant size. Here, we focus on instances where the matrices \(C_{x},C_{y}\) are positive definite and low rank. By utilizing this structure, we develop an algorithm that is guaranteed to find a globally optimal solution and scales well with the number of points.

## 3 The Gromov-Wasserstein problem and low rank QAP

An important special case of the Gromow-Wasserstein problem, considered in [20; 22], is when the point clouds belong to the Euclidean space and the squared Euclidean distance is used as discrepancy. That is, when the set of points are \(x_{1}\ldots,x_{n}\in\mathbb{R}^{\ell_{x}}\) and \(y_{1}\ldots,y_{n}\in\mathbb{R}^{\ell_{y}}\), which we represent by the matrices

\[X=(x_{1},x_{2},\ldots,x_{n})\in\mathbb{R}^{\ell_{x}\times n}, \qquad Y=(y_{1},y_{2},\ldots,y_{n})\in\mathbb{R}^{\ell_{y}\times n}.\]

In this case it can be noted that the distance matrices \(C_{x}\) and \(C_{y}\) has a rank bounded by \(\ell_{x}+2\) and \(\ell_{y}+2\), respectively, which can be seen from the identities

\[C_{x}=(\|x_{i}-x_{j}\|_{2}^{2})_{i,j=1}^{n}=\mathbf{1}m_{x}^{T} -2X^{T}X+m_{\mathbf{x}}\mathbf{1}^{T},\] (3a) \[C_{y}=(\|y_{i}-y_{j}\|_{2}^{2})_{i,j=1}^{n}=\mathbf{1}m_{y}^{T} -2Y^{T}Y+m_{\mathbf{y}}\mathbf{1}^{T},\] (3b)

where \(m_{x}=(\|x_{1}\|^{2},\|x_{2}\|^{2},\ldots,\|x_{n}\|^{2})^{T}\), \(m_{y}=(\|y_{1}\|^{2},\|y_{2}\|^{2},\ldots,\|y_{n}\|^{2})^{T}\), and \(\mathbf{1}\in\mathbb{R}^{n\times 1}\) is a column vector of ones. This observation was also used in [22] for formulating the Gromov-Wasserstein problem as a quadratic problem of rank \((\ell_{x}+2)(\ell_{y}+2)\) and developing fast algorithms for the problem. However, the rank can be even further reduced and the corresponding Gromov-Wasserstein problem can be formulated as a QAP problem of rank \(\ell_{x}\ell_{y}\)[25, Lemma 4.2.3] (cf. [20; Proposition 1]).

**Proposition 1**.: _[_25_, Lemma 4.2.3]_ _Let \(\Gamma\) be a doubly stochastic matrix and the matrices \(C_{x}\) and \(C_{y}\) given by (3), then it holds that_

\[\langle C_{x}\Gamma,\Gamma C_{y}\rangle=\langle 2X\Gamma Y^{T},2X\Gamma Y ^{T}\rangle+\langle L,\Gamma\rangle+2\mathbf{1}^{T}m_{y}\mathbf{1}^{T}m_{x},\]

_where \(L=2nm_{x}m_{y}^{T}-4m_{x}\mathbf{1}^{T}Y^{T}Y-4X^{T}X\mathbf{1}m_{y}^{T}\)._

Proof.: The proposition follows by straightforward computations, where the expressions are simplified using \(\mathbf{1}=\Gamma\mathbf{1}=\Gamma^{T}\mathbf{1}\). See the appendix for the proof.

Hence, the low rank QAP formulation of the discrete Gromov-Wasserstein problem can be stated as

\[\min_{\Gamma\in P}\quad-\langle 2X\Gamma Y^{T},2X\Gamma Y^{T}\rangle- \langle L,\Gamma\rangle+c_{0}\] (4)

where \(L=2nm_{x}m_{y}^{T}-4m_{x}\mathbf{1}^{T}Y^{T}Y-4X^{T}X\mathbf{1}m_{y}^{T}\), and \(c_{0}=(\langle C_{x},C_{x}\rangle+\langle C_{y},C_{y}\rangle-41^{T}m_{y} \mathbf{1}^{T}m_{x})/2\). The Gromov-Wasserstein problem can also be rewritten similarly and formulated as

\[\min_{\Gamma\in P}\quad-\langle 2X\Gamma Y^{T},2X\Gamma Y^{T}\rangle- \langle L,\Gamma\rangle+c_{0},\] (5)

and since the objective function is concave, any optimal solution of (4) is also an optimal solution of the relaxed problem.

**Proposition 2**.: _Any optimal solution of the discrete Gromov-Wasserstein problem (4), is also an optimal solution to the Gromov-Wasserstein problem (5). Conversely, problem (5) always has an optimal solution in one extreme point,1 and any optimal extreme point to (5) is also an optimal solution to (4)._

Footnote 1: An extreme point of a convex set is a point in the set which does not lie in any open line segment joining two points of the set.

Proof.: Since (5) is the minimization of a concave objective function over a convex sets \(\overline{P}\), it attains the optimal value in an extreme point of the feasible set. Since the permutation matrices are the extreme points to the doubly stochastic matrices, i.e., \(P=\operatorname{ext}(\overline{P})\), (5) attains its minimum on \(P\). Further, the set of points in \(P\) for which (5) attains its minimum are the optimal solutions of (4). To show the converse statement, note that a minimum exists since \(\overline{P}\) is compact and the objective function is continuous. Further, since the objective function is concave, an optimum must be at an extreme point. Finally, since the extreme points of \(\overline{P}\) is the permutation matrices \(P\), any optimal extreme point of (5) is also feasible and optimal to (4). 

In the next section we will propose a methodology and an algorithm for solving this problem.

## 4 A cutting plane algorithm utilizing the low rank structure

By Proposition 2, we know that an optimal solution to the discrete Gromov-Wasserstein problem (4) can be obtained by solving the relaxed problem (5). However, the relaxed problem (5) is still a high-dimensional non-convex QP, which is NP-hard [19]. The high dimensionality can, in particular, be a limiting factor in solving the problem. For example, it is known that the performance of spatial branch-and-bound, one of the main approaches for globally optimizing nonconvex problems [10], can scale poorly with the number of variables. Thus, directly optimizing either (1) or (5) by spatial branch-and-bound is not computationally tractable for larger instances. Our idea is to use the low-rank formulation of the Gromov-Wasserstein problem and perform the optimization in a projected subspace of dimension \(\ell_{x}\ell_{y}+1\) by solving a sequence of relaxed problems.

First, we note that problem (5) can be written as

\[\min_{W\in\mathbb{R}^{\ell_{x}\times\ell_{y}},w\in\mathbb{R}, \Gamma\in\overline{P}} -\|W\|_{F}^{2}-w+c_{0}\] (6a) subject to \[W=2X\Gamma Y^{T},\ w=\langle L,\Gamma\rangle.\] (6b)

Equivalence of problems (5) and (6) is shown by simply inserting the expressions for \(w\) and \(W\) into the objective function. Next, we project out the \(\Gamma\) variables, and we define the feasible set in the \((w,W)\)-space as \(\mathcal{F}=\text{Proj}_{W,w}\left(W\in\mathbb{R}^{\ell_{x}\times\ell_{y}},w \in\mathbb{R},\Gamma\in\overline{P}\ \middle|\ W=2X\Gamma Y^{T},\ w=\langle L, \Gamma\rangle\right)\).

Constructing an H-representation of the polytope \(\mathcal{F}\), i.e., representing it by linear constraints of the form \(\langle Z_{r},W\rangle+\alpha_{r}w\leq\beta_{r}\), is not trivial and the number of constraints can grow exponentially with the number of data points. Therefore, we propose an algorithm based on a cutting plane scheme to optimize over \(\mathcal{F}\).

Instead of directly optimizing the objective in (6a) over the feasible set \(\mathcal{F}\), which we don't have a tractable representation for, we relax the problem as

\[\min_{W\in\mathbb{R}^{d_{x}\times t_{y}},w\in\mathbb{R}} -\|W\|_{F}^{2}-w+c_{0}\] (7a) subject to \[\langle Z_{r},W\rangle+\alpha_{r}w\leq\beta_{r},\quad\text{ for }\;r=1,\ldots,N.\] (7b)

The linear constraints (7b) are supporting hyperplanes of the feasible set \(\mathcal{F}\), which we will generate iteratively. The goal is to force the minimizer of problem (7) into the feasible set \(\mathcal{F}\) by using relatively few linear constraints. Keep in mind, we don't need a full representation of set \(\mathcal{F}\), we only need to capture the shape of \(\mathcal{F}\) in some areas of interest, e.g., the constraints defining the faces of \(\mathcal{F}\) at the optimal solution of problem (6) would suffice. The main advantage of the relaxation in problem (7) is that it contains far fewer variables than both problems (5) and (6), and the dimensionality is independent of the number of data points. Problem (7) can, therefore, be solved much more efficiently, especially in early iterations when the number of constraints is low. We will show that the constraints can be determined, as needed, by solving optimal transport problems. Based on this, we will develop an iterative approach that sequentially solves problem (7) and adds a constraint until the the solutions is the same as (6).

To initialize the search we determine a bounding box of \(\mathcal{F}\) and use this to define a set of constraints (7b). The bounding box is determined by the (elementwise) minimum and maximum of the variables \(w\) and \(W\) given by

\[\min_{\Gamma\in\bar{P}}\;2(X\Gamma Y^{T})_{i,j} \leq W_{i,j}\leq\max_{\Gamma\in\bar{P}}\;2(X\Gamma Y^{T})_{i,j} \quad\text{ for }i=1,\ldots,\ell_{x};\;j=1,\ldots,\ell_{y}\] (8a) \[\min_{\Gamma\in\bar{P}}\langle L,\Gamma\rangle \leq w\;\leq\max_{\Gamma\in\bar{P}}\langle L,\Gamma\rangle,\] (8b)

which can each be computed efficiently by solving a standard optimal transport problem. Initializing the set of constraints by the bounding box ensures that (7) is well-defined and bounded.

If the minimizer of problem (7) is within \(\mathcal{F}\), then we can stop as the solution is optimal for (6).2 Otherwise, we improve the outer approximation of \(\mathcal{F}\) by adding new a constraint defined by \(Z_{N+1}\), \(\alpha_{N+1}\) and \(\beta_{N+1}\). Let \((w_{N},W_{N})\) be the current optimal solution of (7), and assume that \((w_{N},W_{N})\notin\mathcal{F}\), then we form a new constraint, a so-called cutting plane, that excludes \((w_{N},W_{N})\) from the feasible set of (7).

Footnote 2: Remember, we are minimizing the objective (6a) over an outer approximation of the feasible set.

We form a new constraint based on the gradient of the objective function (7a), which is given by

\[\nabla_{(w,\operatorname{vec}(W)T)}(-\|W\|_{F}^{2}-w,)=\left(-1,-2 \operatorname{vec}(W)^{T}\right).\]

By letting \(\alpha_{N+1}=1\) and \(Z_{N+1}=2W_{N}\), the hyperplane defining the new constraint will have the (negative) gradient in the optimum \((w_{N},W_{N})\) as normal vector. Then we select \(\beta_{N+1}\) such that the new constraint forms a supporting hyperplane of \(\mathcal{F}\) (7b). This can be found by solving the following optimal transport problem

\[\beta_{N+1}:=\max_{W\in\mathbb{R}^{d_{x}\times t_{y}},w\in\mathbb{ R},\Gamma\in\overline{\mathcal{P}}} \quad\langle Z_{N+1},W\rangle+\alpha_{N+1}w\qquad=\max_{\Gamma\in\overline{P} }\;\langle 4X^{T}W_{N}Y+L,\Gamma\rangle.\] (9) subject to \[W=2X\Gamma Y^{T},\;w=\langle L,\Gamma\rangle\]

When solving this problem, we also obtain a solution \(\Gamma_{N}\) which is a doubly stochastic matrix (generically also a permutation matrix), which gives an upper bound for (6) and a candidate for the optimal solution. In the following subsection, we prove that the that algorithm, described in Algorithm 1, converges to a globally optimal solution.

A geometrical illustration of the algorithm is given in Figure (1). For illustrative purposes, we have used one-dimensional data resulting in a two-dimensional problem in the \((W,w)\)-space. The data sets consist of \(6\) points each where one of the data sets has a reflective symmetry. This results in two global optima and \(6!\) projected permutations in \(P\). The first solution of (7) is located at one of the corners of the bounding box and marked with a "1" (the subsequent solutions are marked "2" "5"). The infeasible point "1" is excluded from the search space by a cutting plane (red line, marked with an "A"). Following the same procedure we obtain point "2", and cutting plane "B". Adding further cutting planes excludes "3" and subsequently "4", resulting in the feasible and optimal point "5". Note that the cutting planes from iterations 3 and 4 almost overlap the cutting planes "A" and "B", since the gradients in the points 1 and 3 are very similar (the same for points 2 and 4).

### Proof of convergence of Algorithm 1

The main result considering convergence is presented in the following theorem.

**Theorem 1**.: _The gap between the upper bound and lower bound in Algorithm 1 converges to \(0\) (if the tolerance is \(\epsilon=0\))._

Proof.: Consider the \(N\)th iteration in Algorithm (1), let \((w_{N},W_{N})\) be an optimal solution to (7), and \(\Gamma_{N}\) is an optimal solution to (9) with corresponding points \((\hat{w},\hat{W})=(\langle L,\Gamma_{N}\rangle,2X\hat{\Gamma}_{N}Y^{T})\), in the \((w,W)\)-space. Assume that the gap in the objective function between those two points is

\[\epsilon_{N}=\|W_{N}\|_{F}^{2}+w_{N}-\|\hat{W}\|_{F}^{2}-\hat{w}.\] (10)

The new constraint is then defined by\(\langle Z_{N+1},W\rangle+w\leq\beta_{N+1}\) where \(Z_{N+1}=2W_{N}\) and \(\beta_{N+1}=2\langle W_{N},\hat{W}\rangle+\hat{w}\), and thus for any point \((w,W)\) that satisfy the constraint it must hold that

\[0\leq 2\langle W_{N},\hat{W}-W\rangle+\hat{w}-w.\]

Figure 1: Left image: An illustrative example of the method on one-dimensional data. Right image: The area around the two global optima highlighting the sequence of optimal extreme points in the approximate cover and generation of cutting planes.

By substituting \(\hat{w}\) from (10), we obtain

\[\epsilon_{N} \leq 2\langle W_{N},\hat{W}-W\rangle-w+\|W_{N}\|_{F}^{2}+w_{N}-\| \hat{W}\|_{F}^{2}\] \[=w_{N}-w+2\langle W_{N},W_{N}-W\rangle-\|\hat{W}-W_{N}\|_{F}^{2}\] \[\leq w_{N}-w+2\langle W_{N},W_{N}-W\rangle\] \[\leq(|w_{N}-w|^{2}+\|W_{N}-W\|_{F}^{2})^{1/2}(1+4\|W_{N}\|_{F}^{2} )^{1/2}\]

where we in the last step have used the Cauchy-Schwarz inequality.

For any iteration number \(N+k\) with \(k>0\), we have that \((w_{N+k},W_{N+k})\) is feasible for \(\langle Z_{N+1},W\rangle+w\leq\beta_{N+1}\), and thus the Euclidean distance between \((w_{N},W_{N})\) and \((w_{N+k},W_{N+k})\) is at least \(\epsilon_{N}/(1+4\|W_{N}\|_{F}^{2})\). If the gap in the algorithm does not converge to \(0\), then there is an \(\epsilon>0\) for which \(\epsilon_{N}\geq\epsilon\) for all \(N\) and thus the distance between any two points in the sequence \(\{(w_{N},W_{N})\}_{N}\) is bounded from below by \(\epsilon/(1+4\max\{\|W\|_{F}^{2}\mid W\in(8)\})\). However, since the infinite sequence of points \(\{(w_{N},W_{N})\}_{N}\) belong to a bounded set defined by (8), there must be a convergent subsequence, which contradicts that there is a positive lower bound on the distance between any two points. 

From Theorem 1 the gap between the upper and lower bound converges to zero, and thus Algorithm 1 converges to a globally optimal solution.

### Considerations when solving the relaxed problem

Problem (7) minimizes a concave function over a convex set. Thus, the solution is located in the extreme points of the convex set, i.e., the outer approximation of \(\mathcal{F}\). The standard approach to solve such problems is by branch and bound methods. However, the low dimension and sequential generation of constraints make it viable to search among the extreme points for an optimal solution.

To simplify notation, we define \(x^{T}=\begin{pmatrix}w&\mathrm{vec}(W)^{T}\end{pmatrix}\in\mathbb{R}^{r}\) where \(r:=\ell_{x}\ell_{y}+1\). Then we can write (7b) on the form \(A_{k}x\leq b_{k}\). Note that, by construction, none of the constraints are strongly redundant as every constraint is satisfied with equality for a permutation. As the constraints are added sequentially, it is actually easy to compute the new extreme points by keeping track of previous extreme points as described in the following proposition.

**Proposition 3**.: _Assume that the extreme points \(\{x_{k}\}_{k}\) of the convex set described by \(Ax\leq b\) are known. When adding a constraint \(A_{N}^{T}x\leq b_{N}\), the additional extreme points are linear combinations of pairs of existing extreme points \(x_{k_{1}}\) and \(x_{k_{2}}\) both satisfying the same \(r-1\) constraints with equality and \(A_{N}^{T}x_{k_{1}}\leq b_{n}\) and \(A_{N}^{T}x_{k_{2}}>b_{n}\) so that the combination satisfies \(A_{N}^{T}(\lambda x_{k_{1}}+(1-\lambda)x_{k_{2}})=b_{N}\)._

Proof.: This is done by counting the number of constraints satisfied with equality. See the appendix for the proof. 

Especially, in lower dimensions, e.g., with two or three dimensional data, this approach of keeping track of all extreme points and calculating new extreme points after adding a constraint can be very efficient for solving problem (7). More details of this method is provided in the appendix. In the numerical results, we present results where problem (7) is solved both by this extreme point search and the spatial branch and bound method in Gurobi.

## 5 Numerical results

### Computational efficiency

In this section we compare the time to solve the problem up to an accuracy measured in relative error with different methods: Algorithm 1 when (7) is solved with the extreme point method as described in section 4.2, Algorithm 1 when (7) solved using Branch & bound using Gurobi 10.0 [13], MILP1 formulation in [11] implemented in Gurobi and finally when (6) is directly solved using Gurobi. The MILP1 formulation can handle a larger class of problems, but is reported to handle very few dimensions. All computations were performed using Matlab on an Intel i5 2.9 GHz PC. The linear optimal mass problem (9) was solved using the package [3] which is based on the network simplex [18]. The model problems tested are evenly distributed points in a unit disc or ball which we denote \(\mathcal{U}\), and normally distributed points \(\mathcal{N}(0,\sigma)\). We denote \(\mathcal{N}_{1}:=\mathcal{N}(0,I)\), \(\mathcal{N}_{2}:=\mathcal{N}(0,\mathrm{diag}(1,1,\frac{1}{10}))\) and \(\mathcal{N}_{3}:=\mathcal{N}(0,\mathrm{diag}(1,\frac{1}{2},\frac{1}{10}))\). See Table 1 for numerical results. Some notes on the results

1. On 2-dimensional data (\(\ell_{x}=\ell_{y}=2\)), the extreme point method is particularly efficient.
2. For problems that need many extreme points (\(>10^{6}\)), which depends on the data itself, the handling of extreme points becomes the driver of computational cost.
3. Problems mainly containing reflections (e.g. \(\mathcal{N}_{3}\)) are easier to solve than those with room for rotations.
4. Directly solving (6) with Gurobi was not feasible for problems with \(n\geq 500\).

### Comparison with local search method

We compare the results using the proposed method to a local search method provided in the Git-hub repository for [20]. The entropy regularization parameter is set to \(0\), and the method is run with random initializations (including the first lower bound [17] used with success in [22]) until the relative error to the global optimum is less than a specific tolerance \(\epsilon\). The problems are the same as in the previous section. Note here that in the local search methods we need an oracle in order to determine when we have reached a given performance level (which of course is not available in practice), whereas we in the proposed method computes upper and lower bounds.

Results for two and three dimensional data (\(\ell_{x}=\ell_{y}=2,3\)) are presented in Table 2. The results show that the proposed method performs better than multi-starting the local method when \(\ell_{x}=\ell_{y}=2\). For the matching of 2-dimensional data to 3-dimensional data, the local search method is surprisingly fast suggesting that the problems is of a completely different nature than when 2-d data is matched to 2-d data or 3-d data is matched to 3-d data.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Type & \(n\) & \(\ell_{x},\ell_{y}\) & Rel. & Algorithm 1 [s] & MILP1 [s] & (6) B\&B [s] \\  & & & error & Extreme point / B\&B & & \\ \hline \(\mathcal{U}\) & 10 & 2,2 & \(10^{-8}\) & 0.14 (0.07-0.3) / 21 (6-47) & 39 (11-58) & 0.15 (0.14-0.16) \\ \(\mathcal{U}\) & 100 & 2,2 & \(10^{-8}\) & 0.48 (0.3-0.7) / 86 (52-107) & - & 25 (19-39) \\ \(\mathcal{U}\) & 500 & 2,2 & \(10^{-8}\) & 11 (9-16) / 408 (269-511) & - & - \\ \(\mathcal{U}\) & 1000 & 2,2 & \(10^{-8}\) & 69 (54-85) / 576 (389-1059) & - & - \\ \(\mathcal{U}\) & 2000 & 2,2 & \(10^{-8}\) & 460 (313-653) / - & - & - \\ \hline \(\mathcal{U}\) & 10 & 2,3 & \(10^{-8}\) & 1.8 (1.2-2.4) / 133 (45-296) & 105 (49-147) & 2.4(1.8-3.4) \\ \(\mathcal{U}\) & 100 & 2,3 & \(10^{-8}\) & 278 (99-813) / - & - & 172 (133-221) \\ \(\mathcal{U}\) & 500 & 2,3 & \(10^{-8}\) & 9568 / - & - & - \\ \hline \(\mathcal{N}_{1}\) & 10 & 2,3 & \(10^{-8}\) & 0.51 (0.39-0.65) / 708 (233-1184) & 146 (66-227) & 3 (2.6-4.0) \\ \(\mathcal{N}_{1}\) & 100 & 2,3 & \(10^{-8}\) & 86 (20-275) / - & - & 95 (73-116) \\ \(\mathcal{N}_{1}\) & 500 & 2,3 & \(10^{-5}\) & 5310! / - & - & - \\ \hline \(\mathcal{N}_{2}\) & 10 & 3,3 & \(10^{-2}\) & 1.8 (0.7-3.2) / 142 (73-210) & 117 (71-163) & 0.2(0.1-0.3) \\ \(\mathcal{N}_{2}\) & 100 & 3,3 & \(10^{-2}\) & 36 (22-55)/ - & - & 45(36-65) \\ \(\mathcal{N}_{2}\) & 500 & 3,3 & \(10^{-2}\) & 436 (228-862) / - & - & - \\ \hline \(\mathcal{N}_{3}\) & 10 & 3,3 & \(10^{-2}\) & 1.2 (0.5-2.3) / 22 (11-43) & 72 (43-94) & 0.2(0.1-0.3) \\ \(\mathcal{N}_{3}\) & 100 & 3,3 & \(10^{-2}\) & 7 (5-8)/ 91 (76-111) & - & 10 (9-12) \\ \(\mathcal{N}_{3}\) & 500 & 3,3 & \(10^{-2}\) & 11 (9-16) / 161 (104-226) & - & - \\ \(\mathcal{N}_{3}\) & 1000 & 3,3 & \(10^{-2}\) & 25 (22-29) / 176 (149-224) & - & - \\ \(\mathcal{N}_{3}\) & 2000 & 3,3 & \(10^{-2}\) & 93 (91-100) / 578 (429-691) & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Computational efficiency. Computational time on the format [mean (low - high)] from 5 repeats for various problem geometries, dimensions and sizes, for the proposed extreme point method, the same method using branch and bound (B&B), the problem formulation MILP1 [11] and finally (6) implemented in Gurobi via Matlab interface. An ”-” indicates that the problem timed out, in such a way being incomparable with the proposed method. An ”!” indicates that the problem reached \(10^{4}\) iterations and stopped to the accuracy indicated.

Note that we have chosen to compare with the method from [20] rather than [22]. This is since we have not optimized the optimal transport computations using the low rank structures in the problem. If we optimized the computations in this way, i.e., as in [21], we expect to get similar improvement as in [22] compared with [20].

### Application to symmetrical data for morphological analysis

In this example we investigate the impact of correctly evaluating the Gromov-Wasserstein discrepancy compared to estimating it by local search. As a test case, we examine the ability to classify Adeno Associated Viral (AAV) particles based on the Gromov-Wasserstein discrepancy on image data originating from transmission electron microscopy, hence \(\ell_{x}=\ell_{y}=2\). AAV particles are nearly round viral particles with multiple near-rotational symmetries as illustrated in Figure 2. By sampling \(n=500\) positions on each AAV particle proportional to the protein density, the point sets from pairs of particles \(X_{i}\) and \(X_{j}\) can subsequently be compared using the Gromov-Wasserstein discrepancy.

Computing the Gromov-Wasserstein discrepancy between all objects in a large set \(\mathcal{X}=\{X_{i}\}_{i}^{N}\) is tedious. Therefore, one may consider calculating the discrepancy to a subset of the objects that are well distributed under the Gromov-Wasserstein discrepancy. To find such a subset without actually calculating all pairs of discrepancies, we use a greedy approach by defining the index subset \(S_{k}=\{s_{i}\}_{i=1}^{k}\) by selecting the first object arbitrarily and then let the set grow by

\[S_{k+1}:=\{S_{k},\operatorname*{argmax}_{i\not\in S_{k}}\min_{j\in S_{k}}d_{ GW}(X_{i},X_{j})\}.\] (11)

In this way all objects are closer than a tolerance to an object in the subset, and the way the subset is produced generates a monotonically decreasing tolerance. By using this procedure, every object obtains a feature vector of distances to the objects indexed by \(S_{k}\). Next the feature vectors are used as input to a K-means clustering and classification quality in terms of purity, adjusted rand index and normalized mutual information compared to an expert evaluation is presented in Figure 3.

The example shows that if the data contains symmetries, local search methods may get stuck on permutations that are locally optimal but that are far from globally optimal, see Figure 2. Thus the discrepancy obtained from local search is inappropriate to use as a discriminating feature, as shown in the lower performance in the classification illustrated in Figure 3. In the left subfigure of Figure 2, self-similarities are visited in the proposed method, and these local optima are in fact also almost optimal when used as initiation point using local search methods e.g., [20].

This example shows that when the Gromov-Wasserstein problem is calculated accurately it provides valuable information and biological meaning as it differentiates viral particles with different cargo and

\begin{table}
\begin{tabular}{l c c c|l|l l l} \hline \hline  & & & Rel. & Algorithm 1 & & & Local search & \\ Type & \(n\) & \(\ell_{x},\ell_{y}\) & error \(\epsilon\) & Exec. time [s] & Exec. time [s] & Initializations & Successful runs \\ \hline \(\mathcal{U}\) & 100 & 2,2 & \(10^{-6}\) & 0.5 (0.4-0.6) & 4 (0.3-12.7) & 64 (4-205) & 5 \\ \(\mathcal{U}\) & 200 & 2,2 & \(10^{-6}\) & 1.6 (1.0-1.9) & 27 (13-42) & 129 (59-197) & 5 \\ \(\mathcal{U}\) & 300 & 2,2 & \(10^{-6}\) & 3.2 (2.7-3.9) & 174 (33-458) & 366 (69-959) & 5 \\ \(\mathcal{U}\) & 400 & 2,2 & \(10^{-6}\) & 6 (5-8) & 322 (97-685) & 321 (97-781) & 3 \\ \hline \(\mathcal{U}\) & 100 & 2,3 & \(10^{-6}\) & 352 (99-669) & 23 (8-60) & 341 (110-853) & 5 \\ \(\mathcal{U}\) & 200 & 2,3 & \(10^{-6}\) & 1238 (643-2612) & 92 (11-168) & 421(49-778) & 3 \\ \(\mathcal{U}\) & 300 & 2,3 & \(10^{-6}\) & 3006 (601-4908) & 131 (4-344) & 270 (10-710) & 5 \\ \(\mathcal{U}\) & 400 & 2,3 & \(10^{-6}\) & 4729 (4279-4868) & 367 (5-343) & 365 (1-859) & 5 \\ \hline \(\mathcal{N}_{1}\) & 100 & 2,2 & \(10^{-6}\) & 0.5 (0.3-0.7) & 0.6 (0.2-1.0) & 10 (3-16) & 5 \\ \(\mathcal{N}_{1}\) & 200 & 2,2 & \(10^{-6}\) & 0.9 (0.7-1.1) & 13.4 (1-37) & 61 (6-168) & 5 \\ \(\mathcal{N}_{1}\) & 300 & 2,2 & \(10^{-6}\) & 2.4 (2.0-2.9) & 37.5 (0.5-90) & 75 (1-182) & 5 \\ \(\mathcal{N}_{1}\) & 400 & 2,2 & \(10^{-6}\) & 4.4 (3.6-5.3) & 149 (22-378) & 142 (21-361) & 4 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Computational efficiency compared with local search [20]. Computational time on the format [mean (low - high)] on two specific problems which contain near symmetries and the number of random initializations needed to achieve the required accuracy on the format [mean (low - high)]. The number of initializations were limited to 1000.

variations in capsid structure, and, at the same time, finds the optimal orientation positively revealing possible chiralities or isomerisms. It also shows that when the distance is calculated accurately, it provides better decision support than using local search methods.

## 6 Discussion

When using distances as input for statistical analyses, the accuracy of the measurement set a bound for the information resolution. If the measurement system introduces error of a certain structure, this can produce artefacts in the result and affect decisions taken on the result. When using distances for such purposes, it is necessary to either know the measurement error, the artefacts being produced, or using an accurate measurement system. In this paper we have provided a method which computes the Gromov-Wasserstein problem accurately, which reduces the uncertainty of such considerations.

## Acknowledgement

This work was funded by Vironova AB and the Swedish innovation agency Vinnova through the innovation milieu GeneNova (2021-02640).

Figure 3: The trajectory of the increased quality of classification compared to an expert evaluation when the distance is computed from all particles to the sequence of particles suggested in the text. The gain of quality using an exact evaluation (blue) of the Gromov-Wasserstein problem is unambiguous over the local search (red).

Figure 2: Left: Each distance calculated with the proposed method and the same distance calculated with the local search method [20]. For near symmetrical data, the confusion of the measurements of the local method is clear. Middle: The relative error of the local method compared to the global solution. Right: Matching of the structure to itself to four different orientations visited by the algorithm where the color indicate the permutation \(\Gamma\). The relative error of the Gromov-Wasserstein-discrepancy and the isometry to the global optimum is written near each matching. The global optimum is able to correctly match to itself, whereas the local search method may get stuck in local optima.

## References

* [1] David Alvarez-Melis and Tommi S Jaakkola. Gromov-Wasserstein alignment of word embedding spaces. _arXiv preprint arXiv:1809.00013_, 2018.
* [2] Florian Beier, Robert Beinert, and Gabriele Steidl. On a linear Gromov-Wasserstein distance. _IEEE Transactions on Image Processing_, 31:7292-7305, 2022.
* [3] Nicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation using lagrangian mass transport. In _Proceedings of the 2011 SIGGRAPH Asia conference_, pages 1-12, 2011.
* [4] Rainer Burkard, Mauro Dell'Amico, and Silvano Martello. _Assignment problems: revised reprint_. SIAM, 2012.
* [5] Rainer E Burkard, Eranda Cela, Panos M Pardalos, and Leonidas S Pitsoulis. The quadratic assignment problem. In _Handbook of combinatorial optimization_, pages 1713-1809. Springer, 1998.
* [6] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in neural information processing systems_, 26:2292-2300, 2013.
* [7] Pinar Demetci, Rebecca Santorella, Bjorn Sandstede, William Stafford Noble, and Ritambhara Singh. Gromov-Wasserstein optimal transport to align single-cell multi-omics data. _BioRxiv_, pages 2020-04, 2020.
* [8] Li Deng. The MNIST database of handwritten digit images for machine learning research [best of the web]. _IEEE signal processing magazine_, 29(6):141-142, 2012.
* [9] Danielle Ezuz, Justin Solomon, Vladimir G Kim, and Mirela Ben-Chen. Gwcnn: A metric alignment layer for deep shape analysis. _Computer Graphics Forum_, 36(5):49-57, 2017.
* [10] Christodoulos A Floudas. _Deterministic global optimization: theory, methods and applications_, volume 37. Springer Science & Business Media, 2013.
* [11] Michael Friesen. _Low rank quadratic assignment problem: Formulations and experimental analysis_. PhD thesis, Science: Department of Mathematics, 2019.
* [12] Ralph E Gomory. Solving linear programming problems in integers. _Combinatorial Analysis_, 10:211-215, 1960.
* [13] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023.
* [14] Tjalling C Koopmans and Martin Beckmann. Assignment problems and the location of economic activities. _Econometrica: journal of the Econometric Society_, pages 53-76, 1957.
* [15] Facundo Memoli. On the use of Gromov-Hausdorff distances for shape comparison. In M. Botsch, R. Pajarola, B. Chen, and M. Zwicker, editors, _Eurographics Symposium on Point-Based Graphics_. The Eurographics Association, 2007.
* [16] Facundo Memoli. Gromov-Hausdorff distances in Euclidean spaces. In _2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops_, pages 1-8. IEEE, 2008.
* [17] Facundo Memoli. Gromov-Wasserstein distances and the metric approach to object matching. _Foundations of computational mathematics_, 11(4):417-487, 2011.
* [18] J.B. Orlin. A polynomial time primal network simplex algorithm for minimum cost flows. _Mathematical Programming_, 78(2):109-129, 1997.
* [19] Panos M Pardalos and Stephen A Vavasis. Quadratic programming with one negative eigenvalue is NP-hard. _Journal of Global optimization_, 1(1):15-22, 1991.
* [20] Gabriel Peyre, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein averaging of kernel and distance matrices. In _International Conference on Machine Learning_, pages 2664-2672. PMLR, 2016.

* [21] Meyer Scetbon, Marco Cuturi, and Gabriel Peyre. Low-rank Sinkhorn factorization. In _International Conference on Machine Learning_, pages 9344-9354. PMLR, 2021.
* [22] Meyer Scetbon, Gabriel Peyre, and Marco Cuturi. Linear-time Gromov-Wasserstein distances using low rank couplings and costs. In _International Conference on Machine Learning_, pages 19347-19365. PMLR, 2022.
* [23] Justin Solomon, Gabriel Peyre, Vladimir G Kim, and Suvrit Sra. Entropic metric alignment for correspondence problems. _ACM Transactions on Graphics (TOG)_, 35(4):1-13, 2016.
* [24] Cole Stiegler. _Efficient local optimization for low-rank large-scale instances of the quadratic assignment problem_. The University of Iowa, 2018.
* [25] Titouan Vayer. A contribution to optimal transport on incomparable spaces. _arXiv preprint arXiv:2011.04447_, 2020.
* [26] C. Villani. _Topics in Optimal Transportation_, volume 58. Graduate studies in Mathematics, AMS, 2003.
* [27] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable Gromov-Wasserstein learning for graph partitioning and matching. _Advances in neural information processing systems_, 32:3052-3062, 2019.
* [28] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-Wasserstein learning for graph matching and node embedding. In _International conference on machine learning_, pages 6932-6941. PMLR, 2019.
* [29] Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and Qingyao Wu. Semi-supervised optimal transport for heterogeneous domain adaptation. In _IJCAI_, volume 7, pages 2969-2975, 2018.

Appendix

### Proof of Proposition 1

**Proposition 4**.: _Let \(\Gamma\) be a doubly stochastic matrix and the matrices \(C_{x}\) and \(C_{y}\) given by_

\[C_{x}=(\|x_{i}-x_{j}\|_{2}^{2})_{i,j=1}^{n}=\mathbf{1}m_{x}^{T}-2 X^{T}X+m_{x}\mathbf{1}^{T},\] \[C_{y}=(\|y_{i}-y_{j}\|_{2}^{2})_{i,j=1}^{n}=\mathbf{1}m_{y}^{T}-2 Y^{T}Y+m_{y}\mathbf{1}^{T},\]

_then it holds that_

\[\langle C_{x}\Gamma,\Gamma C_{y}\rangle=\langle 2X\Gamma Y^{T},2X\Gamma Y^{T} \rangle+\langle L,\Gamma\rangle+2\mathbf{1}^{T}m_{y}\mathbf{1}^{T}m_{x},\]

_where \(L=2nm_{x}m_{y}^{T}-4m_{x}\mathbf{1}^{T}Y^{T}Y-4X^{T}X\mathbf{1}m_{y}^{T}\)._

Proof.: The proposition follows by the following straightforward computations, where we just expand the expressions and use \(\mathbf{1}=\Gamma\mathbf{1}=\Gamma^{T}\mathbf{1}\), thus

\[\operatorname{tr}(C_{x}\Gamma C_{y}\Gamma^{T}) =\operatorname{tr}((\mathbf{1}m_{x}^{T}-2X^{T}X+m_{x}\mathbf{1}^{ T})\Gamma(\mathbf{1}m_{y}^{T}-2Y^{T}Y+m_{y}\mathbf{1}^{T})\Gamma^{T})\] \[=m_{x}^{T}\Gamma(\mathbf{1}m_{y}^{T}-2Y^{T}Y+m_{y}\mathbf{1}^{T}) \Gamma^{T}\mathbf{1}\] \[\quad-2\operatorname{tr}(X^{T}X\Gamma(\mathbf{1}m_{y}^{T}-2Y^{T} Y+m_{y}\mathbf{1}^{T})\Gamma^{T})\] \[\quad+\mathbf{1}^{T}\Gamma(\mathbf{1}m_{y}^{T}-2Y^{T}Y+m_{y} \mathbf{1}^{T})\Gamma^{T}m_{x}\] \[=m_{x}^{T}\mathbf{1}m_{y}^{T}\mathbf{1}-2m_{x}^{T}\Gamma Y^{T}Y \mathbf{1}+nm_{x}^{T}\Gamma m_{y}\] \[\quad-2m_{y}^{T}\Gamma^{T}X^{T}X\mathbf{1}+4\operatorname{tr}(X^ {T}\Lambda YY^{T}Y\Gamma^{T})-2\mathbf{1}^{T}X^{T}X\Gamma m_{y}\] \[\quad+nm_{y}^{T}\Gamma^{T}m_{x}-2\mathbf{1}^{T}Y^{T}Y\Gamma^{T}m_ {x}+\mathbf{1}^{T}m_{y}\mathbf{1}^{T}m_{x}\] \[=4\operatorname{tr}(X^{T}X\Gamma Y^{T}Y\Gamma^{T})-4m_{x}^{T} \Gamma Y^{T}Y\mathbf{1}+2nm_{x}^{T}\Gamma m_{y}\] \[\quad-4\mathbf{1}^{T}X^{T}X\Gamma m_{y}+2\mathbf{1}^{T}m_{y} \mathbf{1}^{T}m_{x}\] \[=\langle 2X\Gamma Y^{T},2X\Gamma Y^{T}\rangle\] \[\quad+\langle 2nm_{x}m_{y}^{T}-4m_{x}\mathbf{1}^{T}Y^{T}Y-4X^{T}X \mathbf{1}m_{y}^{T},\Gamma\rangle\] \[\quad+2\mathbf{1}^{T}m_{y}\mathbf{1}^{T}m_{x}.\]

### Proof of Proposition 3

**Proposition 6**.: _Assume that the extreme points \(\{x_{k}\}_{k}\) of the convex set described by \(Ax\leq b\) are known. When adding a constraint \(A_{N}^{T}x\leq b_{N}\), the additional extreme points are linear combinations of pairs of existing extreme points \(x_{k_{1}}\) and \(x_{k_{2}}\) both satisfying the same \(r-1\) constraints with equality and \(A_{N}^{T}x_{k_{1}}\leq b_{n}\) and \(A_{N}^{T}x_{k_{2}}\geq b_{n}\) so that the combination satisfies \(A_{N}^{T}(\lambda x_{k_{1}}+(1-\lambda)x_{k_{2}})=b_{N}\)._

Proof.: Let \(W\) be a matrix whose columns consist of the extreme points defined by the \(N-1\) constraints \(Ax\leq b\). Also, let \(a_{N}^{T}x\leq b_{N}\) be an additional constraint, \(e_{k}\) be a unit vector with \(1\) on position \(k\), and let \(\alpha\) parametrize the convex cone on \(W\), i.e. \(1^{T}\alpha=1\), \(\alpha\geq 0\) so that \(AW\alpha\leq b\) describes all points in the convex set. Suppose that \(B_{k}\) describes the indices of the constraints that define the kth extreme point by letting \(A_{B_{k}}\) be the sub matrix of \(A\) including the rows denoted by the indices in \(B_{k}\). Then \(A_{B_{k}}We_{k}=b_{B_{k}}\). It then follows that \(AW(\lambda e_{k_{1}}+(1-\lambda)e_{k_{2}})_{j}=b_{j}\) if and \(j\in B_{k_{1}}\) and \(j\in B_{k_{2}}\) and \(0\leq\lambda\leq 1\). The construction in the proposition, \(\lambda x_{k_{1}}+(1-\lambda)x_{k_{2}}\) so that \(A_{N}^{T}(\lambda x_{k_{1}}+(1-\lambda)x_{k_{2}})=b_{N}\) then satisfies \(r\) constraints with equality and all other constraints with inequality, i.e. the point is an extreme point to the set. We also note that every multiple combination (three or more) of extreme points sharing \(r-1\) constraints are not extreme points as they are linear combination of the pairs given by \(\alpha(\lambda_{1}x_{1}+(1-\lambda_{1})x_{2})+(1-\alpha)(\lambda_{2}x_{2}+(1- \lambda_{2})x_{3})\). Every linear combination of pairs of points sharing less than \(r-1\) constraints will not satisfy \(r\) constraints, i.e. they are not extreme points.

### A description of the implementation of the extreme point method

The handling of the extreme points in the paper is done by keeping track of the extreme points, their connection to the boundary constraints, and lookup tables for the adjacent extreme points, i.e., extreme points that satisfies the same \(r-1\) constraints with equality, where \(r\) is the rank of the problem. Let the extreme points be described in the matrix \(E\) where each column describes an extreme point. Thus \(AE\leq b\mathbf{1}\), if the constraints are described by the matrix \(A\) and vector \(b\) such that \(Ax\leq b\) is the constraint equations.

Let the adjacency be described in a (sparse) matrix with binary elements \(D\) where \(D_{i,j}=1\) if extreme point \(i\) and \(j\) are adjacent. Let us also keep track of the constraints that are satisfied by an extreme point with equality. For this purpose let \(B\) be a matrix with binary elements in which the element \(B_{i,j}=1\) if extreme point \(j\) satisfies constraint \(i\) with equality. Thus, \(D_{i,j}=1\) if \((B^{T}B)_{i,j}=r-1\). This is one of the computational drivers for the proposed extreme point method.

When a new constraint \((A_{n},b_{n})\) is added, the new extreme points are generated by a linear combination of the infeasible extreme points \(\mathcal{I}:=\{i:A_{n}E_{i}>b_{n}\}\) and their adjacent feasible extreme points \(f_{i}:=\{j:D(i,j)=1\}\) where \(i\in\mathcal{I}\). Let \(\#\) indicate the number of elements of a finite set, then \(\sum_{i\in\mathcal{I}}\#(f_{i})\) is the number of new extreme points. We place the new extreme points in the matrix \(E\) by adding them in the end as a matrix \(P\). The new matrix containing the extreme points

\[E_{n}=(E\quad P)\]

The matrix keeping track of which extreme points satisfies which constraints with equality is extended with

\[B_{n}=\begin{pmatrix}B&C\\ 0&\mathbf{1}\end{pmatrix}\]

where \(C_{\cdot,k}=B_{\cdot,i}\odot B_{\cdot,(f_{i})_{k}}\), preferably implemented using bitwise operators. Here, the last row describes the newly added constraint and \(k\) a re-enumeration of the new extreme points.

The new adjacency matrix \(D_{n}\) can be concatenated with the old \(D\) and two additional matrices

\[D_{n}=\begin{pmatrix}D&O\\ O^{T}&N\end{pmatrix}\]

where \(O_{i,j}=1\) the old extreme point \(i\) is adjacent to the new extreme point \(j\). This information is already available for us, since the new extreme points are adjacent to \(f_{j}\). Finally, \(N_{i,j}=1\) if \((C^{T}C)_{i,j}=r-2\). This is by far the most computationally expensive operation in the proposed algorithm, which can be implemented with std::popcount in the standard c++ library. For 3-dimensional problems, around 120 bits needs to be compared between all new extreme points.

### Additional tests

### Convergence rate

The convergence proof in Proposition 3 does not include a rate of convergence. In Figure 4, we show the convergence trajectory for the problems \(\mathcal{U}\) and \(\mathcal{N}_{1}\) for 2-dimensional data. The tests show that the convergence rate is linear to its nature up to a number of iterations where the gap closes completely.

In comparison to the result of the proposed method we present the same convergence evaluation with the local search method in [20]. Figure5 shows the trajectory of convergence. The tests show that the rate of convergence is sublinear to its nature. The number of initializations needed to achieve a pre-determined accuracy increase with the number of points for the local search. Note here that in order to determine when to stop one needs to know the optimal value, which is not available for the local search method.

### Evaluation on the MNIST dataset

In this section we present the performance of the proposed method on shapes originating from the MNIST dataset [8]. Even though the arabic numerals are not entirely unique to reflections (e.g. "2" and "5") and rotations (e.g. "6" and "9"), the ability to compute the exact Gromov-Wasserstein discrepancy is of interest. When the images doesn't contain the same number of points, evenly distributed points are taken from the image with more points. The maximum number of points was set to 400. Number of iterations are presented in Table 3 and the computational time is presented in Table 4, showing that the proposed method works as anticipated on the data set. Examples of correspondances of numerals are shown in Figure 6.

Figure 4: Top row: The trajectory of the relative error for the proposed method for the problem \(\mathcal{U}\) on 2-dimensional data. Bottom row: The trajectory of the relative error for the proposed method for the problem \(\mathcal{N}_{1}\) on 2-dimensional data. The solid blue line indicates the mean convergence rate for 20 runs and the dashed red line indicates 1 standard deviation from the mean.

Figure 5: Top row: The trajectory of the relative error for the local search method for the problem \(\mathcal{U}\) on 2-dimensional data. Bottom row: The trajectory of the relative error for the local search method for the problem \(\mathcal{N}_{1}\) on 2-dimensional data. The solid blue line indicates the mean convergence rate for 20 runs and the dashed red line indicates 1 standard deviation from the mean.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c} \hline  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline

[MISSING_PAGE_POST]

 0.5-1.1 \\ \end{tabular}
\end{table}
Table 4: Computational time for the distance between two numeral shapes to a relative error gap of \(10^{-8}\). Numbers presented are [median]s in the upper table and [min - max]s in the lower table. The number of random tests were 10 for each combination.

\begin{table}
\begin{tabular}{c|c c c c c c c c c} \hline  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline
0 & 92.5 & 55.5 & 98.5 & 75.5 & 115.5 & 106.5 & 80 & 81 & 90 & 90 \\
1 & & 21 & 46.5 & 36 & 70.5 & 50.5 & 47 & 41 & 50 & 49 \\
2 & & & 52 & 53.5 & 64 & 58.5 & 60.5 & 57.5 & 58 & 43 \\
3 & & & & 40.5 & 75 & 52.5 & 72.5 & 46.5 & 56 & 56.5 \\
4 & & & & & 84.5 & 57 & 63.5 & 65 & 73.5 & 61.5 \\
5 & & & & & & 50 & 53.5 & 48 & 64.5 & 49.5 \\
6 & & & & & & & 44 & 45.5 & 67 & 56.5 \\
7 & & & & & & & & 48.5 & 64 & 47.5 \\
8 & & & & & & & & & 62.5 & 55.5 \\
9 & & & & & & & & & & 52.5 \\ \hline \end{tabular}
\end{table}
Table 3: Number of iterations to compute the distance between two numeral shapes to a relative error gap of \(10^{-8}\). Numbers presented are median in the upper table and [min - max] in the lower table. The number of random tests were 10 for each combination.

Figure 6: Left image: Matching of the numeral ”6” to the numeral ”9”. Right image: Matching of the numeral ”2” to the numeral ”5”. The colors represent which point in one numeral corresponds to another point in the other numeral.