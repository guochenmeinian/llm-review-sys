# Shielding Regular Safety Properties

in Reinforcement Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

To deploy reinforcement learning (RL) systems in real-world scenarios we need to consider requirements such as safety and constraint compliance, rather than blindly maximizing for reward. In this paper we study RL with regular safety properties. We present a constrained problem based on the satisfaction of regular safety properties with high probability and we compare our setup to the some common constrained Markov decision processes (CMDP) settings. We also present a meta-algorithm with provable safety-guarantees, that can be used to shield the agent from violating the regular safety property during training and deployment. We demonstrate the effectiveness and scalability of our framework by evaluating our meta-algorithm in both the tabular and deep RL setting.

## 1 Introduction

The field of safe reinforcement learning (RL) [6; 28] has gained increasing interest, as practitioners begin to understand the challenges of applying RL in the real world [26]. There exist several distinct paradigms in the literature, including constrained optimization [2; 20; 49; 58; 62; 74], logical constraint satisfaction [17; 24; 36; 37; 38; 66], safety-critical control [15; 19; 53], all of which are unified by prioritizing safety- and risk-awareness during the decision making process.

Constrained Markov decision processes (CMDP) [4] have emerged as a popular framework for modelling safe RL, or RL with constraints. Typically, the goal is to obtain a policy that maximizes reward while simultaneously ensuring that the expected cumulative cost remains below a pre-defined threshold. A key limitation of this setting is that constraint violations are enforced in expectation rather than with high probability, the constraint thresholds also have limited semantic meaning, can be very challenging to tune and in some cases inappropriate for highly safety-critical scenarios [66]. Furthermore, the cost function in the CMDP is typically Markovian and thus fails to capture a significantly expressive class of safety properties and constraints.

Regular safety properties [9] are interesting because for all but the simplest properties the corresponding cost function is non-Markovian. Our problem setup consists of the standard RL objective with regular safety properties as constraints, we note that there has been a significant body of work that combines temporal logic constraints with RL [17; 24; 36; 37; 38; 66], although many of these do not explicitly separate reward and safety in the same way that we do.

Our approach relies on shielding [3], which is a safe exploration strategy that ensures the satisfaction of temporal logic constraints by deploying the learned policy in conjunction with a reactive system

Figure 1: Diagrammatic representation of runtime verification and shielding.

that overrides any _unsafe_ actions. Most shielding approaches typically make highly restrictive assumptions, such as full knowledge of the environment dynamics [3], or access to a simulator [29], although there has been recent work to deal with these restrictions [30; 39; 73]. In this paper, we opt for the most permissive setting, where the dynamics of the environment are unknown, runtime verification of the agent is realized by finite horizon model checking with a learned approximation of the environment dynamics. However, in principle our framework is flexible enough to accommodate more standard model checking procedures as long as certain assumptions are met.

Our approach can be summarised as an online shielding approach (see Fig. 1), that dynamically identifies unsafe actions during training and deployment, and deploys a safe 'backup policy' when necessary. We summarise the main contributions of our paper as follows:

(1) We state a constrained RL problem based on the satisfaction of regular safety properties with high probability, and we identify the conditions whereby our setup generalizes several CMDP settings, including _expected_ and _probabilistic cumulative cost_ constraints.

(2) We present several model checking algorithms that can verify the finite-horizon satisfaction probability of regular safety properties, this includes statistical model checking procedures that can be used if either the transition probabilities are unavailable or if the state space is too large.

(3) We develop a set of sample complexity results for the statistical model checking procedures introduced in point (2), which are then used to develop a shielding meta-algorithm with provable safety guarantees, even in the most permissive setting (i.e., no access to the transition probabilities).

(4) We empirically demonstrate the effectiveness of our framework on a variety of regular safety properties in both a tabular and deep RL settings.

## 2 Related Work

**Safety Paradigms in Reinforcement Learning.** There exist many safety paradigms in RL, the most popular being constrained MDPs. For CMDPs several constrained optimization algorithms have been developed, most are gradient-based methods built upon Lagrange relaxations of the constrained problem [20; 49; 58; 62] or projection-based local policy search [2; 74]. Model-based approaches to CMDP [7; 11; 41; 64] have also gathered recent interest as they enjoy better sample complexity than their model-free counterparts, which can be imperative for safe learning [44].

Linear Temporal Logic (LTL) constraints [17; 24; 36; 37; 38; 66] for RL have been developed as an alternative to CMDPs to specify stricter and more expressive constraints. The LTL formula is typically treated as the entire task specification, although some works have aimed to separate LTL satisfaction and reward into two distinct objectives [66]. The typical procedure in this setting is to identify end components of the MDP that satisfy the LTL constraint and construct a corresponding reward function such that the optimal policy satisfies the LTL constraint with maximal probability. Formal PAC-style guarantees have been developed for this setting [27; 36; 66; 71] although they typically rely on non-trivial assumptions. We note that LTL constraints can capture regular safety properties, although we explicitly separate reward and safety, making the work in this paper distinct from previous work.

More rigorous safety-guarantees can be obtained by using _safety filters_[3], _control barrier functions_ (CBF) [5], and _model predictive safety certification_ (MPSC) [67; 68]. To achieve zero-violation training these methods typically assume that the dynamics of the system are known and thus they are typically restricted to low-dimensional systems. While these methods come from safety-critical control, they are closely related to safe reinforcement learning [15].

**Learning Over Regular Structures.** RL and regular properties have been studied in conjunction before, perhaps most famously as 'Reward Machines' [42; 43] - a type of finite state automaton that specifies a different reward function at each automaton state. Reward machines do not explicitly deal with safety, rather non-Markovian reward functions that depend on histories distinguished by regular languages. Several methods have been developed to exploit the structure of these automata and dramatically speed up learning [42; 43; 55; 61], e.g., _counter factual experiences_.

Regular decision processes (RDP) [13] are a specific class non-Markovian DPs [8] that have also been studied in several works [13; 22; 51; 59; 65]. Most of these works are theoretical and slightly out-of-scope for this paper, as the RDP setting does not explicitly handle safety and encompasses both non-Markovian rewards and transition probabilities.

**Shielding.** From formal methods, shielding for safe RL [3] forces hard constraints on policies, using a reactive system that'shields' the agent from taking unsafe actions. Synthesising a _correct-by-construction_ reactive'shield' typically requires access to the environment dynamics and can be computationally demanding when the state or action space is large. Several recent works have aimed to scale the concept of shielding to more general settings, relaxing the prerequisite assumptions for shielding, by either only assuming access to a 'black box' model for planning [29], or learning a world model from scratch [30, 39, 73]. Other notable works that can be viewed as shielding include, MASE [69] - a safe exploration algorithm with access to an 'emergency reset button', and Recovery-RL [63] - which has access to a'recovery policy' that is activated when the probability of reaching an unsafe state is too high. A simple form of shielding with LTL specifications has also been considered [37, 54], but experimentally these methods have only been tested in quite simple settings.

## 3 Preliminaries

For a finite set \(\mathcal{S}\), let \(Pow(\mathcal{S})\) denote the power set of \(\mathcal{S}\). Also, let \(Dist(\mathcal{S})\) denote the set of distributions over \(\mathcal{S}\), where a distribution \(\mu:\mathcal{S}\rightarrow[0,1]\) is a function such that \(\sum_{s\in\mathcal{S}}\mu(s)=1\). Let \(\mathcal{S}^{*}\) and \(\mathcal{S}^{\omega}\) denote the set of finite and infinite sequences over \(\mathcal{S}\) respectively. The set of all finite and infinite sequences is denoted \(\mathcal{S}^{\infty}=\mathcal{S}^{*}\cup\mathcal{S}^{\omega}\). We denote as \(|\rho|\) the length of a sequence \(\rho\in\mathcal{S}^{\infty}\), where \(|\rho|=\infty\) if \(\rho\in\mathcal{S}^{\omega}\). We also denote as \(\rho[i]\) the \(i+1\)-th element of a sequence, when \(i<|\rho|\), and we denote as \(\rho[i]=\rho[|\rho|-1]\) the last element of a sequence, when \(\rho\in\mathcal{S}^{*}\). A sequence \(\rho_{1}\) is a prefix of \(\rho_{2}\), denoted \(\rho_{1}\preceq\rho_{2}\), if \(|\rho_{1}|\leq|\rho_{2}|\) and \(\rho_{1}[i]=\rho_{2}[i]\) for all \(0\leq i\leq|\rho_{1}|\). A sequence \(\rho_{1}\) is a proper prefix of \(\rho_{2}\), denoted \(\rho_{1}\prec\rho_{2}\), if \(\rho_{1}\preceq\rho_{2}\) and \(\rho_{1}\neq\rho_{2}\).

**Labelled MDPs and Markov Chains.** An MDP is a tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{P}_{0},\mathcal{R}, AP,L)\), where \(\mathcal{S}\) and \(\mathcal{A}\) are finite sets of states and actions resp.; \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow Dist(\mathcal{S})\) is the _transition function_; \(\mathcal{P}_{0}\in Dist(\mathcal{S})\) is the _initial state distribution_; \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is the _reward function_; \(AP\) is a set of _atomic propositions_, where \(\Sigma=Pow(AP)\) is the _alphabet_ over \(AP\); and \(L:\mathcal{S}\rightarrow\Sigma\) is a _labelling function_, where \(L(s)\) denotes the set of atoms that hold in a given state \(s\in\mathcal{S}\). A memory-less (stochastic) _policy_ is a function \(\pi:\mathcal{S}\to Dist(\mathcal{A})\) and its _value function_, denoted \(V_{\pi}:\mathcal{S}\rightarrow\mathbb{R}\) is defined as the _expected reward_ from a given state under policy \(\pi\), i.e., \(V_{\pi}(s)=\mathbb{E}_{\pi}[\sum_{t=0}^{T}\mathcal{R}(s_{t},a_{t})|s_{0}=s]\), where \(T\) is a fixed episode length. Furthermore, denote as \(\mathcal{M}_{\pi}=(\mathcal{S},\mathcal{P}_{\pi},\mathcal{P}_{0},AP,L)\) the _Markov chain_ induced by a fixed policy \(\pi\), where the transition function is such that \(\mathcal{P}_{\pi}(s^{\prime}|s)=\sum_{a\in\mathcal{A}}\mathcal{P}(s^{\prime}|s,a)\pi(a|s)\). A path \(\rho\in\mathcal{S}^{\infty}\) through \(\mathcal{M}_{\pi}\) is a finite (or infinite) sequence of states. Using standard results from measure theory it can be shown that the set of all paths \(\{\rho\in\mathcal{S}^{\omega}\mid\rho_{pref}\preceq\rho\}\) with a common prefix \(\rho_{pref}\) is measurable [9].

**Probabilistic CTL.** (PCTL) [9] is a branching-time temporal logic for specifying properties of stochastic systems. A well-formed PCTL property can be constructed with the following grammar,

\[\Phi::= \text{true}\mid a\mid\neg\Phi\mid\Phi\wedge\Phi\mid\mathbb{P}_{ \bowtie p}[\varphi]\] \[\varphi::= X\Phi\mid\Phi U\Phi\mid\Phi U^{\leq n}\Phi\]

where \(a\in AP\), \(\bowtie\in\{<,>,\leq,\geq\}\) is a binary comparison operator, and \(p\in[0,1]\) is a probability. Negation \(\neg\) and conjunction \(\wedge\) are the familiar logical operators from propositional logic, and next \(X\), until \(U\) and bounded until \(U^{\leq n}\) are the temporal operators from CTL [9]. We make the distinction here between state formula \(\Phi\) and path formula \(\varphi\). The satisfaction relation for state formula \(\Phi\) is defined in the standard way for Boolean connectives. For probabilistic quantification we say that \(s\models\mathbb{P}_{\bowtie p}[\varphi]\) iff \(\Pr(s\models\varphi):=\Pr(\rho\in S^{\omega}\mid\rho[0]=s,\rho\models\varphi) \bowtie p\). Let \(\Pr^{\mathcal{M}}(s\models\varphi)\) be the probability w.r.t. the Markov chain \(\mathcal{M}\). For path formula \(\varphi\) the satisfaction relation is as follows,

\[\begin{array}{lcl}\rho&\models X\Phi&\text{iff}&\rho[1]\models\Phi\\ \rho&\models\Phi_{1}U\Phi_{2}&\text{iff}&\exists j\geq 0\,s.t.\,(\rho[j]\models \Phi_{2}\wedge\forall 0\leq i<j,\rho[i]\models\Phi_{1})\\ \rho&\models\Phi_{1}U^{\leq n}\Phi_{2}&\text{iff}&\exists 0\leq j\leq n\,s.t.\,(\rho[j] \models\Phi_{2}\wedge\forall 0\leq i<j,\rho[i]\models\Phi_{1})\end{array}\]

From the standard operators of propositional logic we may derive disjunction \(\vee\), implication \(\rightarrow\) and coimplication \(\leftrightarrow\). We also note that the common temporal operators 'eventually' \(\Diamond\) and 'always' \(\Box\), and their bounded counterparts \(\Diamond^{\leq n}\) and \(\Box^{\leq n}\) can be derived in a familiar way, i.e., \(\Diamond\Phi::=\text{true}\,U\Phi\), \(\Box\)\(\Phi::=\neg\Diamond\neg\Phi\), resp. \(\Diamond^{\leq n}\Phi::=\text{true}\,U^{\leq n}\Phi\), \(\Box^{\leq n}\Phi:=\neg\Diamond^{\leq n}\neg\Phi\).

**Regular Safety Property.** A linear time property \(P_{\text{safe}}\subseteq\Sigma^{\omega}\) over the alphabet \(\Sigma\) is a safety property if for all words \(w\in\Sigma^{\omega}\setminus P_{\text{safe}}\), there exists a finite prefix \(w_{pref}\) of \(w\) such that \(P_{\text{safe}}\cap\{w^{\prime}\in\Sigma^{\omega}\mid\)\(w_{pref}\preceq w^{\prime}\}=\varnothing\). Any such sequence \(w_{pref}\) is called a _bad prefix_ for \(P_{\textit{safe}}\), a bad prefix \(w_{pref}\) is called _minimal_ iff there does not exist \(w^{\prime\prime}\prec w_{pref}\) such that \(w^{\prime\prime}\) is a bad prefix for \(P_{\textit{safe}}\). Let \(\textit{BadPref}(P_{\textit{safe}})\) and \(\textit{MinBadPref}(P_{\textit{safe}})\) denote the set of of bad and minimal bad prefixes resp.

A safety property \(P_{\textit{safe}}\in\Sigma^{\omega}\) is _regular_ if the set \(\textit{BadPref}(P_{\textit{safe}})\) constitutes a regular language. That is, there exists some _deterministic finite automata_ (DFA) that accepts the bad prefixes for \(P_{\textit{safe}}\)[9], that is, a path \(\rho\in\mathcal{S}^{\omega}\) is 'unsafe' if the trace \(\textit{trace}(\rho)=L(\rho[0]),L(\rho[1]),\ldots\in\Sigma^{\omega}\) is accepted by the corresponding DFA.

**Definition 3.1** (Dfa).: _A deterministic finite automata is a tuple \(\mathcal{D}=(\mathcal{Q},\Sigma,\Delta,\mathcal{Q}_{0},\mathcal{F})\), where \(\mathcal{Q}\) is a finite set of states, \(\Sigma\) is a finite alphabet, \(\Delta:\mathcal{Q}\times\Sigma\rightarrow\mathcal{Q}\) is the transition function, \(\mathcal{Q}_{0}\) is the initial state, and \(\mathcal{F}\subseteq\mathcal{Q}\) is the set of accepting states. The extended transition function \(\Delta^{*}\) is the total function \(\Delta^{*}:\mathcal{Q}\times\Sigma^{*}\rightarrow\mathcal{Q}\) defined recursively as \(\Delta^{*}(q,w)=\Delta(\Delta^{*}(q,w\setminus w\!\downarrow),w\!\downarrow)\). The language accepted by DFA \(\mathcal{D}\) is denoted \(\mathcal{L}(\mathcal{D})=\{w\in\Sigma^{*}\mid\Delta^{*}(\mathcal{Q}_{0},w)\in \mathcal{F}\}\)._

Furthermore, we denote as \(P_{\textit{safe}}^{H}\subseteq\Sigma^{\omega}\) the corresponding finite-horizon safety property for \(H\in\mathbb{Z}_{+}\), where for all words \(w\in\Sigma^{\omega}\setminus P_{\textit{safe}}^{H}\) there exists \(w_{pref}\preceq w\) such that \(|w_{pref}|\leq H\) and \(w_{pref}\in\textit{BadPref}(P_{\textit{safe}})\). We model check regular safety properties by synchronizing the DFA and Markov chain in a standard way - by computing the product Markov chain.

**Definition 3.2** (Product Markov Chain).: _Let \(\mathcal{M}=(\mathcal{S},\mathcal{P},\mathcal{P}_{0},AP,L)\) be a Markov chain and \(\mathcal{D}=(\mathcal{Q},\Sigma,\Delta,\mathcal{Q}_{0},\mathcal{F})\) be a DFA. The product Markov chain is \(\mathcal{M}\otimes\mathcal{D}=(\mathcal{S}\times\mathcal{Q},\mathcal{P}^{ \prime},\mathcal{P}_{0}^{\prime},\{accept\},L^{\prime})\), where \(L^{\prime}(\langle s,q\rangle)=\{accept\}\) if \(q\in\mathcal{F}\) and \(L^{\prime}(\langle s,q\rangle)=\varnothing\) on/u, \(\mathcal{P}_{0}^{\prime}(\langle s,q\rangle)=\mathcal{P}_{0}(s)\) if \(q=\Delta(Q_{0},L(s))\) and \(0\) on/u, and \(\mathcal{P}^{\prime}(\langle s^{\prime},q^{\prime}\rangle|\langle s,q\rangle)= \mathcal{P}(s^{\prime}|)\) if \(q^{\prime}=\Delta(q,L(s^{\prime}))\) and \(0\) on/u._

To compute the satisfaction probability of \(P_{\textit{safe}}\) for a given state \(s\in\mathcal{S}\) we consider the set of paths \(\rho\in\mathcal{S}^{\omega}\) from \(s\) and the corresponding trace in the DFA. We provide the following definition.

**Definition 3.3** (Satisfaction probability for \(P_{\textit{safe}}\)).: _Let \(\mathcal{M}=(\mathcal{S},\mathcal{P},\mathcal{P}_{0},AP,L)\) be a Markov chain and let \(\mathcal{D}=(\mathcal{Q},\Sigma,\Delta,\mathcal{Q}_{0},\mathcal{F})\) be the DFA such that \(\mathcal{L}(\mathcal{D})=BadPref(P_{\textit{safe}})\). For a path \(\rho\in\mathcal{S}^{\omega}\) in the Markov chain, let \(\textit{trace}(\rho)=L(\rho[0]),L(\rho[1]),\ldots\in\Sigma^{\omega}\) be the corresponding word over \(\Sigma=Pow(AP)\). From a given state \(s\in\mathcal{S}\) the satisfaction probability for \(P_{\textit{safe}}\) is defined as follows,_

\[\Pr^{\mathcal{M}}(s\models P_{\textit{safe}}):=\Pr^{\mathcal{M}}(\rho\in \mathcal{S}^{\omega}\mid\rho[0]=s,\textit{trace}(\rho)\not\in\mathcal{L}( \mathcal{D}))\]

_Perhaps more importantly, we note that this satisfaction probability can be written as the following reachability probability in the product Markov chain,_

\[\Pr^{\mathcal{M}}(s\models P_{\textit{safe}})=\Pr^{\mathcal{M}\otimes \mathcal{D}}(\langle s,q_{s}\rangle\not\models\Diamond accept)\]

_where \(q_{s}=\Delta(\mathcal{Q}_{0},L(s))\) and \(\Diamond accept\) is a PCTL path formula that reads, 'eventually accept' [9]._

For the corresponding finite-horizon safety property \(P_{\textit{safe}}^{H}\) we state the following result.

**Proposition 3.4** (Satisfaction probability for \(P_{\textit{safe}}^{H}\)).: _Let \(\mathcal{M}\) and \(\mathcal{D}\) be the MDP and DFA in Defn. 3.3. For a path \(\rho\in\mathcal{S}^{\omega}\) in the Markov chain, let \(\textit{trace}_{H}(\rho)=L(\rho[0]),L(\rho[1])\ldots,L(\rho[H])\) be the corresponding finite word over \(\Sigma=Pow(AP)\). For a given state \(s\in\mathcal{S}\) the finite horizon satisfaction probability for \(P_{\textit{safe}}\) is defined as follows,_

\[\Pr^{\mathcal{M}}(s\models P_{\textit{safe}}^{H}):=\Pr^{\mathcal{M}}(\rho\in \mathcal{S}^{\omega}\mid\rho[0]=s,\textit{trace}_{H}(\rho)\not\in\mathcal{L}( \mathcal{D}))\]

_where \(H\in\mathbb{Z}_{+}\) is some fixed model checking horizon. Similar to before, we show that the finite horizon satisfaction probability can be written as the following bounded reachability probability._

\[\Pr^{\mathcal{M}}(s\models P_{\textit{safe}}^{H})=\Pr^{\mathcal{M}\otimes \mathcal{D}}(\langle s,q_{s}\rangle\not\models\Diamond^{H}accept)\]

_where \(q_{s}=\Delta(\mathcal{Q}_{0},L(s))\) is as before and \(\Diamond^{\leq H}accept\) is the corresponding step-bounded PCTL path formula that reads, 'eventually accept in H timesteps'._

The unbounded reachability probability can be computed by solving a system of linear equations, the bounded reachability probability can be computed with \(\mathcal{O}(H)\) matrix multiplications, in both cases the time complexity of the procedure is a polynomial in the size of the product Markov chain [9].

## 4 Problem Setup

In this paper, we are interested in the quantitative model checking of regular safety properties for a fixed finite horizon \(H\) and in the context of episodic RL, i.e., where the length of the episode \(T\) is fixed. In particular, at every timestep we constrain the (step-bounded) reachability probability \(\Pr(\langle s,q\rangle\not\models\Diamond^{\leq H}accept\)) in the product Markov chain \(\mathcal{M}_{\pi}\otimes\mathcal{D}\). We assume that \(H\) is chosen so as to avoid any irrecoverable states [35, 64], i.e., those that lead to a violation of the safety property no matter the sequence of actions taken, the precise details of this notion are presented in Section 6. We specify the following constrained problem,

**Problem 4.1** (Step-wise bounded regular safety property constraint).: _Let \(P_{\text{safe}}\) be a regular safety property, \(\mathcal{D}\) be the DFA such that \(\mathcal{L}(\mathcal{D})=\text{BadPref}(P_{\text{safe}})\) and \(\mathcal{M}\) be the MDP;_

\[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\Pr\big{(}\langle s_{t},q_{t} \rangle\models\Diamond^{\leq H}accept\big{)}\leq p_{1}\quad\forall t\in[0,T]\]

_where all probability is taken under the product Markov Chain \(\mathcal{M}_{\pi}\otimes\mathcal{D}\), \(p_{1}\in[0,1]\) is a probability threshold, \(H\) is the model checking horizon and \(T\) is the fixed episode length._

The hyperparameter \(p_{1}\) is be directly used to trade-off safety and exploration in a semantically meaningful way; \(p_{1}\) prescribes the probability of satisfying the finite-horizon safety property \(P_{\text{safe}}^{H}\) at each timestep. In particular, if \(p_{1}\) is sufficiently small then we can guarantee (with high-probability) that the regular safety property \(P_{\text{safe}}\) is satisfied for the entire episode length \(T\).

**Proposition 4.2**.: _Let \(P_{\text{safe}}^{T}\) denote the (episodic) regular safety property for a fixed episode length \(T\). Then satisfying \(\Pr\big{(}\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept\big{)}\leq p _{1}\) for all \(t\in[0,T]\) guarantees that \(\Pr(s_{0}\models P_{\text{safe}}^{T})\geq 1-p_{1}\cdot\lceil T/H\rceil\), where \(s_{0}\sim\mathcal{P}_{0}\) is the initial state._

**Comparison to CMDP.** In the remainder of this section, we compare our problem setup to various CMDP settings [4], with the aim of unifying different perspectives from safe RL. The purpose of this is to show that our proposed method for solving Problem 4.1 can also be used to satisfy other more common CMDP constraints. First, we define the following cost function that prescribes a scalar cost \(C>0\) when the regular safety property \(P_{\text{safe}}\) is violated and \(0\) otherwise.

**Definition 4.3** (Cost function).: _Let \(P_{\text{safe}}\) be a regular safety property and let \(\mathcal{D}\) be the DFA such that \(\mathcal{L}(\mathcal{D})=BadPref(P_{\text{safe}})\), modified such that for all \(q\in\mathcal{F}\), \(q\to\mathcal{Q}_{0}\). The cost function is then defined as,_

\[\mathcal{C}(\langle s,q\rangle)=\begin{cases}C&\text{if }accept\in L^{\prime}( \langle s,q\rangle)\\ 0&\text{otherwise}\end{cases}\]

_where \(C>0\) is some generic scalar cost and \(L^{\prime}\) is the labelling function defined in Def. 3.2._

_Resetting the DFA._ Rather than reset the environment, the DFA is reset once it reaches an accepting state, so as to measure the rate of constraint satisfaction over a fixed episode length \(T\). This can easily be realized by replacing any outgoing transitions from the accepting states with transitions back to the initial state, i.e., for all \(q\in\mathcal{F}\), \(q\to\mathcal{Q}_{0}\).

_Non-Markovian costs._ The cost function is Markov on the product states \(\langle s,q\rangle\in\mathcal{S}\times\mathcal{Q}\). However, in most cases the cost function is non-Markovian in the original state space \(\mathcal{S}\), since the automaton state \(q\in\mathcal{Q}\) could depend on some arbitrary history of states. Thus our problem setup generalizes the standard CMDP framework with non-Markovian safety constraints.

_Invariant properties._ Invariant properties \(P_{inv}(\Phi)\), also written \(\Box\Phi\) ('always \(\Phi\)'), where \(\Phi\) is a propositional state formula, are the simplest type of safety properties where the cost function is still Markov in the original state space. In this case we are operating in the standard CMDP framework, we also note that checking invariant properties with a fixed model checking horizon has been studied in previous works, as _bounded safety_[29, 30] and _safety for a finite horizon_[45].

The most common type of CMDP constraints are _expected cumulative (cost) constraints_, which constrain the expected cost below a given threshold.

**Problem 4.4** (Expected cumulative constraint [4, 58]).: \[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\mathbb{E}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\left[\sum_{t=0}^{T}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right]\leq d_{1}\]

_where \(d_{1}\in\mathbb{R}_{+}\) is the cost threshold and \(T\) is the fixed episode length.__Probabilistic cumulative (cost) constraints_, are a stricter class of constraints that constrain the cumulative cost with high probability, rather than in expectation.

**Problem 4.5** (Probabilistic cumulative constraint [18, 56]).: \[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\mathbb{P}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\left[\sum_{t=0}^{T}\mathcal{C} (\langle s_{t},q_{t}\rangle)\leq d_{2}\right]\geq 1-\delta_{2}\]

_where \(d_{2}\in\mathbb{R}_{+}\) is the cost threshold, \(\delta_{2}\) is a tolerance parameter, and \(T\) is the fixed episode length._

We also consider _instantaneous constraints_, which bound the cost 'almost surely' at each timestep \(t\in[0,T]\). These are an even stricter type of constraint for highly safety-critical applications.

**Problem 4.6** (Instantaneous constraint [23, 60, 69]).: \[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\mathbb{P}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\big{[}\mathcal{C}(\langle s_{ t},q_{t}\rangle)\leq d_{3}\big{]}=1\quad\forall t\in[0,T]\]

_where \(d_{3}\in\mathbb{R}_{+}\) is the cost threshold and \(T\) is the fixed episode length._

In particular, these problems define a constrained set of feasible policies \(\Pi\). We make the distinction here between a feasible policy and a solution to the problem, the former being any policy satisfying the constraints of the problem and the later being the optimal policy within the feasible set \(\Pi\).

**Theorem 4.7**.: _A feasible policy for Problem 4.1 is also a feasible policy for Problems 4.4, 4.5 and 4.6 under specific parameter settings for \(p_{1}\), \(d_{1}\), \(d_{2}\) and \(\delta_{2}\), and \(d_{3}\)._

In Appendix G we provide a full set of statements that outline the relationships between the constrained problems presented in this section. The significance of these results is that they demonstrate by solving Problem 4.1 with our proposed method we can obtain feasible policies for Problems 4.4, 4.5 and 4.6, although for most of these problems there is no direct relationship between our problem setup, in particular we can say little about whether the optimal policy for one problem is necessarily optimal for another. Nevertheless, we find it interesting to explore the relationships between our setup and other perhaps more common constrained RL problems.

## 5 Model checking

In this section we outline several procedures for checking the finite-horizon satisfaction probability of regular safety properties and we summarise the settings in which they can be used.

**Assumption 5.1**.: _We are given access to the 'true' transition probabilities \(\mathcal{P}\)._

**Assumption 5.2**.: _We are given access to a 'black box' model that perfectly simulates the 'true' transition probabilities \(\mathcal{P}\)._

**Assumption 5.3**.: _We are given access to an approximate dynamic model \(\widehat{\mathcal{P}}\approx\mathcal{P}\), where the total variation (TV) distance \(D_{TV}(\mathcal{P}_{\pi}(\cdot\mid s),\widehat{\mathcal{P}}_{\pi}(\cdot\mid s ))\leq\epsilon/H\), for all \(s\in\mathcal{S}\).1_

Footnote 1: For two discrete probability distributions \(\mu_{1}\) and \(\mu_{2}\) over the same space \(\mathcal{X}\) the TV distance is defined as: \(D_{TV}(\mu_{1}(\cdot),\mu_{2}(\cdot))=\frac{1}{2}\sum_{x\in X}|\mu_{1}(x)-\mu_ {2}(x)|\)

**Exact model checking.** Under Assumption 5.1 we can precisely compute the (finite horizon) satisfaction probability of \(P_{\text{safe}}\), in the Markov chain \(\mathcal{M}_{\pi}\) induced by the fixed policy \(\pi\) in time \(\mathcal{O}(\text{\rm poly}(\text{size}(\mathcal{M}_{\pi}\otimes\mathcal{D})) \cdot H)\)[9], where \(\mathcal{D}\) is the DFA such that \(\mathcal{L}(\mathcal{D})=BadPref(P_{\text{safe}})\) and \(H\) is the model checking horizon. \(H\) should not be too large and so the complexity of exact model checking ultimately depends on the size of the product \(\mathcal{M}_{\pi}\otimes\mathcal{D}\), and so if the size of either the MDP or DFA is too large then exact model checking may be infeasible.

**Monte-Carlo model checking.** To address the limitations of exact model checking, we can drop Assumption 5.1. Rather, under Assumption 5.2, we can sample sufficiently many paths from a 'black box' model of the environment dynamics and estimate the reachability probability \(\Pr(\langle s,q\rangle\models\zeta^{\leq H}accept)\) in the product Markov chain \(\mathcal{M}_{\pi}\otimes\mathcal{D}\), by computing the proportion of accepting paths. Using statistical bounds, such as Hoeffding's inequality [40] or Bernstein-type bounds [52], we can bound the error of this estimate, with high probability.

**Proposition 5.4**.: _Let \(\epsilon>0\), \(\delta>0\), \(s\in\mathcal{S}\) and \(H\geq 1\) be given. Under Assumption 5.2, we can obtain an \(\epsilon\)-approximate estimate for the probability \(\Pr(\langle s,q\rangle\models\lozenge\leq Haccept)\) with probability at least \(1-\delta\), by sampling \(m\geq\frac{1}{2\epsilon^{2}}\log\left(\frac{2}{\delta}\right)\) paths from the 'black box' model._We note that the time complexity of these statistical methods does not depend in the size of the product MDP or DFA, since the product states \(\langle s,q\rangle\in\mathcal{S}\times\mathcal{Q}\) can be computed _on-the-fly_, rather the time complexity depends on the horizon \(H\), the desired level of accuracy \(\epsilon\), failure probability \(\delta\).

**Model checking with approximate models.** In most realistic cases neither the 'true' transition probabilities nor a perfect 'black box' model is available to us before-hand. Under Assumption 5.3 we can model check with an 'approximate' model of the MDP dynamics, which can either be constructed ahead of time (offline) or learned from experience, with maximum likelihood (or similar). We can then either exact model check in with the 'approximate' probabilities, or if the MDP is too large, we can leverage statistical model checking by sampling paths from the 'approximate' model.

**Proposition 5.5**.: _Let \(\epsilon>0\), \(\delta>0\), \(s\in\mathcal{S}\) and \(H\geq 1\) be given. Under Assumption 5.3 we can make the following two statements:_

_(1) We can obtain an \(\epsilon\)-approximate estimate for \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) with probability \(1\) by exact model checking with the transition probabilities of \(\widehat{\mathcal{P}}_{\pi}\) in time \(\mathcal{O}(\text{poly}(\text{size}(\mathcal{M}_{\pi}\otimes\mathcal{D})) \cdot H)\)._

_(2) We can obtain an \(\epsilon\)-approximate estimate for \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) with probability at least \(1-\delta\), by sampling \(m\geq\frac{2}{\epsilon^{2}}\log\left(\frac{2}{\delta}\right)\) paths from the 'approximate' dynamics model \(\widehat{\mathcal{P}}_{\pi}\)._

## 6 Shielding the policy

At a high-level, the shielding meta-algorithm works by switching between the 'task policy' trained with RL to maximize rewards and a 'backup policy', which typically constitutes a low-reward, possibly rule-based policy that is guaranteed to be safe. In some cases this 'backup policy' may be available to us before training, although in most realistic cases it will need to be learned. In our case we switch from the 'task policy' to the 'backup policy' when the reachability probability \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) exceeds the probability threshold \(p_{1}\). To check this we can use any of the model checking procedures presented earlier. The 'backup policy' is used when the reachability probability exceeds \(p_{1}\). Intuitively if the 'backup policy' is guaranteed to be safe, then our system should satisfy the constraints of Problem 4.1, independent of the 'task policy'.

**Backup policy.** In general we assume no knowledge of the safety dynamics before training and so the 'backup policy' needs to be learned. In particular, we can use the cost function defined in Defn. 4.3 and train the 'backup policy' with RL to minimize the _expected discounted cost_ (\(\mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}\mathcal{C}(s_{t},q_{t})]\)). Importantly, we note that the cost function is defined on the product state space \(\mathcal{S}\times\mathcal{Q}\) and so the 'backup policy' must also operate on this state space, possibly leading to slower convergence. However, we can eliminate this issue entirely by training the 'backup policy' with _counterfactual experiences_[42, 43] - a method originally used for reward machines that generates additional synthetic data for the policy, by simulating experience from each automaton state.

**Meta Algorithm.** We now present the structure of the shielding meta-algorithm (see Algorithm 1). The precise realization of this algorithm can vary depending on problem setting, tabular, deep RL, etc., however the main structure of the algorithm remains the same. In particular, during interaction with the environment we shield the agent by checking that the reachability probability \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) does not exceed threshold \(p_{1}\). Then, with the new accumulated experience we update the 'task policy' denoted \(\pi_{\text{task}}\) and the 'backup policy' denoted \(\pi_{\text{safe}}\) with RL, and if need be 

[MISSING_PAGE_FAIL:8]

experiences [43]. In all cases, by separating reward and safety into two distinct policies, we are able to effectively trade-off the two objectives. Q-learning simply finds the best policy ignoring the costs, and Q-learning with penalties is able to find a safe policy, but struggles to meaningfully balance both objectives (see Fig. 2). Hyperparameter settings for all experiments are detailed in Appendix E. In addition, we provide an extensive series of ablation studies in Appendix F for these experiments. For example, we show that we don't loose much by using Monte Carlo model checking as opposed to exact model checking with the 'true' probabilities. We also show that tuning the cost coefficient \(C\) offers no meaningful way to trade-off reward and the probability of constraint satisfaction.

**Deep RL.** We deploy our version of Algorithm 1 built on DreamerV3 [34] on Atari Seaquest, provided as part of the Arcade Learning Environment (ALE)[10, 50]. We experiment with two different regular safety properties: (1) (\(\square\)-_surface_\(\rightarrow\square\)(_surface_\(\rightarrow\)_diver_)) \(\wedge\) (\(\square\)-_out-of-oxygen_) \(\wedge\) (\(\square\)-_hit_) and (2) \(\square\)_diver_\(\wedge\)

\(\neg\)_surface_\(\rightarrow\)\(\Diamond^{\leq 30}\)_surface_. We compare our approach to the base DreamerV3 algorithm and a version of DreamerV3 that implements the augmented Lagrangian penalty framework, similarly to [7, 41], for additional details see Appendix B.1.

Again our approach is able to effectively trade-off both objectives, while (base) DreamerV3 ignores the cost, the Lagrangian approach appears to learn a safe policy that is not always efficient in terms of reward (see Fig. 3). We refer the reader to Appendix D.2 for more details of the environment and an extended discussion.

**Separating Reward and Safety.** The separation of reward and safety objectives into two distinct policies has been demonstrated as an effective strategy towards safety-aware decision making [3, 30, 46, 63], in many cases the safety objective is simpler and can be more quickly learnt [46]. In our experiments it is clear that when the system enters a critical state, the 'backup policy' is able to efficiently guide the system back to a non-critical state where the task policy can continue collecting reward. However, there is evidence that the complete separation of policies is not always appropriate [31] and penalties or a slight coupling of the policies is required to stop the 'task' and 'backup policy' fighting for control of the system. Furthermore, by separating reward and safety, we typically loose any asymptotic convergence guarantees, similar to the situation faced for hierarchical RL [61], although there has been recent work to develop convergence guarantees for shielding [75].

## 8 Conclusion

In this paper we propose a shielding meta-algorithm for the runtime verification of regular safety properties, given as a probabilistic constraint on the system. We provide a thorough theoretical examination of the problem and develop probabilistic safety guarantees for the meta-algorithm, which hold under reasonable assumptions. Empirically, we demonstrate that shielding is able to effectively balance both reward and safety, in both the tabular and deep RL setting. A more thorough theoretical and empirical examinations of the conditions for when shielding is appropriate would be an interesting direction for future work.

Figure 3: Episode reward and violation rate for deep RL Atari Seaquest.

Figure 2: Episode reward and cost for tabular RL ‘colour’ gridworld environment.

## References

* Abbeel and Ng (2005) Pieter Abbeel and Andrew Y Ng. 2005. Exploration and apprenticeship learning in reinforcement learning. In _Proceedings of the 22nd international conference on Machine learning_. 1-8.
* Achiam et al. (2017) Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. 2017. Constrained policy optimization. In _International conference on machine learning_. PMLR, 22-31.
* Alshiekh et al. (2018) Mohammed Alshiekh, Roderick Bloem, Rudiger Ehlers, Bettina Konighofer, Scott Niekum, and Ufuk Topcu. 2018. Safe reinforcement learning via shielding. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 32.
* Altman (1999) Eitan Altman. 1999. _Constrained Markov decision processes: stochastic modeling_. Routledge.
* Ames et al. (2019) Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath, and Paulo Tabuada. 2019. Control barrier functions: Theory and applications. In _2019 18th European control conference (ECC)_. IEEE, 3420-3431.
* Amodei et al. (2016) Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. 2016. Concrete problems in AI safety. _arXiv preprint arXiv:1606.06565_ (2016).
* As et al. (2022) Yarden As, Ilnura Usmanova, Sebastian Curi, and Andreas Krause. 2022. Constrained policy optimization via bayesian world models. _arXiv preprint arXiv:2201.09802_ (2022).
* Bacchus et al. (1996) Fahiem Bacchus, Craig Boutilier, and Adam Grove. 1996. Rewarding behaviors. In _Proceedings of the National Conference on Artificial Intelligence_. 1160-1167.
* Baier and Katoen (2008) Christel Baier and Joost-Pieter Katoen. 2008. _Principles of model checking_. MIT press.
* Bellemare et al. (2013) M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. 2013. The Arcade Learning Environment: An Evaluation Platform for General Agents. _Journal of Artificial Intelligence Research_ 47 (jun 2013), 253-279.
* Berkenkamp et al. (2017) Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. 2017. Safe model-based reinforcement learning with stability guarantees. _Advances in neural information processing systems_ 30 (2017).
* Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. _JAX: composable transformations of Python+NumPy programs_. http://github.com/google/jax
* Brafman et al. (2019) Ronen I Brafman, Giuseppe De Giacomo, et al. 2019. Regular Decision Processes: A Model for Non-Markovian Domains.. In _IJCAI_. 5516-5522.
* Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym. _arXiv preprint arXiv:1606.01540_ (2016).
* Brunke et al. (2022) Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoellig. 2022. Safe learning in robotics: From learning-based control to safe reinforcement learning. _Annual Review of Control, Robotics, and Autonomous Systems_ 5 (2022), 411-444.
* Brunskill et al. (2009) Emma Brunskill, Bethany R Leffler, Lihong Li, Michael L Littman, and Nicholas Roy. 2009. Provably efficient learning with typed parametric models. (2009).
* Cai et al. (2021) Mingyu Cai, Shaoping Xiao, Zhijun Li, and Zhen Kan. 2021. Optimal probabilistic motion planning with potential infeasible LTL constraints. _IEEE transactions on automatic control_ 68, 1 (2021), 301-316.
* Chen et al. (2024) Weiqin Chen, Dharmashankar Subramanian, and Santiago Paternain. 2024. Probabilistic constraint for safety-critical reinforcement learning. _IEEE Trans. Automat. Control_ (2024).
* Cheng et al. (2019) Richard Cheng, Gabor Orosz, Richard M Murray, and Joel W Burdick. 2019. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 33. 3387-3395.

* Chow et al. (2018) Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. 2018. Risk-constrained reinforcement learning with percentile risk criteria. _Journal of Machine Learning Research_ 18, 167 (2018), 1-51.
* Chua et al. (2018) Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. 2018. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. _Advances in neural information processing systems_ 31 (2018).
* Cipollone et al. (2024) Roberto Cipollone, Anders Jonsson, Alessandro Ronca, and Mohammad Sadegh Talebi. 2024. Provably Efficient Offline Reinforcement Learning in Regular Decision Processes. _Advances in Neural Information Processing Systems_ 36 (2024).
* Dalal et al. (2018) Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. 2018. Safe exploration in continuous action spaces. _arXiv preprint arXiv:1801.08757_ (2018).
* De Giacomo et al. (2020) Giuseppe De Giacomo, Luca Iocchi, Marco Favorito, and Fabio Patrizi. 2020. Restraining bolts for reinforcement learning agents. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 34. 13659-13662.
* Deisenroth and Rasmussen (2011) Marc Deisenroth and Carl E Rasmussen. 2011. PILCO: A model-based and data-efficient approach to policy search. In _Proceedings of the 28th International Conference on machine learning (ICML-11)_. 465-472.
* Dulac-Arnold et al. (2019) Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. 2019. Challenges of real-world reinforcement learning. _arXiv preprint arXiv:1904.12901_ (2019).
* Fu and Topcu (2014) Jie Fu and Ufuk Topcu. 2014. Probably approximately correct MDP learning and control with temporal logic constraints. _arXiv preprint arXiv:1404.7073_ (2014).
* Garcia and Fernandez (2015) Javier Garcia and Fernando Fernandez. 2015. A comprehensive survey on safe reinforcement learning. _Journal of Machine Learning Research_ 16, 1 (2015), 1437-1480.
* Giacobbe et al. (2021) M Giacobbe, Mohammadhosein Hasanbeig, Daniel Kroening, and Hjalmar Wijk. 2021. Shielding atari games with bounded prescience. In _Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS_.
* Goodall and Belardinelli (2023) Alexander W Goodall and Francesco Belardinelli. 2023. Approximate Model-Based Shielding for Safe Reinforcement Learning. _arXiv preprint arXiv:2308.00707_ (2023).
* Goodall and Belardinelli (2024) Alexander W Goodall and Francesco Belardinelli. 2024. Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments. _arXiv preprint arXiv:2402.00816_ (2024).
* Ha and Schmidhuber (2018) David Ha and Jurgen Schmidhuber. 2018. Recurrent World Models Facilitate Policy Evolution. In _Advances in Neural Information Processing Systems_, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f12ba-Paper.pdf
* Hafner et al. (2019) Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. 2019. Learning latent dynamics for planning from pixels. In _International conference on machine learning_. PMLR, 2555-2565.
* Hafner et al. (2023) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2023. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_ (2023).
* Hans et al. (2008) Alexander Hans, Daniel Schneegass, Anton Schafer, and Steffen Udluft. 2008. Safe Exploration for Reinforcement Learning. 143-148.
* Hasanbeig et al. (2018) Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2018. Logically-constrained reinforcement learning. _arXiv preprint arXiv:1801.08099_ (2018).
* Hasanbeig et al. (2020) Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2020. Cautious reinforcement learning with logical constraints. _arXiv preprint arXiv:2002.12156_ (2020).

* Hasanbeig et al. (2020) Mohammad Hasanbeig, Daniel Kroening, and Alessandro Abate. 2020. Deep reinforcement learning with temporal logics. In _Formal Modeling and Analysis of Timed Systems: 18th International Conference, FORMATS 2020, Vienna, Austria, September 1-3, 2020, Proceedings 18_. Springer, 1-22.
* He et al. (2019) Chloe He, Borja G Leon, and Francesco Belardinelli. [n.d.]. Do androids dream of electric fences? Safety-aware reinforcement learning with latent shielding. CEUR Workshop Proceedings. https://ceur-ws.org/Vol-3087/paper_50.pdf
* Hoeffding (1963) Wassily Hoeffding. 1963. Probability Inequalities for Sums of Bounded Random Variables. _J. Amer. Statist. Assoc._ 58, 301 (1963), 13-30.
* Huang et al. (2023) Weidong Huang, Jiaming Ji, Borong Zhang, Chunhe Xia, and Yaodong Yang. 2023. Safe DreamerV3: Safe Reinforcement Learning with World Models. _arXiv preprint arXiv:2307.07176_ (2023).
* Icarte et al. (2018) Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. 2018. Using reward machines for high-level task specification and decomposition in reinforcement learning. In _International Conference on Machine Learning_. PMLR, 2107-2116.
* Icarte et al. (2022) Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A McIlraith. 2022. Reward machines: Exploiting reward function structure in reinforcement learning. _Journal of Artificial Intelligence Research_ 73 (2022), 173-208.
* Janner et al. (2019) Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. When to trust your model: Model-based policy optimization. _Advances in neural information processing systems_ 32 (2019).
* Jansen et al. (2020) Nils Jansen, Bettina Konighofer, Sebastian Junges, Alex Serban, and Roderick Bloem. 2020. Safe reinforcement learning using probabilistic shields. In _31st International Conference on Concurrency Theory (CONCUR 2020)_. Schloss-Dagstuhl-Leibniz Zentrum fur Informatik.
* Jansen et al. (2018) Nils Jansen, Bettina Konighofer, Sebastian Junges, Alexandru C Serban, and Roderick Bloem. 2018. Safe reinforcement learning via probabilistic shields. _arXiv preprint arXiv:1807.06096_ (2018).
* Kakade et al. (2003) Sham Kakade, Michael J Kearns, and John Langford. 2003. Exploration in metric state spaces. In _Proceedings of the 20th International Conference on Machine Learning (ICML-03)_. 306-312.
* Kearns and Singh (2002) Michael Kearns and Satinder Singh. 2002. Near-optimal reinforcement learning in polynomial time. _Machine learning_ 49 (2002), 209-232.
* Liang et al. (2018) Qingkai Liang, Fanyu Que, and Eytan Modiano. 2018. Accelerated primal-dual policy optimization for safe reinforcement learning. _arXiv preprint arXiv:1802.06480_ (2018).
* Machado et al. (2018) Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and Michael Bowling. 2018. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. _Journal of Artificial Intelligence Research_ 61 (2018), 523-562.
* Majeed et al. (2018) Sultan Javed Majeed, Marcus Hutter, et al. 2018. On Q-learning Convergence for Non-Markov Decision Processes.. In _IJCAI_, Vol. 18. 2546-2552.
* Maurer and Pontil (2009) Andreas Maurer and Massimiliano Pontil. 2009. Empirical bernstein bounds and sample variance penalization. _arXiv preprint arXiv:0907.3740_ (2009).
* McIlvanna et al. (2022) Stephen McIlvanna, Nhat Nguyen Minh, Yuzhu Sun, Mien Van, and Wasif Naeem. 2022. Reinforcement learning-enhanced control barrier functions for robot manipulators. _arXiv preprint arXiv:2211.11391_ (2022).
* Mitta et al. (2024) Rohan Mitta, Hosein Hasanbeig, Jun Wang, Daniel Kroening, Yiannis Kantaros, and Alessandro Abate. 2024. Safeguarded Progress in Reinforcement Learning: Safe Bayesian Exploration for Control Policy Synthesis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 38. 21412-21419.

* Ng et al. (1999) Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. In _Icml_, Vol. 99. 278-287.
* Paternain et al. (2022) Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Chamon, and Alejandro Ribeiro. 2022. Safe policies for reinforcement learning via primal-dual methods. _IEEE Trans. Automat. Control_ 68, 3 (2022), 1321-1336.
* Rajeswaran et al. (2020) Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. 2020. A game theoretic framework for model based reinforcement learning. In _International conference on machine learning_. PMLR, 7953-7963.
* Ray et al. (2019) Alex Ray, Joshua Achiam, and Dario Amodei. 2019. Benchmarking safe exploration in deep reinforcement learning. _arXiv preprint arXiv:1910.01708_ 7, 1 (2019), 2.
* Ronca and De Giacomo (2021) Alessandro Ronca and Giuseppe De Giacomo. 2021. Efficient PAC reinforcement learning in regular decision processes. _arXiv preprint arXiv:2105.06784_ (2021).
* Sui et al. (2015) Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. 2015. Safe exploration for optimization with Gaussian processes. In _International conference on machine learning_. PMLR, 997-1005.
* Sutton et al. (1999) Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. _Artificial intelligence_ 112, 1-2 (1999), 181-211.
* Tessler et al. (2018) Chen Tessler, Daniel J Mankowitz, and Shie Mannor. 2018. Reward constrained policy optimization. _arXiv preprint arXiv:1805.11074_ (2018).
* Thananjeyan et al. (2021) Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. 2021. Recoveryrl: Safe reinforcement learning with learned recovery zones. _IEEE Robotics and Automation Letters_ 6, 3 (2021), 4915-4922.
* Thomas et al. (2021) Garrett Thomas, Yuping Luo, and Tengyu Ma. 2021. Safe reinforcement learning by imagining the near future. _Advances in Neural Information Processing Systems_ 34 (2021), 13859-13869.
* Icarte et al. (2019) Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Margarita Castro, and Sheila McIlraith. 2019. Learning reward machines for partially observable reinforcement learning. _Advances in neural information processing systems_ 32 (2019).
* Voloshin et al. (2022) Cameron Voloshin, Hoang Le, Swarat Chaudhuri, and Yisong Yue. 2022. Policy optimization with linear temporal logic constraints. _Advances in Neural Information Processing Systems_ 35 (2022), 17690-17702.
* Wabersich and Zeilinger (2018) Kim P Wabersich and Melanie N Zeilinger. 2018. Linear model predictive safety certification for learning-based control. In _2018 IEEE Conference on Decision and Control (CDC)_. IEEE, 7130-7135.
* Wabersich and Zeilinger (2021) Kim Peter Wabersich and Melanie N Zeilinger. 2021. A predictive safety filter for learning-based control of constrained nonlinear dynamical systems. _Automatica_ 129 (2021), 109597.
* Wachi et al. (2018) Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. 2018. Safe exploration and optimization of constrained mdps using gaussian processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 32.
* Williams and Rasmussen (2006) Christopher KI Williams and Carl Edward Rasmussen. 2006. _Gaussian processes for machine learning_. Vol. 2. MIT press Cambridge, MA.
* Wolff et al. (2012) Eric M Wolff, Ufuk Topcu, and Richard M Murray. 2012. Robust control of uncertain Markov decision processes with temporal logic specifications. In _2012 IEEE 51st IEEE Conference on decision and control (CDC)_. IEEE, 3372-3379.
* Wright (2006) Jorge Nocedal Stephen J Wright. 2006. Numerical optimization.

* Xiao et al. [2023] Wenli Xiao, Yiwei Lyu, and John Dolan. 2023. Model-based Dynamic Shielding for Safe and Efficient Multi-agent Reinforcement Learning. In _Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems_. 1587-1596.
* Yang et al. [2020] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. 2020. Projection-based constrained policy optimization. _arXiv preprint arXiv:2010.03152_ (2020).
* Yang et al. [2023] Wen-Chi Yang, Giuseppe Marra, Gavin Rens, and Luc De Raedt. 2023. Safe reinforcement learning via probabilistic logic shields. _arXiv preprint arXiv:2303.03226_ (2023).

Algorithms

``` Input: model checking parameters (\(p\), \(H\)), current state \(\langle s,q\rangle\), current action \(a\), product MC \(\mathcal{M}_{\pi}\otimes\mathcal{D}=(\mathcal{S}\times\mathcal{Q},\mathcal{P}^{ \prime},\mathcal{P}^{\prime}_{0},\{accept\},L^{\prime})\) Output: true if\(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p_{1}\)  Initialize zero vector \(\mathbf{x}^{(0)}\leftarrow\mathbf{0}\) with size \(|\mathcal{S}|\times|\mathcal{Q}|\)  Initialize probability matrix \(\mathbf{A}\leftarrow(\mathcal{P}^{\prime}(s,t))_{s,t\notin accept}\) (ignoring accepting states)  Initialize probability vector \(\mathbf{b}\leftarrow(\mathcal{P}^{\prime}(s,accept))_{s\notin accept}\) (going to accepting states) // Iterate over the model checking horizon for\(i=1,\ldots,H\)do  Compute \(\mathbf{x}^{(i)}=\mathbf{A}\mathbf{x}^{(i-1)}+\mathbf{b}\) // Get the corresponding probability  Let \(X\leftarrow\mathbf{x}_{\langle s,q\rangle}\) If\(X<p\) return true else return false ```

**Algorithm 2** Exact Model Checking [9]

``` Input: model checking parameters (\(\epsilon\), \(\delta\), \(p\), \(H\)), current state \(\langle s,q\rangle\), current action \(a\), policy \(\pi\), labelling function \(L\), DFA \(\mathcal{D}=(\mathcal{Q},\Sigma,\Delta,\mathcal{Q}_{0},\mathcal{F})\) and (approximate) transition probabilities \(\mathcal{P}\) Output: true if\(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p_{1}\)  Choose \(m\geq 2/(\epsilon^{2})\log(2/\delta)\) for\(i=1,\ldots,m\)do  Set \(s_{0}\gets s\), \(q_{0}\gets q\) and \(a_{0}\gets a\) // Sample a path through the model for\(j=1,\ldots,H\)do  Sample next state \(s_{j}\sim\mathcal{P}(\cdot\mid s_{j-1},a_{j-1})\),  Compute \(q_{j}\leftarrow\Delta(q_{j-1},L(s_{j}))\),  Sample action \(a_{j}\sim\pi(\cdot\mid s_{j})\) // Check if the path is accepting  Let \(X_{i}\gets 1\left[q_{H}\in\mathcal{F}\right]\) // Construct probability estimate  Let \(\widetilde{X}\leftarrow\frac{1}{m}\sum_{i=1}^{m}X_{i}\) If\(\widetilde{X}<p-\epsilon\) return true else return false ```

**Algorithm 3** Monte-Carlo Model Checking

``` Input: MDP \(\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{P}_{0},\mathcal{R},AP,L)\), DFA \(\mathcal{D}=(\mathcal{Q},\Sigma,\Delta,\mathcal{Q}_{0},\mathcal{F})\), discount factor \(\gamma\in(0,1]\), learning rate \(\alpha\in(0,1]\), temperature \(\tau>0\), cost coefficient \(C\) and fixed episode length \(T\) Initialize: (Q-table) \(\hat{Q}(s,q,a)\gets 0\:\forall s\in\mathcal{S},q\in\mathcal{Q},a\in\mathcal{A}\) for each episode do  Observe \(s_{0}\), \(L(s_{0})\) and \(q_{0}\leftarrow\Delta(\mathcal{Q}_{0},L(s_{0}))\) for\(t=0,\ldots,T\)do  Sample action \(a_{t}\) from \(\langle s_{t},q_{t}\rangle\) using the Boltzmann policy derived from \(\hat{Q}\) with temp. \(\tau\)  Play action \(a_{t}\) and observe \(s_{t+1}\), \(L(s_{t+1})\) and \(r_{t}\) (reward is optional). // Generate synthetic data by simulating all automaton transitions for\(\bar{q}\in\mathcal{Q}\)do  Compute \(\bar{q}^{\prime}\leftarrow\Delta(q^{\prime},L(s_{t+1}))\)  Compute cost \(\bar{c}^{\prime}\gets C\cdot 1[\bar{q}^{\prime}\in\mathcal{F}]\)  Compute \(\textit{done}\gets 1[\bar{q}^{\prime}\in\mathcal{F}]\) // Q-learning step \(\hat{Q}(s_{t},\bar{q},a_{t})\leftarrow(1-\alpha)\cdot\hat{Q}(s_{t},\bar{q},a_{t}) +\alpha\cdot(r_{t}+\bar{c}^{\prime}+\gamma\cdot\textit{done}\cdot\max_{a^{ \prime}\in\mathcal{A}}\hat{Q}(s_{t+1},\bar{q}^{\prime},a^{\prime})\)  Compute \(q_{t+1}\leftarrow\Delta(q_{t},L(s_{t+1}))\) and continue ```

**Algorithm 4** Tabular Q-learning (Regular Safety Property) with Counter Factual Experiences [65]

## Appendix B Technical Details

### Augmented Lagrangian

We first define the following objective functions,

\[J_{\mathcal{R}}(\pi) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{R}(s_{t},a_{t})\right]\] (1) \[J_{\mathcal{C}}(\pi) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(s_{t},a_{t})\right]\] (2)

The augmented Lagrangian [72] is an adaptive penalty-based technique for the following constrained optimization problem,

\[\max_{\pi}J_{\mathcal{R}}(\pi)\quad\text{subject to}\quad J_{\mathcal{C}}(\pi)\leq d\] (4)

where \(d\) is some cost threshold. The corresponding Lagrangian is given by,

\[\max_{\pi}\min_{\lambda\geq 0}\left[J_{\mathcal{R}}(\pi)-\lambda\left(J_{ \mathcal{C}}(\pi)-d\right)\right]=\max_{\pi}\begin{cases}J_{\mathcal{R}}(\pi)& \text{if }J_{\mathcal{C}}(\pi)<d\\ -\infty&\text{otherwise}\end{cases}\] (5)

The LHS is an equivalent form for the constrained optimization problem (RHS), since if \(\pi\) is feasible, i.e. \(J_{\mathcal{C}}(\pi)<d\) then the maximum value for \(\lambda\) is \(\lambda=0\). If \(\pi\) is not feasible then \(\lambda\) can be arbitrarily large to solve this equation. Unfortunately this form of the objective function is non-smooth when moving from feasible to infeasible policies, thus we introduce a proximal relaxation of the augmentedLagrangian [72],

\[\max_{\pi}\min_{\lambda\geq 0}\left[J_{\mathcal{R}}(\pi)-\lambda\left(J_{ \mathcal{C}}(\pi)-d\right)+\frac{1}{\mu_{k}}(\lambda-\lambda_{k})^{2}\right]\] (6)

where \(\mu_{k}\) is a non-decreasing penalty multiplier dependent on the gradient step \(k\). The new term that has been introduced here encourages the \(\lambda\) to stay close to the previous value \(\lambda_{k}\), resulting in a smooth and differentiable function. The derivative w.r.t \(\lambda\) gives us the following gradient update step,

\[\lambda_{k+1}=\begin{cases}\lambda_{k}+\mu_{k}(J_{\mathcal{C}}(\pi)-d)&\text{ if }\lambda_{k}+\mu_{k}(J_{\mathcal{C}}(\pi)-d)\geq 0\\ 0&\text{otherwise}\end{cases}\] (7)

At each gradient step, the penalty multiplier \(\mu_{k}\) is updated in a non-decreasing way by using some small fixed (power) parameter \(\sigma\),

\[\mu_{k+1}=\max\{(\mu_{k})^{1+\sigma},1\}\] (8)

The policy \(\pi\) is then updated by taking gradient steps of the following unconstrained objective,

\[\tilde{J}(\pi,\lambda_{k},\mu_{k})=J_{\mathcal{R}}(\pi)-\Psi_{\mathcal{C}}( \pi,\lambda_{k},\mu_{k})\]

where,

\[\Psi_{\mathcal{C}}(\pi,\lambda_{k},\mu_{k})=\begin{cases}\lambda_{k}(J_{ \mathcal{C}}(\pi)-d)+\frac{\mu_{k}}{2}(J_{\mathcal{C}}(\pi)-d)^{2}&\text{if } \lambda_{k}+\mu_{k}(J_{\mathcal{C}}(\pi)-d)\geq 0\\ -\frac{(\lambda_{k})^{2}}{2\mu_{k}}&\text{otherwise}\end{cases}\]

## Appendix C Technical Proofs

### Proof of Proposition 3.4

**Proposition 3.4** (restated) (Satisfaction probability for \(P^{H}_{\text{safe}}\)).: _Let \(\mathcal{M}\) and \(\mathcal{D}\) be the MDP and DFA from before (Defn. 3.3). For a path \(\rho\in\mathcal{S}^{\omega}\) in the Markov chain, let \(\text{trace}_{H}(\rho)=L(\rho[0]),L(\rho[1])\ldots,L(\rho[H])\) be the corresponding finite word over \(\Sigma=Pow(AP)\). For a given state \(s\in\mathcal{S}\) the finite horizon satisfaction probability for \(P_{\text{safe}}\) is defined as follows,_

\[\Pr^{\mathcal{M}}(s\models P^{H}_{\text{safe}}):=\Pr^{\mathcal{M}}(\rho\in \mathcal{S}^{\omega}\mid\rho[0]=s,\text{trace}_{H}(\rho)\not\in\mathcal{L}( \mathcal{D}))\]

_where \(H\in\mathbb{Z}_{+}\) is some fixed model checking horizon. Similar to before, we show that the finite horizon satisfaction probability can be written as the following bounded reachability probability,_

\[\Pr^{\mathcal{M}}(s\models P^{H}_{\text{safe}})=\Pr^{\mathcal{M}\otimes \mathcal{D}}(\langle s,q_{s}\rangle\not\models\Diamond^{\leq H}accept)\]

_where \(q_{s}=\Delta(\mathcal{Q}_{0},L(s))\) is as before and \(\Diamond^{\leq H}accept\) is the corresponding step-bounded PCTL path formula that reads, 'eventually accept in H timesteps'._

Proof.: Let \(P_{\text{safe}}\) be a regular safety property and let \(\mathcal{D}=(\mathcal{Q},\Sigma,\Delta,\mathcal{Q}_{0},\mathcal{F})\) be the DFA such that \(\mathcal{L}(\mathcal{D})=BadPref(P_{\text{safe}})\). We provide a formal definition for \(P_{\text{safe}}\) and the corresponding finite horizon property \(P^{H}_{\text{safe}}\), respectively:

\[P_{\text{safe}} =\{w\in\Sigma^{\omega}\mid\forall w_{pref}\in\Sigma^{\omega}s.t.\ w _{pref}\preceq w,w_{pref}\not\in\mathcal{L}(\mathcal{D})\}\] (9) \[P^{H}_{\text{safe}} =\{w\in\Sigma^{\omega}\mid\forall w_{pref}\in\Sigma^{\omega}s.t. \ w_{pref}\preceq w\wedge|w_{pref}|\leq H+1,w_{pref}\not\in\mathcal{L}( \mathcal{D})\}\] (10)

Let \(\mathcal{M}=(\mathcal{S},\mathcal{P},\mathcal{P}_{0},AP,L)\) be a Markov chain and consider the product Markov chain \(\mathcal{M}\otimes\mathcal{D}\) from Defn. 3.2. For any path \(\rho=s_{0},s_{1},s_{2},\ldots\), there exists a unique run \(q_{0},q_{1},q_{2},\ldots\) for the trace \(trace(\rho)=L(s_{0}),L(s_{1}),L(s_{2})\ldots\), and denote,

\[\rho^{+}=\langle s_{0},q_{0}\rangle,\langle s_{1},q_{1}\rangle,\langle s_{2}, q_{2}\rangle\ldots\] (11)

where start state is \(\langle s_{0},\Delta(\mathcal{Q}_{0},L(s_{0}))\rangle\). Before we deal with probabilities let's just consider a fixed path \(\rho\in\mathcal{S}^{\omega}\), the finite trace \(\text{trace}_{H}(\rho)=L(\rho[0]),L(\rho[1])\ldots,L(\rho[H])\), the unique run \(q_{0},q_{1},q_{2},\ldots,q_{H}\) and the path \(\rho^{+}\in\Sigma^{\omega}\times\mathcal{Q}^{\omega}\) in the product Markov chain. We prove the following statement,

\[\rho\not\models P^{H}_{\text{safe}}\quad\text{if and only if}\quad\rho^{+}\models \Diamond acccept^{\leq H}\] (12)We start with the (\(\rightarrow\)) direction, in particular, \(\rho\not\models P_{\textit{safe}}^{H}\) if and only if \(\textit{trace}_{H}(\rho)\in\mathcal{L}(\mathcal{D})\). Recall that by definition \(\mathcal{L}(\mathcal{D})=\{w\in\Sigma^{*}\mid\Delta^{*}(\mathcal{Q}_{0},w)\in \mathcal{F}\}\), and so \(\textit{trace}_{H}(\rho)\in\mathcal{L}(\mathcal{D})\) implies that \(q_{H}=\Delta^{*}(\mathcal{Q}_{0},\textit{trace}_{H}(\rho))\in\mathcal{F}\), which by construction implies that \(\rho^{+}\models\loz accept\leq^{H}\).

The opposite direction (\(\leftarrow\)) is a little more involved, in particular, \(\rho^{+}\models\loz accept\leq^{H}\) implies that for the unique run \(q_{0},q_{1},q_{2},\ldots,q_{H}\) there exists \(t\leq H\) such that \(q_{t}\in\mathcal{F}\). We notice that since \(\mathcal{L}(\mathcal{D})=\textit{BadPref}(P_{\textit{safe}})\) then once the DFA reaches an accepting state it will remain in an accepting state for the rest of the run. Therefore, \(q_{t}\in\mathcal{F}\) for \(t\leq H\) implies that \(q_{H}\in\mathcal{F}\). Then by definition the trace \(\textit{trace}_{H}(\rho)\) that determined the unique run \(q_{0},q_{1},q_{2},\ldots,q_{H}\) must be in the language \(\mathcal{L}(\mathcal{D})\), which again by definition implies that \(\rho\not\models P_{\textit{safe}}^{H}\).

We now deal with the probabilities. First we note that the DFA \(\mathcal{D}\) does not affect the probabilities of the product Markov chain - it can be shown that for every measurable set \(P\) of paths in \(\mathcal{M}\),

\[\Pr^{\mathcal{M}}(P)=\Pr^{\mathcal{M}\otimes\mathcal{A}}(\rho^{+}\mid\rho\in P)\] (13)

see [9]. It now remains to construct this set \(P\) in the proper way. In particular, if \(P\) is the set of paths starting in some state \(s\in\mathcal{S}\) and that refute \(P_{\textit{safe}}\) in the next \(H\) timesteps, i.e.,

\[P=\{\rho\in\mathcal{S}^{\omega}\mid\rho[0]=s,\{w^{\prime}\in\Sigma^{*}\mid w _{pref}\preceq\textit{trace}(\rho)\land|w_{pref}|\leq H+1)\cap\mathcal{L}( \mathcal{D})\neq\varnothing\}\] (14)

and \(P^{+}\) is defined as the set of paths starting from the corresponding state \(\langle s,q_{s}\rangle\) (where \(q_{s}=\Delta(\mathcal{Q}_{0},L(s))\)) in \(\mathcal{M}\otimes\mathcal{D}\) that eventually reach an accepting state of \(\mathcal{D}\) in the next \(H\) steps, i.e.

\[P^{+}=\{\rho^{+}\in(\mathcal{S}\times\mathcal{Q})^{\omega}\mid\rho^{+}[0]= \langle s,q_{s}\rangle\land\rho^{+}\models\lozenge^{\leq H}accept\}\] (15)

Then by construction we have,

\[\Pr^{\mathcal{M}}(P)=\Pr^{\mathcal{M}\otimes\mathcal{D}}(\rho^{+}\mid\rho[0]=s,\rho\in P)=\Pr^{\mathcal{M}\otimes\mathcal{D}}(P^{+})\] (16)

Finally the probability \(\Pr^{\mathcal{M}}(P)\) and \(\Pr^{\mathcal{M}}(s\models P_{\textit{safe}}^{H})\) are related as follows,

\[\Pr^{\mathcal{M}}(s\models P_{\textit{safe}}^{H}) =1-\Pr^{\mathcal{M}}(P)\] (17) \[=1-\Pr^{\mathcal{M}\otimes\mathcal{D}}(P^{+})\] (18) \[=1-\Pr^{\mathcal{M}\otimes\mathcal{D}}(\langle s,q_{s}\rangle \models\lozenge^{\leq H}accept\rangle\] (19) \[=\Pr^{\mathcal{M}\otimes\mathcal{D}}(\langle s,q_{s}\rangle \not\models\lozenge^{\leq H}accept\rangle\] (20)

### Proof of Proposition 4.2

**Proposition 4.2** (restated).: _Let \(P_{\textit{safe}}^{T}\) denote the (episodic) regular safety property for a fixed episode length \(T\). Then satisfying \(\Pr\left(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept\right)\leq p _{1}\) for all \(t\in[0,T]\) guarantees that \(\Pr(s_{0}\models P_{\textit{safe}}^{T})\geq 1-p_{1}\cdot[T/H]\), where \(s_{0}\sim\mathcal{P}_{0}\) is the initial state._

Proof.: Consider splitting up the episode in to \(\lceil T/H\rceil\) chunks with length at most \(H\). Let \(X_{0},X_{1},\ldots X_{\lceil T/H\rceil-1}\) be the indicator random variables defined as follows,

\[X_{i}=\begin{cases}1&\text{if }\langle s_{i\cdot H},q_{i\cdot H}\rangle\models \lozenge^{\leq H}accept\\ 0&\text{otherwise}\end{cases}\] (21)

Since \(\Pr(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept\leq p_{1}\) for all \(t\in[0,T]\) then the probability \(\Pr(X_{i}=1)\leq p_{1}\). By construction we have,

\[\text{if }\quad\bigcap_{i=0}^{\lceil T/H\rceil-1}X_{i}=0\quad\text{then }\quad s_{0}\models P_{\textit{safe}}^{T}\] (22)Intuitively we satisfy \(P_{\textit{safe}}\) for the entire episode length if we never enter an accepting state in each of the \(\lceil T/H\rceil\) chunks. The final result is then obtained by taking a union bound as follows,

\[\Pr(s_{0}\models P_{\textit{safe}}^{T}) \geq\Pr\left(\bigcap_{i=0}^{\lceil T/H\rceil-1}X_{i}=0\right)\] (23) \[=1-\Pr\left(\bigcup_{i=0}^{\lceil T/H\rceil-1}X_{i}=1\right)\] (24) \[\geq 1-\sum_{i=0}^{\lceil T/H\rceil-1}\Pr(X_{i}=1)\] (25) \[\geq 1-p_{1}\cdot\lceil T/H\rceil\] (26)

### Proof of Proposition 5.4

**Proposition 5.4** (restated).: _Let \(\epsilon>0\), \(\delta>0\), \(s\in\mathcal{S}\) be given. Under Assumption 5.2, we can obtain an \(\epsilon\)-approximate estimate for \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) with probability at least \(1-\delta\), by sampling \(m\geq\frac{1}{2\epsilon^{2}}\log\left(\frac{2}{\delta}\right)\) paths from the 'black box' model._

Proof.: In words, we estimate \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) by sampling \(m\) paths from a 'black box' model of the environment dynamics. We label each path as satisfying or not and return the proportion of satisfying traces as an estimate for \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\). We proceed as follows, let \(\rho_{1},\ldots\rho_{m}\) be a sequence of paths sampled from the 'black box' model and let \(\textit{trace}(\rho_{1}),\ldots\textit{trace}(\rho_{m})\) be the corresponding traces. Furthermore, let \(X_{1},\ldots,X_{m}\) be indicator r.v.s such that,

\[X_{i}=\begin{cases}1&\text{if }\textit{trace}(\rho_{1})\models\lozenge^{ \leq H}accept,\\ 0&\text{otherwise}\end{cases}\] (28)

Recall that \(\textit{trace}(\rho_{1})\models\lozenge^{\leq H}accept\) can be checked in time \(O(\text{poly}(H))\). Now let,

\[\overline{X}=\frac{1}{m}\sum_{i=1}^{m}X_{i}\text{ where }\mathbb{E}[\overline{X}]= \Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\] (29)

then by Hoeffding's inequality [40],

\[\mathbb{P}\left[|\overline{X}-\mathbb{E}[\overline{X}]|\geq\epsilon\right] \leq 2\exp\left(-2m\epsilon^{2}\right)\] (30)

Bounding the RHS from above by \(\delta\) and rearranging gives the desired result. 

### Proof of Proposition 5.5

We start by introducing the following lemma.

**Lemma C.1** (Error amplification for trace distributions).: _Let \(\widehat{\mathcal{P}}\approx\mathcal{P}\) be such that,_

\[D_{TV}\left(\mathcal{P}(\cdot\mid s),\widehat{\mathcal{P}}(\cdot\mid s)\right) \leq\alpha\ \forall s\in S\] (31)

_Let the start state \(s_{0}\in\mathcal{S}\) be given, and let \(\mathcal{P}_{t}(\cdot)\) and \(\widehat{\mathcal{P}}_{t}(\cdot)\) denote the path distribution (at time \(t\)) for the two transition probabilities \(\mathcal{P}\) and \(\widehat{\mathcal{P}}\) respectively. Then the total variation distance between the two path distributions (at time \(t\)) are bounded as follows,_

\[D_{TV}\left(\mathcal{P}_{t}(\cdot),\widehat{\mathcal{P}}_{t}(\cdot)\right) \leq\alpha t\ \forall t\] (32)Proof.: We will prove this fact by doing an induction on \(t\). We recall that \(\mathcal{P}_{t}(\cdot)\) and \(\widehat{\mathcal{P}}_{t}(\cdot)\) denote the path distribution (at time \(t\)) for the two transition probabilities \(\mathcal{P}\) and \(\widehat{\mathcal{P}}\) respectively. Formally we define them as follows,

\[\mathcal{P}_{t}(\rho) =\Pr(s_{0},\ldots,s_{t}\preceq\rho\mid s_{0}=s,\mathcal{P})\] (33) \[\widehat{\mathcal{P}}_{t}(\rho) =\Pr(s_{0},\ldots,s_{t}\preceq\rho\mid s_{0}=s,\widehat{\mathcal{ P}})\] (34)

These probabilities read as follows, 'the probability of the sequence \(s_{0},\ldots,s_{t}\preceq\rho\) at time \(t\)', or similarly 'the probability that the sequence \(s_{0},\ldots,s_{t}\) is a prefix of \(\rho\) at time \(t\)' Since the start state \(s_{0}\in\mathcal{S}\) is given we note that,

\[\mathcal{P}_{0}(\cdot)=\widehat{\mathcal{P}}_{0}(\cdot)\] (35)

Before we continue with the induction on \(t\) we make the following observation, for any path \(\rho\in\mathcal{S}^{\omega}\) we have by the triangle inequality,

\[\left|\mathcal{P}_{t}(\rho)-\widehat{\mathcal{P}}_{t}(\rho)\right| =\left|\mathcal{P}(s_{t}\mid s_{t-1})\mathcal{P}_{t-1}(\rho)- \widehat{\mathcal{P}}(s_{t}\mid s_{t-1})\widehat{\mathcal{P}}_{t-1}(\rho)\right|\] (36) \[\leq\mathcal{P}_{t-1}(\rho)\left|\mathcal{P}(s_{t}\mid s_{t-1})- \widehat{\mathcal{P}}(s_{t}\mid s_{t-1})\right|+\widehat{\mathcal{P}}(s_{t} \mid s_{t-1})\left|\mathcal{P}_{t-1}(\rho)-\widehat{\mathcal{P}}_{t-1}(\rho)\right|\] (37)

Now we continue with the induction on \(t\),

\[2D_{TV}(\mathcal{P}_{t}(\cdot),\widehat{\mathcal{P}}_{t}(\cdot)) =\sum_{\rho\in\mathcal{S}^{\omega}}\left|\mathcal{P}_{t}(\rho)- \widehat{\mathcal{P}}_{t}(\rho)\right|\] (38) \[\leq\sum_{\rho\in\mathcal{S}^{\omega}}\mathcal{P}_{t-1}(\rho) \left|\mathcal{P}(s_{t}\mid s_{t-1})-\widehat{\mathcal{P}}(s_{t}\mid s_{t-1})\right|\] (39) \[\qquad+\sum_{\rho\in\mathcal{S}^{\omega}}\widehat{\mathcal{P}}(s _{t}\mid s_{t-1})\left|\mathcal{P}_{t-1}(\rho)-\widehat{\mathcal{P}}_{t-1}( \rho)\right|\] \[\leq\sum_{\rho\in\mathcal{S}^{\omega}}\mathcal{P}_{t-1}(\rho) \cdot(2\alpha)+\sum_{\rho\in\mathcal{S}^{\omega}}\left|\mathcal{P}_{t-1}( \rho)-\widehat{\mathcal{P}}_{t-1}(\rho)\right|\] (40) \[=2\alpha+2D_{TV}(\mathcal{P}_{t-1}(\cdot),\widehat{\mathcal{P}}_ {t-1}(\cdot))\] (41) \[\leq 2\alpha t\] (42)

The final result is obtained by an induction on \(t\) where the base case comes from \(\mathcal{P}_{0}(\cdot)=\widehat{\mathcal{P}}_{0}(\cdot)\). 

**Proposition 5.5** (restated).: _Let \(\epsilon>0\), \(\delta>0\), \(s\in\mathcal{S}\) and horizon \(H\geq 1\) be given. Under Assumption 5.3 we can make the following two statements:_

_(1) We can obtain an \(\epsilon\)-approximate estimate for \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) with probability \(1\) by exact model checking with the transition probabilities of \(\widehat{\mathcal{P}}_{\pi}\) in time \(\mathcal{O}(\text{poly}(\text{size}(\mathcal{M}_{\pi}\otimes\mathcal{D})) \cdot H)\)._

_(2) We can obtain an \(\epsilon\)-approximate estimate for \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) with probability at least \(1-\delta\), by sampling \(m\geq\frac{2}{\epsilon^{2}}\log\left(\frac{2}{\delta}\right)\) paths from the 'approximate' dynamics model \(\widehat{\mathcal{P}}_{\pi}\)._

Proof.: We start by proving statement (1) and then statement (2) will follow quickly. First let \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) and \(\widehat{\Pr}(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) denote the acceptance probabilities for the two transition probabilities \(\mathcal{P}\) and \(\widehat{\mathcal{P}}\) respectively. We also let \(g(\cdot)\) and \(\widehat{g}(\cdot)\) denote the average trace distribution (over the next \(H\) timesteps) for the two transition probabilities \(\mathcal{P}\) and \(\widehat{\mathcal{P}}\) respectively, where,

\[g(\rho) =\frac{1}{H}\sum_{t=1}^{H}\mathcal{P}_{t}(\rho)\] (43) \[\widehat{g}(\rho) =\frac{1}{H}\sum_{t=1}^{H}\widehat{\mathcal{P}}_{t}(\rho)\] (44)

Before we continue with the proof of (1) we make the following observations,* \(\max_{\langle s,q\rangle}\left|\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept )-\widehat{\Pr}(\langle s,q\rangle\models\lozenge^{\leq H}accept)\right|\leq 1\)
* Let \(f(x):x\in\mathcal{X}\rightarrow[0,1]\) be a real-valued function. Let \(\mathcal{P}_{1}(\cdot)\) and \(\mathcal{P}_{2}(\cdot)\) be probability distributions over the space \(\mathcal{X}\), then. \[\left|\mathbb{E}_{x\sim\mathcal{P}_{1}(\cdot)}[f(x)]-\mathbb{E}_{x\sim \mathcal{P}_{2}(\cdot)}[f(x)]\right|\leq D_{TV}(\mathcal{P}_{1}(\cdot), \mathcal{P}_{2}(\cdot))\]

We continue by showing the following,

\[\left|\Pr(\langle s,q\rangle\models \lozenge^{\leq H}accept)-\widehat{\Pr}(\langle s,q\rangle\models \lozenge^{\leq H}accept)\right|\] (45) \[=\left|\mathbb{E}_{\rho\sim g}\left[1\left[\langle s,q\rangle \models\lozenge^{\leq H}accept\right]\right]-\mathbb{E}_{\rho\sim\widehat{g}} \left[1\left[\langle s,q\rangle\models\lozenge^{\leq H}accept\right]\right]\right|\] (46) \[\leq D_{TV}\left(g(\cdot),\widehat{g}(\cdot)\right)\] (47) \[=\frac{1}{2}\sum_{\rho\in\mathcal{S}^{\omega}}|g(\rho)-\widehat{ g}(\rho)|\] (48) \[=\frac{1}{2H}\sum_{\rho\in\mathcal{S}^{\omega}}\left|\sum_{t=1}^{ H}\mathcal{P}_{t}(\rho)-\widehat{\mathcal{P}}_{t}(\rho)\right|\] (49) \[\leq\frac{1}{2H}\sum_{t=1}^{H}\left|\sum_{\rho\in\mathcal{S}^{ \omega}}\mathcal{P}_{t}(\rho)-\widehat{\mathcal{P}}_{t}(\rho)\right|\] (50) \[\leq\frac{1}{2H}\sum_{t=1}^{H}H(\epsilon/H)\] (51) \[=\epsilon/2\] (52)

The first inequality (Eq. 47) comes from our earlier observations. The second inequality (Eq. 50) is straightforward and the final inequality (Eq. 51) is obtained by applying Lemma C.1 and Assumption 5.3. We note that this result is similar to the _simulation lemma_[48], which has been proved many times for several different settings [1, 16, 47, 57].

This concludes the proof of statement (1), since we have shown that \(\widehat{\Pr}(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) is an \(\epsilon/2\)-approximate estimate of \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\), under the Assumption 5.3.

The proof of statement (2) follows quickly. We have established that,

\[\left|\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)-\widehat{\Pr}( \langle s,q\rangle\models\lozenge^{\leq H}accept)\right|\leq\epsilon/2\] (54)

It remains to obtain an \(\epsilon/2\)-approximate estimate of \(\widehat{\Pr}(\langle s,q\rangle\models\lozenge^{\leq H}accept)\). By using the same reasoning as in the proof of Proposition 5.4. We can obtain an \(\epsilon/2\)-approximate estimate of \(\widehat{\Pr}(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) by sampling \(m\) paths, \(\rho_{1},\ldots\rho_{m}\), from the approximate dynamics model \(\widehat{\mathcal{P}}\). Then provided,

\[m\geq\frac{2}{\epsilon^{2}}\log\left(\frac{2}{\delta}\right)\] (55)

with probability \(1-\delta\) we can obtain \(\epsilon/2\)-approximate estimate of \(\widehat{\Pr}(\langle s,q\rangle\models\lozenge^{\leq H}accept)\) and by extension an \(\epsilon\)-approximate estimate of \(\Pr(\langle s,q\rangle\models\lozenge^{\leq H}accept)\). This concludes the proof. 

### Proof of Theorem 6.5

**Theorem 6.5** (restated).: _Under Assumption 6.3 and 6.4, and provided that every state action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) has been visited at least \(\mathcal{O}\left(\frac{H^{2}|\mathcal{S}|^{2}}{\epsilon^{2}}\log\left(\frac{| \mathcal{A}||\mathcal{S}|^{2}}{\delta}\right)\right)\) times. Then with probability \(1-\delta\) the system satisfies the constraints of Problem 4.1, independent of the 'task policy'._

Proof.: We split the proof up in to three parts, **(1)**, **(2)** and **(3)**. In part **(1)** we show that the given sample complexity bound gives us an approximate model of the environment dynamics with high probability. In part **(2)** we use our assumptions to reason about the probabilistic recoverability of the system when it enters a critical state. In part **(3)** we put everything together and deal with approximation error \(\epsilon\) the remaining failure probability that are both unavoidable for the statistical model checking procedures used to shield the system.

**(1)** We show that the following holds with probability \(1-\delta/2\),

\[D_{TV}\left(\mathcal{P}_{\pi}(\cdot\mid s),\widehat{\mathcal{P}}_{\pi}(\cdot \mid s)\right)\leq\epsilon/H\ \forall s\in\mathcal{S}\] (56)

when every state action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) has been visited at least,

\[\mathcal{O}\left(\frac{H^{2}|\mathcal{S}|^{2}}{\epsilon^{2}}\log\left(\frac{| \mathcal{A}||\mathcal{S}|^{2}}{\delta}\right)\right)\]

times. First we let \(\#(s,a)\) denote the total number of times that \((s,a)\) has been observed, similarly we let \(\#(s^{\prime},s,a)\) denote the total number of times that \((s^{\prime},s,a)\) has been observed. The maximum likelihood estimate for the unknown probability \(\mathcal{P}(s^{\prime}\mid,s,a)\) is \(\widehat{\mathcal{P}}(s^{\prime}\mid s,a)=\#(s^{\prime},s,a)/\#(s,a)\). Let us fix some \((s,a)\in\mathcal{S}\times\mathcal{A}\), and \(s^{\prime}\in\mathcal{S}\), we let \(p_{s^{\prime}}=\mathcal{P}(s^{\prime}\mid s,a)\) denote the true probability of transitioning to \(s^{\prime}\) from \((s,a)\) and we let \(\hat{p}_{s^{\prime}}=\#(s^{\prime},s,a)/\#(s,a)\) denote our estimate. We note that \(\mathbb{E}[\hat{p}_{s^{\prime}}]=p_{s^{\prime}}\), i.e. \(\hat{p}_{s^{\prime}}\) is an unbiased estimator for \(p_{s^{\prime}}\). Let \(m=\#(s,a)\) also be the number of times that \((s,a)\) has been observed, then by Hoeffding's inequality [40] we have,

\[\mathbb{P}\left[|p_{s^{\prime}}-\hat{p}_{s^{\prime}}|\geq\frac{\epsilon}{H| \mathcal{S}|}\right]\leq 2\exp\left(-2m\frac{\epsilon^{2}}{H^{2}|\mathcal{S}|^{2}}\right)\] (57)

Bounding the LHS from above by \(1-\delta/2(|\mathcal{A}||\mathcal{S}|^{2})\) and rearranging gives the following lower bound for \(m\),

\[m\geq\frac{H^{2}|\mathcal{S}|^{2}}{2\epsilon^{2}}\log\left(\frac{4|\mathcal{A }||\mathcal{S}|^{2}}{\delta}\right)\] (58)

Taking a union bound over all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\), then for all state action pairs \((s,a)\in\mathcal{S}\times\mathcal{A}\) we have the following with probability at least \(1-\delta\).

\[2D_{TV}\left(\mathcal{P}(\cdot\mid s,a),\widehat{\mathcal{P}}(\cdot\mid,s,a) \right)=\sum_{s^{\prime}\in\mathcal{S}}|p_{s^{\prime}}-\hat{p}_{s^{\prime}}| \leq\sum_{s^{\prime}\in\mathcal{S}}\frac{\epsilon}{H|\mathcal{S}|}\leq \epsilon/H\] (59)

Now fix some \(s\in\mathcal{S}\) and we observe the following,

\[2D_{TV}\left(\mathcal{P}_{\pi}(\cdot\mid s),\widehat{\mathcal{P }}_{\pi}(\cdot\mid s)\right) =\sum_{s^{\prime}\in\mathcal{S}}|\mathcal{P}_{\pi}(s^{\prime}\mid s )-\widehat{\mathcal{P}}_{\pi}(s^{\prime}\mid s)|\] (60) \[=\sum_{s^{\prime}\in\mathcal{S}}\sum_{a\in\mathcal{A}}|\mathcal{ P}(s^{\prime}\mid s,a)\pi(a\mid s)-\widehat{\mathcal{P}}(s^{\prime}\mid s,a)\pi(a\mid s)|\] (61) \[=\sum_{a\in\mathcal{A}}\pi(a\mid s)\sum_{s^{\prime}\in\mathcal{S }}|\mathcal{P}(s^{\prime}\mid s,a)-\widehat{\mathcal{P}}(s^{\prime}\mid s,a)|\] (62) \[=\sum_{a\in\mathcal{A}}\pi(a\mid s)2D_{TV}\left(\mathcal{P}(\cdot \mid s,a),\widehat{\mathcal{P}}(\cdot\mid,s,a)\right)\] (63) \[\leq\epsilon/H\] (64)

Thus with probability at least \(1-\delta/2\) we have for all \(s\in\mathcal{S}\) that,

\[D_{TV}\left(\mathcal{P}_{\pi}(\cdot\mid s),\widehat{\mathcal{P}}_{\pi}(\cdot \mid s)\right)\leq\epsilon/H\] (65)

**(2)** Using Assumption 6.3 and 6.4 we can argue about the safety of the system. Suppose firstly, that we can check the condition \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept)\leq p_{1}\), precisely and without any failure probability (we will deal with statistical model checking in part **(3)**). From any non-critical state we can transition arbitrarily to a critical state, although under Assumption 6.3 this critical state is not irrecoverable with probability \(\geq p_{1}\). We now consider the following two cases:

(i) \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept)\leq p_{1}\) under the 'task' policy.

(ii) \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle>p_{1}\) under the 'task' policy.

For case (i) we can safely use the 'task' policy and return to a non-critical state within \(H\) timesteps with probability at least \(1-p_{1}\). For case (ii) we deploy the'safe' policy and under Assumption 6.4 we can return to a non-critical state within \(H\) timesteps with probability at least \(1-p_{1}\). We have now established an invariant, since from every non-critical state we can return to a non-critical state with probability \(1-p_{1}\) and thus satisfy \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p_{1}\) at every timestep \(t\in[0,T]\).

**(3)** We now make a similar argument but for the statistical model checking procedure where we can only obtain an \(\epsilon\)-approximate estimate for the probability \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\) with high probability. Let us denote our \(\epsilon\)-approximate estimate \(\widehat{\Pr}(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\), rather than check the condition \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p_{1}\), we can check condition \(\widehat{\Pr}(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p _{1}-\epsilon\), and if \(\widehat{\Pr}(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\) is indeed an \(\epsilon\)-approximate estimate then this guarantees \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p_{1}\). Consider the following two cases:

(i) Our estimate \(\widehat{\Pr}(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p _{1}-\epsilon\)

(ii) Our estimate \(\widehat{\Pr}(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle>p_{1}-\epsilon\)

For case (i) we can safely use the 'task' policy and return to a non-critical state within \(H\) timesteps with probability at least \(1-p_{1}\). For case (ii) we deploy the'safe' policy and under Assumption 6.4 we can return to a non-critical state within \(H\) timesteps with probability at least \(1-p_{1}\). Again we have established an invariant, since from every non-critical state we can return to a non-critical state with probability \(1-p_{1}\) and thus satisfy \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept\rangle\leq p_{1}\) at every timestep \(t\in[0,T]\).

We still need to deal with the failure probability of the statistical model checking procedure at each timestep, by choosing failure probability \(1-\delta/2T\) we can guarantee (by a union bound) an \(\epsilon\)-approximate estimate for each timestep with probability \(1-\delta/2\). Finally, taking a union bound over part **(1)** and **(2)** gives the desired total failure probability \(1-\delta\).

## Appendix D Environment Details

### Colour Gridworld

The colour gridworld environment is a simple \(9\times 9\) grid, with state space \(|\mathcal{S}|=81\) and action space \(|\mathcal{A}|=5\), where each action corresponds to the following movements: _Left,Right, Up, Down, Stay_. The objective is to navigate from the start state in one corner of the grid, to the goal state in the other corner, after reaching the goal state the agent is then sent back to the start state. The agent must navigate to the goal state as many times as possible in a fixed episode length of \(T=1000\). The reward function is a sparse reward that gives the agent \(+1\) reward for reaching the goal and \(0\) otherwise. When the environment is fully deterministic the maximum achievable reward is \(58\).

In addition to the goal state, there are three other distinct states, _green_, _blue_ and _purple_, each labelled with their corresponding colours, see Fig. 4. The set of atomic propositions is thus \(AP=\{green,\textit{blue},\textit{purple},\textit{goal}\}\), the safety properties are specified over the set \(AP\), in particular we conduct experiments with 3 different safety properties of increasing complexity:

* (1) \(\square\)_green_
* (2) \(\square\)_goal_\(\rightarrow\Diamond^{\leq 10}\)_blue_
* (3) \(\square\)_goal_\(\rightarrow\Diamond^{\leq 10}\square^{\leq 5}\)_purple_

Property (1) is a simple invariant property \(P_{inv}(\neg\textit{green})\) that states the green state must always be avoided. Property (2) and (3) are more complex safety properties that interfere with the goal state. In

Figure 4: Colour gridworld environment. Top left hand corner (_agent_) is the start position. The agent must navigate to the _goal_ position in the bottom right hand corner of gridworld. The coloured states labelled _blue_, _green_ and _purple_ correspondingly.

[MISSING_PAGE_FAIL:24]

### Atari Seaquest

Our DreamerV3 [34] based shielding procedure is tested on Atari Seaquest, provided as part of the Arcade Learning Environment (ALE)[10; 50]. Seaquest is a partially observable environment meaning we do not have direct access to the underlying state space \(\mathcal{S}\), we are however provided with observations \(o\in O\) as pixel images which correspond to \(64\times 64\times 3\) tensors. Fortunately DreamerV3 is specifically designed to operate in visual settings and is able to effectively learn a predictive world model that closely approximate the environment dynamics. The action space of Seaquest is finite, specifically \(|\mathcal{A}|=18\), where each action corresponds to a joystick movement and fire button interaction. Rewards are obtained by'shooting' an enemy shark or submarine, or by rescuing divers and returning them to the surface. In addition, the agent must manage its oxygen resources and avoid being hit by sharks and the enemy submarines which fire back, see Fig. 6. The environment is also made stochastic by using'sticky actions' [50], where the agents previous action is repeated with probability \(p=0.25\).

In terms of safety properties we experiment with the following two properties,

* (1) \((\square\neg\text{\emph{surface}}\rightarrow\square(\text{\emph{surface}} \rightarrow\text{\emph{diver}}))\wedge(\square\neg\text{\emph{out-of-oxygen}}) \wedge(\square\neg\text{\emph{hit}})\)
* (2) \(\square\text{\emph{diver}}\wedge\neg\text{\emph{surface}}\rightarrow\lozenge^{ \leq 30}\text{\emph{surface}}\)

Property (1) states that after diving (i.e. not _surface_), the agent must only _surface_ with a _diver_ on board, and never run _out-of-oxygen_ and never get _hit_ by an enemy. The size of the DFA for this property is \(|\mathcal{D}|=4\). Property (2) states that once a _diver_ is on board the agent must _surface_ within 30 timesteps (i.e. rescue the diver).

Figure 5: Episode reward and cost for tabular RL ‘colour’ gridworld environment.

Figure 6: Atari Seaquest environment [10; 50]. The goal is to rescue divers (_small blue people_), while shooting enemy _sharks_ and _submarines_.

**Hyperparameter settings.** For our shielding approach almost all the hyperparameters are specified in Appendix E. The only hyperparameter that varies is the model checking horizon \(H\). For property (1) we use \(H=30\), empirically this seems adequate enough to avoid running _out-of-oxygen_ and begin surfacing in enough time. For property (2) we use \(H=50\), this is to avoid picking up a _diver_ at the bottom of the ocean where it may not be possible to return to the surface in 30 timesteps.

**Extended discussion of results.** First we provide slightly larger figures that than provided in the main paper, see Figure 7

For both safety properties DreamerV3 with shielding obtains comparative performance in terms of reward with the unmodified DreamerV3 baseline. Of course this baseline entirely ignores the safety properties and simply maximizes reward. We remark on the differences between the safety properties themselves, property (1) in particular specifies the natural safety properties of the environment, since violating property (1) results in a death, the agent only start with 4 lives (and can gain one more ever 10000 points) and so satisfying property (1) is beneficial for long term reward, short the behaviour satisfying property (1) is correlated with higher reward and we might expect the globally optimal policy in the environment to never violated property (1). Property (2) specifies that once a diver is recovered the submarine must return to the surface in 30 timesteps, we would not expect that the globally optimal policy satisfies this property (2) rather we would expect to converge to a locally optimal policy satisfying property (2) while still obtaining good reward.

With respect to the baseline DreamerV3 (LAG) which has access to the cost function, we see that in both cases it fails to reliable learn a safe policy that simultaneously maximizes reward. For property (2) DreamerV3 (LAG) appear to do slightly better in terms of safety, however when qualitatively inspecting the runs for property (2) we see the DreamerV3 (LAG) agent intentionally get hit by enemy submarines/sharks to re-spawn on the surface without actually having to navigate there. This may be a more effective way to satisfy the safety property with high probability but it clearly leads to worse long term reward.

## Appendix E Hyperparameters & Implementation Details

### Access to Code

To maintain a high standard of anonymity we provide code for the experiments run on 'colour' gridworld as supplementary material, rather than through GitHub. The colour gridworld environment is implemented with the Gym [14] interface. Tabular Q-learning is implemented with _numpy_ in _Python_, the model checking procedures (both exact and Monte Carlo) are implemented with JAX [12] which supports vectorized computation on GPU and CPU. The code for the Atari Seaquest experiments

Figure 7: Episode reward and violation rate for deep RL Atari Seaquest.

are not currently available, although our code base was heavily derived from the code base for _Approximate Model-based Shielding_ (AMBS) [30], see https://github.com/sacktock/AMBS (MIT License).

Training details.For collecting both sets of experiments we has access to 2 Nvidia Tesla A30 (24GB RAM) GPU and a 24-core/48 thread Intel Xeon CPU each with 32GB RAM. For the 'colour' gridworld experiments each run can take several minutes up to a day depending on which property is being tested, for example one run for property (3) can take roughly 1.5 days as the product state space is fairly large. For the Atari Seaquest experiments each run can take 8 hours to 1 day depending on the precise configuration of DreamerV3, in general we see a slow down of \(\times 2\) when using shielding compared to the unmodified DreamerV3 baseline. Memory requirements may differ depending on the DreamerV3 configuration used, for the _xlarge_ DreamerV3 configuration 32GB of GPU memory should suffice.

Statistical significance.Error bars are provided for each of our experiments. In particular, we report 5 random initializations (seeds) for each experiment, the error bars are non-parametric (bootstrap) 95% confidence intervals, provided by seaborn.lineplot with default parameters: errorbar=('ci', 95), n_boot=1000. The error bars capture the randomness in the initialization of the DreamerV3 world model and policy parameters, the randomness of the environment and any randomness in the batch sampling.

[MISSING_PAGE_EMPTY:28]

### Atari Seaquest

\begin{table}
\begin{tabular}{l c c} \hline \hline Name & Symbol & value \\ \hline \multicolumn{3}{c}{General} \\ \hline Replay capacity & \(|D|\) & \(10^{6}\) \\ Batch size & \(|B|\) & 16 \\ Batch length & - & 64 \\ Number of envs & - & 8 \\ Train ratio & - & 64 \\ Number of MLP layers & - & 5 \\ Number of MLP units & - & 1024 \\ Activation & - & LayerNorm + SiLU \\ \hline \multicolumn{3}{c}{World Model} \\ \hline Configuration size & - & medium \\ Number of latents & - & 32 \\ Classes per latent & - & 32 \\ Number of layers & - & 3 \\ Number of hidden units & - & 640 \\ Number of recurrent units & - & 1024 \\ CNN depth & - & 48 \\ RSSM loss scales & \(\beta_{\text{pred}}\), \(\beta_{\text{dyn}}\), \(\beta_{\text{rep}}\) & 1.0, 0.5, 0.1 \\ Predictor loss scales & \(\beta_{o},\beta_{r},\beta_{c},\beta_{\gamma}\) & 1.0, 1.0, 1.0, 1.0 \\ Learning rate & - & \(10^{-4}\) \\ Adam epsilon & \(\epsilon_{\text{adam}}\) & \(10^{-8}\) \\ Gradient clipping & - & 1000 \\ \hline \multicolumn{3}{c}{Actor Critic} \\ \hline Imagination horizon & \(H\) & 15 \\ Discount factor & \(\gamma\) & 0.997 \\ TD lambda & \(\lambda\) & 0.95 \\ Critic EMA decay & - & 0.98 \\ Critic EMA regularizer & - & 1 \\ Return norm. scale & \(S_{\text{reward}}\) & \(\text{Per}(R,95)-\text{Per}(R,5)\) \\ Return norm. limit & \(L_{\text{reward}}\) & 1 \\ Return norm. decay & - & 0.99 \\ Actor entropy scale & \(\eta_{\text{actor}}\) & \(3\cdot 10^{-4}\) \\ Learning rate & - & \(3\cdot 10^{-5}\) \\ Adam epsilon & \(\epsilon_{\text{adam}}\) & \(10^{-5}\) \\ Gradient clipping & - & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 6: DreamerV3 [34]

\begin{table}
\begin{tabular}{l c c} \hline \hline Name & Symbol & value \\ \hline \multicolumn{3}{c}{Augmented Lagrangian} \\ \hline Penalty multiplier & \(\mu_{k}\) & \(5\cdot 10^{-9}\) \\ Initial Lagrange multiplier & \(\lambda^{k}\) & \(0.01\) \\ Penalty power & \(\sigma\) & \(10^{-6}\) \\ Cost coefficient & \(C\) & \(1.0\) \\ Cost threshold & \(d\) & \(1.0\) \\ \hline \multicolumn{3}{c}{Penalty Critic} \\ \hline \multicolumn{3}{c}{See ‘Actor Critic’ in Table 6} \\ \multicolumn{3}{c}{...} \\ \hline \hline \end{tabular}
\end{table}
Table 7: Augmented Lagrangian [7, 41, 72]

\begin{table}
\begin{tabular}{l c c} \hline \hline Name & Symbol & value \\ \hline \multicolumn{3}{c}{Shielding} \\ \hline Approximation error & \(\epsilon\) & \(0.09\) \\ Number of samples & \(m\) & \(512\) \\ Failure probability & \(\delta\) & \(0.01\) \\ Look-ahead/shielding horizon & \(H\) & varies \\ Satisfaction prob. & \(p\) & \(0.9\) \\ Cost coefficient & \(C\) & \(10\) \\ \hline \multicolumn{3}{c}{‘Task policy’} \\ \hline \multicolumn{3}{c}{See ‘Actor Critic’ in Table 6} \\ \multicolumn{3}{c}{...} \\ \hline \multicolumn{3}{c}{‘Backup policy’} \\ \hline \multicolumn{3}{c}{See ‘Actor Critic’ in Table 6} \\ \multicolumn{3}{c}{...} \\ \hline \hline \end{tabular}
\end{table}
Table 8: DreamerV3 with Shielding (Algorithm 5)Ablation Studies

In this section we provide several ablation studies for the 'colour' gridworld environment. We test the most significant hyperparameters and algorithmic components of our method including the baseline (Q-learning with penalties). In particular we demonstrate the counter factual experiences is crucial for learning the safety properties of the environment when the size of the corresponding DFA is non trivial. We also experiment with using exact model checking - demonstrating that we don't loose much by using statistical model checking procedures. Furthermore, we experiment with the cost coefficient \(C\), the model checking horizon \(H\) and the level of stochasticity \(p\).

### Counter factual experiences

We run our method and the baseline (Q-learning with penalties) without counterfactual experiences to train the 'backup policy' or penalized task policy (baseline).

For property (2) and (3) we see a significant drop in safety performance, since learning to respect the safety property over the much larger product state space will require much more experience and without exploiting the structure of the DFA (using counter factual experiences) to generate synthetic data the task behaviour will be much more quickly learnt. For property (1), the invariant property, we observe identical performance as the DFA is trivial (only 2 states), and so counter factual experiences is essentially redundant in this case.

### Exact model checking

We run our method (Shielding) with two different configurations: exact model checking with the 'approximate' transition probabilities (learning from experience) and exact model checking with the 'true' transition probabilities. We compare these two methods to the configuration used in the main paper: Monte Carlo (statistical) model checking with the learned transition probabilities.

In all cases we see that Shield (MC-Approx) obtains almost identical performance to Shield (Exact-True), which demonstrates that we don't loose much by statistical model checking with the learned probabilities, when for example we don't have access to the transition probabilities ahead of time, or the MDP is too large to exact model check. We see some variance with Shield (Exact-Approx), which can be explained by sub-optimal convergence in terms of reward, although note that the safety performance is consistent with the other configurations. Perhaps exact model checking with an inaccurate model of the transition probabilities restricts exploration to areas of the state space that are actually safe.

Figure 8: Episode reward and cost for Q-learning (Shield) and Q-learning (COST-CF) with and without counterfactual experiences (CF).

Figure 9: Episode reward and cost for Shield (Exact-True) – exact model checking with the ‘true’ probabilities, Shield (Exact-Approx) - exact model checking with the learning transition probabilities, and Shield (MC-Approx) – from the main paper.

### Cost coefficient \(C\)

We experiment with different values for the cost coefficient \(C\) used for our baseline (Q-learning with penalties). In particular, we use \(C\in\{0.1,1.0,10.0,100.0\}\), we expect that a larger cost coefficient will penalize unsafe behaviour more harshly and result in'safer' behaviour (i.e., fewer safety-property violations).

Unsurprisingly, across the board, by increasing the cost coefficient \(C\) we obtain a policy that has fewer safety-property violations. The improved'safety performance' is of course at the expense of reward or task performance, this is a trade-off we would expect. In particular for \(C=100.0\) we see that the learned policy essentially avoids the goal state (achieving zero reward) all but guaranteeing safety (no safety-violations). The purpose of this ablation study is to demonstrate that while we can achieve any desired level of safety by tuning the cost coefficient \(C\), the actual value of \(C\) offers little to no semantic meaning for the probability of violating the safety property.

### Model checking horizon \(H\)

As was alluded to in the main paper, our method can be very sensitive to the model checking horizon (hyperparameter) \(H\). In particular, if \(H\) is too large then we might expect the system to exhibit overly conservative behaviour. As a rule of thumb we suggest that \(H\) should be set to roughly the shortest path in the DFA from the initial state to an accepting state - this can easily be computed by using Dijkstra's (shortest-path) algorithm. In this ablation we experiment with much larger \(H\) than recommended. This significantly impacts the performance of our proposed approach. However, we do propose a solution, Q-learning (Shield-Rec) which in short, checks that the action proposed by the 'task policy' is recoverable with the 'backup policy', or in other words by playing with the action \(a\sim\pi_{\textit{task}}\) proposed by the 'task policy' We can still satisfy \(\Pr(\langle s,q\rangle\models\Diamond^{\leq H}accept)\leq p_{1}\) by using the 'backup policy' after playing \(a\).

In general we observe that when \(H\) is too large our original method (Shield) is overly conservative, sacrificing reward or task performance for safety guarantees. Our proposed solution (Shield-Rec) is alleviates this issue partly, providing reasonable safety performance and comparable task performance. We note that this solution is clearly not perfect as is it appears to be slightly more permissive allowing more safety-violations than necessary. More investigation into this framework would be interesting future work, and perhaps more hyperparameter tuning, specifically by tuning \(p_{1}\), could improve this method. The goal would be to obtain an algorithm that is not overly sensitive to \(H\), and as long as \(H\) is sufficiently big to guarantee safety we don't see much performance degradation by further increasing \(H\).

Figure 11: Episode reward and cost for Q-learning (Shield) - from the main paper, Q-learning (Shield) with bigger \(H\) and Q-learning (Shield-Rec) with bigger \(H\).

Figure 10: Episode reward and cost for Q-learning (COST-CF) – baseline from the main paper, with different cost coefficients \(C\).

### Level of stochasticity \(p\)

Finally we investigate the effect of the level of stochasticity of the environment. Specifically, the value \(p\) corresponding the the probability that the agent's action is ignored and another action is chosen (uniformly at random) from the action space and played instead. For example, of \(p=0.25\) and the agent chooses the action _Right_, there is a \(75\%\) chance that the agent goes right and a \(25\%\) chance the agent goes a different direction. If \(p=0.0\) (deterministic environment) then achieving complete safety (zero-violations) becomes easier as the agent has complete control of the environment through their actions.

We experiment with the following \(p\) values: \(p=0.1\) for property (1), \(p=0.1\) for property (2) and \(p=0.05\) for property (3). For these smaller \(p\) values we would expect it to be easier for our methods including the baseline to achieve a higher-rate of safety and possibly complete safety in some cases.

We see a similar situation as in the main paper, Q-learning (without penalties) simply finds the best policy ignoring costs. However, Q-learning (with penalties) is able to obtain the same performance now as our method Q-learning (Shield), both in terms of reward and cost. With a smaller \(p\) value the safety-property can be satisfied with higher probability while still visiting the goal state frequently and obtaining high reward. In particular, these \(p\) values are chosen such that each of the safety properties can be satisfies with probability at least \(0.9\) from the goal state, thus penalizing safety-violations with \(C=10.0\) appears to be enough to guarantee safety above \(0.9\) at each timestep while still achieving high reward. For different values of \(C\) we might expect the baseline to have a different performance profile.

## Appendix G Comparison to CMDP

In this additional section we analyze the relationships between our problem setup and other common CMDP settings, for both the finite horizon and corresponding (discounted) infinite horizon problems.

### Finite Horizon

For reference we restate Problem 4.1 here.

**Problem 4.1 (restated)** (Step-wise bounded regular safety property constraint).: _Let \(P_{\text{safe}}\) be a regular safety property, \(\mathcal{D}\) be the DFA such that \(\mathcal{L}(\mathcal{D})=\text{BadPref}(P_{\text{safe}})\) and \(\mathcal{M}\) be the MDP;_

\[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\Pr\left(\langle s_{t},q_{t}\rangle \models\lozenge^{\leq H}accept\right)\leq p_{1}\quad\forall t\in[0,T]\]

_where all probability is taken under the product Markov Chain \(\mathcal{M}_{\pi}\otimes\mathcal{D}\), \(p_{1}\in[0,1]\) is a probability threshold, \(H\) is the model checking horizon and \(T\) is the fixed episode length._

#### g.1.1 Expected Cumulative Constraint

First we restate Problem 4.4.

**Problem 4.4 (restated)** (Expected cumulative constraint [4, 58]).: \[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\mathbb{E}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\left[\sum_{t=0}^{T}\mathcal{C} (\langle s_{t},q_{t}\rangle)\right]\leq d_{1}\]

_where \(d_{1}\in\mathbb{R}_{+}\) is the cost threshold and \(T\) is the fixed episode length._

**Proposition G.1**.: _A feasible policy \(\pi\) for Problem 4.1 with parameters \(p_{1}\in[0,1]\) is also a feasible policy for Problem 4.4 with parameter \(d_{1}\in\mathbb{R}_{+}\), provided that \(d_{1}\geq(T+1)\cdot p_{1}\)._

Proof.: For \(t\in[0,T]\) we define, the following random variables, \(X_{0},\ldots,X_{T}\), where

\[X_{t}=\mathcal{C}(\langle s_{t},q_{t}\rangle)=1\left[accept\in L^{\prime}( \langle s_{t},q_{t}\rangle)\right]\] (66)

where,

\[\mathbb{E}\left[X_{t}\right] =\mathbb{E}\left[1\left[accept\in L^{\prime}(\langle s_{t},q_{t} \rangle)\right]\right]\] (67) \[=\Pr\left(accept\in L^{\prime}(\langle s_{t},q_{t}\rangle)\right)\] (68) \[\leq p_{1}\] (69)

The argument is straightforward if at every timestep \(t\in[0,T]\) we have \(\Pr(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept)\leq p_{1}\) then with probability \(\leq p_{1}\) we have \(accept\in L(\langle s_{t},q_{t}\rangle)\). Then, under mild assumptions (i.e. \(\mathcal{C}(\langle s_{t},q_{t}\rangle)<\infty\)) we consider the following decomposition of the expected cumulative cost,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{ t}\rangle)\right] =\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}X_{t}\right]\] (70) \[=\mathbb{E}_{s_{0}\sim\mathcal{P}_{0}(\cdot)}\left[X_{0}\right]+ \mathbb{E}_{s_{1}\sim\mathcal{P}_{1}(\cdot)}\left[X_{1}\right]+\ldots+\mathbb{ E}_{s_{T}\sim\mathcal{P}_{T}(\cdot)}\left[X_{T}\right]\] (71) \[=\mathbb{E}_{\pi}\left[X_{0}\right]+\mathbb{E}_{\pi}\left[X_{1} \right]+\ldots+\mathbb{E}_{\pi}\left[X_{T}\right]\] (72)

We replace the subscript '\(\langle s_{t},q_{t}\rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}\)' here for brevity. Clearly by linearity of expectations this statement holds. Although it is worth noting that each expectation is taken under a different marginal state distribution (i.e. \(\mathcal{P}_{t}(\cdot)\)), which depends on \(\pi\) (apart from the initial state distribution \(\mathcal{P}_{0}(\cdot)\)). From now on we will write this is implicitly (i.e. Eq. 72), rather than writing the marginal state distribution (at time \(t\)) for each expectation. Using our earlier observations we can now bound the expected cumulative cost from above as follows,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{ t}\rangle)\right] =\mathbb{E}_{\pi}\left[X_{0}\right]+\mathbb{E}_{\pi}\left[X_{1} \right]+\ldots+\mathbb{E}_{\pi}\left[X_{T-1}\right]+\mathbb{E}_{\pi}\left[X_{T}\right]\] (73) \[\leq(T+1)\cdot p_{1}\] (74)

**Proposition G.2**.: _The converse is not strictly true, since there may be a feasible policy \(\pi\) for Problem 4.4 with threshold \(d_{1}\leq(T+1)\cdot p_{1}\) which does not satisfy the constraints of Problem 4.1._

Proof.: We want to prove the following statement, a policy \(\pi\) satisfying,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle) \right]\leq(T+1)\cdot p_{1}\] (75)

does not imply that,

\[\Pr\left(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept\right)\leq p _{1}\quad\forall t\in[0,T]\] (76)

To prove this we will show that there may be some policy \(\pi\) that satisfies Eq. 75, but does not satisfy Eq. 76 at some timestep \(t\). For simplicity we consider the first timestep (i.e. \(t=0\)). First we assume \(\pi\) is such that Eq. 75 holds, assuming \(H\leq T\) then clearly we have,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{H}\mathcal{C}(\langle s_{t},q_{t}\rangle) \right]\leq\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t} \rangle)\right]\leq(T+1)\cdot p_{1}\] (77)

Let \(\Pr(\langle s_{0},q_{0}\rangle\models\Diamond^{\leq H}accept)\) denote the proportion of accepting paths from the initial state \(s_{0}\sim\mathcal{P}_{0}(\cdot)\) and automaton state \(q_{0}=\Delta(\mathcal{Q}_{0},L(s_{0}))\). Suppose \(\pi\) is such that \(\Pr(\langle s_{0},q_{0}\rangle\models\Diamond^{\leq H}accept)>p_{1}\). We note that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\Diamond^{\leq H}accept\) the sum \(\sum_{t=0}^{H}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq 1\), and now we have,

\[(T+1)\cdot p_{1}\geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_ {t},q_{t}\rangle)\right]\geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{H}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right]>p_{1}\] (78)Now clearly for all \(p_{1}\in[0,1]\) and \(T\in\mathbb{Z}_{+}\) the following holds,

\[p_{1}<(T+1)\cdot p_{1}\] (79)

This implies that there may exist some \(\pi\) satisfying Eq. 75 and such that \(\Pr(\langle s_{0},q_{0}\rangle\models\lozenge^{\leq H}accept)>p_{1}\), i.e. does not satisfy Eq. 76 at timestep \(t=0\). 

**Proposition G.3**.: _A feasible policy \(\pi\) for Problem 4.4 with threshold \(d_{1}\leq p_{1}\), satisfies \(\Pr(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept)\leq p_{1}\) for all \(t\in[0,T]\). This bound is tight._

Proof.: Firstly, a feasible policy \(\pi\) for Problem 4.4 with threshold \(d_{1}\leq p_{1}\) clearly satisfies,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle) \right]\leq p_{1}\] (80)

Assuming \(H\leq T\), then this implies that for all \(t^{\prime}\in[0,T-H]\) we have,

\[\mathbb{E}_{\pi}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s _{t},q_{t}\rangle)\right]\leq\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right]\leq p_{1}\] (81)

Let \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\lozenge^{\leq H}accept)\) denote the proportion of accepting paths at timestep \(t^{\prime}\), where \(s_{t^{\prime}}\sim\mathcal{P}_{t^{\prime}}(\cdot)\). Here \(\mathcal{P}_{t^{\prime}}(\cdot)\) denotes the marginal state distribution at time \(t^{\prime}\). Recall that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\lozenge^{\leq H}accept\) the sum \(\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq 1\). Without loss of generality fix some \(t^{\prime}\in[0,T-H]\) and suppose that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\lozenge^{\leq H} accept)>p_{1}\). This implies that,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle) \right]\geq\mathbb{E}_{\pi}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{ C}(\langle s_{t},q_{t}\rangle)\right]>p_{1}\] (82)

Which is a contradiction. Therefore, it must be the case that when Eq. 80 is satisfied then so is \(\Pr(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept|)\leq p_{1}\) for all \(t\in[0,T-H]\). For the remaining \(t^{\prime}\in[T-H,T]\) a similar argument can be made, the only detail is to ensure the sum in Eq. 81 is up to \(T\) rather than \(t^{\prime}+H\). To prove that this bound is tight we can again show the possible existence of a counter example. In particular, we want to prove the following statement, a policy \(\pi\) satisfying,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle) \right]\leq p_{1}+c\] (83)

for some constant \(c>0\), does not imply that,

\[\Pr\left(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept\right)\leq p _{1}\quad\forall t\in[0,T]\] (84)

We will show that there may exist some policy \(\pi\) that satisfies Eq. 83 but does not satisfy Eq. 84 at some timestep \(t\). Firstly, we assume \(\pi\) is such that Eq. 83 holds, this implies that for all \(t^{\prime}\in[0,T-H]\) we have,

\[\mathbb{E}_{\pi}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s _{t},q_{t}\rangle)\right]\leq\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right]\leq p_{1}+c\] (85)

Fix some \(t^{\prime}\in[0,T-H]\) and once again let \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\lozenge^{\leq H}accept)\) denote the proportion of accepting paths at timestep \(t^{\prime}\). Suppose \(\pi\) is such that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\lozenge^{\leq H}accept )>p_{1}\). Again recall that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding trace \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\lozenge^{\leq H}accept\) the sum \(\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq 1\), and so,

\[p_{1}+c\geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t} \rangle)\right]\geq\mathbb{E}_{\pi}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H} \mathcal{C}(\langle s_{t},q_{t}\rangle)\right]>p_{1}\] (86)

Now clearly for all \(p_{1}\in[0,1]\) and \(c>0\), the following holds,

\[p_{1}<p_{1}+c\] (87)

This implies that there may exist some \(\pi\) satisfying Eq. 83 and such that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\lozenge^{\leq H}accept )>p_{1}\), i.e. does not satisfy Eq. 84 at timestep \(t=t^{\prime}\)

#### f.1.2 Probabilistic Cumulative Constraint

First we restate Problem 4.5.

**Problem 4.5 (restated)** (Probabilistic cumulative constraint [18, 56]).: \[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\mathbb{P}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\left[\sum_{t=0}^{T}\mathcal{C} (\langle s_{t},q_{t}\rangle)\leq d_{2}\right]\geq 1-\delta_{2}\]

_where \(d_{2}\in\mathbb{R}_{+}\) is the cost threshold, \(\delta_{2}\) is a tolerance parameter and \(T\) is the fixed episode length._

**Proposition G.4**.: _A feasible policy \(\pi\) for Problem 4.1 with parameters \(p_{1}\in[0,1]\) is also a feasible policy for Problem 4.5 with parameters \(d_{2}\in\mathbb{R}_{+}\) and \(\delta_{2}\in(0,1]\), provided that, \(d_{2}\geq\sqrt{(T+1)/2\cdot\log(1/\delta_{2})}+(T+1)\cdot p_{1}\)._

Proof.: For \(t\in[0,T]\) we define the following random variables, \(X_{0},\ldots,X_{T}\), where,

\[X_{t}=\mathcal{C}(\langle s_{t},q_{t}\rangle)=1\left[accept\in L^{\prime}( \langle s_{t},q_{t}\rangle)\right]\] (88)

and we make the same following observation,

\[\mathbb{E}\left[X_{t}\right] =\mathbb{E}\left[1\left[accept\in L^{\prime}(\langle s_{t},q_{t} \rangle)\right]\right]\] (89) \[=\Pr\left(accept\in L^{\prime}(\langle s_{t},q_{t}\rangle)\right)\] (90) \[\leq p_{1}\cdot\delta\] (91)

See the proof of Prop. G.1 for details, the argument is identical. Once again, under mild assumptions (i.e. \(\mathcal{C}(\langle s_{t},q_{t}\rangle)<\infty\)) we consider the following decomposition of the expected cumulative cost,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_ {t}\rangle)\right] =\mathbb{E}_{\pi}\left[X_{0}\right]+\mathbb{E}_{\pi}\left[X_{1} \right]+\ldots+\mathbb{E}_{\pi}\left[X_{T}\right]\] (92) \[\leq(T+1)\cdot p_{1}\] (93)

Again we replace the subscript '\(\langle s_{t},q_{t}\rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}\)' here for brevity, see the proof of Prop. G.1 for the full details. Before we proceed we must first deal with the dependence between the random variables \(X_{0},\ldots,X_{T}\). Strictly speaking it is not the case that \(\Pr(X_{t}=1\mid X_{t-1},\ldots,X_{0})=\Pr(X_{t}=1)\). However, we have already established that \(\Pr(X_{t}=1)\leq p_{1}\), as such we can simulate \(X_{0},\ldots,X_{T}\) as a sequence of independent coin flips \(Y_{0},\ldots,Y_{T}\) with probability \(p_{1}\), it is then the case that \(\mathbb{P}[\sum_{t=0}^{T}X_{t}>d_{2}]\leq\mathbb{P}[\sum_{t=0}^{T}Y_{t}>d_{2}]\). We can now continue by bounding the probability we care about,

\[1-\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t} \rangle)\leq d_{2}\right] =\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t} \rangle)>d_{2}\right]\] (94) \[=\mathbb{P}\left[\sum_{t=0}^{T}X_{t}>d_{2}\right]\] (95) \[\leq\mathbb{P}\left[\sum_{t=0}^{T}Y_{t}>d_{2}\right]\] (96) \[=\mathbb{P}\left[\sum_{t=0}^{T}Y_{t}>(T+1)\cdot p_{1}+d_{2}-(T+1 )\cdot p_{1}\right]\] (97) \[=\mathbb{P}\left[\sum_{t=0}^{T}Y_{t}>\mathbb{E}\left[\sum_{t=0}^ {T}Y_{t}\right]+d_{2}-(T+1)\cdot p_{1}\right]\] (98) \[\leq\exp\left(-\frac{2\cdot(d_{2}-(T+1)\cdot p_{1})^{2}}{\sum_{t=0 }^{T}(\max\{Y_{i}\}-\min\{Y_{i}\})^{2}}\right)\] (99) \[=\exp\left(-\frac{2\cdot(d_{2}-(T+1)\cdot p_{1})^{2}}{(T+1)}\right)\] (100)

The first inequality (Eq. 96) comes from our earlier construction and the second (Eq. 99) is obtained from Hoeffding's inequality [40] for bounded random variables. Finally, bounding the final expression from above by \(\delta_{2}\) and rearranging gives the desired result.

**Proposition G.5**.: _A feasible policy \(\pi\) for Problem 4.5 with parameters \(\delta_{2}\leq p_{1}\) and \(d_{2}<1\), satisfies \(\Pr(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept)\leq p_{1}\) for all \(t\in[0,T]\). This bound is tight._

Proof.: A feasible policy \(\pi\) for Problem 4.5 with parameters \(\delta_{2}\leq p_{1}\) and \(d_{2}<1\) clearly implies that,

\[\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle)<1\right] \geq 1-p_{1}\] (101)

Assuming \(H\leq T\), then this implies that for all \(t^{\prime}\in[0,T-H]\) we have,

\[\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s_{t},q_ {t}\rangle)<1\right]\geq\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{ t},q_{t}\rangle)<1\right]\geq 1-p_{1}\] (102)

Let \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept)\) denote the proportion of accepting paths at timestep \(t^{\prime}\), where \(s_{t^{\prime}}\sim\mathcal{P}_{t^{\prime}}(\cdot)\). Again \(\mathcal{P}_{t^{\prime}}(\cdot)\) denotes the marginal state distribution at time \(t^{\prime}\). Recall that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\Diamond^{\leq H}accept\) the sum \(\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq 1\). Without loss of generality fix some \(t^{\prime}\in[0,T-H]\) and suppose that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept)>p_ {1}\). This implies that,

\[\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq 1 \right]\geq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}( \langle s_{t},q_{t}\rangle)\geq 1\right]>p_{1}\] (103)

Which is a contradiction. Therefore, it must be the case that when Eq. 101 is satisfied then so is \(\Pr(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept)\leq p_{1}\) for all \(t\in[0,T-H]\). For the remaining \(t^{\prime}\in[T-H,T]\) a similar argument can be made, the only detail is to ensure the sum in Eq. 102 is up to \(T\) rather than \(t^{\prime}+H\). To prove that this bound is tight we can show the possible existence of a counter example. In particular, we want to prove the following statement, a policy \(\pi\) satisfying,

\[\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle)<1\right] \geq 1-(p_{1}+c)\] (104)

for some constant \(c>0\) does not imply that,

\[\Pr\left(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept\right)\leq p _{1}\quad\forall t\in[0,T]\] (105)

We will show that there may exist some policy \(\pi\) that satisfies Eq. 104 but does not satisfy Eq. 105 at some timestep \(t\). Firstly, we assume \(\pi\) is such that Eq. 104 holds, this implies that for all \(t^{\prime}\in[0,T-H]\) we have,

\[\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s_{t},q_ {t}\rangle)<1\right]\geq\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_ {t},q_{t}\rangle)<1\right]\geq 1-(p_{1}+c)\] (106)

Fix some \(t^{\prime}\in[0,T-H]\) and let \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept)\) denote the proportion of accepting paths at timestep \(t^{\prime}\). Suppose that \(\pi\) is such that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept )>p_{1}\). Again recall that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\Diamond^{\leq H}accept\) the sum \(\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq 1\), and so,

\[p_{1}+c\geq\mathbb{P}\left[\sum_{t=0}^{T}\mathcal{C}(\langle s_{t},q_{t}\rangle) \geq 1\right]\geq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\mathcal{C}( \langle s_{t},q_{t}\rangle)\geq 1\right]>p_{1}\] (107)

Now clearly for all \(p_{1}\in[0,1]\) and \(c>0\), the following holds,

\[p_{1}<p_{1}+c\] (108)

This implies that there may exist some \(\pi\) satisfying Eq. 104 and such that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept)>p_ {1}\), i.e. does not satisfy Eq. 105 at timestep \(t=t^{\prime}\)

#### g.1.3 Instantaneous constraint

First we restate Problem 4.6.

**Problem 4.6** (restated) (Instantaneous constraint [23, 60, 69]).: \[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\mathbb{P}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\big{[}\mathcal{C}(\langle s_{ t},q_{t}\rangle)\leq d_{3}\big{]}=1\quad\forall t\in[0,T]\]

**Proposition G.6**.: _A feasible policy \(\pi\) for Problem 4.6 with threshold \(d_{3}<1\) (otherwise the problem is trivial) is a feasible policy for Problem 4.1 if and only if \(p_{1}=0\)._

Proof.: We start by proving the 4.6\(\Rightarrow\) 4.1 direction. A feasible policy \(\pi\) for Problem 4.6 with \(d_{3}<1\) satisfies,

\[\Pr\left(\mathcal{C}(\langle s_{t},q_{t}\rangle)<1\right)=1\quad\forall t\in[ 0,T]\] (109)

which implies that,

\[\Pr\left(\mathcal{C}(\langle s_{t},q_{t}\rangle)=0\right)=1\quad\forall t\in [0,T]\] (110)

and by Defn. 4.3,

\[\Pr\left(accept\not\in L^{\prime}(\langle s_{t},q_{t}\rangle)\right)=1\quad \forall t\in[0,T]\] (111)

Then if for all \(t\in[0,T]\), \(accept\not\in L^{\prime}(\langle s_{t},q_{t}\rangle)\) then we have \(\Pr(\langle s_{0},q_{0}\rangle\not\models\Diamond accept)=1\), where \(q_{0}=\Delta(\mathcal{Q}_{0},L(s_{0}))\) and by extension we have \(\Pr(\langle s_{t},q_{t}\rangle\not\models\Diamond accept^{\leq H})=1\) for all \(t\in[0,T]\). This completes the proof of this direction.

Now we prove the 4.1\(\Rightarrow\) 4.6 direction. A policy \(\pi\) satisfying \(\Pr(\langle s_{t},q_{t}\rangle\models\Diamond accept^{\leq H}))=0\) for all \(t\in[0,T]\) implies that \(\Pr(\langle s_{t},q_{t}\rangle\not\models\Diamond accept^{\leq H})=1\) for all \(t\in[0,T]\) which implies the following,

\[\Pr\left(accept\not\in L^{\prime}(\langle s_{t},q_{t}\rangle)\right)=1\quad \forall t\in[0,T]\] (112)

and by Defn. 4.3,

\[\Pr\left[\mathcal{C}(\langle s_{t},q_{t}\rangle)=0\right]=1\quad\forall t\in [0,T]\] (113)

which implies that,

\[\Pr\left[\mathcal{C}(\langle s_{t},q_{t}\rangle)<1\right]=1\quad\forall t\in [0,T]\] (114)

which concludes the proof. 

### Infinite Horizon

While in this paper we only consider finite horizon problems with a fixed episode length \(T\), we note that we can also make a set of similar statements for the infinite horizon (discounted) setting. In this section we provide the corresponding statements and proofs for the infinite horizon setting. Firstly, we consider the following infinite horizon problem.

**Problem G.7** (Step-wise bounded regular safety property constraint).: _Let \(P_{\text{safe}}\) be a regular safety property, \(\mathcal{D}\) be the DFA such that \(\mathcal{L}(\mathcal{D})=\text{BadPref}(P_{\text{safe}})\) and \(\mathcal{M}\) be the MDP;_

\[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\Pr\left(\langle s_{t},q_{t} \rangle\models\Diamond^{\leq H}accept\right)\leq p_{1}\quad\forall t=0,1,2,\ldots\]

_where all probability is taken under the product Markov chain \(\mathcal{M}_{\pi}\otimes\mathcal{D}\), \(p_{1}\in[0,1]\) is a probability threshold \(H\) is the model checking horizon._

#### g.2.1 Expected Cumulative Constraint

**Problem G.8** (Expected cumulative constraint).: \[\max_{\pi}V_{\pi}\quad\text{subject to}\quad\mathbb{E}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\Big{[}\sum_{t=0}^{\infty}\gamma ^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\Big{]}\leq d_{1}\]

_where \(d_{1}\in\mathbb{R}_{+}\) is the cost threshold and \(\gamma\in[0,1)\) is the discount factor._

**Proposition G.9**.: _A feasible policy \(\pi\) for Problem G.7 with parameters \(p_{1}\in[0,1]\), is also a feasible policy for Problem G.8 with parameter \(d_{1}\in\mathbb{R}_{+}\), provided that \(d_{1}\geq T\cdot p_{1}\), where \(T=1/(1-\gamma)\) is the effective horizon._Proof.: For \(t=0,1,2,\ldots\) we define, the following random variables, \(X_{0},X_{1},X_{2},\ldots\), where,

\[X_{t}=\mathcal{C}(\langle s_{t},q_{t}\rangle)=1\left[accept\in L^{\prime}( \langle s_{t},q_{t}\rangle)\right]\] (115)

where,

\[\mathbb{E}\left[X_{t}\right] =\mathbb{E}\left[1\left[accept\in L^{\prime}(\langle s_{t},q_{t} \rangle)\right]\right]\] (116) \[=\Pr\left(accept\in L^{\prime}(\langle s_{t},q_{t}\rangle)\right)\] (117) \[\leq p_{1}\] (118)

The argument for this is straightforward. If at every timestep \(t=0,1,2,\ldots\) we have \(\Pr(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept\rangle\leq p_{1}\) then with probability \(\leq p_{1}\) we have \(accept\in L(\langle s_{t},q_{t}\rangle)\). Let \(T=1/(1-\gamma)\) be the effective horizon, then under mild assumptions (i.e. \(\mathcal{C}(\langle s_{t},q_{t}\rangle)<\infty\)) we can consider the following decomposition of the expected cumulative cost,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right] =\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}X_{t}\right]\] (119) \[=\mathbb{E}_{s_{0}\sim\mathcal{P}_{0}(\cdot)}\left[X_{0}\right]+ \gamma\cdot\mathbb{E}_{s_{1}\sim\mathcal{P}_{1}(\cdot)}\left[X_{1}\right]+\ldots\] (120) \[\quad+\gamma^{T}\cdot\mathbb{E}_{s_{T}\sim\mathcal{P}_{T}(\cdot )}\left[X_{T}\right]+\ldots\] \[=\mathbb{E}_{\pi}\left[X_{0}\right]+\gamma\cdot\mathbb{E}_{\pi} \left[X_{1}\right]+\ldots+\gamma^{T}\cdot\mathbb{E}_{\pi}\left[X_{T}\right]+\ldots\] (121)

We replace the subscript \(`\langle s_{t},q_{t}\rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}^{\prime}\) here for brevity. Clearly by linearity of expectations this statement holds. Although it is worth noting that each expectation is taken under a different marginal state distribution (i.e. \(\mathcal{P}_{t}(\cdot)\)), which depends on \(\pi\) (apart from the initial state distribution \(\mathcal{P}_{0}(\cdot)\)). From now on we will write this is implicitly (i.e. Eq. 121), rather than writing the marginal state distribution (at time \(t\)) for each expectation. Using our earlier observations we can now bound the expected cumulative cost from above as follows,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right] =\mathbb{E}_{\pi}\left[X_{0}\right]+\gamma\cdot\mathbb{E}_{\pi} \left[X_{1}\right]+\ldots+\gamma^{T}\cdot\mathbb{E}_{\pi}\left[X_{T}\right]+\ldots\] (122) \[\leq p_{1}+\gamma\cdot p_{1}+\ldots\qquad+\gamma^{T-1}\cdot p_{1} +\gamma^{T}\cdot p_{1}+\ldots\] (123) \[=p_{1}\cdot\sum_{t=0}^{\infty}\gamma^{t}=p_{1}\cdot(1/(1-\gamma)) =T\cdot p_{1}\] (124)

**Proposition G.10**.: _The converse is not strictly true, since there may be a feasible policy \(\pi\) for Problem G.8 with threshold \(d_{1}\leq T\cdot p_{1}\) which does not satisfy the constraints of Problem G.7_

We want to prove the following statement, a policy \(\pi\) satisfying,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s_{t},q _{t}\rangle)\right]\leq T\cdot p_{1}\] (125)

does not imply that,

\[\Pr\left(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}accept\right)\leq p _{1}\quad\forall t=0,1,2,\ldots\] (126)

Proof.: To prove this we will show that there may be some policy \(\pi\) that satisfies Eq. 125, but does not satisfy Eq. 126 at some timestep \(t\). For simplicity we consider the first timestep (i.e. \(t=0\)). First we assume \(\pi\) is such that Eq. 125 holds, then clearly we have,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{H}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t} \rangle)\right]\leq\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t} \mathcal{C}(\langle s_{t},q_{t}\rangle)\right]\leq T\cdot p_{1}\] (127)

Let \(\Pr(\langle s_{0},q_{0}\rangle\models\Diamond^{\leq H}accept\)) denote the proportion of accepting paths from the initial state \(s_{0}\sim\mathcal{P}_{0}(\cdot)\). Suppose \(\pi\) is such that \(\Pr(\langle s_{0},q_{0}\rangle\models\Diamond^{\leq H}accept)>p_{1}\). We note that for each path and corresponding \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\Diamond^{\leq H}\textit{accept}\) the sum \(\sum_{t=0}^{H}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq\gamma^{H}\), and so,

\[T\cdot p_{1}\geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C} (\langle s_{t},q_{t}\rangle)\right]\geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{H} \gamma^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\right]>p_{1}\cdot\gamma^{H}\] (128)

Now clearly for all \(p_{1}\in[0,1]\), \(\gamma\in[0,1)\), \(H\in\mathbb{Z}_{+}\) and \(T=1/(1-\gamma)\) the following holds,

\[p_{1}\cdot\gamma^{H}<T\cdot p_{1}\] (129)

This implies that there may exist some \(\pi\) satisfying Eq. 125 and such that \(\Pr(\langle s_{0},q_{0}\rangle\models\Diamond^{\leq H}\textit{accept})>p_{1}\), i.e. does not satisfy Eq. 126 at timestep \(t=0\). 

**Proposition G.11**.: _A feasible policy \(\pi\) for Problem 4.4 with threshold \(d_{1}\leq p_{1}\cdot\gamma^{T+H}\) satisfies \(\Pr(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}\textit{accept})\leq p_ {1}\) up to the effective horizon \(T=1/(1-\gamma)\). This bound is tight._

Proof.: Let \(T=1/(1-\gamma)\) be the effective horizon. A feasible policy \(\pi\) for Problem 4.4 with threshold \(d_{1}\leq p_{1}\cdot\gamma^{T+H}\) clearly satisfies,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s_{t}, q_{t}\rangle)\right]\leq p_{1}\cdot\gamma^{T+H}\] (130)

which implies that for all \(t^{\prime}\in[0,T]\) we have,

\[p_{1}\cdot\gamma^{T+H}\geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{ \infty}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\right] \geq\mathbb{E}_{\pi}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma ^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\right]\] (131) \[=\mathbb{E}_{\pi}\left[\gamma^{t^{\prime}}\sum_{t=t^{\prime}}^{t ^{\prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)\right]\] (132) \[=\gamma^{t^{\prime}}\cdot\mathbb{E}_{\pi}\left[\sum_{t=t^{\prime }}^{t^{\prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)\right]\] (133)

Let \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H} \textit{accept})\) denote the proportion of accepting paths at timestep \(t^{\prime}\), where \(s_{t^{\prime}}\sim\mathcal{P}_{t^{\prime}}(\cdot)\). Here \(\mathcal{P}_{t^{\prime}}(\cdot)\) denotes the marginal state distribution at time \(t^{\prime}\). Recall that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\Diamond^{\leq H}\textit{accept}\) the sum \(\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{ t},q_{t}\rangle)\geq\gamma^{H}\). Without loss of generality fix some \(t^{\prime}\in[0,T]\) and suppose that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H} \textit{accept})>p_{1}\). This implies that,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right] \geq\gamma^{t^{\prime}}\cdot\mathbb{E}_{\pi}\left[\sum_{t=t^{ \prime}}^{t^{\prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{t},q_{t} \rangle)\right]\] (134) \[>p_{1}\cdot\gamma^{H}\cdot\gamma^{t^{\prime}}\geq p_{1}\cdot \gamma^{T+H}\] (135)

Which is a contradiction. Therefore, it must be the case that when Eq. 130 is satisfied then so is \(\Pr(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}\textit{accept}])\leq p_ {1}\) for all \(t\in[0,T]\). To prove that this bound is tight we can again show the possible existence of a counter example. In particular, we want to prove the following statement, a policy \(\pi\) satisfying,

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s_{t},q_ {t}\rangle)\right]\leq p_{1}\cdot\gamma^{T+H}+c\] (136)

for some constant \(c>0\), does not imply that,

\[\Pr\left(\langle s_{t},q_{t}\rangle\models\Diamond^{\leq H}\textit{accept} \right)\leq p_{1}\quad\forall t\in[0,T]\] (137)

We will show that there may exist some policy \(\pi\) that satisfies Eq. 136 but does not satisfy Eq. 137 at some timestep \(t\). For simplicity we consider timestep \(t=T\), although we note that with a little extra work we could come up with a proof for any \(t\in[0,T]\). Firstly, we assume \(\pi\) is such that Eq. 136 holds, then we have,

\[p_{1}\cdot\gamma^{T+H}+c\geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t} \mathcal{C}(\langle s_{t},q_{t}\rangle)\right]\geq\mathbb{E}_{\pi}\left[\sum_{ t=T}^{T+H}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\right]\] (138)

Let \(\Pr(\langle s_{T},q_{T}\rangle\models\lozenge^{H}accept)\) denote the proportion of accepting paths at timestep \(T\). Suppose \(\pi\) is such that \(\Pr(\langle s_{T},q_{T}\rangle\models\lozenge^{H}accept)>p_{1}\). We note that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding \(\textit{trace}(\rho)\in\Sigma^{\omega}\) such that \(\textit{trace}(\rho)\models\lozenge^{H}accept\) the sum \(\sum_{t=T}^{T+H}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq\gamma^{ T+H}\), and so,

\[p_{1}\cdot\gamma^{T+H}+c \geq\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{ C}(\langle s_{t},q_{t}\rangle)\right]\] (139) \[\geq\mathbb{E}_{\pi}\left[\sum_{t=T}^{T+H}\gamma^{t}\mathcal{C}( \langle s_{t},q_{t}\rangle)\right]\] (140) \[>p_{1}\cdot\gamma^{T+H}\] (141)

Now clearly for all \(p_{1}\in[0,1]\), \(\gamma\in[0,1)\), \(c>0\), \(H\in\mathbb{Z}_{+}\) and \(T=1/(1-\gamma)\), the following holds,

\[p_{1}\cdot\gamma^{T+H}<p_{1}\cdot\gamma^{T+H}+c\] (142)

This implies that there may exist some \(\pi\) satisfying Eq. 136 and such that \(\Pr(\langle s_{T},q_{T}\rangle\models\lozenge^{H}accept)>p_{1}\), i.e. does not satisfy Eq. 137 at timestep \(t=T\). 

### Probabilistic Cumulative Constraint

**Problem G.12** (Probabilistic cumulative constraint).: \[\max_{\pi}V_{\pi}\quad\textit{subject to}\quad\mathbb{P}_{\langle s_{t},q_{t} \rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}}\Big{[}\sum_{t=0}^{\infty} \gamma^{t}\mathcal{C}(\langle s_{t},q_{t}\rangle)\leq d_{2}\Big{]}\geq 1- \delta_{2}\]

_where \(d_{2}\in\mathbb{R}_{+}\) is the cost threshold, \(\delta_{2}\) is a tolerance parameter and \(\gamma\in[0,1)\) is the discount factor._

**Proposition G.13**.: _A feasible policy \(\pi\) for Problem G.7 with parameters \(p_{1}\in[0,1]\), is also a feasible policy for Problem G.12 with parameters \(d_{2}\in\mathbb{R}_{+}\) and \(\delta_{2}\in(0,1]\), provided that, \(d_{2}\geq\sqrt{(\lceil\log(T)\rceil\cdot T)/2\cdot\log(1/\delta_{2})}+\lceil \log(T)\rceil\cdot T\cdot p_{1}+1\), where \(T=1/(1-\gamma)\) is the effective horizon._

Proof.: Again \(t=0,1,2,\ldots\) we define the following random variables, \(X_{0},X_{1},X_{2},\ldots\), where,

\[X_{t}=\mathcal{C}(\langle s_{t},q_{t}\rangle)=1\left[accept\in L^{\prime}( \langle s_{t},q_{t}\rangle)\right]\] (143)

and we make the following observation,

\[\mathbb{E}\left[X_{t}\right] =\mathbb{E}\left[1\left[accept\in L^{\prime}(\langle s_{t},q_{t} \rangle)\right]\right]\] (144) \[=\Pr\left(accept\in L^{\prime}(\langle s_{t},q_{t}\rangle)\right)\] (145) \[\leq p_{1}\] (146)

See the proof of Prop. G.9, the argument is identical. Under mild assumptions (i.e. \(\mathcal{C}(\langle s_{t},q_{t}\rangle)<\infty\)) we consider the following decomposition of the (undiscounted) expected cumulative cost up to timestep \(\lceil\log(T)\rceil\cdot T-1\),

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1} \mathcal{C}(\langle s_{t},q_{t}\rangle)\right] =\mathbb{E}_{\pi}\left[X_{0}\right]+\mathbb{E}_{\pi}\left[X_{1} \right]+\ldots+\mathbb{E}_{\pi}\left[X_{\lceil\log(T)\rceil\cdot T-1}\right]\] (147) \[\leq\lceil\log(T)\rceil\cdot T\cdot p_{1}\] (148)

Again we replace the subscript '\(\langle s_{t},q_{t}\rangle\sim\mathcal{M}_{\pi}\otimes\mathcal{D}\)' here for brevity, see the proof of Prop. G.9 for more details. Before we proceed we must first deal with the dependence between the random variables \(X_{0},\ldots X_{\lceil\log(T)\rceil\cdot T-1}\). Strictly speaking it is not the case that \(\Pr(X_{t}=1\mid X_{t-1},\ldots,X_{0})=\mathbb{E}_{\pi}\left[X_{0}\right]\). We will show that \(\Pr(X_{t}=1\mid X_{t-1},\ldots,X_{0})=\mathbb{E}_{\pi}\left[X_{0}\right]\).

**Proposition G.14**.: _A feasible policy \(\pi\) for Problem G.7 with parameters \(p_{1}\in[0,1]\), is also a feasible policy for Problem G.12 with parameters \(d_{2}\in\mathbb{R}_{+}\) and \(\delta_{2}\in(0,1]\), provided that, \(d_{2}\geq\sqrt{(\lceil\log(T)\rceil\cdot T)/2\cdot\log(1/\delta_{2})}+\lceil \log(T)\rceil\cdot T\cdot p_{1}+1\), where \(T=1/(1-\gamma)\) is the effective horizon._

Proof.: Again \(t=0,1,2,\ldots\) we define the following random variables, \(X_{0},X_{1},X_{2},\ldots\), where,

\[X_{t}=\mathcal{C}(\langle s_{t},q_{t\(\Pr(X_{t}=1)\). However, we have already established that \(\Pr(X_{t}=1)\leq p_{1}\), as such we can simulate \(X_{0},\ldots,X_{\lceil\log(T)\rceil\cdot T-1}\) as a sequence of independent coin flips \(Y_{0},\ldots,Y_{\lceil\log(T)\rceil\cdot T-1}\) with probability \(p_{1}\), it is then the case that \(\mathbb{P}[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}X_{t}>d_{2}]\leq\mathbb{P} [\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}Y_{t}>d_{2}]\). Now we can bound the probability that we care about,

\[1-\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s _{t},q_{t}\rangle)\leq d_{2}\right] =\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s _{t},q_{t}\rangle)>d_{2}\right]\] (149) \[=\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}X_{t}>d_{2}\right]\] (150) \[=\mathbb{P}\left[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}\gamma ^{t}X_{t}+\sum_{t=\lceil\log(T)\rceil\cdot T}^{\infty}\gamma^{t}X_{t}>d_{2}\right]\] (151) \[\leq\mathbb{P}\left[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}X_{t }+1>d_{2}\right]\] (152) \[\leq\mathbb{P}\left[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}Y_{t }+1>d_{2}\right]\] (153) \[=\mathbb{P}\left[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}Y_{t}> \lceil\log(T)\rceil\cdot T\cdot p_{1}+d_{2}-\lceil\log(T)\rceil\cdot T\cdot p _{1}-1\right]\] (154) \[=\mathbb{P}\left[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}Y_{t} >\mathbb{E}\left[\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}Y_{t}\right]+d_{2} -\lceil\log(T)\rceil\cdot T\cdot p_{1}-1\right]\] (155) \[\leq\exp\left(-\frac{2\cdot(d_{2}-\lceil\log(T)\rceil\cdot T \cdot p_{1}-1)^{2}}{\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}(\max\{Y_{i}\}- \min\{Y_{i}\})^{2}}\right)\] (156) \[=\exp\left(-\frac{2\cdot(d_{2}-\lceil\log(T)\rceil\cdot T\cdot p _{1}-1)^{2}}{\lceil\log(T)\rceil\cdot T}\right)\] (157)

Here the first inequality (Eq. 152) comes from the following two facts, certainly \(\sum_{t=0}^{\lceil\log(T)\rceil\cdot T-1}\gamma^{t}X_{t}\leq\sum_{t=0}^{\lceil \log(T)\rceil\cdot T-1}X_{t}\) and we have that \(\sum_{t=\lceil\log(T)\rceil\cdot T}^{\infty}\gamma^{t}X_{t}\leq 1\). The second fact is a little harder to see, first we note that \(\lim_{\gamma\to 1}\gamma^{T}=1/e\), where \(T=1/(1-\gamma)\) is the effective horizon. Then we can rewrite,

\[\sum_{t=\lceil\log(T)\rceil\cdot T}^{\infty}\gamma^{t}X_{t} =\left(\gamma^{\lceil\log(T)\rceil\cdot T}\right)\cdot\left(\sum_ {t=\lceil\log(T)\rceil\cdot T}^{\infty}\gamma^{t-\lceil\log(T)\rceil\cdot T} X_{t}\right)\] (158) \[=\left((\gamma^{T})^{\lceil\log(T)\rceil}\right)\cdot\left(\sum_ {t=\lceil\log(T)\rceil\cdot T}^{\infty}\gamma^{t-\lceil\log(T)\rceil\cdot T} X_{t}\right)\] (159) \[\leq\left(\frac{1}{e}^{\lceil\log(T)\rceil}\right)\cdot\left(\frac {1}{1-\gamma}\right)\leq\left(\frac{1}{e}^{\log(T)}\right)\cdot T=\frac{1}{T} \cdot T=1\] (160)

The second inequality (Eq. 153) comes from our earlier construction. The final inequality (Eq. 156) is obtained from Hoeffding's inequality [40] for bounded random variables. Finally, by bounding the final expression (Eq. 157) from above by \(\delta_{2}\) and rearranging gives the desired result. 

**Proposition G.14**.: _A feasible policy \(\pi\) for Problem G.12 with parameters \(\delta_{2}\leq p_{1}\) and \(d_{2}<\gamma^{T+H}\), satisfies \(\Pr(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept)\leq p_{1}\) up to the effective horizon \(T=1/(1-\gamma)\). This bound is tight._Proof.: A feasible policy \(\pi\) for Problem G.12 with parameters \(\delta_{2}\leq p_{1}\) and \(d_{2}<\gamma^{T+H}\) clearly implies that,

\[\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t} \rangle)<\gamma^{T+H}\right]\geq 1-p_{1}\] (161)

and certainly for all \(t^{\prime}\in[0,T]\) we have that,

\[1-p_{1} \leq\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}( \langle s_{t},q_{t}\rangle)<\gamma^{T+H}\right]\] (162) \[\leq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t} \mathcal{C}(\langle s_{t},q_{t}\rangle)<\gamma^{T+H}\right]\] (163) \[=\mathbb{P}\left[\gamma^{t^{\prime}}\sum_{t=t^{\prime}}^{t^{ \prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)<\gamma^ {T+H}\right]\] (164) \[=\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t-t^{ \prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)<(\gamma^{T+H}/\gamma^{t^{ \prime}})\right]\] (165) \[\leq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t-t^ {\prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)<\gamma^{H}\right]\] (166)

Let \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\lozenge^{\leq H}accept)\) denote the proportion of accepting paths at timestep \(t^{\prime}\), where \(s_{t^{\prime}}\sim\mathcal{P}_{t^{\prime}}(\cdot)\). Here \(\mathcal{P}_{t^{\prime}}(\cdot)\) denotes the marginal state distribution at time \(t^{\prime}\). Recall that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding \(\text{\emph{trace}}(\rho)\in\Sigma^{\omega}\) such that \(\text{\emph{trace}}(\rho)\models\lozenge^{\leq H}accept\) the sum \(\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{ t},q_{t}\rangle)\geq\gamma^{H}\). Without loss of generality fix some \(t^{\prime}\in[0,T]\) and suppose that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\lozenge^{\leq H}accept )>p_{1}\). This implies that,

\[\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t} \rangle)\geq\gamma^{T+H}\right]\geq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{ \prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq \gamma^{H}\right]>p_{1}\] (167)

Which is a contradiction. Therefore, it must be the case that when Eq. 161 is satisfied then so is \(\Pr(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept])\leq p_{1}\) for all \(t\in[0,T]\). To prove that this bound is tight we can show the possible existence of a counter example. In particular, we want to prove the following statement, a policy \(\pi\) satisfying,

\[\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}(\langle s_{t},q_{t} \rangle)<\gamma^{T+H}\right]\geq 1-(p_{1}+c)\] (168)

for some constant \(c>0\) does not imply that,

\[\Pr\left(\langle s_{t},q_{t}\rangle\models\lozenge^{\leq H}accept\right)\leq p _{1}\quad\forall t\in[0,T]\] (169)

We will show that there may exist some policy \(\pi\) that satisfies Eq. 168 but does not satisfy Eq. 169 at some timestep \(t\). Firstly, we assume \(\pi\) is such that Eq. 168 holds, this implies that for all \(t^{\prime}\in[0,T]\) we have,

\[1-(p_{1}+c) \leq\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}( \langle s_{t},q_{t}\rangle)<\gamma^{T+H}\right]\] (170) \[\leq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t} \mathcal{C}(\langle s_{t},q_{t}\rangle)<\gamma^{T+H}\right]\] (171) \[\leq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t-t^ {\prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)<\gamma^{H}\right]\] (172)Fix some \(t^{\prime}\in[0,T]\) and let \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept)\) denote the proportion of accepting paths at timestep \(t^{\prime}\). Suppose that \(\pi\) is such that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept)>p_ {1}\). Again recall that for each path \(\rho\in\mathcal{S}^{\omega}\) and corresponding _trace\((\rho)\in\Sigma^{\omega}\)_ such that _trace\((\rho)\models\Diamond^{\leq H}accept\)_ the sum \(\sum_{t=t^{\prime}}^{t^{\prime}+H}\gamma^{t-t^{\prime}}\mathcal{C}(\langle s_{ t},q_{t}\rangle)\geq\gamma^{H}\), and so,

\[p_{1}+c \geq\mathbb{P}\left[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{C}( \langle s_{t},q_{t}\rangle)\geq\gamma^{T+H}\right]\] (173) \[\geq\mathbb{P}\left[\sum_{t=t^{\prime}}^{t^{\prime}+b}\gamma^{t -t^{\prime}}\mathcal{C}(\langle s_{t},q_{t}\rangle)\geq\gamma^{H}\right]>p_ {1}\] (174)

Now clearly for all \(p_{1}\in[0,1]\), and \(c>0\), the following holds,

\[p_{1}<p_{1}+c\] (175)

This implies that there may exist some \(\pi\) satisfying Eq. 168 such that \(\Pr(\langle s_{t^{\prime}},q_{t^{\prime}}\rangle\models\Diamond^{\leq H}accept )>p_{1}\), i.e. does not satisfy Eq. 169 at timestep \(t=t^{\prime}\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The proof of safety guarantees (Theorem 6.5) is provided in Appendix C.5. Furthermore, we provide experimental results demonstrating the scalability of our approach in Section 7. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of our framework are discussed under in Section 7 in the paragraph **Separating Reward and Safety**, furthermore the limitations of our proposed method are also discussed and explored in Appendix F. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: For the key proofs in the main paper we explicitly provide the assumptions used, the proofs of each theoretical result from the main paper can also be found in Appendix C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: other than access to code, we provide pseudo-code for the algorithms used in our experiments (see Appendix A) Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide access to the code for our first set of experiments in the supplementary material, with a corresponding script to reproduce the results in the main paper. For the second set of experiments we provide directions to the code base that we adapted and throughout the paper and appendices we provide sufficient details to reproduce these results without too much difficulty. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide a thorough description of the environmental settings in Appendix D.1 and D.2, furthermore, hyperparameters and details with regards to access to the code are provided in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes], Justification: We provide error bars for all of our experiments, over 5 random initializations (seeds), provided by seaborn.lineplot, see Appendix E for details.

[MISSING_PAGE_EMPTY:48]

Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]. Justification: As our paper is mostly foundational we do not foresee any immediate positive or negative societal impact of this research. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). * **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: We do not believe that the new assets provided in the paper pose any such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: We are the original creators/owners of the code used for the first set of experiments, for the second set of experiments we explicitly cite the paper and provide the URL for the code that we have adapted in this paper, which is available under the MIT License as stated in Appendix E.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA].

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.