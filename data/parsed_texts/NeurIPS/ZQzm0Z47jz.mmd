# Rethinking the Role of Token Retrieval in Multi-Vector Retrieval

Jinhyuk Lee

Correspondence: jinhyuklee@google.com

Zhuyun Dai

Sai Meher Karthik Duddu

Tao Lei

Iftekhar Naim

Ming-Wei Chang

Vincent Y. Zhao

Google DeepMind

###### Abstract

Multi-vector retrieval models such as ColBERT [16] allow token-level interactions between queries and documents, and hence achieve state of the art on many information retrieval benchmarks. However, their non-linear scoring function cannot be scaled to millions of documents, necessitating a three-stage process for inference: retrieving initial candidates via token retrieval, accessing all token vectors, and scoring the initial candidate documents. The non-linear scoring function is applied over all token vectors of each candidate document, making the inference process complicated and slow. In this paper, we aim to simplify the multi-vector retrieval by rethinking the role of token retrieval. We present XTR, Conte**X**tualized **T**oken **R**etriever, which introduces a simple, yet novel, objective function that encourages the model to retrieve the most important document tokens first. The improvement to token retrieval allows XTR to rank candidates only using the retrieved tokens rather than all tokens in the document, and enables a newly designed scoring stage that is two-to-three orders of magnitude cheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the state-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis confirms our decision to revisit the token retrieval stage, as XTR demonstrates much better recall of the token retrieval stage compared to ColBERT.

## 1 Introduction

The performance of a dense retrieval model is largely affected by how it defines expressive representations over queries and documents, and whether it can efficiently retrieve and score a document using these vector representations. For example, dual encoder models [23, 17, 16] encode queries and documents into single vectors and compute query-document similarities using dot products. While these models are very efficient for retrieval, their expressivity is limited due to the absence of token-level modeling for scoring. In contrast, multi-vector models such as ColBERT [16, 15] are directly designed to capture token-level interactions. By utilizing a (non-linear) scoring function over all query and document token representations, multi-vector models enjoy much better model expressivity and often achieve superior results across various benchmarks [15].

The enhanced model expressivity, however, comes at a great cost of inference complexity. Unlike the case in dual encoders, the non-linear scoring function in multi-vector retrieval models prohibits the use of efficient Maximum Inner Product Search (MIPS) [14, 15, 16] for finding the maximum scoring documents. As a result, models such as ColBERT adopt an intricate and resource-intensive inference pipeline, which typically consistsof three stages: 1) _token retrieval:_ using each query token to retrieve document tokens, with their source documents becoming candidates; 2) _gathering:_ collecting all the token embeddings from each candidate document, including those that are not retrieved in the first stage (most document tokens are not retrieved); and 3) _scoring:_ ranking candidates using a non-linear function based on all the token embeddings per document.

This procedure leads to two major issues. First, compared to the token retrieval stage, gathering all document token embeddings and re-scoring the documents can introduce orders of magnitude additional data loading and floating operation cost, making multi-vector models extremely expensive to deploy. Secondly, while the candidate documents are decided in the token retrieval stage, previous training objectives are designed for the scoring stage. This creates a significant training-inference gap causing multi-vector models achieve sub-optimal (and often poor) recall performance. Clearly, the three-stage pipeline has largely limited the potential of multi-vector models, raising an interesting research question - _can the token retrieval stage alone be sufficient for great performance?_

We present XTR, Cont**X**xtualized **T**oken **R**etriever: a simplified and efficient method for multi-vector retrieval, through re-thinking the role of token retrieval. The key insight of XTR is that the token retrieval in multi-vector models should be trained to retrieve the most salient and informative document tokens, so that the score between a query and document can be computed using only the retrieved information, just like how single-vector retrieval models work. By doing so, the gathering step can be completely eliminated, and the cost of scoring is significantly reduced as only a fraction of the tokens need to be considered and the dot products from the token retrieval can be reused. To improve the quality of the token retrieval, XTR proposes a novel, yet simple, training objective, which dramatically improves retrieval accuracy, doubling the chances of a gold token being retrieved in the top-\(k\) results. Furthermore, despite the improved token retrieval, some relevant tokens may still be missed (i.e., not retrieved). To address this issue, we propose a simple method, called missing similarity imputation, which accounts for the contribution of the missing tokens to the overall score.

XTR streamlines the inference process, bringing it closer to the straightforward procedure of dual encoders, while maintaining and enhancing the expressive scoring function of multi-vector retrieval models. On the BEIR (Thakur et al., 2021) and LoTTE (Santhanam et al., 2022) benchmarks, XTR attains state-of-the-art performance, requiring neither distillation nor hard negative mining. Notably, our model surpasses state-of-the-art dual-encoder GTR (Ni et al., 2021) by 3.6 nDCG@10 on BEIR without any additional training data. On the EntityQuestions benchmark (Sciavolino et al., 2021), XTR outperforms the previous state-of-the-art by 4.1 points on top-20 retrieval accuracy. XTR also does not require any secondary pre-training for retrieval and greatly outperforms mContriever (Izacard et al., 2022) on MIRACL, which contains multilingual retrieval tasks in 18 languages (Zhang et al., 2022). Our analysis supports that XTR indeed benefits from retrieving more contextualized tokens in relevant contexts, while making the scoring stage two-to-three orders of magnitude cheaper.

## 2 Background

### Multi-vector Retrieval

Single-vector retrieval models, also known as dual encoders, encode an input text sequence as a single dense embedding and define the similarity of a query and a document based on the dot product (Lee et al., 2019; Karpukhin et al., 2020). Multi-vector retrieval models, on the other hand, make use of multiple dense embeddings for each query and document, typically leveraging all contextualized word representations of the input to gain improved model expressivity.

Consider a query \(Q=\{\mathbf{q}_{i}\}_{i=1}^{n}\) and a document \(D=\{\mathbf{q}_{j}\}_{j=1}^{m}\) where \(\mathbf{q}_{i}\) and \(\mathbf{d}_{j}\) denote the \(d\)-dimensional query token vector and the document token vector, respectively. Multi-vector retrieval models compute the query-document similarity as follows: \(f(Q,D)=\sum_{i=1}^{n}\sum_{j=1}^{m}\mathbf{A}_{ij}\mathbf{P}_{ij}\) where \(\mathbf{P}_{ij}=\mathbf{q}_{i}^{\top}\mathbf{d}_{j}\) and \(\mathbf{A}\in\{0,1\}^{n\times m}\) denotes the alignment matrix with \(\mathbf{A}_{ij}\) being the token-level alignment between the query token vector \(\mathbf{q}_{i}\) and the document token vector \(\mathbf{d}_{j}\). The sum-of-max operator of ColBERT (Khattab and Zaharia, 2020) sets \(\mathbf{A}_{ij}=\mathds{1}_{[j\text{argmax}_{j}(\mathbf{P}_{ij})]}\) where the argmax operator is over \(1\leq j^{\prime}\leq m\) (i.e., tokens from a single document \(D\)) and \(\mathds{1}_{[\bullet]}\) is an indicator function. Then, \(f_{\text{ColBERT}}(Q,D)\) is defined as follows:

\[f_{\text{ColBERT}}(Q,D)=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{m}\mathbf{A}_{ij} \mathbf{P}_{ij}=\frac{1}{n}\sum_{i=1}^{n}\max_{1\leq j\leq m}\mathbf{q}_{i}^{ \top}\mathbf{d}_{j}.\] (1)Here, we include the normalizer \(n\), which was not included in the original sum-of-max, as it stabilizes training while not affecting the ranking during inference. After computing the query-document similarity, multi-vector retrieval models are typically trained with a cross-entropy loss over in-batch negatives [14, 15]. Specifically, given a positive document \(D^{+}\) for \(Q\) and a set of mini-batch documents \(D_{1:B}\) = \([D_{1},\dots,D_{B}]\) where \(D^{+}\)\(\in\)\(D_{1:B}\), they minimize the cross-entropy loss defined as: \(\mathcal{L}_{\text{CE}}\) = \(-\log\frac{\exp f(Q,D^{+})}{\sum_{b=1}^{B}\exp f(Q,D_{b})}\).

### Three-stage inference of Multi-vector Retrieval

Unlike dual encoder models, finding the maximum scoring document--the document that maximizes eq. (1)--cannot be directly handled by MIPS as the scoring function uses a non-linear, sum-of-max operation. Instead, a multi-vector retrieval model typically takes the following steps for the inference.

1) **Token Retrieval**: for each of the \(n\) query token vectors, it first retrieves \(k^{t}\) document token vectors, which is simply used to form _initial candidate document set_ by taking the union of source documents of retrieved tokens. The total number of candidate documents is up to \(nk^{t}\) if each token is coming from a unique document.2 **Gathering**: since the scoring function eq. (1) requires the computation over all document tokens, multi-vector models need to load all of the token vectors of the candidate documents. To optimize the loading process, a RAM-based index is often employed. 3) **Scoring**: to provide final ranks of candidate documents, multi-vector models score all the candidate documents with eq. (1). This stage is also called _refinement_. Note that the training of typical multi-vector models only takes care of the scoring stage with mini-batch documents. Finally, top-\(k\) documents are returned based on the computed scores. The three-stage inference is illustrated in the top of Figure 1.

Footnote 2: In fact, each candidate document of a T5-based ColBERT is retrieved by 1.48 tokens per on average, meaning that the most of the candidate documents are unique.

## 3 XTR: Contextualized Token Retriever

Unlike existing multi-vector models that follow the retrieve-gather-score stages, XTR directly scores documents utilizing the tokens retrieved from the token retrieval stage. In this section, we start by showing why the existing cross entropy loss with the sum-of-max scoring function would fail on the first-stage token retrieval. Then, we introduce simple but important modifications for XTR.

Figure 1: Overview of XTR. ColBERT has the three-stage inference combining (a) the token retrieval, (b) the gathering and (c) the scoring stages (§2.2). XTR leverages the token retrieval for both training and inference. XTR efficiently obtains the score of each candidate document by applying \(f_{\text{XTR}}\) (or \(f_{\text{XTR}}\)) on the retrieved tokens, completely removing the gathering stage (§3.2).

[MISSING_PAGE_FAIL:4]

Here, top-\(k^{l}(\mathbf{q_{*}})\) is a union of top-\(k^{l}\) document tokens (from the entire corpus) based on the inner product scores with each query vector (i.e., \(\mathbf{q}^{\top}\mathbf{d}\)). Given the \(n\) query token vectors, there are \(C\) (\(\preceq nk^{l}\)) candidate documents. Previous methods load the entire token vectors of each document and compute eq.1 for every query and candidate document pair, which takes \(\mathcal{O}(n^{2}k^{l}\bar{m}d)\) computation per query (\(\bar{m}\) = average document length). Instead, we propose to score the documents _solely using the retrieved token similarity_. This significantly reduces the computational cost for the scoring stage since re-using the token retrieval scores removes computing redundant inner products and unnecessary (non-max) inner products. Furthermore, the expensive gathering stage (which requires loading all the document token vectors for computing eq.1) can be removed completely. Unlike previous work (Macdonald and Tonellotto, 2021) that leverages token retrieval to sort first-stage candidate documents before the scoring stage, we aim to directly provide the final scores of documents.

Missing similarity imputationDuring inference, we retrieve \(k^{l}\) document tokens for each of \(n\) query tokens. Assume that each document token belongs to a unique document, providing \(C\) = \(nk^{l}\) candidate documents in total. This leaves us with a single token similarity to score each document in the absence of the gathering stage. However, during training--either with eq.1 or eq.2--each positive document has up to \(n\) (max) token similarities to average, which mostly converges to \(n\) as training proceeds. Hence, during inference, we impute the _missing similarity_ for each query token treating each of candidate documents as if it were positive with \(n\) token similarities.

For every candidate document \(\hat{D}\), we first define the following scoring function for the inference:

\[f_{\text{XTR}^{l}}(Q,\hat{D})=\frac{1}{n}\sum_{i=1}^{n}\max_{1\leq j\leq m} \big{[}\hat{\mathbf{A}}_{ij}\mathbf{q}_{i}^{\top}\mathbf{d}_{j}+(1-\hat{ \mathbf{A}}_{ij})m_{i}\big{]}.\] (4)

This is similar to eq.2, but introduces \(m_{i}\in\mathbb{R}\), which estimates the missing similarity for each \(q_{i}\). \(\hat{\mathbf{A}}\) is defined similar to the one described in eq.2 except that it uses \(k^{l}\) for the top-\(k\) operator. Each \(q_{i}\) would take the missing similarity \(m_{i}\) as the maximum value if \(\hat{\mathbf{A}}_{i*}\) = 0 and \(m_{i}\geq 0\). Importantly, \(f_{\text{XTR}^{l}}\) removes the need of recomputing any \(\mathbf{q}_{i}^{\top}\mathbf{d}_{j}\) since when \(\hat{\mathbf{A}}_{ij}=1\) we already know the retrieval score from the token retrieval stage, and when \(\hat{\mathbf{A}}_{ij}=0\) we simply don't need to compute it as \(\hat{\mathbf{A}}_{ij}\mathbf{q}_{i}^{\top}\mathbf{d}_{j}=0\). Note that when every \(\hat{\mathbf{A}}_{ij}=1\), the equation becomes the sum-of-max operator. On the other hand, when no document tokens of \(\hat{D}\) were retrieved for \(q_{i}\) (i.e., \(\hat{\mathbf{A}}_{i*}=0\)), we fall back to the imputed score \(m_{i}\), which provides an approximated sum-of-max result.

\begin{table}
\begin{tabular}{l c c l} \hline \hline  & **Scoring** & **Estimated FLOPs/query** & **Setting** \\ \hline \(f_{\text{ColBERT}}\) & \(n^{2}k^{l}(2\bar{m}d+\bar{m}+1)\) & \(0.36\times 10^{9}\) & \(M\) = 3 \(\times 10^{9}\), \(n\) = 16, \(d\) = 128, \\ \(f_{\text{XTR}^{l}}\) & \(n^{2}k^{l}(\bar{r}+1)\) & \(0.09\times 10^{6}\) & \(k^{l}\) = 100, \(\bar{m}\) = 55, \(\bar{r}\) = 2.5 \\ \hline \hline \end{tabular}
\end{table}
Table 1: FLOPs comparison of ColBERT and XTR for the scoring stage. XTR only adds minimal complexity for scoring each candidate document. The setting is derived from MS MARCO.

Figure 3: Comparison of \(f_{\text{ColBERT}}\) in eq.1 and \(f_{\text{XTR}^{l}}\) in eq.4. Assume that \(D_{a}\) and \(D_{b}\) were selected as initial candidate documents from the token retrieval stage. \(f_{\text{ColBERT}}\) loads all token vectors of \(D_{a}\) and \(D_{b}\) and exhaustively recomputes pairwise token similarity to obtain the max values (**red boxes**). On the other hand, \(f_{\text{XTR}^{l}}\) does not load any token vectors and reuses retrieval scores from the first-stage token retrieval. Assume that, with the top-2 token retrieval results, the first query token retrieved each max score of \(D_{a}\) and \(D_{b}\), but the second query token retrieved two tokens only from \(D_{a}\) but not \(D_{b}\). We impute the missing similarity \(m\) for \(D_{b}\) (denoted as yellow dashed box) by finding its upper bound using the top-2 score (denoted as \(s_{2}\)) of the second query token (i.e., \(m\leq s_{2}\leq s_{1}\)).

In fact, we can find the upper bound of the missing similarity. For every token retrieval with \(\mathbf{q}_{i}\), the missing similarity of the query token for \(\hat{D}\) will be upper bounded by its last top-\(k^{\prime}\) score. Specifically, for each query token \(q_{i}\), we have the following top-\(k^{\prime}\) token similarity during inference: \(\llbracket\mathbf{q}_{i}^{\top}\mathbf{d}_{\{1\}},\dots,\mathbf{q}_{i}^{\top} \mathbf{d}_{\{k^{\prime}\}}\rrbracket\). Here, each \(\mathbf{d}_{(*)}\) could come from a different document. Since the missing similarity would have a score less than equal to the score of the last retrieved token, we know that \(m_{i}\in\mathbf{q}_{i}^{\top}\mathbf{d}_{\{k^{\prime}\}}\). With a larger \(k^{\prime}\), the upper bound becomes tighter. In our experiments, we show that simply choosing \(m_{i}\) = \(\mathbf{q}_{i}^{\top}\mathbf{d}_{\{k^{\prime}\}}\) works well especially when a model is trained with \(f_{\text{XTR}}\).5 While we also tried more complicated imputation methods based on regression, our method was competitive enough despite its simplicity. The imputation process is illustrated in Figure 3.

Footnote 5: We found that directly training with \(f_{\text{XTR}^{\prime}}\) instead of \(f_{\text{XTR}}\) fails to converge, which we leave as future work.

Table 1 shows the estimated FLOPs of ColBERT and XTR (see Appendix B for more details). Due to the differences in hardware and infrastructure, we mainly compared the theoretical FLOPs. XTR reduces the FLOPs at the scoring stage by 4000\(\times\) making multi-vector retrieval more efficient.

## 4 Experiments

Experimental SettingFollowing Ni et al. (2021), we fine-tune XTR on MS MARCO with a fixed set of hard negatives from RocketQA (Qu et al., 2021). Then, we test XTR on MS MARCO (MS; in-domain) and zero-shot IR datasets. For the zero-shot evaluation, we use 13 datasets from BEIR (Thakur et al., 2021) (see Appendix C for acronyms), 12 datasets from LoTTE (Santhanam et al., 2022), and 4 datasets on open-domain QA passage retrieval (EQ: EntityQuestions (Sciavolino et al., 2021), NQ, TQA: TriviaQA, SQD: SQuAD). We also train multilingual XTR (mXTR) and evaluate it on MIRACL (Zhang et al., 2022), which contains retrieval tasks in 18 languages. The performance gap between T5-ColBERT (Qian et al., 2022) and XTR shows the improvement with our methods on a multi-vector retrieval model. For implementation details and baselines, see Appendix C. For the relationship between hyperparameters (e.g., \(k_{\text{train}}\) and \(k^{\prime}\)), see SS5.3.

\begin{table}
\begin{tabular}{l c|c c c c c c c c c c c c c} \hline \hline  & MS & AR & TO & FE & CF & SF & CV & NF & NQ & HQ & FQ & SD & DB & QU & Avg. \\ \hline \multicolumn{11}{c}{_One Retriever per Domain_} \\ \hline GenQ & 40.8 & 49.3 & 18.2 & 66.9 & 17.5 & 64.4 & 61.9 & 31.9 & 35.8 & 53.4 & 30.8 & 14.3 & 32.8 & 83.0 & 43.1 \\ \multicolumn{11}{c}{\(\text{PTR}_{\text{target}}\)} & - & 58.8 & 25.6 & 76.2 & 23.5 & 63.8 & 70.2 & 33.7 & 45.6 & 61.7 & 43.0 & 18.3 & 34.4 & 87.5 & 49.4 \\ \hline \multicolumn{11}{c}{_One Retriever for All_} \\ \hline BM25 & 22.8 & 31.5 & 36.7 & 75.3 & 21.3 & 66.5 & 63.6 & 32.5 & 32.9 & 60.3 & 23.6 & 15.8 & 31.3 & 78.9 & 44.0 \\ ColBERT & 40.1 & 23.3 & 20.2 & 77.1 & 18.4 & 67.1 & 67.7 & 30.5 & 52.4 & 59.3 & 31.7 & 14.5 & 39.2 & 85.4 & 45.1 \\ \multicolumn{11}{c}{\(\text{STR}_{\text{base}}\)} & 42.0 & 51.1 & 21.5 & 66.0 & 24.1 & 60.0 & 53.9 & 30.8 & 49.5 & 53.5 & 34.9 & 14.9 & 39.2 & 88.1 & 45.2 \\ \multicolumn{11}{c}{\(\text{TS-ColBERT}_{\text{base}}\)} & 45.6 & 28.8 & 31.1 & 72.4 & 18.1 & 70.4 & 68.3 & 34.0 & 52.2 & 61.7 & 33.4 & 14.1 & 41.6 & 82.3 & 46.8 \\ \multicolumn{11}{c}{\(\text{XTR}_{\text{base}}\)} & 45.0 & 40.7 & 31.3 & 73.7 & 20.7 & 71.0 & 73.6 & 34.0 & 53.0 & 64.7 & 34.7 & 14.5 & 40.9 & 86.1 & 49.1 \\ \multicolumn{11}{c}{\(\text{Splade},\bullet\)} & 43.3 & 47.9 & 27.2 & 78.6 & 23.5 & 69.3 & 71.0 & 33.4 & 52.1 & 68.4 & 33.6 & 15.8 & 43.5 & 83.8 & 49.9 \\ \multicolumn{11}{c}{\(\text{ColBERT}_{\text{cl}}\)} & - & 46.3 & 26.3 & 78.5 & 17.6 & 69.3 & 73.8 & 33.8 & 56.2 & 66.7 & 35.6 & 15.4 & 44.6 & 85.2 & 49.9 \\ \hline \multicolumn{11}{c}{\(\text{GR}_{\text{val}}\)} & 44.2 & 54.0 & 23.3 & 74.0 & 26.7 & 66.2 & 50.1 & 34.2 & 56.8 & 59.9 & 46.7 & 16.1 & 40.8 & 89.2 & 49.1 \\ \multicolumn{11}{c}{\(\text{TS-ColBERT}_{\text{cl}}\)} & 47.3 & 33.8 & 31.0 & 74.2 & 19.7 & 73.1 & 75.8 & 35.2 & 60.5 & 65.2 & 43.5 & 17.1 & 45.0 & 86.0 & 50.8 \\ \multicolumn{11}{c}{\(\text{XTR}_{\text{val}}\)} & 46.6 & 44.2 & 30.9 & 77.0 & 24.5 & 74.3 & 78.9 & 35.3 & 60.9 & 66.2 & 43.8 & 17.1 & 44.3 & 88.1 & **52.7** \\ \hline \hline \multicolumn{11}{c}{\(\text{\bf{LoTTE Search}}\)} & \multicolumn{11}{c}{**LoTTE Form**} \\ \cline{2-11}  & Writing & Rec. & Sci. & Tech. & Life. & Pooled & Writing & Rec. & Sci. & Tech. & Life. & Pooled \\ \hline BM25 & 60.3 & 56.5 & 32.7 & 41.8 & 63.8 & 48.3 & 64.0 & 55.4 & 37.1 & 39.4 & 60.6 & 47.2 \\ ColBERT & 74.7 & 68.5 & 53.6 & 61.9 & 80.2 & 67.3 & 71.0 & 65.6 & 41.8 & 48.5 & 73.0 & 58.2 \\ \multicolumn{11}{c}{\(\text{GTR}_{\text{test}}\)} & 74.1 & 65.7 & 49.8 & 58.1 & 82.0 & 65.0 & 69.2 & 62.0 & 33.7 & 47.6 & 72.2 & 54.9 \\ \multicolumn{11}{c}{\(\text{STR}_{\text{base}}\)} & 77.0 & 69.4 & 54.9 & 63.2 & 82.1 & 69.0 & 73.9 & 68.7 & 42.2 & 51.9 & 74.4 & 60.1 \\ \hline \multicolumn{11}{c}{\(\text{Splade},\bullet\)} & 77.1 & 69.0 & 55.4 & 62.4 & 82.3 & 68.9 & 73.0 & 67.1 & 43.7 & 50.8 & 74.0 & 60.1 \\ \multicolumn{11}{c}{\(\text{COBERT}_{\text{cl}}\)} & 80.1 & 72.3 & 56.7 & 66.1 & 84.7 & 71.6 & 76.3 & 70.8 & 46.1 & 53.6 & 76.9 & 63.4 \\ \hline \multicolumn{11}{c}{\(\text{GTR}_{\text{val}}\)} & **83.9** & 78.0 & 60.0 & 69.5 & 87.4 & 76.0 & 79.5 & 73.5 & 43.1 & 62.6 & 81.9 & 66.9 \\ \multicolumn{11}{c}{\(\text{XTR}_{\text{val}}\)} & 83.3 & **79.3** & **60.

[MISSING_PAGE_FAIL:7]

## 5 Analysis

### Towards Better Token Retrieval

Gold token retrievalIf the tokens of gold documents are not retrieved at all, multi-vector retrieval models would fail to retrieve the gold documents. Hence, a better token retrieval would contain these _gold tokens_ more often in their top results. In Figure 4 (top), we show the probability of a token at the rank \(k\) coming from the gold documents of a query. To compute the probability for the rank \(k\), we simply count the number of an event where a token at rank \(k\) belongs to the gold document and divide it by the number of tokens at rank \(k\). While this is measuring the precision of the token retrieval, we observed a similar trend for the recall of gold tokens. Compared to T5-ColBERT, XTR retrieves gold tokens with higher probability, even on MS MARCO. This shows that the training objective of XTR encourages it to retrieve tokens from more relevant context.

Lexical token retrievalIn Figure 4 (bottom), we show the probability of a token at the rank \(k\) being the same as its query token (e.g., 'insulin' retrieving 'insulin's). T5-ColBERT has very high probability of retrieving the same token across different ranks and datasets. However, it is unclear to what extent the token retrieval stage should behave as sparse retrieval, as it might suffer from the vocabulary mismatch problem. XTR effectively lowers the reliance on the lexical matching while preserving a good amount of lexical precision so that it would achieve a high retrieval accuracy on the entity-centric dataset (SS4.2). In fact, Table 6 in Appendix shows that having lower lexical matching doesn't necessarily mean a lower retrieval quality, but often means better contextualization.

### Efficient Scoring

In Table 5, we show how we can employ the efficient scoring function \(f_{\text{XTR}^{\prime}}\) in XTR with minimal performance losses. We apply \(f_{\text{XTR}^{\prime}}\) on both T5-ColBERT and XTR, and show their performances on MS MARCO. With T5-ColBERT, even if we use the top-\(k^{l}\) score for the imputation, the performance

Figure 4: (top) Gold token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank \(k\) coming from the gold document. (bottom) Lexical token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank \(k\) being lexically identical to its query token.

is much worse than the original sum-of-max scoring. With XTR, the performance greatly improves as it has better token retrieval. Figure 5 shows how Recall@100 improves with larger \(k^{t}\)'s as it provides more exact upper bound for the missing similarity imputation. Table D.2 shows that even if we use smaller \(k^{t}\), XTR still maintains high performances on BEIR.

### Relationship between Hyperparameters

\(\bm{k_{\text{train}}}\) vs. \(\bm{k^{t}}\)In Figure 6, we show MRR@10 of XTR trained with different \(k_{\text{train}}\) and evaluated with different \(k^{t}\) on the MS MARCO development set. While all variants of XTR prefer larger \(k^{t}\), ones trained with smaller \(k_{\text{train}}\) show higher performances than others under small \(k^{t}\) settings. XTR with larger \(k_{\text{train}}\) exhibits better performances than ones with smaller \(k_{\text{train}}\) as \(k^{t}\) becomes larger.

Training batch size vs. \(\bm{k_{\text{train}}}\)In Figure 7, we show the relationship between the training batch size and \(k_{\text{train}}\) during training XTR. In this experiment, we use \(k^{t}=40,000\). While it is evident that XTR mostly favors large training batch sizes, the optimal top-\(k_{\text{train}}\) can be different for different datasets. While most datasets including MS MARCO favored a large enough \(k_{\text{train}}\), ArguAna prefers smaller \(k_{\text{train}}\). We hypothesize that this is due to the longer query length in ArguAna, which makes multi-vector models fall short compared to dual-encoders (see GTR vs. T5-ColBERT in Table 2).

### Qualitative Analysis

Table 6 shows a prediction sample from MS MARCO. For T5-ColBERT, all of the top retrieved tokens are exact lexical matches. Surprisingly, none of the retrieved passages are about the query, demonstrating T5-ColBERT's failure to retrieve tokens from the correct context. In contrast, XTR retrieves fewer exact lexically matching tokens, but the contexts of the retrieved tokens are much more related to the query. This example explains the lower lexical token retrieval probability of XTR compared to T5-ColBERT in Figure 4 (bottom), but higher gold token retrieval performance in Figure 4 (top). For more qualitative examples, please see Appendix E.

## 6 Related Work

One of the main limitations of dense retrieval models is that encoding the query and document into a single vector constrains the representational power of the models. Polyencoder (Humeau et al., 2020), MEBERT (Luan et al., 2021), and MVR (Zhang et al., 2022) propose to use multiple embeddings, instead of one, to represent the query or the document. A more recent approach is token-level multi-vector retrieval, which stores and retrieves with every token embedding. ColBERT (Khattab and Zaharia, 2020) is probably the most renowned model in this family. ALIGNER (i.e. T5-ColBERT) (Qian et al., 2022) extends ColBERT by scaling up the backbone langauge model and studying various strategies for aggregating the token-level alignment scores. These token-level retrieval models show strong effectiveness and out-of-domain generalization ability.

Efforts for reducing serving costs of multi-vector models have been mostly focused on the token-level retrieval stage. COIL (Gao et al., 2021) accelerates token-level retrieval by confining retrieval within exact match tokens, sharing the spirit of classic inverted indexing. CITADEL (Li et al., 2022) relaxes COIL with a lexical routing mechanism where a query token vector only retrieves from a subset of

Figure 6: MRR@10 of XTR with different \(k_{\text{train}}\) and \(k^{t}\). For T5-ColBERT, we also use \(f_{\text{XTR}^{t}}\) with the top-\(k^{t}\) score imputation method for the inference.

Figure 7: Effect of training XTR with different batch sizes and \(k_{\text{train}}\). For each point of the graph, we train XTRbase with the specified training batch size (128, 256, 320) and \(k_{\text{train}}\) (32, 64, 128, 256) and evaluate on each dataset (MS MARCO and ArguAna). nDCG@10 of each model is reported.

document token vectors routed to the same key. PLAID [14] optimizes the speed of ColBERT by pruning weaker candidates in the earlier stages of retrieval and using better vector quantization. ColBERT-v2 [14] further adopts residual representations with cluster centroids to improve the efficiency of ColBERT. On the other hand, how to accelerate the scoring stage remains under-explored. To the best of our knowledge, XTR is the first work to simplify the scoring stage and remove the gathering stage in multi-vector retrieval.

## 7 Conclusion

Multi-vector retrieval leverages query and document token representations for effective information retrieval. In this paper, we propose XTR that simplifies the existing three-stage inference of multi-vector models by improving the initial token retrieval stage. Specifically, XTR scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens. As a result, XTR achieves state-of-the-art performances on zero-shot information retrieval benchmarks while greatly reducing the FLOPs of the scoring stage. We further show that our objective function indeed encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query.

## Limitations

In most of our experiments, XTR was trained on MS MARCO, a large-scale retrieval dataset in English. While our experiments were conducted in a fair setting where most baseline models also utilize MS MARCO, future use cases might need to remove its dependency on MS MARCO due to the license or language-specific issue. We believe that LLM-based retrieval dataset generation [15] would be able to mitigate the problem in the future.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{4}{c}{**T5-ColBERT** token retrieval for _“what is the usual pay for stock associates at michael?”_} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & usual & routine passport services: the usual waiting time in logan to get your passport is four (4) to eight (8) weeks for routine applications. & No \\
2 & usual & the usual pay days are the 1st and 16th of each month. for annual educational paraprofessionals there is no payroll lag. & No \\
5 & usual & the usual part xiii tax rate is 25\% (unless a tax treaty between canada and your home country reduces the rate). & No \\
50 & usual & this is where one can challenge the judgment debtor’s claim. one option & No \\  & & creditors have is to try and make a deal with the debtor to take less than & 25\% (the usual amount of a wage levy). & No \\
100 & usual & the usual maximum inventory is 1 talisman, 26 elemental runes, and 26 pure essence. the ingredients must be brought to an opposing altar... & No \\  & & from the runes being crafted. & \\ \hline \hline \multicolumn{4}{c}{**XTR** token retrieval for _“what is the usual pay for stock associates at michael?”_} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & usual & store manager. 1 salary: the usual salary a store manager receives can & No \\  & & be anywhere around $52,000 to $115,000 annually. & No \\
2 & usual & 1 salary: the usual salary a store manager receives can be anywhere & No \\  & & around $52,000 to $115,000 annually. 2 bonuses: public provide bonuses & No \\  & & that could reach up to $40,000. & \\
5 & average & average salaries for michaels stores stock associate: $9. michaels stores & \\  & & hourly pay trends based on salaries posted anonymously by michaels & \\
50 & v & i think the avg starting pay is closer to 30k for asst mgr trainees. it is an & No \\  & & hourly position until you are fully trained (40 hours per week). & No \\
100 & average & average macros salaries. the average salary for macros jobs is $32,000. average macros salaries can vary greatly due to company, location, industry, experience and benefits. & No \\ \hline \hline \end{tabular}
\end{table}
Table 6: Token retrieval example from MS MARCO. Among the top 100 retrieved tokens, \(100\%\) of T5-ColBERT tokens are lexically identical as the query token usual while only \(8\%\) of XTR tokens are lexically identical. XTR retrieves the relevant passage by retrieving average for usual.

## Acknowledgements

We would like to thank the anonymous reviewers for their helpful feedback. We also thank Nicholas Monath, Raphael Hoffmann, Kelvin Guu, Slav Petrov, and others at Google DeepMind for their helpful comments and discussion.

## References

* Arabzadeh et al. (2022) Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles LA Clarke. Shallow pooling for sparse labels. _Information Retrieval Journal_, 25(4):365-385, 2022.
* Chen et al. (2021) Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? _arXiv preprint arXiv:2110.06918_, 2021.
* Dai et al. (2022) Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. _arXiv preprint arXiv:2209.11755_, 2022.
* Formal et al. (2021) Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stephane Clinchant. Splade v2: Sparse lexical and expansion model for information retrieval. _arXiv preprint arXiv:2109.10086_, 2021.
* Gao et al. (2021) Luyu Gao, Zhuyun Dai, and Jamie Callan. COIL: revisit exact lexical match in information retrieval with contextualized inverted list. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021_, pages 3030-3042. Association for Computational Linguistics, 2021.
* Guo et al. (2020) Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In _International Conference on Machine Learning_, pages 3887-3896. PMLR, 2020.
* Humeau et al. (2020) Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. _Transactions on Machine Learning Research_, 2022.
* Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answering. _ArXiv_, abs/2004.04906, 2020.
* Khattab and Zaharia (2020) Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In _Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval_, pages 39-48, 2020.
* Lee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 6086-6096, Florence, Italy, July 2019. Association for Computational Linguistics.
* Li et al. (2022) Minghan Li, Sheng-Chieh Lin, Barlas Oguz, Asish Ghoshal, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. Citadel: Conditional token interaction via dynamic lexical routing for efficient and effective multi-vector retrieval. _arXiv preprint arXiv:2211.10411_, 2022.
* Luan et al. (2021) Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, Dense, and Attentional Representations for Text Retrieval. _Transactions of the Association for Computational Linguistics_, 9:329-345, 04 2021.
* Liu et al. (2020)Craig Macdonald and Nicola Tonellotto. On approximate nearest neighbour selection for multi-stage dense retrieval. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 3318-3322, 2021.
* Ni et al. (2021) Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern'andez 'Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. In _Conference on Empirical Methods in Natural Language Processing_, 2021.
* Qian et al. (2022) Yujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu, Zhuyun Dai, Siddhartha Brahma, Iftekhar Naim, Tao Lei, and Vincent Y Zhao. Multi-vector retrieval as sparse alignment. _arXiv preprint arXiv:2211.01267_, 2022.
* Qu et al. (2021) Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5835-5847, 2021.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* Ram et al. (2022) Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages without supervision. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2687-2700, 2022.
* Ram and Gray (2012) Parikshit Ram and Alexander G Gray. Maximum inner-product search using cone trees. In _Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 931-939, 2012.
* Sachan et al. (2022a) Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. Improving passage retrieval with zero-shot question generation. _arXiv preprint arXiv:2204.07496_, 2022a.
* Sachan et al. (2022b) Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. Questions are all you need to train a dense passage retriever. _arXiv preprint arXiv:2206.10658_, 2022b.
* Santhanam et al. (2022a) Keshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. Plaid: an efficient engine for late interaction retrieval. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 1747-1756, 2022a.
* Santhanam et al. (2022b) Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3715-3734, 2022b.
* Sciavolino et al. (2021) Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centric questions challenge dense retrievers. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 6138-6148, 2021.
* Shen et al. (2015) Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, and Heng Tao Shen. Learning binary codes for maximum inner product search. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 4148-4156, 2015.
* Shrivastava and Li (2014) Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In _Advances in Neural Information Processing Systems_, pages 2321-2329, 2014.
* Shrivastava and Li (2015) Anshumali Shrivastava and Ping Li. Improved asymmetric locality sensitive hashing (alsh) for maximum inner product search (mips). In _Conference on Uncertainty in Artificial Intelligence_, 2015.
* Shrivastava et al. (2015)Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* Wang et al. [2022] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. Gpl: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2345-2360, 2022.
* Xue et al. [2021] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 483-498, 2021.
* Yih et al. [2011] Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. Learning discriminative projections for text similarity measures. In _Conference on Computational Natural Language Learning_, 2011.
* Zhang et al. [2022a] Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. Multi-view document representation learning for open-domain dense retrieval. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 5990-6000. Association for Computational Linguistics, 2022a.
* Zhang et al. [2022b] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Making a miracl: Multilingual information retrieval across a continuum of languages. _arXiv preprint arXiv:2210.09984_, 2022b.

Derivatives w.r.t. Similarity Scores

Sum-of-maxHere, we use a cross-entropy loss \(\mathcal{L}_{\text{CE}}\) with the sum-of-max operator \(f_{\text{ColBERT}}\) and analyze the derivatives with respect to the token similarity scores.

\[\mathcal{L}_{\text{CE}}=-\log\frac{\exp f(Q,D^{+})}{\sum_{b=1}^{B}\exp f(Q,D_{b })}=-f_{\text{ColBERT}}(Q,D^{+})+\log\sum_{b=1}^{B}\exp f_{\text{ColBERT}}(Q,D_{ b})\] (5)

\[f_{\text{ColBERT}}(Q,D)=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{m}\mathbf{A}_{ij} \mathbf{P}_{ij}=\frac{1}{n}\sum_{i=1}^{n}\mathbf{P}_{ij}\] (6)

Here, we denote \(\hat{j}\) as the index of the row-wise maximum value, dependent on each \(i\) (i.e., \(\mathbf{A}_{ij}=1\)). Given the cross-entropy loss with the sum-of-max operator, we compute the gradient with respect to one of the maximum token similarities \(\mathbf{P}_{ij}^{+}\) for a positive document \(D^{+}\in D_{1:B}\):

\[\frac{\partial\mathcal{L}_{\text{CE}}}{\partial\mathbf{P}_{ij}^{ +}} =-\frac{f(Q,D^{+})}{\partial\mathbf{P}_{ij}^{+}}+\frac{1}{\sum_{b=1 }^{B}\exp f(Q,D_{b})}\frac{\partial}{\partial\mathbf{P}_{ij}^{+}}\sum_{b=1}^{ B}\exp f(Q,D_{b})\] \[=-\frac{1}{n}+\frac{1}{\sum_{b=1}^{B}\exp f(Q,D_{b})}\sum_{b=1}^{ B}\exp f(Q,D_{b})\frac{\partial f(Q,D_{b})}{\partial\mathbf{P}_{ij}^{+}}\] \[=-\frac{1}{n}+\frac{1}{n}\frac{\exp f(Q,D^{+})}{\sum_{b=1}^{B} \exp f(Q,D_{b})}=-\frac{1}{n}\llbracket 1-P(D^{+}|Q,D_{1:B})\rrbracket.\]

Similarly, the gradient w.r.t. a maximum token similarity \(\mathbf{P}_{ij}^{-}\) for a negative document \(D^{-}\in D_{1:B}\) is computed as follows:

\[\frac{\partial\mathcal{L}_{\text{CE}}}{\partial\mathbf{P}_{ij}^{ -}} =-\frac{f(Q,D^{+})}{\partial\mathbf{P}_{ij}^{-}}+\frac{1}{\sum_{b=1 }^{B}\exp f(Q,D_{b})}\frac{\partial}{\partial\mathbf{P}_{ij}^{-}}\sum_{b=1}^{ B}\exp f(Q,D_{b})\] \[=\frac{1}{n}\frac{\exp f(Q,D^{-})}{\sum_{b=1}^{B}\exp f(Q,D_{b})} =\frac{1}{n}P(D^{-}|Q,D_{1:B}).\]

Hence, the positive token-level score \(\mathbf{P}_{ij}^{+}\) will gradually increase until \(P(D^{+}|Q,D_{1:B})\to 1\) and the negative token-level score \(\mathbf{P}_{ij}^{-}\) will decrease until \(P(D^{-}|Q,D_{1:B})\to 0\). This shows that the token-level scores are trained based on the document-level scores, which might stagnate the token-level scores. For instance, even if \(\mathbf{P}_{ij}^{-}\) is very high--later causing \(\mathbf{d}_{j}^{-}\) to be retrieved instead of ones from positive documents--it will not be penalized as long as \(P(D^{-}|Q,D_{1:B})\) is low enough.

In-batch token retrievalCompared to the sum-of-max operator, our in-batch sum-of-max \(f_{\text{XTR}}\) considers the max values only when they are retrieved over other negative tokens in the mini-batch.

\[f_{\text{XTR}}(Q,D_{1:B})=\frac{1}{Z}\sum_{i=1}^{n}\sum_{j=1}^{m}\mathbf{A}_{ ij}\mathbf{\hat{A}}_{ij}\mathbf{P}_{ij}=\frac{1}{Z}\sum_{i=1}^{n}\mathbf{P}_{ ij}\]

Here, we denote \(\bar{j}\) as the index of the row-wise maximum value that is also within the mini-batch top-\(k_{\text{train}}\) given \(q_{i}\) (i.e., satisfies both \(\mathbf{A}_{ij}=1\) and \(\mathbf{\hat{A}}_{ij}=1\)). If there is no such \(\bar{j}\), we simply use \(\mathbf{P}_{i\bar{j}}=0\). We also use a normalizer \(Z\), which is the number of non-zero \(\mathbf{P}_{i\bar{j}}\). In this analysis, we assume \(Z>0\) since if every \(\mathbf{P}_{i\bar{j}}\) is zero, the gradient is undefined.

The gradient w.r.t. the maximum token similarity \(\mathbf{P}_{ij}^{+}\) (non-zero) for a positive document \(D^{+}\in D_{1:B}\) is computed as follows:

\[\frac{\partial\mathcal{L}_{\text{CE}}}{\partial\mathbf{P}_{ij}^{+}} =-\frac{f(Q,D^{+})}{\partial\mathbf{P}_{ij}^{+}}+\frac{1}{\sum_{b =1}^{B}\exp f(Q,D_{b})}\frac{\partial}{\partial\mathbf{P}_{ij}^{+}}\sum_{b=1} ^{B}\exp f(Q,D_{b})\] \[=-\frac{1}{Z^{+}}\big{[}1-\frac{\exp f(Q,D^{+})}{\sum_{b=1}^{B} \exp f(Q,D_{b})}\big{]}\] \[=-\frac{1}{Z^{+}}\big{[}1-P(D^{+}|Q,D_{1:B})\big{]}.\]

This is a very similar result compared to the sum-of-max operator except that 1) the gradient is defined only when \(\mathbf{P}_{ij}^{+}\) is non-zero (i.e. retrieved) and 2) it is dependent on \(Z^{+}\), which means that the gradient will be large whenever there is a small number of retrieved tokens from the positive document. If only a handful of tokens are retrieved for \(D^{+}\), our objective function increases \(\mathbf{P}_{ij}^{+}\).

For negative similarity score \(\mathbf{P}_{ij}^{-}\), we have the following:

\[\frac{\partial\mathcal{L}_{\text{CE}}}{\partial\mathbf{P}_{ij}^{ -}} =-\frac{f(Q,D^{+})}{\partial\mathbf{P}_{ij}^{-}}+\frac{1}{\sum_{b =1}^{B}\exp f(Q,D_{b})}\frac{\partial}{\partial\mathbf{P}_{ij}^{-}}\sum_{b=1} ^{B}\exp f(Q,D_{b})\] \[=-\frac{1}{Z^{-}}\big{[}-\frac{\exp f(Q,D^{-})}{\sum_{b=1}^{B} \exp f(Q,D_{b})}\big{]}\] \[=\frac{1}{Z^{-}}P(D^{-}|Q,D_{1:B}).\]

Again, it is similar to the sum-of-max result, but it depends on \(Z^{-}\). In this case, even when \(P(D^{-}|Q,D_{1:B})\) is low, if there is a small number of retrieved tokens from \(D^{-}\) (i.e., small \(Z^{-}\)), \(\mathbf{P}_{ij}^{-}\) will be decreased significantly. Note that when \(Z^{-}\) is large, \(Z^{+}\) naturally becomes smaller as they compete for in-batch token retrieval, which causes positive tokens to have higher scores.

## Appendix B Inference Complexity

We compare the complexity of ColBERT and XTR during the scoring stage in terms of FLOPs. We do not measure the complexity for the online query encoding and maximum inner product search (MIPS), which have been extensively studied for both dual encoders and multi-vector retrieval (Santhanam et al., 2022, 2022, Guo et al., 2020).

For the scoring stage, both ColBERT and XTR have \(\mathcal{O}(nk^{l})\) candidate documents. Here, we assume the worst case \(nk^{l}\) where each document token comes from a unique document. For each candidate document, ColBERT loads a set of document vectors of \(\bar{m}d\) floating points (\(\bar{m}\) = average document length) and computes eq. (1) with the query vectors of \(nd\) floating points. Computing eq. (1) per candidate document requires \(2n\bar{m}d\) FLOPs for token-level inner products, \(n\bar{m}\) for finding the row-wise max, and \(n\) for the final average. In total, ColBERT requires \(n^{2}k^{l}(\bar{m}d+\bar{m}+1)\) FLOPs for the scoring stage. Note that this does not include the latency of loading the \(\mathcal{O}(nk^{l}\bar{m}d)\) floating points onto the memory, which amounts up to 450MB per query when \(n=16,k^{l}=1000,\bar{m}=55,d\) = \(128\).

On the other hand, XTR first imputes the missing similarity, which is simply done by caching the \(k^{l}\)-th token retrieval score for each query token. Then, each of \(nk^{l}\) candidate documents requires \(n\bar{r}\) FLOPs for finding row-wise max and \(n\) for the average where \(\bar{r}\) is the average number of retrieved tokens per each candidate document. In total, we have \(n^{2}k^{l}(\bar{r}+1)\) FLOPs. Table 1 shows the estimated FLOPs of the two models. XTR reduces the FLOPs at the scoring stage by 4000\(\times\) making multi-vector retrieval more efficient and practical.

Implementation Details

XTR uses \(k_{\text{train}}\) for retrieving in-batch document tokens. Since we retrieve over mini-batches, the size of mini-batch affects the performance for different \(k_{\text{train}}\), which is shown in SS5.3. In our experiments, we tried \(k_{\text{train}}\) = \(\{32,64,128,256,320\}\) for each batch size and choose the best model based on their performance on the MS MARCO development set. For inference, XTR uses \(k^{l}\) for the token retrieval. We use \(k^{l}\) = \(40,000\), which is possible due to the efficient scoring stage of XTR.6 We analyze the effect of using different \(k^{l}\)'s as well as its relationship to \(k_{\text{train}}\) in SS5.3. We initialize XTR from the base and xxl versions of the T5 encoder (Raffel et al., 2020) and provide XTRbase and XTRself. For multilingual XTR, we initialize XTR from mT5 (Xue et al., 2021). We fine-tune XTR for 50,000 iterations with the learning rate to 1e-3. Up to 256 chips of TPU v3 accelerator were used depending on the size of the model. We use ScANN (Guo et al., 2020) for the MIPS during the token retrieval stage. For BEIR, we use 13 datasets (AR: ArguAna. TO: Touche-2020. FE: Fever. CF: Climate-Fever. SF: Scifact. CV: TREC-COVID. INF: NFCPurs. NQ: Natural Questions. HQ: HotpotQA. FQ: FiQA-2018. SD: SCIDOCS. DB: DBPedia. QU: Quora).

Footnote 6: In fact, XTR with \(k^{l}=40,000\) has still two-to-three orders of magnitude cheaper scoring stage than ColBERT with \(k^{l}\) = \(1,000\) and T5-ColBERT with \(k^{l}\) = \(4,000\).

BaselinesThere are two main paradigms on training retriever models for the out-of-domain evaluation. The first group trains a single retriever for each dataset (or domain) by generating queries for each out-of-domain corpus. Typically, this approach generates \(N\) datasets to train \(N\) independent models for \(N\) different domains. For this _one-retriever-per-domain_ approaches, we include GenQ (Thakur et al., 2021), GPL (Wang et al., 2022), and Promptagator (Dai et al., 2022). The second group builds a single retriever--typically trained on a large-scale IR dataset such as MS MARCO--and directly applies it on the out-of-domain corpora and queries. For this _one-retriever-for-all_ approaches, we present results of state-of-the-art retrievers including Splade\({}_{\gamma 2}\)(Formal et al., 2021), ColBERT\({}_{\gamma 2}\)(Santhanam et al., 2022), and GTR\({}_{\text{xxl}}\)(Ni et al., 2021). We also show the results of T5-ColBERT\({}_{\text{xxl}}\)(Qian et al., 2022), which is a T5-initialized ColBERT model and shares the same backbone LM and training dataset with XTR. Note that T5-ColBERT uses the heavy scoring stage based on the original sum-of-max. All of our one-retriever-for-all baselines, as well as XTR, are trained on English MS MARCO, unless otherwise stated.

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{2}{c}{**T5-ColBERT** token retrieval for “_temple university student population_?”} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & temple & about temple university tuition, cost, financial aid, scholarships, and admission rates & No \\
2 & temple & overview the application fee at temple university is \$55. it is selective, with an acceptance rate of 61.7 percent and an early acceptance rate of 78 percent. & No \\
5 & temple & the application fee at temple university is \$55. it is selective, with an acceptance rate of 61.7 percent and an early acceptance rate of 78 percent. & No \\
50 & temple & temple university staff accountants earn \$52,000 annually, or \$25 per hour, which is 14\% higher than the national average for all staff accountants at \$45,000 annually and 16\% lower than the national salary average for all working americans & No \\
100 & temple & browse expedia’s selection and check out the best hotels close to temple university for the world-class spas and restaurants, or snatch up one of the cheap hotel deals near temple university & No \\ \hline \multicolumn{2}{c}{**XTR** token retrieval for “_temple university student population_?”} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & temple & by gender, the school has 18,009 male and 19,476 female students. by race/ethnicity, 20,664 white, 4,466 black, and 3,819 asian students are attending at temple university. & Yes \\
2 & temple & below tables and charts represent the enrollment statistics including school degree, gender, race/ethnicity, and tranfer-in students at the school. at temple university, 37,485 students are enrolled.... & Yes \\
5 & temple & temple university the big picture: how many students were on campus in fall 2015? of the 28,886 new freshman applicants, 56% were admitted and 31\% of the admitted students enrolled at temple university in fall 2015. & Yes \\
50 & temple & temple university was founded in 1884 by russell conwell, a yale-educated boston lawyer, orator, and ordained baptist minister & No \\
100 & temple & kaiser said temple’s endowment fund is low because the university is late to the idea of fundraising. & No \\ \hline \hline \end{tabular}
\end{table}
Table 2: Token retrieval example from MS MARCO for the token “_temple”_ in the query “_temple university student population?”_. Among the top 100 retrieved tokens, \(100\%\) of T5-ColBERT tokens are lexically identical as the query token temple and \(100\%\) of XTR tokens are also lexically identical. However, top retrieved results from XTR are of the correct context (student population) while those from T5-ColBERT are off-topic (e.g., tuition, salary, etc.).

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{4}{c}{**T5-ColBERT** token retrieval for _“aire is expressed in some skin tumors”_} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & aire & acids: structures, properties, and functions (university science books, sausalito, ca, 2000). humans expressing a defective form of the transcription factor aire (autoimmune regulator) develop multiorgan autoimmune disease. & No \\
2 & aire & the primary biochemical defect in apeced is unknown. we have isolated a novel gene, aire, encoding for a putative nuclear protein featuring two phd-type zinc-finger motifs, suggesting its involvement in transcriptional regulation. & No \\
5 & aire & control of central and peripheral tolerance by aire. the negative selection of self-reactive thymocytes depends on the expression of tissue-specific antigens by medullary thymic epithelial cells. & No \\
50 & aire & we found that a human patient and mice with defects in aire develop similar lung pathology, demonstrating that the aire-deficient model of autoimmunity is a suitable translational system in which to unravel fundamental mechanisms of id pathogenesis. & No \\
100 & aire & cool air initiates just downstream of the major sense transcript poly(a) site and terminates either early or extends into the flc promoter region. & No \\ \hline \hline \multicolumn{4}{c}{**XTR** token retrieval for _“aire is expressed in some skin tumors”_} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & aire & keratin-dependent regulation of aire and gene expression in skin tumor keratinocytes expression of the intermediate filament protein keratin 17 (k17) is robustly upregulated in inflammatory skin diseases and in many tumors.... & Yes \\
2 & aire & the thymic transcription factor autoimmune regulator (aire) prevents autoimmunity in part by promoting expression of tissue-specific self-antigens, which include many cancer antigens. for example, aire-deficient patients are predisposed to viitigo, an autoimmune disease of melanocytes that is often triggered by efficacious immunotherapies against melanoma. & Yes \\
5 & aire & aire regulates negative selection of organ-specific t cells autoimmune polyendorcinopathy syndrome type 1 is a recessive mendelian disorder resulting from mutations in a novel gene, aire, and is characterized by a spectrum of organ-specific autoimmune diseases. & No \\
50 & aire & here we demonstrate a novel role for a cd4+3- inducer cell population, previously linked to development of organized secondary lymphoid structures and maintenance of t cell memory in the functional regulation of aire-mediated promiscuous gene expression in the thymus. & No \\
100 & aire & this localization is dependent on the presence of sperm in the spermath-eca. after fertilization, aire-2 remains associated with chromosomes during each meiotic division. & No \\ \hline \hline \end{tabular}
\end{table}
Table 3: Token retrieval example from MS MARCO for the token _“aire”_ in the query _“aire is expressed in some skin tumors”_. Among the top 100 retrieved tokens, \(77\%\) of T5-ColBERT tokens are lexically identical as the query token aire and \(77\%\) of XTR tokens are also lexically identical. Top retrieved results from XTR are relevant to the query (about cancer, tumor, skin, and melanocyte), while those from T5-ColBERT are off-topic.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{3}{l}{**T5-ColBERT** for _“women with a higher birth weight are more likely to develop breast cancer later in life._”} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & **later** & 
\begin{tabular}{l} context exposure to cardiovascular risk factors during childhood and adolescence may be associated with the development of atherosclerosis later in life. \\ \end{tabular} & No \\
2 & **later** & 
\begin{tabular}{l} n despite the high incidence of febrile seizures, their contribution to the development of epilepsy later in life has remained controversial. \\ \end{tabular} & No \\
5 & **later** & 
\begin{tabular}{l} prospectively collected data from two intervention studies in adults with severe malaria were analysed focusing on laboratory features on presentation and their association with a later requirement for rrt. \\ \end{tabular} & No \\
50 & **later** & 
\begin{tabular}{l} they did have a limited amount of proteolytic activity and were able to kill s. aureus. with time, the nuclear envelope ruptured, and dna \\ \end{tabular} & No \\
100 & **late** & 
\begin{tabular}{l} finally, we address the need for a careful consideration of potential \\ benefits of bisphosphonate therapy and the risk for osteeoncrosis of \\ the jaw, a recently recognized late-toxicity of their use. \\ \end{tabular} & No \\ \hline \hline \multicolumn{3}{l}{**XTR** for _“women with a higher birth weight are more likely to develop breast cancer later in life._”} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & **later** & 
\begin{tabular}{l} life course breast cancer risk factors and adult breast density (united \\ kingdom) objective to determine whether risk factors in childhood \\ and early adulthood affect later mammographic breast density. \\ \end{tabular} & Yes \\
2 & **later** & 
\begin{tabular}{l} exposure to cardiovascular risk factors during childhood and adolescence may be associated with the development of atherosclerosis later in life. \\ \end{tabular} & No \\
5 & subsequent & 
\begin{tabular}{l} emerging evidence suggests an association between female prenatal \\ experience and her subsequent risk of developing breast cancer. \\ \end{tabular} & Yes \\
50 & **later** & 
\begin{tabular}{l} our nested case–control study of eh progression included 138 cases, \\ who were diagnosed with eh and then with carcinoma (1970–2003) \\ at least 1 year (median, 6.5 years) later, and 241 controls.... \\ obesity and being overweight during adulthood have been consistently linked to increased risk for development of dementia later in life, especially alzheimer’s disease. \\ \end{tabular} & No \\ \hline \hline \end{tabular}
\end{table}
Table 4: Token retrieval example from Scifact for the token _“later”_ in the query _“women with a higher birth weight are more likely to develop breast cancer later in life”_. Among the top 100 retrieved tokens, \(72\%\) of T5-ColBERT tokens are lexically identical as the query token later while only \(33\%\) of XTR tokens are lexically identical. Top retrieved results from XTR can retrieves synonyms (sebsequent) from relevant context, while those from T5-ColBERT are off-topic.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{3}{c}{**T5-ColBERT** for _“venues have a **thinner** or absent smooth layer compared to arterioles.”_} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & thinner & platelet cd401 is associated with smaller plaques and thinner caps, while p-selectin is associated with smaller core size. conclusions: blood cell activation is significantly associated with atherosclerotic changes of the carotid wall. & No \\
2 & thin & the periosteum is a thin, cellular and fibrous tissue that tightly adheres to the outer surface of all but the articulated surface of bone and appears to play a pivotal role in driving fracture pain. & No \\
5 & thin & immunohistochemical scoring showed significantly (p\textless{}0.0001) higher median 5hmc levels in bcn and dcn than in thin ssm, thick ssm, and cmd. & No \\
50 & weak & subarachnoid haemorrhage (1\textless{}43 [1\textless{}25\textless{}1\textless{}63]), and stable angina (1\textless{}41 [1\textless{}36\textless{}1\textless{}46]), and weakest for abdominal aortic aneurysm (1\textless{}08 [1\textless{}00\textless{}1\textless{}17]). & No \\
100 & slight & the upc-2 gene expression was widely detected in the whole body with substantial levels in the wat and with slight levels in the skeletal muscle and bat. & No \\ \hline \hline \multicolumn{3}{c}{**XTR** for _“venues have a **thinner** or absent smooth layer compared to arterioles.”_} \\ \hline Rank & Token & Context of Token & Relevance \\ \hline
1 & thinner & platelet cd401 is associated with smaller plaques and thinner caps, while p-selectin is associated with smaller core size. conclusions: blood cell activation is significantly associated with atherosclerotic changes of the carotid wall. & No \\
2 & thin & the periosteum is a thin, cellular and fibrous tissue that tightly adheres to the outer surface of all but the articulated surface of bone and appears to play a pivotal role in driving fracture pain. & No \\
5 & thick & in dense fibrotic zones, thickening of the arterial and venous wall with severe luminal narrowing was present in each patient. & No \\
50 & small & we assessed vasomotor function of the adipose microvasculature using videomicroscopy of small arterioles isolated from different fat compartments. & No \\
100 & particle & context circulating concentration of lipoprotein(a) (lp[a]), a large glycoprotein attached to a low-density lipoprotein-like particle, may be associated with risk of coronary heart disease (chd) and stroke. & No \\ \hline \hline \end{tabular}
\end{table}
Table 5: Token retrieval example from Scifact for the token _“thinner”_ in the query _“vanules have a thinner or absent smooth later compared to arterioles”_. Among the top 100 retrieved tokens, only \(1\%\) of T5-ColBERT tokens are lexically identical as the query token thinner and only \(1\%\) of XTR tokens are also lexically identical.