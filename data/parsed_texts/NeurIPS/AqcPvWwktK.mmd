# Semi-supervised Multi-label Learning with Balanced Binary Angular Margin Loss

Ximing Li\({}^{1,2}\)   Silong Liang\({}^{1,2}\)   Changchun Li\({}^{1,2,}\)1   Pengfei Wang\({}^{3,4}\)   Fangming Gu\({}^{1,2}\)

\({}^{1}\)College of Computer Science and Technology, Jilin University, China

\({}^{2}\)Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China

\({}^{3}\)Computer Network Information Center, Chinese Academy of Sciences, China

\({}^{4}\)University of Chinese Academy of Sciences, Chinese Academy of Sciences, China

liximing86@gmail.com, changchunli93@gmail.com, liangsl23@mails.jlu.edu.cn, pfwang@cnic.cn, gufm@jlu.edu.cn

Footnote 1: Corresponding author.

###### Abstract

Semi-supervised multi-label learning (SSMLL) refers to inducing classifiers using a small number of samples with multiple labels and many unlabeled samples. The prevalent solution of SSMLL involves forming pseudo-labels for unlabeled samples and inducing classifiers using both labeled and pseudo-labeled samples in a self-training manner. Unfortunately, with the commonly used binary type of loss and negative sampling, we have empirically found that learning with labeled and pseudo-labeled samples can result in the variance bias problem between the feature distributions of positive and negative samples for each label. To alleviate this problem, we aim to balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. Specifically, we extend the traditional binary angular margin loss to a balanced extension with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. With this insight, we propose a novel SSMLL method, namely **S**emi-**S**upervised **M**ulti-**L**abel **L**earning with **B**alanced **B**inary **A**ngular **M**argin loss (**S\({}^{2}\)ML\({}^{2}\)-b****b****a**m). To evaluate the effectiveness of S\({}^{2}\)ML\({}^{2}\)-b****b****a**m, we compare it with existing competitors on benchmark datasets. The experimental results validate that S\({}^{2}\)ML\({}^{2}\)-b****a**m can achieve very competitive performance.

## 1 Introduction

Multi-label learning (MLL) refers to the classification problem where each training sample can be associated with multiple labels [1]. For example, in text categorization, a text can involve a certain number of topics simultaneously [2; 3]; and in image annotation, an image can contain multiple objects of interest in one scene [4; 5]. Compared with single-label learning, MLL is a more prevalent paradigm in real-world scenarios, and it has been widely used in many applications such as information retrieval [6; 7] and recommendation systems [8; 9].

Despite the successful application of MLL, the competitive performance of most MLL methods heavily depends on the large volume of training samples with precise supervision [4, 10, 11]. Unfortunately, it is expensive to manually annotate each sample, so it is naturally time-consuming to collect loads of labeled training samples. Accordingly, the community has turned to alternative candidates to MLL, and raised the question of whether one can induce robust MLL classifiers with a small number of labeled samples and a large number of unlabeled samples, which are cheaper to collect. This concept gives birth to the emerging research topic of semi-supervised **m**ulti-**l**abel **I**earning (**SSMLL**), and many attempts have been recently proposed [12, 13, 14, 15, 16, 17].

Generally, the topic of SSMLL, as its name suggests, is in parallel inherited from semi-supervised learning (SSL) and MLL. The current prevalent ideas are estimating pseudo-labels of unlabeled samples with SSL techniques and inducing MLL classifiers with both labeled and pseudo-labeled samples in a self-training manner. Following the prior arts [18, 19], the binary kind of losses, _e.g._ binary cross-entropy loss and asymmetric loss [20], are commonly used to optimize MLL classifiers, where those are equivalent to optimizing the binary loss between the positive and negative samples for each label. To alleviate the imbalanced issue between positive and negative samples, especially for the scenarios with massive labels, the negative sampling tricks are often employed [21, 22, 23]. Unfortunately, in our preliminary experiments, we found such training paradigms suffer from the **variance bias** problem by using the labeled and pseudo-labeled samples in the context of SSMLL, since it is difficult to guarantee estimating accurate pseudo-labels. To be specific, the problem implies that for each label, in SSMLL the variance difference between feature distributions of positive and negative samples is often larger than the ones in fully supervised learning, as illustrated in Fig.1. In this situation, each trained binary boundary tends to keep away from the Bayesian optimal one, resulting in performance degradation.

To tackle this problem, we propose a novel SSMLL method, namely **S**emi-**S**upervised **M**ulti-**L**abel **L**earning with **B**alanced **B**inary **A**ngular **M**argin loss (**S\({}^{2}\)ML\({}^{2}\)-****b****b****b****b****am**). The basic insight of S\({}^{2}\)ML\({}^{2}\)-b****b****b****b****b****am is to balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. To be specific, we extend the binary angular margin (BAM) loss, which measures the prediction loss by using the angle between the feature and binary boundary for each label. We suppose that for each label these feature angles of positive and negative samples are drawn from label-specific "positive" and "negative" Gaussian distributions, which are estimated by employing both labeled and pseudo-labeled samples during classifier training. Therefore, we can apply some linear Gaussian transformations over these feature angle distributions, so as to balance the variance bias between positive and negative samples for each label. Upon this idea, we design a new balanced binary angular margin (BBAM) loss and construct a novel S\({}^{2}\)ML\({}^{2}\)-b****b****b****am method based on the designed BBAM loss and self-training manner. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. We evaluate the proposed S\({}^{2}\)ML\({}^{2}\)-b****b****b****am by comparing the most recent competitors on benchmark datasets. Experimental results indicate the superior performance of S\({}^{2}\)ML\({}^{2}\)-b****b****b****am.

In summary, the main contributions of this paper are listed as follows:

* We develop a novel SSMLL method, namely S\({}^{2}\)ML\({}^{2}\)-b****b****b****am, by balancing the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label.

Figure 1: The variance difference between feature distributions (VDFD) of positive and negative samples computed in semi-supervised and supervised manners across labels \(\{6,7,14,17\}\) of _VOC2012_.

* We design a new BBAM loss by extending the traditional binary angular margin loss with feature angle distribution transformations under the Gaussian assumption, and suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label.
* We construct extensive experiments to evaluate S\({}^{2}\)ml\({}^{2}\)-bbam, and experimental results demonstrate the effectiveness of S\({}^{2}\)ml\({}^{2}\)-bbam.

## 2 Formulation and Analysis

### Problem Formulation

By convention, we use \(\mathbf{x}\) to denote the sample feature vector and \(\mathbf{y}\in\{0,1\}^{K}\) the label indicator vector of \(K\) pre-defined classes, where \(0/1\) implies a sample is irrelevant/relevant to the category. In the task of SSMLL, we are formally given a collection of training samples \(\mathcal{D}=\{\mathcal{D}_{l},\mathcal{D}_{u}\}\), where \(\mathcal{D}_{l}=\{(\mathbf{x}_{i}^{l},\mathbf{y}_{i}^{l})\}_{i=1}^{i=N_{l}}\) and \(\mathcal{D}_{u}=\{\mathbf{x}_{j}^{u}\}_{j=1}^{j=N_{u}}\) are the collections of \(N_{l}\) labeled and \(N_{u}\) unlabeled samples, respectively. The goal of SSMLL is to induce a classifier \(f_{\mathbf{W}}(\mathbf{x})\), parameterized by \(\mathbf{W}\), from \(\mathcal{D}\) and use the classifier \(f_{\mathbf{W}}(\mathbf{x})\) to predict the label indicator vectors for future samples.

Broadly speaking, the classifier \(f_{\mathbf{W}}(\mathbf{x})\) typically consists of a backbone encoder and a classification layer, parameterized by \(\mathbf{W}^{e}\) and \(\mathbf{W}^{c}\), respectively (_i.e._\(\mathbf{W}=\{\mathbf{W}^{e},\mathbf{W}^{c}\}\)). Specifically, the backbone encoder transforms any original feature vector \(\mathbf{x}\) into a more discriminative latent feature \(\mathbf{z}=f_{\mathbf{W}^{e}}(\mathbf{x})\); the classification layer applies \(\mathbf{z}\) to generate its corresponding predictive logits \(\mathbf{p}=f_{\mathbf{W}^{e}}(\mathbf{z})\). Given an SSMLL training dataset \(\mathcal{D}\), the classifier \(f_{\mathbf{W}}(\mathbf{x})\) is commonly optimized by minimizing the following generic self-training objective concerning \(\mathbf{W}\) on \(B_{l}\)-sized labeled and \(B_{u}\)-sized unlabeled batches:

\[\mathcal{L}(\mathbf{W})=\frac{1}{B_{l}K}\sum_{i=1}^{B_{l}}\sum_{k=1}^{K}\ell(p_ {ik}^{l},y_{ik}^{l})+\frac{\lambda}{B_{u}K}\sum_{i=1}^{B_{u}}\sum_{k=1}^{K}\ell (p_{ik}^{u},y_{ik}^{u}), \tag{1}\]

where \(\ell(\cdot,\cdot)\) is a binary loss function; \(\lambda\) is the coefficient parameter; \(\mathbf{p}_{i}^{l}=f_{\mathbf{W}}(\mathbf{x}_{i}^{l})\) and \(\mathbf{p}_{i}^{u}=f_{\mathbf{W}}(\mathbf{x}_{i}^{u})\) are the predictive logits of labeled and unlabeled samples, respectively; \(\mathbf{y}_{i}^{u}\) is the pseudo-label of unlabeled samples induced from its current classifier prediction \(\mathbf{p}_{i}^{u}\).

### How Variance Bias Affects the Performance

As shown in Fig.1, we have observed that the generic self-training objective of SSMLL may suffer from the variance bias problem. Here, we discuss how it will affect the classification performance. We treat SSMLL as \(K\) independent semi-supervised binary classification (SSBC) tasks. For each SSBC task, let \(\{(\mathbf{x}_{i},y_{i}^{*})\}\cup\{\mathbf{x}_{i}\}\) be the training data, where \(\mathbf{x}\in\mathbb{R}^{d}\) and \(y^{*}\in\{-1,+1\}\) is the ground-truth label. Besides, let \(\widehat{y}\in\{-1,+1\}\) be the pseudo-label. For clarity and conciseness, we study the SSBC training data drawn from a mixture Gaussian distribution \(\mathcal{P}^{*}\), which can be defined by the following distribution over \((\mathbf{x},y)\in\mathbb{R}^{d}\times\{\pm 1\}\):

\[y=\begin{cases}+1,\ p=\alpha,\\ -1,\ p=1-\alpha,\end{cases}\qquad\mathbf{x}\sim\begin{cases}\mathcal{N}( \boldsymbol{\mu},\boldsymbol{\Sigma}_{+}^{2})&\text{if }y=+1;\\ \mathcal{N}(-\boldsymbol{\mu},\boldsymbol{\Sigma}_{-}^{2})&\text{if }y=-1,\end{cases} \tag{2}\]

where \(\alpha\) is the prior probability of class "+1", \(\boldsymbol{\mu}=\{\mu_{1},\ldots,\mu_{d}\}^{\top}\), \(\boldsymbol{\Sigma}_{+}=\text{diag}(\{\sigma_{+}^{(1)},\ldots,\sigma_{+}^{(d)}\})\), \(\boldsymbol{\Sigma}_{-}=\text{diag}(\{\sigma_{-}^{(1)},\ldots,\sigma_{-}^{(d)}\})\), \(\mu_{i},\sigma_{-}^{(i)},\sigma_{+}^{(i)}>0\ \forall i\in[d]\), and \(\sum_{i=1}^{d}(\sigma_{+}^{(i)})^{2}:\sum_{i=1}^{d}(\sigma_{-}^{(i)})^{2}=1:M^{2}\) with \(M>0,M\neq 1\). We concentrate on analyzing the effect of the variance proportion \(M\) of the distribution \(\mathcal{D}^{*}\) on the performance of the linear model \(f_{ssl}(\mathbf{x})=\text{sign}(\langle\mathbf{w},\mathbf{x}\rangle+b)\), where the parameters \(\mathbf{w}\in\mathbb{R}^{d},b\in\mathbb{R}\), and \(\text{sign}(t)\) evaluates to \(+1\) if scalar \(t\geq 0\) and to \(-1\) otherwise. For simplicity, we denote

\[\mathcal{R}(f,+1)=\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{P}^{*}}[\mathds{1}(f( \mathbf{x})=-1)|y=+1],\ \mathcal{R}(f,-1)=\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{P}^{*}}[ \mathds{1}(f(\mathbf{x})=+1)|y=-1],\]

where \(\mathds{1}(t)\) is the indicator function that takes 1 where t is true and 0 otherwise. We have the following theorems, whose proof can be found in the Appendix A.

**Theorem 2.1**.: _Given an SSBC dataset with pseudo-labels \(\mathcal{S}=\{(\mathbf{x}_{i},y_{i})\}=\{(\mathbf{x}_{i},y_{i}^{*})\}\cup\{( \mathbf{x}_{i},\widehat{y}_{i})\}\), the optimal linear classifier \(f_{ssl}\) minimizing the average standard classification error is given by:_\[f_{ssl}=\operatorname*{arg\,min}_{f}\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{S}}[ \mathds{1}(f(\mathbf{x})\neq y)]. \tag{3}\]

_When \(M>1\), it has the intra-class standard classification errors for the two classes :_

\[\mathcal{R}(f_{ssl},+1) =\Phi\big{(}A-M\sqrt{A^{2}+q(M,\alpha,\epsilon_{-},\epsilon_{+}) }\big{)},\] \[\mathcal{R}(f_{ssl},-1) =\Phi\big{(}-M\cdot A+\sqrt{A^{2}+q(M,\alpha,\epsilon_{-}, \epsilon_{+})}\big{)},\]

_and when \(M<1\), they are given by:_

\[\mathcal{R}(f_{ssl},+1) =\Phi\big{(}A+M\sqrt{A^{2}+q(M,\alpha,\epsilon_{-},\epsilon_{+}) }\big{)},\] \[\mathcal{R}(f_{ssl},-1) =\Phi\big{(}-M\cdot A-\sqrt{A^{2}+q(M,\alpha,\epsilon_{-}, \epsilon_{+})}\big{)},\]

_where \(\Phi(\cdot)\) is the cumulative distribution function (c.d.f.) of standard Gaussian distribution \(\mathcal{N}(0,1)\), \(A=\frac{2\mu}{(M^{2}-1)^{\Sigma}},\quad q(M,\alpha,\epsilon_{-},\epsilon_{+} )=\frac{2\log M+2C}{M^{2}-1}\), \(C=\log\bigl{(}\frac{\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2 \epsilon_{-}-\epsilon_{+})}\big{)}\), \(\mu=\sum_{i=1}^{i=d}\mu_{i}\), \(\Sigma=\sqrt{\sum_{i=1}^{i=d}(\sigma_{+}^{(i)})^{2}}\), and \(\{\epsilon_{-},\epsilon_{+}\}\) are the proportions of negative instances being treated as positive ones and positive instances being treated as negative ones within pseudo-labels, respectively. If \(\sum_{i=1}^{d}(\sigma_{+}^{(i)})^{2}=\sum_{i=1}^{d}(\sigma_{-}^{(i)})^{2}\), i.e. \(M=1\), the intra-class standard classification errors for the two classes can be expressed as follows:_

\[\mathcal{R}(f_{ssl},+1)=\Phi\big{(}\frac{-2\mu^{2}-C\Sigma^{2}}{2\mu\Sigma} \big{)},\quad\mathcal{R}(f_{ssl},-1)=\Phi\big{(}\frac{-2\mu^{2}+C\Sigma^{2}}{2 \mu\Sigma}\big{)}.\]

Following [24; 25; 26], We employ _variance of class-wise accuracy_ (VCA) to quantitatively measure the model fairness and present the definition of VCA below.

**Definition 2.2**.: (VCA) Given a classifier \(f:\mathcal{X}\rightarrow\mathcal{Y}\) where \(\mathcal{Y}=\{1,2,3,\cdots,K\}\), the variance of class-wise accuracy of \(f\) is defined as \(VCA(f)=\frac{1}{K}\sum_{i=1}^{K}(p(i)-\bar{p})\), where \(p(i)=\mathbb{P}[f(\mathbf{x})=i|y=i]=1-\mathbb{P}[f(\mathbf{x})\neq i|y=i]\) and \(\bar{p}=\frac{1}{K}\sum_{i=1}^{K}p(i)\).

**Theorem 2.3**.: _Given an trained linear SSBC model \(f_{ssl}\) in Eq.(3), the variance of class-wise accuracy \(VCA(f_{ssl})\) is increasing when \(M\rightarrow\infty\) for \(M>1\) and \(M\to 0\) for \(M<1\). Suppose \(\log\bigl{(}\frac{\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2 \epsilon_{-}-\epsilon_{+})}\bigr{)}=0\), then when \(M=1\), \(\mathcal{R}(f_{ssl},+1)=\mathcal{R}(f_{ssl},-1)\) and \(VCA(f_{ssl})=0\)._

_Remark 2.4_.: According to Theorem 2.3, the bigger or smaller value of \(M\) will result in the increase of the variance of class-wise accuracy \(VCA(f_{ssl})\), which implies that the SSBC classifier \(f_{ssl}\) induced by Eq.(3) is unfair. Note that \(M\) is the variance proportion of feature distributions of positive and negative samples as defined in (2). Therefore, to improve the fairness of the induced classifier, we propose to balance the variance bias of positive and negative samples for each label from the feature angle distribution perspective, leading to our S\({}^{2}\)ml\({}^{2}\)-bbam.

## 3 Proposed S\({}^{2}\)ml\({}^{2}\)-bbam Method

In this section, we introduce the proposed SSMLL method named **S\({}^{2}\)ml\({}^{2}\)-bbam**.

### Overview

Generally, our S\({}^{2}\)ml\({}^{2}\)-bbam is built on the generic self-training objective of SSMLL formulated by Eq.1. Specifically, we propose a novel **B**alanced **B**inary **A**ngular **M**argin (**BBAM**) loss \(\ell_{\textsc{bram}}(\cdot,\cdot)\), aiming to balance the variance bias of positive and negative samples for each label from the feature angle distribution perspective with the Gaussian assumption. By applying our proposed BBAM loss to the generic SSMLL self-training objective in Eq.1, the objective of S\({}^{2}\)ml\({}^{2}\)-bbam can be formulated as follows:

\[\mathcal{L}(\mathbf{W})=\frac{1}{B_{l}K}\sum_{i=1}^{B_{l}}\sum_{k=1}^{K}\beta _{ik}\ell_{\textsc{bram}}(p_{ik}^{l},y_{ik}^{l})+\frac{\lambda}{B_{u}K}\sum_ {i=1}^{B_{u}}\sum_{k=1}^{K}\beta_{ik}\ell_{\textsc{bram}}(p_{ik}^{u},y_{ik}^ {u}), \tag{4}\]

where

\[\beta_{ik}=\begin{cases}1&\text{if }(\mathbf{x}_{i},\mathbf{y}_{i})\in\Omega_{k}; \\ 1&\text{if }y_{ik}=1;\\ 0&\text{otherwise},\end{cases}\qquad\forall k\in[K],\ \forall i\in[N_{l}]\ \text{or }[N_{u}],\]and \(\{\Omega_{k}\}_{k=1}^{k=K}\) denotes high-quality negative sample sets constructed by negative sampling.

Here, pseudo-labels of unlabeled data \(\{\mathbf{y}_{i}^{u}\}_{j=1}^{i=N_{u}}\) are produced by employing the Class-Aware Pseudo-labeling (CAP) trick [16], which drives their label distribution towards the prior one that is estimated with the labeled samples. Specifically, given the current classifier predictions \(\{\mathbf{p}_{i}^{u}\}_{i=1}^{i=N_{u}}\) of unlabeled samples, \(\{\mathbf{y}_{i}^{u}\}_{i=1}^{i=N_{u}}\) are given by:

\[y_{ik}^{u}=\begin{cases}1&\text{if }p_{ik}^{u}>=\delta_{k};\\ 0&\text{if }p_{ik}^{u}<=\gamma_{k};\\ -1&\text{otherwise},\end{cases}\quad\forall k\in[K],\ \forall i\in[N_{u}], \tag{5}\]

where the class-aware thresholds \(\{\delta_{k}\}_{k=1}^{k=K}\) and \(\{\gamma_{k}\}_{k=1}^{k=K}\) are calculated by solving the equations:

\[\begin{cases}\frac{\sum_{i=1}^{N_{u}}\mathbf{1}(p_{ik}^{u}>=\delta_{k})}{N_{u }}=\frac{\sum_{i=1}^{N_{l}}\mathbf{1}(p_{ik}^{l}=1)}{N_{l}},\\ \frac{\sum_{i=1}^{N_{u}}\mathbf{1}(p_{ik}^{u}<=\gamma_{k})}{N_{u}}=\frac{\sum_{ i=1}^{N_{l}}\mathbf{1}(p_{ik}^{l}=0)}{N_{l}},\end{cases}\quad\forall k\in[K],\ \forall i\in[N_{u}],\]

and \(y_{ik}^{u}=-1\) means that it will not be used for the classifier training.

### BSAM loss

In this section, we introduce the proposed BBAM loss. As its name suggests, our BBAM loss is extended from the **B**inary **A**ngular **M**argin (BAM) loss, which measures the label-specific prediction risk by using the angle between the latent feature and boundary. Formally, for a training sample \((\mathbf{x}_{i},\mathbf{y}_{i})\), the BAM loss can be formulated as:

\[\ell_{\text{BAM}}(p_{ik},y_{ik})=\begin{cases}-\log(\frac{1}{1+e^{-s(p_{ik}-m)} })&\text{if }y_{ik}=1;\\ -\log(1-\frac{1}{1+e^{-s(p_{ik}-m)}})&\text{if }y_{ik}=0,\end{cases} \tag{6}\]

where \(p_{ik}=\cos(\theta_{ik})=\frac{\mathbf{z}_{i}^{u}\mathbf{W}_{k}^{c}}{\|\mathbf{ z}_{i}\|_{2}\|\mathbf{W}_{k}^{c}\|_{2}}\), \(\|\!\cdot\!\|_{2}\) is the \(\ell_{2}\)-norm of vectors; \(\mathbf{z}_{i}\) and \(\mathbf{W}_{k}^{c}\) denote the latent feature of sample \(i\) and the weight vector of the classification layer for category \(k\), respectively; \(\theta_{ik}\) is the angle between \(\mathbf{z}_{i}\) and \(\mathbf{W}_{k}^{c}\), \(s\) and \(m\) are the parameters used to control the rescaled norm and magnitude of cosine margin, respectively.

Reviewing the BAM loss in Eq.6, one can observe that it calculates the loss by employing the label angles of samples for each category. We consider that its trained binary boundary tends to deviate from the Bayesian optimal one for each category in SSMLL, where for most categories, the differences between feature distribution variances of corresponding positive and negative samples are much larger than ones in fully supervised learning. To address this issue, for each category \(k\), we suppose that label angles of its positive samples and ones of its negative samples are drawn from a label-specific "positive" Gaussian distribution \(\mathcal{N}(\mu_{k}^{(p)},(\sigma_{k}^{2})^{(p)})\) and a label-specific "negative" one \(\mathcal{N}(\mu_{k}^{(n)},(\sigma_{k}^{2})^{(n)})\), respectively. According to the properties of Gaussian distribution, we can easily transfer them into ones \(\mathcal{N}(\mu_{k}^{(p)},\widehat{\sigma}_{k}^{2})\) and \(\mathcal{N}(\mu_{k}^{(n)},\widehat{\sigma}_{k}^{2})\) with balanced variance \(\widehat{\sigma}_{k}^{2}=\frac{(\sigma_{k}^{2})^{(p)}+(\sigma_{k}^{2})^{(n)}} {2}\), by performing the following Gaussian linear transformations on those label angles:

\[\psi_{k}^{(p)}(\theta_{ik})=a_{k}^{(p)}\theta_{ik}+b_{k}^{(p)}, \quad\psi_{k}^{(n)}(\theta_{ik})=a_{k}^{(n)}\theta_{ik}+b_{k}^{(n)},\] \[a_{k}^{(p)}=\frac{\widehat{\sigma}_{k}}{\sigma_{k}^{(p)}},\quad b _{k}^{(p)}=(1-a_{k}^{(p)})\mu_{k}^{(p)},\quad a_{k}^{(n)}=\frac{\widehat{ \sigma}_{k}}{\sigma_{k}^{(n)}},\quad b_{k}^{(n)}=(1-a_{k}^{(n)})\mu_{k}^{(n)}, \quad\forall k\in[K]. \tag{7}\]

With these linear transformation pairs \(\{(\psi_{k}^{(p)}(\cdot),\psi_{k}^{(n)}(\cdot))\}\), for each category, label angles of both positive and negative samples can be refined into ones drawn from balanced angular distributions with one same variance, _e.g._

\[\psi_{k}^{(p)}(\theta_{ik})\sim\mathcal{N}(\mu_{k}^{(p)},\widehat{\sigma}_{k}^ {2})\quad\text{if }y_{ik}=1;\quad\psi_{k}^{(n)}(\theta_{ik})\sim\mathcal{N}(\mu_{k}^{(n)}, \widehat{\sigma}_{k}^{2})\quad\text{if }y_{ik}=0.\]Accordingly, the BAM loss in Eq.6 can be rewritten as the following BBAM loss:

\[\ell_{\text{BBAM}}(p_{ik},y_{ik})=\begin{cases}-\log(\frac{1}{1+e^{-s*(\cos(\psi_{ k}^{(p)}(\theta_{ik}))-m)}})&\text{if }y_{ik}=1;\\ \\ -\log(1-\frac{1}{1+e^{-s*(\cos(\psi_{k}^{(n)}(\theta_{ik}))-m)}})&\text{if }y_{ ik}=0.\end{cases} \tag{8}\]

**Estimating label angle variances.** As mentioned above, we concentrate on estimating label-specific "positive" and "negative" angular distributions, _i.e._\(\{\mathcal{N}(\mu_{k}^{(p)},(\sigma_{k}^{2})^{(p)})\}_{k=1}^{k=K}\) and \(\{\mathcal{N}(\mu_{k}^{(n)},(\sigma_{k}^{2})^{(n)})\}_{k=1}^{k=K}\), for each category whose draws are the angles between its label prototype \(\mathbf{c}_{k}\) and latent features of its corresponding positive and negative samples, respectively. Here, we approximate \(\{(\mu_{k}^{(p)},(\sigma_{k}^{2})^{(p)})\}_{k=1}^{k=K}\), \(\{(\mu_{k}^{(n)},(\sigma_{k}^{2})^{(n)})\}_{k=1}^{k=K}\), and \(\{\mathbf{c}_{k}\}_{k=1}^{k=K}\) with labeled and pseudo-labeled samples per-epoch.

For convenience, we denote \(\mathfrak{D}=\{(\mathbf{z}_{i},\mathbf{y}_{i})\}_{i=1}^{i=N_{i}+N_{u}}=\{( \mathbf{z}_{i}^{l},\mathbf{y}_{i}^{l})\}_{i=1}^{i=N_{l}}\cup\{(\mathbf{z}_{i}^ {u},\mathbf{y}_{i}^{u})\}_{i=1}^{i=N_{u}}\) as the couple set of latent features and labels or pseudo-labels of training samples \(\mathfrak{D}\) in the current epoch. We calculate label prototypes \(\{\mathbf{c}_{k}\}_{k=1}^{k=K}\) by averaging latent features of positive samples in \(\mathfrak{D}\) as:

\[\mathbf{c}_{k}=\frac{\sum_{i=1}^{N_{i}+N_{u}}\mathds{1}(y_{ik}=1)\mathbf{z}_{i} }{\sum_{i=1}^{N_{i}+N_{u}}\mathds{1}(y_{ik}=1)},\;\forall k\in[K]. \tag{9}\]

Consequently, the label angles between label prototypes and latent features of samples are given by:

\[\phi_{ik}=\arccos(\frac{\mathbf{z}_{i}^{\top}\mathbf{c}_{k}}{\|\mathbf{z}_{i} \|_{2}\|\mathbf{c}_{k}\|_{2}}),\;\forall k\in[K],\;\forall i\in[N_{l}+N_{u}],\]

Accordingly, the estimations of \(\{(\mu_{k}^{(p)},(\sigma_{k}^{2})^{(p)})\}_{k=1}^{k=K}\) and \(\{(\mu_{k}^{(n)},(\sigma_{k}^{2})^{(n)})\}_{k=1}^{k=K}\) based on the current negative sample sets \(\{\Omega_{k}\}_{k=1}^{k=K}\) can be formulated as:

\[\mu_{k}^{(p)}=\frac{\sum_{i=1}^{N_{i}+N_{u}}\mathds{1}(y_{ik}=1) \phi_{ik}}{\sum_{i=1}^{N_{i}+N_{u}}\mathds{1}(y_{ik}=1)},\qquad(\sigma_{k}^{2 })^{(p)}=\frac{\sum_{i=1}^{N_{i}+N_{u}}\mathds{1}(y_{ik}=1)(\phi_{ik}-\mu_{k}^ {(p)})^{2}}{\sum_{i=1}^{N_{i}+N_{u}}\mathds{1}(y_{ik}=1)-1},\] \[\mu_{k}^{(n)}=\frac{\sum_{i=1}^{N_{i}+N_{u}}\beta_{ik}\mathds{1}(y _{ik}=0)\phi_{ik}}{\sum_{i=1}^{N_{i}+N_{u}}\beta_{ik}\mathds{1}(y_{ik}=0)}, \qquad(\sigma_{k}^{2})^{(n)}=\frac{\sum_{i=1}^{N_{i}+N_{u}}\beta_{ik}\mathds{1 }(y_{ik}=0)(\phi_{ik}-\mu_{k}^{(n)})^{2}}{\sum_{i=1}^{N_{i}+N_{u}}\beta_{ik} \mathds{1}(y_{ik}=0)-1}. \tag{10}\]

Besides, to avoid the misleading effect of false positive or negative samples, we also employ moving average with a learning rate \(\rho\) over \(\{(\mu_{k}^{(p)},(\sigma_{k}^{2})^{(p)})\}_{k=1}^{k=K}\), \(\{(\mu_{k}^{(n)},(\sigma_{k}^{2})^{(n)})\}_{k=1}^{k=K}\), and \(\{\mathbf{c}_{k}\}_{k=1}^{k=K}\).

### Negative Sampling

For efficiency, we suggest a prototype-based negative sampling method. Specifically, for each label, we tend to select those negative samples that are more similar to its positive samples, because they are more difficult to discriminate and would be more informative for the classifier training [21; 22]. To achieve this, for each category, we measure similarity scores of negative samples based on label prototypes \(\{\mathbf{c}_{k}\}_{k=1}^{k=K}\), and construct the nearest neighbor negative sample sets \(\{\widetilde{\Omega}_{k}\}_{k=1}^{k=K}\) as:

\[\widetilde{\Omega}_{k}=\{(\mathbf{x}_{i},\mathbf{y}_{i})|d(\mathbf{z}_{i}, \mathbf{c}_{k})\in\text{Rank}(\{d(\mathbf{z}_{i},\mathbf{c}_{k})\}_{(\mathbf{ x}_{i},\mathbf{y}_{i})\in\widehat{\Omega}_{k}}),(\mathbf{x}_{i},\mathbf{y}_{i}) \in\widehat{\Omega}_{k}\}\quad\forall k\in[K],\]

where \(d(\cdot)\) is the vector distance (_e.g._ cosine distance), \(\text{Rank}(\cdot)\) outputs a set of samples with the top-\(M\) minimum distance values; and \(\{\widehat{\Omega}_{k}\}_{k=1}^{k=K}\) is the negative sample set of category \(k\) defined as:

\[\widehat{\Omega}_{k}=\{(\mathbf{x}_{i}^{l},\mathbf{y}_{i}^{l})|(\mathbf{x}_{i} ^{l},\mathbf{y}_{i}^{l})\in\mathcal{D}_{l},y_{ik}^{l}=0\}\cup\{(\mathbf{x}_{i} ^{u},\mathbf{y}_{i}^{u})|\mathbf{x}_{i}^{u}\in\mathcal{D}_{u},y_{ik}^{u}=0\}.\]

Accordingly, the final negative sample sets \(\{\Omega_{k}\}_{k=1}^{k=K}\) are generated by:

\[\Omega_{k}=\{(\mathbf{x}_{i},\mathbf{y}_{i})|(\mathbf{x}_{i},\mathbf{y}_{i}) \sim\text{Uniform}(\widetilde{\Omega}_{k})\}\quad\forall k\in[K], \tag{11}\]

with size \(\{|\Omega_{k}|=\eta N_{k}\}_{k=1}^{k=K}\), where \(N_{k}=\sum_{i=1}^{N_{l}}\mathds{1}(y_{ik}^{l}=1)+\sum_{i=1}^{N_{u}}\mathds{1}(y_ {ik}^{u}=1)\), \(\eta\) controls the proportion of positive and negative samples of each category. And we update those negative sample sets \(\{\Omega_{k}\}_{k=1}^{k=K}\) per-epoch for efficiency.

### Model Training Summary

We describe the full training process of \(\text{S}^{2}\text{ml}^{2}\)-bham. To avoid inaccurate pseudo-labels in the early training stage, following [16], we warm up the classifier \(f_{\mathbf{W}}(\cdot)\) with the BAM loss of Eq.6 over labeled samples \(\mathcal{D}_{l}\) by \(T_{0}\) epochs. Given the initialized \(f_{\mathbf{W}}(\cdot)\), we continue to train it with the BBAM loss of Eq.8 over labeled samples \(\mathcal{D}_{l}\) and unlabeled samples \(\mathcal{D}_{u}\) by \(T_{t}\) epochs. At each epoch, we update pseudo labels \(\{y^{\mathbf{u}}_{i}\}_{i=1}^{i=N_{u}}\) by using Eq.5, label prototypes \(\{\mathbf{c}_{k}\}_{k=1}^{k=K}\), \(\{(\mu^{(p)}_{k},(\sigma^{2}_{k})^{(p)})\}_{k=1}^{k=K}\) and \(\{(\mu^{(n)}_{k},(\sigma^{2}_{k})^{(n)})\}_{k=1}^{k=K}\) by using Eqs.9 and 10, and perform the negative sampling by using Eq.11. For clarity, the full training process is outlined in Appendix B.

## 4 Experiments

### Experimental Settings

**Datasets.** We employ 5 widely used MLL datasets, including image datasets Pascal VOC-2012 (VOC) [27], MS-COCO2014 (COCO) [28] and Animals with Attributes2 (AWA) [29], text datasets Ohsumed [30] and AAPD [31]. For clarity, the detailed characteristics of these datasets are displayed in Table 1. Following [16], we transform these datasets into SSL versions. For each dataset, we randomly select \(\pi\) training samples as labeled ones, and the remaining as unlabeled ones. We set \(\pi\in\{5\%,10\%,15\%,20\%\}\), to explore the performance of our method under different data proportions. The image size is resized to 224 for all datasets.

**Baselines.** We employ 5 baseline methods for comparisons, including SoftMatch [32], FlatMatch [33], MIME [34], DRML [15], and CAP [16]. DRML and CAP are SSMLL methods; SoftMatch and FlatMatch are SSL methods; MIME is a single-positive multi-label learning (SPMLL) method. For SSL and SPMLL methods, we follow CAP to apply them to SSMLL tasks.

**Evaluation metrics.** We employ 5 evaluation metrics, including Micro-F1, Macro-F1, mean average precision (mAP), Hamming Loss and One Loss [1], and compute them with the Scikit-Learn tool.2

Footnote 2: [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)

**Implementation details.** We use the pre-trained ResNet-50 [35] as the backbone for image datasets and BERT-base-uncased model [36] for text datasets. We set the decay of EMA as 0.9997. The batch size is 32 for VOC, 128 for AWA and 64 for COCO, Ohsumed and AAPD. The warm-up epoch \(T_{0}\) is 12. The \(s\) and \(m\) are 20 and 0.4 in VOC, 20 and 0.3 in COCO, 10 and 0.2 in AWA, Ohsumed and AAPD. The parameters for negative sampling \(\eta\) are set to 5.

### Results

The experimental results are presented in Table 2 and Table 3. Overall, our method achieves good performance on all metrics. Our model ranks _1st_ on average on five datasets and has a significant advantage over baselines. The detailed analyses are presented as follows.

**Comparing with SSMLL methods:** We can observe that \(\text{S}^{2}\text{ml}^{2}\)-bham has advantages over recent SSMLL methods. Especially in the Micro-F1 and Macro-F1, our method has significant improvement. On both VOC and COCO, our F1 and mAP values increase by an average of 0.1 and 0.01. Furthermore, on Ohsumed and AAPD, we surprised to discover from the results that our method also has good results. In all data proportions, the average improvement on the mAP is 0.11, 0.14 on Macro-F1 and 0.19 on Micro-F1. This result is foreseeable because our method balanced angle variance using

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Dataset & \#Training & \#Testing & \#Classes & \#Avg. Positive Classes \\ \hline VOC & 5,717 & 5,823 & 20 & 1.46 \\ COCO & 82,081 & 40,137 & 80 & 2.94 \\ AWA & 30,337 & 6,985 & 85 & 30.78 \\ Ohsumed & 22,054 & 10,300 & 23 & 1.65 \\ AAPD & 53,840 & 1,000 & 54 & 2.41 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of the dataset statistics

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

domain adaptation strategies. At the same time, how to effectively align pseudo labels with real labels is also an important issue. CAP [16] developed a class-distribution-aware thresholding strategy to control the assignment of positive and negative pseudo-labels. However, the current SSMLL methods have not paid attention to the **variance bias** problem, which affects the performance of the methods.

**MLL methods.** MLL has multiple research directions. Some methods focus on the model structure. For instance, [38] proposed a graph convolutional networks model to improve the performance of multi-label image recognition. [4] proposes a unified framework that combines CNNs and RNNs. Some others focus on exploiting label correlations to improve performance. LSF-CI[39] calculates instance correlation in the feature space and label correlation in the label space through a probabilistic neighborhood graph model and cosine similarity. Due to the complete label information of the training samples, the MLL method can theoretically achieve Bayesian optimal classifier boundaries. However, in semi supervised learning, incorrect pseudo labels may provide incorrect guidance for classification boundaries.

**SSL methods.** Pseudo Label [40] is one of the earliest semi-supervised learning methods for neural networks. It generates pseudo labels for unlabeled data and continuously improves the accuracy of pseudo labels as the model is optimized. As data augmentation technology has advanced, an increasing number of SSL methods are incorporating this technology [41, 42, 43, 44, 45, 46]. Further research has been conducted on the threshold issue of pseudo labels in [47, 48, 49]. By developing dynamic threshold strategies, they have been able to obtain more accurate pseudo labels, effectively enhancing the performance of the SSL methods. In order to utilize pseudo labels with low confidence but correct classification, [32] proposes an effective method that fits the confidence distribution of truncated Gaussian functions. Moreover, [33] discovered that the generalization ability of SSL models is impacted by disconnection between labeled data and unlabeled data, and proposed the FlatMatch method to address this issue. However, it's important to note that these SSL methods are designed to handle multi-class single-label tasks [50, 51] and cannot be directly applied to multi-label learning scenarios.

## 6 Conclusion

In this paper, we proposed a novel SSMLL method, namely S\({}^{2}\)ML\({}^{2}\)-bbam. Our S\({}^{2}\)ML\({}^{2}\)-bbam balances the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. To achieve this, we design a novel balanced binary angular margin loss by extending the traditional binary angular margin loss with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. Empirical results demonstrate that our S\({}^{2}\)ML\({}^{2}\)-bbam outperforms current SSMLL baseline methods.

## Limitations

From the empirical results, we found that S\({}^{2}\)ml\({}^{2}\)-bbam suffers from slightly lower mAP scores on the benchmarks VOC and COOC when increasing the proportion of labeled training samples. This may restrict the range of applications and scenarios in which S\({}^{2}\)ml\({}^{2}\)-bbam can be effectively used. And we will further exploit it in our future works.

## Broader Impacts

The paper focuses solely on the technical aspects of SSMLL algorithms. Therefore, this work can benefit a wide range of machine learning researchers. Also, we do not expect our efforts to have any negative consequences.

## Acknowledgements

We would like to acknowledge support for this project from the National Science and Technology Major Project of China (No.2021ZD0112500), the National Natural Science Foundation of China (No.62276113), and China Postdoctoral Science Foundation (No.2022M721321).

## References

* [1] Zhang, M., Z. Zhou. A review on multi-label learning algorithms. _IEEE TKDE_, 26(8):1819-1837, 2014.
* [2] Fujino, A., H. Isozaki, J. Suzuki. Multi-label text categorization with model combination based on f1-score maximization. In _IJCNLP_, pages 823-828. 2008.
* [3] Wu, H., S. Qin, R. Nie, et al. Effective collaborative representation learning for multilabel text categorization. _IEEE TNNLS_, 33(10):5200-5214, 2021.
* [4] Wang, J., Y. Yang, J. Mao, et al. Cnn-rnn: A unified framework for multi-label image classification. In _CVPR_, pages 2285-2294. 2016.
* [5] Lanchantin, J., T. Wang, V. Ordonez, et al. General multi-label image classification with transformers. In _CVPR_, pages 16478-16488. 2021.
* [6] Zhao, F., Y. Huang, L. Wang, et al. Deep semantic ranking based hashing for multi-label image retrieval. In _CVPR_, pages 1556-1564. 2015.
* [7] Lai, H., P. Yan, X. Shu, et al. Instance-aware hashing for multi-label image retrieval. _IEEE TIP_, 25(6):2469-2479, 2016.
* [8] Zhang, D., S. Zhao, Z. Duan, et al. A multi-label classification method using a hierarchical and transparent representation for paper-reviewer recommendation. _ACM TOIS_, 38(1):1-20, 2020.
* [9] Izadi, M., A. Heydarnoori, G. Gousios. Topic recommendation for software repositories using multi-label classification algorithms. _Empirical Software Engineering_, 26(5):93, 2021.
* [10] Zhu, F., H. Li, W. Ouyang, et al. Learning spatial regularization with image-level supervisions for multi-label image classification. In _CVPR_, pages 5513-5522. 2017.
* [11] Guo, H., K. Zheng, X. Fan, et al. Visual attention consistency under image transforms for multi-label image classification. In _CVPR_, pages 729-739. 2019.
* [12] Wang, B., Z. Tu, J. K. Tsotsos. Dynamic label propagation for semi-supervised multi-class multi-label classification. In _ICCV_, pages 425-432. 2013.
* [13] Zhao, F., Y. Guo. Semi-supervised multi-label learning with incomplete labels. In _IJCAI_, pages 4062-4068. 2015.
* [14] Zhan, W., M. Zhang. Inductive semi-supervised multi-label learning with co-training. In _SIGKDD_, pages 1305-1314. 2017.
* [15] Wang, L., Y. Liu, C. Qin, et al. Dual relation semi-supervised multi-label learning. In _AAAI_, pages 6227-6234. 2020.
* [16] Xie, M.-K., J.-H. Xiao, H.-Z. Liu, et al. Class-distribution-aware pseudo-labeling for semi-supervised multi-label learning. In _NeurIPS_. 2023.
* [17] Zaitian, W., W. Pengfei, L. Kunpeng, et al. A comprehensive survey on data augmentation. _arXiv preprint arXiv:2405.09591_, 2024.
* [18] Cole, E., O. M. Aodha, T. Lorieul, et al. Multi-label learning from single positive labels. In _CVPR_, pages 933-942. 2021.
* [19] Baruch, E. B., T. Ridnik, I. Friedman, et al. Multi-label classification with partial annotations using class-aware selective loss. In _CVPR_, pages 4764-4772. 2022.
* [20] Ridnik, T., E. B. Baruch, N. Zamir, et al. Asymmetric loss for multi-label classification. In _ICCV_, pages 82-91. 2021.
* [21] Jiang, T., D. Wang, L. Sun, et al. Lightxml: Transformer with dynamic negative sampling for high-performance extreme multi-label text classification. In _AAAI_, pages 7987-7994. 2021.
* [22] Dahiya, K., D. Saini, A. Mittal, et al. Deepxml: A deep extreme multi-label learning framework applied to short text documents. In _WSDM_, pages 31-39. 2021.
* [23] Qaraei, M., R. Babbar. Meta-classifier free negative sampling for extreme multilabel classification. _Machine Learning_, pages 1-23, 2023.
* [24] Ma, X., Z. Wang, W. Liu. On the tradeoff between robustness and fairness. In _NeurIPS_. 2022.
* [25] Caton, S., C. Haas. Fairness in machine learning: A survey. _ACM Computing Surveys_, 56(7):166:1-166:38, 2024.

* [26] Mehrabi, N., F. Morstatter, N. Saxena, et al. A survey on bias and fairness in machine learning. _ACM Computing Surveys_, 54(6):115:1-115:35, 2022.
* [27] Everingham, M., S. M. A. Eslami, L. V. Gool, et al. The pascal visual object classes challenge: A retrospective. _IJCV_, 111(1):98-136, 2015.
* [28] Lin, T., M. Maire, S. J. Belongie, et al. Microsoft coco: Common objects in context. In _ECCV_, pages 740-755. 2014.
* [29] Lampert, C. H., H. Nickisch, S. Harmeling. Attribute-based classification for zero-shot visual object categorization. _IEEE TPAMI_, 36(3):453-465, 2013.
* [30] Moschitti, A., R. Basili. Complex linguistic features for text classification: A comprehensive study. In _ECIR_, pages 181-196. 2004.
* [31] Yang, P., X. Sun, W. Li, et al. Sgm: Sequence generation model for multi-label classification. In _COLING_, pages 3915-3926. 2018.
* [32] Chen, H., R. Tao, Y. Fan, et al. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. _ICLR_, 2023.
* [33] Huang, Z., L. Shen, J. Yu, et al. Flatmatch: Bridging labeled data and unlabeled data with cross-sharpness for semi-supervised learning. In _NeurIPS_. 2023.
* [34] Liu, B., N. Xu, J. Lv, et al. Revisiting pseudo-label for single-positive multi-label learning. In _ICML_, pages 22249-22265. 2023.
* [35] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In _CVPR_, pages 770-778. 2016.
* [36] Devlin, J., M. Chang, K. Lee, et al. BERT: pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, pages 4171-4186. 2019.
* [37] Tan, Q., Y. Yu, G. Yu, et al. Semi-supervised multi-label classification using incomplete label information. _Neurocomputing_, 260:192-202, 2017.
* [38] Chen, Z., X. Wei, P. Wang, et al. Multi-label image recognition with graph convolutional networks. In _CVPR_, pages 5177-5186. 2019.
* [39] Zhang, J., C. Li, D. Cao, et al. Multi-label learning with label-specific features by resolving label correlations. _KBS_, 159:148-157, 2018.
* [40] Dong-Hyun, L., et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _ICML Workshop_, page 896. 2013.
* [41] Berthelot, D., N. Carlini, I. J. Goodfellow, et al. Mixmatch: A holistic approach to semi-supervised learning. In _NeurIPS_, pages 5050-5060. 2019.
* [42] Zhang, H., M. Cisse, Y. N. Dauphin, et al. mixup: Beyond empirical risk minimization. In _ICLR_. 2018.
* [43] Berthelot, D., N. Carlini, E. D. Cubuk, et al. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. _ICLR_, 2020.
* [44] Sohn, K., D. Berthelot, N. Carlini, et al. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _NeurIPS_, pages 596-608. 2020.
* [45] Li, C., X. Li, L. Feng, et al. Who is your right mixup partner in positive and unlabeled learning. In _ICLR_. 2022.
* [46] Li, C., Y. Dai, L. Feng, et al. Positive and unlabeled learning with controlled probability boundary fence. In _ICML_. 2024.
* [47] Xu, Y., L. Shang, J. Ye, et al. Dash: Semi-supervised learning with dynamic thresholding. In _ICML_, pages 11525-11536. 2021.
* [48] Guo, L., Y. Li. Class-imbalanced semi-supervised learning with adaptive thresholding. In _ICML_, pages 8082-8094. 2022.
* [49] Wang, Y., H. Chen, Q. Heng, et al. Freematch: Self-adaptive thresholding for semi-supervised learning. _ICLR_, 2023.
* [50] Li, C., X. Li, J. Ouyang. Semi-supervised text classification with balanced deep representation distributions. In _ACL-IJCNLP_, pages 5044-5053. 2021.

* [51] Li, X., Y. Jiang, C. Li, et al. Learning with partial labels from semi-supervised perspective. In _AAAI_, pages 8666-8674. 2023.
* [52] Xu, H., X. Liu, Y. Li, et al. To be robust or to be fair: Towards fairness in adversarial training. In _ICML_, pages 11492-11501. 2021.

## Appendix A Proof of Theoretical Results

Proof.: Proof of Theorem 2.1. For any linear classifier \(f(\mathbf{x})=\text{sign}(\langle\mathbf{w},\mathbf{x}\rangle+b)\), we first calculate its risk:

\[\mathcal{R}_{ssl}(f)\] \[=\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{D}}[\mathds{1}(f(\mathbf{x })\neq y)]\] \[\propto\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{D}^{*}}[\mathds{1}(f (\mathbf{x})\neq y)]+(1-\epsilon_{-}-\epsilon_{+})\mathbb{E}_{(\mathbf{x},y) \sim\mathcal{D}^{*}}[\mathds{1}(f(\mathbf{x})\neq y)]+\] \[\quad\epsilon_{-}\mathbb{E}_{(\mathbf{x},-1)\sim\mathcal{D}^{*}}[ \mathds{1}(f(\mathbf{x})\neq+1)]+\epsilon_{+}\mathbb{E}_{(\mathbf{x},+1)\sim \mathcal{D}^{*}}[\mathds{1}(f(\mathbf{x})\neq-1)]\] \[=(2-\epsilon_{-}-\epsilon_{+})\cdot\big{(}\mathbb{P}_{(\mathbf{x },y)\sim\mathcal{D}^{*}}[f(\mathbf{x})\neq y]+\epsilon_{-}\mathbb{P}_{(\mathbf{ x},-1)\sim\mathcal{D}^{*}}[f(\mathbf{x})\neq+1]+\epsilon_{+}\mathbb{P}_{( \mathbf{x},+1)\sim\mathcal{D}^{*}}[f(\mathbf{x})\neq-1]\] \[=(2-\epsilon_{-}-\epsilon_{+})\cdot\big{(}\mathbb{P}[y=+1]\cdot \mathbb{P}[f(\mathbf{x})=-1|y=+1]+\mathbb{P}[y=-1]\cdot\mathbb{P}[f(\mathbf{x}) =+1|y=-1]\big{)}+\] \[\quad\epsilon_{-}\mathbb{P}[y=-1]\cdot\mathbb{P}[f(\mathbf{x})=-1 |y=-1]+\epsilon_{+}\mathbb{P}[y=+1]\cdot\mathbb{P}[f(\mathbf{x})=+1|y=+1]\] \[=(2-\epsilon_{-}-\epsilon_{+})\cdot\alpha\cdot\mathcal{R}(f,+1)+( 2-\epsilon_{-}-\epsilon_{+})\cdot(1-\alpha)\cdot\mathcal{R}(f,-1)+\] \[\quad\epsilon_{-}\cdot(1-\alpha)\cdot\mathbb{P}[f(\mathbf{x})=-1|y =-1]+\epsilon_{+}\cdot\alpha\cdot\mathbb{P}[f(\mathbf{x})=+1|y=+1]\]

where \(\alpha=\mathbb{P}[y=+1]\), \(\epsilon_{+}=\mathbb{P}[\widehat{y}=-1|y=+1]\) and \(\epsilon_{-}=\mathbb{P}[\widehat{y}=+1|y=-1]\).

Denote \(\mathbf{x}=[x_{1},\cdots,x_{d}]^{\top}\) and \(\mathbf{w}=[w_{1},\cdots,w_{d}]^{\top}\), we can explicitly calculate \(\mathcal{R}(f,+1)\) and \(\mathbb{P}[f(\mathbf{x})=+1|y=+1]\) as:

\[\mathcal{R}(f,+1)=\mathbb{P}[f(\mathbf{x})=-1|y=+1]=\mathbb{P}[\langle\mathbf{ w},\mathbf{x}\rangle+b<0|y=+1]=\mathbb{P}[\sum_{i=1}^{d}w_{i}x_{i}+b<0]\]

\[\mathbb{P}[f(\mathbf{x})=+1|y=+1]=\mathbb{P}[\langle\mathbf{w},\mathbf{x} \rangle+b>0|y=+1\[\mathbb{P}[f_{ssl}(\mathbf{x})=-1|y=-1]=\mathbb{P}[\sum_{i=1}^{d}x_{i} +b^{*}<0] =\mathbb{P}\bigg{[}\frac{\sum_{i=1}^{d}(x_{i}-(-\mu_{i}))}{\sqrt{\sum_{i=1}^{ i=d}(\sigma_{-}^{(i)})^{2}}}<\frac{-b^{*}+\sum_{i=1}^{d}\mu_{i}}{\sqrt{\sum_{i=1}^{ i=d}(\sigma_{-}^{(i)})^{2}}}\bigg{]}\] \[=\Phi\bigg{(}\frac{-b^{*}+\mu}{M\Sigma}\bigg{)}\]

where \(\Phi\) is c.d.f. of normal Gaussian distribution \(\mathcal{N}(0,1)\). Then, we get

\[\mathcal{R}_{ssl}(f_{ssl})= \alpha(2-\epsilon_{-}-\epsilon_{+})\Phi\bigg{(}-\frac{b^{*}+\mu}{ \Sigma}\bigg{)}+(1-\alpha)(2-\epsilon_{-}-\epsilon_{+})\Phi\bigg{(}\frac{b^{* }-\mu}{M\Sigma}\bigg{)}+\] \[(1-\alpha)\epsilon_{-}\Phi\bigg{(}\frac{-b^{*}+\mu}{M\Sigma} \bigg{)}+\alpha\epsilon_{+}\Phi\bigg{(}\frac{b^{*}+\mu}{\Sigma}\bigg{)}\]

We will find the optimal \(b^{*}\) which minimizes the overall standard classification error \(\mathcal{R}_{ssl}(f_{ssl})\) by taking \(\frac{d\mathcal{R}_{ssl}(f_{ssl})}{db^{*}}=0\). In detail, it is:

\[\frac{d\mathcal{R}_{ssl}(f_{ssl})}{db^{*}}= \alpha(2-\epsilon_{-}-\epsilon_{+})\frac{1}{\sqrt{2\pi}}\exp(- \frac{1}{2}(\frac{b^{*}+\mu}{\Sigma})^{2})\frac{-1}{\Sigma}+\] \[(1-\alpha)(2-\epsilon_{-}-\epsilon_{+})\frac{1}{\sqrt{2\pi}}\exp( -\frac{1}{2}(\frac{b^{*}-\mu}{M\Sigma})^{2})\frac{1}{M\Sigma}+\] \[(1-\alpha)\epsilon_{-}\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}(\frac {b^{*}-\mu}{M\Sigma})^{2})\frac{-1}{M\Sigma}+\alpha\epsilon_{+}\frac{1}{\sqrt{2 \pi}}\exp(-\frac{1}{2}(\frac{b^{*}+\mu}{\Sigma})^{2})\frac{1}{\Sigma}=0\]

which can be reformulated as:

\[(\frac{b^{*}+\mu}{\Sigma})^{2}-(\frac{b^{*}-\mu}{M\Sigma})^{2}=2\log\bigg{(} \frac{M\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2\epsilon_{-}- \epsilon_{+})}\bigg{)}\]

Denote \(B=\log\bigg{(}\frac{M\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2 \epsilon_{-}-\epsilon_{+})}\bigg{)}\). Without loss of generality, we assume \(B>0\) and obtain:

\[(M^{2}-1)\Sigma^{2}(b^{*})^{2}+2(M^{2}+1)\mu\Sigma^{2}b^{*}+(M^{2}-1)\Sigma^{2 }\mu^{2}=2BM^{2}\Sigma^{4}. \tag{12}\]

Consequently, \(b^{*}\) can be given by selecting the smaller absolute value:

\[b^{*}=\begin{cases}\frac{-(M^{2}+1)\mu+2M\mu\sqrt{1+B\frac{(M^{2}-1)\Sigma^{2 }}{2\mu^{2}}}}{M^{2}-1}&\text{if }M>1,\\ \\ \frac{-(M^{2}+1)\mu-2M\mu\sqrt{1+B\frac{(M^{2}-1)\Sigma^{2}}{2\mu^{2}}}}{M^{2}- 1}&\text{if }M<1,\end{cases}\]

Then when \(M>1\), the class-wise standard classification errors are:

\[\mathcal{R}(f_{ssl},+1) =\Phi\big{(}A-M\sqrt{A^{2}+q(M,\alpha,\epsilon_{-},\epsilon_{+}) }\big{)},\] \[\mathcal{R}(f_{ssl},-1) =\Phi\big{(}-M\cdot A+\sqrt{A^{2}+q(M,\alpha,\epsilon_{-}, \epsilon_{+})}\big{)},\]

when \(M<1\), they are given by:

\[\mathcal{R}(f_{ssl},+1) =\Phi\big{(}A+M\sqrt{A^{2}+q(M,\alpha,\epsilon_{-},\epsilon_{+}) }\big{)},\] \[\mathcal{R}(f_{ssl},-1) =\Phi\big{(}-M\cdot A-\sqrt{A^{2}+q(M,\alpha,\epsilon_{-}, \epsilon_{+})}\big{)},\]

where

\[A=\frac{2\mu}{(M^{2}-1)\Sigma},\quad q(M,\alpha,\epsilon_{-},\epsilon_{+})= \frac{2\log\frac{M\alpha(2-\epsilon_{-},-2\epsilon_{+})}{(1-\alpha)(2-2 \epsilon_{-}-\epsilon_{+})}}{M^{2}-1}.\]

When \(\sum_{i=1}^{d}(\sigma_{+In this case, \(b^{*}\) can be expressed as follows:

\[b^{*}=\frac{\log\bigl{(}\frac{\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2 \epsilon_{-}-\epsilon_{+})}\bigr{)}\Sigma^{2}}{2\mu},\]

and corresponding class-wise standard classification errors are given by:

\[\mathcal{R}(f_{ssl},+1) =\Phi\biggl{(}\frac{-2\mu^{2}-\log\bigl{(}\frac{\alpha(2-\epsilon _{-}-2\epsilon_{+})}{(1-\alpha)(2-2\epsilon_{-}-\epsilon_{+})}\bigr{)}\Sigma^{ 2}}{2\mu\Sigma}\biggr{)},\] \[\mathcal{R}(f_{ssl},-1) =\Phi\biggl{(}\frac{-2\mu^{2}+\log\bigl{(}\frac{\alpha(2-\epsilon _{-}-2\epsilon_{+})}{(1-\alpha)(2-2\epsilon_{-}-\epsilon_{+})}\bigr{)}\Sigma^ {2}}{2\mu\Sigma}\biggr{)}.\]

Proof.: Proof of Theorem 2.3. According to the results of Theorem 2.1, we can formulate the class-wise accuracy as:

\[p(+1)=1-\mathcal{R}(f_{ssl},+1),\quad p(-1)=1-\mathcal{R}(f_{ssl},-1).\]

Accordingly, the variance of class-wise accuracy can be expressed as:

\[VCA(f_{ssl}) =\text{Var}(p(+1),p(-1))=\text{Var}(1-\mathcal{R}(f_{ssl},+1),1- \mathcal{R}(f_{ssl},-1))\] \[=\text{Var}(\mathcal{R}(f_{ssl},+1),\mathcal{R}(f_{ssl},-1))\] \[=\frac{(\mathcal{R}(f_{ssl},+1)-\mathcal{R}(f_{ssl},-1))^{2}}{2}.\]

For convenience, we assume \(\log\bigl{(}\frac{\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2\epsilon _{-}-\epsilon_{+})}\bigr{)}=0\), and the conclusion will also hold when \(M>\max(\frac{\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2\epsilon_{- }-\epsilon_{+})},1)\) and \(M<\min(\frac{\alpha(2-\epsilon_{-}-2\epsilon_{+})}{(1-\alpha)(2-2\epsilon_{-} -\epsilon_{+})},1)\). When \(M>1\), it has \(\mathcal{R}(f_{ssl},-1)>\mathcal{R}(f_{ssl},+1)\) because \(q(M,\alpha,\epsilon_{-},\epsilon_{+})>0\) and \(A>0\). Then according to Lagrange's Mean Value Theorem, there exists some \(\xi\) such that

\[\mathcal{R}(f_{ssl},-1)-\mathcal{R}(f_{ssl},+1)\] \[=\Phi\bigl{(}-M\cdot A+\sqrt{A^{2}+q(M,\alpha,\epsilon_{-}, \epsilon_{+})}\bigr{)}-\Phi\bigl{(}A-M\sqrt{A^{2}+q(M,\alpha,\epsilon_{-}, \epsilon_{+})}\bigr{)}\] \[=\Phi^{\prime}(\xi)\bigl{(}-M\cdot A+\sqrt{A^{2}+q(M,\alpha, \epsilon_{-},\epsilon_{+})}-A+M\sqrt{A^{2}+q(M,\alpha,\epsilon_{-},\epsilon_ {+})}\bigr{)}\] \[=\frac{1}{\sqrt{2\pi}}\exp(-\frac{\xi^{2}}{2})(M+1)\bigl{(}\sqrt{ A^{2}+q(M,\alpha,\epsilon_{-},\epsilon_{+})}-A\bigr{)}.\]

By analyzing the variation of \(q(M,\alpha,\epsilon_{-},\epsilon_{+})\), we can easily verify that \(\mathcal{R}(f_{ssl},-1)-\mathcal{R}(f_{ssl},+1)\) is increasing when \(M\to\infty\). Similarly, we can prove that \(\mathcal{R}(f_{ssl},+1)>\mathcal{R}(f_{ssl},-1)\) when \(M<1\) and \(\mathcal{R}(f_{ssl},+1)-\mathcal{R}(f_{ssl},-1)\) is increasing when \(M\to 0\).

The training procedure of the model

The _Algorithm_1 provides a detailed description of the training process of the model.

```
0:
1:\(\mathcal{D}_{l}\): the labeled training dataset
2:\(\mathcal{D}_{u}\): the unlabeled training dataset
3:\(T_{0},T_{t}\): the number of warm-up epochs, the number of SSMLL training epochs
4:\(B_{u}\): the number of unlabeled batch size ;
5:the classifier \(f_{\mathbf{W}}(\cdot)\).
6:Initialize the classifier parameter \(\mathbf{W}\);
7:Warm-up \(f_{\mathbf{W}}(\cdot)\) on \(\mathcal{D}_{l}\) with BAM loss Eq.(6) by \(T_{0}\) epochs;
8:for\(t=1\)to\(T_{t}\)do
9: Calculate pseudo-labels \(\{\mathbf{y}_{i}^{u}\}_{i=1}^{i=N_{u}}\) of \(\mathcal{D}_{u}\) with Eq.(5);
10: Estimate \(\{\mathbf{c}_{k}\}_{k=1}^{k=K}\), \(\{(\mu_{k}^{(p)},(\sigma_{k}^{2})^{(p)})\}_{k=1}^{k=K}\) and \(\{(\mu_{k}^{(n)},(\sigma_{k}^{2})^{(n)})\}_{k=1}^{k=K}\) with Eqs.(9) and (10);
11: Construct \(\{\Omega_{k}\}_{k=1}^{k=K}\) with Eq.(11);
12:for\(i=1\)to\(|\mathcal{D}_{u}|/B_{u}\)do
13: Optimize \(f_{\mathbf{W}}(\cdot)\) by minimizing the objective Eq.(4) with \(\mathcal{D}_{l}\), \(\mathcal{D}_{u}\), \(\{\mathbf{y}_{i}^{u}\}_{i=1}^{i=N_{u}}\) and \(\{\Omega_{k}\}_{k=1}^{k=K}\);
14:endfor
```

**Algorithm 1** Training Procedure of S\({}^{2}\)ml\({}^{2}\)-bham

## Appendix C Time cost comparison

To examine the efficiency of S\({}^{2}\)ml\({}^{2}\)-bham, we perform efficiency comparisons over our S\({}^{2}\)ml\({}^{2}\)-bham, SSL baselines (SoftMatch and FlatMatch) and SSMLL baselines (DRML and CAP) on _VOC_ and _COCO_. Table 5 shows the running time averaged 100 epochs. From Table 5, it can be seen that our method is competitive with the current SSMLL methods in the time efficiency and costs less time than the SSL baselines in practice.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please refer to 3 Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: The code will be submitted later.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper specify all the training and test details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars were too small to have any visual impact. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have used a single NVIDIA GeForce RTX 3090 GPU. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No such risks Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.