# Transformers learn through gradual rank increase

Enric Boix-Adsera\({}^{*}\)\({}^{1,2}\) Etai Littwin\({}^{*}\)\({}^{1}\)

Emmanuel Abbe\({}^{1,3}\) Samy Bengio\({}^{1}\) Joshua Susskind\({}^{1}\)

\({}^{1}\)Apple \({}^{2}\)MIT \({}^{3}\)EPFL

eboix@mit.edu,emmanuel.abbe@epfl.ch

{elittwin,bengio,jsusskind}@apple.com

###### Abstract

We identify incremental learning dynamics in transformers, where the difference between trained and initial weights progressively increases in rank. We rigorously prove this occurs under the simplifying assumptions of diagonal weight matrices and small initialization. Our experiments support the theory and also show that phenomenon can occur in practice without the simplifying assumptions.

## 1 Introduction

The transformer architecture achieves state of the art performance in various domains, yet we still lack a solid theoretical understanding of its training dynamics [17, 1, 19, 20]. Nevertheless, the theoretical toolbox has matured over the last years and there are promising new approaches. One important line of work examines the role that initialization scale plays on the trajectory taken by gradient descent [1, 18, 19, 20, 17, 21, 22]. When the weights are initialized small, it has been shown for simple networks that an _incremental learning_ behaviour occurs, where functions of increasing complexity are learned in stages. This regime is known to be richer than the large-initialization regime1, but the incremental learning dynamics are difficult to analyze, and are so far understood only for extremely simple architectures. Can we apply this analysis to transformers? Namely:

Footnote 1: In the large-initialization regime, deep learning behaves as a kernel method [1, 18]. Various separations with kernels are known for smaller initialization: e.g., [1, 17].

_Are there incremental learning dynamics when training a transformer architecture?_

An obstacle is that past work on incremental learning has mainly studied linear networks [1, 1, 2, 3, 4, 5, 6, 7, 8, 9], with one paper studying nonlinear 2-layer fully-connected networks [1]. In contrast, transformers have nonlinear attention heads that do not fall under previous analyses: given \(\bm{X}\in\mathbb{R}^{n\times d}\), an attention head computes

\[\mathrm{attention}(\bm{X};\bm{W}_{K},\bm{W}_{Q},\bm{W}_{V},\bm{W}_{O})=\mathrm{ smax}(\bm{X}\bm{W}_{K}\bm{W}_{Q}^{\top}\bm{X}^{\top})\bm{X}\bm{W}_{V}\bm{W}_{O}^{\top}\] (1)

where \(\bm{W}_{K},\bm{W}_{Q},\bm{W}_{V},\bm{W}_{O}\in\mathbb{R}^{d\times d^{\prime}}\) are trainable matrices, and the softmax is applied row-wise. A transformer is even more complex, since it is formed by stacking alternating layers of attention heads and feedforward networks, along with residual connections.

Main findingOur main finding is that transformers exhibit incremental learning dynamics, where _the difference between the trained and initial weights incrementally increases in rank_. Our results have a theoretical component and an experimental component.

Theoretical contributionsFor our theory, we study a simplification of the transformer architecture, where the attention head weights are diagonal matrices: i.e., in each attention head we have \(\bm{W}_{K}=\mathrm{diag}(\bm{w}_{K})\), where \(\bm{w}_{K}\in\mathbb{R}^{d}\) are trainable weights, and similarly for \(\bm{W}_{Q},\bm{W}_{V}\) and \(\bm{W}_{O}\). We rigorously establish the training dynamics of this architecture under gradient flow when the initialization is small. We prove that dynamics occur in discrete stages: (1) during most of each stage, the loss plateaus because the weights remain close to a saddle point, and (2) at the end, the saddle point is quickly escaped and the rank of the weights increases by at most one.

This theoretical result on transformers follows from a general theorem characterizing the learning dynamics of networks \(f_{\mathsf{NN}}\) that depend on the product of parameters \(\bm{u},\bm{v}\in\mathbb{R}^{p}\) as

\[f_{\mathsf{NN}}(\bm{x};\bm{u},\bm{v})=h(\bm{x};\bm{u}\odot\bm{v})\,,\] (2)

where \(\bm{x}\) is the input, \(\odot\) denotes the elementwise product, and \(h\) is a smooth function.

**Theorem 1.1** (Informal statement of incremental learning dynamics).: _Let \(f_{\mathsf{NN}}\) be a network of the form (2), and suppose that the weights are initialized very small: i.e., the entries of \(\bm{u},\bm{v}\) are initialized on the order \(\Theta(\alpha)\) for some small \(\alpha>0\). Then the dynamics of gradient flow training effectively proceeds in discrete stages, each one lasting time \(\Theta(\log(1/\alpha))\). In each stage, the number of nonnegligible entries of \(\bm{u}\odot\bm{v}\) increases by at most one._

A transformer with diagonal weight matrices falls under this result when we only train the attention head weights. For example, if the transformer has one attention head, then we can take \(\bm{u}=[\bm{w}_{K},\bm{w}_{V}]\in\mathbb{R}^{2d}\) and \(\bm{v}=[\bm{w}_{Q},\bm{w}_{Q}]\in\mathbb{R}^{2d}\) to be concatenations of the diagonal entries of the weights of the head; see Example 3.2 for more details and the extension to transformers with many heads. Then, using Theorem 1.1, we see that in each stage either \(\bm{W}_{K}\bm{W}_{Q}^{\top}=\mathrm{diag}(\bm{w}_{K})\mathrm{diag}(\bm{w}_{Q})\) or \(\bm{W}_{V}\bm{W}_{O}^{\top}=\mathrm{diag}(\bm{w}_{V})\mathrm{diag}(\bm{w}_{O})\) increases in effective rank by at most one.2

Footnote 2: We also remark that Theorem 1.1 is interesting in its own right and may have other applications beyond transformers. It qualitatively recovers the incremental dynamics result of [1, 2] when specialized to linear diagonal networks, i.e., when \(f_{\mathsf{NN}}(\bm{x};\bm{u},\bm{v})=\sum_{i=1}^{p}u_{i}v_{i}x_{i}\).

Experimental contributionsIn our experiments, we first validate our theoretical results, which require the simplifying assumptions of small initialization and diagonal weight matrices.

Then, we conduct experiments on vision and language transformers in settings closer to practice, without any of the assumptions required by our theoretical analysis. Perhaps surprisingly, we again observe incremental learning dynamics, even though the assumptions of the theory are not met. The difference between trained and initial weights has low rank, and the rank of this difference grows gradually during training; see Figure 1. The incremental nature of the dynamics is easier to see for ImageNet, since for CIFAR-10 the rank of the weight difference does not grow as much.

### Related work

Relation to LoRAWe note an intriguing connection to the LoRA algorithm, where a pretrained base model is cheaply fine-tuned by training a low-rank perturbation of the weights [1, 1, 2]. The method is surprisingly powerful, and recently LoRA has been fundamental to allowing the open-source community to inexpensively fine-tune language models [1, 2]. On the other hand, in our work we observe that the trained weights are a low-rank perturbation of the initial weights due to the training dynamics, without having to apply an explicit rank constraint as in LoRA. This raises an exciting open question for future work: _can we explain and improve algorithms like LoRA by better understanding and quantifying the incremental dynamics of large transformers?_

Figure 1: For an attention head in ViT trained on (a) CIFAR-10, and (b) ImageNet, we plot the normalized spectra of \(\bm{W}_{K}\bm{W}_{Q}^{\top}\) at initialization (in red), and of the learned perturbations to \(\bm{W}_{K}\bm{W}_{Q}^{\top}\) at different iterations (in green).

Low-rank bias in nonlinear networksFor 2-layer networks, it is known that low-rank bias in the weights emerges if the target function depends on a low-dimensional subspace of the input [1, 2, 3, 4]. The results of [1, 2] are especially relevant, since they show that the rank of the weights increases in a sequential manner, determined by the "leap complexity" of the target function, which is reminiscent of our empirical observations on transformers. See also [2, 3] for more investigations of low-rank bias in 2-layer networks under different assumptions. For transformers, [15] report that empirically the trained weights (using default initialization) are not low-rank. This is consistent with our claim that the difference between initial and trained weights is low-rank, since the initial weights might not be low-rank.

Incremental learning dynamicsSeveral works prove incremental learning behaviour in deep _linear_ networks when the initialization is small. [1] has shown that gradient descent dynamics on a 2-layer linear network with \(L_{2}\) loss effectively solve a reduced-rank regression problem with gradually increasing rank. [1] prove a dynamical depth separation result, allowing for milder assumptions on initialization scale. [1, 2] show implicit bias towards low rank in deep matrix and tensor factorization. [16] show deep matrix factorization dynamics with small initialization are equivalent to a greedy low-rank learning (GLRL) algorithm. And [17] independently provides a similar description of the dynamics, but without requiring balanced initialization. Finally, [1, 2, 3] overcome a technical hurdle from previous analyses by proving incremental learning for the entire training trajectory, rather than just the first stage. In contrast to our result, these prior works apply only to _linear_ networks with certain convex losses, whereas our result applies to _nonlinear_ networks. In order to make our extension to nonlinear networks possible, we must make stronger assumptions on the training trajectory, which we verify hold empirically. As far as we are aware, one other work on incremental learning handles nonlinear networks: [1] proves that a 2-layer network learns with a two-stage incremental dynamic; but that result needs the stylized assumption that all data points are orthogonal.

### Paper organization

Sections 2, 3, and 4 contain theoretical preliminaries, definitions of the models to which our theory applies, and our main theoretical result on incremental dynamics. Section 5 provides experiments which verify and extend the theory. Section 6 discusses limitations and future directions.

## 2 Preliminaries

We consider training a network \(f_{\text{NN}}(\cdot;\bm{\theta})\) parametrized by a vector of weights \(\bm{\theta}\), to minimize a loss

\[\mathcal{L}(\bm{\theta})=\mathbb{E}_{\bm{x},\bm{y}}[\ell(\bm{y},f_{\text{NN}}( \bm{x};\bm{\theta}))]\,,\]

where the expectation is over samples \((\bm{x},\bm{y})\in\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}}\) from a training data distribution, and \(\ell:\mathbb{R}^{d_{y}}\times\mathbb{R}^{d_{out}}\to\mathbb{R}\). Consider a solution \(\bm{\theta}(t)\) to the gradient flow3

Footnote 3: Gradient flow training can be obtained as a limit of SGD or GD training as the learning rate tends to 0 (see, e.g., [1]). It is a popular testbed for studying learning dynamics (see e.g., [12, 1, 1]), since is generally simpler to analyze than SGD.

\[\bm{\theta}(0)=\alpha\bm{\theta}_{0},\ \ \frac{d\bm{\theta}}{dt}=-\nabla_{\bm{ \theta}}\mathcal{L}(\bm{\theta})\] (3)

where \(\alpha>0\) is a parameter governing the initialization scale, that we will take small. For our theory, we henceforth require the following mild regularity assumption on the loss and data.

**Assumption 2.1** (Regularity of data distribution and loss).: The function \(\ell(\bm{y},\bm{\zeta})\) is continuously twice-differentiable in the arguments \([\bm{y},\bm{\zeta}]\in\mathbb{R}^{d_{y}+d_{out}}\). There exists \(C>0\) such that almost surely the data is bounded by \(\|\bm{x}\|,\|\bm{y}\|\leq C\).

The assumption on \(\ell\) is satisfied in typical cases such as the square and the cross-entropy losses. The data boundedness is often satisfied in practice (e.g., if the data is normalized).

We also use the notation \(\operatorname{supp}(\bm{a}):=\{i:a_{i}\neq 0\}\) to denote the support of a vector \(\bm{a}\).

Neural networks with diagonal weights

Our theory analyzes the training dynamics of networks that depend on products of diagonal weight matrices. We use \(\odot\) to denote elementwise vector product.

**Definition 3.1**.: A network \(f_{\mathsf{NN}}\) is smooth with diagonal weights \(\bm{\theta}=(\bm{u},\bm{v})\in\mathbb{R}^{2p}\) if it is of the form

\[f_{\mathsf{NN}}(\bm{x};\bm{\theta})=h(\bm{x};\bm{u}\odot\bm{v})\]

where \(h:\mathbb{R}^{d_{x}}\times\mathbb{R}^{p}\rightarrow\mathbb{R}^{d_{out}}\) is continuously twice-differentiable in its arguments in \(\mathbb{R}^{d_{x}+p}\).

The assumption on \(h\) precludes the use of the ReLU function since it is not continuously-differentiable. Otherwise the assumption is fairly mild since any \(h\) can be used to express an architecture of any depth as long as the nonlinearities are twice-differentiable, which includes for example GeLUs (as used in ViT). We describe how to express a transformer with diagonal weights.

**Example 3.2** (Transformer with diagonal weights).: A transformer with \(L\) layers and \(H\) attention heads on each layer is defined inductively by \(\bm{Z}_{0}=\bm{X}\in\mathbb{R}^{n\times d}\) and

* (Attention layer) \(\tilde{\bm{Z}}_{\ell}=\bm{Z}_{\ell-1}+\sum_{i=1}^{H}\operatorname{attention}( \bm{Z}_{\ell-1};\bm{W}_{K}^{\ell,i},\bm{W}_{Q}^{\ell,i},\bm{W}_{V}^{\ell,i}, \bm{W}_{O}^{\ell,i})\)
* (Feedforward layer) \(\bm{Z}_{\ell}=\tilde{\bm{Z}}_{\ell}+\sigma(\tilde{\bm{Z}}_{\ell}\bm{W}_{A}^{ \ell})(\bm{W}_{B}^{\ell})^{\top}\),

where \(\bm{W}_{K}^{\ell,i},\bm{W}_{Q}^{\ell,i},\bm{W}_{V}^{\ell,i},\bm{W}_{O}^{\ell,i }\in\mathbb{R}^{d\times d^{\prime}}\) are attention parameters, and \(\bm{W}_{A}^{\ell},\bm{W}_{B}^{\ell}\in\mathbb{R}^{d\times d^{\prime}}\) are the feedforward parameters, and \(\sigma\) is a continuously twice-differentiable activation. Suppose that the attention parameters are diagonal matrices: i.e., \(\bm{W}_{K}^{\ell,i}=\operatorname{diag}(\bm{w}_{K}^{\ell,i})\in\mathbb{R}^{d \times d}\), and similarly for the \(\bm{W}_{Q}^{\ell,i},\bm{W}_{V}^{\ell,i},\bm{W}_{O}^{\ell,i}\) matrices. Then by the definition of the attention layer (1), the final output of the transformer \(\bm{Z}_{L}\) only depends on the attention parameters through the elementwise products \(\bm{w}_{K}^{\ell,i}\odot\bm{w}_{Q}^{\ell,i}\) and \(\bm{w}_{V}^{\ell,i}\odot\bm{w}_{O}^{\ell,i}\). In other words, we can write

\[\bm{Z}_{L}=h(\bm{X};\bm{u}\odot\bm{v})\,,\]

for vectors \(\bm{u}=[\bm{w}_{K}^{\ell,i},\bm{w}_{V}^{\ell,i}]_{(\ell,i)\in[L]\times[H]}\in \mathbb{R}^{2dHL}\) and \(\bm{v}=[\bm{w}_{Q}^{\ell,i},\bm{w}_{O}^{\ell,i}]_{(\ell,i)\in[L]\times[H]}\in \mathbb{R}^{2dHL}\), and some smooth model \(h\). Thus, if only the attention layers are trained, the diagonal transformer fits under Definition 3.1.

## 4 Incremental learning in networks with diagonal weights

We prove that if the initialization scale \(\alpha\) is small, then learning proceeds in incremental stages, where in each stage the effective sparsity of the weights increases by at most one. These stages are implicitly defined by Algorithm 1 below, which constructs a sequence of times \(0=T_{0}<T_{1}<\cdots<T_{k}<\cdots\) and weight vectors \(\bm{\theta}^{0},\bm{\theta}^{1},\ldots,\bm{\theta}^{k},\ldots\in\mathbb{R}^{2p}\) that define the stages. We prove the following:

**Theorem 4.1** (Incremental dynamics at small initialization).: _Let \(f_{\mathsf{NN}}\) be a model with diagonal weights as in Definition 3.1. For any stage \(k\) and time \(t\in(T_{k},T_{k+1})\) the following holds under Assumptions 2.1, 4.3, 4.4 and 4.5. There is \(\alpha_{0}(t)>0\) such that for all \(\alpha<\alpha_{0}\), there exists a unique solution \(\bm{\theta}:[0,t\log(1/\alpha)]\rightarrow\mathbb{R}^{p}\) to the gradient flow (3) and_

\[\lim_{\alpha\to 0}\bm{\theta}(t\cdot\log(1/\alpha))\rightarrow\bm{\theta}^{k}\,\]

_and at each stage the sparsity increases by at most one: \(\operatorname{supp}(\bm{\theta}^{k+1})\setminus\operatorname{supp}(\bm{ \theta}^{k})\subseteq\{i_{k}\}\).4_

Footnote 4: Abusing notation, for \(\bm{\theta}=(\bm{u},\bm{v})\in\mathbb{R}^{p}\times\mathbb{R}^{p}\), we write \(\operatorname{supp}(\bm{\theta})=\operatorname{supp}(\bm{u})\cup\operatorname {supp}(\bm{v})\).

Application: transformer with diagonal weightsBefore giving the intuition for this theorem and stating the assumptions formally, let us discuss its application to the diagonal transformer model from Example 3.2. As a corollary of Theorem 4.1, the gradient flow on a diagonal transformer with small initialization will learn in stages, where in each stage there will be at most one head \(i\in[H]\) in one layer \(\ell\in[L]\) such that either the rank of \(\bm{W}_{K}^{\ell,i}(\bm{W}_{Q}^{\ell,i})^{\top}=\operatorname{diag}(\bm{w}_{K}^ {\ell,i})\operatorname{diag}(\bm{w}_{Q}^{\ell,i})\) or the rank of \(\bm{W}_{V}^{\ell,i}(\bm{W}_{O}^{\ell,i})^{\top}=\operatorname{diag}(\bm{w}_{V }^{\ell,i})\operatorname{diag}(\bm{w}_{O}^{\ell,i})\) increases by at most one. In Figure 2, we illustrate these dynamics in the toy case of a single attention head trained in a student-teacher setup.

### Intuition for incremental learning dynamics

We develop an informal intuition for Theorem 4.1 and fill out the definition of Algorithm 1. A model \(f_{\text{NN}}\) with diagonal weights \(\bm{\theta}=(\bm{u},\bm{v})\) as in Definition 3.1 evolves under the gradient flow (3) as

\[\frac{d\bm{u}}{dt} =\bm{v}\odot\bm{g}(\bm{\theta}),\quad\frac{d\bm{v}}{dt}=\bm{u} \odot\bm{g}(\bm{\theta})\quad\text{ where }\] (4) \[\bm{g}(\bm{\theta}) =-\operatorname{\mathbb{E}}_{\bm{x},\bm{y}}[D\ell(\bm{y},h(\bm{x} ;\bm{u}\odot\bm{v}))^{\top}Dh(\bm{x};\bm{u}\odot\bm{v})^{\top}]\,.\]

Here \(D\ell(\bm{y},\cdot)\in\mathbb{R}^{1\times d_{out}}\) is the derivative of \(\ell\) in the second argument and \(Dh(\bm{x},\cdot)\in\mathbb{R}^{d_{out}\times p}\) is the derivative of \(h\) in the second argument. The first key observation is a conservation law that simplifies the dynamics. It can be viewed as the balancedness property for networks with linear activations [1, 1], specialized to the case of diagonal layers.

**Lemma 4.2** (Conservation law).: _For any \(i\in[p]\) and any time \(t\), we have_

\[u_{i}^{2}(t)-v_{i}^{2}(t)=u_{i}^{2}(0)-v_{i}^{2}(0)\,.\] (5)

Proof.: This follows from \(\frac{d}{dt}(u_{i}^{2}-v_{i}^{2})=u_{i}v_{i}g_{i}(\bm{\theta})-u_{i}v_{i}g_{i}( \bm{\theta})=0\)

Figure 2: (a) Loss versus rescaled time in the toy task of learning an attention head with diagonal weights, for various initialization scales \(\alpha\). The loss curves converge as \(\alpha\to 0\) to a curve with stagewise loss plateaus and sharp decreases, as predicted by the theory; some stagewise learning behavior is already clear with \(\alpha=0.01\). (b) Each line shows the evolution of one of the entries of \(\mathrm{diag}(\bm{w}_{Q})\mathrm{diag}(\bm{w}_{K})\) and \(\mathrm{diag}(\bm{w}_{V})\mathrm{diag}(\bm{w}_{O})\) over rescaled time, demonstrating that the rank of these matrices increases incrementally; see Appendix A for experimental details and further results.

The conservation law reduces the degrees of freedom and means that we need only keep track of \(p\) parameters in total. Specifically, if we define \(w_{i}(t):=u_{i}(t)+v_{i}(t)\), then the vector \(\bm{w}(t)=\bm{u}(t)+\bm{v}(t)\) evolves by

\[\frac{d\bm{w}}{dt}=\bm{w}\odot\bm{g}(\bm{\theta})\,.\] (6)

Using the conservation law (5), we can keep track of the weights in terms of the initialization and \(\bm{w}(t)\):

\[\bm{\theta}(t)=\bigg{(}\frac{1}{2}(\bm{w}(t)+\frac{\bm{u}^{\odot 2}(0)-\bm{v}^{ \odot 2}(0)}{\bm{w}(t)}),\frac{1}{2}(\bm{w}(t)-\frac{\bm{u}^{\odot 2}(0)-\bm{v}^{ \odot 2}(0)}{\bm{w}(t)})\bigg{)}\] (7)

Therefore it suffices to analyze the dynamics of \(\bm{w}(t)\).

#### 4.1.1 Stage 1 of dynamics

Stage 1A of dynamics: loss plateau for time \(\Theta(\log(1/\alpha))\)At initialization, \(\bm{\theta}(0)\approx\bm{0}\) because the weights are initialized on the order of \(\alpha\) which is small. This motivates the approximation \(\bm{g}(\bm{\theta}(t))\approx\bm{g}(\bm{0})\), under which the dynamics solve to:

\[\bm{w}(t)\approx\bm{w}(0)\odot e^{\bm{g}(\bm{0})t}.\] (8)

Of course, this approximation is valid only while the weights are still close to the small initialization. The approximation breaks once one of the entries of \(\bm{\theta}(t)\) reaches constant size. By combining (7) and (8), this happens at time \(t\approx T_{1}\cdot\log(1/\alpha)\) for

\[T_{1}=\min_{i\in[p]}1/|g_{i}(\bm{0})|\,.\]

Until this time, the network remains close to its initialization, and so we observe a loss plateau.

Stage 1B of dynamics: nonlinear dynamics for time \(O(1)\)Subsequently, the loss decreases nonlinearly during a \(O(1)\) time-scale, which is vanishingly short relative to the time-scale of the loss plateau. To prove this, we make the non-degeneracy assumption that there is a unique coordinate \(i_{0}\) such that \(1/|g_{i_{0}}(\bm{0})|=T_{1}\). Under this assumption, in stage 1A all weights except for those at coordinate \(i_{0}\) remain vanishingly small, on the order of \(o_{\alpha}(1)\). Concretely, for any small \(\epsilon>0\), there is a time \(\underline{t}_{1}(\epsilon)\approx T_{1}\cdot\log(1/\alpha)\) and sign \(s\in\{+1,-1\}\) such that5

Footnote 5: Without loss of generality, we can ensure that at initialization \(\bm{u}(0)\) and \(\bm{u}(0)+\bm{v}(0)\) are nonnegative. This implies \(\bm{u}(t)\) is nonnegative. The fact that \(u_{i_{0}}\) and \(v_{i_{0}}\) are roughly equal in magnitude but might differ in sign is due to the conservation law (5). See Appendix C.3 for details.

\[u_{i_{0}}(\underline{t}_{1})\approx\epsilon,v_{i_{0}}(\underline{t}_{1}) \approx s\epsilon\quad\text{and}\quad|u_{i}(\underline{t}_{1})|,|v_{i}( \underline{t}_{1})|=o_{\alpha}(1)\text{ for all }i\neq i_{0}.\]

Because all coordinates except for \(i_{0}\) have vanishingly small \(o_{\alpha}(1)\) weights after stage 1A, we may perform the following approximation of the dynamics. Zero out the weights at coordinates except for \(i_{0}\), and consider the training dynamics starting at \(\tilde{\bm{\theta}}=(\epsilon\bm{e}_{i_{0}},s\epsilon\bm{e}_{i_{0}})\). After \(O(1)\) time, we should expect these dynamics to approach a stationary point. Although the evolution is nonlinear, all entries remain zero except for the \(i_{0}\) entries, so the stationary point is also sparse. Mathematically, there is a time \(\bar{t}_{1}=\underline{t}_{1}+O(1)\approx T_{1}\cdot\log(1/\alpha)\) such that

\[\bm{\theta}(\bar{t}_{1})\approx(a\bm{e}_{i_{0}},s\bm{e}_{i_{0}}):=\bm{\theta}^ {1}\,,\]

for some \(a\in\mathbb{R}_{>0}\), where \(\bm{\theta}^{1}\) is a stationary point of the loss.6 Despite the nonlinearity of the dynamics, the approximation can be proved using Gronwall's inequality since \(\bar{t}_{1}-\underline{t}_{1}=O(1)\) is a constant time-scale.

Footnote 6: The entries of \(\bm{u}\) and \(\bm{v}\) are close in magnitude (but may differ in sign) because of the conservation law (5).

To summarize, we have argued that the network approximately reaches stationary point that is 1-sparse, where only the weights at coordinate \(i_{0}\) are nonzero.

#### 4.1.2 Later stages

We inductively extend the argument to any number of stages \(k\), where each stage has a \(\Theta(\log(1/\alpha))\)-time plateau, and then a \(O(1)\)-time nonlinear evolution, with the sparsity of the weights increasing by at most one. The argument to analyze multiple stages is analogous, but we must also keep track of the magnitude of the weights on the logarithmic scale, since these determine how much longer. Inductively on \(k\), suppose that there is some \(T_{k}\in\mathbb{R},\bm{b}^{k}\in\mathbb{R}^{p}\) and \(\bm{\theta}^{k}\in\mathbb{R}^{2p}\) and a time \(\bar{t}_{k}\approx T_{k}\cdot\log(1/\alpha)\) such that

\[\log_{\alpha}(\bm{w}(\bar{t}_{k}))\approx\bm{b}^{k}\text{ and }\bm{\theta}(\bar{t }_{k})\approx\bm{\theta}^{k},\]

where \(\bm{\theta}^{k}\) is a stationary point of the loss. Our inductive step shows that there is \(T_{k+1}\in\mathbb{R}\) such that during times \(t\in(\bar{t}_{k},T_{k+1}\cdot\log(1/\alpha)-\Omega(1))\) the weights remain close to the stationary point from the previous stage, i.e., \(\bm{\theta}(t)\approx\bm{\theta}^{k}\). And at a time \(\bar{t}_{k+1}\approx T_{k+1}\cdot\log(1/\alpha)\) we have

\[\log_{\alpha}(\bm{w}(\bar{t}_{k+1}))\approx\bm{b}^{k+1}\text{ and }\bm{\theta}(\bar{t}_{k+1})\approx\bm{\theta}^{k+1},\]

where \(\bm{\theta}^{k+1}\) and \(\bm{b}^{k+1}\) are defined below (summarized in Algorithm 1). Most notably, \(\bm{\theta}^{k+1}\) is a stationary point of the loss whose support grows by at most one compared to \(\bm{\theta}^{k}\).

Stage \((k+1)\)A, loss plateau for time \(\Theta(\log(1/\alpha))\)At the beginning of stage \(k+1\), the weights are close to the stationary point \(\bm{\theta}^{k}\), and so, similarly to stage 1A, linear dynamics are valid.

\[\bm{w}(t)\approx\bm{w}(\bar{t}_{k})\odot e^{\bm{g}(\bm{\theta}^{k})(t-\bar{t }_{k})}\,.\] (9)

Using the conservation law (7), we derive a "time until active" for each coordinate \(i\in[p]\), which corresponds to the time for the weight at that coordinate to grow from \(o_{\alpha}(1)\) to \(\Theta(1)\) magnitude:

\[\Delta_{k}(i)=\begin{cases}(b_{i}^{k}-1+\operatorname{sgn}(g_{i}(\bm{\theta}^ {k})))/g_{i}(\bm{\theta}^{k}),&\text{if }g_{i}(\bm{\theta}^{k})\neq 0\\ \infty,&\text{if }g_{i}(\bm{\theta}^{k})=0\end{cases}\] (10)

The linear dynamics approximation (9) breaks down at a time \(t\approx T_{k+1}\cdot\log(1/\alpha)\), where

\[T_{k+1}=T_{k}+\Delta_{k}(i_{k}),\quad i_{k}=\arg\min_{i\in[p]}\Delta_{k}(i)\,,\] (11)

which corresponds to the first time at the weights at a coordinate grow from \(o_{\alpha}(1)\) to \(\Theta(1)\) magnitude. And at times \(t\approx T_{k+1}\cdot\log(1/\alpha)\), on the logarithmic scale \(\bm{w}\) is given by

\[\log_{\alpha}(\bm{w}(t))\approx\bm{b}^{k+1}:=\bm{b}^{k}-\bm{g}(\bm{\theta}^{ k})\Delta_{k}(i_{k})\,,\] (12)

Stage \((k+1)\)B of dynamics: nonlinear dynamics for time \(O(1)\)Subsequently, the weights evolve nonlinearly during \(O(1)\) time. In a similar way to the analysis of Stage 1B, we show that at a time \(\bar{t}_{k+1}=\underline{t}_{k+1}+O(1)\approx T_{k+1}\cdot\log(1/\alpha)\), we have

\[\bm{\theta}(\bar{t}_{k+1})\approx\bm{\theta}^{k+1}:=\lim_{\epsilon\to 0} \lim_{t\to\infty}\bm{\psi}^{k}(t,\epsilon)\,,\] (13)

where the dynamics \(\bm{\psi}^{k}(t,\epsilon)\in\mathbb{R}^{2p}\) are initialized at \(\bm{\psi}^{k}(0,\epsilon)=\bm{\theta}^{k}+(\epsilon\bm{e}_{i_{k}}, \operatorname{sgn}(g_{i}(\bm{\theta}^{k}))\epsilon\bm{e}_{i_{k}})\) and evolve according to the gradient flow \(\frac{d\bm{\psi}^{k}(t,\epsilon)}{dt}=-\nabla_{\bm{\theta}}\mathcal{L}(\bm{\psi }^{k})\). This concludes the inductive step.

### Assumptions for incremental dynamics

To make this intuition rigorous, we formalize below the assumptions required for Theorem 4.1. In Figure 3 and Appendix A, we provide experiments validating these assumptions on the toy model.

The first assumption is that the dynamics are non-degenerate, in the sense that two coordinates do not have weights that grow from \(o_{\alpha}(1)\) to \(\Theta(1)\) size at the same rescaled time. We also place a technical condition to handle the corner case when a coordinate leaves the support of the current stage's stationary point.

**Assumption 4.3** (Nondegeneracy of dynamics in part (A)).: The initialization satisfies \(|u_{i}(0)|\neq|v_{i}(0)|\) for all \(i\). For stage \(k\), either \(T_{k+1}=\infty\) or there is a unique minimizer \(i_{k}\) to \(\min_{i}\Delta_{k}(i_{k})\) in (11). Finally, for all \(i\in\operatorname{supp}(\bm{\theta}^{k-1})\setminus\operatorname{supp}(\bm{ \theta}^{k})\) we have \(g_{i}(\bm{\theta}^{k})\neq 0\).

Next, we require that very small perturbations of the coordinates outside of \(\operatorname{supp}(\bm{\theta}^{k})\) do not change the dynamics. For this, it suffices that \(\bm{\theta}^{k}\) be a strict local minimum.

**Assumption 4.4** (Stationary points are strict local minima).: For stage \(k\), there exist \(\delta_{k}>0\) and \(c_{k}>0\) such that for \(\tilde{\bm{u}}\in B(\bm{u}^{k},\delta)\) supported on \(\operatorname{supp}(\bm{u}^{k})\), we have

\[\mathcal{L}(\tilde{\bm{u}},\bm{s}^{k}\odot\tilde{\bm{u}})\geq c_{k}\|\bm{u}^{ k}-\tilde{\bm{u}}\|^{2}\]

Finally, we require a robust version of the assumption that the limit (13) exists, asking for convergence to a neighborhood of \(\bm{\theta}^{k+1}\) even when the initialization is slightly noisy.

**Assumption 4.5** (Noise-robustness of dynamics in part (B)).: For any stage \(k\) with \(T_{k+1}<\infty\) and any \(\epsilon>0\), there are \(\delta>0\) and \(\tau:\mathbb{R}_{>0}\to\mathbb{R}\) such that the following holds. For any \(\tilde{\bm{u}}\in B(\bm{u}^{k},\delta)\cap\mathbb{R}_{\geq 0}^{p}\) supported on \(\operatorname{supp}(\tilde{\bm{u}})\subseteq\operatorname{supp}(\bm{u}^{k}) \cup\{i_{k}\}\), there exists a unique solution \(\bm{\psi}:[0,\infty)\to\mathbb{R}^{p}\) of the gradient flow \(\frac{d\bm{\psi}}{dt}=-\nabla_{\bm{\theta}}\mathcal{L}(\bm{\psi})\) initialized at \(\bm{\psi}(0)=(\tilde{\bm{u}},\bm{s}^{k+1}\odot\tilde{\bm{u}})\), and at times \(t\geq\tau(\tilde{\psi}_{i_{k}})\),

\[\|\bm{\psi}(t)-\bm{\theta}^{k+1}\|<\epsilon\,.\]

## 5 Experimental results

We run experiments that go beyond the toy diagonal attention head model (see Figures 2 and 3) to test the extent to which low-rank incremental learning occurs in popular models used in practice. We conduct experiments with vision transformers (ViT) [1] trained on the CIFAR-10/100 and ImageNet datasets, and with the GPT-2 language transformer [1] trained on the Wikitext-103 dataset. Full experiments are deferred to Appendix B.

Gradual rank increase in vision and language modelsWe train practical transformer architectures on vision and language tasks using Adam and the cross-entropy loss. We train all layers (including the feedforward layers). To capture the low-rank bias with a non-vanishing initialization scale, we study the spectrum of the difference \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) between the weights post-training and their initial values. Specifically, in Figure 4, we plot the stable rank of the differences \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\). The weight perturbation learned during the training process gradually increases in stable rank during training, and is ultimately low-rank when compared to the initial spectrum. Finally, for CIFAR-10, we plot the spectrum of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) against that of its initialized state in Figure 5 for different self-attention heads, illustrating that the weight perturbation learned during the training process is extremely low-rank when compared to the initial spectrum. In Appendix B, we also study optimization with SGD, which shows similar gradual rank increase behavior.

Effect of initialization scaleWe probe the effect of initialization scale on gradual rank increase dynamics for a ViT trained on CIFAR-10. We use a ViT of depth 6, with 8 self-attention heads per layer (with layer normalization). We use an embedding and MLP dimension of \(d_{\text{emb}}=512\), and a head dimension of \(d_{h}=128\) (i.e \(\bm{W}_{K},\bm{W}_{Q},\bm{W}_{V},\bm{W}_{O}\in\mathbb{R}^{d_{\text{emb}}\times d _{h}}\)). We train the transformer

Figure 3: Validation of assumptions on the toy model of learning a single attention head. (a) Assumption 4.4: weights perturbed at a random time during training (solid lines) tend back to the near-stationary point (dashed lines). (b) Assumption 4.5: weights perturbed at the beginning of a stage (solid lines) have same nonlinear evolution as without perturbation (dashed lines). Details of these experiments and further validations are provided in Appendix A.

using Adam with the cross-entropy loss. We train all layers (including the feedforward layers) while varying the initialization scale of all layers by multiplying their initial values by a scale factor (we fix the scale of the initial token mapper). Figure 6 shows the evolution of the principal components of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) for a randomly-chosen self-attention head and layer throughout training, exhibiting incremental learning dynamics and a low-rank bias. Note that incremental learning and low-rank bias are increasingly evident with smaller initialization scales, as further demonstrated in Figure 7.

## 6 Discussion

We have identified incremental learning dynamics in transformers, proved them rigorously in a simplified setting, and shown them experimentally in networks trained with practical hyperparameters.

LimitationsThere are clear limitations to our theory: the diagonal weights and small initialization assumptions. More subtly, the theory does not apply to losses with exponential-like tails because the weights may not converge to a finite value and so Assumption 4.4 is not met (this could possibly be addressed by adding regularization). Also, the architecture must be smooth, which precludes ReLUs - but allows for smoothed ReLUs such as the GeLUs used in ViT [1]. Finally, the theory is for training with gradient flow, while other optimizers such as Adam are used in practice instead [13]. Nevertheless, our experiments on ViTs indicate that the incremental learning dynamics occur even when training with Adam.

Figure 4: Stable rank of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) (blue) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) (orange) on an arbitrary chosen layer throughout training for four different pairs of networks and tasks. The stable rank of a matrix \(\bm{W}\) is defined as \(\|\bm{W}\|_{F}^{2}/\|\bm{W}\|_{2}^{2}\), and gives a smooth approximation of the rank. Mean and standard deviation (shaded area) are computed across all heads in each attention layer. Full details and results are in Appendix B.

Figure 5: Spectrum of the weight perturbation \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) vs. initialization in a vision transformer trained on CIFAR-10, using Adam and default initialization scale, in random self-attention heads in different layers. The learned perturbation exhibits extreme low-rank bias post-training even in default initialization scales. Analogous plots for CIFAR-100 and ImageNet are in Appendix B.

Future directionsAn interesting avenue of future research is to develop a theoretical understanding of the implicit bias in function space of transformers whose weights are a low-rank perturbation of randomly initialized weights. Another promising direction is to examine the connection between our results on incremental dynamics and the LoRA method [11], with the goal of explaining and improving on this algorithm; see also the discussion in Section 1.1. Along this vein, a concurrent work [23] independently observes gradual rank increase dynamics during transformer training and this inspires a low-rank training algorithm that obtains runtime and memory improvements over regular training. The results of [23] are complementary to ours, since they study the feedforward layers of the transformer, and their theory applies to _linear_ networks in the standard initialization scale; in contrast, we study the attention layers, and our theory applies to _nonlinear_ networks with small initialization scale.

## Acknowledgments

We would like to thank Vimal Thilak for his help in setting up the infrastructure for conducting experiments, and the anonymous reviewers for their helpful feedback.

## References

* [ABM22] Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks, COLT, 2022.

Figure 6: Training a vision transformer on CIFAR-10 using Adam, while varying the initialization scale (unit scale indicates default initialization). Plotted are the evolution of the eigenvalues of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) (a) - (c) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) (d) - (f) in a random self-attention head in the second layer throughout training. Incremental learning dynamics and a low-rank bias are evident for all scales, albeit more pronounced at smaller initialization scales.

Figure 7: Stable rank of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) per initialization scale (Unit scale refers to the default initialization) in different self-attention heads post-training, at layers 1, 3, 5. At each layer, the stable rank mean and standard deviation are computed across 8 heads per layer, for each initialization scale. All models were trained on CIFAR-10 using the Adam optimizer. Smaller initialization scales lead to lower-rank attention heads.

* [ABM23] Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. _arXiv preprint arXiv:2302.11055_, 2023.
* [ACH18] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In _International Conference on Machine Learning_, pages 244-253. PMLR, 2018.
* [ACHL19] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [AZG20] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. _arXiv preprint arXiv:2012.13255_, 2020.
* [Bac20] Francis Bach. Effortless optimization through gradient flows. _Machine Learning Research Blog. https://francisbach. com/gradient-flows_, 2020.
* [BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. _arXiv preprint arXiv:2210.15651_, 2022.
* [Ber22] Raphael Berthier. Incremental learning in diagonal linear networks. _arXiv preprint arXiv:2208.14673_, 2022.
* [BMR\({}^{+}\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [BPVF22] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs. _arXiv preprint arXiv:2206.00939_, 2022.
* [COB18] Lenac Chizat, Edouard Oyallon, and Francis R. Bach. On lazy training in differentiable programming. In _Neural Information Processing Systems_, 2018.
* [DBK\({}^{+}\)20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ArXiv_, abs/2010.11929, 2020.
* [DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _ArXiv_, abs/1810.04805, 2019.
* [DHL18] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. _Advances in Neural Information Processing Systems_, 31, 2018.
* [DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* [FVB\({}^{+}\)22] Spencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in leaky relu networks trained on high-dimensional data. _arXiv preprint arXiv:2210.07082_, 2022.
* [GBLJ19] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [GMMM19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy training of two-layers neural network. _Advances in Neural Information Processing Systems_, 32, 2019.

* [GSJW19] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy learning in deep neural networks: an empirical study. _ArXiv_, abs/1906.08034, 2019.
* [GSSD19] Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental learning drives generalization. _arXiv preprint arXiv:1909.12051_, 2019.
* [HSW\({}^{+}\)21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [JGS\({}^{+}\)21] Arthur Jacot, Francois Gaston Ged, Berlin Simsek, Clement Hongler, and Franck Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity. 2021.
* [JLL\({}^{+}\)23] Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon S Du, and Jason D Lee. Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing. _arXiv preprint arXiv:2301.11500_, 2023.
* [KB14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [KC22] Daesung Kim and Hye Won Chung. Rank-1 matrix completion with gradient descent and small random initialization. _ArXiv_, abs/2212.09396, 2022.
* [LFLY18] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. _arXiv preprint arXiv:1804.08838_, 2018.
* [LLL20] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. _ArXiv_, abs/2012.09839, 2020.
* [LOG\({}^{+}\)19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv_, abs/1907.11692, 2019.
* [MGW\({}^{+}\)20] Edward Moroshko, Suriya Gunasekar, Blake E. Woodworth, J. Lee, Nathan Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. _ArXiv_, abs/2007.06738, 2020.
* [MHPG\({}^{+}\)22] Alireza Mousavi-Hosseini, Sejun Park, Manuela Giroti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. _arXiv preprint arXiv:2209.14863_, 2022.
* [MKAA21] Paolo Milanesi, Hachem Kadri, S. Ayache, and Thierry Artieres. Implicit regularization in deep tensor factorization. _2021 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8, 2021.
* [MKAS21] Eran Malach, Pritush Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using differentiable learning over tangent kernels. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 7379-7389. PMLR, 18-24 Jul 2021.
* [PA23] Dylan Patel and Afzal Ahmad. Google "we have no moat, and neither does openai", May 2023.
* [PF23] Scott Pesme and Nicolas Flammario. Saddle-to-saddle dynamics in diagonal linear networks. _arXiv preprint arXiv:2304.00488_, 2023.

* [RC20] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. _Advances in neural information processing systems_, 33:21174-21187, 2020.
* [SKZ\({}^{+}\)23] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua Albrecht. On the stepwise nature of self-supervised learning. _arXiv preprint arXiv:2303.15438_, 2023.
* [SMG13] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _arXiv preprint arXiv:1312.6120_, 2013.
* [SS21] Dominik Stoger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. In _Neural Information Processing Systems_, 2021.
* [TGZ\({}^{+}\)23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. _Stanford Center for Research on Foundation Models. https://crfhn.stanford. edu/2023/03/13/alpaca. html_, 2023.
* [TVS23] Nadav Timor, Gal Vardi, and Ohad Shamir. Implicit regularization towards rank minimization in relu networks. In _International Conference on Algorithmic Learning Theory_, pages 1429-1459. PMLR, 2023.
* [VSP\({}^{+}\)17] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _ArXiv_, abs/1706.03762, 2017.
* [WGL\({}^{+}\)19] Blake E. Woodworth, Suriya Gunasekar, J. Lee, Edward Moroshko, Pedro H. P. Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. _ArXiv_, abs/2002.09277, 2019.
* [YW23] Hao Yu and Jianxin Wu. Compressing transformers: Features are low-rank, but weights are not! 2023.
* [ZZC\({}^{+}\)23] Jiawei Zhao, Yifei Zhang, Beidi Chen, Florian Schafer, and Anima Anandkumar. Inrank: Incremental low-rank learning. _arXiv preprint arXiv:2306.11250_, 2023.

###### Contents

* 1 Introduction
	* 1.1 Related work
	* 1.2 Paper organization
* 2 Preliminaries
* 3 Neural networks with diagonal weights
* 4 Incremental learning in networks with diagonal weights
	* 4.1 Intuition for incremental learning dynamics
		* 4.1.1 Stage 1 of dynamics
		* 4.1.2 Later stages
	* 4.2 Assumptions for incremental dynamics
* 5 Experimental results
* 6 Discussion
* A Experimental validation of the assumptions in Theorem 4.1
* B Further experiments on vision and language transformers
* B.1 SGD-trained transformers
* B.2 Adam-trained transformers
* C Proof for dynamics of networks with diagonal parametrization (Theorem 4.1)
* C.1 Assumptions
* C.2 Rescaling time for notational convenience
* C.3 Simplifying problem without loss of generality
* C.4 Tracking the sum of the weights
* C.5 Claimed invariants in proof of Theorem C.4
* C.6 Dynamics from time \(\bar{t}_{k}\) to time \(\underline{t}_{k+1}\) (Linear dynamics for \(O(\log(1/\alpha))\) unrescaled time)
* C.6.1 Analysis in case where \(T_{k+1}<\infty\)
* C.6.2 Analysis in case where \(T_{k+1}=\infty\)
* C.7 Dynamics from time \(\underline{t}_{k}\) to time \(\bar{t}_{k}\) (Nonlinear evolution for \(O(1)\) unrescaled time)
* C.8 Concluding the proof of Theorem C.4
* D Technical lemmas
* D.1 Relating the sum of the weights to the original weights using the conservation law
* D.2 Sign of gradients on coordinates that leave support
* D.3 Local lipschitzness and smoothness

Experimental validation of the assumptions in Theorem 4.1

In Figures 2, 8, and 9, we plot the evolution of the losses, of the entries of \(\bm{W}_{K}\bm{W}_{Q}^{\top}=\mathrm{diag}(\bm{w}_{K})\mathrm{diag}(\bm{w}_{Q})\), and of the entries of \(\bm{W}_{V}\bm{W}_{O}^{\top}=\mathrm{diag}(\bm{w}_{V})\mathrm{diag}(\bm{w}_{O})\) in the toy task of training an attention head (1) with diagonal weights. The model is trained with SGD on the mean-squared error loss on 1000 random samples \((\bm{X},\bm{y})\). Each random sample has \(\bm{X}\in\mathbb{R}^{10\times 50}\), which a sequence of 10 tokens, each of dimension 50, which is distributed as isotropic Gaussian. The label \(\bm{y}\) is given by a randomly-generated teacher model that is also an attention head (1) with diagonal weights. In Figures 2, 8, and 9, for \(\alpha\in\{0.1,0.01,0.0001,10^{-8},10^{-16},10^{-32}\}\) we plot the evolution of the loss and of the weights when initialized at \(\bm{\theta}(0)=\alpha\bm{\theta}_{0}\), for some random Gaussian \(\bm{\theta}_{0}\). Qualitatively, as \(\alpha\to 0\) we observe that the loss curve and the trajectories of the weights appear to converge to a limiting stagewise dynamics, where there are plateaus followed by movement on short time-scales, as predicted by the theory.

Validation of Assumption 4.3 (non-degeneracy of dynamics)As \(\alpha\to 0\), notice that the stages appear to separate and happen at distinct times. Furthermore, the extra technical condition on coordinates \(i\in\mathrm{supp}(\bm{\theta}^{k})\setminus\mathrm{supp}(\bm{\theta}^{k-1})\) in Assumption 4.3 is satisfied since no coordinates ever leave the support of \(\bm{\theta}^{k}\).

Validation of Assumption 4.4 (stationary points are strict local minima)In Figure 10 we consider the \(\alpha=10^{-32}\) trajectory, since this is closest to the dynamics in the \(\alpha\to 0\) limit. We randomly select several epochs. Since the transitions between stages are a vanishing fraction of the total training time, each of these randomly-selected epochs is likely during a plateau, as we see in the figure. For each epoch perform the following experiment. For each nonnegligible coordinate of the weights (those where the weight is of magnitude greater than the threshold \(\tau=10^{-5}\)), we perturb the weights by adding noise of standard deviation \(0.05\). We then run the training dynamics starting at this perturbed initialization for 1000 epochs. We observe that the training dynamics quickly converge to the original unperturbed initialization, indicating that the weights were close to a strict local minimum of the loss.

Validation of Assumption 4.5 (noise-robustness of dynamics)In Figure 11 we perform the same experiment as in Figure 10, except that the epochs we select to perturb the weights are those where there is a newly-nonnegligible coordinate (less than \(10^{-5}\) in magnitude in the previous epoch, and more than \(10^{-5}\) in magnitude in this epoch). We find that the nonlinear dynamics are robust and tend to the limiting endpoint even under a random Gaussian perturbation of standard deviation \(10^{-2}\) on each of the nonnegligible coordinates, supporting Assumption 4.5.

Figure 8: Evolution of \(\mathrm{diag}(\bm{w}_{Q})\mathrm{diag}(\bm{w}_{K})\) entries over rescaled time initializing at various scalings \(\alpha\). Notice that as \(\alpha\to 0\), the training trajectories tend to a limiting trajectory. Each line corresponds to a diagonal entry of \(\mathrm{diag}(\bm{w}_{Q})\mathrm{diag}(\bm{w}_{K})\).

Figure 9: Evolution of \(\mathrm{diag}(\bm{w}_{V})\mathrm{diag}(\bm{w}_{O})\) entries in the toy task of learning an attention head with diagonal weights. Each line corresponds to the evolution of an entry of \(\mathrm{diag}(\bm{w}_{V})\mathrm{diag}(\bm{w}_{O})\) over rescaled time. Each plot corresponds to a different initialization magnitude \(\alpha\). Notice that as \(\alpha\to 0\), the training trajectories tend to a limiting trajectory.

Figure 10: Evolution of weights of toy attention model under perturbation, validating Assumption 4.4. At 5 different random times during training, we perturb the nonnegligible weight coordinates and continue to train with SGD. The evolution of each of the weights under the initial perturbation (solid line) is compared to the original evolution without perturbation (dashed line). Observe that the training dynamics quickly brings each weight back to the unperturbed weight trajectory, indicating that the weights are originally close to a strict local minimum.

Figure 11: Validating Assumption 4.5 with the same experiment as in Figure 10, except that the epochs for the perturbation chosen are those where there is a newly nonnegligible coordinate. Perturbed dynamics (solid lines) are again robust to perturbation and track the original dynamics (dashed lines).

[MISSING_PAGE_FAIL:20]

### Adam-trained transformers

CIFAR-10/100For the CIFAR-10/100 datasets we use a VIT with 6 layers, patchsize of 4, 8 heads per self attention layer, an embedding and MLP dimension of 512, and a head dimension of 128. We train the model using the Adam optimizer for 500 epochs with a base learning rate of 1e-4, a cyclic learning rate decay with a linear warmup schedule for 15 epochs and a batchsize of 512. Our results are summarized in Figures 14 and 15 for CIFAR-10, and Figures 16 and 17 for CIFAR-100.

ImageNetFor ImageNet, we use the VIT-Base/16 from [DBK\({}^{+}\)20] trained with Adam for 360 epochs with a base learning rate of 3e-3, a cyclic learning rate decay with a linear warmup schedule for 15 epochs and a batchsize of 4096. Our results are summarized in Figures 18 and 19 for ImageNet.

Figure 14: CIFAR-10, ViT trained with Adam: normalized spectrum at different stages of training. (a) - (c) Normalized spectrum of \(\bm{W}_{K}\bm{W}_{Q}^{\top}\) at initialization and \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) during training for different attention heads at different layers. (d) - (e) equivalent figures for \(\bm{W}_{V}\bm{W}_{O}^{\top}\).

Figure 13: CIFAR-100, ViT trained with SGD: Stable rank of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) (blue) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) (orange) throughout training. Mean and standard deviation (shaded area) are computed across 8 heads per attention layer.

Figure 16: CIFAR-100, ViT trained with Adam: normalized spectrum at different stages of training. (a) - (c) Normalized spectrum of \(\bm{W}_{K}\bm{W}_{Q}^{\top}\) at initialization and \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) during training for different attention heads at different layers. (d) - (e) equivalent figures for \(\bm{W}_{V}\bm{W}_{O}^{\top}\).

Figure 17: CIFAR-100, ViT trained with Adam: Stable rank of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) (blue) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) (red) throughout training. Mean and standard deviation (shaded area) are computed across 8 heads per attention layer.

Figure 15: CIFAR-10, ViT trained with Adam: Stable rank of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) (blue) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) (red) throughout training. Mean and standard deviation (shaded area) are computed across 8 heads per attention layer.

Figure 19: ImageNet, ViT trained with Adam: Stable rank of \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) (blue) and \(\Delta\bm{W}_{V}\bm{W}_{O}^{\top}\) (red) throughout training. Mean and standard deviation (shaded area) are computed across 12 heads per attention layer.

Figure 18: ImageNet, ViT trained with Adam: normalized spectrum at different stages of training. (a) - (c) Normalized spectrum of \(\bm{W}_{K}\bm{W}_{Q}^{\top}\) at initialization and \(\Delta\bm{W}_{K}\bm{W}_{Q}^{\top}\) during training for different attention heads at different layers. (d) - (e) equivalent figures for \(\bm{W}_{V}\bm{W}_{O}^{\top}\).

Wikitext-103The gradual rank increase phenomenon also occurs in the NLP setting with language transformers. We trained GPT-2 on Wikitext-103 using the HuggingFace training script with Adam learning rate 3e-4, per-GPU batch-size 8, and block-length 256. We trained for 3 epochs on 2 A100 GPUs, which took 12 hours. See Figure 20.

Figure 20: Wikitext-103, GPT-2 trained with Adam: Stable rank of \(\Delta\bm{W}_{V}\bm{W}_{Q}^{\top}\) and \(\Delta\bm{W}_{Q}\bm{W}_{K}^{\top}\), versus training iteration. Stable rank of the perturbation increases gradually, but remains small throughout training.

Proof for dynamics of networks with diagonal parametrization (Theorem 4.1)

### Assumptions

Recall we have defined \(\bm{\theta}^{0},\dots,\bm{\theta}^{k},\dots\in\mathbb{R}^{2p}\) as the sequence of weights such that \(\bm{\theta}^{0}=\bm{0}\) and \(\bm{\theta}^{k+1}\) is defined inductively as follows. Consider the dynamics of \(\bm{\psi}^{k}(t,\epsilon)\in\mathbb{R}^{2p}\) initialized at \(\bm{\psi}^{k}(0,\epsilon)=\bm{\theta}^{k}+(\epsilon\bm{e}_{i_{k}},\mathrm{sgn }(g_{i}(\bm{\theta}^{k}))\epsilon\bm{e}_{i_{k}})\) and evolving according to the gradient flow \(\frac{d\bm{\psi}^{k}(t,\epsilon)}{dt}=-\nabla_{\bm{\theta}}\mathcal{L}(\bm{ \psi}^{k})\). We assume that there is a limiting point \(\bm{\theta}^{k+1}\) of these dynamics as \(\epsilon\) is taken small and the time is taken large:

\[\lim_{\epsilon\to 0}\lim_{t\to\infty}\bm{\psi}^{k}(t,\epsilon)=\bm{\theta}^{k+1}\,.\]

Under the above assumption that this sequence \(\bm{\theta}^{0},\dots,\bm{\theta}^{k},\dots\) is well-defined, we can derive a useful property of it for free. Namely, the conservation law (5) implies that \(\bm{u}\odot\bm{u}-\bm{v}\odot\bm{v}\) is preserved. It follows that for each \(k\) we have that \(\bm{\theta}^{k}=(\bm{u}^{k},\bm{v}^{k})\) satisfies \(|\bm{u}^{k}|=|\bm{v}^{k}|\) entrywise. In other words, there is \(\bm{s}^{k}\in\{+1,-1\}^{p}\) satisfying

\[\bm{\theta}^{k}=(\bm{u}^{k},\bm{s}^{k}\odot\bm{u}^{k})\in\mathbb{R}^{2p}\,.\]

We also abuse notation and write \(\mathrm{supp}(\bm{\theta}^{k}):=\mathrm{supp}(\bm{u}^{k})\subseteq[p]\), since the support of \(\bm{\theta}^{k}\) on the first \(p\) coordinates matches its support on the last \(p\) coordinates.

Having fixed this notation, we now recall the main assumptions of the theorem.

**Assumption C.1** (Nondegeneracy of dynamics in part (A); Assumption 4.3).: The initialization satisfies \(|u_{i}(0)|\neq|v_{i}(0)|\) for all \(i\). For stage \(k\), either \(T_{k+1}=\infty\) or there is a unique minimizer \(i_{k}\) to \(\min_{i}\Delta_{k}(i_{k})\) in (11). Finally, for all \(i\in\mathrm{supp}(\bm{\theta}^{k-1})\setminus\mathrm{supp}(\bm{\theta}^{k})\) we have \(g_{i}(\bm{\theta}^{k})\neq 0\).

**Assumption C.2** (Stationary points are strict local minima; Assumption 4.4).: For stage \(k\), there exist \(\delta_{k}>0\) and \(c_{k}>0\) such that for \(\tilde{\bm{u}}\in B(\bm{u}^{k},\delta)\) supported on \(\mathrm{supp}(\bm{u}^{k})\), we have

\[\mathcal{L}(\tilde{\bm{u}},\bm{s}^{k}\odot\tilde{\bm{u}})\geq c_{k}\|\bm{u}^{ k}-\tilde{\bm{u}}\|^{2}\,.\]

**Assumption C.3** (Noise-robustness of dynamics in part (B); Assumption 4.5).: For stage \(k\), either \(T_{k+1}=\infty\) or the following holds. For any \(\epsilon>0\), there are \(\delta>0\) and \(\tau:\mathbb{R}_{>0}\to\mathbb{R}\) such that the following holds. For any \(\tilde{\bm{u}}\in B(\bm{u}^{k},\delta)\cap\mathbb{R}_{\geq 0}^{p}\) supported on \(\mathrm{supp}(\tilde{\bm{u}})\subseteq\mathrm{supp}(\bm{u}^{k})\cup\{i_{k}\}\), there exists a unique solution \(\bm{\psi}:[0,\infty)\to\mathbb{R}^{p}\) of the gradient flow \(\frac{d\bm{v}}{dt}=-\nabla_{\bm{\theta}}\mathcal{L}(\bm{\psi})\) initialized at \(\bm{\psi}(0)=(\tilde{\bm{u}},\bm{s}^{k+1}\odot\tilde{\bm{u}})\), and at times \(t\geq\tau(\tilde{u}_{i_{k}})\),

\[\|\bm{\psi}(t)-\bm{\theta}^{k+1}\|<\epsilon\,.\]

### Rescaling time for notational convenience

For ease of notation, we rescale time

\[\bm{u}_{\alpha}(0)=\alpha\bm{u}(0), \bm{v}_{\alpha}(0)=\alpha\bm{v}(0)\] \[\frac{d\bm{u}_{\alpha}}{dt}=\log(1/\alpha)\bm{v}_{\alpha}\odot \bm{g}(\bm{u}_{\alpha},\bm{v}_{\alpha}), \frac{d\bm{v}_{\alpha}}{dt}=\log(1/\alpha)\bm{u}_{\alpha}\odot\bm{g}( \bm{u}_{\alpha},\bm{v}_{\alpha}).\] (14)

We also define

\[\bm{\theta}_{\alpha}(t)=(\bm{u}_{\alpha}(t),\bm{v}_{\alpha}(t))\in\mathbb{R}^ {2p}\,.\]

Because of this time-rescaling, we equivalently state Theorem 4.1 as:

**Theorem C.4** (Restatement of Theorem 4.1).: _Let \(K\in\mathbb{Z}_{\geq 0}\) be such that Assumptions 4.34.4 hold for all \(k\leq K\) and Assumption 4.5 holds for all \(k<K\). Then for any \(k\leq K\) and time \(t\in(T_{k},T_{k+1})\) the following holds. There is \(\alpha_{0}(t)>0\) such that for all \(\alpha<\alpha_{0}\), there exists a unique solution \(\bm{\theta}_{\alpha}:[0,t]\to\mathbb{R}^{p}\) to the gradient flow (14) and_

\[\lim_{\alpha\to 0}\bm{\theta}_{\alpha}(t)\to\bm{\theta}^{k}\,,\]

_where at each stage \(|\mathrm{supp}(\bm{u}^{k})\setminus\mathrm{supp}(\bm{u}^{k-1})|\leq 1\)._

For shorthand, we also write

\[S_{k}=\mathrm{supp}(\bm{u}^{k})\text{ and }S_{k}^{c}=[p]\setminus\mathrm{ supp}(\bm{u}^{k})\,.\]

### Simplifying problem without loss of generality

For each coordinate \(i\in[p]\) we have \(|u_{\alpha,i}(0)|\neq|v_{\alpha,i}(0)|\) by the non-degeneracy Assumption 4.3. So we can assume \(|u_{\alpha,i}(0)|>|v_{\alpha,i}(0)|\) without loss of generality. Furthermore, we can assume the entrywise inequality

\[\bm{u}_{\alpha}(0)>0\]

by otherwise training weights \(\tilde{\bm{u}}_{\alpha}(t),\tilde{\bm{v}}_{\alpha}(t)\) initialized at \(\tilde{\bm{u}}_{\alpha}(0)=\mathrm{sgn}(\bm{u}_{\alpha}(0))\bm{u}_{\alpha}(0)\) and \(\tilde{\bm{v}}_{\alpha}(0)=\mathrm{sgn}(\bm{v}_{\alpha}(0))\bm{v}_{\alpha}(0)\), as \(\tilde{\bm{u}}_{\alpha}(t)\odot\tilde{\bm{v}}_{\alpha}(t)=\bm{u}_{\alpha}(t )\odot\bm{v}_{\alpha}(t)\) at all times.

Since \(u_{\alpha,i}^{2}(t)-v_{\alpha,i}^{2}(t)=u_{\alpha,i}^{2}(0)-v_{\alpha,i}^{2}(0)\) by the conservation law (5), it holds that \(|u_{\alpha,i}(t)|>|v_{\alpha,i}(t)|\) throughout. So by continuity

\[\bm{u}_{\alpha}(t)>0\]

throughout training.

### Tracking the sum of the weights

We define

\[\bm{w}_{\alpha}(t)=\bm{u}_{\alpha}(t)+\bm{v}_{\alpha}(t)\,.\]

The reason for this definition is that during training we have

\[\frac{d\bm{w}_{\alpha}}{dt}=\log(1/\alpha)\bm{w}_{\alpha}\odot\bm{g}(\bm{ \theta}_{\alpha})\,,\] (15)

Notice that since that we have assumed \(u_{\alpha,i}(0)>|v_{\alpha,i}(0)|\) for each \(i\in[p]\) we have \(\bm{w}_{\alpha}(0)>0\) entrywise. So, by (15) for all \(t>0\),

\[\bm{w}_{\alpha}(t)>0\,.\]

It suffices to track \(\bm{w}_{\alpha}(t)\) because we can relate the log-scale magnitude of \(\bm{w}_{\alpha}(t)\) to the magnitudes of the corresponding coordinates in \(\bm{u}_{\alpha}(t)\) and \(\bm{v}_{\alpha}(t)\) - see technical Lemmas D.1 D.2 and D.3.

### Claimed invariants in proof of Theorem C.4

In order to prove Theorem C.4, we consider any gradient flow \(\bm{\theta}_{\alpha}:[0,T^{*}]\to\mathbb{R}^{p}\) solving (14) where \(T^{*}\in(T_{K},T_{K+1})\). For now, we focus only on proving properties of this gradient flow, and defer its existence and uniqueness to Section C.8.

We show the following invariants inductively on the stage \(k\). For any \(\epsilon>0\), any stage \(k\leq K\), there is \(\alpha_{k}:=\alpha_{k}(\epsilon)>0\) such that for all \(\alpha<\alpha_{k}\) the following holds. There are times \(\bar{t}_{k}:=\bar{t}_{k}(\alpha,\epsilon)\) and \(\underline{t}_{k+1}:=\underline{t}_{k+1}(\alpha,\epsilon)\), such that

\[\bar{t}_{k} \in[T_{k}-\epsilon,T_{k}+\epsilon]\,,\] (16) \[\underline{t}_{k+1} \in\begin{cases}[T_{k+1}-\epsilon,T_{k+1}+\epsilon]\,,&\text{ if }T_{k+1}<\infty\\ \{T^{*}\},&\text{ if }T_{k+1}=\infty\end{cases}\,.\] (17)

and the weights approximate the greedy limit for all times \(t\in[\bar{t}_{k},\underline{t}_{k+1}]\)

\[\|\bm{\theta}_{\alpha}(t)-\bm{\theta}^{k}\|<\epsilon\,,\] (18)

and the weights at times \(\bar{t}_{k}\) and \(\underline{t}_{k+1}\) are correctly estimated by the incremental learning dynamics on the logarithmic-scale

\[\|\log_{\alpha}(\bm{w}_{\alpha}(\bar{t}_{k}))-\bm{b}^{k}\|<\epsilon\] (19)

and if \(T_{k+1}<\infty\) then

\[\|\log_{\alpha}(\bm{w}_{\alpha}(\underline{t}_{k+1}))-\bm{b}^{k+1}\|<\epsilon\,.\] (20)

_Base case \(k=0\)_: Take \(\bar{t}_{0}(\alpha,\epsilon)=0\). Then statement (16) holds since \(T_{0}=0\). Notice that as \(\alpha\to 0\) we have that \(\bm{u}_{\alpha}(0),\bm{v}_{\alpha}(0)\to\bm{0}=\bm{u}^{0}\), and also \(\log_{\alpha}\bm{w}_{\alpha}(0)\to\bm{1}=\bm{b}^{0}\). So statement (19) follows if we take \(\alpha_{0}\) small enough. In Section C.6 we show how to construct time \(\underline{t}_{1}\) such that (18) and (20) hold.

_Inductive step_: Suppose that (16), (18), (19) and (20) hold for some iteration \(k<K\). We prove them for iteration \(k+1\). In Section C.7 we construct time \(\bar{t}_{k}\). In Section C.6 we construct time \(\underline{t}_{k+1}\).

Dynamics from time \(\bar{t}_{k}\) to time \(\underline{t}_{k+1}\) (Linear dynamics for \(O(\log(1/\alpha))\) unrescaled time)

Let \(k\leq K\), and suppose that we know that for any \(\bar{\epsilon}_{k}>0\), there is \(\bar{\alpha}_{k}(\bar{\epsilon}_{k})>0\) such that for all \(0<\alpha<\bar{\alpha}_{k}\), there is a time \(\bar{t}_{k}=\bar{t}_{k}(\alpha,\bar{\epsilon}_{k})\) satisfying

\[|T_{k}-\bar{t}_{k}| <\bar{\epsilon}_{k}\] \[\|\boldsymbol{\theta}_{\alpha}(\bar{t}_{k})-\boldsymbol{\theta}^ {k}\| <\bar{\epsilon}_{k}\] \[\|\log_{\alpha}(\boldsymbol{w}_{\alpha}(\bar{t}_{k}))- \boldsymbol{b}^{k}\| <\bar{\epsilon}_{k}\,.\]

#### c.6.1 Analysis in case where \(T_{k+1}<\infty\)

Consider first the case where \(T_{k+1}<\infty\). We show that, for any \(\underline{\epsilon}_{k+1}>0\), there is \(\rho_{k+1}(\underline{\epsilon}_{k+1})>0\) such that for all \(0<\rho<\rho_{k+1}(\bar{\epsilon}_{k+1})\) there is \(\underline{\alpha}_{k+1}(\rho,\underline{\epsilon}_{k+1})>0\) such that for all \(\alpha<\underline{\alpha}_{k+1}\), there is a time \(\underline{t}_{k+1}=\underline{t}_{k+1}(\alpha,\rho,\underline{\epsilon}_{k+1})\) satisfying

\[|T_{k+1}-\underline{t}_{k+1}| <\underline{\epsilon}_{k+1}\] (21) \[\|\boldsymbol{\theta}_{\alpha}(t)-\boldsymbol{\theta}^{k}\| <\underline{\epsilon}_{k+1}\text{ for all }t\in[\bar{t}_{k},\underline{t}_{k+1}]\] (22) \[\|\log_{\alpha}(\boldsymbol{w}_{\alpha}(\underline{t}_{k+1}))- \boldsymbol{b}^{k+1}\| <\underline{\epsilon}_{k+1}\] (23) \[u_{\alpha,i_{k}}(\underline{t}_{k+1}) \in[\rho,3\rho]\,,\] (24) \[\operatorname{sgn}(v_{\alpha,i_{k}}(\underline{t}_{k+1})) =s_{i_{k}}^{k+1}\,.\] (25)

For any \(\rho,\alpha\), let \(\bar{\epsilon}_{k}=\rho\underline{\epsilon}_{k+1}/(4p)\) and choose \(\bar{t}_{k}=\bar{t}_{k}(\alpha,\bar{\epsilon}_{k})\). Then define

\[\underline{t}_{k+1} =\underline{t}_{k+1}(\alpha,\rho,\underline{\epsilon}_{k+1})\] (26) \[=\inf\{t\in[\bar{t}_{k},\infty):\|\boldsymbol{u}_{\alpha,S_{k}^{ c}}(t)-\boldsymbol{u}_{\alpha,S_{k}^{c}}(\bar{t}_{k})\|+\|\boldsymbol{v}_{ \alpha,S_{k}^{c}}(t)-\boldsymbol{v}_{\alpha,S_{k}^{c}}(\bar{t}_{k})\|>4\rho\}\,.\]

Now we show that the weights \(\boldsymbol{\theta}_{\alpha}(t)\) cannot move much from time \(\bar{t}_{k}\) to \(\underline{t}_{k+1}\). The argument uses the local Lipschitzness of the loss \(\mathcal{L}\) (from technical Lemma D.7), and the strictness of \(\boldsymbol{\theta}^{k}\) as a stationary point (from Assumption 4.4).

**Lemma C.5** (Stability of active variables during part (A) of dynamics).: _There is \(\rho_{k+1}\) small enough and \(\underline{\alpha}_{k+1}(\rho)\) small enough depending on \(\rho\),such that for all \(\rho<\rho_{k+1}\) and \(\alpha<\underline{\alpha}_{k+1}\) and all \(t\in[\bar{t}_{k},\underline{t}_{k+1})\),_

\[\|\boldsymbol{\theta}_{\alpha}(t)-\boldsymbol{\theta}^{k}\|<\rho^{\prime}:= \max(24\rho,18\sqrt{\rho K_{R_{k}}/c_{k}})\,.\] (27)

_where \(c_{k}\) is the strict-minimum constant from Assumption 4.4 and \(K_{R_{k}}\) is the Lipschitzness constant from Lemma D.7 for the ball of radius \(R_{k}=\|\boldsymbol{\theta}^{k}\|+1\)._

Proof.: Assume by contradiction that (27) is violated at some time \(t<\underline{t}_{k+1}\). Let us choose the first such time

\[t^{*}=\inf\{t\in[\bar{t}_{k},\underline{t}_{k+1}):\|\boldsymbol{u}_{\alpha}(t ^{*})-\boldsymbol{u}^{k}\|+\|\boldsymbol{v}_{\alpha}(t^{*})-\boldsymbol{s}^{k} \odot\boldsymbol{u}^{k}\|\geq\rho^{\prime}\}\,.\]

Define \(\tilde{\boldsymbol{\theta}}=(\tilde{\boldsymbol{u}},\tilde{\boldsymbol{v}})\) by

\[\tilde{u}_{i}=\begin{cases}u_{\alpha,i}(t^{*}),&i\in S_{k}\\ 0,&i\not\in S_{k}\end{cases}\quad\text{ and }\quad\tilde{v}_{i}=\begin{cases}v_{ \alpha,i}(t^{*}),&i\in S_{k}\\ 0,&i\not\in S_{k}\end{cases}\,.\]

By the definition of \(\underline{t}_{k+1}\), this satisfies

\[\|\tilde{\boldsymbol{u}}-\boldsymbol{u}_{\alpha}(t^{*})\| =\|\boldsymbol{u}_{\alpha,S_{k}^{c}}(t^{*})\|\leq 4\rho+\| \boldsymbol{u}_{\alpha,S_{k}^{c}}(\bar{t}_{k})\|\leq 4\rho+\underline{\epsilon}_{k}<5 \rho\,,\] \[\|\tilde{\boldsymbol{v}}-\boldsymbol{v}_{\alpha}(t^{*})\| =\|\boldsymbol{v}_{\alpha,S_{k}^{c}}(t^{*})\|\leq 4\rho+\| \boldsymbol{v}_{\alpha,S_{k}^{c}}(\bar{t}_{k})\|\leq 4\rho+\underline{\epsilon}_{k}<5 \rho\,.\]

Also

\[\|\tilde{\boldsymbol{u}}-\boldsymbol{u}^{k}\|+\|\tilde{\boldsymbol{v}}- \boldsymbol{s}^{k}\odot\boldsymbol{u}^{k}\| =\|\boldsymbol{u}_{\alpha,S_{k}}(t^{*})-\boldsymbol{z}_{S_{k}}^{k}\|+\| \boldsymbol{v}_{\alpha,S_{k}}(t^{*})-\boldsymbol{s}_{S_{k}}^{k}\odot \boldsymbol{z}_{S_{k}}^{k}\|\geq\rho^{\prime}-10\rho\geq\rho^{\prime}/2\,.\]Using (a) the strict minimum Assumption 4.4 with constant \(c_{k}\), since \(\|\bar{\bm{\theta}}-\bm{\theta}^{k}\|\leq\rho^{\prime}\) and we take \(\rho^{\prime}\) small enough,

\[\mathcal{L}(\bm{\theta}_{\alpha}(t^{*})) \geq\mathcal{L}(\bar{\bm{\theta}})-4\rho K_{R_{k}}\overset{(a)}{ \geq}\mathcal{L}(\bm{\theta}^{k})-4\rho K_{R_{k}}+\frac{c_{k}(\rho^{\prime})^{2 }}{16}\] \[\geq\mathcal{L}(\bm{\theta}_{\alpha}(\bar{t}_{k}))-(4\rho+\bar{ \epsilon}_{k})K_{R_{k}}+\frac{c_{k}(\rho^{\prime})^{2}}{16}>\mathcal{L}(\bm{ \theta}_{\alpha}(\bar{t}_{k}))\,.\]

This is a contradiction because \(\mathcal{L}\) is nondecreasing along the gradient flow. 

**Lemma C.6** (Log-scale approximation is correct during part (A)).: _There are functions \(\rho_{k+1}(\underline{\epsilon}_{k+1})>0\) and \(\underline{\alpha}_{k+1}(\rho,\underline{\epsilon}_{k+1})>0\) such that for all \(\rho<\rho_{k+1}\) and \(\alpha<\underline{\alpha}_{k+1}\), and for all \(t\in(\bar{t}_{k},\underline{t}_{k+1})\) we have for a constant \(C\) depending on \(k\),_

\[\|\log_{\alpha}(\bm{w}_{\alpha}(t))-\bm{b}^{k}+(t-\bar{t}_{k})\bm{g}(\bm{ \theta}^{k})\|<\rho\underline{\epsilon}_{k+1}+C\rho^{\prime}(t-\bar{t}_{k})\,.\] (28)

_Furthermore, for all \(i\in S^{c}_{k}\) and \(t\in(\bar{t}_{k},\underline{t}_{k+1})\) we have_

\[\operatorname{sgn}(g_{i}(\bm{\theta}_{\alpha}(t)))=\operatorname{sgn}(g_{i}( \bm{\theta}^{k})).\] (29)

Proof.: By Lemma C.5 and Lemma D.7, there is a constant \(C\) depending on \(\bm{\theta}^{k}\) such that for all \(t\in(\bar{t}_{k},\underline{t}_{k+1})\),

\[\|\bm{g}(\bm{\theta}_{\alpha}(t))-\bm{g}(\bm{\theta}^{k})\|\leq C\rho^{\prime }\,.\]

For shorthand, write \(\bar{\bm{g}}(\bm{\theta}^{k})=\bm{g}(\bm{\theta}^{k})+C\rho^{\prime}\mathbf{1}\) and \(\underline{\bm{g}}(\bm{\theta}^{k})=\bm{g}(\bm{\theta}^{k})-C\rho^{\prime} \mathbf{1}\). Since \(\bm{w}_{\alpha}(t)>0\) entrywise as we have assumed without loss of generality (see Section C.3), we have the following entrywise inequalities

\[\underline{\bm{g}}(\bm{\theta}^{k})\odot\bm{w}_{\alpha}(t)<\bm{g}(\bm{\theta} _{\alpha}(t))\odot\bm{w}_{\alpha}(t)<\bar{\bm{g}}(\bm{\theta}^{k})\odot\bm{w} _{\alpha}(t)\,.\] (30)

Since the dynamics are given by \(\frac{d\bm{w}_{\alpha}}{dt}=\log(1/\alpha)\bm{g}(\bm{w}_{\alpha})\odot\bm{w}_ {\alpha}\),

\[\bm{w}_{\alpha}(\bar{t}_{k})e^{(t-\bar{t}_{k})\log(1/\alpha)\bm{g}(\bm{ \theta}^{k})}\leq\bm{w}_{\alpha}(t)\leq\bm{w}_{\alpha}(\bar{t}_{k})e^{(t- \bar{t}_{k})\log(1/\alpha)\bar{\bm{g}}(\bm{\theta}^{k})}\,.\]

Taking the logarithms with base \(\alpha\in(0,1)\),

\[(t-\bar{t}_{k})\underline{\bm{g}}(\bm{u}^{k})\leq\log_{\alpha}(\bm{w}_{ \alpha}(\bar{t}_{k}))-\log_{\alpha}(\bm{w}_{\alpha}(t))\leq(t-\bar{t}_{k}) \bar{\bm{g}}(\bm{u}^{k})\,.\]

The bound (28) follows since \(\|\log_{\alpha}(\bm{w}_{\alpha}(\bar{t}_{k}))-\bm{b}^{k}\|<\bar{\epsilon}_{k} <\rho\underline{\epsilon}_{k+1}\).

Finally, the claim (29) follows from (30) since \(\operatorname{sgn}(\bar{\bm{g}}(\bm{\theta}^{k}))=\operatorname{sgn}(\underline {\bm{g}}(\bm{\theta}^{k}))=\operatorname{sgn}(\bm{g}(\bm{\theta}^{k}))\) if we take \(\rho\) small enough. 

First, we show that the weights must move significantly by time roughly \(T_{k+1}\). This is because of the contribution of coordinate \(i_{k}\).

**Lemma C.7** (\(\underline{t}_{k+1}\) is not much larger than \(T_{k+1}\)).: _Suppose that \(T_{k+1}<\infty\). Then there are \(\rho_{k+1}(\underline{\epsilon}_{k+1})>0\) and \(\underline{\alpha}_{k+1}(\rho,\underline{\epsilon}_{k+1})>0\) such that for all \(\rho<\rho_{k+1}\) and \(\alpha<\underline{\alpha}_{k+1}\), the following holds._

\[\underline{t}_{k+1}<T_{k+1}+\underline{\epsilon}_{k+1}\,.\]

Proof.: Assume by contradiction that \(\underline{t}_{k+1}<T_{k+1}+\underline{\epsilon}_{k+1}\). For all times \(t\in[\bar{t}_{k},\min(\underline{t}_{k+1},T_{k+1}+\underline{\epsilon}_{k+1})]\), by Lemma C.6,

\[|\log_{\alpha}(w_{\alpha,i_{k}}(t)-b_{i_{k}}^{t}+(t-\bar{t}_{k})g_{i_{k}}(\bm{ \theta}^{k})|<O(\sqrt{\rho})\,.\]

Since we know \(|\Delta_{k}(i_{k})-(T_{k+1}-\bar{t}_{k})|<\bar{\epsilon}_{k}\) and \(b_{i}^{k}-\Delta_{k}(i_{k})g_{i_{k}}(\bm{\theta}^{k})\in\{0,2\}\), it follows that

\[\log_{\alpha}(w_{\alpha,i_{k}}(T_{k+1}+\underline{\epsilon}_{k+1}))\not\in(-| g_{i_{k}}(\bm{\theta}^{k})|(\underline{\epsilon}_{k+1}-\bar{\epsilon}_{k+1}),2+|g_{i_{k}}( \bm{\theta}^{k})|(\underline{\epsilon}_{k+1}-\bar{\epsilon}_{k+1}))+O(\sqrt{ \rho}).\]

By taking \(\rho\) small enough, we see that \(|g_{i_{k}}(\bm{\theta}^{k})|(\underline{\epsilon}_{k+1}-\bar{\epsilon}_{k+1})+O( \sqrt{\rho})>\delta>0\) for some \(\delta>0\) that is independent of \(\alpha\), so

\[\log_{\alpha}(w_{\alpha,i_{k}}(T_{k+1}+\underline{\epsilon}_{k+1}))\not\in(- \delta,2+\delta)\,.\]

So \(|u_{\alpha,i_{k}}(T_{k+1}+\underline{\epsilon}_{k+1})|>1\) by Lemma D.2. But by the construction of \(\underline{t}_{k+1}\) this means that \(\underline{t}_{k+1}<T_{k+1}+\underline{\epsilon}_{k+1}\).

Next, we show that until time \(\underline{t}_{k+1}\), none of the coordinates in \(S_{k}^{c}\) move significantly, with the possible exception of coordinate \(i_{k}\).

**Lemma C.8** (No coordinates in \(S_{k}^{c}\setminus\{i_{k}\}\) move significantly during part (A)).: _Suppose \(T_{k+1}<\infty\). Then there are \(\rho_{k+1}(\underline{\epsilon}_{k+1})>0\) and \(\underline{\alpha}_{k+1}(\rho,\underline{\epsilon}_{k+1})>0\) such that for all \(\rho<\rho_{k+1}\) and \(\alpha<\underline{\alpha}_{k+1}\), the following holds. There is a constant \(c>0\) depending on \(k\) such that for all \(i\in S_{k}^{c}\setminus\{i_{k}\}\) and \(t\in[\bar{t}_{k},\underline{t}_{k+1}]\),_

\[|u_{\alpha,i}(t)-u_{\alpha,i}(\bar{t}_{k})|,|v_{\alpha,i}(t)-v_{\alpha,i}(\bar {t}_{k})|<\alpha^{c}+\bar{\epsilon}_{k}\,.\]

Proof.: The previous lemma combined with the inductive hypothesis gives

\[\underline{t}_{k+1}-\bar{t}_{k}<\Delta_{k}(i_{k})+2\underline{\epsilon}_{k+1 }\setminus\{i_{k}\}.\]

We analyze the movement of each coordinate \(i\in S_{k}^{c}\setminus\{i_{k}\}\) by breaking into two cases:

* Coordinate \(i\neq i_{k}\) such that \(b_{i}^{k}\in(0,2)\). By Assumption 4.3, there is a unique winning coordinate so \(b_{i}^{k}-\tau g_{i}(\bm{\theta}^{k})\in(c,2-c)\) for some constant \(c>0\) for all \(\tau\in[0,\underline{t}_{k+1}-\bar{t}_{k}]\subseteq[0,\Delta_{k}(i_{k})+2 \underline{\epsilon}_{k+1}]\). By Lemma C.6, \(\log_{\alpha}(w_{\alpha,i}(t))\in(-c/2,2-c/2)\) for all times \(t\in[\bar{t}_{k},\underline{t}_{k+1}]\). So by Lemma D.1, \(|u_{\alpha,i}(t)|,|v_{\alpha,i}(t)|\leq\alpha^{c/4}\).
* Coordinate \(i\neq i_{k}\) such that \(b_{i}^{k}=0\). By Lemma D.4, we must be in the corner case where \(\overline{i\in S_{k-1}\cap S_{k}^{c}}\) (i.e., the coordinate was active in the previous stage but was dropped from the support in this stage). By Lemma D.4, since \(b_{i}^{k}=0\) we have \(g_{i}(\bm{\theta}^{k})<0\). By Lemma C.6, this means \(\operatorname{sgn}(g_{i}(\bm{\theta}_{\alpha}(t)))=\operatorname{sgn}(g_{i}( \bm{\theta}^{k}))<0\) for all \(t\in(\bar{t}_{k},\underline{t}_{k+1})\). We break the analysis into two parts. Since \(b_{i}^{k}=0\), the sign is \(s_{i}^{k}=+1\). The inductive hypothesis \(\|\bm{\theta}_{\alpha}(\bar{t}_{k})-\bm{\theta}^{k}\|<\bar{\epsilon}_{k}\) implies that \(|u_{\alpha,i}(\bar{t}_{k})-z_{i}^{k}|<\bar{\epsilon}_{k}\) and \(|v_{\alpha,i}(\bar{t}_{k})-z_{i}^{k}|<\bar{\epsilon}_{k}\). For small enough \(\bar{\epsilon}_{k}\) this means that \(\operatorname{sgn}(u_{\alpha,i}(\bar{t}_{k}))=\operatorname{sgn}(v_{\alpha,i} (\bar{t}_{k}))=+1\). Now let \(t^{*}=\min(\underline{t}_{k+1},\inf\{t>\bar{t}_{k}:v_{\alpha,i}(t)=0\})\). Since \(u_{\alpha,i}(t)>v_{\alpha,i}(t)\) without loss of generality (see Section C.3), we have \(\operatorname{sgn}(u_{\alpha,i}(t))=\operatorname{sgn}(v_{\alpha,i}(t))=+1\) for all \(t\in[\bar{t}_{k},t^{*}]\). So \(\frac{du_{\alpha,i}(t)}{dt},\frac{dv_{\alpha,i}(t)}{dt}<0\) for all \(t\in[\bar{t}_{k},t^{*}]\). So, for any \(t\in[\bar{t}_{k},t^{*}]\), \[|u_{\alpha,i}(t)-u_{\alpha,i}(\bar{t}_{k})|,|v_{\alpha,i}(t)-v_{\alpha,i}(\bar {t}_{k})|<\bar{\epsilon}_{k}\] Also, since \(\log_{\alpha}(w_{\alpha,i}(t^{*}))\approx 1\), by Lemma C.6 we have \(t^{*}>c>0\) for some constant \(c\) independent of \(\alpha\). So for all \(t\in[t^{*},\underline{t}_{k+1}]\) we have \(b_{i}^{k}-\tau g_{i}(\bm{\theta}^{k})\in(c,2-c)\) for some constant \(c>0\). So \(|u_{\alpha,i}(t)|,|v_{\alpha,i}(t)|\leq\alpha^{c/4}\) for all \(t\in[t^{*},\underline{t}_{k+1}]\). The conclusion follows by triangle inequality.
* Coordinate \(i\neq i_{k}\) such that \(b_{i}^{k}=2\). The analysis is analogous to the case \(b_{i}^{k}=0\), except that we have \(s_{i}^{k}=-1\) instead and \(g_{i}(\bm{\theta}^{k})>0\) by Lemma D.4.

Finally, we use this conclude that \(\underline{t}_{k+1}\approx T_{k+1}\) and that the weights at coordinate \(i_{k}\) are the only weights that change significantly, and by an amount approximately \(\rho\).

**Lemma C.9** (Coordinate \(i_{k}\) wins the part (A) race at time \(\underline{t}_{k+1}\approx T_{k+1}\)).: _Suppose that \(T_{k+1}<\infty\). Then there are \(\rho_{k+1}(\underline{\epsilon}_{k+1})>0\) and \(\underline{\alpha}_{k+1}(\rho,\underline{\epsilon}_{k+1})>0\) such that for all \(\rho<\rho_{k+1}\) and \(\alpha<\underline{\alpha}_{k+1}\), the following holds._

\[|\underline{t}_{k+1}-T_{k+1}|<\underline{\epsilon}_{k+1}\,,\] \[u_{\alpha,i_{k}}(\underline{t}_{k+1})\in[\rho,3\rho]\,,\] \[\operatorname{sgn}(v_{\alpha,i_{k}}(\underline{t}_{k+1}))=s_{i_{k}}^{k+1}\,.\]Proof.: Let us analyze the case that \(b^{k}_{i_{k}}\in(0,2)\). Notice that \(b^{k+1}_{i_{k}}=b^{k}_{i_{k}}-\Delta_{k}(i_{k})g_{i_{k}}(\bm{\theta}^{k})\in\{0,2\}\) and that if \(b^{k+1}_{i}=0\) then \(g_{i_{k}}(\bm{\theta}^{k})>0\) and if it is \(2\) then \(b^{k+1}_{i_{k}}=g_{i_{k}}(\bm{\theta}^{k})<0\). So by Lemma C.6, for all times \(t\in[\bar{t}_{k},\min(\underline{t}_{k+1},T_{k+1}-\underline{\epsilon}_{k+1})]\), we have \(w_{\alpha,i_{k}}(t)\in(c,2-c)\) for some \(c>0\). So for small enough \(\alpha\) by Lemma D.1, \(|u_{\alpha,i_{k}}(t)|,|v_{\alpha,i_{k}}(t)|\leq\alpha^{c/2}\). Combining this with Lemma C.8, we see that for \(t\in[\bar{t}_{k},\min(\underline{t}_{k+1},T_{k+1}-\underline{\epsilon}_{k+1})]\) we have

\[\|\bm{u}_{\alpha}(t)-\bm{u}_{\alpha}(\bar{t}_{k})\|+\|\bm{v}_{\alpha}(t)-\bm{ v}_{\alpha}(\bar{t}_{k})\|<2(\alpha^{c}+\bar{\epsilon}_{k})p<\rho\,,\]

for small enough \(\alpha\). So by definition of \(\underline{t}_{k+1}\) we must have \(\underline{t}_{k+1}>T_{k+1}-\underline{\epsilon}_{k+1}\). Combined with Lemma C.7, we conclude that \(|T_{k+1}-\underline{t}_{k+1}|<\underline{\epsilon}_{k+1}\), which is the first claim of the lemma. Furthermore, by Lemma C.8,

\[\sum_{i\in S^{c}_{k}\setminus\{i_{k}\}}|u_{\alpha,i}(\underline{t}_{k+1})-u_ {\alpha,i}(\bar{t}_{k})|+|v_{\alpha,i}(\underline{t}_{k+1})-v_{\alpha,i}(\bar {t}_{k})|\leq 2p(\alpha^{c}+\bar{\epsilon}_{k}))<\rho/2,\]

so by definition of \(\underline{t}_{k+1}\) and triangle inequality we have \(|u_{\alpha,i_{k}}(\underline{t}_{k+1})|+|v_{\alpha,i_{k}}(\underline{t}_{k+1 })|\geq 4\rho-\rho/2=7\rho/2\). Also, since \(u^{2}_{\alpha,i_{k}}(\underline{t}_{k+1})-v^{2}_{\alpha,i_{k}}(\underline{t}_ {k+1})=\Theta(\alpha^{2})\) we have \(u_{\alpha,i_{k}}(\underline{t}_{k+1})\in[\rho,3\rho]\). Finally, if \(b^{k+1}_{i_{k}}=2\), then \(s^{k+1}_{i_{k}}=-1\) and \(\log_{\alpha}(w_{\alpha,i_{k}}(\underline{t}_{k+1}))>1.5\) so \(\operatorname{sgn}(v_{\alpha,i_{k}}(t))<0\) by Lemma D.3; analogously, if \(b^{k+1}_{i_{k}}=0\), we have \(s^{k+1}_{i_{k}}=1\) and \(\log_{\alpha}(w_{\alpha,i_{k}}(\underline{t}_{k+1})<0.5\) so \(\operatorname{sgn}(v_{\alpha,i_{k}}(\underline{t}_{k+1})>0\).

The case \(b^{k}_{i_{k}}\in\{0,2\}\) can be proved similarly to the analysis in Lemma C.8, where one shows that during the first period of time the magnitudes of \(|u_{i_{k}}(t)|\) and \(|v_{i_{k}}(t)|\) decrease, until the sign of \(v_{i_{k}}\) flips and they once again increase.

We have shown the claims (21), (22), (23) (24), and (25) for the time \(\underline{t}_{k+1}\). In fact, if we let \(\underline{t}^{\prime}_{k+1}\in[\bar{t}_{k},\infty)\) be the first time \(t\) such that \(u_{\alpha,i_{k}}(t)=\rho\) we still have (21), (22), (23) and (25) by the same analysis as above, and (24) can be replaced with the slightly more convenient

\[u_{\alpha,i_{k}}(t^{\prime}_{k+1})=\rho\,.\]

#### c.6.2 Analysis in case where \(T_{k+1}=\infty\)

In this case that \(T_{k+1}\), we just have to show that the weights remain close to \(\bm{\theta}^{k}\). We show that for any \(\underline{\epsilon}_{k+1}>0\), there is \(\underline{\alpha}_{k+1}(\underline{\epsilon}_{k+1})>0\) such that for all \(\alpha<\underline{\alpha}_{k+1}\) and times \(t\in[T_{k}+\underline{\epsilon}_{k+1},T^{*}]\),

\[\|\bm{\theta}_{\alpha}(t)-\bm{\theta}^{k}\|<\underline{\epsilon}_{k+1}.\]

We can use Lemmas C.5 and C.6, which were developed for the case of \(T_{k+1}<\infty\), but still hold for \(T_{k+1}=\infty\). Lemma C.5 guarantees that the weights do not move much until time \(\underline{t}_{k+1}\), and so we only need to show that \(\underline{t}_{k+1}\geq T^{*}\) when we take \(\rho\) small enough. For this, observe that \(g_{i}(\bm{\theta}^{k})=0\) for all \(i\not\in S_{k}\), because otherwise \(T_{k+1}<\infty\). Therefore Lemma C.6 guarantees that until time \(\min(T_{*},\underline{t}_{k+1})\) all weights are close to the original on the logarithmic scale. Namely,

\[\|\log_{\alpha}(\bm{w}_{\alpha}(t))-\bm{b}^{k}\|<\rho\underline{\epsilon}_{k+1 }+C\rho^{\prime}(T^{*}-\bar{t}_{k})\]

Furthermore, by the non-degeneracy Assumption 4.3 we know that \(b^{k}_{i}\in(0,2)\) for all \(i\not\in S_{k}\) by Lemma D.4. So if we take \(\rho\) small enough and \(\underline{\alpha}_{k+1}\) small enough, we must have that \(\underline{t}_{k+1}\geq T^{*}\).

Dynamics from time \(\underline{t}_{k}\) to time \(\bar{t}_{k}\) (Nonlinear evolution for \(O(1)\) unrescaled time)

Suppose that we know for some \(k\leq K\) that for any \(\underline{\epsilon}_{k}>0\), there is \(\rho_{k}(\underline{\epsilon}_{k})>0\) such that for all \(\rho<\rho_{k}\) there is \(\underline{\alpha}_{k}(\rho,\underline{\epsilon}_{k})>0\) such that for all \(\alpha<\underline{\alpha}_{k}\), there is a time \(\underline{t}_{k}=\underline{t}_{k}(\alpha,\rho,\underline{\epsilon}_{k})\) satisfying

\[|T_{k}-\underline{t}_{k}| <\underline{\epsilon}_{k}\] (31) \[\|\bm{\theta}_{\alpha}(\underline{t}_{k})-\bm{\theta}^{k-1}\| <\underline{\epsilon}_{k}\] (32) \[\|\log_{\alpha}(\bm{w}_{\alpha}(\underline{t}_{k}))-\bm{b}^{k}\| <\underline{\epsilon}_{k}\] (33) \[u_{\alpha,i_{k-1}}(\underline{t}_{k}) =\rho\,,\] (34) \[\operatorname{sgn}(v_{\alpha,i_{k-1}}(\underline{t}_{k})) =s^{k}_{i_{k-1}}\,.\] (35)Now we will show that for any \(\bar{\epsilon}_{k}>0\), there is \(\bar{\alpha}_{k}=\bar{\alpha}_{k}(\bar{\epsilon}_{k})>0\) such that for all \(0<\alpha<\bar{\alpha}_{k}\), there is a time \(\bar{t}_{k}=\bar{t}_{k}(\alpha,\bar{\epsilon}_{k})\) satisfying

\[|T_{k}-\bar{t}_{k}| <\bar{\epsilon}_{k}\] (36) \[\|\bm{\theta}_{\alpha}(\bar{t}_{k})-\bm{\theta}^{k}\| <\bar{\epsilon}_{k}\] (37) \[\|\log_{\alpha}(\bm{w}_{\alpha}(\bar{t}_{k}))-\bm{b}^{k}\| <\bar{\epsilon}_{k}\] (38)

We give the construction for \(\bar{t}_{k}\). For any desired accuracy \(\bar{\epsilon}_{k}>0\) in this stage, we will construct an accuracy \(\underline{\epsilon}_{k}=\underline{\epsilon}_{k}(\bar{\epsilon}_{k})=\bar{ \epsilon}_{k}/3>0\). We will also construct a \(\rho=\rho(\underline{\epsilon}_{k})>0\) which is sufficiently small, and we will construct an cutoff for \(\alpha\) equal to \(\bar{\alpha}_{k}=\bar{\alpha}_{k+1}(\bar{\epsilon}_{k})>0\) which satisfies \(\bar{\alpha}_{k}<\underline{\alpha}_{k}(\rho,\underline{\epsilon}_{k})\). The values for these parameters \(\underline{\epsilon}_{k}\) and \(\rho\) and \(\bar{\alpha}_{k}\) will be chosen in the following lemma, and will depend only on \(\bar{\epsilon}_{k}\).

**Lemma C.10** (New local minimum reached in time \(O(1/\log(1/\alpha))\)).: _For any \(\bar{\epsilon}_{k}>0\), we can choose \(\bar{\alpha}_{k}=\bar{\alpha}_{k}(\bar{\epsilon}_{k})>0\) small enough so that, for any \(0<\alpha<\bar{\alpha}_{k}\), there is \(\bar{t}_{k}=\bar{t}_{k}(\alpha,\bar{\epsilon}_{k})\) for which conditions (36) to (38) hold._

_Furthermore, there is a constant \(C^{\prime\prime}\) independent of \(\alpha\) such that \(|\bm{\theta}_{\alpha}(t)|/|\bm{\theta}_{\alpha}(\underline{t}_{k})|\in[1/C^{ \prime\prime},C^{\prime\prime}]^{2p}\) at all times \(t\in[\underline{t}_{k},\bar{t}_{k}]\)._

Proof.: Let \(\underline{t}_{k}=\underline{t}_{k}(\alpha,\rho,\underline{\epsilon}_{k})\) be given by the induction. Let us compare the dynamics starting at \(\bm{\theta}_{\alpha}(\underline{t}_{k})\) with the dynamics starting at \(\tilde{\bm{\theta}}(\underline{t}_{k})=(\tilde{\bm{u}}(\underline{t}_{k}), \tilde{\bm{v}}(\underline{t}_{k}))\) which is given by

\[\tilde{u}_{i}(\underline{t}_{k})=\begin{cases}u_{\alpha,i}( \underline{t}_{k}),&i\in S_{k-1}\cup\{i_{k-1}\}\\ 0,&\text{otherwise}\end{cases}\quad\text{ and }\quad\tilde{v}_{i}(\underline{t}_{k})= \begin{cases}v_{\alpha,i}(\underline{t}_{k}),&i\in S_{k-1}\cup\{i_{k-1}\}\\ 0,&\text{otherwise}\end{cases}\]

and run with

\[\frac{d\tilde{\bm{\theta}}}{dt}=-\log(1/\alpha)\nabla_{\bm{w}} \mathcal{L}(\tilde{\bm{\theta}})\.\]

By Assumption 4.5 we know there exists a unique solution \(\tilde{\bm{\theta}}:[\underline{t}_{k},\infty)\to\mathbb{R}^{p}\) as long as we take \(\underline{\epsilon}_{k}\) small enough because \(\operatorname{supp}(\tilde{\bm{\theta}}(\underline{t}_{k}))=S_{k-1}\cup\{i_{k -1}\}\) and \(\|\tilde{\bm{\theta}}_{i}(\underline{t}_{k})-\bm{\theta}^{k-1}\|<\underline{ \epsilon}_{k}\). Furthermore, by Assumption 4.5 if we take \(\underline{\epsilon}_{k}\) small enough there must be a time \(\tau:=\tau(\bar{\epsilon}_{k},\rho)<\infty\) such that

\[\|\tilde{\bm{\theta}}(t)-\bm{\theta}^{k}\|<\bar{\epsilon}_{k}/2 \text{ for }t\geq\underline{t}_{k}+\tau/\log(1/\alpha)\] (39)

Define

\[\bar{t}_{k}=\underline{t}_{k}+\tau/\log(1/\alpha).\]

So for \(\alpha\) small enough, \(|T_{k}-\bar{t}_{k}|<2\underline{\epsilon}_{k}<\bar{\epsilon}_{k}\), proving (36).

We now compare \(\bm{\theta}_{\alpha}(\bar{t}_{k})\) with \(\tilde{\bm{\theta}}(\bar{t}_{k})\), and show that if we take \(\alpha\) small enough, then the dynamics of \(\tilde{\bm{\theta}}\) closely match the dynamics of \(\bm{\theta}_{\alpha}(t)\) for times \(\underline{t}_{k}+O(1/\log(1/\alpha))\). The argument uses Gronwall's inequality. Let \(t^{*}=\inf\{t>\underline{t}_{k}:\|\tilde{\bm{\theta}}(t^{*})-\bm{\theta}_{ \alpha}(t)\|>1/3\}\). For times \(t\in[\underline{t}_{k},t^{*})\) by Lemma D.7 we have

\[\|\frac{d}{dt}\tilde{\bm{\theta}}(t)-\frac{d}{dt}\bm{\theta}_{ \alpha}(t)\|=\log(1/\alpha)\|\nabla_{\bm{\theta}}\mathcal{L}(\tilde{\bm{ \theta}}(t))-\nabla_{\bm{\theta}}\mathcal{L}(\bm{\theta}_{\alpha}(t))\|\leq K_ {\tilde{\bm{\theta}}(t)}\log(1/\alpha)\|\tilde{\bm{\theta}}(t)-\bm{\theta}_{ \alpha}(t)\|,\]

where \(K_{\tilde{\bm{\theta}}(t)}\) is the smoothness constant from Lemma D.7. Note that since \(\|\tilde{\bm{\theta}}(t)\|<\infty\) for large enough \(t\) by (39), the trajectory of \(\tilde{\bm{\theta}}\) must lie in a compact set. Therefore, there must be a finite set of times \(s_{1},\ldots,s_{m}\in[\underline{t}_{k},t^{*})\) such that \(\cup_{t\in[\underline{t}_{k},t^{*})}B(\tilde{\bm{\theta}}(t),1/2)\subseteq \cup_{i=1}^{m}B(\tilde{\bm{\theta}}(s_{i}),3/4)\). So letting \(C=\max_{i=1}^{m}K_{\tilde{\bm{\theta}}(s_{i})}<\infty\) for all times \(t\in[\underline{t}_{k},t^{*})\) we have

\[\frac{d}{dt}\|\tilde{\bm{\theta}}(t)-\bm{\theta}_{\alpha}(t)\| \leq C\log(1/\alpha)\|\tilde{\bm{\theta}}(t)-\bm{\theta}_{\alpha}(t)\|\,.\]

By Gronwall's inequality, for all times \(t\in[\underline{t}_{k},t^{*})\),

\[\|\tilde{\bm{\theta}}(t)-\bm{\theta}_{\alpha}(t)\|\leq\|\tilde{\bm{\theta}}( \underline{t}_{k})-\bm{\theta}_{\alpha}(\underline{t}_{k})\|\exp(C\log(1/ \alpha)(t-\underline{t}_{k}))\,.\]We know from Lemma C.8 that there is a constant \(c>0\) such that for any small enough \(0<\alpha<\underline{\alpha}_{k}\), such that

\[\|\tilde{\bm{\theta}}(\underline{t}_{k})-\bm{\theta}_{\alpha}(\underline{t}_{k}) \|<\alpha^{c}\]

If we take \(\alpha\) small enough that \(\alpha^{c}\exp(C\tau)<\bar{\epsilon}_{k}/2<1/3\), we must have \(t^{*}>\underline{t}_{k}+\tau/\log(1/\alpha)\) and so we prove (37)

\[\|\bm{\theta}^{k}-\bm{\theta}_{\alpha}(\bar{t}_{k})\|\leq\bar{\epsilon}_{k}/2+ \|\tilde{\bm{\theta}}(\bar{t}_{k})-\bm{\theta}_{\alpha}(\bar{t}_{k})\|<\bar{ \epsilon}_{k}\,.\]

It remains to show that (38) is satisfied. Since \(\|\tilde{\bm{\theta}}(t)-\bm{\theta}_{\alpha}(t)\|<1/3\) for all \(t\in[\underline{t}_{k},\bar{t}_{k}]\), it holds that the trajectory of \(\bm{\theta}_{\alpha}(t)\) lies in a compact set. So by Lemma D.7 we have \(\|g(\bm{\theta}_{\alpha}(t))\|<C^{\prime}\) for some constant \(C^{\prime}\) at all times \(t\in[\underline{t}_{k},\bar{t}_{k}]\). Since \(\frac{1}{\log(1/\alpha)}|\frac{dw_{\alpha,i}}{dt}|=|w_{\alpha,i}(t)||g_{i}( \bm{w}_{\alpha}(t))|<C^{\prime}|w_{\alpha,i}(t)|\), we must have \(|w_{\alpha,i}(t)|/|w_{\alpha,i}(\underline{t}_{k})|\in[1/C^{\prime\prime},C^ {\prime\prime}]\) for some constant \(C^{\prime\prime}\) independent of \(\alpha\) and all \(t\in[\underline{t}_{k},\bar{t}_{k}]\). Therefore, (38) follows from (33). A similar argument shows that \(|\bm{\theta}_{\alpha}(t)/\bm{\theta}_{\alpha}(\underline{t}_{k})|\in[1/C^{ \prime\prime},C^{\prime\prime}]^{2p}\).

### Concluding the proof of Theorem c.4

We have shown that Theorem 4.1 is true for solutions \(\bm{\theta}_{\alpha}:[0,T^{*}]\to\mathbb{R}^{2p}\) to the gradient flow, where \(T_{*}\in(T_{K},T_{K+1})\). To establish Theorem C.4 it remains only to show that for any \(T_{*}\in(T_{K},T_{K+1})\) and small enough \(\alpha\) such a solution to the gradient flow exists and is unique. To see this, note that in the inductive proof of the invariants we construct a sequence of times \(0=\bar{t}_{0}\leq\underline{t}_{1}\leq\bar{t}_{1}\leq\cdots\leq\bar{t}_{K} \leq\underline{t}_{K+1}>T_{*}\), where we guarantee that any gradient flow solution \(\bm{\theta}_{\alpha}:[0,\underline{t}_{k+1}]\to\mathbb{R}^{p}\) satisfies \(\bm{\theta}_{\alpha}\in\cup_{k\in\{0,\dots,K\}}B(\bm{\theta}^{k},1)\) for all \(t\in\cup_{k\in\{0,\dots,K\}}[\bar{t}_{k},\underline{t}_{k+1}]\). And also for \(t\in\cup_{k\in\{0,\dots,K-1\}}[\underline{t}_{k},\bar{t}_{k+1}]\), we have \(\bm{\theta}_{\alpha}(t)\in B(0,C^{\prime\prime}_{k}\bm{\theta}^{k})\) for some constant \(C^{\prime\prime}_{k}\) independent of \(\alpha\) by Lemma C.10. So \(\bm{\theta}_{\alpha}(t)\in B(0,C_{K})\) for some constant \(C_{K}\) at all times \(t\in[0,T^{*}]\). By Lemma D.7, the loss gradient \(\nabla_{\bm{\theta}}\mathcal{L}(\bm{\theta})=(\bm{v}\odot\bm{g}(\bm{\theta}), \bm{u}\odot\bm{g}(\bm{\theta}))\) is Lipschitz-continuous on the compact set \(B(0,C_{K})\). So \(\bm{\theta}_{\alpha}:[0,T^{*}]\to\mathbb{R}^{p}\) exists and is unique by the Cauchy-Lipschitz theorem.

## Appendix D Technical lemmas

### Relating the sum of the weights to the original weights using the conservation law

**Lemma D.1**.: _If for some constant \(0<c<1\) we have \(\log_{\alpha}(w_{\alpha,i}(t))\in(c,2-c)\), then for small enough \(\alpha\)_

\[\max(|u_{\alpha,i}(t)|,|v_{\alpha,i}(t)|)\leq\alpha^{c/2}\,.\]

Proof.: Let \(\tilde{\bm{w}}_{\alpha}(t)=\bm{u}_{\alpha}(t)-\bm{v}_{\alpha}(t)\). By the conservation law (5), \(w_{\alpha,i}(t)\tilde{w}_{\alpha,i}(t)=w_{\alpha,i}(0)\tilde{w}_{\alpha,i}(0)= u_{\alpha,i}(0)^{2}-v_{\alpha,i}(0)^{2}\). By the non-degeneracy of initialization (Assumption 4.3), the right-hand-side is \(\Theta(\alpha^{2})\). So if \(\log_{\alpha}(w_{\alpha,i}(t))\in(c,2-c)\) then for small enough \(\alpha\), we have \(\log_{\alpha}(|\tilde{w}_{\alpha,i}(t)|)\in(3c/4,2-3c/4)\). So \(|u_{\alpha,i}(t)|\leq|w_{\alpha,i}(t)+\tilde{w}_{\alpha,i}(t)|\leq\alpha^{c/2}\) and \(|v_{\alpha,i}(t)|\leq|w_{\alpha,i}(t)-\tilde{w}_{\alpha,i}(t)|\leq\alpha^{c/2}\). 

**Lemma D.2**.: _If for some constant \(0<c\) we have \(\log_{\alpha}(w_{\alpha,i}(t))\not\in(-c,2+c)\), then for small enough \(\alpha\),_

\[|u_{\alpha,i}(t)|>1\,.\]

Proof.: Define \(\tilde{\bm{w}}_{\alpha}=\bm{u}_{\alpha}-\bm{v}_{\alpha}\) as in the proof of Lemma D.1. If \(\log_{\alpha}(w_{\alpha,i}(t))<-c\) then \(\log_{\alpha}(|\tilde{w}_{\alpha,i}(t)|)>2-c/2\) for small enough \(\alpha\), so \(u_{i}(t)>\alpha^{-c}-\alpha^{2-c/2}>1\). Similarly, if \(\log_{\alpha}(w_{\alpha,i}(t))>2+c\) then \(\log_{\alpha}(|\tilde{w}_{\alpha,i}(t)|)<-c/2\) so \(|u_{i}(\alpha)|>\alpha^{-c/2}-\alpha^{2+c}>1\). 

**Lemma D.3**.: _If for some constant \(c>0\), there is small enough \(\alpha\) such that if we have \(\log_{\alpha}(w_{\alpha,i}(t))>1+c\) then \(\operatorname{sgn}(v_{\alpha,i}(t))<0\). Otherwise, if \(\log_{\alpha}(w_{\alpha,i}(t))<1-c\) then \(\operatorname{sgn}(v_{\alpha,i}(t))>0\)._Proof.: Follows from \(\bm{v}_{\alpha}=\frac{1}{2}(\bm{w}_{\alpha}-\tilde{\bm{w}}_{\alpha})\). Recall that \(\bm{w}_{\alpha}(t)>0\) and notice that \(\tilde{\bm{w}}_{\alpha}(t)>0\). In the first case, \(w_{\alpha,i}(t)<\alpha^{1+c}\) and \(\tilde{w}_{\alpha,i}(t)>\alpha^{1-c/2}\). In the latter case \(w_{\alpha,i}(t)>\alpha^{1-c}\) and \(\tilde{w}_{\alpha,i}(t)<\alpha^{1+c/2}\). 

### Sign of gradients on coordinates that leave support

**Lemma D.4**.: _For any \(k\geq 1\) and \(i\in S^{c}_{k}\), if \(b^{k}_{i}\in\{0,2\}\) then we must have \(i\in\operatorname{supp}(\bm{u}^{k-1})\setminus\operatorname{supp}(\bm{u}^{k})\), and we must have \(g_{i}(\bm{u}^{k})<0\) if \(b^{k}_{i}=0\) and \(g_{i}(\bm{\theta}^{k})>0\) if \(b^{k}_{i}=2\). In particular, \(\Delta_{k}(i_{k})>0\) for all \(k\)._

Proof.: This is by induction on \(k\) and using the non-degeneracy Assumption 4.3. 

### Local lipschitzness and smoothness

We provide several technical lemmas on the local Lipschitzness and smoothness of \(\ell\), \(h\), and \(\bm{g}\).

**Lemma D.5**.: _The function \(\ell(\bm{y},\cdot)\) is locally Lipschitz and smooth in its second argument: for any \(R>0\), there exists \(K_{R}\) such that for any \(\bm{\zeta},\bm{\zeta}^{\prime}\in B(0,R)\)_

\[\left\lvert\ell(\bm{y},\bm{\zeta})-\ell(\bm{y},\bm{\zeta}^{\prime})\right\rvert \leq K_{R}\|\bm{\zeta}-\bm{\zeta}^{\prime}\|\]

\[\left\lVert D\ell(\bm{y},\bm{\zeta})-D\ell(\bm{y},\bm{\zeta}^{\prime})\right\rVert \leq K_{R}\|\bm{\zeta}-\bm{\zeta}^{\prime}\|,\]

_almost surely over \(\bm{y}\). Here \(D\ell(\bm{y},\cdot)^{\top}\in\mathbb{R}^{d_{out}}\) is the derivative in the second argument._

Proof.: Since \(\ell\) is continuously twice-differentiable, for each \(\bm{y}\in\mathbb{R}^{d_{y}},\bm{\zeta}\in\mathbb{R}^{d_{out}}\) there is \(K_{\bm{y},\bm{\zeta}}<\infty\) such that for all \(\bm{y}\in B(\bm{y},1/K_{\bm{y},\bm{\zeta}})\) and \(\bm{\zeta}^{\prime}\in B(\bm{\zeta},1/K_{\bm{y},\bm{\zeta}})\) we have

\[\left\lVert D\ell(\bm{y}^{\prime},\bm{\zeta}^{\prime})\right\rVert \leq K_{\bm{y},\bm{\zeta}}\quad\text{ and }\quad\left\lVert D^{2}\ell(\bm{y}^{\prime},\bm{\zeta}^{\prime})\right\rVert \leq K_{\bm{y},\bm{\zeta}}\,,\]

where \(D\ell\) and \(D^{2}\ell\) denote the first and second derivative in the second argument. So for all such \(\bm{y}^{\prime}\in B(\bm{y},1/K_{\bm{y},\bm{\zeta}})\) and \(\bm{\zeta}^{\prime},\bm{\zeta}^{\prime\prime}\in B(\bm{\zeta},1/K_{\bm{y},\bm {\zeta}})\) we have

\[\left\lvert\ell(\bm{y}^{\prime},\bm{\zeta}^{\prime})-\ell(\bm{y}^{\prime},\bm{ \zeta}^{\prime\prime})\right\rvert \leq K_{\bm{y},\bm{\zeta}}\|\bm{\zeta}^{\prime}-\bm{\zeta}^{\prime \prime}\|\quad\text{ and }\quad\left\lvert D\ell(\bm{y}^{\prime},\bm{\zeta}^{\prime})-D \ell(\bm{y}^{\prime},\bm{\zeta}^{\prime\prime})\right\rvert\leq K_{\bm{y},\bm {\zeta}}\|\bm{\zeta}^{\prime}-\bm{\zeta}^{\prime\prime}\|\,.\]

Cover the set \(\{(\bm{y},\bm{\zeta}):\|\bm{y}\|\leq C,\|\bm{\zeta}\|\leq R\}\) with the balls \(\cup_{\bm{y}}B(\bm{y},1/K_{\bm{y},\bm{\zeta}})\). By compactness, there is a finite subcover \((\bm{y}_{1},\bm{\zeta}_{1}),\dots,(\bm{y}_{r},\bm{\zeta}_{r})\), so we can take \(K_{R}=\max_{i\in[r]}K_{\bm{y}_{i},\bm{\zeta}_{i}}<\infty\) and the lemma holds since \(\|\bm{y}\|\leq C\) almost surely by Assumption 2.1. 

**Lemma D.6**.: _The function \(h(\bm{x};\cdot)\) is locally bounded, Lipschitz and smooth in its second argument: for any \(R>0\) there exists \(K_{R}\) such that for any \(\bm{\psi},\bm{\psi}^{\prime}\in B(0,R)\),_

\[\left\lVert h(\bm{x};\bm{\psi})\right\rVert \leq K_{R}\] \[\left\lVert h(\bm{x};\bm{\psi})-h(\bm{x};\bm{\psi}^{\prime})\right\rVert \leq K_{R}\|\bm{\psi}-\bm{\psi}^{\prime}\|\] \[\left\lVert Dh(\bm{x};\bm{\psi})-Dh(\bm{x};\bm{\psi}^{\prime})\right\rVert \leq K_{R}\|\bm{\psi}-\bm{\psi}^{\prime}\|\,,\]

_almost surely over \(\bm{x}\). Here \(Dh(\bm{x},\cdot)\in\mathbb{R}^{d_{out}}\times R^{p}\) is the derivative in the second argument._

Proof.: Analogous to proof of Lemma D.5, using continuous twice-differentiability of \(h\) and boundedness of \(\|\bm{x}\|\). 

**Lemma D.7** (Local Lipschitzness of loss and loss derivative).: _When \(\bm{\theta}=(\bm{u},\bm{v})\in\mathbb{R}^{2p}\) and \(f_{\mathsf{NN}}(\bm{x};\bm{\theta})=h(\bm{x};\bm{u}\odot\bm{u})\) the following holds for \(\bm{g}(\bm{\theta})\) defined in (4). For any \(R>0\), there exists \(K_{R}<\infty\) such that for any \(\bm{\theta},\bm{\theta}^{\prime}\in B(0,K_{R})\),_

\[\left\lVert\bm{g}(\bm{\theta})-\bm{g}(\bm{\theta}^{\prime})\right\rVert \leq K_{R}\|\bm{\theta}-\bm{\theta}^{\prime}\|\] \[\left\lVert\nabla_{\bm{\theta}}\mathcal{L}(\bm{\theta})-\nabla_{R} \mathcal{L}(\bm{\theta}^{\prime})\right\rVert \leq K_{\bm{\theta}}\|\bm{\theta}-\bm{\theta}^{\prime}\|\] \[\left\lvert\mathcal{L}(\bm{\theta})-\mathcal{L}(\bm{\theta}^{\prime})\right\rvert \leq K_{R}\|\bm{\theta}-\bm{\theta}^{\prime}\|\,.\]

Proof.: Let \(\bm{\theta}=(\bm{u},\bm{v}),\bm{\theta}^{\prime}=(\bm{u}^{\prime},\bm{v}^{ \prime})\). This follows immediately from the local Lipschitzness and smoothness of \(h\) and \(\ell\) in Lemmas D.5 and D.6, as well as

\[\left\lVert\bm{g}(\bm{\theta})-\bm{g}(\bm{\theta}^{\prime})\right\rVert =\left\lVert\,\mathbb{E}_{\bm{x},\bm{y}}[Dh(\bm{x};\bm{u}\odot\bm{ v})^{\top}D\ell(\bm{y},h(\bm{x};\bm{u}\odot\bm{v}))^{\top}-Dh(\bm{x};\bm{u}^{\prime} \odot\bm{v}^{\prime})^{\top}D\ell(\bm{y},h(\bm{x};\bm{u}^{\prime}\odot\bm{ v}^{\prime}))^{\top}]\right\rVert.\]