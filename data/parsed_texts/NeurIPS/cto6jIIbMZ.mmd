# Demystifying Softmax Gating Function in

Gaussian Mixture of Experts

 Huy Nguyen\({}^{\dagger}\)   TrungTin Nguyen\({}^{\diamond}\)   Nhat Ho\({}^{\dagger}\)

Department of Statistics and Data Sciences, The University of Texas at Austin\({}^{\dagger}\)

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK\({}^{\diamond}\)

{huynm, minhnhat}@utexas.edu, trung-tin.nguyen@inria.fr

###### Abstract

Understanding the parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating function: (i) the identifiability only up to the translation of parameters; (ii) the intrinsic interaction via partial differential equations between the softmax gating and the expert functions in the Gaussian density; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Voronoi loss functions among parameters and establishing the convergence rates of maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the true number of experts is unknown and over-specified, our findings show a connection between the convergence rate of the MLE and a solvability problem of a system of polynomial equations.

## 1 Introduction

Softmax gating Gaussian mixture of experts [32; 37], a class of statistical machine learning models that combine multiple simpler models, known as expert functions of the covariates, via softmax gating networks to form more complex and accurate models, has found widespread use in various applications, including speech recognition [51; 64; 65], natural language processing [14; 20; 17; 54; 21], computer vision [52; 3; 15; 44], and other applications [26; 49; 7; 8; 48; 5; 6]. Regarding the applications of the softmax gating Gaussian mixture of experts in medicine [43] and physical sciences [39], the parameters of each expert function play an important role in capturing the heterogeneity of data. Thus, the main objective of these works is to conduct statistical inference for those parameters, which leads to a need for convergence rates of parameter estimation in the softmax gating Gaussian mixture of experts. However, a comprehensive theoretical understanding of parameter estimation in that model has still remained a long-standing open problem in the literature.

Parameter estimation has been studied quite extensively in standard mixture models. In his seminal work, Chen et al. [9] established the convergence rate \(\mathcal{O}(n^{-1/4})\) of parameter estimation in over-fitted univariate mixture models, namely, the settings when the number of true components is unknown and over-specified, and the family of distributions is strongly identifiable in the second order, e.g., location Gaussian distributions. That slow and non-standard rate is due to the collapse of some parameters into single paramameter or the convergence of weights to zero, which leads to the singularity of Fisher information matrix around the true parameters. Then, Nguyen et al. [50] and Ho et al. [29] utilized Wasserstein metrics to achieve this rate under the multivariate settings of second-order strongly identifiable mixture models. Recently, Ho et al. [28] demonstrated that rates of the MLE can strictly depend on the number of over-specified components when the mixture models are not strongly identifiable.

identifiable, such as location-scale Gaussian mixtures. The minimax optimal behaviors of parameter estimation were studied in [27, 46]. From the computational side, the statistical guarantee of the expectation-maximization (EM), e.g., [12], and moment methods had also been studied under both exact-fitted [2, 1, 25] and over-fitted settings [19, 18, 61, 16, 62] of mixture models.

Compared to mixture models, there has been less research on parameter estimation of mixture of experts. When the gating networks are independent of the covariates, Ho et al. [30] employed the generalized Wasserstein loss function [59] to study the convergence rates of parameter estimation in Gaussian mixture of experts. They proved that these rates are determined by the algebraic independence of the expert functions and the partial differential equations with respect to the parameters. Later, Do et al. [13] extended these results to general mixture of experts with covariate-free gating network. Statistical guarantees of optimization methods for solving parameter estimation in Gaussian mixture of experts with covariate-free gating functions were studied in [11, 67, 41, 63]. When the gating networks are softmax functions, parameter estimation becomes more challenging to understand due to the complex structures of the softmax gating function in the Gaussian mixture of experts. Before describing these phenomena in further details, we begin by formally introducing the softmax gating Gaussian mixture of experts and related notions.

**Problem setting:** Assume that \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\in\mathbb{R}^{d}\times\mathbb{R}\) are i.i.d. samples drawn from the softmax gating Gaussian mixture of experts of order \(k_{*}\) whose conditional density function \(g_{G_{*}}(Y|X)\) is given by:

\[g_{G_{*}}(Y|X):=\sum_{i=1}^{k_{*}}\frac{\exp((\beta_{1i}^{*})^{\top}X+\beta_{0 i}^{*})}{\sum_{j=1}^{k_{*}}\exp((\beta_{1j}^{*})^{\top}X+\beta_{0j}^{*})}\cdot f (Y|(a_{i}^{*})^{\top}X+b_{i}^{*},\sigma_{i}^{*}),\] (1)

where \(f(.|\mu,\sigma)\) is a Gaussian density function with mean \(\mu\) and variance \(\sigma\). Here, we define \(G_{*}:=\sum_{i=1}^{k_{*}}\exp(\beta_{0i}^{*})\delta_{(\beta_{1i}^{*},a_{i}^{* },b_{i}^{*},\sigma_{i}^{*})}\) as a true but unknown _mixing measure_, that is, a combination of Dirac measures \(\delta\) associated with true parameters \(\theta_{i}^{*}:=(\beta_{0i}^{*},\beta_{1i}^{*},a_{i}^{*},b_{i}^{*},\sigma_{i} ^{*})\). Notably, \(G_{*}\) is not necessarily a probability measure as the summation of its weights can be different from one. For the purpose of the theory, we assume that \(\theta_{i}^{*}\in\Theta\subset\mathbb{R}\times\mathbb{R}^{d}\times\mathbb{R} ^{d}\times\mathbb{R}\times\mathbb{R}_{+}\) where \(\Theta\) is a compact set, and \(X\in\mathcal{X}\subset\mathbb{R}^{d}\) where \(\mathcal{X}\) is a bounded set. Furthermore, we let \((a_{1}^{*},b_{1}^{*},\sigma_{1}^{*}),\ldots,(a_{k_{*}}^{*},b_{k_{*}}^{*}, \sigma_{k_{*}}^{*})\) be pairwise distinct and at least one among \(\beta_{11}^{*},\ldots,\beta_{1k_{*}}^{*}\) be non-zero to guarantee the dependence of softmax gating function on the covariate \(X\). Finally, we assume that the covariate \(X\) follows a continuous distribution to ensure that the softmax gating Gaussian mixture of experts is at least identifiable up to translations (see Proposition 1).

**Maximum likelihood estimation.** Since the value of true order \(k_{*}\) is unknown in practice, to estimate the unknown parameters in the softmax gating Gaussian mixture of experts (1), we consider using maximum likelihood estimation (MLE) within a class of all mixing measures with at most \(k\) components, which is defined as follows:

\[\widehat{G}_{n}\in\operatorname*{arg\,max}_{G\in\mathcal{O}_{k}(\Theta)}\frac {1}{n}\sum_{i=1}^{n}\log(g_{G}(Y_{i}|X_{i})),\] (2)

where \(\mathcal{O}_{k}(\Theta):=\{G=\sum_{i=1}^{k^{\prime}}\exp(\beta_{0i})\delta_{( \beta_{1i},a_{i},b_{i},\sigma_{i})}:1\leq k^{\prime}\leq k\text{ and }(\beta_{0i},\beta_{1i},a_{i},b_{i},\sigma_{i})\in\Theta\}\). To guarantee that the MLE \(\widehat{G}_{n}\) is a consistent estimator of \(G_{*}\), we need \(k\geq k_{*}\). In this paper, we study the convergence rate of the MLE \(\widehat{G}_{n}\) to the true mixing measure \(G_{*}\) under both the _exact-fitted_ settings, namely when \(k=k_{*}\), and the _over-fitted_ settings, namely when \(k>k_{*}\), of the softmax gating Gaussian mixture of experts.

**Fundamental challenges from the softmax gating function:** There are three fundamental challenges arising from the softmax gating function that create various obstacles in our convergence analysis:

**(i)** Firstly, parameters \(\beta_{1i}^{*},\beta_{0i}^{*}\) of the softmax gating function are not identifiable as those of the covariate-independent gating function in previous work. Instead, they are identifiable up to translations, that is, the softmax gating value does not change when we translate \(\beta_{0i}^{*}\) to \(\beta_{0i}^{*}+t_{1}\) and \(\beta_{1i}^{*}\) to \(\beta_{1i}^{*}+t_{2}\) for any \(t_{1}\in\mathbb{R}\) and \(t_{2}\in\mathbb{R}^{d}\). As a consequence, we need to introduce an infimum operator in the Voronoi loss functions (see equations (4) and (6)) to deal with this issue.

**(ii)** Secondly, a key step in our proof techniques is to decompose the density discrepancy \(g_{\widehat{G}_{n}}(Y|X)-g_{G_{*}}(Y|X)\) into a linear combination of linearly independent elements using Taylor expansions.

However, since the numerators and denominators of softmax gating functions are dependent, we cannot apply the Taylor expansions directly to that density discrepancy as in previous work [30, 13]. Moreover, there are two intrinsic interactions between parameters of the softmax gating's numerators and the Gaussian density function via the following partial differential equations (PDEs), which induce a lot of linearly dependent derivative terms in the Taylor expansions:

\[\frac{\partial^{2}u}{\partial\beta_{1}\partial b}=\frac{\partial u}{\partial a };\qquad\frac{\partial^{2}u}{\partial b^{2}}=2\frac{\partial u}{\partial\sigma},\] (3)

where \(u(Y|X;\beta_{1},a,b,\sigma):=\exp(\beta_{1}^{\top}X)\cdot f(Y|a^{\top}X+b,\sigma)\). Therefore, it takes us great effort to group those linearly dependent terms together to obtain the desired linear combination of linearly independent terms.

**(iii)** Lastly, given the above linear combination of linearly independent elements, when the density estimation \(g_{\tilde{G}_{n}}(Y|X)\) converges to the true density \(g_{G_{*}}(Y|X)\), the associated coefficients in that combination also go to zero. Then, via some transformations, those limits lead to a system of polynomial equations introduced in equation (9). This system admits a much more complex structure than what considered in previous work [30, 13].

These fundamental challenges from the softmax gating function suggest that the previous loss functions, such as Wasserstein distance [50, 28, 30], being employed to study parameter estimation in standard mixture models or mixture of experts with covariate-free gating functions are no longer sufficient as they heavily rely on the assumptions that the weights of these models are independent of the covariates.

**Main contributions:** To tackle these challenges of the softmax gating function, we propose two novel Voronoi losses among parameters and establish the lower bounds of the Hellinger distance, denoted as \(h(\cdot,\cdot)\), of the mixing densities of softmax gating Gaussian mixture of experts in terms of these Voronoi losses to capture the behaviors of the MLE. Our results can be summarized as follows (see also Table 1):

**1. Exact-fitted settings**: When \(k=k_{*}\), we demonstrate that the Hellinger lower bound \(\mathbb{E}_{X}[h(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]\geq C\cdot\mathcal{D}_{1} (G,G_{*})\) holds for any mixing measure \(G\in\mathcal{O}_{k}(\Theta)\), where \(C\) is some universal constant and the Voronoi metric \(\mathcal{D}_{1}(G,G_{*})\) is defined as:

\[\mathcal{D}_{1}(G,G_{*}):=\inf_{t_{1},t_{2}}\ \sum_{j=1}^{k_{*}} \Bigg{[}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})\|(\Delta_{t_{2}}\beta_{1ij},\Delta a_{ij},\Delta b_{ij},\Delta\sigma_{ij})\|\\ +\Big{|}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})-\exp(\beta_{0j }^{*}+t_{1})\Big{|}\Bigg{]},\] (4)

where \(\Delta_{t_{2}}\beta_{1ij}:=\beta_{1i}-\beta_{1j}^{*}-t_{2}\), \(\Delta a_{ij}:=a_{i}-a_{j}^{*}\), \(\Delta b_{ij}:=b_{i}-b_{j}^{*}\), \(\Delta\sigma_{ij}:=\sigma_{i}-\sigma_{j}^{*}\). The infimum over \(t_{1}\in\mathbb{R}\) and \(t_{2}\in\mathbb{R}^{d}\) is to account for the identifiability up to the translation of \((\beta_{0j}^{*},\beta_{1j}^{*})_{j=1}^{k_{*}}\). Furthermore, \(\mathcal{A}_{j}\) is a Voronoi cell of mixing measure \(G\) generated by the true component \(\omega_{j}^{*}:=(\beta_{1j}^{*},a_{j}^{*},b_{j}^{*},\sigma_{j}^{*})\) for all \(1\leq j\leq k_{*}\)[47], which is defined as follows:

\[\mathcal{A}_{j}\equiv\mathcal{A}_{j}(G):=\{i\in\{1,2,\ldots,k\}:\|\omega_{i}- \omega_{j}^{*}\|\leq\|\omega_{i}-\omega_{\ell}^{*}\|,\ \forall\ell\neq j\},\] (5)

where we denote \(\omega_{i}:=(\beta_{1i},a_{i},b_{i},\sigma_{i})\). It is worth noting that the cardinality of each Voronoi cell \(\mathcal{A}_{j}\) indicates the number of components of \(G\) approximating the true component \(\omega_{j}^{*}\) of \(G_{*}\). As \(\mathbb{E}_{X}[h(g_{\tilde{G}_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]=\mathcal{O}( n^{-1/2})\), that lower bound of Hellinger distance indicates that \(\mathcal{D}_{1}(\widehat{G}_{n},G_{*})=\mathcal{O}(n^{-1/2})\). Therefore, the rates of estimating \(\exp(\beta_{0j}^{*}),\beta_{1j}^{*}\) (up to translations) and \(a_{j}^{*},\beta_{j}^{*},\sigma_{j}^{*}\) are of optimal order \(\mathcal{O}(n^{-1/2})\).

**2. Over-fitted settings**: When \(k>k_{*}\), the lower bound of Hellinger distance in terms of the Voronoi metric \(\mathcal{D}_{1}\) in the exact-fitted settings is no longer sufficient due to the collapse of softmax of vectors in possibly \(k\) dimensions to softmax of vectors in \(k_{*}\) dimensions. Our approach is to define more fine-grained Voronoi metric \(\mathcal{D}_{2}(G,G_{*})\) to capture such collapse, which is given by:

\[\mathcal{D}_{2}(G,G_{*}):=\inf_{t_{1},t_{2}}\sum_{j:|\mathcal{A}_{j} |>1}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})\Big{(}\|(\Delta_{t_{2}}\beta_{1ij },\Delta b_{ij})\|^{\bar{r}(|\mathcal{A}_{j}|)}+\|(\Delta a_{ij},\Delta\sigma_{ ij})\|^{\bar{r}(|\mathcal{A}_{j}|)/2}\Big{)}\] \[+\sum_{j:|\mathcal{A}_{j}|=1}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_ {0i})\|(\Delta_{t_{2}}\beta_{1ij},\Delta a_{ij},\Delta b_{ij},\Delta\sigma_{ ij})\|+\sum_{j=1}^{k_{*}}\Big{|}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})-\exp( \beta_{0j}^{*}+t_{1})\Big{|},\] (6)

for any mixing measure \(G\in\mathcal{O}_{k}(\Theta)\). Here, the values of function \(\bar{r}(\cdot)\) are determined by the solvability of a system of polynomial equations defined in equation (9). We then show in Lemma 1 that \(\bar{r}(2)=4\), \(\bar{r}(3)=6\), and we conjecture that \(\bar{r}(m)=2m\) for any \(m\geq 2\).

In high level, the aforementioned system of polynomial equations arises from the PDEs in equation (3) when we establish the lower bound \(\mathbb{E}_{X}[h(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]\geq C^{\prime}\mathcal{D} _{2}(G,G_{*})\) for any \(G\in\mathcal{O}_{k}(\Theta)\) for some universal constant \(C^{\prime}\). Since \(\mathbb{E}_{X}[h(g_{\bar{G}_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]=\mathcal{O}(n^ {-1/2})\), we also have \(\mathcal{D}_{2}(\widehat{G}_{n},G_{*})=\mathcal{O}(n^{-1/2})\) under the over-fitted settings of the softmax gating Gaussian mixture of experts. As a consequence, the rates for estimating true parameters whose Voronoi cells have only one component of the MLE are of order \(\mathcal{O}(n^{-1/2})\). On the other hand, for true parameters \(\exp(\beta_{0j}^{*}),\beta_{1j}^{*},a_{j}^{*},b_{j}^{*},\sigma_{j}^{*}\) whose Voronoi cells have more than one component of the MLE, the estimation rates are respectively \(\mathcal{O}(n^{-1/2\bar{r}(|\mathcal{A}_{j}|)})\) for \(\beta_{1j}^{*},b_{j}^{*}\), \(\mathcal{O}(n^{-1/\bar{r}(|\mathcal{A}_{j}|)})\) for \(a_{j}^{*},\sigma_{j}^{*}\), and \(\mathcal{O}(n^{-1/2})\) for \(\exp(\beta_{0j}^{*})\). This rich spectrum of parameter estimation rates is due to the complex interaction between the softmax gating and the expert functions.

**Practical implications:** Although the slow rates of the MLE under the over-fitted settings of the softmax gating Gaussian mixture of experts may seem discouraging, a practical implication of these results is that we should not choose the number of experts \(k\) to be very large compared to the true number of experts \(k_{*}\). Furthermore, the slow rates can also be useful for post-processing procedures, such as merge-truncate-merge procedure [24], with the MLE to reduce the number of experts so as to consistently estimate \(k_{*}\) when the number of data is sufficiently large. In particular, an important insight from the theoretical results is that we can merge the MLE parameters that are close and within the range of their rates of convergence or truncate the parameters that lead to small weights of the experts. As the sample size becomes sufficiently large, the reduced number of experts may converge to the true number of experts. We leave an investigation of such model selection with the Gaussian mixture of experts via the rates of MLE for future work.

**Organization:** The paper is organized as follows. In Section 2, we first provide background on the identifiability and rate of conditional density estimation in the softmax gating Gaussian mixture of experts. Next, we proceed to establish the convergence rate of the MLE under both the exact-fitted and over-fitted settings of these models in Section 3. Then, we conclude the paper with a few discussions in Section 4. Finally, full proofs of the results and a simulation study are provided in the Appendices.

**Notation:** Firstly, we denote \([n]:=\{1,2,\ldots,n\}\) for any positive integer \(n\). Next, for any vector \(u\in\mathbb{R}^{d}\) and \(z:=(z_{1},z_{2},\ldots,z_{d})\in\mathbb{N}^{d}\), we denote \(u^{z}=u_{1}^{z_{1}}u_{2}^{z_{2}}\ldots u_{d}^{z_{d}}\), \(|u|:=u_{1}+u_{2}+\ldots+u_{d}\) and \(z!:=z_{1}!z_{2}!\dots z_{d}!\), while \(\|u\|\) represents for its \(2\)-norm value. Additionally, the notation \(|A|\) indicates the cardinality of any set \(A\). Given any two positive sequences \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\), we write \(a_{n}=\mathcal{O}(b_{n})\) or \(a_{n}\lesssim b_{n}\) if \(a_{n}\leq Cb_{n}\) for all \(n\in\mathbb{N}\), where \(C>0\) is some universal constant. Lastly, for any two probability density functions \(p,q\) dominated by the Lebesgue measure \(\mu\), we denote

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline
**Setting** & **Loss Function** & \(g_{G_{*}}(Y|X)\) & \(\exp(\beta_{0j}^{*})\) & \(\beta_{1j}^{*},b_{j}^{*}\) & \(a_{j}^{*},\sigma_{j}^{*}\) \\ \hline Exact-fitted & \(\mathcal{D}_{1}\) & \(\mathcal{O}(n^{-1/2})\) & \(\mathcal{O}(n^{-1/2})\) & \(\mathcal{O}(n^{-1/2})\) & \(\mathcal{O}(n^{-1/2})\) \\ \hline Over-fitted & \(\mathcal{D}_{2}\) & \(\mathcal{O}(n^{-1/2})\) & \(\mathcal{O}(n^{-1/2})\) & \(\mathcal{O}(n^{-1/2\bar{r}(|\mathcal{A}_{j}|)})\) & \(\mathcal{O}(n^{-1/\bar{r}(|\mathcal{A}_{j}|)})\) \\ \hline \end{tabular}
\end{table}
Table 1: Summary of density estimation and parameter estimation rates in the softmax gating Gaussian mixture of experts under both the exact-fitted and over-fitted settings. Recall that the cardinality of each Voronoi cell \(\mathcal{A}_{j}\) gives the number of fitted components approximating true component \(\omega_{j}^{*}=(\beta_{1j}^{*},a_{j}^{*},b_{j}^{*},\sigma_{j}^{*})\) (see equation (5)). Furthermore, the notation \(\bar{r}(|\mathcal{A}_{j}|)\) stands for the solvability of the system of polynomial equations (9). For instance, if \(\omega_{j}^{*}\) is fitted by two components, then we have \(|\mathcal{A}_{j}|=2\) and \(\bar{r}(|\mathcal{A}_{j}|)=4\). Please refer to Lemma 1 for more details of the values of function \(\bar{r}\).

\(h^{2}(p,q)=\frac{1}{2}\int(\sqrt{p}-\sqrt{q})^{2}d\mu\) as the their squared Hellinger distance and \(V(p,q)=\frac{1}{2}\int|p-q|d\mu\) as their Total Variation distance.

## 2 Background

In this section, we begin with the following result on the identifiability of the softmax gating Gaussian mixture of experts, which was previously studied in [36].

**Proposition 1** (Identifiability of the softmax gating Gaussian mixture of experts).: _For any mixing measures \(G=\sum_{i=1}^{k}\exp(\beta_{0i})\delta_{(\beta_{1i},a_{i},b_{i},\sigma_{i})}\) and \(G^{\prime}=\sum_{i=1}^{k^{\prime}}\exp(\beta^{\prime}_{0i})\delta_{(\beta^{ \prime}_{1i},a^{\prime}_{i},b^{\prime}_{i},\sigma^{\prime}_{i})}\), if we have \(g_{G}(Y|X)=g_{G^{\prime}}(Y|X)\) for almost surely \((X,Y)\), then it follows that \(k=k^{\prime}\) and \(G\equiv G^{\prime}_{t_{1},t_{2}}\) where \(G^{\prime}_{t_{1},t_{2}}:=\sum_{i=1}^{k^{\prime}}\exp(\beta^{\prime}_{0i}+t_{ 1})\delta_{(\beta^{\prime}_{1i}+t_{2},a^{\prime}_{i},b^{\prime}_{i},\sigma^{ \prime}_{i})}\) for some \(t_{1}\in\mathbb{R}\) and \(t_{2}\in\mathbb{R}^{d}\)._

Proof of Proposition 1 is in Appendix B.1. The identifiability of the softmax gating Gaussian mixture of experts guarantees that the MLE \(\widehat{G}_{n}\) (2) converges to the true mixing measure \(G_{*}\) (up to the translation of the parameters in the softmax gating).

Given the consistency of the MLE, it is natural to ask about its convergence rate to the true parameters. Our next result establishes the convergence rate of conditional density estimation \(g_{\widehat{G}_{n}}(Y|X)\) to the true conditional density \(g_{G_{*}}(Y|X)\), which lays an important foundation for the study of MLE's convergence rate.

**Proposition 2** (Density estimation rate).: _Given the MLE in equation (2), the conditional density estimation \(g_{\widehat{G}_{n}}(Y|X)\) has the following convergence rate:_

\[\mathbb{P}(\mathbb{E}_{X}[h(g_{\widehat{G}_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]> C(\log(n)/n)^{1/2})\lesssim\exp(-c\log n),\]

_where \(c\) and \(C\) are universal constants._

Proof of Proposition 2 is in Appendix B.2. The result of Proposition 2 indicates that under either the exact-fitted or over-fitted settings of the softmax gating Gaussian mixture of experts, the rate of the conditional density function \(g_{\widehat{G}_{n}}(Y|X)\) to the true one \(g_{G_{*}}(Y|X)\) under Hellinger distance is of order \(\mathcal{O}(n^{-1/2})\) (up to some logarithmic factors), which is parametric on the sample size.

**From density estimation to parameter estimation:** The parametric rate of the conditional density estimation in Proposition 2 suggests that as long as we can establish the Hellinger lower bound \(\mathbb{E}_{X}[h(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]\gtrsim\mathcal{D}(G,G_{*})\) for any mixing measure \(G\in\mathcal{O}_{k}(\Theta)\) for some metric \(\mathcal{D}\) among the parameters, then we obtain directly the parametric convergence rate of the MLE under the metric \(\mathcal{D}\). Therefore, the main focus of the next section is to determine such metric \(\mathcal{D}\) and to establish that lower bound under either exact-fitted or over-fitted settings of the Gaussian mixture of experts.

## 3 Convergence Rate of the Maximum Likelihood Estimation

In this section, we first study the convergence rate of the MLE under the exact-fitted settings of the softmax gating Gaussian mixture of experts in Section 3.1. Then, we move to the over-fitted settings in Section 3.2. Finally, we provide a proof sketch of the theories in Section 3.3.

### Exact-fitted Settings

For the exact-fitted settings, namely, when the chosen number of experts \(k\) is equal to the true number of experts \(k_{*}\), as we mentioned in the introduction, the proper metric between the MLE and the true mixing measure is the metric \(\mathcal{D}_{1}\) defined in equation (4), which is given by:

\[\mathcal{D}_{1}(G,G_{*}):=\inf_{t_{1},t_{2}}\ \sum_{j=1}^{k_{*}} \Bigg{[}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})\|(\Delta_{t_{2}}\beta_{1ij},\Delta a_{ij},\Delta b_{ij},\Delta\sigma_{ij})\|\] \[+\Big{|}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})-\exp(\beta^{*}_ {0j}+t_{1})\Big{|}\Bigg{]},\]where \(\Delta_{t_{2}}\beta_{1ij}:=\beta_{1i}-\beta_{1j}^{*}-t_{2}\), \(\Delta a_{ij}:=a_{i}-a_{j}^{*}\), \(\Delta b_{ij}:=b_{i}-b_{j}^{*}\), \(\Delta\sigma_{ij}:=\sigma_{i}-\sigma_{j}^{*}\). Here, \(\mathcal{A}_{j}\) is a Voronoi cell of \(G\) generated by \((\beta_{1j}^{*},a_{j}^{*},b_{j}^{*},\sigma_{j}^{*})\) for all \(1\leq j\leq k_{*}\). Furthermore, the infimum is taken with respect to \((t_{1},t_{2})\in\mathbb{R}\times\mathbb{R}^{d}\) such that \(\beta_{0j}^{*}+t_{1}\) and \(\beta_{1j}^{*}+t_{2}\) still lie inside the domain of the parameter space \(\Theta\).

It is clear that \(\mathcal{D}_{1}(G,G_{*})=0\) if and only if \(G\equiv G_{*}\) (up to translation). When \(\mathcal{D}_{1}(G,G_{*})\) is sufficiently small, there exist \(t_{1},t_{2}\) such that all of \(\Delta_{t_{2}}\beta_{1ij}\), \(\Delta a_{ij}\), \(\Delta b_{ij}\), \(\Delta\sigma_{ij}\), and \(\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})-\exp(\beta_{0j}^{*}+t_{1})\) are sufficiently small as well. Therefore, the loss function \(\mathcal{D}_{1}\) provides a useful metric to measure the difference between the MLE and the true mixing measure. For any fixed \(t_{1},t_{2}\), the computation of the summations in \(\mathcal{D}_{1}\) only has the complexity of the order \(\mathcal{O}(k_{*}^{2})\). To solve the optimization with respect to \(t_{1},t_{2}\) in the metric \(\mathcal{D}_{1}\), we can utilize the projected subgradient method with fixed step size [4], which has the complexity of the order \(\mathcal{O}(\varepsilon^{-2})\) as the functions of \(t_{1}\) and \(t_{2}\) are convex where \(\varepsilon\) is a desired tolerance. Therefore, the total computational complexity of approximating the value of the Voronoi loss function \(\mathcal{D}_{1}\) is at the order of \(\mathcal{O}(k_{*}^{2}/\varepsilon^{2})\).

The following result establishes the lower bound of the Hellinger distance between the conditional densities in terms of the loss function \(\mathcal{D}_{1}\) between corresponding mixing measures, which in turn leads to the convergence rate of the MLE.

**Theorem 1**.: _Given the exact-fitted settings of the softmax gating Gaussian mixture of experts (1), i.e., \(k=k_{*}\), we find that_

\[\mathbb{E}_{X}[h(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]\geq C_{1}\cdot\mathcal{D} _{1}(G,G_{*}),\] (7)

_for any \(G\in\mathcal{E}_{k_{*}}(\Theta):=\mathcal{O}_{k_{*}}(\Theta)\setminus\mathcal{ O}_{k_{*}-1}(\Theta)\) where \(C_{1}\) is some universal constant depending only on \(G_{*}\) and \(\Theta\). As a consequence, there exist universal constants \(C_{1}^{\prime}\) and \(c_{1}\) such that the convergence rate of the MLE \(\widehat{G}_{n}\) under the exact-fitted settings satisfies:_

\[\mathbb{P}(\mathcal{D}_{1}(\widehat{G}_{n},G_{*})>C_{1}^{\prime}(\log(n)/n)^{1 /2})\lesssim\exp(-c_{1}\log n).\] (8)

Proof of Theorem 1 is in Appendix A.1. The parametric convergence rate of the MLE to \(G_{*}\) under the metric \(\mathcal{D}_{1}\) suggests that the rates of estimating the true parameters \(\exp(\beta_{0j}^{*}),\beta_{1j}^{*}\) (up to translation), \(a_{j}^{*},b_{j}^{*},\sigma_{j}^{*}\) for \(j\in[k_{*}]\) are of order \(\mathcal{O}(n^{-1/2})\), which are optimal up to logarithmic factors.

### Over-fitted Settings

We now consider the over-fitted settings of the softmax gating Gaussian mixture of experts. Different from the exact-fitted settings, the softmax weights associated with the MLE collapse to the softmax weights of the mixture of true experts as long as the MLE approaches the true mixing measure \(G_{*}\). More concretely, we can relabel the supports of the MLE \(\widehat{G}_{n}\) with \(\hat{k}_{n}\) components (\(\hat{k}_{n}\leq k\)) based on the Voronoi cells \(\mathcal{A}_{j}^{n}:=\mathcal{A}_{j}(\widehat{G}_{n})\) such that we can rewrite it as \(\widehat{G}_{n}=\sum_{j=1}^{k_{*}}\sum_{i\in\mathcal{A}_{j}^{n}}\exp(\widehat{ \beta}_{0i}^{*})\delta_{(\widehat{\beta}_{1i^{*}}^{n},\widehat{\alpha}_{i}^{n},\widehat{\beta}_{i^{*}}^{n},\widehat{\beta}_{i^{*}}^{n})}\) where \(\sum_{j=1}^{k_{*}}|\mathcal{A}_{j}^{n}|=\hat{k}_{n}\), \((\widehat{\alpha}_{i}^{n},\widehat{b}_{i}^{n},\widehat{\sigma}_{i}^{n})\to(a_ {j}^{*},b_{j}^{*},\sigma_{j}^{*})\),

\[\sum_{i\in\mathcal{A}_{j}^{n}}\frac{\exp((\widehat{\beta}_{1i}^{n})^{\top}X+ \widehat{\beta}_{0i}^{n})}{\sum_{j^{\prime}=1}^{k_{*}}\sum_{i^{\prime}\in \mathcal{A}_{j}^{n}}\exp((\widehat{\beta}_{1i^{\prime}}^{n})^{\top}X+\widehat {\beta}_{0i^{\prime}}^{n})}\to\frac{\exp((\beta_{1j^{\prime}}^{*})^{\top}X+ \beta_{0j}^{*})}{\sum_{j^{\prime}=1}^{k_{*}}\exp((\beta_{1j^{\prime}}^{*})^{ \top}X+\beta_{0j^{\prime}}^{*})}\]

as \(n\) approaches infinity for all \(1\leq i\leq\mathcal{A}_{j}^{n}\) and \(j\in[k_{*}]\).

The collapse of the softmax weights along with the PDEs (3) between the softmax gating and the expert functions in the Gaussian density create a complex interaction among the estimated parameters. To disentangle such interaction, we rely on the solvability of a novel system of polynomial equations defined in equation (9). In particular, for any \(m\geq 2\), we define \(\bar{r}(m)\) as the smallest natural number \(r\) such that the following system of polynomial equations:

\[\sum_{j=1}^{m}\sum_{(\alpha_{1},\alpha_{2},\alpha_{3},\alpha_{4})\in\mathcal{I} _{\ell_{1},\ell_{2}}}\frac{p_{5j}^{\alpha_{1}}p_{1j}^{\alpha_{1}}\;p_{2j}^{ \alpha_{2}}\;p_{3j}^{\alpha_{3}}\;p_{4j}^{\alpha_{4}}}{\alpha_{1}!\;\alpha_{2}! \;\alpha_{3}!\;\alpha_{4}!}=0,\] (9)

for any \((\ell_{1},\ell_{2})\in\mathbb{N}^{d}\times\mathbb{N}\) such that \(0\leq|\ell_{1}|\leq r\), \(0\leq\ell_{2}\leq r-|\ell_{1}|\) and \(|\ell_{1}|+\ell_{2}\geq 1\), does not have any non-trivial solution for the unknown variables \(\{p_{1j},p_{2j},p_{3j},p_{4j},p_{5j}\}_{j=1}^{m}\), namely, all of \(p_{5j}\) are non-zero and at least one among \(p_{3j}\) is different from zero. The ranges of \(\alpha_{1},\alpha_{2},\alpha_{3},\alpha_{4}\) in the above sum satisfy \(\mathcal{I}_{\ell_{1},\ell_{2}}=\{\alpha=(\alpha_{1},\alpha_{2},\alpha_{3},\alpha_ {4})\in\mathbb{N}^{d}\times\mathbb{N}^{d}\times\mathbb{N}\times\mathbb{N}:\ \alpha_{1}+\alpha_{2}=\ell_{1},\ | \alpha_{2}|+\alpha_{3}+2\alpha_{4}=\ell_{2}\}\). When \(d=1\) and \(r=2\), that system of equations becomes

\[\sum_{j=1}^{m}p_{5j}^{2}p_{1j}=0,\ \sum_{j=1}^{m}p_{5j}^{2}p_{1j}^{2}=0, \ \sum_{j=1}^{m}p_{5j}^{2}(p_{1j}p_{3j}+p_{2j})=0,\] \[\sum_{j=1}^{m}p_{5j}^{2}p_{3j}=0,\ \sum_{j=1}^{m}p_{5j}^{2}\Big{(} \frac{1}{2}p_{3j}^{2}+p_{4j}\Big{)}=0.\]

It is clear that we have non-trivial solutions \(p_{5j}=1\), \(p_{1j}=0\) for all \(j\in[m]\), \(|p_{21}|=p_{31}=1\), \(|p_{22}|=p_{32}=-1\), \(p_{41}=p_{42}=-1/2\), \(p_{2j}=p_{3j}=p_{4j}=0\) for \(3\leq j\leq m\).

When \(d=1\) and \(r=3\), the system of equations can be written as follows:

\[\sum_{j=1}^{m}p_{5j}^{2}p_{1j}=0,\quad\sum_{j=1}^{m}p_{5j}^{2}p_{ 3j}=0,\quad\sum_{j=1}^{m}p_{5j}^{2}(p_{2j}+p_{1j}p_{3j})=0,\] \[\sum_{j=1}^{m}p_{5j}^{2}p_{1j}^{2}=0,\quad\sum_{j=1}^{m}p_{5j}^{2 }\Big{(}\frac{1}{2}p_{3j}^{2}+p_{4j}\Big{)}=0,\quad\sum_{j=1}^{m}p_{5j}^{2} \Big{(}\frac{1}{3!}p_{3j}^{3}+p_{3j}p_{4j}\Big{)}=0,\] \[\sum_{j=1}^{m}p_{5j}^{2}p_{1j}^{3}=0,\quad\sum_{j=1}^{m}p_{5j}^{2 }\Big{(}\frac{1}{2}p_{1j}^{2}p_{3j}+p_{1j}p_{2j}\Big{)}=0,\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{2}p_{1j}\cdot p_{3j}^{2} +p_{1j}p_{4j}+p_{2j}p_{3j}\Big{)}=0.\]

It can be seen that the following is a non-trivial solution of the above system: \(p_{5j}=1\), \(p_{1j}=p_{2j}=0\) for all \(j\in[m]\), \(p_{31}=\frac{\sqrt{3}}{3}\), \(p_{32}=-\frac{\sqrt{3}}{3}\), \(p_{41}=p_{42}=-\frac{1}{6}\), \(p_{3j}=p_{4j}=0\) for \(3\leq j\leq m\). Therefore, we obtain that \(\bar{r}(m)\geq 4\) when \(m\geq 2\) and \(d=1\).

In general, when \(d=1\), the system of equations has \((r^{2}+3r)/2\) equations. Intuitively, when \(m\) is sufficiently larger than \((r^{2}+3r)/2\), the system may not have a non-trivial solution. For general dimension \(d\) and parameter \(m\geq 2\), finding the exact value of \(\bar{r}(m)\) is a non-trivial central problem in algebraic geometry [55]. When \(m\) is small, the following lemma provides specific values for \(\bar{r}(m)\).

**Lemma 1**.: _For any \(d\geq 1\), when \(m=2\), \(\bar{r}(m)=4\). When \(m=3\), \(\bar{r}(m)=6\)._

Proof of Lemma 1 is in Appendix B.3. As \(m\) increases, so does the value of \(\bar{r}(m)\). We conjecture that \(\bar{r}(m)=2m\) and leave the proof of that conjecture to future work.

By constructing the Voronoi loss function:

\[\mathcal{D}_{2}(G,G_{*}):=\inf_{i_{1},t_{2}}\sum_{j:|\mathcal{A}_{j}|>1}\sum_{i \in\mathcal{A}_{j}}\exp(\beta_{0i})\Big{(}\|(\Delta_{t_{2}}\beta_{1ij},\Delta \partial_{ij})\|^{\bar{r}(|\mathcal{A}_{j}|)}+\|(\Delta a_{ij},\Delta\sigma_{ ij})\|^{\bar{r}(|\mathcal{A}_{j}|)/2}\Big{)}\]

\[+\sum_{j:|\mathcal{A}_{j}|=1}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})\|( \Delta_{t_{2}}\beta_{1ij},\Delta a_{ij},\Delta b_{ij},\Delta\sigma_{ij})\|+\sum _{j=1}^{k_{*}}\Big{|}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})-\exp(\beta_{0j }^{*}+t_{1})\Big{|},\]

the following result demonstrates that the convergence rates of the MLE under the over-fitted settings of the softmax gating Gaussian mixture of experts are determined by \(\bar{r}(\cdot)\).

**Theorem 2**.: _Under the over-fitted settings of the softmax gating Gaussian mixture of experts (1), namely, when \(k>k_{*}\), we obtain that_

\[\mathbb{E}_{X}[h(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]\geq C_{2}\cdot\mathcal{D}_{ 2}(G,G_{*}),\] (10)

_for any \(G\in\mathcal{O}_{k}(\Theta)\) where \(C_{2}\) is some universal constant depending only on \(G_{*}\) and \(\Theta\). Therefore, that lower bound leads to the following convergence rate of the MLE:_

\[\mathbb{P}(\mathcal{D}_{2}(\widehat{G}_{n},G_{*})>C_{2}^{\prime}(\log(n)/n)^{1/ 2})\lesssim\exp(-c_{2}\log n),\] (11)

_where \(C_{2}^{\prime}\) and \(c_{2}\) are some universal constants._Proof of Theorem 2 is in Appendix A.2. A few comments with the result of Theorem 2 are in order.

**(i) Rates of individual parameters:** The convergence rate \(\mathcal{O}(n^{-1/2})\) (up to some logarithmic term) of the MLE under the loss function \(\mathcal{D}_{2}\) implies that for the true parameters \(\exp(\beta^{*}_{0j}),\beta^{*}_{1j},a^{*}_{j},b^{*}_{j},\sigma^{*}_{j}\) whose Voronoi cells have only one component of the MLE, the rates for estimating them are \(\mathcal{O}(n^{-1/2})\) up to some logarithmic factor. On the other hand, for true parameters with greater than one component in their Voronoi cells, the rates for estimating \(\beta^{*}_{1j}\), \(b^{*}_{j}\) are \(\mathcal{O}(n^{-1/2\tilde{\nu}(|\mathcal{A}^{*}_{j}|)})\) while those for \(a^{*}_{j},\sigma^{*}_{j}\) are \(\mathcal{O}(n^{-1/\tilde{\nu}(|\mathcal{A}^{*}_{j}|)})\) (up to logarithmic factors). As the maximum value of \(|\mathcal{A}^{*}_{j}|\) is \(\hat{k}_{n}-k_{*}+1\), it indicates that these rates (up to logarithmic factors) can be as worse as \(\mathcal{O}(n^{-1/\tilde{\nu}(k_{n}-k_{*}+1)})\) for estimating \(a^{*}_{j},\sigma^{*}_{j}\) and \(\mathcal{O}(n^{-1/2\tilde{\nu}(\hat{k}_{n}-k_{*}+1)})\) for estimating \(\beta^{*}_{1j},b^{*}_{j}\).

**(ii) Computation of Voronoi loss function \(\mathcal{D}_{2}\):** Similar to the Voronoi loss function \(\mathcal{D}_{1}\) in the exact-fitted setting, the loss function \(\mathcal{D}_{2}\) is also computationally efficient. In particular, for any fixed \(t_{1},t_{2}\), the computation of the summations in the formulation of \(\mathcal{D}_{2}\) is at the order \(\mathcal{O}(k\times k_{*})\), which is linear on \(k\) when \(k_{*}\) is fixed. Furthermore, we can solve the convex optimization problem with respect to \(t_{1},t_{2}\) with computational complexity at the order of \(\mathcal{O}(\varepsilon^{-2})\) via the projected gradient descent method with fixed step size where \(\varepsilon\) is the error. Therefore, the total computational complexity of approximating the Voronoi loss function \(\mathcal{D}_{2}\) is at the order of \(\mathcal{O}(k\times k_{*}/\varepsilon^{2})\).

**(iii) Comparison with covariate-free gating network:** We would like to remark that the results being established for parameter estimation under the softmax gating network settings of over-fitted Gaussian mixture of experts are in stark difference from those under the covariate-free gating network settings of these models [30], namely, when the gating function is independent of the covariates \(X\). In particular, Theorem 2 in [30] shows that when the gating networks are independent of the covariates, the convergence rates of estimating \(a^{*}_{j}\) are at the order of \(\mathcal{O}(n^{-1/4})\) (up to some logarithmic factor), which are independent of the number of over-fitted components. It is different from the rates of \(a^{*}_{j}\) whose Voronoi cells have more than one component in the softmax gating settings, which depends on the number of components that we over-fit the Gaussian mixture of experts (see discussion (i) after Theorem 2). Furthermore, the rates of estimating \(b^{*}_{j},\sigma^{*}_{j}\) when the gating networks are independent of covariates are determined by a system of polynomial equations that is much simpler than the system of equations (9) when the gating networks are softmax function. These differences are mainly due to the intrinsic interaction characterized by partial differential equations with respect to the parameters between the softmax gating networks and the expert functions in Gaussian distribution.

### Proof Sketch

In this section, we provide a proof sketch for Theorems 1 and 2. To simplify the ensuing discussion, the loss function \(\mathcal{D}\) in the proof sketch is implicitly understood as either \(\mathcal{D}_{1}\) or \(\mathcal{D}_{2}\) depending on the settings of the softmax gating Gaussian mixture of experts. Since the Hellinger distance \(h\) is lower bounded by the Total Variation distance \(V\), to obtain the bounds in equations (7) and (10), it is sufficient to show that \(\mathbb{E}_{X}[V(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]\gtrsim\mathcal{D}(G,G_{*})\). To establish this bound, we respectively prove its local and global versions by contradiction as follows:

**Local version**: In this part, we aim to show the following local inequality:

\[\lim_{\varepsilon\to 0}\inf_{G\in\mathcal{O}_{k}(\Theta):\mathcal{D}(G,G_{*}) \leq\varepsilon}\mathbb{E}_{X}[V(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{ D}(G,G_{*})>0.\] (12)

Assume that this claim does not hold true, that is, there exists a sequence \(G_{n}=\sum_{i=1}^{k_{n}}\exp(\beta^{n}_{0i})\delta_{(\beta^{n}_{1i},a^{n}_{i},b ^{n}_{i},\sigma^{n}_{i})}^{\top}\in\mathcal{O}_{k}(\Theta)\) such that both \(\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}(G_{n},G_ {*})\) and \(\mathcal{D}(G_{n},G_{*})\) approach zero as \(n\) tends to infinity. This implies that for any \(j\in[k_{*}]\), we have \(\sum_{i\in\mathcal{A}_{j}}\exp(\beta^{n}_{0i})\rightarrow\exp(\beta^{*}_{0j})\) and \((\beta^{n}_{1i},a^{n}_{i},b^{n}_{i},\sigma^{n}_{i})\rightarrow(\beta^{*}_{1j},a^{*}_{j},b^{*}_{j},\sigma^{*}_{j})\) and for all \(i\in\mathcal{A}_{j}\). For the sake of presentation, we simplify the loss function \(\mathcal{D}\) by assuming that it is minimized when \(t_{1}=0\) and \(t_{2}=\mathbf{0}_{d}\). Now, we decompose the quantity

\[Q_{n}=\Big{[}\sum_{j^{\prime}=1}^{k_{*}}\exp((\beta^{*}_{1j^{\prime}})^{\top}X+ \beta^{*}_{0j^{\prime}})\Big{]}\cdot[g_{G_{n}}(Y|X)-g_{G_{*}}(Y|X)]\]as follows:

\[Q_{n} =\sum_{j=1}^{k_{*}}\sum_{i\in\mathcal{A}_{j}}\exp(\beta^{n}_{0i}) \Big{[}u(Y|X;\beta^{n}_{1i},a^{n}_{i},b^{n}_{i},\sigma^{n}_{i})-u(Y|X;\beta^{*}_{ 1j},a^{*}_{j},b^{*}_{j},\sigma^{*}_{j})-v(Y|X;\beta^{n}_{1i})\] \[+v(Y|X;\beta^{*}_{1j})\Big{]}+\sum_{j=1}^{k_{*}}\Big{(}\sum_{i\in \mathcal{A}_{j}}\exp(\beta^{n}_{0i})-\exp(\beta^{*}_{0j})\Big{)}\Big{[}u(Y|X; \beta^{*}_{0j},a^{*}_{j},b^{*}_{j},\sigma^{*}_{j})-v(Y|X;\beta^{*}_{1j})\Big{]},\]

where we define \(u(Y|X;\beta_{1},a,b,\sigma):=\exp(\beta^{\top}_{1}X)f(Y|a^{\top}X+b,\sigma)\) and \(v(Y|X;\beta_{1}):=\exp(\beta^{\top}_{1}X)g_{G_{n}}(Y|X)\). Next, for each \(j\in[k_{*}]\) and \(i\in\mathcal{A}_{j}\), we denote \(h_{1}(X,a^{*}_{j},b^{*}_{j}):=(a^{*}_{j})^{\top}X+b^{*}_{j}\) and then apply the Taylor expansions to the functions \(u(Y|X;\beta^{n}_{1i},a^{n}_{i},b^{n}_{i},\sigma^{n}_{i})\) and \(v(Y|X;\beta^{n}_{1i})\) up to orders \(r_{1j}\) and \(r_{2j}\) (which we will choose later), respectively, as follows:

\[u(Y|X;\beta^{n}_{1i},a^{n}_{i},b^{n}_{i},\sigma^{n}_{i})-u(Y|X; \beta^{*}_{1j},a^{*}_{j},b^{*}_{j},\sigma^{*}_{j})\] \[=\sum_{|\ell_{1}|+\ell_{2}=1}^{2r_{1j}}T^{n}_{\ell_{1},\ell_{2}}( j)X^{\ell_{1}}\exp((\beta^{*}_{1j})^{\top}X)\frac{\partial^{\ell_{2}}f}{ \partial h^{\ell_{2}}_{1}}(Y|(a^{*}_{j})^{\top}X+b^{*}_{j},\sigma^{*}_{j})+R_{ 1ij}(X,Y),\] \[v(Y|X;\beta^{n}_{1i})-v(Y|X;\beta^{*}_{1j})=\sum_{|\gamma|=1}^{r_ {2j}}S^{n}_{\gamma}(j)X^{\gamma}\exp((\beta^{*}_{1j})^{\top}X)g_{G_{n}}(Y|X)+ R_{2ij}(X,Y),\]

where \(R_{1ij}(X,Y)\) and \(R_{2ij}(X,Y)\) are Taylor remainders such that \(R_{\rho ij}(X,Y)/\mathcal{D}(G_{n},G_{*})\) vanishes as \(n\to\infty\) for \(\rho\in\{1,2\}\). As a result, the limit of \(Q_{n}/\mathcal{D}(G_{n},G_{*})\) when \(n\) goes to infinity can be seen as a linear combination of elements of the following set:

\[\mathcal{W}: =\left\{X^{\ell_{1}}\exp((\beta^{*}_{1j})^{\top}X)\frac{ \partial^{\ell_{2}}f}{\partial h^{\ell_{2}}_{1}}(Y|(a^{*}_{j})^{\top}X+b^{*}_ {j},\sigma^{*}_{j}):j\in[k_{*}],\;0\leq 2|\ell_{1}|+\ell_{2}\leq 2r_{1j}\right\}\] \[\cup\left\{X^{\gamma}\exp((\beta^{*}_{1j})^{\top}X)g_{G_{*}}(Y|X ):j\in[k_{*}],\;0\leq|\gamma|\leq r_{2j}\right\},\]

which is shown to be linearly independent. By the Fatou's lemma, we demonstrate that \(Q_{n}/\mathcal{D}(G_{n},G_{*})\) goes to zero as \(n\to\infty\), implying that all the coefficients in the representation of \(Q_{n}/\mathcal{D}(G_{n},G_{*})\), denoted by \(T^{n}_{\ell_{1},\ell_{2}}(j)/\mathcal{D}(G_{n},G_{*})\) and \(S^{n}_{\gamma}(j)/\mathcal{D}(G_{n},G_{*})\), vanish when \(n\to\infty\). Given that result, we aim to select the Taylor orders \(r_{1j}\) and \(r_{2j}\) such that at least one among the limits of \(T^{n}_{\ell_{1},\ell_{2}}(j)/\mathcal{D}(G_{n},G_{*})\) and \(S^{n}_{\gamma}(j)/\mathcal{D}(G_{n},G_{*})\) is different from zero, which leads to a contradiction. Hence, we obtain the local version of the desired inequality.

Below are the details of choosing appropriate Taylor orders in each setting.

**Exact-fitted settings:** Under this setting, since \(k_{*}\) is known, each of the Voronoi cells \(\mathcal{A}_{j}\) for \(j\in[k_{*}]\) has only one element. Thus, for any \(i\in\mathcal{A}_{j}\), we have \(\exp(\beta^{n}_{0i})\to\exp(\beta^{*}_{0j})\) and \((\beta^{n}_{1i},a^{n}_{i},b^{n}_{i},\sigma^{n}_{i})\to(\beta^{*}_{1j},a^{*}_{j },b^{*}_{j},\sigma^{*}_{j})\). Given that result, we will select \(r_{1j}=r_{2j}=1\) for all \(j\in[k_{*}]\) as it suffices to show that at least one among the limits of \(T^{n}_{\ell_{1},\ell_{2}}(j)/\mathcal{D}(G_{n},G_{*})\) and \(S^{n}_{\gamma}(j)/\mathcal{D}(G_{n},G_{*})\) is different from zero. In particular, if all of them vanished, we would take the sum of all the limits of \(T^{n}_{\ell_{1},\ell_{2}}(j)/\mathcal{D}(G_{n},G_{*})\) for \((\ell_{1},\ell_{2})\) such that \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\), which leads to a contradiction that \(1=\mathcal{D}(G_{n},G_{*})/\mathcal{D}(G_{n},G_{*})\to 0\).

**Over-fitted settings:** As \(k_{*}\) becomes unknown in this scenario, we need higher Taylor orders to obtain the same result as in the exact-fitted setting. We will reuse the proof by contradiction method to find out those orders. More specifically, assume that all the limits of \(T^{n}_{\ell_{1},\ell_{2}}(j)/\mathcal{D}(G_{n},G_{*})\) and \(S^{n}_{\gamma}(j)/\mathcal{D}(G_{n},G_{*})\) equal zero. After some steps of considering typical limits as in the previous setting which requires \(r_{2j}=2\) for all \(j\in[k_{*}]\), we encounter the following system of polynomial equations:

\[\sum_{i\in\mathcal{A}_{j}}\sum_{(\alpha_{1},\alpha_{2},\alpha_{3},\alpha_{4})\in \mathcal{I}_{\ell_{1},\ell_{2}}}\frac{p^{2}_{5i}\,p^{\alpha_{1i}}_{1i}\,p^{ \alpha_{2}}_{2i}\,p^{\alpha_{3}}_{3i}\,p^{\alpha_{4}}_{4i}}{\alpha_{1}!\,\alpha_ {2}!\,\alpha_{3}!\,\alpha_{4}!}=0,\]

for all \((\ell_{1},\ell_{2})\in\mathbb{N}^{d}\times\mathbb{N}\) such that \(0\leq|\ell_{1}|\leq r_{1j}\), \(0\leq\ell_{2}\leq r_{1j}-|\ell_{1}|\) and \(|\ell_{1}|+\ell_{2}\geq 1\) for some \(j\in[k_{*}]\). Due to the construction of this system, it must have at least one non-trivialsolution. Therefore, if we choose \(r_{1j}=\bar{r}(|\mathcal{A}_{j}|)\) for all \(j\in[k_{*}]\), then the above system does not admit any non-trivial solutions, which leads to a contradiction. Hence, we obtain the local inequality in equation (12), which suggests that we can find a positive constant \(\varepsilon^{\prime}\) such that \(\inf_{G\in\mathcal{O}_{k}(\Theta):\mathcal{D}(G,G_{*})\leq\varepsilon^{\prime }}\mathbb{E}_{X}[V(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}(G,G_{*})>0\).

**Global version:** Therefore, it is sufficient to demonstrate the following global inequality:

\[\inf_{G\in\mathcal{O}_{k}(\Theta),\mathcal{D}(G,G_{*})>\varepsilon^{\prime}} \mathbb{E}_{X}[V(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}(G,G_{*})>0.\] (13)

Assume that this claim is not true, then we can find a mixing measure \(G^{\prime}\in\mathcal{O}_{k}(\Theta)\) such that \(g_{G^{\prime}}(Y|X)=g_{G_{*}}(Y|X)\) for almost surely \((X,Y)\). According to Proposition 1, we get that \(\mathcal{D}(G^{\prime},G_{*})=0\), which contradicts the hypothesis \(\mathcal{D}(G^{\prime},G_{*})>\varepsilon^{\prime}\). These arguments hold for both exact-fitted and over-fitted settings up to some changes of notations.

Hence, the proof sketch is completed.

## 4 Discussion

In the paper, we study the convergence rates of parameter estimation under both the exact-fitted and over-fitted settings of the softmax gating Gaussian mixture of experts. We introduce novel Voronoi loss functions among parameters to resolve fundamental theoretical challenges posed by the softmax gating function, including identifiability up to the translation of parameters, the interaction between softmax weights and expert functions, and the dependence between the numerator and denominator of the conditional density function. When the true number of experts is known, we demonstrate that the rates for estimating true parameters are parametric on the sample size. On the other hand, when the true number of experts is unknown and over-specified, these estimation rates turn out to be determined by the solvability of a system of polynomial equations.

There are a few natural directions arising from the paper that we leave for future work:

* First, our work does not consider the top-K sparse softmax gating function, which has been widely used to scale up massive deep learning architectures [68; 54; 21]. It is practically important to extend the current theories to establish the convergence rates of parameter estimation in the Gaussian mixture of experts with that gating function.
* Second, the paper only takes into account the regression settings, namely when the distribution of \(Y\) is assumed to be continuous. Given that mixture of experts has also been used in classification settings [22; 31; 53; 34; 35; 60], namely when \(Y\) is a discrete response variable, it is desirable to establish a comprehensive theory for parameter estimation under these settings of mixtures of experts.
* Third, the theories developed in the paper lay an important foundation for understanding parameter estimation in more complex models, including hierarchical mixture of experts [33; 51; 37; 66] and multigate mixture of experts [44; 26; 45].
* Finally, the convergence rates of the MLE in this work are established under the well-specified settings, namely when the data are drawn from the softmax gating Gaussian mixture of experts. Nevertheless, the convergence analysis of the MLE under the misspecified settings, namely when the data are not necessarily generated from that model, has remained poorly understood. Under those settings, the MLE \(\widehat{G}_{n}\) converges to the mixing measures \(\overline{G}\in\arg\min_{G\in\mathcal{O}_{k}(\Theta)}\text{KL}(g_{G}(Y|X),p(Y| X))\) where \(p(Y|X)\) is the true conditional density function of \(Y\) given \(X\), and it is not a softmax gating Gaussian mixture of experts. Additionally, the notation KL stands for the Kullback-Leibler divergence. The insights from our theories under the well-specified setting indicate that the Voronoi loss functions can be used to obtain the precise rates of individual parameters of the MLE \(\widehat{G}_{n}\) to those of the mixing measure \(\overline{G}\).

## Acknowledgements

NH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning.

## References

* [1] A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden markov models. In _COLT_, 2012.
* [2] S. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. _Annals of Statistics_, 45:77-120, 2017.
* [3] H. Bao, W. Wang, L. Dong, Q. Liu, O.-K. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei. VLMo: Unified vision-language pre-training with mixture-of-modality-experts. In _Advances in Neural Information Processing Systems_, 2022.
* [4] S. Boyd and L. Vandenberghe. _Convex Optimization_. Cambridge University Press, 2004.
* [5] F. Chamroukhi and B. T. Huynh. Regularized Maximum-Likelihood Estimation of Mixture-of-Experts for Regression and Clustering. In _2018 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8, 2018.
* [6] F. Chamroukhi and B.-T. Huynh. Regularized Maximum Likelihood Estimation and Feature Selection in Mixtures-of-Experts Models. _Journal de la Societe Francaise de Statistique_, 160(1):57-85, 2019.
* [7] F. Chamroukhi, A. Same, G. Govaert, and P. Aknin. Time series modeling by a regression approach based on a latent process. _Neural Networks_, 22(5-6):593-602, 2009. Publisher: Elsevier.
* [8] F. Chamroukhi, A. Same, G. Govaert, and P. Aknin. A hidden process regression model for functional data description. Application to curve discrimination. _Neurocomputing_, 73(7-9):1210-1221, 2010.
* [9] J. H. Chen. Optimal rate of convergence for finite mixture models. _Annals of Statistics_, 23(1):221-233, 1995.
* [10] K. Chen, L. Xu, and H. Chi. Improved learning algorithms for mixture of experts in multiclass classification. _Neural Networks_, 12(9):1229-1252, 1999.
* [11] Y. Chen, X. Yi, and C. Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In _COLT_, 2014.
* [12] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data Via the EM Algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, Sept. 1977.
* [13] D. Do, L. Do, and X. Nguyen. Strong identifiability and parameter learning in regression with heterogeneous response. _arXiv preprint arXiv:2212.04091_, 2022.
* [14] T. G. Do, H. K. Le, T. Nguyen, Q. Pham, B. T. Nguyen, T.-N. Doan, C. Liu, S. Ramasamy, X. Li, and S. HOI. HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, Singapore, Dec. 2023. Association for Computational Linguistics.
* [15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021.
* 95, 2023. Publisher: Institute of Mathematical Statistics.
* [17] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. Yu, O. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Efficient scaling of language models with mixture-of-experts. In _ICML_, 2022.
* [18] R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, and B. Yu. Sharp analysis of expectation-maximization for weakly identifiable models. _AISTATS_, 2020.
* [19] R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, and B. Yu. Singularity, misspecification, and the convergence rate of EM. _Annals of Statistics_, 44:2726-2755, 2020.

* [20] D. Eigen, M. Ranzato, and I. Sutskever. Learning factored representations in a deep mixture of experts. In _ICLR Workshops_, 2014.
* [21] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _Journal of Machine Learning Research_, 23:1-39, 2022.
* 1477, 2008. Publisher: Institute of Mathematical Statistics.
* [23] P. J. Green. Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and some Robust and Resistant Alternatives. _Journal of the Royal Statistical Society. Series B (Methodological)_, 46(2):149-192, 1984. Publisher: [Royal Statistical Society, Wiley].
* [24] A. Guha, N. Ho, and X. Nguyen. On posterior contraction of parameters and interpretability in Bayesian mixture modeling. _Bernoulli_, 27(4):2159-2188, 2021.
* [25] M. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In _STOC_, 2015.
* [26] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y. Chen, R. Mazumder, L. Hong, and E. Chi. DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 29335-29347. Curran Associates, Inc., 2021.
* [27] P. Heinrich and J. Kahn. Strong identifiability and optimal minimax rates for finite mixture estimation. _The Annals of Statistics_, 46(6):2844-2870, 2018.
* [28] N. Ho and X. Nguyen. Convergence rates of parameter estimation for some weakly identifiable finite mixtures. _Annals of Statistics_, 44:2726-2755, 2016.
* [29] N. Ho and X. Nguyen. On strong identifiability and convergence rates of parameter estimation in finite mixtures. _Electronic Journal of Statistics_, 10:271-307, 2016.
* [30] N. Ho, C.-Y. Yang, and M. I. Jordan. Convergence rates for Gaussian mixtures of experts. _Journal of Machine Learning Research_, 23(323):1-81, 2022.
* [31] B. T. Huynh and F. Chamroukhi. Estimation and feature selection in mixtures of generalized linear experts models. _arXiv preprint arXiv:1907.06994_, 2019.
* [32] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. _Neural Computation_, 3, 1991.
* [33] R. A. Jacobs, F. Peng, and M. A. Tanner. A Bayesian Approach to Model Selection in Hierarchical Mixtures-of-Experts Architectures. _Neural Networks_, 10(2):231-241, 1997.
* [34] W. Jiang and M. A. Tanner. Hierarchical Mixtures-of-Experts for Generalized Linear Models: Some Results on Denseness and Consistency. In D. Heckerman and J. Whittaker, editors, _Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics_, volume R2 of _Proceedings of Machine Learning Research_. PMLR, Jan. 1999.
* [35] W. Jiang and M. A. Tanner. On the Approximation Rate of Hierarchical Mixtures-of-Experts for Generalized Linear Models. _Neural Computation_, 11(5):1183-1198, July 1999.
* [36] W. Jiang and M. A. Tanner. On the identifiability of mixtures-of-experts. _Neural Networks_, 9:1253-1258, 1999.
* [37] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. _Neural Computation_, 6:181-214, 1994.
* [38] B. Krishnapuram, L. Carin, M. Figueiredo, and A. Hartemink. Sparse multinomial logistic regression: fast algorithms and generalization bounds. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 27(6):957-968, 2005.
* towards model-independent searches of new physics. _Journal of Physics: Conference Series_, 368:012032, 2011.
* [40] J. Kwon and C. Caramanis. EM Converges for a Mixture of Many Linear Regressions. In S. Chiappa and R. Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 1727-1736. PMLR, Aug. 2020.

* [41] J. Kwon, N. Ho, and C. Caramanis. On the minimax optimality of the EM algorithm for learning two-component mixed linear regression. In _AISTATS_, 2021.
* [42] J. Kwon, W. Qian, C. Caramanis, Y. Chen, and D. Davis. Global Convergence of the EM Algorithm for Mixtures of Two Component Linear Regression. In A. Beygelzimer and D. Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 2055-2110. PMLR, June 2019.
* [43] Q. Li, R. Shi,, and F. Liang. Drug sensitivity prediction with high-dimensional mixture regression. _PLoS ONE_, 2019.
* [44] H. Liang, Z. Fan, R. Sarkar, Z. Jiang, T. Chen, K. Zou, Y. Cheng, C. Hao, and Z. Wang. M\({}^{3}\)ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design. In _NeurIPS_, 2022.
* [45] J. Ma, Z. Zhao, X. Yi, J. Chen, L. Hong, and E. H. Chi. Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixture-of-Experts. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '18, pages 1930-1939, New York, NY, USA, 2018. Association for Computing Machinery. event-place: London, United Kingdom.
* [46] T. Manole and N. Ho. Uniform convergence rates for maximum likelihood estimation under two-component gaussian mixture models. _arXiv preprint arXiv:2006.00704_, 2020.
* [47] T. Manole and N. Ho. Refined convergence rates for maximum likelihood estimation under finite mixture models. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14979-15006. PMLR, 17-23 Jul 2022.
* [48] L. Montuelle and E. Le Pennec. Mixture of Gaussian regressions model with logistic weights, a penalized maximum likelihood approach. _Electronic Journal of Statistics_, 8(1):1661-1695, 2014.
* [49] B. Mustafa, C. Ruiz, J. Puigcerver, R. Jenatton, and N. Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. In _NeurIPS_, 2022.
* [50] X. Nguyen. Convergence of latent mixing measures in finite and infinite mixture models. _Annals of Statistics_, 4(1):370-400, 2013.
* [51] F. Peng, R. A. Jacobs, and M. A. Tanner. Bayesian Inference in Mixtures-of-Experts and Hierarchical Mixtures-of-Experts Models With an Application to Speech Recognition. _Journal of the American Statistical Association_, 91(435):953-960, 1996.
* [52] C. Ruiz, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Pinto, D. Keysers, and N. Houlsby. Scaling vision with sparse mixture of experts. In _NeurIPS_, 2021.
* [53] M. E. Ruiz and P. Srinivasan. Hierarchical Text Categorization Using Neural Networks. _Information Retrieval_, 5(1):87-118, Jan. 2002.
* [54] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _International Conference on Learning Representations (ICLR)_, 2017.
* [55] B. Sturmfels. _Solving Systems of Polynomial Equations_. Providence, R.I, 2002.
* [56] H. Teicher. On the mixture of distributions. _Annals of Statistics_, 31:55-73, 1960.
* [57] H. Teicher. Identifiability of mixtures. _Annals of Statistics_, 32:244-248, 1961.
* [58] S. van de Geer. _Empirical processes in M-estimation_. Cambridge University Press, 2000.
* [59] C. Villani. _Optimal transport: Old and New_. Springer, 2008.
* [60] S. Waterhouse and A. Robinson. Classification using hierarchical mixtures of experts. In _Proceedings of IEEE Workshop on Neural Networks for Signal Processing_, pages 177-186, 1994.
* [61] Y. Wu and P. Yang. Optimal estimation of Gaussian mixtures via denoised method of moments. _The Annals of Statistics_, 48:1987-2007, 2020.
* [62] Y. Wu and H. H. Zhou. Randomly initialized EM algorithm for two-component Gaussian mixture achieves near optimality in \(o(\sqrt{n})\) iterations. _Mathematical Statistics and Learning_, 4:143-220, 2021.

* [63] X. Yi, C. Caramanis, and S. Sanghavi. Alternating minimization for mixed linear regression. In _ICML_, 2014.
* [64] Z. You, S. Feng, D. Su, and D. Yu. Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts. In _Interspeech_, 2021.
* 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7217-7221, 2022.
* [66] Y. Zhao, R. Schwartz, J. Sroka, and J. Makhoul. Hierarchical mixtures of experts methodology applied to continuous speech recognition. In _Advances in Neural Information Processing Systems_, volume 7, 1994.
* [67] K. Zhong, P. Jain, and I. S. Dhillon. Mixed linear regression with multiple components. In _NeurIPS_, 2016.
* [68] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Y. Zhao, A. M. Dai, Z. Chen, Q. V. Le, and J. Laudon. Mixture-of-Experts with Expert Choice Routing. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.

**Supplement to "Demystifying Softmax Gating Function in Gaussian Mixture of Experts"**

In this supplementary material, we present the proofs of Theorems 1 and 2 in Appendix A, and then provide proofs for the remaining results in Appendix B. Finally, we carry out a simulation study to illustrate the various convergence rates that were derived in Theorems 1 and 2 in Appendix C.

## Appendix A Proofs of Main Results

In this appendix, we provide proof for Theorem 1 in Appendix A.1, while leave that for Theorem 2 in Appendix A.2. Prior to discussing in more detail, let us recall some notations for high-dimensional settings that we will use in our proofs. Firstly, for any vector \(u:=(u_{1},u_{2},\ldots,u_{d})\in\mathbb{R}^{d}\) and \(z:=(z_{1},z_{2},\ldots,z_{d})\in\mathbb{N}^{d}\), we denote \(u^{z}=u_{1}^{z_{1}}u_{2}^{z_{2}}\ldots u_{d}^{z_{d}}\), \(|u|:=u_{1}+u_{2}+\ldots+u_{d}\) and \(z!:=z_{1}!z_{2}!\ldots z_{d}!\). Additionally, \(\mathbf{0}_{d}\) denotes the vector zero in \(\mathbb{R}^{d}\), whereas the notation \(\mathbf{1}\) stands for the indicator function. Finally, we denote \(h_{1}(X,a,b)=a^{\top}X+b\) and \(h_{2}(X,\sigma)=\sigma\) as mean and variance expert functions in this work for any \((a,b,\sigma)\in\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{R}_{+}\) and \(X\in\mathcal{X}\).

### Proof of Theorem 1

**General Picture**: In this proof, we focus mainly on establishing the following bound:

\[\mathbb{E}_{X}[V(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]\geq C_{1}\cdot\mathcal{D} _{1}(G,G_{*}).\] (14)

Then, the above bound together with the result of Proposition 2 lead to the conclusion of Theorem 1.

**Local version**: Firstly, we prove the local version of the inequality (14):

\[\lim_{\varepsilon\to 0}\inf_{G\in\mathcal{E}_{k_{*}}(\Theta):\mathcal{D}_{1}(G,G _{*})\leq\varepsilon}\mathbb{E}_{X}[V(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]/ \mathcal{D}_{1}(G,G_{*})>0.\] (15)

Suppose that the inequality in equation (15) does not hold true, then we can find a sequence \(G_{n}:=\sum_{i=1}^{k_{*}}\exp(\beta_{0i}^{n})\delta_{(\beta_{1i}^{n},a_{i}^{n },b_{i}^{n},\sigma_{i}^{n})}\in\mathcal{E}_{k_{*}}(\Theta)\) such that

\[\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{ D}_{1}(G_{n},G_{*}) \to 0,\] \[\mathcal{D}_{1}(G_{n},G_{*})\to 0,\]

as \(n\to\infty\). Next, we consider the Voronoi cells \(\mathcal{A}_{j}^{n}:=\mathcal{A}_{j}(G_{n})\), for \(j\in[k_{*}]\), of the mixing measure \(G_{n}\) generated by the true components of \(G_{*}\). Since the argument in this proof is asymptotic, we assume without loss of generality (WLOG) that those Voronoi cells are independent of \(n\) for all \(n\in\mathbb{N}\), i.e. \(\mathcal{A}_{j}=\mathcal{A}_{j}^{n}\). Additionally, since \(k_{*}\) is known under the exact-fitted settings and \(\mathcal{D}_{1}(G_{n},G_{*})\to 0\), the Voronoi cell \(\mathcal{A}_{j}\) has only one element for any \(j\in[k_{*}]\). WLOG, we assume that \(\mathcal{A}_{j}=\{j\}\) for all \(j\in[k_{*}]\), which follows that \((a_{j}^{n},b_{j}^{n},\sigma_{j}^{n})\to(a_{j}^{n},b_{j}^{n},\sigma_{j}^{*})\) as \(n\to\infty\). Furthermore, there exist \(t_{1}\in\mathbb{R}\) and \(t_{2}\in\mathbb{R}^{d}\) independent of \(n\) such that \(\exp(\beta_{0j}^{n})\to\exp(\beta_{0j}^{*}+t_{1})\) and \(\beta_{1j}^{n}\to\beta_{1j}^{*}+t_{2}\) as \(n\) approaches infinity for all \(j\in[k_{*}]\). It indicates that we can upper bound the Voronoi loss function \(\mathcal{D}_{1}\) as \(\mathcal{D}_{1}(G_{n},G_{*})\leq\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\), where

\[\mathcal{D}_{1}^{\prime}(G_{n},G_{*}):=\sum_{j=1}^{k_{*}}\left[\exp(\beta_{0j}^ {n})\|(\Delta_{t_{2}}\beta_{1j}^{n},\Delta a_{j}^{n},\Delta b_{j}^{n},\Delta \sigma_{j}^{n})\|+\left|\exp(\beta_{0j}^{n})-\exp(\beta_{0j}^{*}+t_{1})\right| \right],\]

in which

\[\Delta_{t_{2}}\beta_{1j}^{n}:=\beta_{1j}^{n}-\beta_{1j}^{*}-t_{2}, \Delta a_{j}^{n}:=a_{j}^{n}-a_{j}^{*},\] \[\Delta b_{j}^{n}:=b_{j}^{n}-b_{j}^{*}, \Delta\sigma_{j}^{n}:=\sigma_{j}^{n}-\sigma_{j}^{*}.\]

As \(\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}_{1}(G_{n},G_{*})\to 0\) when \(n\to\infty\), we also obtain that

\[\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}_{1}^{ \prime}(G_{n},G_{*})\to 0.\]

#### Step 1: Density Decomposition

Subsequently, we consider \(Q_{n}:=[\sum_{j=1}^{k_{*}}\exp((\beta_{1j}^{*}+t_{2})^{\top}X+\beta_{0j}^{*}+t_{1} )]\cdot[g_{G_{n}}(Y|X)-g_{G_{*}}(Y|X)]\), which can decomposed as

\[Q_{n} =\sum_{j=1}^{k_{*}}\exp(\beta_{0j}^{n})\Big{[}u(Y|X;\beta_{1j}^{n},a_{j}^{n},b_{j}^{n},\sigma_{j}^{n})-u(Y|X;\beta_{1j}^{*}+t_{2},a_{j}^{*},b_{j }^{*},\sigma_{j}^{*})\Big{]}\] \[-\sum_{j=1}^{k_{*}}\exp(\beta_{0j}^{n})\Big{[}v(Y|X;\beta_{1j}^{n} )-v(Y|X;\beta_{1j}^{*}+t_{2})\Big{]}\] \[+\sum_{j=1}^{k_{*}}\Big{(}\exp(\beta_{0j}^{n})-\exp(\beta_{0j}^{* }+t_{1})\Big{)}\Big{[}u(Y|X;\beta_{1j}^{*}+t_{2},a_{j}^{*},b_{j}^{*},\sigma_{j} ^{*})-v(Y|X;\beta_{1j}^{*}+t_{2})\Big{]},\] \[:=A_{n}+B_{n}+E_{n},\] (16)

where we denote \(u(Y|X;\beta_{1},a,b,\sigma):=\exp(\beta_{1}^{\top}X)f(Y|a^{\top}X+b,\sigma)\) and \(v(Y|X;\beta_{1}):=\exp(\beta_{1}^{\top}X)g_{G_{n}}(Y|X)\). Next, by means of the first-order Taylor expansion, we rewrite \(A_{n}\) as

\[A_{n} =\sum_{j=1}^{k_{*}}\sum_{|\alpha|=1}\frac{\exp(\beta_{0j}^{n})}{2 ^{\alpha_{4}}\alpha!}(\Delta_{t_{2}}\beta_{1j}^{n})^{\alpha_{1}}(\Delta a_{ ij}^{n})^{\alpha_{2}}(\Delta b_{j}^{n})^{\alpha_{3}}(\Delta\sigma_{j}^{n})^{ \alpha_{4}}\] \[\qquad\times X^{\alpha_{1}+\alpha_{2}}\exp((\beta_{1j}^{*}+t_{2} )^{\top}X)\cdot\frac{\partial^{|\alpha_{2}|+\alpha_{3}+2\alpha_{4}}f}{\partial h _{1}^{|\alpha_{2}|+\alpha_{3}+2\alpha_{4}}}(Y|(a_{j}^{*})^{\top}X+b_{j}^{*}, \sigma_{j}^{*})+R_{1}(X,Y)\] \[=\sum_{j=1}^{k_{*}}\sum_{2|\ell_{1}|+\ell_{2}=1}^{2}\sum_{\alpha \in\mathcal{I}_{1},\ell_{2}}\frac{\exp(\beta_{0j}^{n})}{2^{\alpha_{4}}\alpha! }(\Delta_{t_{2}}\beta_{1j}^{n})^{\alpha_{1}}(\Delta a_{j}^{n})^{\alpha_{2}}( \Delta b_{j}^{n})^{\alpha_{3}}(\Delta\sigma_{j}^{n})^{\alpha_{4}}\] \[\qquad\times X^{\ell_{1}}\exp((\beta_{1j}^{*}+t_{2})^{\top}X) \cdot\frac{\partial^{\ell_{2}}f}{\partial h_{1}^{\ell_{2}}}(Y|(a_{j}^{*})^{ \top}X+b_{j}^{*},\sigma_{j}^{*})+R_{1}(X,Y),\] (17)

where \(R_{1}(X,Y)\) is a Taylor remainder such that \(R_{1}(X,Y)/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\to 0\) as \(n\to\infty\). Here, the first equality is due to the following partial differential equation for the univariate Gaussian density:

\[\frac{\partial^{\alpha_{4}}f}{\partial h_{2}^{\alpha_{4}}}(Y|(a_{j}^{*})^{ \top}X+b_{j}^{*},\sigma_{j}^{*})=\frac{1}{2^{\alpha_{4}}}\cdot\frac{\partial^{ 2\alpha_{4}}f}{\partial h_{1}^{2\alpha_{4}}}(Y|(a_{j}^{*})^{\top}X+b_{j}^{*}, \sigma_{j}^{*}),\]

while the second equality is obtained by defining \(\ell_{1}=\alpha_{1}+\alpha_{2}\), \(\ell_{2}=|\alpha_{2}|+\alpha_{3}+2\alpha_{4}\) and

\[\mathcal{I}_{\ell_{1},\ell_{2}}:=\left\{\alpha=(\alpha_{i})_{i=1}^{4}\in \mathbb{N}^{d}\times\mathbb{N}^{d}\times\mathbb{N}\times:\alpha_{1}+\alpha_{2 }=\ell_{1},\;\alpha_{3}+2\alpha_{4}=\ell_{2}-|\alpha_{2}|\right\},\] (18)

for all \((\ell_{1},\ell_{2})\in\mathbb{N}^{d}\times\mathbb{N}\) such that \(1\leq 2|\ell_{1}|+\ell_{2}\leq 2\). Analogously, \(B_{n}\) can be rewritten as

\[B_{n}=-\sum_{j=1}^{k_{*}}\sum_{|\gamma|=1}\frac{\exp(\beta_{0j}^{n})}{\gamma!}( \Delta_{t_{2}}\beta_{1j}^{n})^{\gamma}X^{\gamma}\exp((\beta_{1j}^{*}+t_{2})^{ \top}X)g_{G_{n}}(Y|X)+R_{2}(X,Y),\] (19)

where \(R_{2}(X,Y)\) is a Taylor remainder such that \(R_{2}(X,Y)/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\to 0\) as \(n\to\infty\). From the formulations of \(A_{n}\), \(B_{n}\) and \(E_{n}\), we can represent \(Q_{n}\) as the following linear combination

\[Q_{n} =\sum_{j=1}^{k_{*}}\sum_{2|\ell_{1}|+\ell_{2}=0}^{2}T_{\ell_{1}, \ell_{2}}^{n}(j)\cdot X^{\ell_{1}}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)\frac{ \partial^{\ell_{2}}f}{\partial h_{1}^{\ell_{2}}}(Y|(a_{j}^{*})^{\top}X+b_{j}^{*}, \sigma_{j}^{*})\] \[+\sum_{j=1}^{k_{*}}\sum_{|\gamma|=0}^{1}S_{\gamma}^{n}(j)\cdot X^{ \gamma}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)g_{G_{n}}(Y|X)+R_{1}(X,Y)+R_{2}(X,Y),\]

with coefficients being denoted by \(T_{\ell_{1},\ell_{2}}^{n}(j)\) and \(S_{\gamma}^{n}(j)\) for all \(j\in[k_{*}]\), \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\) and \(0\leq|\gamma|\leq 1\) where

\[T_{\ell_{1},\ell_{2}}^{n}(j)=\begin{cases}\sum_{\alpha\in\mathcal{I}_{\ell_{1}, \ell_{2}}}\frac{\exp(\beta_{0j}^{n})}{2^{\alpha_{4}}\alpha!}(\Delta_{t_{2}}\beta_ {1j}^{n})^{\alpha_{1}}(\Delta a_{j}^{n})^{\alpha_{2}}(\Delta b_{j}^{n})^{\alpha _{3}}(\Delta\sigma_{j}^{n})^{\alpha_{4}},&(\ell_{1},\ell_{2})\neq(\mathbf{0}_{ d},0),\\ \exp(\beta_{0j}^{n})-\exp(\beta_{0j}^{*}+t_{1}),&(\ell_{1},\ell_{2})=(\mathbf{0}_ {d},0);\end{cases}\] (20)\[S_{\gamma}^{n}(j)=\begin{cases}-\dfrac{\exp(\beta_{0j}^{n})}{\gamma!}(\Delta_{t_{2} }\beta_{1j}^{n})^{\gamma},&|\gamma|\neq 0,\\ -\exp(\beta_{0j}^{n})+\exp(\beta_{0j}^{*}+t_{1}),&|\gamma|=0.\end{cases}\] (21)

**Step 2: Non-vanishing coefficients**

Now, we will demonstrate by contradiction that at least one among terms of the forms \(T_{\ell_{1},\ell_{2}}^{n}(j)/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\) and \(S_{\gamma}^{n}(j)/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\) does not approach zero. Indeed, assume that all of them vanish when \(n\to\infty\), then we get

\[\frac{1}{\mathcal{D}_{1}^{\prime}(G_{n},G_{*})}\cdot\sum_{j=1}^{k_{*}}\Big{|} \exp(\beta_{0j}^{n})-\exp(\beta_{0j}^{*}+t_{1})\Big{|}=\sum_{j=1}^{k_{*}}\frac {|T_{\bm{0}_{d,0}}^{n}(j)|}{\mathcal{D}_{1}^{\prime}(G_{n},G_{*})}\to 0.\] (22)

Similarly, by considering the limits of \(T_{\ell_{1},\ell_{2}}^{n}(j)/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\) for all \(j\in[k_{*}]\) and \(1\leq 2|\ell_{1}|+\ell_{2}\leq 2\), we obtain that

\[\frac{1}{\mathcal{D}_{1}^{\prime}(G_{n},G_{*})}\cdot\sum_{j=1}^{k_{*}}\exp( \beta_{0j}^{n})\|(\Delta_{t_{2}}\beta_{1j}^{n},\Delta a_{j}^{n},\Delta b_{j}^ {n},\Delta\sigma_{j}^{n})\|\to 0.\] (23)

Combine the results in equations (22) and 23, we have \(1=\mathcal{D}_{1}^{\prime}(G_{n},G_{*})/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\to 0\), which is a contradiction. As a result, not all the limits of \(T_{\ell_{1},\ell_{2}}^{n}(j)/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\) and \(S_{\gamma}^{n}(j)/\mathcal{D}_{1}^{\prime}(G_{n},G_{*})\) equal to zero.

**Step 3: Fatou's lemma involvement**

Thus, let \(m_{n}\) be the maximum of the absolute values of those terms, we have that \(1/m_{n}\not\to\infty\). Then, the Fatou's lemma says that

\[\lim_{n\to\infty}\frac{\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X)) ]}{m_{n}\cdot\mathcal{D}_{1}^{\prime}(G_{n},G_{*})}\geq\int\liminf_{n\to\infty }\frac{|g_{G_{n}}(Y|X)-g_{G_{*}}(Y|X)|}{2m_{n}\cdot\mathcal{D}_{1}^{\prime}(G _{n},G_{*})}\,\mathrm{d}(X,Y).\] (24)

By assumption, the left-hand side of the above equation equals to zero, therefore, the integrand in the right-hand side also equals to zero for almost surely \((X,Y)\), which leads to the following limit: \(Q_{n}/[m_{n}\mathcal{D}_{1}^{\prime}(G_{n},G_{*})]\to 0\) as \(n\to\infty\) for almost surely \((X,Y)\). More specifically, we have

\[\sum_{j=1}^{k_{*}}\sum_{2|\ell_{1}|+\ell_{2}=0}^{2}\eta_{\ell_{1},\ell_{2}}(j)\cdot X^{\ell_{1}}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)\frac{ \partial^{\ell_{2}}f}{\partial h_{1}^{\ell_{2}}}(Y|(a_{j}^{*})^{\top}X+b_{j}^ {*},\sigma_{j}^{*})\] \[+\ \sum_{j=1}^{k_{*}}\sum_{|\gamma|=0}^{1}\omega_{\gamma}(j)\cdot X ^{\gamma}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)g_{G_{*}}(Y|X)=0,\]

for almost surely \((X,Y)\), where \(\eta_{\ell_{1},\ell_{2}}(j)\) and \(\omega_{\gamma}(j)\) are the limits of \(T_{\ell_{1},\ell_{2}}^{n}(j)/[m_{n}\mathcal{D}_{1}^{\prime}(G_{n},G_{*})]\) and \(S_{\gamma}^{n}(j)/[m_{n}\mathcal{D}_{1}^{\prime}(G_{n},G_{*})]\), respectively, for all \(j\in[k_{*}]\), \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\) and \(0\leq|\gamma|\leq 1\). Here, at least one among \(\eta_{\ell_{1},\ell_{2}}(j)\) and \(\omega_{\gamma}(j)\) is different from zero. On the other hand, since the set

\[\mathcal{W}_{1}: =\left\{X^{\ell_{1}}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)\frac{ \partial^{\ell_{2}}f}{\partial h_{1}^{\ell_{2}}}(Y|(a_{j}^{*})^{\top}X+b_{j}^ {*},\sigma_{j}^{*}):j\in[k_{*}],\ 0\leq 2|\ell_{1}|+\ell_{2}\leq 2\right\}\] \[\cup\left\{X^{\gamma}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)g_{G_{*}} (Y|X):j\in[k_{*}],\ 0\leq|\gamma|\leq 1\right\},\] (25)

is linearly independent (see Lemma 2 at the end of this proof), we obtain that \(\eta_{\ell_{1},\ell_{2}}(j)=\omega_{\gamma}(j)=0\) for all \(j\in[k_{*}]\), \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\) and \(0\leq|\gamma|\leq 1\), which is a contradiction.

Thus, we reach the local inequality in 15, that is, there exists \(\varepsilon^{\prime}>0\) that satisfies

\[\inf_{G\in\mathcal{E}_{k_{*}}(\bm{0}):\mathcal{D}_{1}(G,G_{*})\leq\varepsilon^{ \prime}}\mathbb{E}_{X}[V(g_{G}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}_{1}(G,G_{ *})>0.\]

**Global version**: Thus, it suffices to prove its following global inequality:

\[\inf_{\begin{subarray}{c}G\in\mathcal{E}_{k_{*}}(\Theta):\\ \mathcal{D}_{1}(G,G_{*})>\varepsilon^{\prime}\end{subarray}}\mathbb{E}_{X}[V(g_ {G}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}_{1}(G,G_{*})>0.\] (26)

Assume by contrary that there exists a sequence \(G^{\prime}_{n}\in\mathcal{E}_{k_{*}}(\Theta)\) that satisfies

\[\begin{cases}\lim_{n\to\infty}\mathbb{E}_{X}[V(g_{G^{\prime}_{n}}(\cdot|X),g_{ G_{*}}(\cdot|X))]/\mathcal{D}_{1}(G^{\prime}_{n},G_{*})=0,\\ \mathcal{D}_{1}(G^{\prime}_{n},G_{*})>\varepsilon^{\prime}.\end{cases}\]

Therefore, we obtain that \(\mathbb{E}_{X}[V(g_{G^{\prime}_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]\to 0\) as \(n\to\infty\). Since the set \(\Theta\) is compact, we are able to replace the sequence \(G^{\prime}_{n}\) by its subsequence which converges to some mixing measure \(G^{\prime}\in\mathcal{E}_{k_{*}}(\Theta)\) such that \(\mathcal{D}(G^{\prime},G_{*})>\varepsilon^{\prime}\). Then, by the Fatou's lemma, we get

\[\lim_{n\to\infty}\mathbb{E}_{X}[V(g_{G^{\prime}_{n}}(\cdot|X),g_{G_{*}}(\cdot |X))]\geq\frac{1}{2}\int\liminf_{n\to\infty}|g_{G^{\prime}_{n}}(Y|X)-g_{G_{*}} (Y|X)|\;\mathrm{d}(X,Y),\]

which implies that

\[\int|g_{G^{\prime}}(Y|X)-g_{G_{*}}(Y|X)|\mathrm{d}(X,Y)=0\]

Thus, we obtain that \(g_{G^{\prime}}(Y|X)=g_{G_{*}}(Y|X)\) for almost surely \((X,Y)\). Now that the softmax gating Gaussian mixture of experts is identifiable up to a translation (see Proposition 1), the mixing measure \(G^{\prime}\) admits the form \(G^{\prime}=\sum_{i=1}^{k_{*}}\exp(\beta^{*}_{0\tau(i)}+t_{1})\delta_{(\beta^{ *}_{1\tau(i)}+t_{2},a^{*}_{\tau(i)},b^{*}_{\tau(i)},\sigma^{*}_{\tau(i)})}\) for some \(t_{1}\in\mathbb{R}\) and \(t_{2}\in\mathbb{R}^{d}\), where \(\tau\) is some permutation of the set \(\{1,2,\ldots,k\}\). This leads to the fact that \(\mathcal{D}_{1}(G^{\prime},G_{*})=0\), which contradicts the hypothesis \(\mathcal{D}_{1}(G^{\prime},G_{*})>\varepsilon^{\prime}>0\). Hence, we obtain the inequality in equation (14).

To complete the proof, we will show the previous claim regarding the independence of elements in \(\mathcal{W}_{1}\) in the following lemma:

**Lemma 2**.: _The set \(\mathcal{W}_{1}\) defined in equation (25) is linearly independent w.r.t \(X\) and \(Y\)._

Proof of Lemma 2.: Assume that the following holds for almost surely \((X,Y)\):

\[\sum_{j=1}^{k_{*}}\sum_{2|\ell_{1}|+\ell_{2}=0}^{2}\eta_{\ell_{1},\ell_{2}}(j)\cdot X^{\ell_{1}}\exp((\beta^{*}_{1j}+t_{2})^{\top}X)\frac{ \partial^{\ell_{2}}f}{\partial h^{\ell_{2}}_{1}}(Y|(a^{*}_{j})^{\top}X+b^{*} _{j},\sigma^{*}_{j})\] \[+\;\sum_{j=1}^{k_{*}}\sum_{|\gamma|=0}^{1}\omega_{\gamma}(j) \cdot X^{\gamma}\exp((\beta^{*}_{1j}+t_{2})^{\top}X)g_{G_{*}}(Y|X)=0,\]

where \(\eta_{\ell_{1},\ell_{2}}(j)\in\mathbb{R}\) and \(\omega_{\gamma}(j)\in\mathbb{R}\). Then, we need to show that \(\eta_{\ell_{1},\ell_{2}}(j)=\omega_{\gamma}(j)=0\), for all \(j\in[k_{*}]\), \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\) and \(0\leq|\gamma|\leq 1\). The above equation is equivalent to

\[\sum_{j=1}^{k_{*}}\sum_{\zeta=0}^{1}\Big{[}\sum_{\ell_{2}=0}^{2 -2\zeta}\eta_{\zeta,\ell_{2}}(j)\frac{\partial^{\ell_{2}}f}{\partial h^{\ell_ {2}}_{1}}(Y|(a^{*}_{j})^{\top}X+b^{*}_{j},\sigma^{*}_{j})+\omega_{\zeta}(j)g_{ G_{*}}(Y|X)\Big{]}X^{\zeta}\exp((\beta^{*}_{1j}+t_{2})^{\top}X)=0,\]

for almost surely \((X,Y)\). Since \(\beta^{*}_{11},\ldots,\beta^{*}_{1k_{*}}\) are \(k_{*}\) distinct values, we get that the set \(\big{\{}\exp((\beta^{*}_{1j}+t_{2})^{\top}X):j\in[k_{*}]\big{\}}\) is linearly independent, which implies that

\[\sum_{\zeta=0}^{1}\Big{[}\sum_{\ell_{2}=0}^{2-2\zeta}\eta_{\zeta, \ell_{2}}(j)\frac{\partial^{\ell_{2}}f}{\partial h^{\ell_{2}}_{1}}(Y|(a^{*}_{ j})^{\top}X+b^{*}_{j},\sigma^{*}_{j})+\omega_{\zeta}(j)g_{G_{*}}(Y|X)\Big{]}X^{ \zeta}=0,\]

for all \(j\in[k_{*}]\) for almost surely \((X,Y)\). Obviously, the above equation is a polynomial of \(X\in\mathcal{X}\), where \(\mathcal{X}\) is a compact subset of \(\mathbb{R}^{d}\). Then, we achieve that

\[\sum_{\ell_{2}=0}^{2-2\zeta}\eta_{\zeta,\ell_{2}}(j)\frac{\partial^{ \ell_{2}}f}{\partial h^{\ell_{2}}_{1}}(Y|(a^{*}_{j})^{\top}X+b^{*}_{j},\sigma^ {*}_{j})+\omega_{\zeta}(j)g_{G_{*}}(Y|X)=0,\]for all \(j\in[k_{*}]\) and \(\zeta\in\{0,1\}\), for almost surely \((X,Y)\). Again, as \((a^{*}_{j},b^{*}_{j},\sigma^{*}_{j})\) for \(j\in[k_{*}]\) are \(k_{*}\) distinct tuples, we have that \(((a^{*}_{j})^{\top}X+b^{*}_{j},\sigma^{*}_{j})\) for \(j\in[k_{*}]\) are also \(k_{*}\) distinct tuples for almost surely \(X\). Therefore, \(\left\{\frac{\partial^{\ell_{2}}f}{\partial h^{\ell_{2}}_{1}}(Y|(a^{*}_{j})^ {\top}X+b^{*}_{j},\sigma^{*}_{j}),\;g_{G_{*}}(Y|X)\right\}\) is a linearly independent set. As a result, \(\eta_{\ell_{1},\ell_{2}}(j)=\omega_{\gamma}(j)=0\) for all \(j\in[k_{*}]\), \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\) and \(0\leq|\gamma|\leq 1\).

Hence, the proof is completed. 

### Proof of Theorem 2

In this proof, we adapt the framework in Appendix A.1 to the setting of Theorem 2. However, since the arguments utilized for the global version part remain the same (up to some changes of notations) for the over-fitted settings, they will not be presented here again. Thus, we focus only on proving the following local inequality:

\[\lim_{\varepsilon\to 0}\inf_{\begin{subarray}{c}\mathcal{D}_{G}\in \mathcal{D}_{k}(\Theta):\\ \mathcal{D}_{2}(G,G_{*})\leq\varepsilon\end{subarray}}\mathbb{E}_{X}[V(g_{G}( \cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}_{2}(G,G_{*})>0.\] (27)

Assume that the above claim is not true, then there exists a sequence of mixing measures \(G_{n}:=\sum_{i=1}^{k_{n}}\exp(\beta^{n}_{0i})\delta_{(\beta^{n}_{1i},a^{n}_{i },b^{n}_{i},\sigma^{*}_{i})}\in\mathcal{O}_{k}(\Theta)\) such that

\[\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{ D}_{2}(G_{n},G_{*}) \to 0,\] \[\mathcal{D}_{2}(G_{n},G_{*}) \to 0,\]

when \(n\) tends to infinity. Since the proof argument is asymptotic, we also assume that \(k_{n}=k^{\prime}\) for all \(n\geq 1\). Following the proof argument of Theorem 1 in Appendix A.1, we also assume that the Voronoi cells \(\mathcal{A}_{j}=\mathcal{A}^{n}_{j}\) does not change with \(n\) for all \(j\in[k_{*}]\). Additionally, since \(\mathcal{D}_{2}(G_{n},G_{*})\to 0\), we have \((a^{n}_{i},b^{n}_{i},\sigma^{n}_{i})\to(a^{*}_{j},b^{*}_{j},\sigma^{*}_{j})\) for any \(i\in\mathcal{A}_{j}\) as \(n\) approaches infinity. Furthermore, there exist \(t_{1}\in\mathbb{R}\) and \(t_{2}\in\mathbb{R}^{d}\) such that \(\sum_{i\in\mathcal{A}_{j}}\exp(\beta^{n}_{0i})\to\exp(\beta^{*}_{0j}+t_{1})\) and \(\beta^{n}_{1i}\to\beta^{*}_{1j}+t_{2}\) for any \(i\in\mathcal{A}_{j}\) and \(j\in[k_{*}]\). Then, we can upper bound the Voronoi loss function \(\mathcal{D}_{2}\) as \(\mathcal{D}_{2}(G_{n},G_{*})\leq\mathcal{D}^{\prime}_{2}(G_{n},G_{*})\), where

\[\mathcal{D}^{\prime}_{2}(G_{n},G_{*}):=\sum_{j:|\mathcal{A}_{j}|>1}\sum_{i\in \mathcal{A}_{j}}\exp(\beta^{n}_{0i})\Big{(}\|(\Delta_{t_{2}}\beta^{n}_{1ij}, \Delta b^{n}_{ij})\|^{\bar{r}(|\mathcal{A}_{j}|)}+\|(\Delta a^{n}_{ij},\Delta \sigma^{n}_{ij})\|^{\bar{r}(|\mathcal{A}_{j}|)/2}\Big{)}\]

in which

\[\Delta_{t_{2}}\beta^{n}_{1ij}:=\beta^{n}_{1i}-\beta^{*}_{1j}-t_{2},\;\;\; \Delta a^{n}_{ij}:=a^{n}_{i}-a^{*}_{j},\]

\[\Delta b^{n}_{ij}:=b^{n}_{i}-b^{*}_{j},\;\;\;\Delta\sigma^{n}_{ij}:=\sigma^{n }_{i}-\sigma^{*}_{j}.\]

Recall that \(\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}_{2}(G_{n}, G_{*})\to 0\) as \(n\to\infty\), which leads to

\[\mathbb{E}_{X}[V(g_{G_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))]/\mathcal{D}^{\prime} _{2}(G_{n},G_{*})\to 0.\]

#### Step 1: Density Decomposition

In this step, we decompose the quantity \(Q_{n}=[\sum_{j=1}^{k_{*}}\exp((\beta^{*}_{1j}+t_{2})^{\top}X+\beta^{*}_{0j}+t _{1})]\cdot[g_{G_{n}}(Y|X)-g_{G_{*}}(Y|X)]\) with abuse of notations in Appendix A.1 as follows:

\[Q_{n} =\sum_{j=1}^{k_{*}}\sum_{i\in\mathcal{A}_{j}}\exp(\beta^{n}_{0i} )\Big{[}u(Y|X;\beta^{n}_{1i},a^{n}_{i},b^{n}_{i},\sigma^{n}_{i})-u(Y|X;\beta^{* }_{1j}+t_{2},a^{*}_{j},b^{*}_{j},\sigma^{*}_{j})\Big{]}\] \[-\sum_{j=1}^{k_{*}}\sum_{i\in\mathcal{A}_{j}}\exp(\beta^{n}_{0i} )\Big{[}v(Y|X;\beta^{n}_{1i})-v(Y|X;\beta^{*}_{1j}+t_{2})\Big{]}\] \[+\sum_{j=1}^{k_{*}}\Big{(}\sum_{i\in\mathcal{A}_{j}}\exp(\beta^{n} _{0i})-\exp(\beta^{*}_{0j}+t_{1})\Big{)}\Big{[}u(Y|X;\beta^{*}_{1j}+t_{2},a^{* }_{j},b^{*}_{j},\sigma^{*}_{j})-v(Y|X;\beta^{*}_{1j}+t_{2})\Big{]},\] \[:=A_{n}+B_{n}+E_{n},\]

[MISSING_PAGE_FAIL:20]

#### Step 2: Non-vanishing coefficients

Next, we will show that not all the quantities \(T^{n}_{\ell_{1},\ell_{2}}(j)/\mathcal{D}^{\prime}_{2}(G_{n},G_{*})\) and \(S^{n}_{\gamma}(j)/\mathcal{D}^{\prime}_{2}(G_{n},G_{*})\) go to 0 as \(n\to\infty\). Assume that all of them vanish when \(n\) tends to infinity. Then, by arguing similarly as in equations (22) and (23), we obtain that

\[\frac{1}{\mathcal{D}^{\prime}_{2}(G_{n},G_{*})}\cdot\Bigg{[}\sum_ {j=1}^{k_{*}} \Big{|}\sum_{i\in\mathcal{A}_{j}}\exp(\beta^{n}_{0i})-\exp(\beta^{*}_{0j}+t _{1})\Big{|}\] \[+\sum_{j:|\mathcal{A}_{j}|=1}\sum_{i\in\mathcal{A}_{j}}\exp(\beta^ {n}_{0i})\|(\Delta_{t_{2}}\beta^{n}_{1ij},\Delta a^{n}_{ij},\Delta b^{n}_{ij}, \Delta\sigma^{n}_{ij})\|\Bigg{]}\to 0.\]

Putting the above limit and the formulation of \(\mathcal{D}_{2}(G_{n},G_{*})\) together, we deduce that

\[\frac{1}{\mathcal{D}^{\prime}_{2}(G_{n},G_{*})}\cdot\sum_{j:| \mathcal{A}_{j}|>1}\sum_{i\in\mathcal{A}_{j}}\exp(\beta_{0i})\Big{(}\|(\Delta _{t_{2}}\beta^{n}_{1ij},\Delta b^{n}_{ij})\|^{\bar{r}(|\mathcal{A}_{j}|)}+\|( \Delta a^{n}_{ij},\Delta\sigma^{n}_{ij})\|^{\bar{r}(|\mathcal{A}_{j}|)/2} \Big{)}\not\to 0,\]

which indicates that there exists some index \(j^{*}\in[k_{*}]\) such that \(|\mathcal{A}_{j^{*}}|>1\) and

\[\frac{1}{\mathcal{D}^{\prime}_{2}(G_{n},G_{*})}\cdot\sum_{i\in \mathcal{A}_{j^{*}}}\exp(\beta_{0i})\Big{(}\|(\Delta_{t_{2}}\beta^{n}_{1ij^{* }},\Delta b^{n}_{ij^{*}})\|^{\bar{r}(|\mathcal{A}_{j}|)}+\|(\Delta a^{n}_{ij^{ *}},\Delta\sigma^{n}_{ij^{*}})\|^{\bar{r}(|\mathcal{A}_{j}|)/2}\Big{)}\not \to 0,\]

for all \(t_{2}\in\mathbb{R}^{d}\). Without loss of generality, we may assume that \(j^{*}=1\). Recall that for \((\ell_{1},\ell_{2})\in\mathbb{N}^{d}\times\mathbb{N}\) such that \(1\leq|\ell_{1}|+\ell_{2}\leq\bar{r}(|\mathcal{A}_{1}|)\), we have \(T^{n}_{\ell_{1},\ell_{2}}(1)/\mathcal{D}^{\prime}_{2}(G_{n},G_{*})\to 0\) as \(n\to\infty\). Thus, by dividing this ratio and the left hand side of the above equation and let \(t_{2}=0\), we obtain that

\[\frac{\sum_{i\in\mathcal{A}_{1}}\sum_{\alpha\in\mathcal{I}_{\ell_{1},\ell_{2}} }\frac{\exp(\beta^{n}_{0i})}{2^{\alpha_{4}}\alpha!}(\Delta_{t_{2}}\beta^{n}_{1 i1})^{\alpha_{1}}(\Delta a^{n}_{i1})^{\alpha_{2}}(\Delta b^{n}_{i1})^{\alpha_{3}}( \Delta\sigma^{n}_{i1})^{\alpha_{4}}}{\sum_{i\in\mathcal{A}_{1}}\exp(\beta^{n} _{0i})\Big{(}\|(\Delta_{t_{2}}\beta^{n}_{1i1},\Delta b^{n}_{1i})\|^{\bar{r}(| \mathcal{A}_{1}|)}+\|(\Delta a^{n}_{i1},\Delta\sigma^{n}_{i1})\|^{\bar{r}(| \mathcal{A}_{1}|)/2}\Big{)}}\to 0,\] (29)

for all \((\ell_{1},\ell_{2})\) such that \(1\leq|\ell_{1}|+\ell_{2}\leq\bar{r}(|\mathcal{A}_{1}|)\).

Let us define \(\overline{M}_{n}:=\max\{\|\Delta_{t_{2}}\beta^{n}_{1i1}\|,\|\Delta a^{n}_{i1} \|^{1/2},|\Delta b^{n}_{i1}|,|\Delta\sigma^{n}_{i1}|^{1/2}:i\in\mathcal{A}_{1}\}\) and \(\overline{\beta}_{n}:=\max_{i\in\mathcal{A}_{1}}\exp(\beta^{n}_{0i})\). Note that the sequence \(\exp(\beta^{n}_{0i})/\overline{\beta}_{n}\) is bounded, therefore, we can replace it by its subsequence that has a positive limit \(p^{2}_{5i}:=\lim_{n\to\infty}\exp(\beta^{n}_{0i})/\overline{\beta}_{n}\). Thus, at least one among \(p^{2}_{5i}\), for \(i\in\mathcal{A}_{1}\), equals 1.

In addition, we also define

\[(\Delta_{t_{2}}\beta^{n}_{1i1})/\overline{M}_{n}\to p_{1i}, (\Delta a^{n}_{i1})/\overline{M}_{n}\to p_{2i},\] \[(\Delta b^{n}_{1i})/\overline{M}_{n}\to p_{3i}, (\Delta\sigma^{n}_{i1})/[2\overline{M}_{n}]\to p_{4i}.\]

Here, at least one of \(p_{1i}\), \(p_{2i}\), \(p_{3i}\) and \(p_{4i}\) for \(i\in\mathcal{A}_{1}\) equals either 1 or \(-1\). Next, we divide both the numerator and the denominator of the ratio in equation (29) by \(\overline{\beta}_{n}\overline{M}_{n}^{\ell_{1}+\ell_{2}}\), and then achieve the following system of polynomial equations:

\[\sum_{i\in\mathcal{A}_{1}}\sum_{\alpha\in\mathcal{I}_{\ell_{1},\ell_{2}}}\frac {1}{\alpha!}\cdot p^{2}_{5i}p^{\alpha_{1}}_{1i}p^{\alpha_{2}}_{2i}p^{\alpha_{3}}_ {3i}p^{\alpha_{4}}_{4i}=0,\]

for all \((\ell_{1},\ell_{2})\in\mathbb{N}^{d}\times\mathbb{N}\) such that \(1\leq|\ell_{1}|+\ell_{2}\leq\bar{r}(|\mathcal{A}_{1}|)\). However, based on the definition of \(\bar{r}(|\mathcal{A}_{1}|)\), the above system has no non-trivial solutions, which is a contradiction. Thus, not all the quantities \(T^{n}_{\ell_{1},\ell_{2}}(j)/\mathcal{D}^{\prime}_{2}(G_{n},G_{*})\) and \(S^{n}_{\gamma}(j)/\mathcal{D}^{\prime}_{2}(G_{n},G_{*})\) go to 0 as \(n\to\infty\).

#### Step 3: Fatou's lemma involvement

Subsequently, we denote by \(m_{n}\) be the maximum of the absolute values of those quantities. Based on the result in Step 2, we know that \(1/m_{n}\not\to\infty\). Then, by applying the Fatou's lemma as in equation (24), we get that \(Q_{n}/[m_{n}\mathcal{D}^{\prime}_{2}(G_{n},G_{*})]\to 0\) as \(n\to\infty\) for almost surely \((X,Y)\). It followsfrom the decomposition of \(Q_{n}\) in equation (28) that

\[\sum_{j=1}^{k_{*}}\sum_{2|\ell_{1}|+\ell_{2}=0}^{2\bar{r}(|\mathcal{A }_{j}|)} \eta_{\ell_{1},\ell_{2}}(j)\cdot X^{\ell_{1}}\exp((\beta_{1j}^{*}+t_{2})^{ \top}X)\frac{\partial^{\ell_{2}}f}{\partial h_{1}^{\ell_{2}}}(Y|(a_{j}^{*})^{ \top}X+b_{j}^{*},\sigma_{j}^{*})\] \[+\ \sum_{j=1}^{k_{*}}\sum_{|\gamma|=0}^{1+\mathbf{1}_{\{|\mathcal{ A}_{j}|>1\}}}\omega_{\gamma}(j)\cdot X^{\gamma}\exp((\beta_{1j}^{*}+t_{2})^{ \top}X)g_{G_{*}}(Y|X)=0,\]

for almost surely \((X,Y)\), where \(\eta_{\ell_{1},\ell_{2}}(j)\) and \(\omega_{\gamma}(j)\) denote the limits of \(T_{\ell_{1},\ell_{2}}^{n}(j)/[m_{n}\mathcal{D}_{2}^{\prime}(G_{n},G_{*})]\) and \(S_{\gamma}^{n}(j)/[m_{n}\mathcal{D}_{2}^{\prime}(G_{n},G_{*})]\) as \(n\to\infty\), respectively, for all \(j\in[k_{*}]\), \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\bar{r}(|\mathcal{A}_{j}|)\) and \(0\leq|\gamma|\leq 1+\mathbf{1}_{\{|\mathcal{A}_{j}|>1\}}\). By definition, at least one among \(\eta_{\ell_{1},\ell_{2}}(j)\) and \(\omega_{\gamma}(j)\) is different from zero. Nevertheless, as the set

\[\mathcal{W}_{2}: =\left\{X^{\ell_{1}}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)\frac{ \partial^{\ell_{2}}f}{\partial h_{1}^{\ell_{2}}}(Y|(a_{j}^{*})^{\top}X+b_{j}^ {*},\sigma_{j}^{*}):j\in[k_{*}],\ 0\leq 2|\ell_{1}|+\ell_{2}\leq 2\bar{r}(| \mathcal{A}_{j}|)\right\}\] \[\cup\left\{X^{\gamma}\exp((\beta_{1j}^{*}+t_{2})^{\top}X)g_{G_{*} }(Y|X):j\in[k_{*}],\ 0\leq|\gamma|\leq 1+\mathbf{1}_{\{|\mathcal{A}_{j}|>1\}} \right\},\] (30)

is linearly independent w.r.t \(X\) and \(Y\) (proof can be done similarly to Lemma 2), it follows that

\[\eta_{\ell_{1},\ell_{2}}(j)=\omega_{\gamma}(j)=0,\]

for all \(j\in[k_{*}]\), \(0\leq 2|\ell_{1}|+\ell_{2}\leq 2\bar{r}(|\mathcal{A}_{j}|)\) and \(0\leq|\gamma|\leq 1+\mathbf{1}_{\{|\mathcal{A}_{j}|>1\}}\), which is a contradiction. Hence, we achieve the inequality in equation (27), and complete the proof.

## Appendix B Proofs of Auxiliary Results

In this appendix, we provide proofs for the results of Proposition 1 and Proposition 2 in Appendix B.1 and Appendix B.2, respectively, while we leave that for Lemma 1 in Appendix B.3.

### Proof of Proposition 1

Given the notations in Proposition 1, assume that the equation \(g_{G}(Y|X)=g_{G^{\prime}}(Y|X)\) holds true, that is,

\[\sum_{i=1}^{k}\frac{\exp((\beta_{1i})^{\top}X+\beta_{0i})}{\sum_{ j=1}^{k}\exp((\beta_{1j})^{\top}X+\beta_{0j})}\cdot f(Y|(a_{i})^{\top}X+b_{i}, \sigma_{i})\] \[=\sum_{i=1}^{k^{\prime}}\frac{\exp((\beta_{1i}^{\prime})^{\top}X+ \beta_{0i}^{\prime})}{\sum_{j=1}^{k^{\prime}}\exp((\beta_{1j}^{\prime})^{\top }X+\beta_{0j}^{\prime})}\cdot f(Y|(a_{i}^{\prime})^{\top}X+b_{i}^{\prime}, \sigma_{i}^{\prime}),\] (31)

for almost surely \((X,Y)\). Then, it follows from the identifiability of the location-scale Gaussian mixtures [56, 57] that the number of components and the weight set of the mixing measure \(G\) equal to those of its counterpart \(G^{\prime}\), i.e. \(k=k^{\prime}\) and

\[\left\{\frac{\exp((\beta_{1i})^{\top}X+\beta_{0i})}{\sum_{j=1}^{k}\exp((\beta_ {1j})^{\top}X+\beta_{0j})}:i\in[k]\right\}\equiv\left\{\frac{\exp((\beta_{1i} ^{\prime})^{\top}X+\beta_{0i}^{\prime})}{\sum_{j=1}^{k}\exp((\beta_{1j}^{\prime })^{\top}X+\beta_{0j}^{\prime})}:i\in[k]\right\},\]

for almost surely \(X\). For simplicity, we may assume that

\[\frac{\exp((\beta_{1i})^{\top}X+\beta_{0i})}{\sum_{j=1}^{k}\exp((\beta_{1j})^{ \top}X+\beta_{0j})}=\frac{\exp((\beta_{1i}^{\prime})^{\top}X+\beta_{0i}^{ \prime})}{\sum_{j=1}^{k}\exp((\beta_{1j}^{\prime})^{\top}X+\beta_{0j}^{\prime})},\]

for all \(i\in[k]\). Since the softmax function is invariant to translation, we get that \(\beta_{0i}=\beta_{0i}^{\prime}+t_{1}\) and \(\beta_{1i}=\beta_{1i}^{\prime}+t_{2}\) for some \(t_{1}\in\mathbb{R}\) and \(t_{2}\in\mathbb{R}^{d}\). Therefore, equation (31) reduces to

\[\sum_{i=1}^{k}\exp(\beta_{0i})u(Y|X;\beta_{1i},a_{i},b_{i},\sigma_{i})=\sum_{i=1 }^{k}\exp(\beta_{0i})u(Y|X;\beta_{1i},a_{i}^{\prime},b_{i}^{\prime},\sigma_{i}^{ \prime})),\] (32)for almost surely \((X,Y)\), where \(u(Y|X;\beta_{1},a,b,\sigma):=\exp(\beta_{1}^{\top}X)f(Y|a^{\top}X+b,\sigma)\) for all \(i\in[k]\). Next, we will partition the index set \([k]\) into \(q\) subsets \(U_{1},U_{2},\ldots,U_{q}\) such that for each \(\ell\in[q]\), we have \(\exp(\beta_{0i})=\exp(\beta_{0i^{\prime}})\) for any \(i,i^{\prime}\in U_{\ell}\). As a result, equation (32) can be rewritten as

\[\sum_{\ell=1}^{q}\sum_{i\in U_{\ell}}\exp(\beta_{0i})u(Y|X;\beta_{1i},a_{i},b_ {i},\sigma_{i})=\sum_{\ell=1}^{q}\sum_{i\in U_{\ell}}\exp(\beta_{0i})u(Y|X; \beta_{1i},a_{i}^{\prime},b_{i}^{\prime},\sigma_{i}^{\prime}),\]

for almost surely \((X,Y)\). Given the above equation, for each \(\ell\in[q]\), we obtain that

\[\left\{((a_{i})^{\top}X+b_{i},\sigma_{i}):i\in U_{\ell}\right\}\equiv\left\{ ((a_{i})^{\top}X+b_{i}^{\prime},\sigma_{i}^{\prime}):i\in U_{\ell}\right\},\]

for almost surely \(X\), which directly leads to

\[\left\{(a_{i},b_{i},\sigma_{i}):i\in U_{\ell}\right\}\equiv\left\{(a_{i}^{ \prime},b_{i}^{\prime},\sigma_{i}^{\prime}):i\in U_{\ell}\right\}.\]

WLOG, we assume that \((a_{i},b_{i},\sigma_{i})=(a_{i}^{\prime},b_{i}^{\prime},\sigma_{i}^{\prime})\) for all \(i\in U_{\ell}\). Consequently,

\[\sum_{\ell=1}^{q}\sum_{i\in U_{\ell}}\exp(\beta_{0i})\delta_{\{\beta_{1i},a_{ i},b_{i},\sigma_{i}\}}=\sum_{\ell=1}^{q}\sum_{i\in U_{\ell}}\exp(\beta_{0i}^{ \prime}+t_{1})\delta_{\{\beta_{1i}^{\prime}+t_{2},a_{i}^{\prime},b_{i}^{ \prime},\sigma_{i}^{\prime}\}},\]

or equivalently, \(G\equiv G_{t_{1},t_{2}}^{\prime}\). Hence, the proof is completed.

### Proof of Proposition 2

Our proof will be based on the convergence rates of density estimation from MLE in Theorem 7.4 in [58]. Before stating this result here, let us introduce some necessary notations. Firstly, let \(\mathcal{P}_{k}(\Theta)\) be the set of conditional densities of all mixing measures in \(\mathcal{O}_{k}(\Theta)\), i.e., \(\mathcal{P}_{k}(\Theta):=\{g_{G}(Y|X):G\in\mathcal{O}_{k}(\Theta)\}\). Additionally, we define

\[\widetilde{\mathcal{P}}_{k}(\Theta) :=\{g_{(G+G_{*})/2}(Y|X):G\in\mathcal{O}_{k}(\Theta)\},\] \[\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta) :=\{g_{(G+G_{*})/2}^{1/2}(Y|X):G\in\mathcal{O}_{k}(\Theta)\}.\]

Next, for each \(\delta>0\), the Hellinger ball centered around the conditional density \(g_{G_{*}}(Y|X)\) and intersected with the set \(\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta)\) is denoted by

\[\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta,\delta):=\left\{g^{1/2}\in\widetilde {\mathcal{P}}_{k}^{1/2}(\Theta):h(g,g_{G_{*}})\leq\delta\right\}.\]

Finally, in order to measure the size of the above set, [58] proposes using the following quantity:

\[\mathcal{J}_{B}(\delta,\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta,\delta)):= \int_{\delta^{2}/2^{13}}^{\delta}H_{B}^{1/2}(t,\widetilde{\mathcal{P}}_{k}^{1/ 2}(\Theta,t),\|\cdot\|)\;\mathrm{d}t\vee\delta,\] (33)

where \(H_{B}(t,\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta,t),\|\cdot\|)\) denotes the bracketing entropy [58] of \(\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta,u)\) under the \(2\)-norm, and \(t\vee\delta:=\max\{t,\delta\}\). Now, we are ready to recall the statement of Theorem 7.4 in [58]:

**Theorem 3** (Theorem 7.4, [58]).: _Take \(\Psi(\delta)\geq\mathcal{J}_{B}(\delta,\widetilde{\mathcal{P}}_{k}^{1/2}( \Theta,\delta))\) that satisfies \(\Psi(\delta)/\delta^{2}\) is a non-increasing function of \(\delta\). Then, for some universal constant \(c\) and for some sequence \((\delta_{n})\) such that \(\sqrt{n}\delta_{n}^{2}\geq c\Psi(\delta_{n})\), we achieve that_

\[\mathbb{P}\Big{(}\mathbb{E}_{X}[h(g_{\widetilde{G}_{n}}(\cdot|X),g_{G_{*}}( \cdot|X))]>\delta\Big{)}\leq c\exp\left(-\frac{n\delta^{2}}{c^{2}}\right),\]

_for all \(\delta\geq\delta_{n}\)._

The proof of this theorem can be seen in [58].

Proof of Proposition 2.: Back to our main proof, since

\[H_{B}(t,\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta,t),\|\cdot\|)\leq H_{B}(t, \mathcal{P}_{k}(\Theta,t),h)\]for any \(t>0\), it follows from equation (33) that

\[\mathcal{J}_{B}(\delta,\widetilde{\mathcal{P}}_{k}^{1/2}(\Theta,\delta))\leq\int_ {\delta^{2}/2^{13}}^{\delta}H_{B}^{1/2}(t,\mathcal{P}_{k}(\Theta,t),h)\;\mathrm{ d}t\vee\delta\lesssim\int_{\delta^{2}/2^{13}}^{\delta}\log(1/t)dt\vee\delta,\]

where we apply the upper bound of a bracketing entropy in Lemma 3 (cf. the end of this proof) in the second inequality. Let \(\Psi(\delta)=\delta\cdot[\log(1/\delta)]^{1/2}\), we have \(\Psi(\delta)/\delta^{2}\) is a non-increasing function of \(\theta\). Moreover, the above equation deduces that \(\Psi(\delta)\geq\mathcal{J}_{B}(\delta,\widetilde{\mathcal{P}}_{k}^{1/2}( \Theta,\delta))\). Additionally, let \(\delta_{n}=\sqrt{\log(n)/n}\), we have that \(\sqrt{n}\delta_{n}^{2}\geq c\Psi(\delta_{n})\) for some universal constant \(c\). As all the assumptions are met, Theorem 3 gives us

\[\mathbb{P}(\mathbb{E}_{X}[h(g_{\widehat{G}_{n}}(\cdot|X),g_{G_{*}}(\cdot|X))] >C(\log(n)/n)^{1/2})\lesssim\exp(-c\log(n)),\]

for some universal constant \(C\) that depends only on \(\Theta\). 

For completion, we will provide the result regarding the upper bound of a bracketing entropy in the following lemma:

**Lemma 3**.: _Assume that \(\Theta\) is a bounded set, then the following inequality holds true for any \(0\leq\varepsilon\leq 1/2\):_

\[H_{B}(\varepsilon,\mathcal{P}_{k}(\Theta),h)\lesssim\log(1/\varepsilon).\]

Proof of Lemma 3.: Firstly, we will establish an upper bound for the univariate Gaussian density \(f(Y|a^{\top}X+b,\sigma)\). Since both \(\mathcal{X}\) and \(\Theta\) are bounded sets, there exist positive constants \(\kappa,u,\ell\) such that \(-\kappa\leq a^{\top}X+b\leq\kappa\) and \(\ell\leq\sigma\leq u\). As a result,

\[f(Y|a^{\top}X+b,\sigma)=\frac{1}{\sqrt{2\pi h_{2}}}\exp\Big{(}-\frac{(Y-h_{1}) ^{2}}{2h_{2}}\Big{)}\leq\frac{1}{\sqrt{2\pi\ell}}.\]

For any \(|Y|\geq 2\kappa\), we have that \(\frac{(Y-h_{1})^{2}}{2h_{2}}\geq\frac{Y^{2}}{8u}\), which leads to

\[f(Y|a^{\top}X+b,\sigma)\leq\frac{1}{\sqrt{2\pi\ell}}\exp\Big{(}-\frac{Y^{2}}{8 u}\Big{)}.\]

Putting the above results together, we obtain that \(f(Y|a^{\top}X+b,\sigma)\leq K(Y|X)\), where we define \(K(Y|X):=\frac{1}{\sqrt{2\pi\ell}}\exp\Big{(}-\frac{Y^{2}}{8u}\Big{)}\) if \(|Y|\geq 2\kappa\), and \(K(Y|X):=\frac{1}{\sqrt{2\pi\ell}}\) otherwise.

Subsequently, let \(\eta\leq\varepsilon\), we assume that the set \(\mathcal{P}_{k}(\Theta)\) has an \(\eta\)-cover (under \(\ell_{1}\)-norm) denoted by \(\{\pi_{1},\ldots,\pi_{N}\}\), where \(N:=N(\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\) is known as the \(\eta\)-covering number of \(\mathcal{P}_{k}(\Theta)\). Then, we will build up the brackets of the form \([\nu_{i}(Y|X),\mu_{i}(Y|X)]\) for all \(i\in[N]\) as follows:

\[\nu_{i}(Y|X) :=\max\{\pi_{i}(Y|X)-\eta,0\},\] \[\mu_{i}(Y|X) :=\max\{\pi_{i}(Y|X)+\eta,K(Y|X)\}.\]

Consequently, it can be checked that \(\mathcal{P}_{k}(\Theta)\subset\bigcup_{i=1}^{N}[\nu_{i}(Y|X),\mu_{i}(Y|X)]\) with a note that \(\mu_{i}(Y|X)-\nu_{i}(Y|X)\leq\min\{2\eta,K(Y|X)\}\). Next, for each \(i\in[N]\), we attempt to give an upper bound for

\[\|\mu_{i}-\nu_{i}\|_{1} =\int_{|Y|<2\kappa}(\mu_{i}(Y|X)-\nu_{i}(Y|X))\;\mathrm{d}(X,Y)+ \int_{|Y|\geq 2\kappa}(\mu_{i}(Y|X)-\nu_{i}(Y|X))\;\mathrm{d}(X,Y)\] \[\leq R\eta+\exp\Big{(}-\frac{R^{2}}{2u}\Big{)}\leq R^{\prime}\eta,\]

where \(R:=\max\{2\kappa,\sqrt{8u}\}\log(1/\eta)\) and \(R^{\prime}\) is some positive constant. By definition of the bracketing entropy, since \(H_{B}(R^{\prime}\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\) is the logarithm of the smallest number of brackets of size \(R^{\prime}\eta\) necessary to cover \(\mathcal{P}_{k}(\Theta)\), we achieve that

\[H_{B}(R^{\prime}\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\leq\log N=\log N( \eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1}).\]

Assume that the following upper bound for the covering number \(\log N(\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\lesssim\log(1/\eta)\) holds true (proof is provided below), then the above result leads to

\[H_{B}(R^{\prime}\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\lesssim\log(1/\eta).\]By selecting \(\eta=\varepsilon/R^{\prime}\), we receive that \(H_{B}(\varepsilon,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\lesssim\log(1/\varepsilon)\). Furthermore, since the Hellinger distance is upper bounded by the \(L^{1}\)-norm, we reach the desired conclusion:

\[H_{B}(\varepsilon,\mathcal{P}_{k}(\Theta),h)\lesssim\log(1/\varepsilon).\]

**Upper bound of the covering number.** For completion, we will establish the following upper bound for the covering number, i.e.,

\[\log N(\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\lesssim\log(1/\eta).\]

Let us denote \(\Delta:=\{(\beta_{0},\beta_{1})\in\mathbb{R}\times\mathbb{R}^{d}:(\beta_{0}, \beta_{1},a,b,\sigma)\in\Theta\}\) and \(\Omega:=\{(a,b,\sigma)\in\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{R}_{+}:( \beta_{0},\beta_{1},a,b,\sigma)\in\Theta\}\). Since \(\Theta\) is a compact set, \(\Delta\) and \(\Omega\) are also compact. Thus, we can find \(\eta\)-covers \(\Delta_{\eta}\) and \(\Omega_{\eta}\) for \(\Delta\) and \(\Omega\), respectively. It can be verified that \(|\Delta_{\eta}|\leq\mathcal{O}(\eta^{-(d+1)k})\) and \(|\Omega_{\eta}|\lesssim\mathcal{O}(\eta^{-(d+2)k})\).

For each mixing measure \(G=\sum_{i=1}^{k}\exp(\beta_{0i})\delta_{(\beta_{1i},a_{i},b_{i},\sigma_{i})} \in\mathcal{O}_{k}(\Theta)\), we consider another one denoted by \(\widetilde{G}:=\sum_{i=1}^{k}\exp(\beta_{0i})\delta_{(\beta_{1i},\overline{a }_{i},\overline{b}_{i},\overline{\sigma}_{i})}\), where \((\overline{a}_{i},\overline{b}_{i},\overline{\sigma}_{i})\in\Omega_{\eta}\) such that \((\overline{a}_{i},\overline{b}_{i},\overline{\sigma}_{i})\) are the closest to \((a_{i},b_{i},\sigma_{i})\) in that set for all \(i\in[k]\). In addition, we also take into account the following mixing measure \(\overline{G}:=\sum_{i=1}^{k}\exp(\overline{\beta}_{0i})\delta_{(\overline{ \beta}_{1i},\overline{a}_{i},\overline{b}_{i},\overline{\sigma}_{i})}\), where \((\overline{\beta}_{0i},\overline{\beta}_{1i})\in\Delta_{\eta}\) are the closest to \((\beta_{0i},\beta_{1i})\) in that set. We can verify that the conditional density \(g_{\widetilde{G}}\) belongs to the following set:

\[\mathcal{R}:=\left\{g_{G}\in\mathcal{P}_{k}(\Theta):(\beta_{0i},\beta_{1i}) \in\Delta_{\eta},\;(a_{i},b_{i},\sigma_{i})\in\Omega_{\eta},\;\forall i\in[k ]\right\}.\]

Let us denote \(\mathrm{Softmax}(\beta_{1i}^{\top}X+\beta_{0i}):=\frac{\exp(\beta_{1i}^{\top}X +\beta_{0i})}{\sum_{j=1}^{k}\exp(\beta_{1j}^{\top}X+\beta_{0j})}\). From the formulation of \(\widetilde{G}\), we get the following bounds:

\[\|g_{G}-g_{\widetilde{G}}\|_{1} \leq\sum_{i=1}^{k}\int_{\mathcal{X}}\mathrm{Softmax}(\beta_{1i}^{ \top}X+\beta_{0i})\Big{|}f(Y|(a_{i})^{\top}X+b_{i},\sigma_{i})-f(Y|(\overline{ a}_{i})^{\top}X+\overline{b}_{i},\overline{\sigma}_{i})\Big{|}\mathrm{d}X\] \[\leq\sum_{i=1}^{k}\int_{\mathcal{X}}\Big{|}f(Y|(a_{i})^{\top}X+b_ {i},\sigma_{i})-f(Y|(\overline{a}_{i})^{\top}X+\overline{b}_{i},\overline{ \sigma}_{i})\Big{|}\mathrm{d}X\] \[\lesssim\sum_{i=1}^{k}(\|a_{i}-\overline{a}_{i}\|+|b_{i}-\overline {b}_{i}|+|\sigma_{i}-\overline{\sigma}_{i}|)\] \[\lesssim\eta,\] (34)

where the second inequality follows from the facts that \(\mathcal{X}\) is a bounded set. Note that \(\mathrm{Softmax}\) is a Lipschitz function with Lipschitz constant \(L\geq 0\). Additionally, since \(\mathcal{X}\) is a bounded set, there exists a constant \(B>0\) such that \(\|X\|\leq B\) for any \(X\in\mathcal{X}\). As a result, we get

\[\|g_{\widetilde{G}}-g_{\widetilde{G}}\|_{1} \leq\sum_{i=1}^{k}\int_{\mathcal{X}}\Big{|}\mathrm{Softmax}(\beta_ {1i}^{\top}X+\beta_{0i})-\mathrm{Softmax}(\overline{\beta}_{1i}^{\top}X+ \overline{\beta}_{0i})\Big{|}f(Y|(\overline{a}_{i})^{\top}X+\overline{b}_{i}, \overline{\sigma}_{i})\mathrm{d}X\] \[\lesssim L\sum_{i=1}^{k}\int_{\mathcal{X}}\Big{(}\|\beta_{1i}- \overline{\beta}_{1i}\|\cdot\|X\|+|\beta_{0i}-\overline{\beta}_{0i}|\Big{)} \mathrm{d}X\] \[\leq Lk\eta(B+1),\] (35)

where the second inequality follows from the fact that \(\mathrm{Softmax}\) is a Lipschitz function and the Gaussian density \(f(Y|(\overline{a}_{i})^{\top}X+\overline{b}_{i},\overline{\sigma}_{i})\) is bounded. Putting the bounds in equations (34) and (35) together with the triangle inequality, we receive that

\[\|g_{G}-g_{\overline{G}}\|_{1}\leq\|g_{G}-g_{\widetilde{G}}\|_{1}+\|g_{ \widetilde{G}}-g_{\widetilde{G}}\|_{1}\lesssim\eta,\]

which means that \(\mathcal{R}\) is an \(\eta\)-cover (not necessarily smallest) of the metric space \((\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\). By definition of the covering number, we know that

\[N(\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{1})\leq|\mathcal{R}|=|\Delta_{\eta}| \times|\Omega_{\eta}|\leq\mathcal{O}(\eta^{-(d+1)k})\cdot\mathcal{O}(\eta^{(-d +2)k})\leq\mathcal{O}(\eta^{-(2d+3)k}),\]

which implies that,

\[\log N(\eta,\mathcal{P}_{k}(\Theta),\|\cdot\|_{\infty})\lesssim\log(1/\eta).\]

Hence, the proof is completed.

### Proof of Lemma 1

First of all, let us recall the system of polynomial equations of interest here:

\[\sum_{j=1}^{m}\sum_{\alpha\in\mathcal{I}_{\ell_{1},\ell_{2}}}\frac{p_{5j}^{2}\;p _{1j}^{\alpha_{1}}\;p_{2j}^{\alpha_{2}}\;p_{3j}^{\alpha_{3}}\;p_{4j}^{\alpha_{4 }}}{\alpha_{1}!\;\alpha_{2}!\;\alpha_{3}!\;\alpha_{4}!}=0,\] (36)

where \(\mathcal{I}_{\ell_{1},\ell_{2}}=\{\alpha=(\alpha_{1},\alpha_{2},\alpha_{3}, \alpha_{4})\in\mathbb{N}^{d}\times\mathbb{N}^{d}\times\mathbb{N}\times\mathbb{ N}:\;\alpha_{1}+\alpha_{2}=\ell_{1},\;\alpha_{3}+2\alpha_{4}=\ell_{2}-| \alpha_{2}|\}\) for any \((\ell_{1},\ell_{2})\in\mathbb{N}^{d}\times\mathbb{N}\) such that \(0\leq|\ell_{1}|\leq r\), \(0\leq\ell_{2}\leq r-|\ell_{1}|\) and \(|\ell_{1}|+\ell_{2}\geq 1\).

In this proof, we denote \(p_{1j}=(p_{1j1},p_{1j2},\ldots,p_{1jd})\) and \(p_{2j}=(p_{2j1},p_{2j2},\ldots,p_{2jd})\).

**When \(m=2\):** By observing a portion of the above system when \(\ell_{1}=\mathbf{0}_{d}\), which is given by

\[\sum_{j=1}^{m}\sum_{\alpha_{3}+2\alpha_{4}=\ell_{2}}\frac{p_{5j}^{2}\;p_{3j}^ {\alpha_{3}}\;p_{4j}^{\alpha_{4}}}{\alpha_{3}!\;\alpha_{4}!}=0,\quad\ell_{2}=1,2,\ldots,r.\] (37)

It follows from Proposition 2.1 in [28] that the smallest natural number \(r\) such that the system (37) does not have any non-trivial solutions when \(m=2\) is \(r=4\). It is worth noting that a solution of the system 37 is considered non-trivial in [28] if all the values of \(p_{5j}\) are different from zero, whereas at least one among \(p_{3j}\) is non-zero, which aligns with our definition of non-trivial solutions for the system (36). Thus, we get \(\bar{r}(m)\leq 4\), and it suffices to demonstrate that \(\bar{r}(m)>3\). Indeed, when \(r=3\), the system in equation (36) can be written as follows:

\[\sum_{j=1}^{m}p_{5j}^{2}p_{1jl}=0\;\forall l\in[d],\quad\sum_{j=1 }^{m}p_{5j}^{2}p_{3j}=0,\quad\sum_{j=1}^{m}p_{5j}^{2}(p_{2ju}+p_{1jv}p_{3j})=0 \;\forall u,v\in[d],\] \[\sum_{j=1}^{m}p_{5j}^{2}p_{1ju}p_{1jv}=0\;\forall u,v\in[d],\quad \sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{2}p_{3j}^{2}+p_{4j}\Big{)}=0,\quad \sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{3!}p_{3j}^{3}+p_{3j}p_{4j}\Big{)}=0,\] \[\sum_{j=1}^{m}p_{5j}^{2}p_{1ju}p_{1jv}p_{1jl}=0\;\forall u,v,l\in [d],\quad\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{2}p_{1ju}p_{1jv}p_{3j}+p_{1 jl}p_{2j\tau}\Big{)}=0\;\forall u,v,l,\tau\in[d],\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{2}p_{1ju}\cdot p_{3j}^{2} +p_{1jv}p_{4j}+p_{2jl}p_{3j}\Big{)}=0\;\forall u,v,l,\tau\in[d].\] (38)

It can be seen that the following is a non-trivial solution of the above system: \(p_{5j}=1\), \(p_{1j}=p_{2j}=\mathbf{0}_{d}\) for all \(j\in[m]\), \(p_{31}=1\), \(p_{32}=-1\), \(p_{41}=p_{42}=-\frac{1}{2}\). Therefore, we obtain that \(\bar{r}(m)>3\), which leads to \(\bar{r}(m)=4\).

**When \(m=3\):** Note that \(\bar{r}(m)\) is a monotonically increasing function of \(m\). Therefore, it follows from the previous result that \(\bar{r}(m)>\bar{r}(2)=4\), or equivalently, \(\bar{r}(m)\geq 5\). Additionally, according to Proposition 2.1 in [28], we deduce that \(\bar{r}(m)\leq 6\) based on the reduced system in equation (37). Thus, we only need to show that \(\bar{r}(m)>5\). Indeed, the system (36) when \(r=5\) is a combination ofthe system in equation (38) and the following system:

\[\sum_{j=1}^{m}p_{5j}^{2}p_{1ju}p_{1jv}p_{1jl}p_{1j\tau}=0\;\forall u,v,l,\tau\in[d],\quad\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{4!}p_{3j}^{4}+\frac{ 1}{2!}p_{3j}^{2}p_{4j}+\frac{1}{2!}p_{4j}^{2}\Big{)}=0,\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{3!}p_{1ju}p_{3j}^{3}+p_{1 jv}p_{3j}p_{4j}+\frac{1}{2!}p_{2jl}p_{3j}^{2}+p_{2j\tau}p_{4j}\Big{)}=0\; \forall u,v,l,\tau\in[d],\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{3!}p_{1ju_{1}}p_{1ju_{2}} p_{1ju_{3}}p_{3j}+\frac{1}{2!}p_{1jv_{1}}p_{1jv_{2}}p_{2jv_{3}}\Big{)}=0\; \forall(u_{i})_{i=1}^{3},(v_{i})_{i=1}^{3}\in[d]^{3},\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{2!2!}p_{1ju_{1}}p_{1ju_{2} }p_{3j}^{2}+\frac{1}{2!}p_{1ju_{3}}p_{1ju_{4}}p_{4j}+p_{1ju_{5}}p_{1ju_{6}}p_{ 3j}\Big{)}=0\;\forall(u_{i})_{i=1}^{6}\in[d]^{6},\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{4!}p_{1ju_{1}}p_{3j}^{4}+ \frac{1}{2!}p_{1ju_{2}}p_{3j}^{2}p_{4j}+\frac{1}{2!}p_{1ju_{3}}p_{4j}^{2}+ \frac{1}{3!}p_{2ju_{4}}p_{3j}^{3}+p_{2ju_{5}}p_{3j}p_{4j}\Big{)}=0\] \[\forall(u_{i})_{i=1}^{5}\in[d]^{5},\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{4!}\prod_{i=1}^{4}p_{1ju_ {i}}p_{3j}+\frac{1}{3!}\prod_{i=5}^{7}p_{1ju_{i}}p_{2ju_{5}}\Big{)}=0\;\forall (u_{i})_{i=1}^{8}\in[d]^{8},\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{2!3!}\prod_{i=1}^{2}p_{1 ju_{i}}p_{3j}^{3}+\frac{1}{2!}\prod_{i=3}^{4}p_{1ju_{i}}p_{3j}p_{4j}+p_{1ju_{5}} p_{2ju_{6}}(\frac{1}{2}p_{3j}^{2}+p_{4j})+\frac{1}{2!}\prod_{i=7}^{8}p_{2ju_{i}}p_{ 3j}\Big{)}=0\] \[\forall(u_{i})_{i=1}^{8}\in[d]^{8},\] \[\sum_{j=1}^{m}p_{5j}^{2}\Big{(}\frac{1}{3!2!}\prod_{i=1}^{3}p_{1 ju_{i}}p_{3j}^{2}+\frac{1}{3!}\prod_{i=4}^{6}p_{1ju_{i}}p_{4j}+\frac{1}{2!}p_{1 jju_{i}}p_{2ju_{5}}p_{3j}+\frac{1}{2!}p_{1ju_{6}}\prod_{i=10}^{11}p_{2ju_{i}} \Big{)}=0\] \[\forall(u_{i})_{i=1}^{11}\in[d]^{11}.\]

We can verify that the following is a non-trivial solution of this system:

\[p_{5j}=1,\quad p_{1j}=p_{2j}=\bm{0}_{d},\quad\forall j\in[m],\] \[p_{31}=\frac{\sqrt{3}}{3},\quad p_{32}=-\frac{\sqrt{3}}{3},\quad p _{33}=0,\] \[p_{41}=p_{42}=-\frac{1}{6},\quad p_{43}=0.\]

Hence, we conclude that \(\bar{r}(m)=6\).

## Appendix C Experiments

In this appendix, we conduct a simulation study to empirically validate our theoretical results on the convergence rates of maximum likelihood estimation (MLE) in the softmax gating Gaussian mixture of experts established in Theorem 1 and Theorem 2.

### Numerical Schemes

We illustrate the heterogeneity convergence rates of the MLE under the softmax gating Gaussian mixture of experts via exact-fitted and over-fitted models, which correspond to the exact-fittedand over-fitted settings described in Sections 3.1 and 3.2, respectively. For each case, we let \(X\) be uniformly distributed over \([0,1]\) and we generate observations \(Y\) from the conditional density \(g_{G_{*}}(Y|X)\) of softmax gating Gaussian mixture of experts model in equation (1). Here, the true mixing measure \(G_{*}=\sum_{i=1}^{k_{*}}\exp(\beta_{0i}^{*})\delta_{(\beta_{1i}^{*},a_{1}^{*},b_ {i}^{*},\sigma_{i}^{*})}\), where \(k_{*}=2\), is specified as follows:

\[\beta_{01}^{*}=-8,\quad\beta_{11}^{*}=25,\quad a_{1}^{*}=-20,\quad b _{1}^{*}=15,\quad\sigma_{1}^{*}=0.3,\] \[\beta_{02}^{*}=0,\quad\beta_{12}^{*}=0,\quad a_{2}^{*}=20,\quad b _{2}^{*}=-5,\quad\sigma_{2}^{*}=0.4.\]

Then, we compute the MLE \(\widehat{G}_{n}\) w.r.t. a number of components \(k\) for each sample. For both of these settings, we choose \(k\in\{k_{*},k_{*}+1,k_{*}+2\}\). In order to perform the MLE, we use a numerical scheme based on the EM algorithm [12] similar to the one used by Chamroukhi et al. [7, 8]. Note that the main difference with a classical EM is in the maximization step, as there are no closed formulas for updating the softmax gating parameters \((\beta_{0i},\beta_{1i}),\forall i\in[k]\). For this purpose, following the results of Chamroukhi et al. [7, 8], see also [38, 10, 23], we use a multi-class iterative reweighted least-squares algorithm. All code for our simulation study below was written in Python 3.9.13 on a standard Unix machine.

We choose the convergence criteria \(\epsilon=10^{-6}\) and \(2000\) maximum EM iterations. Our goal is to illustrate the theoretical properties of the estimator \(\widehat{G}_{n}\). Therefore, we have initialized the EM algorithm in a favourable way. More specifically, we first randomly partitioned the set \(\{1,\dots,k\}\) into \(k_{*}\) index sets \(J_{1},\dots,J_{k_{*}}\), each containing at least one point, for any given \(k\) and \(k_{*}\) and for each replication. Finally, we sampled \(\beta_{1j}^{*}\) (resp. \(a_{j}^{*},b_{j}^{*},\sigma_{j}^{*}\)) from a unique Gaussian distribution centered on \(\beta_{1t}^{*}\) (resp. \(a_{t}^{*},b_{t}^{*},\sigma_{t}^{*}\)), with vanishing covariance so that \(j\in J_{t}\).

### Empirical Convergence Rates

The empirical mean of discrepancies \(\mathcal{D}_{1}\) and \(\mathcal{D}_{2}\) between \(\widehat{G}_{n}\) and \(G_{*}\), and the choice of \(k\) for exact-fitted and over-fitted models are reported in Figures 1 and 2, respectively. It can be observed that the average discrepancies from \(\widehat{G}_{n}\) to \(G_{0}\) vanish at a rate of \(\mathcal{O}(n^{-1/2})\) up to a logarithmic factor, as envisaged by Theorems 1 and 2. Although these empirical rates of convergence are similar for the two models, they imply that the convergence behaviour of the individual fitted parameters is very different in an over-fitted setting, which was already discussed in more detail in the Section 3.2.

#### c.2.1 Exact-fitted Model

We generate \(40\) samples of size \(n\) for each setting, given \(200\) different choices of sample size \(n\) between \(10^{2}\) and \(10^{5}\). The empirical parametric convergence rate of the MLE to \(G_{*}\) under the metric \(\mathcal{D}_{1}\) in Figure 1 is consistent with the theoretical rates of estimating the true parameters \(\exp(\beta_{0i}^{*}),\beta_{1i}^{*}\) (up to translation), \(a_{i}^{*}\), \(b_{i}^{*}\), \(\sigma_{i}^{*}\) for \(i\in[k_{*}]\), which are of order \(\mathcal{O}(n^{-1/2})\) up to logarithmic factors.

#### c.2.2 Over-fitted Model

In the over-fitted setting, we generate \(40\) samples of size \(n\) for each setting, given \(200\) different choices of sample size \(n\) between \(n_{\min}\approx 14*10^{3}\) for \(k=k_{*}+1\), \(n_{\min}\approx 27*10^{3}\) for \(k=k_{*}+2\) and \(n_{\max}=10^{5}\). To the best of our knowledge, there is still a lack of theoretical understanding of EM performance, in particular an established algorithm that enjoys global convergence for the parameter estimation of the over-fitted softmax gating Gaussian mixture of experts. The most related theoretical results are only for the mixture of expert with covariate-free gating networks in [41, 40, 42]. This explains why in Figure 2 for over-fitted setting, we have not plotted the error bar due to the instability of the EM algorithm for finding the global solution. Moreover, the sample size must be large enough so that the empirical behaviour of the MLE from the EM algorithm matches the theoretical rate of order \(\mathcal{O}(n^{-1/2})\) up to a logarithmic term.

Figure 1: Log-log scaled plots of the simulation results for exact-fitted setting. We compute the estimator \(\widehat{G}_{n}\) on \(40\) independent samples of size \(n\) between \(10^{2}\) and \(10^{5}\). We plot its mean discrepancy from the true mixing measure in red, with error bars representing two empirical standard deviations. We also plot the least-squares fitted linear regression line of these points in a black dash-dotted line.

Figure 2: Log-log scaled plots of the empirical mean of the discrepancy \(\mathcal{D}_{2}\) between \(\widehat{G}_{n}\) and \(G_{*}\) (red curves) and least-squares fitted linear regression (black dash-dotted lines) are shown using 40 independent sample sizes \(n\) between \(n_{\min}\) and \(10^{5}\).