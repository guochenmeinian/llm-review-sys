# Long-Term Fairness with Unknown Dynamics

 Tongxin Yin

Electrical and Computer Engineering

University of Michigan

Ann Arbor, MI 48109

tyin@umich.edu

&Reilly Raab

Computer Science and Engineering

University of California, Santa Cruz

Santa Cruz, CA 95064

reilly@ucsc.edu

&Mingyan Liu

Electrical and Computer Engineering

University of Michigan

Ann Arbor, MI 48109

mingyan@umich.edu

&Yang Liu

University of California, Santa Cruz

ByteDance Research

Santa Cruz, CA 95064

yangliu@ucsc.edu

These authors contributed equally to this work.

###### Abstract

While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness as an online reinforcement learning problem for a policy affecting human populations. This formulation accommodates dynamical control objectives, such as achieving equitable population _states_, that cannot be incorporated into static formulations of fairness. We demonstrate that algorithmic solutions to the proposed fairness problem can adapt to unknown dynamics and, by sacrificing short-term incentives, drive the policy-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning and prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness. In the classification setting subject to group fairness, we compare our proposed algorithm to several baselines, including the repeated retraining of myopic or distributionally robust classifiers, and to a deep reinforcement learning algorithm that lacks fairness guarantees. Our experiments model human populations according to evolutionary game theory and integrate real-world datasets.

## 1 Introduction

As machine learning (ML) algorithms are deployed for tasks with real-world social consequences (e.g., school admissions, loan approval, medical interventions, etc.), the possibility exists for runaway social inequalities (Crawford and Calo, 2016; Chaney et al., 2018; Fuster et al., 2018; Ensign et al., 2018). While "fairness" has become a salient ethical concern in contemporary research, the closed-loop dynamics of real-world systems with feedback, i.e., comprising ML-driven decisions and populations that mutually adapt to each other (Fig. 5 in the supplementary material) remain poorly understood. We assert that realistic scenarios are often described by fundamentally **unknown dynamics**: Even with models of human behavior based on rational behavior or evolutionary game theory, utilities and risk-preferences are generally unknown or uncertain. For this reason, we advocate for dynamics-agnostic formulations of fairness, for which reinforcement learning is a natural fit.

In this paper, our **primary contribution** is to consider the problem of _long-term fairness_, or algorithmic fairness in the context of a dynamically responsive population, as a reinforcement learning (RL)problem subject to constraint. In our formulation, the central learning task is to develop a policy that minimizes cumulative loss (e.g., financial risk, negative educational outcomes, misdiagnoses, etc.) incurred by an ML agent interacting with a human population up to a finite time horizon, subject to constraints on cumulative "violations of fairness", which we refer to in a single time step as _disparity_ and cumulatively as _distortion_.

Our central hypothesis is that an RL formulation of long-term fairness may allow an agent to learn to **sacrificer short-term utility in order to drive the system towards more desirable equilibria**. The core practical difficulties posed by our general problem formulation, however, are the potentially unknown dynamics of the system under control, which must be determined by the RL agent _online_ (i.e., during actual deployment), and the general non-convexity of the losses or constraints considered. Additionally, our problem setting generally requires continuous state and action spaces for the RL agent, which preclude familiar methods that yield performance guarantees in discrete settings.

Our **secondary contributions** are 1) to show that long-term fairness can be solved within asymptotic, probabilistic bounds under certain dynamical assumptions and 2) to demonstrate that the problem can also be addressed more flexibly with existing RL methods. For theoretical guarantees, we develop L-UCBFair, an online RL method, and prove sublinear bounds on regret (suboptimiality of cumulative loss) and distortion (suboptimality of cumulative disparity) with high probability (Section3.1). To demonstrate practical solutions without strong safety guarantees, we apply R-TD3, an adaptation of a well-known deep reinforcement learning method (viz., TD3) to a time-dependent Lagrangian relaxation of the central problem. We compare L-UCBFair and R-TD3 to several baselines (Section3.3), including myopic policies, in interaction with simulated populations initialized with synthetic or real-world data and updated according to evolutionary game theory (Section4).

Finally, this paper considers fairness in terms of statistical regularities across (ideally) socioculturally meaningful _groups_. While internal conflict exists between different statistical measures of fairness (Corbett-Davies and Goel, 2018), we show that an RL approach to long-term fairness can mitigate trade-offs between immediately-fair policy decision _outcomes_ (e.g., acceptance rate disparities (Dwork et al., 2012; Zemel et al., 2013; Feldman et al., 2015)) and causally-fair population states (e.g., qualification rate (Raab and Liu, 2021; Zhang et al., 2020)). In particular, we show that our proposed solutions can learn to avoid the familiar trap of pursuing short-term fairness metrics only to widen underlying disparities that demand escalating interventions at the expense of utility (Section5.2).

### Related Work

Our formulation of long-term fairness bridges recent work on "fairness in machine learning", which has developed in response to the proliferation of data-driven methods in society, and "safe reinforcement learning", which seeks theoretical safety guarantees in the control of uncertain dynamical systems.

**Dynamics of Fairness in Machine Learning** We distinguish long-term fairness from the dynamics of fair allocation problems (Joseph et al., 2016; Jabbari et al., 2017; Tang et al., 2021; Liu et al., 2017) and emphasize side-effects of algorithmic decisions affecting future decision problems. By formalizing long-term fairness in terms of cumulative losses and disparities, we iterate on a developing research trend that accounts for the dynamical response of a human population to deployed algorithmic prediction: both as a singular reaction (Liu et al., 2018; Hu et al., 2019; Hu and Zhang, 2022) or as a sequence of mutual updates between the population and the algorithm (Coate and Loury, 1993; D'Amour et al., 2020; Zhang et al., 2020; Heidari et al., 2019; Wen et al., 2019; Liu et al., 2020; Hu and Chen, 2018; Mouzannar et al., 2019; Williams and Kolter, 2019; Raab and Liu, 2021).

Several prior works have considered the long-term fairness implications in the context of performative stability or optimality (Perdomo et al., 2020) with known, stateless dynamical transitions: Hu and Zhang (2022) characterize the convergence of systems typified by sequential, myopic policies while Hashimoto et al. (2018) contrast myopic policy with a modified objective satisfying distributional robustness. While Mouzannar et al. (2019) and Raab and Liu (2021) address stateful dynamical transitions, the cited works only treat myopic classifiers that optimize immediate utility (subject to fairness constraints) rather than learning to anticipate dynamical population responses. Finally, while Wen et al. (2019) explore reinforcement learning for long-term fairness, the discussion is limited to a tabular explore-then-commit approach over discrete state and action spaces. This approach does not generalize to continuous spaces, where we provide separate and tighter bounds for both utility and disparity.

**Safe Reinforcement Learning** L-UCBFair furthers recent efforts in safe RL. While _model-based_ approaches, in which the algorithm learns an explicit dynamical model of the environment, constitute one thread of prior work (Efroni et al., 2020; Singh et al., 2020; Brantley et al., 2020; Zheng and Ratliff, 2020; Kalagarla et al., 2021; Liu et al., 2021; Ding et al., 2021), leading algorithms in this domain are characterized by significant time and space complexity. Among _model-free_ algorithms, the unknown dynamics of our setting preclude the use of a simulator that allows algorithms to search over arbitrary state-action pairs (Xu et al., 2021; Ding et al., 2020; Bai et al., 2022).

While Wei et al. (2022) introduce a model-free, simulator-free algorithm, the tabular approach they consider is only applicable to discrete state and action spaces, while our setting requires continuous state and action spaces. To tackle continuous _state_ space, Ding et al. (2021) and Ghosh et al. (2022) consider linear dynamics: Ding et al. (2021) develop a primal-dual algorithm with safe exploration, and Ghosh et al. (2022) use a softmax policy design. Both algorithms are based on the work of Jin et al. (2020), which proposed a least squares value iteration (LSVI) method, using an Upper Confidence Bound (UCB) (Auer et al., 2002) to estimate a state-action "\(Q\)" function. In addition to continuous state spaces, L-UCBFair also addresses continuous _action_ spaces. To our knowledge, L-UCBFair is the first model-free, simulator-free RL algorithm to provides theoretical safety guarantees for both discrete and **continuous state _and_ action spaces**. Moreover, L-UCBFair achieves bounds on regret and distortion as tight as any algorithm thus far on discrete action space (Ghosh et al., 2022).

## 2 Problem Formulation

We motivate our formulation of long-term fairness with a binary classification task, though the general formulation we propose is more widely applicable. Given this initial task, we sequentially incorporate _fairness constraints_, then _population dynamics_, and then _cumulative loss and disparity_, before fully formalizing the central problem of optimizing cumulative loss subject to expected dynamics and constraints on cumulative disparity.

For our initial binary classification task, suppose that a random individual, sampled i.i.d. from a fixed population, has _features_\(X\in\mathbf{R}^{d}\), a _label_\(Y\in\{-1,1\}\), and a demographic _group_\(G\in\mathcal{G}\) (where \(\mathcal{G}=[n]\) for \(n\geq 2\)). Denote the joint distribution of these variables in the population as \(s\coloneqq\Pr(X,Y,G)\) such that \(X,Y,G\sim s\). The task is to predict \(Y\) (as \(\hat{Y}\)) from \(X\) and \(G\) by choosing a classifier \(a\), where \(\hat{Y}\sim a(X,G)\), to minimize the empirical loss \(\mathscr{L}(s,a)\overset{e.g}{=}\mathrm{E}\left[L(Y,\hat{Y})\right],\) where \(L\) denotes some individual loss such as zero-one-loss. In general, we allow arbitrary, (unit-interval) bounded loss functions \(\mathscr{L}(s,a)\in[0,1]\) such that the **basic classification task** is \(\min_{a}\mathscr{L}(s,a).\)

The **standard, "fair" classification task** (_without_ a dynamically responsive population) incorporates constraints on the choice of classifier \(a\), such that the _disparity_\(\mathscr{D}\in[0,1]\) induced on distribution \(s\) by \(a\) is bounded by some value \(c\in[0,1]\). Formally, the task is \(\min_{a}\mathscr{L}(s,a)\) subject to \(\mathscr{D}(s,a)\leq c\). A standard measure of disparity \(\mathscr{D}\) is the violation of "demographic parity" (Dwork et al., 2012). For example, when \(\mathcal{G}=\{g_{1},g_{2}\}\), \(\mathscr{D}(s,a)\overset{e.g}{=}\left|\xi_{s,a}(g_{1})-\xi_{s,a}(g_{2})\right|^ {2}\), where \(\xi_{s,a}(g)\coloneqq\Pr(\hat{Y}{=}1\mid G{=}g).\)

In this paper, we also wish to consider measures of fairness inherent to the distribution \(s\) (e.g., parity of group _qualification_ rates \(\Pr(Y{=}1\mid G{=}g)\)). Such measures of fairness can only be affected by changes to \(s\) and thus require dynamics, which are not captured by the above formulation (Raab and Liu, 2021; Zhang et al., 2020). We will see that such notions of disparity are well-suited to an RL formulation of long-term fairness.

State, action, and policyAdopting the semantics of _sequence_ of dependent classification tasks, we identify the time-dependent distribution \(s_{\tau}\in\mathcal{S}\) of individuals in the population as a _state_ and the chosen classifier \(a_{\tau}\in\mathcal{A}\) as an _action_ of some algorithmic _agent_ at time \(\tau\). While state space \(\mathcal{S}\) is arbitrary, we assume that action space \(\mathcal{A}\) admits a Euclidean metric, under which it is closed (i.e., \(\mathcal{A}\) is isomorphic to \([0,1]^{m},m\in\mathbf{Z}_{>0}\)). We specify that \(a_{\tau}\) is sampled stochastically according to the agent's current _policy_\(\pi_{\tau}\), that is, \(a_{\tau}\sim\pi_{\tau}(s_{\tau}).\) Additionally, we assume \(s_{\tau}\) is fully observable at time \(\tau\in\{1,2,...\}\). In practice, \(s_{\tau}\) must be approximated from finitely many empirical samples, though this caveat introduces well-understood errors that vanish in the limit of infinitely many samples.

DynamicsMoving beyond the one-shot "fair" classification task above, let us now assume that a population may react to classification, inducing the distribution \(s\) to change. In practice, such "distribution shift" is a well-known, real-world phenomenon that can increase realized loss and disparity when classifiers are fixed (Chen et al., 2022). For classifiers that free to update in response to a mutating distribution \(s\), subsequent classification tasks depend on the (stochastic) predictions made in previous tasks. In our formulation, we assume the existence of dynamical kernel \(\mathbf{P}\) that maps a state \(s\) and action \(a\) at time \(\tau\) to a _distribution over_ possible states at time \(\tau+1\). That is, \(s_{\tau+1}\sim\mathbf{P}(s_{\tau},a_{\tau})\).

We stipulate that \(\mathbf{P}\) may be initially unknown, but we assume that it does not explicitly depend on time and may be reasonably approximated or learned "online". While real-world dynamics may depend on information other than the current distribution of classification-specific variables \(\Pr(X,Y,G)\) -- e.g., exogenous parameters, history, or additional variables of state -- we have identified the dynamical state \(s\) with the current, fully-observed distribution for simplicity and tractability.

Reward and utility, value and quality functionsBecause the standard RL convention is to _maximize reward_ rather than _minimize loss_, for an RL agent, we define the instantaneous reward \(r(s_{\tau},a_{\tau})\coloneqq 1-\mathscr{L}(s_{\tau},a_{\tau})\in[0,1]\) and a separate "utility" \(g(s_{\tau},a_{\tau})\coloneqq 1-\mathscr{D}(s_{\tau},a_{\tau})\in[0,1]\), where \(r\) and \(g\) do not explicitly depend on time \(\tau\). We next propose to optimize anticipated _cumulative_ reward subject to constraints on anticipated _cumulative_ utility. Let the index \(j\) represent either reward \(r\) or utility \(g\). We use the letter \(V\) (for "value") to denote the future expected accumulation of \(j\) over steps \([h,...,H]\) (without time-discounting) starting from state \(s\), using policy \(\pi\). Likewise, we denote the "quality" of an action \(a\) in initial state \(s\) with the letter \(Q\). For \(j\in\{r,g\}\), we define

\[V^{\pi}_{j,h}(s)\coloneqq\mathrm{E}\left[\sum_{\tau=h}^{H}j\big{(}s_{\tau},a_ {\tau}\big{)}|s_{h}=s\right]\quad;\quad Q^{\pi}_{j,h}(s,a)\coloneqq\mathrm{E} \left[\sum_{\tau=h}^{H}j\big{(}s_{\tau},a_{\tau}\big{)}\ \right]\,|\,s_{h}=s,a_{h}=a\right].\]

Note that \(V\) and \(Q\) marginalize over the stochasticity of state transitions \(s_{\tau+1}\sim\mathbf{P}(s_{\tau},a_{\tau})\) and the sampling of actions \(a_{\tau}\sim\pi_{\tau}(s_{\tau})\). By the boundedness of \(r\) and, \(g\), the values of \(V\) and \(Q\) belong to the interval \([0,H-h+1]\).

"Long-term fairness" via reinforcement learningThe central problem explored in this paper is

\[\max_{\pi}\quad V^{\pi}_{r,1}(s)\quad\text{subject to}\quad V^{\pi}_{g,1}(s) \geq\tilde{c}.\] (1)

We emphasize that this construction considers a finite time horizon of \(H\) steps, and we denote the optimal value of \(\pi\) as \(\pi^{\star}\). We first assume that a solution to the problem exists bounded within the interior of the feasible set (i.e., _strict primal feasibility_ or _Slater's condition_), as in Efroni et al. (2020), Ding et al. (2021), and Ghosh et al. (2022):

**Assumption 2.1** (Slater's Condition).: \(\exists\,\gamma>0,\,\bar{\pi}\), such that \(V^{\pi}_{g,1}\left(s\right)\geq\tilde{c}+\gamma\).

The Online SettingGiven initially unknown dynamics, we formulate long-term fairness for the _online_ setting (in which learning is only possible through actual "live" deployment of policy, rather than through simulation). As it is not possible to unconditionally guarantee constraint satisfaction in Eq. (1) over a finite number of online steps, we instead measure two types of _regret_: the suboptimality of a policy with respect to cumulative incurred loss, denoted \(\mathrm{Regret}\), and the suboptimality of a policy with respect to cumulative induced disparity, denoted denoted \(\mathrm{Drtn}\) for "distortion".

\[\mathrm{Regret}(\pi,s_{1})\coloneqq V^{\pi^{\star}}_{r,1}\left(s_{1}\right)-V ^{\pi}_{r,1}\left(s_{1}\right)\quad;\quad\mathrm{Drtn}(\pi,s_{1})\coloneqq \max\left[0,\tilde{c}-V^{\pi}_{g,1}\left(s_{1}\right)\right].\] (2)

## 3 Algorithms and Analysis

We show that it is possible to provide guarantees for long-term fairness in the online setting. To do this, we develop L-UCBFair, the first model-free, simulator-free algorithm to provide such guarantees with continuous state and action spaces. Specifically, we prove probabilistic, sublinear bounds for regret and distortion under a linear MDP assumption (Assumption 3.1) and properly chosen parameters (Appendix B.1, in the supplementary material). We defer proofs to the supplementary material.

### L-UcbFair

Episodic MDPL-UCBFair inherits from a family of algorithms that treat an episodic Markov decision process (MDP) Jin et al. (2020). Therefore, we first map the long-term fairness problem to the episodic MDP setting, which we denote as \(\text{MDP}(\mathcal{S},\mathcal{A},H,\mathbf{P},\mathscr{L},\mathscr{D})\). The algorithm runs for \(K\)_episodes_, each consisting of \(H\) time steps. At the beginning of each episode, which we index with \(k\), the agent commits to a sequence of policies \(\pi^{k}=(\pi_{1}^{k},\pi_{2}^{k},...,\pi_{H}^{k})\) for the next \(H\) steps. At each step \(h\) within an episode, an action \(a_{h}^{k}\in\mathcal{A}\) is sampled according to policy \(\pi_{h}^{k}\), then the state \(s_{h+1}^{k}\in\mathcal{S}\) is sampled according to the transition kernel \(\mathbf{P}(s_{h}^{k},a_{h}^{k})\). \(s_{1}^{k}\) is sampled arbitrarily for each episode.

Episodic RegretBecause L-UCBFair predetermines its policy for an entire episode, we amend our definitions of regret and distortion over all \(HK\) time steps by breaking them into a sum over \(K\) episodes of length \(H\):

\[\mathrm{Regret}(K)=\sum_{k=1}^{K}\left(V_{r,1}^{\pi^{*}}\left(s_{1}^{k}\right) -V_{r,1}^{\pi^{k}}\left(s_{1}^{k}\right)\right)\;\;\;;\;\;\mathrm{Drtn}(K)= \max\left[0,\sum_{k=1}^{K}\left(\tilde{c}-V_{g,1}^{\pi^{k}}\left(s_{1}^{k} \right)\right)\right].\] (3)

The LagrangianConsider the Lagrangian \(\mathcal{L}\) associated with Eq.1, with dual variable \(\nu\geq 0\):

\[\mathcal{L}(\pi,\nu):=V_{r,1}^{\pi}\left(s\right)+\nu\left(V_{g,1}^{\pi}\left( s\right)-\tilde{c}\right).\] (4)

L-UCBFair approximately solves the primal problem, \(\max_{\pi}\min_{\nu}\mathcal{L}(\pi,\nu)\), which is non-trivial, since the objective function is seldom concave in practical parameterizations of \(\pi\). Let \(\nu^{*}\) denote the optimal value of \(\nu\); L-UCBFair assumes bound \(\nu^{*}\leq\mathscr{V}\), given parameter \(\mathscr{V}\), as described in the supplementary material (Assumption B.1).

**Assumption 3.1** (Linear MDP).: We assume \(\text{MDP}(\mathcal{S},\mathcal{A},H,\mathbf{P},\mathscr{L},\mathscr{D})\) is a linear MDP with feature map \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\). That is, for any \(h\), there exist \(d\) signed measures \(\mu_{h}=\left\{\mu_{h}^{1},\ldots,\mu_{h}^{d}\right\}\) over \(\mathcal{S}\) and vectors \(\theta_{r,h},\theta_{g,h}\in\mathbb{R}^{d}\) such that, for any \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\),

\[\mathbb{P}_{h}\left(s^{\prime}\mid s,a\right)=\left\langle\phi(s,a),\mu_{h} \left(s^{\prime}\right)\right\rangle;\;\;\;\;r\left(s,a\right)=\left\langle \phi(s,a),\theta_{r,h}\right\rangle;\;\;\;g(s,a)=\left\langle\phi(s,a),\theta_ {g,h}\right\rangle.\]

Assumption 3.1 addresses the curse of dimensionality when state space \(\mathcal{S}\) is the space of distributions over \(X,Y,G\). This assumption is also used by Jin et al. (2020) and Ghosh et al. (2022), and a similar assumption is made by Ding et al. (2021). This assumption is well-justified in continuous time if we allow for infinite-dimensional Hilbert spaces Brunton et al. (2021), but in practice we require limited dimensionality \(d\) for the codomain of \(\phi\). In our experiments, we use a neural network to estimate a feature map \(\hat{\phi}\) offline which approximately satisfies the linear MDP assumption (Appendix D.1).

#### 3.1.1 Construction

L-UCBFair, or "LSVI-UCB for Fairness" (Algorithm1) is based on a Least-Squares Value Iteration with an optimistic Upper-Confidence Bound, as in LSVI-UCB Jin et al. (2020). For each \(H\)-step episode \(k\), L-UCBFair maintains estimates for \(Q_{r}^{k}\), \(Q_{g}^{k}\) and a time-indexed policy \(\pi^{k}\). In each episode, L-UCBFair updates \(Q_{r}^{k},Q_{g}^{k}\), interacts with the environment, and updates the dual variable \(\nu_{k}\), which is constant over episode \(k\).

LSVI-UCB Jin et al. (2020)The estimation of \(Q\) is challenging, as it is impossible to iterate over all \(s,a\) pairs when \(\mathcal{S}\) and \(\mathcal{A}\) are continuous and \(\mathbf{P}\) is unknown. LSVI parameterizes \(Q_{h}^{\star}(s,a)\) by the linear form \(\mathrm{w}_{h}^{\top}\phi(s,a)\), as used by Jin et al. (2020), and updates

\[\mathbf{w}_{h}\leftarrow\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}} \sum_{\tau=1}^{k-1}\biggl{[}r_{h}\left(s_{h}^{\tau},a_{h}^{\tau}\right)+\max_{a \in\mathcal{A}}Q_{h+1}\left(s_{h+1}^{\tau},a\right)-\mathbf{w}^{\top}\phi \left(s_{h}^{\tau},a_{h}^{\tau}\right)\biggr{]}^{2}+\varsigma\|\mathbf{w}\|^{2}.\]

In addition, a "bonus term" \(\beta\left(\phi^{\top}\Lambda_{h}^{-1}\phi\right)^{1/2}\) is added to the estimate of \(Q\) in Algorithm1 to encourage exploration.

Adaptive Search PolicyCompared to the works of Ding et al. (2021) and Ghosh et al. (2022), the major challenge we face for long-term fairness is a continuous action space \(\mathcal{A}\), which renders theindependent computation of \(Q_{x}^{k}\), \(Q_{q}^{k}\) for each action impossible. To handle this issue, we propose an adaptive search policy: Instead of drawing an action directly from a distribution over continuous values, \(\pi\) represents a distribution over finitely many \((M)\), Voronoi partitions of \(\mathcal{A}\). After sampling a region with a softmax scheme, the agent draws action \(a\) uniformly at random from it. In addition to defining a Voronoi partitioning of the action space for the adaptive search policy of L-UCBFair (in the supplementary material), we make the following smoothness assumption, which we use to bound the error introduced by this sampling method:

**Assumption 3.2** (Lipschitz).: There exists \(\rho>0\), such that \(\|\phi(s,a)-\phi(s,a^{\prime})\|_{2}\leq\rho\|a-a^{\prime}\|_{2}\).

**Dual Update** For L-UCBFair, the update method for estimating \(\nu\) in Eq.4 is also essential. Since \(V_{r,1}^{\pi}\left(s\right)\) and \(V_{g,1}^{\pi}\left(s\right)\) are unknown, we use \(V_{r,1}^{k}\left(s\right)\) and \(V_{g,1}^{k}\left(s\right)\) to estimate them. An estimate for \(\nu\) is iteratively updated by performing gradient ascent in the Lagrangian (Eq.4) with step-size \(\eta\), subject to the assumed upper bound \(\mathscr{V}\) for \(\nu\)(AssumptionB.1). A similar method is also used in Ding et al. (2021); Ghosh et al. (2022).

#### 3.1.2 Analysis

We next bound the regret and distortion, defined in Eq.3, realizable by L-UCBFair. We then compare L-UCBFair with existing algorithms for discrete action spaces and discuss the importance of the number of regions \(M\) and the maximum distance \(\epsilon_{I}\) from any action to the center of its corresponding Voronoi partition.

**Theorem 3.3** (Boundedness).: _With probability \(1-p\), there exists a constant \(b\) such that L-UCBFair (Algorithm1) achieves_

\[\mathrm{Regret}(K)\leq\big{(}b\zeta H^{2}\sqrt{d^{3}}+(\mathscr{V}+1)H\big{)} \sqrt{K}\quad;\quad\mathrm{Drtn}(K)\leq\frac{b\zeta(1+\mathscr{V})H^{2}\sqrt{ d^{3}}}{\mathscr{V}}\sqrt{K},\] (5)

_for parameter values \(\varsigma{=}1\)\(\epsilon_{I}\leq\frac{1}{2\rho(1+\mathscr{V})KH\sqrt{d}}\), \(\zeta=\log(\log(M)4dHK/p)\), and \(\beta=\mathcal{O}(dH\sqrt{\zeta})\)._

For a detailed proof of Theorem3.3, refer to AppendixB.1. This theorem indicates that L-UCBFair achieves \(\mathcal{O}\left(H^{2}\sqrt{d^{3}K}\right)\) bounds for both regret and distortion with high probability. Compared to the algorithms introduced by Ding et al. (2021) and Ghosh et al. (2022), which work with discrete action space, L-UCBFair guarantees the same asymptotic bounds on regret and distortion.

### R-Td3

Because assumptions in Section3.1 (e.g., the linear MDP assumption) are often violated in the real world, we also consider deep reinforcement learning methods as a flexible alternative. Concretely, we apply "Twin-Delayed Deep Deterministic Policy Gradient" (TD3) Fujimoto et al. (2018), with an implementation and default parameters provided by the open-source package "Stable Baselines 3" Raffin et al. (2021), on a Lagrangian relaxation of the long-term fairness problem with time-dependent multipliers for the reward and utility terms (Eq.6). We term this specific algorithm for long-term fairness R-TD3. While such methods lack provable safety guarantees, they may still confirm our hypothesis that agents trained via RL can learn to sacrifice short-term utility in order to drive dynamics towards preferable long-term states and explicitly incorporate dynamical control objectives provided as functions of state.

To treat the long-term fairness problem (Eq.1) using unconstrained optimization techniques (i.e., methods like TD3), we consider a time-dependent Lagrangian relaxation: We train R-TD3 to optimize

\[\min_{\pi}\underset{a_{\tau}\sim\pi\left(s_{\tau}\right)}{\mathrm{E}}\left[ \sum_{\tau=1}^{H}\Big{(}(1-\lambda_{\tau})\mathscr{L}(s_{\tau},a_{\tau})+ \lambda_{\tau}\mathscr{D}(s_{\tau},a_{\tau})\Big{)}\right],\] (6)

where \(s_{\tau+1}\sim\mathbf{P}(s_{\tau},a_{\tau})\), \(\lambda_{\tau}=\tau/H\).

Strictly applied, myopic fairness constraints can lead to undesirable dynamics and equilibria Raab and Liu (2021). Eq.6 relaxes these constraints (hard \(\rightarrow\) soft) for the near future while emphasizing them long-term. Thus, we hope to develop classifiers that learn to transition to more favorable equilibria.

### Baselines

We compare L-UCBFair and R-TD3 to a greedy agent as a proxy for a myopic status quo in which policy is repeatedly determined by optimizing for immediate utility, without regard for the population dynamics. This standard is known as "Repeated Risk Minimization" (Perdomo et al., 2020; Hu and Zhang, 2022), and we implement it using simple gradient descent on the different classes of (\(\lambda\)-parameterized) objective functions \(f\) we consider (Eq. (7)). Having adopted a notion of fairness that relies on "groups", we presuppose different groups of agents indexed by \(g\), and denote group-conditioned loss as \(\mathscr{L}_{g}\). The objectives that are approximately minimized for each baseline are

\[\text{Myopic:}\quad f(\pi) =\operatorname*{\mathbb{E}}_{a\sim\pi}\left[\mathscr{L}(s,a) \right].\] (7a) Myopic-Fair: \[f_{\lambda}(\pi) =\operatorname*{\mathbb{E}}_{a\sim\pi}\left[(1-\lambda)\mathscr{L} (s,a)+\lambda\mathscr{D}(s,a)\right],\lambda\in(0,1).\] (7b) Maxmin: \[f(\pi) =\operatorname*{\mathbb{E}}_{a\sim\pi}\left[\max_{g}\left(\mathscr{ L}_{g}(s,a)\right)\right].\] (7c) Maxmin-Fair: \[f_{\lambda}(\pi) =\operatorname*{\mathbb{E}}_{a\sim\pi}\left[(1-\lambda)\max_{g} \left(\mathscr{L}_{g}(s,a)\right)+\lambda\mathscr{D}(s,a)\right],\lambda\in( 0,1).\] (7d)

The two "Maxmin" objectives are related to distributionally robust optimization, which has been previously explored in the context of fairness (Hashimoto et al., 2018), while the two "Myopic" objectives are more straight-forward. While our baselines do not guarantee constraint satisfaction, the two objectives labelled "-Fair" are nonetheless "constraint aware" in precisely the same way as a firm that (probabilistically) incurs penalties for violating constraints.

## 4 Simulated Environments

To evaluate the proposed algorithms and baselines, we consider a series of binary (\(Y\in\{-1,1\}\)) classification tasks on a population of two groups \(\mathcal{G}=\{g_{1},g_{2}\}\) modeled according to evolutionarygame theory (using replicator dynamics, as described in Appendix A.1, in the supplementary material). We consider two families of distributions of real-valued features for the population: One that is purely synthetic, for which \(X\sim\mathcal{N}(Y,1)\), independent of group \(G\), and one that is based on logistic regressions to real-world data, described in Appendix A.2 (in the supplementary material). Both families of distributions over \(X\) are parameterized by the joint distribution \(\Pr(Y,G)\). RL agents are trained on episodes of length \(H\) initialized with randomly sampled states.

In order to better handle continuous state space, we make the following assumption, which has been used to simplify similar synthetic environments (Raab and Liu, 2021):

**Assumption 4.1** (Well-behaved feature).: For the purely synthetic setting, we require \(X\) to be a "well-behaved" real-valued feature within each group. That is, for each group \(g\), \(\Pr(Y{=}1\mid G{=}g,X{=}x)\) strictly increases in \(x\).

As an intuitive example of Assumption 4.1, if \(Y\) represents qualification for a fixed loan and \(X\) represents credit-score, we require higher credit scores to imply higher likelihood that an individual is qualified for the loan.

**Theorem 4.2** (Threshold Bayes-optimality).: _For each group \(g\), when Assumption 4.1 is satisfied, the Bayes-optimal, deterministic binary classifier is a threshold policy described by a feature threshold \(a_{g}\) for group \(g\). That is, if \(X\geq a_{g}\) then \(\hat{Y}=1\); otherwise, \(\hat{Y}=-1\)._

As a result of Theorem 4.2, we consider our action space to be the space of group-specific thresholds, and denote an individual action as the vector \(a\coloneqq(a_{1},a_{2})\). Nonetheless, we note that Assumption 4.1 is often violated in practice, as it is in our semi-synthetic setting.

## 5 Experimental Results

Do RL agents learn to seek favorable equilibria against short-term utility? Is Lagrangian relaxation Eq.1 sufficient to encourage this behavior? We give positive demonstrations for both questions.

### Losses and Disparities Considered

Our experiments consider losses \(\mathscr{L}\) which combine true-positive and true-negative rates, i.e.,

\[\mathscr{L}(s,a)=1-\alpha\mathtt{tp}(s,a)-\beta\mathtt{tn}(s,a)\quad;\quad r (s,a)=\alpha\mathtt{tp}(s,a)+\beta\mathtt{tn}(s,a),\] (8)

where \(\alpha,\beta\in[0,1]\); \(\mathtt{tp}(s,a)=\Pr_{s,a}(\hat{Y}{=}1,Y{=}1)\); and \(\mathtt{tn}(s,a)=\Pr_{s,a}(\hat{Y}{=}{-}1,Y{=}{-}1)\).

For disparity \(\mathscr{D}\), we consider functions of the form \(\mathscr{D}(s,a)=\frac{1}{2}\|\xi_{s,a}(g_{1}){-}\xi_{s,a}(g_{2})\|^{2}\) that measure violations of demographic parity (DP) (Dwork et al., 2012), equal opportunity (EOp) (Hardt et al., 2016), equal odds (EO), and qualification rate parity (QR), described in Table1. We note that QR is inconsequential for the baseline agents, which ignore the mutability of the population state.

### Results

Our experiments show that algorithms trained with an RL formulation of long-term fairness can drive a reactive population toward states with higher utility and fairness than myopic policies. In Fig.1, the baseline policies (i.e., subfigures (a) and (b)), which focus on short-term incentives drive disparate qualification rates and attain lower long-term utility than the RL agents, indicating that short-term utility is _misaligned_ with desirable dynamics in this example. In subfigure (b) in particular, the baseline policy increasingly violates the static fairness condition with time, agreeing with previous results by Raab and Liu (2021). Meanwhile, the RL algorithms (subfigures (c) and (d)) learn to drive universally high qualification rates, thus allowing policies that capture higher utility with time (subfigure (e)), Fig.3.

Our central hypothesis, that long-term fairness via RL may induce an algorithm to sacrifice short-term utility for better long-term outcomes, is clearly demonstrated in the purely synthetic environment depicted by subfigures (a1-a3) of Fig. 2, in which the environment provides higher immediate utility (true-positive rates) but lower long-term utility when a policy chooses initially high acceptance rates. In this case, the RL algorithm (subfigure (a2)) drives the system towards high qualification rates by giving up immediate utility maximized by the myopic agent (subfigure (a1)).

With subfigures (b1-b3) of Fig. 2, we demonstrate the capability of RL to incorporate notions of fairness (e.g., qualification rate parity QR), that are impossible to formulate in the myopic setting. In subfigures (b1-b3), both RL agents learn to satisfy qualification rate parity by driving the state of the population towards equal qualification rates by group.

Figure 1: Using a modeled population with scalar features fit to the “Adult” dataset (Dua and Graff, 2017) at each time-step to mirror the evolving qualification rates (Appendix A.2), we compare our baseline algorithms to L-UCBFair and R-TD3 on the same set of 16 initial states with the task of maximizing true positive rates (tp) subject to demographic parity (DP). Plots (a-d) depict evolving group qualification rates under each algorithm with streamlines (_red arrows indicate the author’s emphasis_), while shading indicating immediate violations of demographic parity. We remark that the corresponding plots for the non“-Fair” baselines are qualitatively indistinct and omit them for space. In subplot (e), we visualize mean episode loss where \(H{=}150\) for each algorithm.

Figure 2: Subfigures (a1–a3) compare a baseline policy to L-UCBFair in a purely synthetic environment on the task maximizing the fraction of true-positive classifications (tp) subject to bounded demographic parity violation (DP). Episode length in this environment is \(H=100\). Subfigures (b1-b3) compare L-UCBFair to R-TD3 on a similar task subject to bounded qualification rate disparity instead. In both environments, modeled agent utilities translate higher acceptance rates to lower qualification rates. Figures (a1, a2) and (b1, b2) depict evolving qualification rates with streamlines (_red arrows indicate the author’s emphasis_) and use shading to indicate fairness violations.

Finally, our experiments also show that RL algorithms without theoretical guarantees may be applicable to long-term fairness. In Fig. 2 subfigure (b2), R-TD3 achieves similar qualitative behavior as L-UCBFair (subfigure (b1)), that is, driving high qualification at the expense of short-term utility) while achieving lower episodic mean loss (subfigure (b3)).

LimitationsDespite the potential, highlighted by our experiments, for RL-formulation of fairness to drive positive long-term social outcomes, it is not possible to truly validate such approaches without deployment on actual populations, which may be practically and ethically fraught. In addition, violations of fairness or decreased utility may be difficult to justify to affected populations and stakeholders, especially when the bounds provided by L-UCBFair, while as tight as any known, rely on assumptions that may be violated in practice.

Concluding remarksMachine learning techniques are frequently deployed in settings in which affected populations will _react_ to the resulting policy, closing a feedback loop that must be accounted for. In such settings, algorithms that prioritize immediate utility or static notions of fairness may yield dynamics that are _misaligned_ with these objectives long-term.

In this paper, we have reformulated long-term fairness as an online reinforcement learning problem (Eq. (1)) to address the importance of dynamics. We have shown that algorithmic solutions to this problem (e.g., L-UCBFair) are capable of simultaneous theoretical guarantees regarding cumulative loss and disparity (violations of fairness). We have also shown that these guarantees can be relaxed in practice to accommodate a wider class of RL algorithms, such as R-TD3. Finally, we emphasize again that the RL framework of long-term fairness allows notions of disparity inherent to the _state_ of a population to be explicitly treated, while such definitions are inoperable in the standard, myopic framework of fairness. We hope that our contributions spur interest in long-term mechanisms and incentive structures for machine learning to be a driver of positive social change.

AcknowledgementsThis work is partially supported by the National Science Foundation (NSF) under grants IIS-2143895, IIS-2112471, IIS-2040800, and CCF-2023495.

Figure 4: Comparison of total **accumulated** disparity (blue) and loss (orange) over a single episode of \(H=150\) steps as functions of initial state (initial qualification rates), for R-TD3 and TD3 (no constraint; \(\lambda=0\)) in the semi-synthetic environment (Adult dataset). This figure indicates that the increasingly weighted fairness term in the objective of R-TD3 can play a role in more rapidly reducing cumulative disparity, in this case by driving the system towards the line of equal qualification rates, at the cost of increased cumulative loss.

Figure 3: The mean episodic loss (left) and disparity (right) within a 20-step sliding window obtained by L-UCBFair for the (tp, DP) setting of Fig. 1 (a2). We emphasize that both loss and disparity decrease in time, as required for the sublinear regret and distortion guaranteed by (Theorem 3.3).

## References

* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47(2):235-256, 2002.
* Bai et al. (2022) Qinbo Bai, Amrit Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 3682-3689, 2022.
* Brantley et al. (2020) Kiante Brantley, Miro Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. _Advances in Neural Information Processing Systems_, 33:16315-16326, 2020.
* Brunton et al. (2021) Steven L Brunton, Marko Budisic, Eurika Kaiser, and J Nathan Kutz. Modern koopman theory for dynamical systems. _arXiv preprint arXiv:2102.12086_, 2021.
* Chaney et al. (2018) Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. In _Proceedings of the 12th ACM Conference on Recommender Systems_, pages 224-232. ACM, 2018.
* Chen et al. (2022) Yatong Chen, Reilly Raab, Jialu Wang, and Yang Liu. Fairness transferability subject to bounded distribution shift. _arXiv preprint arXiv:2206.00129_, 2022.
* Coate and Loury (1993) Stephen Coate and Glenn C Loury. Will affirmative-action policies eliminate negative stereotypes? _The American Economic Review_, pages 1220-1240, 1993.
* Corbett-Davies and Goel (2018) Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. _arXiv preprint arXiv:1808.00023_, 2018.
* Crawford and Calo (2016) Kate Crawford and Ryan Calo. There is a blind spot in AI research. _Nature News_, 538(7625):311, 2016.
* D'Amour et al. (2020) Alexander D'Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D Sculley, and Yoni Halpern. Fairness is not static: Deeper understanding of long term fairness via simulation studies. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, pages 525-534, 2020.
* Ding et al. (2020) Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic. Natural policy gradient primal-dual method for constrained markov decision processes. _Advances in Neural Information Processing Systems_, 33:8378-8390, 2020.
* Ding et al. (2021) Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic. Provably efficient safe exploration via primal-dual policy optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 3304-3312. PMLR, 2021.
* Dua and Graff (2017) Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
* Dwork et al. (2012) Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In _Proceedings of the 3rd innovations in theoretical computer science conference_, pages 214-226, 2012.
* Efroni et al. (2020) Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained mdps. _arXiv preprint arXiv:2003.02189_, 2020.
* Ensign et al. (2018) Danielle Ensign, Sorelle A Friedler, Scott Neville, Carlos Scheidegger, and Suresh Venkatasubramanian. Runaway feedback loops in predictive policing. In _Conference of Fairness, Accountability, and Transparency_, 2018.
* Epasto et al. (2020) Alessandro Epasto, Mohammad Mahdian, Vahab Mirrokni, and Emmanouil Zampetakis. Optimal approximation-smoothness tradeoffs for soft-max functions. _Advances in Neural Information Processing Systems_, 33:2651-2660, 2020.
* Efroni et al. (2020)Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In _proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 259-268, 2015.
* Fujimoto et al. (2018) Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* Fuster et al. (2018) Andreas Fuster, Paul Goldsmith-Pinkham, Tarun Ramadorai, and Ansgar Walther. Predictably unequal? The effects of machine learning on credit markets. _The Effects of Machine Learning on Credit Markets_, 2018.
* Ghosh et al. (2022) Arnob Ghosh, Xingyu Zhou, and Ness Shroff. Provably efficient model-free constrained rl with linear function approximation. _arXiv preprint arXiv:2206.11889_, 2022.
* Hardt et al. (2016) Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning, 2016.
* Hashimoto et al. (2018) Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In _International Conference on Machine Learning_, pages 1929-1938. PMLR, 2018.
* Heidari et al. (2019) Hoda Heidari, Vedant Nanda, and Krishna P. Gummadi. On the long-term impact of algorithmic decision policies: Effort unfairness and feature segregation through social learning. _the International Conference on Machine Learning (ICML)_, 2019.
* Hu and Chen (2018) Lily Hu and Yiling Chen. A short-term intervention for long-term fairness in the labor market. In _Proceedings of the 2018 World Wide Web Conference on World Wide Web_, pages 1389-1398. International World Wide Web Conferences Steering Committee, 2018.
* Hu et al. (2019) Lily Hu, Nicole Immorlica, and Jennifer Wortman Vaughan. The disparate effects of strategic manipulation. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, pages 259-268, 2019.
* Hu and Zhang (2022) Yaowei Hu and Lu Zhang. Achieving long-term fairness in sequential decision making. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9549-9557, 2022.
* Jabbari et al. (2017) Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in reinforcement learning. In _International conference on machine learning_, pages 1617-1626. PMLR, 2017.
* Jin et al. (2020) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Joseph et al. (2016) Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. _Advances in neural information processing systems_, 29, 2016.
* Kalagarla et al. (2021) Krishna C Kalagarla, Rahul Jain, and Pierluigi Nuzzo. A sample-efficient algorithm for episodic finite-horizon mdp with constraints. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8030-8037, 2021.
* Liu et al. (2018) Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In _International Conference on Machine Learning_, pages 3150-3158. PMLR, 2018.
* Liu et al. (2020) Lydia T Liu, Ashia Wilson, Nika Haghtalab, Adam Tauman Kalai, Christian Borgs, and Jennifer Chayes. The disparate equilibria of algorithmic decision making when individuals invest rationally. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, pages 381-391, 2020.
* Liu et al. (2021) Tao Liu, Ruida Zhou, Dileep Kalathil, Panganamala Kumar, and Chao Tian. Learning policies with zero or bounded constraint violation for constrained mdps. _Advances in Neural Information Processing Systems_, 34:17183-17193, 2021.
* Liu et al. (2019)* Liu et al. (2017) Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C Parkes. Calibrated fairness in bandits. _arXiv preprint arXiv:1707.01875_, 2017.
* Mouzannar et al. (2019) Hussein Mouzannar, Mesrob I Ohannessian, and Nathan Srebro. From fair decision making to social equality. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, pages 359-368. ACM, 2019.
* Pan et al. (2019) Ling Pan, Qingpeng Cai, Qi Meng, Wei Chen, Longbo Huang, and Tie-Yan Liu. Reinforcement learning with dynamic boltzmann softmax updates. _arXiv preprint arXiv:1903.05926_, 2019.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019.
* Perdomo et al. (2020) Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dunner, and Moritz Hardt. Performative prediction. In _International Conference on Machine Learning_, pages 7599-7609. PMLR, 2020.
* Raab and Liu (2021) Reilly Raab and Yang Liu. Unintended selection: Persistent qualification rate disparities and interventions. _Advances in Neural Information Processing Systems_, 34:26053-26065, 2021.
* Raffin et al. (2021) Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021.
* Singh et al. (2020) Rahul Singh, Abhishek Gupta, and Ness B Shroff. Learning in markov decision processes under constraints. _arXiv preprint arXiv:2002.12435_, 2020.
* Tang et al. (2021) Wei Tang, Chien-Ju Ho, and Yang Liu. Bandit learning with delayed impact of actions. _Advances in Neural Information Processing Systems_, 34:26804-26817, 2021.
* Wei et al. (2022) Honghao Wei, Xin Liu, and Lei Ying. Triple-q: A model-free algorithm for constrained reinforcement learning with sublinear regret and zero constraint violation. In _International Conference on Artificial Intelligence and Statistics_, pages 3274-3307. PMLR, 2022.
* Wen et al. (2019) Min Wen, Osbert Bastani, and Ufuk Topcu. Fairness with dynamics. _arXiv preprint arXiv:1901.08568_, 2019.
* Williams and Kolter (2019) Joshua Williams and J Zico Kolter. Dynamic modeling and equilibria in fair decision making. _arXiv preprint arXiv:1911.06837_, 2019.
* Xu et al. (2021) Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement learning with convergence guarantee. In _International Conference on Machine Learning_, pages 11480-11491. PMLR, 2021.
* Zemel et al. (2013) Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In _International conference on machine learning_, pages 325-333. PMLR, 2013.
* Zhang et al. (2020) Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and Cheng Zhang. How do fair decisions fare in long-term qualification? _arXiv preprint arXiv:2010.11300_, 2020.
* Zheng and Ratliff (2020) Liyuan Zheng and Lillian Ratliff. Constrained upper confidence reinforcement learning. In _Learning for Dynamics and Control_, pages 620-629. PMLR, 2020.

Simulated Environments: Supplementary

### Replicator Dynamics

Our use of replicator dynamics closely mirrors that of Raab and Liu (2021) as an "equitable" model of a population: Individuals my be modeled identically, independently of group membership, yet persistent outcome disparities may emerge from disparate initial conditions between groups. For our experiments, we parameterize the evolving distribution \(\Pr(X,Y\mid G)\), assuming constant group sizes, in terms of "qualification rates" \(q_{g}\coloneqq\Pr(Y{=}1\mid G{=}g)\) and update these qualification rates according to the discrete-time replicator dynamics:

\[q_{g}[t+1]=q_{g}[t]\frac{W_{1}^{g}[t]}{\bar{W}^{g}[t]};\quad\bar{W}^{g}[t] \coloneqq W_{1}^{g}q_{g}+(1-q_{g})W_{-1}^{g}.\]

In this model, the _fitness_\(W_{g}^{g}>0\) of label \(Y{=}y\) in group \(G{=}g\) may be interpreted as the "average utility to the individual" in group \(g\) of possessing label \(y\), and thus relative _replication rate_ of label \(y\) in group \(g\), as agents update their labels by mimicking the successful strategies of in-group peers. We model \(W_{y}^{g}\) in terms of acceptance and rejection rates with a group-independent utility matrix \(U\):

\[W_{y}^{g}=\sum_{\hat{y}\in\{-1,1\}}U_{y,\hat{y}}\Pr\Bigl{(}\hat{Y}{=}\hat{y} \mid Y{=}y,G{=}g\Bigr{)}.\]

We choose the matrix \(U\) to eliminate dominant strategies (i.e., when agents universally prefer one label over another, independent of classification), assert that agents always prefer acceptance over rejection, and to imply that the costs of qualification are greater than the costs of non-qualification among accepted individuals. While other parameterizations of \(U\) are valid, this choice of parameters guarantees internal equilibrium of the replicator dynamics for a Bayes-optimal classifier and "well-behaved" scalar-valued feature \(X\), such that \(\Pr(Y{=}1\mid X{=}x)\) is monotonically increasing in \(x\)(Raab and Liu, 2021).

### Data Synthesis and Processing

In addition to a synthetic distribution, for which we assume \(X\sim\mathcal{N}(Y,1)\), independent of \(G\), for all time, we also consider real-world distributions when simulating and comparing algorithms for "long-term fairness". In both cases, as mentioned above, we wish to parameterize distributions in terms of qualification rates \(q_{g}\). As we perform binary classification on discrete groups and scalar-valued features, in addition to parameterizing a distribution in terms of \(q_{g}\), we desire a scalar-valued feature for each example, rather than the multi-dimensional features common to real-world data.

Our solution is to use an additional learning step for "preprocessing": Given a static dataset \(\mathcal{D}\) from which \((X^{\prime},Y,G)\) is drawn i.i.d., (e.g., the "Adult Data Set" Dua and Graff (2017)), where \(G\) varies over an individual's sex and \(Y\) corresponds to whether an individual has more than 50,000 USD in annual income, at each time-step, we train a stochastic binary classifier \(\tilde{a}\), such that \(\hat{Y}^{\prime}\sim\tilde{a}(X^{\prime},G)\) with a loss that re-weights examples by label value according to \(q_{g}\). That is, at each time step, we solve:

\[\min_{\tilde{a}}\quad\operatorname*{\mathbb{E}}_{\tilde{a},\mathcal{D}}\Big{[} w(X^{\prime},Y,G)L(Y,\hat{Y}^{\prime})\Big{]},\]

where

\[w(X^{\prime},Y,G)=\bigg{(}\frac{1+Y}{2}\bigg{)}\bigg{(}\frac{q}{\Pr(Y{=}1\mid G )}\bigg{)}+\bigg{(}\frac{1-Y}{2}\bigg{)}\bigg{(}\frac{1-q}{\Pr(Y{=}-1\mid G)} \bigg{)},\]

and \(L\) is zero-one loss. In our experiments, we choose \(\tilde{a}\) according to logistic regression. We interpret \(\Pr(\hat{Y}^{\prime}{=}1)\), drawn from the learned "preprocessing" function \(\tilde{a}\), as a new, scalar feature value \(X\in\mathbf{R}\) mapped from from higher-dimensional features \(X^{\prime}\). The threshold policy ultimately operates on \(X\).

Assumption 4.1 is as hard to satisfy in general as solving the Bayes-optimal binary classification task over higher-dimensional features. Nonetheless, we expect Assumption 4.1 to be approximately satisfied by such a "preprocessing" pipeline. It is important to note that the behavior introduced by retraining a logistic classifier at each time step can yield very different qualitative behavior when compared to the static, purely synthetic distribution, as the distribution of \(X\) conditioned on \(Y\) and \(G\)_also_ mutates, and Assumption 4.1 may be violated.

Deferred Proofs

Without loss of generality, we assume \(\|\phi(s,a)\|\leq 1\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), and \(\max\left\{\left\|\mu_{h}(\mathcal{S})\right\|,\|\theta_{h}\|\right\}\leq\sqrt{d}\) for all \(h\in[H]\).

**Assumption B.1** (\(\mathscr{V}\) bound for \(\nu^{*}\)).: We define a parameter \(\mathscr{V}\) that is assumed to bound the optimal value of \(\nu\) in Eq.4. For \(\bar{\pi}\) and \(\gamma>0\) satisfying Slater's Condition (Assumption2.1),

\[\nu^{*}\leq\frac{V_{r,1}^{\pi*}\left(s_{1}\right)-V_{r,1}^{\pi}\left(s_{1} \right)}{\gamma}\leq\frac{H}{\gamma}\coloneqq\mathscr{V}.\]

In AssumptionB.1, \(\mathscr{V}=H/\gamma\), upper-bounds the optimal dual variable \(\nu^{*}\) as an input to L-UCBFair.

**Definition B.2**.: Given a set of distinct actions \(I=\{I_{0},\cdots,I_{M}\}\subset\mathcal{A}\), where \(\mathcal{A}\) is a closed set in Euclidean space, define \(\mathcal{I}_{i}=\{a\colon\|a-I_{i}\|_{2}\leq\|a-I_{j}\|_{2},\forall j\neq i\}\) as the subset of actions closer to \(I_{i}\) than to \(I_{j}\), i.e., the Voronoi region corresponding to locus \(I_{i}\), with tie-breaking imposed by the order of indices \(i\). Also define the locus function \(I(a)=\min_{i}\operatorname*{argmin}_{I_{i}}\|a-I_{i}\|_{2}\).

**Lemma B.3**.: _The Voronoi partitioning described above satisfies \(\mathcal{I}_{i}\cap\mathcal{I}_{j}=\varnothing,\forall i\neq j\) and \(\cup_{i=1}^{M}\mathcal{I}_{i}=\mathcal{A}\)._

Proof.: We will begin by proving \(\mathcal{I}_{i}\cap\mathcal{I}_{j}=\varnothing\) for all \(i\neq j\). To establish this, we will assume the contrary, that there exist indices \(i\) and \(j\) such that \(\mathcal{I}_{i}\cap\mathcal{I}_{j}\neq\varnothing\). Without loss of generality, assume \(i>j\). We will denote an arbitrary action within the interaction of \(\mathcal{I}_{i}\) and \(\mathcal{I}_{j}\) as \(a^{\prime}\in\mathcal{A}\).

Considering that \(a^{\prime}\in\mathcal{I}_{i}\), according to the given DefinitionB.2, we can infer that \(|a^{\prime}-I_{i}|_{2}<|a^{\prime}-I_{j}|_{2}\) (since \(i>j\)). However, this assertion contradicts the fact that \(a^{\prime}\in\mathcal{I}_{j}\), which implies \(|a^{\prime}-I_{j}|_{2}\leq|a^{\prime}-I_{i}|_{2}\). Therefore, \(\mathcal{I}_{i}\cap\mathcal{I}_{j}=\varnothing\) for all \(i\neq j\).

We then proof \(\cup_{i=1}^{M}\mathcal{I}_{i}=\mathcal{A}\). Since \(\mathcal{A}\) is a closed set, for any \(a\in\mathcal{A}\), there must be a \(i\in\{1,2,\cdots,M\}\), such that \(d_{a,i}=\|a-I_{i}\|_{2}\leq\|a-I_{j}\|_{2}=d_{a,j},\forall j\). If \(d_{a,i}<d_{a,j}\) strictly holds for all \(j\), then \(a\in\mathcal{I}_{i}\). Otherwise define a set \(\mathcal{J}=\{j|d_{a,j}=d_{a,i}\}\), then \(a\in\mathcal{I}_{j^{\prime}}\) where \(j^{\prime}=\operatorname*{argmin}_{j\in\mathcal{J}}j\). 

**Theorem B.4**.: _If the number \(M\) of distinct loci or regions partitioning \(\mathcal{A}\) is sufficiently large, there exists a finite set of loci \(I\) such that \(\forall a\in\mathcal{I}_{i},i\in M,\|a-I_{i}\|_{2}\leq\epsilon_{I}\)._

Proof.: Since \(\mathcal{A}\) is closed in euclidean space, denote \(N\) the dimension, \(d=\sup_{a,a^{\prime}\in\mathcal{A}}\|a-a^{\prime}\|_{2}\). Define an Orthonormal Basis \(\{e_{1},e_{2},\cdots,e_{N}\}\). Randomly choose a point \(a\in\mathcal{A}\), Set it as \(I_{0}\), then we form a set of loci \(I=\{I_{0}+\sum_{i=1}^{N}\epsilon k_{i}e_{i}|I_{0}+\sum_{i=1}^{N}\epsilon k_{i}e _{i}\in\mathcal{A},k_{i}\in\mathcal{Z},-\lceil\frac{d}{\epsilon}\rceil\leq k_ {i}\leq\lceil\frac{d}{\epsilon}\rceil\}\). We know that \(|I|\leq\left(2\lceil\frac{d}{\epsilon}\rceil\right)^{N}\). It's not hard to verify that \(\|a-I_{i}\|_{2}\leq\frac{\epsilon}{2}\sqrt{2}^{N-1},\forall a\in\mathcal{I}_ {i}\). Taken \(\epsilon_{I}=\frac{\epsilon}{2}\sqrt{2}^{N-1}\) yields the statement.

### Proof of Theorem3.3

**Theorem 3.3** (Boundedness).: _With probability \(1-p\), there exists a constant \(b\) such that L-UCBFair (Algorithm1) achieves_

\[\mathrm{Regret}(K)\leq\big{(}b\zeta H^{2}\sqrt{d^{3}}+(\mathscr{V}+1)H\big{)} \sqrt{K}\quad;\quad\mathrm{Drtn}(K)\leq\frac{b\zeta(1+\mathscr{V})H^{2}\sqrt{d ^{3}}}{\mathscr{V}}\sqrt{K},\] (5)

_for parameter values \(\varsigma{=}1\)\(\epsilon_{I}\leq\frac{1}{2\rho(1+\mathscr{V})KH\sqrt{d}}\), \(\zeta=\log(\log(M)4dHK/p)\), and \(\beta=\mathcal{O}(dH\sqrt{\zeta})\)._

**Outline** The outline of this proof simulates the proof in Ghosh et al. (2022). For brevity, denote \(\mathbb{P}_{h}V_{j,h+1}^{\pi}(s,a)=\mathbb{E}_{s^{\prime}\sim\mathbb{P}_{h}( \cdot|s,a)}V_{j,h+1}^{\pi}\left(s^{\prime}\right)\) for \(j=r,g\). Then

\[Q_{j,h}^{\pi}(s,a)=\left(r_{h}+\mathbb{P}_{h}V_{j,h+1}^{\pi} \right)(s,a)\] (9) \[V_{j,h}^{\pi}(s)=\left\langle\pi_{h}(\cdot\mid s)Q_{j,h}^{\pi}(s, \cdot)\right\rangle_{\mathcal{A}}\] (10) \[\left\langle\pi_{h}(\cdot\mid s),Q_{j,h}^{\pi}(s,\cdot)\right\rangle _{\mathcal{A}}=\sum_{a\in\mathcal{A}}\pi_{h}(a\mid s)Q_{j,h}^{\pi}(s,a)\] (11)\(\mathcal{T}_{3}\) is easily bounded if \(\eta\). The major task remains bound \(\mathcal{T}_{1}\) and \(\mathcal{T}_{2}\).

**Bound \(\mathcal{T}_{1}\) and \(\mathcal{T}_{2}\).** We have following two lemmas.

**Lemma B.5** (Boundedness of \(\mathcal{T}_{1}\)).: _With probability \(1-p/2\), we have \(\mathcal{T}_{1}\leq KH\left(\frac{\log(M)}{\alpha}+2(1+\mathscr{V})H\rho \epsilon_{I}\sqrt{\frac{dK}{\varsigma}}\right)\). Specifically, if \(\alpha=\frac{\log(M)K}{2(1+\mathscr{V}+H)}\) and \(\varsigma=1\), we have \(\mathcal{T}_{1}\leq 2H(1+\mathscr{V}+H)+2KH^{2}(1+\mathscr{V})\rho\epsilon_{I} \sqrt{dK}\) with probability \(1-p/2\)._

**Lemma B.6** (Boundedness of \(\mathcal{T}_{2}\)).: _Ghosh et al. (2022) With probability \(1-p/2,\mathcal{T}_{2}\leq\mathcal{O}\left((\nu+1)H^{2}\zeta\sqrt{d^{3}K}\right)\), where \(\zeta=\log[\log(M)4dHK/p]\)._

Lemma B.6 follows the same logic in Ghosh et al. (2022), and we delay the proof of Lemma B.5 to Section B.2. Now we are ready to proof Theorem 3.3.

Proof.: For any \(\nu\in[0,\mathscr{V}]\), with prob. \(1-p\),

\[\begin{split}&\text{Regret}(K)+\nu\text{Distortion}(K)\\ \leq&\mathcal{T}_{1}+\mathcal{T}_{2}+\mathcal{T}_{3} \\ \leq&\frac{1}{2\eta}\nu^{2}+\frac{\eta}{2}H^{2}K+ \frac{HK\log M}{\alpha}+2KH^{2}(1+\mathscr{V})\rho\epsilon_{I}\sqrt{dK}+ \mathcal{O}\left((\nu+1)H^{2}\zeta\sqrt{d^{3}K}\right)\end{split}\] (13)

Taking \(\nu=0,\eta=\frac{\mathscr{V}}{\sqrt{KH^{2}}},\alpha=\frac{K\log M}{2(1+ \mathscr{V}+H)},\epsilon_{I}=\frac{1}{2\rho(1+\mathscr{V})KH\sqrt{d}}\), there exist constant \(b\),

\[\begin{split}\text{Regret}(K)&\leq\frac{\mathscr{V }H}{2}\sqrt{K}+2H(1+\mathscr{V}+H)+2H^{2}K(1+\mathscr{V})\rho\epsilon_{I} \sqrt{dK}+\mathcal{O}\left(H^{2}\zeta\sqrt{d^{3}K}\right)\\ &\leq\left(b\zeta H^{2}\sqrt{d^{3}}+(\mathscr{V}+1)H\right) \sqrt{K}=\tilde{\mathcal{O}}(H^{2}\sqrt{d^{3}K}).\end{split}\]

Taking \(\nu=\mathscr{V},\eta=\frac{\mathscr{V}}{\sqrt{KH^{2}}},\alpha=\frac{K\log M}{ 2(1+\mathscr{V}+H)},\epsilon_{I}=\frac{1}{2\rho(1+\mathscr{V})KH\sqrt{d}}\),

\[\begin{split}\text{Regret}(K)&+\mathscr{V}\text{ Distortion}(K)\leq(\mathscr{V}+1)H\sqrt{K}+(1+\mathscr{V})\mathcal{O}\left(H^{2} \zeta\sqrt{d^{3}K}\right)\end{split}\]

Following the idea of Efroni et al. (2020), there exists a policy \(\pi^{\prime}\) such that \(V_{r,1}^{\pi^{\prime}}=\frac{1}{K}\sum_{k=1}^{K}V_{r,1}^{\pi_{k}},V_{g,1}^{\pi^ {\prime}}=\frac{1}{K}\sum_{k=1}^{K}V_{g,1}^{\pi_{k}}\). By the occupancy measure, \(V_{r,1}^{\pi}\) and \(V_{g,1}^{\pi}\) are linear in occupancy measure induced by \(\pi\). Thus, the average of \(K\) occupancy measure also produces an occupancy measure which induces policy \(\pi^{\prime}\) and \(V_{r,1}^{\pi^{\prime}}\), and \(V_{g,1}^{\pi^{\prime}}\). We take \(\nu=0\) when \(\sum_{k=1}^{K}\left(b-V_{g,1}^{\pi_{k}}\left(s_{1}^{k}\right)\right)<0\)otherwise \(\nu=\mathcal{V}\). Hence, we have

\[V_{r,1}^{\pi^{*}}\left(s_{1}\right)-\frac{1}{K}\sum_{k=1}^{K}V_{r,1 }^{\pi_{k}}\left(s_{1}\right)+\mathcal{V}\max\left(\left(c-\frac{1}{K}\sum_{k=1 }^{K}V_{g,1}^{\pi_{k}}\left(s_{1}\right),0\right)\right.\] \[=V_{r,1}^{\pi^{*}}\left(s_{1}\right)-V_{r,1}^{\pi^{\prime}}\left(s _{1}\right)+\mathcal{V}\max\left(c-V_{g,1}^{\pi^{\prime}}\left(s_{1}\right),0\right)\] \[\leq\frac{\mathcal{V}+1}{K}H\sqrt{K}+\frac{\mathcal{V}+1}{K} \mathcal{O}\left(H^{2}\zeta\sqrt{d^{3}K}\right)\] (14)

Since \(\mathcal{V}=2H/\gamma\), and using the result of Lemma B.15, we have

\[\max\left(c-\frac{1}{K}\sum_{k=1}^{K}V_{g,1}^{\pi_{k}}\left(s_{1}^{k}\right),0 \right)\leq\frac{\mathcal{V}+1}{K\mathcal{V}}\mathcal{O}\left(H^{2}\zeta \sqrt{d^{3}K}\right)\]

In this section we proof Lemma B.5 and Lemma B.6.

### Prepare for Lemma B.5

In order to bound \(\mathcal{T}_{1}\) and \(\mathcal{T}_{2}\), we introduce the following lemma.

**Lemma B.7**.: _There exists a constant \(B_{2}\) such that for any fixed \(p\in(0,1)\), with probability at least \(1-p/2\), the following event holds_

\[\|\sum_{\tau=1}^{k-1}\phi_{j,h}^{\tau}\left[V_{j,h+1}^{k}\left(s_{h+1}^{\tau} \right)-\mathbb{P}_{h}V_{j,h+1}^{k}\left(s_{h}^{\tau},a_{h}^{\tau}\right)\|_{ \left(c_{h}^{k}\right)^{-1}}\leq B_{2}dHq\]

_for \(j\in\{r,g\}\), where \(q=\sqrt{\log\left[4\left(B_{1}+1\right)\log(M)dT/p\right]}\) for some constant \(B_{1}\)._

We delay the proof of Lemma B.7 to Appendix B.4.

Lemma B.7 shows the bound of estimated value function \(V_{j,h}^{k}\) and value function \(V_{j,h}^{\pi}\) corresponding in a given policy at k. We now introcuce the following lemma appeared in Ghosh et al. (2022). This lemma bounds the difference between the value function without bonus in L-UCBFair and the true value function of any policy \(\pi\). This is bounded using their expected difference at next step, plus a error term.

**Lemma B.8**.: _Ghosh et al. (2022) There exists an absolute constant \(\beta=C_{1}dH\sqrt{\zeta},\zeta=\log(\log(M)4dT/p)\), and for any fixed policy \(\pi\), for the event defined in Lemma B.7, we have_

\[\left\langle\phi(s,a),w_{j,h}^{k}\right\rangle-Q_{j,h}^{\pi}(s,a)=\mathbb{P}_ {h}\left(V_{j,h+1}^{k}-V_{j,h+1}^{\pi}\right)(s,a)+\Delta_{h}^{k}(s,a)\]

_for some \(\Delta_{h}^{k}(s,a)\) that satisfies \(\left|\Delta_{h}^{k}(s,a)\right|\leq\beta\sqrt{\phi(s,a)^{T}\left(\Lambda_{h} ^{k}\right)^{-1}\phi(s,a)}\)._

**Lemma B.9**.: _Ghosh et al. (2022) With probability at least \(1-p/2\), (for the event defined in Lemma B.7)_

\[Q_{r,h}^{\pi}(s,a)+\nu_{k}Q_{g,h}^{\pi}(s,a)\leq Q_{r,h}^{k}(s,a)+\nu_{k}Q_{g,h}^{k}(s,a)-\mathbb{P}_{h}\left(V_{h+1}^{k}-V_{h+1}^{\pi,\nu_{k}}\right)(s,a)\]

We also introduce the following lemma. This lemma bound the value function by taking L-UCBFair policy and greedy policy.

**Lemma B.10**.: _Define \(\bar{V}_{h}^{k}(\cdot)=\max_{a}\left[Q_{r,h}^{k}(\cdot,a)+\nu_{k}Q_{g,h}^{k}( \cdot,a)\right]\) the value function corresponding to greedy policy, we have_

\[\bar{V}_{h}^{k}(s)-V_{h}^{k}(s)\leq\frac{\log M}{\alpha}+2(1+\mathcal{V})H \rho\epsilon_{I}\sqrt{\frac{dk}{\varsigma}}.\] (15)Proof.: Define \(a_{g}\) the solution of greedy policy,

\[\bar{V}_{h}^{k}(s)-V_{h}^{k}(s) =\big{[}Q_{r,h}^{k}\left(s,a_{g}\right)+\nu_{k}Q_{g,h}^{k}\left(s,a_{ g}\right)\big{]}\] (16) \[\quad-\int_{a}\pi_{h,k}(a\mid s)\left[Q_{r,h}^{k}(s,a)+\nu_{k}Q_{g,h}^{k}(s,a)\right]da\] (17) \[\leq\big{[}Q_{r,h}^{k}\left(s,a_{g}\right)+\nu_{k}Q_{g,h}^{k} \left(s,a_{g}\right)\big{]}\] (18) \[\quad-\sum_{i}\text{SM}_{\alpha}(I_{i}\mid x)\left[Q_{r,h}^{k}(x, I_{i})+\nu_{k}Q_{g,h}^{k}(x,I_{i})\right]+2(1+\mathscr{V})H\rho\epsilon_{I} \sqrt{\frac{dk}{\varsigma}}\] (19) \[\leq\left(\frac{\log\left(\sum_{a}\exp\left(\alpha\left(Q_{r,h}^ {k}(s,I_{i})+\nu_{k}Q_{g,h}^{k}(s,I_{i})\right)\right)\right)}{\alpha}\right)\] (20) \[\quad-\sum_{i}\text{SM}_{\alpha}(I_{i}\mid s)\left[Q_{r,h}^{k}(s, I_{i})+\nu_{k}Q_{g,h}^{k}(s,I_{i})\right]+2(1+\mathscr{V})H\rho\epsilon_{I} \sqrt{\frac{dk}{\varsigma}}\] (21) \[\leq\frac{\log(M)}{\alpha}+2(1+\mathscr{V})H\rho\epsilon_{I} \sqrt{\frac{dk}{\varsigma}}.\] (22)

The first inequality follows from Lemma B.14 and the second inequality holds because of Proposition 1 in Pan et al. (2019).

### Proof of Lemma b.5

Now we're ready to proof Lemma B.5.

Proof.: This proof simulates Lemma 3 in Ghosh et al. (2022).

We use induction to proof this lemma. At step \(H\), we have \(Q_{j,H+1}^{k}=0=Q_{j,H+1}^{\pi}\) by definition. Under the event in Lemma B.13 and using Lemma B.8, we have for \(j=r,g\),

\[\big{|}\big{\langle}\phi(s,a),w_{j,H}^{k}(s,a)\big{\rangle}-Q_{j,H}^{\pi}(s,a )\big{|}\leq\beta\sqrt{\phi(s,a)^{T}\left(\Lambda_{H}^{k}\right)^{-1}\phi(s,a)}\]

Thus \(Q_{j,H}^{\pi}(s,a)\leq\min\left\{\big{\langle}\phi(s,a),w_{j,H}^{k}\big{\rangle} +\beta\sqrt{\phi(s,a)^{T}\left(\Lambda_{H}^{k}\right)^{-1}\phi(s,a)},H\right\} =Q_{j,H}^{k}(s,a)\).

From the definition of \(\bar{V}_{h}^{k}\),

\[\bar{V}_{H}^{k}(s)=\max_{a}\big{[}Q_{r,H}^{k}(s,a)+\nu_{k}Q_{g,h}^{k}(s,a) \big{]}\geq\sum_{a}\pi(a\mid x)\left[Q_{r,H}^{\pi}(s,a)+\nu_{k}Q_{g,H}^{\pi}( s,a)\right]=V_{H}^{\pi,\nu_{k}}(s)\]

for any policy \(\pi\). Thus, it also holds for \(\pi^{*}\), the optimal policy. Using Lemma B.10 we can get

\[V_{H}^{\pi^{*},\nu_{k}}(s)-V_{H}^{k}(s)\leq\frac{\log M}{\alpha}+2(1+\mathscr{ V})H\rho\epsilon_{I}\sqrt{\frac{dk}{\varsigma}}\]

Now, suppose that it is true till the step \(h+1\) and consider the step \(h\). Since, it is true till step \(h+1\), thus, for any policy \(\pi\),

\[\mathbb{P}_{h}\left(V_{h+1}^{\pi,\nu_{k}}-V_{h+1}^{k}\right)(s,a)\leq(H-h) \big{(}\frac{\log M}{\alpha}+2(1+\mathscr{V})H\rho\epsilon_{I}\sqrt{\frac{dk} {\varsigma}}\big{)}\]

From (27) in Lemma 10 and the above result, we have for any \((s,a)\)

\[Q_{r,h}^{\pi}(s,a)+\nu_{k}Q_{g,h}^{\pi}(s,a)\leq Q_{r,h}^{k}(s,a)+\nu_{k}Q_{g, h}^{k}(s,a)+(H-h)\big{(}\frac{\log M}{\alpha}+2(1+\mathscr{V})H\rho\epsilon_{I} \sqrt{\frac{dk}{\varsigma}}\big{)}\]

Hence,

\[V_{h}^{\pi,\nu_{k}}(s)\leq\bar{V}_{h}^{k}(s)+(H-h)\big{(}\frac{\log M}{\alpha} +2(1+\mathscr{V})H\rho\epsilon_{I}\sqrt{\frac{dk}{\varsigma}}\big{)}\]Now, again from Lemma 11, we have \(\bar{V}_{h}^{k}(s)-V_{h}^{k}(s)\leq\frac{\log(|\mathcal{A}|)}{\alpha}\). Thus,

\[V_{h}^{\pi,\nu_{k}}(s)-V_{h}^{k}(s)\leq(H-h+1)\big{(}\frac{\log M}{\alpha}+2(1+ \mathscr{V})H\rho\epsilon_{I}\sqrt{\frac{dk}{\varsigma}}\big{)}\]

Now, since it is true for any policy \(\pi\), it will be true for \(\pi^{*}\). From the definition of \(V^{\pi,\nu_{k}}\), we have

\[\Big{(}V_{r,h}^{\pi^{*}}(s)+\nu_{k}V_{g,h}^{\pi^{*}}(s)\Big{)}-\big{(}V_{r,h}^{ k}(s)+\nu_{k}V_{g,h}^{k}(s)\big{)}\leq(H-h+1)\big{(}\frac{\log M}{\alpha}+2(1+ \mathscr{V})H\rho\epsilon_{I}\sqrt{\frac{dk}{\varsigma}}\big{)}\]

Hence, the result follows by summing over \(K\) and considering \(h=1\).

### Proof of Lemma b.7

We first define some useful sets. Let \(\mathcal{Q}_{j}=\left\{Q\mid Q(\cdot,a)=\min\left\{w_{j}^{T}\phi(\cdot,a)+ \beta\sqrt{\phi^{T}(\cdot,a)^{T}\Lambda^{-1}\phi(\cdot,a)},H\right\},\;a\in \mathcal{A}\right\}\) be the set of Q functions, where \(j\in\{r,g\}\). Since the minimum eigen value of \(\Lambda\) is no smaller than one so the Frobenius norm of \(\Lambda^{-1}\) is bounded.

Let \(\mathcal{V}_{j}=\left\{V_{j}\mid V_{j}(\cdot)=\int_{a}\pi(a\mid\cdot)Q_{j}( \cdot,a)da;Q_{r}\in\mathcal{Q}_{r},Q_{g}\in\mathcal{Q}_{g},\nu\in[0,\mathscr{V }]\right\}\) be the set of Q functions, where \(j\in\{r,g\}\). Define

\[\Pi=\left\{\pi\mid\forall a\in\mathcal{A},\pi(a\mid\cdot)=\frac{1}{\int_{b \in\mathcal{I}(a)}db}\text{SM}_{\alpha}\left(Q_{r}(\cdot,I(a))+\nu Q_{g}(\cdot,I(a))\right),\;Q_{r}\in\mathcal{Q}_{r},Q_{g}\in\mathcal{Q}_{g},\nu\in[0, \mathscr{V}]\right\}\]

the set of policies.

It's easy to verify \(V_{j}^{k}\in\mathcal{V}_{j}\).

Then we introduce the proof of Lemma b.7. To proof Lemma b.7, we need the \(\epsilon\)-covering number for the set of value functions(Lemma B.13Ghosh et al. (2022)). To achieve this, we need to show if two \(Q\) functions and the dual variable \(\nu\) are close, then the bound of policy and value function can be derived(Lemma B.11, Lemma B.12). Though the proof of Lemma B.11 and Lemma B.12 are different from Ghosh et al. (2022), we show the results remain the same, thus Lemma B.13 still holds. We'll only introduce Lemma B.13 and omit the proof.

We now proof Lemma B.11.

**Lemma B.11**.: _Let \(\pi\) be the policy of L-UCBFair corresponding to \(Q_{r}^{k}+\nu_{k}Q_{g}^{k}\), i.e.,_

\[\pi(a\mid\cdot)=\frac{1}{\int_{b\in\mathcal{I}(a)}db}\text{SM}_{\alpha}\left( Q_{r}(\cdot,I(a))+\nu Q_{g}(\cdot,I(a))\right)\] (23)

_and_

\[\tilde{\pi}(a\mid\cdot)=\frac{1}{\int_{b\in\mathcal{I}(a)}db}\text{SM}_{ \alpha}\left(\tilde{Q}_{r}(\cdot,I(a))+\tilde{\nu}\tilde{Q}_{g}(\cdot,I(a)) \right),\] (24)

_if \(\left|Q_{j}-\tilde{Q}_{j}\right|\leq\epsilon^{\prime}\) and \(\left|\nu-\tilde{\nu}\right|\leq\epsilon^{\prime}\), then \(\left|\int_{a}\left(\pi(a\mid x)-\tilde{\pi}(a\mid x)\right)da\right|\leq 2 \alpha\epsilon^{\prime}(1+\mathscr{V}+H)\)._

Proof.: \[\left|\int_{a}\left(\pi(a\mid x)-\tilde{\pi}(a\mid x)\right)da\right|\] (25) \[= \left|\sum_{i=1}^{M}\int_{a\in\mathcal{I}_{i}}\left(\pi(I(a) \mid x)-\tilde{\pi}(I(a)\mid x)\right)da\right|\] \[= \left|\sum_{i=1}^{M}\int_{b\in\mathcal{I}_{i}}db\left(\pi(I_{i} \mid x)-\tilde{\pi}(I_{i}\mid x)\right)\right|\] \[\leq \sum_{i=1}^{M}\left|SM_{\alpha}\big{(}Q_{r}(s,I_{i})+\nu Q_{g}(s, I_{i})\big{)}-SM_{\alpha}\big{(}\tilde{Q}_{r}(s,I_{i})+\tilde{\nu}\tilde{Q}_{g}(s, I_{i})\big{)}\right|\] \[\leq 2\alpha\left|Q_{r}(\cdot,I(a))+\nu Q_{g}(\cdot,I(a))-\tilde{Q}_{ r}(\cdot,I(a))-\tilde{\nu}\tilde{Q}_{g}(\cdot,I(a))\right|\] (26)The last inequatiy holds because of Theorem 4.4 in Epasto et al. (2020). Using Corollary B.17, we have

\[\left|\int_{a}\left(\pi(a\mid x)-\tilde{\pi}(a\mid x)\right)da\right|\leq 2 \alpha\epsilon^{\prime}(1+\mathscr{V}+H)\] (27)

Now since we have Lemma B.11, we can further bound the value functions.

**Lemma B.12**.: _If \(\left|\tilde{Q}_{j}-Q_{j}^{k}\right|\leq\epsilon^{\prime}\), where \(\tilde{Q}_{j}\in\mathcal{Q}_{j}\), then there exists \(\tilde{V}_{j}\in\mathcal{V}_{j}\) such that_

\[\left|V_{j}^{k}-\widetilde{V}_{j}\right|\leq H2\alpha\epsilon^{\prime}(1+ \mathscr{V}+H)+\epsilon^{\prime},\]

Proof.: For any \(x\),

\[V_{j}^{k}(s)-\widetilde{V}_{j}(s)\] \[=\left|\int_{a}\pi(a\mid s)Q_{j}^{k}(s,a)da-\int_{a}\tilde{\pi}(a \mid s)\tilde{Q}_{j}(s,a)da\right|\] \[=\left|\int_{a}\pi(a\mid s)Q_{j}^{k}(s,a)da-\int_{a}\pi(a\mid s) \tilde{Q}_{j}(s,a)da+\int_{a}\pi(a\mid s)\tilde{Q}_{j}(s,a)da-\int_{a}\tilde{ \pi}(a\mid s)\tilde{Q}_{j}(s,a)da\right|\] \[\leq\left|\int_{a}\pi(a\mid s)\left(Q_{j}^{k}(s,a)-\tilde{Q}_{j}( s,a)\right)da\right|+\left|\int_{a}\pi(a\mid s)\tilde{Q}_{j}(s,a)da-\int_{a} \tilde{\pi}(a\mid s)\tilde{Q}_{j}(s,a)da\right|\] \[\leq\epsilon^{\prime}+H\left|\int_{a}\left(\pi(a\mid s)-\tilde{ \pi}(a\mid s)\right)da\right|\] \[\leq\epsilon^{\prime}+H2\alpha\epsilon^{\prime}(1+\mathscr{V}+H)\]

Using Lemmas above, we can have the same result presented in Lemma 13 of Ghosh et al. (2022) as following.

**Lemma B.13**.: _Ghosh et al. (2022) There exists a \(\tilde{V}_{j}\in\mathcal{V}_{j}\) parameterized by \(\left(\tilde{w}_{r},\tilde{w}_{g},\tilde{\beta},\Lambda,\tilde{\mathscr{V}}\right)\) such that \(\text{dist }\left(V_{j},\tilde{V}_{j}\right)\leq\epsilon\) where_

\[\left|V_{j}-\tilde{V}_{j}\right|=\sup_{x}\left|V_{j}(s)-\tilde{V}_{r}(s)\right|.\]

_Let \(N_{\epsilon}^{V_{j}}\) be the \(\epsilon\)-covering number for the set \(\mathcal{V}_{j}\), then,_

\[\log N_{\epsilon}^{V_{j}}\leq d\log\left(1+8H\frac{\sqrt{dk}}{\sqrt{\varsigma }\epsilon^{\prime}}\right)+d^{2}\log\left[1+8d^{1/2}\beta^{2}/\left(\varsigma \left(\epsilon^{\prime}\right)^{2}\right)\right]+\log\left(1+\frac{\mathscr{V }}{\epsilon^{\prime}}\right)\]

_where \(\epsilon^{\prime}=\frac{\epsilon}{H2\alpha(1+\mathscr{V}+H)+1}\)_

**Lemma B.14**.: \(|Q_{j,h}^{k}(s,a)-Q_{j,h}^{k}(s,I(a))|\leq 2H\rho\epsilon_{I}\sqrt{\frac{dK}{ \varsigma}}\)_._

Proof.: \[\left|Q_{j,h}^{k}(s,a)-Q_{j,h}^{k}(s,I(a))\right|\] (28) \[= \left|w_{j,h}^{k}(s,a)^{T}(\phi(s,a)-\phi(s,I(a)))\right|\] (29) \[\leq \|w_{j,h}^{k}(s,a)\|_{2}\|\phi(s,a)-\phi(s,I(a))\|_{2}\] (30)

From Lemma B.16 and Assumption 3.2 we get the result.

### Prior Results

**Lemma B.15**.: _Ding et al. (2021) Let \(\nu^{*}\) be the optimal dual variable, and \(C\geq 2\nu^{*}\), then, if_

\[V_{r,1}^{\pi^{*}}\left(s_{1}\right)-V_{r,1}^{\pi}\left(s_{1}\right)+C\left[c-V_{ g,1}^{\pi}\left(s_{1}\right)\right]_{+}\leq\delta,\]

_we have_

\[\left[c-V_{g,1}^{\bar{\pi}}\left(x_{1}\right)\right]_{+}\leq\frac{2\delta}{C}.\]

**Lemma B.16**.: _Jin et al. (2020) For any \((k,h)\), the weight \(w_{j,h}^{k}\) satisfies_

\[\left\|w_{j,h}^{k}\right\|\leq 2H\sqrt{dk/\varsigma}\]

**Corollary B.17**.: _If \(\mathrm{dist}\left(Q_{r},\tilde{Q}_{r}\right)\leq\epsilon^{\prime},\mathrm{ dist}\left(Q_{g},\tilde{Q}_{g}\right)\leq\epsilon^{\prime}\), and \(\left|\tilde{\nu}_{k}-\nu_{k}\right|\leq\epsilon^{\prime}\), then, \(\mathrm{dist}\left(Q_{r}^{k}+\nu_{k}Q_{g}^{k},\tilde{Q}_{r}+\tilde{\nu}_{k} \tilde{Q}_{g}\right)\leq\epsilon^{\prime}(1+\mathscr{V}+H)\)._

[MISSING_PAGE_EMPTY:22]

Experiments

### Experiment Details

**Device and Packages.** We run all the experiment on a single 1080Ti GPU. We implement the R-TD3 agent using StableBaseline3 (Raffin et al., 2021). The neural network is implemented using Pytorch (Paszke et al., 2019). Figures were generated using code included in the supplementary material in less than 24 hours on a single Nvidia RTX A5000.

**Using a Neural Network to Learn \(\phi\).** We use a multi-layer perceptron to learn \(\phi\). Specifically, we sample 100000 data points using a random policy, storing \(s\), \(a\), \(r\) and \(g\). The inputs of the network are state and action, passing through fully connected (fc) layers with size 256, 128, 64, 64. ReLU is used as activation function between fc layers, while a SoftMax layer is applied after the last fc layer. We treat the outcome of this network as \(\phi\). To learn \(\phi\), we apply two separated fc layers (without bias) with size 1 to \(\hat{\phi}\) and treat the outputs as predicted \(r\) and predicted \(g\). A combination of MSE losses of \(r\) and \(g\) are adopted. We use Adam as the optimizer. Weight decay is set to 1e-4 and learning rate is set to 1e-3, while batch size is 128.

Note that, \(\hat{\phi}\) is linear regarding \(r\) and \(g\), but the linearity of transition kernel cannot be captured using such a schema. Therefore, equivalently we made an assumption that there always exists measure \(\mu_{h}\) such that for given \(\hat{\phi}\), the linearity of transition kernel holds. It's a stronger assumption than Assumption 3.1.

### Maximize True-Positives; Synthetic Data

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Demographic Parity & Equal Opportunity & Equalized Odds \\ \hline \hline \end{tabular}
\end{table}
Table 2: A comparison of baseline and RL policies in a purely synthetic environment with the objective of maximizing the cumulative true positive fraction of a binary classification task (Section 4), subject to three fairness constraints (Section 5.1) (columns). In all cases, the baseline, greedy policies drive the system (indicated by streamlines) to promote low qualification rates in each group (lower left corner of each plot), while the RL agents are able to drive the system to more favorable equilibria characterized by higher qualification rates (upper right). Shading represents local (noncumulative) violations of fairness.

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_EMPTY:26]

[MISSING_PAGE_EMPTY:27]

[MISSING_PAGE_EMPTY:28]

Figure 10: R-TD3 100-step sliding mean & std for the setting in Table 5.

### Reduction of Utility

Figure 11: This figure depicts the short-term impact on utility of the UCBFair algorithm compared to the greedy, myopic agent that operates without fairness constraints. In this experiment, both algorithms were designed to optimize the fraction of true-positive classifications, but only UCBFair was subject to the additional constraint of demographic parity. As the results indicate, the UCBFair algorithm experiences a reduction in utility compared to the greedy baseline, but it is able to drive the system towards a state that is preferable in the long term.