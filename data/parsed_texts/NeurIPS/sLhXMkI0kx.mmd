# On skip connections and normalisation layers in deep optimisation

Lachlan E. MacDonald

Mathematical Institute for Data Science

Johns Hopkins University

lemacdonald@protonmail.com

Jack Valmadre

Australian Institute for Machine Learning

University of Adelaide

Hemanth Saratchandran

Australian Institute for Machine Learning

University of Adelaide

Simon Lucey

Australian Institute for Machine Learning

University of Adelaide

Most done while at the Australian Institute for Machine Learning, University of Adelaide

###### Abstract

We introduce a general theoretical framework, designed for the study of gradient optimisation of deep neural networks, that encompasses ubiquitous architecture choices including batch normalisation, weight normalisation and skip connections. Our framework determines the curvature and regularity properties of multilayer loss landscapes in terms of their constituent layers, thereby elucidating the roles played by normalisation layers and skip connections in globalising these properties. We then demonstrate the utility of this framework in two respects. First, we give the only proof of which we are aware that a class of deep neural networks can be trained using gradient descent to global optima even when such optima only exist at infinity, as is the case for the cross-entropy cost. Second, we identify a novel causal mechanism by which skip connections accelerate training, which we verify predictively with ResNets on MNIST, CIFAR10, CIFAR100 and ImageNet.

## 1 Introduction

Deep, overparameterised neural networks are efficiently trainable to global optima using simple first order methods. That this is true is immensely surprising from a theoretical perspective: modern datasets and deep neural network architectures are so complex and irregular that they are essentially opaque from the perspective of classical (convex) optimisation theory. A recent surge in inspired theoretical works [22, 14, 13, 1, 52, 51, 36, 28, 29, 33, 32] has elucidated this phenomenon, showing linear convergence of gradient descent on certain classes of neural networks, with certain cost functions, to global optima. The formal principles underlying these works are identical. By taking width sufficiently large, one guarantees uniform bounds on _curvature_ (via a Lipschitz gradients-type property) and _regularity_ (via a Polyak-Lojasiewicz-type inequality) in a neighbourhood of initialisation. Convergence to a global optimum in that neighbourhood then follows from a well-known chain of estimates [23].

Despite significant progress, the theory of deep learning optimisation extant in the literature presents at least three significant shortcomings:1. _It lacks a formal framework in which to compare common practical architecture choices_. Indeed, none of the aforementioned works consider the impact of ubiquitous (weight/batch) normalisation layers. Moreover, where common architectural modifications such as skip connections _are_ studied, it is unclear exactly what impact they have on optimisation. For instance, while in [13] it is shown that skip connections enable convergence with width polynomial in the number of layers, as compared with exponential width for chain networks, in [1] polynomial width is shown to be sufficient for convergence of both chain and residual networks.

2. _It lacks theoretical flexibility_. The consistent use of _uniform_ curvature and regularity bounds are insufficiently flexible to enable optimisation guarantees too far away from initialisation, where the local, uniform bounds used in previous theory no longer hold. In particular, proving globally optimal convergence for deep neural nets with the cross-entropy cost was (until now) an open problem [5].

3. _It lacks practical utility_. Although it is presently unreasonable to demand quantitatively predictive bounds on practical performance, existing optimisation theory has been largely unable to inform architecture design even qualitatively. This is in part due to the first item, since practical architectures typically differ substantially from those considered for theoretical purposes.

Our purpose in this article is to take a step in addressing these shortcomings. Specifically:

1. We provide a formal framework, inspired by [45], for the study of multilayer optimisation. Our framework is sufficiently general to include all commonly used neural network layers, and contains formal results relating the curvature and regularity properties of multilayer loss landscapes to those of their constituent layers. As instances, we prove novel results on the _global_ curvature and regularity properties enabled by normalisation layers and skip connections respectively, in contrast to the _local_ bounds provided in previous work.

2. Using these novel, global bounds, we identify a class of weight-normalised residual networks for which, given a linear independence assumption on the data, gradient descent can be provably trained to a global optimum arbitrarily far away from initialisation. From a regularity perspective, our analysis is _strictly more flexible_ than the uniform analysis considered in previous works, and in particular solves the open problem of proving global optimality for the training of deep nets with the cross-entropy cost.

3. Using our theoretical insight that skip connections aid loss regularity, we conduct a systematic empirical analysis of singular value distributions of layer Jacobians for practical layers. We are thereby able to predict that simple modifications to the classic ResNet architecture [20] will improve training speed. We verify our predictions on MNIST, CIFAR10, CIFAR100 and ImageNet.

## 2 Background

In this section we give a summary of the principles underlying recent theoretical advances in neural network optimisation. We discuss related works _after_ this summary for greater clarity.

### Smoothness and the PL-inequality

Gradient descent on a possibly non-convex function \(\ell:\mathbb{R}^{p}\to\mathbb{R}_{\geq 0}\) can be guaranteed to converge to a global optimum by insisting that \(\ell\) have _Lipschitz gradients_ and satisfy the _Polyak-Lojasiewicz inequality_. We recall these well-known properties here for convenience.

**Definition 2.1**.: Let \(\beta>0\). A continuously differentiable function \(\ell:\mathbb{R}^{p}\to\mathbb{R}\) is said to have \(\beta\)**-Lipschitz gradients**, or is said to be \(\beta\)**-smooth** over a set \(S\subset\mathbb{R}^{p}\) if the vector field \(\nabla\ell:\mathbb{R}^{p}\to\mathbb{R}^{p}\) is \(\beta\)-Lipschitz. If \(S\) is convex, \(\ell\) having \(\beta\)-Lipschitz gradients implies that the inequality

\[\ell(\theta_{2})-\ell(\theta_{1})\leq\nabla\ell(\theta_{1})^{T}(\theta_{2}- \theta_{1})+\frac{\beta}{2}\|\theta_{2}-\theta_{1}\|^{2}\] (1)

holds for all \(\theta_{1},\theta_{2}\in S\).

The \(\beta\)-smoothness of \(\ell\) over \(S\) can be thought of as a uniform bound on the _curvature_ of \(\ell\) over \(S\): if \(\ell\) is twice continuously differentiable, then it has Lipschitz gradients over any compact set \(K\) with (possibly loose) Lipschitz constant given by

\[\beta:=\sup_{\theta\in K}\|D^{2}\ell(\theta)\|,\] (2)

where \(D^{2}\ell\) is the Hessian and \(\|\cdot\|\) denotes any matrix norm.

**Definition 2.2**.: Let \(\mu>0\). A differentiable function \(\ell:\mathbb{R}^{p}\to\mathbb{R}_{\geq 0}\) is said to satisfy the \(\mu\)**-Polyak-Lojasiewicz inequality**, or is said to be \(\mu\)-PL over a set \(S\subset\mathbb{R}^{p}\) if

\[\|\nabla\ell(\theta)\|^{2}\geq\mu\bigg{(}\ell(\theta)-\inf_{\theta^{\prime} \in S}\ell(\theta^{\prime})\bigg{)}\] (3)

for all \(\theta\in S\).

The PL condition on \(\ell\) over \(S\) is a uniform guarantee of _regularity_, which implies that all critical points of \(\ell\) over \(S\) are \(S\)-global minima; however, such a function _need not be convex_. Synthesising these definitions leads easily to the following result (cf. Theorem 1 of [23]).

**Theorem 2.3**.: _Let \(\ell:\mathbb{R}^{p}\to\mathbb{R}_{\geq 0}\) be a continuously differentiable function that is \(\beta\)-smooth and \(\mu\)-PL over a convex set \(S\). Suppose that \(\theta_{0}\in S\) and let \(\{\theta_{t}\}_{t=0}^{\infty}\) be the trajectory taken by gradient descent, with step size \(\eta<2\beta^{-1}\), starting at \(\theta_{0}\). If \(\{\theta_{t}\}_{t=0}^{\infty}\subset S\), then \(\ell(\theta_{t})\) converges to an \(S\)-global minimum \(\ell^{*}\) at a linear rate:_

\[\ell(\theta_{t})-\ell^{*}\leq\bigg{(}1-\mu\eta\bigg{(}1-\frac{\beta\eta}{2} \bigg{)}\bigg{)}^{t}\big{(}\ell(\theta_{0})-\ell^{*}\big{)}\] (4)

_for all \(t\in\mathbb{N}\). _

Essentially, while the Lipschitz constant of the gradients controls whether or not gradient descent with a given step size can be guaranteed to _decrease_ the loss at each step, the PL constant determines _by how much_ the loss will decrease. These ideas can be applied to the optimisation of deep neural nets as follows.

### Application to model optimisation

The above theory can be applied to parameterised models in the following fashion. Let \(f:\mathbb{R}^{p}\times\mathbb{R}^{d_{0}}\to\mathbb{R}^{d_{L}}\) be a differentiable, \(\mathbb{R}^{p}\)-parameterised family of functions \(\mathbb{R}^{d_{0}}\to\mathbb{R}^{d_{L}}\) (in later sections, \(L\) will denote the number of layers of a deep neural network). Given \(N\) training data \(\{(x_{i},y_{i})\}_{i=1}^{N}\subset\mathbb{R}^{d_{0}}\times\mathbb{R}^{d_{L}}\), let \(F:\mathbb{R}^{p}\to\mathbb{R}^{d_{L}\times N}\) be the corresponding parameter-function map defined by

\[F(\theta)_{i}:=f(\theta,x_{i}).\] (5)

Any differentiable cost function \(c:\mathbb{R}^{d_{L}}\times\mathbb{R}^{d_{L}}\to\mathbb{R}_{\geq 0}\), convex in the first variable, extends to a differentiable, convex function \(\gamma:\mathbb{R}^{d_{L}\times N}\to\mathbb{R}_{\geq 0}\) defined by

\[\gamma\big{(}(z_{i})_{i=1}^{N}\big{)}:=\frac{1}{N}\sum_{i=1}^{N}c(z_{i},y_{i}),\] (6)

and one is then concerned with the optimisation of the composite \(\ell:=\gamma\circ F:\mathbb{R}^{p}\to\mathbb{R}_{\geq 0}\) via gradient descent.

To apply Theorem 2.3, one needs to determine the smoothness and regularity properties of \(\ell\). By the chain rule, the former can be determined given sufficient conditions on the derivatives \(D\gamma\circ F\) and \(DF\) (cf. Lemma 2 of [45]). The latter can be bounded by Lemma 3 of [45], which we recall below and prove in the appendix for the reader's convenience.

**Theorem 2.4**.: _Let \(S\subset\mathbb{R}^{p}\) be a set. Suppose that \(\gamma:\mathbb{R}^{d_{L}\times N}\to\mathbb{R}_{\geq 0}\) is \(\mu\)-PL over \(F(S)\) with minimum \(\gamma_{S}^{*}\). Let \(\lambda(DF(\theta))\) denote the smallest eigenvalue of \(DF(\theta)DF(\theta)^{T}\). Then_

\[\|\nabla\ell(\theta)\|^{2}\geq\mu\,\lambda(DF(\theta))\,\big{(}\ell(\theta)- \gamma_{S}^{*}\big{)}\] (7)

_for all \(\theta\in S\). _

Note that Theorem 2.4 is vacuous (\(\lambda(DF(\theta))=0\) for all \(\theta\)) unless in the overparameterised regime (\(p\geq d_{L}N\)). Even in this regime, however, Theorem 2.4 does not imply that \(\ell\) is PL unless \(\lambda(\theta)\) can be uniformly lower bounded by a positive constant over \(S\). Although universally utilised in previous literature, such a uniform lower bound will not be possible in our _global_ analysis, and our convergence theorem _does not_ follow from Theorem 2.3, in contrast to previous work. Our theorem requires additional argumentation, which we believe may be of independent utility.

Related works

Convergence theorems for deep _linear_ networks with the square cost are considered in [4, 2]. In [22], it is proved that the tangent kernel of a multi-layer perceptron (MLP) becomes approximately constant over all of parameter space as width goes to infinity, and is positive-definite for certain data distributions, which by Theorem 2.4 implies that all critical points are global minima. Strictly speaking, however, [22] does not prove convergence of gradient descent: the authors consider only gradient flow, and leave Lipschitz concerns untouched. The papers [14, 13, 1, 52, 51, 36] prove that overparameterized neural nets of varying architectures can be optimised to global minima _close to initialisation_ by assuming sufficient width of several layers. While [1] does consider the cross-entropy cost, convergence to a global optimum is _not_ proved: it is instead shown that perfect classification accuracy can be achieved close to initialisation during training. Improvements on these works have been made in [33, 32, 7], wherein large width is required of only a single layer.

It is identified in [28] that linearity of the final layer is key in establishing the approximate constancy of the tangent kernel for wide networks that was used in [14, 13]. By making explicit the implicit use of the PL condition present in previous works [14, 13, 1, 52, 51, 36], [29] proves a convergence theorem even with _nonlinear_ output layers. The theory explicated in [29] is formalised and generalised in [45]. A key weakness of all of the works mentioned thus far (bar the purely formal [45]) is that their hypotheses imply that optimisation trajectories are always close to initialisation. Without this, there is no obvious way to guarantee the PL-inequality along the optimisation trajectory, and hence no way to guarantee one does not converge to a suboptimal critical point. However such training is not possible with the cross-entropy cost, whose global minima only exist at infinity. There is also evidence to suggest that such training must be avoided for state-of-the-art test performance [8, 15, 26]. In contrast, our theory gives convergence guarantees even for trajectories that travel arbitrarily far from initialisation, and is the only work of which we are aware that can make this claim.

Among the tools that make our theory work are skip connections [20] and weight normalisation [41]. The smoothness properties of normalisation schemes have previously been studied [42, 40], however they only give pointwise estimates comparing normalised to non-normalised layers, and do not provide a global analysis of Lipschitz properties as we do. The regularising effect of skip connections on the loss landscape has previously been studied in [35], however this study is not tightly linked to optimisation theory. Skip connections have also been shown to enable the interpretation of a neural network as coordinate transformations of data manifolds [19]. Mean field analyses of skip connections have been conducted in [49, 30] which necessitate large width; our own analysis does not. A similarly general framework to that which we supply is given in [47, 48]; while both encapsulate all presently used architectures, that of [47, 48] is designed for the study of infinite-width tangent kernels, while ours is designed specifically for optimisation theory. Our empirical singular value analysis of skip connections complements existing theoretical work using random matrix theory [18, 37, 39, 34, 16]. These works have not yet considered the shifting effect of skip connections on layer Jacobians that we observe empirically.

Our theory also links nicely to the intuitive notions of gradient propagation [20] and dynamical isometry already present in the literature. In tying Jacobian singular values rigorously to loss regularity in the sense of the Polyak-Lojasiewicz inequality, our theory provides a new link between dynamical isometry and optimisation theory [43, 38, 46]: specifically dynamical isometry ensures better PL conditioning and therefore faster and more reliable convergence to global minima. In linking this productive section of the literature to optimisation theory, our work may open up new possibilities for convergence proofs in the optimisation theory of deep networks. We leave further exploration of this topic to future work.

Due to this relationship with the notion of dynamical isometry, our work also provides optimisation-theoretic support for the empirical analyses of [24, 6] that study the importance of layerwise dynamical isometry for trainability and neural architecture search [27, 44, 25]. Recent work on deep kernel shaping shows via careful tuning of initialisation and activation functions that while skip connections and normalisation layers may be _sufficient_ for good trainability, they are not _necessary_[31, 50]. Other recent work has also shown benefits to inference performance by removing skip connections from the trained model using a parameter transformation [11] or by removing them from the model altogether and incorporating them only into the optimiser [12].

Finally, a line of work has recently emerged on training in the more realistic, large learning rate regime known as "edge of stability" [9; 3; 10]. This intriguing line of work diverges from ours, and its integration into the framework we present is a promising future research direction.

## 4 Formal framework

In this section we will define our theoretical framework. We will use \(\mathbb{R}^{n\times m}\) to denote the space of \(n\times m\) matrices, and vectorise rows first. We use \(\text{Id}_{m}\) to denote the \(m\times m\) identity matrix, and \(1_{n}\) to denote the vector of ones in \(\mathbb{R}^{n}\). We use \(\otimes\) to denote the Kronecker (tensor) product of matrices, and given a vector \(v\in\mathbb{R}^{n}\) we use \(\text{diag}(v)\) to denote the \(n\times n\) matrix whose leading diagonal is \(v\). Given a matrix \(A\in\mathbb{R}^{n\times m}\), and a seminorm \(\|\cdot\|\) on \(\mathbb{R}^{m}\), \(\|A\|_{row}\) will be used to denote the vector of \(\|\cdot\|\)-seminorms of each row of \(A\). The smallest singular value of \(A\) is denoted \(\sigma(A)\), and the smallest eigenvalue of \(AA^{T}\) denoted \(\lambda(A)\). Full proofs of all of our results can be found in the appendix.

Our theory is derived from the following formal generalisation of a deep neural network.

**Definition 4.1**.: By a **multilayer parameterised system (MPS)** we mean a family \(\{f_{l}:\mathbb{R}^{p_{l}}\times\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{ l}\times N}\}_{l=1}^{L}\) of functions. Given a data matrix \(X\in\mathbb{R}^{d_{0}\times N}\), we denote by \(F:\mathbb{R}^{\sum_{i=1}^{L}p_{i}}\to\mathbb{R}^{d_{L}\times N}\) the **parameter-function map2** defined by

Footnote 2: \(\mathbb{R}^{d_{L}\times N}\) is canonically isomorphic to the space of \(\mathbb{R}^{d_{L}}\)-valued functions on an \(N\)-point set.

\[F(\vec{\theta}):=f_{L}(\theta_{L})\circ\cdots\circ f_{1}(\theta_{1})(X),\] (8)

for \(\vec{\theta}=(\theta_{1},\ldots,\theta_{L})^{T}\in\mathbb{R}^{\sum_{i=1}^{L} p_{i}}\).

Definition 4.1 is sufficiently general to encompass all presently used neural network layers, but _we will assume without further comment from here on in that all layers are continuously differentiable_. Before we give examples, we record the following result giving the form of the derivative of the parameter-function map, which follows easily from the chain rule. It will play a key role in the analysis of the following subsections.

**Proposition 4.2**.: _Let \(\{f_{l}:\mathbb{R}^{p_{l}}\times\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{ l}\times N}\}_{l=1}^{L}\) be a MPS, and \(X\in\mathbb{R}^{d_{0}\times N}\) a data matrix. For \(1\leq l\leq L\), denote the derivatives of \(f_{l}\) with respect to the \(\mathbb{R}^{p_{l}}\) and \(\mathbb{R}^{d_{l-1}}\) variables by \(Df_{l}\) and \(Jf_{l}\) respectively, and let \(f_{<l}(\vec{\theta},X)\) denote the composite_

\[f_{<l}(\vec{\theta},X):=f_{l-1}(\theta_{l-1})\circ\cdots\circ f_{1}(\theta_{1} )(X).\] (9)

_The derivative \(DF\) of the associated parameter-function map is given by_

\[DF(\vec{\theta})=\big{(}D_{\theta_{1}}F(\vec{\theta}),\ldots,D_{\theta_{L}}F( \vec{\theta})\big{)},\] (10)

_where for \(1\leq l<L\), \(D_{\theta_{l}}F(\vec{\theta})\) is given by the formula_

\[\Bigg{(}\prod_{j=l+1}^{L}Jf_{j}\big{(}\theta_{j},f_{<j}(\vec{\theta},X)\big{)} \Bigg{)}Df_{l}\big{(}\theta_{l},f_{<l}(\vec{\theta},X)\big{)},\] (11)

_with the product taken so that indices are arranged in descending order from left to right. _

All common differentiable neural network layers fit into this framework; we record some examples in detail in the appendix. We will see in the next section that insofar as one wishes to guarantee global smoothness, the usual parameterisation of affine layers is poor, although this defect can be ameliorated to differing extents by normalisation strategies.

### Smoothness

In this subsection, we give sufficient conditions for the derivative of the parameter-function map of a MPS to be bounded and Lipschitz. We are thereby able to give sufficient conditions for any associated loss function to have Lipschitz gradients on its sublevel sets. We begin with a formal proposition that describes the Lipschitz properties of the derivative of the parameter-function map of a MPS in terms of those of its constituent layers.

**Proposition 4.3**.: _Let \(\{f_{l}:\mathbb{R}^{p_{l}}\times\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{l} \times N}\}_{l=1}^{L}\), and let \(\{S_{l}\subset\mathbb{R}^{p_{l}}\}_{l=1}^{L}\) be subsets of the parameter spaces. Suppose that for each bounded set \(B_{l}\subset\mathbb{R}^{d_{l-1}\times N}\), the maps \(f_{l}\), \(Df_{l}\) and \(Jf_{l}\) are all bounded and Lipschitz on \(S_{l}\times B_{l}\). Then for any data matrix \(X\in\mathbb{R}^{d_{0}\times N}\), the derivative \(DF\) of the associated parameter-function map \(F:\mathbb{R}^{p}\to\mathbb{R}^{d_{L}\times N}\) is bounded and Lipschitz on \(S:=\prod_{j=1}^{L}S_{j}\subset\mathbb{R}^{p}\). _

Proposition 4.3 has the following immediate corollary, whose proof follows easily from Lemma B.1.

**Corollary 4.4**.: _Let \(c:\mathbb{R}^{d_{L}}\to\mathbb{R}\) be any cost function whose gradient is bounded and Lipschitz on its sublevel sets \(Z_{\alpha}:=\{z\in\mathbb{R}^{d_{L}}:c(z)\leq\alpha\}\). If \(\{f_{l}:\mathbb{R}^{p_{l}}\times\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{ l}\times N}\}_{l=1}^{L}\) is any MPS satisfying the hypotheses of Proposition 4.3 and \(X\in\mathbb{R}^{d_{0}\times N}\), then the associated loss function \(\ell:=\gamma\circ F\) has Lipschitz gradients over \(S:=\prod_{l=1}^{L}S_{l}\). _

The most ubiquitous cost functions presently in use (the mean square error and cross entropy functions) satisfy the hypotheses of Corollary 4.4. We now turn to an analysis of common layer types in deep neural networks and indicate to what extent they satisfy the hypotheses of Proposition 4.3.

**Theorem 4.5**.: _Fix \(\epsilon>0\). The following layers satisfy the hypotheses of Proposition 4.3 over all of parameter space._

1. _Continuously differentiable nonlinearities._
2. _Bias-free_ \(\epsilon\)_-weight-normalised or_ \(\epsilon\)_-entry-normalised affine layers_3_._

Footnote 3: The theorem also holds with normalised biases. We make the bias-free assumption assumption purely out of notational convenience.

_3. Any residual block whose branch is a composite of any of the above layer types._

_Consequently, the loss function of any neural network composed of layers as above, trained with a cost function satisfying the hypotheses of Corollary 4.4, has globally Lipschitz gradients along any sublevel set. _

The proof we give of Theorem 4.5 also considers composites bn \(\circ\) aff of batch norm layers with affine layers. Such composites satisfy the hypotheses of Proposition 4.3 only over sets in data-space \(\mathbb{R}^{d\times N}\) which consist of matrices with nondegenerate covariance. Since such sets are generic (probability 1 with respect to any probability measure that is absolutely continuous with respect to Lebesgue measure), batch norm layers satisfy the hypotheses of Proposition 4.3 with high probability over random initialisation.

Theorem 4.5 says that normalisation of parameters enables _global_ analysis of the loss, while standard affine layers, due to their unboundedness, are well-suited to analysis only over bounded sets in parameter space.

### Regularity

Having examined the Lipschitz properties of MPS and given examples of layers with _global_ Lipschitz properties, let us now do the same for the _regularity_ properties of MPS. Formally one has the following simple result.

**Proposition 4.6**.: _Let \(\{f_{l}:\mathbb{R}^{p_{l}}\times\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{l} \times N}\}_{i=1}^{L}\) be a MPS and \(X\in\mathbb{R}^{d_{0}\times N}\) a data matrix. Then_

\[\sum_{l=1}^{L}\lambda\big{(}Df_{l}\big{(}\theta_{l},f_{<l}(\vec{\theta},X) \big{)}\big{)}\prod_{j=l+1}^{L}\lambda\big{(}Jf_{j}(\theta_{j},f_{<j}(\vec{ \theta},X))\big{)}\] (12)

_is a lower bound for \(\lambda\big{(}DF(\vec{\theta})\big{)}\). _

Proposition 4.6 tells us that to guarantee good regularity, it suffices to guarantee good regularity of the constituent layers. In fact, since Equation (12) is a sum of non-negative terms, it suffices merely to guarantee good regularity of the parameter-derivative of the _first_ layer4, and of the input-output Jacobians of every _subsequent_ layer. Our next theorem says that residual networks with appropriately normalised branches suffice for this.

Footnote 4: The parameter derivatives of higher layers are more difficult to analyse, due to nonlinearities.

**Theorem 4.7**.: _Let \(\{g_{l}:\mathbb{R}^{p_{l}}\times\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{ l}\times N}\}_{i=1}^{L}\) be a MPS and \(X\in\mathbb{R}^{d_{0}\times N}\) a data matrix for which the following hold:_1. \(d_{l-1}\geq d_{l}\), and \(\|Jg_{l}(\theta_{l},Z)\|_{2}<1\) for all \(\theta_{l}\in\mathbb{R}^{p_{l}}\), \(Z\in\mathbb{R}^{d_{l-1}\times N}\) and \(l\geq 2\).
2. \(N\leq d_{0}\), \(X\) is full rank, \(p_{1}\geq d_{1}d_{0}\) and \(f_{1}:\mathbb{R}^{p_{1}}\times\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{1} \times N}\) is a \(P\)-parameterised affine layer, for which \(DP(\theta_{1})\) is full rank for all \(\theta_{1}\in\mathbb{R}^{p_{1}}\).

For any sequence \(\{I_{l}:\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{l}\times N}\}_{l=2}^{L}\) of linear maps whose singular values are all equal to 1, define a new MPS \(\{f_{l}\}_{l=1}^{L}\) by \(f_{1}:=g_{1}\), and \(f_{l}(\theta_{l},X):=I_{l}X+g_{l}(\theta_{l},X)\). Let \(F:\mathbb{R}^{p}\to\mathbb{R}^{d_{L}\times N}\) be the parameter-function map associated to \(\{f_{l}\}_{l=1}^{L}\) and \(X\). Then \(\lambda(DF(\vec{\theta}))>0\) (but not uniformly so) for all \(\vec{\theta}\in\mathbb{R}^{p}\). 

In the next and final subsection we will synthesise Theorem 4.5 and Theorem 4.7 into our main result: a global convergence theorem for gradient descent on appropriately normalised residual networks.

## 5 Main theorem

We will consider _normalised residual networks_, which are MPS of the following form. The first layer is an \(\epsilon\)-entry-normalised, bias-free affine layer \(f_{1}=\text{aff}_{\text{en}}:\mathbb{R}^{d_{1}\times d_{0}}\times\mathbb{R}^{ d_{0}\times N}\to\mathbb{R}^{d_{1}\times N}\) (cf. Example A.1). Every subsequent layer is a residual block

\[f_{l}(\theta_{l},X)=I_{l}X+g(\theta_{l},X).\] (13)

Here, for all \(l\geq 2\), we demand that \(d_{l-1}\geq d_{l}\), \(I_{i}:\mathbb{R}^{d_{l-1}\times N}\to\mathbb{R}^{d_{i}\times N}\) is some linear map with all singular values equal to 1, and the residual branch \(g(\theta_{i},X)\) is a composite of weight- or entry-normalised, bias-free5 affine layers \(\text{aff}_{P}\) (cf. Example A.1), rescaled so that \(\|P(w)\|_{2}<1\) uniformly for all parameters \(w\), and elementwise nonlinearities \(\Phi\) for which \(\|D\phi\|\leq 1\) everywhere (Example A.2). These hypotheses ensure that Theorem 4.7 holds for \(\{f_{i}\}_{i=1}^{L}\) and \(X\): see the Appendix for a full proof.

Footnote 5: The theorem also holds with normalised biases.

We emphasise again that our main theorem below does _not_ follow from the usual argumentation using smoothness and the PL inequality, due to the lack of a _uniform_ PL bound. Due to the novelty of our technique, which we believe may be of wider utility where uniform regularity bounds are unavailable, we include an idea of the proof below.

**Theorem 5.1**.: _Let \(\{f_{i}\}_{i=1}^{L}\) be a normalised residual network; \(X\) a data matrix of linearly independent data, with labels \(Y\); and \(c\) any continuously differentiable, convex cost function. Then there exists a learning rate \(\eta>0\) such that gradient descent on the associated loss function converges from **any** initialisation to a global minimum._

Idea of proof.: One begins by showing that Theorems 4.5 and 4.7 apply to give globally \(\beta\)-Lipschitz gradients and a positive smallest eigenvalue of the tangent kernel at all points in parameter space. Thus for learning rate \(\eta<2\beta^{-1}\), there exists a positive sequence \((\mu_{t}=\mu\lambda(DF(\theta_{t})))_{t\in\mathbb{N}}\) (see Theorem 2.4) such that \(\|\nabla\ell_{t}\|^{2}\geq\mu_{t}\ell_{t}\), for which the loss iterates \(\ell_{t}\) therefore obey

\[\ell_{t}-\ell^{*}\leq\prod_{i=0}^{t}(1-\mu_{i}\alpha)(\ell_{0}-\ell^{*}),\]

where \(\alpha=\eta(1-2\beta^{-1}\eta)>0\). To show global convergence one must show that \(\prod_{t=0}^{\infty}(1-\mu_{t}\alpha)=0\).

If \(\mu_{t}\) can be uniformly lower bounded (e.g. for the square cost) then Theorem 2.3 applies to give convergence as in all previous works. However, \(\mu_{t}\)_cannot_ be uniformly lower bounded in general (e.g. for the cross-entropy cost). We attain the general result by showing that, despite admitting no non-trivial lower bound in general, \(\mu_{t}\) can always be guaranteed to vanish _sufficiently slowly_ that global convergence is assured. 

We conclude this section by noting that practical deep learning problems typically do not satisfy the hypotheses of Theorem 5.1: frequently there are more training data than input dimensions (such as for MNIST and CIFAR), and many layers are not normalised or skip-connected. Moreover our Lipschitz bounds are worst-case, and will generally lead to learning rates much smaller than are used in practice. Our strong hypotheses are what enable a convergence guarantee from any initialisation, whereas in practical settings initialisation is of key importance. Despite the impracticality of Theorem 5.1, in the next section we show that the ideas that enable its proof nonetheless have practical implications.

Practical implications

Our main theorem is difficult to test directly, as it concerns only worst-case behaviour which is typically avoided in practical networks which do not satisfy its hypotheses. However, our framework more broadly nonetheless yields practical insights. Informally, Theorem 4.7 gives conditions under which:

_skip connections aid optimisation by improving loss regularity._

In this section, we conduct an empirical analysis to demonstrate that _this insight holds true even in practical settings_, and thereby obtain a novel, _causal_ understanding of the benefits of skip connections in practice. With this causal mechanism in hand, we recommend simple architecture changes to practical ResNets that consistently (albeit modestly) improve convergence speed as predicted by theory. All code is available at https://github.com/lemacdonald/skip-connections-normalisation/.

### Singular value distributions

Let us again recall the setting \(\ell=\gamma\circ F\) of Theorem 2.4. In the overparameterised setting, the smallest singular value of \(DF\) gives a _pessimistic lower bound_ on the ratio \(\|\nabla\ell\|^{2}/(\ell-\ell^{*})\), and hence a _pessimistic_ lower bound on training speed (cf. Theorems 2.4, 2.3). Indeed, this lower bound is only attained when the vector \(\nabla\gamma\) perfectly aligns with the smallest singular subspace of \(DF\): a probabilistically unlikely occurence. In general, since \(\nabla\ell=DF^{T}\,\nabla\gamma\), _the entire singular value distribution_ of \(DF\) at a given parameter will play a role in determining the ratio \(\|\nabla\ell\|^{2}/(\ell-\ell^{*})\), and hence training speed.

Now \(DF\) is partly determined by products of layer Jacobians (cf. Proposition 4.2). As such, the distribution of singular values of \(DF\) is determined in part by the distribution of singular values of such layer Jacobian products, which are themselves determined by the singular value distributions of each of the individual layer Jacobians. In particular, if through some architectural intervention each layer Jacobian could have its singular value distribution shifted upwards, we would expect the singular value distribution of \(DF\) to be shifted upwards, too. Our argument in the previous paragraph then suggests that such an intervention will result in faster training, provided of course that the upwards-shifting is not so large as to cause exploding gradients.

Figure 1 shows in the _linear_ setting that a skip connection constitutes precisely such an intervention. We hypothesise that this continues to hold even in the nonlinear setting.

**Hypothesis 6.1**.: _The addition of a deterministic skip connection, all of whose singular values are 1, across a composite of possibly nonlinear random layers, shifts upwards the singular value distribution of the corresponding composite Jacobian6, thereby improving convergence speed at least initially._

Footnote 6: At least for some common initialisation schemes.

We test Hypothesis 6.1 in the next subsections.

### Mnist

Recall that the ResNet architecture [20] consists in part of a composite of residual blocks

\[f(\theta,A,X)=AX+g(\theta,X),\] (14)

where \(A\) is either the identity transformation, in the case when the dimension of the output of \(g(\theta,X)\) is the same as the dimension of its input; or a randomly initialised 1x1 convolution otherwise.

Figure 1: Singular value histogram of \(500\times 500\) matrix \(A\) (left) with entries sampled iid from \(U(-1/\sqrt{500},1/\sqrt{500})\). Adding an identity matrix (right) shifts the distribution upwards.

Hypothesis 6.1 predicts that the additions of the identity skip connections will shift upwards the singular value distributions of composite layer Jacobians relative to the equivalent chain network, thus improving convergence speed. It also predicts that replacing 1x1 convolutional skip connections \(A\) with \(I+A\), where \(I\) is deterministic with all singular values equal to 1, will do the same.

We first test this hypothesis by doing gradient descent, with a learning rate of 0.1, on 32 randomly chosen data points from the MNIST training set7. We compare three models, all with identical initialisations for each trial run using the default PyTorch initialisation:

Footnote 7: The input-output Jacobians of MPS layers scale with the square of the number of training points, making their computation with a large number of data points prohibitively expensive computationally.

1. (Chain) A batch-normed convolutional chain network, with six convolutional layers.
2. (Res) The same as (1), but with convolutional layers 2-3 and 5-6 grouped into two residual blocks, and with an additional 1x1 convolution as the skip connection in the second residual block, as in Equation (14).
3. (ResAvg) The same as (2), but with the 1x1 convolutional skip connection of Equation (14) replaced by

\[\tilde{f}(\theta,A,X)=(I+A)X+g(\theta,X),\] (15)

where \(I\) is an average pool, rescaled to have all singular values equal to 1.

Hypothesis 6.1 predicts that the singular value distributions of the composite layer Jacobians will be more positively shifted going down the list, resulting in faster convergence. This is indeed what we observe (Figure 2).

### CIFAR and ImageNet

We now test Hypothesis 6.1 on CIFAR and ImageNet. We replace all of the convolution-skip connections in the ResNet architecture [20] with sum(average pool, convolution)-skip connections as in Equation (15) above, leaving all else unchanged. Hypothesis 6.1 predicts that these modifications will improve training speed, which we verify using default PyTorch initialisation schemes.

We trained PreAct-ResNet18 on CIFAR10/100 and PreAct-ResNet50 [20] on ImageNet using standard training regimes (details can be found in the appendix), performing respectively 10 and 3 trial runs

Figure 3: Mean training loss curves for ResNets on CIFAR (10 trials) and ImageNet (3 trials), with 1 standard deviation shaded. The modifications improve training speed as predicted.

Figure 2: (a)-(c) Singular value histograms of composite layer Jacobians averaged over first 10 training iterations. Distributions are shifted upwards as deterministic skip connections are added, resulting in faster convergence (d). Means over 10 trials shown.

on CIFAR and ImageNet. We performed the experiments at a range of different learning rates. We have included figures for the best performing learning rates on the _original_ model (measured by loss value averaged over the final epoch) in Figure 3, with additional plots and validation accuracies in the appendix. Validation accuracies were not statistically significantly different between the two models on CIFAR10/100. Although the modified version had statistically significantly better validation accuracy in the ImageNet experiment, we believe this is only due to the faster convergence, as the training scheme was not sufficient for the model to fully converge.

## 7 Discussion

Our work suggests some open research problems. First, the recently-developed edge of stability theory [9] could be used in place of our Lipschitz bounds to more realistically characterise training of practical nets with large learning rates. Second, like in pervious works [7], the heavy-lifting for our PL-type bounds is all done by the parameter-derivative of a single layer, and the bounds would be significantly improved by an analysis that considers all layers. Third, extension of the theory to SGD is desirable. Fourth, the dependence of Hypothesis 6.1 on weight variance should be investigated. Fifth, our empirical results on the impact of skip connections on singular value distributions suggests future work using random matrix theory [18].

Beyond these specifics, our formal framework provides a setting in which all neural network layers can be analysed in terms of their effect on the key loss landscape properties of smoothness and regularity, and is the first to demonstrate that a _uniform_ bound on regularity is not necessary to prove convergence. We hope the tools we provide in this paper will be of use in extending deep learning optimisation theory to more practical settings than has so far been the case.

## 8 Conclusion

We gave a formal theoretical framework for studying the optimisation of multilayer systems. We used the framework to give the first proof that a class of deep neural networks can be trained by gradient descent even to global optima at infinity. Our theory generates the novel insight that skip connections aid optimisation speed by improving loss regularity, which we verified empirically using practical datasets and architectures.

## 9 Acknowledgements

We thank the anonymous reviewers for their time reviewing the manuscript. Their critiques helped to improve the paper.

## References

* [1] Z. Allen-Zhu, Y. Li, and Z. Song. A Convergence Theory for Deep Learning via Over-Parameterization. In _ICML_, pages 242-252, 2019.
* [2] S. Arora, N. Cohen, N. Golowich, and W. Hu. A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks. In _ICLR_, 2019.
* [3] S. Arora, Z. Li, and A. Panigrahi. Understanding Gradient Descent on Edge of Stability in Deep Learning. In _ICML_, 2022.
* [4] P. L. Bartlett, D. P. Hembold, and P. M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In _ICML_, 2018.
* [5] M. Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. _Acta Numerica_, 30:203-248, 2021.
* [6] K. Bhardwaj, G. Li, and R. Marculescu. How does topology influence gradient propagation and model performance of deep networks with densenet-type skip connections? In _CVPR_, 2021.
* [7] S. Bombari, M. H. Amani, and M. Mondelli. Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization. In _NeurIPS_, 2022.

* [8] L. Chizat, E. Oyallon, and F. Bach. On Lazy Training in Differentiable Programming. In _NeurIPS_, 2019.
* [9] J. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability. In _ICLR_, 2021.
* [10] A. Damian, E. Nichani, and J. Lee. Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability. In _NeurIPS_, 2022.
* [11] X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun. Repvgg: Making vgg-style convnets great again. In _CVPR_, 2021.
* [12] X. Ding, H. Chen, X. Zhang, K. Huang, J. Han, and G. Ding. Re-parameterizing your optimizers rather than architectures. In _ICLR_, 2023.
* [13] S. S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient Descent Finds Global Minima of Deep Neural Networks. In _ICML_, pages 1675-1685, 2019.
* [14] S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient Descent Provably Optimizes Over-parameterized Neural Networks. In _ICLR_, 2019.
* [15] F. He and T. Liu and D. Tao. Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence. In _NeurIPS_, 2019.
* [16] Z. Fan and Z. Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks. In _NeurIPS_, 2020.
* [17] E. Hairer, S. Norsett, and G. Wanner. _Solving Ordinary Differential Equations I: Nonstiff Problems_. Springer, 2008.
* [18] B. Hanin and M. Nica. Products of Many Large Random Matrices and Gradients in Deep Neural Networks. _Communications in Mathematical Physics_, 376:287-322, 2020.
* [19] M. Hauser and A. Ray. Principles of riemannian geometry in neural networks. In _NeurIPS_, 2017.
* [20] K. He, X. Zhang, S. Ren, and J. Sun. Identity Mappings in Deep Residual Networks. In _ECCV_, 2016.
* [21] S. Ioffe and C. Szegedy. Batch Normalization: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In _ICML_, page 448-456, 2015.
* [22] A. Jacot, F. Gabriel, and C. Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. In _NeurIPS_, pages 8571-8580, 2018.
* [23] H. Karimi, J. Nutini, and M. Schmidt. Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Lojasiewicz Condition. In _ECML PKDD_, pages 795--811, 2016.
* [24] N. Lee, T. Ajanthan, S. Gould, and P. H. S. Torr. A signal propagation perspective for pruning neural networks at initialization. In _ICLR_, 2020.
* [25] G. Li, Y. Yang, K. Bhardwaj, and R. Marculescu. Zico: Zero-shot nas via inverse coefficient of variation on gradients. In _ICLR_, 2023.
* [26] Y. Li, C. Wei, and T. Ma. Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks. In _NeurIPS_, 2019.
* [27] M. Lin, P. Wang, Z. Sun, H. Chen, X. Sun, Q. Q, H. Li, and R. Jin. Zen-nas: A zero-shot nas for high-performance deep image recognition. In _ICCV_, 2021.
* [28] C. Liu, L. Zhu, and M. Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. In _NeurIPS_, 2020.
* [29] C. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 59:85-116, 2022.
* [30] Y. Lu, C. Ma, Y. Lu, J. Lu, and L. Ying. A Mean Field Analysis Of Deep ResNet And Beyond: Towards Provably Optimization Via Overparameterization From Depth. In _ICML_, 2020.
* [31] J. Martens, A. Ballard, G. Desjardins, G. Swirszcz, V. Dalibard, J. Sohl-Dickstein, and S. S. Schoenholz. Rapid training of deep neural networks without skip connections or normalization layers using deep kernel shaping. arXiv:2110.01765, 2021.

* Nguyen [2021] Q. Nguyen. On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths. In _NeurIPS_, 2021.
* Nguyen and Mondelli [2020] Q. Nguyen and M. Mondelli. Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology. In _NeurIPS_, 2020.
* Nguyen et al. [2021] Q. Nguyen, M. Mondelli, and G. Montufar. Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. In _ICML_, 2021.
* Orhan and Pitkow [2018] A. E. Orhan and X. Pitkow. Skip Connections Eliminate Singularities. In _ICLR_, 2018.
* Oymak and Soltanolkotabi [2020] S. Oymak and M. Soltanolkotabi. Towards moderate overparameterization: global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1:84-105, 2020.
* Pennington and Worah [2017] J. Pennington and P. Worah. Nonlinear random matrix theory for deep learning. In _NeurIPS_, 2017.
* Pennington et al. [2017] J. Pennington, S. S. Schoenholz, and S. Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In _NeurIPS_, 2017.
* Pennington et al. [2018] J. Pennington, S. Schoenholz, and S. Ganguli. The emergence of spectral universality in deep networks. In _AISTATS_, 2018.
* Qiao et al. [2019] S. Qiao, H. Wang, C. Liu, W. Shen, and A. Yuille. Micro-batch training with batch-channel normalization and weight standardization. arXiv:1903.10520, 2019.
* Salismans and Kingma [2016] T. Salismans and D. Kingma. Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. In _NeurIPS_, 2016.
* Santurkar et al. [2018] S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry. How does batch normalization help optimization? In _NeurIPS_, 2018.
* Saxe et al. [2014] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In _ICLR_, 2014.
* Sun et al. [2022] Z. Sun, M. Lin, X. Sun, Z. Tan, H. Li, and R. Jin. Mae-det: Revisiting maximum entropy principle in zero-shot nas for efficient object detection. In _ICML_, 2022.
* Terjek and Gonzalez-Sanchez [2022] D. Terjek and D. Gonzalez-Sanchez. A framework for overparameterized learning, 2022. arXiv:2205.13507.
* Xiao et al. [2018] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. In _ICML_, 2018.
* Yang [2019] G. Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes. In _NeurIPS_, 2019.
* Yang [2020] G. Yang. Tensor Programs II: Neural Tangent Kernel for Any Architecture. arXiv:2006.14548, 2020.
* Yang and Schoenholz [2017] G. Yang and S. S. Schoenholz. Mean Field Residual Networks: On the Edge of Chaos. In _NeurIPS_, 2017.
* Zhang et al. [2022] G. Zhang, A. Botev, and J. Martens. Deep learning without shortcuts: Shaping the kernel with tailored rectifiers. In _ICLR_, 2022.
* Zou and Gu [2019] D. Zou and Q. Gu. An improved analysis of training over-parameterized deep neural networks. In _NeurIPS_, 2019.
* Zou et al. [2020] D. Zou, Y. Cao, D. Zhou, and Q. Gu. Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks. _Machine Learning_, 109:467-492, 2020.

**Computational resources:** Both the exploratory and final experiments for this paper were conducted using a desktop machine with two Nvidia RTX A6000 GPUs, for a total running time of approximately 150 hours.

## Appendix A Examples of MPS

All commonly used neural network layers fit into the framework of multilayer parameterised systems (MPS). Here we list the examples of most relevance to our work.

**Example A.1**.: An _affine layer_ aff : \(\mathbb{R}^{d_{1}\times d_{0}+d_{1}}\times\mathbb{R}^{d_{0}\times N}\to\mathbb{R }^{d_{1}\times N}\) is given by the formula

\[\text{aff}(A,b,X):=AX+b1_{N}^{T}.\] (16)

A routine calculation shows that

\[J\text{aff}(A,b,X)=A\otimes\text{Id}_{N},\] (17)

while

\[D\text{aff}(A,b,X)=\big{(}\text{Id}_{d_{1}}\otimes X^{T},\text{Id}_{d_{1}} \otimes 1_{N}\big{)}.\] (18)

More generally, if \(P:\mathbb{R}^{p}\to\mathbb{R}^{d_{1}\times d_{0}}\) denotes any continuously differentiable map, then one obtains a _\(P\)-parameterised affine layer_

\[\text{aff}_{P}(w,b,X):=\text{aff}(P(w),b,X)=P(w)X+b1^{T}.\] (19)

One has

\[J\text{aff}_{P}(w,b,X)=P(w)\otimes\text{Id}_{N}\] (20)

and, by the chain rule,

\[D\text{aff}_{P}(w,b,X)=\big{(}(\text{Id}_{d_{1}}\otimes X^{T})DP(w),\text{Id} _{d_{1}}\otimes 1\big{)},\] (21)

where \(DP(w)\in\mathbb{R}^{d_{0}N\times p}\) is the derivative of \(P\) at \(w\). Common examples include _\(\epsilon\)-weight normalisation_\(\text{wn}(w):=(\epsilon+\|w\|_{row}^{2})^{-\frac{1}{2}}w\)[41] and convolutions, which send convolutional kernels to associated Toeplitz matrices. We will also consider _\(\epsilon\)-entry normalisation_\(\text{en}(w):=(\epsilon+w^{2})^{-\frac{1}{2}}w\), with operations applied entrywise.

**Example A.2**.: A _(parameter-free) elementwise nonlinearity_\(\Phi:\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{0}\times N}\) defined by a continuously differentiable function \(\phi:\mathbb{R}\to\mathbb{R}\) is given by applying \(\phi\) to every component of a matrix \(X\in\mathbb{R}^{d_{0}\times N}\). Extension to the parameterised case is straightforward.

**Example A.3**.: A _(parameter-free) batch normalisation (BN) layer_ bn : \(\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{0}\times N}\) is given by the formula

\[\text{bn}(X):=\frac{X-\mathbb{E}[X]}{\sqrt{\epsilon+\sigma[X]^{2}}},\] (22)

where \(\epsilon>0\) is some fixed hyperparameter and \(\mathbb{E}\) and \(\sigma\) denote the row-wise mean and standard deviation. The parameterised BN layer from [21], with scaling and bias parameters \(\gamma\) and \(\beta\) respectively, is given simply by postcomposition \(\text{aff}_{\text{diag}}(\gamma,\beta,\cdot)\circ\text{bn}\) with a diag-parameterised affine layer (Example A.1).

**Example A.4**.: A _residual block_\(f:\mathbb{R}^{p}\times\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{1}\times N}\) can be defined given any other layer (or composite thereof) \(g:\mathbb{R}^{p}\times\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{1}\times N}\) by the formula

\[f(\theta,X):=IX+g(\theta,X),\] (23)

where \(I:\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{1}\times N}\) is some linear transformation. In practice, \(I\) is frequently the identity map [20]; our main theorem will concern the case where \(I\) has all singular values equal to 1.

## Appendix B Proofs

Proof of Theorem 2.4.: Using the fact that \(D\ell=(D\gamma\circ F)\cdot DF\), we compute:

\[\|\nabla\ell(\theta)\|^{2} =\langle DF(\theta)^{T}\nabla\gamma(F(\theta)),DF(\theta)^{T} \nabla\gamma(F(\theta))\rangle\] \[=\langle\nabla\gamma(F(\theta)),DF(\theta)DF(\theta)^{T}\nabla \gamma(F(\theta))\rangle\] \[\geq\lambda(DF(\theta))\|\nabla\gamma(F(\theta))\|^{2}\] \[\geq\mu\lambda(DF(\theta))\bigg{(}\gamma(F(\theta))-\inf_{\theta^{ \prime}}\gamma(F(\theta^{\prime}))\bigg{)},\]where the first inequality follows from the standard estimate \(\langle v,AA^{T}v\rangle\geq\lambda_{min}(AA^{T})\|v\|^{2}\), and the final inequality follows from the fact that \(\gamma\) is \(\mu\)-PL over the set \(\{F(\theta):\theta\in\mathbb{R}^{p}\}\). 

Our proof of Proposition 4.3 requires the following standard lemma.

**Lemma B.1**.: _Let \(\{g_{i}:\mathbb{R}^{p}\rightarrow\mathbb{R}^{m_{i}\times m_{i-1}}\}_{i=1}^{n}\) be a family of matrix-valued functions. If, with respect to some submultiplicative matrix norm, each \(g_{i}\) is bounded by \(b_{i}\) and Lipschitz with constant \(c_{i}\) on a set \(S\subset\mathbb{R}^{p}\), then their pointwise matrix product \(\theta\mapsto\prod_{i=1}^{n}g_{i}(\theta)\) is also bounded and Lipschitz on \(S\), with bound \(\prod_{i=1}^{n}b_{i}\) and Lipschitz constant \(\sum_{i=1}^{n}c_{i}\big{(}\prod_{j\neq i}b_{j}\big{)}\). _

Proof of Lemma b.1.: We prove the lemma by induction. When \(n=2\), adding and subtracting a copy of \(g_{1}(\theta)g_{2}(\theta^{\prime})\) and using the triangle inequality implies that \(\|g_{1}g_{2}(\theta)-g_{1}g_{2}(\theta^{\prime})\|\) is bounded by

\[\|g_{1}(\theta)(g_{2}(\theta)-g_{2}(\theta^{\prime}))\|+\|(g_{1}(\theta)-g_{ 1}(\theta^{\prime}))g_{2}(\theta^{\prime})\|.\]

Applying submultiplicativity of the matrix norm and the bounds provided by the \(b_{i}\) and \(c_{i}\) gives

\[\|g_{1}g_{2}(\theta)-g_{1}g_{2}(\theta^{\prime})\|\leq(b_{1}c_{2}+b_{2}c_{1}) \|\theta-\theta^{\prime}\|.\]

Now suppose we have the result for \(n=k\). Writing \(\prod_{i=1}^{k+1}g_{i}\) as \(g_{1}\prod_{i=2}^{k+1}g_{i}\) and applying the above argument, the induction hypothesis tells us that \(\prod_{i=1}^{k+1}g_{i}\) is indeed bounded by \(\prod_{i=1}^{k+1}b_{i}\) and Lipschitz with Lipschitz constant \(\sum_{i=1}^{k+1}c_{i}\big{(}\prod_{j\neq i}b_{j}\big{)}\). The result follows. 

Proof of Proposition 4.3.: By Proposition 4.2, it suffices to show that for each \(1\leq l\leq L\), the function

\[\vec{\theta}\mapsto\prod_{j=l+1}^{L}Jf_{j}\big{(}\theta_{j},f_{<j}(\vec{\theta },X)\big{)}Df_{l}\big{(}\theta_{l},f_{<l}(\vec{\theta},X)\big{)}\] (24)

is bounded and Lipschitz on \(S\). To show this, we must first prove that each map \(\vec{\theta}\mapsto f_{<j}(\vec{\theta},X)\) is bounded and Lipschitz on \(S\). This we prove by induction.

By hypothesis, \(\vec{\theta}\mapsto f_{1}(\vec{\theta},X)=f_{1}(\theta_{1},X)\) is bounded and Lipschitz on \(S\). Suppose now that for \(j>1\), one has \(\vec{\theta}\mapsto f_{<j}(\vec{\theta},X)\) bounded and Lipschitz on \(S\). Then the range of \(S\ni\theta\mapsto f_{<j}(\vec{\theta},X)\) is a bounded subset of \(\mathbb{R}^{d_{j}\times N}\). By hypothesis on \(f_{j}\), it then follows that \(\theta\mapsto f_{<j+1}(\vec{\theta},X)=f_{j}\big{(}\theta_{j},f_{<j}(\vec{ \theta},X)\big{)}\) is bounded and Lipschitz on \(S\).

The hypothesis on the \(Jf_{j}\) and \(Df_{j}\) now implies that the maps \(\vec{\theta}\mapsto Jf_{j}\big{(}\theta_{j},f_{<j}(\vec{\theta},X)\big{)}\), \(l+1\leq j\leq L\), and \(\vec{\theta}\mapsto Df_{l}(\theta_{l},f_{<l}(\vec{\theta},X))\) are all bounded and Lipschitz on \(S\). In particular, as a product of bounded and Lipschitz functions, the map given in Equation (24) is also bounded and Lipschitz on \(S\). Therefore \(DF\) is bounded and Lipschitz on \(S\). 

Proof of Corollary 4.4.: By hypothesis, \(F(S)\) is a bounded subset of \(\mathbb{R}^{d_{L}\times N}\). Continuity of \(\gamma\) then implies that \(\gamma(F(S))\) is a bounded subset of \(\mathbb{R}\), so that \(F(S)\) is contained in a sublevel set of \(\gamma\). The result now follows from the hypotheses. 

To prove Theorem 4.5 it will be convenient to recall some tensor calculus. If \(f:\mathbb{R}^{n_{1}\times n_{2}}\rightarrow\mathbb{R}^{m_{1}\times m_{2}}\) is a matrix-valued, differentiable function of a matrix-valued variable, its derivative \(Df\) can be regarded as a map \(\mathbb{R}^{n_{1}\times n_{2}}\rightarrow\mathbb{R}^{m_{1}\times m_{2}\times n _{1}\times n_{2}}\) whose components are given by

\[Df_{j_{1},j_{2}}^{i_{1},i_{2}}(X)=\frac{\partial f_{i_{2}}^{i_{1}}}{\partial x _{j_{2}}^{j_{1}}}(X),\qquad X\in\mathbb{R}^{n_{1}\times n_{2}}\]

where \(1\leq i_{\alpha}\leq m_{\alpha}\) and \(1\leq j_{\alpha}\leq n_{\alpha}\) are the indices, \(\alpha=1,2\). It is easily deduced from the chain rule of ordinary calculus that if \(f:\mathbb{R}^{n_{1}\times n_{2}}\rightarrow\mathbb{R}^{m_{1}\times m_{2}}\) and \(g:\mathbb{R}^{m_{1}\times m_{2}}\rightarrow\mathbb{R}^{l_{1}\times l_{2}}\) are differentiable, then \(g\circ f\) is differentiable with derivative \((Dg\circ f)\cdot Df:\mathbb{R}^{n_{1}\times n_{2}}\rightarrow\mathbb{R}^{l_{1} \times l_{2}\times n_{1}\times n_{2}}\), where here \(\cdot\) denotes contraction over the \(m_{1}\times m_{2}\) indices. The following lemmata then follow from routine calculation.

**Lemma B.2**.: _Let \(\text{bn}:\mathbb{R}^{d\times N}\to\mathbb{R}^{d\times N}\) be an \(\epsilon\)-batchnorm layer. Then one can write \(\text{bn}=v\circ m\), where \(v,m:\mathbb{R}^{d\times N}\to\mathbb{R}^{d\times N}\) are given respectively by_

\[v(Y)=(N\epsilon+\|Y\|_{\text{row}}^{2})^{-\frac{1}{2}}\sqrt{N}Y,\] (25)

\[m(X)=X-\frac{1}{N}X1_{N\times N}.\] (26)

_One has_

\[\frac{\partial v_{j}^{i}}{\partial y_{l}^{k}}=\delta_{k}^{i}\sqrt{N}(N\epsilon +\|y^{i}\|^{2})^{-\frac{1}{2}}\big{(}\delta_{l}^{j}-(N\epsilon+\|y^{i}\|^{2}) ^{-1}y_{l}^{i}y_{j}^{i}\big{)}\] (27)

_and_

\[\frac{\partial^{2}v_{j}^{i}}{\partial y_{n}^{m}\partial y_{l}^{k}}= \delta_{k}^{i}\delta_{m}^{i}\sqrt{N}(N\epsilon+\|y^{i}\|^{2})^{- \frac{3}{2}}\times\] \[\times\big{(}3(N\epsilon+\|y^{i}\|^{2})^{-1}y_{n}^{i}y_{l}^{i}y_{ j}^{i}\] \[-(\delta_{l}^{j}y_{n}^{i}+\delta_{n}^{l}y_{j}^{i}+\delta_{n}^{i}y_ {l}^{i})\big{)},\] (28)

_with_

\[\frac{\partial m_{j}^{i}}{\partial x_{l}^{k}}=\delta_{k}^{i}(\delta_{l}^{j}-N ^{-1}).\] (29)

_and all second derivatives of \(m\) being zero. _

**Lemma B.3**.: _Let \(\text{wn}:\mathbb{R}^{d_{1}\times d_{0}}\to\mathbb{R}^{d_{1}\times d_{0}}\) be an \(\epsilon\)-weight normalised parameterisation (Example A.1). Then one has_

\[\frac{\partial\text{wn}_{j}^{i}}{\partial w_{l}^{k}}=\delta_{k}^{i}(\epsilon+ \|w^{i}\|^{2})^{-\frac{1}{2}}\big{(}\delta_{l}^{j}-(\epsilon+\|w^{i}\|^{2})^{ -1}w_{l}^{i}w_{j}^{i}\big{)}\] (30)

_and_

\[\frac{\partial\text{wn}_{j}^{i}}{\partial w_{n}^{m}\partial w_{k} ^{k}}= \delta_{k}^{i}\delta_{m}^{i}(\epsilon+\|w^{i}\|^{2})^{-\frac{3}{ 2}}\times\] \[\times\big{(}3(\epsilon+\|w^{i}\|^{2})^{-1}w_{n}^{i}w_{l}^{i}w_{ j}^{i}\] \[-(\delta_{l}^{j}w_{n}^{i}+\delta_{n}^{l}w_{j}^{i}+\delta_{n}^{j}w _{l}^{i})\big{)}.\] (31)

_Similarly, if \(\text{en}:\mathbb{R}^{d_{1}\times d_{0}}\to\mathbb{R}^{d_{1}\times d_{0}}\) is an \(\epsilon\)-entry-normalised parameterisation, then_

\[\frac{\partial\text{en}_{j}^{i}}{\partial w_{l}^{k}}=\delta_{k}^{i}\delta_{l}^ {j}\epsilon(\epsilon+(w_{j}^{i})^{2})^{-\frac{3}{2}}\] (32)

_and_

\[\frac{\partial\text{en}_{j}^{i}}{\partial w_{m}^{m}\partial w_{l}^{k}}=-\delta _{n}^{i}\delta_{m}^{i}\delta_{k}^{i}\delta_{l}^{j}3\epsilon(\epsilon+(w_{j}^{ i})^{2})^{-\frac{3}{2}}w_{j}^{i}\] (33)

Proof of Theorem 4.5.: (1) follows from continuity of the nonlinearity and its derivative, implying boundedness of both over bounded sets in \(\mathbb{R}^{d\times N}\).

(2) and (3) follow from a similar argument to the following argument for batch norm, which we give following Lemma B.2. Specifically, for the composite \(f:=\text{bn}\circ\text{aff}:\mathbb{R}^{d_{1}\times d_{0}}\times\mathbb{R}^{d_ {0}\times N}\to\mathbb{R}^{d_{1}\times N}\) defined by an \(\epsilon\)-BN layer and an affine layer, we will prove that over any set \(B\subset\mathbb{R}^{d_{0}\times N}\) consisting of matrices \(X\) whose covariance matrix is nondegenerate, one has \(f\), \(Df\) and \(Jf\) all globally bounded and Lipschitz. Indeed, \(v\) (Equation (25)) is clearly globally bounded, while \(Dv\) (Equation (27)) is globally bounded, decaying like \(\|Y\|_{rw}^{-1}\) out to infinity, and \(D^{2}v\) (Equation (28)) is globally bounded, decaying like \(\|Y\|_{rw}^{-2}\) out to infinity. Consequently,

\[\text{bn}\circ\text{aff}=v\circ(m\circ\text{aff}),\] \[D(\text{bn}\circ\text{aff})=(Jv\circ m\circ\text{aff})\cdot(Jm\circ \text{aff})\cdot D\text{aff},\] \[J(\text{bn}\circ\text{aff})=(Jv\circ m\circ\text{aff})\cdot(Jm\circ \text{aff})\cdot J\text{aff},\]and similarly the derivatives of \(D(\text{bn}\circ\text{aff})\) and \(J(\text{bn}\circ\text{aff})\) are all globally bounded over \(\mathbb{R}^{d_{1}\times d_{0}}\times B\). The hypothesis that \(B\) consist of matrices with nondegenerate covariance matrix is needed here because while \(Jv\circ m\circ\text{aff}\) decays like \(\|A(X-\mathbb{E}[X])\|_{\text{Cov}_{N}}^{-1}\) out to infinity, the row-norm \(\|(A(X-\mathbb{E}[X]))^{i})\|^{2}=(A^{i}(X-\mathbb{E}[X])(X-\mathbb{E}[X])^{T }(A^{i})^{T})=\|A^{i}\|_{\text{Cov}(X)}^{\text{Cov}(X)}\) can only be guaranteed to increase with \(A\) if \(\text{Cov}(X)\) is nondegenerate. Thus, for instance, without the nondegeneracy hypothesis on \(\text{Cov}(X)\), \(A\mapsto J(\text{bn}\circ\text{aff})(A,X)\) grows unbounded like \(J\text{aff}(A,X)=A\otimes\text{Id}_{N}\) in any direction of degeneracy of \(\text{Cov}(X)\). Nonetheless, with the nondegenerate covariance assumption on elements of \(B\), \(\text{bn}\circ\text{aff}\) satisfies the hypotheses of Proposition 4.3 over \(\mathbb{R}^{d_{1}\times d_{0}}\times B\).

(2) and (3) now follow from essentially the same boundedness arguments as for batch norm, using Lemma B.3 in the place of Lemma B.2. However, since the row norms in this case are always defined by the usual Euclidean inner product on row-vectors, as opposed to the possibly degenerate inner product coming from the covariance matrix of the input vectors, one does not require any hypotheses aside from boundedness on the set \(B\). Thus entry- and weight-normalised affine layers satisfy the hypotheses of Proposition 4.3.

Finally, (5) follows from the above arguments. More specifically, if \(g:\mathbb{R}^{p}\times\mathbb{R}^{d\times N}\to\mathbb{R}^{d\times N}\) is any composite of layers of the above form, then \(g\) satisfies the hypotheses of Proposition 4.3. Consequently, so too does the residual block \(f(\theta,X):=X+g(\theta,X)\), for which \(Jf(\theta,X)=\text{Id}_{d}\otimes\text{Id}_{N}+Jg(\theta,X)\) and \(Df(\theta,X)=Dg(\theta,X)\).

Proof of Proposition 4.6.: In the notation of Proposition 4.2, the product \(DF(\vec{\theta})DF(\vec{\theta})^{T}\) is the sum of the positive-semidefinite matrices \(D_{\theta_{i}}F(\vec{\theta})D_{\theta_{i}}F(\vec{\theta})^{T}\). Therefore \(\lambda(DF(\vec{\theta}))\geq\sum_{l}\lambda(D_{\theta_{i}}F(\vec{\theta}))\). The result now follows from the inequality \(\lambda(AB)\geq\lambda(A)\lambda(B)\) applied inductively using Equation (11). Note that \(\lambda(AB)\geq\lambda(A)\lambda(B)\) is either trivial if one or both of \(A\) and \(B\) have more rows than columns (in which case the right hand side is zero), and follows from the well-known inequality \(\sigma(AB)\geq\sigma(A)\sigma(B)\) for the smallest singular values if both \(A\) and \(B\) have at least as many columns as rows. 

Theorem 4.7 follows from the following two lemmata.

**Lemma B.4**.: _Let \(g:\mathbb{R}^{p}\times\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{1}\times N}\) be a layer for which there exists \(\delta>0\) such that \(\|Jg(\theta,X)\|_{2}<(1-\delta)\) for all \(\theta\) and \(X\). Let \(I:\mathbb{R}^{d_{0}\times N}\to\mathbb{R}^{d_{1}\times N}\) be a linear map whose singular values are all equal to 1. Then the residual block \(f(\theta,X):=IX+g(\theta,X)\) has \(\sigma(Jf(\theta,X))>\delta\) for all \(\theta\) and \(X\)._

Proof.: Observe that

\[Jf(\theta,X)=I\otimes\text{Id}_{N}+Jg(\theta,X).\] (34)

The result then follows from Weyl's inequality: all singular values of \(I\otimes\text{Id}_{N}\) are equal to 1, so that

\[\sigma\big{(}Jf(\theta,X)\big{)}\geq 1-\|Jg(\theta,X)\|_{2}>\delta\]

for all \(\theta\) and \(X\). 

**Lemma B.5**.: _Let \(P:\mathbb{R}^{p}\to\mathbb{R}^{d_{1}\times d_{0}}\) be a parameterisation. Then_

\[\sigma\big{(}\text{Daff}_{P}(w,X)\big{)}\geq\sigma(X)\sigma\big{(}DP(w)\big{)}\] (35)

_for all \(w\in\mathbb{R}^{p}\) and \(X\in\mathbb{R}^{d_{0}\times N}\)._

Proof.: Follows from Equation (21) and the inequality \(\sigma(AB)\geq\sigma(A)\sigma(B)\). 

Proof of Theorem 4.7.: Hypothesis 1 in Theorem 4.7 says that the residual branches of the \(f_{l}\), \(l\geq 2\), satisfy the hypotheses of Lemma B.4, so that \(\sigma(Jf_{l}(\theta_{l},f_{<l}(\vec{\theta},X)))>0\) for all \(l\geq 2\). By the assumption that \(d_{l-1}\geq d_{l}\), this means that \(\lambda(Jf_{l}(\theta_{l},f_{<l}(\vec{\theta},X)))=\sigma(Jf_{l}(\theta_{l},f_ {<l}(\vec{\theta},X)))^{2}>0\). On the other hand, hypothesis 2 together with Lemma B.5 implies that \(\lambda(Df_{1}(\theta_{1},X))\geq\sigma(Df_{1}(\theta_{1},X))^{2}>0\). The result now follows from Proposition 4.6.

Proof of Theorem 5.1.: By Theorem 4.5, all layers satisfy the Hypotheses of Proposition 4.3 and so by Corollary 4.4, the associated loss function is globally Lipschitz, with Lipschitz constant some \(\beta>0\). Take \(\eta>0\) to be any number smaller than \(2\beta^{-1}\); thus the loss can be guaranteed to be decreasing with every gradient descent step.

We now show that the network satisfies the hypotheses of Theorem 4.7. The dimension constraints in item (1) are encoded directly into the definition of the network, while the operator-norm of each of the residual branches, as products of \(P(w)\) and \(D\Phi\) matrices, are globally bounded by 1 by our hypotheses on these factors. For item (2), our data matrix is full-rank since it consists of linearly independent data, while by definition we have \(p_{1}=d_{1}d_{0}\) with \(Df_{1}=D\text{aff}_{\text{en}}\) being everywhere full-rank since \(\epsilon\)-entry-normalisation is a diffeomorphism onto its image for any \(\epsilon>0\). Its hypotheses being satisfied by our weight-normalised residual network, Theorem 4.7 implies the parameter-function map \(F\) associated to \(\{f_{l}\}_{l=1}^{L}\) and \(X\) satisfies \(\lambda(DF(\vec{\theta}))=\sigma(DF(\vec{\theta}))^{2}>0\) for all parameters \(\vec{\theta}\). There are now two cases to consider.

In the first case, the gradient descent trajectory never leaves some ball of finite radius in \(\mathbb{R}^{d_{1}\times d_{0}}\), the parameter space for the first layer. In any such ball, recalling that the first layer's parameterisation is entry-normalisation (Example A.1), the smallest singular value

\[\sigma(D\operatorname{\text{en}}(w))=\min_{1\leq i\leq d_{1}1\leq j\leq d_{0} }\frac{\epsilon}{(\epsilon+(w_{j}^{i})^{2})^{\frac{3}{2}}}\] (36)

of \(D\operatorname{\text{en}}(w)\) is uniformly lower bounded by some positive constant. Thus by Lemmas B.5 and B.48, the smallest singular value of \(DF\) is also uniformly lower bounded by a positive constant in any such ball. It follows from Theorem 2.4 that the loss satisfies the PL-inequality over such a ball, so that gradient descent converges in this case at a linear rate to a global minimum.

Footnote 8: See the supplementary material.

The second and only other case that must be considered is when for each \(R>0\), there is some time \(T\) for which the weight norm \(\|w_{t}\|\) of the parameters in the first layer is greater than \(R\) for all \(t\geq T\). That is, the parameter trajectory in the first layer is unbounded in time. In this case, inspection of Equation (36) reveals that the smallest singular value of \(DF\)_cannot_ be uniformly bounded below by a positive constant over all of parameter space. Theorem 2.4 then says that there is merely a sequence \((\mu_{t})_{t\in\mathbb{N}}\), with \(\mu_{t}\) proportional to \(\sigma(D\operatorname{\text{en}}(w_{t}))\), for which

\[\ell_{t}-\ell^{*}\leq\prod_{i=0}^{t}(1-\mu_{i}\alpha)(\ell_{0}-\ell^{*}),\] (37)

where \(\alpha=\eta(1-2\beta^{-1}\eta)>0\). To guarantee convergence in this case, therefore, it suffices to show that \(\prod_{t=0}^{\infty}(1-\mu_{t}\alpha)=0\); equivalently, it suffices to show that the infinite series

\[\sum_{t=0}^{\infty}\log(1-\mu_{t}\alpha)\] (38)

diverges.

The terms of the series (38) form a sequence of negative numbers which converges to zero. Hence, for the series (38) to diverge, it is _necessary_ that \(\mu_{t}\) decrease _sufficiently slowly_ with time. By the integral test, therefore, it suffices to find an integrable function \(m:[t_{0},\infty)\to\mathbb{R}_{\geq 0}\) such that \(\mu_{t}\geq m(t)\) for each integer \(t\geq t_{0}\), for which the integral \(\int_{t_{0}}^{\infty}\log(1-m(t)\alpha)\,dt\) diverges.

We construct \(m\) by considering the worst possible case: where each gradient descent step is in exactly the same direction going out to \(\infty\), thereby decreasing \(\sigma(D\operatorname{\text{en}}(w))\) at the fastest possible rate. By applying an orthogonal-affine transformation to \(\mathbb{R}^{d_{1}d_{0}}\), we can assume without loss of generality that the algorithm is initialised at, and consistently steps in the direction of, the first canonical basis vector \(e_{1}\) in \(\mathbb{R}^{d_{1}d_{0}}\). Specifically, letting \(\vec{\theta}\) be the vector of parameters for all layers following the first and \(w\in\mathbb{R}^{d_{1}d_{0}}\) the first layer parameters, for \(r\in\mathbb{R}_{\geq 1}\) we may assume that

\[\nabla_{w}\ell(\vec{\theta},re_{1})=\partial_{w_{1}^{1}}\ell(\vec{\theta},re_{ 1})e_{1},\] (39)

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

On ImageNet, the models were trained using the default PyTorch ImageNet example10, using SGD with weight decay of \(1e-4\) and momentum of \(0.9\), batch size of 256, and random crop/horizontal flip data augmentation. See Table 3 for the validation accuracies, and Figure 6 for the plots of the training losses at different learning rates.

Footnote 10: https://github.com/pytorch/examples/tree/main/imagenet

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{2}{c}{Original} & \multicolumn{2}{c}{Modified} \\ Learning rate & Test accuracy & Final loss & Test accuracy & Final loss \\ \hline
0.20 & \(49.24\pm 34.75\) & \(1.0285\pm 0.0066\) & – & – \\
0.10 & \(74.69\pm 0.12\) & \(0.9201\pm 0.0014\) & \(74.99\pm 0.09\) & \(0.8949\pm 0.0019\) \\
0.05 & \(74.62\pm 0.12\) & \(0.8563\pm 0.0041\) & \(74.84\pm 0.03\) & \(0.8420\pm 0.0030\) \\
0.02 & \(73.87\pm 0.09\) & \(0.8690\pm 0.0025\) & \(74.00\pm 0.11\) & \(0.8441\pm 0.0026\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: ResNet50 on ImageNet

Figure 6: Loss for original and modified ResNet50 at different learning rates on ImageNet. The modified architecture did not converge at the maximum learning rate of 0.2.