# The Benefits of Being Distributional:

Small-Loss Bounds for Reinforcement Learning

 Kaiwen Wang  Kevin Zhou  Runzhe Wu  Nathan Kallus  Wen Sun

Cornell University

{kw437,klz23,rw646,kallus,ws455}@cornell.edu

###### Abstract

While distributional reinforcement learning (DistRL) has been empirically effective, the question of when and why it is better than vanilla, non-distributional RL has remained unanswered. This paper explains the benefits of DistRL through the lens of small-loss bounds, which are instance-dependent bounds that scale with optimal achievable cost. Particularly, our bounds converge much faster than those from non-distributional approaches if the optimal cost is small. As warmup, we propose a distributional contextual bandit (DistCB) algorithm, which we show enjoys small-loss regret bounds and empirically outperforms the state-of-the-art on three real-world tasks. In online RL, we propose a DistRL algorithm that constructs confidence sets using maximum likelihood estimation. We prove that our algorithm enjoys novel small-loss PAC bounds in low-rank MDPs. As part of our analysis, we introduce the \(\ell_{1}\) distributional eluder dimension which may be of independent interest. Then, in offline RL, we show that pessimistic DistRL enjoys small-loss PAC bounds that are novel to the offline setting and are more robust to bad single-policy coverage.

## 1 Introduction

The goal of reinforcement learning (RL) is to learn a policy that minimizes/maximizes the mean loss/return (_i.e._, cumulative costs/rewards) along its trajectory. Classical approaches, such as \(Q\)-learning (Mnih et al., 2015) and policy gradients (Kakade, 2001), often learn \(Q\)-functions via least square regression, which represent the mean loss-to-go and act greedily with respect to these estimates. By Bellman's equation, \(Q\)-functions suffice for optimal decision-making and indeed these approaches have vanishing regret bounds, suggesting we only need to learn means well (Sutton and Barto, 2018). Since the seminal work of Bellemare et al. (2017), however, numerous developments showed that learning the _whole_ loss distribution can actually yield state-of-the-art performance in stratospheric balloon navigation (Bellemare et al., 2020), robotic grasping (Bodnar et al., 2020), algorithm discovery (Fawzi et al., 2022) and game playing benchmarks (Hessel et al., 2018; Dabney et al., 2018; Barth-Maron et al., 2018). In both online (Yang et al., 2019) and offline RL (Ma et al., 2021), distributional RL (DistRL) algorithms often perform better and use fewer samples in challenging tasks when compared to standard approaches that directly estimate the mean.

Despite learning the whole loss distribution, DistRL algorithms use only the mean of the learned distribution for decision making, not extracting any additional information such as higher moments. In other words, DistRL is simply employing a different and seemingly roundabout way of learning the mean: first, learn the loss-to-go distribution via distributional Bellman equations, and then, compute the mean of the learned distribution. Lyle et al. (2019) provided some empirical explanations of the benefits of this two-step approach, showing that learning the distribution, _e.g._, its moments or quantiles, is an auxiliary task that leads to better representation learning. However, the theoreticalquestion remains: does DistRL, _i.e._, learning the distribution and then computing the mean, yield provably stronger finite-sample guarantees and if so stronger how and when?

In this paper, we provide the first mathematical basis for the benefits of DistRL via the lens of small-loss bounds, which are instance-dependent bounds that depend on the minimum achievable cost in the problem [Agarwal et al., 2017].1 For example in linear MDPs, typical worst-case regret bounds scale on the order of \(\mathrm{poly}(d,H)\sqrt{K}\), where \(d\) is the feature dimension, \(H\) is the horizon, and \(K\) is the number of episodes [Jin et al., 2020]. In contrast, small-loss bounds will scale on the order of \(\mathrm{poly}(d,H)\sqrt{K\cdot V^{\star}}+\mathrm{poly}(d,H)\log(K)\), where \(V^{\star}=\min_{\pi}V^{\pi}\) is the optimal expected cumulative cost for the problem. We assume cumulative costs are normalized in \([0,1]\) without loss of generality. As \(V^{\star}\) becomes negligible (approaches \(0\)), the first term vanishes and the small-loss bound yields a faster convergence rate of \(\mathcal{O}(\mathrm{poly}(d,H)\log(K))\), compared to the \(\mathcal{O}(\mathrm{poly}(d,H)\sqrt{K})\) rate in standard uniform bounds. Since we always have \(V^{\star}\leq 1\), small-loss bounds simply match the standard uniform bounds in the worst case.

Footnote 1: “First-order” generally refers to bounds that scale with the optimal value, either the maximum reward or the minimum cost. To highlight that we are minimizing cost, we call our bounds “small-loss”.

As warm-up, we show that maximum likelihood estimation (MLE), _i.e._, maximizing log-likelihood, can be used to obtain small-loss regret bounds for contextual bandits (CB), _i.e._, the one-step RL setting. Then, we turn to the online RL setting, and propose an optimistic DistRL algorithm that optimizes over confidence sets constructed via MLE applied to the distributional Bellman equations. We prove our algorithm attains the first small-loss PAC bounds in low-rank MDPs [Agarwal et al., 2020]. Our proof uses a novel regret decomposition with triangular discrimination and also introduces the \(\ell_{1}\) distributional eluder dimension, which generalizes the \(\ell_{2}\) distributional eluder dimension of Jin et al. [2021] and may be of independent interest. Furthermore, we design an offline distributional RL algorithm using the principle of pessimism, and show our algorithm obtains the first small-loss bounds in offline RL. Our offline small-loss bound holds under the weak single-policy coverage. Notably, our result has a novel robustness property that allows our algorithm to strongly compete with policies that either are well-covered or have small-loss, while prior approaches solely depended on the former. Finally, we find that our distributional CB algorithm empirically outperforms existing approaches in three challenging CB tasks.

Our key contributions are as follows:

1. As warm-up, we propose a distributional CB algorithm and prove that it obtains a small-loss regret bound (Section4). We empirically demonstrate it outperforms state-of-the-art CB algorithms in three challenging benchmark tasks (Section7).
2. We propose a distributional online RL algorithm that enjoys small-loss bounds in settings with low \(\ell_{1}\) distributional eluder dimension, which we show can always capture low-rank MDPs. The \(\ell_{1}\) distributional eluder dimension may be of independent interest (Section5).
3. We propose a distributional offline RL algorithm and prove that it obtains the first small-loss bounds in the offline setting. Our small-loss guarantee exhibits a novel robustness to bad coverage, which implies strong improvement over more policies than existing results in the literature (Section6).

In sum, we show that DistRL can yield small-loss bounds in both online and offline RL, which provide a concrete theoretical justification for the benefits of distribution learning in decision making.

## 2 Related Works

Theory of Distributional RLRowland et al. [2018, 2023] proved asymptotic convergence guarantees of popular distributional RL algorithms such as C51 [Bellemare et al., 2017] and QR-DQN [Dabney et al., 2018]. However, these asymptotic results do not explain the _benefits_ of distributional RL over standard approaches, since they do not imply stronger finite-sample guarantees than those obtainable with non-distributional algorithms. In contrast, our work shows that distributional RL yields adaptive finite-sample bounds that converge faster when the optimal cost of the problem is small. Wu et al. [2023] recently derived finite-sample bounds for distributional off-policy evaluation with MLE, while our offline RL section focuses on off-policy optimization.

First-order bounds in banditsWhen maximizing rewards, first-order "small-return" bounds can be easily derived from EXP4 (Auer et al., 2002), since receiving the worst reward \(0\) with probability (w.p.) \(\delta\) contributes at most \(R^{\star}\delta\) to the regret2. When minimizing costs, receiving the worst loss \(1\) w.p. \(\delta\) may induce large regret relative to \(L^{\star}\) if \(L^{\star}\) is small. To illustrate, if \(R^{\star}=0\) then all policies are optimal, so no learning is needed and the small-return bound is vacuous. Yet if \(L^{\star}=0\), sub-optimal policies may have a large gap from \(L^{\star}\), so small-loss bounds in this regime are meaningful. Small-loss bounds are achievable in multi-arm bandits (Foster et al., 2016), semi-bandits (Neu, 2015; Lykouris et al., 2022), and CBs (Allen-Zhu et al., 2018; Foster and Krishnamurthy, 2021).

Footnote 2: Assume rewards/losses in \([0,1]\) and \(R^{\star}/L^{\star}\) is the maximum/minimum expected reward/loss.

First-order bounds in RLJin et al. (2020); Wagenmaker et al. (2022) obtained small-return regret for tabular and linear MDPs via concentration bounds that scale with the variance. The idea is that the return's variance is bounded by some multiple of the expected value, which is bounded by \(V^{\star}\) in the reward-maximizing setting, _i.e._, \(\mathrm{Var}(\sum_{h}r_{h}\mid\pi^{k})\leq c\cdot V^{\pi^{k}}\leq c\cdot V^{\star}\). However, the last inequality fails in the loss-minimizing setting, so the variance approach does not easily yield small-loss bounds. Small-loss regret for tabular MDPs was resolved by Lee et al. (2020, Theorem 4.1) using online mirror descent with the log-barrier on the occupancy measure. Moreover, Kakade et al. (2020, Theorem 3.8) obtains small-loss regret for linear-quadratic regulators (LQRs), but their Assumption 3 posits that the coefficient of variation for the cumulative costs is bounded, which is false in general even in tabular MDPs. To the best of our knowledge, there are no known first-order bounds for low-rank MDPs or in offline RL.

Risk-sensitive RLA well-motivated use-case of DistRL is risk-sensitive RL, where the goal is to learn risk-sensitive policies that optimize some risk measure, _e.g._, Conditional Value-at-Risk (CVaR), of the loss (Dabney et al., 2018). Orthogonal to risk-sensitive RL, this work focuses on the benefits of DistRL for standard risk-neutral RL. Our insights may lead to first-order bounds for risk-sensitive RL, which we leave as future work.

## 3 Preliminaries

As warmup, we begin with the contextual bandit problem with an arbitrary context space \(\mathcal{X}\), finite action space \(\mathcal{A}\) with size \(A\) and conditional cost distributions \(C:\mathcal{X}\times\mathcal{A}\to\Delta([0,1])\). Throughout, we fix some dominating measure \(\lambda\) on \([0,1]\)(_e.g._, Lebesgue for continuous or counting for discrete) and let \(\Delta([0,1])\) be all distributions on \([0,1]\) that are absolutely continuous with respect to \(\lambda\). We identify such a distribution with its density with respect to \(\lambda\), and we also write \(C(y\mid x,a)\) for \((C(x,a))(y)\). Let \(K\) denote the number of episodes. At each episode \(k\in[K]\), the learner observes a context \(x_{k}\in\mathcal{X}\), samples an action \(a_{k}\in\mathcal{A}\), and then receives a cost \(c_{t}\sim C(x_{t},a_{t})\), which we assume to be normalized, _i.e._, \(c_{t}\in[0,1]\). The goal is to design a learner that attains low regret with high probability, where regret is defined as

\[\mathrm{Regret}_{\text{CB}}(K)=\sum_{k=1}^{K}\bar{C}(x_{k},a_{k})-\bar{C}(x_{k },\pi^{\star}(x_{k})),\]

where \(\bar{f}=\int yf(y)\mathrm{d}\lambda(y)\) for any \(f\in\Delta([0,1])\) and \(\pi^{\star}(x_{k})=\arg\min_{a\in\mathcal{A}}\bar{C}(x_{k},a)\).

The focus of this paper is reinforcement learning (RL) under the Markov Decision Process (MDP) model, with observation space \(\mathcal{X}\), finite action space \(\mathcal{A}\) with size \(A\), horizon \(H\), transition kernels \(P_{h}:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{X})\) and _cost_ distributions \(C_{h}:\mathcal{X}\times\mathcal{A}\to\Delta([0,1])\) at each step \(h\in[H]\). We start with the _Online RL_ setting, which proceeds over \(K\) episodes as follows: at each episode \(k\in[K]\), the learner plays a policy \(\pi^{k}\in[\mathcal{X}\to\Delta(\mathcal{A})]^{H}\); we start from a fixed initial state \(x_{1}\); then for each \(h=1,2,\ldots,H\), the policy samples an action \(a_{h}\sim\pi^{k}_{h}(x_{h})\), receives a cost \(c_{h}\sim C_{h}(x_{h},a_{h})\), and transitions to the next state \(x_{h+1}\sim P_{h}(x_{h},a_{h})\). Our goal is to compete with the optimal policy that minimizes expected the loss, _i.e._, \(\pi^{\star}\in\arg\min_{\pi\in\Pi}V^{\pi}\) where \(V^{\pi}=\mathbb{E}_{\pi}\Big{[}\sum_{h=1}^{H}c_{h}\Big{]}\). Regret bounds aim to control the learner's regret with high probability, where regret is defined as,

\[\mathrm{Regret}_{\text{RL}}(K)=\sum_{k=1}^{K}V^{\pi^{k}}-V^{\star}.\]

If the algorithm returns a single policy \(\widehat{\pi}\), it is desirable to obtain a Probably Approximately Correct (PAC) bound on the sub-optimality of \(\widehat{\pi}\), _i.e._, \(V^{\widetilde{\pi}}-V^{\star}\).

The third setting we study is _Offline RL_, where instead of needing to actively explore and collect data ourselves, we are given \(H\) datasets \(\mathcal{D}_{1},\mathcal{D}_{2},\ldots,\mathcal{D}_{H}\) to learn a good policy \(\widehat{\pi}\). Each \(\mathcal{D}_{h}\) contains \(N\)_i.i.d._ samples \((x_{h,i},a_{h,i},c_{h,i},x^{\prime}_{h,i})\) from the process \((x_{h,i},a_{h,i})\sim\nu_{h},c_{h,i}\sim C_{h}(x_{h,i},a_{h,i}),x^{\prime}_{h,i}\sim P_{h}(x_{h,i},a_{h,i})\), where \(\nu_{h}\in\Delta(\mathcal{X}\times\mathcal{A})\) is arbitrary, _e.g._, the visitations of many policies from the current production system. The goal is to design an offline procedure with a PAC guarantee for \(\widehat{\pi}\), which should improve over the data generating process.

Distributional RLFor a policy \(\pi\) and \(h\in[H]\), let \(Z^{\pi}_{h}(x_{h},a_{h})\in\Delta([0,1])\) denote the distribution of the loss-to-go \(\sum_{t=h}^{H}c_{t}\) conditioned on rolling in \(\pi\) from \(x_{h},a_{h}\). The expectation of the above is \(Q^{\pi}_{h}(x_{h},a_{h})=Z^{\pi}_{h}(x_{h},a_{h})\) and \(V^{\pi}_{h}(x_{h})=\mathbb{E}_{a_{h}\sim\pi_{h}(x_{h})}[Q^{\pi}_{h}(x_{h},a_{h })]\). We use \(Z^{\star}_{h},Q^{\star}_{h},V^{\star}_{h}\) to denote these quantities with \(\pi^{\star}\). Recall the regular Bellman operator acts on a function \(f:\mathcal{X}\times\mathcal{A}\to[0,1]\) as follows: \(\mathcal{T}^{\pi}_{h}f(x,a)=C_{h}(x,a)+\mathbb{E}_{x^{\prime}\sim P_{h}(x,a),a ^{\prime}\sim\pi(x^{\prime})}[f(x^{\prime},a^{\prime})]\). Analogously, the distributional Bellman operator (Morimura et al., 2012; Bellemare et al., 2017) acts on a conditional distribution \(d:\mathcal{X}\times\mathcal{A}\to\Delta([0,1])\) as follows: \(\mathcal{T}^{\pi,D}_{h}d(x,a)\overset{D}{=}C_{h}(x,a)+d(x^{\prime},a^{\prime})\), where \(x^{\prime}\sim P_{h}(x,a),a^{\prime}\sim\pi(x^{\prime})\) and \(\overset{D}{=}\) denotes equality of distributions. Another way to think about the distributional Bellman operator is that a sample \(z\sim\mathcal{T}^{\pi,D}_{h}d(x,a)\) is generated as follow: \(z:=c+y\), where \(c\sim C_{h}(x,a),x^{\prime}\sim P_{h}(x,a),a^{\prime}\sim\pi(x^{\prime}),y\sim d (x^{\prime},a^{\prime})\). We will also use the Bellman optimality operator \(\mathcal{T}^{\star}_{h}\) and its distributional variant \(\mathcal{T}^{\star,D}_{h}\), defined as follows: \(\mathcal{T}^{\star}_{h}f(x,a)=\bar{C}_{h}(x,a)+\mathbb{E}_{x^{\prime}\sim P_{h }(x,a)}[\min_{a\in\mathcal{A}}f(x^{\prime},a^{\prime})]\) and \(\mathcal{T}^{\star,D}_{h}d(x,a)\overset{D}{=}C_{h}(x,a)+d(x^{\prime},a^{ \prime})\) where \(x^{\prime}\sim P_{h}(x,a),a^{\prime}=\operatorname*{arg\,min}_{a}\bar{d}(x^{ \prime},a)\). Please see Table 2 for an index of notations.

## 4 Warm up: Small-Loss Regret for Distributional Contextual Bandits

In this section, we propose an efficient reduction from CB to online maximum likelihood estimation (MLE), which is the standard tool for distribution learning that we will use throughout the paper. In our CB algorithm, we balance exploration and exploitation with the reweighted inverse gap weighting (ReIGW) of Foster and Krishnamurthy (2021), which defines a distribution over actions given predictions \(\widehat{f}\in\mathbb{R}^{A}\) and a parameter \(\gamma\in\mathbb{R}_{++}\): setting \(b=\operatorname*{arg\,min}_{a\in\mathcal{A}}\widehat{f}(a)\) as the best action with respect to the predictions, the weight for any other action \(a\neq b\) is,

\[\text{ReIGW}_{\gamma}(\widehat{f},\gamma)[a]:=\frac{\widehat{f}(b)}{A\widehat {f}(b)+\gamma(\widehat{f}(a)-\widehat{f}(b))},\] (1)

and the rest of the weight is allocated to \(b\): \(\text{ReIGW}_{\gamma}(\widehat{f},\gamma)[b]=1-\sum_{a\neq b}\text{ReIGW}_{ \gamma}(\widehat{f},\gamma)[a]\).

```
1:Input: number of episodes \(K\), failure probability \(\delta\), ReIGW learning rate \(\gamma\).
2:Initialize any cost distribution \(f^{(1)}\).
3:for episode \(k=1,2,\ldots,K\)do
4: Observe context \(x_{k}\).
5: Sample action \(a_{k}\sim p_{k}=\text{ReIGW}(\bar{f}^{(k)}(x_{k},\cdot),\gamma)\) from Eq. (1).
6: Observe cost \(c_{k}\sim C(x_{k},a_{k})\) and update online MLE oracle with \(((x_{k},a_{k}),c_{k})\).
7:endfor ```

**Algorithm 1** Distributional CB (DistCB)

We propose **Distributional**C**ontextual **B**andit (DistCB) in Algorithm 1, a two-step procedure for each episode \(k\in[K]\). Upon seeing context \(x_{k}\), DistCB first samples an action \(a_{k}\) from ReIGW generated by means of our estimated cost distributions for each action, _i.e._, \(\widehat{f}(a)=\bar{f}^{(k)}(x_{k},a),\forall a\in\mathcal{A}\) (Line 5). Then, DistCB updates \(f^{(k)}(\cdot\mid x_{k},a_{k})\) by maximizing the log-likelihood to estimate the conditional cost distribution \(C(\cdot\mid x_{k},a_{k})\) (Line 6). Formally, this second step is achieved via an online MLE oracle with a realizable distribution class \(\mathcal{F}_{CB}\subset\mathcal{X}\times\mathcal{A}\to\Delta([0,1])\); let \(\mathrm{Regret}_{\log}(K)\) be some upper bound on the log-likelihood regret for all possibly adaptive sequences \(\{x_{k},a_{k},c_{k}\}_{k\in[K]}\),

\[\sum_{k=1}^{K}\log C(c_{k}\mid x_{k},a_{k})-\log f^{(k)}(c_{k}\mid x_{k},a_{k}) \leq\mathrm{Regret}_{\log}(K).\]

Under _realizability_, \(C\in\mathcal{F}_{CB}\), we expect \(\mathrm{Regret}_{\log}(K)\in\mathcal{O}(\log(K))\). For instance, if \(\mathcal{F}_{CB}\) is finite, exponentially weighted average forecaster guarantees \(\mathrm{Regret}_{\log}(K)\leq\log\left|\mathcal{F}_{CB}\right|\)(Cesa-Bianchi and Lugosi, 2006; Chapter 9). We now state our main result for DistCB.

**Theorem 4.1**.: _For any \(\delta\in(0,1)\), w.p. at least \(1-\delta\), running DistCB with \(\gamma=10A\vee\sqrt{\frac{40A(C^{*}+\log(1/\delta))}{112\big{(}\mathrm{Regret}_{ \log}(K)+\log(1/\delta)\big{)}}}\) has regret scaling with \(C^{\star}=\sum_{k=1}^{K}\min_{a\in\mathcal{A}}\bar{C}(x_{k},a)\),_

\[\mathrm{Regret}_{\mathrm{DISTCB}}(K)\leq 232\sqrt{AC^{\star}\,\mathrm{Regret}_{ \log}(K)\log(1/\delta)}+2300A\big{(}\mathrm{Regret}_{\log}(K)+\log(1/\delta) \big{)}.\]

The dominant term scales with the optimal sum of costs \(\sqrt{C^{\star}}\) which shows that DistCB obtains small-loss regret. DistCB is also computationally efficient since each episode simply requires computing the ReIGW. FastCB is the only other computationally efficient CB algorithm with small-loss regret (Foster and Krishnamurthy, 2021, Theorem 1). Our bound matches that of FastCB in terms of dependence on \(A,C^{\star}\) and \(\log(1/\delta)\). Our key difference with FastCB is the online supervised learning oracle: in DistCB, we aim to learn the conditional cost distribution by maximizing log-likelihood, while FastCB aims to perform regression with the binary cross-entropy loss. In Section 7, we find that DistCB empirically outperforms SquareCB and FastCB in three challenging CB tasks, which reinforces the practical benefits of distribution learning in CB setting.

### Proof Sketch

First, apply the per-round inequality for ReIGW (Foster and Krishnamurthy, 2021, Theorem 4) to get,

\[\mathrm{Regret}_{\text{DistCB}}(K)\lesssim\sum_{k=1}^{K}\mathbb{E}_{a_{k}\sim p _{k}}\Bigg{[}\frac{A}{\gamma}\bar{C}(s_{k},a_{k})+\gamma\underbrace{\frac{ \big{(}\bar{f}^{(k)}(s_{k},a_{k})-\bar{C}(s_{k},a_{k})\big{)}^{2}}{\bar{f}^{(k )}(s_{k},a_{k})+\bar{C}(s_{k},a_{k})}}_{\bigstar}\Bigg{]}.\]

For any distributions \(f,g\in\Delta([0,1])\), their triangular discrimination3 is defined as \(D_{\triangle}(f\parallel g):=\int\frac{(f(y)-g(y))^{2}}{f(y)+g(y)}\mathrm{d} \lambda(y)\). The key insight is that \(\bigstar\) can be bounded by the triangular discrimination of \(f^{(k)}(s_{k},a_{k})\) and \(C(s_{k},a_{k})\): by Cauchy-Schwartz and \(y^{2}\leq y\) for \(y\in[0,1]\), we have \(\bar{f}-\bar{g}=\int y(f(y)-g(y))\mathrm{d}\lambda(y)\leq\sqrt{\int y(f(y)+g( y))\mathrm{d}\lambda(y)}\sqrt{\int\frac{(f(y)-g(y))^{2}}{f(y)+g(y)}\mathrm{d} \lambda(y)}\), and hence,

Footnote 3: Triangular discrimination is also known as Vincze-Le Cam divergence (Vincze, 1981, Le Cam, 2012).

\[\big{|}\bar{f}-\bar{g}\big{|}\leq\sqrt{\big{(}\bar{f}+\bar{g}\big{)}D_{ \triangle}(f\parallel g)}.\] ( \[\triangle_{1}\] )

So, Eq. (\(\triangle_{1}\)) implies that \(\bigstar\) is bounded by \(D_{\triangle}(f^{(k)}(s_{k},a_{k})\parallel C(s_{k},a_{k}))\). Since \(D_{\triangle}\) is equivalent (up to universal constants) to the squared Hellinger distance, Foster et al. (2021, Lemma A.14) implies the above can be bounded by the online MLE regret, so w.p. at least \(1-\delta\), we have

\[\mathrm{Regret}_{\text{DistCB}}(K)\lesssim\sum_{k=1}^{K}\tfrac{A}{\gamma} \big{(}\bar{C}(s_{k},a_{k})+\log(1/\delta)\big{)}+\gamma\big{(}\mathrm{Regret }_{\log}(K)+\log(1/\delta)\big{)}.\]

From here, we just need to rearrange terms and set the correct \(\gamma\). Appendix C contains the full proof.

## 5 Small-Loss Bounds for Online Distributional RL

We now extend our insights to the online RL setting and propose a DistRL perspective on GOLF (Jin et al., 2021). While GOLF constructs confidence sets of near-minimizers of the squared Bellman error loss, we propose to construct these confidence sets using near-maximizers of the log-likelihood loss to approximate MLE. To leverage function approximation for learning conditional distributions, we use a generic function class \(\mathcal{F}\subseteq(\mathcal{X}\times\mathcal{A}\rightarrow\Delta([0,1]))^{H}\) where each element \(f\in\mathcal{F}\) is a tuple \(f=(f_{1},\ldots,f_{H})\) such that each \(f_{h}\) is a candidate estimator for \(Z_{h}^{\star}\), the distribution of loss-to-go \(\sum_{t=h}^{H}c_{t}\) under \(\pi^{\star}\). For notation, \(f_{H+1}(x,a)=\delta_{0}\) denotes the dirac at zero for all \(x,a\).

We now present our **O**ptimistic **D**istributional **C**onfidence set **O**ptimization (O-DISCO) algorithm in Algorithm 2, consisting of three key steps per episode. At episode \(k\in[K]\), O-DISCO first identifies the \(f^{(k)}\) with the minimal expected value at \(h=1\) over the previous confidence set \(\mathcal{F}_{k-1}\) (Line 4). This step induces _global optimism_. Then, O-DISCO collects data for this episode by rolling in with the greedy policy \(\pi^{k}\) with respect to the mean of \(f^{(k)}\) (Line 6). Finally, O-DISCO constructs aconfidence set \(\mathcal{F}_{k}\) by including a function \(f\) if it exceeds a threshold on the log-likelihood objective using data \(z_{h,i}^{f}\sim\mathcal{T}_{h}^{\star,D}f_{h+1}(x_{h,i},a_{h,i})\) for all steps \(h\) simultaneously (Line 7). This step is called _local fitting_, as each \(f\in\mathcal{F}_{k}\) has the property that \(f_{h}\) is close-in-distribution to \(\mathcal{T}_{h}^{\star,D}f_{h+1}\) for all \(h\). We highlight that O-DISCO only learns the distribution for estimating the mean, _i.e._, Lines 4 and 6 only use the mean \(\bar{f}\). This seemingly roundabout way of estimating the mean is exactly how distributional RL algorithms such as C51 differ from the classic DQN.

To ensure that MLE succeeds for the Temporal-Difference (TD) style confidence sets, we need the following distributional Bellman Completeness (BC) condition introduced in Wu et al. (2023).

**Assumption 5.1** (Bellman Completeness).: For all \(\pi,h\in[H]\), \(f_{h+1}\in\mathcal{F}_{h+1}\implies\mathcal{T}_{h}^{\pi,D}f_{h+1}\in\mathcal{F }_{h}\).

### The \(\ell_{1}\) Distributional Eluder Dimension

We now introduce the \(\ell_{1}\) distributional eluder dimension. Let \(\mathcal{S}\) be an abstract input space, let \(\Psi\) be a set of functions mapping \(\mathcal{S}\to\mathbb{R}\) and let \(\mathcal{D}\) be a set of distributions on \(\mathcal{S}\).

**Definition 5.2** (\(\ell_{p}\)-distributional eluder dimension).: For any function class \(\Psi\subseteq\mathcal{S}\to\mathbb{R}\), distribution class \(\mathcal{D}\subseteq\Delta(\mathcal{S})\) and \(\varepsilon>0\), the \(\ell_{p}\)-distributional eluder dimension (denoted by \(\mathrm{DE}_{p}(\Psi,\mathcal{D},\varepsilon)\)) is the length \(L\) of the longest sequence \(d^{(1)},d^{(2)},\ldots,d^{(L)}\subseteq\mathcal{D}\) such that there exists \(\varepsilon^{\prime}\geq\varepsilon\), such that for all \(t\in[L]\), we have that there exists \(f\in\Psi\) such that \(|\mathbb{E}_{d^{(t)}}f|>\varepsilon\) and also \(\sum_{i=1}^{t-1}\|\mathbb{E}_{d^{(t)}}f|^{p}\leq\varepsilon^{p}\).

When \(p=2\), this is exactly the \(\ell_{2}\) distributional eluder of Jin et al. (2021a, Definition 7). We're particularly interested in the \(p=1\) case, which can be used with MLE's generalization bounds. The following is a key pigeonhole principle for the \(\ell_{1}\) distributional eluder dimension.

**Theorem 5.3**.: _Let \(C:=\sup_{d\in\mathcal{D},f\in\Psi}\|\mathbb{E}_{d}f|\) be the envelope. Fix any \(K\in\mathbb{N}\) and sequences \(f^{(1)},\ldots,f^{(K)}\subseteq\Psi\), \(d^{(1)},\ldots,d^{(K)}\subseteq\mathcal{D}\). Let \(\beta\) be a constant such that for all \(k\in[K]\), we have, \(\sum_{i=1}^{k-1}\bigl{|}\mathbb{E}_{d^{(i)}}f^{(k)}\bigr{|}\leq\beta\). Then, for all \(k\in[K]\), we have_

\[\sum_{t=1}^{k}\Bigl{|}\mathbb{E}_{d^{(t)}}f^{(t)}\Bigr{|}\leq\inf_{0<\varepsilon \leq 1}\{\mathrm{DE}_{1}(\Psi,\mathcal{D},\varepsilon)(2C+\beta\log(C/ \varepsilon))+k\varepsilon\}.\]

As we'll see later, Theorem 5.3 is the key tool that transfers triangular discrimination guarantees on the training distribution to any new test distribution. Another key property is that the \(\ell_{1}\) dimension generalizes the original \(\ell_{2}\) dimension of Jin et al. (2021a).

**Lemma 5.4**.: _For any \(\Psi,\mathcal{D}\) and \(\varepsilon>0\), we have \(\mathrm{DE}_{1}(\Psi,\mathcal{D},\varepsilon)\leq\mathrm{DE}_{2}(\Psi, \mathcal{D},\varepsilon)\)._

Finally, we note that our distributional eluder dimension generalize the regular \(\ell_{1}\) eluder from Liu et al. (2022), which can be seen by taking \(\mathcal{D}\) to be dirac distributions.

### Small-Loss Bounds for O-Disco

We will soon prove small-loss regret bounds with the "Q-type" dimension, where "Q-type" refers to the fact that \(\mathcal{S}=\mathcal{X}\times\mathcal{A}\). While low-rank MDPs are not captured by the "Q-type" dimension, they are captured by the "V-type" dimension where \(\mathcal{S}=\mathcal{X}\)(Jin et al., 2021; Du et al., 2021). For PAC bounds with the V-type dimension, we need to slightly modify the data collection process in Line 6 with _uniform action exploration_ (UAE). Instead of executing \(\pi^{k}\) for a single trajectory, partially roll-out \(\pi^{k}\) for \(H\) times where for each \(h\in[H]\), we collect \(x_{h,k}\sim d_{h}^{\pi^{k}}\), take a random action \(a_{h,k}\sim\mathrm{unif}(\mathcal{A})\), observe \(c_{h,k}\sim C_{h}(x_{h,k},a_{h,k}),x^{\prime}_{h,k}\sim P_{h}(x_{h,k},a_{h,k})\) and augment the dataset \(\mathcal{D}_{h,k}=\mathcal{D}_{h,k-1}\cup\{(x_{h,k},a_{h,k},c_{h,k},x^{\prime }_{h,k})\}\). The modified algorithm is detailed in Appendix B.

We lastly need to define the function and distribution classes measured by the distributional eluder dimension. The Q-type classes are \(\mathcal{D}_{h}=\left\{(x,a)\mapsto d_{h}^{\pi}(x,a):\pi\in\Pi\right\}\) and \(\Psi_{h}=\left\{(x,a)\mapsto D_{\triangle}(f(x,a)\parallel\mathcal{T}^{\star,D}f(x,a)):f\in\mathcal{F}\right\}\). Similarly, the V-type classes are \(\mathcal{D}_{h,v}=\left\{x\mapsto d_{h}^{\pi}(x):\pi\in\Pi\right\}\) and \(\Phi_{h,v}=\left\{x\mapsto\mathbb{E}_{a\sim\mathrm{Unif}(\mathcal{A})}[D_{ \triangle}(f(x,a)\parallel\mathcal{T}^{\star,D}f(x,a))]:f\in\mathcal{F}\right\}\). Finally, define \(\mathrm{DE}_{1}(\varepsilon)=\max_{h}\mathrm{DE}_{1}(\Psi_{h},\mathcal{D}_{h}, \varepsilon)\) and \(\mathrm{DE}_{1,v}(\varepsilon)=\max_{h}\mathrm{DE}_{1}(\Psi_{h,v},\mathcal{D} _{h,v},\varepsilon)\).

**Theorem 5.5**.: _Suppose DistBC holds (Assumption 5.1). For any \(\delta\in(0,1)\), w.p. at least \(1-\delta\), running O-DISCO with \(\beta=\log(HK|\mathcal{F}|/\delta)\) guarantees the following regret bound,_

\[\mathrm{Regret}_{\mathrm{O-DISCO}}(K)\leq 160H\sqrt{KV^{\star}\,\mathrm{DE}_{1}(1/ K)\log(K)\beta}+18000H^{2}\,\mathrm{DE}_{1}(1/K)\log(K)\beta.\]

_If \(\textsc{UAE}=\textsc{True}\) (Algorithm 4), then the learned mixture policy \(\bar{\pi}\) is guaranteed to satisfy,_

\[V^{\bar{\pi}}-V^{\star}\leq 160H\sqrt{\frac{AV^{\star}\,\mathrm{DE}_{1,v}(1/ K)\log(K)\beta}{K}}+\frac{18000H^{2}A\,\mathrm{DE}_{1,v}(1/K)\log(K)\beta}{K}.\]

Compared to prior bounds for GOLF (Jin et al., 2021), the leading \(\sqrt{K}\) terms in our bounds enjoy the same sharp dependence in \(H,K\) and the eluder dimension. Our bounds further enjoy one key improvement: the leading terms are multiplied with the instance-dependent optimal cost \(V^{\star}\), giving our bounds the _small-loss_ property. For example, if \(V^{\star}\leq\mathcal{O}(1/\sqrt{K})\), then our regret bound converges at a fast \(\mathcal{O}(H^{2}\,\mathrm{DE}_{1}(1/K)\log(K)\beta)\) rate. While there are existing first-order bounds in online RL, our bound significantly improves on their generality. For example, Zanette and Brunskill (2019); Jin et al. (2020); Wagenmaker et al. (2022) used Bernstein bonuses that scale with the conditional variance and showed that careful analysis can lead to "small-return" bounds in tabular and linear MDPs. However, "small-return" bounds do not imply "small-loss" bounds and "small-loss" bounds are often harder to obtain4. While it is possible that surgical analysis with variance bonuses can lead to small-loss bounds in tabular and linear MDPs, this approach may not scale to settings with non-linear function approximation such as low-rank MDPs.

Footnote 4: In Appendix J, we show a slight modification of our approach also yields “small-return” bounds.

On Bellman CompletenessExponential error amplification can occur in online and offline RL under only realizability of \(Q\) functions (Wang et al., 2021; 20; 20; Foster et al., 2022). With only realizability, basic algorithms such as TD and Fitted-\(Q\)-Evaluation (FQE) can diverge or converge to bad fixed point solutions (Tsitsiklis and Van Roy, 1996; Munos and Szepesvari, 2008; Kolter, 2011). As a result, BC has risen as a _de facto_ sufficient condition for sample efficient RL (Chang et al., 2022; Xie et al., 2021; Zanette et al., 2021). Finally, we highlight that our method can be easily extended to hold under _generalized completeness_, _i.e._, there exist function classes \(\mathcal{G}_{h}\) such that \(f_{h+1}\in\mathcal{F}_{h+1}\implies\mathcal{T}_{h}^{\pi,D}f_{h+1}\in\mathcal{G }_{h}\) [as in Jin et al., 2021, Assumption 14]. Simply replace \(\max_{g\in\mathcal{F}_{h}}\) in the confidence set construction with \(\max_{g\in\mathcal{G}_{h}}\). While adding functions to \(\mathcal{F}\) may break BC (as BC is not monotonic), we can always augment \(\mathcal{G}\) to satisfy generalized completeness.

Computational complexityWhen taken as is, OLIVE (Jiang et al., 2017), GOLF, and our algorithms are version space methods that suffer from a computational drawback: optimizing over the confidence set is NP-hard (Dann et al., 2018). However, the confidence set is purely for deep exploration via optimism and can be replaced by other computationally efficient exploration strategies. For example, \(\varepsilon\)-greedy suffices in problems that don't require deep and strategic exploration, _i.e._, a large myopic exploration gap (Dann et al., 2022). With \(\varepsilon\)-greedy, a replay buffer, and discretization, our algorithm essentially recovers C51 (Bellemare et al., 2017). We leave developing and analyzing computationally efficient algorithms based on our insights as promising future work.

### Instantiation with Low-Rank MDPs

The low-rank MDP (Agarwal et al., 2020) is a standard abstraction for non-linear function approximation used in theory (Uehara et al., 2021) and practice (Zhang et al., 2022, Chang et al., 2022).

**Definition 5.6** (Low-rank MDP).: A transition model \(P_{h}:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{X})\) has rank \(d\) if there exist unknown features \(\phi_{h}^{\star}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d},\mu_{h}^{ \star}:\mathcal{X}\to\mathbb{R}^{d}\) such that \(P_{h}(x^{\prime}\mid x,a)=\phi_{h}^{\star}(x,a)^{\top}\mu_{h}^{\star}(x^{\prime})\) for all \(x,a,x^{\prime}\). Also, assume \(\max_{x,a}\|\phi_{h}^{\star}(x,a)\|_{2}\leq 1\) and \(\|\int g\mathrm{d}\mu_{h}^{\star}\|_{2}\leq\|g\|_{\infty}\sqrt{d}\) for all functions \(g:\mathcal{X}\to\mathbb{R}\). The MDP is called low-rank if \(P_{h}\) is low-rank for all \(h\in[H]\).

We now specialize Theorem 5.5 to low-rank MDPs with three key steps. First, we bound the V-type eluder dimension by \(\mathrm{DE}_{1,v}(\varepsilon)\leq\mathcal{O}(d\log(d/\varepsilon))\), which is a known result that we reproduce in Theorem G.4. The next step requires access to a realizable \(\Phi\) class, _i.e._, for all \(h\in[H]\), \(\phi_{h}^{\star}\in\Phi\), which is a standard assumption for low-rank MDPs (Agarwal et al., 2020, Uehara et al., 2021, Mhammedi et al., 2023). Given the realizable \(\Phi\), we can construct a specialized \(\mathcal{F}\) for the low-rank MDP: \(\mathcal{F}^{\mathrm{lin}}=\mathcal{F}_{1}^{\mathrm{lin}}\times\cdots\times \mathcal{F}_{H}^{\mathrm{lin}}\times\mathcal{F}_{H+1}^{\mathrm{lin}}\) where \(\mathcal{F}_{H+1}^{\mathrm{lin}}=\{\delta_{0}\}\) and for all \(h\in[H]\),

\[\mathcal{F}_{h}^{\mathrm{lin}}=\bigg{\{} f(z\mid x,a)=\big{\langle}\phi(x,a),w(z)\big{\rangle}\quad:\quad \phi\in\Phi,w:[0,1]\to\mathbb{R}^{d},\] (2) s.t. \[\max_{z}\|w(z)\|_{2}\leq\alpha\sqrt{d}\;\text{ and }\max_{x,a,z}\big{\langle}\phi(x,a),w(z)\big{\rangle}\leq\alpha\bigg{\}},\]

where \(\alpha:=\max_{h,\pi,z,x,a}Z_{h}^{\pi}(z\mid x,a)\) is the largest mass for the cost-to-go distributions. In Appendix D, we show that \(\mathcal{F}^{\mathrm{lin}}\) satisfies DistBC. Further, if costs are discretized into a uniform grid of \(M\) points, its bracketing entropy is bounded by \(\widetilde{\mathcal{O}}(dM+\log|\Phi|)\). Discretization is necessary to bound the statistical complexity of \(\mathcal{F}^{\mathrm{lin}}\) and is common in practice, _e.g._, C51 and Rainbow both set \(M=51\) which works well in Atari games (Bellemare et al., 2017, Hessel et al., 2018).

**Theorem 5.7**.: _Suppose the MDP is low-rank. For any \(\delta\in(0,1)\), w.p. at least \(1-\delta\), running O-DISCO with UAE=True and with \(\mathcal{F}^{\mathrm{lin}}\) as described above learns a policy \(\bar{\pi}\) such that,_

\[V^{\bar{\pi}}-V^{\star}\in\widetilde{\mathcal{O}}\Bigg{(}H\sqrt{\frac{\text{ Ad}V^{\star}(dM+\log(|\Phi|/\delta))}{K}}+\frac{\text{Ad}H^{2}(dM+\log(|\Phi|/ \delta))}{K}\Bigg{)}.\]

Proof.: As described above, we have \(\mathrm{DE}_{1}(1/K)\leq\mathcal{O}(d\log(dK))\) and \(\beta=\log(HK/\delta)+dM+\log|\Phi|\). Since DistBC is satisfied by \(\mathcal{F}^{\mathrm{lin}}\), plugging into Theorem 5.5 gives the result. 

This is the first small-loss bound for low-rank MDPs, and for online RL with non-linear function approximation in general. Again when \(V^{\star}\leq\widetilde{\mathcal{O}}(1/K)\), O-DISCO has a fast \(\widetilde{\mathcal{O}}(1/K)\) convergence rate which improves over all prior results that converge at a slow \(\widetilde{\Omega}(1/\sqrt{K})\) rate (Uehara et al., 2021).

### Proof Sketch of Theorem 5.5

By DistBC (Assumption 5.1), we can deduce two facts about the construction of \(\mathcal{F}_{k}\): (i) \(Z^{\star}\in\mathcal{F}_{k}\), and (ii) elements of \(\mathcal{F}_{k}\) almost satisfy the distributional Bellman equation, _i.e._, for all \(h\in[H]\), we have \(\sum_{i=1}^{k}\mathbb{E}_{\pi^{i}}[\delta_{h,k}(x_{h},a_{h})]\leq\mathcal{O}(\beta)\) where \(\delta_{h,k}(x_{h},a_{h})=D_{\triangle}(f_{h}^{(k)}(x_{h},a_{h})\parallel \mathcal{T}_{h}^{\star,D}f_{h+1}^{(k)}(x_{h},a_{h}))\). Next, we derive a corollary of Eq. (\(\triangle_{1}\)):

\[\big{|}\bar{f}-\bar{g}\big{|}\leq\sqrt{4\bar{g}+D_{\triangle}(f\parallel g)}\cdot \sqrt{D_{\triangle}(f\parallel g)}.\] ( \[\triangle_{2}\] )

To see why this is true, apply AM-GM to Eq. (\(\triangle_{1}\)) to get \(2(\bar{f}-\bar{g})\leq\bar{f}+\bar{g}+D_{\triangle}(f\parallel g)\), which simplifies to \(\bar{f}\leq 3\bar{g}+D_{\triangle}(f\parallel g)\). Plugging this back into Eq. (\(\triangle_{1}\)) yields Eq. (\(\triangle_{2}\)). Then, by iterating Eq. (\(\triangle_{2}\)) and AM-GM, we derive a self-bounding lemma: for any \(f,\pi,h\), we have \(\bar{f}_{h}(x_{h},a_{h})\lesssim Q_{h}^{\pi}(x_{h},a_{h})+H\sum_{t=h}^{H} \mathbb{E}_{\pi,x_{h},a_{h}}[D_{\triangle}(f_{t}(x_{t},a_{t})\parallel\mathcal{T} _{h}^{\pi,D}f_{t+1}(x_{t},a_{t}))]\) (Lemma H.3).

Since \(\mathcal{T}_{h}^{\pi^{k}}\bar{f}^{(k)}_{h+1}(x,a)=\mathcal{T}_{h}^{\pi^{k},D}f^{(k )}_{h+1}(x,a)\) and \(\mathcal{T}_{h}^{\pi^{k},D}f^{(k)}_{h+1}=\mathcal{T}_{h}^{\star,D}f^{(k)}_{h+1}\), we have

\[V^{\pi^{k}}-V^{\star} \leq V^{\pi^{k}}-\bar{f}^{(k)}_{1}(x_{1},\pi^{k}_{1}(x_{1}))\] (optimism from fact (i)) \[=\sum_{h=1}^{H}\mathbb{E}_{\pi^{k}}\Big{[}\mathcal{T}_{h}^{\pi^{k }}\bar{f}^{(k)}_{h+1}(x_{h},a_{h})-\bar{f}^{(k)}_{h}(x_{h},a_{h})\Big{]} \text{(performance difference)}\] \[\leq 2\sum_{h=1}^{H}\sqrt{\mathbb{E}_{\pi^{k}}[\bar{f}^{(k)}_{h}( x_{h},a_{h})+\delta_{h,k}(x_{h},a_{h})]}\sqrt{\mathbb{E}_{\pi^{k}}[\delta_{h,k}(x_{h},a_{h})]}\] (Eq. ( \[\triangle_{2}\] ) \[\lesssim\sqrt{V^{\pi^{k}}w+H\sum_{h=1}^{H}\mathbb{E}_{\pi^{k}}[ \delta_{h,k}(x_{h},a_{h})]}\sqrt{H\mathbb{E}_{\pi^{k}}[\delta_{h,k}(x_{h},a_{h })]}.\] (Lemma H.3 )

The implicit inequality \(V^{\pi^{k}}-V^{\star}\lesssim\sqrt{V^{\star}+H\sum_{h=1}^{H}\mathbb{E}_{\pi^{ k}}[\delta_{h,k}(x_{h},a_{h})]}\sqrt{H\mathbb{E}_{\pi^{k}}[\delta_{h,k}(x_{h},a_{h })]}\) can then be obtained by AM-GM and rearranging. The final step is to sum over \(k\) and bound \(\sum_{k=1}^{K}\mathbb{E}_{\pi^{k}}[\delta_{h,k}(x_{h},a_{h})]\) via the eluder dimension's pigeonhole principle (Theorem 5.3 applied with fact (ii)). Please see Appendix H for the full proof.

## 6 Small-Loss Bounds for Offline Distributional RL

We now propose **P**essimistic **D**n**f**ubi**tional **C**onfidence set **O**ptimization (P-DISCO; Algorithm 3), which adapts the distributional confidence set technique from the previous section to the offline setting by leveraging pessimism instead of optimism. Notably, P-DISCO is a simple two-step algorithm that achieves the first small-loss PAC bounds in offline RL. First, construct a distributional confidence set for each policy \(\pi\) based on a similar log-likelihood thresholding procedure as in O-DISCO, where the difference is we now use data sampled from \(\mathcal{T}_{h}^{\pi,D}f_{h+1}\) instead of \(\mathcal{T}_{h}^{\star,D}f_{h+1}\). Next, output the policy with the most pessimistic mean amongst all the confidence sets.

```
1:Input: datasets \(\mathcal{D}_{1},\dots,\mathcal{D}_{H}\), distribution function class \(\mathcal{F}\), threshold \(\beta\), policy class \(\Pi\).
2: For all \((h,f,\pi)\in[H]\times\mathcal{F}\times\Pi\), sample \(y^{f,\pi}_{h,i}\sim f_{h+1}(x^{\prime}_{h,i},\pi_{h+1}(x^{\prime}_{h,i}))\), where \((x_{h,i},a_{h,i},c_{h,i},x^{\prime}_{h,i})\) is the \(i\)-th datapoint of \(\mathcal{D}_{h}\). Then, set \(z^{f,\pi}_{h,i}=c_{h,i}+y^{f,\pi}_{h,i}\) and define the confidence set, \[\mathcal{F}_{\pi}=\Bigg{\{}f\in\mathcal{F}:\sum_{i=1}^{N}\log f_{h}(z^{f,\pi}_ {h,i}\mid x_{h,i},a_{h,i})\geq\max_{g\in\mathcal{F}_{h}}\sum_{i=1}^{N}\log g(z^ {f,\pi}_{h,i}\mid x_{h,i},a_{h,i})-7\beta,\forall h\in[H]\Bigg{\}}.\]
3: For each \(\pi\in\Pi\), define the pessimistic estimate \(f^{\pi}=\arg\max_{f\in\mathcal{F}_{\pi}}\mathbb{E}_{a\sim\pi(x_{1})}\big{[} \bar{f}_{1}(x_{1},a)\big{]}\).
4:Output:\(\widetilde{\pi}=\arg\max_{\pi\in\Pi}\mathbb{E}_{a\sim\pi(x_{1})}\big{[}\bar{f} ^{\pi}_{1}(x_{1},\pi)\big{]}\). ```

**Algorithm 3**P**essimistic **D**n**l**n**fubi**tional **C**onfidence set **O**ptimization (P-DISCO)

In offline RL, many works made strong all-policy coverage assumptions (Antos et al., 2008; Chen and Jiang, 2019). Recent advancements (Kidambi et al., 2020; Xie et al., 2021; Uehara and Sun, 2022; Rashidinejad et al., 2021; Jin et al., 2021b) have pursued _best effort_ guarantees that aim to compete with any covered policy \(\widetilde{\pi}\), with sub-optimality of the learned \(\widetilde{\pi}\) degrading gracefully as coverage worsens. The coverage is measured by the single-policy concentrability \(C^{\widetilde{\pi}}=\max_{h}\norm{\norm{d}^{d}_{h}/d\nu_{h}}_{\infty}\). We adopt this framework and obtain the first small-loss PAC bound in offline RL.

**Theorem 6.1** (Small-Loss PAC bound for P-DISCO).: _Assume Assumption 5.1. For any \(\delta\in(0,1)\), w.p. at least \(1-\delta\), running P-DISCO with \(\beta=\log(H|\Pi||\mathcal{F}|/\delta)\) learns a policy \(\widetilde{\pi}\) that enjoys the following PAC bound with respect to any comparator policy \(\widetilde{\pi}\in\Pi\):_

\[V^{\widetilde{\pi}}-V^{\widetilde{\pi}}\leq 9H\sqrt{\frac{C^{\widetilde{\pi}}V^{ \widetilde{\pi}}\beta}{N}}+\frac{30H^{2}C^{\widetilde{\pi}}\beta}{N}.\]

To the best of our knowledge, this is the first small-loss bound for offline RL, which we highlight illustrates a novel robustness property against bad coverage. Namely, the dominant term not only scales with the coverage coefficient \(C^{\widetilde{\pi}}\) but also the comparator policy's value \(V^{\widetilde{\pi}}\). In particular, P-DISCO can strongly compete with a comparator policy \(\widetilde{\pi}\) if _one of the following_ is true: (i) \(\nu\) has good coverage over \(\widetilde{\pi}\), so the \(\mathcal{O}(1/\sqrt{N})\) term is manageable; _or_ (ii) \(\widetilde{\pi}\) has small-loss, in which case we may even obtain a fast \(\mathcal{O}(1/N)\) rate. Thus, P-DISCO has _two_ chances at strongly competing with \(\widetilde{\pi}\), while conventional offline RL methods solely rely on (i) to be true.

## 7 Distributional CB Experiments

We now compare DistCB with SquareCB (Foster and Rakhlin, 2020) and the state-of-the-art CB method FastCB (Foster and Krishnamurthy, 2021), which respectively minimize the squared loss and log loss for estimating the conditional mean. The key question we investigate here is whether learning the conditional mean via distribution learning with MLE will demonstrate empirical benefit over the non-distributional approaches. We consider three challenging tasks that are all derived from real-world datasets and we briefly describe the construction below.

King County HousingThis dataset consists of home features and prices, which we normalize to be in \([0,1]\). The action space is \(100\) evenly spaced prices between \(0.01\) and \(1.0\). If the learner overpredicts the true price, the cost is \(1.0\). Else, the cost is \(1.0\) minus predicted price.

Prudential Life InsuranceThis dataset contains customer features and an integer risk level in \([8]\), which is our action space. If the model overpredicts the risk level, the cost is \(1.0\). Otherwise, the cost is \(.1\times(y-\hat{y})\) where \(y\) is the actual risk level, and \(\hat{y}\) is the predicted risk level.

Cifar-100This popular image dataset contains \(100\) classes, which correspond to our actions, and each class is in one of \(20\) superclasses. We assign cost as follows: \(0.0\) for predicting the correct class, \(0.5\) for the wrong class but correct superclass, and \(1.0\) for a fully incorrect prediction.

ResultsAcross tasks, DistCB achieves lower average cost over all episodes (_i.e._, normalized regret) and over the last \(100\) episodes (_i.e._, most updated policies' performance) compared to SquareCB. This indicates the empirical benefit of the distributional approach over the conventional approach based on least square regression, matching the theoretical benefit demonstrated here. Perhaps surprisingly, DistCB also consistently outperforms FastCB. Both methods obtain first-order bounds with the same dependencies on \(A\) and \(C^{\star}\), which suggests that DistCB's empirical improvement over FastCB cannot be fully explained by existing theory. The only difference between DistCB and FastCB is that the former integrates online MLE while the latter directly estimates the mean by minimizing the log loss (binary cross-entropy). An even more fine-grained understanding of the benefits of distribution learning may therefore be helpful in explaining this improvement. Appendix K contains all experiment details. Reproducible code is available at https://github.com/kevinzhou497/distcb.

## 8 Conclusion

We showed that distributional RL leads to small-loss bounds in both online and offline RL, and we also proposed a distributional CB algorithm that outperforms the state-of-the-art FastCB. A fruitful direction would be to investigate connections of natural policy gradient with our MLE distributional-fitting scheme to inspire a practical offline RL algorithm with small loss guarantees, _a la_Cheng et al. (2022). Finally, it would be interesting to investigate other loss functions that yield small-loss or even faster bounds.

AcknowledgementsThis material is based upon work supported by the National Science Foundation under Grant Nos. IIS-1846210 and IIS-2154711.

\begin{table}
\begin{tabular}{l c c} \hline \hline Algorithm: & SquareCB & FastCB & DistCB (Ours) \\ \hline King County Housing (Vanschoren et al., 2013) & & \\ \hline All episodes.756 (.0007).734 (.0007) & **.726** (.0003) \\ Last 100 ep..725 (.0012).719 (.0013) & **.708** (.0019) \\ \hline \hline Prudential Life Insurance (Montoya et al., 2015) & & \\ \hline All episodes.456 (.0082),491 (.0029) & **.411** (.0038) \\ Last 100 ep..481 (.0185).474 (.0111) & **.388** (.0086) \\ \hline \hline CIFAR-100 (Krizhevsky, 2009) & & \\ \hline All episodes.872 (.0010).856 (.0016) & **.838** (.0021) \\ Last 100 ep..828 (.0024).793 (.0031) & **.775** (.0027) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Avg cost over all episodes and last 100 episodes (lower is better). We report ‘mean (sem)’ over \(10\) seeds.

## References

* Agarwal et al. (2017) Alekh Agarwal, Akshay Krishnamurthy, John Langford, Haipeng Luo, et al. Open problem: First-order regret bounds for contextual bandits. In Conference on Learning Theory, pp. 4-7. PMLR, 2017.
* Agarwal et al. (2020) Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. Advances in neural information processing systems33, pp. 20095-20107, 2020.
* Agarwal et al. (2023) Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Provable benefits of representational transfer in reinforcement learning. In The Thirty Sixth Annual Conference on Learning Theory, pp. 2114-2187. PMLR, 2023.
* Allen-Zhu et al. (2018) Zeyuan Allen-Zhu, Sebastien Bubeck, and Yuanzhi Li. Make the minority great again: First-order regret bound for contextual bandits. In International Conference on Machine Learning, pp. 186-194. PMLR, 2018.
* Antos et al. (2008) Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning71:89-129, 2008.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing32(1):48-77, 2002.
* Azar et al. (2017) Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pp. 263-272. PMLR, 2017.
* Barth-Maron et al. (2018) Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributional policy gradients. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SyZip2bCb.
* Bellemare et al. (2017) Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement learning. In International conference on machine learning, pp. 449-458. PMLR, 2017.
* Bellemare et al. (2020) Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Subhodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature588(7836):77-82, 2020.
* Bodnar et al. (2020) Cristian Bodnar, Adrian Li, Karol Hausman, Peter Pastor, and Mrinal Kalakrishnan. Quantile qt-opt for risk-aware vision-based robotic grasping. Robotics: Science and Systems, 2020.
* Cesa-Bianchi and Lugosi (2006) Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* Chang et al. (2022) Jonathan Chang, Kaiwen Wang, Nathan Kallus, and Wen Sun. Learning bellman complete representations for offline policy evaluation. In International Conference on Machine Learning, pp. 2938-2971. PMLR, 2022.
* Chen and Jiang (2019) Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning, pp. 1042-1051. PMLR, 2019.
* Cheng et al. (2022) Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning, pages 3852-3878. PMLR, 2022.
* Dabney et al. (2018a) Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pages 1096-1105. PMLR, 2018a.
* Dabney et al. (2018b) Will Dabney, Mark Rowland, Marc Bellemare, and Remi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018b.
* Dwork et al. (2018b)Chris Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Guarantees for epsilon-greedy reinforcement learning with function approximation. In _International Conference on Machine Learning_, pages 4666-4689. PMLR, 2022.
* Dann et al. (2018) Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient pac rl with rich observations. _Advances in neural information processing systems_, 31, 2018.
* Du et al. (2021) Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_, pages 2826-2836. PMLR, 2021.
* Farsang et al. (2022) Monika Farsang, Paul Mineiro, and Wangda Zhang. Conditionally risk-averse contextual bandits. _arXiv preprint arXiv:2210.13573_, 2022.
* Fawzi et al. (2022) Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammad Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. _Nature_, 610(7930):47-53, 2022.
* Foster and Rakhlin (2020) Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 3199-3210. PMLR, 2020.
* Foster and Krishnamurthy (2021) Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. _Advances in Neural Information Processing Systems_, 34:18907-18919, 2021.
* Foster et al. (2016) Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. _Advances in Neural Information Processing Systems_, 29, 2016.
* Foster et al. (2021) Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* Foster et al. (2022) Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Offline reinforcement learning: Fundamental barriers for value function approximation. In _Conference on Learning Theory_, pages 3489-3489. PMLR, 2022.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016. doi: 10.1109/CVPR.2016.90.
* Hessel et al. (2018) Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Jiang et al. (2017) Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2017.
* Jin et al. (2020a) Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In _International Conference on Machine Learning_, pages 4870-4879. PMLR, 2020a.
* Jin et al. (2020b) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020b.
* Jin et al. (2021) Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418, 2021a.
* Jin et al. (2020)Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline r!? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021b.
* Kakade et al. (2020) Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Information theoretic regret bounds for online nonlinear control. _Advances in Neural Information Processing Systems_, 33:15312-15325, 2020.
* Kakade (2001) Sham M Kakade. A natural policy gradient. _Advances in neural information processing systems_, 14, 2001.
* Kallus et al. (2022) Nathan Kallus, Xiaojie Mao, Kaiwen Wang, and Zhengyuan Zhou. Doubly robust distributionally robust off-policy evaluation and learning. In _International Conference on Machine Learning_, pages 10598-10632. PMLR, 2022.
* Kidambi et al. (2020) Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. _Advances in neural information processing systems_, 33:21810-21823, 2020.
* Kolter (2011) J Kolter. The fixed points of off-policy td. _Advances in Neural Information Processing Systems_, 24, 2011.
* Krizhevsky (2009) Alex Krizhevsky. _Learning Multiple Layers of Features from Tiny Images_. 2009.
* Le Cam (2012) Lucien Le Cam. _Asymptotic methods in statistical decision theory_. Springer Science & Business Media, 2012.
* Lee et al. (2020) Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang. Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. _Advances in neural information processing systems_, 33:15522-15533, 2020.
* Liu et al. (2022) Qinghua Liu, Alan Chung, Csaba Szepesvari, and Chi Jin. When is partially observable reinforcement learning not scary? In _Conference on Learning Theory_, pages 5175-5220. PMLR, 2022.
* Lykouris et al. (2022) Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Small-loss bounds for online learning with partial information. _Mathematics of Operations Research_, 47(3):2186-2218, 2022.
* Lyle et al. (2019) Clare Lyle, Marc G Bellemare, and Pablo Samuel Castro. A comparative analysis of expected and distributional reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4504-4511, 2019.
* Ma et al. (2021) Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative offline distributional reinforcement learning. _Advances in Neural Information Processing Systems_, 34:19235-19247, 2021.
* Mhammedi et al. (2023) Zakaria Mhammedi, Adam Block, Dylan J Foster, and Alexander Rakhlin. Efficient model-free exploration in low-rank mdps. _arXiv preprint arXiv:2307.03997_, 2023.
* Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* Montoya et al. (2015) Anna Montoya, BigJek14, Bull, denisedunleavy, egrad, FleetwoodHack, Imbayoh, PadraicS, Pru_Admin, pittman, and Will Cukierski. Prudential life insurance assessment, 2015. URL https://kaggle.com/competitions/prudential-life-insurance-assessment.
* Morimura et al. (2012) Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka. Parametric return density estimation for reinforcement learning. _arXiv preprint arXiv:1203.3497_, 2012.
* Munos and Szepesvari (2008) Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. _Journal of Machine Learning Research_, 9(5), 2008.
* Neu (2015) Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In _Conference on Learning Theory_, pages 1360-1375. PMLR, 2015.
* Nester et al. (2015)Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34:11702-11716, 2021.
* Rowland et al. (2018) Mark Rowland, Marc Bellemare, Will Dabney, Remi Munos, and Yee Whye Teh. An analysis of categorical distributional reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 29-37. PMLR, 2018.
* Rowland et al. (2023) Mark Rowland, Remi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc G Bellemare, and Will Dabney. An analysis of quantile temporal-difference learning. _arXiv preprint arXiv:2301.04462_, 2023.
* Russo and Van Roy (2013) Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Tsitsiklis and Van Roy (1996) John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffreference learning with function approximation. _Advances in neural information processing systems_, 9, 1996.
* Uehara and Sun (2022) Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=typJsbKAe6.
* Uehara et al. (2021) Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline rl in low-rank mdps. In _International Conference on Learning Representations_, 2021.
* van de Geer (2000) Sara van de Geer. _Empirical Processes in M-estimation_, volume 6. Cambridge university press, 2000.
* Vanschoren et al. (2013) Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: Networked science in machine learning. _SIGKDD Explorations_, 15(2):49-60, 2013. doi: 10.1145/2641190.2641198. URL http://doi.acm.org/10.1145/2641190.2641198.
* Vincze (1981) Istvan Vincze. On the concept and measure of information contained in an observation. In _Contributions to Probability_, pages 207-214. Elsevier, 1981.
* Wagenmaker et al. (2022) Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. First-order regret in reinforcement learning with linear function approximation: A robust estimation approach. In _International Conference on Machine Learning_, pages 22384-22429. PMLR, 2022.
* Wang et al. (2023) Kaiwen Wang, Nathan Kallus, and Wen Sun. Near-minimax-optimal risk-sensitive reinforcement learning with cvar. _International Conference on Machine Learning_, 2023.
* Wang et al. (2021a) Ruosong Wang, Dean Foster, and Sham M. Kakade. What are the statistical limits of offline RL with linear function approximation? In _International Conference on Learning Representations_, 2021a. URL https://openreview.net/forum?id=30EvkP2aQLD.
* Wang et al. (2021b) Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham Kakade. Instabilities of offline rl with pre-trained neural representation. In _International Conference on Machine Learning_, pages 10948-10960. PMLR, 2021b.
* Wang et al. (2021c) Yuanhao Wang, Ruosong Wang, and Sham Kakade. An exponential lower bound for linearly realizable mdp with constant suboptimality gap. _Advances in Neural Information Processing Systems_, 34:9521-9533, 2021c.
* Wu et al. (2023) Runzhe Wu, Masatoshi Uehara, and Wen Sun. Distributional offline policy evaluation with predictive error guarantees. _International Conference of Machine Learning_, 2023.
* Xie et al. (2021) Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34:6683-6694, 2021.
* Xie et al. (2023) Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M. Kakade. The role of coverage in online reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=LQIjzPdDt3q.

Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized quantile function for distributional reinforcement learning. _Advances in neural information processing systems_, 32, 2019.
* Zanette and Brunskill (2019) Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.
* Zanette et al. (2021) Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in neural information processing systems_, 34:13626-13640, 2021.
* Zhang et al. (2022) Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In _International Conference on Machine Learning_, pages 26447-26466. PMLR, 2022.
* Zhang (2023) Tong Zhang. _Mathematical Analysis of Machine Learning Algorithms_. 2023. http://www.tongzhang-ml.org/lt-book.html.
* Zhou et al. (2023) Zhengyuan Zhou, Susan Athey, and Stefan Wager. Offline multi-action policy learning: Generalization and optimization. _Operations Research_, 71(1):148-183, 2023.

## Appendix A Notations

### Statistical Distances

Let \(f,g\) be distributions over \(\mathcal{Y}\). Then,

\[D_{\triangle}(f\parallel g)=\sum_{y}\frac{\left(f(y)-g(y)\right) ^{2}}{f(y)+g(y)},\] \[H(f\parallel g)=\sqrt{\frac{1}{2}\sum_{y}\Bigl{(}\sqrt{f(y)}- \sqrt{g(y)}\Bigr{)}^{2}},\] \[D_{KL}(f\parallel g)=\sum_{y}f(y)\log(f(y)/g(y)),\] \[D_{TV}(f\parallel g)=\frac{1}{2}\sum_{y}|f(y)-g(y)|.\]

The following standard inequalities will be helpful:

\[H^{2}\leq D_{TV}\leq\sqrt{2}H,\] \[2H^{2}\leq D_{\triangle}\leq 4H^{2},\] (Lemma A.1)

**Lemma A.1**.: _For any distributions \(f,g\), we have \(2H^{2}(f\parallel g)\leq D_{\triangle}(f\parallel g)\leq 4H^{2}(f\parallel g)\)._

Proof.: Recall that

\[D_{\triangle}(f\parallel g)=\int_{y}\biggl{(}\frac{f(y)-g(y)}{ \sqrt{f(y)}+g(y)}\biggr{)}^{2}.\]

Applying \(\frac{1}{\sqrt{f(y)}+\sqrt{g(y)}}\leq\frac{1}{\sqrt{f(y)}+g(y)}\leq\frac{\sqrt {2}}{\sqrt{f(y)}+\sqrt{g(y)}}\) concludes the proof.

\begin{table}
\begin{tabular}{l|l} \(\mathcal{S},\mathcal{A},A\) & State and action spaces, and \(A=|\mathcal{A}|\). \\ \(\Delta(S)\) & The set of distributions supported by \(S\). \\ \(\bar{d}\) & The expectation of any real-valued distribution \(d\), _i.e._, \(\bar{d}=\mathbb{E}_{y\sim d}[y]\). \\ \([N]\) & \(\{1,2,\ldots,N\}\) for any natural number \(N\). \\ \(Z_{h}^{\pi}(x,a)\) & Distribution of \(\sum_{t=h}^{H}c_{t}\) given \(x_{h}=x,a_{h}=a\) rolling in from \(\pi\). \\ \(Q_{h}^{\pi}(x,a),V_{h}^{\pi}(x)\) & \(Q_{h}^{\pi}(x,a)=Z_{h}^{\pi}(x,a)\) and \(V_{h}^{\pi}=\mathbb{E}_{a\sim\pi(x)}[Q_{h}^{\pi}(x,a)]\). \\ \(\pi^{\star}\) & Optimal policy, _i.e._, \(\pi^{\star}=\arg\min_{\pi}V_{1}^{\pi}(x_{1})\). \\  & Without loss of optimality, we take \(\pi^{\star}:\mathcal{X}\rightarrow\mathcal{A}\) to be Markov \& deterministic. \\ \(Z_{h}^{\star},Q_{h}^{\star},V_{h}^{\star}\) & \(Z_{h}^{\pi},Q_{h}^{\pi},V_{h}^{\pi}\) with \(\pi=\pi^{\star}\), the optimal policy. \\ \(\mathcal{T}_{h}^{\pi},\mathcal{T}_{h}^{\star}\) & The Bellman operators that act on functions. \\ \(\mathcal{T}_{h}^{\pi,D},\mathcal{T}_{h}^{\star,D}\) & The distributional Bellman operators that act on conditional distributions. \\ \(V^{\pi},Z^{\pi},V^{\star},Z^{\star}\) & \(V^{\pi}=V_{1}^{\pi}(x_{1})\), \(Z^{\pi}=Z_{1}^{\pi}(x_{1})\). \(V^{\star},Z^{\star}\) are defined similarly with \(\pi^{\star}\). \\ \(d_{h}^{\pi}(x,a)\) & The probability of \(\pi\) visiting \((x,a)\) at time \(h\). \\ \(C^{\bar{\pi}}\) & Coverage coefficient \(\max_{h}\bigl{\|}\left\|d_{h}^{d_{h}^{\bar{d}}/d_{\mathcal{U}_{h}}}\right\|_{ \infty}\). \\ \(D_{\triangle}(f\parallel g)\) & Triangular discrimination between \(f,g\). \\ \(H(f\parallel g)\) & Hellinger distance between \(f,g\). \\ \(D_{KL}(f\parallel g)\) & KL divergence between \(f,g\). \\ \end{tabular}
\end{table}
Table 2: List of NotationsModified Algorithms with UAE and for Small Returns Bounds

In this section, we present the O-DISCO algorithm with Uniform Action Exploration (UAE). We also present versions of O-DISCO and P-DISCO for the reward-maximizing setting (instead of the cost-minimizing setting studied throughout the paper); if SmallReturn is turned on, we can derive small-return bounds in Appendix J.

```
1:Input: number of episodes \(K\), distribution function class \(\mathcal{F}\), threshold \(\beta\), flag UAE, flag SmallReturn.
2:Initialize \(\mathcal{D}_{h,0}\leftarrow\emptyset\) for all \(h\in[H]\), and set \(\mathcal{F}_{0}=\mathcal{F}\).
3:Set \(\mathrm{op}=\max\) if SmallReturn else\(\mathrm{op}=\min\).
4:for episode \(k=1,2,\ldots,K\)do
5: Set \(f^{(k)}=\arg\mathrm{op}_{f\in\mathcal{F}_{k-1}}\mathrm{op}_{\mathrm{g}}\, \bar{f}_{1}(x_{1},a)\).
6: Set \(\pi^{k}_{h}(x)=\arg\mathrm{op}_{a}\,\bar{f}^{(k)}_{h}(x,a)\).
7:ifUAEthen
8: For each \(h\in[H]\), collect \(x_{h,k}\sim d_{h}^{\pi^{k}},a_{h,k}\sim\mathrm{unif}(\mathcal{A}),c_{h,k}\sim C _{h}(x_{h,k},a_{h,k}),x^{\prime}_{h,k}\sim P_{h}(x_{h,k},a_{h,k})\), and augment the dataset \(\mathcal{D}_{h,k}=\mathcal{D}_{h,k-1}\cup\Big{\{}(x_{h,k},a_{h,k},c_{h,k},x^{ \prime}_{h,k})\Big{\}}\).
9:else
10: Roll out \(\pi^{k}\) and obtain a trajectory \(x_{1,k},a_{1,k},c_{1,k},\ldots,x_{H,k},a_{H,k},c_{H,k}\).
11: For each \(h\in[H]\), augment the dataset \(\mathcal{D}_{h,k}=\mathcal{D}_{h,k-1}\cup\{(x_{h,k},a_{h,k},c_{h,k},x_{h+1,k})\}\).
12:endif
13: For all \((h,f)\in[H]\times\mathcal{F}\), sample \(y^{f}_{h,i}\sim f_{h+1}(x^{\prime}_{h,i},a^{\prime})\) and \(a^{\prime}=\arg\mathrm{op}_{a}\,\bar{f}_{h+1}(x^{\prime}_{h,i},a)\), where \((x_{h,i},a_{h,i},c_{h,i},x^{\prime}_{h,i})\) is the \(i\)-th datapoint of \(\mathcal{D}_{h,k}\). Also, set \(z^{f}_{h,i}=c_{h,i}+y^{f}_{h,i}\) and define the confidence set, \[\mathcal{F}_{k}=\Bigg{\{}f\in\mathcal{F}:\sum_{i=1}^{k}\log f_{h}(z^{f}_{h,i} \mid x_{h,i},a_{h,i})\geq\max_{\bar{f}\in\mathcal{F}}\sum_{i=1}^{k}\log\widetilde {f}_{h}(z^{f}_{h,i}\mid x_{h,i},a_{h,i})-7\beta,\forall h\in[H]\Bigg{\}}.\]
14:endfor
15:Output:\(\bar{\pi}=\mathrm{unif}(\pi^{1:K})\). ```

**Algorithm 4** O-DISCO (with UAE and small return)

```
1:Input: datasets \(\mathcal{D}_{1},\ldots,\mathcal{D}_{H}\), distribution function class \(\mathcal{F}\), threshold \(\beta\), policy class \(\Pi\), flag SmallReturn.
2:For all \((h,f,\pi)\in[H]\times\mathcal{F}\times\Pi\), sample \(y^{f,\pi}_{h,i}\sim f_{h+1}(x^{\prime}_{h,i},\pi_{h+1}(x^{\prime}_{h,i}))\), where \((x_{h,i},a_{h,i},c_{h,i},x^{\prime}_{h,i})\) is the \(i\)-th datapoint of \(\mathcal{D}_{h}\). Then, set \(z^{f,\pi}_{h,i}=c_{h,i}+y^{f,\pi}_{h,i}\) and define the confidence set, \[\mathcal{F}_{\pi}=\Bigg{\{}f\in\mathcal{F}:\sum_{i=1}^{N}\log f_{h}(z^{f,\pi}_ {h,i}\mid x_{h,i},a_{h,i})\geq\max_{\bar{f}\in\mathcal{F}}\sum_{i=1}^{N}\log \widetilde{f}_{h}(z^{f,\pi}_{h,i}\mid x_{h,i},a_{h,i})-7\beta,\forall h\in[H] \Bigg{\}}.\]
3:Set \(\mathrm{op}=\max\) if SmallReturn else\(\mathrm{op}=\min\).
4:For each \(\pi\in\Pi\), define the pessimistic estimate \(f^{\pi}=\arg\mathrm{op}_{f\in\mathcal{F}_{\pi}}\,\mathbb{E}_{a\sim\pi(x_{1})} \big{[}\bar{f}_{1}(x_{1},a)\big{]}\).
5:Output:\(\widehat{\pi}=\arg\mathrm{op}_{\pi\in\Pi}\,\mathbb{E}_{a\sim\pi(x_{1})}\big{[} \bar{f}_{1}^{\pi}(x_{1},\pi)\big{]}\). ```

**Algorithm 5** P-DISCO (with small return)Proofs for DistCB

**Lemma C.1** (Azuma).: _Let \(\left\{X_{i}\right\}_{i\in[N]}\) be a sequence of random variables supported on \([0,1]\), adapted to filtration \(\left\{\mathcal{F}_{i}\right\}_{i\in[N]}\). For any \(\delta\in(0,1)\), we have w.p. at least \(1-\delta\),_

\[\sum_{t=1}^{N}\mathbb{E}[X_{t}\mid\mathcal{F}_{t-1}] \leq\sum_{t=1}^{N}X_{t}+\sqrt{N\log(2/\delta)},\] (Standard Azuma) \[\sum_{t=1}^{N}\mathbb{E}[X_{t}\mid\mathcal{F}_{t-1}] \leq 2\sum_{t=1}^{N}X_{t}+2\log(1/\delta).\] (Multiplicative Azuma)

Proof.: For standard Azuma, see Zhang (2023, Theorem 13.4). For multiplicative Azuma, apply (Zhang, 2023, Theorem 13.5) with \(\lambda=1\). The claim follows, since \(\frac{1}{1-\exp(-\lambda)}\leq 2\). 

**Theorem 4.1**.: _For any \(\delta\in(0,1)\), w.p. at least \(1-\delta\), running DistCB with \(\gamma=10A\vee\sqrt{\frac{40A(C^{\star}+\log(1/\delta))}{112\left(\operatorname {Regret}_{\log}(K)+\log(1/\delta)\right)}}\) has regret scaling with \(C^{\star}=\sum_{k=1}^{K}\min_{a\in\mathcal{A}}\bar{C}(x_{k},a)\),_

\[\operatorname{Regret}_{\operatorname{DistCB}}(K)\leq 232\sqrt{AC^{\star} \operatorname{Regret}_{\log}(K)\log(1/\delta)}+2300A\big{(}\operatorname{ Regret}_{\log}(K)+\log(1/\delta)\big{)}.\]

Proof of Theorem 4.1.: First, recall the per-step inequality of ReIGW Foster and Krishnamurthy (2021, Theorem 4), which states: for any \(\widehat{f}\) and \(\gamma\geq 2A\), if we set \(p=\text{ReIGW}_{\gamma}(\widehat{f},\gamma)\), then, for all \(f\in[0,1]^{A}\), we have

\[\sum_{a}p(a)(f(a)-f(a^{\star}))\leq\tfrac{5A}{\gamma}\sum_{a}p(a)f(a)+7\gamma \sum_{a}p(a)\frac{(\widehat{f}(a)-f(a))^{2}}{\widehat{f}(a)+f(a)},\]

where \(a^{\star}=\operatorname{arg\,min}_{a}f(a)\). For any \(k\in[K]\), applying this to \(\widehat{f}=\bar{f}^{(k)}(s_{k},\cdot)\), \(p=p_{k}\) and \(f=\bar{C}(s_{k},\cdot)\), we have

\[\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\big{[}\bar{C}(s_{k},a_{k})-\bar{ C}(s_{k},\pi^{\star}(s_{k}))\big{]} \leq\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\Bigg{[}\frac{5A}{\gamma} \bar{C}(s_{k},a_{k})+7\gamma\frac{\big{(}\bar{f}^{(k)}(s_{k},a_{k})-\bar{C}(s_ {k},a_{k})\big{)}^{2}}{\bar{f}^{(k)}(s_{k},a_{k})+\bar{C}(s_{k},a_{k})}\Bigg{]}\] \[\leq\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\Bigg{[}\frac{5A}{\gamma} \bar{C}(s_{k},a_{k})+7\gamma D_{\triangle}(f^{(k)}(s_{k},a_{k})\parallel C(s_ {k},a_{k}))\Bigg{]}\] (Eq. ( \[\triangle_{1}\] ))

Since \(D_{\triangle}\leq 4H^{2}\), we have

\[\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\Big{[}D_{\triangle}(f^{(k)}(s_{k },a_{k})\parallel C(s_{k},a_{k}))\Big{]}\] \[\leq 4\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\Big{[}H^{2}\Big{(}C(s_{k},a _{k})\parallel f^{(k)}(s_{k},a_{k})\Big{)}\Big{]}\] \[\leq 8\sum_{k=1}^{K}H^{2}\Big{(}C(s_{k},a_{k})\parallel f^{(k)}(s_{ k},a_{k})\Big{)}+8\log(1/\delta)\] (Multiplicative Azuma, since \[H^{2}\in[0,1]\] ) \[\leq 8\operatorname{Regret}_{\log}(K)+10\log(1/\delta).\] (Foster et al. (2021, Lemma A.14))

Hence, we have

\[\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\big{[}\bar{C}(s_{k},a_{k})-\bar{ C}(s_{k},\pi^{\star}(s_{k}))\big{]}\leq\frac{5A}{\gamma}\sum_{k=1}^{K} \mathbb{E}_{a_{k}}\big{[}\bar{C}(s_{k},a_{k})\big{]}+70\gamma\big{(} \operatorname{Regret}_{\log}(K)+\log(1/\delta)\big{)}.\]

Finally, recalling that \(1/(1-\varepsilon)\leq 1+2\varepsilon\) when \(\varepsilon\leq\frac{1}{2}\), and the fact that \(\frac{5A}{\gamma}\leq\frac{1}{2}\), we have

\[\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\big{[}\bar{C}(s_{k},a_{k})-\bar{C}(s_{k},\pi^{ \star}(s_{k}))\big{]}\leq\frac{10A}{\gamma}\sum_{k=1}^{K}\mathbb{E}_{a_{k}} \big{[}\bar{C}(s_{k},\pi^{\star}(s_{k}))\big{]}+140\gamma\big{(}\operatorname{ Regret}_{\log}(K)+\log(1/\delta)\big{)}.\]By Azuma's inequality, we have

\[\sum_{k=1}^{K}\bar{C}(s_{k},a_{k})-\bar{C}(s_{k},\pi^{\star}(s_{k}))\] \[\leq 2\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\big{[}\bar{C}(s_{k},a_{k})- \bar{C}(s_{k},\pi^{\star}(s_{k}))\big{]}+2\log(1/\delta)\] \[\leq\frac{20A}{\gamma}\sum_{k=1}^{K}\mathbb{E}_{a_{k}}\big{[}\bar {C}(s_{k},\pi^{\star}(s_{k}))\big{]}+140\gamma\big{(}\mathrm{Regret}_{\log}(K) +\log(1/\delta)\big{)}+2\log(1/\delta)\] \[\leq\frac{40A}{\gamma}(C^{\star}+\log(1/\delta))+140\gamma\big{(} \mathrm{Regret}_{\log}(K)+\log(1/\delta)\big{)}+2\log(1/\delta).\] (Multiplicative Azuma)

Now set \(\gamma=\sqrt{\frac{40A(C^{\star}+\log(1/\delta))}{140\big{(}\mathrm{Regret}_{ \log}(K)+\log(1/\delta)\big{)}}}\lor 10A\).

Case 1 is when \(\sqrt{\frac{40A(C^{\star}+\log(1/\delta))}{140\big{(}\mathrm{Regret}_{\log}(K) +\log(1/\delta)\big{)}}}\quad\leq\quad 10A\), _i.e._, \((C^{\star}+\log(1/\delta))\leq 280A\big{(}\mathrm{Regret}_{\log}(K)+\log(1/ \delta)\big{)}\), we have the above is at most

\[4(C^{\star}+\log(1/\delta))+1120A\big{(}\mathrm{Regret}_{\log}(K )+\log(1/\delta)\big{)}+2\log(1/\delta)\] \[\leq 2240A\big{(}\mathrm{Regret}_{\log}(K)+\log(1/\delta)\big{)}+ 2\log(1/\delta).\]

Case 2 is when the left term dominates, then the bound is,

\[2\sqrt{4480A(C^{\star}+\log(1/\delta))\big{(}\mathrm{Regret}_{ \log}(K)+\log(1/\delta)\big{)}}+2\log(1/\delta)\] \[\leq 2\sqrt{13440AC^{\star}\,\mathrm{Regret}_{\log}(K)\log(1/ \delta)+4480A\log^{2}(1/\delta)}+2\log(1/\delta)\] \[\leq 232\sqrt{AC^{\star}\,\mathrm{Regret}_{\log}(K)\log(1/\delta) }+134\sqrt{A}\log(1/\delta)+2\log(1/\delta).\]

Putting these two cases together, we have the result.

Distributional Bellman Completeness in low-rank MDPs

The goal of this section is to show that, under mild conditions in low-rank MDPs, there always exists a function class with bounded bracketing number that satisfies the distributional BC condition. First, let us recall the low-rank MDP In this section, we show that linear MDPs automatically satisfy the distributional Bellman completeness assumption.

**Definition 5.6** (Low-rank MDP).: A transition model \(P_{h}:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{X})\) has rank \(d\) if there exist unknown features \(\phi^{\star}_{h}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d},\mu^{\star}_{h }:\mathcal{X}\to\mathbb{R}^{d}\) such that \(P_{h}(x^{\prime}\mid x,a)=\phi^{\star}_{h}(x,a)^{\top}\mu^{\star}_{h}(x^{\prime})\) for all \(x,a,x^{\prime}\). Also, assume \(\max_{x,a}\|\phi^{\star}_{h}(x,a)\|_{2}\leq 1\) and \(\|\int g\mathrm{d}\mu^{\star}_{h}\|_{2}\leq\|g\|_{\infty}\sqrt{d}\) for all functions \(g:\mathcal{X}\to\mathbb{R}\). The MDP is called low-rank if \(P_{h}\) is low-rank for all \(h\in[H]\).

Suppose that we have a function class \(\Phi\) such that \(\phi^{\star}_{h}\in\Phi\) for all \(h\), _i.e._, \(\Phi\) is a realizable function class. For example, in linear MDPs, this is automatically satisfied since we know \(\phi^{\star}\)_a priori_, so \(\Phi\) is the singleton with \(\phi^{\star}\). Having a realizable \(\Phi\) class is standard for solving low-rank MDPs (Uehara et al., 2021; Agarwal et al., 2023).

In what follows, let \(\alpha=\max_{h,\pi,z,x,a}Z^{\pi}_{h}(z\mid x,a)\) denote the maximum density/mass value of the loss-to-go distributions. Note that \(\alpha\geq 1\) always since the mass at \(H+1\) is deterministically placed at zero. If we further know that \(Z^{\pi}_{h}\) is discretely distributed, then \(\alpha=1\). If \(Z^{\pi}_{h}\) is continuously distributed, we assume it is bounded.

We consider the function class in Eq.2, which we reproduce here:

\[\mathcal{F}^{\text{in}}_{h}=\bigg{\{}f(z\mid x,a)=\big{\langle} \phi(x,a),w(z)\big{\rangle}\quad:\quad\phi\in\Phi,w:[0,1]\to\mathbb{R}^{d},\] (3)

\[\text{s.t.}\ \ \max_{z}\|w(z)\|_{2}\leq\alpha\sqrt{d}\ \ \text{and}\ \ \max_{x,a,z}\big{\langle}\phi(x,a),w(z)\big{\rangle}\leq\alpha\bigg{\}}.\]

The next lemma (Lemma D.1) shows that this function class satisfies distributional BC.

**Lemma D.1**.: \(\mathcal{F}^{\text{in}}\) _satisfies distributional BC (Assumption 5.1)._

Proof.: We denote \(\|f\|_{\infty}=\max_{z,x,a}f(z\!\mid\!x,a)\). For any \(f_{h+1}\in\mathcal{F}^{\text{in}}_{h+1}\), we have \(\|f_{h+1}\|_{\infty}\leq\alpha\) by the construction of \(\mathcal{F}^{\text{in}}_{h+1}\). Then, let \(\mathcal{T}^{D}\) be either the distributional Bellman operator or distributional optimality operator, the following equalities hold for the appropriate \(a^{\prime}(x^{\prime})\) based on \(\mathcal{T}^{D}\),

\[\mathcal{T}^{D}f_{h+1}(z\!\mid\!x,a)= \int_{\mathcal{X}}\Pr_{h}(x^{\prime}\!\mid\!x,a)\int_{\mathbb{R}} \Pr_{h}(c\mid x,a)f_{h+1}(z-c\!\mid\!x^{\prime},a^{\prime}(x^{\prime}))\, \mathrm{d}x^{\prime}\,\mathrm{d}c\] \[= \bigg{\langle}\phi^{\star}_{h}(x,a),\underbrace{\int_{\mathcal{X}} \mu_{h}(x^{\prime})\int_{\mathbb{R}}\Pr_{h}(c\mid x,a)f_{h+1}(z-c\!\mid\!x^{ \prime},a^{\prime}(x^{\prime}))\,\mathrm{d}c\,\mathrm{d}x^{\prime}}_{:=w_{h}( z)}\bigg{\rangle}\]

Since \(\int_{\mathbb{R}}\Pr_{h}(c\mid x,a)f_{h+1}(z-c\!\mid\!x^{\prime},a^{\prime}(x ^{\prime}))\,\mathrm{d}c\leq\|f_{h+1}\|_{\infty}\), we know that

\[\|w_{h}(z)\|_{2}\leq\|f_{h+1}\|_{\infty}\sqrt{d}\leq\alpha\sqrt{d}.\]

We further note that

\[\max_{x,a,z}\big{\langle}\phi^{\star}_{h}(x,a),w_{h}(z)\big{\rangle}=\max_{x,a, z}\mathcal{T}^{D}f_{h+1}(z\!\mid\!x,a)\leq\|f_{h+1}\|_{\infty}\leq\alpha.\]

Also note that \(\phi^{\star}_{h}\in\Phi\) by realizability. Therefore, \(\mathcal{T}^{D}f_{h+1}\in\mathcal{F}^{\text{in}}_{h}\), which is the distributional BC condition. 

### Bounding the bracketing number via discretized rewards

We now bound the bracketing number of \(\mathcal{F}^{\text{in}}_{h}\) under a _discretization assumption that costs and costs-to-gos can only take \(M\) many discrete values on an evenly spaced grid_. This can be interpreted as discretizing the reward space, and it can be shown that this discretization error is small for regret or PAC bounds (Wang et al., 2023, Section 6). Structural assumptions are necessary to bound the complexity of \(\mathcal{F}_{h}^{\text{lin}}\) and such discretization assumptions are common in practice, _e.g._, C51 (Bellemare et al., 2017) and Rainbow (Hessel et al., 2018) both set \(M=51\) which works well in Atari games. After discretizing, we can consider \(w\) as a mapping from \([M]\), the discrete set on \(M\) elements, rather than from the interval \([0,1]\). Note also that since \(Z_{h}^{\pi}\) are discrete, we have \(\alpha=1\).

Now, let \(\varepsilon>0\) be arbitrary and fixed. Recall that the \(\ell_{\infty}\) bracketing number is equivalent (up to universal constants) to the \(\ell_{\infty}\) covering number, so we will work with the latter. Let \(B(r)\) denote the \(d\)-dimensional ball of radius \(r\) (in \(\ell_{2}\)). Recall that the \(\varepsilon\)-covering number (in \(\ell_{2}\)) of functions \([M]\mapsto B(r)\) scales as \(\mathcal{O}((r/\varepsilon)^{dM})\). Let \(\mathcal{W}_{\varepsilon}\) be such the smallest cover. We can build a \(\ell_{\infty}\) cover of \(\mathcal{F}_{h}^{\text{lin}}\) as follows: \(\mathcal{C}_{\varepsilon}=\{(x,a,z)\mapsto\langle\phi(x,a),w(z)\rangle,w\in \mathcal{W}_{\varepsilon},\phi\in\Phi\}\).

To check this is a \(\varepsilon\) cover, consider any \(f\in\mathcal{F}_{h}^{\text{lin}}\). \(f\) corresponds to some \(\phi\) and \(w\). Let \(w^{\prime}\) be the neighbor of \(w\) in \(\mathcal{W}_{\varepsilon}\) and let \(f^{\prime}(x,a,z)=\langle\phi(x,a),w^{\prime}(z)\rangle\) so indeed \(f^{\prime}\in\mathcal{C}_{\varepsilon}\). Then, for any \(x,a,z\), we have \(|\langle\phi(x,a),w(z)-w^{\prime}(z)\rangle|\leq\|\phi(x,a)\|_{2}\|w(z)-w^{ \prime}(z)\|_{2}\leq\varepsilon\). Hence, \(\mathcal{C}_{\varepsilon}\) is an \(\ell_{\infty}\) cover of size \(\mathcal{O}((\sqrt{d}/\varepsilon)^{dM}\cdot|\Phi|)\), and so we have shown that \(\log N_{\|}(\varepsilon,\mathcal{F}_{h}^{\text{lin}},\|\cdot\|_{\infty})\leq \mathcal{O}(dM\log(d/\varepsilon)+\log|\Phi|)\).

Linear MDPs:Recall that in linear MDPs, we know the true \(\phi^{\star}\) and so \(|\Phi|=1\). Thus, the bracketing number is simply \(\mathcal{O}(dM\log(d/\varepsilon))\) in linear MDPs.

Summary and comparison with regular BC:In summary, under the assumption that rewards are discretized, we know that low-rank MDPs automatically have distributional function classes that satisfy distributional BC and have bounded bracketing numbers. Furthermore, recall that (Wu et al., 2023) showed that Linear Quadratic Regulators (LQRs), with deterministic transitions, also have function classes that satisfy distributional BC and have bounded bracketing numbers. Thus, distributional BC holds for the most interesting cases covered by the standard Bellman completeness, _e.g._, linear MDPs, low-rank MDPs and LQRs. Since learning conditional distributions is statistically harder than learning the conditional mean, we need to pay the price in assuming reward/transitions satisfy regularity assumptions to bound the bracketing number appropriately.

Generalization Bounds for Maximum Likelihood Estimation

This section reviews generalization bounds for the maximum likelihood estimator (MLE). We adopt the same sequential condition probability estimation setup as in Agarwal et al. (2020, Appendix E), which we now recall for completeness. Let \(\mathcal{X}\) be the context/feature space and \(\mathcal{Y}\) be the label space, and we are given a dataset \(D=\left\{(x_{i},y_{i})\right\}_{i\in[n]}\) from a martingale process: for \(i=1,2,...,n\), sample \(x_{i}\sim\mathcal{D}_{i}(x_{1:i-1},y_{1:i-1})\) and \(y_{i}\sim p(\cdot\mid x_{i})\). Let \(f^{\star}(x,y)=p(y\mid x)\) and we are given a realizable, _i.e._, \(f^{\star}\in\mathcal{F}\), function class \(\mathcal{F}:\mathcal{X}\times\mathcal{Y}\rightarrow\Delta(\mathbb{R})\) of distributions. The MLE is an estimate for \(f^{\star}\) that maximizes the log-likelihood objective over our dataset:

\[\widehat{f}_{\text{MLE}}=\operatorname*{arg\,max}_{f\in\mathcal{F}}\sum_{i=1} ^{n}\log f(x_{i},y_{i}).\]

For our guarantees to hold for general hypotheses classes \(\mathcal{F}\), we use the bracketing number to quantify the statistical complexity of \(\mathcal{F}\)(van de Geer, 2000).

**Definition E.1** (Bracketing Number).: Let \(\mathcal{G}\) be a set of functions mapping \(\mathcal{X}\rightarrow\mathbb{R}\). Given two functions \(l,u\) such that \(l(x)\leq u(x)\) for all \(x\in\mathcal{X}\), the bracket \([l,u]\) is the set of functions \(g\in\mathcal{G}\) such that \(l(x)\leq g(x)\leq u(x)\) for all \(x\in\mathcal{X}\). We call \([l,u]\) an \(\varepsilon\)-bracket if \(\|u-l\|\leq\varepsilon\). Then, the \(\varepsilon\)-bracketing number of \(\mathcal{G}\) with respect to \(\|\cdot\|\), denoted by \(N_{\mathbb{D}}[\varepsilon,\mathcal{G},\|\cdot\|)\) is the minimum number of \(\varepsilon\)-brackets needed to cover \(\mathcal{G}\).

Since the triangular discrimination is equivalent to squared Hellinger up to universal constants, we now prove MLE generalization bounds in terms of squared Hellinger.

**Lemma E.2**.: _Let \(f_{1}:\mathcal{X}\rightarrow\Delta(\mathcal{Y})\) and \(f_{2}:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}_{+}\) satisfying \(\sup_{x\in\mathcal{X}}\int_{\mathcal{Y}}f_{2}(x,y)\mathrm{d}y\leq s\), then for any distribution \(\mathcal{D}\in\Delta(\mathcal{X})\), we have_

\[\mathbb{E}_{x\sim\mathcal{D}}\big{[}H^{2}(f_{1}(x)\;\|\;f_{2}(x,\cdot))\big{]} \leq(s-1)-2\log\mathbb{E}_{x\sim\mathcal{D},y\sim f_{1}(x)}\exp\biggl{(}-\frac {1}{2}\log(f_{1}(x,y)/f_{2}(x,y))\biggr{)}.\]

Proof.: This follows from the proof of Wu et al. (2023, Lemma C.1). 

**Lemma E.3**.: _Fix \(\delta\in(0,1)\). Then w.p. at least \(1-\delta\), for any \(f\in\mathcal{F}\), we have_

\[\sum_{i=1}^{n}\mathbb{E}_{x\sim\mathcal{D}_{i}}\big{[}H^{2}(f(x, \cdot)\;\|\;f^{\star}(x,\cdot))\big{]}\] \[\quad\leq 6n\epsilon|\mathcal{Y}|+2\sum_{i=1}^{n}\log\big{(}f^{ \star}(x_{i},y_{i})/f(x_{i},y_{i})\big{)}+8\log\bigl{(}N_{\mathbb{D}}[(\epsilon,\mathcal{F},\|\cdot\|_{\infty})/\delta\bigr{)}.\] (4)

_Rearranging, we also have_

\[\sum_{i=1}^{n}\log\big{(}f(x_{i},y_{i})/f^{\star}(x_{i},y_{i})\big{)}\leq 3n\epsilon|\mathcal{Y}|+4\log\bigl{(}N_{\mathbb{D}}[( \epsilon,\mathcal{F},\|\cdot\|_{\infty})/\delta\bigr{)}.\] (5)

Proof.: We take an \(\epsilon\)-bracketing of \(\mathcal{F}\), \(\{[l_{i},u_{i}]:i=1,2,\dots\}\), and denote \(\widetilde{\mathcal{F}}=\{u_{i}:i=1,2,\dots\}\). Applying Lemma 24 of Agarwal et al. (2020) to function class \(\widetilde{\mathcal{F}}\) and using Chernoff method, w.p. at least \(1-\delta\), for all \(\tilde{f}\in\widetilde{\mathcal{F}}\), we have

\[\underbrace{-\log\mathop{\mathbb{E}}_{\widetilde{D}^{\prime}}\exp(L(\tilde{f}( D),D^{\prime}))}_{\text{(i)}}\leq\underbrace{-L(\tilde{f}(D),D)+2\log\bigl{(}N_{ \mathbb{D}}[(\epsilon,\mathcal{F},\|\cdot\|_{\infty})/\delta\bigr{)}}_{\text{(ii)}}.\] (6)

Now, fix any \(f\in\mathcal{F}\) and pick \(\tilde{f}\in\widetilde{\mathcal{F}}\) as the upper bracket, _i.e._, \(f\leq\tilde{f}\). Now set \(L(f,D)=\sum_{i=1}^{n}-\nicefrac{{1}}{{2}}\log(f^{\star}(x_{i},y_{i})/f(x_{i},y _{i}))\). Then the right hand side of (6) is

\[\text{(ii)}= \frac{1}{2}\sum_{i=1}^{n}\log(f^{\star}(x_{i},y_{i})/\tilde{f}(x _{i},y_{i}))+2\log\bigl{(}N_{\mathbb{D}}[(\epsilon,\mathcal{F},\|\cdot\|_{ \infty})/\delta\bigr{)}\] \[\leq \frac{1}{2}\sum_{i=1}^{n}\log(f^{\star}(x_{i},y_{i})/f(x_{i},y_{i} ))+2\log\bigl{(}N_{\mathbb{D}}[(\epsilon,\mathcal{F},\|\cdot\|_{\infty})/\delta \bigr{)}.\]On the other hand, since \(H\) is a metric, we have

\[\sum_{i=1}^{n}\mathop{\mathbb{E}}_{x\sim\mathcal{D}_{i}}H^{2}\left(f( x,\cdot),f^{\star}(x,\cdot)\right)\leq\sum_{i=1}^{n}\mathop{\mathbb{E}}_{x\sim \mathcal{D}_{i}}\!\!\left(H\left(f(x,\cdot),\tilde{f}(x,y)\right)+H\left(\tilde{ f}(x,y),f^{\star}(x,\cdot)\right)\right)^{2}\] \[\leq 2\underbrace{\sum_{i=1}^{n}\mathop{\mathbb{E}}_{x\sim\mathcal{D} _{i}}H^{2}\left(f(x,\cdot),\tilde{f}(x,y)\right)}_{\text{(iii)}}+2\underbrace{ \sum_{i=1}^{n}\mathop{\mathbb{E}}_{x\sim\mathcal{D}_{i}}H^{2}\left(\tilde{f}(x,y),f^{\star}(x,\cdot)\right)}_{\text{(iv)}}.\]

For (iii), by the definition, we have \(\tilde{f}(x,y)-f(x,y)\in[0,\epsilon]\) for all \(x\), so

\[\text{(iii)}=\sum_{i=1}^{n}\mathop{\mathbb{E}}_{x\sim\mathcal{D}_{i}}H^{2} \left(f(x,\cdot),\tilde{f}(x,y)\right)\leq\sum_{i=1}^{n}\mathop{\mathbb{E}}_ {x\sim\mathcal{D}_{i}}2\int_{y}\Bigl{|}f(x,y)-\tilde{f}(x,y)\Bigr{|}\mathrm{d} y\leq 2n\epsilon|\mathcal{Y}|.\]

For (iv), we apply LemmaE.2 with \(f_{1}=f^{\star}\) and \(f_{2}=\tilde{f}\) (thus \(s=1+\epsilon|\mathcal{Y}|\)) and get

\[\text{(iv)}= n\epsilon|\mathcal{Y}|-2\sum_{i=1}^{n}\log\mathop{\mathbb{E}}_{x,y\sim f^{\star}(x,\cdot)}\exp\left(-\frac{1}{2}\log\left(f^{\star}(x,y)/ \tilde{f}(x,y)\right)\right)\] \[= n\epsilon|\mathcal{Y}|-2\sum_{i=1}^{n}\log\mathop{\mathbb{E}}_{x,y\sim\mathcal{D}_{i}}\exp\left(-\frac{1}{2}\log\left(f^{\star}(x,y)/\tilde{f }(x,y)\right)\right)\] \[= n\epsilon|\mathcal{Y}|-2\log\mathop{\mathbb{E}}_{x,y\sim \mathcal{D}^{\prime}}\left[\exp\left(\sum_{i=1}^{n}-\frac{1}{2}\log\left(f^{ \star}(x,y)/\tilde{f}(x,y)\right)\right)\right]\!D\right]\] \[= n\epsilon|\mathcal{Y}|+2\cdot\text{(i)}.\]

By plugging (iii) and (iv) back we get

\[\sum_{i=1}^{n}\mathop{\mathbb{E}}_{x\sim\mathcal{D}_{i}}H^{2}\left(f(x,\cdot),f^{\star}(x,\cdot)\right)\leq 6n\epsilon|\mathcal{Y}|+4\cdot\text{(i)}.\]

Notice that \(\text{(i)}\leq\text{(ii)}\), so we complete the proof by plugging \(\text{(ii)}\) into the above. 

We first state the MLE generalization result for finite \(\mathcal{F}\).

**Theorem E.4**.: _Suppose \(\mathcal{F}\) is finite. Fix any \(\delta\in(0,1)\), set \(\beta=\log(|\mathcal{F}|/\delta)\) and define_

\[\widehat{\mathcal{F}}=\Biggl{\{}f\in\mathcal{F}:\sum_{i=1}^{n}\log f(x_{i},y_{ i})\geq\max_{\tilde{f}\in\mathcal{F}}\sum_{i=1}^{n}\widetilde{f}(x_{i},y_{i})-4 \beta\Biggr{\}}.\]

_Then w.p. at least \(1-\delta\), the following holds:_

1. _The true distribution is in the version space, i.e.,_ \(f^{\star}\in\widehat{\mathcal{F}}\)_._
2. _Any function in the version space is close to the ground truth data-generating distribution, i.e., for all_ \(f\in\widehat{\mathcal{F}}\)__ \[\sum_{i=1}^{n}\mathbb{E}_{x\sim\mathcal{D}_{i}}\bigl{[}H^{2}(f(x,\cdot)\parallel f ^{\star}(x,\cdot))\bigr{]}\leq 22\beta.\]

Proof.: These two claims follow from LemmaE.3 with \(\epsilon=0\), and so \(N_{[\![}(\epsilon,\mathcal{F},\parallel\cdot\parallel_{\infty})=|\mathcal{F}|\). For (1), apply Eq. (5) to \(f=\widehat{f}_{\text{MLE}}\) to see that \(f^{\star}\in\widehat{\mathcal{F}}\). For (2), apply Eq. (4) and note that the sum term is at most \(4\beta\). Thus, the right hand side of Eq. (4) is at most \((6+8+8)\beta=22\beta\). 

We now state the result for infinite \(\mathcal{F}\) using bracketing entropy.

**Theorem E.5**.: _Fix any \(\delta\in(0,1)\), set \(\beta=\log(N_{\llbracket}((n|\mathcal{Y}|)^{-1},\mathcal{F},\|\cdot\|_{\infty})/\delta)\) and define_

\[\widehat{\mathcal{F}}=\Bigg{\{}f\in\mathcal{F}:\sum_{i=1}^{n}\log f(x_{i},y_{i} )\geq\max_{\widehat{f}\in\mathcal{F}}\sum_{i=1}^{n}\widehat{f}(x_{i},y_{i})- 7\beta\Bigg{\}}.\]

_Then w.p. at least \(1-\delta\), the following holds:_

1. _The true distribution is in the version space, i.e.,_ \(f^{\star}\in\widehat{\mathcal{F}}\)_._
2. _Any function in the version space is close to the ground truth data-generating distribution, i.e., for all_ \(f\in\widehat{\mathcal{F}}\)__ \[\sum_{i=1}^{n}\mathbb{E}_{x\sim\mathcal{D}_{i}}\big{[}H^{2}(f(x,\cdot)\;\|\;f^ {\star}(x,\cdot))\big{]}\leq 28\beta.\]

Proof.: These two claims follow from Lemma E.3 with \(\epsilon=\nicefrac{{1}}{{n}}|\mathcal{Y}|\). For (1), apply Eq. (5) to \(f=\widehat{f}_{\text{MLE}}\) to see that \(f^{\star}\in\widehat{\mathcal{F}}\). For (2), apply Eq. (4) and note that the sum term is at most \(7\beta\). Thus, the right hand side of Eq. (5) is at most \((6+14+8)\beta=28\beta\). 

## Appendix F Confidence set construction with general function class

In this section, we extend the confidence set construction of O-DISCO and P-DISCO to general \(\mathcal{F}\), which can be infinite. Our procedure constructs the confidence set by performing the thresholding scheme on an \(\varepsilon\)-net of \(\mathcal{F}\). While constructing an \(\varepsilon\)-net for \(\mathcal{F}\) is admittedly a computationally hard procedure, this is still information theoretically possible and our focus in O-DISCO and P-DISCO is to show that distributional RL information-theoretically leads to small-loss bounds.

We first define some notations. Let \(\mathcal{F}^{\downarrow}\) and \(\mathcal{F}^{\uparrow}\) denote a lower and upper \(\varepsilon\)-bracketing of \(\mathcal{F}\), _i.e._, for any \(f\in\mathcal{F}\), there exists an \(\varepsilon\)-bracket \([f^{\downarrow},f^{\uparrow}]\) such that for all \(h\), \(f^{\downarrow}_{h}\leq f_{h}\leq f^{\uparrow}_{h}\) with \(f^{\downarrow}\in\mathcal{F}^{\downarrow},f^{\uparrow}\in\mathcal{F}^{\uparrow}\). Recall that a lower bracket \(g\in\mathcal{F}^{\downarrow}\) may not be a valid distribution, but since elements of \(\mathcal{F}\) map to non-negative values, we can assume \(g\) has non-negative entries as well. Also, we have \(\alpha^{g}_{h}(x,a):=\int g_{h}(z\mid x,a)\geq 1-\varepsilon\), so for \(\varepsilon\) small enough, \(g\) is normalizable. Hence, define \(\widetilde{g}(z\mid x,a)=\alpha^{g}_{h}(x,a)^{-1}g(z\mid x,a)\) as the normalized version, which is a valid distribution that we can sample from

Now, consider any martingale \(\{x_{h,i},a_{h,i},c_{h,i}\}_{i\in[n],h\in[H]}\), which could be the online data up to episode \(k\) or the offline data (consisting of \(\mathcal{N}\) i.i.d. samples). We define the MLE with respect to a lower bracket element as follows. For any \(h\in[H],g\in\mathcal{F}^{\downarrow},\pi\in\Pi\), sample \(y^{j,\pi}_{h,i}\sim\widetilde{g}_{h+1}(x^{\prime}_{h,i},\pi(x^{\prime}_{h,i}))\), and \(z^{g,\pi}_{h,i}=c_{h,i}+y^{g,\pi}_{h,i}\), define the MLE solution for \((g,\pi)\) at time \(h\) as,

\[\text{MLE}^{g,\pi}_{h}=\operatorname*{arg\,max}_{\widehat{f}\in\mathcal{F}} \sum_{i=1}^{n}\log f_{h}(z^{g,\pi}_{h,i}\mid x_{h,i},a_{h,i}).\]

Also, define the version space with respect to the above MLE as,

\[\mathcal{F}_{g,\pi,h}=\Bigg{\{}f\in\mathcal{F}:\sum_{i=1}^{n}\log f_{h}(z^{g,\pi}_{h,i}\mid x_{h,i},a_{h,i})\geq\sum_{i=1}^{n}\log\text{MLE}^{g,\pi}_{h}(z ^{g,\pi}_{h,i}\mid x_{h,i},a_{h,i})-\beta\Bigg{\}}.\]

We now prove a key result that implies that \(\mathcal{T}^{\pi}_{h}f^{\downarrow}_{h+1}\) falls into the confidence set \(\mathcal{F}_{f^{\downarrow},\pi,h}\).

**Theorem F.1**.: _For any \(\delta\in(0,1)\) and suppose \(n\geq 2\). Then, w.p. at least \(1-\delta\), for any \(h\in[H],g\in\mathcal{F},f^{\downarrow}\in\mathcal{F}^{\downarrow},\pi\in\Pi\), we have_

\[\sum_{i=1}^{n}\log g_{h}(z^{f^{\downarrow},\pi}_{h,i}\mid x_{h,i},a_{h,i})- \log\mathcal{T}^{\pi}_{h}f^{\downarrow}_{h+1}(z^{f^{\downarrow},\pi}_{h,i}\mid x _{h,i},a_{h,i})\leq\log(e^{4}N_{\llbracket}(n^{-1},\mathcal{F},\|\cdot\|_{ \infty})^{2}|\Pi|/\delta).\]

_where \(z^{f^{\downarrow},\pi}_{h,i}=c_{h,i}+y^{f^{\downarrow},\pi}_{h,i}\) and \(y^{f^{\downarrow},\pi}_{h,i}\sim\widehat{f}^{\downarrow}_{h+1}(\cdot\mid x^{ \prime}_{h,i},\pi_{h+1}(x^{\prime}_{h,i}))\)._Proof of Theorem F.1.: Consider a \(\varepsilon\)-bracketing of \(\mathcal{F}\) where \(\varepsilon\leq 1/n\leq 1/2\); we will study each element and conclude with a union bound. For any lower bracket \(l\) and upper bracket \(u\) in the bracketing (note \(l,u\) need not correspond to the same bracket). Recall that \(\alpha_{h+1}^{l}(x,a):=\int l_{h+1}(z\mid x,a)\), so we have \(1-\varepsilon\leq\alpha_{h+1}^{l}\leq 1\) since \(l\) is a lower \(\varepsilon\)-bracket of distributions. Therefore, we have

\[\mathbb{E}\Bigg{[}\exp\sum_{i=1}^{n}\log\Biggl{(}\frac{u_{h}(z_{h,i}^{l,\pi} \mid x_{h,i},a_{h,i})}{\mathcal{T}_{h}^{\pi}l_{h+1}(z_{h,i}^{l,\pi}\mid x_{h,i },a_{h,i})}\Biggr{)}\Bigg{]}=\prod_{i=1}^{n}\mathbb{E}_{\nu_{h,i}}\Bigg{[} \frac{u_{h}(z_{h,i}^{l,\pi}\mid x_{h,i},a_{h,i})}{\mathcal{T}_{h}^{\pi}l_{h+1 }(z_{h,i}^{l,\pi}\mid x_{h,i},a_{h,i})}\Bigg{]},\]

where \(\nu_{h,i}\) is the distribution of data from \(i\)-th round and time \(h\). Note that \(\nu_{h,i}(x,a,c,x^{\prime})=d_{h,i}(x,a)C_{h}(c\mid x,a)P_{h}(x^{\prime}\mid x,a)\) for some distribution \(d_{h,i}(x,a)\). Now focus on each \(i\), so for all \(i\), we have

\[\mathbb{E}_{\nu_{h,i}}\Bigg{[}\frac{u_{h}(z_{h,i}^{l,\pi}\mid x_{h,i},a_{h,i})}{\mathcal{T}_{h}^{\pi}l_{h+1}(z_{h,i}^{l,\pi}\mid x_{h,i},a_{h,i})} \Bigg{]}\] \[=\int_{x,a,c,x^{\prime},y}\nu_{h,i}(x,a,c,x^{\prime})\widetilde{ l}_{h+1}(y\mid x^{\prime},\pi(x^{\prime}))\frac{u_{h}(c+y\mid x,a)}{\int_{c,x^{ \prime}}\nu_{h,i}(c,x^{\prime}\mid x,a)l_{h+1}(y\mid x^{\prime},\pi(x^{\prime }))}\] \[=\int_{x,a,z}d_{h,i}(x,a)\int_{z}u_{h}(z\mid x,a)\] \[\times\int_{c,x^{\prime}}\nu_{h,i}(c,x^{\prime}\mid x,a) \widetilde{l}_{h+1}(z-c\mid x^{\prime},\pi(x^{\prime}))\frac{1}{\int_{c,x^{ \prime}}\nu_{h,i}(c,x^{\prime}\mid x,a)l_{h+1}(z-c\mid x^{\prime},\pi(x^{ \prime}))}\] \[=\int_{x,a,z}d_{h,i}(x,a)\int_{z}u_{h}(z\mid x,a)\alpha_{h+1}^{l} (x,a)^{-1}\] \[\leq\frac{1+\varepsilon}{1-\varepsilon}=1+\frac{2\varepsilon}{1- \varepsilon}\leq 1+\frac{4}{n}.\]

Therefore,

\[\mathbb{E}\Bigg{[}\exp\sum_{i=1}^{n}\log\Biggl{(}\frac{u_{h}(z_{h,i}^{l,\pi} \mid x_{h,i},a_{h,i})}{\mathcal{T}_{h}^{\pi}l_{h+1}(z_{h,i}^{l,\pi}\mid x_{h,i },a_{h,i})}\Biggr{)}\Bigg{]}\leq(1+4/n)^{n}\leq e^{4}.\]

Thus, by Markov's inequality, w.p. at least \(1-\delta\), we have

\[\sum_{i=1}^{n}\log\Biggl{(}\frac{u_{h}(z_{h,i}^{l,\pi}\mid x_{h,i},a_{h,i})}{ \mathcal{T}_{h}^{\pi}l_{h+1}(z_{h,i}^{l,\pi}\mid x_{h,i},a_{h,i})}\Biggr{)} \leq\ln(e^{4}/\delta).\]

To conclude, apply union bound to get this result for all brackets. 

For the remainder of this section, we assume the policy class \(\Pi\) is finite. However, it is possible to extend our results using policy covers in the Hamming distance; in that case, \(\log|\Pi|\) would be replaced by the log covering number or entropy integral of \(\Pi\)(as in Zhou et al., 2023; Kallus et al., 2022). We note that for the _online_ case, we rely on the assumption that for any \(f\in\mathcal{F}\) we have \(\pi^{f}\in\Pi\), where recall that \(\pi_{h}^{f}(x)=\arg\min_{a}\bar{f}_{h}(x,a)\). This is because \(\mathcal{T}^{\star,D}\) is not a contraction so we cannot operate with \(\mathcal{T}^{\star,D}\) directly and instead operate with \(\mathcal{T}^{\star^{f},D}\). We highlight that this assumption is automatically satisfied in tabular MDPs, since the whole policy space is finite, and \(\log[\Pi]=\mathcal{O}(X\log(A))\) is lower order compared to log of the bracketing entropy of \(\mathcal{F}_{tab}\), which is \(\mathcal{O}(X^{2}A^{2})\). In contrast, in non-distributional methods such as GOLF, the regular Bellman optimality operator is a contraction so standard Lipschitz arguments for covering go through. We note that it is also possible to construct covers of \(\mathcal{F}\) in the Hellinger distance, but the metric entropy of \(\mathcal{F}_{tab}\) seems to be on the same order as its bracketing entropy.

We now describe the version space construction for general \(\mathcal{F}\), first for the online setting. Fix any \(k\), and define the set

\[\mathcal{F}_{f^{\downarrow},\pi,h}=\Bigg{\{}f\in\mathcal{F}:\sum_{i=1}^{k}\log f _{h}(z_{h,i}^{f^{\downarrow},\pi}\mid x_{h,i},a_{h,i})\geq\sum_{i=1}^{k}\log \textsc{Me}_{h}^{f^{\downarrow},\pi}(z_{h,i}^{f^{\downarrow},\pi}\mid x_{h,i},a_ {h,i})-\beta\Bigg{\}}\]

Then, construct the version space as

\[\mathcal{F}_{k}=\big{\{}f\in\mathcal{F}:f_{h}\in\mathcal{F}_{f^{\downarrow}, \pi^{f},h},\forall h\in[H]\big{\}}.\]

[MISSING_PAGE_FAIL:26]

**Theorem F.3**.: _Fix any \(\delta\in(0,1)\) and suppose Assumption 5.1. Set \(\beta=\log(H|\Pi|\cdot N_{\mathbb{D}}[(n|\mathcal{Y}|)^{-1},\mathcal{F},\|\cdot\| _{\infty})/\delta)\). Then, w.p. at least \(1-\delta\), the following holds for all policies \(\pi\in\Pi\):_

1. _The policy cost distribution is in the version space, i.e.,_ \(Z^{\pi}\in\mathcal{F}_{\pi}\)_._
2. _Any function in the version space has bounded triangular discrimination with the ground truth data-generating distribution, i.e., for all_ \(f\in\mathcal{F}_{\pi}\) _and_ \(h\in[H]\)_,_ \[\mathbb{E}_{\nu_{h}}\Big{[}H^{2}(f_{h}(x_{h},a_{h})\ \|\ \mathcal{T}_{h}^{\pi,D}f_{h+1}(x_{h},a_{h}))\Big{]}\leq 6 0\beta N^{-1}.\]

Proof.: The proof is the same as in Theorem F.2, but instead of \(\pi^{f}\), we fix any \(\pi\). 

## Appendix G The \(\ell_{p}\) distributional eluder dimension

Let \(\mathcal{S}\) denote any input space (for example, we will later instantiate \(\mathcal{S}=\mathcal{X}\) or \(\mathcal{S}=\mathcal{X}\times\mathcal{A}\)). Let \(\Psi\) denote a set of functions mapping from \(\mathcal{S}\to\mathbb{R}\). Let \(\mathcal{D}\) be a set of distributions on \(\mathcal{S}\).

Recall the definition of \(\varepsilon\)-independent sequence (of distributions) from Jin et al. (2021).

**Definition G.1** (\(\ell_{2}\)-independent sequence).: A distribution \(\nu\in\mathcal{D}\) is \((\varepsilon,\ell_{2})\)-independent of a sequence \(\big{\{}d^{(1)},\ldots,d^{(n)}\big{\}}\subset\mathcal{D}\) if there exists \(\psi\in\Psi\) such that \(|\mathbb{E}_{\nu}\psi|>\varepsilon\) and also \(\sqrt{\sum_{i=1}^{n}(\mathbb{E}_{d^{(i)}}\psi)}^{2}\leq\varepsilon\).

Note that the definition is on sequences of distributions, which generalizes the original definition on sequences of points from Russo and Van Roy (2013).

We now generalize the above definition for the general \(\ell_{p}\) norm.

**Definition G.2** (\(\ell_{p}\)-independent sequence).: A distribution \(\nu\in\mathcal{D}\) is \((\varepsilon,\ell_{p})\)-independent of a sequence \(\big{\{}d^{(1)},\ldots,d^{(n)}\big{\}}\subset\mathcal{D}\) if there exists \(\psi\in\Psi\) such that \(|\mathbb{E}_{\nu}\psi|>\varepsilon\) and also \(\sum_{i=1}^{n}|\mathbb{E}_{d^{(i)}}\psi|^{p}\leq\varepsilon^{p}\).

Using the definition of independent sequences established so far, we define the \(\ell_{p}\) distributional eluder dimension.

**Definition G.3** (\(\ell_{p}\)-distributional eluder dimension).: For any \(p\), define the \(\ell_{p}\)-distributional eluder dimension (denoted by \(\mathrm{DE}_{p}(\Psi,\mathcal{D},\varepsilon)\)) as the length of the longest sequence \(\{d^{(1)},\ldots,d^{(d)}\}\subset\mathcal{D}\) such that there exists \(\varepsilon^{\prime}\geq\varepsilon\), such that for all \(t\in[d]\), \(d^{(t)}\) is \((\varepsilon^{\prime},\ell_{p})\)-independent of \(d^{(1)},\ldots,d^{(t-1)}\).

Of particular interest to us is the \(\ell_{1}\) case. We show that the \(\ell_{1}\) eluder dimension is dominated by the \(\ell_{2}\) eluder dimension of Jin et al. (2021).

**Lemma 5.4**.: _For any \(\Psi,\mathcal{D}\) and \(\varepsilon>0\), we have \(\mathrm{DE}_{1}(\Psi,\mathcal{D},\varepsilon)\leq\mathrm{DE}_{2}(\Psi, \mathcal{D},\varepsilon)\)._

Proof.: Since \(\sqrt{\sum_{i}x_{i}^{2}}\leq\sum_{i}|x_{i}|\), we have that any witness (long independent sequence) for \(\ell_{1}\) is also a witness for \(\ell_{2}\). So, the maximum length of the \(\ell_{2}\) witnesses is longer than the \(\ell_{1}\) witnesses. Liu et al. (2022, Proposition 19) obtains an analogous result for the non-distributional eluder dimension of Russo and Van Roy (2013). 

We now prove the key pigeonhole result for the \(\ell_{1}\) distributional eluder dimension.

**Theorem 5.3**.: _Let \(C:=\sup_{d\in\mathcal{D},f\in\Psi}|\mathbb{E}_{d}f|\) be the envelope. Fix any \(K\in\mathbb{N}\) and sequences \(f^{(1)},\ldots,f^{(K)}\subseteq\Psi\), \(d^{(1)},\ldots,d^{(K)}\subseteq\mathcal{D}\). Let \(\beta\) be a constant such that for all \(k\in[K]\), we have, \(\sum_{i=1}^{k-1}\bigl{|}\mathbb{E}_{d^{(i)}}f^{(k)}\bigr{|}\leq\beta\). Then, for all \(k\in[K]\), we have_

\[\sum_{t=1}^{k}\Bigl{|}\mathbb{E}_{d^{(t)}}f^{(t)}\Bigr{|}\leq\inf_{0<\varepsilon \leq 1}\{\mathrm{DE}_{1}(\Psi,\mathcal{D},\varepsilon)(2C+\beta\log(C/ \varepsilon))+k\varepsilon\}.\]

Proof.: For any \(\Gamma\subset\mathcal{D}\), \(\nu\in\mathcal{D}\), and \(0<\varepsilon\leq 1\), let \(L(\nu,\Gamma,\varepsilon)\) denote the number of disjoint subsets of \(\Gamma\) such that each subset is \(\varepsilon\)-dependent of \(\nu\), _i.e._, for all such disjoint subsets of \(\Gamma\), it is not the case that \(\nu\) is \((\varepsilon,\ell_{1})\)-independent of each subset.

**Fact 1: For any \(\varepsilon\), if \(\big{|}\mathbb{E}_{d^{(k)}}f^{(k)}\big{|}>\varepsilon\) for some \(k\in[K]\), then \(L(d^{(k)},d^{(1:k-1)},\varepsilon)<\beta/\varepsilon\).**

By definition of \(L:=L(d^{(k)},d^{(1:k-1)},\varepsilon)\), there exist disjoint subsequences \(\mathfrak{G}^{(1)},\ldots,\mathfrak{G}^{(L)}\) of \(d^{(1:k-1)}\) such that each subsequence \(\mathfrak{G}^{(i)}\) satisfies \(\sum_{d\in\mathfrak{G}^{(i)}}\big{|}\mathbb{E}_{d}f^{(k)}\big{|}>\varepsilon\). Therefore, summing over all subsequences, we have \(L\varepsilon<\sum_{i=1}^{k-1}\big{|}\mathbb{E}_{d^{(i)}}f^{(k)}\big{|}\leq\beta\), where the \(\beta\) inequality comes from the premise. This proves Fact 1.

**Fact 2: For any \(\varepsilon\) and any sequence \(\big{\{}\nu^{(1)},\ldots,\nu^{(\kappa)}\big{\}}\subset\mathcal{D}\), there exists \(j\in[\kappa]\) such that \(L(\nu^{(j)},\nu^{(1:j-1)},\varepsilon)\geq J:=\lfloor(\kappa-1)/\operatorname {DE}_{1}(\Psi,\mathcal{D},\varepsilon)\rfloor\).**

If \(J=0\), the claim is vacuously true. Otherwise, consider the following algorithm for finding the \(j\):

1. Initialize \(\mathfrak{G}^{(1)}=[\nu^{(1)}],\ldots,\mathfrak{G}^{(J)}=[\nu^{(J)}]\) and let \(j=J+1\).
2. If \(\nu^{(j)}\) is \(\varepsilon\)-dependent on all of \(\mathfrak{G}^{(i)},i\in[J]\), then the claim is proven and terminate.
3. Otherwise, there exists some \(\mathfrak{G}^{(i)},i\in[J]\) such that \(\nu^{(j)}\) is \(\varepsilon\)-independent of it. Append \(\nu^{(j)}\) to \(\mathfrak{G}^{(i)}\), _i.e._, \(\mathfrak{G}^{(i)}=\mathfrak{G}^{(i)}+[\nu^{(j)}]\). Increment \(j=j+1\) and go back to Step 2.

Hence, we need to argue this process terminates at Step 2 before \(j\) gets to \(\kappa+1\). We prove this by contradiction: assume \(j\) gets to \(\kappa+1\). Let \(i\in[J]\) be such that \(\mathfrak{G}^{(i)}\) has the most elements (break ties arbitrarily). Since \(\kappa=\sum_{i=1}^{J}\big{|}\mathfrak{G}^{(i)}\big{|}\leq J\big{|}\mathfrak{G }^{(i)}\big{|}\), we have that \(\big{|}\mathfrak{G}^{(i)}\big{|}\geq\kappa/J\geq\frac{\kappa}{\kappa-1} \operatorname{DE}_{1}(\Psi,\mathcal{D},\varepsilon)>\operatorname{DE}_{1}( \Psi,\mathcal{D},\varepsilon)\), where we've also used the definition of \(J\). By construction, \(\mathfrak{G}^{(i)}\) is an \(\varepsilon\)-eluder sequence, _i.e._, it is a sequence such that each element is \(\varepsilon\)-independent of its predecessors. However, this is a contradiction because its size is greater than \(\operatorname{DE}_{1}(\Psi,\mathcal{D},\varepsilon)\). Therefore, this process terminates at Step 2 for some \(j\), which is the witness for proving Fact 2.

**Fact 3: For any \(\varepsilon\) and \(k\in[K]\), we have \(\sum_{t=1}^{k}\mathbb{I}\big{[}\big{|}\mathbb{E}_{d^{(t)}}f^{(t)}\big{|}> \varepsilon\big{]}\leq\big{(}\beta\varepsilon^{-1}+1\big{)}\operatorname{DE}_ {1}(\Psi,\mathcal{D},\varepsilon)+1\).**

Fix any \(\varepsilon\) and \(k\in[K]\). Let \(\big{\{}d^{(i_{1})},\ldots,d^{(i_{\kappa})}\big{\}}\) be all the elements of \(d^{(1:k)}\) such that \(\mathbb{E}_{d^{(t)}}f^{(t)}>\varepsilon\) for \(t=i_{1},\ldots,i_{\kappa}\). By Fact 2, there exists \(j\in[\kappa]\) such that \(L(d^{(i_{j})},d^{(i_{1:j-1})},\varepsilon)\geq\lfloor(\kappa-1)/\operatorname {DE}_{1}(\Psi,\mathcal{D},\varepsilon)\rfloor\). By Fact 1, we have \(L(d^{(i_{j})},d^{(1:i_{j})},\varepsilon)\leq\beta/\varepsilon\). Finally notice that \(L(d^{(i_{j})},d^{(i_{1:j-1})},\varepsilon)\leq L(d^{(i_{j})},d^{(1:i_{j})}, \varepsilon)\) since adding more elements can only create more \(\varepsilon\)-dependent-of-\(\nu\) disjoint subsets. Thus, combining these inequalities, we have \(\lfloor(\kappa-1)/\operatorname{DE}_{1}(\Psi,\mathcal{D},\varepsilon)\rfloor< \beta/\varepsilon\). This implies \(\kappa\leq(\beta\varepsilon^{-1}+1)\operatorname{DE}_{1}(\Psi,\mathcal{D}, \varepsilon)+1\), which proves Fact 3.

**Finishing the proof**

Fix any \(k\in[K]\) and \(\omega>0\). We have

\[\sum_{t=1}^{k}\Big{|}\mathbb{E}_{d^{(t)}}f^{(t)}\Big{|} =\sum_{t=1}^{k}\int_{0}^{C}\mathbb{I}\Big{[}\Big{|}\mathbb{E}_{d ^{(t)}}f^{(t)}\Big{|}>y\Big{]}\mathrm{d}y\] \[\leq k\omega+\sum_{t=1}^{k}\int_{\omega}^{C}\mathbb{I}\Big{[} \Big{|}\mathbb{E}_{d^{(t)}}f^{(t)}\Big{|}>y\Big{]}\mathrm{d}y\] \[=k\omega+\int_{\omega}^{C}\sum_{t=1}^{k}\mathbb{I}\Big{[}\Big{|} \mathbb{E}_{d^{(t)}}f^{(t)}\Big{|}>y\Big{]}\mathrm{d}y\] \[\leq k\omega+\int_{\omega}^{C}\{(\beta/y+1)\operatorname{DE}_{1}( \Psi,\mathcal{D},\omega)+1\}\mathrm{d}y\] (Fact 3) \[\leq k\omega+(d+1)C+d\beta\log(C/\omega).\] (Monotonicity of \[\operatorname{DE}_{1}\] )

This completes the proof. 

### Bounding V-type \(\ell_{2}\) eluder dimension in low-rank MDPs

**Theorem G.4** (Bound of \(\ell_{2}\) distributional eluder for low-rank MDPs).: _Suppose the MDP is a low-rank MDP. Let \(\Psi\subset\mathcal{X}\to[0,1]\) be any class of functions mapping \(\mathcal{X}\) to \([0,1]\). Suppose\(\mathcal{D}=\{x\mapsto d_{h}^{\pi}(x):\pi\in\Pi\}\) for some \(h\in[H]\). Then, we have_

\[\mathrm{DE}_{2}(\Psi,\mathcal{D},\varepsilon)\leq\mathcal{O}(d\log(d/\varepsilon )).\] (7)

Proof.: If \(h=1\), then \(\mathcal{D}\) is a singleton. Hence, \(\mathrm{DE}_{2}(\Psi,\mathcal{D},\varepsilon)\leq 1\). Hence, suppose \(h\geq 2\); set \(h:=h-1\) and we will focus on \(d_{h+1}^{\pi}\) in the remainder. Suppose \(\big{\{}d^{(k)},f^{(k)}\big{\}}_{k\in[T]}\) is any sequence such for all \(k\in[T]\), we have that \((d^{(k)},f^{(k)})\) is \((\varepsilon,\ell_{2})\)-independent of its predecessors. For any \(k\), set \(\Sigma_{k}=\sum_{i=1}^{k-1}\mathbb{E}_{d^{(k)}}[\phi_{h}^{\star}(x_{h},a_{h})] \mathbb{E}_{d^{(k)}}[\phi_{h}^{\star}(x_{h},a_{h})]^{\top}+\lambda I\). Then, we have

\[\mathbb{E}_{d^{(k)}}f^{(k)}(x_{h+1}) =\mathbb{E}_{d^{(k)}}\int_{x_{h}}\phi_{h}^{\star}(x_{h},a_{h})^{ \top}\mathrm{d}\mu_{h}^{\star}(x_{h+1})f^{(k)}(x_{h+1})\] \[=\mathbb{E}_{d^{(k)}}\phi_{h}^{\star}(x_{h},a_{h})^{\top}\int_{x _{h+1}}f^{(k)}(x_{h+1})\mathrm{d}\mu_{h}^{\star}(x_{h+1}).\] \[\leq\|\mathbb{E}_{d^{(k)}}\phi_{h}^{\star}(x_{h},a_{h})\|_{\Sigma _{k}^{-1}}\|\int_{x_{h+1}}f^{(k)}(x_{h+1})\mathrm{d}\mu_{h}^{\star}(x_{h+1}) \|_{\Sigma_{k}}.\]

Focusing on the second term,

\[\|\int_{x_{h+1}}f^{(k)}(x_{h+1})\mathrm{d}\mu_{h}^{\star}(x_{h+1})\|_{\Sigma_{ k}}^{2}=\sum_{i=1}^{k-1}\Bigl{(}\mathbb{E}_{d^{(i)}}\left[f^{(k)}(x_{h+1}) \right]\Bigr{)}^{2}+\lambda d\]

Thus, we have shown that

\[\mathbb{E}_{d^{(k)}}f^{(k)}(x_{h+1})\leq\|\mathbb{E}_{d^{(k)}}\phi_{h}^{\star} (x_{h},a_{h})\|_{\Sigma_{k}^{-1}}\sqrt{\sum_{i=1}^{k-1}\bigl{(}\mathbb{E}_{d^{ (i)}}\big{[}f^{(k)}(x_{h+1})\big{]}\bigr{)}^{2}+\lambda d}.\]

Then, by the independent sequence assumption, we have

\[T\varepsilon <\sum_{k=1}^{T}\mathbb{E}_{d^{(k)}}f^{(k)}(x_{h+1})\leq\sum_{k= 1}^{T}\|\mathbb{E}_{d^{(k)}}\phi_{h}^{\star}(x_{h},a_{h})\|_{\Sigma_{k}^{-1}} \sqrt{\sum_{i=1}^{k-1}\bigl{(}\mathbb{E}_{d^{(i)}}\big{[}f^{(k)}(x_{h+1}) \big{]}\bigr{)}^{2}+\lambda d}\] \[\leq\sum_{k=1}^{T}\|\mathbb{E}_{d^{(k)}}\phi_{h}^{\star}(x_{h},a _{h})\|_{\Sigma_{k}^{-1}}\Bigl{(}\varepsilon+\sqrt{\lambda d}\Bigr{)} (\sqrt{\sum_{i=1}^{k-1}\bigl{(}\mathbb{E}_{d^{(i)}}\big{[}f^{(k)}(x_{h+1}) \big{]}\bigr{)}^{2}}\leq\varepsilon)\] \[\leq 2\varepsilon\sum_{k=1}^{T}\|\mathbb{E}_{d^{(k)}}\phi_{h}^{ \star}(x_{h},a_{h})\|_{\Sigma_{k}^{-1}} (\lambda=\varepsilon^{2}/d)\] \[\leq 2\varepsilon\sqrt{T}\sqrt{\sum_{k=1}^{T}\|\mathbb{E}_{d^{(k)}} \phi_{h}^{\star}(x_{h},a_{h})\|_{\Sigma_{k}^{-1}}^{2}}\] \[\leq 2\varepsilon\sqrt{T}\sqrt{d\log(1+\nicefrac{{T}}{{d \lambda}})} (\text{elliptical potential})\] \[\leq 2\varepsilon\sqrt{T}\sqrt{d\log(1+\nicefrac{{T}}{{ \varepsilon^{2}}})}.\] ( \[\lambda=\varepsilon^{2}/d\] )

For a reference of the elliptical potential, see Uehara et al. (2021, Lemmas 19&20). Rearranging, we have \(\sqrt{T}<2\sqrt{d\log(1+\nicefrac{{T}}{{\varepsilon^{2}}})}\), which implies

\[T\leq 4d\log(1+\nicefrac{{T}}{{\varepsilon^{2}}}).\]

By applying Lemma G.5, we have \(T\leq 24d\log(1+\nicefrac{{4d}}{{\varepsilon^{2}}})\). This concludes the proof. 

**Lemma G.5**.: _Let \(c_{1},c_{2}\geq 1\) be constants. Let \(x\geq 0\) be a solution to \(x\leq c_{1}\log(1+c_{2}x)\). Then, we necessarily have \(x\leq 6c_{1}\log(1+c_{1}c_{2})\)._

Proof.: Using change of variables \(B=\frac{x}{c_{1}}\), we have the inequality is equivalent to \(B\leq\log(1+B\cdot c_{1}c_{2})\). Take \(\exp\) of both sides to get \(\exp(B)\leq\alpha B+1\) where \(\alpha=c_{1}c_{2}\). From Step 3 of the proof of Russo and Van Roy (2013, Proposition 6), we have \(B\leq\frac{c}{e-1}\frac{c}{e-1}(\log(1+\alpha)+\log(e/(e-1)))\leq 3(\log(1+c_{1}c_{2})+1)\). Hence, \(x\leq c_{1}\cdot 3(\log(1+c_{1}c_{2})+1)\).

### Bounding Q-type \(\ell_{2}\) eluder dimension in tabular MDPs

**Theorem G.6** (Bound of \(\ell_{2}\) distributional eluder for tabular MDPs).: _Suppose the MDP is a tabular MDP. Let \(\Psi\subset\mathcal{X}\times\mathcal{A}\to[0,1]\) be any class of functions mapping \(\mathcal{X}\times\mathcal{A}\) to \([0,1]\). Suppose \(\mathcal{D}\) be any set of distributions. Then, we have_

\[\mathrm{DE}_{2}(\Psi,\mathcal{D},\varepsilon)\leq\mathcal{O}(SA\log(SA/ \varepsilon)).\] (8)

Proof.: Suppose \(\big{\{}d^{(k)},f^{(k)}\big{\}}_{k\in[T]}\) is any sequence such for all \(k\in[T]\), we have that \((d^{(k)},f^{(k)})\) is \((\varepsilon,\ell_{2})\)-independent of its predecessors. Since the MDP is tabular, we can interpret \(d^{(k)},f^{(k)}\) as \(SA\)-dimensional vectors. For any \(k\), set \(\Sigma_{k}=\sum_{i=1}^{k-1}d^{(i)}(d^{(i)})^{\top}+\lambda I\). Then, we have

\[\mathbb{E}_{d^{(k)}}f^{(k)}(x,a)=(d^{(k)})^{\top}f^{(k)}\leq\|d^{(k)}\|_{ \Sigma_{k}^{-1}}\|f^{(k)}\|_{\Sigma_{k}}.\]

Focusing on the second term, we have

\[\|f^{(k)}\|_{\Sigma_{k}}^{2}=\sum_{i=1}^{k-1}\Bigl{(}(d^{(i)})^{\top}f^{(k)} \Bigr{)}^{2}+\lambda SA.\]

Thus, we have

\[T\varepsilon <\sum_{k=1}^{T}\mathbb{E}_{d^{(k)}}f^{(k)}(x,a)\leq\sum_{k=1}^{T }\|d^{(k)}\|_{\Sigma_{k}^{-1}}\sqrt{\sum_{i=1}^{k-1}\bigl{(}\mathbb{E}_{d^{(i) }}[f^{(k)}(x,a)]\bigr{)}^{2}+\lambda SA}\] \[\leq\sum_{k=1}^{T}\|d^{(k)}\|_{\Sigma_{k}^{-1}}\Bigl{(}\varepsilon +\sqrt{\lambda SA}\Bigr{)}\] \[\leq 2\varepsilon\sum_{k=1}^{T}\|d^{(k)}\|_{\Sigma_{k}^{-1}}\] ( \[\lambda=\varepsilon^{2}/SA\] ) \[\leq 2\varepsilon\sqrt{T}\sqrt{\sum_{k=1}^{T}\|d^{(k)}\|_{\Sigma _{k}^{-1}}^{2}}\] \[\leq 2\varepsilon\sqrt{T}\sqrt{SA\log(1+T/\varepsilon^{2})}.\] (elliptical potential)

Rearranging, we have \(\sqrt{T}<2\sqrt{SA\log(1+T/\varepsilon^{2})}\), which implies \(T\leq 4SA\log(1+T/\varepsilon^{2})\). Then by applying Lemma G.5, we have \(T\leq 24SA\log(1+\nicefrac{{4SA}}{{\varepsilon^{2}}})\). This concludes the proof.

Proofs for Online RL

### Preliminary Lemmas

**Lemma H.1**.: _For any policy \(\pi\), conditional distribution \(d\) and \(h\in[H]\), we have_

\[\overline{\mathcal{T}_{h}^{\pi,D}d(x,a)} =\mathcal{T}_{h}^{\pi}\bar{d}(x,a),\] \[\overline{\mathcal{T}_{h}^{\star,D}d(x,a)} =\mathcal{T}_{h}^{\star}\bar{d}(x,a).\]

Proof.: \[\overline{\mathcal{T}_{h}^{\pi,D}d(x,a)} =\mathbb{E}_{y\sim\mathcal{T}_{h}^{\pi,D}d(x,a)}[y]\] \[=\mathbb{E}_{c\sim C_{h}(x,a),x^{\prime}\sim P_{h}(x,a),a^{ \prime}\sim\pi_{h+1}(x^{\prime}),y^{\prime}\sim d(x^{\prime},a^{\prime})}[c+y^ {\prime}]\] \[=\bar{C}_{h}(x,a)+\mathbb{E}_{x^{\prime}\sim P_{h}(x,a),a^{ \prime}\sim\pi_{h+1}(x^{\prime}),y^{\prime}\sim d(x^{\prime},a^{\prime})}[y^ {\prime}]\] \[=\bar{C}_{h}(x,a)+\mathbb{E}_{x^{\prime}\sim P_{h}(x,a),a^{ \prime}\sim\pi_{h+1}(x^{\prime})}\big{[}\bar{d}(x^{\prime},a^{\prime})\big{]}\] \[=\mathcal{T}_{h}^{\pi}\bar{d}(x,a).\]

\[\overline{\mathcal{T}_{h}^{\star,D}d(x,a)} =\mathbb{E}_{y\sim\mathcal{T}_{h}^{\star,D}d(x,a)}[y]\] \[=\mathbb{E}_{c\sim C_{h}(x,a),x^{\prime}\sim P_{h}(x,a),a^{ \prime}=\arg\min_{\bar{a}}\bar{d}(x^{\prime},\bar{a}),y^{\prime}\sim d(x^{ \prime},a^{\prime})}[c+y^{\prime}]\] \[=\bar{C}_{h}(x,a)+\mathbb{E}_{x^{\prime}\sim P_{h}(x,a),a^{ \prime}=\arg\min_{\bar{a}}\bar{d}(x^{\prime},\bar{a}),y^{\prime}\sim d(x^{ \prime},a^{\prime})}[y^{\prime}]\] \[=\bar{C}_{h}(x,a)+\mathbb{E}_{x^{\prime}\sim P_{h}(x,a),a^{ \prime}=\arg\min_{\bar{a}}\bar{d}(x^{\prime},\bar{a})}\big{[}\bar{d}(x^{ \prime},a^{\prime})\big{]}\] \[=\bar{C}_{h}(x,a)+\mathbb{E}_{x^{\prime}\sim P_{h}(x,a)}\Big{[} \min_{a^{\prime}}\bar{d}(x^{\prime},a^{\prime})\Big{]}\] \[=\mathcal{T}_{h}^{\star}\bar{d}(x,a).\]

**Lemma H.2** (Performance Difference Lemma (PDL)).: _For any \(f:\left(\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\right)^{H}\) and policies \(\pi,\pi^{\prime}\), we have_

\[V^{\pi}-\mathbb{E}_{a\sim\pi^{\prime}(x_{1})}[f_{1}(x_{1},a)]=\sum_{h=1}^{H} \mathbb{E}_{\pi}\Big{[}\mathcal{T}_{h}^{\pi^{\prime}}f_{h+1}(x_{h},a_{h})-f_{ h}(x_{h},\pi^{\prime})\Big{]}.\] (9)

Proof.: We proceed by inducting on the following claim: for all \(h=H+1,H,\ldots,1\),

\[V_{h}^{\pi}(x_{h})-f_{h}(x_{h},\pi^{\prime})=\sum_{t=h}^{H}\mathbb{E}_{\pi,x_ {h}}\Big{[}\mathcal{T}_{t}^{\pi^{\prime}}f_{t+1}(x_{t},a_{t})-f_{t}(x_{t},\pi^ {\prime})\Big{]}.\]

The base case of \(H+1\) is trivially true as everything is \(0\). Now fix any \(h\) and suppose the IH at \(h+1\) is true. Then

\[V_{h}^{\pi}(x_{h})-f_{h}(x_{h},\pi^{\prime})\] \[=\mathbb{E}_{\pi,x_{h}}\big{[}c_{h}+V_{h+1}^{\pi}(x_{h+1})-f_{h+1 }(x_{h+1},\pi^{\prime})+f_{h+1}(x_{h+1},\pi^{\prime})-f_{h}(x_{h},\pi^{\prime })\big{]}\] \[=\mathbb{E}_{\pi,x_{h}}\big{[}V_{h+1}^{\pi}(x_{h+1})-f_{h+1}(x_{h +1},\pi^{\prime})\big{]}+\mathbb{E}_{\pi,x_{h}}[c_{h}+f_{h+1}(x_{h+1},\pi^{ \prime})-f_{h}(x_{h},\pi^{\prime})].\]

By the IH, the first term is equal to \(\sum_{t=h+1}^{H}\mathbb{E}_{\pi,x_{h}}\Big{[}\mathcal{T}_{t}^{\pi^{\prime}}f_{t +1}(x_{t},a_{t})-f_{t}(x_{t},\pi^{\prime})\Big{]}\). The second term is exactly \(\mathbb{E}_{\pi,x_{h}}\Big{[}\mathcal{T}_{h}^{\pi^{\prime}}f_{h+1}(x_{h},a_{h })-f_{h}(x_{h},\pi^{\prime})\Big{]}\), which concludes the proof.

### Proof of Small-Loss Regret and PAC Bounds

Recall that we defined the function class and distribution class, for each \(h\), as

\[\mathcal{D}_{h}(\Pi)=\{(x,a)\mapsto d_{h}^{\pi}(x,a):\pi\in\Pi\}\] (10) \[\Psi_{h}=\big{\{}(x,a)\mapsto D_{\triangle}(f(x,a)\parallel\mathcal{ T}^{\star,D}f(x,a)):f\in\mathcal{F}\big{\}}.\]

Also, define the '\(V\)-type' analogs as follows, which will be useful for PAC instead of regret bounds.

\[\mathcal{D}_{h,v}(\Pi)=\{x\mapsto d_{h}^{\pi}(x):\pi\in\Pi\}\] (11) \[\Psi_{h,v}=\big{\{}x\mapsto\mathbb{E}_{a\sim\operatorname{Unif}( \mathcal{A})}[D_{\triangle}(f(x,a)\parallel\mathcal{T}^{\star,D}f(x,a))]:f\in \mathcal{F}\big{\}}.\]

Let us also overload notation for the eluder dimensions as

\[\operatorname{DE}_{1}(\varepsilon):=\max_{h}\operatorname{DE}_{1 }(\Psi_{h},\mathcal{D}_{h}(\Pi),\varepsilon),\] \[\operatorname{DE}_{1,v}(\varepsilon):=\max_{h}\operatorname{DE}_{ 1}(\Psi_{h,v},\mathcal{D}_{h,v}(\Pi),\varepsilon).\]

Before we prove the following main theorem, a couple of remarks are in order:

1. Recall that by Theorem G.6, we have \(\operatorname{DE}_{1}(\varepsilon)\leq\mathcal{O}(SA\log(SA/\varepsilon))\) and by Theorem G.4, we have \(\operatorname{DE}_{1,v}(\varepsilon)\leq\mathcal{O}(d\log(d/\varepsilon))\). This shows that the Eluder dimension in terms in Theorem 5.5 are appropriately bounded.
2. In Appendix D, we showed that distributional BC (Assumption 5.1) is satisfied in low-rank MDPs and the log bracketing number is bounded by \(\mathcal{O}(dM\log(d/\varepsilon)+\log|\Phi|)\) where \(\Phi\) is a realizable class for \(\phi^{\star}\). This shows that the BC assumption of Theorem 5.5 is satisfied and \(\beta\) is appropriately bounded for low-rank MDPs.

Taken together, these two points imply that we have a small-loss PAC bound for low-rank MDPs: concretely, we have \(V^{\bar{\pi}}-V^{\star}\leq\widetilde{\mathcal{O}}\bigg{(}dH\sqrt{\frac{AV^{ \star}\log|\Phi|}{K}}+\frac{d^{2}H^{2}A\log|\Phi|}{K}\bigg{)}\).

We now prove the our main result for online RL: Theorem 5.5. We will prove the result with general function classes, so we will replace the \(|\mathcal{F}|\) by its \(\ell_{\infty}\) bracketing number, _i.e._, \(\beta=\log(HKN_{\![}(1/K,\mathcal{F},\ell_{\infty})/\delta)\).

**Theorem 5.5**.: _Suppose DistBC holds (Assumption 5.1). For any \(\delta\in(0,1)\), w.p. at least \(1-\delta\), running \(\textsc{O}\textsc{-DISCO}\) with \(\beta=\log(HK|\mathcal{F}|/\delta)\) guarantees the following regret bound,_

\[\operatorname{Regret_{\textsc{O}\textsc{-DISCO}}}(K)\leq 160H\sqrt{ KV^{\star}\operatorname{DE}_{1}(1/K)\log(K)\beta}+18000H^{2}\operatorname{DE}_{ 1}(1/K)\log(K)\beta.\]

_If \(\textsc{UAE}=\textsc{True}\) (Algorithm 4), then the learned mixture policy \(\bar{\pi}\) is guaranteed to satisfy,_

\[V^{\bar{\pi}}-V^{\star}\leq 160H\sqrt{\frac{AV^{\star}\operatorname{DE}_{1,v}(1 /K)\log(K)\beta}{K}}+\frac{18000H^{2}A\operatorname{DE}_{1,v}(1/K)\log(K)\beta }{K}.\]

Proof.: For shorthand, let \(\delta_{h,k}(x,a):=D_{\triangle}(f_{h}^{(k)}(x,a)\parallel\mathcal{T}_{h}^{ \star,D}f_{h+1}^{(k)}(x,a))\) and \(\Delta_{k}:=\sum_{h=1}^{H}\mathbb{E}_{\pi^{k}}[\delta_{h,k}(x_{h},a_{h})]\). Notice that since \(\pi_{h+1}^{k}(x)=\operatorname*{arg\,min}_{h+1}\bar{f}_{h+1}^{(k)}(x,a)\), we have \(\mathcal{T}_{h}^{\pi^{k},D}f_{h+1}^{(k)}(x,a)=\mathcal{T}_{h}^{\star,D}f_{h+1} ^{(k)}(x,a)\), so \(\delta_{h,k}(x,a)=D_{\triangle}(f_{h}^{(k)}(x,a)\parallel\mathcal{T}_{h}^{\pi ^{k},D}f_{h+1}^{(k)}(x,a))\) as well.

By Theorem F.2, we have the following two facts **for all \(k\in[K]\)**,

(i) Optimism: \(\min_{a}\bar{f}_{1}^{(k)}(x_{1},a)\leq V^{\star}\) (since \(Z^{\star}\in\mathcal{F}_{k}\)) and

(ii) Low training error: for all \(h\), we have

If UAE=False. \(\sum_{i<k}\mathbb{E}_{\pi^{i}}[\delta_{h,k}(s_{h},a_{h})]\leq 240\beta\).

If UAE=True. \(\sum_{i<k}\mathbb{E}_{\pi^{i}}\big{[}\mathbb{E}_{a^{\prime}\sim\operatorname {unif}(\mathcal{A})}[\delta_{h,k}(s_{h},a_{h})]\big{]}\leq 240\beta\).

The \(240\) comes from the constants of Theorem F.2 and the fact that \(D_{\triangle}(a,b)\leq 4H^{2}(a,b)\) for all distributions \(a,b\).

Now, fix any episode \(k\in[K]\).

\[V^{\pi^{k}}-V^{\star}\] \[\leq V^{\pi^{k}}-\min_{a}\bar{f}_{1}^{(k)}(x_{1},a)\] (Fact (i)) \[=\sum_{h=1}^{H}\mathbb{E}_{\pi^{k}}\Big{[}\mathcal{T}_{h}^{\pi^{k} }\bar{f}_{h+1}^{(k)}(x_{h},a_{h})-\bar{f}_{h}^{(k)}(x_{h},\pi_{h}^{k}(x_{h})) \Big{]}\] (PDL Lemma H.2) \[=\sum_{h=1}^{H}\mathbb{E}_{\pi^{k}}\Big{[}\overline{\mathcal{T}_{ h}^{\pi^{k},D}f_{h+1}^{(k)}}(x_{h},a_{h})-\bar{f}_{h}^{(k)}(x_{h},a_{h}) \Big{]}\] (Lemma H.1) \[\leq\sum_{h=1}^{H}\sqrt{\mathbb{E}_{\pi^{k}}\Big{[}4\bar{f}_{h}^{ (k)}(x_{h},a_{h})+\delta_{h,k}(x_{h},a_{h})\Big{]}}\cdot\sqrt{\mathbb{E}_{\pi ^{k}}[\delta_{h,k}(x_{h},a_{h})]}\] (Eq. ( \[\triangle_{2}\] )) \[\leq\sum_{h=1}^{H}\sqrt{4eV^{\pi^{k}}+17H\sum_{t=h}^{H}\mathbb{E }_{\pi^{k}}[\delta_{t,k}(x_{t},a_{t})]}\cdot\sqrt{\mathbb{E}_{\pi^{k}}[\delta _{h,k}(x_{h},a_{h})]}\] (Lemma H.3 and \[\mathbb{E}_{\pi}[Q_{h}^{\pi}(s_{h},a_{h})]\leq V^{\pi}\] ) \[\leq\sqrt{4eV^{\pi^{k}}+17H\Delta_{k}}\cdot\sqrt{H\Delta_{k}}\] ( \[\bigstar\] ) \[\leq\sqrt{4eHV^{\pi^{k}}\Delta_{k}}+5H\Delta_{k}\] \[\leq 2\sqrt{H}\eta^{-1}V^{\pi^{k}}+2\sqrt{H}\eta\Delta_{k}+5H \Delta_{k}.\]

In \(\bigstar\), we used Cauchy Schwartz. Setting \(\eta=4\sqrt{H}\) and rearranging, we have

\[V^{\pi^{k}}\leq 2V^{\star}+16H\Delta_{k}+10H\Delta_{k}\leq 2V^{\star}+26H \Delta_{k}.\]

Plugging this into \(\bigstar\), and noting \(104e+17\leq 300\), we have

\[V^{\pi^{k}}-V^{\star}\leq\sqrt{8eV^{\star}+300H\Delta_{k}}\sqrt{H\Delta_{k}}.\]

Thus, summing the instantaneous regrets over all episodes, we get

\[\sum_{k=1}^{K}V^{\pi^{k}}-V^{\star} \leq\sum_{k=1}^{K}\sqrt{8eV^{\star}+300H\Delta_{k}}\sqrt{H\Delta_ {k}}\] \[\leq\sqrt{8eV^{\star}+300H\sum_{k}\Delta_{k}}\sqrt{H\sum_{k} \Delta_{k}}\] (Cauchy-Schwartz) \[\leq 5\sqrt{HKV^{\star}\sum_{k}\Delta_{k}}+18H\sum_{k}\Delta_{k}.\]

Last step: bounding \(\sum_{k}\Delta_{k}\).In this final step, we invoke the pigeonhole property of the eluder dimension, as proven in Theorem 5.3. Note that the precondition of Theorem 5.3 is satisfied by Fact (ii) mentioned at the beginning of this proof. Also, since the triangular discrimination is always bounded by \(1\), we have that \(C\) in Theorem 5.3 is at most \(1\), and we will also pick \(\varepsilon=1/K\).

On one hand, if UAE=False, then,

\[\sum_{k=1}^{K}\Delta_{k}=\sum_{h=1}^{H}\sum_{k=1}^{K}\mathbb{E}_{\pi^{k}}[ \delta_{h,k}(x_{h},a_{h})]\leq 1000H\operatorname{DE}_{1}(1/K)\beta\log(K).\]

On the other hand, if UAE=True, then, we use the V-type analogs,

\[\sum_{k=1}^{K}\Delta_{k} =\sum_{h=1}^{H}\sum_{k=1}^{K}\mathbb{E}_{\pi^{k}}[\delta_{h,k}(x_ {h},a_{h})]\] \[\leq A\sum_{h=1}^{H}\sum_{k=1}^{K}\mathbb{E}_{\pi^{k}}\big{[} \mathbb{E}_{a\sim\operatorname{unif}(\mathcal{A})}\delta_{h,k}(x_{h},a)\big{]}\] \[\leq 1000AH\operatorname{DE}_{1}(1/K)\beta\log(K).\]This concludes the proof for both the regret and PAC bounds. 

**Lemma H.3** (Self-bounding lemma).: _Let \(f\in\mathcal{F}\) and let \(\pi\) be any policy. Let us denote \(\delta_{h}(x,a):=D_{\triangle}(f_{h}(x,a)\parallel\mathcal{T}_{h}^{\pi,D}f_{h+1} (x,a))\). Then, for all \(h\in[H]\), for all \(x_{h},a_{h}\), we have_

\[\bar{f}_{h}(x_{h},a_{h})\leq eQ_{h}^{\pi}(x_{h},a_{h})+4H\sum_{t=h}^{H}\mathbb{ E}_{\pi,x_{h},a_{h}}[\delta_{t}(x_{t},a_{t})].\]

Proof.: We prove the following refined subclaim inductively: for all \(h\in[H]\), for all \(x_{h},a_{h}\), we have

\[\bar{f}_{h}(x_{h},a_{h})\leq\sum_{t=h}^{H}\biggl{(}1+\frac{1}{H}\biggr{)}^{t-h }\mathbb{E}_{\pi,x_{h},a_{h}}[\bar{c}_{t}(x_{t},a_{t})+2H\delta_{t}(x_{t},a_{t })].\] (IH)

For \(H+1\) this is trivially true. Now fix any \(h\) and suppose IH is true for \(h+1\). By Eq. (\(\triangle_{2}\)), for any \(h,x_{h},a_{h}\), we have,

\[\bar{f}_{h}(x_{h},a_{h})-\mathcal{T}_{h}^{\pi}\bar{f}_{h+1}(x_{h},a_{h}) \leq\sqrt{4\mathcal{T}_{h}^{\pi}\bar{f}_{h+1}(x_{h},a_{h})+\delta _{h}(x_{h},a_{h})}\sqrt{\delta_{h}(x_{h},a_{h})}\] \[\leq\sqrt{4\mathcal{T}_{h}^{\pi}\bar{f}_{h+1}(x_{h},a_{h})\delta _{h}(x_{h},a_{h})}+\delta_{h}(x_{h},a_{h})\] \[\leq\frac{1}{H}\mathcal{T}_{h}^{\pi}\bar{f}_{h+1}(x_{h},a_{h})+(H +1)\delta_{h}(x_{h},a_{h}).\] (AM-GM)

In particular, we have that

\[\bar{f}_{h}(x_{h},a_{h})\] \[\leq\biggl{(}1+\frac{1}{H}\biggr{)}\mathcal{T}_{h}^{\pi}\bar{f}_{ h+1}(x_{h},a_{h})+2H\delta_{h}(x_{h},a_{h})\] \[=\biggl{(}1+\frac{1}{H}\biggr{)}\Bigl{(}\bar{c}_{h}(x_{h},a_{h})+ \mathbb{E}_{x_{h+1}\sim P_{h}^{*}(x_{h},a_{h})}\bigl{[}\bar{f}_{h+1}(x_{h+1}, \pi)\bigr{]}\Bigr{)}+2H\delta_{h}(x_{h},a_{h})\] \[\leq\biggl{(}1+\frac{1}{H}\biggr{)}\Biggl{(}\bar{c}_{h}(x_{h},a_{h })+\mathbb{E}_{x_{h+1}\sim P_{h}^{*}(x_{h},a_{h})}\biggl{[}\sum_{t=h+1}^{H} \biggl{(}1+\frac{1}{H}\biggr{)}^{t-h-1}\mathbb{E}_{\pi,x_{h+1}}[\bar{c}_{t}(x_ {t},a_{t})+2H\delta_{t}(x_{t},a_{t})]\biggr{]}\Biggr{)}\] (IH) \[+2H\delta_{h}(x_{h},a_{h}),\]

which proves the inductive claim. Noting that \(\sum_{t=1}^{H}(1+1/H)^{t}\leq e\), we have proven the lemma. 

### Regret Bounds for Tabular MDPs

**Theorem H.4** (Small-loss regret for tabular MDP).: _Suppose the MDP is tabular with \(X\) states and assume Assumption 5.1. Fix any \(\delta\in(0,1)\) and set \(\beta=\log(HK|\mathcal{F}|/\delta)\). Then, w.p. at least \(1-\delta\),_

\[\mathrm{Regret}_{\text{O-DISCO}}(K)\in\mathcal{O}(H\sqrt{XAKV^{\star}\beta}+H^ {2}XA\beta).\]

In terms of \(H,X,A,K\) scaling, our bound matches that of GOLF (Xie et al., 2023) and is only a \(H\) factor looser than that of the minimax lower bound \(\widetilde{\mathcal{O}}(\sqrt{XAK})\). The key benefit over prior bounds is that our leading term scales with the minimum cost of the problem \(V^{\star}\). For example, if \(V^{\star}\approx 0\), O-DISCO attains \(\mathcal{O}(\log K)\) regret while uniform regret bounds are lower bounded by \(\Omega(\sqrt{K})\). Compared to the minimax-optimal UCBVI (Azar et al., 2017), one weakness of our theorem is that it needs a \(\mathcal{F}\) satisfying BC. Fortunately, in tabular MDPs where cost is only revealed at the last step from a known distribution, we can choose \(\mathcal{F}_{tab}\) as described in Wu et al. (2023, Lemma 4.15) to automatically satisfy BC. By extending our theory via bracketing entropy (Appendix F), we can derive that \(\mathcal{F}_{tab}\) yields \(\beta=\mathcal{O}(X^{2}A^{2}\log(XAHK/\delta))\). We note that if costs are unknown but discrete, it is possible to construct a BC function class with \(\beta\) scaling as \(\mathcal{O}(X^{2}A^{2}\log(nXAHK/\delta))\) where \(n\) is the maximum number of possible cumulative costs.

Extension to linear MDPsThe Q-type dimension captures Linear MDPs when squared loss is used by exploiting the fact that the bellman residual is linear in \(\phi^{\star}(x,a)\)[11]. However, since our function class is the set of triangular discriminations, rather than the Bellman residual, we find that the Q-type dimension does not immediately capture Linear MDPs unless regularity assumptions are made. For instance, we believe that Linear MDPs are captured by the Q-type dimension if we assume that \(Z_{h}^{\pi}(z\mid x,a)\) is lower bounded, _i.e._, the value distribution is sufficiently smooth.

## Appendix I Proofs for Offline RL

**Theorem 6.1** (Small-Loss PAC bound for P-Disco).: _Assume Assumption 5.1. For any \(\delta\in(0,1)\), w.p. at least \(1-\delta\), running P-DISCO with \(\beta=\log(H|\Pi||\mathcal{F}|/\delta)\) learns a policy \(\widehat{\pi}\) that enjoys the following PAC bound with respect to any comparator policy \(\widetilde{\pi}\in\Pi\):_

\[V^{\widetilde{\pi}}-V^{\widetilde{\pi}}\leq 9H\sqrt{\frac{C^{ \widetilde{\pi}}V^{\widetilde{\pi}}\widetilde{\beta}}{N}}+\frac{30H^{2}C^{ \widetilde{\pi}}\beta}{N}.\]

Proof of Theorem 6.1.: For shorthand, let \(\delta_{h}^{\pi}(x,a)=D_{\triangle}(f_{h}^{\pi}(x,a)\parallel\mathcal{T}_{h}^ {\pi,D}f_{h+1}^{\pi}(x,a))\) and \(\Delta^{\pi}=\sum_{h=1}^{H}\mathbb{E}_{\pi}[\delta_{h}^{\pi}(x_{h},a_{h})]\). Also, let \(f(x,\pi)=\mathbb{E}_{a\sim\pi(x)}[f(x,a)]\).

By Theorem F.3, we have the following two facts, for all \(\pi\in\Pi\),

(i) Pessimism: \(V^{\pi}\leq\bar{f}_{1}^{\pi}(x_{1},\pi)\) (since \(Z^{\pi}\in\mathcal{F}_{\pi}\)) for all \(\pi\in\Pi\), and

(ii) \(\mathbb{E}_{\nu_{h}}[\delta_{h}^{\pi}(x_{h},a_{h})]\leq\beta^{\prime}N^{-1}\) for all \(h\) where Theorem F.3 and the fact that \(D_{\triangle}\leq 4H^{2}\) certifies that \(\beta^{\prime}=240\beta\) is sufficient.

With these two facts, we can bound the suboptimality of \(\widehat{\pi}\) as follows:

\[V^{\widetilde{\pi}}-V^{\widetilde{\pi}}\] \[\leq\bar{f}_{1}^{\widetilde{\pi}}(x_{1},\widehat{\pi})-V^{ \widetilde{\pi}}\] (Fact (i)) \[\leq\bar{f}_{1}^{\widetilde{\pi}}(x_{1},\widetilde{\pi})-V^{ \widetilde{\pi}}\] (Policy selection scheme in Algorithm 3 (Line 4)) \[=\sum_{h=1}^{H}\mathbb{E}_{\widetilde{\pi}}\Big{[}\bar{f}_{h}^{ \widetilde{\pi}}(x_{h},\widetilde{\pi})-\mathcal{T}_{h}^{\widetilde{\pi}}\bar {f}_{h+1}^{\widetilde{\pi}}(x_{h},a_{h})\Big{]}\] (PDL Lemma H.2) \[\leq\sum_{h=1}^{H}\sqrt{\mathbb{E}_{\widetilde{\pi}}\big{[}4\bar{ f}_{h}^{\widetilde{\pi}}(x_{h},a_{h})+\delta_{h}^{\widetilde{\pi}}(x_{h},a_{h}) \big{]}}\sqrt{\mathbb{E}_{\widetilde{\pi}}\big{[}\delta_{h}^{\widetilde{\pi}}( x_{h},a_{h})\big{]}}\] (Eq. ( \[\triangle_{2}\] )) \[\leq\sum_{h=1}^{H}\sqrt{4eV^{\widetilde{\pi}}+17H\sum_{t=h}^{H} \mathbb{E}_{\widetilde{\pi}}\big{[}\delta_{t}^{\widetilde{\pi}}(x_{t},a_{t}) \big{]}}\sqrt{\mathbb{E}_{\widetilde{\pi}}\big{[}\delta_{h}^{\widetilde{\pi}}( x_{h},a_{h})\big{]}}\] (Lemma H.3) \[\leq\sqrt{4eV^{\widetilde{\pi}}+17H\Delta^{\widetilde{\pi}}}\sqrt {H\Delta^{\widetilde{\pi}}}\] \[\leq 4\sqrt{HV^{\widetilde{\pi}}\Delta^{\widetilde{\pi}}}+5H\Delta^{ \widetilde{\pi}}.\]

Finally, we can bound \(\Delta^{\widetilde{\pi}}\) by a change of measure,

\[\Delta^{\widetilde{\pi}} =\sum_{h=1}^{H}\mathbb{E}_{\widetilde{\pi}}\Big{[}\delta_{h}^{ \widetilde{\pi}}(x_{h},a_{h})\Big{]}\] \[\leq C^{\widetilde{\pi}}\sum_{h=1}^{H}\mathbb{E}_{\nu_{h}}[\delta_ {h}(x_{h},a_{h})]\] \[\leq C^{\widetilde{\pi}}H\cdot\beta^{\prime}N^{-1}.\] (Fact (ii))

Therefore,

\[V^{\widetilde{\pi}}-V^{\widetilde{\pi}} \leq 4H\sqrt{\frac{C^{\widetilde{\pi}}V^{\widetilde{\pi}}\beta^{ \prime}}{N}}+\frac{5H^{2}C^{\widetilde{\pi}}\beta^{\prime}}{N}.\]Extension: Small-Return Bounds

In this section, we show that O-DISCO and P-DISCO can also be used to obtain small-return bounds. Compared to the algorithms presented in the main text for minimizing cost, we simply have to replace \(\min\) with \(\max\) (and vice versa) for maximizing reward, _i.e._, see Appendix B and enable the SmallReturn flag. The proofs are also largely the same, with slight changes to the first few steps.

**Theorem J.1**.: _Assume Assumption 5.1 and suppose we want to maximize returns (instead of minimize cost), so enable the SmallReturn flag. Fix any \(\delta\in(0,1)\) and set \(\beta=\log(HK|\mathcal{F}|/\delta)\) and \(\beta^{\prime}=60\beta\). Then, w.p. at least \(1-\delta\), running O-DISCO (Algorithm 4) with \(\operatorname{UAE}=\textsc{False}\) yields the following small-loss regret bound,_

\[\operatorname{Regret}_{\text{\emph{0-DISCO}}}(K)\leq 5H\sqrt{KV^{\star} \operatorname{LSEC}(K)\beta^{\prime}}+18H^{2}\operatorname{LSEC}(K)\beta^{ \prime}.\] (12)

_If instead \(\operatorname{UAE}=\textsc{True}\), the outputted policy \(\bar{\pi}\) enjoys the following small-loss PAC bound,_

\[V^{\star}-V^{\sharp}\leq 5H\sqrt{\frac{AV^{\star}\operatorname{LSEC}_{v}(K) \beta^{\prime}}{K}}+18H^{2}\frac{A\operatorname{LSEC}_{v}(K)\beta^{\prime}}{K}.\]

Proof.: Adopt the same notation as in the proof of Theorem 5.5. By Theorem F.2, we have the following two facts for all \(k\in[K]\),

(i) Optimism: \(V^{\star}\leq\max_{a}\bar{f}_{1}^{(k)}(x_{1},a)\) (since \(Z^{\star}\in\mathcal{F}_{k}\)) and

(ii) \(\sum_{i<k}\mathbb{E}_{\pi^{i}}[\delta_{h,k}(s_{h},a_{h})]\leq\beta^{\prime}\) for all \(h\). If UAE=True, then \(a_{h}\) is sampled from \(\operatorname{unif}(\mathcal{A})\) rather than \(\pi^{i}\), _i.e._, we have \(\sum_{i<k}\mathbb{E}_{s_{h}\sim\pi^{i},a_{h}\sim\operatorname{unif}(\mathcal{A })}[\delta_{h,k}(s_{h},a_{h})]\leq\beta^{\prime}\), where \(\beta^{\prime}\lesssim\beta\). Theorem F.2 certifies that \(\beta^{\prime}=60\beta\) is sufficient.

Fix any episode \(k\in[K]\). Then,

\[V^{\star}-V^{\pi^{k}}\] \[\leq\max_{a}\bar{f}_{1}^{(k)}(x_{1},a)-V^{\pi^{k}}\] (Fact (i)) \[=\sum_{h=1}^{H}\mathbb{E}_{\pi^{k}}\Big{[}\bar{f}_{h}^{(k)}(x_{h},\pi_{h}^{k}(x_{h}))-\mathcal{T}_{h}^{\pi^{k}}\bar{f}_{h+1}^{(k)}(x_{h},a_{h}) \Big{]}\] (PDL Lemma H.2) \[=\sum_{h=1}^{H}\mathbb{E}_{\pi^{k}}\Big{[}\bar{f}_{h}^{(k)}(x_{h},a_{h})-\overline{\mathcal{T}_{h}^{\pi^{k},D}f_{h+1}^{(k)}}(x_{h},a_{h})\Big{]}\] (Lemma H.1) \[\leq\sum_{h=1}^{H}\sqrt{\mathbb{E}_{\pi^{k}}\Big{[}4\bar{f}_{h}^ {(k)}(x_{h},a_{h})+\delta_{h,k}(x_{h},a_{h})\Big{]}}\cdot\sqrt{\mathbb{E}_{\pi ^{k}}[\delta_{h,k}(x_{h},a_{h})]}\] (Eq. ( \[\triangle_{2}\] )) \[\leq\sum_{h=1}^{H}\sqrt{4eV^{\pi^{k}}+17H\sum_{t=h}^{H}\mathbb{E }_{\pi^{k}}[\delta_{t,k}(x_{t},a_{t})]}\cdot\sqrt{\mathbb{E}_{\pi^{k}}[\delta _{h,k}(x_{h},a_{h})]}\] (Lemma H.3 and \[\mathbb{E}_{\pi}[Q_{h}^{\pi}(s_{h},a_{h})]\leq V^{\pi}\] ) \[\leq\sqrt{4eV^{\pi^{k}}+17H\Delta_{k}}\cdot\sqrt{H\Delta_{k}}\] ( \[\clubsuit\] )

Thus, summing the instantaneous regrets over all episodes, we get

\[\sum_{k=1}^{K}V^{\pi^{k}}-V^{\star} \leq\sum_{k=1}^{K}\sqrt{4eV^{\star}+17H\Delta_{k}}\sqrt{H\Delta_{ k}}\] \[\leq\sqrt{4eV^{\star}+17H\sum_{k}\Delta_{k}}\sqrt{H\sum_{k} \Delta_{k}}\] (Cauchy-Schwartz) \[\leq 5\sqrt{HKV^{\star}\sum_{k}\Delta_{k}}+18H\sum_{k}\Delta_{k}.\]

The bounds for \(\Delta_{k}\) are the same as in Theorem 5.5.

In some sense, the proof for the small-returns bound is actually easier than the small-loss bound. Recall that in the cost-minimizing setting, we needed to perform a crucial Cauchy-Schwartz step to rearrange terms at the step labelled \(\clubsuit\). However, in the reward-maximizing setting, we simply bound \(V^{\pi^{k}}\leq V^{\star}\), without needing to rearrange terms.

**Theorem J.2**.: _Assume Assumption 5.1 and suppose we want to maximize returns (instead of minimize cost), so enable the SmallReturn flag. Fix any \(\delta\in(0,1)\) and set \(\beta=\log(H|\Pi||\mathcal{F}|/\delta)\). Then, w.p. at least \(1-\delta\), \(\textsc{P-DISCO}\) (Algorithm 4) learns a policy \(\widetilde{\pi}\) such that for any comparator policy \(\widetilde{\pi}\in\Pi\), we have_

\[V^{\widetilde{\pi}}-V^{\widetilde{\pi}}\leq 9H\sqrt{\frac{C^{\widetilde{\pi}}V ^{\widetilde{\pi}}\beta}{N}}+\frac{30H^{2}C^{\widetilde{\pi}}\beta}{N}.\]

Proof of Theorem J.2.: Adopt the same notation as in the proof of Theorem 6.1. By Theorem F.3, we have the following two facts, for all \(\pi\in\Pi\),

(i) Pessimism: \(\bar{f}_{1}^{\pi}(x_{1},\pi)\leq V^{\pi}\) (since \(Z^{\pi}\in\mathcal{F}_{\pi}\)) for all \(\pi\in\Pi\), and

(ii) \(\mathbb{E}_{\nu_{h}}[\delta_{h}^{\pi}(x_{h},a_{h})]\leq\beta^{\prime}N^{-1}\) for all \(h\) where \(\beta^{\prime}\leq 60\beta\).

With these two facts, we can bound the suboptimality of \(\widehat{\pi}\) as follows:

\[V^{\widetilde{\pi}}-V^{\widetilde{\pi}}\] \[\leq V^{\widetilde{\pi}}-\bar{f}_{1}^{\widetilde{\pi}}(x_{1}, \widehat{\pi})\] (Fact (i)) \[\leq V^{\widetilde{\pi}}-\bar{f}_{1}^{\widetilde{\pi}}(x_{1}, \widetilde{\pi})\] (Policy selection rule in Line 5 ) \[=\sum_{h=1}^{H}\mathbb{E}_{\widetilde{\pi}}\Big{[}\mathcal{T}_{h} ^{\widetilde{\pi}}\bar{f}_{h+1}^{\widetilde{\pi}}(x_{h},a_{h})-\bar{f}_{h}^{ \widetilde{\pi}}(x_{h},\widetilde{\pi})\Big{]}\] (PDL Lemma H.2 ) \[\leq\sum_{h=1}^{H}\sqrt{\mathbb{E}_{\widetilde{\pi}}\big{[}4\bar{ f}_{h}^{\widetilde{\pi}}(x_{h},a_{h})+\delta_{h}^{\widetilde{\pi}}(x_{h},a_{h}) \big{]}}\sqrt{\mathbb{E}_{\widetilde{\pi}}\big{[}\delta_{h}^{\widetilde{\pi}} (x_{h},a_{h})\big{]}}.\] (Eq. ( \[\triangle_{2}\] ))

From here, the same argument in the proof of Theorem 6.1 finishes the proof. 

## Appendix K Experiment Details

### Experiment Settings

In our experiments, as outlined in Foster and Krishnamurthy (2021), our \(\gamma\) learning rate at each time step \(t\) is set to \(\gamma_{t}=\gamma_{0}t^{p}\) where \(\gamma_{0}\) and \(p\) are hyperparameters. We use batch sizes of \(32\) samples per episode, and the King County and Prudential experiments run for \(5,000\) episodes while the CIFAR-100 experiment runs for \(15,000\).

For each dataset, we select the hyperparameter configuration with the best performance for each algorithm. As we report two metrics, performance over the last \(100\) episodes and over all episodes, we choose the best hyperparameters for each metric as well. While it is often the same hyperparameters that give the best last \(100\) episodes and all episodes results for a model, that is not always the case. We use the WandB (Weights and Biases) library to run sweeps over hyperparameters.

### Oracles

For our regression oracles, we use ResNet18 He et al. (2016), with a modified output layer (so that the output is suited for \(100\) prediction classes) for CIFAR-100, and a simple \(2\) hidden-layer neural network for the Prudential Life Insurance and King County Housing datasets. For DistCB, the oracle's output layer has size \(AC\) where \(A\) is the number of actions and \(C\) is the number of potential costs. This is reshaped so that for each action, there are predictions associated with each potential cost, which then have a softmax function applied to them to represent cost probabilities. For SquareCB and FastCB, the output size is \(A\) because there is just a single prediction associated with each action. As per Foster and Krishnamurthy (2021), a sigmoid function is applied to this output layer. All experiments were implemented using PyTorch.

### Datasets

We now provide an overview table as well as additional details and context to our setups for each dataset. Note that the number of items in each dataset in the table is the count after preprocessing.

**Prudential Life Insurance** This dataset is from the Prudential Life Insurance Kaggle competition (Montoya et al., 2015). It is featured in Farsang et al. (2022), which inspires our experimental setup. The risk level in \([8]\) directly determines the price charged to the customer. Thus, we can consider the chosen risk level as the action taken. If the model overpredicts the risk level, we get a cost of \(1.0\) because this is considered over charging the customer and not getting a sale. Otherwise, the model's prediction is charging too little for the customer. To reiterate, the cost in this case is \(.1*(y-\hat{y})\) where \(y\) is the actual risk level, and \(\hat{y}\) is the predicted risk level.

**King County Housing** The King County housing dataset is also used in Farsang et al. (2022). An interesting part of the setup is that the cost construction in the case of not overpredicting differs from the Prudential experiment, even though they're both effectively about predicting a price point. Here, the model's chosen price is considered the gain, which is why the cost is \(1.0\) minus the chosen price. On the other hand, in the Prudential experiment, the cost is a linear function of the difference between the chosen value and the actual value.

**CIFAR-100** For the CIFAR-100 experiment, we use the training dataset of \(50,000\) images as our dataset. The inclusion of the superclass is critical, as it lets us delineate \(3\) possible costs that DistCB can learn. Without the super class, the cost construction would be a pure binary of correct vs. incorrect. If this were the case, the ability to test the effectiveness of learning the distribution would be nullified. The distribution would just be whether an action is correct or not, which means our algorithm would essentially be predicting the mean directly.

**Results**

The largest advantages DistCB had over the next best algorithm were in the Prudential experiment, with DistCB having a \(.086\) advantage over the last \(100\) episodes and a \(.045\) advantage over all episodes. While the gaps were not as large for the other two datasets, they are still statistically significant and further showcase the benefit of distribution learning.

\begin{table}
\begin{tabular}{|l||l|l|l|} \hline \multicolumn{4}{|c|}{**Datasets**} \\ \hline Dataset & Items & Number of & Number of \\  & & Actions & Costs \\ \hline CIFAR-100 & \(50,000\) & \(100\) & \(3\) \\ Prudential Life Insurance & \(59,381\) & \(8\) & \(9\) \\ King County Housing & \(20,148\) & \(100\) & \(101\) \\ \hline \end{tabular}
\end{table}
Table 3: Overview of the three datasets and their experimental setups