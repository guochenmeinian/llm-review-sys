# Rebel: Reinforcement Learning

via Regressing Relative Rewards

 Zhaolin Gao\({}^{1}\), Jonathan D. Chang\({}^{2}\), Wenhao Zhan\({}^{3}\), Owen Oertell\({}^{1}\), Gokul Swamy\({}^{4}\),

**Kiante Brantley\({}^{5}\), Thorsten Joachims\({}^{1}\), J. Andrew Bagnell\({}^{4,6}\), Jason D. Lee\({}^{3}\), Wen Sun\({}^{1}\)**

\({}^{1}\) Cornell University, \({}^{2}\) Databricks Mosaic Research, \({}^{3}\) Princeton University,

\({}^{4}\) Carnegie Mellon University, \({}^{5}\) Harvard University, \({}^{6}\) Aurora Innovation

Work done at Cornell{zg292,ojoj2,tj36,ws455}@cornell.edu, j.chang@databricks.com, kdbrantley@g.harvard.edu, {wenhao.zhan,jasonlee}@princeton.edu, {gswamy,bagnell2}@andrew.cmu.edu

###### Abstract

While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a _minimalist_ RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the _relative reward_ between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MTBench, and Open LLM Leaderboard. Implementation of REBEL can be found at https://github.com/ZhaolinGao/REBEL, and models trained by REBEL can be found at https://huggingface.co/Cornell-AGI.

## 1 Introduction

The generality of the reinforcement learning (RL) paradigm is striking: from continuous control problems (Kalashnikov et al., 2018) to, more recently, the fine-tuning of generative models (Stiennon et al., 2022; Ouyang et al., 2022), RL has enabled concrete progress across a variety of decision-making tasks. Specifically, when it comes to fine-tuning generative models, Proximal Policy Optimization (PPO, Schulman et al. (2017)) has emerged as the de-facto RL algorithm of choice, from language models (LLMs) (Ziggler et al., 2020; Stiennon et al., 2022; Ouyang et al., 2022; Touvron et al., 2023) to generative image models (Black et al., 2023; Fan et al., 2024; Oertell et al., 2024).

If we take a step back however, it is odd that we are using an algorithm designed for optimizing two-layer networks for continuous control tasks from scratch, even though we are now fine-tuning generative models with billions of parameters. In the continuous control setting, the randomly initialized neural networks and the possible stochasticity in the dynamics necessitate variance reduction through a learned value function as a baseline (Schulman et al., 2015), while clipping updates is important to limit distribution shift from iteration to iteration (Kakade and Langford, 2002). This means that when applied to generative model fine-tuning, we need to store four models in memory simultaneously (the policy, the reference policy, the critic, and the reward model), each with billions of parameters. Furthermore, we often add a KL regularization to the base model for fine-tuning, making explicit clipping unnecessary nor advisable, as pointed out by Ahmadian et al. (2024). Even outside of the generative modeling context, PPO is notorious for the wide range of performances measured, with differences being attributed to seemingly inconsequential implementation details (Henderson et al., 2019; Engstrom et al., 2020). This begs the question: _are there simpler algorithms that better scale to modern RL applications?_

Our answer is REBEL: an algorithm that _reduces the problem of RL to solving a sequence of squared loss regression problems_ on iteratively collected datasets. Each regression problem directly uses the policy to predict the difference in rewards. This allows us to eliminate the complexity of using value functions, avoids heuristics like clipping, and scales easily to problems in both language modeling and image generation. Our key insight is that _a regressor that can predict the difference in rewards between trajectories in a dataset implicitly captures an improved policy._

Rather than being a mere heuristic, REBEL comes with strong guarantees in theory and can be seen as a strict generalization of classical techniques (e.g., NPG) in reinforcement learning. Furthermore, REBEL cleanly incorporates offline datasets when available, can be extended to robustly handle intransitive preferences (Swamy et al., 2024), empirically out-performs techniques like PPO and DPO (Rafailov et al., 2023) in language generation, and has a faster convergence with a similar asymptotic performance in image generation. When fine-tuning a Llama-3-8B model, REBEL also demonstrates very competitive performance on AlpacaEval 2.0, MT-bench, Open LLM Leaderboard, and Arena Hard simultaneously. We begin by formalizing the preference fine-tuning setup before deriving our core algorithmic technique.

## 2 REBEL: REgression to RElative REward Based RL

We consider the contextual bandit formulation (Langford and Zhang, 2007) of RL which has been used to formalize the generation process of models like LLMs (Rafailov et al., 2023; Ramamurthy et al., 2022; Chang et al., 2023) and Diffusion Models (Black et al., 2023; Fan et al., 2024; Oertell et al., 2024) due to the determinism of the transitions. More explicitly, in the deterministic transition

Figure 1: We present REBEL: a simple and scalable RL algorithm that performs policy optimization via _iteratively regressing the difference in rewards in terms of the policy_, allowing us to eliminate much of the complexity (e.g. value functions, clipping) of algorithms like PPO (Schulman et al., 2017). We apply REBEL to problems in both image generation and language modeling and find that despite its conceptual and implementation-level simplicity, REBEL is able to match or sometimes outperform the performance of PPO while out-performing purely offline techniques like DPO (Rafailov et al., 2023). REBEL also achieves strong performance on common benchmarks such as AlpacaEval when fine-tuning a Llama-3-8B model.

setting, explicit states are not required as they are isomorphic to the sequence of actions. Furthermore, the entire sequence of actions can be considered as a single "arm" in a bandit problem with an action space that scales exponentially in size with the horizon of the problem.

We denote by \((x,y)\) a (prompt, response) pair, where \(x\in\mathcal{X}\) is the prompt and \(y\in\mathcal{Y}\) is the response (e.g. a sequence of tokens, or in general a sequence of actions). We assume access to a reward function \(r(x,y)\) from which we can query for reward signals (the exact form of \(r\) does not need to be known). Querying \(r\) at \((x,y)\) will return a scalar \(r(x,y)\), measuring the quality of the prompt completion. Such a reward function could be a pre-defined metric (e.g., Rouge score against human responses) or a learned model from an offline human demonstration or preference data (e.g. the RLHF paradigm (Christiano et al., 2017; Ziegler et al., 2020)), as we focus on in our experiments.

Denote by \(\pi\in\mathcal{X}\mapsto\Delta(Y)\) a policy (e.g. an LLM) that maps from a prompt \(x\) to a distribution over the response space \(\mathcal{Y}\). We use \(\rho\) to denote the distribution over prompts (i.e. initial states / contexts) \(x\) and \(\pi_{\theta}(y|x)\) to denote a policy with parameter \(\theta\). At times, we interchangeably use \(\pi_{t}\) and \(\pi_{\theta_{t}}\) when it is clear from the context. We emphasize that while we focus on the bandit formulation for notational simplicity, the algorithms proposed here can be applied to _any_ deterministic MDP where \(x\) is the initial state and the trajectory \(y\) consists of the sequence of actions.

At each iteration of all algorithms, our goal will be to solve the following KL-constrained RL problem:

\[\pi_{t+1}=\operatorname*{argmax}_{\pi\in\Pi}\mathbb{E}_{x,y\sim\pi(\cdot|x)}r (x,y)-\frac{1}{\eta}\mathbb{E}_{x}\text{KL}\left(\pi(\cdot|x)||\pi_{t}(\cdot| x)\right).\] (2)

Intuitively, this can be thought of asking for the optimizer to fine-tune the policy \(\pi_{t+1}\) according to \(r\) while staying close in terms of action distribution to some baseline policy \(\pi_{t}\).

### Deriving Rebel: REgression to RElative REward Based RL

From Ziebart et al. (2008), we know that there exists a closed-form solution to the above _minimum relative entropy_ problem (Eq. 2, Grunwald and Dawid (2004)):

\[\forall x,y:\pi_{t+1}(y|x)=\frac{\pi_{t}(y|x)\exp(\eta r(x,y))}{Z(x)};\ Z(x)= \sum_{y}\pi_{t}(y|x)\exp(\eta r(x,y)).\] (3)

As observed by Rafailov et al. (2023), we can invert Eq. 3 and write the reward in terms of the policy:

\[\forall x,y:r(x,y)=\frac{1}{\eta}\left(\ln(Z(x))+\ln\left(\frac{\pi_{t+1}(y| x)}{\pi_{t}(y|x)}\right)\right).\] (4)

As soon as \(\mathcal{X}\) and \(\mathcal{Y}\) become large, we can no longer guarantee the above expression holds exactly at all \((x,y)\) and therefore need to turn our attention to choosing a policy such that Eq. 4 is approximately true. We propose using a simple _square loss_ objective between the two sides of Eq. 4 to measure the goodness of a policy, i.e. reducing RL to a regression problem: \(\left(r(x,y)-\frac{1}{\eta}\left(\ln(Z(x))+\ln\left(\frac{\pi_{t+1}(y|x)}{\pi_ {t}(y|x)}\right)\right)\right)^{2}\). Unfortunately, this loss function includes the _partition function_\(Z(x)\), which can be challenging to approximate over large input / output domains.

However, observe that \(Z(x)\) only depends on \(x\) and not \(y\). Thus, if we have access to _paired samples_, i.e. \((x,y)\) and \((x,y^{\prime})\), we can instead regress the _difference in rewards_ to eliminate this term:

\[\left((r(x,y)-r(x,y^{\prime}))-\frac{1}{\eta}\left(\ln\left(\frac{\pi_{t+1}(y|x )}{\pi_{t}(y|x)}\right)-\ln\left(\frac{\pi_{t+1}(y^{\prime}|x)}{\pi_{t}(y^{ \prime}|x)}\right)\right)\right)^{2}.\] (5)

Of course, we need to evaluate this loss function on some distribution of samples. In particular, we propose using an on-policy dataset \(\mathcal{D}_{t}=\{x,y,y^{\prime}\}\) with \(x\sim\rho,y\sim\pi_{t}(\cdot|x),y^{\prime}\sim\mu(\cdot|x)\), where \(\mu\) is some _base distribution_. The base distribution \(\mu\) can either be a fixed offline dataset (e.g. the instruction fine-tuning dataset) or \(\pi_{t}\) itself. Thus, the choice of base distribution \(\mu\) determines whether REBEL is hybrid or fully online. Putting it all together, we arrive at our core REBEL objective in Eq. 1. Critically, observe that if we were able to perfectly solve this regression problem, we would indeed recover the optimal solution to the KL-constrained RL problem we outlined in Eq. 2.

## 3 Understanding Rebel as an Adaptive Policy Gradient

In this section, we interpret REBEL as an adaptive policy gradient method to illuminate the relationship to past techniques. We start by introducing algorithms such as Mirror Descent, NPG, and PPO, followed by illustrating why REBEL addresses the limitations of these past algorithms. For concision, we postpone an in-depth discussion of related work to Appendix A.

### Adaptive Gradient Algorithms for Policy Optimization

**Mirror Descent.** If \(\mathcal{X}\) and \(\mathcal{Y}\) are small discrete spaces, we can use the closed-form expression for the minimum relative entropy problem (Eq. 3). This is equivalent to the classic Mirror Descent (MD) algorithm with KL as the Bregman divergence. Both NPG and PPO are approximations of MD.

**Natural Policy Gradient.** When \(\mathcal{Y}\) and \(\mathcal{X}\) are large, we use a parameterized policy denoted as \(\pi_{\theta}\) with parameter \(\theta\). Natural Policy Gradient (NPG, Kakade (2001)) approximates the KL in Equation 2 via its second-order Taylor expansion, whose Hessian is known as the Fisher Information Matrix (FIM, Bagnell and Schneider (2003)), \(F_{t}\), i.e. \(F_{t}=\mathbb{E}_{x,y\sim\pi_{\theta_{t}}(\cdot|x)}\left[\nabla\ln\pi_{\theta _{t}}(y|x)\nabla\ln\pi_{\theta_{t}}(y|x)^{\top}\right]\). Thus, \(\mathbb{E}_{x}\text{KL}(\pi_{\theta}(\cdot|x)||\pi_{\theta_{t}}(\cdot|x)) \approx(\theta-\theta_{t})^{\top}F_{t}(\theta-\theta_{t})\). The NPG update can be formulated as:

\[\theta_{t+1}=\theta_{t}+\eta F_{t}^{\dagger}\left(\mathbb{E}_{x,y\sim\pi_{ \theta_{t}}(\cdot|x)}\nabla\ln\pi_{\theta_{t}}(y|x)r(x,y)\right)\] (6)

where \(F_{t}^{\dagger}\) is pseudo-inverse of \(F_{t}\). As mentioned above, this update procedure can be understood as performing gradient updates in the local geometry induced by the Fisher information matrix, which ensures that we are taking small steps in _policy space_ rather than in _parameter space_. NPG, unfortunately, does not scale to modern settings due to need of inverting the FIM at each iteration.

**Proximal Policy Optimization.** Proximal Policy Optimization (PPO, Schulman et al. (2017)) takes a more direct route than NPG and uses clipped updates

\[\theta_{t+1}:=\operatorname*{argmax}_{\theta}\mathbb{E}_{x,y\sim\pi_{\theta_{ t}}(\cdot|x)}\text{clip}\left(\frac{\pi_{\theta}(y|x)}{\pi_{\theta_{t}}(y|x)};1- \epsilon,1+\epsilon\right)r(x,y).\] (7)

While the clipping operator can set the gradient to be zero at samples \((x,y)\) where \(\pi_{\theta_{t+1}}(y|x)\) is much larger or smaller than \(\pi_{\theta_{t}}(y|x)\), it cannot actually guarantee \(\pi_{\theta_{t+1}}\) staying close to \(\pi_{\theta_{t}}\), a phenomenon empirically observed in prior work (Hsu et al., 2020). Furthermore, hard clipping is not adaptive - it treats all \((x,y)\) equally and clips whenever the ratio is outside of a fixed range. In contrast, constraining the KL divergence to the prior policy allows one to vary the ratio \(\pi(y|x)/\pi_{t}(y|x)\) at different \((x,y)\), as long as the total KL divergence across the state space is small. Lastly, clipping reduces the effective size of a batch of training examples and thus wastes training samples.

### Connections between Rebel and Mb / Npg

**Exact**rebel is Mirror Descent.** First, to build intuition, we interpret our algorithm's behavior under the assumption that the least square regression optimization returns the exact Bayes Optimal solution (i.e., our learned predictor achieves zero prediction error everywhere):

\[\forall x,y,y^{\prime}:\quad\frac{1}{\eta}\left(\ln\frac{\pi_{\theta_{t+1}}(y |x)}{\pi_{\theta_{t}}(y|x)}-\ln\frac{\pi_{\theta_{t+1}}(y^{\prime}|x)}{\pi_{ \theta_{t}}(y^{\prime}|x)}\right)=r(x,y)-r(x,y^{\prime})\] (8)Conditioned on Eq. 8 being true, a few lines of algebraic manipulation reveal that there must exist a function \(c(x)\) which is independent of \(y\), such that \(\forall x,y:\frac{1}{\eta}\ln\frac{\pi_{\theta_{t+1}}(y|x)}{\pi_{\theta_{t}}(y|x )}=r(x,y)+c(x)\). Taking an \(\exp\) on both sides and re-arrange terms, we get \(\forall x,y:\pi_{\theta_{t}+1}(y|x)\propto\pi_{\theta_{t}}(y|x)\exp\left(\eta r (x,y)\right)\). In other words, under the strong assumption that least square regression returns a point-wise accurate estimator (i.e., Eq. 8), we see the REBEL recovers the exact MD update, which gives it _(a)_ a fast \(1/T\) convergence rate (Shani et al., 2020; Agarwal et al., 2021), _(b)_ conservativity, i.e., \(\max_{x}\text{KL}(\pi_{t+1}(\cdot|x)||\pi_{t}(\cdot|x))\) is bounded as long as \(\max_{x,y}|r(x,y)|\) is bounded, and _(c)_ monotonic policy improvement via the NPG standard analysis (Agarwal et al., 2021).

**NPG is Approximate** REBEL **with Gauss-Newton Updates.** We provide another interpretation of REBEL by showing that NPG (Eq. 6) can be understood as a special case of REBEL where the least square problem in Eq. 1 is approximately solved via a single iteration of the Gauss-Newton algorithm. We start by approximating our predictor \(\frac{1}{\eta}\ln\pi_{\theta}(y|x)/\pi_{\theta_{t}}(y|x)\) by its first order Taylor expansion at \(\theta_{t}\): \(\frac{1}{\eta}\left(\ln\pi_{\theta}(y|x)-\ln\pi_{\theta_{t}}(y|x)\right) \approx\frac{1}{\eta}\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)^{\top}(\theta- \theta_{t})\), where \(\approx\) indicates that we ignore higher order terms in the expansion. Define \(\delta:=\theta-\theta_{t}\) and replace \(\frac{1}{\eta}\left(\ln\pi_{\theta}(y|x)-\ln\pi_{\theta_{t}}(y|x)\right)\) by its first order approximation in Eq. 1. Then, we have :

\[\min_{\delta}\mathbb{E}_{x\sim\rho,y\sim\pi_{\theta_{t}}(\cdot|x),y^{\prime} \sim\mu(\cdot|x)}\left(\frac{1}{\eta}\left(\nabla_{\theta}\ln\pi_{\theta_{t}} (y|x)-\nabla_{\theta}\ln\pi_{\theta_{t}}(y^{\prime}|x)\right)^{\top}\delta-(r (x,y)-r(x,y^{\prime}))\right)^{2}\] (9)

Further simplifying notation, we denote the uniform mixture of \(\pi_{t}\) and \(\mu\) as \(\pi_{mix}(\cdot|x):=(\pi_{t}(\cdot|x)+\mu(\cdot|x))/2\) and the Fisher information matrix \(F_{t}\) averaged under said mixture as \(F_{t}=\mathbb{E}_{x\sim\rho,y\sim\pi_{mix}(\cdot|x)}\left[\nabla_{\theta}\ln \pi_{\theta_{t}}(y|x)\left(\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)\right)^{ \top}\right]\). Solving the above least squares problem to obtain a minimum norm solution, we have the following result.

**Claim 1**.: _The minimum norm minimizer \(\delta^{\star}\) of the least squares problem in Eq. 9 recovers an advantage-based NPG update: \(\delta^{\star}:=\eta F_{t}^{\dagger}\left(\mathbb{E}_{x\sim\rho,y\sim\pi_{mix }(\cdot|x)}\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)[A^{\pi_{t}}(x,y)]\right)\) where \(F_{t}^{\dagger}\) is pseudo-inverse of \(F_{t}\), and the advantage is defined as \(A^{\pi_{t}}(x,y):=r(x,y)-\mathbb{E}_{y^{\prime}\sim\pi_{t}(\cdot|x)}r(x,y)\)._

The proof of this claim is deferred to Appendix B.

**The implicit variance reduction effect of** REBEL **We show that regressing to relative rewards has a variance reduction effect by extending the previous derivation on REBEL with Gauss-Newton update to the setting of finite data \(\mathcal{D}=\{x_{n},y_{n},y_{n}^{\prime}\}_{n=1}^{N}\). Denote the unbiased estimate of the Fisher information matrix as \(\hat{F}_{t}=\frac{1}{N}\sum_{n=1}^{N}\left[\nabla_{\theta}\ln\pi_{\theta_{t}} (y_{n}|x_{n})\left(\nabla_{\theta}\ln\pi_{\theta_{t}}(y_{n}|x_{n})\right)^{ \top}\right]\) and have the following claim.**

**Claim 2**.: _The minimum norm minimizer \(\delta^{\star}\) in Eq. 9 under finite setting has the form \(\delta^{\star}:=\eta\hat{F}_{t}^{\dagger}\frac{1}{2N}\sum_{n}\left(\nabla\ln \pi_{\theta_{t}}(y_{n}|x_{n})(r(x_{n},y_{n})-r(x_{n},y_{n}^{\prime}))+\nabla\ln \pi_{\theta_{t}}(y_{n}^{\prime}|x_{n})(r(x_{n},y_{n}^{\prime})-r(x_{n},y_{n}))\right)\) where \(\hat{F}_{t}^{\dagger}\) is pseudo-inverse of \(\hat{F}_{t}\)._

The proof of this claim is deferred to Appendix C. Looking at the gradient formulation \(\nabla\ln\pi_{\theta_{t}}(y_{n}|x_{n})\left(r(x_{n},y_{n})-r(x_{n},y_{n}^{ \prime})\right)\) in \(\delta^{\star}\), we see that \(r(x_{n},y_{n}^{\prime})\) serves as a baseline for variance reduction. Interestingly, this gradient formulation is similar to RLOO (REINFORCE with leave-one-out) (Kool et al., 2019). However, different from RLOO, we pre-condition this variance reduced policy gradient formulation via the Fisher information matrix, leading to better performance.

**A** REBEL **With a Cause.** Our algorithm REBEL addresses the limitations of NPG (scalability) and PPO (lack of conservativity or adaptivity) from above. First, unlike NPG, it does not rely on the Fisher Information Matrix at all and can easily scale to modern LLM and image generation applications, yet can be interpreted as a _generalization_ of NPG. Second, in contrast to PPO, it doesn't have unjustified heuristics and thus enjoys strong convergence and regret guarantees just like NPG. Building on Swamy et al. (2024), we also show how to extend REBEL to preference-based settings without assuming transitivity in Appendix D.

## 4 Theoretical Analysis

In the previous section, we interpret REBEL as exact MD and show its convergence by assuming that least square regression always returns a predictor that is accurate _everywhere_. While such an explanation is simple and has also been used in prior work (Calandriello et al., 2024; Rosset et al., 2024), point-wise out-of-distribution generalization is an extremely strong condition and is significantly beyond what a standard supervised learning method can promise. In this section, we substantially relax this condition via a reduction-based analysis: _As long as we can solve the regression problems well in an in-distribution manner, REBEL can compete against any policy covered by the training data distributions_. Formally, we assume the following generalization condition holds on the regressors we find.

**Assumption 1** (Regression generalization bounds).: _Over \(T\) iterations, assume that for all \(t\), we have the following for some \(\epsilon\):_

\[\mathbb{E}_{x\sim\rho,y\sim\pi_{t}(\cdot|x),y^{\prime}\sim\mu(\cdot|x)}\left( \frac{1}{\eta}\left(\ln\frac{\pi_{\theta_{t+1}}(y|x)}{\pi_{\theta_{t}}(y|x)}- \ln\frac{\pi_{\theta_{t+1}}(y^{\prime}|x)}{\pi_{\theta_{t}}(y^{\prime}|x)} \right)-(r(x,y)-r(x,y^{\prime}))\right)^{2}\leq\epsilon,\]

Detailed justifications for this assumption are provided in Appendix E.

**Data Coverage**. Recall that the base distribution \(\mu\) can be some behavior policy, which in RLHF can be a human labeler, a supervised fine-tuned policy (SFT), or just the current learned policy (i.e., on-policy). Given a test policy \(\pi\), we denote by \(C_{\mu\rightarrow\pi}\) the concentrability coefficient, i.e.

\[C_{\mu\rightarrow\pi}=\max_{x,y}\frac{\pi(y|x)}{\mu(y|x)}.\] (10)

We say \(\mu\)_covers_\(\pi\) if \(C_{\mu\rightarrow\pi}<+\infty\). Our goal is to bound the regret between our learned policies and an arbitrary comparator \(\pi^{*}\) (e.g. the optimal policy if it is covered by \(\mu\)) using \(\epsilon\) and the concentrability coefficient defined in Eq. 10. The following theorem formally states the regret bound of our algorithm.

**Theorem 1**.: _Under Assumption 1, after \(T\) many iterations, with a proper learning rate \(\eta\), among the learned policies \(\pi_{1},\ldots,\pi_{T}\), there must exist a policy \(\hat{\pi}\), such that:_

\[\forall\pi^{*}:\ \mathbb{E}_{x\sim\rho,y\sim\pi^{*}(\cdot|x)}r(x,y)-\mathbb{E} _{x\sim\rho,y\sim\hat{\pi}(\cdot|x)}r(x,y)\leq O\left(\sqrt{\frac{1}{T}}+\sqrt {C_{\mu\rightarrow\pi^{*}}\epsilon}.\right).\]

The above theorem shows a _reduction from RL to supervised learning_ -- as long as supervised learning works (i.e., \(\epsilon\) is small), then REBEL can compete against any policy \(\pi^{*}\) that is covered by the base data distribution \(\mu\). In the regret bound, the \(1/\sqrt{T}\) comes from Mirror Descent style update, and \(C_{\mu\rightarrow\pi^{*}}\epsilon\) captures the cost of distribution shift: we train our regressors under distribution \(\pi_{t}\) and \(\mu\), but we want the learned regressor to predict well under \(\pi^{*}\). Similar to the NPG analysis from Agarwal et al. (2021), we now have a slower convergence rate \(1/\sqrt{T}\), due to the fact that we have approximation error from learning. Such an agnostic regret bound -- being able to compete against any policy that is covered by training distributions -- is the **strongest type of agnostic learning results known in the RL literature**, matching the best of what has appeared in prior policy optimization work including PSDP (Bagnell et al., 2003), CPI (Kakade and Langford, 2002), NPG (Agarwal et al., 2021), and PC-PG (Agarwal et al., 2020). While in this work we use the simplest and most intuitive definition of coverage - the density ratio-based definition in Eq. 10 - extension to more general ones such as transfer error (Agarwal et al., 2020, 2021) or concentrability coefficients that incorporate the function class (e.g., Song et al. (2023)) is straightforward. We defer the proof of the above theorem and the detailed constants that we omitted in the \(O\) notation to Appendix F. We include an extension of the above analysis to the general preference setting in Appendix G.

**Remark 1** (Discussion on the size of the response space \(|\mathcal{Y}|\) and other design choices of the sampling distributions).: _In REBEL, when sampling a pair \((y,y^{\prime})\), we in default sample \(y\sim\pi_{t}\), i.e., we make sure at least one of them is an on-policy sample. This is to make sure that the training distribution at iteration \(t\) covers \(\pi_{t}\), which plays an essential role in avoiding a polynomial dependency on the size of the action space \(|\mathcal{Y}|\).2 On the other hand, as long as we have some off-policy distribution \(\nu_{t}\) that covers \(\pi_{t}\) for all \(t\), we can use it to sample \(y\) and pay an additional concentrability coefficient \(\max_{x,y,t}\pi_{t}(y|x)/\nu_{t}(y|x)\) in the final bound. In experiments, we test the combination of the best-of-N of \(\pi_{t}\) as the base distribution \(\mu\) and the worst-of-N of \(\pi_{t}\) as the \(\nu_{t}\). Setting \(\mu\) to be the best-of-N of \(\pi_{t}\) makes \(\mu\) cover higher quality comparator policies. Selecting \(\nu_{t}\) as the worst-of-N of \(\pi_{t}\) still ensures coverage to \(\pi_{t}\) while at the same time increasing the reward gap \(r(x,y)-r(x,y^{\prime})\), which we find is helpful experimentally.

## 5 Experiments

Our implementation of REBEL closely follows the psuedocode in Algorithm 1. In each iteration, REBEL collects a dataset \(\mathcal{D}_{t}=\{x,y,y^{\prime}\}\), where \(x\sim\rho,y\sim\pi_{t}(\cdot|x),y^{\prime}\sim\mu(\cdot|x)\). Subsequently, REBEL optimizes the least squares regression problem in Eq. 1 through gradient descent with AdamW(Loshchilov and Hutter, 2017). We choose \(\mu=\pi_{t}\) such that both \(y\) and \(y^{\prime}\) are generated by the current policy. We empirically assess REBEL's performance on both natural language generation and text-guided image generation. Additional experiment details are in Appendix H.

### Summarization

**Task.** We use the _TL;DR_ dataset (Stiennon et al., 2020) where \(x\) is a forum post from Reddit and \(y\) is a summary generated by the policy. The dataset comprises human reference summaries and preference data. We compare REBEL with baseline RL algorithms, REINFORCE (Williams, 1992) and its multi-sample extension, REINFORCE Leave-One-Out (RLOO) (Kool et al., 2019), PPO (Schulman et al., 2017), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Iterative DPO (Guo et al., 2024). Our implementation of Iterative DPO replaces our square regression objective with the DPO objective where the binary preference labels are obtained based on the reward difference. The implementation detail of the baseline methods is provided in Appendix H.1.3. Following prior work (Stiennon et al., 2020; Rafailov et al., 2023; Ahmadian et al., 2024), we train DPO on the preference dataset, while conducting online RL (RLOO, PPO, Iterative DPO, REBEL) on the human reference dataset. We include results with three different model sizes: 1.4B, 2.8B, and 6.9B based on the pre-trained models from Pythia (Biderman et al., 2023). Each model is trained from a supervised fine-tuned (SFT) model using a reward model (RM) of the same size.

**Evaluation.** We evaluate each method by its balance between reward model score and KL-divergence with the SFT policy, testing the effectiveness of the algorithm in optimizing the regularized RL objective. To evaluate the quality of the generation, we compute the winrate (Rafailov et al., 2023) against human references using GPT4 (OpenAI, 2023). The winrate is computed from a randomly sampled subset (\(10\%\)) of the test set with 600 samples. We report the average results over three seeds.

**Quality Analysis.** Table 1 presents a comparison between REBEL and baseline methods. Notably, REBEL outperforms all the baselines on RM score with 1.4B and 2.8B parameters with a slightly larger KL than PPO. In addition, REBEL achieves the highest winrate under GPT4 when evaluated against human references, indicating the benefit of regressing the relative rewards. An ablation analysis on parameter \(\eta\) is in Appendix J and the trade-off between the reward model score and KL-divergence is discussed in Appendix K.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model size & Algorithm & Winrate (\(\uparrow\)) & RM Score (\(\uparrow\)) & KL(\(\pi||\tau_{ref}\)) (\(\downarrow\)) \\ \hline \multirow{4}{*}{\(1.4\text{B}\)} & SFT & 24.9 (\(\pm\)2.73) & -0.51 (\(\pm\)0.05) & - \\  & DPO & 42.7 (\(\pm\)1.79) & 0.10 (\(\pm\)0.02) & 29.6 (\(\pm\)0.63) \\  & Iterative DPO & 47.2 (\(\pm\)1.34) & 1.73 (\(\pm\)0.05) & 29.7 (\(\pm\)0.57) \\  & PPO & 51.7 (\(\pm\)1.42) & 1.74 (\(\pm\)0.04) & 29.3 (\(\pm\)0.61) \\  & REBEL & **55.1** (\(\pm\)1.35) & **1.84** (\(\pm\)0.04) & 32.6 (\(\pm\)0.59) \\ \hline \multirow{4}{*}{\(2.8\text{B}\)} & SFT & 28.2 (\(\pm\)2.31) & -0.38 (\(\pm\)0.06) & - \\  & DPO & 53.7 (\(\pm\)1.63) & 2.40 (\(\pm\)0.02) & 64.3 (\(\pm\)1.25) \\ \cline{1-1}  & Iterative DPO & 63.1 (\(\pm\)1.41) & 2.37 (\(\pm\)0.03) & 28.1 (\(\pm\)0.51) \\ \cline{1-1}  & PPO & 67.4 (\(\pm\)1.30) & 2.37 (\(\pm\)0.03) & 27.2 (\(\pm\)0.55) \\ \cline{1-1}  & REBEL & **70.2** (\(\pm\)1.32) & **2.44** (\(\pm\)0.02) & 29.0 (\(\pm\)0.60) \\ \hline \hline \end{tabular} \begin{tabular}{c c c} \hline \hline Model size & Algorithm & Winrate (\(\uparrow\)) \\ \hline \multirow{4}{*}{\(6.9\text{B}\)} & SFT & 45.2 (\(\pm\)2.49) \\  & DPO & 68.4 (\(\pm\)2.01) \\  & REBEL & 70.7* \\  & PPO & 77.6* \\  & RLOO (\(k=2\)) & 74.2* \\  & RLOO (\(k=4\)) & 77.9* \\  & REBEL & **78.1** (\(\pm\)1.74) \\ \hline \multirow{4}{*}{\(4\) directly obtained from Ahmadian et al. (2024)} \\  & REBEL & **70.1** (\(\pm\)1.74) \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline Model size & Algorithm & Winrate (\(\uparrow\)) \\ \hline \multirow{4}{*}{\(6.9\text{B}\)} & SFT & 45.2 (\(\pm\)2.49) \\  & DPO & 68.4 (\(\pm\)2.01) \\  & REBEL & 70.7* \\  & PPO & 77.6* \\  & RLOO (\(k=2\)) & 74.2* \\  & RLOO (\(k=4\)) & 77.9* \\  & REBEL & **78.1** (\(\pm\)1.74) \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline Model size & Algorithm & Winrate (\(\uparrow\)) \\ \hline \multirow{4}{*}{\(6.9\text{B}\)} & SFT & 45.2 (\(\pm\)2.49) \\  & DPO & 68.4 (\(\pm\)2.01) \\  & REBEL & 70.7* \\  & PPO & 77.6* \\  & RLOO (\(k=2\)) & 74.2* \\  & RLOO (\(k=4\)) & 77.9* \\  & REBEL & **78.1** (\(\pm\)1.74) \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline Model size & Algorithm & Winrate (\(\uparrow\)) \\ \hline \multirow{4}{*}{\(6.9\text{B}\)} & SFT & 45.2 (\(\pm\)2.49) \\  & DPO & 68.4 (\(\pm\)2.01) \\  & REBEL & 70.7* \\  & PPO & 77.6* \\  & RLOO (\(k=2\)) & 74.2* \\  & RLOO (\(k=4\)) & 77.9* \\  & REBEL & **78.1** (\(\pm\)1.74) \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline Model size & Algorithm & Winrate (\(\uparrow\)) \\ \hline \multirow{4}{*}{\(6.9\text{B}\)} & SFT & 45.2 (\(\pm\)2.49) \\  & DPO & 68.4 (\(\pm\)2.01) \\  & REBEL & 70.7* \\  & PPO & 77.6* \\  & RLOO (\(k=2\)) & 74.2* \\  & RLOO (\(k=4\)) & 77.9* \\  & REBEL & **78.1** (\(\pm\)1.74) \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline Model size & Algorithm & Winrate (\(\uparrow\)) \\ \hline \multirow{4}{*}{\(6.9\text{B}\)} & SFT & 45.2 (\(\pm\)2.49) \\  & DPO & 68.4 (\(\pm\)2.01) \\  & REBEL & 70.7* \\  & PPO & 77.6* \\  & RLOO (\(k=2\)) & 74.2* \\  & RLOO (\(k=4\)) & 77.9* \\  & REBEL & **78.1** (\(\pm\)1.74) \\ \hline \hline \end{tabular} 
\begin{tabular}{c c} \hline \hline \multirow{4}{*}{\(6.9\text{B}\)} & SFT & 45.2 (\(\pm\)2.49) \\  & DPO & 68.4 (\(\pm\)2.01) \\  & REBEL & 70.7* \\  & PPO & 77.6* \\  & RLOO (\(k=2\)) & 74.2* \\Runtime & Memory Analysis.We analyze the runtime and peak memory usage for 2.8B models with REINFORCE, RLOO, PPO, DPO, Iterative DPO, and REBEL. The runtime includes both the generation time and the time required for policy updates. Both runtime and peak memory usage are measured on A6000 GPUs using the same hyperparameters detailed in Appendix H.1.5 for a batch of \(512\) prompts. The measurements are averaged over \(100\) batches. Methods are ascendingly ordered by winrate. To the right of the dashed line, PPO, RLOO (\(k=4\)), and REBEL have the highest winrates, which are comparable among them.

While DPO and REINFORCE are more time and memory-efficient, their performance does not match up to REBEL, as shown in Table 1. RLOO (\(k=2\)) and Iterative DPO have similar runtime and memory usage as REBEL since we set \(\mu=\pi_{t}\), making REBEL also generate twice per prompt. However, both methods have worse performance than REBEL. Compared to PPO and RLOO (\(k=4\)), REBEL demonstrates shorter runtimes and lower peak memory usage. PPO is slow and requires more memory since it needs to update two networks (the policy network and the value network). RLOO (\(k=4\)) requires four generations per prompt which makes it slow and less memory efficient. In summary, _compared to the two baselines (PPO and RLOO (\(k=4\))) that achieve similar winrates as REBEL, REBEL is more computationally tractable and simpler to implement._

### General Chat

**Task.** We consider a general chat scenario where \(x\) is a prompt from the user and \(y\) is a response. We adapt the setting from Zhu et al. (2023), using OpenChat-3.5 Wang et al. (2024) as the base model, Starling-RM-7B-alpha Zhu et al. (2023) as the reward model, and the Nectar dataset Zhu et al. (2023). This setup enables a direct comparison between REBEL and APA Zhu et al. (2023) which is used to train Starling-LM-7B-alpha.

**Evaluation.** Following previous works, we use AlpacaEval 2.0 Dubois et al. (2024), MT-Bench Zheng et al. (2023), and Open LLM Leaderboard Beeching et al. (2023) as metrics. AlpacaEval 2.0 uses prompts from AlpacaFarm Dubois et al. (2024) to compare model responses against a reference response generated by GPT-4-Turbo. We report the winrate over the reference responses. MT-Bench consists of 80 open-ended questions on various topics. Answers are scored directly by GPT-4. Open LLM Leaderboard consists of MMLU Hendrycks et al. (2021), GSM8K Cobbe et al. (2021), Arc Clark et al. (2018), Winogrande Sakaguchi et al. (2019), TruthfulQA Lin et al. (2022), and HellaSwag Zellers et al. (2019). The prompts of the tasks consist of zero or few-shot samples.

**Quality Analysis.** The results between models trained with REBEL and baseline methods are shown in Table 2. For MT-Bench and AlpacaEval 2.0, under the same setup, REBEL outperforms APA Zhu et al. (2023) on both metrics, demonstrating the effectiveness of REBEL under chat setting and _its superior performance over APA_. For the metrics on Open LLM Leaderboard, REBEL is able to enhance the performance of GSM8K and HellaSwag and maintain the overall average as the base models. Similar values on MMLU as base models indicate that we preserve the basic capability of the pre-trained model during the RL fine-tuning process. We include a breakdown of MT-Bench in Appendix M.

#### 5.2.1 Ablation: batch size and data sampling distributions

**Task.** In the previous section, we sample \(y\) and \(y^{\prime}\) from \(\pi_{t}(\cdot|x)\) and we use small batch size with \(|\mathcal{D}_{t}|=32\). In this section, we investigate the alternative sampling distribution described in Remark 1. Specifically, at each iteration, we generate \(5\) responses from \(\pi_{t}\) for each prompt in the _entire dataset_ (i.e., \(|\mathcal{D}_{t}|\) is the size of the entire dataset), rank them based on the reward model, and set \(y\) to be the best of the five responses, and \(y^{\prime}\) to be the worst of the five responses. We perform \(3\) iterations for this setup with Meta-Llama-3-8B-Instruct Meta (2024) as the base model, ArmoRM-Llama3

Figure 2: **Plot of runtime and memory usage.** Base-lines on the left-hand side of the dashed line have lower winrates. Methods on the right-hand side of the dashed line have similar winrates to REBEL.

8B-v0.1 (Wang et al., 2023) as the reward model, and the UltraFeedback dataset (Cui et al., 2023). We compare REBEL with DPO which is also trained for one epoch on the entire dataset with best-of-5 as \(y_{w}\) and worst-of-5 as \(y_{l}\) sampled from \(\pi_{0}\). In other words, the training data used for the first iteration of REBEL is the same as the one we use for DPO3. We follow the same evaluation methods as the previous section and include Arena Hard (AH) (Li et al., 2024) in our analysis.

Footnote 3: Directly training DPO on the original Ultrafeedback preference dataset does not provide strong performance under AlpacaEval (e.g., the LC-winrate is around 28%, see Song et al. (2024)). So for a fair comparison to REBEL, we train DPO on the online data generated by \(\pi_{0}\) and labeled by the reward model.

**Quality Analysis.** Results in Table 3 show that REBEL can significantly improve the base model's performance, especially on AlpacaEval 2.0 and Arena Hard. Compared to DPO, the model trained by REBEL with 1 iteration is better in almost all datasets, demonstrating the benefit of using the fine-grained reward gap in policy optimization over just the zero-one labels. In this large batch setting, we find that more iterations in general do not help performance. We conjecture that this is the issue of overfitting to the training dataset. A more diverse and larger dataset can potentially address this issue.

### Image Generation

**Task.** We also consider the setting of image generation, where, given a consistency model (Song et al., 2023) and a target reward function, we seek to train the consistency model to output images that garner a higher reward. We use 45 common animals as generation prompts similar to Black et al. (2023); Oertell et al. (2024) and the latent consistency model (Luo et al., 2023) distillation of the Dreamshaper v7 model, a finetune of stable diffusion (Rombach et al., 2021). We compare REBEL to a clipped, policy gradient objective (Black et al., 2023; Fan et al., 2024; Oertell et al., 2024) with the aim to optimize aesthetic acuity to obtain a high reward from the LAION aesthetic score predictor (Schuhmann, 2022). This baseline does not use critics or GAE for advantage estimates. However, the clipping objective is clearly motivated by PPO, and thus, we simply name this baseline as PPO.

**Evaluation.** We evaluate on the reward under the LAION aesthetic reward model for an equal number of reward queries/samples generated and an equal number of gradient updates. The aesthetic predictor is trained to predict human-labeled scores of images on a scale of 1 to 10. Images that tend to have the highest reward are artwork. Following Agarwal et al. (2021), we report inter-quartile means (IQM) with 95% confidence intervals (CIs) across three seeds for both REBEL and PPO. The CIs were calculated with percentile bootstrap with stratified sampling over three random seeds.

\begin{table}
\begin{tabular}{c|c|c c|c c c c c|c} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{MT-Bench} & \multicolumn{3}{c|}{AlpacaEval 2.0} & \multicolumn{1}{c}{MMLU} & GSM8K & Arc & Winogrande & TruthfulQA & HellaSwag \\  & & LC Win Rate & Win Rate & (5-shot) & (5-shot) & (25-shot) & (5-shot) & (0-shot) & (10-shot) \\ \hline Base & 7.69 & 12.2 & 11.7 & 63.6 & 68.5 & **64.9** & 80.6 & 47.3 & 84.7 \\ APA & 7.43 & 14.7 & **14.2** & 63.4 & 68.0 & **64.9** & **81.1** & 47.3 & 84.8 \\ REBEL & **8.06** & **17.3** & 12.8 & **63.7** & **68.8** & 64.3 & 80.4 & **48.2** & **85.0** \\ \hline \end{tabular}
\end{table}
Table 2: **Results on General Chat.** The best-performing method for each metric is highlighted in bold. Note that the APA result is directly obtained by evaluating the Starling-LM-7B-alpha model.

\begin{table}
\begin{tabular}{c|c|c c|c c c c c|c} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{MT-Bench} & \multicolumn{3}{c|}{AlpacaEval 2.0} & \multicolumn{1}{c}{MMLU} & GSM8K & Arc & Winogrande & TruthfulQA & HellaSwag \\  & & LC Win Rate & Win Rate & (5-shot) & (5-shot) & (25-shot) & (5-shot) & (0-shot) & (10-shot) \\ \hline Base & 8.10 & 22.9 & 22.6 & 65.8 & 75.3 & **62.0** & 75.5 & 51.7 & 78.7 & 22.3 \\ DPO & 8.11 & 44.9 & 41.6 & 66.1 & 74.6 & 61.3 & 75.5 & **51.8** & **78.9** & 34.0 \\ \hline REBEL (iter 1) & **8.13** & 48.3 & 41.8 & **66.3** & **75.8** & 61.7 & **75.9** & **51.8** & 78.7 & **34.5** \\ REBEL (iter 2) & 8.07 & **50.0** & **48.5** & 65.9 & 75.4 & 61.3 & 75.5 & 50.3 & 78.6 & 30.4 \\ REBEL (iter 3) & 8.01 & 49.7 & 48.1 & 66.0 & 75.7 & 61.1 & 75.7 & 49.8 & 78.8 & 30.0 \\ \hline \end{tabular}
\end{table}
Table 3: **Ablation Results.** In this table, REBEL uses a _larger batch size (the entire dataset) with the best-of-N and worst-of-N (\(N=5\)) of \(\pi_{t}\) as the sampling distributions for generating pairs \(y,y^{\prime}\)_. The best-performing method for each metric is highlighted in bold. Note that DPO is trained on the _online data_ generated by the base model and labeled by the RM.

Figure 3: **Learning curves** as a function of reward queries to the LAION aesthetic predictor. The colored areas represent 95% CIs.

**Quality Analysis.** Figure 3 shows REBEL optimizes the consistency model faster during the beginning of training and eventually achieves a performance similar to that of PPO. For our experiments, we tuned both batch size and learning rate for our algorithms, testing batch sizes of \([4,8,16]\) per GPU and learning rates \([1\text{e}-4,3\text{e}-4,6\text{e}-4,1\text{e}-3]\). The main difference in implementation between PPO and REBEL is the replacement of the clipped PPO objective with our regression objective. To maximize LAION-predicted aesthetic quality, both REBEL and PPO transform a model that produces plain images into one that produces artistic drawings. We found across multiple seeds that REBEL produced lush backgrounds when compared to PPO's generations. Please see Appendix I.3 for more examples of generated images.

## 6 Conclusion and Future Work

We propose REBEL, an RL algorithm that reduces the problem of RL to solving a sequence of relative reward regression problems on iteratively collected datasets. In contrast to policy gradient approaches that require additional networks and heuristics like clipping to ensure optimization stability, it suffices for REBEL to merely drive down error on a least squares problem, making it strikingly simple to implement and scale. In theory, REBEL matches the best guarantees we have for RL algorithms in the agnostic setting, while in practice, REBEL is able to match and sometimes outperform methods that are far more complex to implement or expensive to run across both language modeling and guided image generation tasks.

There are several open questions raised by our work. The first is whether using a loss function other than square loss (e.g. log loss or cross-entropy) could lead to better performance in practice (Farebrother et al., 2024) or tighter bounds (e.g. first-order / gap-dependent) in theory (Foster and Krishnamurthy, 2021; Wang et al., 2023, 2024). The second is whether, in the general (i.e. non-utility-based) preference setting, the coverage condition assumed in our analysis is necessary - we conjecture it is. Relatedly, it would be interesting to explore whether using _preference_ (rather than reward) models to provide supervision for REBEL replicates the performance improvements reported by Swamy et al. (2024); Munos et al. (2023); Calandriello et al. (2024). Third, while we focus primarily on the bandit setting in the preceding sections, it would be interesting to consider the more general RL setting and explore how offline datasets can be used to improve the efficiency of policy optimization via techniques like resets (Bagnell et al., 2003; Ross and Bagnell, 2014; Swamy et al., 2023; Chang et al., 2023, 2024; Ren et al., 2024; Dice et al., 2024).

Figure 4: **Generated images** using PPO and REBEL during an intermediate checkpoint. At the same number of epochs, REBEL observes a higher reward under the reward model. This can further be seen by the more diverse background of images generated from REBEL with less training time.

[MISSING_PAGE_FAIL:11]

Rammurthy, R.; Ammanabrolu, P.; Brantley, K.; Hessel, J.; Sifa, R.; Bauckhage, C.; Hajishirzi, H.; Choi, Y. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. _arXiv preprint arXiv:2210.01241_**2022**,
* Chang et al. (2023) Chang, J. D.; Brantley, K.; Ramamurthy, R.; Misra, D.; Sun, W. Learning to Generate Better Than Your LLM. 2023.
* Christiano et al. (2017) Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.; Amodei, D. Deep Reinforcement Learning from Human Preferences. Advances in Neural Information Processing Systems. 2017.
* Ziebart et al. (2008) Ziebart, B. D.; Maas, A. L.; Bagnell, J. A.; Dey, A. K.; others Maximum entropy inverse reinforcement learning. Aaai. 2008; pp 1433-1438.
* Grunwald and Dawid (2004) Grunwald, P. D.; Dawid, A. P. Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory. 2004.
* Kakade (2001) Kakade, S. M. A Natural Policy Gradient. Advances in Neural Information Processing Systems. 2001.
* Bagnell and Schneider (2003) Bagnell, J. A.; Schneider, J. Covariant policy search. Proceedings of the 18th international joint conference on Artificial intelligence. 2003; pp 1019-1024.
* Hsu and Mendler-Dunner (2020) Hsu, C. C.-Y.; Mendler-Dunner, C.; Hardt, M. Revisiting Design Choices in Proximal Policy Optimization. 2020.
* Shani et al. (2020) Shani, L.; Efroni, Y.; Mannor, S. Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. Proceedings of the AAAI Conference on Artificial Intelligence. 2020; pp 5668-5675.
* Agarwal et al. (2021) Agarwal, A.; Kakade, S. M.; Lee, J. D.; Mahajan, G. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _Journal of Machine Learning Research_**2021**, _22_, 1-76.
* Kool et al. (2019) Kool, W.; van Hoof, H.; Welling, M. Buy 4 REINFORCE Samples, Get a Baseline for Free! DeepRLStructPred@ICLR. 2019.
* Calandriello et al. (2024) Calandriello, D.; Guo, D.; Munos, R.; Rowland, M.; Tang, Y.; Pires, B. A.; Richemond, P. H.; Lan, C. L.; Valko, M.; Liu, T.; others Human Alignment of Large Language Models through Online Preference Optimisation. _arXiv preprint arXiv:2403.08635_**2024**,
* Rosset et al. (2024) Rosset, C.; Cheng, C.-A.; Mitra, A.; Santacroce, M.; Awadallah, A.; Xie, T. Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences. _arXiv preprint arXiv:2404.03715_**2024**,
* Bagnell et al. (2003) Bagnell, J.; Kakade, S. M.; Schneider, J.; Ng, A. Policy search by dynamic programming. _Advances in neural information processing systems_**2003**, _16_.
* Agarwal et al. (2020) Agarwal, A.; Henaff, M.; Kakade, S.; Sun, W. Pc-pg: Policy cover directed exploration for provable policy gradient learning. _Advances in neural information processing systems_**2020**, _33_, 13399-13412.
* Song et al. (2023) Song, Y.; Zhou, Y.; Sekhari, A.; Bagnell, J. A.; Krishnamurthy, A.; Sun, W. Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient. 2023.
* Agarwal et al. (2019) Agarwal, A.; Jiang, N.; Kakade, S. M.; Sun, W. Reinforcement learning: Theory and algorithms. 2019.
* Loshchilov and Hutter (2017) Loshchilov, I.; Hutter, F. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_**2017**,
* Huang et al. (2024) Huang, S.; Noukhovitch, M.; Hosseini, A.; Rasul, K.; Wang, W.; Tunstall, L. The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization. 2024.
* Liu et al. (2019)Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.; Voss, C.; Radford, A.; Amodei, D.; Christiano, P. F. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_**2020**, _33_, 3008-3021.
* Williams (1992) Williams, R. J. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. _Mach. Learn._**1992**, \(8\), 229-256.
* Guo et al. (2024) Guo, S.; Zhang, B.; Liu, T.; Liu, T.; Khalman, M.; Llinares, F.; Rame, A.; Mesnard, T.; Zhao, Y.; Piot, B.; others Direct language model alignment from online ai feedback. _arXiv preprint arXiv:2402.04792_**2024**,
* Biderman et al. (2023) Biderman, S.; Schoelkopf, H.; Anthony, Q. G.; Bradley, H.; O'Brien, K.; Hallahan, E.; Khan, M. A.; Purohit, S.; Prashanth, U. S.; Raff, E.; others Pythia: A suite for analyzing large language models across training and scaling. International Conference on Machine Learning. 2023; pp 2397-2430.

OpenAI Gpt-4 technical report. 2023.
* Zhu et al. (2023) Zhu, B.; Frick, E.; Wu, T.; Zhu, H.; Jiao, J. Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF. 2023.
* Wang et al. (2024) Wang, G.; Cheng, S.; Zhan, X.; Li, X.; Song, S.; Liu, Y. OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. 2024.
* Dubois et al. (2024) Dubois, Y.; Galambosi, B.; Liang, P.; Hashimoto, T. B. Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. 2024.
* Zheng et al. (2023) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; Stoica, I. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. 2023.
* Beeching et al. (2023) Beeching, E.; Fourrier, C.; Habib, N.; Han, S.; Lambert, N.; Rajani, N.; Sanseviero, O.; Tunstall, L.; Wolf, T. Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.
* Dubois et al. (2021) Dubois, Y.; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.; Guestrin, C.; Liang, P.; Hashimoto, T. B. AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. 2024.
* Hendrycks et al. (2021) Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; Steinhardt, J. Measuring Massive Multitask Language Understanding. 2021.
* Cobbe et al. (2021) Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; Hesse, C.; Schulman, J. Training Verifiers to Solve Math Word Problems. 2021.
* Clark et al. (2018) Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; Tafjord, O. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. 2018.
* Sakaguchi et al. (2019) Sakaguchi, K.; Bras, R. L.; Bhagavatula, C.; Choi, Y. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale. 2019.
* Lin et al. (2022) Lin, S.; Hilton, J.; Evans, O. TruthfulQA: Measuring How Models Mimic Human Falsehoods. 2022.
* Zellers et al. (2019) Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; Choi, Y. HellaSwag: Can a Machine Really Finish Your Sentence? 2019.
* Zhu et al. (2023) Zhu, B.; Sharma, H.; Frujeri, F. V.; Dong, S.; Zhu, C.; Jordan, M. I.; Jiao, J. Fine-Tuning Language Models with Advantage-Induced Policy Alignment. 2023.

Meta Introducing Meta Llama 3: The most capable openly available LLM to date. 2024; https://ai.meta.com/blog/meta-llama-3/.
* Wang et al. (2020) Wang, H.; Xiong, W.; Xie, T.; Zhao, H.; Zhang, T. Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts. _arXiv preprint arXiv:2406.12845_Cui, G.; Yuan, L.; Ding, N.; Yao, G.; Zhu, W.; Ni, Y.; Xie, G.; Liu, Z.; Sun, M. UltraFeedback: Boosting Language Models with High-quality Feedback. 2023.
* Song et al. (2024) Song, Y.; Swamy, G.; Singh, A.; Bagnell, J. A.; Sun, W. Understanding Preference Fine-Tuning Through the Lens of Coverage. _arXiv preprint arXiv:2406.01462_**2024**,
* Li et al. (2024) Li, T.; Chiang, W.-L.; Frick, E.; Dunlap, L.; Wu, T.; Zhu, B.; Gonzalez, J. E.; Stoica, I. From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline. 2024; https://arxiv.org/abs/2406.11939.
* Song et al. (2023) Song, Y.; Dhariwal, P.; Chen, M.; Sutskever, I. Consistency models. _arXiv preprint arXiv:2303.01469_**2023**,
* Luo et al. (2023) Luo, S.; Tan, Y.; Huang, L.; Li, J.; Zhao, H. Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. 2023.
* Rombach et al. (2021) Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; Ommer, B. High-Resolution Image Synthesis with Latent Diffusion Models. 2021.
* Schuhmann (2022) Schuhmann, C. Laion aesthetics. https://laion.ai/blog/laion-aesthetics/, 2022.
* Agarwal et al. (2021) Agarwal, R.; Schwarzer, M.; Castro, P. S.; Courville, A. C.; Bellemare, M. Deep reinforcement learning at the edge of the statistical precipice. _Advances in neural information processing systems_**2021**, _34_, 29304-29320.
* Farebrother et al. (2024) Farebrother, J.; Orbay, J.; Vuong, Q.; Taiga, A. A.; Chebotar, Y.; Xiao, T.; Irpan, A.; Levine, S.; Castro, P. S.; Faust, A.; Kumar, A.; Agarwal, R. Stop Regressing: Training Value Functions via Classification for Scalable Deep RL. 2024.
* Foster and Krishnamurthy (2021) Foster, D. J.; Krishnamurthy, A. Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination. 2021.
* Wang et al. (2023) Wang, K.; Zhou, K.; Wu, R.; Kallus, N.; Sun, W. The benefits of being distributional: Small-loss bounds for reinforcement learning. _Advances in Neural Information Processing Systems_**2023**, _36_.
* Wang et al. (2024) Wang, K.; Oertell, O.; Agarwal, A.; Kallus, N.; Sun, W. More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning. _arXiv preprint arXiv:2402.07198_**2024**,
* Munos et al. (2023) Munos, R.; Valko, M.; Calandriello, D.; Azar, M. G.; Rowland, M.; Guo, Z. D.; Tang, Y.; Geist, M.; Mesnard, T.; Michi, A.; others Nash learning from human feedback. _arXiv preprint arXiv:2312.00886_**2023**,
* Ross and Bagnell (2014) Ross, S.; Bagnell, J. A. Reinforcement and Imitation Learning via Interactive No-Regret Learning. _ArXiv_**2014**, _abs/1406.5979_.
* Swamy et al. (2023) Swamy, G.; Choudhury, S.; Bagnell, J. A.; Wu, Z. S. Inverse Reinforcement Learning without Reinforcement Learning. _ArXiv_**2023**, _abs/2303.14623_.
* Chang et al. (2024) Chang, J. D.; Shan, W.; Oertell, O.; Brantley, K.; Misra, D.; Lee, J. D.; Sun, W. Dataset Reset Policy Optimization for RLHF. _arXiv preprint arXiv:2404.08495_**2024**,
* Ren et al. (2024) Ren, J.; Swamy, G.; Wu, Z. S.; Bagnell, J. A.; Choudhury, S. Hybrid Inverse Reinforcement Learning. _arXiv preprint arXiv:2402.08848_**2024**,
* Dice et al. (2024) Dice, N. E.; Swamy, G.; Choudhury, S.; Sun, W. Efficient Inverse Reinforcement Learning without Compounding Errors. ICML 2024 Workshop on Models of Human Feedback for AI Alignment. 2024.
* Nemirovskij and Yudin (1983) Nemirovskij, A. S.; Yudin, D. B. Problem complexity and method efficiency in optimization. 1983.
* Konda and Tsitsiklis (1999) Konda, V.; Tsitsiklis, J. Actor-Critic Algorithms. Advances in Neural Information Processing Systems. 1999.
* Richter et al. (2020) Richter, L.; Boustati, A.; Nusken, N.; Ruiz, F.; Akyildiz, O. D. VarGrad: a low-variance gradient estimator for variational inference. _Advances in Neural Information Processing Systems_**2020**, _33_, 13481-13492.
* Raghavan et al. (2019)Zhu, B.; Jordan, M.; Jiao, J. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. International Conference on Machine Learning. 2023; pp 43037-43067.
* Schulman et al. (2015) Schulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; Moritz, P. Trust region policy optimization. International conference on machine learning. 2015; pp 1889-1897.
* Peters and Schaal (2007) Peters, J.; Schaal, S. Reinforcement learning by reward-weighted regression for operational space control. Proceedings of the 24th international conference on Machine learning. 2007; pp 745-750.
* Peng et al. (2019) Peng, X. B.; Kumar, A.; Zhang, G.; Levine, S. Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning. 2019.
* Zhou et al. (2023) Zhou, Z.; Liu, J.; Yang, C.; Shao, J.; Liu, Y.; Yue, X.; Ouyang, W.; Qiao, Y. Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization. 2023.
* Jacq et al. (2019) Jacq, A.; Geist, M.; Paiva, A.; Pietquin, O. Learning from a Learner. Proceedings of the 36th International Conference on Machine Learning. 2019; pp 2990-2999.
* Watson and Huang (2023) Watson, J.; Huang, S. H.; Heess, N. Coherent Soft Imitation Learning. 2023.
* Anthropic Introducing the next generation of Claude. 2024; https://www.anthropic.com/news/claude-3-family.
* Nakano et al. (2022) Nakano, R. et al. WebGPT: Browser-assisted question-answering with human feedback. 2022.
* Lee et al. (2023) Lee, K.; Liu, H.; Ryu, M.; Watkins, O.; Du, Y.; Boutilier, C.; Abbeel, P.; Ghavamzadeh, M.; Gu, S. S. Aligning Text-to-Image Models using Human Feedback. 2023.
* Azar et al. (2023) Azar, M. G.; Rowland, M.; Piot, B.; Guo, D.; Calandriello, D.; Valko, M.; Munos, R. A General Theoretical Paradigm to Understand Learning from Human Preferences. 2023.
* Ethayarajh et al. (2023) Ethayarajh, K.; Xu, W.; Kiela, D. Better, Cheaper, Faster LLM Alignment with KTO. 2023; https://contextual.ai/better-cheaper-faster-llm-alignment-with-kto/.
* Lambert et al. (2024) Lambert, N.; Pyatkin, V.; Morrison, J.; Miranda, L.; Lin, B. Y.; Chandu, K.; Dziri, N.; Kumar, S.; Zick, T.; Choi, Y.; Smith, N. A.; Hajishirzi, H. RewardBench: Evaluating Reward Models for Language Modeling. 2024.
* Tajwar et al. (2024) Tajwar, F.; Singh, A.; Sharma, A.; Rafailov, R.; Schneider, J.; Xie, T.; Ermon, S.; Finn, C.; Kumar, A. Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data. 2024.
* Ross et al. (2011) Ross, S.; Gordon, G.; Bagnell, D. A reduction of imitation learning and structured prediction to no-regret online learning. Proceedings of the fourteenth international conference on artificial intelligence and statistics. 2011; pp 627-635.
* Swamy et al. (2021) Swamy, G.; Choudhury, S.; Bagnell, J. A.; Wu, S. Of moments and matching: A game-theoretic framework for closing the imitation gap. International Conference on Machine Learning. 2021; pp 10022-10032.
* Xiong et al. (2024) Xiong, W.; Dong, H.; Ye, C.; Wang, Z.; Zhong, H.; Ji, H.; Jiang, N.; Zhang, T. Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. 2024.
* Mao et al. (2024) Mao, X.; Li, F.-L.; Xu, H.; Zhang, W.; Luu, A. T. Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration. _arXiv preprint arXiv:2402.16030_**2024**,
* Munos (2003) Munos, R. Error bounds for approximate policy iteration. ICML. 2003; pp 560-567.
* Wu et al. (2024) Wu, Y.; Sun, Z.; Yuan, H.; Ji, K.; Yang, Y.; Gu, Q. Self-Play Preference Optimization for Language Model Alignment. _arXiv preprint arXiv:2405.00675_**2024**,
* Ball et al. (2023) Ball, P. J.; Smith, L.; Kostrikov, I.; Levine, S. Efficient Online Reinforcement Learning with Offline Data. 2023.
* Zhou et al. (2023) Zhou, Y.; Sekhari, A.; Song, Y.; Sun, W. Offline data enhanced on-policy policy gradient with provable guarantees. _arXiv preprint arXiv:2311.08384_**2023**,May, K. O. Intransitivity, utility, and the aggregation of preference patterns. _Econometrica: Journal of the Econometric Society_**1954**, 1-13.
* Tversky (1969) Tversky, A. Intransitivity of preferences. _Psychological review_**1969**, _76_, 31.
* Gardner (1970) Gardner, M. Mathematical games. 1970; https://www.scientificamerican.com/article/mathematical-games-1970-12/.
* Dudik et al. (2015) Dudik, M.; Hofmann, K.; Schapire, R. E.; Slivkins, A.; Zoghi, M. Contextual dueling bandits. Conference on Learning Theory. 2015; pp 563-587.
* Ye et al. (2024) Ye, C.; Xiong, W.; Zhang, Y.; Jiang, N.; Zhang, T. A theoretical analysis of nash learning from human feedback under general kl-regularized preference. _arXiv preprint arXiv:2402.07314_**2024**,
* Kreweras (1962) Kreweras, G. Aggregation of preference orderings. Mathematics and Social Sciences I: Proceedings of the seminars of Menthon-Saint-Bernard, France (1-27 July 1960) and of Gosing, Austria (3-27 July 1962). 1965; pp 73-79.
* Fishburn (1984) Fishburn, P. C. Probabilistic social choice based on simple voting comparisons. _The Review of Economic Studies_**1984**, _51_, 683-692.
* Kramer (1973) Kramer, G. H. On a Class of Equilibrium Conditions for Majority Rule. _Econometrica_**1973**, _41_, 285-97.
* Simpson (1969) Simpson, P. B. On Defining Areas of Voter Choice: Professor Tullock on Stable Voting. _The Quarterly Journal of Economics_**1969**, _83_, 478-490.
* Yue et al. (2012) Yue, Y.; Broder, J.; Kleinberg, R.; Joachims, T. The k-armed dueling bandits problem. _Journal of Computer and System Sciences_**2012**, _78_, 1538-1556.
* Wang et al. (2023) Wang, Y.; Liu, Q.; Jin, C. Is RLHF More Difficult than Standard RL? A Theoretical Perspective. Thirty-seventh Conference on Neural Information Processing Systems. 2023.
* Cui and Du (2022) Cui, Q.; Du, S. S. When are Offline Two-Player Zero-Sum Markov Games Solvable? _Advances in Neural Information Processing Systems_**2022**, _35_, 25779-25791.
* Zhong et al. (2022) Zhong, H.; Xiong, W.; Tan, J.; Wang, L.; Zhang, T.; Wang, Z.; Yang, Z. Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets. International Conference on Machine Learning. 2022; pp 27117-27142.
* Cui and Du (2022) Cui, Q.; Du, S. S. Provably Efficient Offline Multi-agent Reinforcement Learning via Strategy-wise Bonus. Advances in Neural Information Processing Systems. 2022; pp 11739-11751.
* Xiong et al. (2023) Xiong, W.; Zhong, H.; Shi, C.; Shen, C.; Wang, L.; Zhang, T. Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game. The Eleventh International Conference on Learning Representations. 2023.
* Hu et al. (2022) Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W. LoRA: Low-Rank Adaptation of Large Language Models. International Conference on Learning Representations. 2022.
* Huang et al. (2024) Huang, S.; Noukhovitch, M.; Hosseini, A.; Rasul, K.; Wang, W.; Tunstall, L. The N+ Implementation Details of RLHF with PPO: A Case Study on TL; DR Summarization. _arXiv preprint arXiv:2403.17031_**2024**,Detailed Discussion of Related Work

**Policy Gradients.** Policy gradient (PG) methods (Nemirovskij and Yudin, 1983; Williams, 1992; Konda and Tsitsiklis, 1999; Kakade, 2001; Schulman et al., 2017) are a prominent class of RL algorithms due to their direct, gradient-based policy optimization, robustness to model misspecification (Agarwal et al., 2020), and scalability to modern AI applications from fine-tuning LLMs (Siennon et al., 2022) to optimizing text-to-image generators (Oertell et al., 2024).

Broadly speaking, we can taxonomize PG methods into two families. The first family is based on REINFORCE (Williams, 1992) and often includes variance reduction techniques (Kool et al., 2019; Richter et al., 2020; Zhu et al., 2023). While prior work by Ahmadian et al. (2024) has shown that REINFORCE-based approaches can outperform more complex RL algorithms like PPO on LLM fine-tuning tasks like _TL;DR_, we find that a properly optimized version of PPO still out-performs a REINFORCE baseline. The second family is _adaptive_ PG techniques that _precondition_ the policy gradient (usually with the inverse of the Fisher Information Matrix) to ensure it is _covariant_ to re-parameterizations of the policy, which include NPG (Kakade, 2001; Bagnell and Schneider, 2003) and its practical approximations like TRPO (Schulman et al., 2015) and PPO (Schulman et al., 2017). Intuitively, the preconditioning ensures that we make small changes in terms of action distributions, rather than in terms of the actual policy parameters, leading to faster and more stable convergence. Unfortunately, computing and then inverting the Fisher Information Matrix is computationally intensive and therefore we often resort to approximations in practice, as done in TRPO. However, these approximations are still difficult to apply to large-scale generative models, necessitating even coarser approximations like PPO. In contrast, REBEL does not need any such approximations to be implemented at scale, giving us a much closer connection between theory and practice.

**Reward Regression.** The heart of REBEL is a novel reduction from RL to iterative squared loss regression. While using regression to fit either the reward (Peters and Schaal, 2007) or the value (Peng et al., 2019) targets which are then used to extract a policy have previously been explored, our method instead takes a page from DPO (Rafailov et al., 2023; Zhou et al., 2023) and inverse RL methods (Jacq et al., 2019; Watson et al., 2023) to implicitly parameterize the reward regressor in terms of the policy. This collapses the two-stage procedure of prior methods into a single step.

**Preference Fine-Tuning (PFT) of Generative Models.** RL has attracted renewed interest due to its central role in "aligning" language models - i.e., adapting their distribution of prompt completions towards the set of responses preferred by human raters.

One family of techniques for PFT, often referred to as Reinforcement Learning from Human Feedback (RLHF) involves first fitting a reward model (i.e. a classifier) to the human preference data and then using this model to provide reward values to a downstream RL algorithm (often PPO) (Christiano et al., 2017; Ziegler et al., 2020). LLMs fine-tuned by this procedure include GPT-N (OpenAI, 2023), Claude-N (Anthropic, 2024), and Llama-N (Meta, 2024). Similar approaches have proved beneficial for tasks like summarization (Siennon et al., 2022), question answering (Nakano et al., 2022), text-to-image generation (Lee et al., 2023), and instruction following (Ouyang et al., 2022).

Another family of techniques for PFT essentially treats the problem as supervised learning and uses a variety of ranking loss functions. It includes DPO (Rafailov et al., 2023), IPO (Azar et al., 2023), and KTO (Ethayarajh et al., 2023). These techniques are simpler to implement as they remove components like an explicit reward model, value network, and on-policy training from the standard RLHF setup. However, recent work finds their performance to be lesser than that of on-policy methods (Lambert et al., 2024; Tajwar et al., 2024), which agrees with our findings. This is perhaps caused by their lack of interaction during training, leading to the well-known covariate shift/compounding error issue (Ross et al., 2011; Swamy et al., 2021) and the associated lower levels of performance.

The third family of PFT techniques combines elements from the previous two: it involves running an offline algorithm _iteratively_, collecting on-policy preference feedback from either a supervisor model such as GPT4 (Rosset et al., 2024; Xiong et al., 2024; Guo et al., 2024) or from a preference model fit on human data (Calandriello et al., 2024). All of these approaches can be considered instantiations of the general SPO reduction proposed by Swamy et al. (2024), which itself can be thought of as a preference-based variant of DAgger (Ross et al., 2011). Recent work by Tajwar et al. (2024) confirms the empirical strength of these techniques which leverage additional online data. Our approach fits best into this family of techniques - we also iteratively update our model by solving a sequence of supervised learning problems over on-policy datasets. However, REBEL comes with several key differentiating factors from the prior work. Online versions of DPO or IPO (Xiong et al., 2024; Tajwar et al., 2024; Guo et al., 2024; Calandriello et al., 2024; Munos et al., 2023) essentially use a reward / preference model to generate binary win-loss labels while REBEL actually uses the output of the reward model as a regression target, taking advantage of this more nuanced feedback. In contrast to Rosset et al. (2024), algorithmically, REBEL does not use any online preference feedback from GPT4 nor does it require to generate a large number of responses per prompt, both of which are extremely expensive as reported by Rosset et al. (2024). Theoretically, we are able to prove policy performance bounds under a much weaker coverage condition. Unlike Mao et al. (2024) that regularize to the initial policy \(\pi_{0}\) during updates, we perform _conservative_ updates by regularizing \(\pi_{t+1}\) to \(\pi_{t}\). When doing the former, it is difficult to prove convergence or monotonic improvement as the current policy can just bounce around a ball centered at \(\pi_{0}\), a well-known issue in the theory of approximate policy iteration (Kakade and Langford, 2002; Munos, 2003). In contrast, by incorporating the prior policy's probabilities into our regression problem, we are able to prove stronger guarantees for REBEL. When applying REBEL to the general preference setting, our algorithm shares some similarities with the concurrent work of Wu et al. (2024), which also leverages the DPO reparameterization trick to cleanly implement an Online Mirror Descent approach to computing a minimax winner via self-play. The key difference is that REBEL uses paired responses, while the algorithm of Wu et al. (2024) does not. Using paired responses, REBEL is able to cancel out the partition function and therefore does not need to resort to heuristic approximations of it (specifically, assuming the partition function is always equal to a constant). Furthermore, we can run REBEL with datasets consisting of a mixture of on-policy and off-policy data with strong guarantees, enabling _hybrid training_, as previously explored in the RL (Song et al., 2023; Ball et al., 2023; Zhou et al., 2023) and inverse RL (Ren et al., 2024) literature.

Proof of Claim 1

We prove claim 1 in this section. We start from deriving the Fisher information matrix.

\[F_{t} :=\frac{1}{\eta^{2}}\mathbb{E}_{x,y\sim\pi_{t},y^{\prime}\sim\mu} \left(\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)-\nabla_{\theta}\ln\pi_{\theta_{t} }(y^{\prime}|x)\right)\left(\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)-\nabla_{ \theta}\ln\pi_{\theta_{t}}(y^{\prime}|x)\right)^{\top}\] \[=\frac{2}{\eta^{2}}\mathbb{E}_{x,y\sim\pi_{mix}}\nabla_{\theta} \ln\pi_{\theta_{t}}(y|x)\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)^{\top}\]

where the last equality uses the fact that cross terms from completing the square are zero. Now recall Eq. 9 which is an ordinary least square regression problem. The minimum norm solution of the least square regression problem is:

\[\delta =(\eta/2)\tilde{F}_{t}^{\dagger}\left(\mathbb{E}_{x,y\sim\pi_{t}, y^{\prime}\sim\mu}\left(\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)-\nabla_{ \theta}\ln\pi_{\theta_{t}}(y^{\prime}|x)\right)(r(x,y)-r(x,y^{\prime}))\right)\] \[=(\eta/2)\tilde{F}_{t}^{\dagger}\left(\mathbb{E}_{x,y\sim\pi_{t} }\left[\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)r(x,y)\right]+\mathbb{E}_{x,y^{ \prime}\sim\mu}\left[\nabla_{\theta}\ln\pi_{\theta_{t}}(y^{\prime}|x)r(x,y^{ \prime})\right]\right.\] \[\qquad-\mathbb{E}_{x,y\sim\pi_{t},y^{\prime}\sim\mu}\nabla_{ \theta}\ln\pi_{\theta_{t}}(y^{\prime}|x)r(x,y)\Big{)}\] \[=(\eta/2)\tilde{F}_{t}^{\dagger}\left(\mathbb{E}_{x,y\sim\pi_{t} }\left[\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)[r(x,y)-\mathbb{E}_{y^{\prime} \sim\pi_{t}(\cdot|x)}r(x,y^{\prime})\right]\right.\] \[\qquad\qquad+\left.\mathbb{E}_{x,y\sim\mu}\left[\nabla_{\theta} \ln\pi_{\theta_{t}}(y|x)[r(x,y)-\mathbb{E}_{y^{\prime}\sim\pi_{t}(\cdot|x)}r (x,y^{\prime})\right]\right)\] \[=(\eta)\tilde{F}_{t}^{\dagger}\left(\mathbb{E}_{x,y\sim(\pi_{t}+ \mu)/2}\left[\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)[A^{\pi_{t}}(x,y)]\right)\]

where we again use the fact that \(\mathbb{E}_{y\sim\pi_{\theta_{t}}(\cdot|x)}\nabla_{\theta}\ln\pi_{\theta_{t}} (y|x)g(x)=0\) for any function \(g(x)\), and we define _Advantage_\(A^{\pi}(x,y):=r(x,y)-\mathbb{E}_{y^{\prime}\sim\pi(\cdot|x)}r(x,y^{\prime})\).

Proof of Claim 2

We prove claim 2 in this section. We start by approximating our predictor \(\frac{1}{\eta}\ln\pi_{\theta}(y|x)/\pi_{\theta_{t}}(y|x)\) by its first order Taylor expansion at \(\theta_{t}\colon\frac{1}{\eta}\left(\ln\pi_{\theta}(y|x)-\ln\pi_{\theta_{t}}(y| x)\right)\approx\frac{1}{\eta}\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)^{\top}( \theta-\theta_{t})\), where \(\approx\) indicates that we ignore higher order terms in the expansion. Setting \(\delta:=\theta-\theta_{t}\) and replace \(\frac{1}{\eta}\left(\ln\pi_{\theta}(y|x)-\ln\pi_{\theta_{t}}(y|x)\right)\) by its first order approximation in Eq. 1, we arrive at:

\[\min_{\delta}\sum_{(x,y,y^{\prime})\in\mathcal{D}_{t}}\left(\frac{1}{\eta} \left(\nabla_{\theta}\ln\pi_{\theta_{t}}(y|x)-\nabla_{\theta}\ln\pi_{\theta_{ t}}(y^{\prime}|x)\right)^{\top}\delta-(r(x,y)-r(x,y^{\prime}))\right)^{2}\] (11)

under finite setting.

Following the previous derivation, we have the unbiased estimate of Fisher information matrix under the finite setting as:

\[\hat{F}_{t}:=\frac{2}{\eta^{2}N}\sum_{x_{n},y_{n}\sim\pi_{mix}}\nabla_{\theta }\ln\pi_{\theta_{t}}(y_{n}|x_{n})\nabla_{\theta}\ln\pi_{\theta_{t}}(y_{n}|x_{ n})^{\top}\]

Since Eq. 11 is an ordinary least square regression problem. The minimum norm solution of the least square regression problem is:

\[\delta =(\eta/2)\tilde{\hat{F}}_{t}^{\dagger}\frac{1}{N}\sum_{n}\left( \nabla_{\theta}\ln\pi_{\theta_{t}}(y_{n}|x_{n})-\nabla_{\theta}\ln\pi_{\theta_ {t}}(y^{\prime}_{n}|x_{n})\right)(r(x_{n},y_{n})-r(x_{n},y^{\prime}_{n}))\] \[=\eta\tilde{\hat{F}}_{t}^{\dagger}\frac{1}{2N}\sum_{n}\left( \nabla\ln\pi_{\theta_{t}}(y_{n}|x_{n})(r(x_{n},y_{n})-r(x_{n},y^{\prime}_{n}) )+\nabla\ln\pi_{\theta_{t}}(y^{\prime}_{n}|x_{n})(r(x_{n},y^{\prime}_{n})-r(x _{n},y_{n}))\right)\]Extending REBEL to General Preferences

In the above discussion, we assume we are given access to a ground-truth reward function. However, in the generative model fine-tuning applications of RL, we often need to learn from human _preferences_, rather than rewards. This shift introduces a complication: not all preferences can be rationalized by an underlying utility function. In particular, _intransitive_ preferences which are well-known to result from aggregation of different sub-populations or users evaluating different pairs of items on the basis of different features (May, 1954; Tversky, 1969; Gardner, 1970) cannot be accurately captured by a single reward model. To see this, note that if we have \(a\succ b\), \(b\succ c\), and \(c\succ a\), it is impossible to have a reward model that simultaneously sets \(\hat{r}(a)>\hat{r}(b)\), \(\hat{r}(b)>\hat{r}(c)\), and \(\hat{r}(c)>\hat{r}(a)\). As we increase the space of possible choices to that of all possible prompt completions, the probability of such intransitivities sharply increases (Dudik et al., 2015), as reflected in the high levels of annotator disagreement in LLM fine-tuning datasets (Touvron et al., 2023). Thus, rather than assuming access to a reward model, in such settings, we assume access to a _preference model_(Munos et al., 2023; Swamy et al., 2024; Rosset et al., 2024; Ye et al., 2024).

### A Game-Theoretic Perspective on Learning from Preferences

More specifically, for any tuple \((x,y,y^{\prime})\), we assume we have access to \(\mathcal{P}(y\succ y^{\prime}|x)\): the probability that \(y\) is preferred to \(y^{\prime}\). We then define our preference model \(l\) as

\[l(x,y,y^{\prime})\triangleq 2\cdot\mathcal{P}(y\succ y^{\prime}|x)-1.\] (12)

Observe that \(l(x,y,y^{\prime})\in[-1,1]\) is skew-symmetric, i.e., \(l(x,y,y)=0\), \(l(x,y,y^{\prime})+l(x,y^{\prime},y)=0\) for all \(x\in\mathcal{X},y,y^{\prime}\in\mathcal{Y}\). If the learner can only receive a binary feedback \(o\in\{0,1\}\) indicating the preference between \(y\) and \(y^{\prime}\), we assume \(o\) is sampled from a Bernoulli distribution with mean \(\mathcal{P}(y\succ y^{\prime}|x)\), where \(o=1\) means that \(y\) is preferred over \(y^{\prime}\) and \(0\) otherwise.

Given access to such a preference model, a solution concept to the preference aggregation problem with deep roots in the social choice theory literature (Kreweras, 1965; Fishburn, 1984; Kramer, 1973; Simpson, 1969) and the dueling bandit literature (Yue et al., 2012; Dudik et al., 2015) is that of a minimax winner (MW) \(\pi_{\mathsf{MW}}\): the Nash Equilibrium strategy of the symmetric two-player zero-sum game with \(l\) as a payoff function. In particular, due to the skew-symmetric property of \(l\), Swamy et al. (2024) proved that there exists a policy \(\pi_{\mathsf{MW}}\) such that

\[\max_{\pi}\mathbb{E}_{x\sim\rho,y\sim\pi(\cdot|x),y^{\prime}\sim\pi_{\mathsf{ MW}}(\cdot|x)}\left[l(x,y,y^{\prime})\right]=\min_{\pi}\mathbb{E}_{x\sim\rho,y \sim\pi_{\mathsf{MW}}(\cdot|x),y^{\prime}\sim\pi(\cdot|x)}\left[l(x,y,y^{ \prime})\right].\]

This implies that \((\pi_{\mathsf{MW}},\pi_{\mathsf{MW}})\) is a Nash Equilibrium (Wang et al., 2023; Munos et al., 2023; Swamy et al., 2024; Ye et al., 2024). As is standard in game solving, our objective is to obtain an \(\epsilon\)-approximate MW \(\widehat{\pi}\) measured by the duality gap (DG):

\[\mathsf{DG}(\widehat{\pi}):=\max_{\pi}\mathbb{E}_{x\sim\rho,y\sim\pi(\cdot|x), y^{\prime}\sim\widehat{\pi}(\cdot|x)}\left[l(x,y,y^{\prime})\right]-\min_{\pi} \mathbb{E}_{x\sim\rho,y\sim\widehat{\pi}(\cdot|x),y^{\prime}\sim\pi(\cdot|x)} \left[l(x,y,y^{\prime})\right]\leq\epsilon.\]

In the following discussion, we will use \(l(x,y,\pi)\) to denote \(\mathbb{E}_{y^{\prime}\sim\pi(\cdot|x)}[l(x,y,y^{\prime})]\) and \(l(\pi,\pi^{\prime})\) to denote \(\mathbb{E}_{x\sim\rho,y\sim\pi(\cdot|x),y^{\prime}\sim\pi^{\prime}(\cdot|x)}[l (x,y,y^{\prime})]\) for notational convenience.

### Self-Play Preference Optimization (SPO) with REBEL as Base Learner

We can straightforwardly extend REBEL to the general preference setting via an instantiation of the Self-Play Preference Optimization (SPO) reduction of Swamy et al. (2024). In short, Swamy et al. (2024) prove that rather than performing adversarial training, we are able to perform a simple and stable _self-play_ procedure while retaining strong theoretical guarantees. Practically, this corresponds to sampling at least two completions from the current policy, querying a learned preference / supervisor model on each pair, and using the winrate for each completion as its reward. We will now describe how we can adapt REBEL to this mode of feedback.

Assuming that we can query the preference oracle \(l(x,y,y^{\prime})\) at will, we can modify the least square objective Eq. (1) to

\[\theta_{t+1}:=\operatorname*{argmin}_{\theta}\sum_{x,y,y^{\prime},y^{\prime \prime}\in\mathcal{D}_{t}}\left(\frac{1}{\eta}\left(\ln\frac{\pi_{\theta}(y|x )}{\pi_{\theta_{t}}(y|x)}-\ln\frac{\pi_{\theta}(y^{\prime}|x)}{\pi_{\theta_{t }}(y^{\prime}|x)}\right)-(l(x,y,y^{\prime\prime})-l(x,y^{\prime},y^{\prime \prime}))\right)^{2}\]where \(x\sim\rho,y\sim\pi_{t}(\cdot|x),y^{\prime\prime}\sim\pi_{t}(\cdot|x),y^{\prime}\sim \mu(\cdot|x)\). When the exact value of \(l(x,y,y^{\prime})\) is unavailable but only a binary preference feedback \(o_{y,y^{\prime}}\in\{0,1\}\) sampling from Bernoulli with mean \(l(x,y,y^{\prime})\) is available, we can just replace \(l(x,y,y^{\prime\prime})-l(x,y^{\prime},y^{\prime\prime})\) by \(o_{y,y^{\prime}}-o_{y^{\prime},y^{\prime\prime}}\). It is easy to see that the Bayes optimal of the above least square regression problem is equal to:

\[\mathbb{E}_{y^{\prime\prime}\sim\pi_{t}(\cdot|x)}l(x,y,y^{\prime\prime})- \mathbb{E}_{y^{\prime\prime}\sim\pi_{t}(\cdot|x)}l(x,y^{\prime},y^{\prime\prime })=l(x,y,\pi_{t})-l(x,y^{\prime},\pi_{t}).\]

Swamy et al. (2024) define an iteration-dependent reward \(r_{t}(x,y):=\mathbb{E}_{y^{\prime\prime}\sim\pi_{t}(\cdot|x)}l(x,y,y^{\prime \prime})=l(x,y,\pi_{t})\). Thus, the above regression problem can be understood as an extension of REBEL to the setting where the reward function changes at each iteration \(t\). Swamy et al. (2024) shows that running the exact MD (Eq. 3) with this iteration-dependent reward function \(r_{t}\) leads to fast convergence to an approximate Minimax Winner, a property that we will use to provide the regret bound of REBEL in the general preference setting while accounting for nonzero mean squared error.

Justification for Assumption 1

Intuitively, this assumption is saying that there is a function in our class of regressors that is able to accurately fit the difference of rewards. Recall that our class of regressors is isomorphic to our policy class. Therefore, as long as our class of policies is expressive, we would expect this assumption to hold with small \(\epsilon\). For all domains we consider, our policy class is a flexible set of generative models (e.g. Transformer-based LLMs or diffusion models). Thus, we believe it is reasonable to believe this assumption holds in practice - see Figure 6 in Appendix L for empirical evidence of this point and Example 1 for more discussion.

More formally, the above assumption bounds the standard **in-distribution generalization error** (v.s. the point-wise guarantee in Eq. 8) of a well-defined supervised learning problem: least squares regression. The generalization error \(\epsilon\) captures the possible errors from the learning process for \(\theta_{t+1}\) and it could depend on the complexity of the policy class and the number of samples used in the dataset \(\mathcal{D}_{t}\). For instance, when the the function \(\ln\pi-\ln\pi^{\prime}\) induced by the log-difference of two policies \((\pi,\pi^{\prime})\) are rich enough (e.g., policies are deep neural networks) to capture the reward difference, then \(\epsilon\) in this assumption converges to zero as we increase the number of training data. Note that while \(\epsilon\) can be small, it does _not_ imply that the learned predictor will have a small prediction error in a point-wise manner - it almost certainly will not.

**Example 1**.: _One simple example is when \(\pi(y|x)\propto\exp(\theta^{\top}\phi(x,y))\) for some features \(\phi(x,y)\). In this case, \(\ln(\pi(y|x)/\pi_{t}(y|x))-\ln(\pi(y^{\prime}|x)/\pi_{t}(y^{\prime}|x))=(\theta -\theta_{t})^{\top}\left(\phi(x,y)-\phi(x,y^{\prime})\right)\), which means that our regression problem in Eq. 1 is a classic linear regression problem. When the reward \(r(x,y)\) is also linear in feature \(\phi(x,y)\), then Eq. 1 is a well-specified linear regression problem, and \(\epsilon\) typically scales in the rate of \(O\left(d/|\mathcal{D}_{t}|\right)\) with \(d\) being the dimension of feature \(\phi\)._

_We can extend the above example to the case where \(\phi\) is the feature corresponding to some kernel, e.g., RBF kernel or even Neural Tangent Kernel, which allows us to capture the case where \(\pi\) is a softmax wide neural network with the least square regression problem solved by gradient flow. The error \(\epsilon\) again scales \(\text{poly}(d/|\mathcal{D}_{t}|)\), where \(d\) is the effective dimension of the corresponding kernel._Proof of Theorem 1

In this section, we provide the proof of theorem 1. For notation simplicity, throughout the proof, we denote \(\pi_{t}\) for \(\pi_{\theta_{t}}\), and define \(f_{t}(x,y):=\frac{1}{\eta}\ln\frac{\pi_{t+1}(y|x)}{\pi_{t}(y|x)}\).

The following lemma shows that the learned function \(f_{t}\) can predict reward \(r\) well under both \(\pi_{t}\) and \(\mu\) up to terms that are \(y\)-independent.

**Lemma 1**.: _Consider any \(t\in[T]\). Define \(\Delta(x,y)=f_{t}(x,y)-r(x,y)\). Define \(\Delta_{\pi_{t}}(x)=\mathbb{E}_{y\sim\pi_{t}(\cdot|x)}\Delta(x,y)\) and \(\Delta_{\mu}(x)=\mathbb{E}_{y\sim\mu(\cdot|x)}\Delta(x,y)\). Under assumption 1, for all \(t\), we have the following:_

\[\mathbb{E}_{x,y\sim\pi_{t}(\cdot|x)}\left(f_{t}(x,y)-r(x,y)- \Delta_{\pi_{t}}(x)\right)^{2}\leq\epsilon,\] (13) \[\mathbb{E}_{x,y\sim\mu(\cdot|x)}\left(f_{t}(x,y)-r(x,y)-\Delta_{ \mu}(x)\right)^{2}\leq\epsilon,\] (14) \[\mathbb{E}_{x}\left(\Delta_{\pi_{t}}(x)-\Delta_{\mu}(x)\right)^{ 2}\leq\epsilon.\] (15)

Proof.: From assumption 1, we have:

\[\mathbb{E}_{x,y_{1}\sim\pi_{t},y_{2}\sim\mu}\left(f_{t}(x,y_{1}) -\Delta_{\pi_{t}}(x)-r(x,y_{1})-\left(f_{t}(x,y_{2})-\Delta_{\mu}(x)-r(x,y_{2 })\right)+\Delta_{\pi_{t}}(x)-\Delta_{\mu}(x)\right)^{2}\] \[=\mathbb{E}_{x,y_{1}\sim\pi_{t}}\left(f_{t}(x,y_{1})-\Delta_{\pi _{t}}(x)-r(x,y_{1})\right)^{2}+\mathbb{E}_{x,y_{2}\sim\mu}\left(f_{t}(x,y_{2}) -\Delta_{\mu}(x)-r(x,y_{2})\right)^{2}\] \[-2\mathbb{E}_{x,y_{1}\sim\pi_{t},y_{2}\sim\mu}\left(f_{t}(x,y_{1} )-\Delta_{\pi_{t}}(x)-r(x,y_{1})\right)\left(f_{t}(x,y_{2})-\Delta_{\mu}(x)-r( x,y_{2})\right)\] \[+2\mathbb{E}_{x,y_{1}\sim\pi_{t}}\left(f_{t}(x,y_{1})-\Delta_{ \pi_{t}}(x)-r(x,y_{1})\right)\left(\Delta_{\pi_{t}}(x)-\Delta_{\mu}(x)\right)\] \[-2\mathbb{E}_{x,y_{2}\sim\pi_{t}}\left(f_{t}(x,y_{2})-\Delta_{\mu }(x)-r(x,y_{2})\right)\left(\Delta_{\pi_{t}}(x)-\Delta_{\mu}(x)\right)+ \mathbb{E}_{x}(\Delta_{1}(x)-\Delta_{2}(x))^{2}\] \[=\mathbb{E}_{x,y_{1}\sim\pi_{t}}\left(f_{t}(x,y_{1})-\Delta_{\pi _{t}}(x)-r(x,y_{1})\right)^{2}+\mathbb{E}_{x,y_{2}\sim\mu}\left(f_{t}(x,y_{2} )-\Delta_{\mu}(x)-r(x,y_{2})\right)^{2}\] \[\qquad+\mathbb{E}_{x}(\Delta_{\pi_{t}}(x)-\Delta_{\mu}(x))^{2} \leq\epsilon.\]

In the above, we first complete the square, and then we only keep terms that are not necessarily zero. Since all the remaining three terms are non-negative, this concludes the proof. 

By the definition of \(f_{t}\), we have \(\Delta(x,y)=\frac{1}{\eta}\ln\frac{\pi_{t+1}(y|x)}{\pi_{t}(y|x)}-r(x,y)\). Taking \(\exp\) on both sides, we get:

\[\forall x,y:\ \pi_{t+1}(y|x)=\pi_{t}(y|x)\exp\left(\eta(r(x,y)+\Delta(x,y)) \right)=\frac{\pi_{t}(y|x)\exp(\eta(r(x,y)+\Delta(x,y)-\Delta_{\mu}(x)))}{\exp (-\eta\Delta_{\mu}(x))}\]

Denote \(g_{t}(x,y):=r(x,y)+\Delta(x,y)-\Delta_{\mu}(x)\), and the advantage \(A_{t}(x,y)=g_{t}(x,y)-\mathbb{E}_{y^{\prime}\sim\pi_{t}(\cdot|x)}g_{t}(x,y^{ \prime})\). We can rewrite the above update rule as:

\[\forall x,y:\ \pi_{t+1}(y|x)\propto\pi_{t}(y|x)\exp(\eta A_{t}(x,y))\] (16)

In other words, the algorithm can be understood as running MD on the sequence of \(A_{t}\) for \(t=0\) to \(T-1\). The following lemma is the standard MD regret lemma.

**Lemma 2**.: _Assume \(\max_{x,y,t}|A_{t}(x,y)|\leq A\in\mathbb{R}^{+}\), and \(\pi_{0}(\cdot|x)\) is uniform over \(\mathcal{Y}\). Then with \(\eta=\sqrt{\ln(|\mathcal{Y}|)/(A^{2}T)}\), for the sequence of policies computed by REBEL, we have:_

\[\forall\pi,x:\ \sum_{t=0}^{T-1}\mathbb{E}_{y\sim\pi(\cdot|x)}A_{t}(x,y)\leq 2A \sqrt{\ln(|\mathcal{Y}|)T}.\]

Proof.: For completeness, we provide the proof here. Start with \(\pi_{t+1}(y|x)=\pi_{t}(y|x)\exp(\eta A_{t}(x,y))/Z_{t}(x)\) where \(Z_{t}(x)\) is the normalization constant, taking log on both sides, and add \(\mathbb{E}_{y\sim\pi(\cdot|x)}\), we have:

\[-\text{KL}(\pi(\cdot|x)||\pi_{t+1}(\cdot|x))=-\text{KL}(\pi(\cdot|x)||\pi_{t}( \cdot|x))+\eta\mathbb{E}_{y\sim\pi(\cdot|x)}A_{t}(x,y)-\mathbb{E}_{y\sim\pi( \cdot|x)}\ln Z_{t}(x).\]

Rearrange terms, we get:

\[-\text{KL}(\pi(\cdot|x)||\pi_{t}(\cdot|x))+\text{KL}(\pi(\cdot|x)||\pi_{t+1}( \cdot|x)=\mathbb{E}_{y\sim\pi(\cdot|x)}\left[-\eta A_{t}(x,y)+\ln Z_{t}(x)\right]\]For \(\ln Z_{t}(x)\), using the condition that \(\eta\leq 1/A\), we have \(\eta A_{t}(x,y)\leq 1\), which allows us to use the inequality \(\exp(x)\leq 1+x+x^{2}\) for any \(x\leq 1\), which lead to the following inequality:

\[\ln Z_{t}(x) =\ln\left(\mathbb{E}_{y\sim\pi(\cdot|x)}\exp(\eta A_{t}(x,y))\right)\] \[\leq\ln\left(\sum_{y}\pi_{t}(y|x)\left(1+\eta A_{t}(x,y)+\eta^{2} A_{t}(x,y)^{2}\right)\right)\] \[\leq\ln\left(1+0+\eta^{2}A^{2}\right)\leq\eta^{2}A^{2},\]

where the last inequality uses \(\ln(1+x)\leq x\), and we used the fact that \(\mathbb{E}_{y\sim\pi_{t}(x)}A_{t}(x,y)=0\) due to the definition of advantage \(A_{t}\). Thus, we have:

\[-\text{KL}(\pi(\cdot|x)||\pi_{t}(\cdot|x))+\text{KL}(\pi(\cdot|x)||\pi_{t+1}( \cdot|x)\leq-\mathbb{E}_{y\sim\pi(\cdot|x)}[A_{t}(x,y)]+\eta^{2}A^{2}.\]

Sum over all iterations and do the telescoping sum, we get:

\[\sum_{t=0}^{T-1}\mathbb{E}_{y\sim\pi(\cdot|x)}A_{t}(x,y)\leq\text{KL}(\pi( \cdot|x)||\pi_{0}(\cdot|x))/\eta+T\eta A^{2}\leq\ln(|\mathcal{Y}|)/\eta+T\eta A ^{2}.\]

With \(\eta=\sqrt{\ln(|\mathcal{Y}|)/(A^{2}T)}\), we conclude the proof. 

With the above, now we are ready to conclude the proof of the main theorem.

Proof of Theorem 1.: Consider a comparator policy \(\pi^{*}\). We start with the performance difference between \(\pi^{*}\) and the uniform mixture policy \(\bar{\pi}:=\sum_{t=0}^{T-1}\pi_{t}/T\):

\[\frac{1}{T}\sum_{t=0}^{T-1}\left(\mathbb{E}_{x,y\sim\pi^{*}(\cdot|x)}r(x,y)- \mathbb{E}_{x,y\sim\pi_{t}(\cdot|x)}r(x,y)\right)=\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}_{x,y\sim\pi^{*}(\cdot|x)}\left(A^{\pi_{t}}(x,y)\right),\]

where we define the real advantage \(A^{\pi_{t}}(x,y):=r(x,y)-\mathbb{E}_{y\sim\pi_{t}(\cdot|x)}r(x,y)\). Continue, we have:

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{x,y\sim\pi^{*}(\cdot|x)} \left(A^{\pi_{t}}(x,y)\right)\] \[=\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{x,y\sim\pi^{*}(\cdot|x)} \left(A_{t}(x,y)\right)+\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{x,y\sim\pi^{*} (\cdot|x)}\left(A^{\pi_{t}}(x,y)-A_{t}(x,y)\right)\] \[\leq 2A\sqrt{\frac{\ln(|\mathcal{Y}|)}{T}}+\frac{1}{T}\sum_{t=0}^{ T-1}\sqrt{\mathbb{E}_{x}\mathbb{E}_{y\sim\pi^{*}(\cdot|x)}(A^{\pi_{t}}(x,y)-A_{t}(x,y)) ^{2}}\]

where the last inequality uses Lemma 2. We now just need to bound \(\mathbb{E}_{y\sim\pi^{*}(\cdot|x)}(A^{\pi_{t}}(x,y)-A_{t}(x,y))^{2}\).

\[\mathbb{E}_{x}\mathbb{E}_{y\sim\pi^{*}(\cdot|x)}(A^{\pi_{t}}(x,y )-A_{t}(x,y))^{2} =\mathbb{E}_{x}\mathbb{E}_{y\sim\mu(\cdot|x)}\frac{\pi^{*}(y|x)} {\mu(y|x)}(A^{\pi_{t}}(x,y)-A_{t}(x,y))^{2}\] \[\leq C_{\pi^{*}}\mathbb{E}_{x,y\sim\mu(\cdot|x)}(A^{\pi_{t}}(x,y)- A_{t}(x,y))^{2}\]

where the last inequality uses the definition of concentrability coefficient \(C_{\pi^{*}}\). We now bound \(\mathbb{E}_{x,y\sim\mu(\cdot|x)}(A^{\pi_{t}}(x,y)-A_{t}(x,y))^{2}\). Recall the definiton of \(A_{t}\) from Lemma 2.

\[\mathbb{E}_{x,y\sim\mu(\cdot|x)}(A^{\pi_{t}}(x,y)-A_{t}(x,y))^{2}\] \[=\mathbb{E}_{x,y\sim\mu(\cdot|x)}(r(x,y)-\mathbb{E}_{y^{\prime} \sim\pi_{t}(\cdot|x)}r(x,y^{\prime})-g_{t}(x,y)+\mathbb{E}_{y^{\prime}\sim\pi _{t}(\cdot|x)}g_{t}(x,y^{\prime}))^{2}\] \[\leq 2\mathbb{E}_{x,y\sim\mu(\cdot|x)}\left(r(x,y)-g_{t}(x,y) \right)^{2}+2\mathbb{E}_{x}\mathbb{E}_{y^{\prime}\sim\pi_{t}(\cdot|x)}\left(r(x,y^{\prime})-g_{t}(x,y^{\prime})\right)^{2}\]

Recall the \(g_{t}(x,y)=r(x,y)+\Delta(x,y)-\Delta_{\mu}(x)\), and from Lemma 1, we can see that

\[\mathbb{E}_{x,y\sim\mu(\cdot|x)}(r(x,y)-g_{t}(x,y))^{2}=\mathbb{E}_{x,y\sim\mu (\cdot|x)}(\Delta(x,y)-\Delta_{\mu}(x))^{2}\leq\epsilon.\]For \(\mathbb{E}_{x}\mathbb{E}_{y^{\prime}\sim\pi_{t}(\cdot|x)}\left(r(x,y^{\prime})-g_{ t}(x,y^{\prime})\right)^{2}\), we have:

\[\mathbb{E}_{x}\mathbb{E}_{y^{\prime}\sim\pi_{t}(\cdot|x)}\left(r(x,y^{\prime})-g _{t}(x,y^{\prime})\right)^{2}=\mathbb{E}_{x}\mathbb{E}_{y^{\prime}\sim\pi_{t}( \cdot|x)}\left(\Delta(x,y^{\prime})-\Delta_{\mu}(x)\right)^{2}\]

where the last inequality uses Lemma 1 again. This step relies on the fact that one of the samples is always on-policy, i.e., from \(\pi_{t}\).

Combine things together, we can conclude that:

\[\mathbb{E}_{x}\mathbb{E}_{y\sim\pi^{*}(\cdot|x)}(A^{\pi_{t}}(x,y)-A_{t}(x,y))^ {2}\leq C_{\pi^{*}}(10\epsilon).\]

Finally, for the regret, we can conclude:

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{x,y\sim\pi^{*}(\cdot|x)}\left(A^{\pi_{ t}}(x,y)\right)\leq 2A\sqrt{\frac{\ln|\mathcal{Y}|}{T}}+\frac{1}{T}\sum_{t} \sqrt{C_{\pi^{*}}10\epsilon}=2A\sqrt{\frac{\ln|\mathcal{Y}|}{T}}+\sqrt{C_{\pi ^{*}}10\epsilon}.\]Extension of analysis to General Preferences

Extending the above analysis to the general preference case is straightforward except that it requires a stronger coverage condition. This is because we want to find a Nash Equilibrium, which requires a comparison between the learned policy against all the other policies. Results from the Markov Game literature (Cui and Du, 2022; Zhong et al., 2022; Cui and Du, 2022; Xiong et al., 2023) and Cui and Du (2022) have shown that the standard single policy coverage condition used in single-player optimization is provably not sufficient. In particular, they propose using a notion of _unilateral concentrability_ for efficient learning, which can be defined as

\[C_{\text{uni},\mu}:=\max_{\pi,x,y,y^{\prime\prime}}\frac{\pi_{\text{MW}}(y|x) \pi(y^{\prime\prime}|x)}{\mu(y|x)\mu(y^{\prime\prime}|x)},\]

in the general preference setting. Notably, the above unilateral concentrability coefficient \(C_{\text{uni},\mu}\) is equivalent to \(C_{\mu}:=\max_{\pi,x,y}\frac{\pi(y|x)}{\mu(y|x)}\) since \(C_{\mu}\leq C_{\text{uni},\mu}\leq C_{\mu}^{2}\). Therefore in the following discussion, we will use \(C_{\mu}\) as the coverage condition. In addition, we also assume the generalization error of the regression problem is small,

**Assumption 2** (Regression generalization bounds for general preference).: _Over \(T\) iterations, assume that for all \(t,\) we have:_

\[\mathbb{E}_{x\sim\rho,y\sim\pi_{t}(\cdot|x),y^{\prime}\sim\mu(\cdot|x)}\left( \frac{1}{\eta}\left(\ln\frac{\pi_{\theta_{t+1}}(y|x)}{\pi_{\theta_{t}}(y|x)}- \ln\frac{\pi_{\theta_{t+1}}(y^{\prime}|x)}{\pi_{\theta_{t}}(y^{\prime}|x)} \right)-(l(x,y,\pi_{t})-l(x,y^{\prime},\pi_{t}))\right)^{2}\leq\epsilon,\]

_for some \(\epsilon\)._

Under the above coverage condition and generalization bound, we can show that REBEL is able to learn an approximate Minimax Winner:

**Theorem 2**.: _With assumption 2, after \(T\) many iterations, with a proper learning rate \(\eta\), the policy \(\widehat{\pi}=\text{\sf{Unif}}(\{\pi_{t}\}_{t=1}^{T})\) satisfies that:_

\[\mathsf{DG}(\widehat{\pi})\leq O\left(\sqrt{\frac{1}{T}}+\sqrt{C_{\mu}} \epsilon.\right).\]

_Here the \(O\)-notation hides problem-dependent constants that are independent of \(\epsilon,C_{\mu},T\)._

Note that the coverage condition here is much stronger than the single policy coverage condition in the RL setting. We conjecture that this is the cost one has to pay by moving to the more general preference setting and leaving the investigation of the necessarily coverage condition for future work.

### Proof of Theorem 2

Recall that \(r_{t}(x,y)=l(x,y,\pi_{t})\). Let us define \(\Delta^{t}(x,y):=f_{t}(x,y)-r_{t}(x,y)\), \(\Delta^{t}_{\pi_{t}}(x):=\mathbb{E}_{y\sim\pi_{t}(\cdot|x)}\Delta^{t}(x,y)\) and \(\Delta^{t}_{\mu}(x):=\mathbb{E}_{y\sim\mu(\cdot|x)}\Delta^{t}(x,y)\). Then following the same arguments in Lemma 1, we have

\[\mathbb{E}_{x\sim\rho,y\sim\pi_{t}(\cdot|x)}\left[\left(f_{t}(x,y)-r_{t}(x,y) -\Delta^{t}_{\pi_{t}}(x)\right)^{2}\right]\leq\epsilon,\] (17)

\[\mathbb{E}_{x\sim\rho,y\sim\mu(\cdot|x)}\left[\left(f_{t}(x,y)-r_{t}(x,y)- \Delta^{t}_{\mu}(x)\right)^{2}\right]\leq\epsilon,\] (18)

\[\mathbb{E}_{x\sim\rho}\left[\left(\Delta^{t}_{\pi_{t}}(x)-\Delta^{t}_{\mu}(x) \right)^{2}\right]\leq\epsilon.\] (19)

With slight abuse of the notation, We also use \(g_{t}\) and \(A_{t}(x,y)\) to denote \(r_{t}(x,y)+\Delta^{t}(x,y)-\Delta^{t}_{\mu}(x,y)\) and \(g_{t}(x,y)-\mathbb{E}_{y^{\prime}\sim\pi_{t}(\cdot|x)}g_{t}(x,y^{\prime})\). Then following the same arguments in Lemma 2,

\[\forall\pi,x:\ \sum_{t=0}^{T-1}\mathbb{E}_{y\sim\pi(\cdot|x)}A_{t}(x,y)\leq 2A \sqrt{\ln(|\mathcal{Y}|)T}.\] (20)Note that we have

\[\max_{\pi}l(\pi,\widehat{\pi})=\max_{\pi}\frac{1}{T}\sum_{t=1}^{T}l(\pi,\pi_{t})\]

\[=\max_{\pi}\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{x\sim\rho,y\sim\pi(\cdot|x)}[r_{ t}(x,y)]=\max_{\pi}\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{x\sim\rho,y\sim\pi(\cdot|x)}[A ^{t,\pi_{t}}(x,y)],\]

where \(A^{t,\pi_{t}}:=r_{t}(x,y)-\mathbb{E}_{y\sim\pi_{t}(\cdot|x)}[r_{t}(x,y)]\). The last step is due to the skew symmetry of \(l\), i.e., \(\mathbb{E}_{y\sim\pi_{t}(\cdot|x)}[r_{t}(x,y)]=l(x,\pi_{t},\pi_{t})=0\). Then by following the same arguments in the proof of Theorem 1, with (17)(18)(19)(20), we have for any policy \(\pi\),

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{x\sim\rho,y\sim\pi(\cdot|x)}\left(A^{t,\pi_{t}}(x,y)\right)\leq 2A\sqrt{\frac{\ln|\mathcal{Y}|}{T}}+\sqrt{10C_{ \mu\to\pi}}\epsilon.\]

This implies that

\[\max_{\pi}l(\pi,\widehat{\pi})\leq\max_{\pi}\left(2A\sqrt{\frac{\ln|\mathcal{Y }|}{T}}+\sqrt{10C_{\mu\to\pi}}\epsilon\right)\leq 2A\sqrt{\frac{\ln|\mathcal{Y }|}{T}}+\sqrt{10C_{\mu}}\epsilon.\]

Note that due to the skew symmetry of \(l\), we have

\[\min_{\pi}l(\widehat{\pi},\pi)=\min_{\pi}\mathbb{E}_{x\sim\rho,y\sim\widehat{ \pi}(\cdot|x),y^{\prime}\sim\pi(\cdot|x)}\left[l(x,y,y^{\prime})\right]=-\max _{\pi}\mathbb{E}_{x\sim\rho,y\sim\widehat{\pi}(\cdot|x),y^{\prime}\sim\pi(\cdot |x)}\left[-l(x,y,y^{\prime})\right]\]

\[=-\max_{\pi}\mathbb{E}_{x\sim\rho,y\sim\pi(\cdot|x),y^{\prime}\sim\pi(\cdot |x)}\left[l(x,y,y^{\prime})\right]=-\max_{\pi}l(\pi,\widehat{\pi})\geq-2A \sqrt{\frac{\ln|\mathcal{Y}|}{T}}-\sqrt{10C_{\mu}}\epsilon.\]

Therefore we have

\[\mathsf{DG}(\widehat{\pi})\leq 4A\sqrt{\frac{\ln|\mathcal{Y}|}{T}}+2\sqrt{10C _{\mu}}\epsilon.\]Additional Experiment Details

### Summarization

#### h.1.1 Dataset Details

We present dataset details in Table 4. Dataset available at https://github.com/openai/summarize-from-feedback

#### h.1.2 Model Details

For SFT models, we train a Pythia 1.4B (Biderman et al., 2023)4 model for 1 epoch over the dataset with human references as labels, and use the existing fine-tuned 2.8B5 and 6.9B6 models. For reward models, we train a Pythia 1.4B parameter model for 1 epoch over the preference dataset and use the existing reward models with 2.8B7 and 6.9B8 parameters. For both REBEL and baseline methods using 1.4B and 2.8B parameters, we trained the policy and/or the critic using **low-rank adapters (LoRA)**(Hu et al., 2022) on top of our SFT and/or reward model respectively. For the 6.9B models, we perform **full-parameter** training. The 1.4B and 2.8B models are trained on 8 A6000 GPUs for one day and two days respectively. The 6.9B model is train on 8 H100 GPUs for two days.

Footnote 4: HuggingFace Model Card: EleutherAI/pythia-1.4b-deduped

Footnote 5: HuggingFace Model Card: vwxyzjn/EleutherAI_pythia-2.8b-deduped_sft_tldr

Footnote 6: HuggingFace Model Card: vwxyzjn/EleutherAI_pythia-6.9b-deduped_sft_tldr

Footnote 7: HuggingFace Model Card: vwxyzjn/EleutherAI_pythia-2.8b-deduped_reward_tldr

Footnote 8: HuggingFace Model Card: vwxyzjn/EleutherAI_pythia-6.9b-deduped_reward_tldr

#### h.1.3 Baseline Implementation Details

For supervised fine-tuning (SFT), reward modeling training, PPO, and DPO, we follow the implementation at https://github.com/vwxyzjn/summarize_from_feedback_details (Huang et al., 2024). For iterative dpo, we implement as follows:

```
1:Input: Reward \(r\), policy class \(\Pi=\{\pi_{\theta}\}\), parameter \(\beta\)
2: Initialize policy \(\pi_{\theta_{o}}\).
3:for\(t=0\) to \(T-1\)do
4: Collect dataset \(\mathcal{D}_{t}=\{x,y,y^{\prime}\}\) where \(x\sim\rho,y\sim\pi_{t}(\cdot|x),y^{\prime}\sim\pi_{t}(\cdot|x)\)
5: Solve square loss regression problem: \[\theta_{t+1}=\operatorname*{argmin}_{\theta}\sum_{(x,y,y^{\prime})\in \mathcal{D}_{t}}-\left[\ln\sigma\left((\beta\ln\frac{\pi_{\theta}(y|x)}{\pi_ {\theta_{t}}(y|x)}-\beta\ln\frac{\pi_{\theta}(y^{\prime}|x)}{\pi_{\theta_{t}} (y^{\prime}|x)})sgn\left(r(x,y)-r(x,y^{\prime})\right)\right)\right]\]
6:endfor ```

**Algorithm 2** Iterative DPO

#### h.1.4 Baseline Implementation Details

For supervised fine-tuning (SFT), reward modeling training, PPO, and DPO, we follow the implementation at https://github.com/vwxyzjn/summarize_from_feedback_details (Huang et al., 2024). For iterative dpo, we implement as follows:

```
1:Input: Reward \(r\), policy class \(\Pi=\{\pi_{\theta}\}\), parameter \(\beta\)
2: Initialize policy \(\pi_{\theta_{o}}\).
3:for\(t=0\) to \(T-1\)do
4: Collect dataset \(\mathcal{D}_{t}=\{x,y,y^{\prime}\}\) where \(x\sim\rho,y\sim\pi_{t}(\cdot|x),y^{\prime}\sim\pi_{t}(\cdot|x)\)
5: Solve square loss regression problem: \[\theta_{t+1}=\operatorname*{argmin}_{\theta}\sum_{(x,y,y^{\prime})\in \mathcal{D}_{t}}-\left[\ln\sigma\left((\beta\ln\frac{\pi_{\theta}(y|x)}{\pi_ {\theta_{t}}(y|x)}-\beta\ln\frac{\pi_{\theta}(y^{\prime}|x)}{\pi_{\theta_{t}} (y^{\prime}|x)})sgn\left(r(x,y)-r(x,y^{\prime})\right)\right)\right]\]
6:endfor ```

**Algorithm 3** Iterative DPO

For iterative dpo, we implement as follows:

\[\theta_{t+1}=\operatorname*{argmin}_{\theta}\sum_{(x,y,y^{\prime})\in \mathcal{D}_{t}}-\left[\ln\sigma\left((\beta\ln\frac{\pi_{\theta}(y|x)}{\pi_ {\theta_{t}}(y|x)}-\beta\ln\frac{\pi_{\theta}(y^{\prime}|x)}{\pi_{\theta_{t}} (y^{\prime}|x)})sgn\left(r(x,y)-r(x,y^{\prime})\right)\right)\right]\]

6:endfor ```

**Algorithm 3** Iterative DPO

where \(sgn\) is a sign function. Our implementation of iterative DPO is similar to REBEL where, at each iteration, we update with respect to \(\pi_{\theta_{t}}\). The major difference is that REBEL regresses toward the differences in rewards while iterative DPO only utilizes the pairwise preference signal from the rewards.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & Train/Val/Test & Prompt & Generation Length \\ \hline Human Reference & 117K/6.45K/6.55K & “TL;DR;” & 53 \\ Preference & 92.9K/83.8K/- & “TL;DR:” & 53 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Dataset split, prompts, and maximum generation length for _TL;DR_ summarization 

#### h.1.4 Reward Details

To ensure that \(\pi_{\theta}\) remains close to \(\pi_{\theta_{0}}\), we apply an additional KL penalty to the reward:

\[r(x,y)=RM(x,y)-\gamma(\ln\pi_{\theta_{1}}(y|x)-\ln\pi_{\theta_{0}}(y|x))\] (22)

where \(RM(x,y)\) is score from the reward model given prompt \(x\) and response \(y\). Furthermore, to ensure that the generations terminate within the maximum generation length, we penalize any generation that exceeds this length by setting \(r(x,y)\) to a small fixed constant, \(\Gamma\).

For _TL;DR_ summarization, we set \(\gamma=0.05\) and \(\Gamma=-1\).

#### h.1.5 Hyperparameter Details

**Parameter setting for _TL;DR_ summarization

\begin{tabular}{l l l} \hline \hline
**Setting** & **Parameters** & \\ \hline SFT \& RM & batch size: 64 & schedule: cosine decay \\  & learning rate: 3e-6 & train epochs: 1 \\ \hline PPO & batch size: 512 & discount factor: 1 \\  & learning rate: 3e-6 & gae \(\lambda\): 0.95 \\  & schedule: linear decay & clip ratio: 0.2 \\  & train epochs: 1 & value function coeff: 0.1 \\  & num epochs: 4 & kl coefficient: 0.05 \\ \hline DPO & batch size: 64 & train epochs: 1 \\  & learning rate: 3e-6 & \(\beta\): 0.05 \\  & schedule: linear decay & \\ \hline RLOO & batch size: 512 & train epochs: 1 \\  & learning rate: 3e-6 & kl coefficient: 0.05 \\  & schedule: linear decay & K: 2 or 4 \\ \hline REINFORCE & batch size: 512 & train epochs: 1 \\  & learning rate: 3e-6 & kl coefficient: 0.05 \\  & schedule: linear decay & \\ \hline Iterative DPO & batch size: 512 & num epochs: 4 \\  & learning rate: 3e-6 & \(\beta\): 0.05 \\  & schedule: linear decay & kl coefficient: 0.05 \\  & train epochs: 1 & \\ \hline REBEL & batch size: 512 & num epochs: 4 \\  & learning rate: 3e-6 & \(\eta\): 1.0 \\  & schedule: linear decay & kl coefficient: 0.05 \\  & train epochs: 1 & \\ \hline LoRA Adapter & r: 1024 & dropout: 0.0 \\ Config & \(\alpha\): 2048 & bias: False \\ \hline Generation & sampling: true & min length: 53 \\  & top k: 0.0 & max new tokens: 53 \\  & top p: 1.0 & temperature: 0.1 \\ \hline \hline \end{tabular}

#### h.1.6 Winrate Details

We are using gpt-4-0613 checkpoint for winrate computations. Below we show the prompt for winrate evaluation and an example evaluation from GPT4.

Prompt for Winrate

Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? Judge based on accuracy, coverage, and coherence.

### Post: {{post}}

### Summary A: {{summarya}}

### Summary B: {{summaryb}}

Instructions: FIRST provide a one-sentence comparison of the two summaries, explaining which you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your choice. Your response should use the format: Comparison: <one-sentence comparison and explanation > Preferred: <"A" or "B">

Example Evaluation from GPT4

PromptSUBREDDIT: r/AskReddit

TITLE: How do you get someone out of your head?

POST: Hi,

I'm 22, and I have been with my girlfriend for 5 years now. We recently moved together. We've always loved each other intensely.

Problem, I recently started to have feelings for an other person (a friend). This person has had a boyfriend for now 3 years, and has absolutely no ideas. Those feelings were so strong, it was hard to hide them. After 2 months of me being distant and really sad, my girlfriend forced me to say what was bothering me. I'm not a good liar, and now she knows.

We decided to give us a week alone, I went to my parents.

Now, I'm completely lost. I keep on thinking about this person, and I hate that. I would like for those feelings to go away, to leave me alone. But I can't.

What do I do? It's been 3 months now, and I'm just desperate.

TL;DR:

\begin{tabular}{p{113.8pt} p{284.5pt}} \hline \hline
**Reference** & long relationship; fell in love with an other person; admitted it; would like it to disappear, though it doesn't. \\ \hline REBEL & I recently started to have feelings for an other person (a friend). We decided to give us a week alone, I keep on thinking about that person, and I hate it. What do I do? \\ \hline
**Evaluation from GPT4** & Comparison: Summary A is too brief and rather disjointed, while Summary B more accurately conveys the emotional conflict portrayed in the forum post in a coherent manner. Preferred: B \\ \hline \hline \end{tabular}

### General Chat

#### h.2.1 Dataset Details

We present dataset details in Table 5.

#### h.2.2 Model Details

For OpenChat-3.511, we only train the last four layers and keep other layers frozen. For Meta-Llama-3-8B-Instruct12, we perform full-parameter training. For Starling-RM-7B-alpha13 and ArmoRM-Llama3-8B-v0.114, we directly use the reward scores without any normalizations. We filter out prompts that are longer than \(1,024\) tokens (\(2.3\%\)) to fit the input length. OpenChat-3.5 is trained for four days, and Meta-Llama-3-8B-Instruct is train for one day on 8 H100 GPUs.

Footnote 11: https://github.com/huggingface/

Footnote 12: https://github.com/huggingface/

Footnote 13: https://github.com/huggingface/

Footnote 14: https://github.com/huggingface/

#### h.2.3 Reward Details

To ensure that \(\pi_{\theta}\) remains close to \(\pi_{\theta_{0}}\), we apply an additional KL penalty to the reward:

\[r(x,y)=RM(x,y)-\gamma(\ln\pi_{\theta_{1}}(y|x)-\ln\pi_{\theta_{0}}(y|x))\] (23)

where \(RM(x,y)\) is score from the reward model given prompt \(x\) and response \(y\). Furthermore, to ensure that the generations terminate within the maximum generation length, we penalize any generation that exceeds this length by setting \(r(x,y)\) to a small fixed constant, \(\Gamma\).

For the general chat experiments, we set \(\Gamma=-4\).

#### h.2.4 Hyperparameter Details

**Parameter setting for General Chat**

\begin{tabular}{l l} \hline \hline
**Setting** & **Parameters** \\ \hline Base model: OpenChat-3.5 & batch size: 32 \\ Reward Model: Starling-RM-7B-alpha & learning rate: 1e-7 \\ Dataset: Nectar & schedule: linear decay \\  & train epochs: 1 \\  & num epochs: 4 \\  & \(\eta\): 1.0 \\  & \(\gamma\): 0.05 \\  & \(\Gamma\): -4 \\ \hline Base model: Meta-Llama-3-8B-Instruct & mini-batch size: 128 \\ Reward Model: ArmoRM & learning rate: 3e-7 \\ Dataset: UltraFeedback & schedule: cosine decay \\  & warm ratio: 0.1 \\  & train epochs: 1 \\  & iteration: 3 \\  & \(\eta\): 1e6 (iter 1), 1e4 (iter 2), 1e2 (iter 3) \\  & \(\gamma\): 0 \\ \hline \hline \end{tabular}

[MISSING_PAGE_POST]

Footnote 29: footnotemark: [

### Image Generation

#### h.3.1 Dataset Details

Generation prompts: cat, dog, horse, monkey, rabbit, zebra, spider, bird, sheep, deer, cow, goat, lion, tiger, bear, raccoon, fox, wolf, lizard, beetle, ant, butterfly, fish, shark, whale, dolphin, squirrel, mouse, rat, snake, turtle, frog, chicken, duck, goose, bee, pig, turkey, fly, llama, camel, bat, gorilla, hedgehog, kangaroo.

#### h.3.2 Model Details

We use the latent consistency model (Luo et al., 2023) distillation of the Dreamshaper v7 model 15 for our experiments. Experiments are conducted on 4 A6000 GPUs with each run requiring 10 hours.

Footnote 15: Huggingface model card: SimianLuo/LCM_Dreamshaper_v7

#### h.3.3 Hyperparameter Details

**Parameter setting for Consistency Models**

\begin{tabular}{p{113.8pt} p{113.8pt}} \hline \hline
**Setting** & **Parameters** \\ \hline \hline \multirow{2}{*}{PPO} & advantage clip maximum: 10 batches per epoch: 10 clip range: 1e-4 learning rate: 1e-4 gradient accumulation steps: 8 max gradient norm: 5 number of epochs: 100 horizon: 8 number of sample inner epochs: 2 sample batch size (per GPU): 8 rolling statistics buffer size: 32 rolling statistics min count: 16 train batch size (per GPU): 2 LoRA rank: 8 Lora \(\alpha\): 8 \\ \hline REBEL & advantage clip maximum: 10 batches per epoch: 10 learning rate: 3e-4 \(\eta\): 1 gradient accumulation steps: 8 max gradient norm: 5 number of epochs: 100 horizon: 8 number of sample inner epochs: 1 sample batch size (per GPU): 8 rolling statistics buffer size: 32 rolling statistics min count: 16 train batch size (per GPU): 1 LoRA rank: 8 Lora \(\alpha\): 8 \\ \hline \hline \end{tabular}

Example Generations

### Summarization Generations from a 2.8B model

\begin{tabular}{l l} \hline \hline
**Prompt** & SUBREDDIT: r/relationships \\  & TITLE: Me [18 M] with my college friend [19 F] about 1 month, I wanna hang out with her but I don't have a car. \\  & POST: This is the link for an earlier thread I posted about her \\  & She basically doesn't have any friends and will spend all her spring break at home playing video games. I wanna hang out with her, but both of us don't have a car. I live with my grandma 10 minutes away from her house. Would it be weird to ask her to spend a day with us playing video games and eating pizza? \\  & Another thing I could do is ask her to go see a movie with me, but I would just meet her there since I'll be going with a bike. Do you guys think it's okay to do that? I also have 0 experience with girls. You can say this is the first girl in my life. \\  & TL:DR: \\  & My college friend doesn't have any friends and spends all her spring break at home playing video games. I wanna hang out with her but neither of us have a car. Do you think it's okay to ask her to hang out? \\ \hline
**Prompt** & SUBREDDIT: r/AskReddit \\  & TITLE: Reddit, what kind of effect do you think that crime shows/movies might have in reducing the occurrence of thought-out, "intelligent" crime? \\  & POST: I had this thought a few days ago when watching an episode of NCIS and they did the usual "create a super high resolution image from a very low resolution image." I got to wondering how much of it is real and how much of it is fake (or, conversely, how much might be under-exaggerated). Based on these types of shows/movies (bank robbery, conning, etc.) where the bad guy nearly 100% of the time gets caught, only an idiot would think it would be a good idea to do any of this stuff, and from my [limited] knowledge of the general criminal, I'm under the impression that they are mainly uneducated people in dire situations (or psychopaths/mentally unstable.. but I'm thinking mostly with regards the heist type of crime - not assault or murder) that are driven to extreme lengths to make the "easy"/"quick" dollar. So I wondered, is it possible that these shows/movies that show the criminal, no matter how smart and creative, almost always getting caught could actually be (subconsciously or consciously) reducing the amount of more well thought through crime throughout the world? \\  & **Generation** & TL:DR: \\  & I had this thought recently watching NCIS episode where criminals almost always get caught, is it possible that these crime shows/movies that show the criminal almost always getting caught could actually be (subconsciously or consciously) reducing the crime? \\ \hline \hline \end{tabular}

### Chat Generations from REBEL-Llama-3

\begin{tabular}{p{113.8pt} p{284.5pt}} \hline \hline
**Prompt** & Hi, I'd like to play ice hockey. Can you explain how the game works? \\
**Generation** & Hockey enthusiast! I'd be happy to explain the basics of ice hockey. Here's a comprehensive overview to get you started: \\
**Objective:** & The objective of ice hockey is to score more goals than the opposing team by using your stick to hit a puck (a rubber disc) into the opponent's goal. \\
**Gameplay:** & 1. **Game Structure:** A standard ice hockey game consists of three periods, each lasting 20 minutes. \\
2. **Teams:** Each team consists of six players on the ice at a time: \\  & **Forwards** (3): Players who score goals and create scoring opportunities. \\  & **Defensemen** (2): Players who defend their goal and prevent the opposing team from scoring. \\  & **Goalie** (1): The player responsible for preventing the puck from entering the goal. \\
3. **Gameplay Flow:** The game starts with a face-off, where two players from opposing teams try to gain control of the puck after it's dropped onto the ice by the referee. Players can move the puck using their sticks, skates, or by carrying it. \\
4. **Scoring:** A goal is scored when a player hits the puck into the opponent's goal using their stick, skate, or body. \\
5. **Penalties:** If a player breaks the rules, they can be sent to the penalty box for a set amount of time (2-5 minutes). During this time, their team must play shorthanded (with one fewer player on the ice). \\
6. **Power Play:** If the opposing team scores during a penalty, the penalized team gets a power play, where they have a man advantage on the ice. \\
7. **Icing:** If a player shoots the puck from behind their own blue line and it crosses the opponent's goal line without being touched, it's considered icing, and the opposing team is awarded a face-off in their own end zone. \\
8. **Offside:** A player cannot enter the opponent's zone before the puck, or they'll be called offside. \\
**Common Penalties:** & * **Hooking:** Using the stick to hook an opponent's leg or body. \\  & * **Holding:** Grabbing onto another player's jersey or equipment. \\  & * **Tripping:** Using a stick, leg, or body to trip an opponent. \\  & * **Interference:** Interfering with an opponent who doesn't have possession of the puck. \\
**Equipment:** & * **Stick:** A long, flat stick used to pass, shoot, and control the puck. \\  & * **Skates:** Specialized ice skates designed for speed, agility, and stopping. \\  & * **Pads:** Protective gear worn by players, including shoulder pads, elbow pads, gloves, shin guards, and hockey pants. \\  & * **Helmet:** A protective helmet worn by all players to prevent head injuries. \\  & * **Goalie Gear:** Specialized equipment worn by goalies, including a large pad covering their body, gloves, and a mask. \\
**Basic Strategies:** & * **Forecheking:** Applying pressure to the opponent's defensemen to gain possession of the puck. \\  & * **Backchecking:** Pressuring the opponent's forwards to prevent them from scoring. \\  & * **Breakouts:** Quickly moving the puck from your own end zone to the neutral zone to create scoring opportunities. \\ \end{tabular}

This is a solid foundation to get you started. As you play more, you'll learn more about the game's nuances and strategies. Have fun playing ice hockey!

**Prompt**: Why do people give Reddit Gold to admins?

\begin{tabular}{p{14.2pt} p{284.5pt}}
**Generation** & On Reddit, Reddit Gold (also known as Gold) is a virtual currency that allows users to show appreciation for content creators, moderators, and other users who contribute positively to the community. When someone gives Reddit Gold to an admin (or moderator), it's often a way to express gratitude for their efforts in maintaining the community's quality, resolving issues, and keeping the discussion civil. \\  & Here are some reasons why people might give Reddit Gold to admins: \\  & 1. **Appreciation for moderation**: Admins work tirelessly to ensure that discussions remain on-topic, respectful, and follow community guidelines. Giving Gold acknowledges their dedication and hard work in maintaining a positive environment. \\  & 2. **Recognition of expertise**: Admins often possess in-depth knowledge about specific topics or communities. Giving Gold shows appreciation for their expertise and willingness to share it with others. \\  & 3. **Support for community building**: Admins play a crucial role in fostering engagement, encouraging participation, and creating opportunities for users to connect with each other. Gold gifts demonstrate support for their efforts in building a strong community. \\  & 4. **Rewarding good behavior**: When admins demonstrate excellent behavior, such as resolving conflicts fairly or providing helpful guidance, users might give Gold as a way to recognize and reward their good behavior. \\  & 5. **Boosting morale**: Giving Gold can be a morale booster for admins, acknowledging their value to the community and motivating them to continue their excellent work. \\  & It's worth noting that Reddit Gold is not required or expected, and admins don't receive any direct benefits from receiving Gold. However, the gesture of appreciation can go a long way in fostering a positive and supportive community. \\ \end{tabular}

### Image Generations

Example Generations of REBEL

## Appendix A

## Appendix J Ablation Analysis

Just like DPO, tuning REBEL is much more straightforward than PPO since the only hyperparameter REBEL introduced is \(\eta\). We investigate how sensitive REBEL is to learning rate \(\eta\) in the loss. The results of ablation on summarization task and 2.8B model is shown in Table 6 with the same setting detailed in Appendix H.1.5 except for \(\eta\). REBEL achieves the best performance when \(\eta=1\), while increasing or decreasing \(\eta\) leads to decreased performance. Our result here indicates that \(\eta\) is an important hyperparameter that requires tuning for achieving a good performance. Setting \(\eta\) to \(1.0\) is a good starting point since, for all of our experiments from language modeling to image generation, \(\eta=1\) achieves the best results.

## Appendix K Trade-off between Reward Model Score and KL-divergence

The trade-off between the reward model score and KL-divergence is shown in Figure 5. We evaluate the 2.8B REBEL and PPO every 400 gradient updates during training for 8,000 updates on summarization. The sample complexity of each update is held constant across both algorithms for fair comparison. For the left plot, each point represents the average divergence and score over the entire test set, and the eclipse represents the confidence interval with 2 standard deviations. As observed previously, PPO exhibits lower divergence, whereas REBEL shows higher divergence but is capable of achieving larger RM scores. Notably, towards the end of the training (going to the right part of the left plot), REBEL and PPO have similar KL and RM scores. For the right plot in Figure 5, we analyze a single checkpoint for each algorithm at the end of training. For each algorithm, we group every generation from the test set by its KL distribution into \(10\) equally sized bins and calculate the average of the corresponding RM score for each bin. We can see that REBEL achieves higher RM scores for generations with small divergence while requiring larger divergence for generations with the highest scores.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(\eta\) & Winrate (\(\uparrow\)) & RM Score (\(\uparrow\)) & KL(\(\pi\|\pi_{ref}\)) (\(\downarrow\)) \\ \hline
0.3 & 55.5\% & 1.37 & 10.4 \\
0.7 & 59.9\% & 1.60 & 14.2 \\
1.0 & 70.2\% & 2.44 & 29.0 \\
2.0 & 62.5\% & 1.76 & 16.9 \\ \hline \hline \end{tabular}
\end{table}
Table 6: REBEL ablation of the key hyperparameter \(\eta\) on summarization task and 2.8B model. The best-performing \(\eta\) for each metric is highlighted in bold.

Figure 5: Plot of Reward vs KL-Divergence for 2.8B REBEL and PPO for summarization. We evaluate the models across the entire test set every 100 steps for 2,000 steps. Left: each point represents the average reward score and KL-divergence for a specific time step; the eclipse represents the confidence interval with 2 standard deviations. Right: we divide the KL distribution at the 2,000-step into \(10\) bins with equal size and average the corresponding RM scores in each bin.

## Appendix L Regression Loss During Training

Figure 6 shows the observed loss of Eq. 1 that we observed when finetuning the 6.9B Pythia model on summarization. We see that REBEL minimizes the loss throughout training maintaining a relatively low mean squared error given that our observed rewards were mostly between \([-10,10]\). Note that our learned reward model, however, is unbounded.

## Appendix M Breakdown of MT-Bench

Figure 7 shows the breakdown of MT-Bench results. REBEL (REBEL-OpenChat-3.5) outperforms both APA (Starling-LM-7B-alpha) and base (OpenChat-3.5) models on six out of eight dimensions including writing, roleplay, math, extraction, STEM, and humanities.

Figure 6: REBEL’s reward difference prediction error throughout training of our 6.9B parameter policy on the summarization task. The reward used for this task is unbounded with the range of values of the human labels in the validation set being \([-6.81,7.31]\). We plot both the smoothed values with a moving average and the loss vales at each iteration.

Figure 7: Breakdown of MT-Bench results over eight dimensions.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction (Section 1) accurately present the primary claims of the paper, aligning well with the detailed contributions and scope described in the main body. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: * The limitation of exact Bayes Optimal solution assumption in Section 3.2 is discussed in Section 4. * Justification and limitation of Assumption 1 are discussed in Appendix E. * The regret bound of REBEL is discussed in Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All of the assumptions and claims are clearly stated and cross-referenced. * Proof of Claim 1 is provided in Appendix B. * Proof of Claim 2 is provided in Appendix C. * Justification of Assumption 1 is provided in Appendix E. * Proof of Theorem 1 is provided in Appendix F. * Proof of Theorem 2 is provided in Appendix G.1. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We detail the datasets, models, hyperparameters, and evaluation metrics in Section 5 and Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide an anonymized version of data and code as supplemental materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We detail the datasets (splits), model pipelines, and complete sets of hyperparameters in Section 5 and Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: Our results for summarization and image generation are averages of three random seeds. Specifically we report the standard deviation across seeds for summarization and IQM with 95% confidence intervals for our image generation results. For general chat experiments, we train once to obtain the final result. This is consistent with how the community reports general chatbot evaluations on AlpacaEval 2.0 and Chatbot Arena / MTBench. We follow other works in reporting our checkpoint's results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Resources for experiments are discussed in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper introduces an algorithm that could potentially be applied to any reinforcement learning task. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: Datasets and models we used in experiments are cited in Section 5 and the model/dataset cards are detailed in Appendix H.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code used in the paper is well documented with instructions. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification:

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.