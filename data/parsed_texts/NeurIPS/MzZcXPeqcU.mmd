# CORL: Research-oriented Deep Offline Reinforcement Learning Library

 Denis Tarasov

Tinkoff

den.tarasov@tinkoff.ai &Alexander Nikulin

Tinkoff

a.p.nikulin@tinkoff.ai &Dmitry Akimov

Tinkoff

d.akimov@tinkoff.ai &Vladislav Kurenkov

Tinkoff

v.kurenkov@tinkoff.ai &Sergey Kolesnikov

Tinkoff

s.s.kolesnikov@tinkoff.ai

###### Abstract

CORL1 is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.

Footnote 1: CORL Repository: https://github.com/corl-team/CORL

## 1 Introduction

Deep Offline Reinforcement Learning (Levine et al., 2020) has been showing significant advancements in numerous domains such as robotics (Smith et al., 2022; Kumar et al., 2021), autonomous driving (Diehl et al., 2021) and recommender systems (Chen et al., 2022). Due to such rapid development, many open-source offline RL solutions2 emerged to help RL practitioners understand and improve well-known offline RL techniques in different fields. On the one hand, they introduce offline RL algorithms standard interfaces and user-friendly APIs, simplifying offline RL methods incorporation into _existing_ projects. On the other hand, introduced abstractions may hinder the learning curve for newcomers and the ease of adoption for researchers interested in developing _new_ algorithms. One needs to understand the modularity design (several files on average), which (1) can be comprised of thousands of lines of code or (2) can hardly fit for a novel method3.

Footnote 2: https://github.com/hanjuku-kaso/awesome-offline-rl#oss

Footnote 3: https://github.com/takuseno/d3rlpy/issues/141

In this technical report, we take a different perspective on an offline RL library and also incorporate emerging interest in the offline-to-online setup. We propose CORL (Clean Offline Reinforcement Learning) - minimalistic and isolated single-file implementations of deep offline and offline-to-online RL algorithms, supported by open-sourced D4RL (Fu et al., 2020) benchmark results. The uncomplicated design allows practitioners to read and understand the implementations of the algorithms straightforwardly. Moreover, CORL supports optional integration with experiments tracking tools such as Weighs&Biases (Biewald, 2020), providing practitioners with a convenient way to analyzethe results and behavior of all algorithms, not merely relying on a final performance commonly reported in papers.

We hope that the CORL library will help offline RL newcomers study implemented algorithms and aid the researchers in quickly modifying existing methods without fighting through different levels of abstraction. Finally, the obtained results may serve as a reference point for D4RL benchmarks avoiding the need to re-implement and tune existing algorithms' hyperparameters.

## 2 Related Work

Since the Atari breakthrough (Mnih et al., 2015), numerous open-source RL frameworks and libraries have been developed over the last years: (Dhariwal et al., 2017; Hill et al., 2018; Castro et al., 2018; Gauci et al., 2018; Keng & Graesser, 2017; garage contributors, 2019; Duan et al., 2016; Kolesnikov & Hrinchuk, 2019; Fujita et al., 2021; Liang et al., 2018; Fujita et al., 2021; Liu et al., 2021; Huang et al., 2021; Weng et al., 2021; Stooke & Abbeel, 2019), focusing on different perspectives of the RL. For example, stable-baselines (Hill et al., 2018) provides many deep RL implementations that carefully reproduce results to back up RL practitioners with reliable baselines during methods comparison. On the other hand, Ray (Liang et al., 2018) focuses on implementations scalability and production-friendly usage. Finally, more nuanced solutions exist, such as Dopamine (Castro et al., 2018), which emphasizes different DQN variants, or ReAgent (Gauci et al., 2018), which applies RL to the RecSys domain.

At the same time, the offline RL branch and especially offline-to-online, which we are interested in this paper, are not yet covered as much: the only library that precisely focuses on offline RL setting is d3rlpy (Takuma Seno, 2021). While CORL also focuses on offline RL methods (Nair et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021; Fujimoto & Gu, 2021; An et al., 2021; Chen et al., 2021), similar to d3rlpy, it takes a different perspective on library design and provides _non-modular_ independent algorithms implementations. More precisely, CORL does not introduce additional abstractions to make offline RL more general but instead gives an "easy-to-hack" starter kit for research needs. Finally, CORL also provides recent offline-to-online solutions (Nair et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021; Wu et al., 2022; Nakamoto et al., 2023; Tarasov et al., 2023) that are gaining interest among researchers and practitioners.

Figure 1: The illustration of the CORL library design. Single-file implementation takes a yaml configuration file with both environment and algorithm parameters to run the experiment, which logs all required statistics to Weights&Biases (Biewald, 2020).

Although CORL does not represent the first non-modular RL library, which is more likely the CleanRL (Huang et al., 2021) case, it has two significant differences from its predecessor. First, CORL is focused on _offline_ and _offline-to-online_ RL, while CleanRL implements _online_ RL algorithms. Second, CORL intends to minimize the complexity of the requirements and external dependencies. To be more concrete, CORL does not have additional requirements with abstractions such as \(stable\)-\(baselines\)(Hill et al., 2018) or \(envpool\)(Weng et al., 2022) but instead implements everything from scratch in the codebase.

## 3 CORL Design

### Single-File Implementations

Implementational subtleties significantly impact agent performance in deep RL (Henderson et al., 2018; Engstrom et al., 2020; Fujimoto and Gu, 2021). Unfortunately, user-friendly abstractions and general interfaces, the core idea behind modular libraries, encapsulate and often hide these important nuances from the practitioners. For such a reason, CORL unwraps these details by adopting single-file implementations. To be more concrete, we put environment details, algorithms hyperparameters, and evaluation parameters into a single file4. For example, we provide

Footnote 4: We follow the PEP8 style guide with a maximum line length of 89, which increases LOC a bit.

* \(any\_percent\_bc.py\) (404 LOC5) as a baseline algorithm for offline RL methods comparison,
* \(td3\_bc.py\) (511 LOC) as a competitive minimalistic offline RL algorithm (Fujimoto and Gu, 2021),
* \(dt.py\) (540 LOC) as an example of the recently proposed trajectory optimization approach (Chen et al., 2021)

Footnote 5: Lines Of Code

Figure 1 depicts an overall library design. To avoid over-complicated offline implementations, we treat offline and offline-to-online versions of the same algorithms separately. While such design produces code duplications among realization, it has several essential benefits from the both educational and research perspective:

* **Smooth learning curve**. Having the entire code in one place makes understanding all its aspects more straightforward. In other words, one may find it easier to dive into 540 LOC of single-file Decision Transformer (Chen et al., 2021) implementation rather than 10+ files of the original implementation6. Footnote 6: Original Decision Transformer implementation: https://github.com/kzl/decision-transformer
* **Simple prototyping**. As we are not interested in the code's general applicability, we could make it implementation-specific. Such a design also removes the need for inheritance from general primitives or their refactoring, reducing abstraction overhead to zero. At the same time, this idea gives us complete freedom during code modification.
* **Faster debugging**. Without additional abstractions, implementation simplifies to a single for-loop with a global Python name scope. Furthermore, such flat architecture makes accessing and inspecting any created variable easier during training, which is crucial in the presence of modifications and debugging.

### Configuration files

Although it is a typical pattern to use a command line interface (CLI) for single-file experiments in the research community, CORL slightly improves it with predefined configuration files. Utilizing YAML parsing through CLI, for each experiment, we gather all environment and algorithm hyperparameters into such files so that one can use them as an initial setup. We found that such setup (1) simplifies experiments, eliminating the need to keep all algorithm-environment-specific parameters in mind, and (2) keeps it convenient with the familiar CLI approach.

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

AWAC, initially proposed for finetuning purposes, appeared to be the worst of the considered algorithms, where the score is improved only on the most straightforward antmaze-umaze-v2 dataset. At the same time, on other datasets, performances either stay the same or even drop.

**Observation 5**: AWAC does not benefit from online tuning on the considered tasks.

Cal-QL was proposed as a modification of CQL, which is expected to work better in offline-to-online setting. However, in our experiments, after finetuning CQL obtained scores which are not very different from Cal-QL. At the same time, we could not make both algorithms solve Adroit tasks13.

Footnote 13: The issues are Observations 3 and 4. Additional hyperparameters search is needed.

**Observation 6**: There is no big difference between CQL and Cal-QL. On AntMaze, these algorithms perform the best but work poorly on Adroit.

IQL starts with good offline scores on AntMaze, but it is less efficient in finetuning than other algorithms except for AWAC. At the same time, IQL and ReBRAC are the only algorithms that notably improve its scores after tuning on Adroit tasks, making them the most competitive offline-to-online baselines considering the average score.

\begin{table}
\begin{tabular}{l|r r r r r r} \hline \hline
**Task Name** & **AWAC** & **CQL** & **IQL** & **SPOT** & **Cal-QL** & **ReBRAC** \\ \hline antmaze-umaze-v2 & 0.04 \(\pm\) 0.01 & 0.02 \(\pm\) 0.00 & 0.07 \(\pm\) 0.00 & 0.02 \(\pm\) 0.00 & **0.01**\(\pm\) 0.00 & 0.10 \(\pm\) 0.20 \\ antmaze-umaze-diverse-v2 & 0.88 \(\pm\) 0.01 & 0.09 \(\pm\) 0.01 & 0.43 \(\pm\) 0.11 & 0.22 \(\pm\) 0.07 & **0.05**\(\pm\) 0.01 & 0.04 \(\pm\) 0.02 \\ antmaze-medium-play-v2 & 1.00 \(\pm\) 0.00 & 0.08 \(\pm\) 0.01 & 0.09 \(\pm\) 0.01 & 0.06 \(\pm\) 0.00 & **0.04**\(\pm\) 0.01 & 0.02 \(\pm\) 0.00 \\ antmaze-medium-diverse-v2 & 1.00 \(\pm\) 0.00 & 0.08 \(\pm\) 0.00 & 0.10 \(\pm\) 0.01 & 0.05 \(\pm\) 0.01 & **0.04**\(\pm\) 0.01 & 0.03 \(\pm\) 0.00 \\ antmaze-large-play-v2 & 1.00 \(\pm\) 0.00 & 0.21 \(\pm\) 0.02 & 0.34 \(\pm\) 0.05 & 0.29 \(\pm\) 0.07 & **0.13**\(\pm\) 0.02 & 0.14 \(\pm\) 0.05 \\ antmaze-large-diverse-v2 & 1.00 \(\pm\) 0.00 & 0.21 \(\pm\) 0.03 & 0.41 \(\pm\) 0.03 & 0.23 \(\pm\) 0.08 & **0.13**\(\pm\) 0.02 & 0.29 \(\pm\) 0.45 \\ \hline
**AntMaze avg** & 0.82 & 0.11 & 0.24 & 0.15 & **0.07** & 0.10 \\ \hline pen-cloned-v1 & 0.46 \(\pm\) 0.02 & 0.97 \(\pm\) 0.00 & **0.37**\(\pm\) 0.01 & 0.58 \(\pm\) 0.02 & 0.98 \(\pm\) 0.01 & 0.08 \(\pm\) 0.01 \\ door-cloned-v1 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 & **0.83**\(\pm\) 0.03 & 0.99 \(\pm\) 0.01 & 1.00 \(\pm\) 0.00 & 0.18 \(\pm\) 0.06 \\ hammer-cloned-v1 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 & **0.65**\(\pm\) 0.10 & 0.98 \(\pm\) 0.01 & 1.00 \(\pm\) 0.00 & 0.12 \(\pm\) 0.03 \\ relocate-cloned-v1 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 & 0.9 \(\pm\) 0.06 \\ \hline
**Adroit avg** & 0.86 & 0.99 & **0.71** & 0.89 & 0.99 & 0.32 \\ \hline
**Total avg** & 0.84 & 0.47 & **0.43** & 0.44 & 0.44 & 0.19 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Cumulative regret of online finetuning calculated as \(1-\)_average success rate_ averaged over 4 random seeds.

Figure 3: (a) Performance profiles after online tuning (b) Probability of improvement of ReBRAC to other algorithms after online tuning. The curves (Agarwal et al., 2021) are for D4RL benchmark spanning AntMaze and Adroit cloned datasets.

**Observation 7**: Considering offline and offline-to-online results, IQL and ReBRAC appear to be the strongest baselines on average.

## 5 Conclusion

This paper introduced CORL, a single-file implementation library for offline and offline-to-online reinforcement learning algorithms with configuration files and advanced metrics tracking support. In total, we provided implementations of ten offline and six offline-to-online algorithms. All implemented approaches were benchmarked on D4RL datasets, closely matching (sometimes overperforming) the reference results, if available. Focusing on implementation clarity and reproducibility, we hope that CORL will help RL practitioners in their research and applications.

This study's benchmarking results and observations are intended to serve as references for future offline reinforcement learning research and its practical applications. By sharing comprehensive logs, researchers can readily access and utilize our results without having to re-run any of our experiments, ensuring that the results are replicable.

## References

* Abadi et al. (2015) Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
* Agarwal et al. (2021) Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in Neural Information Processing Systems_, 2021.
* An et al. (2021) Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. _Advances in neural information processing systems_, 34:7436-7447, 2021.
* Arakelyan et al. (2020) Gor Arakelyan, Gevorg Soghomonyan, and The Aim team. Aim, 6 2020. URL https://github.com/aimhubio/aim.
* Biewald (2020) Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/. Software available from wandb.com.
* Castro et al. (2018) Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Bellemare. Dopamine: A research framework for deep reinforcement learning. _arXiv preprint arXiv:1812.06110_, 2018.
* Chen et al. (2021) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* Chen et al. (2022) Minmin Chen, Can Xu, Vince Gatto, Devanshu Jain, Aviral Kumar, and Ed H. Chi. Off-policy actor-critic for recommender systems. _Proceedings of the 16th ACM Conference on Recommender Systems_, 2022.
* Dhariwal et al. (2017) Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.
* Diehl et al. (2021) Christopher P. Diehl, Timo Sievernich, Martin Kruger, Frank Hoffmann, and Torsten Bertram. Umbrella: Uncertainty-aware model-based offline reinforcement learning leveraging planning. _ArXiv_, abs/2111.11097, 2021.
* Du et al. (2020)Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In _Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48_, ICML'16, pp. 1329-1338. JMLR.org, 2016.
* Engstrom et al. (2020) Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, L. Rudolph, and Aleksander Madry. Implementation matters in deep rl: A case study on ppo and trpo. In _ICLR_, 2020.
* Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* Fujimoto and Gu (2021) Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* Fujita et al. (2021) Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. Chainerrl: A deep reinforcement learning library. _Journal of Machine Learning Research_, 22(77):1-14, 2021. URL http://jmlr.org/papers/v22/20-376.html.
* (2019) The garage contributors. Garage: A toolkit for reproducible reinforcement learning research. https://github.com/rlworkgroup/garage, 2019.
* Gauci et al. (2018) Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Zhengxing Chen, Yuchen He, Zachary Kaden, Vivek Narayanan, and Xiaohui Ye. Horizon: Facebook's open source applied reinforcement learning platform. _arXiv preprint arXiv:1811.00260_, 2018.
* Ghasemipour et al. (2022) Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters. _Advances in Neural Information Processing Systems_, 35:18267-18281, 2022.
* Henderson et al. (2018) Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, AAAI'18/IAAI'18/EAAI'18. AAAI Press, 2018. ISBN 978-1-57735-800-8.
* Hill et al. (2018) Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.com/hill-a/stable-baselines, 2018.
* Huang et al. (2021) Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, and Jeff Braga. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. _arXiv preprint arXiv:2111.08819_, 2021.
* Keng and Graesser (2017) Wah Loon Keng and Laura Graesser. Slm lab. https://github.com/kengz/SLM-Lab, 2017.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kolesnikov and Hrinchuk (2019) Sergey Kolesnikov and Oleksii Hrinchuk. Catalyst.rl: A distributed framework for reproducible rl research, 2019. URL https://arxiv.org/abs/1903.00027.
* Kostrikov et al. (2021) Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.
* Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Kumar et al. (2021) Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline model-free robotic reinforcement learning. In _5th Annual Conference on Robot Learning_, 2021. URL https://openreview.net/forum?id=fy4ZBWxYbIo.

Vladislav Kurenkov and Sergey Kolesnikov. Showing your offline reinforcement learning work: Online evaluation budget matters. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 11729-11752. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/kurenkov22a.html.
* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Liang et al. (2018) Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2018.
* Liu et al. (2021) Xiao-Yang Liu, Zechu Li, Zhaoran Wang, and Jiahao Zheng. ElegantRL: Massively parallel framework for cloud-native deep reinforcement learning. https://github.com/A14Finance-Foundation/ElegantRL, 2021.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumar, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, February 2015. ISSN 1476-4687. doi: 10.1038/nature14236.
* Nair et al. (2020) Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* Nakamoto et al. (2023) Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. _arXiv preprint arXiv:2303.05479_, 2023.
* Smith et al. (2022) Laura Smith, Ilya Kostrikov, and Sergey Levine. A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning, August 2022.
* Stooke and Abbeel (2019) Adam Stooke and Pieter Abbeel. rlpyt: A research code base for deep reinforcement learning in pytorch, 2019. URL https://arxiv.org/abs/1909.01500.
* Seno (2021) Michita Imai Takuma Seno. \(\mathrm{d}3\)rlpy: An offline deep reinforcement library. In _NeurIPS 2021 Offline Reinforcement Learning Workshop_, December 2021.
* Tarasov et al. (2023) Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the minimalist approach to offline reinforcement learning. _arXiv preprint arXiv:2305.09836_, 2023.
* Weng et al. (2021) Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. _arXiv preprint arXiv:2107.14171_, 2021.
* Weng et al. (2022) Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, Zhongwen Xu, and Shuicheng Yan. EnvPool: A highly parallel reinforcement learning environment execution engine. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), _Advances in Neural Information Processing Systems_, volume 35, pp. 22409-22421. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/8caaf08e49ddbad6694fae067442ee21-Paper-Datasets_and_Benchmarks.pdf.
* Wu et al. (2022) Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. _arXiv preprint arXiv:2202.06239_, 2022.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See section 3 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We release our codebase, configs, and in-depth reports at https://github.com/tinkoff-ai/CORL 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix D 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix D
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] See Appendix C 3. Did you include any new assets either in the supplemental material or as a URL? [N/A] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]

[MISSING_PAGE_EMPTY:12]

Figure 5: Graphical representation of the normalized performance of the best trained policy on D4RL averaged over 4 random seeds. (a) Gym-MuJoCo datasets. (b) Maze2d datasets (c) AntMaze datasets (d) Adroit datasets

Figure 8: Training curves for Walker2d task.

(a) Medium dataset, (b) Medium-expert dataset, (c) Medium-replay dataset

Figure 6: Training curves for HalfCheetah task.

(a) Medium dataset, (b) Medium-expert dataset, (c) Medium-replay dataset

Figure 7: Training curves for Hopper task.

(a) Medium dataset, (b) Medium-expert dataset, (c) Medium-replay datasetFigure 11: Training curves for Pen task.

(a) Human dataset, (b) Colned dataset, (c) Expert dataset

Figure 12: Training curves for Door task.

(a) Human dataset, (b) Colned dataset, (c) Expert dataset

Figure 10: Training curves for AntMaze task.

(a) Umaze dataset, (b) Medium-play dataset, (c) Large-play dataset, (d) Umaze-diverse dataset, (e) Medium-diverse dataset, (f) Large-diverse dataset

Figure 9: Training curves for Maze2d task.

(a) Medium dataset, (b) Medium-expert dataset, (c) Medium-replay datasetFigure 14: Training curves for Relocate task.

(a) Human dataset, (b) Colned dataset, (c) Expert dataset

Figure 13: Training curves for Hammer task.

(a) Human dataset, (b) Colned dataset, (c) Expert dataset

### Offline-to-online

Figure 16: Training curves for AntMaze task during online tuning.

(a) Umaze dataset, (b) Medium-play dataset, (c) Large-play dataset, (d) Umaze-diverse dataset, (e) Medium-diverse dataset, (f) Large-diverse dataset

Figure 15: Graphical representation of the normalized performance of the last trained policy on D4RL after online tuning averaged over 4 random seeds.

(a) AntMaze datasets (b) Adroit datasetsFigure 17: Training curves for Adroit Cloned task during online tuning.

(a) Pen, (b) Door, (c) Hammer, (d) Relocate

## Appendix B Weights&Biases Tracking

## Appendix C License

Our codebase is released under Apache License 2.0. The D4RL datasets (Fu et al., 2020) are released under Apache License 2.0.

Figure 18: Screenshots of Weights&Biases experiment tracking interface.

Experimental Details

We modify reward on AntMaze task by subtracting \(1\) from reward as it is done in previous works except CQL and Cal-QL, where (0, 1) are mapped into (-5, 5).

We used original implementation of TD3 + BC14, SAC-\(N\)/EDAC15, SPOT16, ReBRAC17 and custom implementations of IQL18 and CQL/Cal-QL19 as the basis for ours.

Footnote 14: https://github.com/sfujim/TD3_BC

Footnote 15: https://github.com/snu-mllab/EDAC

Footnote 16: https://github.com/thuml/SPOT

Footnote 17: https://github.com/tinkoff-ai/ReBRAC

Footnote 18: https://github.com/gwhomas/IQL-PyTorch

Footnote 19: https://github.com/young-geng/CQL

For most of the algorithms and datasets, we use default hyperparameters if available. Configuration files for every algorithm and environment are presented in our GitHub repository. Hyperparameters are also provided in subsection D.2.

All the experiments ran using V100 and A100 GPUs, which took approximately 5000 hours of compute in total.

### Number of update steps and evaluation rate

Following original work, SAC-\(N\) and EDAC are trained for 3 million steps (except AntMaze, which is trained for 1 million steps) in order to obtain state-of-the-art performance and tested every 10000 steps. Decision Transformer (DT) training is splitted into datasets pass epochs. We train DT for 50 epochs on each dataset and evaluate every 5 epochs. All other algorithms are trained for 1 million steps and evaluated every 5000 steps (50000 for AntMaze). We evaluate every policy for 10 episodes on Gym-MuJoCo and Adroit tasks and for 100 for Maze2d and AntMaze tasks.

### Hyperparameters

\begin{table}
\begin{tabular}{c l l} \hline \hline  & Hyperparameter & Value \\ \hline \multirow{3}{*}{BC hyperparameters} & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Learning Rate & 3e-4 \\  & Mini-batch size & 256 \\ \hline \multirow{3}{*}{Architecture} & Policy hidden dim & 256 \\  & Policy hidden layers & 2 \\  & Policy activation function & ReLU \\ \hline \multirow{3}{*}{BC-\(N\%\) hyperparameters} & Ratio of best trajectories used & 0.1 \\  & Discount factor\({}^{\dagger}\) & 1.0 \\ \cline{1-1}  & Max trajectory length\({}^{\dagger}\) & 1000 \\ \hline \hline \end{tabular}
\end{table}
Table 5: BC and BC-\(N\%\) hyperparameters. \(\dagger\) used for the best trajectories choice.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Hyperparameter & Value \\ \hline \multirow{8}{*}{SAC hyperparameters} & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Critic learning rate & 3e-4 \\  & Mini-batch size & 256 \\  & Discount factor & 0.99 \\  & Target update rate & 5e-3 \\  & Policy noise & 0.2 \\  & Policy noise clipping & (-0.5, 0.5) \\  & Policy update frequency & 2 \\ \hline \multirow{8}{*}{Architecture} & Critic hidden dim & 256 \\  & Critic hidden layers & 2 \\ \cline{1-1}  & Critic activation function & ReLU \\ \cline{1-1}  & Actor hidden dim & 256 \\ \cline{1-1}  & Actor hidden layers & 2 \\ \cline{1-1}  & Actor activation function & ReLU \\ \hline TD3+BC hyperparameters & \(\alpha\) & 2.5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: TD3+BC hyperparameters.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Hyperparameter & Value \\ \hline \multirow{8}{*}{SAC hyperparameters} & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Critic learning rate & 3e-4 \\  & Critic learning rate & 256 \\  & Mini-batch size & 256 \\  & Discount factor & 0.99 \\  & Target update rate & 5e-3 \\  & Target entropy & -1 \(\cdot\) Action Dim \\  & Entropy in Q target & False \\ \hline \multirow{8}{*}{Architecture} & Critic hidden dim & 256 \\  & Critic hidden layers & 5, AntMaze \\ \cline{1-1}  & Critic activation function & ReLU \\ \cline{1-1}  & Actor hidden dim & 256 \\ \cline{1-1}  & Actor hidden layers & 3 \\ \cline{1-1}  & Actor activation function & ReLU \\ \hline \multirow{8}{*}{CQL hyperparameters} & Lagrange & True, Maze2d and AntMaze \\ \cline{1-1}  & Offline \(\alpha\) & 1.0, Adroit \\ \cline{1-1}  & & 5.0, AntMaze \\ \cline{1-1}  & & 10.0, otherwise \\ \cline{1-1}  & Lagrange gap & 5, Maze2d \\ \cline{1-1}  & & 0.8, AntMaze \\ \cline{1-1}  & Pre-training steps & 0 \\ \cline{1-1}  & Num sampled actions (during eval) & 10 \\ \cline{1-1}  & Num sampled actions (logsumexp) & 10 \\ \hline \multirow{4}{*}{Cal-QL hyperparameters} & Mixing ratio & 0.5 \\ \cline{1-1}  & Online \(\alpha\) & 1.0, Adroit \\ \cline{1-1}  & & 5.0, AntMaze \\ \hline \hline \end{tabular}
\end{table}
Table 7: CQL and Cal-QL hyperparameters. Note: used hyperparameters are suboptimal on Adroit for the implementation we provide.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Hyperparameter & Value \\ \hline  & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Critic learning rate & 3e-4 \\  & Actor learning rate & 3e-4 \\  & Value learning rate & 3e-4 \\  & Mini-batch size & 256 \\  & Discount factor & 0.99 \\  & Target update rate & 5e-3 \\  & Learning rate decay & Cosine \\  & Deterministic policy & True, Hopper Medium and Medium-replay \\  & & False, otherwise \\  & \(\beta\) & 6.0, Hopper Medium-expert \\  & & 10.0, AntMaze \\  & & 3.0, otherwise \\  & \(\tau\) & 0.9, AntMaze \\  & & 0.5, Hopper Medium-expert \\  & & 0.7, otherwise \\ \hline  & Critic hidden dim & 256 \\  & Critic hidden layers & 2 \\  & Critic activation function & ReLU \\  & Actor hidden dim & 256 \\  & Actor hidden layers & 2 \\  & Actor activation function & ReLU \\  & Value hidden dim & 256 \\  & Value activation function & ReLU \\ \hline \hline \end{tabular}
\end{table}
Table 8: IQL hyperparameters.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Hyperparameter & Value \\ \hline  & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Critic learning rate & 3e-4 \\  & Actor learning rate & 3e-4 \\  & Mini-batch size & 256 \\  & Mini-batch size & 0.99 \\  & Target update rate & 5e-3 \\  & Target update rate & 5e-3 \\  & Learning rate decay & Cosine \\  & Deterministic policy & True, Hopper Medium and Medium-replay \\  & & False, otherwise \\  & \(\beta\) & 6.0, Hopper Medium-expert \\  & & 10.0, AntMaze \\  & & 3.0, otherwise \\  & \(\tau\) & 0.9, AntMaze \\  & & 0.5, Hopper Medium-expert \\  & & 0.7, otherwise \\ \hline  & Critic hidden dim & 256 \\  & Critic hidden layers & 2 \\  & Critic activation function & ReLU \\  & Actor hidden dim & 256 \\  & Actor activation function & ReLU \\  & Value hidden dim & 256 \\  & Value hidden layers & 2 \\  & Value activation function & ReLU \\ \hline \hline \end{tabular}
\end{table}
Table 9: AWAC hyperparameters.

\begin{table}
\begin{tabular}{l l l} \hline  & Hyperparameter & Value \\ \hline  & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Critic learning rate & 3e-4 \\  & Actor learning rate & 3e-4 \\ SAC hyperparameters & \(\alpha\) learning rate & 3e-4 \\  & Mini-batch size & 256 \\  & Discount factor & 0.99 \\  & Target update rate & 5e-3 \\  & Target entropy & -1 \(\cdot\) Action Dim \\ \hline  & Critic hidden dim & 256 \\  & Critic hidden layers & 3 \\  & Critic activation function & ReLU \\ Architecture & Actor hidden dim & 256 \\  & Actor hidden layers & 3 \\  & Actor activation function & ReLU \\ \hline  & Number of critics & 10, HalfCheetah \\  & & 20, Walker2d \\ SAC-N hyperparameters & & 25, AntMaze \\  & & 200, Hopper Medium-expert, Medium-replay \\  & & 500, Hopper Medium \\ \hline  & Number of critics & 10, HalfCheetah \\  & & 10, Walker2d, AntMaze \\ EDAC hyperparameters & & 50, Hopper \\  & \(\mu\) & 5.0, HalfCheetah Medium-expert, Walker2d Medium-expert \\  & & 1.0, otherwise \\ \hline \end{tabular}
\end{table}
Table 10: SAC-\(N\) and EDAC hyperparameters.

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & Value \\ \hline Optimizer & AdamW (Loshchilov \& Hutter, 2017) \\ Batch size & \(256\), AntMaze \\  & \(4096\), otherwise \\ Return-to-go conditioning & (12000, 6000), HalfCheetah \\  & (3600, 1800), Hopper \\  & (5000, 2500), Walker2d \\  & (160, 80), Maze2d umaze \\  & (280, 140), Maze2d medium and large \\  & (1, 0.5), AntMaze \\  & (3100, 1550), Pen \\ DT hyperparameters & (2900, 1450), Door \\  & (12800, 6400), Hammer \\  & (4300, 2150), Relocate \\ Reward scale & 1.0, AntMaze \\  & 0.001, otherwise \\ Dropout & 0.1 \\ Learning rate & 0.0008 \\ Adam betas & (0.9, 0.999) \\ Clip grad norm & 0.25 \\ Weight decay & 0.0003 \\ Total gradient steps & 100000 \\ Linear warmup steps & 10000 \\ \hline \multirow{3}{*}{Architecture} & Number of layers & 3 \\  & Number of attention heads & 1 \\ \cline{1-1}  & Embedding dimension & 128 \\ \cline{1-1}  & Activation function & GELU \\ \hline \hline \end{tabular}
\end{table}
Table 11: DT hyperparameters.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Hyperparameter & Value \\ \hline \multirow{6}{*}{VAE hyperparameters} & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Learning rate & 1e-3 \\  & Mini-batch size & 256 \\  & Number of iterations & \(10^{5}\) \\  & KL term weight & 0.5 \\  & Encoder hidden dim & 750 \\  & Encoder layers & 3 \\  & Latent dim & 2 \(\times\) action dim \\  & Decoder hidden dim & 750 \\  & Decoder layers & 3 \\  & Optimizer & Adam (Kingma \& Ba, 2014) \\  & Critic learning rate & 3e-4 \\  & Actor learning rate & 1e-4 \\  & Mini-batch size & 256 \\  & Discount factor & 0.99 \\  & Target update rate & 5e-3 \\  & Policy noise & 0.2 \\  & Policy noise clipping & (-0.5, 0.5) \\  & Policy update frequency & 2 \\ \hline \multirow{6}{*}{Architecture} & Critic hidden dim & 256 \\  & Critic hidden layers & 2 \\  & Critic activation function & ReLU \\  & Actor hidden dim & 256 \\  & Actor hidden layers & 2 \\  & Actor activation function & ReLU \\ \hline \multirow{2}{*}{SPOT hyperparameters} & \(\lambda\) & 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, AntMaze \\  & & 1.0, Adroit \\ \hline \hline \end{tabular}
\end{table}
Table 12: SPOT hyperparameters.