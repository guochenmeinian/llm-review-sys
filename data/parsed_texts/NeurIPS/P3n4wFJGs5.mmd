# Window-Based Distribution Shift Detection for Deep Neural Networks

 Guy Bar-Shalom

Department of Computer Science

Technion - Israel Institute of Technology

guy.b@cs.technion.ac.il

Yonatan Geifman

Deci.AI

yonatan@deci.ai

Ran El-Yaniv

Department of Computer Science

Technion - Israel Institute of Technology, Deci.AI

rani@cs.technion.ac.il

###### Abstract

To deploy and operate deep neural models in production, the quality of their predictions, which might be contaminated benignly or manipulated maliciously by input distributional deviations, must be monitored and assessed. Specifically, we study the case of monitoring the healthy operation of a deep neural network (DNN) receiving a stream of data, with the aim of detecting input distributional deviations over which the quality of the network's predictions is potentially damaged. Using selective prediction principles, we propose a distribution deviation detection method for DNNs. The proposed method is derived from a tight coverage generalization bound computed over a sample of instances drawn from the true underlying distribution. Based on this bound, our detector continuously monitors the operation of the network over a test window and fires off an alarm whenever a deviation is detected. Our novel detection method performs on-par or better than the state-of-the-art, while consuming substantially lower computation time (five orders of magnitude reduction) and space complexity. Unlike previous methods, which require at least linear dependence on the size of the source distribution for each detection, rendering them inapplicable to "Google-Scale" datasets, our approach eliminates this dependence, making it suitable for real-world applications. Code is available at https://github.com/BarSGuy/Window-Based-Distribution-Shift-Detection.

## 1 Introduction

A wide range of artificial intelligence applications and services rely on deep neural models because of their remarkable accuracy. When a trained model is deployed in production, its operation should be monitored for abnormal behavior, and a flag should be raised if such is detected. Corrective measures can be taken if the underlying cause of the abnormal behavior is identified. For example, simple distributional changes may only require retraining with fresh data, while more severe cases may require redesigning the model (e.g., when new classes emerge).

In this paper we focus on distribution shift detection in the context of deep neural models and consider the following setting. Pretrained model \(f\) is given, and we presume it was trained with data sampled from some distribution \(P\). In addition to the dataset used in training \(f\), we are also given an additional unlabeled sample of data from the same distribution, which is used to train a detector \(D\) (we refer to this as the detection-training set or source set). While \(f\) is used in production to process a stream of emerging input data, we continually feed \(D\) with the most recent window \(W_{k}\) of \(k\) input elements.

The detector also has access to the final layers of the model \(f\) and should be able to determine whether the data contained in \(W_{k}\) came from a distribution different from \(P\). We emphasize that in this paper we are not considering the problem of identifying _single-instance_ out-of-distribution or outlier instances [28, 21, 22, 16, 38, 35, 34, 11], but rather the information residing in a population of \(k\) instances. While it may seem straightforward to apply single-instance detectors to a window (by applying the detector to each instance in the window), this approach can be computationally expensive since such methods are not designed for window-based tasks; see discussion in Section 3. Moreover, we demonstrate here that _computationally feasible_ single-instance methods can fail to detect population-based deviations. We emphasize that we are not concerned in characterizing types of distribution shifts, nor do we tackle at all the complementary topic of out-of-distribution robustness.

Distribution shift detection has been scarcely considered in the context of deep neural networks (DNNs), however, it is a fundamental topic in machine learning and statistics. The standard method for tackling it is by performing a dimensionality reduction over both the detection-training (source) and test (target) samples, and then applying a two-sample statistical test over these reduced representations to detect a deviation. This is further discussed in Section 3. In particular, deep models can benefit from the semantic representation created by the model itself, which provides meaningful dimensionality reduction, that is readily available at the last layers of the model. Using the embedding layer (or softmax) along with statistical two-sample tests was recently proposed by Lipton et al. [29] and Rabanser et al. [37] who termed solutions of this structure black-box shift detection (BBSD). Using both the univariate Kolmogorov-Smirnov (KS) test and the maximum mean discrepancy (MMD) method, see details below, Rabanser et al. [37] achieve impressive detection results when using MNIST and CIFAR-10 as proxies for the distribution \(\mathcal{P}\). As shown here, the KS test is also very effective over ImageNet when a stronger model is used (ResNet50 vs ResNet18). However, BBSD methods have the disadvantage of being computationally intensive (and probably inapplicable to read-world datasets) due to the use of two-sample tests between the detection-training set (which can, and is preferred to be the largest possible) and the window \(W_{k}\); a complexity analysis is provided in Section 5.

We propose a different approach termed _Coverage-Based Detection_ (CBD), which builds upon selective prediction principles [9, 14]. In this approach, a model quantifies its prediction uncertainty and abstains from predicting uncertain instances. First, we develop a method for selective prediction with guaranteed coverage. This method identifies the best abstaining threshold and coverage bound for a given pretrained classifier \(f\), such that the resulting empirical coverage will not violate the bound with high probability (when abstention is determined using the threshold). The guaranteed coverage method is of independent interest, and it is analogous to selective prediction with guaranteed risk [14]. Because the empirical coverage of such a classifier is highly unlikely to violate the bound if the underlying distribution remains the same, a systematic violation indicates a distribution shift. To be more specific, given a detection-training sample \(S_{m}\), our coverage-based detection algorithm computes a fixed number of tight generalization coverage bounds, which are then used to detect a distribution shift in a window \(W_{k}\) of test data. The proposed detection algorithm exhibits remarkable computational efficiency due to its ability to operate independently of the size of \(S_{m}\) during detection, which is crucial when considering "Google-Scale" datasets, such as the JFT-3B dataset. In contrast, the currently available distribution shift detectors rely on a framework that requires significantly higher computational requirements (this framework is illustrated in Figure 3 in Appendix A). A run-time comparison of those baselines w.r.t. our CBD method is provided in Figure 1.

In a comprehensive empirical study, we compared our CBD algorithm with the best-performing baselines, including the KS approach of [37]. Additionally, we investigated the suitability of single-instance detection methods for identifying distribution shifts. For a fair comparison, all methods used the same (publicly available) underlying models: ResNet50, MobileNetV3-S, and ViT-T. To evaluate the effectiveness of our approach, we employed the ImageNet dataset to simulate the source distribution. We then introduced distribution shifts using a range of methods, starting with simple noise and progressing to more sophisticated techniques such as adversarial examples. Based on these experiments, it is evident that CBD is overall significantly more powerful than the baselines across a wide range of test window sizes.

To summarize, the contributions of this paper are: (1) A theoretically justified algorithm (Algorithm 1), that produces a coverage bound, which is of independent interest, and allows for the creation of selective classifiers with guaranteed coverage. (2) A theoretically motivated "windowed" detection algorithm, CBD (Algorithm 2), which detects a distribution shift over a window; this proposed algorithm exhibits remarkable efficiency compared to state-of-the-art methods (five orders of magnitude better than the best method). (3) A comprehensive empirical study demonstrating significant improvements relative to existing baselines, and introducing the use of single-instance methods for detecting distribution shifts.

## 2 Problem Formulation

We consider the problem of detecting distribution shifts in input streams provided to pretrained deep neural models. Let \(\mathcal{P}\triangleq P_{X}\) denote a probability distribution over an input space \(\mathcal{X}\), and assume that a model \(f\) has been trained on a set of instances drawn from \(\mathcal{P}\). Consider a setting where the model \(f\) is deployed and while being used in production its input distribution might change or even be attacked by an adversary. Our goal is to detect such events to allow for appropriate action, e.g., retraining the model with respect to the revised distribution.

Inspired by Rabanser et al. [37], we formulate this problem as follows. We are given a pretrained model \(f\), and a detection-training set, \(S_{m}\sim\mathcal{P}^{m}\). Then we would like to train a detection model to be able to detect a distribution shift; namely, discriminate between windows containing in-distribution (ID) data, and _alternative-distribution_ (AD) data. Thus, given an unlabeled test sample window \(W_{k}\sim Q^{k}\), where \(Q\) is a possibly different distribution, the objective is to determine whether \(\mathcal{P}\neq Q\). We also ask what is the smallest test sample size \(k\) required to determine that \(\mathcal{P}\neq Q\). Since typically the detection-training set \(S_{m}\) can be quite large, we further ask whether it is possible to devise an effective detection procedure whose time complexity is \(o(m)\).

## 3 Related Work

Distribution shift detection methods often comprise the following two steps: dimensionality reduction, and a two-sample test between the detection-training sample and test samples. In most cases, these methods are "lazy" in the sense that for each test sample, they make a detection decision based on a computation over the entire detection-training sample. Their performance will be sub-optimal if only a subset of the train sample is used. Figure 3 in Appendix A illustrates this general framework.

The use of dimensionality reduction is optional. It can often improve performance by focusing on a less noisy representation of the data. Dimensionality reduction techniques include no reduction, _principal components analysis_[1], _sparse random projection_[2], _autoencoders_[39; 36], _domain classifiers_, [37] and more. In this work we focus on _black box shift detection_ (BBSD) methods [29], that rely on deep neural representations of the data generated by a pretrained model. The representation extracted from the model will typically utilize either the softmax outputs, acronymed BBSD-Softmax, or the embeddings, acronymed BBSD-Embeddings; for simplicity, we may omit the BBSD acronym. Due to the dimensionality of the final representation, multivariate or multiple univariate two-sample tests can be conducted.

By combining BBSD-Softmax with a Kolmogorov-Smirnov (KS) statistical test [33] and using the Bonferroni correction [3], Rabanser et al. [37] achieved state-of-the-art results in distribution shift detection in the context of image classification (MNIST and CIFAR-10). We acronym their method as BBSD-KS-Softmax (or KS-Softmax). The _univariate_ KS test processes individual dimensions separately; its statistic is calculated by computing the largest difference \(Z\) of the _cumulative density functions_ (CDFs) across all dimensions as follows: \(Z=\sup_{\bm{z}}|F_{\mathcal{P}}(\bm{z})-F_{Q}(\bm{z})|\), where \(F_{Q}\) and \(F_{\mathcal{P}}\) are the empirical CDFs of the detection-training and test data (which are sampled from \(\mathcal{P}\) and \(Q\), respectively; see Section 2). The Bonferroni correction rejects the null hypothesis when the minimal p-value among all tests is less than \(\frac{\alpha}{d}\), where \(\alpha\) is the significance level and \(d\) is the number of dimensions. Although less conservative approaches to aggregation exist [20; 30], they usually assume some dependencies among the tests. The _maximum mean discrepancy_ (MMD) method [18] is a kernel-based multivariate test that can be used to distinguish between probability distributions \(\mathcal{P}\) and \(Q\). Formally, \(MMD^{2}(\mathcal{F},\mathcal{P},Q)=|\bm{\mu}_{\mathcal{P}}-\bm{\mu}_{Q}||_{ \mathcal{F}_{2}}^{2}\), where \(\bm{\mu}_{\mathcal{P}}\) and \(\bm{\mu}_{Q}\) are the mean embeddings of \(\mathcal{P}\) and \(Q\) in a reproducing kernel Hilbert space \(\mathcal{F}\). Given a kernel \(\mathcal{K}\), and samples, \(\{x_{1},x_{2},\ldots,x_{m}\}\sim\mathcal{P}^{m}\) and \(\{x_{1}^{\prime},x_{2}^{\prime},\ldots,x_{k}^{\prime}\}\sim Q^{k}\), an unbiased estimator for \(MMD^{2}\) can be found in [18; 42]. Sutherland et al. [43] and Gretton et al. [18] used the RBF kernel \(\mathcal{K}(x,x^{\prime})=e^{-\frac{1}{2\sigma^{2}}\|x-x^{\prime}\|_{2}^{2}}\), where \(2\sigma^{2}\) is set to the median of the pairwise Euclidean distancesbetween all samples. By performing a permutation test on the kernel matrix, the p-value is obtained. In our experiments (see Section 6.4), we thus use four population based baselines: **KS-Softmax**, **KS-Embeddings**, **MMD-Softmax**, and **MMD-Embeddings**.

As mentioned in the introduction, our work is complementary to the topic of single-instance out-of-distribution (OOD) detection [28; 21; 22; 16; 38; 35; 34; 11]. Although these methods can be applied to each instance in a window, they often fail to capture population statistics within the window, making them inadequate for detecting population-based changes. Additionally, many of these methods are computationally expensive and cannot be applied efficiently to large windows. For example, methods such as those described in [41; 27] extract values from each layer in the network, while others such as [28] require gradient calculations. We note that the application of the best single-instance methods such as [41; 27; 28] in our (large scale) empirical setting is computationally challenging and preclude empirical comparison to our method. Therefore, we consider (in Section 6.4) the detection performance of two computationally efficient single-instance baselines: Softmax-Response (abbreviated as **Single-instance SR** or Single-SR) and Entropy-based (abbreviated as **Single-instance Ent** or Single-Ent), as described in [4; 6; 21]. Specifically, we apply each single-instance OOD detector to every instance in the window and in the detection-training set. We then use a two-sample t-test to determine the p-value between the uncertainty estimators of each sample.

Finally, we mention [14] who developed a risk generalization bound for selective classifiers [9]. The bound presented in that paper is analogous to the coverage generalization bound we present in Theorem 4.2. The risk bound in [14] can also be used for shift-detection. To apply their risk bound to this task, however, labels, which are not available, are required. CBD detects distribution shifts without using any labels.

## 4 Proposed Method - Coverage-Based Detection

In this section we present _Coverage-Based Detection_ (CBD), a novel technique for detecting a distribution shift based on selective prediction principles (definitions follow). We develop a tight generalization coverage bound that holds with high probability for ID data, sampled from the source distribution. The main idea is that violations of this coverage bound indicate a distribution shift with high probability. In Section 6.2, we offer an intuitive demonstration that aids in understanding our approach.

### Selection with Guaranteed Coverage

We begin by introducing basic selective prediction terminology and definitions that are required to describe our method. Consider a standard multiclass classification problem, where \(\mathcal{X}\) is some feature space (e.g., raw image data) and \(\mathcal{Y}\) is a finite label set, \(\mathcal{Y}=\{1,2,3,...,C\}\), representing \(C\) classes. Let \(P(X,Y)\) be a probability distribution over \(\mathcal{X}\times\mathcal{Y}\), and define a _classifier_ as a function \(f:\mathcal{X}\rightarrow\mathcal{Y}\). We refer to \(P\) as the _source distribution_. A _selective classifier_[9] is a pair \((f,g)\), where \(f\) is a classifier and \(g:\mathcal{X}\rightarrow\{0,1\}\) is a _selection function_[9], which serves as a binary qualifier for \(f\) as follows,

\[(f,g)(x)\triangleq\begin{cases}f(x),&\text{if }g(x)=1;\\ \text{don't know},&\text{if }g(x)=0.\end{cases}\]

A general approach for constructing a selection function based on a given classifier \(f\) is to work in terms of a _confidence-rate function_[15], \(\kappa_{f}:\mathcal{X}\rightarrow\mathbb{R}^{+}\), referred to as CF. The CF \(\kappa_{f}\) should quantify confidence in predicting the label of \(x\) based on signals extracted from \(f\). The most common and well-known CF for a classification model \(f\) (with softmax at its last layer) is its _softmax response_ (SR) value [4; 6; 21]. A given CF \(\kappa_{f}\) can be straightforwardly used to define a selection function: \(g_{\theta}(x)\triangleq g_{\theta}(x|\kappa_{f})=\mathds{1}[\kappa_{f}(x) \geq\theta]\), where \(\theta\) is a user-defined constant. For any selection function, we define its _coverage_ w.r.t. a distribution \(\mathcal{P}\) (recall, \(\mathcal{P}\triangleq P_{X}\), see Section 2), and its _empirical coverage_ w.r.t. a sample \(S_{k}\triangleq\{x_{1},x_{2},\ldots x_{k}\}\), as \(c(\theta,\mathcal{P})\triangleq\mathbb{E}_{\mathcal{P}}[g_{\theta}(x)]\), and \(\hat{c}(\theta,S_{k})\triangleq\frac{1}{k}\sum_{i=1}^{k}g_{\theta}(x_{i})\), respectively.

Given a bound on the expected coverage for a given selection function, we can use it to detect a distribution shift via violations of the bound. We will now formally state the problem, develop the bound, and demonstrate its use in detecting distribution shifts.

**Problem statement.**_For a classifier \(f\), a detection-training sample \(S_{m}\sim\mathcal{P}^{m}\), a confidence parameter \(\delta>0\), and a desired coverage \(c^{*}>0\), our goal is to use \(S_{m}\) to find a \(\theta\) value (which implies a selection function \(g_{\theta}\)) that guarantees the desired coverage, with probability \(1-\delta\), namely,_

\[\textbf{Pr}_{S_{m}}\{c(\theta,\mathcal{P})<c^{*}\}<\delta.\] (1)

This means that _under coverage_ should occur with probability of at most \(\delta\).

```
1Input: detection-training set: \(S_{m}\), confidence-rate function: \(\kappa_{f}\), confidence parameter \(\delta\), target coverage: \(c^{*}\).
2Sort \(S_{m}\) according to \(\kappa_{f}(x_{i})\), \(x_{i}\in S_{m}\) (and now assume w.l.o.g. that indices reflect this ordering).
3\(z_{\text{min}}=1\), \(z_{\text{max}}=m\)
4for\(i=1\)to\(k=\lceil\log_{2}m\rceil\)do
5\(z=\lceil(z_{\text{min}}+z_{\text{max}})/2\rceil\)
6\(\theta_{i}=\kappa_{f}(x_{i})\)
7 Calculate \(\hat{c}_{i}(\theta_{i},S_{m})\)
8 Solve for \(b_{i}^{*}(m,m\cdot\hat{c}_{i}(\theta_{i},S_{m}),\frac{\delta}{k})\) {see Lemma 4.1}
9if\(b_{i}^{*}(m,m\cdot\hat{c}_{i}(\theta_{i},S_{m}),\frac{\delta}{k})\leq c^{*}\)then
10\(z_{\text{max}}=z\)
11else
12\(z_{\text{min}}=z\)
13 endif
14
15 endfor
16Output: bound: \(b_{k}^{*}(m,m\cdot\hat{c}_{k}(\theta_{k},S_{m}),\frac{\delta}{k})\), threshold: \(\theta_{k}\). ```

**Algorithm 1**Selection with guaranteed coverage (SGC)

A threshold \(\theta\) that guarantees Equation (1) provides a probabilistic lower bound, guaranteeing that coverage \(c\) of ID unseen population (sampled from \(\mathcal{P}\)) satisfies \(c>c^{*}\) with probability of at least \(1-\delta\). For the remaining of this section, we introduce the _selection with guaranteed coverage_ (SGC) algorithm (Algorithm 1), which outputs a bound (\(b^{*}\)) and its corresponding threshold (\(\theta\)).

The SGC algorithm receives as input a classifier \(f\), a CF \(\kappa_{f}\), a confidence parameter \(\delta\), a target coverage \(c^{*}\), and a detection-training set \(S_{m}\).

The algorithm performs a binary search to find the optimal coverage lower bound with confidence \(\delta\), and outputs a coverage bound \(b^{*}\) and the corresponding threshold \(\theta\), defining the selection function. A pseudo code of the SGC algorithm appears in Algorithm 1.

Our analysis of the SGC algorithm makes use of Lemma 4.1, which gives a tight numerical (generalization) bound on the expected coverage, based on a test over a sample. The proof of Lemma 4.1 is nearly identical to Langford's proof of Theorem 3.3 in [26], p. 278, where instead of the empirical error used in [26], we use the empirical coverage, which is also a Bernoulli random variable.

**Lemma 4.1**.: _Let \(\mathcal{P}\) be any distribution and consider a selection function \(g_{\theta}\) with a threshold \(\theta\) whose coverage is \(c(\theta,\mathcal{P})\). Let \(0<\delta<1\) be given and let \(\hat{c}(\theta,S_{m})\) be the empirical coverage w.r.t. the set \(S_{m}\), sampled i.i.d. from \(\mathcal{P}\). Let \(b^{*}(m,m\cdot\hat{c}(\theta,S_{m}),\delta)\) be the solution of the following equation:_

\[\underset{b}{arg\,min}\left(\sum_{j=0}^{m\cdot\hat{c}(\theta,S_{m})}\binom{m }{j}b^{j}(1-b)^{m-j}\leq 1-\delta\right).\] (2)

_Then,_

\[\textbf{Pr}_{S_{m}}\{c(\theta,\mathcal{P})<b^{*}(m,m\cdot\hat{c}( \theta,S_{m}),\delta)\}<\delta.\] (3)

The following is a uniform convergence theorem for the SGC procedure stating that all the calculated bounds are valid simultaneously with a probability of at least \(1-\delta\).

**Theorem 4.2**.: _(SGC - Uniform convergence) Assume \(S_{m}\) is sampled i.i.d. from \(\mathcal{P}\), and consider an application of Algorithm 1. For \(k=\lceil\log_{2}m\rceil\), let \(b_{i}^{*}(m,m\cdot\hat{c}_{i}(\theta_{i},S_{m}),\frac{\delta}{k})\) and \(\theta_{i}\) be the values obtained in the \(i^{\text{th}}\) iteration of Algorithm 1. Then,_

\[\textbf{Pr}_{S_{m}}\{\exists i:c(\theta_{i},\mathcal{P})<b_{i}^{*}(m,m\cdot \hat{c}_{i}(\theta_{i},S_{m}),\frac{\delta}{k})\}<\delta.\]_Proof (sketch - see full proof in the Appendix B.1)._ Define,

\[\mathcal{B}_{\theta_{i}}\triangleq b_{i}^{*}(m,m\cdot\hat{c}_{i}( \theta_{i},S_{m}),\tfrac{\delta}{k}),\mathcal{C}_{\theta_{i}}\triangleq c( \theta_{i},\mathcal{P})\text{, then,}\] \[\mathbf{Pr}_{S_{m}}\{\exists i:\mathcal{C}_{\theta_{i}}<\mathcal{B }_{\theta_{i}}\} = \sum_{i=1}^{k}\int_{0}^{1}\,d\theta^{\prime}\mathbf{Pr}_{S_{m}}\{ \mathcal{C}_{\theta^{\prime}}<\mathcal{B}_{\theta^{\prime}}\}\cdot\mathbf{Pr}_{S _{m}}\{\theta_{i}=\theta^{\prime}\}\] \[< \sum_{i=1}^{k}\int_{0}^{1}\,d\theta^{\prime}\frac{\delta}{k} \cdot\mathbf{Pr}_{S_{m}}\{\theta_{i}=\theta^{\prime}\}=\sum_{i=1}^{k}\frac{ \delta}{k}=\delta.\]

We would like to note that in addition to Algorithm 1, we also provide a complementary algorithm that returns a tight coverage **upper** (rather than lower) bound, along with the corresponding threshold. Details about this algorithm and its application for detecting distribution shifts are discussed in Appendix F.

### Coverage-Based Detection Algorithm

Our _coverage-based detection_ (CBD) algorithm applies SGC to \(C_{\text{target}}\) target coverages uniformly spread between the interval \([0.1,1]\), excluding the coverage of 1. We set \(C_{\text{target}}=10,\delta=0.01\), and \(\kappa_{f}(x)=1-\text{Entropy}(x)\) for all our experiments; our method appears to be robust to those hyper-parameters, as we demonstrate in Appendix E. Each application \(j\) of SGC on the same sample \(S_{m}\sim\mathcal{P}^{m}\) with a target coverage of \(c_{j}^{*}\) produces a pair: \((b_{j}^{*},\theta_{j})\), which represent a bound and a threshold, respectively. We define \(\delta(\hat{c}|b^{*})\) to be a binary function that indicates a bound violation, where \(\hat{c}\) is the empirical coverage (of a sample) and \(b^{*}\) is a bound, \(\delta(\hat{c}|b^{*})=\mathds{1}[\hat{c}\leq b^{*}]\). Thus, given a window of \(k\) samples from an alternative distribution, \(W_{k}\sim Q^{k}\), we define the _sum of bound violations_, \(V\), as follows,

\[V=\frac{1}{C_{\text{target}}}\sum_{j=1}^{C_{\text{target}}}\,(b_{j}^{*}-\hat{c }(\theta_{j},W_{k}))\cdot\delta(\hat{c}(\theta_{j},W_{k})|b_{j}^{*})=\frac{1} {k\cdot C_{\text{target}}}\sum_{j=1}^{C_{\text{target}}}\sum_{i=1}^{k}\,(b_{j }^{*}-g_{\theta_{j}}(x_{i}))\cdot\delta(\hat{c}_{j}|b_{j}^{*}),\] (4)

where we obtain the last equality by using \(\hat{c}_{j}\triangleq\hat{c}(\theta_{j},W_{k})=\frac{1}{k}\sum_{i=1}^{k}g_{ \theta_{j}}(x_{i})\). Considering Figure 1(b) in Section 6.2, the quantity \(V\) is the sum of distances from the violations (red dots) to the linear diagonal representing the coverage bounds (black line).

```
1//Fit
2Input Training: \(S_{m}\), \(\delta\), \(\kappa_{f}\), \(C_{\text{target}}\)
3Generate \(C_{\text{target}}\) uniformly spread overages \(\mathbf{c}^{*}\)
4for\(j=1\) to \(C_{\text{target}}\)do
5\(b_{j}^{*},\theta_{j}=g_{\text{SGC}(S_{m},\delta,c_{j}^{*},\kappa_{f})}\)
6endfor
7Output Training: \((b_{j}^{*},\theta_{j})_{j=1}^{C_{\text{target}}}\)
8//Detect
9Input Detection: \((\{b_{j}^{*},\theta_{j}\})_{j=1}^{C_{\text{target}}}\)
10whileTuro do
111 Receive window \(W_{k}=\{x_{1},x_{2},\ldots,x_{k}\}\)
12Calculate \(V\) [see Equation (41)]
13Obtain \(p\)-value from test; \(H_{0}:V=0,H_{1}:V>0\)
14\(\lceil p_{\text{value}}\cdot\text{ct}\rceil\)
15Shift_detected = True
16Output Detection: Shift_detected_Pulte
17
18 endif
19
20 endwhile ```

**Algorithm 2**_Coverage-Based Detection_

When \(Q\) equals \(\mathcal{P}\), which implies that there is no distribution shift, we expect that all the bounds (computed by SGC over \(S_{m}\)) will hold over \(W_{k}\), namely \(\delta(\hat{c}_{j}|b_{j}^{*})=0\) for every iteration \(j\); in this case, \(V=0\). Otherwise, \(V\) will indicate the situation magnitude.

Since \(V\) represents the average of \(k\cdot C_{\text{target}}\) values (if at least one bound violation occurs), for cases where \(k\cdot C_{\text{target}}\gg 30\) (as in all our experiments), we can assume that \(V\) follows a nearly normal distribution [25] and perform a t-test1 to test the null hypothesis, \(H_{0}:V=0\), against the alternative hypothesis, \(H_{1}:V>0\). The null hypothesis is rejected if the p-value is less than the specified significance level \(\alpha\). A pseudocode of our _coverage-based detection algorithm_ appears in Algorithm 2.

Footnote 1: In our experiments, we use SciPy’s stats.ttest_1samp implementation [44] for the t-test.

To fit our detector, we apply SGC (Algorithm 2.1). Our detection model utilizes these pairs to monitor a given model, receiving at each time instant a test sample window of size \(k\) (user defined), \(W_{k}=\{x_{1},x_{2},\ldots,x_{k}\}\), which is inspected to see if its content is distributionally fitted from the underlying distribution reflected by the detection-training set \(S_{m}\).

Our approach encodes all necessary information for detecting distribution shifts using only \(C_{\text{target}}\) scalars (in our experiments, we set \(C_{\text{target}}=10\)), which is independent of the size of \(S_{m}\). In contrast, the baselines process the detection-training set, \(S_{m}\), which is typically very large, for every detection they make. This makes our method significantly more efficient than the baselines, see Figure 1 and Table 1 in Section 5.

## 5 Complexity Analysis

This section provides a complexity analysis of our method as well as the baselines mentioned in Section 3. Table 1 summarizes the complexities of each approach. Figure 1 shows the population-based approaches run-time (in seconds) as a function of the detection-training set size, denoted as \(m\).

All population-based baselines are lazy learners (analogous to nearest neighbors) in the sense that they require the entire source (detection-training) set for each detection decision they make. Using only a subset will result in sub-optimal performance, since it might not capture all the necessary information within the source distribution, \(\mathcal{P}\).

In particular, MMD is a permutation test [18] that also employs a kernel. The complexity of kernel methods is dominated by the number of instances and, therefore, the time and space complexities of MMD are \(\mathcal{O}(d(m^{2}+k^{2}+mk))\) and \(\mathcal{O}(m^{2}+k^{2}+mk)\), respectively 2, where in the case of DNNs, \(d\) is the dimension of the embedding or softmax layer used for computing the kernel, and \(k\) is the window size. The KS test [33] is a univariate test, which is applied on each dimension separately and then aggregates the results via a Bonferroni correction. Its time and space complexities are \(\mathcal{O}(d(m\log m+k\log k))\) and \(\mathcal{O}(d(m+k))\), respectively.

Footnote 2: With preprocessing in the fitting stage, this reduces to \(\mathcal{O}(d(k^{2}+mk))\) per detection decision.

The single-instance baselines simply conduct a t-test on the heuristic uncertainty estimators (SR or Entropy-based) between the detection-training set and the window data. By initially determining the mean and standard deviation of the detection-training set, we can efficiently perform the t-test with a time and space complexity of \(\mathcal{O}(k)\).

The fitting procedure of our coverage-based detection algorithm incurs time and space complexities of \(\mathcal{O}(m\log m)\) and \(\mathcal{O}(m)\), respectively. Each subsequent detection round incurs time and space complexities of \(\mathcal{O}(k)\), which is independent of the size of the detection-training (or source) set. Our method's superior efficiency is demonstrated both theoretically and empirically, as shown in Table 1 and Figure 1, respectively. Figure 1 shows the run-time (in seconds) for each population-based detection method3 as a function of the detection-training set size, using a ResNet50 and a fixed window size of \(k=10\) samples. Our detection time remains constant regardless of the size of the detection-training set, as opposed to the baseline methods, which exhibit a run-time that grows as a function of the detection-training set size. In particular, our method achieves a five orders of magnitude improvement in run-time over the best performing baseline, KS-Softmax, when the detection-training set size is 1,000,0004. Specifically, our detection time is \(5.19\cdot 10^{-4}\) seconds, while KS-Softmax's detection time is \(97.1\) seconds, rendering it inapplicable for real-world applications.

Footnote 3: MMD detection-training set is capped at 1,000 due to kernel method’s unfavorable size dependence.

Footnote 4: This specific experiment is simulated synthetically (since the ImageNet validation set size is 50,000).

## 6 Experiments - Detecting Distribution Shifts

In this section we showcase the effectiveness of our method, along with the considered baselines, in detecting distribution shifts. All experiments were conducted using PyTorch, and the corresponding code is publicly available for replicating our results5.

\begin{table}
\begin{tabular}{l c c}
**Detection Method** & **Space** & **Time** \\ \hline \hline
**MMD** & \(\mathcal{O}(m^{2}+k^{2}+mk)\) & \(\mathcal{O}(d(m^{2}+k^{2}+mk))\) \\
**KS** & \(\mathcal{O}(d(m+k))\) & \(\mathcal{O}(d(m\log m+k\log k))\) \\
**Single-instance** & \(\mathcal{O}(\bm{k})\) & \(\mathcal{O}(\bm{k})\) \\ \hline
**CBD** (Ours) & \(\mathcal{O}(\bm{k})\) & \(\mathcal{O}(\bm{k})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Complexity comparison. **bold** entries indicate the best detection complexity. \(m\), \(k\) refers to the detection-training size (or source size), and window size, respectively. \(d\) refers to the number of dimensions after dimensionality reduction.

### Setup

Our experiments are conducted on the ImageNet dataset [7], using its validation dataset as proxies for the source distribution, \(\mathcal{P}\). We utilized three well-known architectures, ResNet50 [19], MobileNetV3-S [24], and ViT-T [8], all of which are publicly available in timm's repository [45], as our pre-trained models. To train our detectors, we randomly split the ImageNet validation data (50,000) into two sets, a detection-training or source set, which is used to fit the detectors\({}^{3}\) (49,000) and a validation set (1,000) for applying the shift. To ensure the reliability of our results, we repeated the shift detection performance evaluation on 15 random splits, applying the same type of shift to different subsets of the data. Inspired by [37], we evaluated the models using various window sizes, \(|W_{k}|\in\{10,20,50,100,200,500,1000\}\).

#### 6.1.1 Distribution Shift Datasets

At test time, the 1,000 validation images obtained via our split can be viewed as the in-distribution (positive) examples. For out-of-distribution (negative) examples, we follow the common setting for detecting OOD samples or distribution shifts [37, 21, 28], and test on several different natural image datasets and synthetic noise datasets. More specifically, we investigate the following types of shifts:

\((1)\) **Adversarial via FGSM**: We transform samples into adversarial ones using the _Fast Gradient Sign Method_ (FGSM) [17], with \(\epsilon\in\{7\cdot 10^{-5},1\cdot 10^{-4},3\cdot 10^{-4},5\cdot 10^{-4}\}\). (2) **Adversarial via PGD**: We convert samples into adversarial examples using _Projected Gradient Descent_ (PGD) [31], with \(\epsilon=1\cdot 10^{-4}\); we use \(10\) steps, \(\alpha=1\cdot 10^{-4}\), and random initialization. (3) **Gaussian noise**: We corrupt test samples with Gaussian noise using standard deviations of \(\sigma\in\{0.1,0.3,0.5,1\}\). (4) **Rotation**: We apply image rotations, \(\theta\in\{5^{\circ},10^{\circ},20^{\circ},25^{\circ}\}\). (5) **Zoom**: We corrupt test samples by applying zoom-out percentages of \(\{90\%,70\%,50\%\}\). (6) **ImageNet-O**: We use the ImageNet-O dataset [23], consisting of natural out-of-distribution samples. (7) **ImageNet-A**: We use the ImageNet-A dataset [23], consisting of natural adversarial samples. (8) **No-shift**: We include the no-shift case to check for false positives. To determine the severity of the distribution shift, we refer the reader to Table 3 in Appendix C.

### Maximizing Detection Power Through Lower Coverages

In this section, we provide an intuitive understanding of our proposed method, as well as a demonstration of the importance of considering lower coverages, when detecting population-based distribution shifts. For all experiments in this section, we employed a ResNet50 [19] and used a detection-training set which was randomly sampled from the ImageNet validation set.

We validate the effectiveness of our CBD method (Algorithm 2) by demonstrating its performance on two distinct scenarios, the no-shift case and the shift case. In the no-shift scenario, as depicted in Figure 1(a), we randomly

Figure 1: Our method outperforms the baselines in terms of scalability, with a significant five orders of magnitude improvement in run-time compared to the best baseline, KS-Softmax, when using a detection-training set of \(m=1,000,000\) samples. One-\(\sigma\) error-bars are shadowed.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

## Acknowledgments

This research was partially supported by the Israel Science Foundation, grant No. 710/18.

## References

* Abdi and Williams [2010] Herve Abdi and Lynne J Williams. Principal component analysis. _Wiley interdisciplinary reviews: computational statistics_, 2(4):433-459, 2010.
* Bingham and Mannila [2001] Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In _Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 245-250, 2001.
* Bland and Altman [1995] J Martin Bland and Douglas G Altman. Multiple significance tests: the bonferroni method. _Bmj_, 310(6973):170, 1995.
* Cordella et al. [1995] Luigi Pietro Cordella, Claudio De Stefano, Francesco Tortorella, and Mario Vento. A method for improving classification reliability of multilayer perceptrons. _IEEE Transactions on Neural Networks_, 6(5):1140-1147, 1995.
* Davis and Goadrich [2006] Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In _Proceedings of the 23rd international conference on Machine learning_, pages 233-240, 2006.
* De Stefano et al. [2000] Claudio De Stefano, Carlo Sansone, and Mario Vento. To reject or not to reject: that is the question-an answer in case of neural classifiers. _IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)_, 30(1):84-94, 2000.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=YicbFdNTTY.
* El-Yaniv et al. [2010] Ran El-Yaniv et al. On the foundations of noise-free selective classification. _Journal of Machine Learning Research_, 11(5), 2010.
* Fawcett [2006] Tom Fawcett. An introduction to roc analysis. _Pattern recognition letters_, 27(8):861-874, 2006.
* Fort et al. [2021] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. _Advances in Neural Information Processing Systems_, 34:7068-7081, 2021.
* Gal and Ghahramani [2016] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _international conference on machine learning_, pages 1050-1059. PMLR, 2016.
* Galil et al. [2023] Ido Galil, Mohammed Dabbah, and Ran El-Yaniv. What can we learn from the selective prediction and uncertainty estimation performance of 523 imagenet classifiers? In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=p66AzKi6Xim.
* Geifman and El-Yaniv [2017] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. _Advances in neural information processing systems_, 30, 2017.
* Geifman et al. [2019] Yonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep neural classifiers. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=SJfb5jCqKm.
* Golan and El-Yaniv [2018] Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. _Advances in neural information processing systems_, 31, 2018.
* Goodfellow et al. [2015] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _International Conference on Learning Representations_, 2015. URL http://arxiv.org/abs/1412.6572.
* Gretton et al. [2012] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.

* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016. doi: 10.1109/CVPR.2016.90.
* Heard and Rubin-Delanchy [2018] Nicholas A Heard and Patrick Rubin-Delanchy. Choosing between methods of combining-values. _Biometrika_, 105(1):239-246, 2018.
* Hendrycks and Gimpel [2017] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=Hkg4TI9x1.
* Hendrycks et al. [2019] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=HyxCxhRcY7.
* Hendrycks et al. [2021] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15262-15271, 2021.
* Howard et al. [2019] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1314-1324, 2019.
* James et al. [2013] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. _An introduction to statistical learning_, volume 112. Springer, 2013.
* Langford and Schapire [2005] John Langford and Robert Schapire. Tutorial on practical prediction theory for classification. _Journal of machine learning research_, 6(3), 2005.
* Lee et al. [2018] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _Advances in neural information processing systems_, 31, 2018.
* Liang et al. [2018] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=H1VGKIxRZ.
* Lipton et al. [2018] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In _International conference on machine learning_, pages 3122-3130. PMLR, 2018.
* Loughin [2004] Thomas M Loughin. A systematic comparison of methods for combining p-values from independent tests. _Computational statistics & data analysis_, 47(3):467-485, 2004.
* Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=rJzIbfZAb.
* Manning and Schutze [1999] Christopher Manning and Hinrich Schutze. _Foundations of statistical natural language processing_. MIT press, 1999.
* J. Masey Jr. [1951] Frank J Massey Jr. The kolmogorov-smirnov test for goodness of fit. _Journal of the American statistical Association_, 46(253):68-78, 1951.
* Nado et al. [2021] Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael W Dusenberry, Sebastian Farquhar, Qixuan Feng, Angelos Filos, Marton Havasi, Rodolphe Jenatton, et al. Uncertainty baselines: Benchmarks for uncertainty & robustness in deep learning. _arXiv preprint arXiv:2106.04015_, 2021.
* Nalisnick et al. [2019] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using typicality. _arXiv preprint arXiv:1906.02994_, 2019.
* Pu et al. [2016] Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence Carin. Variational autoencoder for deep learning of images, labels and captions. _Advances in neural information processing systems_, 29, 2016.
* Rabanser et al. [2019] Stephan Rabanser, Stephan Gunnemann, and Zachary Lipton. Failing loudly: An empirical study of methods for detecting dataset shift. _Advances in Neural Information Processing Systems_, 32, 2019.
* Ren et al. [2019] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. _Advances in neural information processing systems_, 32, 2019.

* [39] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985.
* [40] Takaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets. _PloS one_, 10(3):e0118432, 2015.
* [41] Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with gram matrices. In _International Conference on Machine Learning_, pages 8491-8501. PMLR, 2020.
* [42] Robert J Serfling. _Approximation theorems of mathematical statistics_, volume 162. John Wiley & Sons, 2009.
* [43] Danica J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=HJWHIXqgl.
* [44] Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in python. _Nature methods_, 17(3):261-272, 2020.
* [45] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.

Shift-Detection General Framework

The general framework for shift-detection can be found in the following figure, Figure 3.

## Appendix B Proofs

### Proof for Theorem 4.2

Proof.: Define

\[\mathcal{B}_{\theta_{i}} \triangleq b_{i}^{*}(m,m\cdot\hat{c}_{i}(\theta_{i},S_{m}),\frac{ \delta}{k}),\] \[\mathcal{C}_{\theta_{i}} \triangleq c(\theta_{i},\mathcal{P}).\]

Consider the i-th iteration of SGR over a detection-training set \(S_{m}\), and recall that, \(\theta_{i}=\kappa_{f}(x_{z})\), \(x_{z}\in S_{m}\) (see Algorithm 1). Therefore, \(\theta_{i}\) is a random variable (between zero and one), since it is a function of a random variable (\(x\in S_{m}\)). Let \(\textbf{Pr}_{S_{m}}\{\theta_{i}=\theta^{\prime}\}\) be the probability that \(\theta_{i}=\theta^{\prime}\).

Therefore,

\[\textbf{Pr}_{S_{m}}\{\mathcal{C}_{\theta_{i}}<\mathcal{B}_{\theta _{i}}\}\] \[=\int_{0}^{1}\,d\theta^{\prime}\textbf{Pr}_{S_{m}}\{\mathcal{C}_ {\theta_{i}}<\mathcal{B}_{\theta_{i}}|\theta_{i}=\theta^{\prime}\}\cdot \textbf{Pr}_{S_{m}}\{\theta_{i}=\theta^{\prime}\}\] \[=\int_{0}^{1}\,d\theta^{\prime}\textbf{Pr}_{S_{m}}\{\mathcal{C}_ {\theta^{\prime}}<\mathcal{B}_{\theta^{\prime}}\}\cdot\textbf{Pr}_{S_{m}}\{ \theta_{i}=\theta^{\prime}\}.\]

Given that at the \(i\)-th iteration \(\theta_{i}=\theta^{\prime}\), and considering that \(\mathcal{B}_{\theta^{\prime}}\) is derived using Lemma 4.1 (refer to Algorithm 1), we obtain,

\[\textbf{Pr}_{S_{m}}\{\mathcal{C}_{\theta^{\prime}}<\mathcal{B}_{\theta^{ \prime}}\}<\frac{\delta}{k}.\]

Thus,

\[\textbf{Pr}_{S_{m}}\{\mathcal{C}_{\theta_{i}}<\mathcal{B}_{\theta _{i}}\}\] \[=\int_{0}^{1}\,d\theta^{\prime}\textbf{Pr}_{S_{m}}\{\mathcal{C}_ {\theta^{\prime}}<\mathcal{B}_{\theta^{\prime}}\}\cdot\textbf{Pr}_{S_{m}}\{ \theta_{i}=\theta^{\prime}\}\] \[<\int_{0}^{1}\,d\theta^{\prime}\frac{\delta}{k}\cdot\textbf{Pr}_ {S_{m}}\{\theta_{i}=\theta^{\prime}\}\] \[=\frac{\delta}{k}\cdot\left(\int_{0}^{1}\,d\theta^{\prime}\textbf {Pr}_{S_{m}}\{\theta_{i}=\theta^{\prime}\}\right)\] \[=\frac{\delta}{k}.\] (5)

The following application of the union bound completes the proof,

\[\textbf{Pr}_{S_{m}}\{\exists i:\mathcal{C}_{\theta_{i}}<\mathcal{B}_{\theta _{i}}\}\leq\sum_{i=1}^{k}\textbf{Pr}_{S_{m}}\{\mathcal{C}_{\theta_{i}}<\mathcal{ B}_{\theta_{i}}\}<\sum_{i=1}^{k}\frac{\delta}{k}=\delta.\]

Figure 3: The procedure of detecting a dataset shift using dimensionality reduction and then a two-sample statistical test. The dimensionality reduction is applied to both the detection-training (source) and test (target) data, prior to being analyzed using statistical hypothesis testing. This figure is taken from [37].

Exploring Model Sensitivity: Evaluating Accuracy on Shifted Datasets

In this section, we present Table 3, which displays the accuracy (when applicable) as well as the degradation from the original accuracy over the ImageNet dataset, of the considered models on each of the simulated shifts mentioned in Section 6.1.1.

\begin{table}
\begin{tabular}{l||c|c||c|c||c|c} Shift Dataset & \multicolumn{3}{c||}{**ResNet50**} & \multicolumn{2}{c||}{**MovileNetV3**} & \multicolumn{2}{c}{**ViT-T**} \\  & Acc. & ImageNet Degradation & Acc. & ImageNet Degradation & Acc. & ImageNet Degradation \\ \hline \hline FGSM \(c=7\cdot 10^{-5}\) & 76.68\% & -3.7\% & 62.09\% & -3.15\% & 72.51\% & -2.95\% \\ \hline FGSM \(c=1\cdot 10^{-4}\) & 75.19\% & -5.19\% & 60.72\% & -4.52\% & 71.49\% & -3.97\% \\ \hline FGSM \(c=3\cdot 10^{-4}\) & 66.15\% & -14.23\% & 52.09\% & -13.15\% & 65.06\% & -10.4\% \\ \hline FGSM \(c=5\cdot 10^{-4}\) & 59.23\% & -21.15\% & 44.45\% & -20.79\% & 58.9\% & -16.56\% \\ \hline PGD \(c=1\cdot 10^{-4}\) & 74.64\% & -5.74\% & 60.63\% & -4.61\% & 71.35\% & -4.11\% \\ \hline GAUSSIAN \(\sigma=0.1\) & 79.02\% & -1.36\% & 62.82\% & -2.42\% & 71.79\% & -3.67\% \\ \hline GAUSSIAN \(\sigma=0.3\) & 74.63\% & -5.75\% & 55.06\% & -10.18\% & 50.86\% & -24.6\% \\ \hline GAUSSIAN \(\sigma=0.5\) & 68.56\% & -11.82\% & 42.55\% & -22.69\% & 22.25\% & -53.21\% \\ \hline GAUSSIAN \(\sigma=1\) & 46.1\% & -34.28\% & 13.82\% & -51.42\% & 0.56\% & -74.9\% \\ \hline ZOOM \(50\%\) & 65.55\% & -14.83\% & 36.96\% & -28.28\% & 46.04\% & -29.42\% \\ \hline ZOOM \(70\%\) & 74.31\% & -6.07\% & 53.53\% & -11.71\% & 62.69\% & -12.77\% \\ \hline ZoOM \(90\%\) & 78.6\% & -1.78\% & 61.28\% & -3.96\% & 72.08\% & -3.38\% \\ \hline ROTATION \(\theta=5^{\circ}\) & 76.7\% & -3.68\% & 62.42\% & -2.82\% & 71.27\% & -4.19\% \\ \hline ROTATION \(\theta=10^{\circ}\) & 72.4\% & -7.98\% & 58.22\% & -7.02\% & 67.29\% & -8.17\% \\ \hline ROTATION \(\theta=20^{\circ}\) & 68.29\% & -12.09\% & 49.96\% & -15.28\% & 62.38\% & -13.08\% \\ \hline ROTATION \(\theta=25^{\circ}\) & 70.08\% & -10.3\% & 50.95\% & -14.29\% & 60.97\% & -14.49\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Shifted dataset accuracy and comparison with ImageNet. We display the accuracy for each shifted dataset and model combination, along with the accuracy degradation when compared to the original ImageNet dataset.

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_FAIL:18]

**Problem statement**.: _For a classifier \(f\), a detection-training sample \(S_{m}\sim\mathcal{P}^{m}\), a confidence parameter \(\delta>0\), and a desired coverage \(c^{*}>0\), our goal is to use \(S_{m}\) to find a \(\theta\) value (which implies a selection function \(g_{\theta}\)), such that the coverage satisfies,_

\[\textbf{Pr}_{S_{m}}\{c(\theta,\mathcal{P})>c^{*}\}<\delta.\] (6)

This means that _over coverage_ should occur with probability of at most \(\delta\).

The pseudo code of the algorithm that finds the optimal coverage upper bound (with confidence \(\delta\)) - SGC-UP, appears in Algorithm 3.

```
1Input: train set: \(S_{m}\), confidence-rate function: \(\kappa_{f}\), confidence parameter \(\delta\), target coverage: \(c^{*}\).
2Sort \(S_{m}\) according to \(\kappa_{f}(x_{i})\), \(x_{i}\in S_{m}\) (and now assume w.l.o.g. that indices reflect this ordering).
3\(z_{\text{min}}=1\), \(z_{\text{max}}=m\)
4for\(i=1\)to\(k=\lceil\log_{2}m\rceil\)do
5\(z=\lceil(z_{\text{min}}+z_{\text{max}})/2\rceil\)
6\(\theta_{i}=\kappa_{f}(x_{z})\)
7 Calculate \(\hat{c_{i}}(\theta_{i},S_{m})\)
8 Solve for \(b_{i}^{*}(m,m\cdot\hat{c_{i}}(\theta_{i},S_{m}),\frac{\delta}{k})\) {see Lemma F.1}
9if\(b_{i}^{*}(m,m\cdot\hat{c_{i}}(\theta_{i},S_{m}),\frac{\delta}{k})\geq c^{*}\)then
10\(z_{\text{min}}=z\)
11else
12\(z_{\text{max}}=z\)
13 endif
14
15 endfor
16Output: bound: \(b_{k}^{*}(m,m\cdot\hat{c_{k}}(\theta_{k},S_{m}),\frac{\delta}{k})\), threshold: \(\theta_{k}\). ```

**Algorithm 3**Selection with guaranteed coverage - Upper bound (SGC-UP)

To deduce Equation (6) we make use of Lemma F.1, which is nearly identical to Lemma 4.1.

**Lemma F.1**.: _Let \(\mathcal{P}\) be any distribution and consider a CF threshold \(\theta\) with a coverage of \(c(\theta,\mathcal{P})\). Let \(0<\delta<1\) be given and let \(\hat{c}(\theta,S_{m})\) be the empirical coverage w.r.t. the set \(S_{m}\), sampled i.i.d. from \(\mathcal{P}\). Let \(b^{*}(m,m\cdot\hat{c}(\theta,S_{m}),\delta)\) be the solution of the following equation:_

\[\underset{b}{arg\,min}\left(\sum_{j=0}^{m\cdot\hat{c}(\theta,S_{m})}{m\choose j }b^{j}(1-b)^{m-j}\leq\delta\right).\] (7)

_Then,_

\[\textbf{Pr}_{S_{m}}\{c(\theta,\mathcal{P})>b^{*}(m,\hat{c}(\theta,S_{m}), \delta)\}<\delta.\] (8)

The following is a uniform convergence theorem for SGC-UP (Algorithm 3) procedure stating that all the calculated bounds are valid simultaneously with a probability of at least \(1-\delta\).

**Theorem F.2**.: _Assume \(S_{m}\) is sampled i.i.d. from \(\mathcal{P}\), and consider an application of Algorithm 3. For \(k=\lceil\log_{2}m\rceil\), let \(b_{i}^{*}(m,m\cdot\hat{c}_{i}(\theta_{i},S_{m}),\frac{\delta}{k})\) and \(\theta_{i}\) be the values obtained in the \(\hat{r}^{\text{th}}\) iteration of Algorithm 3. Then,_

\[\textbf{Pr}_{S_{m}}\{\exists i:c(\theta_{i},\mathcal{P})>b_{i}^{*}(m,m\cdot \hat{c}_{i}(\theta_{i},S_{m}),\frac{\delta}{k})\}<\delta.\]

The proof for Theorem F.2 can be easily deduced from the proof of Theorem 4.2, given in Appendix B.1.

### Application of Upper Bound Algorithm for Detecting Distribution Shifts

In this section, we demonstrate the application of Algorithm 3 for detecting shifts in distribution. Specifically, Algorithm 3 is useful in identifying distribution shifts towards 'over-confidence,' where the model exhibits excessive confidence in its predictions on input data. It's important to note that this scenario was not empirically observed in our experiments, leading to the exclusion of this section from the main body of the paper.

Our 'over-confidence' detection algorithm is analogous to Algorithm 2. We start by applying SGC-UP to \(C_{\text{target}}\) target coverages uniformly spread between the interval \([0.1,1]\), excluding the coverage of \(1\). Each application \(j\) of SGC-UP on the same sample \(S_{m}\sim\mathcal{P}_{m}\) with a target coverage of \(c_{j}^{\text{-sup}}\) produces a pair: \((b_{j}^{\text{-sup}},\theta_{j}^{\text{-sup}})\), which represents a threshold and a bound, respectively.

Define, \(\delta^{\text{up}}(\hat{c}|b^{*})=\mathds{1}[\hat{c}\geq b^{*}]\), thus, given a window of k samples from an alternative distribution, \(W_{k}\sim Q^{k}\), we define the _sum of upper bound violations_, \(V^{\text{up}}\), as follows,

\[V^{\text{up}} = \frac{1}{C_{\text{target}}}\sum_{j=1}^{C_{\text{target}}}(\hat{c} (\theta^{\text{up}}_{j},W_{k})-b_{j}^{*\text{up}})\cdot\delta^{\text{up}}( \hat{c}(\theta^{\text{up}}_{j},W_{k})|b_{j}^{*\text{up}})\] \[= \frac{1}{k\cdot C_{\text{target}}}\sum_{j=1}^{C_{\text{target}}} \sum_{i=1}^{k}\left(g_{\theta^{\text{up}}_{j}}(x_{i})-b_{j}^{*\text{up}} \right)\cdot\delta^{\text{up}}(\hat{c}^{\text{up}}_{j}|b_{j}^{*\text{up}}),\]

where we obtain the last equality by using \(\hat{c}^{\text{up}}_{j}\triangleq\hat{c}(\theta^{\text{up}}_{j},W_{k})=\frac {1}{k}\sum_{i=1}^{k}g_{\theta^{\text{up}}_{j}}(x_{i})\).

Similarly to \(V\) in Equation (4), the term \(V^{\text{up}}\) denotes the magnitude of the violation, where \(V^{\text{up}}=0\) indicates an absence of over-confidence in the predictions.

A pseudocode of our algorithm for detecting over-confidence in predictions appears in Algorithm 4.

```
1//Fit
2InputTraining:\(S_{m}\), \(\delta\), \(\kappa_{f}\), \(C_{\text{target}}\)
3Generate\(C_{\text{target}}\) uniformly spread coverages \(\bm{c}^{*}\)
4for\(j=1\)to\(C_{\text{target}}\)do
5\(b_{j}^{*\text{up}}\), \(\theta^{\text{up}}_{j}=\text{{\tt SOC-UP}}(S_{m}\), \(\delta\), \(c_{j}^{*}\), \(\kappa_{f})\)
6endfor
7OutputTraining:\(\{(b_{j}^{*\text{up}},\theta^{\text{up}}_{j})\}_{j=1}^{C_{\text{target}}}\)//Detect
8InputDetection:\(\{(b_{j}^{*\text{up}},\theta^{\text{up}}_{j})\}_{j=1}^{C_{\text{target}}}\), \(\kappa_{f}\), \(\alpha\), \(k\)whileTrue do
9 Receive window \(W_{k}=\{x_{1},x_{2},\ldots,x_{k}\}\)
10 Calculate \(V^{\text{up}}\) {see Equation (9)}
11 Obtain p-value from t-test, \(H_{0}:V^{\text{up}}=0\), \(H_{1}:V^{\text{up}}>0\)
12if\(p_{\text{value}}<\alpha\)then
13 Shift_detected\(\leftarrow\)True
14 OutputDetection: Shift_detected, \(p_{\text{value}}\)
15endif
16 endwhile ```

**Algorithm 4**_Coverage-Based Detection - over confidence_

### Unified Algorithm for Distribution Shift Detection

In this section, we introduce a unified algorithm designed to detect shifts in distribution, accommodating both scenarios of over-confidence and under-confidence in predictions.

Define _sum of violations_ (SOV),

\[\text{SOV}\triangleq V+V^{\text{up}}=\frac{1}{C_{\text{target}}}\sum_{j=1}^{C _{\text{target}}}(\hat{c}^{\text{up}}_{j}-b_{j}^{*\text{up}})\cdot\delta^{ \text{up}}(\hat{c}^{\text{up}}_{j}|b_{j}^{*\text{up}})+(b_{j}^{*}-\hat{c}_{j} )\cdot\delta(\hat{c}_{j}|b_{j}^{*}).\] (10)

A pseudocode of our unified algorithm for detecting both over and under confidence in predictions appears in Algorithm 5.

```
1//Fit
2InputTraining:\(S_{m}\), \(\delta\), \(\kappa_{f}\), \(C_{\text{target}}\)
3 Generate \(C_{\text{target}}\) uniformly spread coverages \(\bm{c}^{*}\)
4for\(j=1\)to\(\bm{U_{\text{target}}}\)do
5\(b_{j}^{*}\), \(\theta_{j}=\texttt{SGC}(S_{m},\delta,c_{j}^{*},\kappa_{f})\)
6\(b_{j}^{*\text{-}\text{out}}\), \(\theta_{j}^{\text{-}}=\texttt{SGC}\texttt{-UP}(S_{m},\delta,c_{j}^{*},\kappa_{ f})\)
7endfor
8OutputTraining:\(\{(b_{j}^{*},\theta_{j})\}_{j=1}^{C_{\text{target}}}\), \(\{(b_{j}^{*\text{-}\text{up}},\theta_{j}^{\text{up}})\}_{j=1}^{C_{\text{target}}}\)//Detect
9InputDetection:\(\{(b_{j}^{*},\theta_{j})\}_{j=1}^{C_{\text{target}}}\), \(\{(b_{j}^{*\text{-}\text{up}},\theta_{j}^{\text{up}})\}_{j=1}^{C_{\text{target}}}\), \(\kappa_{f}\), \(\alpha\), \(k\)
10whileTrue do
11 Receive window \(W_{k}=\{x_{1},x_{2},\ldots,x_{k}\}\)
12 Calculate SOV [see Equation (10)]
13 Obtain p-value from t-test, \(H_{0}:\text{SOV}=0\), \(H_{1}:\text{SOV}>0\)
14if\(p_{\text{value}}<\alpha\)then
15 Shift_detected\(\leftarrow\)True
16 OutputDetection: Shift_detected, \(p_{\text{value}}\)
17 endif
18
19 endwhile ```

**Algorithm 5**_Coverage-Based Detection - Unified_