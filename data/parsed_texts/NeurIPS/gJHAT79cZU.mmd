# NeRF Revisited: Fixing Quadrature Instability in Volume Rendering

 Mikaela Angelina Uy\({}^{1}\) George Kiyohiro Nakayama\({}^{1}\) Guandao Yang\({}^{1,2}\)

**Rahul Krishna Thomas\({}^{1}\) Leonidas Guibas\({}^{1}\) Ke Li\({}^{3,4}\)**

\({}^{1}\)Stanford University \({}^{2}\)Cornell University \({}^{3}\)Simon Fraser University \({}^{4}\)Google {mikacuy, w4756677, guandao, rt03mas, guibas}@stanford.edu, keli@sfu.ca

###### Abstract

Neural radiance fields (NeRF) rely on volume rendering to synthesize novel views. Volume rendering requires evaluating an integral along each ray, which is numerically approximated with a finite sum that corresponds to the exact integral along the ray under piecewise constant volume density. As a consequence, the rendered result is unstable w.r.t. the choice of samples along the ray, a phenomenon that we dub _quadrature instability_. We propose a mathematically principled solution by reformulating the sample-based rendering equation so that it corresponds to the exact integral under piecewise linear volume density. This simultaneously resolves multiple issues: conflicts between samples along different rays, imprecise hierarchical sampling, and non-differentiability of quantiles of ray termination distances w.r.t. model parameters. We demonstrate several benefits over the classical sample-based rendering equation, such as sharper textures, better geometric reconstruction, and stronger depth supervision. Our proposed formulation can be also be used as a drop-in replacement to the volume rendering equation for existing methods like NeRFs. Our project page can be found at pl-nerf.github.io.

## 1 Introduction

The advent of neural radiance fields (NeRF) [18] has sparked a flurry of work on neural rendering and has opened the way to many exciting applications [5, 11, 8, 20]. One of the key underpinnings of NeRF is volume rendering [14] - it is especially well-suited to end-to-end differentiable rendering [9], since the rendered image is a smooth function of the model parameters. This has made it possible to learn the 3D geometry and appearance solely from a 2D photometric loss on rendered images.

In volume rendering, the rendered colour \(\hat{y}\) for every pixel is an expectation of the colours along the ray cast through the pixel w.r.t. the distribution over ray termination distance \(s\)[6].

\[\hat{y}=\mathbb{E}_{s\sim p(s)}[c(s)]=\int_{0}^{\infty}p(s)c(s)\,\mathrm{d}s\] (1)

where \(p(s)\) denotes the probability density function (PDF) of the distribution over ray termination distance \(s\) and \(c(s)\) denotes the the colour as a function of different points along the ray.

In general, \(p(s)\) and \(c(s)\) can be of arbitrary forms, so evaluating this integral analytically is not possible. Therefore, in practice, \(\mathbb{E}_{s\sim p(s)}[c(s)]\) is approximated with quadrature. The quadrature formula that is most commonly used in the NeRF literature takes the following form:

\[\mathbb{E}_{s\sim p(s)}[c(s)]=\sum_{j=0}^{N}T_{j}\left(1-e^{-\tau_{j}(s_{j+1}- s_{j})}\right)c_{j},\] (2)

where \(T_{j}=\exp\Big{(}-\sum_{k=0}^{j}-\tau_{k}(s_{k+1}-s_{k})\Big{)}\) and \(\tau_{k}\) is the opacity evaluated at a sample \(s_{k}\) along the ray.

This expression is derived from the exact integral under a piecewise constant assumption to the opacity and colour along the given ray [14].

However, this seemingly simple, innocuous assumption can result in the rendered image being sensitive to the choice of samples along the ray at which the opacity \(\sigma(s)\) and colour \(c(s)\) are evaluated. While this does not necessarily cause a practical issue in classical rendering pipelines [14; 15; 10], it has surprising consequences when used in neural rendering. Specifically, because the opacity at all points within the interval between two samples is assumed to be the same, there is a band near the surface of the geometry where opacity at all points within the band is as high as points on the surface itself. Because different rays cast from different cameras can pass through this band at different angles and offsets, both the number and the positions of samples within this band can be very different across different rays (see Fig 1 and Fig 2 for two example scenarios). Hence, simultaneously supervising these rays to produce the same colour values as in the real image captures can give rise to conflicting supervisory signals, which can result in artifacts like fuzzy surfaces and blurry texture.

Moreover, because the piecewise constant opacity assumption gives rise to a closed form expression that is equivalent to an expectation w.r.t. a discrete random variable, it is common practice in the NeRF literature to draw samples along the ray from the discrete distribution [17]. This is commonly used to draw importance samples, and also to supervise the samples along the ray [28], for example in losses that penalize deviation of the samples from the true depth [6]. Sampling from the discrete distribution requires the definition of a continuous surrogate function to the cumulative distribution function (CDF) of the discrete random variable, which unfortunately yields imprecise samples. As a result, samples that are drawn may not be close to the surface even if the underlying probability density induced by the NeRF is concentrated at the surface. Additionally, individual supervision cannot be provided to each sample drawn from the surrogate, because the gradient of the loss w.r.t. each sample would be almost zero everywhere.

All these issues, i.e. conflicting supervision, imprecise samples and lack of supervision on the CDF from the samples, stem from the assumption that opacity is piecewise constant causing the _sensitivity to the choice of samples_ both during rendering and sampling. We dub this problem as _quadrature instability_. In this paper, we revisit the quadrature used to approximate volume rendering in NeRF and devise a different quadrature formula [14] based on a different approximation to the opacity. We first show that interestingly a closed-form expression can be derived under any piecewise polynomial approximation for opacity. When the polynomial degree is 0, it reduces to the piecewise constant opacity as in existing literature, and when the degree is 2 or more, we show that it would lead to poor numerical conditioning

Therefore, we further explore a degree of 1 (i.e., piecewise _linear_) and show that it both resolves quadrature instability and has good numerical conditioning. We derive the rendering equation under _piecewise linear opacity_ explicitly and show that it has a simple and intuitive form. This results in a new quadrature method for volume rendering, which can serve as a drop-in replacement for existing methods like NeRFs. We demonstrate that this reduces artifacts, improves rendering quality and results in better geometric reconstruction. We also devise a new way to sample directly from the distribution of samples along each ray induced by NeRF without going through the surrogate, which opens the way to a more refined importance sampling approach and a more effective method to supervise samples using depth.

## 2 Related Work

NeRFs.Neural Radiance Field (NeRF) is a powerful representation for novel-view synthesis [18] that represents a scene using the weights of an MLP that is rendered by volumetric rendering [14]. A key finding to the success of NeRF was the use of positional encoding [26; 29] to effectively increase the capacity of the MLPs that models the opacity and emitted color as a function of a 3D coordinate and viewing direction. Many works extend NeRF such as handling larger or unbounded scenes [40; 3; 37; 25], unconstrained photo collections [13], dynamic and deformable scenes [11; 19] and sparser input views [6; 39; 31; 28]. There are a number of papers that aim to improve the rendering quality of NeRF. Some do so by utilizing different kinds of supervision such as NeRF in the Dark [16], while others tackle this by improving the model [2; 36; 4]. MipNeRF [2] changes the model input by introducing integrated positional encoding (IPE) to reduce the aliasing effect along the xy coordinates. DiVeR [36] predicts the rendered colour within a line interval directly from a trilinearly interpolated feature in a voxel-based representation ZipNeRF [4] modifies the proposal network to a grid enabling it to be used together with IPE. In contrast, our work focuses on changing the objective function by modifying the rendering equation from piecewise constant opacity to piecewise linear, while keeping the model and supervision fixed. Additionally, ZipNeRF [4] also brings up a model specific issue on z-aliasing, where their model struggles under this setting. Similar to z-aliasing observed by ZipNeRF, we consider the setting of having conflicting supervision when presented with training views at different distances from the scene. While they may appear similar on the surface, the phenomena we study is different in that it is general and independent of the model, on having conflicting ray supervision from camera views, e.g. different camera-to-scene distances and the grazing angle setup.

Importance Sampling on NeRFs.Densely sampling and evaluating NeRF along multiple points in each camera ray is inefficient. Inspired by an early work on volume rendering [10], prior works typically use a coarse-to-fine hierarchical sampling strategy where the final samples are obtained by importance sampling of a coarse proposal distribution [18; 2; 3; 8]. These importance samples are drawn using inverse transform sampling where a sample is obtained by taking the inverse of the cumulative density function (CDF) of the proposal ray distribution. However, prior NeRF works that assume piecewise constant opacity result in a non-invertible CDF, and instead introduce a surrogate invertible function derived from the CDF in order to perform importance sampling. In contrast, our work that utilizes a piecewise linear opacity assumption results in an invertible CDF and a closed-form solution to obtain samples with inverse transform sampling. Other works also attempt to alter sampling using neural networks or occupancy caching [31; 23; 24; 12]. These techniques are orthogonal to our work as they propose changes to the model as opposed to our work where importance sampling is derived from a given model.

Volume rendering.Volume rendering is an important technique in various computer graphics and vision applications as explored in different classical works [10; 33; 7; 14]. These works include studying ray sampling efficiency [10] and data structures, e.g. octree [22] and volume hierarchy [21] for coarse-to-fine hierarchical sampling. The crux behind volume rendering is the integration over the weighted average of the color along the ray, where the weights is a function of the volume density (opacity). Max and Chen [14; 15] derive the volume rendering equation under the assumption of piecewise constant opacity and color, which NeRF [17] and its succeeding works use to learn their neural scene representation. However, the piecewise constant assumption results in rendering outputs that are sensitive to the choice of samples as well the non-invertible CDF introducing drawbacks to NeRF training. Following up on [14], works also [34] derive the volume rendering equation under the assumption that both opacity and color are piecewise linear that yield unwieldy expressions that lead to numerical issues and/or are expensive to compute. Some earlier works on rendering unstructured polygonal meshes that attempt to use this model [35], but it is in general not commonly used in practice due to the mentioned issues and hence has yet to be adopted into learning neural scene representations. In this work, we address both sets of issues that arise from the piecewise constant opacity and color and piecewise linear opacity and color by reformulating the volume rendering equation to assume piecewise linear opacity and piecewise constant color. Our derivation results in a simple and closed-form formulation to volume rendering making it suitable for NeRFs.

## 3 Background

### Volume Rendering Review

Definitions.In classical literature, the process of volume rendering [15] mapping a 3D field of optical properties to a 2D image and the visual appearance is computed through the exact _integration_ of these optical properties along the viewing rays. In this optical model, each point in space is an infinitesimal particle with a certain _opacity_\(\tau\) that emits varying amounts of light, represented as a scalar _color_\(c\), in all viewing directions. The opacity \(\tau\) is the differential probability of a viewing ray hitting a particle - that is for a viewing ray \(\mathbf{r}(s)=\mathbf{o}+s\mathbf{d}\), where \(\mathbf{o}\) is the view origin and \(\mathbf{d}\) is the ray direction, the probability of ray \(\mathbf{r}\) hitting a particle along an infinitesimal interval \(\mathrm{d}s\) is \(\tau(\mathbf{r}(s))\mathrm{d}s\). Moreover, the transmittance \(T_{\mathbf{r}}(s)\) is defined as the probability that the viewing ray \(\mathbf{r}\) travels a distance \(s\) from the view origin without terminating, i.e. without hitting any particles.

Continuous probability distribution along the ray \(\mathbf{r}\).As illustrated by Max and Chen [15], the probability of hitting a particle \(s+ds\) only depends on the probability of hitting a particle at \(s\) and not any particles before it the probability of ray \(\mathbf{r}\) terminating at distance \(s\) is given by \(\tau(\mathbf{r}(s))T_{\mathbf{r}}(s)\), where \(T_{\mathbf{r}}(s)=\exp(-\int_{0}^{s}\tau(\mathbf{u})\mathrm{d}u)\). Hence the continuous probability density function (PDF) of ray \(\mathbf{r}(s)\), which describes the likelihood of a ray terminating and emitting at \(s\), is given by

\[p(s)=\tau(\mathbf{r}(s))T_{\mathbf{r}}(s),\] (3)where \(s\in[0,\infty]\) and \(\mathbf{r}(s)\) is a point on the ray \(\mathbf{r}\). For notational simplicity we omit \(\mathbf{r}\) and write it as \(p(s)=\tau(s)T(s)\).

Volume Rendering as a Continuous Integral.The observed color of the ray is then the expected value of the colors \(c(s)\) of all particles \(s\) along the ray weighted by the probability of hitting them. Mathematically, this results in the following continuous integral1:

Footnote 1: Practically the integral is taken with near \(s_{n}\) and far \(s_{f}\) bounds.

\[\mathbb{E}_{s\sim p(s)}[c(s)]=\int_{0}^{\infty}p(s)c(s)\,\mathrm{d}s=\int_{0}^ {\infty}\tau(s)T(s)c(s)\,\mathrm{d}s\,.\] (4)

Quadrature under Piecewise Constant Opacity \(\tau\).Since this integral cannot in general be evaluated analytically, it is approximated with quadrature. Let \(s_{1},s_{2},...,s_{N}\) be \(N\) (ordered) samples on the ray that define the intervals, where \(I_{j}=[s_{j},s_{j+1}]\) is the \(j^{\text{th}}\) interval, and \(I_{0}=[0,s_{1}],I_{N}=[s_{N},\infty]\). The volume density for particles along the interval \(I_{j}\) is then approximated under the assumption that opacity is constant along each interval, making it _piecewise constant_ along the ray [15]. That is, for all \(j\) we have:

\[\forall s\in[s_{j},s_{j+1}],\tau(s)=\tau(s_{j}),\] (5)

for brevity we denote \(\tau(s_{j})=\tau_{j}\), i.e. \(\tau_{j}\) is the opacity for sample \(s_{j}\). Under this piecewise constant opacity assumption, the volume rendering equation Eq. 4 then becomes as follows:

\[\mathbb{E}_{s\sim p(s)}[c(s)]=\sum_{j=0}^{N}P_{j}c_{j}=\sum_{j=0}^{N}\left( \int_{s_{j}}^{s_{j+1}}\tau(u)T(u)\,\mathrm{d}u\right)c_{j}=\sum_{j=0}^{N}T_{j }\left(1-e^{-\tau_{j}(s_{j+1}-s_{j})}\right)c_{j},\] (6)

Figure 1: **Ray Conflicts: Grazing Angle.** (Left) Illustration of conflicting ray supervision at the grazing under the piecewise constant opacity. For the constant setting, to render perpendicular rays (yellow) correctly, the model has to store the associated optical properties at a region in front of the surface as a sample takes the values of the left bin boundary. In the presence of a ray near the grazing angle, it will be crossing this region of high opacity (the gradient in front of the surface), associating it with conflicting opacity/color signals. (Middle) This results in fuzzier surfaces as shown along the side of the microphone as there is a conflict in ray supervision between the perpendicular and grazing angle rays. Our piecewise linear opacity assumption alleviates this issue and results in a clearer rendered view. (Right) As shown, the resulting PDF is peakier and the CDF is sharper for our linear setting, where the plotted distributions correspond to the ray from the marked pixel in red.

where \(T_{j}=\exp\Big{(}-\sum_{k=0}^{j}-\tau_{k}(s_{k+1}-s_{k})\Big{)}\). Here, color \(c_{j}\) is also approximated to be constant along each interval \(I_{j}\), and \(P_{j}\) is the probability of each interval. Now, let us define the discrete random variable \(\tilde{s}=\tilde{f}(s)\), where

\[\tilde{f}(x)=\begin{cases}s_{0}&x\leq s_{0}\\ s_{j}&s_{j}\leq x<s_{j+1}\text{ for all }j\in\{1,...,N-1\}\\ s_{N}&x>s_{N}\end{cases},\] (7)

which gives corresponding probability mass function \(\tilde{P}(s)\). Observe that the analytical expression of the integral Eq. 6 turns out to be the same as taking the expectation w.r.t. the discrete random variable \(\tilde{s}\), i.e. \(\mathbb{E}_{s\sim p(s)}[c(s)]=\mathbb{E}_{\tilde{s}\sim\tilde{P}(s)}[c(\tilde {s})]\). This piecewise constant opacity assumption in the volume rendering equation is used in most, if not all, existing NeRF works. We recommend the reader to read [15] for more detailed derivations and our supplementary for a more thorough walkthrough.

### Neural Radiance Fields.

Following Max and Chen [14], Mildenhall et.al. [18] introduced neural radiance fields, a neural scene representation that uses the volume rendering equation under the piecewise constant opacity assumption for novel view synthesis. A neural radiance field (NeRF) is a coordinate-based neural scene representation, where opacity \(\tau^{\theta}:\mathbb{R}^{3}\rightarrow\mathbb{R}_{\geq 0}\) and color \(c^{\psi}:\mathbb{R}^{3}\times\mathbb{S}^{2}\rightarrow[0,255]^{3}\) are predicted at each continuous coordinate by parameterizing them as a neural network. To train the neural network, 2D images are used as supervision where each viewing ray is associated with a ground truth color. Volume rendering allows for the 3D coordinate outputs to be aggregated into an observed pixel color allowing for end-to-end training with 2D supervision. The supervision signal are on the coordinates of the ray samples \(s_{1},...,s_{N}\), which updates the corresponding output opacity and color at those samples. NeRF uses a importance sampling strategy by drawing samples from the ray distribution from a coarse network to generate better samples for rendering of their fine network. To sample from a distribution, inverse transform sampling is needed, that is, one draws \(u\sim U(0,1)\) then passes it to the inverse of a cumulative distribution (CDF), i.e. a sample \(x=F^{-1}(u)\), where \(F\) is the CDF of the distribution. Under the piecewise constant assumption, the CDF of the discrete random variable \(\tilde{s}\) is given by:

\[\tilde{F}(x)=\begin{cases}0&x\leq s_{0}\\ \sum_{k<j}\tilde{P}(s_{k})&1\leq x<s_{j+1}\text{ for all }j\in\{1,...,N-1\}\\ 1&x>s_{N}\end{cases}.\] (8)

This CDF is however non-continuous and non-invertible. NeRF's approach to get around this is to define a surrogate invertible function \(G\) derived from its CDF, then taking \(x=G^{-1}(u)\). Concretely, \(G(y)=\frac{y-s_{j-1}}{s_{j}-s_{j-1}}\tilde{F}(s_{j})+\frac{s_{j}-y}{s_{j}-s_{ j-1}}\tilde{F}(s_{j-1}),\) where \(y\in[s_{j-1},s_{j}]\). However, this does not necessarily result in the samples from the actual ray distribution \(p(s)\) from the model.

## 4 Drawbacks of Piecewise Constant Opacity \(\tau\) in NeRFs

Unfortunately, there are properties associated with the piecewise constant opacity formulation that may not be desirable in the context of NeRFs. First, it is sensitive to the choice of samples, i.e. sample positions \(s\), along the ray, a phenomenon we dub as _quadrature instability_. This quadrature instability is due to the assumption that all points in an interval take the opacity of the left bin (Eq 5), making it sensitive to sample positions. As illustrated in Figure 1, this would lead to _ray conflicts_ in optimizing a NeRF when you have rays that are directly facing, i.e. perpendicular to, the surface (yellow rays) and rays that are close to the grazing angle (red ray), i.e. parallel to, the object. To render the perpendicular rays correctly, vanilla NeRF has to store the optical properties (opacity and color) associated with the perpendicular rays at a point before its intersection with the surface. This creates inaccurate signals to the optimization process when NeRF renders the ray at a grazing angle, as it will cross multiple conflicting opacity/colors (illustrated by the blue gradient). The sample sensitivity issue also arises when having cameras at different distances from the object as illustrated in Fig. 2, as this would lead to shifted sets of samples, causing inconsistencies when rendering at different camera-to-object distances. Notice that the noise on the texture of the chair is different across different viewing distances, where the middle view has fewer artifacts compared to the closer and further views.

## 5 Generalized Form for \(P_{j}\)

We first show a generalized derivation for the probability \(P_{j}\) of each interval \(I_{j}\), which we use to formulate our approach that alleviates the problems described above. From \(T(s)=\exp{(-\int_{0}^{s}\tau(u)\mathrm{d}u)}\), we first notice that:

\[\frac{\mathrm{d}T}{\mathrm{d}s} =-\exp{(-\int_{0}^{s}\tau(u)\mathrm{d}u)}\tau(s)=-T(s)\tau(s)\] \[T^{\prime}(s) =-T(s)\tau(s).\]

This results in the probability of each interval \(I_{j}\) given as follows:

\[P_{j}=\int_{s_{j}}^{s_{j+1}}\tau(s)T(s)\,\mathrm{d}s=-\int_{s_{j}}^{s_{j+1}}T^ {\prime}(s)\,\mathrm{d}s=T(s_{j})-T(s_{j+1}).\] (9)

Since \(s_{j}\)'s are arbitrarily sampled, \(P_{j}\) can be exactly evaluated in a closed-form expression, if and only if \(T(\cdot)\) is in closed-form.

## 6 Our PL-NeRF

We observe from Eq. 15 that we can obtain a closed-form expression for \(P_{j}\) for any piecewise polynomial function in \(\tau\), which can be of any degree \(d=0,1,2,...,n\). Commonly used in existing NeRF literature is choosing \(d=0\), i.e. piecewise constant, that is unstable w.r.t. the choice of samples as highlighted in the previous section. Interestingly, we also observe and show that the problem becomes numerically ill-conditioned for \(d\geq 2\) making it difficult and unstable to optimize. Please see supplementary for the full proof. Hence, we propose to make opacity piecewise linear (\(d=1\)), which we call **PL-NeRF**, leading to a _simple_ and _closed-form_ expression for the volume rendering integral that is numerically stable and is a drop-in replacement to existing NeRF-based methods. We show both theoretically and experimentally that the piecewise linear assumption is sufficient and alleviates the problems caused by quadrature instability under the piecewise constant assumption.

Figure 2: **Ray Conflicts: Different Camera-to-Scene Distances. (Left) Rendered views from cameras at different distances from the object. At all distances, the rendered output for linear have sharper texture than constant because of the latterâ€™s sensitivity to the choice of samples. We also highlight the instability of the constant model as shown by the noisier texture of the middle view compared to the closer and further views. (Right) An illustration that moving the camera to different distances from the object result in different samples that lead to conflicts.**

The second issue comes from the CDF \(\tilde{F}\) being piecewise constant (Eq 8). This leads to two consequences. First, the piecewise constant assumption makes \(\tilde{F}\) non-invertible, hence, as mentioned in the previous section, importance sampling needs to be performed via a surrogate function \(G\). This results in uniformity across the samples within the bin - samples within a bin are assigned with equal probability, leading to imprecise importance samples. The second consequence comes from the fact that \(\tilde{F}\) is not continuous, leading to an issue in training a NeRF that has a loss based on its samples. In other words, there will be a vanishing gradient effect when taking the gradient w.r.t. the samples, and one such example of a sample-based loss used for NeRFs is depth [28].**Volume Rendering with Piecewise Linear Opacity.We propose an elegant reformulation to the sample-based rendering equation that corresponds to the exact integral under **piecewise linear** opacity while keeping piecewise constant color leading to a simple and closed-form expression for the integral. That is, instead of piecewise constant opacity as in Eq 5, we assume a linear opacity for each interval \(I_{j}\). Concretely, for \(s\in[s_{j},s_{j+1}]\), where \(\tau_{j}=\tau(s_{j}),\tau_{j+1}=\tau(s_{j+1})\), we have

\[\tau(s)=\left(\frac{s_{j+1}-s}{s_{j+1}-s_{j}}\right)\tau_{j}+\left(\frac{s-s_{ j}}{s_{j+1}-s_{j}}\right)\tau_{j+1}.\] (10)

which is linear w.r.t. \(s\in[s_{j},s_{j+1}]\) as illustrated in Fig. 3.

Now, under the piecewise linear opacity assumption, transmittance is derived as the following closed-form expression:

\[T(s_{j})=\exp\left[-\int_{0}^{s_{j}}\tau(u)\,\mathrm{d}u\right]= \prod_{k=1}^{i}\exp\left[-\int_{s_{k-1}}^{s_{k}}\tau(u)\,\mathrm{d}u\right],\] \[\boxed{T(s_{j})=\prod_{k=1}^{i}\exp\left[-\frac{(\tau_{k}+\tau_{k -1})(s_{k}-s_{k-1})}{2}\right].}\] (11)

Together with Eq. 15, this leads to the following simple and closed-form expression for \(P_{j}\), corresponding to the exact integral under the piecewise linear opacity assumption:

\[\boxed{P_{j}=T(s_{j})\cdot\left(1-\exp\left[-\frac{(\tau_{j+1}+\tau_{j})(s_{j +1}-s_{j})}{2}\right]\right).}\] (12)

Precision Importance Sampling.Moreover, it also turns out that with our piecewise linear opacity assumption, we are able to derive an exact closed-form solution for inverse transform sampling. Recall that in Sec 4, we pointed a drawback of the CDF \(\tilde{F}\) being non-invertible and discontinuous under piecewise constant opacity. We show that this is alleviated in our piecewise linear setting. Concretely, given samples \(s_{1},...,s_{N}\) resulting in interval probabilities \(P_{1},...,{P_{N}}\)2 from our derivation (Eq 12), the CDF for _continuous_ random variable \(t\) is then given as

Footnote 2: We note that DS-NeRF [6] shows that this will sum to 1 assuming an opaque far plane. \(s_{N+1}\) would correspond to the far plane.

\[F(t)=\int_{0}^{t}p(s)\,\mathrm{d}s=\sum_{s_{j}<t}P_{j}+\int_{s_{j}}^{t}p(s)\, \mathrm{d}s=\sum_{s_{j}<t}P_{j}+\int_{s_{j}}^{t}\tau(s)T(s)\,\mathrm{d}s.\] (13)

Note that unlike in piecewise constant opacity, we do not convert a continuous random variable \(s\) to a discrete random variable \(\tilde{s}\), thus, the resulting CDF \(F\) being continuous. Now, assuming that opacity \(\tau\geq 0\) everywhere, from Eq. 13 we see that \(F\) is strictly increasing. Since \(F\) is continuous and strictly increasing, then it is invertible.

Finally, we have our precision importance sampling, where by inverse transform sampling, we can solve for the exact sample \(x=F^{-1}(u)\) for \(u\sim U(0,1)\) from the given ray distribution \(p(s)\) since the CDF \(F\) is invertible under piecewise linear opacity. That is, without loss of generality, let sample \(u\sim U(0,1)\) fall into the bin \(u\in[C_{k},C_{k+1}]\), where \(C_{k}=\sum_{j<k}P_{j}\), which is equivalent to solving for \(x\in[s_{k},s_{k+1}]\). Reparameterizing \(x=s_{k}+t\), where \(t\in[0,s_{k+1}-s_{k}]\), the exact solution for sample \(u\) is given by

\[\boxed{t=\frac{s_{k+1}-s_{k}}{\tau_{k+1}-\tau_{k}}\left[-\tau_{k}+\sqrt{\tau_ {k}^{2}+\frac{2(\tau_{k+1}-\tau_{k})\left(-\ln\frac{1-u}{T(s_{k})}\right)}{(s_ {k+1}-s_{k})}}\right].}\] (14)

Please see supplementary for full derivation. This leads to precisely sampling from the ray distribution \(p(s)\) resulting in better importance sampling and stronger depth supervision.

Figure 3: Illustration of opacities \(\tau\) along a ray under the piecewise constant (green) and piecewise linear (orange) assumptions.

## 7 Results

In this section, we present our experimental evaluations to demonstrate the advantages our piecewise linear opacity formulation for volume rendering, which we call **PL-NeRF**.

### Datasets, Evaluation Metrics and Implementation Details.

**Datasets and Evaluation Metrics.** We evaluate our method on the standard datasets: Blender and Real Forward Facing (LLFF) datasets as used in [18]. We use the released training and test splits for each. See supplementary for more details. For quantitative comparison, we follow the standard evaluation metrics and report PSNR, SSIM [32] and LPIPS [41] on unseen test views. We also report the root-mean-squared-error (RSME) on the expected ray termination in our depth experiments.

**Implementation Details.** **PL-NeRF** is implemented on top of NeRF-Pytorch [38], a reproducible Pytorch implementation of the constant (vanilla) NeRF, where we simply change the volume rendering to our formulation under piecewise linear opacity and utilize our exact importance sampling derivation. Similar to [18] we optimize a separate network for the coarse and fine models that are jointly trained with the MSE loss on ground truth images. We use a batch size of 1024 rays and a learning rate of \(5\times 10^{-4}\) that decays exponentially to \(5\times 10^{-5}\) throughout the course of optimization. We train each scene for 500k iterations which takes \(\sim 21\) hours on a single Nvidia V100 GPU 3. Our precision importance sampling enables us to use fewer samples for the fine network, hence keeping the total number of rendering samples the same, we use 128 coarse samples and 64 fine samples to train and test our method.

Footnote 3: We rerun and train the vanilla (constant) model using the released reproducible configs from [38].

### Experiments on Blender and LLFF Datasets

We first evaluate our **PL-NeRF** on the standard Blender and Real Forward Facing datasets. Table 1 shows that our **PL-NeRF** (linear) outperforms the vanilla [18] (constant) model that assumes piecewise constant opacity in all metrics for both the synthetic Blender and Real Forward Facing datasets. Figure 1 and Figure 4 show qualitative results. As shown our **PL-NeRF** is able to achieve sharper

\begin{table}
\begin{tabular}{c l|c|c c c c c c c c}  & **Blender** & Avg. & Chair & Drums & Ficus & Hotdog & Lego & Mat. & Mic & Ship \\ \hline \multirow{2}{*}{PSNR\({}^{\dagger}\)} & Const. (Vanilla) & 30.61 & 32.54 & 24.79 & 29.63 & 36.08 & 32.01 & 29.31 & 32.55 & 27.95 \\  & Linear (Ours) & **31.10** & **32.92** & **25.07** & **30.18** & **36.46** & **32.90** & **29.52** & **33.08** & **28.71** \\ \hline \multirow{2}{*}{SSIM\({}^{\dagger}\)} & Const. (Vanilla) & 0.943 & 0.966 & 0.918 & 0.960 & 0.975 & 0.959 & 0.943 & 0.978 & 0.846 \\  & Linear (Ours) & **0.948** & **0.969** & **0.923** & **0.965** & **0.977** & **0.966** & **0.948** & **0.981** & **0.857** \\ \hline \multirow{2}{*}{LPIPS\({}^{\dagger}\)} & Const. (Vanilla) & 5.17 & 3.19 & 7.97 & 4.14 & 2.48 & 2.33 & 4.32 & 2.16 & 14.8 \\  & Linear (Ours) & **4.39** & **2.85** & **7.10** & **3.03** & **2.28** & **1.81** & **3.21** & **1.73** & **13.1** \\ \hline \hline \multirow{2}{*}{PSNR\({}^{\dagger}\)} & **LLFF** & Avg. & Fern & Flower & Fortress & Horns & Leaves & Orchid & Room & Trex \\ \hline \multirow{2}{*}{PSNR\({}^{\dagger}\)} & Const. (Vanilla) & 27.53 & 26.79 & 28.23 & 32.53 & 28.54 & 22.35 & 21.20 & 33.03 & 27.58 \\  & Linear (Ours) & **28.05** & **26.85** & **28.71** & **32.95** & **29.38** & **22.51** & **21.25** & **33.99** & **28.79** \\ \hline \multirow{2}{*}{SSIM\({}^{\dagger}\)} & Const. (Vanilla) & 0.874 & 0.746 & 0.886 & 0.925 & 0.893 & 0.816 & 0.746 & 0.956 & 0.916 \\  & Linear (Ours) & **0.885** & **0.863** & **0.902** & **0.932** & **0.911** & **0.826** & **0.754** & **0.961** & **0.933** \\ \hline \multirow{2}{*}{LPIPS\({}^{\dagger}\)} & Const. (Vanilla) & 7.37 & 9.67 & 6.34 & 2.92 & 7.26 & 11.0 & 11.8 & 4.33 & 5.66 \\  & Linear (Ours) & **6.06** & **7.92** & **4.93** & **2.46** & **5.51** & **9.59** & **10.2** & **3.54** & **4.38** \\ \hline \end{tabular}
\end{table}
Table 1: **Quantitative Results on Blender and LLFF Datasets. LPIPS scores \(\times 10^{2}\).**

Figure 4: **Qualitative Results for Blender and Real Forward Facing.**textures as shown in the Lego's scooper and the bread's surface in the hodog. Moreover, our approach is also able to recover less fuzzy surfaces as shown in the microphone scene (Figure 1) where training views are close to the grazing angle of its head. As illustrated, the resulting probability density of the ray corresponding to the marked pixel is peakier than constant as our precision importance sampling allows us to have better samples closer to the surface. We also see clearer ropes in ship, less cloudy interior of the drum, and more solid surfaces such as cleaner leg of the swivel chair in the room scene.

### Geometric Extraction

We also show improvement in geometric reconstruction of **PL-NeRF**. We extract the geometry from the learned density field from the trained models of PL-NeRF and Vanilla NeRF using marching cubes with a threshold of 25 following [30]. Figure 5 shows qualitative results on the reconstruction of our piecewise linear vs the original piecewise constant formulation. As shown, we are able to better recover the holes on the body and wheels of the Lego scene as well as the interior structure inside the Mic. Moreover, interestingly, the surface of the drum is reconstructed to be transparent as visually depicted in the images, as opposed to the ground truth being opaque.

### Effectiveness of our formulation on other Radiance Field Methods

We also demonstrate our formulation's effectiveness on other radiance field methods and show that our approach can be used as a drop-in replacement to existing NeRF-based methods. We integrate our piecewise linear opacity formulation to the volume rendering integral into Mip-NeRF (**PL-MipNeRF**). Table 2 shows our quantitative results demonstrating consistent improvement across all scenes in the original hemisphere Blender dataset. Figure 6 shows qualitative examples where we see that under difficult scenarios such as when ray conflicts arise in the fine details of the Chair and in the presence of grazing angle views in the Mic, our PL-MipNeRF shows significant improvement over the baseline. Our results show that our piecewise linear opacity and piecewise constant color formulation scales well to Mip-NeRF as well. See supplementary for implementation details. We also plug our

\begin{table}
\begin{tabular}{l l|c|c c c c c c c} \hline \multicolumn{2}{c|}{**Blender**} & Avg. & Chair & Drums & Ficus & Hotdog & Lego & Mat. & Mic & Ship \\ \hline \multirow{2}{*}{PSNR\(\uparrow\)} & Mip-NeRF & 31.76 & 33.95 & 24.39 & 31.20 & 36.12 & 33.84 & 30.55 & 34.63 & 29.41 \\  & PL-MipNeRF & **32.48** & **35.11** & **24.92** & **32.25** & **36.51** & **35.15** & **30.69** & **35.22** & **30.00** \\ \hline \multirow{2}{*}{SSIM\(\uparrow\)} & Mip-NeRF & 0.955 & 0.975 & 0.921 & 0.971 & 0.978 & 0.971 & 0.957 & 0.987 & 0.876 \\  & PL-MipNeRF & **0.959** & **0.981** & **0.928** & **0.977** & **0.980** & **0.976** & **0.959** & **0.989** & **0.882** \\ \hline \multirow{2}{*}{LPIPS\(\downarrow\)} & Mip-NeRF & 3.64 & 1.80 & 6.82 & 2.35 & 1.97 & 1.44 & 2.39 & 0.973 & 11.4 \\  & PL-MipNeRF & **3.09** & **1.32** & **5.78** & **1.66** & **1.67** & **1.07** & **2.09** & **0.788** & **10.3** \\ \hline \end{tabular}
\end{table}
Table 2: **Quantitative Results of Mip-NeRF v.s. PL-MipNeRF** LPIPS scores \(\times 10^{2}\).

Figure 5: Geometry Extraction Qualitative Examples.

Figure 6: **Qualitative Results for Mip-NeRF vs PL-MipNeRF.** We see that under difficult scenarios such as in fine texture details of the chair and grazing angle views on the mic, PL-MipNeRF visually shows significant improvement.

piecewise linear opacity formulation into DIVeR [36], a voxel-based NeRF model, and show that our formulation is also an effective drop-in replacement outperforming the original DIVeR [36]. Please see supplementary for experiment results and implementation details.

### Experiments on Close-up Views

We further consider the challenging setting of testing on cameras closer to the objects. Table 3 (top) shows quantitative results when training on the original hemisphere dataset and tested on different close-up views. As shown by the drop in metrics, the difficulty increases as the camera moves closer to the object (0.75x to 0.5x to 0.25x of the original radius) where details get more apparent. Our **PL-NeRF** outperforms the vanilla piecewise constant model in all settings and the gap (SSIM and LPIPS) between ours and the constant assumption increases as the setting becomes harder highlighting the importance of recovering shaper texture and less fuzzy surfaces.

We also consider the set-up of training with cameras with different distances to the object that result in different sets of ray samples causing conflicts. We generate training views following the data processing pipeline from [18] with a random distance scaling factor sampled from \(U(0.5,1.0)\). As shown in Table 3 (bottom), our **PL-NeRF** outperforms the vanilla constant baseline in all metrics across different camera distances, where the gap (LPIPS and SSIM) is also larger the closer the camera is to the object, where details are more apparent. The difficulty for the constant (vanilla) case under multiple camera distances is its sensitivity to the choice of samples along the ray. Figure 2 shows that the conflicting rays cause quadrature instability for under piecewise constant opacity leads to unstable outputs as shown by the noisy texture on the chair. For the constant, the level of noise (gold specs) and blurriness vary at different camera distances, whereas our **PL-NeRF** renders crisper and more consistent outputs even as the camera is moved closer or further for the object.

### Experiments with Depth Supervision

Finally, we also show that our **PL-NeRF** enables stronger depth supervision under our piecewise linear opacity assumption due our precision importance sampling that allows for gradients to flow to these more refined samples resulting in more accurate depth. As in previous works [28], we use a sample-based loss for to incorporate depth supervision4. Table 4 shows quantitative results when training and testing on the less constrained Blender dataset with cameras at random distances from the object, as described in the previous section, with depth supervision. As shown, our **PL-NeRF** outperforms the vanilla constant baseline on all metrics including depth RSME demonstrating that our approach allows for stronger depth supervision.

Footnote 4: We use the original hyperparameters from [28] in this experiment.

## 8 Conclusion

We proposed a new way to approximate the volume rendering integral that avoids quadrature instability, by considering a piecewise linear approximation to opacity and a piecewise constant approximation to color. We showed that this results in a simple closed-form expression for the integral that is easy to evaluate. We turned this into a new objective for training NeRFs that is a drop-in replacement to existing methods and demonstrated improved rendering quality and geometric reconstruction, more accurate importance sampling and stronger depth supervision.

\begin{table}
\begin{tabular}{l|c|c c c}  & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & RMSE\(\downarrow\) \\ \hline
**Const. (Vanilla)** & 29.20 & 0.898 & 11.2 & 0.178 \\
**Linear (Ours)** & **29.54** & **0.905** & **10.4** & **0.147** \\ \hline \end{tabular}
\end{table}
Table 4: **Depth Supervision. Reported LPIPS score is multiplied by \(10^{2}\).**

\begin{table}
\begin{tabular}{l|c||c c c|c c c|c c c}  & \multicolumn{3}{c}{Dist 0.25x} & \multicolumn{3}{c}{Dist 0.5x} & \multicolumn{3}{c}{Dist 0.75x} \\ \cline{3-10} Train Set & & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline \multirow{2}{*}{**Hemisphere**} & Const. (Vanilla) & 20.18 & 0.612 & 54.1 & 22.80 & 0.753 & 30.4 & 25.97 & 0.867 & 14.1 \\  & Linear (Ours) & **20.34** & **0.637** & **50.0** & **23.00** & **0.767** & **27.5** & **26.28** & **0.876** & **12.6** \\ \hline \multirow{2}{*}{**Multi Dist.**} & Const. (Vanilla) & 22.30 & 0.677 & 45.7 & 25.51 & 0.811 & 23.1 & 28.02 & 0.891 & 11.3 \\  & Linear (Ours) & **22.66** & **0.705** & **41.1** & **26.04** & **0.828** & **20.3** & **28.55** & **0.900** & **9.90** \\ \hline \end{tabular}
\end{table}
Table 3: **Testing on close-up views:**_Hemisphere are with training cameras located on the original hemisphere. _Multi Dist._ with training cameras are at random distances across a depth scale range of \(0.5-1.0\) of the original hemisphere. Reported LPIPS score is multiplied by \(10^{2}\).

Acknowledgements.This work is supported by a Apple Scholars in AI/ML PhD Fellowship, a Snap Research Fellowship, a Vannevar Bush Faculty Fellowship, ARL grant W911NF-21-2-0104, a gift from the Adobe corporation, the Natural Sciences and Engineering Research Council of Canada (NSERC), the BC DRI Group and the Digital Research Alliance of Canada.

## References

* [1] Henrik Aanaes, Rasmus Ramsbol Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. _International Journal of Computer Vision_, pages 1-16, 2016.
* [2] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _ICCV_, 2021.
* [3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In _CVPR_, 2022.
* [4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. _ICCV_, 2023.
* [5] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In _CVPR_, 2023.
* [6] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In _CVPR_, pages 12882-12891, 2022.
* [7] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Volume rendering. _SIGGRAPH_, 22(4):65-74, 1988.
* [8] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In _ICCV_, pages 5875-5884, 2021.
* [9] Hiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru Matsuoka, Wadim Kehl, and Adrien Gaidon. Differentiable rendering: A survey. _ArXiv_, abs/2006.12057, 2020.
* [10] Marc Levoy. Efficient ray tracing of volume data. _ACM Transactions on Graphics (TOG)_, 9(3):245-261, 1990.
* [11] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In _CVPR_, pages 6498-6508, 2021.
* [12] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. In _NeurIPS_, 2020.
* [13] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In _CVPR_, 2021.
* [14] Nelson Max. Optical models for direct volume rendering. _IEEE Transactions on Visualization and Computer Graphics_, 1(2):99-108, 1995.
* [15] Nelson Max and Min Chen. Local and global illumination in the volume rendering integral. In Hans Hagen, editor, _Scientific Visualization: Advanced Concepts_, volume 1 of _Dagstuhl Follow-Ups_, pages 259-274, Dagstuhl, Germany, 2010. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
* [16] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan, and Jonathan T. Barron. NCRF in the dark: High dynamic range view synthesis from noisy raw images. In _CVPR_, 2022.
* [17] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. _ACM Transactions on Graphics_, 2019.
* [18] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.
* [19] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In _ICCV_, 2021.
* [20] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. _ACM Transactions on Graphics_, 40(6), dec 2021.
* [21] Steven M Rubin and Turner Whitted. A 3-dimensional representation for fast rendering of complex scenes. In _Proceedings of the 7th annual conference on Computer graphics and interactive techniques_, pages 110-116, 1980.
* [22] Hanan Samet. _The design and analysis of spatial data structures_, volume 85. Addison-wesley Reading, MA, 1990.
* [23] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _CVPR_, 2022.
* [24] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Improved direct voxel grid optimization for radiance fields reconstruction. _arXiv preprint arXiv:2206.05085_, 2022.
* [25] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In _CVPR_, 2022.

* [26] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _NeurIPS_, 33:7537-7547, 2020.
* [27] Matthew Tancik, Ethan Weber, Evonne Ng, Ruulong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. Nerfstudio: A modular framework for neural radiance field development. In _ACM SIGGRAPH 2023 Conference Proceedings_, 2023.
* [28] Mikaela Angelina Up, Ricardo Martin-Brualla, Leonidas Guibas, and Ke Li. Scade: Nerfs from space carving with ambiguity-aware depth estimates. In _CVPR_, 2023.
* [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* [30] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In _NeurIPS_, 2021.
* [31] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In _ICCV_, 2021.
* [32] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.
* [33] Lee Westover. Interactive volume rendering. In _Proceedings of the 1989 Chapel Hill workshop on Volume visualization_, pages 9-16, 1989.
* [34] Peter L Williams and Nelson Max. A volume density optical model. pages 61-68, 1992.
* [35] Peter L Williams, Nelson L Max, and Clifford M Stein. A high accuracy volume renderer for unstructured data. _IEEE Transactions on Visualization and Computer Graphics_, 4(1):37-54, 1998.
* [36] Liwen Wu, Jae Yong Lee, Anand Bhattad, Yuxiong Wang, and David Forsyth. Diver: Real-time and accurate neural radiance fields with deterministic integration for volume rendering. In _CVPR_, 2022.
* [37] Yuanbo Xiangli, Limning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In _ECCV_, 2022.
* [38] Lin Yen-Chen. Nerf-pytorch. https://github.com/yenchenlin/nerf-pytorch/, 2020.
* [39] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In _CVPR_, pages 4578-4587, 2021.
* [40] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. _ArXiv_, abs/2010.07492, 2020.
* [41] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, pages 586-595, 2018.

## Appendix

We conduct further experiments, analysis and discussions on our proposed reformulation, where we take a piecewise _linear_ approximation to opacity and piecewise _constant_ approximation to color that results in an integral that is a simple and closed-form expression. This allows us to address the drawbacks of current piecewise constant assumption in NeRFs such as ray conflicts during optimization and a non-invertible CDF that lead to imprecise importance samples and vanishing gradients. We provide additional results in Sec A.1: ablation study (Sec A.1.1), a video demo (Sec A.1.2), additional results on a real dataset (Sec A.1.3), additional qualitative results (Sec A.1.4), comparison with PL-DIVeR A.1.5, additional geometric extraction results A.1.6 and comparison with less number of samples A.1.7. We then provide a walkthrough of the piecewise constant derivation from [15] (Sec A.2), which is followed by the thorough step-by-step derivation of our piecewise linear opacity in volume rendering and precise importance sampling (Sec A.3). We also include analyses on piecewise quadratic and higher order polynomials in Sec A.4. Finally, we end with additional implementation and experiment details (Sec A.5), Limitations (Sec A.6) and Societal Impact (Sec A.7).

### Additional Results

#### a.1.1 Ablation Study

We conduct a further ablation study on our precise importance sampling. As described in Sec. 4 in the main paper, our piecewise linear opacity approximation allows to solve for a closed-form solution for inverse transform sampling leading to the formulation of our **precise** importance sampling. Unlike the vanilla piecewise constant opacity approximation, our approach results in an invertible CDF, and hence we do not need to define an invertible surrogate function \(G\) for inverse transform sampling as in piecewise constant opacity (see Eq. 8 of main paper), which does not necessarily result in samples from the actual ray distribution \(p(s)\). We quantitatively ablate the effectiveness of our precise importance sampling (**Precise**) by replacing our formulation (Eq. 13 main paper) with the surrogate function \(G\) as in the vanilla constant setting (**Surrogate**). As shown in Table A1 (first and second row of each metric), our precise importance sampling consistently outperforms using the surrogate on all metrics across all 8 scenes in the Blender dataset (hemisphere). Moreover, we also show that our precise importance sampling enables us to use fewer samples for the fine network as it is able to sample correctly from the ray distribution. Hence, keeping the same total number of rendering samples, we are able to achieve a further boost in performance by using 128 coarse and 64 fine samples as shown in Table A1 (second and third row of each metric). We use \(N_{c}\) and \(N_{i}\) to denote the number of coarse and fine samples, respectively.

#### a.1.2 Video Demo

We also show a video demo of our results. Please see our project page (pl-nerf.github.io) for the video (best viewed in full screen). In the mic scene, we observe that the structure inside the mic is lost in the piecewise constant opacity approximation. Moreover, we see the blurriness along the sides of the body of the mic caused by ray conflicts from the perpendicular and grazing angle views during optimization of the vanilla NeRF model. For the chair scene, we see that our **PL-NeRF** is able to achieve shaper textures on the back of the chair. Moreover, we see that there is an instability to the samples as illustrated by the inconsistencies in the noise - the gold specs on the green texture (please view in full screen). This is also evident when we vary the camera-to-scene distance. The chair model was trained on multi-distance Blender data to highlight the difference when camera distances vary.

#### a.1.3 Real Dataset Results

We further evaluate our **PL-NeRF** on a real dataset - DTU [1]. We train and evaluate our approach on the 15 test scenes used in [39], and report the standard metrics (PSNR, SSIM, LPIPS). We follow the protocol used in [18] for the Real Forward Facing scene where \(\frac{1}{8}\) of the views were held out for testing while the rest are used for training. Table A2 shows the quantitative results averaged over the 15 scenes, where our **PL-NeRF** outperforms the constant [18] baseline.

Figure A1: **DTU Qualitative Result** Visualizations of rendered DTU dataset test views. Because of the issue of grazing angle and binning inaccuracy of the piecewise constant opacity assumption, the vanilla NeRF (**constant**) exhibits blurry geometry, and rendering artifacts in the zoomed-in views (middle column). On the other hand, the piecewise linear opacity assumption in our model (**linear**) alleviates these issues (right column). Overall, the rendered views exhibit sharper surface boundaries and more faithful reconstruction compared to the constant model.

Figure A2: **Blender Qualitative Result** Additonal visualizations of rendered Blender dataset test views. We see that our **PL-NeRF** is able to achieve sharper and crisper texture (chair and hotdog surface), better capture fine geometric detail (hole in lego, rope in ship) and avoid blurriness caused by conflicting rays, e.g. grazing angle views as shown in the mic.

#### a.1.4 Additional Qualitative Results

We additionally show qualitative results from DTU dataset in Fig. A1. More qualitative results are also shown in Fig. A2.

#### a.1.5 Pl-DiVer

We plug our method into DIVeR by using their voxel-based representation and feature integration, and dropping in our piecewise linear opacity formulation for volume rendering (PL-DIVeR). Results are shown in Table A3 demonstrating that our approach is on-par if not better across the different scenes in the Blender dataset. We highlight that this shows the improvement of using our piecewise linear opacity formulation, which is a drop-in replacement to existing methods.

#### a.1.6 Geometric Extraction

We also show quantitative results in geometric extraction improvement of PL-NeRF compared to the original Vanilla NeRF. Table A4 reports the distance between the surface of the ground truth model to the predicted meshes by sampling point clouds via ray casting. We see that our piecewise linear approach achieves a lower error compared to Vanilla NeRF on almost all the scenes in the Blender dataset.

#### a.1.7 Comparison with Less Samples

We run both our PL-NeRF and Vanilla NeRF with 64 coarse and 64 fine samples results in an average of (30.09, 0.939, 0.056) and (29.86, 0.937, 0.059) for (PSNR, SSIM, LPIPS), respectively, on the Blender dataset. This shows that under less number of samples our piecewise linear opacity formulation is better than the original piecewise constant opacity assumption.

### Volume Rendering: Walkthrough of Piecewise Constant Derivation from [15]

From [15], under the approximation that both opacity and color are piecewise constant, for \(s\in[s_{i},s_{i+1}]\), where \(\tau_{i}=\tau(s_{i})\) and \(\tau(s)=\tau_{i}\forall s\in[s_{i},s_{i+1}]\), the probability of the interval \(P_{i}\) is derived as follows:

\[P_{i} =\int_{s_{i}}^{s_{i+1}}\tau(s)T(s)\mathrm{d}s\] \[=\int_{s_{i}}^{s_{i+1}}\tau_{i}\exp{(-\int_{0}^{s}\tau(u)\mathrm{ d}u)}\mathrm{d}s\] \[=\int_{s_{i}}^{s_{i+1}}\tau_{i}\exp{(-\int_{0}^{s_{i}}\tau(u) \mathrm{d}u)}\exp{(-\int_{s_{i}}^{s}\tau(u)\mathrm{d}u)}\mathrm{d}s\] \[=\int_{s_{i}}^{s_{i+1}}\tau_{i}T(s_{i})\exp{(-\int_{s_{i}}^{s} \tau_{i}\mathrm{d}u)}\mathrm{d}s\] \[=\tau_{i}T(s_{i})\int_{s_{i}}^{s_{i+1}}\exp{(-\tau_{i}(s-s_{i})) }\mathrm{d}s\] \[=\tau_{i}T(s_{i})\frac{\exp{(-\tau_{i}(s-s_{i}))}}{-\tau_{i}} \Big{|}_{s_{i}}^{s_{i+1}}\] \[=\tau_{i}T(s_{i})\Big{(}1-\exp{(-\tau_{i}(s_{i+1}-s_{i}))}\Big{)}.\]

Moreover, under the piecewise constant assumption, transmittance \(T\) is derived and given by:

\[T(s_{i}) =\exp{(-\int_{0}^{s_{i}}\tau(u)\mathrm{d}u)}\] \[=\prod_{j=1}^{i}\exp{(-\int_{s_{j-1}}^{s_{j}}\tau(u)\mathrm{d}u)}\] \[=\prod_{j=1}^{i}\exp{(-\int_{s_{j-1}}^{s_{j}}\tau_{j-1}\mathrm{d}u)}\] \[=\prod_{j=1}^{i}\exp{(\tau_{j-1}(s_{j}-s_{j-1}))}.\]

This is the formulation that is used in most, if not all, NeRF works, which has the drawbacks that we raised such as ray conflicts during NeRF optimization and non-invertible CDF causing imprecise importance sampling and vanishing gradients when defining a loss w.r.t. the samples. We propose to approach this issue by deriving the volume rendering equation under a piecewise linear approximation to opacity, which we detail in the next sections.

\begin{table}
\begin{tabular}{c c|c|c c c c c c c} \hline \multicolumn{2}{c|}{**Blender**} & Avg. & Chair & Drums & Ficus & Hotdog & Lego & Mat. & Mic & Ship \\ \hline \multirow{2}{*}{CD\(\downarrow\)} & Vanilla NeRF & 10.43 & 5.162 & **6.842** & 29.94 & 7.555 & 7.474 & 6.833 & 5.214 & 11.44 \\  & PL-NeRF & **10.10** & **4.676** & 7.754 & **29.58** & **7.004** & **6.825** & **6.061** & **5.213** & **10.44** \\ \hline \end{tabular}
\end{table}
Table A4: **Geometry Extraction Error** Distance between the surface of the GT to the predicted meshes. Scores are \(\times 10^{3}\)

### Volume Rendering: Our Piecewise Linear \(\tau\) Derivation

We now show our full derivation for the volume rendering equation, under the assumption that the opacity \(\tau(s)\) is piecewise linear, i.e. it is linear within each interval \([s_{i},s_{i+1}]\), and piecewise constant color. We then derive the probability of an interval under this assumption.

#### a.3.1 Generalized form for \(P_{i}\).

Recall the generalized form of \(P_{i}\) as derived in the main paper. First from the definition of transmittance, we have

\[T(s) =\exp{(-\int_{0}^{s}\tau(u)\mathrm{d}u)}\] \[\frac{\mathrm{d}T}{\mathrm{d}s} =-\exp{(-\int_{0}^{s}\tau(u)\mathrm{d}u)}\tau(s)=-T(s)\tau(s)\] \[T^{\prime}(s) =-T(s)\tau(s).\]

This results in the exact expression for the probability \(P_{i}\) of an interval given as follows:

\[P_{i}=\int_{s_{i}}^{s_{i+1}}\tau(s)T(s)\,\mathrm{d}s=-\int_{s_{i}}^{s_{i+1}}T^{ \prime}(s)\,\mathrm{d}s=T(s_{i})-T(s_{i+1}).\] (15)

#### a.3.2 Evaluating \(\tau(s)\) for \(s\in[s_{i},s_{i+1}]\).

Let \(\tau_{j}=\tau(s_{i})\), \(\tau_{i+1}=\tau(s_{i+1})\), be sampled points along the ray. For \(s\in[s_{i},s_{i+1}]\), assuming piecewise linear opacity \(\tau\), i.e. \(\tau(s)\) is linear within each bin, we have

\[\tau(s) =(\frac{s_{i+1}-s}{s_{i+1}-s_{i}})\tau_{i}+(\frac{s-s_{i}}{s_{i+1} -s_{i}})\tau_{i+1}\] \[=\frac{1}{s_{i+1}-s_{i}}[(\tau_{i+1}-\tau_{i})s+(s_{i+1}\tau_{i}- s_{i}\tau_{i+1})]\]

#### a.3.3 Transmittance \(T(s_{i})\)

We first derive expression for transmittance \(T(s_{i})\) under the piecewise linear \(\tau\) assumption.

\[T(s_{i}) =\exp{(-\int_{0}^{s_{i}}\tau(u)\mathrm{d}u)}\] \[=\prod_{j=1}^{i}\exp{(-\int_{s_{j-1}}^{s_{j}}\tau(u)\mathrm{d}u)}\] \[=\prod_{j=1}^{i}\exp{(\frac{-1}{s_{j}-s_{j-1}}\int_{s_{j-1}}^{s_{j} }[(\tau_{j}-\tau_{j-1})u+(s_{j}\tau_{j-1}-s_{j-1}\tau_{j})]\mathrm{d}u)}\] \[=\prod_{j=1}^{i}\exp{\Big{(}\frac{-1}{s_{j}-s_{j-1}}\Big{[}(\frac{ \tau_{j}-\tau_{j-1}}{2})(s_{j}^{2}-s_{j-1}^{2})+(s_{j}\tau_{j-1}-s_{j-1}\tau_{ j})(s_{j}-s_{j-1})\Big{]}\Big{)}}\] \[=\prod_{j=1}^{i}\exp{\Big{(}-\Big{[}(\frac{\tau_{j}-\tau_{j-1}}{2 })(s_{j}+s_{j-1})+(s_{j}\tau_{j-1}-s_{j-1}\tau_{j})\Big{]}\Big{)}}\] \[=\prod_{j=1}^{i}\exp{\Big{(}-\frac{1}{2}\Big{[}\tau_{j}s_{j}+\tau _{j}s_{j-1}-\tau_{j-1}s_{j}-\tau_{j-1}s_{j-1}+2s_{j}\tau_{j-1}-2s_{j-1}\tau_{ j}\Big{]}\Big{)}}\] \[=\prod_{j=1}^{i}\exp{\Big{(}-\frac{1}{2}\Big{[}\tau_{j}s_{j}-\tau _{j}s_{j-1}+\tau_{j-1}s_{j}-\tau_{j-1}s_{j-1}\Big{]}\Big{)}}\]

Thus we get

\[\boxed{T(s_{i})=\prod_{j=1}^{i}\exp{\Big{(}-\frac{(\tau_{j}+\tau_{j-1})(s_{j}- s_{j-1})}{2}\Big{)}}}.\] (16)

#### a.3.4 Probability of interval \([s_{i},s_{i+1}]\)

From the generalized form for \(P_{i}\) as derived in the main paper, we plug in the expression for transmittance \(T(s_{i})\) as derived above to obtain:

\[P_{i} =\int_{s_{i}}^{s_{i+1}}\tau(s)T(s)\mathrm{d}s\] \[=T(s_{i})-T(s_{i+1})\] \[=\prod_{j=1}^{i}\exp{\Big{(}-\frac{(\tau_{j}+\tau_{j-1})(s_{j}-s_{ j-1})}{2}\Big{)}}-\prod_{j=1}^{i+1}\exp{\Big{(}-\frac{(\tau_{j}+\tau_{j-1})(s_{j}- s_{j-1})}{2}\Big{)}}\]

Hence, we obtain

\[\boxed{P_{i}=T(s_{i})\cdot\Big{(}1-\exp{\Big{[}-\frac{(\tau_{i+1}+\tau_{i})(s_ {i+1}-s_{i})}{2}\Big{]}\Big{)}}}.\] (17)

#### a.3.5 Our Precision Importance Sampling

To sample from the ray distribution, inverse transform sampling is needed, that is, one draws \(u\sim U(0,1)\) then passes it to the inverse of a cumulative distribution (CDF), i.e. a sample \(x=F^{-1}(u)\), where \(F\) is the CDF of the distribution. Unlike the piecewise constant case, where \(F\) is not invertible, needing for a surrogate function \(G\) derived from F, we show that under our piecewise linear opacity assumption, we can solve for the solution \(x\) for each corresponding \(u\).

As illustrated in the main paper, since \(F\) is continuous and increasing, under our assumption that \(\tau>0\)5, then \(F\) is invertible. Now, without loss of generality, let sample \(u\) fall into the CDF interval \([c_{k},c_{k+1}]\), where \(c_{k}=\sum_{j<k}P_{j}\). We know that the probability of the corresponding interval is \(P_{k}\) as given by Eq. 17. Thus we have:

Footnote 5: In practice, we can simply add a small \(\epsilon\), say \(\epsilon=10^{-6}\), to the model output resulting in positive \(\tau\), to make \(\tau\) positive everywhere.

\[c_{k+1}-c_{k} =P_{k}\] \[=\int_{s_{k}}^{s_{k+1}}T(s)\tau(s)\mathrm{d}s\] \[=T(s_{k})\cdot\Big{(}1-\exp\Big{(}-\frac{(\tau_{k+1}+\tau_{k})(s_ {k+1}-s_{k})}{2}\Big{)}\Big{)}\]

We want to solve for sample \(x\in[s_{k},s_{k+1}]\), such that \(x=F^{-1}(u)\). Equivalently, since we know that \(x\in[s_{k},s_{k+1}]\), then we reparameterize and let \(x=s_{k}+t\), where \(t\in[0,s_{k+1}-s_{k}]\). We are solving for \(x\) as follows:

\[u =\int_{0}^{x}T(s)\tau(s)\mathrm{d}s\] \[=\int_{0}^{s_{k}}T(s)\tau(s)\mathrm{d}s+\int_{s_{k}}^{x}T(s) \tau(s)\mathrm{d}s\] \[=c_{k}+\int_{s_{k}}^{x}T(s)\tau(s)\mathrm{d}s\] \[u-c_{k} =\int_{s_{k}}^{x}T(s)\tau(s)\mathrm{d}s\]

Now, from the derivation of the general form for \(P_{i}\) in Eq. 15, we can similarly obtain

\[u-c_{k} =T(s_{k})-T(x).\] \[=T(s_{k})\cdot(1-\exp\left(-\int_{s_{k}}^{x}\tau(u)\mathrm{d}u \right)).\]

Thus, simplifying we get

\[\frac{u-c_{k}}{T(s_{k})} =1-\exp\left(-\int_{s_{k}}^{x}\tau(u)\mathrm{d}u\right)\] \[\exp\left(-\int_{s_{k}}^{x}\tau(u)\mathrm{d}u\right) =1-\frac{u-c_{k}}{T(s_{k})}\] \[-\int_{s_{k}}^{x}\tau(u)\mathrm{d}u =\ln\left(1-\frac{u-c_{k}}{T(s_{k})}\right)\]

which gives us the expression

\[\int_{s_{k}}^{x}\tau(u)\mathrm{d}u=\ln(T(s_{k}))-\ln(T(s_{k})-(u-c_{k})).\] (18)This holds for \(T(s_{k})\neq 0\), which is true under our assumption that \(\tau>0\), and \(T(s_{k})-(u-c_{k})\geq 0\), which we will show in Sec. A.3.8 below.

#### a.3.6 Evaluating \(-\int_{s_{k}}^{x}\tau(u)\mathrm{d}u\).

We evaluate the expression \(-\int_{s_{k}}^{x}\tau(u)\mathrm{d}u\) in order to solve for the exact sample \(x\). Recall, for \(s\in[s_{k},s_{k+1}]\), we have

\[\tau(s) =(\frac{s_{k+1}-s}{s_{k+1}-k_{i}})\tau_{k}+(\frac{s-k_{i}}{s_{k+1 }-s_{k}})\tau_{k+1}\] \[=\frac{1}{s_{k+1}-s_{k}}[(\tau_{k+1}-\tau_{k})s+(s_{k+1}\tau_{k}- s_{k}\tau_{k+1})]\]

Let constants \(a=\tau_{k+1}-\tau_{k}\), \(b=s_{k+1}\tau_{k}-s_{k}\tau_{k+1}\), \(d=\frac{1}{s_{k+1}-s_{k}}\), thus we can write \(\tau(s)\) as follows:

\[\tau(s)=d(as+b).\] (19)

Thus, from Eq 19 we have:

\[\int_{s_{k}}^{x}\tau(u)\mathrm{d}u =\int_{s_{k}}^{x}d(au+b)\mathrm{d}u\] \[=d(\int_{s_{k}}^{x}au\mathrm{d}u+\int_{s_{k}}^{x}b\mathrm{d}u)\] \[=d[\frac{au^{2}}{2}\Big{|}_{s_{k}}^{x}+b(x-s_{i})]\] \[=d[\frac{a(x^{2}-s_{k}^{2})}{2}+b(x-s_{k})]\] \[=d[\frac{a((s_{k}+t)^{2}-s_{k}^{2})}{2}+b((s_{k}+t)-s_{k})]\] \[=d[\frac{a(t^{2}+2s_{k}t)}{2}+bt]\] \[=d[\frac{a}{2}t^{2}+(as_{k}+b)t]\] \[=\frac{1}{s_{k+1}-s_{k}}[\frac{\tau_{k+1}-\tau_{k}}{2}t^{2}+(( \tau_{k+1}-\tau_{k})s_{k}+s_{k+1}\tau_{k}-s_{k}\tau_{k+1})t]\] \[=\frac{1}{s_{k+1}-s_{k}}[\frac{\tau_{k+1}-\tau_{k}}{2}t^{2}+(s_{ k+1}\tau_{k}-s_{k}\tau_{k})t]\] \[=\frac{\tau_{k+1}-\tau_{k}}{2(s_{k+1}-s_{k})}t^{2}+\tau_{k}t\]

Hence, plugging this in Eq. 18, we get the quadratic equation

\[\frac{\tau_{k+1}-\tau_{k}}{2(s_{k+1}-s_{k})}t^{2}+\tau_{k}t-(\ln(T(s_{k}))-\ln (T(s_{k})-(u-c_{k})))=0.\]

We want to solve for \(t\in[0,s_{k+1}-s_{k}]\), and the roots of the quadratic equation is given by \[t=\frac{(s_{k+1}-s_{k})(-\tau_{k}\pm\sqrt{\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{k}) (\ln T(s_{k})-\ln(T(s_{k})-(u-c_{k})))}{(s_{k+1}-s_{k})})}}{(\tau_{k+1}-\tau_{k})}\] (20)

That means to compute for the solution, we need to find the root

\[\frac{(-\tau_{k}\pm\sqrt{\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{k})(\ln T(s_{k}) -\ln(T(s_{k})-(u-c_{k})))}{(s_{k+1}-s_{k})})}}{(\tau_{k+1}-\tau_{k})}\in(0,1)\]

which we will show always exists and is unique.

a.3.7 Bounding \(\Delta=\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{k})(\ln T(s_{k})-\ln(T(s_{k})-(u- c_{k})))}{(s_{k+1}-s_{k})}\)

Let us first bound the discriminant of the quadratic formula. We have

\[u-c_{k} \leq c_{k+1}-c_{k}\] \[=\sum_{j=0}^{k}P_{j}-\sum_{j=0}^{k-1}P_{j}=P_{k}\] \[=T(s_{k})\cdot(1-\exp\Big{(}-\frac{(\tau_{k+1}+\tau_{k})(s_{k+1} -s_{k})}{2}\Big{)})\]

Thus we have

\[\ln(T(s_{k})-(u-c_{k})) \geq\ln\Big{(}T(s_{k})-T(s_{k})\cdot(1-\exp\Big{(}-\frac{(\tau_{k +1}+\tau_{k})(s_{k+1}-s_{k})}{2}\Big{)})\Big{)}\] \[=\ln(T(s_{k})\cdot\exp\Big{(}-\frac{(\tau_{k+1}+\tau_{k})(s_{k+1} -s_{k})}{2}\Big{)})\] \[=\ln(T(s_{k}))-\frac{(\tau_{k+1}+\tau_{k})(s_{k+1}-s_{k})}{2}\] \[\ln T(s_{k})-\ln(T(s_{k})-(u-c_{k})) \leq\frac{(\tau_{k+1}+\tau_{k})(s_{k+1}-s_{k})}{2}\]

\[\frac{2(\tau_{k+1}-\tau_{k})(\ln T(s_{k})-\ln(T(s_{k})-(u-c_{k})) )}{(s_{k+1}-s_{k})} \leq\frac{2(\tau_{k+1}-\tau_{k})(\frac{(\tau_{k+1}+\tau_{k})(s_{k +1}-s_{k})}{2})}{(s_{k+1}-s_{k})}\] \[=\tau_{k+1}^{2}-\tau_{k}^{2}\]

Hence, computing the discriminant we get:

\[\Delta=\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{k})(\ln T(s_{k})-\ln(T(s_{k})-(u -c_{k})))}{(s_{k+1}-s_{k})}\quad\leq\tau_{k}^{2}+(\tau_{k+1}^{2}-\tau_{k}^{2} )=\tau_{k+1}^{2}.\]

Similarly, \(u-c_{k}\geq 0\), where equality holds when \(u=c_{k}\). This gives us \(\Delta\geq\tau_{k}^{2}\).

Hence, we know that \(\tau_{k+1}^{2}\geq\Delta\geq\tau_{k}^{2}\). Since we need

\[\frac{(-\tau_{k}\pm\sqrt{\Delta})}{(\tau_{k+1}-\tau_{k})}\in(0,1),\text{and} \ \frac{(-\tau_{k}-\sqrt{\Delta})}{(\tau_{k+1}-\tau_{k})}\leq 0\]Thus to find the solution \(t\), we need to take the positive root. We have

\[\frac{(-\tau_{k}+\sqrt{\Delta})}{(\tau_{k+1}-\tau_{k})}\geq\frac{(-\tau_{k}+ \sqrt{\tau_{k}^{2}}}{(\tau_{k+1}-\tau_{k})}=0\]

\[\frac{(-\tau_{k}+\sqrt{\Delta})}{(\tau_{k+1}-\tau_{k})}\leq\frac{(-\tau_{k}+ \sqrt{\tau_{k+1}^{2}}}{(\tau_{k+1}-\tau_{k})}=1\]

This shows that the solution is within the desired interval. Hence, the solution is

\[t=\frac{(s_{k+1}-s_{k})(-\tau_{k}+\sqrt{\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{k })(\ln T(s_{k})-\ln(T(s_{k})-(u-c_{k})))}{(s_{k+1}-s_{k})})}}{(\tau_{k+1}-\tau_ {k})}\] (21)

Hence, for the positive root, we know that \(t\in[0,s_{k-1}-s_{k}]\).

#### a.3.8 Proof for \(T(s_{k})\geq(u-c_{k})\)

\[c_{k} =\sum_{j=0}^{k-1}P_{j}\] \[=\sum_{j=0}^{k-1}T(s_{j})\cdot(1-\exp(-\frac{(\tau_{j+1}+\tau_{j} )(s_{j+1}-s_{j})}{2}))\]

We know that

\[T(s_{j})=\prod_{i=1}^{j}\exp(-\frac{(\tau_{i}+\tau_{i-1})(s_{i}-s_{i-1})}{2})\]

Let \(a_{i}=\exp(-\frac{(\tau_{i}+\tau_{i-1})(s_{i}-s_{i-1})}{2})\), and \(T(s_{0})=1\). Hence we have

\[T(s_{j})=\prod_{i=1}^{j}a_{i},\]

\[c_{k} =T(s_{0})(1-a_{1})+\sum_{j=1}^{k-1}(\prod_{i=1}^{j}a_{i})\cdot(1-a _{j+1}))\] \[=(1-a_{1})+(a_{1})(1-a_{2})+(a_{1}a_{2})(1-a_{3})+...\] \[=1-a_{1}a_{2}...a_{k}\]

Since \(T(s_{k})=a_{1}a_{2}...a_{k}\) and \(a_{i}>0\forall i\) then

\[T(s_{k})+c_{k} =a_{1}a_{2}...a_{k}+(1-a_{1}a_{2}...a_{k})\] \[=1\] \[\geq c_{k+1}\] \[\geq u_{\cdot\Box}\]

Note that above proof and solutions hold for \(\tau_{k}\neq\tau_{k+1}\) and \(T(s_{k})\neq 0\), which is all holds since we have \(\tau(s)>0\forall s\), which is equivalent to \(F\) being an increasing function.

#### a.3.9 The solution for sample \(u\):

Putting everything together, we have the solution \(t\) given as

\[t=\frac{(s_{k+1}-s_{k})(-\tau_{k}+\sqrt{\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{k}) (\ln T(s_{k})-\ln(T(s_{k})-(u-c_{k})))}{(s_{k+1}-s_{k})})}}{(\tau_{k+1}-\tau_{k})}.\] (22)

From Sec A.3.8, we have \(T(s_{k})=a_{1}a_{2}...a_{k}\) and \(c_{k}=1-a_{1}a_{2}...a_{k}\), thus \(T(s_{k})-(u-c_{k})=1-u\). Hence we can simplify it to

\[t=\frac{(s_{k+1}-s_{k})(-\tau_{k}+\sqrt{\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{k })(\ln T(s_{k})-\ln(1-u))}{(s_{k+1}-s_{k})}})}{(\tau_{k+1}-\tau_{k})}\]

\[t=\frac{(s_{k+1}-s_{k})(-\tau_{k}+\sqrt{\tau_{k}^{2}+\frac{2(\tau_{k+1}-\tau_{ k})(-\ln\frac{(1-u)}{T(s_{k})})}{(s_{k+1}-s_{k})}})}{(\tau_{k+1}-\tau_{k})}.\] (23)

### Piecewise Quadratic and Higher Order Polynomials

Now, we first consider the full derivation for volume rendering equation under the assumption that opacity is _piecewise quadratic_ and color is piecewise constant. Consider opacities \(\tau_{1},\ldots,\tau_{n}\) queried at \(n\) samples \(s_{1},\ldots,s_{n}\) along the ray. Here, we set \(s_{0}=t_{n}\) and \(s_{n+1}=t_{f}\) to the near and far plane, with \(\tau_{0}=0\) and \(\tau_{n+1}=10^{10}\) denoting empty and opaque space.

To interpolate opacity, because a quadratic function can only be uniquely defined with \(3\) points, we choose \(\tau(s)\) to be quadratic within each interval \([s_{j},s_{j+2}]\) for even values of \(j\). To encapsulate all points, this forces \(n\) to be _odd_, e.g. using \(127\) coarse samples and \(64\) fine samples.

#### a.4.1 Derivation: Computing \(T(s)\)

In the same way as Sec. A.3.3, we can derive transmittance, which is in closed-form. The only modification is that the formulae for integrals of opacity are different over left and right subintervals \([s_{j},s_{j+1}]\) and \([s_{j+1},s_{j+2}]\), with \(j\) even (see the next section for a derivation of these integrals):

\[T(s_{2i})=\exp{(-\int_{s_{0}}^{s_{2i}}\tau(u)\mathrm{d}u)}=\prod_{j=1}^{i} \exp{(-\int_{s_{2j-1}}^{s_{2j}}\tau(u)\mathrm{d}u)}\exp{(-\int_{s_{2j}}^{s_{2 j+1}}\tau(u)\mathrm{d}u)}.\] (24)

Substituting the expressions in Eqs. 28 and 29 gives a closed form expression in terms of the \(s_{j}\)'s and \(\tau_{j}\)'s. We can similarly compute

\[T(s_{2i+1})=\exp(-\int_{s_{2i}}^{s_{2i+1}}\tau(u)\mathrm{d}u)\prod_{j=1}^{i} \exp{(-\int_{s_{2j-1}}^{s_{2j}}\tau(u)\mathrm{d}u)}\exp{(-\int_{s_{2j}}^{s_{2 j+1}}\tau(u)\mathrm{d}u)}.\] (25)

Then the probability of the \(i\)th interval for each \(0\leq i\leq n\) is, as before,

\[P_{i}=T(s_{i})-T(s_{i+1}).\] (26)

This leads to a closed-form expression for \(P_{i}\). This means the behavior of \(P_{i}\) depends entirely on that of the opacity integral. However, due to the form of \(\int\tau(s)ds\) detailed in the next section, this means \(P_{i}\) involves a _piecewise exponential of a rational function in \(\tau_{j}\)'s and \(s_{j}\)'s_, which leads to poor numerical conditioning and thus optimization instability.

#### a.4.2 Derivation: Computing and Integrating \(\tau(s)\) on Intervals

We now compute \(\tau(s)\) and its integral on each interval. Fix the interval \([s_{j},s_{j+2}]\), with \(j\) odd, and \(\tau_{j}=\tau(s_{j})\), \(\tau_{j+1}=\tau(s_{j+1})\), \(\tau_{j+2}=\tau(s_{j+2})\). By _Lagrange interpolation_, the quadratic \(\tau(s)\) passing through \((s_{j},\tau_{j}),(s_{j+1},\tau_{j+1}),(s_{j+2},\tau_{j+2})\) is given by:

\[\begin{split}\tau(s)=\frac{\tau_{j}}{\alpha_{j}\gamma_{j}}(s-s_{ j+1})(s-s_{j+2})-\frac{\tau_{j+1}}{\alpha_{j}\beta_{j}}(s-s_{j})(s-s_{j+2})+ \frac{\tau_{j+2}}{\beta_{j}\gamma_{j}}(s-s_{j})(s-s_{j+1}),\\ \alpha_{j}=s_{j+1}-s_{j},\quad\beta_{j}=s_{j+2}-s_{j+1},\quad \gamma_{j}=s_{j+2}-s_{j}.\end{split}\] (27)

Note the integrals of the three monic quadratics over \([s_{j},s_{j+1}]\) can be expressed in terms of \(\alpha_{j},\beta_{j},\gamma_{j}\):

\[\begin{split}\int_{s_{j}}^{s_{j+1}}(s-s_{j+1})(s-s_{j+2})\mathrm{ d}s&=\int_{-\alpha_{j}}^{0}s(s-\beta_{j})\mathrm{d}s=\frac{\alpha_{j} ^{3}}{3}+\frac{\alpha_{j}^{2}\beta_{j}}{2},\\ \int_{s_{j}}^{s_{j+1}}(s-s_{j})(s-s_{j+2})\mathrm{d}s& =\int_{0}^{\alpha_{j}}s(s-\gamma_{j})\mathrm{d}s=\frac{\alpha_{j} ^{3}}{3}-\frac{\alpha_{j}^{2}\gamma_{j}}{2},\\ \int_{s_{j}}^{s_{j+1}}(s-s_{j})(s-s_{j+1})\mathrm{d}s& =\int_{0}^{\alpha_{j}}s(s-\alpha_{j})\mathrm{d}s=-\frac{\alpha_{j} ^{3}}{6}.\end{split}\]

Thus, using Eq. 27 gives the integral of opacity over \([s_{j},s_{j+1}]\):

\[\int_{s_{j}}^{s_{j+1}}\tau(s)\mathrm{d}s=\frac{\tau_{j}}{\gamma_{j}}\cdot \left[\frac{\alpha_{j}^{2}}{3}+\frac{\alpha_{j}\beta_{j}}{2}\right]-\frac{ \tau_{j+1}}{\beta_{j}}\left[\frac{\alpha_{j}^{2}}{3}-\frac{\alpha_{j}\gamma_{ j}}{2}\right]+\frac{\tau_{j+2}}{\beta_{j}\gamma_{j}}\cdot\left[-\frac{\alpha_{j}^{3}}{6} \right].\] (28)

Similarly, the integrals of those same three quadratics over \([s_{j+1},s_{j+2}]\) factor out \(\beta_{j}\):

\[\begin{split}\int_{s_{j+1}}^{s_{j+2}}(s-s_{j+1})(s-s_{j+2}) \mathrm{d}s&=\int_{0}^{\beta_{j}}s(s-\beta_{j})\mathrm{d}s=- \frac{\beta_{j}^{3}}{6},\\ \int_{s_{j+1}}^{s_{j+2}}(s-s_{j})(s-s_{j+2})\mathrm{d}s& =\int_{-\beta_{j}}^{0}(s+\gamma_{j})s\mathrm{d}s=\frac{\beta_{j} ^{3}}{3}-\frac{\beta_{j}^{2}\gamma_{j}}{2},\\ \int_{s_{j+1}}^{s_{j+2}}(s-s_{j})(s-s_{j+1})\mathrm{d}s& =\int_{0}^{\beta_{j}}(s+\alpha_{j})s\mathrm{d}s=\frac{\beta_{j} ^{3}}{3}+\frac{\beta_{j}^{2}\alpha_{j}}{2}.\end{split}\]

Then, summing these up along Eq. 27 gives the integral of opacity over \([s_{j+1},s_{j+2}]\):

\[\int_{s_{j+1}}^{s_{j+2}}\tau(s)\mathrm{d}s=\frac{\tau_{j}}{\alpha_{j}\gamma_{ j}}\cdot\left[-\frac{\beta_{j}^{3}}{6}\right]-\frac{\tau_{j+1}}{\alpha_{j}} \left[\frac{\beta_{j}^{2}}{3}-\frac{\beta_{j}\gamma_{j}}{2}\right]+\frac{\tau_ {j+2}}{\gamma_{j}}\cdot\left[\frac{\beta_{j}^{2}}{3}+\frac{\beta_{j}\alpha_{j }}{2}\right].\] (29)

Observe in Eqs. 28 and 29 that the integral of opacity involves some terms with \(\alpha_{j},\beta_{j},\gamma_{j}\) in the denominator, which _do not cancel_. Thus, the integral of opacity over an interval _is not a polynomial in \(\tau_{i}\)'s and \(s_{i}\)'s_, but is instead a _rational function_. This contrasts with the linear derivation in Sec. A.3.3, where this integral was a degree \(2\) multivariate polynomial in \(\tau_{i}\)'s and \(s_{i}\)'s. This caveat causes numerical instability, which will be discussed further in Secs. A.4.3 and A.4.4.

Generally, following the steps of the above derivation shows that if we interpolate \(\tau\) piecewise by _any degree d polynomial_, \(d\geq 2\), then the result is a rational function in \(s_{i}\)'s and \(\tau_{i}\)'s, but not a polynomial, which would also lead to training instability as in the quadratic case.

#### a.4.3 Piecewise Quadratic Problem 1: Negative Interpolated Opacity

As seen above, one problem with the piecewise quadratic model is that the integral of opacity is a _rational function in \(\alpha_{i},\beta_{i},\gamma_{i}\)_. In particular, due to the presence of negatives in front of certain rational terms in Eqs. 28 and 29, it may become _negative_ as the denominators of these terms approach zero. One example is shown in Figure A.4.3, for samples \(s_{1},s_{2},s_{3}\) with \(\tau_{1}=\tau_{2}<\tau_{3}\) and \(s_{3}-s_{2}\ll s_{2}-s_{1}\), where the interpolated quadratic dips far below the \(x\)-axis. Note this interpolation is physically implausible, as _opacity should be nonnegative everywhere_.

Furthermore, from Eqs. 24 and 25, when the opacity integral is negative on intervals, transmittance can then be a product with exponentials of negative terms, and potentially be greater than \(1\). This is incompatible with the physical interpretation of transmittance as a _probability_ that a light travels a distance along a ray without being absorbed.

#### a.4.4 Piecewise Quadratic Problem 2: Instability from Sample Proximity

In Eqs. 28 and 29, we observe the presence of terms such as \(\alpha_{j},\beta_{j},\gamma_{j}\) in the denominator. _These do not appear in the piecewise linear model._ As a result, because these quantities approach zero as samples \(s_{j},s_{j+1},s_{j+2}\) become closer, then the integral of opacity can become an arbitrarily large positive or even _negative_ (as shown qualitatively in the previous section). Referencing the transmittance formulae in Eqs. 24 and 25, this means transmittance can approach zero or infinity and gradients can explode as samples are clustered together. This is fairly common in stratified sampling, and even more so with importance sampling.

To formally describe this instability, we first look at what happens when \(s_{j}\) and \(s_{j+1}\) coincide:

\[\lim_{s_{j+1}\to s_{j}^{+}}\int_{s_{j}}^{s_{j+1}}\tau(s)\mathrm{d}s=0\]

because \(\tau:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}\) is continuous. There is no instability in this integral when \(s_{j+1}\to s_{j}^{+}\). However, when \(s_{j+1}\) and \(s_{j+2}\) coincide, because \(\beta_{j}\to 0^{+}\) and \(\gamma_{j}\rightarrow\alpha_{j}^{+}\), the integral approaches

\[\lim_{s_{j+2}\to s_{j+1}^{+}}\int_{s_{j}}^{s_{j+1}}\tau(s) \mathrm{d}s =\lim_{s_{j+2}\to s_{j+1}^{+}}\left[\frac{\tau_{j}}{\gamma_{j} }\cdot\left[\frac{\alpha_{j}^{2}}{3}+\frac{\alpha_{j}\beta_{j}}{2}\right]- \frac{\tau_{j+1}}{\beta_{j}}\left[\frac{\alpha_{j}^{2}}{3}-\frac{\alpha_{j} \gamma_{j}}{2}\right]+\frac{\tau_{j+2}}{\beta_{j}\gamma_{j}}\cdot\left[-\frac{ \alpha_{j}^{3}}{6}\right]\right]\] \[=\frac{\tau_{j}\alpha_{j}}{3}-\lim_{s_{j+2}\to s_{j+1}^{+}} \frac{\frac{\tau_{j+1}\alpha_{j}^{2}}{3}-\frac{\tau_{j+1}\alpha_{j}\gamma_{j} }{2}+\frac{\tau_{j+2}\alpha_{j}^{3}}{6\gamma_{j}}}{\beta_{j}}\] \[=\frac{\tau_{j}\alpha_{j}}{3}-\lim_{s_{j+2}\to s_{j+1}^{+}} \frac{2\tau_{j+1}\alpha_{j}^{2}\gamma_{j}-3\tau_{j+1}\alpha_{j}\gamma_{j}^{2} +\tau_{j+2}\alpha_{j}^{3}}{6\gamma_{j}\beta_{j}}\] \[=\frac{\tau_{j}\alpha_{j}}{3}-\lim_{s_{j+2}\to s_{j+1}^{+}} \frac{\tau_{j+1}\alpha_{j}(\alpha_{j}^{2}+2\alpha_{j}\gamma_{j}-3\gamma_{j} ^{2})+\alpha_{j}^{3}(\tau_{j+2}-\tau_{j+1})}{6\gamma_{j}\beta_{j}}\] \[=\frac{\tau_{j}\alpha_{j}}{3}-\lim_{s_{j+2}\to s_{j+1}^{+}} \frac{\tau_{j+1}\alpha_{j}(\alpha_{j}+3\gamma_{j})(\alpha_{j}-\gamma_{j})+ \alpha_{j}^{3}(\tau_{j+2}-\tau_{j+1})}{6\gamma_{j}\beta_{j}}\] \[=\frac{\tau_{j}\alpha_{j}}{3}-\lim_{s_{j+2}\to s_{j+1}^{+}} \frac{-\tau_{j+1}\alpha_{j}(\alpha_{j}+3\gamma_{j})\beta_{j}+\alpha_{j}^{3}( \tau_{j+2}-\tau_{j+1})}{6\gamma_{j}\beta_{j}}\]\[=\frac{\tau_{j}\alpha_{j}}{3}+\frac{2\tau_{j+1}\alpha_{j}}{3}-\lim_{s_{j+2} \to s_{j+1}^{+}}\frac{\alpha_{j}^{2}(\tau_{j+2}-\tau_{j+1})}{6\gamma_{j}\beta_{j}}\] \[=\frac{\tau_{j}\alpha_{j}}{3}+\frac{2\tau_{j+1}\alpha_{j}}{3}- \frac{\alpha_{j}^{2}}{6}\lim_{s_{j+2}\to s_{j+1}^{+}}\frac{\tau_{j+2}-\tau_{j +1}}{\beta_{j}}.\]

The behavior of this integral limit depends on the last limit. To analyze this, let \(h:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}\) be the network function which takes in samples on the ray and outputs opacity. The last limit is

\[\lim_{s_{j+2}\to s_{j+1}^{+}}\frac{\tau_{j+2}-\tau_{j+1}}{\beta_{j}}= \lim_{s_{j+2}\to s_{j+1}^{+}}\frac{h(s_{j+2})-h(s_{j+1})}{s_{j+2}-s_{j+1}}.\] (30)

_Because \(h\) is almost always differentiable at \(s_{j+1}\), this becomes \(h^{\prime}(s_{j+1})\), and so_

\[\lim_{s_{j+2}\to s_{j+1}^{+}}\int_{s_{j}}^{s_{j+1}}\tau(s)\mathrm{d}s= \frac{\tau_{j}\alpha_{j}}{3}+\frac{2\tau_{j+1}\alpha_{j}}{3}-\frac{\alpha_{j} ^{2}}{6}h^{\prime}(s_{j+1}).\] (31)

_There are no constraints on the value of \(h^{\prime}(s_{j+1})\) as the network trains, so the above limit can achieve any real value. In particular, we can derive a condition for when the limit is negative:_

\[\lim_{s_{j+2}\to s_{j+1}^{+}}\int_{s_{j}}^{s_{j+1}}\tau(s)\mathrm{d}s<0\iff h ^{\prime}(s_{j+1})>\frac{2\tau_{j}+4\tau_{j+1}}{\alpha_{j}}.\] (32)

_That is, whenever there is a sharp enough increase in opacity (which can happen at, say, a surface crossing) and samples \(s_{j+1},s_{j+2}\) are sufficiently close, the interpolated quadratic can have a negative integral on \([s_{j},s_{j+1}]\), which leads to the issues described in Sec. A.4.3. In other words, the integral of opacity over \([s_{j},s_{j+1}]\) is unstable as \(s_{j+2}\to s_{j+1}^{+}\). A similar analysis holds to show the instability of the opacity integral on \([s_{j+1},s_{j+2}]\) as \(s_{j+1}\to s_{j}^{+}\)._

#### a.4.5 Piecewise Quadratic Problem 3: Importance Sampling

Suppose we wish to importance sample in the same manner as piecewise linear, that is, we use inverse transform sampling. In essence, we draw \(u\sim U(0,1)\) and then sample \(x=F^{-1}(u)\), with \(F\) the CDF of the distribution. Recall \(F\) is computed as

\[f(x)=\int_{s_{0}}^{x}T(s)\tau(s)ds=\int_{s_{0}}^{s_{k}}T(s)\tau(s)ds+\int_{s_{ k}}^{x}T(s)\tau(s)ds=c_{k}+\int_{s_{k}}^{x}T(s)\tau(s)ds.\] (33)

As before, the latter equation becomes

\[f(x)=c_{k}+T(s_{k})-T(x).\] (34)

Hence, computing \(x\) amounts to solving the equation \(f(x)=u\), which becomes from above:

\[T(x)=c_{k}+T(s_{k})-u.\] (35)

As \(T\) is the exponential of a cubic, this amounts to solving a cubic above. This does have a real solution, as \(F\) is increasing and continuous, so the Intermediate Value Theorem implies \(f(x)=u\) has a solution; and the solution is unique because \(F\) is strictly increasing, as it is an exponential of a polynomial. _However, the complexity of exact importance sampling would be large._

This analysis reveals another downside of higher order polynomials. In general, suppose we wish to interpolate opacity \(\tau\) with a piecewise degree \(n\) polynomial. Then following the same method as above, we see transmittance \(T\) is the piecewise exponential of a degree \(n+1\) polynomial, which is the integral of \(\tau\). So inverse transform sampling reduces to solving a degree \(n+1\) polynomial. The _Abel-Ruffini Theorem_ asserts for \(n+1\geq 5\) that this polynomial is in general not solvable by radicals. In other words, there is no simple closed form for exact importance sampling when \(n\geq 4\).

Theoretically, this could be exactly solved for \(n=2\) and \(n=3\) (i.e. when opacity is piecewise quadratic or cubic, respectively), but the formulae to derive cubic and quartic solutions can become sufficiently complicated and the resulting complicated expressions may result in numerical instability during optimization, especially when taking the gradient w.r.t. the samples.

### Experiment details, Reproducibility and Compute

#### a.5.1 Implementation Details

We include the core code snippets of our implementation of **PL-NeRF**. Figure A4 shows the volume rendering equation that includes the implementation of Eq. 10 and 11 (main paper). This is a direct replacement of the original constant approximation, where we also show the code snippet in the figure as reference. Figure A5 shows the implementation of our precise importance sampling from Eq. 13 (main paper), which is also a direct replacement of the constant importance sampling implementation (Figure A6) for reference. We highlight that our formulation is a direct replacement of the functions from the original implementation, and hence for the depth experiments, we are also able to directly adapt the codebase from [28]. We use Nvidia v100 and A5000 GPU's for our experiments. Each scene is trained on a single GPU and takes \(~{}15-20\) hours. We used an internal academic cluster and cloud compute resources to train and evaluate our models.

For the MipNeRF-based experiments, our experiments are also run on the standard train and test split of the Blender dataset with the official released hyperparameters of Mip-NeRF using the NerfStudio [27] codebase. For PL-MipNeRF, we use the two-MLP training scheme with a coarse loss weight of 1.0.

For DIVeR-based experiments, we use their official implementation and configuration for DIVeR64 at 128 voxels. For PL-DIVeR, we utilize their voxel-based representation and feature integration and dropping in our piecewise linear opacity rendering formulation. We similarly run the DIVeR models on a single Nvidia v100 GPU trained using their default configurations and hyperparameters for DIVeR64 at 128 voxels.

The total training time for 500k iterations on a single Nvidia v100 GPU is 17.78 and 21.43 hours for Vanilla NeRF and PL-NeRF, respectively. Figure 2-c shows the head-to-head comparison of training PSNR (y-axis) with respect to time (x-axis) of Vanilla NeRF vs PL-NeRF on the Lego scene. Rendering a single 800x800 image takes 25.59 and 32.35 seconds for Vanilla NeRF and PL-NeRF, respectively.

#### a.5.2 Computational complexity under different number of samples

We measured the total rendering time for a single 800x800 image under different numbers of samples for our PL-NeRF. The total rendering time for (64+64), (64+128) and (128+64) are 19.20, 25.85 and 32.35 seconds, respectively.

#### a.5.3 Convergence plots under different number of samples

Figure 29 shows the convergence plots under different numbers of samples of our PL-NeRF vs Vanilla NeRF. We see that under different number of samples, our linear approach converges to a higher training PSNR.

### Limitations

Our piecewise linear opacity approximation is able to handle arbitrarily small opacity, e.g. \(1e^{-6}\), however, we cannot handle coordinates with exactly zero opacity. This special case is not an issue in practice. Also theoretically, any atom absorbs light and thus will not have exactly zero opacity, except in a vacuum. Another limitation is our method is slightly slower than the original piecewise constant approximation, and this is due to requiring more FLOPS for our importance sampling computation (Eq. 13 main paper). Another additional limitation is we still assume piecewise color, i.e. within a bin we do not handle color integration. Modeling this can potentially handle difficult scenarios such as double-walled colored glass or atmospheric effects such as fog or smoke. Lastly, we also inherit the limitations of NeRFs in general, such as requiring known camera poses.

### Broader Impact

Our models require the usage of GPUs both in training time and rendering time, and GPUs use up energy to run and power them. We acknowledge that this contributes to climate change that is an important societal issue. Despite this, we observe the improvement in the results that is theoreticallygrounded and believe that it is beneficial for the pursuit of science. We responsibly ran our models by first prototyping on selected scenes before scaling up to different scenes across datasets to minimize its impact to climate change.

[MISSING_PAGE_EMPTY:30]

Figure A6: This is the original importance sampling for the constant approximation for reference.