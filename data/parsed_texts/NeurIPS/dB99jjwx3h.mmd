Learning Linear Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity

Jikai Jin

Institute for Computational and Mathematical Engineering

Stanford University

Stanford, CA 94305

jkjin@stanford.edu

&Vasilis Syrgkanis

Management Science and Engineering

Stanford University

Stanford, CA 94305

vsyrgk@stanford.edu

###### Abstract

We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we consider the task of learning causal representation learning with data collected from _general environments_. We show that even when the causal model and the mixing function are both linear, there exists a _surrounded-node ambiguity_ (SNA) [46] which is basically unavoidable in our setting. On the other hand, in the same linear case, we show that identification up to SNA is possible under mild conditions, and propose an algorithm, LiNGCReL which provably achieves such identifiability guarantee. We conduct extensive experiments on synthetic data and demonstrate the effectiveness of LiNGCReL in the finite-sample regime.

## 1 Introduction

Artificial intelligence (AI) has achieved tremendous success in various domains in the past decade [4, 40, 6]. However, current approaches are largely based on learning the _statistical_ structures and relationships in the data that we observe. As a result, it is not surprising that these approaches often capture spurious statistical dependencies between different features, resulting in poor performance in the presence of test distribution shift [30, 22] or adversarial attacks [3, 50].

In view of these pitfalls, a recent line of work has explored the problem of _causal representation learning_ (CRL) [34], the task of learning the causal relationships between high-level latent variables underlying our low-level observations. Notably, it is widely believed in cognitive psychology that humans take a causal approach to distill information from the world and make decisions to achieve their goals [37, 12, 19]. As a result, there is reason to believe that learning causal representations has the potential to significantly improve the power of AI, especially on tasks where performance lags far behind human level [17].

Despite such promise, a crucial challenge in CRL is the _identifability_ of the data generating process; in other words, given the data that we observe, can we uniquely identify the underlying causal model. It has been shown that given observational data (_i.e._, i.i.d. data generated from a single environment), the model is already non-identifiable in strictly simpler settings where the latent variables are known to be independent [25; 26], or where there is no mixing function and one directly observes the latent variables [39]. As a result, existing algorithms for CRL with observational data [52; 53; 11] typically require additional assumptions on the structure of the underlying causal graph. A natural question that arises is what types of data do we need to acquire to make identification possible in the general case.

One line of works assumes access to counterfactual data [27; 48; 5], where some form of _weak supervision_ is typically required. A common assumption here is that one observes data in _pairs_, where each pair of data is related via sharing part of the latent representation. However, such data is hard to acquire since it requires direct control on the latent representation.

Another line of works [1; 49; 7; 47] instead considers an interventional setting, where the learner observes data generated from multiple different environments. This is arguably a much more realistic setup and reflects common practices in robotics [24] and genomics [28; 43] applications. However, a vast majority of identifiability guarantees assume that each environment corresponds to _single-node_, _hard_ interventions, which is defined as interventions that isolate a single latent variable from its causal parents. Again, this is quite a restrictive assumption because of two reasons. _First_, since the latent variables are unknown and need to be learned from data, it is unclear how to perform interventions that only affect one variable. _Second_, even if one can perform single-node interventions, it may not be feasible to artificially remove causal effects in the data generating processes. This issue is ubiquitous in real-world applications as pointed out in Campbell [8], Eberhardt [14], Eronen [15]. Motivated by these challenges, we make the following contributions in this paper:

* Assuming access to data collected from multiple environments, but not necessarily from single-node, hard interventions, we identify an intrinsic surrounded-node ambiguity (SNA) in learning the underlying causal representations. We show in Theorem3 that SNA is unavoidable even if (1) both the mixing function and the causal model are known to be linear and (2) one has access to single-node, soft interventions. This highlights a remarkable difference with existing literature which shows that perfect identification can be achieved with hard interventions.
* When the causal model and the mixing function are both linear, we prove in Theorem1 that identification up to SNA is achievable with \(\mathcal{O}(d)\) diverse environments (Assumption4), where \(d\) is the size of the latent causal graph. To the best of our knowledge, this is the first identification guarantee that applies to fully general environments and makes no assumption on their relationship or similarity. Interestingly, we also show in Theorem2 that one would require \(\Omega(d^{2})\) single-node soft interventions to achieve the same identification guarantee, indicating the benefit of learning from diverse environments.
* We propose an algorithm, LiNGCReL, in Section5 that provably recovers the ground-truth model up to SNA (Theorem4) in the setting of Theorem1 when perfect information of the observation distributions is available. To demonstrate the effectiveness of LiNGCReL in finite-sample regime, we conduct extensive experiments on synthetic data, and our results reported in Section6 show that LiNGCReL is capable of recovering the true causal model up to SNA with high accuracy.

Due to space limit, proofs of all our statements and additional theoretical results are given in the appendix.

## 2 Preliminaries

We consider the standard setup of CRL from multiple environments \(E\in\mathfrak{E}\). Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be the ground-truth causal graph which is directed and acylic (DAG), where \(\mathcal{V}=[d]\) and \(\mathcal{E}\) describes the causal relationship between different nodes. Each node corresponds to a latent variable \(\bm{z}_{i}\in\mathbb{R}\).

For any node \(i\), we let \(\mathrm{pa}_{\mathcal{G}}(i)\), \(\mathrm{ch}_{\mathcal{G}}(i)\), \(\mathrm{ans}_{\mathcal{G}}(i)\) and \(\mathrm{nd}_{\mathcal{G}}(i)\) to be the set of all parents, children, ancestors and non-descendants of \(i\) in \(\mathcal{G}\) respectively. We also define \(\overline{\mathrm{pa}}_{\mathcal{G}}(i)=\mathrm{pa}_{\mathcal{G}}(i)\cup\{i\}\) and similarly for \(\overline{\mathrm{ch}}_{\mathcal{G}}(i),\overline{\mathrm{ans}}_{\mathcal{G}} (i)\) and \(\overline{\mathrm{nd}}_{\mathcal{G}}(i)\). Assuming that all probability distributions have continuous densities, the joint density of the latent variables \(\bm{z}\) can then be written as

\[p_{E}(\bm{z})=\prod_{i=1}^{d}p_{i}^{E}\left(\bm{z}_{i}\mid\bm{z}_{\mathrm{pa}_{ \mathcal{G}}(i)}\right).\] (1)

where \(p_{i}^{E}\) is the (unknown) latent generating distribution from environment \(E\) at node \(i\). Here for a given vector \(\bm{v}\), we write \(\bm{v}_{i}=\bm{e}_{i}^{\top}\bm{v}\), and let \(\bm{v}_{S}=(\bm{v}_{i}:i\in S)\in\mathbb{R}^{|S|}\).

The causal graph model with density given by (1) necessarily enjoys the following property:

**Definition 1** (Causal Markov Condition).: _For any node \(i\), conditioned on \(\bm{z}_{\mathrm{pa}_{\mathcal{G}}(i)}\), \(\bm{z}_{i}\) is independent of \(\bm{z}_{\mathrm{nd}_{\mathcal{G}}(i)}\). As a consequence, for any node \(i,j\in[d]\) and \(S\subseteq[d]\), if \(S\)\(d\)-separates \(i\) from \(j\) (cf. Definition 7), then \(\bm{z}_{i}\perp\bm{z}_{j}\mid\bm{z}_{S}\)._

The latent variables \(\bm{z}\) are unknown to the learner. Instead, the learner has access to observations \(\bm{x}\in\mathbb{R}^{n}\) (\(n\geqslant d\)) from all environments \(E\in\mathfrak{E}\) that are related to the latent \(\bm{z}\) via an injective mixing function \(\bm{g}\):

\[\bm{x}=\bm{g}(\bm{z}).\] (2)

The main assumption here that the mixing function is the same across all environments:

**Assumption 1**.: _All environments \(E\in\mathfrak{E}\) share the same diffeomorphic mixing function \(\bm{g}:\mathbb{R}^{d}\mapsto\mathbb{R}^{n}\)._

In CRL, the goal of the learner is to 1) recover the inverse of the mixing function \(\bm{h}=\bm{g}^{-1}\) (often called the _unmixing_ function) which allows recovering the latent variables given any observations, and, 2) recover the underlying causal graph \(\mathcal{G}\). In the remaining part of this paper, we refer to \((\bm{h},\mathcal{G})\) as the causal model to be learned. Obviously, there would be some ambiguities in learning \((\bm{h},\mathcal{G})\). For example, choosing a different permutation of the nodes in the causal graph would lead to a different model, and so does element-wise transformations on each component \(\bm{h}_{i}\) of \(\bm{h}\).

A line of recent works show that the ground-truth model can be identified up to these ambiguities in various settings, assuming access to single-node hard interventions [36; 49; 47]. On the other hand, some weaker notions of identifiability have also been proposed and studied in the literature [36; 46; 23] for single-node soft interventions. Here, we provide a generic definition of single-node soft interventions that we will rely on in this paper.

**Definition 2**.: _We say that a collection of environments \(\hat{\mathfrak{E}}\) is a set of (soft) interventions on a subset of latent variables \(\{\bm{z}_{j},j\in S\}\) if for any \(i\in[d]\) and any \(E_{1},E_{2}\in\hat{\mathfrak{E}},E_{1}\neq E_{2}\), we have \(p_{i}^{E_{1}}=p_{i}^{E_{2}}\) if and only if \(i\notin S\) (the notation \(p_{i}^{E}\) comes from (1)). Equivalently, we write \(\mathcal{I}_{\bm{z}}^{\hat{\mathfrak{E}}}=S\)._

We note that soft interventions are very different from hard interventions, since they do not remove causal relationships between latent variables. The goal of this paper is to address the following question:

_What is the best-achievable identification guarantee when hard interventions are not available, and what are the intrinsic ambiguities?_

## 3 The surrounding set and a notion of identifiability

One may expect that identifiability with soft interventions is not much different from hard interventions, since soft interventions can approximate hard interventions with arbitrary accuracy. However, we will show that this is not the case. At a high level, hard intervention is more powerful than soft intervention because it is capable of isolating a latent variable from its direct cause while soft interventions is not, so soft interventions can sometimes fail to identify the true causal relationship from a mixture of causal effects.

To quantify what kind of ambiguities may arise, we can define the surrounding set for each node in a causal graph \(\mathcal{G}\) as follows:

**Definition 3**.: _(46, Definition 3) For two nodes \(i,j\in[d]\) in \(\mathcal{G}\), we say that \(j\) is surrounded by \(i\), or \(i\in\mathrm{sur}_{\mathcal{G}}(j)\) if \(i\in\mathrm{pa}_{\mathcal{G}}(j)\), and \(\mathrm{ch}_{\mathcal{G}}(j)\subseteq\mathrm{ch}_{\mathcal{G}}(i)\). Moreover, we define \(\overline{\mathrm{sur}}_{\mathcal{G}}(j)=\mathrm{sur}_{\mathcal{G}}(j)\cup\{j\}\)._

Intuitively, if there exists some \(i\in\mathrm{sur}_{\mathcal{G}}(j)\), then ambiguities may arise for the causal variable at node \(j\), since any effect of \(j\) on any of its child \(k\) can also be interpreted as a mixture of the effect of and \(j\). In Appendix E we discuss an example with three causal variables to further illustrate such ambiguities.

Definition 3 naturally induces the following relationship between causal models:

**Definition 4**.: _Using the notations in Definition 10, we write \((\bm{h},\mathcal{G})\sim_{\mathrm{sur}}(\hat{\bm{h}},\hat{\mathcal{G}})\) if there exists a permutations \(\pi\) on \([d]\), and a diffeomorphism \(\psi:\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) where the \(j\)-th component of \(\psi\), denoted by \(\psi_{j}(\bm{z})\), is a function of \(\bm{z}_{\overline{\mathrm{sur}}_{\mathcal{G}}(j)}\) for \(\forall j\in[d]\), such that the following holds:_

* _For any_ \(i,j\in[d]\)_,_ \(i\in\mathrm{pa}_{\mathcal{G}}\left(j\right)\) _if and only if_ \(\pi(i)\in\mathrm{pa}_{\mathcal{G}}\left(\pi(j)\right)\)_, and_
* \(\bm{P}_{\overline{\pi}}\circ\hat{\bm{h}}=\psi\circ\bm{h}\)_, where_ \(\bm{P}_{\overline{\pi}}\) _is a permutation matrix satisfying_ \((\bm{P}_{\overline{\pi}})_{ij}=1\) _if_ \(j=\pi(i)\) _and_ \((\bm{P}_{\overline{\pi}})_{ij}=0\) _otherwise._

In other words, \(\sim_{\mathrm{sur}}\) requires that the causal graph to be exactly the same up to some permutation of nodes, but allows each latent variable \(\bm{v}_{i}\) to be a mixture of \(\bm{z}_{\mathrm{sur}_{\mathcal{G}}(i)}\). Although not obvious from definition, one can actually check that \(\sim_{\mathrm{sur}}\) defines an _equivalence relation_ (see Lemma11). Moreover, we will show in the following section that \(\sim_{\mathrm{sur}}\) is in general the best that we can hope for in our problem setting.

## 4 Identifiability theory for linear CRL with general environments

In this section, we consider learning causal models from _general_ environments. Specifically, we assume that the environments \(E_{k},k\in[K]\) share the same causal graph, but the dependencies between connected nodes (latent variables) are completely unknown, and, in contrast with existing literature on single-node interventions, we impose no similarity constraints on the environments. We begin our investigation of identifiability in this setting in the context of linear causal models with a linear mixing function.

### Problem setup

Formally, we assume the following generative model in \(K\) distinct environments \(\mathfrak{E}=\{E_{k}:k\in[K]\}\) with data generating process

\[\bm{z}=\bm{A}_{k}\bm{z}+\bm{\Omega}_{k}^{\frac{1}{2}}\epsilon,\quad\bm{x}=\bm {G}\bm{z}\quad k\in[K],\] (3)

where the matrix \(\bm{A}_{k}\) satisfies \((\bm{A}_{k})_{ij}\neq 0\) if and only if \(j\to i\) in \(\mathcal{G}\). We refer to \((\bm{A}_{k},\bm{\Omega}_{k})\) as the weight matrices of latent variables \(\bm{z}\) in the environment \(E_{k}\). It is easy to see that Assumption1 in our general setup translates into the following assumption:

**Assumption 2**.: _The mixing matrix \(\bm{G}\in\mathbb{R}^{n\times d}\) has full column rank. Equivalently, the unmixing matrix \(\bm{H}=\bm{G}^{\dagger}\) has full row rank._

Let \(\bm{B}_{k}=\bm{\Omega}_{k}^{-\frac{1}{2}}(\bm{I}-\bm{A}_{k}),k\in[K]\), then we have \(\epsilon=\bm{B}_{k}\bm{z}=\bm{B}_{k}\bm{H}\bm{x}\). Since in the linear case, there is an easy to see one-to-one correspondence between the matrix \(\bm{H}\) and the un-mixing function \(\bm{x}\mapsto\bm{H}\bm{x}\), we abuse the notation and write \((\bm{H},\mathcal{G})\) to represent the model instead of \((\bm{h},\mathcal{G})\). Using \(\bm{h}_{i}\) to denote the \(i\)-th row of \(\bm{H}\), the following lemma translates Definition4 the the linear setting:

**Lemma 1**.: _According to Definition4, \((\bm{H},\mathcal{G})\sim_{\mathrm{sur}}(\hat{\bm{H}},\hat{\mathcal{G}})\) if and only if there exists a permutation \(\pi\) on \([d]\), such that the following statements hold:_

Figure 1: An illustration of Definition3; here \(i\in\mathrm{sur}_{\mathcal{G}}(j)\).

1. _For all_ \(i,j\in[d]\)_,_ \(i\in\mathrm{pa}_{\mathcal{G}}(j)\) _if and only if_ \(\pi(i)\in\mathrm{pa}_{\hat{\mathcal{G}}}(\pi(j))\)_, and_
2. _For all_ \(i\in[d]\)_,_ \(\hat{\bm{h}}_{i}\in\mathrm{span}\left\langle\bm{h}_{j}:\pi(j)\in\overline{ \mathrm{su}}_{\mathcal{G}}(i)\right\rangle\)_._

We also need to make the following assumption on noise.

**Assumption 3**.: _The noise vector \(\epsilon\in\mathbb{R}^{d}\) has independent components, at most one component is Gaussian distributed, and any two components have different distribution._

The non-gaussianity of the noise vectors is a typical assumption in causal discovery within linear models [9, 39] and is always assumed in the LinGAM setting [38]. The assumption that all components have a different distribution is not so standard, but is quite natural in real-world scenarios.

### Identifiability guarantee

For each node \(i\in[d]\) of \(\mathcal{G}\), we use \(\bm{w}_{k}(i)\) to be the _weight vector_ of environment \(E_{k}\) at node \(i\), _i.e._, \(\bm{w}_{k}(i)=\left((\bm{A}_{k})_{ij}:j\in\mathrm{pa}_{\mathcal{G}}(i)\right) \in\mathbb{R}^{|\mathrm{pa}_{\mathcal{G}}(i)|}\). In other words, the structural equation for node \(i\) in environment \(k\) is of the form:

\[z_{i}=w_{k}(i)^{\top}z_{\mathrm{pa}_{\mathcal{G}}(i)}+\sqrt{\omega_{k,i,i}} \epsilon_{i}\] (4)

To obtain our identifiability result, the main assumption we need to make is the non-degeneracy of the weights at each node:

**Assumption 4**.: _For each node \(i\in[d]\) of \(\mathcal{G}\), we have \(\mathrm{aff}\left(\bm{w}_{k}(i):k\in[K]\right)=\mathbb{R}^{\left|\mathrm{pa}_{ \mathcal{G}}(i)\right|}\) where \(\mathrm{aff}(\cdot)\) denotes the affine hull. Equivalently, the weights \(\bm{w}_{k}(i),k=1,2,\cdots,K\) do not lie in a \(\left(\left|\mathrm{pa}_{\mathcal{G}}(i)\right|-1\right)\)-dimensional hyperplane of \(\mathbb{R}^{\left|\mathrm{pa}_{\mathcal{G}}(i)\right|}\)._

This assumption is quite mild since it only requires the weight vectors to be in general positions, and it holds with probability \(1\) if the weights at each node are sampled from continuous distributions. Moreover, as shown in Lemma 5, it is equivalent to the following assumption.

**Assumption 5** (Node-level non-degeneracy).: _We say that the matrices \(\left\{\bm{B}_{k}\right\}_{k=1}^{K}\) are node-level non-degenerate if for all node \(i\in[d]\), we have \(\dim\mathrm{span}\left\langle(\bm{B}_{k})_{i}:k\in[K]\right\rangle=\left| \mathrm{pa}_{\mathcal{G}}(i)\right|+1\), where \((\bm{B}_{k})_{i}\) is the \(i\)-th row of \(\bm{B}_{k}\)._

In the following, we state our main result in this section, which shows that \(K=d\) non-degenerate environments suffices for the model to be identifiable up to \(\sim_{\mathrm{sur}}\).

**Theorem 1**.: _Suppose that \(K\geqslant d\) and we have access to observations generated from the linear causal model \((\bm{H},\mathcal{G})\) across multiple environments \(\mathfrak{E}=\left\{E_{k}:k\in[K]\right\}\) with observation distributions \(\{\mathbb{P}_{\bm{x}}^{E}\}_{E\in\mathfrak{E}}\), and the data generating processes are given by (3). Let \((\hat{\bm{H}},\hat{\mathcal{G}})\) be any candidate solution with the hypothetical data generating process_

\[\bm{v}=\hat{\bm{A}}_{k}\bm{v}+\hat{\bm{\Omega}}_{k}^{\frac{1}{2}}\hat{ \epsilon},\quad\bm{x}=\hat{\bm{H}}^{\dagger}\bm{v}\quad\text{in the environment $E_{k}$}\]

_where \(\hat{\bm{H}}\) has full row rank, such that_

1. _the observation distribution that this hypothetical model generates in_ \(E_{k}\) _is exactly_ \(\mathbb{P}_{\bm{x}}^{E_{k}}\)_;_
2. _all environments share the same causal graph:_ \(\forall k\in[K]\) _and_ \(i,j\in[d]\)_,_ \((\bm{A}_{k})_{ij}\neq 0\Leftrightarrow j\in\mathrm{pa}_{\mathcal{G}}(i)\)_,_ \((\hat{\bm{A}}_{k})_{ij}\neq 0\Leftrightarrow j\in\mathrm{pa}_{\hat{ \mathcal{G}}}(i)\) _and_ \(\bm{\Omega}_{k},\hat{\bm{\Omega}}_{k}\) _are diagonal matrices with positive entries;_
3. \(\left\{\bm{B}_{k}\right\}_{k=1}^{K}\) _and_ \(\left\{\hat{\bm{B}}_{k}=\hat{\bm{\Omega}}_{k}^{-\frac{1}{2}}(\bm{I}-\hat{\bm{A} }_{k})\right\}_{k=1}^{K}\) _are non-degenerate in the sense of Assumption_ 5_;_
4. _the noise variables_ \(\epsilon\) _and_ \(\hat{\epsilon}\) _satisfy Assumption_ 3_._

_Then we must have \((\bm{H},\mathcal{G})\sim_{\mathrm{sur}}(\hat{\bm{H}},\hat{\mathcal{G}})\)._

The proof of Theorem 1 is given in Appendix H.1. In the next section, we will introduce an algorithm, LiNGCReL, that provably recovers the ground-truth up to \(\sim_{\mathrm{sur}}\).

To the best of our knowledge, this is the first identifiability guarantee in the literature for CRL from general environments, even for the linear case. Our result is closely related but fundamentally different from Xie et al. [52, 53], Dong et al. [11] that consider the task of linear CRL using _observational data_. As discussed before, with observational data the causal graph can at best be identified up to Markov equivalence. As a result, one typically requires additional assumptions on the structure of the causal graph to obtain stronger guarantees. In contrast, we show that with data from multiple environments, exact recovery of the causal graph is possible without any structural assumptions.

Interestingly, while the fact that existing works focus on single-node interventions seem to suggest that learning from diverse environments is hard, it turns out that such diversity is actually helpful. Specifically, we show that in the worst case, \(\Theta(d^{2})\) interventions are required for identifying the ground-truth model under \(\sim_{\mathrm{sur}}\):

**Theorem 2** (informal version of Theorem 6).: _There exists a causal graph \(\mathcal{G}\) with \(\Theta(d^{2})\) edges, such that for any unmixing matrix \(\bm{H}\in\mathbb{R}^{d\times n}\) with full row rank, any independent noise variables \(\epsilon\), and any \(0<s_{i}\leqslant\left|\mathrm{pa}_{\mathcal{G}}(i)\right|,i\in[d]\), the ground-truth model \((\bm{H},\mathcal{G})\) is non-identifiable up to \(\sim_{\mathrm{sur}}\) with \(s_{i}\) soft interventions for node \(i\), unless the (ground-truth and intervened) weights of the causal model lie in a null set (w.r.t the Lebesgue measure)._

A formal version and the proof of Theorem 2 can be found in Appendix H.2. On the other hand, by having \(d\) single-node interventions per node, Assumption 5 can be satisfied as long as the weights are in general positions, so in this case we have \((\bm{H},\mathcal{G})\sim_{\mathrm{sur}}(\hat{\bm{H}},\hat{\mathcal{G}})\) by Theorem 1. Therefore, Theorems 1 and 6 together imply that \(\Theta(d^{2})\) single-node interventions are necessary and sufficient for identification up to \(\sim_{\mathrm{sur}}\).

Given that Theorem 1 only guarantees identification up to \(\sim_{\mathrm{sur}}\) that is strictly weaker than full identification, one might naturally ask whether Theorem 1 can be further improved. Our last theorem in this section indicates that \(\sim_{\mathrm{sur}}\) is indeed a fundamental barrier that exists even when we access to _single node_, soft interventions.

**Theorem 3** (Counterpart to Theorem 1, informal version of Theorem 9).: _For any linear causal model \((\bm{H},\mathcal{G})\) and any set of environments \(\mathfrak{E}=\{E_{k}:k\in[K]\}\) such that all conditions in Theorem 1 are satisfied, there must exists a candidate solution \((\hat{\bm{H}},\mathcal{G})\) and a hypothetical data generating process that satisfy the same set of conditions, but_

\[\frac{\partial\bm{v}_{i}}{\partial\bm{z}_{j}}\neq 0,\quad\forall j\in \overline{\mathrm{sur}}_{\mathcal{G}}(i).\]

_Moreover, if we additionally assume that the environments are groups of single-node soft interventions, then we can guarantee the existence of \((\hat{\bm{H}},\mathcal{G})\) and weight matrices which, besides the properties listed above, are also groups of single-node soft interventions._

## 5 LinGReL: Algorithm for linear non-Gaussian causal representation learning

In this section, we introduce Linear Non-Gaussian Causal Representation Learning (LiNGCReL), an algorithm that provably recovers the underlying causal graph and latent variables up to \(\sim_{\mathrm{sur}}\) in the infinite-sample limit. At this point, it is instructive to recall the celebrated LiNGAM algorithm [38] for linear causal graph discovery. Different from their setting, we only observe some unknown linear mixture of the latent variables. Hence, running linear ICA as in LiNGAM only gives us \(\bm{M}_{k}=\bm{B}_{k}\bm{H}\) rather than the weight matrix \(\bm{B}_{k}\) itself.

The key idea in our approach is an effect cancellation scheme that allows us to determine the "remaining degree of freedom" (RDF) of any node (_a.k.a._ latent variable) given any subset of its ancestors. This scheme allows us to not only find a topological order of the nodes, but also figure out direct causes by tracking the changes of the RDF. In the following, we present the main steps of LiNGCReL in more details.

Suppose that we are given samples of observations \(\bm{X}^{(k)}=\left\{\bm{x}_{i}^{(k)}\right\}_{i=1}^{N},k\in[K]\) where \(\bm{x}_{i}^{(k)}\) is the \(i\)-th sample from the \(k\)-th environment.

**Step 1. Recover the matrices \(\bm{M}_{k}=\bm{B}_{k}\bm{H}\)** Since \(\epsilon=\bm{B}_{k}\bm{z}=\bm{B}_{k}\bm{H}\bm{x}\) in the \(k\)-th environment, so we can use any identification algorithm for linear ICA to recover the matrix \(\bm{M}_{k}\). Then we properly rearrange the rows of \(\bm{M}_{k}\) so that all \(\bm{M}_{k}\bm{x},k=1,2,\cdots,K\) correspond to the same permutation of noise variables. This step is quite standard and details can be found in Appendix B.1.

**Step 2. CRL based on \(\bm{M}_{k}\)** Now we have obtained \(\bm{M}_{k}=\bm{B}_{k}\bm{H}\), but the unmixing matrix \(\bm{H}\) is still unknown. We propose Algorithm 3 to learn \(\bm{H}\) and the causal graph \(\mathcal{G}\). The main part of Algorithm 3 contains a loop that maintains a node set \(S\) which, we will show later, is ancestral, _i.e._, \(i\in S\Rightarrow\operatorname{ans}_{\mathcal{G}}(i)\subseteq S\). In each round the algorithm finds a new node \(i\notin S\) such that \(\operatorname{ans}_{\mathcal{G}}(i)\subseteq S\), and a subroutine Identify-Parents (Algorithm 2) is used to find all parents of \(i\). After that, we append \(i\) into \(S\) and continue until \(S\) contains all nodes in \(\mathcal{G}\). Finally, the rows of the mixing matrix \(\bm{H}\) is obtained by intersections of properly-chosen row spaces of \(\bm{M}_{k}\).

Both Algorithm 2 and Algorithm 3 include a crucial step, which we call it _orthogonal projection_, as described in Algorithm 1. At a high level, it helps determine the minimal RDF for \(\bm{z}_{i}\) after fixing the latent variables \(\bm{z}_{S}\), and this exactly corresponds to the number of parents of \(\bm{z}_{i}\) that are not in \(\bm{z}_{S}\). We provide a simple example in Appendix E.2 to illustrate why this approach works.

The following result states that Algorithm 3 can recover the ground-truth causal model up to \(\sim_{\mathrm{sur}}\):

**Theorem 4**.: _Suppose that \(\bm{M}_{k},k\in[K]\) are perfectly identified in **Step 1**. Let \((\hat{\bm{H}},\hat{\mathcal{G}})\) be the solution returned by Algorithm 3, then we must have \((\bm{H},\mathcal{G})\sim_{\mathrm{sur}}(\hat{\bm{H}},\hat{\mathcal{G}})\)._

The full proof of Theorem 4 is given in Appendix H.3. It crucially relies on the following two propositions that reveal how Algorithm 3 and the subroutine Algorithm 2 work.

```
1:Input: Ordered set \(S=\{s_{1},s_{2},\cdots,s_{m}\}\subseteq[d]\), index \(i\notin S\), matrices \(\bm{M}_{k}\in\mathbb{R}^{d\times n}\), \(k\in[K]\)
2:Output: Set of vectors \(\{\bm{p}_{k}\}_{k=1}^{K}\)
3:for\(k\gets 1\) to \(K\)do
4:\(\bm{W}\leftarrow\operatorname{span}\left\langle(\bm{M}_{k})_{s}:s\in S\right\rangle \triangleright(\bm{M}_{k})_{s}\) is the \(s\)-th row of \(\bm{M}_{k}\)
5:\(\bm{p}_{k}\leftarrow\operatorname{proj}_{\bm{W}^{\perp}}\left((\bm{M}_{k})_{i}\right)\)
6:endfor ```

**Algorithm 1**Orthogonal-projections

**Proposition 1**.: _The following two propositions hold for Algorithm 3:_

* \(\operatorname{ans}_{\mathcal{G}}(i)\subseteq S\Leftrightarrow\) _the if condition in line 8 of Algorithm_ 3 _is fulfilled;_
* _the set_ \(S\) _maintained in Algorithm_ 3 _is always an ancestral set, in the sense that_ \(j\in S\Rightarrow\operatorname{ans}_{\mathcal{G}}(j)\subseteq S\)_._

**Proposition 2**.: _Given any ordered ancestral set \(S\) that contains \(\operatorname{pa}_{\mathcal{G}}(i)\) for some \(i\notin S\), Algorithm 2 returns a set \(P_{i}\subseteq S\) that is exactly \(\operatorname{pa}_{\mathcal{G}}(i)\)._

```
1:Input: An ordered set \(S=\{s_{1},s_{2},\cdots,s_{m}\}\subseteq[d]\), a node \(i\notin S\) and matrices \(\bm{M}_{k},k\in[K]\)
2:Output: The parent set \(P_{i}\) of node \(i\)
3:\(P_{i}\leftarrow\emptyset\)
4:for\(m^{\prime}\gets 0\) to \(m\)do
5:\(\{\bm{p}_{k}\}_{k=1}^{K}\leftarrow\)Orthogonal-projections\(\left(\{s_{j}:j\leqslant m^{\prime}\},i,\{\bm{M}_{k}\}_{k\in[K]}\right)\)
6:\(r_{m^{\prime}}\leftarrow\dim\operatorname{span}\left\langle\bm{p}_{k}:k\in[ K]\right\rangle\)
7:if\(m^{\prime}\geq 1\) and \(r_{m^{\prime}}=r_{m^{\prime}-1}-1\)then
8:\(P_{i}\gets P_{i}\cup\{m^{\prime}\}\)
9:endif
10:endfor ```

**Algorithm 2**Identify-Parents

## 6 Experiments

In this section, we present our experimental setup and results for LiNGCReL. Note that LiNGCReL as described in the previous section only works in the population regime. When the number of samples is limited, two main challenges in implementing LiNGCReL are to accurately compute the dimension of a subspace (line 6 of Algorithm 2 and line 8 of Algorithm 3), and to find a vector in the intersection of multiple subspaces (line 20, Algorithm 3). Due to space limit, the implementation details are described in Appendix B.2.

```
1:Input: Matrices \(\bm{M}_{k},k\in[K]\)
2:Output: The edge set \(\mathcal{E}\) on the vertex set \([d]\) and the mixing matrix \(\hat{\bm{H}}\)
3:\(S\leftarrow\emptyset\); \(\triangleright\)\(S\) is an ordered set of nodes
4:\(\mathcal{E}\leftarrow\emptyset\); \(\triangleright\)\(\mathcal{E}\) is the edge set
5:while\(|S|<d\)do
6:for\(i\notin S\)do
7:\(\{\bm{p}_{k}\}_{k=1}^{K}\leftarrow\texttt{Orthogonal-projections}\left(S,i,\{ \bm{M}_{k}\}_{k\in[K]}\right)\)
8:if\(\dim\operatorname{span}\left\langle\bm{q}_{k}:k\in[K]\right\rangle=1\)then
9:breakable\(\triangleright\) Proposition 1 guarantees that such an \(i\) must exist
10:endif
11:endfor
12:\(P_{i}\leftarrow\texttt{Identify-Parents}(S,i)\)
13:\(S\gets S\cup\{i\}\)
14:\(\mathcal{E}\leftarrow\mathcal{E}\cup\{(j,i):j\in P_{i}\}\)
15:endwhile
16:for\(i=1\) to \(d\)do
17:\(E_{i}\leftarrow\operatorname{span}\left\langle(\bm{M}_{k})_{i}:k\in[K]\right\rangle\)
18:endfor
19:for\(i=1\) to \(d\)do
20:\(\hat{\bm{h}}_{i}\leftarrow\) any non-zero vector in \(\left(\cap_{j:(i,j)\in\mathcal{E}}E_{j}\right)\cap E_{i}\)
21:endfor
22:\(\hat{\bm{H}}\leftarrow\left[\hat{\bm{h}}_{1}^{\top},\hat{\bm{h}}_{2}^{\top}, \cdots,\hat{\bm{h}}_{d}^{\top}\right]^{\top}\) ```

**Algorithm 3**Learn-Causal-Model

**Experimental setup.** We generate the independent noise variables from generalized Gaussian distributions \(p_{\beta}(x)\propto\exp\left(-\left|x\right|^{\beta}\right)\) with parameters \(\beta_{k}=0.2k^{2},k=1,2,\cdots,d\), multiplied by normalization constants to make their variances equal to \(1\). The ground-truth causal graph is generated by first fixing a total order of the vertices, say \(1,2,\cdots,d\), then add directed edges \(i\to j(i<j)\) according to i.i.d. Bernoulli(\(p\)) distributions, where \(p\in(0,1)\). The non-zero entries of matrices \(\bm{B}_{k}\) and \(\bm{H}\) are all generated independently from Gaussian distributions. For simplicity, we focus on the case \(n=d\) since recovery of the latent graphs only requires information from \(d\) components of \(\bm{x}\).

**Metrics of estimation error.** Since CRL seeks to learn both the causal graphs and the latent variables, for each output of our algorithm we first check if it exactly recovers the ground-truth causal graph. Then, recall that the latent variables and the observations are related by \(\bm{z}=\bm{H}\bm{x}\), given any output unmixing matrix \(\hat{\bm{H}}\) from Algorithm 3, we define the relative estimation error \(\Delta_{i}\) for \(\bm{z}_{i}\) as the solution of the following optimization problem:

\[\begin{split}\min\left\|\bm{\Delta}\right\|_{\infty}& s.t.\Delta_{i}=\frac{\left\|\operatorname{proj}_{ \operatorname{span}(\bm{h}_{i};j\in\overline{\operatorname{var}}_{\mathcal{G} }(i))}(\hat{\bm{h}}_{i})\right\|_{2}}{\left\|\hat{\bm{h}}_{i}\right\|_{2}}, \\ \hat{\bm{H}}=\bm{P}\hat{\bm{H}}\text{ for some signed permutation matrix }\bm{P}.\end{split}\] (5)

where signed permutation is allowed here since the noise distribution in our experiments is symmetric and the order of latent variables \(\bm{z}_{i},i=1,2,\cdots,d\) does not matter. We refer to the errors \(\Delta_{i}\) defined in (5) as the _SNA error_. The SNA error measures how much of the row \(\hat{\hat{\bm{h}}}_{i}\) that we learn is contained in the span of the ground-truth rows \(\bm{h}_{j},j\in\overline{\operatorname{var}_{\mathcal{G}}}(i)\). Indeed, recall that given any observation \(\bm{x}\), the ground-truth latent variable is \(\bm{z}=\bm{H}\bm{x}\) while our algorithm outputs \(\hat{\bm{v}}_{i}=\hat{\hat{\bm{h}}}_{i}^{\top}\bm{x}\), so the SNA error essentially captures whether the recovered latent variable is close to some linear mixture of latent variables in the effect-dominating set of \(i\). When the SNA error is zero for some node \(i\), we know that the recovered latent variable at node \(i\) is exactly a linear mixture of the ground-truth latent variables in \(\overline{\operatorname{var}_{\mathcal{G}}}(i)\), according to Lemma 1.

We also define the _true error_ for estimating each latent variable. Formally, let \(\hat{\bm{H}}\) be the unmixing matrix that corresponds to the solution of (5), then we define the true estimation error \(\tilde{\Delta}_{i}\) of \(\bm{z}_{i}\) as

\[\tilde{\Delta}_{i}=\left\|\left(\bm{I}-\bm{h}_{i}\bm{h}_{i}^{\top}\right)\hat{ \bm{h}}_{i}\right\|_{2}.\] (6)

**Results.** We randomly sample \(100\) causal models with size \(d=5\), \(30\) causal models with size \(d=8\) ad \(30\) causal models of size \(d=10\). In light of Theorem 1, for each \(d\in\{5,8,10\}\), we sample data from \(K=d\) randomly chosen environments; for \(d=5\) we also consider \(K=20\) to study how different choices of \(K\) can affect the result. We run LiNGCReL for each model with different sample sizes, compute the SNA error and true error of the obtained solution from (5) and (6) respectively for each latent variable, and check whether the ground-truth causal graph is exactly recovered.

Figure 2 shows how the average SNA error (over all latent variables) and the accuracy of graph recovery changes when sample size grows. We can see LiNGCReL successfully recovers about \(80\%\) of all models within each category, and the median of the average SNA error is smaller than \(1\%\). Moreover, by comparing Figure 1(a) with Figure 1(b), one can observe that if we fix the total number of samples but choose a larger \(K\) (_i.e._, fewer samples per environment), LiNGCReL can still achieve the same level of performance compared with the choice \(K=d\). Intuitively, this is because \(K\gg d\) vectors sampled from an \(r(r\leqslant d)\) dimensional subspace are unlikely to approximately lie in an \((r-1)\)-dimensional subspace, so that the calculation of line 6 of Algorithm 2 and line 8 of Algorithm 3 can be more accurate. We leave a better and quantitative understanding of the trade-off between \(d\) and \(K\) to future work.

**SNA error v.s. true error.** To understand the implication of our theory, we dive deeper by looking into the learning outcome of LiNGCReL on a specific model, of which the causal graph is shown in

Figure 2: _First two rows_: plots of SNA Error and graph recovery accuracy achieved by LiNGCReL as functions of sample size (per environment) for different choices of graph size \(d\) and number of environments \(K\). _Third row_: an example of causal graph generated in our experiments, and the estimation error of LiNGCReL for each node.

Figure 2e. In Figure 2f, we list the surrounding set of each node and the corresponding SNA error and true error. We can see that if \(\operatorname{sur}_{\mathcal{G}}(i)=\emptyset\), the two errors equal and both are small, but if \(\operatorname{sur}_{\mathcal{G}}(i)\neq\emptyset\), the true error is much larger than the SNA error. This indicates that LiNGCReL indeed learns the ground-truth model up to \(\sim_{\operatorname{sur}}\), as Theorem 1 predicts.

## 7 Conclusions

This paper studies the limit of learning identifiable causal representations using data from multiple environments. When hard interventions are not available, we provide theory and algorithm for identification up to SNA, and also show that SNA is an intrinsic ambiguity in our setting.

It is interesting to further investigate the setting where we do not assume that the causal model is linear. Moreover, it is important to understand the concrete form of available interventions in real-world applications. For instance, it is suggested that for single-cell genomics, the intervention is sometimes a "mixture" of hard and soft interventions, and sometimes can even reverse the direction of an edge [43]. Modelling such more complicated interventions appears to be crucial to reveal the underlying causal mechanisms in real-world problems.

## Acknowledgments and Disclosure of Funding

VS is supported by NSF Award IIS-2337916 and a 2023 Google Research Scholar Award.

## References

* Ahuja et al. [2023] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In _International Conference on Machine Learning_, pages 372-407. PMLR, 2023.
* Ahuja et al. [2023] Kartik Ahuja, Amin Mansouri, and Yixin Wang. Multi-domain causal representation learning via weak distributional invariances. _arXiv preprint arXiv:2310.02854_, 2023.
* Akhtar and Mian [2018] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. _Ieee Access_, 6:14410-14430, 2018.
* Bengio et al. [2013] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. _IEEE transactions on pattern analysis and machine intelligence_, 35(8):1798-1828, 2013.
* Brehmer et al. [2022] Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal representation learning. _Advances in Neural Information Processing Systems_, 35:38319-38331, 2022.
* Bubeck et al. [2023] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* Buchholz et al. [2023] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Scholkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. _arXiv preprint arXiv:2306.02235_, 2023.
* Campbell [2007] John Campbell. An interventionist approach to causation in psychology. _Causal learning: Psychology, philosophy, and computation_, pages 58-66, 2007.
* Comon [1994] Pierre Comon. Independent component analysis, a new concept? _Signal processing_, 36(3):287-314, 1994.
* Cooper and Yoo [1999] Gregory F Cooper and Changwon Yoo. Causal discovery from a mixture of experimental and observational data. In _Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence_, pages 116-125, 1999.

* [11] Xinshuai Dong, Biwei Huang, Ignavier Ng, Xiangchen Song, Yujia Zheng, Songyao Jin, Roberto Legaspi, Peter Spirtes, and Kun Zhang. A versatile causal discovery framework to allow causally-related hidden variables. In _The Twelfth International Conference on Learning Representations_, 2023.
* [12] Kevin N Dunbar and Jonathan A Fugelsang. Causal thinking in science: How scientists and students interpret the unexpected. In _Scientific and technological thinking_, pages 57-79. Psychology Press, 2004.
* [13] Frederick Eberhardt. Almost optimal intervention sets for causal discovery. In _Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence_, pages 161-168, 2008.
* [14] Frederick Eberhardt. Direct causes and the trouble with soft interventions. _Erkenntnis_, 79:755-777, 2014.
* [15] Markus I Eronen. Causal discovery and the problem of psychological interventions. _New Ideas in Psychology_, 59:100785, 2020.
* [16] Ronald Aylmer Fisher et al. The design of experiments. _The design of experiments._, (7th Ed), 1960.
* [17] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2(11):665-673, 2020.
* [18] Alain Hauser and Peter Buhlmann. Two optimal strategies for active learning of causal models from interventional data. _International Journal of Approximate Reasoning_, 55(4):926-939, 2014.
* [19] Keith J Holyoak and Patricia W Cheng. Causal learning and inference as a rational process: The new synthesis. _Annual review of psychology_, 62:135-163, 2011.
* [20] Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal discovery. _Journal of Machine Learning Research_, 14:3041-3071, 2013.
* [21] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217. PMLR, 2020.
* [22] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* [23] Wendong Liang, Armin Kekic, Julius von Kugelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, and Bernhard Scholkopf. Causal component analysis. _arXiv preprint arXiv:2305.17225_, 2023.
* [24] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. Biscuit: Causal representation learning from binary interactions. _arXiv preprint arXiv:2306.09643_, 2023.
* [25] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _international conference on machine learning_, pages 4114-4124. PMLR, 2019.
* [26] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. A sober look at the unsupervised learning of disentangled representations and their evaluation. _The Journal of Machine Learning Research_, 21(1):8629-8690, 2020.
* [27] Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In _International Conference on Machine Learning_, pages 6348-6359. PMLR, 2020.

* [28] Romain Lopez, Natasa Tagasovska, Stephen Ra, Kyunghyun Cho, Jonathan Pritchard, and Aviv Regev. Learning causal representations of single cells via sparse mechanism shift modeling. In _Conference on Causal Learning and Reasoning_, pages 662-691. PMLR, 2023.
* [29] Chaochao Lu, Yuhuai Wu, Jose Miguel Hernandez-Lobato, and Bernhard Scholkopf. Invariant causal representation learning for out-of-distribution generalization. In _International Conference on Learning Representations_, 2021.
* [30] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. _Advances in neural information processing systems_, 32, 2019.
* [31] Judea Pearl. _Causality_. Cambridge university press, 2009.
* [32] Geoffrey Roeder, Luke Metz, and Durk Kingma. On linear identifiability of learned representations. In _International Conference on Machine Learning_, pages 9030-9039. PMLR, 2021.
* [33] Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 62(12):1707-1739, 2009.
* [34] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* [35] J Schwartz. The formula for change in variables in a multiple integral. _The American Mathematical Monthly_, 61(2):81-85, 1954.
* [36] Anna Seigal, Chandler Squires, and Caroline Uhler. Linear causal disentanglement via interventions. _arXiv preprint arXiv:2211.16467_, 2022.
* [37] David R Shanks and Anthony Dickinson. Associative accounts of causality judgment. In _Psychology of learning and motivation_, volume 21, pages 229-261. Elsevier, 1988.
* [38] Shohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. _Journal of Machine Learning Research_, 7(10), 2006.
* [39] Ricardo Silva, Richard Scheines, Clark Glymour, Peter Spirtes, and David Maxwell Chickering. Learning the structure of linear latent variable models. _Journal of Machine Learning Research_, 7(2), 2006.
* [40] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* [41] Peter Spirtes, Clark N Glymour, and Richard Scheines. _Causation, prediction, and search_. 2000.
* [42] Michael Strevens. Review of woodward," making things happen", 2007.
* [43] Alejandro Tejada-Lapuerta, Paul Bertin, Stefan Bauer, Hananeh Aliee, Yoshua Bengio, and Fabian J Theis. Causal machine learning for single-cell genomics. _arXiv preprint arXiv:2310.14935_, 2023.
* [44] Robert E Tillman and Frederick Eberhardt. Learning causal structure from multiple datasets with similar variable sets. _Behaviormetrika_, 41(1):41-64, 2014.
* [45] Simon Tong and Daphne Koller. Active learning for structure in bayesian networks. In _International joint conference on artificial intelligence_, volume 17, pages 863-869. Citeseer, 2001.

* [46] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-based causal representation learning with interventions. _arXiv preprint arXiv:2301.08230_, 2023.
* [47] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, and Ali Tajer. General identifiability and achievability for causal representation learning. _arXiv preprint arXiv:2310.15450_, 2023.
* [48] Julius Von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. _Advances in neural information processing systems_, 34:16451-16467, 2021.
* [49] Julius von Kugelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic, Elias Bareinboim, David M Blei, and Bernhard Scholkopf. Nonparametric identifiability of causal representations from unknown interventions. _arXiv preprint arXiv:2306.00542_, 2023.
* [50] Tony Tong Wang, Adam Gleave, Tom Tseng, Kellin Pelrine, Nora Belrose, Joseph Miller, Michael D Dennis, Yawen Duan, Viktor Pogrebniak, Sergey Levine, et al. Adversarial policies beat superhuman go ais. 2023.
* [51] James Woodward. _Making things happen: A theory of causal explanation_. Oxford university press, 2005.
* [52] Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. Generalized independent noise condition for estimating latent variable causal graphs. _Advances in neural information processing systems_, 33:14891-14902, 2020.
* [53] Feng Xie, Biwei Huang, Zhengming Chen, Yangbo He, Zhi Geng, and Kun Zhang. Identification of linear non-gaussian latent hierarchical structure. In _International Conference on Machine Learning_, pages 24370-24387. PMLR, 2022.
* [54] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. _arXiv preprint arXiv:2307.06250_, 2023.

Related works

**The interventionist approach to causation** For the problem of causal graph discovery, it is well-known that the underlying causal structure is non-identifiable given only "passively observed" (equivalently, _i.i.d._) data alone. As a result, randomized controlled experiments [16] is often used to infer causality. These experiments typically take the form of interventions [41, 31], _i.e._, manipulations on the "natural state" of the system of interest. Early works [51, 42] define the "hard" (also called "surgical" or "arrow-breaking") interventions in which the value of the intervened variable is entirely determined by the experimenter, thereby removing the dependence of this variable on its direct causes. This type of intervention is arguably the most natural one to consider, and following this definition, a line of works explore sufficient conditions for designing experiments that guarantee identifiability of the causal model in various settings [10, 45, 13, 20, 18].

**Intervention _v.s._ passive observation** While extensive works demonstrate the success of the interventionist approach, it faces several key challenges that significantly limit its applicability. First, Eberhardt [14] finds that in the presence of unobserved variables, certain causal structures are indistinguishable if we only perform hard interventions. This issue can be resolved by performing soft interventions _i.e._, interventions that do not remove the dependency on direct causes but only changes the conditional distribution. Second, as pointed out in [44], interventions -- whether hard or soft -- are often expensive or even infeasible to perform in practice. For example, a psychological intervention is likely to affect multiple psychological variables simultaneously Eronen [15]. As a result, [44] returns to the "passive observation" setting but with multiple datasets with overlapping latent variables.

**Interventional causal representation learning** Motivated by the interventionist literature in causal graph discovery, a recent line of works [1, 36, 46, 49, 7, 54, 47] consider performing interventions to resolve the non-identifiability issue in causal representation learning [25]. Roughly speaking, these result indicate that identification (possibly with some ambiguities) is possible if one can perform intervention on every latent variable. However, it is unclear how to perform such interventions in practice, given that the underlying latent variables are unknown. Khemakhem et al. [21], Lu et al. [29], Roeder et al. [32] do not require single-node interventions to achieve identifiability, but assumes that the joint distribution of latent variables in each environment lie in a certain exponential family. This assumption can be understood as a prior on the latent variables, but it is unclear when or why it is reasonable to make in reality. Recently, Ahuja et al. [2] considers learning causal representations from multiple domains that relate to each other via an invariance constraint on the subset \(\mathcal{S}\) of _stable_ latent variables, and they prove identification up to affine mixtures within \(\mathcal{S}\).

## Appendix B Experiment details for Section 6

### Details for step 1 in Section 5

Since \(\epsilon=\bm{B}_{k}\bm{z}=\bm{B}_{k}\bm{H}\bm{x}\) in the \(k\)-th environment, so we can use any identification algorithm for linear ICA to recover the matrix \(\bm{M}_{k}\). Note that while standard linear ICA algorithms only apply to the case where \(n=d\), for \(n>d\) we can arbitrarily choose \(d\) principal components of \(\bm{x}\) to reduce it to the \(n=d\) case. This is without loss of generality, since when \(n>d\) there is redundant information in \(\bm{x}\).

After recovering \(\bm{M}_{k}\) for each \(k\) by running linear ICA, we still do not know whether each \(\bm{M}_{k}\bm{x}\) corresponds to the same permutation of the ground-truth noise variables \(\epsilon\). To resolve this issue, we choose test function \(\Psi\) mapping any distribution on \(\mathbb{R}\) to a deterministic real value, which we expect to take different values for different \(\epsilon_{i}\)'s. We choose \(\Psi(\mathbb{P})=\mathbb{P}\left|\left|X\right|\leqslant 1\right|\) in our experiments. For all \(k\geqslant 2\), we calculate the \(\Psi\) value of each component of the \(d\)-dimensional empirical distribution \(\hat{\mathbb{P}}_{k}=\frac{1}{N}\sum_{i=1}^{N}\mathds{1}_{\bm{M}_{k}\bm{x}_{ i}^{(k)}}\), and choose a permutation \(\pi_{k}\) to rearrange them in increasing order. Then, we rearrange the columns of \(\bm{M}_{k}\) using the same permutation \(\pi_{k}\). This procedure would asymptotically produce correct alignments as long as \(\Psi(\epsilon_{i}),i\in[d]\) are different, and we find that it empirically works well.

Alternatively, this alignment step can be done as follows: for each pair of environments \((E_{1},E_{t})\), and for each pair of nodes \((i,j)\), we calculate the distribution distance between \(\epsilon_{i}\) in environment \(E_{1}\) and \(\epsilon_{j}\) in environment \(E_{t}\), based on some notion of distribution distance (_e.g._ kernel maximum mean discrepancy). Then we find the min-cost perfect matching, where the cost of an edge is the distribution distance.

### Details for the implementation of LiNGCReL in the finite-sample regime

Although LiNGCReL provably works in the population regime, it faces several challenges when there is only a finite number of samples:

* First, since rank is not a continuous function, it is sensitive to finite-sample estimation errors. In our implementation of Algorithm3, in each iteration we instead choose \(i\notin S\) that has the largest ratio between the first and second singular values of \([\bm{q}_{1},\bm{q}_{2},\cdots,\bm{q}_{K}]\). And in line6 of Algorithm2, we introduce a hyper-parameter tl such that the matrix \([\bm{q}_{1},\bm{q}_{2},\cdots,\bm{q}_{K}]\) is considered to have rank \(r_{m^{\prime}-1}\) if its \(r_{m^{\prime}}\)-th singular value is smaller than tl. Since the smallest singular value of a random matrix \(\bm{A}\in\mathbb{R}^{K\times m}(K\geqslant m)\) is at the order of \(\sqrt{K}-\sqrt{m-1}\) with high probability [33], when \(K=d\) one shall choose \(\texttt{tl}\sim\sqrt{d}-\sqrt{d-1}=\mathcal{O}\left(\frac{1}{\sqrt{d}}\right)\). On the other hand, for larger \(K\) we can correspondingly choose a larger tl. Note that a small tl potentially has the risk of being dominated the noise in the estimation, which means that we need more samples per environment to reduce the noise. In contrast, for larger tl the estimation is more robust to noise and we can use fewer samples.
* Second, finite-sample estimation errors of \(\bm{M}_{k}\) make it harder to obtain \(\bm{h}_{i}\) in Algorithm3 of Algorithm3. We implement this step in the following way: first let \(\bm{Q}_{j}\) be the orthogonal projection matrix onto \(E_{j}^{\top}\)_i.e._, \(\bm{Q}_{j}^{\top}\bm{x}=\operatorname{proj}_{E_{j}^{\bot}}(\bm{x})\), then choose \(\bm{h}_{i}\) to be the singular vector of \(\sum_{j:(j,i)\in\mathcal{E}\text{ or }j=i}\bm{Q}_{j}^{\top}\bm{Q}_{j}\) that corresponds to the smallest singular value (including zero). Indeed, in the noiseless case we would have \(\left(\sum_{j:(j,i)\in\mathcal{E}\text{ or }j=i}\bm{Q}_{j}^{\top}\bm{Q}_{j}\right)\bm{h}_{i}=0\) if and only if \(\bm{h}_{i}\in\left(\cap_{j:(i,j)\in\mathcal{E}}E_{j}\right)\cap E_{i}\).

## Appendix C Further experiment results

**SNA error v.s. true error** We plot the SNA error v.s. true error achieved by LiNGCReL in Figure3. We observe that

* For most nodes, SNA error is exactly equal to the true error and both errors are small, indicating that the corresponding latent variables have been successfully learned by LiNGCReL.
* The remaining nodes typically have true error much larger than SNA error. This indicates that there exists some ambiguities at these nodes in the sense that \(\operatorname{sur}_{\mathcal{G}}(i)\neq\emptyset\). Note that the true error for many nodes are close to \(1\); one possible reason is that one selects the wrong singular vector in the second part of AppendixB.2, so that it is orthogonal to the ground-truth vector.

**Sensitivity of LiNGCReL to the hyperparameter tl** We examine how different choices of tl would affect the performance of LiNGCReL. Specifically, we run LiNGCReL on the \(100\) models with size \(d=5\) and number of environments \(K=5\) sampled in Section6 with \(\texttt{tl}\in\{0.1,0.15,0.2,0.25,0.3\}\) and the results are reported in Figure4. We can see that the permance is actually quite sensitive to tl.

## Appendix D Background on causal representation learning

It is common to assume some axioms on what kind of (conditional) dependency information is encoded in a causal graph (see 41, Section 3.4 for a detailed discussion). The most natural one is the Causal Markov Condition introduced in Definition 1 that gives sufficient conditions for conditional independence via \(d\)-separation. We introduce the formal definition of \(d\)-separation below:

**Definition 5** (paths and colliders).: _Let \(i,j\) be two nodes of a DAG \(\mathcal{G}\), a path is a sequence of nodes \(i_{0}=i,i_{1},\cdots,i_{k}=j\) such that there is an edge (in either direction) between \(i_{j}\) and \(i_{j+1},j=0,1,\cdots,k-1\). A node \(i_{j}\) is called a collider on this path if \(i_{j}\in\mathrm{ch}_{\mathcal{G}}(i_{j-1})\cap\mathrm{ch}_{\mathcal{G}}(i_{j+1})\)._

**Definition 6** (blocked path).: _A path in a DAG \(\mathcal{G}\) between node \(i\) and node \(j\) is said to be blocked by a node set \(S\) if either of the following holds:_

* _there exists a node_ \(v\) _on the path that is in_ \(S\) _but not a collider, or_
* _there exists a node_ \(v\) _on the path that is a collider, but none of its descendants (including itself) are in_ \(S\)_._

**Definition 7** (d-separation).: _For a DAG \(\mathcal{G}\) with node set \([d]\), any two nodes \(i\neq j\) are said to be \(d\)-separated by a set \(S\subset[d]\setminus\{i,j\}\) if all paths from \(i\) to \(j\) are blocked by \(S\)._

The _minimality_ condition states that there is no redundant edges in the causal graph, and is a natural consequence of the Occam's Razor Principle.

**Assumption 6** (Causal minimality, 41, Section 3.4.2).: _For latent variables \(\bm{z}\), removing any edge from \(\mathcal{G}\) would render violation of the causal Markov condition Definition 1. In other words, let \(\mathcal{G}_{1}\) be the graph obtained by removing any single edge from \(\mathcal{G}\), then there must exist \(i\in[d]\) such that \(\bm{z}_{i}\not\models\bm{z}_{\mathrm{nd}_{\mathcal{G}_{1}}(i)}\mid\bm{z}_{ \mathrm{pa}_{\mathcal{G}_{1}}(i)}\)._

Figure 4: Performance of LiNGCReL as a function of tl. tl\(=0.15\) achieves the best performance in terms of both SNA error and graph recovery accuracy.

Figure 3: Comparing SNA error with true error for the \(500\) latent variables in the \(100\) graphs of size \(d=5\) that we sample in Section 6.

The _faithfulness_ condition states that the Causal Markov Condition actually entails all (conditional) independence in the latent variables.

**Assumption 7** (Faithfulness, 41, Section 3.4.3).: _Every (conditional) independence in the latent variables \(\bm{z}\) is entailed by the Causal Markov Condition applied to \(\mathcal{G}\). In other words, \(\bm{z}_{i}\perp\!\!\!\!\perp\bm{z}_{j}\mid\bm{z}_{S}\Leftrightarrow i,j\) are \(d\)-separated by \(S\)._

Existing works have explored different notions of identifiability. For observational data, it is well known that Markov equivalence of graphs is an intrinsic ambiguity that one cannot resolve:

**Definition 8** (Markov equivalence/Faithful Indistinguishability, 41, Section 4.2).: _If two DAGs encodes the same set of dependency relations, we say that they are Markov equivalent._

Any DAG \(\mathcal{G}\) induces a partial order on its nodes which we denote by \(\prec_{\mathcal{G}}\). In the special case when for all \(i\neq j\), either \(i\prec_{\mathcal{G}}j\) or \(j\prec_{\mathcal{G}}i\) holds, we say that \(\prec_{\mathcal{G}}\) is a total order. This partial order is equivalent to the transitional closure of the graph, as defined below:

**Definition 9** (Transitional closure).: _Given any DAG \(\mathcal{G}\), its transitional closure \(\mathcal{\bar{G}}\) is defined to be the graph obtained by connecting all edges \(i\to j\) where \(i\) is an ancestor of \(j\) in \(\mathcal{G}\)._

When \(\prec_{\mathcal{G}}\) is a total order, each pair of nodes are connected by a directed edge in its transitive closure \(\mathcal{\bar{G}}\). Such \(\mathcal{\bar{G}}\) is often called a _tournament_ in graph theory.

In the following, we list different forms of identifiability that appear in the literature:

**Definition 10** (different notions of identifiability).: _Let \(\mathcal{H}:\mathbb{R}^{n}\supseteq\mathcal{X}\mapsto\mathbb{R}^{d}\) be the space of diffeomorphic mappings from observation to latent, and \(\mathfrak{G}\) be the space of all DAGs with \(d\) nodes, then for \(h,\hat{h}\in\mathcal{H}\) and \(\mathcal{G},\hat{\mathcal{G}}\in\mathfrak{G}\), we write_

1. _[_13_,_ 23_]_ \((h,\mathcal{G})\stackrel{{ T}}{{\sim}}_{G}(\hat{h},\hat{\mathcal{G}})\) _if there exists a permutation_ \(\pi\) _on_ \([d]\) _such that_ \(\pi(\mathcal{G})\) _and_ \(\hat{\mathcal{G}}\) _have the same transitional closure;_
2. _[_49_,_ 47_]_ \((h,\mathcal{G})\sim_{\mathrm{CRL}}(\hat{h},\hat{\mathcal{G}})\) _if we actually have_ \(\mathcal{G}=\hat{\mathcal{G}}\) _for the_ \(\phi\) _defined above._

Given an equivalence relation \(\sim\) on \(\mathcal{H}\times\mathfrak{G}\), we say that a causal model \((\bm{h},\mathcal{G})\) is identifiable under \(\sim\) if any candidate solution \((\hat{\bm{h}},\hat{\mathcal{G}})\) satisfies \((\hat{\bm{h}},\hat{\mathcal{G}})\sim(\bm{h},\mathcal{G})\). The notion of identification up to \(\stackrel{{ T}}{{\sim}}_{G}\), as shown in Seigal et al. [36] with single-node soft interventions on linear causal models, is highly related to this paper. Compared with their result, our \(\sim_{\mathrm{sur}}\) guarantee is must stronger, since not only the causal graph can be fully recovered, but the latent variables can be identified up to mixtures of the effect-dominating sets as well.

## Appendix E Illustrating examples for our theory and algorithm

### An example for understanding the SNA ambiguity

We provide a simple example below to illustrate the SNA ambiguity discussed in Section3.

**Example 1**.: _Let \(G\) be a causal graph with \(d=3\) nodes and edges \(1\to 2\) and \(2\to 3\). We have access to observations from a set of environments \(\mathfrak{G}\). It turns out that there is no way to distinguish between the following two structural equation models:_

\[\bm{z}_{1} =\epsilon_{1}^{E} \bm{v}_{1} =\epsilon_{1}^{E}\] \[\bm{z}_{2} =f_{2}^{E}(\bm{z}_{1},\epsilon_{2}^{E}) \bm{v}_{2} =f_{2}^{E}(\bm{v}_{1},\epsilon_{2}^{E})\] \[\bm{z}_{3} =f_{3}^{E}(\bm{z}_{2},\epsilon_{3}^{E}) \bm{v}_{3} =\bm{v}_{2}+f_{3}^{E}(\bm{v}_{2},\epsilon_{3}^{E})\] \[\bm{x} =(\bm{z}_{1},\bm{z}_{2},\bm{z}_{3})^{\top} \bm{x} =(\bm{v}_{1},\bm{v}_{2},\bm{v}_{3}-\bm{v}_{2})^{\top}\]

_where \(\epsilon_{i}^{E},i=1,2,3\) are independent noise variables, if we do not change the causal graph \(\mathcal{G}\), no matter what environment \(E\) that we have._

_This issue does not exist when we assume access to hard interventions on node \(3\), which effectively removes the edge \(2\to 3\). Specifically, with hard intervention on \(\bm{z}_{3}\), the variables \(\bm{z}_{2}\) and \(\bm{z}_{3}\) become independent. But by definition, \(\bm{v}_{2}=\bm{z}_{2}\) and \(\bm{v}_{3}=\bm{z}_{2}+\bm{z}_{3}\) must be dependent, so this intervention cannot be realized by any hard intervention on \(\bm{v}_{3}\), thereby providing a way to distinguish between the above models._

_Without node \(3\), the same ambiguity would arise on node \(2\). However, node \(3\) can help us to overcome this ambiguity, thanks to the fact that node \(2\) is the only causal parent of node \(3\). Suppose for example that \(\bm{v}_{2}=m(\bm{z}_{1},\bm{z}_{2})\) is some mixture of \(\bm{z}_{1}\) and \(\bm{z}_{2}\), then \(\bm{v}_{3}=\hat{f}_{3}^{E}\left(\bm{v}_{2},\bm{\epsilon}_{3}^{E}\right)=\hat{f }_{3}^{E}\left(m(\bm{z}_{1},\bm{z}_{2}),\bm{\epsilon}_{3}^{E}\right)\). Since all environments share the same mixing function, \(\bm{v}_{3}\) must be some deterministic function \(\psi_{3}(\bm{z})\) of \(\bm{z}\), where \(\psi_{3}\) is the same across all environment \(E\). Hence, we have_

\[\hat{f}_{3}^{E}\left(m(\bm{z}_{1},\bm{z}_{2}),\epsilon_{3}^{E}\right)=\psi_{3 }\left(\bm{z}_{1},\bm{z}_{2},f_{3}^{E}(\bm{z}_{2},\bm{\epsilon}_{3}^{E})\right)\] (7)

_Now we note that the dependencies of LHS on \(\bm{z}_{1}\) and \(\bm{z}_{2}\) are through a single scalar-valued function \(m\), but since we would have different \(f_{3}^{E}\)'s in different environments, this in general does not hold for the RHS. Therefore, any causal model with latent variable \(\bm{v}_{2}\) as a mixture of \(\bm{z}_{1}\) and \(\bm{z}_{2}\) cannot be equivalent to the ground-truth model._

According to Definition3, in Example1 we have \(\operatorname{sur}_{\mathcal{G}}(1)=\operatorname{sur}_{\mathcal{G}}(2)=\emptyset\) but \(\operatorname{sur}_{\mathcal{G}}(3)=\{2\}\).

### An example for the main idea behind LiNGCReL

To illustrate our main algorithm on how we can recover the graph \(\mathcal{G}\) and the matrix \(\bm{H}\), we first provide some intuition using a simple three-node example:

**Example 2**.: _Let \(\mathcal{G}\) be the graph with \(d=3\) nodes and edges \(1\to 2,1\to 3\) and \(2\to 3\), so that each \(\bm{B}_{k}\) is of form_

\[\bm{B}_{k}=\begin{pmatrix}\times&0&0\\ \times&\times&0\\ \times&\times&\times\end{pmatrix}\begin{pmatrix}\rightsquigarrow&\bm{b}_{k1}\\ \rightsquigarrow&\bm{b}_{k2}\\ \rightsquigarrow&\bm{b}_{k3}\end{pmatrix}\] (8)

_We can identify the graph as follows: first, for \(i\in\{1,2,3\}\), look at the space \(\bm{W}_{i}\) spanned by the rows \((\bm{M}_{k})_{i},k\in[K]\). If \(\dim\bm{W}_{i}=1\), we know that \(i\) is a source node (i.e., \(\operatorname{pa}_{\mathcal{G}}(i)=\emptyset\)) in \(\mathcal{G}\). Otherwise it is not, due to Assumption5. Hence we can know that node \(1\) is a source node._

_In our example, there is no other node that satisfies this requirement. We then proceed to search for some \(i\neq 1\) such that the projection of \(\bm{W}_{i}\) onto \(\bm{W}_{1}^{\perp}\) has dimension \(1\). If this holds, then one can show that \(\operatorname{pa}_{\mathcal{G}}(i)=\{1\}\). Otherwise, \(i\) must have parents other than \(1\)._

_It turns this requirement is satisfied for node \(2\) since \(\dim\left(\operatorname{proj}_{\bm{h}_{1}}\operatorname{span}\left\langle\bm {h}_{1},\bm{h}_{2}\right\rangle\right)=1\), but is not satisfied for node \(3\) since \(\dim\left(\operatorname{proj}_{\bm{h}_{1}}\operatorname{span}\left\langle\bm {h}_{1},\bm{h}_{2},\bm{h}_{3}\right\rangle\right)\geqslant 2\) (by Lemma4). Hence we know that \(\operatorname{pa}_{\mathcal{G}}(2)=\{1\}\)._

_Finally, it remains to determine \(\operatorname{pa}_{\mathcal{G}}(3)\). To do this, we first note that \(\dim\bm{W}_{3}=3\). Then we project \(\bm{W}_{3}\) onto \(\bm{W}_{1}^{\perp}\) and \(\bm{W}_{2}^{\perp}\) respectively, and the resulting dimensions are \(2\) and \(1\). As we rigorously show in Proposition2, a decrease of the dimension exactly indicates finding a new parent. Thus we have \(\operatorname{pa}_{\mathcal{G}}(3)=\{1,2\}\), completing the recovery of the graph._

_Finally, we recover the unmixing matrix \(\bm{H}\) (and thus the latent variables) by noticing that \(\bm{h}_{1}\in\bm{W}_{1}\), \(\bm{h}_{2}\in\bm{W}_{2}\cap\bm{W}_{3}\) and \(\bm{h}_{3}\in\bm{W}_{3}\). Ambiguities would arise at nodes \(2\) and \(3\), which are exactly the nodes that have non-empty effect-dominating sets._

## Appendix F Auxiliary lemmas

**Lemma 2**.: _For any family of \(m\)-dimensional vectors \(\{\bm{v}_{k}\}_{k=1}^{K}\) and \(\{\bm{z}_{k}\}_{k=1}^{K}\) if \(\bm{v}_{k}=\bm{z}_{k}\bm{T}\) and \(\bm{T}\in\mathbb{R}^{m\times m}\) is invertible, then_

\[\dim\operatorname{span}\left\langle\bm{v}_{k}:k\in[K]\right\rangle=\dim \operatorname{span}\left\langle\bm{z}_{k}:k\in[K]\right\rangle\]

**Theorem 5** (Darmois-Skitovic Theorem).: _Let \(\epsilon_{i},i\in[d]\) be independent random variables and \(X=\sum_{i=1}^{d}\alpha_{i}\epsilon_{i},Y=\sum_{i=1}^{d}\beta_{i}\epsilon_{i}\). If \(X\perp\!\!\!\perp Y\), then for \(\forall i\in[d]\), \(\alpha_{i}\beta_{i}\neq 0\Rightarrow\epsilon_{i}\) is Gaussian distributed._

**Lemma 3**.: _Suppose that \(\epsilon=(\epsilon_{1},\cdots,\epsilon_{d})\) is a \(d\)-dimensional random vector with independent components such that \(\operatorname{Var}(\epsilon_{i})=1,\forall i\in[d]\), and there exists an invertible and non-diagonal matrix \(\bm{M}\) such that \(\bm{M}\epsilon\stackrel{{ d}}{{=}}\epsilon\), then at least one of the following statements must hold:_1. _there exists at least two Gaussian variables in_ \(\epsilon_{1},\cdots,\epsilon_{d}\)_;_
2. \(\bm{M}\) _is a permutation matrix and there exists_ \(1\leqslant i<j\leqslant d\) _such that_ \(\epsilon_{i}\stackrel{{ d}}{{=}}\epsilon_{j}\)_._

Proof.: Suppose that (1) does not hold, then there is at most one Gaussian variable in \(\epsilon_{1},\cdots,\epsilon_{d}\). We assume WLOG that \(\epsilon_{1},\cdots,\epsilon_{d-1}\) are all non-Gaussian. Then by the Darmois-Skitovic Theorem, we know that for \(\forall 1\leqslant j<k\leqslant[d]\) and \(i\in[d-1]\), \(\bm{M}_{ji}\cdot\bm{M}_{ki}=0\Rightarrow\) there is at most one non-zero entry in each of the first \(d-1\) columns of \(\bm{M}\).

Assume that \(\bm{M}_{k_{i},i}\neq 0\), \(i\in[d-1]\). Since \(\bm{M}\) is invertible, we know that \(k_{i},i\in[d-1]\) must be different. Let \(k_{d}\) be the remaining element in \([d]\) that does not appear in \(k_{i},i<d\), then \((\bm{M}\epsilon)_{k_{d}}=\bm{M}_{k_{d},d}\epsilon_{d}\), while \((\bm{M}\epsilon)_{k_{i}}=\bm{M}_{k_{i},i}\epsilon_{i}+\bm{M}_{k_{i},d}\epsilon _{d}\). Since the components of \(\bm{M}\epsilon\) are independent, it is easy to see that \(\bm{M}_{id}\neq 0,\forall i\neq k_{d}\). In other words, \(\bm{M}\) only has non-zero entries at \((k_{i},i),i\in[d]\).

Since \(\mathrm{Var}(\epsilon_{i})=1\), we know that \(\bm{M}\) must be a signed permutation matrix. Finally, let \(\pi\) be the permutation on \([d]\) such that \(\bm{M}_{i,\pi(i)}\neq 0\). Since \(\bm{M}\) is not diagonal, \(\pi\) must have a cycle \((i_{1},i_{2},\cdots,i_{k})\) with length \(k\geqslant 2\), so that \(\epsilon_{i_{1}},\cdots,\epsilon_{i_{k}}\) all have the same distribution, which implies that (2) holds, as desired. 

**Lemma 4**.: _Let \(\bm{V}_{1},\bm{V}_{2}\) be two subspaces of \(\mathbb{R}^{d}\) such that \(\bm{V}_{1}\cap\bm{V}_{2}=\{\bm{0}\}\), and \(\bm{P}_{\bm{V}_{1}^{\perp}}\) be the orthogonal projection onto \(\bm{V}_{1}^{\perp}\), then we have that \(\dim(\bm{V}_{2})=\dim\left(\bm{P}_{\bm{V}_{1}^{\perp}}\bm{V}_{2}\right)\)._

Proof.: Obviously we have \(\dim(\bm{V}_{2})\geqslant\dim\left(\bm{P}_{\bm{V}_{1}^{\perp}}\bm{V}_{2}\right)\). On the other hand, let \(\bm{u}_{1},\bm{u}_{2},\cdots,\bm{u}_{m}\) be a basis of \(\bm{V}_{2}\), then \(\bm{w}_{i}=\bm{P}_{\bm{V}_{1}^{\perp}}\bm{u}_{i},i=1,2,\cdots,m\) are also independent. Indeed, suppose that \(\lambda_{i},i=1,2,\cdots,m\) satisfy \(\sum_{i=1}^{m}\lambda_{i}\bm{w}_{i}=0\), then \(\bm{P}_{\bm{V}_{1}^{\perp}}\left(\sum_{i=1}^{m}\lambda_{i}\bm{u}_{i}\right)=0\), implying that \(\sum_{i=1}^{m}\lambda_{i}\bm{u}_{i}\in\bm{V}_{1}\). However, we know that \(\bm{V}_{1}\cap\bm{V}_{2}=\{\bm{0}\}\), so \(\lambda_{1}=\cdots=\lambda_{m}=0\). This concludes the proof. 

**Lemma 5**.: _Assumption 4 is equivalent to Assumption 5._

Proof.: The main observation is that for each \(k\in[K]\), \((\bm{B}_{k})_{i}\) only has non-zero entries at the \(j\)-th coordinate where \(j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\). Moreover, let \(\tilde{\bm{w}}_{k}(i)\) be the vector consisting of these entries, then \(\tilde{\bm{w}}_{k}(i)=(\bm{\Omega}_{k})_{ii}^{-\frac{1}{2}}\left(-\bm{w}_{k}(i ),1\right)\). Hence,

\[\dim\operatorname{span}\left\langle(\bm{B}_{k})_{i}:k\in[K]\right\rangle=\dim \operatorname{span}\left\langle(-\bm{w}_{k}(i),1):k\in[K]\right\rangle.\]

Suppose that Assumption 4 holds, then for \(\forall\bm{x}\in\mathbb{R}^{|\mathrm{pa}_{\mathcal{G}}(i)|}\), there exists \(\lambda_{k}\in\mathbb{R},1\leqslant k\leqslant\left|\mathrm{pa}_{\mathcal{G}} (i)\right|\) such that \(\sum_{k}\lambda_{k}=1\) and \(\sum_{k}\lambda_{k}\bm{w}_{k}(i)=\bm{x}\). Hence,

\[(\bm{x},1)=\sum_{k}\lambda_{k}\tilde{\bm{w}}_{k}(i)\in\operatorname{span} \left\langle(\bm{B}_{k})_{i}:k\in[K]\right\rangle.\]

This immediately implies that \(\operatorname{span}\left\langle(\bm{B}_{k})_{i}:k\in[K]\right\rangle=\mathbb{R }^{|\mathrm{pa}_{\mathcal{G}}(i)|+1}\), so that Assumption 5 holds.

Conversely, suppose that Assumption 5 holds, then for \(\forall\bm{x}\in\mathbb{R}^{|\mathrm{pa}_{\mathcal{G}}(i)|}\), there exists \(\lambda_{k}\in\mathbb{R},1\leqslant k\leqslant\left|\mathrm{pa}_{\mathcal{G}} (i)\right|\) such that \(\sum_{k}\lambda_{k}\tilde{\bm{w}}_{k}(i)=(\bm{x},1)\). Hence we have \(\sum_{k}\lambda_{k}\bm{w}_{k}(i)=\bm{x}\) and \(\sum_{k}\lambda_{k}=1\), implying Assumption 4. 

## Appendix G Properties of effect-domination sets

**Lemma 6**.:
* \(j\in\mathrm{sur}_{\mathcal{G}}(i)\) _if and only if_ \(\overline{\mathrm{ch}}_{\mathcal{G}}(i)\subseteq\mathrm{ch}_{\mathcal{G}}(j)\)_;_
* _when_ \(i\neq j\)_,_ \(j\in\mathrm{sur}_{\mathcal{G}}(i)\) _if and only if_ \(\overline{\mathrm{ch}}_{\mathcal{G}}(i)\subseteq\overline{\mathrm{ch}}_{ \mathcal{G}}(j)\)_._

Proof.: If \(j\in\mathrm{sur}_{\mathcal{G}}(i)\), by definition \(i\in\mathrm{ch}_{\mathcal{G}}(j)\) and \(\mathrm{ch}_{\mathcal{G}}(i)\subseteq\mathrm{ch}_{\mathcal{G}}(j)\), so that \(\overline{\mathrm{ch}}_{\mathcal{G}}(i)\subseteq\mathrm{ch}_{\mathcal{G}}(j)\). Conversely, \(\overline{\mathrm{ch}}_{\mathcal{G}}(i)\subseteq\mathrm{ch}_{\mathcal{G}}(j)\) implies that \(i\in\mathrm{ch}_{\mathcal{G}}(j)\) and \(\mathrm{ch}_{\mathcal{G}}(i)\subseteq\mathrm{ch}_{\mathcal{G}}(j)\), so \(j\in\mathrm{sur}_{\mathcal{G}}(i)\). This proves the first claim.

To prove the second claim, assume that \(\operatorname{ch}_{\mathcal{G}}(i)\subseteq\operatorname{ch}_{\mathcal{G}}(j)\) holds but \(\operatorname{ch}_{\mathcal{G}}(i)\subseteq\operatorname{ch}_{\mathcal{G}}(j)\) does not hold, then we must have \(j\in\operatorname{ch}_{\mathcal{G}}(i)\). since \(j\neq i\), we have \(j\in\operatorname{ch}_{\mathcal{G}}(i)\), but then \(i\notin\overline{\operatorname{ch}}_{\mathcal{G}}(j)\), which is a contradiction. Hence \(\overline{\operatorname{ch}}_{\mathcal{G}}(i)\subseteq\operatorname{ch}_{ \mathcal{G}}(j)\) and the conclusion follows from the first claim. 

**Lemma 7**.: _Let \(\mathcal{G}\) be a DAG and \(i\) be its node, then for \(\forall j\in\operatorname{pa}_{\mathcal{G}}(i)\), we have \(\operatorname{sur}_{\mathcal{G}}(j)\subseteq\operatorname{pa}_{\mathcal{G}}(i)\)._

Proof.: Let \(k\in\operatorname{sur}_{\mathcal{G}}(j)\), then by definition we have \(\operatorname{ch}_{\mathcal{G}}(j)\subseteq\operatorname{ch}_{\mathcal{G}}(k)\). In particular, we have \(i\in\operatorname{ch}_{\mathcal{G}}(k)\Rightarrow k\in\operatorname{pa}_{ \mathcal{G}}(i)\). 

**Lemma 8**.: _Let \(\mathcal{G}\) be a DAG and \(i\) be its node, then for \(\forall j\in\operatorname{sur}_{\mathcal{G}}(i)\), we have \(\operatorname{sur}_{\mathcal{G}}(j)\subseteq\operatorname{sur}_{\mathcal{G}}(i)\)._

Proof.: Let \(k\in\operatorname{sur}_{\mathcal{G}}(j)\), then by definition we have \(\overline{\operatorname{ch}}_{\mathcal{G}}(j)\subset\overline{\operatorname{ ch}}_{\mathcal{G}}(k)\). We also know that \(\overline{\operatorname{ch}}_{\mathcal{G}}(i)\subset\overline{\operatorname{ ch}}_{\mathcal{G}}(j)\), so \(\overline{\operatorname{ch}}_{\mathcal{G}}(i)\subset\overline{\operatorname{ ch}}_{\mathcal{G}}(k)\), implying that \(k\in\operatorname{sur}_{\mathcal{G}}(i)\). 

**Lemma 9**.: _If \(\bm{M}\in\mathcal{M}^{0}_{\operatorname{sur}}(\mathcal{G})\), then \(\bm{M}^{-1}\in\mathcal{M}^{0}_{\operatorname{sur}}(\mathcal{G})\)._

Proof.: Assume WLOG that the nodes of \(\mathcal{G}\) satisfy \(i\in\operatorname{pa}_{\mathcal{G}}(j)\Rightarrow i<j\) (otherwise we can choose a different index of the nodes and correspondingly swap some rows and columns of \(\bm{M}\)). Since \(i\in\operatorname{sur}_{\mathcal{G}}(j)\Rightarrow i\in\operatorname{pa}_{ \mathcal{G}}(j)\), it follows that \(\bm{M}\) must be lower triangular and the diagonal entries are nonzero.

Let \(\bm{N}=\bm{M}^{-1}\), then for \(\forall i\in[d]\), we have

\[\sum_{j=1}^{d}\bm{N}_{ij}\bm{M}_{j\ell}=0,\quad\forall\ell\notin\overline{ \operatorname{sur}}_{\mathcal{G}}(i).\] (9)

Since \(\bm{M}\in\mathcal{M}^{0}_{\operatorname{sur}}(\mathcal{G})\), we have \(\bm{M}_{j\ell}=0\) for \(\forall j\) such that \(\ell\notin\overline{\operatorname{sur}}_{\mathcal{G}}(j)\). By Lemma 8, if \(j\in\overline{\operatorname{sur}}_{\mathcal{G}}(i)\), then \(\ell\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i)\) necessarily implies that \(\ell\notin\overline{\operatorname{sur}}_{\mathcal{G}}(j)\). Hence the left hand side of (9) is essentially a sum over \(j\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i)\), _i.e._,

\[\sum_{j\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i)}\bm{N}_{ij}\bm{M}_ {j\ell}=0,\quad\forall\ell\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i).\]

Viewing the above as a system of linear equations in \(\bm{N}_{ij},j\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i)\), the coefficient matrix \(\left(\bm{M}_{j\ell}\right)_{j,\ell\in\notin\overline{\operatorname{sur}}_{ \mathcal{G}}(i)}\) must be invertible since it is a sub-matrix of the invertible lower-triangular matrix \(\bm{M}\). As a result, we necessary have \(\bm{N}_{ij}=0,\forall j\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i)\). Finally, \(\bm{N}=\bm{M}^{-1}\) must be invertible, so \(\bm{N}\in\mathcal{M}^{0}_{\operatorname{sur}}(\mathcal{G})\) as desired. 

**Lemma 10**.: _Suppose that \(\psi:\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) is a diffeomorphism and \(\mathcal{G}\) be a DAG, such that for \(\forall i\in[d]\), \(\psi_{i}(\bm{z})\) is a function of \(\bm{z}_{\overline{\operatorname{sur}}_{\mathcal{G}}(i)}\). Then for \(\forall j\in[d]\), \((\psi^{-1})_{j}(\bm{v})\) is a function of \(\bm{v}_{\overline{\operatorname{sur}}_{\mathcal{G}}(j)}\)._

Proof.: Let \(\bm{J_{z}}=\bm{J}_{\psi}(\bm{z})\) be the Jacobian matrix of \(\psi\). Since \(\psi\) is a diffeomorphism, \(\bm{J_{z}}\) is invertible for any \(\bm{z}\in\mathbb{R}^{d}\). Moreover, our assumption implies that \((\bm{J_{z}})_{ij}=0,\forall j\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i)\), so \(\bm{J_{z}}\in\mathcal{M}^{0}_{\operatorname{sur}}(\mathcal{G})\). By Lemma 9, \(\bm{J_{z}}^{-1}\in\mathcal{M}^{0}_{\operatorname{sur}}(\mathcal{G})\). But \(\bm{J_{z}}^{-1}\) is exactly the Jacobian matrix of \(\psi^{-1}\) at \(\bm{v}=\psi(\bm{z})\), hence it follows that \((\psi^{-1})_{j}(\bm{v})\) is only a function of \(\bm{v}_{\overline{\operatorname{sur}}_{\mathcal{G}}(j)}\), as desired. 

**Lemma 11**.: _The binary relation \(\sim_{\operatorname{sur}}\) defined in Definition 4 is an equivalence relation._

Proof.: It is obvious that \((\bm{h},\mathcal{G})\sim_{\operatorname{sur}}(\bm{h},\mathcal{G})\) holds for any model \((\bm{h},\mathcal{G})\).

Suppose that \((\bm{h}_{1},\mathcal{G}_{1})\sim_{\operatorname{sur}}(\bm{h}_{2},\mathcal{G}_{2})\), then there exists a permutation \(\pi\) on \([d]\) and a diffeomorphism \(\psi:\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) where \(\psi_{i}(\bm{z})\) is a function of \(\bm{z}_{\overline{\operatorname{sur}}_{\mathcal{G}_{1}}(i)}\), such that \(i\in\operatorname{pa}_{\mathcal{G}_{1}}(j)\Leftrightarrow\pi(i)\in\operatorname {pa}_{\mathcal{G}_{2}}(\pi(j))\) and \(\bm{P}_{\pi}\circ\bm{h}_{2}=\psi\circ\bm{h}_{1}\). Then we can write \(\bm{P}_{\pi}^{-1}\circ\bm{h}_{1}=\hat{\psi}\circ\bm{h}_{2}\) where \(\hat{\psi}=\bm{P}_{\pi}^{-1}\circ\psi^{-1}\circ\bm{P}_{\pi}\). By Lemma 10, we know that \(\big{(}\psi^{-1}\big{)}_{j}(\bm{v})\) is a function of \(\bm{v}_{\overline{\operatorname{sur}}_{\mathcal{G}_{1}}(j)}\), so \((\hat{\psi})_{j}\) is a function of \(\bm{v}_{\pi(\overline{\operatorname{sur}}_{\mathcal{G}_{1}}(j))}=\bm{v}_{ \overline{\operatorname{sur}}_{\mathcal{G}_{2}}(j)}\), implying that \((\bm{h}_{2},\mathcal{G}_{2})\sim_{\operatorname{sur}}(\bm{h}_{1},\mathcal{G}_{1})\).

Finally, let \((\bm{h}_{1},\mathcal{G}_{1})\sim_{\mathrm{sur}}(\bm{h}_{2},\mathcal{G}_{2})\) and \((\bm{h}_{2},\mathcal{G}_{2})\sim_{\mathrm{sur}}(\bm{h}_{3},\mathcal{G}_{3})\), then we can write

\[\bm{P}_{\pi}\circ\bm{h}_{2}=\psi\circ\bm{h}_{1}\quad\text{and}\quad\bm{P}_{ \hat{\pi}}\circ\bm{h}_{3}=\hat{\psi}\circ\bm{h}_{2}\]

where: for \(\forall i\in[d]\), \(\psi_{i}(\bm{z})\) is a function of \(\bm{z}_{\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(i)}\), \(\hat{\psi}_{i}(\bm{z})\) is a function of \(\bm{z}_{\overline{\mathrm{sur}}_{\mathcal{G}_{2}}(i)}\), \(i\in\mathrm{pa}_{\mathcal{G}_{1}}(j)\Leftrightarrow\pi(i)\in\mathrm{pa}_{ \mathcal{G}_{2}}(\pi(j))\) and \(i\in\mathrm{pa}_{\mathcal{G}_{2}}(j)\Leftrightarrow\hat{\pi}(i)\in\mathrm{pa} _{\mathcal{G}_{2}}(\hat{\pi}(j))\). Then, we can write

\[\bm{P}_{\pi}\circ\bm{P}_{\hat{\pi}}\circ\bm{h}_{3}=\bm{P}_{\pi}\circ\hat{\psi} \circ\bm{P}_{\pi}^{-1}\circ\psi\circ\bm{h}_{1}.\]

Since \(\hat{\psi}_{i}(\bm{z})\) is a function of \(\bm{z}_{\overline{\mathrm{sur}}_{\mathcal{G}_{2}}(i)}\), we deduce that \(\left(\bm{P}_{\pi}\circ\hat{\psi}\circ\bm{P}_{\pi}^{-1}\right)_{i}(\bm{z})\) is a function of \(\bm{z}_{\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(i)}\). Hence, \(\left(\bm{P}_{\pi}\circ\hat{\psi}\circ\bm{P}_{\pi}^{-1}\circ\psi\right)_{i}( \bm{z})=\left(\bm{P}_{\pi}\circ\hat{\psi}\circ\bm{P}_{\pi}^{-1}\right)_{i}( \psi(\bm{z}))\) is a function of \(\psi_{\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(i)}(\bm{z})\). The definition of \(\psi\) implies that for each \(j\in\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(i)\), \(\psi_{j}(\bm{z})\) is a function of \(\bm{z}_{\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(j)}\). By Lemma 8, we have \(\cup_{j\in\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(i)}\overline{\mathrm{sur}} _{\mathcal{G}_{1}}(j)\subseteq\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(i)\). Hence \(\left(\bm{P}_{\pi}\circ\hat{\psi}\circ\bm{P}_{\pi}^{-1}\circ\psi\right)_{i}( \bm{z})\) is still a function of \(\bm{z}_{\overline{\mathrm{sur}}_{\mathcal{G}_{1}}(i)}\). Moreover, we also have \(i\in\mathrm{pa}_{\mathcal{G}_{1}}(j)\Leftrightarrow\pi(i)\in\mathrm{pa}_{ \mathcal{G}_{2}}(\pi(j))\Leftrightarrow\hat{\pi}\circ\pi(i)\in\mathrm{pa}_{ \mathcal{G}_{2}}(\hat{\pi}\circ\pi(j))\), so by definition, \((\bm{h}_{1},\mathcal{G}_{1})\sim_{\mathrm{sur}}(\bm{h}_{3},\mathcal{G}_{3})\), as desired. 

## Appendix H Omitted proofs from Section 4 and Section 5

### Proof of Theorem 1

According to the assumption, we have that \(\epsilon=\bm{B}_{k}\bm{H}\bm{x}\) and \(\hat{\epsilon}=\hat{\bm{B}}_{k}\hat{\bm{H}}\bm{x}\), so that \(\epsilon=\bm{B}_{k}\bm{H}(\hat{\bm{B}}_{k}\hat{\bm{H}})^{\dagger}\hat{\epsilon},\forall k\in[K]\). By Lemma 3, we know that for each \(k\), \(\bm{P}_{k}:=\bm{B}_{k}\bm{H}(\hat{\bm{B}}_{k}\hat{\bm{H}})^{\dagger}\) is a signed permutation matrix, so that \(\epsilon=\bm{P}_{k}\hat{\epsilon}\). Since for any \(i\neq j\), \(\hat{\epsilon}_{i}\neq\hat{\epsilon}_{j}\), we must have \(\left|\bm{P}\right|_{1}=\left|\bm{P}\right|_{2}=\cdots=\left|\bm{P}\right|_{K} =:\bm{P}\) and \(\epsilon=\bm{P}\hat{\epsilon}\), where \(|M|\) denotes the resulting matrix by taking the absolute value of all entries in \(\bm{M}\). Thus, we can WLOG assume that \(\epsilon=\hat{\epsilon}\), since otherwise we can permute the noise variables \(\hat{\epsilon}\), and also permute the rows of \(\bm{B}_{k}\) correspondingly. In other words, suppose that the permutation matrix \(\left|\bm{P}\right|\) has \(\left|\bm{P}\right|_{k_{i},i}=1,i\in[d]\), then we can assign to each node \(i\) in \(\hat{\mathcal{G}}\) a new index \(k_{i}\) and work with the new indices.

In this case, by Lemma 3 we have \(\bm{B}_{k}\bm{H}=\bm{\Sigma}_{k}\hat{\bm{B}}_{k}\hat{\bm{H}},\forall k\in[K]\) or equivalently \(\bm{\Sigma}_{k}\hat{\bm{B}}_{k}=\bm{B}_{k}\bm{T}\), where \(\bm{T}=\bm{H}\hat{\bm{H}}^{\dagger}\in\mathbb{R}^{d\times d}\), and \(\bm{\Sigma}_{k}\) is a diagonal matrix with diagonal entries in \(\{+1,-1\}\). Let \(\hat{\bm{B}}_{k}=\bm{\Sigma}_{k}\hat{\bm{B}}_{k}\), then the rows of \(\hat{\bm{B}}_{k}\) equals (up to sign) to the rows of \(\hat{\bm{B}}_{k}\).

To summarize, we now know that i) \(\hat{\bm{B}}_{k}=\bm{B}_{k}\bm{T},k\in[K]\), ii) \((\bm{B}_{k})_{ij}\neq 0\Leftrightarrow j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\), and similarly, \((\hat{\bm{B}}_{k})_{ij}\neq 0\Leftrightarrow j\in\overline{\mathrm{pa}}_{ \mathcal{G}}(i)\), and iii) Both \(\{\bm{B}_{k}\}\) and \(\left\{\hat{\bm{B}}_{k}\right\}\) satisfy the node-level non-degeneracy assumption 5. For any two such matrices that satisfy such a set of conditions, it must necessarily be true that \(\mathcal{G}=\hat{\mathcal{G}}\).

**Lemma 12** (Graph Identifiability).: _Consider any two sets matrices \(\{\hat{\bm{B}}_{k}\}_{k\in[K]}\) and \(\{\bm{B}_{k}\}_{k\in[K]}\) and associated graphs \(\mathcal{G},\hat{\mathcal{G}}\). If these sets and graphs satisfy that:_

1. \(\hat{\bm{B}}_{k}=\bm{B}_{k}\bm{T},k\in[K]\)_;_
2. \((\bm{B}_{k})_{ij}\neq 0\Leftrightarrow j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\)_, and similarly,_ \((\hat{\bm{B}}_{k})_{ij}\neq 0\Leftrightarrow j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\)_._
3. _Both_ \(\{\bm{B}_{k}\}\) _and_ \(\left\{\hat{\bm{B}}_{k}\right\}\) _satisfy the node-level non-degeneracy assumption_ _Assumption_ _5._

_then it must hold that \(\mathcal{G}=\hat{\mathcal{G}}\)._

Proof.: We prove this via induction on the size of the graph \(d\). Note that here \(\mathcal{G}=\hat{\mathcal{G}}\) is not up to permutation and our statement is equivalent to \(\mathrm{pa}_{\mathcal{G}}(i)=\mathrm{pa}_{\hat{\mathcal{G}}}(i),\forall i\in[d]\).

If \(d=1\), _i.e._, \(\mathcal{G}=\hat{\mathcal{G}}\) obviously holds since both are graphs with only \(1\) node.

Suppose that for all graphs \(\mathcal{G}\) of size \(d-1\), the graph \(\hat{\mathcal{G}}\) satisfying all given assumptions must necessarily be equal to \(\mathcal{G}\). Now, we consider the case that \(\mathcal{G}\) has \(d\) nodes. WLOG we can assume that the nodes of \(\mathcal{G}\) are properly indexed such that \(i\in\mathrm{pa}_{\mathcal{G}}(j)\Rightarrow i<j\), so \(\bm{B}_{k},k\in[K]\) are lower-triangular matrices. (However, it is currently unknown whether \(\hat{\bm{B}}_{k}\) are also lower-triangular.)

By our assumption that \(i\in\mathrm{pa}_{\mathcal{G}}(j)\Rightarrow i<j\), the node \(d\) in \(\mathcal{G}\) has no child. Thus we can write

\[\bm{B}_{k}=\left(\begin{array}{cc}\bm{B}_{k}^{-}&\bm{0}\\ \bm{b}_{k}&c_{k}\end{array}\right),\bm{T}=\left(\begin{array}{cc}\bm{T}^{-}& \times\\ \times&\times\end{array}\right)\text{ and }\hat{\bm{B}}_{k}=\bm{B}_{k}\bm{T}= \left(\begin{array}{cc}\hat{\bm{B}}_{k}^{-}&\times\\ \times&\times\end{array}\right)\]

where \(\bm{B}_{k}^{-},\bm{T},\hat{\bm{B}}_{k}^{-}=\bm{B}_{k}^{-}\bm{T}^{-}\in\mathbb{ R}^{(d-1)\times(d-1)},\bm{b}_{k}\in\mathbb{R}^{d-1},c_{k}\in\mathbb{R}\) and \(\times\) denotes irrelevant entries.

Let \(\bm{A}_{k}^{-},\hat{\bm{A}}_{k}^{-},\bm{\Omega}_{k}^{-}\) and \(\hat{\bm{\Omega}}_{k}^{-}\) be the top-left \((d-1)\times(d-1)\) sub-matrices of \(\bm{A}_{k},\hat{\bm{A}},\bm{\Omega}_{k}\) and \(\hat{\bm{\Omega}}_{k}\) respectively, and \(\mathcal{G}^{-}\) and \(\hat{\mathcal{G}}^{-}\) are graphs obtained by deleting node \(d\) and all related edges from \(\mathcal{G}\) and \(\hat{\mathcal{G}}\). Then it is easy to see that

\[\left(\bm{A}_{k}^{-}\right)_{ij}\neq 0\Leftrightarrow j\in\mathrm{pa}_{ \mathcal{G}^{-}}(i)\quad\text{and}\quad\left(\hat{\bm{A}}_{k}^{-}\right)_{ij} \neq 0\Leftrightarrow j\in\mathrm{pa}_{\hat{\mathcal{G}}^{-}}(i).\] (10)

Moreover,

\[\left(\begin{array}{cc}\bm{B}_{k}^{-}&\bm{0}\\ \bm{b}_{k}&c_{k}\end{array}\right)=\bm{B}_{k}=\bm{\Omega}_{k}^{-\frac{1}{2}} \left(\bm{I}-\bm{A}_{k}\right)=\left(\begin{array}{cc}\left(\bm{\Omega}_{k} ^{-}\right)^{-\frac{1}{2}}&\bm{0}\\ \bm{0}&\times\end{array}\right)\left(\begin{array}{cc}\bm{I}-\bm{A}_{k}^{-} &\times\\ \times&\times\end{array}\right)=\left(\begin{array}{cc}\left(\bm{\Omega}_{k} ^{-}\right)^{-\frac{1}{2}}\left(\bm{I}-\bm{A}_{k}^{-}\right)&\times\\ \times&\times\end{array}\right)\]

so that \(\bm{B}_{k}^{-}=\left(\bm{\Omega}_{k}^{-}\right)^{-\frac{1}{2}}\left(\bm{I}- \bm{A}_{k}^{-}\right)\). Similarly, we have \(\hat{\bm{B}}_{k}^{-}=\left(\hat{\bm{\Omega}}_{k}^{-}\right)^{-\frac{1}{2}} \left(\bm{I}-\hat{\bm{A}}_{k}^{-}\right)\).

We can also verify that \(\left\{\bm{B}_{k}^{-}\right\}_{k=1}^{K}\) and \(\left\{\hat{\bm{B}}_{k}^{-}\right\}_{k=1}^{K}\) are node-level independent in the sense of Assumption 5. We only prove this for \(\left\{\hat{\bm{B}}_{k}\right\}_{k=1}^{K}\); the arguments used for \(\left\{\bm{B}_{k}\right\}_{k=1}^{K}\) are exactly the same as the first case considered below. Now for each \(i\in[d-1]\), let \(\bm{R}_{i}\in\mathbb{R}^{K\times d}\) be the matrix whose \(k\)-th row is the \(i\)-th row of \(\hat{\bm{B}}_{k}\), and \(\bm{R}_{i}^{-}\in\mathbb{R}^{K\times(d-1)}\) be the matrix whose \(k\)-th row is the \(i\)-th row of \(\hat{\bm{B}}_{k}^{-}\), then obviously \(\bm{R}_{i}\) is of form \(\left[\bm{R}_{i}^{-},\bm{r}_{i}\right]\). We consider two cases:

* **Case 1.**\(d\notin\mathrm{pa}_{\mathcal{G}}(i)\) This means that the last entry of the \(i\)-th row of \(\hat{\bm{B}}_{k}\) is zero. Thus \(\bm{r}_{i}=\bm{0}\), and \(\mathrm{rank}\left(\bm{R}_{i}^{-}\right)=\mathrm{rank}\left(\bm{R}_{i}\right) =\left|\overline{\mathrm{pa}}_{\mathcal{G}}(i)\right|=\left|\overline{\mathrm{ pa}}_{\mathcal{G}^{-}}(i)\right|\), where the second equality follows from Assumption 5.
* **Case 2.**\(d\in\mathrm{pa}_{\mathcal{G}}(i)\) In this case we have \(\mathrm{rank}\left(\bm{R}_{i}^{-}\right)\geqslant\mathrm{rank}\left(\bm{R}_{i} \right)-1=\left|\overline{\mathrm{pa}}_{\mathcal{G}}(i)\right|-1=\left|\overline {\mathrm{pa}}_{\mathcal{G}^{-}}(i)\right|\). Due to our assumption on \(\hat{\bm{A}}_{k}\) and the relationship \(\hat{\bm{B}}_{k}^{-}=\left(\hat{\bm{\Omega}}_{k}^{-}\right)^{-\frac{1}{2}} \left(\bm{I}-\hat{\bm{A}}_{k}^{-}\right)\), we know that each row of \(\bm{R}_{i}^{-}\), namely the \(i\)-th row of some \(\hat{\bm{B}}_{k}\) only has \(\left|\overline{\mathrm{pa}}_{\mathcal{G}}(i)\right|-1=\left|\overline{\mathrm{ pa}}_{\mathcal{G}^{-}}(i)\right|\) non-zero entries, so that \(\mathrm{rank}\left(\bm{R}_{i}^{-}\right)=\left|\overline{\mathrm{pa}}_{\mathcal{G} ^{-}}(i)\right|\) holds.

Since we have shown that the matrices \(\bm{B}_{k}^{-}\) and \(\hat{\bm{B}}_{k}^{-}\) satisfy the three properties that we assume for induction with \(\bm{T}\) replaced by \(\bm{T}^{-}\) and \(\mathcal{G},\hat{\mathcal{G}}\) replaced by \(\mathcal{G}^{-},\hat{\mathcal{G}}^{-}\) respectively, by induction hypothesis, we can thus deduce that \(\mathcal{G}^{-}=\hat{\mathcal{G}}^{-}\). To prove \(\mathcal{G}=\hat{\mathcal{G}}\) it remains to show that the dependency of node \(d\) on the remaining nodes are the same in \(\mathcal{G}\) and \(\hat{\mathcal{G}}\).

_First_, we show that \(\mathrm{ch}_{\hat{\mathcal{G}}}(d)=\emptyset\). Suppose in contrary that there is some \(i\in\mathrm{ch}_{\hat{\mathcal{G}}}(d)\), then \(\left|\mathrm{pa}_{\mathcal{G}}(i)\right|=\left|\mathrm{pa}_{\mathcal{G}^{-}}(i) \right|=\left|\mathrm{pa}_{\hat{\mathcal{G}}^{-}}(i)\right|=\left|\mathrm{ pa}_{\mathcal{G}}(i)\right|-1\). Recalling that \(\left(\bm{B}\right)_{i}\) denotes the \(i\)-th row of matrix \(\bm{B}\), we have

\[\dim\left(\mathrm{span}\left\langle\left(\hat{\bm{B}}_{k}\right)_{i}:1 \leqslant k\leqslant K\right\rangle\right) =\dim\left(\mathrm{span}\left\langle\left(\bm{B}_{k}\right)_{i}:1 \leqslant k\leqslant K\right\rangle\right)\] (11) \[\leqslant\left|\mathrm{pa}_{\mathcal{G}}(i)\right|+1<\left| \mathrm{pa}_{\mathcal{G}}(i)\right|+1,\]

where the first inequality follows from \(\left(\hat{\bm{B}}_{k}\right)_{i}=\left(\bm{B}_{k}\right)_{i}\bm{T}\) and Lemma 2, the second holds since each \(\left(\bm{B}_{k}\right)_{i}\) has nonzero elements only at coordinates in \(j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\), and the last one holds since \(\left|\mathrm{pa}_{\mathcal{G}}(i)\right|=\left|\mathrm{pa}_{\hat{\mathcal{G}}}(i) \right|-1\). However, (11) contradicts the non-degeneracy condition Assumption 5 that we assume for matrices \(\hat{\bm{B}}_{k},k\in[K]\) in the statement of the theorem. Therefore we have \(\mathrm{ch}_{\hat{\mathcal{G}}}(d)=\emptyset=\mathrm{ch}_{\mathcal{G}}(d)\).

_Second_, by a similar argument comparing the number of nonzero elements in the last row of \(\bm{B}_{k}\) and \(\hat{\bm{B}}_{k}\), we can also deduce that

\[\left|\mathrm{pa}_{\mathcal{G}}(d)\right|=\left|\mathrm{pa}_{\hat{\mathcal{G}} }(d)\right|.\]

Indeed, since \(\left(\hat{\bm{B}}_{k}\right)_{d}=\left(\bm{B}_{k}\right)_{d}\bm{T}\), by Lemma 2 we have

\[\dim\left(\mathrm{span}\left\langle\left(\hat{\bm{B}}_{k}\right)_{d}:1\leqslant k \leqslant K\right\rangle\right)=\dim\left(\mathrm{span}\left\langle\left(\bm{B }_{k}\right)_{d}:1\leqslant k\leqslant K\right\rangle\right)\]

However, since we assume that Assumption 5 is satisfied for \(\left\{\bm{B}_{k}\right\}_{k=1}^{K}\) and \(\left\{\hat{\bm{B}}_{k}\right\}_{k=1}^{K}\), we know that the LHS and RHS of the above equation are equal to \(\left|\mathrm{pa}_{\mathcal{G}}(d)\right|+1\) and \(\left|\mathrm{pa}_{\mathcal{G}}(d)\right|+1\) respectively, implying (12).

_Third_, we show that \(\mathrm{pa}_{\mathcal{G}}(d)=\mathrm{pa}_{\hat{\mathcal{G}}}(d)\). Suppose the contrary, let \(\ell\) be the smallest element in \(\mathrm{pa}_{\mathcal{G}}(d)\Delta\mathrm{pa}_{\hat{\mathcal{G}}}(d)\), where \(A\Delta B:=(A\setminus B)\cup(B\setminus A)\). Recall that while \(\mathcal{G}\) and \(\hat{\mathcal{G}}\) are originally not symmetric as nodes are topologically sorted according to \(\mathcal{G}\), now we have shown that \(\mathcal{G}^{-}\equiv\hat{\mathcal{G}}^{-}\) and that \(\mathrm{ch}_{\mathcal{G}}(d)=\mathrm{ch}_{\hat{\mathcal{G}}}(d)=\emptyset\), so we can assume WLOG that \(\ell\in\mathrm{pa}_{\mathcal{G}}(d)\) and \(\ell\notin\mathrm{pa}_{\mathcal{G}}(d)\), and the other case can be handled symmetrically. Since \(\bm{B}_{k}\) is lower triangular and \(\left(\bm{B}_{k}\right)_{jj}=\left(\Omega_{k}\right)_{jj}^{-\frac{1}{2}}\neq 0, \forall j\in[d]\), the top-left \(\ell\times\ell\) sub-matrix of \(\bm{B}_{k}\), which we denote by \(\left[\bm{B}_{k}\right]_{\ell,\ell}\), must be invertible. This implies that \(\left\{\left[\bm{B}_{k}\right]_{\ell,\ell}^{\top}\bm{\lambda}:\bm{\lambda} \in\mathbb{R}^{\ell}\right\}=\mathbb{R}^{\ell}\), so we can always find coefficients \(\lambda_{kj},j\in[\ell]\) such that the first \(\ell\) entries of the vector \((\bm{B}_{k})_{d}-\sum_{i=1}^{\ell}\lambda_{ki}(\bm{B}_{k})_{i}\in\mathbb{R}^{d}\) are all zero. Since \(\hat{\bm{B}}_{k}=\bm{B}_{k}\bm{T}\) and \(\bm{T}\) is invertible, we have \(\left(\hat{\bm{B}}_{k}\right)_{d}-\sum_{j=1}^{\ell}\lambda_{kj}\left(\hat{\bm{ B}}_{k}\right)_{j}=\left(\left(\bm{B}_{k}\right)_{d}-\sum_{j=1}^{\ell}\lambda_{kj} \left(\bm{B}_{k}\right)_{j}\right)\bm{T},\forall k\in[K]\) and

\[\dim\left(\mathrm{span}\left\langle\left(\hat{\bm{B}}_{k}\right) _{d}-\sum_{j=1}^{\ell}\lambda_{kj}\left(\hat{\bm{B}}_{k}\right)_{j}:k\in[K] \right\rangle\right) =\dim\left(\mathrm{span}\left\langle\left(\bm{B}_{k}\right)_{d} -\sum_{j=1}^{\ell}\lambda_{kj}\left(\bm{B}_{k}\right)_{j}:k\in[K]\right\rangle\right)\] \[\leqslant\left|\mathrm{pa}_{\mathcal{G}}(d)\setminus[\ell]\right|+1.\]

Here, the inequality holds because for any coordinate \(t\in[d]\),

\[\left(\left(\bm{B}_{k}\right)_{d}-\sum_{j=1}^{\ell}\lambda_{kj}\left(\bm{B}_{k }\right)_{j}\right)_{t}=\left\{\begin{aligned} & 0&\text{if }t\leqslant\ell\\ &\left(\bm{B}_{k}\right)_{d,t}&\text{otherwise}\end{aligned}\] (12)

where we note that \(\bm{B}_{k}\) is lower-triangular and thus \(\left(\bm{B}_{k}\right)_{j,t}=0,\forall j\leqslant\ell,t>\ell\). This implies that \(\left(\left(\bm{B}_{k}\right)_{d}-\sum_{j=1}^{\ell}\lambda_{kj}\left(\bm{B}_{ k}\right)_{j}\right)_{t}\) is nonzero only if \(t>\ell\) and \(t\in\mathrm{pa}_{\mathcal{G}}(d)\).

On the other hand, let \(S=\left(\mathrm{pa}_{\mathcal{G}}(d)\cap[\ell]^{c}\right)\cup\{d\}\), then

\[\dim\left(\mathrm{span}\left\langle\left(\hat{\bm{B}}_{k}\right) _{d}-\sum_{j=1}^{\ell}\lambda_{kj}\left(\hat{\bm{B}}_{k}\right)_{j}:k\in[K] \right\rangle\right) \geqslant\dim\left(\mathrm{span}\left\langle\left(\left(\hat{\bm{ B}}_{k}\right)_{d}-\sum_{j=1}^{\ell}\lambda_{kj}\left(\hat{\bm{B}}_{k} \right)_{j}\right)_{S}:k\in[K]\right\rangle\right)\] \[=\dim\left(\mathrm{span}\left\langle\left(\left(\hat{\bm{B}}_{k} \right)_{d}\right)_{S}:k\in[K]\right\rangle\right)=|S|.\]

where we recall that \(\bm{u}_{S}\) denotes the vector \((u_{i}:i\in S)\in\mathbb{R}^{|S|}\). Here the first equality holds due to the same reason as (12), and the second follows from Assumption 5. To see why this is the case, note that Assumption 5 implies that the \(K\times\left(\left|\mathrm{pa}_{\mathcal{G}}(d)\right|+1\right)\) having \(\left(\left(\bm{B}_{k}\right)_{d}\right)_{\overline{\mathrm{pa}}_{\mathcal{G}}(d)}\) as the \(k\)-th row has full column rank, so that the sub-matrix obtained by extracting columns corresponding to the node set \(S\) also has full column rank.

We have shown that \(\left|\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(d)\cap\lceil\ell\rceil^{c}\right| =|S|\leqslant\left|\mathrm{pa}_{\hat{\mathcal{G}}}(d)\cap\lceil\ell\rceil^{c} \right|+1=\left|\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(d)\cap\lceil\ell \rceil^{c}\right|\). On the other hand, recall that by our choice of \(\ell\), we have \(\left|\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(d)\cap\lceil\ell-1\rceil \right|=\left|\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(d)\cap\lceil\ell-1 \rceil\right|\) and \(\ell\in\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(d)\setminus\overline{ \mathrm{pa}}_{\hat{\mathcal{G}}}(d)\). Putting these together, we have \(\left|\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(d)\right|>\left|\overline{ \mathrm{pa}}_{\hat{\mathcal{G}}}(d)\right|\). However, we know from (12) that \(\left|\mathrm{pa}_{\hat{\mathcal{G}}}(d)\right|=\left|\mathrm{pa}_{\hat{ \mathcal{G}}}(d)\right|\), leading to a contradiction. Hence, such \(\ell\) shouldn't exist and we must have \(\mathrm{pa}_{\hat{\mathcal{G}}}(d)=\mathrm{pa}_{\hat{\mathcal{G}}}(d)\), completing the induction step for graphs of size \(d\).

By the principle of induction, we have shown that \(\mathcal{G}=\hat{\mathcal{G}}\) holds for any graphs under given assumptions. 

Now that we have established that \(\mathcal{G}=\hat{\mathcal{G}}\), we prove the remaining part of the theorem. Note that for any \(i,j\in[d]\) such that \(i\notin\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(j)\), we have \((\bm{B}_{k})_{ji}=(\hat{\bm{B}}_{k})_{ji}=0,\forall k\in[K]\). Since \(\hat{\bm{B}}_{k}=\bm{B}_{k}\bm{T}\), we have

\[\sum_{\ell\in\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(j)}(\bm{B}_{k})_{j\ell }\bm{T}_{\ell i}=0.\]

By Assumption 5, the above implies that \(\bm{T}_{\ell i}=0\) for \(\forall\ell\in\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(j)\). In short, we have argued that if there exists \(j\) such that \(i\notin\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(j)\) and \(\ell\in\overline{\mathrm{pa}}_{\hat{\mathcal{G}}}(j)\), then \(\bm{T}_{\ell i}=0\).

This implies that \(T_{\ell i}\) is non-zero only if \(\mathrm{ch}_{\hat{\mathcal{G}}}(\ell)\subseteq\mathrm{ch}_{\hat{\mathcal{G}}}(i)\). Since \(\bm{v}=\bm{T}\bm{z}\), we have \(\bm{v}_{\ell}=\sum_{i=1}^{d}\bm{T}_{\ell i}\bm{z}_{i}=\sum_{i\in[d]:\mathrm{ch} _{\hat{\mathcal{G}}}(\ell)\subseteq\mathrm{ch}_{\hat{\mathcal{G}}}(i)}\bm{T}_ {\ell i}\bm{z}_{i}\). Note that when \(i\neq\ell\), \(\mathrm{ch}_{\hat{\mathcal{G}}}(\ell)\subseteq\mathrm{ch}_{\hat{\mathcal{G}}}(i)\) is equivalent to \(i\in\mathrm{sur}_{\mathcal{G}}(\ell)\), so \(\bm{v}_{\ell}\) only depends on \(\bm{z}_{\overline{\mathrm{sur}}_{\hat{\mathcal{G}}}(\ell)}\) by Lemma 6, as desired.

### Formal version and proof of Theorem 2

In previous works [36, 54], it is common to consider single-node soft interventions in the following sense:

**Assumption 8**.: _For \(\forall 2\leqslant k\leqslant K\), there exists \(i_{k}\in[d]\), such that the structural equation in environment \(k\) satisfies (4) satisfies \(\bm{w}_{k}(i)=\bm{w}_{1}(i)\) and \(\omega_{k,i,i}=\omega_{1,i,i}\) for \(\forall i\neq i_{k}\)._

Let \(S_{i}=\left\{k:2\leqslant k\leqslant K,i_{k}=i\right\},i\in[d]\) and \(s_{i}=|S_{i}|\). Suppose that \(\mathcal{G}\) has \(e=\sum_{i=1}^{d}\left|\mathrm{pa}_{\hat{\mathcal{G}}}(i)\right|\) edges, then we can view the weight vectors \(\left\{\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.

**Lemma 13**.: _Suppose that \(m\in\mathbb{Z}_{+}\) and \(\bm{V}\) is a subspace of \(\mathbb{R}^{m}\). Then for any set of vectors \(\bm{u}_{i}\in\mathbb{R}^{m},i=1,2,\cdots,n\) that does not lie in \(\bm{V}\), there must exists \(\bm{v}\in\mathbb{R}^{m}\) such that \(\bm{u}_{i}^{\top}\bm{v}\neq 0,\forall i\in[n]\) but \(\bm{v}\in\bm{V}^{\perp}\), where \(\bm{V}^{\perp}\) is the orthogonal space of \(\bm{V}\)._

Proof.: Let \(\bm{w}_{i}\) be the orthogonal projection of \(\bm{u}_{i}\) onto \(\bm{V}^{\perp}\). Since \(\bm{u}_{i}\notin\bm{V}\), we know that \(\bm{w}_{i}\neq\bm{0}\). The solution space of each equation \(\bm{w}_{i}^{\top}\bm{v}=0\) in \(\bm{V}^{\perp}\) must then be a proper subspace of \(\bm{V}^{\perp}\). Equipped with the Lebesgue measure, all these spaces are null sets in \(\bm{V}^{\perp}\), so one can always choose a \(\bm{v}\in\bm{V}^{\perp}\) that does not lie in any of these solution spaces. Such \(\bm{v}\) satisfies all the requirements. 

We choose \(\mathcal{G}\) to be the graph with \(i\to j\) for \(\forall 1\leqslant i<j\leqslant d\), so that \(\mathcal{G}\) has \(\frac{d(d-1)}{2}\) edges. Suppose that \(i_{0}\in[d]\) satisfies \(s_{i}\leqslant\left|\mathrm{pa}_{\mathcal{G}}(i)\right|-1\), then we must have \(i_{0}\geqslant 2\), so there is an edge \(1\to i_{0}\) in \(\mathcal{G}\), Let \(\hat{\mathcal{G}}\) be the resulting graph obtained via removing the edge \(1\to i_{0}\) in \(\mathcal{G}\), then \(\mathcal{G}\) and \(\hat{\mathcal{G}}\) are clearly non-isomorphic.

Note that the \(i\)-th row of \(\bm{B}_{k}\) can be written as \(\omega_{k,i,i}^{-\frac{1}{2}}\left(\bm{e}_{i}-(\bm{A}_{k})_{i}\right)\). Let's choose an lower-triangular matrix \(\bm{T}=(t_{ij})_{i,j=1}^{d}\in\mathbb{R}^{d\times d}\) with columns \(\bm{t}_{i},i\in[d]\) such that the following holds:

\[(\bm{e}_{i}-(\bm{A}_{k})_{i})^{\top}\bm{t}_{j}=\begin{cases}=0,&\forall k\in \{1\}\cup S_{i_{0}},j=1\text{ and }i=i_{0}\\ >0,&\forall i=j\text{ and }k\in\{1\}\cup S_{i}\\ \neq 0,&\forall\text{ remaining }(i,j,k)\in\{k=1,j<i\}\cup\{k\geqslant 2,i=i_{k},j<i\} \end{cases}\] (13)

and

\[t_{ii}\neq 0,\quad\forall i\in[d].\] (14)

We now show that: except from a null set in the weight space, such \(\bm{T}\) can always be chosen. To see why this is the case, we first consider all the constraints on \(\bm{t}_{1}\):

\[(\bm{e}_{i}-(\bm{A}_{k})_{i})^{\top}\bm{t}_{1}=\begin{cases}=0,&\forall k\in \{1\}\cup S_{i_{0}}\text{ and }i=i_{0}\\ >0,&\forall i=1\text{ and }k\in\{1\}\cup S_{i}\\ \neq 0,&\forall\text{ remaining }(i,k)\in\{k=1,i>1\}\cup\{k\geqslant 2,i=i_{k}>1\} \end{cases}\] (15)

Now let \(\bm{V}=\mathrm{span}\left\langle\bm{e}_{i}-(\bm{A}_{k})_{i}:k\in\{1\}\cup S_{ i_{0}}\text{ and }i=i_{0}\right\rangle\) and \(R\) be the set of pairs \((i,k)\) specified in the second and third row of (15). For \(\forall(i,k)\), let \(\bm{w}_{k}(i)\) be the weight vector of node \(i\) in the environment \(k\), _i.e._, the vector of nonzero entries in \((\bm{A}_{k})_{i}\). Then for \(\forall(i,k)\in R\), the following set (as a subset of the weight space)

\[\bigcup_{k^{*}\in\{1\}\cup S_{i_{0}}}\left\{\bm{e}_{i_{0}}-\bm{w}_{k^{*}}(i_{0 })\in\mathrm{span}\left\langle\bm{e}_{i}-\bm{w}_{k}(i),\bm{e}_{i_{0}}-\bm{w}_{k^ {\prime}}(i_{0}):k^{\prime}\in\{1\}\cup S_{i_{0}}\setminus\{k^{*}\})\right\}\] (16)

must be a null set. Thus

\[\bm{E}=\bigcup_{(i,k)\in R}\bigcup_{k^{*}\in\{1\}\cup S_{i_{0}}}\left\{\bm{e}_ {i_{0}}-\bm{w}_{k^{*}}(i_{0})\in\mathrm{span}\left\langle\bm{e}_{i}-\bm{w}_{k^{* }}(i),\bm{e}_{i_{0}}-\bm{w}_{k^{\prime}}(i_{0}):k^{\prime}\in\{1\}\cup S_{i_{0 }}\setminus\{k^{*}\}\right\rangle\right\}\] (17)

is also a null set. For any weights that are not in \(\bm{E}\), we necessarily have

\[\bm{e}_{i}-\bm{w}_{k}(i)\notin\mathrm{span}\left\langle\bm{e}_{i}-(\bm{A}_{k})_ {i}:k\in\{1\}\cup S_{i_{0}}\text{ and }i=i_{0}\right\rangle=\bm{V},\quad(i,k)\in R.\]

Let \(U=\{\bm{e}_{i}-\bm{w}_{k}(i):(i,k)\in R\}\), then we can apply Lemma 13 to deduce that there exists \(\bm{t}_{1}\) such that

\[(\bm{e}_{i}-(\bm{A}_{k})_{i})^{\top}\bm{t}_{1}=\begin{cases}=0,&\forall k\in \{1\}\cup S_{i_{0}}\text{ and }i=i_{0}\\ \neq 0,&\forall\text{ remaining }(i,k)\in\{k=1\}\cup\{k\geqslant 2,i=i_{k}\} \end{cases}\] (18)

Note that the only difference between (18) and (15) is that the latter one further requires that

\[(\bm{e}_{1}-(\bm{A}_{k})_{1})^{\top}\bm{t}_{1}>0,\quad\forall k\in\{1\}\cup S_ {i}.\]

while the former only guarantees that these terms are nonzero. However, recall that \((\bm{A}_{k})_{ij}\neq 0\Rightarrow j\in\mathrm{pa}_{\mathcal{G}}(i)\Rightarrow j<i\), so the above essentially says that \(t_{11}>0\). This can be easily guaranteed by replacing the solution \(\bm{t}_{1}\) we obtained satisfying (18) with \(-\bm{t}_{1}\) if needed.

Assuming that the weights do not lie in the null set \(\bm{E}\) we have shown that \(\bm{t}_{1}\) can always be chosen to satisfy all constraints imposed on it. We now proceed to choose the remaining entries of \(\bm{T}\). The remaining entries in \(\bm{t}_{1}\) can be chosen arbitrarily. For \(\bm{t}_{j},j>1\), we note that the remaining constraints in (13) that need to be satisfied consist of the "nonzero" part and the "positivity" part. The positivity constrains can always be satisfied by choosing a sufficiently large \(t_{jj}\) for \(j>1\).

After choosing the \(\bm{t}_{j}\)'s satisfying the positivity constraints, the nonzero constraints along with (14) are easy to fulfill by slightly perturbing \(\bm{t}_{j}\) if they are violated; since each of these constraints are only violated in a zero-measure set of the weight space. Hence, we have shown that except a null set \(\bm{E}\) in the weight space, there always exists some \(\bm{T}\) satisfying (13). Such \(\bm{T}\) must be invertible since it is lower-triangular and its diagonal entries are nonzero. Now let \(\hat{\bm{H}}=\bm{T}^{-1}\bm{H}\) and \(\hat{\bm{\Omega}}_{k}\) be the diagonal matrix with entries \(\hat{\omega}_{k,i,i}=t_{ii}^{-2}\cdot\omega_{k,i,i},i\in[d]\) and

\[\hat{\bm{A}}_{k}=\bm{I}-\hat{\bm{\Omega}}_{k}^{\frac{1}{2}}\bm{\Omega}_{k}^{- \frac{1}{2}}(\bm{I}-\bm{A}_{k})\bm{T}.\] (19)

First since \(\bm{T}\) is invertible and \(\bm{H}\) has full rank, \(\hat{\bm{H}}\) must also have full row rank. Second,

\[(\hat{\bm{A}}_{k})_{ij}=\begin{cases}\quad\quad\quad\quad\quad 1-\hat{\omega}_{k,i, \omega}^{\frac{1}{2}}\omega_{k,i,i}^{-\frac{1}{2}}t_{ii}=0&\text{if $j=i$}\\ -\hat{\omega}_{k,i,i}^{\frac{1}{2}}\omega_{k,i,i}^{-\frac{1}{2}}\left(\bm{e}_ {i}-\left(\bm{A}_{k}\right)_{i}\right)^{\top}\bm{t}_{j}=0&\text{if $j>i$}\\ -\hat{\omega}_{k,i,i}^{\frac{1}{2}}\omega_{k,i,i}^{-\frac{1}{2}}\left(\bm{e}_ {i}-\left(\bm{A}_{k}\right)_{i}\right)^{\top}\bm{t}_{j}&\text{if $j<i$.}\end{cases}\]

where we again recall that both \(\bm{A}_{k}\) and \(\bm{T}\) are lower-triangular. From (13) we can see that

* When \(i=i_{0}\) and \(j=1\), we have
* \((\hat{\bm{A}}_{k})_{i_{0},1}=0\) if \(k\in\{1\}\cup S_{i_{0}}\), and
* \((\hat{\bm{A}}_{k})_{i_{0},1}=(\hat{\bm{A}}_{1})_{i_{0},1}=0\) if \(k\notin\{1\}\cup S_{i_{0}}\), by definition of \(S_{i_{0}}\) and Assumption 8.
* When \(i>j\) and \((i,j)\neq(i_{0},1)\), we have
* \((\hat{\bm{A}}_{k})_{ij}\neq 0\) if \(k=1\) or \(i=i_{k}\), which directly follows from (13), and
* \((\hat{\bm{A}}_{k})_{ij}=(\hat{\bm{A}}_{1})_{ij}\neq 0\), by Assumption 8.

To summarize, for each \(k\), \((\bm{A}_{k})_{ij}\neq 0\Leftrightarrow j\in\mathrm{pa}_{\mathcal{G}}(i)\) and \((i,j)\neq(i_{0},1)\).

Finally, let \(\hat{\bm{w}}_{k}(i)\) be the weight vector of node \(i\) in environment \(k\) in the hypothetical model _i.e._, the vector of nonzero entries in \((\bm{A}_{k})_{i}\), and \(\bm{T}_{S}\) be the submatrix of \(\bm{T}\) by selecting the rows and columns in the index set \(S\), then by (19) we have that

\[\hat{\omega}_{k,i,i}=t_{ii}^{-2}\cdot\omega_{k,i,i},\quad\hat{\omega}_{k,i,i} ^{\frac{1}{2}}\omega_{k,i,i}^{-\frac{1}{2}}\bm{w}_{k}(i)\bm{T}_{\mathrm{pa}_{ \mathcal{G}}(i)}=\begin{cases}\quad\hat{\bm{w}}_{k}(i)&\text{if $i\neq i_{0}$}\\ \left[0,\hat{\bm{w}}_{k}(i)\right]&\text{if $i=i_{0}$}\end{cases}\] (20)

By our assumption, for \(\forall k\geqslant 2\), \(i\neq i_{k}\Rightarrow\bm{w}_{k}(i)=\bm{w}_{1}(i)\) and \(\omega_{k,i,i}=\omega_{1,i,i}\). Thus (20) imply that \(\forall k\geqslant 2\), \(i\neq i_{k}\Rightarrow\hat{\bm{w}}_{k}(i)=\hat{\bm{w}}_{1}(i)\) and \(\hat{\omega}_{k,i,i}=\hat{\omega}_{1,i,i}\). In other words, a single-node intervention on node \(i_{k}\) in environment \(k\) in the ground-truth model corresponds to a single-node intervention on node \(i_{k}\) in environment \(k\) in the hypothetical model, thereby completing the proof.

### Proof of Theorem 4

We first prove two lemmas.

**Lemma 14**.: \(\forall i\in[d]\)_, we have \(\mathrm{span}\left\langle(\bm{M}_{k})_{i}:k\in[K]\right\rangle=\mathrm{span} \left\langle\bm{h}_{j}:j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\right\rangle\)._

Proof.: Since \((\bm{M}_{k})_{i}=(\bm{B}_{k})_{i}\bm{H}\), and \((\bm{B}_{k})_{ij}\neq 0\Leftrightarrow j\in\overline{\mathrm{pa}}_{ \mathcal{G}}(i)\), we can see that \((\bm{M}_{k})_{i}\in\mathrm{span}\left\langle\bm{h}_{j}:j\in\overline{\mathrm{pa }}_{\mathcal{G}}(i)\right\rangle\). On the other hand, since \(\bm{H}\) is invertible, by Assumption 5 we have \(\dim\mathrm{span}\left\langle(\bm{M}_{k})_{i}:k\in[K]\right\rangle=\dim\mathrm{ span}\left\langle(\bm{B}_{k})_{i}:k\in[K]\right\rangle=\left|\overline{ \mathrm{pa}}_{\mathcal{G}}(i)\right|\). Thus we must have \(\mathrm{span}\left\langle(\bm{M}_{k})_{i}:k\in[K]\right\rangle=\mathrm{span} \left\langle\bm{h}_{j}:j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\right\rangle\). 

**Lemma 15**.: _Let \(\hat{S}\) be an ancestral set of graph \(\mathcal{G}\) and \(\hat{\bm{V}}_{k}=\mathrm{span}\left\langle(\bm{M}_{k})_{s}:s\in\hat{S}\right\rangle, k\in[K]\). Then we have \(\bm{V}_{1}=\bm{V}_{2}=\cdots=\bm{V}_{K}=\mathrm{span}\left\langle\bm{h}_{s}:s\in\hat{S}\right\rangle\)._Proof.: Recall that \(\bm{M}_{k}=\bm{B}_{k}\bm{H}\), so for \(\forall s\in\hat{S}\), the \(s\)-th row of \(\bm{M}_{k}\) can be written as

\[(\bm{M}_{k})_{s}=\sum_{t=1}^{d}(\bm{B}_{k})_{st}\bm{h}_{t}=\sum_{t\in\overline{ \mathrm{pa}}_{\mathcal{G}}(s)}(\bm{B}_{k})_{st}\bm{h}_{t}\in\mathrm{span}\left< \bm{h}_{s}:s\in\hat{S}\right>\] (21)

where the last equation is because \(\hat{S}\) is ancestral \(\Rightarrow\overline{\mathrm{pa}}_{\mathcal{G}}(s)\subseteq\hat{S}\). Thus, for \(\forall k\in[K]\), \(\hat{\bm{V}}_{k}=\mathrm{span}\left<(\bm{M}_{k})_{s}:s\in\hat{S}\right> \subseteq\mathrm{span}\left<\bm{h}_{s}:s\in\hat{S}\right>\). On the other hand, recall that both \(\bm{B}_{k}\) and \(\bm{H}\) have full rank, so \(\bm{M}_{k}\) has full row rank as well, which implies that \(\dim\bm{V}_{k}=|S|=\dim\mathrm{span}\left<\bm{h}_{s}:s\in\hat{S}\right>\). Hence, \(\bm{V}_{k}=\mathrm{span}\left<\bm{h}_{s}:s\in\hat{S}\right>,\forall k\in[K]\). 

The following two propositions show that our algorithm always maintain an ancestral set, recursively adds a new node into the set and correctly identifies its parents.

**Proposition 3** (Proposition 1 restated).: _The following two propositions hold for Algorithm 3:_

* \(\mathrm{ans}_{\mathcal{G}}(i)\subseteq S\Leftrightarrow\) _the_ if condition in line 8 of Algorithm 3 is fulfilled;_
* _the set_ \(S\) _maintained in Algorithm_ 3 _is always an ancestral set, in the sense that_ \(j\in S\Rightarrow\mathrm{ans}_{\mathcal{G}}(j)\subseteq S\)_._

Proof.: At the starting point, we have \(S=\emptyset\) which is obviously an ancestral set. Now suppose that after the \(\ell\)-th iteration, \(S=\{s_{1},s_{2},\cdots,s_{\ell}\}\) is an ancestral set. In the following, we show that \(\mathrm{ans}_{\mathcal{G}}(i)\subseteq S\Leftrightarrow\) the if condition in line 8 is fulfilled. This would immediately imply that there always exists a node \(i\) that can be added into \(S\) in the \((\ell+1)\)-th iteration, and that after adding \(i\), \(S\) is still an ancestral set.

Suppose that \(\mathrm{ans}_{\mathcal{G}}(i)\subseteq S\) for some \(i\notin S\), by Lemma 14 we know that \((\bm{M}_{k})_{i}\in\mathrm{span}\left<\bm{h}_{j}:j\in\overline{\mathrm{pa}}_{ \mathcal{G}}(i)\right>\), so there exists \(\alpha_{k}\in\mathbb{R}\) such that \((\bm{M}_{k})_{i}-\alpha_{k}\bm{h}_{i}\in\mathrm{span}\left<\bm{h}_{j}:j\in \mathrm{pa}_{\mathcal{G}}(i)\right>\). Moreover, since \((\bm{M}_{k})_{i}=\sum_{j\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)}(\bm{B}_{k} )_{jj}\bm{h}_{j}\), \((\bm{B}_{k})_{ii}=\omega_{k,i,i}^{-\frac{1}{2}}\neq 0\) and \(\bm{H}\) has full row rank by assumption, we must have \((\bm{M}_{k})_{i}\notin\mathrm{span}\left<\bm{h}_{j}:j\in\mathrm{pa}_{\mathcal{G }}(i)\right>\) and so \(\alpha_{k}\neq 0\). Thus, we have by the linearity of the projection operator

\[\bm{q}_{k}:=\mathrm{proj}_{\bm{V}_{k}^{\perp}}\left((\bm{M}_{k})_{i}\right)= \mathrm{proj}_{\bm{V}_{k}^{\perp}}\left((\bm{M}_{k})_{i}-\alpha_{k}\bm{h}_{i} \right)+\mathrm{proj}_{\bm{V}_{k}^{\perp}}\left(\alpha_{k}\bm{h}_{i}\right)= \alpha_{k}\mathrm{proj}_{\bm{V}_{k}^{\perp}}\left(\bm{h}_{i}\right).\]

Recall that all the \(\bm{V}_{k}\)'s are the same and equal \(\mathrm{span}\left<\bm{h}_{s}:s\in S\right>\) by Lemma 15. So \(\dim\mathrm{span}\left<\bm{q}_{k}:k\in[K]\right>\leqslant 1\). Since \(\bm{H}\) has full row rank, we have \(\bm{h}_{i}\notin\mathrm{span}\left<\bm{h}_{s}:s\in S\right>=\bm{V}_{k}\), so that \(\dim\mathrm{span}\left<\bm{q}_{k}:k\in[K]\right>=1\) holds, which is exactly the if condition in line 8.

Conversely, suppose that there is an \(i\notin S\) such that \(\mathrm{ans}_{\mathcal{G}}(i)\nsubseteq\) but \(\dim\mathrm{span}\left<\bm{q}_{k}:k\in[K]\right>=1\) holds. Since \(S\) is ancestral, we know that there must be some \(j\in\mathrm{pa}_{\mathcal{G}}(i)\) such that \(j\notin S\). Since \(\bm{e}_{i}\) and \(\bm{e}_{j}\) both have support on the coordinates in \(\overline{\mathrm{pa}}_{\mathcal{G}}(i)\), by Assumption 5 we know that \(\mathrm{span}\langle\bm{e}_{i},\bm{e}_{j}\rangle\subseteq\mathrm{span}\langle( \bm{B}_{k})_{i}:k\in[K]\rangle\), so that \(\mathrm{span}\langle\bm{h}_{i},\bm{h}_{j}\rangle=\mathrm{span}\langle\bm{e}_ {i},\bm{e}_{j}\rangle\bm{H}\subseteq\mathrm{span}\langle(\bm{B}_{k})_{i}:k\in[ K]\rangle\bm{H}=\mathrm{span}\langle(\bm{M}_{k})_{i}:k\in[K]\rangle\). Since \(\dim\mathrm{span}\left<\bm{q}_{k}:k\in[K]\right>=1\), there must exist some vector \(\bm{u}\in\mathbb{R}^{n}\) and \(\alpha_{i},\alpha_{j}\in\mathbb{R}\) such that \(\bm{h}_{i}-\alpha_{i}\bm{u},\bm{h}_{j}-\alpha_{j}\bm{u}\in\bm{V}_{k}=\mathrm{ span}\langle\bm{h}_{s}:s\in S\rangle\). Since \(i,j\notin S\) and \(\bm{H}\) has full row rank, we can deduce that \(\bm{h}_{i},\bm{h}_{j}\notin\mathrm{span}\langle\bm{h}_{s}:s\in S\rangle\), and so both of \(\alpha_{i}\) and \(\alpha_{j}\) are non-zero. Hence \(\alpha_{j}\bm{h}_{i}-\alpha_{i}\bm{h}_{j}\in\mathrm{span}\langle\bm{h}_{s}:s\in S\rangle\), which is impossible since we know that \(\bm{H}\) has full row-rank. 

**Proposition 4** (Proposition 2 restated).: _Given any ordered ancestral set \(S\) that contains \(\mathrm{pa}_{\mathcal{G}}(i)\) for some \(i\notin S\), Algorithm 2 returns a set \(P_{i}\subseteq S\) that is exactly \(\mathrm{pa}_{\mathcal{G}}(i)\)._

Proof.: As we have shown in Proposition 1, for each possible input \((S,i)\) to Algorithm 2, both \(S\) and \(S\cup\{i\}\) are ancestral sets, so that \(\mathrm{ans}_{\mathcal{G}}(i)\subseteq S\). Similarly one can see that inside the set \(S:=\{s_{1},s_{2},\cdots,s_{m}\}\), all the ancestors of \(s_{j}\) are contained in \(\{s_{1},s_{2},\cdots,s_{j-1}\}\). In the following, we show that \(\forall m^{\prime}\in\{0,\ldots,m\}\), \(r_{m^{\prime}}=\left|\overline{\mathrm{pa}}_{\mathcal{G}}(i)-\{s_{j}:j\leqslant m ^{\prime}\}\right|\) (*).

[MISSING_PAGE_EMPTY:28]

_;_
3. _the intervention distributions on each node are non-degenerate in the sense of Definition_ 11_: there exists_ \(\bm{N_{z}}\subseteq\bm{O_{z}}\) _and_ \(\bm{N_{v}}\subseteq\bm{O_{v}}\) _satisfying_ \(\bm{N_{v}^{\circ}}=\bm{N_{v}^{\circ}}=\emptyset\) _where_ \(S^{\circ}\) _denotes the interior of a set_ \(S\)_, such that for all_ \(i\in[d]\)_,_ \(\left\{p_{i}^{E}(\cdot):E\in\mathfrak{E}_{\pi^{-1}(i)}\right\}\) _(resp._ \(\left\{q_{i}^{E}(\cdot):E\in\mathfrak{E}_{\pi^{\prime-1}(i)}\right\}\)_) is non-degenerate on node_ \(i\) _in_ \(\bm{O_{z}}\setminus\bm{N_{z}}\) _(resp._ \(\bm{O_{v}}\setminus\bm{N_{v}}\)_)._

_Then we must have \(\left(\bm{h},\mathcal{G}\right)\sim_{\mathrm{sur}}(\hat{\bm{h}},\hat{ \mathcal{G}})\)._

Previous works on the identifiability of non-parametric causal models typically require that all the joint distributions are supported on the whole space \(\mathbb{R}^{d}\)[49, 23, 47]. In contrast, we only assume that the densities have common and unknown support across all interventions.

Theorem 7 can be regarded as a soft-intervention version of 49, Theorem 4.3, which assumes access to hard interventions and only need two paired interventions per node. While they are able to show full identifiability, we show in the following that identifiability up to \(\sim_{\mathrm{sur}}\) is the best we can hope for with soft interventions.

**Theorem 8** (Counterpart to Theorem 7, informal version of Theorem 10).: _For any causal model \(\left(\bm{h},\mathcal{G}\right)\) and any set of environments \(\mathfrak{E}=\left\{E_{k}:k\in[K]\right\}\) such that all conditions in Theorem 7 are satisfied, there must exists a candidate solution \(\left(\hat{\bm{h}},\mathcal{G}\right)\) and a hypothetical data generating process that satisfy the same set of conditions, but_

\[\frac{\partial\bm{v}_{i}}{\partial\bm{z}_{j}}\neq 0,\quad\forall j\in\overline{ \mathrm{sur}}_{\mathcal{G}}(i).\]

_Finally, the ambiguity still exists if we additionally assume standard axioms such as causal minimality (Assumption 6) and faithfulness (Assumption 7) on the causal model._

### Proof of Lemma 16

Let \(\bm{w}_{k}(i)\in\mathbb{R}^{\left|p_{\mathrm{ap}}(i)\right|}\) be the vector obtained by removing all zero entries in the \(i\)-th row of \(\bm{A}_{k}\) and \(\omega_{k,i,i}\) be the \(i\)-th diagonal entry in \(\bm{\Omega}_{k}\), then for the \(k\)-th environment we have \(\bm{z}_{i}=\bm{w}_{k}(i)^{\top}\bm{z}_{\mathrm{pa}_{\mathcal{G}}(i)}+\omega_{ k,i,i}^{\frac{1}{2}}\epsilon_{i}\), so that

\[\hat{p}_{k}\left(\bm{z}_{i}\mid\bm{z}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)= \omega_{k,i,i}^{-\frac{1}{2}}p_{\epsilon_{i}}\left(\omega_{k,i,i}^{-\frac{1}{2 }}(\bm{z}_{i}-\langle\bm{w}_{k}(i),\bm{z}_{\mathrm{pa}_{\mathcal{G}}(i)} \rangle)\right)\]

where \(p_{\epsilon_{i}}(\cdot)\) is the density of \(\epsilon_{i}\). As a result, we have

\[\nabla\frac{\hat{p}_{1}}{\hat{p}_{k}}\left(\bm{z}_{i}\mid\bm{z}_ {\mathrm{pa}_{\mathcal{G}}(i)}\right) =\frac{\hat{p}_{1}}{\hat{p}_{k}}\left(\bm{z}_{i}\mid\bm{z}_{ \mathrm{pa}_{\mathcal{G}}(i)}\right)\cdot\nabla\log\frac{\hat{p}_{1}}{\hat{p} _{k}}\left(\bm{z}_{i}\mid\bm{z}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)\] \[=\frac{\hat{p}_{1}}{\hat{p}_{k}}\left(\bm{z}_{i}\mid\bm{z}_{ \mathrm{pa}_{\mathcal{G}}(i)}\right)\cdot\left[c_{i1}(1,-\bm{w}_{1}(i))-c_{ik }(1,-\bm{w}_{k}(i))\right]\]

where for convenience we use \(\nabla\) to denote the gradient with respect to all variables \(\bm{z}_{\overline{\mathrm{pa}}_{\mathcal{G}}(i)}\), and \(c_{ik}=\omega_{k,i,i}^{-\frac{1}{2}}\), \(\frac{p_{\epsilon_{i}}^{\prime}}{p_{\epsilon_{i}}}\left(\omega_{k,i,i}^{-\frac {1}{2}}(\bm{z}_{i}-\langle\bm{w}_{k}(i),\bm{z}_{\mathrm{pa}_{\mathcal{G}}(i) }\rangle)\right)\) (we omit the dependency on \(\bm{z}\) for simplicity).

Definition 11 implies that \(\mathrm{span}\left\langle c_{i1}(1,-\bm{w}_{1}(i))-c_{ik}(1,-\bm{w}_{k}(i)):2 \leqslant k\leqslant K\right\rangle\ =\ \mathbb{R}^{\left|p_{\mathrm{ap}}(i)\right|+1}\), thus it holds that \(\mathrm{span}\langle(1,-\bm{w}_{k}(i)):k\in[K]\rangle=\mathbb{R}^{\left|p_{ \mathrm{ap}}(i)\right|+1}\) as well. By definition of \(\bm{B}_{k}\), this immediately implies that \(\dim\left(\mathrm{span}\left\langle(\bm{B}_{k})_{i}:k\in[K]\rangle\right)= \left|p_{\mathrm{ap}_{\mathcal{G}}}(i)\right|+1\) as desired.

### Proof of Theorem 7

Define \(\bm{\tau}:=\hat{\bm{h}}\circ\bm{h}^{-1}:\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\), then we have that \(\bm{v}=\bm{\tau}(\bm{z})\). Since both \(\bm{h}\) and \(\hat{\bm{h}}\) are diffeomorphisms by assumption, so is \(\bm{\tau}\). To avoid confusion, in this section we use \(\bm{z}\) (resp. \(\bm{v}\)) to denote random variables while using \(\hat{\bm{z}}\) (resp. \(\hat{\bm{v}}\)) to denote (deterministic) vectors.

Let \(\mathfrak{E}_{j}=\left\{E_{k}^{(j)}:k\in[K_{j}]\right\}\) be the \(j\)-th collection of environments according to our assumption. We first prove the following lemma:

**Lemma 17**.: \(\bm{O_{v}}=\bm{\tau}(\bm{O_{z}})\)_._Proof.: By the change of variable formula [35], for \(\forall\hat{\bm{z}}\in\mathbb{R}^{d}\) and \(\forall E\in\mathfrak{E}\) we have \(p_{E}(\hat{\bm{z}})=q_{E}(\hat{\bm{v}})\left|\det\bm{J}_{\bm{\tau}}(\hat{\bm{z} })\right|\), where \(\hat{\bm{v}}=\bm{\tau}(\hat{\bm{z}})\). Since \(\bm{\tau}\) is a diffeomorphism, we must have \(\left|\det\bm{J}_{\bm{\tau}}(\hat{\bm{z}})\right|\neq 0\), so \(\hat{\bm{z}}\in\bm{O}_{\bm{z}}\Leftrightarrow\hat{\bm{v}}=\bm{\tau}(\hat{\bm{ z}})\in\bm{O}_{\bm{v}}\), concluding the proof. 

**Lemma 18**.: _Let \(\hat{\bm{z}}\in\bm{O}_{\bm{z}}\). For \(\forall j\in[d]\) and \(2\leqslant k\leqslant K_{j}\), we have_

\[\frac{p_{j}^{E_{k}^{(j)}}}{p_{j}^{E_{1}^{(j)}}}\left(\hat{\bm{z}}_{j}\mid\hat {\bm{z}}_{\mathrm{pa}_{\mathcal{G}}(j)}\right)=\frac{q_{j}^{E_{j}^{(j)}}}{q_ {j}^{E_{1}^{(j)}}}\left(\hat{\bm{v}}_{j}\mid\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{ G}}(j)}\right),\] (22)

_where \(\hat{\bm{v}}=\bm{\tau}(\hat{\bm{z}})\in\bm{O}_{\bm{v}}\)._

Proof.: Since \(\bm{v}=\bm{\tau}(\bm{z})\), by the change-of-measure formula [35] we have that for \(\forall\hat{\bm{z}}\in\bm{O}_{\bm{z}}\),

\[\prod_{i=1}^{d}p_{i}^{E}\left(\hat{\bm{z}}_{i}\mid\hat{\bm{z}}_{\mathrm{pa}_{ \mathcal{G}}(i)}\right)=p_{E}(\hat{\bm{z}})=q_{E}(\hat{\bm{v}})\left|\det\bm{J }_{\bm{\tau}}(\hat{\bm{z}})\right|=\prod_{i=1}^{d}q_{i}^{E}\left(\bm{\tau}_{i} (\hat{\bm{z}})\mid\bm{\tau}_{\mathrm{pa}_{\mathcal{G}}(i)}(\hat{\bm{z}}) \right)\left|\det\bm{J}_{\bm{\tau}}(\hat{\bm{z}})\right|\] (23)

for all \(E\in\mathfrak{E}_{j}\), where \(\hat{\bm{v}}=\bm{\tau}(\hat{\bm{z}})\). By Assumption \((ii)\) and Definition 2, we know that \(p_{i}^{E_{k}^{1}}=p_{i}^{E_{1}^{(1)}}\Leftrightarrow i\neq 1\) and \(q_{i}^{E_{k}^{1}}=q_{i}^{E_{1}^{(1)}}\Leftrightarrow i\neq 1\) for all \(k>1\). Thus, we have that

\[\prod_{i=1}^{d}\frac{p_{i}^{E_{k}^{(j)}}\left(\hat{\bm{z}}_{i}\mid\hat{\bm{z} }_{\mathrm{pa}_{\mathcal{G}}(i)}\right)}{p_{i}^{E_{i}^{(j)}}\left(\hat{\bm{z} }_{i}\mid\hat{\bm{z}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)}=\frac{p_{j}^{E_{k }^{(j)}}}{p_{j}^{E_{i}^{(j)}}}(\hat{\bm{z}}_{j}\mid\hat{\bm{z}}_{\mathrm{pa} _{\mathcal{G}}(j)})\]

and

\[\prod_{i=1}^{d}\frac{q_{i}^{E_{k}^{(j)}}\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v} }_{\mathrm{pa}_{\mathcal{G}}(i)}\right)}{q_{i}^{E_{1}^{(j)}}\left(\hat{\bm{v} }_{i}\mid\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)}=\frac{q_{j}^{E_{k }^{(j)}}}{q_{j}^{E_{i}^{(j)}}}(\hat{\bm{v}}_{j}\mid\hat{\bm{v}}_{\mathrm{pa}_{ \mathcal{G}}(j)}).\]

Since the LHS of the above two equations are the same by (23), the RHS must also be the same, concluding the proof. 

We assume WLOG that the vertices of \(\mathcal{G}\) are labelled such that \(i\to j\Rightarrow i<j\), and that \(\pi(i)=i,\forall i\in[d]\). Also we can assume the nodes are fixed and only consider how they are connected, _i.e., \(\pi^{\prime}(i)=i,\forall i\in[d]\)_. 1

Footnote 1: This is also WLOG because we now have groups of soft interventions where each group corresponds to a single node, so we can just relabel the node in \(\hat{\mathcal{G}}\) that corresponds to the \(i\)-th group as node \(i\).

**Lemma 19**.: _We have \(\left(\bm{\tau}(\bm{N}_{\bm{z}})\right)^{\mathrm{o}}=\left(\bm{\tau}^{-1}(\bm{N }_{\bm{v}})\right)^{\mathrm{o}}=\emptyset\)._

Proof.: The result immediately follows from the assumption that \(\bm{N}_{\bm{z}}^{\mathrm{o}}=\bm{N}_{\bm{v}}^{\mathrm{o}}=\emptyset\) and that \(\tau\) is a diffeomorphism. 

For any vertex set \(V\), we use \(\mathcal{G}_{V}\) to denote its corresponding induced subgraph of \(\mathcal{G}\). We first prove the following statements by induction on \(j\):

1. \(\forall i\neq j\), \(i\in\mathrm{pa}_{\mathcal{G}}(j)\Leftrightarrow i\in\mathrm{pa}_{\mathcal{G}} \);
2. \(\forall j\in[d]\), there exists a continuously differentiable function \(\phi_{i}\) such that \(\bm{v}_{j}=\phi_{j}\left(\bm{z}_{\overline{\mathrm{pa}}_{\mathcal{G}}(j)}\right)\). Moreover, \(\frac{\partial\phi_{j}}{\partial\bm{z}_{j}}\not\equiv 0\) (_i.e._, not always zero).
3. \(\forall j\in[d]\), there exists a continuously differentiable function \(\Upsilon_{j}\) such that \(\bm{v}_{\overline{\mathrm{pa}}_{\mathcal{G}}(j)}=\Upsilon_{j}(\bm{z}_{ \overline{\mathrm{pa}}_{\mathcal{G}}(j)})\).

For \(j=1\), by assumption \(\mathrm{pa}_{\mathcal{G}}(j)=\emptyset\). Lemma 18 implies that for any \(\hat{\bm{z}}\in\bm{O}_{\bm{z}}\),

\[\frac{p_{1}^{E_{k}^{(1)}}}{p_{1}^{E_{1}^{(1)}}}(\hat{\bm{z}}_{1})=\frac{q_{1}^{E _{k}^{(1)}}}{q_{1}^{E_{1}^{(1)}}}\left(\hat{\bm{v}}_{1}\mid\hat{\bm{v}}_{ \mathrm{pa}_{\mathcal{G}}(1)}\right),\forall 2\leqslant k\leqslant K_{1}.\] (24)

[MISSING_PAGE_FAIL:31]

Hence, we must have \(\operatorname{pa}_{\hat{\mathcal{G}}}(j)\subseteq\{i:i<j\}\). Furthermore, if there exists \(i\in\operatorname{pa}_{\hat{\mathcal{G}}}(j)\) such that \(i\notin\operatorname{pa}_{\mathcal{G}}(j)\), then the induction hypothesis implies that \(\frac{\partial\mathbf{v}_{i}}{\partial\mathbf{z}_{i}}\neq 0\), but \(\bm{v}_{i}\) is a function of \(\bm{z}_{\overline{\operatorname{pa}}_{\mathcal{G}}(j)}\) as previously derived, which is also a contradiction. Thus we actually have \(\operatorname{pa}_{\hat{\mathcal{G}}}(j)\subseteq\operatorname{pa}_{\mathcal{ G}}(j)\).

In a completely symmetric manner, we can take the derivatives of (25) w.r.t. \(\bm{v}_{i},\forall i\in\overline{\operatorname{pa}}_{\hat{\mathcal{G}}}(j)\) and obtain that \(\operatorname{pa}_{\mathcal{G}}(j)\subseteq\operatorname{pa}_{\hat{\mathcal{ G}}}(j)\). Hence, \(\operatorname{pa}_{\hat{\mathcal{G}}}(j)=\operatorname{pa}_{\mathcal{G}}(j)\), completing the proof of (1) and (3) for the \(j\) case.

Finally, if \(\frac{\partial\mathbf{v}_{i}}{\partial\mathbf{z}_{j}}\equiv 0\), then by (3) and the induction hypothesis, \(\bm{v}_{1},\cdots,\bm{v}_{j}\) are all functions of \(\bm{z}_{[j-1]}\), which implies that \((\bm{v}_{1},\cdots,\bm{v}_{j})\) lies on a submanifold with dimension \(\leqslant j-1\), again contradicting assumption _(i)_. Thus \(\frac{\partial\mathbf{v}_{j}}{\partial\mathbf{z}_{j}}\neq 0\). This completes the proof of our inductive step.

To recap, we now know that

* \(\mathcal{G}=\hat{\mathcal{G}}\), and
* For \(\forall i\in[d]\), there exists a function \(\Upsilon_{i}\) such that \(\bm{v}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i)}=\Upsilon_{i}\left(\bm{ z}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i)}\right)\).

It remains to show that for \(\forall k\in\operatorname{pa}_{\mathcal{G}}(i)\setminus\operatorname{sur}_{ \mathcal{G}}(i)\), \(\Upsilon_{i}\) doesn't depend on \(\bm{z}_{k}\).

By definition, if \(k\in\operatorname{pa}_{\mathcal{G}}(i)\setminus\operatorname{sur}_{\mathcal{G }}(i)\), we know that there exists \(j\in\operatorname{ch}_{\mathcal{G}}(i)\) such that \(j\notin\operatorname{ch}_{\mathcal{G}}(k)\). We have shown that \(\bm{v}_{i}\), as a component of \(\bm{v}_{\overline{\operatorname{pa}}_{\mathcal{G}}(j)}\), is a function of \(\bm{z}_{\overline{\operatorname{pa}}_{\mathcal{G}}(j)}\). By the choice of \(k\), we have \(k\notin\overline{\operatorname{pa}}_{\mathcal{G}}(j)\), so that \(\bm{v}_{i}\) does not depend on \(\bm{z}_{k}\). The conclusion follows.

## Appendix J Omitted Proofs for Theorem 3 and Theorem 8

In this section we provide detailed proofs of main ambiguity results.

**Definition 12**.: _We say that a matrix \(\bm{M}\in\mathbb{R}^{d\times d}\) is effect-respecting for a causal graph \(\mathcal{G}\), or \(\bm{M}\in\mathcal{M}_{\operatorname{sur}}(\mathcal{G})\), if \(\bm{M}_{ij}\neq 0\Leftrightarrow j\in\overline{\operatorname{sur}}_{ \mathcal{G}}(i)\). We also write \(\bm{M}\in\mathcal{M}_{\operatorname{sur}}^{0}(\mathcal{G})\) if \(\bm{M}\) is invertible and \(\bm{M}_{ij}\neq 0\Rightarrow j\in\overline{\operatorname{sur}}_{\mathcal{G}}(i)\). Finally, we write \(\bm{M}\in\overline{\mathcal{M}}_{\operatorname{sur}}(\mathcal{G})\) if \(\bm{M}_{ij}\neq 0\Rightarrow j\in\overline{\operatorname{sur}}_{\mathcal{G}}(i)\)._

**Remark 1**.: _By definition \(\mathcal{M}_{\operatorname{sur}}^{0}(\mathcal{G})\) is the set of all matrices \(\bm{M}\) where \(\bm{M}_{ij}\neq 0,\forall j\notin\overline{\operatorname{sur}}_{\mathcal{G}}(i)\), so it can be identified as \(\mathbb{R}^{d+d_{\mathcal{G}}}\) where \(d_{\mathcal{G}}=\sum_{i=1}^{d}|\operatorname{sur}_{\mathcal{G}}(i)|\). Equipped with the Lebesgue measure, we have \(\mathcal{M}_{\operatorname{sur}}(\mathcal{G})\subset\mathcal{M}_{\operatorname {sur}}^{0}(\mathcal{G})\subset\overline{\mathcal{M}}_{\operatorname{sur}}( \mathcal{G})\) and \(\overline{\mathcal{M}}_{\operatorname{sur}}(\mathcal{G})\setminus\mathcal{M}_{ \operatorname{sur}}(\mathcal{G})\) is a null set. In the remaining part of this section, we will use measure-theoretic statement for \(\bm{M}\in\mathcal{M}_{\operatorname{sur}}(\mathcal{G})\) in the above sense._

We first present a result that serves as a good starting point to understand why this is the case. It states that latent representations that are equivalent under \(\sim_{\operatorname{sur}}\) are essentially generated from the same causal graph.

**Proposition 5**.: _Let \(\bm{M}\) be an invertible matrix such that \(\bm{M}_{ij}\neq 0\Rightarrow j\in\overline{\operatorname{sur}}_{\mathcal{G}}(i)\). Suppose that the latent variables \(\bm{z}\in\mathbb{R}^{d}\) are generated from any distributions \(p_{i}\left(\bm{z}_{i}\mid\bm{z}_{\operatorname{pa}_{\mathcal{G}}(i)}\right),i \in[d]\) with joint density \(p(\bm{z})=\prod_{i=1}^{d}p_{i}\left(\bm{z}_{i}\mid\bm{z}_{\operatorname{pa}_{ \mathcal{G}}(i)}\right)\), then the joint density of \(\bm{v}=\bm{M}\bm{z}\) can be written as \(q(\bm{v})=\prod_{i=1}^{d}q_{i}\left(\bm{v}_{i}\mid\bm{v}_{\operatorname{pa}_{ \mathcal{G}}(i)}\right)\) for some density functions \(q_{i},i\in[d]\)._

### Proof of Proposition 5

We first prove the following lemma:

**Lemma 20**.: _Let \(\bm{M}\in\mathcal{M}_{\operatorname{sur}}^{0}(\mathcal{G})\) and latent variables \(\bm{v}=\bm{M}\bm{z}\), then for \(\forall i\in[d]\), there exists invertible matrices \(\bm{M}_{i}\) and \(\bm{M}_{i}^{-}\) such that \(\bm{v}_{\operatorname{pa}_{\mathcal{G}}(i)}=\bm{M}_{i}^{-}\bm{z}_{\operatorname{ pa}_{\mathcal{G}}(i)}\) and \(\bm{v}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i)}=\bm{M}_{i}\bm{z}_{\overline{ \operatorname{pa}}_{\mathcal{G}}(i)}\)._

Proof.: \(\forall j\in\overline{\operatorname{pa}}_{\mathcal{G}}(i)\), we know that \(\bm{v}_{j}\) is a linear function of \(\bm{z}_{\ell},\ell\in\overline{\operatorname{sur}}_{\mathcal{G}}(j)\). By Lemma 7, we know that \(\overline{\operatorname{sur}}_{\mathcal{G}}(j)\subseteq\overline{\operatorname{pa}}_{ \mathcal{G}}(i)\), so each \(\bm{v}_{j},j\in\overline{\operatorname{pa}}_{\mathcal{G}}(i)\) is a linear function of \(\bm{z}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i)}\). Thus we can write \(\bm{v}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i)}=\bm{M}_{i}\bm{z}_{ \overline{\operatorname{pa}}_{\mathcal{G}}(i)}\). In the following we argue that \(\bm{M}_{i}\) is invertible. Let \(\pi\) be a permutation on \(\overline{\operatorname{pa}}_{\mathcal{G}}(i)\) such that \(k\in\operatorname{pa}_{\mathcal{G}}(\ell)\Rightarrow\pi(k)<\pi(\ell)\) (such \(\pi\) can always be chosen since \(\mathcal{G}\) is acyclic), then we can write

\[\left(\hat{\bm{v}}_{\pi(j)}:j\in\overline{\operatorname{pa}}_{\mathcal{G}}(i) \right)^{\top}=\bm{\tilde{M}}_{i}\left(\hat{\bm{z}}_{\pi(j)}:j\in\overline{ \operatorname{pa}}_{\mathcal{G}}(i)\right)^{\top}\] (26)where \(\hat{\bm{M}}_{i}\) is an upper triangular matrix with non-zero diagonal entries by our choice of \(\bm{M}\). Since \(\bm{M}_{i}\) can be obtained from \(\hat{\bm{M}}_{i}\) be exchanging a few rows and columns, \(\bm{M}_{i}\) is invertible as well.

Similarly, using the fact that \(\forall j\in\mathrm{pa}_{\mathcal{G}}(i)\), \(\overline{\mathrm{surf}}_{\mathcal{G}}(j)\subseteq\mathrm{pa}_{\mathcal{G}}(i)\), we can prove the existence of an invertible matrix \(\bm{M}_{i}^{-}\) such that \(\bm{v}_{\mathrm{pa}_{\mathcal{G}}(i)}=\bm{M}_{i}^{-}\bm{z}_{\mathrm{pa}_{ \mathcal{G}}(i)}\). 

Returning to the proof of Proposition 5. Assume WLOG that the nodes of \(\mathcal{G}\) are ordered in a way such that \(i\in\mathrm{pa}_{\mathcal{G}}(j)\Rightarrow i<j\), so that \(\bm{M}\) is a lower-triangular matrix. The joint density of \(\bm{v}\) can be written as

\[q(\bm{v})=\prod_{i=1}^{d}q\left(\bm{v}_{i}\mid\bm{v}_{1},\cdots,\bm{v}_{i-1} \right).\]

Since \(\bm{v}=\bm{M}z\) and \(\bm{M}\) is lower triangular and invertible (hence, with non-zero diagonals), we know that \((\bm{v}_{1},\bm{v}_{2},\cdots,\bm{v}_{i-1})\) is an invertible linear function of \((\bm{z}_{1},\bm{z}_{2},\cdots,\bm{z}_{i-1})\) and \((\bm{v}_{1},\bm{v}_{2},\cdots,\bm{v}_{i})\) is an invertible linear function of \((\bm{z}_{1},\bm{z}_{2},\cdots,\bm{z}_{i})\). Let \(\hat{\bm{v}}=\bm{M}\hat{\bm{z}}\in\mathbb{R}^{d}\), then we have

\[q\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{1},\cdots,\hat{\bm{v}}_ {i-1}\right) =\frac{q(\hat{\bm{v}}_{1},\hat{\bm{v}}_{2},\cdots,\hat{\bm{v}}_{i })}{q(\hat{\bm{v}}_{1},\hat{\bm{v}}_{2},\cdots,\hat{\bm{v}}_{i-1})}=\frac{p( \hat{\bm{z}}_{1},\hat{\bm{z}}_{2},\cdots,\hat{\bm{z}}_{i})\det\hat{\bm{M}}_{1:i -1,1:i}}{p(\hat{\bm{z}}_{1},\hat{\bm{z}}_{2},\cdots,\hat{\bm{z}}_{i-1})\det \hat{\bm{M}}_{1:i-1,1:i-1}}\] \[\propto\frac{p(\hat{\bm{z}}_{1},\hat{\bm{z}}_{2},\cdots,\hat{\bm{ z}}_{i})}{p(\hat{\bm{z}}_{1},\hat{\bm{z}}_{2},\cdots,\hat{\bm{z}}_{i-1})}=p \left(\hat{\bm{z}}_{i}\mid\hat{\bm{z}}_{1},\cdots,\hat{\bm{z}}_{i-1}\right)=p_ {i}\left(\hat{\bm{z}}_{i}\mid\hat{\bm{z}}_{\mathrm{pa}_{\mathcal{G}}(i)} \right),\]

where \(\hat{\bm{M}}_{1:i,i:i}\) denotes that top-left submatrix of \(\hat{\bm{M}}\) of size \(i\times i\), and the last step follows from the causal Markov condition (Definition 1). On the other hand, let \(q_{i}\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)\) be the conditional density of \(\bm{v}_{i}\) on its parents at \(\hat{\bm{v}}\in\mathbb{R}^{d}\). For \(\forall j\in\mathrm{pa}_{\mathcal{G}}(i)\), from \(\bm{v}=\bm{M}\bm{z}\) we know that \(\bm{v}_{j}\) is a linear function of \(\bm{z}_{\overline{\mathrm{suf}}_{\mathcal{G}}(j)}\). By Lemma 20 we know that \(\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)}\) is a linear function of \(\hat{\bm{z}}_{\mathrm{pa}_{\mathcal{G}}(i)}\) and \(\hat{\bm{v}}_{\overline{\mathrm{pa}}_{\mathcal{G}}(i)}\) is a linear function of \(\hat{\bm{z}}_{\overline{\mathrm{pa}}_{\mathcal{G}}(i)}\), so that

\[q\left(\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)\propto p\left(\hat{ \bm{z}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)\quad\text{ and }\quad q\left(\hat{\bm{v}}_{\overline{\mathrm{pa}}_{\mathcal{G}}(i)} \right)\propto p\left(\hat{\bm{z}}_{\overline{\mathrm{pa}}_{\mathcal{G}}(i)}\right)\]

and

\[q_{i}\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right) \propto\frac{p\left(\hat{\bm{z}}_{\overline{\mathrm{pa}}_{\mathcal{G}}(i)} \right)}{p\left(\hat{\bm{z}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right)}=p_{i} \left(\hat{\bm{z}}_{i}\mid\hat{\bm{z}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right).\]

Hence, we have \(q_{i}\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)} \right)\propto q\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{1},\cdots,\hat{\bm{v}}_ {i-1}\right)\), so that

\[q(\hat{\bm{v}})=\prod_{i=1}^{d}q_{i}\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{ \mathrm{pa}_{\mathcal{G}}(i)}\right)\propto\prod_{i=1}^{d}q_{i}\left(\hat{\bm{v }}_{i}\mid\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right).\]

Since both sides integrate to \(1\), it turns out that they are equal, as desired.

### Formal version and proof of Theorem 3: the linear case

**Theorem 9** (Counterpart to Theorem 1).: _For any causal model \((\bm{H},\mathcal{G})\) and any set of environments \(\mathfrak{C}=\{E_{k}:k\in[K]\}\), suppose that we have observations \(\left\{P_{\bm{X}}^{E}\right\}_{E\in\mathfrak{C}}\) satisfying Assumption 1:_

\[\forall k\in[K],\quad\bm{z}=\bm{A}_{k}\bm{z}+\bm{\Omega}_{k}^{\frac{1}{2}}\epsilon,\quad\bm{x}=\bm{H}^{\dagger}\bm{z}\]

_such that_

1. _the unmixing matrix_ \(\bm{H}\in\mathbb{R}^{d\times n}\) _has full row rank;_
2. \(\forall k\in[K]\) _and_ \(i,j\in[d]\)_,_ \((\bm{A}_{k})_{ij}\neq 0\Leftrightarrow j\in\mathrm{pa}_{\mathcal{G}}(i)\) _and_ \(\bm{\Omega}_{k}\) _is a diagonal matrix with positive entries;_
3. \(\left\{\bm{B}_{k}=\bm{\Omega}_{k}^{-\frac{1}{2}}(\bm{I}-\bm{A}_{k})\right\}_{k=1 }^{K}\) _are node level non-degenerate in the sense of Assumption_ 5_,_

_then there must exist a candidate solution \((\hat{\bm{H}},\mathcal{G})\) and a hypothetical data generating process_

\[\forall k\in[K],\quad\bm{v}=\hat{\bm{A}}_{k}\bm{v}+\hat{\bm{\Omega}}_{k}^{\frac{1 }{2}}\epsilon,\quad\bm{x}=\hat{\bm{H}}^{\dagger}\bm{v}\]

_such that_* _the unmixing matrix_ \(\hat{\bm{H}}\in\mathbb{R}^{d\times n}\) _has full row rank;_
* \(\forall k\in[K]\) _and_ \(i,j\in[d]\)_,_ \((\hat{\bm{A}}_{k})_{ij}\neq 0\Leftrightarrow j\in\mathrm{pa}_{\mathcal{G}}(i)\) _and_ \(\hat{\bm{\Omega}}_{k}\) _is a diagonal matrix with positive entries;_
* \(\left\{\hat{\bm{B}}_{k}=\hat{\bm{\Omega}}_{k}^{-\frac{1}{2}}(\bm{I}-\hat{\bm{ A}}_{k})\right\}_{k=1}^{K}\) _are node level non-degenerate in the sense of Assumption_ 5_,_

_but_

\[\frac{\partial\bm{v}_{i}}{\partial\bm{z}_{j}}\neq 0,\quad\forall j\in\overline{ \mathrm{sur}}_{\mathcal{G}}(i).\]

_Finally, if we additionally assume that_

* _the environments are groups of single-node interventions: there exists a partition_ \(\mathfrak{E}=\cup_{i=1}^{d}\mathfrak{E}_{i}\) _such that_ \(\mathcal{I}_{\bm{z}}^{\mathfrak{E}_{i}}=\{i\}\) _(see Definition_ 2_),_

_then we can guarantee the existence of \((\hat{\bm{H}},\mathcal{G})\) and weight matrices which, besides the properties listed above, also satisfy_

* _for the same partition_ \(\mathfrak{E}=\cup_{i=1}^{d}\mathfrak{E}_{i}\)_, we have_ \(\mathcal{I}_{\bm{v}}^{\mathfrak{E}_{i}}=\{i\}\)_._

_In other words, additionally assuming that the environments are from single-node interventions does not resolve the ambiguity._

**Remark 2**.: _Compared with our identifiability guarantee Theorem 1, Theorem 9 actually demonstrates a stronger form of impossibility. Specifically, it states that the SNA cannot be resolved even if both the ground-truth causal graph and the noise variables are known._

We define

\[\bm{v}=\bm{M}\bm{z}\] (27)

where \(\bm{M}\) is an effect-respecting matrix. At this point we do not make any other restrictions on \(\bm{M}\), but we will specify the appropriate choise of \(\bm{M}\) later.

By assumption, the latent variables in the \(k\)-th environment are generated by

\[\bm{z}=\bm{A}_{k}\bm{z}+\bm{\Omega}_{k}^{\frac{1}{2}}\epsilon,\]

then \(\bm{v}=\bm{M}(\bm{I}-\bm{A}_{k})^{-1}\bm{\Omega}_{k}^{\frac{1}{2}}\epsilon\). Let \(\hat{\bm{\Omega}}_{k}\) be the diagonal matrix with entries \(\bm{M}_{ii}^{2}\cdot(\bm{\Omega}_{k})_{ii},i\in[d]\) and \(\hat{\bm{A}}_{k}=\bm{I}-\hat{\bm{\Omega}}_{k}^{\frac{1}{2}}\bm{\Omega}_{k}^{ -\frac{1}{2}}(\bm{I}-\bm{A}_{k})\bm{M}^{-1}\), then \(\bm{v}=\hat{\bm{A}}_{k}\bm{v}+\hat{\bm{\Omega}}_{k}^{\frac{1}{2}}\epsilon\). Note that the choice of \(\hat{\bm{\Omega}}_{k}\) here is to that the diagonal entries of \(\hat{\bm{A}}_{k}\) are zero, as we show below. It remains to show that: for almost all \(\bm{M}\in\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\), it holds for \(\forall k\in[K]\) that \((\hat{\bm{A}}_{k})_{ij}=0\Leftrightarrow j\notin\mathrm{pa}_{\mathcal{G}}(i)\).

For the \(\Leftarrow\) direction, since \(\bm{M}\in\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\), \(\bm{M}^{-1}\in\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\) as well. Thus, \(\forall j\notin\mathrm{pa}_{\mathcal{G}}(i)\) we have

\[\left[(\bm{I}-\bm{A}_{k})\bm{M}^{-1}\right]_{ij} =\sum_{\ell=1}^{d}(\bm{I}-\bm{A}_{k})_{i\ell}\cdot(\bm{M}^{-1})_ {\ell j}=\sum_{\ell\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)\cap\{\ell^{ \prime}:j\in\overline{\mathrm{sur}}_{\mathcal{G}}(\ell^{\prime})\}}(\bm{I}- \bm{A}_{k})_{i\ell}\cdot(\bm{M}^{-1})_{\ell j}\] \[=\begin{cases}\quad\quad\quad\quad 0&\text{if }j\notin\overline{ \mathrm{pa}}_{\mathcal{G}}(i)\\ (\bm{M}^{-1})_{ii}&\text{if }j=i\end{cases}\]

where the last step holds because \(\forall\ell\in[d]\), \(\ell\in\overline{\mathrm{pa}}_{\mathcal{G}}(i),j\in\overline{\mathrm{sur}}_{ \mathcal{G}}(\ell)\Rightarrow j\in\overline{\mathrm{pa}}_{i}\), and when \(j=i\), the only such \(\ell\) is \(\ell=i\). Hence, we can see that our choice of \(\hat{\bm{A}}_{k}\) satisfies

\[\left(\hat{\bm{A}}_{k}\right)_{ij}=\begin{cases}\quad\quad\quad\quad\quad 0-0=0& \text{if }j\notin\overline{\mathrm{pa}}_{\mathcal{G}}(i)\\ 1-\phi_{k,i,i}^{\frac{1}{2}}\omega_{k,i,i}^{-\frac{1}{2}}(\bm{M}^{-1})_{ii}=0& \text{if }j=i,\end{cases}\]

so \(\left(\hat{\bm{A}}_{k}\right)_{ij}\neq 0\Rightarrow j\in\mathrm{pa}_{\mathcal{G}}(i)\).

Conversely, for \(\forall j\in\mathrm{pa}_{\mathcal{G}}(i)\),

\[(\hat{\bm{A}}_{k})_{ij}=0\Leftrightarrow\sum_{s\in\overline{\mathrm{pa}}_{ \mathcal{G}}(i)}(\bm{I}-\bm{A}_{k})_{is}(\bm{M}^{-1})_{sj}=0\Leftrightarrow \sum_{s\in\overline{\mathrm{pa}}_{\mathcal{G}}(i)}(-1)^{s}(\bm{I}-\bm{A}_{k})_{ is}\det\bm{M}_{sj}^{-}=0\] (28)where \(\bm{M}_{sj}^{-}\) is the \((d-1)\times(d-1)\) matrix obtained by removing the \(s\)-th row and \(j\)-th column of \(\bm{M}\), and the second step in the equation above follows from the fact that \(\bm{M}^{-1}=\det(\bm{M})^{-1}\mathrm{adj}(\bm{M})\), where \(\mathrm{adj}(\bm{M})\) denotes the adjugate matrix of \(\bm{M}\) whose \((i,j)\)-th entry is \((-1)^{i+j}\det\bm{M}_{ij}^{-}\).

(28) holds if only if \(\bm{M}\) takes values on a lower-dimensional algebraic manifold of its embedded space \(\mathbb{R}^{d+d_{\mathcal{G}}}\) (see Remark 1). As a result, for almost every \(\bm{M}\in\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\), \(\bm{v}\) is generated from a linear causal model with graph \(\mathcal{G}\) as defined in (3). Moreover, let \(\hat{\bm{B}}_{k}=\bm{B}_{k}\bm{M}^{-1},k\in[K]\), so that \(\epsilon=\hat{\bm{B}}_{k}\bm{v}\) in the \(k\)-th environment. Then for all nodes \(i\in[d]\) and \(S\subseteq\mathrm{pa}(i)\cup\{i\}\), we have

\[\dim\mathrm{span}\left\langle\left(\hat{\bm{B}}_{k}^{\top}\bm{e} _{i}\right)_{S}:k\in[K]\right\rangle = \dim\mathrm{span}\left\langle\bm{M}^{-\top}\left(\left(\bm{B}_{ k}^{\top}\bm{e}_{i}\right)_{S}:k\in[K]\right)\right\rangle\] \[= \dim\mathrm{span}\left\langle\left(\bm{B}_{k}^{\top}\bm{e}_{i} \right)_{S}:k\in[K]\right\rangle=\left|\mathrm{pa}_{\mathcal{G}}(i)\right|+1,\]

implying that \(\hat{\bm{B}}_{k},k\in[K]\) satisfy Assumption 5.

Now we have shown that for almost every \(\bm{M}\in\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\), we can construct a hypothetical data generating process with latent variables \(\bm{v}=\bm{M}\bm{z}\) that satisfies all requirements in Theorem 9. Choose an arbitrary \(\bm{M}\) that is in \(\mathcal{M}_{\mathrm{sur}}(\mathcal{G})\), then we have that

\[\frac{\partial\bm{v}_{i}}{\partial\bm{z}_{j}}\neq 0,\quad j\notin\overline{ \mathrm{sur}}_{\mathcal{G}}(i).\]

Finally, if we additionally assume single-node interventions, \(\forall k,\ell\in\mathfrak{E}_{i}\), we have that \((\bm{B}_{k})_{j}\neq(\bm{B}_{\ell})_{j}\Leftrightarrow j=i\). For any \(\bm{M}\in\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\) (and specifically the \(\bm{M}\) that we have already chosen above), we have \((\hat{\bm{B}}_{k})_{j}=(\bm{B}_{k})_{j}\bm{M}^{-1}\) and \((\hat{\bm{B}}_{\ell})_{j}=(\bm{B}_{\ell})_{j}\bm{M}^{-1},\forall j\in[d]\). Thus, \((\hat{\bm{B}}_{k})_{j}\neq(\hat{\bm{B}}_{\ell})_{j}\Leftrightarrow j=i\) as well, implying that \(\mathfrak{E}_{i}\) is also a group of single-node interventions on \(\bm{v}\), concluding the proof.

### Formal statement and proof of Theorem 10: the non-parametric case

**Theorem 10** (Counterpart to Theorem 7).: _For any causal model \((\bm{h},\mathcal{G})\) and any set of environments \(\mathfrak{E}\), suppose that we have observations \(\left\{P_{\bm{X}}^{E}\right\}_{E\in\mathfrak{E}}\) satisfying Assumption 1:_

\[\forall E\in\mathfrak{E},\bm{z}\sim p_{E}(\hat{\bm{z}})=\prod_{i=1}^{d}p_{i}^{ E}\left(\hat{\bm{z}}_{i}\mid\hat{\bm{z}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right),\bm{x}= \bm{h}^{-1}(\bm{z})\]

_such that_

* _all densities_ \(p_{i}^{E}\) _are continuously differentiable and the joint density_ \(p_{E}\) _is positive everywhere;_
* _the environments are groups of single-node interventions: there exists a partition_ \(\mathfrak{E}=\cup_{i=1}^{d}\mathfrak{E}_{i}\) _such that_ \(\mathcal{I}_{\bm{z}}^{\mathfrak{E}_{i}}=\{i\}\)_;_
* _the intervention distributions on each node are non-degenerate:_ \(\forall i\in[d]\)_, the set of distributions_ \(\left\{p_{i}^{E}:E\in\mathfrak{E}_{i}\right\}\) _satisfy Definition_ 11 _at any point_ \(\hat{\bm{z}}\in\mathbb{R}^{d}\)_,_

_then there must exist a candidate solution \((\hat{\bm{h}},\mathcal{G})\) and a hypothetical data generating process_

\[\forall E\in\mathfrak{E},\bm{v}\sim q_{E}(\hat{\bm{v}})=\prod_{i=1}^{d}q_{i}^{ E}\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{\mathrm{pa}_{\mathcal{G}}(i)}\right),\bm{x}= \hat{\bm{h}}^{-1}(\bm{v})\]

_such that_

* _all densities_ \(q_{i}^{E}\) _are continuously differentiable and the joint density_ \(q_{E}\) _is positive everywhere;_
* _for the same partition_ \(\mathfrak{E}=\cup_{i=1}^{d}\mathfrak{E}_{i}\)_, we have_ \(\mathcal{I}_{\bm{v}}^{\mathfrak{E}_{i}}=\{i\}\)_;_
* \(\forall i\in[d]\)_, the set of distributions_ \(\left\{q_{i}^{E}:E\in\mathfrak{E}_{i}\right\}\) _satisfy Definition_ 11 _at any point_ \(\hat{\bm{v}}\in\mathbb{R}^{d}\)_but_

\[\frac{\partial\bm{v}_{i}}{\partial\bm{z}_{j}}\neq 0,\quad\forall j\in\overline{ \operatorname{s\overline{ur}}}_{\mathcal{G}}(i).\]

**Remark 3**.: _Similar to the case of Theorem 9, Appendix J.3 also establishes a stronger form of identifiability. First, it is assumed that the causal graph \(\mathcal{G}\) is known. Second, we only focus on a special case of the setting of Theorem 7 by assuming that the support is the whole space, and the non-degeneracy condition Definition 11 holds at any point. Even in this case, we show that our identification guarantee up to SNA cannot be improved._

We state and prove a stronger version of Theorem 10:

**Theorem 11**.: _For any causal model \((\bm{h},\mathcal{G})\) and any set of environments \(\mathfrak{E}\), suppose that we have observations \(\left\{P_{\mathfrak{X}}^{E}\right\}_{E\in\mathfrak{E}}\) satisfying Assumption 1:_

\[\forall E\in\mathfrak{E},\quad\bm{z}\sim p_{E}(z)=\prod_{i=1}^{d}p_{i}^{E} \left(z_{i}\mid z_{\operatorname{pa}_{\mathcal{G}}(i)}\right),\quad\bm{x}=\bm {h}^{-1}(\bm{z})\]

_such that_

* _all densities_ \(p_{i}^{E}\) _are continuously differentiable and the joint density_ \(p_{E}\) _is positive everywhere;_
* _the environments are groups of single-node interventions: there exists a partition_ \(\mathfrak{E}=\cup_{i=1}^{d}\mathfrak{E}_{i}\) _such that_ \(\mathcal{I}_{\bm{x}}^{\bm{\xi}_{i}}=\{i\}\)_;_
* _the intervention distributions on each node are non-degenerate:_ \(\forall i\in[d]\)_, the set of distributions_ \(\left\{p_{i}^{E}:E\in\mathfrak{E}_{i}\right\}\) _satisfy Definition_ 11_,_

_then there must exist a candidate solution \((\hat{\bm{h}},\mathcal{G})\) and a hypothetical data generating process_

\[\forall E\in\mathfrak{E},\quad\bm{v}\sim q_{E}(v)=\prod_{i=1}^{d}q_{i}^{E} \left(v_{i}\mid v_{\operatorname{pa}_{\mathcal{G}}(i)}\right),\quad\bm{x}= \hat{\bm{h}}^{-1}(\bm{v})\]

_such that_

* _all densities_ \(q_{i}^{E}\) _are continuously differentiable and the joint density_ \(q_{E}\) _is positive everywhere;_
* _for the same partition_ \(\mathfrak{E}=\cup_{i=1}^{d}\mathfrak{E}_{i}\)_, we have_ \(\mathcal{I}_{\bm{v}}^{\bm{\xi}_{i}}=\{i\}\)_;_
* \(\forall i\in[d]\)_, the set of distributions_ \(\left\{q_{i}^{E}:E\in\mathfrak{E}_{i}\right\}\) _satisfy Definition_ 11_,_

_but_

\[\frac{\partial\bm{v}_{i}}{\partial\bm{z}_{j}}\neq 0,\quad\forall j\in \overline{\operatorname{s\overline{ur}}}_{\mathcal{G}}(i).\]

_Finally, if we additionally assume minimality (Assumption 6) and/or faithfulness (Assumption 7) of all \(p_{E}\)'s, we can guarantee the existence of \((\hat{\bm{h}},\mathcal{G})\) and \(q_{E}\)'s satisfying minimality and/or faithfulness in addition to the properties listed above. In other words, assuming minimality and/or faithfulness does not resolve the ambiguity._

Proof.: We define

\[\bm{v}=\bm{M}\bm{z}\] (29)

where \(\bm{M}\) is an effect-respecting matrix. At this point we do not make any other restrictions on \(\bm{M}\), and we will choose appropriate \(\bm{M}\) later. By Lemma 20, there exists invertible matrices \(\bm{M}_{i}\) and \(\bm{M}_{i}^{-}\) such that \(\bm{v}_{\operatorname{pa}_{\mathcal{G}}(i)}=\bm{M}_{i}^{-}\bm{z}_{\operatorname {pa}_{\mathcal{G}}(i)}\) and \(\bm{v}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i)}=\bm{M}_{i}\bm{z}_{ \overline{\operatorname{pa}}_{\mathcal{G}}(i)}\), so for all environment \(E\in\mathfrak{E}\) we have

\[q_{i}^{E}(\bm{v}_{\operatorname{pa}_{\mathcal{G}}(i)})=p_{i}^{E}(\bm{z}_{ \operatorname{pa}_{\mathcal{G}}(i)})\cdot\left|\det(\bm{M}_{i}^{-})^{-1} \right|,\quad q_{i}^{E}(\bm{v}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i )})=p_{i}^{E}(\bm{z}_{\overline{\operatorname{pa}}_{\mathcal{G}}(i)})\cdot \left|\det(\bm{M}_{i})^{-1}\right|\]

so that

\[q_{i}^{E}\left(\bm{v}_{i}\mid\bm{v}_{\operatorname{pa}_{\mathcal{G}}(i)} \right)=p_{i}^{E}\left(\bm{z}_{i}\mid\bm{z}_{\operatorname{pa}_{\mathcal{G}}(i )}\right)\frac{\left|\det\bm{M}_{i}^{-1}\right|}{\left|\det(\bm{M}_{i}^{-})^{- 1}\right|},\quad\forall i\in[d].\] (30)In the following, assuming that \(\left(p_{i}^{E}:E\in\mathfrak{E}\right)\) satisfies any of the listed assumptions, we show that \(\left(q_{i}^{E}:E\in\mathfrak{E}\right)\) satisfies the same assumption as well.

Firstly, (30) immediately implies that the density of \(\bm{v}\) is continuous differentiable and positive everywhere. Secondly, \(\forall k,\ell\in\mathfrak{E}_{i}\), we have that

\[p_{j}^{E_{k}}\left(\bm{z}_{j}\mid\bm{z}_{\mathrm{pa}_{\mathcal{Q}}(j)}\right)=p _{j}^{E_{\ell}}\left(\bm{z}_{j}\mid\bm{z}_{\mathrm{pa}_{\mathcal{Q}}(j)}\right) \Leftrightarrow j=i.\]

By (30) it is easy to see that

\[q_{j}^{E_{k}}\left(\bm{v}_{j}\mid\bm{v}_{\mathrm{pa}_{\mathcal{Q}}(j)}\right) =q_{j}^{E_{\ell}}\left(\bm{v}_{j}\mid\bm{v}_{\mathrm{pa}_{\mathcal{Q}}(j)} \right)\Leftrightarrow j=i\]

as well, _i.e.,_\(q^{k},k\in\mathfrak{E}_{i}\) are single-node interventions on \(\bm{v}_{i}\) according to Definition 2.

Thirdly, we verify the non-degeneracy condition for \(q_{i}^{E_{i}}\)'s. Indeed we have for \(\forall k\geqslant 2\) that

\[\nabla_{\bm{v}_{\overline{\mu}\mathbb{E}_{\mathcal{Q}}(i)}}\frac{q_{i}^{E_{1} }}{q_{i}^{E_{k}}}\left(\bm{v}_{i}\mid\bm{v}_{\mathrm{pa}_{\mathcal{Q}}(i)} \right)=\frac{\partial\bm{z}_{\overline{\mu}\mathbb{E}_{\mathcal{Q}}(i)}}{ \partial\bm{v}_{\overline{\mu}\mathbb{E}_{\mathcal{Q}}(i)}}\nabla_{\bm{z}_{ \overline{\mu}\mathbb{E}_{\mathcal{Q}}(i)}}\frac{q_{i}^{E_{1}}}{q_{i}^{E_{k} }}\left(\bm{z}_{i}\mid\bm{z}_{\mathrm{pa}_{\mathcal{Q}}(i)}\right)=\bm{M}_{i}^ {-1}\nabla_{\bm{z}_{\overline{\mu}\mathbb{E}_{\mathcal{Q}}(i)}}\frac{q_{i}^{E _{1}}}{q_{i}^{E_{k}}}\left(\bm{z}_{i}\mid\bm{z}_{\mathrm{pa}_{\mathcal{Q}}(i) }\right).\]

Since \(\bm{M}_{i}\) is invertible, the above equation and the non-degeneracy of \(p^{E_{k}},k\in[K]\) immediately implies that non-degeneracy of \(q^{E_{k}},k\in[K]\).

Thus, for arbitrary \(\bm{M}\in\mathcal{M}_{\mathrm{sur}}(\mathcal{G})\), we have constructed a hypothetical data generating process with latent variable \(\bm{v}=\bm{M}\bm{z}\) that satisfies all given conditions. It remains to show that such construction is still possible under additional minimality and faithfulness conditions.

**Claim 1. There exists a neighbourhood \(O\) of the identity matrix \(\bm{I}\) in \(\overline{\mathcal{M}}_{\mathrm{sur}}(\mathcal{G})\) (in the sense of Remark 1) such that for \(\forall\bm{M}\in O\cap\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\), \(p^{E_{k}},k\in[K]\) satisfy Assumption 7\(\Rightarrow q^{E_{k}},k\in[K]\) satisfy Assumption 7.**

For \(\forall i,j\) not \(d\)-separated by \(S\subseteq[d]\), for all \(k\in[K]\) there exists \(\hat{\bm{z}}\in\mathbb{R}^{d}\) such that \(\Delta_{k}^{(i,j,S)}=p^{E_{k}}\left(\hat{\bm{z}}_{i},\hat{\bm{z}}_{j}\mid\hat {\bm{z}}_{S}\right)-p^{E_{k}}\left(\hat{\bm{z}}_{i}\mid\hat{\bm{z}}_{S}\right) p^{E_{k}}\left(\hat{\bm{z}}_{j}\mid\hat{\bm{z}}_{S}\right)\neq 0\). By continuous differentiability of \(p^{E_{k}}\), we know that there exists \(\delta_{k}^{(i,j,S)}>0\) such that for all \(\bm{M}\in\overline{\mathcal{M}}_{\mathrm{sur}}(\mathcal{G})\) such that \(\left\|\bm{M}-\bm{I}\right\|_{F}\leqslant\delta_{k}^{(i,j,S)}\), the density of the variable \(\bm{v}=\bm{M}\bm{z}\) satisfies \(q^{E_{k}}\left(\hat{\bm{v}}_{i},\hat{\bm{v}}_{j}\mid\hat{\bm{v}}_{S}\right) \neq q^{E_{k}}\left(\hat{\bm{v}}_{i}\mid\hat{\bm{v}}_{S}\right)q^{k}\left( \hat{\bm{v}}_{j}\mid\hat{\bm{v}}_{S}\right)\) for \(\hat{\bm{v}}=\bm{M}\hat{\bm{z}}\), which implies that \(\bm{v}_{i}\) and \(\bm{v}_{j}\) are dependent given \(\bm{v}_{S}\). Now choose \(\delta=\min_{k,i,j,S}\delta_{k}^{(i,j,S)}>0\), then for all \(\bm{M}\in\overline{\mathcal{M}}_{\mathrm{sur}}(\mathcal{G})\) such that \(\left\|\bm{M}-\bm{I}\right\|_{F}\leqslant\delta\), the resulting distributions \(q^{E_{k}},k\in[K]\) satisfy assumption Assumption 6.

**Claim 2. There exists a neighbourhood \(O\) of \(\bm{I}\) in \(\overline{\mathcal{M}}_{\mathrm{sur}}(\mathcal{G})\) (in the sense of Remark 1) such that for almost all \(\bm{M}\in O\cap\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\), \(p^{E_{k}},k\in[K]\) satisfies Assumption 6\(\Rightarrow p^{E_{k}},k\in[K]\) satisfies Assumption 6.**

The proof is similar to the previous statement. Since Assumption 6 causal minimality is satisfied for \(\bm{z}\), for \(\forall k\in[K],i\in[d]\), let \(\mathcal{G}_{ij}\) be the resulting graph obtained by removing the edge \(j\to i\) from \(\mathcal{G}\), then there must exists some \(\alpha_{ijk}\in[d]\) such that

\[\bm{z}_{\alpha_{ijk}}\perp\bm{z}_{\mathrm{nd}_{\mathcal{G}_{ij}}(\alpha_{ijk})} \mid\bm{z}_{\mathrm{pa}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\mid\bm{z}_{\mathrm{ pa}_{\mathcal{G}_{ij}}(\alpha_{ijk})}^{ijk}.\]

Hence, there exists \(\hat{\bm{z}}^{ijk}\in\mathbb{R}^{d}\) such that

\[p^{E_{k}}\left(\hat{\bm{z}}^{ijk}_{\alpha_{ijk}}\mid\hat{\bm{z}}^{ijk}_{ \mathrm{pa}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\right)p^{E_{k}}\left(\hat{\bm{z}}^{ ijk}_{\mathrm{nd}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\mid\hat{\bm{z}}^{ijk}_{\mathrm{pa}_{ \mathcal{G}_{ij}}(\alpha_{ijk})}\right)\neq p^{E_{k}}\left(\hat{\bm{z}}^{ijk}_{ \mathrm{nd}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\mid\hat{\bm{z}}^{ijk}_{\mathrm{pa}_{ \mathcal{G}_{ij}}(\alpha_{ijk})}\right).\]

By continuous differentiability of \(p^{E_{k}}\), there exists \(\delta_{k}^{(i,j)}>0\) such that for all \(\bm{M}\in\bar{\mathcal{M}}_{\mathrm{sur}}(\mathcal{G})\) such that \(\left\|\bm{M}-\bm{I}\right\|_{F}\leqslant\delta_{k}^{(i,j)}\), the density \(q_{ij}^{E_{k}}\) of the variable \(\hat{\bm{v}}^{ijk}=\bm{M}\hat{\bm{z}}^{ijk}\) satisfies

\[q^{E_{k}}\left(\hat{\bm{v}}^{ijk}_{\alpha_{ijk}}\mid\hat{\bm{v}}^{ijk}_{\mathrm{pa} _{\mathcal{G}_{ij}}(\alpha_{ijk})}\right)q^{E_{k}}\left(\hat{\bm{v}}^{ijk}_{ \mathrm{nd}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\mid\hat{\bm{v}}^{ijk}_{\mathrm{ pa}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\right)\neq q^{E_{k}}\left(\hat{\bm{v}}^{ijk}_{ \mathrm{nd}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\mid\hat{\bm{v}}^{ijk}_{\mathrm{ pa}_{\mathcal{G}_{ij}}(\alpha_{ijk})}\right).\]

for \(\hat{\bm{v}}^{ijk}=\bm{M}\hat{\bm{z}}^{ijk}\). This implies that removing the edge \(j\to i\) in \(\mathcal{G}\) would break the causal Markov condition for \(q^{E_{k}}\). Now let \(\delta=\min_{k,i,j}\delta_{k}^{(i,j)}>0\), then for all \(\bm{M}\in\bar{\mathcal{M}}_{\mathrm{sur}}(\mathcal{G})\) such that \(\left\|\bm{M}-\bm{I}\right\|_{F}\leqslant\delta\), the resulting distributions \(q^{E_{k}},k\in[K]\) satisfy assumption Assumption 1.

Combining the above two statements and what we have proven before, it is straightforward to see that one can choose some \(\bm{M}\in\mathcal{M}_{\mathrm{sur}}(

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[NA]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction provide the readers a sense of our main results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We compare with existing works in the introduction and the related work sections.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Rigorous proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We introduce our experimental setup in details. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Code will be released after review. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details are provided. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: we run experiments on \(100\) random causal graphs and report the overall accuracy. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The experiments do not require huge computational resources and can be run on a local computer. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification:Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.