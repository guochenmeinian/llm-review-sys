# Asynchronous Multi-Agent Reinforcement Learning

with General Function Approximation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study multi-agent reinforcement learning (RL) where agents cooperate through asynchronous communications with a central server to learn a shared environment. Our first focus is on the case of multi-agent contextual bandits with general function approximation, for which we introduce the Async-NLin-UCB algorithm. This algorithm is proven to achieve a regret of \(\widetilde{O}(\sqrt{T\dim_{E}(\mathcal{F})\log N(\mathcal{F})})\) and a communication complexity of \(\widetilde{O}(M^{2}\dim_{E}(\mathcal{F}))\), where \(M\) is the total number of agents and \(T\) is the number of rounds, while \(\dim_{E}(\mathcal{F})\) and \(N(\mathcal{F})\) are the Eluder dimension and the covering number of function space \(\mathcal{F}\) respectively. We then progress to the more intricate setting of multi-agent RL with general function approximation, and present the Async-NLSVI-UCB algorithm. This algorithm enjoys a regret of \(\widetilde{O}(H^{2}\sqrt{K\dim_{E}(\mathcal{F})\log N(\mathcal{F})})\) and a communication complexity of \(\widetilde{O}(HM^{2}\dim_{E}(\mathcal{F}))\), where \(H\) is the horizon length and \(K\) the number of episodes. Our findings showcase the provable efficiency of both algorithms for collaborative learning within nonlinear environments and minimal communication overhead.

## 1 Introduction

Multi-agent reinforcement learning (RL) is an important paradigm in RL, and has been successfully applied to real-world tasks such as robotics (Williams et al., 2016; Liu et al., 2019; Ding et al., 2020; Liu et al., 2020; Na et al., 2022), games (Vinyals et al., 2017; Berner et al., 2019; Jaderberg et al., 2019; Ye et al., 2020), and control systems (Bazzan, 2009; Yu et al., 2014, 2020; Min et al., 2022; Xu et al., 2023). By learning cooperatively, agents benefit from sharing learning experiences, enabling them to collectively enhance their decision-making capabilities. This collaborative process is usually accomplished through the utilization of a central server, whose task is to aggregate local data and deliver feedback for the agents.

There has been an excellent line of work establishing provably efficient algorithms for multi-agent bandits and RL. However, most existing works are restricted to the synchronous setting, where communications between all agents and the server must happen simultaneously. This is impractical since in many scenarios the availability of agents may vary and be unpredictable. Ideally, communication should be allowed to happen asynchronously to offer the agents more flexibility. He et al. (2022) and Min et al. (2023) studied this setting respectively for _linear contextual bandits_ and _linear Markov Decision Processes_ (MDPs), both of which assumes linearity in the environment, and introduced algorithms with low regret and communication cost. Yet the linear function class is quite limited, and does not encompass practical reinforcement learning scenarios where nonlinearity is prevalent.

To address the aforementioned drawback, in this work, we tackle environments with general function approximation, broadening the applicability of the algorithm to more realistic and complex scenarios. We first delve into multi-agent contextual bandits with general function approximation, where multiple agents interact with homogeneous environments in parallel to solve a common objective. Notably, the communication protocol is designed to be flexible and asynchronous, allowing agents to initiate communication with the server and acquire new policy functions whenever the need arises. The primary objective is to minimize total regret while reducing communication cost as much as possible.

We propose an algorithm Async-NLin-UCB, which adapts a fully asynchronous communication protocol, and leverages various methods for tackling nonlinear function approximation. Despite the flexibility of communication, our algorithm performs almost as well as a single agent, in terms of a regret that is mostly independent of the number of agents and a low communication cost.

We then progress to multi-agent RL with general function approximation under similar requirements and objectives. We propose an algorithm named Async-NLSVI-UCB based on Least-Squares Value Iteration (LSVI) to learn the underlying Markov decision processes (MDPs), which demonstrates similar advantages with provably low regret and communication cost.

Our main contributions are summarized in the following:

* For asynchronous multi-agent nonlinear contextual bandits, we propose the algorithm Async-NLin-UCB, which enjoys an \(\widetilde{O}(\sqrt{T\dim_{E}(\mathcal{F})\log N(\mathcal{F})}+\dim_{E}( \mathcal{F}))\) regret and an \(\widetilde{O}(M^{2}\dim_{E}(\mathcal{F}))\) communication complexity, where \(\dim_{E}(\mathcal{F})\) and \(N(\mathcal{F})\) are respectively the Eluder dimension and the covering number of function space \(\mathcal{F}\).
* For asynchronous multi-agent nonlinear MDPs, we propose the algorithm Async-NLSVI-UCB, which enjoys an \(\widetilde{O}(H^{2}\sqrt{K\dim_{E}(\mathcal{F})\log N(\mathcal{F})}+H^{2} \dim_{E}(\mathcal{F}))\) regret and a communication complexity of \(\widetilde{O}(HM^{2}\dim_{E}(\mathcal{F}))\).
* At the core of our algorithm, we design a _communication criterion_ in order to tackles the challenges posed by both asynchronous communication and the nonlinearity of function approximation. To guarantee a low communication cost, we propose a low switching communication criterion that allows the agent to trigger communication rounds.
* We carefully design our _download content_ from server to local agents, which consist only of decision and bonus functions, with no mention of any specific historical data. This effectively protects user data against exposure by disallowing local users from obtaining the data of others.

**Notation.** We use lower case letters to denote scalars. We denote by \([n]\) the set \(\{1,\ldots,n\}\). For two positive sequences \(\{a_{n}\}\) and \(\{b_{n}\}\) with \(n=1,2,\ldots\), we write \(a_{n}=O(b_{n})\) if there exists an absolute constant \(C>0\) such that \(a_{n}\leq Cb_{n}\) holds for all \(n\geq 1\). We use \(\widetilde{O}(\cdot)\) to further hide the polylogarithmic factors. For two non-negative integers \(a,b\) satisfying \(a<b\) and a sequence \(\{s_{i}\}\) indexed by integers \(i\), we use \(s_{[a\cdot b]}\) to denote the subsequence \(\{s_{a},s_{a+1},\cdots,s_{b}\}\).

## 2 Related Work

### Multi-Agent Bandits

First, there is a multitude of previous work on distributed or federated multi-armed bandits and stochastic linear bandits (Liu and Zhao, 2010; Szorenyi et al., 2013; Landgren et al., 2016; Chakraborty et al., 2017; Landgren et al., 2018; Martinez-Rubio et al., 2019; Sankararaman et al., 2019; Wang et al., 2020; Ma et al., 2021; Huang et al., 2021). For the more realistic setting of contextual bandits, most previous work are within the scope of linear contextual bandits with synchronized communication. Korda et al. (2016) introduced two novel distributed confidence ball (DCB) algorithms for linear bandit problems in peer-to-peer networks. Wang et al. (2020) considered both P2P and star-shaped communication, achieving near-optimal regret and low communication cost that is largely independent of the time horizon in their algorithm DisLinUCB. Dubey and Pentland (2020) proposed FedUCB, an algorithm focusing on differential-privacy.

Li and Wang (2022) first considered an asynchronous communication protocol and proposed the algorithm Async-LinUCB with near-optimal regret, yet the algorithm contains a download step for all agents triggered by the central server. Their results are flexible and contains a parameter to control the trade-off between regret and communication cost. He et al. (2022) improved the setting to a fully asynchronous communication, proposing the algorithm FedLinUCB with near-optimal regret of \(\widetilde{O}(d\sqrt{T})\) and low communication cost of \(\widetilde{O}(dm^{2})\), comparable to the benchmark in single-agent contextual linear bandits (Abbasi-Yadkori et al., 2011). We consider the same communication protocol in our results. A summary of these results along with ours can be found in the first four rows of Table 1.

### Multi-Agent RL

Multi-agent reinforcement learning is decidedly more challenging than contextual bandits. There is also a vast literature on this setting, with many works discussing different aspects of multi-agent RLthan ours. For example, there are works focusing on convergence guarantees (Zhang et al., 2018, 2018, 2018), non-stationary or heterogeneous environments (Lowe et al., 2017; Yu et al., 2021; Dubey and Pentland, 2021; Kuba et al., 2022; Liu et al., 2022; Jin et al., 2022), and deep federated RL (Clemente et al., 2017; Espeholt et al., 2018; Horgan et al., 2018; Nair et al., 2015; Zhuo et al., 2019), to name a few. We refer to a recent survey on federated reinforcement learning Qi et al. (2021) for a more comprehensive summary.

Narrowing it down to multi-agent RL with function approximation, the benchmark is the LSVI-UCB algorithm in the single-agent setting (Jin et al., 2020), with an \(\widetilde{O}(d^{3/2}H^{2}\sqrt{K})\) regret. Dubey and Pentland (2021) proposed CoopLSVI for multi-agent linear MDPs, which requires a synchronized communication through central server, and proves a regret of \(\widetilde{O}(d^{3/2}H^{2}\sqrt{MK})\). They also extended their result to the heterogeneous setting. Min et al. (2023) considered the fully asynchronous setting and introduced the Async-Coop-LSVI-UCB algorithm, with a \(\widetilde{O}(d^{3/2}H^{2}\sqrt{K})\) regret not dependent on the number of agents \(M\), as well as a low communication cost. A summary of these results along with ours can be found in the last three rows of Table 1.

### General function approximation

Reinforcement learning with general function approximation extends the well-studied case of linear MDPs to more general classes of MDPs, and has gained a lot of traction in recent years (Wang et al., 2020; Jin et al., 2021; Foster et al., 2023; Du et al., 2021; Agarwal and Zhang, 2022; Agarwal et al., 2023). Previous works focus on different measures of complexity for the function classes, for example the Bellman rank proposed by Jiang et al. (2017), the Bellman Eluder dimension introduced in Jin et al. (2021), the Decision-Estimation Coefficient in Foster et al. (2023), and generalized Eluder dimension in Agarwal et al. (2023). Our work considers the Eluder dimension with the introduction of uncertainty estimators \(D^{2}\), which has been widely utilized to establish results in RL with general function approximation (Agarwal et al., 2023; Zhao et al., 2023; Ye et al., 2023; Di et al., 2023).

## 3 Preliminaries

In this section, we introduce the formal definition of both multi-agent nonlinear contextual bandits and MDPs and some related concepts, and discuss the asynchronous communication protocol.

### Multi-Agent Contextual Bandits with General Function Approximation

We assume a global action set \(\mathcal{A}\) that is known to all agents. At each round \(t\in[T]\), a single arbitrary agent \(m_{t}\in[M]\) is chosen to participate. The agent receives a contextual decision set \(\mathcal{A}_{t}\subseteq\mathcal{A}\) and chooses from the set an action \(a_{t}\in\mathcal{A}_{t}\) to perform, and subsequently receives a random reward \(r_{t}\)

\begin{table}
\begin{tabular}{c c c c} \hline \hline Algorithm & Regret & Communication & 
\begin{tabular}{c} Fully \\ asynchronous \\ \end{tabular} \\ \hline DisLinUCB & \(d\sqrt{MT}\log^{2}T\) & \(d^{3}M^{3/2}\) & ✗ \\ \hline Async-LinUCB & \(dM^{(1-\gamma)/2}\sqrt{T}\log T\) & \(dM^{1+\gamma}\log T\) & ✗ \\ \hline FedLinUCB & & \(d\sqrt{T}\log T\) & \(dM^{2}\log T\) & ✓ \\ \hline Async-NLin-UCB & \(\sqrt{\dim_{E}\log NT}\log T\) & \(\dim_{E}M^{2}\log^{2}T\) & ✓ \\ (ours) & \(\sqrt{\dim_{E}\log NT}\log T\) & \(\dim_{E}M^{2}\log^{2}T\) & ✓ \\ \hline Coop-LSVI & & & \\ [Dubey and Pentland, 2021] & \(d^{3/2}H^{2}\sqrt{MK}\log K\) & \(dHM^{3}\) & ✗ \\ \hline Async-Coop-LSVI-UCB & \(d^{3/2}H^{2}\sqrt{K\log K}\) & \(dHM^{2}\log K\) & ✓ \\ \hline Async-NLSVI-UCB & & & \\ (ours) & \(\sqrt{\dim_{E}\log N}H^{2}\sqrt{K}\log K\) & \(\dim_{E}HM^{2}\log^{2}K\) & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of our result against baseline methods for multi-agent contextual bandits and MDPs. Note that the first four rows are for contextual bandits, and the last three are for reinforcement learning. Only our algorithms are in the general function approximation setting. We abbreviate \(\dim_{E}=\dim_{E}(\mathcal{F})\) and \(N=N(\mathcal{F})\), and hide logarithmic factors. For algorithms with synchronized communication, each communication round actually corresponds to \(M\) rounds in asynchronous settings, which explains the extra \(M\) terms.

The assumption of general function approximation is that the reward is generated according to

\[r_{t}=f^{*}(a_{t})+\eta_{t},\] (1)

where \(f^{*}\) is the ground truth objective function, and \(\eta_{t}\) is a random noise variable. We assume the the objective function lies within a known function class \(\mathcal{F}\). In addition, we also make the following assumptions regarding the function class and noise variables, which are standard assumptions for contextual bandits (Abbasi-Yadkori et al., 2011; He et al., 2022):

**Assumption 3.1**.: Suppose the following conditions hold for the contextual bandits environment:

* For any \(f\in\mathcal{F}\) and \(a\in\mathcal{A}\), \(|f(a)|\leq 1\);
* \(\eta_{t}\) is \(R\)-sub-Gaussian conditioned on data history: \(\mathbb{E}\big{[}e^{\lambda\eta_{t}}\big{|}a_{1:t},m_{1:t},r_{1:t-1}\big{]} \leq\exp(R^{2}\lambda^{2}/2),\forall\lambda\).

**Learning Objective.** The primary goal of contextual bandits is to minimize the cumulative regret

\[\text{Reg}(T)=\sum_{t=1}^{T}[f^{*}(a_{t})-\max_{a\in\mathcal{A}_{t}}f^{*}(a)].\]

Notice that this summation is across all time steps does not depend on agent participation order, as should be the case for the resulting regret bound. To achieve this goal, agents are allowed to communicate with the server to upload their interaction history and update their policy. The secondary learning objective is to reduce communication overhead. We will explain the communication protocol further in Section 3.4.

### Multi-Agent Episodic MDPs with General Function Approximation

We consider episodic MDPs, which are a classic family of models in reinforcement learning (Sutton and Barto, 2018). It is characterized by the following elements, which we assume to be homogeneous across all agents: a state space \(\mathcal{S}\), an action space \(\mathcal{A}\), the horizon length \(H\), transition probability functions \(\mathbb{P}=\{\mathbb{P}_{h}(\cdot|\cdot,\cdot)\}_{h=1}^{H}\) and reward functions \(\{r_{h}(\cdot,\cdot)\}_{h=1}^{H})\). Similar to the bandit case, for each episode \(k=1,\cdots,K\), a single agent \(m=m_{k}\) is chosen to participate. An episode \(k\) begins with an initial state \(s_{1}^{*}\), which is drawn from an unknown fixed distribution. Then for steps \(h=1,\cdots,H\), the participating agent \(m\) selects an action \(a_{h}^{k}\) based on the observed state \(s_{h}^{k}\). After each action, the agent receives a reward \(r_{h}^{k}=r_{h}(s_{h}^{k},a_{h}^{k})\), where \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the reward function at step \(h\). Here for the sake of convenience, we assume the reward function to be deterministic, but it is not difficult to generalize our result to stochastic rewards. We also assume \(r_{h}(s,a)\in[0,1]\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\) without loss of generality. The environment then transitions to the next state according to \(s_{h+1}^{k}\sim\mathbb{P}_{h}(\cdot|s_{h}^{k},a_{h}^{k})\), where \(\mathbb{P}_{h}\) is the transition probability at step \(h\). The episode terminates when \(r_{H}\) is observed.

The strategy an agent employs to interact with the environment is called the agent's _policy_, which can be described by a set of decision functions \(\pi=\{\pi_{h}\}_{h=1}^{H}\), where \(\pi_{h}:\mathcal{S}\rightarrow\mathcal{A}\) is the decision function at level \(h\), mapping the current state to an action to select.

**Value Functions.** For any policy \(\pi=\{\pi_{h}\}\), we define \(Q\)-value functions and \(V\)-value functions:

\[Q_{h}^{\pi}(s_{h},a_{h}):=\mathbb{E}\bigg{[}\sum_{h^{\prime}=h}^{H}r_{h^{ \prime}}(s_{h^{\prime}},a_{h^{\prime}})\bigg{|}s_{h},a_{h}\bigg{]},\quad V_{h }^{\pi}(s_{h}):=\mathbb{E}\bigg{[}\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(s_{h^{ \prime}},a_{h^{\prime}})\bigg{|}s_{h}\bigg{]},\] (2)

where the expectation is taken over the trajectory \((s_{1},a_{1},\cdots,s_{h},a_{h})\), determined by the transition probability functions \(\mathbb{P}\) and policy \(\pi\). The optimal strategy \(\pi^{*}\) is the maximizer of the value functions:

\[\pi^{*}:=\operatorname*{argmax}_{\pi}V_{h}^{\pi}(s_{1}),\forall s_{1}.\]

We also have optimal value functions \(Q_{h}^{*}:=Q_{h}^{\pi^{*}}\) and \(V_{h}^{*}:=V_{h}^{\pi^{*}}\), which satisfy Bellman equations

\[Q_{h}^{*}(s_{h},a_{h})=r_{h}(s_{h},a_{h})+\mathbb{E}\big{[}V_{h+1}^{*}(s_{h+1} )\big{|}s_{h},a_{h}\big{]},\quad V_{h}^{*}(s_{h})=\max_{a\in\mathcal{A}}Q_{h}^ {*}(s_{h},a).\] (3)

**Function Approximation.** We approximate \(Q\)-value functions with function classes \(\{\mathcal{F}_{h}\}_{h=1}^{H}\), which contain real value functions with domain \(\mathcal{S}\times\mathcal{A}\). One basic assumption is that \(Q_{h}^{*}\in\mathcal{F}_{h}\) for all steps \(h\in[H]\). Now with the convention that functions at level \(H+1\) are uniformly zero, i.e., \(f_{H+1}=0\), we define the Bellman operator \(\mathcal{T}_{h}\):

\[(\mathcal{T}_{h}f_{h+1})(s_{h},a_{h}):=\mathbb{E}\big{[}r_{h}(s_{h},a_{h})+f_{h +1}(s_{h+1})\big{|}s_{h},a_{h}\big{]},\]

and we expect \(\mathcal{T}_{h}\) to map any function in \(\mathcal{F}_{h+1}\) to a function in \(\mathcal{F}_{h}\), i.e., \(\mathcal{T}_{h}\mathcal{F}_{h+1}\subseteq\mathcal{F}_{h}\). This is called the completeness assumption, which is a fundamental assumption in RL with general function approximation (Wang et al., 2020; Jin et al., 2021).

**Learning Objective.** The primary goal in multi-agent MDPs is to minimize the cumulative regret over \(K\) episodes

\[\text{Reg}(K)=\sum_{k=1}^{K}\big{[}V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi_{m,k}}(s_{1}^{ k})\big{]},\]

where \(\pi_{m,k}\) is the policy of agent \(m=m_{k}\) at round \(k\), while the secondary objective is to minimize the communication cost.

### Eluder Dimension and Covering Number

To measure the complexity of the learning objective, Russo and Van Roy (2013) first proposed the concept of Eluder dimension, which we define below.

**Definition 3.2** (\(\epsilon\)-dependence).: For a function class \(\mathcal{F}\) on domain \(\mathcal{D}\), a point \(z\in\mathcal{D}\) is _\(\epsilon\)-dependent_ on \(\mathcal{Z}\subseteq\mathcal{D}\) if, for any \(f_{1},f_{2}\in\mathcal{F}\) satisfying \(\sqrt{\sum_{z^{\prime}\in\mathcal{Z}}\big{(}f_{1}(z^{\prime})-f_{2}(z^{\prime })\big{)}^{2}}\leq\epsilon\), it must hold that \(|f_{1}(z)-f_{2}(z)|\leq\epsilon\). Accordingly, \(z\) is _\(\epsilon\)-independent_ of \(\mathcal{Z}\) if it is not \(\epsilon\)-dependent on \(\mathcal{Z}\).

**Definition 3.3** (Eluder dimension).: The _\(\epsilon\)-Eluder dimension_\(\dim_{E}(\mathcal{F},\epsilon)\) is the length of the longest sequence of elements in \(\mathcal{D}\) satisfying that, for some \(\epsilon_{0}>\epsilon\), each element is \(\epsilon_{0}\)-independent of the set consisting of its predecessors.

It has been demonstrated that the Eluder dimension roughly corresponds to regular dimension concepts in linear and quadratic cases (Russo and Van Roy, 2013), and that the Eluder family is strictly larger than the generalized linear class (Li et al., 2022). Note that our Eluder definition can be applied to either the contextual bandit case with \(\mathcal{D}=\mathcal{A}\) or the MDPs case with \(\mathcal{D}=\mathcal{S}\times\mathcal{A}\).

We also introduce covering number for function classes (Wainwright, 2019) in the following:

**Definition 3.4** (Covering number).: An _\(\epsilon\)-cover_ of \(\mathcal{F}\) is any subset \(\mathcal{F}_{\epsilon}\subseteq\mathcal{F}\) such that for any \(f\in\mathcal{F}\), there exists \(f^{\prime}\in\mathcal{F}_{\epsilon}\) that \(\|f-f^{\prime}\|_{\infty}\leq\epsilon\). The _covering number_ of \(\mathcal{F}\), denoted by \(N(\mathcal{F},\epsilon)\), is the minimal cardinality of its \(\epsilon\)-cover.

### Communication Protocol

We consider a star-shaped communication model (He et al., 2022; Min et al., 2023), where the agents communicate through a central server to collaborate. To ensure asynchronous communication, we mandate that all communications must be initiated by a participating agent. Specifically, at the end of a time step / episode, the agent will decide whether or not to trigger a communication round. If so, the agent uploads its local data history and receives some global data for future decision making. The _communication cost_ is the total number of communication rounds initiated by the agents.

One variability is the form of global data that the communicating agent downloads from server. It may be tempting to have the server send all its stored trajectories to the agent for future decision making, but this will unnecessarily expose other agents' data to the current participating agent. We will come back to this issue and our solution in Section 4.2.

## 4 Multi-Agent Contextual Bandits

In this section, we introduce the Asynchronous Nonlinear UCB (Async-NLin-UCB) algorithm designed for multi-agent contextual bandits with general function approximation, and provide a theoretical result for its regret and communication cost.

### Algorithm: Async-NLin-UCB

Algorithm 1 takes as input the total number of time steps \(T\), regularization parameter \(\lambda\), communication parameter \(\alpha\) and exploration radii \(\{\beta_{t}\}_{t=1}^{T}\).

In the algorithm, there are some variables that go through different versions as \(t\) progresses through \(1,\cdots,T\). For clarity, here we give them an extra subscript \(t\) to denote the version of that variable before (not included) the least squares calculation on Line 12 at round \(t\).

Throughout the learning process, the server maintains a global history set \(Z_{t}^{\text{ser}}\) that stores action-reward pairs \((a,r)\in\mathcal{A}\times[0,1]\), initialized on Line 2 and updated only during communication rounds. Each local agent \(m\) maintains a decision function \(f_{m,t}\) for taking action, a bonus function \(b_{m,t}\) for checking communication criterion, and a local data history set \(Z_{m,t}^{\text{loc}}\), all initialized on Line 3. Each step of Algorithm 1 contains two parts: local exploration and server updates.

**Part I: Local Exploration.** At step \(t\) a single agent \(m=m_{t}\) is active (Line 5). It receives a decision set, finds the greedy action according to its decision function \(f_{m,t}\), receives a reward, and updates its local dataset \(Z_{m,t}^{\text{loc}}\) (Lines 5 - 7).

After exploration, the agent checks if the switch condition is true using its bonus function:

\[\sum_{(a,r)\in Z^{\text{loc}}_{m,t}}b_{m,t}^{2}(a)/\big{(}\beta_{\ell^{\prime}}^{ 2}+\lambda\big{)}\geq\alpha,\] (4)

where \(t^{\prime}\) is the last time step when agent \(m\) communicated with the server. If so, the agent initiates a communication round and uploads its local data (Line 9), prompting the server to begin global policy updates. We will discuss the reasons behind this switch condition in Section 4.2.

**Part II: Server Updates.** After receiving a new local data history from an agent, the server merges the data into its global dataset \(Z^{\text{ser}}_{t}\) (Line 11), and calculate a function \(\widehat{f}_{t+1}\in\mathcal{F}\) which minimizes the sum of squares error according to the current dataset \(Z^{\text{ser}}_{t}\) (Line 12):

\[\widehat{f}_{t+1}=\operatorname*{argmin}_{f\in\mathcal{F}}\sum_{(a,r)\in Z^{ \text{ser}}_{t}}\big{(}f(a)-r\big{)}^{2}.\] (5)

The next step is to obtain a bonus function \(b_{t+1}\) from the oracle \(\mathcal{B}_{\mathcal{A}}\) from Definition 4.1 (Line 12). We discuss the specifics of this construction in detail up next in Section 4.2. Finally, the server sends the optimistic value function \(\widehat{f}_{t+1}+b_{t+1}\) and the bonus function \(b_{t+1}\) back to agent \(m\) for future exploration and updates; agent \(m\) also resets its local data history to an empty set (Lines 13 and 15).

### Uncertainty Estimators and Bonus Functions

In this section, we introduce uncertainty estimators and bonus functions, and give a detailed explanation for our communication criterion (4). Most of these apply to the MDPs setting as well.

**Uncertainty Estimators.** First we define the uncertainty estimator of new data \(a\) against data history \(Z\), which is considered in many works on bandits and RL with general function approximation (Gentile et al., 2022; Agarwal et al., 2023):

\[D_{\lambda,\mathcal{F}}(a;Z)=\sup_{f_{1},f_{2}\in\mathcal{F}}|f_{1}(a)-f_{2} (a)|\big{/}\sqrt{\lambda+\sum_{(a^{\prime},r)\in Z}|f_{1}(a^{\prime})-f_{2}( a^{\prime})|^{2}},\] (6)

here \(\lambda\) is the regularization parameter, \(\mathcal{F}\) is a function class. Intuitively, the uncertainty estimator measures the difference between functions on new data \(a\) against the difference on historical data \(Z\).

**Switch Condition Based On Uncertainty Estimators.** The determinant-based criterion is a common technique used in contextual bandits and RL with linear function approximation to reduce policy switching or communication cost (Abassi-Yadkori et al., 2011). For nonlinear function approximation, one can use uncertainty estimators to formulate a new form of switch condition:

\[\sum_{(a,r)\in Z^{\text{new}}_{t}}D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{old }}_{t})\geq\alpha.\] (7)

where we use \(Z^{\text{new}}_{t}\) and \(Z^{\text{old}}_{t}\) to denote newly accumulated data and old historical data. This criterion has a similar function as the determinant-based criterion in linear settings. Parameter \(\alpha\) controls communication frequency: smaller \(\alpha\) indicates more frequent communication, more accurate decision functions and smaller regret, thus implying a trade-off between regret and communication cost.

**Bonus Function Oracle.** Next, we introduce bonus functions obtained through oracles that approximate the uncertainty estimators.

**Definition 4.1** (Bonus Function Oracle \(\mathcal{B}_{\mathcal{D}}\)).: Given domain \(\mathcal{D}\), the oracle \(\mathcal{B}_{\mathcal{D}}(Z,\mathcal{F};\lambda,\beta)\) takes the following as inputs: a dataset \(Z\) consisting of a series of data points \((z,e)\), where \(z\in\mathcal{D}\) and \(e\) is some additional data content; function class \(\mathcal{F}\) with functions \(f:\mathcal{D}\rightarrow\mathbb{R}_{\geq 0}\); regularization parameter \(\lambda\) and exploration radius \(\beta\). It returns a function \(b\in\mathcal{W}_{\mathcal{D}}:\mathcal{D}\rightarrow\mathbb{R}_{\geq 0}\) satisfying for any \(z\in\mathcal{D}\) that

* \(b(z)\geq\max\Big{\{}\big{|}f_{1}(z)-f_{2}(z)\big{|}:f_{1},f_{2}\in\mathcal{F}, \sum_{(z,e)\in Z}\big{(}f_{1}(z)-f_{2}(z)\big{)}^{2}\leq\beta^{2}\Big{\}}\);
* \(D_{\lambda,\mathcal{F}}(z;Z)\leq b(z)/\sqrt{\beta^{2}+\lambda}\leq C_{\mathcal{ B}}D_{\lambda,\mathcal{F}}(z;Z)\),

where \(C_{\mathcal{B}}\) is an absolute constant.

_Remark 4.2_.: Similar bonus function oracles have been proposed in previous works (Definition 3 in Agarwal et al. (2023)). The accessibility of these oracles is also supported by previous works that proposed methods to compute bonus functions (Kong et al., 2023; Wang et al., 2020b). In this definition, we leave the domain and data format to be variable so the oracle can be applied to both contextual bandits and MDPs. For bandits, the domain is \(\mathcal{A}\), and the data format has \(z=a\) and \(e=r\). The first property of the bonus function guarantees the optimism of decision functions \(\widehat{f}_{t+1}+b_{t+1}\) (see Lemma 6.1 for MDPs or Lemma A.2 for bandits), while the second property links bonuses to uncertainty estimators.

**Switch Condition Based On Bonus Functions.** If we try to adapt the switch condition (7) in our setting, a local agent will require access to historical data \(Z_{t}^{\text{old}}\) to calculate uncertainty estimators \(D_{\lambda,\mathcal{F}}^{2}(a;Z_{t}^{\text{old}})\). For multi-agent learning, this dataset consists of the collective data from all agents, and giving local agent access is a clear violation of data privacy. Our solution is to let local agents download bonus functions and set communication criterion to (4), using bonus functions instead of uncertainty estimators.

**Decision Functions Based On Bonus Functions.** Another benefit of introducing the bonus function is evident from our exploration method in line 6. A common practice for nonlinear RL algorithms is to construct _confidence sets_ of functions during policy update, and find the optimal function within the confidence sets during exploration (Agarwal et al., 2023; Ye et al., 2023). However, in a multi-agent setting, this would involve the download of confidence sets, which is impractical due to the complex nature of function classes. With the bonus function, local agents need only download the _decision function_ from the server for future exploration, which for contextual bandits is simply \(\widehat{f}_{t+1}+b_{t+1}\).

### Theoretical Results

Our main results for Algorithm 1 are summarized in the following theorem, which provides a regret upper bound and communication complexity order.

**Theorem 4.3**.: _By taking \(\gamma=O(1/T)\), \(\beta_{t}=C_{\beta,1}\big{(}\sqrt{\lambda}+RC(M,\alpha)\log(3MN(\mathcal{F}, \gamma)/\delta)\big{)}\) and \(C(M,\alpha)=\sqrt{1+M\alpha}\big{(}\sqrt{1+M\alpha}+M\sqrt{\alpha}\big{)}\), the regret of Algorithm 1 within \(T\) rounds is_

\[O\Big{(}\sqrt{T}\widetilde{\beta}_{1}\sqrt{(1+M\alpha)\dim_{E}}\log(T/\min\{1,\lambda\})+(1+M\alpha)\dim_{E}\log^{2}(T/\min\{1,\lambda\})\Big{)},\]

_where we abbreviate \(\dim_{E}:=\dim_{E}(\mathcal{F},\lambda/T)\); the total communication complexity is_

\[O\Big{(}(1+M\alpha)^{2}/\alpha\dim_{E}\log^{2}(T/\min\{1,\lambda\})\Big{)}\]

_Remark 4.4_.: When reduced to linear contextual bandits, where \(\dim_{E}(\mathcal{F},\lambda/T)=\widetilde{O}(d)\) and \(\log N(\mathcal{F},\gamma)=\widetilde{O}(d)\), our result on regret correspond exactly to Theorem 5.1 of He et al. (2022), except for an extra \(1+M\alpha\) term in the communication cost, an unimportant term when taking \(\alpha=1/M^{2}\) that comes from the complication of communication cost analysis in nonlinear settings.

## 5 Multi-Agent Reinforcement Learning

In this section, we introduce the Asynchronous Nonlinear Least Squares Value Iteration UCB (Async-NLin-UCB) algorithm for multi-agent MDPs with general function approximation, and a corresponding theoretical result.

### Algorithm: Async-NLSVI-UCB

To better represent the elements in the datasets, we sometimes use \(o_{h}\) to represent the tuple \((s_{h},a_{h},r_{h},s_{h+1})\) and \(z_{h}\) to represent \((s_{h},a_{h})\) when there is no confusion. Similar to the band-dit case, we give some variables an extra subscript \(k\) here for clarity, which denotes the version of the variable before (not included) Line 14 at episode \(k\).

```
1:Input: total number of rounds \(K\), parameters \(\lambda\), \(\alpha\), \(\beta_{k,h}\) for \(k=[K]\) and \(h\in[H]\)
2:Server init: Set \(Z_{h}^{\text{ser}}=\varnothing\) for all \(h\in[H]\).
3:Local init:\(\forall m\in[M]\) and \(h\in[H]\), set \(Q_{m,h}=1\), \(b_{m,h}=\mathcal{B}(\varnothing,\mathcal{F}_{h};\lambda,\beta_{0,h})\), \(Z_{m,h}^{\text{loc}}=\varnothing\).
4:for\(k=1,\dots,K\)do
5: Agent \(m=m_{k}\in[M]\) is active and receives initial state \(s_{1}^{k}\in\mathcal{S}\).
6:for\(h=1,\dots,H\)do
7: Take action \(a_{h}^{k}=\operatorname*{argmax}_{a\in\mathcal{A}}Q_{m,h}(s_{h}^{k},a)\), receive reward \(r_{h}^{k}\) and next state \(s_{h+1}^{k}\).
8: Update \(Z_{m,h}^{\text{loc}}=Z_{m,h}^{\text{loc}}\cup\{(s_{h}^{k},a_{h}^{k},r_{h}^{k}, s_{h+1}^{k})\}\).
9:endfor
10:if switch condition (8) is met then
11: Send new data \(\{Z_{m,h}^{\text{loc}}\}_{h\in[H]}\) to server.
12:on server:
13: Update \(Z_{h}^{\text{ser}}=Z_{h}^{\text{ser}}\cup Z_{m,h}^{\text{loc}}\).
14: Initialize \(Q_{H+1}=V_{H+1}=0\).
15:for\(h=H,H-1,\cdots,1\)do
16: Calculate \(\hat{f}_{h}\) according to (9) and bonus function \(b_{h}=\mathcal{B}_{\mathcal{S}\times\mathcal{A}}(Z_{h}^{\text{ser}},\mathcal{ F}_{h};\lambda,\beta_{k,h})\).
17: Calculate \(Q_{h}\) and \(V_{h}\) according to (11).
18:endfor
19: Send \(\{Q_{h}\}_{h=1}^{H}\) and \(\{b_{h}\}_{h=1}^{H}\) to agent \(m\).
20:endof server
21: Agent \(m\) receives \(Q_{m,h}=Q_{h}\), \(b_{m,h}=b_{h}\) and resets \(Z_{m,h}^{\text{loc}}=\varnothing\) for all \(h\in[H]\).
22:endif
23:endfor ```

**Algorithm 2** Federated Nonlinear MDPs

The server maintains global historical datasets \(Z_{k,h}^{\text{ser}}\) containing sequences of tuples \((s_{h},a_{h},r_{h},s_{h+1})\), initialized in Line 2. Each local agent \(m\) maintains optimistic value functions \(\{Q_{m,k,h}\}_{h=1}^{H}\), bonus functions \(\{b_{m,k,h}\}_{h=1}^{H}\), and local datasets \(\{Z_{m,k,h}^{\text{loc}}\}_{h=1}^{H}\), all initialized in Line 3.

Each episode \(k\) of Algorithm 2 also consists of the two parts local exploration and server updates.

**Part I: Local Exploration.** At step \(k\) an agent \(m=m_{k}\) is active (Line 5). It interacts with the environment by executing the greedy policy according to \(\{Q_{m,k,h}\}_{h=1}^{H}\), obtaining a trajectory \(\{(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_{h+1}^{k})\}_{h=1}^{H}\), which is then stored into the local historical datasets \(Z_{m,k,h}^{\text{loc}}\) (lines 6 - 9).

After exploration, the agent checks for the following switch condition: there exists \(h\in[H]\) so that

\[\sum_{o_{h}\in Z_{m,k,h}^{\text{loc}}}b_{m,k,h}^{2}(s_{h},a_{h})/\big{(}\beta_{ k^{\prime},h}^{2}+\lambda\big{)}\geq\alpha,\] (8)

where \(k^{\prime}\) is the last communication round for \(m\). If so, the agent triggers communication (Line 11).

**Part II: Server Updates.** After receiving new data, the server merges it with its global datasets \(Z_{k,h}^{\text{ser}}\) (Line 13) and calculates value function estimates \(\{Q_{k+1,h}\}_{h=1}^{H}\) and \(\{V_{k+1,h}\}_{h=1}^{H}\) using LSVI.

Suppose we already have \(Q\)- and \(V\)-value function estimates \(Q_{k+1,h+1}\) and \(V_{k+1,h+1}\) at level \(h+1\). We solve the least squares problem for \(\widehat{f}_{h}\) to minimize the Bellman error (Line 16):

\[\widehat{f}_{k+1,h}=\operatorname*{argmin}_{f_{h}\in\mathcal{F}_{h}}\sum_{o_{h} \in Z_{k,h}^{\text{loc}}}\big{(}f_{h}(z_{h})-r_{h}-V_{k+1,h+1}(s_{h+1})\big{)} ^{2}.\] (9)

We now also define the uncertainty estimator of a new pair of data \(z=(s,a)\) against data history \(Z\) with normalization parameter \(\lambda\) and function class \(\mathcal{F}\) as

\[D_{\lambda,\mathcal{F}}(z;Z)=\sup_{f_{1},f_{2}\in\mathcal{F}}\big{|}f_{1}(z)-f _{2}(z)\big{|}\big{/}\sqrt{\lambda+\sum_{o^{\prime}\in Z}|f_{1}(z^{\prime})-f _{2}(z^{\prime})|^{2}}.\] (10)

Similar to the bandits setting, the uncertainty can be approximated with the bonus function acquired from an oracle \(\mathcal{B}_{\mathcal{S}\times\mathcal{A}}\) in Definition 4.1. In this case, the domain \(\mathcal{D}=\mathcal{S}\times\mathcal{A}\), and the data format corresponds to \(z=(s,a)\) and \(e=(r,s^{\prime})\). Despite these definitions not depending on the step \(h\), we expect the parameters \(z,Z,\mathcal{F}\) to always come from same step \(h\). Finally, we allow the bonus function classes \(\mathcal{W}_{h}=\mathcal{W}_{h,\mathcal{S}\times\mathcal{A}}\) to vary between different levels.

After calling oracle for \(b_{k+1,h}\) (Line 16), we can obtain value function estimates (Line 17):

\[Q_{k+1,h}(s,a)=\widehat{f}_{k+1,h}(s,a)+b_{k+1,h}(s,a),\quad V_{k+1,h}(s)=\sup_ {a\in\mathcal{A}}Q_{k+1,h}(s,a).\] (11)

Iterating through \(h=H,\cdots,1\), the server calculates a set of updated \(Q\)-value functions \(\{Q_{k+1,h}\}_{h=1}^{H}\) and bonus functions \(\{b_{k+1,h}\}_{h=1}^{H}\), and send them back to agent \(m\) for future exploration and updates (lines 19 and 21).

### Theoretical Results

We summarize the regret and communication cost of Algorithm 2 in the following theorem:

**Theorem 5.1**.: _Taking \(\gamma=O(1/HK)\), \(\beta_{h,k}=C_{\beta,2}\Big{[}\sqrt{\lambda}+HC(M,\alpha)\sqrt{\log(3HMN(\gamma)/ \delta)}\Big{]}\) and \(N(\gamma):=\max_{h}N(\mathcal{F}_{h},\gamma)N(\mathcal{F}_{h+1},\gamma)N( \mathcal{W}_{h+1},\gamma)\), the regret within \(K\) rounds is bounded by \(O\Big{(}H\widetilde{\beta}_{2}\sqrt{(1+M\alpha)\dim_{E}K}\log(K/\min\{1,\lambda \})+H^{2}(1+M\alpha)\dim_{E}\log^{2}(K/\min\{1,\lambda\})\Big{)}\)._

_where we abbreviate \(\dim_{E}:=\dim_{E}(\mathcal{F},\lambda/K)\); the total communication complexity is_

\[O\big{(}H(1+M\alpha)^{2}\alpha\dim_{E}(\mathcal{F},\lambda/K)\log^{2}(K/\min\{ 1,\lambda\})\big{)}.\]

_Remark 5.2_.: This result when reduced to linear MDPs correspond well to Theorem 5.1 in Min et al. (2023). Taking \(\alpha=1/M^{2}\), we get a regret of \(\widetilde{O}\big{(}H^{2}\sqrt{K\dim_{E}\log N}+H^{2}\dim_{E}\big{)}\) and a communication cost of \(\widetilde{O}\big{(}HM^{2}\dim_{E}\big{)}\), where \(N=\max_{h}\{N(\mathcal{F}_{h},\gamma),N(\mathcal{W}_{h},\gamma)\}\).

## 6 Proof Sketch

In this section, we provide an outline for the proof of Theorem 5.1, while a more detailed proof can be found in Appendix B, and the full versions of the following lemmas are in Appendix B.1.

### Regret Upper Bound

For the regret upper bound, the first lemma establishes optimism of value function estimates.

**Lemma 6.1**.: _Taking \(\beta_{k,h}\) as in Theorem 5.1, with probability at least \(1-\delta\), for all \(k\), \(z\in\mathcal{S}\times\mathcal{A}\) and \(h\in[H]\), \(|\mathcal{T}_{h}Q_{k+1,h+1}(z)-\widehat{f}_{k+1,h}(z)|\leq b_{k+1,h}(z)\)._

This allows us to decompose regret into a sum of bonuses:

\[\text{Reg}(K)=\sum_{k=1}^{K}\big{[}V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi_{m,h}}(s_{ 1}^{k})\big{]}\]

\[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{m,k}}\big{[}Q_{m,k,h}- \mathcal{T}_{h}Q_{m,k,h+1}\big{]}(s_{h}^{k},a_{h}^{k})\leq\sum_{k=1}^{K}\!\sum _{h=1}^{H}\!2b_{m,k,h}(s_{h}^{k},a_{h}^{k}).\] (12)

The sum of bonuses is equal to the sum of uncertainty up to a constant, which we bound in the following lemma corresponding to the elliptical potential lemma (Abbasi-Yadkori et al., 2011).

**Lemma 6.2**.: _Define universal datasets as \(Z_{k,h}^{\text{all}}=\{o_{h}^{k^{\prime}}\}_{k^{\prime}\in[k]}\). Then we have for any \(h\in[H]\):_

\[\sum_{k=1}^{K}\!D_{\lambda,\mathcal{F}}^{2}(z_{h}^{k};Z_{k-1,h}^{\text{all}}) =O\big{(}\dim_{E}(\mathcal{F},\lambda/K)\log^{2}(K/\min\{1,\lambda\})\big{)}.\]

Careful examination exposes a problem: the uncertainty \(D_{\lambda,\mathcal{F}}(z;Z_{k,h}^{\text{ser}})\) corresponding to bonuses are based on server data \(Z_{k,h}^{\text{ser}}\) instead of universal data \(Z_{k,h}^{\text{all}}\). The next lemma bridges this gap:

**Lemma 6.3**.: _For any \(z\in\mathcal{S}\times\mathcal{A}\), \(k\in[K]\), \(h\in[H]\), \(D_{\lambda,\mathcal{F}}^{2}(z;Z_{k,h}^{\text{ser}})\leq(1+M\alpha)D_{\lambda, \mathcal{F}}^{2}(z;Z_{k,h}^{\text{all}})\)._

With these, we can deduce the regret bound from (12).

### Communication Cost

For communication cost, we employ an _epoch segmentation_ scheme, which defines \(N\) epochs segmented by episodes \(\{k_{i}\}_{i=1}^{N}\), with \(k_{i}\) being the smallest episode satisfying

\[\sum_{o_{h}\in Z_{k_{i},h}^{\text{ser}}\setminus Z_{k_{i-1},h}^{\text{int}}} \sum_{h=1}^{H}D_{\lambda,\mathcal{F}_{h}}^{2}(z_{h};Z_{k_{i-1},h}^{\text{ser}}) \geq 1.\] (13)

This is a generalization of epoch segmentation based on doubling determinants in linear settings, yet the lack of determinant in the nonlinear case dramatically increases its complexity. Intuitively, switch condition (8) suggests an agent must gather a substantial amount of data to trigger communication, yet a careful analysis according to (13) yields a maximum of \(M+C/\alpha\) communication rounds within one epoch. With this we only need an upper bound for the number of epochs \(N\). This is derived by summing (13) over all epochs, then using Lemma 6.1 and Lemma 6.3 to bound the left hand side.

## 7 Conclusions

We propose the algorithms Async-NLin-UCB and Async-NLSVI-UCB to tackle multi-agent nonlinear contextual bandits and MDPs with asynchronous communication. We prove that our algorithms enjoy low regret and communication cost, which are comparable to previous results.

Our algorithms employ a communication criterion that allows the agents to trigger communication rounds, effectively controlling communication cost while promoting the asynchronous protocol. Moreover, we carefully design the contents of server download to guard against data exposure.

## References

* Abbasi-Yadkori et al. [2011] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24:2312-2320, 2011.
* Agarwal and Zhang [2022] Alekh Agarwal and Tong Zhang. Model-based RL with optimistic posterior sampling: Structural conditions and sample complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Agarwal et al. [2023] Alekh Agarwal, Yujia Jin, and Tong Zhang. Voql: Towards optimal regret in model-free rl with nonlinear function approximation. In Gergely Neu and Lorenzo Rosasco, editors, _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 987-1063. PMLR, 12-15 Jul 2023.
* Bazzan [2009] Ana LC Bazzan. Opportunities for multiagent systems and multiagent reinforcement learning in traffic control. _Autonomous Agents and Multi-Agent Systems_, 18(3):342-375, 2009.
* Berner et al. [2019] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Jozefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.
* Chakraborty et al. [2017] Mithun Chakraborty, Kee Yuan Peh Chua, Sanmay Das, and Brendan Juba. Coordinated versus decentralized exploration in multi-agent multi-armed bandits. In _IJCAI_, 2017.
* Clemente et al. [2017] Alfredo V Clemente, Humberto N Castejon, and Arjun Chandra. Efficient parallel methods for deep reinforcement learning. _arXiv preprint arXiv:1705.04862_, 2017.
* Di et al. [2023] Qiwei Di, Heyang Zhao, Jiafan He, and Quanquan Gu. Pessimistic nonlinear least-squares value iteration for offline reinforcement learning. _arXiv preprint arXiv:2310.01380_, 2023.
* Ding et al. [2020] Guohui Ding, Joewie J Koh, Kelly Merckaert, Bram Vanderborght, Marco M Nicotra, Christoffer Heckman, Alessandro Roncone, and Lijun Chen. Distributed reinforcement learning for cooperative multi-robot object manipulation. In _Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems_, pages 1831-1833, 2020.
* Du et al. [2021] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 2826-2836. PMLR, 18-24 Jul 2021.
* Dubey and Pentland [2021] Abhimanyu Dubey and Alex Pentland. Provably efficient cooperative multi-agent reinforcement learning with function approximation. _arXiv preprint arXiv:2103.04972_, 2021.
* Dubey and Pentland [2020] Abhimanyu Dubey and AlexSandy' Pentland. Differentially-private federated linear bandits. _Advances in Neural Information Processing Systems_, 33:6003-6014, 2020.
* Espeholt et al. [2018] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In _International conference on machine learning_, pages 1407-1416. PMLR, 2018.
* Foster et al. [2023] Dylan J. Foster, Sham M. Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making, 2023.
* Gentile et al. [2022] Claudio Gentile, Zhilei Wang, and Tong Zhang. Fast rates in pool-based batch active learning, 2022.
* He et al. [2022] Jiafan He, Tianhao Wang, Yifei Min, and Quanquan Gu. A simple and provably efficient algorithm for asynchronous federated contextual linear bandits. In _Advances in Neural Information Processing Systems_, 2022.
* Horgan et al. [2018] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver. Distributed prioritized experience replay. In _International Conference on Learning Representations_, 2018.
* Huang et al. [2021] Ruiquan Huang, Weiqiang Wu, Jing Yang, and Cong Shen. Federated linear contextual bandits. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.

* Jaderberg et al. [2019] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-level performance in 3d multiplayer games with population-based reinforcement learning. _Science_, 364(6443):859-865, 2019.
* Jiang et al. [2017] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1704-1713. PMLR, 06-11 Aug 2017.
* Jin et al. [2020] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Jin et al. [2021] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* Jin et al. [2022] Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated reinforcement learning with environment heterogeneity. In _International Conference on Artificial Intelligence and Statistics_, pages 18-37. PMLR, 2022.
* Kong et al. [2023] Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F. Yang. Online sub-sampling for reinforcement learning with general function approximation, 2023.
* Volume 48_, ICML'16, page 1301-1309. JMLR.org, 2016.
* Kuba et al. [2022] Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. In _International Conference on Learning Representations_, 2022.
* Landgren et al. [2016] Peter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard. On distributed cooperative decision-making in multiarmed bandits. In _2016 European Control Conference (ECC)_, pages 243-248, 2016.
* Landgren et al. [2018] Peter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard. Social imitation in cooperative multiarmed bandits: Partition-based algorithms with strictly local information. In _2018 IEEE Conference on Decision and Control (CDC)_, 2018.
* Li and Wang [2022] Chuanhao Li and Hongning Wang. Asynchronous upper confidence bound algorithms for federated linear bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 6529-6553. PMLR, 2022.
* Li et al. [2022] Gene Li, Pritish Kamath, Dylan J Foster, and Nati Srebro. Understanding the eluder dimension. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 23737-23750. Curran Associates, Inc., 2022.
* Liu et al. [2019] Boyi Liu, Lujia Wang, and Ming Liu. Lifelong federated reinforcement learning: a learning architecture for navigation in cloud robotic systems. _IEEE Robotics and Automation Letters_, 4(4):4555-4562, 2019.
* Liu et al. [2022] Dianbo Liu, Vedant Shah, Oussama Boussif, Cristian Meo, Anirudh Goyal, Tianmin Shu, Michael Mozer, Nicolas Heess, and Yoshua Bengio. Stateful active facilitator: Coordination and environmental heterogeneity in cooperative multi-agent reinforcement learning. _arXiv preprint arXiv:2210.03022_, 2022.
* Liu et al. [2020] Dongfang Liu, Yiming Cui, Zhiwen Cao, and Yingjie Chen. Indoor navigation for mobile agents: A multimodal vision fusion model. In _2020 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2020.
* Liu and Zhao [2010] Keqin Liu and Qing Zhao. Distributed learning in multi-armed bandit with multiple players. _IEEE Transactions on Signal Processing_, 58(11):5667-5681, 2010. doi: 10.1109/TSP.2010.2062509.
* Lowe et al. [2017] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. _Advances in neural information processing systems_, 30, 2017.
* Martinez-Rubio et al. [2019] David Martinez-Rubio, Varun Kanade, and Patrick Rebeschini. Decentralized cooperative stochastic bandits. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2019.
* Min et al. [2022] Yifei Min, Tianhao Wang, Ruitu Xu, Zhaoran Wang, Michael Jordan, and Zhuoran Yang. Learn to match with no regret: Reinforcement learning in markov matching markets. In _Advances in Neural Information Processing Systems_, 2022.

* Min et al. [2023] Yifei Min, Jifan He, Tianhao Wang, and Quanquan Gu. Cooperative multi-agent reinforcement learning: Asynchronous communication and linear function approximation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 24785-24811. PMLR, 23-29 Jul 2023.
* Na et al. [2022] Seongin Na, Tomas Roucek, Jiri Ulrich, Jan Pikman, Tomas Krajnik, Barry Lennox, and Farshad Arvin. Federated reinforcement learning for collective navigation of robotic swarms, 2022.
* Nair et al. [2015] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, et al. Massively parallel methods for deep reinforcement learning. _arXiv preprint arXiv:1507.04296_, 2015.
* Qi et al. [2021] Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: techniques, applications, and open challenges. _arXiv preprint arXiv:2108.11887_, 2021.
* Russo and Van Roy [2013] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013.
* Sankararaman et al. [2019] Abishek Sankararaman, Ayalvadi Ganesh, and Sanjay Shakkottai. Social learning in multi agent multi armed bandits. _Proc. ACM Meas. Anal. Comput. Syst._, 3(3), 2019.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Szorenyi et al. [2013] Balazs Szorenyi, Robert Busa-Fekete, Istvan Hegedus, Robert Ormandi, Mark Jelasity, and Balazs Kegl. Gossip-based distributed stochastic bandit algorithms. In _Proceedings of the 30th International Conference on Machine Learning_, 2013.
* Vinyals et al. [2017] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. _arXiv preprint arXiv:1708.04782_, 2017.
* Wai et al. [2018] Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning via double averaging primal-dual optimization. _Advances in Neural Information Processing Systems_, 31, 2018.
* Wainwright [2019] Martin Wainwright. _High-Dimensional Statistics: A Non-Asymptotic Viewpoint_. 02 2019. ISBN 9781108498029.
* Wang et al. [2020a] Po-An Wang, Alexandre Proutiere, Kaito Ariu, Yassir Jedra, and Alessio Russo. Optimal algorithms for multiplayer multi-armed bandits. In Silvia Chiappa and Roberto Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 4120-4129. PMLR, 26-28 Aug 2020a.
* Wang et al. [2020b] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 6123-6135. Curran Associates, Inc., 2020b.
* Wang et al. [2020c] Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, and Liwei Wang. Distributed bandit learning: Near-optimal regret with efficient communication. In _International Conference on Learning Representations_, 2020c.
* Williams et al. [2016] Grady Williams, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou. Aggressive driving with model predictive path integral control. In _2016 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1433-1440. IEEE, 2016.
* Xu et al. [2023] Ruitu Xu, Yifei Min, Tianhao Wang, Michael I Jordan, Zhaoran Wang, and Zhuoran Yang. Finding regularized competitive equilibria of heterogeneous agent macroeconomic models via reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 375-407. PMLR, 2023.
* Ye et al. [2020] Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and Markov decision processes. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 39834-39863. PMLR, 23-29 Jul 2023.
* Ye et al. [2020] Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning. _Advances in Neural Information Processing Systems_, 33:621-632, 2020.

* [498] Chao Yu, Akash Veliu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games. _arXiv preprint arXiv:2103.01955_, 2021.
* [500] Shuai Yu, Xu Chen, Zhi Zhou, Xiaowen Gong, and Di Wu. When deep reinforcement learning meets federated learning: Intelligent multitimescale resource management for multiaccess edge computing in 5g ultradense network. _IEEE Internet of Things Journal_, 8(4):2238-2251, 2020.
* [501] Tao Yu, HZ Wang, Bin Zhou, Ka Wing Chan, and J Tang. Multi-agent correlated equilibrium q (\(\lambda\)) learning for coordinated smart generation control of interconnected power grids. _IEEE transactions on power systems_, 30(4):1669-1679, 2014.
* [502] Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Networked multi-agent reinforcement learning in continuous spaces. In _2018 IEEE conference on decision and control (CDC)_, pages 2771-2776. IEEE, 2018a.
* [503] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In _International Conference on Machine Learning_, pages 5872-5881. PMLR, 2018b.
* [504] Heyang Zhao, Jiafan He, and Quanquan Gu. A nearly optimal and low-switching algorithm for reinforcement learning with general function approximation, 2023.
* [505] Zhaowei Zhu, Jingxuan Zhu, Ji Liu, and Yang Liu. Federated bandit: A gossiping approach. _Proc. ACM Meas. Anal. Comput. Syst._, 5(1), 2021.
* [506] Hankz Hankui Zhuo, Wenfeng Feng, Yufeng Lin, Qian Xu, and Qiang Yang. Federated deep reinforcement learning. _arXiv preprint arXiv:1901.08277_, 2019.

Impact Statement

Our work has the potential to enhance cooperative learning systems across diverse fields. By introducing algorithms that enable efficient collaboration among agents with minimal communication overhead, our research paves the way for advancements in distributed systems, including robotics, traffic management, and distributed sensor networks. This could lead to more adaptive, efficient, and scalable systems capable of tackling complex problems in dynamic environments, ultimately contributing to technological progress and societal well-being.

As far as we can tell, there is hardly any negative social impact from our work, mainly because we do not include experiments apart from our theoretical analysis.

The Bandit Case: Proof of Theorem 4.3

Before we begin the analysis of Algorithm 1, we reiterate and add some notations for clarity and convenience. Define the data collected by agent \(m\) that has already been uploaded to the server by round \(t\) as \(Z_{m,t}^{\text{up}}\), and the universal data at round \(t\) as \(Z_{t}^{\text{all}}\). Apart from these we also have from the algorithm the datasets \(Z_{m,t}^{\text{loc}}\) and \(Z_{t}^{\text{ser}}\). It is not difficult to check that they satisfy the following relation:

\[Z_{t}^{\text{all}}=\bigcup_{m=1}^{M}\big{(}Z_{m,t}^{\text{up}}\cup Z_{m,t}^{ \text{loc}}\big{)}.\]

Furthermore, when \(t\) is not a communication round, we also have

\[Z_{t}^{\text{ser}}=\bigcup_{m=1}^{M}Z_{m,t}^{\text{up}},\]

and when it is a communication round that

\[Z_{t}^{\text{ser}}=\bigg{[}\bigcup_{m=1}^{M}Z_{m,t}^{\text{up}}\bigg{]}\cup Z _{m_{t},t}^{\text{loc}},\]

which will be useful in our proof of Lemma A.1 and B.1 in Section C.1.

Next, we assume that at rounds \(0=t_{0}<t_{1}<\cdots<t_{L}<t_{L+1}=T+1\), the participating agent communicates with the server, where \(t_{0}\) and \(t_{K+1}\) are dummy rounds. The subscripts will be denoted as \(l=1,\cdots,L\) in the future.

We now describe a participant reordering trick for our asynchronous multi-agent setting, which we will use multiple times in the proof. The basic idea is that, as long as the _communication order_ remains the same, and for any given agent, the _number of rounds_ between two consecutive communication rounds remains the same, one can switch the episodes around and change the order of agent participation to a certain degree. For example, we may assume that \(m_{t}=m_{t_{l}}\) for all \(t\in(t_{l-1},t_{l}]\) by reordering the participants, which means all participation of any given agent happens immediately _before_ a certain communication round; as another example, we may assume \(m_{t}=m_{t_{l-1}}\) for all \(t\in[t_{l-1},t_{l})\), which means all participation happen immediately _after_ communication rounds. It should be noted that one needs to be careful when utilizing this argument, since switching the participation order changes the values of \(t_{l}\) and many associated elements, so applying this trick twice in succession would lead to contradictions.

For a dataset \(Z\), we define the \(Z\)-norm on function set \(\mathcal{F}\) as \(\|f\|_{Z}^{2}:=\sum_{(a,r)\in Z}f^{2}(a)\) for any \(f\in\mathcal{F}\). Then we have the shortened notation

\[D_{\lambda,\mathcal{F}}(a;Z)=\sup_{f_{1},f_{2}\in\mathcal{F}}\frac{|f_{1}(a)- f_{2}(a)|}{\sqrt{\lambda+\|f_{1}-f_{2}\|_{Z}^{2}}}.\]

Finally, we define the confidence set of functions at round \(t+1\) as:

\[\mathcal{F}_{t+1}=\bigg{\{}f\in\mathcal{F}:\sum_{(a,r)\in Z_{t}^{\text{ex}}} \big{(}f(a)-\widehat{f}_{t+1}(a)\big{)}^{2}\leq\beta_{t}^{2}\bigg{\}},\] (14)

which is a common construction in reinforcement learning.

### Auxiliary Lemmas

In this section we present some auxiliary lemmas that will be used in the proof of Theorem 4.3. Note these lemmas correspond well to the lemmas presented in 6, only that these are for the contextual bandit case. The proofs for these lemmas can be found in Section C.

**Lemma A.1**.: _For any \(t\in[T]\), \(m\in[M]\) and \(f_{1},f_{2}\in\mathcal{F}\), as long as agent \(m\) does not communicate with the server at time step \(t\), we have_

\[\lambda+\sum_{m^{\prime}\in[M]}\|f_{1}-f_{2}\|_{Z_{m^{\prime},t}^{\text{ex}} }^{2}\geq\frac{1}{\alpha}\|f_{1}-f_{2}\|_{Z_{m,t}^{\text{ex}}}^{2}.\]

_Furthermore, for any \(t\in[T]\) and \(f_{1},f_{2}\in\mathcal{F}\),_

\[\lambda+\|f_{1}-f_{2}\|_{Z_{t}^{\text{ex}}}^{2}\geq\frac{1}{1+M\alpha}\big{(} \lambda+\|f_{1}-f_{2}\|_{Z_{t}^{\text{ex}}}^{2}\big{)},\]_and as a corollary, for any \(a\in\mathcal{A}\),_

\[D^{2}_{\lambda,\mathcal{F}}(a;Z^{\text{zer}}_{t})\leq(1+M\alpha)D^{2}_{\lambda, \mathcal{F}}(a;Z^{\text{all}}_{t})\]

This lemma describes the discrepancy between different datasets. Crucially, it provides a worst case ratio between uncertainty measured on the server dataset and universal dataset. This is an important tool for bridging between the different uncertainty estimators in the following proofs. The proof can be found in Section C.1.

**Lemma A.2**.: _By taking \(\gamma=O(1/T)\) and_

\[\beta_{t}=\widetilde{\beta}_{1}:=C_{\beta,1}\big{[}\sqrt{\lambda}+\sqrt{( \gamma^{2}+\gamma R)T}+RC(M,\alpha)\log(3MN(\mathcal{F},\gamma)/\delta)\big{]},\]

_with \(C_{\beta,1}=6\), where \(C(M,\alpha):=\sqrt{1+M\alpha}+M\sqrt{\alpha}\), we have \(f^{*}\in\mathcal{F}_{t+1}\) for all \(t\in\{t_{l}\}_{l=1}^{L}\) with probability at least \(1-\delta\). As a corollary, we also have \(|f_{*}(a)-\widehat{f}_{t+1}(a)|\leq b_{t+1}(a)\) for any \(a\in\mathcal{A}_{t}\) and \(t\in\{t_{l}\}_{l=1}^{L}\)._

This is the central optimism lemma present in all provably efficient reinforcement learning literature. It states that the confidence function set contains the ground truth function \(f^{*}\) with high probability, and in our case, that the decision function \(\widehat{f}_{t}+b_{t}\) is optimistic. With this, we define the good event \(\mathcal{E}_{T}=\{f^{*}\in\mathcal{F}_{t+1},\forall t\in\{t_{l}\}_{l=1}^{L}\}\). Then according to A.2, \(\mathbb{P}(\mathcal{E}_{T})\geq 1-\delta\). The proof can be found in Section C.2.

**Lemma A.3**.: _The sum of squared uncertainty estimators of new data over all historical data can be bounded as follows with some absolute constant \(C_{D}\):_

\[\sum_{t=1}^{T}D^{2}_{\lambda,\mathcal{F}}(a_{t};Z^{\text{all}}_{t-1})\leq C_{ D}\dim_{E}(\mathcal{F},\lambda/T)\log^{2}(T/\min\{1,\lambda\})\]

This lemma corresponds to the elliptical potential argument from the linear setting [1]. In the nonlinear setting, this lemma essentially reveals the relationship between the sum of Eluder-like confidence quantities and the Eluder dimension. The proof can be found in Section C.3.

### The Epoch Segmentation Scheme

In this section, we introduce an epoch segmentation scheme, which is needed for both the regret and communication cost proofs presented in the next two sections. It is a generalization of the epoch segmentation scheme based on doubling determinant in the linear bandits / MDPs setting [14, 15], but the lack of a Gram matrix (used for linear regression) in the nonlinear case complicates matters significantly.

We segment the entire run of \(t=1,\cdots,T\) into \(N\) epochs as follows. Define iteratively \(0=l_{0}<l_{1}<\cdots<l_{N}\leq L\) as

\[l_{i}=\min\bigg{\{}l>l_{i-1}:\sum_{l^{\prime}=l_{i-1}+1}^{l}\sum_{(a,r)\in Z^{ \text{loc}}_{m,t^{\prime}}}D^{2}_{\lambda,\mathcal{F}}(a;Z^{\text{ser}}_{t_{i -1}})\geq 1\bigg{\}},\]

where for a given \(l^{\prime}\) in the summation, \(m=m_{t_{l^{\prime}}}\) is the participating agent at \(t_{l^{\prime}}\). In the iterative process, if the above minimum does not exist, simply define \(N=i-1\) and end the process there. Correspondingly, the \(i\)-th epoch is defined by the time steps \([t_{l_{i-1}},t_{l_{i}})\).

The following sections will make use of this epoch scheme as befit their needs, but here we shall give an upper bound for the total number of epochs \(N\). Based on the definition of \(l_{i}\), we have for any \(l_{i-1}\leq l<l_{i}\) that

\[1 \geq\sum_{l^{\prime}=l_{i-1}+1}^{l}\sum_{(a,r)\in Z^{\text{suc}}_{m_{ t^{\prime}},t^{\prime}}}D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{ser}}_{t_{l_{i-1}}})\] \[=\sum_{l^{\prime}=l_{i-1}+1}^{l}\sum_{(a,r)\in Z^{\text{suc}}_{t^ {\prime}}}\sum_{Z^{\text{suc}}_{t^{\prime}-1}}\sup_{f_{1},f_{2}\in\mathcal{F}} \frac{[f_{1}(a)-f_{2}(a)]^{2}}{\lambda+\|f_{1}-f_{2}\|_{Z^{\text{suc}}_{t_{l_{ i-1}}}}^{2}}\] \[\geq\sup_{f_{1},f_{2}\in\mathcal{F}}\frac{\sum_{(a,r)\in Z^{ \text{suc}}_{t_{l}},Z^{\text{suc}}_{t_{i-1}}}[f_{1}(a)-f_{2}(a)]^{2}}{\lambda+ \|f_{1}-f_{2}\|_{Z^{\text{suc}}_{t_{i-1}}}^{2}}\] \[=\sup_{f_{1},f_{2}\in\mathcal{F}}\frac{\lambda+\|f_{1}-f_{2}\|_{Z ^{\text{suc}}_{t_{i}}}^{2}}{\lambda+\|f_{1}-f_{2}\|_{Z^{\text{suc}}_{t_{i-1} }}^{2}}-1,\]

which gives \(\lambda+\|f_{1}-f_{2}\|_{Z^{\text{suc}}_{t_{i}}}^{2}\leq 2\big{(}\lambda+\|f_{1}-f _{2}\|_{Z^{\text{suc}}_{t_{i-1}}}^{2}\big{)}\) for any \(f_{1},f_{2}\in\mathcal{F}\). Then we have

\[D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{ser}}_{t_{i-1}})\leq 2D_{\lambda, \mathcal{F}}^{2}(a;Z^{\text{ser}}_{t_{i}})\] (15)

for any \(a\), and so

\[1 \leq\sum_{(a,r)\in Z^{\text{suc}}_{t_{i}},Z^{\text{suc}}_{t_{i-1 }}}D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{ser}}_{t_{i-1}})\] \[=\sum_{l=l_{i-1}+1}^{l_{i}}\sum_{(a,r)\in Z^{\text{suc}}_{t_{l}} \setminus Z^{\text{suc}}_{t_{i-1}}}D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{ser }}_{t_{i-1}})\] \[\leq 2\sum_{l=l_{i-1}+1}^{l_{i}}\sum_{(a,r)\in Z^{\text{suc}}_{t_ {l}}\setminus Z^{\text{suc}}_{t_{i-1}}}D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{ ser}}_{t_{i-1}}),\]

and summing over \(i=1,\cdots,N-1\) that:

\[N-1\leq 2\sum_{l=1}^{L}\sum_{(a,r)\in Z^{\text{suc}}_{t_{l}}\setminus Z^{ \text{suc}}_{t_{i-1}}}D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{ser}}_{t_{i-1}}).\]

If we apply the participant reordering trick and let \(m_{t}=m_{t_{l}}\) for all \(t\in(t_{l-1},t_{l}]\) and \(l\in[L]\), we get \(Z^{\text{sect}}_{t_{l}}\setminus Z^{\text{ser}}_{t_{l-1}}=\{(a_{t},r_{t})\}_{ t=t_{l-1}+1}^{t_{l}}\), and so applying Lemma A.1 and Lemma A.3, we get

\[N-1 \leq 2\sum_{l=1}^{L}\sum_{t=t_{l-1}+1}^{t_{l}}D_{\lambda, \mathcal{F}}^{2}(a_{t};Z^{\text{ser}}_{t_{l-1}})\] \[\leq 2(1+M\alpha)\sum_{l=1}^{L}\sum_{t=t_{l-1}+1}^{t_{l}}D_{ \lambda,\mathcal{F}}^{2}(a_{t};Z^{\text{all}}_{t-1})\] \[\leq 2(1+M\alpha)\sum_{t=1}^{T}D_{\lambda,\mathcal{F}}^{2}(a_{t};Z ^{\text{all}}_{t-1})\] \[\leq C(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log(T/\lambda) \log T,\]

which gives the order of total number of epochs:

\[N=O\bigg{(}(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log^{2}(T/\min\{1, \lambda\})\bigg{)}.\] (16)

Notice that the participant reordering trick is only used to bound the number of epochs, which itself does not depend on the specific order of participation. This is crucial since it suggests this reordering does not change anything essential, and is in fact not necessary for the proof - it just made the proof easier to read. Therefore we can still reorder participants as we see fit in other parts of our proof.

### Proof of Regret Upper Bound

Now we are ready to prove the first part of Theorem 4.3 concerning the regret upper bound. We begin by applying the participation reordering trick to assume, without loss of generality, that the same agent is active within the rounds \([t_{l},t_{l+1}-1]\), i.e. \(m_{t_{l}}=m_{t_{l}+1}=\cdots=m_{t_{l+1}-1}\). Under this assumption, we have \(t_{1}=1\).

Let \(a_{t}^{*}:=\operatorname*{argmax}_{a\in D_{l}}f_{*}(a)\) be the best arm at time \(t\). Then by Lemma A.2, \(f_{*}(a_{t}^{*})\leq\big{(}\widehat{f}_{m_{t},t}+b_{m_{t},t}\big{)}(a_{t}^{*}) \leq\big{(}\widehat{f}_{m_{t},t}+b_{m_{t},t}\big{)}(a_{t})\), where the second inequality is due to the choice of \(a_{t}\) at round \(t\). Hence we get

\[\text{Reg}(T) =\sum_{t=1}^{T}\big{[}f_{*}(a_{t}^{*})-f_{*}(a_{t})\big{]}\] \[\leq\min\bigg{\{}\sum_{t=1}^{T}\big{(}\widehat{f}_{m_{t},t}+b_{m _{t},t}-f^{*}\big{)}(a_{t}),4\bigg{\}}\] \[\leq\sum_{t=1}^{T}\min\{2b_{m_{t},t}(a_{t}),4\}\] \[=2\sum_{l=1}^{L}\sum_{t=t_{l}+1}^{t_{l+1}-1}b_{t_{l}}(a_{t})+2 \sum_{l=1}^{L}\min\{b_{m_{t_{l}},t_{l}}(a_{t_{l}}),2\},\] (17)

where the first inequality is due to \(|f|\leq 1\) from Assumption 3.1, and the second inequality again uses Lemma A.2. We first bound the second term here using the epoch scheme in Section A.2. We start by converting the bonus term to uncertainty:

\[b_{m_{t_{l}},t_{l}}(a_{t_{l}}) =b_{t_{l-1}}(a_{t_{l}})\] \[\leq C_{\mathcal{B}}\sqrt{\beta_{t_{l}-1}^{2}+\lambda}\cdot D_{ \lambda,\mathcal{F}}(a_{t_{l}};Z^{\text{ser}}_{t_{l-1}}).\] (18)

Now consider the episodes in an epoch \(i\), specifically \(\{t_{l_{i-1}},t_{l_{i-1}+1},\cdots,t_{l_{i}}\}\). For any \(l_{i-1}<l<l_{i}\), since \(Z^{\text{ser}}_{t_{l_{i-1}}}\subseteq Z^{\text{ser}}_{t_{l-1}}\), we can deduce that

\[D_{\lambda,\mathcal{F}}^{2}(z_{t_{l}};Z^{\text{ser}}_{t_{l-1}})\leq D_{\lambda,\mathcal{F}}^{2}(z_{t_{l}};Z^{\text{ser}}_{t_{l_{i-1}}})\leq 2D_{\lambda, \mathcal{F}}^{2}(z_{t_{l}};Z^{\text{ser}}_{t_{l}}),\]

where the second inequality is borrowed from (15) from Section A.2. Therefore continuing from (18),

\[\sum_{l=1}^{L}\min\{b_{m_{t_{l}},t_{l}}(z_{t_{l}}),2\} \leq\sum_{l\not\in\{l_{i}\}_{i=1}^{N}}\bigg{[}\sqrt{2}C_{\mathcal{ B}}\sqrt{\beta_{t_{l}-1}^{2}+\lambda}\cdot D_{\lambda,\mathcal{F}}(z_{t_{l}};Z^{ \text{ser}}_{t_{l}})\bigg{]}+\sum_{i=1}^{N}2\] \[\leq\sqrt{2}C_{\mathcal{B}}\sum_{l=1}^{L}D_{\lambda,\mathcal{F}} (z_{t_{l}};Z^{\text{ser}}_{t_{l}})\sqrt{\beta_{h}^{2}+\lambda}+2N.\] (19)

Now combine this result with the first term in (17) and use again (18), we get

\[\text{Reg}(T) \leq 2C_{\mathcal{B}}\sum_{l=1}^{L}\sum_{t=t_{l}+1}^{t_{l+1}-1}D_{ \lambda,\mathcal{F}}(a_{t};Z^{\text{ser}}_{t_{l}})\sqrt{\beta_{t_{l}}^{2}+ \lambda}+2\sqrt{2}C_{\mathcal{B}}\sum_{l=1}^{L}D_{\lambda,\mathcal{F}}(z_{t_{l }};Z^{\text{ser}}_{t_{l}})\sqrt{\beta_{h}^{2}+\lambda}+4N\] \[\leq 2\sqrt{2}C_{\mathcal{B}}\sum_{l=1}^{L}\sum_{t=t_{l}}^{t_{l+1} -1}D_{\lambda,\mathcal{F}}(a_{t};Z^{\text{ser}}_{t_{l}})\sqrt{\beta_{t_{l}}^{2 }+\lambda}+4N\] \[\leq 2\sqrt{2}C_{\mathcal{B}}\bigg{[}\sum_{l=1}^{L}\sum_{t=t_{l }}^{t_{l+1}-1}D_{\lambda,\mathcal{F}}^{2}(a_{t};Z^{\text{ser}}_{t_{l}})\bigg{]} ^{1/2}\bigg{[}\sum_{t=1}^{T}\big{(}\widetilde{\beta}_{1}^{2}+\lambda\big{)} \bigg{]}^{1/2}+4N\]

where

\[\widetilde{\beta}_{1}=C_{\beta}\bigg{[}\sqrt{\lambda}+RC(M,\alpha)\sqrt{\log( 3N()M/\delta)}\bigg{]}.\]According to Lemma A.1 and Lemma A.3, the term

\[\sum_{l=1}^{L}\sum_{t=t_{l}}^{t_{l+1}-1}D_{\lambda,\mathcal{F}}^{2}(a _{t};Z_{t_{l}}^{\text{ser}}) \leq(1+M\alpha)\sum_{l=1}^{L}\sum_{t=t_{l}}^{t_{l+1}-1}D_{\lambda,\mathcal{F}}^{2}(a_{t};Z_{t-1}^{\text{all}})\] \[=(1+M\alpha)\sum_{t=1}^{T}D_{\lambda,\mathcal{F}}^{2}(a_{t};Z_{t-1 }^{\text{all}})\] \[\leq C(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log^{2}\big{(}T/ \min\{1,\lambda\}\big{)}.\]

combining this with (16), we get

\[\text{Reg}(T) \leq C\big{[}(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log^{2} \big{(}T/\min\{1,\lambda\}\big{)}\big{]}^{1/2}\bigg{[}\sum_{t=1}^{T}(\beta_{t} ^{2}+\lambda)\bigg{]}^{1/2}+4N\] \[=O\bigg{(}\sqrt{T}\widetilde{\beta}_{1}\sqrt{(1+M\alpha)\dim_{E} (\mathcal{F},\lambda/T)}\log(T/\min\{1,\lambda\})\] \[\quad+(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log^{2}(T/\min\{ 1,\lambda\})\bigg{)}.\]

### Proof of Communication Cost

In this section we prove the second part of Theorem 4.3, by calculating the communication complexity. First, for each communication round \(t_{l}\), assume the last time before \(t_{l}\) when the agent \(m_{t_{l}}\) communicated with the server was \(t_{\nu}\), then

\[\sum_{(a,r)\in Z_{m_{t}}^{\text{loc}}}D_{\lambda,\mathcal{F}}^{2}(a;Z_{m_{t}, t_{l}}^{\text{up}})\geq\sum_{(a,r)\in Z_{m_{t}}^{\text{loc}}}\frac{\big{[}b_{t_{l }}(a)/C\big{]}^{2}}{\beta_{t_{l^{\prime}}}^{2}+\lambda}\geq\frac{\alpha}{C^{2}},\]

Now employing the epoch segmentation scheme from section A.2, for the \(i\)-th epoch consisting of the time steps \([t_{l_{i-1}},t_{l_{i}})\), we have the inequality

\[1 \geq\sum_{l=l_{i-1}+1}^{l_{i}-1}\sum_{(a,r)\in Z_{m_{t},l_{l}}^{ \text{loc}}}D_{\lambda,\mathcal{F}}^{2}(a;Z_{t_{l_{i-1}}}^{\text{ser}})\] \[\geq\sum_{l=l_{i-1}+1}^{l_{i}-1}\sum_{(a,r)\in Z_{m_{t},l_{l}}^{ \text{loc}}}D_{\lambda,\mathcal{F}}^{2}\big{(}a;Z_{m_{t},l_{l}}^{\text{up}} \cup Z_{t_{l_{i-1}}}^{\text{ser}}\big{)}.\]

For \(m\in[M]\), assume the agent \(m\) communicated with the server a total of \(n_{m}\) times within \([t_{l_{i-1}},t_{l_{i}})\). Then except for the first of these communication rounds, for each \(l\in[l_{i-1}+1,l_{i}-1]\) with \(m_{t_{l}}=m\), there exists \(l^{\prime}\in[l_{i-1},l)\) with \(m_{t_{l^{\prime}}}=m\), thus we have \(Z_{m_{t},l_{l}}^{\text{up}}\supset Z_{m_{t},l_{l^{\prime}}+1}^{\text{up}}=Z_{ t_{l^{\prime}}}^{\text{ser}}\supset Z_{t_{l_{i-1}}}^{\text{ser}}\). With this we have the corresponding term

\[\sum_{(a,r)\in Z_{m_{t},l_{l}}^{\text{loc}}}D_{\lambda,\mathcal{F}}^{2}(a;Z_{m,t_{l}}^{\text{up}}\cup Z_{t_{l_{i-1}}}^{\text{ser}})=\sum_{(a,r)\in Z_{m_{t}, l_{l}}^{\text{loc}}}D_{\lambda,\mathcal{F}}^{2}(a;Z_{m,t_{l}}^{\text{up}})\geq \frac{\alpha}{C_{\mathcal{B}}^{2}},\]

therefore

\[1\geq\sum_{m=1}^{M}(n_{m}-1)\cdot\frac{\alpha}{4C^{2}}\Rightarrow\sum_{m=1}^ {M}n_{m}\leq M+\frac{C_{\mathcal{B}}^{2}}{\alpha}\]

Notice that \(\sum_{m=1}^{M}n_{m}=l_{i}-l_{i-1}\) is the number of communication rounds within \([t_{l_{i-1}},t_{l_{i}})\), hence summing over \(i\) the total number of communication rounds is upper bounded by \(N(M+C_{\mathcal{B}}^{2}/\alpha)\). Combine this with (16), we have the total number of communication rounds throughout the algorithm is

\[O\bigg{(}\frac{(1+M\alpha)^{2}}{\alpha}\dim_{E}(\mathcal{F},\lambda/T)\log^{2} \big{(}T/\min\{1,\lambda\}\big{)}\bigg{)}.\]The MDPs Case: Proof of Theorem 5.1

Similar to the bandit case, we define \(Z^{\text{loc}}_{m,k,h}\), \(Z^{\text{up}}_{m,k,h}\), \(Z^{\text{fer}}_{k,h}\), and \(Z^{\text{all}}_{k,h}\) to be the local, uploaded, server and universal data, with corresponding subscripts of agent \(m\in[M]\), episode \(k\in[K]\), \(h\in[H]\).

Suppose at rounds \(0=k_{0}<k_{1}<\dots<k_{L}<k_{L+1}=T+1\), the participating agent communicates with the server, where \(k_{0}\) and \(k_{L+1}\) are dummy rounds.

For a dataset \(Z_{h}\) in the MDPs setting, we again define the \(Z_{h}\)-norm on function set \(\mathcal{F}_{h}\) as \(\|f\|_{Z}^{2}:=\sum_{o_{h}\in Z}f^{2}(z_{h})\) for any \(f\in\mathcal{F}\). As a reminder, the tuples \(o_{h}=(s_{h},a_{h},r_{h},s_{h+1})\) and \(z_{h}=(s_{h},a_{h})\). Then we have the shortened notation

\[D_{\lambda,\mathcal{F}_{h}}(z_{h};Z_{h})=\sup_{f_{1},f_{2}\in\mathcal{F}_{h}} \frac{|f_{1}(z_{h})-f_{2}(z_{h})|}{\sqrt{\lambda+\|f_{1}-f_{2}\|_{Z_{h}}^{2}}}.\]

Finally, we define the confidence set of functions at round \(k+1\) and step \(h\) as:

\[\mathcal{F}_{k+1,h}=\bigg{\{}f\in\mathcal{F}_{h}:\sum_{o_{h}\in Z^{\text{fer} }_{k,h}}\big{(}f(z_{h})-\widehat{f}_{k+1,h}(z_{h})\big{)}^{2}\leq(\beta_{k,h}) ^{2}\bigg{\}}.\] (20)

### Auxiliary Lemmas

In this section we present some auxiliary lemmas that will be used in the proof of Theorem 5.1. These lemmas are generalizations / restatements to the lemmas presented in 6, and their detailed proofs can be found in Section C.

**Lemma B.1** (Restatement of Lemma 6.3).: _For any \(k\in[K]\), \(m\in[M]\), \(h\in[H]\) and \(f_{1},f_{2}\in\mathcal{F}\), as long as agent \(m\) does not communicate with the server at episode \(k\), we have_

\[\lambda+\sum_{m^{\prime}\in[M]}\|f_{1}-f_{2}\|_{Z^{\text{up}}_{m^{\prime},k,h }}^{2}\geq\frac{1}{\alpha}\|f_{1}-f_{2}\|_{Z^{\text{loc}}_{m,k,h}}^{2}.\]

_Furthermore, we have for any \(k\in[K]\) and \(f_{1},f_{2}\in\mathcal{F}\),_

\[\lambda+\|f_{1}-f_{2}\|_{Z^{\text{up}}_{k,h}}^{2}\geq\frac{1}{1+M\alpha}\big{(} \lambda+\|f_{1}-f_{2}\|_{Z^{\text{up}}_{k,h}}^{2}\big{)},\]

_and as a corollary, for any \(z=(s,a)\in\mathcal{S}\times\mathcal{A}\),_

\[D_{\lambda,\mathcal{F}}^{2}(z;Z^{\text{scr}}_{k,h})\leq(1+M\alpha)D_{\lambda, \mathcal{F}}^{2}(z;Z^{\text{aff}}_{k,h})\]

Similar to Lemma A.1, this lemma provides a worst case ratio between uncertainty measured on the server dataset and universal dataset. The proof can be found in Section C.1.

**Lemma B.2** (Restatement of Lemma 6.1).: _By taking \(\gamma=1/(C_{\gamma}KH)\) with \(C_{\gamma}\geq 20\), as well as_

\[\beta_{k,h}=\widetilde{\beta}_{2}:=C_{\beta,2}\bigg{[}\sqrt{\lambda}+HC(M, \alpha)\sqrt{\log(3HMN_{h}(\gamma)/\delta)}\bigg{]},\]

_with \(C_{\beta,2}=12\) for all \(k\in[K]\) and \(h\in[H]\), where \(N_{h}(\gamma)=N(\mathcal{F}_{h},\gamma)\cdot N(\mathcal{F}_{h+1},\gamma)\cdot N (\mathcal{W}_{h+1},\gamma)\), we have with probability at least \(1-\delta\) that \(\mathcal{T}_{h}Q_{k+1,h+1}\in\mathcal{F}_{k+1,h}\) for all \(k\in\{k_{l}\}_{l=1}^{L}\) with probability at least \(1-\delta\). As a corollary, we also have \(|\mathcal{T}_{h}Q_{k+1,h+1}(s,a)-\widehat{f}_{k+1,h}(s,a)|\leq b_{k+1,h}(s,a)\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(k\in\{k_{l}\}_{l=1}^{L}\) and \(h\in[H]\)._

This is the central optimism lemma. It states that the Bellman operator of \(Q\)-value function at level \(h+1\) is within the confidences set at level \(h\). The conclusion immediately gives the optimism inequality \(\mathcal{T}_{h}Q_{k+1,h+1}(s,a)\leq Q_{k+1,h}(s,a)\), which we will use at the start of the regret upper bound prove. The proof of the lemma can be found in Section C.2.

With this, we define the good event \(\mathcal{E}_{T}=\{\mathcal{T}_{h}Q_{k+1,h+1}\in\mathcal{F}_{k+1,h},\forall k\in \{k_{l}\}_{l=1}^{L},h\in[H]\}\). Then according to Lemma B.2, \(\mathbb{P}(\mathcal{E}_{T})\geq 1-\delta\).

**Lemma B.3**.: _For some absolute constant \(C_{D}\), the following holds for all level \(h\in[H]\):_

\[\sum_{k=1}^{K}D_{\lambda,\mathcal{F}}^{2}(z_{h}^{k};Z^{\text{all}}_{k-1,h})\leq C _{D}\dim_{E}(\mathcal{F},\lambda/T)\log^{2}(T/\min\{1,\lambda\})\]

This lemma is essentially the same as Lemma A.3. It reveals the relationship between the sum of Eluder-like confidence quantities and the Eluder dimension. The proof can be found in Section C.3.

### The Epoch Segmentation Scheme

In this section, we introduce the epoch segmentation scheme for MDPs, which is again needed for both the regret and communication cost proofs presented in the next two sections. All of this is quite similar to the bandit case in Section A.2, but the introduction of multiple levels \(h\in[H]\) does complicate things a bit.

We segment the entire run of episodes \(k=1,\cdots,K\) into \(N\) epochs as follows. Define iteratively \(0=l_{0}<l_{1}<\cdots<l_{N}\leq L\) as

\[l_{i}=\min\bigg{\{}l>l_{i-1}:\sum_{l^{\prime}=l_{i-1}+1}^{l}\sum_{h=1}^{H}\sum _{o_{h}\in Z^{\text{suc}}_{m,k_{l^{\prime}},h}}D^{2}_{\lambda,\mathcal{F}_{h}} (z_{h};Z^{\text{ser}}_{k_{l_{i-1}},h})\geq 1\bigg{\}},\]

where for a given \(l^{\prime}\) in the summation, \(m=m_{k_{l^{\prime}}}\) is the participating agent at \(k_{l^{\prime}}\). In the iterative process, if the above minimum does not exist, simply define \(N=i-1\) and end the process there. Correspondingly, the \(i\)-th epoch is defined by the episodes \([k_{l_{i-1}},k_{l_{i}})\).

The following sections will make use of this epoch scheme as befit their needs, but here we shall give an upper bound for the total number of epochs \(N\). Based on the definition of \(l_{i}\), we have for any \(l_{i-1}\leq l<l_{i}\) that

\[1 \geq\sum_{l^{\prime}=l_{i-1}+1}^{l}\sum_{h=1}^{H}\sum_{o_{h}\in Z ^{\text{suc}}_{m,k_{l^{\prime}},h}}D^{2}_{\lambda,\mathcal{F}}(z_{h};Z^{\text{ ser}}_{k_{l_{i-1}},h})\] \[=\sum_{l^{\prime}=l_{i-1}+1}^{l}\sum_{h=1}^{H}\sum_{o_{h}\in Z^{ \text{suc}}_{k_{l^{\prime}},h}\setminus Z^{\text{suc}}_{k_{l^{\prime}-1},h}} \sup_{f_{1},f_{2}\in\mathcal{F}}\frac{[f_{1}(z_{h})-f_{2}(z_{h})]^{2}}{\lambda+ \|f_{1}-f_{2}\|_{Z^{\text{suc}}_{k_{l_{i-1}},h}}^{2}}\] \[\geq\sum_{h=1}^{H}\sup_{f_{1},f_{2}\in\mathcal{F}_{h}}\frac{\sum _{o_{h}\in Z^{\text{suc}}_{k_{l_{i}},h}\setminus Z^{\text{suc}}_{k_{l_{i-1}},h }}[f_{1}(z_{h})-f_{2}(z_{h})]^{2}}{\lambda+\|f_{1}-f_{2}\|_{Z^{\text{suc}}_{k_{ l_{i-1}},h}}^{2}}\] \[=\sum_{h=1}^{H}\bigg{[}\sup_{f_{1},f_{2}\in\mathcal{F}_{h}}\frac{ \lambda+\|f_{1}-f_{2}\|_{Z^{\text{suc}}_{k_{l_{i}},h}}^{2}}{\lambda+\|f_{1}-f_ {2}\|_{Z^{\text{suc}}_{k_{l_{i-1}},h}}^{2}}-1\bigg{]},\]

which gives \(\lambda+\|f_{1}-f_{2}\|_{Z^{\text{suc}}_{k_{l},h}}^{2}\leq 2\big{(}\lambda+\|f_{1}- f_{2}\|_{Z^{\text{suc}}_{k_{l_{i-1}},h}}^{2}\big{)}\) for any \(h\in[H]\) and \(f_{1},f_{2}\in\mathcal{F}_{h}\). Then we have

\[D^{2}_{\lambda,\mathcal{F}}(z_{h};Z^{\text{ser}}_{k_{l_{i-1}},h})\leq 2D^{2}_{ \lambda,\mathcal{F}}(z_{h};Z^{\text{ser}}_{k_{l},h})\] (21)

for any \(h\in[H]\) and \(z_{h}\in\mathcal{S}\times\mathcal{A}\), and so

\[1 \leq\sum_{l=l_{i-1}+1}^{l_{i}}\sum_{h=1}^{H}\sum_{o_{h}\in Z^{ \text{suc}}_{m,k_{l},h}}D^{2}_{\lambda,\mathcal{F}_{h}}(z_{h};Z^{\text{ser}}_{ k_{l_{i-1}},h})\] \[\leq 2\sum_{l=l_{i-1}+1}^{l_{i}}\sum_{h=1}^{H}\sum_{o_{h}\in Z^{ \text{suc}}_{k_{l_{i}},h}\setminus Z^{\text{suc}}_{k_{l_{i-1}},h}}D^{2}_{ \lambda,\mathcal{F}_{h}}(z_{h};Z^{\text{ser}}_{k_{l-1},h}),\]

and summing over \(i=1,\cdots,N-1\) that:

\[N-1\leq 2\sum_{h=1}^{H}\sum_{l=1}^{L}\sum_{o_{h}\in Z^{\text{suc}}_{k_{l_{i}},h} \setminus Z^{\text{suc}}_{k_{l-1},h}}D^{2}_{\lambda,\mathcal{F}_{h}}(z_{h};Z^{ \text{ser}}_{k_{l-1},h}).\]If we apply the participant reordering trick and let \(m_{k}=m_{k_{l}}\) for all \(k\in(k_{l-1},k_{l}]\) and \(l\in[L]\), we get \(Z^{\text{ser}}_{k_{l},h}\backslash Z^{\text{ser}}_{k_{l-1},h}=\{o^{k}_{h}\}_{k=k _{l-1}+1}^{k_{l}}\), and so applying Lemma 6.3 and Lemma 6.2, we get

\[N-1 \leq 2\sum_{h=1}^{H}\sum_{l=1}^{L}\sum_{k=k_{l-1}+1}^{k_{l}}D_{ \lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k};Z^{\text{ser}}_{k_{l-1},h})\] \[\leq 2(1+M\alpha)\sum_{h=1}^{H}\sum_{l=1}^{L}\sum_{k=k_{l-1}+1}^{ k_{l}}D_{\lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k};Z^{\text{all}}_{k-1,h})\] \[\leq 2(1+M\alpha)\sum_{h=1}^{H}\sum_{k=1}^{K}D_{\lambda, \mathcal{F}_{h}}^{2}(z_{h}^{k};Z^{\text{all}}_{k-1,h})\] \[\leq CH(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log(T/\lambda) \log T,\]

which gives the order of total number of epochs:

\[N=O\bigg{(}H(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log^{2}(T/\min\{1, \lambda\})\bigg{)}.\] (22)

### Proof of Regret Upper Bound

In this section, we prove the first half of Theorem 5.1, which gives an upper bound for the cumulative regret of Algorithm 2.

Using the participant reordering trick, assume without loss of generality that the same agent is active within the rounds \([k_{l},k_{l+1}-1]\), i.e. \(m_{k_{l}}=m_{k_{l}+1}=\cdots=m_{k_{l+1}-1}\). Under this assumption, we have \(k_{1}=1\).

We first prove via induction that \(Q^{*}_{h}\leq Q_{m,k,h}\) for any \(m\in[M]\), \(k\in[K]\) and \(h\in[H+1]\). This holds true for \(h=H+1\) trivially since both value functions at \(H+1\) are uniformly \(0\). Suppose we already have \(Q^{*}_{h+1}\leq Q_{m,k,h+1}\), we have from Lemma B.2 that for the last communication round \(k^{\prime}\) for agent \(m\), the server functions satisfy \(\mathcal{T}_{h}Q_{k^{\prime}+1,h+1}(s,a)\leq\widehat{f}_{k^{\prime}+1,h}(s,a) +b_{k^{\prime}+1,h}(s,a)=Q_{k^{\prime}+1,h}(s,a)\). Couple this with the fact that \(Q_{m,k,h}=Q_{k^{\prime}+1,h}\), we can prove that

\[Q^{*}_{h}=\mathcal{T}_{h}Q^{*}_{h+1}\leq\mathcal{T}_{h}Q_{m,k,h+1}\leq Q_{m,k, h},\]

which finishes the induction process.

Now let \(a^{k_{*}}_{h}:=\operatorname*{argmax}_{a\in\mathcal{A}}Q^{*}_{h}(s^{k}_{h},a)\) be the best action at time \(t\), then \(V^{*}_{h}(s^{k}_{h})=Q^{*}_{h}(s^{k}_{h},a^{k_{*}}_{h})\leq Q_{m,k,h}(s^{k}_{h },a^{k_{*}}_{h})\leq Q_{m,k,h}(s^{k}_{h},a^{k_{*}}_{h})\), where the second inequality is due to the choice of \(a^{k}_{h}\) at round \(k\). Hence we get

\[\text{Reg}(K) =\sum_{k=1}^{K}\big{[}V^{*}_{1}(s^{k}_{1})-V^{\pi_{k}}_{1}(s^{k} _{1})\big{]}\] \[\leq\sum_{k=1}^{K}\min\big{\{}V_{m,k,1}(s^{k}_{1})-V^{\pi_{k}}_{1 }(s^{k}_{1}),2H\big{\}}\] \[=\sum_{k=1}^{K}\sum_{h=1}^{H}\min\big{\{}\mathbb{E}_{\pi_{k}} \big{[}Q_{m,k,h}(s^{k}_{h},a^{k}_{h})-\mathcal{T}_{h}Q_{m,k,h+1}(s^{k}_{h},a^ {k}_{h})\big{]},2\big{\}}\] \[=\sum_{k=1}^{K}\sum_{h=1}^{H}\min\big{\{}\mathbb{E}_{\pi_{k}} \big{[}\widehat{f}_{k^{\prime}+1,h}(s^{k}_{h},a^{k}_{h})+b_{k^{\prime}+1,h}(s ^{k}_{h},a^{k}_{h})-\mathcal{T}_{h}Q_{m,k,h+1}(s^{k}_{h},a^{k}_{h})\big{]},2 \big{\}}\] \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\min\big{\{}2b_{k^{\prime},h}(s^{ k}_{h},a^{k}_{h}),2\big{\}}\] \[=2\sum_{l=1}^{L}\sum_{k=k_{l}+1}^{k_{l+1}-1}\sum_{h=1}^{H}b_{k_{l }+1,h}(z^{k}_{h})+2\sum_{l=1}^{L}\sum_{h=1}^{H}\min\{b_{m_{k_{l}},k_{l},h}(z^{ k_{l}}_{h}),1\}.\] (23)

where the second equality uses the Value-decomposition Lemma from Jiang et al. (2017), the second inequality uses again Lemma B.2, and from the third inequality onward we let \(k^{\prime}\) be the last time agent \(m\) communicated with the server.

We now bound the second term here using the epoch scheme in Section B.2. We start by converting the bonus term to uncertainty:

\[b_{m_{k_{l}},k_{l},h}(z_{h}^{k_{l}}) =b_{k_{l-1},h}(z_{h}^{k_{l}})\] \[\leq C_{\mathcal{B}}\sqrt{\beta_{k_{l}-1,h}^{2}+\lambda}\cdot D_{ \lambda,\mathcal{F}_{h}}(z_{h}^{k_{l}};Z_{k_{l-1},h}^{\text{ser}}).\] (24)

Now consider the episodes in an epoch \(i\), specifically \(\{k_{l_{i-1}},k_{l_{i-1}+1},\cdots,k_{l_{i}}\}\). For any \(l_{i-1}<l<l_{i}\), since \(Z_{k_{l_{i-1}},h}^{\text{ser}}\subseteq Z_{k_{l-1},h}^{\text{ser}}\), we can deduce that

\[D_{\lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k_{l}};Z_{k_{l-1},h}^{\text{ser}})\leq D _{\lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k_{l}};Z_{k_{l-1},h}^{\text{ser}})\leq 2D _{\lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k_{l}};Z_{k_{l},h}^{\text{ser}}),\]

where the second inequality is borrowed from (21) from Section B.2. Therefore continuing from (24),

\[\sum_{l=1}^{L}\sum_{h=1}^{H}\min\{b_{m_{k_{l}},k_{l},h}(z_{h}^{k_ {l}}),1\} \leq\sum_{l\not\in\{l_{i}\}_{i=1}^{N}}\sum_{h=1}^{H}\bigg{[}\sqrt{ 2}C_{\mathcal{B}}\sqrt{\beta_{k_{l}-1,h}^{2}+\lambda}\cdot D_{\lambda, \mathcal{F}_{h}}(z_{h}^{k_{l}};Z_{k_{l},h}^{\text{ser}})\bigg{]}+\sum_{i=1}^{N} \sum_{h=1}^{H}1\] \[\leq\sqrt{2}C_{\mathcal{B}}\sum_{l=1}^{L}\sum_{h=1}^{H}D_{\lambda,\mathcal{F}_{h}}(z_{h}^{k_{l}};Z_{k_{l},h}^{\text{ser}})\sqrt{\beta_{h}^{2}+ \lambda}+NH.\] (25)

Now combine this result with the first term in (23) and use again (24), we get

\[\text{Reg}(K) \leq C_{\mathcal{B}}\sum_{l=1}^{L}\sum_{k=k_{l}+1}^{k_{l+1}-1} \sum_{h=1}^{H}\bigg{[}D_{\lambda,\mathcal{F}_{h}}(z_{h}^{k};Z_{k_{l},h}^{\text {ser}})\sqrt{\beta_{h}^{2}+\lambda}\bigg{]}+\sqrt{2}C_{\mathcal{B}}\sum_{l=1}^ {L}\sum_{h=1}^{H}\bigg{[}D_{\lambda,\mathcal{F}_{h}}(z_{h}^{k_{l}};Z_{k_{l},h}^ {\text{ser}})\sqrt{\beta_{h}^{2}+\lambda}\bigg{]}+NH\] \[\leq\sqrt{2}C_{\mathcal{B}}\bigg{[}\sum_{l=1}^{L}\sum_{k=k_{l}}^ {k_{l+1}-1}\sum_{h=1}^{H}D_{\lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k};Z_{k_{l},h}^ {\text{ser}})\bigg{]}^{1/2}\bigg{[}\sum_{k=1}^{K}\sum_{h=1}^{H}\big{(}\beta_{h} ^{2}+\lambda\big{)}\bigg{]}^{1/2}+NH.\]

According to Lemma 6.3 and Lemma 6.2, the term

\[\sum_{l=1}^{L}\sum_{k=k_{l}}^{k_{l+1}-1}\sum_{h=1}^{H}D_{\lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k};Z_{k_{l},h}^{\text{ser}}) \leq(1+M\alpha)\sum_{l=1}^{L}\sum_{k=k_{l}}^{k_{l+1}-1}\sum_{h=1} ^{H}D_{\lambda,\mathcal{F}_{h}}^{2}(z_{h}^{k};Z_{k-1,h}^{\text{all}})\] \[\leq(1+M\alpha)\sum_{k=1}^{K}\sum_{h=1}^{H}D_{\lambda,\mathcal{F} _{h}}^{2}(z_{h}^{k};Z_{k-1,h}^{\text{all}})\] \[\leq H(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/T)\log(T/\lambda) \log T.\]

Now with \(\gamma=O(1/KH)\), we have

\[\beta_{h}=O(1)\beta_{h+1}+C_{\beta}\bigg{[}\sqrt{\lambda}+H\bigg{(}\sqrt{(1+M \alpha)\log(3HN_{h}(\gamma)/\delta)}+M\sqrt{\alpha\log(3HMN_{h}(\gamma)/\delta )}\bigg{)}\bigg{]}\]

therefore, with \(C(M,\alpha)=\sqrt{1+M\alpha}+M\sqrt{\alpha}\) and the upper bound for number of epochs \(N\) in (22), we have

\[\sum_{l=1}^{L}\sum_{k=k_{l}+1}^{k_{l+1}-1}\sum_{h=1}^{H}b_{k_{l}, h}(z_{h}^{k})\] \[\leq O\bigg{(}\big{[}H(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/K) \log^{2}(K/\min\{1,\lambda\})\big{]}^{1/2}\bigg{[}K\sum_{h=1}^{H}(\beta_{h}^{2} +\lambda)\bigg{]}^{1/2}+HN\bigg{)}\] \[=O\bigg{(}H\sqrt{K}\widetilde{\beta}_{2}\sqrt{(1+M\alpha)\dim_{E} (\mathcal{F},\lambda/K)}\log(K/\min\{1,\lambda\})\] \[\qquad+H^{2}(1+M\alpha)\dim_{E}(\mathcal{F},\lambda/K)\log^{2}(K/ \min\{1,\lambda\})\bigg{)},\]where \(\widetilde{\beta}_{2}=C_{\beta,2}\bigg{[}\sqrt{\lambda}+HC(M,\alpha)\log\big{(} HMN(\mathcal{F},\gamma)N(\mathcal{W},\gamma)/\delta\big{)}\bigg{]}\) is the choice of \(\beta_{k,h}\) in the algorithm.

### Proof of Communication Cost

Next up, we calculate the communication complexity of Algorithm 2 and prove the second half of Theorem 5.1. For each communication round \(k_{l}\), assume the last time before \(k_{l}\) when the agent \(m=m_{k_{l}}\) communicated with the server was \(k_{l^{\prime}}\), then by the communication rule there exists \(h_{l}\in[H]\) such that \(\sum_{o_{h_{l}}\in\mathcal{Z}_{m,k_{l},h_{l}}^{\text{suc}}}b_{k_{l^{\prime}}, h_{l}}^{2}(z_{h_{l}})/(\beta_{k_{l^{\prime}},h_{l}}^{2}+\lambda)\geq\alpha\),

\[\sum_{o_{h_{l}}\in\mathcal{Z}_{m,k_{l},h_{l}}^{\text{suc}}}D_{\lambda, \mathcal{F}_{h_{l}}}^{2}(z_{h_{l}};Z_{m,k_{l},h_{l}}^{\text{up}})\geq\sum_{o_ {h_{l}}\in\mathcal{Z}_{m,k_{l},h_{l}}^{\text{suc}}}\frac{\big{[}b_{k_{l^{ \prime}},h_{l}}(z_{h_{l}})/C\big{]}^{2}}{\beta_{k_{l^{\prime}},h_{l}}^{2}+ \lambda}\geq\frac{\alpha}{C^{2}},\]

Next we will make use of the epoch segmentation scheme in Section B.2. For the \(i\)-th epoch consisting of the time steps \([k_{l_{i-1}},k_{l_{i}})\), we have the inequality

\[1 \geq\sum_{l=l_{i-1}+1}^{l_{i}-1}\sum_{h=1}^{H}\sum_{o_{h}\in \mathcal{Z}_{m,k_{l},h}^{\text{suc}}}D_{\lambda,\mathcal{F}_{h}}^{2}(z_{h};Z_ {k_{l_{i-1}},h}^{\text{ser}})\] \[\geq\sum_{l=l_{i-1}+1}^{l_{i}-1}\sum_{h=1}^{H}\sum_{o_{h}\in \mathcal{Z}_{m,k_{l},h}^{\text{suc}}}D_{\lambda,\mathcal{F}_{h}}^{2}\big{(}z_ {h};Z_{m,k_{l},h}^{\text{up}}\cup Z_{k_{l_{i-1}},h}^{\text{ser}}\big{)}.\]

For \(m\in[M]\), assume the agent \(m\) communicated with the server a total of \(n_{m}\) times within \([k_{l_{i-1}},k_{l_{i}})\). Then except for the first of these communication rounds, for each \(l\in[l_{i-1}+1,l_{i}-1]\) with \(m_{k_{l}}=m\), there exists \(l^{\prime}\in[l_{i-1},l)\) with \(m_{k_{l^{\prime}}}=m\), thus we have \(Z_{m,k_{l},h}^{\text{up}}\supset Z_{m,k_{l^{\prime}}+1,h}^{\text{up}}=Z_{k_ {l^{\prime}},h}^{\text{ser}}\supset Z_{k_{l_{i-1}},h}^{\text{ser}}\) for all \(h\in[H]\). With this we have

\[\sum_{h=1}^{H}\sum_{o_{h}\in\mathcal{Z}_{m,k_{l},h}^{\text{suc}}}D_{\lambda, \mathcal{F}_{h}}^{2}\big{(}z_{h};Z_{m,k_{l},h}^{\text{up}}\cup Z_{k_{l_{i-1}},h}^{\text{ser}}\big{)}=\sum_{h=1}^{H}\sum_{o_{h}\in\mathcal{Z}_{m,k_{l},h}^ {\text{suc}}}D_{\lambda,\mathcal{F}_{h}}^{2}\big{(}z_{h};Z_{m,k_{l},h}^{ \text{up}}\big{)}\geq\frac{\alpha}{4C^{2}},\]

therefore

\[1\geq\sum_{m=1}^{M}(n_{m}-1)\cdot\frac{\alpha}{4C^{2}}\Rightarrow\sum_{m=1} ^{M}n_{m}\leq M+\frac{4C^{2}}{\alpha}\]

Notice that \(\sum_{m=1}^{M}n_{m}=l_{i}-l_{i-1}\) is the number of communication rounds within \([k_{l_{i-1}},k_{l_{i}})\), hence summing over \(i\) the total number of communication rounds is upper bounded by \(N(M+4C^{2}/\alpha)\). Combine this with the result in (22), we have the total number of communication rounds throughout the algorithm is

\[O\bigg{(}H\frac{(1+M\alpha)^{2}}{\alpha}\dim_{E}(\mathcal{F},\lambda/K)\log^{ 2}(K/\min\{1,\lambda\})\bigg{)}.\]

## Appendix C Proof of Auxiliary Lemmas

In this section we prove all the auxiliary lemmas in Section A.1 and Section \(B.1\). Note that some of these lemmas are very similar in nature, for which we will only give the proof for the version for the MDPs case, and briefly remark on the version for the bandit case.

### Proof of Lemma a.1 and Lemma b.1

Here we prove Lemma B.1 in detail. The proof for Lemma A.1 is very similar, and so we will only give a short remark on how to apply this to the bandit case.

Proof of Lemma b.1.: First, for an episode \(k\in[K]\) and agent \(m\in[M]\) such that \(m\) does not communicate with the server at episode \(k\) (either \(m\) is not participating or \(k\) is not a communication round), from the communication criterion we have

\[\alpha \geq\sum_{o_{h}\in Z^{\text{sup}}_{m,k,h}}\frac{b_{m,k,h}^{2}(a)}{ \beta_{k^{\prime},h}^{2}+\lambda}\] \[\geq\sum_{o_{h}\in Z^{\text{sup}}_{m,k,h}}D_{\lambda,\mathcal{F}_{ h}}^{2}(z_{h};Z^{\text{ser}}_{k^{\prime},h})\] \[=\sum_{o_{h}\in Z^{\text{sup}}_{m,k,h}}\sup_{f_{1},f_{2}\in \mathcal{F}_{h}}\frac{|f_{1}(z_{h})-f_{2}(z_{h})|^{2}}{\lambda+\|f_{1}-f_{2}\| _{Z^{\text{sup}}_{k^{\prime},h}}}\] \[\geq\sup_{f_{1},f_{2}\in\mathcal{F}_{h}}\frac{\|f_{1}-f_{2}\|_{Z^ {\text{sup}}_{m,k,h}}^{2}}{\lambda+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k^{\prime },h}}^{2}},\]

where \(k^{\prime}\) is the last communication round for agent \(m\). This means that for any \(f_{1},f_{2}\in\mathcal{F}_{h}\), \((1/\alpha)\|f_{1}-f_{2}\|_{Z^{\text{loc}}_{m,k,h}}^{2}\leq\lambda+\|f_{1}-f_{2 }\|_{Z^{\text{sup}}_{k^{\prime},h}}^{2}\). Observing that \(Z^{\text{ser}}_{k^{\prime},h}\subset Z^{\text{ser}}_{k,h}=\bigcup_{m^{\prime }=1}^{M}Z^{\text{up}}_{m^{\prime},k,h}\) proves the first conclusion that

\[\frac{1}{\alpha}\|f_{1}-f_{2}\|_{Z^{\text{loc}}_{m,k,h}}^{2}\leq\lambda+\sum_ {m^{\prime}=1}^{M}\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{m^{\prime},k,h}}^{2}.\]

Second, for any \(f_{1},f_{2}\in\mathcal{F}_{h}\), from the above conclusion we have for any \(k\in[K]\backslash\{k_{l}\}_{l=1}^{L}\) that

\[\lambda+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h}}^{2} =\lambda+\sum_{m=1}^{M}\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{m,k,h}}^{2}\] \[\geq\frac{1}{M\alpha}\sum_{m=1}^{M}\|f_{1}-f_{2}\|_{Z^{\text{sup} }_{m,k,h}}^{2}\] \[=\frac{1}{M\alpha}\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h}\backslash Z ^{\text{sup}}_{k,h}}^{2},\]

and when \(k=k_{l}\) for some \(l\in[L]\), we have alternatively

\[\lambda+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h}}^{2} =\lambda+\sum_{m^{\prime}\neq m_{t}}\|f_{1}-f_{2}\|_{Z^{\text{ sup}}_{m^{\prime},k,h}}^{2}+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{m_{k,k,h}}\cup Z^{ \text{sup}}_{m_{k,k,h}}}^{2}\] \[\geq\lambda+\sum_{m=1}^{M}\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{m,k,h}} ^{2}\] \[\geq\frac{1}{(M-1)\alpha}\sum_{m^{\prime}\neq m_{k}}\|f_{1}-f_{2} \|_{Z^{\text{sup}}_{m^{\prime},k,h}}^{2}\] \[\geq\frac{1}{M\alpha}\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h} \backslash Z^{\text{sup}}_{k,h}}^{2}.\]

Either way, we can deduce for any \(k\in[K]\) that

\[(1+M\alpha)\big{(}\lambda+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h}}^{2}\big{)} \geq\lambda+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h}}^{2}.\]

Finally, from the above we immediately have

\[D_{\lambda,\mathcal{F}}^{2}(z_{h};Z^{\text{ser}}_{k,h}) =\sup_{f_{1},f_{2}\in\mathcal{F}_{h}}\frac{[f_{1}(z_{h})-f_{2}(z_{ h})]^{2}}{\lambda+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h}}^{2}}\] \[\leq(1+M\alpha)\sup_{f_{1},f_{2}\in\mathcal{F}}\frac{[f_{1}(z_{h} )-f_{2}(z_{h})]^{2}}{\lambda+\|f_{1}-f_{2}\|_{Z^{\text{sup}}_{k,h}}^{2}}\] \[=(1+M\alpha)D_{\lambda,\mathcal{F}}^{2}(a;Z^{\text{all}}_{k,h}).\]_Remark C.1_.: Notice that this prove does not depend on the multi-level structure of episodic MDPs, but is a direct result of the communication criterion and protocol. This means the proof can be converted to the bandit case of Lemma A.1 without any essential changes: simply change episode \(k\) into time step \(t\), disregard all mentions of level \(h\), and consider \(z=a\) instead of \(z=(s,a)\).

### Proof of Lemma A.2 and Lemma B.2

We begin with the proof of Lemma A.2, which is an almost direct application of Lemma D.3.

Proof of Lemma a.2.: We invoke Lemma D.3 with \(\epsilon_{0}=0\), then with probability at least \(1-\delta\), for all

\[\sum_{(a,r)\in Z_{i}^{\text{\tiny{opt}}}}\left(\widehat{f}_{t+1}(a)-f^{*}(a) \right)^{2}\leq C_{\text{\tiny{ERM}}}\bigg{[}\lambda+\gamma^{2}T+\gamma TR+R^ {2}(1+M\alpha)\log(3N/\delta)+R^{2}M^{2}\alpha\log(3NM/\delta)\bigg{]}\leq \widetilde{\beta}_{1}^{2},\]

if we let \(\gamma=O(1/T)\) be sufficiently small and take \(\widetilde{\beta}_{1}=C_{\beta,1}\bigg{[}\sqrt{\lambda}+RC(M,\alpha)\log(3MN( \mathcal{F},\gamma)/\delta)\bigg{]}\) with \(C_{\beta,1}=\sqrt{C_{\text{\tiny{ERM}}}}=6\). Thus taking \(\beta_{t}=\widetilde{\beta}_{1}\), according to the definition of \(\mathcal{F}_{t+1}\), this directly implies \(f^{*}\in\mathcal{F}_{t+1}\).

With this, since the bonus function satisfy

\[b_{t+1}(a)\geq|f_{1}(a)-f_{2}(a)|,\quad\forall f_{1},f_{2}\in\mathcal{F}\quad \text{s.t.}\quad\sum_{(a,r)\in Z_{t}^{\text{\tiny{opt}}}}\left(f_{1}(a)-f_{2}( a)\right)^{2}\leq\beta_{t}^{2},\]

which is based on the first property of the bonus oracle in Definition 4.1, by taking \(f_{1}=\widehat{f}_{t+1}\) and \(f_{2}=f^{*}\) we get for any \(a\in\mathcal{A}\) that \(b_{t+1}(a)\geq|f_{*}(a)-\widehat{f}_{t+1}(a)|\), which finishes the proof. 

Next we prove Lemma B.2, which is more challenging and requires an analysis on the least squares value iteration method.

Proof of Lemma b.2.: Take \(\mathcal{F}_{h+1,\gamma}\) as a \(\gamma\)-cover of \(\mathcal{F}_{h+1}\), and \(\mathcal{W}_{h+1,\gamma}\) as a \(\gamma\)-cover of \(\mathcal{W}_{h+1}\). Select \(\widetilde{f}_{k+1,h+1}\in\mathcal{F}_{h+1,\gamma}\bigoplus\mathcal{W}_{h+1,\gamma}\) so that \(\|Q_{k+1,h+1}-\widetilde{f}_{k+1,h+1}\|_{\infty}\leq\bar{\epsilon}:=(1+\beta _{k+1,h+1})\gamma\). For \(o_{h}=(s_{h},a_{h},r_{h},s_{h+1})\), define the corresponding \(y_{h}=r_{h}+V_{k+1,h+1}(s_{h+1})\) and \(\bar{y}_{h}=r_{h}+\sup_{a\in\mathcal{A}}\widetilde{f}_{k+1,h+1}(s_{h+1},a)\). Let

\[\widetilde{f}_{k+1,h}=\operatorname*{argmin}_{f_{h}\in\mathcal{F}_{h}}\sum_{o_ {h}\in Z_{k,h}^{\text{\tiny{opt}}}}\big{(}f_{h}(s_{h},a_{h})-\bar{y}_{h}\big{)} ^{2}.\]

Then we have

\[\bigg{(}\sum_{o_{h}\in Z_{k,h}^{\text{\tiny{opt}}}}\left(\widehat {f}_{k+1,h}(s_{h},a_{h})-\bar{y}_{h}\right)^{2}\bigg{)}^{1/2} \leq\bigg{(}\sum_{o_{h}\in Z_{k,h}^{\text{\tiny{opt}}}}\left( \widehat{f}_{k+1,h}(s_{h},a_{h})-y_{h}\right)^{2}\bigg{)}^{1/2}+\bar{\epsilon} \sqrt{k}\] \[\leq\bigg{(}\sum_{o_{h}\in Z_{k,h}^{\text{\tiny{opt}}}}\left( \widetilde{f}_{k+1,h}(s_{h},a_{h})-y_{h}\right)^{2}\bigg{)}^{1/2}+\bar{\epsilon} \sqrt{k}\] \[\leq\bigg{(}\sum_{o_{h}\in Z_{k,h}^{\text{\tiny{opt}}}}\left( \widetilde{f}_{k+1,h}(s_{h},a_{h})-\bar{y}_{h}\right)^{2}\bigg{)}^{1/2}+2\bar{ \epsilon}\sqrt{k}.\]

Now notice that \(\mathbb{E}\bar{y}_{h}=\mathcal{T}_{h}\bar{f}_{k+1,h}(s_{h},a_{h})\), and the difference \(\bar{y}_{h}-\mathcal{T}_{h}\bar{f}_{k+1,h}(s_{h},a_{h})\) is bounded in \([-H,H]\), hence we may apply Lemma D.3 with \(f^{*}=\mathcal{T}_{h}\bar{f}_{k+1,h}\), \(r_{t}=\bar{y}_{h}\), \(R=H\), \(\epsilon_{0}=2\bar{\epsilon}\) and \(\delta=\delta/3HN(\mathcal{F}_{h+1},\gamma)\cdot N(\mathcal{W}_{h+1},\gamma)\), taking a union bound over \(\bar{f}\in\mathcal{F}_{h+1,\gamma}\bigoplus\mathcal{W}_{h+1,\gamma}\) and \(h\in[H]\), we have

\[\bigg{(}\sum_{o_{h}\in Z^{\text{\tiny{cor}}}_{k,h}}\big{(}\bar{f}_{k+ 1,h}(s_{h},a_{h})-\mathcal{T}_{h}Q_{k+1,h+1}(s_{h},a_{h})\big{)}^{2}\bigg{)}^{1/2}\] \[\leq\bigg{(}\sum_{o_{h}\in Z^{\text{\tiny{cor}}}_{k,h}}\big{(}\bar {f}_{k+1,h}(s_{h},a_{h})-\mathcal{T}_{h}\bar{f}_{k+1,h+1}(s_{h},a_{h})\big{)}^{2 }\bigg{)}^{1/2}+\gamma\sqrt{k}\] \[\leq\sqrt{C_{\text{ERM}}}\sqrt{\lambda+(\gamma+2\bar{\epsilon})^{2 }K+(\gamma+2\bar{\epsilon})KH+H^{2}(1+M\alpha)\log(3HN_{h}(\gamma)/\delta)+H^{2 }M^{2}\alpha\log(3HMN_{h}(\gamma)/\delta)}+\gamma\sqrt{k}\] \[\leq\sqrt{C_{\text{ERM}}}\bigg{[}\sqrt{\lambda}+\gamma(3+2\beta_{ k+1,h+1})\sqrt{K}+\sqrt{\gamma(3+2\beta_{k+1,h+1})KH}+HC(M,\alpha)\sqrt{\log(3HMN_{h} (\gamma)/\delta)}\bigg{)}\bigg{]},\]

where \(N_{h}(\gamma)=N(\mathcal{F}_{h},\gamma)\cdot N(\mathcal{F}_{h+1},\gamma)\cdot N (\mathcal{W}_{h+1},\gamma)\). By taking \(\gamma=1/(C_{\gamma}KH)\) with sufficiently large absolute constant \(C_{\gamma}\) (for example, \(C_{\gamma}=20\)), the second and third terms within the bracket above are both less than \((1/2)\beta_{k+1,h+1}\), and hence we can easily prove via induction on \(h\) that the above is no greater than \(\widetilde{\beta}_{2}\), where

\[\widetilde{\beta}_{2}=C_{\beta,2}\bigg{[}\sqrt{\lambda}+HC(M,\alpha)\sqrt{ \log(3HMN(\gamma)/\delta)}\bigg{]}\]

with \(C_{\beta,2}=2\sqrt{C_{\text{ERM}}}=12\) and \(N(\gamma)=\max_{h\in[H]}N_{h}(\gamma)\).

### Proof of Lemma a.3 and Lemma b.3

In this section we prove Lemma B.3 in detail. The proof for Lemma A.3 is very similar, and so we will again only give a short remark on how to apply this to the bandit case.

Proof of Lemma b.3.: We fix the level \(h\in[H]\) throughout the proof. For an index set \(\mathcal{K}_{0}\subseteq[K]\), we denote \(\mathcal{Z}(\mathcal{K}_{0}):=\{z_{h}^{k}:k\in\mathcal{K}_{0}\}\).

First, let \(n=\lceil\log(K/\lambda)/\log 2\rceil\), and we divide the set of episodes \(\mathcal{K}=[K]\) into \(n+1\) disjoint episode sets as follows. For any \(1\leq l\leq L\) and \(k_{l}\leq k<k_{l+1}\), let

\[(\bar{f}_{k,1},\bar{f}_{k,2})=\operatorname*{argmax}_{f_{1},f_{2}\in \mathcal{F}_{h}}\frac{\big{(}f_{1}(z_{h}^{k})-f_{2}(z_{h}^{k})\big{)}^{2}}{ \lambda+\|f_{1}-f_{2}\|_{Z^{\text{\tiny{th}}}_{k,k-1}}^{2}},\]

and define \(L_{k}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) as \(L_{k}(z)=\big{(}\bar{f}_{k,1}(z)-\bar{f}_{k,2}(z)\big{)}^{2}\). Now we define \(\mathcal{K}^{\iota}:=\{k\in\mathcal{K}:L_{k}(z_{h}^{k})\in(2^{-\iota-1},2^{- \iota}]\}\) for \(\iota\in\{0,1,\cdots,n-1\}\) and \(\mathcal{K}^{\iota}:=\{k\in\mathcal{K}:L_{k}(z_{h}^{k})\in[0,2^{-n}]\}\). We note that for \(k\in\mathcal{K}^{\iota}\), \(L_{k}(z_{h}^{k})\leq\lambda/K\).

Now define the mapping \(\tau:[K]\to[K]\), such that for any \(k\in[K]\), \(\tau(k)\) is the last episode when agent \(m_{k}\) communicated with the server (not including \(k\)). We will bound \(\sum_{k\in\mathcal{K}^{\iota}}D^{2}_{\lambda,\mathcal{F}_{h}}(z_{h}^{k};Z^{ \text{\tiny{th}}}_{k,k-1})\) for \(\iota\in\{0,\cdots,n-1\}\).

For a fixed \(\iota\leq n-1\), we now decompose \(\mathcal{K}^{\iota}=\bigcup_{j=1}^{n^{\iota}+1}\mathcal{K}^{\iota}_{j}\), where \(n^{\iota}=\big{\lceil}|\mathcal{K}^{\iota}|/\dim_{E}(\mathcal{F}_{h},2^{- \iota-1})\big{\rceil}\). We start off each set \(\mathcal{K}^{\iota}_{j}=\varnothing\), and fill them up gradually by iterating through \(k\in\mathcal{K}^{\iota}\) one by one in increasing order to decide which subset \(\mathcal{K}^{\iota}_{j}\) should \(k\) belong to. Specifically, we define \(j(k)\) to be the smallest index \(j<n^{\iota}\) such that is \(z_{h}^{k}\) is \(2^{-(\iota+1)/2}\)-independent of \(\mathcal{Z}(\mathcal{K}^{\iota}_{j})\), and assign \(k\) to the set \(\mathcal{K}^{\iota}_{j(k)}\). If such a \(j\) does not exist, we simply let \(j(k)=n^{\iota}+1\) assign \(k\) to \(\mathcal{K}^{\iota}_{n^{\iota}+1}\). Finally after the assignment process, we define \(\mathcal{K}^{\iota}_{j,k}=\mathcal{K}^{\iota}_{j}\cap[k]\) for any \(k\in[K]\). Then we have the elements added into \(\mathcal{K}^{\iota}_{j(k)-1,k}\) form a sequence where each data corresponding to a new member is \(2^{-(\iota+1)/2}\)-independent of the old members, and so there are no more than \(\dim_{E}(\mathcal{F}_{h},2^{-\iota-1})\) members within each of them. Moreover, for all \(k\in\mathcal{K}^{\iota}\) that \(z_{h}^{k}\) is \(2^{-(\iota+1)/2}\)-dependent on each of \(\mathcal{Z}(\mathcal{K}^{\iota}_{1,k}),\cdots,\mathcal{Z}(\mathcal{K}^{\iota}_{j( k)-1,k})\).

Now for any \(k\in\mathcal{K}^{\iota}\) by the definition of \(\mathcal{K}^{\iota}\), we have \(\big{(}\bar{f}_{k,1}(z_{h}^{k})-\bar{f}_{k,2}(z_{h}^{k})\big{)}^{2}\geq 2^{- \iota-1}\). This combined with the \(2^{-\iota-1}\)-dependencies imply that for each \(j^{\prime}=1,\cdots,j(k)-1\), \(\|f_{k,1}-\bar{f}_{k,2}\|_{\mathcal{Z}(\mathcal{K}^{\iota}_{j^{\prime},k})}^{2} \geq 2^{-\iota-1}\).

Notice that \(\mathcal{Z}(\mathcal{K}^{i}_{j^{\prime},k})\subset Z^{\text{all}}_{h,k-1}\) for any \(j^{\prime}\in[j(k)-1]\), and that \(\mathcal{Z}(\mathcal{K}^{i}_{j^{\prime},k})\) for \(j^{\prime}\in[j(k)-1]\) are disjoint, therefore

\[(j(k)-1)2^{-\iota-1}\leq\sum_{j^{\prime}=1}^{j(k)-1}\|\bar{f}_{k,1}-\bar{f}_{k,2}\|^{2}_{\mathcal{Z}(\mathcal{K}^{i}_{j^{\prime},k})}\leq\|\bar{f}_{k,1}- \bar{f}_{k,2}\|^{2}_{Z^{\text{all}}_{h,k-1}}.\]

It follows that

\[D^{2}_{\lambda,\mathcal{F}_{h}}(z^{k}_{h};Z^{\text{all}}_{h,k-1}) =\frac{\big{(}\bar{f}_{k,1}(z^{k}_{h})-\bar{f}_{k,2}(z^{k}_{h}) \big{)}^{2}}{\lambda+\|\bar{f}_{k,1}-\bar{f}_{k,2}\|^{2}_{Z^{\text{all}}_{h,k- 1}}}\] \[\leq\frac{2^{-\iota}}{\lambda+(j(k)-1)2^{-\iota-1}}\] \[=\frac{2}{(j(k)-1)+2^{\iota+1}\lambda},\]

where the first inequality uses the definition of \(\mathcal{K}^{i}\). Summing over \(k\in\mathcal{K}^{i}\), we have

\[\sum_{k\in\mathcal{K}^{i}}D^{2}_{\lambda,\mathcal{F}_{h}}(z^{k}_ {h};Z^{\text{all}}_{h,k-1}) =\sum_{j=1}^{n^{\iota}+1}\sum_{k\in\mathcal{K}^{i}_{j}}D^{2}_{ \lambda,\mathcal{F}_{h}}(z^{k}_{h};Z^{\text{all}}_{h,k-1})\] \[\leq\sum_{j=1}^{n^{\iota}}\frac{2\big{|}\mathcal{K}^{i}_{j} \big{|}}{(j-1)+2^{\iota+1}\lambda}+\frac{2\big{|}\mathcal{K}^{\iota}_{n^{\iota }+1}\big{|}}{n^{\iota}+2^{\iota+1}\lambda}\] \[\leq\frac{2\dim_{E}(\mathcal{F}_{h},2^{-\iota-1})}{2^{\iota+1} \lambda}+\sum_{j=2}^{n^{\iota}}\frac{2\dim_{E}(\mathcal{F}_{h},2^{-\iota-1}) }{j-1}+2\big{|}\mathcal{K}^{i}\big{|}\cdot\frac{\dim_{E}(\mathcal{F}_{h},2^{- \iota-1})}{\big{|}\mathcal{K}^{i}\big{|}}\] \[\leq\dim_{E}(\mathcal{F}_{h},2^{-\iota-1})\big{(}2\log n^{\iota} +4+1/(2^{\iota}\lambda)\big{)},\]

where we used the relation \(\big{|}\mathcal{K}^{i}_{j}\big{|}\leq\dim_{E}(\mathcal{F}_{h},2^{-\iota-1})\) and the definition of \(n^{\iota}\) in the second inequality. Additionally, for \(\iota=n\) we also have

\[\sum_{k\in\mathcal{K}^{n}}D^{2}_{\lambda,\mathcal{F}_{h}}(z^{k}_{h};Z^{\text{all }}_{h,k-1})\leq\sum_{k\in\mathcal{K}^{n}}\frac{L_{k}(z^{k}_{h})}{\lambda}\leq| \mathcal{K}^{n}|\cdot\frac{\lambda/K}{\lambda}\leq 1,\]

and so finally we sum over \(\iota=0,\cdots,n\) to get

\[\sum_{k=1}^{K}D^{2}_{\lambda,\mathcal{F}_{h}}(z^{k}_{h};Z^{\text{all}}_{h,k- 1}) \leq\sum_{\iota=0}^{n-1}\dim_{E}(\mathcal{F}_{h},2^{-\iota-1}) \big{(}2\log n^{\iota}+4+1/(2^{\iota}\lambda)\big{)}+1\] \[\leq n\dim_{E}(\mathcal{F}_{h},2^{-n})\big{(}2\log K+4+1/\lambda \big{)}+1\] \[\leq C\dim_{E}(\mathcal{F}_{h},\lambda/K)\log(K/\min\{1,\lambda\}),\]

where the final step makes the assumption that \(\lambda=O(1/\log K)\), in which case it holds with some absolute constant \(C_{D}\). 

_Remark C.2_.: Again, this prove does not depend on the multi-level structure of episodic MDPs. In fact, it only relies on the Eluder dimensionality of \(\mathcal{F}_{h}\). This means the proof can be converted to the bandit case of Lemma A.3 without any essential changes: simply change episode \(k\) into time step \(t\), disregard all mentions of level \(h\), and consider \(z=a\) instead of \(z=(s,a)\).

## Appendix D Technical Lemmas

In this section, we provide a technical concentration lemma that serves as the core of our results. For one, this lemma is based on the following concentration inequality:

**Lemma D.1**.: _For a sequence of random variables \(\{Z_{t}\}_{t\in\mathbb{N}}\) adapted to the filtration \(\{\mathcal{S}_{t}\}_{t\in\mathbb{N}}\) and function \(f\in\mathcal{F}\), for any \(\lambda>0\), with probability at least \(1-\delta\), for all \(t\in\mathbb{N}\), we have_

\[-\frac{1}{\lambda}\sum_{s=1}^{t}\log\mathbb{E}\big{[}\exp[-\lambda f(Z_{s})] \big{|}\mathcal{S}_{s-1}\big{]}-\sum_{s=1}^{t}f(Z_{s})\leq\frac{1}{\lambda \delta}.\]The proof for this lemma can be found under Lemma 4 of Russo and Van Roy (2013). Apart from this, we need yet another basic concentration lemma:

**Lemma D.2**.: _Suppose \(\{\eta_{t}\}_{t=1}^{T}\) is a sequence of conditional \(R\)-sub-Gaussian random variables satisfying \(\mathbb{E}\big{[}e^{\mu\eta_{t}}|\mathcal{H}_{t-1}\big{]}\leq\exp\big{(}R^{2} \mu^{2}/2\big{)}\), where \(\mathcal{H}_{t-1}\) denotes all history before time \(t\), with probability \(1-\delta\), we have_

\[\sum_{t=1}^{T}\eta_{t}^{2}\leq 2T\sigma^{2}+3\sigma^{2}\log(1/\delta).\]

A proof of this lemma can be found under Lemma G.2 of Ye et al. (2023). With this, we can prove the following lemma characterizing the accuracy of least squares solution. Even though we need this lemma for both bandit and RL settings, we will follow the notations presented in multi-agent contextual bandits. Detailed explanation of how this translates to multi-agent MDPs can be found in Section C.2.

**Lemma D.3**.: _Suppose we have a sequence of inputs \(\{(a_{t},r_{t})\}_{t=1}^{T}\) that follow the rule \(r_{t}=f^{*}(a_{t})+\eta_{t}\) for some ground truth \(f^{*}\in\mathcal{F}\), with \(\eta_{t}\) being conditionally \(R\)-sub-Gaussian:_

\[\mathbb{E}\big{[}e^{\mu\eta_{t}}\big{|}a_{1:t},r_{1:t-1}\big{]}\leq\exp(R^{2} \mu^{2}/2),\forall\mu\in\mathbb{R}.\]

_We also have server datasets \(Z_{t}^{\text{ser}}\) at different time steps, collected following the communication protocol in our settings. Note that strictly speaking, the conditions under which \(\eta_{t}\) is sub-Gaussian should also include the former participants \(m_{1:t}\), but we will omit this dependency for convenience. Consider \(\widehat{f}_{t+1}^{\text{ser}}\), the approximate ERM solution to the least squares problem:_

\[\bigg{(}\sum_{(a,r)\in Z_{t}^{\text{ser}}}\big{(}\widehat{f}_{t+1}^{\text{ser }}(a)-r\big{)}^{2}\bigg{)}^{1/2}\leq\min_{f\in\mathcal{F}_{t}}\bigg{(}\sum_{(a,r)\in Z_{t}^{\text{ser}}}\big{(}f(a)-r\big{)}^{2}\bigg{)}^{1/2}+\epsilon_{0} \sqrt{t},\]

_Then abbreviating \(N=N(\mathcal{F},\gamma)\) and taking \(C_{\text{ERM}}=36\), with probability at least \(1-\delta\),_

\[\sum_{(a,r)\in Z_{t}^{\text{ser}}}\big{(}\widehat{f}_{t+1}^{\text{ser}}(a)-f^ {*}(a)\big{)}^{2}\leq C_{\text{ERM}}\bigg{[}\lambda+(\gamma+\epsilon_{0})^{2} T+(\gamma+\epsilon_{0})TR+R^{2}(1+M\alpha)\log(3N/\delta)+R^{2}M^{2}\alpha\log(3NM/ \delta)\bigg{]}.\]

Proof of Lemma D.3.: Let \(\mathcal{F}_{\gamma}\) be a \(\gamma\)-cover of the function class \(\mathcal{F}\) with respect to the infinity norm \(\|\cdot\|_{\infty}\). For \(f\in\mathcal{F}\) and \((a_{t},r_{t})\) for some \(t\in[T]\), let

\[\phi(f,a_{t},r_{t})=-(f(a_{t})-r_{t})^{2}+(f^{*}(a_{t})-r_{t})^{2},\]

Since \(r_{t}=f^{*}(a_{t})+\eta_{t}\), we can write \(\phi(f,a_{t},r_{t})\) as

\[\phi(f,a_{t},r_{t}) =-\big{(}f(a_{t})-f^{*}(a_{t})+\eta_{t}\big{)}^{2}+\eta_{t}^{2}\] \[=-2\big{(}f(a_{t})-f^{*}(a_{t})\big{)}\eta_{t}-\big{(}f(a_{t})-f^ {*}(a_{t})\big{)}^{2}\]

Since \(\eta_{t}\) is \(R\)-sub-Gaussian conditional on \(Z_{t-1}^{\text{all}},a_{t}\), we have for any positive parameter \(\mu\) that

\[\log\mathbb{E}\big{[}\exp(\mu\phi(f,a_{t},r_{t}))\big{|}Z_{t-1}^{ \text{all}},a_{t}\big{]} \leq 2\mu^{2}R^{2}(f(a_{t})-f^{*}(a_{t}))^{2}-\mu(f(a_{t})-f^{*}(a_{ t}))^{2}\] \[=(2\mu^{2}R^{2}-\mu)(f(a_{t})-f^{*}(a_{t}))^{2}\]

Using Lemma D.1, we have with probability at least \(1-\delta/3\), for all \(f\in\mathcal{F}_{\gamma}\) and \(t\in[T]\),

\[\mu_{\text{all}}\sum_{(a,r)\in Z_{t}^{\text{all}}}\phi(f,a,r)\leq(2\mu_{\text{ all}}^{2}R^{2}-\mu_{\text{all}})\sum_{(a,r)\in Z_{t}^{\text{all}}}(f(a)-f^{*}(a))^{2}+ \log(3N/\delta),\] (26)

where \(\mu_{\text{all}}>0\) is a parameter we will determine later.

On the other hand, if we consider any local agent \(m\), when \(m_{t}=m\), we have \(\eta_{t}\) is \(R\)-sub-Gaussian conditional on \(Z_{m,t-1}^{\text{up}}\cup Z_{m,t-1}^{\text{loc}}\) and \(a_{t}\), i.e. all the data agent \(m\) has received from the environment up to this point. Thus we have for any \(\mu>0\) that

\[\log\mathbb{E}\big{[}\exp(-\mu\phi(f,a_{t},r_{t}))\big{|}Z_{m,t-1} ^{\text{up}}\cup Z_{m,t-1}^{\text{loc}},a_{t}\big{]} \leq 2\mu^{2}R^{2}(f(a_{t})-f^{*}(a_{t}))^{2}+\mu(f(a_{t})-f^{*}(a_{t} ))^{2}\] \[=(2\mu^{2}R^{2}+\mu)(f(a_{t})-f^{*}(a_{t}))^{2}\]Then again using Lemma D.1 and taking summation on \(Z^{\text{loc}}_{m,t}\), with probability at least \(1-\delta/3\), the following holds for any \(m\in[M]\):

\[-\mu_{\text{loc}}\sum_{(a,r)\in Z^{\text{loc}}_{m,t}}\phi(f,a,r)\leq(2\mu_{\text {loc}}^{2}R^{2}+\mu_{\text{loc}})\sum_{(a,r)\in Z^{\text{loc}}_{m,t}}(f(a)-f^{ *}(a))^{2}+\log(3NM/\delta),\] (27)

where \(\mu_{\text{loc}}>0\) is a parameter we will determine later.

Taking the summation of (27) for all \(m\in[M]\) and combining (26), while observing that \(Z^{\text{ser}}_{t}=Z^{\text{all}}_{t}\backslash\bigcup_{m=1}^{M}Z^{\text{loc}}_ {m,t}\), we get

\[\sum_{(a,r)\in Z^{\text{ser}}_{t}}\phi(f,a,r) =\sum_{(a,r)\in Z^{\text{sls}}_{t}}\phi(f,a,r)-\sum_{m=1}^{M}\sum_{ (a,r)\in Z^{\text{loc}}_{m,t}}\phi(f,a,r)\] \[\leq(2\mu_{\text{all}}R^{2}-1)\sum_{(a,r)\in Z^{\text{sls}}_{t}}( f(a)-f^{*}(a))^{2}+\frac{1}{\mu_{\text{all}}}\log(3N/\delta)\] \[\qquad+(2\mu_{\text{loc}}R^{2}+1)\sum_{m=1}^{M}\sum_{(a,r)\in Z^{ \text{loc}}_{m,t}}(f(a)-f^{*}(a))^{2}+\frac{1}{\mu_{\text{loc}}}M\log(3NM/\delta)\] \[=2R^{2}(\mu_{\text{all}}+\mu_{\text{loc}})\|f-f^{*}\|_{Z^{\text{ int}}_{t}}^{2}-(2\mu_{\text{loc}}R^{2}+1)\|f-f^{*}\|_{Z^{\text{ser}}_{t}}^{2}\] \[\qquad+\frac{1}{\mu_{\text{all}}}\log(3N/\delta)+\frac{1}{\mu_{ \text{loc}}}M\log(3NM/\delta).\]

From Lemma A.1, we have \(\lambda+\|f-f^{*}\|_{Z^{\text{sls}}_{t}}^{2}\leq(1+M\alpha)\big{(}\lambda+\|f -f^{*}\|_{Z^{\text{ser}}_{t}}^{2}\big{)}\Leftrightarrow\|f-f^{*}\|_{Z^{\text{ int}}_{t}}^{2}\leq M\alpha\lambda+(1+M\alpha)\|f-f^{*}\|_{Z^{\text{sur}}_{t}}^{2}\). Plugging this inequality into the above and letting \(\mu_{\text{all}}=1/8R^{2}(1+M\alpha)\) and \(\mu_{\text{loc}}=1/8R^{2}M\alpha\), we get

\[\sum_{(a,r)\in Z^{\text{ser}}_{t}}\phi(f,a,r) \leq 2R^{2}(\mu_{\text{all}}+\mu_{\text{loc}})M\alpha\lambda- \big{(}1-2M\alpha\mu_{\text{loc}}R^{2}-2(1+M\alpha)\mu_{\text{all}}R^{2}\big{)} \|f-f^{*}\|_{Z^{\text{sur}}_{t}}^{2}\] \[\qquad+\frac{1}{\mu_{\text{all}}}\log(3N/\delta)+\frac{1}{\mu_{ \text{loc}}}M\log(3NM/\delta)\] \[\leq-\frac{1}{2}\|f-f^{*}\|_{Z^{\text{sur}}_{t}}^{2}+\frac{1}{2} \lambda+8R^{2}(1+M\alpha)\log(3N/\delta)+8R^{2}M^{2}\alpha\log(3NM/\delta).\] (28)

Now for \(\widehat{f}^{\text{ser}}_{t+1}\), there exists \(\widetilde{f}\in\mathcal{F}_{\gamma}\) such that \(\|\widetilde{f}-\widehat{f}^{\text{ser}}_{t+1}\|_{\infty}\leq\gamma\). Using Lemma D.2, this gives us the following with probability at least \(1-\delta/3\):

\[-\sum_{(a,r)\in Z^{\text{ser}}_{t}}\phi(\widetilde{f},a,r) =\sum_{(a,r)\in Z^{\text{ser}}_{t}}\left[\big{(}\widetilde{f}(a) -r\big{)}^{2}-\big{(}f^{*}(a)-r\big{)}^{2}\right]\] \[\leq\left(\sqrt{\sum_{(a,r)\in Z^{\text{sur}}_{t}}\big{(} \widehat{f}^{\text{ser}}_{t+1}(a)-r\big{)}^{2}}+\sqrt{t\gamma^{2}}\right)^{2}- \sum_{(a,r)\in Z^{\text{ser}}_{t}}\big{(}f^{*}(a)-r\big{)}^{2}\] \[\leq\left(\sqrt{\sum_{(a,r)\in Z^{\text{sur}}_{t}}\big{(}f^{*}(a )-r\big{)}^{2}}+\sqrt{t}(\gamma+\epsilon_{0})\right)^{2}-\sum_{(a,r)\in Z^{ \text{ser}}_{t}}\big{(}f^{*}(a)-r\big{)}^{2}\] \[=(\gamma+\epsilon_{0})^{2}t+2(\gamma+\epsilon_{0})\sqrt{t}\bigg{(} \sum_{s=1}^{t}\eta_{s}^{2}\bigg{)}^{1/2}\] \[\leq(\gamma+\epsilon_{0})^{2}t+2(\gamma+\epsilon_{0})\sqrt{2T^{2} R^{2}+3TR^{2}\log(3/\delta)},\]

where we used the basic inequality \(\sqrt{\sum(a+b)^{2}}\leq\sqrt{\sum a^{2}}+\sqrt{\sum b^{2}}\) in the first inequality and used the property of \(\widehat{f}^{\text{ser}}_{t+1}\) in the second inequality. Finally, taking a union bound and combining this with (28), we have with probability at least \(1-\delta\),

\[\sum_{(a,r)\in Z_{t}^{\text{err}}}\left(\widehat{f}_{t+1}^{\text{ ser}}(a)-f^{*}(a)\right)^{2}\] \[\leq 2\gamma^{2}t+2\sum_{(a,r)\in Z_{t}^{\text{err}}}\left(\widetilde {f}(a)-f^{*}(a)\right)^{2}\] \[\leq 2\gamma^{2}t-2\sum_{(a,r)\in Z_{t}^{\text{err}}}\phi(\widetilde {f},a,r)+\lambda+32R^{2}(1+M\alpha)\log(3N/\delta)+32R^{2}M^{2}\alpha\log(3NM/\delta)\] \[\leq 2\gamma^{2}T+2(\gamma+\epsilon_{0})^{2}T+4(\gamma+\epsilon_{ 0})\sqrt{2T^{2}R^{2}+3TR^{2}\log(3/\delta)}+\lambda+32R^{2}(1+M\alpha)\log(3N/ \delta)+32R^{2}M^{2}\alpha\log(3NM/\delta)\] \[\leq C_{\text{ERM}}\bigg{[}\lambda+(\gamma+\epsilon_{0})^{2}T+( \gamma+\epsilon_{0})TR+R^{2}(1+M\alpha)\log(3N/\delta)+R^{2}M^{2}\alpha\log(3 NM/\delta)\bigg{]},\]

where the first inequality uses again \(\|\widetilde{f}-\widehat{f}_{t+1}^{\text{err}}\|_{\infty}\leq\gamma\), and it can be verified that the last inequality holds when \(C_{\text{ERM}}\geq 36\).

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We accurately summarize our contribution and main results in the abstract, and elaborate further on motivation and main techniques in our introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: All assumptions made for our theoretical analysis are present and stated clearly within the main paragraphs of our paper. Discussions on the applicability of these assumptions are also included. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All assumptions are listed in the main paper, while a very detailed and sound proof is displayed in the appendices. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our theoretical paper does not present any experimental results. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [NA]

Justification: Our theoretical paper does not present any experimental results.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Our theoretical paper does not present any experimental results. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Our theoretical paper does not present any experimental results. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Our theoretical paper does not present any experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have thoroughly reviewed the NeurIPS Code of Ethics, and believe our work conforms fully the the Code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Impact Statement before appendices Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our theoretical paper does not present any experimental results, and thus does not feature such data and models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our paper does not include any such assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our theoretical paper does not introduce any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our theoretical paper does not present any experimental results. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our theoretical paper does not present any experimental results. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.