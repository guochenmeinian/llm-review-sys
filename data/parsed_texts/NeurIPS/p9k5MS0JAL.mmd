# Demystifying the Optimal Performance

of Multi-Class Classification

 Minoh Jeong

Electrical and Computer Engineering

University of Minnesota

Minneapolis, MN 55455

jeong316@umn.edu

&Martina Cardone

Electrical and Computer Engineering

University of Minnesota

Minneapolis, MN 55455

mcardone@umn.edu

&Alex Dytso

Qualcomm Flarion Technology, Inc.

Bridgewater, NJ 08807

odytso2@gmail.com

###### Abstract

Classification is a fundamental task in science and engineering on which machine learning methods have shown outstanding performances. However, it is challenging to determine whether such methods have achieved the Bayes error rate, that is, the lowest error rate attained by any classifier. This is mainly due to the fact that the Bayes error rate is not known in general and hence, effectively estimating it is paramount. Inspired by the work by Ishida et al. (2023), we propose an estimator for the Bayes error rate of supervised multi-class classification problems. We analyze several theoretical aspects of such estimator, including its consistency, unbiasedness, convergence rate, variance, and robustness. We also propose a denoising method that reduces the noise that potentially corrupts the data labels, and we improve the robustness of the proposed estimator to outliers by incorporating the median-of-means estimator. Our analysis demonstrates the consistency, asymptotic unbiasedness, convergence rate, and robustness of the proposed estimators. Finally, we validate the effectiveness of our theoretical results via experiments both on synthetic data under various noise settings and on real data.

## 1 Introduction

Supervised classification problems are typical tasks in various fields of science and engineering, such as machine learning, statistical signal processing, estimation, and detection. In supervised classification, a dataset consisting of several input feature-label pairs is given. The goal of a supervised classification task is to design effective classifiers, by leveraging the given dataset, to suitably label future input features, or equivalently, to classify future input features into one of the classes.

As the dataset is the only available resource on the data distribution, the performance of a classifier is typically measured by its empirical misclassification rate on the test dataset (which is a subset of the given dataset). The best performance of an existing classifier, that is, the so-called state-of-the-art (SOTA) performance, tends to be the point of reference to measure the improvement of a new classifier. However, there are no guarantees that the SOTA performance is close to the theoretical minimum misclassification rate, namely the Bayes error rate (BER). Thus, comparing the empirical misclassification rate with the SOTA error rate provides only a relative improvement.

Having the knowledge of the BER is essential in theory and practice. The BER indeed provides a fundamental limit on the misclassification rate, which is critical for designing high-performingclassifiers. Moreover, one can leverage the BER to assess how good the SOTA performance is with respect to the theoretically optimal error rate. If the SOTA performance is nearly close to the BER, we can avoid wasting time and effort in designing a new classifier. Furthermore, the BER indicates the inherent hardness of a task and hence, it can be seen as a benchmark for comparing the hardness of different tasks [13; 46; 83]. Knowing the BER also brings the opportunity to detect whether test dataset overfitting occurs (which has sporadically happened [3; 51; 59; 91]); this overfitting can be detected since the BER provides the minimum misclassification rate and hence, no classifier will perform strictly better than it. We refer an interested reader to Appendix A for a thorough literature overview on methods to estimate and bound the BER.

In this paper, we investigate the problem of estimating the BER of an \(M\)-class classification task directly from a dataset, where \(M\geq 2\) is arbitrary. Our technique to estimate the BER is different from a plug-in approach that first estimates the distribution from which the data is drawn, and then evaluates the BER. Indeed, our BER estimators, which are proved to be unbiased, consistent and robust to label noise and outliers, do not require the estimation of the data probability density to perform an effective BER estimation. We start by assuming that the data labels are soft and real-valued, approximating the posterior probability of the class. We then relax this assumption on the data labels, and show the applicability of our estimators on one-hot labels and other noisy datasets.

Contributions.Our contribution is summarized as follows. Inspired by [46], in Section 3 we first propose a BER estimator for the case of soft data labels, and we show that it benefits from several appealing properties, e.g., it is consistent, unbiased and asymptotically normal. In Section 3, we also propose a methodology, inspired by the median-of-means estimator [65], to make any BER estimator robust. Then, in Section 4 we study the performance of the proposed estimators in scenarios where the soft labels are corrupted by two typical types of noise, i.e., the case of a noise that permutes/shuffles the labels, and the case of additive noise. For the first type of noise, our estimators have similar properties as in the noiseless case. However, for the additive noise case, our proposed estimators are not consistent. Because of this, we propose a denoising method that averages noisy labels associated with the same feature. The corresponding constructed estimator is shown to be consistent. In Section 4, we also showcase that the noisy soft label framework can be used to study the case of one-hot labels, and we provide a denoising method that encompasses the one proposed for the case of additive noise. In Section 5 we validate the effectiveness of the proposed estimators via experiments both on synthetic data under various noise settings (e.g., one-hot labels) and on real data using three different datasets, i.e., CIFAR-10H [4], Fashion-MNIST-H [46] and MovieLens [38]. Finally, in Section 6 we conclude the paper with some discussion on future research directions, which are worth further investigation.

Notation.Deterministic scalar quantities are denoted by lowercase letters, scalar random variables are denoted by uppercase letters, vectors are denoted by bold lowercase letters, and random vectors by bold uppercase letters (e.g., \(x\), \(X\), \(\bm{x}\), \(\mathbf{X}\)). We let \(x_{i}\) (resp., \((\bm{x}_{k})_{i}\)) indicate the \(i\)-th value of \(\bm{x}\) (resp., \(\bm{x}_{k}\)). Calligraphic letters \(\mathcal{X}\) denote sets, and \(|\mathcal{X}|\) is the cardinality of \(\mathcal{X}\). \(\mathbbm{1}\{\mathcal{S}\}\) is the indicator function that yields \(1\) if \(\mathcal{S}\) is true and \(0\) otherwise. \([M]:=\{1,\ldots,M\}\). For \(\mathbf{X}\in\mathbb{R}^{M}\), we let \(X_{i:M}\) be the \(i\)-th order statistics [18] of \(\mathbf{X}\), i.e., the \(i\)-th smallest value of \(\mathbf{X}\) with \(i\in[M]\). Finally, \(I_{n}\) is the identity matrix of dimension \(n\), and \(\stackrel{{ d}}{{\rightarrow}}\) (resp., \(\stackrel{{ d}}{{=}}\)) denotes convergence (resp., equality) in distribution.

## 2 Problem formulation

We consider an \(M\)-class classification task in which an input feature \(\bm{x}\) is classified into a class \(c\in\mathcal{C}:=[M]\). Our goal is to estimate the minimum misclassification probability, that is the BER. In particular, we seek to estimate the BER based on a dataset \(\mathcal{D}=\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{n}\) that follows an unknown data distribution, i.e., \((\bm{x}_{i},\bm{y}_{i})\stackrel{{ i.i.d.}}{{\sim}}P_{\mathbf{X },\mathbf{Y}}\), where \(\mathbf{X}\in\mathcal{X}\) and \(\mathbf{Y}\in\mathcal{Y}\subseteq[0,1]^{M}\).1

Footnote 1: We assume that \(\mathcal{X}\) is a finite set, but several of our results easily extend to the case when \(\mathcal{X}\) is an infinite set.

We assume that the label data \(\bm{y}\) contains the information about the class \(c\) of the input feature \(\bm{x}\), implying that one can retrieve \((\bm{x},c)\) from \((\bm{x},\bm{y})\). When a classifier \(\phi:\mathcal{X}\rightarrow\mathcal{C}\) is employed for a classification task, the corresponding misclassification probability is defined as \(\mathcal{E}(\phi):=\Pr(\phi(\mathbf{X})\neq C)\), where \(C\) is the true class for \(\mathbf{X}\). The minimum value of this misclassification probability is the so-called BER, denoted by \(P_{e}\), which is formally defined next.

**Definition 1** (Bayes error rate [50]).: Consider an \(M\)-class classification problem, where an input feature \(\bm{x}\in\mathcal{X}\) has to be classified into a class \(c\in\mathcal{C}:=[M]\). The BER is defined as

\[P_{e}=\inf_{\phi\in\Phi}\mathcal{E}(\phi)=\inf_{\phi\in\Phi}\mathbb{E}\left[ \mathbbm{1}\{\phi(\mathbf{X})\neq C\}\right],\] (1)

where \(\Phi\) is the set of all measurable functions \(\phi:\mathcal{X}\to\mathcal{C}\), and the expectation is taken over \(P_{\mathbf{X},C}\).

The misclassification probability \(\mathcal{E}(\phi)\) depends on the quality of the classifier \(\phi\) and the BER is obtained by choosing an optimal classifier. In fact, an optimal classifier is theoretically equivalent to the Maximum a Posteriori (MAP) classifier [50], that is, \(\phi_{\mathrm{MAP}}(\bm{x})=\arg\max_{k\in\mathcal{C}}\Pr(C=k|\mathbf{X}=\bm{ x})\). Plugging the MAP classifier into (1), the BER can be written as

\[P_{e}=\mathbb{E}\left[1-\max_{k\in\mathcal{C}}\Pr(C=k|\mathbf{X})\right],\] (2)

where the expectation is taken with respect to \(P_{\mathbf{X}}\). Note that \(P_{e}\in\left[0,1-\frac{1}{M}\right]\).

Our main objective is to effectively estimate the BER from the dataset \(\mathcal{D}\). In the remaining of the paper, we let \(\psi:\mathcal{D}\to[0,1]\) denote the estimator of the BER.

## 3 BER estimation with soft labels

Soft labels have several favorable properties (e.g., they help to prevent an overfitting problem, they lead to a well-structured model, and they improve the prediction performance) that make them widely used in machine learning. For example, they are essential in label smoothing and knowledge distillation, which are widely applied methods to improve model performance [81, 97, 98, 99, 100, 101].

There exist several types of soft labels and here we assume that a label is soft if it represents the posterior probability. Specifically, we say that \(\mathcal{D}=\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{n}\) is a dataset with soft labels if \(\bm{x}_{i}\in\mathcal{X}\) and \(\bm{y}_{i}\in\mathcal{Y}\) are such that

\[\bm{y}_{i}=\begin{bmatrix}\Pr(C=1|\mathbf{X}=\bm{x}_{i})\\ \Pr(C=2|\mathbf{X}=\bm{x}_{i})\\ \vdots\\ \Pr(C=M|\mathbf{X}=\bm{x}_{i})\end{bmatrix}.\] (3)

This is a standard and widely adopted assumption; it has indeed been argued [17, 35, 69, 61, 101, 69] that using soft labels to approximate posterior probabilities enhances model performance.

**Definition 2** (Base BER estimator).: The BER estimator \(\psi_{\mathrm{soft}}(\mathcal{D})\) is defined as

\[\psi_{\mathrm{soft}}(\mathcal{D})=\frac{1}{n}\sum_{(\bm{x},\bm{y})\in\mathcal{ D}}\left(1-\max_{j\in[M]}y_{j}\right).\] (4)

We will use the base estimator \(\psi_{\mathrm{soft}}\) in (4) as a building block for a robust BER estimation. We note that \(\psi_{\mathrm{soft}}\) in (4) with \(M=2\) retrieves the estimator proposed in [46]. The next theorem (proof in Appendix B.1) provides three important properties of \(\psi_{\mathrm{soft}}\) in (4).

**Theorem 1**.: _Assume that \(\mathcal{D}\) contains soft labels as defined in (3). Then, \(\psi_{\mathrm{soft}}(\mathcal{D})\) satisfies the following properties:_

1. (Unbiasedness): \(\mathbb{E}[\psi_{\mathrm{soft}}(\mathcal{D})]=P_{e}\)_, that is,_ \(\psi_{\mathrm{soft}}(\mathcal{D})\) _is an unbiased estimator of the BER;_
2. (Consistency): _For any_ \(\delta\in(0,1)\)_, it holds that_ \(|\psi_{\mathrm{soft}}-P_{e}|<\sqrt{\frac{\left(1-\frac{1}{M}\right)^{2}}{2n} \ln\frac{2}{\delta}}\) _with probability at least_ \(1-\delta\)_, that is,_ \(\psi_{\mathrm{soft}}(\mathcal{D})\) _is a consistent estimator of the BER;_
3. (Asymptotic Normality): \(\sqrt{n}(\psi_{\mathrm{soft}}-P_{e})\stackrel{{ d}}{{\to}} \mathcal{N}(0,\text{Var}(Y_{M:M}))\) _as_ \(n\to\infty\)_._

Theorem 1 shows that the BER can be effectively estimated directly from a dataset that contains soft labels as in (3). Moreover, it highlights that the convergence rate of \(\psi_{\mathrm{soft}}(\mathcal{D})\) is \(n^{-\frac{1}{2}}\), which is indeed the optimal (parametric) convergence rate.

Another important aspect to assess the performance of \(\psi_{\mathrm{soft}}(\mathcal{D})\) is to measure how far it is from the BER \(P_{e}\). Since \(\psi_{\mathrm{soft}}(\mathcal{D})\) is unbiased (from Theorem 1) this distance can be measured by computing the variance of \(\psi_{\mathrm{soft}}(\mathcal{D})\), which is denoted as \(\text{Var}(\psi_{\mathrm{soft}})\) and provided by the next proposition (proof in Appendix B.2).

**Proposition 1**.: _It holds that \(\text{Var}(\psi_{\mathrm{soft}})\!=\!\frac{1}{n}\text{Var}(Y_{M:M})\) and \(\text{Var}(\psi_{\mathrm{soft}})\!\leq\!\frac{\left(1-\frac{1}{M}\right)P_{e}- P_{e}^{2}}{n}\!\leq\!\frac{\left(1-\frac{1}{M}\right)^{2}}{4n}\)._

The exact computation of \(\text{Var}(\psi_{\mathrm{soft}})\) in Proposition 1 requires the knowledge of the order statistics of \(\mathbf{Y}\) and hence, of the label distribution \(P_{\mathbf{Y}}\). The upper bounds on \(\text{Var}(\psi_{\mathrm{soft}})\) are instead distribution-independent. In particular, both upper bounds show that the rate of convergence of \(\text{Var}(\psi_{\mathrm{soft}})\) is \(1/n\), which is in line with Theorem 1. Moreover, the first upper bound on \(\text{Var}(\psi_{\mathrm{soft}})\) implies that \(\text{Var}(\psi_{\mathrm{soft}})\to 0\) when either \(P_{e}\to 0\) (i.e., also known as realizability assumption [76]) or \(P_{e}\to 1-\frac{1}{M}\) (i.e., labels and features are independent). The first upper bound on \(\text{Var}(\psi_{\mathrm{soft}})\) also allows to find an upper bound on \(P_{e}\) which becomes tight when \(P_{e}\to 1-1/M\).

### Robustness of \(\psi_{\mathrm{soft}}\)

We consider robustness to outliers, where an outlier is a data sample that is corrupted by high noise. We use the concept of breakdown point [44, 60] to measure the robustness of \(\psi_{\mathrm{soft}}\). The breakdown point captures how robust an estimator is with respect to outliers.2 In the classical definition of breakdown point, the worst estimator (in terms of robustness) for a dataset \(\mathcal{D}\) with outliers is defined as \(\psi(\mathcal{D})=\infty\). However, in our setting \(\psi_{\mathrm{soft}}(\mathcal{D})\leq 1-\frac{1}{M}\) since \(\psi_{\mathrm{soft}}(\mathcal{D})\) is an estimate of \(P_{e}\). Because of this, we next adopt an alternative definition for the breakdown point which is commonly used for a bounded parameter space \(\Theta\)[45].

Footnote 2: The sample mean can fail to estimate the true mean by a single outlier (e.g., a sample \(x\) with a value very different from the true mean). Thus, the sample mean has a breakdown point equal to \(1/n\), meaning that one outlier can break the estimate. Differently, the median is more robust since it has a breakdown point of \(1/2\), i.e., half of the samples need to be outliers for breaking the estimate.

**Definition 3** (Breakdown point).: Consider an estimator \(\psi:\Omega^{n}\to\Theta\) of \(\theta\in\Theta\subseteq\mathbb{R}^{L}\). Let \(\mathcal{D}^{(\kappa)}=\{D_{1},\ldots,D_{\kappa}\}\), where \(D_{i}\in\Omega,\;\forall i\in[\kappa]\), be a clean dataset without outliers, and let \(\widetilde{\mathcal{D}}^{(\tau)}=\{\widetilde{D}_{1},\ldots,\widetilde{D}_{\tau}\}\) be a noisy dataset that is composed of \(\tau\) noisy data samples that can be arbitrarily replaced by outliers. Then, the breakdown point of \(\psi\) is defined as

\[B(\psi)=\min_{\tau:1\leq\tau\leq\kappa}\left\{\frac{\tau}{\kappa+\tau}:\sup\| \psi(\mathcal{D}^{(\kappa+\tau)})-\psi(\mathcal{D}^{(\kappa)}\cup\widetilde{ \mathcal{D}}^{(\tau)})\|\geq\|\mathsf{rad}(\Theta)\|\right\},\] (5)

where the \(\sup\) is taken over all \(\mathcal{D}^{(\kappa+\tau)},\mathcal{D}^{(\kappa)}\) and \(\widetilde{\mathcal{D}}^{(\tau)}\) such that \(\mathcal{D}^{(\kappa)}\subset\mathcal{D}^{(\kappa+\tau)}\), and \(\mathsf{rad}(\Theta)\) is the vector consisting of the \(L\) radii of the largest \(L\)-dimensional ellipsoid in \(\Theta\).3

Footnote 3: Any \(L\)-dimensional ellipsoid (after rotation) can be defined as \(\frac{x_{1}^{2}}{a_{1}^{2}}+\frac{x_{2}^{2}}{a_{2}^{2}}+\ldots+\frac{x_{L}^{2} }{a_{L}^{2}}=1\). Then, the \(L\) radii are \(a_{1},a_{2},\ldots,a_{L}\).

\(B(\psi)\) in (5) quantifies the value of the breakdown point for an estimator \(\psi:\Omega^{\tau+\kappa}\to\Theta\). In particular, an estimator \(\psi\) "breaks down" if \(\psi(\mathcal{D}^{(\kappa+\tau)})\) changes of at least \(\|\mathsf{rad}(\Theta)\|\) when \(\tau\) clean data samples are replaced with \(\tau\) outliers. The breakdown point is defined as the minimum ratio between the number of outliers (\(\tau\)) and the total number of data samples (\(\kappa+\tau\)), such that \(\tau\) outliers are sufficient to break the estimator \(\psi(\mathcal{D}^{(\kappa+\tau)})\). From Definition 3, it is clear that the higher the value of the breakdown point, the more robust the estimator is. To measure the breakdown point of \(\psi_{\mathrm{soft}}\), we start by noting that \(\psi_{\mathrm{soft}}\) is the sample mean of \(\{1-\max_{j\in[M]}(\bm{y}_{i})\}_{i=1}^{n}\) (see (4)). Thus, using the notation as in Definition 3, we have that \(\Theta=\left[0,1-\frac{1}{M}\right]\), which leads to \(\mathsf{rad}(\Theta)=\frac{1}{2}-\frac{1}{2M}\). This implies that \(B(\psi_{\mathrm{soft}})=\frac{1}{n}\) (e.g., when \(\mathcal{D}\) contains an outlier \(\bm{y}\) such that \(\max_{j\in[M]}y_{j}=\infty\)) and hence, \(\psi_{\mathrm{soft}}\) is not robust to outliers.

We next propose a methodology, inspired by the median-of-means estimator [65], to make any BER estimator (and hence, also \(\psi_{\mathrm{soft}}\)) robust.

**Definition 4** (Median-of-BERs (MoB) estimator).: Consider any BER estimator \(\psi\) and a dataset \(\mathcal{D}\). First, partition \(\mathcal{D}\) into \(K\) sub-datasets \(\mathcal{D}_{k},\;k\in[K]\) such that \(|\mathcal{D}|=n=cK\), where \(c\in\mathbb{N}\) is the number of data samples in \(\mathcal{D}_{k}\). Then, the Median-of-BERs (MoB) estimator is defined as

\[\mathsf{MoB}_{K}(\psi,\mathcal{D})=\mathsf{med}(\{\psi(\mathcal{D}_{k}):k\in[K] \}),\] (6)

where \(\mathsf{med}(\mathcal{S})\) is the sample median of \(\mathcal{S}\).

The next theorem (proof in Appendix B.3) provides three important properties of \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\).

**Theorem 2**.: _Assume that \(\mathcal{D}\) contains soft labels as defined in (3). Then, the MoB estimator \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) satisfies the following properties:_

1. (Consistency): _For all_ \(t\lesssim K\)_, with probability at least_ \(1-4e^{-2t}\)_, it holds that_ \[|\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})-P_{e}|\leq\left(\frac{ \left(1-\frac{1}{M}\right)^{3}}{2\sqrt{3\text{Var}}(Y_{M:M})}\frac{K}{\sqrt{n \!-\!K}}\!+\!3\sqrt{t\text{Var}(Y_{M:M})}\right)\sqrt{\frac{1}{n\!-\!K}},\] (7) _and hence,_ \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) _is a consistent estimator of the BER;_
2. (Breakdown): _Its breakdown point is given by_ \(B\left(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\right)=\left\lfloor \frac{K+1}{2}\right\rfloor\frac{1}{n}\)_;_
3. (Asymptotic Normality): _If_ \(K\to\infty\) _and_ \(K=o(\sqrt{n})\) _as_ \(n\to\infty\)_, then_ \(\sqrt{n}\left(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})-P_{e}\right) \stackrel{{ d}}{{\to}}\mathcal{N}\left(0,\frac{\pi}{2}\text{Var}(Y _{M:M})\right)\)_._

Theorem 2 shows that \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) has the same rate of convergence of \(n^{-\frac{1}{2}}\) as \(\psi_{\mathrm{soft}}\) (see Theorem 1). However, \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) has a higher breakdown point (and hence, is more robust) than \(\psi_{\mathrm{soft}}\). Nonetheless, this robustness is attained at two expenses: (i) \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) is not an unbiased estimator of \(P_{e}\); and (ii) the variance of \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) is larger than the one of \(\psi_{\mathrm{soft}}\) by a factor of \(\pi/2\) in the asymptotic regime.

Theorem 2 also points out to an interesting trade-off, with respect to \(K\), between accuracy and robustness. For example, setting \(K=\sqrt{n}\) leads to a higher breakdown point (and hence, is more robust) than setting \(K=\ln n\). However, the concentration bound in (7) (which measures the accuracy of the estimation) has a much larger value with \(K=\sqrt{n}\) than with \(K=\ln n\).

## 4 BER estimation with noisy labels

The soft label data discussed in Section 3 might be challenging to obtain as data labels are often corrupted by noise or other perturbations. In such scenarios, data labels are unreliable [27; 64; 68] and several studies have been conducted on them [37; 57; 80; 88; 90; 92; 94; 96].

With the goal to broaden the applicability of \(\psi_{\mathrm{soft}}\) (or its robust version \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\)), we here study their performance in scenarios where the soft labels are corrupted by various typical types of noise.4 We denote by \(\widetilde{\mathbf{Y}}\sim P_{\widetilde{\mathbf{Y}}|\mathbf{Y}}\) the noisy soft label, i.e., a noisy soft label \(\widetilde{\mathbf{Y}}=\tilde{\bm{y}}_{i}\) is distributed according to the conditional probability distribution \(P_{\widetilde{\mathbf{Y}}|\mathbf{Y}=\bm{y}_{i}}\), where \(\bm{y}_{i}\) is the true soft label.

Footnote 4: We refer an interested reader to Appendix B.4 where a noise that randomly shuffles the labels is also studied.

### Additive noise on the data labels

We here consider the case where the labels are corrupted by additive noise, i.e., we have \((\bm{x},\tilde{\bm{y}})\in\widetilde{\mathcal{D}}_{\mathrm{A}}\) where \(\tilde{\bm{y}}=\bm{y}+\bm{z}\), where \(\bm{z}\) is an \(M\)-dimensional random noise vector. We assume that \(\bm{z}\) has i.i.d. components each with zero mean.

We start our analysis by showing that \(\psi_{\mathrm{soft}}\) in (4) applied on \(\widetilde{\mathcal{D}}_{\mathrm{A}}\) is neither a consistent nor an unbiased estimator of \(P_{e}\). We, in fact, have that

\[\psi_{\mathrm{soft}}(\widetilde{\mathcal{D}}_{\mathrm{A}})=1-\frac{1}{n}\sum_{i= 1}^{n}\max_{j\in[M]}\{(\bm{y}_{i})_{j}+(\bm{z}_{i})_{j}\},\] (8)

which, by the law of large numbers, converges to

\[\lim_{n\to\infty}\psi_{\mathrm{soft}}(\widetilde{\mathcal{D}}_{\mathrm{A}})=1- \mathbb{E}\left[\max_{j\in[M]}\left\{Y_{j}+Z_{j}\right\}\right].\] (9)

Then, the inequalities \(\max_{i}x_{i}+\min_{j}y_{j}\leq\max_{i}\{x_{i}+y_{i}\}\leq\max_{i}x_{i}+\max_ {j}y_{j}\) imply that

\[\mathbb{E}[Z_{1:M}]\leq P_{e}-\lim_{n\to\infty}\psi_{\mathrm{soft}}(\widetilde{ \mathcal{D}}_{\mathrm{A}})\leq\mathbb{E}[Z_{M:M}],\] (10)which follow from (2) and (3). The bounds in (10) demonstrate that with labels corrupted by additive noise, \(\psi_{\mathrm{soft}}\) is (in general) an inconsistent estimator of \(P_{e}\). By following similar steps, it can be easily proved that \(\psi_{\mathrm{soft}}\) is also a biased estimator of \(P_{e}\). In particular, the bounds in (10) show that \(\psi_{\mathrm{soft}}\) has a bias which is bounded by the expected value of the smallest (lower bound) and of the largest (upper bound) order statistics of the noise. In what follows, we provide two examples of distributions for which bounds on these expected values have been computed.

_Example 1_.: Let \(\mathbf{Z}\stackrel{{ i.i.d.}}{{\sim}}\mathrm{Uniform}(-a,a)\). Then, \(\mathbb{E}[Z_{i:M}]=\frac{2ai}{M+1}-a\), which as \(n\to\infty\) implies that \(|P_{e}-\psi_{\mathrm{soft}}(\widetilde{\mathcal{D}}_{\mathrm{A}})|\leq a\frac{ M-1}{M+1}\).

_Example 2_.: Let \(\mathbf{Z}\stackrel{{ i.i.d.}}{{\sim}}\gamma^{2}\)-sub-Gaussian.5 Then, we have that \(\mathbb{E}[Z_{M:M}]=-\mathbb{E}[Z_{1:M}]\) and \(\mathbb{E}[Z_{M:M}]\leq\sqrt{2\gamma^{2}\ln M}\)[9] which as \(n\to\infty\) implies that \(|P_{e}-\psi_{\mathrm{soft}}(\widetilde{\mathcal{D}}_{\mathrm{A}})|\leq\sqrt{2 \gamma^{2}\ln M}\).

Footnote 5: We say that a random variable \(Z\) is \(\gamma^{2}\)-sub-Gaussian if \(\mathbb{E}[e^{\lambda(Z-\mathbb{E}[Z])}]\leq e^{\frac{\lambda^{2}\gamma^{2}}{ 2}},\;\forall\lambda\in\mathbb{R}\).

Our analysis above shows that \(\psi_{\mathrm{soft}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\) is (in general) an inconsistent and biased estimator of \(P_{e}\). Because of this, we next propose a new BER estimator, which (different from \(\psi_{\mathrm{soft}}\)) leverages the features \(\{\bm{x}_{i}\}_{i=1}^{n}\) to denoise \(\tilde{\bm{y}}_{i}\). We refer to this estimator as \(\psi_{\mathrm{DN}}\). Our main intuition behind proposing \(\psi_{\mathrm{DN}}\) stems from the fact that data samples having the same feature \(\bm{x}\) should be labeled with the same (or at least similar) label. This can be attained by noting that, even if labels are noisy, it is possible to minimize the effect of a zero-mean noise by averaging the noisy labels associated with the same feature. We next formally define our denoising BER estimator \(\psi_{\mathrm{DN}}:\mathbb{R}^{M}\to[0,1]\).

**Definition 5** (Denoise estimator).: For a noisy dataset \(\widetilde{\mathcal{D}}_{\mathrm{A}}=\{(\bm{x}_{i},\tilde{\bm{y}}_{i})\}_{i=1}^ {n}\), let the denoised label \(\bm{s}(\bm{x}_{i})\) for all \(i\in[n],\) be defined as

\[\bm{s}(\bm{x}_{i})=\frac{\sum_{j=1}^{n}\mathbbm{1}\{\bm{x}_{j}=\bm{x}_{i}\} \tilde{\bm{y}}_{j}}{\sum_{j=1}^{n}\mathbbm{1}\{\bm{x}_{j}=\bm{x}_{i}\}},\] (11)

and let \(\text{idx}(\bm{x}_{i})=\arg\max_{j\in[M]}\{(\bm{s}(\bm{x}_{i}))_{j}\}\). Then, the denoise BER estimator is defined as

\[\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})=\frac{1}{n}\sum_{i=1} ^{n}\left(1-(\tilde{\bm{y}}_{i})_{\text{idx}(\bm{x}_{i})}\right).\] (12)

The next theorem (proof in Appendix B.5) provides some important properties of \(\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\).

**Theorem 3**.: _Let \(\widetilde{\mathcal{D}}_{\mathrm{A}}=\{(\bm{x}_{i},\tilde{\bm{y}}_{i})\}_{i=1} ^{n}\) be a dataset that consists of noisy soft labels \(\tilde{\bm{y}}_{i}=\bm{y}_{i}+\bm{z}_{i}\) where \(\bm{z}_{i}\stackrel{{ i.i.d.}}{{\sim}}P_{\mathbf{Z}}\) is the zero mean noise. Then, \(\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\) is a consistent estimator of \(P_{e}\). Moreover, if the noise has bounded support, that is \(\Pr(\mathbf{Z}\in[a,b]^{M})=1\), then_

1. (Asymptotical unbiasedness): \(\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\) _is an asymptotically unbiased estimator of_ \(P_{e}\)_;_
2. (Denoising Consistency): _For any_ \(\delta\in(0,1)\)_, it holds that_ \(|\bm{s}(\bm{x})-\bm{y}|<\sqrt{\frac{\left(1-\frac{1}{M}-a+b\right)^{2}}{2n_{\bm {x}}}\ln\frac{2}{\delta}}\) _with probability at least_ \(1-\delta\)_, where_ \(n_{\bm{x}}=\sum_{j=1}^{n}1\{\bm{x}_{j}=\bm{x}\}\) _and_ \(\bm{s}(\bm{x})\) _is the denoised label defined in (_11_)._

The results in Theorem 3 broadens the applicability of an effective BER estimator to a larger class of datasets, e.g., to scenarios where the dataset includes multiple data samples representing the same feature. For example, \(\psi_{\mathrm{DN}}\) works well on a noisy dataset where each data sample has multiple labels. Moreover, as we will see in Section 4.2, \(\psi_{\mathrm{DN}}\) is also useful for one-hot labels. We also expect that \(\psi_{\mathrm{DN}}\) will work well with privatized datasets where noise was added to enhance privacy [26].

_Remark 1_.: Our estimator \(\psi_{\mathrm{DN}}\) only requires the dataset \(\widetilde{\mathcal{D}}_{\mathrm{A}}\). This is a more practical approach than the one in [46] which instead also requires the true one-hot label. The estimator in [46], which is referred to as a genie estimator \(\psi_{\mathrm{genie}}\), can be obtained by replacing \(\bm{s}(\bm{x})\) in (11) with the true one-hot label. In Appendix B.6, we provide properties of \(\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\) that generalize the results in [46] for any \(M\geq 2\). As expected, \(\psi_{\mathrm{genie}}\) is a consistent and unbiased estimator of \(P_{e}\).

### One-hot labels

A dataset consisting of one-hot labels, denoted as \(\overline{\mathcal{D}}=\{(\bm{x}_{i},\overline{\bm{y}}_{i})\}_{i=1}^{n}\), contains little information about the class posterior probability. We assume that the one-hot label is constructed from the corresponding soft label as follows,

\[\overline{\mathbf{Y}}=\ \mathbf{e}_{i}\ \text{with probability}\ \ Y_{i},\] (13)

where: (i) \(\overline{\mathbf{Y}}\) is the one-hot random vector; (ii) \(\mathbf{Y}\) is the soft label random vector in (3); and (iii) \(\mathbf{e}_{i}\in\mathbb{R}^{M}\) is the standard basis vector with a one in the \(i\)-th position and zero in all the other entries.

We can think of the one-hot label construction in (13) as a noisy label. Specifically, the noise \(\mathbf{Z}\in\{\mathbf{e}_{i}-\bm{y}\}_{i=1}^{M}\) with the probability mass function \(p_{\mathbf{Z}|\mathbf{Y}}(\bm{x}|\bm{y})=\sum_{j=1}^{M}y_{j}\mathbbm{1}\{\bm{ z}=\mathbf{e}_{j}-\bm{y}\}\) satisfies \(\overline{\mathbf{Y}}=\mathbf{Y}+\mathbf{Z}\). It is not difficult to see that \(\mathbb{E}[\mathbf{Z}]=\mathbb{E}[\mathbb{E}[\mathbf{Z}|\mathbf{Y}]]=\bm{0}\) and hence, our denoise estimator \(\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\) in (12), or its robust version \(\mathsf{MoB}_{K}(\psi_{\mathrm{DN}},\widetilde{\mathcal{D}}_{\mathrm{A}})\), can estimate the BER also for the case of one-hot labels. However, we also note that with the above construction, the labels might be too noisy to estimate the BER in practice. This suggests that a more refined denoising method than the one in Definition 5 for one-hot labels might be needed for a more effective BER estimation.

Motivated by the above discussion, we next propose a denoising method for one-hot labels, which leverages neighbor samples to mitigate the noise.6 Our choice of such a denoising method mainly stems from the fact that one would expect that neighboring samples should have similar posterior probabilities. In particular, for each \(\bm{x}_{i},i\in[n]\), we consider all of its neighbors (features that are at most at a distance \(r\) from \(\bm{x}_{i}\)) and we average the corresponding noisy labels in a spirit similar to (11). We next formally define our denoising BER estimator, which we refer to as \(\psi_{\mathrm{C}}\).

Footnote 6: Several works have taken advantage of neighbor samples to estimate the BER or the probability divergence. We refer an interested reader to [67] and references therein.

**Definition 6** (Cluster denoise estimator).: Given \(\mathsf{d}:\mathcal{X}^{2}\to\mathbb{R}_{+}\) and \(r\in\mathbb{R}_{+}\), define \(\mathsf{Cluster}_{i}:=\{(\bm{x},\bm{y})\in\widetilde{\mathcal{D}}_{\mathrm{A} }:\mathsf{d}(\bm{x}_{i},\bm{x})\leq r\}\). Then, the denoised label for \(\tilde{\bm{y}}_{i},\) for all \(i\in[n]\), is defined as

\[\hat{\bm{y}}_{i}=\frac{\sum_{(\bm{x},\tilde{\bm{y}})\in\mathsf{ Cluster}_{i}}\tilde{\bm{y}}}{|\mathsf{Cluster}_{i}|},\] (14)

and the cluster-based BER estimator is defined as

\[\psi_{\mathrm{C}}(\widetilde{\mathcal{D}}_{\mathrm{A}},\mathsf{d},r)=\frac{1} {n}\sum_{i=1}^{n}\left(1-\max_{j\in[M]}\{(\hat{\bm{y}}_{i})_{j}\}\right).\] (15)

_Remark 2_.: Preprocessing the dataset that aggregates data samples of similar features can also be helpful in improving the BER estimate for noisy labels, and it can be paired with the cluster denoise estimator in Definition 6. A naive approach would consist of reducing the feature dimensionality and increasing the number of data samples inside clusters (14), which helps mitigate the effect of the noise. Some notable examples of data preprocessing are the classical principal component analysis (PCA) [77] and the representation learning [5] (e.g., variational auto encoder [52]).

_Remark 3_.: Another viewpoint to the denoising method in (14) is the connection to the non-parametric estimation of the conditional expectation \(\mathbb{E}[\mathbf{Y}|\mathbf{X}=\bm{x}]\), where \(\mathbf{Y}\) is the true soft label corresponding to \(\mathbf{X}=\bm{x}\). In particular, estimating \(\mathbb{E}[\mathbf{Y}|\mathbf{X}=\bm{x}]\) is equivalent to denoising the noisy label associated with \(\bm{x}\) as in (14). Under such a viewpoint, the BER estimator in Definition 6 resembles the Nadaraya-Watson estimator with Parzen window kernel [63, 89].

## 5 Experiments

We empirically validate our results using various datasets, namely: 1) a synthetic dataset with different noises, including one-hot labels; 2) two benchmark datasets CIFAR-10H [4] and Fashion-MNIST-H [46]; and 3) MovieLens [38], a real-world dataset for movie recommendations. We compare our estimators with two SOTA BER bounds [72], namely the generalized Henze-Penrose (GHP) BER bounds [75] and the \(k\)-Nearest Neighbor (NN) BER bounds [16]. Details on the \(k\)-NN bounds and GHP bounds, and additional results can be found in Appendix C.

Synthetic dataset with soft labels and noisy labels.We consider a \(4\)-class classification problem with equiprobable classes \(C\in\mathcal{C}_{\mu}:=\{(\mu,\mu),(-\mu,\mu),(-\mu,-\mu),(\mu,-\mu)\}\), where \(\mu>0\) is a parameter that controls the classification hardness. We generate the feature \(\mathbf{X}\in\mathbb{R}^{2}\) according to a \(2\)-dimensional Gaussian distribution with mean \(\bm{c}\) (i.e., a realization of \(C\in\mathcal{C}_{\mu}\)) and covariance matrix \(I_{2}\). The corresponding soft labels \(\bm{y}_{i}\)'s are then obtained by the Bayes' theorem. In particular, since \(f_{\mathbf{X}|C}\sim\mathcal{N}(C,I_{2})\) and \(P_{C}(\bm{c})=1/4\) if \(\bm{c}\in\mathcal{C}_{\mu}\) (and zero otherwise), according to (3) we have that \((\bm{y}_{i})_{k}=\frac{f_{\mathbf{X}|C}(\bm{x}_{i}|\bm{c})P_{C}(\bm{c})}{\sum_ {\alpha\in C_{\mu}}f_{\mathbf{X}|C}(\bm{x}_{i}|\alpha)P_{C}(\alpha)}\).7 Figure 1 illustrates \(n=1,000\) features (left figure), and the corresponding soft labels (right figure) for the synthetic dataset generated as explained above; in particular, for each feature \(\bm{x}_{i}\), we only reported the label \((\bm{y}_{i})_{j}\), where \(j^{\star}=\arg\max_{j\in[4]}(\bm{y}_{i})_{j}\) which is required for evaluating \(\psi_{\mathrm{soft}}\) in (4). As expected, a point near the decision boundary (lines for \(x_{1}=0\) or \(x_{2}=0\)) has a smaller maximum value in its soft label.

Footnote 7: Without loss of generality, we consider the following mapping: \(k=1\) if \(\bm{c}=(\mu,\mu)\); \(k=2\) if \(\bm{c}=(-\mu,\mu)\); \(k=3\) if \(\bm{c}=(-\mu,-\mu)\); and \(k=4\) if \(\bm{c}=(\mu,-\mu)\).

The results in Figure 2 empirically demonstrate the effectiveness of our estimators, which consistently outperform the others.8 Moreover, our estimators estimate the exact value of the BER, while the others derive upper and lower bounds on the BER, which might not be tight.

Figure 1: \(4\)-classes Gaussian data samples. The left figure shows the samples of each class, and the right figure displays the corresponding soft label information as the maximum value of the soft label.

We further conducted experiments to verify the effectiveness of our denoising and robustifying methods on data samples with label noise and outliers. Figure 3 shows the comparison of the denoising estimator \(\mathsf{MoB}_{K}(\psi_{\mathrm{C}})\) with the base estimator \(\psi_{\mathrm{soft}}\). We observe that \(\psi_{\mathrm{soft}}\) suffers from the label noise, which leads to a bias in the estimation. However, our denoising estimator \(\mathsf{MoB}_{K}(\psi_{\mathrm{C}})\) suitably denoises the noisy labels and effectively estimates the BER. As shown in Figure 2(b), in fact, the absolute error between the estimate and the BER as well as the variance decrease as \(n\) grows.

Synthetic dataset with one-hot labels.We analyze the performance of \(\mathsf{MoB}_{K}(\psi_{\mathrm{C}})\) in the same Gaussian setting described above, but with one-hot labels. Each soft label of the samples is mapped to a one-hot label according to the categorical distribution with probabilities equal to the soft labels (see (13)). Figure 4 shows that our denoising method is capable of suitably reducing the noise in one-hot labels; in fact, it correctly estimates the BER after around \(10^{4}\) samples when \(\mathsf{MoB}_{K}(\psi_{\mathrm{C}})\) converges to the BER. From Figure 4, we also observe that \(\psi_{\mathrm{soft}}\) performs poorly; this suggests that denoising methods are essential for the BER estimation task, hence worth further investigation.

Cifar-\(10\)H [4] and Fashion-MNIST-H [46].The CIFAR-\(10\)H dataset is a variation of CIFAR-\(10\)[53], constructed by labeling \(10^{4}\) images in the test dataset of CIFAR-\(10\) by multiple labelers. The Fashion-MNIST-H dataset is populated by \(10^{4}\) images in the test dataset of Fashion-MNIST [93] in a similar manner. These two datasets have \(10\) classes, but we further categorized these into \(M\in\{2,3\}\) classes. In particular, for CIFAR-\(10\)H, we assigned \(C_{1}=\{\mathrm{airplane},\mathrm{automobile},\mathrm{ship},\mathrm{truck}\}\), \(C_{2}=\{\mathrm{bird},\mathrm{cat},\mathrm{deer},\mathrm{dog},\mathrm{frog}, \mathrm{horse}\}\) when \(M=2\), and \(C_{1}=\{\mathrm{airplane},\mathrm{automobile},\mathrm{ship},\mathrm{truck}\}\), \(C_{2}=\{\mathrm{cat},\mathrm{dog},\mathrm{frog}\}\), \(C_{3}=\{\mathrm{bird},\mathrm{deer},\mathrm{horse}\}\) when \(M=3\). Similarly, for Fashion-MNIST-H, we assigned \(C_{1}=\{\mathrm{t-shirt}/\mathrm{top},\mathrm{pullover},\mathrm{dress}, \mathrm{coat},\mathrm{shirt}\}\), \(C_{2}=\{\mathrm{trouser},\mathrm{sandal},\mathrm{sneaker},\mathrm{bag},\mathrm{ ankleboot}\}\) when \(M=2\), and \(C_{1}=\{\mathrm{t-shirt}/\mathrm{top},\mathrm{pullover},\mathrm{coat},\mathrm{ shirt}\}\), \(C_{2}=\{\mathrm{trouser},\mathrm{dress},\mathrm{bag}\}\), and \(C_{3}=\{\mathrm{sandal},\mathrm{sneaker},\mathrm{ankleboot}\}\) when \(M=3\).

From Table 1, we observe that the estimates using \(\mathsf{MoB}_{K}(\psi_{\mathsf{DN}})\) with \(K=\lfloor\sqrt{n}\rfloor\) are slightly lower than \(\psi_{\mathsf{DN}}\), which is due to the robustness of \(\mathsf{MoB}_{K}(\psi_{\mathsf{DN}})\) to outliers.9 We also highlight that the BER estimates can be leveraged to determine which task is more difficult. Intuitively, performing

Figure 4: Comparison of \(\mathsf{MoB}_{K}(\psi_{\mathrm{C}})\) with \(\psi_{\mathrm{soft}}\) for Gaussian samples with one-hot labels.

Figure 3: Comparison of \(\mathsf{MoB}_{K}(\psi_{\mathrm{C}})\) (evaluated with \(\psi_{\mathrm{C}}\) in (15)) with \(\psi_{\mathrm{soft}}\) on noisy soft labels. We consider \(\mathbf{Z}\sim\mathcal{N}\left(\mathbf{0},1/5I_{4}\right)\). To model outliers, we added \(\tilde{Z}=0.2\cdot\mathrm{Ber}\left(1/2\right)\) with probability \(1/10\) to each entry of the soft labels. For different \(n\), the parameters of \(\mathsf{MoB}_{K}(\psi_{\mathrm{C}})\) are chosen as \(K=\lfloor\sqrt{n}\rfloor\), \(\mathsf{d}\) is the Euclidean distance, and \(r=1/5\). We iterate the experiment \(50\) times for each \(n\).

the classification task over CIFAR-\(10\)H is much easier than performing it over Fashion-MNIST-H, and this is supported by the results in Table 1, i.e., the BERs associated with CIFAR-\(10\)H are smaller than those over Fashion-MNIST-H. It is worth noting that test dataset overfitting can happen when benchmark datasets are considered [3; 46; 51; 59; 91]. This can also be observed by the results in Table 1, where our BER estimates of the \(10\)-class classifications are larger than the SOTA error rates of \(0.005\) on CIFAR-\(10\) by [24] and of \(0.0309\) on Fashion-MNIST by [82]. We also suspect that, since the labels are assigned by humans, who might not be experts, the datasets might still have a considerable amount of label noise, which would lead to incorrect BER estimates. However, no estimators (including ours) can estimate the BER from a dataset that contains very noisy labels.

M MovieLens [38].This dataset consists of \(25\) million ratings to \(62,000\) movies by \(162,000\) users. Each rating ranges from \(0.5\) to \(5\) with step size \(0.5\). We first considered a movie classification task: a user either likes a movie or not. This is a complex binary classification task with the input feature being a movie (in general, a two-hour long video with audio) and the class being either \(0\) (dislike the movie) or \(1\) (like the movie). To ensure that a label belongs to \([0,1]\), we applied min-max normalization to each rating. Moreover, in order to have enough ratings for each movie when applied the denoising method, we filtered out some data if the number of ratings was smaller than \(100\). Then, \(\psi_{\text{DN}}\) and \(\mathsf{MoB}_{K}(\psi_{\text{DN}})\) estimated the BER of such movie classification task, and they yielded a BER of around \(0.3\). We then categorized the ratings into \(M=3\) classes, i.e., we assigned \(C_{1}=\{0.5,1.0,1.5\}\), \(C_{2}=\{2.0,\ldots,3.5\}\) and \(C_{3}=\{4.0,4.5,5.0\}\). On this task, the BER estimate is around \(0.4\). These BERs are quite large, implying that the movie classification is a hard task to perform. This may be justified by the fact that ratings of movies are subjective, and movie recommendations are indeed challenging without users' information (e.g., content-based [47] or item-based [74] recommendation systems use users' information).

## 6 Conclusion

In this paper, we investigated the challenge of estimating the BER for multi-class classifications. We proposed a BER estimator, \(\psi_{\text{soft}}\), which we proved to be unbiased, consistent, and asymptotically normal when applied to soft-labeled datasets. By leveraging the median-of-mean method, we also proposed a methodology to make any BER estimate robust. To ensure the applicability of the BER estimator in practical scenarios, we analyzed the challenges posed by noisy soft labels, including those with additive noise and one-hot labels. For such noisy labeled datasets, we developed denoising techniques that effectively mitigate the label noise by using the corresponding features. We showed that these denoising BER estimators are unbiased and consistent under mild noise assumptions. Our experimental results, drawn from synthetic and real-world datasets, validated our theoretical results.

Although in this work we assumed that the set of features is finite, several of our results (e.g., Theorem 1 and Theorem 2) can be easily shown to hold also for the infinite feature space. A research direction worth further investigation would consist of proving that the cluster denoise estimator in Definition 6 has similar appealing properties (see Theorem 3) as the denoise estimator in Definition 5 (which assumes that the set of the features is discrete). Always along these lines, a second interesting future work would be to analyze the rate of convergence of our BER estimator paired with the denoising method as a function of the characteristics (e.g., cardinality, distribution) of the feature space. Such an analysis would indeed provide insights into the difficulty of a classification task. Another relevant avenue for future research lies in assessing the optimal performance of multi-label learning, where a single data point can be associated with multiple classes. While adapting our framework to predict the minimum Hamming loss for multi-label learning might be relatively straightforward, estimating other metrics (e.g., the F1 score and the exact match ratio) might be a difficult task, potentially necessitating new methodologies.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{CIFAR-10H} & \multicolumn{3}{c}{Fashion-MNIST-H} & \multicolumn{3}{c}{MovieLens} \\ \cline{2-9} \# classes & 2 & 3 & 10 & 2 & 3 & 10 & 2 & 3 \\ \hline \(\psi_{\text{DN}}\) & 0.0050 & 0.0177 & 0.0456 & 0.0348 & 0.0932 & 0.2825 & 0.3063 & 0.4031 \\ \(\mathsf{MoB}_{K}(\psi_{\text{DN}})\) & 0.0044 & 0.0168 & 0.0440 & 0.0347 & 0.0931 & 0.2816 & 0.3065 & 0.4035 \\ \hline \hline \end{tabular}
\end{table}
Table 1: BER estimate on benchmark and real-world datasets.

## References

* Antos et al. [1999] A. Antos, L. Devroye, and L. Gyorfi. Lower bounds for Bayes error estimation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 21(7):643-645, 1999.
* Avi-Itzhak and Diep [1996] H. Avi-Itzhak and T. Diep. Arbitrarily tight upper and lower bounds on the Bayesian probability of error. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 18(1):89-91, 1996.
* Barz and Denzler [2020] B. Barz and J. Denzler. Do we train on test data? purging CIFAR of near-duplicates. _Journal of Imaging_, 6(6):41, 2020.
* Battleday et al. [2020] R. M. Battleday, J. C. Peterson, and T. L. Griffiths. Capturing human categorization of natural images by combining deep networks and cognitive models. _Nature Communications_, 11(1):5418, 2020. doi: 10.1038/s41467-020-18946-z. URL https://doi.org/10.1038/s41467-020-18946-z.
* Bengio et al. [2013] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 35(8):1798-1828, 2013. doi: 10.1109/TPAMI.2013.50.
* Berisha et al. [2016] V. Berisha, A. Wisler, A. O. Hero, and A. Spanias. Empirically estimable classification bounds based on a nonparametric divergence measure. _IEEE Transactions on Signal Processing_, 64(3):580-591, 2016. doi: 10.1109/TSP.2015.2477805.
* Bhatia and Davis [2000] R. Bhatia and C. Davis. A better bound on the variance. _The American Mathematical Monthly_, 107(4):353-357, 2000.
* Boekee and van der Lubbe [1979] D. E. Boekee and J. C. van der Lubbe. Some aspects of error bounds in feature selection. _Pattern Recognition_, 11(5-6):353-360, 1979.
* Boucheron et al. [2013] S. Boucheron, G. Lugosi, and P. Massart. _Concentration Inequalities: A Nonasymptotic Theory of Independence_. Oxford University Press, 2013.
* Buturovic [1993] L. Buturovic. Improving k-nearest neighbor density and error estimates. _Pattern Recognition_, 26(4):611-616, 1993.
* Chen [1971] C. Chen. Theoretical comparison of a class of feature selection criteria in pattern recognition. _IEEE Transactions on Computers_, C-20(9):1054-1056, 1971. doi: 10.1109/T-C.1971.223402.
* Chen [1976] C. H. Chen. On information and distance measures, error bounds, and feature selection. _Information Sciences_, 10(2):159-173, 1976.
* Chen et al. [2023] Q. Chen, F. Cao, Y. Xing, and J. Liang. Evaluating classification model against Bayes error rate. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-16, 2023. doi: 10.1109/TPAMI.2023.3240194.
* Chernoff [1952] H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. _The Annals of Mathematical Statistics_, pages 493-507, 1952.
* Cover [1999] T. M. Cover. _Elements of Information Theory_. John Wiley & Sons, 1999.
* Cover and Hart [1967] T. M. Cover and P. Hart. Nearest neighbor pattern classification. _IEEE Transactions on Information Theory_, 13(1):21-27, 1967. doi: 10.1109/TIT.1967.1053964.
* Dao et al. [2021] T. Dao, G. M. Kamath, V. Syrgkanis, and L. Mackey. Knowledge distillation as semiparametric inference. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=m4UCf24r0Y.
* David and Nagaraja [2004] H. A. David and H. N. Nagaraja. _Order Statistics_. Wiley Online Library, 2004.
* Devijver [1974] P. A. Devijver. On a new class of bounds on Bayes risk in multihypothesis pattern recognition. _IEEE Transactions on Computers_, C-23(1):70-80, 1974. doi: 10.1109/T-C.1974.223779.
* Devijver and Kittler [1982] P. A. Devijver and J. Kittler. _Pattern Recognition_. Prentice Hall, Old Tappan, NJ, July 1982.
* Devroye [1981] L. Devroye. On the asymptotic probability of error in nonparametric discrimination. _The Annals of Statistics_, 9(6):1320-1327, 1981.
* Devroye et al. [2013] L. Devroye, L. Gyorfi, and G. Lugosi. _A Probabilistic Theory of Pattern Recognition_, volume 31. Springer Science & Business Media, 2013.

* [23] J. L. Doob. Regularity properties of certain families of chance variables. _Transactions of the American Mathematical Society_, 47(3):455-486, 1940.
* [24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
* [25] R. O. Duda, P. E. Hart, et al. _Pattern Classification_. John Wiley & Sons, 2006.
* [26] C. Dwork. Differential privacy. In _Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II 33_, pages 1-12. Springer, 2006.
* [27] B. Frenay and M. Verleysen. Classification in the presence of label noise: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, 25(5):845-869, 2014. doi: 10.1109/TNNLS.2013.2292894.
* [28] J. H. Friedman and L. C. Rafsky. Multivariate generalizations of the wald-wolfowitz and smirnov two-sample tests. _The Annals of Statistics_, pages 697-717, 1979.
* [29] K. Fukunaga. _Introduction to Statistical Pattern Recognition (2nd Ed.)_. Academic Press Professional, Inc., USA, 1990. ISBN 0122698517.
* [30] K. Fukunaga and L. Hostetler. k-nearest-neighbor Bayes-risk estimation. _IEEE Transactions on Information Theory_, 21(3):285-293, 1975. doi: 10.1109/TIT.1975.1055373.
* [31] K. Fukunaga and D. M. Hummels. Bayes error estimation using Parzen and k-NN procedures. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, (5):634-643, 1987.
* [32] K. Fukunaga and D. M. Hummels. Bias of nearest neighbor error estimates. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, (1):103-112, 1987.
* [33] K. Fukunaga and D. Kessell. Nonparametric Bayes error estimation using unclassified samples. _IEEE Transactions on Information Theory_, 19(4):434-440, 1973. doi: 10.1109/TIT.1973.1055049.
* [34] F. Garber and A. Djouadi. Bounds on the Bayes classification error based on pairwise risk functions. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 10(2):281-288, 1988. doi: 10.1109/34.3891.
* [35] V. Grossmann, L. Schmarje, and R. Koch. Beyond hard labels: investigating data label distributions. _arXiv preprint arXiv:2207.06224_, 2022.
* [36] L.-Z. Guo, Z.-Y. Zhang, Y. Jiang, Y.-F. Li, and Z.-H. Zhou. Safe deep semi-supervised learning for unseen-class unlabeled data. In _International Conference on Machine Learning_, pages 3897-3906. PMLR, 2020.
* [37] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _Advances in Neural Information Processing Systems_, 31, 2018.
* [38] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. _ACM Trans. Interact. Intell. Syst._, 5(4), dec 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL https://doi.org/10.1145/2827872.
* [39] W. Hashlamoun, P. Varshney, and V. Samarasooriya. A tight upper bound on the Bayesian probability of error. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 16(2):220-224, 1994. doi: 10.1109/34.273728.
* [40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* [41] M. Hellman and J. Raviv. Probability of error, equivocation, and the Chernoff bound. _IEEE Transactions on Information Theory_, 16(4):368-372, 1970. doi: 10.1109/TIT.1970.1054466.
* [42] M. Huai, C. Miao, Y. Li, Q. Suo, L. Su, and A. Zhang. Metric learning from probabilistic labels. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1541-1550, 2018.

* Huang et al. [2017] G. Huang, Z. Liu, L. V. D. Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2261-2269, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society. doi: 10.1109/CVPR.2017.243. URL https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.243.
* 101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/aoms/1177703732.
* Huber [2011] P. J. Huber. Robust statistics. In _International Encyclopedia of Statistical Science_, pages 1248-1251. Springer, 2011.
* Ishida et al. [2023] T. Ishida, I. Yamane, N. Charoenphakdee, G. Niu, and M. Sugiyama. Is the performance of my deep network too good to be true? a direct approach to estimating the Bayes error in binary classification. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=FZdJQgy05rz.
* Javed et al. [2021] U. Javed, K. Shaukat, I. A. Hameed, F. Iqbal, T. M. Alam, and S. Luo. A review of content-based and context-based recommendation systems. _International Journal of Emerging Technologies in Learning (iJET)_, 16(3):274-306, 2021.
* Jeon [2020] E. Jeon. ViT-PyTorch, 2020. https://github.com/jeonsworld/ViT-pytorch,.
* Kailath [1967] T. Kailath. The divergence and Bhattacharyya distance measures in signal selection. _IEEE Transactions on Communication Technology_, 15(1):52-60, 1967. doi: 10.1109/TCOM.1967.1089532.
* Kay [1998] S. M. Kay. _Fundamentals of Statistical Signal Processing, vol. 2: Detection Theory_. Prentice Hall PTR, 1998.
* Kiela et al. [2021] D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and A. Williams. Dynabench: Rethinking benchmarking in NLP. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4110-4124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.324. URL https://aclanthology.org/2021.naacl-main.324.
* Kingma and Welling [2013] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Krizhevsky et al. [2009] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Kumar [2002] P. Kumar. Moments inequalities of a random variable defined over a finite interval. _J. Inequal. Pure Appl. Math_, 3(3):1-24, 2002.
* Lin [1991] J. Lin. Divergence measures based on the Shannon entropy. _IEEE Transactions on Information Theory_, 37(1):145-151, 1991. doi: 10.1109/18.61115.
* Lissack and Fu [1976] T. Lissack and K.-S. Fu. Error estimation in pattern recognition via \(L^{\alpha}\)-distance between posterior density functions. _IEEE Transactions on Information Theory_, 22(1):34-45, 1976. doi: 10.1109/TIT.1976.1055512.
* Liu et al. [2020] S. Liu, J. Niles-Weed, N. Razavian, and C. Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. _Advances in Neural Information Processing Systems_, 33:20331-20342, 2020.
* Loizou and Maybank [1987] G. Loizou and S. J. Maybank. The nearest neighbor and the Bayes error rates. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PAMI-9(2):254-262, 1987. doi: 10.1109/TPAMI.1987.4767899.
* Mania et al. [2019] H. Mania, J. Miller, L. Schmidt, M. Hardt, and B. Recht. Model similarity mitigates test set overuse. _Advances in Neural Information Processing Systems_, 32, 2019.
* Maronna et al. [2019] R. A. Maronna, R. D. Martin, V. J. Yohai, and M. Salibian-Barrera. _Robust Statistics: Theory and Methods (with R)_. John Wiley & Sons, 2019.
* Menon et al. [2021] A. K. Menon, A. S. Rawat, S. Reddi, S. Kim, and S. Kumar. A statistical perspective on distillation. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 7632-7642. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/menon21a.html.

- 5252, 2019. doi: 10.1214/19-EJS1647. URL https://doi.org/10.1214/19-EJS1647.
* Nadaraya [1964] E. A. Nadaraya. On estimating regression. _Theory of Probability & Its Applications_, 9(1):141-142, 1964.
* Natarajan et al. [2013] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari. Learning with noisy labels. _Advances in Neural Information Processing Systems_, 26, 2013.
* Nemirovskij and Yudin [1983] A. S. Nemirovskij and D. B. Yudin. _Problem Complexity and Method Efficiency in Optimization_. Wiley-Interscience, 1983.
* Neyman and Pearson [1933] J. Neyman and E. S. Pearson. Ix. on the problem of the most efficient tests of statistical hypotheses. _Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character_, 231(694-706):289-337, 1933.
* Noshad et al. [2019] M. Noshad, L. Xu, and A. Hero. Learning to benchmark: Determining best achievable misclassification error from training data. _arXiv preprint arXiv:1909.07192_, 2019.
* Patrini et al. [2017] G. Patrini, A. Rozza, A. Krishna Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1944-1952, 2017.
* Peterson et al. [2019] J. C. Peterson, R. M. Battleday, T. L. Griffiths, and O. Russakovsky. Human uncertainty makes classification more robust. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9617-9626, 2019.
* Phan [2021] H. Phan. PyTorch_CIFAR10, 2021. https://github.com/huyvnphan/PyTorch_CIFAR10,.
* Ren et al. [2020] Z. Ren, R. Yeh, and A. Schwing. Not all unlabeled data are equal: Learning to weight data in semi-supervised learning. _Advances in Neural Information Processing Systems_, 33:21786-21797, 2020.
* Renggli et al. [2021] C. Renggli, L. Rimanic, N. Hollenstein, and C. Zhang. Evaluating Bayes error estimators on real-world datasets with FeeBee. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021. URL https://openreview.net/forum?id=ukv5inrWeld.
* Sakai et al. [2017] T. Sakai, M. C. Plessis, G. Niu, and M. Sugiyama. Semi-supervised classification based on classification from positive and unlabeled data. In _International Conference on Mmachine Learning_, pages 2998-3006. PMLR, 2017.
* Sarwar et al. [2001] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based collaborative filtering recommendation algorithms. In _Proceedings of the 10th International Conference on World Wide Web_, WWW '01, page 285-295, New York, NY, USA, 2001. Association for Computing Machinery. ISBN 1581133480. doi: 10.1145/371920.372071. URL https://doi.org/10.1145/371920.372071.
* Sekeh et al. [2020] S. Y. Sekeh, B. Oselio, and A. O. Hero. Learning to bound the multi-class Bayes error. _IEEE Transactions on Signal Processing_, 68:3793-3807, 2020. doi: 10.1109/TSP.2020.2994807.
* Shalev-Shwartz and Ben-David [2014] S. Shalev-Shwartz and S. Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, 2014.
* Shlens [2014] J. Shlens. A tutorial on principal component analysis. _arXiv preprint arXiv:1404.1100_, 2014.
* Simonyan and Zisserman [2015] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. pages 1-14. Computational and Biological Learning Society, 2015.
* Singh et al. [2008] A. Singh, R. Nowak, and J. Zhu. Unlabeled data: Now it helps, now it doesn't. _Advances in Neural Information Processing Systems_, 21, 2008.
* Song et al. [2022] H. Song, M. Kim, D. Park, Y. Shin, and J.-G. Lee. Learning from noisy labels with deep neural networks: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, pages 1-19, 2022. doi: 10.1109/TNNLS.2022.3152527.
* Szegedy et al. [2016] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 2818-2826, 2016.
* Tanveer et al. [2021] M. S. Tanveer, M. U. Karim Khan, and C.-M. Kyung. Fine-tuning darts for image classification. In _2020 25th International Conference on Pattern Recognition_, pages 4789-4796, 2021. doi: 10.1109/ICPR48806.2021.9412221.

* Theisen et al. [2021] R. Theisen, H. Wang, L. R. Varshney, C. Xiong, and R. Socher. Evaluating state-of-the-art classification models against Bayes optimality. _Advances in Neural Information Processing Systems_, 34:9367-9377, 2021.
* Thorndike [1953] R. L. Thorndike. Who belongs in the family? _Psychometrika_, 18(4):267-276, 1953. doi: 10.1007/BF02289263. URL https://doi.org/10.1007/BF02289263.
* Toussaint [1972] G. T. P. Toussaint. _Feature Evaluation Criteria and Contextual Decoding Algorithms in Statistical Pattern Recognition_. PhD thesis, University of British Columbia, 1972.
* Tumer and Ghosh [2003] K. Tumer and J. Ghosh. Bayes error rate estimation using classifier ensembles. _International Journal of Smart Engineering System Design_, 5(2):95-109, 2003. doi: 10.1080/10255810305042. URL https://doi.org/10.1080/10255810305042.
* Unnikrishnan et al. [2018] J. Unnikrishnan, S. Haghighatshoar, and M. Vetterli. Unlabeled sensing with random linear measurements. _IEEE Transactions on Information Theory_, 64(5):3237-3253, 2018.
* Wang et al. [2021] X. Wang, Y. Hua, E. Kodirov, D. A. Clifton, and N. M. Robertson. ProSelfLC: Progressive self label correction for training robust deep neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 752-761, 2021.
* Watson [1964] G. S. Watson. Smooth regression analysis. _Sankhya: The Indian Journal of Statistics, Series A_, pages 359-372, 1964.
* Wei et al. [2023] Q. Wei, L. Feng, H. Sun, R. Wang, C. Guo, and Y. Yin. Fine-grained classification with noisy labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11651-11660, June 2023.
* Werbachowski et al. [2019] R. Werbachowski, A. Gyorgy, and C. Szepesvari. Detecting overfitting via adversarial examples. _Advances in Neural Information Processing Systems_, 32, 2019.
* Xia et al. [2021] X. Xia, T. Liu, B. Han, C. Gong, N. Wang, Z. Ge, and Y. Chang. Robust early-learning: Hindering the memorization of noisy labels. In _International Conference on Learning Representations_, 2021.
* Xiao et al. [2017] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* Yao et al. [2021] Y. Yao, T. Liu, M. Gong, B. Han, G. Niu, and K. Zhang. Instance-dependent label-noise learning under a structural causal model. _Advances in Neural Information Processing Systems_, 34:4409-4420, 2021.
* Yao et al. [2021] Y. Yao, L. Peng, and M. Tsakiris. Unlabeled principal component analysis. _Advances in Neural Information Processing Systems_, 34:30452-30464, 2021.
* Yi and Wu [2019] K. Yi and J. Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7017-7025, 2019.
* Yuan et al. [2023] H. Yuan, N. Xu, Y. Shi, X. Geng, and Y. Rui. Learning from biased soft labels. _arXiv preprint arXiv:2302.08155_, 2023.
* Zhang et al. [2021] C.-B. Zhang, P.-T. Jiang, Q. Hou, Y. Wei, Q. Han, Z. Li, and M.-M. Cheng. Delving deep into label smoothing. _IEEE Transactions on Image Processing_, 30:5984-5996, 2021.
* Zhang et al. [2021] H. Zhang, P. Koniusz, S. Jian, H. Li, and P. H. Torr. Rethinking class relations: Absolute-relative supervised and unsupervised few-shot learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9432-9441, 2021.
* Zhang et al. [2019] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3713-3722, 2019.
* Zhou et al. [2021] H. Zhou, L. Song, J. Chen, Y. Zhou, G. Wang, J. Yuan, and Q. Zhang. Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=gIHd-5X324.

## Appendix A Related work

It is well known that the BER of a binary classification problem can be achieved by a likelihood ratio test [66] and the BER of a multiclass classification problem can be attained by a Maximum a Posteriori (MAP) classifier [50]. The problem of estimating the BER using a dataset has been extensively investigated from the middle of the \(20\)th century [16] to the present [13], leveraging different approaches, such as the \(k\)-nearest neighbors (NN) method, probabilistic divergence, and bounds on the BER. In the early stage, the main approach was to derive upper and lower bounds on the BER, while recent work mainly focuses on directly estimating the BER rather than deriving bounds on it.

BER bounds.Several bounds on the BER in binary classification tasks have been proposed using probability divergence and distance metrics. Notable approaches use the Mahalanobis distance [20, 86], the Bhattacharyya distance between two class conditional distributions [20, 49, 86], the Chernoff coefficient (a generalization of the Bhattacharyya distance) [14, 25, 29], the Henze-Penrose divergence [6], the Jensen-Shannon divergence [55], bounds for the \(\min\{a,b\}\) function [2, 39] (a generalization of the equivocation bound [41] and of the Bayesian bound [12]), the \(M_{0}\)-distance [85], the Fano's inequality [15], and the \(L_{2}\) norm of posterior probabilities [19]. Some of these bounds are tight, but they still require estimating the data distribution (e.g., class conditional distributions or posterior probabilities), which may not be practical, particularly for modern data (e.g., images and videos).10 Bounds on the BER were also derived using non-parametric approaches [16, 21, 30, 33], such as the \(k\)-NN classifier. A remarkable advantage of a \(k\)-NN classifier for this problem is its distribution-free property to bound the BER (i.e., it works under no assumption on the data distribution). A convergence rate analysis for universal BER estimators was developed in [1, 22].

Footnote 10: Interested readers are referred to [2, 11, 56, 72]. In particular, comparisons of a variety of upper bounds on the BER were studied in [56, 72], and an arbitrary tight upper bound was proposed in [2].

Direct estimation of the BER.As an alternative approach to using bounds on the BER, some recent research directly estimates the BER [13, 46, 67, 83]. In particular, the authors in [67] expressed the BER as a function of the \(f\)-divergence and proposed a density ratio estimator to evaluate such an \(f\)-divergence that yields the BER. Under various label assumptions, a direct BER estimator was proposed in [46]; this estimator is unbiased and consistent for soft labels, and it has desirable properties. Another direct estimator for the BER was proposed in [13]. In particular, this estimator uses the relationship between the BER and the miss-classified samples.

Extension to multi-class classification.The extension of the BER in binary classification to multi-class classification has been extensively investigated both in terms of generalizing existing bounds and deriving new approaches. For example, the Chernoff bound [34], the Jensen-Shannon divergence [55], the Fano's inequality [15], the general mean distance [8], the \(L_{2}\) norm of posterior probabilities [19], and the generalized Henze-Penrose divergence [75] can be used to derive bounds on the BER in multi-class classification problems. Non-parametric methods [31, 58], such as the \(k\)-NN classifier, can also bound the BER in multi-class classification settings. Bounds based on the error probability of a \(k\)-NN classifier in the asymptotic regime were studied in [16], and some bounds in the non-asymptotic regime can be found in [10, 32].

Proofs and auxiliary results

### Proof of Theorem 1

Proof.: By taking the expectation of \(\psi_{\mathrm{soft}}\) in (4) with respect to \(P_{\mathbf{X},\mathbf{Y}}\), we obtain

\[\mathbb{E}[\psi_{\mathrm{soft}}(\mathcal{D})] =\mathbb{E}\left[\frac{1}{n}\sum_{(\bm{x},\bm{y})\in\mathcal{D}} \left(1-\max_{j\in[M]}y_{j}\right)\right]\] \[=\frac{1}{n}\sum_{(\bm{x},\bm{y})\in\mathcal{D}}\mathbb{E}\left[1 -\max_{j\in[M]}\Pr(C=j|\mathbf{X})\right]\] \[=1-\mathbb{E}\left[\max_{j\in[M]}\Pr(C=j|\mathbf{X})\right]\] \[=P_{e},\] (16)

where the second equality follows from (3), and the last equality is due to (2). This shows that \(\psi_{\mathrm{soft}}\) in (4) is an unbiased estimator of \(P_{e}\).

Next, let \(U_{k}\overset{i.i.d.}{\sim}Y_{M:M}\) for all \(k\in[n]\), and note that \(U_{k}\in\left[\frac{1}{M},1\right]:=\mathcal{U}\). Define a function \(f:\mathcal{U}^{n}\to\left[0,1-\frac{1}{M}\right]\) as \(f(U_{1},\ldots,U_{n}):=\frac{1}{n}\sum_{k=1}^{n}\left(1-U_{k}\right)\), which is equivalent to \(\psi_{\mathrm{soft}}\) in (4). Then, for any \(u_{k}\in\mathcal{U}\) and for all \(k\in[n]\), we have that

\[\sup_{v\in\mathcal{U}}|f(u_{1},\ldots,u_{i-1},u_{i},u_{i+1}, \ldots,u_{n})-f(u_{1},...,u_{i-1},v,u_{i+1},...,u_{n})|\] \[=\sup_{v\in\mathcal{U}}\left|\frac{1}{n}\left((1-u_{i})-(1-v) \right)\right|\] \[\leq\frac{1}{n}\left(1-\frac{1}{M}\right):=c_{i}.\] (17)

By using McDiarmid's inequality [23], together with (16) and (17), we arrive at

\[\Pr\left(|\psi_{\mathrm{soft}}-P_{e}|\geq\varepsilon\right) =\Pr\left(|f(U_{1},\ldots,U_{n})-\mathbb{E}[f(U_{1},\ldots,U_{n}) ]|\geq\varepsilon\right)\] \[\leq 2\exp\left(-\frac{2n\varepsilon^{2}}{\left(1-\frac{1}{M} \right)^{2}}\right).\] (18)

By taking \(n\to\infty\), we obtain

\[\lim_{n\to\infty}\Pr\left(|\psi_{\mathrm{soft}}-P_{e}|\geq\varepsilon\right) \leq\lim_{n\to\infty}2\exp\left(-\frac{2n\varepsilon^{2}}{\left(1-\frac{1}{M} \right)^{2}}\right)=0,\] (19)

which shows that \(\psi_{\mathrm{soft}}\) in (4) is a consistent estimator of \(P_{e}\).

By setting \(\varepsilon=\sqrt{\frac{\left(1-\frac{1}{M}\right)^{2}}{2n}\ln\frac{2}{\delta}}\) in (18) where \(\delta\in(0,1)\), we obtain

\[\Pr\left(|\psi_{\mathrm{soft}}-P_{e}|<\sqrt{\frac{\left(1-\frac{1}{M}\right)^{ 2}}{2n}\ln\frac{2}{\delta}}\right)>1-\delta.\] (20)

This shows the convergence rate.

The estimator \(\psi_{\mathrm{soft}}\) in (4) can be viewed as the sample mean of \(1-\max_{j\in[M]}y_{j}\) in \(\mathcal{D}\). Since \(\max_{j\in[M]}y_{j}\sim Y_{M:M}\), from the central limit theorem, as \(n\to\infty\), we obtain that

\[\sqrt{n}\left(\psi_{\mathrm{soft}}-P_{e}\right) \to\mathcal{N}(0,\text{Var}(1-Y_{M:M}))\] \[=\mathcal{N}(0,\text{Var}(Y_{M:M})).\] (21)

This concludes the proof of Theorem 1.

### Proof of Proposition 1

Proof.: We have that \(Y_{M:M}\sim\max_{j\in[M]}Y_{j}\). Then, by letting \(U_{k}\stackrel{{ i.i.d.}}{{\sim}}Y_{M:M}\) for all \(k\in[n]\), we obtain

\[\text{Var}(\psi_{\mathrm{soft}}) =\text{Var}\left(\frac{1}{n}\sum_{k=1}^{n}\left(1-U_{k}\right)\right)\] \[=\frac{1}{n}\text{Var}\left(1-U_{k}\right)\] \[=\frac{1}{n}\text{Var}(U_{k})\] \[=\frac{1}{n}\text{Var}(Y_{M:M}).\] (22)

Moreover, by using the Bhatia-Davis inequality [7], we obtain \(\text{Var}\left(Y_{M:M}\right)=\text{Var}\left(1-Y_{M:M}\right)\leq\left(1- \frac{1}{M}-P_{e}\right)P_{e}\) since \(\mathbb{E}[\psi_{\mathrm{soft}}]=P_{e}\), which gives

\[\text{Var}(\psi_{\mathrm{soft}})\leq\frac{\left(1-\frac{1}{M}\right)P_{e}-P_{ e}^{2}}{n}.\] (23)

Finally, we can maximize the term \(\left(1-\frac{1}{M}-P_{e}\right)P_{e}\) over \(0\leq P_{e}\leq 1-\frac{1}{M}\). The maximum is indeed \(\frac{\left(1-\frac{1}{M}\right)^{2}}{4}\) attained by \(P_{e}=\frac{1}{2}\left(1-\frac{1}{M}\right)\). Hence,

\[\text{Var}(\psi_{\mathrm{soft}}) \leq\frac{\left(1-\frac{1}{M}\right)P_{e}-P_{e}^{2}}{n}\] \[\leq\frac{\left(1-\frac{1}{M}\right)^{2}}{4n},\] (24)

which concludes the proof of Proposition 1. 

### Proof of Theorem 2

Proof.: We start by noting that, by viewing \(\psi_{\mathrm{soft}}\) in (4) as a sample mean of \((1-Y_{M:M})\), the estimator \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) is a median-of-means estimator. It has been shown in [62] that, provided that \(\mathbb{E}[|1-Y_{M:M}-P_{e}|^{3}]<\infty\), a median-of-means estimator satisfies

\[|\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})-P_{e}|\leq 3(\text{Var}(Y_{M:M }))^{\frac{1}{2}}\left(\frac{\mathbb{E}[|1-Y_{M:M}-P_{e}|^{3}]}{\text{Var}(Y_{ M:M})^{\frac{3}{2}}}\frac{K}{n-K}+\sqrt{\frac{s}{n-K}}\right),\] (25)

with probability at least \(1-4e^{-2s}\), for all \(s\lesssim K\). From [54, eq. (3.8)], since \((1-Y_{M:M})\in\left[0,1-\frac{1}{M}\right]\) we have that

\[\mathbb{E}[|1-Y_{M:M}-P_{e}|^{3}] \leq P_{e}\left(1-\frac{1}{M}-P_{e}\right)\left(1-\frac{1}{M}-2P_ {e}\right)\] \[\leq\frac{\left(1-\frac{1}{M}\right)^{3}}{6\sqrt{3}},\] (26)

where the last inequality follows by maximizing \(P_{e}\left(1-\frac{1}{M}-P_{e}\right)\left(1-\frac{1}{M}-2P_{e}\right)\) with respect to \(0\leq P_{e}\leq 1-\frac{1}{M}\). Substituting (26) into (25) leads to

\[|\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})-P_{e}| \leq 3(\text{Var}(Y_{M:M}))^{\frac{1}{2}}\left(\frac{\left(1- \frac{1}{M}\right)^{3}}{6\sqrt{3}(\text{Var}(Y_{M:M}))^{\frac{3}{2}}}\frac{K}{n -K}+\sqrt{\frac{s}{n-K}}\right)\] \[=\left(\frac{\left(1-\frac{1}{M}\right)^{3}}{2\sqrt{3}\text{Var}( Y_{M:M})}\frac{K}{\sqrt{n-K}}+3\sqrt{s\text{Var}(Y_{M:M})}\right)\sqrt{\frac{1}{n-K}}.\] (27)

Under an appropriate choice of \(n\) and \(K\) (e.g., \(K=o(n)\)), the upper bound in (27) converges to \(0\), which proves the consistency of \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\).

For the breakdown point of \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\), we start by observing that, if we have \(\left\lfloor\frac{K+1}{2}\right\rfloor\) outliers, then at most \(\left\lfloor\frac{K+1}{2}\right\rfloor\) base estimators \(\psi_{\mathrm{soft}}(\mathcal{D}_{k})\)'s in (6) can be affected by these outliers, i.e., they can be bad. In this case, \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) gives a bad estimate since more than half of the base estimators \(\psi_{\mathrm{soft}}(\mathcal{D}_{k})\)'s are bad. However, if the number of outliers is smaller than \(\left\lfloor\frac{K+1}{2}\right\rfloor\), e.g., \(\left\lfloor\frac{K+1}{2}\right\rfloor-1\), then the median of \(\psi_{\mathrm{soft}}(\mathcal{D}_{k}),\ k\in[K]\) is not a bad estimate since the number of outliers is smaller than half of the base estimators. Thus, from Definition 3 with \(\tau=\left\lfloor\frac{K+1}{2}\right\rfloor\) and \(\kappa+\tau=n\), we obtain \(B\left(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\right)=\left\lfloor \frac{K+1}{2}\right\rfloor\frac{1}{n}\).

The proof of the asymptotic normality of \(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})\) follows directly from (62, Theorem 4). In particular, if \(K\to\infty\) and \(K=o(\sqrt{n})\) as \(n\to\infty\), it holds that

\[\sqrt{n}\left(\mathsf{MoB}_{K}(\psi_{\mathrm{soft}},\mathcal{D})-P_{e}\right) \stackrel{{ d}}{{\to}}\mathcal{N}\left(0,\frac{\pi}{2}\mathsf{ Var}(Y_{M:M})\right).\] (28)

This concludes the proof of Theorem 2. 

### Random permutation noise

One of the most common type of label noises is a noise that randomly permutes/shuffles the data labels [27], i.e., we have \((\bm{x},\tilde{\bm{y}})\in\widetilde{\mathcal{D}}_{\mathrm{P}}\) where \(\widetilde{\mathcal{D}}_{\mathrm{P}}\) denotes the dataset with randomly permuted labels. This noise model can be categorized into two types [27], namely _instance-independent_ and _instance-dependent_. The former perturbs the label \(\bm{y}\) with a random permutation according to some probability distribution (the extreme noise case, namely unlabeled data or data without correspondence, where the label is permuted with probability one using a random permutation matrix, has also been studied extensively [36, 71, 73, 79, 87, 95]), which is independent of the input features. For example, a corruption by an instance-independent noise to \(\bm{y}=[0.1,0.7,0.2]^{\top}\) may result in \(\tilde{\bm{y}}=[0.1,0.2,0.7]^{\top}\). The other type of noise, the instance-dependent noise, flips/permutes the labels depending on the instance. For example, the label "car" is more likely to be flipped to "truck" rather than "cat".

We note that \(\psi_{\mathrm{soft}}\) in (4) only depends on the maximum value of \(\bm{y}\), which does not change after that a random permutation is applied on \(\bm{y}\). This property ensures that \(\psi_{\mathrm{soft}}\) is still an effective estimator of \(P_{e}\) even when the labels are randomly permuted, as stated in the next theorem.

**Theorem 4**.: _Let \(\widetilde{\mathcal{D}}_{\mathrm{P}}=\{(\bm{x}_{i},\tilde{\bm{y}}_{i})\}_{i=1 }^{n}\) be a dataset that consists of noisy soft labels \(\tilde{\bm{y}}_{i}=P_{i}\bm{y}_{i}\) where \(P_{i},i\in[M]\) is any random permutation matrix of size \(M\times M\). Then, \(\psi_{\mathrm{soft}}(\widetilde{\mathcal{D}}_{\mathrm{P}})\) satisfies all of the properties in Theorem 1 and Proposition 1._

Proof.: Since \(\max_{j\in[M]}y_{j}\) is permutation-invariant with respect to \(\bm{y}\), it follows that

\[\psi_{\mathrm{soft}}(\{(\bm{x}_{i},\tilde{\bm{y}}_{i})\}_{i=1}^{n}) =\psi_{\mathrm{soft}}(\{(\bm{x}_{i},P_{i}\bm{y}_{i})\}_{i=1}^{n})\] \[=\psi_{\mathrm{soft}}(\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{n}),\] (29)

where the first equality follows since \(\tilde{\bm{y}}_{i}=P_{i}\bm{y}_{i}\), and the second equality is due to the permutation-invariance property of the \(\max\) function. The results in Theorem 1 and Proposition 1 then prove Theorem 4. 

### Proof of Theorem 3

Proof.: Without loss of generality, we let \(\mathcal{X}=\{\bm{u}_{1},\bm{u}_{2},\ldots,\bm{u}_{K}\}\), where \(K=|\mathcal{X}|\) is the cardinality of \(\mathcal{X}\), and \(\bm{u}_{k}\) is the unique feature vector (or tensor). We denote by \(\bm{v}_{k}\in\mathcal{Y}\) the soft label corresponding to \(\bm{u}_{k}\), i.e., \(\bm{v}_{k}=\left[\Pr(C=1|\mathbf{X}=\bm{u}_{k}),\ldots,\Pr(C=M|\mathbf{X}=\bm{ u}_{k})\right]^{\top}\). With these definitions, we observe that due to the law of large numbers, we have

\[\lim_{n\to\infty}\bm{s}(\bm{u}_{k}) =\mathbb{E}[\widetilde{\mathbf{Y}}|\mathbf{X}=\bm{u}_{k}]\] \[\stackrel{{\mathrm{(a)}}}{{=}}\mathbb{E}[\mathbb{E}[ \widetilde{\mathbf{Y}}|\mathbf{Y}]|\mathbf{X}=\bm{u}_{k}]\] \[\stackrel{{\mathrm{(b)}}}{{=}}\mathbb{E}[\mathbf{Y}| \mathbf{X}=\bm{u}_{k}]\] \[=\bm{v}_{k},\] (30)where \((\mathrm{a})\) follows from the law of total expectation and \((\mathrm{b})\) is due to the fact that \(\mathbb{E}[\widetilde{\mathbf{Y}}|\mathbf{Y}]=\mathbf{Y}\) since the noise is zero mean. Now, since for any \(i\in[n]\) there exists a \(k\in[K]\) such that \(\bm{x}_{i}=\bm{u}_{k}\), we can write that

\[\lim_{n\to\infty}\mathsf{idx}(\bm{x}_{i}) =\lim_{n\to\infty}\mathsf{idx}(\bm{u}_{k})\] \[\stackrel{{\mathrm{(a)}}}{{=}}\arg\max_{j\in[M]}\{( \bm{v}_{k})_{j}\}\] \[=\arg\max_{j\in[M]}\{\Pr(C=j|\mathbf{X}=\bm{u}_{k})\}\] \[:=\widehat{\mathsf{idx}}(\bm{u}_{k}),\] (31)

where the equality in \((\mathrm{a})\) follows from (30). Then, we can write \(\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\) in (12) as

\[\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})=\sum_{k=1}^{K}\frac{ 1}{n}\sum_{i=1}^{n}\mathbbm{1}\{\bm{x}_{i}=\bm{u}_{k}\}(1-(\tilde{\bm{y}}_{i} )_{\mathsf{idx}(\bm{x}_{i})}).\] (32)

When \(n\to\infty\), it follows that

\[\lim_{n\to\infty}\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{ \mathrm{A}}) \stackrel{{\mathrm{(a)}}}{{=}}\sum_{k=1}^{K}\mathbb{E} \left[\mathbbm{1}\{\mathbf{X}=\bm{u}_{k}\}\left(1-\widetilde{\mathbf{Y}}_{ \widehat{\mathsf{idx}}(\bm{u}_{k})}\right)\right]\] \[\stackrel{{\mathrm{(b)}}}{{=}}\sum_{k=1}^{K}\mathbb{E} \left[\mathbbm{1}\{\mathbf{X}=\bm{u}_{k}\}\left(1-\mathbb{E}[\widetilde{ \mathbf{Y}}|\mathbf{Y}]_{\widehat{\mathsf{idx}}(\bm{u}_{k})}\right)\right]\] \[\stackrel{{\mathrm{(c)}}}{{=}}\sum_{k=1}^{K}\mathbb{E} \left[\mathbbm{1}\{\mathbf{X}=\bm{u}_{k}\}\left(1-\mathbf{Y}_{\widehat{ \mathsf{idx}}(\bm{u}_{k})}\right)\right]\] \[=\sum_{k=1}^{K}\mathbb{E}\left[\mathbbm{1}\{\mathbf{X}=\bm{u}_{k }\}\left(1-\max_{j\in[M]}\Pr(C=j|\mathbf{X}=\bm{u}_{k})\right)\right]\] \[=\sum_{k=1}^{K}\mathbb{E}\left[1-\max_{j\in[M]}\Pr(C=j|\mathbf{X }=\bm{u}_{k})\;\middle|\;\mathbf{X}=\bm{u}_{k}\right]\Pr(\mathbf{X}=\bm{u}_{k})\] \[=\mathbb{E}\left[1-\max_{j\in[M]}\Pr(C=j|\mathbf{X})\right]\] \[=P_{e},\] (33)

where the labeled equalities follow from: \((\mathrm{a})\) the law of large numbers, the assumption that \((\bm{x}_{i},\tilde{\bm{y}}_{i})\sim P_{\mathbf{X},\widetilde{\mathbf{Y}}}\), and using (31); \((\mathrm{b})\) using the law of total expectation; and \((\mathrm{c})\) the fact that \(\mathbb{E}[\widetilde{\mathbf{Y}}|\mathbf{Y}]=\mathbf{Y}\) since the noise is zero mean. This proves the consistency of \(\psi_{\mathrm{DN}}\).

The asymptotic unbiasedness directly follows from the consistency together with the assumption of bounded support for the noise. Specifically, since \(Z_{j}\in[a,b]\) for all \(j\in[M]\), we have that

\[\lim_{n\to\infty}\mathbb{E}\left[\psi_{\mathrm{DN}}(\widetilde{ \mathcal{D}}_{\mathrm{A}})\right] =\mathbb{E}\left[\lim_{n\to\infty}\psi_{\mathrm{DN}}(\widetilde{ \mathcal{D}}_{\mathrm{A}})\right]\] \[=\mathbb{E}[P_{e}]\] \[=P_{e},\] (34)

where the first equality follows by the dominated convergence theorem that can be applied since \(|\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})|\leq\max\{|a|,|1- \frac{1}{M}+b|\}\), where \(a\) and \(b\) are the minimum and the maximum value in the support of the noise, respectively, and they are both finite. This shows the asymptotic unbiasedness of \(\psi_{\mathrm{DN}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\).

We are left to prove the denoising consistency. The denoised label \(\bm{s}(\bm{x}_{i})\) in (11) is an unbiased estimator of \(\bm{y}_{i}\) since

\[\mathbb{E}[\bm{s}(\mathbf{X}_{i})] \stackrel{{\rm(a)}}{{=}}\mathbb{E}\left[\frac{\sum_{j= 1}^{n}\mathbbm{1}\{\mathbf{X}_{i}=\mathbf{X}_{j}\}\widetilde{\mathbf{Y}}_{j}}{ \sum_{j=1}^{n}\mathbbm{1}\{\mathbf{X}_{i}=\mathbf{X}_{j}\}}\right]\] \[\stackrel{{\rm(b)}}{{=}}\mathbb{E}\left[\mathbb{E} \left[\frac{\sum_{j=1}^{n}\mathbbm{1}\{\mathbf{X}_{i}=\mathbf{X}_{j}\} \widetilde{\mathbf{Y}}_{j}}{\sum_{j=1}^{n}\mathbbm{1}\{\mathbf{X}_{i}=\mathbf{ X}_{j}\}}\ \middle|\ N,\mathbf{X}_{i}\right]\right]\] \[\stackrel{{\rm(c)}}{{=}}\mathbb{E}\left[\frac{1}{N} \sum_{k=1}^{N}\mathbb{E}\left[\widetilde{\mathbf{Y}}_{k}\ \middle|\ N,\mathbf{X}_{i}\right]\right]\] \[\stackrel{{\rm(d)}}{{=}}\mathbb{E}\left[\frac{1}{N} \sum_{k=1}^{N}\mathbb{E}\left[\mathbf{Y}_{i}\ \middle|\ \mathbf{X}_{i}\right]\right]\] \[=\mathbb{E}[\mathbf{Y}_{i}|\mathbf{X}_{i}],\] (35)

where the labeled equalities follow from: \(\rm(a)\) letting \(\mathbf{X}_{j}\), \(\mathbf{Y}_{j}\), and \(\widetilde{\mathbf{Y}}_{j}\) for all \(j\in[n]\) be independent copies of \(\mathbf{X}\), \(\mathbf{Y}\), and \(\widetilde{\mathbf{Y}}\), respectively; \(\rm(b)\) introducing a random variable \(N\in[n]\) that counts the number of \(\mathbf{X}_{j}\) such that \(\mathbf{X}_{j}=\mathbf{X}_{i}\) and using the law of total expectation; \(\rm(c)\) re-indexing \(\widetilde{\mathbf{Y}}_{k},\ k\in[N]\) for the \(N\) pairs \((\mathbf{X}_{j},\widetilde{\mathbf{Y}}_{k})\) such that \(\mathbf{X}_{i}=\mathbf{X}_{j}\); and \(\rm(d)\) the fact that the noise is zero mean.

Note that \(\bm{s}(\bm{x}_{i})\) is the average of \(n_{\bm{x}_{i}}\) noisy labels \(\tilde{\bm{y}}\)'s corresponding to \(\bm{x}_{i}\). By Hoeffding's inequality, we then obtain

\[\Pr\left(|\bm{s}(\bm{x}_{i})-\bm{y}_{i}|\geq\varepsilon\right) \leq 2\exp\left(-\frac{2n_{\bm{x}_{i}}\varepsilon^{2}}{\left(1-\frac{1}{M}-a +b\right)^{2}}\right).\] (36)

Setting \(\varepsilon=\sqrt{\frac{\left(1-\frac{1}{M}-a+b\right)^{2}}{2n_{\bm{x}_{i}}} \ln\frac{2}{\delta}}\) leads to

\[\Pr\left(|\bm{s}(\bm{x}_{i})-\bm{y}_{i}|<\sqrt{\frac{\left(1-\frac{1}{M}-a+b \right)^{2}}{2n_{\bm{x}_{i}}}\ln\frac{2}{\delta}}\right)>1-\delta.\] (37)

This concludes the proof of Theorem 3. 

### Properties of \(\psi_{\rm genie}(\widetilde{\mathcal{D}}_{\rm A})\)

The following proposition defines \(\psi_{\rm genie}\) and provides some properties of it.

**Proposition 2**.: _Let \(\widetilde{\mathcal{D}}_{\rm A}=\{(\bm{x}_{i},\tilde{\bm{y}}_{i})\}_{i=1}^{n}\) be a dataset that consists of noisy soft labels \(\tilde{\bm{y}}_{i}=\bm{y}_{i}+\bm{z}_{i}\) with \(\bm{z}_{i}\stackrel{{ i.i.d.}}{{\sim}}P_{\mathbf{Z}}\) such that \(\mathbb{E}[\mathbf{Z}]=\bm{0}\). Let \(c_{i}=\arg\max_{j\in[M]}(\bm{y}_{i})_{j}\). Then, the following estimator_

\[\psi_{\rm genie}(\widetilde{\mathcal{D}}_{\rm A})=\frac{1}{n} \sum_{i=1}^{n}\left(1-(\tilde{\bm{y}}_{i})_{c_{i}}\right)\] (38)

_satisfies the following properties:_

1. (Unbiasedness): _It is an unbiased estimator of the BER;_
2. (Consistency): _It is a consistent estimator of the BER;_
3. (Variance): \(\text{Var}\left(\psi_{\rm genie}(\widetilde{\mathcal{D}}_{\rm A})\right)=\frac {\text{Var}(Y_{M,M})+\text{Var}(Z)}{n}\)_;_
4. (Asymptotic Normality): \(\sqrt{n}(\psi_{\rm genie}(\widetilde{\mathcal{D}}_{\rm A})-P_{e})\stackrel{{ d}}{{\to}}\mathcal{N}(0,\text{Var}(Y_{M:M})+\text{Var}(Z))\) _as_ \(n\to\infty\)Proof.: Define the random variable \(C=\arg\max_{j\in[M]}Y_{j}\) that indicates the index of \(\mathbf{Y}\) having the largest value, i.e., \(Y_{C}=\max_{j\in[M]}Y_{j}\). We have that

\[\mathbb{E}[\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{\mathrm{ A}})] =\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[1-\widetilde{\mathbf{Y}}_{C}]\] \[\stackrel{{\mathrm{(a)}}}{{=}}\mathbb{E}[\mathbb{E} [1-\widetilde{\mathbf{Y}}_{C}|\mathbf{Y}]]\] \[\stackrel{{\mathrm{(b)}}}{{=}}\mathbb{E}[1- \mathbf{Y}_{C}]\] \[=\mathbb{E}\left[1-\max_{j\in[M]}Y_{j}\right]\] \[=P_{e},\] (39)

where the labeled equalities follow from: \(\mathrm{(a)}\) the law of total expectation; and \(\mathrm{(b)}\) the fact that \(\mathbb{E}[\widetilde{\mathbf{Y}}|\mathbf{Y}]=\mathbf{Y}\) since \(\mathbb{E}[\mathbf{Z}]=\mathbf{0}\). This shows the unbiasedness of \(\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\).

Now, we show the consistency of \(\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\). Due to the law of large numbers, as \(n\to\infty\), we obtain that

\[\lim_{n\to\infty}\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{ \mathrm{A}}) =\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\left(1-(\tilde{\bm{y} }_{i})_{c_{i}}\right)\] \[=\mathbb{E}[1-\widetilde{\mathbf{Y}}_{C}]\] \[=\mathbb{E}[\mathbb{E}[1-\widetilde{\mathbf{Y}}_{C}|\mathbf{Y}]]\] \[=\mathbb{E}[1-\mathbf{Y}_{C}]\] \[=P_{e},\] (40)

which demonstrates the consistency of \(\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\).

We now compute the variance of \(\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{\mathrm{A}})\). We have that

\[\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{\mathrm{A}}) =\frac{1}{n}\sum_{i=1}^{n}\left(1-(\tilde{\bm{y}}_{i})_{c_{i}}\right)\] \[=\frac{1}{n}\sum_{i=1}^{n}\left(1-(\bm{y}_{i})_{c_{i}}-(\bm{z}_{ i})_{c_{i}}\right)\] \[=\psi_{\mathrm{soft}}(\mathcal{D})-\frac{1}{n}\sum_{i=1}^{n}(\bm{ z}_{i})_{c_{i}}.\] (41)

This yields

\[\text{Var}\left(\psi_{\mathrm{genie}}(\widetilde{\mathcal{D}}_{ \mathrm{A}})\right) =\text{Var}\left(\psi_{\mathrm{soft}}(\mathcal{D})-\frac{1}{n}\sum_ {i=1}^{n}Z_{c_{i}}\right)\] \[\stackrel{{\mathrm{(a)}}}{{=}}\text{Var}\left(\psi_{ \mathrm{soft}}(\mathcal{D})\right)+\frac{1}{n}\text{Var}\left(Z\right)\] \[\stackrel{{\mathrm{(b)}}}{{=}}\frac{\text{Var}(Y_{M :M})}{n}+\frac{1}{n}\text{Var}\left(Z\right),\] (42)

where \(\mathrm{(a)}\) follows from the independence between \(\mathbf{Y}\) and \(\mathbf{Z}\), and the fact that \(Z_{j}\)'s (with \(j\in[M]\)) are i.i.d.; and \(\mathrm{(b)}\) follows by Proposition 1.

Finally, to show the asymptotic normality, we observe that from (41), we have that

\[\psi_{\rm genie}(\widetilde{\mathcal{D}}_{\rm A}) =\frac{1}{n}\sum_{i=1}^{n}\left(1-(\bm{y}_{i})_{c_{i}}-(\bm{z}_{i}) _{c_{i}}\right)\] \[=\frac{1}{n}\sum_{i=1}^{n}\left(1-\max_{j\in[M]}(\bm{y}_{i})_{j}-( \bm{z}_{i})_{c_{i}}\right)\] \[\stackrel{{ d}}{{=}}\frac{1}{n}\sum_{i=1}^{n}\left(1- \max_{j\in[M]}(\bm{y}_{i})_{j}-(\bm{z}_{i})_{1}\right),\] (43)

where the last equality follows since \((\bm{z}_{i})_{k}\stackrel{{ d}}{{=}}(\bm{z}_{i})_{\ell}\) for all \((k,\ell)\in[M]^{2}\). Since (43) is the sample mean of \(n\) realizations from \(1-Y_{M:M}-Z\), the central limit theorem leads to

\[\sqrt{n}(\psi_{\rm genie}(\widetilde{\mathcal{D}}_{\rm A})-P_{e}) \stackrel{{ d}}{{\to}}\mathcal{N}(0,\text{Var}(1-Y_{M:M} -Z))\] \[\stackrel{{ d}}{{=}}\mathcal{N}(0,\text{Var}(Y_{M:M} )+\text{Var}(Z)),\] (44)

where the last equality is due to the independence between \(Y_{M:M}\) and \(Z\). This concludes the proof of Proposition 2. 

## Appendix C Details on existing BER estimators / Additional experiments

### Details on existing BER estimators

#### c.1.1 k-NN BER bounds [16]

Let \(P_{e}^{\rm NN}\) be the error rate of the 1-nearest neighbor (NN) classifier with \(n\) samples. Then, for binary classification, it follows that for \(n\to\infty\)[16],

\[\frac{1}{2}\left(1-\sqrt{1-2P_{e}^{\rm NN}}\right)\leq P_{e}\leq P_{e}^{\rm NN}.\] (45)

The above bounds were proved by showing the convergence of the conditional posterior probability of the nearest neighbor to the true conditional posterior probability. For details, we refer an interested reader to [16].

To generalize (45) to \(M\)-classification problems, it was shown that for \(n\to\infty\)[16],

\[\frac{M-1}{M}\left(1-\sqrt{1-\frac{M}{M-1}P_{e}^{\rm NN}}\right)\leq P_{e}\leq P _{e}^{\rm NN}.\] (46)

In our experiments, we chose \(k=1\) as this value provides the tightest bounds among larger values of \(k\)[72, 16], and we plotted the upper and lower bounds defined in (46). In particular, in order to evaluate \(P_{e}^{\rm NN}\) on given data samples, we generated true one-hot labels for all data samples from their soft labels by taking the index having the maximum value in the soft labels.

#### c.1.2 Generalized Henze-Penrose (GHP) divergence BER bounds [75]

Consider a parameter \(p\in(0,1)\) and two distributions \(f_{0}\) and \(f_{1}\). Then, the Henze-Penrose (HP) divergence between \(f_{0}\) and \(f_{1}\) is defined as,

\[D_{p}(f_{0},f_{1})=\frac{1}{4p(1-p)}\left(\int\frac{(pf_{0}(\bm{x})-(1-p)f_{1} (\bm{x}))^{2}}{pf_{0}(\bm{x})+(1-p)f_{1}(\bm{x})}{\rm d}\bm{x}-(2p-1)^{2}\right).\] (47)

An appealing feature of \(D_{p}\) is that it can be estimated directly from the data using the generalized Friedman-Rafsky (FR) statistic [75, 28], which uses the Euclidean minimal spanning tree.

Then, for \(f_{0}\) and \(f_{1}\) with prior probabilities \(p\) and \(1-p\), the BER is bounded as [6],

\[\frac{1}{2}-\frac{1}{2}\sqrt{u_{p}(f_{0},f_{1})}\leq P_{e}\leq\frac{1}{2}- \frac{1}{2}u_{p}(f_{0},f_{1}),\] (48)where

\[u_{p}(f_{0},f_{1})=4p(1-p)D_{p}(f_{0},f_{1})+(2p-1)^{2},\] (49)

and \(D_{p}(f_{0},f_{1})\) can be estimated based on the minimal spanning tree [6].

To generalize the HP bounds in (48), consider an \(M\)-class classification problem with \(p_{1},\ldots,p_{M}\) as class prior probabilities and class conditional probability densities given by \(f_{k}(\bm{x}):=f(\bm{x}|c=k),k=1,\ldots,M\). Let \(f^{(M)}(\bm{x}):=\sum_{k=1}^{M}p_{k}f_{k}(\bm{x})\). Then, the generalized Henze-Penrose (GHP)-integral is defined as,

\[\mathsf{GHP}^{(M)}(f_{i},f_{j})=\int_{\mathcal{S}}\frac{f_{i}(\bm{x})f_{j}(\bm {x})}{f^{(M)}(\bm{x})}\mathrm{d}\bm{x},\] (50)

where \(\mathcal{S}\) is the support of \(f^{(M)}(\bm{x})\). Let \(\delta^{(M)}_{i,j}:=\int\frac{p_{i}p_{j}f_{i}(\bm{x})f_{j}(\bm{x})}{f^{(M)}( \bm{x})}\mathrm{d}\bm{x}\). Then, it was shown in [75] that the BER upper bound is

\[P_{e}\leq 2\sum_{i=1}^{M-1}\sum_{j=i+1}^{M}\delta^{(M)}_{i,j},\] (51)

and the BER lower bound is

\[P_{e}\geq\frac{M-1}{M}\left(1-\left(1-\frac{2M}{M-1}\sum_{i=1}^{M-1}\sum_{j=i +1}^{M}\delta^{(M)}_{i,j}\right)^{\frac{1}{2}}\right).\] (52)

In our experiments, we plotted the GHP BER upper and lower bounds defined in (51) and (52). Similar to the \(k\)-NN BER bounds, we generated true one-hot labels for all data samples before evaluating the GHP divergence \(D_{p}\). Computing the above bounds requires to evaluate \(\delta^{(M)}_{i,j}\) and this can be done by using the generalized FR statistics (for more details, see [75, Section IV.]).

### Additional experiments

In Figure 5, we compare the estimated BER with the misclassification error rate of a few popular neural network models on the CIFAR-\(10\) and CIFAR-\(10\)H datasets. We chose ResNet [40], VGG [78],

Figure 5: Misclassification error rates of several classifiers on CIFAR-10 and CIFAR-10H datasets.

DenseNet [43], and Vision Transformer (ViT) [24] as classifiers to compare with and we trained these models on the CIFAR-\(10\) dataset.11 For the error rate evaluated on the CIFAR-10H dataset, we followed the same experiment setup as in [46], under which we generated the ground-truth labels for testset images from the soft labels (i.e., the probabilities for each class being associated with the input image) and evaluated the testset error rate. We iterated this procedure \(20\) times and plotted the average of the error rates. Figure 5 shows that the error rates on CIFAR-\(10\) from the models are worse than the estimated BER, with the exception of ViT. As noted in [46], the reasons for having a lower error rate for ViT than the estimated BER are mainly due to 1) testset over-fitting and 2) various labeling difficulties on the dataset CIFAR-\(10\)H that is used for estimating the BER. After compensating for such difficulties on CIFAR-\(10\)H, all models' error rates (evaluated on CIFAR-\(10\)H) are worse than the estimated BER; this suggests that the estimated BER may be a good estimation since the error rate of any classifier is larger than the BER.

Footnote 11: We fine-tuned the official ViT-16B model (pre-trained on imagenet21k) on CIFAR-\(10\) dataset; for this we used the implementation in [48]. Moreover, we trained the other models using the implementation in [70].

Figure 6 shows the comparison of \(\mathsf{MoB}_{K}(\psi_{\mathsf{C}})\) with the GHP and \(k\)-NN bounds on the BER. For each class \(\bm{c}\in\{(1,1),(-1,1),(-1,-1),(1,-1)\}\), we generated \(500\) Gaussian samples \(\mathcal{N}(\bm{c},\sigma^{2}I_{2})\) to have a total of \(2,000\) samples. The label noise was generated according to \(\mathbf{Z}\sim\mathcal{N}(\mathbf{0},\frac{1}{5}I_{4})\). The case of no outliers is considered in this experiment. We observe that our BER estimator outperforms the other estimators over \(\sigma^{2}\in[0.1,2]\) in both cases of noiseless soft labels and noisy soft labels.

We also made performance comparisons varying different parameters (i.e., \(\mu,\sigma^{2},r\)) in Figures 7, 8, 9, and 10. Similar to the experiment setting for Figure 6, we generated a total of \(2,000\) Gaussian samples with different parameters that are specified in the caption of each figure. The effect of \(\mu\) and \(\sigma^{2}\) on the BER estimation is shown in Figures 7 and 8 under different choices of the radius \(r\). We also provide performance comparisons of our estimator with the other estimators for different values of the noise power \(\sigma^{2}_{\mathbf{Z}}\) in Figures 9 and 10. Choosing an appropriate value for \(r\) is critical in the estimation of the BER as shown in Figure 10. An appropriate value for \(r\) can be empirically determined by using the elbow method [84], which is commonly used for choosing the number of clusters in \(k\)-means clustering algorithms. Using Figure 10, we selected \(r=0.2\) (elbow point) for the main experiment in Figure 2. Overall, from these figures, we observe that our estimator outperforms the GHP and the \(1\)-NN classifier BER bounds in various settings.

Figure 6: Comparison of \(\mathsf{MoB}_{K}(\psi_{\mathsf{C}})\) with the GHP bounds and the \(k\)-NN bounds with \(k=1\). The Gaussian samples are generated according to \(\mathcal{N}(\bm{c},\sigma^{2}I_{2})\), where \(\bm{c}\in\{(1,1),(-1,1),(-1,-1),(1,-1)\}\) is equally likely. We consider \(\mathbf{Z}\sim\mathcal{N}\left(\mathbf{0},\frac{1}{5}I_{4}\right)\). \(\mathsf{MoB}_{K}(\psi_{\mathsf{C}})\) uses \(K=\lfloor\sqrt{n}\rfloor\), the Euclidean distance for d, and \(r=1/5\). We iterate the experiments \(10\) times and plot the average of them.

Figure 8: Comparison of \(\mathsf{MoB}_{K}(\psi_{\mathsf{C}})\) with the GHP bounds and the \(k\)-NN bounds with \(k=1\). The Gaussian samples are generated according to \(\mathcal{N}(\bm{c},\sigma^{2}I_{2})\), where \(\bm{c}\in\{(1,1),(-1,1),(-1,-1),(1,-1)\}\) with equal probability. We consider \(\mathbf{Z}\sim\mathcal{N}\left(\bm{0},\frac{1}{5}I_{4}\right)\). \(\mathsf{MoB}_{K}(\psi_{\mathsf{C}})\) uses \(K=\lfloor\sqrt{n}\rfloor\), the Euclidean distance for d, and \(r\in\{0.3,0.4\}\). We iterate the experiments \(10\) times and plot the average of them. As expected, a larger value of \(r\) in Figure 7(b) yields better performance than a smaller value of \(r\) in Figure 7(a).

Figure 10: Comparison of \(\mathsf{MoB}_{K}(\psi_{\mathsf{C}})\) with the GHP bounds and the \(k\)-NN bounds with \(k=1\) with respect to the cluster radius \(r\). The Gaussian samples are generated according to \(\mathcal{N}(\bm{c},I_{2})\), where \(\bm{c}\in\{(1,1),(-1,1),(-1,-1),(1,-1)\}\). We consider \(\mathbf{Z}\sim\mathcal{N}\left(\bm{0},\sigma_{\mathbf{Z}}^{2}I_{4}\right)\). \(\mathsf{MoB}_{K}(\psi_{\mathsf{C}})\) uses \(K=\lfloor\sqrt{n}\rfloor\) and the Euclidean distance for \(\mathsf{d}\). We iterate the experiments \(10\) times and plot the average of them.