# Consistency of Neural Causal Partial Identification

Jiyuan Tan

Management Science and Engineering

Stanford University

Stanford, CA 94305

jiyuantan@stanford.edu

&Jose Blanchet

Management Science and Engineering

Stanford University

Stanford, CA 94305

jose.blanchet@stanford.edu

&Vasilis Syrgkanis

Management Science and Engineering

Stanford University

Stanford, CA 94305

vsyrgk@stanford.edu

###### Abstract

Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [52, 3]. However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models. In this work, we prove the consistency of partial identification via NCMs in a general setting with both continuous and categorical variables. Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase. In particular, we provide a counterexample showing that without Lipschitz regularization this method may not be asymptotically consistent. Our results are enabled by new results on the approximability of Structural Causal Models (SCMs) via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds.

## 1 Introduction

Identifying causal quantities from observational data is an important problem in causal inference which has wide applications in economics [1], social science [20], health care [34, 18], and recommendation systems [9]. One common approach is to transform causal quantities into statistical quantities using the ID algorithm [49] and deploy general purpose methods to estimate the statistical quantity. However, in the presence of unobserved confounding, typically the causal quantity of interest will not be point-identified by observational data, unless special mechanisms are present in the data generating process (e.g. instruments, unconfounded mediators, proxy controls). In the presence of unobserved confounding, the ID algorithm will typically fail to return a statistical quantity and declare the causal quantity as non-identifiable.

One remedy to this problem, which we focus on in this paper, is partial identification, which aims to give informative bounds for causal quantities based on the available data. At a high level, partial identification bounds can be defined as follows: find the maximum and the minimum value that a target causal quantity can take, among all Structural Causal Models (SCMs) that give rise to the same observed data distribution and respect the given causal graph (as well as any other structuralconstraints that one is willing to impose). Note that in the presence of unobserved confounding, there will typically exist many structural mechanisms that could give rise to the same observational distribution but have vastly different counterfactual distributions. Hence, partial identification can be formulated as solving a max and a min optimization problem [3]

\[\max_{\mathcal{M}\in\mathcal{C}} \setminus\min_{\mathcal{M}\in\mathcal{C}}\theta(\mathcal{M}), \tag{1}\] \[\text{subject to }P^{\mathcal{M}}(\mathbf{V})=P^{\mathcal{M}^{*}}(\mathbf{V}), \qquad\text{and}\qquad\mathcal{G}_{\mathcal{M}}=\mathcal{G}_{\mathcal{M}^{*}},\]

where \(\theta(\mathcal{M})\) is the causal quantity of interest, \(\mathcal{M}^{*}\) is the true model, \(\mathbf{V}\) is the set of observed nodes, \(P^{\mathcal{M}}(\mathbf{V})\) is the distribution of \(\mathbf{V}\) in SCM \(\mathcal{M}\), \(\mathcal{C}\) is a collection of causal models and \(\mathcal{G}_{\mathcal{M}}\) is the causal graph of \(\mathcal{M}\) (see Section 2 for formal definitions). Two recent lines of work explore the optimization approach to partial identification. The first line deals with discrete Structure Causal Models (SCMs), where all observed variables are finitely supported. In this case, (1) becomes a Linear Programming (LP) or polynomial programming problem and tight bounds can be obtained [37, 4, 5, 27, 44, 45, 40, 8, 54, 56, 14]. The second line of work focuses on continuous models and explores ways of solving (1) in continuous settings using various techniques [24, 32, 29, 38, 3, 41].

Recently, Xia et al. [51] formalized the connection between SCMs and generative models (see also [33] for an earlier version of a special case of this connection). This work showcased that SCMs can be interpreted as neural generative models, namely Neural Causal Models (NCMs), that follow a particular architecture that respects the constraints encoded in a given causal graph. Hence, counterfactual quantities of SCMs can be learned by optimizing over the parameters of the underlying generative models. However, there could be multiple models that lead to the same observed data distribution, albeit have different counterfactual distributions. Xia et al. [51] first analyze the approximation power of NCMs for discrete SCMs and employ the max/min approach to verify the identifiability of causal quantities, without the need to employ the ID algorithm. Balazadeh et al. [3] and Hui et al. [30], extend the method in [51] and re-purpose it to perform partial identification by solving the min and max problem in the partial identification formulation over neural causal models.

However, for SCMs with general random variables and functional relationships, the approximation error and consistency of this optimization-based approach to partial identification via NCMs has not been established. In particular, two problems remain open. First, given an arbitrary SCM, it is not yet known if we can find an NCM that produces approximately the same intervention distribution as the original one. Although Xia et al.[51] show it is possible to represent any discrete SCM by an NCM, their construction highly relies on the discrete assumption and cannot be directly generalized to the general case. Moreover, Xia et al. [51] use step functions as the activation function in their construction, which may create difficulties in the training process since step functions are discontinuous with a zero gradient almost everywhere.

Second, since we only have access to \(n\) samples from the true distribution, \(P^{\mathcal{M}}(\mathbf{V})\), we need to replace the constraints in (1) with their empirical version that uses the empirical distribution of samples \(P_{n}^{\mathcal{M}^{*}}(\mathbf{V})\) in place of the population distribution and looks for NCMs, whose implied distribution lies within a small distance from the empirical distribution. Moreover, even the NCM distribution is typically only accessible through sampling, hence we will need to generate \(m_{n}\) samples from the NCM and use the empirical distribution of the \(m_{n}\) samples from the NCM in place of the true distribution implied by the NCM. Thus, in practice, we will use a constraint of the form \(d(P_{n}^{\mathcal{M}^{*}}(\mathbf{V}),P_{m_{n}}^{\mathcal{M}}(\mathbf{V}))\leqslant \alpha_{n}\), where \(d\) is some notion of distribution distance and \(\alpha_{n}\) accounts for the sampling error. It is not clear that this approach is consistent, converges to the correct partial identification bounds, when the sample size \(n\) grows to infinity. Balazadeh et al. [3] only show the consistency of this approach for linear SCMs. Consistency results concerning more general SCMs is still lacking in the neural causal literature.

In this paper, we establish representation and consistency results for general SCMs. Our contributions are summarized as follows.

* We show that under suitable regularity assumptions, given any Lipschitz SCM, we can approximate it using an NCM such that the Wasserstein distance between any interventional distribution of the NCM and the original SCM is small. Each random variable of the SCM is allowed to be continuous or categorical. We specify two architectures of the Neural Networks (NNs) that can be trained using common gradient-based optimization algorithms (Theorem 2, Theorem 3 and Corollary 1).

* To construct the NCM approximation, we develop a novel representation theorem of probability measures (Proposition 1) that may be of independent interest. Proposition 1 implies that under certain assumptions, probability distributions supported on the unit cube can be simulated by pushing forward a multivariate uniform distribution.
* We discover the importance of Lipschitz regularization by constructing a counterexample where the neural causal approach is not consistent without regularization (Proposition 2).
* Using Lipschitz regularization, we prove the consistency of the neural causal approach (Theorem 4).

Related WorkThere exists a rich literature on partial identification of average treatment effects (ATE) [37, 4, 5, 27, 44, 45, 40, 8, 54, 56, 14, 7, 39, 26]. Balke and Pearl [4, 5] first give an algorithm to calculate bounds on the ATE in the Instrumental Variable (IV) setting. They show that regardless of the exact distribution of the latent variables, it is sufficient to consider discrete latent variables as long as all endogenous variables are finitely supported. Moreover, they discover that (1) is an LP problem with a closed-form solution. This LP-based technique was generalized to several special classes of SCMs [44, 8, 27, 46]. For general discrete SCMs, [14, 56, 55] consider transforming the problem (1) into a polynomial programming problem. Xia et al. [51] discover the connection between generative models and SCMs. They show that NCMs are expressive enough to approximate discrete SCMs. By setting \(\mathcal{C}\) in problem (1) to be the collection of NCMs, they apply NCMs for identification and estimation.

For causal models with continuous random variables, the constraints in (1) become integral equations, which makes the problem more difficult. One approach is to discretize the constraints. [24] uses stochastic process representation of the causal model in the continuous IV setting and transforms the problem into a semi-infinite program. [32] relaxes the constraints to finite moment equations and solves the problem by the Augmented Lagrangian Method (ALM). The other approach is to use generative models to approximate the observational distribution and use some metric to measure the distance between distributions. [33] first propose to use GAN to generate images. Later, [29] uses Wasserstein distance in the constraint and transforms the optimization problem into a min and max problem. Similarly, [3] solves the optimization problem using Sinkhorn distance to avoid instability during training. They propose to estimate the Average Treatment Derivative (ATD) and use ATD to obtain a bound on the ATE. They also prove the consistency of the proposed estimator for linear SCMs. [41] uses a linear combination of basis functions to approximate response functions. [26] uses sieve to solve the resulting optimization problem in the IV setting. [19] proposes a neural network framework for sensitivity analysis under unobserved confounding.

Organization of this paperIn Section 2, we introduce the notations and some basic concepts used throughout the paper. Next, in Section 3, we demonstrate how to construct an NCM so that they can approximate a given SCM arbitrarily well. Two kinds of architecture are given along with an approximation error analysis. In Section 4, we highlight the importance of Lipschitz regularization by giving a counterexample that is not consistent. Then, leveraging the previous approximation results, we are able to prove the consistency of this approach under regularization. Finally, we compare our method with the traditional polynomial programming method empirically in Section 4.1.

## 2 Preliminary

First, we introduce the definition of an SCM. Throughout the paper, we use bold symbols to represent sets of random variables.

**Definition 1**.: _(Structural Causal Model) A Structural Causal Model (SCM) is a tuple \(\mathcal{M}=(\mathbf{V},\mathbf{U},F,P(\mathbf{U}),\mathcal{G}_{0})\), where \(\mathbf{V}=\{V_{i}\}_{i=1}^{n_{V}}\) is the set of observed variables; \(\mathbf{U}=\{U_{j}\}_{j=1}^{n_{U}}\) is the set of latent variables; \(P(\mathbf{U})\) is the distribution of latent variables; \(\mathcal{G}_{0}\) is an acyclic directed graph whose nodes are \(\mathbf{V}\). The values of each observed variable \(V_{i}\) are generated by_

\[V_{i}=f_{i}\left(\text{Pa}(V_{i}),\mathbf{U}_{V_{i}}\right),\quad\text{where }V_{i} \notin\text{Pa}(V_{i})\text{ and }\mathbf{U}_{V_{i}}\subset\mathbf{U}, \tag{2}\]

_where \(F=(f_{1},\cdots,f_{n_{V}})\), \(\text{Pa}(V)\) is the set of parents of \(V\) in graph \(\mathcal{G}_{0}\) and \(\mathbf{U}_{V_{i}}\) is the set of latent variables that affect \(V_{i}\). \(V_{i}\) takes either continuous values \(\mathbb{R}^{d_{i}}\) or categorical in \([n_{i}]\). We extend graph \(\mathcal{G}_{0}\) by adding bi-directed arrows between any \(V_{i},V_{j}\in\mathcal{G}_{0}\) if there exists a correlated latent variable pair \((U_{k},U_{l}),U_{k}\in\mathbf{U}_{V_{i}},U_{l}\in\mathbf{U}_{V_{j}}\). We call the extended graph \(\mathcal{G}_{\mathcal{M}}\) the causal graph of \(\mathcal{M}\)._When we write \(\text{Pa}(V_{i})\), we refer to the parents of \(V_{i}\) in the \(\mathcal{G}_{0}\). To connect with the literature, the causal graph we define is a kind of Acyclic Directed Mixed Graph (ADMG), which is often used to represent SCMs with unobserved variables [49]. Note that we allow one latent variable to enter several nodes, which differs from the common definition. We use \(n_{U}\) and \(n_{V}\) to denote the number of latent variables and observable variables. Let \(\mathbf{T}\subset\mathbf{V}\) be a set of treatment variables. The goal is to estimate causal quantities under a given intervention \(\mathbf{T}=\mathbf{t}\). Formally, the structural equations of the intervened model are

\[T_{i}=t_{i},\quad\forall T_{i}\in\mathbf{T},\] \[V_{i}(t)=f_{i}\left(\text{Pa}(V_{i}),\mathbf{U}_{V_{i}}\right),\quad \forall V_{i}\notin\mathbf{T}.\]

We denote \(V_{i}(\mathbf{t})\) to be the value of \(V_{i}\) under the intervention \(\mathbf{T}=\mathbf{t}\) and \(P^{\mathcal{M}}(\mathbf{V}(\mathbf{t}))\) to be the distribution of \(\mathbf{V}(\mathbf{t})\) in \(\mathcal{M}\). The notion of a \(C^{2}\) component [51] is defined as follows.

**Definition 2** (\(C^{2}\)-Component).: _For a causal graph \(\mathcal{G}\), a subset \(C\subset\mathbf{V}\) is \(C^{2}\)-component if each pair \(V_{i},V_{j}\in C\) is connected by a bi-directed arrow in \(\mathcal{G}\) and \(C\) is maximal._

We provide a concrete example in Appendix A to explain all these notions. We will make the following standard assumption about the independence of latent variables. Note that since we allow latent variables to enter in multiple structural equations, this is more a notational convention and not an actual assumption. Also note that under this convention a bi-directed arrow essentially represents the existence of a common latent parental variable.

**Assumption 1**.: _All the latent variables in \(\mathbf{U}\) are independent._

To deal with categorical variables, we assume that latent variables that influence categorical variables contain two parts: the shared confounding that influences the propensity functions and the independent noise that generates categorical distributions.

**Assumption 2**.: _The set of latent variables consists of two parts \(\mathbf{U}=\{U_{1},\cdots,U_{n_{U}}\}\cup\{G_{V_{i}}:V_{i}\text{ is categorical}\}\). Precisely, if \(V_{i}\in\mathbf{V}\) is a categorical variable, the data generation process of \(V_{i}\) satisfies \(V_{i}=\arg\max_{k\in[n_{i}]}\left\{g_{k}^{V_{i}}+\log\left(f_{i}\left(\text{Pa }(V_{i}),\mathbf{U}_{V_{i}}\right)\right)_{k}\right\}\sim\text{Categorical}(f_{i }/\|f_{i}\|_{1})\), where \(G_{V_{i}}=(g_{1}^{V_{i}},\cdots,g_{n_{i}}^{V_{i}})\) are i.i.d. standard Gumbel variables, \(\mathbf{U}_{V_{i}}\subset\{U_{1},\cdots,U_{n_{U}}\}\)._

This convention is without loss of generality at this point, but will be useful when introducing Lischitz restrictions on the structural equation functions. The Gumbel variables in the assumption can be replaced by any random variables that can generate categorical variables. It can be proven that all discrete SCMs satisfy this assumption. Note that we implicitly assume that all categorical variables \(V_{i}\) are supported on \([n_{i}]\) for some \(n_{i}\). It is straightforward to generalize all results to any finite support. Next, we introduce Neural Causal Models (NCMs).

**Definition 3**.: _(Neural Causal Model) A Neural Causal Model (NCM) is a special kind of SCM where \(\mathbf{U}=\{U_{1},\cdots,U_{n_{U}}\}\cup\{G_{V_{i}}:V_{i}\text{ categorical}\}\), all \(U_{i}\) are i.i.d. multivariate uniform variables, \(G_{V_{i}}\) are i.i.d Gumbel variables and functions in (2) are Neural Networks (NNs)._

The definition of NCMs we use is slightly different from that in [52] because we need to deal with mixed variables in our models. In (1), we usually take \(\mathcal{C}\) to be the set of all SCMs. However, it is difficult to search over all SCMs since (1) becomes an infinite-dimensional polynomial programming problem. As an alternative, we can search over all NCMs. One quantity of common interest in causal inference is the Average Treatment Effect (ATE).

**Definition 4**.: _(Average Treatment Effect). For SCM \(\mathcal{M}\), the ATE at \(\mathbf{T}=\mathbf{t}\) with respect to \(\mathbf{T}=\mathbf{t}_{0}\) is given by \(\text{ATE}_{\mathcal{M}}(\mathbf{t})=\mathbb{E}_{\mathbf{u}\sim P(\mathbf{U})}[Y(\mathbf{t})-Y (\mathbf{t}_{0})]\)._

Partial identification can be formulated as estimating the solution to the optimization problems (1) [3]. The max and min values \(\overline{F}\) and \(\underline{F}\) define the interval \([\underline{F},\overline{F}]\) which is the smallest interval we can derive from the observed data without additional assumptions. In particular, if \(\overline{F}=\underline{F}\), then the causal quantity is point-identified.

**Notations.** We use \(\|\cdot\|,\|\cdot\|_{\infty}\) for the \(1\)-norm and \(\infty\)-norm and \([n]\) for \(\{1,\cdots,n\}\). Bold letters represent sets of random variables. We let \(\mathcal{F}_{L}(K_{1},K_{2})\) be the class of Lipschitz \(L\)-continuous functions \(f:K_{1}\to K_{2}\). We may omit the domain and use \(\mathcal{F}_{L}\) when the domain is clear from context. Let \(\mathcal{H}(K_{1},K_{2})\) be the set of homeomorphisms from \(K_{1}\) to \(K_{2}\), i.e., injective and continuous maps in both directions. We define \(\epsilon(\mathcal{F}_{1},\mathcal{F}_{2})=\sup_{f_{2}\in\mathcal{F}_{2}}\inf_{f_{1} \in\mathcal{F}_{1}}\|f_{2}-f_{1}\|\) for function classes \(\mathcal{F}_{1},\mathcal{F}_{2}\). We use standard asymptotic notation \(O(\cdot),\Omega(\cdot)\). Given a measure \(\mu\) on \(\mathbb{R}^{d_{1}}\) and a measurable function \(f:\mathbb{R}^{d_{1}}\rightarrow\mathbb{R}^{d_{2}}\), the push-forward measure \(f_{\#}\mu\) is defined as \(f_{\#}\mu(B)=\mu\left(f^{-1}(B)\right)\) for all measurable sets \(B\). We use \(P(X)\) to represent the distribution of random variable \(X\). Let \(\Delta_{n}=\{(p_{1},\cdots,p_{n}):\sum_{i=1}^{n}p_{i}=1,p_{i}\geq 0\}\) be the probability simplex. We use \(\text{Categorical}(\mathbf{p}),\mathbf{p}\in\Delta_{n}\) to represent categorical distribution with event probability \(\mathbf{p}\). We let \(W(\cdot,\cdot)\) be the Wasserstein-1 distance and \(S_{\lambda}(\mu,\nu)\) be the Sinkhorn distance [12] with regularization parameter \(\lambda>0\).

## 3 Approximation Error of Neural Causal Models

In this section, we study the expressive power of NCMs, which serves as a key ingredient in proving the consistency result. In particular, given an SCM \(\mathcal{M}^{*}\), we want to construct an NCM \(\hat{\mathcal{M}}\) such that the two causal models produce similar interventional results. Unlike in the discrete case [4, 51], latent distributions can be extremely complicated in general cases. The main challenge is how to design the structure of NCMs to ensure strong approximation power.

In the following, we first derive an upper bound on the Wasserstein distance between two causal models sharing the same causal graph. Using this result, we decompose the approximation error into two parts: the error caused by approximating structural functions via neural networks and the error of approximating the latent distributions. Then, we design different architectures for these two parts.

Decomposing the Approximation ErrorFirst, we present a canonical representation of an SCM, which essentially states that we only need to consider the case where each latent variable \(U_{i}\) corresponds to a \(C^{2}\) component of \(\mathcal{G}\).

**Definition 5** (Canonical representation).: _A SCM \(\mathcal{M}\) with causal graph \(\mathcal{G}\) has canonical form if_

1. _The set of latent variables consists of two sets,_ \[\mathbf{U}=\left\{U_{C}:C\text{ is a $C^{2}$-component of $\mathcal{G}$} \right\}\cup\left\{G_{V_{i}}=(g_{1}^{V_{i}},\cdots,g_{n_{i}}^{V_{i}}):V_{i} \text{ is categorical}\right\},\] _where_ \(U_{C}\) _and_ \(g_{j}^{V_{i}}\) _are independent and_ \(g_{j}^{V_{i}}\) _are standard Gumbel variables._
2. _The structure equations have the form_ \[V_{i}=\begin{cases}f_{i}\left(\text{Pa}(V_{i}),\mathbf{U}_{V_{i}}\right),&\text{ $V_{i}$ is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{g_{k}^{V_{i}}+\log\left(f_{i}\left(\text{Pa}(V_{ i}),\mathbf{U}_{V_{i}}\right)\right)_{k}\right\},&\text{ $V_{i}$ is categorical},\|f_{i}\|_{1}=1,\end{cases}\] (3) _where_ \(\mathbf{U}_{V_{i}}=\{U_{C}:V_{i}\in C,C\text{ is a $C^{2}$-component of $\mathcal{G}$}\}\) _and_ \((x)_{k}\) _is the_ \(k\)_-th coordinate of the vector_ \(x\)_. We further assume that_ \(f_{i}\) _are normalized for categorical variables._

_Given a function class \(\mathcal{F}\), the SCM class \(\mathcal{M}(\mathcal{G},\mathcal{F},\mathbf{U})\) consists of all canonical SCM models with causal graph \(\mathcal{G}\) such that \(f_{i}\in\mathcal{F},i\in[n_{V}]\)._

Proposition 4 in the appendix shows that any SCM satisfying Assumption 1,2 can be represented in this way and we provide an example in Appendix B.1 to illustrate how to obtain the canonical representation for a given SCM. Therefore, we restrict our attention to the class \(\mathcal{M}(\mathcal{G},\mathcal{F},\mathbf{U})\). For two SCM classes \(\mathcal{M}(\mathcal{G},\mathcal{F},\mathbf{U}),\mathcal{M}(\mathcal{G},\hat{ \mathcal{F}},\hat{\mathbf{U}})\), we want to study how well we can represent the models in the first class by the second class. The Wasserstein distance between the intervention distributions is used to measure the quality of the approximation. To approximate the functions in the structural equations, we need to make the following regularity assumptions on the functions.

**Assumption 3**.: _If \(V_{i}\) is continuous, \(f_{i}\) in (3) are \(L_{f}\)-Lipschitz continuous. If \(V_{i}\) is categorical, the propensity functions \(f_{i}(\text{Pa}(V_{i}),\mathbf{U}_{V_{i}})\triangleq\mathbb{P}(V_{i}=j|\text{Pa}(V _{i}),\mathbf{U}_{V_{i}}),j\in[n_{i}]\) are \(L_{f}\)-Lipschitz continuous. There exists a constant \(K>0\) such that \(\max_{i\in[n_{V}],j\in[n_{U}]}\{\|V_{i}\|_{\infty},\|U_{j}\|_{\infty}\}\leqslant K\)._

The following theorem summarizes our approximation error decomposition.

**Theorem 1**.: _Given any SCM model \(\mathcal{M}\in\mathcal{M}(\mathcal{G},\mathcal{F}_{L},\mathbf{U})\), let the treatment variable set be \(\mathbf{T}=\{T_{k}\}_{k=1}^{n_{k}}\) and suppose that Assumption 1, 2 and 3 hold for \(\mathcal{M}\) with Lipschitz constant \(L\), constant \(K\).__For any intervention \(\mathbf{T}=\mathbf{t}\) and \(\hat{\mathcal{M}}\in\mathcal{M}(\mathcal{G},\hat{\mathcal{F}},\hat{\mathbf{U}})\), we have_

\[W\left(P^{\mathcal{M}}(\mathbf{V}(\mathbf{t})),P^{\hat{\mathcal{M}}}(\hat{\mathbf{V}}(\mathbf{t} ))\right)\leqslant C_{\mathcal{G}}(L,K)\left(\sum_{i=1}^{n_{V}}\|f_{i}-\hat{f }_{i}\|_{\infty}+W\left(P^{\mathcal{M}}(\mathbf{U}),P^{\hat{\mathcal{M}}}(\hat{\bm {U}})\right)\right), \tag{4}\]

_where \(C_{\mathcal{G}}(L,K)\) is a constant that only depends on \(L,K\) and the causal graph \(\mathcal{G}\) and \(f_{i},\hat{f}_{i}\) are structural functions of \(\mathcal{M}\) and \(\hat{\mathcal{M}}\) respectively._

Theorem 1 separates the approximation error into two parts, which motivates us to construct the NCM in the following way. First, we approximate the functions \(f_{i}\) in (3) by NNs \(\hat{f}_{i}\). Then, we approximate the distribution of latent variables by pushing forward uniform and Gumbel variables using neural networks, i.e., \(\hat{U}_{C_{j}}=\hat{g}_{j}(Z_{C_{j}})\), where \(\{C_{j}\}\) are \(C^{2}\) components and \(Z_{C_{j}}\) are multi-variate uniform and Gumbel random variables. The structural equations of the resulting approximated model \(\hat{\mathcal{M}}\) are

\[\hat{Y}_{i}=\begin{cases}\hat{f}_{i}\left(\text{Pa}(\hat{V}_{i}),(\hat{g}_{j}( Z_{C_{j}}))_{U_{C_{j}}\in U_{V_{i}}}\right),&\text{$V_{i}$ is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{g_{k}+\log\left(\hat{f}_{i}\left(\text{Pa}(\hat{V} _{i}),(\hat{g}_{j}(Z_{C_{j}}))_{U_{C_{j}}\in U_{V_{i}}}\right)\right)_{k} \right\},&\text{$V_{i}$ is categorical},\end{cases} \tag{5}\]

where \(N_{C,j}\) are constants to be specified later.

For the first part, we need to approximate Lipschitz continuous functions. For simplicity, we assume that the domain of the functions are uniform cubes. Similar arguments hold for any bounded cubes. We denote \(\mathcal{NN}_{k_{1},k_{2}}(W,L)\) to be the set of ReLu NNs with input dimension \(k_{1}\), output dimension \(k_{2}\), width \(W\) and depth \(L\). It has been shown that \(\epsilon(\mathcal{NN}_{k_{1},1}(2d_{1}+10,L_{0}),\mathcal{F}_{L}([0,1]^{d_{1} },\mathbb{R}))\leqslant O(L_{0}^{-2/k_{1}})\)[53], where \(\epsilon(\cdot,\cdot)\) denotes the approximation error defined in Section 2. For a vector valued function, we can use a wider NN to approximate each coordinate and get a similar rate.

For the second part, we approximate each \(U_{i}\) individually by pushing forward i.i.d. multivariate uniform and Gumbel variables \(\hat{U}_{C_{i}}=\hat{g}_{i}(Z_{C_{i}})\) since the latent variables are independent by Assumption 1. To do so, we examine under what assumptions on the measure \(\mathbb{P}\) over \(\mathbb{R}^{n}\) we can find a NN \(\hat{g}\) such that \(W(\hat{g}_{\#}\lambda,\mathbb{P})\) is small, where \(\lambda(\cdot)\) is some reference measure.

### Approximating Mixed Distributions by Wide Neural Networks

In this subsection, we will extend the results in [43] to construct a wide neural network as the push-forward map. It turns out that to get a good approximation of the targeted distribution, the shape of the support is essential.

**Assumption 4** (Mixed Distribution).: _The support of measure \(\mathbb{P}\) has finite connected components \(C_{1},C_{2}\cdots C_{N_{C}}\), i.e., \(\text{supp}(\mathbb{P})=\bigcup_{i=1}^{N_{C}}C_{i}\), and each component \(C_{i}\) satisfies \(\mathcal{H}([0,1]^{d_{i}^{C}},C_{i})\cap\mathcal{F}_{L}([0,1]^{d_{i}^{C}},C_{i} )\neq\emptyset\) for some \(d_{i}^{C}\geqslant 0\). Recall that \(\mathcal{H}(K_{1},K_{2})\) is the set of homeomorphisms from \(K_{1}\) to \(K_{2}\) defined at the end of Section 2._

Assumption 4 encompasses almost all natural distributions. For example, distributions supported on \([0,1]^{d}\) and closed balls, finitely supported distributions and mixtures of them all satisfy this assumption. Assumption 4 allows us to transform the support of the targeted distribution into unit cubes and the nice geometric properties of unit cubes facilitate our construction.

Now, we briefly explain the construction of the push-forward maps. An example is provided in Appendix B.1 to illustrate the construction. The NN architecture consists of three parts (see Figure 0(a)). The input dimension is the same as the number of connected components of the support \(N_{C}\). For each component \(C_{i}\), let \(H_{i}\in\mathcal{H}([0,1]^{d_{i}^{C}},C_{i})\cap\mathcal{F}_{L}([0,1]^{d_{i}^ {C}},C_{i})\), where \(d_{i}^{C}\) is the dimension of component \(C_{i}\) in Assumption 4. By \(\mathbb{P}=(H_{i})_{\#}(H_{i}^{-1})_{\#}\mathbb{P}\) on \(C_{i}\), we can approximate \((H_{i}^{-1})_{\#}\mathbb{P}\) first, which is supported on a unit cube. [43] constructs a wide NN \(\hat{g}\) of width \(W\) and constant depth such that \(W(\hat{g}_{\#}\lambda,(H_{i})_{\#}^{-1}\mathbb{P})\leqslant O(W^{-1/d_{i}^{C}})\) where \(\lambda\) is the uniform measure on \([0,1]\). Then, we approximate the Lipschitz map \(H_{i}\) to \(C_{i}\) to pull the distribution back to \(C_{i}\). These are the first two parts (yellow and blue blocks in Figure 0(a)) of the architecture.

Suppose that the output of \(i\)-th coordinate in the first two parts is \(\vec{v}_{i}\), the Gumbel-softmax layer in the third part (green box) combines different components of the support. In particular, we want to output \(\vec{v}_{i}\) with probability \(p_{i}=\mathbb{P}(C_{i})\). Let \(V=[\vec{v}_{1},\cdots,\vec{v}_{i}]\), this can be achieved by outputting \(VX\), where \(X=(X_{1},\cdots,X_{N_{C}})^{T}\) is a one-hot random vector with \(\mathbb{P}(X_{i}=1)=p_{i}\). To use backpropagation in training, we use the Gumbel-Softmax trick [31] to (approximately) simulate such a random vector, \(\hat{X}_{i}^{\tau}=\frac{\exp(\log p_{i}+G_{i})/\tau}{\sum_{k=1}^{N_{C}}\exp(( \log p_{k}+G_{k})/\tau)}\), where \(\tau>0\) is the temperature parameter (a hyperparameter) and \(G_{i}\sim\text{Gumbel}(0,1)\) are i.i.d. standard Gumbel variables. As \(\tau\to 0\), the distribution of \(\hat{X}^{\tau}\) converges almost surely to the categorical distribution [36, Proposition 1]. In particular, when \(\tau=0\), we denote \(\hat{X}_{i}^{0}=\mathbb{I}_{i=\arg\max_{j}\{\log p_{i}+G_{i}\}}\). The output of the last layer is \(V\hat{X}^{\tau}\). Note that the Gumbel-softmax function is differentiable with respect to parameter \(\{\log(p_{i})\}_{i=1,\cdots,N_{C}}\). Therefore, we can train the network with common gradient-based algorithms. Putting things together, we obtain the following theorem.

**Theorem 2**.: _Given any probability measure \(\mathbb{P}\) on \(\mathbb{R}^{d}\) that satisfies Assumption 4, let \(\lambda\) be the Lebesgue measure on \([0,1]^{N_{C}}\), where \(N_{C}\) is defined in Assumption 4. There exists a neural network \(\hat{g}=\tilde{g}_{3}^{\tau}\circ\hat{g}_{2}\circ\hat{g}_{1}\) with the above antichesture (Figure 0(a)) such that_

\[W(\hat{g}_{\#}\lambda,\mathbb{P})\leqslant O\left(W_{1}^{-1/\max_{i}\{d_{i}^ {C}\}}+L_{2}^{-2/\max_{i}\{d_{i}^{C}\}}+(\tau-\tau\log\tau)\right).\]

_Here, \(\hat{g}_{i},i=1,2\) has the separable form \(\left(\hat{g}_{i}^{1}(x_{1}),\cdots,\hat{g}_{i}^{N_{C}}(x_{N_{C}})\right)\) and \(\hat{g}_{1}^{j}\in\mathcal{NN}_{1,d_{j}^{C}}(W_{1},\Theta(d_{j}^{C}))\), \(\hat{g}_{2}^{j}\in\mathcal{NN}_{d_{j}^{C},d}(\Theta(d\cdot d_{j}^{C}),L_{2}), \ j\in[N_{C}]\). \(\{d_{j}^{C}\}\) are the dimension of cubes in Assumption 4. \(\tilde{g}_{3}^{\tau}\) is the Gumbel-Softmax layer with temperature parameter \(\tau>0\)._

Note that \(\hat{g}_{3}^{\tau}\) (the Gumbel-softmax layer) is actually a random function since the coefficient vector \(\hat{X}^{\tau}\) is a random variable. In this sense, \(\hat{g}_{\#}\lambda\) can be viewed as pushing forward uniform and Gumbel variables using a neural net.

### Approximating Mixed Distributions by Deep Neural Networks

In this subsection, we will show that under one additional assumption on the distribution, deep ReLu networks have a stronger approximation power in approximating distributions, which means we can use fewer computational units to achieve the same worst-case theoretical approximation error.

**Assumption 5** (Lower Bound).: _Suppose that \(\mathbb{P}\) is supported on a compact set \(K\subset\mathbb{R}^{D}\), there exists a constant \(C_{f}>0\), \(f\in\mathcal{H}([0,1]^{d},K)\cap\mathcal{F}_{L}([0,1]^{d},K)\), such that for any measurable set \(B\subset[0,1]^{d}\), \(\mathbb{P}\left(f(B)\right)\geqslant C_{f}\lambda(B)\). Besides, if \(d>0\), \(f_{\#}\mathbb{P}\) vanishes on the boundary \(\partial[0,1]^{d}\)._

Assumption 5 implies that \(d\lambda/d(f_{\#}^{-1}\mathbb{P})\) exists and is lowered bounded by a constant \(C_{f}\). The next proposition extends the Skorohod representation theorem [48]. It shows that under Assumption 5, itis possible to simulate any distribution on the unit cubes with Holder continuous curves and uniform distribution on \([0,1]\).

**Proposition 1**.: _Let \(\lambda\) be the Lebesgue measure on \([0,1]\). Given any probability measure \(\mathbb{P}\) that satisfies Assumption 5, there exists a continuous curve \(\gamma:[0,1]\rightarrow\text{supp}(\mathbb{P})\) such that \(\gamma_{\#}\lambda=\mathbb{P}\). Furthermore, if \(d\geqslant 1\), \(\gamma\) is \(1/d\)-Holder continuous._

Results from [53] show that we can approximate any Holder continuous \(d\)-dimensional function with index \(\alpha\) by a deep ReLu network with depth \(L\) and error \(O(L^{-2\alpha/d})\). Leveraging this result, we can replace the first part of the architecture in the previous subsection with deep ReLu networks (See Figure 8 in the appendix). The remaining two parts are the same as the construction in Figure 0(a). The following theorem gives a sharper bound on the approximation error compared with Theorem 2.

**Theorem 3**.: _Under the Assumption 4, if in addition, \(\mathbb{P}\) constrained to each component satisfies Assumption 5, there exists a neural network \(\hat{g}=\hat{g}_{3}^{\tau}\circ\hat{g}_{2}\circ\hat{g}_{1}\) with the above architecture such that_

\[W(\hat{g}_{\#}\lambda,\mathbb{P})\leqslant O\left(L_{1}^{-2/\max_{i}\{d_{i}^ {C}\}}+L_{2}^{-2/\max_{i}\{d_{i}^{C}\}}+(\tau-\tau\log\tau)\right),\]

_where \(d_{i}^{C}\) are the dimensions of the connected components in Assumption 4, \(\hat{g}_{i},i=1,2\) has the form \(\left(\hat{g}_{i}^{1}(x_{1}),\cdots,\hat{g}_{i}^{N_{C}}(x_{N_{C}})\right)\) and \(\hat{g}_{1}^{j}\in\mathcal{NN}_{1,d_{j}^{C}}(\Theta(d_{j}^{C}),L_{1})\). \(\hat{g}_{2}^{j},\hat{g}_{3}^{\tau},\tau\) are the same as in Theorem 2._

Let \(N\) be the number of nonzero weights in a neural network, Theorem 3 shows that a deep NN can achieve \(O(N^{-2/d})\) error while by Theorem 2 a wide network can only achieve \(O(N^{-1/d})\) error. Now, we can put things together to construct NCM approximations. For simplicity, we omit the input and output dimensions of the neural network. As we mention in previous sections, our construction (5) consists of two parts, \(\hat{f}_{i}\) approximating the structural functions \(f_{i}\) in (3), and \(\hat{g}_{j}(Z_{j})=\hat{g}_{3,j}^{\tau}\circ\hat{g}_{2,j}\circ\hat{g}_{1,j}(Z_ {j})\) approximating the latent variables \(U_{j}\). Let \(\text{NCM}_{\mathcal{G}}(\mathcal{F}_{0},\mathcal{F}_{1},\mathcal{F}_{2},\tau)\) be a collection of NCMs with structural equation (5) and

\[\hat{f}_{i}\in\mathcal{F}_{0},\hat{g}_{1,j}=(\hat{g}_{1,j}^{1},\cdots,\hat{g}_ {1,j}^{N_{C,j}}),\hat{g}_{1,j}^{i}\in\mathcal{F}_{1},\hat{g}_{2,j}=(\hat{g}_{ 2,j}^{1},\cdots,\hat{g}_{2,j}^{N_{C,j}}),\hat{g}_{2,j}^{i}\in\mathcal{F}_{2},\]

where \(N_{C,j}\) is the number of connected components for \(U_{j}\) and \(\mathcal{F}_{i}\) are function classes.

**Corollary 1**.: _Given a causal model \(\mathcal{M}^{*}\in\mathcal{M}(\mathcal{G},\mathcal{F}_{L},\mathbf{U})\), suppose that Assumptions 1-3 hold and the distributions of \(U_{C}\) for all \(C^{2}\)-component satisfy the assumptions of Theorem 3. Let \(d_{\max}^{\text{in}}\) and \(d_{\max}^{\text{out}}\) be the largest input and output dimension of \(f_{i}\) in (3) and \(d_{\max}^{U}\) be the largest dimension of all latent variables. There exists a neural causal model_

\[\hat{\mathcal{M}}\in\text{NCM}_{\mathcal{G}}(\mathcal{NN}(\Theta(d_{\max}^{ \text{in}}d_{\max}^{\text{out}}),L_{0}),\mathcal{NN}\left(\Theta\left(d_{\max} ^{U}\right),L_{1}\right),\mathcal{NN}\left(\Theta\left((d_{\max}^{U})^{2} \right),L_{2}\right),\tau)\]

_with structure equations (5). For any intervention \(\mathbf{T}=\mathbf{t}\), \(\hat{\mathcal{M}}\) satisfies_

\[W\left(P^{\mathcal{M}^{*}}(\mathbf{V}(\mathbf{t})),P^{\hat{\mathcal{M}}}(\mathbf{V}(\mathbf{t }))\right)\leqslant O(L_{0}^{-2/d_{\max}^{\text{in}}}+L_{1}^{-2/d_{\max}^{U}} +L_{2}^{-2/d_{\max}^{U}}+(\tau-\tau\log\tau)).\]

Similar approximation results also hold for wide NN approximation, as presented in Section 3.1. The proof can be easily adapted to the wide NNs architecture.

## 4 Consistency of Neural Causal Partial Identification

In this section, we prove the consistency of the max/min optimization approach to partial identification. In the finite sample setting, we consider the following estimator \(F_{n}\) of the optimal values of (1).

\[F_{n}=\operatorname*{arg\,min}_{\hat{\mathcal{M}}\in\text{NCM}_{ \mathcal{G}}(\mathcal{F}_{0,n},\mathcal{F}_{1,n},\mathcal{F}_{2,n},\tau_{n})} \mathbb{E}_{t\sim\mu_{T}}\mathbb{E}_{\hat{\mathcal{M}}}[F(V_{1}(t),\cdots,V_{ n_{V}}(t))], \tag{6}\] \[s.t. S_{\lambda_{n}}(P_{m_{n}}^{\hat{\mathcal{M}}}(\mathbf{V}),P_{n}^{\hat{ \mathcal{M}}^{*}}(\mathbf{V}))\leqslant\alpha_{n},\]

where \(P_{n}^{\hat{\mathcal{M}}^{*}},P_{m_{n}}^{\hat{\mathcal{M}}}\) are the empirical distribution of \(P^{\hat{\mathcal{M}}^{*}}\), \(P^{\hat{\mathcal{M}}}\) with sample size \(n,m_{n}\), \(\mu_{T}\) is some given measure and \(\mathcal{F}_{i,n}\) will be specified later. For example, the counterfactual outcome \(\mathbb{E}[Y(1)]\) is a special case of the objective. Our results can be easily generalized to any linear combination of objective functions of this form. We use the Sinkhorn distance because it can be computed efficiently in practice [15]. We want to study if \(F_{n}\) gives a useful lower bound as \(n\rightarrow\infty\).

[MISSING_PAGE_FAIL:9]

**Proposition 3** (Holder continuity of ATE).: _Given two causal models \(\mathcal{M},\hat{\mathcal{M}}\in\mathcal{M}(\mathcal{G},\mathcal{F}_{L},\mathbf{U})\) satisfying Assumption 1 and Assumption 3, let their observational distributions be \(\nu,\mu\). Suppose the norms of all variables are bounded by \(K>0\). If(1) (Overlap) \(\nu\) is absolutely continuous with respect to one probability measure \(P\) and the density \(p_{\nu}\)\((t|\text{Pa}(T)=x)\geqslant\delta>0\) for x almost surely and \(t\in[t_{1},t_{2}]\) and (2) (No Confounding) there is no confounding in the causal graph \(\mathcal{G}\), we have_

\[\int_{t_{1}}^{t_{2}}(\mathbb{E}_{\mathcal{M}}[Y(t)]-\mathbb{E}_{\hat{\mathcal{ M}}}[\hat{Y}(t)])^{2}P(dt)\leqslant\frac{2C_{W}}{\delta}W(\mu,\nu)+2(L+1)^{n_{V }}W^{2}(\mu,\nu)(t_{2}-t_{1}),\]

_where \(C_{W}=4(L+1)^{n_{V}}K+2K\max\left\{(L+1)^{n_{V}},1\right\}\)._

**Corollary 2**.: _Let \(F,\mu_{T}\) in Theorem 4 to be \(F(\mathbf{V})=Y,\mu_{T}\sim\text{Unif}([t_{1},t_{2}]),\epsilon>0\). Suppose that the assumptions in Proposition 3 and Theorem 4 hold, with probability at least \(1-O(n^{-2})\), we have \(|F_{n}-F_{*}|\leqslant O(\sqrt{\alpha_{n}})\), where \(F_{n},F_{*},\alpha_{n}\) are defined in Theorem 4._

### Experiments

In this section, we examine the performance of our algorithm in two settings. We compare our algorithm with the Autobounds algorithm [14] in a binary IV example in [14] and in a continuous IV model 1. Since Autobounds can only deal with discrete models, we discretize the continuous model for comparison purposes. The implementation details are provided in the Appendix D. The setting of the first experiment is taken from [14, Section D.1]. This is a binary IV problem and we can calculate the optimal bound using LP [4]. We find that our bound is close to the optimal bound. The second experiment is a general IV example where the treatment is binary but the rest of the variables are continuous. The program that Autobounds solves after discretization contains \(\approx 2^{14}\) variables. Even with such a fine discretization, the bound obtained by Autobounds is not tighter than our NCM approach. The details of the structural equations and analysis can be found in Appendix D. We also provide an extra experiment on the counterexample of Proposition 5 in the appendix to show the effect of Lipschitz regularization.

Footnote 1: The code can be found in [https://github.com/Jiyuan-Tan/NeuralPartialIID](https://github.com/Jiyuan-Tan/NeuralPartialIID)

ConclusionIn this paper, we provide theoretical justification for using NCMs for partial identification. We show that NCMs can be used to represent SCMs with complex unknown latent distributions under mild assumptions and prove the asymptotic consistency of the max/min estimator for partial identification of causal effects in general settings with both discrete and continuous variables. Our results also provide guidelines on the practical implementation of this method and on what hyperparameters are important, as well as recommendations on values that these hyperparameters should take for the consistency of the method. These practical guidelines were validated with a small set of targeted experiments, which also showcase superior performance of the neural-causal approach as compared to a prior main contender approach from econometrics and statistics, that involves discretization and polynomial programming.

An obvious next step in the theoretical foundation of neural-causal models is providing finite sample guarantees for this method, which requires substantial further theoretical developments in the understanding of the geometry of the optimization program that defines the bounds on the causal effect of interest. We take a first step in that direction for the special case, when there are no unobserved confounders and view the general case as an exciting avenue for future work.

AcknowledgementVasilis Syrgkanis is supported by NSF Award IIS-2337916 and a 2022 Amazon Research Award.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Setting & Algorithm & Average Bound & Optimal Bound & True Value \\ \hline \multirow{2}{*}{Binary IV} & NCM (Ours) & [-0.49,0.05] & [-0.45, -0.04] & -0.31 \\ \cline{2-5}  & Autobounds & [-0.45,-0.05] & [-0.45, -0.04] & \\ \hline \multirow{2}{*}{General IV} & NCM (Ours) & [2.49,3.24] & – & \multirow{2}{*}{3} \\ \cline{2-5}  & Autobounds & [1.40, 3.48] & – & \\ \hline \end{tabular}
\end{table}
Table 1: Experiment results of 2 IV settings. The sample sizes are taken to be \(5000\) in each experiment. STD is the standard derivation. The experiments are repeated 10 times for binary IV and 50 times for continuous IV. In all experiments, the bounds given by both algorithms all cover the true values.

## References

* [1] Joshua Angrist and Guido Imbens. Identification and estimation of local average treatment effects, 1995.
* [2] Martin Anthony, Peter L Bartlett, Peter L Bartlett, et al. _Neural network learning: Theoretical foundations_, volume 9. cambridge university press Cambridge, 1999.
* [3] Vahid Balazadeh, Vasilis Syrgkanis, and Rahul G. Krishnan. Partial identification of treatment effects with implicit generative models, October 2022.
* [4] Alexander Balke and Judea Pearl. Counterfactual probabilities: Computational methods, bounds and applications. In _Uncertainty Proceedings 1994_, pages 46-54. Elsevier, 1994.
* [5] Alexander Balke and Judea Pearl. Bounds on treatment effects from studies with imperfect compliance. _Journal of the American Statistical Association_, 92(439):1171-1176, September 1997.
* [6] Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. _Journal of Machine Learning Research_, 20(63):1-17, 2019.
* [7] Alexis Bellot. Towards bounding causal effects under markov equivalence. _arXiv preprint arXiv:2311.07259_, 2023.
* [8] Blai Bonet. Instrumentality tests revisited, January 2013.
* [9] Leon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. _Journal of Machine Learning Research_, 14(11), 2013.
* [10] Leon Bungert, Rene Raab, Tim Roith, Leo Schwinn, and Daniel Tenbrinck. CLIP: Cheap Lipschitz Training of Neural Networks. In Abderrahim Elmoataz, Jalal Fadili, Yvain Queau, Julien Rabin, and Loic Simon, editors, _Scale Space and Variational Methods in Computer Vision_, volume 12679, pages 307-319, Cham, 2021. Springer International Publishing.
* [11] Victor Chernozhukov, Han Hong, and Elie Tamer. Estimation and confidence regions for parameter sets in econometric models 1. _Econometrica_, 75(5):1243-1284, 2007.
* [12] Lenaic Chizat, Pierre Roussillon, Flavien Leger, Francois-Xavier Vialard, and Gabriel Peyre. Faster wasserstein distance estimation with the sinkhorn divergence. _Advances in Neural Information Processing Systems_, 33:2257-2269, 2020.
* [13] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In _International conference on machine learning_, pages 854-863. PMLR, 2017.
* [14] Guilherme Duarte, Noam Finkelstein, Dean Knox, Jonathan Mummolo, and Ilya Shpitser. An automated approach to causal inference in discrete settings, September 2021.
* [15] Jean Feydy. Geometric data analysis, beyond convolutions. _Applied Mathematics_, 2020.
* [16] Jean Feydy, Thibault Sejourne, Francois-Xavier Vialard, Shun-ichi Amari, Alain Trouve, and Gabriel Peyre. Interpolating between optimal transport and mmd using sinkhorn divergences. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 2681-2690, 2019.
* [17] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the empirical measure. _Probability theory and related fields_, 162(3-4):707-738, 2015.
* [18] Dennis Frauen, Tobias Hatt, Valentyn Melnychuk, and Stefan Feuerriegel. Estimating average causal effects from patient trajectories. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 7586-7594, 2023.

* [19] Dennis Frauen, Fergus Imrie, Alicia Curth, Valentyn Melnychuk, Stefan Feuerriegel, and Mihaela van der Schaar. A Neural Framework for Generalized Causal Sensitivity Analysis, April 2024.
* [20] David A Freedman. _Statistical models and causal inference: a dialogue with the social sciences_. Cambridge University Press, 2010.
* [21] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient regression in metric spaces via approximate lipschitz extension. _IEEE Transactions on Information Theory_, 63(8):4838-4849, 2017.
* [22] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisation of neural networks by enforcing Lipschitz continuity. _Machine Learning_, 110(2):393-416, February 2021.
* [23] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. _Machine Learning_, 110:393-416, 2021.
* [24] Florian Gunsilius. A path-sampling method to partially identify causal effects in instrumental variable models, June 2020.
* [25] Chris H. Hamilton and Andrew Rau-Chaplin. Compact hilbert indices for multi-dimensional data. In _First International Conference on Complex, Intelligent and Software Intensive Systems (CISIS'07)_, pages 139-146. IEEE.
* [26] Sukjin Han and Shenshen Yang. A computational approach to identification of treatment effects for policy evaluation. _Journal of Econometrics_, 240(1):105680, 2024.
* [27] James J. Heckman and Edward J. Vytlacil. Instrumental variables, selection models, and tight bounds on the average treatment effect. In Wolfgang Franz, Michael Lechner, and Friedhelm Pfeiffer, editors, _Econometric Evaluation of Labour Market Policies_, volume 13, pages 1-15. Physica-Verlag HD, Heidelberg, 2001.
* [28] David Hilbert. Uber die stetige abbildung einer linie auf ein flachenstuck. _Dritter Band: Analysis- Grundlagen der Mathematik- Physik Verschiedenes: Nebst Einer Lebensgeschichte_, pages 1-2, 1935.
* [29] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. A generative adversarial framework for bounding confounded causal effects. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(13):12104-12112, May 2021.
* [30] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. A generative adversarial framework for bounding confounded causal effects. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 12104-12112, 2021.
* [31] Eric Jang, S. Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. _ArXiv_, abs/1611.01144, 2016.
* [32] Niki Kilbertus, Matt J. Kusner, and Ricardo Silva. A class of algorithms for general instrumental variable models, October 2020.
* [33] Murat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. Causalgan: Learning causal implicit generative models with adversarial training. _arXiv preprint arXiv:1709.02023_, 2017.
* [34] Mohammad Lotfollahi, Anna Klimovskaia Susmelj, Carlo De Donno, Yuge Ji, Ignacio L Ibarra, F Alexander Wolf, Nafissa Yakubova, Fabian J Theis, and David Lopez-Paz. Compositional perturbation autoencoder for single-cell response modeling. _BioRxiv_, 2021.
* [35] Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto. Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance, May 2018.
* [36] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables, March 2017.

* [37] Charles F Manski. Nonparametric bounds on treatment effects. _The American Economic Review_, 80(2):319-323, 1990.
* [38] Chengzhi Mao, Augustine Cha, Amogh Gupta, Hao Wang, Junfeng Yang, and Carl Vondrick. Generative interventions for causal learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3947-3956, 2021.
* [39] Valentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. _Advances in Neural Information Processing Systems_, 36:32020-32060, December 2023.
* [40] Caleb H. Miles, Phyllis Kanki, Seema Meloni, and Eric J. Tchetgen Tchetgen. On Partial Identification of the Pure Direct Effect, September 2015.
* [41] Kirtan Padh, Jakob Zeitler, David Watson, Matt Kusner, Ricardo Silva, and Niki Kilbertus. Stochastic causal programming for bounding treatment effects. In _Conference on Causal Learning and Reasoning_, pages 142-176. PMLR, 2023.
* [42] Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgower. Training robust neural networks using Lipschitz bounds. _IEEE Control Systems Letters_, 6:121-126, 2021.
* [43] Dmytro Perekrestenko, Leandre Eberhard, and Helmut Bolcskei. High-Dimensional Distribution Generation Through Deep Neural Networks, August 2022.
* [44] Roland R. Ramsahai. Causal bounds and observable constraints for non-deterministic models. _Journal of Machine Learning Research_, 13(29):829-848, 2012.
* [45] Amy Richardson, Michael G. Hudgens, Peter B. Gilbert, and Jason P. Fine. Nonparametric Bounds and Sensitivity Analysis of Treatment Effects. _Statistical Science_, 29(4), November 2014.
* [46] Michael C. Sachs, Gustav Jonzon, Arvid Sjolander, and Erin E. Gabriel. A general method for deriving tight symbolic bounds on causal effects, December 2021.
* [47] E. Shchepin. On holder maps of cubes. _Mathematical Notes_, 87:757-767, 2010.
* [48] Anatoly V Skorokhod. Limit theorems for stochastic processes. _Theory of Probability & Its Applications_, 1(3):261-290, 1956.
* [49] Jin Tian and Judea Pearl. On the testable implications of causal models with hidden variables, December 2002.
* [50] Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: Analysis and efficient estimation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [51] Kevin Xia, Kai-Zhan Lee, Yoshua Bengio, and Elias Bareinboim. The causal-neural connection: Expressiveness, learnability, and inference, October 2022.
* [52] Kevin Xia, Yushu Pan, and Elias Bareinboim. Neural causal models for counterfactual identification and estimation. _arXiv preprint arXiv:2210.00035_, 2022.
* [53] D. Yarotsky. Optimal approximation of continuous functions by very deep relu networks. _ArXiv_, abs/1802.03620, 2018.
* [54] Junzhe Zhang and Elias Bareinboim. Bounding causal effects on continuous outcome. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(13):12207-12215, May 2021.
* [55] Junzhe Zhang and Elias Bareinboim. Non-parametric methods for partial identification of causal effects. _Columbia CausalAI Laboratory Technical Report_, 2021.
* [56] Junzhe Zhang, Jin Tian, and Elias Bareinboim. Partial counterfactual identification from observational and experimental data, October 2021.

## Appendix A Illustration of Notions in Section 2

To further explain the notions in Section 2, we consider the following example. Let \(\mathcal{M}\) be an SCM with the following structure equations.

\[\begin{split} V_{1}&=f_{1}(V_{2},U_{1}),\\ V_{2}&=f_{2}(U_{1},U_{2}),\\ V_{3}&=f_{3}(V_{1},V_{2},U_{2},U_{4}),\\ V_{4}&=f_{4}(V_{3}.U_{3},U_{4}),\\ V_{5}&=f_{5}(V_{4},U_{3}),\end{split} \tag{8}\]

where \(U_{1}\) and \(U_{2}\) are correlated and \(U_{3},U_{4}\) and \((U_{1},U_{2})\) are independent. The causal model is shown in Figure 2 and its causal graph is shown in Figure 3. In this example, \(\mathbf{U}_{V_{1}}=\{U_{1}\},\mathbf{U}_{V_{2}}=\{U_{1},U_{2}\},\mathbf{U}_{V_{3}}=\{U_{2 },U_{4}\},\mathbf{U}_{V_{4}}=\{U_{3},U_{4}\},\mathbf{U}_{V_{5}}=\{U_{3}\}\). Since \(U_{1},U_{2}\) are correlated, \(C_{1}=\{V_{1},V_{2},V_{3}\}\) is one \(C^{2}\) component because all nodes in \(C_{1}\) are connected by bi-directed edges. Note that \(\{V_{1},V_{2},V_{3},V_{4}\}\) is not a \(C^{2}\) component because \(V_{4}\) and \(V_{2}\) is not connected by any bi-directive edge. The rest of \(C^{2}\) components are \(C_{2}=\{V_{4},V_{5}\},C_{3}=\{V_{3},V_{4}\}\).

Now, we consider the intervention \(V_{1}=t\). Under this intervention, the structure equations can be obtained by setting \(V_{1}=t\) while keeping all other equations unchanged, i.e.,

\[\begin{split} V_{1}(t)&=t,\\ V_{2}(t)&=f_{2}(U_{1},U_{2}),\\ V_{3}&=f_{3}(V_{1},V_{2},U_{2},U_{4}),\\ V_{4}&=f_{4}(V_{3}.U_{3},U_{4}),\\ V_{5}(t)&=f_{5}(V_{4},U_{3}).\end{split}\]

The figure of this model under intervention is shown in Figure 4.

Figure 4: The SCM after intervention \(V_{1}=t\).

Figure 3: The causal graph of this SCM.

Proof of approximation Theorems

### Illustration of how to construct canonical representations and neural architectures

In this section, we illustrate how to construct canonical representations and neural architectures given a causal graph via a simple example. We consider the example in the previous section. The causal graph is shown in Figure 3.

Following the construction in Proposition 4, we use one latent variable for each \(C^{2}\) component. As we explain in Appendix A, this causal model has three \(C^{2}\) components, \(\{V_{1},V_{2},V_{3}\},\{V_{3},V_{4}\},\{V_{4},V_{5}\}\). In the canonical representation, exactly the latent variables enter their corresponding \(C^{2}\) component. The structure equation of this SCM is as follows.

\[\begin{split} V_{1}&=f_{1}(V_{2},E_{1}),\\ V_{2}&=f_{2}(E_{1}),\\ V_{3}&=f_{3}(V_{1},V_{2},E_{1},E_{2}),\\ V_{4}&=f_{4}(V_{3}.E_{2}),\\ V_{5}&=f_{5}(V_{4},E_{3}),\end{split} \tag{9}\]

If we set \(E_{1}=(U_{1},U_{2}),E_{2}=U_{4},E_{3}=U_{3}\), (10) is equivalent to (8). Therefore, we can see from this example that the canonical representation does not lose any information about the SCM.

Now, we show how to construct the NCM architecture from a canonical representation. As we mentioned in Section 3, we approximate the latent distribution by pushing forward uniform and Gumbel variables. The structure equation of the NCM is

\[\begin{split} V_{1}&=f_{1}^{\theta_{1}}(V_{2},g_{1 }^{\theta_{1}}(Z_{1})),\\ V_{2}&=f_{2}^{\theta_{2}}(g_{1}^{\theta_{1}}(Z_{1}) ),\\ V_{3}&=f_{3}^{\theta_{2}}(V_{1},V_{2},g_{1}^{\theta_{ 1}}(Z_{1}),g_{2}^{\theta_{1}}(Z_{2})),\\ V_{4}&=f_{4}^{\theta_{4}}(V_{3}.g_{2}^{\theta_{1}}( Z_{2})),\\ V_{5}&=f_{5}^{\theta_{5}}(V_{4},g_{2}^{\theta_{3}}( Z_{3})),\end{split} \tag{10}\]

where \(f_{i}^{\theta_{i}},g_{j}^{\theta_{j}}\) are neural networks, \(Z_{i}\) are join distribution of independent uniform and Gumbel variables, and \(g_{j}^{\theta_{j}}\) has the special architecture described in Section 3.1 and Section 3.2. Figure 6 shows the architecture of the NCM. Each in-edge represents an input of a neural net.

### Proof of Theorem 1

The causal graph \(\mathcal{G}\) does not specify how latent variables influence the observed variables. There could be many ways of recovering the latent variables from a graph \(\mathcal{G}\). Figure 7 shows an example where two different causal models have the same causal graph. The next proposition gives a canonical representation of a causal model.

**Proposition 4**.: _Suppose that Assumption 1 and Assumption 2 hold, given any SCM \(\mathcal{M}\) with causal graph \(\mathcal{G}\) and latent variables \(\mathbf{U}\), we can construct a canonical SCM \(\hat{\mathcal{M}}\) of the form (3) by merging the latent variables in \(\mathbf{U}\) such that \(\mathcal{M}\) and \(\hat{\mathcal{M}}\) produce the same intervention results. Besides, functions in \(\hat{\mathcal{M}}\) have the same smoothness as \(\mathcal{M}\)._

Proof of Proposition 4.: Let \(\mathbf{G}=\{G_{V_{i}}:V_{i}\text{ is categorical}\}\) and the latent variables in the original model \(\mathcal{M}\) be \(\mathbf{U}=\{U_{1},\cdots,U_{n_{U}}\}\cup\mathbf{G}\). By Assumption 1 and Assumption 2, \(\mathbf{U}\) are independent and the structure equations of \(\mathcal{M}\) have the form

\[V_{i}=\begin{cases}f_{i}\left(\text{Pa}(V_{i}),\mathbf{U}_{V_{i}}\right),&\text{ $V_{i}$ is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{g_{k}^{V_{i}}+\log\left(f_{i}\left(\text{Pa}(V_{i }),\mathbf{U}_{V_{i}}\right)\right)_{k}\right\},&\text{$V_{i}$ is categorical},\end{cases}\]

where \(g_{k}^{V_{i}}\) are i.i.d. standard Gumbel variables and \(\mathbf{U}_{V_{i}}\subset\{U_{1},\cdots,U_{n_{U}}\}\) contains the latent variables that affect \(V_{i}\). We regroup and merge the variables \(U_{i}\) to make the model have a canonical form while not changing the functions in the structure equations. Let \(D_{1},\cdots,D_{n_{C}}\subset\mathbf{V}\) be the \(C^{2}\)-component of \(\mathcal{M}\). For each \(U_{i}\), we define the vertices that are affected by \(U_{i}\) as

\[I(U_{i})=\{V_{j}:U_{i}\in\mathbf{U}_{V_{j}}\}.\]

We partition \(\{U_{1},\cdots,U_{n_{U}}\}\) into \(n_{C}\) sets \(\hat{\mathbf{U}}_{1},\cdots,\hat{\mathbf{U}}_{n_{C}}\) in the following way. For each \(U_{k}\), \(U_{k}\) is in the set \(\hat{\mathbf{U}}_{i}\) if \(I(U_{k})\subset D_{i}\). If there are two components \(D_{i},D_{j}\) satisfy the condition, we put \(U_{k}\) into either of the sets \(\hat{\mathbf{U}}_{i},\hat{\mathbf{U}}_{j}\). Let \(\hat{U}_{i}\) have the same distribution as the joint distribution of the random variables in set \(\hat{\mathbf{U}}_{i}\). Let \(\hat{\mathcal{M}}\in\mathcal{M}(\mathcal{G},\mathcal{F},\hat{\mathbf{U}})\) the SCM with structure equations

\[\hat{V}_{i}=\begin{cases}f_{i}\left(\text{Pa}(\hat{V}_{i}),\hat{U}_{k_{1}}, \cdots,\hat{U}_{k_{n_{i}}}\right),&\text{$V_{i}$ is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{g_{k}^{V_{i}}+\log\left(f_{i}\left(\text{Pa}(\hat {V}_{i}),\hat{U}_{k_{1}},\cdots,\hat{U}_{k_{n_{i}}}\right)\right)_{k}\right\},&\text{$V_{i}$ is categorical},\end{cases}\]

Here, we slightly abuse the notation \(f_{i}\). \(f_{i}\) ignores inputs from \(\hat{\mathbf{U}}_{k_{1}},\cdots,\hat{\mathbf{U}}_{k_{n_{i}}}\) that are not in \(\mathbf{U}_{V_{i}}\). Note that \(\hat{\mathbf{U}}_{1},\cdots,\hat{\mathbf{U}}_{n_{C}},\mathbf{G}\) has the same distribution as \(\mathbf{U}\) because we only merge some latent variables. In addition, the functions in the new model \(\hat{\mathcal{M}}\) has the same smoothness as the original model.

We first verify that the causal graph of \(\hat{\mathcal{M}}\) is \(\mathcal{G}\). If there is a bi-directed arrow between nodes \(V_{i},V_{j}\) in \(\mathcal{G}\), by the independence assumption, there must be one latent variable \(U_{k}\in\mathbf{U}_{V_{i}}\cap\mathbf{U}_{V_{j}}\). There exist one \(D_{l}\) such that \(I(U_{k})\subset D_{l}\) and \(\hat{\mathbf{U}}_{l}\cap\mathbf{U}_{V_{i}}\neq\emptyset,\hat{\mathbf{U}}_{l}\cap\mathbf{U}_{ V_{j}}\neq\emptyset\). Therefore, \(\hat{U}_{l}\) will affect both \(V_{i}\) and \(V_{j}\) and there is a bi-directed arrow between node \(V_{i},V_{j}\) in \(\mathcal{G}_{\hat{\mathcal{M}}}\). Suppose that there is a bi-directed arrow between node \(V_{i},V_{j}\) in \(\mathcal{G}_{\hat{\mathcal{M}}}\), it means there exist a \(\hat{U}_{k}\) such that \(\hat{\mathbf{U}}_{k}\cap\mathbf{U}_{V_{i}}\neq\emptyset,\hat{\mathbf{U}}_{k}\cap\mathbf{U}_{ V_{j}}\neq\emptyset\). Let \(U_{l_{1}}\in\hat{\mathbf{U}}_{k}\cap\mathbf{U}_{V_{i}},U_{l_{2}}\in\hat{\mathbf{U}}_{k} \cap\mathbf{U}_{V_{j}}\), then

\[V_{i}\in I(U_{l_{1}})\subset D_{k},V_{j}\in I(U_{l_{2}})\subset D_{k}.\]

Since \(D_{k}\) is a \(C^{2}\)-component, there exist a bi-directed arrow between node \(V_{i},V_{j}\) in \(\mathcal{G}\). Therefore, \(\mathcal{G}=\mathcal{G}_{\hat{\mathcal{M}}}\).

Figure 7: Example: two different SCMs with the same causal graph.

Finally, we verify that \(\mathcal{M}\) and \(\hat{\mathcal{M}}\) produce the same intervention results. The intervention distribution can be viewed as the push-forward of the latent distribution, i.e., \(\mathbf{V}=F(\mathbf{U})\), and the intervention operation only changes the function \(F\). In our construction, we only merge the latent variables and leave the functions in structure equations being the same (except that they may ignore some coordinates in input). For any intervention \(T=t\), suppose in \(\mathcal{M}\) we have \(\mathbf{V}(t)=F_{t}(\hat{\mathbf{U}})\). Then, in \(\hat{\mathcal{M}}\), we get \(\hat{\mathbf{V}}(t)=F_{t}(\hat{\mathbf{U}})\). Since \(\mathbf{U}\) and \(\hat{\mathbf{U}}\) has the same distribution, the distribution of \(\mathbf{V}(t),\hat{\mathbf{V}}(t)\) are the same. 

Proof of Theorem 1.: The structure equations of \(\mathcal{M}\) have the form

\[V_{i}=\begin{cases}f_{i}\left(\mathsf{Pa}(V_{i}),\mathbf{U}_{V_{i}}\right),&\text{ $V_{i}$ is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{g_{k}^{V_{i}}+\log\left(f_{i}\left(\mathsf{Pa}(V_ {i}),\mathbf{U}_{V_{i}}\right)\right)_{k}\right\},&\text{ $V_{i}$ is categorical},\|f_{i}\|_{1}=1,\end{cases}\]

where \(g_{k}^{V_{i}}\) are i.i.d. standard Gumbel variables and \(\hat{\mathcal{M}}\in\mathcal{M}(\mathcal{G},\hat{\mathcal{F}},\hat{\mathbf{U}})\) has structure equations

\[\hat{V}_{i}=\begin{cases}\hat{f}_{i}\left(\mathsf{Pa}(\hat{V}_{i}),\hat{\mathbf{U} }_{V_{i}}\right),&\text{ $V_{i}$ is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{\hat{g}_{k}^{V_{i}}+\log\left(\hat{f}_{i}\left( \mathsf{Pa}(\hat{V}_{i}),\hat{\mathbf{U}}_{V_{i}}\right)\right)_{k}\right\},&\text { $V_{i}$ is categorical},\|\hat{f}_{1}\|_{1}=1,\end{cases}\]

where \(\hat{g}_{k}^{V_{i}}\) are i.i.d. standard Gumbel variables. Let the treatment variables set be \(\mathbf{T}=\{T_{1},\cdots,T_{n_{T}}\}\). We may give all vertices and \(\mathbf{U}_{0}=\{U_{1},\cdots,U_{n_{U}}\}\), an topology ordering (rearrange the subscript if necessary)

\[U_{1},\cdots,U_{n_{U}},T_{1},\cdots,T_{n_{T}},V_{1}(\mathbf{t}),\cdots,V_{n_{V}-n_ {T}}(\mathbf{t})\]

such that for each directed edge \((V_{i}(\mathbf{t}),V_{j}(\mathbf{t}))\), vertex \(V_{i}(\mathbf{t})\) lies before vertex \(V_{j}(\mathbf{t})\) in the ordering, ensuring that all edges start from vertices that appear early in the order and end at vertices that appear later in the order. We put \(\mathbf{U}_{0}\) and \(\mathbf{T}\) at the beginning because they are the root nodes of the intervened model. We denote \(\mu_{\mathbf{U}_{0},\mathbf{T},1:k}^{\mathbf{T}}\) (resp. \(\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k}^{\mathbf{T}}\)) to be the distribution of \(\mathbf{U}_{0},\mathbf{T},V_{1}(\mathbf{t}),\cdots,V_{k}(\mathbf{t})\) (resp. \(\hat{\mathbf{U}}_{0},\mathbf{T},\hat{V}_{1}(\mathbf{t}),\cdots,\hat{V}_{k}(\mathbf{t})\)), \(\mu_{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\) the distribution of \(V_{k}\) given \(\mathbf{U}_{0},\mathbf{T},V_{1}(\mathbf{t}),\cdots,V_{k-1}(\mathbf{t})\).

Let \(S=\sum_{i=1}^{n_{V}}\|f_{i}-\hat{f}_{i}\|_{\infty}\). Next, we prove that

\[W\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k}^{\mathbf{T}},\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k}^ {\mathbf{T}}\right)\leqslant(L+1)W\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}, \hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\right)+2K^{2}S. \tag{11}\]

By definition of Wasserstein-1 distance,

\[W\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k}^{\mathbf{T}},\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k}^{\mathbf{T}}\right) =\sup_{g\in Lip(1)}\int g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k})d\left( \mu_{\mathbf{U}_{0},\mathbf{T},1:k}^{\mathbf{T}}-\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k}^{\mathbf{T} }\right)\] \[=\sup_{g\in Lip(1)}\int g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k})\:d\mu _{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\:d\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{ \mathbf{T}}\] \[\qquad\qquad-\int g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k})\:d\hat{\mu} _{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\:d\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1 }^{\mathbf{T}}\] \[=\sup_{g\in Lip(1)}\underbrace{\int g(\mathbf{u},\mathbf{t},v_{1}, \cdots,v_{k})\:d\left(\mu_{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}-\hat{\mu}_ {V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\right)\:d\hat{\mu}_{\mathbf{U}_{0},\mathbf{T },1:k-1}^{\mathbf{T}}}_{(1)}\] \[\qquad+\underbrace{\int g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k})\:d\mu _{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\:d\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1 }^{\mathbf{T}}-\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\right)}_{(2)}.\]

For \((1)\), if \(V_{k}\) is a continuous variable, we have

\[\int g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k})\:d\left(\mu_{V_{k}|\mathbf{U }_{0},\mathbf{T},1:k-1}^{\mathbf{T}}-\hat{\mu}_{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathbf{T}}\right)\] \[=g\left(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1},f_{k}\left(\mathsf{pa} (v_{k}),\mathbf{u}_{V_{k}}\right)\right)-g\left(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1}, \hat{f}_{k}\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)\right)\] \[\leqslant\left\|f_{k}\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right) -\hat{f}_{k}\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)\right\|_{\infty} \leqslant S, \tag{12}\]where we have used the Lipschitz property of \(g\) in the first inequality. If \(V_{k}\) is categorical, let \(\hat{p}\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)=(\hat{p}_{1},\cdots,\hat{p}_ {n_{i}})=\hat{f}\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)\) and \(p\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)=(p_{1},\cdots,p_{n_{i}})=f\left( \mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)\), we get

\[\int g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k})\ d\left(\mu_{V_{k}|\mathbf{U} _{0},\mathbf{T},1:k-1}^{\mathcal{T}}-\hat{\mu}_{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{ \mathcal{T}}\right)\] \[=\sum_{k=1}^{n_{i}-1}(g(\mathbf{u},\mathbf{t},v_{1},\cdots,k)\ -g(\mathbf{u},\mathbf{t},v_{1},\cdots,n_{i}))(p_{k}-\hat{p}_{k})\leqslant K\sum_{k=1}^{n_{i}-1}| p_{k}-\hat{p}_{k}|\leqslant K^{2}S, \tag{13}\]

where we use \(\|V_{k}\|_{\infty}\leqslant K,n_{i}\leqslant K\).

For (2), if \(V_{i}\) is continuous,

\[\int g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k})\ d\mu_{V_{k}|\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}\ d\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T }}-\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}\right)\] \[=\int g\left(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1},f_{k}\left( \mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)\right)\ d\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}-\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}\right). \tag{14}\]

Since \(g,f_{k}\) are Lipschitz continuous functions, \(g\left(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1},f_{k}\left(\mathsf{pa}(v_{k}),\mathbf{u}_ {V_{k}}\right)\right)\) is \((L+1)\)-Lipschitz continuous with respect to \((\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1})\). We have

\[\int g\left(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1},f_{k}\left( \mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)\right)d\left(\mu_{\mathbf{U}_{0},\mathbf{T},1 :k-1}^{\mathcal{T}}-\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}\right)\] \[\leqslant(L+1)W\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}, \hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}\right). \tag{15}\]

If \(V_{i}\) is categorical,

\[\int g(\mathbf{u},\mathbf{t},x_{1},\cdots,x_{k})\ d\mu_{X_{k}|\mathbf{U},\bm {T},1:k-1}^{\mathcal{T}}\ d\left(\mu_{\mathbf{U},\mathbf{T},1:k-1}^{\mathcal{T}}-\hat{ \mu}_{\mathbf{U},\mathbf{T},1:k-1}^{\mathcal{T}}\right)\] \[=\sum_{k=1}^{n_{i}}p_{k}\int g(\mathbf{u},\mathbf{t},x_{1},\cdots,k)\ d \left(\mu_{\mathbf{U},\mathbf{T},1:k-1}^{\mathcal{T}}-\hat{\mu}_{\mathbf{U},\mathbf{T},1:k-1} ^{\mathcal{T}}\right).\]

Since \(g,f_{k}\) are \(L\)-Lipschitz continuous functions, \(g(\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1},i)\) is \(L\)-Lipschitz continuous with respect to \((\mathbf{u},\mathbf{t},v_{1},\cdots,v_{k-1})\). We have

\[\sum_{k=1}^{n_{i}}\int p_{k}g(\mathbf{u},\mathbf{t},x_{1},\cdots,k)\ d \left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}-\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}\right)\leqslant LW\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{ \mathcal{T}},\hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:k-1}^{\mathcal{T}}\right). \tag{16}\]

Combine (12)-(16), we prove Equation (11). By induction, one can easily get

\[W\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:n_{V}-n_{T}}^{\mathcal{T}},\hat{ \mu}_{\mathbf{U}_{0},\mathbf{T},1:n_{V}-n_{T}}^{\mathcal{T}}\right) \leqslant(L+1)^{n_{V}-n_{T}}\left(W\left(\mu_{\mathbf{U}_{0},\mathbf{T} }^{\mathcal{T}},\hat{\mu}_{\mathbf{U}_{0},\mathbf{T}}^{\mathcal{T}}\right)+2K^{2}S/L \right)-2K^{2}S/L\] \[=\frac{(L+1)^{n_{V}-n_{T}}-1}{L}2K^{2}S+(L+1)^{n_{V}-n_{T}}W\left( \mu_{\mathbf{U}_{0},\mathbf{T}}^{\mathcal{T}},\hat{\mu}_{\mathbf{U}_{0},\mathbf{T}}^{\mathcal{ T}}\right)\]

Let \(C_{\mathcal{G}}(L,K)=2K^{2}\cdot\max\{\frac{(L+1)^{n_{V}-n_{T}}-1}{L},(L+1)^{n_{V }-n_{T}}\}\) and notice that \(W\left(\mu_{\mathbf{U}_{0},\mathbf{T}}^{\mathcal{T}},\hat{\mu}_{\mathbf{U}_{0},\mathbf{T}}^{ \mathcal{T}}\right)=W\left(P^{\mathcal{M}}(\mathbf{U}),P^{\mathcal{M}}(\hat{\mathbf{U} })\right)\) because interventions \(\mathbf{T}=\mathbf{t}\) are the same, we get

\[W\left(P^{\mathcal{M}}(\mathbf{V}(\mathbf{t})),P^{\mathcal{M}}(\mathbf{V}( \mathbf{t}))\right) \leqslant W\left(\mu_{\mathbf{U}_{0},\mathbf{T},1:n_{V}-n_{T}}^{\mathcal{T}}, \hat{\mu}_{\mathbf{U}_{0},\mathbf{T},1:n_{V}-n_{T}}^{\mathcal{T}}\right)\] \[\leqslant C_{\mathcal{G}}(L,K)\left(S+W\left(P^{\mathcal{M}}(\mathbf{U}),P^{ \mathcal{M}}(\hat{\mathbf{U}})\right)\right). \tag{17}\]

### Proof of results in Section 3.1

Proof of Theorem 2.: Recall that the neural network consists of three parts \(\hat{g}=\hat{g}_{3}^{\tau}\circ\hat{g}_{2}\circ\hat{g}_{1}\) and \(\hat{g}_{2},\hat{g}_{1}\) have a separable form, dealing with each coordinate of the input individually. As mentioned before, each coordinate approximates the distribution of one connected component of the support. We first construct \(\hat{g}_{1}\) and \(\hat{g}_{2}\), then use Gumbel-Softmax layer to combine each component together.

To construct the first two parts, we only need to consider each coordinate individually. For the \(i\)-th component \(C_{i}\), let \(\mu_{i}=\mathbb{P}_{C_{i}}/\mathbb{P}(C_{i})\), where \(\mathbb{P}_{C_{i}}\) is measure \(\mathbb{P}\) restricted to component \(C_{i}\). Under Assumption 4, there exists a Lipschitz map \(g_{2}^{i}\in\mathcal{H}([0,1]^{d_{i}^{C}},C_{i})\cap\mathcal{F}_{L}([0,1]^{d_ {i}^{C}},C_{i})\). By [43, Theorem VIII.1], there exists quantized ReLu network \(\hat{g}_{1}^{i}\) of width \(W_{1}\) and depth \(\Theta(d_{i}^{C})\) (let \(s=\frac{1}{\lceil n(n-1)\rceil}\) in [43, Theorem VIII.1]), such that

\[W\left(\left((g_{2}^{i})^{-1}\right)_{\#}\mu_{i},P(\hat{g}_{1}^{i}(U_{i})) \right)\leqslant O\left(W_{1}^{-1/d_{i}^{C}}\right),\]

where \(U_{i}\) are i.i.d. uniform random variables on \([0,1]\). By [53, Theorem 2], there exist a deep ReLu network \(\hat{g}_{2}^{i,j}\) of width \(\Theta\left(d_{i}^{C}\right)\) and depth \(L_{2}\) such that

\[\|\left(g_{2}^{i}\right)_{j}-\hat{g}_{2}^{i,j}\|_{\infty}\leqslant O\left(L_{2} ^{-2/d_{i}^{C}}\right),\]

Let \(\hat{g}_{2}^{i}(x)=\left(\hat{g}_{2}^{i,1}(x),\cdots,\hat{g}_{2}^{i,d}(x)\right)\), we get

\[\|g_{2}^{i}-\hat{g}_{2}^{i}\|_{\infty}\leqslant O\left(L_{2}^{-2/d_{i}^{C}} \right).\]

Thus, the width of \(\hat{g}_{2}^{i}\) is \(\Theta\left(d\cdot d_{i}^{C}\right)\). Let

\[\hat{g}_{i}(x)=\left(\hat{g}_{i}^{1}(x_{1}),\cdots,\hat{g}_{i}^{N_{C}}(x_{N_{C }})\right),i=1,2,x_{k}\in\mathbb{R}^{d_{i}},d_{1}=1,d_{2}=d,k=1,\cdots,N_{C},\]

where \(N_{C}\) is the number of connected components. By Lemma 3, we get

\[W\left(\mu_{i},P(\hat{g}_{2}^{i}\circ\hat{g}_{1}^{i}(U_{i}))\right) =W\left(\left(g_{2}^{i}\right)_{\#}\left(g_{2}^{i}\right)_{\#}^{- 1}\mu_{i},\left(\hat{g}_{2}^{i}\right)_{\#}P(\hat{g}_{1}^{i}(U_{i}))\right)\] \[\leqslant LW\left(\left(g_{2}^{i}\right)_{\#}^{-1}\mu_{i},P(\hat {g}_{1}^{i}(U_{i}))\right)+\|g_{2}^{i}-\hat{g}_{2}^{i}\|_{\infty}\] \[=O\left(W_{1}^{-1/d_{i}^{C}}+L_{2}^{-2/d_{i}^{C}}\right).\]

Next, let \(p_{i}=\mathbb{P}(C_{i})\) and the distribution of \(\hat{g}_{2}^{k}\circ\hat{g}_{1}^{k}(U_{k})\) be \(\hat{\mu}_{k}\), then we have \(\mathbb{P}=\sum_{k=1}^{N_{C}}p_{k}\mu_{k}\). By Lemma 4, we have

\[W\left(\mathbb{P},\sum_{k=1}^{N_{C}}p_{k}\hat{\mu}_{k}\right)\leqslant\sum_{k= 1}^{N_{C}}p_{k}W\left(\mu_{i},\hat{\mu}_{k}\right)=O\left(W_{1}^{-1/\max\{d_{ i}^{C}\}}+L_{2}^{-2/\max\{d_{i}^{C}\}}\right).\]

Finally, we analyze the error caused by the Gumbel-Softmax layer. Note that as temperature parameter \(\tau\to 0\),

\[w^{\tau}=\left(\frac{\exp((\log p_{i}+G_{i})/\tau)}{\sum_{k=1}^{N_{C}}\exp(( \log p_{k}+G_{k})/\tau)}\right)_{i=1,\cdots,N_{C}}\stackrel{{ a.s.}}{{\longrightarrow}}\text{One-hot}(\arg\max_{i}\{G_{i}+\log p_{i}\})\]

where \(\text{One-hot}(k)\) is \(N_{C}\)-dimensional vector with the \(k\)-th coordinate equals to \(1\) and the remaining coordinates are \(0\) and \(G_{i}\sim\text{Gumbel}(0,1)\) are i.i.d. Gumbel distribution. Let \(\nu^{\tau}\) be the distribution of \(w^{\tau}\). By Lemma 6, for \(0<\tau<1\), we have

\[W\left(\nu^{\tau},\nu^{0}\right)\leqslant O(\tau-\tau\log\tau).\]

By Lemma 5,

\[W\left(\left(\nu^{\tau},P(\left(\hat{g}_{2}^{i}\circ\hat{g}_{1}^{i}\right)(U_{i })\right))\,,\left(\nu^{0},P(\left(\hat{g}_{2}^{i}\circ\hat{g}_{1}^{i}\right)(U_ {i}))\right)\right)\leqslant O\left(\tau-\tau\log\tau\right). \tag{18}\]

Let

\[\hat{g}_{3}^{\tau}(x_{1},\cdots,x_{N_{C}})=\sum_{k=1}^{N_{C}}w_{k}^{\tau}\cdot x _{k},\quad x_{k}\in\mathbb{R}^{d},\;w^{\tau}=\left(w_{1}^{\tau},\cdots,w_{N_{C }}^{\tau}\right).\]We denote \(g_{3}^{0}=\lim_{\tau\to 0}g_{3}^{\tau}\). By Assumption 4, the support is bounded. Thus, \(g_{2}^{i}\) and \(\hat{g}_{2}^{i}\) are bounded, i.e., \(\|\hat{g}_{2}^{i}\ (x)\|_{\infty}\leqslant K\). Since functions \(h(w,X)=w^{\mathsf{T}}X\) is Lipschitz continuous in the region \(\|w\|_{1}=1,\|X\|_{\infty}\leqslant K\), we get

\[W\left(P(\hat{g}_{3}^{0}\circ\hat{g}_{2}\circ\hat{g}_{1}(U_{k})),P(\hat{g}_{3} ^{\tau}\circ\hat{g}_{2}\circ\hat{g}_{1}(U_{k}))\right)\leqslant O\left(\tau- \tau\log\tau\right) \tag{19}\]

from (18). Putting things together, we get

\[W\left(\mathbb{P},P(\hat{g}_{3}^{\tau}\circ\hat{g}_{2}\circ\hat{ g}_{1}(U_{k}))\right) \leqslant W\left(\mathbb{P},\sum_{k=1}^{N_{C}}p_{k}\hat{\mu}_{k} \right)+W\left(\sum_{k=1}^{N_{C}}p_{k}\hat{\mu}_{k},P(\hat{g}_{3}^{\tau}\circ \hat{g}_{2}\circ\hat{g}_{1}(U_{k}))\right)\] \[=W\left(\mathbb{P},\sum_{k=1}^{N_{C}}p_{k}\hat{\mu}_{k}\right)+W \left(P(\hat{g}_{3}^{0}\circ\hat{g}_{2}\circ\hat{g}_{1}(U_{k})),P(\hat{g}_{3}^ {\tau}\circ\hat{g}_{2}\circ\hat{g}_{1}(U_{k}))\right)\] \[\leqslant O\left(\tau-\tau\log\tau+W_{1}^{-1/\max\{d_{i}^{C}\}}+L _{2}^{-2/\max\{d_{i}^{C}\}}\right),\]

where we use (19). 

### Proof of results in Section 3.2

Before we prove Proposition 1, we need to introduce some important notions. Let \(\{\mathcal{C}_{n}\}\) be a sequence of partitions of the unit cube \([0,1]^{d}\). We say \(\{\mathcal{C}_{n}\}\) has property (*) if the following properties are satisfied.

1. \(\mathcal{C}_{0}=\{C_{-1}\},C_{-1}=[0,1]^{d}\)
2. \(\mathcal{C}_{n+1}=\{C_{i_{1},i_{2}\cdots,i_{n+1}}\}_{i_{j}=0,\cdots,2^{d}-1}\) is a refinement of \(\mathcal{C}_{n}=\{C_{i_{1},i_{2}\cdots,i_{n}}\}_{i_{j}=0,\cdots,2^{d}-1}\), i.e., \(C_{i_{1},i_{2}\cdots,i_{n}}=\bigcup_{i_{n+1}=0}^{2^{d}-1}C_{i_{1},i_{2}\cdots, i_{n+1}}\). Besides, \(\mathcal{C}_{n}\) is obtained by cutting the unit cube \([0,1]^{d}\) by planes that are paralleled to the coordinate planes.

The following lemma is important in the construction of Hilbert curve.

**Lemma 1** (Algorithm 3 in [25]).: _Given a sequence of partitions of the unit cube \(\{\mathcal{C}_{n}\}\) that has property (*), there exist a ordering for each partition \(\mathcal{C}_{n}\), i.e., \(O_{n}:\left\{1,\cdots,2^{nd}\right\}\rightarrow\left\{\overline{i_{1}i_{2} \cdots i_{n}}:i_{j}=0,\cdots,2^{d}-1\right\}\), such that the following two conclusions hold._

1. _If_ \((j-1)\cdot 2^{d}<i\leqslant j\cdot 2^{d}\)_, the first_ \(n\) _digits of_ \(O_{n+1}(i)\) _are the same as_ \(O_{n}(j)\)_._
2. _Adjacent cubes in the ordering intersect, i.e.,_ \(C_{O_{n}(i)}\cap C_{O_{n}(i+1)}\neq\emptyset\)_. Furthermore,_ \(C_{O_{n}(i)}\cap C_{O_{n}(i+1)}\) _is a_ \(d-1\) _dimensional cube._

Figure 8: Architecture of the deep neural network for \(4-\)dimensional output. The first (yellow) part approximates the distribution on different connected components using deep ReLu Networks. The remaining two parts are similar to the wide neural network in Figure 0(a).

Proof of Proposition 1.: Without loss of generality, we can assume that \(\mathbb{P}\) is supported on \([0,1]^{d}\), vanishes at the boundary and \(\mathbb{P}(B)\geqslant C_{1}\lambda(B)\) for some constant \(C_{1}>0\) and all measurable sets \(B\in[0,1]^{d}\). Suppose we have proven the conclusion for this case. By Assumption 5, there exists \(f\in\mathcal{H}([0,1]^{d},K)\cap\mathcal{F}_{L}([0,1]^{d},K)\) such that \(f_{\#}^{-1}\mathbb{P}\) satisfies these conditions. There exists continuous function \(\gamma\) such that \(f_{\#}^{-1}\mathbb{P}=\gamma_{\#}\lambda\), which implies \(\mathbb{P}=(f\circ\gamma)_{\#}\lambda\). In particular, if \(\gamma\) is \(1/d\)-Holder continuous, \(f\circ\gamma\) is also \(1/d\)-Holder continuous because \(f\) is Lipschitz continuous.

When \(d=0\), the constant map satisfies the requirement. We focus on the case \(d\geqslant 1\) in the following. The proof is to modify the construction of the Hilbert space-filling curve. By changing the velocity of the curve, the push-forward measure of the Hilbert space-filling curve can simulate a great number of distributions. We use \(\lambda\) to represent the Lebesgue measure \([0,1]\).

**Proof Sketch:** The construction of \(f\) is inspired by the famous Hilbert space-filling curve [28]. To illustrate the idea, let us first assume that \(\mathbb{P}\) is absolutely continuous with respect to the Lebesgue measure \(\lambda\) and consider \(d=2\). In the \(k\)-th step, we divide the unit cube into \(2^{2k}\) evenly closed cubes \(\mathcal{C}_{n}=\{C_{1,n},\cdots,C_{2^{2k},n}\}\) such that \(\mathcal{C}_{n}\) is a refinement of \(\mathcal{C}_{n-1}\).

The construction of a standard Hilbert curve is to find a sequence of curves \(\gamma_{n}\) that go through all the cubes in \(\mathcal{C}_{n}\). The curve \(\gamma_{n}\) has one special property. If \(\gamma_{n-1}(t)\in C_{k,n-1}\), then \(\gamma_{m}(t)\in C_{k,n-1},\forall m\geqslant n\). For example, in Figure 9, the points on the curve in the lower-let cubes will stay in the lower-left cubes. Note that \((\gamma_{n})_{\#}\lambda(C_{k,n})=\lambda\left(\gamma_{n}^{-1}(C_{k,n})\right)\) is the time curve \(\gamma_{n}\) stays in cubes \(C_{k,n}\). The idea is to change the speed of the curve so that \(\mathbb{P}\) and \((\gamma_{n})_{\#}\lambda\) agree on cubes in \(\mathcal{C}_{n}\), i.e., \(\mathbb{P}(C_{k,n})=\lambda\left(\gamma_{n}^{-1}(C_{k,n})\right)\). Since we assume that \(\mathbb{P}\) is absolutely continuous, \(\mathbb{P}(\partial C_{k,n})=0\) and we don't need to worry about how to divide the mass on the boundary. For example, let the green cubes in the Figure 9 to be \(C_{0}\) and suppose that \(\gamma_{1}\) starts from \(C_{0}\). We will change the speed so that \(\gamma_{1}\) spends \(\mathbb{P}(C_{0})\) time in this region. In the next step, we divide \(C_{0}\) into four colored cubes \(C_{1},\cdots,C_{4}\) on the right. We change the speed again to let the time spent in each cube equal to \(\mathbb{P}(C_{i})\). Note that this construction preserves the aforementioned property, i.e., for \(t\in[0,\mathbb{P}(C_{0})]\), \(\gamma_{n}(t)\in C_{0},\forall n\geqslant 1\). As \(n\to\infty\), it can be proven that \(\gamma_{n}\) converges uniformly to a curve \(\gamma\). We can also prove that \(\mathbb{P}\) and \(\gamma_{\#}\lambda\) agree on \(\cup_{n=1}^{\infty}\mathcal{C}_{n}\). Given that \(\cup_{n=1}^{\infty}\mathcal{C}_{n}\) generate the standard Borel algebra on \([0,1]^{2}\), we can conclude that \(\mathbb{P}=\gamma_{\#}\lambda\).

In the general case, \(\mathbb{P}\) may not be absolutely continuous. As a result, the boundary may not be \(\mathbb{P}\)-measure zero. We will need to perturb the cutting planes to ensure their boundaries are measured zero.

**Preparation.** For a \(n\)-dimensional cube \(C=\prod_{i=1}^{n}[a_{i},b_{i}]\), the diameter of \(C\) is \(\text{diam}(C)=\sqrt{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\). We also define the function \(R\) that measures the ratio between maximum edge and minimum edge. For a collection of cubes \(\mathcal{C}=\{C_{1},\cdots,C_{m}\},C_{k}=\prod_{i=1}^{n}\left[a_{i}^{k},b_{i}^{ k}\right]\), \(R(\mathcal{C})\) is defined to be

\[R(\mathcal{C})=\frac{\max_{i,k}|a_{i}^{k}-b_{i}^{k}|}{\min_{i,k}|a_{i}^{k}-b_ {i}^{k}|}.\]

Figure 9: The construction of Hilbert curve.

\(R(\mathcal{C})\) measures the shape of the cubes in a collection. We need to control \(R(\mathcal{C})\) in the construction to obtain Holder continuity in **Step 2**.

**Step 1: Define partition.** First, we construct a sequence of partitions that has the property (*) recursively. Let \(\mathcal{C}_{0}=\left\{[0,1]^{d}\right\}\). Suppose we have defined close cubes collection \(\mathcal{C}_{n}=\{C_{i_{1},i_{2}\cdots,i_{n}}\}_{i_{j}=0,\cdots,2^{d}-1}\),, such that

1. \(\mathcal{C}_{n}\) is obtained by cutting the unit cube \([0,1]^{d}\) by planes that are paralleled to the coordinate planes.
2. \(C_{i_{1},i_{2}\cdots,i_{n-1}}=\bigcup_{i_{n}}C_{i_{1},i_{2}\cdots,i_{n}}\) and \(\text{diam}(C_{i_{1},i_{2}\cdots,i_{n-1}})\leqslant 2/3\cdot\text{diam}(C_{i_{1},i_{2}\cdots,i_{n}})\).
3. \(\mathbb{P}(\partial C_{i_{1},i_{2}\cdots,i_{n}})=0\) and \(R(\mathcal{C}_{n+1})\leqslant\left(1+2^{-n}\right)R(\mathcal{C}_{n})\).

Next, we construct \(\mathcal{C}_{n+1}\) from \(\mathcal{C}_{n}\) that preserves these properties. We refine the division \(\mathcal{C}_{n}=\{C_{i_{1},i_{2}\cdots,i_{n}}\}_{i_{j}=0,\cdots,2^{d}-1}\) by union of hyperplane \(P_{i,r}=\{y:y_{i}=r\}\) to get \(\{C_{i_{1},i_{2}\cdots,i_{n+1}}\}\) such that \(C_{i_{1},i_{2}\cdots,i_{n}}=\bigcup_{i_{n+1}}C_{i_{1},i_{2}\cdots,i_{n+1}}\) and \(\text{diam}(C_{i_{1},i_{2}\cdots,i_{n-1}})\leqslant 2/3\cdot\text{diam}(C_{i_{1},i_{2}\cdots,i_{n}})\). By property 1, \(\mathcal{C}_{n}\) is obtained by dividing \([0,1]^{d}\) using

\[P_{1,r_{0}^{1}},\cdots,P_{1,r_{2^{n-1}}^{1}},\cdots,P_{i,r_{j}^{1}},\cdots,P_{d,r_{2^{n}}^{d}},r_{0}^{i}=0,r_{2^{n}}^{i}=1,\]

we can further cut the unit cube using hyperplane \(\{P_{i,\left(r_{j}^{i}+r_{j+1}^{i}\right)/2}\}_{i=1,\cdots d,j=1,\cdots 2^{n}-1}\). However, this construction may not satisfy property 3, \(\mathbb{P}(\partial C_{i_{1},i_{2}\cdots,i_{n+1}})=0\). We need to perturb each hyperplane to ensure a measure-zero boundary. Since

\[Z_{i}=\left\{r\in\mathbb{R}:\mathbb{P}\left(\left\{x\in[0,1]^{d},[x]_{i}=r \right\}\right)\neq 0\right\}\]

is at most countable, we can perturb the hyperplanes a little bit, i.e., \(\{P_{i,\left(r_{j}^{i}+r_{j+1}^{i}\right)/2+\epsilon_{i,j}}\}_{i=1,\cdots d,j= 1,\cdots 2^{n}-1}\) to ensure property 3. In this way, we can choose \(|\epsilon_{i,j}|\) to be sufficiently small such that

\[\text{diam}(C_{i_{1},i_{2}\cdots,i_{n+1}})\leqslant 2/3\cdot\text{diam}(C_{i_{1},i_{2}\cdots,i_{n}}),\]

and

\[\frac{\max_{i,j}\left(\left(r_{j+1}^{i}-r_{j}^{i}\right)/2+|\epsilon_{i,j}| \right)}{\min_{i,j}\left(\left(r_{j+1}^{i}-r_{j}^{i}\right)/2-|\epsilon_{i,j}| \right)}\leqslant\]

Therefore, \(R(\mathcal{C}_{n+1})\leqslant\left(1+2^{-n}\right)R(\mathcal{C}_{n})\) and it is easy to see that properties 1,2,3 are satisfied.

**Step 2: Construct Hilbert Curve.** By Lemma 1, there exist an sequence of ordering \(\{O_{n}\}\) of \(\{\mathcal{C}_{n}\}\) that satisfies

* The first \(n\) digits of \(O_{n+1}(i)\) with \(O_{n}(j),\ (j-1)\cdot 2^{d}<i\leqslant j\cdot 2^{d}\).
* \(C_{O_{n+1}(i)}\cap C_{O_{n+1}(i+1)}\) are \(d-1\) dimensional cubes.

Let \(\mathcal{I}_{0}=\{[0,1]\}\), we define \(\mathcal{I}_{n}\) recursively. Suppose we have define interval collection \(\mathcal{I}_{n}=\{\{I_{i_{1},i_{2}\cdots,i_{n}}\}_{i_{j}=0,\cdots,2^{d}-1}\}\) such that \(I_{i_{1},i_{2}\cdots,i_{n-1}}=\bigcup_{i_{n}}I_{i_{1},i_{2}\cdots,i_{n}}\) and \(\mathbb{P}(C_{i_{1},i_{2}\cdots,i_{n}})=|I_{i_{1},i_{2}\cdots,i_{n}}|\). Since \(\mathbb{P}(\partial C_{i_{1},i_{2}\cdots,i_{n+1}})=0\), we have \(\mathbb{P}(C_{iThe idea is to construct a piecewise linear curve going through all cubes in \(\mathcal{C}_{n+1}\) exactly once and modify its speed. Take \(v_{0}\in\text{Center}(C_{O_{n+1}(1)}),v_{i}\in\text{Center}(C_{O_{n+1}(i)}\cap C_{ O_{n+1}(i+1)}),v_{2^{d(n+1)}}\in\text{Center}(C_{O_{n+1}(2^{d(n+1)})})\), where \(\text{Center}(\prod_{k=1}^{k}[a_{i},b_{i}])=((a_{i}+b_{i})/2)_{i=1,\cdots,k}\) is the center of a cube. \(v_{i}\) is well-defined since \(C_{O_{n+1}(i)}\cap C_{O_{n+1}(i+1)}\) are cubes of dimension \(d-1\) by Lemma 1. One possible choice of \(\gamma_{n+1}(t)\) is

\[\gamma_{n+1}(t)=v_{0}+\sum_{i=1}^{2^{(n+1)d}}\frac{1}{p_{i}}(v_{i}-v_{i-1}) \left(\left(t-\sum_{k=1}^{i-1}p_{k}\right)^{+}-\left(t-\sum_{k=1}^{i}p_{k} \right)^{+}\right). \tag{22}\]

The first two curves \(\gamma_{1},\gamma_{2}\) are shown in Figure 9. It is straightforward to verify that \(\gamma_{n+1}(\sum_{k=1}^{i}p_{k})=v_{i}\) and

\[\gamma_{n+1}(I_{O_{n+1}(i)})=\gamma_{n+1}([\sum_{k=1}^{i-1}p_{k},\sum_{k=1}^{i }p_{k}])\subset C_{O_{n+1}(i)}.\]

In fact, since \(v_{i-1},v_{i}\) are on different surfaces of the cube \(C_{O_{n+1}(i)}\), the line segment between \(v_{i-1},v_{i}\) lies inside the interior of \(C_{O_{n+1}(i)}\) and \(\gamma_{n+1}\) goes through all the cubes \(C_{O_{n+1}(i)}\) exactly once. We have \(\gamma_{n+1}^{-1}(\text{Int}(C_{O_{n+1}(i)}))\subset[\sum_{k=1}^{i-1}p_{k},\sum _{k=1}^{i}p_{k}]\) and \((\gamma_{n+1})_{\#}\lambda(C_{O_{n+1}(i)})=\mathbb{P}(C_{O_{n+1}(i)})\), where \(\text{Int}(\cdot)\) is the interior of a set. Besides, we also have \((\gamma_{n+1})_{\#}\lambda(\partial C_{O_{n+1}(i)})\leqslant\lambda(\gamma_{n+ 1}^{-1}(\{v_{j}\}_{j=1\cdots 2^{d(n+1)}}))=0\).

Finally, for any \(k_{1}\geqslant k_{2}\geqslant n,t\in[0,1]\), by property 2 in step 1, (20) and (22), \(\gamma_{k_{1}}(t),\gamma_{k_{2}}(t)\) are in one cube \(C_{i_{1},i_{2}\cdots,i_{n}}\). and \(\text{diam}(C_{i_{1},i_{2}\cdots,i_{n}})\leqslant\left(\frac{2}{3}\right)^{n}\sqrt {d}\). Thus, \(|\gamma_{k_{1}}(t)-\gamma_{k_{2}}(t)|\leqslant\left(\frac{2}{3}\right)^{n}\sqrt {d}\) which implies \(\{\gamma_{n}(t)\}\) converges uniformly to one continuous function \(\gamma(t)\).

**Step 3: Verify Conclusion**\(\mathbb{P}=\gamma_{\#}\lambda\). By \(W((\gamma_{\#})_{\#}\lambda,\gamma_{\#}\lambda)\leqslant\|\gamma_{k}-\gamma\|_ {\infty}\to 0\), \((\gamma_{k})_{\#}\lambda\) converges weakly to \(\gamma_{\#}\lambda\). By construction, \(\mathbb{P}(\partial C_{i_{1},i_{2}\cdots,i_{n}})=0\) and \((\gamma_{n})_{\#}\lambda(C_{i_{1},i_{2}\cdots,i_{n}})=\mathbb{P}(C_{i_{1},i_ {2}\cdots,i_{n}})\). Therefore, by condition 2 in step 1, for any \(k\geqslant n\),

\[(\gamma_{k})_{\#}\lambda(C_{i_{1},i_{2}\cdots,i_{n}}) =(\gamma_{k})_{\#}\lambda(\bigcup_{i_{n+1},\cdots,i_{k}}C_{i_{1}, i_{2}\cdots,i_{k}})\] \[=\sum_{i_{n+1},\cdots,i_{k}}(\gamma_{k})_{\#}\lambda(C_{i_{1},i_{ 2}\cdots,i_{k}})\] \[=\sum_{i_{n+1},\cdots,i_{k}}\lambda\left(\gamma_{k}^{-1}(C_{i_{1},i_{2}\cdots,i_{k}})\right)\] \[=\sum_{i_{n+1},\cdots,i_{k}}\mathbb{P}(C_{i_{1},i_{2}\cdots,i_{k }})=\mathbb{P}(C_{i_{1},i_{2}\cdots,i_{n}}), \tag{23}\]

where we use \(\mathbb{P}(\partial C_{i_{1},i_{2}\cdots,i_{n}})=0\) and \((\gamma_{k})_{\#}\lambda(\partial C_{i_{1},i_{2}\cdots,i_{n}})=0\) in the second and the last equation. We claim that \(\gamma_{\#}\lambda(\partial C_{i_{1},i_{2}\cdots,i_{n}})=0\). For \(k>n\), we define

\[B_{k}=\bigcup_{\partial C_{i_{1}},i_{2}\cdots,i_{k}}\cap\partial C_{i_{1},i_{ 2}\cdots,i_{n}}\neq\emptyset\quad C_{i_{1},i_{2}\cdots,i_{k}}.\]

Let \(k>k_{1}\geqslant n\) and \(B_{n}^{\epsilon}=\{x:d(x,\partial C_{i_{1},i_{2}\cdots,i_{n}})<\epsilon\}\), we have

\[(\gamma_{k})_{\#}\lambda\left(\text{Int}(B_{k_{1}})\right)\leqslant(\gamma_{k })_{\#}\lambda(B_{k_{1}})=\mathbb{P}(B_{k_{1}})\leqslant\mathbb{P}\left(B_{n }^{(2/3)^{k_{1}}\sqrt{d}}\right),\quad\forall k>k_{1},\]

where we use (23) and the fact that \((\gamma_{k})_{\#}\lambda\) vanishes on \(\partial C_{i_{1},i_{2}\cdots,i_{k}}\) in the first equality and \(\text{diam}(C_{i_{1},i_{2}\cdots,i_{k_{1}}})\leqslant\left(\frac{2}{3}\right)^{k _{1}}\sqrt{d}\) in the last inequality. Let \(k\to\infty\), by Portmanteau Theorem, we get

\[\gamma_{\#}\lambda(\partial C_{i_{1},i_{2}\cdots,i_{n}})\leqslant\gamma_{\#} \lambda\left(\text{Int}(B_{k_{1}})\right)\leqslant\liminf_{k\to\infty}(\gamma _{k})_{\#}\lambda(B_{k_{1}})\leqslant\liminf_{k\to\infty}\mathbb{P}\left(B_{n }^{(2/3)^{k_{1}}\sqrt{d}}\right)=\mathbb{P}\left(B_{n}^{(2/3)^{k_{1}}\sqrt{d}} \right).\]

Since \(k_{1}\) is arbitrary, let \(k_{1}\to\infty\),

\[\gamma_{\#}\lambda(\partial C_{i_{1},i_{2}\cdots,i_{n}})\leqslant\lim_{k_{1}\to \infty}\mathbb{P}\left(B_{n}^{(2/3)^{k_{1}}\sqrt{d}}\right)=\mathbb{P}( \partial C_{i_{1},i_{2}\cdots,i_{n}})=0\]and we conclude that \(\gamma_{\#}\lambda(\partial C_{i_{1},i_{2}\cdots,i_{n}})=0\). By Portmanteau Theorem, let \(k\to\infty\) in (23), we obtain \(\gamma_{\#}\lambda(C_{i_{1},i_{2}\cdots,i_{n}})=\mathbb{P}(C_{i_{1},i_{2}\cdots,i _{n}})\). Let \(\mathcal{C}^{*}=\bigcup_{i=1}^{\infty}\mathcal{C}_{i}\). Notice that for any open set \(U\in[0,1]^{d}\), \(U=\bigcup_{C\in U,C\in\mathcal{C}^{*}}C\), which means the \(\sigma\)-algebra generated by \(\mathcal{C}^{*}\) contains all Borel sets. Besides, \(\mathcal{C}^{*}\) is a \(\pi\)-system. Hence, \(\lambda\left(\gamma^{-1}(U)\right)=\mathbb{P}(U)\) for all Lebesgue measurable set \(U\).

**Step 4: Holder Continuity of \(\gamma\).** We verify the condition of [47, Theorem 1]. \(\mathcal{I}_{n}\) and \(\mathcal{C}_{n}\) in the above construction correspond to developments \(\alpha_{k},\beta_{k}\) in [47, Theorem 1]. Define \(f_{k}(I_{O_{k}(i)})=C_{O_{k}(i)},\;i=1,\cdots,2^{kd}\) as a map from \(\mathcal{I}_{n}\) to \(\mathcal{C}_{n}\).

1. By construction, if \(I_{1}\in\mathcal{I}_{k},I_{2}\in\mathcal{I}_{k-1},I_{1}\subset I_{2}\), \(f_{k}(I_{1})\subset f_{k-1}(I_{2})\).
2. If \(I_{1},I_{2}\in\mathcal{I}_{k}\) and \(I_{1}\cap I_{2}\neq\emptyset\), \(I_{1}\), \(I_{2}\) are adjacent. By Lemma 1, \(f_{k}(I_{1}),f_{k}(I_{2})\) share a \((d-1)\)-dimensional boundary. Therefore, \(f_{k}(I_{1})\cap f_{k}(I_{2})\neq\emptyset\).
3. We have \(|\mathcal{C}_{k-1}|\leqslant 2^{d}|\mathcal{C}_{k}|\).
4. By property 3, \[R(\mathcal{C}_{n})\leqslant\prod_{k=1}^{n-1}\left(1+2^{-k}\right)<\prod_{k=1}^{ \infty}\left(1+2^{-k}\right)<\infty.\] Let \(A=\prod_{k=1}^{\infty}\left(1+2^{-k}\right)\). We have for any \(C\in\mathcal{C}_{n}\), by Assumption 5 \[\text{diam}^{d}(C)\leqslant R(\mathcal{C}_{n})\lambda(C)\leqslant AC_{1}^{-1} \mathbb{P}(C)\leqslant AC_{1}^{-1}\max_{I\in\mathcal{I}}\left(\text{diam}(I) \right),\] where \(C_{1}\) is the constant in Assumption 5.
5. Since \(\mathcal{I}_{k}\) consists of one-dimensional intervals, we have \(\text{diam}(\mathcal{I}_{k})=\text{gap}(\mathcal{I}_{k})\).

Therefore, \(\gamma\) is \(1/d\)-Holder continuous. 

Proof of Theorem 3.: The proof is similar to the proof of Theorem 2. The difference is that in the first part, we use a deep ReLU network to approximate the Holder continuous curve \(\gamma\) instead of wide ReLU network.

Let \(\mu_{i}=\mathbb{P}_{C_{i}}/\mathbb{P}(C_{i})\). By Assumption 4 and Assumption 5, for each connected component \(C_{i}\) of the support, there exists Lipschitz maps \(g_{2}^{i}\in\mathcal{H}([0,1]^{d_{i}^{C}},C_{i})\cap\mathcal{F}_{L}([0,1]^{d_{ i}^{C}},C_{i})\) such that

\[(g_{2}^{i})_{\#}^{-1}\mu_{i}(B)\geqslant C_{g}\lambda(B)/\mathbb{P}(C_{i}),\]

for any measurable set \(B\subset[0,1]^{d_{i}^{C}}\). By Proposition 1, there exist a \(1/d_{i}^{C}\)-Holder continuous curve \(\gamma_{i}:[0,1]\to[0,1]^{d_{i}^{C}}\) such that \((\gamma_{i})_{\#}\lambda=(g_{2}^{i})_{\#}^{-1}\mu_{i}\).

By [53, Theorem 2] (take \(\omega(x)=C\|x\|^{1/d}\) where \(C\) is the Holder continuity constant factor), there exists deep ReLu network \(\hat{g}_{1}^{i,j}\) of width \(12\) and depth \(L_{1}\) such that

\[\|\hat{g}_{1}^{i,j}-(\gamma_{i})_{j}\|_{\infty}\leqslant O\left(L_{1}^{-2/d_{ i}^{C}}\right),\]

where \((\gamma_{i})_{j}\) is the \(j\)-coordinate of \(\gamma_{i}\). Let \(\hat{g}_{1}^{i}(x)=\left(\hat{g}_{1}^{i,1}(x),\cdots,\hat{g}_{1}^{i,d}(x)\right)\), we get

\[\|\hat{g}_{1}^{i}-\gamma_{i}\|_{\infty}\leqslant O\left(L_{1}^{-2/d_{i}^{C}} \right).\]

Thus, the width of \(\hat{g}_{1}^{i}\) is \(\Theta\left(d_{i}^{C}\right)\). By Lemma 3

\[W\left(\left(g_{2}^{i}\right)_{\#}^{-1}\mu_{i},P(\hat{g}_{1}^{i}(U_{i})) \right)=W\left((\gamma_{i})_{\#}U_{i},P(\hat{g}_{1}^{i}(U_{i}))\right)\leqslant \|\hat{g}_{1}^{i}-\gamma_{i}\|_{\infty}\leqslant O\left(L_{1}^{-2/d_{i}^{C}} \right).\]

The rest are the same as proof of Theorem 2.

Proof of Corollary 1.: Suppose that structure equations of \(\mathcal{M}\) are

\[V_{i}=\begin{cases}f_{i}\left(\mathsf{Pa}(V_{i}),\mathbf{U}_{V_{i}}\right),&V_{i}\text { is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{g_{k}^{V_{i}}+\log\left(f_{i}\left(\mathsf{Pa}(V_{ i}),\mathbf{U}_{V_{i}}\right)\right)_{k}\right\},&V_{i}\text{ is categorical},\|f_{i}\|_{1}=1\end{cases},\quad f_{i}\in\mathcal{F}_{L}.\]

Let \(d_{i}^{\text{in}}\) be the input dimension of \(f_{i}\), \(d_{i}^{\text{out}}\) be the output dimension,

\[\hat{f}_{i}=\begin{cases}\arg\min_{f_{i}\in\mathcal{N}\mathcal{N}_{d_{i}^{ \text{in}},d_{i}^{\text{out}}}}(d_{i}^{\text{out}}(2d_{i}^{\text{in}}+10),L_{ 0})\;\|\hat{f}_{i}-f_{i}\|_{\infty},&V_{i}\text{ continuous},\\ \max\{0,\arg\min_{\hat{f}_{i}\in\mathcal{N}\mathcal{N}_{d_{i}^{\text{in}},d_{i }^{\text{out}}}}(d_{i}^{\text{out}}(2d_{i}^{\text{in}}+10),L_{0})\;\|\hat{f} _{i}-f_{i}\|_{\infty}\},&V_{i}\text{ categorical}.\end{cases}\]

We truncate the neural network at \(0\) for categorical variables because the propensity functions are required to be non-negative. This truncate operation will not influence the approximation error since \(f_{i}\) are non-negative. According to Assumption 3, \(f_{i}\) are Lipschitz continuous. By [53, Theorem 2], we have, if \(V_{i}\) is continuous, \(\|\hat{f}_{i}-f_{i}\|_{\infty}\leq O(L_{0}^{-1/d_{i}^{\text{in}}})\leq O(L_{0} ^{-1/d_{\text{max}}^{\text{in}}})\).

If \(V_{i}\) is categorical, let \(S=\|\hat{f}\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)-f\left(\mathsf{pa}( v_{k}),\mathbf{u}_{V_{k}}\right)\|_{\infty}\), \(\mathbf{p}=f\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right),\hat{\mathbf{p}}=\hat{f} \left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)/\|\hat{f}\|_{1}\). If \(S>1/(2K)\), \(\|\mathbf{p}-\hat{\mathbf{p}}\|_{\infty}\leq 2KS\). If \(S\leq 1/(2K)\),

\[\|\mathbf{p}-\hat{\mathbf{p}}\|_{\infty} \leqslant\frac{1}{\|\hat{f}\|_{1}}\|\hat{f}\left(\mathsf{pa}(v_{k }),\mathbf{u}_{V_{k}}\right)-f\left(\mathsf{pa}(v_{k}),\mathbf{u}_{V_{k}}\right)\|_{ \infty}+\frac{1}{\|\hat{f}\|_{1}}\|\|\hat{f}\|_{1}-1\|\|f\|_{\infty}\] \[\leqslant\frac{(n_{i}+1)}{\|\hat{f}\|_{1}}S\leqslant\frac{(n_{i} +1)}{1-n_{i}S}S\leqslant 2(K+1)S\]

where we use Assumption 3 that \(n_{i}\leqslant K\) and \(S\leqslant 1/(2K)\). Thus,

\[\|f_{i}-\hat{f}_{i}/\|\hat{f}_{i}\|_{1}\|_{\infty}\leq O(\|f_{i}-\hat{f}_{i}\| _{\infty})\leqslant O(L_{0}^{-1/d_{\text{max}}^{\text{in}}}).\]

By Theorem 3, there exists a neural network \(\hat{g}_{j}\) with architecture in Theorem 3 such that

\[W(P(U_{j}),P(\hat{g}_{j}(Z_{j})))\leqslant O\left(L_{1}^{-2/d_{j}^{U}}+L_{2}^ {-2/d_{j}^{U}}+(\tau-\tau\log\tau)\right),\]

where \(Z_{j}\sim U([0,1]^{N_{C,j}})\) i.i.d., \(N_{C,j}\) is the number of connected components of support of \(U_{j}\), \(d_{j}^{U}\) is the dimension of latent variables \(U_{j}\) and \(L_{1},L_{2},\tau\) are hyperparameters of the neural network defined in Theorem 3. Let \(\hat{U}_{j}=\hat{g}_{j}(Z_{j})\), By Lemma 5,

\[W(P(U_{1},\cdots,U_{n_{U}}),P(\hat{U}_{1},\cdots,\hat{U}_{n_{U}}))\leqslant \sum_{j=1}^{n_{U}}W(P(U_{j}),P(\hat{g}_{j}(Z_{j})))\leqslant O\left(L_{1}^{-2 /d_{j}^{U}}+L_{2}^{-2/d_{j}^{U}}+(\tau-\tau\log\tau)\right).\]

Let \(\hat{\mathcal{M}}\) be the casual model with the following structure equations

\[\hat{V}_{i}=\begin{cases}\hat{f}_{i}\left(\mathsf{Pa}(\hat{V}_{i}),(\hat{g}_{j }(Z_{C_{j}}))_{U_{C_{j}}\in\mathbf{U}_{V_{i}}}\right),&V_{i}\text{ is continuous},\\ \arg\max_{k\in[n_{i}]}\left\{g_{k}+\log\left(\hat{f}_{i}\left(\mathsf{Pa}( \hat{V}_{i}),(\hat{g}_{j}(Z_{C_{j}}))_{U_{C_{j}}\in\mathbf{U}_{V_{i}}}\right) \right)_{k}\right\},&V_{i}\text{ is categorical}.\end{cases}\]

By Theorem 1, we get

\[W\left(P^{\mathcal{M}^{*}}(\mathbf{V}(\mathbf{t})),P^{\hat{\mathcal{M}}} (\mathbf{V}(\mathbf{t}))\right) \leqslant O(\sum_{V_{i}\text{ continuous}}\|f_{i}-\hat{f}_{i}\|_{\infty}+\sum_{V_{i}\text{ categorical}}\|f_{i}-\hat{f}_{i}/\|\hat{f}_{i}\|_{1}\|_{\infty}\] \[\quad+W(P(U_{1},\cdots,U_{n_{U}}),P(\hat{U}_{1},\cdots,\hat{U}_{n_ {U}})))\] \[\leqslant O(L_{0}^{-2/d_{\text{max}}^{\text{in}}}+L_{1}^{-2/d_{\text{max}}^{U }}+L_{2}^{-2/d_{\text{max}}^{U}}+(\tau-\tau\log\tau)),\]

for any intervention \(\mathbf{T}=\mathbf{t}\). 

## Appendix C Proof of Consistency

### Inconsistent Counterexample (Proposition 2)

**Proposition 5**.: _There exists a constant \(c>0\) and an SCM \(\mathcal{M}^{*}\) satisfying Assumptions 1-5 with a backdoor causal graph such that there is no unobserved confounding in \(\mathcal{M}^{*}\). For any \(\epsilon>0\), there exists an SCM \(\mathcal{M}_{\epsilon}\) with the same causal graph such that \(W(P^{\mathcal{M}^{*}}(\mathbf{V}),P^{\mathcal{M}_{*}}(\mathbf{V}))\leq\epsilon\) and \(|\text{ATE}_{\mathcal{M}^{*}}-\text{ATE}_{\mathcal{M}_{*}}|>c\)._Proof.: Let \(\mathbf{V}=\{X,T,Y\}\) be the covariate, treatment and outcome respectively. \(T\) is a binary variable. Let structure equations of causal model \(\mathcal{M}_{\delta}\) be

\[X =\begin{cases}-1&\text{,w.p. }1/2\\ 0&\text{,w.p. }1/4\\ \delta&\text{,w.p. }1/4,\end{cases}\] \[P(T=1|X=x) =\begin{cases}1/2&\text{,}x=-1\text{ or }0,\\ 3/4&\text{,}x=\delta,\end{cases}\] \[Y =\begin{cases}T+U_{y}&\text{,}X<0\\ X/\delta+U_{y}&\text{,}X\geqslant 0\end{cases}\]

where all latent variables are independent and \(U_{y}\) is mean-zero noise. The distribution of \(\mathcal{M}_{\delta}\) is

\[P_{X,T,Y}(0,1,y)=p_{U_{y}}(y)/8,P_{X,T,Y}(0,0,y)=p_{U_{y}}(y)/8\] \[P_{X,T,Y}(\delta,1,y)=3p_{U_{y}}(y-1)/16,P_{X,T,Y}(\delta,0,y)=p_ {U_{y}}(y-1)/16.\] \[P_{X,T,Y}(-1,1,y)=p_{U_{y}}(y-1)/4,P_{X,T,Y}(-1,0,y)=p_{U_{y}}(y )/4.\]

As \(\delta\to 0\), distribution of \(\mathcal{M}_{\delta}\overset{d}{\rightarrow}\mathcal{M}^{*}\), where structure equations of \(\mathcal{M}^{*}\) are

\[P(U_{1}=1)=3/5,P(U_{1}=0)=2/5,P(U_{2}=1)=1/3,P(U_{1}=0)=2/3,\] \[X=\begin{cases}-1&\text{,w.p. }1/2,\\ 0&\text{,w.p. }1/2,\end{cases}\] \[P(T=1|X=x) =\begin{cases}1/2&\text{,}x=-1,\\ 5/8&\text{,}x=0,\end{cases}\] \[Y =\begin{cases}T+U_{y}&\text{,}X=-1,\\ U_{1}+U_{y}&\text{,}X=0,T=1,\\ U_{2}+U_{y}&\text{,}X=0,T=0,\end{cases}\]

where \(U_{1},U_{2},U_{y}\) are independent. The distribution of \(\mathcal{M}^{*}\) is

\[P_{X,T,Y}(0,1,y) =p_{U_{y}}(y)/8+3p_{U_{y}}(y-1)/16,\] \[P_{X,T,Y}(0,0,y) =p_{U_{y}}(y)/8+p_{U_{y}}(y-1)/16,\] \[P_{X,T,Y}(-1,1,y) =p_{U_{y}}(y-1)/4,\] \[P_{X,T,Y}(-1,0,y) =p_{U_{y}}(y)/4.\]

It is easy to see that \(\mathcal{M}^{*}\) satisfies Assumption 1-5. Some calculation gives \(W(P^{\mathcal{M}^{*}}(\mathbf{V}),P^{\mathcal{M}_{\delta}}(\mathbf{V}))\leqslant\delta/2\). It is easy to calculate the ATE of the two models.

\[\text{ATE}_{\mathcal{M}^{*}}=19/30,\text{ATE}_{\mathcal{M}_{\delta}}=1/2.\]

This example implies that even as the Wasserstein distance between \(P^{\mathcal{M}^{*}}(\mathbf{V}),P^{\mathcal{M}_{\delta}}(\mathbf{V})\) converges to zero, their ATEs do not change. As mentioned in the main body, this problem is caused by the violation of Lipschitz continuity assumption for \(\mathcal{M}_{\delta}\). Note that in \(\mathcal{M}_{\delta}\), \(\mathbb{E}[Y|X,T]=X/\delta\). As \(\delta\to 0\), the Lipschitz constant explodes.

This problem may also arise in (6). If the distribution ball \(B_{n}=\{\hat{\mathcal{M}}:W(P^{\hat{\mathcal{M}}},P^{\mathcal{M}^{*}}_{n}) \leqslant\alpha_{n}\}\) includes a small ball around the true distribution \(S_{\epsilon}=\{\hat{\mathcal{M}}:W(P^{\hat{\mathcal{M}}},P^{\mathcal{M}^{*}} )\leqslant\epsilon\}\subset B_{n}\) and the NCM is expressive enough to approximate all the SCMs in the \(S_{\epsilon}\), this example tells us the confidence interval may not shrink to one point as sample size increases to infinity even if the model is identifiable.

### Proof of Theorem 4

To prove Theorem 4, we will need the following proposition.

**Proposition 6**.: _Given a metric space \((M,d)\) and sets \(\Theta_{1}\subset\Theta_{2}\cdots\subset\Theta_{n}\subset\cdots\subset\Theta_{ \infty}\subset M\), where \(\Theta_{\infty}=\overline{\cup_{n=1}^{\infty}\Theta_{n}}\), positive sequences \(\{\epsilon_{n}\}_{n\in\mathbb{N}},\{\delta_{n}\}_{n\in\mathbb{N}}\) such that \(\lim_{n\to\infty}\epsilon_{n}=\lim_{n\to\infty}\tau_{n}=\lim_{n\to\infty} \delta_{n}=0\) and continuous functions \(f,g_{n}:\Theta_{\infty}\to\mathbb{R}\), suppose that_

1. \(\Theta_{\infty}\) _is compact._
2. \(g_{n}\) _satisfies_ \[\|g_{n}(\theta_{1})-g_{n}(\theta_{2})\|\leqslant L_{g}d(\theta_{1},\theta_{2}) +\tau_{n},\quad\forall\theta_{1},\theta_{2}\in\Theta\] (24) _and_ \(\sup_{\theta\in\Theta_{\infty}}\|g_{n}(\theta)-g(\theta)\|\leqslant\delta_{n},g_{n}\geqslant 0\)_._
3. _There exists a compact subset_ \(\tilde{\Theta}_{\infty}\subset\Theta_{\infty}\) _such that for any_ \(\theta_{0}\in\tilde{\Theta}_{\infty}\)_, there exists_ \(\theta\in\Theta_{n}\) _such that_ \(d(\theta,\theta_{0})\leqslant\epsilon_{n}\)_._

_Consider the following optimization problems:_

\[\min_{\theta\in\Theta_{n}} f(\theta),\] \[s.t. g_{n}(\theta)\leqslant L_{g}\epsilon_{n}+\delta_{n}, \tag{25}\] \[\min_{\theta\in\Theta_{\infty}} f(\theta),\] \[s.t. g(\theta)=0. \tag{26}\]

_and_

\[\min_{\theta\in\tilde{\Theta}_{\infty}} f(\theta),\] \[s.t. g(\theta)=0. \tag{27}\]

_We assume that the feasible region of (26) and (27) are nonempty. Let \(f_{n}^{*},f^{*},\tilde{f}^{*}\) be the minimal of (25), (26) and (27) respectively. Then, \([\liminf_{k\to\infty}f_{n}^{*},\limsup_{k\to\infty}f_{n}^{*}]\subset[f^{*}, \tilde{f}^{*}]\)._

Proof of Proposition 6.: By compactness of \(\Theta_{\infty}\) and \(\tilde{\Theta}_{\infty}\), the minimizers of (26) and (27) are achievable inside these two sets. Let \(\theta^{*}\in\Theta_{\infty},\tilde{\theta}^{*}\in\tilde{\Theta}_{\infty}\) be the minimizer of (26) and (27). We first prove that \(\limsup_{n\to\infty}f_{n}^{*}\leqslant\tilde{f}^{*}\). Note that by (24), the limiting function \(g\) is \(L_{g}\)-Lipschitz. By condition 3, there exist \(\theta_{n}\in\Theta_{n}\) such that \(d\left(\theta_{n},\tilde{\theta}^{*}\right)\leqslant\epsilon_{n}\). Note that

\[g_{n}(\theta_{n}) \leqslant|g(\theta_{n})-g_{n}(\theta_{n})|+|g(\theta_{n})-g\left( \theta^{*}\right)|+|g\left(\theta^{*}\right)|\] \[\leqslant\delta_{n}+L_{g}d\left(\theta_{n},\theta^{*}\right) \leqslant\delta_{n}+L_{g}\epsilon_{n}.\]

Therefore, \(\theta_{n}\) is a feasible point of (25). We have \(\limsup_{n\to\infty}f_{n}^{*}\leqslant\limsup_{n\to\infty}f(\theta_{n})=f \left(\tilde{\theta}^{*}\right)=\tilde{f}^{*}\).

Next, we argue that \(\liminf_{n\to\infty}f_{n}^{*}\geqslant f^{*}\). If this equation does not hold, there exists \(\epsilon>0\) and subsequence \(\left\{f_{n_{k}}^{*}\right\}_{k\in\mathbb{N}}\) such that \(f_{n_{k}}^{*}<f^{*}-\epsilon,\forall k\in\mathbb{N}\). By compactness of \(\Theta_{\infty}\), for each \(k\), there exists a subsequence of \(\theta_{n_{k}}^{*}\in\Theta_{\infty}\) such that \(\theta_{n_{k}}^{*}\) satisfies constraint of (25) and \(f_{n_{k}}^{*}=f\left(\theta_{n_{k}}^{*}\right)\). By compactness, \(\left\{\theta_{n_{k}}^{*}\right\}_{k\in\mathbb{N}}\) has a converging subsequence. Without loss of generality, we may assume that \(\left\{\theta_{n_{k}}^{*}\right\}_{k\in\mathbb{N}}\) converges to \(\tilde{\theta}^{*}\in\Theta_{\infty}\). Since \(g_{k}\) converge uniformly to \(g\),

\[\lim_{k\to\infty}g_{k}\left(\theta_{n_{k}}^{*}\right)\leqslant\lim_{k\to\infty} |g_{k}\left(\theta_{n_{k}}^{*}\right)-g\left(\theta_{n_{k}}^{*}\right)|+g \left(\theta_{n_{k}}^{*}\right)=g\left(\hat{\theta}^{*}\right)\leqslant\lim_{k \to\infty}\alpha_{k}=0.\]

\(\hat{\theta}^{*}\) is a feasible point of Equation (26). Since \(f\) is continuous on \(\Theta\),

\[f\left(\hat{\theta}^{*}\right)=\lim_{k\to\infty}f\left(\theta_{n_{k}}^{*} \right)\leqslant f^{*}-\epsilon.\]

which leads to contradiction.

Proof of Theorem 4.: We begin by defining a proper metric space. By Assumption 3, all random variables are bounded. Suppose that \(\max_{i,j}\{\|V_{i}\|_{\infty},\|U_{j}\|_{\infty}\}\leqslant K\). A canonical causal model \(\mathcal{M}\in\mathcal{M}(\mathcal{G},\mathcal{F},\mathbf{U})\) is decided by

\[\theta_{\mathcal{M}}=(f_{1},\cdots,f_{n_{V}},P(U_{1}),\cdots,P(U_{n_{U}})),\]

where \(f_{i}\) are functions in the structure equations (3) and \(U_{j}\) are uniform parts of the latent variables. We denote \(\mathcal{M}^{\theta}\) to be the SCM represented by \(\theta\), the underlying SCM be \(\mathcal{M}^{\theta^{*}}\) and \(P^{\theta}\) to be the distribution of \(\mathcal{M}^{\theta}\). We consider the space

\[M=\mathcal{F}_{V_{1}}\times\cdots\times\mathcal{F}_{V_{n_{V}}}\times\mathcal{P }\left([-K,K]^{d^{V}_{1}}\right)\cdots\times\mathcal{P}\left([-K,K]^{d^{V}_{ n_{U}}}\right),\]

where

\[\mathcal{F}_{V_{i}}=\begin{cases}\mathcal{F}_{L}^{K}\left([-K,K]^{d^{V}_{i,u}},[-K,K]^{d^{V}_{i,\text{out}}}\right),&\text{$V_{i}$ continuous},\\ \{f:\|f\|_{1}=1,f\in\mathcal{F}_{L}^{K}\left([-K,K]^{d^{V}_{i,u}},[-K,K]^{d^{V} _{i,\text{out}}}\right)\}&\text{$V_{i}$ categorical},\end{cases}\]

\(d^{V}_{i,\text{in}},d^{V}_{i,\text{out}}\) are the input and output dimensions of \(f_{i}\),

\[\mathcal{F}_{L}^{K}=\{f:\|f\|_{\infty}\leqslant K,f\text{ Lipschitz continuous}\}\]

and \(\mathcal{P}(K)\) is the probability space on \(K\). For \(\theta=(f_{1},\cdots,f_{n_{V}},P(U_{1}),\cdots,P(U_{n_{U}})),\theta^{\prime}= \left(f^{\prime}_{1},\cdots,f^{\prime}_{n_{V}},P(U^{\prime}_{1}),\cdots,P(U^{ \prime}_{n_{U}})\right)\), we define a metric on \(M\)

\[d(\theta,\theta^{\prime})=\sum_{k=1}^{n_{V}}\|f_{k}-f^{\prime}_{k}\|_{\infty}+ \sum_{k=1}^{n_{U}}W(P(U_{k}),P(U^{\prime}_{k})).\]

Theorem 1 states that the Wasserstein distance between two causal models is Lipschitz with respect to metric \(d\). Now, we define \(\Theta_{n}\). Let

\[\mathcal{P}_{n}=\left\{P(\mathbf{U}):\mathbf{U}\text{ is the latent distribution of }\hat{\mathcal{M}}\in\text{NCM}_{\mathcal{G}}(\mathcal{F}_{0,n},\mathcal{F}_{1,n}, \mathcal{F}_{2,n},\tau_{n})\right\}.\]

In other words, \(\mathcal{P}_{n}\) contains all the push-forward measures of the uniform distribution by neural networks. We denote \(\Theta_{n}=\hat{\mathcal{F}}_{V_{1},n}\times\cdots\times\hat{\mathcal{F}}_{V_ {n_{V}},n}\times\times\mathcal{P}_{n}\), where

\[\hat{\mathcal{F}}_{V_{i},n}=\begin{cases}\mathcal{F}_{0,n},&\text{$V_{i}$ continuous},\\ \{f/\|f\|_{1}:f\in\mathcal{F}_{0,n}\}&\text{$V_{i}$ categorical},\end{cases}\]

and \(\mathcal{F}_{0,n}\) is defined in Theorem 4. Note that by construction, latent variables are independent and \(\mathcal{P}_{n}\) can be decomposed into direct produce \(\mathcal{P}_{n,1}\times\cdots\times\mathcal{P}_{n,n_{U}}\). Let

\[\Theta_{\infty}=\overline{\cup_{n=1}^{\infty}\Theta_{n}},g_{n}(\theta)=S_{ \lambda_{n}}\left(P_{n}^{\theta^{*}}(\mathbf{V}),P_{m_{n}}^{\theta}(\mathbf{V})\right),f(\theta)=\mathbb{E}_{t\sim\mu_{T}}\mathbb{E}_{\mathcal{M}^{\theta}}[F(V_{1}(t),\cdots,V_{n_{V}}(t))]\]

and \(\tilde{\Theta}_{\infty}=\{\theta^{*}\}\). Note that \(f(\theta)\) is a continuous function since by Theorem 1 and the fact that \(F\) is Lispchitz continuous,

\[|f(\theta)-f(\theta^{\prime})| \leqslant\mathbb{E}_{t\sim\mu_{T}}|W(P^{\theta}(\mathbf{V}(t)),P^{ \theta^{\prime}}(\mathbf{V}(t))|\] \[\leqslant\mathbb{E}_{t\sim\mu_{T}}[O(d(\theta,\theta^{\prime}))]= O(d(\theta,\theta^{\prime})).\]

Now, we verify the conditions in Proposition 6.

1. By Arzela-Ascoli theorem, \(\mathcal{F}_{L}^{K}\left([-K,K]^{d^{V}_{i,\text{in}}},[-K,K]^{d^{V}_{i,\text{out}}}\right)\) are precompact set with respect to the infinity norm in space of continuous functions. And thus \(\mathcal{F}_{V_{i}}\) are compact sets. Since measures in \(\mathcal{P}\left([-K,K]^{d^{V}_{j}}\right)\) are tight, \(\mathcal{P}\left([-K,K]^{d^{V}_{j}}\right)\) are compact with respect to weak topology by Prokhorov's theorem. And the Wasserstein distance metricizes the weak topology. Thus, \(\mathcal{P}\left([-K,K]^{d^{V}_{j}}\right)\) are compact and the space \((M,d)\) is compact space. Closed set \(\Theta_{\infty}\subset M\) is also compact.

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_FAIL:30]

Therefore, we get that

\[\sup_{f\in\mathcal{F}_{L}^{K}([-K,K]^{m},[-K,K]^{m^{\prime}})}\inf_{\hat{f}\in \mathcal{N}\mathcal{N}^{\sqrt{m^{\prime}m^{\prime}}L,K}_{m,m^{\prime}}(W_{0,n}, \Theta(\log m))}\|f-\hat{f}\|_{\infty}\leqslant O\left(W_{0,n}^{-1/m}\right). \tag{32}\]

Theorem 3 implies that for each latent variable \(U_{i}\), there exist a neural network \(g_{i}\) with architecture in Corollary 1 such that

\[W(P(U_{i}),P(g_{i}(Z_{i})))\leqslant O(\sum_{i=1}^{n}L_{i,n}^{-2/d_{\max}}+ \tau_{n}(1-\log\tau_{n})). \tag{33}\]

By Assumption 3, we know \(\|U_{i}\|_{\infty}\leqslant K\), which implies

\[W(P(U_{i}),P((T_{K}g_{i})(Z_{i})))\leqslant W(P(U_{i}),P(g_{i}(Z_ {i})))\leqslant O(\sum_{i=1}^{n}L_{i,n}^{-2/d_{\max}^{U}}+\tau_{n}(1-\log\tau_ {n})). \tag{34}\]

Combining Equations (32) and (33) with the same proof as Corollary 1, it can be proven in the same way as Corollary 1 that there exists a \(\theta_{n}\in\Theta_{n}\) satisfying

\[d(\theta^{*},\theta_{n})\leqslant\epsilon_{n}=O\left(W_{0,n}^{-1/d_{\max}^{n}} +L_{1,n}^{-2/d_{\max}^{U}}+L_{2,n}^{-2/d_{\max}^{U}}+\tau_{n}(1-\log\tau_{n}) \right).\]

Let \(\tilde{\Theta}_{\infty}=\{\theta^{*}\}\), we have verified the third assumption in Proposition 6.

Take the Wasserstein radius to be \(\alpha_{n}=O(s_{n}+\epsilon_{n})\), Proposition 6 implies the conclusion. 

### Proof of Proposition 3

**Lemma 2**.: _Let \((\hat{T},\hat{Y})\sim\mu,(T,Y)\sim\nu\) and suppose that \(f(t)=\mathbb{E}_{\nu}[Y|T],\hat{f}(t)=\mathbb{E}_{\mu}[\hat{Y}|\hat{T}]\) are \(L\)-Lipschitz continuous and \(|f(t)|\leqslant K,|\hat{f}(t)|\leqslant K\), then we have_

\[\int(f(t)-\hat{f}(t))^{2}d\nu(dt)\leqslant C_{W}W(\mu,\nu),\]

_where \(C_{W}=4LK+2K\max\{L,1\}\)._

Proof of Lemma 2.: By the duality formulation of Wasserstein-1 distance, we have

\[W(\mu,\nu)=\sup_{g\in\mathsf{Lip}(1)}\mathbb{E}_{\mu}[g(\hat{T},\hat{Y})]- \mathbb{E}_{\nu}[g(T,Y)].\]

Let \(g_{0}(t,y)=(\hat{f}(t)-f(t))y\), we verify \(g_{0}\) is a Lipschitz continuous function in \(\{(t,y):\|(t,y)\|_{\infty}\leqslant K\}\).

\[|g_{0}(t_{1},y_{1})-g_{0}(t_{2},y_{2})| \leqslant|g_{0}(t_{1},y_{1})-g_{0}(t_{1},y_{2})|+|g_{0}(t_{1},y_ {2})-g_{0}(t_{2},y_{2})|\] \[=|(\hat{f}(t_{1})-f(t_{1}))(y_{1}-y_{2})|+|y_{2}(\hat{f}(t_{1})- f(t_{1})-(\hat{f}(t_{2})-f(t_{2})))|\] \[\leqslant 2K|y_{1}-y_{2}|+K(|\hat{f}(t_{1})-\hat{f}(t_{2})|+|f(t_{ 1})-f(t_{2})|)\] \[\leqslant 2K|y_{1}-y_{2}|+2KL|t_{1}-t_{2}|.\]

Let \(L_{g}=2K\max\{L,1\}\), we have proven that \(g_{0}\) is \(L_{g}\)-Lipschitz continuous in \(\{(t,y):\|(t,y)\|_{\infty}\leqslant K\}\). Thus,

\[W(\mu,\nu) \geqslant\frac{1}{L_{g}}(\mathbb{E}_{\mu}[g_{0}(\hat{T},\hat{Y})]- \mathbb{E}_{\nu}[g_{0}(T,Y)])\] \[=\frac{1}{L_{g}}(\mathbb{E}_{\mu}[(\hat{f}(\hat{T})-f(\hat{T})) \mathbb{E}[\hat{Y}|\hat{T}=t]]-\mathbb{E}_{\nu}[(\hat{f}(T)-f(T))\mathbb{E}[Y [T=t]])\] \[=\frac{1}{L_{g}}(\mathbb{E}_{\mu}[(\hat{f}(\hat{T})-f(\hat{T})) \hat{f}(\hat{T})]-\mathbb{E}_{\nu}[(\hat{f}(T)-f(T))f(T)]). \tag{35}\]Now, let \(h(t)=(\hat{f}(t)-f(t))\hat{f}(t)\). Following the same argument, it can be proven that \(h(t)\) is Lipschitz continuous with Lipschitz constant being \(L_{h}=4LK\). Hence,

\[\mathbb{E}_{\mu}[h(\hat{T})]\geqslant\mathbb{E}_{\nu}[h(T)]-L_{h}W(\mu,\nu).\]

Plug into (34), and we get

\[(L_{g}+L_{h})W(\mu,\nu) \geqslant\mathbb{E}_{\nu}[(\hat{f}(T)-f(T))\hat{f}(T)-(\hat{f}(T) -f(T))f(T)]\] \[=\mathbb{E}_{\nu}\left[(\hat{f}(T)-f(T))^{2}\right].\]

Proof of Proposition 3.: If \(Y\) is not descendant of \(T\), \(\mathbb{E}[Y(t)]=\mathbb{E}[Y]\) and we have

\[\int_{t_{1}}^{t_{2}}(\mathbb{E}_{\mathcal{M}}[Y(t)]-\mathbb{E}_{ \hat{\mathcal{M}}}[\hat{Y}(t)])^{2}dt =\int_{t_{1}}^{t_{2}}(\mathbb{E}_{\mathcal{M}}[Y]-\mathbb{E}_{ \hat{\mathcal{M}}}[\hat{Y}])^{2}dt\] \[=(t_{2}-t_{1})(\mathbb{E}_{\mathcal{M}}[Y]-\mathbb{E}_{\hat{ \mathcal{M}}}[\hat{Y}])^{2}\leqslant(t_{2}-t_{1})W(P^{\mathcal{M}}(\mathbf{V}),P^ {\hat{\mathcal{M}}}(\hat{\mathbf{V}}))^{2}.\]

Now, suppose that \(Y\) is a descendant of \(T\). Let \(X=\text{Pa}(T)\). Note that \(f_{y}(x,t)=\mathbb{E}_{\mathcal{M}}[Y|X=x,T=t]\) is \(L_{y}\)-Lipschitz continuous with Lipschitz constant \(L_{y}\leqslant(L+1)^{n_{V}}\). This is because \(Y\) can be written as \(Y=F_{0}(X,T,\mathbf{U}_{y})\) where \(\mathbf{U}_{y}\) are latent variables independent of \(T,X\) and \(F_{0}\) is composition of \(f_{i}\) in structure equations. The composition of Lipschitz functions is still Lipschitz and \(F_{0}\) is a composition of at most \(n_{V}\)\(L\)-Lipschitz functions. \(F_{0}\) is \((L+1)^{n_{V}}\)-Lipschitz and so is \(f_{y}(x,t)=\mathbb{E}_{\mathbf{U}_{y}}[F_{0}(X,T,\mathbf{U}_{y})]\). Recall that \(\nu,\mu\) are the observation distributions of \(\mathcal{M}\) and \(\hat{\mathcal{M}}\) respectively in Proposition 3. Similarly, \(\hat{f}_{y}(x,t)=\mathbb{E}_{\hat{\mathcal{M}}}[\hat{Y}|X=x,T=t]\) is \(L_{y}\)-Lipschitz continuous. By Lemma 2 and the overlap assumption, we have

\[C_{W}W(\mu,\nu) \geqslant\mathbb{E}_{\nu}\left[(f_{y}(X,T)-\hat{f}_{y}(X,T))^{2}\right]\] \[=\int(f_{y}(x,t)-\hat{f}_{y}(x,t))^{2}\nu(dt|x)\nu(dx)\] \[\geqslant\delta\int_{t_{1}}^{t_{2}}\int(f_{y}(x,t)-\hat{f}_{y}(x, t))^{2}\nu(dx)P(dt).\] \[\geqslant\delta\int_{t_{1}}^{t_{2}}\left(\int f_{y}(x,t)\nu(dx)- \int\hat{f}_{y}(x,t)\nu(dx)\right)^{2}P(dt).\]

Note that \(\hat{f}_{y}(x,t)\) is Lipschitz continuous, we have

\[\Big{|}\int\hat{f}_{y}(x,t)\nu(dx)-\int\hat{f}_{y}(x,t)\mu(dx)\Big{|} \leqslant L_{y}W(\mu,\nu).\]

Therefore,

\[\left(\int f_{y}(x,t)\nu(dx)-\int\hat{f}_{y}(x,t)\nu(dx)\right)^{2} \geqslant\frac{1}{2}\left(\int f_{y}(x,t)\nu(dx)-\int\hat{f}_{y}(x,t)\mu(dx)\right)^{2}\] \[\qquad\qquad-\left(\int\hat{f}_{y}(x,t)\mu(dx)-\int\hat{f}_{y}(x,t)\nu(dx)\right)^{2}\] \[\geqslant\frac{1}{2}\left(\int f_{y}(x,t)\nu(dx)-\int\hat{f}_{y}( x,t)\mu(dx)\right)^{2}-L_{y}^{2}W^{2}(\mu,\nu)\] \[=\frac{1}{2}(\mathbb{E}_{\mathcal{M}}[Y(t)]-\mathbb{E}_{\hat{ \mathcal{M}}}[\hat{Y}(t)])^{2}-L_{y}^{2}W^{2}(\mu,\nu).\]

Combine all the equations, we get

\[\int_{t_{1}}^{t_{2}}(\mathbb{E}_{\mathcal{M}}[Y(t)]-\mathbb{E}_{\hat{\mathcal{ M}}}[\hat{Y}(t)])^{2}P(dt)\leqslant\frac{2C_{W}}{\delta}W(\mu,\nu)+2L_{y}^{2}W^{2 }(\mu,\nu)(t_{2}-t_{1}).\]Proof of Corollary 2.: In the proof of Theorem 4, we know with probability at least \(1-O(n^{-2})\), (6) has feasible solutions and

\[W(P_{n}^{\mathcal{M}^{*}}(\mathbf{V}),P^{\mathcal{M}^{*}}(\mathbf{V}))\leqslant O(\alpha _{n}),\quad W(P_{m_{n}}^{\theta_{n}}(\mathbf{V}),P^{\theta_{n}}(\mathbf{V}))\leqslant O (\alpha_{n}),\]

where notations \(\theta\) and \(P^{\theta}\) are defined in the proof of Theorem 4 and \(\theta_{n}\) is one of the minimizers of (6). By Lemma 7, we know that

\[W(P_{n}^{\mathcal{M}^{*}}(\mathbf{V}),P_{m_{n}}^{\theta_{n}}(\mathbf{V}))\leqslant S_{ \lambda_{n}}(P_{n}^{\mathcal{M}^{*}}(\mathbf{V}),P_{m_{n}}^{\theta_{n}}(\mathbf{V}))+2 (\log(m_{n}n)+1)\lambda_{n}\leqslant O(\alpha_{n}).\]

We get that

\[W(P^{\mathcal{M}^{*}}(\mathbf{V}),P^{\theta_{n}}(\mathbf{V}))\leqslant W(P_{n}^{ \mathcal{M}^{*}}(\mathbf{V}),P^{\mathcal{M}^{*}}(\mathbf{V}))+W(P^{\mathcal{M}^{*}}( \mathbf{V}),P^{\theta_{n}}(\mathbf{V}))+W(P_{m_{n}}^{\theta_{n}}(\mathbf{V}),P^{\theta_{n} }(\mathbf{V}))\leqslant O(\alpha_{n}).\]

By Proposition 3, we get \(|F_{n}-F_{*}|\leqslant O(\sqrt{\alpha_{n}})\). 

## Appendix D Experiments

The structure equations of the generative models are (5). We use three-layer feed-forward neural networks with width 128 for each \(\hat{f}_{i}\) and six-layer neural networks with width 128 for each \(\hat{g}_{j}\). We use the Augmented Lagrangian Multiplier (ALM) method to solve the optimization problems as in [3]. We run \(600\) epochs and use a batch size of 2048 in each epoch. \(m_{n}\) is set to be \(m_{n}=n\). The "geomloss" package [16] is used to calculate the Sinkhorn distance. To impose Lipschitz regularization, we use the technique from [23] to do layer-wise normalization to the weight matrices with respect to infinity norm. The upper bound of the Lipschitz constant in each layer is set to be \(8\). The \(\tau\) in the Gumbel-softmax layer is set to be \(0\).

For the choice of Wasserstein ball radius \(\alpha_{n}\), we use the subsampling technique from [11, Section 3.4]. We can take the criterion function in [11] to be \(Q(\theta)=W(P^{\mathcal{M}^{*}}(\mathbf{V}),P^{\mathcal{M}^{\theta}}(\mathbf{V}))\) and its empirical estimation to be \(Q_{n}(\theta)=W(P_{n}^{\mathcal{M}^{*}}(\mathbf{V}),P_{n}^{\mathcal{M}^{\theta}}( \mathbf{V}))\). In the first step, we minimize the Wasserstein distance \(\hat{\theta}_{n}^{*}=\arg\min_{\theta}W(P_{n}^{\mathcal{M}^{*}}(\mathbf{V}),P_{n} ^{\mathcal{M}^{\theta}}(\mathbf{V}))\). Then, [11] propose to refine the radius \(Q_{n}(\hat{\theta}_{n}^{*})\) by taking the quantile of subsample \(\{\sup_{\theta:Q_{n}(\hat{\theta})\leqslant Q_{n}(\hat{\theta}_{n}^{*})}Q_{j, b}(\theta)\}\), where \(Q_{j,b}\) denotes the criterion function estimated at \(j\)-th subsample of size \(b\). However, it is time-consuming to solve this optimization problem many times. In practice, we set the radius \(\alpha_{n}\) to be the \(95\%\) quantile of \(\{W(\hat{P}_{j}^{\mathcal{M}^{*}})(\mathbf{V}),P_{n}^{\mathcal{M}^{\theta_{n}^{*} }}(\mathbf{V}))\), where we use 50 subsamples \(\hat{P}_{j}^{\mathcal{M}^{*}},j=1,\cdots,50\) from \(P^{\mathcal{M}^{*}_{n}(\mathbf{V})}\) with size \(b=\lfloor 15\sqrt{n}\rfloor\).

### Discrete IV [14]

We consider the noncompliance binary IV example in [14, Section D.1]. The causal graph is shown in Figure 10. We could see from Table 2 that the bound given by NCM is slightly worse than Autobounds but is still close to the optimal bound. Besides, the Autobounds bound does not cover the optimal bound in this example.

Figure 10: Instrumental Variable (IV) graph. \(Z\) is the instrumental variable, \(T\) is the treatment and \(Y\) is the outcome.

### Continuous IV

Now, we turn to the continuous IV setting, where \(T\) is still binary, but \(Y\) and \(Z\) can be continuous. Let \(E_{i}^{\lambda}\sim\lambda Z_{1}+(1-\lambda)\text{Unif}(-1,1)\), where \(Z_{1}\) is the Gaussian variable conditioning on \([-1,1]\) and the structure equations of \(\mathcal{M}^{\lambda}\) to be

\[E_{Y}=E_{1}^{\lambda}, U=E_{2}^{\lambda},Z=E_{3}^{\lambda},\] \[P(T=1|Z)=(1.5+Z+0.5U)/3,\] \[Y=3T-1.5TU+U+E_{Y},\]

where \(E_{i}^{\lambda}\) are independent. It is easy to see the ATE of this model is \(3\) regardless of \(\lambda\). In the experiment, we randomly choose ten \(\lambda\sim\text{Unif}(0,1)\). For each \(\lambda\), we run the algorithms 5 times to get the bound. We choose different latent distributions (indexed by \(\lambda\)) in the experiments to create more difficulty.

Since Autobounds can only deal with discrete variables, we discretize \(Z\) and \(Y\). Suppose that \(Z\in[l,u]\), we map all points in intervals \([l+i(u-l)/k,l+(i+1)(u-l)/k],\)\(I=0,1\cdots,k-1\) to the middle point \(l+(i+1/2)(u-l)/k.\) We choose \(k=8\) in the following experiments. The choice will give rise to polynomial programming problems with \(2^{14}\) decision variables, which is quite large.

Table 3 demonstrate the results. While both algorithms cover the true ATE well, we can see that NCM gives much tighter bounds on average. The main reason may be that the discretized problem does not approximate the original problem well enough. It is possible that a larger discretized parameter \(k\) can give a better bound, but since the size of the polynomial problem grows exponentially with \(k\), the optimization problem may be intractable for large \(k\). On the contrary, NCM does not suffer from computational difficulties.

### Counterexample

We test our neural causal method on the counterexample in Appendix C.1. We choose the noise \(U_{y}\) to be the normal variable. The structure equations are

\[P(U_{1}=1)=3/5,P(U_{1}=0)=2/5,P(U_{2}=1)=1/3,P(U_{1}=0)=2/3,\] \[X=\begin{cases}-1&\text{, w.p. }1/2,\\ 0&\text{, w.p. }1/2,\end{cases}\] \[P(T=1|X=x) =\begin{cases}1/2&\text{, }x=-1,\\ 5/8&\text{, }x=0,\end{cases}\] \[Y =\begin{cases}T+U_{y}&\text{, }X=-1,\\ U_{1}+U_{y}&\text{, }X=0,T=1,\\ U_{2}+U_{y}&\text{, }X=0,T=0,\end{cases}\]

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Algorithm & Average Bound & SD of length & Optimal Bound & True Value \\ \hline NCM (Ours) & [-0.49,0.05] & 0.05 & [-0.45, -0.04] & -0.31 \\ \hline Autobounds ([14]) & [-0.45,-0.05] & 0.02 & [-0.45, -0.04] & -0.31 \\ \hline \end{tabular}
\end{table}
Table 2: The sample size is taken to be \(5000\). We average over 10 runs with different random seeds. SD is short for the standard derivation.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Algorithm & Average Bound & SD of length & Success Rate & True Value \\ \hline NCM (Ours) & [2.49,3.24] & 0.49 & 50/50 & 3 \\ \hline Autobounds ([14]) & [1.40, 3.48] & 0.26 & 50/50 & 3 \\ \hline \end{tabular}
\end{table}
Table 3: We take a sample size of 5000. We randomly choose 10 \(\lambda\sim\text{Unif}(0,1)\) and get 10 models \(\mathcal{M}^{\lambda_{i}}\). For each model \(\mathcal{M}^{\lambda_{i}}\), we run the two algorithms for 5 times. The success rate is the number of times when the obtained bounds cover the true ATE divided by the total number of experiments. SD is short for the standard derivation.

We compare the confidence intervals of the unregularized neural casual method and the Lipschitz regularized one under different architectures. We choose the layerwise Lispchitz constants upper bound to be \(5\) and \(1.2\). As a benchmark, we also include the bound obtained by the Double Machine Learning estimator using the EconML package. The result is shown in Appendix D.3. For this identifiable case example, the double ML estimator produces better bounds for the ATE. The intervals given by regularized and unregularized NCM are similar to the regularized one slightly better in the left figure, where we use medium-sized NNs. However, in the right figure, the obtained intervals after regularization are tighter, although slightly biased, compared with the regularized setting. From these two experiments, we conclude that the architecture of NNs will also influence the results and adding regularization during the training process can prevent extreme confidence intervals or inconsistency.

## Appendix E Technical Lemmas

**Lemma 3**.: _Let \(\mu,\hat{\mu}\) be two measures on compact set \(K\subset\mathbb{R}^{d_{1}}\), for any measurable functions \(F,\hat{F}:K\to\mathbb{R}^{d_{2}}\), if \(F\) is \(L\)-Lipschitz continuous, then_

\[W(F_{\#}\mu,\hat{F}_{\#}\hat{\mu}) \leqslant LW(\mu,\hat{\mu})+\|F_{1}-F_{2}\|_{\infty}.\]

Proof.: For any \(1\)-Lipschitz function \(g:\mathbb{R}^{d_{2}}\to\mathbb{R}\),

\[\mathbb{E}_{X\sim F_{\#}\mu}[g(X)]-\mathbb{E}_{X\sim\hat{F}_{\#} \hat{\mu}}[g(X)] =\mathbb{E}_{X\sim\mu}[g\circ F(X)]-\mathbb{E}_{X\sim\hat{\mu}}[g \circ\hat{F}(X)]\] \[=(\mathbb{E}_{X\sim\mu}[g\circ F(X)]-\mathbb{E}_{X\sim\hat{\mu}}[ g\circ F(X)])\] \[\quad+(\mathbb{E}_{X\sim\hat{\mu}}[g\circ F(X)]-\mathbb{E}_{X\sim \hat{\mu}}[g\circ\hat{F}(X)]).\]

Note that for any \(x,y\in\mathbb{R}^{d_{1}}\),

\[|g\circ F(x)-g\circ F(y)| \leqslant\|F(x)-F(y)\|\leqslant L\|x-y\|.\]

Thus, \(g\circ F\) is \(L\)-Lipschitz continuous and

\[\mathbb{E}_{X\sim\mu}[g\circ F(X)]-\mathbb{E}_{X\sim\hat{\mu}}[g \circ F(X)] \leqslant LW(\mu,\hat{\mu}). \tag{35}\]

For the second term,

\[\mathbb{E}_{X\sim\hat{\mu}}[g\circ F(X)]-\mathbb{E}_{X\sim\hat{ \mu}}[g\circ\hat{F}(X)] \leqslant\mathbb{E}_{X\sim\hat{\mu}}[\|g\circ F(X)-g\circ\hat{F} (X)\|\] \[\leqslant\mathbb{E}_{X\sim\hat{\mu}}[\|F(X)-\hat{F}(X)\|]\] \[\leqslant\|F(X)-\hat{F}(X)\|_{\infty}. \tag{36}\]

Combine (35) and (36), we have the conclusion.

Figure 11: Comparison of Lipschitz regularized and unregularized neural causal algorithm. The two figures show the results of different architectures. The figure on the left side uses a medium-sized NN (width 128, depth 3) to approximate each structural function, while the right figure uses extremely small NNs (width 3, depth 1). In all experiments, we use the projected gradient to regularize the weight of the neural network. For each sample size, we repeat the experiment 5 times and take the average of the upper (lower) bound.

**Lemma 4**.: _Let \(\mu_{1},\cdots,\mu_{n},\hat{\mu}_{1},\cdots,\hat{\mu}_{n}\) be measure on \(\mathbb{R}^{d}\), for any \(p_{1},\cdots,p_{n}\in[0,1]\) such that \(\sum_{k=1}^{n}p_{k}=1\), we have_

\[W\left(\sum_{k=1}^{n}p_{k}\mu_{k},\sum_{k=1}^{n}p_{k}\hat{\mu}_{k}\right) \leqslant\sum_{k=1}^{n}p_{k}W(\mu_{k},\hat{\mu}_{k}).\]

Proof.: For any \(1\)-Lipschitz function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\), we have

\[\mathbb{E}_{X\sim\sum_{k=1}^{n}p_{k}\mu_{k}}[f(X)]-\mathbb{E}_{X \sim\sum_{k=1}^{n}p_{k}\hat{\mu}_{k}}[f(X)] =\sum_{k=1}^{n}p_{k}(\mathbb{E}_{X_{k}\sim\mu_{k}}[f(X_{k})]- \mathbb{E}_{X_{k}\sim\hat{\mu}_{k}}[f(X_{k})])\] \[\leqslant\sum_{k=1}^{n}p_{k}W(\mu_{k},\hat{\mu}_{k}).\]

By dual formulation of Wasserstein-1 distance,

\[W\left(\sum_{k=1}^{n}p_{k}\mu_{k},\sum_{k=1}^{n}p_{k}\mu_{k}\right) =\sup_{f\text{ 1-Lipschitz}}\mathbb{E}_{X\sim\sum_{k=1}^{n}p_{k} \mu_{k}}[f(X)]-\mathbb{E}_{X\sim\sum_{k=1}^{n}p_{k}\hat{\mu}_{k}}[f(X)]\] \[\leqslant\sum_{k=1}^{n}p_{k}W(\mu_{k},\hat{\mu}_{k}).\]

**Lemma 5**.: _Given two measures \(\mu=\mu_{1}\otimes\cdots\otimes\mu_{n},\hat{\mu}=\hat{\mu}_{1}\otimes\cdots \otimes\hat{\mu}_{n}\), we have_

\[W(\mu,\hat{\mu})\leqslant\sum_{k=1}^{n}W(\mu_{k},\hat{\mu}_{k}).\]

Proof.: For any \(\epsilon>0\), let \(\pi_{k}\) be a coupling of \(\mu_{k},\hat{\mu}_{k}\) such that

\[\mathbb{E}_{X_{k},Y_{k}\sim\pi_{k}}[\|X_{k}-Y_{k}\|_{1}]\leqslant W(\mu_{k}, \hat{\mu}_{k})+\epsilon.\]

Then,

\[W(\mu,\hat{\mu}) \leqslant\mathbb{E}_{\pi_{1}\otimes\cdots\otimes\pi_{n}}[\|X-Y\|]\] \[=\sum_{k=1}^{n}\mathbb{E}_{X_{k},Y_{k}\sim\pi_{k}}[\|X_{k}-Y_{k} \|_{1}]\] \[\leqslant W(\mu_{k},\hat{\mu}_{k})+\epsilon.\]

Let \(\epsilon\to 0\), we get the result. 

**Lemma 6** (Approximation error of Gumbel-Softmax layer).: _Given \(1>\tau>0\) and \(p_{1},\cdots,p_{n}>0\), let \(\mu^{\tau}\sim X^{\tau}=(X_{1}^{\tau},\cdots,X_{n}^{\tau})\) be_

\[X_{k}=\frac{\exp((\log p_{k}+G_{k})/\tau)}{\sum_{k=1}^{n}\exp((\log p_{k}+G_{k })/\tau)}, \tag{37}\]

_where \(G_{k}\sim\exp(-x-\exp(-x))\) are i.i.d. standard Gumbel variables. Let \(\mu^{0}\) be the distribution of \(X^{0}=\lim_{\tau\to 0}X^{\tau}=\left(X_{1}^{0},\cdots,X_{n}^{0}\right)\). Then,_

\[W\left(\mu^{\tau},\mu^{0}\right)\leqslant 2(n-1)\tau-2n(n-1)e^{-1}\tau\log\tau.\]

Proof.: Without loss of generality, we may assume that \(\sum_{k=1}^{n}p_{k}=1\). Otherwise, we can divide the denominator and numerator in (37) by \(\sum_{k=1}^{n}p_{k}\). Let \(\Delta_{n}=\{(x_{1},\cdots,x_{n}):\sum_{k=1}^{n}x_{k}=1\}\). We construct a transportation map \(T:\Delta_{n}\rightarrow\Delta_{n}\) as follows. \(T(x_{1},\cdots,x_{n})\) is a \(n\)-dimensional vector with all zeros except that the \(i=\arg\max_{j}x_{j}\)-th entry being one. If there are multiple coordinatesthat is maximum, we choose the first coordinate and let the remaining coordinate be zero. It is easy to verify that \(T_{\#}\mu^{\tau}\) and \(\mu^{0}\) have the same distribution [36]. For any \(\delta>0\), we have

\[P(|G_{k_{1}}+\log p_{k_{1}}-(G_{k_{2}}+\log p_{k_{2}})|<\delta) =\int_{-\infty}^{\infty}\int_{y+\log\frac{p_{k_{2}}}{p_{k_{1}}}- \delta}^{y+\log\frac{p_{k_{2}}}{p_{k_{1}}}+\delta}\exp(-x-y-\exp(-y)-\exp(-x))dxdy\] \[\leqslant 2\delta e^{-1}\int_{-\infty}^{\infty}\exp(-y-\exp(-y))dy=2\delta,\]

where we have used \(\exp(-x-\exp(-x))\leqslant\exp(-1)\). Let event \(E_{\delta}=\{\exists i,j\in[n]:|G_{i}+\log p_{i}-(G_{j}+\log p_{j})|<\delta\}\), we have

\[P(E_{\delta}) \leqslant\sum_{i\neq j}P(|G_{i}+\log p_{i}-(G_{j}+\log p_{j})|<\delta)\] \[\leqslant\delta n(n-1)e^{-1}.\]

Now, we calculate the transportation cost

\[\mathbb{E}_{X^{\tau}\sim\mu^{\tau}}\left[\|T\left(X^{\tau}\right)-X^{\tau}\|_{ 1}\right]\leqslant P(E_{\delta})\mathbb{E}_{X^{\tau}\sim\mu^{\tau}}\left[\|T \left(X^{\tau}\right)-X^{\tau}\|_{1}|E_{\delta}\right]+P\left(E_{\delta}^{c} \right)\mathbb{E}_{X^{\tau}\sim\mu^{\tau}}\left[\|T\left(X^{\tau}\right)-X^{ \tau}\|_{1}|E_{\delta}^{c}\right].\]

Note that \(\|T\left(X^{\tau}\right)-X^{\tau}\|_{1}\leqslant 2\). For the first term, we have

\[P(E_{\delta})\mathbb{E}_{X^{\tau}\sim\mu^{\tau}}\left[\|T\left(X^{\tau}\right)- X^{\tau}\|_{1}|E_{\delta}\right]\leqslant 2\delta n(n-1)e^{-1}.\]

For the second term, under event \(E_{\delta}^{c}\), let \(k_{\max}=\arg\max_{i}X_{i}\), if \(j=k_{\max}=\arg\max_{i}(\log p_{k}+G_{k})\)

\[\left|\frac{\exp((\log p_{j}+G_{j})/\tau)}{\sum_{k=1}^{n}\exp(( \log p_{k}+G_{k})/\tau)}-1\right| =\left|\frac{\sum_{k\neq j}\exp((\log p_{k}+G_{k})/\tau)}{\sum_{k =1}^{n}\exp((\log p_{k}+G_{k})/\tau)}\right|\] \[\leqslant\left|\frac{\sum_{k\neq j}\exp((\log p_{k}+G_{k})/\tau) }{\exp((\log p_{j}+G_{j})/\tau)}\right|\] \[=\sum_{k\neq j}\exp((\log p_{k}+G_{k}-(\log p_{j}+G_{j}))/\tau)\] \[\leqslant(n-1)\exp(-\delta/\tau).\]

If \(j\neq k_{\max}\), we have

\[\left|\frac{\exp((\log p_{j}+G_{j})/\tau)}{\sum_{k=1}^{n}\exp((\log p_{k}+G_{ k})/\tau)}\right|\leqslant\exp((\log p_{j}+G_{j}-(\log p_{\max}+G_{\max}))/ \tau)\leqslant\exp(-\delta/\tau).\]

Therefore,

\[\mathbb{E}_{X^{\tau}\sim\mu^{\tau}}\left[\|T\left(X^{\tau}\right)-X^{\tau}\|_ {1}|E_{\delta}^{c}\right]\leqslant 2(n-1)\exp(-\delta/\tau).\]

We get an upper bound of the transportation cost,

\[W\left(\mu^{\tau},\mu^{0}\right)\leqslant\mathbb{E}_{X^{\tau}\sim\mu^{\tau}} \left[\|T\left(X^{\tau}\right)-X^{\tau}\|_{1}\right]\leqslant 2\delta n(n-1)e^{-1} +2(n-1)\exp(-\delta/\tau).\]

Let \(\delta=-\tau\log\tau\), we get

\[W\left(\mu^{\tau},\mu^{0}\right)\leqslant 2(n-1)\tau-2n(n-1)e^{-1}\tau\log\tau.\]

**Lemma 7** (Approximation Error of Sinkhorn distance).: _For any \(\mu\in\Delta_{n},\nu\in\Delta_{m}\) and \(\lambda>0\), we have_

\[0\leqslant W_{1,\lambda}(\mu,\nu)-W(\mu,\nu)\leqslant(\log(mn)+1)\lambda,\]

_where \(W_{1,\lambda}(\cdot,\cdot)\) is the entropy regularized Wasserstein-1 distance. Moreover, we have the following estimation of approximation error._

\[|S_{\lambda}(\mu,\nu)-W(\mu,\nu)|\leqslant 2(\log(mn)+1)\lambda.\]Proof.: Let \(h(T)=-\sum_{i,j=1}^{n,m}T_{ij}\log T_{ij}+1\), by [35, Proposition 1], we have

\[0\leqslant W_{1,\lambda}(\mu,\nu)-W(\mu,\nu)\leqslant c\lambda, \tag{38}\]

where \(c=\max\left\{h(T):\text{transportation plan }T\text{ is achieve optimal loss }W(\mu,\nu)\right\}\). Since \(T_{ij}\in[0,1]\) and \(f(x)=-x\log x\) is concave, we have

\[h(T)= -nm\cdot\frac{1}{nm}\sum_{i,j=1}^{n,m}T_{ij}\log T_{ij}+1.\] \[\leqslant-nm\cdot\left(\frac{1}{nm}\sum_{i,j=1}^{n,m}T_{ij}\right) \log\left(\frac{1}{nm}\sum_{i,j=1}^{n,m}T_{ij}\right)+1\] \[=1+\log(nm).\]

By definition of Sinkhorn distance,

\[S_{\lambda}(\mu,\nu)=W_{1,\lambda}(\mu,\nu)-W_{1,\lambda}(\mu,\mu)/2-W_{1, \lambda}(\nu,\nu)/2.\]

By (38), we get

\[|S_{\lambda}(\mu,\nu)-W(\mu,\nu)|\leqslant 2(\log(mn)+1)\lambda.\]

**Lemma 8**.: _Given a measure \(\mu\) on \(\mathbb{R}^{d}\) and a real function class \(\mathcal{F}\) with output dimension \(d\), suppose that the pseudo-dimension of \(\mathcal{F}\) is less than \(d_{\mathcal{F}}<\infty\) and there exists \(K>0\) such that \(|f(x)|\leqslant K,\forall f\in\mathcal{F},x\in\mathbb{R}^{d}\), then with probability at least \(1-\exp\left(O\left(\delta^{-d}\log\left(\delta^{-1}\right)+d_{\mathcal{F}}\log \epsilon^{-1}-n\epsilon^{2}\right)\right)\),_

\[\sup_{f\in\mathcal{F}}W(f_{\#}\mu,f_{\#}\mu_{n})\leqslant\delta+\epsilon\]

_for all \(\delta,\epsilon>0\), where \(\mu_{n}\) is the empirical distribution of \(\mu\)._

Proof.: By the dual formulation of the Wasserstein distance,

\[\sup_{f\in\mathcal{F}}W(f_{\#}\mu,f_{\#}\mu_{n})=\sup_{h\in\mathcal{F}_{1}( \mathbb{R}^{d},\mathbb{R}^{d}),h(0)=0}\sup_{f\in\mathcal{F}}\mathbb{E}_{X \sim\mu_{n}}[h\circ f(X)]-\mathbb{E}_{X\sim\mu}[h\circ f(X)].\]

We define

\[\mathcal{N}_{1}(\epsilon,\mathcal{F},n)=\sup_{x_{1},\cdots,x_{n}}\mathcal{N}( \epsilon,\{(f(x_{1}),\cdots,f(x_{n}):f\in\mathcal{F})\},\|\cdot\|_{1}),\]

where \(\mathcal{N}(\epsilon,S,\|\cdot\|_{1})\) is the covering number of set \(S\) in \(\ell_{1}\) norm. Obviously, if \(h\) is \(1\)-Lipschitz,

\[\mathcal{N}_{1}(\epsilon,h\circ\mathcal{F},n)\leqslant\mathcal{N}_{1}( \epsilon,\mathcal{F},n).\]

By Theorem 18.4 in [2], for any fixed \(h\),

\[\mathcal{N}_{1}(\epsilon,h\circ\mathcal{F},n)\leqslant\mathcal{N}_{1}( \epsilon,\mathcal{F},n)\leqslant e(d_{\mathcal{F}}+1)\left(\frac{2e}{\epsilon }\right)^{d_{\mathcal{F}}}.\]

By Theorem 17.1 in [2],

\[P\left(\sup_{f\in\mathcal{F}}\mathbb{E}_{X\sim\mu_{n}}[h\circ f(X)]-\mathbb{E }_{X\sim\mu}[h\circ f(X)]>\epsilon\right)\leqslant\exp\left(O\left(d_{ \mathcal{F}}\log\epsilon^{-1}-n\epsilon^{2}\right)\right).\]

Let \(\mathcal{H}_{\delta}\) be the \(\delta\)-net of the set \(\mathcal{H}=\left\{h:h\in\mathcal{F}_{1}\left([-K,K]^{d},[-K,K]^{d}\right),h(0 )=0\right\}\)in \(\ell_{\infty}\) norm. By Lemma 6 in [21],

\[|\mathcal{H}_{\delta}|\leqslant\exp\left(O\left(\delta^{-d}\log\delta^{-1} \right)\right).\]

Therefore, with probability no more than \(\exp\left(O\left(\delta^{-d}\log\delta^{-1}+d_{\mathcal{F}}\log\epsilon^{-1}-n \epsilon^{2}\right)\right)\)

\[\sup_{h\in\mathcal{H}_{\delta},f\in\mathcal{F}}\mathbb{E}_{X\sim\mu_{n}}[h \circ f(X)]-\mathbb{E}_{X\sim\mu}[h\circ f(X)]>\epsilon.\]

Notice that

\[\sup_{h\in\mathcal{H},f\in\mathcal{F}}\mathbb{E}_{X\sim\mu_{n}}[h\circ f(X)] -\mathbb{E}_{X\sim\mu}[h\circ f(X)]\leqslant 2\delta+\sup_{h\in\mathcal{H}_{ \delta},f\in\mathcal{F}}\mathbb{E}_{X\sim\mu_{n}}[h\circ f(X)]-\mathbb{E}_{X \sim\mu}[h\circ f(X)],\]

which implies that with probability no more than \(\exp\left(O\left(\delta^{-d}\log\delta^{-1}+d_{\mathcal{F}}\log\epsilon^{-1}-n \epsilon^{2}\right)\right)\)

\[\sup_{h\in\mathcal{H},f\in\mathcal{F}}\mathbb{E}_{X\sim\mu_{n}}[h\circ f(X)] -\mathbb{E}_{X\sim\mu}[h\circ f(X)]>2\delta+\epsilon.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We accurately summarize our contributions in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We mention some limitations as future directions in the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All assumptions and detailed proof are given.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all implementation details in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: We provide all implementation details in the appendix. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all implementation details in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the confidence interval. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide all implementation details in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have reviewed the Code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theoretical work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This is a theoretical work. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This is a theoretical work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification:This is a theoretical work. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This is a theoretical work. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This is a theoretical work. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.