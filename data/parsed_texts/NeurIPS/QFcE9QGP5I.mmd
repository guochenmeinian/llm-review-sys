# Adaptive Quasi-Newton and Anderson Acceleration Framework with Explicit Global Convergence Rates

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Despite the impressive numerical performance of quasi-Newton and Anderson/nonlinear acceleration methods, their global convergence rates have remained elusive for over 50 years. This paper addresses this long-standing question by introducing a framework that derives novel and adaptive quasi-Newton or nonlinear/Anderson acceleration schemes. Under mild assumptions, the proposed iterative methods exhibit explicit, non-asymptotic convergence rates that blend those of gradient descent and Cubic Regularized Newton's method. Notably, these rates are achieved adaptively, as the method autonomously determines the optimal step size using a simple backtracking strategy. The proposed approach also includes an accelerated version that improves the convergence rate on convex functions. Numerical experiments demonstrate the efficiency of the proposed framework, even compared to a fine-tuned BFGS algorithm with line search.

## 1 Introduction

Consider the problem of finding the minimizer \(x^{\star}\) of the unconstrained minimization problem

\[f(x^{\star})=f^{\star}=\min_{x\in\mathbb{R}^{d}}f(x),\]

where \(d\) is the problem's dimension, and the function \(f\) has a Lipschitz continuous Hessian.

**Assumption 1**.: _The function \(f(x)\) has a Lipschitz continuous Hessian with a constant \(L\),_

\[\forall\;\;y,\;z\in\mathbb{R}^{d},\quad\|\nabla^{2}f(z)-\nabla^{2}f(y)\|\leq L \|z-y\|.\] (1)

In this paper, \(\|.\|\) stands for the maximal singular value of a matrix and for the \(\ell_{2}\) norm for a vector. Many twice-differentiable problems like logistic or least-squares regression satisfy Assumption 1.

The Lipschitz continuity of the Hessian is crucial when analyzing second-order algorithms, as it extends the concept of smoothness to the second order. The groundbreaking work by Nesterov et al. [45] has sparked a renewed interest in second-order methods, revealing the remarkable convergence rate improvement of Newton's method on problems satisfying Assumption 1 when augmented with cubic regularization. For instance, if the problem is also convex, accelerated gradient descent typically achieves \(O(\frac{1}{t^{2}})\), while accelerated second-order methods achieve \(O(\frac{1}{t^{3}})\). Recent advancements have further pushed the boundaries, achieving even faster convergence rates of up to \(\mathcal{O}(\frac{1}{t^{7/2}})\) through the utilization of hybrid methods [42; 14] or direct acceleration of second-order methods [43; 27; 39].

Unfortunately, second-order methods may not always be feasible, particularly in high-dimensional problems common in machine learning. The limitation is that exact second-order methods require solving a linear system that involves the Hessian of the function \(f\). This main limitation motivated alternative approaches that balance the efficiency of second-order methods and the scalability of first-order methods, such as _inexact/subspace/stochastic techniques_, _nonlinear/Anderson acceleration_, and _quasi-Newton_ methods.

### Contributions

Despite the impressive numerical performance of quasi-Newton methods and nonlinear acceleration schemes, there is currently no knowledge about their global explicit convergence rates. In fact, global convergence cannot be guaranteed without using either exact or Wolfe-line search techniques. This raises the following long-standing question **that has remained unanswered for over 50 years**:

_What are the non-asymptotic global convergence rates of quasi-Newton and Anderson/nonlinear acceleration methods?_

This paper provides a partial answer by introducing generic updates (see algorithms 1 to 3) that can be viewed as cubic-regularized quasi-Newton methods or regularized nonlinear acceleration schemes.

Under mild assumptions, the iterative methods constructed within the proposed framework (see algorithms 3 and 6) exhibit _explicit, global and non-asymptotic_ convergence rates that interpolate the one of first order and second order methods (more details in appendix A):

* Convergence rate on non-convex problems (Theorem 4): \(\min_{i}\|\nabla f(x_{i})\|\leq O(t^{-\frac{2}{3}}+t^{-\frac{1}{3}})\),
* Convergence rate on (star-)convex problems (Theorems 5 and 6): \(f(x_{t})-f^{\star}\leq O(t^{-2}+t^{-1})\),
* Accelerated rate on convex problems (Theorem 8): \(f(x_{t})-f^{\star}\leq O(t^{-3}+t^{-2})\).

### Related work

Inexact, subspace, and stochastic methods.Instead of explicitly computing the Hessian matrix and Newton's step, these methods compute an approximation using sampling [2], inexact Hessian computation [29; 19], or random subspaces [20; 31; 34]. By adopting a low-rank approximation for the Hessian, these approaches substantially reduce per-iteration costs without significantly compromising the convergence rate. The convergence speed in such cases often represents an interpolation between the rates observed in gradient descent methods and (cubic) Newton's method.

Nonlinear/Anderson acceleration.Nonlinear acceleration techniques, including Anderson acceleration [1], have a long standing history [3; 4; 28]. Driven by their promising empirical performance, they recently gained interest in their convergence analysis [61; 26; 60; 37; 66; 64; 69; 68; 53; 62; 63; 6; 63; 6; 67; 8; 54]. In essence, Anderson acceleration is an optimization technique that enhances convergence by extrapolating a sequence of iterates using a combination of previous gradients and corresponding iterates. Comprehensive reviews and analyses of these techniques can be found in notable sources such as [37; 7; 36; 35; 5; 17]. However, these methods do not generalize well outside quadratic minimization and their convergence rate can only be guaranteed asymptotically when using a line-search or regularization techniques [59; 65; 53].

Quasi-Newton methods.Quasi-Newton schemes are renowned for their exceptional efficiency in continuous optimization. These methods replace the exact Hessian matrix (or its inverse) in Newton's step with an approximation that is updated iteratively during the method's execution. The most widely used algorithms in this category include DFP [18; 25] and BFGS [58; 30; 24; 10; 9]. Most of the existing convergence results predominantly focus on the asymptotic super-linear rate of convergence [67; 32; 12; 11; 15; 22; 70; 71]. However, recent research on quasi-Newton updates has unveiled explicit and non-asymptotic rates of convergence [49; 51; 50; 40; 41]. Nonetheless, these analyses suffer from several significant drawbacks, such as assuming an infinite memory size and/or requiring access to the Hessian matrix. These limitations fundamentally undermine the essence of quasi-Newton methods, which are typically designed to be Hessian-free and maintain low per-iteration cost through their low-memory requirement and low-rank structure.

Recently, Kamzolov et al. [38] introduced an adaptive regularization technique combined with cubic regularization, with global, explicit (accelerated) convergence rates for any quasi-Newton method. The method incorporates a backtracking line search on the secant inexactness inequality that introduces a quadratic regularization. However, this algorithm relies on prior knowledge of the Lipschitz constant specified in Assumption 1. Unfortunately, the paper does not provide an adaptive method to find jointly the Lipschitz constant as well, as it is _a priory_ too costly to know which parameter to update. This aspect makes the method impractical in real-world scenarios.

Paper OrganizationSection 2 introduces the proposed novel generic updates and some essential theoretical results. Section 3 presents the convergence analysis of the iterative algorithm, which uses one of the proposed updates. Section 4 is dedicated to the accelerated version of the proposed framework. Section 5 presents examples of methods generated by the proposed framework.

## 2 Type-I and Type-II Step

This section first examines a remarkable property shared by quasi-Newton and Anderson acceleration: the sequence of iterates of these methods can be expressed as a combination of _directions_ formed by previous iterates and the current gradient. Building upon this observation, section 2.1 investigates how to obtain second-order information without directly computing the Hessian of the function \(f\) by _approximating_ the Hessian within the subspace formed by these directions. Subsequently, section 2.2 demonstrates how to utilize this approximation to establish an _upper bound_ for the function \(f\) and its gradient norm \(\|\nabla f(x)\|\). Minimizing these upper bounds, respectively, leads to a type-I and type-II method.

Motivation: what quasi-Newton and nonlinear acceleration schemes actually do?The BFGS update is a widely used quasi-Newton method for unconstrained optimization. It approximates the inverse Hessian matrix using updates based on previous gradients and iterates. The update reads

\[x_{t+1}=x_{t}-h_{t}H_{t}\nabla f(x_{t}),\ \ H_{t}=H_{t-1}\left(I-\frac{g_{t}d_{t} ^{T}}{g_{t}^{T}d_{t}}\right)+d_{t}\left(d_{t}^{T}\frac{d_{t}^{T}d_{t}+g_{t}^{T }H_{t-1}d_{t}}{(g_{t}^{T}d_{t})^{2}}-\frac{g_{t}^{T}H_{t-1}}{g_{t}^{T}d_{t}}\right)\]

where \(H_{t}\) is the approximation of the inverse Hessian at iteration \(t\), \(h_{t}\) is the step size, \(d_{t}=x_{t}-x_{t-1}\) is the step direction, \(g_{t}=\nabla f(x_{t})-\nabla f(x_{t-1})\) is the gradient difference. After unfolding the equation, the BFGS update can be seen as a combination of the \(d_{i}\)'s and \(\nabla f(x_{t})\),

\[x_{t+1}-x_{t}=H_{0}P_{0}\ldots P_{t}\nabla f(x_{t})+\sum_{i=1}^{t}\alpha_{i}d_ {i},\] (2)

where \(P_{i}\) are projection matrices in \(\mathbb{R}^{d\times d}\) and \(\alpha_{i}\) are coefficients. Similar reasoning can be applied to other quasi-Newton formulas (see appendix B for more details).

This observation aligns with the principles of Anderson acceleration methods. Considering the same vectors \(d_{t}\) and \(g_{t}\), Anderson acceleration updates \(x_{t+1}\) as:

\[\alpha^{\star}=\min_{\alpha}\|\nabla f(x_{t})+\sum_{i=0}^{t-1}\alpha_{i}r_{i} \|,\quad x_{t+1}-x_{t}=\sum_{i=0}^{t}\alpha_{i}^{\star}\left(d_{i}-h_{t}g_{i} \right),\]

where \(h_{t}\) is the relaxation parameter, which can be seen as the step size of the method. As all \(x_{i}\)'s belong to the span of previous gradients, the update is similar to (2), see appendix B for more details. This is not surprising, as it has been shown that Anderson acceleration can be viewed as a quasi-Newton method [23]. Some studies have explored the relationship between these two classes of optimization techniques and established strong connections in terms of their algorithmic behavior [23, 73, 56, 13].

Hence, quasi-Newton algorithms and nonlinear/Anderson acceleration methods utilize previous directions \(d_{i}\) and the current gradient \(\nabla f(x_{t})\) in subsequent iterations. However, their convergence is guaranteed only if a line search is used, and their convergence speed is heavily dependent on \(H_{0}\) (quasi-Newton) or \(h_{t}\) (Anderson acceleration) [48].

### Error Bounds on the Hessian-Vector Product Approximation by a Difference of Gradients

Consider the following \(d\times N\) matrices that represent the _algorithm's memory_,

\[Y=[y_{1},\ldots,y_{N}],\quad Z=[z_{1},\ldots,z_{N}],\quad D=Y-Z,\quad G=[ \ldots,\nabla f(y_{i})-\nabla f(z_{i}),\ldots].\] (3)

For example, to mimic quasi-Newton techniques, the matrices \(Y\) and \(Z\) can be defined such that,

\[D=[\ldots,x_{t-i+1}-x_{t-i},\ldots],\quad G=[\ldots,\nabla f(x_{t-i+1})- \nabla f(x_{t-i}),\ldots],\ \ i=1\ldots N.\]

Motivated by (2), this paper studies the following update, defined as a linear combination of the previous directions \(d_{i}\),

\[x_{+}-x=D\alpha\quad\text{where}\quad\alpha\in\mathbb{R}^{N}.\] (4)

The objective is to determine the optimal coefficients \(\alpha\) based on the information contained in the matrices defined in (3). Notably, the absence of the gradient in the update (4) distinguishes this approach from (2), allowing for the development of an adaptive method that eliminates the need for an initial matrix \(H_{0}\) (quasi-Newton methods) or a mixing parameter \(h_{t}\) (Anderson acceleration).

Under assumption (1), the following bounds hold for all \(x,y,z,x_{+}\in\mathbb{R}^{d}\)[45],

\[\|\nabla f(y)-\nabla f(z)-\nabla^{2}f(z)(y-z)\|\leq\tfrac{L}{2}\| y-z\|^{2},\] (5) \[\big{|}f(x_{+})-f(x)-\nabla f(x)(x_{+}-x)-\tfrac{1}{2}(x_{+}-x)^{ T}\nabla^{2}f(x)(x_{+}-x)\big{|}\leq\tfrac{L}{6}\|x_{+}-x\|^{3}.\] (6)

The accuracy of the estimation of the matrix \(\nabla^{2}f(x)\), depends on the _error vector_\(\varepsilon\),

\[\varepsilon\stackrel{{\text{def}}}{{=}}[\varepsilon_{1},\dots, \varepsilon_{N}],\quad\text{and}\quad\varepsilon_{i}\stackrel{{ \text{def}}}{{=}}\|d_{i}\|\left(\|d_{i}\|+2\|z_{i}-x\|\right).\] (7)

The following Theorem 1 explicitly bounds the error of approximating \(\nabla^{2}f(x)D\) by \(G\).

**Theorem 1**.: _Let the function \(f\) satisfy Assumption 1. Let \(x_{+}\) be defined as in (4) and the matrices \(D,\,G\) be defined as in (3) and vector \(\varepsilon\) as in (7). Then, for all \(w\in\mathbb{R}^{d}\) and \(\alpha\in\mathbb{R}^{N}\),_

\[-\tfrac{L\|w\|}{2}\sum_{i=1}^{N}|\alpha_{i}|\varepsilon_{i} \leq w^{T}(\nabla^{2}f(x)D-G)\alpha\leq\tfrac{L\|w\|}{2}\sum_{i=1} ^{N}|\alpha_{i}|\varepsilon_{i},\] (8) \[\|w^{T}(\nabla^{2}f(x)D-G)\| \leq\tfrac{L\|w\|}{2}\|\varepsilon\|.\] (9)

Proof sketch and interpretation.The theorem states that the Hessian-vector product \(\nabla^{2}f(x)(y-z)\) can be approximated by the difference of gradients \(\nabla f(y)-\nabla f(z)\), providing a cost-effective approach to estimate \(\nabla^{2}f\) without computing it. This property is the basis of quasi-Newton methods. The detailed proof can be found in appendix F. The main idea of the proof is as follows. From (5) with \(y=y_{i}\) and \(z=z_{i}\), writing \(d_{i}=y_{i}-z_{i}\), and Assumption 1,

\[\|\nabla f(y_{i})-\nabla f(z_{i})-\nabla^{2}f(x)(y_{i}-z_{i})\| \leq\frac{L}{2}\|d_{i}\|^{2}+\|\nabla^{2}f(x)-\nabla^{2}f(z)\|\|d_{i}\|\leq \frac{L}{2}\varepsilon_{i}.\]

The _first_ term in \(\varepsilon_{i}\) bounds the error of (5), while the _second_ comes from the distance between (5) and the current point \(x\) where the Hessian is estimated. Then, it suffices to combine the inequalities with coefficients \(\alpha\) to obtain Theorem 1.

### Type I and Type II Inequalities and Methods

In the literature, Type-I methods often refer to algorithms that aim to minimize the function value \(f(x)\), while type-II methods minimize the gradient norm \(\|\nabla f(x)\|\)[23; 73; 13]. Applying the bounds (6) and (5) to the update in (4) yields the following Type-I and Type-II upper bounds, respectively.

**Theorem 2**.: _Let the function \(f\) satisfy Assumption 1. Let \(x_{+}\) be defined as in (4), the matrices \(D,\,G\) be defined as in (3) and \(\varepsilon\) be defined as in (7). Then, for all \(\alpha\in\mathbb{R}^{N}\),_

\[f(x_{+})\leq f(x)+\nabla f(x)^{T}D\alpha+\tfrac{\alpha^{T}H\alpha }{2}+\tfrac{L\|D\alpha\|^{3}}{6},\quad H\stackrel{{\text{def}}}{{= }}\tfrac{G^{T}D+D^{T}G+\text{IL}\|D\|\|\varepsilon\|}{2}\] (10) \[\|\nabla f(x_{+})\|\leq\|\nabla f(x)+G\alpha\|+\tfrac{L}{2}\Big{(} \sum_{i=1}^{N}|\alpha_{i}|\varepsilon_{i}+\|D\alpha\|^{2}\Big{)},\] (11)

The proof can be found in appendix F. Minimizing eqs. (10) and (11) leads to algorithms 1 and 2, respectively, whose constant \(L\) is replaced by a parameter \(M\), found by backtracking line-search. A study of the (strong) link between these proposed algorithms and nonlinear/Anderson acceleration and quasi-Newton methods can be found in appendix B.

Solving the sub-problemsIn algorithms 1 and 2, the coefficients \(\alpha\) are computed by solving a minimization sub-problem in \(O(N^{3}+Nd)\) (see appendix C for more details). Usually, \(N\) is rather small (e.g. between \(5\) and \(100\)); hence solving the subproblem is negligible compared to computing a new gradient \(\nabla f(x)\). Here is the summary:

* **In algorithm 1**, the subproblem can be solved easily by a convex problem in two variables, which involves an eigenvalue decomposition of the matrix \(H\in\mathbb{R}^{N\times N}\)[45].
* **In algorithm 2**, the subproblem can be cast into a linear-quadratic problem of \(O(N)\) variables and constraints that can be solved efficiently with SDP solvers (e.g., SDPT3).

```
0: First-order oracle for \(f\), matrices \(G,\,D\), vector \(\varepsilon\), iterate \(x\), initial smoothness \(M_{0}\).
1: Initialize \(M\leftarrow\frac{M_{0}}{2}\)
2:do
3:\(M\gets 2M\) and \(H\leftarrow\frac{G^{T}D+D^{T}G}{2}+\mathrm{I}_{N}\frac{M\|D\|\|\varepsilon\|}{2}\)
4:\(\alpha^{\star}\leftarrow\arg\min_{\alpha}f(x)+\nabla f(x)^{T}D\alpha+\frac{1} {2}\alpha^{T}H\alpha+\frac{M\|D\alpha\|^{3}}{6}\)
5:\(x_{+}\gets x+D\alpha\)
6:while\(f(x_{+})\geq f(x)+\nabla f(x)^{T}D\alpha^{\star}+\frac{1}{2}[\alpha^{\star}]^{T}H \alpha^{\star}+\frac{M\|D\alpha^{\star}\|^{3}}{6}\)
7:return\(x_{+}\), \(M\) ```

**Algorithm 1** Type-I Subroutine with Backtracking Line-search

## 3 Iterative Type-I Method: Framework and Rates of Convergences

The rest of the paper analyzes the convergence rate of methods that use algorithm 1 as a subroutine; see algorithm 3. The analysis of methods that uses algorithm 2 is left for future work.

### Main Assumptions and Design Requirements

This section lists the important assumptions on the function \(f\). Some subsequent results require an upper bound on the radius of the sub-level set of \(f\) at \(f(x_{0})\).

**Assumption 2**.: _The radius of the sub-level set \(\{x:f(x)\leq f(x_{0})\}\) is bounded by \(\mathrm{R}<\infty\)._

To ensure the convergence toward \(f(x^{\star})\), some results require \(f\) to be star-convex or convex.

**Assumption 3**.: _The function \(f\) is star convex if, for all \(x\in\mathbb{R}^{d}\) and \(\forall\tau\in[0,1]\),_

\[f((1-\tau)x+\tau x^{\star})\leq(1-\tau)f(x)+\tau f(x^{\star}).\]

**Assumption 4**.: _The function \(f\) is convex if, for all \(y,\,z\in\mathbb{R}^{d}\), \(f(y)\geq f(z)+\nabla f(z)(y-z)\)._

The matrices \(Y,\,Z,\,D\) must meet some conditions listed below as "requirements" (see section 5 for details). All convergence results rely on _one_ of these conditions on the projector onto \(\mathbf{span}(D)\),

\[P_{t}\stackrel{{\text{def}}}{{=}}D_{t}(D_{t}^{T}D_{t})^{-1}D_{t}^ {T}.\] (12)

**Requirement 1a**.: _For all \(t\), the projector \(P_{t}\) of the stochastic matrix \(D_{t}\) satisfies \(\mathbb{E}[P_{t}]=\frac{N}{d}\textbf{I}\)._

**Requirement 1b**.: _For all \(t\), the projector \(P_{t}\) satisfies \(P_{t}\nabla f(x_{t})=\nabla f(x_{t})\)._

The first condition guarantees that, in expectation, the matrix \(D_{t}\) spans partially the gradient \(\nabla f(x_{t})\), since \(\mathbb{E}[P_{t}\nabla f(x_{t})]=\frac{N}{d}\nabla f(x_{t})\). The second condition simply requires the possibility to move towards the current gradient when taking the step \(x+D\alpha\). This condition resonates with the idea presented in (2), where the step \(x_{+}-x\) combines previous directions and the current gradient \(\nabla f(x_{t})\).

In addition, it is required that the norm of \(\|\varepsilon\|\) does not grow too quickly, hence the next assumption.

**Requirement 2**.: _For all \(t\), the relative error \(\frac{\|\varepsilon_{t}\|}{\|D_{t}\|}\) is bounded by \(\delta\)._

The Requirement 2 is also non-restrictive, as it simply prevents taking secant equations at \(y_{i}-z_{i}\) and \(z_{i}-x_{i}\) too far apart. Most of the time, \(\delta\) satisfies \(\delta\leq O(R)\).

Finally, the condition number of the matrix \(D\) also has to be bounded.

**Requirement 3**.: _For all \(t\), the matrix \(D_{t}\) is full-column rank, which implies that \(D_{t}^{T}D_{t}\) is invertible. In addition, its condition number \(\kappa_{D_{t}}\stackrel{{\text{def}}}{{=}}\sqrt{\|D_{t}^{T}D_{t} \|\|(D_{t}^{T}D_{t})^{-1}\|}\) is bounded by \(\kappa\)._

The condition on the rank of \(D\) is not overly restrictive. In most practical scenarios, this condition is typically satisfied without issue. However, the second condition might be hard to meet, but section 5 studies strategies that prevent \(\kappa_{D}\) from exploding by taking orthogonal directions or pruning \(D\).

### Rates of Convergence

When \(f\) satisfies Assumption 1, algorithm 3 ensures a minimal function decrease at each step.

**Theorem 3**.: _Let \(f\) satisfy Assumption 1. Then, at each iteration \(t\geq 0\), algorithm 3 achieves_

\[f(x_{t+1})\leq f(x_{t})-\tfrac{M_{t+1}}{12}\|x_{t+1}-x_{t}\|^{3},\quad M_{t+1}< \max\left\{2L\ ;\ \tfrac{M_{0}}{2^{\Gamma}}\right\}.\] (13)

Under some mild assumptions, algorithm 3 converges to a critical point for non-convex functions.

**Theorem 4**.: _Let \(f\) satisfy Assumption 1, and assume that \(f\) is bounded below by \(f^{*}\). Let Requirements 1b to 3 hold, and \(M_{t}\geq M_{\min}\). Then, algorithm 3 starting at \(x_{0}\) with \(M_{0}\) achieves_

\[\min_{i=1,\,\dots,\,t}\|\nabla f(x_{i})\|\leq\max\left\{\frac{3L}{t^{2/3}} \left(12\frac{f(x_{0})-f^{\star}}{M_{\min}}\right)^{2/3}\ ;\ \left(\frac{C_{1}}{t^{1/3}}\right)\left(12\frac{f(x_{0})-f^{ \star}}{M_{\min}}\right)^{1/3}\right\},\]

_where \(C_{1}=\delta L\left(\frac{\kappa+2\kappa^{2}}{2}\right)+\max_{i\in[0,t]}\|(I- P_{i})\nabla^{2}f(x_{i})P_{i}\|\)._

Going further, algorithm 3 converges to an optimum when the function is star-convex.

**Theorem 5**.: _Assume \(f\) satisfy Assumptions 1 to 3. Let Requirements 1b to 3 hold. Then, algorithm 3 starting at \(x_{0}\) with \(M_{0}\) achieves, for \(t\geq 1\),_

\[(f(x_{t})-f^{\star})\leq 6\frac{f(x_{t})-f^{\star}}{t(t+1)(t+2)}+\frac{1}{(t +1)(t+2)}\frac{L(3R)^{3}}{2}+\frac{1}{t+2}\frac{C_{2}(3R)^{2}}{4},\]

_where \(C_{2}\overset{\text{def}}{=}\delta L^{\frac{\kappa+2\kappa^{2}}{2}}+\max_{i \in[0,t]}\|\nabla^{2}f(x_{i})-P_{i}\nabla^{2}f(x_{i})P_{i}\|\)._

Finally, the next theorem shows that when algorithm 3 uses a stochastic \(D\) that satisfies Requirement 1a, then \(f(x_{t})\) also converges in expectation to \(f(x^{\star})\) when \(f\) is convex.

**Theorem 6**.: _Assume \(f\) satisfy Assumptions 1, 2 and 4. Let Requirements 1a, 2 and 3 hold. Then, in expectation over the matrices \(D_{i}\), algorithm 3 starting at \(x_{0}\) with \(M_{0}\) achieves, for \(t\geq 1\),_

\[\mathbb{E}_{D_{t}}[f(x_{t})-f^{\star}]\leq\frac{1}{1+\frac{1}{4}\left[\frac{N }{d}t\right]^{3}}(f(x_{0})-f^{\star})+\frac{1}{\left[\frac{N}{d}t\right]^{2}} \frac{L(3R)^{3}}{2}+\frac{1}{\left[\frac{N}{d}t\right]}\frac{C_{3}(3R)^{2}}{2},\]

_where \(C_{3}\overset{\text{def}}{=}\delta L^{\frac{\kappa+2\kappa^{2}}{2}}+\frac{( d-N)}{d}\max_{i\in[0,t]}\|\nabla^{2}f(x_{i})\|\)._

InterpretationThe rates presented in Theorems 4 to 6 combine the ones of cubic regularized Newton's method and gradient descent (or coordinate descent, as in Theorem 6) for functions with Lipschitz-continuous Hessian. As \(C_{1},C_{2}\), and \(C_{3}\) decrease, the rates approach those of cubic Newton.

The constants \(C_{1}\), \(C_{2}\), and \(C_{3}\) quantify the error of approximating \(D\nabla^{2}f(x)D\) by \(H\) in (10) into two terms. The first represents the error made by approximating \(\nabla^{2}f(x)D\) by \(G\), while the second describes the low-rank approximation of \(\nabla^{2}f(x)\) in the subspace spanned by the columns of \(D\). The approximation is more explicit in \(C_{3}\), where increasing \(N\) reduces the constant up to \(N=d\).

To retrieve the convergence rate of Newton's method with cubic regularization, the approximation needs to satisfy three properties: **1)** the points contained in \(Y_{t}\) and \(Z_{t}\) must be close to each other, and to \(x_{t}\) to reduce \(\delta\) and \(\|\varepsilon\|\); **2)** the condition number of \(D\) should be close to 1 to reduce \(\kappa\); **3)**\(D\) should span a maximum dimension in \(\mathbb{R}^{d}\) to improve the approximation of \(\nabla^{2}f(x)\) by \(P\nabla^{2}f(x)P\).

For example, \(Z_{t}=x_{t}\textbf{1}_{N}^{T}\), \(D_{t}=h\mathrm{I}_{N}\) with \(h\) small, and \(Y_{t}=Z_{t}+D_{t}\) achieve these conditions. This (naive) strategy estimates all directional second derivatives with a finite difference for all coordinates and is equivalent to performing a Newton's step in terms of complexity.

``` First-order oracle \(f\), matrices \(G,\,D\), vector \(\varepsilon\), iterate \(x\), smoothness \(M_{0}\), minimal norm \(\Delta\)  Initialize \(M\leftarrow\frac{M_{0}}{2}\), \(\gamma+\frac{1}{4}\frac{\|\varepsilon\|}{\|D\|}\left(1+\kappa_{D}^{2}\right)\), \(\texttt{ExitFlag}\leftarrow\texttt{False}\) whileExitFlag is False do  Update \(M\) and \(H\leftarrow\frac{G^{T}D+D^{T}}{2}+\text{I}_{N}\frac{M\|D\|\|\varepsilon\|}{2}\) \(\alpha^{*}\leftarrow\arg\min_{\alpha}f(x)+\nabla f(x)^{T}D\alpha+\frac{1}{2} \alpha^{T}H\alpha+\frac{M\|D\alpha\|^{3}}{6}\) \(x_{+}\gets x+D\alpha\) If\(-\nabla f(x_{+})^{T}D\alpha\geq\frac{\|\nabla f(x_{+})\|^{3/2}}{\sqrt{3M}}\) and \(\|D\alpha\|\geq\Delta\)thenExitFlag\(\leftarrow\)LargeStep If\(-f(x_{+})^{T}D\alpha\geq\frac{\|\nabla f(x_{+})\|^{2}}{M\left(\gamma+\frac{\|D \alpha\|}{2}\right)}\)thenExitFlag\(\leftarrow\)SmallStep endwhile return\(x_{+}\), \(\alpha\), \(M\), \(\gamma\),ExitFlag ```

**Algorithm 4** Type-I subroutine with backtracking for the accelerated method

## 4 Accelerated Algorithm for Convex Functions

This section introduces algorithm 5, an accelerated variant of algorithm 3 for convex functions, designed using the estimate sequence technique from [43]. It consists in iteratively building a function \(\Phi_{t}(x)\), a regularized lower bound on \(f\), that reads

\[\Phi_{t}(x)=\frac{1}{\sum_{i=0}^{\lfloor\frac{1}{2}\cdot b_{i}}}\left(\sum_{i= 0}^{t}b_{i}\left(f(x_{i})+\nabla f(x_{i})(x-x_{i})\right)+\lambda_{t}^{(1)} \frac{\|x-x_{0}\|^{2}}{2}+\lambda_{t}^{(2)}\frac{\|x-x_{0}\|^{3}}{6}\right),\]

where \(\lambda_{t}^{(1,2)}\) are non-decreasing. The key aspects of acceleration are as follows (see section 4 for more details): **1)** The accelerated algorithm makes a step at a linear combination between \(v_{t}\), the optimum of \(\Phi_{t}\), and the previous iterate \(x_{t}\). **2)** It uses a modified version of algorithm 1, see algorithm 4. **3)** Under some conditions, the step size can be considered as "large", i.e., similar to a cubic-Newton step. The \(\Delta>0\) ensures the step is sufficiently large to ensure theoretical convergence - but setting \(\Delta=0\) does not seem to impact the numerical convergence. The presence of both small and large steps is crucial to obtain the theoretical rate of convergence.

``` First-order oracle \(f\), initial iterate and smoothness \(x_{0},\,M_{0}\), number of iterations \(T\).  Initialize \(G_{0},\,D_{0},\,\varepsilon_{0},\,\lambda_{0}^{(1)},\,\lambda_{0}^{(2)},\,\Delta\), \(x_{1},\,M_{1},(M_{0})_{1}\). for\(t=1,\,\ldots,\,T-1\)do  Update \(G_{t},\,D_{t},\,\varepsilon_{t}\).  do  Compute \(v_{t}\leftarrow\arg\min\Phi_{t}\), set \(y_{t}=\frac{t}{t+3}x_{t}+\frac{3}{t+3}v_{t}\), and update \((M_{0})_{t}\) \(\{x_{t+1},\texttt{ExitFlag}\}\leftarrow\texttt{[algorithm\ 4]}(f,G_{t},D_{t}, \varepsilon_{t},y_{t},(M_{0})_{t},\Delta)\) if\(\Phi_{t+1}(v_{t+1})\leq f(x_{t+1})\)then%% Parameters adjustment if needed  ValidBound\(\leftarrow\)False ifExitFlag isSmallStepthen\(\lambda_{t}^{(1)}\gets 2\lambda_{t}^{(1)}\), otherwise\(\lambda_{t}^{(2)}\gets 2\lambda_{t}^{(2)}\) else  ValidBound\(\leftarrow\)True%% Successful iteration endif whileValidBound isFalse endfor return\(x_{T}\) ```

**Algorithm 5** Adaptive Accelerated Type-I Algorithm (Sketch, see appendix D for the full version)

**Theorem 7**.: _Assume \(f\) satisfy Assumptions 1, 2 and 4. Let Requirements 1b to 3 hold. Then, algorithm 5 starting at \(x_{0}\) with \(M_{0}\) achieves, for all \(\Delta>0\) and for \(t\geq 1\),_

\[f(x_{t})-f^{\star}\leq\frac{(M_{0})_{\max}^{2}}{L}\left(\frac{3R}{t+3}\right)^{2 }+\frac{4(M_{0})_{\max}}{3\sqrt{3}}\max\left\{1\;;\;\frac{2}{\Delta}\right\} \left(\frac{3R}{t+3}\right)^{3}+\frac{\frac{\tilde{\lambda}^{(1)}R^{2}}{2}+ \frac{\tilde{\lambda}^{(2)}R^{3}}{6}}{(t+1)^{3}}.\]

_where \(\tilde{\lambda}^{(1)}=0.5\cdot\delta\left(L\kappa+M_{1}\kappa^{2}\right)+\| \nabla f(x_{0})-P_{0}\nabla f(x_{0})P_{0}\|,\qquad\tilde{\lambda}^{(2)}=M_{1}+L\),_

\[(M_{0})_{\max}=\frac{L}{2}(2\Delta+(2\kappa^{2}+\kappa)\delta)+(2\sqrt{3}-1)\max _{0\leq i\leq t}\|(I-P_{i})\nabla^{2}f(x_{i})P_{i}\|.\]

[MISSING_PAGE_FAIL:8]

Random Subspace:Inspired by [34], this technique randomly generates \(D_{t}\) at each iteration by either taking \(D_{t}\) to be \(N\) random (rescaled) canonical vectors or by using the \(Q\) matrix from the QR decomposition of a random \(N\times D\) matrix. This ensures that \(D_{t}\) satisfies Requirement 1a. For clarity, in the experiments, only the QR version is considered.

## 6 Numerical Experiments

This section compares the methods generated by this paper's framework to the fine-tuned \(\ell\)-BFGS algorithm from minFunc[52]. More experiments are conducted in appendix E. The tested methods are the Type-I iterative algorithms (algorithm 3 with the techniques from section 5). The step size of the forward estimation was set to \(h=10^{-9}\), and the condition number \(\kappa_{D_{t}}\) is maintained below \(\kappa=10^{9}\) with the iterates only and Greedy techniques. The accelerated algorithm 6 is used only with the _Forward Estimates Only_ technique. The compared methods are evaluated on a logistic regression problem with no regularization on the Madelon UCI dataset [33]. The results are shown in fig. 1.

Regarding the number of iterations, the greedy orthogonalized version outperforms the others due to the orthogonality of directions (resulting in a condition number of one) and the meaningfulness of previous gradients/iterates. However, in terms of gradient oracle calls, the recommended method, _orthogonal forward iterates only_, achieves the best performance by striking a balance between the cost per iteration (only two gradients per iteration) and efficiency (small and orthogonal directions, reducing theoretical constants). Surprisingly, the accelerated method's performance is suboptimal, possibly because it tightens the theoretical analysis, diminishing its inherent adaptivity.

## 7 Conclusion, Limitation, and Future work

This paper introduces a generic framework for developing novel quasi-Newton and Anderson/Nonlinear acceleration schemes, offering a global convergence rate in various scenarios, including accelerated convergence on convex functions, with minimal assumptions and design requirements.

One limitation of the current approach is requiring an additional gradient step for the _forward estimate_, as discussed in Section 5. However, this forward estimate is crucial in enabling the algorithm's adaptivity, eliminating the need to initialize a matrix \(H_{0}\) (quasi-Newton) or employ a mixing parameter \(h_{0}\) (Anderson acceleration).

In future research, although unsuitable for large-scale problems, the method presented in this paper can achieve super-linear convergence rates, as with infinite memory, they would be as fast as cubic Newton methods. Utilizing the average-case analysis framework from existing literature, such as [47, 55, 21, 16, 46], could also improve the constants in Theorems 4 and 5 to match those in Theorem 6. Furthermore, exploring convergence rates for type-2 methods, which are believed to be effective for variational inequalities, is a worthwhile direction.

Ultimately, the results presented in this paper open new avenues for researchs. It may also provide a potential foundation for investigating additional properties of existing quasi-Newton methods and may even lead to the discovery of convergence rates for an adaptive, cubic-regularized BFGS variant.

Figure 1: Comparison between the type-1 methods proposed in this paper and the optimized implementation of \(\ell\)-BFGS from minFunc[52] with default parameters, except for the memory size. All methods use a memory size of \(N=25\).

## References

* [1] Donald G Anderson. "Iterative procedures for nonlinear integral equations". In: _Journal of the ACM (JACM)_ 12.4 (1965), pp. 547-560.
* [2] Kimon Antonakopoulos, Ali Kavis, and Volkan Cevher. "Extra-Newton: A First Approach to Noise-Adaptive Accelerated Second-Order Methods". In: _arXiv preprint arXiv:2211.01832_ (2022).
* [3] Claude Brezinski. "Application de l'\(\varepsilon\)-algorithme a la resolution des systemes non lineaires". In: _Comptes Rendus de l'Academie des Sciences de Paris_ 271.A (1970), pp. 1174-1177.
* [4] Claude Brezinski. "Sur un algorithme de resolution des systemes non lineaires". In: _Comptes Rendus de l'Academie des Sciences de Paris_ 272.A (1971), pp. 145-148.
* [5] Claude Brezinski and Michela Redivo-Zaglia. "The genesis and early developments of Aitken's process, Shanks' transformation, the \(\varepsilon\)-algorithm, and related fixed point methods". In: _Numerical Algorithms_ 80.1 (2019), pp. 11-133.
* [6] Claude Brezinski, Michela Redivo-Zaglia, and Yousef Saad. "Shanks sequence transformations and Anderson acceleration". In: _SIAM Review_ 60.3 (2018), pp. 646-669.
* [7] Claude Brezinski and M Redivo Zaglia. _Extrapolation methods: theory and practice_. Elsevier, 1991.
* [8] Claude Brezinski et al. "Shanks and Anderson-type acceleration techniques for systems of nonlinear equations". In: _arXiv:2007.05716_ (2020).
* [9] Charles G Broyden. "The convergence of a class of double-rank minimization algorithms: 2. The new algorithm". In: _IMA journal of applied mathematics_ 6.3 (1970), pp. 222-231.
* [10] Charles George Broyden. "The convergence of a class of double-rank minimization algorithms 1. general considerations". In: _IMA Journal of Applied Mathematics_ 6.1 (1970), pp. 76-90.
* [11] Richard H Byrd and Jorge Nocedal. "A tool for the analysis of quasi-Newton methods with application to unconstrained minimization". In: _SIAM Journal on Numerical Analysis_ 26.3 (1989), pp. 727-739.
* [12] Richard H Byrd, Jorge Nocedal, and Ya-Xiang Yuan. "Global convergence of a cass of quasi-Newton methods on convex problems". In: _SIAM Journal on Numerical Analysis_ 24.5 (1987), pp. 1171-1190.
* [13] Marco Canini and Peter Richtarik. "Direct nonlinear acceleration". In: _Operational Research_ 2192 (2022), p. 4406.
* [14] Yair Carmon et al. "Recapp: Crafting a more efficient catalyst for convex optimization". In: _International Conference on Machine Learning_. PMLR. 2022, pp. 2658-2685.
* [15] Andrew R Conn, Nicholas IM Gould, and Ph L Toint. "Convergence of quasi-Newton matrices generated by the symmetric rank one update". In: _Mathematical programming_ 50.1-3 (1991), pp. 177-195.
* [16] Leonardo Cunha et al. "Only tails matter: Average-Case Universality and Robustness in the Convex Regime". In: 2022.
* [17] Alexandre d'Aspremont, Damien Scieur, Adrien Taylor, et al. "Acceleration methods". In: _Foundations and Trends(r) in Optimization_ 5.1-2 (2021), pp. 1-245.
* [18] William C Davidon. "Variable metric method for minimization". In: _SIAM Journal on Optimization_ 1.1 (1991), pp. 1-17.
* [19] Nikita Doikov, El Mahdi Chayti, and Martin Jaggi. "Second-order optimization with lazy Hessians". In: _arXiv preprint arXiv:2212.00781_ (2022).
* [20] Nikita Doikov, Peter Richtarik, et al. "Randomized block cubic Newton method". In: _International Conference on Machine Learning_. PMLR. 2018, pp. 1290-1298.
* [21] Carles Domingo-Enrich, Fabian Pedregosa, and Damien Scieur. "Average-case acceleration for bilinear games and normal matrices". In: _arXiv preprint arXiv:2010.02076_ (2020).
* [22] John R Engels and Hector J Martinez. "Local and superlinear convergence for partially known quasi-Newton methods". In: _SIAM Journal on Optimization_ 1.1 (1991), pp. 42-56.

* [23] Haw-Ren Fang and Yousef Saad. "Two classes of multisecant methods for nonlinear acceleration". In: _Numerical Linear Algebra with Applications_ 16.3 (2009), pp. 197-221.
* [24] Roger Fletcher. "A new approach to variable metric algorithms". In: _The computer journal_ 13.3 (1970), pp. 317-322.
* [25] Roger Fletcher and Michael JD Powell. "A rapidly convergent descent method for minimization". In: _The computer journal_ 6.2 (1963), pp. 163-168.
* [26] William F Ford and Avram Sidi. "Recursive algorithms for vector extrapolation methods". In: _Applied numerical mathematics_ 4.6 (1988), pp. 477-489.
* [27] Alexander Gasnikov et al. "Near optimal methods for minimizing convex functions with lipschitz \(p\)-th derivatives". In: _Conference on Learning Theory_. PMLR. 2019, pp. 1392-1393.
* [28] Eckart Gekeler. "On the solution of systems of equations by the epsilon algorithm of Wynn". In: _Mathematics of Computation_ 26.118 (1972), pp. 427-436.
* [29] Saeed Ghadimi, Han Liu, and Tong Zhang. "Second-order methods with cubic regularization under inexact information". In: _arXiv preprint arXiv:1710.05782_ (2017).
* [30] Donald Goldfarb. "A family of variable-metric methods derived by variational means". In: _Mathematics of computation_ 24.109 (1970), pp. 23-26.
* [31] Robert Gower et al. "Rsn: Randomized subspace newton". In: _Advances in Neural Information Processing Systems_ 32 (2019).
* [32] Andreas Griewank and Ph L Toint. "Local convergence analysis for partitioned quasi-Newton updates". In: _Numerische Mathematik_ 39.3 (1982), pp. 429-448.
* [33] Isabelle Guyon. "Design of experiments of the NIPS 2003 variable selection benchmark". In: _NIPS 2003 workshop on feature extraction and feature selection_. Vol. 253. 2003.
* [34] Filip Hanzely et al. "Stochastic subspace cubic Newton method". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 4027-4038.
* [35] K Jbilou and H Sadok. "Vector extrapolation methods. Applications and numerical comparison". In: _Journal of Computational and Applied Mathematics_ 122.1-2 (2000), pp. 149-165.
* [36] Khalide Jbilou and Hassane Sadok. "Analysis of some vector extrapolation methods for solving systems of linear equations". In: _Numerische Mathematik_ 70.1 (1995), pp. 73-89.
* [37] Khalide Jbilou and Hassane Sadok. "Some results about vector extrapolation methods and related fixed-point iterations". In: _Journal of Computational and Applied Mathematics_ 36.3 (1991), pp. 385-398.
* [38] Dmitry Kamzolov et al. "Accelerated Adaptive Cubic Regularized Quasi-Newton Methods". In: _arXiv preprint arXiv:2302.04987_ (2023).
* [39] Dmitry Kovalev and Alexander Gasnikov. "The first optimal acceleration of high-order methods in smooth convex optimization". In: _arXiv preprint arXiv:2205.09647_ (2022).
* [40] Dachao Lin, Haishan Ye, and Zhihua Zhang. "Explicit convergence rates of greedy and random quasi-Newton methods". In: _Journal of Machine Learning Research_ 23.162 (2022), pp. 1-40.
* [41] Dachao Lin, Haishan Ye, and Zhihua Zhang. "Greedy and random quasi-newton methods with faster explicit superlinear convergence". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 6646-6657.
* [42] Renato DC Monteiro and Benar Fux Svaiter. "An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods". In: _SIAM Journal on Optimization_ 23.2 (2013), pp. 1092-1125.
* [43] Yurii Nesterov. "Accelerating the cubic regularization of Newton's method on convex problems". In: _Mathematical Programming_ 112.1 (2008), pp. 159-181.
* [44] Yurii Nesterov. _Introductory lectures on convex optimization_. Springer, 2004.
* [45] Yurii Nesterov and Boris T Polyak. "Cubic regularization of Newton method and its global performance". In: _Mathematical Programming_ 108.1 (2006), pp. 177-205.
* [46] Courtney Paquette et al. "Halting Time is predictable for large models: A universality property and average-case analysis". In: _Foundations of Computational Mathematics_ (2022).

* Pedregosa and Scieur [2020] Fabian Pedregosa and Damien Scieur. "Acceleration through spectral density estimation". In: _Proceedings of the 37th International Conference on Machine Learning (ICML)_. 2020.
* Powell [1986] MJD Powell. "How bad are the BFGS and DFP methods when the objective function is quadratic?" In: _Mathematical Programming_ 34 (1986), pp. 34-47.
* Rodomanov and Nesterov [2021] Anton Rodomanov and Yurii Nesterov. "Greedy quasi-Newton methods with explicit superlinear convergence". In: _SIAM Journal on Optimization_ 31.1 (2021), pp. 785-811.
* Rodomanov and Nesterov [2021] Anton Rodomanov and Yurii Nesterov. "New results on superlinear convergence of classical quasi-Newton methods". In: _Journal of optimization theory and applications_ 188 (2021), pp. 744-769.
* Rodomanov and Nesterov [2021] Anton Rodomanov and Yurii Nesterov. "Rates of superlinear convergence for classical quasi-Newton methods". In: _Mathematical Programming_ (2021), pp. 1-32.
* Schmidt [2005] Mark Schmidt. "minFunc: unconstrained differentiable multivariate optimization in Matlab". In: _Software available at http://www. cs. ubc. ca/~ schmidtn/Software/minFunc. htm_ (2005).
* Scieur et al. [2016] Damien Scieur, Alexandre d'Aspremont, and Francis Bach. "Regularized nonlinear acceleration". In: _Advances in Neural Information Processing Systems (NIPS)_. 2016.
* Scieur et al. [2020] Damien Scieur, Alexandre d'Aspremont, and Francis Bach. "Regularized nonlinear acceleration". In: _Mathematical Programming_ (2020).
* Scieur and Pedregosa [2020] Damien Scieur and Fabian Pedregosa. "Universal Asymptotic Optimality of Polyak Momentum". In: _Proceedings of the 37th International Conference on Machine Learning (ICML)_. 2020.
* Scieur et al. [2021] Damien Scieur et al. "Generalization of Quasi-Newton methods: application to robust symmetric multisecant updates". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2021, pp. 550-558.
* Scieur et al. [2018] Damien Scieur et al. "Online Regularized Nonlinear Acceleration". In: _arXiv:1805.09639_ (2018).
* Shanno [1970] David F Shanno. "Conditioning of quasi-Newton methods for function minimization". In: _Mathematics of computation_ 24.111 (1970), pp. 647-656.
* Sidi [1986] Avram Sidi. "Convergence and stability properties of minimal polynomial and reduced rank extrapolation algorithms". In: _SIAM Journal on Numerical Analysis_ 23.1 (1986), pp. 197-209.
* Sidi [1991] Avram Sidi. "Efficient implementation of minimal polynomial and reduced rank extrapolation methods". In: _Journal of Computational and Applied Mathematics_ 36.3 (1991), pp. 305-337.
* Sidi [1988] Avram Sidi. "Extrapolation vs. projection methods for linear systems of equations". In: _Journal of Computational and Applied Mathematics_ 22.1 (1988), pp. 71-88.
* Sidi [2017] Avram Sidi. "Minimal polynomial and reduced rank extrapolation methods are related". In: _Advances in Computational Mathematics_ 43.1 (2017), pp. 151-170.
* Sidi [2017] Avram Sidi. _Vector extrapolation methods with applications_. SIAM, 2017.
* Sidi [2008] Avram Sidi. "Vector extrapolation methods with applications to solution of large systems of equations and to PageRank computations". In: _Computers & Mathematics with Applications_ 56.1 (2008), pp. 1-24.
* Sidi and Bridger [1988] Avram Sidi and Jacob Bridger. "Convergence and stability analyses for some vector extrapolation methods in the presence of defective iteration matrices". In: _Journal of Computational and Applied Mathematics_ 22.1 (1988), pp. 35-61.
* Sidi and Shapira [1998] Avram Sidi and Yair Shapira. "Upper bounds for convergence rates of acceleration methods with initial iterations". In: _Numerical Algorithms_ 18.2 (1998), pp. 113-132.
* Stachurski [1981] Andrzej Stachurski. "Superlinear convergence of Broyden's bounded \(\theta\)-class of methods". In: _Mathematical Programming_ 20.1 (1981), pp. 196-212.
* Toth and Kelley [2015] Alex Toth and CT Kelley. "Convergence analysis for Anderson acceleration". In: _SIAM Journal on Numerical Analysis_ 53.2 (2015), pp. 805-819.
* Walker and Ni [2011] Homer F Walker and Peng Ni. "Anderson acceleration for fixed-point iterations". In: _SIAM Journal on Numerical Analysis_ 49.4 (2011), pp. 1715-1735.

* [70] Zengxin Wei et al. "The superlinear convergence of a modified BFGS-type method for unconstrained optimization". In: _Computational optimization and applications_ 29 (2004), pp. 315-332.
* [71] Hiroshi Yabe, Hideho Ogasawara, and Masayuki Yoshino. "Local and superlinear convergence of quasi-Newton methods based on modified secant conditions". In: _Journal of Computational and Applied Mathematics_ 205.1 (2007), pp. 617-632.
* [72] Hiroshi Yabe and Naokazu Yamaki. "Local and superlinear convergence of structured quasi-Newton methods for nonlinear optimization". In: _Journal of the Operations Research Society of Japan_ 39.4 (1996), pp. 541-557.
* [73] Junzi Zhang, Brendan O'Donoghue, and Stephen Boyd. "Globally convergent type-I Anderson acceleration for nonsmooth fixed-point iterations". In: _SIAM Journal on Optimization_ 30.4 (2020), pp. 3170-3197.