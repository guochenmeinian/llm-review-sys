Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation

 Nikki Lijing Kuang

University of California, San Diego

likuang@ucsd.edu

&Ming Yin

Princeton University

my0049@princeton.edu

&Mengdi Wang

Princeton University

mengdiw@princeton.edu

&Yu-Xiang Wang

University of California, Santa Barbara

yuxiangw@cs.ucsb.edu

&Yi-An Ma

University of California, San Diego

yianma@ucsd.edu

Equal contribution.

###### Abstract

Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce _Delayed-PSVI_, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves \(\widetilde{O}(\sqrt{d^{3}H^{3}T}+d^{2}H^{2}\mathbb{E}[\tau])\) worst-case regret in the presence of unknown stochastic delays. Here \(\mathbb{E}[\tau]\) is the expected delay. To further improve its computational efficiency and to expand its applicability in high-dimensional RL problems, we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for _Delayed-LPSVI_, which maintains the same order-optimal regret guarantee with \(\widetilde{O}(dHK)\) computational cost. Empirical evaluations are performed to demonstrate the statistical and computational efficacy of our algorithms.

## 1 Introduction

Reinforcement Learning (RL) is the main workhorse for sequential decision-making problems where an agent needs to balance the trade-off between exploitation and exploration in the unknown environment. The flexible and powerful function approximation endowed by deep neural networks greatly contributes to the empirical success of RL in domains such as Large Language Models (LLMs) [50; 59], robotics [51], and AI for Science [37]. In general, collecting real-world training data from such practical systems can be expensive, which requires algorithms to be both sample efficient and computationally efficient. Recently, there have been growing efforts towards studying provably efficient RL algorithms in settings ranging from tabular Markov Decision Processes (MDPs) [29; 45; 69] to large-scale RL with function approximation [13; 35]. However, these algorithms typically rely on the availability of immediate observations of states, actions and rewards in learning no-regret policies. Unfortunately, such an assumption is rarely satisfied in real-world domains, where delayed feedback is ubiquitous and fundamental. In recommender systems and online advertisement, for instance, responses from users (e.g. click, purchase) may not be immediately observable, which can take hours or days. In healthcare and clinical trials, medical feedback from patients on the effectiveness of treatments can only be determined at a deferred time frame. More examples exist in platforms that involve human interaction and evaluation, including human-robot collaboration in teleoperating systems and multi-agent systems [15; 39], aligning LLMs with human values [50; 63], and fine-tuning generative AI models using RL with human feedback (RLHF) [11; 41].

Despite the practical importance of addressing delays in decision-making problems, theoretical understanding of delayed feedback in RL remains limited. Recent parallel works study exploration under delayed feedback via upper confidence bound (UCB) algorithms [8] in tabular RL [29; 45], adversarial MDPs [36; 40], and RL with low policy-switching scheme [68] (see Table 1). Nevertheless, posterior sampling (PS) analysis that handles delayed feedback remains untackled in both bandit and RL literature. We aim to bridge the gap in this work.

PS is a randomized Bayesian algorithm that extends Thompson sampling (TS) [57] to RL, which selects an action according to its posterior probability of being the best. This philosophy inspires a number of promising exploration strategies that explicitly or implicitly adopt PS to explore [52], including bootstrapped DQN [42; 47] and RLSVI [49]. Compared to the popular UCB algorithms, it bears greater robustness in the presence of delays [14], and provides exceptional computational efficiency with competitive empirical performance [14; 65]. The fact that posteriors are often intractable in practice necessitates the use of approximate Bayesian inference such as ensemble sampling, variational inference (VI) and Markov Chain Monte Carlo (MCMC) [20; 38; 47].

In this paper, we provide the first analysis for the class of PS algorithms that handles delayed feedback in RL frameworks, in which the trajectory information is randomly delayed according to some unknown distribution. We highlight that delayed feedback model imposes new challenges that do not arise in standard RL settings. Algorithmically, it requires the computation of new posterior variance due to the weaker concentration arising from delays. Theoretically, it complicates the frequentist analysis of PS algorithms in several ways: (a) the lack of timely update in posterior learning can cause distribution shift, especially in the case of approximate sampling; (b) delays need to be carefully disentangled to quantify the penalty in regret decomposition and it prohibits the direct application of previous analysis; (c) balance between concentration and anti-concentration needs to be handled deliberately to achieve sub-linear regret.

To tackle these challenges, we introduce two novel value-based algorithms for _linear MDPs_ under unknown stochastic delayed feedback. Developed upon Bayesian linear modeling with a multi-round ensembling mechanism (\(M\approx\text{Polylog}(H,K,d,\delta)\) round), our algorithms achieve a sub-linear worst-case regret without requiring the knowledge of delay, thereby addressing the question raised in [60] that "No frequentist analysis exists for posterior sampling with delayed feedback". Empirical studies show that our algorithms outperform UCB-based methods in terms of both statistical accuracy and computational efficiency when delays are well-behaved or even long-tailed. We summarize our main contributions as follows.

* We propose the _Delayed Posterior Sampling Value Iteration_ (Delayed-PSVI, Algorithm 1) for linear MDPs. It achieves a high-probability worst-case regret of \(\widetilde{O}(\sqrt{d^{3}H^{3}T}+d^{2}H^{2}\mathbb{E}[\tau])\)2, where \(\mathbb{E}[\tau]\) is the expected delay. Footnote 2: It provides a stronger guarantee as opposed to the weaker worst-case expected regret and Bayesian regret.
* We leverage _Langevin Monte Carlo (LMC)_ for approximate inference and introduce _Delayed Langevin Posterior Sampling Value Iteration_ (Delayed-LPSVI, Algorithm 2), which maintains the same order-optimal worst-case regret of \(\widetilde{O}(\sqrt{d^{3}H^{3}T}+d^{2}H^{2}\mathbb{E}[\tau])\). To the best of our knowledge, this is the first analysis that provably incorporates LMC in linear MDPs and jointly considers the impact of delays.
* Both algorithms achieve the optimal dependence on the parameters \(d\) and \(T\) in leading terms under the class of PS algorithms, and recover the best-available frequentist regret of \(\widetilde{O}(\sqrt{d^{3}H^{3}T})\)[31; 72] as in non-delayed linear MDPs when \(\mathbb{E}[\tau]=0\). In particular, Delayed-LPSVI reduces the computational complexity of Delayed-PSVI from \(\widetilde{O}(d^{3}HK)\)to \(\widetilde{O}(dHK)\), expanding the applicability in complex high-dimensional RL tasks while potentially providing a more flexible form of approximation.

### Related Work.

**Delayed feedback.** In bandit literature, delay is extensively studied in both stochastic [22; 56; 60; 77] and adversarial settings [32; 58; 78] for UCB-based methods. In comparison, while delay draws much attention in empirical RL studies [12; 17; 18], there is a lack of theoretical understanding until very recently. Parallel works focus on UCB-based methods in various RL settings [16; 28; 36; 40; 45; 68]. To provide the first analysis for PS algorithms in this context, we consider stochastic delays under linear function approximation without requiring any policy-switch scheme as in [68].

**Posterior sampling.** To encourage efficient exploration, PS is adopted in value-based methods to inject randomness in empirical Bellman update via Gaussian noise. From the Bayesian perspective, it is equivalent to maintaining an approximate Gaussian posterior for parameterized value function. Its sample complexity is studied in tabular settings [48; 49; 53], with the sharp worst-case regret of \(\widetilde{O}(H^{2}S\sqrt{AT})\)[5]. Under linear function approximation, frequentist regret of \(\widetilde{O}(\sqrt{d^{3}H^{3}T})\)[31; 72] and Bayesian regret of \(\widetilde{O}(d\sqrt{H^{3}T})\)[19] are established. However, in complex problem domains that require higher computational efficiency and more refined surrogates, approximate inference is the remedy. Toward this end, we resort to a gradient-based MCMC method.

**Langevin Monte Carlo.** LMC is a class of MCMC methods tailored for large-scale online learning with strong convergence guarantee by utilizing the first-order gradient information [64]. It has been successfully applied to stochastic bandits [43], linear bandits [65] and tabular RL [38], In this work, we extend its usage in linear MDPs and demonstrate its convergent property under delay.

**RL with Function Approximation.** Function approximation is widely adopted to empower RL for large-scale applications. Fruitful results have been established for regret minimization in two types of MDPs under linear function approximation: linear mixture MDPs [9; 67], and linear MDPs [35; 66]. In linear mixture MDPs where transition kernel is parameterized as a linear combination of base models, provably efficient algorithms are discussed [13; 75; 76] and [75] provides the corresponding lower bound of \(\Omega(dH\sqrt{T})\). In contrast, linear MDPs enjoy a linear structure in value functions by assuming a low-rank representation for both transitions and reward function, where algorithms are shown to enjoy polynomial sample complexity [27; 35; 62; 73]. When it comes to general function approximation, theoretical guarantees are developed based on measures of eluder dimension [54; 61] and Bellman rank [33]. In this work, we focus on delayed feedback in linear MDPs.

## 2 Preliminaries

We study the finite-horizon episodic MDP \((\mathcal{S},\mathcal{A},H,\mathbb{P},r)\), which is time-inhomogeneous, and denote by \(\mathcal{S}\), \(\mathcal{A}\) the state and action spaces respectively, \(H\) the episode length, \(\mathbb{P}=\left\{\mathbb{P}_{h}\right\}_{h=1}^{H}\) the

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**Algorithms** & **Setting** & **Exploration** & **Worst-case regret** & **Computation** \\ \hline
[28] & Linear Bandits & UCB & \(\widetilde{O}(d\sqrt{T}+d^{3/2}\mathbb{E}[\tau])\) & Confidence set optimization \\
[29] & Tabular MDPs & UCB & \(\widetilde{O}(\sqrt{SAH^{3}T}+S^{2}AH^{2}\mathbb{E}[\tau])\) & Active update \\
[68] & Linear MDPs & UCB & \(\widetilde{O}(\sqrt{A^{3}H^{3}T}+dH^{2}\mathbb{E}[\tau])\) & Multi-batch reduction \\
[40] & Adversarial MDPs & UCB & \(\widetilde{O}(H^{2}S\sqrt{AK}+H^{3/2}\sqrt{S}\sum_{k=1}^{K}\tau_{h})\) & Confidence set optimization \\ \hline Delayed-PSVI (Thm 1) & Linear MDPs & PS & \(\widetilde{O}(\sqrt{d^{3}H^{3}T}+d^{4}H^{2}\mathbb{E}[\tau])\) & \(O((d^{3}+M)dHK)\) \\ Delayed-LPSVI (Thm 2) & Linear MDPs & PS & \(\widetilde{O}(\sqrt{d^{3}H^{3}T}+d^{4}H^{2}\mathbb{E}[\tau])\) & \(O((N+d)MHK)\) \\ Delayed-PSLB (Cor 2) & Linear Bandits & PS & \(\widetilde{O}(\sqrt{d^{3}T}+d^{2}\mathbb{E}[\tau])\) & \(O((N+d)MK)\) \\ \hline UCB Lower bound [27] & Linear MDPs & UCB & \(\Omega(dH\sqrt{T})\) & — \\ PS Lower bound [24] & Linear Bandits & PS & \(\Omega(\sqrt{d^{3}T})\) & — \\ \hline \end{tabular}
\end{table}
Table 1: Summary of regret bounds in linear bandits and episodic MDPs under stochastic delay. We denote by \(T\) the time horizon, \(K\) the number of episodes, \(H\) the episode length, \(d\) the dimension of feature space, \(M\) the number of sampling rounds, and \(N\) the total iterations in running LMC. Our choice of \(M\) and \(N\) has order of Polylog(\(H,K,d,\delta\)), ensuring both Delayed-PSVI and Delayed-LPSVI are computationally efficient and statistically sample-efficient. We remark that the gap in the frequentist regret between PS and best UCB-based methods is unavoidable by a factor of \(\sqrt{d}\)[24]. Thus, our dependencies on \(d\) and \(T\) are optimal for the class of PS algorithms. Our results fulfill the caveat [60] that no worst-case analysis exists for PS with delay.

transition dynamics, and \(r=\{r_{h}\}_{h=1}^{H}\) reward function. At each step \(h\in[H]\), \(\mathbb{P}_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta_{\mathcal{S}}\) specifies the probabilities of transitioning from the current state-action pair into the next state, and \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) emits a bounded reward. We adopt the prior protocol of linear MDPs as follows.

**Definition 1** (Linear MDPs [35, 66]).: _Suppose there exists a known feature map \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) that encodes each state-action pair into a \(d\)-dimensional feature vector. An MDP is a linear MDP3 if for any time step \(h\in[H],\;\;\forall(s,a)\in\mathcal{S}\times\mathcal{A}\), both the transition dynamics \(\mathbb{P}\) and reward function \(r\) are linear in \(\phi\):_

Footnote 3: Linear MDPs recover tabular MDPs by taking \(d=|\mathcal{S}||\mathcal{A}|\), where feature map is a one-one mapping for each state-action pair.

\[\mathbb{P}_{h}(\cdot|s,a)=\phi(s,a)^{\mathrm{T}}\mu_{h}(\cdot), \qquad r_{h}(s,a)=\phi(s,a)^{\mathrm{T}}\theta_{h},\] (1)

_where \(\mu_{h}:\mathcal{S}\rightarrow\mathbb{R}^{d}\) contains \(d\) unknown probability measures over \(\mathcal{S}\), and \(\theta_{h}\in\mathbb{R}^{d}\). Furthermore, we assume that \(\forall(s,a)\in\mathcal{S}\times\mathcal{A},\|\phi(s,a)\|\leq 1\), and \(\forall h\in[H],\|\theta_{h}\|\leq\sqrt{d}\), \(\left\|\int_{\mathcal{S}}\mathrm{d}\mu_{h}(s^{\prime})\right\|\leq\sqrt{d}\), where \(\left\|\cdot\right\|\) denotes the Euclidean norm._

A non-stationary policy \(\pi=\{\pi_{h}\}_{h=1}^{H}\) assigns the action to take at step \(h\) in state \(s_{h}\in\mathcal{S}\). Accordingly, we define the value functions of a policy \(\pi\) as the expected rewards received under \(\pi\):

\[Q_{h}^{\pi}(s,a)=\mathbb{E}_{\pi}\left[\sum\nolimits_{h^{\prime}=h }^{H}r_{h^{\prime}}|s_{h}=s,a_{h}=a\right],\quad V_{h}^{\pi}(s)=\mathbb{E}_{ \pi}\left[\sum\nolimits_{h^{\prime}=h}^{H}r_{h^{\prime}}|s_{h}=s\right].\]

We further denote by \(\pi^{*}\) the optimal policy whose value functions are defined as \(V_{h}^{*}(s):=V_{h}^{\pi^{*}}(s)=\sup_{\pi_{h}}V_{h}^{\pi}(s)\) and \(Q_{h}^{*}(s,a):=Q_{h}^{\pi^{*}}(s,a)=\sup_{\pi}Q_{h}^{\pi}(s,a)\). Under Definition 1, the action-value functions are always linear in the feature map, and there exists some \(w_{h}^{*}\) such that \(Q_{h}^{*}=\phi^{\mathrm{T}}w_{h}^{*}\) (Lemma A.1). For ease of notation, \(\forall(s,a)\), denote \([\mathbb{P}_{h}V_{h+1}^{\pi}](s,a)=\mathbb{E}_{s^{\prime}\sim\mathbb{P}_{h}( \cdot|s,a)}[V(s^{\prime})]\). By Bellman equation and Bellman optimality equation,

\[Q_{h}^{\pi}(s,a)=(r_{h}+\mathbb{P}_{h}V_{h+1}^{\pi})(s,a), V_{h}^{\pi}(s)=Q_{h}^{\pi}(s,\pi_{h}(s)),\] \[Q_{h}^{*}(s,a)=(r_{h}+\mathbb{P}_{h}V_{h+1}^{*})(s,a), V_{h}^{*}(s)=\max_{a}(r_{h}+\mathbb{P}_{h}V_{h+1}^{*})(s,a).\]

The goal of the agent is to maximize the cumulative episodic rewards or equivalently, minimize the regret that quantifies the difference between the value of the optimal policy \(\pi^{*}\) and that of the executed policies. Formally, the _worse-case regret_ over \(K\) episodes is given as:

\[R(T)=\sum_{k=1}^{K}V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi_{k}}(s_{1}^{k}).\] (2)

**Remark 1**.: _Different types of regret are used in literature to measure the performance of PS algorithms. Bayesian regret \(\mathbb{E}_{w^{*}\sim p_{0}(\cdot)}\mathbb{E}[R(T)|w^{*}]\) is often considered when assuming a prior \(p_{0}(w)\) over the true parameter \(w^{*}\). Frequentist regret \(\mathbb{E}[R(T)]\) is considered when \(w^{*}\) is fixed, where the expectation is taken over all the randomness over data and algorithm. As explained in Appendix A.2, the worst-case regret that we study is stronger than the frequentist regret._

### Delayed Feedback Model

In this work, we consider stochastic delays across episodes. More specifically, the trajectory (i.e., sequence of states, actions and rewards) generated in each episode is not immediately observable in the presence of delay. The formal definition is given as follows.

**Definition 2** (Episodic Delayed Feedback).: _In each episode \(k\in[K]\), the execution of a fixed policy \(\pi^{k}\) generates a trajectory \(\{s_{h}^{k},a_{h}^{k},r_{h}^{k},\)\(s_{h+1}^{k}\}_{h\in[H]}\). Such trajectory information is called the feedback of episode \(k\). Let \(\tau_{k}\) represent the random delay between the rollout completion of episode \(k\) and the time point at which its feedback becomes observable._

**Remark 2**.: _Various types of delays have been independently studied in the literature, including delays in states [4, 12, 16], delays in rewards [25, 45, 60], delays in actions[56], and delays in trajectories [29, 68]. We focus on the last scheme which facilitates the delayed analysis of value-based methods in episodic linear MDPs._Episodic delays do not disrupt the policy rollout within an episode, but alter the utilization of information in subsequent episodes. More precisely, the feedback of episode \(k\) remains inaccessible for the following \(\tau_{k}-1\) episodes, becoming observable only at the onset of the (\(k+\tau_{k}\))-th episode. To track whether the feedback generated at episode \(k\) is revealed at episode \(k^{\prime}\), we utilize the indicator \(\mathds{1}_{k,k^{\prime}}:=\mathds{1}\{k+\tau_{k}\leq k^{\prime}\}\) (where \(1\) denotes "yes" and \(0\) denotes "no"). We follow the standard assumption in literature in [28; 68] to assume delays are sub-exponential. It is crucial to note that this assumption primarily serves the purpose of theoretical analysis and is not a prerequisite for the effective functioning of our algorithms in practical settings. Without loss of generality, we discuss the performance bound under general random delays in Section 4 and empirically study the performance against different types of delays in Section 5.

**Assumption 1** (Sub-exponential Episodic Delay).: _The episodic delays \(\{\tau_{k}\}_{k=1}^{K}\) are non-negative, integer-valued, independent and identically distributed \((v,b)\)-subexponential random variables: \(\tau_{k}\overset{i.i.d.}{\sim}f_{\tau}(\cdot)\) with \(f_{\tau}(\cdot)\) being the probability mass function, and \(\mathbb{E}[\tau]\) being the expected value. For all \(k\in[K]\), the moment generating function of \(\tau_{k}\) satisfies:_

\[\mathbb{E}\left[\exp\left(\gamma\left(\tau_{k}-\mathbb{E}\left[\tau\right] \right)\right)\right]\leq\exp\left(\frac{1}{2}v^{2}\gamma^{2}\right),\]

_where \(v\) and \(b\) are non-negative, and \(|\gamma|\leq 1/b\)._

## 3 Delayed Posterior Sampling Value Iteration

In this section, we introduce a novel optimistic value-based algorithm, namely, _Delayed Posterior Sampling Value Iteration_ (Delayed-PSVI), which efficiently explores the value function space in linear MDPs by embracing several critical components: posterior sampling that injects random noise when performing the least-square value iteration, optimism via multi-round sampling to achieve the optimal worst-case regret and delayed feedback model that encodes episodic trajectory delays.

``` Input: priors \(p_{0}(w_{h}^{k})\leftarrow\mathcal{N}(0,\lambda I)\), scaling factor \(\nu\), multi-round paramter \(M\), hyper parameters \(\lambda\) and \(\sigma^{2}\).
1Initialization:\(\forall k,h,\widetilde{Q}_{H+1}^{k}(\cdot,\cdot),\widetilde{V}_{H+1}(\cdot, \cdot),\widetilde{V}_{h}(\cdot,\cdot)\gets 0\), \(\mathcal{D}_{h}\leftarrow\emptyset\).
2forepisode \(k=1,\ldots,K\)do
3 Sample initial state \(s_{1}^{k}\) for time step \(h=H,\ldots,1\)do
4\(y_{h}\leftarrow[y_{1}^{1},\ldots,y_{h}^{k-1}]\), with \(y_{h}^{\tau}\leftarrow\mathds{1}_{\pi,k-1}\cdot[r_{h}^{\tau}+\widetilde{V}_{h+ 1}(s_{h+1}^{\tau})]\)
5\(\Phi_{h}\leftarrow[\phi_{1}^{\top},\phi^{2},\ldots,\phi^{k-1}]\) with \(\phi^{\tau}=\mathds{1}_{\pi,k-1}\cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\)
6\(\Omega_{k}^{k}\leftarrow\sigma^{-2}\Phi_{h}\Phi_{h}^{T}+\lambda I\), \(\widetilde{w}_{h}^{k}\leftarrow\sigma^{-2}(\Omega_{h}^{k})^{-1}\Phi_{h}y_{h}\)\(\overline{\bm{n}}^{T}\)
7\(p(w_{h}^{k}\mid\mathcal{D}_{h},\bm{y}_{h})\leftarrow\mathcal{N}(\widetilde{w}_{h}^{k}, \nu^{2}\cdot(\Omega_{h}^{k})^{-1})\)
8for m = 1,...,Mdo
9 Sample \(\widetilde{w}_{h}^{k,m}\sim p(w_{h}^{k}\mid\mathcal{D}_{h},\bm{y}_{h})\)
10\(\widetilde{Q}_{h}^{k,m}(\cdot,\cdot)\leftarrow\phi(\cdot,\cdot)^{\mathrm{T}} \widetilde{w}_{h}^{k,m}\)
11 Update \(\widetilde{Q}_{h}^{k}(\cdot,\cdot)\leftarrow\max_{m}\widetilde{Q}_{h}^{k,m}\)
12\(\widetilde{V}_{h}(\cdot,\cdot)\leftarrow\max_{a}\min\{\widetilde{Q}_{h}^{k}( \cdot,a),H-h+1\}\)
13 Update \(\pi_{h}^{k}(\cdot)\leftarrow\operatorname*{argmax}_{a\in\mathcal{A}}\min\{ \widetilde{Q}_{h}^{k}(\cdot,a),H-h+1\}\)
14for time step \(h=1,\ldots,H\)do
15 Choose action \(a_{h}^{k}=\pi_{h}^{k}(s_{h}^{k})\)
16 Collect trajectory observations \(\mathcal{D}_{h}\leftarrow\mathcal{D}_{h}\cup\{(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_ {h+1}^{k})\}\)
17 /* Feedback generated in episode \(k\) cannot be immediately observed in the presence of delay */ ```

**Algorithm 1**Delayed Posterior Sampling Value Iteration (Delayed-PSVI)

**Noisy value iteration via posterior sampling.** At the beginning of each episode, we apply PS to sample an estimated value function from the posterior, which is maintained using the observed feedback \(\mathcal{D}\) over the previous episodes. Specifically, at each time step, the \(Q\)-function is parameterized by some \(w\in\Gamma\) such that \(\widetilde{Q}(s,a)=\phi(s,a)^{\mathrm{T}}w\) is an approximation of the corresponding true optimal \(Q\)-function \(Q^{*}(s,a)\). Let \(p_{0}(w)\) be the prior of \(w\), and \(p(\bm{y}|w,\mathcal{D})\) be the likelihood of the observation \(\bm{y}\), then the posterior of \(w\) satisfies:

\[p(w|\mathcal{D},\bm{y})\propto\exp(-L(w,\bm{y},\mathcal{D}))p_{0}(w),\]where \(L(\cdot)\) is the log-likelihood. Unlike the case of model-based RL (MBRL), where PS is utilized to maintain an exact posterior over the environment model, we aim to adopt PS to perform noisy value-iteration by injecting randomness for efficient exploration of the value function space. Specifically, at each step \(h\in[H]\), we consider Gaussian-noise perturbation in Delayed-PSVI by setting prior as \(p_{0}(w_{h})=\mathcal{N}(0,\lambda I_{d})\), and log-likelihood (with \(\mathcal{D}_{h}=\{s_{h}^{\tau},a_{h}^{\tau},r_{h}^{\tau},s_{h+1}^{\tau}\}_{h\in [H]}^{\tau\in[k-1]}\)) as

\[L(w_{h},\bm{y}_{h},\mathcal{D}_{h})=\sum\nolimits_{\tau=1}^{k-1}(\phi(s_{h}^{ \tau},a_{h}^{\tau})^{\mathrm{T}}w_{h}-y_{h}^{\tau})^{2},\] (3)

where \(\bm{y}_{h}=[y_{h}^{1},\ldots,y_{h}^{k-1}]\) with \(y_{h}^{\tau}=r_{h}^{\tau}(s_{h}^{\tau},a_{h}^{\tau})+\widetilde{V}_{h+1}(s_{h +1}^{\tau})\). Then for all step \(h\in[H]\) of episode \(k\), the posterior of \(w_{h}^{k}\) follows a Gaussian distribution,

\[p(w_{h}^{k}|\mathcal{D}_{h},\bm{y}_{h})\propto\mathcal{N}\Big{(}(\Omega_{h}^ {k})^{-1}\Phi_{h}\bm{y}_{h}^{\mathrm{T}},(\Omega_{h}^{k})^{-1}\Big{)},\]

where \(\Omega_{h}^{k}:=\Phi_{h}\Phi_{h}^{\mathrm{T}}+\lambda I_{d}\) and \(\Phi_{h}=[\phi(s_{h}^{1},a_{h}^{1}),\phi(s_{h}^{2},a_{h}^{2}),\ldots,\phi(s_{h }^{k-1},a_{h}^{k-1})]\). Adding the scaling factors \(\sigma^{2}\) and \(\nu^{2}\) yields the Line 10 of Algorithm 1. It is important to note that while the induced likelihood \(\exp(-L(w_{h}^{k},\bm{y}_{h}^{k},\mathcal{D}_{h}^{k}))\) from (3) is Gaussian, we do not assume \(y_{h}^{\tau}=r_{h}^{\tau}(s_{h}^{\tau},a_{h}^{\tau})\)\(+\widetilde{V}_{h+1}(s_{h+1}^{\tau})\) follows a Gaussian distribution. Instead, the above likelihood model can be used for non-Gaussian problems as we need not sample from the exact Bayesian posterior model [2, 74].

On the other hand, the \(\widetilde{w}_{h}^{k}\) computed in Line 9 of Algorithm 1 together with the greedy choice \(\widetilde{V}(\cdot)\approx\max_{a}\widetilde{Q}(\cdot,a)\) (Line 15) approximates the solution of Bellman optimality equation via the least-square ridge regression: \(\widehat{w}_{h}^{k}=\operatorname*{argmin}_{w}\sum_{\tau=1}^{k-1}(\phi(s_{h}^ {\tau},a_{h}^{\tau})^{\mathrm{T}}w-(r+\max\widetilde{Q}_{h}^{k}))^{2}+\lambda I _{d}\).4 Consequently, Line 5-10 essentially performs the Posterior Sampling Value Iteration.

Footnote 4: Here \(\widetilde{Q}:=\min\{\widetilde{Q}(\cdot,a),H-h+1\}\) is the truncated version.

**Optimism via multi-round sampling scheme.** Unlike the Bayesian regret or the worst-case expected regret, the high-probability worst-case regret in (2) needs to control the sub-optimal gap with arbitrarily high probability of at least \(1-\delta\). However, sampling once at each time step only provides a constant-probability optimistic estimation, which breaks the high probability requirement. In addition, the estimation error incurred by sampling (i.e. constant-probability pessimistic estimation) at each timestep will propagate to the previous time steps during the backward posterior sampling value iteration. This phenomenon does not appear in the \(1\)-horizon bandit problem due to a saturated-arm analysis [2, 6]. To remedy this issue, we design a multi-round sampling scheme that generates \(M\) estimates \(\{\widetilde{Q}^{m}\}_{m\in[M]}\) for \(Q\)-fuction through \(M\) i.i.d. sampling procedures, and constructs an optimistic estimate by setting \(\widetilde{Q}=\max_{m}\widetilde{Q}^{m}\). Notably, our choice of \(M\) has order \(\text{Polylog}(H,K,d,\delta)\), and thus makes our algorithm sample-efficient without increasing the overall complexity dependence. As shown in Line 11-14 of Algorithm 1, this scheme guarantees the optimistic estimates \(\widetilde{Q}\geq Q^{*}\) can be achieved as desired. Lastly, ensemble sampling methods enjoy empirical success and popularity in RL [21, 23, 31], including double q-learning [26] and bootstraped DQN [42, 47]. We are among the first few works to explain its theoretical effectiveness.

**Episodic delayed feedback model.** Recall that by Definition 2, when delay \(\tau_{k}\) takes place, the feedback \(\{s_{h}^{t},a_{h}^{t},r_{h}^{t},s_{h+1}^{t}\}_{h\in[H]}\) of episode \(k\) cannot be observed until the beginning of the \(k+\tau_{k}\)-th episode. Accordingly, the delayed version of the fully observed \(y^{\tau},\Omega_{h}^{k}\) now becomes,

\[y_{h}^{\tau}\leftarrow\mathds{1}_{\tau,k-1}\cdot[r_{h}^{\tau}(s_{h}^{\tau},a_{ h}^{\tau})+\widetilde{V}_{h+1}(s_{h+1}^{\tau})],\ \Phi_{h}\leftarrow[\mathds{1}_{1,k-1}\cdot\phi(s_{h}^{1},a_{h}^{1}),\ldots, \mathds{1}_{k-1,k-1}\cdot\phi(s_{h}^{k-1},a_{h}^{k-1})].\]

As a result, episodic delays are considered during the posterior updates in subsequent episodes. This completes the design of Delayed-PSVI as presented in Algorithm 1. In the remainder of this section, we present the main theoretical guarantees of Delayed-PSVI and the proof sketch of Theorem 1.

**Theorem 1**.: _Suppose delays satisfy Assumption 1. In any episodic linear MDP with time horizon \(T=KH\), where \(K\) is the total number of episodes, for any \(0<\delta<1\), let \(\lambda=1\), \(\sigma^{2}=1\), \(M=\log(4HK/\delta)/\log(64/63)\) and \(\nu=C_{\delta/4}\approx\widetilde{O}(\sqrt{MMH^{2}})\) (\(C_{\delta/4}\) in Lemma B.10). Then with probability at least \(1-\delta\), there exists some absolute constants \(c,c^{\prime},c^{\prime\prime}>0\) such that the regret of Delayed-PSVI (Algorithm 1) satisfies:_

\[R(T)\leq c\sqrt{d^{3}H^{3}T\iota}+c^{\prime}d^{2}H^{2}\mathbb{E}[\tau]\iota+c^ {\prime\prime}\iota.\]

_Here \(\iota\) is a Polylog term of \(H,d,K,\delta\)._

**On the complexity bound.** Theorem 1 provides the first analysis for PS algorithms under delay and answers the conjecture from [60]. Our result recovers the best-available frequentist regret of \(\widetilde{O}(\sqrt{d^{3}H^{3}T})\) for PS algorithms when there is no delay (\(\mathbb{E}[\tau]=0\)). According to [24], the worst-case regret of linear Thompson sampling is lower bounded by \(\Omega(\sqrt{d^{3}T})\), and this implies our regret dependencies on parameter \(d\) and \(T\) are optimal under the class of PS algorithms.5 The order \(\sqrt{H^{3}}\) in our regret is \(\sqrt{H}\)-suboptimal to the optimal dependence in [27]. As an initial study for posterior sampling with delayed feedback, improving the horizon dependence is beyond our pursuit and we leave it for future work. Moreover, the presence of delay incurs an additive regret term \(\widetilde{O}(d^{2}H^{2}\mathbb{E}[\tau])\). As \(T\) grows, the impact of delay will not dominate the overall regret. Furthermore, our high-probability regret bound directly implies the following worst-case expected regret.

Footnote 5: Note for non-sampling based on algorithms, e.g. UCB, the regret can attain \(\widetilde{O}(\sqrt{d^{2}T})\)[1].

**Corollary 1**.: _Under the setting of Theorem 1, the expected regret of Delayed-PSVI is bounded by_

\[\mathbb{E}[R(T)]\leq O(\sqrt{d^{3}H^{3}T\iota})+O(d^{2}H^{2}\mathbb{E}[\tau] \iota)+O(\iota)\]

_Here \(\iota\) is a Polylog of \(H,d,K\). The expectation is taken over the randomness in data and algorithm._

Proof of Corollary 1 is included in Appendix A.2. Additionally, we present the following corollary in linear bandits, whose main regret \(\widetilde{O}\sqrt{d^{3}T}\) is optimal for PS algorithms.

**Corollary 2** (Delayed Posterior Sampling for Linear Bandits).: _For the linear bandit with \(y_{t}=x_{t}^{\mathrm{T}}\theta_{*}+\eta_{t}\), where \(x_{t}\in D_{t}\subseteq\mathbb{R}^{d}\) and \(\eta_{t}\) be a mean-zero noise with \(B\)-subgaussian. Let \(T\) be the total number of steps. Under Assumption 1, for any \(0<\delta<1\), with probability at least \(1-\delta\), the regret of Delayed-PSLB satisfies:_

\[R(T)\leq O(\sqrt{d^{3}T\iota})+O(d^{2}\mathbb{E}[\tau]\iota)+O(\iota).\]

_Here \(\iota\) is a Polylog term of \(d,K,\delta\)._

### Sketch of the analysis

Due to the space limit, we outline the key steps in our analysis and defer the complete proof of Theorem 1 in Appendix B. To bound the worst-case regret in (2), first note that

\[R(T)=\sum_{k=1}^{K}\underbrace{V_{1}^{*}(s_{1}^{k})-\widetilde{V}_{1}^{k}(s_ {1}^{k})}_{\Delta_{opt}^{\star}}+\underbrace{\widetilde{V}_{1}^{k}(s_{1}^{k})- V_{1}^{\tau_{k}}(s_{1}^{k})}_{\Delta_{est}^{k}}.\]

Our goal is to attain an optimistic estimation so that \(\Delta_{opt}^{k}\leq 0\) while controlling the estimation error \(\Delta_{est}^{k}\). For optimistic PS algorithms, Gaussian anti-concentration is the main tool [6, 7, 65] to achieve optimism with constant probability. However, the probability of optimism will diminish as the algorithm back-propagates with respect to time. In contrast, we maintain \(m\in[M]\) independent ensembles \(Q^{m}\) so that roughly speaking, \(\mathbb{P}(Q^{m}\geq Q^{*})\geq\frac{1}{64}\) for all valid \(m\). For any \(0<\delta<1\), with the choice \(M=\log(1/m)/\log(64/63)\), the optimistic estimator \(Q=\max_{m}Q^{m}\) satisfies \(\mathbb{P}(Q\geq Q^{*})\geq 1-\delta\) (Lemma B.6). We can then proceed to prove \(\Delta_{opt}^{k}\leq 0\).

To control \(\Delta_{est}^{k}\), one key challenge is to bound the error term \(\sum_{k=1}^{K}\left\|\phi\left(s^{k},a^{k}\right)\right\|_{\left(\Omega^{k} \right)^{-1}}\). Due to the presence of delays, we cannot directly apply the Elliptical Potential Lemma as in the non-delayed settings. Therefore, we decompose \((\Omega^{k})^{-1}\) into \((\Sigma^{k})^{-1}+M_{k}\), where \(\Sigma^{k}:=\sum_{\tau=1}^{k-1}\phi\left(s_{h}^{\tau},a_{h}^{\tau}\right)\phi \left(s_{h}^{\tau},a_{h}^{\tau}\right)^{\mathrm{T}}+\lambda I\) is the full information matrix, and show

\[\sum_{k=1}^{K}\left\|\phi(s^{k},a^{k})\right\|_{M_{k}}\lesssim\max_{k\in[K]} \tau_{k}\sum_{k=1}^{K}\left\|\phi(s^{k},a^{k})\right\|_{(\Sigma^{k})^{-1}}^{2}.\]

By doing so, \(\sum_{k=1}^{K}\left\|\phi(s^{k},a^{k})\right\|_{(\Sigma^{k})^{-1}}^{2}\) can be upper bounded by \(\widetilde{O}(d\log(K))\) via the Elliptical Potential Lemma and \(\max_{k\in[K]}\tau_{k}\) can be upper bounded by \(\widetilde{O}(\mathbb{E}[\tau])\) via the sub-exponential tail bound. Combing all these steps completes the proof.

## 4 Delayed Posterior Sampling via Langevin Dynamics

Delayed-PSVI performs noisy value iteration for linear MDPs by injecting randomness for exploration via Gaussian noise. From the Bayesian perspective, it constructs a Laplace approximation to obtain a Gaussian posterior given the observed data. However, sampling from a Gaussian distribution with a general covariance matrix \(\Omega_{h}^{k}\) can be computationally expensive in high-dimensional RL tasks. Specifically, Line 10 of Algorithm 1 is conducted via \(\widetilde{w}:=\widehat{w}+\nu\cdot\Omega^{-1/2}\boldsymbol{\zeta}\), where \(\boldsymbol{\zeta}\sim\mathcal{N}(0,I_{d})\). The complexity of computing the matrix inverse involved (_e.g._ via Cholesky decomposition) is at least \(O(d^{3})\), which is prohibitively high for large \(d\). More importantly, in complex problem domains, a flexible form of non-Gaussian noise perturbation may be desirable.

To tackle these challenges, we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for PS algorithms, namely, LMC, and introduce the _Delayed-Langevin Posterior Sampling Value Iteration_ (Delayed-LPSVI) in Algorithm 2. The update rule of LMC essentially performs the following noisy gradient update:

\[w_{t}\gets w_{t-1}-\eta\nabla\mathcal{L}(w_{t-1})+\sqrt{2\eta\gamma} \epsilon_{t},\]

where \(\epsilon_{t}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N}(0,I_{d})\). It is based on the Euler-Murayama discretization of the Langevin stochastic differential equation (SDE):

\[\mathrm{d}\boldsymbol{w}(t)=-\nabla L(\boldsymbol{w}(t))\mathrm{d}t+\sqrt{2 \beta^{-1}}\;\mathrm{d}\boldsymbol{B}(t),\] (4)

where \(\boldsymbol{B}(t)\in\mathbb{R}^{d}\) is a Brownian motion, \(\beta>0\) and \(t>0\). Under certain regularity conditions on the drift term \(\nabla L(\boldsymbol{w}(t))\) in (4), it can be shown that the Langevin dynamics converges to a unique stationary distribution \(\pi(d\mathbf{w})\propto\exp{(-\beta L(\mathbf{w}))}\mathbf{d}\mathbf{w}\). As a result, LMC is capable of generating samples from arbitrarily complex distributions which can be intractable without closed form. With sufficient number of iterations, the posterior of \(w_{t}\) is in proportional to \(\exp(-\sqrt{1/\gamma}\mathcal{L}(w))\).

In our problem, we specify \(\mathcal{L}\) to be the following delayed loss function

\[L_{h}^{k}(w):=\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\left(\langle\phi(s_{h} ^{\tau},a_{h}^{\tau}),w\rangle-\tilde{y}_{h}^{\tau}\right)^{2}+\lambda\|w\|_{2 }^{2},\] (5)

where \(\tilde{y}_{h}^{\tau}:=r_{h}^{\tau}+\widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})\). Compared to Delayed-PSVI, Algorithm 2 does not require the matrix inversion computation. Below we present the worst-case regret of Delayed-LPSVI and discuss the key insights in our analysis. The full proof is deferred to Appendix C.

``` Input:\(w_{0}\), \(\eta_{k}\), \(N_{k}\), \(\gamma\) and rounds \(M,\lambda\). Delayed loss \(L_{h}^{k}\) as (5). Initialization:\(\forall k\in[K],h\in[H],\widetilde{Q}_{H+1}^{k}(\cdot,\cdot)\gets 0, \widetilde{V}_{H+1}^{k}(\cdot,\cdot)\gets 0,\widetilde{V}_{h}^{0}(\cdot,\cdot)\gets 0\) forepisode \(k=1,\ldots,K\)do Sample initial state \(s_{1}^{k}\) fortime step \(h=H_{1},\ldots,H\)do \(\widetilde{w}_{h}^{k,m}\gets LMC(L_{h}^{k},w_{0},\eta_{k},N_{k},\gamma)\) //\(LMC\) is given by Algorithm 3 \(\widetilde{Q}_{h}^{k,m}(\cdot,\cdot)\leftarrow\phi(\cdot)^{T}\widetilde{w}_{h}^ {k,m}\) Update \(\widetilde{Q}_{h}^{k}(\cdot,\cdot)\leftarrow\max_{\alpha}\widetilde{Q}_{h}^{k,m}\) \(\widetilde{V}_{h}^{k}(\cdot,\cdot)\leftarrow\max_{\alpha}\min\{\widetilde{Q}_ {h}^{k}(\cdot,a),H-h+1\}\) Update policy \(\pi_{h}^{k}(\cdot)\leftarrow\operatorname*{argmax}_{a\in\mathcal{A}}\min\{ \widetilde{Q}_{h}^{k}(\cdot,a),H-h+1\}\) fortime step \(h=1,\ldots,H\)do Choose action \(a_{h}^{k}=\pi_{h}^{k}(s_{h}^{k})\) Collect trajectory observations \(\mathcal{D}_{h}\leftarrow\mathcal{D}_{h}\cup\{(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_ {h+1}^{k})\}\) /* Feedback generated in episode \(k\) cannot be immediately observed in the presence of delay */ ```

**Algorithm 2**Delayed Langevin Posterior Sampling Value Iteration (Delayed-LPSVI)

**Theorem 2**.: _Suppose delays satisfy Assumption 1. In any episodic linear MDP with time horizon \(T=KH\), where \(K\) is the total number of episodes and \(H\) is the fixed episode length, for any \(0<\delta<1\), let \(\lambda=1\), \(N_{k}=\max\{\log(\frac{32H^{2}(K+\lambda)dk}{\gamma\lambda}+1)/[2\log(1/(1- \frac{1}{2\kappa_{h}}))],\)\(\frac{\log 2}{2\log(1/(1-\frac{1}{2\kappa_{h}}))},\log(\frac{4HK^{3}}{\sqrt{ \lambda/dK}})/\log(1/(1-\frac{1}{2\kappa_{h}}))\}\), \(\eta_{k}=\frac{1}{4\lambda_{\max}(\Omega_{h}^{k})}\), \(\gamma=16C_{\delta/4}^{2}\approx\widetilde{O}(dMH^{2})\)\(w_{0}=\bm{0}\) and \(M=\log(4HK/\delta)/\log(64/63)\). Then with probability at least \(1-\delta\), there exists some absolute constants \(c,c^{\prime},c^{\prime\prime}>0\) such that the regret of Algorithm 2 satisfies:_

\[R(T)\leq c\sqrt{d^{3}H^{3}Tt}+c^{\prime}d^{2}H^{2}\mathbb{E}[\tau]+c^{\prime \prime}\iota.\]

_Here \(\iota\) is a Polylog term of \(H,d,K,\delta\) and \(C_{\delta}\) is defined in Lemma C.9._

Neglecting the constants and Polylog factors, Delayed-LPSVI maintains the same order regret of \(\widetilde{O}(\sqrt{d^{3}H^{3}T}+d^{2}H^{2}\mathbb{E}[\tau])\) as Delayed-PSVI while significantly improving the computational efficiency. Precisely, LMC requires \(O(N)\) complexity to perform gradient steps in Line 6 of Algorithm 2 and an extra \(O(d)\) operations to compute \(\widetilde{Q}_{h}^{k,m}\) in Line 7. Thus, the total computation complexity of LMC is \(O((N+d)MHK)\). On the other hand, sampling without LMC (Line5-8 in Algorithm 1) requires \(O(d^{3})\) operations, and the multi-round sampling (Line9-11) incurs \(O(dM)\) additional operations, which implies for a total computation complexity of \(O((d^{3}+dM)HK)\). As the choice of \(N\) in Algorithm 2 has logarithmic order, and \(M=\log(4HK/\delta)/\log(64/63)\), the overall complexity of Delayed-LPSVI is \(\widetilde{O}(dHK)\), whereas the overall computational complexity of Delayed-PSVI is \(\widetilde{O}(d^{3}HK)\). Notably, Delayed-LPSVI reduces the computational overhead of Delayed-PSVI by \(\widetilde{O}(d^{2})\).

**On the analysis.** The key step in the proof of Theorem 2 is to show the convergence guarantee of LMC. Indeed, by recursion, one can show

\[w_{N}=A_{h,k}^{N}w_{0}+\left(I-A_{h,k}^{N}\right)\widehat{w}_{h}^{k}+\sqrt{2 \eta\gamma}\sum_{l=0}^{N-1}A_{h,k}^{l}\epsilon_{N-l},\]

where \(A_{h,k}:=I-2\eta_{k}\Omega_{h}^{k}\). For any \(w_{0}\), it implies \(w_{N}\) follows the Gaussian distribution \(\mathcal{N}\left(A_{h,k}^{N_{k}}w_{0}+\left(I-A_{h,k}^{N_{k}}\right)\widehat{w }_{h}^{k},\Theta_{h}^{k}\right)\). With the choice of \(\eta_{k}=\frac{1}{4\lambda_{\max}\left(\Omega_{h}^{k}\right)}\), \(A_{h,k}\prec I_{d}\) and \(\frac{\gamma}{2}(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}})\left(\Omega_{h}^{k} \right)^{-1}\prec\Theta_{h}^{k}\prec\gamma\left(\Omega_{h}^{k}\right)^{-1}\), which is the key to connect \(\Theta_{h}^{k}\) with \((\Omega_{h}^{k})^{-1}\) (Lemma C.2), the main analysis for Delayed-PSVI goes through by utilizing this connection.

**On arbitrary delayed feedback.** The current study considers the stochastic delays that are sub-exponential Assumption 1. What if delay has an arbitrary distribution (_e.g._ Cauchy distribution has unbounded mean)? Indeed, the regret can be (roughly) bounded by \(\widetilde{O}(\frac{1}{q}\sqrt{d^{3}H^{3}T}+dH^{2}d_{\tau}(q))\) for \(d_{\tau}(q)\) to be the \(q\)-th quantile of delay \(\tau\). We do not focus on this setting since there is a \(1/q\) blow-up in the main regret that many distributions (_e.g._ sub-exponential) do not need to sacrifice. We include the discussion in Appendix A.3.

## 5 Experiments

To validate whether our posterior sampling algorithms are competitive or outperform the non-sampling-based algorithms in the delayed setting, in this section, we examine their empirical performance in two simulated RL environments with different delayed feedback distributions. In particular, we consider a linear MDP environment following [44, 46], and a variant of the popular River-Swim [55]. In both environments, we benchmark Delayed-PSVI (Algorithm 1), Delayed-LPSVI (Algorithm 2) against LSVI-UCB [35] with delayed feedback, namely, Delayed-UCBVI. In this section, we discuss results in the first setting and defer the discussion of RiverSwim in Appendix E.

### Synthetic Linear MDP

We construct a synthetic linear MDP instance with \(|\mathcal{S}|=2\), \(|\mathcal{A}|=50\), \(d=10\), and \(H=20\). The linear feature mapping embeds each state-action pair with its binary representation and induces the following reward function: \(r(s,a)=0.99\) if \(s=0,a=0\); \(r(s,a)=0.01\) otherwise. The design of the environment results in the same optimal value \(V_{1}^{*}(s_{1})\) when \(d\) and \(H\) are fixed. Algorithms are examined under three types of delays that are commonly encountered in real-world phenomena, including sub-exponential delays and long-tail delays:* **Multinomial delay.** Delays follow a Multinomial distribution with three categories \(\{10,20,30\}\), with the corresponding probabilities as \(\{0.5,0.3,0.2\}\).
* **Poisson delay.** Delays follow a Poisson distribution with the expected delay as \(\mathbb{E}[\tau]=50\).
* **Long-tail delay.** Delays are discretized from a Pareto distribution 6 with the shape parameter as \(1.0\) and the scale parameter as \(500\).

Footnote 6: Pareto distribution with shape parameter less than \(5.0\) are known to have hovy right tails.

To run Delayed-LPSVI, we warm start LMC by initializing \(w_{0}\) at each time step with the previous sample, and let \(M=2\), \(N=40\), \(\eta=c_{\eta}/\lambda_{\max}(\Omega_{h}^{k})\). For Delayed-PSVI, we set parameters \(M=2\), \(\nu=\sqrt{d}H\). In the case of Delayed-UCBVI, we set the bonus coefficient as \(\beta=c_{\beta}/2\cdot dH\ \sqrt{\log(dH)}\). To make a fair comparison, we perform a grid search to determine the optimal hyperparameter values and fix \(c_{\beta}=0.1\), \(c_{\eta}=0.5\), \(\gamma=0.02\). Experiments are repeated with 10 different random seeds, and the returns are averaged over episodes in Figure 1. Further elaboration on additional metrics is available in Appendix E.2.

**Results and Discussions.** Both Delayed-PSVI and Delayed-LPSVI exhibit consistent and robust performance with resilience, not only under the well-behaved delays that decay exponentially fast, as assumed in Assumption 1, but also under the heavy-tailed delays, such as those following Pareto distributions. Notably, when confronted with the challenge of long-tail delays, our algorithms excel Delayed-UCBVI in terms of statistical accuracy (yielding higher return) and convergence rate. Specifically, the performance of Delayed-UCBVI degrades under long-tail delays, resulting from its computational inefficiency in iteratively constructing confidence intervals. In contrast, PS methods offer a higher degree of flexibility to adjust the range of exploration, owing to the inherent randomized algorithmic nature. To assess the computational advantages facilitated by LMC, we consider additional synthetic environments with varied dimensions for a more comprehensive analysis. For detailed statistics and further discussions, please refer to Appendix E.2. It is noteworthy that in practical high-dimensional RL tasks, the computational savings achieved by Delayed-LPSVI, in comparison to Delayed-PSVI, are considerably more significant.

## 6 Conclusion

In this paper, we study posterior sampling with episodic delayed feedback in linear MDPs. We introduce two novel value-based algorithms: Delayed-PSVI and Delayed-LPSVI. Both algorithms are proved to achieve \(\widetilde{O}(\sqrt{d^{3}H^{3}T}+d^{2}H^{2}\mathbb{E}[\tau])\) worst-case regret. Notably, by incorporating LMC for approximate sampling, Delayed-LPSVI reduces the computational cost by \(\widetilde{O}(d^{2})\) while maintaining the same order of regret. Our empirical experiments further validate the effectiveness of our algorithms by demonstrating their superiority over the UCB-based methods.

This work provides the first delayed-feedback analysis for posterior sampling algorithms in RL, paving the way to several promising avenues for future research. Firstly, it is interesting to extend the current results to settings with general function approximation [34; 71]. Additionally, leveraging the sharp analysis outlined in [27] to improve the suboptimal dependence on \(H\) for posterior sampling algorithms presents an intriguing avenue for exploration. Furthermore, addressing other types of delay (e.g. adversarial delay) that differ from stochastic one will contribute to the ongoing field of delayed feedback studies in online learning, and we leave the investigation in future works.

Figure 1: Left:(a) Multinomial delay with delay categories \(\{10,20,30\}\). (b) Poisson delay with rate \(\mathbb{E}[\tau]=50\). (c) Long-tail Pareto delay with shape 1.0, scale 500. Results are reported over 10 experiments. Delayed-PSVI and Delayed-LPSVI demonstrate robust performance under both well-behaved and long-tail delays.

## Acknowledgements

Ming Yin and Yu-xiang Wang are gratefully supported by National Science Foundation (NSF) Awards #2007117 and #2003257. Nikki Kuang and Yi-An Ma are supported by the NSF SCALE MoDL-2134209 and the CCF-2112665 (TILOS) awards, as well as the U.S. Department of Energy, Office of Science, and the Facebook Research award. Mengdi Wang gratefully acknowledges funding from Office of Naval Research (ONR) N00014-21-1-2288, Air Force Office of Scientific Research (AFOSR) FA9550-19-1-0203, and NSF 19-589, CMMI-1653435.

## References

* Abbasi-Yadkori et al. [2011] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Abeille and Lazaric [2017] Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. In _Artificial Intelligence and Statistics_, pages 176-184. PMLR, 2017.
* Abramowitz and Stegun [1964] Milton Abramowitz and Irene A Stegun. _Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables_, volume 55. US Government Printing Office, 1964.
* Agarwal and Aggarwal [2021] Mridul Agarwal and Vaneet Aggarwal. Blind decision making: Reinforcement learning with delayed observations. _Pattern Recognition Letters_, 150:176-182, 2021.
* Agrawal et al. [2021] Priyank Agrawal, Jinglin Chen, and Nan Jiang. Improved worst-case regret bounds for randomized least-squares value iteration. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 6566-6573, 2021.
* Agrawal and Goyal [2013] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In _International conference on machine learning_, pages 127-135. PMLR, 2013.
* Agrawal and Jia [2017] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. _Advances in Neural Information Processing Systems_, 30, 2017.
* Auer et al. [2008] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_, 21, 2008.
* Ayoub et al. [2020] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR, 2020.
* Azar et al. [2017] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* Black et al. [2020] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. _arXiv preprint arXiv:2305.13301_, 2020.
* Bouteiller et al. [2020] Yann Bouteiller, Simon Ramstedt, Giovanni Beltrame, Christopher Pal, and Jonathan Binas. Reinforcement learning with random delays. In _International conference on learning representations_, 2020.
* Cai et al. [2020] Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_, pages 1283-1294. PMLR, 2020.
* Chapelle and Li [2011] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. _Advances in neural information processing systems_, 24, 2011.
* Chen et al. [2020] Baiming Chen, Mengdi Xu, Zuxin Liu, Liang Li, and Ding Zhao. Delay-aware multi-agent reinforcement learning for cooperative and competitive environments. _arXiv preprint arXiv:2005.05441_, 2020.

* [16] Minshuo Chen, Yu Bai, H Vincent Poor, and Mengdi Wang. Efficient rl with impaired observability: Learning to act with delayed and missing state observations. _arXiv preprint arXiv:2306.01243_, 2023.
* [17] Esther Derman, Gal Dalal, and Shie Mannor. Acting in delayed environments with non-stationary markov policies. In _International Conference on Learning Representations_, 2020.
* [18] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. _arXiv preprint arXiv:1904.12901_, 2019.
* [19] Ying Fan and Yifei Ming. Model-based reinforcement learning for continuous control with posterior sampling. In _International Conference on Machine Learning_, pages 3078-3087. PMLR, 2021.
* [20] Matthew Fellows, Anuj Mahajan, Tim GJ Rudner, and Shimon Whiteson. Virel: A variational inference framework for reinforcement learning. _Advances in neural information processing systems_, 32, 2019.
* [21] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* [22] Mangeueu Anne Gael, Claire Vernade, Alexandra Carpentier, and Michal Valko. Stochastic bandits with arm-dependent delays. In _International Conference on Machine Learning_, pages 3348-3356. PMLR, 2020.
* [23] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* [24] Nima Hamidi and Mohsen Bayati. On frequentist regret of linear thompson sampling. _arXiv preprint arXiv:2006.06790_, 2020.
* [25] Beining Han, Zhizhou Ren, Zuofan Wu, Yuan Zhou, and Jian Peng. Off-policy reinforcement learning with delayed rewards. In _International Conference on Machine Learning_, pages 8280-8303. PMLR, 2022.
* [26] Hado Hasselt. Double q-learning. _Advances in neural information processing systems_, 23, 2010.
* [27] Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear markov decision processes. In _International Conference on Machine Learning_, pages 12790-12822. PMLR, 2023.
* [28] Benjamin Howson, Ciara Pike-Burke, and Sarah Filippi. Delayed feedback in generalised linear bandits revisited. In _International Conference on Artificial Intelligence and Statistics_, pages 6095-6119. PMLR, 2023.
* [29] Benjamin Howson, Ciara Pike-Burke, and Sarah Filippi. Optimism and delays in episodic reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 6061-6094. PMLR, 2023.
* [30] Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian random vectors. 2012.
* [31] Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang. Randomized exploration in reinforcement learning with general value function approximation. In _International Conference on Machine Learning_, pages 4607-4616. PMLR, 2021.
* [32] Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura, and Ken-Ichi Kawarabayashi. Delay and cooperation in nonstochastic linear bandits. _Advances in Neural Information Processing Systems_, 33:4872-4883, 2020.

* [33] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2017.
* [34] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418, 2021.
* [35] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* [36] Tiancheng Jin, Tal Lancewicki, Haipeng Luo, Yishay Mansour, and Aviv Rosenberg. Near-optimal regret for adversarial mdp with delayed bandit feedback. _arXiv preprint arXiv:2201.13172_, 2022.
* [37] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [38] Amin Karbasi, Nikki Lijing Kuang, Yian Ma, and Siddharth Mitra. Langevin thompson sampling with logarithmic communication: bandits and reinforcement learning. In _International Conference on Machine Learning_, pages 15828-15860. PMLR, 2023.
* [39] Parham M Kebria, Abbas Khosravi, Saeid Nahavandi, Peng Shi, and Roohallah Alizadehsani. Robust adaptive control scheme for teleoperation systems with delay and uncertainties. _IEEE transactions on cybernetics_, 50(7):3243-3253, 2019.
* [40] Tal Lancewicki, Aviv Rosenberg, and Yishay Mansour. Learning adversarial markov decision processes with delayed feedback. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7281-7289, 2022.
* [41] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. _arXiv preprint arXiv:2302.12192_, 2023.
* [42] Ziniu Li, Yingru Li, Yushun Zhang, Tong Zhang, and Zhi-Quan Luo. Hyperdqn: A randomized exploration method for deep reinforcement learning. In _International Conference on Learning Representations_, 2021.
* [43] Eric Mazumdar, Aldo Pacchiano, Yian Ma, Michael Jordan, and Peter Bartlett. On approximate thompson sampling with langevin algorithms. In _International Conference on Machine Learning_, pages 6797-6807. PMLR, 2020.
* [44] Yifei Min, Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Variance-aware off-policy evaluation with linear function approximation. _Advances in neural information processing systems_, 34:7598-7610, 2021.
* [45] Washim Uddin Mondal and Vaneet Aggarwal. Reinforcement learning with delayed, composite, and partially anonymous reward. _arXiv preprint arXiv:2305.02527_, 2023.
* [46] Thanh Nguyen-Tang, Ming Yin, Sunil Gupta, Svetha Venkatesh, and Raman Arora. On instance-dependent bounds for offline reinforcement learning with linear function approximation. _arXiv preprint arXiv:2211.13208_, 2022.
* [47] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_, 29, 2016.
* [48] Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via randomized value functions. _J. Mach. Learn. Res._, 20(124):1-62, 2019.
* [49] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. In _International Conference on Machine Learning_, pages 2377-2386. PMLR, 2016.

* [50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [51] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. _arXiv preprint arXiv:2310.08864_, 2023.
* [52] Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling. In _International Conference on Learning Representations_, 2018.
* [53] Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. _Advances in Neural Information Processing Systems_, 32, 2019.
* [54] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* [55] Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for markov decision processes. _Journal of Computer and System Sciences_, 74(8):1309-1331, 2008.
* [56] Wei Tang, Chien-Ju Ho, and Yang Liu. Bandit learning with delayed impact of actions. _Advances in Neural Information Processing Systems_, 34:26804-26817, 2021.
* [57] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294, 1933.
* [58] Tobias Sommer Thune, Nicolo Cesa-Bianchi, and Yevgeny Seldin. Nonstochastic multiarmed bandits with unrestricted delays. _Advances in Neural Information Processing Systems_, 32, 2019.
* [59] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [60] Claire Vernade, Alexandra Carpentier, Tor Lattimore, Giovanni Zappella, Beyza Ermis, and Michael Brueckner. Linear bandits with stochastic delayed feedback. In _International Conference on Machine Learning_, pages 9712-9721. PMLR, 2020.
* [61] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_, 33:6123-6135, 2020.
* [62] Yining Wang, Ruosong Wang, Simon S Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. _arXiv preprint arXiv:1912.04136_, 2019.
* [63] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. _arXiv preprint arXiv:2307.12966_, 2023.
* [64] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688, 2011.
* [65] Pan Xu, Hongkai Zheng, Eric V Mazumdar, Kamyar Azizzadenesheli, and Animashree Anandkumar. Langevin monte carlo for contextual bandits. In _International Conference on Machine Learning_, pages 24830-24850. PMLR, 2022.
* [66] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning_, pages 6995-7004. PMLR, 2019.

* Yang and Wang [2020] Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In _International Conference on Machine Learning_, pages 10746-10756. PMLR, 2020.
* Yang et al. [2023] Yunchang Yang, Han Zhong, Tianhao Wu, Bin Liu, Liwei Wang, and Simon S Du. A reduction-based framework for sequential decision making with delayed feedback. _arXiv preprint arXiv:2302.01477_, 2023.
* Yin et al. [2021] Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline policy evaluation for reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1567-1575. PMLR, 2021.
* Yin et al. [2022] Ming Yin, Yaqi Duan, Mengdi Wang, and Yu-Xiang Wang. Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. _International Conference on Learning Representations_, 2022.
* Yin et al. [2023] Ming Yin, Mengdi Wang, and Yu-Xiang Wang. Offline reinforcement learning with differentiable function approximation is provably efficient. _International Conference on Learning Representations_, 2023.
* Zanette et al. [2020] Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In _International Conference on Artificial Intelligence and Statistics_, pages 1954-1964. PMLR, 2020.
* Zanette et al. [2020] Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.
* Zhang [2022] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_, 4(2):834-857, 2022.
* Zhou et al. [2021] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.
* Zhou et al. [2021] Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted mdps with feature mapping. In _International Conference on Machine Learning_, pages 12793-12802. PMLR, 2021.
* Zhou et al. [2019] Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet. Learning in generalized linear contextual bandits with stochastic delays. _Advances in Neural Information Processing Systems_, 32, 2019.
* Zimmert and Seldin [2020] Julian Zimmert and Yevgeny Seldin. An optimal algorithm for adversarial bandits with arbitrary delays. In _International Conference on Artificial Intelligence and Statistics_, pages 3285-3294. PMLR, 2020.

## Appendix A Some Properties

### Properties of Linear MDPs

**Lemma A.1**.: _In linear MDPs, the action-value function is also linear in feature map. \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\), \(h\in[H]\) and \(\phi\in\mathbb{R}^{d}\), under any fixed policy \(\pi\),_

\[Q_{h}^{\pi}(s,a)=\phi(s,a)^{\mathrm{T}}w_{h}^{\pi},\]

_where \(w_{h}^{\pi}:=\theta_{h}+\mathbb{E}_{\mu}[V_{h+1}^{\pi}(s^{\prime})]\) and \(w_{h}\in\mathbb{R}^{d}\). As a corollary, there exists \(w_{h}^{*}\) such that \(Q_{h}^{*}=\phi^{\mathrm{T}}w_{h}^{*}\)._

Proof of Lemma a.1.: By Bellman equation,

\[Q_{h}^{\pi}(s,a) =r_{h}(s,a)+\mathbb{E}_{s^{\prime}\sim\mathbb{B}_{h}(\cdot|s,a)}[ V_{t+1}^{\pi}(s^{\prime})]\] \[=\phi(s,a)^{\mathrm{T}}\theta_{h}+\int V_{h+1}^{\pi}(s^{\prime}) \;d(\phi(s,a)^{\mathrm{T}}\mu_{h}(s^{\prime}))\] \[=\phi(s,a)^{\mathrm{T}}w_{h}^{\pi},\]

where \(w_{h}^{\pi}:=\theta_{h}+\mathbb{E}_{\mu_{h}}[V_{h+1}^{\pi}(s^{\prime})]\). 

### Worst-case regret as a stronger criterion

We use Theorem1 as an example. Using the worst-case result, _i.e._ with probability \(1-\delta\),

\[R(T)\leq c\sqrt{d^{3}H^{3}T_{t}}+c^{\prime}d^{2}H^{2}\mathbb{E}[\tau]\iota+c^ {\prime\prime}\iota.\]

Here \(\iota\) has the functional form \(\iota=\text{ Polylog}(d,K,H,\delta)\). Then choosing \(\delta=1/(HK)\) to obtain with probability \(1-1/(HK)\),

\[R(T)\leq c\sqrt{d^{3}H^{3}T_{t}}+c^{\prime}d^{2}H^{2}\mathbb{E}[\tau]\iota+c^ {\prime\prime}\iota:=A\]

for \(\iota=\text{ Polylog}(d,K,H)\). Therefore,

\[\mathbb{E}[R(T)]\leq\mathbb{E}[R(T)\mathds{1}_{\{R(T)\leq A\}}] +\mathbb{E}[R(T)\mathds{1}_{\{R(T)\geq A\}}]\] \[\leq A\cdot 1+HK\cdot\mathbb{P}(R(T)\geq A)\leq A+1.\]

This completes Corollary1.

### Discussion on the arbitrary delay

For completeness of our study, we also briefly discuss the case when delay is arbitrary. In general, the regret can be (roughly) bounded by \(\widetilde{O}(\frac{1}{q}\sqrt{d^{3}H^{3}T}+dH^{2}d_{\tau}(q))\) for \(d_{\tau}(q)\) to be the \(q\)-th quantile of delay \(\tau\). This could be achieved by creating a low-switching variant of our Theorem1/Theorem2 and applying the reduction of the concurrent work [68]. We do not focus on this setting since there is a \(1/q\) blow-up in the main regret that many distributions (_e.g._ sub-exponential) do not need to sacrifice.

## Appendix B Regret Analysis for Delayed-PSVI

To proceed with the regret analysis, we introduce some helpful notations. Besides \(\widetilde{Q}_{h}^{k}(s,a)=\max_{m}\phi(s,a)^{\mathrm{T}},\widetilde{w}_{h}^{ k,m},\widetilde{V}_{h}^{k}(s)=\max_{a}\widetilde{Q}_{h}^{k}(s,a)\) in Algorithm1, we define

\[\widehat{Q}_{h}^{k}(s,a)=\phi(s,a)^{\mathrm{T}}\widehat{w}_{h}^{ k},\quad\widehat{V}_{h}^{k}(s)=\max_{a}\widehat{Q}_{h}^{k}(s,a),\quad \bar{Q}_{h}^{k}=\min\{\widetilde{Q}_{h}^{k},H-h+1\};\] \[(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k})(s,a):=\phi(s,a) ^{\mathrm{T}}w_{h}^{k},\text{ with }w_{h}^{k}:=\theta_{h}+\int_{\mathcal{S}} \widetilde{V}_{h+1}^{k}(s^{\prime})\mathrm{d}\mu_{h}(s^{\prime}).\]

**Regret decomposition:** We start by rewriting regret in terms of value-function error decomposition following the standard analysis of optimistic algorithms [10]:

\[R(T)=\sum_{k=1}^{K}\underbrace{V_{1}^{*}(s_{1}^{k})-\widetilde{V}_{1}^{k}(s_{1} ^{k})}_{\Delta_{opt}^{k}}+\underbrace{\widetilde{V}_{1}^{k}(s_{1}^{k})-V_{1}^ {\pi_{k}}(s_{1}^{k})}_{\Delta_{est}^{k}},\]

where at each episode \(k\), \(\Delta_{opt}^{k}\) corresponds to the regret resulting from optimism, and \(\Delta_{est}^{k}\) tracks down the regret incurred from estimation error. Efficient RL algorithms thus need to strike a balance between both terms. More specifically, it is desirable to generate optimistic estimations over the true value function, while keeping estimation error relatively small. By cautious design of noise perturbation, we show in Theorem1 that Algorithm1 effectively achieves \(\sqrt{T}\) order regret in episodic MDPs with linear function approximation.

Proof of Theorem1.: The proof proceeds by bounding \(\Delta_{opt}^{k}\) and \(\Delta_{est}^{k}\) respectively.

**Step 1: bound regret from optimism.**

By LemmaB.7, the optimism provided by our algorithm guarantees with probability at least \(1-\delta/2\), for all \(k\in[K]\), \(\Delta_{opt}^{k}:=V_{1}^{*}(s_{1}^{k})-\widetilde{V}_{1}^{k}(s_{1}^{k})\leq 0\).

**Step 2: bound regret from estimation error.** To bound the estimation error, we first condition on the following event

\[\mathcal{E}:=\{\|\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-(r_{h}^{k}+\mathbb{P }_{h}\widetilde{V}_{h+1}^{k})(s,a)|\leq\beta\left\|\phi(s,a)\right\|_{(\Omega_ {h}^{k})^{-1}}+\frac{1}{K^{3}},\,\forall s,a,h,k\},\]

with \(\beta:=\sqrt{2\nu^{2}\log(16C_{d}HMK/\delta)}+\sqrt{8H^{2}\left[\frac{d}{2} \log\left(\frac{k+\lambda}{\lambda}\right)+dM\log(1+\frac{2\sqrt{8k^{3}}C_{H, d,k,M,\delta/8}}{H\sqrt{\lambda}})+\log\frac{16}{\delta}\right]}+2\sqrt{\lambda}\sqrt{d}H\).7 Here \(C_{d}\) and \(C_{H,d,k,M,\delta}\) are defined in LemmaB.8.

Footnote 7: Note here the \(\delta\) equals \(\delta/4\) as of LemmaB.8. Therefore, by LemmaB.8, \(\mathbb{P}(\mathcal{E})\geq 1-\delta/4\).

Recall that \(\Delta_{est}^{k}:=\widetilde{V}_{1}^{k}(s_{1}^{k})-V_{1}^{\pi_{k}}(s_{1}^{k})\) and define \(\zeta_{h}^{k}=\mathbb{E}[\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+ 1}^{\pi_{k}}\left(s_{h+1}^{k}\right)|s_{h}^{k},a_{h}^{k}]-\widetilde{V}_{h+1} ^{k}\left(s_{h+1}^{k}\right)+V_{h+1}^{\pi_{k}}\left(s_{h+1}^{k}\right)\). Then by applying LemmaB.1 recursively, the total estimation error \(\sum_{k=1}^{K}\Delta_{est}^{k}\) can be decomposed as:

\[\sum_{k=1}^{K}\Delta_{est}^{k}=\sum_{k=1}^{K}\widetilde{V}_{1}^{k }(s_{1}^{k})-V_{1}^{\pi_{k}}(s_{1}^{k})\] (6) \[\leq \sum_{k=1}^{K}\left(\widetilde{V}_{2}^{k}\left(s_{2}^{k}\right)- V_{2}^{\pi_{k}}\left(s_{2}^{k}\right)+\zeta_{1}^{k}+\beta\left\|\phi(s_{1}^{k},a_{1}^ {k})\right\|_{(\Omega_{1}^{k})^{-1}}+\frac{1}{K^{3}}\right)\] \[\leq \ldots\] \[\leq \sum_{k=1}^{K}\sum_{h=1}^{H}\zeta_{h}^{k}+\beta\sum_{k=1}^{K} \sum_{h=1}^{H}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{(\Omega_{h}^{k})^{-1}} +\frac{H}{K^{2}}.\]

On one hand, by definition, \(|\zeta_{h}^{k}|\leq 2H\) for all \(h\in[H],k\in[K]\). Therefore, \(\{\zeta_{h}^{k}\}\) is a martingale difference sequence (since the computation of \(\widetilde{V}_{h}^{k}\) is independent of the new observation at episode \(k\)). By Azuma-Hoeffding's inequality (for \(t>0\)),

\[\mathbb{P}\left(\sum_{k=1}^{K}\sum_{h=1}^{H}\zeta_{h}^{k}>t\right)\geq\exp \left(\frac{-t^{2}}{2K\cdot H^{3}}\right):=\delta/8,\]

which implies with probability \(1-\delta/8\),

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\zeta_{h}^{k}\leq\sqrt{2KH^{3}\cdot\log(8/\delta)}= \sqrt{2H^{2}T\cdot\log(8/\delta)}.\] (7)

**Step 3: bounding the delayed error.** By Lemma B.4, with probability \(1-\delta/8\),

\[\sum_{h-1}^{H}\sum_{k=1}^{K}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{(\Omega_{h}^ {k})^{-1}}\leq H\sqrt{2dK\log((d+K)/d)}+dHD_{\tau,\delta,H,K}\log((d+K)/d).\]

Here \(D_{\tau,\delta,H,K}:=1+2\mathbb{E}[\tau]+2\sqrt{2\mathbb{E}[\tau]\log(\frac{24 KH}{\delta})}+\frac{4}{3}\log(\frac{24KH}{\delta})+D_{\tau,K,\frac{\delta}{16H}}\) and \(D_{\tau,K,\delta}\) is defined in Lemma D.6. Consequently,

\[\beta\sum_{k=1}^{K}\sum_{h=1}^{H}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{( \Omega_{h}^{k})^{-1}}\leq\beta H\sqrt{2dK\log((d+K)/d)}+\beta dHD_{\tau,\delta,H,K}\log((d+K)/d).\] (8)

Note that by Lemma B.8, event \(\mathcal{E}\) holds with probability \(1-\delta/4\), and by a union bound with (7) and (8), we have with probability \(1-\delta/2\),

\[\sum_{k=1}^{K}\Delta_{est}^{k}\leq\sqrt{2H^{2}T\cdot\log(8/\delta)}+\beta H \sqrt{2dK\log((d+K)/d)}+\beta dHD_{\tau,\delta,H,K}\log((d+K)/d)+\frac{H}{K^{2}}.\]

Finally, by a union bound over Step1, Step2 and Step3, we obtain with probability \(1-\delta\),

\[R(T)= \sum_{k=1}^{K}\Delta_{opt}^{k}+\sum_{k=1}^{K}\Delta_{est}^{k} \leq\sum_{k=1}^{K}\Delta_{est}^{k}\] \[\leq \sqrt{2H^{2}T\cdot\log(8/\delta)}+\beta H\sqrt{2dK\log((d+K)/d)}+ \beta dHD_{\tau,\delta,H,K}\log((d+K)/d)+\frac{H}{K^{2}}\] \[\leq c\sqrt{d^{3}H^{3}T_{t}}+c^{\prime}d^{2}H^{2}\mathbb{E}[\tau]_{t}+O (\iota)\]

where \(c>0\) is some universal constant and \(\iota\) is a Polylog term of \(H,d,K,\delta\). The last step is due to: by the choice of \(\lambda=1\), \(\sigma^{2}=1\), \(\nu=C_{\delta/4}\) and \(M=\log(4HK/\delta)/\log(64/63)\), we can bound \(C_{\delta}\) (in Lemma B.10) by \(C_{\delta}\leq c_{0}H\sqrt{dM_{t\delta}}\) with \(c_{0}\) a universal constant and \(\iota_{\delta}\) contains only the Polylog terms. This implies \(\nu^{2}\leq c_{1}H^{2}dM_{t\delta}\). Note \(C_{d}\leq d_{t\delta}\), therefore \(\beta\) is dominated by the first term \(\beta\leq C_{2}\sqrt{2\nu^{2}\log(16C_{d}HMK/\delta)}\leq C_{3}dH_{t\delta}\) for some universal constants \(C_{2},C_{3}\). Since \(R(T)\) is dominated by the second term in the second to last inequality, plug back the upper bound for \(\beta\) gets the result. Finally, it is readily to verify \(D_{\tau,\delta,H,K}\) is bounded by \(c^{\prime}\mathbb{E}[\tau]_{t}+O(\iota)\). 

**Lemma B.1**.: _Define \(\zeta_{h}^{k}=\mathbb{E}[\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+ 1}^{\pi_{k}}\left(s_{h+1}^{k}\right)|s_{h}^{k},a_{h}^{k}]-\widetilde{V}_{h+1}^ {k}\left(s_{h+1}^{k}\right)+V_{h+1}^{\pi_{k}}\left(s_{h+1}^{k}\right)\) and condition on the event (10) in Lemma B.8. Then for all \(k\in[K]\), \(h\in[H]\), the following holds,_

\[\widetilde{V}_{h}^{k}\left(s_{h}^{k}\right)-V_{h}^{\pi_{k}}\left(s_{h}^{k} \right)\leq\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+1}^{\pi_{k}} \left(s_{h+1}^{k}\right)+\zeta_{h+1}^{k}+\beta\left\|\phi(s_{h}^{k},a_{h}^{k}) \right\|_{(\Omega_{h}^{k})^{-1}}+\frac{1}{K^{3}}.\]

Proof of Lemma b.1.: Since \(a_{h}^{k}=\pi_{k}(s_{h}^{k})\), it implies \(V_{h}^{k}\left(x_{h}^{k}\right)=\bar{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k})\) (recall \(\bar{Q}_{h}^{k}:=\min\{\widetilde{Q}_{h}^{k},H-h+1\}\)) and \(V_{h}^{\pi_{k}}\left(x_{h}^{k}\right)=Q_{h}^{\pi_{k}}(s_{h}^{k},a_{h}^{k})\). Hence,

\[|(\widetilde{V}_{h}^{k}\left(s_{h}^{k}\right)-V_{h}^{\pi_{k}}\left( s_{h}^{k}\right))-(\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+1}^{\pi_{k}} \left(s_{h+1}^{k}\right))-\zeta_{h}^{k}|\] \[= |(\widetilde{V}_{h}^{k}\left(s_{h}^{k}\right)-V_{h}^{\pi_{k}}\left( s_{h}^{k}\right))-\mathbb{E}[\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+1}^{ \pi_{k}}\left(s_{h+1}^{k}\right)|s_{h}^{k},a_{h}^{k}]|\] \[= |\widetilde{V}_{h}^{k}\left(s_{h}^{k}\right)-r_{h}^{k}-(\mathbb{P}_ {h}\widetilde{V}_{h+1}^{k})(s_{h}^{k},a_{h}^{k})|\] \[= |\bar{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k})-r_{h}^{k}-(\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s_{h}^{k},a_{h}^{k})|\] \[\leq \beta\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{(\Omega_{h}^{k})^{ -1}}+\frac{1}{K^{3}},\]

where the last step is by the event defined in (10). 

Bounding the delayed error term \(\sum_{k=1}^{K}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{(\Omega_{h}^{k})^{-1}}\).

Recall the delayed covariance matrix \(\Omega_{h}^{k}=\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{k},a_{h}^{ \tau})\phi(s_{h}^{\tau},a_{h}^{\tau})^{\mathrm{T}}+\lambda I\) with \(\mathds{1}_{s,t}:=\mathds{1}[s+\tau_{s}\leq t]\), then we can define the full design matrix \(\Sigma_{h}^{k}\) and the complement matrix \(\Lambda_{h}^{k}\) as

\[\Sigma_{h}^{k}:=\sum_{\tau=1}^{k-1}\phi(s_{h}^{\tau},a_{h}^{\tau})\phi(s_{h}^{ \tau},a_{h}^{\tau})^{\mathrm{T}}+\lambda I,\quad\Lambda_{h}^{k}:=\sum_{\tau=1}^{k-1 }\mathds{1}[s+\tau_{s}>t]\phi(s_{h}^{\tau},a_{h}^{\tau})\phi(s_{h}^{\tau},a_{h}^{ \tau})^{\mathrm{T}},\] (9)then \(\Sigma_{h}^{k}=\Omega_{h}^{k}+\Lambda_{h}^{k}\). Also, denote the number of missing episodes as: \(U_{k}=\sum_{s=1}^{k}\mathds{1}[s+\tau_{s}>k]\). Then we have the following Lemmas.

**Lemma B.2**.: _For \(\lambda>0\), \((\Omega_{h}^{k})^{-1}=(\Sigma_{h}^{k})^{-1}+(\Sigma_{h}^{k})^{-1}\Lambda_{h}^ {k}(\Omega_{h}^{k})^{-1}\)._

Proof of Lemma b.2.: Since \(\lambda>0\), both \(\Omega_{h}^{k}\) and \(\Sigma_{h}^{k}\) are invertible with:

\[(\Omega_{h}^{k})^{-1}= (\Sigma_{h}^{k})^{-1}+(\Omega_{h}^{k})^{-1}-(\Sigma_{h}^{k})^{-1}\] \[= (\Sigma_{h}^{k})^{-1}+(\Sigma_{h}^{k})^{-1}\Sigma_{h}^{k}(\Omega_ {h}^{k})^{-1}-(\Sigma_{h}^{k})^{-1}\Omega_{h}^{k}(\Omega_{h}^{k})^{-1}\] \[= (\Sigma_{h}^{k})^{-1}+(\Sigma_{h}^{k})^{-1}\Lambda_{h}^{k}(\Omega _{h}^{k})^{-1}\]

**Lemma B.3**.: _Denote \(\phi_{h}^{k}:=\phi(s_{h}^{k},a_{h}^{k})\). Let \(\lambda>0\), then_

\[\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1} \Lambda_{h}^{k}(\Omega_{h}^{k})^{-1}}\leq\frac{1}{2}\sum_{k=1}^{K}(1+\max_{k \in[K]}U_{k}+\tau_{k})\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}}^{2}\]

Proof of Lemma b.3.: By definition and Trace of matrix, we have

\[\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}\Lambda_{h}^{k }(\Omega_{h}^{k})^{-1}}=\sqrt{(\phi_{h}^{k})^{\mathrm{T}}(\Sigma_{h}^{k})^{-1 }\Lambda_{h}^{k}(\Omega_{h}^{k})^{-1}\phi_{h}^{k}}\] \[= \sqrt{\mathrm{Tr}[(\phi_{h}^{k})^{\mathrm{T}}(\Sigma_{h}^{k})^{-1 }\Lambda_{h}^{k}(\Omega_{h}^{k})^{-1}\phi_{h}^{k}]}=\sqrt{\mathrm{Tr}[(\Sigma _{h}^{k})^{-1}\Lambda_{h}^{k}(\Omega_{h}^{k})^{-1}\phi_{h}^{k}(\phi_{h}^{k})^ {\mathrm{T}}]}\]

Denote \(A=(\Sigma_{h}^{k})^{-1}\Lambda_{h}^{k}\) and \(B=(\Omega_{h}^{k})^{-1}\phi_{h}^{k}(\phi_{h}^{k})^{\mathrm{T}}\), then \(A,B\) both have non-negative eigenvalues (by Lemma D.14) and this implies

\[\mathrm{Tr}(AB)=\mathrm{Tr}\left(AB^{1/2}B^{1/2}\right)=\mathrm{Tr}\left(B^{1 /2}AB^{1/2}\right)\leq\mathrm{Tr}\left(B^{1/2}(\mathrm{Tr}(A))IB^{1/2}\right)= \mathrm{Tr}(A)\,\mathrm{Tr}(B)\]

and this implies

\[\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}\Lambda_{h}^{k }(\Omega_{h}^{k})^{-1}}= \sqrt{\mathrm{Tr}[AB]}\leq\sqrt{\mathrm{Tr}[A]\mathrm{Tr}[B]}\leq \frac{1}{2}\mathrm{Tr}(A)+\frac{1}{2}\mathrm{Tr}(B)\] \[= \frac{1}{2}\left\|\phi_{h}^{k}\right\|_{(\Omega_{h}^{k})^{-1}}^{2 }+\frac{1}{2}\sum_{t=1}^{k-1}\mathds{1}[t+\tau_{t}>k-1]\left\|\phi_{h}^{t} \right\|_{(\Sigma_{h}^{k})^{-1}}^{2}\] \[\leq \frac{1+\max_{k\in[K]}U_{k}}{2}\left\|\phi_{h}^{k}\right\|_{( \Sigma_{h}^{k})^{-1}}^{2}+\frac{1}{2}\sum_{t=1}^{k-1}\mathds{1}[t+\tau_{t}>k-1 ]\left\|\phi_{h}^{t}\right\|_{(\Sigma_{h}^{k})^{-1}}^{2}\]

where the last inequality uses Lemma D.15. Next, by changing the order summation, we have

\[\sum_{k=1}^{K}\sum_{t=1}^{k-1}\mathds{1}[t+\tau_{t}>k-1]\left\| \phi_{h}^{t}\right\|_{(\Sigma_{h}^{t})^{-1}}^{2}=\sum_{t=1}^{K-1}\sum_{k=t+1}^ {K}\mathds{1}[t+\tau_{t}>k-1]\left\|\phi_{h}^{t}\right\|_{(\Sigma_{h}^{t})^{-1}}^ {2}\] \[= \sum_{t=1}^{K-1}\sum_{s=0}^{K-t-1}\mathds{1}[\tau_{t}>s]\left\| \phi_{h}^{t}\right\|_{(\Sigma_{h}^{t})^{-1}}^{2}\leq\sum_{t=1}^{K-1}\sum_{s=0}^ {\infty}\mathds{1}[\tau_{t}>s]\left\|\phi_{h}^{t}\right\|_{(\Sigma_{h}^{t})^{- 1}}^{2}=\sum_{t=1}^{K-1}\tau_{t}\left\|\phi_{h}^{t}\right\|_{(\Sigma_{h}^{t})^ {-1}}^{2},\]

which implies

\[\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1} \Lambda_{h}^{k}(\Omega_{h}^{k})^{-1}}\] \[\leq\frac{1+\max_{k\in[K]}U_{k}}{2}\sum_{k=1}^{K}\left\|\phi_{h} ^{k}\right\|_{(\Sigma_{h}^{k})^{-1}}^{2}+\frac{1}{2}\sum_{k=1}^{K}\sum_{t=1}^{k-1 }\mathds{1}[t+\tau_{t}>k-1]\left\|\phi_{h}^{t}\right\|_{(\Sigma_{h}^{k})^{-1}}^ {2}\] \[\leq\frac{1+\max_{k\in[K]}U_{k}}{2}\sum_{k=1}^{K}\left\|\phi_{h} ^{k}\right\|_{(\Sigma_{h}^{k})^{-1}}^{2}+\sum_{t=1}^{K-1}\tau_{t}\left\|\phi_{h}^{ t}\right\|_{(\Sigma_{h}^{t})^{-1}}^{2},\]where the second step uses \((\Sigma_{h}^{k})^{-1}\succeq(\Sigma_{h}^{t})^{-1}\) for \(k\geq t\). 

**Lemma B.4** (Bounding the delayed error).: _With probability \(1-\delta/8\),_

\[\sum_{h-1}^{H}\sum_{k=1}^{K}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{(\Omega_{ h}^{k})^{-1}}\leq H\sqrt{2dK\log((d+K)/d)}+dHD_{\tau,\delta,H,K}\log((d+K)/d).\]

_Here \(D_{\tau,\delta,H,K}:=1+2\mathbb{E}[\tau]+2\sqrt{2\mathbb{E}[\tau]\log(\frac{24 KH}{\delta})}+\frac{4}{3}\log(\frac{24KH}{\delta})+D_{\tau,K,\frac{\delta}{16 \delta H}}\) and \(D_{\tau,K,\delta}\) is defined in Lemma D.6._

Proof of Lemma b.4.: Now Combine Lemma B.2 and Lemma B.3, we obtain

\[\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Omega_{h}^{k})^{-1}} \leq\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{ -1}}+\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}\Lambda_ {h}^{k}(\Omega_{h}^{k})^{-1}}\] \[\leq\underbrace{\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{( \Sigma_{h}^{k})^{-1}}}_{(*)}+\underbrace{\frac{1}{2}\sum_{k=1}^{K}(1+\max_{k \in[K]}U_{k}+\tau_{k})\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}}^{2}}_ {(**)}.\]

For term \((*)\), since \(\lambda=1\), by Cauchy-Schwartz inequality and Elliptical Potential Lemma D.8,

\[\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}} \leq\sqrt{K\sum_{k=1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}}^ {2}}\leq\sqrt{2K\log\left(\frac{\det(\Sigma_{h}^{K+1})}{\det(\Sigma_{h}^{1})} \right)}\leq\sqrt{2dK\log((d+K)/d)}\]

For term \((**)\), by Lemma D.15 and Lemma D.6 and a union bound, with probability \(1-\delta/8\),

\[\frac{1}{2}\sum_{k=1}^{K}(1+\max_{k\in[K]}U_{k}+\tau_{k})\left\| \phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}}^{2}\] \[\leq \frac{1}{2}(1+\max_{k\in[K]}U_{k}+\max_{k\in[K]}\tau_{k})\sum_{k= 1}^{K}\left\|\phi_{h}^{k}\right\|_{(\Sigma_{h}^{k})^{-1}}^{2}\] \[\leq \frac{1}{2}(1+\max_{k\in[K]}U_{k}+\max_{k\in[K]}\tau_{k})2d\log(1 +K)\] \[\leq d(1+\mathbb{E}[\tau]+2\sqrt{2\mathbb{E}[\tau]\log(\frac{24K}{ \delta})}+\frac{4}{3}\log(\frac{24K}{\delta})+\max_{k\in[K]}\tau_{k})\log((d+ K)/d)\] \[\leq d(1+\mathbb{E}[\tau]+2\sqrt{2\mathbb{E}[\tau]\log(\frac{24K}{ \delta})}+\frac{4}{3}\log(\frac{24K}{\delta})+\mathbb{E}[\tau]+D_{\tau,\frac {\delta}{16}})\log((d+K)/d)\]

Denote \(D_{\tau,\delta,K}:=1+\mathbb{E}[\tau]+2\sqrt{2\mathbb{E}[\tau]\log(\frac{24K}{ \delta})}+\frac{4}{3}\log(\frac{24K}{\delta})+\mathbb{E}[\tau]+D_{\tau,K,\frac {\delta}{16}}\), then we have with probability \(1-\delta/8\),

\[\sum_{k=1}^{K}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{(\Omega_{h}^{k})^{-1}} \leq\sqrt{2dK\log((d+K)/d)}+dD_{\tau,\delta,K}\log((d+K)/d),\]

then apply a union bound over \(h\in[H]\) to obtained the stated result. 

### Proofs of Anti-concentration for Delayed-PSVI

In this section, we prove the optimism via anti-concentration for Delayed-PSVI. We first present two assisting lemmas.

**Lemma B.5** (Anti-concentration for Optimism).: _Suppose the event_

\[E=\left\{\left|\widetilde{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{ V}_{h+1}^{k})(s,a)\right|\leq C_{\delta^{\prime}}\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}},\;\forall s,a,h,k\right\}\]

_holds. Choose \(\nu=C_{\delta^{\prime}}\) and \(M_{\delta}=\log(HK/\delta)/\log(64/63)\). Then we have with probability \(1-\delta\),_

\[\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k})(s, a),\;\forall(s,a)\in\mathcal{S}\times\mathcal{A},h\in[H],k\in[K].\]Proof of Lemma b.5.: For the rest of the proof, we condition on the event

\[E=\left\{\left|\widehat{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+ 1}^{k})(s,a)\right|\leq C_{\delta^{\prime}}\left\|\phi(s,a)\right\|_{(\Omega_{h}^ {k})^{-1}},\;\forall s,a,h,k\right\}\]

where \(\delta^{\prime}\) will be specified later and \(C_{\delta}\) is defined in the Lemma B.10. Also note

\[\widetilde{Q}_{h}^{k,m}(s,a)-\widehat{Q}_{h}^{k}(s,a)= \phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}) \sim\mathcal{N}(0,\nu^{2}\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a))\] \[\Leftrightarrow \frac{\widetilde{Q}_{h}^{k,m}(s,a)-\widehat{Q}_{h}^{k}(s,a)}{ \sqrt{\nu^{2}\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}}\sim \mathcal{N}(0,1).\]

Therefore,

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k,m}(s,a)\geq(r_{h}+\mathbb{P }_{h}\widetilde{V}_{h+1}^{k})(s,a),\forall s,a\;\Big{|}\;\widehat{Q}_{h}^{k}\right)\] \[= \mathbb{P}\Bigg{(}\frac{\widetilde{Q}_{h}^{k,m}(s,a)-\widehat{Q} _{H}^{k}(s,a)}{\sqrt{\nu^{2}\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}}\geq\frac{(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1})(s,a)-\widehat{Q}_{h}^ {k}(s,a)}{\sqrt{\nu^{2}\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}},\;\forall s,a\;\Big{|}\;\widehat{Q}_{h}^{k}\Bigg{)}\] \[= \mathbb{P}\Bigg{(}\mathcal{N}(0,1)\geq\frac{(r_{h}+\mathbb{P}_{h }\widetilde{V}_{h+1})(s,a)-\widehat{Q}_{h}^{k}(s,a)}{\sqrt{\nu^{2}\phi(s,a)^{ \mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}},\;\forall s,a\;\Big{|}\;\widehat{Q }_{h}^{k}\Bigg{)}\] \[\geq \mathbb{P}\Bigg{(}\mathcal{N}(0,1)\geq C_{\delta^{\prime}}/\nu \Bigg{)}\] \[\geq \frac{1}{2\sqrt{8\pi}}e^{-1/2}\geq\frac{1}{64},\]

where the first event uses the condition on \(E\) and the second inequality chooses \(\nu=C_{\delta^{\prime}}\) and uses Lemma D.5. Apply Lemma B.6 with \(f=r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}\), for \(M_{\delta}=\log(1/\delta)/\log(64/63)\),

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a),\forall s,a\;\Big{|}\;\widehat{Q}_{h}^{k}\right) \geq 1-\delta.\]

By law of total expectation \(\mathbb{E}[\mathbb{E}[\mathbf{1}_{A}|X]]=\mathbb{E}[\mathbf{1}_{A}]=\mathbb{ P}[A]\), it implies

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a),\forall s,a\right)\geq 1-\delta.\]

Apply a union bound for \(h,k\), we have for \(M_{\delta}=\log(HK/\delta)/\log(64/63)\), with probability \(1-\delta\),

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a),\forall s,a,h,k\right)\geq 1-\delta.\]

The following lemma is used to prove Lemma B.5.

**Lemma B.6**.: _For any function \(f:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\). For any \(0<\delta<1\). Suppose for any \((k,h,m)\in[K]\times[H]\times[M]\), \(\mathbb{P}\left(\widetilde{Q}_{h}^{k,m}(s,a)\geq f(s,a),\forall s,a\;|\; \widehat{Q}_{h}^{k}\right)\geq c\) for some constant \(c>0\). Let \(M=\log(1/\delta)/\log(1/(1-c))\). Then_

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k}(s,a)\geq f(s,a),\forall s,a\;|\; \widehat{Q}_{h}^{k}\right)\geq 1-\delta.\]

Proof of Lemma b.6.: For any fixed \((k,h)\in[K]\times[H]\), we have

\[\mathbb{P}\left(\exists(s,a)\;s.t.\;\max_{m\in[M]}\widetilde{Q} _{h}^{k,m}(s,a)\leq f(s,a)\;|\;\widehat{Q}_{h}^{k}\right)\] \[= \mathbb{P}\left(\exists(s,a)\;s.t.\;\forall m\in[M],\;\widetilde{Q }_{h}^{k,m}(s,a)\leq f(s,a)\;|\;\widehat{Q}_{h}^{k}\right)\] \[\leq \mathbb{P}\left(\forall m\in[M],\exists(s_{m},a_{m})\;s.t.\; \widetilde{Q}_{h}^{k,m}(s_{m},a_{m})\leq f(s_{m},a_{m})\;|\;\widehat{Q}_{h}^{k}\right)\] \[= \prod_{m=1}^{M}\mathbb{P}\left(\exists(s,a)\;s.t.\;\widetilde{Q}_{ h}^{k,m}(s,a)\leq f(s,a)\;|\;\widehat{Q}_{h}^{k}\right)\] \[= \prod_{m=1}^{M}\left[1-\mathbb{P}\left(\widetilde{Q}_{h}^{k,m}(s,a) \geq f(s,a),\forall s,a\;|\;\widehat{Q}_{h}^{k}\right)\right]\leq(1-c)^{M}=\delta,\]then this implies

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k}(s,a)\geq f(s,a),\forall s,a\mid\widehat{Q} _{h}^{k}\right)\geq 1-\delta.\]

With the above two lemmas, we are ready to prove the optimism achieved by Delayed-PSVI with respect to \(\widetilde{Q}_{h}^{k}\).

**Lemma B.7** (Optimism).: _For any \(0\leq\delta<1\), we set the input in Algorithm 1 as \(\nu=C_{\delta/4}\) and \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\), then with probability \(1-\delta/2\), we have_

\[\widetilde{Q}_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a),\;\widetilde{V}_{h}^{k}(s)\geq V _{h}^{*}(s)\quad\forall s,a\in\mathcal{S}\times\mathcal{A},\forall h\in[H],k \in[K].\]

_Here \(C_{\delta}\) is defined in Lemma B.10._

Proof of Lemma b.7.: **Step1:** Suppose the event

\[E=\{\left|\widetilde{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{ h+1}^{k})(s,a)\right|\leq C_{\delta^{\prime}}\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}},\;\forall s,a,h,k\}\]

holds. Choose \(\nu=C_{\delta^{\prime}}\) and \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\). Then we show, for any \(h\in[H]\), with probability \(1-\delta/4\), \(\widetilde{Q}_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a)\), \(\widetilde{V}_{h}^{k}(s)\geq V_{h}^{*}(s)\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(h\in[H]\), \(k\in[K]\).

First, due to our choice of \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\), by Lemma B.5, with probability \(1-\delta/4\),

\[\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k})(s,a),\;\forall(s,a)\in\mathcal{S}\times\mathcal{A},h\in[H],k\in[K],\]

which we condition on.

Next, we finish the proof by backward induction. Base case: for \(h=H+1\), the value functions are zero, and thus \(\widetilde{Q}_{H+1}^{k}\geq Q_{H+1}^{*}\) holds trivially, which also implies \(\widetilde{V}_{H+1}^{k}\geq V_{H+1}^{*}\). Suppose the conclusion holds true for \(h+1\). Then for time step \(h\) and any \(k\in[K]\),

\[\widetilde{Q}_{h}^{k}-Q_{h}^{*} =\widetilde{Q}_{h}^{k}-(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{ k})+(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k})-Q_{h}^{*}\] \[\geq\widetilde{Q}_{h}^{k}-(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1 }^{k})+(r_{h}+\mathbb{P}_{H}V_{h+1}^{*})-Q_{h}^{*}\] \[=\widetilde{Q}_{h}^{k}-(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{ k})\geq 0\]

where the first inequality uses the induction hypothesis and the second inequality uses the condition. Lastly, \(\widetilde{V}_{h}^{k}(\cdot)=\max_{a}\min\{\widetilde{Q}_{h}^{k}(\cdot,a),H-h+1 \}\leq\max_{a}\min\{Q_{h}^{*}(\cdot,a),H-h+1\}=\max_{a}Q_{h}^{*}(\cdot,a)=V_{h} ^{*}(\cdot)\). By induction, this finishes the Step1.

**Step2:** By Lemma B.10, with probability \(1-\delta/4\), for all \(k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\), it holds

\[\left|\widetilde{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^ {k})(s,a)\right|\leq C_{\delta/4}\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{ -1}}.\]

Therefore, in Step1, choose \(\delta^{\prime}=\delta/4\), and a union bound we obtain: for the choice \(\nu=C_{\delta/4}\) and \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\), then with probability \(1-\delta/2\), we have

\[\widetilde{Q}_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a),\;\widetilde{V}_{h}^{k}(s)\geq V _{h}^{*}(s)\;\forall(s,a)\in\mathcal{S}\times\mathcal{A},\;h\in[H],\;k\in[K].\]

### Proofs of Concentration for Delayed-PSVI

**Lemma B.8** (Pointwise Concentration).: _Algorithm 1 guarantees that with probability \(1-\delta\), \(\forall k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\), it holds:_

\[\left|\left|\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-(r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a)\right|\leq\beta\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}}+\frac{1}{K^{3}}\] (10)

_where \(\beta:=\sqrt{2\nu^{2}\log(4C_{d}HMK/\delta)}+\sqrt{8H^{2}\left[\frac{\delta} {2}\log\left(\frac{k+\lambda}{\lambda}\right)+dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,M}}{H\sqrt{\lambda}})+\log\frac{4}{\delta}\right]}+2\sqrt{\lambda} \sqrt{d}H\). In particular, here \(\log C_{d}=d\log(1+(8\sqrt{2\nu^{2}\log(4/\delta)/\lambda}+8H\sqrt{d})K^{3})\) and \(C_{H,d,k,M,\delta}=2H\sqrt{\frac{dk}{\lambda}}+\frac{\nu\sqrt{2}d+\nu\sqrt{2 }\log(M/\delta)}{\sqrt{\lambda}}\)._Proof of Lemma b.8.: Recall that \(|r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}|\leq H-h+1\), therefore \(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}=\min\{r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k},H-h+1\}\). This implies \(|\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{ V}_{h+1}^{k}](s,a)|=|\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-\min\{r_{h}^{k}+[ \mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a),H-h+1\}|\leq|\widetilde{Q}_{h}^{k }(s,a)-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)|\). Hence

\[\Big{|}\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-r_{h}^{k}-[ \mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)\Big{|}\leq\Big{|}\widetilde{Q}_{h }^{k}(s,a)-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)\Big{|}\] \[=\Big{|}\widetilde{Q}_{h}^{k}(s,a)-\widehat{Q}_{h}^{k}(s,a)+ \widehat{Q}_{h}^{k}(s,a)-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)\Big{|}\] \[\leq\underbrace{\Big{|}\widetilde{Q}_{h}^{k}(s,a)-\widehat{Q}_{h }^{k}(s,a)}_{R_{1}}\Big{|}+\underbrace{\Big{|}\widehat{Q}_{h}^{k}(s,a)-r_{h}^ {k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)}_{R_{2}}.\]

The proof then directly follows Lemma B.9 and Lemma B.10 to bound \(R_{1}\) and \(R_{2}\) respectively (together with a union bound). 

**Lemma B.9** (Concentration of \(R_{1}\)).: _For any \(0<\delta<1\), define the event \(\widetilde{E}\) as_

\[\widetilde{E}=\Big{\{}\Big{|}\widetilde{Q}_{h}^{k}(s,a)-\phi(s,a )^{\mathrm{T}}\widehat{u}_{h}^{k}\Big{|} \leq\sqrt{2\nu^{2}\log(2C_{d}HMK/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k})^{ -1}}+\frac{1}{K^{3}},\] \[\forall k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\Big{\}},\] (11)

_then \(\widetilde{E}\) happens with probability \(1-\delta\). Here \(\log C_{d}=d\log(1+(8\sqrt{2\nu^{2}\log(2/\delta)/\lambda}+8H\sqrt{d})K^{3})\)._

Proof of Lemma b.9.: In the Step1 and Step2, we abuse \(\widetilde{w}_{h}^{k}\) to denote \(\widetilde{w}_{h}^{k,m}\) for arbitrary \(m\) to avoid notation redundancy.

In **Step1**: We first show for any \(k\in[K],h\in[H],(s,a)\in\mathcal{S}\times\mathcal{A}\), with probability \(1-\delta\),

\[\big{|}\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widetilde{w}_{h}^{k}) \big{|}\leq\sqrt{2\nu^{2}\log(2/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k})^{-1} }\,.\]

Indeed, by design of Algorithm 1, \(\widetilde{w}_{h}^{k}\sim\mathcal{N}(\widehat{w}_{h}^{k},\nu^{2}(\Omega_{h}^ {k})^{-1})\), which gives,

\[\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widetilde{w}_{h}^{k})\sim \mathcal{N}(0,\nu^{2}\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)).\]

Therefore, \(\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widetilde{w}_{h}^{k})\) is \(\nu^{2}\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)\)-sub-Gaussian. By concentration of sub-Gaussian random variables, we have

\[\mathbb{P}\left(\Big{|}\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widetilde {w}_{h}^{k})\Big{|}\geq t\right)\leq 2\exp\left(-\frac{t^{2}}{2\nu^{2}\phi(s,a)^{ \mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}\right):=\delta\]

Solving for \(\delta\) gives with probability \(1-\delta\),

\[\big{|}\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widetilde{w}_{h}^{k}) \big{|}\leq\sqrt{2\nu^{2}\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a) \log(2/\delta)}=\sqrt{2\nu^{2}\log(2/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k} )^{-1}}\]

**Step2:** For any \(0<\delta<1\), define the event \(\widetilde{E}\) as

\[\widetilde{E}=\Big{\{}\Big{|}\phi(s,a)^{\mathrm{T}}\widetilde{w}_ {h}^{k}-\phi(s,a)^{\mathrm{T}}\widehat{w}_{h}^{k}\Big{|} \leq\sqrt{2\nu^{2}\log(2C_{d}HK/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k} )^{-1}}+\frac{1}{K^{3}},\] \[\forall k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\Big{\}},\] (12)

then \(\widetilde{E}\) happens with probability \(1-\delta\). Here \(\log C_{d}=d\log(1+(8\sqrt{2\nu^{2}\log(2/\delta)/\lambda}+8H\sqrt{d})K^{3})\).

In Lemma D.12, set \(\theta=\widetilde{w}_{h}^{k}-\widetilde{w}_{h}^{k}\) and \(A=(\Omega_{h}^{k})^{-1}\) and \(B=1/\lambda\), and let \(\mathcal{V}\) be the \(\frac{1}{2K^{3}}\)-epsilon net for the class of values \(\{|\langle\phi,\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle|-C\sqrt{\phi^{ \top}(\Omega_{h}^{k})^{-1}\phi}:\|\phi\|\leq 1\}\) (where \(C=\sqrt{2\nu^{2}\log(2/\delta)}\)), then it must also be the \(\frac{1}{2K^{3}}\)-epsilon net for the class of values \(\mathcal{F}=\{|\langle\phi(s,a),\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k} \rangle|-C\sqrt{\phi(s,a)^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}:(s,a)\in\mathcal{ S}\times\mathcal{A}\}\), let \(\bar{\mathcal{V}}\) is the smallest subset of \(\mathcal{V}\) such that it is \(\frac{1}{2K^{3}}\)-epsilon net for the class of values \(\mathcal{F}\). Then we can select \(\mathcal{V}_{\mathcal{S}\times\mathcal{A}}\) to be the set of state-action pairs such that for any \(f_{\phi}:=|\langle\phi,\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle|-C \sqrt{\phi^{\top}(\Omega_{h}^{k})^{-1}\phi}\in\bar{\mathcal{V}}\), there exists \((s,a)\in\mathcal{V}_{\mathcal{S}\times\mathcal{A}}\) satisfies \(|\langle\phi(s,a),\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle|C\sqrt{ \phi(s,a)^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}|-f_{\phi}\leq 1/2K^{3}\), then we have \(\mathcal{V}_{\mathcal{S}\times\mathcal{A}}\) is a \(1/K^{3}\)-epsilon net of \(\mathcal{F}\) and \(|\mathcal{V}_{\mathcal{S}\times\mathcal{A}}|\leq|\bar{\mathcal{V}}|\leq| \mathcal{V}|\). Therefore,

\[\sup_{s,a}|\langle\phi(s,a),\widetilde{w}_{h}^{k}-\widehat{w}_{h }^{k}\rangle|-C\sqrt{\phi(s,a)^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}\] \[\leq \sup_{(s,a)\in\mathcal{V}_{\mathcal{S}\times\mathcal{A}}}|\langle \phi(s,a),\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle|-C\sqrt{\phi(s,a) ^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}+1/K^{3}\]

Then by a union bound over \((1+(8\sqrt{2\nu^{2}\log(2/\delta)/\lambda}+8H\sqrt{d})K^{3})^{d}\), \(H\) and \(K\), we have the stated the result.

**Step3:** Note \(\widetilde{Q}_{h}^{k}=\max_{m}\phi^{\mathrm{T}}\widetilde{w}_{h}^{k,m}\), hence by a union bound over \(M\), we have

\[\left|\widetilde{Q}_{h}^{k}(s,a)-\phi(s,a)^{\mathrm{T}}\widehat{w }_{h}^{k}\right|=|\max_{m}\phi(s,a)^{\mathrm{T}}\widetilde{w}_{h}^{k,m}-\phi( s,a)^{\mathrm{T}}\widehat{w}_{h}^{k}|\] \[\leq \max_{m}|\phi(s,a)^{\mathrm{T}}\widetilde{w}_{h}^{k,m}-\phi(s,a) ^{\mathrm{T}}\widehat{w}_{h}^{k}|\] \[\leq \sqrt{2\nu^{2}\log(2C_{d}HMK/\delta)}\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}}+\frac{1}{K^{3}}\]

for all \(k,h,s,a\) with probability \(1-\delta\). Here the last inequality follows Step2, which completes the proof. 

**Lemma B.10** (Concentration of \(R_{2}\)).: _For any \(0<\delta<1\), with probability \(1-\delta\), for all \(k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\), it holds_

\[\left|\widetilde{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^ {k})(s,a)\right|\leq C_{\delta}\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}\]

_where \(C_{\delta}=\sqrt{8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,\delta}}{H\sqrt{\lambda}})+ \log\frac{2}{\delta}\right]}+2\sqrt{\lambda}\sqrt{d}H\) and the quantity \(C_{H,d,k,M,\delta}=2H\sqrt{\frac{dk}{\lambda}}+\frac{\nu\sqrt{2d}+\nu\sqrt{2 \log(M/\delta)}}{\sqrt{\lambda}}\)._

Proof of Lemma b.10.: For any \((k,h)\in[K]\times[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\), denote

\[\phi(s,a)^{\mathrm{T}}w_{h}^{k}:=(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^ {k})(s,a),\text{where }\ w_{h}^{k}:=\theta_{h}+\int_{\mathcal{S}}\widetilde{V}_{h+1}^{k}(s^{ \prime})\mathrm{d}\mu_{h}(s^{\prime}).\]

Recall \(y_{h}^{\tau}=\mathds{1}_{\tau,k-1}\cdot[r_{h}^{\tau}(s_{h}^{\tau},a_{h}^{\tau} )+\widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})]\) from Algorithm 1 and denote \(\bar{y}_{h}^{\tau}:=r_{h}^{\tau}(s_{h}^{\tau},a_{h}^{\tau})+\widetilde{V}_{h+1} ^{k}(s_{h+1}^{\tau})\). Then by definition,

\[\widehat{w}_{h}^{k}=(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k- 1}\cdot\phi(s_{h}^{\tau},a_{h}^{\tau})y_{h}^{\tau}=(\Omega_{h}^{k})^{-1}\sum_{ \tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\bar{y}_{h }^{\tau}.\]

From \(\Omega_{h}^{k}\) defined in line 7 of Algorithm 1, we have \(\Phi_{h}\Phi_{h}^{\mathrm{T}}=\Omega_{h}^{k}-\lambda I\). Plug it into the definition of \(\widehat{w}_{h}^{k}\), we have

\[\widehat{w}_{h}^{k} =(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1} \cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\left(\bar{y}_{h}^{\tau}-\phi(s_{h}^{\tau},a _{h}^{\mathrm{T}})^{\mathrm{T}}w_{h}^{k}+\phi(s_{h}^{\tau},a_{h}^{\mathrm{T}})^ {\mathrm{T}}w_{h}^{k}\right)\] \[=(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1} \cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\left(\bar{y}_{h}^{\tau}-\phi(s_{h}^{\tau},a _{h}^{\mathrm{T}})^{\mathrm{T}}w_{h}^{k}\right)+(\Omega_{h}^{k})^{-1}\left( \Omega_{h}^{k}-\lambda I\right)w_{h}^{k}.\]We then proceed to bound \(\widehat{w}_{h}^{k}-w_{h}^{k}\), which gives

\[\widehat{w}_{h}^{k}-w_{h}^{k} =(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot \phi(s_{h}^{\tau},a_{h}^{\tau})\left(\bar{y}_{h}^{\tau}-\phi(s_{h}^{\tau},a_{h}^ {\tau})^{\mathrm{T}}w_{h}^{k}\right)-\lambda(\Omega_{h}^{k})^{-1}w_{h}^{k}\] \[=\underbrace{(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{ \tau,k-1}\cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\left(\widetilde{V}_{h+1}^{k}(s_{ h+1}^{\tau})-\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}(s_{h}^{\tau},a_{h}^{ \tau})\right)}_{(\mathrm{ii})}-\underbrace{\lambda(\Omega_{h}^{k})^{-1}w_{h}^ {k}}_{(\mathrm{ii})}.\]

**Term (i).** Since \(\Omega_{h}^{k}\) is positive definite, multiplying the first term \((i)\) with \(\phi(s,a)\) and by Cauchy-Schwartz inequality, we obtain,

\[\left|\phi(s,a)^{\mathrm{T}}(\mathrm{i})\right|\leq\left\|\phi(s,a)\right\|_{ (\Omega_{h}^{k})^{-1}}\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot \phi(s_{h}^{\tau},a_{h}^{\tau})\left(\widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})- \mathbb{P}_{h}\widetilde{V}_{h+1}^{k}(s_{h}^{\tau},a_{h}^{\tau})\right)\right\| _{(\Omega_{h}^{k})^{-1}}.\]

Apply Lemma B.11, we have with probability at least \(1-\delta\), for any \((k,h)\in[K]\times[H]\), and \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\left|\phi(s,a)^{\mathrm{T}}(\mathrm{i})\right|\leq C_{1}\left\|\phi(s,a) \right\|_{(\Omega_{h}^{k})^{-1}},\] (13)

where \(C_{1}=\sqrt{8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda}\right)+ dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,\delta}}{H\sqrt{\lambda}})+\log\frac{2}{ \delta}\right]}\).

**Term (ii).** By Lemma B.12, \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\), and \((k,h)\in[K]\times[H]\), \(\left|\phi(s,a)^{\mathrm{T}}(\mathrm{ii})\right|\) can be bounded as

\[\left|\phi(s,a)^{\mathrm{T}}(\mathrm{ii})\right|=\lambda\left|\phi(s,a)^{ \mathrm{T}}(\Omega_{h}^{k})^{-1}w_{h}^{k}\right|\leq 2\sqrt{\lambda}\sqrt{d}H \left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}.\] (14)

Combining (13), (14), we have with probability \(1-\delta\), for any \((k,h)\in[K]\times[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\left|\widetilde{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a)\right| =\left|\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-w_{h}^{k}) \right|\leq\left|\phi(s,a)^{\mathrm{T}}(\mathrm{i})\right|+\left|\phi(s,a)^{ \mathrm{T}}(\mathrm{ii})\right|\] \[\leq(C_{1}+2\sqrt{\lambda}\sqrt{d}H)\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}},\]

This concludes the proof. 

**Lemma B.11**.: _For any \(0<\delta<1\), with probability \(1-\delta\), we have \(\forall(k,h)\in[K]\times[H]\),_

\[\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot\phi(s_{h}^{ \tau},a_{h}^{\tau})\left(\widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})-\mathbb{P}_{h} \widetilde{V}_{h+1}^{k}(s_{h}^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h} ^{k})^{-1}}^{2}\] \[\leq 8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda}\right) +dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,\delta}}{H\sqrt{\lambda}})+\log\frac{ 2}{\delta}\right],\]

_here \(C_{H,d,k,M,\delta}=2H\sqrt{\frac{dk}{\lambda}}+\frac{\nu\sqrt{2d}+\nu\sqrt{2 \log(M/\delta)}}{\sqrt{\lambda}}\).8_

Footnote 8: Note here \(\nu\) is in the line 10 of Algorithm 1. At the end we will choose \(\nu\) to be Poly(\(H,d,K\)) and this will not affect the overall dependence of the guarantee since \(C_{H,d,k,M,\delta}\) is inside the log term.

Proof of Lemma b.11.: First note that

\[\widetilde{V}_{h}^{k}(\cdot):=\max_{a}\min\{\widetilde{Q}_{h}^{k }(\cdot,a),(H-h+1)\} =\max_{a}\min\max_{m}\{\widetilde{Q}_{h}^{k,m},(H-h+1)\}\] \[=\max_{a}\min\{\max_{m}\phi(\cdot,a)^{\mathrm{T}}\widetilde{w}_{h }^{k,m},(H-h+1)\}.\]

Recall that \((\Omega_{h}^{k})^{1/2}(\widetilde{w}_{h}^{k,m}-\widehat{w}_{h}^{k})/\nu\sim \mathcal{N}(0,I_{d})\), then by Lemma D.7, with probability \(1-\delta/2\), we have

\[\frac{\sqrt{\lambda}}{\nu}\left\|\widetilde{w}_{h}^{k,m}-\widehat{w}_{h}^{k} \right\|\leq\frac{1}{\nu}\left\|(\Omega_{h}^{k})^{1/2}(\widetilde{w}_{h}^{k,m }-\widehat{w}_{h}^{k})\right\|\leq\sqrt{2d}+\sqrt{2\log(1/\delta)}.\]Apply the union bound over all \(m\), then above implies with probability \(1-\delta/2\), \(\forall m\in[M]\)

\[\left\|\widetilde{w}_{h}^{k,m}\right\|\leq\left\|\widehat{w}_{h}^{k}\right\|+ \frac{\nu\sqrt{2d+\nu}\sqrt{2\log(M/\delta)}}{\sqrt{\lambda}}\leq 2H\sqrt{ \frac{dk}{\lambda}}+\frac{\nu\sqrt{2d+\nu}\sqrt{2\log(M/\delta)}}{\sqrt{ \lambda}}:=C_{H,d,k,M,\delta}.\] (15)

Now consider the function class \(\bar{\mathcal{V}}:=\{\max_{a}\max_{m}\phi(\cdot,a)^{\mathrm{T}}w^{m}:\,\|w^{m} \|\leq C_{H,d,k,M,\delta}\}\), so by Lemma D.13 the \(\epsilon\)-log covering number for \(\bar{\mathcal{V}}\) is \(dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})\). Since \(\min\{\cdot,\cdot\}\) is a non-expansive operator, the \(\epsilon\)-log covering number for the function class \(\mathcal{V}:=\{\max_{a}\min\{\max_{m}\phi(\cdot,a)^{\mathrm{T}}w^{m},(H-h+1) \}:\|w^{m}\|\leq C_{H,d,k,M,\delta}\}\), is at most \(dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})\). Hence, for any \(V\in\mathcal{V}\), there exists \(V^{\prime}\) in the \(\epsilon\)-covering such that \(V=V^{\prime}+\Delta_{V}\) with \(\left\|\Delta_{V}\right\|_{\infty}\leq\epsilon\). Then with probability \(1-\delta/2\),

\[\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau}, a_{h}^{\tau})\left(V(s_{h+1}^{\tau})-\mathbb{P}_{h}V(s_{h}^{\tau},a_{h}^{\tau}) \right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] (16) \[\leq 2\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau },a_{h}^{\tau})\left(V^{\prime}(s_{h+1}^{\tau})-\mathbb{P}_{h}V^{\prime}(s_{h} ^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] \[+2\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau },a_{h}^{\tau})\left(\Delta_{V}(s_{h+1}^{\tau})-\mathbb{P}_{h}\Delta_{V}(s_{h} ^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] \[\leq 2\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau },a_{h}^{\tau})\left(V^{\prime}(s_{h+1}^{\tau})-\mathbb{P}_{h}V^{\prime}(s_{h} ^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}+\frac{8k^{2} \epsilon^{2}}{\lambda}\] \[\leq 4H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})+\log\frac{2}{\delta} \right]+\frac{8k^{2}\epsilon^{2}}{\lambda}\]

where the second inequality can be conducted using a direct calculation and the third inequality uses Lemma D.9 and a union bound over the covering number. Now by (15) and (16) and a union bound, we have for any \(\epsilon>0\), with probability \(1-\delta\),

\[\left\|\sum_{\tau=1}^{k-1}\phi(s_{h}^{\tau},a_{h}^{\tau})\left( \widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})-\mathbb{P}_{h}\widetilde{V}_{h+1}^{k} (s_{h}^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] \[\leq 4H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})+\log\frac{2}{\delta} \right]+\frac{8k^{2}\epsilon^{2}}{\lambda}\] \[\leq 8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,\delta}}{H\sqrt{\lambda}})+ \log\frac{2}{\delta}\right],\]

where the last step choose \(\epsilon^{2}=H^{2}\lambda/8k^{2}\) so \(\frac{8k^{2}\epsilon^{2}}{\lambda}\leq 4H^{2}\). Lastly, apply the union bound over \(H,K\) to obtain the stated result. 

**Lemma B.12**.: \(\forall(s,a)\in\mathcal{S}\times\mathcal{A},h\in[H],k\in[K]\)_, it holds that_

\[\left|\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}w_{h}^{k}\right|\leq\frac{2}{ \sqrt{\lambda}}\sqrt{dH}\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}.\]

Proof of Lemma b.12.: Note that the precision matrix \(\Omega_{h}^{k}\) for any step \(h\) and episode \(k\) is always positive definite. By Cauchy-Schwartz inequality and Lemma D.1,

\[\left|\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}w_{h}^{k}\right| =\left|\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1/2}(\Omega_{h}^{ k})^{-1/2}w_{h}^{k}\right|\] \[\leq\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}\left\|w_{h}^{ k}\right\|_{(\Omega_{h}^{k})^{-1}}\] \[\leq\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}\sqrt{\left\| w_{h}^{k}\right\|^{2}\left\|(\Omega_{h}^{k})^{-1}\right\|}\] \[\leq\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}\left\|w_{h}^{ k}\right\|\frac{1}{\sqrt{\lambda_{\min}(\Omega_{h}^{k})}}\]

Note that \(\lambda_{\min}(\Omega_{h}^{k})\geq\lambda\). Applying Lemma D.3 for \(\left\|w_{h}^{k}\right\|\) completes the proof.

Regret Analysis for Delayed-LPSVI

Proof of Theorem 2.: The proof structure is similar to that of Theorem 1. We proceed by bounding \(\Delta_{opt}^{k}\) and \(\Delta_{est}^{k}\) respectively.

**Step 1: bound regret from optimism.** By Lemma C.5, with probability \(1-\delta/2\),

\[\Delta_{opt}^{k}:=V_{1}^{*}(s_{1}^{k})-\widetilde{V}_{1}^{k}(s_{1}^{k})\leq 0,\quad\forall k\in[K].\]

**Step 2: bound regret from estimation error.** We first condition on the event that

\[\mathcal{E}:=\{\|\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-(r_{h}^{k}+\mathbb{ P}_{h}\widetilde{V}_{h+1}^{k})(s,a)|\leq\beta\,\|\phi(s,a)\|_{(\Omega_{h}^{k})^{-1}}+ \frac{1}{K^{3}},\,\forall s,a,h,k\},\]

with \(\beta:=\sqrt{2\gamma\log(16C_{d}HMK/\delta)}+\sqrt{8H^{2}\left[\frac{d}{2} \log\left(\frac{k+\lambda}{\lambda}\right)+dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d, k,M,S/\delta}}{H\sqrt{\lambda}})+\log\frac{16}{\delta}\right]}+2\sqrt{ \lambda}\sqrt{d}H\). Here \(C_{d}\) and \(C_{H,d,k,M,\delta}\) are defined in Lemma C.6.

Similarly, define

\[\zeta_{h}^{k}=\mathbb{E}[\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+ 1}^{\pi_{k}}\left(s_{h+1}^{k}\right)|s_{h}^{k},a_{h}^{k}]-\widetilde{V}_{h+1} ^{k}\left(s_{h+1}^{k}\right)+V_{h+1}^{\pi_{k}}\left(s_{h+1}^{k}\right).\]

Then by Lemma C.1,

\[\sum_{k=1}^{K}\Delta_{est}^{k}=\sum_{k=1}^{K}\widetilde{V}_{1}^{k }(s_{1}^{k})-V_{1}^{\pi_{k}}(s_{1}^{k})\] (17) \[\leq \sum_{k=1}^{K}\left(\widetilde{V}_{2}^{k}\left(s_{2}^{k}\right)- V_{2}^{\pi_{k}}\left(s_{2}^{k}\right)+\zeta_{1}^{k}+\beta\left\|\phi(s_{1}^{k},a_{1 }^{k})\right\|_{(\Omega_{1}^{k})^{-1}}+\frac{1}{K^{3}}\right)\] \[\leq \sum_{k=1}^{K}\sum_{h=1}^{H}\zeta_{h}^{k}+\beta\sum_{k=1}^{K}\sum _{h=1}^{H}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{(\Omega_{h}^{k})^{-1}}+ \frac{H}{K^{2}}.\]

By definition, \(|\zeta_{h}^{k}|\leq 2H\) for all \(h\in[H],k\in[K]\), therefore \(\{\zeta_{h}^{k}\}\) is a martingale difference sequence. By Azuma-Hoeffding's inequality,

\[\mathbb{P}\left(\sum_{k=1}^{K}\sum_{h=1}^{H}\zeta_{h}^{k}>t\right)\geq\exp \left(\frac{-t^{2}}{2K\cdot H^{3}}\right):=\delta/8,\quad\forall t>0.\]

Thus, with probability \(1-\delta/8\),

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\zeta_{h}^{k}\leq\sqrt{2KH^{3}\cdot\log(8/\delta) }=\sqrt{2H^{2}T\cdot\log(8/\delta)}.\] (18)

**Step 3: bounding the delayed error.** By Lemma B.4, with probability \(1-\delta/8\),

\[\beta\sum_{k=1}^{K}\sum_{h=1}^{H}\left\|\phi(s_{h}^{k},a_{h}^{k})\right\|_{( \Omega_{h}^{k})^{-1}}\leq\beta H\sqrt{2dK\log((d+K)/d)}+\beta dHD_{\tau,\delta,H,K}\log((d+K)/d).\] (19)

Here \(D_{\tau,\delta,H,K}:=1+2\mathbb{E}[\tau]+2\sqrt{2\mathbb{E}[\tau]\log(\frac{ 24KH}{\delta})}+\frac{4}{3}\log(\frac{24KH}{\delta})+D_{\tau,K,\frac{\delta}{ 16H}}\) and \(D_{\tau,K,\delta}\) is defined in Lemma D.6. By Lemma C.6, event \(\mathcal{E}\) holds with probability \(1-\delta/4\), by a union bound with (18) and (19), we have with probability \(1-\delta/2\),

\[\sum_{k=1}^{K}\Delta_{est}^{k}\leq\sqrt{2H^{2}T\cdot\log(8/\delta)}+\beta H \sqrt{2dK\log((d+K)/d)}+\beta dHD_{\tau,\delta,H,K}\log((d+K)/d)+\frac{H}{K^{2}}.\]

Finally, by a union bound over Step1, Step2 and Step3, we obtain with probability \(1-\delta\),

\[R(T)= \sum_{k=1}^{K}\Delta_{opt}^{k}+\sum_{k=1}^{K}\Delta_{est}^{k} \leq\sum_{k=1}^{K}\Delta_{est}^{k}\] \[\leq \sqrt{2H^{2}T\cdot\log(8/\delta)}+\beta H\sqrt{2dK\log((d+K)/d)}+ \beta dHD_{\tau,\delta,H,K}\log((d+K)/d)+\frac{H}{K^{2}}\] \[\leq c\sqrt{d^{3}H^{3}Tt}+c^{\prime}d^{2}H^{2}\mathbb{E}[\tau]_{t}+O(\iota)\]where \(c>0\) is some universal constant and \(\iota\) is a Polylog term of \(H,d,K,\delta\). Similarly, we can bound \(\beta\leq CdH\iota_{\delta}\) for some universal constant \(C\), and it is readily to verify \(D_{\tau,\delta,H,K}\) is bounded by \(c^{\epsilon}\mathbb{E}[\tau]\iota+O(\iota)\). 

**Lemma C.1**.: _Define \(\zeta_{h}^{k}=\mathbb{E}[\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+ 1}^{\pi_{k}}\left(s_{h+1}^{k}\right)|s_{h}^{k},a_{h}^{k}]-\widetilde{V}_{h+1} ^{k}\left(s_{h+1}^{k}\right)+V_{h+1}^{\pi_{k}}\left(s_{h+1}^{k}\right)\) and condition on the event (21) in Lemma C.6. Then for all \(k\in[K]\), \(h\in[H]\), the following holds,_

\[\widetilde{V}_{h}^{k}\left(s_{h}^{k}\right)-V_{h}^{\pi_{k}}\left(s_{h}^{k} \right)\leq\widetilde{V}_{h+1}^{k}\left(s_{h+1}^{k}\right)-V_{h+1}^{\pi_{k}} \left(s_{h+1}^{k}\right)+\zeta_{h+1}^{k}+\beta\left\|\phi(s_{h}^{k},a_{h}^{k} )\right\|_{(\Omega_{h}^{k})^{-1}}+\frac{1}{K^{3}}.\]

Proof of Lemma c.1.: By the event defined in (21), the proof follows exactly as in that of Lemma B.1. 

### Convergence of Langevin Monte Carlo

The following lemma is crucial to prove the optimism and bound the error in Langevin analysis. For ease of notation, within the episode \(k\), we simply use \(\eta\) to denote \(\eta_{k}\) for conciseness.

**Lemma C.2** (Convergence of LMC).: _Denote \(\{\widetilde{w}_{h}^{k,m}\}_{m\in[M]}\) to be the weights returned by Line 6 of Algorithm 2. Set \(\eta=\frac{1}{4\lambda_{\max}(\Omega_{h}^{k})}\), we have_

\[\widetilde{w}_{h}^{k,m}\sim\mathcal{N}(A_{h,k}^{N_{k}}w_{0}+(I-A_{h,k}^{N_{k} })\widetilde{w}_{h}^{k},\Theta_{h}^{k})\quad\forall m\in[M]\]

_where_

\[A_{h,k} :=I-2\eta\Omega_{h}^{k}\] \[\Omega_{h}^{k} :=\lambda I+\sum_{k=1}^{K}\phi_{h}(s_{h}^{k},a_{h}^{k})\phi_{h}( s_{h}^{k},a_{h}^{k})^{T}\] \[\widetilde{w}_{h}^{k} :=(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\phi_{h}(s_{h}^{\tau},a _{h}^{\tau})y_{h}^{\tau}\] \[\Theta_{h}^{k} :=\gamma(I-A_{h,k}^{2N_{k}})(\Omega_{h}^{k})^{-1}(I+A_{h,k})^{-1}.\]

_Furthermore, we have_

\[\frac{\gamma}{2}\left(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}}\right)(\Omega_{h}^ {k})^{-1}\prec\Theta_{h}^{k}\prec\gamma(\Omega_{h}^{k})^{-1},\]

_where \(\kappa_{h}:=\frac{\lambda_{\max}(\Omega_{h}^{k})}{\lambda_{\min}(\Omega_{h}^ {k})}\) is the condition number._

Proof of Lemma c.2.: Let \(b_{h}^{k}:=\sum_{\tau=1}^{k-1}\phi(s_{h}^{\tau},a_{h}^{\tau})y_{h}^{\tau}\), then

\[\nabla L_{h}^{k}(w)=2\Omega_{h}^{k}w-2b_{h}^{k}.\]

Therefore, fix \(h,k,m\), and within the Algorithm 3 we have

\[w_{N}= w_{N-1}-2\eta(\Omega_{h}^{k}\cdot w_{N-1}-b_{h}^{k})+\sqrt{2\eta \gamma}\epsilon_{N}\] \[= (I-2\eta\Omega_{h}^{k})w_{N-1}+2\eta b_{h}^{k}+\sqrt{2\eta\gamma }\epsilon_{N}\] \[= A_{h,k}w_{N-1}+2\eta b_{h}^{k}+\sqrt{2\eta\gamma}\epsilon_{N}\] \[= A_{h,k}^{N}w_{0}+2\eta\sum_{l=0}^{N-1}A_{h,k}^{l}b_{h}^{k}+\sqrt {2\eta\gamma}\sum_{l=0}^{N-1}A_{h,k}^{l}\epsilon_{N-l}\] \[= A_{h,k}^{N}w_{0}+(I-A_{h,k}^{N})\widetilde{w}_{h}^{k}+\sqrt{2 \eta\gamma}\sum_{l=0}^{N-1}A_{h,k}^{l}\epsilon_{N-l}\]

where the last equality uses \((\Omega_{h}^{k})^{-1}b_{h}^{k}=\widetilde{w}_{h}^{k}\) and \(I\succ I-2\eta\Omega_{h}^{k}\succ\bm{0}\), so \(\sum_{l=0}^{N-1}A^{l}=(I-A^{N})(I-A)^{-1}\). Since \(\epsilon_{i}\) are i.i.d gaussian noise, from the above we directly have

\[w_{N}\sim\mathcal{N}(A_{h,k}^{N}w_{0}+(I-A_{h,k}^{N})\widehat{w}_{h}^{k}, \Theta_{h}^{k})\]where

\[\Theta_{h}^{k}= \mathrm{Cov}[\sqrt{2\eta\gamma}\sum_{l=0}^{N-1}A_{h,k}^{l}\epsilon_{N -l}]=2\eta\gamma\cdot\mathrm{Cov}[\sum_{l=0}^{N-1}A_{h,k}^{l}\epsilon_{N-l}]\] \[= 2\eta\gamma\cdot\sum_{l=0}^{N-1}A_{h,k}^{2l}=2\eta\gamma(I-A_{h, k}^{2N})(I-A_{h,k}^{2})^{-1}\] \[= \gamma(I-A_{h,k}^{2N_{k}})(\Omega_{h}^{k})^{-1}(I+A_{h,k})^{-1}.\]

Next, due to the choice of \(\eta=\frac{1}{4\lambda_{\mathrm{max}}(\Omega_{h}^{k})}\), we have

\[\frac{1}{2}I\prec A_{h,k}=I-2\eta\Omega_{h}^{k}\prec(1-2\eta \lambda_{\mathrm{min}}(\Omega_{h}^{k}))I\] (20) \[\Rightarrow \frac{1}{2^{2N}}I\prec A_{h,k}^{2N}\prec(1-2\eta\lambda_{ \mathrm{min}}(\Omega_{h}^{k}))^{2N}I\] \[\Rightarrow \left(1-(1-2\eta\lambda_{\mathrm{min}}(\Omega_{h}^{k}))^{2N} \right)I\prec I-A_{h,k}^{2N}\prec(1-\frac{1}{2^{2N}})I\]

In addition,

\[\frac{1}{2}I\prec A_{h,k}=I-2\eta\Omega_{h}^{k}\prec(1-2\eta \lambda_{\mathrm{min}}(\Omega_{h}^{k}))I\] \[\Rightarrow \frac{3}{2}I\prec I+A_{h,k}\prec(2-2\eta\lambda_{\mathrm{min}}( \Omega_{h}^{k}))I\] \[\Rightarrow \frac{1}{2-2\eta\lambda_{\mathrm{min}}(\Omega_{h}^{k})}I\prec(I+ A_{h,k})^{-1}\prec\frac{2}{3}I\]

The above two implies

\[\gamma\frac{\left(1-(1-2\eta\lambda_{\mathrm{min}}(\Omega_{h}^{k} ))^{2N}\right)}{2-2\eta\lambda_{\mathrm{min}}(\Omega_{h}^{k})}(\Omega_{h}^{k} )^{-1}\prec\Theta_{h}^{k}\prec\gamma\frac{2}{3}(1-\frac{1}{2^{2N}})(\Omega_{ h}^{k})^{-1}\] \[\Rightarrow \gamma\frac{\left(1-(1-2\eta\lambda_{\mathrm{min}}(\Omega_{h}^{ k}))^{2N}\right)}{2}(\Omega_{h}^{k})^{-1}\prec\Theta_{h}^{k}\prec\gamma( \Omega_{h}^{k})^{-1}\]

Replacing \(N\) with \(N_{k}\) and \(w_{N}\) with \(\widetilde{w}_{h}^{k,m}\) for all \(m\in[M]\) completes the proof. 

### Proofs of optimism for Delayed-LPSVI

**Lemma C.3** (Anti-concentration for Optimism).: _Suppose the event_

\[E=\left\{\left|\widehat{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{ V}_{h+1}^{k})(s,a)\right|\leq C_{\delta^{\prime}}\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}},\;\forall s,a,h,k\right\}\]

_holds. Choose \(N_{k}\geq\max\{\log(\frac{32H^{2}(K+\lambda)dk}{\gamma\lambda}+1)/[2\log(1/(1- \frac{1}{2\kappa_{h}}))],\frac{\log 2}{2\log(1/(1-\frac{1}{2\kappa_{h}}))}\}\), \(\gamma=16C_{\delta^{\prime}}^{2}\) and \(M_{\delta}=\log(HK/\delta)/\log(64/63)\). Then we have with probability \(1-\delta\),_

\[\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k})(s,a),\;\forall(s,a)\in\mathcal{S}\times\mathcal{A},h\in[H],k\in[K].\]

Proof of Lemma c.3.: For the rest of the proof, we condition on the event

\[E=\left\{\left|\widehat{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{ V}_{h+1}^{k})(s,a)\right|\leq C_{\delta^{\prime}}\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}},\;\forall s,a,h,k\right\}\]

where \(\delta^{\prime}\) will be specified later and \(C_{\delta}\) is defined in the Lemma C.9.

\[\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-(I-A_{h,k}^{N_{k}})\widetilde{w}_ {h}^{k})\sim\mathcal{N}(0,\phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi(s,a)).\]Also note

\[\widetilde{Q}_{h}^{k,m}(s,a)-\phi(s,a)^{\mathrm{T}}(I-A_{h,k}^{N_{k} })\widehat{w}_{h}^{k}\sim\mathcal{N}(0,\phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi( s,a))\] \[\Leftrightarrow \frac{\widetilde{Q}_{h}^{k,m}(s,a)-\phi(s,a)^{\mathrm{T}}(I-A_{h,k}^{N_{k}})\widehat{w}_{h}^{k}}{\sqrt{\phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi (s,a)}}\sim\mathcal{N}(0,1).\]

Therefore,

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k,m}(s,a)\geq(r_{h}+\mathbb{P }_{h}\widetilde{V}_{h+1}^{k})(s,a),\forall s,a\right)\] \[= \mathbb{P}\Bigg{(}\frac{\widetilde{Q}_{h}^{k,m}(s,a)-\phi(s,a)^{ \mathrm{T}}(I-A_{h,k}^{N_{k}})\widehat{w}_{h}^{k}}{\sqrt{\phi(s,a)^{\mathrm{T }}\Theta_{h}^{k}\phi(s,a)}}\geq\frac{(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1} )(s,a)-\phi(s,a)^{\mathrm{T}}(I-A_{h,k}^{N_{k}})\widehat{w}_{h}^{k}}{\sqrt{ \phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi(s,a)}},\;\forall s,a\Bigg{)}\] \[= \mathbb{P}\Bigg{(}\mathcal{N}(0,1)\geq\frac{(r_{h}+\mathbb{P}_{h }\widetilde{V}_{h+1})(s,a)-\phi(s,a)^{\mathrm{T}}(I-A_{h,k}^{N_{k}})\widehat{ w}_{h}^{k}}{\sqrt{\phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi(s,a)}},\;\forall s,a \Bigg{)}\] \[\geq \mathbb{P}\Bigg{(}\mathcal{N}(0,1)\geq\frac{(r_{h}+\mathbb{P}_{h }\widetilde{V}_{h+1})(s,a)-\phi(s,a)^{\mathrm{T}}(I-A_{h,k}^{N_{k}})\widehat{ w}_{h}^{k}}{\sqrt{\frac{\gamma}{2}\left(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}} \right)\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}},\;\forall s,a \Bigg{)}\] \[\geq \mathbb{P}\Bigg{(}\mathcal{N}(0,1)\geq 1\Bigg{)}\geq\frac{1}{2 \sqrt{8\pi}}e^{-1/2}\geq\frac{1}{64},\]

where the first two inequalities follow Lemma C.2 and Lemma C.4 respectively and the third inequality results from Lemma D.5. Applying Lemma B.6 with \(f=r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}\) and without conditioning, for \(M_{\delta}=\log(1/\delta)/\log(64/63)\),

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a),\forall s,a\right)\geq 1-\delta.\]

Apply a union bound for \(h,k\), we have for \(M_{\delta}=\log(HK/\delta)/\log(64/63)\), with probability \(1-\delta\),

\[\mathbb{P}\left(\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a),\forall s,a,h,k\right)\geq 1-\delta.\]

**Lemma C.4**.: _Suppose the event_

\[E=\left\{\left|\widehat{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a)\right|\leq C_{\delta^{\prime}}\left\|\phi(s,a) \right\|_{(\Omega_{h}^{k})^{-1}},\;\forall s,a,h,k\right\}\]

_holds. Choose \(N_{k}\geq\max\{\log(\frac{32H^{2}(K+\lambda)dk}{\gamma\lambda}+1)/[2\log(1/(1- \frac{1}{2\kappa_{h}}))],\frac{\log 2}{2\log(1/(1-\frac{1}{2\kappa_{h}}))}\}\) and \(\gamma=16C_{\delta^{\prime}}^{2}\). Then_

\[\frac{|(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1})(s,a)-\phi(s,a)^{ \mathrm{T}}(I-A_{h,k}^{N_{k}})\widehat{w}_{h}^{k}|}{\sqrt{\frac{\gamma}{2} \left(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}}\right)\phi(s,a)^{\mathrm{T}}( \Omega_{h}^{k})^{-1}\phi(s,a)}}\leq 1,\quad\forall s,a\in\mathcal{S}\times\mathcal{A},h\in[H],k\in[K].\]

Proof of Lemma C.4.: By direct calculation,

\[\frac{|(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1})(s,a)-\phi(s,a)^{ \mathrm{T}}(I-A_{h,k}^{N_{k}})\widehat{w}_{h}^{k}|}{\sqrt{\frac{\gamma}{2} \left(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}}\right)\phi(s,a)^{\mathrm{T}}(\Omega _{h}^{k})^{-1}\phi(s,a)}}\leq \frac{|\phi(s,a)^{\mathrm{T}}A_{h,k}^{N_{k}}\widehat{w}_{h}^{k}|}{ \sqrt{\frac{\gamma}{2}\left(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}}\right)\phi(s,a )^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}}\] \[+\frac{|(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1})(s,a)-\phi(s,a)^{ \mathrm{T}}\widehat{w}_{h}^{k}|}{\sqrt{\frac{\gamma}{2}\left(1-(1-\frac{1}{2 \kappa_{h}})^{2N_{k}}\right)\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a )}}\]For the first term above, by CS inequality we have

\[|\phi(s,a)^{\mathrm{T}}A_{h,k}^{N_{k}}\widehat{w}_{h}^{k}|\leq\sqrt{ \phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}\cdot\left\|(\Omega_{h}^{k })^{1/2}A_{h,k}^{N_{k}}\widehat{w}_{h}^{k}\right\|\] \[\leq \sqrt{\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}\cdot \left\|(\Omega_{h}^{k})^{1/2}\right\|\left\|A_{h,k}\right\|^{N_{k}}\cdot 2H\sqrt{ \frac{dk}{\lambda}}\] \[\leq \sqrt{\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}\cdot \sqrt{k+\lambda}\cdot(1-\frac{1}{2\kappa_{h}})^{N_{k}}\cdot 2H\sqrt{\frac{dk}{ \lambda}}\]

and this indicates

\[\frac{|\phi(s,a)^{\mathrm{T}}A_{h,k}^{N_{k}}\widehat{w}_{h}^{k}|}{\sqrt{\frac{ \gamma}{2}\left(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}}\right)\phi(s,a)^{\mathrm{ T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}}\leq\frac{\sqrt{k+\lambda}\cdot(1-\frac{1}{2 \kappa_{h}})^{N_{k}}\cdot 2H\sqrt{\frac{dk}{\lambda}}}{\sqrt{\frac{\gamma}{2} \left(1-(1-\frac{1}{2\kappa_{h}})^{2N_{k}}\right)}}\leq\frac{1}{2}\]

where the last inequality is by \(N_{k}\geq\log(\frac{32H^{2}(K+\lambda)dk}{\gamma\lambda}+1)/[2\log(1/(1-\frac{ 1}{2\kappa_{h}}))]\).

For the second term above,

\[\frac{|(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1})(s,a)-\phi(s,a)^{\mathrm{T}} \widehat{w}_{h}^{k}|}{\sqrt{\frac{\gamma}{2}\left(1-(1-\frac{1}{2\kappa_{h}}) ^{2N_{k}}\right)\phi(s,a)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(s,a)}}\leq \frac{C_{\delta^{\prime}}}{\sqrt{\frac{\gamma}{2}\left(1-(1-\frac{1}{2\kappa_ {h}})^{2N_{k}}\right)}}\leq\frac{C_{\delta^{\prime}}}{\sqrt{\frac{\gamma}{2} \left(1-\frac{1}{2\kappa_{h}}\right)}}=\frac{1}{2}.\]

Here the second inequality uses \(N_{k}\geq\frac{\log 2}{2\log(1/(1-\frac{1}{2\kappa_{h}}))}\) and the last equal sign comes from \(\gamma=16C_{\delta^{\prime}}^{2}\). 

**Lemma C.5** (Optimism for Langevin Posterior Sampling).: _For any \(0\leq\delta<1\), we set the input in Algorithm 2 as \(N_{k}\geq\max\{\log(\frac{32H^{2}(K+\lambda)dk}{\gamma\lambda}+1)/[2\log(1/(1- \frac{1}{2\kappa_{h}}))],\frac{\log 2}{2\log(1/(1-\frac{1}{2\kappa_{h}}))}\}\), \(\gamma=16C_{\delta/4}^{2}\) and \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\), then with probability \(1-\delta/2\), we have_

\[\widetilde{Q}_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a),\;\widetilde{V}_{h}^{k}(s)\geq V _{h}^{*}(s)\quad\forall s,a\in\mathcal{S}\times\mathcal{A},\forall h\in[H],k \in[K].\]

_Here \(C_{\delta}\) is defined in Lemma C.9._

Proof of Lemma c.5.: **Step1:** Suppose the event

\[E=\left\{\left|\widetilde{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a)\right|\leq C_{\delta^{\prime}}\left\|\phi(s,a) \right\|_{(\Omega_{h}^{k})^{-1}},\;\forall s,a,h,k\right\}\]

holds. Choose \(N_{k}\geq\max\{\log(\frac{32H^{2}(K+\lambda)dk}{\gamma\lambda}+1)/[2\log(1/(1- \frac{1}{2\kappa_{h}}))],\frac{\log 2}{2\log(1/(1-\frac{1}{2\kappa_{h}}))}\}\), \(\gamma=16C_{\delta^{\prime}}^{2}\) and \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\). Then we show, for any \(h\in[H]\), with probability \(1-\delta/4\), \(\widetilde{Q}_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a)\), \(\widetilde{V}_{h}^{k}(s)\geq V_{h}^{*}(s)\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(h\in[H]\), \(k\in[K]\).

First, due to our choice of \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\), by Lemma C.3, with probability \(1-\delta/4\),

\[\widetilde{Q}_{h}^{k}(s,a)\geq(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k})(s,a ),\;\forall(s,a)\in\mathcal{S}\times\mathcal{A},h\in[H],k\in[K],\]

which we condition on.

Next, we finish the proof by backward induction. Base case: for \(h=H+1\), the value functions are zero, and thus \(\widetilde{Q}_{H+1}^{k}\geq Q_{H+1}^{*}\) holds trivially, which also implies \(\widetilde{V}_{H+1}^{k}\geq V_{H+1}^{*}\). Suppose the conclusion holds true for \(h+1\). Then for time step \(h\) and any \(k\in[K]\),

\[\widetilde{Q}_{h}^{k}-Q_{h}^{*} =\widetilde{Q}_{h}^{k}-(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k} )+(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k})-Q_{h}^{*}\] \[\geq\widetilde{Q}_{h}^{k}-(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1 }^{k})+(r_{h}+\mathbb{P}_{H}V_{h+1}^{*})-Q_{h}^{*}\] \[=\widetilde{Q}_{h}^{k}-(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k} )\geq 0\]where the first inequality uses the induction hypothesis and the second inequality uses the condition. Lastly, \(\widetilde{V}_{h}^{k}(\cdot)=\max_{a}\min\{\widetilde{Q}_{h}^{k}(\cdot,a),H-h+1\} \leq\max_{a}\min\{Q_{h}^{*}(\cdot,a),H-h+1\}=\max_{a}Q_{h}^{*}(\cdot,a)=V_{h}^{*} (\cdot)\). By induction, this finishes the Step1.

**Step2:** By Lemma C.9, with probability \(1-\delta/4\), for all \(k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\), it holds

\[\Big{|}\widehat{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^ {k})(s,a)\Big{|}\leq C_{\delta/4}\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{- 1}}.\]

Therefore, in Step1, choose \(\delta^{\prime}=\delta/4\), and a union bound we obtain: for the choice \(N_{k}\geq\max\{\log(\frac{32H^{2}(K+\lambda)dk}{\gamma\lambda}+1)/[2\log(1/(1- \frac{1}{2\kappa_{h}}))],\frac{\log 2}{2\log(1/(1-\frac{1}{2\kappa_{h}}))}\}\), \(\gamma=16C_{\delta/4}^{2}\) and \(M_{\delta}=\log(4HK/\delta)/\log(64/63)\), then with probability \(1-\delta/2\), we have

\[\widetilde{Q}_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a),\;\widetilde{V}_{h}^{k}(s)\geq V _{h}^{*}(s)\;\forall(s,a)\in\mathcal{S}\times\mathcal{A},\;h\in[H],\;k\in[K].\]

### Proofs of Concentration for Delayed-LPSVI

**Lemma C.6** (Pointwise Concentration for Langevin Posterior Sampling).: _Choose \(N_{k}\geq\log(\frac{4HK^{3}}{\sqrt{\lambda/dK}})/\log(1/(1-\frac{1}{2\kappa_{ h}}))\). Algorithm 2 guarantees that \(\forall k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\), the following holds with probability \(1-\delta\),_

\[\Big{|}\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-(r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a)\Big{|}\leq\beta\left\|\phi(s,a)\right\|_{(\Omega _{h}^{k})^{-1}}+\frac{1}{K^{3}}.\] (21)

_where \(\beta:=\sqrt{2\gamma\log(4C_{d}HMK/\delta)}+\sqrt{8H^{2}\left[\frac{d}{2}\log \left(\frac{k+\lambda}{\lambda}\right)+dM\log(1+\frac{2\sqrt{8k^{3}C_{H,d,k,M /2}}}{H\sqrt{\lambda}})+\log\frac{4}{\delta}\right]}\)\(+2\sqrt{\lambda}\sqrt{dH}\). In particular, here \(\log C_{d}=d\log(1+(16\sqrt{2\gamma\log(2/\delta)/\lambda}+16H\sqrt{d})K^{3})\) and \(C_{H,d,k,M,\delta}=2H\sqrt{\frac{dk}{\lambda}}+\frac{\sqrt{2\delta T}+\sqrt{2 \gamma\log(M/\delta)}}{\sqrt{\lambda}}\)._

Proof of Lemma c.6.: Recall that \(|r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}|\leq H-h+1\), therefore \(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}=\min\{r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k},H-h+1\}\). This implies \(|\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{ V}_{h+1}^{k}](s,a)|=|\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-\min\{r_{h}^{k}+[ \mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a),H-h+1\}|\leq|\widetilde{Q}_{h}^{k }(s,a)-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)|\). Hence

\[\Big{|}\min\{\widetilde{Q}_{h}^{k}(s,a),H-h+1\}-r_{h}^{k}-[ \mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)\Big{|}\leq\Big{|}\widetilde{Q}_{h }^{k}(s,a)-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)\Big{|}\] \[=\Big{|}\widetilde{Q}_{h}^{k}(s,a)-\widehat{Q}_{h}^{k}(s,a)+ \widehat{Q}_{h}^{k}(s,a)-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a) \Big{|}\] \[\leq\underbrace{\Big{|}\widetilde{Q}_{h}^{k}(s,a)-\widehat{Q}_{h} ^{k}(s,a)\Big{|}}_{\widetilde{R}_{1}}+\underbrace{\Big{|}\widehat{Q}_{h}^{k}(s,a )-r_{h}^{k}-[\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}](s,a)\Big{|}}_{\widetilde{R }_{2}}.\]

The proof then directly follows Lemma C.7 and Lemma C.9 to bound \(R_{1}\) and \(R_{2}\) respectively (together with a union bound). 

**Lemma C.7** (Concentration of \(R_{1}\) with Langevin Posterior Sampling).: _Suppose \(N_{k}\geq\log(\frac{4HK^{3}}{\sqrt{\lambda/dK}})/\log(1/(1-\frac{1}{2\kappa_{h}}))\). For any \(0<\delta<1\), define the event \(\widetilde{E}\) as_

\[\widetilde{E}=\Big{\{}\Big{|}\widetilde{Q}_{h}^{k}(s,a)-\phi(s,a)^ {\mathrm{T}}\widetilde{w}_{h}^{k}\Big{|} \leq\sqrt{2\gamma\log(2C_{d}HMK/\delta)}\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}}+\frac{1}{K^{3}},\] \[\forall k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\Big{\}},\] (22)

_then \(\widetilde{E}\) happens w.p. \(1-\delta\). Here \(\log C_{d}=d\log(1+(16\sqrt{2\gamma\log(2/\delta)/\lambda}+16H\sqrt{d})K^{3})\)._Proof of Lemma c.7.: In the Step1 and Step2, we abuse \(\widetilde{w}_{h}^{k}\) to denote \(\widetilde{w}_{h}^{k,m}\) for arbitrary \(m\) to avoid notation redundancy.

In **Step1**: We first show for any \(k\in[K],h\in[H],(s,a)\in\mathcal{S}\times\mathcal{A}\), with probability \(1-\delta\),

\[\big{|}\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}) \big{|}\leq\sqrt{2\gamma\log(2/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k})^{-1}} +\frac{1}{2K^{3}}.\]

Indeed, by Lemma c.2 we have \((\widetilde{w}_{h}^{k}-(I-A_{h,k}^{N_{k}})\widehat{w}_{h}^{k})\sim\mathcal{N} (0,\Theta_{h}^{k})\), which gives,

\[\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-(I-A_{h,k}^{N_{k}})\widehat{w}_{ h}^{k})\sim\mathcal{N}(0,\phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi(s,a)).\]

Therefore, \(\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-(I-A_{h,k}^{N_{k}})\widehat{w}_{ h}^{k})\) is \(\phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi(s,a)\)-sub-Gaussian. By concentration of sub-Gaussian random variables, we have

\[\mathbb{P}\left(\left|\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-(I-A_{h,k} ^{N_{k}})\widehat{w}_{h}^{k})\right|\geq t\right)\leq 2\exp\left(-\frac{t^{2}}{2 \phi(s,a)^{\mathrm{T}}\Theta_{h}^{k}\phi(s,a)}\right):=\delta\]

Solving for \(\delta\) gives with probability \(1-\delta\),

\[\left|\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-(I-A_{h,k}^{N_{k}}) \widehat{w}_{h}^{k})\right|\leq\sqrt{2\log(2/\delta)}\,\|\phi(s,a)\|_{\Theta_ {h}^{k}}\leq\sqrt{2\gamma\log(2/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k})^{-1} }\,,\]

where the last inequality is by Lemma c.2, and by Lemma c.8, the above further implies

\[\big{|}\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}) \big{|}\leq\sqrt{2\gamma\log(2/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k})^{-1} }+\frac{1}{2K^{3}}.\]

**Step2:** we prove that for any \(0<\delta<1\), define the event \(\widetilde{E}\) as

\[\widetilde{E}=\left\{\left|\phi(s,a)^{\mathrm{T}}\widetilde{w}_{ h}^{k}-\phi(s,a)^{\mathrm{T}}\widehat{w}_{h}^{k}\right|\leq\sqrt{2\gamma \log(2C_{d}HK/\delta)}\,\|\phi(s,a)\|_{(\Omega_{h}^{k})^{-1}}+\frac{1}{K^{3}},\right.\] \[\left.\forall k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A} \right\}\!,\] (23)

then \(\widetilde{E}\) happens w.p. \(1-\delta\). Here \(\log C_{d}=d\log(1+(16\sqrt{2\gamma\log(2/\delta)/\lambda}+16H\sqrt{d})K^{3})\).

In Lemma d.12, set \(\theta=\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\) and \(A=(\Omega_{h}^{k})^{-1}\) and \(B=1/\lambda\), and let \(\mathcal{V}\) be the \(\frac{1}{4K^{3}}\)-epsilon net for the class of values \(\{|\langle\phi,\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle|-C\sqrt{\phi^ {\top}(\Omega_{h}^{k})^{-1}\phi}-\frac{1}{2K^{3}}:\|\phi\|\leq 1\}\) (where \(C=\sqrt{2\gamma\log(2/\delta)}\)), then it must also be the \(\frac{1}{4K^{3}}\)-epsilon net for the class of values \(\mathcal{F}=\{|\langle\phi(s,a),\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k} \rangle|-C\sqrt{\phi(s,a)^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}-\frac{1}{2K^{3 }}:(s,a)\in\mathcal{S}\times\mathcal{A}\}\), let \(\bar{\mathcal{V}}\) is the smallest subset of \(\mathcal{V}\) such that it is \(\frac{1}{2K^{3}}\)-epsilon net for the class of values \(\mathcal{F}\). Then we can select \(\mathcal{V}_{\mathcal{S}\times\mathcal{A}}\) to be the set of state-action pairs such that for any \(f_{\phi}:=|\langle\phi,\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle|-C \sqrt{\phi^{\top}(\Omega_{h}^{k})^{-1}\phi}\in\bar{\mathcal{V}}-\frac{1}{2K^{3}}\), there exists \((s,a)\in\mathcal{V}_{\mathcal{S}\times\mathcal{A}}\) satisfies \(\left|\left|\langle\phi(s,a),\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle \right|-C\sqrt{\phi(s,a)^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}-\frac{1}{2K^{3 }}-f_{\phi}\right|\leq 1/4K^{3}\), then we have \(\mathcal{V}_{\mathcal{S}\times\mathcal{A}}\) is a \(1/(2K^{3})\)-epsilon net of \(\mathcal{F}\) and \(|\mathcal{V}_{\mathcal{S}\times\mathcal{A}}|\leq|\bar{\mathcal{V}}|\leq| \mathcal{V}|\). Therefore,

\[\sup_{s,a}\left(|\langle\phi(s,a),\widetilde{w}_{h}^{k}-\widehat {w}_{h}^{k}\rangle|-C\sqrt{\phi(s,a)^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}- \frac{1}{2K^{3}}\right)\] \[\leq\sup_{(s,a)\in\mathcal{V}_{\mathcal{S}\times\mathcal{A}}}\left(| \langle\phi(s,a),\widetilde{w}_{h}^{k}-\widehat{w}_{h}^{k}\rangle|-C\sqrt{\phi(s,a)^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}-\frac{1}{2K^{3}}\right)+1/(2K^{3})\] \[\leq 1/(2K^{3}),\]

where the last inequality is from Step1. Then by a union bound over \(H\), \(K\) and \((1+(16\sqrt{2\gamma\log(2/\delta)/\lambda}+16H\sqrt{d})K^{3})^{d}\), we have with probability \(1-\delta\),

\[\sup_{s,a,h,k}\left(|\langle\phi(s,a),\widetilde{w}_{h}^{k}- \widetilde{w}_{h}^{k}\rangle|-\sqrt{2\gamma\log(2C_{d}HK/\delta)}\sqrt{\phi(s,a )^{\top}(\Omega_{h}^{k})^{-1}\phi(s,a)}\right)\] \[\leq\frac{1}{2K^{3}}+\frac{1}{2K^{3}}=\frac{1}{K^{3}},\]where \(\log C_{d}=d\log(1+(16\sqrt{2\gamma\log(2/\delta)/\lambda}+16H\sqrt{d})K^{3})\).

**Step3:** We finish the proof. Note \(\widetilde{Q}_{h}^{k}=\max_{m}\phi^{\mathrm{T}}\widetilde{w}_{h}^{k,m}\), hence by a union bound over \(M\), we have

\[\left|\widetilde{Q}_{h}^{k}(s,a)-\phi(s,a)^{\mathrm{T}}\widehat{w }_{h}^{k}\right|=|\max_{m}\phi(s,a)^{\mathrm{T}}\widetilde{w}_{h}^{k,m}-\phi(s,a)^{\mathrm{T}}\widehat{w}_{h}^{k}|\] \[\leq \max_{m}|\phi(s,a)^{\mathrm{T}}\widetilde{w}_{h}^{k,m}-\phi(s,a) ^{\mathrm{T}}\widehat{w}_{h}^{k}|\] \[\leq \sqrt{2\gamma\log(2C_{d}HMK/\delta)}\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}}+\frac{1}{K^{3}}\]

for all \(k,h,s,a\) with probability \(1-\delta\). Here the last inequality uses Step2. This finishes the proof. 

**Lemma C.8**.: _Let \(N_{k}\geq\log(\frac{4HK^{3}}{\sqrt{\lambda/dk}})/\log(1/(1-\frac{1}{2\kappa_{ h}}))\) and \(\eta=\frac{1}{4\lambda_{\max}(\Omega_{h}^{k})}\), then_

\[\left\|\phi(s,a)^{\mathrm{T}}A_{h,k}^{N_{k}}\widehat{w}_{h}^{k}\right\|\leq \frac{1}{2K^{3}}\]

Proof of Lemma c.8.: By direct calculation,

\[\left\|\phi(s,a)^{\mathrm{T}}A_{h,k}^{N_{k}}\widehat{w}_{h}^{k}\right\|\leq \left\|\phi(s,a)\right\|\left\|A_{h,k}\right\|^{N_{k}}\left\|\widehat{w}_{h}^ {k}\right\|\leq\left\|A_{h,k}\right\|^{N_{k}}\left\|\widehat{w}_{h}^{k}\right\|\] \[\leq \left\|A_{h,k}\right\|^{N_{k}}2H\sqrt{\frac{dk}{\lambda}}\leq \left(1-\frac{1}{2\kappa_{h}}\right)^{N_{k}}\cdot 2H\sqrt{\frac{dk}{\lambda}} \leq\frac{1}{2K^{3}}\]

where the third inequality is by Lemma D.4 and the fourth inequality is by (20). The last inequality is by the choice of \(N_{k}\). 

**Lemma C.9** (Concentration of \(R_{2}\) with Langevin Posterior Sampling).: _For any \(0<\delta<1\), with probability \(1-\delta\), for all \(k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\), it holds_

\[\left|\widehat{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k })(s,a)\right|\leq C_{\delta}\left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}\]

_where \(C_{\delta}=\sqrt{8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2\sqrt{8K^{3}}C_{H,d,k,M,k}}{H\sqrt{\lambda}})+\log \frac{2}{\delta}\right]}+2\sqrt{\lambda}\sqrt{d}H\) and the quantity \(C_{H,d,k,M,\delta}=2H\sqrt{\frac{dk}{\lambda}}+\frac{\sqrt{2d\gamma}+\sqrt{2 \gamma\log(M/\delta)}}{\sqrt{\lambda}}\)._

Proof of Lemma c.9.: For any \((k,h)\in[K]\times[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\), denote

\[\phi(s,a)^{\mathrm{T}}w_{h}^{k}:=(r_{h}^{k}+\mathbb{P}_{h}\widetilde{V}_{h+1} ^{k})(s,a),\text{where }\ w_{h}^{k}:=\theta_{h}+\int_{\mathcal{S}}\widetilde{V}_{h+1}^{k}(s^{ \prime})\mathrm{d}\mu_{h}(s^{\prime}).\]

Recall \(y_{h}^{\tau}=\mathds{1}_{\tau,k-1}\cdot[r_{h}^{\tau}(s_{h}^{\tau},a_{h}^{ \tau})+\widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})]\) from Algorithm 1 and denote \(\bar{y}_{h}^{\tau}:=r_{h}^{\tau}(s_{h}^{\tau},a_{h}^{\tau})+\widetilde{V}_{h+1 }^{k}(s_{h+1}^{\tau})\). Then by definition,

\[\widehat{w}_{h}^{k}=(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k -1}\cdot\phi(s_{h}^{\tau},a_{h}^{\tau})y_{h}^{\tau}=(\Omega_{h}^{k})^{-1}\sum_ {\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\bar{y }_{h}^{\tau}.\]

By definition of \(\Omega_{h}^{k}\), we have \(\Phi_{h}\Phi_{h}^{\mathrm{T}}=\Omega_{h}^{k}-\lambda I\). Plug it into the definition of \(\widehat{w}_{h}^{k}\), we have

\[\widehat{w}_{h}^{k} =(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1} \cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\left(\bar{y}_{h}^{\tau}-\phi(s_{h}^{\tau},a_{h}^{\tau})^{\mathrm{T}}w_{h}^{k}+\phi(s_{h}^{\tau},a_{h}^{\tau})^{\mathrm{T }}w_{h}^{k}\right)\] \[=(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1} \cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\left(\bar{y}_{h}^{\tau}-\phi(s_{h}^{\tau},a_{h}^{\tau})^{\mathrm{T}}w_{h}^{k}\right)+(\Omega_{h}^{k})^{-1}\left(\Omega_{h }^{k}-\lambda I\right)w_{h}^{k}.\]We then proceed to bound \(\widehat{w}_{h}^{k}-w_{h}^{k}\), which gives

\[\widehat{w}_{h}^{k}-w_{h}^{k} =(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot \phi(s_{h}^{\tau},a_{h}^{\tau})\left(\bar{y}_{h}^{\tau}-\phi(s_{h}^{\tau},a_{h}^{ \tau})^{\mathrm{T}}w_{h}^{k}\right)-\lambda(\Omega_{h}^{k})^{-1}w_{h}^{k}\] \[=\underbrace{(\Omega_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\mathds{1}_{ \tau,k-1}\cdot\phi(s_{h}^{\tau},a_{h}^{\tau})\left(\widetilde{V}_{h+1}^{k}(s_{ h+1}^{\tau})-\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}(s_{h}^{\tau},a_{h}^{\tau}) \right)}_{\mathrm{(i)}}-\underbrace{\lambda(\Omega_{h}^{k})^{-1}w_{h}^{k}}_{ \mathrm{(ii)}}.\]

**Term (i).** Since \(\Omega_{h}^{k}\) is positive definite, multiplying the first term \((i)\) with \(\phi(s,a)\) and by Cauchy-Schwartz inequality, we obtain,

\[\left|\phi(s,a)^{\mathrm{T}}(\mathrm{i})\right|\leq\|\phi(s,a)\|_{(\Omega_{h}^ {k})^{-1}}\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot\phi(s_{h}^{\tau },a_{h}^{\tau})\left(\widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})-\mathbb{P}_{h} \widetilde{V}_{h+1}^{k}(s_{h}^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h }^{k})^{-1}}.\]

Apply Lemma C.10, we have with probability at least \(1-\delta\), for any \((k,h)\in[K]\times[H]\), and \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\left|\phi(s,a)^{\mathrm{T}}(\mathrm{i})\right|\leq C_{1}\left\|\phi(s,a) \right\|_{(\Omega_{h}^{k})^{-1}},\] (24)

where \(C_{1}=\sqrt{8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda}\right) +dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,\delta}}{H\sqrt{\lambda}})+\log\frac{ 2}{\delta}\right]}\).

**Term (ii).** By Lemma B.12, \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\), and \((k,h)\in[K]\times[H]\), \(\left|\phi(s,a)^{\mathrm{T}}(\mathrm{ii})\right|\) can be bounded as

\[\left|\phi(s,a)^{\mathrm{T}}(\mathrm{ii})\right|=\lambda\left|\phi(s,a)^{ \mathrm{T}}(\Omega_{h}^{k})^{-1}w_{h}^{k}\right|\leq 2\sqrt{\lambda}\sqrt{d}H \left\|\phi(s,a)\right\|_{(\Omega_{h}^{k})^{-1}}.\] (25)

Combining (24), (25), we have with probability \(1-\delta\), for any \((k,h)\in[K]\times[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\left|\widetilde{Q}_{h}^{k}(s,a)-(r_{h}^{k}+\mathbb{P}_{h} \widetilde{V}_{h+1}^{k})(s,a)\right| =\left|\phi(s,a)^{\mathrm{T}}(\widetilde{w}_{h}^{k}-w_{h}^{k}) \right|\leq\left|\phi(s,a)^{\mathrm{T}}(\mathrm{i})\right|+\left|\phi(s,a)^{ \mathrm{T}}(\mathrm{ii})\right|\] \[\leq(C_{1}+2\sqrt{\lambda}\sqrt{d}H)\left\|\phi(s,a)\right\|_{( \Omega_{h}^{k})^{-1}},\]

This concludes the proof. 

**Lemma C.10**.: _For any \(0<\delta<1\), with probability \(1-\delta\), we have \(\forall(k,h)\in[K]\times[H]\),_

\[\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\cdot\phi(s_{h}^{ \tau},a_{h}^{\tau})\left(\widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})-\mathbb{P}_{h} \widetilde{V}_{h+1}^{k}(s_{h}^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h }^{k})^{-1}}^{2}\] \[\leq 8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,\delta}}{H\sqrt{\lambda}})+ \log\frac{2}{\delta}\right],\]

_here \(C_{H,d,k,M,\delta}=2H\sqrt{\frac{dk}{\lambda}}+\frac{\sqrt{2d\gamma}+\sqrt{2 \gamma}\log(M/\delta)}{\sqrt{\lambda}}\).9_

Footnote 9: We will choose \(\gamma\) to be \(\mathrm{Poly}(H,d,K)\) and this will not affect the overall dependence of the guarantee since \(C_{H,d,k,M,\delta}\) is inside the log term.

Proof of Lemma c.10.: First note that

\[\widetilde{V}_{h}^{k}(\cdot):=\max_{a}\min\{\widetilde{Q}_{h}^{k}( \cdot,a),(H-h+1)\} =\max_{a}\min\max_{m}\{\widetilde{Q}_{h}^{k,m},(H-h+1)\}\] \[=\max_{a}\min\{\max_{m}\phi(\cdot,a)^{\mathrm{T}}\widetilde{w}_{h }^{k,m},(H-h+1)\}.\]

Choosing \(w_{0}=0\), then by Lemma C.2 and \((\Theta_{h}^{k})^{-1/2}(\widetilde{w}_{h}^{k,m}-(I-A_{h,k}^{N_{\delta}}) \widehat{w}_{h}^{k})\sim\mathcal{N}(0,I_{d})\), and by Lemma D.7, with probability \(1-\delta/2\), we have

\[\frac{\sqrt{\lambda}}{\sqrt{\gamma}}\left\|\widetilde{w}_{h}^{k,m}-(I-A_{h,k}^{ N_{k}})\widehat{w}_{h}^{k}\right\|\leq\left\|(\Theta_{h}^{k})^{-1/2}(\widetilde{w}_{h}^{k,m}-(I-A_{h,k }^{N_{k}})\widehat{w}_{h}^{k})\right\|\leq\sqrt{2d}+\sqrt{2\log(1/\delta)},\]where the first inequality uses Lemma C.2 again. Apply the union bound over all \(m\), then above implies with probability \(1-\delta/2\), \(\forall m\in[M]\)

\[\left\|\widetilde{w}_{h}^{k,m}\right\|\leq\left\|\widehat{w}_{h}^{k}\right\|+ \frac{\sqrt{2d\gamma}+\sqrt{2\gamma\log(M/\delta)}}{\sqrt{\lambda}}\leq 2H \sqrt{\frac{dk}{\lambda}}+\frac{\sqrt{2d\gamma}+\sqrt{2\gamma\log(M/\delta)}} {\sqrt{\lambda}}:=C_{H,d,k,M,\delta}.\] (26)

(where we used \(\left\|(I-A_{h,k}^{N_{k}})\widehat{w}_{h}^{k}\right\|\leq\left\|(I-A_{h,k}^{N_ {k}})\right\|\left\|\widehat{w}_{h}^{k}\right\|\leq\left\|\widehat{w}_{h}^{k}\right\|\). Now consider the function class \(\bar{\mathcal{V}}:=\{\max_{a}\max_{m}\phi(\cdot,a)^{T}w^{m}:\left\|w^{m} \right\|\leq C_{H,d,k,M,\delta}\}\), so by Lemma D.13 the \(\epsilon\)-log covering number for \(\bar{\mathcal{V}}\) is \(dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})\). Since \(\min\{\cdot,\cdot\}\) is a non-expansive operator, the \(\epsilon\)-log covering number for the function class \(\mathcal{V}:=\{\max_{a}\min\{\max_{m}\phi(\cdot,a)^{T}w^{m},(H-h+1)\}:\left\| w^{m}\right\|\leq C_{H,d,k,M,\delta}\}\), is at most \(dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})\). Hence, for any \(V\in\mathcal{V}\), there exists \(V^{\prime}\) in the \(\epsilon\)-covering such that \(V=V^{\prime}+\Delta_{V}\) with \(\left\|\Delta_{V}\right\|_{\infty}\leq\epsilon\). Then with probability \(1-\delta/2\),

\[\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau}, a_{h}^{\tau})\left(V(s_{h+1}^{\tau})-\mathbb{P}_{h}V(s_{h}^{\tau},a_{h}^{ \tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] (27) \[\leq 2\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau },a_{h}^{\tau})\left(V^{\prime}(s_{h+1}^{\tau})-\mathbb{P}_{h}V^{\prime}(s_{h} ^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] \[+2\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau },a_{h}^{\tau})\left(\Delta_{V}(s_{h+1}^{\tau})-\mathbb{P}_{h}\Delta_{V}(s_{h} ^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] \[\leq 2\left\|\sum_{\tau=1}^{k-1}\mathds{1}_{\tau,k-1}\phi(s_{h}^{\tau },a_{h}^{\tau})\left(V^{\prime}(s_{h+1}^{\tau})-\mathbb{P}_{h}V^{\prime}(s_{h} ^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}+\frac{8k^{2} \epsilon^{2}}{\lambda}\] \[\leq 4H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda}\right) +dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})+\log\frac{2}{\delta}\right]+ \frac{8k^{2}\epsilon^{2}}{\lambda}\]

where the second inequality can be conducted using a direct calculation and the third inequality uses Lemma D.9 and a union bound over the covering number. Now by (26) and (27) and a union bound, we have for any \(\epsilon>0\), with probability \(1-\delta\),

\[\left\|\sum_{\tau=1}^{k-1}\phi(s_{h}^{\tau},a_{h}^{\tau})\left( \widetilde{V}_{h+1}^{k}(s_{h+1}^{\tau})-\mathbb{P}_{h}\widetilde{V}_{h+1}^{k}( s_{h}^{\tau},a_{h}^{\tau})\right)\right\|_{(\Omega_{h}^{k})^{-1}}^{2}\] \[\leq 4H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})+\log\frac{2}{\delta} \right]+\frac{8k^{2}\epsilon^{2}}{\lambda}\] \[\leq 8H^{2}\left[\frac{d}{2}\log\left(\frac{k+\lambda}{\lambda} \right)+dM\log(1+\frac{2\sqrt{8k^{3}}C_{H,d,k,M,\delta}}{H\sqrt{\lambda}})+ \log\frac{2}{\delta}\right],\]

where the last step choose \(\epsilon^{2}=H^{2}\lambda/8k^{2}\) so \(\frac{8k^{2}\epsilon^{2}}{\lambda}\leq 4H^{2}\). Lastly, apply the union bound over \(H,K\) to obtain the stated result. 

## Appendix D Auxiliary lemmas

### Useful Norm Inequalities

**Lemma D.1**.: _Suppose \(v\in\mathbb{R}^{d}\), and \(A\) is some positive definite matrix whose eigenvalues satisfy \(\lambda_{\max}(A)\geq\cdots\geq\lambda_{\min}(A)>0\). It can be shown that_

\[\sqrt{\lambda_{\min}(A)}\left\|v\right\|\leq\left\|v\right\|_{A}\leq\sqrt{ \lambda_{\max}(A)}\left\|v\right\|.\]

Proof of Lemma D.1.: Consider the eigenvalue decomposition of \(A\), which gives \(A=U\Lambda U^{\mathrm{T}}\), where \(\Lambda=\text{diag}(\lambda_{\max}(A),\ldots,\lambda_{\min}(A))\). Then

\[\left\|v\right\|_{A}=\sqrt{\sum_{i=1}^{d}\lambda_{i}(A)(u_{i}^{\mathrm{T}}v)^{2 }}\leq\sqrt{\lambda_{\max}(A)\left\|u_{i}^{\mathrm{T}}v\right\|^{2}}=\sqrt{ \lambda_{\max}(A)}\left\|v\right\|.\]Similar argument shows \(\left\|v\right\|_{A}\geq\sqrt{\lambda_{\min}(A)}\). 

**Lemma D.2** (Lemma D.1 of [35]).: _Let \(\Omega_{h}^{k}\) be the precision matrix of the posterior distribution of \(w_{h}^{k}\) at step \(h\) of episode \(k\), where \(\Omega_{h}^{k}:=\sigma^{-2}\Phi_{h}\Phi_{h}^{\mathrm{T}}+\Sigma^{-1}\) with \(\Sigma^{-1}=\lambda I_{d}\) and \(\sigma^{2}=1\). Then_

\[\sum_{\tau=1}^{k-1}\left\|\phi(s_{h}^{\tau},a_{h}^{\tau})\right\|_{(\Omega_{h}^ {k})^{-1}}^{2}\leq d.\]

**Lemma D.3** (Bound on Weights of Q-function).: _Suppose the linear MDP assumption and at each step \(h\in[H]\), rewards \(r_{h}\) are bounded between \([0,1]\), then the norm of the true parameter \(w_{h}^{\pi}\) under fixed policy \(\pi\) satisfies_

\[\forall h\in[H],\quad\left\|w_{h}^{\pi}\right\|\leq 2H\sqrt{d}.\]

_In addition, for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), let \(\phi(s,a)^{\mathrm{T}}w_{h}^{k}:=(r_{h}+\mathbb{P}_{h}\widetilde{V}_{h+1}^{k} )(s,a)\), we also have_

\[\forall h\in[H],k\in[K],\quad\left\|w_{h}^{k}\right\|\leq 2H\sqrt{d}.\]

Proof of Lemma d.3.: By definition in Lemma A.1, the true parameter \(w_{h}\) at time step \(h\) is

\[w_{h}^{\pi}:=\theta_{h}+\mathbb{E}_{s^{\prime}\sim_{h}}[V_{h+1}^{\pi}(s^{ \prime})].\]

With bounded rewards \(r_{h}\in[0,1]\), we have \(V_{h+1}^{\pi}(s)\leq H,\ \forall s\in\mathcal{S}\). Since \(\left\|\theta_{h}\right\|\leq\sqrt{d}\), and \(\left\|\mathbb{E}_{\mu_{h}}[V_{h+1}^{\pi}(s^{\prime})]\right\|\leq\left\|\int_{ \mathcal{S}}H\mathrm{d}\mu_{h}(s^{\prime})\right\|\leq H\sqrt{d}\).

Similarly, by definition of the constructed weights \(w_{h}^{k}\),

\[w_{h}^{k}:=\theta_{h}+\int_{\mathcal{S}}\widetilde{V}_{h+1}^{k}(s^{\prime}) \mathrm{d}\mu_{h}(s^{\prime}).\]

From Line 15 of Algorithm 1, for any \(h\in[H]\) and \(s\in\mathcal{S}\), \(\widetilde{V}_{h}^{k}(s)=\max_{a}\min\{\widetilde{Q}_{h}^{k}(\cdot,a),H-h+1\} \leq H\). Applying triangle inequality, we have

\[\left\|w_{h}^{k}\right\| \leq\left\|\theta_{h}\right\|+\left\|\int_{\mathcal{S}}\widetilde {V}_{h+1}^{k}(s^{\prime})\mathrm{d}\mu_{h}(s^{\prime})\right\|\] \[\leq\sqrt{d}+\left\|\int_{\mathcal{S}}H\mathrm{d}\mu_{h}(s^{ \prime})\right\|\] \[\leq 2H\sqrt{d}.\]

**Lemma D.4** (Bound on Estimated Weights of Algorithm 1).: _For any step \(h\in[H]\) and episode \(k\in[K]\), the weight \(\widehat{w}_{h}^{k}\) output by Algorithm 1 satisfies,_

\[\left\|\widehat{w}_{h}^{k}\right\|\leq 2H\sqrt{\frac{dk}{\lambda}}.\]

Proof of Lemma d.4.: For any vector \(\mathbf{v}\in\mathbb{R}^{d}\), it holds

\[\left|\mathbf{v}^{\top}\widehat{w}_{h}^{k}\right| =\left|\mathbf{v}^{\top}\left(\Omega_{h}^{k}\right)^{-1}\sum_{ \tau=1}^{k-1}\boldsymbol{\phi}_{h}^{\tau}\left[r\left(s_{h}^{\tau},a_{h}^{ \tau}\right)+\widetilde{V}_{h}^{k}(s_{h+1}^{\tau})\right]\right|\] \[\leq\sum_{\tau=1}^{k-1}\left|\mathbf{v}^{\top}\left(\Omega_{h}^{ k}\right)^{-1}\boldsymbol{\phi}_{h}^{\tau}\right|\cdot 2H\leq\sqrt{\left[\sum_{\tau=1}^{k-1} \mathbf{v}^{\top}\left(\Omega_{h}^{k}\right)^{-1}\mathbf{v}\right]\cdot\left[ \sum_{\tau=1}^{k-1}\left(\boldsymbol{\phi}_{h}^{\tau}\right)^{\top}\left( \Omega_{h}^{k}\right)^{-1}\boldsymbol{\phi}_{h}^{\tau}\right]\cdot 2H\] \[\leq 2H\|\mathbf{v}\|\sqrt{dk/\lambda},\]

where the last step is by Lemma D.2. The above directly imply the stated result by the definition of \(l_{2}\) norm.

### Concentration Inequalities

**Lemma D.5** ([3]).: _Suppose \(Z\) is a random variable following a Gaussian distribution \(\mathcal{N}(\mu,\sigma^{2})\), where \(\sigma>0\). The following concentration and anti-concentration inequalities hold for any \(z\geq 1\):_

\[\frac{1}{2\sqrt{\pi}z}e^{-z^{2}/2}\leq\mathbb{P}\left(|Z-\mu|>z\sigma\right)\leq \frac{1}{\sqrt{\pi}z}e^{-z^{2}/2}.\]

_And for \(0\leq z\leq 1\), we have,_

\[\mathbb{P}\left(|Z-\mu|>z\sigma\right)\geq\frac{1}{\sqrt{8\pi}}e^{-z^{2}/2}.\]

**Lemma D.6** (Sub-exponential tail bound).: _Suppose \(\{\tau_{k}\}_{k=1}^{\infty}\) are \((v,b)\)-sub-exponential random variables. denote \(D_{\tau,K,\delta}:=\min\left\{\sqrt{2v^{2}\log\left(\frac{3K}{2\delta}\right)},2b\log\left(\frac{3K}{2\delta}\right)\right\}\). Then with probability \(1-\delta\),_

\[\max_{k\in[K]}\tau_{k}\leq\mathbb{E}[\tau]+D_{\tau,K,\delta}.\]

**Lemma D.7** (Multivariate Gaussian Concentration).: _Suppose \(X\sim\mathcal{N}(0,I_{d})\). Then with probability \(1-\delta\),_

\[\|X\|\leq\sqrt{2d}+\sqrt{2\log(1/\delta)}.\]

Proof.: Apply Proposition 1 of [30], choose \(A=I_{d}\), then \(\Sigma=I_{d}\) and \(Tr(\Sigma)=d\), \(\|\Sigma\|=1\). Then

\[P\left[\|X\|^{2}\geq d+2\sqrt{dt}+2t\right]\leq e^{-t}\Rightarrow P[\|X\|^{2 }\geq 2(\sqrt{d}+\sqrt{t})^{2}]\leq e^{-t}:=\delta\]

which implies with probability \(1-\delta\), \(\|X\|\leq\sqrt{2d}+\sqrt{2\log(1/\delta)}\). 

**Lemma D.8** (Elliptical Potential Lemma [1]).: _Suppose \(\{\phi_{t}\}_{t=1}^{\infty}\) is an \(\mathbb{R}^{d}\)-valued sequence, \(\Omega_{0}\in\mathbb{R}^{d\times d}\) is positive definite, and \(\Omega_{t}=\Omega_{0}+\sum_{\tau=1}^{t-1}\phi_{\tau}\phi_{\tau}^{\mathrm{T}}\). If \(\lambda_{\min}(\Omega_{0})\geq 1\), and \(\left\|\phi_{\tau}\right\|_{2}\leq 1\) for all \(\tau\in\mathbb{Z}_{+}\), then for any \(t\in\mathbb{Z}_{+}\),_

\[\log\left(\frac{\det(\Omega_{t+1})}{\det(\Omega_{1})}\right)\leq\sum_{\tau=1} ^{t}\phi_{\tau}^{\mathrm{T}}(\Omega_{\tau})^{-1}\phi_{\tau}\leq 2\log\left( \frac{\det(\Omega_{t+1})}{\det(\Omega_{1})}\right).\]

**Lemma D.9** (Self-normalized process [1]).: _Let \(\{\mathcal{F}_{t}\}_{t=0}^{\infty}\) be a filtration, and \(\{\eta_{t}\}_{t=1}^{\infty}\) be a real-valued stochastic process such that \(\eta_{t}\) is \(\mathcal{F}_{t}\)-measurable and \(\eta_{t}|\mathcal{F}_{t-1}\) is zero-mean (i.e. \(\mathbb{E}[\eta_{t}|\mathcal{F}_{t-1}]=0\)). Assume that conditioning on \(\mathcal{F}_{t}\), \(\eta_{t}\) is \(C\)-sub-Gaussian. Let \(\{\phi_{t}\}_{t=1}^{\infty}\) be an \(\mathbb{R}^{d}\) real-valued stochastic process such that \(\phi_{t}\) is \(\mathcal{F}_{t}\)-measurable. Let \(\Omega_{0}\in\mathbb{R}^{d\times d}\) be a positive definite matrix and \(\Omega_{t}=\Omega_{0}+\sigma^{-2}\sum_{\tau=1}^{t}\phi_{\tau}\phi_{\tau}^{ \mathrm{T}}\). Then for \(\delta>0\), with probability at least \(1-\delta\), for all \(t\geq 0\),_

\[\left\|\sum_{\tau=1}^{t}\phi_{\tau}\eta_{\tau}\right\|_{\Omega_{t}^{-1}}^{2} \leq 2C^{2}\log\left(\frac{\det(\Omega_{t})^{1/2}\mathrm{det}(\Omega_{0})^{ -1/2}}{\delta}\right).\]

**Lemma D.10**.: _Suppose \(\Omega_{0}:=\lambda I_{d}\) is a positive definite matrix in \(\mathbb{R}^{d\times d}\) and \(\Omega_{t}=\Omega_{0}+\sigma^{-2}\sum_{\tau=1}^{t-1}\phi_{\tau}\phi_{\tau}^{ \mathrm{T}}\)._

\[\frac{\det(\Omega_{t+1})}{\det(\Omega_{1})}\leq\left(\frac{\lambda+\sigma^{-2} t}{\lambda}\right)^{d}.\]

Proof of Lemma d.10.: By definition, \(\det(\Omega_{1})=\det(\lambda I)=\lambda^{d}\). For any \(\tau\in\mathbb{Z}_{+}\) and \(\phi_{\tau}\in\ R^{d}\), notice that \(\phi_{\tau}\phi_{\tau}^{\mathrm{T}}\) is a rank-1 matrix with eigenvalues \(\|\phi_{\tau}\|\) and \(0\). By Definition 1 and triangle inequality,

\[\left\|\sum_{\tau=1}^{t}\phi_{\tau}\phi_{\tau}^{\mathrm{T}}\right\|\leq\sum_{ \tau=1}^{t}\left\|\phi_{\tau}\phi_{\tau}^{\mathrm{T}}\right\|\leq t.\]

Consider the eigenvalue decomposition for \(\sum_{\tau=1}^{t-1}\phi_{\tau}\phi_{\tau}^{\mathrm{T}}\):

\[\sum_{\tau=1}^{t-1}\phi_{\tau}\phi_{\tau}^{\mathrm{T}}=U\mathrm{diag}(\lambda_{1}, \ldots,\lambda_{d})U^{\mathrm{T}},\]which suggests

\[\det(\Omega_{t+1})=\det(\lambda I+\sigma^{-2}\sum_{\tau=1}^{t-1}\phi_{\tau}\phi_{ \tau}^{T})=\prod_{i=1}^{d}(\sigma^{-2}\lambda_{i}+\lambda)\leq(\sigma^{-2}\max _{i}|\lambda_{i}|+\lambda)^{d}\leq(\lambda+\sigma^{-2}t)^{d}.\]

### Covering Argument

**Lemma D.11** (Covering number of Euclidean Ball).: _Consider an Euclidean ball \(B_{R}\) equipped with the Euclidean metric, whose radius is \(R>0\). The \(\epsilon\)-covering number of \(B_{R}\) satisfies,_

\[\mathcal{N}_{\epsilon}(B_{R})\leq\left(1+\frac{2R}{\epsilon}\right)^{d}.\]

**Lemma D.12**.: _Define \(\mathcal{V}\) to be a class of values with the parametric form_

\[f_{\phi}:=|\langle\phi,\theta\rangle|-C\sqrt{\phi^{\top}A\cdot\phi}\]

_where the feature space is \(\{\phi:\left\|\phi\right\|_{2}\leq 1\}\) and \(\left\|A\right\|_{2}\leq B\), \(\left\|\theta\right\|\leq 2H\sqrt{d}\). Let \(\mathcal{N}_{\epsilon}^{\mathcal{V}}\) be the covering number of \(\epsilon\)-net with respect to the absolute value distance, then we have_

\[\log\mathcal{N}_{\epsilon}^{\mathcal{V}}\leq d\log(1+\frac{4C\sqrt{B}+4H\sqrt{ d}}{\epsilon}).\]

Proof of Lemma D.12.: \[|f_{\phi_{1}}-f_{\phi_{2}}|\leq\left|\left|\langle\phi_{1}, \theta\rangle\right|-C\sqrt{\phi_{1}^{\top}A\cdot\phi_{1}}-(|\langle\phi_{2}, \theta\rangle|-C\sqrt{\phi_{2}^{\top}A\cdot\phi_{2}})\right|\] \[\leq \left\|\phi_{1}-\phi_{2}\right\|\cdot\left\|\theta\right\|+C \sqrt{\left\|\phi_{1}^{\top}A\cdot\phi_{1}-\phi_{2}^{\top}A\cdot\phi_{2}\right|}\] \[\leq \left\|\phi_{1}-\phi_{2}\right\|\cdot 2H\sqrt{d}+C\sqrt{\left\|\phi_{1 }\right\|\left\|A\right\|\left\|\phi_{1}-\phi_{2}\right\|}+C\sqrt{\left\|\phi _{1}-\phi_{2}\right\|\left\|A\right\|\left\|\phi_{2}\right\|}\] \[\leq \left\|\phi_{1}-\phi_{2}\right\|\cdot 2H\sqrt{d}+2C\sqrt{B\left\| \phi_{1}-\phi_{2}\right\|}\leq(2C\sqrt{B}+2H\sqrt{d\left\|\phi_{1}-\phi_{2} \right\|})\cdot\left\|\phi_{1}-\phi_{2}\right\|\] \[\leq 2C\sqrt{B\left\|\phi_{1}-\phi_{2}\right\|}\leq(2C\sqrt{B}+2H \sqrt{d})\cdot\left\|\phi_{1}-\phi_{2}\right\|\]

Let \(\mathcal{C}_{\phi}\) be the \(\frac{\epsilon}{2C\sqrt{B}+2H\sqrt{d}}\)-net of space \(\{\phi:\left\|\phi\right\|_{2}\leq 1\}\), then by Lemma D.11,

\[|\mathcal{C}_{\phi}|\leq(1+\frac{4C\sqrt{B}+4H\sqrt{d}}{\epsilon})^{d}\]

Therefore, the covering number of space \(\mathcal{V}\) satisfies

\[\log\mathcal{N}_{\epsilon}^{\mathcal{V}}\leq d\log(1+\frac{4C\sqrt{B}+4H\sqrt{ d}}{\epsilon}).\]

**Lemma D.13**.: _Let \(\mathcal{V}\) denote the function class from \(\mathcal{S}\) to \(\mathbb{R}\)_

\[V(\cdot):=\max_{a}\max_{m}\phi(\cdot,a)^{\mathrm{T}}w^{m},\text{where}\left\|w ^{m}\right\|\leq C_{H,d,k,M,\delta},\forall m\in[M]\]

_let \(\mathcal{N}_{\epsilon}\) be the \(\epsilon\)-covering number of \(\mathcal{V}\) with respect to the distance \(\textbf{dist}(V,V^{\prime})=\sup_{s}|V(s)-V^{\prime}(s)|\). Then_

\[\log\mathcal{N}_{\epsilon}\leq dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon}).\]

_Here \(C_{H,d,k,M,\delta}=2H\sqrt{\frac{dk}{\lambda}}+\frac{\sqrt{2d}+\sqrt{2\log(M/ \delta)}}{\sqrt{\lambda}}\)._Proof.: Let \(V_{1}=\max_{a}\max_{m_{\theta}}\phi(\cdot,a)^{\mathrm{T}}w_{1}^{m}\) and \(V_{2}=\max_{a}\max_{m}\phi(\cdot,a)^{\mathrm{T}}w_{2}^{m}\). Then

\[\begin{split}\textbf{dist}(V_{1},V_{2})&=\max_{s}| \max_{a}\max_{m}\phi(\cdot,a)^{\mathrm{T}}w_{1}^{m}-\max_{a}\max_{m}\phi(\cdot,a )^{\mathrm{T}}w_{2}^{m}|\\ &\leq\max_{s,a,m}\left\|\phi(s,a)\right\|\cdot\left\|w_{1}^{m}-w_ {2}^{m}\right\|\leq\max_{s,a,m}\left\|w_{1}^{m}-w_{2}^{m}\right\|,\end{split}\]

For any \(m\in[M]\), let \(\mathcal{C}^{m}\) be the \(\epsilon\)-net for \(\{w^{m}:\left\|w^{m}\right\|\leq C_{H,d,k,M,\delta}\}\), then by Lemma D.11, \(|\mathcal{N}_{\epsilon}^{m}|\leq(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon})^{d}\), implies the total log covering number

\[\log|\mathcal{N}_{\epsilon}|\leq\log\Pi_{m=1}^{M}|\mathcal{N}_{\epsilon}^{m}| \leq dM\log(1+\frac{2C_{H,d,k,M,\delta}}{\epsilon}).\]

### Delayed Feedback

**Lemma D.14** (Lemma 9 of [28]).: _Let \(A,B\in\mathbb{R}^{d\times d}\) be two symmetric positive semi-definite matrices. Then, \(A^{\frac{1}{2}}BA^{\frac{1}{2}}\) and \(AB\) share the same set of eigenvalues. Further, these eigenvalues are all non-negative._

**Lemma D.15**.: _Let \(\Sigma_{h}^{k},\Omega_{h}^{k},\Lambda_{h}^{k}\) be the full design, delayed, and complement matrix respectively. Then \((1+\frac{U_{k}}{\lambda})(\Sigma_{h}^{k})^{-1}\succeq(\Omega_{h}^{k})^{-1}\). In addition, with probability \(1-\delta\),_

\[\max_{k\in[K]}U_{k}\leq\mathbb{E}[\tau]+2\sqrt{2\mathbb{E}[\tau]\log(3K/2 \delta)}+\frac{4}{3}\log(3K/2\delta).\]

Proof.: The proof follows from Lemma 11 of [28] with \(\frac{U_{k}}{\lambda}(\Sigma_{h}^{k})^{-1}\succeq(\Sigma_{h}^{k})^{-1}\Lambda _{h}^{k}(\Omega_{h}^{k})^{-1}\), and then apply Lemma B.2 that \((\Omega_{h}^{k})^{-1}=(\Sigma_{h}^{k})^{-1}+(\Sigma_{h}^{k})^{-1}\Lambda_{h}^ {k}(\Omega_{h}^{k})^{-1}\). The second part comes from Lemma 4 of [28]. 

## Appendix E Experimental Details

In this section, we provide the experimental details of both simulated environments (synthetic linear MDP and RiverSwim) and discuss their results respectively.

### Delayed-UCBVI

As shown in Table 1 and Section 2, there is no prior UCB method that concerns exactly the same delayed linear MDP setting without resorting to specific policy-switching schemes. To benchmark our posterior sampling algorithms, we modify the existing LSVI-UCB method to accommodate the delayed feedback, which is referred to as the Delayed-UCBVI. Below we include the algorithm of delayed-UCBVI for completeness.

``` Input: bonus parameter \(\beta\), regularization \(\lambda\).
1Initialization:\(\forall k,h,\tilde{Q}_{H+1}^{k}(\cdot,\cdot),\tilde{V}_{H+1}(\cdot,\cdot), \tilde{V}_{h}(\cdot,\cdot)\gets 0\), \(\mathcal{D}_{h}\leftarrow\emptyset\).
2forepisode\(k=1,\ldots,K\)do
3 Sample initial state \(s_{1}^{\mathrm{s}}\)fortime step \(h=H_{1},\ldots,1\)do
4\(y_{h}\leftarrow[y_{1}^{h},\ldots,y_{h}^{h-1}]\), with \(y_{h}^{\mathrm{s}}\leftarrow\mathds{1}_{\tau,k-1}\cdot[r_{h}^{\mathrm{s}}+ \tilde{V}_{h+1}(s_{h+1}^{\mathrm{s}})]\)
5\(\Phi_{h}\leftarrow[\phi_{1}^{\mathrm{s}},\phi^{2},\ldots,\phi^{\mathrm{s}-1}]\) with \(\phi^{\mathrm{s}}=\mathds{1}_{\tau,k-1}\cdot\phi(s_{h}^{\mathrm{s}},a_{h}^{ \mathrm{s}})\)
6\(\Omega_{h}^{k}\leftarrow\Phi_{h}\Phi_{h}^{\mathrm{T}}+\lambda I\)
7\(w_{h}^{k}\leftarrow(\Omega_{h}^{k})^{-1}\Phi_{h}y_{h}^{\mathrm{T}}\)
8\(Q_{h}^{k}(\cdot,\cdot)\leftarrow\phi(\cdot,\cdot)^{\mathrm{T}}w_{h}^{k}+ \beta\sqrt{\phi(\cdot,\cdot)^{\mathrm{T}}(\Omega_{h}^{k})^{-1}\phi(\cdot,\cdot)}\)
9\(V_{h}(\cdot,\cdot)\leftarrow\max_{a}\min\{Q_{h}^{k}(\cdot,a),H-h+1\}\)
10 Update \(\pi_{h}^{k}(\cdot)\leftarrow\operatorname*{argmax}_{a\in\mathcal{A}}\min\{Q_{h}^ {k}(\cdot,a),H-h+1\}\)
11fortime step \(h=1,\ldots,H\)do
12 Choose action \(a_{h}^{k}\sim\pi_{h}^{k}(s_{h}^{k})\)
13 Collect transitions \(\mathcal{D}_{h}\leftarrow\mathcal{D}_{h}\cup\{(s_{h}^{k},a_{h}^{k},r_{h}^{k},s _{h+1}^{k})\}\)
14 /* Feedback generated in episode \(k\) cannot be immediately observed in the presence of delay */
15
16 end for ```

**Algorithm 4**Delayed Value Iteration with UCB (Delayed-UCBVI)

### Synthetic Linear MDP Environment

In this section, we describe the further details in Section 5.1.

**Environment Details.** Following [44, 46, 70], we construct a set of synthetic linear MDP environments with \(|\mathcal{S}|=2\), feature dimension \(d=10\), planning horizon \(H=20\), and varying action space \(|\mathcal{A}|\in\{20,50,100\}\). Each action \(a\in\mathcal{A}\subseteq\{0,1\}^{d}\) is encoded with its \(8\)-bit binary representation and represented by a vector \(\bm{b}_{a}\in\mathbb{R}^{8}\). The feature map \(\phi(\cdot,\cdot)\) can then be defined as

\[\phi(s,a)=[\bm{b}_{a}^{\mathrm{T}},\delta(s,a),1-\delta(s,a)]^{\mathrm{T}}\in \mathbb{R}^{10},\ \ \ \ \ \forall(s,a)\in\mathcal{S}\times\mathcal{A},\]

where

\[\delta(s,a)=\begin{cases}1&\text{if }\mathds{1}(s=0)=\mathds{1}(a=0),\\ 0&\text{otherwise}.\end{cases}\]

In addition, let \(\theta_{h}\) that induces the reward functions \(r\) be

\[\theta_{h}=[0,\ldots,0,r,1-r]^{\mathrm{T}}\in\mathbb{R}^{10},\]

with the choice of \(r=0.99\), and further define the measures \(\mu_{h}\) that govern the transition dynamics \(\mathbb{P}\) as

\[\mu_{h}(s)=[0,\ldots,0,(1-s)\oplus\alpha_{h},s\oplus\alpha_{h}],\]

where \(\{\alpha_{h}\}_{h\in[H]}\in\{0,1\}^{H}\) is a sequence of integers taking values \(0\) or \(1\), \(\oplus\) is the XOR operator. By design, the set of environments with identical \(d\) and \(H\) has the same optimal value \(V_{1}^{*}(s_{1})\).

**Further Results and Discussions.** Figure 2 depicts the empirical distributions of delays considered in section 5.1. Additionally, the average return achieved by each method upon convergence is reported in Table 2, corresponding to the results shown in Figure 1. Our empirical findings indicate that posterior sampling methods excel UCB-based methods in terms of both statistical accuracy and computational efficiency. More specifically, under different types of delays, both Delayed-PSVI and Delayed-LPSVI achieve higher return (lower regret) and exhibit faster convergence compared to Delayed-UCBVI.

While delays following multinomial distribution and Poisson distributions decay exponentially fast, Pareto delays are heavy-tailed. When computational budget is limited or when episodes are finite, feedback is only partially observable under long-tailed delays and is not guaranteed to be revealed to the agent. This setup captures the practical scenarios when small time windows are considered for decision-making or in online recommender systems, where only positive feedback (e.g. click, make a purchase) are often observed. As shown in Table 2, performance of Delayed-UCBVI can dramatically deteriorate in the presence of long-tailed delays.

Furthermore, our results presented in Table 4 and Table 3 illustrate the consistent behavior of posterior sampling in environments with delayed feedback, considering both statistical and computational aspects. When employing feature mapping, performance of the algorithms is much less dependent on the sizes of state and action space in contrast to tabular settings. It is worth noting that in large state and action space, the neighborhoods of a substantial number of state-action pairs may remain unvisited, leading to increased uncertainty in estimation. In such cases, adjusting the scale of exploration by decreasing the noise scaling factor \(\sigma\) for Delayed-PSVI can yield faster convergence. Finally, as shown in Table 3, Delayed-LPSVI achieves appealing performance as Delayed-PSVI while reducing computation through the use of approximate sampling with Langevin dynamics.

\begin{table}
\begin{tabular}{c|c|c|c} \hline  & Multinomial Delay & Poisson Delay & Pareto Delay (Shape \\  & \((10,20,30)\) & \((\mathbb{E}[\tau]=50)\) & \(1.0\), Scale 500 \\ \hline Delayed-PSVI (\(\sigma=0.1\)) & \(11.53\pm 0.76\) & \(11.48\pm 0.81\) & \(11.53\pm 0.74\) \\ Delayed-LPSVI (\(c_{\eta}=0.5\)) & \(11.56\pm 0.48\) & \(11.37\pm 0.48\) & \(10.98\pm 0.40\) \\ Delayed-UCBVI (\(c_{\beta}=0.1\)) & \(10.61\pm 0.76\) & \(10.54\pm 0.81\) & \(7.20\pm 0.38\) \\ \hline \end{tabular}
\end{table}
Table 2: Average return achieved by Delayed-PSVI, Delayed-LPSVI and Delayed-UCBVI upon convergence under different delays. Environment setup: \(|\mathcal{S}|=2\), \(|\mathcal{A}|=20\), \(d=10\), \(H=20\). Optimal average return is \(V_{1}^{*}(s_{1})=11.96\). Results are obtained over 10 experiments.

### RiverSwim

RiverSwim environment is known to be a difficult exploration problem for least-squares value iteration with \(\epsilon\)-greedy exploration due to the sparse reward setting. It models an agent swimming in the river who can either swim towards the right (against the current) or towards the left (with the current). While trying to move rightwards may fail with some probability, moving leftwards always yield successful transition. We consider the environment with linear feature maps where \(|\mathcal{S}|=5\), \(d=10\), \(H=20\), and Poisson delays. Accordingly, the tabular environment can be recovered with canonical basis in \(\mathbb{R}^{d}\) as its feature mapping:

\[\phi(s,a)=\bm{e}_{s,a}\in\mathbb{R}^{10},\ \ \ \ \ (s,a)\in\mathcal{S}\times \mathcal{A}.\]

Define \(\theta_{h}\) as

\[\theta_{h}(s,a)=[0.005,0,\dots,0,1.0]^{\mathrm{T}}\in\mathbb{R}^{10},\]

then reward functions induced by \(\theta_{h}\) are given by:

\[r_{h}(s,a)=\begin{cases}0.005&\text{if $s=0$, $a=\text{left};$}\\ 1.0&\text{if $s=4,a=\text{right};$}\\ 0.0&\text{otherwise.}\end{cases}\]

In this environment, We warm start LMC for Delayed-LPSVI by reusing the previous sample for initialization, and set \(M=2\), \(N=40\), \(\eta=c_{\eta}/\lambda_{\max}(\Omega_{h}^{k})\), \(\gamma=c_{\gamma}^{2}dMH^{2}\). We set parameters \(M=2\), \(\nu=1.0\) for Delayed-PSVI, and the bonus coefficient in Delayed-UCBVI as

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline  & \(|\mathcal{S}||\mathcal{A}|=20\) & \(|\mathcal{S}||\mathcal{A}|=40\) & \(|\mathcal{S}||\mathcal{A}|=100\) & \(|\mathcal{S}||\mathcal{A}|=200\) \\ \hline Delayed-PSVI (\(\sigma=0.3\)) & \(11.23\pm 1.00\) & \(11.07\pm 1.05\) & \(10.93\pm 1.11\) & \(10.80\pm 1.13\) \\ Delayed-PSVI (\(\sigma=0.2\)) & \(11.39\pm 0.91\) & \(11.28\pm 0.94\) & \(11.16\pm 1.02\) & \(11.11\pm 1.03\) \\ Delayed-PSVI (\(\sigma=0.1\)) & \(11.57\pm 0.74\) & \(11.48\pm 0.81\) & \(11.39\pm 0.86\) & \(11.33\pm 0.92\) \\ Delayed-LPSVI (\(c_{\eta}=0.5\)) & \(11.31\pm 0.46\) & \(11.37\pm 0.48\) & \(11.57\pm 0.48\) & \(11.57\pm 0.78\) \\ Delayed-UCBVI (\(c_{\beta}=0.1\)) & \(10.98\pm 1.78\) & \(10.54\pm 0.81\) & \(9.67\pm 0.54\) & \(10.01\pm 0.16\) \\ \hline \end{tabular}
\end{table}
Table 4: Average return achieved by Delayed-PSVI, Delayed-LPSVI and Delayed-UCBVI upon convergence in different linear MDP environments with varied \(|\mathcal{S}|\) and \(|\mathcal{A}|\). Optimal average return is \(V_{1}^{*}(s_{1})=11.96\) for all environments (\(d=10\), \(H=20\)). Results are obtained over 10 experiments with Poisson delays (\(\mathbb{E}[\tau]=50\)).

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline  & \(|\mathcal{S}||\mathcal{A}|=20\) & \(|\mathcal{S}||\mathcal{A}|=40\) & \(|\mathcal{S}||\mathcal{A}|=100\) & \(|\mathcal{S}||\mathcal{A}|=200\) \\ \hline Delayed-PSVI (\(\sigma=0.3\)) & \(1418\) & \(1290\) & \(1669\) & \(2633\) \\ Delayed-PSVI (\(\sigma=0.2\)) & \(531\) & \(1114\) & \(1323\) & \(826\) \\ Delayed-PSVI (\(\sigma=0.1\)) & \(391\) & \(571\) & \(650\) & \(709\) \\ Delayed-LPSVI (\(c_{\eta}=0.5\)) & \(293\) & \(246\) & \(517\) & \(566\) \\ Delayed-UCBVI (\(c_{\beta}=0.1\)) & 3205 & 2713 & \(3351\) & \(3694\) \\ \hline \end{tabular}
\end{table}
Table 3: Number of episodes for each method to achieve its highest expected return. Different synthetic environments are examined with varied \(|\mathcal{S}|\) and \(|\mathcal{A}|\). Optimal average return is \(V_{1}^{*}(s_{1})=11.96\) for all environments (\(d=10\), \(H=20\)). Results are obtained over 10 experiments with Poisson delays (\(\mathbb{E}[\tau]=50\)).

Figure 2: Empirical distributions of three types of delays. (a) Multinomial delays with delay categories \(\{10,20,30\}\). (b) Poisson delays with rate \(\mathbb{E}[\tau]=50\). (c) Long-tail Pareto delays with shape 1.0, scale 500. The first two types of delays are well-behaved and decay exponentially fast, while pareto delays are heavy-tailed.

\(d\sqrt{k}(H-h)\). Optimal hyperparameters are determined by gridsearch and we fix \(c_{\beta}=0.04\), \(c_{\eta}=0.5\), \(c_{\gamma}=0.005\), \(\sigma=1.13\). Experiments are repeated with 5 different random seeds. Cumulative regrets are then depicted in Figure 3.

**Results and Discussions.** Compared to the previous synthetic environment where dense rewards are available, posterior sampling methods are shown to be robust with spare rewards even in the presence of delays. Figure 3 shows that both Delayed-PSVI and Delayed-LPSVI outperform Delayed-UCBVI in delayed-feedback settings with linear function approximation. In particular, LMC (Algorithm 3) provides strong concentration such that Delayed-LPSVI is able to maintain the order-optimal regret as Delayed-PSVI when exploring the value-function space.

Figure 3: Delayed-PSVI and Delayed-LPSVI outperform Delayed-UCBVI in sparse-reward setting with Poisson delays (\(\mathbb{E}[\tau]=5\)). Results are reported over \(5\) experiments.