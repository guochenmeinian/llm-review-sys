[MISSING_PAGE_EMPTY:1]

supervised models. Given that self-supervised learning (SSL) is useful for various tasks (_e.g._, NLP [23; 18; 51] and CV [41]) with limited annotated datasets, there have been works on SSL-based audio representation learning [26; 33; 32; 2; 20; 49; 58] and music pre-trained models [43; 35; 69; 62; 31; 11; 29; 53; 66; 37]. The existing benchmarks, GLUE [57], SuperGLUE [56], and ERASER [10] in NLP, along with VTAB [68] and VISSL [16] in CV, all play an active role in promoting the development of SSL-related research topics in the corresponding domains. However, there are only scattered and fragmented evaluations of the existing music models rather than comprehensive benchmarks, making it difficult to objectively compare and draw insights across techniques.

In the current context, the SSL music systems are evaluated with downstream task datasets, including genre classification [26; 35; 69; 62; 31; 8; 29; 66; 37; 34], emotion classification [35; 31; 8; 29; 37; 34], instrument classification [35; 62; 49; 37; 34], music tagging [69; 31; 8; 53; 43; 29; 66; 37; 34], key detection [31; 8; 29; 37; 34], music detection [49], beat tracking [34] and cover song detection [66]. Existing works usually conduct evaluations with different experimental setups, and few of them explore sequential tasks such as beat tracking and source separation. Although in similar domains, SUPERB [65] and HEAR [54] are proposed to facilitate unified analysis of the learned representations of speech and sound events, the distribution of musical audio is significantly different. Thus, there is an urgent need to construct comparable, extensive, and easy-to-use benchmarks to enhance the development of music SSL.

In this paper, we propose a Music Audio Representation Benchmark for universal Evaluation (MARBLE) to address this problem. MARBLE aims to examine the full spectrum of model capabilities, and thus proposes a taxonomy adapted from Dai et al. [9] to categorise MIR tasks, including acoustic, performance, score, and high-level description. The four-level hierarchy aligned to musician consensus serves as a guideline to further organise the datasets and helps to identify a diversified set of downstream tasks. We select popular tasks in the (now defunct) Music Information Retrieval Evaluation eXchange (MIREX) Challenge5, and use the corresponding public datasets with limited annotations. As demonstrated in Tab. 1, the current version of MARBLE contains 18 downstream tasks, spread over 13 task categories on 12 publicly or commercially available datasets. Except for the common classification tasks, we also integrate the missing piece of the puzzle - sequence labelling tasks that require frame-wise prediction, including source separation and beat tracking. The datasets used in MARBLE are ensured easy-to-access: all datasets are available for download directly from the official repository or an external website for downloading a specific version.

Footnote 5: [https://www.music-ir.org/mirex/wiki/MIREX_HOME](https://www.music-ir.org/mirex/wiki/MIREX_HOME)

In addition, we design a unified protocol and build tool-kits to evaluate the generalisation ability of the models. In MARBLE protocol, the models are regarded as backbones to provide universal representations for all tasks, and task-specific prediction heads are concatenated to further trained under _unconstrained_, _semi-constrained_, and _constrained_ settings, which is defined by whether the training hyperparameters are restricted and whether the backbone model is frozen (cf. SS 3.2). The evaluation suite provides codes for dataset preprocessing and examples of evaluating existing popular SSL models in the benchmark. We select 7 representative music SSL models as our baselines (cf. SS 3.1) and release the evaluation results at our publicly available leaderboards6 as a reference.

Footnote 6: Considering potential legal constraints, MARBLE allows to submit results on the tasks partially (_e.g._, tasks on commercially available datasets) for the future participants.

Our key contributions are listed as follows: (1) providing a diversified music understanding benchmark with well-defined taxonomy of the MIR tasks; (2) incorporating and organising a wide range of datasets to facilitate comprehensive music model evaluation; (3) designing a unified assessment protocol and building corresponding evaluation suites for processing, training, and benchmarking.

## 2 Benchmark Tasks

As demonstrated in Tab. 1, we collect datasets in MARBLE to provide the community with a standard, general-purpose, easy-to-use benchmark for various tasks covering all aspects of music. Generally, music processing involves discriminative and generative tasks. The discriminative tasks either classify or regress musical recordings as a whole or use a seq2seq model to make frame-by-frame decisions on entire sequences. The generative tasks include audio synthesis and music composition. For the initial release of MARBLE, we focus on discriminative tasks, and generative tasks are currently outside our scope. The task collection is guided by the principles of (1) receiving a high level of interest 

[MISSING_PAGE_FAIL:3]

- area under the curve) and the average precision (AP) / PR-AUC (precision-recall
- area under the curve). These metrics provide comprehensive insights into the model's performance across all tags.

**Genre classification** aims to assign each song the most suitable genre label. This study uses two distinct datasets: GTZAN [55] and MTG-Genre. GTZAN consists of 30-second audio clips from 10 genres, making it suitable for a multi-class classification task. To assess the performance of this dataset, we report the accuracy metric. To ensure consistent evaluation, we utilise the "fail-filtered" split as described in [24] for GTZAN. The filtered dataset comprises 930 audio tracks corresponding to approximately 8 hours of music. Besides, MTG-Genre, derived from MTG-Jamendo, contains 55k tracks but focuses solely on 95 genre tags, resulting in a multi-label classification problem. We employ the ROC and AP metrics to evaluate the performance of MTG-Genre.

**Emotion Recognition** in music aims to determine the emotional content of music pieces. In our study, we utilise two distinct datasets to evaluate the performance of emotion recognition: Emomisic [52] and MTG-MoodTheme [7]. Emomusic contains 744 pieces of 45-second music clips and is annotated with valence and arousal scores. The valence represents the positivity of emotional responses, while arousal indicates emotional intensity. The official evaluation metrics for this dataset is the determination coefficient (\(r^{2}\)) between the model's regression results and human annotations of arousal and valence [52]. During inference, we split the 45-second clips into 5-second sliding windows and computed the average prediction probability as the final prediction. Since no standard dataset split is available for Emomusic, we adopt the same partitioning as [8]. It is important to note that direct comparison of the SoTA model's results with the benchmark may be challenging due to the different dataset splits. Additionally, we utilise MTG-MoodTheme, a subset of MTG-Jamendo consisting of 18.5k audio tracks annotated with 59 human emotion labels. This is a multi-label task with ROC and AP as evaluation metrics.

### Score-level Tasks

**Pitch Classification in Music (Monophonic)** involves determining the appropriate pitch category for a given audio sample, ranging from MIDI note numbers 0 to 127 on a semitone scale. In this study, we perform pitch classification using the Nsynth dataset [14] within the music information retrieval benchmark. It comprises 340 hours of music, with each excerpt lasting 4 seconds. Since the audio recordings in this dataset are monophonic, the pitch classification task is formulated as a 128-class classification problem, covering all possible MIDI pitch categories (fundamental frequencies from 8Hz to 12.5kHz). The evaluation metric used for this task is the accuracy achieved across all audio clips.

**Beat Tracking** determines the presence of a beat and a downbeat in each frame of a given music piece. In this benchmark, we only focus on beat tracking, making it a binary-classification task7. An offline approach is employed for beat tracking, allowing the model to utilise frame-level information during inference. The model generates frame-by-frame predictions at a specific frequency, which are then post-processed using a dynamic Bayesian network (DBN) [6] implemented with madmom[5] to obtain the final result. The GTZAN Rhythm dataset [36] is used in this study. The dataset provides frame-level annotations for each music clip in GTZAN. To enhance model performance and ensure a fair comparison with the SOTA model, adjacent frames of each beat label are also labelled as beats using a label smoothing technique commonly employed in beat tracking. The model is evaluated using the f_measure metric implemented in mir_eval[45]. A prediction is considered correct if the difference between the predicted event and the ground truth does not exceed 20ms. It is important to note that while some models may have been trained on other datasets, the GTZAN-train subset is used as the training set, and GTZAN-test is used as the test set for all MARBLE submissions.

Footnote 7: Due to the limitation of time and the size of the dataset, tracking the time signature (_e.g._, 4/4 metre) and downbeat is deferred to future versions with other datasets.

**Chord Estimation** is to recognise the temporal music chord of a given piece of music. We implemented this task as a 421-class classification including 35 types of chords on 12 different root notes, and none. More information on the chord vocabulary can be found in Appendix D. The probing model consists of an MLP with a hidden size of 512 and concludes with a fully connected output layer. There is no post-processing involved from frame-level prediction to event-level. The predictions are aligned with the token rate of the pre-trained model. Performances are measured as by 6 measures in SOTA model [22] including root, majmin, mirex, thirds, triads, and sevenths. We have added two additional evaluation metrics inspired by MIREX: majmin_inv and sevenths_inv, to assess the performance of chord recognition at the level of inversions. The metrics are implemented by mir_eval[45]. We use the GuitarSet [63] dataset for this task. The dataset comprises 360 excerpts, each around 30 seconds, recorded by 6 players performing 30 lead sheets in two versions (comping and solving) across 5 styles, 3 progressions, and 2 different tempos. Four audio versions are provided, and we selected the "mix" (a monophonic mixture of the original 6-channel file) for our audio collection. The dataset offers two types of chord annotations for selection, and we chose "performed chord" as our primary annotation and used "instructed chord" to substitute specific colour chords. We divided the audio into 5-second segments and allocated 5 singers into the validation and training sets while designating one singer for the test set. Out of the 5 singers, we allocated 30% to the validation set and 70% to the training set. Segments from the same song are assigned to the same set, for instance, all 5-second segments from a particular song are grouped into the training set.

**Melody Extraction** is to recognise the pitch of melody for a given music, typically pop songs. Adhering to the methodology in [59], we divided the frequency spectrum between 0 and 8000 Hz into 360 bins, treating our task as a classification problem. The probing model consists of a single-layer bidirectional LSTM with a hidden size of 512, followed by a linear layer. The predictions are aligned with the token rate of the pre-trained model, which is then resampled to match the label rate using the nearest interpolation. Performances are measured as the Overall Accuracy metric from the mir_eval[45] library. We use the MedleyDB [4] dataset for this task. It has 108 full tracks, collectively lasting 7.3 hours. All tracks come with three types of melody labels given at intervals of roughly 5.8 ms. For our study, we focused on the second annotation, which indicates the fundamental frequency of mixed stems. For data splitting, we followed the partitioning strategy of [59] giving 67, 15 and 26 tracks for training, validation and testing sets respectively which was achieved after omitting a redundant track from the test set in the popular split.

**Lyrics Transcription** aims to identify the linguistic content in audio recordings of singing. In MARBLE, we focus on the evaluation of multilingual lyrics transcription, an aspect that has been under-explored in the field of lyrics transcription. We perform the task using the MulJam dataset [70], which comprises 6031 songs in 6 languages: English, French, Spanish, Italian, German, and Russian. This dataset offers a rich repository of around 153k lines with lyrics annotations. The training, validation and testing sets contain 147k, 3k and 2k lines, respectively. We set a standard train/valid/test splitting and re-labelled the MulJam dataset as MulJam2.0 with more human annotation. More information can be found in Appendix E. We also use Jamendo [12] as a test set which includes English, French, German and Spanish pop songs. There are 20 songs for each language and the dataset comes with line-level human annotation for lyrics. In line with recent literature [15, 42], the backend adopts a hybrid CTC/Attention architecture design [60]. Given the task's complexity and the necessity to capture long-term dependencies, we use a transformer with 3 encoder layers and 3 decoder layers. The output from the encoder is further processed by a fully connected layer to map it to the target dimension for the CTC loss computation [17]. We also calculate a sequence-to-sequence (S2S) loss between the output from the decoder and the true lyrics text. The final loss is a balanced combination of the CTC loss and the S2S loss. For validation and testing, we employ beam search on the transformer decoder to iteratively select the best predictions. Additionally, a transformer language model is trained from the same data split to incorporate language knowledge at test time. Performance evaluation is conducted using Character Error Rate (CER) and Word Error Rate (WER). Different from all the metrics in other tasks, WER and CER values are the less the better.

### Performance-level Tasks

**Vocal Technique Detection** task involves identifying different singing techniques within an audio clip. For this task, the MARBLE benchmark utilises the VocalSet dataset [61], the sole publicly available dataset specifically designed for studying singing techniques. This dataset comprises recordings of 20 professional singers (9 female and 11 male) performing 17 distinct singing techniques in various contexts, amounting to a total duration of 10.1 hours. Given that the audio clips are segmented into 3-second intervals, the task focuses on determining the type of technique (_e.g._Vibrato, Straight) rather than the precise start and end times. To evaluate the performance of models, we employ Accuracy as the evaluation metric. We use a subset of 10 different singing techniques used in Yamamoto et al. [64], which contains 15 singers in the training and validation set, and 5 for the test set. Since there is no predetermined division between the training and validation sets, we assign 9 singers to the training set and 6 singers to the validation set. It is important to note that all 3-second segments originate from the same audio recording file within the same part of the split, such as being exclusively part of the training set. Detailed data partitioning can be found in our provided code.

### Acoustic-level Tasks

**Instrument Classification** refers to the multi-label or multi-class identification of instruments present in a given audio recording. In the MARBLE benchmark, we utilise two datasets: Nsynth and MTG-instrument. The Nsynth dataset comprises 306,000 audio tracks, each corresponding to one of 11 different instruments. The evaluation metric for this dataset is accuracy. On the other hand, MTG-instrument is a subset of MTG-Jamendo, containing 25,000 audio tracks and 41 instrument tags. Each track can have multiple instrument tags and is evaluated based on ROC and AP.

**Singer Identification** involves recognizing the singer or vocal performer from an audio recording. In previous work on Singer Identification using the VocalSet dataset [61], different splits are employed. For the MARBLE benchmark, we randomly split the dataset into training, validation, and test sets, maintaining a ratio of 12:8:5. All sets contain the same 20 singers. The specific data divisions can be found in the provided code.

**Source Separation** aims to separate different components of a music recording, such as vocals, drums, bass, and others. In MARBLE, we adopt the widely-used MUSDB18 dataset [46] for this task. MUSDB18 consists of 150 full-length music tracks, totalling approximately 10 hours of audio and multiple isolated stems. Our training set consists of 86 tracks, the validation set contains 14 tracks, and the evaluation set comprises 50 tracks, following the official MUSDB18 setting. During training, we randomly sample 6-second segments and apply random track mixing for data augmentation. Due to the complexity of this task, we utilise the baseline architecture from the Music Demixing Challenge (MDX) 2021 [38]. This architecture consists of three linear layers and three bi-directional LSTM layers. The optimization is performed by directly computing the l2-loss between the predicted and ground-truth spectrograms. The evaluation metric for this task is the Source-to-Distortion Ratio (SDR) as defined in [38], which is calculated as the mean across the SDR scores of all songs.

## 3 Evaluation Framework

We aim to explore the generality and standardisation of the framework. Therefore, we freeze the parameters of the pre-trained model to extract pre-trained features as fixed depth embeddings fed to each downstream task-specific prediction head. This allows for as lightweight a solution as possible for all tasks, thus testing whether the representations are easily reusable across different downstream tasks. We describe pre-trained baseline models, downstream models, and protocols in the following sections.

### Pre-trained baseline systems

The audio pre-training models explored in this paper are summarised in Table. 2. Note that we do not cover models designed entirely for speech or not open source models. We also examine all the open-source SSL systems specifically designed from music audio, in total 9 different versions of 7 pre-trained features; see Table. 2 for information on pre-trained models.

**MusiCNN**[43] is a convolutional model pre-trained on the music audio tagging task using the MSD dataset [3]. We use the default configuration of the method, which is to concatenate the mean pooling of the CNN features for a 3-second input with the output of the maximum pool.

**Contrastive learning of musical representations (CLMR)**[53] leverages a 9-layer 1-D convolutional kernel as the feature extractor, employing a number of data augmentation, and is trained on both MSD and MTT. Both are trained with a contrastive learning approach. The model extracts an embedding every 2.69 seconds.

**Jukebox**[11] is a music generation model trained using codified audio language modelling (CALM). It is trained on 1.2 million private songs, and the size of the training set is difficult to estimate the exact number of hours. However, assuming an average song length of 3-6 minutes, the total length could be 60k-120k hours, which is large and diverse to allow Jukebox to learn patterns and structures of different musical genres and styles. We use the same mid-layer representation as [8] to improve computational efficiency. Unlike other representations that run on short context windows, JUKEBOX is trained on a long window of 8192 sample points (23.78 seconds) of audio. We use the same strategy as [8] to extract the audio features on the downstream dataset.

**MULE (Musicnet-ULarge)**[37] is a SSL system based on **SF NFNet-F0**[58], SlowFast Normalizer-Free ResNet. It combines a SlowFast (SF) part (including a slower pathway that captures spatial information and a faster pathway that captures temporal information) with a more efficient and scalable variant of the Normalizer-Free ResNet (NFNet). MULE is contrastively pre-trained on the whole MusicSet dataset [37] and provides promising results on classification tasks. The model extracts an embedding with a 3-second window length and a 2-second hop length.

**MAP-Music2Vec**[31] is a self-supervised learning (SSL) model specifically based on a bootstrapping mask prediction pre-training strategy. It consists of two main components: the student and teacher models. Both share the same architecture with 12 transformer layers, with the teacher model's parameters being exponential moving averages of the student model's parameters. The student model takes in masked input, and during training, it aims to learn deep features from the teacher model based on the output of the unmasked input. Specifically, it computes the average of the top 8 layers of the Transformer's output in the teacher model. To train the MAP-Music2Vec model, a private dataset comprising approximately 1,000 hours of music data was used. The input length of the MAP-Music2Vec model is set to 30 seconds, producing 50 embeddings per second. These embeddings capture essential features of the music data and can be utilised for various downstream tasks, including sequential tasks such as source separation and beat tracking.

**MAP-MERT-v0**, also referred to as MERT-95M\({}^{\text{K-means}}\) in the work by Li et al. [29], is a pre-trained model built upon the speech self-supervised learning (SSL) system HUBERT [20]. It undergoes pre-training for masked prediction, with discrete pseudo-labels obtained from K-Means clustering on music features. The pre-training task of MAP-MERT-v0 involves two pseudo-labels based on logmel and Chroma, along with a CQT reconstruction task that emphasises pitch information. Two versions of the MAP-MERT-v0 model are included: MAP-MERT-v08, trained on a private dataset of 1,000 hours, and MAP-MERT-v0-public9, trained on Music4ALL [50]. The input length of the MAP-MERT-v0 model is set to 5 seconds, generating 50 embeddings per second. This design facilitates fine-tuning for sequential tasks, enabling efficient and effective processing of music data.

Footnote 8: [https://huggingface.co/m-a-p/MERT-v0](https://huggingface.co/m-a-p/MERT-v0)

Footnote 9: [https://huggingface.co/m-a-p/MERT-v0-public](https://huggingface.co/m-a-p/MERT-v0-public)

**MAP-MERT-v1** encompasses two variants: (MAP-)MERT-v1-base10 and (MAP-)MERT-v1-large11. These models, also known as MERT-95M\({}^{\text{RVQ-VAE}}\) and MERT-330M\({}^{\text{RVQ-VAE}}\) in the work by Li _et al_. [29], employ EnCodec, a pre-trained discrete deep feature, as a replacement for the K-means feature. This modification facilitates the scaling up of the model. Similar to MAP-MERT-v0, the input length of the MAP-MERT-v1 models is 5 seconds, but they produce 75 embeddings per second.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multirow{2}{*}{\begin{tabular}{c} **MusicCNN** \\ **MSD-big** \\ \end{tabular} } & \multirow{2}{*}{**CLMR**} & \multirow{2}{*}{**Jukebox**} & \multirow{2}{*}{**MULE**} & \multirow{2}{*}{**MAP-Music2Vec**} & \multirow{2}{*}{\begin{tabular}{c} **MAP-MERT-v0** \\ **base** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **MAP-MERT-v1** \\ **base** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **MAP-MERT-v1** \\ **base** \\ \end{tabular} } \\ \cline{5-10}  & & & & & & & & & \\ \hline \hline \multirow{2}{*}{**Network**} & \multirow{2}{*}{CNN} & \multirow{2}{*}{9-Conv} & 3-Conv, & 22-Conv, & 7-Conv, & 7-Conv, & 7-Conv, & 7-Conv, & 7-Conv, & 7-Conv, \\  & & 36-Trans & 2-Trans & 12-Trans & 12-Trans & 12-Trans & 12-Trans & 12-Trans \\ \hline \hline \multirow{2}{*}{**\#Params**} & \multirow{2}{*}{8M} & \multirow{2}{*}{2.5M} & \multirow{2}{*}{5B} & \multirow{2}{*}{62.4M} & \multirow{2}{*}{95M} & \multirow{2}{*}{95M} & \multirow{2}{*}{95M} & \multirow{2}{*}{95M} & \multirow{2}{*}{95M} & \multirow{2}{*}{330M} \\ \cline{1-1} \cline{5-10}  & & & & & & & & & & \\ \hline \hline \multirow{2}{*}{**Input**} & \multirow{2}{*}{\(\log\)-mel} & \multirow{2}{*}{waveform} & \multirow{2}{*}{waveform} & \multirow{2}{*}{log-mel} & \multirow{2}{*}{waveform} & \multirow{2}{*}{waveform} & \multirow{2}{*}{waveform} & \multirow{2}{*}{waveform} & \multirow{2}{*}{waveform} & \multirow{2}{*}{waveform} \\  & & & & & & & & & \\ \hline \hline \multirow{2}{*}{**Stride**} & \multirow{2}{*}{3s} & \multirow{2}{*}{2.69s} & \multirow{2}{*}{23.78s} & \multirow{2}{*}{2s} & \multirow{2}{*}{20ms} & \multirow{2}{*}{20ms} & \multirow{2}{*}{20ms} & \multirow{2}{*}{13.3ms} & \multirow{2}{*}{13.3ms} \\ \cline{1-1} \cline{5-10}  & & & & & & & & & \\ \hline \hline \multirow{2}{*}{**Context Length**} & \multirow{2}{*}{10–20k} & \multirow{2}{*}{1.7k} & \multirow{2}{*}{60–120k} & \multirow{2}{*}{117.5k} & \multirow{2}{*}{1k} & \multirow{2}{*}{1k} & \multirow{2}{*}{0.9k} & \multirow{2}{*}{17k} & \multirow{2}{*}{160k} \\ \cline{1-1} \cline{5-10}  & & & & & & & & & \\ \hline \hline \multirow{2}{*}{**Pre-training Task**} & \multirow{2}{*}{\begin{tabular}{c} Music \\ Tagging \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Contrastive \\ Learning \\ \end{tabular} } & \multirow{2}{*}{CALM} & \multirow{2}{*}{\begin{tabular}{c} Contrastive \\ Learning \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} MLM \\ Learning \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} MLM \\ Boosting \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} MLM \\ Clustering \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} MLM \\ Clustering \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} MLM \\ Clustering \\ \end{tabular} } \\ \hline \hline \end{tabular}
\end{table}
Table 2: Information of Baseline Systems.

[MISSING_PAGE_FAIL:8]

they are able to approach, if not surpass, the previous state-of-the-art (SOTA) in many tasks. For instance, the best performance on NSynth Pitch classification have achieved up to 94.4% accuracy. Nonetheless, the majority of tasks are still far from being solved, including music tagging and source separation tasks. Notably, the performance on MUSDB18 is merely half of the previous SOTAs.

Figure 1: SSL Baselines Compared to previous SOTA. The performances of the tasks are merged according to the task types demonstrated in Tab. 1. Results not applicable are set to \(0\).

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline
**Dataset** & **Methods/DBh** & **Medium** & **Jammando** & & & & & & & & & & & \\
**Task** & **Mobility** & **Lyrics** & & & & & & & & & & & & \\ \hline
**Metrics** & **Acc** & **CIR** & **WIR** & **CIR** & **WIR** & **rect** & **mpa** & **mision** & **mision** & **mision** & **mision** & **mision** & **mision** & **mision** & **mision** & **mision** & **mision** \\ \hline \hline
**M.M-MossezNet**[11] & 36.1 & 36.4 & 87.8 & 85.7 & 89.6 & 13.7 & 11.1 & 10.4 & 10.4 & 10.4 & 10.4 & 11.1 & 9.4 & 9.4 \\
**M.M-MossezNet**[10] & 46.0 & 52.6 & 83.4 & 87.6 & 48.7 & 38.9 & 10.7 & 36.6 & 36.5 & 37.5 & 30.3 & **29.6** \\
**M.M-MossezNet**[10] & 36.1 & 53.5 & 82.7 & 52.6 & 85.2 & 94.1 & 38.7 & 36.4 & 36.6 & 36.4 & 37.5 & 29.9 & 25.6 \\
**M.M-MossezNet**[11] & 60.8 & 40.4 & 77.9 & **20.6** & **82.2** & 50.5 & 38.8 & 36.5 & 36.7 & 36.4 & 36.6 & 34.2 & 28.9 \\
**M.M-MossezNet**[12] & **48.5** & **48.8** & **77.8** & 50.3 & 83.1 & **51.6** & **46.8** & **45.1** & **46.0** & **46.0** & **46.2** & 27.9 & 28.5 \\ \hline
**Previous SOTA** & 68.3*[9] & 30.5 & 54.8*[4] & 25.4 & 44.4*[4] & 34.8 & 33.5 & 33.6 & 33.3 & 33.2 & 24.0 & 33.1 & 23.9*[21] \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performances of Baselines Evaluated on MARBLE with constrained settings (3/3). The overall average scores are calculated on the systems applicable to all tasks.

Figure 2: Results Analysis Regarding to Training Data Size. Since some models are not applicable to the sequence labelling tasks, the performances of _source separation_ and _beat tracking_ tasks are excluded on acoustic-level and score-level average score calculation correspondingly. The radii of the scatter points are isometrically log scaling with the parameter sizes.

The MAP family achieves balanced results, successfully performing tasks including sequence labelling, which other models fail to accomplish (as they do not provide frame-level representations or are too cumbersome to train). This series of models excel at multiple taxonomy levels. On certain tasks, MAP-MERTS achieve results close to or surpass the previous state-of-the-art. However, music tagging tasks are dominated by Jukebox-5B and MULE. Jukebox may benefit from its massive parameter size and generative modelling of detailed information, as well as the introduction of metadata during the pre-training period. Conversely, MULE benefits from its proprietary large-scale, high-quality dataset, MusicSet, and the highly discriminative representations learned by contrastive pre-training.

Based on Fig. 1(a) and 1(b), excluding sequence labelling tasks (as some baselines do not support them), we observe a general trend: as the volume of data and the size of model parameters increase, the performance of tasks across four levels correspondingly improves. The choice of pre-training method and model size significantly influences the performance. For instance, MAP-Music2Vec-95M, utilizing only 1k hours of data for self-supervised learning, outperforms both supervised pre-trained MusiCNN-8M and contrastive pre-trained CLMR-2.5M on the same scale of data. More analysis could be referred to Appendix B.

## 5 Conclusion

In this work, we introduce the Music Audio Representation Benchmark for universalEvaluation (MARBLE) as a comprehensive benchmark for evaluating pre-trained music features. It encompasses a hierarchy taxonomy that covers acoustic, performance, score, and high-level description levels, and utilises publicly available datasets for 18 MIR tasks. We establish a standardised preprocessing and data splitting protocol, along with a unified evaluation framework, to ensure fair and reproducible assessment. We report the results of all 9 open-sourced pre-trained models developed on music recordings, showcasing their performance across multiple tasks. The results demonstrate that several pre-trained models achieve comparable or even superior performance to the state-of-the-art models on various tasks within MARBLE. However, there is still ample room for improvement, particularly in music tagging and source separation. With the release of the toolkit, we hope to facilitate future research by providing easy access, reproducibility, and fair comparison of SSL pre-trained models for music understanding. We encourage engagement from researchers in the audio and AI communities to contribute to the advancement of representation learning for music information retrieval.

## Discussion and Future Work

Our benchmark has some shortcomings that can be further improved. To begin with, some of the tasks, such as beat tracking and piano transcription, typically use multiple evaluation metrics, but we only include one or two for each of the tasks due to the copyright issues preventing many datasets from being publicly available, lack of standard pre-processing or maintenance, and the limitation of time. Although the selected metric is fundamental and a good indicator, an average of all the metrics might be a better choice. Besides, some of the datasets are not sufficient for a single task. For example, the GTZAN dataset does not have a commercially-available license, and it only includes less than 10 hours of music recordings, making the evaluation more subject to bias. We will include more commercially-available larger datasets on the same tasks. Moreover, we do not include some MIR tasks that lack a common dataset currently, such as cover song detection and query-by-humming. In the future version, we will include more datasets and tasks. Last but not least, MIR on symbolic music is not included in the first version of our benchmark as well.

Apart from the traditional MIR tasks, some interesting tasks deserve more attention for benchmark development in the computer music and AIGC communities. With the benchmark and pre-trained models in MIR, developing an evaluation score on music generation and synthesis might be possible. There may not exist a perfect solution on the subject metrics for music generation to build a benchmark; otherwise, composing musical art will simply search for the waveform with the highest scores. But one can expect such a benchmark can be helpful for the music industry or music education to preclude some bad music generation. Besides, multi-modal approaches that combine music audio with symbolic music and language (_e.g._, lyrics and music description) also deserve a benchmark.

## Acknowledgements

We would like to express sincere gratitude to our friends Anqiao Yang and Wei Fan for their invaluable support during the writing of this paper.

Ruibin Yuan is funded by Theme-based Research Scheme (T45-205/21-N) and Early Career Scheme (ECS-HKUST22201322), Research Grants Council of Hong Kong. Yinghao Ma and Jiawen Huang are research students at the UKRI Centre for Doctoral Training in Artificial Intelligence and Music, supported by UK Research and Innovation (Grant Number: EP/S022694/1). Yizhi Li is fully funded by an industrial PhD studentship (Grant Number: 171362) from the University of Sheffield, UK. Emmanouil Benetos is supported by RAEng/Leverhulme Trust Research Fellowship LTRF2223-19-106.

We acknowledge IT Services at The University of Sheffield for the provision of services for High-Performance Computing. This project also made use of time on Tier 2 HPC facility JADE2, funded by EPSRC (EP/T022205/1)

Besides, we would like to give special thanks to the following researchers or musicians on the lyrics labelling: Leo Nebel in LIP6 at Sorbonne Universite; Nick Magal in School of Music at Carnegie Mellon University; Carey Bunk, Yannis Vasilakis, Christopher Mitcheltree, Nelly Victoria Alexandra Garcia-Sihuay, Teresa Pelinski Ramos, Ilaria Manco, David Sudholt, Jordan Shier, and Matthew Rice in Centre for Digital Music at Queen Mary University of London; Emilian Postolache in Sapienza University of Rome; as well as Wenqin Yu in the Chinese Music Institute at Peking University.

## References

* Alonso-Jimenez et al. (2022) Alonso-Jimenez, P., Serra, X., and Bogdanov, D. (2022). Music representation learning based on editorial metadata from discogs.
* Baevski et al. (2020) Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. _Advances in neural information processing systems_, 33:12449-12460.
* Bertin-Mahieux et al. (2011) Bertin-Mahieux, T., Ellis, D. P., Whitman, B., and Lamere, P. (2011). The million song dataset.
* Bittner et al. (2014) Bittner, R., Salamon, J., Tierney, M., Mauch, M., Cannam, C., and Bello, J. (2014). Medleydb: A multitrack dataset for annotation-intensive mir research.
* Bock et al. (2016a) Bock, S., Korzeniowski, F., Schluter, J., Krebs, F., and Widmer, G. (2016a). madmom: a new Python Audio and Music Signal Processing Library. In _Proceedings of the 24th ACM International Conference on Multimedia_, pages 1174-1178, Amsterdam, The Netherlands.
* Bock et al. (2016b) Bock, S., Krebs, F., and Widmer, G. (2016b). Joint beat and downbeat tracking with recurrent neural networks. In _ISMIR_, pages 255-261. New York City.
* Bogdanov et al. (2019) Bogdanov, D., Won, M., Tovstogan, P., Porter, A., and Serra, X. (2019). The mtg-jamendo dataset for automatic music tagging. In _International Conference on Machine Learning_. ICML.
* Castellon et al. (2021) Castellon, R., Donahue, C., and Liang, P. (2021). Codified audio language modeling learns useful representations for music information retrieval. _arXiv preprint arXiv:2107.05677_.
* Dai et al. (2018) Dai, S., Zhang, Z., and Xia, G. G. (2018). Music style transfer: A position paper. _arXiv preprint arXiv:1803.06841_.
* DeYoung et al. (2020) DeYoung, J., Jain, S., Rajani, N. F., Lehman, E., Xiong, C., Socher, R., and Wallace, B. C. (2020). Eraser: A benchmark to evaluate rationalized nlp models. _Transactions of the Association for Computational Linguistics_.
* Dhariwal et al. (2020) Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I. (2020). Jukebox: A generative model for music. _arXiv preprint arXiv:2005.00341_.
* Durand et al. (2023a) Durand, S., Stoller, D., and Ewert, S. (2023a). Contrastive learning-based audio to lyrics alignment for multiple languages. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE.
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5.
* Engel et al. (2017) Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi, M., Eck, D., and Simonyan, K. (2017). Neural audio synthesis of musical notes with wavenet autoencoders. In _International Conference on Machine Learning_, pages 1068-1077. PMLR.
* Gao et al. (2023) Gao, X., Gupta, C., and Li, H. (2023). Polyscriber: Integrated fine-tuning of extractor and lyrics transcriber for polyphonic music. _IEEE ACM Trans. Audio Speech Lang. Process._, 31:1968-1981.
* Goyal et al. (2019) Goyal, P., Mahajan, D., Gupta, A., and Misra, I. (2019). Scaling and benchmarking self-supervised visual representation learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6391-6400.
* Graves et al. (2006) Graves, A., Fernandez, S., Gomez, F. J., and Schmidhuber, J. (2006). Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In _Proc. ICML_, volume 148, pages 369-376. ACM.
* Gururangan et al. (2020) Gururangan, S., Marasovic, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A. (2020). Don't stop pretraining: Adapt language models to domains and tasks. _arXiv preprint arXiv:2004.10964_.

* Heydari et al. [2021] Heydari, M., Cwikowitz, F., and Duan, Z. (2021). Beatnet: Crnn and particle filtering for online joint beat downbeat and meter tracking. _arXiv preprint arXiv:2108.03576_.
* Hsu et al. [2021] Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. (2021). Hubert: Self-supervised speech representation learning by masked prediction of hidden units. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 29:3451-3460.
* Huang et al. [2022] Huang, Q., Jansen, A., Lee, J., Ganti, R., Li, J. Y., and Ellis, D. P. (2022). Mulan: A joint embedding of music audio and natural language. _arXiv preprint arXiv:2208.12415_.
* Jiang et al. [2019] Jiang, J., Chen, K., Li, W., and Xia, G. (2019). Large-vocabulary chord transcription via chord structure decomposition. In _ISMIR_, pages 644-651.
* Kenton and Toutanova [2019] Kenton, J. D. M.-W. C. and Toutanova, L. K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186.
* Kereliuk et al. [2015] Kereliuk, C., Sturm, B. L., and Larsen, J. (2015). Deep learning and music adversaries. _IEEE Transactions on Multimedia_, 17(11):2059-2071.
* Knees et al. [2015] Knees, P., Faraldo Perez, A., Boyer, H., Vogl, R., Bock, S., Horschlager, F., Le Goff, M., et al. (2015). Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections. In _Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR); 2015 Oct 26-30; Malaga, Spain.[Malaga]: International Society for Music Information Retrieval, 2015. p. 364-70_. International Society for Music Information Retrieval (ISMIR).
* Kong et al. [2020] Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., and Plumbley, M. D. (2020). Panns: Large-scale pretrained audio neural networks for audio pattern recognition. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 28:2880-2894.
* Korzeniowski and Widmer [2017] Korzeniowski, F. and Widmer, G. (2017). End-to-end musical key estimation using a convolutional neural network. In _2017 25th European Signal Processing Conference (EUSIPCO)_, pages 966-970. IEEE.
* Law et al. [2009] Law, E., West, K., Mandel, M. I., Bay, M., and Downie, J. S. (2009). Evaluation of algorithms using games: The case of music tagging. In _ISMIR_, pages 387-392. Citeseer.
* Li et al. [2023] Li, Y., Yuan, R., Zhang, G., Ma, Y., Chen, X., Yin, H., Lin, C., Ragni, A., Benetos, E., Gyenge, N., Dannenberg, R., Liu, R., Chen, W., Xia, G., Shi, Y., Huang, W., Guo, Y., and Fu, J. (2023). Mert: Acoustic music understanding model with large-scale self-supervised training.
* Li et al. [2022a] Li, Y., Yuan, R., Zhang, G., Ma, Y., Lin, C., Chen, X., Ragni, A., Yin, H., Hu, Z., He, H., et al. (2022a). Large-scale pretrained model for self-supervised music audio representation learning.
* Li et al. [2022b] Li, Y., Yuan, R., Zhang, G., MA, Y., Lin, C., Chen, X., Ragni, A., Yin, H., Hu, Z., He, H., et al. (2022b). Map-music2vec: A simple and effective baseline for self-supervised music audio representation learning. In _Ismir 2022 Hybrid Conference_.
* Ling et al. [2020] Ling, S., Liu, Y., Salazar, J., and Kirchhoff, K. (2020). Deep contextualized acoustic representations for semi-supervised speech recognition. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 6429-6433. IEEE.
* Liu et al. [2020] Liu, A. T., Yang, S.-w., Chi, P.-H., Hsu, P.-c., and Lee, H.-y. (2020). Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 6419-6423. IEEE.
* Ma et al. [2023] Ma, Y., Yuan, R., Li, Y., Zhang, G., Chen, X., Yin, H., Lin, C., Benetos, E., Ragni, A., Gyenge, N., et al. (2023). On the effectiveness of speech self-supervised learning for music. International Society for Music Information Retrieval Conference (ISMIR).
* Manco et al. [2022] Manco, I., Benetos, E., Quinton, E., and Fazekas, G. (2022). Learning music audio representations via weak language supervision. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 456-460. IEEE.

* [36] Marchand, U. and Peeters, G. (2015). Swing ratio estimation. In _Digital Audio Effects 2015 (Dafx15)_.
* [37] McCallum, M. C., Korzeniowski, F., Oramas, S., Gouyon, F., and Ehmann, A. F. (2022). Supervised and unsupervised learning of audio representations for music understanding. _Ismir 2022 Hybrid Conference_.
* [38] Mitsufuji, Y., Fabbro, G., Uhlich, S., Stoter, F.-R., Defossez, A., Kim, M., Choi, W., Yu, C.-Y., and Cheuk, K.-W. (2022). Music demixing challenge 2021. _Frontiers in Signal Processing_, 1:18.
* [39] Modrzejewski, M., Szachewicz, P., and Rokita, P. (2023). Transfer learning with deep neural embeddings for music classification tasks. In _Artificial Intelligence and Soft Computing: 21st International Conference, ICAISC 2022, Zakopane, Poland, June 19-23, 2022, Proceedings, Part I_, pages 72-81. Springer.
* [40] Muller, M. (2015). _Fundamentals of music processing: Audio, analysis, algorithms, applications_, volume 5. Springer.
* [41] Newell, A. and Deng, J. (2020). How useful is self-supervised pretraining for visual tasks? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7345-7354.
* [42] Ou, L., Gu, X., and Wang, Y. (2022). Transfer learning of wav2vec 2.0 for automatic lyric transcription. In Rao, P., Murthy, H. A., Srinivasamurthy, A., Bittner, R. M., Repetto, R. C., Goto, M., Serra, X., and Miron, M., editors, _Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022_, pages 891-899.
* [43] Pons, J. and Serra, X. (2019). musicnn: Pre-trained convolutional neural networks for music audio tagging. _arXiv preprint arXiv:1909.06654_.
* [44] Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. (2023). Robust speech recognition via large-scale weak supervision. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 28492-28518. PMLR.
* [45] Raffel, C., McFee, B., Humphrey, E. J., Salamon, J., Nieto, O., Liang, D., Ellis, D. P., and Raffel, C. C. (2014). Mir_eval: A transparent implementation of common mir metrics. In _ISMIR_, pages 367-372.
* [46] Rafii, Z., Liutkus, A., Stoter, F.-R., Mimilakis, S. I., and Bittner, R. (2017). The MUSDB18 corpus for music separation.
* [47] Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.-W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na, H., Gao, Y., Mori, R. D., and Bengio, Y. (2021). SpeechBrain: A general-purpose speech toolkit. arXiv:2106.04624.
* [48] Rouard, S., Massa, F., and Defossez, A. (2023). Hybrid transformers for music source separation. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE.
* [49] Saeed, A., Grangier, D., and Zeghidour, N. (2021). Contrastive learning of general-purpose audio representations. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3875-3879. IEEE.
* [50] Santana, I. A. P., Pinhelli, F., Donini, J., Catharin, L., Mangolin, R. B., Feltrim, V. D., Domingues, M. A., et al. (2020). Music4all: A new music database and its applications. In _2020 International Conference on Systems, Signals and Image Processing (IWSSIP)_, pages 399-404. IEEE.

* [51] Sarzynska-Wawer, J., Wawer, A., Pawlak, A., Szymanowska, J., Stefaniak, I., Jarkiewicz, M., and Okruszek, L. (2021). Detecting formal thought disorder by deep contextualized word representations. _Psychiatry Research_, 304:114135.
* [52] Soleymani, M., Caro, M. N., Schmidt, E. M., Sha, C.-Y., and Yang, Y.-H. (2013). 1000 songs for emotional analysis of music. In _Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia_, pages 1-6.
* [53] Spijkervet, J. and Burgoyne, J. A. (2021). Contrastive learning of musical representations. _arXiv preprint arXiv:2103.09410_.
* [54] Turian, J., Shier, J., Khan, H. R., Raj, B., Schuller, B. W., Steinmetz, C. J., Malloy, C., Tzanetakis, G., Velarde, G., McNally, K., et al. (2022). Hear: Holistic evaluation of audio representations. In _NeurIPS 2021 Competitions and Demonstrations Track_, pages 125-145. PMLR.
* [55] Tzanetakis, G. and Cook, P. (2002). Musical genre classification of audio signals. _IEEE Transactions on speech and audio processing_, 10(5):293-302.
* [56] Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2019). Superglue: A stickier benchmark for general-purpose language understanding systems. _Advances in neural information processing systems_, 32.
* [57] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-task benchmark and analysis platform for natural language understanding. In _International Conference on Learning Representations_.
* [58] Wang, L., Luc, P., Wu, Y., Recasens, A., Smaira, L., Brock, A., Jaegle, A., Alayrac, J.-B., Dieleman, S., Carreira, J., et al. (2022a). Towards learning universal audio representations. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4593-4597. IEEE.
* [59] Wang, X., Liu, L., and Shi, J. (2022b). Dilated Convolutional Model for Melody Extraction.
* [60] Watanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. (2017). Hybrid ctc/attention architecture for end-to-end speech recognition. _IEEE Journal of Selected Topics in Signal Processing_, 11(8):1240-1253.
* [61] Wilkins, J., Seetharaman, P., Wahl, A., and Pardo, B. (2018). Vocalset: A singing voice dataset. In _ISMIR_, pages 468-474.
* [62] Wu, H.-H., Kao, C.-C., Tang, Q., Sun, M., McFee, B., Bello, J. P., and Wang, C. (2021). Multi-task self-supervised pre-training for music classification. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 556-560. IEEE.
* [63] Xi, Q., Bittner, R. M., Pauwels, J., Ye, X., and Bello, J. P. (2018). Guitarset: A dataset for guitar transcription. In _ISMIR_, pages 453-460.
* [64] Yamamoto, Y., Nam, J., and Terasawa, H. (2022). Deformable cnn and imbalance-aware feature learning for singing technique classification. _arXiv preprint arXiv:2206.12230_.
* [65] Yang, S.-w., Chi, P.-H., Chuang, Y.-S., Lai, C.-I. J., Lakhotia, K., Lin, Y. Y., Liu, A. T., Shi, J., Chang, X., Lin, G.-T., et al. (2021). Superb: Speech processing universal performance benchmark. _arXiv preprint arXiv:2105.01051_.
* [66] Yao, D., Zhao, Z., Zhang, S., Zhu, J., Zhu, Y., Zhang, R., and He, X. (2022). Contrastive learning with positive-negative frame mask for music representation. In _Proceedings of the ACM Web Conference 2022_, pages 2906-2915.
* [67] Zai El Amri, W., Tautz, O., Ritter, H., and Melnik, A. (2022). Transfer learning with jukebox for music source separation. In _Artificial Intelligence Applications and Innovations: 18th IFIP WG 12.5 International Conference, AIAI 2022, Hersonisos, Crete, Greece, June 17-20, 2022, Proceedings, Part II_, pages 426-433. Springer.

* [68]Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neumann, M., Dosovitskiy, A., et al. (2019). The visual task adaptation benchmark.
* [69] Zhao, Y. and Guo, J. (2021). Musicoder: A universal music-acoustic encoder based on transformer. In _International Conference on Multimedia Modeling_, pages 417-429. Springer.
* [70] Zhuo, L., Yuan, R., Pan, J., Ma, Y., Li, Y., Zhang, G., Liu, S., Dannenberg, R., Fu, J., Lin, C., et al. (2023). Lyricwhiz: Robust multilingual lyrics transcription by whispering to chatgpt. International Society for Music Information Retrieval Conference (ISMIR).

## Appendix A Evaluation Protocol Details

The hyper-parameter search range of the constrained evaluation track is given as follow:

1. **Layer**: {every single layer, weighted sum}
2. **Model**: {one-layer 512-units MLP, one-layer 512-unit LSTM (melody extraction only), 3-layer 512-unit LSTM (source separation only), 3-encoder-3-decoder layers transformer (lyrics transcription only)}
3. **Batch size**: {64}
4. **Learning rate**: {5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
5. **Dropout probability**: {0.2}

## Appendix B Detail Analysis

**What have the music audio pre-trained representations learned?** We observe that all the representations have learned multiple levels of knowledge in Fig. 1. Most of the selected baselines are particularly good at high-level music description tasks, such as genre classification and emotion recognition. However, when pre-trained with a full supervision paradigm, the representations may not be able to model pitch and key well, as they could overfit the supervision signal less relevant to pitch-related information. On the contrary, SSL methods usually mitigate this issue by providing more generalisable representations. Some representations do not support frame-level representations, which makes it difficult to evaluate their performance on tasks such as source-separation and beat tracking. Therefore, it is unclear how well these models have learned such information.

**How can we design better pre-training strategies for music audio representation learning?** As mentioned in the above paragraph, we suggest that a good pre-training strategy needs to prevent overfitting the supervision signal, which makes self-supervised learning a more promising approach. Moreover, we argue that an optimal method for music pre-training should be able to scale up to larger data and model size. Based on observations from Figure 2, it appears that larger data and model size have a greater impact on performance than the training paradigm (generative, contrastive, or mask prediction) at the current stage of research. Besides, stacked transformer models are good candidates for future pre-training architecture, as they can be easily scaled up, and usually provide frame-level representations in a well-considered design.

**How does context length affect performance?** According to Fig. 3, the relationship between context length and performance exhibits a rather complex and irregular pattern, for which it is currently difficult to draw any conclusive insights. This is due to the limited number of music audio representations available at the moment, coupled with challenges in controlling variables. However, we are able to derive some preliminary observations when considering factors such as data size (D)

Figure 3: Results Analysis Regarding Training Context Length. The performances of _source separation_ and _beat tracking_ tasks are ignored similar to Fig. 2.

[MISSING_PAGE_FAIL:18]

## 6 Conclusion

Figure 5: Music Understanding Model Leaderboard Hosted on the MARBLE Website.

## Appendix D Details on Chord Estimation

### Chord Vocabulary

Our chord vocabulary includes "none" and 35 different chords on each of the 12 root notes, 421 in total. The root notes are listed as follows: {C Db D Eb E F Gb G Ab A Bb B}. We do not distinguish between equal notes under the twelve equal temperatures. For example, we think that C# and Db are the same note and have the essentially equivalent function in chord prediction. We use sharp in the code implementation for identification but use flat in the following tables.

The following Tables of the 35 types of chords with examples and a number of samples in the datasets.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \hline chord name & maj & min & aug & maj6 & min6 & 7 & maj7 & min7 & dim7 & hdim7 \\ \hline \hline example & C:maj & C:min & C:aug & C:maj6 & C:min6 & C:7 & C:maj7 & C:min7 & C:dim7 & C:dim7 \\ \hline \hline chord tones & 1,3,55 & 1,83,5 & 1,3,5 & 1,3,5,6 & 1,83,5 & 1,3,5,7 & 1,3,5,7 & 1,3,5,7 & 1,83,5,8 & 1,83,5,8 & 1,83,5,8 & 1,83,5,8 & 1,83,5,8 & 1,83,5,8 & 1,83,5,8 \\ \hline chord number & 1120 & 368 & 16 & 70 & 12 & 374 & 292 & 204 & 2 & 106 \\ \hline \hline \end{tabular}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \hline chord name & 9 & min9 & min9 & 11 & min2 & min4 & min9/3 & min9/5 & min9/10 & min5. **traids**: Chords are considered at the level of triads (major, minor, augmented, diminished, suspended). In addition to the root, the quality is only considered through #5th scale degree (for augmented chords). For example, ('A:7', 'A:maj') are equivalent, while ('A:min', 'A:dim') and ('A:aug', 'A:maj') are not.
6. **sevenths**: Compares according to MIREX "sevenths" rules. Only major, major seventh, seventh, minor, minor seventh and no chord labels are compared.
7. **maimin_inv**: Compares major/minor chords, with inversions. The bass note must exist in the triad.
8. **sevenths_inv**: Compares according to MIREX "sevenths" rules, with inversions. The bass note must exist in the chord.

During the evaluation process, frame-level predictions are directly merged to event-level by the mir_eval function so we do not apply any post-processing to the prediction.

## Appendix E Details on Lyrics Transcription

### MulJam2.0 dataset

MulJam2.0 is derived from MulJam, featuring larger and more refined human annotation on the test set. We select 34 songs from the training set and obtain human lyrics annotation to expand the test set. For each language, 20 songs are randomly selected from the original training set to form the validation set. A few songs are excluded due to poor alignment for obtaining the line-level annotations (For details, please refer to [70]). We also exclude the songs in the training and validation sets that were present in Jamendo (3 songs in training and 1 song in validation), ensuring that the songs in the evaluation datasets remain unseen during training. The numbers of songs by language can be found in Tab. 6.

The human annotation is performed at the song level. We applied similar procedures to obtain line-level annotations, as was done for the training set in MulJam. We use the timestamps provided by Whisper [44], and align the lines predicted by Whisper with the human annotation. As in [70], lines with unusually high character rates (exceeding 37.5 Hz) are removed. However, for the test set we choose not to filter by the similarity between the aligned text pairs, to prevent introducing excessive bias in favor of Whisper predictions.

### Language Model and Tokenizer

The language model (LM) is trained using a speechbrain [47] language model recipe 12. The model comprises of 12 transformer encoder layers, with an attention dimension of 768, 12 attention heads, and a position-wise feed-forward layer dimension of 3072. The LM is trained using cross-entropy loss for 20 epochs, and the model with the lowest loss is selected.

Footnote 12: [https://github.com/speechbrain/speechbrain/blob/develop/recipes/LibriSpeech/LM/bparams/transformer.yaml](https://github.com/speechbrain/speechbrain/blob/develop/recipes/LibriSpeech/LM/bparams/transformer.yaml)

The target character set is the union of the character sets from 6 languages, resulting in a total of 91 tokens: \(\epsilon\), <bos>, <eos>, <unk>, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, A, A, A, A, AE, C, E, E, E, E, I, I, I, I, N, O, O, O, O, O, U, U, U, U, G, Y, E, A, B, P, I, E, \(\mathcal{H}\), \(\mathcal{H}\), \(\mathcal{H}\), \(\mathcal{K}\), \(\mathcal{H}\), \(\mathcal{K}\), \(\mathcal{I}\), M, H, O, \(\mathcal{H}\), P, C, T, Y, \(\Phi\), X, II, \(\mathcal{U}\), III, III, \(\mathcal{U}\), \(\mathcal{B}\), \(\mathcal{H}\), b, \(\mathcal{B}\), \(\mathcal{H}\), \(\mathcal{B}\), \(\mathcal{H}\).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Dataset** & \multicolumn{3}{c}{**MulJam2.0**} & \multicolumn{2}{c}{**Jamendo**} \\
**Split** & Train & Valid & Test & Test \\ \hline \hline English (an) & 3557 & 20 & 28 & 20 \\ French (tr) & 977 & 19 & 19 & 20 \\ Spanish (ex) & 384 & 19 & 13 & 20 \\ German (ex) & 107 & 20 & 3 & 20 \\ Italian (tr) & 278 & 20 & 7 & 20 \\ Russian (an) & 106 & 16 & 4 & - \\ \hline Total & 5609 & 114 & 74 & 80 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Number of songs in MulJam2.0 and Jamendo datasets.

### Training Details

The beam search used for validation and testing incorporates a combination of CTC probabilities, LM probabilities (applied only at test time), and S2S probabilities. We assign a weight of 0.4 to the CTC probabilities and 0.3 to the LM probabilities. During validation, we utilize a beam size of 10 and calculate Word Error Rate every 5 epochs to optimize processing efficiency. For thorough evaluation, we scale up the beam size to 40 during the testing phase. The accuracy of the S2S branch output is continually monitored to determine whether early stopping should be triggered and to facilitate model selection.

### Results and Discussion

The results of multilingual lyrics transcription using different pretrained features can be found in Tab. 7. In addition to MulJam, we also present WERs on the Multilingual Jamendo evaluation set [13]. This dataset consists of 80 songs in 4 languages: English, French, Spanish, and German. While Italian and Russian songs are not included, Jamendo's human-annotated line-level annotation aligns well with our evaluation setting. For comparison, we reference the state-of-the-art model Whisper [44], a robust model designed for speech recognition but also performs effectively on singing voice. Whisper has been trained on an extensive corpus of multilingual and multitask supervised data collected from the internet. It is also the foundation of the MulJam dataset.

Lyrics transcription is a challenging task that involves detecting vocal pronunciations in the presence of background music and making the most probable predictions based on linguistic knowledge. The multilingual context makes this task even more demanding. When performing lyrics transcription with SSL features, it is essential that these features capture clear vocal information, and that the backend provides robust inference to generate coherent text from the vocal pronunciations. Achieving this with SSL features is indeed a significant challenge. The results presented in Table 7 indicate that there is room for improvement in this task.

Among the six languages we considered, English, French, and Spanish, which have a larger number of songs than the other three, yield better results. This suggests that there may be an impact from the imbalanced training data. Russian, on the other hand, produces the worst result for two main reasons: 1. Russian employs the Cyrillic writing system, which has its own set of characters. 2. The training data for Russian is insufficient for the model to establish a connection between the pronunciation rules of Cyrillic and Latin alphabets.

The MulJam test set is human-annotated at the song level but relies on the alignment with Whisper results to derive line-level annotations. Therefore, it is worth noting that bias is introduced, as the alignment is reliable only when the human annotation closely matches the Whisper's prediction.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c} \hline \hline
**Language** & \multicolumn{2}{c}{**English**} & \multicolumn{2}{c}{**French**} & \multicolumn{2}{c}{**Spanish**} & \multicolumn{2}{c}{**German**} & \multicolumn{2}{c}{**Italian**} & \multicolumn{2}{c}{**Russian**} & \multicolumn{2}{c}{**Whole**} \\
**Metric** & CER & WER & CER & WER & CER & WER & CER & WER & CER & WER & CER & WER & CER & WER \\ \hline \multicolumn{11}{c}{MulJam2.0 test} \\ \hline \hline \multirow{3}{*}{MAP-Music2Vec [31]} & 54.7 & 79.2 & 58.3 & 90.9 & 43.2 & 83.7 & 63.4 & 99.5 & 53.0 & 91.9 & 101.6 & 125.6 & 56.4 & 87.8 \\  & 48.7 & 71.2 & 55.5 & 85.4 & 41.0 & 80.1 & 65.9 & 100.9 & 49.1 & 86.3 & 99.5 & 124.9 & 52.6 & 82.3 \\  & 49.0 & 71.2 & 55.3 & 85.4 & 39.0 & 76.6 & 63.5 & 99.9 & 50.3 & 90.3 & 104.7 & 129.3 & 52.5 & 82.7 \\  & 45.5 & 66.5 & 52.5 & 81.9 & 38.2 & 73.9 & 58.8 & 93.2 & 44.4 & 81.6 & **96.1** & **117.8** & 49.4 & 77.9 \\  & 45.5 & **65.9** & **50.7** & **79.6** & **35.9** & **71.9** & **58.3** & **93.1** & **42.4** & **80.3** & 100.5 & 125.5 & **48.5** & **77.0** \\ \hline \hline \multicolumn{11}{c}{SOTA [44]} & 31.2 & 44.8 & 52.9 & 70.1 & 29.9 & 43.8 & 36.5 & 53.0 & 28.1 & 58.5 & 34.7 & 53.7 & 29.5 & 54.8 \\ \hline \hline \multicolumn{11}{c}{Jamendo} \\ \hline \hline \multirow{3}{*}{MAP-Music2Vec [31]} & 49.0 & 73.6 & 55.3 & 87.1 & 50.3 & 90.7 & 67.8 & 108.8 & - & - & - & - & 55.7 & 89.6 \\  & 48.5 & 71.8 & 54.0 & 85.1 & 49.3 & 87.6 & 67.6 & 108.1 & - & - & - & - & 54.8 & 87.6 \\  & 46.9 & 71.5 & 52.0 & 81.5 & 44.8 & 82.8 & 66.3 & 106.8 & - & - & - & - & 52.6 & 85.2 \\  & 43.6 & **67.2** & **49.4** & **79.6** & **43.2** & **80.6** & 62.1 & 103.3 & - & - & - & - & **49.6** & **82.2** \\  & 45.7 & 68.8 & 50.2 & 80.1 & 44.1 & 82.8 & **61.0** & **102.3** & - & - & - & - & 50.3 & 83.1 \\ \hline \hline \multicolumn{11}{c}{SOTA [44]} & 24.9 & 39.3 & 29.2 & 49.9 & 21.2 & 41.7 & 25.8 & 46.6 & - & - & - & - & 25.4 & 44.4 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Multilingual lyrics transcription results on MulJam and Jamendo.