# On the Parameter Identifiability of Partially Observed Linear Causal Models

Xinshuai Dong\({}^{1}\)* Ignavier Ng\({}^{1}\)* Biwei Huang\({}^{2}\) Yuewen Sun\({}^{3}\) Songyao Jin\({}^{3}\)

Roberto Legaspi\({}^{4}\) Peter Spirtes\({}^{1}\) Kun Zhang\({}^{1,3}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)University of California San Diego

\({}^{3}\)Mohamed bin Zayed University of Artificial Intelligence \({}^{4}\)KDDI Research

###### Abstract

Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper, we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior research--we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime. Code: https://github.com/dongxinshuai/scm-identify.

## 1 Introduction and Related Work

Causal models, which serve as a fundamental tool to capture causal relations among random variables, have achieved great success in many fields [49, 39, 40, 44]. A fundamental problem in the field is how and to what extent can we identify the underlying causal model given observational data. When all variables are observed, the problem has been well studied: the underlying structure can be identified up to the Markov equivalence class, e.g., by the PC [49] or GES [13] algorithm; when the structure is given, the causal coefficient (direct causal effect) between two variables can also be identified [8, 39].

However, in real-world systems, the variables of interest may only be partially observed. Thus, considerable efforts have been dedicated to identification of causal models in the presence of latent variables. One line of research focuses on structure learning given partially observed variables. Notable approaches include FCI and its variants [49, 38, 14, 2], as well as ICA-based [23, 43], tetrad-based [48, 28], high-order moments-based [46, 11, 58, 1, 12], and rank constraint-based [48, 24, 18] methods.

In this paper, we focus on the the identification of parameters of a partially observed model. Specifically, given the causal structure of and observational data from a partially observed causal model, we are interested in identifying all the parameters, and thus the underlying causal model can be fully specified. To identify the parameters, a classical way is to project the directed acyclic graph (DAG) with latent variables to an acyclic directed mixed graph (ADMG) or partially ancestral graph [42], without explicitly modeling the latent confounders. Based on ADMG, graphical criteria such as half-trek [20], G-criterion [9], and some further developments [51, 29] have been proposed to establish the parameter identifiability. Another way is to leverage do-calculus, proxy variables, and instrumentalvariables [47; 39; 25] to identify the direct causal effect, which corresponds to the edge coefficient in linear causal models. For a more detailed discussion of related work, please refer to Appendix D.

Despite the effectiveness of current methods for parameter identification, however, they have two main drawbacks: they require all the variables to be connected in specific ways, and only focus on identifying the edge coefficients between observed variables. To this end, in this paper we propose a novel framework that considers a more general setting for parameter identification. To be specific, we allow all variables, including both observed and latent ones, to be flexibly related, and we aim to recover the edge coefficients among all variables, even including those from a latent variable to another latent variable or another observed variable, which previous methods cannot handle. We summarize our contributions as follows.

* To the best of our knowledge, we are the first to consider parameter identifiability of partially observed causal model in the most general scenario--all variables, including both observed and latent ones, are allowed be flexibly related, and edge coefficients between any pair of variables are concerned. In contrast, most existing works consider only the edges between observed variables.
* Theoretically, we identify three types of parameter indeterminacy in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. These necessary conditions also offer insights into scenarios where the parameters are guaranteed to be non-identifiable.
* Methodologically, we propose a novel likelihood-based parameter estimation method, which parameterizes population covariance in specific ways to address variance indeterminacy. Our empirical studies on both synthetic and real-world data validate the effectiveness of our method in the finite-sample regime, even under certain misspecification of the underlying causal model.

## 2 Preliminaries

### Problem Setting

In this work, we focus on partially observed linear causal models, defined as follows.

**Definition 1** (Partially Observed Linear Causal Models).: _Let \(\mathcal{G}:=(\mathbf{V}_{\mathcal{G}},\mathbf{E}_{\mathcal{G}})\) be a DAG. Each variable \(V_{i}\in\mathbf{V}_{\mathcal{G}}\) follows a linear structural equation model \(\mathbf{V}_{i}=\sum_{\mathbf{V}_{j}\in\mathbf{\mathcal{U}_{\mathcal{G}}}( \mathbf{V}_{i})}f_{j,i}\mathbf{V}_{j}+\epsilon_{\mathbf{V}_{i}}\), where \(\mathbf{V}_{\mathcal{G}}:=\mathbf{L}_{\mathcal{G}}\cup\mathbf{X}_{\mathcal{G }}=\{\mathbf{L}_{i}\}_{i=1}^{m}\cup\{\mathbf{X}_{i}\}_{i=m+1}^{m+n}\) contains \(m\) latent variables and \(n\) observed variables. \(\text{Pa}_{\mathcal{G}}(\mathbf{V}_{i})\) denotes the parent set of \(\mathbf{V}_{i}\), \(f_{j,i}\) denotes the edge coefficient from \(V_{j}\) to \(V_{i}\), and \(\epsilon_{\mathbf{V}_{i}}\) represents the Gaussian noise term of \(\mathbf{V}_{i}\)._

We drop the subscript \(\mathcal{G}\) in \(\mathbf{L}_{\mathcal{G}}\) and \(\mathbf{X}_{\mathcal{G}}\) when the context is clear. We use \(\mathbf{V}\), \(\mathbf{V}\), and \(\mathcal{V}\) to denote a random variable, a set of variables, and a set of sets of variables, respectively. In Definition 1, the relations between variables can also be written in the matrix form as \(\mathbf{V}_{\mathcal{G}}=F^{T}\mathbf{V}_{\mathcal{G}}+\epsilon_{\mathbf{V}_{ \mathcal{G}}}\), where \(F=(f_{j,i})_{i,j\in[m+n]}\) is the weighted adjacency matrix. Here, \(f_{j,i}\neq 0\) if and only if \(V_{j}\) is a parent of \(V_{i}\) in \(\mathcal{G}\). We also write

\[F=\begin{matrix}\mathbf{L}&\mathbf{X}\\ \mathbf{X}&\begin{pmatrix}F_{\mathbf{L}\mathbf{L}}&F_{\mathbf{L}\mathbf{X}}\\ F_{\mathbf{X}\mathbf{L}}&F_{\mathbf{X}\mathbf{X}}\end{pmatrix}&\text{and}& \Omega=\begin{pmatrix}\Omega_{\epsilon_{\mathbf{L}}}&0\\ 0&\Omega_{\epsilon_{\mathbf{X}}}\end{pmatrix},\end{matrix}\]

where \(\Omega\) is the diagonal covariance matrix of \(\epsilon_{\mathbf{V}_{\mathcal{G}}}\).

Our objective is to identify \(F\), the causal edge coefficients of the model, given observational data and the causal structure \(\mathcal{G}\). Denote by \(\Sigma_{\mathbf{L}}\) and \(\Sigma_{\mathbf{X}}\) the population covariance matrix of latent variables \(\mathbf{L}\) and observed variables \(\mathbf{X}\), respectively; their precise formulations are provided in Proposition 1. We also denote by \(\sigma_{i,j}\) the \((i,j)\)-th entry of \(\Sigma_{\mathbf{X}}\). In this work, we assume that the noise terms of latent variables, \(\epsilon_{\mathbf{L}}\), have unit variance, i.e., \(\Omega_{\epsilon_{\mathbf{L}}}=I\), which will be justified later in Section 3.1. Note that variables are partially observed and thus we only have access to i.i.d. samples of observed variables \(\mathbf{X}\). As variables are jointly Gaussian, the observations can asymptotically be summarized as the population covariance matrix \(\Sigma_{\mathbf{X}}\). In other words, we aim to identify \(F\) and \(\Omega\) given \(\Sigma_{\mathbf{X}}\) and \(\mathcal{G}\). The identification of parameters is important in that, once we identify the parameters, the underlying causal model is fully specified, and thus we can flexibly calculate causal effects, infer interventional distributions, and finally answer counterfactual queries [39]. It is worth noting that, for parameter identification, the structure \(\mathcal{G}\) is assumed to be known, which is different from the setting of causal discovery where the goal is to identify \(\mathcal{G}\) from data.

### Framework Comparison

Without latent variables, it has been shown all parameters are identifiable [8]. However, the problem becomes very challenging when latent variables exist. There are two lines of research. One focuses on the use of do-calculus, proxy variables, and instrumental variables to identify direct causal effects among observed variables [47; 39; 25] (in linear models the direct causal effect is captured by the edge coefficient). Another line addresses latent confounders by projecting a DAG with latent variables into an ADMG, where the confounding effects of latent variables are simplified and represented by correlation among noise terms [20; 9; 51; 29]. An example is in Figure 1, where (a) is the original graph and (c) is the projected ADMG whose bidirected edges correspond to correlated noise terms.

Compared to the two previous lines of thought, our framework has two advantages. To begin with, we additionally considers the identifiability of coefficients of edges that involve latent variables. For example, in Figure 1, we aim to identify all the coefficients including the one from L\({}_{1}\) to X\({}_{3}\), i.e., \(f_{1,3}\). In contrast, the proxy variable framework and the latent projection framework identify only the coefficients among observed variables: the proxy variable framework focuses only on the direct causal effect from one observed variable to another observed variable, while the latent projection framework transforms all latent variables into bidirected edges and thus can never identify the coefficient of the edge that has a latent variable as the head or tail.

Furthermore, the projection framework deals with latent variables in a rather brute-force way: dense latent confounding effects among observed variables may be caused by only a small number of latent variables, but that information is lost during projection. For example, in Figure 1, (a) and (b) share the same ADMG after projection, i.e., (c). However, as we will show later, parameters in (a) can be identified, while in (b) the parameters cannot. If we only consider the ADMG in (c), then we can never capture this nuance and thus cannot identify the coefficients that we might be able to.

## 3 Identifiability Theory

### Definition of Parameter Identifiability and Indeterminacy

We follow the notion of generic identifiability and define parameter identifiability as follows.

**Definition 2** (Identifiability of Parameters of Partially Observed Linear Causal Models).: _Let \(\theta=(F,\Omega)\in\Theta\). We say that \(\theta\) is generically identifiable, if the mapping \(\phi(\theta)=\Sigma_{\mathbf{X}}\) is injective, for almost all \(\theta\in\Theta\) with respect to the Lebesgue measure._

Definition 2 indicates if parameter \(\theta\) is identifiable, then there does not exist \(\theta^{\prime}\in\Theta\) that entails the same observations as those of \(\theta\). As in the typical literature of parameter identification, we consider generic identifiability to rule out some rare cases where the parameters for that structure is generally identifiable, but with some specific parameterization, the parameters cannot be identified. This is similar to faithfulness in causal discovery [49] and we will provide an example in Example 1. We next introduce three important indeterminacies about parameter identification when latent variables exist.

**Theorem 1** (Indeterminacy of Scaling of \(\Omega_{\epsilon_{\mathbf{L}}}\)).: _Consider a model that follows Def. 1 with number of latent variables \(m\geq 1\) and \(\theta=(F_{\mathbf{LL}},F_{\mathbf{LX}},F_{\mathbf{XL}},F_{\mathbf{XX}}, \Omega_{\epsilon_{\mathbf{L}}},\Omega_{\epsilon_{\mathbf{X}}})\). Let \(\Lambda\) be any invertible diagonal matrix, and \(\tilde{\theta}=(\tilde{F}_{\mathbf{LL}},\tilde{F}_{\mathbf{LX}},\tilde{F}_{ \mathbf{XL}},\tilde{F}_{\mathbf{XX}},\tilde{\Omega}_{\epsilon_{\mathbf{L}}}, \tilde{\Omega}_{\epsilon_{\mathbf{X}}})\), where_

\[\tilde{F}_{\mathbf{LL}}=\Lambda^{-1}F_{\mathbf{LL}}\Lambda,\ \tilde{F}_{ \mathbf{LX}}=\Lambda^{-1}F_{\mathbf{LX}},\ \tilde{F}_{\mathbf{XL}}=F_{\mathbf{XL}}\Lambda,\ \tilde{F}_{\mathbf{XX}}=F_{\mathbf{XX}},\ \tilde{\Omega}_{\epsilon_{\mathbf{L}}}= \Lambda^{2}\Omega_{\epsilon_{\mathbf{L}}},\ \tilde{\Omega}_{\epsilon_{\mathbf{X}}}=\Omega_{ \epsilon_{\mathbf{X}}}.\]

_Then, \(\tilde{\theta}\) and \(\theta\) entail the same observations, i.e., \(\tilde{\Sigma}_{\mathbf{X}}=\Sigma_{\mathbf{X}}\). Furthermore, we have \(\tilde{\Sigma}_{\mathbf{L}}=\Lambda\Sigma_{\mathbf{L}}\Lambda\)._

Figure 1: Illustrations of the advantage of our framework. Within our framework, it can be shown that \(\mathcal{G}_{1}\)’s parameters can be identified (up to sign) while \(\mathcal{G}_{2}\)’s cannot. In contrast, the latent projection framework cannot even differentiate \(\mathcal{G}_{1}\) from \(\mathcal{G}_{2}\) as they share the same ADMG (c) after projection. Furthermore, with ADMG, any edge coefficient that involves a latent variable cannot be considered.

A similar theoretical result is provided in [4], and yet our setting is much more general and takes that of [4] as a special case: in our setting, all variables including latent and observed ones can be arbitrarily related while in [4] latent variables cannot be the effect of observed variables.

**Remark 1** (Implication of Theorem 1).: _A key implication of Theorem 1 is that, without further assumption, the edge coefficients involving latent variables, i.e., \((F_{\mathbf{LL}},F_{\mathbf{LX}},F_{\mathbf{XL}})\), can never be identified, as there always exists a diagonal matrix \(\Lambda\) such that \(\tilde{\theta}\) and \(\theta\) entail the same observations but \((\tilde{F}_{\mathbf{LL}},\tilde{F}_{\mathbf{LX}},\tilde{F}_{\mathbf{XL}}) \neq(F_{\mathbf{LL}},F_{\mathbf{XL}},F_{\mathbf{XL}})\). Thus, in the rest of this paper, we assume that the noise terms of latent variables, \(\epsilon_{\mathbf{L}}\), have unit variance, i.e., \(\Omega_{\epsilon_{\mathbf{L}}}=I\). Under this assumption, we have \((\tilde{\Omega}_{\epsilon_{\mathbf{L}}})_{i,i}=\Lambda_{i,i}^{2}(\Omega_{ \epsilon_{\mathbf{L}}})_{i,i}=1,i\in[m]\), which implies \(\Lambda_{i,i}=\pm 1\). As such, this assumption makes parameter identifiability possible. However, even though we fix the scaling of \(\Omega_{\epsilon_{\mathbf{L}}}\), there still exists indeterminacy about the sign of parameters, captured by Theorem 2._

**Theorem 2** (Group Sign Indeterminacy).: _Consider a model that follows Def. 1 with number of latent variables \(m\geq 1\), \(\theta=(F_{\mathbf{LL}},F_{\mathbf{LX}},F_{\mathbf{XL}},F_{\mathbf{XX}}, \Omega_{\epsilon_{\mathbf{L}}},\Omega_{\epsilon_{\mathbf{X}}})\), and \(\Omega_{\epsilon_{\mathbf{L}}}=I\). Let \(S\) be a diagonal sign matrix (entries are either \(1\) or \(-1\)), and \(\tilde{\theta}=(\tilde{F}_{\mathbf{LL}},\tilde{F}_{\mathbf{LX}},\tilde{F}_{ \mathbf{XL}},\tilde{F}_{\mathbf{XX}},\tilde{\Omega}_{\epsilon_{\mathbf{L}}}, \tilde{\Omega}_{\epsilon_{\mathbf{X}}})\), where_

\[\tilde{F}_{\mathbf{LL}}=SF_{\mathbf{LL}}S,\;\tilde{F}_{\mathbf{LX}}=SF_{ \mathbf{LX}},\;\tilde{F}_{\mathbf{XL}}=F_{\mathbf{XL}}S,\;\tilde{F}_{\mathbf{ XX}}=F_{\mathbf{XX}},\;\tilde{\Omega}_{\epsilon_{\mathbf{L}}}=\Omega_{ \epsilon_{\mathbf{L}}}=I,\;\tilde{\Omega}_{\epsilon_{\mathbf{X}}}=\Omega_{ \epsilon_{\mathbf{X}}}.\]

_Then, \(\tilde{\theta}\) and \(\theta\) entail the same observations, i.e., \(\tilde{\Sigma}_{\mathbf{X}}=\Sigma_{\mathbf{X}}\), and \((\tilde{\Sigma}_{\mathbf{L}})_{ii}=(\Sigma_{\mathbf{L}})_{ii},\;\forall i\in[m]\)._

**Remark 2** (Remark on Theorem 2).: _The indeterminacy described in Theorem 2 is referred to as group sign indeterminacy for the following reason: According to the theorem, flipping the sign of \(S_{i,i}\) is equivalent to flipping the signs of all coefficients of edges involving the latent variable \(\mathsf{L}_{i}\). This transformation preserves the resulting observations \(\Sigma_{\mathbf{X}}\). In essence, each group consists of coefficients of edges involving a particular latent variable._

**Example 1** (Example for Group Sign Indeterminacy and Generic Identifiability).: _In Figure 2 (b), given the structure and \(\Sigma_{\mathbf{X}}\), by assuming \(\Omega_{\epsilon_{\mathbf{L}}}=I\), the parameters are generally identifiable up to group sign indeterminacy. Specifically, there exist three equality constraints with three free parameters: \(f_{1,2}f_{1,3}=\sigma_{2,3}\), \(f_{1,2}f_{1,4}=\sigma_{2,4}\), and \(f_{1,3}f_{1,4}=\sigma_{3,4}\). The solutions are: (i) \(f_{1,2}=\sqrt{\frac{\sigma_{2,3}\sigma_{2,4}}{\sigma_{3,4}}}\), \(f_{1,3}=\sigma_{2,3}/f_{1,2}\), \(f_{1,4}=\sigma_{2,4}/f_{1,2}\) and (ii) \(f_{1,2}=-\sqrt{\frac{\sigma_{2,3}\sigma_{2,4}}{\sigma_{3,4}}}\), \(f_{1,3}=-\sigma_{2,3}/f_{1,2}\), \(f_{1,4}=-\sigma_{2,4}/f_{1,2}\). The two solutions are different only in terms of group sign. However, if we set \(f_{1,2}=0\), then the parameters are not identifiable (as we will encounter division where the divisor is zero). These rare cases of parameters are of zero Lebesgue measure so we rule out these cases for the definition of identifiability, as in Definition 2._

Intuitively speaking, group sign indeterminacy arises because one may multiply the latent variable \(\mathsf{L}_{i}\) by \(-1\) and accordingly flip the signs of all edge coefficients involving \(\mathsf{L}_{i}\). Note that such an indeterminacy is rather minor for the following reason. (i) In practice, we can always anchor the sign of some edges according to our preference or prior knowledge in order to eliminate the group sign indeterminacy. For example, in Figure 4, if we expect that L2 should be understood as Extraversion instead of non-Exterversion, we can add one additional constraint during our parameter estimation such that the edge coefficient from L2 to E1 ("I am the life of party.") will be positive (as we believe E1 should be positively related to Extraversion). (ii) On the other hand, there are some application scenarios that are not influenced by the group sign indeterminacy, such as causal effect estimations between certain variables. We note that, as the indeterminacy of group sign is rather minor, in the following if the parameters are identifiable only up to group sign indeterminacy, we still say that the parameters are identifiable.

Figure 2: Illustrative examples to show that the graphical condition for structure-identifiability and parameter-identifiability could be very different.

**Definition 3** (Orthogonal Transformation Indeterminacy).: _Consider a model that follows Def. 1 with number of latent variables \(m\geq 1\), \(\theta=(F_{\mathbf{LL}},F_{\mathbf{LX}},F_{\mathbf{XL}},F_{\mathbf{XX}},\Omega_{ \epsilon_{\mathbf{L}}},\Omega_{\epsilon_{\mathbf{X}}})\), and \(\Omega_{\epsilon_{\mathbf{L}}}=I\). We say that there exists an orthogonal transformation indeterminacy in the identification of parameters if there exists a non-diagonal orthogonal matrix \(Q\) such that \((F_{\mathbf{LL}},F_{\mathbf{LX}},F_{\mathbf{XL}},F_{\mathbf{XX}},\Omega_{ \epsilon_{\mathbf{L}}},\Omega_{\epsilon_{\mathbf{X}}})\) and \((\tilde{F}_{\mathbf{LL}},\tilde{F}_{\mathbf{LX}},\tilde{F}_{\mathbf{XL}}, \tilde{F}_{\mathbf{XX}},\tilde{\Omega}_{\epsilon_{\mathbf{L}}},\tilde{\Omega }_{\epsilon_{\mathbf{X}}})\) share the same support and entail the same observations, where_

\[\tilde{F}_{\mathbf{LL}}=Q^{T}F_{\mathbf{LL}}Q,\;\tilde{F}_{\mathbf{LX}}=Q^{T} F_{\mathbf{LX}},\;\tilde{F}_{\mathbf{XL}}=F_{\mathbf{XL}}Q,\;\tilde{F}_{ \mathbf{XX}}=F_{\mathbf{XX}},\;\tilde{\Omega}_{\epsilon_{\mathbf{L}}}=I,\; \tilde{\Omega}_{\epsilon_{\mathbf{X}}}=\Omega_{\epsilon_{\mathbf{X}}}.\]

The orthogonal transformation indeterminacy is the major indeterminacy we consider in the presence of latent variables. Such an indeterminacy also arises in factor analysis [45; 7], which can be viewed as a special case of the data generating procedure considered in Definition 1. Here we only give the definition and will later provide Theorem 4 with an example that captures the scenarios where such indeterminacy exists.

It is worth noting that the graphical condition for structure identifiability and parameter identifiability could be very different. For example, \(\mathcal{G}_{1}\) in Figure 2 (a) is structure-identifiable, and yet the parameters are not identifiable even if the structure is given. In contrast \(\mathcal{G}_{2}\) in Figure 2 (b) is not structure-identifiable, as there exists another structure \(\mathcal{G}_{3}\) in Figure 2 (c) such that \(\mathcal{G}_{2}\) and \(\mathcal{G}_{3}\) can never be differentiated from observational distribution; and yet if \(\mathcal{G}_{2}\) is given, its parameters are identifiable (as in Example 1). Therefore, in this paper, we first consider the cases where the structure can be identified and then study which further conditions are needed for the identifiability of parameters. This will give rise to conditions under which the whole causal model can be fully specified.

### Graphical Condition for Structure Identifiability

To explore the conditions for the whole causal model to be specified, we start with the structure identifiability of partially observed linear causal models. Recent advances have shown that if certain graphical conditions are satisfied [24; 18], even though all variables including latent ones are allowed to be very flexibly related, the causal structure can still be identified. Next, we focus on the conditions by [18], which takes that of [24] as special cases. Roughly speaking, the identifiability of the structure of a partially observed linear causal model is built upon the identifiability of atomic covers, defined as follows (with _effective cardinality_ defined as \(||\mathcal{V}||=|(\cup_{\mathbf{V}\in\mathcal{V}}\mathbf{V})|\) and \(\text{PCh}_{\mathcal{G}}\) defined in Appendix B.2).

**Definition 4** (Atomic Cover [18]).: _Let \(\mathbf{V}\in\mathbf{V}_{\mathcal{G}}\) be a set of variables, where \(l\) out of \(|\mathbf{V}|\) are latent, and the remaining \(|\mathbf{V}|-l\) are observed. \(\mathbf{V}\) is an atomic cover if \(\mathbf{V}\) is a single observed variable, or if the following conditions hold:_

1. _There exists a set of atomic covers_ \(\mathcal{C}\)_, with_ \(||\mathcal{C}||\geq l+1\)_, such that_ \(\cup_{\mathcal{C}\in\mathcal{C}}\mathbf{C}\subseteq\text{PCh}_{\mathcal{G}}( \mathbf{V})\)_._
2. _There exists a set of covers_ \(\mathcal{N}\) _with_ \(||\mathcal{N}||\geq l+1\)_, s.t._ \((\cup_{\mathbf{N}\in\mathcal{N}}\mathbf{N})\cap(\cup_{\mathcal{C}\in\mathcal{C }}\mathbf{C})=\emptyset\)_, every element in_ \(\cup_{\mathbf{N}\in\mathcal{N}}\mathbf{N}\) _is a neighbour of every element in_ \(\mathbf{V}\)_, and_ \(\mathbf{V}\) _d-separates_ \(\mathcal{N}\) _and_ \(\mathcal{C}\)_._
3. _There does not exist a partition_ \(\mathcal{P}\) _of_ \(\mathbf{V}\)_, s.t., all elements in_ \(\mathcal{P}\) _are atomic covers._

The intuition that we build structure identifiability upon the notion of atomic covers is as follows. When a set of latent variables share the same set of children and neighbors, it is impossible to differentiate these latent variables from each other, and thus we need to consider them together as the minimal identifiable group to build up the identifiability of the whole structure. Such a minimal identifiable group of variables is defined as an atomic cover. Roughly, for a group of variables to be qualified as an atomic cover, it has to have enough children and neighbors. An example is as follows.

**Example 2** (Example of Atomic Cover).: _Consider the graph in Fig. 3. \(\mathbf{V}=\{\mathsf{L}_{1},\mathsf{X}_{\mathcal{G}}\}\) is an atomic cover. This is because there exist \(\mathcal{C}=\{\{\mathsf{X}_{\mathcal{G}}\},\{\mathsf{X}_{\mathcal{10}}\}\}\) with \(||\mathcal{C}||\geq l+1=2\) such that (i) in Def. 4 is satisfied. And there exist \(\mathcal{N}=\{\{\mathsf{X}_{\mathcal{11}}\},\{\mathsf{X}_{\mathcal{12}}\}\}\) (or, \(\mathcal{N}=\{\{\mathsf{X}_{\mathcal{4}}\},\{\mathsf{X}_{\mathcal{5}}\}\}\)) with \(||\mathcal{N}||\geq l+1=2\) such that (ii) in Def. 4 is satisfied. We can also find that (iii) in Def. 4 is satisfied. Thus \(\{\mathsf{L}_{1},\mathsf{X}_{\mathcal{6}}\}\) is an atomic cover. Another example would be in Figure 8, where \(\{\mathsf{L}_{1},\mathsf{L}_{2}\}\) is an atomic cover._

Figure 3: An illustrative graph that satisfies the conditions for structure-identifiability. At the same time, it also satisfies the condition for parameter identifiability - given the structure and \(\Sigma_{\mathbf{X}}\), all the parameters are identifiable only up to group sign indeterminacy.

**Condition 1** (Basic Conditions for Structure Identifiability [18]).: \(\mathcal{G}\) _satisfies the basic graphical condition for identifiability, if every latent variable belongs to at least one atomic cover in \(\mathcal{G}\) and for each atomic cover with latent variables, any of its children is not adjacent to any of its neighbours._

**Condition 2** (Condition on Colliders [18]).: _In \(\mathcal{G}\), if (i) there exists sets of variables \(\mathbf{V}\), \(\mathbf{V_{1}}\), \(\mathbf{V_{2}}\), and \(\mathbf{T}\) such that every variable in \(\mathbf{V}\) is a collider of two atomic covers \(\mathbf{V_{1}}\), \(\mathbf{V_{2}}\), and \(\mathbf{T}\) is a minimal set of variables that d-separates \(\mathbf{V_{1}}\) from \(\mathbf{V_{2}}\), and (ii) there exists at least one latent variable in \(\mathbf{V}\cup\mathbf{V_{1}}\cup\mathbf{V_{2}}\cup\mathbf{T}\), then we must have \(|\mathbf{V}|+|\mathbf{T}|\geq|\mathbf{V_{1}}|+|\mathbf{V_{2}}|\)._

**Example 3** (Example that satisfies Conditions 1 and 2).: _Consider Figure 3. All latent variables in the graph belong to at least one atomic cover and thus Condition 1 is satisfied. Plus, Condition 2 is also satisfied. This is because the sets of variables \(\mathbf{V}\), \(\mathbf{V_{1}}\), \(\mathbf{V_{2}}\), and \(\mathbf{T}\) that satisfy (i) and (ii) in Condition 2 are \(\mathbf{V}=\{\chi_{12}\}\), \(\mathbf{V_{1}}=\{\mathsf{L_{1}},\chi_{6}\}\), \(\mathbf{V_{2}}=\{\mathsf{L_{2}}\}\), and \(\mathbf{T}=\{\chi_{4},\chi_{5}\}\), and we also have \(|\mathbf{V}|+|\mathbf{T}|\geq|\mathbf{V_{1}}|+|\mathbf{V_{2}}|\). Therefore, the graph in Figure 3 satisfies both Conditions 1 and 2._

The identifiability theory of structure is as follows. For a graph \(\mathcal{G}\), if Condition 1 and Condition 2 are satisfied, then asymptotically the structure is identifiable up to the Markov equivalence class (MEC) of \(\mathcal{O}_{a}(\mathcal{O}_{s}(\mathcal{G}))\) (definitions of \(\mathcal{O}_{a}(\cdot)\) and \(\mathcal{O}_{s}(\cdot)\) can be found in Appendix B.4). Roughly speaking, the underlying causal structure of \(\mathcal{G}\) can be identified except that the directions of some edges cannot be determined. Next, we will show that, given any DAG in the identified equivalence class together with \(\Sigma_{\mathbf{X}}\), the parameters of the model are also identifiable, if certain conditions are satisfied.

### Identifiability of Parameters

In this section we show that, given graphical Conditions 1 and 2, the causal coefficients \(F\) in Definition 1 are also identifiable, if certain conditions are satisfied.

**Theorem 3** (Sufficient Condition for Parameter Identifiability (up to group sign), Based on Structure Identifiability).: _Assume that \(\mathcal{G}\) satisfies Conditions 1 and 2 and thus the structure can be identified up to the MEC of \(\mathcal{O}_{a}(\mathcal{O}_{s}(\mathcal{G}))\). For any DAG in the equivalence class, the parameters are identifiable, if both the following hold:_

1. _For any atomic cover_ \(\mathbf{V}=\mathbf{X}\cup\mathbf{L}\)_,_ \(|\mathbf{L}|\leq 1\)_._
2. _If an atomic cover_ \(\mathbf{V}=\mathbf{X}\cup\mathbf{L}\) _satisfies_ \(|\mathbf{L}|\neq 0\) _and_ \(|\mathbf{X}|\geq 1\)_, then all simple treks (Def._ 5_) between_ \(\mathbf{L}\) _and_ \(\mathbf{X}\) _do not contain any latent variables that are not in_ \(\mathbf{L}\)_._

Theorem 3 provides a sufficient condition such that the parameters are identifiable. Now, for a better understanding of Theorem 3, we provide an example of it as follows.

**Example 4** (Example for Theorem 3).: _The graph \(\mathcal{G}\) in Figure 3 satisfies the conditions for parameter identifiability in Theorem 3. Specifically, condition (i) in Theorem 3, is satisfied as all atomic covers contain no more than one latent variable. Plus, condition (ii) in Theorem 3 is also satisfied, as the atomic cover \(\mathbf{V}=\mathbf{X}\cup\mathbf{L}=\{\mathsf{L_{1}}\}\cup\{\mathcal{X}_{6}\}\) satisfies \(|\mathbf{L}|\neq 0\) and \(|\mathbf{X}|\geq 1\) and all simple treks between \(\{\mathsf{L_{1}}\}\) and \(\{\mathcal{X}_{6}\}\) contain only observed variables except \(\{\mathsf{L_{1}}\}\). Therefore, the parameters are identifiable for the graph in Figure 3._

Next, we discuss under which conditions the parameters are guaranteed to be not identifiable. As discussed in Section 3.1, there are three kinds of indeterminacy. The first one can be solved by assuming unit variance of the noise terms of latent variables while the second one group sign indeterminacy is rather trivial such that we still consider parameters as identifiable even if group sign indeterminacy exists. Therefore, we will focus on the third one, orthogonal transformation indeterminacy, in what follows.

**Theorem 4** (Condition for the Existence of Orthogonal Transformation Indeterminacy).: _Consider the model in Definition 1. If a set of latent variables \(\mathbf{L}\) with \(|\mathbf{L}|\geq 2\), have the same parents and children, then there must exist orthogonal transformation indeterminacy regarding the edge coefficients \(F\). In other words, \(F\) can at most be identified up to orthogonal transformation indeterminacy._

**Example 5** (Example for Thm. 4).: _Consider Fig. 8. The graph satisfies the conditions in Thm. 4 as the parents and children of \(\mathsf{L_{1}}\) and \(\mathsf{L_{2}}\) are exactly the same. Therefore, there must exist orthogonal transformation indeterminacy for the edge coefficients \(F\) and thus the parameters are not identifiable._

The Theorem 4 above indicates that, if there exist two latent variables that share the same parents and children, then the edge parameters can at most be identified up to orthogonal transformation. This directly implies a necessary condition for parameter identifiability as follows.

**Corollary 1** (General Necessary Condition for Parameter Identifiability).: _For parameters to be identifiable, every pair of latent variables has to have at least one different parent or child._Corollary 1 captures a necessary condition in the general cases such that parameters are identifiable. If we further consider the graphs that are also structure identifiable (as we need to identify the structure first to fully specified the causal model), we further have the following Corollary 2 by considering the notion of atomic covers (the proofs of both corollaries can be found in the Appendix).

**Corollary 2** (Necessary Condition about Atomic Covers for Parameter Identifiability).: _Assume \(\mathcal{G}\) satisfies Conditions 1 and 2 and thus the structure can be identified up to the MEC of \(\mathcal{O}_{a}(\mathcal{O}_{s}(\mathcal{G}))\). For any DAG \(\mathcal{G}\) in the equivalence class, for \(\mathcal{G}\)'s parameters to be identifiable, every atomic cover must contain no more than one latent variable._

**Remark 3** (Necessity of Conditions in Theorem 3).: _Condition (i) in Theorem 3 is provably necessary: by Corollary 2, for parameters to be identifiable, one has to assume (i) in Theorem 3._

Establishing a necessary and sufficient condition is always highly non-trivial in various tasks. For example, for the identification of linear non-Gaussian causal structure with latent variables, researchers initially developed sufficient conditions with three pure children in [46], later relaxed to two in [11, 58], before ultimately achieving both necessary and sufficient conditions in [1]. Similarly, for parameter identification, although the condition we proposed is not a necessary and sufficient one, it could serve as a stepping stone towards tighter and ultimately the necessary and sufficient condition for the field.

Below, we also provide a sufficient condition for parameter identifiability that does not rely on structure identifiability in Theorem 5. It is particularly useful when the structure is directly given by some domain experts.

**Theorem 5** (Sufficient Condition for Parameter Identifiability (up to group sign) without Requiring Structure Identifiability).: _In \(\mathcal{G}\), if for every latent variable \(\mathsf{L}\) there always exist another three distinct variables (which can be latent or observed), such that two of the three are pure children of \(\mathsf{L}\) and the rest one is a neighbor of \(\mathsf{L}\), then the parameters are identifiable._

Identifiability theory often focuses on the asymptotic case, i.e., we assume that we know the structure and the population covariance matrix \(\Sigma_{\mathbf{X}}\). However, in practice, we only have access to i.i.d. data with finite sample size and thus only have the sample covariance matrix. Therefore, in the next section, we will propose a novel method to estimate the parameters in the finite sample cases.

## 4 Parameter Estimation Method

### Objective

Our goal is to estimate \(F\) in Definition 1, given the causal structure \(\mathcal{G}\) and observational data. The key is to parameterize the population covariance \(\Sigma_{\mathbf{X}}\) using \(\theta=(F,\Omega)\) and then maximize the likelihood of observed sample covariance \(\hat{\Sigma}_{\mathbf{X}}\). To make this technically precise, we provide a closed-form expression of \(\Sigma_{\mathbf{X}}\) in terms of \(\theta\) in the following proposition, with a proof given in Appendix A.7.

**Proposition 1** (Parameterization of Population Covariance).: _Consider the model defined in Def. 1. Let \(M\coloneqq\big{(}I-F_{\mathbf{L}\mathbf{L}}-F_{\mathbf{L}\mathbf{X}}(I-F_{ \mathbf{X}\mathbf{X}})^{-1}F_{\mathbf{X}\mathbf{L}}\big{)}^{-1}\) and \(N\coloneqq\big{(}(I-F_{\mathbf{L}\mathbf{L}})F_{\mathbf{X}\mathbf{L}}^{-1}(I-F_ {\mathbf{X}\mathbf{X}})-F_{\mathbf{L}\mathbf{X}}\big{)}^{-1}\). Then, the population covariance matrices of \(\mathbf{L}\) and \(\mathbf{X}\) can be formulated as_

\[\Sigma_{\mathbf{L}}= M^{T}\Omega_{\epsilon_{\mathbf{L}}}M+N^{T}\Omega_{\epsilon_{ \mathbf{X}}}N,\] (1) \[\Sigma_{\mathbf{X}}= (I-F_{\mathbf{X}\mathbf{X}})^{-T}\Big{(}F_{\mathbf{L}\mathbf{X} }^{T}\Sigma_{\mathbf{L}_{\mathcal{G}}}F_{\mathbf{L}\mathbf{X}}+\Omega_{ \epsilon_{\mathbf{X}}}NF_{\mathbf{L}\mathbf{X}}+F_{\mathbf{L}\mathbf{X}}^{T}N^{ T}\Omega_{\epsilon_{\mathbf{X}}}\Big{)}(I-F_{\mathbf{X}\mathbf{X}})^{-1}.\] (2)

The formulations of \(\Sigma_{\mathbf{L}}\) and \(\Sigma_{\mathbf{X}}\) are rather complicated due to the general scenario we considered, i.e., latent variables can be the cause or the effect of latent and observed variables. That is, the submatrices \(F_{\mathbf{L}\mathbf{L}},F_{\mathbf{L}\mathbf{X}},F_{\mathbf{X}\mathbf{L}}\) and \(F_{\mathbf{X}\mathbf{X}}\) defined in the above proposition can all have nonzero entries. In most existing works, at least one of these submatrices are assumed to be zero. For instance, factor analysis assumes that \(F_{\mathbf{L}\mathbf{L}},F_{\mathbf{X}\mathbf{L}}\) and \(F_{\mathbf{X}\mathbf{X}}\) are zero, while [32] assumes that \(F_{\mathbf{L}\mathbf{L}}\) and \(F_{\mathbf{X}\mathbf{L}}\) are zero. Furthermore, Proposition 1 also provides insight into the indeterminacy involved when identifying the parameters, such as the indeterminacy of variance in Theorem 1 and the orthogonal transformation indeterminacy in Theorem 4.

Similar to factor analysis [45, 7, 21], we assume \(\epsilon_{\mathbf{V}}\) are Gaussian and thus \(\mathbf{X}\) are jointly Gaussian. Thus, the negative log-likelihood of observational data can be formulated as

\[\mathcal{L}=(K/2)(\operatorname{tr}((\Sigma_{\mathbf{X}})^{-1}\hat{\Sigma}_{ \mathbf{X}})+\log\det\Sigma_{\mathbf{X}}),\] (3)

where \(K\) is the number of i.i.d. observations. With the parameterized negative log-likelihood, we estimate the edge coefficients by minimizing the negative log-likelihood, as \[\hat{F},\hat{\Omega}=\arg\min_{F,\Omega}\ \mathcal{L},\quad\text{subject to }\Omega_{ \epsilon_{\mathbf{L}}}=I,\] (4)

where the entries of matrix \(F\) that do not correspond to an edge in \(\mathcal{G}\) are constrained to be zero during the optimization.

Note that in Eq. (4) the constraint that the noise terms of latent variables have unit variance is crucial to deal with the variance indeterminacy defined in Theorem 1. In practice, it is also favorable to use another constraint to address the variance indeterminacy, i.e., the constraint that all the latent variables have unit variance. This leads to an alternative objective as

\[\hat{F},\hat{\Omega}=\arg\min_{F,\Omega}\ \mathcal{L},\quad\text{subject to }( \Sigma_{\mathbf{L}})_{ii}=1,\ i\in[m],\] (5)

where the entries of \(F\) that do not correspond to an edge in \(\mathcal{G}\) are also constrained to be zero.

Both objectives in Eqs. (4) and (5) can be employed, and yet using the second one gives rise to edge coefficients that are easier to understand. To be concrete, if we normalize all observed variables to have unit variance, then using Eq. (5) would give rise to \(\hat{F}\) such that \(-1\leq\hat{F}_{i,j}\leq 1,\forall i,j\in[m]\). An example can be found in Figure 4. However, it may not be straightforward to realize the constraint in Eq. (5). To this end, in the next section we introduce a way to parameterize \(\Sigma_{\mathbf{X}}\) using \(F\), such that the required constraint in Eq. (5) can be automatically satisfied. Later in Section 5.2, we also empirically compare the performance of using Eq. (4) with that of using Eq. (5).

### Parameterization Trick of Covariance Matrix

In this section, we introduce how trek rules can be employed to parameterize \(\Sigma_{\mathbf{X}}\) while the unit variance constraint on latent variables in Eq. (5) can be elegantly satisfied. We start with the definition of trek. For readers who are less familiar with trek, please refer to Appendix B.1 for examples.

**Definition 5** (Treks [50]).: _In \(\mathcal{G}\), a trek from \(\mathsf{X}\) to \(\mathsf{Y}\) is an ordered pair of directed paths \((P_{1},P_{2})\) where \(P_{1}\) has a sink \(\mathsf{X}\), \(P_{2}\) has a sink \(\mathsf{Y}\), and both \(P_{1}\) and \(P_{2}\) have the same source \(\mathsf{Z}\), i.e., \(\text{top}(P_{1},P_{2})=\mathsf{Z}\). A Trek is simple if \(P_{1}\) and \(P_{2}\) have no intersection except their common source \(\mathsf{Z}\)._

At this point, we are able to parameterize each entry of \(\Sigma_{\mathbf{X}}\) using (\(F,\{\sigma_{ii}\}_{i=1}^{n+m}\)), instead of (\(F,\Omega\)), by making use of the (simple) trek rule [50], as follows:

\[\sigma_{ij}=\sum_{P_{1},P_{2}\in\mathcal{S}(\mathsf{V}_{i},\mathsf{V}_{j})} \sigma_{\text{top}(P_{1},P_{2})}f^{P_{1}}f^{P_{2}},\] (6)

where \(\mathcal{S}(\mathsf{V}_{i},\mathsf{V}_{j})\) is the set of all simple treks between \(\mathsf{V}_{i}\) and \(\mathsf{V}_{j}\), and \(f^{P}\) is the path monomial along \(P\) defined as \(f^{P}:=\Pi_{k\to l\in P}f_{kl}\).

By this form of parameterization, we can simply set all entries of \(\{\sigma_{ii}\}_{i=1}^{n+m}\) as 1 (which is equivalent to requiring all variables to have unit variance), such that the constraint in Eq. (5) can be automatically satisfied. For a better understanding of how to use the simple trek rule for parameterization, we provide an example as follows.

**Example 6** (Example for Parameterization using Simple Trek).: _In Figure 7 (a), there are four simple treks between \(\mathsf{X}_{4}\) and \(\mathsf{X}_{5}\), as shown in (b). By the simple trek rule and further assuming that all variables have unit variance, the covariance between \(\mathsf{X}_{4}\) and \(\mathsf{X}_{5}\), \(\sigma_{4,5}\), can be formulated as \(f_{1,4}f_{1,5}+f_{3,4}f_{3,5}+f_{2,1}f_{1,4}f_{2,3}f_{3,5}+f_{2,3}f_{3,4}f_{2, 1}f_{1,5}\)._

## 5 Experiments

We validate our identifiability theory and parameter estimation method on synthetic and real-life data.

### Setting and Evaluation Metric

We begin with our experimental setting of synthetic data. The causal strength \(f_{ij}\) is uniformly sampled from \([-2,2]\) and the noise terms are Gaussian with variance uniformly from \([1,5]\). We consider 20 graphs. 10 of them should be parameter-identifiable up to group sign indeterminacy according to our identifiability theory and we refer to them as _GS Case_ (examples in Figure 10 in Appendix). Another 10 should be parameter-identifiable up to group sign and orthogonal transformation indeterminacy and we refer to them as _OT Case_ (examples in Figure 11 in Appendix). On average each graph contains 15 variables, 3 out of them are latent. We consider three different sample sizes: 2k, 5k, and 10k. We use three random seeds to generate the causal model and report the mean performance as well as the std.

As the optimization in Eq (4) is nonconvex, we will rely on 30 random starts and choose the one with the best likelihood. We report the performance of the proposed method with two different objectives. **(i)** Parameter Estimator with objective defined in Eq. (4), referred to as Estimator, and **(ii)** Parameter Estimator with objective defined in Eq. (5) and Trek Rule parameterization trick in Eq (6), referred to as Estimator-TR.

It is worth noting that our setting is very general in that we allow latent variables and observed variables to be causally connected in a very flexible way, and we consider the identification of parameters of edges that can involve both observed and latent variables. Therefore, to the best of our knowledge, no current method can achieve the same goal to serve as the baseline (which also shows the novelty of the proposed method). As such, we mainly focus on comparing our estimation result with the ground truth parameters. We use two MSE-based metrics defined as follows.

**MSE up to group sign:** suppose the ground truth parameter is \(F\) and our estimation is \(\hat{F}\). The MSE up to group sign is defined as \(\frac{\||F||-\hat{F}||_{2}^{2}}{\|F\|_{0}}\), where \(|\cdot|\) takes element wise absolute value, \(\|\cdot\|_{2}\) denotes the Frobenius norm and \(\|\cdot\|_{0}\) denotes the number of nonzero entries of a matrix.

**MSE up to orthogonal transformation:** the MSE up to orthogonal transformation is defined as

\[\min_{Q\cdot Q^{T}Q=I}\frac{\||F_{\mathbf{LL}}|-|Q^{T}\hat{F}_{\mathbf{LL}}Q \|_{2}^{2}+\||F_{\mathbf{LX}}|-|Q^{T}\hat{F}_{\mathbf{LX}}||_{2}^{2}+\||F_{ \mathbf{XL}}|-|\hat{F}_{\mathbf{XL}}Q\|_{2}^{2}+\||F_{\mathbf{XX}}|-|\hat{F}_{ \mathbf{XX}}||_{2}^{2}}{\|F\|_{0}},\] (7)

where the optimization is solved by Adam [27] and the orthogonal matrix \(Q\) can be directly parameterized in PyTorch.

### Synthetic Data Performance

We report the performance using synthetic data in Tables (a)a and (b)b, where both our Estimator and Estimator-TR achieve very good identification performance. For example, in the GS scenario with 10k samples, our Estimator achieves 0.0.0012 MSE up to group sign and our Estimator-TR achieves 0.0003 MSE up to group sign. The good performance by Estimator and Estimator-TR not only validates our estimation method, but also empirically verifies our identifiability theory.

\begin{table}

\end{table}
Table 1: Experimental result on synthetic data using MSE (mean (std)).

Figure 4: Estimated edge coefficients by the proposed method for Big Five human personality dataset. Variables whose name starts with “L” are latent variables while the others are observed variables.

### Misspecification Behavior

In this section, we show that the proposed estimation method still performs well, even under model misspecification: violation of normality and violation of linearity.

As for violation of normality, we use uniform noise terms for the underlying model, and thus the distribution is not jointly Gaussian anymore. We aim to see to what extent can the proposed method still recover the correct parameters. The result is shown in Table 2 in the Appendix, which shows even when the normality is violated, we can still estimate the parameters pretty well. The reason lies in that our proposed asymptotic identifiability result holds true, even when we do not assume Gaussianity; as we only make use of the second-order statistics of the distribution, the additive noise in Definition 1 can follow any other continuous distribution.

To simulate the violation of linearity, we employ the leaky ReLU function during the generation process, as \(\mathsf{V}_{i}=\text{LRELU}(\sum_{\mathsf{V}_{i}\in\text{Pa}(\mathsf{V}_{i})} f_{ji}V_{j}+\epsilon_{\mathsf{V}_{i}})\), \(\text{LRELU}(x)=\max(\alpha x,x)\). When \(\alpha\) is close to 1, the function is close to a linear one, and when \(\alpha\) is close to 0, the model is very nonlinear. The result is shown in Table 3 and we found that our estimation method is quite robust to small violations of linearity. For example, for Estimator-TR in GS case with 10k sample size, if we set \(\alpha=0.8\), we still get a small MSE of 0.001. Even when \(\alpha\) decreases to 0.6, the MSE is around 0.005, which is still small. However, when \(\alpha\) is decreased to \(0.3\), the underlying model is considerably nonlinear, and the MSE increases to 0.027.

### Implementation Details, Runtime Analysis, and Scalability

Our code is based on Python3.7 and PyTorch [37]. Data is standardized and the optimization in Eqs. (4), (5), and (7) are solved by Adam [27], with a learning rate of \(0.02\). We conduct all the experiments with single Intel(R) Xeon(R) CPU E5-2470. All experiments can be finished within \(2\) hours. We note that our method is very computationally efficient. First, the computational cost is almost irrelevant to sample size: we only need to calculate the sample covariance matrix once and cache it for further use during the optimization. Plus, our estimation method can handle a large number of variables. For example, the running time of our method are roughly 10 seconds, 2 minutes, and 10 minutes for 20 variables, 50 variables, and 100 variables respectively. For 300 variables, which is a considerably large number for typical experiments considered in causal discovery papers, the estimation can still be finished within around one hour.

It is also worth noting that model misspecifications do not influence the computation cost of our method. We briefly discuss the efficiency of checking whether conditions in Theorem 3 hold, together with what if conditions do not hold in solving real-life problems in Appendices A.8 and A.9.

### Real-World Data Performance

In this section, we employ a famous psychometric dataset - Big Five dataset https://openpsychometrics.org/, to validate our method. It consists of 50 indicators and close to 20,000 data points. There are five dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism (O-C-E-A-N). Each is measured with 10 indicators. Data is standardized.We employ the RLCD method [18] to determine the MEC and GIN [58] to decide the remaining directions. Then we employ the proposed Estimator-TR to estimate all the edge coefficients. The structure satisfies the condition in Theorem 5 so we know that the parameters are identifiable.

The estimated edge coefficients are shown in Figure 4. We found that our estimated coefficients are well aligned with existing psychology studies. For example, according to [16; 17], being successful in exploratory endeavors depends on the stability to pursue them. This is illustrated in our result where \(\text{L5}\frac{+\,0.26}{\text{L6}}\) and \(\text{L3}\frac{+\,0.39}{\text{L2}}\) indicates that Conscientiousness positively influence openness and Agreeableness positively influences Extraversion. Moreover, it has been shown that people are likely to weigh the outcomes of their actions, thus, their level of Conscientiousness coupled with Neuroticism may prohibit them from engaging in risky behaviors (\(\text{L5}\frac{-\,0.27}{\text{N10}}\frac{-\,0.12}{\text{L3}}\frac{+\,0.39}{ \text{L2}}\)) [54]. Such consistency with current psychometric studies again validates the effectiveness of the proposed method in parameter estimation of real-life systems.

## 6 Conclusion

In this paper, we characterize indeterminacy of parameter identification and provide conditions for identifiability. Finally, we propose a novel estimation method and validate it by empirical study.

Acknowledgement

This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, Florin Court Capital, and the MBZUAI-WIS grant.

## References

* [1] Jeffrey Adams, Niels Hansen, and Kun Zhang. Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases. _Advances in Neural Information Processing Systems_, 34:22822-22833, 2021.
* [2] Sina Akbari, Ehsan Mokhtarian, AmirEmad Ghassami, and Negar Kiyavash. Recursive causal structure learning in the presence of latent variables and selection bias. In _Advances in Neural Information Processing Systems_, volume 34, pages 10119-10130, 2021.
* [3] Animashree Anandkumar, Daniel Hsu, Adel Javanmard, and Sham Kakade. Learning linear bayesian networks with latent variables. In _International Conference on Machine Learning_, pages 249-257, 2013.
* [4] Ankur Ankan, Inge Wortel, Kenneth Bollen, and Johannes Textor. Combining graphical and algebraic approaches for parameter identification in latent variable structural equation models. In _International Conference on Artificial Intelligence and Statistics_, pages 7252-7264. PMLR, 2023.
* [5] Rina Foygel Barber, Mathias Drton, Nils Sturma, and Luca Weihs. Half-trek criterion for identifiability of latent variable models. _The Annals of Statistics_, 50(6):3174-3196, 2022.
* [6] Elias Bareinboim and Jin Tian. Recovering causal effects from selection bias. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29, 2015.
* [7] Paul A Bekker and Jos MF ten Berge. Generic global indentification in factor analysis. _Linear Algebra and its Applications_, 264:255-263, 1997.
* [8] Carlos Brito and Judea Pearl. A new identification condition for recursive models with correlated errors. _Structural Equation Modeling_, 9(4):459-474, 2002.
* [9] Carlos Brito and Judea Pearl. Generalized instrumental variables. _arXiv preprint arXiv:1301.0560_, 2012.
* [10] Philippe Brouillard, Sebastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable causal discovery from interventional data. In _Advances in Neural Information Processing Systems_, 2020.
* [11] Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, and Kun Zhang. Triad constraints for learning causal structure of latent variables. _Advances in neural information processing systems_, 32, 2019.
* [12] Zhengming Chen, Feng Xie, Jie Qiao, Zhifeng Hao, Kun Zhang, and Ruichu Cai. Identification of linear latent variable model with arbitrary distribution. In _Proceedings 36th AAAI Conference on Artificial Intelligence (AAAI)_, 2022.
* [13] David Maxwell Chickering. Optimal structure identification with greedy search. _Journal of machine learning research_, 3(Nov):507-554, 2002.
* [14] Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson. Learning high-dimensional directed acyclic graphs with latent and selection variables. _The Annals of Statistics_, pages 294-321, 2012.
* [15] Philipp Dettling, Mathias Drton, and Mladen Kolar. On the lasso for graphical continuous lyapunov models. In _Causal Learning and Reasoning_, pages 514-550. PMLR, 2024.
* [16] Colin G. DeYoung. Higher-order factors of the big five in a multi-informant sample. _J Pers Soc Psychol_, 91(6)(6):1138-1151, 2006.

* [17] Colin G. DeYoung. Cybernetic big five theory. _J Res Pers_, 56:33-56, 2015.
* [18] Xinshuai Dong, Biwei Huang, Ignavier Ng, Xiangchen Song, Yujia Zheng, Songyao Jin, Roberto Legaspi, Peter Spirtes, and Kun Zhang. A versatile causal discovery framework to allow causally-related hidden variables. In _International Conference on Learning Representation_, 2024.
* [19] Mathias Drton, Rina Foygel, and Seth Sullivant. Global identifiability of linear structural equation models. _The Annals of Statistics_, 39(2):865-886, 2011.
* [20] Rina Foygel, Jan Draisma, and Mathias Drton. Half-trek criterion for generic identifiability of linear structural equation models. _The Annals of Statistics_, pages 1682-1713, 2012.
* [21] Richard L Gorsuch. _Factor analysis: Classic edition_. Routledge, 2014.
* [22] Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge University Press, 2 edition, 2012.
* [23] Patrik O Hoyer, Shohei Shimizu, Antti J Kerminen, and Markus Palviainen. Estimation of causal effects using linear non-gaussian causal models with hidden variables. _International Journal of Approximate Reasoning_, 49(2):362-378, 2008.
* [24] Biwei Huang, Charles Jia Han Low, Feng Xie, Clark Glymour, and Kun Zhang. Latent hierarchical causal structure discovery with rank constraints. _arXiv preprint arXiv:2210.01798_, 2022.
* [25] Yimin Huang and Marco Valtorta. Pearl's calculus of intervention is complete. _arXiv preprint arXiv:1206.6831_, 2012.
* [26] Yonghan Jung, Jin Tian, and Elias Bareinboim. Learning causal effects via weighted empirical risk minimization. _Advances in neural information processing systems_, 33:12697-12709, 2020.
* [27] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [28] Erich Kummerfeld and Joseph Ramsey. Causal clustering for 1-factor measurement models. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1655-1664. ACM, 2016.
* [29] Daniel Kumor, Carlos Cinelli, and Elias Bareinboim. Efficient identification in linear structural causal models with auxiliary cutsets. In _International Conference on Machine Learning_, pages 5501-5510. PMLR, 2020.
* [30] Manabu Kuroki and Judea Pearl. Measurement bias and effect restoration in causal inference. _Biometrika_, 101(2):423-437, 2014.
* [31] Sebastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural DAG learning. In _International Conference on Learning Representations_, 2020.
* [32] Dennis Leung, Mathias Drton, and Hisayuki Hara. Identifiability of directed gaussian graphical models with one latent source. _Electronic Journal of Statistics_, 10, 05 2015.
* [33] Wang Miao, Zhi Geng, and Eric J Tchetgen Tchetgen. Identifying causal effects with proxy variables of an unmeasured confounder. _Biometrika_, 105(4):987-993, 2018.
* [34] Ignavier Ng, Xinshuai Dong, Haoyue Dai, Biwei Huang, Peter Spirtes, and Kun Zhang. Score-based causal discovery of latent variable causal models. In _Forty-first International Conference on Machine Learning_, 2024.
* [35] Ignavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and dag constraints for learning linear dags. _Advances in Neural Information Processing Systems_, 33:17943-17954, 2020.
* [36] R. M. O'BRIEN. Identification of simple measurement models with multiple latent variables and correlated errors. _Sociological Methodology_, 24, 1994.

* [37] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
* [38] Judea Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, New York, NY, USA, 2000.
* [39] Judea Pearl. Causal inference in statistics: An overview. _Statistics surveys_, 3:96-146, 2009.
* [40] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of causal inference: foundations and learning algorithms_. The MIT Press, 2017.
* [41] O. Reiersol. On the identifiability of parameters in thurstone's multiple factor analysis. _Psychometrika_, 15(2), 1950.
* [42] Thomas Richardson and Peter Spirtes. Ancestral graph Markov models. _The Annals of Statistics_, 30(4):962-1030, 2002.
* [43] Saber Salehkaleybar, AmirEmad Ghassami, Negar Kiyavash, and Kun Zhang. Learning linear non-gaussian causal models in the presence of latent variables. _Journal of Machine Learning Research_, 21(39):1-24, 2020.
* [44] Bernhard Scholkopf. Causality for machine learning. _arXiv preprint arXiv:1911.10500_, 2019.
* [45] Alexander Shapiro. Identifiability of factor analysis: Some results and open problems. _Linear Algebra and its Applications_, 70:1-7, 1985.
* [46] Shohei Shimizu, Patrik O Hoyer, and Aapo Hyvarinen. Estimation of linear non-gaussian acyclic models for latent factors. _Neurocomputing_, 72(7-9):2024-2027, 2009.
* [47] Ilya Shpitser and Judea Pearl. Identification of joint interventional distributions in recursive semimarkovian causal models. In _Proceedings of the National Conference on Artificial Intelligence_, volume 21, page 1219. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
* [48] Ricardo Silva, Richard Scheines, Clark Glymour, Peter Spirtes, and David Maxwell Chickering. Learning the structure of linear latent variable models. _Journal of Machine Learning Research_, 7(2), 2006.
* [49] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. _Causation, prediction, and search_. MIT press, 2000.
* [50] Seth Sullivant, Kelli Talaska, and Jan Draisma. Trek separation for gaussian graphical models. _arXiv:0812.1938._, 2010.
* [51] Jin Tian. Parameter identification in a class of linear structural equation models. In _Twenty-First International Joint Conference on Artificial Intelligence_, 2009.
* [52] Jin Tian and Judea Pearl. A general identification condition for causal effects. In _Aaait/iaai_, pages 567-573, 2002.
* [53] Sivan Toledo. Locality of reference in lu decomposition with partial pivoting. _SIAM Journal on Matrix Analysis and Applications_, 18(4):1065-1081, 1997.
* [54] Nicholas A. Turiano, Daniel K. Mroczek, Jan Moynihan, and Benjamin P. Chapman. Big 5 personality traits and interleukin-6: evidence for "healthy neuroticism" in a us population sample. _Brain Behav Immun_, pages 83-89, 2013.
* [55] Gherardo Varando and Niels Richard Hansen. Graphical continuous lyapunov models. In _Conference on Uncertainty in Artificial Intelligence_, pages 989-998. Pmlr, 2020.
* [56] L. L. WEGGE. Local identifiability of the factor analysis and measurement error model parameter. _Journal of Econometrics_, 70, 1996.
* [57] B. Williams. Identification of the linear factor model. _Econometric Reviews_, 39(1), 2020.

* [58] Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. Generalized independent noise condition for estimating latent variable causal graphs. _Advances in neural information processing systems_, 33:14891-14902, 2020.
* [59] Xun Zheng. _Learning DAGs with Continuous Optimization_. PhD thesis, PhD thesis, Carnegie Mellon University, 2020.

Proofs

### Proof of Theorem 1

**Theorem 1** (Indeterminacy of Scaling of \(\Omega_{\epsilon_{\mathbf{L}}}\)).: _Consider a model that follows Def. 1 with number of latent variables \(m\geq 1\) and \(\theta=(F_{\mathbf{LL}},F_{\mathbf{LX}},F_{\mathbf{XL}},F_{\mathbf{XX}},\Omega_{ \epsilon_{\mathbf{L}}},\Omega_{\epsilon_{\mathbf{X}}})\). Let \(\Lambda\) be any invertible diagonal matrix, and \(\tilde{\theta}=(\tilde{F}_{\mathbf{LL}},\tilde{F}_{\mathbf{LX}},\tilde{F}_{ \mathbf{XL}},\tilde{F}_{\mathbf{XX}},\tilde{\Omega}_{\epsilon_{\mathbf{L}}}, \tilde{\Omega}_{\epsilon_{\mathbf{X}}})\), where \(\tilde{F}_{\mathbf{LL}}=\Lambda^{-1}F_{\mathbf{LL}}\Lambda,\;\tilde{F}_{ \mathbf{LX}}=\Lambda^{-1}F_{\mathbf{LX}},\;\tilde{F}_{\mathbf{XL}}=F_{\mathbf{ XL}}\Lambda,\;\tilde{F}_{\mathbf{XX}}=F_{\mathbf{XX}},\;\tilde{\Omega}_{ \epsilon_{\mathbf{L}}}=\Lambda^{2}\Omega_{\epsilon_{\mathbf{L}}},\;\tilde{ \Omega}_{\epsilon_{\mathbf{X}}}=\Omega_{\epsilon_{\mathbf{X}}}\). Then, \(\tilde{\theta}\) and \(\theta\) entail the same observations, i.e., \(\tilde{\Sigma}_{\mathbf{X}}=\Sigma_{\mathbf{X}}\). Furthermore, we have \(\tilde{\Sigma}_{\mathbf{L}}=\Lambda\Sigma_{\mathbf{L}}\Lambda\)._

Proof of Theorem 1.: Let \(\begin{pmatrix}F_{\mathbf{LL}}&F_{\mathbf{LX}}\\ F_{\mathbf{XL}}&F_{\mathbf{XX}}\end{pmatrix}=\begin{pmatrix}A&B\\ C&D\end{pmatrix}\) and \(\begin{pmatrix}\tilde{F}_{\mathbf{LL}}&\tilde{F}_{\mathbf{LX}}\\ \tilde{F}_{\mathbf{XL}}&\tilde{F}_{\mathbf{XX}}\end{pmatrix}=\begin{pmatrix} \tilde{A}&\tilde{B}\\ \tilde{C}&\tilde{D}\end{pmatrix}\). Let \(M\) and \(N\) be matrices defined as in Proposition 1, and similarly for \(\tilde{M}\) and \(\tilde{N}\). We then have

\[\tilde{M} =\Big{(}I-\tilde{A}-\tilde{B}(I-\tilde{D})^{-1}\tilde{C}\Big{)}^{-1}\] \[=\left(\Lambda^{-1}\Lambda-\Lambda^{-1}A\Lambda-(\Lambda^{-1}B)( I-D)^{-1}(C\Lambda)\right)^{-1}\] \[=\Lambda^{-1}\left(I-A-B(I-D)^{-1}C\right)^{-1}\Lambda\] \[=\Lambda^{-1}M\Lambda\]

and

\[\tilde{N} =\Big{(}(I-\tilde{A})\tilde{C}^{-1}(I-\tilde{D})-\tilde{B}\Big{)} ^{-1}\] \[=\left((\Lambda^{-1}\Lambda-\Lambda^{-1}A\Lambda)(C\Lambda)^{-1}( I-D)-\Lambda^{-1}B\right)^{-1}\] \[=\left((I-A)C^{-1}(I-D)-B\right)^{-1}\Lambda\] \[=N\Lambda.\]

By Proposition 1, the latent covariance matrix \(\tilde{\Sigma}_{\mathbf{L}}\) after rescaling of the parameters is given by

\[\tilde{\Sigma}_{\mathbf{L}} =\tilde{M}^{T}\tilde{\Omega}_{\epsilon_{\mathbf{L}}}\tilde{M}+ \tilde{N}^{T}\tilde{\Omega}_{\epsilon_{\mathbf{X}}}\tilde{N}\] \[=(\Lambda^{T}M^{T}\Lambda^{-T})(\Lambda\Omega_{\epsilon_{\mathbf{ X}}}\Lambda)(\Lambda^{-1}M\Lambda)+\Lambda^{T}N^{T}\Omega_{\epsilon_{ \mathbf{X}}}N\Lambda\] \[=\Lambda(M^{T}\Omega_{\epsilon_{\mathbf{L}}}M+N^{T}\Omega_{ \epsilon_{\mathbf{X}}}N)\Lambda\] \[=\Lambda\Sigma_{\mathbf{L}}\Lambda.\]

This implies that the variance of each latent variable \(\mathsf{L}_{i}\) is scaled by \(\Lambda_{ii}^{2}\). By Proposition 1, the observed covariance matrix \(\tilde{\Sigma}_{\mathbf{X}}\) after rescaling of the parameters is given by

\[\tilde{\Sigma}_{\mathbf{X}} =(I-\tilde{D})^{-T}\Big{(}\tilde{B}^{T}\tilde{\Sigma}_{\mathbf{L }}B+\tilde{\Omega}_{\epsilon_{\mathbf{X}}}+\tilde{\Omega}_{\epsilon_{\mathbf{X }}}\tilde{N}\tilde{B}+\tilde{B}^{T}\tilde{N}^{T}\tilde{\Omega}_{\epsilon_{ \mathbf{X}}}\Big{)}(I-\tilde{D})^{-1}\] \[=(I-D)^{-T}\Big{(}(\Lambda^{-1}B)^{T}(\Lambda\Sigma_{\mathbf{L} }\Lambda)(\Lambda^{-1}B)\] \[\qquad\qquad\qquad+\Omega_{\epsilon_{\mathbf{X}}}+\Omega_{ \epsilon_{\mathbf{X}}}(N\Lambda)(\Lambda^{-1}B)+(\Lambda^{-1}B)^{T}(N\Lambda)^ {T}\Omega_{\epsilon_{\mathbf{X}}}\Big{)}(I-D)^{-1}\] \[=(I-D)^{-T}\Big{(}B^{T}\Sigma_{\mathbf{L}}B+\Omega_{\epsilon_{ \mathbf{X}}}+\Omega_{\epsilon_{\mathbf{X}}}NB+B^{T}N^{T}\Omega_{\epsilon_{ \mathbf{X}}}\Big{)}(I-D)^{-1}\] \[=\Sigma_{\mathbf{X}}.\]

### Proof of Theorem 2

**Theorem 2** (Group Sign Indeterminacy).: _Consider a model that follows Def. 1 with number of latent variables \(m\geq 1\), \(\theta=(F_{\mathbf{LL}},F_{\mathbf{LX}},F_{\mathbf{XL}},F_{\mathbf{XX}},\Omega_ {\epsilon_{\mathbf{L}}},\Omega_{\epsilon_{\mathbf{X}}})\), and \(\Omega_{\epsilon_{\mathbf{L}}}=I\). Let \(S\) be a diagonal sign matrix (entries are either \(1\) or \(-1\)), and \(\tilde{\theta}=(\tilde{F}_{\mathbf{LL}},\tilde{F}_{\mathbf{LX}},\tilde{F}_{ \mathbf{XL}},\tilde{F}_{\mathbf{XX}},\tilde{\Omega}_{\epsilon_{\mathbf{L}}}, \tilde{\Omega}_{\epsilon_{\mathbf{X}}})\), where_

\[\tilde{F}_{\mathbf{LL}}=SF_{\mathbf{LL}}S,\;\tilde{F}_{\mathbf{LX}}=SF_{\mathbf{ LX}},\;\tilde{F}_{\mathbf{XL}}=F_{\mathbf{XL}}S,\;\tilde{F}_{\mathbf{XL}}=F_{ \mathbf{XL}}S,\;\tilde{F}_{\mathbf{XX}}=F_{\mathbf{XX}},\;\tilde{\Omega}_{ \epsilon_{\mathbf{L}}}=\Omega_{\epsilon_{\mathbf{L}}}=I,\;\tilde{\Omega}_{ \epsilon_{\mathbf{X}}}=\Omega_{\epsilon_{\mathbf{X}}}.\]

_Then, \(\tilde{\theta}\) and \(\theta\) entail the same observations, i.e., \(\tilde{\Sigma}_{\mathbf{X}}=\Sigma_{\mathbf{X}}\), and \((\tilde{\Sigma}_{\mathbf{L}})_{ii}=(\Sigma_{\mathbf{L}})_{ii},\;\forall i\in[m]\)._Proof of Theorem 2.: Let \(\begin{pmatrix}F_{\mathbf{LL}}&F_{\mathbf{LX}}\\ F_{\mathbf{XL}}&F_{\mathbf{XX}}\end{pmatrix}=\begin{pmatrix}A&B\\ C&D\end{pmatrix}\) and \(\begin{pmatrix}\tilde{F}_{\mathbf{LL}}&\tilde{F}_{\mathbf{LX}}\\ \tilde{F}_{\mathbf{XL}}&\tilde{F}_{\mathbf{XX}}\end{pmatrix}=\begin{pmatrix} \tilde{A}&\tilde{B}\\ \tilde{C}&\tilde{D}\end{pmatrix}\). Since \(S\) is a diagonal sign matrix, we have

\[\tilde{A}\coloneqq S^{-1}AS,\quad\tilde{B}\coloneqq S^{-1}B,\quad\tilde{C} \coloneqq CS,\quad\tilde{D}\coloneqq D,\quad\tilde{\Omega}_{\epsilon_{ \mathbf{L}}}\coloneqq S^{2}\Omega_{\epsilon_{\mathbf{L}}},\quad\text{and} \quad\tilde{\Omega}_{\epsilon_{\mathbf{X}}}\coloneqq\Omega_{\epsilon_{\mathbf{ X}}}.\]

Note that \(S\) is an invertible diagonal matrix. By Theorem 1, we have \(\tilde{\Sigma}_{\mathbf{X}_{\mathcal{G}}}=\Sigma_{\mathbf{X}_{\mathcal{G}}}\) and \(\tilde{\Sigma}_{\mathbf{L}_{\mathcal{G}}}=S\Sigma_{\mathbf{L}_{\mathcal{G}}}S\), and thus \((\tilde{\Sigma}_{\mathbf{L}_{\mathcal{G}}})_{ii}=(\Sigma_{\mathbf{L}_{ \mathcal{G}}})_{ii},\;\forall i\in[m]\). 

### Proof of Theorem 3

The structure identifiability part is that if \(\mathcal{G}\) satisfies Condition 1 and Condition 2, the structure of \(\mathcal{G}\) can be identified up to the Markov equivalence class of \(\mathcal{O}_{\mathrm{a}}(\mathcal{O}_{s}(\mathcal{G}))\), which is by Theorem 12 in [18].

Next we will focus on the proof of parameter identifiability part, i.e., for any DAG in the equivalence class, if (i) and (ii) in Theorem 3 are satisfied, the parameters are identifiable (up to group sign). Without loss of generality, we assume that all variables have unit variance and zero mean. The reason is that if we can show that the parameters are identifiable (up to group sign) under this assumption, then it is straightforward to show that they are also identifiable under the original assumption where \(\Omega_{\epsilon_{\mathbf{L}}}=I\).

**Lemma 1**.: _Let \(\mathbf{X},\mathbf{Y}\) be two set of variables, we have \(\Sigma_{\mathbf{Y}|\mathbf{X}=x}=\Sigma_{\mathbf{Y}}-\Sigma_{\mathbf{YX}} \Sigma_{\mathbf{X}}^{-1}\Sigma_{\mathbf{XY}}\)._

**Lemma 2**.: _Consider a graph \(\mathcal{G}\) that satisfies (i) and (ii) in Theorem 3. For an atomic cover \(\mathbf{V}\) in \(\mathcal{G}\) with one latent variable, \(\mathbf{V}=\{\mathsf{L}\}\cup\{\mathsf{X}_{i}\}_{i=1}^{k}\) (\(k\) could be zero), if it has an observed pure child \(\mathsf{C}\) and the coefficients of the edges from \(\mathbf{V}\) to \(\mathsf{C}\), i.e., \(f_{\mathsf{L}\rightarrow\mathsf{C}}\), \(f_{\mathsf{X}_{1}\rightarrow\mathsf{C}}\),..., \(f_{\mathsf{X}_{n}\rightarrow\mathsf{C}}\), are known, then for any variable \(\mathsf{A}\), such that \(\mathsf{A}\neq\mathsf{C}\) and \(\mathsf{A}\) is not a descendant of \(\mathsf{C}\), \(\sigma_{\mathsf{L},\mathsf{A}}\) can be calculated as \((\sigma_{\mathsf{A},\mathsf{C}}-\Sigma_{i=1}^{k}\sigma_{\mathsf{X}_{i},\mathsf{ A}}f_{\mathsf{X}_{i}\rightarrow\mathsf{C}})/f_{\mathsf{L}\rightarrow\mathsf{C}}\)._

Proof of Lemma 2.: By the definition of atomic covers, all variables in \(\mathbf{V}=\{\mathsf{L}\}\cup\{\mathsf{X}_{i}\}_{i=1}^{k}\) are not adjacent. By trek rule and the fact that \(\mathsf{C}\) is a pure child of \(\mathbf{V}\), all treks from \(\mathsf{C}\) to \(\mathsf{A}\) go through \(\mathbf{V}\), and thus by the trek rule we have \(\sigma_{\mathsf{L},\mathsf{A}}f_{\mathsf{L}\rightarrow\mathsf{C}}+\Sigma_{i=1 }^{k}\sigma_{\mathsf{X}_{i},\mathsf{A}}f_{\mathsf{X}_{i}\rightarrow\mathsf{C} }=\sigma_{\mathsf{A},\mathsf{C}}\). 

## Appendix B

Figure 5: A simple graph that satisfies conditions in Theorem 3, as for each atomic cover with one latent variable, it has no observed variable.

Figure 6: A more complicated graph that satisfies conditions in Theorem 3 and there is an atomic cover that has one latent variable and nonzero observed variables, e.g., \(\{\mathsf{L}_{1},\mathsf{X}_{5}\}\) in the graph. The condition (ii) in Theorem 3 is satisfied in that \(\mathsf{L}_{1},\mathsf{X}_{5}\) can be d-separated by \(\mathsf{X}_{3}\).

**Remark:** This lemma implies that we can find all edge coefficients of the graph in a bottom-up fashion. Roughly speaking, for a latent variable \(L\) that belongs to an atomic cover \(V\), once we identify \(f_{L\to C}\), \(f_{X_{1}\to C}\),..., \(f_{X_{n}\to C}\) where \(C\) is an observed pure child of \(V\), we can take \(L\) as if it is observed. More specific explanations can be found in the following proof.

**Theorem 3** (Sufficient Condition for Parameter Identifiability (up to group sign), Based on Structure Identifiability).: _Assume that \(\mathcal{G}\) satisfies Conditions 1 and 2 and thus the structure can be identified up to the MEC of \(\mathcal{O}_{a}(\mathcal{O}_{s}(\mathcal{G}))\). For any DAG in the equivalence class, the parameters are identifiable, if both the following hold:_

1. _For any atomic cover_ \(V=X\cup L\)_,_ \(|L|\leq 1\)_._
2. _If an atomic cover_ \(V=X\cup L\) _satisfies_ \(|L|\neq 0\) _and_ \(|X|\geq 1\)_, then all simple treks (Def._ 5_) between_ \(L\) _and_ \(X\) _do not contain any latent variables that are not in_ \(L\)_._

Proof of Theorem 3.: Consider a graph \(\mathcal{G}\) that satisfies (i) and (ii) in Theorem 3. We first show that if all the pure children of an atomic cover \(V\) are observed, then all the edge coefficients from the atomic cover to its children are identifiable (up to group sign). To this end, we categorize the scenarios into four cases and prove them separately.

(a) \(V=\{X\}\) contains a single observed variable. The proof for this case is trivial as the edge coefficient from \(X\) to its pure child \(C\) is simply \(\sigma_{X,C}\).

(b) \(V=\{L\}\) contains a single latent variable. By Condition 1 there must exist \(C_{1}\), \(C_{2}\), and \(X_{N}\), such that \(C_{1}\), \(C_{2}\) are pure children of \(V\) and \(X_{N}\) is an observed variable that has a tre to \(V\). Then we have

\[\sigma_{C_{1},C_{2}}=f_{L\to C_{1}}f_{L\to C_{2}},\quad\sigma_{C_{1},X_{N}}=f_ {L\to C_{1}}\sigma_{L,X_{N}},\quad\text{and}\quad\sigma_{C_{2},X_{N}}=f_{L\to C _{2}}\sigma_{L,X_{N}}.\]

By these three equations, we can solve \(f_{L\to C_{1}}\) and \(f_{L\to C_{2}}\). If \(V\) has more than two pure children, we can prove the identifiability similarly in a pairwise fashion.

(c) \(V=\{L\}\cup\{X_{i}\}_{i=1}^{k}\) (\(k\geq 1\)) contains a single latent variable and \(k\) observed variables, where \(V\) has at least three pure children. We assume that there exist \(C_{1}\), \(C_{2}\), and \(C_{3}\), such that \(C_{1}\), \(C_{2}\), and \(C_{3}\) are pure children of \(V\). Let \(\sigma_{L|(X)_{i=1}^{k}}=t\). In this case, we have

\[\sigma_{C_{1},C_{2}|(X_{i})_{i=1}^{k}} =tf_{L\to C_{1}}f_{L\to C_{2}},\] (8) \[\sigma_{C_{1},C_{2}|(X_{i})_{i=1}^{k}} =tf_{L\to C_{1}}f_{L\to C_{3}},\] (9) \[\sigma_{C_{2},C_{3}|(X_{i})_{i=1}^{k}} =tf_{L\to C_{2}}f_{L\to C_{3}}.\] (10)

By these three equations, we can solve \(f_{L\to C_{1}}f_{L\to C_{2}}\), and \(f_{L\to C_{3}}\), with the only remaining free parameter \(t\). In other words, we have \(f_{L\to C_{1}}(t),f_{L\to C_{2}}(t)\), and \(f_{L\to C_{3}}(t)\).

Next, we show that \(\forall i=1,...,k,j=1,...,3,f_{X_{n}\to C_{i}}\) can be identified. Specifically, as all simple treks between \(L\) and \(X\) contain only observed variables except \(L\), and \(L\) and \(X\) are not directly adjacent, there must exist \(\hat{X}\) such that \(\hat{X}\) d-separates \(L\) and \(X\). Thus, \(f_{X_{i}\to C_{i}}\sigma_{X_{i}|\hat{X}\cup X\setminus X_{i}}=\sigma_{X_{i} |\hat{X}\cup X\setminus X_{i}}\), by which \(f_{X_{i}\to C_{i}}\) can be solved.

Now we solve \(t\). The key is that all the edge coefficients along all simple treks between \(X\) and \(L\) can be identified, by using Lemma 2, with only one free parameter \(t\). Thus, we can make use of simple trek rule to parameterize \(\Sigma_{L,X}\) as a function of \(t\). By Lemma 1, \(\sigma_{L|X}=t\) can also be formulated as a function of \(\Sigma_{L,X}\), and thus \(t\) can be solved. If \(V\) has more than three pure children, we just choose all the combinations of any three pure children.

(d) \(V=\{L\}\cup\{X_{i}\}_{i=1}^{k}\) (\(k\geq 1\)) contains a single latent variable and \(k\) observed variables, where \(V\) has two pure children. By Condition 1, there must exist \(C_{1}\), \(C_{2}\) as the pure children of \(V\). If there exists one additional pure child, then it is the same as (c). Plus, if there only exist these two pure children, there must exist \(V_{N}\) such that \(V_{N}\) is a neighbor of \(V\). If \(V_{N}\) is observed, let \(X_{N}=V_{N}\), otherwise we recursively take \(X_{N}\) as the observed pure children of \(V_{N}\).

Let \(\sigma_{L|(X_{i})_{i=1}^{k}}=t\). In this case, we have

\[\sigma_{C_{1},C_{2}|(X_{i})_{i=1}^{k}}=tf_{L\to C_{1}}f_{L\to C_{2}},\] (11) \[\sigma_{C_{1},X_{N}|(X_{i})_{i=1}^{k}}=tf_{L\to C_{1}}\sigma_{X_{N}},\] (12) \[\sigma_{C_{2},X_{N}|(X_{i})_{i=1}^{k}}=tf_{L\to C_{2}}\sigma_{X_{N}}.\] (13)Similar to (c), by solving the above, we have \(f_{\mathsf{L}\rightarrow\mathsf{C}_{1}}(t)\) and \(f_{\mathsf{L}\rightarrow\mathsf{C}_{2}}(t)\).

Next, we show that \(\forall i=1,...,k,j=1,...,2,f_{\mathsf{X}_{i}\rightarrow\mathsf{C}_{i}}\) can be identified. Specifically, as all simple treks between \(\mathbf{L}\) and \(\mathbf{X}\) contain only observed variables except \(\mathbf{L}\), and \(\mathbf{L}\) and \(\mathbf{X}\) are not directly adjacent, there must exist \(\mathbf{\hat{X}}\) such that \(\mathbf{\hat{X}}\) d-separates \(\mathbf{L}\) and \(\mathbf{X}\). Thus, \(f_{\mathsf{X}_{i}\rightarrow\mathsf{C}_{i}}\sigma_{\mathsf{X}_{i}|\mathbf{ \hat{X}}\cup\mathbf{X}\setminus\mathsf{X}_{i}}=\sigma_{\mathsf{X}_{i}|\mathbf{ \hat{X}}\cup\mathbf{X}\setminus\mathsf{X}_{i}}\), by which \(f_{\mathsf{X}_{i}\rightarrow\mathsf{C}_{i}}\) can be solved.

Now we solve \(t\). The key is that all the edge coefficients along all simple treks between \(\mathbf{X}\) and \(\mathbf{L}\) can be identified, by using Lemma 2, with only one free parameter \(t\). Thus, we can make use of simple trek rule to parameterize \(\Sigma_{\mathsf{L},\mathbf{X}}\) as a function of \(t\). By Lemma 1, \(\sigma_{\mathsf{L}|\mathbf{X}}=t\) can also be formulated as a function of \(\Sigma_{\mathsf{L},\mathbf{X}}\), and thus \(t\) can be solved.

Taking (a), (b), (c), (d) into consideration, for a graph that satisfies the conditions in Theorem 3, for an atomic cover \(\mathbf{V}\) in the graph, if all pure children of it are observed, then all the edge coefficients from \(\mathbf{V}\) to its pure children can be identified.

Now, we will prove by induction to show that, for a graph that satisfies the conditions in Theorem 3, for any atomic cover \(\mathbf{V}\) in the graph, all the edge coefficients from \(\mathbf{V}\) to its children can be identified, and thus all the edge coefficients of the graph can be identified (the set of all edge coefficients in the graph is the union of the set of edge coefficients from each \(\mathbf{V}\) to each \(\mathbf{V}\)'s children).

To this end, we first index all the atomic covers by the inverse causal ordering, such that leaf nodes have smaller indexes. Then we have a sequence of atomic covers \(\mathbf{V_{i}}\), \(i=1,...,C\) in the graph, where \(C\) is the number of atomic covers in the graph.

(i) We show for \(\mathbf{V_{i}},i=1\), all the edge coefficients from \(\mathbf{V_{1}}\) to its children can be identified. This is proved by considering (a) (b) (c) (d), as \(\mathbf{V_{1}}\)'s children must be all observed; otherwise it cannot be indexed as 1.

(ii) We show that, for \(i>1\), if for all \(\mathbf{V_{j}}\), \(1\leq j<i\), all the edge coefficients from \(\mathbf{V_{j}}\) to \(\mathbf{V_{j}}\)'s children has been identified, then all the edge coefficients from \(\mathbf{V_{i}}\) to \(\mathbf{V_{i}}\)'s children can also be identified. This can be proved by combining (a) (b) (c) (d) with Lemma 2. If \(\mathbf{V_{i}}\) has children that are latent, then the latent children must belong to an atomic cover with a smaller index. Therefore, as all the edge coefficients from \(\mathbf{V_{j}}\) to \(\mathbf{V_{j}}\)'s children have been identified, by the use of Lemma 2, the latent children of \(\mathbf{V_{i}}\) can be taken as if they are observed. Therefore, all the edge coefficients from \(\mathbf{V_{i}}\) to \(\mathbf{V_{i}}\)'s children can also be identified.

Taking (i) and (ii) together, all the edge coefficients of the graph can be identified.

### Proof of Theorem 5

**Theorem 5** (Sufficient Condition for Parameter Identifiability (up to group sign) without Requiring Structure Identifiability).: _In \(\mathcal{G}\), if for every latent variable \(\mathsf{L}\) there always exist another three distinct variables (which can be latent or observed), such that two of the three are pure children of \(\mathsf{L}\) and the rest one is a neighbor of \(\mathsf{L}\), then the parameters are identifiable._

Proof of Theorem 5.: The proof is a special case of (b) in the proof of Theorem 3. 

### Proof of Theorem 4

**Lemma 3**.: _Let \(\Sigma_{\mathbf{X}}\) be the observed covariance matrix entailed by \(F_{\mathbf{L}\mathbf{L}},F_{\mathbf{L}\mathbf{X}},F_{\mathbf{X}\mathbf{L}},F_ {\mathbf{X}\mathbf{X}},\Omega_{\epsilon_{\mathbf{L}}},\Omega_{\epsilon_{ \mathbf{X}}}\). Let \(Q\) be an orthogonal matrix, and_

\[\tilde{F}_{\mathbf{L}\mathbf{L}}=Q^{T}F_{\mathbf{L}\mathbf{L}}Q,\;\tilde{F}_{ \mathbf{L}\mathbf{X}}=Q^{T}F_{\mathbf{L}\mathbf{X}},\;\tilde{F}_{\mathbf{X} \mathbf{L}}=F_{\mathbf{X}\mathbf{L}}Q,\;\tilde{F}_{\mathbf{X}\mathbf{X}}=F_{ \mathbf{X}\mathbf{X}},\;\tilde{\Omega}_{\epsilon_{\mathsf{L}}}=Q^{T}\Omega_{ \epsilon_{\mathsf{L}}}Q,\;\text{and}\;\tilde{\Omega}_{\epsilon_{\mathbf{X}}}= \Omega_{\epsilon_{\mathbf{X}}}.\]

_Then, the matrices \(\tilde{F}_{\mathbf{L}\mathbf{L}},\tilde{F}_{\mathbf{L}\mathbf{X}},\tilde{F}_{ \mathbf{X}\mathbf{L}},\tilde{F}_{\mathbf{X}\mathbf{X}},\tilde{\Omega}_{\epsilon _{\mathbf{L}}},\tilde{\Omega}_{\epsilon_{\mathbf{X}}}\) can also entail the covariance matrix \(\Sigma_{\mathbf{X}}\)._Proof of Lemma 3.: Let \(\begin{pmatrix}F_{\mathbf{L}\mathbf{L}}&F_{\mathbf{L}\mathbf{X}}\\ F_{\mathbf{X}\mathbf{L}}&F_{\mathbf{X}\mathbf{X}}\end{pmatrix}=\begin{pmatrix}A &B\\ C&D\end{pmatrix}\) and \(\begin{pmatrix}\tilde{F}_{\mathbf{L}\mathbf{L}}&\tilde{F}_{\mathbf{L}\mathbf{X }}\\ \tilde{F}_{\mathbf{X}\mathbf{L}}&\tilde{F}_{\mathbf{X}\mathbf{X}}\end{pmatrix}= \begin{pmatrix}\tilde{A}&\tilde{B}\\ \tilde{C}&\tilde{D}\end{pmatrix}\). Let \(M\) and \(N\) be matrices defined as in Proposition 1, and similarly for \(\tilde{M}\) and \(\tilde{N}\). We then have

\[\tilde{M} =\left(I-\tilde{A}-\tilde{B}(I-\tilde{D})^{-1}\tilde{C}\right)^{ -1}\] \[=\left(Q^{T}Q-Q^{T}AQ-(Q^{T}B)(I-D)^{-1}(CQ)\right)^{-1}\] \[=Q^{-1}\left(I-A-B(I-D)^{-1}C\right)^{-1}Q^{-T}\] \[=Q^{T}MQ\]

and

\[\tilde{N} =\left((I-\tilde{A})\tilde{C}^{-1}(I-\tilde{D})-\tilde{B}\right) ^{-1}\] \[=\left((Q^{T}Q-Q^{T}AQ)(CQ)^{-1}(I-D)-Q^{T}B\right)^{-1}\] \[=\left((I-A)C^{-1}(I-D)-B\right)^{-1}Q^{-T}\] \[=NQ.\]

By Proposition 1, the latent covariance matrix \(\tilde{\Sigma}_{\mathbf{L}}\) is given by

\[\tilde{\Sigma}_{\mathbf{L}} =\tilde{M}^{T}\tilde{\Omega}_{\epsilon_{\mathbf{L}}}\tilde{M}+ \tilde{N}^{T}\tilde{\Omega}_{\epsilon_{\mathbf{X}}}\tilde{N}\] \[=(Q^{T}M^{T}Q^{-T})(Q^{T}\Omega_{\epsilon_{\mathbf{L}}}Q)(Q^{-1} MQ)+Q^{T}N^{T}\Omega_{\epsilon_{\mathbf{X}}}NQ\] \[=Q^{T}(M^{T}\Omega_{\epsilon_{\mathbf{L}}}M+N^{T}\Omega_{ \epsilon_{\mathbf{X}}}N)Q\] \[=Q^{T}\Sigma_{\mathbf{L}}Q.\]

By Proposition 1, the observed covariance matrix \(\tilde{\Sigma}_{\mathbf{X}}\) is given by

\[\tilde{\Sigma}_{\mathbf{X}} =(I-\tilde{D})^{-T}\Big{(}\tilde{B}^{T}\tilde{\Sigma}_{\mathbf{L }}B+\tilde{\Omega}_{\epsilon_{\mathbf{X}}}+\tilde{\Omega}_{\epsilon_{\mathbf{ X}}}\tilde{N}\tilde{B}+\tilde{B}^{T}\tilde{N}^{T}\tilde{\Omega}_{\epsilon_{ \mathbf{X}}}\Big{)}(I-\tilde{D})^{-1}\] \[=(I-D)^{-T}\Big{(}(Q^{T}B)^{T}(Q^{T}\Sigma_{\mathbf{L}}Q)(Q^{T}B) +\Omega_{\epsilon_{\mathbf{X}}}+\Omega_{\epsilon_{\mathbf{X}}}(NQ)(Q^{T}B)+( Q^{T}B)^{T}(NQ)^{T}\Omega_{\epsilon_{\mathbf{X}}}\Big{)}(I-D)^{-1}\] \[=(I-D)^{-T}\Big{(}B^{T}\Sigma_{\mathbf{L}}B+\Omega_{\epsilon_{ \mathbf{X}}}+\Omega_{\epsilon_{\mathbf{X}}}NB+B^{T}N^{T}\Omega_{\epsilon_{ \mathbf{X}}}\Big{)}(I-D)^{-1}\] \[=\Sigma_{\mathbf{X}}.\]

This indicates that the matrices \(\tilde{A},\tilde{B},\tilde{C},\tilde{D},\tilde{\Omega}_{\epsilon_{\mathbf{L}}}\) and \(\tilde{\Omega}_{\epsilon_{\mathbf{X}}}\) can also entail the covariance matrix \(\Sigma_{\mathbf{X}}\). 

Using Lemma 3, we can prove Theorem 4.

**Theorem 4** (Condition for the Existence of Orthogonal Transformation Indeterminacy).: _Consider the model in Definition 1. If a set of latent variables \(\mathbf{L}\) with \(|\mathbf{L}|\geq 2\), have the same parents and children, then there must exist orthogonal transformation indeterminacy regarding the edge coefficients \(F\). In other words, \(F\) can at most be identified up to orthogonal transformation indeterminacy._

Proof of Theorem 4.: Let \(\mathbf{S}_{1}\), \(\mathbf{S}_{2}\), \(\mathbf{S}_{3}\), \(\mathbf{S}_{4}\), and \(\mathbf{S}_{5}\) be the indices of \(\mathbf{L}\), their latent parents, their latent children, their measured parents, and their measured children in \(\mathcal{G}\), respectively. Let \(Q\) be a \(|\mathbf{L}|\times|\mathbf{L}|\) orthogonal matrix. Let \(\begin{pmatrix}F_{\mathbf{L}\mathbf{L}}&F_{\mathbf{L}\mathbf{X}}\\ F_{\mathbf{X}\mathbf{L}}&F_{\mathbf{X}\mathbf{X}}\end{pmatrix}=\begin{pmatrix}A &B\\ C&D\end{pmatrix}\) and \(\begin{pmatrix}\tilde{F}_{\mathbf{L}\mathbf{L}}&\tilde{F}_{\mathbf{L} \mathbf{X}}\\ \tilde{F}_{\mathbf{X}\mathbf{L}}&\tilde{F}_{\mathbf{X}\mathbf{X}}\end{pmatrix}= \begin{pmatrix}\tilde{A}&\tilde{B}\\ \tilde{C}&\tilde{D}\end{pmatrix}\). For matrices \(A,B,C\), and \(D\) from matrix \(F\), suppose that we replace \(A_{\mathbf{S}_{2},\mathbf{S}_{1}}\), \(A_{\mathbf{S}_{1},\mathbf{S}_{3}}\), \(C_{\mathbf{S}_{4},\mathbf{S}_{1}}\), and \(B_{\mathbf{S}_{1},\mathbf{S}_{5}}\) with \(A_{\mathbf{S}_{2},\mathbf{S}_{1}}Q\), \(Q^{T}A_{\mathbf{S}_{1},\mathbf{S}_{3}}\), \(C_{\mathbf{S}_{4},\mathbf{S}_{1}}Q\), and \(Q^{T}B_{\mathbf{S}_{1},\mathbf{S}_{5}}\), respectively. Then, we will show that the entailed covariance matrix \(\Sigma_{\mathbf{X}}\) is unchanged.

Let \(U\) be an \(m\times m\) orthogonal matrix such that: (i) \(U_{\mathbf{S}_{1},\mathbf{S}_{1}}=Q\), (ii) the remaining diagonal entries are ones, and (iii) the remaining non-diagonal entries are zeros. Let

\[\tilde{A}\coloneqq U^{T}AU,\quad\tilde{B}\coloneqq U^{T}B,\quad\tilde{C} \coloneqq CU,\quad\tilde{D}\coloneqq D,\quad\tilde{\Omega}_{\epsilon_{ \mathbf{L}}}\coloneqq U^{T}\Omega_{\epsilon_{\mathbf{L}}}U=I,\quad\text{and} \quad\tilde{\Omega}_{\epsilon_{\mathbf{X}}}\coloneqq\Omega_{\epsilon_{\mathbf{X}}}.\]By Lemma 3, the matrices above can entail the same covariance matrix \(\Sigma_{\mathbf{X}}\).

By construction of \(U\), left multiplication of \(U^{T}\) on \(B\) only affects \(B_{\mathbf{S}_{1},*}\); specifically, it is equivalent to replacing \(B_{\mathbf{S}_{1},*}\) with \(Q^{T}B_{\mathbf{S}_{1},*}\). Furthermore, only the columns of \(\mathbf{S}_{5}\) in \(B_{\mathbf{S}_{1},*}\), will be affected, because those columns correspond to the measured children of \(\mathbf{L}\). Therefore, all entries of \(\tilde{B}\) are the same as \(B\), except that \(B_{\mathbf{S}_{1},\mathbf{S}_{5}}\) is replaced with \(Q^{T}B_{\mathbf{S}_{1},\mathbf{S}_{5}}\). Similar reasoning shows that all entries of \(\tilde{C}\) are the same as \(C\), except that \(C_{\mathbf{S}_{4},\mathbf{S}_{1}}\) is replaced with \(C_{\mathbf{S}_{4},\mathbf{S}_{1}}Q\).

Now consider \(U^{T}AU\). By the reasoning above, left multiplication of \(U^{T}\) on \(A\) only is equivalent to replacing \(A_{\mathbf{S}_{1},\mathbf{S}_{3}}\) with \(Q^{T}A_{\mathbf{S}_{1},\mathbf{S}_{3}}\). Further right multiplication of \(U\) on \(U^{T}A\) is equivalent to replacing \((U^{T}A)_{\mathbf{S}_{2},\mathbf{S}_{1}}\) with \((U^{T}A)_{\mathbf{S}_{2},\mathbf{S}_{1}}Q\). Since \(\mathbf{S}_{1}\), \(\mathbf{S}_{2}\), and \(\mathbf{S}_{3}\) are mutually disjoint, all entries of \(\tilde{A}=U^{T}AU\) are the same as \(A\), except that \(A_{\mathbf{S}_{2},\mathbf{S}_{1}}\) and \(A_{\mathbf{S}_{1},\mathbf{S}_{3}}\) are replaced with \(A_{\mathbf{S}_{2},\mathbf{S}_{1}}Q\) and \(Q^{T}A_{\mathbf{S}_{1},\mathbf{S}_{3}}\), respectively.

Hence, for matrices \(A,B,C\), and \(D\), suppose we replace \(A_{\mathbf{S}_{2},\mathbf{S}_{1}}\), \(A_{\mathbf{S}_{1},\mathbf{S}_{3}}\), \(C_{\mathbf{S}_{4},\mathbf{S}_{1}}\), and \(B_{\mathbf{S}_{1},\mathbf{S}_{5}}\) with \(A_{\mathbf{S}_{2},\mathbf{S}_{1}}Q\), \(Q^{T}A_{\mathbf{S}_{1},\mathbf{S}_{3}}\), \(C_{\mathbf{S}_{4},\mathbf{S}_{1}}Q\), and \(Q^{T}B_{\mathbf{S}_{1},\mathbf{S}_{5}}\), respectively. By the reasoning above, this is equivalent to replacing \(A\), \(B\), \(C\), and \(D\) with \(\tilde{A}\), \(\tilde{B}\), \(\tilde{C}\), and \(\tilde{D}\), respectively, which (generically) share the same support and entail the same covariance matrix \(\Sigma_{\mathbf{X}}\). 

### Proof of Corollary 1 and Corollary 2

**Corollary 1** (General Necessary Condition for Parameter Identifiability).: _For parameters to be identifiable, every pair of latent variables has to have at least one different parent or child._

Proof of Corollary 1.: Proof by contradiction. If it is not the case that every pair of latent variables has to have at least one different parent or child, then there exist \(\mathbf{L}\) such that \(|\mathbf{L}|\geq 2\) and \(\mathbf{L}\) share the same parents and children. Therefore by Theorem 4 there must exist orthogonal transformation indeterminacy regarding \(F\), and thus the parameters are not identifiable. 

**Corollary 2** (Necessary Condition about Atomic Covers for Parameter Identifiability).: _Assume \(\mathcal{G}\) satisfies Conditions 1 and 2 and thus the structure can be identified up to the MEC of \(\mathcal{O}_{a}(\mathcal{O}_{s}(\mathcal{G}))\). For any DAG \(\mathcal{G}\) in the equivalence class, for \(\mathcal{G}\)'s parameters to be identifiable, every atomic cover must contain no more than one latent variable._

Proof of Corollary 2.: Proof by contradiction. If for a DAG in the equivalence class, there is an atomic cover that has more than one latent variable, then according to the definition of the concerned equivalence class, the latent variables in that atomic cover share the same parents and children. Then by Theorem 4 there must exist orthogonal transformation indeterminacy regarding \(F\), and thus the parameters are not identifiable. 

### Proof of Proposition 1

**Proposition 1** (Parameterization of Population Covariance).: _Consider the model defined in Def. 1. Let \(M\coloneqq\big{(}I-F_{\mathbf{L}\mathbf{L}}-F_{\mathbf{L}\mathbf{X}}(I-F_{ \mathbf{X}\mathbf{X}})^{-1}F_{\mathbf{X}\mathbf{L}}\big{)}^{-1}\) and \(N\coloneqq\big{(}(I-F_{\mathbf{L}\mathbf{L}})F_{\mathbf{X}\mathbf{L}}^{-1}(I- F_{\mathbf{X}\mathbf{X}})-F_{\mathbf{L}\mathbf{X}}\big{)}^{-1}\). Then, the population covariance matrices of \(\mathbf{L}\) and \(\mathbf{X}\) can be formulated as_

\[\Sigma_{\mathbf{L}}= M^{T}\Omega_{\epsilon_{\mathbf{L}}}M+N^{T}\Omega_{\epsilon_{ \mathbf{X}}}N,\] (1) \[\Sigma_{\mathbf{X}}= (I-F_{\mathbf{X}\mathbf{X}})^{-T}\Big{(}F_{\mathbf{L}\mathbf{X} }^{T}\Sigma_{\mathbf{L}_{\mathcal{G}}}F_{\mathbf{L}\mathbf{X}}+\Omega_{\epsilon_ {\mathbf{X}}}+\Omega_{\epsilon_{\mathbf{X}}}NF_{\mathbf{L}\mathbf{X}}+F_{ \mathbf{L}\mathbf{X}}^{T}N^{T}\Omega_{\epsilon_{\mathbf{X}}}\Big{)}(I-F_{ \mathbf{X}\mathbf{X}})^{-1}.\] (2)

Proof of Proposition 1.: Let \(F=\begin{pmatrix}F_{\mathbf{L}\mathbf{L}}&F_{\mathbf{L}\mathbf{X}}\\ F_{\mathbf{X}\mathbf{L}}&F_{\mathbf{X}\mathbf{X}}\end{pmatrix}=\begin{pmatrix}A& B\\ C&D\end{pmatrix}\).

Since matrices \(A\) and \(D\) are invertible, using the formula of \(2\times 2\) block matrix inversion [22, Chapter 0.7], we obtain

\[(I-F)^{-1}=\begin{pmatrix}M&-MB(I-D)^{-1}\\ -(I-D)^{-1}CM&(I-D)^{-1}+(I-D)^{-1}CMB(I-D)^{-1}\end{pmatrix},\]which implies

\[(I-F)^{-T}=\begin{pmatrix}M^{T}&-M^{T}C^{T}(I-D)^{-T}\\ -(I-D)^{-T}B^{T}M^{T}&(I-D)^{-T}+(I-D)^{-T}B^{T}M^{T}C^{T}(I-D)^{-T}\end{pmatrix}\]

and

\[(I-F)^{-T}\Omega=\begin{pmatrix}M^{T}\Omega_{\epsilon_{\mathbf{L}}}&-M^{T}C^{ T}(I-D)^{-T}\Omega_{\epsilon_{\mathbf{X}}}\\ -(I-D)^{-T}B^{T}M^{T}\Omega_{\epsilon_{\mathbf{L}}}&(I-D)^{-T}\Omega_{ \epsilon_{\mathbf{X}}}+(I-D)^{-T}B^{T}M^{T}C^{T}(I-D)^{-T}\Omega_{\epsilon_{ \mathbf{X}}}\end{pmatrix}.\]

We then have

\[\Sigma_{L} =M^{T}\Omega_{\epsilon_{\mathbf{L}}}M+M^{T}C^{T}(I-D)^{-T}\Omega_ {\epsilon_{\mathbf{X}}}(I-D)^{-1}CM\] \[=M^{T}\Omega_{\epsilon_{\mathbf{L}}}M+N^{T}\Omega_{\epsilon_{ \mathbf{X}}}N\]

and

\[\Sigma_{X} =(I-D)^{-T}B^{T}M^{T}\Omega_{\epsilon_{\mathbf{L}}}MB(I-D)^{-1}+( I-D)^{-T}\Omega_{\epsilon_{\mathbf{X}}}(I-D)^{-1}\] \[\quad+(I-D)^{-1}\Omega_{\epsilon_{\mathbf{X}}}(I-D)^{-1}CMB(I-D) ^{-1}+(I-D)^{-T}B^{T}M^{T}C^{T}(I-D)^{-T}\Omega_{\epsilon_{\mathbf{X}}}(I-D)^ {-1}\] \[\quad+(I-D)^{-T}B^{T}M^{T}C^{T}(I-D)^{-T}\Omega_{\epsilon_{ \mathbf{X}}}(I-D)^{-1}CMB(I-D)^{-1}\] \[=(I-D)^{-T}\Big{(}\Omega_{\epsilon_{\mathbf{X}}}+B^{T}M^{T} \Omega_{\epsilon_{\mathbf{L}}}MB+\Omega_{\epsilon_{\mathbf{X}}}(I-D)^{-1}CMB+ B^{T}M^{T}C^{T}(I-D)^{-T}\Omega_{\epsilon_{\mathbf{X}}}\] \[\quad\quad\quad\quad\quad\quad\quad+B^{T}M^{T}C^{T}(I-D)^{-T} \Omega_{\epsilon_{\mathbf{X}}}(I-D)^{-1}CMB\Big{)}(I-D)^{-1}\] \[=(I-D)^{-T}\Big{(}\Omega_{\epsilon_{\mathbf{X}}}+B^{T}\Sigma_{ \mathbf{L}}B+\Omega_{\epsilon_{\mathbf{X}}}NB+B^{T}N^{T}\Omega_{\epsilon_{ \mathbf{X}}}\Big{)}(I-D)^{-1}.\]

We now discuss how \(M\) and \(N\) defined in Proposition 1 are invertible. Note that matrices \(I-D\) and \(I-F\) are invertible because structure \(\mathcal{G}\) is acyclic. This implies \(\det(I-F)\neq 0\) and \(\det(I-D)\neq 0\). Define

\[U=\begin{pmatrix}I&0\\ -(I-D)^{-1}C&I\end{pmatrix},\]

which implies

\[(I-F)U=\begin{pmatrix}M&B\\ 0&I-D\end{pmatrix}\]

and thus

\[\det((I-F)U)=\det(M)\det(I-D).\]

Since \(\det(U)=1\) and \(\det(I-F)\neq 0\), we have

\[\det((I-F)U)=\det(I-F)\det(U)\neq 0.\]

By the statement above and \(\det(I-D)\neq 0\), we have

\[\det(M)=\frac{\det((I-F)U)}{\det(I-D)}\neq 0,\]

which implies that \(M\) is invertible. Similar reasoning can be used to show that \(N\) is invertible.

### Computational Cost of Checking Whether the Conditions in Theorem 3 Hold

Here we want to investigate, given a structure, can we efficiently check whether the proposed sufficient conditions hold? To this end, we generate random graphs and each graph has 100 variables. According to our empirical result, such a check can be done very efficiently. Specifically, on average, given a structure with 100 variables, it only takes our Python code around 3 seconds to check whether the conditions hold.

### In Practice, What If the Conditions Do not Hold?

Our condition is useful in solving real-life problems. For example, in the psychometric study, we can properly design the questions with domain knowledge following the condition in Theorem 3 such that each single latent variable has enough observed variables as pure children and thus it can be ensured that all parameters are identifiable (as illustrated in our real-life data result in Figure 4).

On the other hand, even though sometimes the questionnaires and data were designed not so well such that the conditions are not satisfied for the identification of parameters, our Theorem 3 is still useful. In this case, we can still make use of our conditions to check the given structure, and find some local sub-structures where our conditions are satisfied. Consequently, it can be ensured that all the parameters of some sub-structures are identifiable, and we can employ our estimation method to find all the edge coefficients of these sub-structures.

## Appendix B Additional Definitions, Graphs, Results, and Examples

### Example of Treks

**Example 7** (Example of Treks).: _In Figure 7 (a), there are four treks between \(\mathsf{X}_{4}\) and \(\mathsf{X}_{5}\): (i) \(\mathsf{X}_{4}\leftarrow\mathsf{L}_{1}\rightarrow\mathsf{X}_{5}\), (ii) \(\mathsf{X}_{4}\leftarrow\mathsf{X}_{3}\rightarrow\mathsf{X}_{5}\), (iii) \(\mathsf{X}_{4}\leftarrow\mathsf{L}_{1}\leftarrow\mathsf{X}_{2}\rightarrow \mathsf{X}_{3}\rightarrow\mathsf{X}_{5}\), and (iv) \(\mathsf{X}_{4}\leftarrow\mathsf{X}_{3}\leftarrow\mathsf{X}_{2}\rightarrow \mathsf{L}_{1}\rightarrow\mathsf{X}_{5}\), illustrated in Figure 7 (b)._

### Definition of Pure Children

**Definition 6** (Parents, Children, and Descendants of a Set of Nodes [18]).: _For a set of nodes \(\mathbf{X}\) in \(\mathcal{G}\), we have \(\text{Ch}_{\mathcal{G}}(\mathbf{X})=\cup_{\mathbf{X}\in\mathbf{X}}\text{Ch}_{ \mathcal{G}}(\mathbf{X})\), \(\text{Pa}_{\mathcal{G}}(\mathbf{X})=\cup_{\mathbf{X}\in\mathbf{X}}\text{Pa}_{ \mathcal{G}}(\mathbf{X})\), and \(\text{De}_{\mathcal{G}}(\mathbf{X})=\cup_{\mathbf{X}\in\mathbf{X}}\text{De}_{ \mathcal{G}}(\mathbf{X})\)._

**Definition 7** (Pure Children of a Set of Nodes [18]).: \(\mathbf{Y}\) _are pure children of a set of nodes \(\mathbf{X}\) in graph \(\mathcal{G}\), i.e., \(\mathbf{Y}\in\text{P}\text{Ch}_{\mathcal{G}}(\mathbf{X})\), iff all of the following hold: (i) \(\mathbf{X}\cap\mathbf{Y}=\emptyset\), (ii) \(\mathbf{Y}\subseteq\text{Ch}_{\mathcal{G}}(\mathbf{X})\), (iii) \(\text{Pa}_{\mathcal{G}}(\mathbf{Y})=\mathbf{X}\), and (iv) \(\text{De}_{\mathcal{G}}(\mathbf{Y})\cap\mathbf{X}=\emptyset\)._

### Definition of Neighbor and MEC

**Definition 8** (Neighbor).: _In \(\mathcal{G}\), nodes \(\mathsf{X}\) and \(\mathsf{Y}\) are neighbor of each other iff there exist an edge from \(\mathsf{X}\) to \(\mathsf{Y}\) or an edge from \(\mathsf{Y}\) to \(\mathsf{X}\)._

**Definition 9** (Markov Equivalence Class (MEC)).: _Two DAGS \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) belong to the same MEC, if they share the same skeleton and v-structures._

### Definition of Rank-invariant Graph Operator

The definitions are as follows with examples.

**Definition 10** (Skeleton Operator [18]).: _Given an atomic cover \(\mathbf{V}\) in a graph \(\mathcal{G}\), and let \(\mathcal{S}\) be the set of all atomic covers in \(\mathcal{G}\) such that for all \(\mathbf{S}\in\mathcal{S}\), \(\mathbf{S}\subset\mathbf{V}\). For all \(\mathsf{V}_{1}\in\mathbf{V}\) and all \(\mathsf{V}_{2}\in\text{P}\text{CH}_{\mathcal{G}}(\mathbf{V})\backslash(\bigcup_ {\mathbf{S}\in\mathcal{S}}\text{P}\text{CH}_{\mathcal{G}}(\mathbf{S}))\), if \(\mathsf{V}_{1}\) and \(\mathsf{V}_{2}\) are not adjacent, draw an edge from \(\mathsf{V}_{1}\) to \(\mathsf{V}_{2}\). We denote such an operator as skeleton operator \(\mathcal{O}_{s}(\mathcal{G})\)._

**Definition 11** (Intra atomic operator).: _For every atomic cover \(\mathbf{V}\) in structure \(\mathcal{G}\), if \(|\mathbf{V}|\geq 2\), we add edges between elements in \(\mathbf{V}\) such that edges among \(\mathbf{V}\) form a fully connected DAG. We denote such an operator as intra atomic operator \(\mathcal{O}_{a}(\mathcal{G})\)._

**Example 8**.: _Suppose the original graph is in Figure 9 (a). After the skeleton operator, we have \(\mathcal{O}_{s}(\mathcal{G})\), which is shown in Figure 9 (b). After the intro atomic operator, we have \(\mathcal{O}_{a}(\mathcal{O}_{s}(\mathcal{G}))\), which is shown in Figure 9 (c)._

### Graphs for Synthetic Data Experiments

Please refer to Figures 10 and 11.

### Additional Result under Model Misspecification

Please refer to Tables 2and 3.

## Appendix C Other Discussions

Our optimization problem in Eq. (4) is solved by gradient descent using PyTorch. Our current implementation is based on CPU but it can be further accelerated by using GPU. A very related discussion can also be found in [35].

The optimization problem in Eq. (5) is solved by gradient descent, which involves evaluating the LogDet and matrix inverse (for the gradient) terms (which is similar to continuous causal discovery methods based on Gaussian likelihood [35]). According to [53], the computational complexity is \(O(td^{3})\), where \(d\) is the number of variables and \(t\) is the number of iterations of gradient descent respectively. Note that the computational cost is largely independent of the sample size as we only need to calculate the sample covariance once and save it for further use.

It is possible to perform inference on the learned parameters in our framework. To be specific, as we use maximum likelihood estimation for the parameters, some standard techniques can be readily used. For example, bootstrap can be employed to provide standard errors on linear coefficients and Chi-square test can also be done to examine the fitness of the model.

## Appendix D Extended Related Work

One main line of research in latent variable estimation centers on factor-analysis-based methods. Representative studies include [41, 45, 36, 56, 7, 57]. Various other techniques have also been employed for latent structure and parameter identification, including over-complete ICA-based techniques [23, 43, 1] that leverage non-Gaussianity and matrix decomposition-based approaches [3]. However, these approaches typically consider latent variables with observed children, without considering parameter identification in latent hierarchical structures. A more related work is [4], but it considers a much simpler structure.

Another direction would be to project the graph to an ADMG and the latent confounding effects are encoded by correlated noise terms. Following this idea, graphical criteria such as half-trek [20, 5], G-criterion [9], and some further developments [51, 29, 42, 8, 19] has been proposed. Furthermore, another line of works involve studies on causal effect estimation in the presence of latent confounders [52, 6, 30, 33, 26], which often rely on instrumental variables or proxy variables for identification. Notice that in this task, the parameters may not be identified [30], although the causal effect from the treatment variable to the outcome variable can be identified.

Figure 8: An illustrative graph to show orthogonal transformation indeterminacy. An atomic cover of it, \(\{\mathsf{L}_{1},\mathsf{L}_{2}\}\), has more than one latent variable, and thus there exists orthogonal transformation indeterminacy regarding coefficients of edges that involve \(\{\mathsf{L}_{1},\mathsf{L}_{2}\}\).

Figure 7: Illustrative figure to show how to parameterize \(\Sigma_{\mathbf{X}_{\mathcal{G}}}\) by the use of simple trek rule.

Furthermore, several existing works also solve an optimization problem that involves parameterization of maximum likelihood, such as those in continuous optimization for causal discovery [35; 59; 31; 10; 34] and parameter estimation of Lyapunov models [55; 15]. Differently, our formulation involving likelihood parameterization aims to estimate parameters of partially observed linear causal models.

## Appendix E Limitations

One limitation of this work is that our theoretical results are based on the assumption of linear gaussian causal models. When data is not linear gaussian, we have also conducted experiments to see the performance of our method. It turns out that our method still performs well in the presence of certain extents of violation of normality and linearity. However, theoretical analysis under violation of linearity and normality would be interesting and the focus of future work.

## Appendix F Broader Impacts

The goal of this paper is to advance the field of machine learning. We do not see any potential negative societal impacts of the work.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline \multicolumn{2}{|c|}{Metric} & \multicolumn{2}{c|}{MSE up to group sign} \\ \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Estimator} & \multicolumn{2}{c|}{Estimator-TR} \\ \cline{2-4}  & 2k & 0.0017 & 0.0005 \\ \hline \multirow{2}{*}{GS Case} & 5k & 0.0018 & 0.0004 \\ \cline{2-4}  & 10k & 0.0018 & 0.0003 \\ \hline \end{tabular}
\end{table}
Table 2: Performance under violation of normality using uniform noise terms in MSE (mean (std)).

Figure 9: Example to illustrate graph operators \(\mathcal{O}_{\text{a}}\) and \(\mathcal{O}_{\text{s}}\).

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Metric & \multicolumn{2}{c|}{MSE up to group sign} \\ \hline Method & \multicolumn{2}{c|}{Estimator} & Estimator-TR \\ \hline \multirow{3}{*}{GS Case with 10k sample size} & \(\alpha=0.8\) (close to linear) & 0.004 & 0.001 \\ \cline{2-4}  & \(\alpha=0.6\) (quite nonlinear) & 0.013 & 0.005 \\ \cline{2-4}  & \(\alpha=0.3\) (very nonlinear) & 0.046 & 0.027 \\ \hline \end{tabular}
\end{table}
Table 3: Performance under violation of linearity using leaky relu in MSE (mean (std)).

Figure 10: Examples of graphs in the GS case. The parameters of them are identifiable up to group sign indeterminacy.

Figure 11: Examples of graphs in the OT case. Parameters of them are Identifiable up to group sign and orthogonal transformation indeterminacy.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: the claims stated in the abstract and introduction are the same as what are stated in the theory and method part. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: A discussion about the limitations can be found in Appendix E. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All assumptions together with necessary definitions are provided in the main text and all complete proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The implementation details and the experimental settings are all provided in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: The code and the data will be publicly available. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See the experiment section for the setting and details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The synthetic experiments were supported by error bars to show statistical significance. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix F. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: It is not related to this work. No such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the assets are properly credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.