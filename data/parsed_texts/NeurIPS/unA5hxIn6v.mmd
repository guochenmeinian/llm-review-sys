# Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input

Ziang Chen

Department of Mathematics

Massachusetts Institute of Technology

Cambridge, MA 02139

ziang@mit.edu

&Rong Ge

Department of Computer Science and Department of Mathematics

Duke University

Durham, NC 27708

rongge@cs.duke.edu

###### Abstract

In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We establish a necessary condition for SGD-learnability, involving both the characteristics of the target function and the expressiveness of the activation function. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.

## 1 Introduction

Neural Networks (NNs) are powerful in practice to approximate mappings on certain data structures, such as Conventional Neural Networks (CNNs) for image data, Graph Neural Networks (GNNs) for graph data, and Recurrent Neural Networks (RNNs) for sequential data, stimulating numerous breakthroughs in application of machine learning in many branches of science, engineering, etc. The surprising performance of neural networks is often explained by arguing that neural networks automatically learns useful representations of the data. However, how simple training procedures such as stochastic gradient descent (SGD) extract features remains a major open problem.

Optimization of neural networks has received lots of attention. For simpler networks such as linear neural networks, local minima are also globally optimal [14; 15; 17]. However, this is not true for nonlinear networks even of depth 2 [23]. Neural Tangent Kernel (NTK, [4; 12; 13]) is a line of work that establishes strong convergence results for wide neural networks. However, in the NTK regime, neural network is equivalent to a kernel, which cannot learn useful features based on the target function. Such limitation prevents neural networks in NTK regime from efficiently learning even simple single index models [28].

As an alternative, the behavior of SGD can also be understood via mean-field analysis, for both two-layer neural networks [9; 19; 20; 22; 24; 25] and multi-layer neural networks [5; 21; 22]. Neural networks in the mean-field regime have the potential to do feature learning. Recently, [1] showed an interesting setup where a two-layer neural network can learn representations if the target function satisfies a merged-staircase property. More precisely, [1] considers a sparse polynomial as apolynomial \(f^{*}:\mathbb{R}^{d}\to\mathbb{R}\) defined on the hypercube \(\{-1,1\}^{d}\), i.e., \(f^{*}(x)=h^{*}(z)=h^{*}(x_{I})\) where \(z=x_{I}=(x_{i})_{i\in I}\), \(I\) is an unknown subset of \(\{1,2,\ldots,d\}\) with \(|I|=p\), and \(h^{*}:\{-1,1\}^{p}\to\mathbb{R}\) is a function on the subset of coordinates in \(I\). They prove that a condition called the merged-staircase property is necessary and in some sense sufficient for learning such \(f^{*}\) using SGD and two-layer neural networks. The merged-staircase property proposed in [1] states that all monomials of \(h^{*}\) can be ordered such that each monomial contains at most one \(z_{i}\) that does not appear in any previous monomial. For example, \(h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}\) satisfies the merged-staircase property while \(h^{*}(z)=z_{1}+z_{1}z_{2}z_{3}\) does not. Results on similar structures can also be found in [3]. The work [2] proposes the concept of leap complexity and generalizes the results in [1] to a larger family of sparse polynomials.

In this work, we consider "subspace-sparse" polynomial that is more general. Concretely, let \(f^{*}(x)=h^{*}(z)=h^{*}(x_{V})\), where \(V\) is a subspace of \(\mathbb{R}^{d}\) with \(\text{dim}(V)=p\ll d\), \(x_{V}\) is the orthogonal projection of \(x\) onto the subspace \(V\), and \(h^{*}:V\to\mathbb{R}\) is an underlying polynomial map. In other words, the sparsity is in the sense that \(f^{*}(x)\) only depends on the projection of the input \(x\in\mathbb{R}^{d}\) in a low-dimensional subspace. Throughout this paper, the input data distribution is the standard \(d\)-dimensional normal distribution, i.e., \(x\ \sim\mathcal{N}(0,I_{d})\), which is rotation-invariant in the sense that \(Ox\sim\mathcal{N}(0,I_{d})\) for any orthogonal matrix \(O\in\mathbb{R}^{d\times d}\). Similar rotation-invariant/basis-free settings are also considered in some recent studies, including [2, 8, 10, 11].

Our contribution and related worksOur first contribution is a basis-free necessary condition for SGD-learnability. More specially, we propose the reflective property of the underlying polynomial \(h^{*}:V\to\mathbb{R}\) with respect to some subspace \(S\subset V\), which also involves the expressiveness of the activation function. We prove that as long as the reflective property is satisfied with respect to nontrivial \(S\), the training dynamics cannot learn any information about the behavior of \(h^{*}\) on \(S\) (see Theorem 3.4). Therefore the loss functional will be bounded away from 0 during the whole training procedure.

One key point is that our reflective property precisely characterizes the necessary expressiveness of the activation function. If the activation function is expressive enough, the reflective property equivalently recovers a necessary condition characterized by isoLeap [2] that is the maximal leap complexity over all orthonormal basis and can be viewed as a basis-free generalization of the merged-staircase property. This also indicates that our necessary condition is a bit weaker. Other related rotation-invariant conditions in the previous literature include leap exponent/index [8, 10], subspace conditioning [10] and even-symmetric directions [11]. The analysis in [10, 11] is for training the first layer for finitely many iterations with fixed second-layer, and [8] studies the joint learning dynamics where they assume that for any fixed first layer, the optimal parameters in second layer can be found efficiently and reformulate the loss as a function of the first layer. Differently and more generally, our analysis for the necessary condition does not require specific learning strategies and works for any learning rates satisfying some mild conditions.

Our second contribution is a sufficient condition for SGD-learnability that is also basis-free and is slightly stronger than the necessary condition. In particular, we show that if the training dynamics cannot be trapped in any proper subspace of \(V\), then one can choose the initial parameter distribution and the learning rate such that the loss functional decays to zero exponentially fast with dimension-free rates (see Theorem 4.3). Our training strategy is inspired by [1] with the difference that we take the average of \(p\) independent training trajectories, which can lift some linear independence property required for polynomials on hypercube to algebraic independence in the general polynomial setting.

Technical challengesIt may seem simple to leave the standard basis and generalize the results of [1, 2] to learn subspaces, because SGD itself is independent of the basis, and we can consider a symmetric Gaussian input distribution. However, there are some significant barriers that motivated our training process. The condition and the analysis in [1, 2] rely on an orthonormal basis of the input space \(\mathbb{R}^{d}\). This is natural for polynomials on the hypercube \(\{-1,1\}^{d}\), but not for general polynomials on \(\mathbb{R}^{d}\). Particularly, their theory does not work for Gaussian input data \(x\sim\mathcal{N}(0,I_{d})\), which is probably the most common distribution in data science, unless an orthonormal basis of \(\mathbb{R}^{d}\) is specified and \(V\) is known to be spanned by \(p\) elements in the basis. In this work, we consider a more general setting in which specifying a basis is not required and the space \(V\) can be any \(p\)-dimensional subspace of \(\mathbb{R}^{d}\). This setting is consistent with the rotation-invariant property of \(\mathcal{N}(0,I_{d})\) and introduces more difficulties since less knowledge of \(V\) is available prior to training.

OrganizationThe rest of this paper will be organized as follows. We introduce some preliminaries on mean-field dynamics in Section 2. The basis-free necessary and sufficient conditions for SGD-learnability are discussed in Section 3 and Section 4, respectively. We conclude in Section 5.

## 2 Preliminaries on Mean-Field Dynamics

The mean-field dynamics describes the limiting behavior of the training procedure when the step-size/learning rate converges to zero, i.e., the evolution of a neuron converges to the solution of a differential equation with continuous time, and when the number of neurons converges to infinity, i.e., the empirical distribution of all neurons converges to some limiting probability distribution. For two-layer neural networks, some quantitative results are established in [20] that characterize the distance between the SGD trajectory and the mean-field evolution flow, and these results are further improved as dimension-free in [19]. Such results suggest that analyzing the mean-field flow is sufficient for understanding the SGD trajectory in some settings. In this section, we briefly review the setup of two-layer neural networks, SGD, and their mean-field versions, following [19; 20].

Two-layer neural network and SGDThe two-layer neural network is of the following form:

\[f_{\text{NN}}(x;\Theta):=\frac{1}{N}\sum_{i=1}^{N}\tau(x;\theta_{i})=\frac{1} {N}\sum_{i=1}^{N}a_{i}\sigma(w_{i}^{\top}x),\] (2.1)

where \(N\) is the number of neurons, \(\Theta=(\theta_{1},\theta_{2},\ldots,\theta_{N})\) with \(\theta_{i}=(a_{i},w_{i})\in\mathbb{R}^{d+1}\) is the set of parameters, and \(\sigma:\mathbb{R}\to\mathbb{R}\) is the activation functions with \(\tau(x;\theta):=a\sigma(w^{\top}x)\) for \(\theta=(a,w)\). Then the task is to find some parameter \(\Theta\) such that the \(\ell_{2}\)-distance between \(f^{*}\) and \(f_{\text{NN}}\) is minimized:

\[\min_{\Theta}\,\mathcal{E}_{N}(\Theta):=\frac{1}{2}\mathbb{E}_{x\sim\mathcal{ N}(0,I_{d})}\left[|f^{*}(x)-f_{\text{NN}}(x;\Theta)|^{2}\right].\] (2.2)

In practice, a widely used algorithm for solving (2.2) is the stochastic gradient descent (SGD) that iterates as

\[\theta_{i}^{(k+1)}=\theta_{i}^{(k)}+\gamma^{(k)}\left(f^{*}(x_{k})-f_{\text{ NN}}(x_{k};\Theta^{(k)})\right)\nabla_{\theta}\tau(x_{k};\theta_{i}^{(k)}),\] (2.3)

where \(x_{k},\ k=1,2,\ldots\) are the i.i.d. samples drawn from \(\mathcal{N}(0,I_{d})\) and \(\gamma^{(k)}=\text{diag}(\gamma_{a}^{(k)},\gamma_{w}^{(k)}I_{d})\succeq 0\) is the stepsize or the learning rate. In this paper, we only consider the one-pass model with each data point being used exactly once, following [19].

Mean-field dynamicsOne can generalize (2.1) to an infinite-width two-layer neural network:

\[f_{\text{NN}}(x;\rho):=\int\tau(x;\theta)\rho(d\theta)=\int a\sigma(w^{\top} x)\rho(da,dw),\]

where \(\rho\in\mathcal{P}(\mathbb{R}^{d+1})\) is a probability measure on the parameter space \(\mathbb{R}^{d+1}\), and generalize the loss/energy functional (2.2) to

\[\mathcal{E}(\rho):=\frac{1}{2}\mathbb{E}_{x\sim\mathcal{N}(0,I_{d})}\left[|f^ {*}(x)-f_{\text{NN}}(x;\rho)|^{2}\right].\]

We will use \(\mathcal{P}(X)\) to denote the collection of probability measures on a space \(X\) throughout this paper. The limiting behavior of the SGD trajectory (2.3) when \(\gamma^{(k)}\to 0\) and \(N\to\infty\) can be described by the following mean-field dynamics:

\[\begin{cases}\partial_{t}\rho_{t}=\nabla_{\theta}\cdot\left(\rho_{t}\xi(t) \nabla_{\theta}\Phi(\theta;\rho_{t})\right),\\ \rho_{t}\big{|}_{t=0}=\rho_{0},\end{cases}\] (2.4)

where \(\xi(t)=\text{diag}(\xi_{a}(t),\xi_{w}(t)I_{d})\in\mathbb{R}^{(d+1)\times(d+1)}\) with \(\xi_{a}(t)\geq 0\) and \(\xi_{w}(t)\geq 0\) being the learning rates and

\[\Phi(\theta;\rho)=a\mathbb{E}_{x\sim\mathcal{N}(0,I_{d})}\left[\left(f_{\text{ NN}}(x;\rho)-f^{*}(x)\right)\sigma(w^{\top}x)\right].\]

One can also write \(\Phi(\theta;\rho)\) as

\[\Phi(\theta;\rho)=V(\theta)+\int U(\theta,\theta^{\prime})\rho(d\theta^{ \prime}),\]where

\[V(\theta)=-a\mathbb{E}_{x}\left[f^{*}(x)\sigma(w^{\top}x)\right]\quad\text{and} \quad U(\theta,\theta^{\prime})=aa^{\prime}\mathbb{E}_{x}\left[\sigma(w^{\top}x) \sigma((w^{\prime})^{\top}x)\right].\] (2.5)

The PDE (2.4) is understood in the weak sense, i.e., \(\rho_{t}\) is a solution to (2.4) if and only if \(\rho_{t}\big{|}_{t=0}=\rho_{0}\) and

\[\iint\left(-\partial_{t}\eta+\nabla_{\theta}\eta\cdot\left(\xi(t)\nabla_{ \theta}\Phi(\theta;\rho_{t})\right)\right)\rho_{t}(d\theta)dt=0,\quad\forall \;\eta\in\mathcal{C}_{c}^{\infty}(\mathbb{R}^{d+1}\times(0,+\infty)),\]

where \(\mathcal{C}_{c}^{\infty}(\mathbb{R}^{d+1}\times(0,+\infty))\) is the collection of all smooth and compactly supported functions on \(\mathbb{R}^{d+1}\times(0,+\infty)\). It can also be computed that the energy functional is non-increasing along \(\rho_{t}\):

\[\frac{d}{dt}\mathcal{E}(\rho_{t})=-\int\nabla_{\theta}\Phi(\theta;\rho_{t})^{ \top}\xi(t)\nabla_{\theta}\Phi(\theta;\rho_{t})\rho_{t}(d\theta)\leq 0.\] (2.6)

There have been standard results in the existing literature that provide dimension-free bounds for the distance between the empirical distribution of the parameters generalized by (2.3) and the solution to (2.4). For the simplicity of reading, we will not present those results and the proof; interested readers are referred to [19]. In the rest of this paper, we will focus on the analysis of (2.4) and briefly discuss the sample complexity results implied by our mean-field analysis.

## 3 Necessary Condition for SGD-Learnability

This section introduces a condition that can prevent SGD from recovering all information about \(f^{*}\), or in other words, prevent the loss functional \(\mathcal{E}(\rho_{t})\) decaying to a value sufficiently close to \(0\).

### Reflective Property

Before rigorously presenting our main theorem, we state the assumptions used in this section.

**Assumption 3.1**.: _Assume that the followings hold:_

1. _The activation function_ \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) _is twice continuously differentiable with_ \(\|\sigma\|_{L^{\infty}(\mathbb{R})}\leq K_{\sigma}\)_,_ \(\|\sigma^{\prime}\|_{L^{\infty}(\mathbb{R})}\leq K_{\sigma}\)_, and_ \(\|\sigma^{\prime\prime}\|_{L^{\infty}(\mathbb{R})}\leq K_{\sigma}\) _for some constant_ \(K_{\sigma}>0\)_._
2. _The learning rates_ \(\xi_{a},\xi_{w}:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}\) _satisfy that_ \(\|\xi_{a}\|_{L^{\infty}(\mathbb{R}_{\geq 0})}\leq K_{\xi}\) _and_ \(\|\xi_{w}\|_{L^{\infty}(\mathbb{R}_{\geq 0})}\leq K_{\xi}\) _for some constant_ \(K_{\xi}>0\)_. Furthermore,_ \(\xi_{a}\) _and_ \(\xi_{w}\) _are Lipschitz continuous with_ \(\int_{0}^{+\infty}\xi_{a}(t)dt=+\infty\) _and_ \(\int_{0}^{+\infty}\xi_{w}(t)dt=+\infty\)_._
3. _The initialization is_ \(\rho_{0}=\rho_{a}\times\rho_{w}\) _such that_ \(\rho_{a}\) _is symmetric and is supported in_ \([-K_{\rho},K_{\rho}]\) _for some constant_ \(K_{\rho}>0\)_._

In Assumption 3.1, the Condition (i) is satisfied by some commonly used activation functions, such as \(\sigma(x)=\frac{1}{1+e^{-x}}\) and \(\sigma(x)=\cos(x)\), and is required for establishing the existence and uniqueness of the solution to (2.4). The Condition (ii) and (iii) are also standard and easy to satisfy in practice.

**Remark 3.2**.: _The symmetry of \(\rho_{a}\) implies that \(f_{\text{NN}}(x;\rho_{0})=0\). Therefore, the initial loss \(\mathcal{E}(\rho_{0})=\frac{1}{2}\mathbb{E}_{x}[|f^{*}(x)|^{2}]=\frac{1}{2} \mathbb{E}_{x_{V}}[|h^{*}(x_{V})|^{2}]=\frac{1}{2}\mathbb{E}_{z}[|h^{*}(z)|^{2}]\), where \(x_{V}=z\sim\mathcal{N}(0,I_{V})\), can be viewed as a constant depending only on \(h^{*}\) and \(p\), independent of \(d\). Noticing also the decay property (2.6), the loss at any time \(t\) can be bounded as \(\mathcal{E}(\rho_{t})\leq\mathcal{E}(\rho_{0})=\frac{1}{2}\mathbb{E}_{z}[|h^{* }(z)|^{2}]\)._

The main goal of this section is to generalize the merged-staircase property in a basis-free setting for general polynomials. Without a standard basis, it is hard to talk about having a "staircase" of monomials. Even with a fixed basis, it is still nontrial to define the merged-staircase property for general polynomials since the analysis in [1] highly depends on \(z_{i}^{2}=1\) that is only true for polynomials on the hypercube. Instead, we use the observation that when a function does not satisfy the merged staircase property, it implies that two of the variables will behave the same in the training dynamics. Such a symmetry can be generalized to the basis-free setting for general polynomials and we summarize this as the following reflective property:

**Definition 3.3** (Reflective property).: _Let \(S\subset V\subset\mathbb{R}^{d}\) be a subspace of \(V\). We say that the underlying polynomial \(h^{*}:V\to\mathbb{R}\) satisfies the reflective property with respect to the subspace \(S\) and the activation function \(\sigma\) if_

\[\mathbb{E}_{z\sim\mathcal{N}(0,I_{V})}\left[h^{*}(z)\sigma^{\prime}\left(u+v^{ \top}z_{S}^{\perp}\right)z_{S}\right]=0,\quad\forall\ u\in\mathbb{R},\ v\in V,\] (3.1)

_where \(z_{S}=\mathcal{P}_{S}^{V}(z)\) and \(z_{S}^{\perp}=z-\mathcal{P}_{S}^{V}(z)\), with \(\mathcal{P}_{S}^{V}:V\to S\) being the orthogonal projection from \(V\) onto \(S\)._

The reflective property defined above is closely related to the merged-staircase property in [1]. Let us illustrate the intuition using a simple example. Consider \(V=\mathbb{R}^{3}\) and \(h^{*}(z)=z_{1}+z_{1}z_{2}z_{3}\). Then \(h^{*}\) does not satisfy the merged-staircase property since \(z_{1}z_{2}z_{3}\) involves two new coordinates that do not appear in the first monomial \(z_{1}\). In our setting, this \(h^{*}\) satisfies the reflective with respect to \(S=\text{span}\{e_{2},e_{3}\}\), where \(e_{i}\) is the vector in \(\mathbb{R}^{3}\) with the \(i\)-th entry being \(1\) and other entries being \(0\). More specifically, for \(z=(z_{1},z_{2},z_{3})\), one has that \(z_{S}=(0,z_{2},z_{3})\) and \(z_{S}^{\perp}=(z_{1},0,0)\). Thus, one has for any \(u\in\mathbb{R}\) and \(v\in V\) that \(\sigma^{\prime}\left(u+v^{\top}z_{S}^{\perp}\right)\) is independent of \(z_{2},z_{3}\) and that

\[\mathbb{E}_{z_{2},z_{3}}\left[h^{*}(z)\sigma^{\prime}\left(u+v^{ \top}z_{S}^{\perp}\right)z_{S}\right] =\sigma^{\prime}\left(u+v^{\top}z_{S}^{\perp}\right)\mathbb{E}_{z _{2},z_{3}}\left[\left(0,z_{1}z_{2}+z_{1}z_{2}^{2}z_{3},z_{1}z_{3}+z_{1}z_{2}z _{3}^{2}\right)\right]\] \[=(0,0,0),\]

which leads to (3.1). One can see from this example that satisfying the reflective property with respect to a nontrivial subspace \(S\subset V\) is in the same spirit as not satisfying the merged-staircase property. Furthermore, the reflective property is rotation-invariant, meaning that using a different orthonormal basis does not change the property. In this sense, our proposed condition is more general than that in [1]. We also remark that there have been other rotation-invariant conditions generalizing [1], see e.g., [2, 8, 10, 11].

Another comment is that the reflective property (3.1) depends on the activation function \(\sigma\), while conditions in previous works [1, 2, 8, 10, 11] are all defined for the target function \(f^{*}\) or \(h^{*}\) itself. There does exist a variant of our reflective property that is independent of \(\sigma^{\prime}\), namely,

\[\mathbb{E}_{z_{S}\sim\mathcal{N}(0,I_{S})}[h^{*}(z)z_{S}]=0,\quad\forall\ z_{S}^{ \perp},\] (3.2)

which actually implies (3.1). But these two conditions are different: \(h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}\) does not satisfy (3.2) but still satisfies (3.1) if \(\sigma(\zeta)=\zeta\). We use (3.1) with \(\sigma^{\prime}\) because we want to emphasize that the SGD learnability depends on the activation function \(\sigma\). If \(\sigma\) is less expressive, then SGD may not learn the target function even if \(h^{*}\) itself satisfies the merged-staircase property. Typically people use activation functions that are expressive enough, for which (3.1) and (3.2) are similar. In addition, it can be verified that (3.2) with some nontrivial \(S\) is equivalent to \(\text{isoLeap}(h^{*})\geq 2\) that means \(h^{*}:V\to\mathbb{R}\) does not satisfy the merged-staircase property for some orthonormal basis of \(V\)[2], and the idea of leaps is used in [8, 10]. We include the proof of equivalence in Appendix B.1.

Our main result in this section is that the reflective property with nontrivial \(S\) would lead to a positive lower bound of \(\mathcal{E}(\rho_{t})\) along the training dynamics, which provides a necessary condition for the SGD-learnability and is formally stated as follows.

**Theorem 3.4**.: _Suppose that Assumption 3.1 holds with \(\rho_{w}\sim\mathcal{N}(0,\frac{1}{d}I_{d})\), and that \(h^{*}:V\to\mathbb{R}\) satisfies the reflective property with respect to some subspace \(S\subset V\) and activation function \(\sigma\). Then for any \(T>0\), there exists a constant \(C>0\) depending only on \(p\), \(h^{*}\), \(K_{\sigma}\), \(K_{\xi}\), \(K_{\rho}\), and \(T\), such that_

\[\inf_{0\leq t\leq T}\mathcal{E}(\rho_{t})\geq\frac{1}{2}\mathbb{E}_{z\sim \mathcal{N}(0,I_{V})}\left[|h^{*}(z)-h^{*}_{S^{\perp}}(z_{S}^{\perp})|^{2} \right]-\frac{C}{d^{1/2}},\] (3.3)

_where \(h^{*}_{S^{\perp}}(z_{S}^{\perp})=\mathbb{E}_{z_{S}}[h^{*}(z)]\). In particular, if \(h^{*}(z)\) is not independent of \(z_{S}\), then for any \(T>0\), there exists \(d(T)>0\) depending only on \(p\), \(h^{*}\), \(K_{\sigma}\), \(K_{\xi}\), \(K_{\rho}\), and \(T\), such that for any \(d>d(T)\), we have_

\[\inf_{0\leq t\leq T}\mathcal{E}(\rho_{t})\geq\frac{1}{4}\mathbb{E}_{z\sim \mathcal{N}(0,I_{V})}\left[|h^{*}(z)-h^{*}_{S^{\perp}}(z_{S}^{\perp})|^{2} \right]>0.\] (3.4)

It is worth remarking that in Theorem 3.4, the training time \(T\) is a constant independent of the dimension \(d\). If a longer \(d\)-dependent training beyond a constant time is allowed, then \(\mathcal{E}(\rho_{t})\) might be reasonably small even if the necessary condition is not satisfied, see e.g. [2, 18, 26].

In Appendix B.2, we include a brief discussion of the sample complexity result of SGD implied by Theorem 3.4. In particular, SGD with \(\mathcal{O}(d)\) samples cannot recover \(f^{*}\) reliably if the refelctiveproperty holds, which is consistent with observations in previous works such as [1; 2]. We also remark that our result in Theorem 3.4 is established for the mean-field dynamics corresponding to the one-pass SGD (2.3), any may not apply for other variants of SGD. In particular, some recent works [6; 11; 16] prove that multi-pass SGD with batch-reuse mechanism can learn some target functions with fewer samples than one-pass SGD.

### Proof Sketch for Theorem 3.4

To prove Theorem 3.4, the main intuition is that under some mild assumptions, if (3.1) is satisfied and the initial distribution \(\rho_{0}\) is supported in \(\{(a,w)\in\mathbb{R}^{d+1}:w_{S}=0\}\), where \(w_{S}\) is the orthogonal projection of \(w\in\mathbb{R}^{d}\) onto \(S\), then \(\rho_{t}\) is supported in \(\{(a,w)\in\mathbb{R}^{d+1}:w_{S}=0\}\) for all \(t\geq 0\). This means that the trained neural network \(f_{\text{NN}}(x;\rho_{t})\) learns no information about \(x_{S}\), the orthogonal projection of \(x\in\mathbb{R}^{d}\) onto \(S\), and hence cannot approximate \(f^{*}(x)=h^{*}(x_{V})\) with arbitrarily small error if \(h^{*}(z)\) is dependent on \(z_{S}\). We formulate this observation in the following theorem.

**Theorem 3.5**.: _Suppose that Assumption 3.1 hold and let \(\rho_{t}\) be the solution to (2.4). Let \(S\subset\mathbb{R}^{d}\) be a subspace with the projection map \(\mathcal{P}_{S}:\mathbb{R}^{d+1}\to S\) that maps \((a,w)\) to \(w_{S}\). If \((\mathcal{P}_{S})_{\#}\rho_{0}=\delta_{S}\), where \(\delta_{S}\) is the delta measure on \(S\) and_

\[\mathbb{E}_{x}\left[f^{*}(x)\sigma^{\prime}\left(w^{\top}x_{S}^{\perp}\right) x_{S}\right]=0,\quad\forall\;w\in\mathbb{R}^{d},\] (3.5)

_where \(x_{S}^{\perp}=x-x_{S}\), then it holds for any \(t\geq 0\) that_

\[(\mathcal{P}_{S})_{\#}\rho_{t}=\delta_{S}.\] (3.6)

Here, the delta measure \(\delta_{S}\) on \(S\) is a probability measure on \(S\) such that for any continuous and compactly supported function \(\varphi:S\to\mathbb{R}\), it holds that \(\int_{S}\varphi(x)\delta_{S}(dx)=\varphi(0)\). In Theorem 3.5, the condition (3.5) is stated in terms of \(f^{*}\). We will show later that it is closely related to and is actually implied by (3.1), via a decomposition \(w^{\top}x_{S}^{\perp}=w^{\top}(x-x_{V})+w^{\top}(x_{V}-x_{S})\), with \(w^{\top}(x-x_{V})\) and \(w^{\top}(x_{V}-x_{S})\) corresponding to \(u\) and \(v^{\top}z_{S}^{\top}\) in (3.1), respectively. The main idea in the proof of Theorem 3.5 is to construct a flow \(\hat{\rho}_{t}\) in the space \(\mathcal{P}(\mathbb{R}\times S^{\perp})\), where \(S^{\perp}\) is the orthogonal complement of \(S\) in \(\mathbb{R}^{d}\), and then show that \(\rho_{t}=\hat{\rho}_{t}\times\delta_{S}\) is the solution to (2.4). More specifically, the flow \(\hat{\rho}_{t}\) is constructed as the solution to the following evolution equation in \(\mathcal{P}(\mathbb{R}\times S^{\perp})\):

\[\begin{cases}\partial_{t}\hat{\rho}_{t}=\nabla_{\hat{\theta}}\cdot\left(\hat {\rho}_{t}\hat{\xi}(t)\nabla_{\hat{\theta}}\hat{\Phi}(\hat{\theta};\hat{\rho} _{t})\right),\\ \hat{\rho}_{t}\big{|}_{t=0}=\hat{\rho}_{0},\end{cases}\] (3.7)

where \(\hat{\rho}_{0}\in\mathcal{P}(\mathbb{R}\times S^{\perp})\) satisfies \(\rho_{0}=\hat{\rho}_{0}\times\delta_{S}\), \(\hat{\theta}=(a,w_{S}^{\perp})\), \(\hat{\xi}(t)=\text{diag}(\xi_{a}(t),\xi_{w}(t)I_{S^{\perp}})\), and

\[\hat{\Phi}(\hat{\theta};\hat{\rho})=a\mathbb{E}_{x}\left[\left(\hat{f}_{\text {NN}}(x_{S}^{\perp};\hat{\rho})-f^{*}(x)\right)\sigma\left((w_{S}^{\perp})^{ \top}x_{S}^{\perp}\right)\right]=\hat{V}(\hat{\theta})+\int\hat{U}(\hat{ \theta},\hat{\theta}^{\prime})\hat{\rho}(d\hat{\theta}^{\prime}),\]

with

\[\hat{f}_{\text{NN}}(x_{S}^{\perp};\hat{\rho})=\int a\sigma\left((w_{S}^{\perp })^{\top}x_{S}^{\perp}\right)\hat{\rho}(da,dw_{S}^{\perp}),\]

and

\[\hat{V}(\hat{\theta})=-a\mathbb{E}_{x}\left[f^{*}(x)\sigma\left((w_{S}^{\perp })^{\top}x_{S}^{\perp}\right)\right],\quad\hat{U}(\theta,\theta^{\prime})=aa^ {\prime}\mathbb{E}_{x}\left[\sigma\left((w_{S}^{\perp})^{\top}x_{S}^{\perp} \right)\sigma\left(((w_{S}^{\perp})^{\prime})^{\top}x_{S}^{\perp}\right)\right].\]

The detailed proof will be presented in Appendix A.1.

In practice, both \(V\) and \(S\) are unknown and it is nontrivial to choose an initialization \(\rho_{0}\) supported in \(\{(a,w)\in\mathbb{R}^{d+1}:w_{S}=0\}\). However, one can set \(\rho_{0}=\rho_{a}\times\rho_{w}\) with \(\rho_{w}\sim\mathcal{N}(0,\frac{1}{d}I_{d})\) and this can make the marginal distribution of \(\rho_{0}\) on \(S\) very close to the the delta measure \(\delta_{S}\) if \(d>>p=\text{dim}(V)\geq\text{dim}(S)\), which fits the setting of subspace-sparse polynomials. Rigorously, we have the following theorem stating dimension-free stability with respect to initial distribution, with the proof deferred to Appendix A.2.

**Theorem 3.6**.: _Suppose that Assumption 3.1 holds for both \(\rho_{0}\) and \(\tilde{\rho}_{0}\). Let \(\rho_{t}\) solve \(\partial_{t}\rho_{t}=\nabla_{\theta}\cdot(\rho_{t}\xi(t)\nabla_{\theta}\Phi( \theta;\rho_{t}))\) and let \(\tilde{\rho}_{t}\) solve \(\partial_{t}\tilde{\rho}_{t}=\nabla_{\theta}\cdot(\tilde{\rho}_{t}\xi(t)\nabla_{ \theta}\Phi(\theta;\tilde{\rho}_{t}))\). Then for any \(T\in(0,+\infty)\), there exists a constant \(C_{s}>0\) depending only on \(p\), \(h^{*}\), \(K_{\sigma}\), \(K_{\xi}\), \(K_{\rho}\), and \(T\), such that_

\[\sup_{0\leq t\leq T}\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x ;\tilde{\rho}_{t})|^{2}\right]\leq C_{s}W_{2}^{2}(\rho_{0},\tilde{\rho}_{0}),\] (3.8)

_where \(W_{2}(\cdot,\cdot)\) is the \(2\)-Wasserstein metric._

Based on Theorem 3.5 and Theorem 3.6, Theorem 3.4 can be proved by some straightforward computation, for which the details can be found in Appendix A.3.

## 4 Sufficient Condition for SGD-Learnability

In this section, we propose a sufficient condition and a training strategy that can guarantee the exponential decay of \(\mathcal{E}(\rho_{t})\) with constants independent of the dimension \(d\).

### Training Procedure and Convergence Guarantee

We prove in Section 3 that if the trained parameters always stay in a proper subspace \(\{(a,w)\in\mathbb{R}^{d+1}:w_{S}=0\}\), then \(f_{\text{NN}}(x;\rho_{t})\) cannot learn all information about \(f^{*}\) or \(h^{*}\). Ideally, one would expect the negation to be a sufficient condition for the SGD-learnability, i.e., the existence of a choice of learning rates and initial distribution that guarantees \(\lim_{t\to\infty}\mathcal{E}(\rho_{t})\) with dimension-free rate. This is almost true but we need a slightly stronger condition due to technical issues. More specifically, we need that the Taylor's expansion of some dynamics (not the dynamics itself) is not trapped in any proper subspace.

**Assumption 4.1**.: _Consider the following flow \(\hat{w}_{V}(t)\) in \(V\):_

\[\begin{cases}\frac{d}{dt}\hat{w}_{V}(t)=\mathbb{E}_{z}\left[zh^{*}(z)\sigma^{ \prime}(\hat{w}_{V}(t)^{\top}z)\right],\\ \hat{w}_{V}(0)=0.\end{cases}\] (4.1)

_We assume that for some \(s\in\mathbb{N}_{+}\), the Taylor's expansion up to \(s\)-th order of \(\hat{w}_{V}(t)\) at \(t=0\) is not contained in any proper subspace of \(V\)._

Assumption 4.1 aims to state the same observation as the merged-staircase property in [1]. As a simple example, if \(V=\mathbb{R}^{p}\) and \(h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}+\dots+z_{1}z_{2}\cdots z_{p}\) which satisfies the merged-staircase property, then it can be computed that the leading order terms of the coordinates of \(\hat{w}_{V}(t)\) are given by \((c_{1}t,c_{2}t^{2},c_{3}t^{2^{2}},\dots,c_{p}t^{2^{p-1}})\) with nonzero constants \(c_{1},c_{2},\dots,c_{p}\) if \(\sigma\in\mathcal{C}^{*}(\mathbb{R})\) with \(s=2^{p-1}\) and \(\sigma^{(1)}(0),\sigma^{(2)}(0),\dots,\sigma^{(p)}(0)\) are all nonzero (see Proposition 33 in [1]). This is to say that Assumption 4.1 with \(s=2^{p-1}\) is satisfied for this example. We provide further characterization of Assumption 4.1 by verifying it in a more general setting in Appendix D.1.

We also remark that the Taylor's expansion of the flow \(\hat{w}_{V}(t)\) that solves (4.1) depends only on the \(h^{*}\) and \(\sigma^{(1)}(0),\sigma^{(2)}(0),\dots,\sigma^{(s)}(0)\). We require some additional regularity assumption on higher-order derivatives of \(\sigma\).

**Assumption 4.2**.: _Assume that \(\sigma\) satisfies \(\sigma\in\mathcal{C}^{L+1}(\mathbb{R})\) and \(\sigma,\sigma^{\prime},\sigma^{\prime\prime},\sigma^{(L+1)}\in L^{\infty}( \mathbb{R})\), where \(L=2sn\binom{n+p}{p}\) with \(n=\text{deg}(f^{*})=\text{deg}(h^{*})\) and \(s\) being the positive integer in Assumption 4.1._

Our proposed training strategy is stated in Algorithm 1. The training strategy is inspired by the two-stage strategy proposed in [1] that trains the parameters \(w\) with fixed \(a\) for \(t\in[0,T]\) and then trains the parameter \(a\) with fixed \(w\) and a perturbed activation function for \(t\geq T\). Several important modifications are made since we consider general polynomials, rather than polynomials on hypercubes as in [1]. In particular,

* We need to repeat Step 2 (training \(w\)) for \(p\) times and use their average as the initialization of training \(a\), while this step only needs to be done once in [1]. The reason is that the space of polynomials on the hypercube \(\{\pm 1\}^{p}\) is essentially a linear space with dimension \(2^{p}\). However, the space of general polynomials on \(V\) is an \(\mathbb{R}\)-algebra that is also a linearspace but is of infinite dimension. Therefore, to make the kernel matrix in training \(a\) non-degenerate, we require some algebraic independence which can be guaranteed by \(u(a_{1},\dots,a_{p},t))=\frac{1}{p}\sum_{i=1}^{p}w(a_{i},t),\ 0<t\leq T\), though linear independence suffices for [1]. Let us also emphasize that each run of Step 2 involves training an interacting particle system instead of training a single particle.
* In Step 4, we use a new activation function \(\hat{\sigma}(\zeta)=(1+\zeta)^{n}\) that is a polynomial of the same degree as \(f^{*}\) and \(h^{*}\). The reason is still that we work with the space general polynomials whose dimension as a linear space is infinite. Thus, we need the specific form \(\hat{\sigma}(\zeta)=(1+\zeta)^{n}\) to guarantee the trained neural network \(f_{\text{NN}}(x;\rho_{t})\) is a polynomial with degree at most \(n=\text{deg}(f^{*})=\text{deg}(h^{*})\). As a comparison, in the setting of [1], all functions on \(\{\pm 1\}^{p}\) can be understood as a polynomial, and no specific format of the new activation function is needed.

Our main theorem in this section is as follows, stating that the loss functional \(\mathcal{E}(\rho_{t})\) can decay to \(0\) exponentially fast, with rates independent of the dimension \(d\).

**Theorem 4.3**.: _Suppose that Assumption 4.1 and 4.2 hold and let \(\rho_{t}\) be the flow generated by Algorithm 1. There exist constants \(C_{1},C_{2}>0\) depending on \(h^{*},\sigma,n,p,s\), such that_

\[\mathcal{E}(\rho_{t})\leq C_{1}\exp(-C_{2}t),\quad\forall\,t\geq 0.\]

Let us also remark that it is possible to use the original dynamics \(\hat{w}_{V}\) defined in (4.1) when we state Assumption 4.1, which can actually imply its Taylor's expansion up to some order is not trapped in any proper subspace of \(V\) if we further assume \(\hat{w}_{V}(t)\) is analytic. We choose to directly use Taylor's expansion in Assumption 4.1 since we want to avoid the additional analytic assumption and to emphasize that the constants \(C_{1},C_{2}\) in Theorem 4.3 depend on the order \(s\) of the Tayler's expansion satisfying Assumption 4.1.

Discussion about the sample complexity implied by Theorem 4.3 is included in Appendix D.2, suggesting that \(\mathcal{O}(d)\) samples suffices for SGD to learn \(f^{*}\) reliably if conditions in Theorem 4.3 are true. This is also consistent with previous works such as [1, 2].

### Proof Sketch for Theorem 4.3

To prove Theorem 4.3 we follow the same general strategy as [1], though some technical analysis is significantly different due to the roatation-invariant setting. The main goal here is to show before Step 4, the algorithm already learned a diverse set of features. After that, note that Step 4 in Algorithm 1 is essentially a convex/quadratic optimization problem (since we only train \(a\) and set \(\xi_{w}(t)=0\)). In addition, thanks to the new activation function \(\hat{\sigma}(\zeta)=(1+\zeta)^{n}\), one only needs to consider \(\mathbb{P}_{V,n}\) that is the space of all polynomials on \(V\) with degree at most \(n=\text{deg}(h^{*})=\text{deg}(f^{*})\). The dimension of \(\mathbb{P}_{V,n}\) as a linear space is \(\binom{n+p}{p}\). Let \(p_{1},p_{2},\dots,\binom{n+p}{p}\) be the orthonormal basis of \(\mathbb{P}_{V,n}\) with input \(z\sim\mathcal{N}(0,I_{V})\) and define the kernel matrix

\[\mathcal{K}_{i_{1},i_{2}}(t)=\mathbb{E}_{a_{1},\dots,a_{p}}\left[\mathbb{E}_{ z,z^{\prime}}\left[p_{i_{1}}(z)\hat{\sigma}(u(a_{1},\dots,a_{p},t)^{\top}z) \hat{\sigma}(u(a_{1},\dots,a_{p},t)^{\top}z^{\prime})p_{i_{2}}(z^{\prime}) \right]\right],\] (4.2)

where \(\hat{\sigma}(\xi)=(1+\xi)^{n}\), \((a_{1},\dots,a_{p})\sim\mathcal{U}([-1,1]^{p})\), \(1\leq i_{1},i_{2}\leq\binom{n+p}{p}\), and \(0\leq t\leq T\). As long as this kernel matrix is non-degenerate, we know that the loss functional is strongly convex with respect to the parameters in the second layer when fixing the first layer, and thus, it can be computed straightforwardly that the loss decays to \(0\) exponentially fast for \(t\geq T\), leading to the desired convergence rate in Theorem 4.3; see Appendix C.3 for details. Thus, the main part in the proof of Theorem 4.3 is to establish the non-degeneracy of the kernel matrix.

**Proposition 4.4**.: _Suppose that Assumption 4.1 and 4.2 hold. There exist constants \(C,T>0\) depending on \(h^{*},\sigma,n,p,s\), such that_

\[\lambda_{\min}(\mathcal{K}(t))\geq Ct^{2sn\binom{n+p}{p}},\quad\forall\,0\leq t \leq T,\] (4.3)

_where \(\lambda_{\min}(\mathcal{K}(t))\) is the smallest eigenvalue of \(\mathcal{K}(t)\)._

In the rest of this subsection, we sketch the main ideas in the proof of Proposition 4.4, with the details of the proof being deferred to Appendix C. We first show that \(w(a_{i},t)\) and \(u(a_{1},\dots,a_{p},t)\) can be approximated well by \(\hat{w}(a_{i},t)\) and \(\hat{u}(a_{1},\ldots,a_{p},t)=\frac{1}{p}\sum_{i=1}^{p}\hat{w}(a_{i},t)\) that are polynomials in \(a_{i}\) and \(a_{1},\ldots,a_{p}\) respectively. This approximation step follows [1] closely and is analyzed detailedly in Appendix C.1. Therefore, to give a positive lower bound of \(\lambda_{\min}(\mathcal{K}(t))\), one only needs to show the non-degeneracy of the matrix \(\hat{M}(\mathbf{a},t)\in\mathbb{R}^{\binom{n+p}{p}\times\binom{n+p}{p}}\) with

\[\hat{M}_{i_{1},i_{2}}(\mathbf{a},t)=\mathbb{E}_{z}\left[p_{i_{1}}(z)\hat{ \sigma}(\hat{u}(\mathbf{a}_{i_{2}},t)^{\top}z)\right],\]

where \(\mathbf{a}=\left(\mathbf{a}_{1},\mathbf{a}_{2},\ldots,\mathbf{a}_{\binom{n+p}{ p}}\right)\) and \(\mathbf{a}_{i}\in\mathbb{R}^{p}\) for \(i=1,2,\ldots,\binom{n+p}{p}\). Intuitively, this non-degeneracy can be implied by

\[\text{span}\left\{\hat{\sigma}(\hat{u}(a_{1},\ldots,a_{p},t)^{\top}z):a_{1},a _{2},\ldots,a_{p}\in[-1,1]\right\}=\mathbb{P}_{V,n},\]

which is true if \(\hat{u}_{i}(a_{1},\ldots,a_{p},t),\ 1\leq i\leq p\) are \(\mathbb{R}\)-algebraically independent polynomials in \(a_{1},a_{2},\ldots,a_{p}\), where \(\hat{u}_{i}\) is the \(i\)-th coefficient of \(\hat{u}\) under some basis of \(V\), and algebraic independence can be obtained from linear independence by taking the average of independent copies. We illustrate this intuition with a bit more detail.

**Algebraic independence of \(\hat{u}_{i}\).** With Assumption 4.1, \(\hat{w}_{1}(a,t),\hat{w}_{2}(a,t),\ldots,\hat{w}_{1}(a,t)\) can be proved as \(\mathbb{R}\)-linear independent polynomials in \(a\in\mathbb{R}\). Then one can apply the following theorem to boost the linear independence of \(\hat{w}_{i}\), whose constant term is zero since initialization in training is set as \(\rho_{0}=\rho_{a}\times\delta_{\mathbb{R}^{d}}\), to the algebraic independence of \(\hat{u}_{i}\).

**Theorem 4.5**.: _Let \(v_{1},v_{2},\ldots,v_{p}\in\mathbb{R}[a]\) be \(\mathbb{R}\)-linearly independent polynomials with the constant terms being zero. Then \(\frac{1}{p}(v_{1}(a_{1})+\cdots+v_{1}(a_{p})),\ldots,\frac{1}{p}(v_{p}(a_{1}) +\cdots+v_{p}(a_{p}))\in\mathbb{R}[a_{1},a_{2},\ldots,a_{p}]\) are \(\mathbb{R}\)-algebraically independent._

The proof of Theorem 4.5 is deferred to Appendix C.2 and is based on the celebrated Jacobian criterion stated as follows.

**Theorem 4.6** (Jacobian criterion [7]).: \(v_{1},v_{2},\ldots,v_{p}\in\mathbb{R}[a_{1},a_{2},\ldots,a_{p}]\) _are \(\mathbb{R}\)-algebraically independent if and only if_

\[\det\begin{pmatrix}\frac{\partial v_{1}}{\partial a_{1}}&\frac{\partial v_{1} }{\partial a_{2}}&\cdots&\frac{\partial v_{1}}{\partial a_{p}}\\ \frac{\partial v_{2}}{\partial a_{1}}&\frac{\partial v_{2}}{\partial a_{2}}& \cdots&\frac{\partial v_{2}}{\partial a_{p}}\\ \vdots&\vdots&\ddots&\vdots\\ \frac{\partial v_{p}}{\partial a_{1}}&\frac{\partial v_{p}}{\partial a_{2}}& \cdots&\frac{\partial v_{2}}{\partial a_{p}}\end{pmatrix}\]

_is a nonzero polynomial in \(\mathbb{R}[a_{1},a_{2},\ldots,a_{p}]\)._

**Non-degeneracy of \(\hat{M}(\mathbf{a},t)\).** With the observation that \(\text{span}\left\{\hat{\sigma}(q^{\top}z):q\in V\right\}=\mathbb{P}_{V,n}\) (see Lemma C.13), we define another matrix \(X(\mathbf{q})\in\mathbb{R}^{\binom{n+p}{p}\times\binom{n+p}{p}}\) via

\[X_{i_{1},i_{2}}(\mathbf{q})=\mathbb{E}_{z}\left[p_{i_{1}}(z)\sigma(\mathbf{q}_{ i_{2}}^{\top}z)\right],\]

where \(\mathbf{q}=\left(\mathbf{q}_{1},\mathbf{q}_{2},\ldots,\mathbf{q}_{\binom{n+p}{ p}}\right)\) with \(\mathbf{q}_{i}\in V\), and prove that \(\det(X(\mathbf{q}))\) is a non-zero polynomial in \(\mathbf{q}\) of the form

\[\det(X(\mathbf{q}))=\sum_{i=1}^{\binom{n+p}{p}}\sum_{0\leq\|\mathbf{j}_{i}, \|_{1}\leq n}X_{\mathbf{j}}\mathbf{q}^{\mathbf{j}}=\sum_{i=1}^{\binom{n+p}{p} }\sum_{0\leq\|\mathbf{j}_{i}\|_{1}\leq n}X_{\mathbf{j}}\prod_{l=1}^{\binom{n+ p}{p}}\mathbf{q}_{l}^{\mathbf{j}_{i}},\]

where \(\mathbf{q}_{i}\) is understood as a (coefficient) vector in \(\mathbb{R}^{p}\) associated with a fixed orthonormal basis of \(V\) and \(\mathbf{j}=\left(\mathbf{j}_{1},\mathbf{j}_{2},\ldots,\mathbf{j}_{\binom{n+p}{ p}}\right)\) with \(\mathbf{j}_{i}\in\mathbb{N}^{p}\). Then setting \(\mathbf{q}=\hat{u}(\mathbf{a}_{i_{2}},t)\) leads to

\[\det(\hat{M}(\mathbf{a},t))=\sum_{i=1}^{\binom{n+p}{p}}\sum_{0\leq\|\mathbf{j}_ {i},\|_{1}\leq n}X_{\mathbf{j}}\prod_{l=1}^{\binom{n+p}{p}}\hat{u}(\mathbf{a}_{ l},t)^{\mathbf{j}_{l}}.\]

To prove that \(\det(\hat{M}(\mathbf{a},t))\) is a non-zero polynomial in \(\mathbf{a}\), i.e., \(\hat{M}(\mathbf{a},t)\) is non-degenerate, we use the following lemma linking algebraic independence back to linear independence.

**Lemma 4.7**.: _Suppose that \(v_{1},v_{2},\ldots,v_{p}\in\mathbb{R}[a_{1},a_{2},\ldots,a_{p}]\) are \(\mathbb{R}\)-algebraically independent. For any \(m\geq 1\), the following polynomials in \(\mathbf{a}=(\mathbf{a}_{1},\mathbf{a}_{2},\ldots,\mathbf{a}_{m})\in(\mathbb{R} ^{p})^{m}\) are \(\mathbb{R}\)-linearly independent_

\[\prod_{l=1}^{m}\mathbf{v}(\mathbf{a}_{l})^{\mathbf{j}i},\quad 1\leq\|\mathbf{j} _{i}\|_{1}\leq n,\;1\leq i\leq m,\]

_where \(\mathbf{v}=(v_{1},v_{2},\ldots,v_{p})\)._

The proof of Lemma 4.7 and some other related analysis are deferred to Appendix C.3.

## 5 Conclusion and Discussions

In this work, we generalize the merged-staircase property in [1] to a basis-free version and establish a necessary condition for learning a subspace-sparse polynomial on Gaussian input with arbitrarily small error. Moreover, we prove the exponential decay property of the loss functional under a sufficient condition that is slightly stronger than the necessary one. The bounds and rates are all dimension-free. Our work provides some understanding of the mean-field dynamics, though its general behavior is extremely difficult to characterize due to the non-convexity of the loss functional.

Let us also make some comments on limitations and future directions. Firstly, there is still a gap between the necessary condition and the sufficient condition, which is basically from the fact that the sufficient condition is built on the Taylor's expansion of the flow (4.1). One future research question is whether we can fill the gap by considering the original flow (4.1) rather than its Taylor's expansion. Secondly, Algorithm 1 repeats training \(w\) for \(p\) times and takes the average of parameters, which is different from the usual strategy for training neural networks. This step is used to guarantee the algebraic independence. We conjecture that this step can be removed since the general algebraic independence is too strong when we have some preknowledge on the degree of \(f^{*}\) or \(h^{*}\), which deserves future research.

## Acknowledgments and Disclosure of Funding

The work of R. Ge is supported by NSF Award DMS-2031849 and CCF-1845171 (CAREER). We thank Joan Bruna for helpful comments and discussion.

## References

* [1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* [2] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2552-2623. PMLR, 2023.
* [3] Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. _Advances in Neural Information Processing Systems_, 34:26989-27002, 2021.
* [4] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International conference on machine learning_, pages 242-252. PMLR, 2019.
* [5] Dyego Araujo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep neural networks. _arXiv preprint arXiv:1906.00193_, 2019.
* [6] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita invant: Data repetition allows SGD to learn high-dimensional multi-index functions. _arXiv preprint arXiv:2405.15459_, 2024.
* [7] Malte Beecken, Johannes Mittmann, and Nitin Saxena. Algebraic independence and blackbox identity testing. _Information and Computation_, 222:2-19, 2013.

* [8] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-index models with gradient flow. _arXiv preprint arXiv:2310.19793_, 2023.
* [9] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* [10] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How two-layer neural networks learn, one (giant) step at a time. In _NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning_, 2023.
* [11] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova, and Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents. In _Forty-first International Conference on Machine Learning_, 2024.
* [12] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2018.
* [13] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [14] Kenji Kawaguchi. Deep learning without poor local minima. _Advances in neural information processing systems_, 29, 2016.
* [15] Kenji Kawaguchi and Yoshua Bengio. Depth with nonlinearity creates no bad local minima in resnets. _Neural Networks_, 118:167-174, 2019.
* [16] Jason D Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit. _arXiv preprint arXiv:2406.01581_, 2024.
* [17] Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. _arXiv preprint arXiv:1702.08580_, 2017.
* [18] Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma. Beyond NTK with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time. _Advances in Neural Information Processing Systems_, 36, 2024.
* [19] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In _Conference on Learning Theory_, pages 2388-2464. PMLR, 2019.
* [20] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* [21] Phan-Minh Nguyen. Mean field limit of the learning dynamics of multilayer neural networks. _arXiv preprint arXiv:1902.02880_, 2019.
* [22] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An interacting particle system approach. _Communications on Pure and Applied Mathematics_, 75(9):1889-1935, 2022.
* [23] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. In _International conference on machine learning_, pages 4433-4441. PMLR, 2018.
* [24] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. _Stochastic Processes and their Applications_, 130(3):1820-1852, 2020.
* [25] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large numbers. _SIAM Journal on Applied Mathematics_, 80(2):725-752, 2020.

* [26] Taiji Suzuki, Denny Wu, Kazusato Oko, and Atsushi Nitanda. Feature learning via mean-field langevin dynamics: classifying sparse parities and beyond. _Advances in Neural Information Processing Systems_, 36, 2024.
* [27] Alain-Sol Sznitman. Topics in propagation of chaos. _Lecture notes in mathematics_, pages 165-251, 1991.
* [28] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.

Proofs for Section 3

This section collects the proofs of Theorem 3.5, Theorem 3.6, and Theorem 3.4.

### Proof of Theorem 3.5

Existence and uniqueness of solutions to (2.4)Before proving Theorem 3.5, let us remark on the existence and uniqueness of solution to the mean-field dynamics (2.4). According to Remark 7.1 in [20] and Theorem 1.1 in [27], the PDE (2.4) admits a unique solution if Assumption 3.1 (ii) holds and both \(\nabla V(\theta)\) and \(\nabla_{\theta}U(\theta,\theta^{\prime})\) are bounded and Lipschitz continous. Here we recall that \(V\) and \(U\) are defined in (2.5). With Assumption 3.1 (i) and (iii), it is not hard to verify the boundedness and Lipschitz continuity of \(\nabla V(\theta)\) and \(\nabla_{\theta}U(\theta,\theta^{\prime})\) by noticing that any finite-order moment of \(\mathcal{N}(0,I_{d})\) is finite.

Proof of Theorem 3.5.: Since \((\mathcal{P}_{S})_{\#}\rho_{0}=\delta_{S}\), the initial distribution \(\rho_{0}\) can be decomposed as \(\rho_{0}=\hat{\rho}_{0}\times\delta_{S}\), where \(\hat{\rho}_{0}\in\mathcal{P}(\mathbb{R}\times S^{\perp})\). Consider the following evolution equation (3.7) in \(\mathcal{P}(\mathbb{R}\times S^{\perp})\). By the discussion at the beginning of Section A.1, we know that \(\rho_{t}\) is the unique solution to (2.4). Similar arguments also leads to the existence and uniquess of the solution to (3.7).

We will show that the solution to (2.4) must be of the form

\[\rho_{t}=\hat{\rho}_{t}\times\delta_{S},\] (A.1)

where \(\hat{\rho}_{t}\) solves (3.7), and this decomposition can immediatel imply (3.6). By the uniquess of the solution, it suffices to verify that \(\rho_{t}=\hat{\rho}_{t}\times\delta_{S}\) is a solution to (2.4). It follows directly from (A.1) that

\[f_{\text{NN}}(x;\rho_{t})=f_{\text{NN}}(x;\hat{\rho}_{t}\times\delta_{S})=\hat {f}_{\text{NN}}(x_{S}^{\perp};\hat{\rho}_{t}),\]

and hence that

\[\partial_{a}\Phi(\theta;\rho_{t})=\partial_{a}\hat{\Phi}(\hat{\theta};\hat{ \rho}_{t}),\quad\text{if }w_{S}=0.\] (A.2)

We also have that

\[\nabla_{w_{S}^{\perp}}\Phi(\theta;\rho_{t}) =a\mathbb{E}_{x}\left[\left(f_{\text{NN}}(x;\rho_{t})-f^{*}(x) \right)\sigma^{\prime}(w^{\top}x)x_{S}^{\perp}\right]\] \[=a\mathbb{E}_{x}\left[\left(\hat{f}_{\text{NN}}(x_{S}^{\perp}; \hat{\rho}_{t})-f^{*}(x)\right)\sigma^{\prime}\left((w_{S}^{\perp})^{\top}x_{ S}^{\perp}\right)x_{S}^{\perp}\right]\] (A.3) \[=\nabla_{w_{S}^{\perp}}\hat{\Phi}(\hat{\theta};\hat{\rho}_{t}),\]

if \(w_{S}=0\). In addition, it holds also for \(w_{S}=0\) that

\[\nabla_{w_{S}}\Phi(\theta;\rho_{t}) =a\mathbb{E}_{x}\left[\left(f_{\text{NN}}(x;\rho_{t})-f^{*}(x) \right)\sigma^{\prime}(w^{\top}x)x_{S}\right]\] \[=a\mathbb{E}_{x}\left[\hat{f}_{\text{NN}}(x_{S}^{\perp};\hat{\rho }_{t})\sigma^{\prime}\left((w_{S}^{\perp})^{\top}x_{S}^{\perp}\right)x_{S} \right]-a\mathbb{E}_{x}\left[f^{*}(x)\sigma^{\prime}\left((w_{S}^{\perp})^{ \top}x_{S}^{\perp}\right)x_{S}\right]\] (A.4) \[=0,\]

where we used \(\mathbb{E}_{x_{S}}[x_{S}]=0\) and (3.5). Combining (A.2), (A.3), and (A.4), we have for any \(\eta\in\mathcal{C}_{c}^{\infty}(\mathbb{R}^{d+1}\times(0,+\infty))=\mathcal{C}_ {c}^{\infty}(\mathbb{R}\times S^{\perp}\times S\times(0,+\infty))\) that

\[\iint\left(-\partial_{t}\eta+\nabla_{\theta}\eta\cdot\left(\xi(t) \nabla_{\theta}\Phi(\theta;\rho_{t})\right)\right)\rho_{t}(d\theta)dt\] \[=\iiint\Big{(}-\partial_{t}\eta(\theta,t)+\xi_{a}(t)\partial_{a} \eta(\theta,t)\cdot\partial_{a}\Phi(\theta;\rho_{t})+\xi_{w}(t)\nabla_{w_{S}^{ \perp}}\eta(\theta,t)\cdot\nabla_{w_{S}^{\perp}}\Phi(\theta;\rho_{t})\] \[\qquad\qquad\qquad\qquad+\xi_{w}(t)\nabla_{w_{S}}\eta(\theta,t) \cdot\nabla_{w_{S}}\Phi(\theta;\rho_{t})\Big{)}\hat{\rho}_{t}(d\hat{\theta}) \delta_{S}(dw_{S})dt\] \[=\iint\Big{(}-\partial_{t}\eta(\hat{\theta},0,t)+\xi_{a}(t) \partial_{a}\eta(\hat{\theta},0,t)\cdot\partial_{a}\hat{\Phi}(\hat{\theta}; \hat{\rho}_{t})\] \[\qquad\qquad\qquad+\xi_{w}(t)\nabla_{w_{S}^{\perp}}\eta(\hat{ \theta},0,t)\cdot\nabla_{w_{S}^{\perp}}\hat{\Phi}(\hat{\theta};\hat{\rho}_{t}) \Big{)}\hat{\rho}_{t}(d\hat{\theta})dt\] \[=\iint\Big{(}-\partial_{t}\eta(\hat{\theta},0,t)+\nabla_{\hat{ \theta}}\eta(\hat{\theta},0,t)\cdot(\hat{\xi}(t)\nabla_{\hat{\theta}}\hat{\Phi}( \hat{\theta};\hat{\rho}_{t}))\Big{)}\hat{\rho}_{t}(d\hat{\theta})dt\] \[=0,\]

where the last equality holds by applying the test function \(\eta(\cdot,0,\cdot)\in\mathcal{C}_{c}^{\infty}(\mathbb{R}\times S^{\perp} \times(0,+\infty))\) to (3.7). The proof is hence completed.

### Proof of Theorem 3.6

The proof of Theorem 3.6 uses some ideas from the proof of Theorem 16 in [1]. Similar ideas also exist in earlier works (see e.g., [19, 20]).

**Lemma A.1**.: _Suppose that Assumption 3.1 holds and let \(\rho_{t}\) solve (2.4). Then for any \(t>0\), \(\rho_{t}\) is supported in \(\big{\{}\theta=(a,w)\in\mathbb{R}^{d+1}:|a|\leq K_{\rho}+K_{\xi}K_{\sigma} \mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]^{1/2}t\big{\}}\)._

Proof.: The particle dynamics for \(a_{t}\) associated with (2.4) is given by

\[\frac{d}{dt}a_{t}=\xi_{a}(t)\mathbb{E}_{x}\left[(f_{\text{NN}}(x;\rho_{t})-f^{ *}(x))\sigma(w_{t}^{\top}x)\right],\]

which implies that

\[\left|\frac{d}{dt}a_{t}\right|\leq K_{\xi}K_{\sigma}\left|\mathbb{E}_{x}\left[ f_{\text{NN}}(x;\rho_{t})-f^{*}(x)\right]\right|\leq K_{\xi}K_{\sigma}(2 \mathcal{E}(\rho_{0}))^{1/2}=K_{\xi}K_{\sigma}\mathbb{E}_{z}\left[|h^{*}(z)|^ {2}\right]^{1/2}.\]

Therefore, one has \(|a_{t}|\leq|a_{0}|+K_{\xi}K_{\sigma}\mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]^ {1/2}t\), which completes the proof. 

**Lemma A.2**.: _Suppose that Assumption 3.1 holds for both \(\rho_{0}\) and \(\tilde{\rho}_{0}\). Let \(\rho_{t}\) solve \(\partial_{t}\rho_{t}=\nabla_{\theta}\cdot(\rho_{t}\xi(t)\nabla_{\theta}\Phi( \theta;\rho_{t}))\) and let \(\tilde{\rho}_{t}\) solve \(\partial_{t}\tilde{\rho}_{t}=\nabla_{\theta}\cdot(\tilde{\rho}_{t}\xi(t) \nabla_{\theta}\Phi(\theta;\tilde{\rho}_{t}))\). For any coupling \(\gamma_{0}\in\Gamma(\rho_{0},\tilde{\rho}_{0})\), let \(\gamma_{t}\in\Gamma(\rho_{t},\tilde{\rho}_{t})\) be the associated coupling during the evolution and define_

\[\Delta(t)=\iint\left(|a-\tilde{a}|^{2}+\|w-\tilde{w}\|^{2}\right)\gamma_{t}(d \theta,d\tilde{\theta}).\]

_Then it holds for any \(0\leq t\leq T\) that_

\[\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x;\tilde{\rho}_{ t})|^{2}\right]\leq C_{f}\Delta(t),\] (A.5)

_and_

\[\frac{d}{dt}\Delta(t)\leq C_{\Delta}\Delta(t),\] (A.6)

_where \(C_{f}\) and \(C_{\Delta}\) are constants depending only on \(p\), \(h^{*}\), \(K_{\sigma}\), \(K_{\xi}\), \(K_{\rho}\), and \(T\)._

Proof of Theorem 3.6.: Let \(C_{f}\) and \(C_{\Delta}\) be the constants in Lemma A.2. For any \(\epsilon>0\), there exists a coupling \(\gamma_{0}\in\Gamma(\rho_{0},\tilde{\rho}_{0})\) such that

\[\iint(|a-\tilde{a}|^{2}+\|w-\tilde{w}\|^{2})\gamma_{0}(d\theta,d\tilde{\theta} )\leq W_{2}^{2}(\rho_{0},\tilde{\rho}_{0})+\epsilon.\]

Define \(\gamma_{t}\in\Gamma(\rho_{t},\tilde{\rho}_{t})\) and \(\Delta(t)\) as in Lemma A.2. According (A.6) and the Gronwall's inequality, it holds that

\[\Delta(t)\leq\Delta(0)e^{C_{\Delta}t}\leq\left(W_{2}^{2}(\rho_{0},\tilde{\rho }_{0})+\epsilon\right)e^{C_{\Delta}t},\quad\forall\;0\leq t\leq T,\]

which combined with (A.5) yields that

\[\sup_{0\leq t\leq T}\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})-f_{\text{ NN}}(x;\tilde{\rho}_{t})|^{2}\right]\leq\left(W_{2}^{2}(\rho_{0},\tilde{\rho}_{0})+ \epsilon\right)C_{f}e^{C_{\Delta}T}.\]

Then we can conclude (3.8) be setting \(\epsilon\to 0\) and \(C_{s}=C_{f}e^{C_{\Delta}T}\). 

**Corollary A.3**.: _Under the same setting as in Theorem 3.6, one has_

\[\sup_{0\leq t\leq T}|\mathcal{E}(\rho_{t})-\mathcal{E}(\tilde{\rho}_{t})|\leq \left(C_{s}\mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]\right)^{1/2}W_{2}(\rho_{ 0},\tilde{\rho}_{0})+\frac{1}{2}C_{s}W_{2}^{2}(\rho_{0},\tilde{\rho}_{0}).\] (A.7)

Proof.: It can be computed that

\[\mathcal{E}(\rho_{t})= \frac{1}{2}\mathbb{E}_{x}\left[|(f_{\text{NN}}(x;\tilde{\rho}_{t}) -f^{*}(x))+(f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x;\tilde{\rho}_{t}))|^{2}\right]\] \[= \mathcal{E}(\tilde{\rho}_{t})+\mathbb{E}_{x}\left[(f_{\text{NN}}(x; \tilde{\rho}_{t})-f^{*}(x))(f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x;\tilde{ \rho}_{t}))\right]\] \[\quad+\frac{1}{2}\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})-f_{ \text{NN}}(x;\tilde{\rho}_{t})|^{2}\right],\]which implies that

\[\sup_{0\leq t\leq T}|\mathcal{E}(\rho_{t})-\mathcal{E}(\tilde{\rho}_{ t})|\leq \mathbb{E}_{x}\left[|f_{\text{NN}}(x;\tilde{\rho}_{t})-f^{*}(x)|^{2} \right]^{1/2}\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x; \tilde{\rho}_{t})|^{2}\right]^{1/2}\] \[\qquad+\frac{1}{2}\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})- f_{\text{NN}}(x;\tilde{\rho}_{t})|^{2}\right]\] \[\leq \left(C_{s}\mathbb{E}_{x_{V}}\left[|h^{*}(x_{V})|^{2}\right] \right)^{1/2}W_{2}(\rho_{0},\tilde{\rho}_{0})+\frac{1}{2}C_{s}W_{2}^{2}(\rho_{ 0},\tilde{\rho}_{0}),\]

where we used Theorem 3.6 and Remark 3.2. 

Proof of Lemma a.2.: We first prove (A.5). It can be computed that

\[|f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x;\tilde{\rho}_{t})|= \left|\int a\sigma(w^{\top}x)\rho_{t}(d\theta)-\int\tilde{a}\sigma(\tilde{w}^ {\top}x)\tilde{\rho}_{t}(d\tilde{\theta})\right|\] \[\leq \left|\iint(a-\tilde{a})\sigma(w^{\top}x)\gamma_{t}(d\theta,d \tilde{\theta})\right|+\left|\iint\tilde{a}\left(\sigma(w^{\top}x)-\sigma( \tilde{w}^{\top}x)\right)\gamma_{t}(d\theta,d\tilde{\theta})\right|,\]

and hence that

\[\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x; \tilde{\rho}_{t})|^{2}\right]\] \[\leq 2\mathbb{E}_{x}\left[\left|\iint(a-\tilde{a})\sigma(w^{\top}x) \gamma_{t}(d\theta,d\tilde{\theta})\right|^{2}\right]+2\mathbb{E}_{x}\left[ \left|\iint\tilde{a}\left(\sigma(w^{\top}x)-\sigma(\tilde{w}^{\top}x)\right) \gamma_{t}(d\theta,d\tilde{\theta})\right|^{2}\right].\]

We then bound the two terms above as follows:

\[\mathbb{E}_{x}\left[\left|\iint(a-\tilde{a})\sigma(w^{\top}x)\gamma_{t}(d \theta,d\tilde{\theta})\right|^{2}\right]\leq K_{\sigma}^{2}\iint|a-\tilde{a}|^ {2}\gamma_{t}(d\theta,d\tilde{\theta})\leq K_{\sigma}^{2}\Delta(t),\]

and

\[\mathbb{E}_{x}\left[\left|\iint\tilde{a}\left(\sigma(w^{\top}x)- \sigma(\tilde{w}^{\top}x)\right)\gamma_{t}(d\theta,d\tilde{\theta})\right|^{ 2}\right]\] \[\leq K_{\sigma}^{2}\left(K_{\rho}+\sqrt{2}K_{\xi}K_{\sigma}\mathcal{E} (\rho_{0})^{1/2}T\right)^{2}\mathbb{E}_{x}\left[\left|\iint|(w-\tilde{w})^{ \top}x|\gamma_{t}(d\theta,d\tilde{\theta})\right|^{2}\right]\] \[\leq K_{\sigma}^{2}\left(K_{\rho}+K_{\xi}K_{\sigma}\mathbb{E}_{z} \left[|h^{*}(z)|^{2}\right]^{1/2}T\right)^{2}\iint\mathbb{E}_{x}\left[|(w- \tilde{w})^{\top}x|^{2}\right]\gamma_{t}(d\theta,d\tilde{\theta})\] \[= K_{\sigma}^{2}\left(K_{\rho}+K_{\xi}K_{\sigma}\mathbb{E}_{z} \left[|h^{*}(z)|^{2}\right]^{1/2}T\right)^{2}\iint\|w-\tilde{w}\|^{2}\gamma_{t }(d\theta,d\tilde{\theta})\] \[\leq K_{\sigma}^{2}\left(K_{\rho}+K_{\xi}K_{\sigma}\mathbb{E}_{z} \left[|h^{*}(z)|^{2}\right]^{1/2}T\right)^{2}\Delta(t),\]

where we used Lemma A.1 and \((w-\tilde{w})^{\top}x\sim\mathcal{N}(0,\|w-\tilde{w}\|^{2})\) if \(x\sim\mathcal{N}(0,I_{d})\). Then we can conclude (A.5) with \(C_{f}=K_{\sigma}^{2}+K_{\sigma}^{2}\left(K_{\rho}+K_{\xi}K_{\sigma}\mathbb{E}_ {z}\left[|h^{*}(z)|^{2}\right]^{1/2}T\right)^{2}\) by combining all estimations above.

We then head into the proof of (A.6), for which we need the particle dynamics

\[\begin{cases}\frac{d}{dt}a_{t}=\xi_{a}(t)\mathbb{E}_{x}\left[(f_{\text{NN}}(x; \rho_{t})-f^{*}(x))\sigma(w_{t}^{\top}x)\right],\\ \frac{d}{dt}\tilde{a}_{t}=\xi_{a}(t)\mathbb{E}_{x}\left[(f_{\text{NN}}(x; \tilde{\rho}_{t})-f^{*}(x))\sigma(\tilde{w}_{t}^{\top}x)\right],\end{cases}\]

and

\[\begin{cases}\frac{d}{dt}w_{t}=\xi_{w}a_{t}\mathbb{E}_{x}\left[(f_{\text{NN}}(x; \rho_{t})-f^{*}(x))\sigma^{\prime}(w_{t}^{\top}x)x\right],\\ \frac{d}{dt}\tilde{w}_{t}=\xi_{w}\tilde{a}_{t}\mathbb{E}_{x}\left[(f_{\text{NN}}( x;\tilde{\rho}_{t})-f^{*}(x))\sigma^{\prime}(\tilde{w}_{t}^{\top}x)x\right].\end{cases}\]

The distance between the particle dynamics can be decomposed as

\[\begin{split}\left|\frac{d}{dt}a_{t}-\frac{d}{dt}\tilde{a}_{t} \right|\leq& K_{\xi}\left[\mathbb{E}_{x}\left[(f_{\text{NN}}(x; \rho_{t})-f_{\text{NN}}(x;\tilde{\rho}_{t}))\sigma(w_{t}^{\top}x)\right]\right| \\ &+K_{\xi}\left[\mathbb{E}_{x}\left[(f_{\text{NN}}(x;\tilde{\rho}_{t})-f^{ *}(x))(\sigma(w_{t}^{\top}x)-\sigma(\tilde{w}_{t}^{\top}x))\right]\right|,\end{split}\] (A.8)\[\begin{split}&\left|\left\langle w_{t}-\tilde{w}_{t},\frac{d}{dt}w_{t}- \frac{d}{dt}\tilde{w}_{t}\right\rangle\right|\\ \leq& K_{\xi}\left|(a_{t}-\tilde{a}_{t})\mathbb{E}_{ x}\left[(f_{\text{NN}}(x;\rho_{t})-f^{*}(x))\sigma^{\prime}(w_{t}^{\top}x)(w_{t}- \tilde{w}_{t})^{\top}x\right]\right|\\ &\quad+K_{\xi}\left|\tilde{a}_{t}\mathbb{E}_{x}\left[(f_{\text{ NN}}(x;\rho_{t})-f_{\text{NN}}(x;\tilde{\rho}_{t}))\sigma^{\prime}(w_{t}^{\top}x)(w_{t}- \tilde{w}_{t})^{\top}x\right]\right|\\ &\quad+K_{\xi}\left|\tilde{a}_{t}\mathbb{E}_{x}\left[(f_{\text{ NN}}(x;\tilde{\rho}_{t})-f^{*}(x))\left(\sigma^{\prime}(w_{t}^{\top}x)-\sigma^{ \prime}(\tilde{w}_{t}^{\top}x)\right)(w_{t}-\tilde{w}_{t})^{\top}x\right] \right|.\end{split}\] (A.9)

Therefore, it can be computed that

\[\begin{split}&\frac{d}{dt}\iint|a-\tilde{a}|^{2}\gamma_{t}(d \theta,d\tilde{\theta})\\ =&\frac{d}{dt}\iint|a_{t}-\tilde{a}_{t}|^{2}\gamma_{0 }(d\theta_{0},d\tilde{\theta}_{0})\\ =& 2\iint(a_{t}-\tilde{a}_{t})\left(\frac{d}{dt}a_{ t}-\frac{d}{dt}\tilde{a}_{t}\right)\gamma_{0}(d\theta_{0},d\tilde{\theta}_{0}) \\ \leq&\iint|a_{t}-\tilde{a}_{t}|^{2}\gamma_{0}(d \theta_{0},d\tilde{\theta}_{0})+\iint\left|\frac{d}{dt}a_{t}-\frac{d}{dt} \tilde{a}_{t}\right|^{2}\gamma_{0}(d\theta_{0},d\tilde{\theta}_{0})\\ \overset{\eqref{eq:2}}{\leq}\Delta(t)&+2K_{\xi}^{2 }K_{\sigma}^{2}\iint\mathbb{E}_{x}\left[\left|f_{\text{NN}}(x;\rho_{t})-f_{ \text{NN}}(x;\tilde{\rho}_{t})\right|\right]^{2}\gamma_{0}(d\theta_{0},d \tilde{\theta}_{0})\\ &\quad+2K_{\xi}^{2}K_{\sigma}^{2}\iint\mathbb{E}_{x}\left[\left| f_{\text{NN}}(x;\tilde{\rho}_{t})-f^{*}(x)\right|\cdot\left|(w_{t}-\tilde{w}_{t})^{ \top}x\right|\right]^{2}\gamma_{0}(d\theta_{0},d\tilde{\theta}_{0})\\ \overset{\eqref{eq:2}}{\leq}\Delta(t)&+2K_{\xi}^{2 }K_{\sigma}^{2}C_{f}\Delta(t)\\ &+2K_{\xi}^{2}K_{\sigma}^{2}\iint\mathbb{E}_{x}\left[\left|f_{ \text{NN}}(x;\tilde{\rho}_{t})-f^{*}(x)\right|\cdot\left|(w_{t}-\tilde{w}_{t} )^{\top}x\right|\right]^{2}\gamma_{0}(d\theta_{0},d\tilde{\theta}_{0})\\ \leq&\Delta(t)&+2K_{\xi}^{2}K_{\sigma}^{2 }C_{f}\Delta(t)\\ &\quad+2K_{\xi}^{2}K_{\sigma}^{2}\iint\mathbb{E}_{x}\left[\left| f_{\text{NN}}(x;\tilde{\rho}_{t})-f^{*}(x)\right|^{2}\right]\mathbb{E}_{x} \left[\left|(w_{t}-\tilde{w}_{t})^{\top}x\right|^{2}\right]\gamma_{0}(d \theta_{0},d\tilde{\theta}_{0})\\ \leq&\Delta(t)+2K_{\xi}^{2}K_{\sigma}^{2}C_{f}\Delta( t)\\ \leq&\Delta(t)+2K_{\xi}^{2}K_{\sigma}^{2}C_{f}\Delta( t)+2K_{\xi}^{2}K_{\sigma}^{2}\mathbb{E}_{z}[\left|h^{*}(z)\right|^{2}] \right)\Delta(t),\end{split}\]

where we used Remark 3.2, and that

\[\begin{split}&\frac{d}{dt}\iint\|w-\tilde{w}\|^{2}\gamma_{t}(d \theta,d\tilde{\theta})\\ =&\frac{d}{dt}\iint\|w_{t}-\tilde{w}_{t}\|^{2}\gamma_{ 0}(d\theta_{0},d\tilde{\theta}_{0})\\ =& 2\iint\left\langle w_{t}-\tilde{w}_{t},\frac{d}{dt}w_{ t}-\frac{d}{dt}\tilde{w}_{t}\right\rangle\gamma_{0}(d\theta_{0},d\tilde{\theta}_{0})\\ \overset{\eqref{eq:2}}{\leq}2K_{\xi}K_{\sigma}\iint|a_{t}- \tilde{a}_{t}|\cdot\mathbb{E}_{x}\left[\left|f_{\text{NN}}(x;\rho_{t})-f^{*}(x )\right|\cdot\left|(w_{t}-\tilde{w}_{t})^{\top}x\right|\right]\gamma_{0}(d \theta_{0},d\tilde{\theta}_{0})\\ &\quad\quad+2K_{\xi}K_{\sigma}\iint\left|\tilde{a}_{t}\right|\cdot \mathbb{E}_{x}\left[\left|f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x;\tilde{ \rho}_{t})\right|\cdot\left|(w_{t}-\tilde{w}_{t})^{\top}x\right|\right]\gamma_ {0}(d\theta_{0},d\tilde{\theta}_{0})\\ &\quad\quad+2K_{\xi}K_{\sigma}\iint|\tilde{a}_{t}|\cdot\mathbb{E}_ {x}\left[\left|f_{\text{NN}}(x;\tilde{\rho}_{t})-f^{*}(x)\right|\cdot\left|(w_{t }-\tilde{w}_{t})^{\top}x\right|^{2}\right]\gamma_{0}(d\theta_{0},d\tilde{ \theta}_{0})\\ \leq& 2K_{\xi}K_{\sigma}\iint|a_{t}-\tilde{a}_{t}|\cdot \mathbb{E}_{z}\left[\left|h^{*}(z)\right|^{2}\right]^{1/2}\mathbb{E}_{x}\left[ \left|(w_{t}-\tilde{w}_{t})^{\top}x\right|^{2}\right]^{1/2}\gamma_{0}(d\theta_{0 },d\tilde{\theta}_{0})\\ &\quad\quad+2K_{\xi}K_{\sigma}\int\int|\tilde{a}_{t}|\cdot\left(C_{f }\Delta(t)\right)^{1/2}\mathbb{E}_{x}\left[\left|(w_{t}-\tilde{w}_{t})^{\top} x\right|^{2}\right]^{1/2}\gamma_{0}(d\theta_{0},d\tilde{\theta}_{0})\\ &\quad\quad+2K_{\xi}K_{\sigma}\iint|\tilde{a}_{t}|\cdot\mathbb{E}_ {z}\left[\left|h^{*}(z)\right|^{2}\right]^{1/2}\mathbb{E}_{x}\left[\left|(w_{t}- \tilde{w}_{t})^{\top}x\right|^{4}\right]^{1/2}\gamma_{0}(d\theta_{0},d \tilde{\theta}_{0})\end{split}\]\[\leq \,2K_{\xi}K_{\sigma}\mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]^{1/2} \iint|a_{t}-\tilde{a}_{t}|\cdot\|w_{t}-\tilde{w}_{t}\|\gamma_{0}(d\theta_{0},d \tilde{\theta}_{0})\] \[\qquad+2K_{\xi}K_{\sigma}\left(K_{\rho}+K_{\xi}K_{\sigma}\mathbb{E }_{z}\left[|h^{*}(z)|^{2}\right]^{1/2}T\right)\left(C_{f}\Delta(t)\right)^{1/2} \int\int\|w_{t}-\tilde{w}_{t}\|\gamma_{0}(d\theta_{0},d\tilde{\theta}_{0})\] \[\qquad\qquad\cdot\iint\sqrt{3}\|w_{t}-\tilde{w}_{t}\|^{2}\gamma_ {0}(d\theta_{0},d\tilde{\theta}_{0})\] \[\leq \,K_{\xi}K_{\sigma}\mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]^{1/ 2}\Delta(t)+2K_{\xi}K_{\sigma}\left(K_{\rho}+K_{\xi}K_{\sigma}\mathbb{E}_{z} \left[|h^{*}(z)|^{2}\right]^{1/2}T\right)C_{f}^{1/2}\Delta(t)\] \[\qquad+2\sqrt{3}K_{\xi}K_{\sigma}\left(K_{\rho}+K_{\xi}K_{\sigma} \mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]^{1/2}T\right)\mathbb{E}_{z}\left[|h ^{*}(z)|^{2}\right]^{1/2}\Delta(t),\]

where we used Remark 3.2, (A.5), Lemma A.1, and \((w-\tilde{w})^{\top}x\sim\mathcal{N}(0,\|w-\tilde{w}\|^{2})\) if \(x\sim\mathcal{N}(0,I_{d})\). Therefore, we can conclude that

\[\frac{d}{dt}\Delta(t)=\frac{d}{dt}\iint|a-\tilde{a}|^{2}\gamma_{t}(d\theta,d \tilde{\theta})+\frac{d}{dt}\iint\|w-\tilde{w}\|^{2}\gamma_{t}(d\theta,d \tilde{\theta})\leq C_{\Delta}\Delta(t),\]

where

\[C_{\Delta}= 1+2K_{\xi}^{2}K_{\sigma}^{2}C_{f}+2K_{\xi}^{2}K_{\sigma}^{2} \mathbb{E}_{x_{V}}[|h^{*}(x_{V})|^{2}]\] \[+K_{\xi}K_{\sigma}\mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]^{1/2 }+2K_{\xi}K_{\sigma}\left(K_{\rho}+K_{\xi}K_{\sigma}\mathbb{E}_{z}\left[|h^{*} (z)|^{2}\right]^{1/2}T\right)C_{f}^{1/2}\] \[+2\sqrt{3}K_{\xi}K_{\sigma}\left(K_{\rho}+K_{\xi}K_{\sigma} \mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]^{1/2}T\right)\mathbb{E}_{z}\left[|h ^{*}(z)|^{2}\right]^{1/2}.\]

This proves (A.6). 

### Proof of Theorem 3.4

The proof of Theorem 3.4 is based on Theorem 3.5 and Theorem 3.6.

Proof of Theorem 3.4.: Let \(\mathcal{P}_{S}:\mathbb{R}^{d+1}\to S\) be the projection in Theorem 3.5 and let \(\mathcal{P}_{S}^{\perp}=I_{d+1}-\mathcal{P}_{S}\). Let \(\tilde{\rho}_{t}\) solve (2.4) with \(\tilde{\rho}_{0}=\mathcal{P}_{S}^{\perp}\rho_{w}\times\delta_{S}\). It is clear that \((\mathcal{P}_{S})_{\#}\tilde{\rho}_{0}=\delta_{S}\). In addition, with the decomposition \(x=x_{1}+x_{2}+x_{3}\) where \(x_{1}=x_{V}^{\perp}=x-x_{V}\), \(x_{2}=x_{V}-x_{S}\), and \(x_{3}=x_{S}\) are independent Gaussian random variables, we have for any \(w\in\mathbb{R}^{d}\) that

\[\mathbb{E}_{x}\left[f^{*}(x)\sigma^{\prime}\left(w^{\top}x_{S}^{\perp}\right) x_{S}\right]=\mathbb{E}_{x_{1}}\mathbb{E}_{x_{2},x_{3}}\left[\left[h^{*}(x_{2}+x_{3}) \sigma^{\prime}\left(w^{\top}x_{1}+w^{\top}x_{2}\right)x_{3}\right]\right]=0,\]

where we used (3.1). Then according to Theorem 3.5, for any \(t\geq 0\) that \((\mathcal{P}_{S})_{\#}\tilde{\rho}_{t}=\delta_{S}\), which implies that \(f_{\text{NN}}(x;\rho_{t})\) is a constant function in \(x_{S}\) for any fixed \(x_{S}^{\perp}\), giving a lowerbound on its loss:

\[\mathcal{E}(\tilde{\rho}_{t}) =\frac{1}{2}\mathbb{E}_{x}\left[\|f^{*}(x)-f_{\text{NN}}(x;\tilde {\rho}_{t})\|^{2}\right]\] \[=\frac{1}{2}\mathbb{E}_{x_{S}^{\perp}}\left[\mathbb{E}_{x_{S}} \left[\|f^{*}(x)-f_{\text{NN}}(x;\tilde{\rho}_{t})\|^{2}\right]\right]\] \[\geq\frac{1}{2}\mathbb{E}_{x_{S}^{\perp}}\left[\mathbb{E}_{x_{S}} \left[\|f^{*}(x)-\mathbb{E}_{x_{S}}[f^{*}(x)]\|^{2}\right]\right]\] \[=\frac{1}{2}\mathbb{E}_{z_{S}^{\perp}}\left[\mathbb{E}_{z_{S}} \left[\|h^{*}(z)-h^{*}_{S^{\perp}}(z_{S}^{\perp})\|^{2}\right]\right]\] \[=\frac{1}{2}\mathbb{E}_{z}\left[\|h^{*}(z)-h^{*}_{S^{\perp}}(z_{ S}^{\perp})\|^{2}\right],\]

where \(z_{S}=\mathcal{P}_{S}^{V}z\), \(z_{S}^{\perp}=z-z_{S}\), and \(h^{*}_{S^{\perp}}(z_{S}^{\perp})=\mathbb{E}_{z_{S}}[h^{*}(z)]\).

Now we show the actual flow is not very different. For \(\rho_{0}=\rho_{a}\times\rho_{w}\) with \(\rho_{w}\sim\mathcal{N}(0,I_{d})\) and \(\tilde{\rho}_{0}=\mathcal{P}_{S}^{\perp}\rho_{w}\times\delta_{S}\), it can be estimated that

\[W_{2}^{2}(\rho_{0},\tilde{\rho}_{0})\leq\frac{\dim S}{d}\leq\frac{p}{d}.\]Then applying Corollary A.3, we can conclude for any \(T>0\) that

\[\inf_{0\leq t\leq T}\mathcal{E}(\tilde{\rho}_{t}) \geq\inf_{0\leq t\leq T}\mathcal{E}(\tilde{\rho}_{t})-\left(C_{s} \mathbb{E}_{z}\left[|h^{*}(z)|^{2}\right]\right)^{1/2}W_{2}(\rho_{0},\tilde{\rho }_{0})-\frac{1}{2}C_{s}W_{2}^{2}(\rho_{0},\tilde{\rho}_{0})\] \[\geq\frac{1}{2}\mathbb{E}_{z}\left[\|h^{*}(z)-h_{S^{\perp}}^{*}(z _{S}^{\perp})\|^{2}\right]-\frac{\left(pC_{s}\mathbb{E}_{z}\left[|h^{*}(z)|^{2 }\right]\right)^{1/2}}{d^{1/2}}-\frac{pC_{s}}{2d},\]

where \(C_{s}>0\) is the constant in Theorem 3.6 depending only on \(p\), \(h^{*}\), \(K_{\sigma}\), \(K_{\xi}\), \(K_{\rho}\), and \(T\). Therefore, we can obtain (3.3) and (3.4) immediately. 

## Appendix B Further Discussion and Characterization of the Reflective Property and Theorem 3.4

### Equivalence between (3.2) and \(\text{isoLeap}(h^{*})\geq 2\)

We prove the following equivalence where \(\text{isoLeap}(h^{*})\) is the isotropic leap complexity defined in [2, Appendix B.2].

**Proposition B.1**.: _For any polynomial \(h^{*}:V\to\mathbb{R}\), then it satisfies (3.2) with some nontrivial subspace \(S\subset V\) if and only if \(\text{isoLeap}(h^{*})\geq 2\)._

Proof.: Without loss of generality, we assume that \(V=\mathbb{R}^{p}\). Suppose that \(\text{isoLeap}(h^{*})\geq 2\), which means that the leap complexity of \(h^{*}\) (as defined in [2, Definition 1]) is greater than one for some orthonormal basis of \(V\). We can assume that the basis is \(\{e_{1},e_{2},\dots,e_{p}\}\), where \(e_{j}\) is the vector in \(\mathbb{R}^{p}\) with the \(j\)-th entry being \(1\) and other entries being \(0\). Denote the Hermite decomposition of \(h^{*}\) as

\[h^{*}(z)=\sum_{i=1}^{m}c_{i}\prod_{j=1}^{p}\text{He}_{\alpha_{i}(j)}(z_{j}),\] (B.1)

where \(c_{1},c_{2},\dots,c_{m}\) are nonzero coefficients, \(\alpha_{1},\alpha_{2},\dots,\alpha_{m}\) are pairwise distinct elements in \(\mathbb{N}^{p}\) with \(\alpha_{i}(j)\) being the \(j\)-th entry of \(\alpha_{i}\), and \(\text{He}_{k}\) is the \(k\)-th order Hermite polynomial. Since the leap complexity of \(h^{*}\) is at least two, the following is true after applying some permutation on \(\{1,2,\dots,m\}\): There exists some \(m_{1}\in\{1,2,\dots,m-1\}\), such that it holds for any \(i\in\{m_{1}+1,\dots,m\}\) that

\[\sum_{j\in J_{m_{1}}}\alpha_{i}(j)\geq 2,\] (B.2)

where

\[J_{m_{1}}=\{j\in\{1,2,\dots,p\}:\alpha_{i}(j)=0,\;\forall\;i\in\{1,2,\dots,m_ {1}\}\}.\]

Thus, by the orthogonality of Hermite polynomials, we have for \(i\in\{m_{1}+1,\dots,m\}\) that

\[\mathbb{E}_{z_{j}\sim\mathcal{N}(0,1)}\left[z_{j}\prod_{j=1}^{p}\text{He}_{ \alpha_{i}(j)}(z_{j})\right]=0,\quad\forall\;j\in J_{m_{1}}.\]

The above also holds for \(i\in\{1,2,\dots,m_{1}\}\) by the definition of \(J_{m_{1}}\). Therefore, we can conclude that

\[\mathbb{E}_{z_{S}\sim\mathcal{N}(0,I_{S})}[h^{*}(z)z_{S}]=0,\quad\forall\;z_ {S}^{\perp},\]

i.e., (3.2) holds, for \(S=\text{span}\{e_{j}:j\in J_{m_{1}}\}\). Moreover, \(S\) is nontrivial since (B.2) implies \(J_{m_{1}}\) is not empty.

On the other hand, suppose that (3.2) is satisfied with some nontrivial subspace \(S\subset V\) that can be assumed as \(S=\{(z_{1},\dots,z_{p_{1}},0,\dots,0):z_{1},\dots,z_{p_{1}}\in\mathbb{R}\}\) with \(1\leq p_{1}\leq p\). We still consider the Hermite decomposition as in (B.1) and rewrite it as

\[h^{*}(z)=\sum_{i=1}^{m_{2}}c_{i}^{\prime}\prod_{j=1}^{p_{1}}\text{He}_{\alpha_{ i}^{\prime}(j)}(z_{j})h_{i}(z_{p_{1}+1},\dots,z_{p}),\]where \(c^{1}_{1},c^{\prime}_{2},\ldots,c^{\prime}_{m_{2}}\) are nonzero coefficients, \(\alpha^{\prime}_{1},\alpha^{\prime}_{2},\ldots,\alpha^{\prime}_{m_{2}}\) are pairwise distinct elements in \(\mathbb{N}^{p_{1}}\), and \(h_{1},h_{2},\ldots,h_{m_{2}}\) are nonzero polynomials defined on \(\mathbb{R}^{p-p^{\prime}}\). Then it follows from (3.2) that

\[\sum_{i=1}^{m_{2}}c^{\prime}_{i}h_{i}(z_{p_{1}+1},\ldots,z_{p})\mathbb{E}_{z_{j }\sim\mathcal{N}(0,1)}\left[z_{j}\text{He}_{\alpha^{\prime}_{i}(j)}(z_{j}) \right]\prod_{j^{\prime}=1,j^{\prime}\neq j}^{p_{1}}\mathbb{E}_{z_{j^{\prime}} \sim\mathcal{N}(0,1)}\text{He}_{\alpha^{\prime}_{i}(j^{\prime})}(z_{j^{\prime }})=0,\]

for any \(j\in\{1,2,\ldots,p_{1}\}\) and \(z_{p_{1}+1},\ldots,z_{p}\in\mathbb{R}\), which implies that

\[\sum_{j=1}^{p_{1}}\alpha_{i}(j)^{\prime}\neq 1,\quad\forall\;i\in\{1,2, \ldots,m_{2}\}.\]

Therefore, the leap complexity of \(h^{*}\) is at least \(2\) with respect to this basis, which leads to \(\text{isoLeap}(h^{*})\geq 2\). 

### Discretization Results Implied by Theorem 3.4

We discuss the sample complexity result of SGD implied by Theorem 3.4 in this subsection. Recall that there have been standard dimension-free results for bounding the distance between SGD and the mean-field dynamics; see e.g., [19]. So the result in this subsection is somehow a direct corollary. However, one needs to make minor modifications to guarantee that all boundedness assumptions in [19] are satisfied.

Given a constant \(C^{b}_{f}>0\), define

\[\tilde{f}^{*}(x)=\text{sign}(f^{*}(x))\min\{|f^{*}(x),C^{b}_{f}|\},\] (B.3)

which is bounded with \(|\tilde{f}^{*}(x)|\leq C^{b}_{f}\). One observation is that the subspace-sparse structure of \(f^{*}\) implies that for any \(\delta>0\), there exists a dimension-free constant \(C^{b}_{f}\) depending on \(h^{*}\) and \(\delta\) such that \(\mathbb{E}_{x\sim\mathcal{N}(0,I_{d})}[|f^{*}(x)-\tilde{f}^{*}(x)|^{2}]<\delta\). The associated mean-field dynamics is

\[\begin{cases}\partial_{t}\tilde{\rho}_{t}=\nabla_{\theta}\cdot\left(\tilde{ \rho}_{t}\xi(t)\nabla_{\theta}\tilde{\Phi}(\theta;\tilde{\rho}_{t})\right),\\ \tilde{\rho}_{t}\big{|}_{t=0}=\rho_{0},\end{cases}\] (B.4)

where the learning rate \(\xi(t)=\text{diag}(\xi_{a}(t),\xi_{w}(t)I_{d})\) and the initialization \(\rho_{0}\) are shared with (2.4), and

\[\tilde{\Phi}(\theta;\rho)=a\mathbb{E}_{x\sim\mathcal{N}(0,I_{d})}\left[\left(f _{\text{NN}}(x;\rho)-\tilde{f}^{*}(x)\right)\sigma(w^{\top}x)\right].\]

The corresponding SGD is given by

\[\theta_{i}^{(k+1)}=\theta_{i}^{(k)}+\gamma^{(k)}\left(\tilde{f}^{*}(x_{k})-f_{ \text{NN}}(x_{k};\Theta^{(k)})\right)\nabla_{\theta}\tau(x_{k};\theta_{i}^{(k) }),\quad i=1,2,\ldots,N,\] (B.5)

where \(N\) is the number of neurons and \(\gamma^{(k)}=\text{diag}(\gamma^{(k)}_{a},\gamma^{(k)}_{w}I_{d})\succeq 0\) is the learning rate with \(\gamma^{(k)}_{a}=\epsilon\xi_{a}(k\epsilon)\) and \(\gamma^{(k)}_{w}=\epsilon\xi_{w}(k\epsilon)\).

Suppose that assumptions made in Theorem 3.4 hold and fix \(T>0\). Using similar analysis as in Appendix A.2, one can conclude that for any \(\delta>0\), there exists a dimension-free constant \(C^{b}_{f}\) such that

\[\sup_{0\leq t\leq T}|\mathcal{E}(\rho_{t})-\mathcal{E}(\tilde{\rho}_{t})|<\delta.\]

Applying Theorem 3.4 and [19, Theorem 1], we can conclude that for any \(\mu\in(0,1)\), there exists dimension-free constants \(N_{0},d_{0},C_{\epsilon}\), such that for any \(N\geq N_{0}\) and \(d\geq d_{0}\), the following holds with probability at least \(\mu\) for any \(\epsilon\leq\frac{C_{\epsilon}}{d+\log N}\):

\[\inf_{k\in[0,T/\epsilon]\cap\mathbb{N}}\mathcal{E}_{N}(\Theta^{(k)})\geq\frac {1}{8}\mathbb{E}_{z\sim\mathcal{N}(0,I_{V})}\left[|h^{*}(z)-h^{*}_{S^{\perp}}( z_{S}^{\perp})|^{2}\right]>0.\]

If we further assume that \(N=\mathcal{O}(e^{d})\), this indicates that SGD as in (B.5) cannot learn the subspace-sparse polynomial \(f^{*}\) within finite time horizon and with \(\mathcal{O}(d)\) samples/data points.

Proofs for Section 4

### Approximation of \(w(a,t)\) by Polynomials

This subsection follows [1] closely to approximate and analyze the behavior of \(w(a,t)\) for \(0\leq t\leq T\) with \(\xi_{a}(t)=0\) and \(\xi_{w}(t)=1\). The dynamics of a single particle starting at \(\theta=(a,0)\in\mathbb{R}^{d+1}\) can be described by the following ODE:

\[\begin{cases}\frac{\partial}{\partial t}w(a,t)=a\mathbb{E}_{x}\left[g(x,t) \sigma^{\prime}(w(a,t)^{\top}x)x\right],\\ w(a,0)=0,\end{cases}\] (C.1)

where

\[g(x,t)=f^{*}(x)-f_{\text{NN}}(x;\rho_{t})\]

is the residual. The first observation is that

\[\mathbb{E}_{x}\left[f^{*}(x)\sigma(w^{\top}x_{V})x_{V}^{\downarrow}\right]= \mathbb{E}_{x_{V}}\left[h^{*}(x_{V})\sigma(w^{\top}x_{V})\mathbb{E}_{x_{V}^{ \downarrow}}\left[x_{V}^{\downarrow}\right]\right]=0,\quad\forall\ w\in\mathbb{ R}^{d}.\]

By Theorem 3.5, we have that \(\rho_{t}\) is supported in \(\left\{(a,w):w_{V}^{\downarrow}=0\right\}\), and hence that

\[w_{V}^{\downarrow}(a,t)=0,\quad\forall\ 0\leq t\leq T.\]

We then analyze the behaviour of \(w_{V}(a,t)\). Let \(\{e_{1},e_{2},\ldots,e_{p}\}\) be an orthonormal basis of \(V\) and we denote \(w_{i}=w^{\top}e_{i}\) and \(x_{i}=x^{\top}e_{i}\) for any \(w,x\in\mathbb{R}^{d}\) and \(i\in\{1,2,\ldots,p\}\).

By Assumption 4.2, it holds that

\[\sigma^{\prime}\left(w(a,t)^{\top}x\right)=m_{1}+\sum_{l=1}^{L-1}\frac{m_{l+1 }}{l!}(w(a,t)^{\top}x)^{l}+\mathcal{O}\left((w(a,t)^{\top}x)^{L}\right)\] (C.2)

with \(m_{l}=\sigma^{(l)}(0)\) and then an approximated solution to (C.1) (by polynomial expansion with high-order terms omitted) can be written as

\[\tilde{w}_{i}(a,t)=\sum_{1\leq j\leq L}Q_{i,j}(t)a^{j},\quad 1\leq i\leq p,\]

where \(Q(t)\) is given by \(Q(0)=0\) and the following dynamics:

\[\begin{cases}\frac{d}{dt}Q_{i,1}(t)=\mathbb{E}_{x}[x_{i}g(x,t)m_{1}],\\ \frac{d}{dt}Q_{i,j}(t)=\mathbb{E}_{x}\left[x_{i}g(x,t)\sum_{l=1}^{L-1}\frac{m _{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+\cdots+j_{l}=j-1} \prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}}\right],\ \ 2\leq j\leq L.\end{cases}\] (C.3)

Let us remark that even if every single \(\tilde{w}_{i}(a,t)\) depends on the basis \(\{e_{1},e_{2},\ldots,e_{p}\}\), the linear combination

\[\tilde{w}(a,t)=\sum_{i=1}^{p}e_{i}\tilde{w}_{i}(a,t)\] (C.4)

is basis-independent. To see this, let \(Q_{j}(t)\in\mathbb{R}^{d}\) be the \(j\)-th column of \(Q(t)\) and it holds that

\[\begin{cases}\frac{d}{dt}Q_{1}(t)=\mathbb{E}_{x}[xg(x,t)m_{1}],\\ \frac{d}{dt}Q_{j}(t)=\mathbb{E}_{x}\left[xg(x,t)\sum_{l=1}^{L-1}\frac{m_{l+1 }}{l!}\sum_{j_{1}+\cdots+j_{l}=j-1}\prod_{s=1}^{l}x^{\top}Q_{j_{s}}(t)\right],\end{cases}\]

The distance between \(w_{i}(a,t)\) and \(\tilde{w}_{i}(a,t)\) for \(i\in I\) can be bounded as follows.

**Proposition C.1**.: _Suppose that Assumption 4.2 holds. Then_

\[|w_{i}(a,t)-\tilde{w}_{i}(a,t)|=\mathcal{O}((|a|t)^{L+1}),\quad\forall\ i\in \{1,2,\ldots,p\}.\] (C.5)

We need the following two lemmas to prove Proposition C.1.

**Lemma C.2**.: _For any \(i\in\{1,2\ldots,p\}\) and \(j\in\{1,2,\ldots,L\}\), it holds that_

\[Q_{i,j}(t)=\mathcal{O}(t^{j}).\] (C.6)

Proof.: The non-increasing property of the energy functional implies that

\[2\mathcal{E}(\rho_{t})=\mathbb{E}_{x}[|g(x,t)|^{2}]\leq\mathbb{E}_{x}[|f^{*}(x )-f_{\text{NN}}(x,\rho_{0})|^{2}]=\mathbb{E}_{z}[|h^{*}(z)|^{2}]<+\infty.\]

Then (C.6) can be proved by induction. For \(j=1\), it follows from the boundedness of

\[\left|\frac{d}{dt}Q_{i,1}(t)\right|=|\mathbb{E}_{x}[x_{i}g(x,t)m_{1}]|\leq|m_{ 1}|\left(\mathbb{E}_{x}[x_{i}^{2}]\cdot\mathbb{E}_{x}[|g(x,t)|^{2}]\right)^{1/2}\]

that \(Q_{i,e_{l}}(t)=\mathcal{O}(t)\). Consider any \(2\leq j\leq L\) and assume that \(Q_{i,j^{\prime}}=\mathcal{O}(t^{j^{\prime}})\) holds for any \(1\leq i\leq p\) and \(1\leq j^{\prime}<j\). Then one has that

\[\left|\frac{d}{dt}Q_{i,j}(t)\right|\] \[\leq \left(\mathbb{E}_{x}[|g(x,t)|^{2}]\cdot\mathbb{E}_{x}\left[\left| x_{i}\sum_{l=1}^{L-1}\frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p\,j_{1 }+\cdots+j_{l}=j-1}\prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}}\right|^{2}\right] \right)^{1/2}\] \[= \mathcal{O}(t^{j-1}),\]

which implies that \(Q_{i,j}(t)=\mathcal{O}(t^{j})\). 

**Lemma C.3**.: _Suppose that Assumption 4.2 holds. We have for all \(i\in\{1,2,\ldots,p\}\) that_

\[w_{i}(a,t)=\mathcal{O}(|a|t).\] (C.7)

Proof.: There exists a constant \(C>0\) and a open subset \(A\subset\{w\in\mathbb{R}^{d}:w_{V}^{\perp}=0\}\) containing \(0\), such that

\[\left|\mathbb{E}_{x}\left[x_{i}g(x,t)\sigma^{\prime}(w^{\top}x)\right]\right| \leq C,\quad\forall\;w\in A,\;t\geq 0,\;i\in\{1,2,\ldots,p\}.\]

Thus, we have

\[\left|\frac{\partial}{\partial t}w_{i}(a,t)\right|\leq|a|,\]

as long as \(w(a,t)\) does not leave \(A\). This implies (C.7). 

Now we can proceed to prove Proposition C.1.

Proof of Proposition c.1.: Set \(\tilde{w}_{V}^{\perp}(a,t)=0\). It can be estimated for any \(i\in\{1,2,\ldots,p\}\) that

\[\left|\frac{\partial}{\partial t}\tilde{w}_{i}(a,t)-a\mathbb{E}_{ x}\left[x_{i}g(x,t)\left(m_{1}+\sum_{l=1}^{L-1}\frac{m_{l+1}}{l!}(\tilde{w}(a,t)^{ \top}x)^{l}\right)\right]\right|\] \[\leq \left|\sum_{1\leq j\leq L}\frac{d}{dt}Q_{i,j}(t)a^{j}-a\mathbb{E} _{x}\left[x_{i}g(x,t)\left(m_{1}+\sum_{l=1}^{L-1}\frac{m_{l+1}}{l!}\left( \sum_{1\leq i^{\prime}\leq p}\sum_{1\leq j\leq L}Q_{i^{\prime},j}(t)a^{j}x_{i^ {\prime}}\right)^{l}\right)\right]\right|\] \[\leq \sum_{L+1\leq j\leq L^{L-1}+1}\left|a^{j}\cdot\mathbb{E}_{x} \left[x_{i}g(x,t)\sum_{l=1}^{L-1}\frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_ {l}\leq p\,j_{1}+\cdots+j_{l}=j-1}\prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}} \right]\right|\] \[\leq \sum_{L+1\leq j\leq L^{L-1}+1}|a|^{j}\] \[\qquad\qquad\cdot\left(\mathbb{E}[|g(x,t)|^{2}]\cdot\mathbb{E}_{ x}\left[\left|x_{i}\sum_{l=1}^{L-1}\frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_ {l}\leq p\,j_{1}+\cdots+j_{l}=j-1}\prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}} \right|^{2}\right]\right)^{1/2}\]\[= \mathcal{O}(|a|^{L+1}t^{L}),\]

which combined with (C.2) yields that

\[\frac{\partial}{\partial t}\sum_{i=1}^{p}|w_{i}(a,t)-\tilde{w}_{i}( a,t)|\] \[\leq \sum_{i=1}^{p}\left|\frac{\partial}{\partial t}w_{i}(a,t)-\frac{ \partial}{\partial t}\tilde{w}_{i}(a,t)\right|\] \[\leq \sum_{i=1}^{p}\left|a\mathbb{E}_{x}\left[x_{i}g(x,t)\left(m_{1}+ \sum_{l=1}^{L-1}\frac{m_{l+1}}{l!}(w(a,t)^{\top}x)^{l}\right)\right]\right.\] \[\left.\qquad\qquad-a\mathbb{E}_{x}\left[x_{i}g(x,t)\left(m_{1}+ \sum_{l=1}^{L-1}\frac{m_{l+1}}{l!}(\tilde{w}(a,t)^{\top}x)^{l}\right)\right] \right|\] \[\left.\qquad\qquad+|a\mathbb{E}_{x}\left[x_{i}g(x,t)\right]|\cdot \mathcal{O}\left((w(a,t)^{\top}x)^{L}\right)+\mathcal{O}\left(|a|^{L+1}t^{L}\right)\] \[\leq \sum_{i=1}^{p}\left|\mathbb{E}_{x}\left[x_{i}a^{\top}g(x,t)\sum _{l=1}^{L-1}\frac{m_{l+1}}{l!}\left((w(a,t)^{\top}x)^{l}-(\tilde{w}(a,t)^{ \top}x)^{l}\right)\right]\right|+\mathcal{O}\left(|a|^{L+1}t^{L}\right)\] \[= \mathcal{O}\left(\sum_{i=1}^{p}|w_{i}(a,t)-\tilde{w}_{i}(a,t)| \right)+\mathcal{O}\left(|a|^{L+1}t^{L}\right).\]

Then one can conclude (C.5) from Gronwall's inequality. 

Even if \(\tilde{w}(a,t)\) approximates \(w(a,t)\) using polynomial expansion, the coefficients \(Q(t)\) are still very difficult to analyze. Thus, we follow [1] to consider the following dynamics that is obtained by replacing \(g(x,t)=f^{*}(x)-f_{\text{NN}}(x;\rho_{t})\) by \(f^{*}(x)\) in (C.3):

\[\hat{w}_{i}(a,t)=\sum_{1\leq j\leq L}\hat{Q}_{i,j}(t)a^{j},\quad i\in\{1,2, \ldots,p\},\] (C.8)

where \(\hat{Q}(t)\) is given by \(\hat{Q}(0)=0\) and

\[\begin{cases}\frac{d}{dt}\hat{Q}_{i,1}(t)=\mathbb{E}_{x}[x_{i}f^{*}(x)m_{1}], \\ \frac{d}{dt}\hat{Q}_{i,j}(t)=\mathbb{E}_{x}\left[x_{i}f^{*}(x)\sum_{l=1}^{L-1} \frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+\cdots+j_{l }=j-1}\prod_{s=1}^{l}\hat{Q}_{i_{s},j_{s}}(t)x_{i_{s}}\right],\ \ 2\leq j\leq L.\end{cases}\] (C.9)

Similar to \(\tilde{w}(a,t)\), the linear combination \(\hat{w}(a,t)\) defined as

\[\hat{w}(a,t)=\sum_{i=1}^{p}e_{i}\hat{w}_{i}(a,t)\]

is also independent of the orthogonal basis \(\{e_{1},e_{2},\ldots,e_{p}\}\). \(\hat{Q}(t)\) can be understood clearly as follows.

**Proposition C.4**.: _For any \(i\in\{1,2,\ldots,p\}\) and \(j\in\{1,2,\ldots,L\}\), there exists a constant \(\hat{q}_{i,j}\) depending only on \(h^{*}\) and \(\sigma\), such that_

\[\hat{Q}_{i,j}(t)=\hat{q}_{i,j}t^{j}.\] (C.10)

Proof.: The proof is straightforward by induction on \(j\). 

The next proposition quantifies the distance between \(\hat{Q}(t)\) and \(Q(t)\).

**Proposition C.5**.: _It holds for any \(i\in\{1,2,\ldots,p\}\) and \(j\in\{1,2,\ldots,L\}\) that_

\[|Q_{i,j}(t)-\hat{Q}_{i,j}(t)|=\mathcal{O}(t^{j+1}).\] (C.11)We need the following lemma for the proof of Proposition C.5.

**Lemma C.6**.: _It holds that_

\[\left(\mathbb{E}_{x}\left[|f_{\text{NN}}(x;\rho_{t})|^{2}\right]\right)^{1/2}= \mathcal{O}(t).\] (C.12)

Proof.: Noticing that \(f_{\text{NN}}(x;\rho_{0})=0\) by the symmetry of \(\rho_{a}\), one has that

\[f_{\text{NN}}(x;\rho_{t}) =f_{\text{NN}}(x;\rho_{t})-f_{\text{NN}}(x;\rho_{0})\] \[=\int a\left(\sigma(w(a,t)^{\top}x)-\sigma(0)\right)\rho_{a}(da)\] \[=\int a\left(\sum_{l=1}^{L}\frac{m_{l}}{l!}(w(a,t)^{\top}x)^{l}+ \mathcal{O}\left((w(a,t)^{\top}x)^{L+1}\right)\right)\rho_{a}(da),\]

and hence by Lemma C.3 and \(w_{\mathcal{V}}^{\downarrow}(a,t)=0\) that

\[\mathbb{E}_{x}\left[|f_{\text{NN}}(x,\rho_{t})|^{2}\right]=\mathcal{O}(t^{2}),\]

which implies (C.12). 

Proof of Proposition c.5.: We prove (C.11) by introduction on \(j\). For \(j=1\), one has

\[\left|\frac{d}{dt}Q_{i,1}(t)-\frac{d}{dt}\hat{Q}_{i,1}(t)\right|=|\mathbb{E}_{ x}[x_{i}f_{\text{NN}}(x,\rho_{t})m_{1}]|=|m_{1}|\left(\mathbb{E}_{x}[x_{i}^{2}] \cdot\mathbb{E}_{x}[|f_{\text{NN}}(x,\rho_{t})|^{2}]\right)^{1/2}=\mathcal{O} (t),\]

which leads to \(|Q_{i,e_{t}}(t)-\hat{Q}_{i,e_{t}}(t)|=\mathcal{O}(t^{2})\). Then we consider \(2\leq j\leq L\) and assume that \(|Q_{i,j^{\prime}}(t)-\hat{Q}_{i,j^{\prime}}(t)|=\mathcal{O}(t^{j^{\prime}+1})\) holds for \(1\leq j^{\prime}<j\). It can be estimated that

\[\left|\frac{d}{dt}Q_{i,j}(t)-\frac{d}{dt}\hat{Q}_{i,j}(t)\right|\] \[= \left|\mathbb{E}_{x}\left[x_{i}g(x,t)\sum_{l=1}^{L-1}\frac{m_{l+1 }}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+\cdots+j_{l}=j-1}\prod _{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}}\right]\right.\] \[\left.\qquad-\mathbb{E}_{x}\left[x_{i}f^{*}(x)\sum_{l=1}^{L-1} \frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+\cdots+j_{l} =j-1}\prod_{s=1}^{l}\hat{Q}_{i_{s},j_{s}}(t)x_{i_{s}}\right]\right|\] \[= \left|\mathbb{E}_{x}\left[x_{i}f_{\text{NN}}(x;\rho_{t})\sum_{l=1 }^{L-1}\frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+ \cdots+j_{l}=j-1}\prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}}\right]\right.\] \[\left.\qquad-\mathbb{E}_{x}\left[x_{i}f^{*}(x)\sum_{l=1}^{L-1} \frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+\cdots+j_{l} =j-1}\left(\prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)-\prod_{s=1}^{l}\hat{Q}_{i_{s},j_ {s}}(t)\right)x_{i_{s}}\right]\right|\] \[= \left|\mathbb{E}_{x}\left[x_{i}f_{\text{NN}}(x,\rho_{t})\sum_{l=1 }^{L-1}\frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+ \cdots+j_{l}=j-1}\prod_{s=1}^{l}\mathcal{O}(t^{j_{s}})x_{i_{s}}\right]\right.\] \[\left.\qquad-\mathbb{E}_{x}\left[x_{i}f^{*}(x)\sum_{l=1}^{L-1} \frac{m_{l+1}}{l!}\sum_{1\leq i_{1},\ldots,i_{l}\leq p}\sum_{j_{1}+\cdots+j_{l} =j-1}\right.\] \[\left.\qquad\qquad\qquad\qquad\qquad\qquad\left.\left(\prod_{s=1 }^{l}\left(\hat{q}_{i_{s},j_{s}}t^{j_{s}}+\mathcal{O}(t^{j_{s}+1})\right)- \prod_{s=1}^{l}\hat{q}_{i_{s},j_{s}}t^{j_{s}}\right)x_{i_{s}}\right]\right|\] \[= \mathcal{O}(t^{j}),\]

where we used Lemma C.6. Then one can conclude (C.11).

### From Linear Independence to Algebraic Independence

We prove Theorem 4.5 in this subsection.

**Definition C.7** (Algebraic independence).: _Let \(v_{1},v_{2},\ldots,v_{m}\in\mathbb{R}[a_{1},a_{2},\ldots,a_{p}]\) be polynomials in \(a_{1},a_{2},\ldots,a_{p}\). We say that \(v_{1},v_{2},\ldots,v_{m}\) are algebraically independent if for any nonzero polynomial \(F:\mathbb{R}^{m}\to\mathbb{R}\),_

\[F(v_{1}(a_{1},a_{2},\ldots,a_{p}),\ldots,v_{m}(a_{1},a_{2},\ldots,a_{p}))\neq 0 \in\mathbb{R}[a_{1},a_{2},\ldots,a_{p}].\]

**Lemma C.8**.: _If \(v_{1},v_{2},\ldots,v_{p}\in\mathbb{R}[a]\) are \(\mathbb{R}\)-linearly independent, then_

\[\det\begin{pmatrix}v_{1}(a_{1})&v_{1}(a_{2})&\cdots&v_{1}(a_{p})\\ v_{2}(a_{1})&v_{2}(a_{2})&\cdots&v_{2}(a_{p})\\ \vdots&\vdots&\ddots&\vdots\\ v_{p}(a_{1})&v_{p}(a_{2})&\cdots&v_{p}(a_{p})\end{pmatrix}\] (C.13)

_is a non-zero polynomial in \(\mathbb{R}[a_{1},a_{2},\ldots,a_{p}]\)._

Proof.: Let \(n_{i}\) be the smallest degree of nonzero monomials of \(v_{i}\) and let \(c_{i}\) be the accociated coefficient for \(i=1,2,\ldots,p\). Without loss of generality, we assume that \(n_{0}<n_{1}<\cdots<n_{p}\) (otherwise one can perform some row reductions or row permutations). The polynomial defined in (C.13) consists of monomials of degree at least \(n_{1}+n_{2}+\cdots+n_{p}\). So it suffices to prove that the sum of monomials with degree being \(n_{1}+n_{2}+\cdots+n_{p}\) is nonzero, which is true since

\[\det\begin{pmatrix}c_{1}a_{1}^{n_{1}}&c_{1}a_{1}^{n_{1}}&\cdots&c_{1}a_{1}^{n_ {1}}\\ c_{2}a_{1}^{n_{2}}&c_{2}a_{2}^{n_{2}}&\cdots&c_{2}a_{p}^{n_{2}}\\ \vdots&\vdots&\ddots&\vdots\\ c_{p}a_{1}^{n_{p}}&c_{p}a_{2}^{n_{p}}&\cdots&c_{p}a_{p}^{n_{p}}\end{pmatrix}= c_{1}c_{2}\ldots c_{p}\cdot\det\begin{pmatrix}a_{1}^{n_{1}}&a_{1}^{n_{1}}& \cdots&a_{p}^{n_{1}}\\ a_{1}^{n_{2}}&a_{2}^{n_{2}}&\cdots&a_{p}^{n_{2}}\\ \vdots&\vdots&\ddots&\vdots\\ a_{1}^{n_{p}}&a_{2}^{n_{p}}&\cdots&a_{p}^{n_{p}}\end{pmatrix}\]

is nonzero as a generalized Vandermonde matrix. 

Proof of Theorem 4.5.: Since \(v_{1},v_{2},\ldots,v_{p}\in\mathbb{R}[a]\) be \(\mathbb{R}\)-linearly independent with the constant terms being zero, we can see that \(v_{1}^{\prime},v_{2}^{\prime},\ldots,v_{p}^{\prime}\in\mathbb{R}[a]\) are also \(\mathbb{R}\)-linearly independent. Noticing that

\[\frac{\partial}{\partial a_{j}}\left(\frac{1}{p}(v_{i}(a_{1})+v_{i}(a_{2})+ \cdots+v_{i}(a_{p}))\right)=\frac{1}{p}v_{i}^{\prime}(a_{j}),\]

one can conclude that \(\frac{1}{p}(v_{1}(a_{1})+\cdots+v_{1}(a_{p})),\ldots,\frac{1}{p}(v_{p}(a_{1}) +\cdots+v_{p}(a_{p}))\in\mathbb{R}[a_{1},\ldots,a_{p}]\) are \(\mathbb{R}\)-algebraically independent by using Theorem 4.6 and Lemma C.8. 

### Proofs of Proposition 4.4 and Theorem 4.3

Some ideas in this subsection are from [1], but the proofs are significantly different since we need to show the algebraic independence to obtain a non-degenerate kernel, as discussed in Section 4.

**Lemma C.9**.: _Suppose that Assumption 4.1 holds with \(s\in\mathbb{N}_{+}\). There exists some orthonormal basis \(\{e_{1},e_{2},\ldots,e_{p}\}\) of \(V\) such that the coefficients \(\hat{q}_{i,j},1\leq i\leq p,1\leq j\leq L\) in (C.10) satisfies_

\[s_{1}<s_{2}<\cdots<s_{p}\leq s,\] (C.14)

_where_

\[s_{i}=\min\{j:\hat{q}_{i,j}\neq 0\},\quad i=1,2,\ldots,p.\]

Proof.: Let \(\hat{w}_{V}(t)\) be the dynamics defined in (4.1). It can be seen that the Taylor's expansion of \(\hat{w}_{V}(t)\) at \(t=0\) up to \(s\)-th order is given by

\[\sum_{i=1}^{p}\sum_{j=1}^{s}e_{i}\hat{Q}_{i,j}(t)=\sum_{i=1}^{p}\sum_{j=1}^{s}e _{i}\hat{q}_{i,j}t^{j},\]

where \(\hat{Q}_{i,j}\) and \(\hat{q}_{i,j}\) are as in (C.9) and (C.5). According to Assumption 4.1, the matrix \((\hat{q}_{i,j})_{1\leq i\leq p,1\leq j\leq s}\) is of full-row-rank. One can thus perform the QR decomposition, or equivalently choose some orthogonal basis, to obtain (C.14).

In the rest of this subsection, we will always denote \(\mathbf{s}=(s_{1},s_{2},\ldots,s_{P})\) and

\[u(a_{1},\ldots,a_{p},t) =\frac{1}{p}(w(a_{1},t)+w(a_{2},t)+\cdots+w(a_{p},t)),\] \[\tilde{u}(a_{1},\ldots,a_{p},t) =\frac{1}{p}(\tilde{w}(a_{1},t)+\tilde{w}(a_{2},t)+\cdots+\tilde{ w}(a_{p},t)),\] \[\hat{u}(a_{1},\ldots,a_{p},t) =\frac{1}{p}(\hat{w}(a_{1},t)+\hat{w}(a_{2},t)+\cdots+\hat{w}(a_{p },t)),\]

for \(t\in[0,T]\), where \(w(a,t)\), \(\tilde{w}(a,t)\), and \(\hat{w}(a,t)\) are defined in (C.1), (C.4), and (C.8), respectively. Recall that \(p_{1},p_{2},\ldots,p_{\binom{n+p}{p}}\) are the orthonormal basis of \(\mathbb{P}_{V,n}\) with input \(z\sim\mathcal{N}(0,I_{V})\), where \(\mathbb{P}_{V,n}\) is the collection of all polynomials on \(V\) with degree at most \(n=\text{deg}(h^{*})=\text{deg}(f^{*})\). Proposition 4.4 aims to bound from below the smallest eigenvalue of the kernel matrix (4.2) whose definition is restated as follows

\[\mathcal{K}_{i_{1},i_{2}}(t)=\mathbb{E}_{a_{1},\ldots,a_{p}}\left[\mathbb{E}_ {z,z^{\prime}}\left[p_{i_{1}}(z)\hat{\sigma}(u(a_{1},\ldots,a_{p},t)^{\top}z) \hat{\sigma}(u(a_{1},\ldots,a_{p},t)^{\top}z^{\prime})p_{i_{2}}(z^{\prime}) \right]\right],\]

where \(\hat{\sigma}(\xi)=(1+\xi)^{n}\), \((a_{1},\ldots,a_{p})\sim\mathcal{U}([-1,1]^{p})\), and \(1\leq i_{1},i_{2}\leq\binom{n+p}{p}\). To do this, we define three \(\binom{n+p}{p}\times\binom{n+p}{p}\) matrices

\[\tilde{\mathcal{K}}_{i_{1},i_{2}}(t)=\mathbb{E}_{a_{1},\ldots,a_{p}}\left[ \mathbb{E}_{z,z^{\prime}}\left[p_{i_{1}}(z)\hat{\sigma}(\tilde{u}(a_{1},\ldots,a_{p},t)^{\top}z)\hat{\sigma}(\tilde{u}(a_{1},\ldots,a_{p},t)^{\top}z^{\prime })p_{i_{2}}(z^{\prime})\right]\right],\]

and

\[\tilde{M}_{i_{1},i_{2}}(\mathbf{a},t) =\mathbb{E}_{z}\left[p_{i_{1}}(z)\hat{\sigma}(\tilde{u}(\mathbf{a }_{i_{2}},t)^{\top}z)\right],\] \[\tilde{M}_{i_{1},i_{2}}(\mathbf{a},t) =\mathbb{E}_{z}\left[p_{i_{1}}(z)\hat{\sigma}(\tilde{u}(\mathbf{a }_{i_{2}},t)^{\top}z)\right],\]

where \(\mathbf{a}=\left(\mathbf{a}_{1},\mathbf{a}_{2},\ldots,\mathbf{a}_{\binom{n+p} {p}}\right)\) and \(\mathbf{a}_{i}\in\mathbb{R}^{p}\) for \(i=1,2,\ldots,\binom{n+p}{p}\).

**Lemma C.10**.: _It holds that_

\[\det(\hat{M}(\mathbf{a},t))=\sum_{i=1}^{\binom{n+p}{p}}\sum_{0\leq\|\mathbf{ j}_{i}\|_{1}\leq Ln}\hat{h}_{\mathbf{j}}t^{\|\|_{1}}\mathbf{a}^{\mathbf{j}},\] (C.15)

_where \(\mathbf{j}=\left(\mathbf{j}_{1},\mathbf{j}_{2},\ldots,\mathbf{j}_{\binom{n+p} {p}}\right)\) with \(\mathbf{j}_{i}\in\mathbb{N}^{p}\) for \(i=1,2,\ldots,\binom{n+p}{p}\), \(\hat{h}_{\mathbf{j}}\) is a constant depending on \(h^{*}\) and \(\sigma\), and \(\mathbf{a}^{\mathbf{j}}\) represents the product of entrywise powers._

Proof.: The result follows directly from Proposition C.4. 

**Lemma C.11**.: _It holds that_

\[\det(\tilde{M}(\mathbf{a},t))=\sum_{i=1}^{\binom{n+p}{p}}\sum_{0\leq\| \mathbf{j}_{i}\|_{1}\leq Ln}\tilde{h}_{\mathbf{j}}(t)\mathbf{a}^{\mathbf{j}},\]

_with_

\[\tilde{h}_{\mathbf{j}}(t)=\hat{h}_{\mathbf{j}}t^{\|\mathbf{j}\|_{1}}+\mathcal{ O}(t^{\|\mathbf{j}\|_{1}+1}).\]

Proof.: The result follows directly from Proposition C.4 and Proposition C.5. 

**Lemma C.12**.: _For any \(m\in\mathbb{N}\), one has that_

\[\text{span}\left\{(q^{\top}z)^{m}:q\in V\right\}=\mathbb{P}_{V,m}^{h},\] (C.16)

_where \(\mathbb{P}_{V,m}^{h}\) is the collection of all homogeneous polynomials in \(z\in V\) with degree \(m\)._

Proof.: Without loss of generality, we assume that \(V=\mathbb{R}^{p}\) and prove the result by induction on \(p\). (C.16) is clearly true for \(p=1\). Then we consider \(p\geq 2\) and assume that (C.16) holds for \(p-1\) and any \(m\).

[MISSING_PAGE_FAIL:26]

Proof.: Let \(X(\mathbf{q})\in\mathbb{R}^{\binom{n+p}{p}\times\binom{n+p}{p}}\) be defined as

\[X_{i_{1},i_{2}}(\mathbf{q})=\mathbb{E}_{z}\left[p_{i_{1}}(z)\sigma(\mathbf{q}_{i _{2}}^{\top}z)\right],\]

where \(\mathbf{q}=\left(\mathbf{q}_{1},\mathbf{q}_{2},\ldots,\mathbf{q}_{\binom{n+p}{ p}}\right)\) with \(\mathbf{q}_{i}\in V\). Then \(\det(X(\mathbf{q}))\) is a polynomial in \(\mathbf{q}\) of the form

\[\det(X(\mathbf{q}))=\sum_{i=1}^{\binom{n+p}{p}}\sum_{0\leq\|\hat{\mathbf{J}}_{ i}\|_{1}\leq n}X_{\mathbf{j}}\mathbf{q}^{\mathbf{j}},\] (C.17)

where we understand \(\mathbf{q}_{i}\) as a (coefficient) vector in \(\mathbb{R}^{p}\) associated with a fixed orthonormal basis \(\{e_{1},e_{2},\ldots,e_{p}\}\) of \(V\) that satisfies Lemma C.9. By Lemma C.13, there exists some \(\mathbf{q}\) such that \(\sigma(\mathbf{q}_{1}^{\top}z),\sigma(\mathbf{q}_{2}^{\top}z),\ldots,\sigma( \mathbf{q}_{\binom{n+p}{p}}^{\top}z)\) form a basis of \(\mathbb{P}_{V,n}\), which implies that \(\det(X(\mathbf{q}))\neq 0\) for this \(\mathbf{q}\). Thus, (C.17) is a non-zero polynomial in \(\mathbf{q}\). Let \(\mathbf{s}=(s_{1},s_{2},\ldots,s_{p})\) collect all indices \(s_{i}\) from Lemma C.9. Denote

\[S=\min\left\{\sum_{l=1}^{\binom{n+p}{p}}\mathbf{s}^{\top}\mathbf{j}_{l}:X_{ \mathbf{j}}\neq 0,\;0\leq\|\mathbf{j}_{i}\|\leq n,\;1\leq i\leq\binom{n+p}{p} \right\},\]

and

\[J_{S}=\left\{\mathbf{j}:\;\sum_{l=1}^{\binom{n+p}{p}}\mathbf{s}^{\top}\mathbf{ j}_{l}=S,\;X_{\mathbf{j}}\neq 0,\;0\leq\|\mathbf{j}_{i}\|\leq n,\;1\leq i\leq \binom{n+p}{p}\right\}.\]

Then we have that

\[\det(\hat{M}(\mathbf{a},t))= \det\left(X\left(\hat{u}(\mathbf{a}_{1},t),\hat{u}(\mathbf{a}_{2},t),\ldots,\hat{u}(\mathbf{a}_{\binom{n+p}{p}},t)\right)\right)\] \[= \sum_{i=1}^{\binom{n+p}{p}}\sum_{0\leq\|\hat{\mathbf{J}}_{i}\|_{ 1}\leq n}X_{\mathbf{j}}\prod_{l=1}^{\binom{n+p}{p}}\hat{u}(\mathbf{a}_{l},t)^{ \mathbf{j}_{l}}\] \[= \sum_{\mathbf{j}\in J_{S}}X_{\mathbf{j}}\prod_{l=1}^{\binom{n+p} {p}}\hat{u}_{\mathbf{s}}(\mathbf{a}_{l},t)^{\mathbf{j}_{l}}+\mathcal{O}(t^{S+ 1}),\]

where \(\hat{u}_{\mathbf{s}}(\mathbf{a}_{l},t)=(\hat{u}_{2,s_{1}}(\mathbf{a}_{l},t), \ldots,\hat{u}_{p,s_{p}}(\mathbf{a}_{l},t))\) and \(\hat{u}_{i,s_{i}}(a_{1},\ldots,a_{p},t)=\frac{1}{p}\sum_{k=1}^{p}\hat{q}_{i,s_ {i}}t^{s_{i}}a_{k}^{s_{i}}\) collects the leading order terms of \(\hat{u}_{i}(a_{1},\ldots,a_{p},t)\). According to Assumption C.9, Theorem 4.5, and Lemma 4.7, we have that

\[\sum_{\mathbf{j}\in J_{S}}X_{\mathbf{j}}\prod_{l=1}^{\binom{n+p}{p}}\hat{u}_ {\mathbf{s}}(\mathbf{a}_{l},t)^{\mathbf{j}_{l}}\neq 0,\]

which provides at least one non-zero term in \(\det(\hat{M}(\mathbf{a},t))\) whose degree in \(t\) is

\[S\leq sn\binom{n+p}{p}.\]

This completes the proof. 

Proof of Proposition 4.4.: According to Lemma C.14, there exist some \(\hat{\mathbf{j}}\) with \(\|\hat{\mathbf{j}}\|_{1}\leq sn\binom{n+p}{p}\) such that \(\hat{h}_{\hat{\mathbf{j}}}\neq 0\). According to Lemma C.11 and Lemma 103 in [1], it holds that

\[\mathbb{E}_{\mathbf{a}}\left[\left(\det(\tilde{M}(\mathbf{a},t))\right)^{2} \right]\geq C_{1}\left|\tilde{h}_{\hat{\mathbf{j}}}(t)\right|^{2}=C_{1}\left| \hat{h}_{\hat{\mathbf{j}}}t^{\|\hat{\mathbf{j}}\|_{1}}+\mathcal{O}(t^{\|\hat{ \mathbf{j}}\|_{1}+1})\right|^{2}\geq\frac{C_{1}}{2}\left|\hat{h}_{\hat{ \mathbf{j}}}\right|t^{2\|\hat{\mathbf{j}}\|_{1}},\] (C.18)

where \(\mathbf{a}\sim\mathcal{U}\left(([-1,1]^{p})^{\binom{n+p}{p}}\right)\) for some constant \(C_{1}\) depending only on \(n,p,s\), and for sufficiently small \(t\). It can be seen that

\[\tilde{\mathcal{K}}(t)=\frac{1}{\binom{n+p}{p}}\mathbb{E}_{\mathbf{a}}\left[ \tilde{M}(\mathbf{a},t)\tilde{M}(\mathbf{a},t)^{\top}\right].\]By Jensen's inequality, one has that

\[\lambda_{\min}(\tilde{\mathcal{K}}(t))\geq\frac{1}{{n+p\choose p}}\mathbb{E}_{ \mathbf{a}}\left[\lambda_{\min}\left(\tilde{M}(\mathbf{a},t)\tilde{M}(\mathbf{a },t)^{\top}\right)\right].\] (C.19)

Since entries of \(\tilde{M}(a,t)\) are all \(\mathcal{O}(1)\) by Lemma C.2, which implies the boundedness of the eigenvalues \(\lambda_{i}\left(\tilde{M}(\mathbf{a},t)\tilde{M}(\mathbf{a},t)^{\top}\right)\), \(i=1,2,\ldots,{n+p\choose p}\), one has that

\[\left(\det(\tilde{M}(\mathbf{a},t))\right)^{2}=\det\left(\tilde{M}(\mathbf{a },t)\tilde{M}(\mathbf{a},t)^{\top}\right)\leq C_{2}\lambda_{\min}\left(\tilde{ M}(\mathbf{a},t)\tilde{M}(\mathbf{a},t)^{\top}\right).\] (C.20)

Combining (C.18), (C.19), and (C.20), one can conclude that

\[\lambda_{\min}(\tilde{\mathcal{K}}(t))\geq\frac{C_{1}}{2}\left|\hat{h}_{ \mathbf{j}}(\alpha,m)\right|t^{2\|\hat{\mathbf{j}}\|_{1}}\geq\frac{C_{1}}{2} \left|\hat{h}_{\mathbf{j}}(\alpha,m)\right|t^{2sn{n+p\choose p}},\]

where we used \(\|\hat{\mathbf{j}}\|\leq N{n+p\choose p}\max_{i\in I}s_{i}\) and only considered \(t\leq 1\). By Proposition C.1, we have that

\[\left|\lambda_{\min}(\mathcal{K}(t))-\lambda_{\min}(\tilde{\mathcal{K}}(t)) \right|=\mathcal{O}(t^{L+1}).\]

Then we can obtain (4.3) as we set \(L=2sn{n+p\choose p}\). 

Proof of Theorem 4.3.: Let \(C,T\) be the constants in Proposition 4.4 and consider \(t>T\). It follows from Theorem 3.5 that \(w_{V}^{\perp}(a,T)=0\), which leads to that \(u_{V}^{\perp}(a_{1},\ldots,a_{p},T)=0\) and hence that \(f_{\text{NN}}(x;\rho_{t})\) and \(g(x,t)\) only depend on \(x_{V}\) for all \(t\geq T\). Define \(\mathbf{g}(t)\in\mathbb{R}^{{n+p\choose p}}\) via \(\mathbf{g}_{i}(t)=\mathbb{E}_{x}[g(x,t)p_{i}(x_{V})]\). Then according to (2.6) and (4.3), one has for \(t>T\) that

\[\frac{d}{dt}\mathcal{E}(\rho_{t})= -\sum_{i=1}^{P}\mathbb{E}_{u}\mathbb{E}_{x,x^{\prime}}\left[g_{i }(x,t)\sigma(w(a,T)^{\top}x)\sigma(w(a,T)^{\top}x^{\prime})g_{i}(x^{\prime}, t)\right]\] \[= -\mathbf{g}(t)^{\top}\mathcal{K}(T)\mathbf{g}(t)\leq-\lambda_{ \min}(\mathcal{K}(T))\|\mathbf{g}(t)\|^{2}\] \[= -2\lambda_{\min}(\mathcal{K}(T))\mathcal{E}(\rho_{t})\leq-2CT^{2 sn{n+p\choose p}}\mathcal{E}(\rho_{t}),\]

which implies that

\[\mathcal{E}(\rho_{t})\leq\mathcal{E}(\rho_{T})\exp\left(-2CT^{2sn{n+p\choose p }}(t-T)\right),\]

by Gronwall's inequality. Then we obtain the desired exponential decay property by noticing that \(\mathcal{E}(\rho_{T})=\frac{1}{2}\mathbb{E}_{z}[\|h^{*}(z)\|^{2}]\). 

## Appendix D Further Discussion and Characterization of Assumption 4.1 and Theorem 4.3

### Verification of Assumption 4.1

We provide some verification or characterization of Assumption 4.1. Without loss of generality, we fix an orthonormal basis and \(V\) and view that \(V=\mathbb{R}^{p}\). The results in this subsection are independent of the choice of the orthonormal basis.

It has been discussed in Section 4.1 that if \(\sigma\in\mathcal{C}^{s}(\mathbb{R})\) with \(s=2^{p-1}\) and \(\sigma^{(1)}(0),\sigma^{(2)}(0),\ldots,\sigma^{(p)}(0)\) are all nonzero, then Assumption 4.1 can be verified with \(s\) for \(h^{*}(z)=z_{1}+z_{1}z_{2}+\cdots+z_{1}z_{2}\cdots z_{p}\), using the calculation in [1, Proposition 33]. Similarly, the same result is also true for \(h^{*}(z)=c_{1}z_{1}+c_{2}z_{1}z_{2}+\cdots+c_{p}z_{1}z_{2}\cdots z_{p}\) if \(c_{1},c_{2},\ldots,c_{p}\) are nonzero. More generally, we have the following.

**Proposition D.1**.: _Let \(\{\alpha_{1},\alpha_{2},\ldots,\alpha_{m}\}\) be a set of pairwise distinct elements in \(\mathbb{N}^{p}\) that contains \((1,0,\ldots,0),(1,1,0,\ldots,0),\ldots,(1,1,\ldots,1)\). If \(\sigma\in\mathcal{C}^{s}(\mathbb{R})\) with \(s=2^{p-1}\) and \(\sigma^{(1)}(0),\sigma^{(2)}(0),\ldots,\sigma^{(p)}(0)\) are all nonzero, then_

\[h^{*}(z)=\sum_{i=1}^{m}c_{i}\prod_{j=1}^{p}\text{He}_{\alpha_{i}(j)}(z_{j}),\]

_satisfies Assumption 4.1 with \(s\) unless \((c_{1},c_{2},\ldots,c_{m})\) is in some measure-zero subset of \(\mathbb{R}^{m}\) with respect to the Lebesgue measure._Proof.: We assume that \(\alpha_{1}=(1,0,\dots,0),\alpha_{2}=(1,1,0,\dots,0),\dots,\alpha_{m}=(1,1,\dots,1)\). As in the proof of Lemma C.9, Assumption 4.1 is true with \(s\) if and only if the matrix \(\hat{q}:=(\hat{q}_{i,j})_{1\leq i\leq p,1\leq j\leq s}\) is of full-row-rank, i.e., \(\text{det}(\hat{q}^{\top})\neq 0\). By (C.9), each entry in \(\hat{q}\) is a polynomial in \((c_{1},c_{2},\dots,c_{m})\), which implies that \(\text{det}(\hat{q}\hat{q}^{\top})\) is also a polynomial in \((c_{1},c_{2},\dots,c_{m})\). This polynomial is nonzero since it takes nonzero value at \((1,1,\dots,1,0,\dots,0)\) with the \(p\) entries being \(1\) and all other entries being \(0\), which is because that Assumption 4.1 is true for \(h^{*}(z)=z_{1}+z_{1}z_{2}+\dots+z_{1}z_{2}\cdots z_{p}\). Finally, the conclusion of Proposition D.1 is true since the set of roots of a nonzero polynomial is of measure zero with respect to the Lebesgue measure. 

### Discretization Results Implied by Theorem 4.3

The discussion in this subsection is similar to those in Appendix B.2. We slightly modified the flow \(\rho_{t}\) generated by Algorithm 1 to guarantee some boundedness conditions, and then use the standard dimension-free estimate [19] to derive a sample complexity result implied by Theorem 4.3.

We use the same bounded modification \(\tilde{f}^{*}\) as in (B.3) for a given constant \(C^{b}_{f}>0\). Similarly, for any \(\delta>0\) and \(C_{w}>0\), there exists dimension-free constant \(C^{b}_{\sigma}>0\) depending on \(h^{*},\delta,C_{w}\), such that one can modify the activation function \(\hat{\sigma}(\zeta)=(1+\zeta)^{n}\) to \(\tilde{\sigma}\) satisfying that \(\|\tilde{\sigma}\|_{L^{\infty}(\mathbb{R})}\leq C^{b}_{\sigma},\|\tilde{ \sigma}^{\prime}\|_{L^{\infty}(\mathbb{R})}\leq C^{b}_{\sigma},\|\tilde{\sigma }^{\prime\prime}\|_{L^{\infty}(\mathbb{R})}\leq C^{b}_{\sigma}\), and \(\mathbb{E}_{x\sim\mathcal{N}(0,I_{d})}\left[|\hat{\sigma}(w^{\top}x)-\tilde{ \sigma}(w^{\top}x)|^{2}\right]<\delta,\ \mathbb{E}_{x\sim\mathcal{N}(0,I_{d})}\left[|\hat{ \sigma}(w^{\top}x)-\tilde{\sigma}(w^{\top}x)|^{2}\right]<\delta\), for all \(w\in\mathbb{R}^{d}\) with \(\|w\|\leq C_{w}\).

The associated mean-field dynamics \(\tilde{\rho}_{t}\), that can be viewed as a slight modification of \(\rho_{t}\) generated by Algorithm 1, is given by (B.4) for \(0\leq t\leq T\) and follows

\[\begin{cases}\partial_{t}\tilde{\rho}_{t}=\nabla_{\theta}\cdot\left(\tilde{ \rho}_{t}\xi(t)\nabla_{\theta}\tilde{\Phi}^{\prime}(\theta;\tilde{\rho}_{t}) \right),\\ \tilde{\rho}_{t}\big{|}_{t=T}=\tilde{\rho}_{T},\end{cases}\] (D.1)

with

\[\tilde{\Phi}^{\prime}(\theta;\rho)=a\mathbb{E}_{x\sim\mathcal{N}(0,I_{d})} \left[\left(\tilde{f}_{\text{NN}}(x;\rho)-\tilde{f}^{*}(x)\right)\tilde{\sigma }(w^{\top}x)\right],\]

\[\tilde{f}_{\text{NN}}(x;\rho)=\int a\tilde{\sigma}(w^{\top}x)\rho(da,dw),\]

for \(t\geq T\). Here, the learning rate \(\xi(t)=\text{diag}(\xi_{a}(t),\xi_{w}(t)I_{d})\) is shared with Algorithm 1, namely \(\xi_{a}(t)=0,\xi_{w}(t)=1\) for \(0\leq t\leq T\) and \(\xi_{a}(t)=1,\xi_{w}(t)=0\) for \(t\geq T\).

The SGD associated to the modified mean-field dynamics is given by

\[w_{i}^{(k+1)} =w_{i}^{(k)}+\epsilon\left(\tilde{f}^{*}(x_{k})-f_{\text{NN}}(x_{ k};\Theta^{(k)})\right)a_{i}^{(k)}\sigma^{\prime}\left(\left(w_{i}^{(k)} \right)^{\top}x_{k}\right)x_{k},\] (D.2) \[a_{i}^{(k+1)} =a_{i}^{(k)},\]

for \(i=1,2,\dots,N\) and \(k=0,1,\dots,T/\epsilon-1\), where \(N\) is the number of neurons and \(T/\epsilon\) is assumed to be an integer, and

\[w_{i}^{(k+1)} =w_{i}^{(k)},\] (D.3) \[a_{i}^{(k+1)} =a_{i}^{(k)}+\epsilon\left(\tilde{f}^{*}(x_{k})-\tilde{f}_{\text{ NN}}(x_{k};\Theta^{(k)})\right)\tilde{\sigma}\left(\left(w_{i}^{(k)} \right)^{\top}x_{k}\right),\]

for \(i=1,2,\dots,N\) and \(k=T/\epsilon,T/\epsilon+1,\dots\)

Suppose that assumptions made in Theorem 4.3 hold and fix \(T^{\prime}>T>0\). Using similar analysis as in Appendix A.2, one can conclude that for any \(\delta>0\), there exist dimension-free constants \(C^{b}_{f}\) and \(C^{b}_{\sigma}\) such that

\[\sup_{0\leq t\leq T^{\prime}}|\mathcal{E}(\rho_{t})-\mathcal{E}(\tilde{\rho}_{t })|<\delta.\]

Applying Theorem 4.3 and [19, Theorem 1], we can conclude that for any \(\mu\in(0,1)\) and any \(\delta>0\), there exists dimension-free constants \(N_{0},d_{0},C_{\epsilon}\), such that for any \(N\geq N_{0}\) and \(d\geq d_{0}\), the following holds with probability at least \(\mu\) for any \(\epsilon\leq\frac{C_{\epsilon}}{d+\log N}\) with \(T/\epsilon\in\mathbb{N}\):

\[\inf_{k\in[0,T^{\prime}/\epsilon]\cap\mathbb{N}}\mathcal{E}_{N}(\Theta^{(k)})<\delta.\]

If we further assume that \(N=\mathcal{O}(e^{d})\), this indicates that SGD with (D.2) and (D.3) can learn the subspace-sparse polynomial \(f^{*}\) within finite time horizon and with \(\mathcal{O}(d)\) samples/data points.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The theoretical results claimed in the abstract and the introduction are established in Section 3 and Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed in the last paragraph of Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The assumptions and the proof sketches are included in Section 3 and Section 4. The complete proofs are in the appendices. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This paper is purely theoretical and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA] Justification: This paper is purely theoretical and does not include experiments. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This paper is purely theoretical and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This paper is purely theoretical and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This paper is purely theoretical and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed the NeurIPS Code of Ethics and confirm that this paper conforms with it in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper conducts fundamental research and is purely theoretical. Thus, it has no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper is purely theoretical and poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper is purely theoretical and does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper is purely theoretical and does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper is purely theoretical/mathematical and does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper is purely theoretical/mathematical and does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.