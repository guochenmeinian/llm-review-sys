# Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars

 Kaiyue Wen

Tsinghua University

wenky20@mails.tsinghua.edu.cn

&Yuchen Li

Carnegie Mellon University

yuchenl4@cs.cmu.edu

&Bingbin Liu

Carnegie Mellon University

bingbinl@cs.cmu.edu

&Andrej Risteski

Carnegie Mellon University

aristek@andrew.cmu.edu

###### Abstract

Transformer interpretability aims to understand the algorithm implemented by a learned Transformer by examining various aspects of the model, such as the weight matrices or the attention patterns. In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole. We consider a simple synthetic setup of learning a (bounded) Dyck language. Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma). We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be "nearly randomized", while preserving the functionality of the network. We also show via extensive experiments that these constructions are not merely a theoretical artifact: even with severe constraints to the architecture of the model, vastly different solutions can be reached via standard training. Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading.

## 1 Introduction

Transformer-based models power many leading approaches to natural language processing. With their growing deployment in various applications, it is increasingly essential to understand the inner working of these models. Towards addressing this, there have been great advancement in the field of interpretability presenting various types of evidence (Clark et al., 2019; Vig and Belinkov, 2019; Wiegreffe and Pinter, 2019; Nanda et al., 2023; Wang et al., 2023), some of which, however, can be misleading despite being highly intuitive (Jain and Wallace, 2019; Serrano and Smith, 2019; Rogers et al., 2020; Grimsley et al., 2020; Brunner et al., 2020; Meister et al., 2021).

In this work, we aim to understand the theoretical limitation of certain interpretability methods by characterizing the set of viable solutions. We focus on myopic interpretability methods, i.e. methods based on examining individual components only. We adopt a particular toy setup in which Transformers are trained to generate _Dyck grammars_, a classic type of formal language grammar consisting of balanced parentheses of multiple types. Dyck is a useful sandbox, as it captures properties like long-range dependency and hierarchical tree-like structure that commonly appear in natural and programming language syntax, and has been an object of interest in many theoretical studies (Hahn, 2020; Yao et al., 2021; Liu et al., 2022b, 2023). Dyck is canonically parsed usinga stack-like data structure. Such stack-like patterns (Figure 1) have been observed in the attention heads (Ebrahimi et al., 2020), which was later bolstered by mathematical analysis in Yao et al. (2021).

From a representational perspective and via explicit constructions of Transformer weights, recent work (Liu et al., 2023; Li et al., 2023) show that Transformers are sufficiently expressive to admit very different solutions that perform equally well on the training distribution. Thus, the following questions naturally arise:

* Do Transformer solutions found empirically match the theoretical constructions given in these representational results (Figure 1)? In particular, are interpretable stack-like pattern in Ebrahimi et al. (2020) the norm or the exception in practice?
* More broadly, can we understand in a principled manner the fundamental obstructions to reliably "reverse engineering" the algorithm implemented by a Transformer by looking at individual attention patterns?
* Among models that perform (near-)optimally on the training distribution, even if we cannot fully reverse engineer the algorithm implemented by the learned solutions, can we identify properties that characterize performance beyond the training distribution?

Our contributions.We first prove several theoretical results to provide evidence for why individual components (e.g. attention patterns or weights) of a Transformer should not be expected to be interpretable. In particular, we prove:

* A **perfect balance** condition (Theorem 1) on the attention pattern that is sufficient and necessary for 2-layer Transformers with a _minimal first layer_ (Assumption 1) to predict optimally on Dyck of _any_ length. We then show that this condition permits abundant _non-stack-like_ attention patterns that do not necessarily reflect any structure of the task, including _uniform_ attentions (Corollary 1).
* An **approximate balance** condition (Theorem 3), the _near-optimal_ counterpart of the condition above, for predicting on _bounded_-length Dyck. Likewise, non-stack-like attention patterns exist.
* **Indistinguishability from a single component** (Theorem 2), proved via a _Lottery Ticket Hypothesis_ style argument that any Transformer can be approximated by pruning a larger random Transformer, implying that interpretations based exclusively on local components may be unreliable.

We further accompany these theoretical findings with an extensive set of empirical investigations.

_Is standard training biased towards interpretable solutions?_ While both stack-like and non-stack like patterns can process Dyck theoretically, the inductive biases of the architecture or the optimization process may prefer one solution over the other in practice. In Section 4.1, based on a wide range of Dyck distributions and model architecture ablations, we find that Transformers that generalize near-perfectly in-distribution (and reasonably well out-of-distribution) do _not_ typically produce stack-like attention patterns, showing that the results reported in prior work (Ebrahimi et al., 2020) should not be expected from standard training.

_Do non-interpretable solutions perform well in practice?_ Our theory predicts that balanced (or even uniform) attentions suffice for good in- and out-of-distribution generalization. In Section 4.2, we

Figure 1: **Second-layer attention patterns of two-layer Transformers on Dyck: typical attention patterns do _not_ exactly match the intuitively interpretable stack-like pattern prescribed in Ebrahimi et al. (2020); Yao et al. (2021). The blue boxes indicate the locations of the last unmatched open brackets, as they would appear in a stack-like pattern. All models reach \(\geq 97\%\) accuracy (defined in Section 4.1). In the heatmap, darker color indicates larger value.**

empirically verify that with standard training, the extent to which attentions are balanced is positively correlated with generalization performance. Moreover, we can guide Transformers to learn more balanced attention by regularizing for the balance condition, leading to better length generalization.

### Related Work

There has been a flourishing line of work on interpretability in natural language processing. Multiple "probing" tasks have been designed to extract syntactic or semantic information from the learned representations (Raganato and Tiedemann, 2018; Liu et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019). However, the effectiveness of probing often intricately depend on the architecture choices and task design, and sometimes may even result in misleading conclusions (Jain and Wallace, 2019; Serrano and Smith, 2019; Rogers et al., 2020; Brunner et al., 2020; Prasanna et al., 2020; Meister et al., 2021). While these challenges do not completely invalidate existing approaches (Wiegreffe and Pinter, 2019), it does highlight the need for more rigorous understanding of interpretability.

Towards this, we choose to focus on the synthetic setup of Dyck whose solution space is easier to characterize than natural languages, allowing us to identify a set of feasible solutions. While similar representational results have been studied in prior work (Yao et al., 2021; Liu et al., 2023; Zhao et al., 2023), our work emphasizes that theoretical constructions do not resemble the solutions found in practice. Moreover, the multiplicity of valid constructions suggest that understanding Transformer solutions require analyzing the optimization process, which a number of prior work has made progress on (Jelassi et al., 2022; Li et al., 2023; Deng et al., 2023).

Finally, it is worth noting that the challenges highlighted in our work do not contradict the line of prior work that aim to improve _mechanistic interpretability_ into a trained model or the training process (Elhage et al., 2021; Olsson et al., 2022; Nanda et al., 2023; Chughtai et al., 2023; Li et al., 2023), which aim to develop circuit-level understanding of a particular model or the training process.

We defer discussion on additional related work to Appendix A.

## 2 Problem Setup

Dyck languagesA Dyck language (Schutzenberger, 1963) is generated by a context-free grammar, where the valid strings consist of balanced brackets of different types (for example, "\([()]\)" is valid but "\([()]\)" is not). Dyck\({}_{k}\) denote the Dyck language defined on \(k\) types of brackets. The alphabet of Dyck\({}_{k}\) is denoted as \([2k]\equiv\{1,2,\cdots,2k\}\), where for each type \(t\in[k]\), tokens \(2t-1\) and \(2t\) are a pair of corresponding open and closed brackets. Dyck languages can be recognized by a push-down automaton. For a string \(w\) and \(i\leq j\in\mathbb{Z}_{+}\), we use \(w_{i:j}\) to denote the substring of \(w\) between position \(i\) and position \(j\) (both ends included). For a valid prefix \(w_{1:i}\), the _grammar depth_ of \(w_{1:i}\) is defined as the depth of the stack after processing \(w_{1:i}\):

\[\mathrm{depth}(w_{1:i})=\#\text{Open Brackets in }w_{1:i}-\#\text{Closed Brackets in }w_{1:i}.\]

We overload \(\mathrm{depth}(w_{1:i})\) to also denote the grammar depth of the bracket at position \(i\). For example, in each pair of matching brackets, the closing bracket is one depth smaller than the open bracket. We will use \(\tau_{i,d}\) to denote a token of type \(i\in[2k]\) placed at grammar depth \(d\in\mathbb{N}\).

We consider _bounded-depth_ Dyck languages following Yao et al. (2021). Specifically, Dyck\({}_{k,D}\) is a subset of Dyck\({}_{k}\) such that the depth of any prefix of a word is bounded by \(D\),

\[\mathsf{Dyck}_{k,D}:=\{w_{1:n}\in\mathsf{Dyck}_{k}\mid\max_{i\in[n]}\ \mathrm{depth}(w_{1:i})\leq D\}.\] (1)

While a bounded grammar depth might seem restrictive, it suffices to capture many practical settings. For example, the level of recursion occurring in natural languages is typically bounded by a small constant (Karlsson, 2007; Jin et al., 2018). We further define the _length-\(N\) prefix set_ of Dyck\({}_{k,D}\) as

\[\mathsf{Dyck}_{k,D,N}=\{w_{1:N}\mid\exists n\geq N,w_{N+1:n}\in[2k]^{n-N},s.t. \ w_{1:n}\in\mathsf{Dyck}_{k,D}\}.\] (2)

Our theoretical setup uses the following data distribution \(\mathcal{D}_{q,k,D,N}\):

**Definition 1** (Dyck distribution).: _The distribution \(\mathcal{D}_{q,k,D,N}\), specified by \(q\in(0,1)\), is defined over Dyck\({}_{k,D,N}\) such that \(\forall w_{1:N}\in\mathsf{Dyck}_{k,D,N}\),_

\[\mathbb{P}(w_{1:N})\propto(q/k)^{\#\{i\,|w_{i}\text{ is open, }\mathrm{depth}(w_{1:i})>1\}}\cdot(1-q)^{\#\{i\,|w_{i}\text{ is closed, }\mathrm{depth}(w_{1:i})<D-1\}}.\] (3)That is, \(q\in(0,1)\) denote the probability of seeing an open bracket at the next position, except for two corner cases: 1) the next bracket has to be open if the current grammar depth is 0 (1 after seeing the open bracket); 2) the next bracket has to be closed if the current grammar depth is \(D\).

Training Objectives.Given a model \(f_{\theta}\) parameterized by \(\theta\), we train with a _next-token prediction_ language modeling objective on a given \(\mathcal{D}_{q,k,D,N}\). Precisely, given a loss function \(l(\cdot,\cdot)\to\mathbb{R}\), \(f_{\theta}\) is trained to minimize the loss function \(\min_{\mathcal{L}}(\theta;\mathcal{D}_{q,k,D,N})\) with

\[\mathcal{L}(\theta;\mathcal{D}_{q,k,D,N})=\mathbb{E}_{w_{1:N}\sim\mathcal{D}_ {q,k,D,N}}[\frac{1}{N}\sum_{i=1}^{N}l(f_{\theta}(w_{1:i-1}),z(w_{i}))]\] (4)

in which \(z(w_{i})\in\{0,1\}^{2k}\) denotes the one-hot embedding of token \(w_{i}\). We will omit the distribution \(\mathcal{D}_{q,k,D,N}\) when it is clear from the context. We will also consider a \(\ell_{2}\)-regularized version \(\mathcal{L}^{\text{reg}}(\theta)=\mathcal{L}(\theta)+\lambda\frac{\|\theta\|_ {2}^{2}}{2}\) with parameter \(\lambda>0\).

For our theory, we will consider the mean squared error as the loss function: 1

Footnote 1: The challenge of applying our theory to cross-entropy loss is that for some prefixes, their grammatical immediate continuations strictly exclude certain tokens in the vocabulary (e.g. “]” cannot immediately follow “[”], so the optimal cross-entropy loss can only be attained if some parameters are set to infinity. However, when label smoothing is added, the optima is finite again, and analysis similar to ours could apply.

\[l:=l_{sq}(x,z_{i})=\|x-z_{i}\|_{2}^{2}.\] (5)

In our experiments, we apply the cross entropy loss following common practice.

Transformer Architecture.We consider a general formulation of Transformer in this work: the \(l\)-th layer is parameterized by \(\theta^{(l)}:=\{W_{Q}^{(l)},W_{K}^{(l)},W_{V}^{(l)},\mathrm{param}(\mathrm{g}^{ (l)})\}\in\Theta\), where \(W_{K}^{(l)},W_{Q}^{(l)}\in\mathbb{R}^{m_{a}\times m}\), and \(W_{V}^{(l)}\in\mathbb{R}^{m\times m}\) are the key, query, and value matrices of the attention module; \(\mathrm{param}(\mathrm{g}^{(l)})\) are parameters of a feed-forward network \(\mathrm{g}^{(l)}\), consisting of fully connected layers, (optionally) LayerNorms and residual links. Given \(X\in\mathbb{R}^{m\times N}\), the matrix of \(m\)-dimensional features on a length-\(N\) sequence, the \(l\)-th layer of a Transformer computes the function

\[f_{l}(X;\theta^{(l)})= \mathrm{g}^{(l)}\Big{(}\mathrm{LN}\Big{(}W_{V}^{(l)}X\underbrace{ \sigma\Big{(}\mathcal{C}+(W_{K}^{(l)}X)^{\top}(W_{Q}^{(l)}X)\Big{)}}_{ \text{attention pattern}}\Big{)}+X\Big{)},\] (6)

where \(\sigma\) is the column-wise softmax operation defined as \(\sigma(A_{i,j}=\frac{\exp(A_{i,j})}{\sum_{k=1}^{N}\exp(A_{k,j})}\), \(\mathcal{C}\) is the causal mask matrix defined as \(\mathcal{C}_{i,j}=-\inf\cdot 1[i>j]\) where \(\inf\) denotes infinity. We call \(\sigma\left(\mathcal{C}+(W_{K}^{(l)}X)^{\top}(W_{Q}^{(l)}X)\right)\) the _Attention Pattern_ of the Transformer layer \(l\). \(\mathrm{LN}\) represents column-wise LayerNorm operation, whose \(j_{th}\) output column is defined as

\[\mathrm{LN}_{C_{LN}}(A)_{:,j}=\frac{\mathcal{P}_{\perp}A_{:,j}}{\max\{\| \mathcal{P}_{\perp}A_{:,j}\|_{2},C_{LN}\}},\mathcal{P}_{\perp}=\mathcal{I}_{ m}-\frac{1}{m}\mathbf{1}\mathbf{1}^{\top}.\] (7)

Here \(\mathcal{P}_{\perp}\) denotes the projection orthogonal to the \(\mathbf{1}\mathbf{1}^{\top}\) subspace 2 and \(C_{LN}\) is called the normalizing constant for LayerNorm.

Footnote 2: this is just a compact way to write the standard mean subtraction operation

We will further define the _attention output_ at the \(l\)-th layer as

\[a_{l}(X;\theta^{(l)})= W_{V}^{(l)}X\sigma\Big{(}\mathcal{C}+(W_{K}^{(l)}X)^{\top}(W_{Q} ^{(l)}X)\Big{)}.\] (8)

When \(C_{LN}=0\), we will also consider the _unnormalized attention output_ as

\[\tilde{a}_{l}(X;\theta^{(l)})= W_{V}^{(l)}X\tilde{\sigma}\Big{(}\mathcal{C}+(W_{K}^{(l)}X)^{\top}(W_{Q} ^{(l)}X)\Big{)}.\] (9)

where \(\tilde{\sigma}(A)_{i,j}=\exp(A_{i,j})\) and it holds by definition that \(\mathrm{LN}_{0}(\tilde{a}_{l}(X;\theta^{(l)}))=\mathrm{LN}_{0}(a_{l}(X;\theta ^{(l)}))\).

An \(L\)-layer Transformer \(\mathcal{T}_{L}\) consists of a composition of \(L\) of the above layers, along with a word embedding matrix \(W_{E}\in\mathbb{R}^{m\times 2k}\) and a linear decoding head with weight \(W_{\mathrm{Head}}\in\mathbb{R}^{2k\times w}\). Wheninputting a sequence of tokens into Transformer, we will append a _starting token_\(t_{\mathcal{S}}\) that is distinct from any token in the language at the beginning of the sequence. Let \(\mathcal{Z}\in\mathbb{R}^{2k\times(N+1)}\) denote the one-hot embedding of a length-\(N\) sequence, then \(\mathcal{T}_{L}\) computes for \(\mathcal{Z}\) as

\[\mathcal{T}(\mathcal{Z})=W_{\mathrm{Head}}\Big{[}f_{L}(\cdots(f_{1}\left(W_{E} \mathcal{Z}\right)))\Big{]}_{1:2k,(N+1)}).\] (10)

## 3 Theoretical Analyses

Many prior works have looked for intuitive interpretations of Transformer solutions by studying the attention patterns of particular heads or some individual components of a Transformer (Clark et al., 2019; Vig and Belinkov, 2019; Dar et al., 2022). However, we show in this section why this methodology can be insufficient even for the simple setting of Dyck. Namely, for Transformers that generalize well on Dyck (both in-distribution and out-of-distribution), neither attention patterns nor individual local components are guaranteed to encode structures specific for parsing Dyck. We further argue that the converse is also insufficient: when a Transformer does produce interpretable attention patterns, there could be limitations of such interpretation as well, as discussed in Appendix B. Together, our results provide theoretical evidence that careful analyses (beyond heuristics) are required when interpreting the components of a learned Transformer.

### Interpretability Requires Inspecting More Than Attention Patterns

This section focuses on Transformers with 2 layers, which are sufficient for processing Dyck (Yao et al., 2021). We will show that even under this simplified setting, attention patterns alone are not sufficient for interpretation. In fact, we will further restrict the set of 2-layer Transformers by requiring the first-layer outputs to only depend on information necessary for processing Dyck:

**Assumption 1** (Minimal First Layer).: _We consider 2-layer Transformers with a minimal first layer \(f_{1}\). That is, let \(\bm{Z}\in\mathbb{R}^{2k\times(N+1)}\) denote the one-hot embeddings of any input sequence \(t_{\mathcal{S}},t_{1},\dots,t_{N}\in[2k]\), then the \((j+1)_{th}\) column of the output \(f_{1}(W^{E}\bm{Z})\) only depends on the type and depth of \(t_{j}\), \(\forall j\in[N]\)._

Assumption 1 requires the first layer output to depend only on the bracket type and depth, disregarding any other information such as positions; one such example is given by Yao et al. (2021). The construction of a minimal first layer can vary, hence we _directly parameterize its output_ instead:

**Definition 2** (Minimal first layer embeddings).: _Given a minimal first layer, \(\bm{e}(\tau_{t,d})\in\mathbb{R}^{m}\) denotes its output embedding of \(\tau_{t,d}\) for \(t\in[2k]\), \(d\in[D]\). \(\bm{e}(t_{\mathcal{S}})\in\mathbb{R}^{m}\) is the embedding of the starting token._

It is important to note that while the minimal first layer is a strong condition, it does not weaken our results: We will show that the function class allows for a rich set of solutions, none of which are necessarily interpretable. Relaxing to more complex classes will only expand the solution set, and hence our conclusion will remain valid. See Appendix C.2 for more technical details.

#### 3.1.1 Perfect Balance Condition: Ideal Generalization of Unbounded Length

Some prior works have tried to understand the model by inspecting the attention patterns (Ebrahimi et al., 2020; Clark et al., 2019; Vig and Belinkov, 2019). However, we will show that the attention patterns alone are too flexible to be helpful, even for the restricted class of a 2-layer Transformer with a minimal first layer (Assumption 1) and even on a language as simple as Dyck. In particular, the Transformer only needs to satisfy what we call the _balanced condition_:

**Definition 3** (Balance condition).: _A 2-layer Transformer (Equation (10)) with a minimal first layer (Assumption 1 and Definition 2) is said to satisfy the balance condition, if for any \(i,j_{1},j_{2}\in[k]\) and \(d^{\prime},d_{1},d_{2}\in[D]\),_

\[\left(\bm{e}(\tau_{2i-1,d^{\prime}})-\bm{e}(\tau_{2i,d^{\prime}-1})\right)^{ \top}(W_{K}^{(2)})^{\top}W_{Q}^{(2)}\left(\bm{e}(\tau_{2j_{1},d_{1}})-\bm{e}( \tau_{2j_{2},d_{2}})\right)=0.\] (11)

The following result shows that under minor conditions the balance condition is both necessary and sufficient:

**Theorem 1** (Perfect Balance).: _Consider a two-layer Transformer \(\mathcal{T}\) (Equation (10)) with a minimal first layer (Assumption 1) and \(C_{LN}=0\) (Equation (7)). Let \(\mathcal{O}\) denote the optimal prediction scenario, that is, when the first layer embeddings \(\{\bm{e}(\tau_{i,d})\}_{d\in[D],i\in[2k]}\) (Definition 2) and second layer parameters \(\theta^{(2)}\) satisfy_

\[\theta:=\{\bm{e}(\tau_{i,d})\}_{d\in[D],i\in[2k]},\theta^{(2)}\}=\arg\min_{ \tilde{\theta}}\mathcal{L}(\tilde{\theta};\mathcal{D}_{q,k,D,N}),\forall N,\]

_where the objective \(\mathcal{L}\) is defined in Equation (4). Then,_

* _Equation (_11_) a necessary condition of_ \(\mathcal{O}\)_, if_ \(W_{V}^{(2)}\) _satisfies_ \(\mathcal{P}_{\perp}W_{V}^{(2)}\bm{e}(\tau_{t,d})\neq 0,\forall t\in[k],d\in[D]\)_._
* _Equation (_11_) is a sufficient condition of_ \(\mathcal{O}\)_, if the set of_ \(2k+1\) _encodings_ \(\{\bm{e}(\tau_{2i-1,d}),\bm{e}(\tau_{2i,d})\}_{i\in[k]}\cup\{\bm{e}(t_{\mathcal{ S}})\}\) _are linearly independent for any_ \(d\in[D]\)_, and the projection function_ \(\mathrm{g}^{(2)}\) _is a 6-layer MLP_ 3 _with_ \(O(k^{2}D^{2})\) _width._

Footnote 3: The 6 layers are by our construction. We will first use 4 layers to convert the input of the projection function to a triplet indicating the type and depth of the last token and the type of the last unmatched bracket when the last token is a closed bracket. We will then use another \(2\) layers to predict the next token probability based on the triplet. This construction may be improved.

_Remark_: Recall from Equation (7) that \(\mathcal{P}_{\perp}\) projects to the subspace orthogonal to \(\bm{11}^{\top}\). The assumption in the necessary condition can be intuitively understood as requiring all tokens to have nonzero contributions to the prediction after the LayerNorm.

Recall that \(\bm{e}(\tau_{2i-1,d^{\prime}}),\bm{e}(\tau_{2i,d^{\prime}-1})\) denote the first-layer outputs for a matching pair of brackets. Intuitively, Equation (11) says that since matching brackets should not affect future predictions, their embeddings should balance out each other. The balance condition Equation (11) is "perfect" in the sense that the theory assumes the model can minimize the loss for any length \(N\); we will see an approximate version later in Theorem 3.

Proof of the necessity of the balance condition.: The key idea is reminiscent of the pumping lemma for regular languages. For any prefix \(p\) ending with a closed bracket \(\tau_{2j,d}\) for \(d\geq 1\) and containing brackets of all depths in \([D]\), let \(p_{\beta}\) be the prefix obtained by inserting \(\beta\) pairs of \(\{\tau_{2i-1,d^{\prime}},\tau_{2i,d^{\prime}-1}\}\) for arbitrary \(i\in[k]\) and \(d^{\prime}\in[D]\). Denote the _projection of the unnormalized attention output_ by

\[u(\tau_{t_{1},d_{1}},\tau_{t_{2},d_{2}}):=\mathcal{P}_{\perp}\exp\left(\bm{e} \big{(}\tau_{t_{1},d_{1}}\big{)}^{\top}(W_{K}^{(2)})^{\top}W_{Q}^{(2)}\bm{e} \big{(}\tau_{t_{2},d_{2}}\big{)}\right)W_{V}^{(2)}\bm{e}\big{(}\tau_{t_{1},d_{ 1}}\big{)}.\] (12)

We ignored the normalization in softmax above, since the attention output will be normalized directly by LayerNorm according to Equation (6).

By Equation (10), there exists a vector \(v\in\mathbb{R}^{m}\) such that for any \(\beta\in\mathbb{N}\), the next-token logits given by Transformer \(\mathcal{T}\) are

\[\mathcal{T}(p_{\beta})=W_{\mathrm{Head}}g^{(2)}\left(\frac{v+\beta\left(u( \tau_{2j,d},\tau_{2i,d^{\prime}-1})+u(\tau_{2j,d},\tau_{2i-1,d^{\prime}}) \right)}{\|v+\beta\left(u(\tau_{2j,d},\tau_{2i,d^{\prime}-1})+u(\tau_{2j,d}, \tau_{2i-1,d^{\prime}})\right)\|_{2}}+\bm{e}(\tau_{2j,d})\right).\] (13)

The proof proceeds by showing a contradiction. Suppose \(u(\tau_{2j,d},\tau_{2i,d^{\prime}-1})+u(\tau_{2j,d},\tau_{2i-1,d^{\prime}})\neq 0\). Based on the continuity of the projection function and the LayerNorm Layer, we can show that \(\lim_{\beta\to\infty}\mathcal{T}(p_{\beta})\) depend only on grammar depths \(d,d^{\prime}\) and types \(2j,2i-1,2i\). However, these are not sufficient to determine the next-token probability from \(p_{\beta}\), since the latter depends on the type of the last unmatched open bracket in \(p\). This contradicts the assumption that the model can minimize the loss for any length \(N\). Hence we must have

\[u(\tau_{2j,d},\tau_{2i,d^{\prime}-1})+u(\tau_{2j,d},\tau_{2i-1,d^{\prime}})=0.\] (14)

Finally, as we assumed that \(\mathcal{P}_{\perp}W_{V}^{(2)}e\left(\tau_{t,d}\right)\neq 0\), we conclude that

\[\left(e\left(\tau_{2i-1,d^{\prime}}\right)-e\left(\tau_{2i,d^{\prime}-1} \right)\right)^{\top}(W_{K}^{(2)})^{\top}W_{Q}^{(2)}e\left(\tau_{2j+1,d}\right) =\ln\left(\frac{\|\mathcal{P}_{\perp}W_{V}e\left(\tau_{2i,d^{\prime}-1}\right) \|_{2}}{\|\mathcal{P}_{\perp}W_{V}e\left(\tau_{2i-1,d^{\prime}}\right)\|_{2} }\right),\]

where the right hand side is independent of \(j,d\), concluding the proof for necessity. The proof of sufficiency are given in Appendix C.1.

Note that the perfect balance condition is an orthogonal consideration to interpretability. For example, even the uniform attention satisfies the condition and can solve Dyck: 4

Footnote 4: This is verified empirically: the uniform-attention models have attention weights fix to 0 and are to fit the distribution almost perfectly (\(>99\%\) accuracy).

**Corollary 1**.: _There exists a 2-layer Transformer with uniform attention and no position embedding (but with causal mask and a starting token 5 ) that generates the Dyck language of arbitrary length._

Footnote 5: Here the starting token is necessary because otherwise, the Transformer with uniform attention will have the same outputs for prefix \(p\) and prefix \(p\oplus p\), in which \(\oplus\) denotes concatenation, i.e. \(p\oplus p\) means the same string \(p\) repeated twice.

Since uniform attention patterns are hardly reflective of any structure of Dyck, Corollary 1 proves that attention patterns can be oblivious about the underlying task, violating the "faithfulness" criteria for an interpretation (Jain & Wallace, 2019). We will further show in Appendix B.1 that empirically, seemingly structured attention patterns may not accurately represent the inherent structure of the task.

_Extension to approximate balance condition_: Theorem 1 assumes the model reaches the optimal loss for Dyck prefixes of any length. However, in practice, due to finite samples and various sources of randomness, training often does not end exactly at a population optima. In this case, the condition in Theorem 1 is not precisely met. However, even for models that _approximately_ meet those conditions, we will prove that when the second-layer projection function \(g^{(2)}\) is Lipschitz, a similar condition as in Equation (14) is still necessary. Details are deferred to Appendix C.4.

### Interpretability Requires Inspecting More Than Any Single Weight Matrix

Another line of interpretability works involves inspecting the weight matrices of the model (Li et al., 2016; Dar et al., 2022; Eldan & Li, 2023). Some of the investigations are done locally, neglecting the interplay between different parts of the model. Our result in this section shows that from a representational perspective, isolating single weights can also be misleading for interpretability. For this section only, we will assume the linear head \(W_{\mathrm{Head}}\) is identity for simplicity. To consider the effect of pruning, we will also extend the parameterization of LayerNorm module (Equation (7)) as

\[\mathrm{LN}_{C_{LN}}[b](A)_{:,j}=b\frac{\mathcal{P}_{\perp}A_{:,j}}{\max\{\| \mathcal{P}_{\perp}A_{:,j}\|_{2},\epsilon\}}+(1-b)A_{:,j},\]

which corresponds to a weighted residual branch; note that the original LayerNorm corresponds to \(\mathrm{LN}_{C}[1]\). Let \(\hat{\theta}\) denote the set of parameters of this extended parameterization.

We define the _nonstructural pruning_6 as:

Footnote 6: This is as opposed to _structural pruning_, which prunes entire rows/columns of weight matrices.

**Definition 4** (Nonstructural pruning).: _Under the extended parameterization, a nonstructural pruning of a Transformer with parameter \(\hat{\theta}\) is a Transformer with the same architecture and parameter \(\hat{\theta}^{\prime}\), so that for any weight matrix \(W\) in \(\hat{\theta}\), the corresponding matrix \(W^{\prime}\) in \(\hat{\theta}^{\prime}\) has \(W^{\prime}_{i,j}\in\{W_{i,j},0\}\), \(\forall i,j\)._

To measure the quality of the pruning, define the \(\epsilon\)-approximation:

**Definition 5** (\(\epsilon\)-approximation).: _Given two metric spaces \(A,B\) with the same metric \(\|\cdot\|\), a function \(f:A\to B\) is an \(\epsilon\)-approximation of function \(g\) with respect to that metric, if and only if,_

\[\forall x\in A,\|f(x)-g(x)\|\leq\epsilon\|x\|.\]

The metric, unless otherwise specified, will be the \(2\)-norm for vectors and the \(1,2\)-norm for matrices:

**Definition 6**.: _The \(1,2\)-norm of a matrix \(A\) is the max row norm, i.e. \(\|A\|_{1,2}=\max_{i\in[d^{\prime}]}\|A_{:,i}\|_{2}\)._

With these definitions, we are ready to state the main result of this section:

**Theorem 2** (Indistinguishability From a Single Component).: _Consider any \(L\)-layer Transformer \(\mathcal{T}\) (Equation (10)) with embedding dimension \(m\), attention dimension \(m_{a}\), and projection function \(\mathrm{g}\) as 2-layer ReLU MLP with width \(w\). For any \(\delta\in(0,1)\) and \(N\in\mathbb{N}^{+}\), consider a \(4L\)-layer random Transformer \(\mathcal{T}_{\mathrm{large}}\) with embedding dimension \(m_{\mathrm{large}}=O(m\log(Lm/\delta))\), attention dimension \(m_{\mathrm{large,a}}=O(m_{a}L\log\frac{m_{a}mL}{\epsilon\delta})\), and projection function \(\mathrm{g}_{\mathrm{large}}\) as 4-layer ReLU MLP with width \(w_{\mathrm{large}}=O(\max\{m,w\}L\log\frac{wmLN}{\epsilon\delta})\).__Assume that \(\|W\|_{2}\leq 1\) for every weight matrix \(W\) in \(\mathcal{T}\), and suppose the weights are randomly sampled as \(W_{i,j}\sim U(-1,1)\) for every \(W\in\mathcal{T}_{\text{large}}\). Then, with probability \(1-\delta\) over the randomness of \(\mathcal{T}_{\text{large}}\), there exists a nonstructural pruning (Definition 4) of \(\mathcal{T}_{\text{large}}\), denoted as \(\tilde{\mathcal{T}}_{\text{large}}\), which \(\epsilon\)-approximates \(\mathcal{T}\) with respect to \(\|\cdot\|_{1,2}\) for any input \(X\in\mathbb{R}^{m\times N}\) satisfying \(\|\bm{X}\|_{1,2}\leq 1\). 7_

Footnote 7: Here the input and output dimension of \(\tilde{\mathcal{T}}_{\text{large}}\) is actually \(m_{\text{large}}\) which is larger than \(m\); additional dimensions are padded with zeroes. The norm constraint can be easily extended to an arbitrary constant.

**Proof sketch: connection to Lottery Tickets.** Theorem 2 can be interpreted as a lottery ticket hypothesis (Frankle and Carbin, 2018; Malach et al., 2020) for randomly initialized Transformers, which can be of independent interest. The proof repeatedly uses an extension of Theorem 1 of Pensia et al. (2020), where it 1) first prunes the \((2l-1)\)-th and \(2l\)-th layers of \(\mathcal{T}_{\text{large}}\) to approximate \(\mathcal{T}^{(l)}\) for each \(l\in[L]\) (Lemma 6), and 2) then prunes the remaining \(2L+1\) to \(4L\)-th layers of \(\mathcal{T}_{\text{large}}\) to approximate the identity function. The full proof is deferred to Appendix C.5.

Noting that the layers used to approximate the identity can appear at arbitrary depth in \(\mathcal{T}_{\text{large}}\), a direct corollary of Theorem 2 is that one cannot distinguish between two functionally different Transformers by inspecting any single weight matrix only:

**Corollary 2**.: _Let \(\mathcal{T}_{1},\mathcal{T}_{2}\) and \(\mathcal{T}_{\text{large}}\) follow the same definition and assumptions as \(\mathcal{T}\) and \(\mathcal{T}_{\text{large}}\) in Theorem 2. Pick any weight matrix \(W\) in \(\mathcal{T}_{\text{large}}\), then with probability \(1-\delta\) over the randomness of \(\mathcal{T}_{\text{large}}\), there exist two Transformers \(\mathcal{T}_{\text{large},1},\mathcal{T}_{\text{large},2}\) pruned from \(\mathcal{T}_{\text{large}}\), such that \(\mathcal{T}_{\text{large},i}\)\(\epsilon\)-approximate \(\mathcal{T}_{i}\), \(\forall i\in\{1,2\}\), and \(\mathcal{T}_{\text{large},1}\), \(\mathcal{T}_{\text{large},2}\) coincide on the pruned versions of \(W\)._

Hence, one should be cautious when using methods based solely on individual components to interpret the overall function of a Transformer.

## 4 Experiments

Our theory in Section 3 proves the existence of abundant _non-stack-like_ attention patterns, all of which suffice for (near-)optimal generalization on Dyck. However, could it be that stack-like solutions are more frequently discovered empirically, due to potential _implicit biases_ in the architecture and the training procedure? In this section, we show there is no evidence for such implicit bias in standard training (Section 4.1). Additionally, we propose a regularization term based on the balance condition (Theorem 1), which leads to better length generalization (Section 4.2).

### Different Attention Patterns Can Be Learned To Generate Dyck

We empirically verify our theoretical findings that Dyck solutions can give rise to a variety of attention patterns, by evaluating the accuracy of predicting the last bracket of a prefix (Equation 2) given the rest of the prefix. We only consider prefixes ending with a closing bracket, so that there exists a unique correct closing bracket which a correct parser should be able to determine. The experiments in this section are based on Transformers with \(2\) layers and \(1\) head, hidden dimension \(50\) and embedding dimension \(50\), trained using Adam. Additional results for three-layer Transformers are provided in Appendix D.3. The training data consists of valid Dyck\({}_{2,4}\) sequences of length less than \(28\) generated with \(q=0.5\). When tested in-distribution, all models are able to achieve \(\geq 97\%\) accuracy.

Variation in attention patternsFirst, as a response to (Q1), we observe that attention patterns of Transformers trained on Dyck are not always stack-like (Figure 1). In fact, the attention patterns differ even across different random initialization. Moreover, while Theorem 1 implies that position encoding is not necessary for a Transformer to generate Dyck, 8 adding the position encoding 9 does affect the attention patterns (Figures 0(c) and 0(d)).

Footnote 8: This is verified empirically, as Transformers with no positional encoding achieve \(\geq 97\%\) accuracy.

Specifically, for 2-layer Transformers with a minimal first layer, we experiment with three different types of embeddings \(\bm{e}\): let \(\bm{o}_{t}\) denote the one-hot embedding where \(\bm{o}_{t}[t]=1\),

\[\bm{e}\big{(}\tau_{t,d}\big{)} =\bm{o}_{(t-1)D+d}\in\mathbb{R}^{2kD},\] (15) \[\bm{e}\big{(}\tau_{t,d}\big{)} =\bm{o}_{t}\oplus\bm{o}_{d}\in\mathbb{R}^{2k+D},\] (16) \[\bm{e}\big{(}\tau_{t,d}\big{)} =\bm{o}_{t}\oplus[\cos\left(\theta_{d}\right),\sin\left(\theta_{d }\right)]\in\mathbb{R}^{2k+2},\theta_{d}=\arctan\left(d/(D+2-d)\right),\] (17)

where \(\oplus\) denotes vector concatenation. Equation (15) is the standard one-hot embedding for \(\tau_{t,d}\); Equation (16) is the concatenation of one-hot embedding of types and depths. Finally, Equation (17) is the embedding constructed in Yao et al. (2021). As shown in Figure 2, the attention patterns learned by Transformers exhibit large variance between different choices of architectures and learning rates, and most learned attention patterns are not stack-like.

Quantifying the variationWe now quantify the variation in attention by comparing across multiple random initializations. We define the _attention variation_ between two attention patterns \(A_{1},A_{2}\) as \(\mathrm{Variation}(A_{1},A_{2})=\|A_{1}-A_{2}\|_{F}^{2}\), for \(A_{1},A_{2}\in\mathbb{R}^{N\times N}\) over an length-\(N\) input sequence. We report the _average attention variation_ of each architecture based on \(40\) random initializations.

On the prefix \([[[[[[[[[[(((((()))))))))))\)10, we observe that for standard two layer training, the average attention variation is \(2.20\) with linear position embedding, and is \(2.27\) without position embedding. Both numbers are close to the random baseline value of \(2.85\)11, showing that the attention head learned by different initializations indeed tend to be very different. We also experiment with Transformer with a minimal first layer and the embedding in Equation (15), where the average variation is reduced to \(0.24\). We hypothesize that the structural constraints in this setting provide sufficiently strong inductive bias that limit the variation.

Figure 3: **Relationship Between Balance Violation and Length Generalization.** Accuracy from Transformers with minimal first layer with embedding 15, using both standard training and contrastive regularization (Equation (18)). Standard training leas to high balance violations which negatively correlate with length generalization performance. Contrastive regularization helps reduce the balance violation and improve the length generalization performance.

Figure 2: **Second-layer attention patterns of two-layer Transformers with a minimal first layer**: (a), (b) are based on embedding 15 with different learning rates, where the attention patterns show much variance as Theorem 1 predicts. (c), (d) are based on embedding 17 and 16. Different embedding functions lead to diverse attention patterns, most of which are not stack-like.

### Guiding The Transformer To Learn Balanced Attention

In our experiments, we observe that although models learned via standard training that can generalize well in distribution, the _length generalization_ performance is far from optimal. This implies that the models do not correctly identify the parsing algorithm for Dyck when learning from finite samples. A natural question is: can we guide Transformers towards correct algorithms, as evidenced by improved generalization performance on longer Dyck sequences?

In the following, we measure length generalization performance by the model accuracy on valid Dyck prefixes with length randomly sampled from \(400\) to \(500\), which corresponds to around 16 times the length of the training sequences. Inspired by results in Section 3, we propose a regularization term to encourage more balanced attentions, which leads to better length generalization.

Regularizing for balance violation improves length generalization accuracyWe denote the _balance violation_ of a Transformer as \(\beta:=\mathbb{E}_{d,d^{\prime},i,j}\left[S_{d,d^{\prime},i,j}/P_{d,j}\right]\) for \(S,P\) defined in Equations (31) and (33). Theorem 1 predicts that for models with a minimal first layer, perfect length generalization requires \(\beta\) to be zero. Inspired by this observation, we design a contrastive training objective to reduce the balance violation, which ideally would lead to improved length generalization. Specifically, let \(p_{r}\) denote a prefix of \(r\) nested pairs of brackets of for \(r\sim U([D])\), and let \(\mathcal{T}(s\mid p_{r}\oplus s)\) denote the logits for \(s\) when \(\mathcal{T}\) takes as input the concatenation of \(p_{r}\) and \(s\). We define the _contrastive regularization term_\(R_{\text{contrastive}}(s)\) as the mean squared error between the logits of \(\mathcal{T}(s)\) and \(\mathcal{T}(s\mid p_{r}\oplus s)\), taking expectation over \(r\) and \(p_{r}\):

\[\mathbb{E}_{r\sim U([D]),p_{r}}\left[\|\mathcal{T}(s\mid p_{r}\oplus s)- \mathcal{T}(s)\|_{F}^{2}\right].\] (18)

Following the same intuition as in the proof of Theorem 1, if the model can perfectly length-generalize, then the contrastive loss will be zero. Models trained with contrastive loss show reduced balance violation as well as improved length generalization performance, as shown in Figure 3.

## 5 Conclusion

Why interpreting individual components sometimes leads to misconceptions? Through a case study of the Dyck grammar, we provide theoretical and empirical evidence that even in this simple and well-understood setup, Transformers can implement a rich set of non-interpretable solutions. This is reflected both by diverse attention patterns and by the absence of task-specific structures in local components. Our results directly imply similar conclusions for more complex Transformer models; see Appendix C.2 for technical details. Together, this work provides definite proof that myopic interpretability, i.e. methods based on examining individual components only, are not sufficient for understanding the functionality of a trained Transformer.

Our results do not preclude that interpretable attention patterns can emerge; however, they do suggest that interpretable patterns can be infrequent. We discuss the implications for multi-head, overparameterized Transformers trained on more complex data distributions in Appendix B. Moreover, our current results pertain to the existence of solutions; an interesting next step is to study how "inductive biases" given by the synergy of the optimization algorithm and the architecture affect the solutions found.

## Acknowledgement

This work was in part supported by NSF awards IIS-2211907, CCF-2238523, and Amazon Research Award.

## References

* Belinkov (2022) Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. _Computational Linguistics_, 48(1):207-219, March 2022. doi: 10.1162/coli_a_00422. URL https://aclanthology.org/2022.cl-1.7.
* Bhattacharya et al. (2016) Satwik Bhattacharya, Kabir Ahuja, and Navin Goyal. On the Ability and Limitations of Transformers to Recognize Formal Languages. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 7096-7116, Online, November2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.576. URL https://aclanthology.org/2020.emnlp-main.576.
* Bhattacharya et al. (2020b) Satwik Bhattacharya, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. In _Proceedings of the 24th Conference on Computational Natural Language Learning_, pp. 455-475, Online, November 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.conll-1.37. URL https://aclanthology.org/2020.conll-1.37.
* Bolukbasi et al. (2021) Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viegas, and Martin Wattenberg. An interpretability illusion for bert. _arXiv preprint arXiv: Arxiv-2104.07143_, 2021.
* Brunner et al. (2020) Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. On identifiability in transformers. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=BJg1f6EFDB.
* Cammarata et al. (2020) Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lim. Thread: Circuits. _Distill_, 2020. doi: 10.23915/distill.00024. https://distill.pub/2020/circuits.
* Chughtai et al. (2023) B. Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. _International Conference on Machine Learning_, 2023. doi: 10.48550/arXiv.2302.03025.
* Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT's attention. In _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pp. 276-286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology.org/W19-4828.
* Dar et al. (2022) Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space, 2022.
* Deng et al. (2023) Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression, 2023.
* Ebrahimi et al. (2020) Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. How can self-attention networks recognize Dyck-n languages? In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 4301-4306, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.384. URL https://aclanthology.org/2020.findings-emnlp.384.
* Edelman et al. (2022) Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 5793-5831. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/edelman22a.html.
* Eldan and Li (2023) Ronen Eldan and Yuanzhi Li. Tinysteries: How small can language models be and still speak coherent english?, 2023.
* Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. https://Transformer-circuits.pub/2021/framework/index.html.
* Foret et al. (2020) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. _arXiv preprint arXiv:2010.01412_, 2020.
* Frankle and Carbin (2018) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _International Conference On Learning Representations_, 2018.
* Gao et al. (2023) Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential regression, 2023.

F. Gers and J. Schmidhuber. Lstm recurrent networks learn simple context-free and context-sensitive languages. _IEEE transactions on neural networks_, 12 6:1333-40, 2001.
* Grimsley et al. (2020) Christopher Grimsley, Elijah Mayfield, and Julia R.S. Bursten. Why attention is not explanation: Surgical intervention and causal reasoning about neural models. In _Proceedings of the Twelfth Language Resources and Evaluation Conference_, pp. 1780-1790, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.220.
* Haab et al. (2023) Jonathan Haab, Nicolas Deutschmann, and Maria Rodriguez Martinez. Is attention interpretation? a quantitative assessment on sets. In _Machine Learning and Principles and Practice of Knowledge Discovery in Databases_, pp. 303-321, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-23618-1.
* Hahn (2020) Michael Hahn. Theoretical limitations of self-attention in neural sequence models. _Trans. Assoc. Comput. Linguistics_, 8:156-171, 2020. doi: 10.1162/tacl_a_00306. URL https://doi.org/10.1162/tacl_a_00306.
* Hewitt and Liang (2019) John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 2733-2743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1275. URL https://aclanthology.org/D19-1275.
* Hewitt and Manning (2019) John Hewitt and Christopher D Manning. A structural probe for finding syntax in word representations. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 4129-4138, 2019.
* Hewitt et al. (2020) John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D Manning. Rnn's can generate bounded hierarchical languages with optimal memory. _arXiv preprint arXiv:2010.07515_, 2020.
* Htut et al. (2019) Phu Mon Htut, Jason Phang, Shikha Bordia, and Samuel R. Bowman. Do attention heads in bert track syntactic dependencies?, 2019.
* Jain and Wallace (2019) Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 3543-3556, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357. URL https://aclanthology.org/N19-1357.
* Jelassi et al. (2022) Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=eMW9AKxAREI.
* Jin et al. (2018) Lifeng Jin, Finale Doshi-Velez, Timothy Miller, William Schuler, and Lane Schwartz. Unsupervised grammar induction with depth-bounded pcfg. _Transactions of the Association for Computational Linguistics_, 6:211-224, 2018.
* Karlsson (2007) Fred Karlsson. Constraints on multiple center-embedding of clauses. _Journal of Linguistics_, 43(2):365-392, 2007.
* Kobayashi et al. (2020) Goro Kobayashi, Tatsuki Kurbayashi, Sho Yokoi, and Kentaro Inui. Attention is not only a weight: Analyzing transformers with vector norms. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 7057-7075, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.574. URL https://aclanthology.org/2020.emnlp-main.574.
* Kovaleva et al. (2019) Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of BERT. In _Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 4365-4374, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1445. URL https://aclanthology.org/D19-1445.
* Li et al. (2016) Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in NLP. In _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 681-691, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1082. URL https://aclanthology.org/N16-1082.
* Li and Gong (2021) Xian Li and Hongyu Gong. Robust optimization for multilingual translation with imbalanced data. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 25086-25099. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/d324a0cc02881779dcda44a675fdcaaa-Paper.pdf.
* Li and Risteski (2021) Yuchen Li and Andrej Risteski. The limitations of limited context for constituency parsing. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 2675-2687, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.208. URL https://aclanthology.org/2021.acl-long.208.
* Li et al. (2023) Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 19689-19729. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/li23p.html.
* Lin et al. (2019) Yongjie Lin, Yi Chern Tan, and Robert Frank. Open sesame: Getting inside BERT's linguistic knowledge. In _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pp. 241-253, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4825. URL https://aclanthology.org/W19-4825.
* Liu et al. (2022a) Bingbin Liu, Daniel Hsu, Pradeep Kumar Ravikumar, and Andrej Risteski. Masked prediction: A parameter identifiability view. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022a. URL https://openreview.net/forum?id=Hbvlb4D1aFC.
* Liu et al. (2023) Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=De4FYqje7e2.
* Liu et al. (2022b) Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. _arXiv preprint arXiv:2210.14199_, 2022b.
* Liu et al. (2020) Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 5747-5763, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.463. URL https://aclanthology.org/2020.emnlp-main.463.
* Liu et al. (2019) Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 1073-1094, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1112. URL https://aclanthology.org/N19-1112.
* Malach et al. (2020) Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. In _International Conference on Machine Learning_, pp. 6682-6691. PMLR, 2020.
* Malach et al. (2019)Clara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell. Is sparse attention more interpretable? In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pp. 122-129, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.17. URL https://aclanthology.org/2021.acl-short.17.
* Merrill [2019] William Merrill. Sequential neural networks as automata. In _Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges_, pp. 1-13, Florence, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-3901. URL https://www.aclweb.org/anthology/W19-3901.
* Michel et al. [2019] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf.
* Nanda et al. [2023] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=9XFSbDPDrW.
* Nguyen and Salazar [2019] Toan Q. Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. In _Proceedings of the 16th International Conference on Spoken Language Translation_, Hong Kong, November 2-3 2019. Association for Computational Linguistics. URL https://aclanthology.org/2019.iwslt-1.17.
* Olsson et al. [2022] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. _Transformer Circuits Thread_, 2022. https://Transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
* Pensia et al. [2020] Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos. Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient. _Advances in neural information processing systems_, 33:2599-2610, 2020.
* Perez et al. [2021] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing-complete. _Journal of Machine Learning Research_, 22(75):1-35, 2021. URL http://jmlr.org/papers/v22/20-302.html.
* Prasanna et al. [2020] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT Plays the Lottery, All Tickets Are Winning. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 3208-3229, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.259. URL https://aclanthology.org/2020.emnlp-main.259.
* Raganato and Tiedemann [2018] Alessandro Raganato and Jorg Tiedemann. An analysis of encoder representations in transformer-based machine translation. In _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pp. 287-297, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5431. URL https://aclanthology.org/W18-5431.
* Rogers et al. [2020] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert works. _Transactions of the Association for Computational Linguistics_, 8:842-866, 2020.
* Schutzenberger [1963] M.P. Schutzenberger. On context-free languages and push-down automata. _Information and Control_, 6(3):246-264, 1963. ISSN 0019-9958. doi: https://doi.org/10.1016/S0019-9958(63)90306-1. URL https://www.sciencedirect.com/science/article/pii/S0019995863903061.
* Serrano and Smith [2019] Sofia Serrano and Noah A. Smith. Is attention interpretable? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 2931-2951, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1282. URL https://aclanthology.org/P19-1282.

Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. In _Proceedings of the Fifth Annual Workshop on Computational Learning Theory_, COLT '92, pp. 440-449, New York, NY, USA, 1992. Association for Computing Machinery. ISBN 089791497X. doi: 10.1145/130385.130432. URL https://doi.org/10.1145/130385.130432.
* Sun and Marasovic (2021) Kaiser Sun and Ana Marasovic. Effective attention sheds light on interpretability. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pp. 4126-4135, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.361. URL https://aclanthology.org/2021.findings-acl.361.
* Suzgun et al. (2019) Mirac Suzgun, Yonatan Belinkov, Stuart Shieber, and Sebastian Gehrmann. LSTM networks can perform dynamic counting. In _Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges_, pp. 44-54, Florence, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-3905. URL https://www.aclweb.org/anthology/W19-3905.
* Tenney et al. (2019) Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. _arXiv preprint arXiv:1905.05950_, 2019.
* Vig and Belinkov (2019) Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language model. In _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pp. 63-76, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4808. URL https://aclanthology.org/W19-4808.
* Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 5797-5808, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1580. URL https://aclanthology.org/P19-1580.
* Wang et al. (2023) Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In _International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=NpsVSN604ul.
* Wei et al. (2021) Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers, 2021. URL https://arxiv.org/abs/2107.13163.
* Weiss et al. (2018) Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite precision rnns for language recognition. _arXiv preprint arXiv:1805.04908_, 2018.
* Weiss et al. (2021) Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 11080-11090. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/weiss21a.html.
* Wiegreffe and Pinter (2019) Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 11-20, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1002. URL https://aclanthology.org/D19-1002.
* Wu et al. (2020) Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 4166-4176, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.383. URL https://aclanthology.org/2020.acl-main.383.
* Xiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In _Proceedings of the 37th International Conference on Machine Learning_, ICML'20. JMLR.org, 2020.
* Xiong et al. (2021)Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 3770-3785, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.292. URL https://aclanthology.org/2021.acl-long.292.
* Yun et al. (2020) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=ByxRM0tvr.
* Zhang et al. (2020) Marvin Zhang, Henrik Marklund, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: A meta-learning approach for tackling group shift. _arXiv preprint arXiv:2007.02931_, 2020.
* Zhang et al. (2022) Yi Zhang, Arturs Backurs, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task, 2022. URL https://arxiv.org/abs/2206.04301.
* Zhao et al. (2023) Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word?, 2023.
* Zhong et al. (2023) Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. _arXiv preprint arXiv: 2306.17844_, 2023.

###### Contents

* 1 Introduction
	* 1.1 Related Work
* 2 Problem Setup
* 3 Theoretical Analyses
	* 3.1 Interpretability Requires Inspecting More Than Attention Patterns
		* 3.1.1 Perfect Balance Condition: Ideal Generalization of Unbounded Length
	* 3.2 Interpretability Requires Inspecting More Than Any Single Weight Matrix
* 4 Experiments
	* 4.1 Different Attention Patterns Can Be Learned To Generate Dyck
	* 4.2 Guiding The Transformer To Learn Balanced Attention
* 5 Conclusion
* A Additional Related Work
* B Are interpretable attention patterns useful?
* B.1 Can interpretable attention patterns be misleading?
* B.2 Are attention heads with interpretable patterns more important?
* C Omitted Proofs in Section 3
* C.1 Proof of Theorem 1
* C.2 Implication of our results to larger models
* C.3 Proof of Corollary 1
* C.4 Approximate Balance Condition For Finite Length Training Data
* C.4.1 Proof of Theorem 3
* C.5 Proof of Theorem 2
* C.5.1 Helper lemmas for Theorem 2
* C.6 Technical Lemmas
* C.7 Discussion on Architecture Choices
* D Experiments
* D.1 Training Details
* D.2 Additional Results on Dyck Prefix
* D.3 Extended Experiments

Additional Related Work

Interpreting Transformer solutionsPrior empirical works show that Transformers trained on natural language data can produce representations that contain rich syntactic and semantic information, by designing a wide range of "probing" tasks (Raganato & Tiedemann, 2018; Liu et al., 2019; Hewitt & Manning, 2019; Clark et al., 2019; Tenney et al., 2019; Hewitt & Liang, 2019; Kovaleva et al., 2019; Lin et al., 2019; Wu et al., 2020; Belinkov, 2022) (or other approaches using the attention weights or parameters in neurons directly Vig & Belinkov, 2019; Hutt et al., 2019; Sun & Marasovic, 2021; Eldan & Li, 2023). However, there is no canonical way to probe the model, partially due to the huge design space of probing tasks, and even a slight change in the setup may lead to very different (sometimes even seemingly contradictory) interpretations of the result (Hewitt & Liang, 2019). In this work, we tackle such ambiguity through a different perspective--by developing formal (theoretical) understanding of solutions learned by Transformers. Our results imply that it may be challenging to try to interpret Transformer solutions based on individual parameters (Li et al., 2016; Dar et al., 2022), or based on constructive proofs (unless the Transformer is specially trained to be aligned with a certain algorithm, as in Weiss et al., 2021).

Interpreting attention patternsPrior works (Jain & Wallace, 2019; Serrano & Smith, 2019; Rogers et al., 2020; Grimsley et al., 2020; Brunner et al., 2020; Prasanna et al., 2020; Meister et al., 2021; Bolukbasi et al., 2021; Haab et al., 2023, _inter alia_) present negative results on deriving explanations from attention weights using approaches by Vig & Belinkov (2019); Kobayashi et al. (2020, _inter alia_). However, Wiegreffe & Pinter (2019) argues to the contrary by pointing out flaws in the experimental design and arguments of some of the prior works; they also call for theoretical analysis on the issue. Hence, a takeaway from these prior works is that expositions on explainability based on attention requires clearly defining the notion of explainability adopted (often task-specific). In our work, we restrict our main theoretical analysis to the fully defined data distribution of Dyck language (Definition 1), and define "interpretable attention pattern" as the stack-like pattern proposed in prior theoretical (Yao et al., 2021) and empirical (Ebrahimi et al., 2020) works. These concrete settings and definitions allow us to mathematically state our results and provide theoretical reasons.

Theoretical understanding of representabilityMethodologically, our work joins a long line of prior works that characterize the solution of neural networks via the lens of simple synthetic data, from class results on RNN representability (Siegelmann & Sontag, 1992; Gers & Schmidhuber, 2001; Weiss et al., 2018; Suzgun et al., 2019; Merrill, 2019; Hewitt et al., 2020), to the more recent Transformer results on parity (Hahn, 2020), Dyck (Yao et al., 2021), topic model (Li et al., 2023), and formal grammars in general (Bhattamishra et al., 2020; Li & Risteski, 2021; Zhang et al., 2022; Liu et al., 2023; Zhao et al., 2023). Our work complements prior works by showing that although representational results can be obtained via intuitive "constructive proofs" that assign values to the weight matrices, the model does not typically converge to those intuitive solutions in practice. Similar messages are conveyed in Liu et al. (2023), which presents different types of constructions using different numbers of layers. In contrast, we show that there exist multiple different constructions even when the number of layers is kept the same.

There are also theoretical results on Transformers in terms of Turing completeness (Bhattamishra et al., 2020; Perez et al., 2021), universal approximatability (Yun et al., 2020), and statistical sample complexity (Wei et al., 2021; Edelman et al., 2022), which are orthogonal to our work.

Transformer optimizationGiven multiple global optima, understanding Transformer solutions requires analyzing the training dynamics. Recent works theoretically analyze the learning process of Transformers on simple data distributions, e.g. when the attention weights only depend on the position information (Jelassi et al., 2022), or only depend on the content (Li et al., 2023). Our work studies a syntax-motivated setting in which both content and position are critical. We also highlight that Transformer solutions are very sensitive to detailed changes, such as positional encoding, layer norm, sharpness regularization (Foret et al., 2020), or pre-training task (Liu et al., 2022). On a related topic but towards different goals, a series of prior works aim to improve the training process of Transformers with algorithmic insights (Nguyen & Salazar, 2019; Xiong et al., 2020; Liu et al., 2020; Zhang et al., 2020; Li & Gong, 2021, _inter alia_). An end-to-end theoretical characterization of the training dynamics remains an open problem; recent works that propose useful techniques towards this goal include Gao et al., 2023; Deng et al., 2023.

Mechanistic interpretabilityFinally, it is worth noting that the challenges highlighted in our work do not contradict the line of prior works that aim to improve _mechanistic interpretability_ into a trained model or the training process (Cammarata et al., 2020; Elhage et al., 2021; Olsson et al., 2022; Nanda et al., 2023; Chughtai et al., 2023; Li et al., 2023; Wang et al., 2023; Zhong et al., 2023): although we prove that components (e.g. attention scores) of trained Transformers do not generally admit intuitive interpretations based on the data distribution, it is still possible to develop circuit-level understanding about a particular model, or measures that closely track the training process, following these prior works.

Are interpretable attention patterns useful?

Our results Section 3 and Section 4.1 demonstrate that Transformers are sufficiently expressive that a (near-)optimal loss on Dyck languages can be achieved by a variety of attention patterns, many of which may not be interpretable.

However, multiple prior works have shown that for multi-layer multi-head Transformers trained on natural language datasets, it is often possible to locate attention heads that produce interpretable attention patterns (Vig and Belinkov, 2019; Htut et al., 2019; Sun and Marasovic, 2021). Hence, it is also illustrative to consider the _"converse question"_ of (Q1): when some attention heads do learn to produce attention patterns that suggest intuitive interpretations, what benefits can they bring?

We discuss this through two perspectives:

* **Reliability of interpretation:** Is the Transformer necessarily implementing a solution consistent with such interpretation based on the attention patterns? (Section B.1)
* **Usefulness for task performance:** Are those interpretable attention heads more important for the task than other uninterpretable attention heads? (Section B.2)

We present preliminary analysis on these questions, and motivate future works on the interpretability of attention patterns using rigorous theoretical analysis and carefully designed experiments.

### Can interpretable attention patterns be misleading?

We show through a simple argument that interpretations based on attention patterns can sometimes be misleading, as we formalize in the following proposition:

**Proposition 1**.: _Consider an L-layer Transformer \(\mathcal{T}\) (Equation (10)). For any \(W_{K}^{(l)},W_{Q}^{(l)}\in\mathbb{R}^{m_{a}\times m}\ (l\in[L])\), there exist \(W_{\mathrm{Head}}\in\mathbb{R}^{2k\times w}\) and \(b_{\mathrm{Head}}\in\mathbb{R}^{2k}\) such that \(\mathcal{T}(\mathcal{Z})=0,\forall\mathcal{Z}\)._

While its proof is trivial (simply setting \(W_{\mathrm{Head}}=0\) and \(b_{\mathrm{Head}}=0\) suffices), Proposition 1 implies that the solution represented by the Transformer could possibly be independent of the attention patterns in all the layers (\(1\) through \(l\)). Hence, it could be misleading to interpret Transformer solutions solely based on these attention patterns.

Empirically, Transformers trained on Dyck indeed sometimes produce misleading attention patterns.

We present one representative example in Figure 4, and Figure 5, in which _all interpretable attention patterns are misleading_.

We also present additional results in Figure 6, in which _some interpretable attention patterns are misleading, and some are not_.

Similar message has been conveyed in prior works Bolukbasi et al. (2021), and future works may aim to achieve the _faithfulness_, _completeness_, and _minimality_ conditions in Wang et al. (2023).

Figure 4: **Even interpretable attention patterns can be misleading**: For a 4-layer Transformer trained on Dyck with the _copying_ task (with \(>96\%\) validation accuracy), i.e. the output should be exactly the same as the input, the attention patterns in some layers seem interpretable: (layer 2) attending to bracket type a) or (b; (layer 3) attending to closing brackets; (layer 4) new attending to bracket type a); However, none of them are informative of the copying task. This is possible because Transformers can use the residual connections (or weights MLPs or the value matrices) to solve copying, bypassing the need of using attention.

Figure 5: **Even interpretable attention patterns can be misleading**: For a 1-layer Transformer trained on Dyck with the _copying_ task (with \(>90\%\) validation accuracy), i.e. the output should be exactly the same as the input, the attention pattern seems to be attending to closing brackets only, but that is not informative of the copying task.

Figure 6: **Even interpretable attention patterns can be misleading**: For a 4-layer Transformer trained on Dyck with the _copying_ task (with \(>96\%\) validation accuracy), i.e. the output should be exactly the same as the input, both types of attention patterns are common: (a) attending to closing brackets, which is uninformative of the copying task; (b) attending to the current position, which solves the copying task.

### Are attention heads with interpretable patterns more important?

Kovaleva et al. (2019) observes that, when the "importance" of an attention head is defined as the performance drop the model suffers when the head is disabled, then for most tasks they test, the most important attention head in each layer _does not_ tend to be interpretable.

However, experiments by Voita et al. (2019) led to a seemingly contradictory observation: when attention heads are systematically pruned by finetuning the Transformer with a relaxation of \(L_{0}\)-penalty (i.e. encouraging the number of remaining attention heads to be small), most remaining attention heads that survive the pruning can be associated with certain functionalities such as positional, syntactic, or attending to rare tokens.

These works seem to bring mixed conclusions to our question: are interpretable attention heads more important for a task than uninterpretable ones? We interpret these results by conjecturing that the definition of "importance" (reflected in their experimental design) plays a crucial role:

* When the importance of an attention head is defined _treating all other attention heads as fixed_, motivating experiments that prune/disable certain heads while keeping other heads unchanged (Michel et al., 2019; Kovaleva et al., 2019), the conclusion may be mostly pessimistic: mostly no strong connection between interpretability and importance.
* On the other hand, when the importance of an attention head is defined _allowing all other attention heads to adapt to its change_, motivating experiments that jointly optimize all attention heads while penalizing the number of heads (Voita et al., 2019), the conclusion may be more optimistic: the heads obtained as a result of this optimization tend to be interpretable.

We think the following trade-offs apply:

* On one hand, the latter setting is more practical, since Transformers are typically not trained to explicitly ensure that the model performs well when a single attention head is individually disabled; rather, it would be more intuitive to think of a group of attention heads as jointly representing some transformation, so when one head is disabled, other heads should be fine-tuned to adapt to the change.
* On the other hand, when all other heads change too much during such fine-tuning, the resulting set of attention heads no longer admit an unambiguous one-to-one map with the original set of (unpruned) attention heads. As a result, the interpretability and importance obtained from the set of pruned heads do not necessarily imply those properties of the original heads.

A comprehensive study of this question involves multi-head extensions of our theoretical results (Section 3), and carefully-designed experiments that take the above-mentioned trade-offs into consideration. We think these directions are interesting future work.

Omitted Proofs in Section 3

### Proof of Theorem 1

The key step is already shown in Section 3. We will restate the proof rigorously here.

**Theorem 1** (Perfect Balance).: _Consider a two-layer Transformer \(\mathcal{T}\) (Equation (10)) with a minimal first layer (Assumption 1) and \(C_{LN}=0\) (Equation (7)). Let \(\mathcal{O}\) denote the optimal prediction scenario, that is, when the first layer embeddings \(\{\bm{e}(\tau_{i,d})\}_{d\in[D],i\in[2k]}\) (Definition 2) and second layer parameters \(\theta^{(2)}\) satisfy_

\[\theta:=\{\bm{e}(\tau_{i,d})\}_{d\in[D],i\in[2k]},\theta^{(2)}\}=\arg\min_{ \hat{\theta}}\mathcal{L}(\tilde{\theta};\mathcal{D}_{q,k,D,N}),\forall N,\]

_where the objective \(\mathcal{L}\) is defined in Equation (4). Then,_

* _Equation (_11_) a necessary condition of_ \(\mathcal{O}\)_, if_ \(W^{(2)}_{V}\) _satisfies_ \(\mathcal{P}_{\perp}W^{(2)}_{V}\bm{e}(\tau_{t,d})\neq 0,\forall t\in[k],d\in[D]\)_._
* _Equation (_11_) is a sufficient condition of_ \(\mathcal{O}\)_, if the set of_ \(2k+1\) _encodings_ \(\{\bm{e}(\tau_{2i-1,d}),\bm{e}(\tau_{2i,d})\}_{i\in[k]}\cup\{\bm{e}(t_{S})\}\) _are linearly independent for any_ \(d\in[D]\)_, and the projection function_ \(\mathrm{g}^{(2)}\) _is a 6-layer MLP_ 12 _with_ \(O(k^{2}D^{2})\) _width._ Footnote 12: The 6 layers are by our construction. We will first use 4 layers to convert the input of the projection function to a triplet indicating the type and depth of the last token and the type of the last unmatched bracket when the last token is a closed bracket. We will then use another \(2\) layers to predict the next token probability based on the triplet. This construction may be improved.

Proof.: We prove the **sufficiency of the balanced condition** below; the proof for the _necessity_ has been given in Section 3.1.

We will denote the dimension of \(\bm{e}(\tau_{t,d})\) as \(m\).

For any \(i\in[k],d^{\prime}\in[D]\), by Equation (11), we can assume that there exists \(a_{i,d^{\prime}}\in\mathbb{R}\) such that for any \(j\in[k]\), \(d\in[D]\), it holds that,

\[a_{i,d^{\prime}}\triangleq\left(\bm{e}\left(\tau_{2i-1,d^{\prime}}\right)-\bm {e}\left(\tau_{2i,d^{\prime}-1}\right)\right)^{\top}(W^{(2)}_{K})^{\top}W^{( 2)}_{Q}\bm{e}\left(\tau_{2j,d}\right).\] (19)

We will first define the possible index sets of \(\tau_{t,d}\) as \(\mathcal{I}=\{(2t,d)\mid t\in[k],0\ \leq d\leq D-1\}\cup\{(2t-1,d)\mid t\in[k],1\leq d \leq D\}\), and we will define the rank of \((t,d)\) as

\[r(t,d)\triangleq\#\{(t_{1},d_{1})\mid t_{1}<t\text{ or }t_{1}=t,d_{1}\leq d,(t_{1},d_{1})\in\mathcal{I}\}\] (20)

Then it is clear that \(r(t,d)\) is a one-to-one mapping from \(\mathcal{I}\) to \([2kD]\). We will then define the collection of all \(\bm{e}(\tau_{t,d})\) as \(\bm{E}\), satisfying that \(\bm{E}_{:,r(t,d)}=\bm{e}(\tau_{t,d}),\bm{E}_{:,2kD+1}=\bm{e}(t_{S})\).

Because \(\bm{e}(\tau_{t,d})\) are linearly independent, for any \((i,d)\neq(j,d^{\prime})\in\mathcal{I}\), it holds that \(\bm{e}(\tau_{i,d})-\bm{e}(\tau_{j,d^{\prime}})\neq 0\). Then based on Lemma 16, there exists a set of orthonormal vectors \(\{\mathrm{b}_{i}\}_{i\in[m-2]}\), such that for any \((i,d),(j,d^{\prime})\in\mathcal{I}\), it holds that

\[\sum_{i=1}^{m-2}\mathrm{b}_{i}\mathrm{b}_{i}^{\top}\left(\bm{e}( \tau_{i,d})-\bm{e}(\tau_{j,d^{\prime}})\right) \neq(\bm{e}(\tau_{i,d})-\bm{e}(\tau_{j,d^{\prime}})\] (21) \[\mathrm{b}_{i}^{\top}1^{m} =0\] (22)

We will further construct matrix \(\bm{O}\) as 13

Footnote 13: Recall the definition of \(r\) in Equation (20). Comparing \(\bm{O}_{:,r(2t,d-1)}\) and \(\bm{O}_{:,r(2t-1,d)}\): the idea is that a pair of matched brackets are represented by the same direction (i.e. the direction along \(\mathrm{b}_{iD+d}\)), just with different norms.

\[\bm{O}_{:,r(2t,d-1)} =-\exp(a_{t,d})\mathrm{b}_{tD+d},\] \[\bm{O}_{:,r(2t-1,d)} =\mathrm{b}_{tD+d}.\] (23) \[\bm{O}_{:,2kD+1} =0.\]

[MISSING_PAGE_FAIL:24]

By Equation (26) and Equation (27),

\[u(\tau_{2t_{k},d_{k}-1},\tau_{2j,d})+u(\tau_{2t_{k}-1,d_{k}},\tau_{ 2j,d})\] \[=\exp\left(\bm{e}\big{(}\tau_{2t_{k}-1,d_{k}}\big{)}^{\top}(W_{K}^{ (2)})^{\top}W_{Q}^{(2)}\bm{e}\big{(}\tau_{2j,d}\big{)}\right)\mathrm{b}_{t_{k}D+ d_{k}}\] \[\quad-\exp\left(\bm{e}\big{(}\tau_{2t_{k},d_{k}-1}\big{)}^{\top}(W_ {K}^{(2)})^{\top}W_{Q}^{(2)}\bm{e}\big{(}\tau_{2j,d}\big{)}\right)\exp(a_{t_{k},d_{k}})\mathrm{b}_{t_{k}D+d_{k}}\] \[=\big{[}\exp\left(\bm{e}\big{(}\tau_{2t_{k}-1,d_{k}}\big{)}^{\top }(W_{K}^{(2)})^{\top}W_{Q}^{(2)}\bm{e}\big{(}\tau_{2j,d}\big{)}\right)\] \[\quad-\exp\left(\bm{e}\big{(}\tau_{2t_{k},d_{k}-1}\big{)}^{\top}(W _{K}^{(2)})^{\top}W_{Q}^{(2)}\bm{e}\big{(}\tau_{2j,d}\big{)}+a_{t_{k},d_{k}} \right)\big{]}\mathrm{b}_{t_{k}D+d_{k}}\] \[=0\] (28)

in which the last line is because the terms inside \(\big{[}\cdots\big{]}\) cancel each other, because by Equation (19)

Plugging Equation (28) and Equation (26) into Equation (25),

\[\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1} =\sum_{s=1}^{d}u(\tau_{2j_{s}-1,s},\tau_{2j,d})\] \[=\sum_{s=1}^{d}\exp\left(\bm{e}\big{(}\tau_{2j_{s}-1,s}\big{)}^{ \top}(W_{K}^{(2)})^{\top}W_{Q}^{(2)}\bm{e}\big{(}\tau_{2j,d}\big{)}\right) \mathrm{b}_{j_{s}D+s}\] (29)

Therefore, \(\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1}\) lies in the span of \(\{\mathrm{b}_{j_{s}D+s}\}_{s\in[d]}\). We will from now on assume \(\langle\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n}),\mathrm{b}_{j_{s}D+s }\rangle>M\) for all possible choices of \(p\) ending with a closed bracket with grammar depth at least \(1\) for some constant \(M\in(0,1)\). Here \(M\) exists because

\[\langle\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n}),\mathrm{b}_{j_{s}D+s }\rangle=\frac{\exp\left(\bm{e}\big{(}\tau_{2j_{s}-1,s}\big{)}^{\top}(W_{K}^{ (2)})^{\top}W_{Q}^{(2)}\bm{e}\big{(}\tau_{2j,d}\big{)}\right)}{\sqrt{\sum_{s^ {\prime}=1}^{d}\exp\left(2\bm{e}\big{(}\tau_{2j^{\prime}_{s}-1,s}\big{)}^{ \top}(W_{K}^{(2)})^{\top}W_{Q}^{(2)}\bm{e}\big{(}\tau_{2j,d}\big{)}\right)}}>0,\]

for all possible combination of \(j_{k},k\in[d]\) and \(s\), and there are only finite number of such combinations.

Constructing the projection function \(\mathrm{g}^{(2)}\)We will finally show there exists a 6-layer MLP \(\mathrm{g}^{(2)}\) with width \(O(D^{2}k^{2})\), such that for any dyck prefix \(q\) with \(n\) being the length of \(q\), \(X\) being the input of the second layer given \(q\) and \(\mathbb{P}(p)\) being the groundtruth next-token probability vector given \(q\)15, it holds that, \(\mathrm{g}^{(2)}\left(\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1})+X_{:,n +1}\right)=\mathbb{P}(q)\).

Footnote 15: That is \(\mathbb{P}(q)_{t}=\mathbb{P}(\text{The next token of $q$ has type $t$})\)

We will assume the last token of \(q\) is \(\tau_{t,d}\). Suppose that \(\mathrm{b}_{m-1},\mathrm{b}_{m}\) is an orthonormal basis of the normal space of \(\text{span}\{\mathrm{b}_{1},..,\mathrm{b}_{m-2}\}\), then we can first observe that for \(U=\mathrm{b}_{m}\mathrm{b}_{m}^{\top}+\mathrm{b}_{m-1}\mathrm{b}_{m-1}^{\top}\), it holds that

\[U(\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1})+X_{:,n+1})=U\bm{e}(\tau_{ t,d}).\]

is unique for every \(t,d\). Then based on Lemma 15, there exists a 2-layer MLP with width \(4kD\) that maps \(U(\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1})+X_{:,n+1})\) to \((t,d)\). This implies that there exists a 2-layer MLP with width \(4kD\) that maps \(\mathrm{LN}((\tilde{a}_{2}(X;\theta^{(2)})_{:,n})+X_{:,n}\) to \((t,d)\).

Further, let matrix \(U^{\prime}=\sum_{j=1}^{Dk}\bm{o}_{j}\mathrm{b}_{j}^{\top}\) where \(\bm{o}_{j}\) is the \(Dk\) dimension one-hot vector with the \(j-\)th entries being \(1\). Then when \(t\) is an even number and \(d\geq 1\), based on Equation (29) and the definition of \(M\),

\[U^{\prime}(\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1})+X_{:,n+1})_{t^{ \prime}D+d^{\prime}}\begin{cases}=0,&\tau_{2t^{\prime}-1,d^{\prime}}\text{ is not an unmatched open brackets in $p$}.\\ >M,&\tau_{2t^{\prime}-1,d^{\prime}}\text{ is an unmatched open brackets in $p$}.\end{cases}\]Then based on Lemma 18, there exists 2-layer MLP with width \(kD\) that operates on \(\big{(}U^{\prime}(\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1})+X_{:,n+1})_{ t^{\prime}D+d^{\prime}}\big{)}_{t^{\prime}\in[k]}\) for a fixed \(d^{\prime}\) and output the nonzero index in it, if such index exists. Hence, we can choose the weight of the first and second layer of \(\mathrm{g}^{(2)}\), such that the output of the second layer is \((t,d)\oplus x\), where \(2x_{d^{\prime}}-1\) is the type of the unmatched open brackets with grammar depth \(d^{\prime}\) if \(t\) is an even number, \(d\geq d^{\prime}\geq 1\).

Now based on Lemma 17, we can choose the third and fourth layer of \(\mathrm{g}^{(2)}\) to perform indexing and let the output of the fourth layer be \((t,d,y)\), where \(y=x_{d}\) when \(d\geq 1\). 16 Notice that this triplet contains all the necessary information to infer \(\mathbb{P}(q)\) because it uniquely determines the type of last unmatched open bracket,

Footnote 16: When \(d=0\), \(y\) does not matter since there is no unmatched open brackets.

1. If \(t\) is odd (i.e. the last bracket is open), and then the type of last unmatched open bracket is \(t\).
2. If \(t\) is even and \(d=0\), then all the brackets is matched.
3. If \(t\) is even and \(d\geq 1\), then the type of last unmatched bracket is \(y\).

One may finally construct a 2-layer MLP \(f\) that maps \((t,d,y)\) to the corresponding probability vector. As the input of \(\mathrm{g}\) has bounded norm,

\[\|\mathrm{LN}(\tilde{a}_{2}(X;\theta^{(2)})_{:,n+1})+X_{:,n+1}\|_{2}\leq 1+ \max_{t,d}\|\bm{e}(\tau_{t,d})\|,\]

the output of the constructed 4 layers also has a bounded norm. Hence, we can assume there exists constant \(M^{\prime}>1\), such that \(y\leq M^{\prime}\). Now we will discuss by the value of \(t\),

1. \(t\) is odd, then one can neglect the third dimension and the correct probability is determined by \(d\) and can be represented by a width-\(2D\) network based on Lemma 15.
2. \(t\) is even. When \(d=0\), one can construct a width-\(1\) network mapping any \(y\) to the correct probability distribution as it is unique. When \(d\geq 1\), one can construct a width-\(2K\) network mapping \(x_{d}\in[K]\) to the correct probability distribution based on Lemma 15. Then by Lemma 19, one can construct a width-\(4KD\) network that maps \((d,y)\) to the corresponding probability distribution.

Putting together and using Lemma 19 again, one can construct a width-\(8K^{2}D\) network that maps \((t,d,y)\) to the correct next token probability prediction. The proof is then completed.

### Implication of our results to larger models

Recall that the main conclusion of our paper is that interpretability based on a single Transformer component (e.g. an attention pattern or an MLP block) can be unreliable, since the set of optimal solutions can give rise to a large set of attention patterns and pruned MLP weights. Section 3 has demonstrated this with simple two-layer Transformers. The simplicity of this architecture choice is intentional, since our theory on two-layer Transformers directly implies similar conclusions for larger models, as we discuss in this section.

Intuitively, when moving to more complex architectures, the set of solutions can only grow and complicate interpretability further, hence our main conclusion still stands. For example, even though Theorem 1 and Theorem 3 are stated for 2-layer Transformers only, the constructed solutions can be trivially extended to multiple layers by e.g. letting the higher layers perform the identity function, or removing Assumption 1 and allowing the model to flexibly use or ignore positional information. More precisely:

* For Transformers with greater width, our Theorem 1 applies directly, since the construction does not depend on the width.
* For Transformers with greater depth, it suffices to show that additional layers can perform the identity function. To this end, one can utilize the residue link in the Transformer layer and choose the value matrix to be zero and the FFN (with or without residue connection) to be identity. Thisconstruction is implicitly assuming LayerNorm will map zero vector to zero vector, which is true for the common PyTorch implementation and for our paper. Also, it is worth noting that this holds for both the architecture we considered in the paper and the standard GPT-2 architecture.

### Proof of Corollary 1

**Corollary 1**.: _There exists a 2-layer Transformer with uniform attention and no position embedding (but with causal mask and a starting token 17 ) that generates the Dyck language of arbitrary length._

Footnote 17: Here the starting token is necessary because otherwise, the Transformer with uniform attention will have the same outputs for prefix \(p\) and prefix \(p\oplus p\), in which \(\oplus\) denotes concatenation, i.e. \(p\oplus p\) means the same string \(p\) repeated twice.

Proof.: We will first construct a uniform attention first layer that can generate the embedding in Equation (15). Suppose \(Z\) is the one-hot embeddings of a prefix \(p\) of length \(n\), where each token of type \(t\) for \(t\in[2k]\) is encoded as \(\bm{o}_{t}\) and the starting token is encoded as \(\bm{o}_{2k+1}\). Then it holds that

\[\Big{[}Z\sigma\Big{(}\mathcal{C}\cdot(W_{K}^{(1)}Z)^{\top}(W_{Q}^{(1)}Z)\Big{)} \Big{)}\Big{]}_{:,n+1}=\sum_{i=1}^{2k}\#\{\text{token of type $t$ in $p$}\}\bm{o}_{t}+\bm{o}_{2k+1}.\] (30)

Then we can choose \(W_{V}^{(1)}\) such that for \(x\in\mathbb{R}^{2k+1}\),

\[(W_{V}^{(1)}x)_{1}= \sum_{i=1}^{k}x_{2i-1}-x_{2i},\] \[(W_{V}^{(1)}x)_{2}= x_{2k+1},\] \[(W_{V}^{(1)}x)_{i}= 0,\forall i\geq 3.\]

Hence it holds That

\[\Big{[}W_{V}^{(1)}Z\sigma\Big{(}\mathcal{C}\cdot(W_{K}^{(1)}Z)^{\top}(W_{Q}^{( 1)}Z)\Big{)}\Big{)}\Big{]}_{:,n+1}=\#\{\text{depth of $p_{n}$}\}\bm{o}_{1}+\bm{o}_{2}.\]

It is then easy to check \(\operatorname{LN}\left(\Big{[}W_{V}^{(1)}Z\sigma\Big{(}\mathcal{C}\cdot(W_{K} ^{(1)}Z)^{\top}(W_{Q}^{(1)}Z)\Big{)}\Big{)}\Big{]}_{:,n+1}\right)+Z_{:,n+1}\) is uniquely determined by the type and depth of \(p_{n}\) without repetition. Then by Lemma 15, there exists a 2-layer ReLU MLP with width \(O(k^{2}D^{2})\) that can map \(\operatorname{LN}\left(\Big{[}W_{V}^{(1)}Z\sigma\Big{(}\mathcal{C}\cdot(W_{K} ^{(1)}Z)^{\top}(W_{Q}^{(1)}Z)\Big{)}\Big{)}\Big{]}_{:,n+1}\right)+Z_{:,n+1}\) to the embedding in Equation (15). It is then easy to see that the condition in Theorem 1 is satisfied as \(W_{K}^{(2)}=W_{Q}^{(2)}=0\). Hence the second layer can be constructed to let the Transformer to output the correct next token probability. 

### Approximate Balance Condition For Finite Length Training Data

Theorem 1 assumes the model reaches the optimal loss for Dyck prefixes of any length. However, in practice, due to finite samples and various sources of randomness, training often does not end exactly at a population optima. In this case, the condition in Theorem 1 is not precisely met. However, even for models that _approximately_ meet those conditions, we will prove that when the second-layer projection function \(\mathrm{g}^{(2)}\) is Lipschitz, a similar condition as in Equation (14) is still necessary.

We will show this by bounding the amount of deviations from the perfect balance. The idea is that for two long prefixes that differ in only the last open bracket, correct next token prediction requires the Transformer outputs on these prefixes to be sufficiently different, hence the part irrelevant to the prediction (i.e. matched brackets) should not have a large contribution.

To formalize this intuition, let's define two quantities: 1) \(S_{d,d^{\prime},i,j}\) which measures the effect from one matching pair, and 2) \(P_{d,j}\) which measures the effect on the last position from all tokens in a prefix.

Let \(u\) be defined as in Equation (12). \(S_{d,d^{\prime},i,j}\) is defined as

\[S_{d,d^{\prime},i,j}[\theta^{(2)}]=u(\tau_{2j,d},\tau_{2i,d^{\prime}-1})+u(\tau_ {2j,d},\tau_{2i-1,d^{\prime}}),\] (31)

which measures how much a matching pair of brackets \((\tau_{2i,d^{\prime}-1},\tau_{2i-1,d^{\prime}})\) changes the input to the LayerNorm upon seeing the last token \(\tau_{2j,d}\). Note that under the perfect balance condition, \(S_{d,d^{\prime},i,j}[\theta^{(2)}]=0\).

The second quantity \(P_{d,j}[\theta^{(2)}]\) is defined via an intermediate quantity \(Q(2j,d,\tilde{\bm{t}})\): for any \(i\in[k],d\in[D]\) and a length-\((d-1)\) prefix \(\tilde{\bm{t}}\in[2k]^{d-1}\), \(Q(i,d,\tilde{\bm{t}})\) is defined as

\[Q(i,d,\tilde{\bm{t}}) :=u(\tau_{2i,d-1},t_{\mathcal{S}})+\sum_{1\leq d^{\prime}<d}u( \tau_{2i,d-1},\tau_{\tilde{\bm{t}}_{d^{\prime}},d^{\prime}})\] (32) \[\quad+u(\tau_{2i,d-1},\tau_{2i-1,d})+u(\tau_{2i,d-1},\tau_{2i,d-1 }),\]

where \(\tilde{\bm{t}}_{d^{\prime}}\) denotes the \(d^{\prime}_{th}\) entry of \(\tilde{\bm{t}}\). Intuitively, \(Q(i,d,\tilde{\bm{t}})\) denotes the unnormalized second-layer attention output at the last position, given the input sequence \(\tilde{\bm{t}}\oplus\tau_{2i-1,d}\tau_{2i,d-1}\), 18

Footnote 18: We use \(s\oplus t\) to denote the concatenation of two strings \(s,t\), same as in Equation (15)-(16), and use \(\tau_{i}\tau_{j}\) to denote the concatenation of two tokens \(\tau_{i},\tau_{j}\).

For results in this subsection, it suffices to consider prefixes consisting only of open brackets. Let \(\bm{t}:=\arg\min_{\tilde{\bm{t}}\in\{2i-1\}_{i\in[k]}^{d-1}}\|Q(2j,d,\tilde{ \bm{t}})\|_{2}\), and let \(\bm{t}^{\prime}\) denote the prefix that minimizes \(\|Q(2j,d,\tilde{\bm{t}})\|_{2}\) subject to the constraint that \(\bm{t}^{\prime}\) differs from \(\bm{t}\) at the last (i.e. \(\left(d-1\right)_{th}\)) position, i.e.

\[\bm{t}^{\prime}=\arg\min_{\tilde{\bm{t}}^{\prime}\in\{2i-1\}_{i\in[k]}^{d-1} \bm{t}^{\prime}_{d-1}\neq\bm{t}_{d-1}}Q(2j,d,\tilde{\bm{t}}^{\prime}).\]

Such choices of \(\bm{t},\bm{t}^{\prime}\) guarantees that the two prefixes differ at the last open bracket and hence must have different next-word distributions. Finally, define

\[P_{d,j}[\bar{\theta}^{(2)}]=\|Q(2j,d,\bm{t}^{\prime})\|.\] (33)

In the following theorem, \(P_{d,j}\) will be used as a reference to upper bound \(S_{d,d^{\prime},i,j}[\theta^{(2)}]\), meaning that the model should not be sensitive to the insertion of a matching pair of brackets.

**Theorem 3** (Approximate Balance).: _Consider a 2-layer Transformer \(\mathcal{T}\) (Equation (10)) with a minimal first layer (Assumption 1) and a \(\gamma\)-Lipschitz \(\mathrm{g}^{(2)}\) for \(\gamma>0\), trained on sequences of length \(N\) with the mean squared loss (Equation (5))._

_Suppose the loss is approximately optimal, namely, the set of second-layer weights \(\bar{\theta}^{(2)}_{N}\) satisfies \(\mathcal{L}(\mathcal{T}[\bar{\theta}^{(2)}_{N}],\mathcal{D}_{q,k,D,N})\leq q ^{-N}\epsilon\), for any positive integer \(N>8D\) and sufficiently small \(\epsilon>0\). Then, there exists a constant \(C_{\gamma,\epsilon,D}\), such that \(\forall 0\leq d^{\prime}\leq D,1\leq d\leq D,i,j\in[k]\), it holds that_

\[\|S_{d,d^{\prime},i,j}[\bar{\theta}^{(2)}_{N}]\|\leq\frac{C_{\gamma,\epsilon, D}}{N}P_{d,j}[\bar{\theta}^{(2)}_{N}].\] (34)

Intuitively, Theorem 3 states that when the loss \(\mathcal{L}(\theta)\) is sufficiently small, \(S_{d,d^{\prime},i,j}[\theta^{(2)}]\) must be small relative to \(P_{d,j}[\bar{\theta}^{(2)}_{N}]\). Inequality 34 can be interpreted as a relaxation of Equation (14), which is equivalent to \(S_{d,d^{\prime},i,j}[\theta^{(2)}]=0\). The proof of Theorem 3 shares a similar intuition as Theorem 1 and is given in Appendix C.4.1.

A direct corollary of Theorem 3 additionally consider weight decay, in which case approximate balance condition still holds, as the regularization strength goes to 0:

**Corollary 3**.: _Consider the setting where a Transformer with a fixed minimal first layer is trained to minimize \(\mathcal{L}^{\text{reg}}_{\lambda}=\mathcal{L}_{\theta}(x)+\lambda\frac{\| \theta\|_{2}^{2}}{2}\), which is the squared loss with \(\lambda\) weight decay. Suppose \(\mathrm{g}^{(1)}\) of the Transformer is a \(2\)-layer fully connected network and \(\mathrm{g}^{(2)}\) of the Transformer is a \(6\)-layer fully connected network. Then, there exists constant \(C>0\), such that if a set of parameters \(\theta_{\lambda,N}\) minimizes \(\mathcal{L}^{\text{reg}}_{\lambda}\), then it holds \(\forall 0\leq d^{\prime}\leq D,1\leq d\leq D,i,j\in[k]\) that,_

\[\forall N,\exists\lambda_{N},\text{ such that }\forall\lambda\in[0,\lambda_{N}],S_{d,d ^{\prime},i,j}[\theta^{(2)}_{\lambda,N}]\leq\frac{C}{N}P_{d,i}[\theta^{(2)}_{ \lambda,N}].\]

#### c.4.1 Proof of Theorem 3

Proof.: The key idea is similar to the proof of necessity in Theorem 1. That is, we will construct two input sequences with different next-word distributions, and show that the approximate balance condition must hold so that inserting (a bounded number of) pairs of matching brackets does not collapse the two predicted distributions given by the Transformer.

**Constructing the input sequences.**

Let \(\bm{t}:=\arg\min_{\tilde{\bm{t}}\in[k]^{d-1}}\|Q(2j,d,\tilde{\bm{t}})\|_{2}\), and let \(\bm{t}^{\prime}\) denote the prefix that minimizes \(\|Q(2j,d,\tilde{\bm{t}})\|_{2}\) subject to the constraint that \(\bm{t}^{\prime}\) must differ from \(\bm{t}\) in the last (i.e. \(\left(d-1\right)_{th}\)) position, i.e.

\[\bm{t}^{\prime}=\arg\min_{\tilde{\bm{t}}^{\prime}\in[k]^{d-1},\bm{t}^{\prime}_ {d-1}\neq\bm{t}_{d-1}}Q(2j,d,\tilde{\bm{t}}^{\prime}).\]

The motivation for such choices of \(\bm{t},\bm{t}^{\prime}\) is that since they differ at least by the last position which is an open bracket, they must lead to different next-word distributions. Note also that \(P_{d,j}[\bar{\theta}^{(2)}]=\|Q(2j,d,\bm{t}^{\prime})\|\).

With the above definition of \(\bm{t},\bm{t}^{\prime}\), consider two valid Dyck prefixes \(p_{1}\) and \(p_{2}\) with length no longer than \(N\), defined as follows: for any \(d,d^{\prime}\in[D],i,j\in[k]\), consider a common prefix \(p=\underbrace{\tau_{2i-1}\dots\tau_{2i-1}}_{d^{\prime}\text{ open brackets}}\underbrace{\tau_{2i-1}\tau_{2i}\dots\tau_{2i-1}\tau_{2i}}_{ \left(\lfloor\frac{N}{2}\rfloor-d^{\prime}-d-1\right)\text{ pairs}}\underbrace{\tau_{2i}\dots\tau_{2i}}_{d^{\prime}\text{ closed brackets}}\), where \(\tau_{i}\) denotes a token with type \(i\) whose depth is implicit from the context. Set \(p_{1},p_{2}\) as

\[p_{1} =p\oplus\bm{t}\oplus\tau_{2j-1}\tau_{2j},\] \[p_{2} =p\oplus\bm{t}^{\prime}\oplus\tau_{2j-1}\tau_{2j}.\]

That is, \(p_{1},p_{2}\) differ in the last unmatched open bracket. In the following, we will show that the approximate balance condition must hold for the predictions on \(p_{1},p_{2}\) to be sufficiently different.

**Bounding the difference in Transformer outputs.** For a Transformer \(\mathcal{T}\) with second layer parameters \(\bar{\theta}_{N}^{(2)}\), its outputs on \(p_{1},p_{2}\) satisfy

\[\|\mathcal{T}[\bar{\theta}_{N}^{(2)}](p_{1})-\mathcal{T}[\bar{ \theta}_{N}^{(2)}](p_{2})\|_{2}\] (35) \[\geq \|p_{1}-p_{2}\|_{2}-\|\mathcal{T}[\bar{\theta}_{N}^{(2)}](p_{1})- p_{1}\|_{2}-\|\mathcal{T}[\bar{\theta}_{N}^{(2)}](p_{2})-p_{2}\|_{2}\] (36) \[\geq \frac{1}{\sqrt{2k}}\|p_{1}-p_{2}\|_{1}-\left(\|\mathcal{T}[\bar{ \theta}_{N}^{(2)}](p_{1})-p_{1}\|_{1}+\|\mathcal{T}[\bar{\theta}_{N}^{(2)}](p_ {2})-p_{2}\|_{1}\right)\] (37) \[\geq \frac{1}{\sqrt{2k}}\text{TV}(p_{1},p_{2})-o_{\epsilon}(1)\quad \text{(since $\mathcal{L}(\mathcal{T}[\bar{\theta}_{N}^{(2)}],\mathcal{D}_{q,k,D,N})\leq q ^{-N}\epsilon$)}\] (38) \[= \Omega(1),\] (39)

where \(\text{TV}(p_{1},p_{2})\) denotes the TV distance in the next-word distributions from \(p_{1}\) and \(p_{2}\), and \(o_{\epsilon}(1)\) means the term will go to zero as \(\epsilon\) goes to zero. The TV distance is lower bounded by the construction of \(p_{1},p_{2}\), where \(\bm{t},\bm{t}^{\prime}\) differ at the last open bracket. The error \(\epsilon\) is upper bounded because of the assumption on \(\bar{\theta}_{N}^{(2)}\), i.e. \(\mathcal{L}(\mathcal{T}[\bar{\theta}_{N}^{(2)}],\mathcal{D}_{q,k,D,N})\leq q^{ -N}\epsilon\) with sufficiently small \(\epsilon\).

Define by \(A_{p}\) the contribution of \(p\) to the attention output (before LayerNorm) of the last position of \(p_{1},p_{2}\):

\[A_{p}= \sum_{1\leq d^{\prime\prime}<d^{\prime}}\left(u(\tau_{2j,d-1}, \tau_{2i,d^{\prime\prime}-1})+u(\tau_{2j,d-1},\tau_{2i-1,d^{\prime\prime}})\right)\] \[+\left\lfloor\frac{N-2d^{\prime}-2d}{2}\right\rfloor\left(u(\tau_ {2j,d-1},\tau_{2i,d^{\prime}})+u(\tau_{2j,d-1},\tau_{2i-1,d^{\prime}+1}) \right).\] (40)

The attention outputs (before LayerNorm) of \(p_{1},p_{2}\), denoted by \(A(p_{1})\) and \(A(p_{2})\), satisfy that

\[\mathcal{P}_{\perp}A(p_{1})=\mathcal{P}_{\perp}(A_{p}+Q(2j,d,\bm{t })),\] \[\mathcal{P}_{\perp}A(p_{2})=\mathcal{P}_{\perp}(A_{p}+Q(2j,d,\bm{t }^{\prime})).\] (41)Note that for any prefix \(p^{\prime}\),

\[\mathcal{T}[\bar{\theta}_{N}^{(2)}](p^{\prime})=\mathrm{g}^{(2)} \big{(}\mathrm{LN}_{C_{LN}}(\mathcal{P}_{\perp}A(p^{\prime}))\big{)}+\bm{e}( \tau_{2i,d^{\prime}})\] (42) \[= \mathrm{g}^{(2)}\Big{(}\frac{\mathcal{P}_{\perp}A(p^{\prime})}{ \|\mathcal{P}_{\perp}A(p^{\prime})\|}\Big{)}+\bm{e}(\tau_{2i,d^{\prime}}),\] (43)

where \(\mathrm{g}^{(2)}\) is \(\gamma\)-Lipschitz. Hence the Lipschitz constant with respect to and we have

\[\Big{\|}\frac{\mathcal{P}_{\perp}A(p_{1})}{\|\mathcal{P}_{\perp}A(p_{1})\|_{2 }}-\frac{\mathcal{P}_{\perp}A(p_{2})}{\|\mathcal{P}_{\perp}A(p_{2})\|_{2}} \Big{\|}_{2}\geq\frac{\mathrm{TV}(p_{1},p_{2})-o_{\epsilon}(1)}{\sqrt{2k\gamma }}=\Omega_{\frac{1}{\gamma},\epsilon}(1).\] (44)

We show that \(A_{p}\) should not be too much larger in norm than \(Q(2j,d,\bm{t})\) or \(Q(2j,d,\bm{t}^{\prime})\). First, let's state a helper lemma about the contrapositive:

**Lemma 1**.: _For any \(\epsilon>0\), there exists a constant \(R_{\epsilon}\), such that for any \(a,b\in\mathbb{R}^{d}\) and any \(r\in\mathbb{R}^{d}\) such that \(\|r\|_{2}\geq R_{\epsilon}\cdot\max\{\|a\|_{2},\|b\|_{2}\}\), it holds that_

\[\Big{\|}\frac{a+r}{\|a+r\|_{2}}-\frac{b+r}{\|b+r\|_{2}}\Big{\|}_{2}\leq\epsilon.\]

Proof.: Denote \(r_{0}:=\max\{\|a\|_{2},\|b\|_{2}\}\). Then \(R_{\epsilon}:=\frac{4r_{0}}{\epsilon}+1\) suffices:

\[\Big{\|}\frac{r+a}{\|r+a\|_{2}}-\frac{r+b}{\|r+b\|_{2}}\Big{\|} \leq\|r\|\cdot\Big{|}\frac{1}{\|r+a\|}-\frac{1}{\|r+b\|}\Big{|}+\frac{\|a\|}{ \|r+a\|}+\frac{\|b\|}{\|r+b\|}\] \[\leq \|r\|\cdot\Big{(}\frac{1}{\|r\|-r_{0}}-\frac{1}{\|r\|+r_{0}} \Big{)}+\frac{2r_{0}}{\|r\|-r_{0}}\] \[= \frac{2r_{0}}{\|r\|-r_{0}}\cdot\Big{(}\frac{\|r\|}{\|r\|+r_{0}}+1 \Big{)}\leq\frac{4r_{0}}{\|r\|-r_{0}}\leq\frac{4r_{0}}{R_{\epsilon}-r_{0}} \leq\epsilon.\]

Lemma 1 implies that if \(A_{p}\) is too large, then the output on \(p_{1},p_{2}\) (Equation (44)) won't be sufficiently different. Let \(P_{d,j}[\bar{\theta}_{N}^{(2)}]\) be defined as in Equation (33) and let \(R_{\epsilon}\) be the constant in Lemma 1, we need to bound \(\|\mathcal{P}_{\perp}A_{p}\|\) by

\[\|\mathcal{P}_{\perp}A_{p}\|_{2}\leq R_{\epsilon}\|P_{d,j}[\bar{\theta}_{N}^{(2 )}]\|_{2}.\] (45)

As Equation (45) holds for \(p\) with any \(d,d^{\prime}\), if one choose \(d^{\prime}=1\), this shows

\[\|u(\tau_{2j,d-1},\tau_{2i,1})+u(\tau_{2j,d-1},\tau_{2i-1,2})\|_{2}\leq\frac{ 4R_{\epsilon}\|P_{d,j}[\bar{\theta}_{N}^{(2)}]\|_{2}}{N}.\] (46)

Further, it holds that for any \(1<d^{\prime}\leq d-1\),

\[\|\sum_{1\leq d^{\prime\prime}<d^{\prime}}\left(u(\tau_{2j,d-1}, \tau_{2i,d^{\prime\prime}-1})+u(\tau_{2j,d-1},\tau_{2i-1,d^{\prime\prime}})\right)\] \[+\lfloor\frac{N-2d^{\prime}-2d}{2}\rfloor\left(u(\tau_{2j,d-1}, \tau_{2i,d^{\prime}})+u(\tau_{2j,d-1},\tau_{2i-1,d^{\prime}+1})\right)\|_{2} \leq R_{\epsilon}\|P_{d,j}[\bar{\theta}_{N}^{(2)}]\|_{2}.\] \[\|\sum_{1\leq d^{\prime\prime}<d^{\prime}+1}\left(u(\tau_{2j,d-1}, \tau_{2i,d^{\prime\prime}-1})+u(\tau_{2j,d-1},\tau_{2i-1,d^{\prime\prime}})\right)\] \[+\lfloor\frac{N-2d^{\prime}-2d-2}{2}\rfloor\left(u(\tau_{2j,d-1}, \tau_{2i,d^{\prime}+1})+u(\tau_{2j,d-1},\tau_{2i-1,d^{\prime}+2})\right)\|_{2} \leq R_{\epsilon}\|P_{d,j}[\bar{\theta}_{N}^{(2)}]\|_{2}.\]

The triangle inequality then yields,\[\lfloor\frac{N-2d^{\prime}-2d-2}{2}\rfloor\| \,(u(\tau_{2j,d-1},\tau_{2i,d^{\prime}+1})+u(\tau_{2j,d-1},\tau_{2i- 1,d^{\prime}+2}))\] \[-(u(\tau_{2j,d-1},\tau_{2i,d^{\prime}})+u(\tau_{2j,d-1},\tau_{2i-1, d^{\prime}+1}))\,\|_{2}\leq 2R_{\epsilon}\|P_{d,j}[\tilde{\theta}_{N}^{(2)}]\|_{2}.\]

Because \(N\geq 8D\), we have that \(\lfloor\frac{N-2d^{\prime}-2d-2}{2}\rfloor\geq\frac{N}{8}\), hence it holds that

\[\| \,(u(\tau_{2j,d-1},\tau_{2i,d^{\prime}+1})+u(\tau_{2j,d-1},\tau_{2 i-1,d^{\prime}+2}))\] \[-(u(\tau_{2j,d-1},\tau_{2i,d^{\prime}})+u(\tau_{2j,d-1},\tau_{2i- 1,d^{\prime}+1}))\,\|_{2}\leq\frac{16R_{\epsilon}\|P_{d,j}[\tilde{\theta}_{N}^ {(2)}]\|_{2}}{N}.\]

Combined with Equation (46), one can conclude that,

\[S_{d,d^{\prime},i,j}=\|u(\tau_{2j,d-1},\tau_{2i,d^{\prime}-1})+u(\tau_{2j,d-1 },\tau_{2i-1,d^{\prime}-1})\|\leq\frac{16DR_{\epsilon}}{N}\|P_{d,j}[\tilde{ \theta}_{N}^{(2)}]\|_{2}.\] (47)

The proof is then completed. 

Proof of Corollary 3.: This proof is in fact a direct combination of Theorems 1 and 3. By Theorem 1 we know there exists a weight \(\theta^{(2)*}\) that can reach zero loss for arbitrarily length \(N\). Then it holds that \(\|\theta_{\lambda,N}\|_{2}\leq\|\theta^{(2)*}\|\) as \(\theta_{\lambda,N}\) minimizes the regularized loss. Noticing that bounded weight implies bounded Lipschitzness of \(\mathrm{g}^{(2)}\), the rest follows as Theorem 3. 

### Proof of Theorem 2

We now show the limitation of interpretability from a single component, using a Lottery-Ticket-style argument by pruning from large random Transformers.

**Theorem 2** (Indistinguishability From a Single Component).: _Consider any \(L\)-layer Transformer \(\mathcal{T}\) (Equation (10)) with embedding dimension \(m\), attention dimension \(m_{a}\), and projection function \(\mathrm{g}\) as 2-layer ReLU MLP with width \(w\). For any \(\delta\in(0,1)\) and \(N\in\mathbb{N}^{+}\), consider a \(4L\)-layer random Transformer \(\mathcal{T}_{\text{large}}\) with embedding dimension \(m_{\mathrm{large}}=O(m\log(Lm/\delta))\), attention dimension \(m_{\mathrm{large},a}=O(m_{a}L\log\frac{m_{a}mLN}{\epsilon\delta})\), and projection function \(\mathrm{g}_{\mathrm{large}}\) as 4-layer ReLU MLP with width \(w_{\mathrm{large}}=O(\max\{m,w\}L\log\frac{wmLN}{\epsilon\delta})\)._

_Assume that \(\|W\|_{2}\leq 1\) for every weight matrix \(W\) in \(\mathcal{T}\), and suppose the weights are randomly sampled as \(W_{i,j}\sim U(-1,1)\) for every \(W\in\mathcal{T}_{\text{large}}\). Then, with probability \(1-\delta\) over the randomness of \(\mathcal{T}_{\text{large}}\), there exists a nonstructural pruning (Definition 4) of \(\mathcal{T}_{\text{large}}\), denoted as \(\tilde{\mathcal{T}}_{\text{large}}\), which \(\epsilon\)-approximates \(\mathcal{T}\) with respect to \(\|\cdot\|_{1,2}\) for any input \(X\in\mathbb{R}^{m\times N}\) satisfying \(\|\bm{X}\|_{1,2}\leq 1\). 19_

Footnote 19: Here the input and output dimension of \(\tilde{\mathcal{T}}_{\text{large}}\) is actually \(m_{\mathrm{large}}\) which is larger than \(m\); additional dimensions are padded with zeroes. The norm constraint can be easily extended to an arbitrary constant.

Proof.: We will first introduce some notation. For vector \(x\in\mathbb{R}^{a}\) and \(y\in\mathbb{R}^{b}\), we will use \(x\oplus y\) to denote their concatenation. We will use \(0^{a}\) to denote the all-zero vector with dimension \(a\). We will also assume without loss of generality that \(w\geq 2m\). 20

Footnote 20: We can always pad dimensions if \(w\) is too small.

We will use \(\tilde{\bm{X}}\) to denote \(\begin{bmatrix}\bm{X}\\ 0^{(m_{\mathrm{large}}-m^{\prime})\times N}\end{bmatrix}\) for \(\bm{X}\in\mathbb{R}^{m^{\prime}\times N}\) with \(m^{\prime}\leq m_{\mathrm{large}}\).

In the following, a _random network_ refers to a network whose weights have entries sampled from a uniform distribution, i.e. \(W_{i,j}\sim U(-1,1)\) for every weight \(W\) in the random network.

We will first recall Lemma 2 from Pensia et al. (2020) which shows that a pruned 2-layer random network can approximate a linear function.

**Lemma 2** (Approximating a linear function; Theorem 1 of Pensia et al. (2020) restated).: _Let \(W\in\mathbb{R}^{m^{\prime}\times m},\|W\|_{2}=O(1)\), then for \(\sigma\in\{\mathrm{ReLU},\mathcal{I}\}\), where \(\mathcal{I}\) represents the identity operator for a random network \(g(x)=W_{2}\sigma(W_{1}x)\) with \(W_{2}\in\mathbb{R}^{m^{\prime}\times h},W_{1}\in\mathbb{R}^{h\times m}\) for hidden dimension \(h=O(m\log(\frac{mm^{\prime}}{\min\{\epsilon,\delta\}}))\), with probability \(1-\delta\), there exists boolean masking matrices \(M_{1},M_{2}\), such that for any \(x\in\mathbb{R}^{w}\),_

\[\|(M_{2}\odot W_{2})\sigma\big{(}(M_{1}\odot W_{1})x\big{)}-Wx\|\leq\epsilon\| x\|_{2},\]

_where \(\odot\) denotes the Hadamard product._

We then derive two approximation results Lemmas 3 and 4 based on Lemma 2.

**Lemma 3**.: _Under the setting of Theorem 2, with probability \(1-2\delta/3\), for any \(l\in[L],l^{\prime}\in[4L-1]\), let \(\mathcal{T}^{(l)}\) be the \(l\)-th layer of \(\mathcal{T}\), there exists a pruning of the \((l^{\prime}-1)\)-th and the \((l^{\prime})\)-th layer \(\mathcal{T}^{(l^{\prime}-1)}_{\text{large}},\mathcal{T}^{l^{\prime}}_{\text{ large}}\), named \(\tilde{\mathcal{T}}^{(l^{\prime}-1)}_{\text{large}},\tilde{\mathcal{T}}^{l^{ \prime}}_{\text{large}}\) such that when defined on domain \(\|\bm{X}\|_{1,2}\leq 2L,\bm{X}\in\mathbb{R}^{m\times N}\),_

1. \(\tilde{\mathcal{T}}^{(l^{\prime}-1)}_{\text{large}}\) _is independent of the last_ \(m_{\mathrm{large}}-m\) _rows of the input._
2. \(\tilde{\mathcal{T}}^{l^{\prime}}_{\text{large}}\circ\tilde{\mathcal{T}}^{(l^{ \prime}-1)}_{\text{large}}\left(\bar{\bm{X}}\right)\) _is an_ \(\left(\frac{C}{1000L^{2}}\right)^{4L-3}\epsilon\)_-approximation of_ \(\overline{\mathcal{T}^{(l)}(\bm{X})}\) _with respect to_ \(1,2\)_-norm._

**Lemma 4**.: _Under the setting of Theorem 2, for any matrix \(W\in\mathbb{R}^{4m\times 4m},\|W\|_{2}\leq 1\), with probability \(1-\delta/4\), for any \(l^{\prime}\in[4L]\), there exists a pruning of the \(l\)-th layer \(\mathcal{T}^{(l^{\prime})}_{\text{large}}\), named \(\tilde{\mathcal{T}}^{(l^{\prime})}_{\text{large}}\) such that when defined on domain \(\bm{X}\in\mathbb{R}^{m\times N}\),_

1. \(\tilde{\mathcal{T}}^{(l^{\prime})}_{\text{large}}\) _is independent of the last_ \(m_{\mathrm{large}}-4m\) _rows of the input._
2. \(a(x)=\tilde{\mathcal{T}}^{(l^{\prime})}_{\text{large}}\left(\bar{\bm{X}}\right)\) _is an_ \(\left(\frac{C}{1000L^{2}}\right)^{4L}\epsilon\)_-approximation of_ \(\hat{g}(\bm{X})=\overline{W\bm{X}}\) _with respect to_ \(1,2\)_-norm._

The proof of Lemmas 3 and 4 is deferred to Appendix C.5.1 We can now prove the theorem.

We will first show with induction that if we 1) prune the \((2l-1)\)-th and \(2l\)-th layers of \(\mathcal{T}_{\text{large}}\) to approximate \(\mathcal{T}^{(l)}\) for each \(l\in[L]\), and 2) prune the \(2L+1\) to \(4L\)-th layers of \(\mathcal{T}_{\text{large}}\) to approximate identity, then the pruned large transformer will be an \(\epsilon\)-approximation of \(\mathcal{T}\) for any input \(\|\bm{X}\|_{1,2}\leq 1\).

We will perform induction on \(l\): Let \(\mathcal{T}^{(1:l)}\) define the composition of layer 1 to \(l\), i.e. \(\mathcal{T}^{(1:l)}(\bm{X}):=\mathcal{T}^{(l)}\circ\mathcal{T}^{(l-1)}\circ \cdots\circ\mathcal{T}^{(1)}(\bm{X})\), and define \(\epsilon_{l}:=\left(\frac{C}{1000L^{2}}\right)^{4L-3-l}\epsilon\). Suppose that \(\mathcal{T}^{(1:2l)}_{\text{large}}\) is an \(\epsilon_{l}\)-approximation of \(\mathcal{T}^{(1:l)}\). Note that \(\|\mathcal{T}^{(1:l)}(\bm{X})\|_{1,2}\leq(l+1),\) since each attention output has a bounded norm of 1 and every weight matrix in projection function \(\mathrm{g}\) has spectral norm smaller than \(1\), hence the norm will at most increment \(1\) (due to residual connection) after each layer. We have that

\[\left\|\tilde{\mathcal{T}}^{(1:2l)}_{\text{large}}\Big{(}\bar{\bm{X}}\Big{)} \right\|_{1,2}\leq 4l\leq 4L.\]

Then according to Lemma 13, \(\mathcal{T}^{(l+1)}\) is \((1+200L^{2}/C)\)-Lipschitz on the set of intermediate outputs \(\{\left(\tilde{\mathcal{T}}^{(1:2l)}_{\text{large}}(\bar{\bm{X}})\right)_{1:m} \mid\|\bm{X}\|_{1,2}\leq 1\}\). We also have that \(\mathcal{T}^{(1:l)}(\bm{X})\) is \((1+200L^{2}/C)^{l}\)-Lipschitz. Now we can apply Lemma 5 to show that \(\mathcal{T}^{(1:2l+2)}_{\text{large}}\) can \(\epsilon^{\prime}\)-approximate \(\mathcal{T}^{(1:l+1)}\) with

\[\epsilon^{\prime} =\epsilon_{l}(1+200L^{2}/C)+\epsilon\left(\frac{C}{1000L^{2}} \right)^{4L-3}(1+200L^{2}/C)^{l}+\epsilon_{l}\left(\frac{C}{1000L^{2}}\right)^{ 4L-3}\epsilon\] \[\leq\left(\frac{C}{1000L^{2}}\right)^{4L-4-l}\epsilon=\epsilon_{l +1}.\]

The induction is then completed and we have the composition of \(\tilde{\mathcal{T}}^{i}_{\text{large}}\) for \(i\in[2L]\)\(\epsilon_{L}\)-approximates the composition of \(\mathcal{T}\) with \(\epsilon_{L}=\left(\frac{C}{1000L^{2}}\right)^{3L-3}\epsilon\). We will then perform another induction showing that the composition of \(\tilde{\mathcal{T}}^{i}_{\text{large}}\) for \(i\in[2L+l]\)\(\epsilon_{l+L}\)-approximates \(\mathcal{T}\) with \(\epsilon_{l+L}=\left(\frac{C}{1000L^{2}}\right)^{3L-3-l}\epsilon\). Suppose the statement holds for \(L-1\geq l\geq 0\).

The induction step is similar, because we have \(\mathcal{T}\) is \((1+200L^{2}/C)^{L}\) Lipschitz, by Lemma 5, it holds that the composition of \(\mathcal{T}_{\text{large}}^{i}\) for \(i\in[2L+l+1]\)\(\epsilon^{\prime}\)-approximates \(\mathcal{T}\) with,

\[\epsilon^{\prime} =\epsilon_{l+L}+\epsilon\left(\frac{C}{1000L^{2}}\right)^{4L}(1+2 00L^{2}/C)^{L}+\epsilon_{l+L}\epsilon\left(\frac{C}{1000L^{2}}\right)^{4L}\] \[\leq\epsilon\left(\frac{C}{1000L^{2}}\right)^{3L-4-l}\epsilon= \epsilon_{L+l+1}.\]

This concludes the induction and prove the first claim of the theorem. For the second claim, notice that through similar induction steps, we can prune arbitrary layer of \(\mathcal{T}_{\text{large}}\) to approximate identity function and obtain the same approximation rate, this concludes the proof for the second claim. 

#### c.5.1 Helper lemmas for Theorem 2

Error AnalysisOur first lemma shows that the composition of \(\epsilon\)-approximation can approximate the composition of the original function.

**Lemma 4**.: _Under the setting of Theorem 2, for any matrix \(W\in\mathbb{R}^{4m\times 4m},\|W\|_{2}\leq 1\), with probability \(1-\delta/4\), for any \(l^{\prime}\in[4L]\), there exists a pruning of the \(l\)-th layer \(\mathcal{T}_{\text{large}}^{(l^{\prime})}\), named \(\tilde{\mathcal{T}}_{\text{large}}^{(l^{\prime})}\) such that when defined on domain \(\bm{X}\in\mathbb{R}^{m\times N}\),_

1. \(\tilde{\mathcal{T}}_{\text{large}}^{(l^{\prime})}\) _is independent of the last_ \(m_{\mathrm{large}}-4m\) _rows of the input._
2. \(a(x)=\tilde{\mathcal{T}}_{\text{large}}^{(l^{\prime})}\left(\bar{\bm{X}}\right)\) _is an_ \(\left(\frac{C}{1000L^{2}}\right)^{4L}\epsilon\)_-approximation of_ \(\hat{g}(\bm{X})=\overline{W\bm{X}}\) _with respect to_ \(1,2\)_-norm._

Proof.: One can prune the value matrix on layer \(l^{\prime}\) to zero and the rest is a direct consequence of Lemmas 2 and 2. 

**Lemma 5**.: _Given three metric spaces \(A,B,C\) equipped with same metric \(\|\cdot\|\). Suppose \(f_{1}:A\to B,f_{2}:B\to C\) are \(\epsilon_{1}\), \(\epsilon_{2}\)-approximations of \(g_{1},g_{2}\) with respect to \(\|\cdot\|\), where \(g_{1}\) is a Lipschitz function with constant \(\lambda_{1}\) with respect to \(\|\cdot\|\) and \(\|g_{2}(x)\|\leq\lambda_{2}x\), then it holds that, \(f_{1}\circ f_{2}\) is an \(\epsilon^{\prime}\)-approximation of \(g_{1}\circ g_{2}\), with \(\epsilon^{\prime}=(\lambda_{2}+\epsilon_{1})(\lambda_{1}+\epsilon_{2})- \lambda_{1}\lambda_{2}\)_

Proof.: For any \(x\in\mathbb{R}^{d_{1}}\), it holds that,

\[\|f_{1}(x)-g_{1}(x)\|\leq\epsilon_{1}\|x\|.\]

This then suggests that,

\[\|f_{2}(f_{1}(x))-g_{2}(g_{1}(x))\|\] \[\leq \|f_{2}(f_{1}(x))-g_{2}(f_{1}(x))\|+\|g_{2}(f_{1}(x))-g_{2}(g_{1} (x))\|\] \[\leq \epsilon_{2}\|f_{1}(x)\|+\lambda_{2}\|f_{1}(x)-g_{1}(x)\|\] \[\leq \epsilon_{2}\|g_{1}(x)\|+(\lambda_{2}+\epsilon_{2})\|f_{1}(x)-g_ {1}(x)\|\] \[\leq (\epsilon_{2}\lambda_{1}+\epsilon_{1}\lambda_{2}+\epsilon_{1} \epsilon_{2})\left\|x\right\|\]

Approximating ReLU MLPWe will first show an extension of Lemma 2, illustrating that a pruned wide 4-layer ReLU MLP can approximate any 2-layer ReLU MLP.

**Lemma 6**.: _Consider any 2-layer ReLU MLP \(g:\mathbb{R}^{4m}\rightarrow\mathbb{R}^{4m}\) parameterized by \(W_{1}\in\mathbb{R}^{4m\times w},W_{2}\in\mathbb{R}^{w\times 4m},\|W_{1}\|_{2},\|W_{2}\|_ {2}\leq 2\sqrt{2}\), for any \(\delta,\epsilon\in(0,1)\), consider a random 4-layer ReLU MLP \(f\) with input and output dimension \(4m\) and width \(w^{\prime}=O(w\log(\frac{wm}{\min\{\epsilon,\delta\}}))\) parameterized by \(W_{\mathrm{large},i}\), with probability \(1-\delta\) over the randomness of weight of \(f\), there exists a nonstructural pruning of \(f\) named \(\tilde{f}\), such that \(\tilde{f}\) is an \(\epsilon-\)approximation of \(f\) with respect to \(2-\)norm._Proof.: Choose \(\epsilon_{0}=\epsilon/8\). We only need to show there exists boolean matrices \(M_{1},M_{2},M_{3},M_{4}\), such that,

\[\Big{\|}\Big{(}M_{4}\odot W_{\mathrm{large},4}\mathrm{ReLU}\big{(} (M_{3}\odot W_{\mathrm{large},3})\mathrm{ReLU}\big{(}(M_{2}\odot W_{\mathrm{ large},2})\mathrm{ReLU}\big{(}(M_{1}\odot W_{\mathrm{large},1})x\big{)}\big{)} \big{)}\Big{)}\] \[-W_{2}\mathrm{ReLU}\left(W_{1}\bm{X}\right)\Big{\|}_{2}\leq\epsilon.\]

By Lemma 2, there exists boolean matrices \(M_{1}\in\mathbb{R}^{w^{\prime}\times 4m}\) and \(M_{2}^{\prime}\in\mathbb{R}^{w\times w^{\prime}}\), such that for any \(x\in\mathbb{R}^{4m}\),

\[\|\left(\begin{bmatrix}M_{2}^{\prime}\\ 0^{(w^{\prime}-w)\times w^{\prime}}\end{bmatrix}\odot W_{\mathrm{large},2} \right)\mathrm{ReLU}\big{(}(M_{1}\odot W_{\mathrm{large},1})x\big{)}-\begin{bmatrix} W_{1}x\\ w^{\prime}-w\end{bmatrix}\|_{2}\leq\epsilon_{0}\|x\|_{2}.\]

Hence we can choose \(M_{2}=\begin{bmatrix}M_{2}^{\prime}\\ 0^{(w^{\prime}-w)\times w^{\prime}}\end{bmatrix}\) and have \(f_{1}(x)=\mathrm{ReLU}\big{(}(M_{2}\odot W_{\mathrm{large},2})\mathrm{ReLU} \big{(}(M_{1}\odot W_{,1})x\big{)}\big{)}\) is \(\epsilon_{0}\)-approximation of \(g_{1}(x)=\begin{bmatrix}\mathrm{ReLU}(W_{1}x)\\ 0^{w^{\prime}-w}\end{bmatrix}\).

Again by Lemma 2, there exists boolean matrices \(M_{3}^{\prime}\in\mathbb{R}^{w^{\prime}\times w}\) and \(M_{4}\in\mathbb{R}^{4m\times w^{\prime}}\), such that for any \(y\in\mathbb{R}^{w}\),

\[\|\left(M_{4}\odot W_{\mathrm{large},4}\right)\mathrm{ReLU}\left(\left[M_{3}^ {\prime},0^{w^{\prime}\times(w^{\prime}-w)}\right]\begin{bmatrix}y\\ 0^{w^{\prime}-w}\end{bmatrix}\right)\leq\epsilon_{0}\|y\|_{2}\]

Hence we can choose \(M_{3}=\begin{bmatrix}M_{3}^{\prime},0^{w^{\prime}\times(w^{\prime}-w)}\end{bmatrix}\), and have \(f_{2}(x)=\mathrm{ReLU}\big{(}(M_{4}\odot W_{\mathrm{large},4})\mathrm{ReLU} \big{(}(M_{3}\odot W_{\mathrm{large},3})x\big{)}\big{)}\) is \(\epsilon_{0}\)-approximation of \(g_{2}(x)=W_{2}x\).

It is also easy to check \(g_{1}\) and \(g_{2}\) are both \(2\sqrt{2}\)-lipschitz and \(g_{1}(0)=0\). By Lemma 5, we conclude that \(\tilde{f}=f_{1}\odot f_{2}\) is \(\epsilon^{\prime}\)-approximation of \(g=g_{1}\odot g_{2}\), with \(\epsilon^{\prime}=4\sqrt{2}\epsilon_{0}+\epsilon_{0}^{2}\leq\epsilon\). 

This lemma then yields the following corollaries.

**Corollary 1**.: _Under the setting of Theorem 2, with probability \(1-\delta/4\), for any \(l\in[L],l^{\prime}\in[4L]\), there exists a pruning of the projection function \(\mathrm{g}_{\mathrm{large}}^{(l^{\prime})}\), named \(\mathrm{g}_{\mathrm{large}}^{(\tilde{l^{\prime}})}\), such that_

1. \(\mathrm{g}_{\mathrm{large}}^{(\tilde{l^{\prime}})}\) _is independent of the last_ \(m_{\mathrm{large}}-m\) _dimension of the input._
2. \(a(x)=\mathrm{g}_{\mathrm{large}}^{(\tilde{l^{\prime}})}\left(\begin{bmatrix} x\\ 0^{m_{\mathrm{large}}-m}\end{bmatrix}\right)\) _is an_ \(\left(\frac{C}{1000L^{2}}\right)^{4L}\epsilon\)_-approximation of_ \(\hat{g}(x)=\begin{bmatrix}\mathrm{g}^{(l)}(x)\\ 0^{m_{\mathrm{large}}-m}\end{bmatrix}\) _with respect to_ \(2-\)_norm._

Proof.: One can construct such pruning by pruning the last \(m_{\mathrm{large}}-m\) rows of the weight of the last layer and the last \(m_{\mathrm{large}}-m\) columns of the weight of the first layer of \(\mathrm{g}_{\mathrm{large}}^{(l^{\prime})}\) to zero and then apply Lemma 6. 

Approximating Attention PatternsWe will now show that the attention pattern can be approximated by pruning random Transformer layers.

**Lemma 7**.: _For any \(\delta,\epsilon\in(0,1)\), for any \(W\in\mathbb{R}^{m},\|W\|_{2}\leq 1\), for two random matrix \(W_{1},W_{2}\in\mathbb{R}^{m^{\prime}\times m}\) where \(m^{\prime}=O(m\log(\frac{m}{\min(\epsilon,\delta)}))\), suppose \(\bm{X}\in\mathbb{R}^{m\times N}\), then there exists nonstructural pruning of \(W_{1},W_{2}\), named \(\tilde{W}_{1},\tilde{W}_{2}\), such that_

\[\|\bm{X}^{\top}\tilde{W_{1}}^{\top}\tilde{W_{2}}\bm{X}-\bm{X}^{\top}W\bm{X}\|_{ \infty}\leq\epsilon\|\bm{X}\|_{1,2}^{2}\]

_Here we adopt \(\|\|_{\infty}\) in vector sense, meaning the entry with largest absolute value._

Proof.: Suppose without loss of generality, \(\|\bm{X}\|_{:,i}\leq 1\). According to Lemma 2, there exists nonstructural pruning of \(W_{1},W_{2}\), named \(\tilde{W_{1}},\tilde{W_{2}}\), such that for any \(x\in\mathbb{R}^{m},\|x\|_{2}\leq 1\),

\[\|\tilde{W_{1}}^{\top}\tilde{W_{2}}x-Wx\|_{2}\leq\epsilon.\]This then suggests that,

\[\|y^{\top}(\tilde{W_{1}}^{\top}\tilde{W_{2}}x-Wx)\|_{2}\leq\epsilon\|y\|_{2}\leq\epsilon.\]

This concludes the proof. 

The next lemma shows how error propogates through the softmax operators.

**Lemma 8**.: _For any dimension \(d\), suppose \(x,y\in\mathbb{R}^{d}\) satisfies \(\|x-y\|_{\infty}\leq\epsilon\), then it holds that,_

\[\sum_{i=1}^{d}\big{|}\frac{\exp(x_{i})}{\sum_{i=1}^{n}\exp(x_{i})}-\frac{\exp(y _{i})}{\sum_{i=1}^{n}\exp(y_{i})}\big{|}\leq\exp(2\epsilon)-1.\]

Proof.: One can observe that,

\[\exp(-\epsilon)\exp(x_{i})\leq\exp(y_{i})\leq\exp(\epsilon)\exp(x_{i})\]

This then suggests,

\[\frac{\exp(x_{i})}{\sum_{i=1}^{n}\exp(x_{i})}\exp(-2\epsilon)\leq\frac{\exp(y _{i})}{\sum_{i=1}^{n}\exp(y_{i})}\leq\exp(2\epsilon)\frac{\exp(x_{i})}{\sum_{ i=1}^{n}\exp(x_{i})}.\]

Hence,

\[\sum_{i=1}^{d}\big{|}\frac{\exp(x_{i})}{\sum_{i=1}^{n}\exp(x_{i})}-\frac{\exp (y_{i})}{\sum_{i=1}^{n}\exp(y_{i})}\big{|}\leq\max\{\exp(2\epsilon)-1,1-\exp(- 2\epsilon)\}=\exp(2\epsilon)-1.\]

This concludes the proof. 

Approximating Attention ModuleWe will need the following lemma showing there exists a pruning of the value matrix in \(\mathcal{T}_{\text{large}}\) such that it has eigenvalues with magnitude \(\Theta(1)\).

**Lemma 9**.: _For a matrix \(W\in\mathbb{R}^{m_{\text{large}}\times m_{\text{large}}}\), with probability at least \(1-\frac{\delta}{10L}\), there exists a pruning of \(W\), named \(W^{\prime}\), such that all the nonzero entries is contained in a \(d\times d\) submatrix of \(W^{\prime}\) that satisfies that (1) all its eigenvalues are within \((\frac{1}{2},1)\), (2) the index of row specifying the submatrix and the index of column specifying the submatrix are disjoint._

Proof.: As \(w_{\text{large}}=\Omega(m\log(\frac{dL}{\delta})),\) hence we can split \(W_{1:\lceil m_{\text{large}}/2\rceil,\lceil m_{\text{large}}/2\rceil+1:m_{ \text{large}}}\) into \((m\times(m\) blocks, each with width at least \(O(\log(\frac{(m}{\delta}))\)21. Within each block, with probability \(1-\frac{\delta}{10Lm_{\text{large}}}\), there exists at least one entry that has value at least \(\frac{1}{2}\). We can then choose \(d\) disjoint entries in \(W\) that are all at least \(\frac{1}{2}\), indexed with \(\{(a_{i},b_{i})\}_{i\in[d]}\) where \(a_{i}<a_{j}\) and \(b_{i}<b_{j}\) for \(i<j\). We can then prune all other entries to zero. Consider the submatrix defined by entries \((a,b)\) for \(a\in\{a_{i}\}_{i\in m}\) and \(b\in\{b_{i}\}_{i\in m}\). Then, this submatrix will be diagonal and contains eigenvalues within \((\frac{1}{2},1)\). Further \(\{a_{i}\}_{i\in m}\) and \(\{b_{i}\}_{i\in m}\) must be disjoint because \(a_{i}\leq\lceil m_{\text{large}}/2\rceil<b_{i}\). The proof is then completed. 

Footnote 21: \(O(\cdot)\) hides absolute constants arising from the change of basis in the logarithm.

We will also prove that LayerNorm with nonzero normalization constant is Lipschitz.

**Lemma 10**.: _For LayerNorm function defined as \(\operatorname{LN}(x)=\frac{\mathcal{P}_{\perp}x}{\max\{\|\mathcal{P}_{\perp}x \|_{2},C\}},x\in\mathbb{R}^{m}\), for any \(x,y\in\mathbb{R}^{m}\), it holds that,_

\[\Big{\|}\operatorname{LN}(x)-\operatorname{LN}(y)\Big{\|}_{2}\leq 2\|x-y\|_{2} /C.\]

Proof.: We will proceed by a case analysis:

1. If \(\|\mathcal{P}_{\perp}x\|_{2},\|\mathcal{P}_{\perp}y\|_{2}\leq C\), then \(\Big{\|}\operatorname{LN}(x)-\operatorname{LN}(y)\Big{\|}_{2}=\frac{\| \mathcal{P}_{\perp}x-\mathcal{P}_{\perp}y\|_{2}}{C}\leq\frac{1}{C}\|x-y\|_{2}\).
2. If \(\|\mathcal{P}_{\perp}x\|_{2},\|\mathcal{P}_{\perp}y\|_{2}>C\), then \(\Big{\|}\operatorname{LN}(x)-\operatorname{LN}(y)\Big{\|}_{2}=\frac{\| \mathcal{P}_{\perp}x-\mathcal{P}_{\perp}y\|_{2}}{\|\mathcal{P}_{\perp}y\|_{2 }}+\big{|}1-\frac{\|\mathcal{P}_{\perp}x\|_{2}}{\|\mathcal{P}_{\perp}y\|_{2} }\big{|}\leq\frac{2}{C}\|x-y\|_{2}\).

3. If \(\|\mathcal{P}_{\perp}x\|_{2}<C\) and \(\|\mathcal{P}_{\perp}y\|_{2}>C\), then \(\left\|\mathrm{LN}(x)-\mathrm{LN}(y)\right\|_{2}=\frac{\|\mathcal{P}_{\perp}x- \mathcal{P}_{\perp}y\|_{2}}{\|\mathcal{P}_{\perp}y\|_{2}}+\left|\frac{\| \mathcal{P}_{\perp}x\|_{2}}{C}-\frac{\|\mathcal{P}_{\perp}x\|_{2}}{\|\mathcal{ P}_{\perp}y\|_{2}}\right|\leq\frac{2}{C}\|x-y\|_{2}.\)

The cases exhaust all possibilities, thus the proof is completed. 

Finally, we will need a lemma showing how error accumulates when we consider both attention patterns and the value matrices.

**Lemma 11**.: _For any dimension \(d\) and positive number \(N\), for \(P,Q\in\mathbb{R}^{d\times d}\) satisfying that \(\|P\|_{2},\|Q\|_{2}\leq 1\), for any \(x\in\mathbb{R}^{d\times N}\), if matrix \(A\in\mathbb{R}^{N\times N},B\in\mathbb{R}^{d\times N}\) satisfy that,_

\[\|A-\sigma(x^{\top}Qx)\|_{1,1} \leq\epsilon_{1}.\] \[\|B-Px\|_{1,2} \leq\epsilon_{2}.\] \[\forall i,k\in[N]\sum_{j\in[N]}A_{j,i} =1,A_{k,i}\geq 0.\]

_Then it holds that,_

\[\|BA-Px\sigma(x^{\top}Qx)\|_{1,2}\leq(\epsilon_{1}\|PX\|_{1,2}+ \epsilon_{2}).\] \[\|\mathrm{LN}_{C}(BA)-\mathrm{LN}_{C}(Px\sigma(x^{\top}Qx))\|_{1,2}\leq 2(\epsilon_{1}\|PX\|_{1,2}+\epsilon_{2})/C.\]

Proof.: For any \(i\in N\), we will have

\[\left\|(BA)_{:,i}-\left(Px\sigma(x^{\top}Qx)\right)_{:,i}\right\| _{2}\] \[= \Big{\|}\sum_{j\in[N]}A_{j,i}B_{:,j}-\left(\sigma(x^{\top}Qx) \right)_{j,i}(PX)_{:,j}\Big{\|}_{2}\] \[\leq \|\sum_{j\in[N]}A_{j,i}(PX)_{:,j}-\left(\sigma(x^{\top}Qx) \right)_{j,i}(PX)_{:,j}\Big{\|}_{2}+\|\sum_{j\in[N]}A_{j,i}\left(PX-B\right)_ {:,j}\|_{2}\] \[\leq \|PX\|_{1,2}\sum_{j\in[N]}|A_{j,i}-\left(\sigma(x^{\top}Qx) \right)_{j,i}+\|PX-B\|_{1,2}\] \[\leq \|PX\|_{1,2}\|A-\sigma(x^{\top}Qx)\|_{1,1}+\|PX-B\|_{1,2}\leq \epsilon_{1}\|PX\|_{1,2}+\epsilon_{2}.\]

The rest follows from Lemma 10

A LayerNorm of larger dimension can be made to be functionally equivalent to a LayerNorm of a smaller dimension. Precisely:

**Lemma 12**.: _Given any dimension \(d<d^{\prime}\), it holds that for any \(x\in\mathbb{R}^{d}\),_

\[\mathrm{LN}_{C}(\begin{bmatrix}\mathcal{P}_{\perp}x\\ 0^{d^{\prime}-d}\end{bmatrix})=\begin{bmatrix}\mathrm{LN}_{C}(x)\\ 0\end{bmatrix}.\]

Proof.: The proof follows directly from definition. 

We will now formally define attention module.

**Definition 7** (Attention Module).: _We will define attention module \(a(\bm{X}\mid W_{V},W_{K},W_{Q})\) as_

\[a(\bm{X})=\mathrm{LN}_{C}\left(W_{V}\bm{X}\sigma(\bm{X}^{\top}W_{K}^{\top}W_{Q} \bm{X})\right).\]

**Lemma 13**.: _Attention module is lipschitz with respect to \(1,2\)-norm for bounded input. Precisely, consider attention module (Definition 7)parameterized by \(\|W_{V}\|_{2},\|W_{K}\|_{2},\|W_{Q}\|_{2}\leq 1\) with input domain \(\|\bm{X}\|_{1,2}\leq 4L\), \(a(\bm{X})\) is \(200L^{2}/C\)-lipschitz with respect to \(1,2-\)norm._Proof.: We have that

\[a(\bm{X})=\operatorname{LN}_{C}\left(W_{V}\bm{X}\sigma(\bm{X}^{\top}W_{K}^{\top}W_ {Q}\bm{X})\right).\]

Choose \(\epsilon\) to be a sufficiently small constant, such that, \(\exp(32L\epsilon)-1\leq 64L\epsilon\). Consider \(\bm{X}\) and \(\tilde{\bm{X}}\) satisfying that \(\|\bm{X}-\tilde{\bm{X}}\|_{1,2}\leq\epsilon\) and \(\|\bm{X}\|_{1,2}\leq 4L\), \(\|\tilde{\bm{X}}\|_{1,2}\leq 4L\), we will have

\[\Big{|}\left(\bm{X}^{\top}W_{K}^{\top}W_{Q}\bm{X}-(\tilde{\bm{X}} )^{\top}W_{K}^{\top}W_{Q}(\tilde{\bm{X}})\right)_{i,j}\Big{|}\] \[= \Big{|}(\bm{X}_{\cdot:i}-\tilde{\bm{X}}_{\cdot:i})^{\top}W_{K}^{ \top}W_{Q}\bm{X}_{\cdot:j}+(\tilde{\bm{X}}_{\cdot:i})^{\top}W_{K}^{\top}W_{Q}( \bm{X}_{\cdot:j}-\tilde{\bm{X}}_{\cdot:j})+(\bm{X}_{\cdot:i}-\tilde{\bm{X}}_{ \cdot:i})^{\top}W_{K}^{\top}W_{Q}(\bm{X}_{\cdot:j}-\tilde{\bm{X}}_{\cdot:j})\] \[\leq 8L\epsilon+\epsilon^{2}\leq 16L\epsilon.\]

By Lemma 8, this implies,

\[\|\sigma(\bm{X}^{\top}W_{K}^{\top}W_{Q}\bm{X})-\sigma((\tilde{\bm{X}})^{\top}W _{K}^{\top}W_{Q}(\tilde{\bm{X}}))\|_{1,1}\leq\exp(32L\epsilon)-1\leq 64L\epsilon.\]

We also have

\[\|W_{V}\left(\bm{X}-\tilde{\bm{X}}\right)\|_{1,2}\leq\epsilon.\] \[\|W_{V}\bm{X}\|_{1,2}\leq 4L\]

Lemma 11 then implies that

\[\|a(\bm{X})-a(\tilde{\bm{X}})\|_{1,2}\leq 200L^{2}\epsilon/C.\]

This then concludes the proof. 

We can now prove that a large Transformer Layer and an attention module of the larger Transformer can be pruned to approximate the attention module of a smaller Transformer Layer module.

**Lemma 14**.: _Under the setting of Theorem 2, with probability \(1-\delta/2\), for any \(l\in[L],l^{\prime}\in[4L-1]\), let \(a^{(l)}\) be the attention module on the \(l\)-th layer of \(\mathcal{T}\), there exists a pruning of the \((l^{\prime}-1)\)-th layer \(\mathcal{T}^{(l^{\prime}-1)}_{\text{large}}\), named \(\tilde{\mathcal{T}}^{(l^{\prime}-1)}_{\text{large}}\) and the attention module on \(l^{\prime}\)-th layer \(a^{l^{\prime}}_{\text{large}}\) named \(a^{l^{\prime^{\prime^{\prime}}}}_{\text{large}}\), such that when defined on domain \(\|\bm{X}\|_{1,2}\leq 2L\),_

1. \(\tilde{\mathcal{T}}^{(l^{\prime}-1)}_{\text{large}}\) _is independent of the last_ \(m_{\text{large}}-m\) _rows of the input._
2. \(\left(a^{l^{\prime^{\prime^{\prime}}}}_{\text{large}}\circ\tilde{\mathcal{T}} ^{(l^{\prime}-1)}_{\text{large}}\left(\begin{bmatrix}x\\ 0^{(m_{\text{large}}-m)\times N}\end{bmatrix}\right)\right)_{1:m}\) _is an_ \(\left(\frac{C}{1000L^{2}}\right)^{4L-1}\epsilon\)_-approximation of_ \(a^{(l)}(x)\) _with respect to_ \(1,2\)_-norm._
3. \(\left(\tilde{\mathcal{T}}^{(l^{\prime}-1)}_{\text{large}}\left(\begin{bmatrix} x\\ 0^{(m_{\text{large}}-m)\times N}\end{bmatrix}\right)\right)_{1:m}\) _is an_ \(\left(\frac{C}{1000L^{2}}\right)^{4L}\epsilon\)_-approximation of_ \(\bm{X}\) _with respect to_ \(1,2\)_-norm._

Proof.: We will use the shorthand \(\epsilon_{0}=\left(\frac{C}{1000L^{2}}\right)^{4L}\epsilon\) and prune in the following order. It holds that for \(\epsilon\leq 1\), \(\exp(8L^{2}\epsilon_{0})-1\leq 16L^{2}\epsilon_{0}\).

1. We will prune \(W_{V}^{\text{large},(l^{\prime})}\) according to Lemma 9 and name the pruned matrix \(W_{V}^{\text{large},(l^{\prime})}\). By Lemma 9, all the nonzero entries is contained in a \(d\times d\) submatrix of \(W^{\prime}\) that satisfies that all its eigenvalues are within \((\frac{1}{2},1)\). We will assume WLOG the submatrix is the one specified by row \(1\ldots d\) and column \(d+1\ldots 2d\) and name the submatrix as \(W\).
2. We will then prune \(\mathcal{T}^{(l^{\prime}-1)}_{\text{large}}\) according to Lemma 4 to output \(\epsilon_{0}\)-approximation of \(\bm{X}\in\mathbb{R}^{m\times N}\to\begin{bmatrix}\bm{X}\\ 0^{(m_{\text{large}}-2m)\times N}\end{bmatrix}\). As \(W\) is defined as the submatrix pruned by \(W_{V}^{(t+1)}\), it holds that \(W_{V}^{\text{large},(l^{\prime})}\begin{bmatrix}\bm{X}\\ 0^{(m_{\text{large}}-m)\times N}\end{bmatrix}=\begin{bmatrix}\bm{\mathcal{P}}_{ \perp}W_{V}^{(l)}\bm{X}\\ 0^{(m_{\text{large}}-m)\times N}\end{bmatrix}\).

3. Finally we will prune \(W_{K}^{\operatorname{large},(l^{\prime})},W_{Q}^{\operatorname{large},(l^{\prime})}\) according to Lemma 7 to approximate \((W_{K}^{(l)})^{\top}W_{Q}^{(l)}\) up to \(\epsilon_{0}\) error.

we can now calculate the approximation error. For any \(\bm{X}\in\mathbb{R}^{m\times N},\|\bm{X}\|_{1,2}\leq 2L\), suppose

\[\mathcal{T}^{(\vec{l^{\prime}}-1)}(\bm{X})=\begin{bmatrix}\bm{X}+\delta_{1}\\ W^{-1}\mathcal{P}_{\perp}W_{V}^{(l)}\bm{X}+\delta_{2}\\ 0^{(m_{\operatorname{large}}-2m)\times N}\end{bmatrix}\]

Then by our constrution, it holds that \(\forall i\in\{1,2\},\|\delta_{i}\|_{1,2}\leq\epsilon_{0}\|\bm{X}\|_{1,2}\).

We would then have

\[\tilde{W_{V}^{\operatorname{large},(l^{\prime})}}\mathcal{T}^{(\vec{l^{ \prime}}-1)}(\bm{X})=\begin{bmatrix}\mathcal{P}_{\perp}W_{V}^{(l)}\bm{X}+W_{V }^{\operatorname{large},(l^{\prime})}\delta_{2}\\ 0^{(m_{\operatorname{large}}-m)\times N}\end{bmatrix}\] (48)

By our construction, it holds that \(\|W_{V}^{\operatorname{large},(l^{\prime})}\delta_{2}\|_{1,2}\leq 2\|\delta_{2}\|_{ 1,2}\leq 2\epsilon_{0}\|\bm{X}\|_{1,2}\).

Further, by the construction of \(W_{K}^{\operatorname{large},(l^{\prime})},W_{Q}^{\operatorname{large},(l^{ \prime})}\), it holds that,

\[\Big{\|}\left(W_{K}^{\operatorname{large},(l^{\prime})}\mathcal{ T}^{(\vec{l^{\prime}}-1)}(\bm{X})\right)^{\top}\left(W_{Q}^{\operatorname{large},(l^{\prime})}\mathcal{T}^{(\vec{l^{\prime}}-1)}(\bm{X})\right)\] \[-(W_{K}^{(l)}\bm{X}+W_{K}^{(l)}\delta_{1})^{\top}(W_{Q}^{(l)}\bm{ X}+W_{Q}^{(l)}\delta_{1})\Big{\|}_{\infty}\leq\epsilon_{0}\] (49)

As for any \(i,j\in[N]\)

\[\left|\left((W_{K}^{(l)}\bm{X}+W_{K}^{(l)}\delta_{1})^{\top}(W_{Q }^{(l)}\bm{X}+W_{Q}^{(l)}\delta_{1})-(W_{K}^{(l)}\bm{X})^{\top}W_{Q}^{(l)}\bm{ X}\right)_{i,j}\right|\] \[\leq \left|(W_{K}^{(l)}\bm{X}_{:,i})^{\top}(W_{Q}^{(l)}\delta_{1})_{:, j}\right|+\left|(W_{K}^{(l)}\delta_{1})_{:,i}^{\top}(W_{Q}^{(l)}\bm{X})_{:,j} \right|+\left|(W_{K}^{(l)}\delta_{1})_{:,i}^{\top}(W_{Q}^{(l)}\delta_{1})_{:, j}\right|\] \[\leq \|\bm{X}\|_{1,2}^{2}(2\epsilon_{0}+\epsilon^{2})\leq 4\|\bm{X}\|_{ 1,2}^{2}\epsilon_{0}.\]

combined with Equation (49),

\[\Big{\|}\left(W_{K}^{\operatorname{large},(l^{\prime})}\mathcal{T}^{(\vec{l^ {\prime}}-1)}(\bm{X})\right)^{\top}\left(W_{Q}^{\operatorname{large},(l^{ \prime})}\mathcal{T}^{(\vec{l^{\prime}}-1)}(\bm{X})\right)-(W_{K}^{(l)}\bm{X}) ^{\top}W_{Q}^{(l)}\bm{X}\Big{\|}_{\infty}\leq\epsilon_{0}(1+4\|\bm{X}\|_{1,2}^ {2}).\] (50)

By Lemma 8, this implies

\[\Big{\|}\sigma\left(\left(W_{K}^{\operatorname{large},(l^{\prime })}\mathcal{T}^{(\vec{l^{\prime}}-1)}(\bm{X})\right)^{\top}\left(W_{Q}^{ \operatorname{large},(l^{\prime})}\mathcal{T}^{(\vec{l^{\prime}}-1)}(\bm{X}) \right)\right)\] \[-\sigma\left((W_{K}^{(l)}\bm{X})^{\top}W_{Q}^{(l)}\bm{X}\right) \Big{\|}_{1,1}\leq 4\epsilon_{0}(1+4\|\bm{X}\|_{1,2}^{2}).\] (51)

By Lemma 11, Equations (48) and (51) imply,

\[\Big{\|}W_{V}^{\operatorname{large},(l^{\prime})}\mathcal{T}^{( \vec{l^{\prime}}-1)}(\bm{X})\sigma\left(\left(W_{K}^{\operatorname{large},(l^{ \prime})}\mathcal{T}^{(\vec{l^{\prime}}-1)}(\bm{X})\right)^{\top}\left(W_{Q}^{ \operatorname{large},(l^{\prime})}\mathcal{T}^{(\vec{l^{\prime}}-1)}(\bm{X}) \right)\right)\] \[-\begin{bmatrix}\mathcal{P}_{\perp}W_{V}^{(l)}\bm{X}\sigma\left(( W_{K}^{(l)}\bm{X})^{\top}W_{Q}^{(l)}\bm{X}\right)\\ 0^{(m_{\operatorname{large}}-m)\times N}\end{bmatrix}\Big{\|}_{1,2}\leq 8 \epsilon_{0}(1+4\|\bm{X}\|_{1,2}^{2})\|\bm{X}\|_{1,2}\leq 80L^{2}\epsilon_{0}.\]

Now according to Lemmas 10 and 12, it holds that

\[\|a_{\operatorname{large}}^{l^{\prime^{\prime}}}\circ\tilde{\mathcal{T}}_{ \operatorname{large}}^{(l^{\prime}-1)}\left(\begin{bmatrix}x\\ 0^{(m_{\operatorname{large}}-m)\times N}\end{bmatrix}\right)_{1:m}-a^{(l)}(x)\|_ {1,2}\leq 160L^{2}\epsilon_{0}/C.\]

This concludes the proof.

Approximating Transformer LayersWe will finally show that two random Transformer layers can be pruned to approximate a given Transformer layer.

**Lemma 3**.: _Under the setting of Theorem 2, with probability \(1-2\delta/3\), for any \(l\in[L],l^{\prime}\in[4L-1]\), let \(\mathcal{T}^{(l)}\) be the \(l\)-th layer of \(\mathcal{T}\), there exists a pruning of the \((l^{\prime}-1)\)-th and the \((l^{\prime})\)-th layer \(\mathcal{T}^{(l^{\prime}-1)}_{\text{large}},\mathcal{T}^{l^{\prime}}_{\text{ large}}\), named \(\tilde{\mathcal{T}}^{(l^{\prime}-1)}_{\text{large}},\tilde{\mathcal{T}}^{l^{ \prime}}_{\text{large}}\) such that when defined on domain \(\|\bm{X}\|_{1,2}\leq 2L,\bm{X}\in\mathbb{R}^{m\times N}\),_

1. \(\tilde{\mathcal{T}}^{(l^{\prime}-1)}_{\text{large}}\) _is independent of the last_ \(m_{\mathrm{large}}-m\) _rows of the input._
2. \(\tilde{\mathcal{T}}^{l^{\prime}}_{\text{large}}\circ\tilde{\mathcal{T}}^{(l^{ \prime}-1)}_{\text{large}}\left(\bar{\bm{X}}\right)\) _is an_ \(\left(\frac{C}{1000L^{2}}\right)^{4L-3}\epsilon\)_-approximation of_ \(\overline{\mathcal{T}^{(l)}(\bm{X})}\) _with respect to_ \(1,2\)_-norm._

Proof.: We will prune the \((l^{\prime}-1)\)-th layer and the attention module of the \(l^{\prime}\)-th layer according to Lemma 14 to approximate \(a^{(l)}\) and the projection function of the \(l^{\prime}\)-th layer according to Corollary 1. Notice that \(\left\|a^{(l)}(\bm{X})+\bm{X}\right\|_{1,2}\leq(\frac{2}{C}+1)\|\bm{X}\|_{1,2}\) and \(\mathrm{g}^{(l)}\) is \(1-\)ipschitz, according to Lemma 5,

\[\left(\tilde{\mathcal{T}}^{l^{\prime}}_{\text{large}}\odot\tilde{ \mathcal{T}}^{(l^{\prime}-1)}_{\text{large}}\left(\begin{bmatrix}x\\ 0^{(m_{\mathrm{large}}-m)\times N}\end{bmatrix}\right)\right)_{1:m}\text{ is an $\epsilon^{\prime}$-approximation of $\mathcal{T}^{(l)}(x)$, with}\] \[\epsilon^{\prime}\leq(\frac{2}{C}+1)\left(\frac{C}{1000L^{2}} \right)^{4L}\epsilon+\left(\frac{C}{1000L^{2}}\right)^{4L-2}\epsilon+\left( \frac{C}{1000L^{2}}\right)^{8L-2}\epsilon^{2}\leq\left(\frac{C}{1000L^{2}} \right)^{4L-3}\epsilon.\]

This concludes the proof. 

### Technical Lemmas

**Lemma 15**.: _Given any dimension \(d\) and number of samples \(n\), for any size-\(n\) dataset \(\{(\bm{x}_{i},y_{i})\}_{i\in[n]}\) with \(\bm{x}_{i}\in\mathbb{R}^{d}\) and \(y_{i}\in\mathbb{R}\), there exists a width-\(2n\) two-layer MLP \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) with ReLU activation such that, \(f(\bm{x}_{i})=y_{i}\) for any \(i\in[n]\)._

Proof.: We will first choose direction \(w\in\mathbb{R}^{d},\|w\|_{2}=1\) and margin \(\gamma>0\) such that for any \(i\neq j\) in \([n]\), it holds that,

\[\left|\left\langle w,\bm{x}_{i}-\bm{x}_{j}\right\rangle\right|\geq 2\gamma.\]

We will assume WLOG \(w^{\top}\bm{x}_{i}\) is increasing in \(i\).

Then we will construct an auxilliary series \(z_{i}\) for \(i\in[n]\) such that,

\[z_{1} =y_{1}/\gamma\] \[z_{i} =y_{i}/\gamma-2\sum_{j=1}^{i-1}z_{j},i\in\{2,\ldots n\}.\]

Finally consider the following two-layer MLP with ReLU activation,

\[f(\bm{x})=\sum_{i=1}^{n}z_{i}\mathrm{ReLU}\left(\left\langle w,\bm{x}-\bm{x}_ {i}\right\rangle+\gamma\right)-z_{i}\mathrm{ReLU}\left(\left\langle w,\bm{x}- \bm{x}_{i}\right\rangle-\gamma\right),\]

we will show that \(f(\bm{x}_{i})=y_{i}\) for any \(i\in[n]\). Notice that

\[z_{j}\mathrm{ReLU}\left(\left\langle w,\bm{x}_{i}-\bm{x}_{j}\right\rangle+ \gamma\right)-z_{j}\mathrm{ReLU}\left(\left\langle w,\bm{x}_{i}-\bm{x}_{j} \right\rangle-\gamma\right)=\begin{cases}0,&j>i,\\ \gamma z_{i},&j=i,\\ 2\gamma z_{j},&j<i.\end{cases}\]

Thus it holds,

\[f(\bm{x}_{i}) =\sum_{j=1}^{n}z_{j}\mathrm{ReLU}\left(\left\langle w,\bm{x}_{i} -\bm{x}_{j}\right\rangle+\gamma\right)-z_{j}\mathrm{ReLU}\left(\left\langle w, \bm{x}_{i}-\bm{x}_{j}\right\rangle-\gamma\right)\] \[=\sum_{j=1}^{i-1}2\gamma z_{j}+\gamma z_{i}=y_{i}.\]

**Lemma 16**.: _Given any sets \(\{x_{i}\}_{i\in m}\) satisfying that \(x_{i}\in\mathbb{R}^{n}\) and \(x_{i}\neq 0\), there exists a set of orthonormal vectors \(\{u_{j}\}_{j\in[n-2]}\) of \(\mathbb{R}^{n}\) such that (1) \(u_{j}^{\top}1^{n}=0\) for any \(j\in[n-2]\) and (2) \(\sum_{j\in[n-2]}u_{j}^{\top}x_{i}u_{j}\neq x_{i}\) for any \(i\in[m]\)._

Proof.: There exists a vector \(v\in\mathbb{R}^{n}\) such that \(v^{\top}x_{i}\neq 0\) for any \(i\in[m]\). We can then construct an orthonormal basis \(\{u_{j}\}_{j\in[n-2]}\) of \(\mathbb{R}^{n}\) as the basis of the normal space of \(\text{span}(v,1^{n})\). Then the lemma holds. 

**Lemma 17**.: _Given any dimension \(n\) and constant \(M\), there exists a 2-layer width-\(2n\) ReLU network \(f:\mathbb{R}^{n+1}\to\mathbb{R}\) such that for any \(x\in[0,M]^{n},y\in[n]\), \(f(x\oplus y)=x_{y}\)._

Proof.: The construction is as followed, we will choose \(f\) as

\[f(x\oplus y)=\sum_{i=1}^{n}\mathrm{ReLU}(x_{i}+M(y-i))-\sum_{i=1}^{n}\mathrm{ ReLU}(x_{i}+M(y-i-1))-M(y-1).\]

Then as we have

\[\mathrm{ReLU}(x_{i}+M(y-i))-\sum_{i=1}^{n}\mathrm{ReLU}(x_{i}+M(y-i-1))=\begin{cases} M,&i\leq y-1;\\ x_{i},&i=y;\\ 0,&i\geq y+1.\end{cases}\]

The proof is completed. 

**Lemma 18**.: _Given any dimension \(n\) and constant \(M>0\), there exists a 2-layer width-\(2n\) ReLU network \(f:\mathbb{R}^{n}\to\mathbb{R}\) such that for any \(x\in\mathbb{R}^{n}\) satisfying there exists \(i\in[n]\), \(x_{i}>M\) and \(\forall j\neq i,x_{j}=0\), it holds that \(f(x)=i\)._

Proof.: The construction is as followed, we will choose \(f\) as

\[f(x)=\sum_{i=1}^{m}i\left(\mathrm{ReLU}(x_{i})-\mathrm{ReLU}(x_{i}-M)+M \right)/M.\]

The proof is completed. 

**Lemma 19**.: _Given any dimension \(n\) and natural numbers \(K,m,M\), if there exists \(K\) different 2-layer width-\(m\) ReLU networks \(f_{k}:\mathbb{R}^{n}\to\mathbb{R}\), then there exists a 2-layer width-\(2Km\) ReLU network \(f:\mathbb{R}^{n+1}\to\mathbb{R}\), such that \(f(\begin{bmatrix}k\\ x\end{bmatrix})=f_{k}(x)\) when \(x\in[0,M]^{n}\)._

Proof.: Suppose that

\[f_{k}(x)=\sum_{i=1}^{m}a_{k,i}\mathrm{ReLU}(w_{k,i}^{\top}x+b_{k,i})+b_{k}.\]

Then we can construct

\[f(\begin{bmatrix}y\\ x\end{bmatrix})= \sum_{k=1}^{K}\sum_{i=1}^{m}a_{k,i}\mathrm{ReLU}(w_{k,i}^{\top}x +b_{k,i}+M(y-k))-a_{k,i}\mathrm{ReLU}(w_{k,i}^{\top}x+b_{k,i}+M(y-k-1))\] \[+b_{k}-c_{k,i}\mathrm{ReLU}(y+1-k),\]

where \(c_{k,i}\) satisfies

\[\forall i,k^{\prime},\sum_{k=1}^{k^{\prime}}c_{k,i}(k^{\prime}+1-k)=M\sum_{k=1 }^{k^{\prime}-1}a_{k,i}.\]

The proof is then completed. 

**Lemma 20**.: _Given any dimension \(n\) and \(W\in\mathbb{R}^{n\times n},\|W\|_{2}\leq 2\), there exists a 2-layer width-\(2n\) ReLU network \(f:\mathbb{R}^{n}\to\mathbb{R}\) such that for any \(x\in\mathbb{R}^{n}\), it holds that \(f(x)=Wx\) and both weight matrices parameterizing \(f\) has spectral norm less than \(2\sqrt{2}\)._Proof.: The construction is straightforward, one can choose

\[f(x)=[I_{n},-I_{n}]^{\top}\mathrm{ReLU}\left(\begin{bmatrix}Wx\\ -Wx\end{bmatrix}\right).\]

### Discussion on Architecture Choices

The reader may notice that Equation (6) is not the same as the standard GPT architecture,

\[f_{l}(X;\theta^{(l)})= \mathrm{g}^{(l)}\Big{(}\mathrm{LN}\Big{(}W_{V}^{(l)}X\sigma\Big{(} \mathcal{C}+(W_{K}^{(l)}X)^{\top}(W_{Q}^{(l)}X)\Big{)}+X\Big{)}\Big{)}.\] (52)

We will shortly discuss the impact of considering Equation (52) here.

With similar arguments to Theorem 3 and the necessity part of Theorem 1, one can prove that similar balance conditions should also hold for a transformer with a layer specified by Equation (52) and a minimal first layer that can nearly perfectly generate bounded Dyck languages.

However, the sufficiency part of Theorem 1 no longer holds, when the balance condition holds, the last column of the term \(W_{V}^{(2)}X\sigma\Big{(}\mathcal{C}+(W_{K}^{(2)}X)^{\top}(W_{Q}^{(2)}X)\Big{)}\) will converge to zero when input length converges to infinity. Hence, if not all \(\bm{\epsilon}(\tau_{t,d})\) where \(\tau_{t,d}\) is a closed bracket aligns with \(1^{m}\), then it is impossible for the model to perfectly generate Dyck for arbitrary length. Although it remains possible to refine a sharper condition for standard GPT architecture to perfectly generate Dyck Language, we find considering Equation (6) more elegant in theory. We also verify with experiments that our architecture with standard training can learn bounded Dyck language to more than \(97\%\) accuracy. Also, the learned attention patterns are also similarly not interpretable as standard architectures.

[MISSING_PAGE_FAIL:42]

### Extended Experiments

We include more experiments on the attention variation of different Dyck languages and architectures. The results are summarized in Table 1.

Attention pattern visualization for three-layer experiments.The first-layer attention is close to uniform, while the higher-layer attention shows no clear patterns.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|} \hline \#types \(k\) & Grammar depth \(m\) & \#Layers \(l\) & Layer 1 & Layer 2 & Layer 3 \\ \hline
2 & 4 & 2 & \(0.047_{(0.006)}\) & \(7.721_{(0.908)}\) & \\ \hline
2 & 4 & 3 & \(0.070_{(0.013)}\) & \(5.072_{(0.645)}\) & \(24.063_{(1.166)}\) \\ \hline
2 & 8 & 2 & \(0.087_{(0.012)}\) & \(7.583_{(0.961)}\) & \\ \hline
2 & 8 & 3 & \(0.059_{(0.011)}\) & \(5.560_{(0.714)}\) & \(23.590_{(0.829)}\) \\ \hline
3 & 4 & 2 & \(0.182_{(0.024)}\) & \(9.313_{(0.815)}\) & \\ \hline
3 & 4 & 3 & \(0.225_{(0.032)}\) & \(8.426_{(0.877)}\) & \(25.749_{(0.897)}\) \\ \hline
3 & 8 & 2 & \(0.178_{(0.028)}\) & \(7.000_{(0.884)}\) & \\ \hline
3 & 8 & 3 & \(0.154_{(0.036)}\) & \(6.280_{(0.711)}\) & \(25.451_{(0.871)}\) \\ \hline \end{tabular}
\end{table}
Table 1: **Extended attention variation.** “Layer \(i\)” shows the mean (and standard deviation) of the attention variation on layer \(i\), calculated on \(40\) sentences. The embedding width and FFN width are fixed as \(50\) in the experiments. We train using sentences from \(\text{Dyck}_{k,m}\) of length less than 28 and test the variation on 40 randomly sampled sentences with length \(19\) (the sampled sentence is fixed across different architectures). The random attention variation baseline here is 3.33. The numbers in this table are different from previous discussion, since the results here are from a slightly different architecture than the standard GPT-2 architecture: a residue link is appended after the LayerNorm to match our theory better. The models are trained to convergence and have in-distribution accuracy higher than \(97\%\).

Figure 8: **Second-layer attention patterns of two-layer Transformers on Longer Dyck Prefix**: Models for (a),(b) are under the same setup but different random seeds. All models reach \(\geq 97\%\) accuracy (defined in Section 4.1). In the heatmap, darker color indicates larger value.

Figure 9: **Second-layer attention patterns of two-layer Transformers with a minimal first layer**: (a), (b) are based on embedding 15 with different random seeds. (c), (d) are based on embedding 17 and 16. Different embedding functions lead to diverse attention patterns, most of which are not stack-like.

Figure 11: **Third-layer attention.** The test sentence is fixed and the attention patterns learned by different 3-layer models with the same architectures on the same dataset show large variation visually.

Figure 10: **Relationship Between Balance Violation and Length Generalization.** Accuracy from Transformers with minimal first layer with embedding 15, using both standard training and contrastive regularization (Equation (18)). We again observe that contrastive regularization helps reduce the balance violation and improve the length generalization performance.