# Offline Imitation Learning with Variational Counterfactual Reasoning

 Zexu Sun\({}^{\$}\), Bowei He\({}^{\dagger}\), Jinxin Liu\({}^{\ddagger}\), Xu Chen\({}^{\$\$}\), Chen Ma\({}^{\dagger}\), Shuai Zhang\({}^{\$}\)

\({}^{\$}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{\dagger}\)Department of Computer Science, City University of Hong Kong

\({}^{\ddagger}\)School of Engineering, Westlake University \({}^{\ddagger}\)DiDi Chuxing

{sunzexu21, xu.chen}@ruc.edu.cn, boweihe2-c@my.cityu.edu.hk

liujinxin@westlake.edu.cn, chemma@cityu.edu.hk, shuai.zhang@tju.edu.cn

Corresponding author

###### Abstract

In offline imitation learning (IL), an agent aims to learn an optimal expert behavior policy without additional online environment interactions. However, in many real-world scenarios, such as robotics manipulation, the offline dataset is collected from suboptimal behaviors without rewards. Due to the scarce expert data, the agents usually suffer from simply memorizing poor trajectories and are vulnerable to the variations in the environments, lacking the capability of generalizing to new environments. To automatically generate high-quality expert data and improve the generalization ability of the agent, we propose a framework named Offline Imitation Learning with Counterfactual data Augmentation (OILCA) by doing counterfactual inference. In particular, we leverage identifiable variational autoencoder to generate _counterfactual_ samples for expert data augmentation. We theoretically analyze the influence of the generated expert data and the improvement of generalization. Moreover, we conduct extensive experiments to demonstrate that our approach significantly outperforms various baselines on both DeepMind Control Suite benchmark for in-distribution performance and CausalWorld benchmark for out-of-distribution generalization.

## 1 Introduction

By utilizing the pre-collected expert data, imitation learning (IL) allows us to circumvent the difficulty in designing proper rewards for decision-making tasks and learning an expert policy. Theoretically, as long as adequate expert data are accessible, we can easily learn an imitator policy that maintains a sufficient capacity to approximate the expert behaviors [25, 30, 29, 31]. However, in practice, several challenging issues hinder its applicability to practical tasks. In particular, expert data are often limited, and due to the typical requisite for online interaction with the environment, performing such online IL may be costly or unsafe in real-world scenarios such as self-driving or industrial robotics [11, 34, 37]. Alternatively, in such settings, we might instead have access to large amounts of pre-collected unlabeled data, which are of unknown quality and may consist of both good-performing and poor-performing trajectories. For example, in self-driving tasks, a number of human driving behaviors may be available; in industrial robotics domains, one may have access to large amounts of robot data. The question then arises: can we perform offline IL with only limited expert data and the previously collected unlabeled data, thus relaxing the costly online IL requirements?

Traditional behavior cloning (BC) [5] directly mimics historical behaviors logged in offline data (both expert data and unlabeled data) via supervised learning. However, in our above setting, BCsuffers from unstable training, as it relies on sufficient high-quality offline data, which is unrealistic. Besides, utilizing unlabeled data indiscriminately will lead to severe catastrophes: Bad trajectories mislead policy learning, and good trajectories fail to provide strong enough guidance signals for policy learning. In order to better distinguish the effect of different quality trajectories on policy learning, various solutions are proposed correspondingly. For example, ORIL [39] learns a reward model to relabel previously collected data by contrasting expert and unlabeled trajectories from a fixed dataset; DWBC [36] introduces a discriminator-weighted task to assist the policy learning. However, such discriminator-based methods are prone to overfitting and suffer from poor generalization ability, especially when only very limited expert data are provided.

Indeed, the expert data play an important role in offline IL and indicate the well-performing agent's intention. Thus, how to fully understand the expert's behaviors or preferences via limited available expert data becomes pretty crucial. In this paper, we propose to investigate counterfactual techniques for interpreting such expert behaviors. Specifically, we leverage the _variational counterfactual reasoning_[23; 4] to augment the expert data, as typical imitation learning data augmentation methods [3; 7] easily generate noisy data and inevitably bias the learning agent. Here, we conduct the counterfactual reasoning of the expert by answering the following question:

_"What action might the expert take if a different state is observed?"_

Throughout this paper, we consider the structure causal model (SCM) underlying the offline training data and introduce an exogenous variable that influences the states and yet is unobserved. This variable is utilized as the minimal edits to existing expert training data so that we can generate counterfactual states that cause an agent to change its behavior. Intuitively, this exogenous variable captures variations of features in the environment. By introducing additional variations in the states during training, we encourage the model to rely less on the idiosyncrasies of a given environment. In detail, we leverage an identifiable generative model to generate the counterfactual expert data, thus enhancing the agent's capabilities of generalization in test environments (Figure 1).

The main contributions of this paper are summarized as follows:

* We propose a novel learning framework OILCA for offline IL. Using both training data and the augmentation model, we can generate counterfactual expert data and improve the generalization of the learned agent.
* We theoretically analyze the disentanglement identifiability of the constructed exogenous variable and the influence of augmented counterfactual expert data via a sampler policy. We also guarantee the improvement of generalization ability from the perspective of error bound.
* We conduct extensive experiments and provide related analysis. The empirical results about in-distribution performance on DeepMind Control Suite benchmark and out-of-distribution generalization on CausalWorld benchmark both demonstrate the effectiveness of our method.

## 2 Related Works

Offline IL.A significant inspiration for this work grows from the offline imitation learning technique on how to learn policies from the demonstrations. Most of these methods take the idea of behavior cloning (BC) [5] that utilizes supervised learning to learn to act. However, due to the presence of suboptimal demonstrations, the performance of BC is limited to mediocre levels on many datasets. To address this issue, ORIL [39] learns a reward function and uses it to relabel offline trajectories.

Figure 1: Agent is trained with the collected dataset containing limited expert data and large amounts of unlabeled data, and tested on both in-distribution and out-of-distribution environments.

However, it suffers from high computational costs and the difficulty of performing offline RL under distributional shifts. Trained on all data, BCND [26] reuses another policy learned by BC as the weight of the original BC objective, but its performance can even be worse if the suboptimal data occupies the major part of the offline dataset. LobsDICE [12] learns to imitate the expert policy via optimization in the space of stationary distributions. It solves a single convex minimization problem, which minimizes the divergence between the two-state transition distributions induced by the expert and the agent policy. CEIL [16] explicitly learns a hindsight embedding function together with a contextual policy. To achieve the expert matching objective for IL, CEIL advocates for optimizing a contextual variable such that it biases the contextual policy towards mimicking expert behaviors. DWBC [36] introduces an additional discriminator to distinguish expert and unlabeled demonstrations, and the outputs of the discriminator serve as the weights of the BC loss. CLUE [17] proposes to learn an intrinsic reward that is consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. OILCA aims to augment the the scarce expert data to improve the performance of the learned policy.

Causal Dynamics RLAdopting this formalism allows one to cast several important problems within RL as questions of causal inference, such as off-policy evaluation [6; 22], learning baselines for model-free RL [20], and policy transfer [10; 15]. CTRL [19] applies SCM dynamics to the data augmentation in continuous sample spaces and discusses the conditions under which the generated transitions are uniquely identifiable counterfactual samples. This approach models state and action variables as unstructured vectors, emphasizing benefits in modeling action interventions for scenarios such as clinical healthcare where exploratory policies cannot be directly deployed. MOCODA [24] applies a learned locally factored dynamics model to an augmented distribution of states and actions to generate counterfactual transitions for RL. FOCUS [38] can reconstruct the causal structure accurately and illustrate the feasibility of learning causal structure in offline RL. OILCA uses the identifiable generative model to infer the distribution of the exogenous variable in the causal MDP, then performs the counterfactual data augmentation to augment the scarce expert data in offline IL.

## 3 Preliminaries

### Problem Definition

We consider the causal Markov Decision Process (MDP) [19] with an additive noise. In our problem setting, we have an offline static dataset consisting of _i.i.d_ tuples \(\mathcal{D}_{\text{all}}=\left\{s_{t}^{i},a_{t}^{i},s_{t+1}^{i}\right\}_{i=1}^ {n_{\text{all}}}\) s.t. \(\left(s_{t},a_{t}\right)\sim\rho(s_{t},a_{t}),s_{t+1}\sim f_{\varepsilon}(s_{ t},a_{t},u_{t+1})\), where \(\rho(s_{t},a_{t})\) is an offline state-action distribution resulting from some behavior policies, \(f_{\varepsilon}(s_{t},a_{t},u_{t+1})\) represents the causal transition mechanism, \(u_{t+1}\) is the sample of the exogenous variable \(u\), which is unobserved, and \(\varepsilon\) is a small permutation. Let \(\mathcal{D}_{E}\) and \(\mathcal{D}_{U}\) be the sets of expert and unlabeled demonstrations respectively, our goal is to only leverage the offline batch data \(\mathcal{D}_{\text{all}}:=\mathcal{D}_{E}\cup\mathcal{D}_{U}\) to learn an optimal policy \(\pi\) without any online interaction.

### Counterfactual Reasoning

We provide a brief background on counterfactual reasoning. Further details can be found in [23].

**Definition 1** (Structural Causal Model (SCM)).: _A structural causal model \(\mathcal{M}\) over variables \(\mathbf{X}=\left\{X_{1},\ldots,X_{n}\right\}\) consists of a set of independent exogenous variables \(\mathbf{U}=\left\{\mathbf{u}_{1},\ldots,\mathbf{u}_{n}\right\}\) with prior distributions \(P\left(\mathbf{u}_{i}\right)\) and a set of functions \(f_{1},\ldots,f_{n}\) such that \(X_{i}=f_{i}\left(\mathbf{PA}_{i},\mathbf{u}_{i}\right)\), where \(\mathbf{PA}_{i}\subset\mathbf{X}\) are parents of \(X_{i}\). Therefore, the distribution of the SCM, which is denoted \(P^{\mathcal{M}}\), is determined by the functions and the prior distributions of exogenous variables._

Inferring the exogenous random variables based on the observations, we can intervene in the observations and inspect the consequences.

**Definition 2** (_do_-intervention in SCM).: _An intervention \(I=\text{do}\left(X_{i}:=f_{i}\left(\tilde{\mathbf{PA}}_{i},\mathbf{u}_{i} \right)\right)\) is defined as replacing some functions \(f_{i}\left(\mathbf{PA}_{i},\mathbf{u}_{i}\right)\) with \(f_{i}\left(\tilde{\mathbf{PA}}_{i},\mathbf{u}_{i}\right)\), where \(\tilde{\mathbf{PA}}_{i}\) is the intervened parents of \(X_{i}\). The intervened SCM is indicated as \(\mathcal{M}^{I}\), and, consequently, its distribution is denoted as \(P^{\mathcal{M};I}\)._

The counterfactual inference with which we can answer the _what if_ questions will be obtained in the following process:1. Infer the posterior distribution of exogenous variable \(P\left(\mathbf{u}_{i}\mid\mathbf{X}=\mathbf{x}\right)\), where \(\mathbf{x}\) is a set of observations. Replace the prior distribution \(P\left(\mathbf{u}_{i}\right)\) with the posterior distribution \(P\left(\mathbf{u}_{i}\mid\mathbf{X}=\mathbf{x}\right)\) in the SCM.
2. We denote the resulted SCM as \(\mathcal{M}_{\mathbf{x}}\) and its distribution as \(P^{\mathcal{M}_{\mathbf{x}}}\), perform an intervention \(I\) on \(\mathcal{M}_{\mathbf{x}}\) to reach \(P^{\mathcal{M}_{\mathbf{x}};I}\).
3. Return the output of \(P^{\mathcal{M}_{\mathbf{x}};I}\) as the counterfactual inference.

SCM representation of causal MDPWe encode causal MDP (under a policy \(\pi\)) into an SCM \(\mathcal{M}\). The SCM shown in Figure 2 consists of an exogenous variable \(u\) and a set of functions that transmit the state \(s_{t}\), action \(a_{t}\) and latent representation \(u_{t+1}\) of \(u\) to the next state \(s_{t+1}\)_e.g._\(s_{t+1}\sim\boldsymbol{f}_{\boldsymbol{e}}\left(s_{t},a_{t},u_{t+1}\right)\), where \(\boldsymbol{e}\) is a small perturbation, and subsequently, to the next action \(a_{t+1}\), _e.g._\(a_{t+1}\sim\pi(\cdot\mid s_{t+1})\). To perform counterfactual inference on the SCM \(\mathcal{M}\), we intervene in the original parents pair \(\left(s_{t},a_{t}\right)\) to \(\left(\tilde{s}_{t},\tilde{a}_{t}\right)\). Specifically, we sample the latent representation \(\tilde{u}_{t+1}\) from the distribution of exogenous variable \(u\), and then generate the counterfactual next state \(\tilde{s}_{t+1}\), _i.e._\(\tilde{s}_{t+1}\sim\boldsymbol{f}_{\boldsymbol{e}}\left(\tilde{s}_{t},\tilde{a }_{t},\tilde{u}_{t+1}\right)\), and also the counterfactual next action \(\tilde{a}_{t+1}\), _i.e._\(\tilde{a}_{t+1}\sim\hat{\pi}(\cdot\mid\tilde{s}_{t+1})\), where \(\hat{\pi}\) is the sampler policy.

### Variational Autoencoder and Identifiability

We briefly introduce the Variational Autoencoder (VAE) and present its identifiability results. The VAE can be conceptualized as the amalgamation of a generative latent variable model and an associated inference model, both of which are parameterized by neural networks [13]. Specifically, the VAE learns the joint distribution \(p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z})=p_{\boldsymbol{\theta }}(\boldsymbol{x}\mid\boldsymbol{z})p_{\boldsymbol{\theta}}(\boldsymbol{z})\), where \(p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z})\) represents the conditional distribution of observing \(\boldsymbol{x}\) given \(\boldsymbol{z}\). Here, \(\boldsymbol{\theta}\) denotes the set of generative parameters, and \(p_{\boldsymbol{\theta}}(\boldsymbol{z})=\Pi_{i=1}^{I}p_{\boldsymbol{\theta}} \left(z_{i}\right)\) represents the factorized prior distribution of the latent variables. By incorporating an inference model \(q_{\phi}(\boldsymbol{z}\mid\boldsymbol{x})\), the parameters \(\boldsymbol{\phi}\) and \(\boldsymbol{\theta}\) can be jointly optimized by maximizing the evidence lower bound (ELBO) on the marginal likelihood \(p_{\theta}(\boldsymbol{x})\).

\[\begin{split}\mathcal{L}&=\mathbb{E}_{q_{\boldsymbol {\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log p_{\boldsymbol{\theta}}( \boldsymbol{x}\mid\boldsymbol{z})\right]-D_{\mathrm{KL}}\left(q_{\boldsymbol{ \phi}}(\boldsymbol{z}\mid\boldsymbol{x})\|p(\boldsymbol{z})\right)\\ &=\log p_{\boldsymbol{\theta}}(\boldsymbol{x})-D_{\mathrm{KL}} \left(q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})\|p_{ \boldsymbol{\theta}}(\boldsymbol{z}\mid\boldsymbol{x})\right)\leq\log p_{ \boldsymbol{\theta}}(\boldsymbol{x}),\end{split}\] (1)

where \(D_{\mathrm{KL}}\) denotes the KL-divergence between the approximation and the true posterior, and \(\mathcal{L}\) is a lower bound of the marginal likelihood \(p_{\boldsymbol{\theta}}(\boldsymbol{x})\) because of the non-negativity of the KL-divergence. Recently, it has been shown that VAEs with unconditional prior distributions \(p_{\boldsymbol{\theta}}(\boldsymbol{z})\) are not identifiable [18], but the latent factors \(\boldsymbol{z}\) can be identified with a conditionally factorized prior distribution \(p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z})\) over the latent variables to break the symmetry [9].

## 4 Offline Imitation Learning with Counterfactual Data Augmentation

At a high level, OILCA consists of following steps: (1) data augmentation via variational counterfactual reasoning, and (2) cooperatively learning a discriminator and a policy by using \(\mathcal{D}_{U}\) and augmented \(\mathcal{D}_{E}\). In this section, we detail these two steps and present our method's theoretical guarantees.

Figure 2: SCM of causal Markov Decision Process (MDP). We incorporate an exogenous variable in the SCM that is learned and utilized for counterfactual reasoning about _do_-intervention.

### Counterfactual Data Augmentation

Our intuition is that the counterfactual expert data can model deeper relations between states and actions, which can help the learned policy be generalized better. Hence, in this section, we aim to generate counterfactual expert data by answering the _what if_ question in Section 1. Consequently, counterfactual expert data augmentation is especially suitable for some real-world settings, where executing policies during learning can be too costly or slow.

As presented in Section 3.2, the counterfactual data can be generated by using the posterior of the exogenous variable obtained from the observations. Thus, we can briefly introduce the conditional VAE [13, 28] to build the latent representation of the exogenous variable \(u\). Moreover, Considering the identifiability of unsupervised disentangled representation learning [18], an additionally observed variable \(c\) is needed, where \(c\) could be, for example, the time index or previous data points in a time series, some kind of (possibly noisy) class label, or another concurrently observed variable [9]. Formally, let \(\bm{\theta}=(\bm{f},\bm{T},\bm{\lambda})\) be the parameters of the following conditional generative model:

\[p_{\bm{\theta}}(s_{t+1},u\mid s_{t},a_{t},c)=p_{\bm{f}}(s_{t+1}\mid s_{t},a_{t },u)p_{\bm{T},\bm{\lambda}}(u\mid c),\] (2)

where we first define:

\[p_{\bm{f}}(s_{t+1}\mid s_{t},a_{t},u)=p_{\bm{e}}(s_{t+1}-\bm{f}_{s_{t},a_{t}}( u)).\] (3)

Equation (2) describes the generative mechanism of \(s_{t+1}\) given the underlying exogenous variable \(u\), along with \((s_{t},a_{t})\). Equation (3) implies that the observed representation \(s_{t+1}\) is an additive noise function, _i.e._, \(s_{t+1}=\bm{f}_{s_{t},a_{t}}(u)+\bm{\varepsilon}\) where \(\bm{\varepsilon}\) is independent of \(\bm{f}_{s_{t},a_{t}}\) or \(u\). Moreover, this formulation also can be found in the SCM representation of causal MDP in Section 3.2, where we treat the function \(f\) as the parameter \(\bm{f}\) of the model.

Following standard conditional VAE [28] derivation, the evidence lower bound (ELBO) for each sample for \(s_{t+1}\) of the above generative model can be written as:

\[\begin{split}\log p(s_{t+1}\mid s_{t},a_{t},c)\geq& \mathcal{L}(\bm{\theta},\bm{\phi})\coloneqq\log p_{\bm{f}}(s_{t+1} \mid s_{t},a_{t},u)+\log p(c)-\\ & D_{\mathrm{KL}}(q_{\bm{\phi}}(u\mid s_{t},a_{t},s_{t+1},c)\|p_{ \bm{T},\bm{\lambda}}(u\mid c)).\end{split}\] (4)

Note that the ELBO in Equation (4) contains an additional term of the \(\log p(c)\) that does not affect identifiability but improves the estimation for the conditional prior [21], we present the theoretical guarantees of disentanglement identifiability in Section 4.3. Moreover, the encoder \(q_{\bm{\phi}}\) contains all the observable variables. The reason is that, as shown in Figure 2(a), from a causal perspective, \(s_{t},a_{t}\) and \(u\) form a collider at \(s_{t+1}\), which means that when given \(s_{t+1}\), \(s_{t}\) and \(a_{t}\) are related to \(u\).

We seek to augment the expert data in \(\mathcal{D}_{E}\). As shown in Figure 2(b), once the posterior of exogenous variable \(q_{\bm{\phi}}(u\mid s_{t},a_{t},s_{t+1},c)\) is obtained, we can perform _do_-intervention. In particular, we intervene in the parents \((s_{t},a_{t})\) of \(s_{t+1}\) in \(\mathcal{D}_{E}\) by re-sampling a different pair \((\tilde{s}_{t},\tilde{a}_{t})\) from the collected data in \(\mathcal{D}_{U}\). Moreover, we sample the latent representation \(\tilde{u}_{t+1}\) from the learned posterior distribution of exogenous variable \(u\). Then utilizing \(\tilde{s}_{t}\), \(\tilde{a}_{t}\), and \(\tilde{u}_{t+1}\), we can generate the counterfactual next state according to Equation (3), which is denoted as \(\tilde{s}_{t+1}\). Subsequently, we pre-train a sampler policy \(\tilde{\pi}_{E}\) with the original \(\mathcal{D}_{E}\) to sample the counterfactual next action \(\tilde{a}_{t+1}=\tilde{\pi}_{E}(\cdot\mid\tilde{s}_{t+1})\). Thus, for all the generated tuples \((\tilde{s}_{t+1},\tilde{a}_{t+1})\), we constitute them in \(\mathcal{D}_{E}\).

### Offline Agent Imitation Learning

In general, the counterfactual data augmentation of our method can enhance many offline IL methods. It is worth noting that as described in [40], discriminator-based methods are easy to be over-fitted, which can more directly show the importance and effectiveness of the counterfactual augmented data. Thus, in this section, also to best leverage the unlabeled data, we use Discriminator-Weighted Behavioral Cloning (DWBC) [36], a state-of-the-art discriminator-based offline IL method. This method introduces a unified framework to learn the policy and discriminator cooperatively. The discriminator training gets information from the policy \(\pi_{\bm{\omega}}\) as additional input, yielding a new discriminating task whose learning objective is as follows:

\[\begin{split}\mathcal{L}_{\bm{\psi}}(\mathcal{D}_{E},\mathcal{D} _{U})=&\eta\mathop{\mathbb{E}}_{(s_{t},a_{t})\sim\mathcal{D}_{E}}[- \log D_{\bm{\psi}}(s_{t},a_{t},\log\pi_{\bm{\omega}})]+\mathop{\mathbb{E}}_{(s _{t}^{\prime},a_{t}^{\prime})\sim\mathcal{D}_{U}}[-\log(1-D_{\bm{\psi}}(s_{t}^ {\prime},a_{t}^{\prime},\log\pi_{\bm{\omega}})]\\ &-\eta\mathop{\mathbb{E}}_{(s_{t},a_{t})\sim\mathcal{D}_{E}}[- \log(1-D_{\bm{\psi}}(s_{t},a_{t},\log\pi_{\bm{\omega}}))],\end{split}\] (5)where \(D_{\bm{\psi}}\) is the discriminator, \(\eta\) is called the class prior. In the previous works [35; 39], \(\eta\) is a fixed hyperparameter often assigned as 0.5.

Notice that now \(\pi_{\bm{\omega}}\) appears in the input of \(D_{\bm{\psi}}\), which means that imitation information from \(\log\pi_{\bm{\omega}}\) will affect \(\mathcal{L}_{\bm{\psi}}\), and further impact the learning of \(D_{\bm{\psi}}\). Thus, inspired by the idea of adversarial training, DWBC [36] introduces a new learning objective for BC Task:

\[\begin{split}\mathcal{L}_{\pi}=&\alpha\mathop{ \mathbb{E}}_{(s_{t},a_{t})\sim\mathcal{D}_{E}}[-\log\pi_{\bm{\omega}}(a_{t}\mid s _{t})]-\mathop{\mathbb{E}}_{(s_{t},a_{t})\sim\mathcal{D}_{E}}\left[-\log\pi_{ \bm{\omega}}(a_{t}\mid s_{t})\cdot\frac{\eta}{d(1-d)}\right]\\ &+\mathop{\mathbb{E}}_{(s^{\prime}_{t},a^{\prime}_{t})\sim \mathcal{D}_{U}}\left[-\log\pi_{\bm{\omega}}(a^{\prime}_{t}\mid s^{\prime}_{t })\cdot\frac{1}{1-d}\right],\quad\alpha>1.\end{split}\] (6)

where \(d\) represents \(D_{\bm{\psi}}(s_{t},a_{t},\log\pi_{\bm{\omega}})\) for simplicity, \(\alpha\) is the weight factor (\(\alpha>1\)). The detailed training procedure of OILCA is shown in Algorithm 1. Moreover, we also present more possible combinations with other offline IL methods and the related results in Appendix F.

```
0: Dataset \(\mathcal{D}_{E}\), \(\mathcal{D}_{U}\), \(\mathcal{D}_{\text{all}}\), pre-trained sampler policy \(\hat{\pi}_{E}\), hyperparameters \(\eta\), \(\alpha\), initial variational counterfactual parameters \(\bm{\theta},\bm{\phi}\), discriminator parameters \(\bm{\psi}\), policy parameters \(\bm{\omega}\), data augmentation batch number \(B\).
0: Learned policy parameters \(\bm{\omega}\).
1:while counterfactual training do\(\triangleright\) Variational counterfactual reasoning
2: Sample \((s_{t},a_{t},s_{t+1},c)\sim\mathcal{D}_{\text{all}}\) to form a training batch
3: Update \(\bm{\theta}\) and \(\bm{\phi}\) according to Equation (4)
4:endwhile
5:for\(b=1\) to \(B\)do\(\triangleright\) Expert data augmentation
6: Sample \((s_{t},a_{t},s_{t+1},c)\sim\mathcal{D}_{E}\), \((\tilde{s}_{t},\tilde{a}_{t})\sim\mathcal{D}_{U}\) to form an augmentation batch
7: Generate the counterfactual \(\tilde{s}_{t+1}\) according to Equation (2), then predict \(\tilde{a}_{t+1}=\hat{\pi}_{E}(\cdot\mid\tilde{s}_{t+1})\)
8:\(\mathcal{D}_{E}\cup(\tilde{s}_{t+1},\tilde{a}_{t+1})\)
9:endfor
10:while agent training do\(\triangleright\) Learning the discriminator and policy cooperatively
11: Sample \((s_{t},a_{t})\sim\mathcal{D}_{E}\) and \((s^{\prime}_{t},a^{\prime}_{t})\sim\mathcal{D}_{U}\) to form a training batch
12: Update \(\bm{\psi}\) according Equation (5) every 100 training steps\(\triangleright\) Discriminator learning
13: Update \(\bm{\omega}\) according to Equation (6) every 1 training step\(\triangleright\) Policy learning
14:endwhile ```

**Algorithm 1** Training procedure of OILCA.

### Theoretical Analysis

In this section, we theoretically analyze our method, which mainly contains three aspects: (1) disentanglement identifiability, (2) the influence of the augmented data, and (3) the generalization ability of the learned policy.

Our disentanglement identifiability extends the theory of iVAE [9]. To begin with, some assumptions are needed. Considering the SCM in Figure 2(a), we assume the data generation process in Assumption 1.

**Assumption 1**.: _(a) The distribution of the exogenous variable \(u\) is independent of time but dependent on the auxiliary variable c. (b) The prior distributions of exogenous variable \(u\) are different across auxiliary variable c. (c) The transition mechanism \(p(s_{t+1}\mid s_{t},a_{t},u)\) are invariant across different auxiliary variable c. (d) Given the exogenous variable \(u\), the next state \(s_{t+1}\) is independent of the auxiliary variable c. i.e. \(s_{t+1}\perp c\mid u\)._

Part of the above assumption is also used in [19]; considering the identifiability of iVAE, we also add some necessary assumptions, which are also practical. In the following discussion, we will show that when the underlying data-generating mechanism satisfies Assumption 1, the exogenous variable \(u\) can be identified up to permutation and affine transformations if the conditional prior distribution \(p(u|c)\) belongs to a general exponential family distribution.

**Assumption 2**.: _The prior distribution of the exogenous variable \(p(u|c)\) follows a general exponential family with its parameter specified by an arbitrary function \(\bm{\lambda}(c)\) and sufficient statistics \(\bm{T}(u)=[\bm{T}_{\bm{d}}(u),\bm{T}_{NN}(u)]\), here \(\bm{T}_{\bm{d}}(u)\) is defined by the concatenation of \([\bm{T}_{1}(u^{1})^{T},\cdots,\bm{T}_{d}(u^{d})^{T}]^{T}\) from a factorized exponential family and the outputs of a neural network \(\bm{T}_{NN}(u)\) with universal approximation power. The probability density can be written as:_

\[p_{\bm{T},\bm{\lambda}}(u\mid c)=\frac{\bm{Q}(u)}{\bm{Z}(u)}\exp \bigl{[}\bm{T}(u)^{T}\bm{\lambda}(c)\bigr{]}.\] (7)

Under the Assumption 2, and leveraging the ELBO in Equation (4), we can obtain the following identifiability of the parameters in the model. For convenience, we omit the subscript of \(\bm{f}_{s_{t},a_{t}}\) as \(\tilde{\bm{f}}\).

**Theorem 1**.: _Assume that we observe data sampled from a generative model defined according to Equation (2)-(3) and Equation (7) with parameters \((\bm{f},\bm{T},\bm{\lambda})\), the following holds:_

1. _The set_ \(\{s_{t+1}\in\mathcal{S}:\varphi_{\bm{e}}(s_{t+1}=0)\}\) _has measure zero, where_ \(\varphi_{\bm{e}}\) _is the characteristic function of the density_ \(p_{\bm{e}}\) _defined in Equation (_3_)._
2. _The function_ \(\bm{f}\) _is injective and all of its second-order cross partial derivatives exist._
3. _The sufficient statistics_ \(\bm{T}_{\bm{d}}\) _are twice differentiable._
4. _There exist_ \(k+1\) _distinct points_ \(c^{0},\ldots,c^{k}\) _such that the matrix_ \[L=\left(\bm{\lambda}\left(c^{1}\right)-\bm{\lambda}\left(c^{0}\right),\ldots, \bm{\lambda}\left(c^{k}\right)-\bm{\lambda}\left(c^{0}\right)\right)\] (8) _of size_ \(k\times k\) _is invertible._

_Then, the parameters \(\bm{\theta}=(\bm{f},\bm{T},\bm{\lambda})\) are identifiable up to an equivalence class induced by permutation and component-wise transformations._

Theorem 1 guarantees the identifiability of Equation (2). We present its proof in Appendix A.

Moreover, Theorem 1 further implies a consistent result on the conditional VAE. If the variational distribution of encoder \(q_{\bm{\phi}}\) is a broad parametric family that includes the true posterior, we have the following results.

**Theorem 2**.: _Assume the following holds:_

1. _There exists the_ \((\bm{\theta},\bm{\phi})\) _such that the family of distributions_ \(q_{\bm{\phi}}\left(u\mid s_{t},a_{t},s_{t+1},c\right)\) _contains_ \(p_{\bm{\theta}}\left(u\mid s_{t},a_{t},s_{t+1},c\right)\)_._
2. _We maximize_ \(\mathcal{L}(\bm{\theta},\bm{\phi})\) _with respect to both_ \(\bm{\theta}\) _and_ \(\bm{\phi}\)_._

_Then, given infinite data, OILCA can learn the true parameters \(\bm{\theta}^{*}:=(\bm{f}^{*},\bm{T}^{*},\bm{\lambda}^{*})\)._

We present the corresponding proof in Appendix B. Theorem 2 is proved by assuming our conditional VAE is flexible enough to ensure the ELBO is tight for some parameters and the optimization algorithm can achieve the global maximum of ELBO.

In our framework, the current generated expert sample pairs \((\tilde{s}_{t+1},\tilde{a}_{t+1})\) are estimated based on the sampler policy \(\hat{\pi}_{E}\). However, \(\hat{\pi}_{E}\) may be not perfect, and its predicted results may contain noise. Thus, we would like to answer: "given the noise level of the sampler policy, how many samples one need to achieve sufficiently well performance?". Using \(\kappa\in(0,0.5)\) indicates the noise level of \(\hat{\pi}_{E}\). If \(\hat{\pi}_{E}\) can exactly recover the true action \(a_{t+1}\) (i.e., \(\kappa=0\) ), then the generated sequences are perfect without any noise. On the contrary, \(\kappa=0.5\) means that \(\hat{\pi}_{E}\) can only produce random results, and the generated sequences are fully noisy. Then we have the following theorem:

**Theorem 3**.: _Given a hypothesis class \(\mathcal{H}\), for any \(\epsilon,\delta\in(0,1)\) and \(\kappa\in(0,0.5)\), if \(\hat{\pi}_{E}\in\mathcal{H}\) is the pretrained policy model learned based on the empirical risk minimization (ERM), and the sample complexity (i.e., number of samples) is larger than \(\frac{2\log\left(\frac{2k}{2}\right)}{\epsilon^{2}(1-2\kappa)^{2}}\), then the error between the model estimated and true results is smaller than \(\epsilon\) with probability larger than \(1-\delta\)._

The related proof details are presented in Appendix C. From Theorem 3, we can see: in order to guarantee the same performance with a given probability (i.e., \(\epsilon\) and \(\delta\) are fixed), one needs to generate more than \(\frac{2\log\left(\frac{2k}{2}\right)}{\epsilon^{2}(1-2\kappa)^{2}}\) sequences. If the noise level \(\kappa\) is larger, more samples have to be generated. Extremely, if the pre-trained policy can only produce fully noisy information (i.e., \(\kappa=0.5\)), then infinity number of samples are required, which is impossible in reality.

For the generalization ability, [8] explains the efficacy of counterfactual augmented data by the empirical evidence. In the context of offline IL, a straightforward yet very relevant conclusion from the analysis of generalization ability is the strong dependence on the number of expert data [25]. We work with finite state and action spaces (\(|\mathcal{S}|,|\mathcal{A}|<\infty\)), and for the learned policy \(\pi_{\bm{\omega}}\), we can analyze the generalization ability from the perspective of error upper bound with the optimal expert policy \(\pi^{\star}\).

**Theorem 4**.: _Let \(|\mathcal{D}_{E}|\) be the number of empirical expert data used to train the policy and \(\epsilon\) be the expected upper bound of generalization error. There exists constant \(h\) such that, if_

\[|\mathcal{D}_{E}|\geq\frac{h|\mathcal{S}||\mathcal{A}|\log(|\mathcal{S}|/ \delta)}{\epsilon^{2}},\] (9)

_and each state \(s_{t}\) is sampled uniformly, then, with probability at least \(1-\delta\), we have:_

\[\max_{s_{t}}\left\lvert\pi^{\star}(\cdot\mid s_{t})-\pi_{\bm{\omega}}(\cdot \mid s_{t})\right\rvert_{1}\leq\epsilon.\] (10)

_which shows that increasing \(|\mathcal{D}_{E}|\) drastically improves the generalization guarantee._

Note that we provide the proof details for Theorem 4 in D.

## 5 Experiments

In this section, we evaluate the performance of OILCA, aiming to answer the following questions.

**Q1:** With the synthetic toy environment of the causal MDP, can OILCA disentangle the latent representations of the exogenous variable? **Q2:** For an in-distribution test environment, can OILCA improve the performance? **Q3:** For an out-of-distribution test environment, can OILCA improve the generalization? In addition, we use five baseline methods: BC-exp (BC on the expert data \(\mathcal{D}_{E}\)), BC-all (BC on all demonstrations \(\mathcal{D}_{\text{all}}\)), ORIL [39], BCND [26], LobsDICE [12], DWBC [36]. More details about these methods are presented in Appendix E. Furthermore, the hyper-parameters of our method and baselines are all detail-tuned for better performance.

### Simulations on Toy Environment (Q1)

In real-world situations, we can hardly get the actual distributions of the exogenous variable. To evaluate the disentanglement identifiability in variational counterfactual reasoning, we build a toy environment to show that our method can indeed get the disentangled distributions of the exogenous variable.

Toy environmentFor the toy environment, we consider a simple 2D navigation task. The agent can move a fixed distance in each of the four directions. States are continuous and are considered as the 2D position of the agent. The goal is to navigate to a specific target state within a bounding box. The reward is the negative distance between the agent's state and the target state. For the initialization of the environment, we consider \(C\) kinds of Gaussian distributions (Equation (7)) for the exogenous variable, where \(C\) = \(3\), and the conditional variable \(c\) is the class label. We design the transition function by using a multi-layer perceptron (MLP) with invertible activations. We present the details about data collection and model learning in Appendix E.

ResultsWe visualize the identifiability in such a 2D case in Figure 3, where we plot the sources, the posterior distributions learned by OILCA, and the posterior distributions learned by a vanilla conditional VAE, respectively. Our method recovers the original sources to trivial indeterminacies (rotation and sign flip), whereas the vanilla conditional VAE fails to separate the latent variables well. To show the effectiveness of our method in the constructed toy environment, we also conduct repeated experiments for 1K episodes (500 steps per episode) to compare OILCA with all baseline methods. The results are presented in Figure 4, showing that OILCA achieves the highest average return. To analyze the influence of the number of augmented samples (Theorem 4), we also conduct the experiments with varying numbers of counterfactual expert data; the result is shown in Figure 5. The X-axis represents the percentage of \(|\mathcal{D}_{E}|/|\mathcal{D}_{U}|\), where \(\mathcal{D}_{E}\) is the augmented expert data. We can observe that within a certain interval, the generalization ability of the learned policy does improve with the increase of \(|\mathcal{D}_{E}|\).

### In-distribution Performance (Q2)

We use DeepMind Control Suite[32] to evaluate the performance of in-distribution performance. Similar to [39], we also define an episode as positive if its episodic reward is among the top 20% episodes for the task. For the auxiliary variable \(c\), we add three kinds of different Gaussian noise distributions into the environment (encoded as \(c=\{0,1,2\}\)). Moreover, we present the detailed data collection and statistics in Appendix E.1). We report the results in Table 1. We can observe that OILCA outperforms baselines on all tasks; this shows the effectiveness of the augmented counterfactual expert data. We also find that the performance of ORIL and DWBC tends to decrease in testing for some tasks (ORIL: Cheetah Run, Walker Walk; DWBC: Cheetah Run, Manipulator Insert Ball); this "overfitting" phenomenon also occurs in experiments of previous offline RL works [14, 33]. This is perhaps due to limited data size and model generalization bottleneck. DWBC gets better results on most tasks, but for some tasks, such as Manipulator Insert Ball and the Walker Walk, OILCA achieves more than twice the average return than DWBC.

### Out-of-distribution Generalization (Q3)

To evaluate the out-of-distribution generalization of OILCA, we use a benchmark named Causal-World[2], which provides a combinatorial family of tasks with a common causal structure and

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline Task Name & BC-exp & BC-all & ORIL & BCND & LobsDICE & DWBC & **OILCA** \\ \hline \multirow{3}{*}{Cartpole Swingup} & 195.44 & 269.03 & 221.24 & 243.52 & 292.96 & 382.55 & **608.38** \\  & \(\pm\) 7.39 & \(\pm\) 7.06 & \(\pm\) 14.49 & \(\pm\) 11.33 & \(\pm\) 11.05 & \(\pm\) 8.95 & \(\pm\) 35.54 \\ Cheetah Run & 66.59 & 90.01 & 45.08 & 96.06 & 74.53 & 66.87 & **116.05** \\  & \(\pm\) 11.09 & \(\pm\) 31.74 & \(\pm\) 9.88 & \(\pm\) 16.15 & \(\pm\) 7.75 & \(\pm\) 4.60 & \(\pm\) 14.65 \\ Finger Turn Hard & 129.20 & 104.56 & 185.57 & 204.67 & 109.03 & 243.47 & **298.73** \\  & \(\pm\) 4.51 & \(\pm\) 8.32 & \(\pm\) 26.75 & \(\pm\) 13.18 & \(\pm\) 12.19 & \(\pm\) 17.12 & \(\pm\) 5.11 \\ Fish Swim & 74.59 & 68.87 & 84.90 & 153.28 & 188.84 & 212.39 & **290.28** \\  & \(\pm\)11.73 & \(\pm\) 11.93 & \(\pm\)19.96 & \(\pm\)19.29 & \(\pm\)11.28 & \(\pm\)7.62 & \(\pm\) 10.07 \\ Humanoid Run & 77.59 & 138.93 & 96.88 & 257.01 & 120.87 & 302.33 & **460.96** \\  & \(\pm\) 6.63 & \(\pm\) 9.14 & \(\pm\)10.76 & \(\pm\)11.21 & 41.66 & \(\pm\)14.53 & \(\pm\) 17.58 \\ Manipulator Insert Ball & 91.61 & 98.59 & 98.25 & 141.71 & 197.79 & 107.86 & **296.83** \\  & \(\pm\)7.49 & \(\pm\)1.62 & \(\pm\)2.68 & \(\pm\)15.30 & \(\pm\)1.98 & \(\pm\)15.01 & \(\pm\) 3.43 \\ Manipulator Insert Peg & 92.02 & 119.63 & 105.86 & 220.66 & 299.19 & 238.39 & **305.66** \\  & \(\pm\)15.04 & \(\pm\)6.37 & \(\pm\)5.10 & \(\pm\)15.14 & \(\pm\)3.40 & \(\pm\)19.76 & \(\pm\) 4.91 \\ Walker Stand & 169.14 & 192.14 & 181.23 & 279.66 & 252.34 & 280.07 & **395.51** \\  & \(\pm\) 8.00 & \(\pm\)37.91 & \(\pm\) 10.31 & \(\pm\) 12.69 & \(\pm\) 7.73 & \(\pm\) 5.79 & \(\pm\) 8.05 \\ Walker Walk & 29.44 & 75.79 & 41.43 & 157.44 & 102.14 & 166.95 & **377.19** \\  & \(\pm\) 2.18 & \(\pm\) 6.34 & \(\pm\) 4.05 & \(\pm\) 9.31 & \(\pm\) 5.94 & \(\pm\) 10.68 & \(\pm\) 15.87 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results for in-distribution performance on DeepMind Control Suite. We report the average return of episodes (with the length of 1K steps) over five random seeds. The best results and second best results are **bold** and underlined, respectively.

underlying factors. Furthermore, the auxiliary variable \(c\) represents the different _do_-interventions on the task environments [2]. We collect the offline data by using three different _do_-interventions on environment features (stage_color, stage_friction. floor_friction) to generate offline datasets, while other features are set as the defaults. More data collection and statistics details are presented in Appendix E.1. We show the comparative results in Table 2. It is evident that OILCA also achieves the best performance on all the tasks; ORIL and DWBC perform the second-best results on most tasks. BEND performs poorly compared to other methods. The reason may be that the collected data in space \(\mathbf{A}\) can be regarded as low-quality data when evaluating on space \(\mathbf{B}\), which may lead to the poor performance of a single BC and cause cumulative errors for the BC ensembles of BEND. LobsDICE even performs poorer than BC-exp on most tasks. This is because the KL regularization, which regularizes the learned policy to stay close to \(\mathcal{D}_{U}\) is too conservative, resulting in a suboptimal policy, especially when \(\mathcal{D}_{U}\) contains a large collection of noisy data. This indeed hurts the performance of LobsDICE for out-of-distribution generalization.

## 6 Conclusion

In this paper, we propose an effective and generalizable offline imitation learning framework OILCA, which can learn a policy from the expert and unlabeled demonstrations. We apply a novel identifiable counterfactual expert data augmentation approach to facilitate the offline imitation learning. We also analyze the influence of generated data and the generalization ability theoretically. The experimental results demonstrate that our method achieves better performance in simulated and public tasks.

## Acknowledgement

This work is supported in part by National Key R&D Program of China (2022ZD0120103), National Natural Science Foundation of China (No. 62102420), Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China, Public Computing Cloud, Renmin University of China, fund for building world-class universities (disciplines) of Renmin University of China, Intelligent Social Governance Platform.

## References

* [1] Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms, 2021.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline Task Name & BC-exp & BC-all & ORIL & BEND & LobsDICE & DWBC & **OILCA** \\ \hline \multirow{2}{*}{Reaching} & 281.18 & 176.54 & 339.40 & 228.33 & 243.29 & 479.92 & **976.60** \\  & \(\pm\) 16.45 & \(\pm\) 9.75 & \(\pm\) 12.98 & \(\pm\) 7.14 & \(\pm\) 9.84 & \(\pm\) 18.75 & \(\pm\) 20.13 \\  & \(\pm\) 256.64 & 235.58 & 283.91 & 191.23 & 206.44 & 298.09 & **405.08** \\ \multirow{2}{*}{Pushing} & \(\pm\) 12.70 & 10.23 & \(\pm\) 19.72 & \(\pm\) 12.64 & \(\pm\) 13.55 & \(\pm\) 14.94 & \(\pm\) 24.03 \\  & 270.01 & 258.54 & 388.15 & 221.89 & 37.78 & 366.26 & **491.09** \\ \multirow{2}{*}{Picking} & \(\pm\) 13.13 & \(\pm\) 16.53 & \(\pm\) 19.21 & \(\pm\) 7.68 & \(\pm\) 12.09 & \(\pm\) 8.77 & \(\pm\) 6.44 \\  & 294.06 & 225.42 & 270.75 & 259.12 & 266.09 & 349.66 & **490.24** \\ \multirow{2}{*}{Pick and Place} & \(\pm\) 7.34 & \(\pm\) 12.24 & \(\pm\) 14.87 & \(\pm\) 8.01 & \(\pm\) 10.31 & \(\pm\) 7.39 & \(\pm\) 11.69 \\  & \(\pm\) 496.63 & 394.91 & 388.55 & 339.18 & 362.47 & 481.07 & **831.82** \\ \multirow{2}{*}{Stacking2} & \(\pm\) 7.68 & \(\pm\) 16.98 & \(\pm\) 10.93 & \(\pm\) 9.46 & \(\pm\) 17.05 & \(\pm\) 10.11 & \(\pm\) 11.78 \\  & 667.81 & 784.88 & 655.96 & 139.30 & 535.74 & 768.68 & **994.82** \\ \multirow{2}{*}{Towers} & \(\pm\) 9.27 & \(\pm\) 17.17 & \(\pm\) 15.14 & \(\pm\) 18.22 & \(\pm\) 13.76 & \(\pm\) 24.77 & \(\pm\) 5.76 \\  & 581.91 & 452.88 & 702.15 & 341.97 & 250.44 & 1596.96 & **2617.71** \\ \multirow{2}{*}{Creatve Stacked Blocks} & \(\pm\) 26.92 & \(\pm\) 18.78 & \(\pm\) 15.30 & \(\pm\) 33.69 & \(\pm\) 14.08 & \(\pm\) 8.184 & \(\pm\) 88.07 \\  & 496.32 & 529.82 & 882.27 & 288.55 & 317.95 & 700.23 & **1348.49** \\ \multirow{2}{*}{General} & \(\pm\) 26.92 & \(\pm\) 31.01 & \(\pm\) 46.79 & \(\pm\) 19.63 & \(\pm\) 32.03 & \(\pm\) 13.71 & \(\pm\) 55.05 \\ \multirow{2}{*}{General} & \(\pm\) 492.78 & 547.32 & 647.95 & 195.06 & 458.27 & 585.98 & **891.14** \\  & \(\pm\) 17.64 & \(\pm\) 8.49 & \(\pm\) 24.39 & \(\pm\)9.80 & \(\pm\)18.69 & \(\pm\) 19.25 & \(\pm\) 423.12 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results for out-of-distribution generalization on CausalWorld. We report the average return of episodes (length varies for different tasks) over five random seeds. All the models are trained on _space_\(\mathbf{A}\) and tested on _space_\(\mathbf{B}\) to show the out-of-distribution performance [2].

* [2] Ossama Ahmed, Frederik Trauble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Scholkopf, Manuel Wuthrich, and Stefan Bauer. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. _arXiv preprint arXiv:2010.04296_, 2020.
* [3] Dafni Antotsiou, Carlo Ciliberto, and Tae-Kyun Kim. Adversarial imitation learning with trajectorial augmentation and correction. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 4724-4730. IEEE, 2021.
* [4] Leon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. _Journal of Machine Learning Research_, 14(11), 2013.
* [5] Ivan Bratko, Tanja Urbancic, and Claude Sammut. Behavioural cloning: phenomena, results and problems. _IFAC Proceedings Volumes_, 28(21):143-149, 1995.
* [6] Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search. _arXiv preprint arXiv:1811.06272_, 2018.
* [7] Satoshi Hoshino, Tomoki Hisada, and Ryota Oikawa. Imitation learning based on data augmentation for robotic reaching. In _2021 60th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)_, pages 417-424. IEEE, 2021.
* [8] Divyansh Kaushik, Amrith Setlur, Eduard Hovy, and Zachary C Lipton. Explaining the efficacy of counterfactually augmented data. _arXiv preprint arXiv:2010.02114_, 2020.
* [9] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217. PMLR, 2020.
* [10] Taylor W Killian, Marzyeh Ghassemi, and Shalmali Joshi. Counterfactually guided policy transfer in clinical settings. In _Conference on Health, Inference, and Learning_, pages 5-31. PMLR, 2022.
* [11] Beomjoon Kim and Joelle Pineau. Socially adaptive path planning in human environments using inverse reinforcement learning. _International Journal of Social Robotics_, 8:51-66, 2016.
* [12] Geon-Hyeong Kim, Jongmin Lee, Youngsoo Jang, Hongseok Yang, and Kee-Eung Kim. Lobsdice: offline imitation learning from observation via stationary distribution correction estimation. _arXiv preprint arXiv:2202.13536_, 2022.
* [13] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [14] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* [15] Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, Jianye Hao, and Ping Luo. Chipformer: Transferable chip placement via offline decision transformer. _arXiv preprint arXiv:2306.14744_, 2023.
* [16] Jinxin Liu, Li He, Yachen Kang, Zifeng Zhuang, Donglin Wang, and Huazhe Xu. Ceil: Generalized contextual imitation learning. _arXiv preprint arXiv:2306.14534_, 2023.
* [17] Jinxin Liu, Lipeng Zu, Li He, and Donglin Wang. Clue: Calibrated latent guidance for offline reinforcement learning. _arXiv preprint arXiv:2306.13412_, 2023.
* [18] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _international conference on machine learning_, pages 4114-4124. PMLR, 2019.

* [19] Chaochao Lu, Biwei Huang, Ke Wang, Jose Miguel Hernandez-Lobato, Kun Zhang, and Bernhard Scholkopf. Sample-efficient reinforcement learning via counterfactual-based data augmentation. _arXiv preprint arXiv:2012.09092_, 2020.
* [20] Thomas Mesnard, Theophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade, Anna Harutyunyan, Will Dabney, Tom Stepleton, Nicolas Heess, Arthur Guez, et al. Counterfactual credit assignment in model-free reinforcement learning. _arXiv preprint arXiv:2011.09464_, 2020.
* [21] Graziano Mita, Maurizio Filippone, and Pietro Michiardi. An identifiable double vae for disentangled representations. In _International Conference on Machine Learning_, pages 7769-7779. PMLR, 2021.
* [22] Michael Oberst and David Sontag. Counterfactual off-policy evaluation with gumbel-max structural causal models. In _International Conference on Machine Learning_, pages 4881-4890. PMLR, 2019.
* [23] Judea Pearl. Models, reasoning and inference. _Cambridge, UK: CambridgeUniversityPress_, 19 (2), 2000.
* [24] Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based counterfactual data augmentation. _arXiv preprint arXiv:2210.11287_, 2022.
* [25] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
* [26] Fumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In _International Conference on Learning Representations_, 2021.
* [27] Shai Shalev-Shwartz and Shai Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* [28] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. _Advances in neural information processing systems_, 28, 2015.
* [29] Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and J Andrew Bagnell. Feedback in imitation learning: The three regimes of covariate shift. _arXiv preprint arXiv:2102.02872_, 2021.
* [30] Wen Sun, Anirudh Vemula, Byron Boots, and Drew Bagnell. Provably efficient imitation learning from observation alone. In _International conference on machine learning_, pages 6036-6045. PMLR, 2019.
* [31] Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and matching: A game-theoretic framework for closing the imitation gap. In _International Conference on Machine Learning_, pages 10022-10032. PMLR, 2021.
* [32] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.
* [33] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. _arXiv preprint arXiv:1911.11361_, 2019.
* [34] Zheng Wu, Liting Sun, Wei Zhan, Chenyu Yang, and Masayoshi Tomizuka. Efficient sampling-based maximum entropy inverse reinforcement learning with application to autonomous driving. _IEEE Robotics and Automation Letters_, 5(4):5355-5362, 2020.
* [35] Danfei Xu and Misha Denil. Positive-unlabeled reward learning. In _Conference on Robot Learning_, pages 205-219. PMLR, 2021.

* [36] Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imitation learning from suboptimal demonstrations. In _International Conference on Machine Learning_, pages 24725-24742. PMLR, 2022.
* [37] Zhenting Zhao, Bowei He, Wenhao Luo, and Rui Liu. Collective conditioned reflex: A bio-inspired fast emergency reaction mechanism for designing safe multi-robot systems. _IEEE Robotics and Automation Letters_, 7(4):10985-10990, 2022.
* [38] Zheng-Mao Zhu, Xiong-Hui Chen, Hong-Long Tian, Kun Zhang, and Yang Yu. Offline reinforcement learning with causal structured world models. _arXiv preprint arXiv:2206.01474_, 2022.
* [39] Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf Aytar, Misha Denil, Nando de Freitas, and Scott Reed. Offline learning from demonstrations and unlabeled experience. _arXiv preprint arXiv:2011.13885_, 2020.
* [40] Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learning. In _Conference on Robot Learning_, pages 247-263. PMLR, 2021.