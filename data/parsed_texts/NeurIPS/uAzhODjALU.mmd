# The Mamba in the Llama:

Distilling and Accelerating Hybrid Models

Junxiong Wang\({}^{1}\)1 Daniele Paliotta\({}^{2,3}\)1 Avner May\({}^{3}\) Alexander M. Rush\({}^{1}\) Tri Dao\({}^{3,4}\)

\({}^{1}\)Cornell University \({}^{2}\)University of Geneva \({}^{3}\)Together AI \({}^{4}\)Princeton University

Equal Contribution. Order determined by coin flip. Correspondence to: junxiong@cs.cornell.edu and daniele.paliotta@unige.ch

Footnote 1: footnotemark:

###### Abstract

Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN model. We also find that the distilled model has natural length extrapolation, showing almost perfect accuracy in the needle-in-a-haystack test at 20x the distillation length. Code and pre-trained checkpoints are open-sourced at https://github.com/jxiw/MambaInLlama and https://github.com/itsdaniele/speculative_mamba.

## 1 Introduction

While Transformers [73] have been an essential architecture in deep learning and have driven the success of large language models such as GPT [9], Llama [71], and Mistral [37], they are prohibitively slow for very long sequence generation due to their quadratic complexity with respect to sequence length and large key-value (KV) cache requirement. Recent linear RNN models (Mamba [26], Mamba2 [18], GLA [79], RWKV [55], RetNet [68], Griffin [19]) beat Transformers in controlled experiments at small to medium scale, although the best Transformers still significantly outperform these models on downstream tasks. We note that the training times of linear RNN models are similar to those of highly optimized Transformers [79], and therefore scaling up either of these models requires substantial computational resources.

The primary benefit of linear RNN models (Mamba [26], Mamboa2 [18]) is that they have faster inference (5\(\times\) higher throughput) than Transformers. Efficient inference is emerging as a critical need for LLM systems such as new applications currently bottlenecked by the large KV cache of Transformers, e.g. reasoning over multiple long documents [30; 65; 56] and files in large codebases [61; 42]). Emerging workflows with agents [81; 77] also require large-batch inference to explore more trajectories and long-context to model complex environments.

These properties motivate the goal of distilling a large pretrained Transformer model into a linear RNN in order to generate as efficiently as possible. The technical challenges are two-fold: how to map pretrained Transformer weights to linear RNN weights for distillation, and how to adapt best-practice Transformer inference techniques, such as speculative decoding, to the new architecture. We make the following contributions:

* We show that by reusing weights from attention layers, it is possible to distill a large transformer into a large hybrid-linear RNN with minimal additional compute while preserving much of its generation quality. We propose a modified Mamboa architecture that can be directly initialized from the attention block of a pretrained model.
* We propose a multistage distillation approach that mirrors the standard LLM pipeline combining progressive distillation, supervised fine-tuning [39], and directed preference optimization [58]. This approach shows better perplexity and downstream evaluation compared with vanilla distillation.
* We develop a hardware-aware speculative sampling algorithm and a fast kernel for speculative decoding on Mamboa and hybrid architectures. We achieve a throughput of over 300 tokens/second for a Mamboa 7B model. Additionally, we show that speculative decoding can be effectively applied to our hybrid architecture.

Our experiments distill different large-scale open chat LLMs, Zephyr-7B [72], Llama-3 8B [21] to linear RNN models (hybrid Mamboa and Mamboa2), using only 20B tokens of training. Results show that the distilled approach matches the teacher model in standard Chat benchmarks [84; 43]. We also show that it performs on par or better with all similarly sized pretrained-from-scatch Mamboa models including Mamboa 7B models [52; 26] trained from scratch with 1.2T tokens or NVIDIA Hybrid Mamboa2 models [74] trained from scratch with 3.5T tokens in multiple tasks (e.g., MMLU [34], TruthfulQA [47]) in the LM evaluation [25]. Concurrent with this work, MOHAWK [6] distills a Mamboa-2 variant based on the Phi-1.5 architecture with limited computation costs and performance loss.

## 2 From Transformer to Mamboa

### Relationship Between Attention and Linear RNNs

We begin by reviewing multihead attention to clarify the shapes of intermediate objects. Notationally, we use explicit subscripts for the sequence position instead of matrix representation, to better highlight similarities between the two models.

Attention is computed in parallel for multiple differently parameterized heads. Each head takes sequence \(\bm{o}\) with hidden size \(D\) as an argument and computes,

\[\mathbf{Q}_{t}=\mathbf{W}^{Q}\bm{o}_{t},\hskip 14.226378pt\mathbf{K}_{t} =\mathbf{W}^{K}\bm{o}_{t},\hskip 14.226378pt\mathbf{V}_{t}=\mathbf{W}^{V}\bm{o}_ {t}\hskip 14.226378pt\text{for all }t,\] \[\alpha_{1}\dots\alpha_{T}=\operatorname{softmax}\bigl{(}[m_{1,t} \mathbf{Q}_{t}^{\top}\mathbf{K}_{1}\dots m_{T,t}\mathbf{Q}_{t}^{\top}\mathbf{ K}_{T}]/\sqrt{D}\bigr{)}\hskip 14.226378pt\bm{y}_{t}=\sum_{s=1}^{t}\alpha_{s} \mathbf{V}_{s}\] \[\text{where }\bm{o}_{t}\in\mathbb{R}^{D\times 1},\hskip 14.226378pt \mathbf{W}\in\mathbb{R}^{N\times D}\hskip 14.226378pt\mathbf{Q}_{t},\mathbf{K}_{t}, \mathbf{V}_{t}\in\mathbb{R}^{N\times 1}\hskip 14.226378ptm_{s,t}=\mathbf{1}(s \leq t)\]

Recent work has argued that linear RNNs can be serious competitors to attention in large language models. Several different linear RNN formulations have been proposed with similar formulations. For now, we leave the shapes of the parameters \(\mathbf{A}_{t},\mathbf{B}_{t},\mathbf{C}_{t}\) abstract, and note that linear RNNs all take the following form that maps a 1-dimensional sequence to another through an implicit matrix-valued hidden state \(\bm{h}\).

\[\bm{h}_{t}=\mathbf{A}_{t}\bm{h}_{t-1}+\mathbf{B}_{t}x_{t},\hskip 14.226378pty_{t} =\mathbf{C}_{t}^{\top}\bm{h}_{t}\] (1)Linear RNNs have several computational advantages over attention. During training, all \(\bm{y}_{t}\) values can be computed more efficiently than attention since there is no softmax non-linearity. During inference, each next \(\bm{y}_{t}\) can be computed serially without requiring a cache.

Despite the superficially different form, there is a natural relationship between linear RNNs and attention. _Linearizing_ the attention formula by removing the softmax yields:

\[y_{t}=\sum_{s=1}^{t}\alpha_{s}v_{s}=\frac{1}{\sqrt{D}}\sum_{s=1}^{t}m_{s,t} \mathbf{Q}_{t}^{\top}\mathbf{K}_{s}v_{s}=\frac{1}{\sqrt{D}}\mathbf{Q}_{t}^{ \top}\sum_{s=1}^{t}m_{s,t}\mathbf{K}_{s}v_{s}\]

This implies that there exists a linear RNN form of linearized attention, specifically:

\[\bm{h}_{t}=m_{t-1,t}\bm{h}_{t-1}+\mathbf{K}_{t}v_{t} \quad y_{t}=\frac{1}{\sqrt{D}}\mathbf{Q}_{t}^{\top}\bm{h}_{t}\] \[\downarrow\] \[\bm{h}_{t}=\mathbf{A}_{t}\bm{h}_{t-1}+\mathbf{B}_{t}x_{t}, \quad y_{t}=\mathbf{C}_{t}^{\top}\bm{h}_{t}\] \[\mathbf{A}_{t}=m_{t-1,t},\ \mathbf{B}_{t}=\mathbf{W}^{K}\bm{o}_{t}, \ \mathbf{C}_{t}=\mathbf{W}^{Q}\bm{o}_{t},\ \bm{x}_{t}=\mathbf{W}^{V}\bm{o}_{t}\]

Note though that this version uses a hidden state of size \(\bm{h}\in\mathbb{R}^{N\times 1}\). Effectively tracking only one scalar over time per hidden dimension. Naively applying this transformation leads to poor results. The issue is that linearizing attention produces a degraded representation of the original model, as the softmax nonlinearity is critical to attention.

The key to improving these models is to increase the capacity of the linear hidden state to better capture long-term structure. For example, previous work has shown the use of kernel methods to improve this approximation [63, 36, 83]. These approaches expand the size of the hidden state representation to \(\bm{h}\) to \(\mathbb{R}^{N\times N^{\prime}}\) to better match the modeling capacity of softmax.

### Distilling to an Expanded Linear RNN

To design a effective distilled linear RNN, we aim to stay as close as possible to the original Transformer parameterization, while also expanding the capacity of the linear RNN in an efficient manner. We will _not_ attempt to have the new model capture the exact original attention function, but instead use the linearized form as a starting point for distillation.

Specifically, we adapt the parameterization from Mamba, [26] to increase the hidden state size, while initializing from the attention representation. Mamba uses a continuous time state-space model (SSM) to parameterize a linear RNN at run time, described by the differential equation,

\[\bm{h}^{\prime}(k)=\mathbf{A}\bm{h}(k)+\mathbf{B}(k)\bm{x}(k)\quad\quad\bm{y}( k)=\mathbf{C}(k)\bm{h}(k)\]

Where \(\mathbf{A}\) is a diagonal matrix and other values are continuous signals. To apply this formulation to a discrete-time problem like language modeling, we use a neural network to produce a sequence of sampling intervals \(\Delta_{t}\) and samples of the signals at these time steps. Given these sampling intervals, and \(T\) samples of \(\mathbf{B},\mathbf{C}\), Mambo approximates the continuous-time equation using a linear RNN as a discretization. We use an overbar to indicate the discrete-time form, which is reconstructed dynamically.

\[\overline{\mathbf{A}}_{1\dots T},\overline{\mathbf{B}}_{1\dots T},\overline{ \mathbf{C}}_{1\dots T}=\text{Discretize}(\mathbf{A},\mathbf{B}_{1\dots T}, \mathbf{C}_{1\dots T},\Delta_{1\dots T})\]In this simplest case, with \(N^{\prime}=1\) and an identity discretization, this approach recovers the linear attention to linear RNN conversion discussed in the previous section. The benefit of Mampa is that with \(N^{\prime}>1\) the continuous-time parameterization allows the model to learn significantly richer functions, without many more parameters or decreased efficiency. Specifically the only additional learned parameters will be the sampling rate \(\Delta\) and the dynamic \(\mathbf{A}\). These new parameters will control the constructed linear RNN through the discretization function yielding the new matrix valued linear RNN. Specifically, we take in the same \(\mathbf{B}_{t},\mathbf{C}_{t}\in\mathbb{R}^{N\times 1}\) and \(\Delta_{t}\in\mathbb{R}^{N^{\prime}}\), but output \(\overline{\mathbf{B}}_{t},\overline{\mathbf{C}}_{t}\in\mathbb{R}^{N^{\prime} \times N\times 1}\), effectively increasing the hidden size by a factor of \(N^{\prime}\) over the naive linear attention.

A core contribution of Mampa [26; 18] is to demonstrate a hardware-aware factorization of this algorithm. Implementing the algorithm naively would be prohibitively slow as the new expanded parameters are quite large. Their approach fuses discretization, state expansion, and applying the linear RNN into a single kernel, which circumvents fully materializing the discrete parameters. This allows for large \(N^{\prime}\) with relatively small efficiency costs.

### Attention-to-Mamba Initialization and Hybrid Stepwise Training

Our full approach is shown in Algorithm 1. This algorithm feeds the standard \(\mathbf{Q},\mathbf{K},\mathbf{V}\) heads from attention directly into the Mampa discretization, and then applies the resulting linear RNN. As noted above, this can seen as roughly initializing with linearized attention and allowing the model to learn richer interactions through the expanded hidden state.

Figure 1 shows the resulting architecture. Our version directly replaces Transformer attention heads directly with fine-tune linear RNN layers. We keep the Transformer MLP layers as is and do not train them. This approach also requires processing additional components like grouped query attention that shares keys and values across heads. We note that this architecture differs from the architecture used in many Mampa systems, which combines MLP-SSM layers and uses a single head.

This initialization allows us to replace any attention block with a linear RNN block. We experiment with _hybrid_ models where we keep every \(n\) attention layers. Empirically we found that replacing layers in a stepwise manner was the most effective strategy, i.e. we first keep every 2 layers, distill, and then every 4, and continue distillation.

Figure 1: Transferring Transformer to Mampa. Weights, in orange, are initialized from the Transformer (Linear projections for \(\mathbf{Q}\), \(\mathbf{K}\), and \(\mathbf{V}\) are initialized using linear projection for \(\mathbf{C}\), \(\mathbf{B}\), and \(\mathbf{X}\) respectively). We replace individual attention heads with Mampa heads, and then finetune Mampa blocks while freezing the MLP blocks. Shapes are kept mainly the same. Weights in green are added. New parameters are introduced for the learned \(\mathbf{A}\) and \(\Delta\) parameters.

Knowledge Distillation for Aligned LMs

Knowledge distillation (KD) [35] serves as a compression technique aimed at training a smaller network that mimics the behavior of a larger teacher network. After initializing the model from the Transformer parameters, we aim to distill it to perform on par with the original language model. We assume that most of the knowledge from the transformer is maintained in the MLP layers which were transferred from the original model, and focus on distilling the fine-tuning and alignment steps of the LLM. During this stage, the MLP layers are kept frozen and the Mamba layers are trained as in Figure 1.

**Supervised Fine-Tuning** We first apply knowledge distillation to redo the supervised fine-tuning (SFT) stage of language model adaptation. During this stage, an LLM is trained to maximize the likelihood of a response \(y\) given an input prompt \(x\), i.e. \(p(y\mid x)\). The task looks similar to conditional generation.

There are two common approaches for distillation in this setting. One method is to use word-level KL-Divergence. In this setting, the full probability distribution of the student model \(p(\cdot;\theta)\) is trained to match the full distribution of the teacher model \(p(\cdot;\theta_{T})\) by minimizing the KL divergence over the entire set of next possible tokens at position \(t\). The second method is sequence-level knowledge distillation (SeqKD) [39]. SeqKD suggests a simple method for distillation on this style of task, by replacing the ground truth text \(y_{1\cdots t}\) with the teacher generation output \(\hat{y}_{1\cdots t}\), also known as pseudo-labels.

\[\mathcal{L}(\theta)=-\sum_{t=1}^{T}\alpha\ \log\ p(\hat{y}_{t+1}\mid\hat{y}_{1 \cdots t},x,\theta)+\beta\ \text{KL}\left[p(\cdot\mid\hat{y}_{1\cdots t},x,\theta_{T})\mid\mid p(\cdot \mid\hat{y}_{1\cdots t},x,\theta)\right]\] (2)

Here \(\theta\) is trainable parameters of the student model and \(\alpha\) and \(\beta\) control the weights of sequence and word loss term respectively.

**Preference Optimization** The second stage of instruction-tuning for LLMs is to align them to a set of user preferences. During this stage, a set of desired preference pairs is used to improve the model's output. The objective is to produce outputs \(y\) to prompts \(x\) that maximize a reward model \(r\) while maintaining close to a reference model. Typically the reference model is chosen to be the model after supervised fine-tuning. For distillation, we can conveniently utilize the original teacher, i.e.

\[\max_{\theta}\mathbb{E}_{x\sim\mathcal{D},y\sim p(y|x;\theta)}\big{[}r_{ \phi}(x,y)\big{]}-\beta\text{KL}\big{[}p(y\mid x;\theta)\mid\mid\pi(y\mid x ;\theta_{T})\big{]}\] (3)

This preference model is defined by a reward function \(r_{\phi}(x,y)\) dependent on the method used. Previous research utilizing AI feedback has primarily focused on employing reinforcement learning methods, such as proximal policy optimization (PPO) [64], to optimize \(\phi\) concerning this reward. Recently, methods using direct preference optimization (DPO) [58] have been effective at optimizing this objective with direct gradient updates. Specifically, DPO shows that, if we have access to preferred \(y_{w}\) and dispreferred \(y_{l}\) outputs for a given prompt \(x\), we can reformulate this optimization problem as,

\[\pi_{\theta}=\max_{\theta}\mathbb{E}_{(x,y_{w},y_{l})}\sim\mathcal{D}\log \sigma\left(\beta\log\frac{p(y_{w}|x;\theta)}{p(y_{w}|x;\theta_{T})}-\beta \log\frac{p(y_{l}|x;\theta)}{p(y_{l}|x;\theta_{T})}\right).\] (4)

This optimization can be performed at the sequence level by scoring the preferred and dispreferred outputs of the model with the teacher and student and then backpropagating to the student. As far as we are aware this is the first use of DPO as a distillation objective.

## 4 Speculative Decoding Algorithms For Linear RNNs

The main goal of the linear RNN formulation is to improve decoding efficiency. For both attention and linear RNNs, the serial dependency of autoregressive generation inherently bottlenecks efficiency. Systems cannot utilize all available compute, as they need to wait for the generation of previous tokens to proceed [67; 41; 11; 76; 10]. _Speculative decoding_ has emerged as a method for breaking this bottleneck by spending extra compute to speculate 

### Challenges in RNN Speculation

Speculative decoding uses two models: a draft model, \(\theta_{D}\), and a verification model, \(\theta_{V}\). The fast draft model produces potential future completions, \(y^{*}=\arg\max_{y_{1:T}}p(y_{1},\dots,y_{T};\theta_{D})\), and the larger verification model checks that these are top ranking at each time step, i.e. checking \(p(y^{*}_{t}|y^{*}_{1:t-1};\theta_{V})\). The longer a chain before a verification failure the faster the output. If a partial chain matches, we can rewind to the last match.

Attention-based models are particularly amenable to speculation, as they are slow at generation due to sequential nature, but fast at verification due to their ability to check multiple tokens in parallel. Linear RNN models like Mampa have significantly different performance characteristics that make them less amenable to speculative decoding. Sequential decoding using recurrent-style sampling is already significantly faster than attention. Like attention, there are parallel modes for models like Mampa which are used at training. These are efficient, but are tuned for extremely long sequences. In addition, they rely on hardware-aware optimizations, such as avoiding materializing intermediate states. These properties make it difficult to use for speculation for relatively short chains when it is unknown when a conflict will occur.

An additional challenge arises from caching states in RNN models. The state of an attention model is represented by the key-value cache, \(\mathbf{K}_{1:t},\mathbf{V}_{1:t}\); whereas the state of an RNN model is simply \(\bm{h}_{t}\). To be competitive with attention this single RNN state needs to be very large. During speculation, we need to rewind to a previous state at time step \(t^{\prime}\). For attention, this is simply \(\mathbf{K}_{1:t^{\prime}},\mathbf{V}_{1:t^{\prime}}\); however, for RNNs this would require caching all \(\bm{h}_{1:t}\) which would require a large memory overhead.

### Multi-Step Linear RNN Speculation

We propose a new algorithm for linear RNN speculative decoding using hardware-aware multi-step generation. The core to the approach generation kernel that computes,

\[y_{j:k},\bm{h}_{j},\bm{h}_{k}\leftarrow\textsc{Mult}\textsc{Step}(\bm{h}_{i}, y_{1:n},i,j,k;\mathbf{A},\mathbf{B},\mathbf{C},\Delta)\]

Where \(i\) is the starting hidden state, \(i\leq j\leq k\), and \(j\dots k\) is the range of \(\bm{y}\) outputs needed. The kernel is hardware-aware because it avoids materializing key terms off of the fast GPU

Figure 2: Multi-Step RNN Speculative Decoding. _Left (top)_: The draft model generates the set of blue draft tokens sequentially. The draft tokens are then verified. _Right (top)_: Verification uses the multistep kernel, without materializing the intermediate states. The last token is rejected and replaced with the true best token. Note, that even though more tokens are generated we cannot advance the hidden state cache. _Left (bottom)_ The draft model can now generate more blue draft tokens from the current tokens, resulting in six total. _Right (bottom)_ When the new draft is verified, the multi-step kernel returns both the hidden state after the yellow token and the final hidden state, since verification will fall between those positions. on future generations. In this section, we consider approaches for applying this technique to large Mampa models, which can then be applied to the distilled models.

memory. Specifically, it avoids instantiating most \(\bm{h}_{1:n}\) as well as the discrete-time linear RNN parameters. This kernel is aimed to target the issues presented above. Specifically, it can save a snapshot of the state \(\bm{h}_{j}\) before evaluating the draft tokens. This allows recomputing the correct state on the fly after a token is rejected. The assumption is that decoding is bottlenecked by memory and not by compute, as we can compute multiple steps of decoding with very little overhead over single-step decoding.

Algorithm 2 and Figure 2 show the full algorithm. The approach maintains only one RNN hidden state in cache for verification and advances it lazily based on the success of the multi-step kernel. Since the distilled models contain transformer layers, we also extend speculative decoding to Attention/RNN hybrid architectures. In this setting, the RNN layers perform verification according to Algorithm 2, while the transformer layers simply perform parallel verification.

Note that if the draft model is a Mambo or hybrid model, the speculation part of the algorithm gets more complicated, as the draft model needs to recompute the state for the tokens accepted in the previous iteration. This is done similarly to the verifier model, by caching older entries and recomputing on the fly during the next round of speculation.

### Speculation Analysis and Hardware Specific Optimization

To verify the effectiveness of this approach we run the speculation using Mambo 7B and Mambo 2.8B as target models. Results are shown in Table 1. Figure 3 shows the performance characteristics of the Multi-Step kernel itself.

Speedup on H100 GPUs.A naive implementation of our algorithm already shows strong performance on Ampere GPUs as shown in Table 1. However, achieving strong performance on H100 GPUs is much more challenging. This is mainly due to GEMM operations being much faster, which makes the overhead incurred from the caching and recomputation operations more visible. In practice, the naive implementation of our algorithm, with several different kernel

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Model Size** & **GPU** & **K** & **\# Gen. Tokens** & **Throughput (toks/s)** & **Speedup** \\ \hline
2.8B & 3090 & 3 & 3.01 & 259 & 2.3x \\
2.8B & 3090 & 4 & 3.28 & 289 & 2.6x \\ \hline
2.8B & H100 & 3 & 4.04 & 389 & 1.71x \\
2.8B & H100 & 4 & 3.9 & 421 & 1.85x \\ \hline \hline
7B & 3090 & 3 & 3.19 & 109 & 2.1x \\
7B & 3090 & 4 & 3.56 & 110 & 2.1x \\ \hline
7B & H100 & 3 & 3.28 & 271 & 1.95x \\
7B & H100 & 4 & 3.6 & 272 & 2x \\ \hline \hline \end{tabular}
\end{table}
Table 1: Speedup results for speculative decoding with pure Mambo models. The 2.8B verifier uses a 130M Mambo draft. The 7B verifier uses a Llama3 1B draft we trained. Data is from The Pile. \(K\) is number of draft tokens produced, # _Gen_ includes an additional token from the last verifier logits.

Figure 3: Performance of the multi-step SSM kernel for generating 32 tokens.

calls, achieves a decent speedup on 3090 GPUs (1.5x for Mamba 2.8B with \(60\%\) acceptance rate) but no speedup at all on H100s.

We optimized our implementation by fusing kernels, and by adapting the implementation to easily allow caching and recomputing old steps. Specifically, the verifier model performs i) recomputation of previous steps from the cache, ii) multistep decoding for the new sequence of draft tokens and iii) caching within a single kernel 2. For the draft model, recomputation, decoding and caching are also fused in a single kernel. The resulting implementations archives speedups on H100s GPUs, as shown in Table 1.

Footnote 2: Additionally, we implement the convolutional part of the Mamba block using a circular buffer which allows us to keep track of the old entries and include them in the convolution when they are needed for recomputation.

## 5 Results

### Experimental Setup

**Target models.** We perform experiments using two LLM chat models: Zephyr-7B [72], which is a chat fine-tuned Mistral 7B [37], Llama-3 Instruct 8B [21]. For the linear RNN models, we use hybrid versions of Mamba and Mamba2 with 50%, 25%, 12.5%, and 0% attention layers. We refer to 0% as a pure Mamba model. Mamba2 is a variant architecture of Mamba that is designed to be more targeted to recent GPU architectures. Zephyr-Mamba refers to a distillation from Zephyr [72], while Llama3-Mamba / Llama3-Mamba2 indicates distillation from Llama-3 instruct 8B [71]. Strictly speaking, our distilled Mamba-Zephyr is a subquartic model, since Zephyr/Mistral-8B uses sliding window attention architecture. Our distilled Mamba-Zephyr (50%) has the similar architecture as Samba [60].

**Training.** Distillation does not require any language modeling pretraining data, but instead uses the post-training process to adapt the new model. We use a three-stage process. In the first stage, we use UltraChat [20] and UltraFeedback [17] as seed prompts and use the teacher model to generate pseudo-labels. The student model is trained in one epoch using the loss \(\mathcal{L}\) in Eq 2 with \(\alpha=1\) and \(\beta=0.1\). Models are trained using AdamW optimizer with \(\beta=(0.9,0.98)\) with a batch size \(64\). We use a linear learning rate warm-up (for the first \(500\) steps) followed by cosine annealing. In the second stage, we use supervised finetuning with our model on the GenQA [12], InfinityInstruct [3] and OpenHermes 2.5 [70] datasets using SFT in one epoch, with the same hyperparameters as Zephyr [72]. In the final stage, for models distilled from Zephyr, we do distilled alignment with our model using DPO on the UltraFeedback [17] dataset which is consistent with teacher model. While models distilled from Llama-3 instructed 8B, we use datasets from SimPO [51] and Zephyr [72]. We only freeze Gated MLP (FFN) in the first stage, while in the second and final stage all parameters are trained 3. The total distillation process for each hybrid model (e.g., Mamba-Llama3 (50% attl)) takes less than five days in 8x80G A100.

Footnote 3: We freeze the MLP layers in the first stage because we want to produce a model similar to the initialization model. However, in the end-to-end distillation, we only focus on the KL loss, so training all parameters (not freezing the MLP layers) will give better results.

**Baselines.** In addition to the core Transformer architectures, the main baselines we compare against are other large-scale linear RNN models. We compare with both pure SSM architectures, such as TRI Mamba 7B [52] trained with 1.2T tokens and Falcon Mamba 7B4 trained with more than ST tokens, hybrid SSM architectures, such as Nvidia Hybrid Mamba 2 [74] trained with 3.7T tokens, and other linear hybrid RNN models, such as Recurrent Gemma-9B Instruct [8, 19].

Footnote 4: https://huggingface.co/tiuae/falcon-mamba-7b

After the release of the new SoTA transformer models at the 8B and 3B scales, Llama-3.1 and Llama-3.2, we have streamlined the distillation process and are now distilling using the larger Llama-3.1 70B teacher model while initializing models with similarly sized 3B and 8B scales, respectively. We distill our model on the GenQA [12] and InfinityInstruct [3] datasets, resulting in Mamba-Llama3.2-3B, Mamba2-Llama3.2-3B, Mamba-Llama3.1-8B, and Mamba2-Llama3.1-8B. Additionally, we perform further DPO on top of these models using the same dataset as before, resulting in Mamba-Llama3.2-3B-dpo, Mamba2-Llama3.2-3B-dpo, Mamba-Llama3.1-8B-dpo, and Mamba2-Llama3.1-8B-dpo. The distillation phase takes eight days on 8xA100 and four days on 8xH100.

### Evaluation on Chat Benchmarks

We evaluate our models using both single-turn, AlpacaEval [43] and multi-turn chat benchmarks, MT-Bench [84]. These benchmarks assess the model's ability to follow instructions and respond to challenging prompts across a wide variety of domains.

Table 2 shows the performance of our models on chat benchmarks compared with large transformer models. The distilled hybrid Mamba model (50%) achieves a similar score in the MT-benchmark as the teacher model, and slightly better than the teacher model on the AlpacaEval benchmark in both LC win rate and overall win rate. The distilled hybrid Mamba (25% and 12.5%) performance is slightly worse than that of the teacher models in the MT benchmark but still surpasses some large transformers even with more parameters in AlpacaEval. The distilled pure (0%) model does degrade significantly in accuracy. Notably, the distilled hybrid model performs better than Falcon Mamba, which was trained from scratch with more than 5T tokens.

### Evaluation on General Benchmarks

**Zero Shot Evaluation.** We utilize the open-source LM Evaluation Harness library [25] (branch big-refactor) to assess 10 tasks, with the following evaluation metrics: WinoGrande (WG) accuracy [62], PIQA (PQ) accuracy [7], HellaSwag (HS) normalized accuracy [82], ARC-Easy and ARC-Challenge (AE and AC) accuracy and normalized accuracy, [15], MMLU (MM), accuracy [33], OpenBookQA (OB) normalized accuracy [54], TruthFuIQA (TQ) accuracy [46], PubMedQA (PM) accuracy [38], and RACE (RA), accuracy [40]. Each task is evaluated by analyzing the probability assigned by the model to each potential answer choice.

Table 3 shows zero shot evaluation in LM Eval benchmark for Mamba and Mamba2 distilled from different teacher models. Both hybrid Mamba-Llama3 and Mamba2-Llama3 models, distilled from the Llama-3 Instruct 8B, perform better compared to the open-source TRI Mamba and Nvidia Mamba models trained from scratch. Performance degrades with more linear RNN layers, but is still competitive at 25% to models trained from scratch.

### Hybrid speculative decoding

**Setup** We perform speculative decoding using the distilled hybrid models. We run experiments using both Hybrid Mamba \(50\%\) and Hybrid Mamba \(25\%\) as main models. For the draft models, we train 2 and 4-layer Transformer Draft models on the OpenHermes2.5

\begin{table}
\begin{tabular}{l c c|c c c c c} \hline \hline
**Model (\% Att)** & Size & Align & \begin{tabular}{c} MT-Bench \\ (score) \\ \end{tabular} & \begin{tabular}{c} MT-Bench \\ (Round 1) \\ \end{tabular} & \begin{tabular}{c} MT-Bench \\ (Round 2) \\ \end{tabular} & \begin{tabular}{c} AlpacaEval \\ (LC win \%) \\ \end{tabular} & \begin{tabular}{c} AlpacaEval \\ (win \%) \\ \end{tabular} \\ \hline Zephyr & 7B & DPO & **7.34** & - & - & \begin{tabular}{c} 13.20\({}_{0.96}\) \\ \end{tabular} & 
\begin{tabular}{c} 10.99\({}_{0.96}\) \\ \end{tabular} \\
** Mamba-Zephyr (50\%)** & 7B & DPO & 7.31 & - & - & \begin{tabular}{c} **20.66\({}_{0.74}\)** \\ \end{tabular} & 
\begin{tabular}{c} **16.69\({}_{1.10}\) \\ \end{tabular} \\
** Mamba-Zephyr (25\%)** & 7B & DPO & 6.40 & - & - & \begin{tabular}{c} 17.16\({}_{0.69}\) \\ \end{tabular} & \begin{tabular}{c} 13.11\({}_{1.00}\) \\ \end{tabular} \\ \hline \begin{tabular}{l} Llama-3.1-Instruct \\ **Mambo-Llama3.1 (50\%)** \\ \end{tabular} & 8B & RLHF & **8.0** & - & - & \begin{tabular}{c} **20.9** \\ \end{tabular} & 
\begin{tabular}{c} **21.8** \\ \end{tabular} \\
** Mamba-Llama3.1 (50\%)** & 8B & & 7.7 & 8.0 & 7.3 & \begin{tabular}{c} 18.97\({}_{1.23}\) \\ \end{tabular} & 
\begin{tabular}{c} 21.22\({}_{1.23}\) \\ \end{tabular} \\
** Mamba-Llama3.1 (50\%)** & 8B & & 7.6 & 8.1 & \begin{tabular}{c} 7.0 \\ \end{tabular} & \begin{tabular}{c} 18.99\({}_{1.24}\) \\ \end{tabular} & 
\begin{tabular}{c} 21.55\({}_{1.24}\) \\ \end{tabular} \\
** Mamba-Llama3.2 (50\%)** & 3B & & 6.9 & 7.6 & \begin{tabular}{c} 6.1 \\ \end{tabular} & \begin{tabular}{c} 13.57\({}_{1.05}\) \\ \end{tabular} & 
\begin{tabular}{c} 15.54\({}_{1.08}\) \\ \end{tabular} \\
** Mamba2-Llama3.2 (50\%)** & 3B & & 6.5 & 7.1 & \begin{tabular}{c} 5.8 \\ \end{tabular} & \begin{tabular}{c} 12.61\({}_{1.05}\) \\ \end{tabular} & \begin{tabular}{c} 14.34\({}_{1.05}\) \\ \end{tabular} \\ \hline \begin{tabular}{l} Llama-3-Instruct \\ **Mambo-Llama3 (50\%)** \\ \end{tabular} & 8B & RLIHF & **8.00** & - & - & \begin{tabular}{c} 22.90\({}_{1.26}\) \\ \end{tabular} & 
\begin{tabular}{c} 22.60\({}_{1.26}\) \\ \end{tabular} \\
** Mamba-Llama3 (50\%)** & 8B & DPO & 7.35 & 7.82 & 6.88 & \begin{tabular}{c} **29.61\({}_{1.31}\)** \\ \end{tabular} & 
\begin{tabular}{c} **26.69\({}_{1.31}\) \\ \end{tabular} \\
** Mamba-Llama3 (25\%)** & 8B & DPO & 6.86 & 7.56 & \begin{tabular}{c} 6.15 \\ \end{tabular} & \begin{tabular}{c} 25.55\({}_{1.26}\) \\ \end{tabular} & 
\begin{tabular}{c} 22.50\({}_{2.26}\) \\ \end{tabular} \\
** Mamba-Llama3 (12.5\%)** & 8B & DPO & 6.46 & 6.91 & \begin{tabular}{c} 6.01 \\ \end{tabular} & \begin{tabular}{c} 20.76\({}_{1.16}\) \\ \end{tabular} & \begin{tabular}{c} 17.93\({}_{1.16}\) \\ \end{tabular} \\ \hline \begin{tabular}{l} **Mambo-2-Llama3 (50\%)** \\ \end{tabular} & 8B & DPO & 7.32 & 7.93 & 6.70 & \begin{tabular}{c} 26.78\({}_{1.26}\) \\ \end{tabular} & 
\begin{tabular}{c} 22.69\({}_{1.26}\) \\ \end{tabular} \\
** Mamba-Llama3 (25\%)** & 8B & DPO & 6.74 & 7.24 & \begin{tabular}{c} 6.24 \\ \end{tabular} & \begin{tabular}{c} 22.75\({}_{1.18}\) \\ \end{tabular} & 
\begin{tabular}{c} 19.01\({}_{1.18}\) \\ \end{tabular} \\
** Mamba2-Llama3 (12.5\%)** & 8B & DPO & 6.48 & 6.83 & \begin{tabular}{c} 6.13 \\ \end{tabular} & \begin{tabular}{c} 20.25\({}_{1.13}\) \\ \end{tabular} & 
\begin{tabular}{c} 16.88\({}_{1.13}\) \\ \end{tabular} \\
** Mamba2-Llama3 (0\%)** & 8B & DPO & 5.64 & 6.16 & \begin{tabular}{c} 5.11 \\ \end{tabular} & \begin{tabular}{c} 14.49\({}_{0.93}\) \\ \end{tabular} & \begin{tabular}{c} 10.88\({}_{0.93}\) \\ \end{tabular} \\ \hline \begin{tabular}{l} Falcon Mamba Instruct \\ \end{tabular} & 7B & SFT & 6.40 & 7.25 & 5.55 & \begin{tabular}{c} 4.04\({}_{0.45}\) \\ \end{tabular} & \begin{tabular}{c} 21.5\({}_{0.45}\) \\ \end{tabular} \\ \hline \begin{tabular}{l} GPT-3.5-turbo \\ \end{tabular} & - & RLHF & 7.94 & - & - & \begin{tabular}{c} 22.70 \\ \end{tabular} & \begin{tabular}{c} 14.10 \\ \end{tabular} \\ \hline \begin{tabular}{l} GPT-4.0 \\ \end{tabular} & - & RLHF & - & - & \begin{tabular}{c} **57.46\({}_{1.47}\)** \\ \end{tabular} & 
\begin{tabular}{c} **51.33\({}_{1.47}\) \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 2: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. MT-Bench scores model responses using GPT-4. AlpacaEval version two measures the win-loss rate between baseline models and GPT-4 scored by GPT-4 Turbo.

dataset [70], for approximately 3 full epochs, following the "shrink and fine-tune" approach from [66]. Specifically, we initialize the draft layers using layers from the Zephyr-7B model (we take layers at indices \([0,31]\) for the 2-layer model and \([0,10,20,31]\) for the 4-layer model), and the embeddings and language model head also from the Zephyr-7B model [72]. We perform loss masking on the prompt, thus only considering next token prediction loss (cross-entropy) on the chat continuations from the training set. Speculative decoding experiments are run on a single NVIDIA RTX 3090 on data from OpenHermes2.5.

\begin{table}
\begin{tabular}{l l l c c} \hline \hline Draft Model & K & Target Model (\% Att) & \# Gen. Tokens & Speedup \\ \hline \multirow{2}{*}{2 layers} & 4 & Mamboa-Zephyr (\(50\%\)) & 2.48 & 1.8x \\  & 4 & Mamboa-Zephyr (\(25\%\)) & 2.64 & 1.88x \\ \hline \multirow{2}{*}{4 layers} & 4 & Mamboa-Zephyr (\(50\%\)) & 3 & 1.81x \\  & 4 & Mamboa-Zephyr (\(25\%\)) & 3 & 1.8x \\ \hline \multirow{2}{*}{4 layers} & 3 & Mamboa-Llama3 (\(50\%\)) & 2.7 & 1.6x \\  & 4 & Mamboa-Llama3 (\(50\%\)) & 3.6 & 1.58x \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance metrics for different draft and target model configurations for \(K=4\) on data from OpenHermes2.5. # _Gen_ is the average number of generated tokens per speculative decoding step and includes an additional token from the last verifier logits.

**Results** Table 4 shows results for hybrid speculative decoding with, using both the Zephyr and Llama hybrid models with different configurations. For both the \(50\%\) and \(25\%\) distilled models, we achieve speedups of over \(1.8\)x on the Zephyr-Hybrid compared to the non-speculative baseline. We also show that the 4-layer draft model we trained achieves a higher acceptance rate, but it adds some additional overhead due to the increased draft model size. For the Llama-hybrid models, the speedups are more modest since the draft model is larger due to the large embedding table of Llama 3. In subsequent work, we will focus on making these draft models smaller.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline Model (\% Att) & WG & PI & HS & AE & AC & MM & OB & PQ & PM & RA & AVG \\ \hline TRI Mamboa-7B & 71.42 & 81.01 & 77.93 & 77.53 & 46.67 & 33.39 & **46.20** & 32.09 & 72.30 & 37.99 & 57.65 \\ Nvidia Hybrid Mamboa-8B & 71.72 & 79.65 & 77.68 & 77.23 & 47.70 & 51.46 & 42.80 & 38.72 & 69.80 & 39.71 & 59.60 \\ \hline Llama-31-8Birstirst & 73.38 & 88.07 & **79.21** & 87.28 & 55.02 & **68.12** & 43.20 & 42.67 & **75.20** & 44.78 & 64.48 \\
**Llama3.1-Mamboa (50\%)** & 72.77 & 79.33 & 75.91 & 82.24 & 53.84 & 62.13 & 42.80 & 40.02 & 72.00 & 42.11 & 62.32 \\
**Llama3.1-Mamboa-DPO (50\%)** & 73.80 & 80.41 & 77.36 & 84.01 & 56.57 & 63.30 & **44.20** & 46.07 & 74.40 & 43.44 & 64.38 \\
**Llama3.1-Mamboa-20 (50\%)** & 71.74 & 78.89 & 75.36 & 82.02 & 52.65 & 61.01 & 41.60 & 40.31 & 72.60 & 42.11 & 61.85 \\
**Llama3.1-Mamboa-20PO (50\%)** & **74.11** & 80.03 & **79.69** & **84.11** & **59.73** & 59.74 & 44.00 & **50.22** & 74.60 & **46.12** & **65.31** \\ \hline Llama-32-32-Instruct & **67.48** & 75.68 & 70.43 & 74.07 & 45.90 & **64.33** & 36.00 & 38.01 & 69.60 & 40.67 & 57.83 \\
**Llama3.2-Mamboa (50\%)** & 67.32 & 77.31 & 70.37 & 77.65 & 48.38 & 54.48 & 39.40 & 42.02 & 66.40 & 42.98 & 53.66 \\
**Llama3.2-Mamboa-DPO (50\%)** & 67.40 & 77.31 & 72.56 & 79.97 & 52.65 & 55.09 & 41.60 & 48.53 & **70.00** & **43.64** & **60.88** \\
**Llama3.2-Mamboa-20 (50\%)** & 66.06 & 76.11 & 69.13 & 76.68 & 46.67 & 53.12 & 38.80 & 34.78 & 63.00 & 39.81 & 56.49 \\
**Llama3.2-Mamboa-20 (50\%)** & 67.32 & 77.67 & **74.58** & **62.60** & 54.10 & 52.47 & **42.40** & **50.28** & 65.40 & 43.44 & 60.78 \\ \hline
**Mamboa-Zephyr (50\%)** & 68.82 & 80.36 & 76.91 & 81.40 & 55.63 & 55.43 & 42.60 & 41.99 & 72.60 & 42.20 & 61.79 \\ \hline
**Mamboa-Llama3 (50\%)** & 68.98 & 78.02 & 78.43 & 74.51 & 51.96 & **57.81** & 44.00 & 47.69 & 73.00 & 38.56 & 61.30 \\
**Mamboa-Llama3 (25\%)** & 62.83 & 70.70 & 75.00 & 74.28 & 47.35 & 53.50 & 40.00 & 43.64 & 65.40 & 36.94 & 57.70 \\
**Mamboa-Llama3 (12.5\%)** & 59.75 & 75.08 & 71.71 & 70.58 & 43.60 & 49.81 & 41.40 & 41.41 & 62.40 & 34.45 & 55.02 \\ \hline
**Mamboa-Llama3 (50\%)** & 71.51 & **81.45** & **79.47** & **78.83** & **81.99** & 55.70 & 44.20 & **57.74** & **72.4** & **38.85** & **63.84** \\
**Mamboa-Llama3 (25\%)** & 64.80 & 78.73 & 77.76 & 73.53 & **52.47** & 53.71 & 42.40 & 55.53 & 64.80 & 39.23 & 60.55 \\
**Mamboa-Llama3 (12.5\%)** & 63.38 & 76.82 & 73.14 & 75.84 & 50.26 & 50.78 & 39.60 & 50.00 & 65.80 & 36.46 & 58.21 \\
**Mamboa2-Llama3 (0\%)** & 58.56 & 76.82 & 70.75 & 74.12 & 47.95 & 45.19 & 39.00 & 40.20 & 62.20 & 32.63 & 54.74 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Evaluation on LM Eval benchmark for Mamboa and Mamboa distilled from Llama-3 Instruct 8B.

## Acknowledgement

We thank Together AI for providing compute for some of the experiments. This work has benefited from helpful discussions with Albert Gu at CMU, Francois Fleuret and Vincent Micheli at the University of Geneva, Albert Tseng and Wen-Ding Li at Cornell University.

## References

* [1] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Re. Zoology: Measuring and improving recall in efficient language models. _arXiv preprint arXiv:2312.04927_, 2023.
* [2] S. Arora, S. Eyuboglu, M. Zhang, A. Timalsina, S. Alberti, D. Zinsley, J. Zou, A. Rudra, and C. Re. Simple linear attention language models balance the recall-throughput tradeoff. _arXiv preprint arXiv:2402.18668_, 2024.
* [3] BAAI. Infinity instruct. _arXiv preprint arXiv:2406.XXXX_, 2024.
* [4] M. Beck, K. Poppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory. _arXiv preprint arXiv:2405.04517_, 2024.
* [5] N. Bhendawade, I. Belousova, Q. Fu, H. Mason, M. Rastegari, and M. Najibi. Speculative streaming: Fast llm inference without auxiliary models. _arXiv preprint arXiv:2402.11131_, 2024.
* [6] A. Bick, K. Y. Li, E. P. Xing, J. Z. Kolter, and A. Gu. Transformers to ssms: Distilling quadratic knowledge to subquadratic models. _arXiv preprint arXiv:2408.10189_, 2024.
* [7] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about Physical Commonsense in Natural Language. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 7432-7439, 2020.
* [8] A. Botev, S. De, S. L. Smith, A. Fernando, G.-C. Muraru, R. Haroun, L. Berrada, R. Pascanu, P. G. Sessa, R. Dadashi, et al. Recurrentgemma: Moving past transformers for efficient open language models. _arXiv preprint arXiv:2404.07839_, 2024.
* [9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [10] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. _arXiv preprint arXiv:2401.10774_, 2024.
* [11] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper. Accelerating Large Language Model Decoding with Speculative Sampling, 2023.
* [12] J. Chen, R. Qadri, Y. Wen, N. Jain, J. Kirchenbauer, T. Zhou, and T. Goldstein. Genqa: Generating millions of instructions from a handful of prompts. _arXiv preprint arXiv:2406.10323_, 2024.
* [13] Z. Chen, X. Yang, J. Lin, C. Sun, J. Huang, and K. C.-C. Chang. Cascade speculative drafting for even faster llm inference. _arXiv preprint arXiv:2312.11462_, 2023.
* [14] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* [15] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. _arXiv preprint arXiv:1803.05457_, 2018.

[MISSING_PAGE_FAIL:12]

* [33] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring Massive Multitask Language Understanding. In _International Conference on Learning Representations_, 2020.
* [34] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* [35] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [36] K. Irie, I. Schlag, R. Csordas, and J. Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. _Advances in neural information processing systems_, 34:7703-7717, 2021.
* [37] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [38] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu. PubMedQA: A Dataset for Biomedical Research Question Answering. _arXiv preprint arXiv:1909.06146_, 2019.
* [39] Y. Kim and A. M. Rush. Sequence-level knowledge distillation. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 1317-1327, 2016.
* [40] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding Comprehension Dataset From Examinations. _arXiv preprint arXiv:1704.04683_, 2017.
* [41] Y. Leviathan, M. Kalman, and Y. Matias. Fast Inference from Transformers via Speculative Decoding. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 19274-19286. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/leviathan23a.html.
* [42] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. Starcode: may the source be with you! _arXiv preprint arXiv:2305.06161_, 2023.
* [43] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.
* [44] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language model. _arXiv preprint arXiv:2403.19887_, 2024.
* [45] B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/yuchenlin/ZeroEval.
* [46] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. _arXiv preprint arXiv:2109.07958_, 2021.
* [47] S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, 2022.
* [48] X. Liu, L. Hu, P. Bailis, I. Stoica, Z. Deng, A. Cheung, and H. Zhang. Online speculative decoding. _arXiv preprint arXiv:2310.07177_, 2023.
* [49] S. Massaroli, M. Poli, D. Fu, H. Kumbong, R. Parnichkun, D. Romero, A. Timalsina, Q. McIntyre, B. Chen, A. Rudra, et al. Laughing hyena distillery: Extracting compact recurrences from convolutions. _Advances in Neural Information Processing Systems_, 36, 2024.

* Mehta et al. [2023] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long Range Language Modeling via Gated State Spaces. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=5MKYYCbva.
* Meng et al. [2024] Y. Meng, M. Xia, and D. Chen. Simpo: Simple preference optimization with a reference-free reward. _arXiv preprint arXiv:2405.14734_, 2024.
* Mercat et al. [2024] J. Mercat, I. Vasiljevic, S. Keh, K. Arora, A. Dave, A. Gaidon, and T. Kollar. Linearizing large language models. _arXiv preprint arXiv:2405.06640_, 2024.
* Merity et al. [2016] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_, 2016.
* Mihaylov et al. [2018] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. _arXiv preprint arXiv:1809.02789_, 2018.
* Peng et al. [2023] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV, et al. Rwkv: Reinventing rnns for the transformer era. _arXiv preprint arXiv:2305.13048_, 2023.
* Peng et al. [2023] B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarm: Efficient context window extension of large language models. _arXiv preprint arXiv:2309.00071_, 2023.
* Poli et al. [2023] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re. Hyena hierarchy: Towards larger convolutional language models. In _International Conference on Machine Learning_, pages 28043-28078. PMLR, 2023.
* Rafailov et al. [2024] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ralambomihanta et al. [2024] T. R. Ralambomihanta, S. Mohammadzadeh, M. S. N. Islam, W. Jabbour, and L. Liang. Scavenging hyena: Distilling transformers into long convolution models. _arXiv preprint arXiv:2401.17574_, 2024.
* Ren et al. [2024] L. Ren, Y. Liu, Y. Lu, Y. Shen, C. Liang, and W. Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. _arXiv preprint arXiv:2406.07522_, 2024.
* Roziere et al. [2023] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* Sakaguchi et al. [2021] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
* Schlag et al. [2021] I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In _International Conference on Machine Learning_, pages 9355-9366. PMLR, 2021.
* Schulman et al. [2017] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Shaham et al. [2022] U. Shaham, E. Segal, M. Ivgi, A. Efrat, O. Yoran, A. Haviv, A. Gupta, W. Xiong, M. Geva, J. Berant, et al. Scrolls: Standardized comparison over long language sequences. _arXiv preprint arXiv:2201.03533_, 2022.
* Shleifer and Rush [2020] S. Shleifer and A. M. Rush. Pre-trained summarization distillation. _CoRR_, abs/2010.13002, 2020. URL https://arxiv.org/abs/2010.13002.
* Spector and Re [2023] B. Spector and C. Re. Accelerating llm inference with staged speculative decoding. _arXiv preprint arXiv:2308.04623_, 2023.

* [68] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models. _arXiv preprint arXiv:2307.08621_, 2023.
* [69] Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efficient transformers. In _International Conference on Learning Representations_, 2020.
* [70] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.
* [71] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [72] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib, et al. Zephyr: Direct distillation of lm alignment. _arXiv preprint arXiv:2310.16944_, 2023.
* [73] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [74] R. Waleffe, W. Byeon, D. Riach, B. Norick, V. Korthikanti, T. Dao, A. Gu, A. Hatamizadeh, S. Singh, D. Narayanan, et al. An empirical study of mamba-based language models. _arXiv preprint arXiv:2406.07887_, 2024.
* [75] J. Wang, J. N. Yan, A. Gu, and A. M. Rush. Pretraining without attention. _arXiv preprint arXiv:2212.10544_, 2022.
* [76] H. Xia, T. Ge, P. Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 3909-3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.
* [77] J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press. Swe-agent: Agent computer interfaces enable software engineering language models, 2024.
* [78] N. Yang, T. Ge, L. Wang, B. Jiao, D. Jiang, L. Yang, R. Majumder, and F. Wei. Inference with reference: Lossless acceleration of large language models. _arXiv preprint arXiv:2304.04487_, 2023.
* [79] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardware-efficient training. _arXiv preprint arXiv:2312.06635_, 2023.
* [80] S. Yang, B. Wang, Y. Zhang, Y. Shen, and Y. Kim. Parallelizing linear transformers with the delta rule over sequence length. _arXiv preprint arXiv:2406.06484_, 2024.
* [81] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* [82] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_, 2019.
* [83] M. Zhang, K. Bhatia, H. Kumbong, and C. Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. _arXiv preprint arXiv:2402.04347_, 2024.
* [84] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Evaluation on Long Context Tasks

Figure 4 illustrates the results of Needle in a Haystack. Although the distillation length is only 2k, our distilled 3B models (Mamba-Llama3.2-3B (50%) and Mamba2-Llama3.2-3B (50%)) achieve perfect accuracy up to 10k, which is better than Llama-3.2-3B-Instruct. Similarly, the distilled 8B models (Mamba-Llama3.1-8B (50%) and Mamba2-Llama3.1-8B (50%)) achieve perfect accuracy up to 16k, with Mamba-Llama3.1-8B demonstrating good results up to 38k.

## Appendix B Benchmark Evaluation

We also report few-shot evaluations on OpenLLMLeaderboard by conducting 25 shots on ARC-Challenge [14], 10 shots on HellaSwag [82], 5 shots on MMLU [34], and 5 shots on Winogrande [62]. For TruthFulQA, the mc2 metric is reported in this benchmark. For GSM8K [16], we follow the evaluation for instruct tuned model [51], which uses ZeroEval [45], a benchmark designed for chat models. We also include the CRUX [29] from that benchmark, which is designed for evaluating reasoning on code. All models are evaluated with greedy decoding in the ZeroEval.

Table 5 shows that the performance of our distilled hybrid models matches that of the best open-source linear RNN models on the Open LLM Leaderboard, while outperforming their corresponding open-source instruct models in GSM8K and CRUX.

## Appendix C Analysis

Comparison with other distillation approachesTable 6 (left) compares the perplexity of different model variants. We distill using Ultrachat as seed prompt [20] in one epoch and compare the perplexity. We find that removing more layers gets significantly worse. We also compare our distillation approach with a previous baseline. This approach distills a Transformer model into a Hyena model [57], as proposed in [59]. They use a different distillation approach using progressive knowledge transfer, wherein the student model is trained starting from the first layer and progressively extending to subsequent layers. While

Figure 4: Needle in a Haystack evaluation. Green squares represent a high retrieval success rate, while the white dashed line marks the longest examples encountered during distillation training. The Y-axis indicates the distance to the retrieved target.

it is challenging to compare, our distill shows a smaller degradation (1.03 for 50 % attention, 1.09 for 25 % attention, 1.22 for 6.35% attention, and 3.36 for no attention), while the Distill Hyena model is trained in WikiText [53] dataset with a much smaller model and shows large perplexity degrade.

Does distilling from preferences help?In Table 6 (Right), we show the impact of different steps in the alignment process of the distillation. We observe that SFT or DPO alone does not yield much improvement, while SFT + DPO yields the best score. Models are trained using Zephyr as the teacher model and the OpenHermes 2.5 [70] dataset as the SFT dataset, and UltraFeedback [17] as the DPO dataset.

Pseudo Label Distillation Ablations.We consider several different model ablation studies in Table 7. For these experiments we consider training for 5k steps using the pseudo-label approaches on the Ultrachat [20] dataset. Table 7 (Left) presents the results of distillation with various initializations. According to this table, initializing weights from a transformer model, we can see that the proposed method is able to predict the best score.

\begin{table}
\begin{tabular}{l c c} \hline \hline Model (\% Att) & PPL & Ratio \\ \hline Teacher: Zephyr (7B) & 2.02 & 1 \\ Mamba-Zephyr (50\%) & 2.09 & 1.03 \\ Mamba-Zephyr (25\%) & 2.20 & 1.09 \\ Mamba-Zephyr (6.25\%) & 2.46 & 1.22 \\ Mamba-Zephyr (0\%) & 3.36 & 1.66 \\ \hline \hline Teacher: Pythia (70M) & 51.4 & 1 \\ Distill Hyena & **121.2** & 2.36 \\ \hline \hline \end{tabular} \begin{tabular}{l c} \hline \hline Model & Hyb Mamba (50\% Att) & Hyb Mamba (25\% Att) \\ \hline Dis & 5.55 & 5.01 \\ Dis+SFT & 5.61 & 4.97 \\ Dis+DPO & 5.42 & 4.84 \\ Dis+SFT+DPO & **6.69** & **6.10** \\ \hline \hline \end{tabular} 
\begin{tabular}{l c} \hline \hline Model & Hyb Mamba (50\% Att) \\ \hline Dis & 5.55 & 5.01 \\ Dis+SFT & 5.61 & 4.97 \\ Dis+DPO & 5.42 & 4.84 \\ Dis+SFT+DPO & **6.69** & **6.10** \\ \hline \hline \end{tabular}
\end{table}
Table 6: (Left) Perplexity comparison between our distillation approach and [59]. (Right) Ablation study of different alignment methods of the Distilled Hybrid Mamba on the MT-benchmark using OpenHermes 2.5 as the SFT dataset.

\begin{table}
\begin{tabular}{l|c c c c|c c} \hline \hline Model & \begin{tabular}{c} Mamba \\ (0\% Att) \\ \end{tabular} & \begin{tabular}{c} Hyb Mamba \\ (50\% Att) \\ \end{tabular} \\  & \begin{tabular}{c} Proz \\ \end{tabular} & \begin{tabular}{c} -Froz \\ \end{tabular} & \begin{tabular}{c} Proz \\ \end{tabular} & \begin{tabular}{c} -Froz \\ \end{tabular} \\ \hline + Attention-Init & **3.36** & 66.7 & **2.09** & 9.1 \\ -Attention-Init & 18.2 & 20.3 & 7.4 & 11.2 \\ \hline \hline \end{tabular} \begin{tabular}{l|c c c} \hline \hline Model & \begin{tabular}{c} Hyb Mamba \\ (25\% Att) \\ \end{tabular} & \begin{tabular}{c} Hyb Mamba \\ (50\% Att) \\ \end{tabular} \\  & \begin{tabular}{c} Proz \\ \end{tabular} & \begin{tabular}{c} Proz \\ \end{tabular} & \begin{tabular}{c} Proz \\ \end{tabular} & \begin{tabular}{c} Proz \\ \end{tabular} & 
\begin{tabular}{c} Proz \\ \end{tabular} \\ \hline + Attention-Init & **3.36** & 66.7 & **2.09** & 9.1 \\ -Attention-Init & 18.2 & 20.3 & 7.4 & 11.2 \\ \hline \hline \end{tabular}
\end{table}
Table 7: (Left) Perplexity comparison with different initialization at first stage. (Right) Perplexity comparison with different Mamba interleaving layers and stepwise distillation at first stage.

\begin{table}
\begin{tabular}{l|c c c|c c|c c} \hline \hline Model (\% Att) & ARC & HS & MMLU & WG & TQ & GSM8K & CRUX \\ \hline Falcon Mamba-7B & 62.03 & 80.82 & 62.11 & 73.64 & 53.42 & 41.32 & 8.88 \\ RecurrentGemma-9B & 52.00 & 80.40 & 60.50 & 73.60 & 38.60 & 38.51 & 26.25 \\ \hline \hline
**Mamba-Llam3 (50\%)** & 56.57 & 78.99 & 59.26 & 69.06 & 58.85 & 67.85 & 27.88 \\
**Mamba-Llama3 (25\%)** & 55.03 & 75.66 & 52.68 & 62.83 & 55.03 & 40.64 & 15.62 \\
**Mamba-Llama3 (12.5\%)** & 52.90 & 72.46 & 49.20 & 59.19 & 53.00 & 26.91 & 11.25 \\ \hline
**Mamba2-Llama3 (50\%)** & 60.41 & 77.97 & 56.67 & 71.35 & 66.60 & 59.36 & 24.88 \\
**Mamba2-Llama3 (25\%)** & 59.22 & 76.88 & 53.94 & 64.88 & 64.64 & 38.13 & 13.25 \\
**Mamba2-Llama3 (12.5\%)** & 53.33 & 72.16 & 50.85 & 63.61 & 61.12 & 35.03 & 10.25 \\
**Mamba2-Llama3 (0\%)** & 53.51 & 70.31 & 44.21 & 58.91 & 52.31 & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results on the Open LLM Leaderboard and ZeroEval Leaderboard. For GSM8K and CRUX, we chose the zero-shot evaluation using ZeroEval, which is designed for evaluating instruct models. We evaluated the corresponding instruct-tuned models for Falcon Mamba-7b and RecurrentGemma-9B, specifically Falcon Mamba-7b-instruct and RecurrentGemma-9B-it.

is crucial for performance. Without weight initialization from a transformer, perplexity significantly worsens for both pure Mambo models and hybrid models. Also, freezing MLP layers can help the student model focus on learning the interaction of tokens and better mimic attention layers. Table 7 (Right) shows also see smaller benefits from progressive distillation and interleaving the attention layers with Mambo.

Attention Initialization.We compare the default random initialization of Mambo with reusing the linear projection from the attention using the same recipe. Both models are trained using Zephyr as the teacher model and the OpenHermes 2.5 [70] dataset as the SFT dataset, and UltraFeedback [17] as the DPO dataset.

Table 8 compares the performance of the hybrid model using two different initialization methods: default random initialization and reusing the linear projection from the attention. The model performs significantly better with reusing the linear projection from the attention compared to random initialization, across all evaluated benchmarks. This result confirms that initialization from attention weights is critical.

Necessity of Linear RNN.We train a model that removes Mambo blocks from the model entirely using the same recipe to see if the model can adapt. Both models are trained using Zephyr as the teacher model, with the OpenHermes 2.5 [70] dataset as the SFT dataset and UltraFeedback [17] as the DPO dataset. Table 9 compares the performance of the model with and without Mambo blocks. The model with Mambo performs significantly better than the one without it. This confirms that adding Mambo layers is critical and that the improved performance is not solely attributable to the remaining attention mechanism.

## Appendix D Related Work

Attention-free models.Attention-free models offer improved computational and memory efficiency, making them increasingly popular for various language processing tasks, including autoregressive language modeling. Models like S4 [27] and its subsequent variants [31, 28] have shown promising results in long-range synthetic tasks [69]. Gated SSM architectures, such as GSS [50] and BiGS [75], incorporate a gating mechanism into SSMs for (bidirectional) language modeling. The recently introduced Mambo model [26] argues that the static dynamics of these methods fail to incorporate input-specific context selection within the hidden state, which could be crucial for tasks like language modeling. Mambo has been shown to outperform Transformers across different model sizes and scales. Additionally, several other sub-quadratic model architectures [57, 79, 19, 1, 2, 22, 4, 80] and hybrid architectures [23, 44] have also been proposed.

**Distillation from Transformers.** There have been relatively few attempts to distill on to linear RNN style models. Laughing Hyena [49] proposes to distill the long convolution into a state space representation, enabling constant time inference in Hyena [57]. Ralambomihanta et al. [59] introduces a progressive knowledge approach to distill small transformer models (70M) into Hyena models. [6]

**Speculative Decoding.** Speculative decoding [67, 41, 11, 76, 10] has recently emerged as a promising method to accelerate the inference process of large language models, particularly

\begin{table}
\begin{tabular}{l|c c c c c c} \hline Model & \begin{tabular}{c} LAMBADA \\ (ppl) \\ \end{tabular} & MMLU & ARC-C & TruthfulQA & HellaSwag & \begin{tabular}{c} MT-Bench \\ (score) \\ \end{tabular} & 
\begin{tabular}{c} AlApacaEval \\ (LC win \%) \\ \end{tabular} \\ \hline + Attention init & 6.20 & **47.98** & **49.15** & **46.67** & **75.07** & **6.69** & **14.11** \\ - Attention init & 55.01 & 26.21 & 25.26 & 34.01 & 27.91 & 1.04 & 0.02 \\ \hline \end{tabular}
\end{table}
Table 8: Performance of Zephyr-Mambo (50% attention) with different initialization.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline Model & \begin{tabular}{c} LAMBADA \\ (ppl) \\ \end{tabular} & MMLU & ARC-C & TruthfulQA & HellaSwag & \begin{tabular}{c} MT-Bench \\ (score) \\ \end{tabular} & 
\begin{tabular}{c} AlApacaEval \\ (LC win \%) \\ \end{tabular} \\ \hline
50\% **Att w Mambo** & 6.20 & **47.98** & **49.15** & **46.67** & **75.07** & **6.69** & **14.11** \\
50\% Att w/o Mambo & 151.98 & 24.46 & 21.93 & 32.39 & 27.91 & 1.01 & 0 \\ \hline \end{tabular}
\end{table}
Table 9: Performance of Hybrid-Mambo with different initialization.

Transformers. This approach utilizes a smaller draft model to speculatively generate candidate tokens, which the larger target model then verifies. Leviathan et al. [41], Chen et al. [11] proposed a rejection sampling scheme to improve inference quality, while Spector and Re [67] organized candidate tokens into a tree structure to enable more efficient verification. Subsequent work has examined both trained draft models [5, 13, 48] and training-free draft models [32, 78, 24].

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[NA]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist",**
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and introduction are supported by our theoretical analysis in section 2 and experimental results in section 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper.

* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation of this paper in Section **??** Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We show all the theorems and formulas in Section 2. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems.

* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We will release our model and code. Everything will be open-sourced. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will definitely release our model and code. Everything will be open-sourced. we will discuss the detailed steps and resources required for reproduction in Section 5.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We report the training details, include dataset, hyper-parameters and optimizer and etc in section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We present the error bars in the evaluation benchmarks in Tables 2 and 5, following the standard for these benchmarks. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report the number of GPUs and the training time for GPUs in Section 5. Given the extensive GPU time required for LLM training, we use only a very limited amount, which is conducive to reproduction. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and respect the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss Broader Impacts in section 7. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We discuss risk of pretrained language models in section 7. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the original paper that produced the code package or dataset. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We communicate the details of the dataset/code/model as part of their submissions via structured templates. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.