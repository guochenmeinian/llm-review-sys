# Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors

 Pengchong Hu  Zhizhong Han

Machine Perception Lab, Wayne State University, Detroit, USA

pchu@wayne.edu h312h@wayne.edu

###### Abstract

Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Please see our project page for code and data at https://machineperceptionlab.github.io/Attentive_DF_Prior/.

## 1 Introduction

3D reconstruction from multi-view images has been studied for decades. Traditional methods like Structure from Motion (SfM) [61], Multi-View Stereo (MVS) [62], and Simultaneous Localization and Mapping (SLAM) [49] estimate 3D structures as point clouds by maximizing multi-view color consistency. Current methods [84, 28] mainly adopt data-driven strategies to learn depth estimation priors from large scale benchmarks using deep learning models. However, the reconstructed 3D point clouds lack geometry details, due to their discrete representation essence, which makes them not friendly to downstream applications.

More recent methods use implicit functions such as signed distance functions (SDFs) [77] or occupancy functions [51] as continuous representations of 3D shapes and scenes. Using volume rendering, we can learn neural implicit functions by comparing their 2D renderings with multi-view ground truth including color [88, 97], depth [88, 3, 97] or normal [88, 76, 16] maps. Although the supervision of using depth images as rendering target can provide detailed structure information and guide importance sampling along rays [88], both the missing depth at holes and the unawareness of occluded structures make it hard to significantly improve the reconstruction accuracy. Hence, how to moreeffectively leverage depth supervision for geometry inference through volume rendering is still a challenge.

To overcome this challenge, we introduce to learn neural implicit through volume rendering with an attentive depth fusion prior. Our key idea is to provide neural networks the flexibility of choosing geometric clues, i.e., the geometry that has been learned and the Truncated Signed Distance Function (TSDF) fused from all available depth images, and combining them into neural implicit for volume rendering. We regard the TSDF as a prior sense of the scene, and enable neural networks to directly use it as a more accurate representation. The TSDF enables accessing the missing depth at holes on one depth image and the occluded structures that are invisible from the current view, which remedies the demerits of using depth ground truth to supervise rendering. To achieve this, we introduce an attention mechanism to allow neural networks to balance the contributions of currently learned geometry and the TSDF in the neural implicit, which leads the TSDF into an attentive depth fusion prior. Our method works with either known camera poses or camera tracking in the context of SLAM, where our prior could be either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene. We evaluate our performance on benchmarks containing synthetic and real-world scans, and report our superiority over the latest methods with known or estimated camera poses. Our contributions are listed below.

1. We present a novel volume rendering framework to learn neural implicit representations from RGBD images. We enable neural networks to use either currently learned geometry or the one from depth fusion in volume rendering, which leads to a novel attentive depth fusion prior for learning neural implicit functions inheriting the merits of both the TSDF and the inference.
2. We introduce a novel attention mechanism and a neural network architecture to learn attention weights for the attentive depth fusion prior in neural rendering with either known camera poses or camera pose tracking in SLAM.
3. We report the state-of-the-art performance in surface reconstruction and camera tracking on benchmarks containing synthetic and real-world scans.

## 2 Related Work

Learning 3D implicit functions using neural networks has made huge progress [61; 62; 46; 51; 88; 78; 73; 76; 16; 56; 34; 25; 11; 93; 42; 44; 9; 43; 4; 33; 5; 19; 26; 47; 20; 70; 12; 48; 55; 17; 27; 79; 35; 32; 21; 7; 58; 86; 52]. We can learn neural implicit functions from 3D ground truth [23; 8; 54; 45; 68; 39; 69], 3D point clouds [94; 41; 15; 1; 92; 2; 10] or multi-view images [46; 14; 51; 77; 88; 73; 76; 16]. We briefly review methods with multi-view supervision below.

### Multi-view Stereo

Classic multi-view stereo (MVS) [61; 62] employs multi-view photo consistency to estimate depth maps from multiple RGB images. They rely on matching key points on different views, and are limited by large viewpoint variations and complex illumination. Without color, space carving [30] is also an effective way of reconstructing 3D structure as voxel grids.

Recent methods employ data driven strategy to train neural networks to predict depth maps from either depth supervision [84] or multi-view photo consistency [95].

### Neural Implicit from Multi-view Supervision

Early works leverage various differentiable renderers [63; 38; 26; 89; 37; 80; 50; 36; 67] to render the learned implicit functions into images, so that we can measure the error between rendered images and ground truth images. These methods usually require masks to highlight the object, and use surface rendering to infer the geometry, which limits their applications in scenes. Similarly, DVR [50] and IDR [87] predict the radiance near surfaces.

With volume rendering, NeRF [46] and its variations [53; 47; 57; 60; 97; 3; 75; 6; 98; 67] model geometry and color together. They can generate plausible images from novel viewpoints, and do not need masks during rendering procedure. UNISURF [51] and NeuS [77] learn occupancy functions and SDFs by rendering them with colors using revised rendering equations. Following methodsimprove accuracy of implicit functions using additional priors or losses related to depth [88; 3; 97], normals [88; 76; 16], and multi-view consistency [14].

Depth images are also helpful to improve the inference accuracy. Depth information can guide sampling along rays [88] or provide rendering supervision [88; 3; 97; 96; 24; 31; 82; 98], which helps neural networks to estimate surfaces.

### Neural Implicit with SLAM

Given RGBD images, more recent methods [91; 81; 72; 59] employ neural implicit representations in SLAM. iMAP [66] shows that an MLP can serve as the only scene representation in a realtime SLAM system. NICE-SLAM [97] introduces a hierarchical scene representation to reconstruct large scenes with more details. NICER-SLAM [96] uses easy-to-obtain monocular geometric cues without requiring depth supervision. Co-SLAM [74] jointly uses coordinate and sparse parametric encodings to learn neural implicit functions. Segmentation priors [29; 18] show their potentials to improve the performance of SLAM systems. With segmentation priors, vMAP [29] represents each object in the scene as a neural implicit in a SLAM system.

Instead of using depth ground truth to only supervise rendering, which contains incomplete depth and unawareness of occlusion, we allow our neural network to directly use depth fusion priors as a part of neural implicit, and determine where and how to use depth priors along with learned geometry.

## 3 Method

**Overview.** We present an overview of our method in Fig. 1. Given RGBD images \(\{I_{j},D_{j}\}_{j=1}^{J}\) from \(J\) view angles, where \(I_{j}\) and \(D_{j}\) denote the RGB and depth images, available either all together or in a streaming way, we aim to infer the geometry of the scene as an occupancy function \(f\) which predicts the occupancy \(f(\bm{q})\) at arbitrary locations \(\bm{q}=(x,y,z)\). Our method works with camera poses \(\{M_{j}\}\) that are either known or estimated by our camera tracking method in the context of SLAM.

We learn the occupancy function through volume rendering using the RGBD images \(\{I_{j},D_{j}\}_{j=1}^{J}\) as supervision. We render \(f\) with a color function \(c\) which predicts an RGB color \(c(\bm{q})\) at locations \(\bm{q}\) into an RGB image \(I_{j}^{\prime}\) and a depth image \(D_{j}^{\prime}\), which are optimized to minimize their rendering errors to the supervision \(\{I_{j},D_{j}\}\).

We start from shooting rays \(\{V_{k}\}\) from current view \(I_{j}\), and sample queries \(\bm{q}\) along each ray \(V_{k}\). For each query \(\bm{q}\), we employ learnable feature grids \(G_{l}\), \(G_{h}\), and \(G_{c}\) covering the scene to interpolate its hierarchical geometry features \(\bm{t}_{l}\), \(\bm{t}_{h}\), and a color feature \(\bm{t}_{c}\) by trilinear interpolation. Each feature is further transformed into an occupancy probability or RGB color by their corresponding decoders \(f_{l}\), \(f_{h}\), and \(f_{c}\). For queries \(\bm{q}\) inside the bandwidth of a TSDF grid \(G_{s}\) fused from available depth images \(\{D_{i}\}\), we leverage its interpolation from \(G_{s}\) as a prior of coarse occupancy estimation. The prior is attentive by a neural function \(f_{a}\) which determines the occupancy function \(f(\bm{q})\) by combining currently learned geometry and the coarse estimation using the learned attention weights from \(f_{a}\).

Lastly, we use the occupancy function \(f(\bm{q})\) and the color function \(c(\bm{q})\) to render \(I^{\prime}\) and \(D^{\prime}\) through volume rendering. With a learned \(f\), we run the marching cubes [40] to reconstruct a surface.

Figure 1: Overview of our method.

**Depth Fusion and Bandwidth Awareness.** With known or estimated camera poses \(\{M_{j}\}\), we get a TSDF by fusing depth images \(\{D_{j}\}\) that are available either all together or in a streaming way. Our method can work with a TSDF that describes either a whole scene or just a part of the scene, and we will report the performance with different settings in ablation studies. We use the TSDF to provide a coarse occupancy estimation which removes the limit of the missing depth or the unawareness of occlusion on single depth supervision. The TSDF is a grid which predicts signed distances at any locations inside it through trilinear interpolation. The predicted signed distances are truncated with a threshold \(\mu\) into a range of \([-1,1]\) which covers a band area on both sides of the surface in Fig. 1.

Since the TSDF predicts signed distances for queries within the bandwidth with higher confidence than the ones outside the bandwidth, we only use the depth fusion prior within the bandwidth. This leads to different ways of learning geometries, as shown in Fig. 1. Specifically, for queries \(\bm{q}\) within the bandwidth, we model the low-frequency surfaces using a low resolution feature grid \(G_{l}\), learn the high-frequency details as a complementation using a high resolution feature grid \(G_{h}\), and let our attention mechanism to determine how to use the depth fusion prior from the TSDF \(G_{s}\). Instead, we only use the low resolution feature grid \(G_{l}\) to model densities outside the bandwidth during volume rendering. Regarding color modeling, we use the feature grid \(G_{c}\) with the same size for interpolating color of queries over the scene.

**Feature Interpolation.** For queries \(\bm{q}\) sampled along rays, we use the trilinear interpolation to obtain its features \(\bm{h}_{l}\), \(\bm{h}_{h}\), and \(\bm{h}_{c}\) from learnable \(G_{l}\), \(G_{h}\), \(G_{c}\) and occupancy estimation \(o_{s}\) from \(G_{s}\). Both feature grids and the TSDF grid cover the whole scene and use different resolutions, where learnable feature vectors are associated with vertices on each feature grid. Signed distances at vertices on \(G_{s}\) may change if we incrementally fuse depth images in the context of SLAM.

**Occupancy Prediction Priors.** Similar to NICE-SLAM [97], we use pre-trained decoders \(f_{l}\) and \(f_{h}\) to predict low frequency occupancies \(o_{l}\) and high frequency ones \(o_{h}\) from the interpolated features \(\bm{t}_{l}\) and \(\bm{t}_{h}\), respectively. We use \(f_{l}\) and \(f_{h}\) as an MLP decoder in ConvONet [54] respectively, and minimize the binary cross-entropy loss to fit the ground truth. After pre-training, we fix the parameters in \(f_{l}\) and \(f_{h}\), and use them to predict occupancies below,

\[o_{l}=f_{l}(\bm{q},\bm{t}_{l}),\ o_{h}=f_{h}(\bm{q},\bm{t}_{h}),\ o_{lh}=o_{l }+o_{h},\] (1)

where we enhance \(f_{h}\) by concatenating \(f_{l}\) as \(f_{h}\leftarrow[f_{h},f_{l}]\) and denote \(o_{lh}\) as the occupancy predicted at query \(\bm{q}\) by the learned geometry.

**Color Predictions.** Similarly, we use the interpolated feature \(\bm{t}_{c}\) and an MLP decoder \(f_{c}\) to predict color at query \(\bm{q}\), i.e., \(c(\bm{q})=f_{c}(\bm{q},\bm{t}_{c})\). We predict color to render RGB images for geometry inference or camera tracking in SLAM. The decoder is parameterized by parameters \(\bm{\theta}\) which are optimized with other learnable parameters in the feature grids.

**Attentive Depth Fusion Prior.** We introduce an attention mechanism to leverage the depth fusion prior. We use a deep neural network to learn attention weights to allow networks to determine how to use the prior in volume rendering.

As shown in Fig. 1, we interpolate a signed distance \(s\in[-1,1]\) at query \(\bm{q}\) from the TSDF \(G_{s}\) using trilinear interpolation, where \(G_{s}\) is fused from available depth images. We formulate this interpolation as \(s=f_{s}(\bm{q})\in[-1,1]\). We normalize \(s\) into an occupancy \(o_{s}\in[0,1]\), and regard \(o_{s}\) as the occupancy predicted at query \(\bm{q}\) by the depth fusion prior.

For query \(\bm{q}\) within the bandwidth, our attention mechanism trains an MLP \(f_{a}\) to learn attention weights \(\alpha\) and \(\beta\) to aggregate the occupancy \(o_{lh}\) predicted by the learned geometry and the occupancy \(o_{s}\) predicted by the depth fusion prior, which leads the TSDF to an attentive depth fusion prior. Hence, our attention mechanism can be formulated as,

\[[\alpha,\beta]=f_{a}(o_{lh},o_{s}),\text{ and }\alpha+\beta=1.\] (2)

We implement \(f_{a}\) using an MLP with 6 layers. The reason why we do not use a complex network like Transformer is that we want to justify the effectiveness of our idea without taking too much credit from complex neural networks. Regarding the design, we do not use coordinates or positional encoding as a part of input to avoid noisy artifacts. Moreover, we leverage a Softmax normalization layer to achieve \(\alpha+\beta=1\), and we do not predict only one parameter \(\alpha\) and use \(1-\alpha\) as the second weight, which also degenerates the performance. We will justify these alternatives in experiments.

For query \(\bm{q}\) outside the bandwidth, we predict the occupancy using the feature \(\bm{t}_{l}\) interpolated from the low frequency grid \(G_{l}\) and the decoder \(f_{l}\) to describe the relatively simpler geometry. In summary, we eventually formulate our occupancy function \(f\) as a piecewise function below,

\[f(\bm{q})=\begin{cases}\alpha\times o_{lh}+\beta\times o_{s},&f_{s}(\bm{q})\in( -1,1)\\ o_{l},&f_{s}(\bm{q})=1\text{ or }-1\end{cases}\] (3)

**Volume Rendering.** We render the color function \(c\) and occupancy function \(f\) into RGB \(I^{\prime}\) and depth \(D^{\prime}\) images to compare with the RGBD supervision \(\{I,D\}\).

With camera poses \(M_{j}\), we shoot a ray \(V_{k}\) from view \(I_{j}\). \(V_{k}\) starts from the camera origin \(\bm{m}\) and points a direction of \(\bm{r}\). We sample \(N\) points along the ray \(V_{k}\) using stratified sampling and uniformly sampling near the depth, where each point is sampled at \(\bm{q}_{n}=\bm{m}+d_{n}\bm{r}\) and \(d_{n}\) corresponds to the depth value of \(\bm{q}_{n}\) on the ray. Following UNISURF [51], we transform occupancies \(f(\bm{q}_{n})\) into weights \(w_{n}\) which is used for color and depth accumulation along the ray \(V_{k}\) in volume rendering,

\[w_{n}=f(\bm{q}_{n})\prod_{n^{\prime}=1}^{n-1}(1-f(\bm{q}_{n^{\prime}})),\;{I( k)}^{\prime}=\sum_{n^{\prime}=1}^{N}w_{n^{\prime}}\times c(\bm{q}_{n^{\prime}}), \;D(k)^{\prime}=\sum_{n^{\prime}=1}^{N}w_{n^{\prime}}\times d_{n^{\prime}}.\] (4)

**Loss.** With known camera pose \(M_{j}\), we render the scene into the color and depth images at randomly sampled \(K\) pixels on the \(j\)-th view, and optimize parameters by minimizing the rendering errors,

\[L_{I}=\frac{1}{JK}\sum_{j,k=1}^{J,K}||I_{j}(k)-I^{\prime}_{j}(k)||_{1},L_{D}= \frac{1}{JK}\sum_{j,k=1}^{J,K}||D_{j}(k)-D^{\prime}_{j}(k)||_{1},\min_{\bm{ \theta},G_{l},G_{h},G_{e}}L_{D}+\lambda L_{I}.\] (5)

**In the Context of SLAM.** We can jointly do camera tracking and learning neural implicit from streaming RGBD images. To achieve this, we regard the camera extrinsic matrix \(M_{j}\) as learnable parameters, and optimize them by minimizing our rendering errors. Here, we follow [97] to weight the depth rendering loss to decrease the importance at pixels with large depth variance along the ray,

\[\min_{M_{j}}\frac{1}{JK}\sum_{j,k=1}^{J,K}\frac{1}{Var(D^{\prime}_{j}(k))}||D _{j}(k)-D^{\prime}_{j}(k)||_{1}+\lambda_{1}L_{I},\] (6)

where \(Var(D^{\prime}_{j}(k))=\sum_{n=1}^{N}w_{n}(D^{\prime}_{j}(k)-d_{n})^{2}\). Moreover, with streaming RGBD images, we incrementally fuse the most current depth image into TSDF \(G_{s}\). Specifically, the incremental fusion includes a pre-fusion and an after-fusion stage. The pre-fusion aims to use a camera pose coarsely estimated by a traditional method to fuse a depth image onto a current TSDF to calculate a rendering error for a more accurate pose estimation at current frame. The after-fusion stage will refuse the depth image onto the current TSDF for camera tracking at the next frame. Please refer to our supplementary materials for more details.

We do tracking and mapping iteratively. For mapping procedure, we render \(E\) frames each time and back propagate rendering errors to update parameters. \(E\) frames include the current frame and \(E-1\) key frames that have overlaps with the current frame. For simplicity, we merely maintain a key frame list by adding incoming frames with an interval of \(50\) frames for fair comparisons.

**Details.** The optimization is performed at three stages, which makes optimization converge better. We first minimize \(L_{D}\) by optimizing low frequency feature grid \(G_{l}\), and then both low and high frequency feature grid \(G_{l}\) and \(G_{h}\), and finally minimize Eq 5 by optimizing \(G_{l}\), \(G_{h}\), and \(G_{c}\). Our bandwidth from the TSDF \(G_{s}\) covers \(5\) voxels on both sides of the surface. We shoot \(K=1000\) or \(5000\) rays for reconstruction or tracking from each view, and render \(E=5\) or \(10\) frames each time for fair comparison with other methods. We set \(\lambda=0.2\), \(\lambda_{1}=0.5\) in loss functions. We sample \(N=48\) points along each ray for rendering. More implementation details can be found in our supplementary materials.

## 4 Experiments and Analysis

### Experimental Setup

**Datasets.** We report evaluations on both synthetic datasets and real scans including Replica [64] and ScanNet [13]. For fair comparisons, we report results on the same scenes from Replica and ScanNetas the latest methods. Specifically, we report comparisons on all \(8\) scenes in Replica. As for ScanNet, we report comparison on scene 50, 84, 580, and 616 used by MonoSDF [88], and also scene 59, 106, 169, and 207 used by NICE-SLAM [97]. We mainly report average results over the dataset, please refer to our supplementary materials for results on each scene.

**Metrics.** With the learned occupancy function \(f\), we reconstruct the surface of a scene by extracting the zero level set of \(f\) using the marching cubes algorithm [40]. Following previous studies [97; 22], we use depth L1 [cm], accuracy [cm], completion [cm], and completion ratio [!5cm\(\%\)] as metrics to evaluate reconstruction accuracy on Replica. Additionally, we report Chamfer distance (L1), precision, recall, and F-score with a threshold of \(0.05\) to evaluate reconstruction accuracy on ScanNet. To evaluate the accuracy in camera tracking, we use ATE RMSE [65] as a metric. We follow the same parameter setting in these metrics as [97].

### Evaluations

**Evaluations on Replica.** We use the same set of RGBD images as [66; 96]. We report evaluations in surface reconstruction and camera tracking in Tab. 1, Tab. 2 and Tab. 3, respectively.

We jointly optimize camera poses and learn geometry in the context of SLAM. The depth fusion prior \(G_{s}\) incrementally fuses depth images using estimated camera poses. We report the accuracy of reconstruction from \(G_{s}\) as "TSDF". Compared to this baseline, we can see that our method improves the reconstruction using the geometry learned through volume rendering and occupancy prediction priors. We visualize the advantage of learning geometry in Fig. 2. Note that the holes in TSDF are caused by the absence of RGBD scans. We can see that our neural implicit keeps the correct structure in TSDF and plausibly completes the missing structures in TSDF. We further visualize the attention weights \(\beta\) learned for depth fusion in Fig. 3. We visualize the cross sections on \(4\) scenes, where \(\beta\) learned in bandwidth are shown in color red. Generally, the neural implicit mostly focuses more on the depth fusion priors in areas where TSDF is incomplete. In the area where TSDF is complete, the network also pays some attention to the inferred occupancy because the occupancy interpolated from TSDF may not be accurate, especially on the most front of surfaces in Fig. 3.

Moreover, our method outperforms the latest implicit-based SLAM methods like NICE-SLAM [97] and NICER-SLAM [96]. We present visual comparisons in Fig. 6, where our method produces more accurate and compact geometry. For method using GT camera poses like vMAP [29] and MonoSDF [88], we achieve much better performance as shown by "Ours*" in Tab. 1 and "Ours" in Tab. 2. Although we require the absolute dense depth maps, compared to MonoSDF [88] that can work with

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & COLMAP [62] & TSDF [90] & iMAP [66] & DI [22] & NICE [97] & Vux [83] & DROD [71] & NICER [96] & Ours & vMAP [29] & Ours* \\ \hline
**Depth.1**\(\downarrow\) & - & 6.56 & 7.64 & 23.33 & 3.53 & - & - & - & **3.01** & - & **2.60** \\
**Acc. \(\downarrow\)** & 8.69 & **1.56** & 6.95 & 19.40 & 2.85 & 2.67 & 5.50 & 3.65 & 2.77 & 3.20 & **2.59** \\
**Comp. \(\downarrow\)** & 12.12 & 3.33 & 5.33 & 10.19 & 3.00 & 4.55 & 12.29 & 4.16 & **2.45** & 2.39 & **2.28** \\
**Ratio \(\uparrow\)** & 67.62 & 87.61 & 66.60 & 72.96 & 89.33 & 86.59 & 63.62 & 79.37 & **92.79** & 92.59 & **93.38** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Reconstruction Comparisons on Replica.

Figure 3: Visualization of attention on depth fusion.

Figure 2: Merits of attentive depth fusion prior.

scaled depth maps, our method only can use the views in front of the current time step, where MonoSDF [88] can utilize all views during the whole training process.

In camera tracking, our results in Tab. 3 achieve the best in average. We compare the estimated trajectories on scene off-4 in Fig. 4 (c). The comparison shows that our attentive depth fusion prior can also improve the camera tracking performance through volume rendering.

**Evaluations on ScanNet.** We further evaluate our method on ScanNet. For surface reconstruction, we compare with the latest methods for learning neural implicit from multiview images. We report both their results and ours with GT camera poses, where we also fuse every \(10\) depth images into the TSDF \(G_{s}\) and render every \(10\) frames for fair comparisons. Numerical comparisons in Tab. 4 show that our method achieves the best performance in terms of all metrics, where we use the culling strategy introduced in MonoSDF [88] to clean the reconstructed mesh. We highlight our significant improvements in visual comparisons in Fig. 7. We see that our method can reconstruct sharper, more compact and detailed surfaces than other methods. We detail our comparisons on every scene with the top results reported by GO-Surf [75] and NICE-SLAM [97] in Tab. 5 and Tab. 6. The comparisons in Tab. 5 show that our method achieves higher reconstruction accuracy while GO-Surf produces more complete surfaces. We present visual comparisons with error maps in Fig. 5.

In camera tracking, we compare with the latest methods. We incrementally fuse the most current depth frames into TSDF \(G_{s}\) which is used for attentive depth fusion priors in volume rendering. Numerical comparisons in Tab. 7 show that our results are better on \(2\) out of \(4\) scenes and achieve the best in average. Tracking trajectory comparisons in Fig. 4 (a) and (b) also show our superiority.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Test Split} & \multicolumn{3}{c}{Train Split} \\  & Normal C.\(\uparrow\) & Chamfer-\(L_{1}\downarrow\) & F-score \(\uparrow\) & Normal C.\(\uparrow\) & Chamfer-\(L_{1}\downarrow\) & F-score \(\uparrow\) \\ \hline MonoSDF [88] & 90.56 & 4.26 & 76.42 & **91.80** & 3.59 & 85.67 \\ \hline Ours & **90.69** & **2.43** & **92.47** & 91.05 & **2.73** & **90.52** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Reconstruction Comparison with MonoSDF on Replica.

Figure 4: Visual comparisons in camera tracking.

Figure 5: Visual comparison of error maps (Red: Large) in surface reconstructions on Replica.

Figure 6: Visual comparisons of error maps (Red: Large) in surface reconstructions on Replica.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & rm-0 & rm-1 & rm-2 & off-0 & off-1 & off-2 & off-3 & off-4 & Avg. \\ \hline NICE-SLAM [97] & 1.69 & 2.04 & 1.55 & **0.99** & **0.90** & **1.39** & 3.97 & 3.08 & 1.95 \\ NICER-SLAM [96] & **1.36** & 1.60 & **1.14** & 2.12 & 3.23 & 2.12 & **1.42** & 2.01 & 1.88 \\ \hline Ours & 1.39 & **1.55** & 2.60 & 1.09 & 1.23 & 1.61 & 3.61 & **1.42** & **1.81** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Camera Tracking Comparisons (ATE RMSE) on Replica.

### Ablation Studies

We report ablation studies to justify the effectiveness of modules in our method on Replica. We use estimated camera poses in ablation studies.

**Effect of Depth Fusion.** We first explore the effect of different ways of depth fusion on the performance. According to whether we use GT camera poses or incremental fusion, we try \(4\) alternatives with \(G_{s}\) obtained by fusing all depth images at the very beginning using GT camera poses (Offline) or fusing the current depth image incrementally (Online), and using tracking to estimate camera poses (Tracking) or GT camera poses (GT) in Tab. 8. The comparisons show that our method can work well with either GT or estimated camera poses and fusing depth either all together or incrementally in a streaming way. Additional conclusion includes that GT camera poses in either depth fusion and rendering do improve the reconstruction accuracy, and the structure fused from more recent frames is more important than the whole structure fused from all depth images.

\begin{table}
\begin{tabular}{l c c c c|c c c c c} \hline \hline  & & \multicolumn{3}{c|}{NICE [97]} & \multicolumn{3}{c}{Ours} \\ Scene ID & 0050 & 0084 & 0580 & 0616 & Avg. & 0050 & 0084 & 0580 & 0616 & Avg. \\ \hline Acc \(\downarrow\) & **0.030** & **0.031** & **0.032** & **0.026** & **0.030** & **0.030** & 0.039 & 0.041 & **0.026** & 0.034 \\ Comp \(\downarrow\) & 0.053 & 0.020 & **0.031** & 0.076 & 0.045 & **0.043** & **0.014** & 0.035 & **0.063** & **0.039** \\ Chamfer-\(L_{1}\downarrow\) & 0.041 & **0.025** & **0.032** & 0.051 & **0.037** & **0.037** & 0.026 & 0.038 & **0.045** & **0.037** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Reconstruction Comparison with NICE-SLAM on ScanNet.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & Acc \(\downarrow\) & Comp \(\downarrow\) & Chamfer-\(L_{1}\downarrow\) & Prec \(\uparrow\) & Recall \(\uparrow\) & F-score \(\uparrow\) \\ \hline COLMAP [62] & 0.047 & 0.235 & 0.141 & 0.711 & 0.441 & 0.537 \\ UNISURF [51] & 0.554 & 0.164 & 0.359 & 0.212 & 0.362 & 0.267 \\ NeuS [77] & 0.179 & 0.208 & 0.194 & 0.313 & 0.275 & 0.291 \\ VolSDF [85] & 0.414 & 0.120 & 0.267 & 0.321 & 0.394 & 0.346 \\ Manhattan-SDF [16] & 0.072 & 0.068 & 0.070 & 0.621 & 0.586 & 0.602 \\ NeuRIS [76] & 0.050 & 0.049 & 0.050 & 0.717 & 0.669 & 0.692 \\ MonoSDF [88] & 0.035 & 0.048 & 0.042 & 0.799 & 0.681 & 0.733 \\ \hline Ours & **0.034** & **0.039** & **0.037** & **0.913** & **0.894** & **0.902** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Reconstruction Comparisons on ScanNet.

\begin{table}
\begin{tabular}{l c c c c|c c c c c} \hline \hline  & & \multicolumn{3}{c|}{GO-Surf [75]} & \multicolumn{3}{c}{Ours} \\ Scene ID & 0050 & 0084 & 0580 & 0616 & Avg. & 0050 & 0084 & 0580 & 0616 & Avg. \\ \hline Acc \(\downarrow\) & 0.056 & 0.073 & 0.057 & **0.026** & 0.053 & **0.030** & **0.039** & **0.041** & **0.026** & **0.034** \\ Comp \(\downarrow\) & **0.024** & 0.017 & **0.024** & **0.023** & **0.022** & 0.043 & **0.014** & 0.035 & 0.063 & 0.039 \\ Chamfer-\(L_{1}\downarrow\) & 0.040 & 0.045 & 0.040 & **0.025** & 0.038 & **0.037** & **0.026** & **0.038** & 0.045 & **0.037** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Reconstruction Comparison with GO-Surf on ScanNet.

Figure 7: Visual comparisons in surface reconstructions on ScanNet.

**Effect of Attention.** We explore the effect of attention on the performance in Tab. 9. First of all, we remove the attention mechanism, and observe a severe degeneration in accuracy, which indicates that the attention plays a big role in directly applying depth fusion priors in neural implicit. Then, we try to apply attention different features including low frequency \(\bm{t}_{l}\), high frequency \(\bm{t}_{h}\), and both of them. The comparisons show that adding details from depth fusion priors onto low frequency surfaces does not improve the performance. More analysis on attention mechanism can be found in Fig. 9.

**Attention Alternatives.** Our preliminary results show that using softmax normalization layer achieves better performance than other alternatives. Since we merely need two attention weights \(\alpha\) and \(\beta\) to combine the learned geometry and the depth fusion prior, one alternative is to use a sigmoid function to predict one attention weight and use its difference to 1 as another attention weight. However, sigmoid can not effectively take advantage of the depth fusion prior. We also try to use coordinates as an input, which pushes the network to learn spatial sensitive attentions. While the reconstructed surfaces turn out to be noisy in Fig. 8. This indicates that our attention learned a general attentive pattern for all locations in the scene. We also report the numerical comparisons in Tab. 10. Please refer to our supplementary materials for more results related to attention alternatives.

**Effect of Bandwidth.** We further study the effect of bandwidth on the performance in Tab. 11. We try to use attentive depth fusion priors everywhere in the scene with no bandwidth. The degenerated results indicate that the truncated area outside the bandwidth does not provide useful structures to improve the performance. Moreover, wider bandwidth may cause more artifacts caused by the calculation of the TSDF \(G_{s}\) while narrower bandwidth brings more incomplete structures, neither of which improves the performance. Instead of using low frequency geometry outside the bandwidth, we also try to use high frequency surfaces. We can see that low frequency geometry is more suitable to describe the scene outside the bandwidth.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Coordinates & Sigmoid & Softmax (Ours) \\ \hline
**DepthL1 \(\downarrow\)** & 1.81 & 1.96 & **1.44** \\
**Acc. \(\downarrow\)** & 2.66 & 2.86 & **2.54** \\
**Comp. \(\downarrow\)** & 2.74 & 2.66 & **2.41** \\
**Ratio \(\uparrow\)** & 93.13 & **93.27** & 93.22 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Ablation Studies on Attention Alternatives.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Scene ID & 0059 & 0106 & 0169 & 0207 & Avg. \\ \hline iMAP [66] & 32.06 & 17.50 & 70.51 & 11.91 & 33.00 \\ DI [22] & 128.00 & 18.50 & 75.80 & 100.19 & 80.62 \\ NICE [97] & 12.25 & 8.09 & 10.28 & **5.59** & 9.05 \\ CO [74] & 12.29 & 9.57 & **6.62** & 7.13 & **8.90** \\ \hline Ours & **10.50** & **7.48** & 9.31 & 5.67 & **8.24** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Camera Tracking Comparisons (ATE RMSE) on ScanNet.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{1}{c}{w/o Attention} & \multicolumn{1}{c}{Low} & \multicolumn{1}{c}{Low + High} & \multicolumn{1}{c}{High (Ours)} \\ \hline
**DepthL1 \(\downarrow\)** & 1.86 & 2.75 & 2.96 & **1.44** \\
**Acc. \(\downarrow\)** & 2.69 & 2.96 & 3.85 & **2.54** \\
**Comp. \(\downarrow\)** & 2.81 & 3.14 & 2.91 & **2.41** \\
**Ratio \(\uparrow\)** & 91.46 & 89.73 & **93.63** & 93.22 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation Studies on Attention.

Figure 8: Visual comparison of error maps with different attention alternatives (Red: Large).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{1}{c}{w/o Attention} & \multicolumn{1}{c}{Low} & \multicolumn{1}{c}{Low + High} & \multicolumn{1}{c}{High (Ours)} \\ \hline
**DepthL1 \(\downarrow\)** & 1.86 & 2.75 & 2.96 & **1.44** \\
**Acc. \(\downarrow\)** & 2.69 & 2.96 & 3.85 & **2.54** \\
**Comp. \(\downarrow\)** & 2.81 & 3.14 & 2.91 & **2.41** \\
**Ratio \(\uparrow\)** & 91.46 & 89.73 & **93.63** & 93.22 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation Studies on Attention.

**Effect of Prediction Priors.** Prediction priors from decoders \(f_{l}\) and \(f_{h}\) are also important for accuracy improvements. Instead of using fixed parameters in these decoders, we try to train \(f_{l}\) (Fix \(f_{h}\)) or \(f_{h}\) (Fix \(f_{l}\)) with other parameters. Numerical comparisons in Tab. 12 show that training \(f_{l}\) or \(f_{h}\) cannot improve the performance.

**More Analysis on How Attention Works.** Additionally, we do a visual analysis on how attention works in Fig. 9. We sample points on the GT mesh, and get the attention weights in Fig. 9 (a). At each point, we show its distance to the mesh from the TSDF in Fig. 9 (b) and the mesh from the inferred occupancy in Fig. 9 (c), respectively. Fig. 9 (d) indicates where the former is smaller than the latter. The high correlation between Fig. 9 (a) and Fig. 9 (d) indicates that the attention network focuses more on the occupancy producing smaller errors to the GT surface. Instead, the red in Fig. 9 (e) indicates where the interpolated occupancy is larger than the inferred occupancy is not correlated to the one in Fig. 9 (a). The uncorrelation indicates that the attention network does not always focus on the larger occupancy input but the one with smaller errors, even without reconstructing surfaces.

## 5 Conclusion

We propose to learn neural implicit through volume rendering with attentive depth fusion priors. Our novel prior alleviates the incomplete depth at holes and the unawareness of occluded structures when using depth images as supervision in volume rendering. We also effectively enable neural networks to determine how much depth fusion prior can be directly used in the neural implicit. Our method can work well with depth fusion from either all depth images together or the ones available in a streaming way, using either known or estimated camera poses. To this end, our novel attention successfully learns how to combine the learned geometry and the depth fusion prior into the neural implicit for more accurate geometry representations. The ablation studies justify the effectiveness of our modules and training strategy. Our experiments on benchmarks with synthetic and real scans show that our method learns more accurate geometry and camera poses than the latest neural implicit methods.

## Acknowledgements and Disclosure of Funding

This work was partially supported by a Richard Barber Interdisciplinary Research Award.

\begin{table}
\begin{tabular}{l c c c c|c c} \hline \hline  & w/o & Fix & Fix \(f_{l}\) & Fix \(f_{h}\) & Fix \(f_{l}+f_{h}\) (Ours) \\ \hline
**DepthL1**\(\downarrow\) & 125.95 & 43.46 & F & **1.44** \\
**Acc.**\(\downarrow\) & 127.62 & 67.83 & F & **2.54** \\
**Comp.**\(\downarrow\) & 56.73 & 18.79 & F & **2.41** \\
**Ratio**\(\uparrow\) & 5.27 & 103.51 & F & **93.22** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Ablation Studies on Prediction Priors.

\begin{table}
\begin{tabular}{l c c c c|c c} \hline \hline  & w/o & Bandwidth & Bandwidth=3 & Bandwidth=5 (Ours) & Bandwidth=7 & High & Low (Ours) \\ \hline
**DepthL1**\(\downarrow\) & 1.87 & 1.61 & **1.44** & 2.01 & 124.21 & **1.44** \\
**Acc.**\(\downarrow\) & **2.40** & 2.85 & 2.54 & 2.93 & 25.06 & **2.54** \\
**Comp.**\(\downarrow\) & 3.49 & 2.61 & **2.41** & 2.73 & 3.62 & **2.41** \\
**Ratio**\(\uparrow\) & 90.94 & 92.84 & 93.22 & **93.55** & 86.92 & **93.22** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Ablation Studies on the Effect of Bandwidth.

Figure 9: Analysis on Attentions. (a) Map the attention weights on occupancies interpolated from TSDF. (b) The point-to-surface distances to the mesh reconstructed from TSDF. (c) The point-to-surface distances to the mesh reconstructed from the inferred occupancies. (d) Binary map indicating smaller errors (red) of TSDF. (e) Binary map indicating larger (red) interpolated occupancies than the inferred occupancies.

## References

* [1]M. Atzmon and Y. Lipman (2020) SALD: sign agnostic learning of shapes from raw data. In IEEE Conference on Computer Vision and Pattern Recognition, Cited by: SS1.
* [2]M. Atzmon and J. Lipman (2021) SALD: sign agnostic learning with derivatives. In International Conference on Learning Representations, Cited by: SS1.
* [3]D. Azinovic, R. Martin-Brualla, D. B. Goldman, M. Niessner, and J. Thies (2022) Neural rgb-d surface reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 6290-6301. Cited by: SS1.
* [4]M. Baorui, L. Yu-Shen, and H. Zhizhong (2022) Reconstructing surfaces for sparse point clouds with on-surface priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1.
* [5]M. Baorui, H. Zhizhong, L. Yu-Shen, and Z. Matthias (2021) Neural-pull: learning signed distance functions from point clouds by learning to pull space onto surfaces. In International Conference on Machine Learning (ICML), Cited by: SS1.
* [6]A. Bozic, P. Palafox, J. Thies, A. Dai, and M. Niessner (2021) Transformerfusion: monocular rgb scene reconstruction using transformers. Advances in Neural Information Processing Systems. Cited by: SS1.
* [7]T. Brooks, A. Holynski, and A. A. Efros (2023) Instructpix2pix: learning to follow image editing instructions. In CVPR, Cited by: SS1.
* [8]R. Chabra, J. E. Lenssen, E. Ilg, T. Schmidt, J. Straub, S. Lovegrove, and R. A. Newcombe (2020) Deep local shapes: learning local SDF priors for detailed 3D reconstruction. In European Conference on Computer Vision, Vol. 12374, pp. 608-625. Cited by: SS1.
* [9]C. Chen, Y. Liu, and Z. Han (2022) Latent partition implicit with surface codes for 3d representation. In European Conference on Computer Vision, Cited by: SS1.
* [10]C. Chen, Y. Liu, and Z. Han (2022) Gridpull: towards scalability in learning implicit representations from 3d point clouds. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Cited by: SS1.
* [11]C. Chen, Y. Liu, and Z. Han (2022) Gridpull: towards scalability in learning implicit representations from 3d point clouds. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Cited by: SS1.
* [12]Z. Chen, T. Funkhouser, P. Hedman, and A. Tagliascachi (2023) Mobilenerf: exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In The Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1.
* [13]A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Niessner (2017) Scannet: richly-annotated 3d reconstructions of indoor scenes. CoRRabs/1702.04405. Cited by: SS1.
* [14]Q. Fu, Q. Xu, Y. Ong, and W. Tao (2022) Geo-Neus: geometry-consistent neural implicit surfaces learning for multi-view reconstruction. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [15]A. Gropp, L. Yariv, N. Haim, M. Atzmon, and Y. Lipman (2020) Implicit geometric regularization for learning shapes. In International Conference on Machine Learning, Vol. 119 of Proceedings of Machine Learning Research, pp. 3789-3799. Cited by: SS1.
* [16]H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou (2022) Neural 3d scene reconstruction with the manhattan-world assumption. In IEEE Conference on Computer Vision and Pattern Recognition, Cited by: SS1.
* [17]A. Gupta, W. Xiong, Y. Nie, I. Jones, and B. Oguz (2023) 3dgen: triplane latent diffusion for textured mesh generation. Cited by: SS1.
* [18]Y. Haghighi, S. Kumar, J. Thiran, and L. Van Gool (2023) Neural implicit dense semantic slam. Cited by: SS1.
* [19]Z. Han, G. Qiao, Y. Liu, and M. Zwicker (2020) Seqx2seqz: structure learning for 3d shapes by sequentially predicting 1d occupancy segments from 2d coordinates. Cited by: SS1.
* [20]A. Haque, M. Tancik, A. A. Efros, A. Holynski, and A. Kanazawa (2023) Instruct-nerf2nerf: editing 3d scenes with instructions. Cited by: SS1.
* [21]A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-Or (2022) Prompt-to-prompt image editing with cross attention control. Cited by: SS1.
* [22]J. Huang, S. Huang, H. Song, and S. Hu (2021) Di-fusion: online implicit 3d reconstruction with deep priors. Cited by: SS1.
* [23]C. Jiang, A. Sud, A. Makadia, J. Huang, M. Niessner, and T. Funkhouser (2020) Local implicit grid representations for 3D scenes. In IEEE Conference on Computer Vision and Pattern Recognition, Cited by: SS1.
* [24]H. Jiang, C. Zeng, R. Chen, S. Liang, Y. Han, Y. Gao, and C. Wang (2023) Depth-neus: neural implicit surfaces learning for multi-view reconstruction based on depth information optimization. Cited by: SS1.
* [25]H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou (2022) Neural 3d scene reconstruction with the manhattan-world assumption. In IEEE Conference on Computer Vision and Pattern Recognition, Cited by: SS1.
* [26]H. Huang, A. Yu-Shen, A. A. Efros, A.

* [25] Sijia Jiang, Jing Hua, and Zhizhong Han. Coordinate quantized neural implicit representations for multi-view 3d reconstruction. In _IEEE International Conference on Computer Vision_, 2023.
* [26] Yue Jiang, Dantong Ji, Zhizhong Han, and Matthias Zwicker. SDFDiff: Differentiable rendering of signed distance fields for 3D shape optimization. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2020.
* [27] Leonid Keselman and Martial Hebert. Flexible Techniques for Differentiable Rendering with 3D Gaussians, 2023.
* [28] Lukas Koestler, Nan Yang, Niclas Zeller, and Daniel Cremers. TANDEM: tracking and dense mapping in real-time using deep multi-view stereo. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, _Conference on Robot Learning_, volume 164, pages 34-45, 2021.
* [29] Xin Kong, Shikun Liu, Marwan Taher, and Andrew J Davison. vmap: Vectorised object mapping for neural field slam. _arXiv preprint arXiv:2302.01838_, 2023.
* [30] A. Laurentini. The visual hull concept for silhouette-based image understanding. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 16(2):150-162, 1994.
* [31] Seunghwan Lee, Gwanmo Park, Hyewon Son, Jiwon Ryu, and Han Joo Chae. Fastsurf: Fast neural rgb-d surface reconstruction using per-frame intrinsic refinement and tsdf fusion prior learning. _arXiv preprint arXiv:2303.04508_, 2023.
* [32] Jiabao Lei, Jiapeng Tang, and Kui Jia. Rgbd2: Generative scene synthesis via incremental view inpainting using rgbd diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2023.
* [33] Tianyang Li, Xin Wen, Yu-Shen Liu, Hua Su, and Zhizhong Han. Learning deep implicit functions for 3d shapes with dynamic code clouds. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [34] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralcap: High-fidelity neural surface reconstruction. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [35] Zhihao Liang, Zhangjin Huang, Changxing Ding, and Kui Jia. Helixsurf: A robust and efficient neural implicit surface learning of indoor scenes with iterative intertwined regularization, 2023.
* [36] Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey. SDF-SRN: Learning signed distance 3D object reconstruction from static images. In _Advances in Neural Information Processing Systems_, 2020.
* [37] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li. Learning to infer implicit surfaces without 3D supervision. In _Advances in Neural Information Processing Systems_, 2019.
* [38] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng Cui. DIST: Rendering deep implicit signed distance function with differentiable sphere tracing. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2020.
* [39] Shi-Lin Liu, Hao-Xiang Guo, Hao Pan, Pengshuai Wang, Xin Tong, and Yang Liu. Deep implicit moving least-squares functions for 3D reconstruction. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2021.
* [40] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3D surface construction algorithm. _Computer Graphics_, 21(4):163-169, 1987.
* [41] Baorui Ma, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Neural-pull: Learning signed distance functions from point clouds by learning to pull space onto surfaces. In _International Conference on Machine Learning_, 2021.
* [42] Baorui Ma, Yu-Shen Liu, and Zhizhong Han. Learning signed distance functions from noisy 3d point clouds via noise to noise mapping. In _International Conference on Machine Learning (ICML)_, 2023.
* [43] Baorui Ma, Yu-Shen Liu, Matthias Zwicker, and Zhizhong Han. Surface reconstruction from point clouds by learning predictive context priors. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2022.
* [44] Baorui Ma, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Towards better gradient consistency for neural signed distance functions via level set alignment. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [45] Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon Wetzstein. ACORN: adaptive coordinate networks for neural scene representation. _CoRR_, abs/2105.02788, 2021.
* [46] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In _European Conference on Computer Vision_, 2020.
* [47] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _arXiv:2201.05989_, 2022.
* [48] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8280-8290, June 2022.

* [49] Richard A. Newcombe, Steven J. Lovegrove, and Andrew J. Davison. Dtam: Dense tracking and mapping in real-time. In _International Conference on Computer Vision_, pages 2320-2327, 2011.
* [50] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2020.
* [51] Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In _International Conference on Computer Vision_, 2021.
* [52] Joseph Ortiz, Alexander Clegg, Jing Dong, Edgar Sucar, David Novotny, Michael Zollhoefer, and Mustafa Mukadam. isdf: Real-time neural signed distance fields for robot perception. In _Robotics: Science and Systems_, 2022.
* [53] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. _IEEE International Conference on Computer Vision_, 2021.
* [54] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In _European Conference on Computer Vision_, 2020.
* [55] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv_, 2022.
* [56] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast multi-view reconstruction with implicit surfaces using permutohedral lattices. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [57] Darius Ruckert, Linus Franke, and Marc Stamminger. Adop: Approximate differentiable one-pixel point rendering. _arXiv:2110.06635_, 2021.
* [58] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023.
* [59] Erik Sandstrom, Kevin Ta, Luc Van Gool, and Martin R. Oswald. Uncle-slam: Uncertainty learning for dense neural slam, 2023.
* [60] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2022.
* [61] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2016.
* [62] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In _European Conference on Computer Vision_, 2016.
* [63] Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3D-structure-aware neural scene representations. In _Advances in Neural Information Processing Systems_, 2019.
* [64] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard A. Newcombe. The replica dataset: A digital replica of indoor spaces. _CoRR_, abs/1906.05797, 2019.
* [65] Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for the evaluation of rgb-d slam systems. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 573-580, 2012.
* [66] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davison. imap: Implicit mapping and positioning in real-time. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6229-6238, 2021.
* [67] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. _IEEE Conference on Computer Vision and Pattern Recognition_, 2021.
* [68] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2021.
* [69] Jiapeng Tang, Jiabao Lei, Dan Xu, Feiying Ma, Kui Jia, and Lei Zhang. SA-ConvONet: Sign-agnostic optimization of convolutional occupancy networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021.
* [70] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, and Gang Zeng. Delicate textured mesh recovery from nerf via adaptive surface refinement. _arXiv preprint arXiv:2303.02091_, 2022.
* [71] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. _Advances in neural information processing systems_, 2021.

* [72] Andreas L. Teigen, Yeonsoo Park, Annette Stahl, and Rudolf Mester. Rgb-d mapping and tracking in a plenoxel radiance field, 2023.
* [73] Delio Vicini, Sebastien Speierer, and Wenzel Jakob. Differentiable signed distance function rendering. _ACM Transactions on Graphics_, 41(4):125:1-125:18, 2022.
* [74] Hengyi Wang, Jingwen Wang, and Lourdes Agapito. Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam, 2023.
* [75] Jingwen Wang, Tymoteusz Bleja, and Lourdes Agapito. Go-surf: Neural feature grid optimization for fast, high-fidelity rgb-d surface reconstruction. In _International Conference on 3D Vision_, 2022.
* [76] Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian Theobalt, Taku Komura, Lingjie Liu, and Wenping Wang. NeuRIS: Neural reconstruction of indoor scenes using normal priors. In _European Conference on Computer Vision_, 2022.
* [77] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In _Advances in Neural Information Processing Systems_, pages 27171-27183, 2021.
* [78] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. HF-NeuS: Improved surface reconstruction using high-frequency details. 2022.
* [79] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiview compressive coding for 3d reconstruction, 2023.
* [80] Yunie Wu and Zhengxing Sun. DFR: differentiable function rendering for learning 3D generation from images. _Computer Graphics Forum_, 39(5):241-252, 2020.
* [81] Liu Xinyang, Li Yijin, Teng Yanbin, Bao Hujun, Zhang Guofeng, Zhang Yinda, and Cui Zhaopeng. Multi-modal neural radiance field for monocular dense slam with a light-weight tof sensor. In _International Conference on Computer Vision (ICCV)_, 2023.
* [82] Xiangyu Xu, Lichang Chen, Changjiang Cai, Huangying Zhan, Qingan Yan, Pan Ji, Junsong Yuan, Heng Huang, and Yi Xu. Dynamic voxel grid optimization for high-fidelity rgb-d supervised surface reconstruction, 2023.
* [83] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In _2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)_, Dec 2022.
* [84] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. _European Conference on Computer Vision_, 2018.
* [85] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In _Advances in Neural Information Processing Systems_, 2021.
* [86] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron, and Ben Mildenhall. Bakedsf: Meshing neural sdfs for real-time view synthesis. _arXiv_, 2023.
* [87] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. _Advances in Neural Information Processing Systems_, 33, 2020.
* [88] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. MonoSDF: Exploring monocular geometric cues for neural implicit surface reconstruction. _ArXiv_, abs/2022.00665, 2022.
* [89] Sergey Zakharov, Wadim Kehl, Arjun Bhargava, and Adrien Gaidon. Autolabeling 3D objects with differentiable rendering of sdf shape priors. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2020.
* [90] Andy Zeng, Shuran Song, Matthias Niessner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In _CVPR_, 2017.
* [91] Youmin Zhang, Fabio Tosi, Stefano Mattoccia, and Matteo Poggi. Go-slam: Global optimization for consistent 3d instant reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, October 2023.
* [92] Wenbin Zhao, Jiabao Lei, Yuxin Wen, Jianguo Zhang, and Kui Jia. Sign-agnostic implicit learning of surface self-similarities for shape modeling and reconstruction from raw point clouds. _CoRR_, abs/2012.07498, 2020.
* [93] Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, and Zhizhong Han. Learning a more continuous zero level set in unsigned distance fields through level set projection. In _Proceedings of the IEEE/CVF international conference on computer vision_, 2023.
* [94] Junsheng Zhou, Baorui Ma, Yu-Shen Liu, Yi Fang, and Zhizhong Han. Learning consistency-aware unsigned distance functions progressively from raw point clouds. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [95] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe. Unsupervised learning of depth and ego-motion from video. In _IEEE Conference on Computer Vision and Pattern Recognition_, pages 6612-6619, 2017.
* [96] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R. Oswald, Andreas Geiger, and Marc Pollefeys. NICER-SLAM: neural implicit scene encoding for RGB SLAM. _CoRR_, abs/2302.03594, 2023.

* [97] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2022.
* [98] Zi-Xin Zou, Shi-Sheng Huang, Yan-Pei Cao, Tai-Jiang Mu, Ying Shan, and Hongbo Fu. Mononeuralfusion: Online monocular neural 3d reconstruction with geometric priors. _CoRR_, abs/2209.15153, 2022.