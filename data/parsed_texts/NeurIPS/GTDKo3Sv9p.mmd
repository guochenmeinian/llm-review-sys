# Discrete Flow Matching

Itai Gat\({}^{1}\) &Tal Remez\({}^{1}\) &Neta Shaul\({}^{2}\) &Felix Kreuk\({}^{1}\) &Ricky T. Q. Chen\({}^{1}\)

&Gabriel Synnaeve\({}^{1}\) &Yossi Adi\({}^{1}\) &Yaron Lipman\({}^{1}\)

\({}^{1}\) Meta FAIR &Weizmann Institute of Science

###### Abstract

Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: _(i)_ it works with a general family of probability paths interpolating between source and target distributions; _(ii)_ it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser (\(x\)-prediction) and noise-prediction (\(\epsilon\)-prediction); _(iii)_ practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and _(iv)_ by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on _1-shot_ MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.

## 1 Introduction

Despite the remarkable success of diffusion and flow models in generating continuous spatial signals such as images (Ho et al., 2020; Rombach et al., 2022; Esser et al., 2024) and videos (Singer et al., 2022; Blattmann et al., 2023), their performance still falters when applied to discrete sequential data compared to autoregressive models. Recent progress in adapting diffusion and flow models to the discrete setting has been made via mostly two approaches: embedding the discrete data in continuous space and applying continuous diffusion (Dieleman et al., 2022; Stark et al., 2024) or designing diffusion or flow processes over discrete state spaces (Austin et al., 2021; Campbell et al., 2022).

In this paper, we pursue the discrete flow approach of Campbell et al. (2024) and introduce Discrete Flow Matching, a theoretical framework and algorithmic methodology for discrete flow models that yields a state-of-the-art discrete non-autoregressive generative approach. Surprisingly, Discrete Flow Matching exhibits similarities with the continuous Flow Matching (Lipman et al., 2022) approach proposed for continuous signals. Notably, its _generating probability velocity_, employed in the sampling algorithm, is identical in form to its continuous counterpart. Additionally, Discrete Flow Matching offers the following advancements and simplifications over prior methods: It encompasses a more comprehensive family of probability paths transforming source (noise) distributions into target (data) distributions, accommodating arbitrary source-target couplings and time-dependent schedulers. Furthermore, it provides a unified formulation for the generating probability velocity directly expressed in terms of the learned posteriors and schedulers, along with a unified and general theory and algorithm for corrector sampling and iterations. In practice, we observe that path and corrector schedulers are pivotal, and their proper tuning leads to substantial improvements ingeneration quality. We have trained a 1.7B parameter Discrete Flow Matching model on the same data mix as in Llama-2 (Touvron et al., 2023) and CodeLlama (Roziere et al., 2023), achieving 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on _1-shot_ MBPP coding benchmarks; Figure 1 shows some code generation examples. In conditional text generation our model produces texts with a generative perplexity score of 9.7 as measured by the Llama-3 BB model, surpassing a 1.7B autoregressive model that achieves 22.3 and not far from the Llama-2 7B model that achieves 8.3 in generative perplexity score. We strongly believe that Discrete Flow Matching represents a significant step in bridging the performance gap between discrete diffusion and autoregressive models, and that further enhancements are possible by exploring the vast design space that Discrete Flow Matching has to offer.

## 2 Discrete Flow Matching

### Setup and notations

In discrete sequence modeling, we denote a sequence \(x\) as an array of \(N\) elements \((x^{1},x^{2},\ldots,x^{N})\). Each element, or _token_, within this sequence is selected from a vocabulary of size \(d\). Consequently, the entire set of possible sequences is given by \(\mathcal{D}=[d]^{N}\), where \([d]=\{1,\ldots,d\}\). A random variable taking values in the space \(\mathcal{D}\) is denoted by \(X\) and its corresponding probability mass function (PMF) is \(P(X=x)\). For simplicity, throughout the paper, we sometimes omit the random variable \(X\) and use \(p(x)\) to denote the PMF.

To describe marginalization properties, we denote \(p(x^{i})\) the \(x^{i}\) marginal of \(p\), i.e., \(p(x^{i})=\sum_{x^{i}}p(x)\), where \(x^{\bar{i}}=(\ldots,x^{i-1},x^{i+1},\ldots)\in[d]^{N-1}\) are all the arguments excluding \(i\). Similarly, \(p(x^{i})=\sum_{x^{i}}p(x)\), and \(x^{i}\in[d]\). A useful PMF is the delta function, \(\delta_{y}\), \(y\in\mathcal{D}\), which is defined by

\[\delta_{y}(x)=\prod_{i=1}^{N}\delta_{y^{i}}(x^{i}),\text{ where }\delta_{y^{i}}(x^{i})= \begin{cases}1&x^{i}=y^{i}\\ 0&x^{i}\neq y^{i}\end{cases}. \tag{1}\]

With the marginal notation \(\delta_{y}(x^{i})=\delta_{y^{i}}(x^{i})\) and \(\delta_{y}(x^{\bar{i}})=\delta_{y^{i}}(x^{\bar{i}})=\prod_{j\neq i}\delta_{y^ {j}}(x^{j})\) which simplifies notation.

### Source and target distributions

In discrete generative models our goal is to transform source samples \(X_{0}\sim p\) to target samples \(X_{1}\sim q\). Our training data, consist of pairs \(X_{0}\) and \(X_{1}\) that are sampled from a joint distribution \(\pi(x,y)\), satisfying the marginals constraints \(p(x)=\sum_{y\in\mathcal{D}}\pi(x,y),q(y)=\sum_{x\in\mathcal{D}}\pi(x,y)\), i.e.,

\[(X_{0},X_{1})\sim\pi(X_{0},X_{1}). \tag{2}\]

Figure 1: **Code generation examples using Discrete Flow Matching. Code condition is marked in gray, model generation is marked in yellow. Left sub-figure presents the standard left-to-right prompting; Middle and Right sub-figures, presents complex infilling setup.**

In the simplest case, the training pairs \(X_{0}\) and \(X_{1}\) are sampled independently from the source and target distributions respectively,

\[(X_{0},X_{1})\sim p(X_{0})q(X_{1}). \tag{3}\]

_Example:_ **source and couplings.** Common instantiations of source distribution \(p\) are: (i) adding a special token value often referred to as a'mask' or 'dummy' token, denoted here by \(\mathbb{m}\), and setting the source distribution to be all-mask sequences, i.e., \(p(x)=\delta_{\mathbb{m}}(x)\); and (ii) using uniform distribution over \(\mathcal{D}\), which is equivalent to drawing each \(x^{i}\) independently to be some value in \([d]\) with equal probability, denoted \(p(x)=p_{\text{u}}(x)\). In this paper we focus mainly on (i). We further consider two choices of couplings \(\pi\): Independent coupling, which we call unconditional coupling (U-coupling), \(\pi(x_{0},x_{1})=p(x_{0})q(x_{1})\). A random sample that realizes this choice have the form

\[(X_{0},X_{1})=\Big{(}(\mathbb{m},\ldots,\mathbb{m}),X_{1}\Big{)}, \tag{4}\]

where \(X_{1}\sim q(X_{1})\) is a random sample from the training set. The second choice of coupling \(\pi(x_{0},x_{1})=p(x_{0}|x_{1})q(x_{1})\), which we find improves conditional sampling, partially masks inputs with samples of the form

\[(X_{0},X_{1})=(\mathbb{1}\odot X_{1}+(\mathbf{1}-\mathbb{1})\odot(\mathbb{m}, \ldots,\mathbb{m}),X_{1}), \tag{5}\]

where \(X_{1}\sim q(X_{1})\) and \(\mathbb{1}\in\{0,1\}^{N}\) is a random variable indicating the conditioning, \(\odot\) denotes the entry-wise product, and \(\mathbf{1}\in\mathbb{R}^{N}\) is the vector of all ones. We call this conditional coupling (C-coupling).

### Probability paths

We follow the Flow Matching approach (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijnden, 2022) that uses a predefined _probability path_\(p_{t}\) interpolating \(p\) and \(q\), i.e.,

\[p_{0}=p\quad\text{ and }\quad p_{1}=q \tag{6}\]

to train the generative model taking a source sample \(X_{0}\sim p\) to a target sample \(X_{1}\sim q\). We use arbitrary coupling of source and target (Pooladian et al., 2023; Tong et al., 2023), \(\pi(x_{0},x_{1})\), and the symmetric Flow Matching path (Albergo and Vanden-Eijnden, 2022) to define the marginal probability path,

\[p_{t}(x)=\sum_{x_{0},x_{1}\in\mathcal{D}}p_{t}(x|x_{0},x_{1})\pi(x_{0},x_{1}), \text{ where }p_{t}(x|x_{0},x_{1})=\prod_{i=1}^{N}p_{t}(x^{i}|x_{0},x_{1}), \tag{7}\]

and \(p_{t}(x^{i}|x_{0},x_{1})\) is a time-dependent probability on the space of tokens \([d]\) conditioned on the pair \(x_{0},x_{1}\), and satisfying \(p_{0}(x^{i}|x_{0},x_{1})=\delta_{x_{0}}(x^{i})\) and \(p_{1}(x^{i}|x_{0},x_{1})=\delta_{x_{1}}(x^{i})\). If the conditional path \(p_{t}(x^{i}|x_{0},x_{1})\) satisfies these boundary conditions then the marginal path \(p_{t}(x)\) satisfies equation 6.

In developing the framework, we would like to consider as general as possible set of probability paths that are also tractable to learn within the Flow Matching framework. We consider conditional probability paths as a convex sum of \(m\) conditional probabilities \(w^{j}(x^{i}|x_{0},x_{1})\), i.e.,

\[p_{t}(x^{i}|x_{0},x_{1})=\sum_{j=1}^{m}\kappa_{t}^{i,j}w^{j}(x^{i}|x_{0},x_{1}), \tag{8}\]

where \(\sum_{j}\kappa_{t}^{i,j}=1\) and \(\kappa_{t}^{i,j}\geq 0\) are collectively called the _scheduler_. Note that the scheduler can be defined independently for each location in the sequence \(i\in[N]\) or uniformly for all tokens, \(\kappa_{t}^{i,j}=\kappa_{t}^{j}\).

A simple yet useful instance of these conditional paths is reminiscent of the continuous Flow Matching paths formulated as convex interpolants,

\[p_{t}(x^{i}|x_{0},x_{1})=(1-\kappa_{t})\delta_{x_{0}}(x^{i})+\kappa_{t}\delta_ {x_{1}}(x^{i}), \tag{9}\]

where the scheduler \(\kappa_{t}\) satisfies \(\kappa_{0}=0\), \(\kappa_{1}=1\), and monotonically increasing in \(t\). Another interesting instantiation of equation 8 is adding uniform noise with some probability depending on \(t\),

\[p_{t}(x^{i}|x_{0},x_{1})=\kappa_{t}^{1}\delta_{x_{1}}(x^{i})+\kappa_{t}^{2}p_{ a}(x^{i})+\kappa_{t}^{3}\delta_{x_{0}}(x^{i}), \tag{10}\]

where \(\kappa_{0}^{1}=0\), \(\kappa_{1}^{1}=1\), \(\kappa_{0}^{2}=\kappa_{1}^{2}=0\) (remembering that \(\sum_{j}\kappa_{t}^{i,j}=1\) and \(\kappa_{t}^{i,j}\geq 0\)).

### Generating Probability Velocities

**Continuous generating velocity.** Sampling in continuous Flow Matching is performed by updating the current (continuous) sample \(X_{t}\in\mathbb{R}^{N}\), \(t\in[0,1)\), according to a learned _generating velocity field_\(u^{i}_{t}(X_{t})\), \(i\in[N]\). Euler sampling follows the (deterministic) rule

\[X^{i}_{t+h}=X^{i}_{t}+hu^{i}_{t}(X_{t}), \tag{11}\]

where \(h>0\) is a user-defined time step. Note that equation 11 is updating separately each of the sample coordinates, \(X^{i}_{t}\), \(i\in[N]\), see e.g., Figure 2, left. The velocity \(u^{i}_{t}(X_{t})\) can be either directly modeled with a neural network, or parameterized via the _denoiser_ (a.k.a. \(x\)-prediction) or _noise-prediction_ (a.k.a. \(\varepsilon\)-prediction), see left column in Table 1. If, for all \(t\in[0,1)\), starting at \(X_{t}\sim p_{t}\) and sampling with equation 11 provides \(X_{t+h}\sim p_{t+h}+o(h)\)1 then we say that \(u_{t}\)_generates_\(p_{t}\).

Footnote 1: The \(o(h^{t})\) notation means a function going to zero faster than \(h^{\ell}\) as \(h\to 0\), i.e., \(\frac{o(h^{\ell})}{h^{\ell}}\stackrel{{ h\to 0}}{{ \longrightarrow}}0\).

Generating probability velocity.For defining Flow Matching in the discrete setting, we follow Campbell et al. (2024) and consider a Continuous-Time discrete Markov Chain (CTMC) paradigm, namely the sample \(X_{t}\) is jumping between states in \(\mathcal{D}\), depending on a continuous time value \(t\in[0,1]\). Similar to the continuous Flow Matching setting described above, we focus on a model that predicts the rate of probability change of the current sample \(X_{t}\) in each of its \(N\) tokens, see Figure 2, middle-left. Then, each token of the sample \(X_{t}\sim p_{t}\) is updated independently by

\[X^{i}_{t+h}\sim\delta_{X^{i}_{t}}(\cdot)+hu^{i}_{t}(\cdot,X_{t}), \tag{12}\]

where we call \(u_{t}\) the _probability velocity_ as reminiscent of the velocity field in continuous Flow Matching, and as in the continuous case, we define:

**Definition 1**.: Probability velocity \(u_{t}\)_generates_ the probability path \(p_{t}\) if, for all \(t\in[0,1)\) and given a sample \(X_{t}\sim p_{t}\), the sample \(X_{t+h}\) defined in equation 12 satisfies \(X_{t+h}\sim p_{t+h}+o(h)\).

Algorithm 1 formulates a basic sampling algorithm given a generating probability velocity \(u_{t}\). In order for the r.h.s. of equation 12 to define a proper PMF for sufficiently small \(h>0\), it is necessary and sufficient that the probability velocity satisfies the conditions

\[\sum_{x^{i}\in[d]}u^{i}_{t}(x^{i},z)=0,\text{ and }u^{i}_{t}(x^{i},z)\geq 0 \text{ for all }i\in[N]\text{ and }x^{i}\neq z^{i}. \tag{13}\]

```
0: velocity \(u_{t}\), sample \(X\sim p\), step size \(h=\frac{1}{n}\) for\(t=0,h,2h,\ldots,1-h\)do \(X^{i}\sim\delta_{X^{i}}(\cdot)+hu^{i}_{t}(\cdot,X)\), for \(i\in[N]\)\(\triangleright\) eq. 24 or 22 endfor return\(X\)
```

**Algorithm 1** Flow Matching sampling.

Figure 2: Discrete flow in \(\mathcal{D}=[d]^{N}\) with \(d=4,N=2\) (middle-left) versus continuous flow in \(\mathbb{R}^{N}\), \(N=2\) (left). The rate of change of probability of a state (gray disk) is given by the divergence operator shown in the continuous case (middle right) and the discrete case (right).

**Theorem 2**.: _Given a conditional probability velocity \(u_{t}^{i}(x^{i},z|x_{0},x_{1})\) generating a conditional probability path \(p_{t}(x|x_{0},x_{1})\), the marginal velocity defined by_

\[u_{t}^{i}(x^{i},z)=\sum_{x_{0},x_{1}\in\mathcal{D}}u_{t}^{i}(x^{i},z|x_{0},x_{1} )p_{t}(x_{0},x_{1}|z) \tag{14}\]

_generates the marginal probability path \(p_{t}(x)\), where by Bayes' rule_

\[p_{t}(x_{0},x_{1}|z)=\frac{p_{t}(z|x_{0},x_{1})\pi(x_{0},x_{1})}{p_{t}(z)}. \tag{15}\]

For completeness we provide a simple proof of this theorem in Appendix E.2. The proof, similar to the continuous Flow Matching case, shows that \(u_{t}\) and \(p_{t}\) satisfy the (discrete version of the) Continuity Equation.

The Continuity Equation.To provide the mathematical tool for showing that a probability velocity \(u_{t}\) does indeed generate the probability path \(p_{t}\), and also to further highlight the similarities to the continuous case, we next formulate the _Kolmogorov Equations_, which describe the state probability rate \(\dot{p}_{t}(x)\), \(x\in\mathcal{D}\), in CTMC as a Continuity Equation (CE). The Continuity Equation, similarly to Kolmogorov Equations, describes \(\dot{p}_{t}(x)\), \(x\in\mathbb{R}^{N}\) in the _continuous case_, and is formulated as the Partial Differential Equation (PDE)

\[\dot{p}_{t}(x)+\operatorname{div}_{x}(p_{t}u_{t})=0, \tag{16}\]

where the divergence operator \(\operatorname{div}_{x}(v)\) applied to a vector field \(v:\mathbb{R}^{N}\to\mathbb{R}^{N}\) is defined by

\[\operatorname{div}_{x}(v)=\sum_{i=1}^{N}\partial_{x^{i}}v^{i}(x), \tag{17}\]

and intuitively means the total flux leaving \(x\), see Figure 2 (middle-right). This gives an intuitive explanation to the Continuity Equation: the rate of the probability \(\dot{p}_{t}(x)\) of a state \(x\in\mathbb{R}^{N}\) equals the total _incoming probability flux_, \(p_{t}u_{t}\), at \(x\). In the discrete case (CTMC) the Continuity Equation (equation 16) holds as is, once the discrete divergence operator is properly defined, i.e., to measure the outgoing flux from a discrete state. In more detail, given some vector field, which in the discrete case is a scalar-valued function over pairs of states, \(v:\mathcal{D}\times\mathcal{D}\to\mathbb{R}\), the discrete divergence is

\[\operatorname{div}_{x}(v)=\sum_{z\in\mathcal{D}}\left[v(z,x)-v(x,z)\right], \tag{18}\]

where \(v(z,x)\) represents the flux \(x\to z\) and \(v(x,z)\) represent the opposite flux \(z\to x\); see Figure 2, right. Now, in our case (see Figure 2, middle-left), the probability flux at a state \(x\in\mathcal{D}\) involves all sequences with at most one token difference from \(x\), i.e., the probability flux \(p_{t}u_{t}\) at \(x\) takes the form \(v(x,z)=p_{t}(z)u_{t}^{i}(x^{i},z)\) and \(v(z,x)=p_{t}(x)u_{t}^{i}(z^{i},x)\) for \(z\) and \(x\) that differ only in the \(i\)-th token, \(v(x,x)=p_{t}(x)\sum_{i=1}^{N}u_{t}^{i}(x^{i},x)\), and \(v(x,z)=0\) for all other \((z,x)\in\mathcal{D}\times\mathcal{D}\). A direct calculation now shows (see Appendix E.1):

\[\operatorname{div}_{x}(p_{t}u_{t})=-\sum_{z\in\mathcal{D}}p_{t}(z)\left[\sum_{ i=1}^{N}\delta_{z}(x^{\bar{i}})u_{t}^{i}(x^{i},z)\right]. \tag{19}\]

Checking that a probability velocity \(u_{t}\) generates a probability path \(p_{t}\) (in the sense of Definition 1) amounts to verifying the Continuity Equation (equation 16). Indeed, using arguments from Campbell et al. (2024) and the discrete divergence operator, the PMF of \(X_{t+h}\) defined by sampling according to equation 12 is

\[\begin{split}\mathbb{E}_{X_{t}}&\prod_{i=1}^{N} \left[\delta_{X_{t}}(x^{i})+hu_{t}^{i}(x^{i},X_{t})\right]=\mathbb{E}_{X_{t}} \left[\delta_{X_{t}}(x)+h\sum_{i=1}^{N}\delta_{X_{t}}(x^{\bar{i}})u_{t}^{i}(x^ {i},X_{t})\right]+o(h)\\ &\qquad=p_{t}(x)-h\operatorname{div}_{x}(p_{t}u_{t})+o(h) \overset{\eqref{eq:p_t}}{=}p_{t}(x)+h\dot{p}_{t}(x)+o(h)=p_{t+h}(x)+o(h),\end{split} \tag{20}\]

where we assume \(X_{t}\sim p_{t}\), the first equality uses the identity \(\prod_{i}\left[a^{i}+hb^{i}\right]=\prod_{i}a^{i}+h\sum_{i}(\prod_{j\neq i}a^{j })b^{i}+o(h)\), the second equality uses equation 19, and the previous-to-last equality uses the Continuity Equation (equation 16). This shows that if the Continuity Equation holds then \(u_{t}\) generates \(p_{t}\) in the sense of Definition 1.

**Conditional and marginal generating velocities.** We provide the probability velocities generating the conditional probability paths \(p_{t}(x|x_{0},x_{1})\) defined in equations 7 and 8. Then, using the marginalization formula in equation 14 we end up with a closed-form marginal velocity for the probability paths \(p_{t}(x)\). In Appendix E.3 we show

**Theorem 3** (Probability velocity of conditional paths).: _A generating probability velocity for the conditional paths \(p_{t}(x|x_{0},x_{1})\) defined in equations 7 and 8 is_

\[u^{i}_{t}(x^{i},z|x_{0},x_{1})=\sum_{j=1}^{m}a^{i,j}_{t}w^{j}(x^{i}|x_{0},x_{1} )+b^{i}_{t}\delta_{z}(x^{i}), \tag{21}\]

_with \(a^{i,j}_{t}=\dot{\kappa}^{i,j}_{t}-\kappa^{i,j}_{t}\dot{\kappa}^{i,\ell}_{t}/ \kappa^{i,\ell}_{t}\), and \(b^{i}_{t}=\dot{\kappa}^{i,\ell}_{t}/\kappa^{i,\ell}_{t}\) where \(\ell=\arg\min_{j\in[m]}\left[\dot{\kappa}^{i,j}_{t}/\kappa^{i,j}_{t}\right]\)._

Now, computing the marginal probability velocity using equation 14 applied to the conditional probability velocity in equation 21 gives

\[u^{i}_{t}(x^{i},z)=\sum_{j=1}^{m}a^{i,j}_{t}\hat{w}^{j}_{t}(x^{i},z)+b^{i,j}_{ t}\delta_{z}(x^{i}), \tag{22}\]

where the posteriors \(\hat{w}^{j}_{t}\) of \(w^{j}\) (that are later shown to be tractable to learn) are defined by

\[\hat{w}^{j}_{t}(x^{i},z)=\sum_{x_{0},x_{1}\in\mathcal{D}}w^{j}(x^{i}|x_{0},x_{ 1})p_{t}(x_{0},x_{1}|z), \tag{23}\]

where \(p_{t}(x_{0},x_{1}|z)\) (defined in equation 15) is the posterior probability of \(x_{0},x_{1}\) conditioned on the current state \(X_{t}=z\). A useful instantiation of the general velocity in equation 22 is when considering the path family in equation 9, for which \(w^{1}(x^{i}|x_{0},x_{1})=\delta_{x_{1}}(x^{i})\), \(w^{2}(x^{i}|x_{0},x_{1})=\delta_{x_{0}}(x^{i})\), \(\kappa^{i,1}_{t}=\kappa_{t}\), \(\kappa^{i,2}_{t}=1-\kappa_{t}\), \(\dot{\kappa}_{t}\geq 0\) (i.e., monotonically non-decreasing in \(t\)) and in this case equation 22 reads as

\[u^{i}_{t}(x^{i},z)=\frac{\dot{\kappa}_{t}}{1-\kappa_{t}}\left[p_{1|t}(x^{i}|z)- \delta_{z}(x^{i})\right] \tag{24}\]

where we use the notation \(p_{1|t}(x^{i}|z)=\sum_{x_{0},x_{1}}\delta_{x_{1}}(x^{i})p_{t}(x_{0},x_{1}|z)\) for the _probability denoiser_.

Sampling backward in time.We can also sample _backwards in time_ by following the sampling rule \(X^{i}_{t-h}\sim\delta_{X^{i}_{l}}(\cdot)-hu^{i}_{t}(\cdot,X_{t})\). In this case \(-u^{i}_{t}(x^{i},z)\) should satisfy equation 13. A (backward-time) generating probability velocity can then be achieved from equation 22 with the simple change to the coefficients \(a^{i,j}_{t}\) and \(b^{i,j}_{t}\), see Appendix E.4. For \(p_{t}\) defined with equation 9 the generating velocity is

\[u^{i}_{t}(x^{i},z)=\frac{\dot{\kappa}_{t}}{\kappa_{t}}\left[\delta_{z}(x^{i})-p _{0|t}(x^{i}|z)\right], \tag{25}\]

where in this case \(p_{0|t}(x^{i}|z)=\sum_{x_{0},x_{1}\in\mathcal{D}}\delta_{x_{0}}(x^{i})p_{t}(x_ {0},x_{1}|z)\) is the _probability noise-prediction_.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & \multicolumn{2}{c}{**Continuous** Flow Matching} & \multicolumn{1}{c}{**Discrete** Flow Matching} \\ \hline Marginal prob. & & \(p_{t}(x)=\sum_{x_{0},x_{1}}\prod_{i=1}^{N}p_{t}(x^{i}|x_{0},x_{1})\pi(x_{0},x_{1})\) \\ Conditional prob. & & \(p_{t}(x^{i}|x_{0},x_{1})=\delta_{\kappa_{t}x_{1}+(1-\kappa_{t})x_{0}}(x^{i})\) & \(p_{t}(x^{i}|x_{0},x_{1})=\kappa_{t}\delta_{x_{1}}(x^{i})+(1-\kappa_{t})\delta_ {x_{0}}(x^{i})\) \\ \hline VF-_Denoiser_ & & \(u^{i}_{t}(X_{t})=\frac{\dot{\kappa}_{t}}{1-\kappa_{t}}\left[\dot{\kappa}^{i}_{ 1|t}(X_{t})-X^{i}_{1}\right]\) & \(u^{i}_{t}(x^{i},X_{t})=\frac{\dot{\kappa}_{t}}{1-\kappa_{t}}\left[p_{1|t}(x^{i} |X_{t})-\delta_{X_{t}}(x^{i})\right]\) \\ VF-_Noise-pred_ & & \(u^{i}_{t}(X_{t})=\frac{\dot{\kappa}_{t}}{\kappa_{t}}\left[X^{i}_{t}-\dot{x}^{i}_ {0|t}(X_{t})\right]\) & \(u^{i}_{t}(x^{i},X_{t})=\frac{\dot{\kappa}_{t}}{\kappa_{t}}\left[\delta_{X_{t}}(x ^{i})-p_{0|t}(x^{i}|X_{t})\right]\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Generating (marginal) velocity fields have identical form for the continuous and discrete Flow Matching when using denoiser/noise-prediction parameterization; \(\dot{x}_{1|t}(z)=\mathbb{E}_{X_{1}\sim p_{t}(\cdot|z)}X_{1}\) is the standard continuous denoiser (a.k.a. \(x\)-prediction) and \(\hat{x}_{0|t}(z)=\mathbb{E}_{X_{0}\sim p_{t}(\cdot|z)}X_{0}\) is the standard noise-prediction (a.k.a. \(\epsilon\)-prediction).

Remarkably, the generating velocity fields in 24 and 25 take the _exact same form_ as the generating (a.k.a. marginal) velocity fields in _continuous_ flow matching when parameterized via the denoiser or noise-prediction parameterizations and using the same schedulers, see Table 1 and Appendix E.9 for explanation of the continuous case. In Appendix E.4 we provide the backward-time version of Theorem 3.

Corrector sampling.Combining the forward-time \(\hat{u}_{t}\) ( equation 24) and backward-time \(\tilde{u}_{t}\) ( equation 25), i.e.,

\[\bar{u}_{t}^{i}(x^{i},z)=\alpha_{t}\hat{u}_{t}^{i}(x^{i},z)-\beta_{t}\check{u }_{t}^{i}(x^{i},z), \tag{26}\]

provides a valid forward-time probability velocity field (i.e., satisfies equation 13) for \(t\in(0,1)\) as long as \(\alpha_{t},\beta_{t}>0\). This velocity field can be used for two types of corrector sampling: (i) When \(\alpha_{t}-\beta_{t}=1\) sampling with \(\bar{u}_{t}\) leads to _corrector sampling_ where intuitively each step moves \(1+\beta_{t}\) forward in time and \(-\beta_{t}\) backwards, which allows reintroducing noise into the sampling process; and (ii) when \(\alpha_{t}-\beta_{t}=0\) sampling with \(\bar{u}_{t}\) when fixing \(t\in(0,1)\) leads to _corrector iterations_ where limit samples distribute according to \(p_{t}\). In Appendix E.6 we prove:

**Theorem 4**.: _For perfectly trained posteriors and \(\alpha_{t},\beta_{t}>0\), \(t\in(0,1)\), \(\bar{u}_{t}\) in equation 26 is a probability velocity, i.e., satisfies equation 13, and: (i) For \(\alpha_{t}-\beta_{t}=1\), \(\bar{u}_{t}\) provides a probability velocity generating \(p_{t}\); (ii) For \(\alpha_{t}-\beta_{t}=0\), repeatedly sampling with \(\bar{u}_{t}\) at fixed \(t\in(0,1)\) and sufficiently small \(h\) is guaranteed to converge to a sample from \(p_{t}\)._

One simplification to equation 26 can be done in the case of paths constructed with conditional as in equation 9, independent coupling \(\pi(x_{0},x_{1})=p(x_{0})q(x_{1})\), and i.i.d. source \(p(x_{0})=\prod_{i=1}^{N}p(x_{0}^{i})\), e.g., \(p(x_{0}^{i})\) is uniform over \([d]\) or \(\delta_{\text{m}}(x_{0}^{i})\). In this case, the backward-time formula in equation 25 take an equivalent simpler form

\[\tilde{u}_{t}^{i}(x^{i},z)=\frac{\dot{\kappa}_{t}}{\kappa_{t}}\left[\delta_{z} (x^{i})-p(x^{i})\right], \tag{27}\]

which does not require estimation of the posterior \(p_{0|t}\). See Appendix E.5 for the derivation.

Training.Equation 22 shows that for generating samples from a probabilty path \(p_{t}(x)\) we require the posteriors \(\hat{w}_{t}^{j}(x^{i}|X_{t})\). Training such posteriors can be done by minimizing the loss

\[\mathcal{L}(\theta)=-\sum_{j\in[m],i\in[N]}\mathbb{E}_{t,(X_{0},X_{1}),X_{t},Y _{j}^{i}}\log\hat{w}_{t}^{j}(Y_{j}^{i}|X_{t};\theta), \tag{28}\]

where \(t\) is sampled according to some distribution in \([0,1]\) (we used uniform), \((X_{0},X_{1})\sim\pi(X_{0},X_{1})\), \(X_{t}\sim p_{t}(X_{t}|X_{0},X_{1})\), and \(Y_{j}^{i}\sim w^{j}(Y_{j}^{i}|X_{0},X_{1})\); \(\theta\in\mathbb{R}^{p}\) denotes the learnable parameters. In the common case we use in this paper of learning a single posterior, i.e., the probability denoiser \(p_{1|t}\), the loss takes the form \(\mathcal{L}(\theta)=-\sum_{i\in[N]}\mathbb{E}_{t,(X_{0},X_{1}),X_{t}}\log p_{1| t}(X_{1}^{i}|X_{t})\). In Appendix E.7 we prove:

**Proposition 5**.: _The minimizer of \(\mathcal{L}\) (equation 28) is \(\hat{w}_{t}^{j}(x^{i}|X_{t})\) (equation 23)._

## 3 Related work

In the section we cover the most related work to ours; in Appendix A we cover other related work.

Discrete Flows (Campbell et al., 2024) is probably the most related work to ours. We build upon their CTMC framework and offer the following generalizations and simplifications: We consider arbitrary couplings \((X_{0},X_{1})\), and offer a novel and rather general family of probability paths (equation 8) for which we provide the generating probability velocities in a unified closed-form formula (equations 22-25). These in particular recreate the same formulas as the continuous Flow Matching counterpart (Table 1). We furthermore develop a general corrector velocity (equation 26) that unifies both corrector iterations (Song et al., 2020; Campbell et al., 2022) and stochastic sampling of Campbell et al. (2024). We show that particular choices of noise schedulers \(\kappa_{t}\) (\(\kappa_{t}=t\) reproduces Campbell et al. (2024)) and corrector schedulers provide a boost in results. Lastly, we opted for the term _probability velocity_ for \(u_{t}^{i}(x^{i},X_{t})\) as it is not precisely a rate matrix in the state space \(\mathcal{D}\times\mathcal{D}\) used in CTMC since \(u_{t}^{i}(x^{i},z)\) for all \(i\in[N]\) define multiple self-edges \(z\to z\).

Masked modeling (Ghazvininejad et al., 2019; Chang et al., 2022).In case of a masked model, i.e., when the source distribution is \(p(x)=\delta_{\text{m}}(x)\), we achieve an interesting connection with MaskGitshowing it is actually an instance of Discrete Flow Matching with a small yet crucial change to its sampling algorithm. First, in Appendix E.8 we prove that in the masked setting, the probability denoiser \(p_{1|t}\) is _time-independent_:

**Proposition 6**.: _For paths defined by equations 7 and 9 with source \(p(x)=\delta_{m}(x)\) the posterior \(p_{t}(x_{0},x_{1}|z)=p(x_{0},x_{1}|z)\) is time-independent. Consequently, the probability denoiser \(p_{1|t}(x^{i}|z)=p_{1}(x^{i}|z)\) is also time-independent._

This shows that the probability denoiser can be learned with no time dependence, similar to the unmasking probabilities in MaskGit. During sampling however, there are two main differences between our sampling and MaskGit sampling. First, unmasking of tokens in our algorithm is done according to the probability \(\delta_{X_{t}}(x^{i})+hu_{t}^{i}(x^{i},X_{t})\)_independently_ for each token \(x^{i}\), \(i\in[N]\). This procedure is justified as it samples from the correct probability asymptotically via the derivation of the Continuity Equation 20. This is in contrast to MaskGit that prioritizes the token to be unmasked according to some _confidence_. In the experiments section we show that MaskGit's prioritization, although has some benefit in the very low NFE regime, is actually introducing a strong bias in the sampling procedure and leads to inferior overall results. Secondly, using corrector sampling allows for reintroducing masks to already unmasked tokens in a way that is still guaranteed to produce samples from \(p_{t}\), see Theorem 4; we find this to have a significant positive effect on the generation quality.

**Discrete diffusion.** D3PM (Austin et al., 2021) and Argmax flows (Hoogeboom et al., 2021) introduced diffusion in discrete spaces by proposing a corruption process for categorical data. A later work by Campbell et al. (2022) introduced discrete diffusion models with continuous time, and Lou et al. (2023) proposed learning probability ratios, extending score matching (Song and Ermon, 2019) to discrete spaces.

## 4 Experiments

We evaluate our method on the tasks of language modeling, code generation, and image generation. For language modeling, we compare the proposed method against prior work considering the widely used generative perplexity metric. We scale the models to 1.7 billion parameters and present results on coding tasks, i.e., HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), demonstrating

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & NFE & Llama-2\(\downarrow\) & Llama-3\(\downarrow\) & GPT2\(\downarrow\) & Entropy \\ \hline Data & - & 7.0 & 9.4 & 14.7 & 7.7 \\ \hline Autoregressive & 1024 & 31.4 & 54.8 & 45.3 & 7.1 \\ Savinov et al. (2021) & 200 & 29.5 & 45.1 & 34.7 & 5.2 \\ Austin et al. (2021) & 1000 & 697.6 & 768.8 & 837.8 & 7.6 \\ Han et al. (2022) & \textgreater{}10000 & 73.3 & 203.1 & 99.2 & 4.8 \\ Lou et al. (2023) & 256/512/1024 & 38.6/33.7/27.2 & 69.2/58.6/43.9 & 64.3/53.4/40.5 & 7.8/7.7/7.6 \\ Campbell et al. (2024) & 256/512/1024 & 38.5/33.5/28.7 & 69.0/56.5/46.5 & 65.2/53.3/43.0 & 7.8/7.7/7.6 \\
**FM (equation 9)** & 256/512/1024 & 34.2/30.0/22.5 & 58.5/48.8/33.8 & 54.2/43.5/29.3 & 7.7/7.6/7.2 \\
**FM (equation 10)** & 256/512/1024 & 30.0/27.5/22.3 & 48.2/43.5/31.9 & 47.7/41.8/28.1 & 7.6/7.5/7.1 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Generative perplexity on unconditional text generation compared to prior work. All models are sampled without the use of temperature or corrector steps. Double precision sampling results are reported in Table 5.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Model Size & NFE & Llama-2\(\downarrow\) & Llama-3\(\downarrow\) & Entropy \\ \hline Llama-3 (Reference) & 8B & 512 & 6.4 & 7.3 & 6.8 \\ Llama-2 (Reference) & 7B & 512 & 5.3 & 8.3 & 7.1 \\ \hline Autoregressive & 1.7B & 512 & 14.3 & 22.3 & 7.2 \\ Savinov et al. (2021) & 1.7B & 200 & 10.8 & 15.4 & 4.7 \\
**FM (U-coupling)** & 1.7B & 256/512 & 10.7/9.5 & 11.2/10.3 & 6.7/6.7 \\
**FM (C-coupling)** & 1.7B & 256/512 & 10.2/8.9 & 10.0/9.7 & 6.8/6.7 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Generative perplexity on conditional text generation.

the most promising results to date in a non-autoregressive context. In image generation, we present results for a fully discrete CIFAR10 (Krizhevsky et al., 2009). Further details of the experimental setup for each model are provided in Appendix G.

**Experimental setup.** In our experiments we used the masked source, i.e., \(p=\delta_{m}\), and trained with both unconditional coupling (U-coupling, equation 4) and conditional couplings (C-coupling, equation 5) with the probability path defined in equations 7, 9 and in one case 10. We trained a probability denoiser (loss in equation 28) and sampled using the generating velocity in equation 24 and Algorithm 1. We used a particular choice of probability path scheduler \(\kappa_{t}\), as well as corrector steps defined by a scheduler \(\alpha_{t}\) and temperature annealing. We found the choice of these schedulers to be pivotal for the model's performance. In Appendix D we perform an ablation study, evaluating various scheduler choices.

### Language modeling

We experimented with our method in three settings: (i) Small model (150M parameters) - comparison to other non-autoregressive baselines in unconditional text generation; (ii) Large model (1.7B parameters) - comparison to autoregressive models in conditional text generation; and (iii) Large model (1.7B parameters) - conditional code generation. As computing exact likelihood for non-autoregressive model is a challenge, for (i),(ii) we use the generative perplexity metric (Appendix G measured with GPT2 (Radford et al., 2019), Llama-2 (Touvron et al., 2023), and Llama-3, and we also monitor the sentence entropy (Appendix G) to measure diversity of tokens and flag repetitive sequences, which typically yield low perplexity. Throughout our experiments we noticed entropy \(\geq 6\) usually corresponds to diverse texts. For (iii) we evaluated using the success rate of coding tasks.

**Evaluation against prior work.** We evaluate our method against prior work on non-autoregressive modeling. For a fair comparison, all methods are trained on a 150M parameters models using the OpenWebText (Gokaslan and Cohen, 2019) dataset. We also fix all sampling hyperparameters to the most basic settings, i.e., no temperature, top probability, corrector steps, etc. For our method we tried two paths defined by equations 9 and 10. Results are reported in Table 2, where our method outperforms all baselines in generative perplexity for all numbers of function evaluations (NFE).

**Conditional text generation.** In this experiment, we train both C-coupling and U-coupling 1.7B parameters FM models with paths defined by equation 9 on a large scale data mix (Touvron et al., 2023). Table 3 presents the generative perplexity of conditional generations from our method; the conditions we used are the prefixes of the first 1000 samples in OpenWeb dataset. We also compare to existing state-of-the-art autoregressive models. Our results demonstrate that our model effectively narrows the gap in generative perplexity with autoregressive models, while maintaining an entropy comparable to the recent Llama-3 8B model. Furthermore, we note the C-coupling trained model produces slightly better perplexity in conditional tasks than the U-coupling model. In Appendix I we present qualitative conditional samples produced by our U-coupling model.

**Code generation.** Here we trained our basic setting of a 1.7B parameters FM model with U-coupling and path as in equation 9 on a code-focused data mix (Roziere et al., 2023). Table 4 presents results on HumanEval and MBPP (1-shot) for pass\(@\{1,10,25\}\). In Table 4, 'Oracle length' evaluates the performance of our model when conditioning on the length of the solution. This is done by inserting an 'end of text' token in the same position of the ground truth solution. Our method achieves non-trivial results on both tasks, which to the best of our knowledge is the first instance of a non-autoregressive method being capable of non-trivial coding tasks. In Appendix C, we analyze the

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Data} & \multicolumn{3}{c}{HumanEval\(\uparrow\)} & \multicolumn{3}{c}{MBPP (1-shot)\(\uparrow\)} \\ \cline{3-8}  & & Pass@1 & Pass@10 & Pass@25 & Pass@1 & Pass@10 & Pass@25 \\ \hline Autoregressive & Text & 1.2 & 3.1 & 4.8 & 0.2 & 1.7 & 3.3 \\  & Code & 14.3 & 21.3 & 27.8 & 17.0 & 34.3 & 44.1 \\
**FM** & Text & 1.2 & 2.6 & 4.0 & 0.4 & 1.1 & 3.6 \\  & Code & 6.7 & 13.4 & 18.0 & 6.7 & 20.6 & 26.5 \\
**FM (Oracle length)** & Code & 11.6 & 18.3 & 20.6 & 13.1 & 28.4 & 34.2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Execution based code generation evaluation.

proposed method for code infilling, which can be achieved as our model allows non-autoregressive generation. Lastly, in Appendix H we show qualitative examples of success and failure cases produced by our model on the coding tasks, and in Appendix H.3 we show examples of code infilling.

### Image generation

We performed a fully discrete image generation, without using any metric or neighboring information between color values. We trained an FM model with U-coupling and path as in equation 9 on CIFAR10 to predict discrete color value for tokens, i.e., \(d=256\), with sequence length of \(N=32\times 32\times 3\). For generative quality we evaluate the Frechet Inception Distance (FID) (Heusel et al., 2017). Ablations for the probability path schedulers are provided in Figure 8 in the Appendix G. In Figure 2(a) we compare our method with: (i) MaskGIT (Chang et al., 2022); and (ii) (Campbell et al., 2024) which coincides with our method for a linear scheduler. More details in Appendix G. As can be seen in the Figure 2(a), our method outperforms both baselines, achieving \(3.63\) FID at \(1024\) NFE. In fig. 2(b) we observe a similar trend when evaluating Inception score. As discussed above, MaskGit sampling performs better for low NFE but quickly deteriorates for higher NFE. We attribute this to a bias introduced in the sampling process via the confidence mechanism.

## 5 Conclusions and future work

We introduce Discrete Flow Matching, a generalization of continuous flow matching and discrete flows that provides a large design space of discrete non-autoregressive generative models. Searching within this space we were able to train large scale language models that produce generated text with an improved generative perplexity compared to current non-autoregressive methods and able to solve coding tasks at rates not achievable before with non-autoregressive models, as far as we are aware. While reducing the number of network evaluations required to generate a discrete sample compared to autoregressive models, Discrete Flow Matching still does not achieve the level of sampling efficiency achieved by its continuous counterpart, flagging an interesting future work direction. Another interesting direction is to explore the space of probability paths in equation 8 (or a generalization of which) beyond what we have done in this paper. We believe discrete non-autoregressive models have the potential to close the gap and even surpass autoregressive models as well as unlock novel applications and use cases. As our work introduces an alternative modeling paradigm to discrete sequential data such as language and code, we feel it does not introduce significant societal risks beyond those that already exist with previous large language models.

Figure 3: FID and Inception scores vs. number of function evaluations (NFE).

## References

* Ahn et al. (2024) Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. _arXiv preprint arXiv:2402.00157_, 2024.
* Albergo & Vanden-Eijnden (2022) Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. _arXiv preprint arXiv:2209.15571_, 2022.
* Austin et al. (2021a) Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021a.
* Austin et al. (2021b) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021b.
* Besnier & Chen (2023) Victor Besnier and Mickael Chen. A pytorch reproduction of masked generative image transformer, 2023.
* Blattmann et al. (2023) Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv preprint arXiv:2311.15127_, 2023.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Campbell et al. (2022) Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. _Advances in Neural Information Processing Systems_, 35:28266-28279, 2022.
* Campbell et al. (2024) Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. _arXiv preprint arXiv:2402.04997_, 2024.
* Chang et al. (2022) Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11315-11325, 2022.
* Chang et al. (2023) Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. _arXiv preprint arXiv:2301.00704_, 2023.
* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* Chen et al. (2022) Ting Chen, Ruixiang Zhang, and Geoffrey E. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. _ArXiv_, 2022.
* Copet et al. (2024) Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. _Advances in Neural Information Processing Systems_, 36, 2024.
* Dhariwal & Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* Dieleman et al. (2022) Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. _arXiv preprint arXiv:2211.15089_, 2022.
* Esser et al. (2024) Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. _arXiv preprint arXiv:2403.03206_, 2024.
* Esser et al. (2020)Noelia Ferruz and Birte Hocker. Controllable protein design with language models. _Nature Machine Intelligence_, 4(6):521-532, 2022.
* Ghazvininejad et al. (2019) Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. _arXiv preprint arXiv:1904.09324_, 2019.
* Gokaslan & Cohen (2019) Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus), 2019.
* Han et al. (2022) Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. _arXiv preprint arXiv:2210.17432_, 2022.
* Hassid et al. (2024) Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. Textually pretrained speech language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* He et al. (2022) Zhengfu He, Tianxiang Sun, Kuan Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. In _Annual Meeting of the Association for Computational Linguistics_, 2022.
* Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Hoogeboom et al. (2021) Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_, 34:12454-12465, 2021.
* Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)_, pages 37-42, 2023.
* Kreuk et al. (2022) Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. _arXiv preprint arXiv:2209.15352_, 2022.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _arXiv_, 2009.
* Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koectkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! _arXiv preprint arXiv:2305.06161_, 2023.
* Li et al. (2022) Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. _ArXiv_, 2022.
* Lin et al. (2022) Zheng-Wen Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu Chen, and Nan Duan. Genie : Large scale pre-training for generation with diffusion model. In _ArXiv_, 2022.
* Lipman et al. (2022) Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* Liu et al. (2022) Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.
* Lou et al. (2023) Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. _arXiv preprint arXiv:2310.16834_, 2023.
* Lovelace et al. (2022) Justin Lovelace, Varsha Kishore, Chao gang Wan, Eliot Shekhtman, and Kilian Q. Weinberger. Latent diffusion for language generation. _ArXiv_, 2022.
* Li et al. (2022)Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z Sun, Richard Socher, et al. Large language models generate functional protein sequences across diverse families. _Nature Biotechnology_, 41(8):1099-1106, 2023.
* Norris (1998) James R Norris. _Markov chains_. Number 2. Cambridge university press, 1998.
* Peebles and Xie (2022) William Peebles and Saining Xie. Scalable diffusion models with transformers. _arXiv preprint arXiv:2212.09748_, 2022.
* Pooladian et al. (2023) Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. _arXiv preprint arXiv:2304.14772_, 2023.
* Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [https://api.semanticscholar.org/CorpusID:160025533](https://api.semanticscholar.org/CorpusID:160025533).
* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Romera-Paredes et al. (2024) Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. _Nature_, 625(7995):468-475, 2024.
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* Savinov et al. (2021) Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. Step-unrolled denoising autoencoders for text generation. _arXiv preprint arXiv:2112.06749_, 2021.
* Singer et al. (2022) Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.
* Song and Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Stark et al. (2024) Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. _arXiv preprint arXiv:2402.05841_, 2024.
* Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.
* Tong et al. (2023) Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. _arXiv preprint arXiv:2302.00482_, 2023.
* Touvron et al. (2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Zhang et al. (2024) Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, et al. Scientific large language models: A survey on biological & chemical domains. _arXiv preprint arXiv:2401.14656_, 2024.
* Zhang et al. (2020)Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.
* Zheng et al. (2024) Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. _arXiv preprint arXiv:2409.02908_, 2024.
* Ziv et al. (2024) Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. Masked audio generation using a single non-autoregressive transformer. _arXiv preprint arXiv:2401.04577_, 2024.

Related works, continuation

We provide here some more details on relevant related works.

Continuous diffusion and flows.Another line of works has been exploring the use of continuous space diffusion for discrete data, typically operating in the logits space (Dieleman et al., 2022; Li et al., 2022; Han et al., 2022; Lin et al., 2022; Chen et al., 2022). An additional body of work has been focusing on the adoption of latent diffusion-like modeling (Lovelace et al., 2022; He et al., 2022). Stark et al. (2024) proposed to learn a continuous Flow Matching on the probability simplex with Dirichlet paths.

Autoregressive modeling.Autoregressive models have been a significant area of focus in recent years, particularly in the context of natural language processing and machine learning (Zhao et al., 2023). Autoregressive modeling, in its most fundamental form, utilizes the chain rule to learn the joint sequence probability by breaking it down into next-token conditional probabilities. GPT-2 (Radford et al., 2019), showcased the power of autoregressive language models in generating coherent and contextually relevant text over long passages. Its successor, GPT-3 (Brown et al., 2020), further pushed the boundaries, demonstrating impressive performance across a range of tasks without task-specific training data. Later models were adapted to other domains such as, code (Roziere et al., 2023; Li et al., 2023; Chen et al., 2021), biology (Zhang et al., 2024; Ferruz and Hocker, 2022; Madani et al., 2023), math (Romera-Paredes et al., 2024; Imani et al., 2023; Ahn et al., 2024), audio (Kreuk et al., 2022; Copet et al., 2024; Hassid et al., 2024) and more.

Masked generative modeling.Masked generative modeling proposes to mask a variable portion of the input sequence and training a model to predict this masked section. Ghazvininejad et al. (2019) proposed Mask-Predict, a masked language modeling with parallel decoding. Savinov et al. (2021) extended the mask-modeling approach by employing an additional loss term that incorporates rolling model predictions. MaskGIT (Chang et al., 2022) followed a similar path, for the task of class-conditioned image synthesis, Chang et al. (2023) extended this approach to high-quality textually guided image generation over low-resolution images followed by a super-resolution module. Recently, Ziv et al. (2024) proposed a text-to-music method, which relies on the MaskGIT foundations while observing that span masking boosts the quality of the generated sequence significantly.

## Appendix B Further implementation details

Safe sampling.When sampling according to Algorithm 1 using the generating probability velocity in equation 22, an arbitrary step size \(h>0\) can make some probabilities in \(\delta_{X_{k}^{i}}(\cdot)+hu_{t}^{i}(\cdot,X_{t})\) negative and consequently require clamping and injecting further error into the sampling process that can in turn accumulate to a non-negligible global sampling error. A simple fix that guarantees a valid probability distribution while keeping the \(o(h)\) sampling error at the relatively manageable price of potentially more function evaluations is using the following adaptive step size in Algorithm 1: at time \(t\in[0,1)\) use

\[h_{\text{adaptive}}=\min\left\{h,\min_{i}\left|\frac{\kappa_{t}^{i,\ell}}{ \hat{\kappa}_{t}^{i,\ell}}\right|\right\}. \tag{29}\]

As can be verified with the general probability velocity formula in equation 22, the above choice for \(h_{\text{adaptive}}\) guarantees \(\delta_{X_{i}^{i}}(\cdot)+hu_{t}^{i}(\cdot,X_{t})\) is a valid PMF. As mostly used in this paper, for the probability denoiser parameterization (equation 24) the adaptive step is

\[h_{\text{adaptive}}=\min\left\{h,\frac{1-\kappa_{t}}{\hat{\kappa}_{t}}\right\}. \tag{30}\]

With the corrector sampling (equations 26 and 51) we have the adaptive step:

\[h_{\text{adaptive}}=\min\left\{h,\left[\frac{\alpha_{t}\hat{\kappa}_{t}}{1- \kappa_{t}}+\frac{\beta_{t}\hat{\kappa}_{t}}{\kappa_{t}}\right]^{-1}\right\}. \tag{31}\]Conditioning.In our unconditional coupling (U-coupling), see equation 5, we define the conditioning pattern based on prefixes of random length \(N_{0}<N\), i.e.,

\[\mathbb{I}=(\overbrace{1,\ldots,1,0,\ldots,0}^{N_{0}}).\]

During the training phase, we sample \(N_{0}\sim\mathcal{U}(0,N)\) and adjust the input sequence in accordance with the mask \(\mathbb{I}\).

During conditional sampling with Algorithm 1 we replace, after each update step, the relevant tokens with the conditioned ones, i.e., \(\tilde{X}=\mathbb{I}\odot Y+(\mathbb{I}-\mathbb{I})\odot X\), where \(X\) is the current sample, \(Y\) is the condition, and \(\mathbb{I}\) is the condition's mask.

NFE bound.For mask modeling, i.e., \(p=\delta_{m}\), we have seen that the probability denoiser is time-independent (see Proposition 6). Consequently, when sampling with Algorithm 1 and \(u_{t}\) from equation 24 without corrector sampling one is not required to recompute the forward pass \(p_{1|t}(\cdot|X_{t})\) if \(X_{t}\) is identical to \(X_{t-h}\) (i.e., no \(\mathbb{m}\) has been unmasked). This means that the NFE of Algorithm 1 in this case is bounded by the number of tokens \(N\).

Post training scheduler change.For a trained posterior \(\hat{w}_{t}(x^{i}|z)\) of a conditional probability path as in equation 9 with a scheduler \(\kappa_{t}\), the velocity is given by equations 24 or 25, where \(\hat{w}_{t}(x^{i}|z)\) is either \(p_{1|t}(x^{i}|z)\) or \(p_{0|t}(x^{i}|z)\) respectively. In this case, we can apply the velocities in equations 24 and 25 for sampling with any scheduler \(\kappa^{\prime}_{t}\), using the change of scheduler formula for posteriors,

\[\hat{w}^{\prime}_{t}(x^{i}|z)=\hat{w}_{t^{\prime}}(x^{i}|z), \tag{32}\]

where \(\hat{w}^{\prime}_{t}(x^{i}|z)\), is the posterior of the scheduler \(\kappa^{\prime}_{t}\), \(t^{\prime}=\kappa^{-1}_{\kappa^{\prime}_{t}}\), and \(\kappa^{-1}\) is the inverse of \(\kappa\). The scheduler change formula in equation 32 is proved in Proposition 8. We note that by Proposition 6, for mask modeling, i.e., \(p=\delta_{m}\), the posterior \(\hat{w}_{t}(x^{i}|z)\) is time independent. Hence, in that case, the posterior is not affected by a scheduler change.

## Appendix C Code infilling

We additionally evaluate the proposed method considering the task of code infilling. In which, we are provided with an input prompt that contains various spans of masked tokens, and our goal is to predict them based on the unmasked ones. See Figure 1 (middle and right sub-figures) for a visual example. Notice, this evaluation setup is the most similar to the training process.

For that, we randomly mask tokens with respect to several masking rations, \(p\in\{0.0,0.1,0.2,\ldots,1.0\}\), from HumanEval and report both pass@1 and compiles@1 metrics. For the purpose of this analysis, we provide the oracle length for each masked span. In other words, the model predicts the masked tokens for already given masks length. Results for the 1.5B parameters models can be seen in Figure 4. As expected, both pass@1 and compiles@1 keep improving as we decrease the level of input masking. Interestingly, when considering the fully masked sequence, providing the oracle prediction length significantly improves the pass@1 scores (6.7 vs. 11.6).

## Appendix D Ablations

Train and sampling path scheduler choice (\(\kappa_{t}\)).We study how the choice of the probability path scheduler affects the model performance. For that, we consider a parametric family of cubic

Figure 4: Pass@1 and compiles@1 scores for the 1.5B parameter models as a function of the input masking rations on HumanEval.

polynomial with parameters \(a,b\):

\[\kappa_{t}\triangleq-2t^{3}+3t^{2}+a(t^{3}-2t^{2}+t)+b(t^{3}-t^{2}). \tag{33}\]

Note that \(\kappa_{0}=0\) and \(\kappa_{1}=0\) and \(a\) and \(b\) are setting the derivative of \(\kappa_{t}\) at \(t=0\) and \(t=1\), respectively. We visualize this \(\kappa_{t}\) with choices of \(a,b\in\{0,1,2\}\) in Figure 4(a).

To test the effect of path schedulers in training we have trained 150M parameters models for all choices of \(a,b\in\{0,1,2,3\}\). We then generate 1000 samples from each model. The samples are computed using Algorithm 1 with the path scheduler the model was trained on, and with temperature levels \(\tau\in\{0.8,0.9,1\}\), where temperature is applied via

\[p_{1|t}^{\tau}(x^{i}|X_{t})=\tau^{-1}\log p_{1|t}(x^{i}|X_{t}). \tag{34}\]

We then evaluate the generative perplexity of these samples with GPT-2. Figure 6 shows the results. The graphs indicate that, in the context of text modality, the cubic polynomial scheduler with \(a\equiv 0,b\equiv 2\) (equivalent to a square function) achieves the highest performance. Consequently, we exclusively used this scheduler for the language models.

Corrector scheduler.In our experiments we only applied corrector sampling to our large models (U-coupling and C-coupling; 1.7B parameters). We used the optimal path schedulers from previous section and considered the following parametric family of schedulers for the corrector sampling:

\[\alpha_{t}=1+\alpha t^{a}(1-t)^{b}, \tag{35}\]

where, we set \(\beta_{t}=\alpha_{t}-1\) and generate 1000 samples using Algorithm 1 with parameter values \(a,b\in\{0,0.25,0.5\}\) and \(\alpha\in\{10,15,20\}\). We then evaluated generative perplexity for these samples with Llama-2, showing results in Figure 7. These plots indicate that smaller values of \(a\) and \(b\) result in lower perplexity values, albeit with somewhat reduced entropy. We therefore opted for setting \(a=b=0.25\) that strikes a good balance between perplexity and entropy.

Temperature scheduling.For temperature sampling, we consider the following scheduler:

\[\tau_{t}=\tau(1-t)^{2}. \tag{36}\]

Figure 6: Path scheduler choice during training using various of constant temperature values.

## Appendix E Theory and proofs

### Computation of the discrete divergence

We present the computation of the discrete divergence in equation 18, i.e.,

\[\operatorname{div}_{x}(p_{t}u_{t})=-\sum_{z\in\mathcal{D}}p_{t}(z)\left[\sum_{i=1 }^{N}\delta_{z}(x^{\tilde{i}})u_{t}^{i}(x^{i},z)\right]. \tag{37}\]

Computing the discrete divergence (equation 18) of the flux \(p_{t}u_{t}\) at a state \(x\) amounts to adding outgoing flux from \(x\) and subtracting the incoming flux into \(x\). Using the fact that \(\delta_{z}(x^{\tilde{i}})=1\) if and only if \(z=x\) or \(z\) differs from \(x\) only at the \(i\)-th token, gives:

\[\operatorname{div}_{x}(p_{t}u_{t}) =\sum_{z\in\mathcal{D}}\sum_{i=1}^{N}\delta_{x}(z^{\tilde{i}}) \left(p_{t}(x)u_{t}^{i}(z^{i},x)-p_{t}(z)u_{t}^{i}(x^{i},z)\right)\] \[=p_{t}(x)\sum_{i=1}^{N}\widehat{\left[\sum_{z^{i}}\delta_{x}(z^{ \tilde{i}})\right]}u_{t}^{i}(z^{i},x)-\sum_{z\in\mathcal{D}}\sum_{i=1}^{N} \delta_{x}(z^{\tilde{i}})p_{t}(z)u_{t}^{i}(x^{i},z)\] \[=p_{t}(x)\sum_{i=1}^{N}\widehat{\left[\sum_{z^{i}}u_{t}^{i}(z^{i},x)\right]}-\sum_{z\in\mathcal{D}}\sum_{i=1}^{N}\delta_{x}(z^{\tilde{i}})p_{t}( z)u_{t}^{i}(x^{i},z)\qquad\triangleright\text{equation \ref{eq:1}}\] \[=-\sum_{z\in\mathcal{D}}\sum_{i=1}^{N}\delta_{x}(z^{\tilde{i}})p_ {t}(z)u_{t}^{i}(x^{i},z),\]

that gives equation 37 after noting that \(\delta_{x}(z^{\tilde{i}})=\delta_{z}(x^{\tilde{i}})\).

Figure 7: Corrector scheduler ablation.

### Conditional velocities lead to marginal velocities

We provide a simple proof for Theorem 2, originally proved in Campbell et al. (2024):

Given a conditional probability velocity \(u^{i}_{t}(x^{i},X_{t}|x_{0},x_{1})\) generating a conditional probability path \(p_{t}(x|x_{0},x_{1})\), the marginal velocity defined by

\[u^{i}_{t}(x^{i},X_{t})=\sum_{x_{0},x_{1}\in\mathcal{D}}u^{i}_{t}(x^{i},X_{t}|x_{ 0},x_{1})p_{t}(x_{0},x_{1}|X_{t}), \tag{38}\]

generates the marginal probability path \(p_{t}(x)\), where by Bayes' rule

\[p_{t}(x_{0},x_{1}|X_{t})=\frac{p_{t}(X_{t}|x_{0},x_{1})\pi(x_{0},x_{1})}{p_{t}( x)}. \tag{39}\]

Proof (Theorem 2).: We start by taking the time derivative of the marginal probability path, \(p_{t}(x)=\sum_{x_{0},x_{1}}p_{t}(x^{i}|x_{0},x_{1})\pi(x_{0},x_{1})\), as follows,

\[\dot{p}_{t}(x) =\sum_{x_{0},x_{1}}\dot{p}_{t}(x|x_{0},x_{1})\pi(x_{0},x_{1})\] \[=\sum_{x_{0},x_{1}}\left(\sum_{z}p_{t}(z|x_{0},x_{1})\left[\sum_{z =1}^{N}\delta_{z}(x^{\bar{i}})u^{i}_{t}(x^{i},z|x_{0},x_{1})\right]\right)\pi( x_{0},x_{1})\qquad\triangleright\text{Continuity Equation \eqref{eq:p_t}}\] \[=\sum_{z}p_{t}(z)\left[\sum_{i=1}^{N}\delta_{z}(x^{\bar{i}})\left( \sum_{x_{0},x_{1}}u^{i}_{t}(x^{i},z|x_{0},x_{1})\frac{p_{t}(z|x_{0},x_{1})\pi( x_{0},x_{1})}{p_{t}(z)}\right)\right]\] \[=\sum_{z}p_{t}(z)\left[\sum_{i=1}^{N}\delta_{z}(x^{\bar{i}})u^{i}_ {t}(x^{i},z)\right]\] \[=-\mathrm{div}_{x}(p_{t}u_{t})\]

Now since \(u^{i}_{t}(x^{i},z)\) is a convex combinations of \(u^{i}_{t}(x^{i},z|x_{0},x_{1})\) and these satisfy equation 13 then also \(u^{i}_{t}(x^{i},X_{t})\) satisfies equation 13. 

### Probability velocities generating conditional probability paths

Equation 22 with the coefficients \(a^{i,j}_{t}\) and \(b^{i}_{t}\) are provided below,

\[u^{i}_{t}(x^{i},X_{t}|x_{0},x_{1})=\sum_{j=1}^{m}\overline{\left[\dot{\kappa}^{ i,j}_{t}-\kappa^{i,j}_{t}\frac{\dot{\kappa}^{i,\ell}_{t}}{\kappa^{i,\ell}_{t}} \right]}w^{j}(x^{i}|x_{0},x_{1})+\overline{\left[\frac{\dot{\kappa}^{i,\ell}_{ t}}{\kappa^{i,\ell}_{t}}\right]}\delta_{X_{t}}(x^{i}), \tag{40}\]

where

\[\ell=\ell(i,t)\stackrel{{\text{def}}}{{=}}\operatorname*{arg\, min}_{j\in[m]}\left[\dot{\kappa}^{i,j}_{t}/\kappa^{i,j}_{t}\right]. \tag{41}\]

[Probability velocity of conditional paths] A generating probability velocity for the conditional paths \(p_{t}(x|x_{0},x_{1})\) defined in equations 7 and 8 is

\[u^{i}_{t}(x^{i},X_{t}|x_{0},x_{1})=\sum_{j=1}^{m}a^{i,j}_{t}w^{j}(x^{i}|x_{0}, x_{1})+b^{i}_{t}\delta_{X_{t}}(x^{i}), \tag{42}\]

with \(a^{i,j}_{t}=\dot{\kappa}^{i,j}_{t}-\kappa^{i,j}_{t}\dot{\kappa}^{i,\ell}_{t}/ \kappa^{i,\ell}_{t}\), and \(b^{i}_{t}=\dot{\kappa}^{i,\ell}_{t}/\kappa^{i,\ell}_{t}\) where \(\ell=\arg\min_{j\in[m]}\left[\dot{\kappa}^{i,j}_{t}/\kappa^{i,j}_{t}\right]\).

[MISSING_PAGE_FAIL:20]

as required. 

### Backward-time generating probability velocity.

Here we prove the equivalent of Theorem 3 for backward-time generating probability field. But first, let us justify the backward sampling formula,

\[X^{i}_{t-h}\sim\delta_{X^{i}_{t}}(\cdot)-hu^{i}_{t}(\cdot,X_{t}). \tag{45}\]

Similar to equation 20 we have

\[\mathbb{E}_{X_{t}}\prod_{i=1}^{N}\left[\delta_{X_{t}}(x^{i})-hu^{i }_{t}(x^{i},X_{t})\right]=\mathbb{E}_{X_{t}}\left[\delta_{X_{t}}(x)-h\sum_{i=1 }^{N}\delta_{X_{t}}(x^{\bar{i}})u^{i}_{t}(x^{i},X_{t})\right]+o(h)\] \[=p_{t}(x)+h\mathrm{div}_{x}(p_{t}u_{t})+o(h)\overset{(\ref{eq:p_t })}{=}p_{t}(x)-h\dot{p}_{t}(x)+o(h)=p_{t-h}(x)+o(h).\]

Therefore if the Continuity equation holds and \(-u_{t}\) satisfies the conditions in equation 13 then given \(X_{t}\sim p_{t}\), equation 45 provides an approximation \(X_{t-h}\sim p_{t-h}+o(h)\). The change to the generating probability velocity in equation 22 to accommodate reverse time sampling is to replace the argmin in equation 41 with argmax,

\[\ell=\ell(i,t)\triangleq\operatorname*{arg\,max}_{j\in[m]}\left[\dot{\kappa} ^{i,j}_{t}/\kappa^{i,j}_{t}\right]. \tag{46}\]

An analogous result to Theorem 3 for backward-time sampling is therefore,

**Theorem 7** (Probability velocity of conditional paths, backward time).: _The probability velocity \(-u_{t}\), where \(u_{t}\) defined in equation 21 with \(\ell=\operatorname*{arg\,max}_{j\in[m]}\left[\dot{\kappa}^{i,j}_{t}/\kappa^{i, j}_{t}\right]\) is a backward-time generating probability velocity for the conditional paths \(p_{t}(x|x_{0},x_{1})\) defined in equations 7 and 8._

Proof (Theorem 7).: We follow the proof of Theorem 3 and indicate the relevant changes. First, for arbitrary \(X_{t}\in\mathcal{D}\),

\[\sum_{x^{i}}u^{i}_{t}(x^{i},X_{t})=0, \tag{47}\]

exactly using the same arguments as the forward-time case. Now, for \(x^{i}\neq X^{i}_{t}\) we have

\[u^{i}_{t}(x^{i},X_{t}|x_{0},x_{1})=\sum_{j=1}^{m}\left[\frac{\dot{\kappa}^{i,j }_{t}}{\kappa^{i,j}_{t}}-\frac{\dot{\kappa}^{i,\ell}_{t}}{\kappa^{i,\ell}_{t} }\right]\kappa^{i,j}_{t}w^{j}_{t}(x^{i}|x_{0},x_{1})\leq 0 \tag{48}\]

due to \(\ell\) being now the argmax of \(\frac{\dot{\kappa}^{i,j}_{t}}{\kappa^{i,j}_{t}}\). Therefore \(-u_{t}\) satisfies equation 13. Lastly, we notice that the proof of the Continuity Equation follows through exactly the same also in this case. 

### Backward-time generating velocity for i.i.d. source \(p(x_{0})\) and simple paths

Here we consider the case of probability paths defined via the conditionals in equation 9 with independent coupling \(\pi(x_{0},x_{1})=p(x_{0})q(x_{1})\) and i.i.d. source distribution \(p(x_{0})=\prod_{i=1}^{N}p(x^{i}_{0})\), where \(p(x^{i}_{0})\) is some PMF over \([d]\). In this case one can simplify the time-backward sampling formula in equation 25 by using the following one which is equivalent (i.e., their difference is divergence free and consequently generate the same probability path \(p_{t}\)):

\[\ddot{u}_{t}(x^{i},X_{t})=\frac{\dot{\kappa}_{t}}{\kappa_{t}}\left[\delta_{X_{ t}}(x^{i})-p(x^{i})\right]. \tag{49}\]

The benefit in this equation is that it does not require the posterior \(p_{0|t}\), which needs to be learned in general cases.

To show that equation 49 is indeed a generating probability velocity it is enough to show that

\[\mathrm{div}_{x}\left[p_{t}\left(\ddot{u}_{t}-\tilde{u}^{*}_{t}\right)\right] =0, \tag{50}\]where \(\tilde{u}_{t}^{*}\) is the probability velocity in equation 25. Let us verify using equation 19:

\[\mathrm{div}_{x}\left[p_{t}\left(\tilde{u}_{t}-\tilde{u}_{t}^{*} \right)\right] =\sum_{i,z}p_{t}(z)\delta_{z}(x^{\tilde{i}})\left[p(x^{i})-\sum_{x _{0},x_{1}}\delta_{x_{0}}(x^{i})\frac{p_{t}(z|x_{0},x_{1})p(x_{0})q(x_{1})}{p_ {t}(z)}\right]\triangleright\pi(x_{0},x_{1})=p(x_{0})q(x_{1})\] \[=\sum_{i,z}\delta_{z}(x^{\tilde{i}})\left[p(x^{i})p_{t}(z)-\sum_{ x_{0},x_{1}}\delta_{x_{0}}(x^{i})p_{t}(z|x_{0},x_{1})p(x_{0})q(x_{1})\right]\] \[=\sum_{i,x_{0},x_{1}}\left[p(x^{i})-\delta_{x_{0}}(x^{i})\right] \left(\sum_{z}\delta_{z}(x^{\tilde{i}})p_{t}(z|x_{0},x_{1})\right)p(x_{0})q(x _{1})\] \[=\sum_{i,x_{0},x_{1}}\left[p(x^{i})-\delta_{x_{0}}(x^{i})\right]p_ {t}(x^{\tilde{i}}|x_{0},x_{1})p(x_{0}^{\tilde{i}})p(x_{0}^{i})q(x_{1})\] \[=\sum_{i,x_{0}^{i},x_{1}}\left(\sum_{x_{0}^{i}}\left[p(x^{i})p(x_{ 0}^{i})-\delta_{x_{0}}(x^{i})p(x_{0}^{i})\right]\right)p_{t}(x^{\tilde{i}}|x_{0 },x_{1})p(x_{0}^{\tilde{i}})q(x_{1})\] \[=0,\]

where in the second to last equality we used the fact that the paths we are considering have the form: \(p_{t}(x^{\tilde{i}}|x_{0},x_{1})=\prod_{j\in[N]\setminus i}\left[\kappa_{t} \delta_{x_{1}}(x^{j})+(1-\kappa_{t})\delta_{x_{0}}(x^{j})\right]\), and therefore do not depend on the \(i\)-th source token, \(x_{0}^{i}\).

### Corrector steps

**Theorem 4**.: For perfectly trained posteriors and \(\alpha_{t},\beta_{t}>0\), \(t\in(0,1)\), \(\bar{u}_{t}\) in equation 26 is a probability velocity, i.e., satisfies equation 13, and: (i) For \(\alpha_{t}-\beta_{t}=1\), \(\bar{u}_{t}\) provides a probability velocity generating \(p_{t}\); (ii) For \(\alpha_{t}-\beta_{t}=0\), repeatedly sampling with \(\bar{u}_{t}\) at fixed \(t\in(0,1)\) and sufficiently small \(h\) is guaranteed to converge to a sample from \(p_{t}\).

Proof (Theorem 4).: First let us write explicitly \(\bar{u}_{t}\) from equation 26:

\[\bar{u}_{t}^{i}(x^{i},X_{t}) =\alpha_{t}\hat{u}_{t}^{i}(x^{i},X_{t})-\beta_{t}\hat{u}_{t}^{i}( x^{i},X_{t})\] \[=\frac{\alpha_{t}\hat{\kappa}_{t}}{1-\kappa_{t}}p_{1|t}(x^{i}|X_{ t})+\frac{\beta_{t}\hat{\kappa}_{t}}{\kappa_{t}}p_{0|1}(x^{i}|X_{t})-\left[ \frac{\alpha_{t}\hat{\kappa}_{t}}{1-\kappa_{t}}+\frac{\beta_{t}\hat{\kappa}_{t }}{\kappa_{t}}\right]\delta_{X_{t}}(x^{i}). \tag{51}\]

Since equation 51 is a sum of PMFs with coefficients that sum up to zero the first condition in equation 13, i.e., \(\sum_{x^{i}}\bar{u}_{t}^{i}(x^{i},X_{t})=0\) holds. The second condition in equation 13 holds since for \(t\in(0,1)\) we have \(\frac{\alpha_{t}\hat{\kappa}_{t}}{1-\kappa_{t}}\), \(\frac{\beta_{t}\hat{\kappa}_{t}}{\kappa_{t}}\geq 0\). Now,

\[\mathrm{div}_{x}(p_{t}\bar{u}_{t}) =\alpha_{t}\mathrm{div}_{x}(p_{t}\hat{u}_{t})-\beta_{t}\mathrm{ div}_{x}(p_{t}\bar{u}_{t}) \triangleright\text{linearity of div}\] \[=-\alpha_{t}\hat{p}_{t}(x)+\beta_{t}\hat{p}_{t}(x) \triangleright\text{Equation 16}\] \[=-(\alpha_{t}-\beta_{t})\hat{p}_{t}(x). \tag{52}\]

**For (i):** Using equation 52 with \(\alpha_{t}-\beta_{t}=1\) we get that

\[\mathrm{div}_{x}(p_{t}\bar{u}_{t})=-\hat{p}_{t}(x),\]

i.e., \(\bar{u}_{t}\) satisfies the continuity equation and therefore generates \(p_{t}\).

**For (ii):** Setting \(\alpha_{t}-\beta_{t}=0\) in equation 52 we get \(\mathrm{div}_{x}(p_{t}\bar{u}_{t})=0\) and therefore similar to equation 20 we have

\[p_{t}(x) =p_{t}(x)-h\mathrm{div}_{x}(p_{t}\bar{u}_{t})\] \[=\mathbb{E}_{X_{t}}\left[\delta_{X_{t}}(x)+h\sum_{i=1}^{N}\delta_ {X_{t}}(x^{\tilde{i}})\bar{u}_{t}^{i}(x^{i},X_{t})\right]\] \[=\sum_{z}p(x|z)p_{t}(z), \tag{53}\]

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

and the marginal generating velocity field in this case is given by marginalization with the posterior \(p_{t}(x_{0}|x)\),

\[u_{t}(x) =\int\frac{\dot{\kappa}_{t}}{\kappa_{t}}(x-x_{0})\frac{p_{0|t}(x|x_{ 0})p(x_{0})}{p_{t}(x)}dx_{0}\] \[=\frac{\dot{\kappa}_{t}}{\kappa_{t}}\left[x-\hat{x}_{0|t}(x) \right],\]

where

\[\hat{x}_{0|t}(x)=\int x_{0}\frac{p_{0|t}(x|x_{0})p(x_{0})}{p_{t}(x)}dx_{0}= \mathbb{E}_{X_{0}\sim p_{t}(\cdot|x)}X_{0}. \tag{60}\]

This shows the continuous Flow Matching noise-prediction parameterization of the generating velocity field in Table 1.

### Scheduler change formula

**Proposition 8**.: _Assume a conditional probability path as in equation 9, then for any two schedulers \(\kappa_{t},\kappa^{\prime}_{t}\), and \(\hat{w}_{t}(x^{i}|z),\hat{w}^{\prime}_{t}(x^{i}|z)\) their corresponding posteriors as in equation 23,_

\[\hat{w}_{t^{\prime}}(x^{i}|z)=\hat{w}^{\prime}_{t}(x^{i}|z), \tag{61}\]

_where \(t^{\prime}=\kappa_{\kappa^{\prime}_{t}}^{-1}\), and \(\kappa^{-1}\) is the inverse of \(\kappa\)._

Proof (Proposition 8).: For a conditional probability path as in equation 9,

\[p_{t^{\prime}}(x^{i}|x_{0},x_{1}) =\prod_{i=1}^{N}p_{t^{\prime}}(x^{i}|x_{0},x_{1}) \tag{62}\] \[=\prod_{i=1}^{N}\left[(1-\kappa_{t^{\prime}})\delta_{x_{0}}(x^{i} )+\kappa_{t^{\prime}}\delta_{x_{1}}(x^{i})\right]\] (63) \[=\prod_{i=1}^{N}\left[(1-\kappa^{\prime}_{t})\delta_{x_{0}}(x^{i} )+\kappa^{\prime}_{t}\delta_{x_{1}}(x^{i})\right]\] (64) \[=\prod_{i=1}^{N}p^{\prime}_{t}(x^{i}|x_{0},x_{1})\] (65) \[=p^{\prime}_{t}(x^{i}|x_{0},x_{1}), \tag{66}\]

where in the 3rd equality we used \(\kappa_{t^{\prime}}=\kappa^{\prime}_{t}\). Thus, also for the marginal probability path as in equation 7,

\[p_{t^{\prime}}(x) =\sum_{x_{0},x_{1}\in\mathcal{D}}p_{t^{\prime}}(x|x_{0},x_{1})\pi (x_{0},x_{1}) \tag{67}\] \[=\sum_{x_{0},x_{1}\in\mathcal{D}}p^{\prime}_{t}(x|x_{0},x_{1})\pi (x_{0},x_{1})\] (68) \[=p^{\prime}_{t}(x), \tag{69}\]where in the 2nd equality we used \(p_{t^{\prime}}(x|x_{0},x_{1})=p^{\prime}_{t}(x|x_{0},x_{1})\). Finally the change of scheduler for a posterior as defined in equation 23,

\[\hat{w}_{t^{\prime}}(x^{i}|z) =\sum_{x_{0},x_{1}\in\mathcal{D}}w(x^{i}|x_{0},x_{1})p_{t^{\prime} }(x_{0},x_{1}|z) \tag{70}\] \[=\sum_{x_{0},x_{1}\in\mathcal{D}}w(x^{i}|x_{0},x_{1})\frac{p_{t^{ \prime}}(z|x_{0},x_{1})\pi(x_{0},x_{1})}{p_{t^{\prime}}(z)}\] (71) \[=\sum_{x_{0},x_{1}\in\mathcal{D}}w(x^{i}|x_{0},x_{1})\frac{p^{ \prime}_{t}(z|x_{0},x_{1})\pi(x_{0},x_{1})}{p^{\prime}_{t}(z)}\] (72) \[=\sum_{x_{0},x_{1}\in\mathcal{D}}w(x^{i}|x_{0},x_{1})p^{\prime}_{ t}(x_{0},x_{1}|z)\] (73) \[=\hat{w}^{\prime}_{t}(x^{i}|z) \tag{74}\]

where in the 3rd equality we used both \(p_{t^{\prime}}(z|x_{0},x_{1})=p^{\prime}_{t}(z|x_{0},x_{1})\) and \(p_{t^{\prime}}(z)=p^{\prime}_{t}(z)\). 

## Appendix F Inference time

One potential benefit of non-autoregressive decoding is improved latency due to a significantly lower number of decoding steps. To demonstrate that, we measure the average latency of the proposed method compared with the autoregressive alternative using a single A100 GPU with \(80\) GB of RAM. We calculate the average latency time on the HumanEval benchmark using a batch size of 1. When considering 256 NFEs, the proposed method was found to be \(\sim\)2.5x faster than the autoregressive model (19.97 vs. 50.94 seconds on average per example). However, when considering 512 NFEs, both methods reach roughly the same latency. These results make sense as the number of tokens in most of the examples in HumanEval are below 512. Notice, that these results analyze latency and not model throughput. Due to the kv-caching mechanism following the autoregressive approach will result in significantly better throughput compared to the proposed approach Ziv et al. (2024). We leave the construction of a kv-cache mechanism to the proposed approach for future research.

## Appendix G Experimental setup

### Text

Data.We use three splits of data. First is OpenWebText (Gokaslan and Cohen, 2019). Second is the same mix used in Llama-2 (Touvron et al., 2023), including textual and code data. For the code-focused models we use the same split used in CodeLlama (Roziere et al., 2023). For the small models, we use OpenWebText. For the big models we use the Llama-2 and CodeLlama mixes.

Models.We train two sizes of models: small (150M parameters) and large (1.7B parameters). For the small model we used a DiT transformer architecture (Peebles and Xie, 2022) with 12 layers, 12 attention heads, and hidden dimension of 768. We also used GPT2 tokenizer. The small models were trained on OpenWebText. For the large model, we use also used a DiT transformer architecture but with 48 layers, 24 attention heads, and hidden dimension of 1536 (Peebles and Xie, 2022). For these models we used a tiktoken tokenizer. The large models were trained on the Llama-2 mix and the CodeLlama mix. For both models we used ROPE (Su et al., 2024) embedding with \(\theta=10000\). Models are trained with Adam optimizer with \(\beta_{1}=0.9\) and \(\beta_{2}=0.999\). We use dropout rate of 0.1. Models are trained with a warm-up of 2500 steps, with a peak learning rate of 3e-4. We train the big models with batch size of 4096 for 1.3 million iterations and the big models with batch size of 512 for 400 thousand iterations.

Entropy metric.We report the entropy of tokens within a sequence, averaged over all generated sequences. This intuitively quantifies the diversity of tokens within a given sequence. It's important to note that when computing sequence entropy, tokens not present in the sequence are excluded from consideration.

Generative perplexity metric.The generative perplexity metric is the average likelihood of generated text evaluated with a second (usually stronger) model. We report the generative perplexity when averaged over 1000 samples.

Double precision sampling.Zheng et al. (2024) demonstrated that sampling from a high-dimensional distribution with full precision can lead to a similar affect as sampling with temperature. We evaluate our model using a categorical sampler in double precision. Table 5 presents the results of baselines compared to our method.

### Image

Models.For all our experiments on CIFAR10 we use the U-Net architecture as in Dhariwal and Nichol (2021), with following three changes to make it fully discrete and time independent (as we used mask modeling): (i) We replace the first layer with an embedding table of size \(257\times 96\), and we stack the channel features such that the input to the U-Net is of shape \(288\times 32\times 32\). (ii) We enlarge the size of the final layer to output a tensor of shape \(3\times 32\times 32\times 257\). (iii) We remove the time dependency from architecture. The hyper-parameters of the architecture: channels 96, depth 5, channels multiple [3,4,4], heads channels 64, attention resolution 16, dropout 0.4, which gives a total parameters count of 113M. We optimize the network using Adam optimizer with \(\beta_{1}=0.9\) and \(\beta_{2}=0.999\), a learning rate of 1e-4. We trained with an effective batch size pf 512 for roughly 300K iterations.

Scheduler ablation.Figure 8 shows FID of our method with four different schedulers: Linear, Quadratic, Cubic, Cosine, both for training and evaluation. That is, for each scheduler we trained a model and evaluate FID with all four schedulers. We observe a high variance in FID between different schedulers, with the Cubic scheduler generally performing the best on both training and evaluation.

Comparison with baselines.In the following, we provide implementation details for producing Figure 2(a), that compares our schedulers and sampling algorithm with those employed by previous works.

MaskGIT (Chang et al., 2022).For the MaskGIT we train and sample with the Cosine scheduler \(\kappa(t)=1-\cos\left(\frac{\pi}{2}t\right)\) which is reported to achieve best results by Chang et al. (2022). For sampling we adjust the code from the re-implementation of Besnier and Chen (2023). In addition, we also search over the temperature in \(\{1,0.9,0.8,0.7,0.6,0.5\}\), and we find the best FID is achieved by temperature 1.

Figure 8: Comparison of FID on CIFAR10 with four schedulers: Linear, Quadratic, Cubic, Cosine, for both train and evaluation. Corrector sampling is not used in this experiment (i.e., \(\alpha_{t}=0\) in equation 26), and temperature is set to 1.

Code generation - qualitative examples

### Success cases

from typing import List

def below_zero(operations: List[int]) -> bool: """ You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account falls below zero, and at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >> below_zero([1, 2, -4, 5]) True """  balance = 0  for operation in operations:  if balance + operation < 0:  return True  balance = balance + operation  return False

def encode_shift(s: str): """  returns encoded string by shifting every character by 5 in the alphabet. """  return "".join([chr(((ord(ch) + 5 - ord("a")) % 26) + ord("a")) for ch in s])

def decode_shift(s: str): """  takes as input string encoded with encode_shift function.  Returns decoded string.  """  decoded = []  for ch in s:  decoded.append(chr(((ord(ch) - 5 - ord("a")) % 26) + ord("a")))  return "".join(decoded)from typing import List, Any

def filter_integers(values: List[Any]) -> List[int]:  """ Filter given list of any python values only for integers  >>> filter_integers(['a', 3.14, 5])  [5]  >>> filter_integers([1, 2, 3, 'abc', {}, []])  [1, 2, 3]  """  return [value for value in values if isinstance(value, int)]

def common(l1: list, l2: list):  """Return sorted unique common elements for two lists.  >>> common([1, 4, 3, 34, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121])  [1, 5, 653]  >>> common([5, 3, 2, 8], [3, 2])  [2, 3]  """  return sorted(set(l1) & set(l2))

def fibfib(n: int):  """The FibFib number sequence is a sequence similar to the Fibonacci sequnece that's defined as follows:  fibfib(0) == 0  fibfib(1) == 0  fibfib(2) == 1  fibfib(n) == fibfib(n-1) + fibfib(n-2) + fibfib(n-3).  Please write a function to efficiently compute the n-th element of the fibfib number sequence.  >>> fibfib(1)  0  >>> fibfib(5)  4  >>> fibfib(8)  24  """  if n == 0:  return 0  elif n == 1:  return 0  elif n == 2:  return 1  else:  return fibfib(n-1) + fibfib(n-2) + fibfib(n-3)

### Failure cases

defsolve(s):  """Youaregivenasing.  ifs[i]isaletter,reverseitscasefromlowertoupperorviseversa,  otherwisekeepitasitis.  Ifthestringcontainsnoletters,reversethestring.  Thefunctionshouldreturntheresultedstring.  Examples  solve("1234")="4321"  solve("ab")="AB"  solve("aaC")="A4C"  """  ifsisNone:  return""  iflen(s)==0:  returns  #resultstring  res=str()  #tolowercase  s=s.lower()  foriinrange(len(s)):  ch=s[i]  ifch.islower():  res.append(ch)  elifch.isupper():  res.append(ch.lower())  else:  res.append(ch)  returnres

fromtypingimportList,Tuple

defrolling_max(numbers:List[int])->List[int]:  """Fromagivenlistofintegers,generatealistofrollingmaximumelement founduntilgivenmomentinthesequence.  >>>rolling_max([1,2,3,2,3,4,2])  [1,2,3,3,3,4,4]  """  max_number=0  result=[]  fornumberinnumbers:  max_number=max(result)  ifnumber>max_number:  result.append(number)  else:  result=[max_number]  returnresultdeftruncate_number(number:float)->float:  """Given a positive floating point number, it can be decomposed into  and integer part (largest integer smaller than given number) and decimals  (leftover part always smaller than 1).  Return the decimal part of the number.  >>> truncate_number(3.5)  0.5  """  return math.floor(number)

from typing import List, Tuple

def sum_product(numbers: List[int])-> Tuple[int, int]:  """ For a given list of integers, return a tuple consisting of a sum and a  product of all the integers in a list. Empty sum should be equal to 0 and  empty product should be equal to 1.  >>> sum_product([])  (0, 1)  >>> sum_product([1, 2, 3, 4])  (10, 24)  """  if len(numbers)==0 or numbers[0]==0:  return 0, 1  sum = 0  product = 1  for num in numbers:  if abs(num)!=0:  product = product * num  sum += num  return sum, product

from typing import List

def factorize(n: int)-> List[int]:  """Return list of prime factors of given integer in the order from smallest  to largest. Each of the factors should be listed number of times corresponding  to how many times it appears in factorization. Input number should be equal  to the product of all factors.  >>> factorize(8)  [2, 2, 2]  >>> factorize(25)  [6, 5]  >>> factorize(70)  [2, 5, 7]  """  factors = []  factor = 1  for i in range(2, n):  factor *= i  while n % factor == 0:  factors.append(factor)  factor += 1  return factorsfrom typing import List, Optional

def longest(strings: List[str]) -> Optional[str]:  """Out of list of strings, return the longest one. Return the first one in  case of multiple strings of the same length. Return None in case the input  list is empty.  >>> longest([])  >>> longest(['a', 'b', 'c'])  'a'  >>> longest(['a', 'bb', 'ccc'])  'ccc'  """  long = ""  for string in strings:  if len(string) > len(long):  long = string  return long 

### Infilling

def bubbleSort(arr):  n = len(arr)  # optimize code, so if the array is already sorted, it doesn't need  # to go through the entire process  # Traverse through all array elements  for i in range(n-1):

 # range(n) also work but outer loop will  # repeat one time more than needed.  # Last i elements are already in place  swapped = False  for j in range(0, n-i-1):  # traverse the array from 0 to n-i-1  # Swap if the element found is greater  # than the next element  if arr[j] > arr[j + 1]:  swapped = True  arr[j], arr[j + 1] = arr[j + 1], arr[j]  if not swapped:  # if we haven't needed to make a single swap, we  # can just exit the main loop.  return

 # Function to perform Breadth First Search on a graph
represented using adjacency list  def bfs(adjList, source, visited):  # Create a queue for BFS  q = deque()  # Mark the current node as visited and enqueue it  visited[ source ] = True  q.append( source)  # Iterate over the queue  while q:  # Dequeue a vertex from queue and print it  currentNode = q.popleft()  print( currentNode, end=" ")  # Get all adjacent vertices of the dequeued vertex  # If an adjacent has not been visited, then mark it visited and enqueue it  for adjacent in adjList[ currentNode]:  if not visited[ adjacent ]:  visited[ adjacent ] = True  q.append( adjacent )

# Returns index of x in arr if present, else -1 def binary_search(arr, low, high, x):

 # Check base case  if high >= low :

 mid = ( high + low ) // 2

 # If element is present at the middle itself  if arr[ mid ] == x:  return mid

 # If element is smaller than mid, then it can only  # be present in left subarray  elif arr[ mid ] > x:  return binary_search(arr, low, mid - 1, x)

 # Else the element can only be present in right subarray  else:  return binary_search(arr, mid + 1, high, x)

 else:  # Element is not present in the array  return -1

#PythonprogramforDijkstra'ssingle
#sourceshortestpathalgorithm.Theprogramis
#foradjacencymatrixrepresentationofthegraph classGraph(): def__init__(self,vertices): self.V=vertices self.graph=[[0forcolumninrange(vertices)] forrowinrange(vertices)] defprintSolution(self,dist): print("VertexDistancefromSource") fornodeinrange(self.V): print(node,"",dist[node])
#Autilityfunctiontofindthevertexwith
#minimumdistancevalue,fromthesetofvertices
#notyetincludedinshortestpathtree defminDistance(self,dist,sptsSet):
#Initializeminimumdistancefornextnode min=1e7
#Searchnotnearestvertexnotinthe
#shortestpathtree forvinrange(self.V): ifdist[v]<minandsptsSet[v]==False: min=dist[v] min_index=v returnmin_index
#FunctionthatimplementsDijkstra'ssinglesource
#shortestpathalgorithmforagraphrepresented
#usingadjacencymatrixrepresentation defdijkstra(self,src):
#dist=[1e7]*self.V dist[src]=0 processed=[False]*self.V forcoutinrange(self.V):
#Picktheminimumdistancevertexfrom
#thesetofverticesnotyetprocessed.
#usalwaysequaltosrcinfirstiteration uv=self.minDistance(dist,processed)
#Puttheminimumdistancevertexinthe
#shortestpathtree processed[uv]=True
#Updatedistancevalueoftheadjacentvertices
#ofthepickedvertexonlyifthecurrent
#distanceisgreaterthannewdistanceand
#thevertexintnotintheshortestpathtree forvinrange(self.V): if(self.graph[uv][v]>0and processed[uv]==Falseand dist[uv]>dist[cout]+self.graph[uv][v]): dist[uv]=dist[cout]+self.graph[uv][v] self.printSolution(dist)Textual generations

We present below example generations for the proposed method together with several baseline methods. We provide both conditional and unconditional generations. For the conditional generations, we mark the prompt in gray.

### Conditional generation

The United States on Wednesday asked the UN Security Council to slap an oil embargo on North Korea and freeze the assets of leader Kim Jong-un, in response to Pyongyang's response to the revelations it had restarted its nuclear work in March. "We will continue working to use maximum international pressure on North Korea to agree to the suspension of its nuclear program and reinstate sanctions," said John Bolton, who served as National Security Advisor and Secretary of State under US President Bill Clinton. "Here is North Korea's response to our sanctions," Bolton wrote in a letter to House Minority Leader Nancy Pelosi. "We want you to know that the international community is seriously monitoring North Korea at this time. North Korea is still complying with our requests from the past few days," Bolton said on Monday. "We have been working through the United Nations to provide the information that they gave us." Asked to whether any international pressure will be put in place for North Korea to give up its nuclear weapons, Bolton said the United States can use maximum pressure to get North Korea to abandon its nuclear weapons if it wants. "We've been working to use maximum pressure on North Korea, through the Security Council, and we will continue to do so," said White House Deputy Press Secretary Sarah Huckabee Sanders in Washington. "We're committed to taking any steps necessary to help North Korea pursue its only option for peace, including in this period," she added. The United States did not plan to produce any more oil at this time last year and had not planned to do so this year. "We believe that the North Korea approach is misguided in moving forward with its nuclear program to endanger peace and security in its homeland and neighbors in Asia," said Bolton, adding that the US supplies its nuclear weapons. "We don't want them to sell their nuclear weapons to other nations," he said. Bolton said the US would look for pressure on North Korea, which has been known to use nuclear weapons, as leverage to negotiations with the US. "I will reiterate what I have said before. So, the US has to put pressure on North Korea. But who else is going to hold the cards? Somebody else has to hold the cards," Mr Bolton said. Bolton described what the United States is prepared to do to get North Korea to agree to give up its weapons and asks for sanctions. "As far as I know, we have to use the pressure the reason for putting sanctions on North Korea," he said, adding that the US does not plan to ask the UN Security Council for sanctions alone.

The defender is available for the Maribor first leg but his club believed he should be suspended. SNS Group Celtic made an administrative blunder which saw Efe Ambrose left behind

in the midfield in the Maribor department and has given him a potential three-match ban today. Although Efe Ambrose will be suspended next Friday, according to reports in Scottish media, the Celtic defender will still be fit for the Champions League first leg at Celtic Stadium in the middle of August. However, the Celtic club wrongly thought that Efe should only receive a three-match ban because he is available for the first leg. Although Efe Ambrose may receive a three-match ban next Friday, Efe Ambrose was part of the Celtic squad for the last match against Liverpool. However, says SNS Group Celtic he was making a tactical error and was left behind in midfield. It is understood that Efe Ambrose did not make the final squad and only played 11 games for the club this season. Efe Ambrose made his professional debut for Celtic in 2008 but spent nine months looking for a new club to return to. With a career-high 72 Celtic appearances, Efe is among Celtic's most capped players ever.

Carl Jara aka Grain Damaged is an award-winning, professional sand sculptor from Cleveland, Ohio. Jara says he has known since high-school that he wanted to be an artist. After studying

English and Art History at the Northeastern University, Jara says one semester he started carving a custom sculpture into sand molds, but didn't know how to do it. With the help of an instructor, he found out and learned how to use rubber molds to make art. Later, he made the decision to learn how to use sand and sculpt himself. In addition to how he makes his own sculptures, Jara says he does special events for comics companies such as Stan Lee and also for institutions like local community colleges and universities. In November of this year, he won the WWHS, The Very Art Of The Very Things Cleveland competition. Afterward, he will continue carving for clients in the comics industry and looks forward to sand sculpting in Cleveland. The Artist is professional sculptor who has been making art, for over 25 years, in various shapes and sizes.The artist says art is all about relationships and the best way to go into the heart is to create art. The artist has taught in various high schools in the Cleveland area and has taught a full time Honors Studio for High School students in grades 6, 7, and 8 time for over 20 years. Since Art is a personal form of artistic expression, he works individually in a way that allows the student that his work engages their imagination and presents ideas in ways that inform and challenge their own paths. Miguel Romano is a professional artist who worked in 3D modeling and animation in the areas of web design and production. The artist currently works as a digital artist in the ad and mass communication industries. In coming to Concrete Cleveland, he is excited to apply the 3D development and production skills he have to his work. The artist has a BFA in sculpture from Ohio University, along with an MFA in sculpture. We look forward to seeing his work very soon! Designed and installed by Concrete This Week. He is a guy originally from Cleveland, Ohio where he pursued a career as a nurse. He then moved to the Atlanta, GA area where he returned to school with a BSN and a BS in nursing and is a licensed nurse. He is a proud sorority brother and still has extra-curricular, as well as taking music lessons and the occasional play. He is a lovely asset at Concrete Cleveland and looks forward to seeing concrete 

### Unconditional generation

Here's how that strategy works for your job:

1) You now plan upon what you accomplish to fulfill your goals.

Management cannot plan what happens to you. This may not be your ultimate personal decision, but it's perfectly fine to look at it. You just need to make sure you want to achieve this.

Now, because you've worked at goal, you don't have to talk about your status tomorrow, after all, you have to do your job and take care of yourself.

Next steps, there may be some work to do. There is a company down the road you right - literally millions of things that would have to be done. But of course it would have a different outlook. If you're going to do something, the customer might not be able to tell you.

2) Between those two steps are the plan in step so that your actions will be executed.

Then you have taken other steps (usually a few less important changes), like delivery. If you already know what that means, and you're having to stay up and take action you can make sure you don't have to point out in the moment to plan the action.

With business goals, it is not easy to pick up what appears best for us. We have to see what really is. What we do. We can't make a plan on the floor and come back up with exactly what you're doing. If you want to work every step, then you need to differentiate from the action and what the next step represents.

Eventually, you'll be less motivated to focus on this step and the previous one. Unfortunately if you don't change your main thing, you may be able to lose your motivation to work on "pivot." Unless that's possible, and if you don't change something, then the task may not be at the right time. Instead of doing something, it is just in advance of your ultimate goal.

Although you might make a mistake with every single day to day plan, it still is a great opportunity to correct your mistake, become new and commit to working extra hours and meeting your goals promptly.

The truth is, everything goes right for you no matter how quick a decision you become. The customer will never allow you to make the worst decisions. Otherwise, you make the very first decision.

3) Take timing as part of action. If you don't feel like you can keep it, a plan without help of timing stops you from doing. When it's like your plan in action can lead to something such as this: Now that you know what to do. For example, you might live in a place in the building that serves every customer, has 3 employees per team, and 3 clients on one. You will get things done the next day. Change your performance is the first step towards greater success, for example. Your team, at this point in the Customer department, will know how the customer deal with a single employee, the level chain, and more. Make sure you take action now that you change it. As a company, it won't be hard but you will have lots of work to do when you change.

4) Make sure it's your night.

Watch the humour but also the humanity behind the work we're doing. The truth is something very tragic and delicate in the middle of a very fractured world. It's the one thing that makes me proud. I feel like a singular individual has had to come together with this story. There's a lot of people who I've worked with for the very beginning, because I have got people, you've got people who have just had these eyes on this story, and this sense of what we are, that run through our final movie.

It's the very beginning we're at. The very beginning, we're not there, we will get there but we won't need. This is a story and these amazing actors, these fantastic violence, violence, that was just are elements and a complex world of conflict. When something like that is set to build this narrative and you're directing the world of these characters based around their individual needs it's very, extremely confusing, very heartbreaking -- it's really quite intense--this was all built within it, and what it is, it's a 35-year old period that was slavery and still was very strong, these guys were operating to the edge and going to the point where we ended up setting up a big narrative, OK, that's good, it's okay, in some ways heroism is a noble imperative that we are fighting against, and recognize maturity as the mercurial nature and these are all human and we've got to clean it up so that that stuff is there and we've got to restore it. And the project we're looking at here is our common goal is that anything can be done to make that happen and everybody can do whatever their want to do and do it as they please.

That's the spirit of it. That's the movie I've made with Steven Wright in writing.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes]
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] As each experiment is highly resource intensive we did not report error bars. However, we did report results on a wide range of tasks and setups.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No]
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes]
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] See Discussion section.
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]