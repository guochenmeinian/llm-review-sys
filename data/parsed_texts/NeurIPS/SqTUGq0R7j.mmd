# Persuading Farsighted Receivers in MDPs:

the Power of Honesty

 Martino Bernasconi

Bocconi University

martino.bernasconi@unibocconi.it &Matteo Castiglioni

Politecnico di Milano

matteo.castiglioni@polimi.it &Alberto Marchesi

Politecnico di Milano

alberto.marchesi@polimi.it &Mirco Mutti

Technion

mirco.m@technion.ac.il

Work done while the author was at Politecnico di Milano.

Technion

mirco.m@technion.ac.il

###### Abstract

_Bayesian persuasion_ studies the problem faced by an informed sender who strategically discloses information to influence the behavior of an uninformed receiver. Recently, a growing attention has been devoted to settings where the sender and the receiver interact _sequentially_, in which the receiver's decision-making problem is usually modeled as a _Markov decision process_ (MDP). However, previous works focused on computing optimal information-revelation policies (a.k.a. _signaling schemes_) under the restrictive assumption that the receiver acts _myopically_, selecting actions to maximize the one-step reward and disregarding future rewards. This is justified by the fact that, when the receiver is _farsighted_ and thus considers future rewards, finding an optimal Markovian signaling scheme is \(\NP\)-hard. In this paper, we show that Markovian signaling schemes do _not_ constitute the "right" class of policies. Indeed, differently from most of the MDPs settings, we prove that Markovian signaling schemes are _not_ optimal, and general _history-dependent_ signaling schemes should be considered. Moreover, we also show that history-dependent signaling schemes circumvent the negative complexity results affecting Markovian signaling schemes. Formally, we design an algorithm that computes an optimal and \(\epsilon\)-persuasive history-dependent signaling scheme in time polynomial in \(1/\epsilon\) and in the instance size. The crucial challenge is that general history-dependent signaling schemes cannot be represented in polynomial space. Nevertheless, we introduce a convenient subclass of history-dependent signaling schemes, called _promise-form_, which are as powerful as general history-dependent ones and efficiently representable. Intuitively, promise-form signaling schemes compactly encode histories in the form of _honest_ promises on future receiver's rewards.

## 1 Introduction

_Bayesian persuasion_(Kamenica and Gentzkow, 2011) is the problem faced by an informed _sender_ who wants to influence the behavior of an uninformed, self-interested _receiver_ through the provision of payoff relevant information. Bayesian persuasion captures many fundamental problems arising in real-world applications, _e.g._, online advertising (Bro Miltersen and Sheffet, 2012), voting (Alonso and Camara, 2016; Castiglioni et al., 2020; Castiglioni and Gatti, 2021), traffic routing (Bhaskar et al., 2016; Castiglioni et al., 2021), recommendation systems (Mansour et al., 2016), security (Rabinovich et al., 2015; Xu et al., 2016), marketing (Babichenko and Barman, 2017; Candogan, 2019), medical research (Koltoilin, 2015), and financial regulation (Goldstein and Leitner, 2018).

Most of the previous works study the classical, one-shot version of the Bayesian persuasion problem. However, in many application scenarios it is natural to assume that the sender and the receiver interact multiple times in a sequential manner. In spite of this, only a few very recent works addressed _sequential_ versions of the Bayesian persuasion problem (Wu et al., 2022; Bernasconi et al., 2022; Gan et al., 2022a,b). In particular, Wu et al. (2022) and Gan et al. (2022a,b) study settings where the sender and the receiver interact sequentially in a _Markov decision process_ (MDP).

In Bayesian persuasion problems in MDPs, at each step of the interaction both the sender and the receiver know the current state of the MDP, and the former has also access to some _private observation_ drawn according to a commonly-known, state-dependent distribution. The sender commits beforehand to an information-revelation policy, which is implemented by means of a _signaling scheme_ that sends randomized _action recommendations_ to the receiver, conditioned on the current (public) state and the sender's private observation. Specifically, the sender commits to a _persuasive_ signaling scheme, meaning that the receiver is always incentivized to follow recommendations. At the end of each step, the next state of the MDP and the agents' rewards are determined as a function of the current state, the action actually played by the receiver, and the sender's private observation in the current step.

Wu et al. (2022) and Gan et al. (2022a,b) provide algorithms that compute an optimal (_i.e._, reward-maximizing) persuasive signaling scheme under the restrictive assumption that the receiver acts _myopically_, selecting actions to maximize the one-step reward and disregarding future ones. This is justified by the fact that, when the receiver is _farsighted_ and thus considers future rewards, finding an optimal Markovian signaling scheme is \(\mathsf{NP}\)-hard to approximate, as shown by Gan et al. (2022a) for infinite-horizon MDPs. An analogous result also holds in finite-horizon MDPs for non-stationary Markovian signaling schemes, as we prove in this work as a preliminary result.

In this paper, we show that Wu et al. (2022) and Gan et al. (2022a,b) failed to provide positive results with farsighted receivers since Markovian signaling schemes do not constitute the "right" class of policies to consider. This is in stark contrast with most of the MDPs settings in which Markovian policies are optimal. Indeed, we prove that Markovian signaling schemes are _not_ optimal, and general _history-dependent_ signaling schemes should be considered. As a result, we focus on the problem of computing an optimal persuasive history-dependent signaling scheme. Surprisingly, we show that taking history into account allows to circumvent the negative result affecting Markovian signaling schemes. We do that by providing an approximation scheme that finds an optimal \(\epsilon\)-persuasive (_i.e._, one approximately incentivizing the receiver to follow action recommendations) history-dependent signaling scheme in time polynomial in \(1/\epsilon\) and the size of the problem instance.

The crucial challenge in designing our approximation scheme is that general, history-dependent signaling schemes cannot be represented in polynomial space. Our algorithm overcomes such an issue by using a convenient subclass of history-dependent signaling schemes, which we call _promise-form_ signaling schemes. The core idea of such signaling schemes is to compactly encode all the relevant information contained in an history into a _promise_ on future receiver's rewards. At each step of the process, a promise-form signaling scheme does _not_ only determines an action recommendation for the receiver, but it also makes a promise to them. First, we prove that promise-form signaling schemes are as powerful as general history-dependent ones. Then, we show how an optimal \(\epsilon\)-persuasive promise-form signaling scheme can be computed in polynomial time by means of a recursive procedure. To do that, we rely on a crucial result showing that, for signaling schemes that _honestly_ keep promises made to the receiver, persuasiveness constraints can be expressed as conditions defined locally at each step of the MDP, since the receiver only cares about sender's promises on their future rewards.2

Footnote 2: The complete proofs of all the results in the paper can be found in Appendices C, D, and E.

## 2 Preliminaries

In this work, we study Bayesian persuasion problems where a _farsighted_ receiver takes actions in a _time-inhomogeneous finite-horizon_ MDP (Puterman, 2014). Formally, a problem instance is a tuple

\[\left(\mathcal{S},\mathcal{A},\mathcal{H},\Theta,\{r_{h}^{\mathsf{S}}\}_{h\in \mathcal{H}},\{r_{h}^{\mathsf{R}}\}_{h\in\mathcal{H}},\{p_{h}\}_{h\in\mathcal{ H}},\{\mu_{h}\}_{h\in\mathcal{H}},\beta\right),\text{where:}\]

\(\mathcal{S}\) is a finite set of states, \(\mathcal{A}\) is a finite set of receiver's actions available in each state, \(\mathcal{H}\coloneqq[1,\ldots,H]\) is a set of time steps with \(H\) being the time horizon, \(\Theta\) is a finite set of sender's private observations, \(r_{h}^{\mathsf{S}},r_{h}^{\mathsf{R}}:\mathcal{S}\times\mathcal{A}\times \Theta\rightarrow[0,1]\) are reward functions for the sender and the receiver, respectively,\(p_{h}:\mathcal{S}\times\mathcal{A}\times\Theta\rightarrow\Delta(\mathcal{S})\) is a transition function, \(\mu_{h}:\mathcal{S}\rightarrow\Delta(\Theta)\) is a function defining probabilities of sender's private observations at each state, while \(\beta\in\Delta(\mathcal{S})\) is the initial state distribution.3

Footnote 3: In this paper, we let \(\Delta(X)\) be the set of all the probability distributions over a finite set \(X\), with \(d(x)\) denoting the probability assigned to \(x\in X\) by a distribution \(d\in\Delta(X)\). Moreover, given a function \(f:X\rightarrow\Delta(Y)\) with \(X,Y\) any two finite sets, for every \(x\in X\) we denote by \(f(y|x)\) the probability that \(f(x)\) assigns to \(y\in Y\).

We consider the most general setting in which the sender commits to a _non-stationary_ and _non-Markovian signaling scheme_ (henceforth called _history-dependent_ signaling scheme for short). For every step \(h\in\mathcal{H}\) and state \(s_{h}\in\mathcal{S}\) reached at that step, a history-dependent signaling scheme defines a randomized mapping from sender's private observations to action recommendations for the receiver, based on the whole history of states and receiver's actions observed up to step \(h\).4 Formally, in the following we let \(\mathcal{T}_{h}\) be the set of all the possible _histories_ up to step \(h\), which is defined as

Footnote 4: In this work, we use _direct_ signaling schemes which send signals in the form of action recommendations for the receiver. This is w.l.o.g. by well-known revelation principle arguments [10].

\[\mathcal{T}_{h}\coloneqq\{\tau\mid\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_ {h}):s_{i}\in\mathcal{S},a_{i}\in\mathcal{A}\},\]

while we let \(\mathcal{T}:=\mathcal{T}_{1}\cup\ldots\cup\mathcal{T}_{H}\) be the set of all the possible histories (of any length).5 Then, a history-dependent signaling scheme is defined as a set \(\phi:=\{\phi_{\tau}\}_{\tau\in\mathcal{T}}\) of functions \(\phi_{\tau}:\Theta\rightarrow\Delta(\mathcal{A})\), which define a mapping from sender's private observations to probability distributions over action recommendations for every possible history.

Footnote 5: By a simple revelation-principle-style argument, we can focus w.l.o.g. on signaling schemes which depend on histories that do _not_ include the sequence of private observations observed by the sender.

The interaction between the sender and the receiver goes as follows (Algorithm 1). **(i)** The sender publicly commits to a history-dependent signaling scheme \(\phi:=\{\phi_{\tau}\}_{\tau\in\mathcal{T}}\). **(ii)** An initial state \(s_{1}\sim\beta\) is drawn. **(iii)** At each step \(h\in\mathcal{H}\), both agents observe the current state \(s_{h}\in\mathcal{S}\) and the sender also gets a private observation \(\theta_{h}\in\Theta\) drawn according to \(\mu_{h}(s_{h})\), with the function \(\mu_{h}\) being known to both the sender and the receiver. **(iv)** The sender communicates to the receiver an action recommendation \(a_{h}\in\mathcal{A}\) sampled according to \(\phi_{\tau_{h}}(\theta_{h})\), where \(\tau_{h}\in\mathcal{T}_{h}\) is the history up to step \(h\). **(v)** The receiver plays an action \(\widehat{a}_{h}\in\mathcal{A}\), possibly different from \(a_{h}\). **(vi)** The sender and the receiver get rewards \(r^{\mathsf{R}}_{h}(s_{h},a_{h},\theta_{h})\) and \(r^{\mathsf{S}}_{h}(s_{h},a_{h},\theta_{h})\), respectively. **(vii)** If \(h=H\), the interaction ends, otherwise the next state \(s_{h+1}\sim p_{h}(s_{h},\widehat{a}_{h},\theta_{h})\) is drawn and the interaction continues to step \(h+1\) starting from the third point. As customary in the literature (see, _e.g._, [1]), we assume that, if the receiver does _not_ follow recommendations at some step \(h\in\mathcal{H}\) by playing an action \(\widehat{a}_{h}\neq a_{h}\), then the sender stops issuing future recommendations to the receiver.

For ease of presentation, we introduce the sender's value function \(V^{\mathsf{S},\phi}_{h}:\mathcal{T}_{h}\rightarrow\mathbb{R}\) to encode sender's expected rewards by using a history-dependent signaling scheme \(\phi:=\{\phi_{\tau}\}_{\tau\in\mathcal{T}}\) from step \(h\in\mathcal{H}\) onwards, assuming the receiver always follows recommendations. Given a history \(\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\) up to step \(h\), such a value function is recursively defined as:

\[V^{\mathsf{S},\phi}_{h}(\tau)=\sum_{\theta\in\Theta}\sum_{a\in\mathcal{A}}\mu_{ h}(\theta|s_{h})\phi_{\tau}(a|\theta)\left(r^{\mathsf{S}}_{h}(a,s_{h},\theta)+ \sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)V^{\mathsf{S}, \phi}_{h+1}(\tau\oplus(a,s^{\prime}))\right).\]

Similarly, we introduce the receiver's action-value function \(V^{\mathsf{R},\phi}_{h}:\mathcal{A}\times\mathcal{T}_{h}\rightarrow\mathbb{R}\) to encode the receiver's expected rewards by following sender's action recommendations from step \(h\in\mathcal{H}\) onwards. Formally, given a history \(\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\) up to step \(h\), the receiver's expected reward by following the recommendation to play \(a\in\mathcal{A}\) is recursively defined as follows:

\[V^{\mathsf{R},\phi}_{h}(a,\tau)=\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h})\phi_{ \tau}(a|\theta)\left(r^{\mathsf{R}}_{h}(a,s_{h},\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)V^{\mathsf{R},\phi}_{h+1}(\tau \oplus(a,s^{\prime}))\right),\]where \(V_{h}^{\mathsf{R},\phi}:\mathcal{T}_{h}\rightarrow\mathbb{R}\) is such that \(V_{h}^{\mathsf{R},\phi}(\tau)=\sum_{a\in\mathcal{A}}V_{h}^{\mathsf{R},\phi}(a,\tau)\) for every \(h\in\mathcal{H}\) and \(\tau\in\mathcal{T}_{h}\).

Finally, we introduce an additional receiver's value function, denoted by \(\widehat{V}_{h}^{\mathsf{R}}:\mathcal{S}\rightarrow\mathbb{R}\), to encode receiver's expected rewards from step \(h\in\mathcal{H}\) onwards _after having deviated_ from recommendations. Formally, for every state \(s\in\mathcal{S}\), such a value function is recursively defined as follows:

\[\widehat{V}_{h}^{\mathsf{R}}(s)=\max_{a\in\mathcal{A}}\sum_{\theta\in\Theta} \mu_{h}(\theta|s)\left(r_{h}^{\mathsf{R}}(a,s,\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\widehat{V}_{h+1}^{\mathsf{R}}(s^{ \prime})\right),\]

where the maximum operator encodes the fact that the receiver plays so as to maximize future rewards without knowledge of realized sender's private observations after having deviated.6

Footnote 6: Notice that, in all the steps reached after having deviated from recommendations, the receiver does _not_ get any clue about the senderâ€™s private information, and, thus, their expected reward only depends on the current state (_not_ on the history). In other words, after deviating the receiver is playing a new MDP in which, by taking expectations with respect to \(\mu_{h}\), rewards and transition probabilities are defined as \(\tilde{r}_{h}(s,a):=\sum_{\theta\in\Theta}\mu_{h}(\theta|s)r_{h}^{\mathsf{R}}( s,a,\theta)\) and \(\tilde{p}_{h}(s^{\prime}|s,a):=\sum_{\theta\in\Theta}\mu_{h}(\theta|s)p_{h}(s^ {\prime}|s,a,\theta)\), respectively, for all \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(a\in\mathcal{A}\), and \(s^{\prime}\in\mathcal{S}\).

By the revelation principle [Kamenica and Gentzkow, 2011], it is well known that in order to find an optimal signaling scheme it is possible to focus w.l.o.g. on (direct) history-dependent signaling schemes under which the receiver is always incentivized to follow sender's action recommendations. These are called _persuasive_ signaling scheme, and they are formally defined as follows:

**Definition 1** (\(\epsilon\)-persuasiveness).: _Let \(\epsilon\geq 0\). A history-dependent signaling scheme \(\phi:=\{\phi_{\tau}\}_{\tau\in\mathcal{T}}\) is said to be \(\epsilon\)-persuasive if, for every step \(h\in\mathcal{H}\), history \(\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\) up to step \(h\), and pair of actions \(a,a^{\prime}\in\mathcal{A}\), the following holds:_

\[V_{h}^{\mathsf{R},\phi}(a,\tau)\geq\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h} )\phi_{\tau}(a|\theta)\left(r_{h}^{\mathsf{R}}(s_{h},a^{\prime},\theta)+\sum_ {s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a^{\prime},\theta)\widehat{V }_{h+1}^{\mathsf{R}}(s^{\prime})-\epsilon\right).\]

_Moreover, we say that the signaling scheme is persuasive if the conditions above hold for \(\epsilon=0\). We denote the set of all the persuasive signaling schemes by \(\Phi\)._

Whenever the sender commits to an \(\epsilon\)-persuasive history-dependent signaling scheme, we assume that the receiver always follows the recommendation issued by the sender. Notably, the recommended action is guaranteed to provide the receiver expected rewards that are at worst \(\epsilon\) less than those attained by the best possible action.

In conclusion, the goal is to find an _optimal_ signaling scheme for the sender, which is one achieving a sender's expected reward (from step one) \(V^{\mathsf{S},\phi}\) greater than or equal to \(\mathsf{OPT}\), defined as follows:

\[\mathsf{OPT}:=\max_{\phi\in\Phi}\,V^{\mathsf{S},\phi},\quad\text{where we let }V^{\mathsf{S},\phi}:=\sum_{s\in\mathcal{S}}\beta(s)V_{1}^{\mathsf{S},\phi}((s)).\]

## 3 History-dependent signaling schemes are necessary

Previous works studying Bayesian persuasion in MDPs [Wu et al., 2022, Gan et al., 2022a] focus on Markovian signaling schemes, in which the action recommendation at step \(h\) only depends on the current state \(s_{h}\) and private observation \(\theta_{h}\). Indeed, considering this class of signaling schemes is sufficient to optimize the utility of a sender facing a myopic receiver. Here, we show that this is _not_ the case when the receiver is farsighted. In particular, we show that non-stationary Markovian signaling scheme are suboptimal. To do so, we show that there exists an MDP (see Appendix A) in which the optimal persuasive signaling scheme is history-dependent.7 Intuitively, an history-dependent signaling scheme can adjust action recommendations depending on the choices available to the receiver in previous steps. Thus, if the receiver had profitable opportunities in the past, the sender must provide a larger expected reward to the receiver in order to be persuasive. Otherwise, the sender can aggressively maximize their expected rewards irrespective of receiver's ones. Formally:

Footnote 7: We defer to Appendix A the complete description of the instance, together with calculations of the optimal signaling schemes (for each class) and their corresponding value functions.

**Theorem 1**.: _There exist instances in which a persuasive history-dependent signaling scheme guarantees sender's expected rewards strictly greater than that obtained by an optimal persuasive non-stationary Markovian signaling scheme._Moreover, we show that optimal non-stationary Markovian signaling schemes are \(\mathsf{NP}\)-hard to approximate in polynomial time, even when the persuasiveness requirement is relaxed. This further motivates the use of history-dependent signaling schemes when addressing Bayesian persuasion problems in finite-horizon MDPs with a farsighted receiver. Formally, we prove the following:

**Theorem 2**.: _There exist two constants \(\alpha<1\) and \(\epsilon>0\) such that computing an \(\epsilon\)-persuasive non-stationary Markovian signaling that provides the sender with at least a fraction \(\alpha\) of the optimal sender's expected reward \(\mathsf{OPT}\) is \(\mathsf{NP}\)-hard._

## 4 A sufficient subclass of efficiently-representable signaling schemes

Working with history-dependent signaling schemes begets unavoidable computational issues. These are due to the fact that explicitly representing such signaling schemes requires a number of bits growing exponentially in the size of the problem instance, since the number of possible histories is exponential in the time horizon \(H\). In this section, we show how to circumvent such an issue by introducing a convenient subclass of signaling schemes--called _promise-form_ signaling schemes--which are efficiently representable while being as good as history-dependent ones. In particular, our main result in this section (Theorem 3) shows that there always exists a promise-form signaling scheme which results in sender's expected rewards equal to its optimal value \(\mathsf{OPT}\).

### Promise-form signaling schemes

A promise-form signaling scheme is defined by a set \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) of triplets, where:

* \(I_{h}:\mathcal{S}\to 2^{[0,H]}\) is a function defining, for every state \(s\in\mathcal{S}\), a finite set \(I_{h}(s)\subseteq[0,H]\) of _promises_ for step \(h\in\mathcal{H}\). We add the additional requirement that \(0\in I_{1}(s)\) for all \(s\in\mathcal{S}\), and, for ease of notation, we set \(I_{H+1}(s):=\{0\}\) for every \(s\in\mathcal{S}\) and \(\mathcal{I}:=\bigcup_{h\in\mathcal{H}}\bigcup_{s\in\mathcal{S}}I_{h}(s)\).
* \(\varphi_{h}:\mathcal{S}\times\mathcal{I}\times\Theta\to\Delta(\mathcal{A})\) is an _action-recommendation strategy_ to be employed at step \(h\in\mathcal{H}\), where \(\varphi_{h}(a|s,\iota,\theta)\) is the probability of recommending action \(a\in\mathcal{A}\) in state \(s\in\mathcal{S}\) when the promise is \(\iota\in I_{h}(s)\) and the sender's private observation is \(\theta\in\Theta\).
* \(g_{h}:\mathcal{S}\times\mathcal{A}\times\mathcal{I}\times\mathcal{S}\to \mathcal{I}\) is a _promise function_ for step \(h\in\mathcal{H}\) such that, whenever \(h\leq H\) and \(\iota\in I_{h}(s)\), \(g_{h}(s,a,\iota,s^{\prime})\in I_{h+1}(s^{\prime})\) represents the promise for step \(h+1\) if the next state is \(s^{\prime}\in\mathcal{S}\) and, at the current step \(h\), action \(a\in\mathcal{A}\) is recommended in state \(s\in\mathcal{S}\).8

Footnote 8: Notice that, in order to completely specify a promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\), it is sufficient to specify the functions \(\varphi_{h}\) and \(g_{h}\), since the functions \(I_{h}\) can always be inferred by looking at the images of the functions \(g_{h}\). However, we included \(I_{h}\) in the definition of promise-form signaling schemes since this will considerably ease notation when dealing with them in Sections 5 and 6.

Intuitively, the rationale behind promise-form signaling schemes is that, when reaching a state \(s_{h}\in\mathcal{S}\) at step \(h\in\mathcal{H}\), the sender "promises" a value \(\iota\in I_{h}(s_{h})\) to the receiver, representing a lower bound on future rewards obtained by following recommendations. Moreover, sender's action recommendations only depend on the current state \(s_{h}\in\mathcal{S}\), the sender's private observation \(\theta_{h}\in\Theta\), and the current promise \(\iota\in I_{h}(s_{h})\), through the distribution \(\varphi_{h}(s_{h},\iota_{h},\theta_{h})\). Notice that it is always possible to infer the current promise by looking at the history of past states and action recommendations, by "composing" the functions \(g_{h^{\prime}}\) for the steps \(h^{\prime}<h\). This crucially avoids having to specify an explicit dependency on the full history of past states and action recommendations.

Notice that a promise-form signaling scheme as defined above does _not_ automatically guarantee that the sender _honestly_ keeps their promises. Indeed, in order to ensure that this is the case, we need to enforce additional constraints on the components of the signaling scheme, as we show in Section 4.3.

Let us remark that representing promise-form signaling schemes requires a number of bits polynomial in the size of the problem instance and in \(|\mathcal{I}|\), which is the cardinality of the set of promises. While \(|\mathcal{I}|\) could be arbitrarily large in general, the algorithm that we will present in the following Section 5 guarantees that \(|\mathcal{I}|\) has "small" size, by means of a clever choice of the functions \(I_{h}\).

### From promise-form to history-dependent signaling schemes

In the following, we show how the sender can implement promise-form signaling schemes, proving that they represent a subclass of history-dependent ones.

The sender can implement a promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) as follows. After committing to \(\sigma\) (Line 1 of Algorithm 1), at each step \(h\in\mathcal{H}\), in Line 6 of Algorithm 1 they select which action-recommendation strategy to use by reconstructing the current promise on the basis of the history \(\tau_{h}\). Such a reconstruction is done by recursively "composing" functions \(g_{h^{\prime}}\) for the preceding steps \(h^{\prime}<h\), by means of the procedure in Algorithm 2 with \(\sigma\) and \(\tau_{h}\) as inputs. By letting \(\iota\in I_{h}(s_{h})\) be the continuation value obtained by running Algorithm 2, the action recommendation \(a_{h}\) in Line 6 is then sampled from \(\varphi_{h}(s_{h},\iota,\theta_{h})\). Algorithm 2 clearly runs in time polynomial in the instance size, and, thus, the sender can implement a promise-form signaling scheme efficiently.

```
0:\(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\), \(\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\)
1: Initialize \(\iota\gets 0\in I_{1}(s_{1})\)
2:for each step \(h^{\prime}=1,\ldots,h-1\)do
3:\(\iota\gets g_{h^{\prime}}(s_{h^{\prime}},a_{h^{\prime}},\iota,s_{h^{ \prime}+1})\)
4:return\(\iota\) ```

**Algorithm 2** From histories to promises

In the rest of this section, given a promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\), we denote by \(\phi^{\sigma}:=\{\phi^{\sigma}_{\tau}\}_{\tau\in\mathcal{T}}\) the history-dependent signaling scheme _induced_ by \(\sigma\) thorough the implementation procedure described above. Formally, for every history \(\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\) up to step \(h\in\mathcal{H}\), the function \(\phi^{\sigma}_{\tau}:\Theta\rightarrow\Delta(\mathcal{A})\) is defined so that \(\phi^{\sigma}_{\tau}(\theta)=\varphi(s_{h},\iota^{\sigma}_{\tau},\theta)\) for every \(\theta\in\Theta\), where \(\iota^{\sigma}_{\tau}\in I_{h}(s_{h})\) denotes the promise value corresponding to history \(\tau\), as computed by Algorithm 2 with \(\sigma\) and \(\tau\) as inputs. As it is easy to see, implementing the promise-form signaling scheme \(\sigma\) as described above is equivalent to using \(\phi^{\sigma}\) in Algorithm 1.

Next, we show that the value functions of the sender and the receiver associated with the induced history-dependent signaling scheme \(\phi^{\sigma}\) can be efficiently computed by only accessing the components of the promise-form signaling scheme \(\sigma\). In the following, given any \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\), for every step \(h\in\mathcal{H}\) we introduce the functions \(\mathcal{V}^{\mathsf{R},\sigma}_{h}:\mathcal{A}\times\mathcal{S}\times\mathcal{ I}\rightarrow\mathbb{R}\) and \(\mathcal{V}^{\mathsf{R},\sigma}_{h}:\mathcal{S}\times\mathcal{I}\rightarrow \mathbb{R}\), which are jointly recursively defined so that, for every \(a\in\mathcal{A}\), \(s\in\mathcal{S}\), and \(\iota\in I_{h}(s)\), it holds:

\[\mathcal{V}^{\mathsf{R},\sigma}_{h}(a,s,\iota)= \sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota, \theta)\Bigg{(}r^{\mathsf{R}}_{h}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p _{h}(s^{\prime}|s,a,\theta)\mathcal{V}^{\mathsf{R},\sigma}_{h+1}(s^{\prime},g_ {h}(s,a,\iota,s^{\prime}))\Bigg{)}\]

and \(\mathcal{V}^{\mathsf{R},\sigma}_{h}(s,\iota)=\sum_{a\in\mathcal{A}}\mathcal{V }^{\mathsf{R},\sigma}_{h}(a,s,\iota)\). Similarly, \(\mathcal{V}^{\mathsf{S},\sigma}_{h}:\mathcal{S}\times\mathcal{I}\rightarrow \mathbb{R}\) is such that, for \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\):

\[\mathcal{V}^{\mathsf{S},\sigma}_{h}(s,\iota)= \sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \varphi_{h}(a|s,\iota,\theta)\Bigg{(}r^{\mathsf{S}}_{h}(s,a,\theta)+\sum_{s^{ \prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\mathcal{V}^{\mathsf{S}, \sigma}_{h+1}(s^{\prime},g_{h}(s,a,\iota,s^{\prime}))\Bigg{)}.\]

Then, we can prove the following lemma:

**Lemma 1**.: _Given a promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\), for every \(h\in\mathcal{H}\) and history \(\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\) up to step \(h\), the following holds:_

\[V^{\mathsf{R},\phi^{\sigma}}_{h}(a,\tau)=\mathcal{V}^{\mathsf{R},\sigma}_{h}(a,s _{h},\iota^{\sigma}_{\tau}),\ V^{\mathsf{R},\phi^{\sigma}}_{h}(\tau)=\mathcal{V }^{\mathsf{R},\sigma}_{h}(s_{h},\iota^{\sigma}_{\tau}),\ \text{and}\ V^{\mathsf{S},\phi^{\sigma}}_{h}(\tau)=\mathcal{V}^{\mathsf{S}, \sigma}_{h}(s_{h},\iota^{\sigma}_{\tau}).\]

Intuitively, Lemma 1 establishes that the functions \(\mathcal{V}^{\mathsf{R},\sigma}_{h}\), \(\mathcal{V}^{\mathsf{R},\sigma}_{h}\), and \(\mathcal{V}^{\mathsf{S},\sigma}_{h}\) "correctly" encode the value functions of the sender and the receiver for a promise-form signaling scheme \(\sigma\), when it is implemented according to the procedure described at the beginning of the section.

Finally, given how a promise-form signaling scheme \(\sigma\) is implemented, in the following we say that \(\sigma\) is \(\epsilon\)-persuasive (for some \(\epsilon\geq 0\)) if the induced history-dependent signaling scheme \(\phi^{\sigma}\) is \(\epsilon\)-persuasive according to Definition 1. However, using such a definition to check whether \(\sigma\) is \(\epsilon\)-persuasive is clearly computationally inefficient, since it would require working with exponentially-many histories. In Section 4.3, we introduce an easy way to ensure that a promise-form signaling scheme "keeps its promises", and we show that this allows to encode \(\epsilon\)-persuasiveness constraints in an efficient way.

### The power of honesty

Next, we introduce a particular class of promise-form signaling schemes which always guarantee that the sender _honestly_ (approximately) assures promised rewards to the receiver. We call \(\eta\)_-honest_ the promise-form signaling schemes with such a property, which are formally defined as follows:

**Definition 2** (\(\eta\)-honesty).: _Let \(\eta\geq 0\). A promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) is \(\eta\)-honest if, for every step \(h\in\mathcal{H}\), state \(s\in\mathcal{S}\), and promise \(\iota\in I_{h}(s)\), the following holds:_

\[\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s, \iota,\theta)\left(r^{\mathsf{R}}_{h}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p _{h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^{\prime})\right)\geq\iota-\eta.\] (1)Then, we can prove the following result on \(\eta\)-honest promise-form signaling schemes:

**Lemma 2**.: _Let \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) be a promise-form signaling scheme. If \(\sigma\) is \(\eta\)-honest, then, for every step \(h\in\mathcal{H}\) and state \(s\in\mathcal{S}\), it holds that \(\mathcal{V}_{h}^{\mathcal{R},\sigma}(s,\iota)\geq\iota-\eta(H-h+1)\) for all \(\iota\in I_{h}(s)\)._

Since by Lemma 1 the function \(\mathcal{V}_{h}^{\mathcal{R},\sigma}\) encodes the receiver's value function when \(\sigma\) is implemented by the sender, Lemma 2 intuitively establishes that, at each step \(h\in\mathcal{H}\) and state \(s\in\mathcal{S}\), the signaling scheme actually "keeps the promise" of giving at least \(\iota\in I_{h}(s)\) future rewards to the receiver, up to an error depending on \(\eta\). Checking whether a promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) is \(\eta\)-honest or not can be done in time polynomial in the instance size and \(|\mathcal{I}|\).

Now, we are ready to prove the following crucial lemma:

**Lemma 3**.: _Let \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) be an \(\eta\)-honest promise-form signaling scheme such that, for every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\), and \(a,a^{\prime}\in\mathcal{A}\), the following constraint is satisfied:_

\[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota, \theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{ h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^{\prime})\right)\geq\] \[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota, \theta)\left(r_{h}^{\mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a^{\prime},\theta)\widehat{V}_{h+1}^{\mathsf{R }}(s^{\prime})\right).\] (2)

_Then, we can conclude that \(\sigma\) is \((\eta H)\)-persuasive._

Intuitively, Lemma 3 states that for an \(\eta\)-honest promise-form signaling scheme, the constraint in Equation (2) is equivalent to the one in Definition 1. The crucial advantage of Equation (2) is that it allows to express persuasiveness conditions as "local" constraints which do _not_ require recursion.

### Promise-form signaling schemes are sufficient

Finally, by exploiting Lemma 3, we can prove the main result of this section: promise-form signaling schemes represent a sufficient subclass of history-dependent ones. Formally:

**Theorem 3**.: _There is always a persuasive promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) with sender's expected reward equal to \(\mathsf{OPT}\). More formally, it holds that \(V^{\mathcal{S},\phi^{\prime}}=\mathsf{OPT}\) for the history-dependent signaling scheme \(\phi^{\sigma}\) induced by \(\sigma\)._

## 5 Approximation scheme

Theorem 3 shows that, in order to find an optimal signaling scheme, one can focus on promise-form signaling schemes that have the nice property of being polynomially representable in the instance size and the cardinality \(|\mathcal{I}|\) of the set of promises. In this section, we show how to compute an \(\epsilon\)-persuasive promise-form signaling scheme with sender's expected rewards at least \(\mathsf{OPT}\) in polynomial time.

We design an algorithm working with sets \(I_{h}(s)\) defined on a suitable grid, whose size can be properly controlled by a discretization step \(\delta\). The algorithm solves a recursively-defined optimization problem for each \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), and \(\iota\in I_{h}(s)\), by starting from step \(H\) and proceeding in bottom up fashion.

For every step \(h\in\mathcal{H}\) and state \(s\in\mathcal{S}\), the set \(I_{h}(s)\) of promises is defined as a suitable subset of a grid \(\mathcal{D}_{\delta}\coloneqq\{k\,\delta\mid k\in\mathbb{N}\wedge k\leq\lfloor \nicefrac{{H}}{{\delta}}\rfloor\},\) where \(\delta>0\) is a discretization step to be set depending on the desired relaxation \(\epsilon\) of the persuasiveness constraints. This also allows us to control the representation size of promise-form signaling schemes, as well as the running time of the algorithm. In particular, it holds \(|\mathcal{D}_{\delta}|=O(1/\delta)\) and, thus, \(|\mathcal{I}|=O(1/\delta)\) since \(\mathcal{I}\subseteq\mathcal{D}_{\delta}\) by definition. In the following, for ease of notation, we let \(\lceil x\rfloor_{\delta}:=\min_{k\in\mathbb{N}:k\delta\geq x}k\delta\) be the smallest multiple of \(\delta\) greater than \(x\), while \(\lfloor x\rfloor_{\delta}:=\max_{k\in\mathbb{N}:k\delta\leq x}k\delta\) is the greatest multiple of \(\delta\) smaller than \(x\).

The algorithm keeps track of recursively-computed values in a set of tables, one for each step. The table at step \(h\in\mathcal{H}\) is encoded by means of a function \(M_{h}^{\delta}:\mathcal{S}\times\mathcal{D}_{\delta}\to\mathbb{R}\cup\{-\infty\}\). Intuitively, for every \(s\in\mathcal{S}\) and \(\iota\in\mathcal{D}_{\delta}\), the entry \(M_{h}^{\delta}(s,\iota)\) is related to the expected rewards achieved by the sender when "promising" the receiver expected rewards "approximately equal" to \(\iota\) in state \(s\) at step \(h\). We also admit the functions \(M_{h}^{\delta}\) to take value \(-\infty\), which semantically corresponds to the case in which it is impossible to guarantee the promise \(\iota\) to the receiver in state \(s\) at step \(h\). The entry \(M_{h}^{\delta}(s,\iota)\) of the table \(M_{h}^{\delta}\) at step \(h\) is computed recursively by solving a problem \(\mathcal{P}_{h,s,\iota}(M_{h+1}^{\delta})\) that we define in the following, where \(M_{h+1}^{\delta}\) is the (previously-computed) table at step \(h+1\).

By letting \(M:\mathcal{S}\times\mathcal{D}_{\delta}\to\mathbb{R}\cup\{-\infty\}\) be a function encoding a generic table over \(\mathcal{S}\times\mathcal{D}_{\delta}\), for every step \(h\in\mathcal{H}\), state \(s\in\mathcal{S}\), and promise \(\iota\in I_{h}(s)\), we define the value \(\Pi_{h,s,\iota}(M)\) of the optimization problem \(\mathcal{P}_{h,s,\iota}(M)\) as follows:

\[\Pi_{h,s,\iota}(M):=\max_{\begin{subarray}{c}\kappa:\Theta\to\Delta(\mathcal{ A})\\ \mathcal{G}:\mathcal{A}\times\mathcal{S}\to\mathcal{D}_{\delta}\end{subarray}}F _{h,s,M}(\kappa,q)\quad\text{s.t.}\quad(\kappa,q)\in\Psi_{\iota}^{h,s},\]

where problem variables are encoded by the functions \(\kappa:\Theta\to\Delta(\mathcal{A})\) and \(q:\mathcal{A}\times\mathcal{S}\to\mathcal{D}_{\delta}\), which represent an action-recommendation strategy and a promise function, respectively.9 The objective function \(F_{h,s,M}(\kappa,q)\) of the optimization problem is defined as:

Footnote 9: The optimization problem over functions \(\kappa\) and \(q\) can be rewritten as an equivalent program with tabular variables, since the functions \(\kappa\) and \(q\) map discrete sets to discrete sets (or a randomization over them).

\[F_{h,s,M}(\kappa,q):=\sum_{\theta\in\Theta}\sum_{a\in\mathcal{A}}\mu_{h}( \theta|s)\kappa(a|\theta)\left(r_{h}^{\mathsf{S}}(s,a,\theta)+\sum_{s^{\prime} \in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)M(s^{\prime},q(a,s^{\prime}))\right),\]

which encodes the sender's expected reward when their values for the next step \(h+1\) are those specified by the table \(M\). We assume that \(0\cdot(-\infty)=-\infty\). Moreover, the set \(\Psi_{\iota}^{h,s}\) is comprised of the functions \(\kappa:\Theta\to\Delta(\mathcal{A})\) and \(q:\mathcal{A}\times\mathcal{S}\to\mathcal{D}_{\delta}\) that satisfy the following constraints:

\[\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \kappa(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)q(a,s^{\prime})\right)\geq\iota\] (4a) \[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\kappa(a|\theta)\left(r_{ h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a, \theta)q(a,s^{\prime})\right)\geq\] \[\qquad\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\kappa(a|\theta) \left(r_{h}^{\mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime}\in\mathcal{S}}p _{h}(s^{\prime}|s,a^{\prime},\theta)\widehat{\mathcal{V}}_{h+1}^{\mathsf{R}}(s ^{\prime})\right)\qquad\forall a,a^{\prime}\in\mathcal{A},\] (4b)

where Equation (4a) and Equation (4b) play the role of the honesty and the persuasiveness constraints, respectively. Notice that relaxing the honesty constraint yields larger feasible sets. Formally, for any \(\iota\geq\iota^{\prime}\geq 0\) we have that the following holds: \(\Psi_{\iota}^{h,s}\subseteq\Psi_{\iota^{\prime}}^{h,s}\).

If \(F_{h,s,M}(\kappa,q)=-\infty\) for all \((\kappa,q)\in\Psi_{\iota}^{h,s}\), we have that \(\Pi_{h,s,\iota}(M)=-\infty\). This intuitively comes from the fact that, if \(\Pi_{h,s,\iota}(M)=-\infty\), then the value \(\iota\) promised to the receiver is not realizable.

The optimization problem \(\mathcal{P}_{h,s,\iota}(M)\) could be easily cast as a mixed-integer quadratic program, which are too general to be solved efficiently. Thus, we need specifically-tailored procedures to find an approximate solution to it. This discussion is deferred to Section 6. In the following, we assume to have access to an oracle \(\mathcal{O}_{h,s,\iota}(M)\) that provides a suitable approximate solution to \(\mathcal{P}_{h,s,\iota}(M)\). In particular, \(\mathcal{O}_{h,s,\iota}(M)\) must satisfy the requirements introduced by the following definition:

**Definition 3** (Approximate Oracle).: _An algorithm \(\mathcal{O}_{h,s,\iota}(M)\) is an approximate oracle for \(\mathcal{P}_{h,s,\iota}(M)\) if it returns a tuple \((\kappa,q,v)\) such that \(\Pi_{h,s,\iota}(M)\leq v\leq F_{h,s,M}(\kappa,q)\) and \((\kappa,q)\in\Psi_{\iota-\delta}^{h,s}\)._

Intuitively, we ask that an approximate oracle finds a solution \((\kappa,q)\) in a slightly larger set \(\Psi_{\iota-\delta}^{h,s}\). The oracle also returns a value \(v\) to be inserted into \(M_{h}^{\delta}(s,\iota)\), where \(v\) is possibly different from the value \(F_{h,s,M}(\kappa,q)\) of the objective function. This is needed for technical reasons in order to recover some concavity properties of the functions defining the tables used by the algorithm, which may be lost due to approximations. A complete discussion on this last aspect can be found in Section 6.

Equipped with an approximate oracle as in Definition 3, we are ready to design our approximation scheme that computes an \(\epsilon\)-persuasive promise-form signaling scheme attaining sender's expected reward at least \(\mathsf{OPT}\) (Algorithm 3). The algorithm iteratively builds each table \(M_{h}^{\delta}\) by filling it with the values \(v\) returned by the approximate oracle. Moreover, it sets action-recommendation strategies \(\varphi_{h}\) and promise functions \(g_{h}\) of the signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) to be equal to the functions \(\kappa\), \(q\) returned by the approximate oracle.

[MISSING_PAGE_FAIL:9]

\(s^{\prime}\in\mathcal{S}\). Notice that, for ease of notation, we drop the dependence of the operator \(\mathbb{E}\) from \(M\), as such a dependence is always clear from the context.

By exploiting the notation introduced above, the relaxed optimization problem \(\mathcal{R}_{h,s,\iota}(M)\) reads as:

\[\Omega_{h,s,\iota}(M):=\max_{\begin{subarray}{c}\kappa:\Theta\to\Delta( \mathcal{A})\\ \tilde{q}:\mathcal{A}\times\mathcal{S}\to\Delta(D_{\delta})\end{subarray}} \widetilde{F}_{h,s,M}(\kappa,\tilde{q})\quad\text{s.t.}\quad(\kappa,\tilde{q} _{\mathbb{E}})\in\Psi_{\iota}^{h,s},\]

where we relax the functions \(\tilde{q}\) to be distributions over \(\mathcal{D}_{\delta}\), rather than deterministic. Then, we can prove the following:

**Lemma 6**.: _The problem \(\mathcal{R}_{h,s,\iota}(M)\) can be solved in time polynomial in \(1/\delta\) and the instance size._

By relying on a solution \((\kappa,\tilde{q})\) to \(\mathcal{R}_{h,s,\iota}(M)\), Algorithm 4 is an approximate oracle for \(\mathcal{P}_{h,s,\iota}(M)\). In particular, when the relaxed problem is feasible, the algorithm returns \(k\) and the function \(q\) obtained by de-randomizing the randomized function \(\tilde{q}\) with its discrete average. Formally:

**Theorem 5**.: _For every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), and \(\iota\in\mathcal{D}_{\delta}\), if Algorithm 4 is used for all \(h^{\prime}>h\) as approximate oracle \(\mathcal{O}_{h^{\prime},s,\iota}\) in Algorithm 3, then it implements an approximate oracle as in Definition 3._

Notice that the value \(v\) returned by Algorithm 4 is the optimal value of the relaxed problem \(\mathcal{R}_{h,s,\iota}(M)\). It is easy to prove (see the proof of Theorem 5) that the tables built by Algorithm 3 through Algorithm 4 are encoded by concave functions. This is the key feature that allows us to go from a randomized solution to its discrete average without loss in sender's expected rewards, and it is the reason why we need to assume that Algorithm 4 is employed at every step \(h\) in Theorem 5.

## 7 Conclusion

In this paper, we provided a crucial advancement in Bayesian persuasion for sequential decision making, mastering the setting in which a farsighted receiver interacts with an MDP, a problem that was previously thought to be intractable (Wu et al., 2022, Gan et al., 2022, 2022).

First, we have shown that the class of Markovian signaling schemes, the standard choice in previous works and most MDP settings, do not constitute the "right" class of policies, since finding an optimal Markovian signaling scheme is \(\mathsf{NP}\)-hard. Instead, we demonstrated that history-dependent signaling schemes allow to both circumvent the negative complexity result affecting Markovian signaling schemes and guarantee higher expected rewards to the sender in general. Specifically, we designed an algorithm to find an optimal \(\epsilon\)-persuasive history-dependent signaling scheme in time polynomial in \(1/\epsilon\) and in the instance size. The crucial component of the algorithm is to restrict the optimization to a convenient subclass of history-dependent signaling schemes, which we call _promise-form_ as they encode the history into a promise of future rewards to the receiver. We showed that promise-form signaling schemes are both efficient to represent and as powerful as the general class of history-dependent signaling schemes. An interesting insight of our analysis reveals that being _honest_, _i.e._, keeping promises of future rewards, is the key to persuade farsighted receivers in MDPs.

As a future work, we will investigate the online learning variant of the problem studied in this paper, as done in (Zu et al., 2021, Wu et al., 2022, Bernasconi et al., 2022), in which the transition function is _not_ known. Moreover, it would be interesting to study the problem faced by a sender that needs to persuade a stream of receivers having adversarially-selected types, as done in (Balcan et al., 2015, Castiglioni et al., 2020, 2021).

## Acknowledgements

This paper is supported by FAIR (Future Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRR-PE-AI scheme (M4C2, Investment 1.3, Line on Artificial Intelligence), and by the EU Horizon project ELIAS (European Lighthouse of AI for Sustainability, No. 101120237).

## References

* Alimonti and Kann (2000) Paola Alimonti and Viggo Kann. Some APX-completeness results for cubic graphs. _Theoretical Computer Science_, 237:123-134, 04 2000.
* Alimonti et al. (2015)Ricardo Alonso and Odilon Camara. Persuading voters. _American Economic Review_, 2016.
* Babichenko and Barman (2017) Yakov Babichenko and Siddharth Barman. Algorithmic aspects of private Bayesian persuasion. In _ITCS_, 2017.
* Balcan et al. (2015) Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Commitment without regrets: Online learning in stackelberg security games. In _EC_, 2015.
* Bernasconi et al. (2022) Martino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Francesco Trovo. Sequential information design: Learning to persuade in the dark. In _NeurIPS_, 2022.
* Bernasconi et al. (2023) Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Francesco Trovo, and Nicola Gatti. Optimal rates and efficient algorithms for online bayesian persuasion. In _ICML_, 2023.
* Bertsekas (1998) Dimitri Bertsekas. _Network optimization: continuous and discrete models_, volume 8. Athena Scientific, 1998.
* Bhaskar et al. (2016) Umang Bhaskar, Yu Cheng, Young Kun Ko, and Chaitanya Swamy. Hardness results for signaling in bayesian zero-sum and network routing games. In _EC_, 2016.
* Miltersen and Sheffet (2012) Peter Bro Miltersen and Or Sheffet. Send mixed signals: earn more, work less. In _EC_, 2012.
* Candogan (2019) Ozan Candogan. Persuasion in networks: Public signals and k-cores. In _EC_, 2019.
* Castiglioni and Gatti (2021) Matteo Castiglioni and Nicola Gatti. Persuading voters in district-based elections. In _AAAI_, 2021.
* Castiglioni et al. (2020a) Matteo Castiglioni, Andrea Celli, and Nicola Gatti. Persuading voters: It's easy to whisper, it's hard to speak loud. In _AAAI_, 2020a.
* Castiglioni et al. (2020b) Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online Bayesian persuasion. In _NeurIPS_, 2020b.
* Castiglioni et al. (2021a) Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Signaling in Bayesian network congestion games: the subtle power of symmetry. In _AAAI_, 2021a.
* Castiglioni et al. (2021b) Matteo Castiglioni, Alberto Marchesi, Andrea Celli, and Nicola Gatti. Multi-receiver online Bayesian persuasion. In _ICML_, pages 1314-1323, 2021b.
* Gan et al. (2022a) Jiarui Gan, Rupak Majumdar, Goran Radanovic, and Adish Singla. Bayesian persuasion in sequential decision-making. In _AAAI_, 2022a.
* Gan et al. (2022b) Jiarui Gan, Rupak Majumdar, Goran Radanovic, and Adish Singla. Sequential decision making with information asymmetry. In _CONCUR_, 2022b.
* Goldstein and Leitner (2018) Itay Goldstein and Yaron Leitner. Stress tests and information disclosure. _Journal of Economic Theory_, 177:34-69, 2018.
* Kamenica and Gentzkow (2011) Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. _American Economic Review_, 101(6):2590-2615, 2011.
* Kolotilin (2015) Anton Kolotilin. Experimental design to persuade. _Games and Economic Behavior_, 90:215-226, 2015.
* Mansour et al. (2016) Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian exploration: Incentivizing exploration in Bayesian games. In _EC_, 2016.
* Puterman (2014) Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Rabinovich et al. (2015) Zinovi Rabinovich, Albert Xin Jiang, Manish Jain, and Haifeng Xu. Information disclosure as a means to security. In _AAMAS_, 2015.
* Wu et al. (2022) Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng Xu. Markov persuasion processes and reinforcement learning. In _EC_, 2022.
* Wu et al. (2018)* Xu et al. (2016) Haifeng Xu, Rupert Freeman, Vincent Conitzer, Shaddin Dughmi, and Milind Tambe. Signaling in Bayesian Stackelberg games. In _AAMAS_, 2016.
* Zu et al. (2021) You Zu, Krishnamurthy Iyer, and Haifeng Xu. Learning to persuade on the fly: Robustness against ignorance. In _EC_, 2021.

History-dependent signaling schemes are necessary

In this section, we provide an illustrative MDP instance in which an history-dependent signaling scheme is required in order to optimally solve a Bayesian persuasion problem with farsighted receiver. This instance also proves Theorem 1. The instance is depicted in Figure 1. It is easy to check that the receiver can achieve expected rewards

\[\widehat{V}_{1}^{\mathsf{R}}(s_{0})=5,\quad\widehat{V}_{2}^{\mathsf{R}}(s_{1})= 10,\quad\widehat{V}_{2}^{\mathsf{R}}(s_{2})=0,\quad\widehat{V}_{3}^{\mathsf{R} }(s_{3})=0,\quad\widehat{V}_{3}^{\mathsf{R}}(s_{4})=0,\]

without sender's recommendations. If the sender commits to a _non-stationary Markovian_ signaling scheme \(\phi^{\star}=\{\phi_{h}\}_{h\in\mathcal{H}}\), where \(\phi_{h}:\mathcal{S}\times\Theta\rightarrow\Delta(\mathcal{A})\), they can achieve an expected reward of \(V^{\mathsf{S},\phi^{\star}}=V_{1}^{\mathsf{S},\phi^{\star}}(s_{0})=25\) while being persuasive. The latter is obtained by recommending actions deterministically as follows:11

Footnote 11: States where action selection is irrelevant are omitted for brevity.

\[\phi_{2}^{\star}(s_{2})=a_{1},\quad\phi_{3}^{\star}(s_{3},\theta_{0})=a_{0}, \quad\phi_{3}^{\star}(s_{3},\theta_{1})=a_{0}.\]

Instead, an _history-dependent_ signaling scheme \(\phi^{\dagger}:=\{\phi_{\tau}\}_{\tau\in\mathcal{T}}\) can provide the sender with an expected reward of \(V^{\mathsf{S},\phi^{\dagger}}=V_{1}^{\mathsf{S},\phi^{\dagger}}(s_{0})=30\) while being persuasive. This can only be obtained by adapting the action recommendation strategy in state \(s_{3}\) according to whether the receiver passed through state \(s_{1}\) or state \(s_{2}\), as follows:

\[\phi_{(s_{0},s_{1},s_{3})}^{\dagger}(\theta_{0})=a_{0},\quad\phi_{(s_{0},s_{1},s_{3})}^{\dagger}(\theta_{1})=a_{0},\quad\phi_{(s_{0},s_{2},s_{3})}^{\dagger}( \theta_{0})=a_{0},\quad\phi_{(s_{0},s_{2},s_{3})}^{\dagger}(\theta_{1})=a_{1},\]

which makes the recommendation \(\phi_{(s_{0},s_{2})}^{\dagger}=a_{0}\) persuasive as well.

Figure 1: Visual representation of a Bayesian persuasion problem defined over an MDP with \(\mathcal{S}=\{s_{0},s_{1},s_{2},s_{3}\},\mathcal{A}=\{a_{0},a_{1}\},\Theta=\{ \theta_{0},\theta_{1}\},\mathcal{H}=[1,2,3]\), and \(\beta(s_{0})=1\). The probabilities of the state transitions \(p_{h}\) and the private observations \(\mu_{h}\) are reported on the dashed edges. The reward functions \(r_{h}^{\mathsf{S}},r_{h}^{\mathsf{R}}\) for the sender and the receiver, respectively, are reported on the solid edges, when different from \(0\). For the sake of clarity, the visualization omits some irrelevant information, such as private observations in \(s_{0},s_{1},s_{2},s_{4}\) and deterministic transitions.

The latter example proves the existence of an MDP instance where history-dependent signaling schemes are necessary. The reported instance does _not_ require bizarre constructions, and we believe that exploiting history to optimize action recommendations is crucial in practical scenarios as well.

Intuition of Figure 1.In such a scenario, the MDP in Figure 1 can model a ride-sharing platform, where the drivers act as receivers and the platform is the sender. The platform can recommend routes and matching orders with the drivers, which then share with the platform a portion of the profit originating from the trips. In this example, \(s_{3}\) can be thought as a location with high demand that is costly to reach, such as an airport terminal far from the city center. We can further think of the path \(s_{1}\to s_{3}\) as a driver's trip to the airport while serving an order, and \(s_{2}\to s_{3}\) as an empty trip instead. The platform can then adapt its order matching strategy according to whether the driver suffered empty costs or got a profit to come to the airport, such as guaranteeing a quick yet cheap match to the former, while waiting a more lucrative trip for the latter. This adaptive strategy can only be executed through an history-dependent signaling scheme. Instead, a Markovian signaling scheme cannot lure a driver into coming to the airport without serving an order, with diminishing profits for the platform.

## Appendix B Proofs omitted from Section 3

**Theorem 2**.: _There exist two constants \(\alpha<1\) and \(\epsilon>0\) such that computing an \(\epsilon\)-persuasive non-stationary Markovian signaling that provides the sender with at least a fraction \(\alpha\) of the optimal sender's expected reward \(\mathsf{OPT}\) is \(\mathsf{NP}\)-hard._

Proof.: We reduce from a promise version of vertex cover in cubic graphs. In particular, given a cubic graph \((V,E)\), there exists a \(\gamma\in(0,2]\) such that it is \(\mathsf{NP}\)-hard to decide whether there exists a vertex cover of size \(k\) or all the vertex covers have size at least \((1+\gamma)k\)[1].

We design an instance such that the set of states \(\mathcal{S}\) includes \(s_{0}\), \(s_{1}\), \(s_{2}\) and \(s_{3}\). Moreover, it includes a state \(s_{e}\) for each \(e\in E\), and a state \(s_{v}\) for each \(v\in V\). The time horizon is set as \(H=3\). All the transition function and reward are independent from the time step \(h\in[H]\). Hence, we will remove the subscript \(h\) from the notation.

The rewards and transition in the different states are as follows:

* In state \(s_{0}\) there is a single state of nature \(\theta_{0}\) and two actions \(a_{0,1}\) and \(a_{0,2}\). The transition function is such that \(p(s_{3}|s_{0},a_{0,1},\theta_{0})=1\) while \(p(s_{e}|s_{0},a_{0,2},\theta_{0})=\frac{1}{|E|}\) for each \(e\in E\). The rewards of the receiver in state \(s_{0}\) are \(r^{\mathsf{R}}(a_{0,1},s_{0},\theta_{0})=1\) and \(r^{\mathsf{R}}(a_{0,2},s_{0},\theta_{0})=0\). The rewards of the sender in state \(s_{0}\) are \(r^{\mathsf{S}}(a_{0,1},s_{0},\theta_{0})=0\) and \(r^{\mathsf{S}}(a_{0,2},s_{0},\theta_{0})=1\).
* In state \(s_{1}\), there is a single state of nature \(\theta_{1}\) and a single actions \(a_{1}\). The transition function is \(p(s_{2}|s_{1},a_{1},\theta_{1})=1\), while the rewards of the sender and the receiver in \(s_{1}\) are always \(0\).
* In state \(s_{2}\), there is a single state of nature \(\theta_{2}\) and a single actions \(a_{2}\). The transition function is \(p(s_{v}|s_{2},a_{2},\theta_{2})=\frac{1}{|V|}\) for each \(v\in V\). The sender's and receiver's rewards in \(s_{2}\) are \(0\).
* In state \(s_{3}\), there is a single state of nature \(\theta_{3}\) and a single actions \(a_{3}\). The transition function is such that \(p(s_{3}|s_{3},a_{3},\theta_{3})=1\). The rewards of the sender and the receiver in \(s_{3}\) are always \(0\).
* For each \(e=(v,u)\in E\), in the state \(s_{e}\), there is a single state of nature \(\theta_{e}\), and two actions \(a_{e,v}\) and \(a_{e,u}\). The transition function is such that \(p(s_{v}|s_{e},a_{e,v},\theta_{e})=1\) and \(p(s_{u}|s_{e},a_{e,u},\theta_{e})=1\). The rewards of the sender and the receiver in state \(s_{e}\) are \(0\).
* For each \(v\in V\) in the state \(s_{v}\), there are two states of nature \(\theta_{v,1}\) and \(\theta_{v,2}\). There are three actions \(a_{v,1}\), \(a_{v,2}\), and \(a_{v,3}\). The transition function is such that \(p(s_{3}|s_{v},a,\theta)=1\) for each \(a\) and \(\theta\). The rewards of the receiver in \(s_{v}\) are \(r^{\mathsf{R}}(s_{v},a_{v,1},\theta_{v,1})=1\), \(r^{\mathsf{R}}(s_{v},a_{v,2},\theta_{v,2})=1\), \(r^{\mathsf{R}}(s_{v},a_{v,3},\theta_{v,1})=r^{\mathsf{R}}(s_{v},a_{v,3},\theta _{v,2})=\frac{1}{2}\) and \(0\) otherwise. The rewards of the sender in \(s_{v}\) is \(\frac{1}{2}\) if the receiver plays \(a_{v,3}\), _i.e._, \(r^{\mathsf{S}}(s_{v},a_{v,3},\theta_{v,1})=r^{\mathsf{S}}(s_{v},a_{v,3},\theta _{v,2})=\frac{1}{2}\), and \(0\) otherwise.
* All nature's states are drawn uniformly on the nature's outcome at each node.
* Finally, the initial distribution over the states is \(\beta\) such that \(\beta(s_{0})=\beta(s_{1})=\frac{1}{2}\).

In Figure 2 we have a simple instance of a cubic graph (undirected graph with degree bounded by \(3\)), and in Figure 3 we have the instance the persuasion MDP built from the cubic graph in Figure 2.

We recall that a _non-stationary Markovian_ signaling scheme is defined by a set of functions \(\{\phi_{h}\}_{h\in\mathcal{H}}\), where \(\phi_{h}:\mathcal{S}\times\Theta\to\Delta(\mathcal{A})\). However, we use both states \(s_{1}\) and \(s_{2}\) so that it takes two steps to arrive in states \(s_{v}\) both when starting from \(s_{0}\) and when starting from \(s_{1}\). Hence, each state \(s\) (excluding the sink state \(s_{3}\)) can be reached only with a specific time step \(h\). This allows us to work only with stationary signaling scheme, _i.e._, such that the signals do not depend on the time step. Hence, we can remove the subscript \(h\) from the signaling scheme notation.

We will show that if there exists a vertex cover of size \(k\) then there exists a persuasive signaling scheme such that the sender's expected rewards is at least \(\frac{3}{4}-\frac{k}{4|V|}\), while if all the vertex covers have size at least \((1+\gamma)k\) then all the \(\epsilon\)-persuasive signaling schemes have sender's expected rewards strictly less than \(\alpha\left(\frac{3}{4}-\frac{k}{4|V|}\right)\), where \(\epsilon=\frac{1}{3}\cdot 10^{-4}\gamma^{2}\) and \(\alpha=1-\frac{\gamma}{100}\). This will conclude the

Figure 3: Instance of Bayesian persuasion in MDP constructed from the cubic graph \((V,E)\) of Figure 2. Senderâ€™s and receiverâ€™s rewards are reported only when different from \(0\). Solid lines represents actions (that are reported on the corresponding edges) while dashed lines represents stochastic outcomes (either of the transition model or of the prior). For each \(v\in V\) the gadget \(\mathsf{G}_{v}\) is reported Figure 2(b).

Figure 2: Illustrative instance of a cubic graph \((V,E)\).

proof as we could use an algorithm for our problem with approximation factor less or equal to \(\alpha\) to distinguish between the two cases of the original problem.

Completeness.Suppose there exists a vertex cover \(V^{*}\) of size \(k\).

Consider the signaling scheme \(\phi^{*}\) such that \(\phi^{*}(a_{0,2}|s_{0},\theta_{0})=1\), \(\phi^{*}(a_{1}|s_{1},\theta_{1})=1\), \(\phi^{*}(a_{2}|s_{2},\theta_{2})=1\), and \(\phi^{*}(a_{3}|s_{3},\theta_{3})=1\). Moreover, by the definition of vertex cover, for each \(e\in E\) there exists a vertex \(v_{e}\in V^{*}\) such that \(v_{e}\in e=(v_{e},u)\). We set \(\phi^{*}(a_{v,v_{e}}|s_{e},\theta_{e})=1\). Finally, for each \(v\in V^{*}\), we set \(\phi^{*}(a_{v,1}|s_{v},\theta_{v,1})=1\), \(\phi^{*}(a_{v,2}|s_{v},\theta_{v,2})=1\), and \(\phi^{*}(a_{v,3}|s_{v},\theta_{v,1})=\phi^{*}(a_{v,3}|s_{v},\theta_{v,2})=1\) for all \(v\notin V^{*}\).

First, we show that the signaling scheme is persuasive. Consider the state \(s_{0}\). The receiver following the recommendations will always reach a state \(v\in V^{*}\) and in this state will get utility \(1\). It is easy to see that playing action \(a_{0,1}\) the utility is \(1\). Hence, in state \(s_{0}\) the receiver will not deviate. In states \(s_{1}\), \(s_{2}\), and \(s_{3}\), there is only one action available, hence the signaling scheme is trivially persuasive in these states. In a state \(s_{e}\), \(e\in E\), the receiver has no incentive to deviate. Indeed, following the recommendation they will transition to a state \(s_{v}\), \(v\in V^{*}\), and will get \(1\), while not following the recommendation they can get at most \(1\). In all the states \(s_{v}\), for \(v\in V^{*}\), the receiver gets \(1\), while deviating can get at most \(1\). Finally, in a state \(s_{v}\), \(v\notin V^{*}\), the receiver gets \(\frac{1}{2}\), while deviating can get at most \(\frac{1}{2}\). Hence, the signaling scheme is persuasive.

Moreover, the sender's expected rewards for playing according to \(\phi^{*}\) is \(1\) if the initial state is \(s_{0}\), as in this state the signaling scheme recommends \(a_{0,2}\). On the other hand, if the initial state is \(s_{1}\), then a state \(s_{v}\) with \(v\in V^{*}\) is reached with probability \(k/|V|\) and a state \(s_{v}\) with \(v\notin V^{*}\) is reached with probability \(1-k/|V|\). Then, it is easy to see that the sender's expected rewards in reaching a state \(s_{v}\) with \(v\in V^{*}\) is \(0\) and \(\frac{1}{2}\) otherwise. Thus the total sender's expected rewards of the signaling scheme \(\phi^{*}\) is:

\[\frac{1}{2}+\frac{1}{4}\left(1-\frac{k}{|V|}\right)=\frac{3}{4}-\frac{k}{4|V|}.\]

Soundness.Consider a signaling scheme \(\phi\), and suppose that all the vertex covers have size at least \((1+\gamma)k\). We show that the sender's expected rewards is strictly less than \(\alpha\left(\frac{3}{4}-\frac{k}{4|V|}\right)\). First, notice that if the sender's expected rewards is at least \(\alpha\left(\frac{3}{4}-\frac{k}{4|V|}\right)\) it has to recommend \(a_{0,2}\) with probability at least \(\ell\coloneqq 3/10\), _i.e._, \(\phi(a_{0,2}|s_{0},\theta_{0})\geq\frac{3}{10}\). Indeed, if this is not the case the sender's expected rewards is at most

\[\frac{1}{2}\frac{3}{10}\left(1+\frac{1}{2}\right)+\frac{1}{2}\frac{1}{2}= \frac{19}{50}<\frac{\alpha}{2}\leq\alpha\left(\frac{3}{4}-\frac{k}{4|V|} \right),\]

as it gives at most \(1+\frac{1}{2}\) from \(s_{0}\) when \(a_{0,2}\) is played (which happens with probability less then \(3/10\)) and \(\frac{1}{2}\) from \(s_{1}\). Thus we can only consider the cases in which \(\phi(a_{0,2}|s_{0},\theta_{0})\geq\frac{3}{10}\).

Then, suppose that for \(n\) states \(s_{e}\), \(e\in E\), the receiver's expected rewards following the recommendations from there on is at most \(1-\delta\), where \(\delta=\gamma/100\). Denote with \(\widehat{E}\subseteq E\) be this set, where \(|\widehat{E}|=n\), and with \(\widetilde{E}\coloneqq E\setminus\widehat{E}\). The \(\epsilon\)-persuasiveness constraint in state \(s_{0}\) when the recommended action is \(a_{0,2}\) implies:

\[\phi(a_{0,2}|s_{0},\theta_{0})\bigg{[}\frac{n}{|E|}\left(1-\delta\right)+ \left(1-\frac{n}{|E|}\right)\bigg{]}\geq\phi(a_{0,2}|s_{0},\theta_{0})r^{ \mathsf{S}}(a_{0,1},s_{0},\theta_{0})-\epsilon,\]

as the left hand side is an upper bound on the receiver's expected rewards starting from \(s_{0}\). The equation above implies that,

\[\frac{n}{|E|}\left(1-\delta\right)+\left(1-\frac{n}{|E|}\right)\geq 1-\frac{ \epsilon}{\ell},\]

and by rearranging it:

\[n\leq\frac{|E|\epsilon}{\ell\delta}=\frac{\frac{1}{3}|E|\gamma^{2}10^{-4}}{3 \gamma/1000}=\frac{\gamma|E|}{90}.\]Let \(\widetilde{V}\) be the set of states \(s_{v}\), \(v\in V\), such that the receiver's expected rewards from there on is at least \(1-\delta\). Notice that for each state \(s_{e}\), with \(e\in\widetilde{E}\), there exists a state \(s_{v}\), with \(v\in\widetilde{V}\), such that \(v\in e\), _i.e._, \(v\) covers \(e\). This implies that \(\widetilde{V}\) covers \(\widetilde{E}\).

By contradiction, we show that \(|\widetilde{V}|\geq(1+\gamma)k-n\). Suppose otherwise. Then, the set \(\widetilde{V}\) is such that \(|\widetilde{V}|<(1+\gamma)k-n\) and covers at least \(|\widetilde{E}|\) edges. Hence, there exists a super-set of \(\widetilde{V}\) of size strictly smaller than \((1+\gamma)k\) that covers all the edges, reaching a contradiction. This holds as adding a vertex \(v\) for each \(e=(v,u)\in\widehat{E}\) to \(\widetilde{V}\), we obtain a vertex cover of \(E\).

Then, we show that in all the states \(s_{v}\), \(v\in\widetilde{V}\) the sender's expected rewards from there on is at most \(\delta\). It is easy to see that action \(a_{3}\) is recommended with probability at most \(2/\delta\) in each state \(s_{v}\), \(v\in\widetilde{V}\) and the sender collects \(\frac{1}{2}\). If this is not the case, the receiver's expected rewards is strictly smaller than \(2\delta\frac{1}{2}+(1-2\delta)1=1-\delta\). This implies that the sender's expected rewards from a state \(s_{v}\), \(v\in\widetilde{V}\) is at most \(\delta\). A similar argument shows that the sender's expected rewards from a state \(s_{e}\), \(e\in\widetilde{E}\), is at most \(\delta\).

We conclude the proof by showing an upper bound on the sender's expected rewards. In the following, we will exploit that the graph is cubic and \(|V|\geq k\geq\frac{|E|}{3}\). Moreover, we assume w.l.o.g. that there are no unconnected vertices, which implies that \(|E|\geq\frac{|V|}{2}\) and thus that \(k\geq\frac{|V|}{6}\). Hence, the sender's expected rewards when the initial state is \(s_{0}\) is at most:

\[1+\frac{1}{2}\frac{|\widehat{E}|}{|E|}+\delta\frac{|\widetilde{E}|}{|E|}=1+ \frac{1}{2}\frac{n}{|E|}+\delta\left(1-\frac{n}{|E|}\right)<1+\frac{\gamma}{1 80}+\delta.\]

Moreover, the sender's expected rewards when the initial state is \(s_{1}\) is at most:

\[\delta\frac{|\widetilde{V}|}{|V|}+\frac{1}{2}\left(\frac{|V|-| \widetilde{V}|}{|V|}\right) \leq\delta\frac{(1+\gamma)k-n}{|V|}+\frac{1}{2}\left(1-\frac{(1+ \gamma)k-n}{|V|}\right)\] \[=\frac{1}{2}-\left(\frac{1}{2}-\delta\right)\frac{(1+\gamma)k}{ |V|}+\frac{1}{2}\frac{n}{|V|}-\frac{\delta n}{|V|}\] \[<\frac{1}{2}-\left(\frac{1}{2}-\delta\right)\frac{(1+\gamma)k}{ |V|}+\frac{1}{2}\frac{n}{|V|}\] \[\leq\frac{1}{2}-\left(\frac{1}{2}-\delta\right)\frac{(1+\gamma)k }{|V|}+\frac{1}{2}\frac{\gamma|E|}{90|V|}\] \[\leq\frac{1}{2}-\left(\frac{1}{2}-\delta\right)\frac{(1+\gamma)k }{|V|}+\frac{\gamma}{60},\]

where we used that \(|\widetilde{V}|\geq(1+\gamma)k-n\), \(n\leq\gamma|E|/90\) and that \(|E|/|V|\leq 3\).

Thus, since we start from \(s_{0}\) or \(s_{1}\) with probability \(1/2\), the sender's expected rewards is at most:

\[\frac{1}{2}\bigg{(}1+\frac{\gamma}{180}+\delta+ \frac{1}{2}-\left(\frac{1}{2}-\delta\right)\frac{(1+\gamma)k}{|V|} +\frac{\gamma}{60}\bigg{)}\] \[<\frac{3}{4}-\frac{k}{4|V|}+\frac{\gamma}{30}+\frac{\delta}{2} \left(1+\frac{(1+\gamma)k}{|V|}\right)-\frac{\gamma k}{2|V|}\] \[\leq\frac{3}{4}-\frac{k}{4|V|}+\frac{\gamma}{30}+\frac{\delta}{2} \left(1+\frac{3k}{|V|}\right)-\frac{\gamma k}{2|V|}\] \[\leq\frac{3}{4}-\frac{k}{4|V|}+\frac{\gamma}{30}+\frac{\delta}{2} \left(1+3\right)-\frac{\gamma k}{2|V|}\] \[\leq\frac{3}{4}-\frac{k}{4|V|}+\frac{\gamma}{30}+\frac{\gamma}{5 0}-\frac{\gamma k}{2|V|}\] \[\leq\frac{3}{4}-\frac{k}{4|V|}+\gamma\frac{8}{150}-\frac{\gamma k }{2|V|}\] \[\leq\frac{3}{4}-\frac{k}{4|V|}+\gamma\frac{8}{150}-\frac{\gamma}{1 2}\]\[\leq\frac{3}{4}-\frac{k}{4|V|}-\frac{3}{100}\gamma\] \[\leq\frac{3}{4}-\frac{k}{4|V|}-\frac{3}{100}\gamma\left(\frac{3}{4}- \frac{k}{4|V|}\right)\] \[\leq\left(1-\frac{\gamma}{100}\right)\left(\frac{3}{4}-\frac{k}{4| V|}\right),\]

where we used that \(k\geq|V|/6\) and \(\delta:=\gamma/100\).

This concludes the proof. 

## Appendix C Proofs omitted from Section 4

See 1

Proof.: The statement can be easily proved by induction on the steps \(\mathcal{H}\).

The base case of the induction is \(h=H\). By definition of \(V_{H}^{\mathsf{R},\phi^{\sigma}}(a,\tau)\), for every action \(a\in\mathcal{A}\) and history \(\tau=(s_{1},a_{1},\dots,s_{H-1},a_{H-1},s_{H})\in\mathcal{T}_{H}\), the following holds:

\[V_{H}^{\mathsf{R},\phi^{\sigma}}(a,\tau)=\sum_{\theta\in\Theta}\mu_{H}(\theta| s_{H})\phi_{\tau}^{\sigma}(a|\theta)r_{H}^{\mathsf{R}}(s_{H},a,\theta).\]

Since by construction \(\phi_{\tau}^{\sigma}(\theta)=\varphi_{H}(s_{H},t_{\tau}^{\sigma},\theta)\) for every \(\theta\in\Theta\), we have that:

\[V_{H}^{\mathsf{R},\phi^{\sigma}}(a,\tau) =\sum_{\theta\in\Theta}\mu_{H}(\theta|s_{H})\phi_{\tau}^{\sigma} (a|\theta)r_{H}^{\mathsf{R}}(s_{H},a,\theta)\] \[=\sum_{\theta\in\Theta}\mu_{H}(\theta|s_{H})\varphi_{h}(a|s_{H}, t_{\tau}^{\sigma},\theta)r_{H}^{\mathsf{R}}(s_{H},a,\theta)\] \[=\mathcal{V}_{H}^{\mathsf{R},\sigma}(a,s_{H},t_{\tau}^{\sigma}).\]

Now, take \(h<H\) and assume that the statement holds for \(h+1\). By definition of \(V_{h}^{\mathsf{R},\phi^{\sigma}}(a,\tau)\), for every \(a\in\mathcal{A}\) and \(\tau=(s_{1},a_{1},\dots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\), it holds:

\[V_{h}^{\mathsf{R},\phi^{\sigma}}(a,\tau)=\sum_{\theta\in\Theta}\mu_{h}(\theta |s_{h})\phi_{\tau}^{\sigma}(a|\theta)\left(r_{h}^{\mathsf{R}}(s_{h},a,\theta) +\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)V_{h+1}^{ \mathsf{R},\phi^{\sigma}}(\tau\oplus(a,s^{\prime}))\right).\]

Moreover, for every \(s^{\prime}\in\mathcal{S}\), the relation \(V_{h+1}^{\mathsf{R},\phi^{\sigma}}(\tau\oplus(a,s^{\prime}))=V_{h+1}^{ \mathsf{R},\sigma}(s^{\prime},t_{\tau\oplus(a,s^{\prime})}^{\sigma})\) holds by induction since \(\tau\oplus(a,s^{\prime})\) belongs to \(\mathcal{T}_{h+1}\). Given how Algorithm 2 is designed, it holds:

\[t_{\tau\oplus(a,s^{\prime})}^{\sigma}=g_{h}(s_{h},a,t_{\tau}^{\sigma},s^{ \prime}),\]

which, together with \(\phi_{\tau}^{\sigma}(\theta)=\varphi_{h}(s_{h},t_{\tau}^{\sigma},\theta)\) for every \(\theta\in\Theta\), gives \(V_{h}^{\mathsf{R},\phi^{\sigma}}(a,\tau)=\mathcal{V}_{h}^{\mathsf{R},\sigma}(a,s_{h},t_{\tau}^{\sigma})\).

Similar inductive arguments show that \(V_{h}^{\mathsf{S},\phi^{\sigma}}(\tau)=\mathcal{V}_{h}^{\mathsf{S},\sigma}(s_ {h},t_{\tau}^{\sigma})\) for every step \(h\in\mathcal{H}\) and history \(\tau=(s_{1},a_{1},\dots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\) up to \(h\), concluding the proof. 

See 2

Proof.: We prove the statement by induction.

The base case of the induction is \(h=H\). Since \(\sigma\) is \(\eta\)-honest, for every \(s\in\mathcal{S}\) and \(\iota\in I_{H}(s)\):

\[\mathcal{V}_{H}^{\mathsf{R},\sigma}(s,\iota)=\sum_{a\in\mathcal{A}}\sum_{ \theta\in\Theta}\mu_{H}(\theta|s)\varphi_{H}(a|s,\iota,\theta)r_{H}^{\mathsf{ R}}(s,a,\theta)\geq\iota-\eta.\]Now, take \(h<H\) and assume that the statement holds for \(h+1\). For every \(s\in\mathcal{S}\) and \(\iota\in I_{h}(s)\):

\[\mathcal{V}_{h}^{\mathsf{R},\sigma}(s,\iota)\] \[\quad=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s )\varphi_{h}(a|s,\iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{ \prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\mathcal{V}_{h+1}^{\mathsf{ R},\sigma}(s^{\prime},g_{h}(s,a,\iota,s^{\prime}))\right)\] \[\quad\geq\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}( \theta|s)\varphi_{h}(a|s,\iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+ \sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)(g_{h}(s,a,\iota,s ^{\prime})-\eta(H-h))\right)\] \[\quad\geq\iota-\eta-\eta(H-h)\sum_{a\in\mathcal{A}}\sum_{\theta \in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota,\theta)\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\] \[\quad=\iota-\eta(H-h+1),\]

where the first inequality holds by induction, the second one by \(\eta\)-honesty, while the last equality holds since \(\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota,\theta)\sum_{s^ {\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)=1\). This concludes the proof. 

**Lemma 3**.: _Let \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) be an \(\eta\)-honest promise-form signaling scheme such that, for every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\), and \(a,a^{\prime}\in\mathcal{A}\), the following constraint is satisfied:_

\[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota, \theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{ h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^{\prime})\right)\geq\] \[\quad\quad\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s, \iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a^{\prime},\theta)\widehat{V}_{h+1}^{\mathsf{ R}}(s^{\prime})\right).\] (2)

_Then, we can conclude that \(\sigma\) is \((\eta H)\)-persuasive._

Proof.: We recall that, for \(\epsilon\geq 0\), the promise-form signaling scheme \(\sigma\) is \(\epsilon\)-persuasive if the history-dependent signaling scheme \(\phi^{\sigma}:=\{\phi_{\tau}^{\sigma}\}_{\tau\in\mathcal{T}}\) induced by \(\sigma\) is \(\epsilon\)-persuasive according to Definition 1.

As a consequence, in order to prove the statement, we have to show that, for every step \(h\in\mathcal{H}\), history \(\tau=(s_{1},a_{1},\ldots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\) up to step \(h\), and pair of actions \(a,a^{\prime}\in\mathcal{A}\), it holds:

\[V_{h}^{\mathsf{R},\phi^{\sigma}}(a,\tau)\geq\sum_{\theta\in\Theta}\mu_{h}( \theta|s_{h})\phi_{\tau}^{\sigma}(a|\theta)\left(r_{h}^{\mathsf{R}}(s_{h},a^{ \prime},\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a^{ \prime},\theta)\widehat{V}_{h+1}^{\mathsf{R}}(s^{\prime})-\epsilon\right).\]

By Lemma 1, we have that \(V_{h}^{\mathsf{R},\phi^{\sigma}}(a,\tau)=\mathcal{V}_{h}^{\mathsf{R},\sigma}(a,s_{h},\iota_{\tau}^{\sigma})\). Moreover, the following holds:

\[\mathcal{V}_{h}^{\mathsf{R},\sigma}(a|s_{h},\iota_{\tau}^{\sigma})\] \[\quad=\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h})\varphi_{h}(a|s_ {h},\iota_{\tau}^{\sigma},\theta)\left(r_{h}^{\mathsf{R}}(s_{h},a,\theta)+\sum_ {s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)\mathcal{V}_{h+1}^{ \mathsf{R},\sigma}(s^{\prime},g_{h}(s_{h},a,\iota_{\tau}^{\sigma},s^{\prime}) )\right)\] \[\quad\geq\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h})\varphi_{h}(a| s_{h},\iota_{\tau}^{\sigma},\theta)\left(r_{h}^{\mathsf{R}}(s_{h},a,\theta)+\sum_ {s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)\left(g_{h}(s_{h},a, \iota_{\tau}^{\sigma},s^{\prime})-\eta(H-h)\right)\right)\] \[\quad\geq\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s_ {h},\iota_{\tau}^{\sigma},\theta)\left(r_{h}^{\mathsf{R}}(s_{h},a^{\prime}, \theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a^{\prime},\theta) \widehat{V}_{h+1}^{\mathsf{R}}(s^{\prime})-\eta(H-h)\right)\]

where the first inequality holds by Lemma 2, while the second one holds thanks to Equation 2. Finally, the statement is readily proved by noticing that \(\eta(H-h)\leq\eta H\). 

**Theorem 3**.: _There is always a persuasive promise-form signaling scheme \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) with sender's expected reward equal to \(\mathsf{OPT}\). More formally, it holds that \(V^{\mathsf{S},\phi^{\sigma}}=\mathsf{OPT}\) for the history-dependent signaling scheme \(\phi^{\sigma}\) induced by \(\sigma\)._

Proof.: The proof works by showing that, given any persuasive history-dependent signaling scheme, one can always build an \(\eta\)-honest and persuasive promise-form signaling scheme whose sender's expected reward is at least as good. This, together with Lemma 1 clearly proves the statement.

Given any persuasive history-dependent signaling scheme \(\phi=\{\phi_{\tau}\}_{\tau\in\mathcal{T}}\), we build a promise-form signaling scheme \(\sigma=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) as follows:

* \(I_{h}(s):=\left\{V_{h}^{\mathsf{R},\phi}(\tau)\mid\tau=(s_{1},a_{1},\ldots,s_{h- 1},a_{h-1},s_{h})\in\mathcal{T}_{h}\wedge s_{h}=s\right\}\) for all \(h>1,s\in\mathcal{S}\).
* \(\varphi_{h}(s,\iota,\theta)=\phi_{\tau_{h,s,\iota}^{*}}(\theta)\) for all \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\), and \(\theta\in\Theta\), where: \[\tau_{h,s,\iota}^{\star}\in\operatorname*{argmax}_{\tau=(s_{1},a_{1},\ldots,s _{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}:}\left\{V_{h}^{\mathsf{S},\phi}(\tau) \right\},\] which is guaranteed to exist given how \(I_{h}(s)\) is defined.
* \(g_{h}(s,a,\iota,s^{\prime})=V_{h+1}^{\mathsf{R},\phi}(\tau_{h,s,\iota}^{\star }\oplus(a,s^{\prime}))\) for all \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(a\in\mathcal{A}\), \(\iota\in I_{h}(s)\), and \(s^{\prime}\in\mathcal{S}\).

As a first step, we prove that, if \(\phi\) is persuasive, then the promise-form signaling scheme \(\sigma\) that we have just built is persuasive as well. This can be easily proved by exploiting Lemma 3.

First, we prove that \(\sigma\) is an \(\eta\)-honest for \(\eta=0\). Formally, for every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), and \(\iota\in I_{h}(s)\):

\[\iota =V_{h}^{\mathsf{R},\phi}(\tau_{h,s,\iota}^{\star})\] (6) \[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \phi_{\tau_{h,s,\iota}^{\star}}(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta) +\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)V_{h+1}^{\mathsf{ R},\phi}(\tau_{h,s,\iota}^{\star}\oplus(a,s^{\prime}))\right)\] (7) \[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \phi_{\tau_{h,s,\iota}^{\star}}(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta) +\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^ {\prime})\right)\] (8) \[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \varphi_{h}(a|s,\iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{ \prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^{\prime}) \right),\] (9)

where Equation (6) holds by definition of \(\tau_{h,s,\iota}^{\star}\), Equation (7) holds by definition of \(V_{h}^{\mathsf{R},\phi}(\tau_{h,s,\iota}^{\star})\), Equation (8) holds by definition of \(g_{h}(s,a,\iota,s^{\prime})\), while Equation (9) holds by definition of \(\varphi_{h}(a|s,\iota,\theta)\). This proves that \(\sigma\) is an \(\eta\)-honest promise-form signaling scheme, for \(\eta=0\).

In order to apply Lemma 3, we also need to to prove that \(\sigma\) satisfies the conditions in Equation (2). By applying definitions, it is easy to check that, for every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\), and \(a\in\mathcal{A}\):

\[V_{h}^{\mathsf{R},\phi}(a,\tau_{h,s,\iota}^{\star})=\sum_{\theta \in\Theta}\mu_{h}(\theta|s)\phi_{\tau_{h,s,\iota}^{\star}}(a|\theta)\left(r_{ h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a, \theta)V_{h+1}^{\mathsf{R},\phi}(\tau_{h,s,\iota}^{\star}\oplus(a,s^{\prime}))\right)\] \[\quad=\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\phi_{\tau_{h,s, \iota}^{\star}}(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime} \in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^{\prime})\right)\] \[\quad=\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s, \iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}} p_{h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^{\prime})\right).\] (10)

Moreover, since \(\phi\) is persuasive we have that, for every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\), and \(a,a^{\prime}\in\mathcal{A}\):

\[V_{h}^{\mathsf{R},\phi}(a,\tau_{h,s,\iota}^{\star}) =\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\phi_{\tau_{h,s,\iota}^{ \star}}(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)V_{h+1}^{\mathsf{R},\phi}(\tau_{h,s, \iota}^{\star}\oplus(a,s^{\prime}))\right)\] \[\geq\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\phi_{\tau_{h,s,\iota}^{ \star}}(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime} \in\mathcal{S}}p_{h}(s^{\prime}|s,a^{\prime},\theta)\widehat{V}_{h+1}^{\mathsf{ R},\phi}(s^{\prime})\right)\] \[=\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota, \theta)\left(r_{h}^{\mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a^{\prime},\theta)\widehat{V}_{h+1}^{\mathsf{ R},\phi}(s^{\prime})\right),\] (11)where the last equality holds by definition of \(\varphi_{h}(a|s,\iota,\theta)\). Then, by combining Equation (10) and Equation (11), we get that, for every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\), and \(a,a^{\prime}\in\mathcal{A}\):

\[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota,\theta)\left(r_{h }^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)g_{h}(s,a,\iota,s^{\prime})\right)\geq\]

\[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}(a|s,\iota,\theta)\left(r_{h }^{\mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{ \prime}|s,a^{\prime},\theta)\widehat{V}_{h+1}^{\mathsf{R},\phi}(s^{\prime}) \right),\]

which means that \(\sigma\) satisfies Equation (2). Thus, by Lemma 3 we can conclude that \(\sigma\) is persuasive.

In order to conclude the proof, it remains to show that \(\sigma\) achieves a sender's expected reward at least as good as that obtained by \(\phi\). Formally, we prove that \(V_{h}^{\mathsf{S},\phi^{\sigma}}(\tau)\geq V_{h}^{\mathsf{S},\phi}(\tau)\) for every step \(h\in\mathcal{H}\) and history \(\tau\in\mathcal{T}_{h}\) up to \(h\). We will prove such a result by induction.

The base case of the induction is \(h=H\). For every \(\tau=(s_{1},a_{1},\dots,s_{H-1},a_{H-1},s_{H})\in\mathcal{T}_{H}\):

\[V_{H}^{\mathsf{S},\phi}(\tau) \leq V_{H}^{\mathsf{S},\phi}(\tau_{H,s_{H},\iota_{\tau}^{\sigma} }^{\star})\] \[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{H}(\theta|s_{H} )\phi_{\tau_{H,s_{H},\iota_{\tau}^{\sigma}}^{\star}}(a|\theta)r_{H}^{\mathsf{S }}(s_{H},a,\theta)\] \[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{H}(\theta|s_{H} )\varphi_{H}(a|s_{H},\iota_{\tau}^{\sigma},\theta)r_{H}^{\mathsf{S}}(s_{H},a,\theta)\] \[=\mathcal{V}_{H}^{\mathsf{S},\sigma}(s_{H},\iota_{\tau}^{\sigma})\] \[=V_{H}^{\mathsf{S},\phi^{\sigma}}(\tau),\]

where the last equality holds by Lemma 1.

Now, let us take \(h<H\) and assume that the statement that we want to prove holds for \(h+1\). Then, for every \(\tau=(s_{1},a_{1},\dots,s_{h-1},a_{h-1},s_{h})\in\mathcal{T}_{h}\), it holds:

\[V_{h}^{\mathsf{S},\phi}(\tau)\leq V_{h}^{\mathsf{S},\phi}(\tau_{h,s_{h}, \iota_{\tau}^{\sigma}}^{\star})\] (12)

\[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h})\phi_{\tau_{ h,s_{h},\iota_{\tau}^{\sigma}}^{\star}}(a|\theta)\left(r_{h}^{\mathsf{S}}(s,a, \theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)V_{h+1}^{ \mathsf{S},\phi}(\tau_{h,s_{h},\iota_{\tau}^{\sigma}}^{\star}\oplus(a,s^{ \prime}))\right)\]

\[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h})\varphi_{h}(a |s_{h},\iota_{\tau}^{\sigma},\theta)\left(r_{h}^{\mathsf{S}}(s_{h},a,\theta)+ \sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)V_{h+1}^{ \mathsf{S},\phi}(\tau_{h,s_{h},\iota_{\tau}^{\sigma}}^{\star}\oplus(a,s^{ \prime}))\right)\] (13)

\[\leq\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h})\varphi_{h }(a|s_{h},\iota_{\tau}^{\sigma},\theta)\left(r_{h}^{\mathsf{S}}(s_{h},a,\theta) +\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)V_{h+1}^{ \mathsf{S},\phi^{\sigma}}(\tau_{h+1,s^{\prime},g_{h}(s_{h},a,\iota_{\tau}^{ \sigma},s^{\prime})}^{\star})\right)\] (14)

\[\leq\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s_{h})\varphi_{h }(a|s_{h},\iota_{\tau}^{\sigma},\theta)\left(r_{h}^{\mathsf{S}}(s_{h},a,\theta) +\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s_{h},a,\theta)V_{h+1}^{ \mathsf{S},\phi^{\sigma}}(\tau_{h+1,s^{\prime},g_{h}(s_{h},a,\iota_{\tau}^{ \sigma},s^{\prime})}^{\star})\right)\] (15)

\[=\mathcal{V}_{h}^{\mathsf{S},\sigma}(s_{h},\iota_{\tau}^{\sigma})\] (16)

\[=V_{h}^{\mathsf{S},\phi^{\sigma}}(\tau).\] (17)

where Equation (12) holds by definition of \(\tau_{h,s_{h},\iota_{\tau}^{\sigma}}^{\star}\) for the promise \(\iota_{\tau}^{\sigma}\in I_{h}(s_{h})\), Equation (13) holds by definition of \(\varphi_{h}(s_{h},\iota_{\tau}^{\sigma},\theta)\), Equation (14) holds by definition of \(\tau_{h+1,s^{\prime},g_{h}(s_{h},a,\iota_{\tau}^{\sigma},s^{\prime})}^{\star}\) together with the fact that \(g_{h}(s_{h},a,\iota_{\tau}^{\sigma},s^{\prime})=V_{h+1}^{\mathsf{R},\phi}(\tau_{ h,s_{h},\iota_{\tau}^{\sigma}}^{\star}\oplus(a,s^{\prime}))\), Equation (15) holds by induction, while Lemma 1 proves Equation (16) and Equation (17).

The proof is completed by applying the definition of \(\mathsf{OPT}\). 

## Appendix D Proofs omitted from Section 5

**Lemma 4**.: _Let \(\sigma:=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) be returned by Algorithm 3 instantiated with any oracle \(\mathcal{O}_{h,s,\iota}\) as in Definition 3. For every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), \(\iota\in I_{h}(s)\), it holds that \(\mathcal{V}_{h}^{\mathsf{S},\sigma}(s,\iota)\geq M_{h}^{\delta}(s,\iota)\)._

Proof.: Let \(\sigma=\{(I_{h},\varphi_{h},g_{h})\}_{h\in\mathcal{H}}\) be the promise-form signaling scheme returned by Algorithm 3 instantiated with an approximate oracle \(\mathcal{O}_{h,s,\iota}\) as in Definition 3, and let \((\kappa,q,v)\leftarrow\mathcal{O}_{h,s,\iota-\delta}(M_{h+1}^{\delta})\). We prove the statement by induction. The base case for \(h=H\) of the induction holds trivially. Then, for every \(h<H\), \(s\in\mathcal{S}\), and \(\iota\in I_{h}(s)\), assuming the statement holds for \(h+1\) we have that:

\[\mathcal{V}_{h}^{\mathsf{S},\sigma}(s,\iota) =\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \varphi_{h}(a|s,\iota,\theta)\left(r_{h}^{\mathsf{S}}(s,a,\theta)+\sum_{s^{ \prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\mathcal{V}_{h+1}^{\mathsf{S },\sigma}(s^{\prime},g_{h}(s,a,\iota,s^{\prime}))\right)\] \[\geq\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \varphi_{h}(a|s,\iota,\theta)\left(r_{h}^{\mathsf{S}}(s,a,\theta)+\sum_{s^{ \prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)M_{h+1}^{\delta}(s^{\prime},g _{h}(s,a,\iota,s^{\prime}))\right)\] \[=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \kappa(a|\theta)\left(r_{h}^{\mathsf{S}}(s,a,\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)M_{h+1}^{\delta}(s^{\prime},q(a,s^{ \prime}))\right)\] \[=F_{h,s,M_{h+1}^{\delta}}(\kappa,q)\] \[\geq v=M_{h}^{\delta}(s,\iota)\]

where the first inequality holds by the the inductive assumption, while the second inequality holds thanks to Definition 3. 

**Lemma 5**.: _Let \(M_{h}^{\delta}\), \(I_{h}\) (for \(h\in\mathcal{H}\)) be computed by Algorithm 3 instantiated with any oracle \(\mathcal{O}_{h,s,\iota}\) as in Definition 3, and let \(\sigma^{*}=\{(I_{h}^{*},\varphi_{h}^{*},g_{h}^{*})\}_{h\in\mathcal{H}}\) be an optimal promise-form signaling scheme. For every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), and \(\iota\in I_{h}^{*}(s)\), it holds that \(\lceil\iota\rfloor_{\delta}\rvert\in I_{h}(s)\) and \(M_{h}^{\delta}(s,\lceil\iota\rceil_{\delta})\geq\mathcal{V}_{h}^{\mathsf{S}, \sigma^{*}}(s,\iota)\)._

Proof.: We prove by induction that \(M_{h}^{\delta}(s,\lceil\iota\rceil_{\delta})\geq\mathcal{V}_{h}^{\sigma^{*}}(s,\iota)\) for all \(h\in\mathcal{H}\), \(s\in\mathcal{S}\),and \(\iota\in I_{h}^{*}(s)\).

As a base for the induction, we consider \(h=H\). Notice that \(M_{H}(s,\lceil\iota\rceil_{\delta})\) is an upper bound on the solution of \(\mathcal{P}_{H,s,\lfloor\iota\rfloor_{\delta}}(M_{H+1}^{\delta})\) (this holds by Definition 3). Moreover, \(\hat{\kappa}(a|\theta)=\varphi_{H}^{\star}(a|s,\iota,\theta)\) and \(\hat{q}(a,s^{\prime})=0\) is a feasible solution to \(\mathcal{P}_{H,s,\lfloor\iota\rfloor_{\delta}}(M_{H+1}^{\delta})\) and so \((\hat{\kappa},\hat{q})\in\Psi_{\lfloor\iota\rfloor_{\delta}}^{h,s}\subseteq \Psi_{\iota_{-\delta}}^{h,s}\). This readily implies that \(M_{H}^{\delta}(s,\lceil\iota\rceil_{\delta})\geq\mathcal{V}_{H}^{\mathsf{S}, \sigma^{*}}(s,\iota)\), since:

\[M_{H}^{\delta}(s,\lceil\iota\rceil_{\delta}) =v\] \[\geq\Pi_{h,s,\iota-\delta}(M_{H+1}^{\delta})\] \[=\max_{(\kappa,q)\in\Psi_{\iota_{-\delta}}}F_{h,s,M_{H+1}^{ \delta}}(\kappa,q)\] \[\geq F_{h,s,M_{H+1}^{\delta}}(\hat{\kappa},\hat{q})=\mathcal{V}_{ H}^{\sigma^{*}}(s,\iota).\]

As for the inductive step, consider any \(h<H\). First, we show that \(\hat{\kappa}(a|\theta)=\varphi_{h}^{*}(a|s,\iota,\theta)\) and \(\hat{q}(a,s^{\prime})=\lceil g_{h}^{*}(s,a,\iota,s^{\prime})\rceil_{\delta}\) is a feasible solution to \(\mathcal{P}_{h,s,\lfloor\iota\rfloor_{\delta}}(M_{h+1}^{\delta})\). Then, we show that it gets expected rewards larger than \(\mathcal{V}_{h}^{\mathsf{S},\sigma^{*}}(s,\iota)\).

First we prove feasibility of \((\hat{\kappa},\hat{q})\). Consider first the constraint of Equation (4a).

\[\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\hat{ \kappa}(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\hat{q}(a,s^{\prime})\right)\] \[\qquad=\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta| s)\varphi_{h}^{*}(a|s,\iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{ \prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\lceil g_{h}^{*}(s,a,\iota,s ^{\prime})\rceil_{\delta}\right)\]\[\geq\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \varphi_{h}^{\star}(a|s,\iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_ {s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)g_{h}^{\star}(s,a,\iota, s^{\prime})\right)\] \[\geq\iota\geq\lfloor\iota\rfloor_{\delta},\]

which proves that \((\hat{\kappa},\hat{q})\) satisfies the constraint of Equation (4a).

Now, let us turn our attention to the constraint of Equation (4b):

\[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\hat{\kappa}(a|\theta) \left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p _{h}(s^{\prime}|s,a,\theta)\hat{q}(a,s^{\prime})\right)\] \[=\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}^{\star}(a|s, \iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{ S}}p_{h}(s^{\prime}|s,a,\theta)\lceil g_{h}^{\star}(s,a,\iota,s^{\prime})\rceil_{ \delta}\right)\] \[\geq\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}^{\star}(a| s,\iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{ S}}p_{h}(s^{\prime}|s,a,\theta)g_{h}^{\star}(s,a,\iota,s^{\prime})\right)\] \[\geq\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\varphi_{h}^{\star}(a| s,\iota,\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{ S}}p_{h}(s^{\prime}|s,a,\theta)\widehat{V}_{h+1}^{\mathsf{R}}(s^{\prime}) \right),\]

which proves \((\hat{\kappa},\hat{q})\) satisfies the constraint of Equation (4b) and thus \(\mathcal{P}_{h,s,\lfloor\iota\rfloor_{\delta}}(M_{h+1}^{\delta})\) is feasible, _i.e._, \((\hat{\kappa},\hat{q})\in\Psi_{\lfloor\iota\rfloor_{\delta}}\subset\Psi_{ \iota-\delta}\).

Now, we prove that it gets expected rewards larger than \(\sigma^{\star}\).

\[M_{h}(s,\lceil\iota\rceil_{\delta})=v\] \[\geq\Pi_{H,s,\iota-\delta}(M_{h+1}^{\delta})\] \[=\max_{(\kappa,q)\in\Psi_{\iota-\delta}}F_{h,s,M_{h+1}^{\delta}( \kappa,q)}\] \[\geq F_{h,s,M_{h+1}^{\delta}}(\hat{\kappa},\hat{q})\] \[=\sum_{\theta\in\Theta}\sum_{a\in\mathcal{A}}\mu_{h}(\theta|s) \hat{\kappa}(a|\theta)\left(r_{h}^{\mathsf{S}}(s,a,\theta)+\sum_{s^{\prime} \in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)M_{h+1}^{\delta}(s^{\prime},\hat{q }(a,s^{\prime}))\right)\] \[=\sum_{\theta\in\Theta}\sum_{a\in\mathcal{A}}\mu_{h}(\theta|s) \varphi_{h}^{\star}(a|\theta,\iota,\theta)\left(r_{h}^{\mathsf{S}}(s,a, \theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)M_{h+1}^{ \delta}(s^{\prime},\lceil g_{h}^{\star}(s,a,\iota,s^{\prime})\rceil_{\delta})\right)\] \[\geq\sum_{\theta\in\Theta}\sum_{a\in\mathcal{A}}\mu_{h}(\theta|s) \varphi_{h}^{\star}(a|\theta,\iota,\theta)\left(r_{h}^{\mathsf{S}}(s,a,\theta) +\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a,\theta){V}_{h+1}^{s, \sigma^{\star}}(s^{\prime},g_{h}^{\star}(s,a,\iota,s^{\prime}))\right)\] \[={V}^{\mathcal{S},\sigma^{\star}}(s^{\prime},\iota)\]

where we used the induction assumption in the second inequality.

In conclusion, for every \(\iota\in I_{h}^{\star}(s)\), it holds \({V}_{h}^{\sigma^{\star}}(s,\iota)\geq 0\), and, thus, \(M_{h}^{\delta}(s,\lceil\iota\rceil_{\delta})\geq 0\), implying that \(\mathcal{P}_{h,s,\iota-\delta}\) is feasible and thus \(\iota\in I_{h}(s)\) 

**Theorem 4**.: _For any \(\epsilon>0\), given an approximate oracle as in Definition 3, Algorithm 3 instantiated with \(\delta=\nicefrac{{\epsilon}}{{2H}}\) runs in time polynomial in \(1/\epsilon\) and the instance size, while it finds an \(\epsilon\)-persuasive promise-form signaling scheme that guarantees expected reward at least \(\mathsf{OPT}\) to the sender._

Proof.: First of all we need to prove that the \(\sigma\) returned by Algorithm 3 is indeed a promise-form signaling scheme. All the properties are trivially satisfied except for i) \(g_{h}(s,a,\iota,s^{\prime})\in I_{h+1}(s^{\prime})\) for each \(s\in\mathcal{S},a\in\mathcal{A},s^{\prime}\in\mathcal{S}\), and \(\iota\in I_{h}(s)\), and ii) \(0\in I_{1}(s)\) for each \(s\).

We start proving the first property. Let \(s\in\mathcal{S},a\in\mathcal{A},s^{\prime}\in\mathcal{S}\), and \(\iota\in I_{h}(s)\). Since \(\iota\in I_{h}(s)\), by the definition of Algorithm 3 it holds that \(v>-\infty\), where \((\kappa,q,v)\leftarrow\mathcal{O}_{h,s,\iota-\delta}(M_{h+1}^{\delta})\) is the solution returned by the call to the oracle. It is easy to see that this implies that \(M_{h+1}^{\delta}(s^{\prime},q(a,s^{\prime}))>-\infty\). We consider two cases.

[MISSING_PAGE_FAIL:24]

\[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\kappa(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a ^{\prime},\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a^{\prime}, \theta)\widehat{V}_{h+1}^{\mathsf{R}}(s^{\prime})\right) \forall a,a^{\prime}\in\mathcal{A}\] (18c)

Even if the optimization problem is defined over functions, clearly we can represent the functions \(\kappa\) and \(q\) with finite number of variables with linear constraints (to assure that the the function's outputs are distributions). In order to handle the quadratic terms of \(\mathcal{R}_{h,s,\iota}(M)\) we define a "product" variable \(z_{a,s^{\prime},\iota}\) for every \(a\in\mathcal{A},s^{\prime}\in\mathcal{S}\), and \(\iota\in\mathcal{D}_{\delta}(s^{\prime},M)\), which is used in place of

\[\sum_{\theta\in\Theta}\mu_{h}(\theta,s)\kappa(a|\theta)p_{h}(s^{\prime}|s,a, \theta)\tilde{q}(\iota^{\prime}|a,s^{\prime})\]

in Program 18a and has to satisfy the constraint for all \(a\in\mathcal{A}\) and \(s^{\prime}\in\mathcal{S}\):

\[\sum_{\iota^{\prime}\in\mathcal{D}_{\delta}(s^{\prime},M)}z_{a,s^{\prime}, \iota^{\prime}}:=\sum_{\theta\in\Theta}\mu_{h}(\theta,s)\kappa(a|\theta)p_{h }(s^{\prime}|s,a,\theta).\]

For the linear variable we can introduce the non-negative variables \(\xi_{a,\theta}=\kappa(a|\theta)\) for each \(\theta\in\Theta\) and \(a\in\mathcal{A}\), that need to satisfy the simplex constraint for all \(\theta\in\Theta\), _i.e._, \(\sum_{a\in\mathcal{A}}\xi_{a,\theta}=1\). By using these variables we can write the problem as an LP\({}_{h,s,\iota}(M)\):

\[\max_{\begin{subarray}{c}\xi_{a,\theta}\in\mathbb{R}_{+}\\ z_{a,s^{\prime},\iota^{\prime}}\in\mathbb{R}_{+}\end{subarray}} \sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h}( \theta|s)r_{h}^{\mathsf{S}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}\sum_{a \in\mathcal{A}}\sum_{\iota^{\prime}\in\mathcal{D}_{\delta}(s^{\prime},M)}z_{a,s^{\prime},\iota^{\prime}}M(s^{\prime},\iota^{\prime})\] (19a) s.t. \[\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h} (\theta|s)r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}\sum_{a \in\mathcal{A}}\sum_{\iota^{\prime}\in\mathcal{D}_{\delta}(s^{\prime},M)} \iota^{\prime}\cdot z_{a,s^{\prime},\iota^{\prime}}\geq\iota\] (19b) \[\sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h}(\theta|s)r_{h}^{ \mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}\sum_{\iota^{\prime}\in \mathcal{D}_{\delta}(s^{\prime},M)}\iota^{\prime}\cdot z_{a,s^{\prime},\iota^{ \prime}}\geq\] \[\sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h}(\theta|s)\left(r_{h}^{ \mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{ \prime}|s,a^{\prime},\theta)\widehat{V}_{h+1}^{\mathsf{R}}(s^{\prime})\right) \forall a,a^{\prime}\in\mathcal{A}\] (19c) \[\sum_{\iota^{\prime}\in\mathcal{D}_{\delta}(s^{\prime},M)}z_{a,s^ {\prime},\iota^{\prime}}=\sum_{\theta\in\Theta}\mu_{h}(\theta,s)\xi_{a,\theta} p_{h}(s^{\prime}|s,a,\theta) \forall s^{\prime}\in\mathcal{S},\forall a\in\mathcal{A}\] (19d) \[\sum_{a\in\mathcal{A}}\xi_{a,\theta}=1 \forall\theta\in\Theta.\] (19e)

Since LP\({}_{h,s,\iota}(M)\) has polynomial number of variable and constraints one can find a solution in polynomial time.

The next two lemmas show that one can use LP\({}_{h,s,\iota}(M)\). The first shows that any solution to \(\mathcal{R}_{h,s,\iota}(M)\) can be used to find a solution to LP\({}_{h,s,\iota}(M)\).

**Lemma 7**.: _Let \((\kappa,q)\in\Psi_{\iota}^{h,s}\) be a feasible solution to \(\mathcal{R}_{h,s,\iota}(M)\), then there exists a feasible solution to LP\({}_{h,s,\iota}(M)\) with the same value._

Proof.: Let \((\kappa,\tilde{q})\) be a feasible solution to \(\mathcal{R}_{h,s,\iota}(M)\). Then define:

\[z_{a,s^{\prime},\iota^{\prime}}:=\sum_{\theta\in\Theta}\mu_{h}(\theta,s)\kappa( a|\theta)p_{h}(s^{\prime}|s,a,\theta)\tilde{q}(\iota^{\prime}|a,s^{\prime}), \forall a\in\mathcal{A},s^{\prime}\in\mathcal{S},\iota^{\prime}\in\mathcal{D}_{ \delta}(s^{\prime},M),\]

and

\[\xi_{a,\theta}:=\kappa(a|\theta),\forall a\in\mathcal{A},\theta\in\Theta.\]

Then one can show by direct calculation that all the constraints of LP\({}_{h,s,\iota}(M)\) are satisfied and that the objective value of LP\({}_{h,s,\iota}(M)\) is \(\Omega_{h,s,\iota}(M)\) (which is the value of program \(\mathcal{R}_{h,s,\iota}(M)\)). 

The next lemma show a result which is "complementary" to the one above.

**Lemma 8**.: _Given a feasible solution of LP\({}_{h,s,\iota}(M)\) one can find a solution to \(\mathcal{R}_{h,s,\iota}(M)\) with at least the same value._Proof.: Given a feasible solution \(z\) and \(\xi\) to \(\text{LP}_{h,s,\iota}(M)\). Construct a solution \((\kappa,\tilde{q})\) to \(\mathcal{R}_{h,s,\iota}(M)\) as follow for each \(\iota^{\prime}\in\mathcal{D}_{\delta}(s^{\prime},M),a\in\mathcal{A}\) and \(s\in\mathcal{S}\):

\[\tilde{q}(\iota^{\prime}|a,s^{\prime}):=\begin{cases}\frac{z_{a,s^{\prime}, \iota^{\prime}}}{\sum\limits_{\theta\in\Theta}\mu_{h}(\theta,s)\xi_{a,\theta} p_{h}(s^{\prime}|s,a,\theta)}&\text{if }\sum\limits_{\theta\in\Theta}\mu_{h}(\theta,s)\xi_{a,\theta}p_{h}(s^{\prime}| s,a,\theta)>0\\ \mathbb{I}(\iota^{\prime}=0)&\text{otherwise}\end{cases}\]

and for all \(a\in\mathcal{A}\) and \(\theta\in\Theta\):

\[\kappa(a|\theta)=\xi_{a,\theta}.\]

Notice that if for a specific \(h\in\mathcal{H},s\in\mathcal{S},s^{\prime}\in\mathcal{S}\) and \(a\in\mathcal{A}\), the constraint of Equation (19d) impose that if \(\sum_{\theta\in\Theta}\mu_{h}(\theta,s)\xi_{a,\theta}p_{h}(s^{\prime}|s,a, \theta)=0\) then \(z_{a,s^{\prime},\iota^{\prime}}=0\) for all \(\iota^{\prime}\). This means that in any case the following condition holds:

\[z_{a,s^{\prime},\iota^{\prime}}=\sum_{\theta\in\Theta}\mu_{h}(\theta,s)\xi_{a, \theta}p_{h}(s^{\prime}|s,a,\theta)\tilde{q}(\iota^{\prime}|a,s^{\prime}).\] (20)

We now fix any \(h\in\mathcal{H},s\in\mathcal{S}\) and \(\iota\in\mathcal{D}_{\delta}\) show that the constraints of \(\mathcal{R}_{h,s,\iota}(M)\) are satisfied, by using Equation (20). Let us start with the constraint of Equation (18b):

\[\sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\mu_{h}(\theta|s) \kappa(a|\theta)\left(r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in \mathcal{S}}p_{h}(s^{\prime}|s,a,\theta)\left(\sum_{\iota^{\prime}\in \mathcal{D}_{\delta}(s^{\prime},M)}\iota^{\prime}\cdot\tilde{q}(\iota^{\prime }|a,s^{\prime})\right)\right)\] \[= \sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h} (\theta|s)r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{a\in\mathcal{A}}\sum_{\theta\in \Theta}\sum_{\iota\in\mathcal{D}_{\delta}(s^{\prime},M)}\sum_{s^{\prime}\in \mathcal{S}}\iota^{\prime}\xi_{a,\theta}\mu_{h}(\theta|s)p_{h}(s^{\prime}|s,a, \theta)\tilde{q}(\iota^{\prime}|a,s^{\prime})\] \[= \sum_{a\in\mathcal{A}}\sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h} (\theta|s)r_{h}^{\mathsf{R}}(s,a,\theta)+\sum_{a\in\mathcal{A}}\sum_{s\in \mathcal{S}}\sum_{\iota\in\mathcal{D}_{\delta}(s^{\prime},M)}\iota^{\prime} \cdot z_{a,s^{\prime},\iota^{\prime}}\] \[\geq\iota,\]

where the last inequality hold as \((z,\xi)\) is a feasible solution to \(\text{LP}_{h,s,\iota}(M)\). This proves that the constraint of Equation (18b) is satisfied.

For Equation (18c) similarly:

\[\sum_{\theta\in\Theta}\mu_{h}(\theta|s)\kappa(a|\theta)\left(r_{h }^{\mathsf{R}}(s,a,\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{\prime}|s,a, \theta)\left(\sum_{\iota^{\prime}\in\mathcal{D}_{\delta}(s^{\prime},M)}\iota ^{\prime}\cdot\tilde{q}(\iota^{\prime}|a,s^{\prime})\right)\right)\] \[= \sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h}(\theta|s)r_{h}^{ \mathsf{R}}(s,a,\theta)+\sum_{\theta\in\Theta}\sum_{\iota\in\mathcal{D}_{ \delta}(s^{\prime},M)}\sum_{s^{\prime}\in\mathcal{S}}\iota^{\prime}\xi_{a, \theta}\mu_{h}(\theta|s)p_{h}(s^{\prime}|s,a,\theta)\tilde{q}(\iota^{\prime}|a,s^{\prime})\] \[= \sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h}(\theta|s)r_{h}^{ \mathsf{R}}(s,a,\theta)+\sum_{\theta\in\Theta}\sum_{\iota\in\mathcal{D}_{\delta }(s^{\prime},M)}\sum_{s^{\prime}\in\mathcal{S}}\iota^{\prime}\cdot z_{a,s^{ \prime},\iota^{\prime}}\] \[\geq \sum_{\theta\in\Theta}\xi_{a,\theta}\mu_{h}(\theta|s)\left(r_{h} ^{\mathsf{R}}(s,a^{\prime},\theta)+\sum_{s^{\prime}\in\mathcal{S}}p_{h}(s^{ \prime}|s,a^{\prime},\theta)\widehat{V}_{h+1}^{\mathsf{R}}(s^{\prime})\right),\]

where in the last inequality we used that \((z,\xi)\) is a feasible solution to \(\text{LP}_{h,s,\iota}(M)\). Thus the constraint of Equation (18c) is verified. This proves that \((\kappa,\tilde{q})\) is feasible for \(\mathcal{R}_{h,s,\iota}(M)\). Then, by plugging Equation (20) into the objective of \(\mathcal{R}_{h,s,\iota}(M)\) one directly prove that the values of the two problems is the same. 

Now we are ready to prove the main result of this section:

**Lemma 6**.: _The problem \(\mathcal{R}_{h,s,\iota}(M)\) can be solved in time polynomial in \(1/\delta\) and the instance size._

Proof.: Take a solution \((z,\xi)\) to \(\text{LP}_{h,s,\iota}(M)\). This can be done in polynomial time as \(\text{LP}_{h,s,\iota}(M)\) is a linear program with polynomial many variables and constraints. Then apply to the solution \((z,\xi)\) the trasformation used in Lemma 8 to obtain a feasible solution to \(\mathcal{R}_{h,s,\iota}(M)\). This gives an optimal solution to \(\mathcal{R}_{h,s,\iota}(M)\). To prove the last result, assume that there would exists a feasible solution \((\kappa^{\prime},\tilde{q}^{\prime})\) to \(\mathcal{R}_{h,s,\iota}(M)\) such that \(\tilde{F}_{h,s,M}(\kappa^{\prime},\tilde{q}^{\prime})>\tilde{F}_{h,s,M}(\kappa, \tilde{q})\), and apply the trasformation defined in Lemma 7. This would find a solution \((z^{\prime},\xi^{\prime})\) strictly better then \((z,\xi)\) which contradicts the optimality of \((z,\xi)\) for \(\text{LP}_{h,s,\iota}(M)\)

**Theorem 5**.: _For every \(h\in\mathcal{H}\), \(s\in\mathcal{S}\), and \(\iota\in\mathcal{D}_{\delta}\), if Algorithm 4 is used for all \(h^{\prime}>h\) as approximate oracle \(\mathcal{O}_{h^{\prime},s,\iota}\) in Algorithm 3, then it implements an approximate oracle as in Definition 3._

Proof.: Fix any \(\bar{h}\in\mathcal{H},\bar{s}\in\mathcal{S}\) and \(\bar{\iota}\in\mathcal{D}_{\delta}\). We show that for \(h=\bar{h}+1,s\in\mathcal{S}\) and for any distribution \(\gamma\in\Delta(\mathcal{D}_{\delta})\), it holds that:

\[M_{h}^{\delta}\left(s,\lfloor\sum_{\iota\in\mathcal{D}_{\delta}(s,M_{h}^{ \delta})}\iota\gamma(\iota)\rfloor_{\delta}\right)\geq\sum_{\iota\in\mathcal{ D}_{\delta}(s,M_{h}^{\delta})}\gamma(\iota)M_{h}^{\delta}(s,\iota),\] (21)

where the table \(M_{h}^{\delta}\) is the one built by Algorithm 3 with oracle of Algorithm 4.

Define \(\iota_{\mathbb{E}}:=\sum_{\iota\in\mathcal{D}_{\delta}(s,M_{h}^{\delta})}\iota \cdot\gamma(\iota)\). Then for every \(\iota,\iota^{\prime}\in\mathcal{D}_{\delta}\) and \(\iota^{\prime}\leq\iota\) we have that

\[\Omega_{h,s,\iota^{\prime}}(M_{h}^{\delta})\geq\Omega_{h,s,\iota}(M_{h}^{ \delta}),\]

as \(\iota\) it only appears in the RHS of the constraint of the problem \(\mathcal{R}_{h,s,\iota}(M_{h}^{\delta})\).

By construction of Algorithm 3 we have that:

\[M_{h}^{\delta}(s,\iota)=\Omega_{h,s,\iota}(M_{h+1}^{\delta}),\]

and that by Lemma 7 and Lemma 8, \(\Omega_{h,s,\iota}(M_{h+1}^{\delta})\) is equal to the value of \(\text{LP}_{h,s,\iota}(M)\) described by Equations (19), for each \(\iota\). This means that the function \(\iota\mapsto\Omega_{h,s,\iota}(M_{h+1}^{\delta})\) is concave [1, Theorem 5.1].

Thus for every distribution \(\gamma\in\Delta(\mathcal{D}_{\delta})\) we have:

\[\Omega_{h,s,(\sum_{\iota\in\mathcal{D}_{\delta}}\iota\cdot\gamma(\iota))}(M_{ h+1}^{\delta})\geq\sum_{\iota\in\mathcal{D}_{\delta}}\gamma(\iota)\cdot \Omega_{h,s,\iota}(M_{h+1}^{\delta}).\]

Observe that we cannot apply directly the above concavity property as, for every \(s\in\mathcal{S}\), in Equation (21) we are only selecting the components of \(\gamma\) such that \(M_{h}^{\delta}>-\infty\).

To solve this problem we can, for any distribution \(\gamma\in\Delta(\mathcal{D}_{\delta})\), table \(M\) and \(s\in\mathcal{S}\). define the new distribution \(\tilde{\gamma}\) on \(\mathcal{D}_{\delta}(s,M)\) that puts all the mass on the \(-\infty\) components of \(M\) on \(0\). Formally \(\tilde{\gamma}(\iota)=\gamma(\iota)\) for all \(\iota\in\mathcal{D}_{\delta(s,M)}\), and \(\tilde{\gamma}(0)=\gamma(0)+\sum_{\iota\in\mathcal{D}_{\delta}\setminus \mathcal{D}_{\delta}(s,M)}\gamma(\iota)\). Note that with this definition we have \(\iota_{\mathbb{E}}=\sum_{\iota\in\mathcal{D}_{\delta}(s,M)}\iota\cdot\tilde{ \gamma}(\iota)\) and \(\sum_{\iota\in\mathcal{D}_{\delta}(s,M)}\gamma(\iota)\Omega_{h,s,\iota}(M) \leq\sum_{\iota\in\mathcal{D}_{\delta}(s,M)}\tilde{\gamma}(\iota)\Omega_{h,s, \iota}(M)\). Combining these inequalities we can conclude that:

\[M_{h}^{\delta}(s,\lfloor\sum_{\iota\in\mathcal{D}_{\delta}(s,M_{ h}^{\delta})}\iota\gamma(\iota)\rfloor_{\delta}) =\Omega_{h,s,\lfloor\iota_{\mathbb{E}}\rfloor_{\delta}}(M_{h+1}^ {\delta})\] \[\geq\Omega_{h,s,\iota_{\mathbb{E}}}(M_{h+1}^{\delta})\] \[\geq\sum_{\iota\in\mathcal{D}_{\delta}(s,M_{h}^{\delta})}\tilde{ \gamma}(\iota)\cdot\Omega_{h,s,\iota}(M_{h+1}^{\delta})\] \[=\sum_{\iota\in\mathcal{D}_{\delta}(s,M_{h}^{\delta})}\gamma(\iota) \cdot M_{h}^{\delta}(s,\iota),\]

where the last inequality follows since it is easy to see that \(\Omega_{h,s,0}(M_{h+1}^{\delta})\geq 0\) and the last equality follows from Lemma 7. This proves Equation (21).

Now assume that \((\kappa,\tilde{q})\) is a the optimal solution to \(\mathcal{R}_{\bar{h},\bar{s},\bar{\iota}}(M_{h+1}^{\delta})\). Clearly if \((\kappa,\tilde{q}_{\mathbb{E}})\in\Psi_{\bar{\iota}}^{\bar{h},\bar{s}}\) then \((\kappa,q)\in\Psi_{\bar{\iota}-\delta}^{\bar{h},\bar{s}}\), where \(q=\lfloor\tilde{q}_{\mathbb{E}}\rfloor_{\delta}\).

Using the inequality of Equation (21) we can readily perform the following inequalities:

\[F_{\bar{h},\bar{s},M_{\bar{h}+1}^{\delta}}(\kappa,q)\] \[=\sum_{\theta\in\Theta}\sum_{a\in\mathcal{A}}\mu_{h}(\theta|\bar{s })\kappa(a|\theta)\left(r_{h}^{\mathsf{S}}(\bar{s},a,\theta)+\sum_{s^{\prime} \in\mathcal{S}}p_{h}(s^{\prime}|\bar{s},a,\theta)M_{\bar{h}+1}^{\delta}(s^{ \prime},q(a,s^{\prime}))\right)\]

[MISSING_PAGE_FAIL:28]