# Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models

 Dat Do*

Department of Statistics

University of Michigan at Ann Arbor

Ann Arbor, MI 48109

dodat@umich.edu

&Huy Nguyen*

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

huynm@utexas.edu

&Khai Nguyen

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

khainb@utexas.edu

&Nhat Ho

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

minhnhat@utexas.edu

 Equal contribution.

###### Abstract

We study the maximum likelihood estimation (MLE) in the multivariate deviated model where the data are generated from the density function \((1-\lambda^{*})h_{0}(x)+\lambda^{*}f(x|\mu^{*},\Sigma^{*})\) in which \(h_{0}\) is a known function, \(\lambda^{*}\in[0,1]\) and \((\mu^{*},\Sigma^{*})\) are unknown parameters to estimate. The main challenges in deriving the convergence rate of the MLE mainly come from two issues: (1) The interaction between the function \(h_{0}\) and the density function \(f\); (2) The deviated proportion \(\lambda^{*}\) can go to the extreme points of \([0,1]\) as the sample size tends to infinity. To address these challenges, we develop the _distinguishability condition_ to capture the linear independent relation between the function \(h_{0}\) and the density function \(f\). We then provide comprehensive convergence rates of the MLE via the vanishing rate of \(\lambda^{*}\) to zero as well as the distinguishability of two functions \(h_{0}\) and \(f\).

## 1 Introduction

The goodness-of-fit test [11] is one of the foundational tools in statistics with several applications in data-driven scientific fields, namely kernel Stein discrepancy [27; 31], point processes [37] and Bayesian statistics [32], etc. Given a sample set of data and a pre-specified distribution with density function \(h_{0}\), the test indicates whether the samples are reasonably distributed according to \(h_{0}\) (_null hypothesis_) or to another family of distributions \(\{p(\cdot|\theta):\theta\in\Theta\}\) (_alternative hypothesis_). It is worth noting that knowledge about the null hypothesis distribution can come from prior knowledge of scientists. A key to understanding the statistical efficiency of testing is via the likelihood ratio and the maximum likelihood estimation (MLE) methods. [6].

While traditional testing problems often assume the null distribution \(h_{0}=p(\cdot|\theta_{0})\) and the alternative one \(p(\cdot|\theta)\) are from a single simple family of distributions such as exponential families, there are also many problems in science require to test \(h_{0}\) against the alternative \(f(\cdot|\theta)\) that can be _deviated_ from \(h_{0}\) by a distribution from a potentially different family. Specifically, in this paper, we consider the family of distributions named _multivariate deviated model_ with density functions defined as follows:

\[p_{G}(x):=(1-\lambda)h_{0}(x)+\lambda f(x|\mu,\Sigma),\] (1)where \(x\in\mathbb{R}^{d}\), \(G:=(\lambda,\mu,\Sigma)\) are the model's parameters with \(\lambda\in[0,1]\) being the _deviated proportion_ (from \(h_{0}\)) and \((\mu,\Sigma)\in\Theta\times\Omega\) are parameters of a vector-matrix family of distributions \(f\), where \(\Theta\subset\mathbb{R}^{d}\) and \(\Omega\subset\mathbb{R}^{d\times d}\) being compact. When \(\lambda=0\), this recovers the null hypothesis distribution \(h_{0}\).

The deviated model can be motivated by many applications in science. For instance, in microarray data analysis, it can be used to detect differentially expressed genes under two or more conditions [1, 2], where \(h_{0}\) is the uniform distribution and \(f(\cdot|\mu,\Sigma)\) is required to estimate. Many other applications can be seen in many contamination problems in astronomy and biology [29]. Besides, the deviated model can also be viewed as a low-rank adaptation model in the domain adaptation problem [23], where \(h_{0}\) is a pre-trained model on large data, and \(f\) is a simpler component to be estimated from the smaller data domain. Our goal in this paper is to study the parameter estimation rate of the deviated model.

**Problem setup.** Suppose that we observe \(n\) i.i.d. samples \(X_{1},\ldots,X_{n}\) from the true multivariate deviated model:

\[p_{G_{*}}(x):=(1-\lambda^{*})h_{0}(x)+\lambda^{*}f(x|\mu^{*},\Sigma^{*}),\] (2)

where \(G_{*}:=(\lambda^{*},\mu^{*},\Sigma^{*})\) are true but unknown parameters with \(\lambda^{*}\neq 0\). Throughout the paper, we allow \(G_{*}\) to change with the sample size \(n\) (see Appendix F.1 for a discussion). To facilitate our presentation, we suppress the dependence of \(G_{*}\) on \(n\), and then estimate \(G_{*}\) from the data. The main focus of this paper is to establish both a uniform convergence rate and minimax rate for parameter estimation via the MLE approach, which is given by:

\[\widehat{G}_{n}\in\operatorname*{arg\,max}_{G\in\Xi}\sum_{i=1}^{n}\log p_{G}( X_{i}),\] (3)

where \(\widehat{G}_{n}:=(\widehat{\lambda}_{n},\widehat{\mu}_{n},\widehat{\Sigma}_{ n})\) and \(\Xi:=[0,1]\times\Theta\times\Omega\).

**Contribution.** There are two main challenges in studying the convergence rate of the MLE \(\widehat{G}_{n}\): (1) The interaction between the function \(h_{0}\) and the density function \(f\), e.g., \(h_{0}\) belongs to the family of \(f\) and \((\mu^{*},\Sigma^{*})\) approaches \(h_{0}\) as the sample size \(n\) goes to infinity; (2) The deviated proportion \(\lambda^{*}\) can go to the extreme points of \([0,1]\) as the sample size goes to infinity and make the estimation become more challenging, because when \(\lambda^{*}=0\), all the parameters \((\mu^{*},\Sigma^{*})\) yield the same model. To address these _singularity_ and _identifiability_ issues, we first develop the _distinguishability condition_ to capture the linear independent relation between the function \(h_{0}\) and the density function \(f\). We then study the optimal convergence rate of parameters under both distinguishable and non-distinguishable settings of the multivariate deviated model. Our theoretical results can be summarized as follows:

**1. Distinguishable settings:** We demonstrate that as long as the function \(h_{0}\) and the density function \(f\) are distinguishable, the convergence rate of \(\widehat{\lambda}_{n}\) to \(\lambda^{*}\) is \(\mathcal{O}(n^{-1/2})\) while \((\widehat{\mu}_{n},\widehat{\Sigma}_{n})\) converges to \((\mu^{*},\Sigma^{*})\) at a rate determined by the vanishing rate of \(\lambda^{*}\) as follows:

\[\lambda^{*}\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu^{*},\Sigma^{*})\|= \mathcal{O}(n^{-1/2}).\]

It indicates that if \(\lambda^{*}\) goes to 0, the convergence rate of estimating \((\mu^{*},\Sigma^{*})\) is slower than the parametric rate.

**2. Non-distinguishable settings:** When \(h_{0}\) and \(f\) are not distinguishable, it becomes complicated to capture the convergence rate of the MLE. To shed light on the behaviors of the MLE under the non-distinguishable settings of multivariate deviated model, we specifically study the settings when \(h_{0}\) belongs to the same family as \(f\), namely, \(h_{0}(.)=f(.|\mu_{0},\Sigma_{0})\) for some \((\mu_{0},\Sigma_{0})\). To precisely characterize the rates of the MLE under this setting, we consider the second-order strong identifiability of \(f\), which requires the linear independence up to second-order derivatives of \(f\) with respect to its parameters. The second-order identifiability had also been considered in the literature to investigate the convergence rate of parameter estimation in finite mixtures [9, 28, 22, 21, 20, 19].

**2.1. Strongly identifiable and non-distinguishable settings:** When \(f\) is strongly identifiable in the second order, we demonstrate that \(\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}|\widehat{\lambda}_{n}-\lambda^{*}|= \mathcal{O}(n^{-1/2})\) and

\[\lambda^{*}(\|(\Delta\mu^{*},\Delta\Sigma^{*})\|+\|(\Delta\widehat{\mu}_{n}, \Delta\widehat{\Sigma}_{n}\|)\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu^ {*},\Sigma^{*})\|=\mathcal{O}(n^{-1/2}),\]where \(\Delta\mu:=\mu-\mu_{0}\) and \(\Delta\Sigma:=\Sigma-\Sigma_{0}\). It indicates that the convergence rate of \(\widehat{\lambda}_{n}\) to \(\lambda^{*}\) depends on that of \((\mu^{*},\Sigma^{*})\) to \((\mu_{0},\Sigma_{0})\) while the convergence rate of \((\widehat{\mu}_{n},\widehat{\Sigma}_{n})\) to \((\mu^{*},\Sigma^{*})\) depends on both the rate of \(\lambda^{*}\) to 0 and the rate of \((\mu^{*},\Sigma^{*})\) to \((\mu_{0},\Sigma_{0})\). These results are strictly different from those in the distinguishable settings, which is mainly due to the non-distinguishability between \(h_{0}\) and \(f\).

**2.2. Weakly identifiable and non-distinguishable settings:** When \(f\) is weakly identifiable, i.e., it is not strongly identifiable in the second order, we specifically consider the popular setting when \(f\) is the density of a multivariate Gaussian distribution. The loss of the strong identifiability of the Gaussian distribution is due to the following partial differential equation (PDE) between the location and scale parameters (the heat equation):

\[\frac{\partial^{2}f}{\partial\mu\partial\mu^{\top}}(x|\mu,\Sigma)=2\frac{ \partial f}{\partial\Sigma}(x|\mu,\Sigma).\]

Due to the above PDE, the convergence rate of the MLE under this setting exhibits very different behaviors from those under the strongly identifiable setting. In particular, we prove that \(\big{[}\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2}\big{]}\,|\widehat{ \lambda}_{n}-\lambda^{*}|=\mathcal{O}(n^{-1/2})\) and

\[\lambda^{*}(\|\Delta\mu^{*}\|^{2}+\|\Delta\widehat{\mu}_{n}\|^{2}+\|\Delta \Sigma^{*}\|+\|\Delta\widehat{\Sigma}_{n}\|)(\|\widehat{\mu}_{n}-\mu^{*}\|^{2 }+\|\widehat{\Sigma}_{n}-\Sigma^{*}\|)=\mathcal{O}(n^{-1/2}).\]

Notably, there is a mismatch in the orders of convergence rates of the location vector and covariance matrix. Furthermore, the rate of the deviated mixing proportion also depends on different orders of \(\mu^{*}\) to \(\mu_{0}\) and \(\Sigma^{*}\) to \(\Sigma_{0}\). Such rich behaviors of the MLE are mainly due to the PDE between the location and scale parameters.

**Comparing to moment methods.** We would like to remark that the results for the MLE under the non-distinguishable settings in the paper are (much) tighter than those obtained from moment methods for a general mixture of two components in the literature. In particular, when \(f\) is multivariate Gaussian distribution with fixed covariance matrix, i.e., \(f\) is strongly identifiable in the second order and we do not estimate \(\Sigma^{*}\), an application of the results with moment methods from [36] to the deviated models leads to \(\|\Delta\mu^{*}\|^{3}|\lambda_{n}^{\text{moment}}-\lambda^{*}|=\mathcal{O}(n ^{-1/2})\) and \(\lambda^{*}\|\mu_{n}^{\text{moment}}-\mu^{*}\|^{3}=\mathcal{O}(n^{-1/2})\), which are much slower compared to the results for the MLE in the strongly identifiable and non-distinguishable settings, where \((\lambda_{n}^{\text{moment}},\mu_{n}^{\text{moment}})\) denote moment estimators of \(\lambda^{*}\) and \(\mu^{*}\).

When \(f\) is a multivariate Gaussian density and we estimate both the location vector and covariance matrix, i.e., \(f\) is weakly identifiable, an adaptation of the moment estimators from the seminal work [18] to the multivariate deviated models shows that \(\lambda^{*}\|\|\tilde{\mu}_{n}^{\text{moment}}-\mu^{*}\|^{6}=\mathcal{O}(n^{ -1/2})\), \(\lambda^{*}\|\|\tilde{\Sigma}_{n}^{\text{moment}}-\Sigma^{*}\|^{3}=\mathcal{O }(n^{-1/2})\) and \((\|\Delta\mu^{*}\|^{6}+\|\Delta\Sigma^{*}\|^{3})|\tilde{\lambda}_{n}^{\text{ moment}}-\lambda^{*}|=\mathcal{O}(n^{-1/2})\), where \((\tilde{\lambda}_{n}^{\text{moment}},\tilde{\mu}_{n}^{\text{moment}}, \tilde{\Sigma}_{n}^{\text{moment}})\) are moment estimators of \((\lambda^{*},\mu^{*},\Sigma^{*})\). These results are also much slower than those of the MLE in weakly identifiable settings.

**Other related work.** The hypothesis testing and MLE problem related to the multivariate deviated model had been considered in previous work, including the problem of detecting sparse homogeneous and heteroscedastic mixtures [14, 15, 4, 3, 5, 34], the problem of determining the number of components [8, 26, 10, 24, 25], and the problem of multiple testing [30, 12]. In particular, [4] considers testing problem for the deviated model with \(h_{0}=N(0,1)\) and \(f=N(\mu^{*},1)\) being one-dimensional Gaussian distributions. They show that no test can reliably detect \(\lambda^{*}=0\) against \(\lambda^{*}>0\) if \(\lambda^{*}\mu^{*}=o(n^{-1/2})\), while the Likelihood Ratio test can consistently do it when \(\lambda^{*}\mu^{*}\gtrsim n^{-1/2+\epsilon}\) for any \(\epsilon>0\). However, no guarantee for estimation of \(\lambda^{*}\) and \(\mu^{*}\) is provided. In the same setting where \(f\) is the density of a location Gaussian distribution, the convergence rate of parameter estimation in the deviated model had been studied in the work of [16]. Since the location Gaussian distribution is a special case of the strongly identifiable distribution, our result in the strongly identifiable and non-distinguishable settings is a generalization of the results in [16], but with a different proof technique as their proof technique relies strictly on the properties of the location Gaussian distribution.

**Organization.** The paper is organized as follows. In Section 2, we provide background on the identifiability and density estimation rate of the multivariate deviated model. Then, we establish the lower bounds of the Total Variation distance between two densities in terms of loss functions among parameters under both the distinguishable and non-distinguishable settings in Section 3. Next, we characterize the convergence rates of parameter estimation as well as derive the corresponding minimax lower bounds in Section 4. In Section 5, we carry out a simulation study to empirically verify our theoretical results before concluding the paper in Section 6. Rigorous proofs and additional results are deferred to the supplementary material.

**Notations.** For any \(a,b\in\mathbb{R}\), we denote \(a\lor b:=\max\left\{a,b\right\}\) and \(a\wedge b:=\min\left\{a,b\right\}\). Next, we say that \(h_{0}\) is identical to \(f\) if \(h_{0}(x)=f(x|\mu_{0},\Sigma_{0})\) for some \((\mu_{0},\Sigma_{0})\in\Theta\times\Sigma\). For each parameter \(G\in\Xi\), let \(\mathbb{E}_{p_{G}}\) be the expectation taken with respect to product measure with density \(p_{G}\). Lastly, for any two density functions \(p\) and \(q\) (with respect to the Lebesgue measure \(m\)), the Total Variation distance between them is given by \(V(p,q):=\frac{1}{2}\int|p(x)-q(x)|\hat{dm}(x)\), while we define their squared Hellinger distance as \(h^{2}(p,q):=\frac{1}{2}\int[\sqrt{p(x)}-\sqrt{q(x)}]^{2}dm(x)\).

## 2 Preliminaries

### Identifiability Condition

Our principal goal in this paper is to assess the statistical efficiency of parameter estimation from the MLE method. To do that, we should be able to guarantee the parameter identifiability of the deviated model (2), i.e., if \(p_{G}(x)=p_{G_{*}}(x)\) for almost surely \(x\in\mathcal{X}\) where \(G=(\lambda,\mu,\Sigma)\), then \(G\equiv G_{*}\). That identifiability condition leads to the following notion of distinguishability between the density function \(h_{0}(\cdot)\) and the family of density functions \(\{f(\cdot|\mu,\Sigma):(\mu,\Sigma)\in\Theta\times\Omega\}\).

**Definition 2.1** (Distinguishability).: We say that the family of density functions \(\{f(\cdot|\mu,\Sigma),(\mu,\Sigma)\in\Theta\times\Omega\}\) (or in short, \(f\)) is _distinguishable_ from \(h_{0}\) if the following holds:

* For any two distinct components \((\mu_{1},\Sigma_{1})\) and \((\mu_{2},\Sigma_{2})\), if we have real coefficients \(\eta_{i}\) for \(1\leq i\leq 3\) such that \(\eta_{1}\eta_{2}\leq 0\) and \(\eta_{1}f(x|\mu_{1},\Sigma_{1})+\eta_{2}f(x|\mu_{2},\Sigma_{2})+\eta_{3}h_{0} (x)=0\), for almost surely \(x\in\mathbb{R}^{d}\), then \(\eta_{1}=\eta_{2}=\eta_{3}=0\).

We can verify that as long as \(f\) is distinguishable from \(h_{0}\), the parameter identifiability of our multivariate deviated model follows. In particular, assume that there exists \(G=(\lambda,\mu,\Sigma)\) such that

\[(1-\lambda^{*})h_{0}(x)+\lambda^{*}f(x|\mu^{*},\Sigma^{*})=(1-\lambda)h_{0}(x )+\lambda f(x|\mu,\Sigma),\] (4)

for almost surely \(x\in\mathcal{X}\). The above equation is equivalent to \((\lambda-\lambda^{*})h_{0}(x)+\lambda^{*}f(x|\mu^{*},\Sigma^{*})-\lambda f(x| \mu,\Sigma)=0\). Assume that \(f\) is distinguishable from \(h_{0}\), then equation (4) indicates that if \((\mu,\Sigma)\neq(\mu^{*},\Sigma^{*})\), we have \(\lambda=\lambda^{*}=0\). Since \(\lambda^{*}\neq 0\) from our assumption, we obtain that \((\mu,\Sigma)=(\mu^{*},\Sigma^{*})\). As a result, equation (4) becomes \((\lambda-\lambda^{*})h_{0}(x)+(\lambda^{*}-\lambda)f(x|\mu,\Sigma)=0\). By applying the distinguishability condition again, we get \(\lambda=\lambda^{*}\). Therefore, the multivariate deviated model (2) is identifiable.

In the following example, we will verify the distinguishability condition in Definition 2.1 given some specific choices of function \(h_{0}\) and density \(f\).

**Example 2.2**.: (a) Assume that \(f\) belongs to a location family of density functions, i.e., \(f(x|\mu,\Sigma)=f_{\Sigma}(x-\mu)\) for all \(x\) where \(\Sigma\) is a fixed covariance matrix. If \(h_{0}(x)\neq f(x)\) for almost surely \(x\in\mathcal{X}\), then \(f\) is distinguishable from \(h_{0}\).

(b) When \(h_{0}\) is a finite mixture of multivariate Gaussian densities and \(f\) belongs to a class of multivariate Student's density functions with any fixed odd degree of freedom \(\nu>1\), we get that \(f\) is distinguishable from \(h_{0}\).

(c) When \(f\) is identical to \(h_{0}\), then \(f\) is not distinguishable from \(h_{0}\).

### Convergence Rate of Density Estimation

Our strategy to obtain the convergence rate of the MLE \(\widehat{G}_{n}\) is by first establishing the convergence rate of density \(p_{\widehat{G}_{n}}\) and then studying the geometric inequalities between the parameter space and density space. For the former, the standard method is to use the empirical process theory [17; 33], while for the latter step, we investigate those inequalities under various settings of distinguishability in Section 3. Due to space constraints and the popularity of empirical process theory, we choose to informally present a main result for yielding the parametric convergence rate for density estimation in this section. For full explanation and definition, readers are referred to Appendix B. The convergence rate for density estimation can be characterized by bounding the complexity of the parameter space \(\Xi\) via a function called _bracketing entropy integral_\(\mathcal{J}_{B}(\epsilon,\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon))\) (cf. equation (8)).

**Theorem 2.3**.: _Assume the following assumption holds:__._
2. _Given a universal constant_ \(J>0\)_, there exists_ \(N>0\)_, possibly depending on_ \(\Xi\)_, such that for all_ \(n\geq N\) _and all_ \(\epsilon>(\log(n)/n)^{1/2}\)_, we have_ \(\mathcal{J}_{B}(\epsilon,\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon))\leq J \sqrt{n}\epsilon^{2}\)_._

_Then, there exists a constant \(C>0\) depending only on \(\Xi\) such that for all \(n\geq 1\),_

\[\sup_{G_{*}\in\Xi}\mathbb{E}_{p_{G_{*}}}h(p_{\widehat{G}_{n}},p_{G_{*}})\leq C \sqrt{\log n/n}.\]

Therefore, in order to get the convergence rate for density estimators based on the MLE method, we only need to check Assumption A2, which holds true for several parametric models [33]. For our model, we give an example that it holds for a general class of \(f\) and \(h_{0}\).

**Proposition 2.4**.: _Suppose that both \(\Theta\) and \(\Omega\) are compact, and \(\{f(x|\mu,\Sigma):\mu\in\Theta,\Sigma\in\Omega\}\) is a vector-matrix family of densities being uniformly bounded, Lipschitz, and light tail, i.e. there exists constants \(M,L,B,b_{1},b_{2},b_{3}>0\) such that \(|f(x|\mu,\Sigma)|\leq M,|f(x|\mu,\Sigma)-f(x|\mu^{\prime},\Sigma^{\prime})| \leq L(\|\mu-\mu^{\prime}\|+\|\Sigma-\Sigma^{\prime}\|)\) for all \(x\in\mathbb{R}^{d}\), and_

\[|f(x|\mu,\Sigma)|\leq b_{1}\exp(-b_{2}\,\|x\|^{b_{3}})\quad\forall\ \|x\|>B,\]

_for all \((\mu,\Sigma)\in\Theta\times\Omega\). Additionally, if the density \(h_{0}\) is bounded, then the corresponding multivariate deviated model defined in equation (1) satisfies assumption A2._

**Example 2.5**.: We can check that the location-scale Gaussian density \(f(x|\mu,\Sigma)\) with \(\Sigma\in\Omega\) having eigenvalues bounded below by a positive constant satisfies the condition of Proposition 2.4. This condition for \(h_{0}\) is mild and is satisfied by most distributions such as Gaussian and t-distribution.

## 3 From the Convergence Rate of Densities to Rate of Parameters

The objective of this section is to develop a general theory according to which a small distance between \(p_{G}\) and \(p_{G}\), under the Hellinger distance (or Total Variation distance) would imply that \(G\) and \(G_{*}\) are also close under appropriate distance where \(G=(\lambda,\mu,\Sigma)\) and \(G_{*}=(\lambda^{*},\mu^{*},\Sigma^{*})\). By combining those results with Theorem 2.3, we can obtain the convergence rate for parameter estimation (cf. Section 4). The distinguishability condition between \(h_{0}\) and \(f\) implicitly requires that \(p_{G}=p_{G_{*}}\) would entail \(G=G_{*}\); however, to obtain quantitative bounds for their Total Variation distance, we need stronger notions of both distinguishability and classical parameter identifiability, ones which involve higher order derivatives of the densities \(h_{0}\) and \(f\), taken with respect to mixture model parameters. Throughout the rest of this section, we denote \(G=(\lambda,\mu,\Sigma)\) and \(G_{*}=(\lambda^{*},\mu^{*},\Sigma^{*})\).

### Distinguishable Settings

**Definition 3.1** (**First-order Distinguishability**).: We say that \(f\) is distinguishable from \(h_{0}\) up to the first order if \(f\) is differentiable in \((\mu,\Sigma)\), and the following holds:

1. For any component \((\mu^{\prime},\Sigma^{\prime})\in\Theta\times\Omega\), if we have real coefficients \(\eta,\tau_{\alpha}\) for all \(\alpha=(\alpha_{1},\alpha_{2})\in\mathbb{N}^{d_{1}}\times\mathbb{M}^{d_{2} \times d_{2}}\), \(|\alpha|=|\alpha_{1}|+|\alpha_{2}|\leq 1\) such that \[\eta h_{0}(x)+\sum_{|\alpha|\leq 1}\tau_{\alpha}\frac{\partial^{|\alpha|}f}{ \partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2}}}(x|\mu^{\prime},\Sigma^{ \prime})=0\] for all \(x\in\mathcal{X}\), then \(\eta=\tau_{\alpha}=0\) for all \(|\alpha|\leq 1\).

We can verify that the examples from part (a) and part (b) of Example 2.2 satisfy the first-order distinguishability condition. Next, we introduce a notion of uniform Lipschitz condition in the following definition.

**Definition 3.2** (**Uniform Lipschitz).: We say that \(f\) admits uniform Lipschitz condition up to the first order if the following holds: there are positive constants \(\delta_{1},\delta_{2}\) such that for any \(R_{1},R_{2},R_{3}>0,\gamma_{1}\in\mathbb{R}^{d_{1}},\gamma_{2}\in\mathbb{R}^{d _{2}\times d_{2}},R_{1}\leq\lambda_{\min}^{1/2}(\Sigma_{1})\leq\lambda_{\max}^ {1/2}(\Sigma_{2})\leq R_{2}\), \(\|\mu_{1}\|,\|\mu_{2}\|\leq R_{3}\), \(\mu_{1},\mu_{2}\in\Theta\),\(\Sigma_{1},\Sigma_{2}\in\Omega\), we can find positive constants \(C(R_{1},R_{2})\) and \(C(R_{3})\) such that for all \(x\in\mathcal{X}\),

\[\left|\gamma_{1}^{\top}\left(\frac{\partial f}{\partial\mu}(x| \mu_{1},\Sigma)-\frac{\partial f}{\partial\mu}(x|\mu_{2},\Sigma)\right)\right| \leq C(R_{1},R_{2})\|\mu_{1}-\mu_{2}\|^{\delta_{1}}\|\gamma_{1}\|,\] \[\left|\mathrm{tr}\left(\left(\frac{\partial f}{\partial\Sigma}(x| \mu,\Sigma_{1})-\frac{\partial f}{\partial\Sigma}(x|\mu,\Sigma_{2})\right)^{ \top}\gamma_{2}\right)\right|\leq C(R_{3})\|\Sigma_{1}-\Sigma_{2}\|^{\delta_{2}} \|\gamma_{2}\|.\]Now, we have the following results characterizing the behavior of \(V(p_{G},p_{G_{*}})\) regarding the variation of \(G\) and \(G_{*}\).

**Theorem 3.3**.: _Assume that \(f\) is distinguishable from \(h_{0}\) up to the first order. Furthermore, \(f\) admits uniform Lipschitz condition up to the first order. For any \(G\) and \(G_{*}\), we define_

\[\mathcal{K}(G,G_{*}):=|\lambda-\lambda^{*}|+(\lambda+\lambda^{*}) \|(\mu,\Sigma)-(\mu^{*},\Sigma^{*})\|.\]

_Then, the following holds:_

\[C.\mathcal{K}(G,G_{*})\leq V(p_{G},p_{G_{*}})\leq C_{1}.\mathcal{K}(G,G_{*}),\]

_for all \(G\) and \(G_{*}\), where \(C\) and \(C_{1}\) are two positive constants depending only on \(\Theta\), \(\Omega\), and \(h_{0}\)._

See Appendix C.1 for the proof of Theorem 3.3. Since the MLE approach yields the convergence rate \(n^{-1/2}\) up to some logarithmic factor for \(p_{G_{*}}\) under the first order uniform Lipschitz condition of \(f\), the result of Theorem 3.3 directly yields the convergence rate \(n^{-1/2}\) up to some logarithmic factor for \(G_{*}\) under metric \(\mathcal{K}\). This entails that the estimation of weight \(\lambda_{*}\) converges at rate \(n^{-1/2}\) up to some logarithmic factor while the convergence rate of estimating \((\mu^{*},\Sigma^{*})\) is typically much slower than \(n^{-1/2}\) as it depends on the rate of convergence of \(\lambda^{*}\) to 0 (cf. Theorem 4.1).

### Non-distinguishable Settings

When \(f\) is not distinguishable to \(h_{0}\) up to the first order, the bound in Theorem 3.3 may not hold in general. In this section, we investigate the inverse bounds under the specific settings of non-distinguishable in the first-order models when \(h_{0}\) belongs to the family \(f(.|\mu,\Sigma)\), i.e., \(h_{0}(x)=f(x|\mu_{0},\Sigma_{0})\) for some \((\mu_{0},\Sigma_{0})\in\Theta\times\Sigma\). Our studies are divided into two separate regimes of \(f\): the first setting is when \(f\) is strongly identifiable in the second order (cf. Definition 3.4), while the second setting is when it is not. For the simplicity of the presentation in the paper, we define \((\Delta\mu,\Delta\Sigma)=(\mu-\mu_{0},\Sigma-\Sigma_{0})\) for any element \((\mu,\Sigma)\in\Theta\times\Omega\).

**Definition 3.4** (Strong Identifiability).: We say that \(f\) is strongly identifiable in the second order if \(f\) is twice differentiable in \((\mu,\Sigma)\) and the following holds:

* For any positive integer \(k\), given \(k\) distinct pairs \((\mu_{1},\Sigma_{1}),\ldots,(\mu_{k},\Sigma_{k})\), if we have \(\alpha_{\eta}^{(i)}\) such that \[\sum_{\ell=0}^{2}\sum_{|\eta|=\ell}\sum_{i=1}^{k}\alpha_{\eta}^{(i)}\frac{ \partial^{|\eta|f}}{\partial\mu^{\eta_{1}}\partial\Sigma^{\eta_{2}}}(x|\mu_{i },\Sigma_{i})=0,\] for almost all \(x\in\mathcal{X}\), then \(\alpha_{\eta}^{(i)}=0\) for all \(i\in[k]\) and \(|\eta|\leq 2\).

#### 3.2.1 Strongly Identifiable Settings

Now, we have the following result regarding the lower bound of \(V(p_{G},p_{G_{*}})\) under the strongly identifiable settings of \(f\).

**Theorem 3.5**.: _Assume that \(h_{0}(x)=f(x|\mu_{0},\Sigma_{0})\) for some \((\mu_{0},\Sigma_{0})\in\Theta\times\Sigma\) and \(f\) is strongly identifiable in the second order and admits uniform Lipschitz condition up to the second order. Furthermore, we denote_

\[\mathcal{D}(G,G_{*}) :=\lambda\|(\Delta\mu,\Delta\Sigma)\|^{2}+\lambda^{*}\|(\Delta\mu^ {*},\Delta\Sigma^{*})\|^{2}-\min\left\{\lambda,\lambda^{*}\right\}\left(\|( \Delta\mu,\Delta\Sigma)\|^{2}\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\right)\] \[+\left(\lambda\|(\Delta\mu,\Delta\Sigma)\|+\lambda^{*}\|(\Delta \mu^{*},\Delta\Sigma^{*})\|\right)\|(\mu,\Sigma)-(\mu^{*},\Sigma^{*})\|\]

_for any \(G\) and \(G_{*}\). Then, there exists a positive constant \(C\) depending only on \(\Theta\), \(\Omega\), and \((\mu_{0},\Sigma_{0})\) such that \(V(p_{G},p_{G_{*}})\geq C.\mathcal{D}(G,G_{*})\), for all \(G\) and \(G_{*}\)._

The proof of Theorem 3.5 and the second-order uniform Lipschitz condition are deferred to Appendix C.2. Several remarks regarding Theorem 3.5 are in order:

**(i)** For any \(G\) and \(G_{*}\), by defining

\[\overline{\mathcal{D}}(G,G_{*}) :=|\lambda^{*}-\lambda|\|(\Delta\mu,\Delta\Sigma)\|\|(\Delta\mu^{ *},\Delta\Sigma^{*})\|\] \[+\|(\mu,\Sigma)-(\mu^{*},\Sigma^{*})\|\big{(}\lambda\|(\Delta\mu,\Delta\Sigma)\|+\lambda^{*}\|(\Delta\mu^{*},\Delta\Sigma^{*})\|\big{)}\]we can verify that \(1/2\leq\mathcal{D}(G,G_{*})/\mathcal{D}(G,G_{*})\leq 2\), i.e., \(\mathcal{D}(G,G_{*})\asymp\overline{\mathcal{D}}(G,G_{*})\). The reason that we prefer to use the formation of \(\mathcal{D}(G,G_{*})\) over that of \(\overline{\mathcal{D}}(G,G_{*})\) is not only due to the convenience of the proof argument of Theorem 3.5 later in Appendix C but also due to its partial connection with Wasserstein metric that we are going to discuss in the next remark.

**(ii)** When \(f\) is a multivariate location family and is identical to \(h_{0}\), i.e., \(\mu_{0}=\mathbf{0}\), it was demonstrated recently in [16] that

\[V(p_{G},p_{G_{*}})\gtrsim|\lambda-\lambda^{*}|\|\mu|\|\|\mu^{*}\|+(\lambda^{*} \|\mu^{*}\|+\lambda\|\mu|)\|\mu-\mu^{*}\|,\] (5)

which is also the key result for establishing the convergence rates of parameter estimation in their work. However, their proof technique only works for the location family and it is unclear what is the sufficient condition for the family of density functions beyond the location family such that the inequality (5) will hold. As the location family is strongly identifiable in the second order, we can verify that the lower bound in Theorem 3.5 and inequality (5) are in fact similar. Therefore, the result in Theorem 3.5 gives a generalization of inequality (5) in [16] under the strongly identifiable in the second order setting of \(f\).

**(iii)** As indicated in [16], we can further lower bound the right-hand side of inequality (5) in terms of the second order Wasserstein metric \(W_{2}\)[35] between \(G\) and \(G_{*}\) when we present \(G\) and \(G_{*}\) as two discrete probability measures with two components. In particular, with an abuse of the notations we denote that \(G=(1-\lambda)\delta_{(\mu_{0},\Sigma_{0})}+\lambda\delta_{(\mu,\Sigma)}\) and \(G^{*}=(1-\lambda)\delta_{(\mu_{0},\Sigma_{0})}+\lambda\delta_{(\mu^{*},\Sigma^ {*})}\), i.e., we think of \(G\) and \(G_{*}\) as two mixing measures with one fixed atom to be \((\mu_{0},\Sigma_{0})\). In light of Lemma E.1 in Appendix E, we have

\[W_{2}^{2}(G,G_{*})\asymp\lambda\|(\Delta\mu,\Delta\Sigma)\|^{2} +\lambda^{*}\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\] \[\qquad-\min\left\{\lambda,\lambda^{*}\right\}\biggl{(}\|(\Delta \mu,\Delta\Sigma)\|^{2}+\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\biggr{)}+ \min\left\{\lambda,\lambda^{*}\right\}\|\|(\mu,\Sigma)-(\mu^{*},\Sigma^{*})\| ^{2}.\]

Therefore, \(\mathcal{D}(G,G_{*})\) and \(W_{2}^{2}(G,G_{*})\) share the similar term \(\lambda\|(\Delta\mu,\Delta\Sigma)\|^{2}+\lambda^{*}\|(\Delta\mu^{*},\Delta \Sigma^{*})\|^{2}-\min\left\{\lambda,\lambda^{*}\right\}\biggl{(}\|(\Delta\mu,\Delta\Sigma)\|^{2}+\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\biggr{)}\) in their formulations. However, as \(\lambda\|(\Delta\mu,\Delta\Sigma)\|+\lambda^{*}\|(\Delta\mu^{*},\Delta\Sigma ^{*})\|\geq\min\left\{\lambda,\lambda^{*}\right\}\|\|(\mu,\Sigma)-(\mu^{*}, \Sigma^{*})\|\), the remaining term in \(\mathcal{D}(G,G_{*})\) is stronger than that of \(W_{2}^{2}(G,G_{*})\). Moreover, as \(\lambda=\lambda^{*}\), we further obtain that

\[\mathcal{D}(G,G_{*})/W_{2}^{2}(G,G_{*})\asymp(\|(\Delta\mu,\Delta\Sigma)\|+\| (\Delta\mu^{*},\Delta\Sigma^{*})\|)/\|(\mu,\Sigma)-(\mu^{*},\Sigma^{*})\|.\]

Hence, as long as the right-hand side term in the above display goes to \(\infty\), i.e., \(||(\Delta\mu+\Delta\mu^{*},\Delta\Sigma+\Delta\Sigma^{*})|\rvert\to 0\), we have \(\mathcal{D}(G,G_{*})/W_{2}^{2}(G,G_{*})\to\infty\). This strong refinement of the Wasserstein metric is due to the special structure of \(G\) and \(G_{*}\) as one of their components is always fixed to be \((\mu_{0},\Sigma_{0})\).

**(iv)** Under the setting when \(G_{*}\) is varied, \(d_{1}=1\), and \(d_{2}=0\), by means of Fatou's lemma the result from Theorem 4.6 in [19] yields \(V(p_{G},p_{G_{*}})\geq C^{\prime}.W_{3}^{3}(G,G_{*})\) if the kernel density function \(f\) is 4-strongly identifiable (cf. Definition 2.2 in [19]) and satisfies uniform Lipschitz condition up to the fourth order where \(C^{\prime}\) is some positive constant depending only on \(G\) and \(G_{*}\). Since \(\mathcal{D}(G,G_{*})\gtrsim W_{2}^{2}(G,G_{*})\geq W_{1}^{2}(G,G_{*})\gtrsim W_{ 3}^{3}(G,G_{*})\), it indicates that the bound in Theorem 3.5 is much tighter than this bound. The loss of efficiency in this bound is again due to the special structures of \(G\) and \(G_{*}\) as one of their components is always fixed to be \((\mu_{0},\Sigma_{0})\).

Unlike the convergence rate results from the strongly distinguishable in the first order setting between \(f\) and \(h_{0}\) in Theorem 3.3, the convergence rate of \(\lambda^{*}\) under the setting of Theorem 3.5 depends on the rate of convergence of \(\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\) to 0 (cf. Theorem A.1). Additionally, the convergence rate of estimating \((\mu^{*},\Sigma^{*})\) will be determined based on the convergence rates of \(\lambda^{*}\) and \((\Delta\mu^{*},\Delta\Sigma^{*})\) to 0.

#### 3.2.2 Weakly Identifiable Settings

Thus far, as \(h_{0}\) belongs to the family \(f\), our results regarding the lower bounds between \(p_{G}\) and \(p_{G_{0}}\) under Total Variation distance rely on the strongly identifiable in the second order assumption of kernel \(f\). However, there are various families of density functions that do not satisfy such an assumption, which we refer to as the weakly identifiable condition. To illustrate the non-uniform natures of \(V(p_{G},p_{G_{*}})\) under the weakly identifiable condition of \(f\), we consider specifically a popular setting of \(f\) in this section: multivariate location-covariance Gaussian kernel.

**Location-covariance multivariate Gaussian kernel:** As indicated in the previous work in the literature [7, 25, 21], if \(f\) is a family of multivariate location-covariance Gaussian distributions in \(d\) dimension, it exhibits the _heat partial differential equation (PDE)_ with respect to the location and covariance parameter \(\frac{\partial^{2}f}{\partial\mu\partial\mu^{\top}}(x|\mu,\Sigma)=2\frac{ \partial f}{\partial\Sigma}(x|\mu,\Sigma)\), for any \(x\in\mathbb{R}^{d}\) and \((\mu,\Sigma)\in\Theta\times\Omega\). We can verify that this structure leads to the loss of the second-order strong identifiability condition of the Gaussian kernel. Note that, the PDE structure of the Gaussian kernel has been shown to lead to very slow convergence rates of parameter estimation under general over-fitted Gaussian mixture models (cf. Theorem 1.1 in [21]). For the setting of the multivariate deviated model, since the parameters \(\lambda^{*}\) and \((\mu^{*},\Sigma^{*})\) are allowed to vary with the sample size, we may expect that the estimation of these parameters will also suffer from the very slow rate. In fact, we achieve the following lower bound of \(V(p_{G},p_{G_{*}})\) under the multivariate location-covariance Gaussian kernel.

**Theorem 3.6**.: _Assume that \(h_{0}(x)=f(x|\mu_{0},\Sigma_{0})\) for some \((\mu_{0},\Sigma_{0})\in\Theta\times\Sigma\) and \(f\) is a family of multivariate location-covariance Gaussian distributions. We denote_

\[\mathcal{Q}(G,G_{*}) :=\lambda(\|\Delta\mu\|^{4}+\|\Delta\Sigma\|^{2})+\lambda^{*}(\| \Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2})\] \[-\min\left\{\lambda,\lambda^{*}\right\}\biggl{(}\|\Delta\mu\|^{4 }+\|\Delta\Sigma\|^{2}+\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2}\biggr{)}\] \[+\biggl{(}\lambda(\|\Delta\mu\|^{2}+\|\Delta\Sigma\|)+\lambda^{* }(\|\Delta\mu^{*}\|^{2}+\|\Delta\Sigma^{*}\|)\biggr{)}\biggl{(}\|\mu-\mu^{*} \|^{2}+\|\Sigma-\Sigma^{*}\|\biggr{)},\]

_for any \(G\) and \(G_{*}\). Then, we can find a positive constant \(C\) depending only on \(\Theta\), \(\Omega\), and \((\mu_{0},\Sigma_{0})\) such that \(V(p_{G},p_{G_{*}})\geq C.\mathcal{Q}(G,G_{*})\), for any \(G\) and \(G_{*}\)._

See Appendix C.3 for the proof of Theorem 3.6. A few comments with Theorem 3.6 are in order.

**(i)** Different from the formulation of \(\mathcal{D}(G,G_{*})\) in Theorem 3.5 where we have the same power between \(\mu\) and \(\Sigma\), there is a mismatch of power between \(\|\Delta\mu\|^{2},\|\Delta\mu^{*}\|^{2}\) and \(\|\Delta\Sigma\|,\|\Delta\Sigma^{*}\|\) in the formulation of \(\mathcal{Q}(G,G_{*})\). This interesting phenomenon is mainly due to the structure of the heat equation where the second-order derivative of the location parameter and the first-order derivative of the covariance parameter is linearly dependent.

**(ii)** If we denote \(\mathcal{Q}^{\prime}(G,G_{*}):=\lambda(\|\Delta\mu\|^{4}+\|\Delta\Sigma\|^{2}) +\min\left\{\lambda,\lambda^{*}\right\}\biggl{(}\|\mu-\mu^{*}\|^{4}+\|\Sigma -\Sigma^{*}\|^{2}\biggr{)}+\lambda^{*}(\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{ *}\|^{2})-\min\left\{\lambda,\lambda^{*}\right\}\biggl{(}\|\Delta\mu\|^{4}+\| \Delta\Sigma\|^{2}+\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2}\biggr{)}\), then we can verify that \(\mathcal{Q}(G,G_{*})\gtrsim\mathcal{Q}^{\prime}(G,G_{*})\) for any \(G,G_{*}\). If we treat \(G\) and \(G_{*}\) as two-components measures as in the remark (iii) after Theorem 3.5, we would have

\[\mathcal{Q}^{\prime}(G,G_{*})\asymp W_{4}^{4}(G_{1},G_{1,*})+W_{2}^{2}(G_{2}, G_{2,*}),\] (6)

where \(G_{1}=(1-\lambda)\delta_{\mu^{\prime}_{0}}+\lambda\delta_{\mu}\), \(G_{2}=(1-\lambda)\delta_{\Sigma^{\prime}_{0}}+\lambda\delta_{\Sigma}\) and similarly for \(G_{1,*}\) and \(G_{2,*}\). Here, \((\mu_{0},\Sigma_{0})=(\mu^{\prime}_{0},\Sigma^{\prime}_{0})\), and \(W_{2},W_{4}\) are respectively second and fourth order Wasserstein metrics. The formulations of \(\mathcal{Q}^{\prime}(G,G_{*})\), therefore, can be thought of as a combination of two Wasserstein metrics: one is with only parameter \(\mu\) and another one is only with parameter \(\Sigma\). The division into two Wasserstein metrics can be traced back again to the PDE structure of the heat equation.

If \(\lambda=\lambda^{*}\) and \((\|\Delta\mu\|^{2}+\|\Delta\mu^{*}\|^{2}+\|\Delta\Sigma\|+\|\Delta\Sigma^{*}\| )/\bigl{(}\|\mu-\mu^{*}\|^{2}+\|\Sigma-\Sigma^{*}\|\bigr{)}\to\infty\), we will have that \(\mathcal{Q}(G,G_{*})/\mathcal{Q}^{\prime}(G,G_{*})\to\infty.\) It proves that the result from Theorem 3.6 under the multivariate setting of Gaussian kernel is a strong refinement of the summation of Wasserstein metrics regarding location and covariance parameter in equation (6).

A consequence of Theorem 3.6 is that the convergence rate of estimating \(\lambda^{*}\) is determined by \(\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2}\), instead of \(\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\) as in the strongly identifiable setting of \(f\). Furthermore, we also encounter a phenomenon that the rate of convergence of estimating \(\Sigma^{*}\) is much faster than that of estimating \(\mu^{*}\). In particular, estimating \(\Sigma^{*}\) depends on the rate in which \(\lambda^{*}(\|\mu^{*}\|^{2}+\|\Sigma^{*}\|)\) converges to 0 while estimating \(\mu^{*}\) relies on square root of this rate (cf. Theorem A.2).

Minimax Lower Bounds and Convergence Rates of Parameter Estimation

In this section, we study the convergence rates of MLE \(\widehat{G}_{n}\) as well as minimax lower bounds of estimating \(G_{*}\) under various settings of \(h_{0}\) and \(f\). Due to space constraints, we present the theory in the distinguishable regime of \(h_{0}\) and \(f\). Non-distinguishable cases, though more interesting, are deferred to Appendix A.

**Theorem 4.1**.: **(Distinguishable settings)** _Assume that classes of densities \(h_{0}\) and \(f\) satisfy the conditions in Theorem 3.3. Then, we achieve that_

_(a) (Minimax lower bound) Assume that \(f\) satisfies the following assumption S.1:_

_(S.1)_ \[\sup_{\|(\mu,\Sigma)-(\mu^{\prime},\Sigma^{\prime})\|\leq c_{0}}\int\frac{ \left(\partial^{|\alpha|}f(x|\mu,\Sigma)/\partial\mu^{\alpha_{1}}\partial \Sigma^{\alpha_{2}}\right)^{2}}{f(x|\mu^{\prime},\Sigma^{\prime})}dx<\infty\text { for some sufficiently small }c_{0}>0,\]

_where \(\alpha_{1}\in\mathbb{N}^{d_{1}},\alpha_{2}\in\mathbb{N}^{d_{2}}\) in the partial derivative of \(f\) take any combination such that \(|\alpha|=|\alpha_{1}|+|\alpha_{2}|\leq 1\)._

_Then for any \(r<1\), there exist two universal positive constants \(c_{1}\) and \(c_{2}\) such that_

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi}\mathbb{E}_{p_{G}}\bigg{(}\lambda^{ 2}\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu,\Sigma)\|^{2}\bigg{)}\geq c _{1}n^{-1/r},\]

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi}\mathbb{E}_{p_{G}}\bigg{(}|\widehat{ \lambda}_{n}-\lambda|^{2}\bigg{)}\geq c_{2}n^{-1/r}.\]

_Here, the infimum is taken over all sequences of estimates \(\widehat{G}_{n}=(\widehat{\lambda}_{n},\widehat{\mu}_{n},\widehat{\Sigma}_{n})\)._

_(b) (MLE rate) Let \(\widehat{G}_{n}\) be the MLE defined in equation (3), and the family \(\{p_{G}:G\in\Xi\}\) satisfies condition A2. Then, we have the convergence rate for the MLE:_

\[\sup_{G_{*}\in\Xi}\mathbb{E}_{p_{G*}}\bigg{(}(\lambda^{*})^{2}\| (\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu^{*},\Sigma^{*})\|^{2}\bigg{)} \lesssim\frac{\log^{2}n}{n},\] \[\sup_{G_{*}\in\Xi}\mathbb{E}_{p_{G*}}\bigg{(}|\widehat{\lambda}_{ n}-\lambda^{*}|^{2}\bigg{)} \lesssim\frac{\log^{2}n}{n}.\]

Proof of Theorem 4.1 is in Appendix D.1. The results of Theorem 4.1 imply that even though we still can estimate \(\lambda^{*}\) at the standard rate \(n^{-1/2}\), the convergence rate of \((\widehat{\mu}_{n},\widehat{\Sigma}_{n})\) to \((\mu^{*},\Sigma^{*})\) strictly depends on the vanishing rate of \(\lambda^{*}\) to \(0\). Therefore, the convergence rate of estimating \((\mu^{*},\Sigma^{*})\) can be generally slower than \(n^{-1/2}\) as long as \(\lambda^{*}\) goes to \(0\) at a rate slower than \(n^{-1/2}\).

We can also use the geometric inequalities developed in Section 3.2 to investigate the behaviors of \(\widehat{G}_{n}\) in the non-distinguishable settings. We will further see how the _non-identifiability_ and _singularity_ of the model affect the convergence rate for density estimation. Due to space constraints, the results for this setting are presented in Appendix A.

## 5 Experiments

We now demonstrate the convergence rates of parameter estimation in the strongly distinguishable setting, where the choice of \(h_{0}\) is a standard Cauchy distribution, and \(f(\cdot|\mu,\sigma^{2})\) is the normal distribution with mean \(\mu\) and variance \(\sigma^{2}\). The additional experiments with the non-distinguishable setting are deferred to Appendix F.

Assume that \(X_{1},\ldots,X_{n}\) are i.i.d. samples drawn from the true density function \(p_{G_{*}}\) with \(G_{*}=(\lambda^{*},\mu^{*},(\sigma^{*})^{2})\) and we obtain the MLE \((\hat{\lambda}_{n},\hat{\mu}_{n},\hat{\sigma}_{n}^{2})\). We consider two following cases:

**(i)**\(\lambda^{*}=0.5\), \(\mu^{*}=2.5\), \((\sigma^{*})^{2}=0.25\);

**(ii)**\(\lambda^{*}=0.5/n^{1/4}\), \(\mu^{*}=2.5\), \((\sigma^{*})^{2}=0.25\).

Two histograms for samples from the density \(p_{G_{*}}\) with \(n=10000\) corresponding to the above two cases are illustrated in Figure 1(a) and 2(a). For each case, we take into account multiple sample sizes \(n\) ranging from \(10^{2}\) to \(10^{4}\). For each sample size \(n\), we calculate the MLE \((\hat{\lambda}_{n},\hat{\mu}_{n},\hat{\sigma}_{n}^{2})\) via the EM algorithm [13] and measure the errors \(|\widehat{\lambda}_{n}-\lambda^{*}|\), \(|\widehat{\mu}_{n}-\mu^{*}|\), and \(|\widehat{\sigma}_{n}^{2}-(\sigma^{*})^{2}|\). We repeat this procedure 64 times and plot the mean (blue dot) and quartile error bars (yellow bar) of the logarithm of estimation errors against the log of \(n\). Theorm 4.1 suggests that the log convergence rate of \(\lambda^{*}\) is of order \(-1/2\) for all cases, and so is the rate for \((\mu,(\sigma)^{2})\) in the first case. Meanwhile, the convergence rates of \((\mu^{*},(\sigma^{*})^{2})\) in the second case are slower, which is in the order of \(-1/4\). The empirical rates found in the experiments match this theoretical result, where the least square line shows that the logarithm of the rate for estimating \(\mu^{*}\) in case (i) is -0.5 and that of the case (ii) is -0.27 \(\approx-1/4\) (similar for \((\sigma^{*})^{2}\)).

We once again emphasize that this interesting phenomenon of the rates of convergence is due to the _singularity_ and _identifiability_ of the multivariate deviated model. Our theory and simulation have accurately shown quantitative convergence rates for parameter estimation when \(\lambda^{*}\) near the singularity point \(0\), where all pairs of \((\mu^{*},\Sigma^{*})\) in model (2) give the same model. Together with the non-distinguishable settings, we provide a comprehensive study of the large-sample theory for this type of model, thanks to the newly developed notion of distinguishability that helps to control the linear independent relation between \(h_{0}\) and \(f\). The developed optimal minimax lower bounds and convergence rates will certainly help Machine Learning practitioners understand better the role of sample sizes in the accuracy of estimation in the multivariate deviated model, and are also inspired theorists to study more about the estimation rate of complex hierarchical/mixture models.

## 6 Conclusion

In this paper, we establish the uniform rate for estimating true parameters in the multivariate deviated model by using the maximum likelihood estimation (MLE) method. During our derivation, we have to overcome two major obstacles, which are firstly the interaction between the known function \(h_{0}\) and the Gaussian density \(f\), and secondly the likelihood of the deviated proportion \(\lambda^{*}\) vanishing to either one or zero. To this end, we introduce a notion of distinguishability to control the linearly independent relation between two functions \(h_{0}\) and \(f\). Finally, we achieve the optimal convergence rate of the MLE under both the distinguishable and non-distinguishable settings.

## Acknowledgements

NH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning.

## References

* [1] L. Bordes, C. Delmas, and P. Vandekerkhove. Semiparametric estimation of a two-component mixture model where one component is known. _Scandinavian journal of statistics_, 33(4):733-752, 2006.
* [2] L. Bordes, S. Mottelet, and P. Vandekerkhove. Semiparametric estimation of a two-component mixture model. _The Annals of Statistics_, 34(3):1204-1232, 2006.
* [3] T. Cai, X. J. Jeng, and J. Jin. Optimal detection of heterogeneous and heteroscedastic mixtures. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 73(5):629-662, 2011.
* [4] T. Cai, J. Jin, and M. G. Low. Estimation and confidence sets for sparse normal mixtures. _The Annals of Statistics_, 35(6):2421-2449, 2007.
* 2232, 2014.
* [6] G. Casella and R. L. Berger. _Statistical inference_. Cengage Learning, 2021.
* [7] H. Chen and J. Chen. Tests for homogeneity in normal mixtures in the presence of a structural parameter. _Statistica Sinica_, 13:351-365, 2003.
* [8] H. Chen, J. Chen, and J. D. Kalbfleisch. A modified likelihood ratio test for homogeneity in finite mixture models. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 63(1):19-29, 2001.
* [9] J. Chen. Optimal rate of convergence for finite mixture models. _Annals of Statistics_, 23(1):221-233, 1995.
* [10] J. Chen, P. Li, and Y. Fu. Inference on the order of a normal mixture. _Journal of the American Statistical Association_, 107:1096-1105, 2012.
* [11] W. G. Cochran. The \(\chi\)2 test of goodness of fit. _The Annals of mathematical statistics_, pages 315-345, 1952.
* [12] N. Deb, S. Saha, A. Guntuboyina, and B. Sen. Two-component mixture model in the presence of covariates. _Journal of the American Statistical Association_, 117(540):1820-1834, 2022.
* [13] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data Via the EM Algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, Sept. 1977.
* [14] D. Do, N. Ho, and X. Nguyen. Beyond black box densities: Parameter learning for the deviated components. _arXiv preprint arXiv:2202.02651_, 2022.
* [15] D. Donoho and J. Jin. Higher criticism for detecting sparse heterogeneous mixtures. _Annals of Statistics_, 32(3):962-994, 2004.
* [16] S. Gadat, J. Kahn, C. Marteau, and C. Maugis-Rabusseau. Parameter recovery in two-component contamination mixtures: The \(l^{2}\) strategy. In _Annales de l'Institut Henri Poincare, Probabilites et Statistiques_, volume 56, pages 1391-1418. Institut Henri Poincare, 2020.
* [17] E. Gine and R. Nickl. _Mathematical foundations of infinite-dimensional statistical models_. Cambridge university press, 2021.
* [18] M. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In _STOC_, 2015.
* [19] P. Heinrich and J. Kahn. Strong identifiability and optimal minimax rates for finite mixture estimation. _Annals of Statistics_, 46(6A):2844-2870, 2018.
* [20] N. Ho and L. Nguyen. Singularity structures and impacts on parameter estimation in finite mixtures of distributions. _SIAM Journal on Mathematics of Data Science_, 1(4):730-758, 2019.

* [21] N. Ho and X. Nguyen. Convergence rates of parameter estimation for some weakly identifiable finite mixtures. _Annals of Statistics_, 44:2726-2755, 2016.
* [22] N. Ho and X. Nguyen. On strong identifiability and convergence rates of parameter estimation in finite mixtures. _Electronic Journal of Statistics_, 10:271-307, 2016.
* [23] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [24] H. Kasahara and K. Shimotsu. Non-parametric identification and estimation of the number of components in multivariate mixtures. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 76(1):97-111, 2014.
* [25] H. Kasahara and K. Shimotsu. Testing the number of components in normal mixture regression models. _Journal of the American Statistical Association_, 110(512):1632-1645, 2015.
* [26] P. Li and J. Chen. Testing the order of a finite mixture. _Journal of the American Statistical Association_, 105(491):1084-1092, 2010.
* [27] Q. Liu, J. Lee, and M. Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 276-284. PMLR, 20-22 Jun 2016.
* [28] X. Nguyen. Convergence of latent mixing measures in finite and infinite mixture models. _Annals of Statistics_, 4(1):370-400, 2013.
* [29] R. Patra and B. Sen. Estimation of a two-component mixture model with applications to multiple testing. _J. R. Stat. Soc. Series B Stat. Methodol._, 78(4):869-893, 2016.
* [30] R. Patra and B. Sen. Estimation of a two-component mixture model with applications to multiple testing. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 78(4):869-893, 2016.
* [31] A. Schrab, B. Guedj, and A. Gretton. KSD aggregated goodness-of-fit test. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [32] S. Talts, M. Betancourt, D. Simpson, A. Vehtari, and A. Gelman. Validating bayesian inference algorithms with simulation-based calibration, 2018.
* [33] S. van de Geer. _Empirical Processes in M-estimation_, volume 6. Cambridge university press, 2000.
* [34] N. Verzelen and E. Arias-Castro. Detection and feature selection in sparse mixture models. _Annals of Statistics_, 45(5):1920-1950, 2017.
* [35] C. Villani. _Topics in Optimal Transportation_. American Mathematical Society, 2003.
* [36] Y. Wu and P. Yang. Optimal estimation of Gaussian mixtures via denoised method of moments. _The Annals of Statistics_, 48:1987-2007, 2020.
* [37] J. Yang, V. Rao, and J. Neville. A stein-papangelou goodness-of-fit test for point processes. In _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pages 226-235. PMLR, 16-18 Apr 2019.

## Supplement to "Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models"

In this supplementary material, we present additional results and proofs. The minimax lower bounds and convergence rates of parameter estimation in the non-distinguishable settings are presented in Section A. The general theory for the convergence rates of densities and their proofs can be found in Appendix B. Proofs of the geometric inequalities that relate the convergence of density estimation to that of parameter estimation are in Section C, while those for minimax lower bounds and convergence rates of parameter estimation are left in Appendix D. Then, we provide a necessary lemma for those results along with its proof in Appendix E. Finally, some discussion about the general setting of the paper is presented in Section F, followed by a set of simulations to support the developed theory.

Appendix A Minimax Lower Bounds and Convergence Rates of Parameter Estimation under the Non-distinguishable Settings

**Theorem A.1**.: **(Strongly identifiable and non-distinguishable settings)** _Assume that classes of densities \(h_{0}\) and \(f\) satisfy the conditions in Theorem 3.5. We define_

\[\Xi_{1}(l_{n}):=\Bigg{\{}G=(\lambda,\mu,\Sigma)\in\Xi:\frac{l_{n}}{\min\limits \limits_{\begin{subarray}{c}1\leq i\leq d_{1}\\ 1\leq u,v\leq d_{2}\end{subarray}}\{|(\Delta\mu)_{i}|^{2},|(\Delta\Sigma)_{uv} |^{2}\}\sqrt{n}}\leq\lambda\Bigg{\}},\]

_for any sequence \(\{l_{n}\}\). Then, we achieve_

_(a) (Minimax lower bound) Assume that \(f\) satisfies assumption S.1 in Theorem 4.1. Then for any \(r<1\) and sequence \(\{l_{n}\}\), there exist two universal positive constants \(c_{1}\) and \(c_{2}\) such that_

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi_{1}(l_{n})}\mathbb{E}_{p_{G}}\bigg{(} \lambda^{2}\|(\Delta\mu,\Delta\Sigma)\|^{2}\|(\widehat{\mu}_{n},\widehat{ \Sigma}_{n})-(\mu,\Sigma)\|^{2}\bigg{)}\geq c_{1}n^{-1/r},\]

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi_{1}(l_{n})}\mathbb{E}_{p_{G}}\bigg{(} \|\Delta\mu,\Delta\Sigma)\|^{4}|\widehat{\lambda}_{n}-\lambda|^{2}\bigg{)}\geq c _{2}n^{-1/r}.\]

_(b) (MLE rate) Let \(\widehat{G}_{n}\) be the MLE defined in equation (3), and the family \(\{p_{G}:G\in\Xi\}\) satisfies condition A.2. Then, for any sequence \(\{l_{n}\}\) such that \(l_{n}/\log n\to\infty\),_

\[\sup_{G_{*}\in\Xi_{1}(l_{n})}\mathbb{E}_{p_{G_{*}}}\bigg{(}(\lambda^{*})^{2}\| (\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\|(\widehat{\mu}_{n},\widehat{\Sigma}_{ n})-(\mu^{*},\Sigma^{*})\|^{2}\bigg{)}\lesssim\frac{\log^{2}n}{n},\]

\[\sup_{G_{*}\in\Xi_{1}(l_{n})}\mathbb{E}_{p_{G_{*}}}\bigg{(}\|(\Delta\mu^{*}, \Delta\Sigma^{*})\|^{4}|\widehat{\lambda}_{n}-\lambda^{*}|^{2}\bigg{)}\lesssim \frac{\log^{2}n}{n}.\]

Proof of Theorem A.1 is in Appendix D.2. The results of part (b) are the generalization of those in Theorem 3.1 and Theorem 3.2 in [16] to the setting of strongly identifiable in the second-order kernel. The condition regarding the lower bound of \(\lambda\) in the formation of \(\Xi_{1}(l_{n})\) is necessary to guarantee that \((\widehat{\mu}_{n},\widehat{\Sigma}_{n})\) and \(\widehat{\lambda}_{n}\) are consistent estimators of \((\mu^{*},\Sigma^{*})\) and \(\lambda^{*}\) respectively. In particular, from the results in equation (34) of the proof of Theorem A.1, we have for any \(G_{*}\in\Xi\) that

\[\mathbb{E}_{p_{G_{*}}}(\lambda^{*})^{2}\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2 }\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu^{*},\Sigma^{*})\|^{2}\lesssim \frac{\log^{2}n}{n}.\]

Therefore, for any \(1\leq i\leq d_{1}\) and \(1\leq u,v\leq d_{2}\) we get

\[\mathbb{E}_{p_{G_{*}}}\bigg{\{}\bigg{(}\frac{(\Delta\widehat{\mu} _{n})_{i}}{(\Delta\mu^{*})_{i}}-1\bigg{)}^{2}\bigg{\}} \lesssim \frac{\log^{2}n}{n(\lambda^{*})^{2}\left\{(\Delta\mu^{*})_{i} \right\}^{4}},\] \[\mathbb{E}_{p_{G_{*}}}\bigg{\{}\bigg{(}\frac{(\Delta\widehat{ \Sigma}_{n})_{uv}}{(\Delta\Sigma^{*})_{uv}}-1\bigg{)}^{2}\bigg{\}} \lesssim \frac{\log^{2}n}{n(\lambda^{*})^{2}\left\{(\Delta\Sigma^{*})_{uv} \right\}^{4}}.\]It indicates that

\[\frac{\log n}{\sqrt{n}\lambda^{*}}\min_{1\leq i\leq d_{1},1\leq u,v\leq d_{2}}\{|( \Delta\mu^{*})_{i}|^{2},|(\Delta\Sigma^{*})_{i}|^{2}\}\to 0\]

for the left-hand-side terms of the above display to go to 0 for all \(1\leq i\leq d_{1}\) and \(1\leq u,v\leq d_{2}\).

The results of Theorem A.1 imply that as long as the kernel functions are strongly identifiable in the second order, the convergence rates of \(\widehat{\mu}_{n}\) to \(\mu^{*}\) and \(\widehat{\Sigma}_{n}\) to \(\Sigma^{*}\) are similar, which depend on the vanishing rate of \((\lambda^{*})^{2}\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{2}\) to 0. In our next result of location-covariance multivariate Gaussian distribution, we will demonstrate that such uniform convergence rates of different parameters no longer hold.

**Theorem A.2**.: **(Weakly identifiable and non-distinguishable settings)** _Assume that \(f\) is a family of location-covariance multivariate Gaussian distributions, and \(h_{0}(x)=f(x|\mu_{0},\Sigma_{0})\) for some \((\mu_{0},\Sigma_{0})\in\Theta\times\Sigma\). We define_

\[\Xi_{2}(l_{n}):=\bigg{\{}G=(\lambda,\mu,\Sigma)\in\Xi:\frac{l_{n}}{\min_{1\leq i \leq d,1\leq u,v\leq d}\{|(\Delta\mu)_{i}|^{4},|(\Delta\Sigma)_{uv}|^{2}\}\, \sqrt{n}}\leq\lambda\bigg{\}},\]

_for any sequence \(\{l_{n}\}\). Then, the following holds:_

_(a) (Minimax lower bound) For any \(r<1\) and sequence \(\{l_{n}\}\), there exist two universal positive constants \(c_{1}\) and \(c_{2}\) such that_

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi_{2}(l_{n})}\mathbb{E}_{p_{G}} \bigg{(}\lambda^{2}\left\{\|\Delta\mu\|^{4}+\|\Delta\Sigma\|^{2}\right\}\left\{ \|\widehat{\mu}_{n}-\mu\|^{4}+\|\widehat{\Sigma}_{n}-\Sigma\|^{2}\right\} \bigg{)}\geq c_{1}n^{-1/r},\]

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi_{2}(l_{n})}\mathbb{E}_{p_{G}} \bigg{(}\left\{\|\Delta\mu\|^{8}+\|\Delta\Sigma\|^{4}\right\}|\widehat{\lambda }-\lambda|^{2}\bigg{)}\geq c_{2}n^{-1/r}.\]

_(b) (MLE rate) Let \(\widehat{G}_{n}\) be the estimator defined in (3). Then, for any sequence \(\{l_{n}\}\) such that \(l_{n}/\log n\to\infty\) the following holds_

\[\sup_{G_{*}\in\Xi_{2}(l_{n})}\mathbb{E}_{p_{G_{*}}}\bigg{(}(\lambda^{*})^{2} \left\{\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2}\right\}\left\{\|\widehat {\mu}_{n}-\mu^{*}\|^{4}+\|\widehat{\Sigma}_{n}-\Sigma^{*}\|^{2}\right\}\bigg{)} \lesssim\frac{\log^{2}n}{n},\]

\[\sup_{G_{*}\in\Xi_{2}(l_{n})}\mathbb{E}_{p_{G_{*}}}\bigg{(}\left\{\|\Delta \mu^{*}\|^{8}+\|\Delta\Sigma^{*}\|^{4}\right\}|\widehat{\lambda}_{n}-\lambda^ {*}|^{2}\bigg{)}\lesssim\frac{\log^{2}n}{n}.\]

Proof of Theorem A.2 is in Appendix D.3. A few comments are in order:

**(i)** Similar to the argument after Theorem A.1, the condition regarding \(\lambda\) in the formulation of \(\Xi_{2}(l_{n})\) is to guarantee that \((\widehat{\mu}_{n},\widehat{\Sigma}_{n})\) and \(\widehat{\lambda}_{n}\) are consistent estimators of \((\mu^{*},\Sigma^{*})\) and \(\lambda^{*}\), respectively.

**(ii)** The results of part (b) indicate that the convergence rate of estimating \(\Sigma^{*}\) is generally much faster than that of estimating \(\mu^{*}\) regardless of the circumstance of \((\lambda^{*})^{2}\left\{\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2}\right\}\). The non-uniformity of these convergence rates is mainly due to the structure of the heat partial differential equation, where the second-order derivative of the location parameter and the first-order derivative of covariance parameter correlate.

**(iii)** From the results of part (b), it is clear that when \(\|\Delta\mu^{*}\|+\|\Delta\Sigma^{*}\|\not\to 0\), i.e., \((\mu^{*},\Sigma^{*})\to(\overline{\mu},\overline{\Sigma})\not=(\mu_{0},\Sigma _{0})\), and \(\lambda^{*}\not\to 0\), the convergence rate of \(\widehat{\lambda}_{n}\) to \(\lambda^{*}\) is \(n^{-1/2}\). Furthermore, by using the result from part (a) of Proposition C.4 we can verify that

\[\sup_{G_{*}}\mathbb{E}_{p_{G_{*}}}\bigg{(}(\lambda^{*})^{2}\left\{\|\widehat {\mu}_{n}-\mu^{*}\|^{2}+\|\widehat{\Sigma}_{n}-\Sigma^{*}\|^{2}\right\}\bigg{)} \lesssim\frac{\log^{2}n}{n},\]

where the supremum is taken over \(\{G_{*}\in\Xi_{2}(l_{n}):\mathcal{K}(G_{*},\overline{G})\leq\epsilon\}\), and \(\overline{G}=(\overline{\lambda},\overline{\mu},\overline{\Sigma})\), \(\lambda^{*}\to\overline{\lambda}\), and \(\epsilon\) is some sufficiently small positive constant. Since \(\overline{\lambda}\not=0\), we achieve the optimal convergence rate \(n^{-1/2}\) of estimating \((\mu^{*},\Sigma^{*})\) within a sufficiently small neighborhood of \(\overline{G}\) under metric \(\mathcal{K}\). These resultswhen \(G_{*}\) moves over the whole space \(\Xi_{2}(l_{n})\) (global convergence), such convergence rate can be at standard rate \(n^{-1/2}\) when \(G_{*}\) moves within a sufficiently small neighborhood of some appropriate parameters \(\overline{G}\) (local convergence).

As we have seen from the convergence rate results from location-covariance multivariate Gaussian distributions, the heat PDE structure plays a key role in the slow convergence rates of location and covariance parameters as well as the mismatch of orders of these rates.

## Appendix B Convergence Rate of Density Estimation

### General Theory and Proof of Theorem 2.3

We now describe the convergence rate of density estimation under the Hellinger distance in detail and give a general result for the multivariate deviated model. We recall some popular notions in Empirical Process theory as follows. An \(\epsilon-\)net for a metric space \((\mathcal{P},d)\) is a collection of balls with radius \(\epsilon\) (with respect to metric \(d\)) having union contains \(\mathcal{P}\). The minimal cardinality of such \(\epsilon-\)nets is called the covering number and denoted by \(N(\epsilon,\mathcal{P},d)\). The logarithm of \(N(\epsilon,\mathcal{P},d)\) is called the entropy number and is denoted by \(H(\epsilon,\mathcal{P},d)\). The bracketing number \(N_{B}(\epsilon,\mathcal{P},d)\) is the minimal number \(n\) such that there exists \(n\) pairs \((\underline{f}_{{}_{i}},\overline{f}_{{}_{i}})_{i=1}^{n}\) such that \(\underline{f}_{{}_{i}}<\overline{f}_{{}_{i}},d(\underline{f}_{{}_{i}}, \overline{f}_{{}_{i}})<\epsilon\), and their union covers \(\mathcal{P}\). The logarithm of \(N_{B}(\epsilon,\mathcal{P},d)\) is called the bracketing entropy number and is denoted by \(H_{B}(\epsilon,\mathcal{P},d)\). In the following discussion, if \(\mathcal{P}\) is a family of density and we omit \(d\), we understand that \(d\) is the distance associated with \(L^{2}(m)\), where \(m\) is the Lebesgue measure.

Denote \(\mathcal{P}(\Xi)=\{p_{G}:G\in\Xi\}\) and \(\overline{\mathcal{P}}(\Xi)=\{(p_{G_{*}}+p_{G})/2:G\in\Xi\}\) for the fixed true parameter \(G_{*}\). The convergence rate can be deduced from the complexity of the set:

\[\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon)=\left\{\bar{p}_{G}^{1/2}:G\in\Xi, \;h(\bar{p}_{G},p_{G_{*}})\leq\epsilon\right\},\] (7)

where for any \(G\in\Xi\), we denote \(\bar{p}_{G}:=(p_{G}+p_{G_{*}})/2\). We measure the complexity of this class through the bracketing entropy integral

\[\mathcal{J}_{B}(\epsilon,\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon))=\int_{ \epsilon^{2}/2^{13}}^{\epsilon}H_{B}^{1/2}(u,\overline{\mathcal{P}}^{1/2}( \Xi,\epsilon))du\vee\epsilon,\] (8)

where \(H_{B}(\epsilon,\mathcal{P})\) denotes the \(\epsilon\)-bracketing entropy number of a metric space \(\mathcal{P}\). We recall assumption A2:

* Given a universal constant \(J>0\), there exists \(N>0\), possibly depending on \(\Theta\) and \(k\), such that for all \(n\geq N\) and all \(\epsilon>(\log n/n)^{1/2}\), \[\mathcal{J}_{B}(\epsilon,\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon))\leq J \sqrt{n}\epsilon^{2}.\]

**Theorem B.1**.: _Assume that Assumption A2 holds, and let \(k\geq 1\). Then, there exists a constant \(C>0\) depending only on \(\Theta\) and \(k\) such that for all \(n\geq 1\),_

\[\sup_{G_{*}\in\Xi}\mathbb{E}_{p_{G_{*}}}h(p_{\widehat{G}_{n}},p_{G_{*}})\leq C \sqrt{\log n/n}.\]

This result can be obtained by modifying the proof of Theorem 7.4 in [33]. Recall that we defined the function class

\[\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon)=\left\{\bar{p}_{G}^{1/2}:G\in\Xi, \;h(\bar{p}_{G},p_{G_{*}})\leq\epsilon\right\},\] (9)

where for any \(G\in\Xi\), we write \(\bar{p}_{G}=(p_{G}+p_{G_{*}})/2\), and measure the complexity of this class through the bracketing entropy integral

\[\mathcal{J}_{B}(\epsilon,\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon),\nu)=\int_ {\epsilon^{2}/2^{13}}^{\epsilon}\sqrt{\log N_{B}(u,\overline{\mathcal{P}}^{1/ 2}(\Xi,u),\nu)}du\vee\epsilon,\]

where \(N_{B}(\epsilon,X,\eta)\) denotes the \(\epsilon\)-bracketing number of a metric space \((X,\eta)\) and \(\nu\) is the Lebesgue measure. We denote by \(P_{G}\) the distribution corresponding to the density \(p_{G}\). The technique to prove this theorem is to bound the convergence rate by the increments of an empirical process:

\[\nu_{n}(G)=\sqrt{n}\int_{\{p_{G_{*}}>0\}}\frac{1}{2}\log\frac{\bar{p}_{G}}{p_{ G_{*}}}d(P_{n}-P_{G_{*}}),\]where \(P_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{X_{i}}\) is the empirical measure (\(X_{1},\ldots,X_{n}\stackrel{{ iid}}{{\sim}}p_{G_{*}}\)). We first recall Theorem 5.11 in [33] with the notations adapted from our setting:

**Theorem B.2**.: _Let \(R>0\), \(k\geq 1\), and \(\mathcal{G}\) be a subset of \(\Xi\), which contains \(G_{*}\). Given \(C_{1}<\infty\), for all \(C\) sufficiently large, and for \(n\in\mathbb{N}\) and \(t>0\) satisfying_

\[t\leq\sqrt{n}((8R)\wedge(C_{1}R^{2})),\] (10)

_and_

\[t\geq C^{2}(C_{1}+1)\left(R\vee\int_{t/(2^{8}\sqrt{n})}^{R}H_{B}^{1/2}\left( \frac{u}{\sqrt{2}},\overline{\mathcal{P}}^{1/2}(\Xi,R),\nu\right)du\right),\] (11)

_we have_

\[\mathbb{P}_{\lambda^{*}G_{*}}\left(\sup_{G\in\mathcal{G},h(\overline{p}_{G},p _{G_{*}})\leq R}|\nu_{n}(G)|\geq t\right)\leq C\exp\left(-\frac{t^{2}}{C^{2}(C _{1}+1)R^{2}}\right).\] (12)

Now we proceed to prove Theorem 2.3, the proof is divided into three parts: Bounding the tail probability of \(h(p_{\tilde{G}_{n}},p_{G_{*}})\) by sums of empirical processes increments using the chaining technique, bounding the empirical processes increments using Theorem B.2, and bounding the expectation of \(h(p_{\tilde{G}_{n}},p_{G_{*}})\) using its tail probability.

**Step 1 (Bounding the tail probability \(h(p_{\tilde{G}_{n}},p_{G_{*}})\) by sums of empirical processes increments):**

Firstly, by Lemma 4.1 and 4.2 of [33], we have

\[\frac{1}{16}h^{2}(p_{\tilde{G}_{n}},p_{G_{*}})\leq h^{2}(\overline{p}_{\tilde {G}_{n}},p_{G_{*}})\leq\frac{1}{\sqrt{n}}\nu_{n}(\hat{G}_{n}).\]

Hence, for any \(\delta>\delta_{n}:=(\log n/n)^{1/2}\), we have

\[\mathbb{P}_{G_{*}}(h(p_{\tilde{G}_{n}},p_{G_{*}})\geq\delta) \leq\mathbb{P}_{G_{*}}\left(\nu_{n}(\hat{\lambda}_{n}\hat{G}_{n})- \sqrt{n}h^{2}(\overline{p}_{\tilde{G}_{n}},p_{G_{*}})\geq 0,\right.\] \[\left.h(\overline{p}_{\tilde{G}_{n}},p_{G_{*}})\geq\delta/4\right)\] \[\leq\mathbb{P}_{G_{*}}\left(\sup_{G:h(\overline{p}_{G},p_{G_{*}} )\geq\delta/4}[\nu_{n}(G)-\sqrt{n}h^{2}(\overline{p}_{G},p_{G_{*}})]\geq 0\right)\] \[\leq\sum_{s=0}^{S}\mathbb{P}_{G_{*}}\left(\sup_{G:2^{s}\delta/4 \leq h(\overline{p}_{G},p_{G_{*}})\leq 2^{s+1}\delta/4}|\nu_{n}(G)|\geq\sqrt{n}2^{2 s}(\delta/4)^{2}\right),\]

where \(S\) is a smallest number such that \(2^{S}\delta/4>1\), as \(h(\overline{p}_{G},p_{G_{*}})\leq 1\). Now we will bound each term above using Theorem B.2.

Step 2 (Bounding the empirical processes increments using Theorem B.2):In Theorem B.2, choose \(R=2^{s+1}\delta,C_{1}=15\) and \(t=\sqrt{n}2^{2s}(\delta/4)^{2}\), we can readily check that Condition (10) satisfies (because \(2^{s-1}\delta/4\leq 1\) for all \(s=0,\ldots,S\)). Condition (11) satisfies thanks to Assumption A3:

\[\int_{t/(2^{6}\sqrt{n})}^{R}H_{B}^{1/2}\left(\frac{u}{\sqrt{2}}, \mathcal{P}^{1/2}(\Xi,R),\nu\right)du\lor 2^{s+1}\delta =\sqrt{2}\int_{R^{2}/2^{13}}^{R/\sqrt{2}}H_{B}^{1/2}\left(u, \mathcal{P}^{1/2}(\Xi,R),\nu\right)du\lor 2^{s+1}\delta\] \[\leq 2\mathcal{J}_{B}(R,\mathcal{P}^{1/2}(\Xi,R),\nu)\] \[\leq 2J\sqrt{n}2^{2s+1}\delta^{2}=2^{6}Jt.\]

So the conclusion of Theorem B.2 gives us

\[\mathbb{P}_{G_{*}}(h(p_{\tilde{G}_{n}},p_{G_{*}})>\delta)\leq C\sum_{s=0}^{ \infty}\exp\left(\frac{2^{2s}n\delta^{2}}{J^{2}2^{14}}\right)\leq c\exp\left( \frac{n\delta^{2}}{c^{2}}\right),\] (13)

where \(c\) is a large constants that does not depend on \(G_{*}\).

Step 3 (Implying the bound on supremum of expectation):Thus, we have

\[\mathbb{E}h(p_{\hat{G}_{n}},p_{G_{*}})=\int_{0}^{\infty}\mathbb{P}(h(p_{\hat{G}_ {n}},p_{G_{*}})>\delta)d\delta\leq\delta_{n}+c\int_{\delta_{n}}^{\infty}\exp \left(-\frac{n\delta^{2}}{c^{2}}\right)\leq\tilde{c}\delta_{n},\]

for some \(\tilde{c}\) does not depend on \(\lambda^{*},G_{*}\). Hence, we finally proved that

\[\sup_{G_{*}\,\in\Xi}\mathbb{E}_{G_{*}}h(p_{\widehat{G}_{n}},p_{G_{*}})\leq C \sqrt{\log n/n}.\]

As a consequence, we obtain the conclusion of the theorem.

### Proof for Proposition 2.4

We further introduce some more notations that are required for the proof. Let \(N(\epsilon,\mathcal{P}(\Xi),\|\cdot\|_{\infty})\) be the \(\epsilon-\)covering number of \((\mathcal{P}(\Xi),\|\cdot\|_{\infty})\) and \(N_{B}(\epsilon,\mathcal{P}(\Xi),h)\) be the bracketing number of \(\mathcal{P}(\Xi)\) measured by Hellinger metric \(h\). \(H_{B}(\epsilon,\mathcal{P}(\Xi),h)=\log N_{B}(\epsilon,\mathcal{P}(\Xi),h)\) is called the bracketing entropy of \(\mathcal{P}(\Xi)\) under metric \(h\). We want to show that

\[\mathcal{J}_{B}(\epsilon,\overline{\mathcal{P}}^{1/2}(\Xi,\epsilon),L^{2}(m)) =\left(\int_{\epsilon^{2}/2^{213}}^{\epsilon}H_{B}^{1/2}(\delta, \overline{\mathcal{P}}^{1/2}(\Xi,\delta),L^{2}(m))d\delta\vee\delta\right) \lesssim\sqrt{n}\epsilon^{2},\] (14)

for all \(n>N\) large enough and \(\epsilon>(\log n/n)^{1/2}\). We proceed to show that claim (14) will be proved if

\[\log N(\epsilon,\mathcal{P}(\Xi),\left\|\cdot\right\|_{\infty}) \lesssim\log(1/\epsilon),\] (15) \[H_{B}(\epsilon,\mathcal{P}(\Xi),h) \lesssim\log(1/\epsilon),\] (16)

and then prove claim (15) and (16).

Proof of that claim (16) implies claim (14)Because \(\overline{\mathcal{P}}^{1/2}(\Xi,\delta)\subset\overline{\mathcal{P}}^{1/2}(\Xi)\) and from the definition of Hellinger distance,

\[H_{B}(\delta,\overline{\mathcal{P}}^{1/2}(\Xi,\delta),\mu)\leq H_{B}(\delta, \overline{\mathcal{P}}^{1/2}(\Xi),\mu)=H_{B}(\frac{\delta}{\sqrt{2}}, \overline{\mathcal{P}}(\Xi),h).\]

Now use the fact that for densities \(f_{*},f_{1},f_{2}\), we have \(h^{2}((f_{1}+f_{*})/2,(f_{2}+f_{*})/2)\leq h^{2}(f_{1},f_{2})/2\), one can readily check that \(H_{B}(\frac{\delta}{\sqrt{2}},\overline{\mathcal{P}}(\Xi),h)\leq H_{B}(\delta,\mathcal{P}(\Xi),h)\). Hence, if claim (16) holds true, then

\[H_{B}(\delta,\overline{\mathcal{P}}^{1/2}(\Xi,\delta),\mu)\leq H_{B}(\delta, \mathcal{P}(\Xi),h)\lesssim\log(1/\delta),\]

which implies that

\[\mathcal{J}_{B}(\epsilon,\overline{\mathcal{P}}^{1/2}(\Xi,\delta),\mu) \lesssim\epsilon(\log(2^{13}/\epsilon^{2}))^{1/2}<n\epsilon^{2},\]

for all \(\epsilon>(\log n/n)^{1/2}\). Hence, claim (14) is proved.

Proof of claim (15)As \(\lambda\in[0,1]\), we can choose an \(\epsilon-\)net for it with the cardinality no more than \(\frac{1}{\epsilon}\). Similarly, because \(\Theta\) and \(\Omega\) are compact, we can cover them by hypercube \([-a,a]^{d_{1}}\) and \([-b,b]^{d_{2}\times d_{2}}\). Hence, there exists \(\epsilon-\)nets for them with the cardinality no more than \(\left(\frac{2a}{\epsilon}\right)^{d_{1}}\) and \(\left(\frac{2b}{\epsilon}\right)^{d_{2}^{2}}\). Let \(\mathcal{S}\) be the Cartesian product of them. We have \(\log|\mathcal{S}|\lesssim\log(1/\epsilon)\) and for every \(\hat{G}=(\lambda,\mu,\Sigma)\in\Xi\), there exists \(G^{\prime}=(\lambda^{\prime},\mu^{\prime},\Sigma^{\prime})\in\mathcal{S}\) such that \(|\lambda-\lambda^{\prime}|,\left\|\mu-\mu^{\prime}\right\|,\left\|\Sigma- \Sigma^{\prime}\right\|\leq\epsilon\). By triangle inequalities,

\[\left\|p_{G}-p_{G^{\prime}}\right\|_{\infty}\leq|\lambda-\lambda^{\prime}|( \left\|h_{0}\right\|_{\infty}+\left\|f\right\|_{\infty})+\lambda|f(x|\mu, \Sigma)-f(x|\mu^{\prime},\Sigma^{\prime})|\lesssim\epsilon,\]

thanks to the uniform bounded and Lipchitz assumptions. Hence,

\[\log N(\epsilon,\mathcal{P}(\Xi),\left\|\cdot\right\|_{\infty})\lesssim\log(1/ \epsilon).\]Proof of claim (16)Now, from the entropy number, we are going to bound the bracketing number, we let \(\eta\leq\epsilon\) which will be chosen later. Let \(f_{1},\ldots,f_{N}\) be a \(\eta\)-net for \(\mathcal{P}(\Xi)\), where \(f_{i}(x)=(1-\lambda_{i})h_{0}(x)+\lambda_{i}f(x|\mu_{i},\Sigma_{i})\). Let

\[H(x)=\begin{cases}b_{1}\exp(-b_{2}\left\|x\right\|^{b_{3}}),&\left\|x\right\|_ {2}\geq B_{1},\\ M,&\text{otherwise}\end{cases}\] (17)

is an envelop for \(f(x|\mu,\Sigma)\). We can construct brackets \([p_{i}^{L},p_{i}^{U}]\) as follows.

\[p_{i}^{L}(x) =(1-\lambda_{i})h_{0}(x)+\lambda_{i}\max\{f(x|\mu_{i},\Sigma_{i} )-\eta,0\},\] \[p_{i}^{U}(x) =(1-\lambda_{i})h_{0}(x)+\lambda_{i}\min\{f(x|\mu_{i},\Sigma_{i} )+\eta,H(x)\}.\]

Because for each \(f\in\mathcal{P}(\Xi)\), there is \(f_{i}\) such that \(\left\|f-f_{i}\right\|_{\infty}<\eta\), we have \(p_{i}^{L}\leq f\leq p_{i}^{U}\). Moreover, for any \(\overline{B}\geq B\),

\[\int_{\mathbb{R}^{d}}(p_{i}^{U}-p_{i}^{L})d\mu \leq\lambda_{i}\left(\int_{\left\|x\right\|\leq\overline{B}}2\eta dx +\int_{\left\|x\right\|\geq\overline{B}}H(x)dx\right)\] \[\lesssim\eta\overline{B}^{d}+\overline{B}^{d}\exp\left(-b_{2} \overline{B}^{b_{3}}\right),\] (18)

where we use spherical coordinate to have

\[\int_{\left\|x\right\|\leq\overline{B}}dx=\frac{\pi^{d/2}}{\Gamma(d/2+1)} \overline{B}^{d}\lesssim\overline{B}^{d},\]

and

\[\int_{\left\|x\right\|\geq\overline{B}}\exp\left(-b_{2}\left\|x \right\|^{b_{3}}\right) \lesssim\int_{r\geq\overline{B}}r^{d-1}\exp\left(-b_{2}r^{b_{3}} \right)dr\] \[=\frac{1}{b_{3}b_{2}^{1/b_{3}}}\int_{\mathbb{B}^{b_{3}}}^{ \infty}u^{d/b_{3}-1}\exp(-u)du\quad(\text{change of variable }u=b_{2}r^{b_{3}})\] \[\leq\frac{1}{b_{3}b_{2}^{1/b_{3}}}\overline{B}^{d-b_{3}}\exp(- \overline{B}^{b_{3}}).\]

Hence, in (18), if we choose \(\overline{B}=B(\log(1/\eta))^{1/b_{3}}\) then

\[\int_{\mathbb{R}^{d}}(p_{i}^{U}-p_{i}^{L})d\mu\lesssim\eta\left(\log\left( \frac{1}{\eta}\right)\right)^{d/b_{3}}.\] (19)

Therefore, there exists a positive constant \(c\) which does not depend on \(\eta\) such that

\[H_{B}(c\eta\log(1/\eta)^{d/b_{3}},\mathcal{P}(\Xi),\left\|\cdot\right\|_{1}) \lesssim\log(1/\eta).\]

Let \(\epsilon=c\eta(\log(1/\eta))^{d/b_{3}}\), we have \(\log(1/\epsilon)\asymp\log(1/\eta)\), which combines with inequality \(\left\|\cdot\right\|_{1}\leq h^{2}\) leads to

\[H_{B}(\epsilon,\mathcal{P}(\Xi),h)\leq H_{B}(\epsilon^{2},\mathcal{P}(\Xi), \left\|\cdot\right\|_{1})\lesssim\log(1/\epsilon^{2})\lesssim\log(1/\epsilon).\]

Thus, we have proved claim (16).

## Appendix C Proofs for Geometric Inverse Bounds

### Proof of Theorem 3.3

The second inequality in Theorem 3.3 is straightforward from the equivalent form of \(W_{1}(G,G_{*})\) in Lemma E.1 (see Appendix E). Therefore, we will only focus on establishing the first inequality in that theorem. We start with the following key result:

**Proposition C.1**.: _Given the assumptions in Theorem 3.5 and \(\overline{G}=(\overline{\lambda},\overline{\mu},\overline{\Sigma})\) such that \(\overline{\lambda}\in[0,1]\) and \((\overline{\mu},\overline{\Sigma})\) can be equal to \((\mu_{0},\Sigma_{0})\). Then, we have_

\[\lim_{\epsilon\to 0}\inf_{G,G}\left\{\frac{V(p_{G},p_{G_{*}})}{\mathcal{K}(G,G_{*} )}:\ \mathcal{K}(G,\overline{G})\vee\mathcal{K}(G_{*},\overline{G})\leq\epsilon \right\}>0.\]Proof.: The high level idea of the proof of Proposition C.3 is to utilize the Taylor expansion techniques previously employed in [9, 28, 22, 19]. Indeed, following Fatou's argument from Theorem 3.1 in [22], to obtain the conclusion of Proposition C.3 it suffices to demonstrate that

\[\lim_{\epsilon\to 0}\inf_{G,G_{*}}\bigg{\{}\frac{\|p_{G}-p_{G_{*}}\|_{ \infty}}{\mathcal{K}(G,G_{*})}:\,\mathcal{K}(G,\overline{G})\vee\mathcal{K}(G_ {*},\overline{G})\leq\epsilon\bigg{\}}>0.\]

Assume that the above conclusion does not hold. It implies that we can find two sequences \(G_{n}=(\lambda_{n},\mu_{n},\Sigma_{n})\) and \(G_{*,n}=(\lambda_{n}^{*},\mu_{n}^{*},\Sigma_{n}^{*})\) such that \(\mathcal{K}(G_{n},\overline{G})\to 0\), \(\mathcal{K}(G_{*,n},\overline{G})\to 0\), and \(\|p_{G_{n}}-p_{G_{*,n}}\|_{\infty}/\mathcal{K}(G_{n},G_{*,n})\to 0\) as \(n\to\infty\). Now, we only consider the most challenging setting of \((\mu_{n},\Sigma_{n})\) and \((\mu_{n}^{*},\Sigma_{n}^{*})\) when they share the same limit point \((\mu^{\prime},\Sigma^{\prime})\). The other settings of these two components can be argued in the same fashion. Here, \((\mu^{\prime},\Sigma^{\prime})\) is not necessarily equal to \((\mu_{0},\Sigma_{0})\) or \((\overline{\mu},\overline{\Sigma})\) as \(\lambda_{n},\lambda_{n}^{*}\) can go to 0 or 1 in the limit. Under that setting, by means of Taylor expansion up to the first order we obtain

\[\frac{p_{G_{n}}(x)-p_{G_{*,n}}(x)}{\mathcal{K}(G_{n},G_{*,n})} = \frac{(\lambda_{n}^{*}-\lambda_{n})[h_{0}(x|\mu_{0},\Sigma_{0})- f(x|\mu_{n}^{*},\Sigma_{n}^{*})]+\lambda_{n}[f(x|\mu_{n},\Sigma_{n})-f(x|\mu_{n}^{*}, \Sigma_{n}^{*})]}{\mathcal{K}(G_{n},G_{*,n})}\] \[= \frac{(\lambda_{n}^{*}-\lambda_{n})[h_{0}(x|\mu_{0},\Sigma_{0})- f(x|\mu_{n}^{*},\Sigma_{n}^{*})]}{\mathcal{K}(G_{n},G_{*,n})}\] \[+ \frac{\lambda_{n}\bigg{(}\sum\limits_{|\alpha|=1}\frac{(\mu_{n}- \mu_{n}^{*})^{\alpha_{1}}(\Sigma_{n}-\Sigma_{n}^{*})^{\alpha_{2}}}{\alpha!} \frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})+R_{1}(x)\bigg{)}}{\mathcal{K}(G_{n},G_{*,n})}\]

where \(R_{1}(x)\) is Taylor remainder and \(\alpha=(\alpha_{1},\alpha_{2})\) in the summation of the second equality satisfies \(\alpha_{1}=(\alpha_{1}^{(1)},\ldots,\alpha_{d_{1}}^{(1)})\in\mathbb{N}^{d_{1}}\), \(\alpha_{2}=(\alpha_{uv}^{(2)})\in\mathbb{N}^{d_{2}\times d_{2}}\), \(|\alpha|=\sum\limits_{i=1}^{d_{1}}\alpha_{i}^{(1)}+\sum\limits_{1\leq u,v\leq d _{2}}\alpha_{uv}^{(2)}\), and \(\alpha!=\prod\limits_{i=1}^{d_{1}}\alpha_{i}^{(1)}!\prod\limits_{1\leq u,v\leq d _{2}}\alpha_{uv}^{(2)}\). As \(f\) admits the first order uniform Lipschitz condition, we have \(R_{1}(x)=O(\|(\mu_{n},\Sigma_{n})-(\mu_{n}^{*},\Sigma_{n}^{*})\|^{1+\gamma})\) for some \(\gamma>0\), which implies that

\(\lambda_{n}|R_{1}(x)|/\mathcal{K}(G_{n},G_{*,n})=O(\|(\mu_{n},\Sigma_{n})-(\mu_ {n}^{*},\Sigma_{n}^{*})\|^{\gamma})\to 0\)

as \(n\to\infty\). Therefore, we can treat \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{K}(G_{n},G_{*,n})\) as the linear combination of \(h_{0}(x|\theta_{0},\Sigma_{0})\) and \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) when \(|\alpha|\leq 1\). Assume that the coefficients of these terms go to 0. Then, by studying the coefficients of \(h_{0}(x|\theta_{0},\Sigma_{0})\), \(\frac{\partial f}{\partial\mu_{i}}(x|\mu_{0},\Sigma_{0})\), and \(\frac{\partial f}{\partial\Sigma_{uv}}(x|\mu_{0},\Sigma_{0})\), we achieve

\((\lambda_{n}^{*}-\lambda_{n})/\mathcal{K}(G_{n},G_{*,n})\to 0\), \(\lambda_{n}(\mu_{n}-\mu_{n}^{*})_{i}/\mathcal{K}(G_{n},G_{*,n})\to 0\), \(\lambda_{n}(\Sigma_{n}-\Sigma_{n}^{*})_{uv}/\mathcal{K}(G_{n},G_{*,n})\to 0\) for all \(1\leq i\leq d_{1}\) and \(1\leq u,v\leq d_{2}\) where \((a)_{i}\) denotes the \(i\)-th element of vector \(a\) and \(A_{uv}\) denotes the \((u,v)\)-th element of matrix \(A\). It would imply that

\((\lambda_{n}+\lambda_{n}^{*})\|(\mu_{n},\Sigma_{n})-(\mu_{n}^{*},\Sigma_{n}^{*} )\|/\mathcal{K}(G_{n},G_{*,n})\to 0\).

Therefore, we achieve

\[1= \bigg{(}|\lambda_{n}^{*}-\lambda_{n}|+(\lambda_{n}+\lambda_{n}^{*})\|( \mu_{n},\Sigma_{n})-(\mu_{n}^{*},\Sigma_{n}^{*})\|\bigg{)}/\mathcal{K}(G_{n},G_ {*,n})\to 0,\]

a contradiction. Therefore, not all the coefficients of \(h_{0}(x|\theta_{0},\Sigma_{0})\) and \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) go to 0. If we denote \(m_{n}\) to be the maximum of the absolute values of the coefficients of \(h_{0}(x|\theta_{0},\Sigma_{0})\) and \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\), then we get \(1/m_{n}\not\to\infty\) as \(n\to\infty\), i.e., \(1/m_{n}\) is uniformly bounded. Hence, we achieve for all \(x\) that

\[\frac{1}{m_{n}}\frac{p_{G_{n}}(x)-p_{G_{*,n}}(x)}{K(G_{n},G_{*,n})}\to\eta f(| \mu_{0},\Sigma_{0})+\sum_{|\alpha|\leq 1}\tau_{\alpha}\frac{\partial^{|\alpha|}f}{ \partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2}}}(x|\mu_{n}^{\prime},\Sigma^{ \prime})=0\]

for some coefficients \(\eta\) and \(\tau_{\alpha}\) such that they are not all 0. However, as \(f\) is distinguishable from \(h_{0}\) up to the first order, the above equation indicates that \(\eta=\tau_{\alpha}=0\) for all \(|\alpha|\leq 1\), a contradiction. As a consequence, we achieve the conclusion of the proposition.

Now, assume that the conclusion of Theorem (3.3) does not hold. It implies that we can find two sequences \(G^{\prime}_{n}\) and \(G^{\prime}_{*,n}\) such that \(A_{n}=\|p_{G^{\prime}_{n}}-p_{G^{\prime}_{*,n}}\|_{2}/\mathcal{K}(G^{\prime}_{n},G^{\prime}_{*,n})\to 0\) as \(n\to\infty\). Since \(\Theta\) and \(\Omega\) are two bounded subsets, we can find subsequences of \(G^{\prime}_{n}\) and \(G^{\prime}_{*,n}\) such that \(\mathcal{K}(G^{\prime}_{n},\overline{G}_{1})\) and \(\mathcal{K}(G^{\prime}_{*,n},\overline{G}_{2})\) vanish to 0 as \(n\to\infty\) where \(\overline{G}_{1},\overline{G}_{2}\) are some discrete measures having one component to be \((\mu_{0},\Sigma_{0})\). Because \(A_{n}\to 0\), we obtain \(V(p_{G^{\prime}_{n}},p_{G^{\prime}_{*,n}})\to 0\) as \(n\to\infty\). By means of Fatou's lemma, we have

\[0=\lim_{n\to\infty}\int\Big{|}(p_{G^{\prime}_{n}}(x)-p_{G^{\prime}_{*,n}}(x)) \Big{|}\,dx\geq\int\liminf_{n\to\infty}\Big{|}\big{(}p_{G^{\prime}_{n}}(x)-p_ {G^{\prime}_{*,n}}(x))\Big{|}\,dx=V(p_{\overline{G}_{1}}(x),p_{\overline{G}_{ 2}}(x)).\]

Due to the fact that \(f\) is distinguishable from \(h_{0}\) up to the first order, the above equation implies that \(\overline{G}_{1}\equiv\overline{G}_{2}\). However, from the result of Proposition C.1, regardless of the value of \(\overline{G}_{1}\) we would have \(A_{n}\not\to 0\) as \(n\to\infty\), which is a contradiction. Therefore, we obtain the conclusion of the theorem.

### Proof of Theorem 3.5

Prior to presenting the proof of Theorem 3.5, we introduce the definition of second-order uniform Lipschitz:

**Definition C.2** (Second-order Uniform Lipschitz).: We say that \(f\) is uniformly Lipschitz up to the second order if the following holds: there are positive constants \(\delta_{3}\), \(\delta_{4}\) such that for any \(R_{4},R_{5},R_{6}>0\), \(\gamma_{1}\in\mathbb{R}^{d_{1}}\), \(\gamma_{2}\in\mathbb{R}^{d_{2}\times d_{2}}\), \(R_{4}\leq\sqrt{\lambda_{1}(\Sigma)}\leq\sqrt{\lambda_{d_{2}}(\Sigma)}\leq R_{5}\), \(\|\theta\|\leq R_{6}\), \(\theta_{1},\theta_{2}\in\Theta\), \(\Sigma_{1},\Sigma_{2}\in\Omega\), there are positive constants \(C_{1}\) depending on \((R_{4},R_{5})\) and \(C_{2}\) depending on \(R_{6}\) such that for all \(x\in\mathcal{X}\),

\[\Big{|}\gamma_{1}^{\top}\Big{(}\frac{\partial^{2}f}{\partial \theta^{2}}(x|\theta_{1},\Sigma)-\frac{\partial^{2}f}{\partial\theta^{2}}(x| \theta_{2},\Sigma)\Big{)}\gamma_{1}\Big{|}\leq\|\theta_{1}-\theta_{2}\|_{1}^{ \delta_{3}}\|\gamma_{1}\|_{2}^{2},\] \[\Big{|}\operatorname{tr}\Big{(}\Big{[}\frac{\partial}{\partial \Sigma}\Big{(}\operatorname{tr}\Big{(}\frac{\partial f}{\partial\Sigma_{1}}( x|\theta,\Sigma)^{\top}\gamma_{2}\Big{)}\Big{)}-\frac{\partial}{\partial\Sigma} \Big{(}\operatorname{tr}\Big{(}\frac{\partial f}{\partial\Sigma}(x|\theta, \Sigma_{2})^{\top}\gamma_{2}\Big{)}\Big{)}\Big{]}^{\top}\gamma_{2}\Big{)}\Big{|}\] \[\leq C_{2}\|\Sigma_{1}-\Sigma_{2}\|_{2}^{\delta_{4}}\|\gamma\|_{2} ^{2}.\]

Now, we are back to the main proof. Utilizing the same Fatou's argument as that of Proposition C.1, to achieve the conclusion of the first inequality in Theorem 3.5 it suffices to demonstrate the following result

**Proposition C.3**.: _Given the assumptions in Theorem 3.5 and \(\overline{G}=(\overline{\lambda},\overline{\mu},\overline{\Sigma})\) such that \(\overline{\lambda}\in[0,1]\) and \((\overline{\mu},\overline{\Sigma})\) can be identical to \((\mu_{0},\Sigma_{0})\). Then, the following holds_

* _If_ \((\mu_{0},\Sigma_{0})\not=(\overline{\mu},\overline{\Sigma})\) _and_ \(\overline{\lambda}>0\)_, then_ \[\lim_{\epsilon\to 0}\inf_{G,G_{*}}\left\{\frac{\|p_{G}-p_{G_{*}}\|_{\infty}}{ \mathcal{K}(G,G_{*})}:\;\mathcal{K}(G,\overline{G})\vee\mathcal{K}(G_{*}, \overline{G})\leq\epsilon\right\}>0.\]
* _If_ \((\mu_{0},\Sigma_{0})\equiv(\overline{\mu},\overline{\Sigma})\) _or_ \((\mu_{0},\Sigma_{0})\not=(\overline{\mu},\overline{\Sigma})\) _and_ \(\overline{\lambda}=0\)_, then_ \[\lim_{\epsilon\to 0}\inf_{G,G_{*}}\left\{\frac{\|p_{G}-p_{G_{*}}\|_{\infty}}{ \mathcal{D}(G,G_{*})}:\;\mathcal{D}(G,\overline{G})\vee\mathcal{D}(G_{*}, \overline{G})\leq\epsilon\right\}>0.\]

Proof.: The proof of part (a) is essentially similar to that of Proposition C.1; therefore, we only provide the proof for the challenging settings of part (b). Here, we only consider the setting that \((\mu_{0},\Sigma_{0})\equiv(\overline{\mu},\overline{\Sigma})\) as the proof for other possibilities of \((\mu_{0},\Sigma_{0})\) can be argued in the similar fashion. Under this assumption, \((\mu_{0},\Sigma_{0})=(\mu_{0},\Sigma_{0})\), \(\overline{G}=(\overline{\lambda},\theta_{0},\Sigma_{0})\), and \(h_{0}(x|\theta_{0},\Sigma_{0})=f(x|\theta_{0},\Sigma_{0})\) for all \(x\in\mathcal{X}\). Assume that the conclusion of Proposition C.3 does not hold. It implies that we can find two sequences \(G_{n}=(\lambda_{n},\mu_{n},\Sigma_{n})\) and \(G_{*,n}=(\lambda_{n}^{*},\mu_{n}^{*},\Sigma_{n}^{*})\) such that \(\mathcal{D}(G_{n},\overline{G})=\lambda_{n}\|(\Delta\mu_{n},\Delta\Sigma_{n})\|^ {2}\to 0\), \(\mathcal{D}(G_{*,n},\overline{G})=\lambda_{n}^{*}\|(\Delta\mu_{n}^{*},\Delta \Sigma_{n}^{*})\|^{2}\to 0\), and \(\|p_{G_{n}}-p_{G_{n,n}}\|_{\infty}/\mathcal{D}(G_{n},G_{*,n})\to 0\) as \(n\to\infty\). For the transparency of presentation, we denote \(A_{n}=\|(\Delta\mu_{n},\Delta\Sigma_{n})\|\), \(B_{n}=\|(\Delta\mu_{n}^{*},\Delta\Sigma_{n}^{*})\|\), and \(C_{n}=\|(\mu_{n},\Sigma_{n})-(\mu_{n}^{*},\Sigma_{n}^{*})\|=\|(\Delta\mu_{n}, \Delta\Sigma_{n})-(\Delta\mu_{n}^{*},\Delta\Sigma_{n}^{*})\|\). Now, we have three main cases regarding the convergence behaviors of \((\mu_{n},\Sigma_{n})\) and \((\mu_{n}^{*},\Sigma_{n}^{*})\)Case 1:Both \(A_{n}\to 0\) and \(B_{n}\to 0\), i.e., \((\mu_{n},\Sigma_{n})\) and \((\mu_{n}^{*},\Sigma_{n}^{*})\) vanish to \((\mu_{0},\Sigma_{0})\) as \(n\to\infty\). Due to the symmetry between \(\lambda_{n}\) and \(\lambda_{n}^{*}\), we assume without loss of generality that \(\lambda_{n}^{*}\geq\lambda_{n}\) for infinite values of \(n\). Without loss of generality, we replace these subsequences of \(G_{n},G_{*,n}\) by the whole sequences of \(G_{n}\) and \(G_{*,n}\). Now, the formulation of \(\mathcal{D}(G_{n},G_{*,n})\) is

\[\mathcal{D}(G_{n},G_{*,n})=(\lambda_{n}^{*}-\lambda_{n})B_{n}^{2}+\bigg{(} \lambda_{n}A_{n}+\lambda_{n}^{*}B_{n}\bigg{)}C_{n}.\]

Now, by means of Taylor expansion up to the second order, we get

\[\frac{p_{G_{n}}(x)-p_{G_{*,n}}(x)}{\mathcal{D}(G_{n},G_{*,n})} = \frac{(\lambda_{n}^{*}-\lambda_{n})[f(x|\mu_{0},\Sigma_{0})-f(x| \mu_{n}^{*},\Sigma_{n}^{*})]+\lambda_{n}[f(x|\mu_{n},\Sigma_{n})-f(x|\mu_{n}^{ *},\Sigma_{n}^{*})]}{\mathcal{D}(G_{n},G_{*,n})}\] \[= \frac{(\lambda_{n}^{*}-\lambda_{n})\bigg{(}\sum\limits_{|\alpha| =1}^{2}\frac{(-\Delta\mu_{n}^{*})^{\alpha_{1}}(-\Delta\Sigma_{n}^{*})^{\alpha _{2}}}{\alpha!}\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial \Sigma^{\alpha_{2}}}(x|\mu_{n}^{*},\Sigma_{n}^{*})+R_{1}(x)\bigg{)}}{ \mathcal{D}(G_{n},G_{*,n})}\] \[+ \frac{\lambda_{n}\bigg{(}\sum\limits_{|\alpha|=1}^{2}\frac{( \Delta\mu_{n}-\Delta\mu_{n}^{*})^{\alpha_{1}}(\Delta\Sigma_{n}-\Delta\Sigma_{ n}^{*})^{\alpha_{2}}}{\alpha!}\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}} \partial\Sigma^{\alpha_{2}}}(x|\mu_{n}^{*},\Sigma_{n}^{*})+R_{2}(x)\bigg{)}}{ \mathcal{D}(G_{n},G_{*,n})}\]

where \(R_{1}(x)\) and \(R_{2}(x)\) are Taylor remainders that satisfy \(R_{1}(x)=O(B_{n}^{2+\gamma})\) and \(R_{2}(x)=O(C_{n}^{2+\gamma})\) for some positive number \(\gamma\) due to the second order uniform Lipschitz condition of kernel density function \(f\). From the formation of \(\mathcal{D}(G_{n},G_{*,n})\), since \(A_{n}+B_{n}\geq C_{n}\) (triangle inequality), as \(A_{n}\to 0\) and \(B_{n}\to 0\) it is clear that

\[(\lambda_{n}-\lambda_{n}^{*})|R_{1}(x)|/\mathcal{D}(G_{n},G_{*,n})\leq|R_{1}( x)|/B_{n}^{2}=O(B_{n}^{\gamma})\to 0\]

as \(n\to\infty\) for all \(x\in\mathcal{X}\). Therefore, we achieve for all \(x\in\mathcal{X}\) that

\[\bigg{(}(\lambda_{n}-\lambda_{n}^{*})|R_{1}(x)|+\lambda_{n}|R_{2}(x)|\bigg{)} /\mathcal{D}(G_{n},G_{*,n})\to 0.\]

Hence, we can treat \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{D}(G_{n},G_{*,n})\) as a linear combination of \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) for all \(x\) and \(\alpha=(\alpha_{1},\alpha_{2})\) such that \(1\leq|\alpha|\leq 2\). Assume that all the coefficients of these terms go to 0 as \(n\to\infty\). By studying the vanishing behaviors of the coefficients of \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) as \(|\alpha|=1\), we achieve the following limits

\[\bigg{(}\lambda_{n}(\Delta\mu_{n})_{i}-\lambda_{n}^{*}(\Delta\mu_{n}^{*})_{i} \bigg{)}/\mathcal{D}(G_{n},G_{*,n})\to 0,\ \bigg{(}\lambda_{n}(\Delta\Sigma_{n})_{uv}-\lambda_{n}^{*}(\Delta\Sigma_{n}^{*} )_{uv}\bigg{)}/\mathcal{D}(G_{n},G_{*,n})\to 0\]

for all \(1\leq i\leq d_{1}\) and \(1\leq u,v\leq d_{2}\) where \((a)_{i}\) denotes the \(i\)-th element of vector \(a\) and \(A_{uv}\) denotes the \((u,v)\)-th element of matrix \(A\). Furthermore, for any \(1\leq i,j\leq d\) (\(i\) and \(j\) can be equal), the coefficient of \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) when \((\alpha_{1})_{i}=(\alpha_{1})_{j}=1\) and \(\alpha_{2}=0\) leads to

\[\bigg{[}(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}^{*})_{i}(\Delta\mu_{n}^{*}) _{j}+\lambda_{n}(\Delta\mu_{n}-\Delta\mu_{n}^{*})_{i}(\Delta\mu_{n}-\Delta\mu_ {n}^{*})_{j}\bigg{]}/\mathcal{D}(G_{n},G_{*,n})\to 0.\] (20)

When \(i=j\), the above limits lead to

\[\bigg{[}(\lambda_{n}^{*}-\lambda_{n})\left\{(\Delta\mu_{n}^{*})_{i}\right\}^{2} +\lambda_{n}\left\{(\Delta\mu_{n}-\Delta\mu_{n}^{*})_{i}\right\}^{2}\bigg{]}/ \mathcal{D}(G_{n},G_{*,n})\to 0.\]

Therefore, we would have

\[\bigg{[}(\lambda_{n}^{*}-\lambda_{n})\|\Delta\mu_{n}^{*}\|^{2}+\lambda_{n}\| \Delta\mu_{n}-\Delta\mu_{n}^{*}\|^{2}\bigg{]}/\mathcal{D}(G_{n},G_{*,n})\to 0.\] (21)Now, as \(\bigg{(}\lambda_{n}(\Delta\mu_{n})_{i}-\lambda_{n}^{*}(\Delta\mu_{n}^{*})_{i} \bigg{)}/\mathcal{D}(G_{n},G_{*,n})\to 0\) we obtain that

\[\bigg{(}\lambda_{n}(\Delta\mu_{n})_{i}(\Delta\mu_{n})_{j}-\lambda_ {n}^{*}(\Delta\mu_{n}^{*})_{i}(\Delta\mu_{n})_{j}\bigg{)}/\mathcal{D}(G_{n},G_{ *,n}) \rightarrow 0,\] \[\bigg{(}\lambda_{n}(\Delta\mu_{n})_{i}(\Delta\mu_{n}^{*})_{j}- \lambda_{n}^{*}(\Delta\mu_{n}^{*})_{i}(\Delta\mu_{n}^{*})_{j}\bigg{)}/ \mathcal{D}(G_{n},G_{*,n}) \rightarrow 0.\] (22)

Plugging the results from (22) into (20), we ultimately achieve for any \(1\leq i,j\leq d\) that

\[(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}^{*})_{i}(\Delta\mu_{n})_{j}/ \mathcal{D}(G_{n},G_{*,n})\to 0.\] (23)

Using the results from (20) and (23), we would have

\[\frac{\lambda_{n}(\Delta\mu_{n})_{i}(\Delta\mu_{n}-\Delta\mu_{n} ^{*})_{j}}{\mathcal{D}(G_{n},G_{*,n})}\rightarrow\frac{(\lambda_{n}^{*}- \lambda_{n})(\Delta\mu_{n})_{i}(\Delta\mu_{n}^{*})_{j}}{\mathcal{D}(G_{n},G_{ *,n})}\to 0,\] \[\frac{\lambda_{n}^{*}(\Delta\mu_{n}^{*})_{i}(\Delta\mu_{n}- \Delta\mu_{n}^{*})_{j}}{\mathcal{D}(G_{n},G_{*,n})}\rightarrow\frac{(\lambda_ {n}^{*}-\lambda_{n})(\Delta\mu_{n}^{*})_{i}(\Delta\mu_{n})_{j}}{\mathcal{D}(G_ {n},G_{*,n})}\to 0\]

for any \(1\leq i,j\leq d\). Therefore, it leads to

\[\frac{\sum\limits_{1\leq i,j\leq d}\lambda_{n}|(\Delta\mu_{n})_{i}| |(\Delta\mu_{n}-\Delta\mu_{n}^{*})_{j}|}{\mathcal{D}(G_{n},G_{*,n})}=\frac{ \lambda_{n}\sum\limits_{1\leq i\leq d}|(\Delta\mu_{n})_{i}|\sum\limits_{1\leq i \leq d}|(\Delta\mu_{n}-\Delta\mu_{n}^{*})_{i}|}{\mathcal{D}(G_{n},G_{*,n})} \to 0,\] \[\frac{\sum\limits_{1\leq i,j\leq d}\lambda_{n}^{*}|(\Delta\mu_{n} ^{*})_{i}|(\Delta\mu_{n}-\Delta\mu_{n}^{*})_{j}|}{\mathcal{D}(G_{n},G_{*,n})} =\frac{\lambda_{n}^{*}\sum\limits_{1\leq i\leq d}|(\Delta\mu_{n} )_{i}^{*}|\sum\limits_{1\leq i\leq d}|(\Delta\mu_{n}-\Delta\mu_{n}^{*})_{i}|}{ \mathcal{D}(G_{n},G_{*,n})}\to 0.\]

The above results mean that

\[\lambda_{n}\|\Delta\mu_{n}\|\|\Delta\mu_{n}-\Delta\mu_{n}^{*}\|/\mathcal{D}(G _{n},G_{*,n})\to 0,\;\lambda_{n}^{*}\|\Delta\mu_{n}^{*}\|\|\Delta\mu_{n}- \Delta\mu_{n}^{*}\|/\mathcal{D}(G_{n},G_{*,n})\to 0.\] (24)

By applying the above argument with the coefficients of \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) when \(\alpha_{1}=0\) and \((\alpha_{2})_{u_{1}v_{1}}=(\alpha_{2})_{u_{2}v_{2}}=1\) for any two pairs \((u_{1},v_{1}),(u_{2},v_{2})\) (not necessarily distinct) such that \(1\leq u_{1},u_{2},v_{1},v_{2}\leq d\) or \((\alpha_{1})_{i}=1\) and \((\alpha_{2})_{uv}=1\) for any \(1\leq i\leq d\) and \(1\leq u,v\leq d\), we respectively obtain that

\[\bigg{[}(\lambda_{n}^{*}-\lambda_{n})\|\Delta\Sigma_{n}^{*}\|^{2}+ \lambda_{n}\|\Delta\Sigma_{n}-\Delta\Sigma_{n}^{*}\|^{2}\bigg{]}/\mathcal{D}(G_ {n},G_{*,n})\to 0,\] \[\lambda_{n}\|\Delta\Sigma_{n}\|\|\Delta\Sigma_{n}-\Delta\Sigma_{n} ^{*}\|/\mathcal{D}(G_{n},G_{*,n})\to 0,\;\lambda_{n}^{*}\|\Delta\Sigma_{n}^{*}\|\| \Delta\Sigma_{n}-\Delta\Sigma_{n}^{*}\|/\mathcal{D}(G_{n},G_{*,n})\to 0,\] \[\lambda_{n}\|\Delta\mu_{n}\|\|\Delta\Sigma_{n}-\Delta\Sigma_{n}^{ *}\|/\mathcal{D}(G_{n},G_{*,n})\to 0,\;\lambda_{n}^{*}\|\Delta\mu_{n}^{*}\|\| \Delta\Sigma_{n}-\Delta\Sigma_{n}^{*}\|/\mathcal{D}(G_{n},G_{*,n})\to 0.\] (25)

Combining the results from (21), (24), and (25) leads to

\[1=\mathcal{D}(G_{n},G_{*,n})/\mathcal{D}(G_{n},G_{*,n})\to 0,\]

which is a contradiction. As a consequence, not all the coefficients of \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2 }}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) go to 0 as \(1\leq|\alpha|\leq 2\). Follow the argument of Proposition C.1, by denoting \(m_{n}\) to be the maximum of the absolute values of the coefficients of \(\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial\bar{\Sigma}^{\alpha_{ 2}}}(x|\mu_{n}^{*},\Sigma_{n}^{*})\) we achieve for all \(x\) that

\[\frac{1}{m_{n}}\frac{p_{G_{n}}(x)-p_{G_{*,n}}(x)}{W_{2}^{2}(G_{n},G_{*,n})} \rightarrow\sum_{|\alpha|=1}^{2}\tau_{\alpha}\frac{\partial^{|\alpha|}f}{\partial \mu^{\alpha_{1}}\partial\bar{\Sigma}^{\alpha_{2}}}(x|\mu_{0},\Sigma_{0})=0\]

where \(\tau_{\alpha}\in\mathbb{R}\) are some coefficients such that not all of them are 0. Due to the second order identifiability condition of \(f\), the above equation implies that \(\tau_{\alpha}=0\) for all \(\alpha\) such that \(|\alpha|=2\), which is a contradiction. As a consequence, Case 1 cannot happen.

Case 2:Exactly one of \(A_{n}\) and \(B_{n}\) goes to 0, i.e., there exists at least one component among \((\mu_{n},\Sigma_{n})\) and \((\mu^{*}_{n},\Sigma^{*}_{n})\) that does not converge to \((\mu_{0},\Sigma_{0})\) as \(n\to\infty\). Due to the symmetry of \(A_{n}\) and \(B_{n}\), we assume without loss of generality that \(A_{n}\not\to 0\) and \(B_{n}\to 0\), which is equivalent to \((\mu_{n},\Sigma_{n})\to(\mu^{\prime},\Sigma^{\prime})\neq(\mu_{0},\Sigma_{0})\) while \((\mu^{*}_{n},\Sigma^{*}_{n})\to(\mu_{0},\Sigma_{0})\) as \(n\to\infty\). We denote

\[\mathcal{D}^{\prime}(G_{n},G_{*,n})=|\lambda^{*}_{n}-\lambda_{n}|B_{n}+\lambda _{n}A_{n}+\lambda^{*}_{n}B_{n}.\]

Since \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{D}(G_{n},G_{*,n})\to 0\), we achieve that \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{D}^{\prime}(G_{n},G_{*,n})\)\(\to 0\) for all \(x\) as \(\mathcal{D}(G_{n},G_{*,n})\lesssim\mathcal{D}^{\prime}(G_{n},G_{*,n})\). By means of Taylor expansion up to the first order, we have

\[\frac{p_{G_{n}}(x)-p_{G_{*,n}}(x)}{\mathcal{D}^{\prime}(G_{n},G_{ *,n})} = \frac{(\lambda^{*}_{n}-\lambda_{n})[f(x|\mu_{0},\Sigma_{0})-f(x| \mu^{*}_{n},\Sigma^{*}_{n})]+\lambda_{n}f(x|\mu_{n},\Sigma_{n})-\lambda_{n}f( x|\mu^{*}_{n},\Sigma^{*}_{n})}{\mathcal{D}^{\prime}(G_{n},G_{*,n})}\] \[= \frac{(\lambda^{*}_{n}-\lambda_{n})\bigg{(}\sum\limits_{|\alpha|= 1}\frac{(-\Delta\mu^{*}_{n})^{\alpha_{1}}(-\Delta\Sigma^{*}_{n})^{\alpha_{2}}} {\alpha!}\frac{\partial f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2} }}(x|\mu^{*}_{n},\Sigma^{*}_{n})+R^{\prime}_{1}(x)\bigg{)}}{\mathcal{D}^{ \prime}(G_{n},G_{*,n})}\] \[+ \frac{\lambda_{n}f(x|\mu_{n},\Sigma_{n})-\lambda_{n}f(x|\mu^{*}_{ n},\Sigma^{*}_{n})}{\mathcal{D}^{\prime}(G_{n},G_{*,n})}\]

where \(R^{\prime}_{1}(x)\) is Taylor remainder that satisfies \((\lambda^{*}_{n}-\lambda_{n})|R^{\prime}_{1}(x)|/\mathcal{D}^{\prime}(G_{n},G _{*,n})=O(B_{n}^{\gamma^{\prime}})\to 0\) for some positive number \(\gamma^{\prime}>0\). Since \((\mu_{n},\Sigma_{n})\) and \((\mu^{*}_{n},\Sigma^{*}_{n})\) do not have the same limit, they will be different when \(n\) is large enough, i.e., \(n\geq M^{\prime}\) for some value of \(M^{\prime}\). Now, as \(n\geq M^{\prime}\), \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{D}^{\prime}(G_{n},G_{*,n})\) becomes a linear combination of \(\frac{\partial f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2}}}(x|\mu^ {*}_{n},\Sigma^{*}_{n})\) for all \(|\alpha|\leq 1\) and \(f(x|\mu_{n},\Sigma_{n})\). If all of the coefficients of these terms go to 0, we would have \(\lambda_{n}/\mathcal{D}^{\prime}(G_{n},G_{*,n})\to 0\), \((\lambda^{*}_{n}-\lambda_{n})(-\Delta\mu^{*}_{n})_{i}/\mathcal{D}^{\prime}(G_{ n},G_{*,n})\to 0\), and \((\lambda^{*}_{n}-\lambda_{n})(-\Delta\Sigma^{*}_{n})_{uv}/\mathcal{D}^{\prime}(G _{n},G_{*,n})\to 0\) for all \(1\leq i\leq d_{1}\) and \(1\leq u,v\leq d_{2}\). It would imply that \((\lambda^{*}_{n}-\lambda_{n})B_{n}/\mathcal{D}^{\prime}(G_{n},G_{*,n})\to 0\), \(\lambda_{n}A_{n}/\mathcal{D}^{\prime}(G_{n},G_{*,n})\to 0\), and \(\lambda_{n}B_{n}/\mathcal{D}^{\prime}(G_{n},G_{*,n})\to 0\). These results lead to

\[1=\biggr{(}|\lambda^{*}_{n}-\lambda_{n}|B_{n}+\lambda_{n}A_{n}+\lambda^{*}_{n} B_{n}\biggr{)}/\mathcal{D}^{\prime}(G_{n},G_{*,n})\to 0,\]

a contradiction. Therefore, not all the coefficients of \(\frac{\partial f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2}}}(x|\mu^ {*}_{n},\Sigma^{*}_{n})\) and \(f(x|\mu_{n},\Sigma_{n})\) go to 0. By defining \(m^{\prime}_{n}\) to be the maximum of these coefficients, we achieve for all \(x\) that

\[\frac{1}{m^{\prime}_{n}}\frac{p_{G_{n}}(x)-p_{G_{*,n}}(x)}{\mathcal{D}^{\prime} (G_{n},G_{*,n})}\to\eta^{\prime}f(x|\mu_{0},\Sigma_{0})+\sum\limits_{|\alpha|= 0}^{1}\tau^{\prime}_{\alpha}\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1 }}\partial\Sigma^{\alpha_{2}}}(x|\mu^{\prime},\Sigma^{\prime})=0,\]

where \(\eta^{\prime}\) and \(\tau^{\prime}_{\alpha}\) are coefficients such that not all of them are 0, which is a contradiction to the first order identifiability of \(f\). As a consequence, Case 2 cannot hold.

Case 3:Both \(A_{n}\) and \(B_{n}\) do not go to 0, i.e., \((\mu_{n},\Sigma_{n})\) and \((\mu^{*}_{n},\Sigma^{*}_{n})\) do not converge to \((\mu_{0},\Sigma_{0})\) as \(n\to\infty\). Since \(\mathcal{D}_{n}(G_{n},G_{*,n})\lesssim\mathcal{K}(G_{n},G_{*,n})=\big{|}\lambda_{n }-\lambda^{*}_{n}|+(\lambda_{n}+\lambda^{*}_{n})C_{n}\) and \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{D}(G_{n},G_{*,n})\to 0\), we achieve that \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{K}(G_{n},G_{*,n})\to 0\) for all \(x\). From here, by using the same argument as that of the proof of Proposition C.1, we also reach the contradiction. Therefore, Case 3 cannot happen.

In sum, we achieve the conclusion of the proposition. 

### Proof of Theorem 3.6

For the simplicity of proof argument, we will only consider the univariate setting of Gaussian kernel, i.e., when both \(\mu\) and \(\Sigma=\sigma^{2}\) are scalars. The argument for the multivariate setting of Gaussian kernel can be argued in the rather similar fashion, which is omitted. Throughout this proof, we denote \(v:=\sigma^{2}\). Now, according to the proof argument of Proposition C.1 and Proposition C.3, to achieve the conclusion of the theorem it suffices to demonstrate the following result:

**Proposition C.4**.: _Given \(\overline{G}=(\overline{\lambda},\overline{\mu},\overline{v})\) such that \(\overline{\lambda}\in[0,1]\) and \((\overline{\mu},\overline{v})\) can be identical to \((\mu_{0},v_{0})\). Then, the following holds_1. _If_ \((\mu_{0},v_{0})\neq(\overline{\mu},\overline{v})\) _and_ \(\overline{\lambda}>0\)_, then_ \[\liminf_{\epsilon\to 0}\inf_{G,G_{*}}\left\{\frac{\|p_{G}-p_{G_{*}}\|_{ \infty}}{\mathcal{K}(G,G_{*})}:\;\mathcal{K}(G,\overline{G})\vee\mathcal{K}(G_ {*},\overline{G})\leq\epsilon\right\}>0.\]
2. _If_ \((\mu_{0},v_{0})\equiv(\overline{\mu},\overline{v})\) _or_ \((\mu_{0},v_{0})\neq(\overline{\mu},\overline{v})\) _and_ \(\overline{\lambda}=0\)_, then_ \[\liminf_{\epsilon\to 0}\inf_{G,G_{*}}\left\{\frac{\|p_{G}-p_{G_{*}}\|_{ \infty}}{\mathcal{Q}(G,G_{*})}:\;\mathcal{Q}(G,\overline{G})\vee\mathcal{Q}(G _{*},\overline{G})\leq\epsilon\right\}>0.\]

Proof.: We will only provide the proof for part (b) since the proofs for part (a) can be argued in similar fashion as that of Proposition C.1. Assume that the conclusion of Proposition C.4 does not hold. It implies that we can find two sequences \(G_{n}=(\lambda_{n},\mu_{n},v_{n})\) and \(G_{*,n}=(\lambda_{n}^{*},\mu_{n}^{*},v_{n}^{*})\) such that \(\mathcal{Q}(G_{n},\overline{G})\to 0\), \(\mathcal{Q}(G_{*,n},\overline{G})\to 0\), and \(\|p_{G_{n}}-p_{G_{*,n}}\|_{\infty}/\mathcal{Q}(G_{n},G_{*,n})\to 0\) as \(n\to\infty\). Due to the symmetry between \(\lambda_{n}\) and \(\lambda_{n}^{*}\), we can assume without loss of generality that \(\lambda_{n}^{*}\geq\lambda_{n}\). Therefore, we achieve that

\[\mathcal{Q}(G_{n},G_{*,n})=(\lambda_{n}^{*}-\lambda_{n})(|\Delta\mu_{n}^{*}|^{ 4}+|\Delta v_{n}^{*}|^{2})+\left(\lambda_{n}(|\Delta\mu_{n}|^{2}+|\Delta v_{n} |)+\lambda_{n}^{*}(|\Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|)\right)\times\]

\[\times\left(|\mu_{n}-\mu_{n}^{*}|^{2}+|v_{n}-v_{n}^{*}|\right).\]

In this proof, we only consider the scenario when \(\|(\Delta\mu_{n},\Delta v_{n})\|\to 0\) and \(\|(\Delta\mu_{n}^{*},\Delta v_{n}^{*})\|\to 0\) since the arguments for other settings of these two terms are similar to those of Case 2 and Case 3 in the proof of Proposition C.3. As being indicated in Section 3.2.2, the univariate Gaussian kernel contains the partial differential equation structure \(\frac{\partial^{2}f}{\partial\mu^{2}}(x|\mu,v)=2\frac{\partial f}{\partial v} (x|\mu,v)\) for all \(\mu\in\Theta\) and \(v\in\Omega\). Therefore, for any \(\alpha=(\alpha_{1},\alpha_{2})\) we can check that

\[\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}}\partial v^{\alpha_{2}}}( x|\mu,v)=\frac{1}{2^{\alpha_{2}}}\frac{\partial^{\beta}f}{\partial\mu^{\beta}}(x| \mu,v)\]

where \(\beta=\alpha_{1}+2\alpha_{2}\). Now, by means of Taylor expansion up to the fourth order, we obtain

\[\frac{p_{G_{n}}(x)-p_{G_{*,n}}(x)}{\mathcal{Q}(G_{n},G_{*,n})} = \frac{(\lambda_{n}^{*}-\lambda_{n})\bigg{(}\sum\limits_{|\alpha|=1 }^{4}\frac{(-\Delta\mu_{n}^{*})^{\alpha_{1}}(-\Delta v_{n}^{*})^{\alpha_{2}}} {\alpha_{1}!\alpha_{2}!}\frac{\partial^{|\alpha|}f}{\partial\mu^{\alpha_{1}} \partial v^{\alpha_{2}}}(x|\mu_{n}^{*},v_{n}^{*})+R_{1}(x)\bigg{)}}{\mathcal{ Q}(G_{n},G_{*,n})}\] \[+ \frac{\lambda_{n}\bigg{(}\sum\limits_{|\alpha|=1}^{4}\frac{( \Delta\mu_{n}-\Delta\mu_{n}^{*})^{\alpha_{1}}(\Delta v_{n}-\Delta v_{n}^{*})^{ \alpha_{2}}}{\alpha_{1}!\alpha_{2}!}\frac{\partial^{|\alpha|}f}{\partial\mu^{ \alpha_{1}}\partial v^{\alpha_{2}}}(x|\mu_{n}^{*},v_{n}^{*})+R_{2}(x)\bigg{)} }{\mathcal{Q}(G_{n},G_{*,n})}\] \[= \sum_{\beta=1}^{8}\sum_{\alpha_{1},\alpha_{2}}\frac{(\lambda_{n}^ {*}-\lambda_{n})(-\Delta\mu_{n}^{*})^{\alpha_{1}}(-\Delta v_{n}^{*})^{\alpha_{2} }+\lambda_{n}(\Delta\mu_{n}-\Delta\mu_{n}^{*})^{\alpha_{1}}(\Delta v_{n}- \Delta v_{n}^{*})^{\alpha_{2}}}{2^{\alpha_{2}}\alpha_{1}!\alpha_{2}!\mathcal{ Q}(G_{n},G_{*,n})}\] \[\times \frac{\partial^{\beta}f}{\partial\mu^{\beta}}(x|\mu_{n}^{*},v_{ n}^{*})+\frac{(\lambda_{n}^{*}-\lambda_{n})R_{1}(x)+\lambda_{n}R_{2}(x)}{ \mathcal{Q}(G_{n},G_{*,n})}\]

where \(R_{1}(x),R_{2}(x)\) are Taylor remainders and the range of \(\alpha_{1},\alpha_{2}\) in the summation of the second equality satisfies \(\beta=\alpha_{1}+2\alpha_{2}\). As Gaussian kernel admits fourth-order uniform Lipschitz condition, it is clear that

\[\frac{(\lambda_{n}^{*}-\lambda_{n})|R_{1}(x)|+\lambda_{n}|R_{2}(x)|}{\mathcal{ Q}(G_{n},G_{*,n})}=\mathcal{O}(\|(\Delta\mu_{n}^{*},\Delta v_{n}^{*})\|^{\gamma}+\|( \mu_{n},v_{n})-(\mu_{n}^{*},v_{n}^{*})\|^{\gamma})\to 0\]

as \(n\to\infty\) for some \(\gamma>0\). Therefore, we can consider \([p_{G_{n}}(x)-p_{G_{*,n}}(x)]/\mathcal{Q}(G_{n},G_{*,n})\) as a linear combination of \(\frac{\partial^{\beta}f}{\partial\mu^{\beta}}(x|\mu_{n}^{*},v_{n}^{*})\) for \(1\leq\beta\leq 8\). If all of the coefficients of these terms go to 0, then we obtain

\[L_{\beta}=\frac{\sum\limits_{\alpha_{1},\alpha_{2}}\frac{(\lambda_{n}^{*}-\lambda_ {n})(-\Delta\mu_{n}^{*})^{\alpha_{1}}(-\Delta v_{n}^{*})^{\alpha_{2}}+\lambda_{n}( \Delta\mu_{n}-\Delta\mu_{n}^{*})^{\alpha_{1}}(\Delta v_{n}-\Delta v_{n}^{*})^{ \alpha_{2}}}{2^{|\alpha_{2}|}\alpha_{1}!\alpha_{2}!}\to 0}{\mathcal{Q}(G_{n},G_{*,n})}\to 0\]

for any \(1\leq\beta\leq 8\). Now, we divide our argument with \(L_{\beta}\) into two key casesCase 1:\(\left(\lambda_{n}(|\Delta\mu_{n}|^{2}+|\Delta v_{n}|)+\lambda_{n}^{*}|(\Delta\mu_{ n}^{*}|^{2}+|\Delta v_{n}^{*}|)\right)/\bigg{\{}\lambda_{n}(|\mu_{n}-\mu_{n}^{*}|^{2}+|v_{n} -v_{n}^{*}|)\bigg{\}}\not\rightarrow\infty\).

It implies that as \(n\) is large enough, we would have

\[Q(G_{n},G_{*,n})\lesssim(\lambda_{n}^{*}-\lambda_{n})(|\Delta\mu_{n}^{*}|^{4}+ |\Delta v_{n}^{*}|^{2})+\lambda_{n}(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{4}+| \Delta v_{n}-\Delta v_{n}^{*}|^{2}).\]

Combining the above result with \(L_{\beta}\to 0\) for all \(1\leq\beta\leq 8\), we get

\[H_{\beta}=\frac{\sum\limits_{\alpha_{1},\alpha_{2}}\frac{(\lambda_{n}^{*}- \lambda_{n})(-\Delta\mu_{n}^{*})^{\alpha_{1}}(-\lambda v_{n}^{*})^{\alpha_{2}} +\lambda_{n}(\Delta\mu_{n}-\Delta\mu_{n}^{*})^{\alpha_{1}}(\Delta v_{n}- \Delta v_{n}^{*})^{\alpha_{2}}}{2^{|\alpha_{2}|}\alpha_{1}\alpha_{2}!}}\to 0\]

Note that, when the denominator of the above limits is \((\lambda_{n}^{*}-\lambda_{n})(|\Delta\mu_{n}^{*}|^{4}+|\Delta v_{n}^{*}|^{4})+ \lambda_{n}(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{4}+|\Delta v_{n}-\Delta v_{n}^ {*}|^{4})\), the technique for studying the above system of limits with this denominator has been considered in Proposition 2.3 in [21]. However, since the current denominator of \(H_{\beta}\) strongly dominates by the previous denominator, we must develop a more sophisticated control of \(H_{\beta}\) as \(1\leq\beta\leq 8\) to obtain a concrete understanding of their limits. Due to the symmetry between \(\lambda_{n}^{*}-\lambda_{n}\) and \(\lambda_{n}\), we assume without loss of generality that \(\lambda_{n}^{*}-\lambda_{n}\leq\lambda_{n}\) for all \(n\) (by the subsequence argument). We have two possibilities regarding \(\lambda_{n}\) and \(\lambda_{n}^{*}\)

Case 1.1:\((\lambda_{n}^{*}-\lambda_{n})/\lambda_{n}\not\to 0\) as \(n\rightarrow\infty\). Under that setting, we define \(p_{n}=\max\left\{\lambda_{n}^{*}-\lambda_{n},\lambda_{n}\right\}\) and

\[M_{n}=\max\Big{\{}|\Delta\mu_{n}^{*}|,|\Delta\mu_{n}^{*}-\Delta\mu_{n}|,| \Delta v_{n}^{*}|^{1/2},|\Delta v_{n}^{*}-\Delta v_{n}|^{1/2}\Big{\}}\]

Additionally, we let \((\lambda_{n}^{*}-\lambda_{n})/p_{n}\to c_{1}^{2},\lambda_{n}/p_{n} \to c_{2}^{2},\Delta\mu_{n}^{*}/M_{n}\rightarrow-a_{1}\), \((\Delta\mu_{n}^{*}-\Delta\mu_{n})/M_{n}\to a_{2}\), \(\Delta v_{n}^{*}/M_{n}^{2}\rightarrow-2b_{1}\), and \((\Delta v_{n}-\Delta v_{n}^{*})/M_{n}^{2}\to 2b_{2}\). From here, at least one among \(a_{1},a_{2},b_{1},b_{2}\) and both \(c_{1},c_{2}\) are different from 0. Now, by dividing both the numerators and the denominators of \(H_{\beta}\) as \(1\leq\beta\leq 4\) by \(p_{n}M_{n}^{\beta}\), we achieve the following system of polynomial equations

\[c_{1}^{2}a_{1}+c_{2}^{2}a_{2}=0\] \[\frac{1}{2}(c_{1}^{2}a_{1}^{2}+c_{2}^{2}a_{2}^{2})+c_{1}^{2}b_{1 }+c_{2}^{2}b_{2}=0\] \[\frac{1}{3!}(c_{1}^{2}a_{1}^{3}+c_{2}^{2}a_{2}^{3})+c_{1}^{2}a_{1 }b_{1}+c_{2}^{2}a_{2}b_{2}=0\] \[\frac{1}{4!}(c_{1}^{2}a_{1}^{4}+c_{2}^{2}a_{2}^{4})+\frac{1}{2!}( c_{1}^{2}a_{1}^{2}b_{1}+c_{2}^{2}a_{2}^{2}b_{2})+\frac{1}{2!}(c_{1}^{2}b_{1}^{2}+c_{ 2}^{2}b_{2}^{2})=0,\]

As being indicated in Proposition 2.1 in [21], this system will only admits the trivial solution, i.e., \(a_{1}=a_{2}=b_{1}=b_{2}=0\), which is a contradiction. Therefore, Case 1.1 cannot happen.

Case 1.2:\((\lambda_{n}^{*}-\lambda_{n})/\lambda_{n}\to 0\), i.e., \(\lambda_{n}^{*}/\lambda_{n}\to 1\), as \(n\rightarrow\infty\). Under that setting, if \(M_{n}\in\max\left\{|\Delta\mu_{n}-\Delta\mu_{n}^{*}|,|\Delta v_{n}-\Delta v_{n} ^{*}|^{1/2}\right\}\), then we have

\[\lambda_{n}M_{n}^{4}=\max\biggl{\{}(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n} ^{*}|^{4},(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{4}, \lambda_{n}|\Delta v_{n}^{*}|^{2},\lambda_{n}|\Delta v_{n}-\Delta v_{n}^{*}|^{ 2}\biggr{\}}.\]

By dividing both the numerator and the denominator of \(H_{1}\) by \(\lambda_{n}M_{n}\), given that the new denominator of \(H_{1}\) goes to 0, its new numerator also goes to 0, i.e., we obtain

\[(\lambda_{n}^{*}-\lambda_{n})(-\Delta\mu_{n}^{*})/\left\{\lambda_{n}M_{n}\right\} +(\Delta\mu_{n}-\Delta\mu_{n}^{*})/M_{n}\to 0.\]

Since \((\lambda_{n}^{*}-\lambda_{n})/\lambda_{n}\to 0\) and \(|\Delta\mu_{n}^{*}|\leq M_{n}\), we have \((\lambda_{n}^{*}-\lambda_{n})(-\Delta\mu_{n}^{*})/\left\{\lambda_{n}M_{n} \right\}\to 0\). Therefore, we have \((\Delta\mu_{n}-\Delta\mu_{n}^{*})/M_{n}\to 0\). With the previous results, by dividing both the numerator and the denominator of \(H_{2}\) by \(\lambda_{n}M_{n}^{2}\) and given that the new denominator goes to 0, we have

\[(\lambda_{n}^{*}-\lambda_{n})(-\Delta v_{n}^{*})/\left\{\lambda_{n}M_{n}^{2} \right\}+(\Delta v_{n}-\Delta v_{n}^{*})/M_{n}^{2}\to 0.\]

As \((\lambda_{n}^{*}-\lambda_{n})(-\Delta v_{n}^{*})/\left\{\lambda_{n}M_{n}^{2} \right\}\to 0\) (due to the assumption of \(M_{n}\)), we get \((\Delta v_{n}-\Delta v_{n}^{*})/M_{n}^{2}\to 0\). These results imply that

\[1=\frac{\max\left\{|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}-\Delta v_{ n}^{*}|\right\}}{M_{n}^{2}}\to 0,\]which is a contradiction. Therefore, we would only have \(M_{n}\in\max\left\{|\Delta\mu_{n}^{*}|,|\Delta\upsilon_{n}^{*}|^{1/2}\right\}\). For the simplicity of the proof, we only consider the setting when \(M_{n}=|\Delta\mu_{n}^{*}|\) for all \(n\) (by subsequence argument). The setting that \(M_{n}=|\Delta\upsilon_{n}^{*}|^{1/2}\) for all \(n\) can be argued in the similar fashion. Now, if we have

\[\max\Big{\{}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|,|\Delta v_{n}-\Delta\upsilon_{n} ^{*}|^{1/2}\Big{\}}/M_{n}\not\to 0,\]

then by dividing the numerator and denominator of \(H_{i}\) with \(\lambda_{n}\left(\max\big{\{}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|,|\Delta v_{n}- \Delta\upsilon_{n}^{*}|^{1/2}\big{\}}\right)^{i}\) as \(1\leq i\leq 2\), we would achieve

\[1=\frac{\max\big{\{}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}- \Delta\upsilon_{n}^{*}|\big{\}}}{\max\big{\{}|\Delta\mu_{n}-\Delta\mu_{n}^{* }|^{2},|\Delta v_{n}-\Delta\upsilon_{n}^{*}|\big{\}}}\to 0,\]

a contradiction. Therefore, we must have

\[\max\Big{\{}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|,|\Delta v_{n}-\Delta\upsilon_{n }^{*}|^{1/2}\Big{\}}/M_{n}\not\to 0\] (26)

as \(n\to\infty\). Now, we further divide the argument under that setting of \(M_{n}\) into two small cases

**Case 1.2.1:** \((\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{4}\leq\lambda_{n}|\Delta\mu _{n}-\Delta\mu_{n}^{*}|^{4}\) for all \(n\) (by subsequence argument). Since \(M_{n}=|\Delta\mu_{n}^{*}|\), we would have \((\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{i}\leq\lambda_{n}|\Delta\mu _{n}-\Delta\mu_{n}^{*}|^{i}\) for all \(n\) and \(1\leq l\leq 4\). From here, we obtain that

\[\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{4}}{ \lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|}\leq\frac{\lambda_{n}|\Delta\mu _{n}-\Delta\mu_{n}^{*}|^{4}}{\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|} \to 0,\] \[\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta\upsilon_{n}^{*}|^{2}} {\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|}\leq\frac{(\lambda_{n}^{*}- \lambda_{n})|\Delta\mu_{n}^{*}|^{4}}{\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^ {*}|}\to 0.\]

If \(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|/|\Delta v_{n}-\Delta\upsilon_{n}^{*}|^{1/2} \not\to 0\), by diving both the numerator and the denominator of \(H_{1}\) by \(\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|\) and given that the new denominator goes to 0, the new numerator must converge to 0, i.e. we have

\[(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}^{*}/\left\{\lambda_{n}(\Delta\mu_{ n}-\Delta\mu_{n}^{*})\right\}\to-1.\]

However, since we have \(|(\Delta\mu_{n}-\Delta\mu_{n}^{*})/\Delta\mu_{n}^{*}\to 0\), the above result would imply that

\[(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{4}/\left\{\lambda_{n}|\Delta \mu_{n}-\Delta\mu_{n}^{*}|^{4}\right\}\to\infty,\]

which is a contradiction to the assumption of Case 1.2.1.1. As a consequence, we must have \(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|/|\Delta v_{n}-\Delta\upsilon_{n}^{*}|^{1/2}\to 0\). Now, we also have that

\[\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{4}}{\lambda_{n}| \Delta\upsilon_{n}-\Delta\upsilon_{n}^{*}|^{i/2}}\leq\frac{\lambda_{n}|\Delta \upsilon_{n}-\Delta\upsilon_{n}^{*}|^{2}}{\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{ n}^{*}|^{i/2}}\to 0,\]

\[\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta\upsilon_{n}^{*}|^{4}}{\lambda_{n}| \Delta\upsilon_{n}-\Delta\upsilon_{n}^{*}|^{i/2}}\leq\frac{(\lambda_{n}^{*}- \lambda_{n})|\Delta\mu_{n}^{*}|^{4}}{\lambda_{n}|\Delta\upsilon_{n}-\Delta \upsilon_{n}^{*}|^{i/2}}\to 0.\]

for all \(1\leq i\leq 3\). Without loss of generality, we assume that \(\Delta v_{n}-\Delta\upsilon_{n}^{*}>0\) for all \(n\). We denote \((-\Delta\mu_{n}^{*})=q_{1}^{n}(\Delta v_{n}-\Delta\upsilon_{n}^{*})\) and \(\Delta\upsilon_{n}^{*}=q_{2}^{n}(\Delta\upsilon_{n}-\Delta\upsilon_{n}^{*})\) for all \(n\). From the result of (26), we would have \(|q_{1}^{n}|\to\infty\). Given the above results, by dividing the numerators and the denominators of \(H_{\beta}\) by \(\lambda_{n}(\Delta\upsilon_{n}-\Delta\upsilon_{n}^{*})^{\beta/2}\) for any \(1\leq\beta\leq 3\), we would have the new denominators go to 0. Therefore, all the new numerators of these \(H_{\beta}\) also go to 0, i.e. we achieve the following system of limits

\[\frac{\lambda_{n}^{*}-\lambda_{n}}{\lambda_{n}}q_{1}^{n}\to 0,\ \frac{\lambda_{n}^{*}- \lambda_{n}}{\lambda_{n}}\left\{(q_{1}^{n})^{2}+q_{2}^{n}\right\}+1\to 0,\ \frac{\lambda_{n}^{*}- \lambda_{n}}{\lambda_{n}}\bigg{(}\frac{(q_{1}^{n})^{3}}{6}+\frac{q_{1}^{n}q_{2} ^{n}}{2}\bigg{)}\to 0.\]

Since \(|q_{1}^{n}|\to\infty\), the last limit in the above system implies that \((\lambda_{n}^{*}-\lambda_{n})\bigg{(}\frac{(q_{1}^{n})^{2}}{3}+q_{2}^{n}\bigg{)}/ \lambda_{n}\to 0\). Combining this result with the second limit in the above system yields that \((\lambda_{n}^{*}-\lambda_{n})(q_{1}^{n})^{2}/\lambda_{n}+3/2\to 0\), which cannot happen. Therefore, Case 1.2.1 does not hold.

**Case 1.2.2:**: \((\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{4}>\lambda_{n}|\Delta\mu_{n}- \Delta\mu_{n}^{*}|^{4}\) for all \(n\) (by subsequence argument). If \((\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{4}\leq\lambda_{n}|\Delta v_{n} -\Delta v_{n}^{*}|^{2}\) for all \(n\), the by using the same argument as that of Case 1.2.1, we quickly achieve the contradiction. Therefore, we must have \((\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{4}>\lambda_{n}|\Delta v_{n} -\Delta v_{n}^{*}|^{2}\). Denote \((\Delta\mu_{n}-\Delta\mu_{n}^{*})=m_{1}^{n}(-\Delta\mu_{n}^{*}),(-\Delta v_{n }^{*})=m_{2}^{n}(\Delta\mu_{n}^{*})^{2}\), and \((\Delta v_{n}-\Delta v_{n}^{*})=m_{3}^{n}(\Delta\mu_{n}^{*})^{2}\). Since \(M_{n}=|\Delta\mu_{n}^{*}|\), we would have \(|m_{n}^{*}|\leq 1\) for all \(1\leq i\leq 3\). Denote \(m_{n}^{n}\to m_{i}\) for all \(1\leq i\leq 3\) (by subsequence argument). The results of (26) lead to \(m_{1}=m_{3}=0\). Now by dividing both the numerator and denominator of \(H_{\beta}\) by \((\lambda_{n}^{*}-\lambda_{n})(-\Delta\mu_{n}^{*})^{\beta}\) for any \(1\leq\beta\leq 4\), as the new denominators of \(H_{|\beta|}\) do not go to \(\infty\), we would also achieve that the new numerators of \(H_{|\beta|}\) go to 0, i.e. the following system of limits hold

\[1+\frac{\lambda_{n}^{*}-\lambda_{n}}{\lambda_{n}}m_{1}^{n}\to 0, \ \bigg{[}1+\frac{\lambda_{n}^{*}-\lambda_{n}}{\lambda_{n}}(m_{1}^{n})^{2} \bigg{]}+m_{2}^{n}+\frac{\lambda_{n}^{*}-\lambda_{n}}{\lambda_{n}}m_{3}^{n} \to 0,\] \[\bigg{(}1+\frac{\lambda_{n}^{*}-\lambda_{n}}{\lambda_{n}}(m_{1}^{ n})^{3}\bigg{)}/6+\bigg{(}m_{2}^{n}+\frac{\lambda_{n}^{*}-\lambda_{n}}{ \lambda_{n}}m_{1}^{n}m_{3}^{n}\bigg{)}/2\to 0,\] \[\bigg{(}1+\frac{\lambda_{n}^{*}-\lambda_{n}}{\lambda_{n}}(m_{1}^{ n})^{4}\bigg{)}/24+\bigg{(}m_{2}^{n}+\frac{\lambda_{n}^{*}-\lambda_{n}}{ \lambda_{n}}(m_{3}^{n})^{2}\bigg{)}/4+\bigg{(}(m_{2}^{n})^{2}+\frac{\lambda_{n} ^{*}-\lambda_{n}}{\lambda_{n}}(m_{3}^{n})^{2}\bigg{)}/8\to 0.\]

Combining with \(m_{1}^{n}\to 0\), the first and third limit of the above system of limits imply that \(m_{2}=-1/3\). From here, the second and fourth limit yields that \(1/6+m_{2}+m_{2}^{2}/2=0\), which is a contradiction. Therefore, Case 1.2.2 cannot hold.

**Case 2:**: \(\bigg{(}\lambda_{n}(|\Delta\mu_{n}|^{2}+|\Delta v_{n}|)+\lambda_{n}^{*}(| \Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|)\bigg{)}/\bigg{\{}\lambda_{n}(|\mu_{ n}-\mu_{n}^{*}|^{2}+|v_{n}-v_{n}^{*}|)\bigg{\}}\to\infty.\)

We define

\[\overline{\mathcal{Q}}(G_{n},G_{*,n}) = (\lambda_{n}^{*}-\lambda_{n})(|\Delta\mu_{n}|^{2}+|\Delta v_{n}| )(|\Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|)+\bigg{(}\lambda_{n}(|\Delta\mu_{ n}|^{2}+|\Delta v_{n}|)\] \[+ \lambda_{n}^{*}(|\Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|))\bigg{(} |\mu_{n}-\mu_{n}^{*}|^{2}+|v_{n}-v_{n}^{*}|\bigg{)}.\]

We will demonstrate that \(\mathcal{Q}(G_{n},G_{*,n})\asymp\overline{\mathcal{Q}}(G_{n},G_{*,n})\). In fact, from the above formulation of \(\overline{\mathcal{Q}}(G_{n},G_{*,n})\), we would have that

\[\overline{\mathcal{Q}}(G_{n},G_{*,n}) \leq 2(\lambda_{n}^{*}-\lambda_{n})(|\Delta\mu_{n}^{*}|^{2}+|\Delta \mu_{n}-\Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|+|\Delta v_{n}-\Delta v_{n}^{* }|)(|\Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|)\] \[+ 2\bigg{(}\lambda_{n}(|\Delta\mu_{n}|^{2}+|\Delta v_{n}|)+\lambda_ {n}^{*}(|\Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|)\bigg{)}\bigg{(}|\mu_{n}- \mu_{n}^{*}|^{2}+|v_{n}-v_{n}^{*}|\bigg{)}\] \[\leq 2\mathcal{Q}(G_{n},G_{*,n})\]

where the first inequality is due to the triangle inequality and basic inequality \((a+b)^{2}\leq 2(a^{2}+b^{2})\) and the second inequality is due to the following result

\[(\lambda_{n}^{*}-\lambda_{n})(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{ 2}+|\Delta v_{n}-\Delta v_{n}^{*}|)\leq\lambda_{n}^{*}\bigg{(}|\mu_{n}-\mu_{n} ^{*}|^{2}+|v_{n}-v_{n}^{*}|\bigg{)}\] \[\geq \mathcal{Q}(G_{n},G_{*,n})/2\]

where the last inequality is due to triangle inequality and basic inequality \((a+b)^{2}\leq 2(a^{2}+b^{2})\). Therefore, we conclude that \(\mathcal{Q}(G_{n},G_{*,n})\asymp\overline{\mathcal{Q}}(G_{n},G_{*,n})\). Now, since \(H_{\beta}\to 0\) for all \(1\leq\beta\leq 8\), we would have that

\[F_{\beta}=\frac{\sum\limits_{\alpha_{1},\alpha_{2}}\frac{(\lambda_{n}^{*}- \lambda_{n})(-\Delta\mu_{n}^{*})^{\alpha_{1}}(-\Delta v_{n}^{*})^{\alpha_{2}}+ \lambda_{n}(\Delta\mu_{n}-\Delta\mu_{n}^{*})^{\alpha_{1}}(\Delta v_{n}-\Delta v_{n} ^{*})^{\alpha_{2}}}{2^{|\alpha_{2}|}\alpha_{1}!\alpha_{2}!}}{\overline{ \mathcal{Q}}(G_{n},G_{*,n})}\to 0.\]

Similar to Case 1, under Case 2 we also consider two distincts setting of \(\lambda_{n}^{*}/\lambda_{n}\)

**Case 2.1:** \(\lambda_{n}^{*}/\lambda_{n}\not\to\infty\). Under this case, we denote

\[M_{n}^{\prime}:=\max\left\{|\Delta\mu_{n}|^{2},|\Delta v_{n}|,|\Delta\mu_{n}^{*} |^{2},|\Delta v_{n}^{*}|\right\}.\]

From the assumption of Case 2, we would have

\[|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2}/M_{n}^{\prime}\to 0,\ |\Delta v_{n}- \Delta v_{n}^{*}|/M_{n}^{\prime}\to 0.\] (27)

Due to the symmetry between \((|\Delta\mu_{n}|^{2},|\Delta v_{n}|)\) and \((|\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}^{*}|)\), we assume without loss of generality that \(M_{n}^{\prime}\in\max\left\{|\Delta\mu_{n}|^{2},|\Delta v_{n}|\right\}\). Under that assumption, we have two distinct cases

**Case 2.1.1:** \(M_{n}^{\prime}=|\Delta\mu_{n}|^{2}\) for all \(n\) (by the subsequence argument). From (27), we have \(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|/|\Delta\mu_{n}|\to 0\), i.e., \(\Delta\mu_{n}/\Delta\mu_{n}^{*}\to 1\). To be able to utilize the assumptions of Case 2, we will need to study the formulations of \(F_{\beta}\) more deeply. In fact, when \(\beta=1\) simple calculation yields

\[A_{1}:=(\lambda_{n}\Delta\mu_{n}-\lambda_{n}^{*}\Delta\mu_{n}^{*})/\overline{ \mathcal{Q}}(G_{n},G_{*,n})\to 0.\]

When \(\beta=2\), we have

\[F_{2}=\frac{(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}^{*})^{2}+\lambda_{n}( \Delta\mu_{n}-\Delta\mu_{n}^{*})^{2}+(\lambda_{n}^{*}-\lambda_{n})(-\Delta v _{n}^{*})+\lambda_{n}(\Delta v_{n}-\Delta v_{n}^{*})}{\overline{Q}(G_{n},G_{*,n})}\to 0.\]

Combining with the result of \(A_{1}\), it is clear that

\[\frac{(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}^{*})^{2}+\lambda_{n}( \Delta\mu_{n}-\Delta\mu_{n}^{*})^{2}}{\overline{\mathcal{Q}}(G_{n},G_{*,n})} \to\frac{(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}\Delta\mu_{n}^{*}}{ \overline{\mathcal{Q}}(G_{n},G_{*,n})}.\]

Combining the above result with \(F_{2}\to 0\), we would have

\[A_{2}:=\frac{(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}\Delta\mu_{n}^{*}+ \lambda_{n}\Delta v_{n}-\lambda_{n}^{*}\Delta v_{n}^{*}}{\overline{\mathcal{Q }}(G_{n},G_{*,n})}\to 0.\]

Now, we have two small cases

**Case 2.1.1.1:** \(\Delta v_{n}/(\Delta\mu_{n})^{2}\to 0\) as \(n\to\infty\). From (27), since we have \(|\Delta v_{n}-\Delta v_{n}^{*}|/(\Delta\mu_{n})^{2}\to 0\), it implies that \(\Delta v_{n}^{*}/(\Delta\mu_{n})^{2}\to 0\). Since \(\Delta\mu_{n}/\Delta\mu_{n}^{*}\to 1\), we also have that \(\Delta v_{n}^{*}/(\Delta\mu_{n}^{*})^{2}\to 0\). Now, from the formulations of \(\overline{\mathcal{Q}}(G_{n},G_{*,n})\) we have

\[(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}\Delta v_{n}^{*}|/ \overline{\mathcal{Q}}(G_{n},G_{*,n}) \leq|\Delta v_{n}|/|\Delta\mu_{n}|^{2}\to 0,\] \[(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}(\Delta\mu_{n}^{*})^{2} |/\overline{\mathcal{Q}}(G_{n},G_{*,n}) \leq|\Delta v_{n}|/|\Delta\mu_{n}|^{2}\to 0,\] \[(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}^{*}(\Delta\mu_{n})^{2} |/\overline{\mathcal{Q}}(G_{n},G_{*,n}) \leq|\Delta v_{n}^{*}|/|\Delta\mu_{n}^{*}|^{2}\to 0.\] (28)

From the result that \(A_{2}\to 0\), by multiplying \(A_{2}\) with \(\Delta\mu_{n}\Delta\mu_{n}^{*}\), we would also have that

\[\frac{(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}\Delta\mu_{n}^{*})^{2}+( \lambda_{n}\Delta v_{n}-\lambda_{n}^{*}\Delta v_{n}^{*})\Delta\mu_{n}\Delta\mu _{n}^{*}}{\overline{Q}(G_{n},G_{*,n})}\to 0.\] (29)

As \(\lambda_{n}^{*}/\lambda_{n}\not\to\infty\), we have two distinct settings of \(\lambda_{n}^{*}/\lambda_{n}\)

**Case 2.1.1.1.1:** \(\lambda_{n}^{*}/\lambda_{n}\not\to 1\). Using the result from (28) and the fact that \(\Delta\mu_{n}/\Delta\mu_{n}^{*}\to 1\), we would obtain that

\[(\lambda_{n}\Delta v_{n}-\lambda_{n}^{*}\Delta v_{n}^{*})\Delta\mu_{n}\Delta \mu_{n}^{*}/\overline{\mathcal{Q}}(G_{n},G_{*,n})\to 0.\]

Combining the above result with (29), it leads to

\[(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}\Delta\mu_{n}^{*})^{2}/\overline{ \mathcal{Q}}(G_{n},G_{*,n})\to 0.\] (30)

Combining (28) and (30), we would achieve that

\[\frac{(\lambda_{n}^{*}-\lambda_{n})(|\Delta\mu_{n}|^{2}+|\Delta v_{n}|)(| \Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|)}{\overline{\mathcal{Q}}(G_{n},G_{*,n })}\to 0.\]From the formulation of \(\overline{\mathcal{Q}}(G_{n},G_{s,n})\), the above limit implies that

\[E:=\frac{\bigg{(}\lambda_{n}(|\Delta\mu_{n}|^{2}+|\Delta v_{n}|)+ \lambda_{n}^{*}|(\Delta\mu_{n}^{*}|^{2}+|\Delta v_{n}^{*}|)\bigg{)}\bigg{(}|\mu _{n}-\mu_{n}^{*}|^{2}+|v_{n}-v_{n}^{*}|\bigg{)}}{\overline{\mathcal{Q}}(G_{n},G _{s,n})}\to 1.\]

Due to the previous assumptions, we obtain that

\[E\lesssim\frac{\max\big{\{}\lambda_{n}(\Delta\mu_{n})^{2}( \Delta\mu_{n}-\Delta\mu_{n}^{*})^{2},\lambda_{n}(\Delta\mu_{n})^{2}(\Delta v_ {n}-\Delta v_{n}^{*})\big{\}}}{\overline{\mathcal{Q}}(G_{n},G_{s,n})}.\]

By combining the results from (28) and (30), we can verify that

\[\frac{\lambda_{n}(\Delta\mu_{n})^{2}(\Delta\mu_{n}-\Delta\mu_{n} ^{*})^{2}}{\overline{\mathcal{Q}}(G_{n},G_{s,n})}\to\frac{(\lambda_{n}^{*}- \lambda_{n})\bigg{(}-(\Delta\mu_{n})^{2}(\Delta\mu_{n}^{*})^{2}+(\Delta\mu_{ n})^{3}\Delta\mu_{n}^{*}\bigg{)}}{\overline{\mathcal{Q}}(G_{n},G_{s,n})}\to 0,\] \[\frac{\lambda_{n}(\Delta\mu_{n})^{2}(\Delta v_{n}-\Delta v_{n}^{ *})}{\overline{\mathcal{Q}}(G_{n},G_{s,n})}\to\frac{(\lambda_{n}^{*}-\lambda_ {n})(\Delta\mu_{n})^{2}\Delta v_{n}^{*}}{\overline{\mathcal{Q}}(G_{n},G_{s,n} )}\to 0.\]

Therefore, we achieve \(E\to 0\), which is a contradiction. As a consequence, Case 2.1.1.1.1 cannot happen.

Case 2.1.1.1.2:\(\lambda_{n}^{*}/\lambda_{n}\to 1\). Under this case, if we have

\[\max\bigg{\{}\frac{\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{ 2}}{(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}|^{2}},\frac{\lambda_{n}|\Delta v _{n}-\Delta v_{n}^{*}|}{(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}|^{2}} \bigg{\}}\to\infty,\]

then we will achieve that

\[(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}\Delta\mu_{n}^{*})^{2 }/\overline{\mathcal{Q}}(G_{n},G_{*,n})\leq\min\bigg{\{}\frac{(\lambda_{n}^{* }-\lambda_{n})|\Delta\mu_{n}^{*}|^{2}}{\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n }^{*}|^{2}},\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}^{*}|^{2}}{ \lambda_{n}|\Delta v_{n}-\Delta v_{n}^{*}|}\bigg{\}}\to 0.\]

From here, by using the same argument as that of Case 2.1.1.1.1, we will obtain \(E\to 0\), which is a contradiction. Therefore, we would have that

\[\max\bigg{\{}\frac{\lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{ 2}}{(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}|^{2}},\frac{\lambda_{n}| \Delta v_{n}-\Delta v_{n}^{*}|}{(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}|^ {2}}\bigg{\}}\not\to\infty.\] (31)

With that assumption, it leads to \(\overline{\mathcal{Q}}(G_{n},G_{*,n})\asymp(\lambda_{n}^{*}-\lambda_{n})( \Delta\mu_{n}^{*})^{2}(\Delta\mu_{n})^{2}\asymp(\lambda_{n}^{*}-\lambda_{n})( \Delta\mu_{n}^{*})^{4}\) as \(\Delta\mu_{n}^{*}/\Delta\mu_{n}\to 1\). Now, we denote \(\Delta\mu_{n}-\Delta\mu_{n}^{*}=\tau_{1}^{1}\Delta\mu_{n}^{*}\) and \(\Delta v_{n}-\Delta v_{n}^{*}=\tau_{2}^{n}(\Delta\mu_{n}^{*})^{2}\). From the assumption of Case 2.1.1.1, we would have that \(\tau_{1}^{n}\to 0\) and \(\tau_{2}^{n}\to 0\). By dividing both the numerator and the denominator of \(F_{3}\) by \((\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}^{*})^{3}\), as the new denominators of \(F_{3}\) goes to 0, we also obtain the numerator of this term goes to 0, i.e., the following holds

\[\left\{-1+\frac{\lambda_{n}}{\lambda_{n}^{*}-\lambda_{n}}(\tau_{1} ^{n})^{3}\right\}/6+\frac{\lambda_{n}}{2(\lambda_{n}^{*}-\lambda_{n})}\tau_{1} ^{n}\tau_{2}^{n}\to 0.\]

From (31), we have that \(\lambda_{n}(\tau_{1}^{n})^{2}/(\lambda_{n}^{*}-\lambda_{n})\not\to\infty\) and \(\lambda_{n}\tau_{2}^{n}/(\lambda_{n}^{*}-\lambda_{n})\not\to\infty\). Therefore, since \(\tau_{1}^{n}\to 0\) and \(\tau_{2}^{n}\to 0\), we would achieve that \(\lambda_{n}(\tau_{1}^{n})^{3}/(\lambda_{n}^{*}-\lambda_{n})\to 0\) and \(\lambda_{n}\tau_{1}^{n}\tau_{2}^{n}/(\lambda_{n}^{*}-\lambda_{n})\to 0\). By plugging these results to the above limit, it implies that \(-1/6=0\), which is a contradiction. As a consequence, Case 2.1.1.1.2 cannot hold.

Case 2.1.1.2:\(\Delta v_{n}/(\Delta\mu_{n})^{2}\not\to 0\) as \(n\to\infty\). Under that case, we will only consider the setting that \(\lambda_{n}^{*}/\lambda_{n}\to 1\) as the argument for other settings of that ratio can be argued in the similar fashion. Since we have \(|\Delta v_{n}-\Delta v_{n}^{*}|/(\Delta\mu_{n})^{2}\to 0\), it leads to \(\Delta v_{n}^{*}/(\Delta\mu_{n})^{2}\not\to 0\). Combining with \(\Delta\mu_{n}/\Delta\mu_{n}^{*}\to 1\), it implies that as \(n\) is large enough we would have

\[\max\big{\{}(\Delta\mu_{n})^{2},(\Delta\mu_{n}^{*})^{2}\big{\}} \lesssim\min\left\{|\Delta v_{n}|,|\Delta v_{n}^{*}|\right\}.\] (32)

According the formulation of \(\overline{\mathcal{Q}}(G_{n},G_{*,n})\), we achieve

\[\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}\Delta v_{n}^{*}|}{ \overline{\mathcal{Q}}(G_{n},G_{*,n})}\leq\min\bigg{\{}\frac{(\lambda_{n}^{*}- \lambda_{n})|\Delta v_{n}^{*}|}{\lambda_{n}|\Delta v_{n}-\Delta v_{n}^{*}|}, \frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}|}{\lambda_{n}^{*}|\Delta v_{n}- \Delta v_{n}^{*}|},\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}^{*}|}{ \lambda_{n}|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2}},\] \[\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}|}{\lambda_{n}^{*}| \Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2}}\bigg{\}}=B.\]If we have \(B\to 0\), we would get \(\frac{(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}\Delta v_{n}^{*}|}{ \overline{\mathcal{Q}}(G_{n},G_{*,n})}\to 0\). Combining with (32), we can check that all the results in (28), (30), and (31) hold. With similar argument as Case 2.1.1.1, we achieve \(\overline{\mathcal{Q}}(G_{n},G_{*,n})/\overline{\mathcal{Q}}(G_{n},G_{*,n})\to 0\), a contradiction. Therefore, we must have \(B\not\to 0\). It implies that as \(n\) is large enough we must have

\[\max\left\{\lambda_{n},\lambda_{n}^{*}\right\}\max\left\{|\Delta\mu_{n}-\Delta \mu_{n}^{*}|^{2},|\Delta v_{n}-\Delta v_{n}^{*}|\right\}\lesssim(\lambda_{n}^ {*}-\lambda_{n})\min\left\{|\Delta v_{n}|,|\Delta v_{n}^{*}|\right\}.\]

Furthermore, as \((\lambda_{n}^{*}-\lambda_{n})/\lambda_{n}\to 0\), we obtain \(|\Delta v_{n}^{*}|/|\Delta v_{n}-\Delta v_{n}^{*}|\to\infty\) and \(|\Delta v_{n}^{*}|/|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2}\to\infty\), i.e., \(\Delta v_{n}/\Delta v_{n}^{*}\to 1\). With all of these results, we can check that \(\overline{\mathcal{Q}}(G_{n},G_{*,n})\lesssim(\lambda_{n}^{*}-\lambda_{n})| \Delta v_{n}^{*}|^{2}\). Denote \((\Delta v_{n}-\Delta v_{n}^{*})=k_{1}^{n}|\Delta v_{n}^{*}|\), \((\Delta\mu_{n}-\Delta\mu_{n}^{*})=k_{2}^{n}|\Delta v_{n}^{*}|^{1/2}\), and \(\Delta\mu_{n}^{*}=k_{3}^{n}|\Delta v_{n}^{*}|^{1/2}\) for all \(n\). From all the assumptions we have thus far, we get \(k_{1}^{n}\to 0\), \(k_{2}^{n}\to 0\), and \(|k_{3}^{n}|\not\to\infty\). Additionally, as \(B\not\to 0\), we further have \(\lambda_{n}|k_{1}^{n}|/(\lambda_{n}^{*}-\lambda_{n})\not\to\infty\) and \(\lambda_{n}(k_{2}^{n})^{2}/(\lambda_{n}^{*}-\lambda_{n})\not\to\infty\). By dividing both the numerator and the denominator of \(F_{3}\) and \(F_{4}\) respectively by \((\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}^{*}|^{3/2}\) and \((\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}^{*}|^{2}\), as the new denominators of \(F_{3},F_{4}\) do not go to infinity, we obtain the new numerators of these terms go to 0, i.e., the following holds

\[\bigg{\{}-(k_{3}^{n})^{3}+\frac{\lambda_{n}}{\lambda_{n}^{*}- \lambda_{n}}(k_{2}^{n})^{3}\bigg{\}}/6+\bigg{\{}k_{3}^{n}+\frac{\lambda_{n}}{ \lambda_{n}^{*}-\lambda_{n}}k_{1}^{n}k_{2}^{n}\bigg{\}}/2\to 0,\] \[\bigg{\{}(k_{3}^{n})^{4}+\frac{\lambda_{n}}{\lambda_{n}^{*}- \lambda_{n}}(k_{2}^{n})^{4}\bigg{\}}/24+\bigg{\{}-(k_{3}^{n})^{2}+\frac{ \lambda_{n}}{\lambda_{n}^{*}-\lambda_{n}}k_{1}^{n}(k_{2}^{n})^{2}\bigg{\}}/4+ \bigg{\{}1+\frac{\lambda_{n}}{\lambda_{n}^{*}-\lambda_{n}}(k_{1}^{n})^{2} \bigg{\}}/8\to 0.\]

With the assumptions with \(k_{1}^{n},k_{2}^{n}\), and \(k_{3}^{n}\), we would have

\[\frac{\lambda_{n}}{\lambda_{n}^{*}-\lambda_{n}}(k_{2}^{n})^{i}\to 0,\;\frac{ \lambda_{n}}{\lambda_{n}^{*}-\lambda_{n}}k_{1}^{n}(k_{2}^{n})^{j}\to 0,\frac{ \lambda_{n}}{\lambda_{n}^{*}-\lambda_{n}}(k_{1}^{n})^{2}\to 0\]

for any \(3\leq i\leq 4\) and \(1\leq j\leq 2\). If we denote \(k_{3}^{n}\to k_{3}\), by combining all the above results we achieve the following system of equations

\[-k_{3}^{3}/6+k_{3}/2=0,\;k_{3}^{4}/24-k_{3}^{2}/4+1/8=0,\]

which does not admit a solution, a contradiction. Hence, Case 2.1.1.2 cannot hold.

Case 2.1.2:\(M_{n}^{\prime}=|\Delta v_{n}|\) for all \(n\) (by the subsequence argument). From (27), we would have \(|\Delta v_{n}-\Delta v_{n}^{*}|/|\Delta v_{n}|\to 0\), i.e., \(\Delta v_{n}/\Delta v_{n}^{*}\to 1\), and \(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2}/|\Delta v_{n}|\to 0\). The argument under this case is rather similar to that of Case 2.1; therefore, we only sketch the key steps. By using the result that \(A_{1}\to 0\) and \(A_{2}\to 0\), we would obtain that

\[\frac{(\lambda_{n}^{*}-\lambda_{n})(\Delta\mu_{n}^{*})^{4}+\lambda_{n}(\Delta \mu_{n}-\Delta\mu_{n}^{*})^{4}}{24\overline{\mathcal{Q}}(G_{n},G_{*,n})}\to \frac{(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}\Delta\mu_{n}^{*}\bigg{[}( \Delta\mu_{n})^{2}-3\Delta\mu_{n}\Delta\mu_{n}^{*}+3(\Delta\mu_{n}^{*})^{2} \bigg{]}}{24\overline{\mathcal{Q}}(G_{n},G_{*,n})},\]

\[\frac{(\lambda_{n}^{*}-\lambda_{n})(\Delta v_{n}^{*})^{2}+\lambda_{n}(\Delta v_{n }-\Delta v_{n}^{*})^{2}}{8\overline{\mathcal{Q}}(G_{n},G_{*,n})}\to\frac{( \lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}^{*}\bigg{[}\Delta\mu_{n}\Delta v_{n}- \Delta\mu_{n}^{*}\Delta v_{n}-\Delta u_{n}\Delta v_{n}^{*}\bigg{]}}{8 \overline{\mathcal{Q}}(G_{n},G_{*,n})},\]

\[\frac{\lambda_{n}(\Delta\mu_{n}-\Delta\mu_{n}^{*})^{2}(\Delta v_{n}-\Delta v_{n}^ {*})}{4\overline{\mathcal{Q}}(G_{n},G_{*,n})}\to\frac{(\lambda_{n}^{*}-\lambda_{n })\bigg{[}\Delta\mu_{n}\Delta\mu_{n}^{*}\Delta v_{n}^{*}-\Delta\mu_{n}\Delta\mu _{n}^{*}\Delta v_{n}+\Delta v_{n}\Delta v_{n}^{*}\bigg{]}}{4\overline{ \mathcal{Q}}(G_{n},G_{*,n})}.\]

As \(F_{4}\to 0\), we equivalently have

\[A_{4} := (\lambda_{n}^{*}-\lambda_{n})\bigg{\{}\frac{\Delta\mu_{n}\Delta \mu_{n}^{*}\bigg{[}(\Delta\mu_{n})^{2}-3\Delta\mu_{n}\Delta\mu_{n}^{*}+3( \Delta\mu_{n}^{*})^{2}\bigg{]}}{24\overline{\mathcal{Q}}(G_{n},G_{*,n})}\] \[+ \frac{\Delta\mu_{n}\Delta\mu_{n}^{*}\Delta v_{n}-2(\Delta\mu_{n}^{* })^{2}\Delta v_{n}-\Delta\mu_{n}\Delta\mu_{n}^{*}\Delta v_{n}^{*}+\Delta v_{n} \Delta v_{n}^{*}}{8\overline{\mathcal{Q}}(G_{n},G_{*,n})}\bigg{\}}\to 0.\]

Under Case 2.1.2, we only consider the setting when \((\Delta\mu_{n})^{2}/\Delta v_{n}\to 0\) as other settings of this term can be argued in the similar fashion as that of Case 2.1.2. Since \(|\Delta\mu_{n}-\Delta\mu_{n}^{*}|^{2}/|\Delta v_{n}|\to 0\)we have \((\Delta\mu_{n}^{*})/\Delta v_{n}\to 0\). As \(\Delta v_{n}/\Delta v_{n}^{*}\to 1\), we also further have that \((\Delta\mu_{n}^{*})^{2}/\Delta v_{n}^{*}\to 0\) and \((\Delta\mu_{n})^{2}/\Delta v_{n}\to 0\). Therefore, we have \(\Delta\mu_{n}\Delta\mu_{n}^{*}/\Delta v_{n}\to 0\) and \(\Delta\mu_{n}\Delta\mu_{n}^{*}/\Delta v_{n}^{*}\to 0\). Now, from the formulation of \(\overline{Q}(G_{n},G_{*,n})\), we achieve

\[(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}\Delta\mu_{n}^{*}|^{2} /\overline{Q}(G_{n},G_{*,n})\leq|\Delta\mu_{n}|^{2}/|\Delta v_{n}^{*}|^{2}\to 0,\] \[(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}(\Delta\mu_{n}^{*})^{2 }|/\overline{Q}(G_{n},G_{*,n})\leq|\Delta\mu_{n}^{*}|^{2}/|\Delta v_{n}^{*}| \to 0,\] \[(\lambda_{n}^{*}-\lambda_{n})|\Delta v_{n}^{*}(\Delta\mu_{n})^{2 }|/\overline{Q}(G_{n},G_{*,n})\leq|\Delta\mu_{n}|^{2}/|\Delta v_{n}|\to 0,\] \[(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}|^{3}|\Delta\mu_{n}^{ *}|/\overline{Q}(G_{n},G_{*,n})\leq|\Delta\mu_{n}||\Delta\mu_{n}^{*}|/|\Delta v _{n}^{*}|\to 0,\] \[(\lambda_{n}^{*}-\lambda_{n})|\Delta\mu_{n}||\Delta\mu_{n}^{*}|^{ 3}/\overline{Q}(G_{n},G_{*,n})\leq|\Delta\mu_{n}||\Delta\mu_{n}^{*}|/|\Delta v _{n}|\to 0.\]

Combining these results with \(A_{4}\to 0\), we achieve \((\lambda_{n}^{*}-\lambda_{n})\Delta v_{n}\Delta v_{n}^{*}/\overline{Q}(G_{n}, G_{*,n})\to 0\). From here, we can easily verify that all the results in (31) hold. Thus, by using the same argument as that of Case 2.1.1, we would get \(\overline{\mathcal{Q}}(G_{n},G_{*,n})/\overline{Q}(G_{n},G_{*,n})\to 0\), a contradiction. As a consequence, Case 2.1.2 cannot happen.

Case 2.2:\(\lambda_{n}^{*}/\lambda_{n}\to\infty\). Remind that \(M_{n}^{\prime}=\max\left\{|\Delta\mu_{n}|^{2},|\Delta v_{n}|,|\Delta\mu_{n}^{ *}|^{2},|\Delta v_{n}^{*}|\right\}\). We can verify that \(\overline{Q}(G_{n},G_{*,n})\lesssim\lambda_{n}^{*}(M_{n}^{\prime})^{4}\). By dividing both the numerator and the denominator of \(A_{1}\) and \(A_{2}\) respectively by \(\lambda_{n}^{*}(M_{n}^{\prime})^{1/2}\) and \(\lambda_{n}^{*}M_{n}^{\prime}\), given that the new denominators go to 0 we would obtain the new numerators also go to 0, i.e., we have the following results

\[\lambda_{n}\Delta\mu_{n}^{n}/\left\{\lambda_{n}^{*}(M_{n}^{\prime})^{1/2} \right\}-\Delta\mu_{n}^{*}/(M_{n}^{\prime})^{1/2}\to 0,\]

\[\left[(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}\Delta\mu_{n}^{*}+\lambda_{n} \Delta v_{n}-\lambda_{n}^{*}\Delta v_{n}^{*}\right]/\left\{\lambda_{n}^{*}M_{n }^{\prime}\right\}\to 0.\]

Since \(\lambda_{n}/\lambda_{n}^{*}\to 0\), the first limit implies that \(\Delta\mu_{n}^{*}/M_{n}^{\prime}\to 0\). Combining this result with the second limit, we obtain \(\Delta v_{n}^{*}/M_{n}^{\prime}\to 0\). Therefore, we would have \(M_{n}^{\prime}=\max\left\{|\Delta\mu_{n}|^{2},|\Delta v_{n}|\right\}\). Without loss of generality, we assume that \(M_{n}^{\prime}=|\Delta\mu_{n}|^{2}\) as the argument for other possibility of \(M_{n}^{\prime}\) can be argued in the similar fashion. With these assumptions, \(|\Delta v_{n}-\Delta v_{n}^{*}|/|\Delta\mu_{n}|^{2}\not\to\infty\), i.e., as \(n\) is large enough we get \(|\Delta v_{n}-\Delta v_{n}^{*}|\lesssim|\Delta\mu_{n}|^{2}\). Now, we have two distinct cases

Case 2.2.1:\(\lambda_{n}^{*}\max\left\{|\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}^{*}|\right\}/( \lambda_{n}|\Delta\mu_{n}|^{2})\to\infty\). Due to this assumption, we can check that as \(n\) is large enough, \(\overline{Q}(G_{n},G_{*,n})\asymp\lambda_{n}^{*}|\Delta\mu_{n}|^{2}\max\left\{| \Delta\mu_{n}^{*}|^{2},|\Delta v_{n}^{*}|\right\}\). If \(\max\left\{|\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}^{*}|\right\}=|\Delta\mu_{n} ^{*}|^{2}\) for all \(n\), then by dividing both the numerator and denominator of \(A_{1}\) by \(\lambda_{n}^{*}\Delta\mu_{n}^{*}\), given that the new denominator of \(A_{1}\) goes to 0, its new numerator must go to 0, i.e., we have

\[\lambda_{n}\Delta\mu_{n}/(\lambda_{n}^{*}\Delta\mu_{n}^{*})\to 1,\]

which cannot hold since \(\lambda|\Delta\mu_{n}|^{2}/(\lambda_{n}^{*}|\Delta\mu_{n}^{*}|^{2})\to 0\) (assumption of Case 2.2.1) and \(|\Delta\mu_{n}|/|\Delta\mu_{n}^{*}|\to\infty\). Therefore, we must have \(\max\left\{|\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}^{*}|\right\}=|\Delta v_{n}^{*}|\) for all \(n\). By dividing both the numerator and denominator of \(A_{2}\) by \(\lambda_{n}^{*}\Delta v_{n}^{*}\), as the new denominator of \(A_{2}\) goes to 0, we would have

\[\frac{(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}\Delta\mu_{n}^{*}}{\lambda_{n}^{* }\Delta v_{n}^{*}}+\frac{\lambda_{n}\Delta v_{n}}{\lambda_{n}^{*}\Delta v_{n}^{* }}-1\to 0.\]

Since \(\frac{\lambda_{n}|\Delta v_{n}|}{\lambda_{n}^{*}|\Delta v_{n}^{*}|}\leq\frac{ \lambda_{n}|\Delta\mu_{n}|^{2}}{\lambda_{n}^{*}|\Delta v_{n}^{*}|}\to 0\) and \((\lambda_{n}^{*}-\lambda_{n})/\lambda_{n}^{*}\to 1\), the above limit shows that \(\Delta\mu_{n}\Delta\mu_{n}^{*}/\Delta v_{n}^{*}\to 1\). Since \((\Delta\mu_{n})^{2}/|\Delta v_{n}^{*}|\to\infty\), it implies that \((\Delta\mu_{n}^{*})^{2}/\Delta v_{n}^{*}\to 0\). Now, by combing the result that \(A_{1}\to 0\) and \(A_{2}\to 0\), since \(F_{3}\to 0\), we can verify that it is equivalent to

\[A_{3}:=\frac{\left[(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}\Delta\mu_{n}^{*}( \Delta\mu_{n}-2\Delta\mu_{n}^{*})\right]/3+(\lambda_{n}^{*}-\lambda_{n}) \Delta\mu_{n}^{*}\Delta v_{n}}{\overline{Q}(G_{n},G_{*,n})}\to 0.\]

By dividing both the numerator and the denominator of \(A_{3}\) by \(\lambda_{n}^{*}\Delta\mu_{n}\Delta v_{n}^{*}\), we obtain

\[\frac{\left[(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n}\Delta\mu_{n}^{*}(\Delta \mu_{n}-2\Delta\mu_{n}^{*})\right]/3+(\lambda_{n}^{*}-\lambda_{n})\Delta\mu_{n} ^{*}\Delta v_{n}}{\lambda_{n}^{*}\Delta\mu_{n}\Delta v_{n}^{*}}\to 0.\]As \((\Delta\mu_{n}^{*})^{2}/\Delta v_{n}^{*}\to 0\) and \(\Delta\mu_{n}\Delta\mu_{n}^{*}/\Delta v_{n}^{*}\to 1\), the above limit leads to \(\Delta\mu_{n}^{*}\Delta v_{n}/(\Delta\mu_{n}\Delta v_{n}^{*})\to-1/3\). Now, by studying \(A_{4}\to 0\) with the assumption that \(\overline{\mathcal{Q}}(G_{n},G_{*,n})\asymp\lambda_{n}^{*}|\Delta\mu_{n}|^{2}| \Delta v_{n}^{*}|\), we eventually get the equation \(1/24-1/12=0\), which is a contradiction. Therefore, Case 2.2.1 cannot hold.

Case 2.2.2:\(\lambda_{n}^{*}\max\left\{|\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}^{*}|\right\}/ \lambda_{n}|\Delta\mu_{n}|^{2}\not\to\infty\). Therefore, as \(n\) is large enough, we would have \(\lambda_{n}^{*}\max\left\{|\Delta\mu_{n}^{*}|^{2},|\Delta v_{n}^{*}|\right\} \lesssim(\lambda_{n}|\Delta\mu_{n}|^{2})\). Hence, we achieve under this case that \(\overline{\mathcal{Q}}(G_{n},G_{*,n})\asymp\lambda_{n}|\Delta\mu_{n}|^{4}\). Denote \(\Delta\mu_{n}^{*}=l_{1}^{n}\Delta\mu_{n}\), \(\Delta v_{n}=l_{2}^{n}(\Delta\mu_{n})^{2}\), and \(\Delta v_{n}^{*}=l_{3}^{n}(\Delta\mu_{n})^{2}\). From the assumptions of Case 2.2.2, we would have \(l_{1}^{n}\to 0\) and \(l_{3}^{n}\to 0\) while \(l_{2}^{n}\not\to\infty\). Additionally, \(\lambda_{n}^{*}\max\left\{(l_{1}^{n})^{2},|l_{3}^{n}|\right\}/\lambda_{n}\not \to 0\). By dividing the numerators and denominators of \(A_{i}\) by \(\lambda_{n}(\Delta\mu_{n})^{i}\) for \(1\leq i\leq 3\), we achieve the following system of limits

\[\frac{\lambda_{n}^{*}l_{1}^{n}}{\lambda_{n}}-1\to 0,\;\frac{(\lambda_{n}^{*}- \lambda_{n})l_{1}^{n}}{\lambda_{n}}+l_{2}^{n}-\frac{\lambda_{n}^{*}l_{3}^{n}}{ \lambda_{n}}\to 0,\;\frac{\lambda_{n}^{*}-\lambda_{n}}{\lambda_{n}}\left\{ \frac{l_{1}^{n}-(l_{1}^{n})^{2}}{3}+l_{1}^{n}l_{2}^{n}\right\}\to 0.\] (33)

As \(l_{1}^{n}\to 0\), the first limit in the above system implies that \(\lambda_{n}^{*}(l_{1}^{n})^{2}/\lambda_{n}\to 0\). If we have \(\max\left\{(l_{1}^{n})^{2},|l_{2}^{n}|\right\}=|l_{1}^{n}|^{2}\) for all \(n\), the previous result would mean that \(\lambda_{n}^{*}l_{3}^{n}/\lambda_{n}\to 0\). Therefore, the second limit in (33) demonstrates that \(l_{2}^{n}\to-1\). However, plugging these results to the third limit in this system would yield \(1/3-1=0\), which is a contradiction. Hence, we must have \(\max\left\{(l_{1}^{n})^{2},|l_{2}^{n}|\right\}=|l_{3}^{n}|\) for all \(n\). Under this setting, by denoting \(\frac{\lambda_{n}^{*}l_{3}^{n}}{\lambda_{n}}\to a\) as \(n\to\infty\), the first and second limit in (33) leads to \(l_{2}^{n}\to a-1\). With this result, the third limit in this system shows that \(a=2/3\). With these results, by dividing both the numerator and denominator of \(A_{4}\) by \(\lambda_{n}(\Delta\mu_{n})^{4}\), we quickly achieve the equation \(1/24-5/72=0\), which is a contradiction. Therefore, Case 2.2.2 cannot hold.

In sum, not all the coefficients of \(\frac{\partial^{|\beta|}f}{\partial\mu^{\beta}}(x|\mu_{n}^{*},v_{n}^{*})\) as \(1\leq|\beta|\leq 8\) go to 0. From here, by using the same argument as that of Proposition C.1 and Proposition C.3, we achieve the result of part (b) of the proposition. As a consequence, we reach the conclusion of the theorem. 

## Appendix D Proofs for Convergence Rates of Parameter Estimation and Minimax Lower Bounds

In this appendix, we provide the proofs for the convergence rates of the MLE as well as the corresponding minimax lower bounds introduced in Section D.

### Proof of Theorem 4.1

(a) For any \(G_{1}=G_{1}(\lambda_{1},\mu_{1},\Sigma_{1})\) and \(G_{2}=G_{2}(\lambda_{2},\mu_{2},\Sigma_{2})\), we denote the following distance

\[d_{1}(G_{1},G_{2}) = \lambda_{1}||(\mu_{1},\Sigma_{1})-(\mu_{2},\Sigma_{2})||,\] \[d_{2}(G_{1},G_{2}) = |\lambda_{1}-\lambda_{2}|^{2}.\]

Even though \(d_{2}(G_{1},G_{2})\) is a proper distance, it is clear that \(d(G_{1},G_{2})\) is not symmetric and only satisfies a weak triangle inequality, i.e. we have

\[d_{1}(G_{1},G_{3})+d_{1}(G_{2},G_{3})\geq\min\left\{d_{1}(G_{1},G_{2}),d_{1}(G_{ 2},G_{1})\right\}.\]

Therefore, we will utilize the modification of Le Cam method for nonsymmetric loss in Lemma 6.1 of [16] to deal with such distance. We start with the following proposition

**Proposition D.1**.: _Given that \(f\) satisfies assumption (S.1) in Theorem 4.1, we achieve for any \(r<1\) that_

1. \(\lim_{\epsilon\to 0}\inf_{G_{1}=(\lambda,\mu_{1},\Sigma_{1}),G_{2}=( \lambda,\mu_{2},\Sigma_{2})}\left\{h(p_{G_{1}},p_{G_{2}})/d_{1}^{r}(G_{1},G_{2}) :d_{1}(G_{1},G_{2})\leq\epsilon\right\}=0.\)__
2. \(\lim_{\epsilon\to 0}\inf_{G_{1}=(\lambda_{1},\mu,\Sigma),G_{2}=(\lambda_{2},\mu, \Sigma)}\left\{h(p_{G_{1}},p_{G_{2}})/d_{2}^{r}(G_{1},G_{2}):d_{2}(G_{1},G_{2}) \leq\epsilon\right\}=0.\)__Proof.: (i) For any sequences \(G_{1,n}=(\lambda_{n},\mu_{1,n},\Sigma_{1,n})\) and \(G_{2,n}=(\lambda_{n},\mu_{2,n},\Sigma_{2,n})\), we have

\[h^{2}(p_{G_{1,n}},p_{G_{2,n}}) \leq \frac{1}{\lambda_{n}}\int\frac{(p_{G_{1,n}}(x)-p_{G_{2,n}}(x))^{2 }}{f(x|\mu_{2,n},\Sigma_{2,n})}dx\] \[= \lambda_{n}\int\frac{(f(x|\mu_{1,n},\Sigma_{1,n})-f(x|\mu_{2,n}, \Sigma_{2,n}))^{2}}{f(x|\mu_{2,n},\Sigma_{2,n})}dx\]

where the first inequality is due to \(\sqrt{p_{G_{1,n}}(x)}+\sqrt{p_{G_{2,n}}(x)}>\sqrt{\lambda_{n}f(x|\mu_{2,n}, \Sigma_{2,n})}\). By Taylor expansion up to the first order, we have

\[f(x|\mu_{1,n},\Sigma_{1,n})-f(x|\mu_{2,n},\Sigma_{2,n})=\sum_{|\alpha|=1}\frac {(\mu_{1,n}-\mu_{2,n})^{\alpha_{1}}(\Sigma_{1,n}-\Sigma_{2,n})^{\alpha_{2}}}{ \alpha_{1}!\alpha_{2}!}\frac{\partial f}{\partial\mu^{\alpha_{1}}\partial \Sigma^{\alpha_{2}}}(x|\mu_{2,n},\Sigma_{2,n})\]

\[+\sum_{|\alpha|=1}\frac{(\mu_{1,n}-\mu_{2,n})^{\alpha_{1}}(\Sigma_{1,n}- \Sigma_{2,n})^{\alpha_{2}}}{\alpha_{1}!\alpha_{2}!}\int\limits_{0}^{1}\frac{ \partial f}{\partial\mu^{\alpha_{1}}\partial\Sigma^{\alpha_{2}}}(x|\mu_{2,n} +t(\mu_{1,n}-\mu_{2,n}),\Sigma_{2,n}+t(\Sigma_{1,n}-\Sigma_{2,n}))dt\]

Now, by choosing \(\lambda_{n}^{1-2r}\|(\mu_{1,n},\Sigma_{1,n})-(\mu_{2,n},\Sigma_{2,n})\|^{2-2r}\to 0\), and \(\|(\mu_{1,n},\Sigma_{1,n})-(\mu_{2,n},\Sigma_{2,n})\|\to 0\) and using condition (S.1), we can easily verify that \(h(p_{G_{1,n}},p_{G_{2,n}})/d_{1}^{r}(G_{1,n},G_{2,n})\to 0.\) Therefore, we achieve the conclusion of part (i).

(ii) The argument for this part is essentially similar to that in part (i). In fact, for any two sequences \(G_{1,n}^{\prime}=(\lambda_{1,n},\mu_{n},\Sigma_{n})\) and \(G_{2,n}^{\prime}=(\lambda_{2,n},\mu_{n},\Sigma_{n})\), we also obtain

\[\frac{h^{2}(p_{G_{1,n}^{\prime}},p_{G_{2,n}^{\prime}})}{d_{2}^{2r }(G_{1,n}^{\prime},G_{2,n}^{\prime})} \leq \frac{(\lambda_{1,n}-\lambda_{2,n})^{2-2r}}{(1-\lambda_{1,n}) \wedge\lambda_{1,n}}\int\frac{(h_{0}(x|\mu_{0},\Sigma_{0})-f(x|\mu_{n},\Sigma_ {n}))^{2}}{h_{0}(x|\mu_{0},\Sigma_{0})+f(x|\mu_{n},\Sigma_{n})}dx\] \[\leq \frac{2(\lambda_{1,n}-\lambda_{2,n})^{2-2r}}{(1-\lambda_{1,n}) \wedge\lambda_{1,n}}\]

By choosing \((\lambda_{1,n}-\lambda_{2,n})^{2-2r}/\left\{(1-\lambda_{1,n})\wedge\lambda_{1,n}\right\}\to 0\), we also achieve the conclusion of part (ii). 

Now, given \(G_{*}=(\lambda^{*},\mu^{*},\Sigma^{*})\) and \(r<1\). Let \(C_{0}\) be any fixed constant. According to part (i) of Proposition D.1, for any sufficiently small \(\epsilon>0\), there exists \(G_{*}^{\prime}=(\lambda^{*},\mu_{1}^{*},\Sigma_{1}^{*})\) such that \(d_{1}(G_{*},G_{*}^{\prime})=d_{1}(G_{*}^{\prime},G_{*})=\epsilon\) and \(h(p_{G_{*}},p_{G_{*}^{\prime}})\leq C_{0}\epsilon^{r}\). By means of Lemma 6.1 of [16], we achieve

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi}\mathbb{E}_{p_{G}}\bigg{(}\lambda^{2 }\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu,\Sigma)\|^{2}\bigg{)}\geq \frac{\epsilon^{2}}{2}\bigg{(}1-V(p_{G_{*}}^{n},p_{G_{*}^{\prime}}^{n})\bigg{)}.\]

where \(p_{G_{*}}^{n}\) denotes the density of the \(n\)-iid sample \(X_{1},\ldots,X_{n}\). From there,

\[V(p_{G_{*}}^{n},p_{G_{*}^{\prime}}^{n}) \leq h(p_{G_{*}}^{n},p_{G_{*}^{\prime}}^{n})\] \[= \sqrt{1-\big{(}1-h^{2}(p_{G_{*}},p_{G_{*}^{\prime}})\big{)}^{n}}\] \[\leq \sqrt{1-(1-C_{0}^{2}\epsilon^{2r})^{n}}.\]

Hence, we obtain

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi}\mathbb{E}_{p_{G}}\bigg{(}\lambda^{2 }\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu,\Sigma)\|^{2}\bigg{)}\geq\frac{ \epsilon^{2}}{2}\sqrt{1-(1-C_{0}^{2}\epsilon^{2r})^{n}}.\]

By choosing \(\epsilon^{2r}=\frac{1}{C_{0}^{2}n}\), we achieve

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi}\mathbb{E}_{p_{G}}\bigg{(}\lambda^{2 }\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-(\mu,\Sigma)\|^{2}\bigg{)}\geq c_{1} n^{-1/r}.\]

for any \(r<1\) where \(c_{1}\) is some positive constant. Using the similar argument, with the result of (ii) in Proposition D.1 we also immediately obtain the result \(\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi}\mathbb{E}_{p_{G}}\bigg{(}|\widehat{ \lambda}_{n}-\lambda|^{2}\bigg{)}\geq c_{2}n^{-1/r}.\) As a consequence, we reach the conclusion of part (a) of the theorem.

[MISSING_PAGE_EMPTY:34]

### Proof of Theorem a.2

(a) Similar to the proof argument of part (a) of Theorem 4.1, we define

\[d_{5}(G_{1},G_{2}) = \lambda_{1}\|(\mu_{1},\Sigma_{1})-(\mu_{2},\Sigma_{2})\|^{4},\] \[d_{6}(G_{1},G_{2}) = |\lambda_{1}-\lambda_{2}\|(\Delta\mu_{1},\Delta\Sigma_{1})\|^{4}.\]

for any \(G_{1}=G_{1}(\lambda_{1},\mu_{1},\Sigma_{1})\) and \(G_{2}=G_{2}(\lambda_{2},\mu_{2},\Sigma_{2})\). It is clear that \(d_{6}(G_{1},G_{2})\) satisfies weak triangle inequality while \(d_{5}(G_{1},G_{2})\) no longer satisfies weak triangle inequality. In particular, we have

\[d_{5}(G_{1},G_{3})+d_{5}(G_{2},G_{3})\geq\frac{\min\left\{d_{5}(G_{1},G_{2}),d_ {5}(G_{2},G_{1})\right\}}{8}.\]

A close investigation of Lemma 6.1 of [16] reveals that modified Le Cam method still works under this setting of \(d_{5}\) metric. More specifically, for any \(\epsilon>0\) the following holds

\[\inf_{\widehat{G}_{n}\in\Xi}\sup_{G\in\Xi_{2}(l_{n})}\mathbb{E}_{p_{G}}\bigg{(} d_{5}^{2}(G,\widehat{G}_{n})\bigg{)}\geq\frac{\epsilon^{2}}{128}\bigg{\{}1-V(p_{G_{1} }^{n},p_{G_{2}}^{n})\bigg{\}}\]

where \(G_{1},G_{2}\in\Xi_{2}(l_{n})\) such that \(d_{5}(G_{1},G_{2})\wedge d_{5}(G_{1},G_{2})\geq\epsilon/4\). From here, to achieve the conclusion of part (a), it suffices to demonstrate for any \(r<1\) that

1. There exists two sequences \(G_{1,n}=(\lambda_{n},\mu_{1,n},\Sigma_{1,n})\in\Xi_{2}(l_{n})\) and \(G_{2,n}=(\lambda_{n},\mu_{2,n},\Sigma_{2,n})\in\Xi_{1}(l_{n})\) such that \(d_{5}(G_{1,n},G_{2,n})\to 0\) and \(h(p_{G_{1,n}},p_{G_{2,n}})/d_{5}^{r}(G_{1},G_{2,n})\) as \(n\to\infty\).
2. There exists two sequences \(G^{\prime}_{1,n}=(\lambda_{1,n},\mu_{n},\Sigma_{n})\in\Xi_{2}(l_{n})\) and \(G^{\prime}_{2,n}=(\lambda_{2,n},\mu_{n},\Sigma_{n})\in\Xi_{1}(l_{n})\) such that \(d_{6}(G_{1,n},G_{2,n})\to 0\) and \(h(p_{G^{\prime}_{1,n}},p_{G^{\prime}_{2,n}})/d_{6}^{r}(G_{1,n},G_{2,n})\) as \(n\to\infty\).

Following the proof argument of Proposition D.1, we can quickly verify the above results. As a consequence, we reach the conclusion of part (a) of the theorem.

(b) From the discussion after Theorem 3.5, we can show that:

\[\mathcal{Q}(G,G_{*})\asymp|\lambda-\lambda^{*}|(\|\Delta\mu\|^{2}\|\Delta \Sigma\|)(\|\Delta\mu^{*}\|^{2}\|\Delta\Sigma^{*}\|)+(\|\mu-\mu^{*}\|^{2}+\| \Sigma-\Sigma^{*}\|)\|(\lambda(\|\Delta\mu\|^{2}+\|\Delta\Sigma\|)+\lambda^{*} (\|\Delta\mu^{*}\|^{2}+\|\Delta\Sigma^{*}\|)).\]

Hence, from Theorem 3.6 combining with Theorem 2.3, we have

\[\sup_{G_{*}}\mathbb{E}_{p_{G_{*}}}(\lambda^{*})^{2}(\|\widehat{ \mu}_{n}-\mu^{*}\|^{4}+\|\widehat{\Sigma}_{n}-\Sigma^{*}\|^{2})(\|\Delta\mu^{* }\|^{4}+\|\Delta\Sigma^{*}\|^{2})\lesssim\frac{\log^{2}(n)}{n}\] \[\sup_{G_{*}}\mathbb{E}_{p_{G_{*}}}[\widehat{\lambda}_{n}-\lambda^ {*}]^{2}(\|\Delta\widehat{\mu}_{n}\|^{4}\|\Delta\widehat{\Sigma}_{n}\|^{2})( \|\Delta\mu^{*}\|^{4}\|\Delta\Sigma^{*}\|^{2})\lesssim\frac{\log^{2}(n)}{n}.\]

Similar to the proof of Theorem A.1 and with the definition of \(\Xi_{2}(l_{2})\), we have

\[\mathbb{E}_{p_{G_{*}}}[\widehat{\lambda}_{n}-\lambda^{*}|^{2}(\|\Delta\widehat {\mu}_{n}\|^{4}\|\Delta\widehat{\Sigma}_{n}\|^{2})\gtrsim(\|\Delta\mu^{*}\|^{ 4}\|\Delta\Sigma^{*}\|^{2})\mathbb{E}_{p_{G_{*}}}[\widehat{\lambda}_{n}-\lambda ^{*}]^{2}\]

uniformly in \(G_{*}\in\Xi_{2}(l_{2})\). Hence,

\[\sup_{G_{*}\in\Xi_{2}(l_{2})}\mathbb{E}_{p_{G_{*}}}[\widehat{\lambda}_{n}- \lambda^{*}|^{2}(\|\Delta\mu^{*}\|^{8}\|\Delta\Sigma^{*}\|^{4})\lesssim\frac{ \log^{2}(n)}{n}.\]

As a consequence, we obtain the conclusion of the theorem.

## Appendix E Proofs for Auxiliary Results

**Lemma E.1**.: _For any \(r\geq 1\), we define_

\[D_{r}(G,G_{*}) = \lambda\|(\Delta\mu,\Delta\Sigma)\|^{r}+\lambda^{*}\|(\Delta\mu^{ *},\Delta\Sigma^{*})\|^{r}\] \[- \min\left\{\lambda,\lambda^{*}\right\}\bigg{(}\|(\Delta\mu,\Delta \Sigma)\|^{r}+\|(\Delta\mu^{*},\Delta\Sigma^{*})\|^{r}-\|(\mu,\Sigma)-(\mu^{* },\Sigma^{*})\|^{r}\bigg{)}\]

_for any \(G\) and \(G_{*}\). Then, we have \(W_{r}^{r}(G,G_{*})\asymp D_{r}(G,G_{*})\) for any \(r\geq 1\) where \(W_{r}\) is the \(r\)-th order Wasserstein distance._Proof.: Without loss of generality, we assume throughout the lemma that \(\lambda<\lambda^{*}\). Therefore, we obtain from the formulation of \(D_{r}(G,G_{*})\) that

\[D_{r}(G,G_{*})=(\lambda^{*}-\lambda)||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}+ \lambda||(\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}.\]

Direct computation of \(W_{r}^{r}(G,G_{*})\) yields three distinct cases:

Case 1:If \(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}\geq|| (\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}\), then

\[W_{r}^{r}(G,G_{*}) = \lambda||(\Delta\mu,\Delta\Sigma)||^{r}+\lambda^{*}||(\Delta\mu^{ *},\Delta\Sigma^{*})||^{r}\] \[- \min\left\{\lambda,\lambda^{*}\right\}(||(\Delta\mu,\Delta\Sigma )||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}-||(\mu,\Sigma)-(\mu^{*},\Sigma ^{*})||^{r})\] \[= D_{r}(G,G_{*}).\]

Case 2:If \(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}<|| (\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}\) and \(\lambda+\lambda^{*}\leq 1\), then

\[W_{r}^{r}(G,G_{*}) = \lambda||(\Delta\mu,\Delta\Sigma)||^{r}+\lambda^{*}||(\Delta\mu^{ *},\Delta\Sigma^{*})||^{r}\] \[= (\lambda^{*}-\lambda)||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}+ \lambda||(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*}) ||^{r}).\]

From Cauchy-Schartz's inequality, we have \(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r} \geq||(\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}\). Therefore, under Case 2 we have \(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}\asymp ||(\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}\), which directly implies that \(W_{r}^{r}(G,G_{*})\asymp D_{r}(G,G_{*})\).

Case 3:If \(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}<|| (\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}\) and \(\lambda+\lambda^{*}>1\), then

\[W_{r}^{r}(G,G_{*}) = (1-\lambda^{*})||(\Delta\mu,\Delta\Sigma)||^{r}+(1-\lambda)||( \Delta\mu^{*},\Delta\Sigma^{*})||^{r}\] \[+ (\lambda+\lambda^{*}-1)||(\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}\] \[= (\lambda^{*}-\lambda)||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}+(1- \lambda^{*})(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*} )||^{r})\] \[+ (\lambda^{*}+\lambda-1)||(\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}.\]

Since \(||(\Delta\mu,\Delta\Sigma)||^{r}+||(\Delta\mu^{*},\Delta\Sigma^{*})||^{r}\asymp ||(\mu,\Sigma)-(\mu^{*},\Sigma^{*})||^{r}\), we achieve

\[(1-\lambda^{*})(||(\Delta\mu,\Delta\Sigma)||^{r}\asymp(1-\lambda^{*})||(\mu, \Sigma)-(\mu^{*},\Sigma^{*})||^{r}.\]

Therefore, we also have \(W_{r}^{r}(G,G_{*})\asymp D_{r}(G,G_{*})\) under Case 3.

Combining the results from these cases, we reach the conclusion of the lemma. 

## Appendix F Discussion and Additional Experiments

### Parameter Changes with the Sample Size

In statistics and machine learning, researchers often want to know how many samples are enough to achieve some pre-specified \(\epsilon\) error for the estimation of parameter \(\theta\) in the fitted model. In the language of probability, we want to find an inequality such as \(E\|\widehat{\theta}_{n}-\theta\|<C*rate(n)\), where \(rate(n)\) is a decreasing sequence in \(n\) and \(C\) does not depend on \(n\). Usually, in the parametric models, we have \(rate(n)=1/\sqrt{n}\), and therefore it takes \(C^{2}/\epsilon^{2}\) samples to achieve average \(\epsilon\) error in estimation. In complex models such as hierarchical models or the multivariate deviated model that we consider in this paper, difficulties arise because of the _singularity_ and _identifiability_ of the model. For example, in Eq. (2), if \(\lambda^{*}=0\), any pair of \((\mu^{*},\Sigma^{*})\) yields the same model. Hence, when \(\lambda^{*}\approx 0\), it should be harder to estimate \((\mu^{*},\Sigma^{*})\), and researchers may need more samples to have an accurate estimation for them. Notably, we have shown, for example in Theorem 4.1, the precise dependence of the convergence rate of \((\mu^{*},\Sigma^{*})\) on the magnitude of \(\lambda^{*}\). In particular, we have

\[\mathbb{E}|\widehat{\lambda}_{n}-\lambda^{*}|\leq C\frac{\log n}{\sqrt{n}}, \quad\mathbb{E}\lambda^{*}\left\|(\widehat{\mu}_{n},\widehat{\Sigma}_{n})-( \mu^{*},\Sigma^{*})\right\|\leq C\frac{\log n}{\sqrt{n}},\]

where \(C\)_is a constant that does not depend on \(\lambda^{*},\mu^{*},\Sigma^{*}\) and \(n\)_. Hence, one can have a good estimation (with error \(\epsilon\)) for \(\lambda^{*}\) with \(C^{2}/\epsilon^{2}\) samples, while he needs \(C^{2}/(\epsilon*\lambda^{*})^{2}\) samples to achieve such a good estimation for \((\mu^{*},\Sigma^{*})\). The simulation studies in the next section will make this clearer.

### Additional Experiments for the Distinguishable Settings

We have seen in the main text that in two cases where \(\lambda^{*}\) is either fixed or decreasing with rate \(n^{-1/4}\), the convergence rate of \(\lambda^{*}\) is \(C\times n^{-1/2}\), where the constants \(C\) is the same for both cases. The convergence rate for \((\mu^{*},\Sigma^{*})\) is \(C\times n^{-1/4}\) for the latter case, which is much slower than the parametric rate in the former case. This phenomenon is quite rare for parametric models. We want to further bring readers' attention to two more extreme cases:

1. \(\lambda^{*}=0.5/n^{3/8}\) as \(n\) increases;
2. \(\lambda^{*}=0.5/n^{1/2}\) as \(n\) increases,

where we consider the same \((\mu^{*},\Sigma^{*})\) with the experiments in the main text. The convergence rate for \((\lambda,\mu,\sigma^{2})\) in both cases in the log domain can be seen in Figure 3 and Figure 4. Hence, in all cases, the rate of convergence for \(\lambda^{*}\) is always of order \(n^{-1/2}\), meanwhile, the rate for \((\mu^{*},\Sigma^{*})\) becomes slower as \(\lambda^{*}\) tends to 0 faster. From the theoretical result, when \(\lambda^{*}=0.5/n^{3/8}\), we expect the rate for \((\mu^{*},\Sigma^{*})\) to be of order \(n^{-1/8}\), which is demonstrated in Figure 3(b)&(c)). At the extreme case \(\lambda^{*}=0.5/n^{1/2}\), it is even impossible to recover \((\mu^{*},\Sigma^{*})\) as \(n\to\infty\) (cf. Figure 4(b)&(c)). This suggests practitioners collect more data when \(\hat{\lambda}_{n}\) is small to have a good estimate for \((\mu^{*},\Sigma^{*})\). Finally, in the case \(\hat{\lambda}_{n}\) is extremely small (of order \(n^{-1/2}\)), we suggest not to report the estimated values \((\widehat{\mu}_{n},\widehat{\Sigma}_{n})\), as they are highly uncertain.

### Non-distinguishable Settings

Finally, we consider the weakly identifiable and non-distinguishable setting here to demonstrate that the convergence rate for \(\lambda^{*}\) can be slower than the parametric rate when \(f\) is near \(h_{0}\). Let both \(h_{0}\) and \(f\) belong to the location-scale Gaussian family. \(h_{0}(x)=f(x|0,1)\) and consider two cases of \((\lambda^{*},\mu^{*},\Sigma^{*})\):

1. \(\lambda^{*}=0.25,\mu^{*}=0.\) are fixed and \(\sigma^{*}=1+n^{-1/8}\) as \(n\) increases;
2. \(\lambda^{*}=0.25,\sigma^{*}=1.\) are fixed and \(\mu^{*}=n^{-1/8}\) as \(n\) increases;Recall that we have proved that \(\left\{\|\Delta\mu^{*}\|^{4}+\|\Delta\Sigma^{*}\|^{2}\right\}|\widehat{\lambda}_{n }-\lambda^{*}|=\mathcal{O}(n^{-1/2})\) and \(\lambda^{*}(\|\Delta\mu^{*}\|^{2}+\|\Delta\widehat{\mu}_{n}\|^{2}+\|\Delta \Sigma^{*}\|+\|\Delta\widehat{\Sigma}_{n}\|)(\|\widehat{\mu}_{n}-\mu^{*}\|^{2}+ \|\widehat{\Sigma}_{n}-\Sigma^{*}\|)=\mathcal{O}(n^{-1/2})\), where there is a mismatch in the orders of convergence rates of the location and scale parameter. Notably, the rate of convergence for \(\lambda^{*}\) also depends on the rate \(\Delta\mu^{*}\) and \(\Delta(\sigma^{*})^{2}\to 0\). The experiments do support this theoretical finding, where we have the rate for \(\lambda^{*}\) is \(\approx n^{-1/4}\) is the first case (as \(\left\|\Delta(\sigma^{*})^{2}\right\|^{2}=O(n^{-1/4})\)) and it does not convergence in the second case \(\lambda^{*}\) is \(\approx n^{-1/4}\) is the first case (as \(\left\|\Delta\mu^{*}\right\|^{4}=O(n^{-1/2})\)). The mismatch rate for \(\|\widehat{\mu}_{n}-\mu^{*}\|\) and \(\|\widehat{\sigma}_{n}^{2}-(\sigma^{*})^{2}\|\) can also be seen clearly in the second case, where the rate for the scale parameter is still of the parametric rate, whereas it is slower for the mean.