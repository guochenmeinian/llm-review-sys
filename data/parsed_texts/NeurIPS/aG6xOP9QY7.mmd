# Optimal Unbiased Randomizers for

Regression with Label Differential Privacy

 Ashwinkumar Badanidiyuru

Google

Mountain View, CA

&Badih Ghazi

Google Research

Mountain View, CA

&Pritish Kamath

Google Research

Mountain View, CA

&Ravi Kumar

Google Research

Mountain View, CA

&Ethan Leeman

Google

Cambridge, MA

&Pasin Manurangsi

Google Research

Bangkok, Thailand

&Avinash V Varadarajan

Google

Mountain View, CA

&Chiyuan Zhang

Google Research

Mountain View, CA

###### Abstract

We propose a new family of label randomizers for training _regression_ models under the constraint of label differential privacy (DP). In particular, we leverage the trade-offs between bias and variance to construct better label randomizers depending on a privately estimated prior distribution over the labels. We demonstrate that these randomizers achieve state-of-the-art privacy-utility trade-offs on several datasets, highlighting the importance of reducing bias when training neural networks with label DP. We also provide theoretical results shedding light on the structural properties of the optimal unbiased randomizers.

## 1 Introduction

Differential privacy (DP) [14] has gained significant importance in recent years as a mathematically sound metric for rigorously quantifying the potential disclosure of personal user information through ML models [1, 1, 13]. DP guarantees that the model generated by the training process remains statistically indistinguishable, even if the data contributed by any individual user to the training dataset is modified arbitrarily.

In certain scenarios, the training _features_ of a specific example are already accessible to potential adversaries, while only the training _label_ is considered sensitive. For instance, within the context of digital advertising, it is typical to train conversion models that aim to predict whether a user, who interacts with an advertisement on a publisher's website, will ultimately make a purchase of the advertised item on the advertiser's website. The conversion label, which indicates whether the user completed the purchase or not, is not initially known to the publisher website and is considered sensitive information that spans multiple websites.1 This particular setting, referred to as label DP, was initially studied in the work of [1]. Subsequently, it has garnered attention in various recent works, such as [1, 1, 2].

Footnote 1: Given the announcement of various web browsers and mobile platforms regarding the deprecation of third-party cookies, which were previously employed to track user behavior across different websites, the need for training models that protect the privacy of labels has become increasingly important.

For regression objectives (such as the squared loss and the Poisson log loss), the state-of-the-art is given by the prior work of [13]. Their algorithm works in the so-called _features-oblivious_ setting (Figure 1), which consists of a _features party_ that has access to all the features, and a _labels party_ that has access to all the labels. The labels party applies a mechanism that is DP with respect to the labels in order to produce the message \(M(\cdot)\) that it sends to the features party, who then uses thefeatures along with the incoming message in order to train the model (which as a consequence would also satisfy label DP). It proceeds by using part of the privacy budget in order to estimate a prior over the labels, and then constructs a randomizer optimizing the "noisy label loss" (see Definition4) with respect to this prior.

In this work, we introduce a novel mechanism with bias-limiting constraints, motivated by the theory of bias-variance trade-offs. We show that while these constraints lead to significantly higher noisy label loss, the models trained on the privatized labels performs surprisingly well, achieving the state-of-the-art utility-privacy trade-off on multiple real world datasets.

Organization.Section2 provides some background definitions related to (label-)DP and learning. In Section3, we present the new label DP randomizers obtained by imposing unbiasedness constraints. Section4 provides an experimental evaluation demonstrating that our method achieves state-of-the-art privacy-utility trade-offs across multiple datasets. We provide some related work in Section5. We conclude with some future directions in Section6. All missing proofs along with additional related work, as well as details on the experimental evaluation are provided in the Appendix.

## 2 Preliminaries

Let \(\mathcal{D}\) be an unknown distribution on \(\mathcal{X}\times\mathcal{Y}\). We consider the regression setting where \(\mathcal{Y}\subseteq\mathbb{R}\); let \(\mathcal{P}\) denote the marginal distribution of \(\mathcal{D}\) on \(\mathcal{Y}\). In supervised learning, we have a set \(\{(x,y)\}\) of examples drawn from \(\mathcal{D}\). The goal is to learn a predictor \(f_{\theta}\) to minimize the expected loss \(\mathcal{L}_{\mathcal{D}}(f_{\theta}):=\mathbb{E}_{(x,y)\sim\mathcal{D}}\, \ell(f_{\theta}(x),y)\), for some loss function \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}\). Some commonly-used loss functions used in regression tasks are the _Poisson log loss_\(\ell_{\mathrm{Poi}}(\tilde{y},y):=\tilde{y}-y\cdot\log(\tilde{y})\) and the _squared loss_\(\ell_{\mathrm{sq}}(\tilde{y},y):=\frac{1}{2}(\tilde{y}-y)^{2}\).

We recall the definition of DP; for more background, see the book of Dwork and Roth [14].

**Definition 1** (Dp; [13]).: Let \(\varepsilon\in\mathbb{R}_{>0}\). A randomized algorithm \(\mathcal{A}\) is said to be _\(\varepsilon\)-differentially private_ (denoted \(\varepsilon\)-DP) if for any two "adjacent" input datasets \(X\) and \(X^{\prime}\), and any measurable subset \(S\) of outputs of \(\mathcal{A}\), it holds that \(\Pr[\mathcal{A}(X)\in S]\leq e^{\varepsilon}\cdot\Pr[\mathcal{A}(X^{\prime}) \in S]\).

In the context of supervised learning, an algorithm produces a model as its output, while the labeled training set serves as the input. Two inputs are considered "adjacent" if they differ on a single training example. This concept of adjacency is intended to safeguard both the features and the label of any individual example. However, in certain scenarios, protecting the features may either be unnecessary or infeasible, and the focus is solely on protecting the labels. This leads to the following definition:

**Definition 2** (Label DP; [14]).: A randomized training algorithm \(\mathcal{A}\) is said to be _\(\varepsilon\)-label differentially private_ (denoted \(\varepsilon\)-LabelDP) if it is \(\varepsilon\)-DP when two input datasets are "adjacent" if they differ on the label of a single training example.

We recall the notion of feature-oblivious label DP [11]. In this scenario, there are two parties: the _features party_ and the _labels party_. The features party has the sequence \((x_{i})_{i=1}^{n}\) of all feature vectors across the \(n\) users; the labels party has the sequence \((y_{i})_{i=1}^{n}\) of the corresponding labels. The labels party sends a single (possibly randomized) message \(M(y_{1},\dots,y_{n})\) to the features party. Based on its input and the received message, the features party generates an ML model as its output.

**Definition 3** (Feature-Oblivious Label DP [11]).: In the above scenario, the output of the features party satisfies _feature-oblivious_\(\varepsilon\)-LabelDP if the message \(M(y_{1},\dots,y_{n})\) is \(\varepsilon\)-DP where two inputs are considered "adjacent" if they differ on a single \(y_{i}\).

Figure 1: Feature-oblivious label DP model training studied in [11].

Label-DP Randomizers

A standard recipe for learning with label DP is to: (i) compute noisy labels using a local DP randomizer \(\mathcal{M}\) and (ii) use a learning algorithm on the dataset with noisy labels. Many of the baseline algorithms that we consider follow this recipe, through different ways of generating noisy labels, such as, (a) (continuous/discrete) Laplace mechanism, (b) (continuous/discrete) staircase mechanism, etc. (see Appendix F for formal definitions). Prior work by [6] argues that intuitively such a learning algorithm will be most effective when the noisy label "mostly agrees" with the true label. This was formalized in the goal of minimizing the _noisy label loss_, which is the expected loss between the true label and the noisy label, for the true label drawn from a prior distribution, for some loss function \(\ell\).

**Definition 4** (Noisy label loss).: For a loss function \(g:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}\), the _noisy label loss_ of a randomizer \(\mathcal{M}\) with respect to prior \(\mathcal{P}\) is \(\mathcal{G}(\mathcal{M};\mathcal{P}):=\mathbb{E}_{y\sim\mathcal{P},\hat{g} \sim\mathcal{M}(y)}\,g(\hat{y},y)\), where \(\hat{y}\) is the noisy label generated by \(\mathcal{M}\) on input \(y\).

This was motivated by the triangle inequality [6, Eq. (1)], namely,

\[\mathop{\mathbb{E}}_{(x,y)\sim\mathcal{D}}\ell(f_{\theta}(x),y)\ \leq\ \mathop{ \mathbb{E}}_{\begin{subarray}{c}y\sim\mathcal{D}\\ \hat{y}\sim\mathcal{M}(y)\end{subarray}}\ell(\hat{y},y)\ +\ \mathop{ \mathbb{E}}_{\begin{subarray}{c}(x,y)\sim\mathcal{D}\\ \hat{y}\sim\mathcal{M}(y)\end{subarray}}\ell(f_{\theta}(x),\hat{y})\,.\] (1)

The work of [6] focused on optimizing noisy label loss; they showed that the optimal randomizer takes the form of "Randomized-Response on Bins" (see Appendix F for more details). However, if we notice carefully, the noisy label loss does not depend on the number of examples, so the RHS of (1) does not go to zero as the number of examples goes to infinity, even though we would like the excess population loss of the learnt predictor to asymptotically go to zero. In this paper, we provide an alternative insight into the goal of minimizing the noisy label loss.

For any distribution \(\mathcal{D}\) over \(\mathcal{X}\times\mathcal{Y}\), and any label randomizer \(\mathcal{M}\) mapping \(\mathcal{Y}\) to \(\hat{\mathcal{Y}}\subseteq\mathbb{R}\), let \(\mathcal{D}_{\mathcal{M}}\) be the distribution over \(\mathcal{X}\times\hat{\mathcal{Y}}\) sampled as \((x,\mathcal{M}(y))\) for \((x,y)\sim\mathcal{D}\). The _Bayes optimal predictor_ for any distribution \(\mathcal{D}\) over \(\mathcal{X}\times\mathcal{Y}\) w.r.t. loss \(\ell\), is a predictor \(f_{\mathcal{D}}^{*}:\mathcal{X}\to\mathbb{R}\) that minimizes \(\mathcal{L}_{\mathcal{D}}(f)\), which roughly corresponds to the best predictor we could hope to learn with unbounded number of samples from \(\mathcal{D}\). We show that, when training with a loss from a broad family that includes \(\ell_{\text{sq}}\) and \(\ell_{\text{Poi}}\), the Bayes optimal predictor is preserved after applying the label randomizer iff \(\mathbb{E}[\mathcal{M}(y)]=y\) holds for all \(y\in\mathcal{Y}\) (i.e., the randomizer is _unbiased_).

**Theorem 5**.: _Suppose \(\ell\) is a loss function such that for all distributions \(\mathcal{P}\) over \(\mathcal{Y}\), the minimizer \(\hat{y}_{*}:=\min_{\hat{y}\in\mathbb{R}}\mathbb{E}_{y\sim\mathcal{P}}\ell(\hat {y},y)\) exists and is given as \(\hat{y}_{*}=\mathbb{E}_{y\sim\mathcal{P}}[y]\). Then the following are equivalent:_

* \(\mathbb{E}[\mathcal{M}(y)]=y\) _holds for all_ \(y\in\mathcal{Y}\)_,_
* _For all distributions_ \(\mathcal{D}\) _over_ \(\mathcal{X}\times\mathcal{Y}\)_, it holds that_ \(f_{\mathcal{D}}^{*}=f_{\mathcal{D}_{\mathcal{M}}}^{*}\)_._

The main observation is that \(f_{\mathcal{D}}^{*}(x_{0})=\mathbb{E}[y\mid x=x_{0}]\) with probability \(1\) over \(x_{0}\), and similarly, \(f_{\mathcal{D}_{\mathcal{M}}}^{*}(x_{0})=\mathbb{E}[\mathcal{M}(y)\mid x=x_{0}]\) with probability \(1\) over \(x_{0}\). We defer the full proof to Appendix A, but for now we demonstrate a simple example where a predictor learned using noisy labels produced by the optimal \(\mathsf{RR}\)-on-\(\mathsf{Bins}\) randomizer can in fact be sub-optimal. Let \(\mathcal{X}=\{\mathtt{a},\mathtt{b}\}\) and \(\mathcal{Y}=\{0,1,2\}\) and the distribution \(\mathcal{D}\) be defined in Table 1. The Bayes optimal predictor for \(\mathcal{D}\) w.r.t. \(\ell_{\text{sq}}\) (or even \(\ell_{\text{Poi}}\)) is given as \(f_{\mathcal{D}}^{*}(\mathtt{a})=0.4\) and \(f_{\mathcal{D}}^{*}(\mathtt{b})=0.7\). On the other hand, the optimal \(\mathcal{M}=\mathsf{RR}\)-on-\(\mathsf{Bins}_{e}^{\Phi}\) randomizer for \(\varepsilon=0.5\) (see Appendix F for notation), corresponds to the map \(\Phi(0)\approx 0.396\) and \(\Phi(1)=\Phi(2)\approx 0.720\). The Bayes optimal predictor for \(\mathcal{D}_{\mathcal{M}}\) is given as \(f_{\mathcal{D}_{\mathcal{M}}}^{*}(\mathtt{a})=0.542\) and \(f_{\mathcal{D}_{\mathcal{M}}}^{*}(\mathtt{b})=0.558\). The squared loss of these predictors are given as \(\mathcal{L}_{\mathcal{D}}(f_{\mathcal{D}}^{*})=2.625\) and \(\mathcal{L}_{\mathcal{D}}(f_{\mathcal{D}_{\mathcal{M}}}^{*})=2.726\), the latter being sub-optimal.

Thus, preserving the Bayes optimal predictor is a desirable property of a label randomizer, as any learning algorithm that converges to the optimal predictor can also be trained on the noisy labels and will approach the predictor for the original distribution.

Additionally, we can relate the property \(\mathbb{E}[\mathcal{M}(y)]=y\) to the unbiasedness of the gradients obtained in SGD. First, we observe that the gradient with respect to the parameters for \(\ell_{\text{sq}}\) and \(\ell_{\text{Poi}}\) are affine in \(y\), namely,

\[\nabla_{\theta}\ell_{\text{sq}}(f_{\theta}(x),y)=(f_{\theta}(x)-y)\cdot\nabla_{ \theta}f_{\theta}(x)\,,\qquad\nabla_{\theta}\ell_{\text{Poi}}(f_{\theta}(x),y)=( 1-y/f_{\theta}(x))\cdot\nabla_{\theta}f_{\theta}(x).\]Thus, the error in the gradient estimate when using the noisy label \(\hat{y}\) instead of \(y\) is given as

\[\nabla_{\theta}\ell_{\text{sq}}(f_{\theta}(x),\hat{y})-\nabla_{ \theta}\ell_{\text{sq}}(f_{\theta}(x),y) = (y-\hat{y})\cdot\nabla_{\theta}f_{\theta}(x),\] \[\nabla_{\theta}\ell_{\text{Poi}}(f_{\theta}(x),\hat{y})-\nabla_{ \theta}\ell_{\text{Poi}}(f_{\theta}(x),y) = (y-\hat{y})\cdot\frac{\nabla_{\theta}f_{\theta}(x)}{f_{\theta}(x)}.\]

Let \(S=\{(x_{1},y_{1}),\ldots,(x_{b},y_{b})\}\) be a mini-batch of examples and let \(\hat{S}=\{(x_{1},\hat{y}_{1}),\ldots,(x_{b},\hat{y}_{b})\}\) be the mini-batch with noisy labels as returned by the randomizer \(\mathcal{M}\). Also consider the dataset with expected noisy labels \(\tilde{S}=\{(x_{1},\tilde{y}_{1}),\ldots,(x_{b},\tilde{y}_{b})\}\), where \(\tilde{y}_{i}=\mathbb{E}_{\hat{y}\sim\mathcal{M}(y)}\,\hat{y}\). The difference between a mini-batch gradient w.r.t. noisy labels and the gradient of the population loss decomposes for \(\ell_{\text{sq}}\) as follows (similar decomposition holds for \(\ell_{\text{Poi}}\)):

\[\nabla_{\theta}\mathcal{L}_{\hat{S}}(f_{\theta})-\nabla_{\theta} \mathcal{L}_{\mathcal{D}}(f_{\theta})\] \[= \nabla_{\theta}\mathcal{L}_{S}(f_{\theta})-\nabla_{\theta} \mathcal{L}_{\mathcal{D}}(f_{\theta})\ +\ \nabla_{\theta}\mathcal{L}_{\tilde{S}}(f_{\theta})-\nabla_{\theta} \mathcal{L}_{S}(f_{\theta})\ +\ \nabla_{\theta}\mathcal{L}_{\tilde{S}}(f_{\theta})-\nabla_{ \theta}\mathcal{L}_{\tilde{S}}(f_{\theta})\] \[= \underbrace{\nabla_{\theta}\mathcal{L}_{S}(f_{\theta})-\nabla_{ \theta}\mathcal{L}_{\mathcal{D}}(f_{\theta})}_{(a)}\ +\ \underbrace{(\mathop{\mathbb{E}}_{(x,y)\in S}(y- \mathbb{E}\,\hat{y})\cdot\nabla_{\theta}f_{\theta}(x)}_{(b)}\ +\ \underbrace{(\mathop{\mathbb{E}}_{(x,y)\in S}( \mathbb{E}\,\hat{y}-\hat{y})\cdot\nabla_{\theta}f_{\theta}(x)}_{(c)}.\]

The term \((a)\) is the difference between the mini-batch gradient and the population gradient, the inherent stochasticity in mini-batch SGD, which has zero bias. The terms \((b)\) and \((c)\) form precisely the bias-variance decomposition for the additional stochasticity in the gradient introduced by the label noise, with the term \((c)\) having zero bias. In the case of stochastic convex optimization, it is well known that with access to biased gradients, it is impossible to achieve vanishingly small excess loss; whereas, with access to gradients with zero bias and any finite variance, it is possible to achieve an arbitrarily small excess loss using sufficient number of steps of stochastic gradient descent (see Appendix D for more details). Motivated by this reasoning, our approach is to use a randomizer that has the _smallest variance possible while ensuring zero bias_, namely the term \((b)\) is zero.

### Computing Optimal Unbiased Randomizers

We consider a label randomizer that minimizes the noisy label loss, while being unbiased. Namely, we say that an \(\varepsilon\)-DP randomizer \(\mathcal{M}\) that maps the label set \(\mathcal{Y}\subseteq\mathbb{R}\) to \(\mathbb{R}\) is _unbiased_ if it satisfies \(\mathbb{E}_{\hat{y}\sim\mathcal{M}(y)}\,\hat{y}=y\) for all input labels \(y\in\mathcal{Y}\). We use a linear programming (LP) based algorithm (Algorithm 1) to compute an unbiased randomizer that minimizes the noisy label loss. In order to keep this approach computationally tractable we require that both \(\mathcal{Y}\) and \(\hat{\mathcal{Y}}\) are finite; as we discuss shortly, it is possible to handle general \(\mathcal{Y}\) by discretization using randomized rounding, and moreover the excess noisy label loss due to consideration of a discrete \(\hat{\mathcal{Y}}\) can be bounded as well. Even though Algorithm 1 is general, we will only consider \(g=\ell_{\text{sq}}\) henceforth (irrespective of the training loss \(\ell\)).

``` Parameters: Privacy parameter \(\varepsilon\geq 0\). Input:\(\mathcal{P}=(p_{y})_{y\in\mathcal{Y}}\)--prior over input labels \(\mathcal{Y}\), \(\hat{\mathcal{Y}}=(\hat{y}_{i})_{i\in\mathcal{I}}\)--a finite sequence of potential output labels. Output: An \(\varepsilon\)-DP label randomizer. ```

**Algorithm 1**ComputeOptUnbiasedRand\({}_{\varepsilon}\)

In Figure 2, we illustrate a prior \(\mathcal{P}\) over \(\mathcal{Y}=\{0,1,2\}\), using the example in Table 1, and the corresponding unbiased randomizer returned by \(\texttt{ComputeOptUnbiasedRand}_{\varepsilon=0.5}(\mathcal{P},\hat{ \mathcal{Y}})\) for a certain fine-grained choice of \(\hat{\mathcal{Y}}\). It is worth noting that this randomizer is quite unlike "randomized response" or any additive noise mechanism! Additionally, the output labels are all outside \([0,2]\), which is the convex hull of \(\mathcal{Y}\). We have observed the same in our experiments in Section4 as well. One may find it surprising that these "out of domain" noisy labels in fact yield better trained models! This could be attributed to the Bayes optimal predictor for this randomizer being precisely \(f_{\mathcal{D}}^{*}\), since \(\mathbb{E}[\mathcal{M}(y)]=y\) for all \(y\in\mathcal{Y}\) (Theorem5).

There are however a couple of challenges to be addressed. First, it is unclear if the optimal unbiased randomizer has only finitely many labels, and even if so, what is the number of distinct output labels. Secondly, one needs to fix the choice of \(\hat{\mathcal{Y}}\) before hand, and it is unclear how one can choose it in a way that the LP is feasible and moreover, the solution is close to the optimal unbiased randomizer. We address these challenges as follows.

Structure of optimal unbiased randomizers.Addressing the first challenge, we show that there is an optimal unbiased randomizer that has a small number of distinct output labels.

**Theorem 6**.: _For any convex loss \(g\), all distributions \(P\) over \(\mathbb{R}\) with finite support, and \(\varepsilon>0\), there exists an \(\varepsilon\)-\(\mathsf{DP}\) unbiased randomizer \(\mathcal{M}_{*}\in\inf_{\varepsilon\text{-}\mathsf{DP}\text{ unbiased }\mathcal{M}}\tilde{\mathcal{G}}(\mathcal{M};\mathcal{P})\) with at most \(2|\mathcal{Y}|\) output labels._

The main ideas in the proof are as follows: we use the convexity of \(g\) to show that an optimal \(\varepsilon\)-\(\mathsf{DP}\) unbiased randomizer \(\mathcal{M}\) must be of a certain form; if not we can use Jensen's inequality to construct another randomizer \(\mathcal{M}^{\prime}\) with that form such that \(\mathcal{G}(\mathcal{M}^{\prime};P)\leq\mathcal{G}(\mathcal{M};\mathcal{P})\). In particular, we show that the randomizer must be a _staircase mechanism_ as defined in [11], a mechanism that maximally satisfies the DP constraints. This already implies that the number of output labels is at most \(2^{|\mathcal{Y}|}\). We use the ordering of the labels, along with similar reductions using convexity of \(g\), to show that the optimal \(\varepsilon\)-\(\mathsf{DP}\) unbiased randomizer further lies within a special subset of staircase mechanisms with \(2|\mathcal{Y}|\) output labels. The detailed proof is provided in AppendixB.

Choosing \(\hat{\mathcal{Y}}\) to ensure feasibility and good coverage.To address the second challenge, we use a finite set of output labels. We show an upper bound on the excess noisy label loss due to discretization.

We use a heuristic (Algorithm2) for setting \(\hat{\mathcal{Y}}\) to be a grid, in a way that ensures that the LP in Algorithm1 has a feasible solution while keeping \(\hat{\mathcal{Y}}\) small enough to be able to efficiently solve the LP. To compute the endpoints of the grid, we use an unbiased randomizer with a finite and bounded support: the \(\varepsilon\)-\(\mathsf{DP}\)_debiased randomized response_ (\(\varepsilon\)-\(\mathsf{dbRR}\)) on the set of labels, which operates by mapping the inputs to a unique set of values such that under randomized response the randomizer is unbiased (see AppendixF for a definition). We choose the endpoints of our grid to be precisely the minimum and maximum among the possible outputs of \(\varepsilon\)-\(\mathsf{dbRR}\) (see Algorithm2 for details). With those two endpoints defined, we create the grid by evenly spacing output labels along this interval. The number of output labels we generate is as large as possible while maintaining that the LP solver terminates in a reasonable amount of time. We show that having these endpoints suffices to ensure feasibility of the LP in Algorithm1.

**Proposition 7**.: _If \(\hat{\mathcal{Y}}\) contains just the smallest and largest values among the outputs of \(\varepsilon\)-\(\mathsf{dbRR}\), the LP in \(\mathsf{ComputeOptUnbiasedRand}_{\varepsilon}(P,\hat{\mathcal{Y}})\) is feasible._

Figure 2: (a) An example prior \(\mathcal{P}\) over \(\mathcal{Y}=\{0,1,2\}\). (b) The optimal unbiased randomizer returned by \(\mathsf{ComputeOptUnbiasedRand}_{\varepsilon=0.5}(\mathcal{P},\hat{ \mathcal{Y}})\) using a fine grid for \(\hat{\mathcal{Y}}\), indicated by the probability mass function for each input label.

We defer the proof to Appendix F; the main idea is that these endpoints create an interval that contains the interval one would obtain from the debiased randomized response on the two label set of the minimum label and maximum label. Because the \(\hat{\mathcal{Y}}\) we choose has labels less than this minimum and larger than this maximum, all labels in between can be generated by interpolation.

Additionally, we show that using the randomizer optimized on the grid \(\hat{\mathcal{Y}}\) gives bounded excess noisy label loss compared to optimizing over a continuous set of labels for Lipschitz loss functions.

**Lemma 8**.: _Let \(\mathcal{M}\) be the optimal unbiased randomizer with \(\hat{\mathcal{Y}}=[L,U],\) and let \(\mathcal{M}_{\Delta}\) be the optimal unbiased randomizer with \(\hat{\mathcal{Y}}=\{L,L+\Delta,\ldots,U-\Delta,U\}\). If \(g(\hat{y},y)\) is \(K\)-Lipschitz in \(\hat{y}\), then \(\mathcal{G}(\mathcal{M}_{\Delta};\mathcal{P})\leq\mathcal{G}(\mathcal{M}; \mathcal{P})+K\Delta\)._

In the case of \(g(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^{2}\), we have \(K=(U-L).\) Therefore, as the grid gets finer, the excess noisy label loss of \(\mathcal{M}_{\Delta}\) obtained from 1 over the noisy label loss of \(\mathcal{M}\) scales linearly in the discretization parameter \(\Delta\). We present the proof in Appendix E.

For large \(\varepsilon\) values, we experimentally observe that the optimal unbiased randomizer appears to approach \(\varepsilon\)-\(\mathsf{bR}\mathsf{R}\). For small \(\varepsilon\) values, we also experimentally observe that the optimal unbiased randomizer appears to be supported on the labels of the debiased randomized response on the two label set of the minimum label and the maximum label. This justifies our choice of endpoints for grid.

``` Parameters: Privacy parameter \(\varepsilon\geq 0\). Input: Set of input labels \(\mathcal{Y}\). Positive integer \(n\geq 2\) representing size of output \(|\hat{\mathcal{Y}}|\). Output: Set of output values \(\hat{\mathcal{Y}}\) that guarantee feasibility of LP in Algorithm 1. \(L\leftarrow\left((e^{\varepsilon}+|\mathcal{Y}|-1)\cdot\min(\mathcal{Y})-\sum_ {y\in\mathcal{Y}}y\right)/(e^{\varepsilon}-1)\) \(U\leftarrow\left((e^{\varepsilon}+|\mathcal{Y}|-1)\cdot\max(\mathcal{Y})-\sum _{y\in\mathcal{Y}}y\right)/(e^{\varepsilon}-1)\) \(\Delta\leftarrow(U-L)/(n-1)\) return\(\hat{\mathcal{Y}}=(L,L+\Delta,L+2\Delta,\ldots,U-\Delta,U)\) ```

**Algorithm 2**\(\mathsf{FeasibleOutputSet}_{\varepsilon}\)

**Splitting budget between prior and label.** So far, we have assumed a known prior \(P\) over input labels \(\mathcal{Y}\). However, this is typically not the case, and so we use the standard Laplace mechanism to privately estimate the prior. Given \(n\) samples drawn from \(P\), \(\mathcal{M}_{\varepsilon}^{\mathrm{Lap}}\) constructs a histogram over \(\mathcal{Y}\) and adds Laplace noise with scale \(2/\varepsilon\) to each entry, followed by clipping (to ensure that entries are non-negative) and normalization. For completeness, we include a formal description of \(\mathcal{M}_{\varepsilon}^{\mathrm{Lap}}\) in Appendix C.

Our randomizer for the unknown prior case, described in Algorithm 3, thus operates by splitting the privacy budget into \(\varepsilon_{1},\varepsilon_{2}\), using \(\mathcal{M}_{\varepsilon_{1}}^{\mathrm{Lap}}\) to get an approximate prior distribution \(P^{\prime}\), and using the randomizer returned by \(\mathsf{ComputeOptUnbiasedRand}_{\varepsilon_{2}}(\mathcal{P}^{\prime},\hat{ \mathcal{Y}})\) to randomize the labels. It follows that the entire algorithm is \((\varepsilon_{1}+\varepsilon_{2})\)-\(\mathsf{DP}\).

Handling continuous \(\boldsymbol{\mathcal{Y}}\).While we focused on the case of finite sets \(\mathcal{Y}\), the approach can be extended to the case where \(\mathcal{Y}\) is a continuous set, say \(\mathcal{Y}=[L,U]\). In this case, we can first choose a finite discrete subset \(\tilde{\mathcal{Y}}\subseteq\mathcal{Y}\) that contains both \(L\) and \(U\), e.g., \(\tilde{\mathcal{Y}}=\{L,L+\Delta,\ldots,U\}\), and then apply _unbiased randomized rounding_\(\mathsf{URR}_{\tilde{\mathcal{Y}}}(\cdot)\) to all labels before applying any mechanism \(\mathcal{M}\).

Namely, for any label \(y\in[L,U]\) such that \(y_{1}\leq y\leq y_{2}\) for \(y_{1},y_{2}\) being consecutive elements of \(\tilde{\mathcal{Y}}\), \(\mathsf{URR}_{\tilde{\mathcal{Y}}}(y)\) returns \(\tilde{y}\) drawn as \(y_{1}\) with probability \(\frac{y_{2}-y}{y_{2}-y_{1}}\) and \(y_{2}\) with probability \(\frac{y-y_{1}}{y_{2}-y_{1}}\). This ensures that \(\mathbb{E}[\tilde{y}]=y\) and hence, for any unbiased mechanism \(\mathcal{M}\) that acts on inputs in \(\tilde{\mathcal{Y}}\), it holds for all \(y\in\mathcal{Y}\) that \(\mathbb{E}[\mathcal{M}(\mathsf{URR}_{\widetilde{\mathcal{Y}}}(y))]=\mathbb{E}[ \mathsf{URR}_{\widetilde{\mathcal{Y}}}(y)]=y\). Furthermore, as the gap between consecutive points of \(\widetilde{\mathcal{Y}}\) decreases, the variance introduced by \(\mathsf{URR}_{\widetilde{\mathcal{Y}}}\) also decreases.

## 4 Experimental Evaluation

We evaluate our randomizer on three datasets, and compare with the Laplace mechanism [1], the additive staircase mechanism [1], and the \(\mathsf{RR}\)-\(\mathsf{on}\)-\(\mathsf{Bins}\) method [13]. The Laplace mechanism and the additive staircase mechanism both have a discrete and a continuous variant. Following [13], we use the continuous variants for real-valued labels (the Criteo Sponsored Search Conversion dataset), and the discrete variants for integer-valued labels (the US Census dataset and the App Ads Conversion Count dataset). Note that in the experiments from [13], the noised labels from both the Laplace mechanism and the additive staircase mechanism were clipped to be in the valid label value range, as for small \(\varepsilon\)'s, the magnitude of the noised labels could be orders of magnitudes larger than the original label values, potentially causing numerical instability in model training. However, in our study, we found that with more careful hyperparameter tuning and early stopping (see Appendix G), the learning could be stabilized for sufficiently small values (e.g., \(\varepsilon\geq 0.3\)) that is practically useful in ML, and in this case, the unclipped (therefore also unbiased) version of both the Laplace and the additive staircase mechanisms outperform their clipped counterpart. For reference, we present the results for both clipped and unclipped variants for those two mechanisms. We note that all of these mechanisms can be implemented in the feature-oblivious label DP setting (Figure 1). More details on model and training configurations are presented in Appendix G.

### Criteo Sponsored Search Conversion

The Criteo Sponsored Search Conversion Log Dataset [14] is a collection of \(15,995,634\) data points derived from a sample of \(90\)-day logs of live traffic from Criteo Predictive Search (CPS). Each example contains information of an user action (e.g., a click on an advertisement) and a subsequent conversion (purchase of the related product) within a \(30\)-day attribution window. We use the setup of feature-oblivious label DP to predict the revenue in \(\mathfrak{C}\) (i.e., the \(\mathsf{SalesAmountInEuro}\) field in the dataset) of a conversion. We apply the following preprocessing steps: filtering out examples with \(\mathsf{SalesAmountInEuro}\) being \(-1\), and clipping the labels at the \(95\)th percentile of the value distribution (\(400\mathfrak{C}\)).

In Figure 3, we visualize an example of the various label randomizers. For \(\varepsilon=4\), the optimal \(\mathsf{RR}\)-\(\mathsf{on}\)-\(\mathsf{Bins}\) randomizer maps the input values to one of four distinct values. On the other hand, for the optimal unbiased randomizer, the joint distribution of the sensitive labels and randomized labels maintains an overall concentration along the diagonal. The joint distribution for the Laplace mechanism is quite spread out.

In Figure 4, we compare the noisy label loss on the training set and the mean squared error on the test set for all the randomizers considered. For each randomizer, the label randomization and training was run with 10 independent random seeds, and the plots show the average and standard deviation bars. For \(\mathsf{RR}\)-\(\mathsf{on}\)-\(\mathsf{Bins}\) and the optimal unbiased randomizer, we use \(\varepsilon_{1}=0.017\) for privately estimating the prior using \(\mathcal{M}_{\varepsilon_{1}}^{\mathrm{Lap}}\), and \(\varepsilon_{2}=\varepsilon-\varepsilon_{1}\) for randomizing the label, following the guidance in Appendix C.

Figure 3: Illustration of various \(\varepsilon\)-DP label randomizers for \(\varepsilon=4\). The 2D density plot contours are generated in log scale using Gaussian kernel density estimates. The legends show the MSE between the original labels and the \(\varepsilon\)-DP randomized labels.

We find that the optimal unbiased randomizer (Algorithm 3) achieves the smallest test mean squared error across a wide range of \(\varepsilon\) values. It is interesting to note that the unbiased randomizers (Laplace and additive staircase mechanisms and the optimal unbiased randomizer) vastly outperform the biased randomizers (clipped versions of Laplace and additive staircase mechanisms as well as the RR-on-Bins) in terms of test loss, even though the noisy label loss is an order of magnitude larger for the unbiased randomizers.

### US Census

The \(1940\) US Census dataset3 is widely used in the evaluation of data analysis with DP [21, 19, 17, 18]. This digitally released dataset in 2012 contains \(131,903,909\) examples. We follow the task studied in [18] and evaluate label DP algorithms by learning to predict the number of weeks each respondent worked during the previous year (the WKSWORK1 field).

Footnote 3: https://www.archives.gov/research/census/1940

In Figure 4(a) we compare the mean squared error on the test set for all the randomizers considered. For each randomizer, the label randomization and training was run with 10 independent random seeds, and the plots show the average and standard deviation bars. For RR-on-Bins and the optimal unbiased randomizer, we use \(\varepsilon_{1}=0.002\) for privately estimating the prior using \(\mathcal{M}_{\varepsilon_{1}}^{\mathrm{Lap}}\), and \(\varepsilon_{2}=\varepsilon-\varepsilon_{1}\) for randomizing the label, following the guidance in Appendix C. We find that the optimal unbiased randomizer achieves the smallest test mean squared error across a wide range of \(\varepsilon\) values.

Figure 4: Comparison of different label DP randomizers on the Criteo Search dataset: (a) shows the mean squared error between the original labels and the privatized labels on the training set from each randomizer; (b) shows the mean squared error between the model predictions and the groundtruth labels on the test set. The solid line indicates the non-private baseline.

Figure 5: Comparison of different label DP randomizers: (a) shows the mean squared error of the prediction on the US Census test set. The solid line indicates the non-private baseline. (b) shows the relative Poisson loss (i.e., \((L-L^{\star})/L^{\star}\)) with respect to the non-private baseline Poisson loss \(L^{\star}\).

### App Ads Conversion Count

We also evaluate on a conversion count prediction dataset collected from a commercial mobile app store. Each example in this dataset corresponds to an ad click and the task is to predict the number of post-click conversion events in the app after a user installs the app within a certain time window.

In Figure 4(b) we compare the relative Poisson loss on the test set for all the randomizers considered. For each randomizer, the label randomization and training was run with 6 independent random seeds. For the optimal unbiased randomizer, Laplace and additive staircase mechanisms, at low \(\varepsilon\)'s we experience blowup in the training loss during training. The plot show the average and standard deviation bars, at the time of lowest training loss, right before blowup. We see that for all \(\varepsilon\)'s, all of the unbiased randomizers vastly outperform the others, including RR-on-Bins. Among the unbiased randomizers (optimal unbiased randomizer, Laplace and additive staircase mechanisms), the optimal unbiased randomizer performs the best at all \(\varepsilon\)'s. One caveat is that for the smaller \(\varepsilon\)'s, one can see the standard error increase. We hypothesize this is not an inherent feature of the randomizers but due to the blow up of the model training, reducing the training length and adding additional variance to the error, a behavior that seems specific to the AppAds dataset.

## 5 Related Work

DP learning has been the subject of considerable research spanning different settings that include empirical risk minimization [10], PAC learning [11], training neural networks [1], online learning [13], and regression [21]. For the label DP setting, [14, 15] studied the sample complexity of classification, while [21] studied sparse linear regression in the local DP model [12, 13]. For training deep neural networks with label DP, [16] studied the classification setting and gave a multi-stage training procedure where the priors on the labels in the previous stage are used to define an LP that is optimized to find the randomization mechanism for the next stage. For the classification loss, they characterized the optimal solution as the so-called RRTop-\(k\). By contrast, the work of [16] showed that for regression objectives the optimal solution to the LP is an RR-on-Bins solution. A crucial insight in our work is that the addition of an unbiasedness constraint to the LP leads to solutions that (i) are not RR-on-Bins solutions, (ii) can have substantially higher variance than RR-on-Bins, and (iii) nevertheless have a much lower train and test error due to the reduction in bias.

Kairouz et al. [16] defined a family of staircase mechanisms and showed they are optimal among local DP algorithms for minimizing an objective function given a prior. We extend the notion of staircase mechanisms to include real-valued labels that affect the optimization function. The structural properties that we prove (Theorem 6) utilize the ordering of the labels and the unbiasedness condition, which we show is critical for training neural networks for regression with label DP and high accuracy. Moreover, their work shows the presence of staircase mechanisms in the context of optimizing mutual information and KL-divergence objectives, whereas we show their presence in the context of regression.

A two-party learning setting with one features party and one labels party was recently studied in the work of [17]. They focus on the interactive setting, which is arguably less practical than the (non-interactive) feature-oblivious setting studied in [15] that we also consider.

We also note that the label DP mechanism of [14], which builds on the PATE framework [18, 19], and the works of [16, 20], which leverage unsupervised and semi-supervised learning algorithms, cannot be implemented in the feature-oblivious setting. The DP-SGD algorithm of [1], which protects both the features and the label of each training example and which has been applied to different domains including computer vision [15] and language models [20] also cannot implemented in the feature-oblivious label DP setting. The same is also true for label DP algorithms for logistic regression [11] that are based on linear queries (where the features are assumed to be known and non-sensitive, and the linearity is with respect to the labels).

Over the past decade, there has been a significant body of research on DP ML (e.g., [10, 13]). In particular, DP regression has been the focus of several prior papers including [14, 15, 16, 20].

The label DP setting has also been studied in several papers including [11, 16, 17, 19, 20, 21, 22, 23, 24]. The Randomized Response (RR) mechanism [26], a basic form of label DP, was introduced several decades ago and is widely studied/used.

## 6 Conclusion

In this work, we show that using unbiased \(\varepsilon\)-DP label randomizers lead to better trained models, and in particular, choosing the label randomizer that minimizes the noisy label squared loss seems to perform the best in terms of test performance, by empirically demonstrating this on three datasets. We also provide theoretical results shedding light on the structure of these randomizers, as well as why they might lead to better trained models.

Discussion.While we focused entirely on \(\varepsilon\)-DP ("pure-DP"), for our specific approach, it does not seem that relaxing to \((\varepsilon,\delta)\)-DP ("approximate-DP") will be beneficial. For the first stage of estimating the histogram privately, one could potentially use an approximate-DP mechanism, but we believe that is unlikely to change the prior significantly. For the second stage of randomizing labels, it is known that approximate-DP may not be helpful in the local model [16].

In this work, we use hyperparameter tuning to choose the best architecture, which in general has additional privacy costs, and how to tune hyperparameters privately and efficiently is an active research topic [14, 15]. Consequently, it is common in the private ML literature to separate the question of private hyperparameter tuning and private training, and focus on comparing the privacy-utility trade-off under optimal hyperparameters, e.g., [17, 18, 23, 24]. We also follow this convention.

Limitations and future work.While LPs are somewhat efficient, it would be desirable to have a more efficient algorithm for computing the optimal unbiased randomizer. The RR-on-Bins randomizer introduced in [20] has an important advantage that one does not need to construct the set of potential output labels \(\hat{\mathcal{Y}}\) before hand, and in fact, there is a simple dynamic programming algorithm to compute the optimal randomizer along with the output labels. Moreover, the RR-on-Bins family of randomizers has a clean structure. Understanding more structural properties of the optimal unbiased randomizers, and designing better algorithms for computing them, would be an interesting future direction to pursue.

To further improve the benefit of this optimal prior-based unbiased label randomizer, it might be possible to first partition the input examples into groups of "related examples", purely using the input features and compute the optimal unbiased randomizer for each group, by privately estimating the prior over labels for each group. This could lead to better test performance, as the randomizer can adapt to the different priors across these groups, unlike the "static" label randomizers such as Laplace and additive staircase mechanisms. For example, such an approach was used in the setting of image classification using self-supervised learning in [21].

As shown in (2), there is a bias-variance trade-off when it comes to choosing a label randomizer. While RR-on-Bins minimizes the noisy label loss (corresponding to the variance), the method in our work minimizes the bias (in fact, making it zero), at the cost of greatly increasing the variance. However, it might be possible in some settings that allowing a small amount of bias might greatly reduce the variance, thereby improving performance. This might be especially relevant when \(\varepsilon\) is very small. This could be done for example by adding a regularizer to reduce the bias, without enforcing hard unbiased constraints, as currently done in Algorithm 1. We leave this investigation to future work.

## References

* [ACG\({}^{+}\)16] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _CCS_, pages 308-318, 2016.
* [AMS\({}^{+}\)22] Daniel Alabi, Audra McMillan, Jayshree Sarathy, Adam Smith, and Salil Vadhan. Differentially private simple linear regression. _PETS_, 2:184-204, 2022.
* [BLM20] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. In _FOCS_, 2020.
* [BMS22] Raef Bassily, Mehryar Mohri, and Ananda Theertha Suresh. Differentially private learning with margin guarantees. In _NeurIPS_, 2022.
* [BNS16] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. _ToC_, 12(1):1-61, 2016.
* [BNS18] Mark Bun, Jelani Nelson, and Uri Stemmer. Heavy hitters and the structure of local privacy. In _PODS_, pages 435-447, 2018.
* [CH11] Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning. In _COLT_, pages 155-186, 2011.
* [CJG21] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Data poisoning attacks to local differential privacy protocols. In _USENIX Security_, pages 947-964, 2021.
* [CMS11] Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization. _JMLR_, 12(3), 2011.
* [DBH\({}^{+}\)22] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy differentially private image classification through scale. _arXiv preprint arXiv:2204.13650_, 2022.
* [DHS15] Ilias Diakonikolas, Moritz Hardt, and Ludwig Schmidt. Differentially private learning of structured discrete distributions. In _NIPS_, pages 2566-2574, 2015.
* [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In _TCC_, pages 265-284, 2006.
* [DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* [EGS03] Alexandre Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches in privacy preserving data mining. In _PODS_, pages 211-222, 2003.
* [EMSV22] Hossein Esfandiari, Vahab Mirrokni, Umar Syed, and Sergei Vassilvitskii. Label differential privacy via clustering. In _AISTATS_, pages 7055-7075, 2022.
* [GGK\({}^{+}\)21] Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, and Chiyuan Zhang. Deep learning with label differential privacy. _NeurIPS_, pages 27131-27145, 2021.
* [GKK\({}^{+}\)23] Badih Ghazi, Pritish Kamath, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Avinash Varadarajan, and Chiyuan Zhang. Regression with label differential privacy. In _ICLR_, 2023.
* [GKM\({}^{+}\)21] Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh, and Amer Sinha. Differentially private aggregation in the shuffle model: Almost central accuracy in almost a single message. In _ICML_, pages 3692-3701, 2021.
* [GR21] Alexandre Gilotte and David Rohde. Learning a logistic model from aggregated data. _AdKDD Workshop_, 2021.
* [GV14] Quan Geng and Pramod Viswanath. The optimal mechanism in differential privacy. In _ISIT_, pages 2371-2375, 2014.
* [Har23] Charlie Harrison. Consider a randomized-response-like privacy mechanism, April 2023. github.com/patcg-individual-drafts/ipa/issues/60.
* [Haz22] Elad Hazan. _Introduction to Online Convex Optimization_. MIT Press, 2022.
* [HLY\({}^{+}\)23] Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, and Jiang Bian. Exploring the limits of differentially private deep learning with group-wise clipping. In _ICLR_, 2023.

* [JKT12] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta. Differentially private online learning. In _COLT_, pages 24:1-24:34, 2012.
* [KCS\({}^{+}\)22] Alexey Kurakin, Steve Chien, Shuang Song, Roxana Geambasu, Andreas Terzis, and Abhradeep Thakurta. Toward training at imagenet scale with differential privacy. _CoRR_, abs/2201.12328, 2022.
* [KLN\({}^{+}\)11] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. What can we learn privately? _SIAM J. Comput._, 40(3):793-826, 2011.
* [KOV16] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. Extremal mechanisms for local differential privacy. _JMLR_, 17:17:1-17:51, 2016.
* [KST12] Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization and high-dimensional regression. In _COLT_, pages 25.1-25.40, 2012.
* [LSY\({}^{+}\)21] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, and Chong Wang. Label leakage and protection in two-party split learning. In _ICLR_, 2021.
* [MEMP\({}^{+}\)21] Mani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, and Florian Tramer. Antipodes of label differential privacy: PATE and ALIBI. _NeurIPS_, 34:6934-6945, 2021.
* [PAE\({}^{+}\)17] Nicolas Papernot, Martin Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervised knowledge transfer for deep learning from private training data. In _ICLR_, 2017.
* [PS22] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with Renyi differential privacy. In _ICLR_, 2022.
* [PSM\({}^{+}\)18] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Ulfar Erlingsson. Scalable private learning with PATE. In _ICLR_, 2018.
* [RE19] Carey Radebaugh and Ulfar Erlingsson. Introducing TensorFlow Privacy: Learning with Differential Privacy for Training Data, March 2019. blog.tensorflow.org.
* [SAZL18] Michael Thomas Smith, Mauricio A. Alvarez, Max Zwiesele, and Neil Lawrence. Differentially private regression with Gaussian processes. In _AISTATS_, 2018.
* [SCS13] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In _GlobalSIP_, pages 245-248, 2013.
* [SSS23] Tom Sander, Pierre Stock, and Alexandre Sablayrolles. TAN without a burn: Scaling laws of DP-SGD. In _ICML_, pages 29937-29949, 2023.
* [TM20] Davide Testugine and Ilya Mironov. PyTorch Differential Privacy Series Part 1: DP-SGD Algorithm Explained, August 2020. medium.com.
* [TNM\({}^{+}\)22] Xinyu Tang, Milad Nasr, Saeed Mahloujifar, Virat Shejwalkar, Liwei Song, Amir Houmansadr, and Prateek Mittal. Machine learning with differentially private labels: Mechanisms and frameworks. _PETS_, 4:332-350, 2022.
* [TY18] Marcelo Tallis and Pranjul Yadav. Reacting to variations in product demand: An application for conversion rate (CR) prediction in sponsored search. In _IEEE BigData_, pages 1856-1864, 2018.
* [Wan18] Yu-Xiang Wang. Revisiting differentially private linear regression: optimal and adaptive prediction & estimation in unbounded domain. In _UAI_, pages 93-103, 2018.
* [War65] Stanley L Warner. Randomized response: A survey technique for eliminating evasive answer bias. _JASA_, 60(309):63-69, 1965.
* [WDZ\({}^{+}\)19] Tianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong, Zhicong Huang, Ninghui Li, and Somesh Jha. Answering multi-dimensional analytical queries under local differential privacy. In _SIGMOD_, pages 159-176, 2019.
* [WX19] Di Wang and Jinhui Xu. On sparse linear regression in the local differential privacy model. In _ICML_, pages 6628-6637, 2019.

* [YNB\({}^{+}\)22] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. In _ICLR_, 2022.
* [YSMN21] Sen Yuan, Milan Shen, Ilya Mironov, and Anderson Nascimento. Label private deep learning training based on secure multiparty computation and differential privacy. In _NeurIPS Workshop Privacy in Machine Learning_, 2021.
* [ZZX\({}^{+}\)12] Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and Marianne Winslett. Functional mechanism: regression analysis under differential privacy. _VLDB_, 5(11):1364-1375, 2012.

## Appendix A Bayes Optimal Predictor Using Noisy Labels

We prove Theorem5, restated below for convenience.

See 5

Proof.: If \(\mathcal{M}\) is unbiased, then \(\mathbb{E}_{\hat{y}\sim\mathcal{M}(y)}[\hat{y}]=\sum_{\hat{y}\in\hat{\mathcal{Y} }}\hat{y}\cdot\Pr[\mathcal{M}(y)=\hat{y}]=y\) for all input labels \(y\in\mathcal{Y}\). We have the following equalities (we restrict ourselves to the case of finite \(\mathcal{Y}\) and \(\hat{\mathcal{Y}}\), but the proof generalizes easily to measurable sets \(\mathcal{Y}\) and \(\hat{\mathcal{Y}}\)):

\[\mathop{\mathbb{E}}_{\begin{subarray}{c}\hat{y}\sim\mathcal{M}(y )\\ (x,y)\sim\mathcal{D}\end{subarray}}[\hat{y}\mid x=x_{0}] = \sum_{y\in\mathcal{Y},\hat{y}\in\hat{\mathcal{Y}}}\hat{y}\cdot \Pr[\hat{y}\mid y,x=x_{0}]\cdot\Pr[y\mid x=x_{0}]\] \[= \sum_{y\in\mathcal{Y},\hat{y}\in\hat{\mathcal{Y}}}\hat{y}\cdot \Pr[\hat{y}\mid y]\cdot\Pr[y\mid x=x_{0}]\quad(\because\text{feature obliviousness})\] \[= \sum_{y\in\mathcal{Y}}\Pr[y\mid x=x_{0}]\cdot\left(\sum_{\hat{y} \in\hat{\mathcal{Y}}}\hat{y}\cdot\Pr[\hat{y}\mid y]\right)\] \[= \sum_{y\in\mathcal{Y}}\Pr[y\mid x=x_{0}]\cdot y\quad(\because\text {unbiasedness})\] \[= \mathop{\mathbb{E}}_{y\sim D(x_{0})}[y\mid x=x_{0}],\]

showing that the Bayes optimal predictors are equal at all values \(x_{0}\).

Conversely, suppose we have a \(y^{\prime}\in\mathcal{Y}\) such that \(\mathbb{E}_{\hat{y}\sim\mathcal{M}(y^{\prime})}[\hat{y}]\neq y^{\prime}\). Then take any distribution \(D\) for which there is some \(x_{0}\) with \(\Pr[y=y^{\prime}\mid x=x_{0}]=1\). Then clearly \(\mathbb{E}_{y\sim D(x_{0})}[y\mid x=x_{0}]=y^{\prime}\). However, \(\mathbb{E}_{\hat{y}\sim\mathcal{M}(y),y\sim D(x_{0})}[\hat{y}\mid x=x_{0}]= \mathbb{E}_{\hat{y}\sim\mathcal{M}(y^{\prime})}[\hat{y}]\neq y^{\prime}\). Hence the Bayes optimal predictors differ at \(x_{0}\). 

## Appendix B Structural Properties of Optimal Unbiased Randomizers

We prove Theorem6, restated below for convenience. We use the following assumption about the label loss \(g\), which in particular, is satisfied by \(\ell_{\text{sq}}\) that we primarily use.

See 9

Proof.: If \(\mathcal{M}\) is unbiased, then \(\mathbb{E}_{\hat{y}\sim\mathcal{M}(y)}[\hat{y}]=\sum_{\hat{y}\in\hat{\mathcal{ Y}}}\hat{y}\cdot\Pr[\mathcal{M}(y)=\hat{y}]=y\) for all input labels \(y\in\mathcal{Y}\). We have the following equalities (we restrict ourselves to the case of finite \(\mathcal{Y}\) and \(\hat{\mathcal{Y}}\), but the proof generalizes easily to measurable sets \(\mathcal{Y}\) and \(\hat{\mathcal{Y}}\)):

\[\mathop{\mathbb{E}}_{\begin{subarray}{c}\hat{y}\sim\mathcal{M}(y )\\ (x,y)\sim\mathcal{D}\end{subarray}}[\hat{y}\mid x=x_{0}] = \sum_{y\in\mathcal{Y},\hat{y}\in\hat{\mathcal{Y}}}\hat{y}\cdot \Pr[\hat{y}\mid y,x=x_{0}]\cdot\Pr[y\mid x=x_{0}]\] \[= \sum_{y\in\mathcal{Y},\hat{y}\in\hat{\mathcal{Y}}}\hat{y}\cdot \Pr[\hat{y}\mid y]\cdot\Pr[y\mid x=x_{0}]\quad(\because\text{feature obliviousness})\] \[= \sum_{y\in\mathcal{Y}}\Pr[y\mid x=x_{0}]\cdot\left(\sum_{\hat{y} \in\hat{\mathcal{Y}}}\hat{y}\cdot\Pr[\hat{y}\mid y]\right)\] \[= \sum_{y\in\mathcal{Y}}\Pr[y\mid x=x_{0}]\cdot y\quad(\because\text {unbiasedness})\] \[= \mathop{\mathbb{E}}_{y\sim D(x_{0})}[y\mid x=x_{0}],\]

showing that the Bayes optimal predictors are equal at all values \(x_{0}\).

Conversely, suppose we have a \(y^{\prime}\in\mathcal{Y}\) such that \(\mathbb{E}_{\hat{y}\sim\mathcal{M}(y^{\prime})}[\hat{y}]\neq y^{\prime}\). Then take any distribution \(D\) for which there is some \(x_{0}\) with \(\Pr[y=y^{\prime}\mid x=x_{0}]=1\). Then clearly \(\mathbb{E}_{y\sim D(x_{0})}[y\mid x=x_{0}]=y^{\prime}\). However, \(\mathbb{E}_{\hat{y}\sim\mathcal{M}(y),y\sim D(x_{0})}[\hat{y}\mid x=x_{0}]= \mathbb{E}_{\hat{y}\sim\mathcal{M}(y^{\prime})}[\hat{y}]\neq y^{\prime}\). Hence the Bayes optimal predictors differ at \(x_{0}\). 
We show that for any \(\varepsilon\)-DP unbiased randomizer \(\mathcal{M}^{(0)}\), there exists another \(\varepsilon\)-DP unbiased randomizer \(\mathcal{M}\), which satisfies certain nice properties, while not increasing the noisy label loss.

**Claim 11**.: _Suppose \(|\mathcal{Y}|\geq 2\). For any \(\varepsilon\)-DP unbiased randomizer \(\mathcal{M}^{(0)}\) mapping to a finite set of output labels, there exists an \(\varepsilon\)-DP unbiased randomizer \(\mathcal{M}\) with output label sequence \(\hat{\mathcal{Y}}=(\hat{y}_{i})_{i\in\mathcal{I}}\), encoded by \((M_{y\to i})_{y,i}\) such that the following hold:_

* \(\mathcal{G}(\mathcal{M}^{0};P)\geq\mathcal{G}(\mathcal{M};P)\)_._
* _Each column of_ \(S_{M}\) _consists of only_ U_'s and_ L_'s, with at least one_ U _and one_ L_._
* _With two indices in_ \(\mathcal{Y}\) _sorted in increasing order from top to bottom, every non-zero column of_ \(S_{M}\) _matches the regular expression_ \(\mathsf{L}^{*}\mathsf{U}^{+}\mathsf{L}^{*}\)_. For each_ \(i\in\mathcal{I}\)_, let_ \(\Phi_{1}(i)\) _and_ \(\Phi_{2}(i)\) _be the smallest and the largest_ \(y\) _respectively for which_ \(S_{M}(y,i)=\mathsf{U}\)_._
* _For all_ \(i<i^{\prime}\) _it holds that_ \(\Phi_{1}(i)\leq\Phi_{1}(i^{\prime})\) _and_ \(\Phi_{2}(i)\leq\Phi_{2}(i^{\prime})\)_, with at least one of the inequalities being strict._

Before we prove Claim 11, we prove Theorem 6 using it.

Proof of Theorem 6.: If \(|\mathcal{Y}|=1\), then it is immediate that the optimal unbiased randomizer simply returns the unique value in \(\mathcal{Y}\) with probability \(1\). If \(|\mathcal{Y}|\geq 2\), then for any optimal unbiased randomizer \(\mathcal{M}^{(0)}\), we can apply Claim 11 to get another optimal unbiased randomizer \(\mathcal{M}\) that satisfies the stated conditions. Let \(\mathcal{I}_{\neq 0}\subseteq\mathcal{I}\) be the subsequence of all outputs for which \(M_{y\to i}\neq 0\) (for all \(y\)). The sequence \((\Phi_{1}(i),\Phi_{2}(i))\) ordered in increasing order of \(i\in\mathcal{I}_{\neq 0}\) is strictly increasing in the partial order \(\mathcal{Y}\times\mathcal{Y}\), that is, at least one of \(\Phi_{1}(i)\) or \(\Phi_{2}(i)\) strictly increases from one \(i\) to next. It is easy to see that this can happen at most \(2|\mathcal{Y}|\) times, thereby concluding that \(|\mathcal{I}_{\neq 0}|\leq 2|\mathcal{Y}|\). 

To prove Claim 11, we perform several transformations to the given \(\varepsilon\)-DP randomizer \(\mathcal{M}^{(0)}\), which is say encoded by \(M^{(0)}=(M^{(0)}_{y\to i})_{y,i}\) for some \(\hat{\mathcal{Y}}=(\hat{y}_{i})_{i\in\mathcal{I}}\), such that the randomizer obtained in each step satisfies \(\mathbb{E}\,\mathcal{M}^{(r)}(y)=\mathbb{E}\,\mathcal{M}^{(r+1)}(y)\) for all \(y\in\mathcal{Y}\) and \(\mathcal{G}(\mathcal{M}^{(r)};P)\geq\mathcal{G}(\mathcal{M}^{(r+1)};P)\).

Combining columns with identical signatures.For convenience let \(k:=|\mathcal{Y}|\). Let \(b_{j}\) be the \(k\)-dimensional binary vector corresponding to the binary representation of \(j\) for \(j\leq 2^{k}-1\). A matrix \(S^{(k)}\in\{1,e^{\varepsilon}\}^{k\times(2^{k}-2)}\) is called a _Staircase Pattern Matrix_[13] if the \(j\)th column of \(S^{(k)}\) is \(S^{(k)}_{j}=(e^{\varepsilon}-1)b_{j}+1\), for \(j\in\{1,\ldots,2^{k}-2\}\). For example,

\[S^{(3)}=\begin{bmatrix}1&1&1&e^{\varepsilon}&e^{\varepsilon}&e^{\varepsilon}\\ 1&e^{\varepsilon}&e^{\varepsilon}&1&1&e^{\varepsilon}\\ e^{\varepsilon}&1&e^{\varepsilon}&1&e^{\varepsilon}&1\end{bmatrix}\]

Since \(\mathcal{M}^{(0)}\) is \(\varepsilon\)-DP, we have that each column of \(M^{(0)}\), namely \((M^{(0)}_{y\to i})_{y\in\mathcal{Y}}\) is a conic combination of the columns of \(S^{(k)}\).

**Claim 12**.: _Let \(\mathcal{M}^{(0)}\) be an \(\varepsilon\)-DP randomizer, mapping \(\mathcal{Y}\) to a finite subset \(\hat{\mathcal{Y}}^{(0)}\) of \(\mathbb{R}\). Then there exists an \(\varepsilon\)-DP randomizer \(\mathcal{M}^{(1)}\) mapping \(\mathcal{Y}\) to \(\hat{\mathcal{Y}}^{(1)}\subseteq\mathbb{R}\) encoded by \((M_{y\to\hat{y}})_{y\in\hat{\mathcal{Y}},\hat{y}\in\hat{\mathcal{Y}}^{(1)}}\) such that:_

* \(\mathcal{G}(\mathcal{M}^{(1)};\mathcal{P})\leq\mathcal{G}(\mathcal{M}^{(0)}; \mathcal{P})\) _for any convex loss_ \(g\)_._
* \(\mathbb{E}[\mathcal{M}^{(1)}(y)|y]=\mathbb{E}[\mathcal{M}^{(0)}(y)|y]\)_._
* \(|\mathcal{Y}|\leq 2^{|\mathcal{Y}|}\)_._
* _The vector_ \((M_{y\to\hat{y}})_{y\in\mathcal{Y}}\) _is a positive multiple of a staircase column, with the staircase column being unique for each_ \(\hat{y}\in\hat{\mathcal{Y}}^{(1)}\)_._

The proof idea of combining columns of staircase type to potentially decrease loss was shown in [13].

Proof.: By \(\varepsilon\)-DP constraint, the columns of \(M^{(0)}\) lie in the cone made by convex combinations of the rays \(\{c,c\cdot e^{\varepsilon}\}^{|\mathcal{Y}|}\) where \(c\geq 0\). Because positive multiples of the staircase columns are specifically these rays, except the ray \(\{c\}^{|\mathcal{Y}|}\) which is already in the convex hull, this column of \(\mathcal{M}^{(0)}\) is a convex combination of positive multiples of the staircase columns.

Let \(v_{\hat{y}}\) be the columns of \(M^{(0)}\) indexed by \(\hat{\mathcal{Y}}^{(0)}\), which by the above can be written as:

\[v_{\hat{y}}=\sum_{j=1}^{2^{k}-2}c_{\hat{y}}^{j}\cdot S_{j}^{(k)}.\]

Define for each \(j\):

\[C^{j}=\sum_{\hat{y}\in\hat{\mathcal{Y}}^{0}}c_{\hat{y}}^{j}\qquad\text{and} \qquad\hat{y}_{j}=\frac{\sum_{\hat{y}\in\hat{\mathcal{Y}}^{0}}\hat{y}\cdot c_{ \hat{y}}^{j}}{C^{j}}.\]

Define \(M^{(1)}\) as being supported only at \(\hat{y}_{j}\) with the vector of probabilities being \(C^{j}\cdot S_{j}^{(k)}\). We now prove \(M^{(1)}\) has all the desired properties.

* (Defines a Mechanism): \[\sum_{j=1}^{2^{k}-2}C^{j}\cdot S_{j}^{(k)}=\sum_{j=1}^{2^{k}-2}\left(\sum_{ \hat{y}\in\hat{\mathcal{Y}}^{0}}c_{\hat{y}}^{j}\right)\cdot S_{j}^{(k)}=\sum _{\hat{y}\in\hat{\mathcal{Y}}^{0}}v_{\hat{y}}=\overrightarrow{1}.\]
* (Does not increase loss): This follows by Jensen's inequality and the linearity of expectation: \[\mathcal{G}(\mathcal{M}^{(0)};\mathcal{P})=\sum_{j=1}^{2^{k}-2}\sum_{y\in \mathcal{Y}}(S_{j}^{(k)})_{y}\cdot\left(\sum_{\hat{y}}g(\hat{y},y)\cdot c_{ \hat{y}}^{j}\right)\geq\sum_{j=1}^{2^{k}-2}\sum_{y\in\mathcal{Y}}(S_{j}^{(k)} )_{y}\cdot\left(g(\hat{y}_{j},y)\cdot C^{j}\right)=\mathcal{G}(\mathcal{M}^{( 1)};\mathcal{P}).\]
* (Expectation does not change): Because \(M^{(0)}\) is unbiased: \[\sum_{\hat{y}\in\hat{\mathcal{Y}}^{0}}\hat{y}\cdot v_{\hat{y}}=\left(y\right)_{ y\in\hat{Y}}.\]

We have

\[\sum_{j=1}^{2^{k}-2}\hat{y}_{j}\cdot C^{j}\cdot S_{j}^{(k)}=\left(y\right)_{ y\in\hat{Y}},\]

showing that \(M^{(1)}\) is unbiased.
* (Bounded by \(2^{|\mathcal{Y}|}\) and multiple of unique staircase columns): This is clear by construction. 

While the finite case is easier to read, the above proof can be done also when the range of \(M^{(0)}\) is \(\mathbb{R}\) and \(v_{\hat{y}}\) is a vector of absolutely continuous probability distributions. The only change is that sums over \(\hat{y}\in\hat{\mathcal{Y}}^{(0)}\) become integrals. The projections of \(v_{\hat{y}}\) onto the staircase columns are continuous maps. Lastly, because a continuous function that is bounded by a function in \(L^{1}\) is itself in \(L^{1}\), all the integrals become well-defined and finite. Therefore, our theorem regarding the structure of the optimal unbiased \(\varepsilon\)-DP mechanism does not depend on a finiteness condition.

Each row matches the pattern \(\mathsf{L}^{\star}\mathsf{U}^{\star}\mathsf{L}^{\star}\).We now use the ordering of the input and output labels as real numbers to make further deductions.

**Claim 13**.: _Suppose \(|\mathcal{Y}|\geq 2\). For any \(\varepsilon\)-DP randomizer \(\mathcal{M}^{(1)}\) obtained via Claim 12, there exists an \(\varepsilon\)-DP randomizer \(\mathcal{M}^{(2)}\) mapping \(\mathcal{Y}\) to \(\hat{\mathcal{Y}}\subseteq\mathbb{R}\) encoded by \(M:=\left(M_{y\to\hat{y}}\right)_{y\in\mathcal{Y},\hat{y}\in\hat{\mathcal{Y}}}\) such that:_

* \(\mathcal{G}(\mathcal{M}^{(2)};\mathcal{P})\leq\mathcal{G}(\mathcal{M}^{(1)}; \mathcal{P})\) _for any convex loss_ \(g\)_,_
* \(\mathbb{E}[\mathcal{M}^{(2)}(y)|y]=\mathbb{E}[\mathcal{M}^{(1)}(y)|y]\)_,_
* _Every row of_ \(S_{M}\) _matches the regular expression_ \(\mathsf{L}^{\star}\mathsf{U}^{+}\mathsf{L}^{\star}\)_._

Proof.: The only way a row of \(S_{M}\) does not match the regular expression \(\mathsf{L}^{\star}\mathsf{U}^{+}\mathsf{L}^{\star}\) is if (i) there is no \(\mathsf{U}\) present, or (ii) if there is an \(\mathsf{L}\) between an two \(\mathsf{U}\)'s. It is easy to see that (i) is not possible because \(\sum_{\hat{y}}M_{y\to i}=1\) for all \(y\in\mathcal{Y}\), and if one row has no \(\mathsf{U}\)'s, this would imply no row has any \(\mathsf{U}\)'s, which is not possible by the uniqueness of the columns if \(|\mathcal{Y}|\geq 2\), and we know such an \(\mathcal{M}\) exists.

If (ii) is true, then we can construct \(\mathcal{M}^{(2)}\) as follows. For any \(y\) and \(\hat{y}_{1}<\hat{y}_{2}<\hat{y}_{3}\) such that \(S_{M^{(1)}}(y,\hat{y}_{1})=S_{M^{(1)}}(y,\hat{y}_{3})=\mathsf{U}\) and \(S_{M^{(1)}}(y,\hat{y}_{2})=\mathsf{L}\), consider a perturbation (for a small enough \(\eta>0\)), where we set

\[M^{(2)}_{y\to\hat{y}_{1}} =\ M^{(1)}_{y\to\hat{y}_{1}}-(y_{3}-y_{2})\eta\,,\] \[M^{(2)}_{y\to\hat{y}_{2}} =\ M^{(1)}_{y\to\hat{y}_{2}}+(y_{3}-y_{1})\eta\,,\] \[M^{(2)}_{y\to\hat{y}_{3}} =\ M^{(1)}_{y\to\hat{y}_{3}}-(y_{2}-y_{1})\eta\,.\]

This choice ensures that \(M^{(2)}\) remains an unbiased randomizer. By Jensen's inequality, due to convexity of \(g\), it holds that \(\mathcal{G}(\mathcal{M}^{(2)};P)<\mathcal{G}(\mathcal{M}^{(1)};P)\). We can keep repeating these steps, in addition to Claim 12 if necessary, till we arrive at \(\mathcal{M}^{(2)}\) such that each row of \(S_{M^{(2)}}\) matches the regular expression \(\mathsf{L}^{*}\mathsf{U}^{+}\mathsf{L}^{*}\). 

Set of \(\mathsf{U}\)'s in any row cannot be a subset of the set of \(\mathsf{U}\)'s in another, and are "moving forward".

**Claim 14**.: _For an \(\varepsilon\)-\(\mathsf{DP}\) unbiased randomizer \(\mathcal{M}^{(2)}\) obtained via Claim 13, let \(T(y)\subseteq\hat{\mathcal{Y}}\) be defined as \(T(y):=\{\hat{y}\,:\,S_{M}(y,\hat{y})=\mathsf{U}\}\). Then, for all \(y\neq y^{\prime}\), it holds that \(T(y)\not\subset T(y^{\prime})\), and moreover if \(y<y^{\prime}\), then \(\min T(y)<\min T(y^{\prime})\) and \(\max T(y)<\max T(y^{\prime})\)._

Proof.: If \(T(y)\subset T(y^{\prime})\), then it holds that \(M_{y\to\hat{y}}\leq M_{y^{\prime}\to\hat{y}}\) with the inequality being strict for some \(\hat{y}\). Thus, we have \(\sum_{\hat{y}}M_{y\to\hat{y}}<\sum_{\hat{y}}M_{y^{\prime}\to\hat{y}}=1\), which is a contradiction.

Given that both \(T(y)\) and \(T(y^{\prime})\) are contiguous subsets of \(\hat{\mathcal{Y}}\), it follows that one of following holds:

* \(\min T(y)<\min T(y^{\prime})\) and \(\max T(y)<\max T(y^{\prime})\), or
* \(\min T(y)>\min T(y^{\prime})\) and \(\max T(y)>\max T(y^{\prime})\).

However, if \(y<y^{\prime}\), the latter is not possible because in that case, we would have \(y^{\prime}=\mathbb{E}_{\hat{y}\sim\mathcal{M}^{(2)}}(y^{\prime})<\mathbb{E}_{ \hat{y}\sim\mathcal{M}^{(2)}}(y)=y\), which is a contradiction. 

Putting together the claims.Finally, we put together Claims 12 to 14 to prove Claim 11.

Proof of Claim 11.: Given any \(\varepsilon\)-\(\mathsf{DP}\) unbiased randomizer \(\mathcal{M}\), we apply Claims 12 and 13 to obtain the \(\varepsilon\)-\(\mathsf{DP}\) unbiased randomizer \(\mathcal{M}^{(2)}\), which satisfies the conditions in Claim 14. Consider the signature matrix \(S_{M}\) corresponding to \(\mathcal{M}^{(2)}\), with the row indices sorted in increasing order of \(\mathcal{Y}\).

If a column has a signature with an \(\mathsf{L}\) between two \(\mathsf{U}\)'s, this would contradict Claim 14, since the \(\mathsf{U}\)'s only "move forward".

Additionally, for the signatures of column \(\hat{y}_{1}<\hat{y}_{2}\), the first \(\mathsf{U}\) of column \(\hat{y}_{2}\) cannot come before the first \(\mathsf{U}\) of column \(\hat{y}_{1}\), as this would also contradict the claim of the \(\mathsf{U}\)'s moving forward. Similarly, the last \(\mathsf{U}\) of column \(\hat{y}_{1}\) cannot come after the last \(\mathsf{U}\) of column \(\hat{y}_{2}\). Finally, the columns must be unique, since each column is a unique staircase matrix vector. Thus, each column of \(S_{M}\) matches the regular expression \(\mathsf{L}^{*}\mathsf{U}^{+}\mathsf{L}^{*}\). 

## Appendix C Optimal Unbiased Randomizers with Approximate Prior

We elaborate on the approach of splitting privacy budget for estimating the prior and for generating noisy labels as described in Section 3.

To privately estimate the prior, we use the Laplace mechanism. The Laplace distribution with scale parameter \(b\), denoted by \(\operatorname{Lap}(b)\), is the distribution supported on \(\mathbb{R}\) whose probability density function is \(\frac{1}{2b}\exp(-|x|/b)\). The Laplace mechanism is presented in Algorithm 4.

It is well-known (e.g., [14]) that this mechanism satisfies \(\varepsilon\)-\(\mathsf{DP}\). It is also well-known that this yields an approximation of the true distribution up to small total variation distance (e.g., [13]).

In [15], it is argued that the best choice of \((\varepsilon_{1},\varepsilon_{2})\) that minimizes the excess noisy label loss achieved by randomizer computed in Algorithm 3 over the optimal unbiased randomizer computed using the true prior and privacy parameter \(\varepsilon\), is given as \(\varepsilon_{1}=\sqrt{k/n}\). (We follow the same budget splitting method in our experiments.)

## Appendix D Excess Loss of SGD with Biased Gradient Oracles

Consider a _convex_ objective \(\mathcal{L}(\cdot)\) over \(\mathbb{R}^{D}\), for which we have a gradient oracle, that given \(w\), returns a stochastic estimate \(g(w)\) of \(\nabla\mathcal{L}(w)\). We say that the gradient oracle has bias \(\alpha\) and variance \(\sigma^{2}\) if \(g(w)=\nabla\mathcal{L}(w)+\zeta(w)\) such that \(\|\operatorname{\mathbb{E}}\zeta(w)\|\leq\alpha\) and \(\operatorname{\mathbb{E}}\|\zeta(w)-\operatorname{\mathbb{E}}\zeta(w)\|^{2} \leq\sigma^{2}\) holds for all \(w\). When optimizing over a convex set \(\mathcal{K}\subseteq\mathbb{R}^{D}\), projected GD with step size \(\eta\) is defined as iteratively performing \(w_{t+1}\leftarrow\Pi_{\mathcal{K}}(w_{t}-\eta g(w_{t}))\), where \(\Pi_{\mathcal{K}}(\cdot)\) is the projection onto \(\mathcal{K}\). We recall the following guarantee on the expected excess loss using a standard analysis (see [10]).

**Lemma 15**.: _For a \(L\)-Lipschitz loss function, and a gradient oracle with bias \(\alpha\) and variance \(\sigma^{2}\), projected GD over a set \(\mathcal{K}\) with diameter \(R\), with step size \(\eta=\frac{R}{\sqrt{((L+\alpha)^{2}+\sigma^{2})T}}\) achieves_

\[\operatorname{\mathbb{E}}\left[\mathcal{L}\left(\tfrac{1}{T}\sum_{i=1}^{T}w_{i }\right)\right]-\mathcal{L}(w^{*})\;\leq\;\tfrac{RL}{\sqrt{T}}\cdot\sqrt{1+ \tfrac{2\alpha}{L}+\tfrac{\alpha^{2}+\sigma^{2}}{L^{2}}}+\alpha R.\]

_In particular, if \(\alpha=0\), we get_

\[\operatorname{\mathbb{E}}\left[\mathcal{L}\left(\tfrac{1}{T}\sum_{i=1}^{T}w_{i }\right)\right]-\mathcal{L}(w^{*})\;\leq\;\tfrac{RL}{\sqrt{T}}\cdot\sqrt{1+ \tfrac{\sigma^{2}}{L^{2}}}.\]

The dependence on the bias is essentially tight, for _any_ first order method. Consider the loss function \(\mathcal{L}(w)=\frac{1}{2}\alpha w\) defined over the domain \(\mathcal{K}=[0,R]\). However, if the gradient oracle is allowed to have a bias of magnitude \(\alpha\), it is impossible using any first order algorithm to distinguish between the gradients of \(\mathcal{L}\) and that of \(\mathcal{L}^{\prime}(w):=-\frac{1}{2}\alpha w\). Note that both \(\mathcal{L}\) and \(\mathcal{L}^{\prime}\) are \(L\)-Lipschitz for \(L\geq\frac{1}{2}\alpha\). While \(\mathcal{L}\) is minimized at \(w=0\), \(\mathcal{L}^{\prime}\) is minimized at \(w=R\). For any \(w\in\mathcal{K}\), \(\min\{\mathcal{L}(w)-\mathcal{L}(0),\mathcal{L}^{\prime}(w)-\mathcal{L}^{ \prime}(R)\}\geq\frac{1}{4}\alpha R\).

Thus, with access to gradients with bias \(\alpha\), it is impossible for any first order method to achieve an excess loss that is smaller than \(\Omega(\alpha R)\), whereas, with access to gradients with zero bias and variance \(\sigma^{2}\), it is possible to achieve an arbitrarily small excess loss using sufficient number of steps of SGD.

## Appendix E Noisy Label Loss Bound from Discretization

Here we prove Lemma8, restated below for convenience.

**Lemma 8**.: _Let \(\mathcal{M}\) be the optimal unbiased randomizer with \(\hat{\mathcal{Y}}=[L,U],\) and let \(\mathcal{M}_{\Delta}\) be the optimal unbiased randomizer with \(\hat{\mathcal{Y}}=\{L,L+\Delta,\ldots,U-\Delta,U\}\). If \(g(\hat{y},y)\) is \(K\)-Lipschitz in \(\hat{y}\), then \(\mathcal{G}(\mathcal{M}_{\Delta};\mathcal{P})\leq\mathcal{G}(\mathcal{M}; \mathcal{P})+K\Delta\)._

Proof.: Given any unbiased mechanism \(\mathcal{M}\), we construct a mechanism \(\mathcal{M}_{u}\) supported on \(\hat{\mathcal{Y}}^{\prime}:=\{L,L+\Delta,\ldots,U-\Delta,U\}\) by applying "unbiased randomized rounding" \(\mathsf{URR}_{\hat{\mathcal{Y}}^{\prime}}\) to the output of \(\mathcal{M}\), i.e., \(\mathcal{M}_{u}(y):=\mathsf{URR}_{\hat{\mathcal{Y}}^{\prime}}(\mathcal{M}(y))\).

For any \(\hat{y}\in[L,U]\) such that \(\hat{y}_{1}\leq\hat{y}\leq\hat{y}_{2}\) for \(\hat{y}_{1},\hat{y}_{2}\) being consecutive elements of \(\hat{\mathcal{Y}}^{\prime}\), \(\mathsf{URR}_{\hat{\mathcal{Y}}^{\prime}}(y)\) returns \(\hat{y}^{\prime}\) drawn as \(\hat{y}_{1}\) with probability \(\frac{\hat{y}_{2}-\hat{y}}{\hat{y}_{2}-\hat{y}_{1}}\) and \(\hat{y}_{2}\) with probability \(\frac{\hat{y}-\hat{y}_{1}}{\hat{y}_{2}-\hat{y}_{1}}\). Observe that \(\operatorname{\mathbb{E}}[\mathsf{URR}_{\hat{\mathcal{Y}}^{\prime}}(\hat{y})] =\hat{y}\) and \(|\mathsf{URR}_{\hat{\mathcal{Y}}^{\prime}}(\hat{y})-\hat{y}|\leq\Delta\) with probability \(1\).

Thus, we have that \(\mathbb{E}[\mathcal{M}_{u}(y)]=\mathbb{E}[\mathsf{URR}_{\tilde{y}^{\prime}}( \mathcal{M}(y))]=\mathbb{E}[\mathcal{M}(y)]=y\), and hence \(\mathcal{M}_{u}\) is also unbiased. Moreover,

\[\mathcal{G}(\mathcal{M}_{u};P) =\begin{array}{l}\underset{\begin{subarray}{c}y\sim\mathcal{P}\\ \hat{y}^{\prime}\sim\mathcal{M}_{u}(y)\end{subarray}}{\mathbb{E}}\;\;[g(\hat{y} ^{\prime},y)]\;=\;\underset{\begin{subarray}{c}y\sim\mathcal{P}\\ \hat{y}^{\prime}\sim\mathsf{URR}_{\tilde{y}^{\prime}}(\hat{y})\end{subarray}}{ \mathbb{E}}\;\;[g(\hat{y}^{\prime},y)]\\ \leq\underset{\begin{subarray}{c}y\sim\mathcal{P}\\ \hat{y}^{\prime}\sim\mathcal{M}(y)\\ \hat{y}^{\prime}\sim\mathsf{UR}_{\tilde{y}^{\prime}}(\hat{y})\end{subarray}}{ \mathbb{E}}\;\;[g(\hat{y},y)+K|\hat{y}^{\prime}-\hat{y}|]\;=\;\mathcal{G}( \mathcal{M};P)+K\Delta,\]

where the last line follows from \(K\)-Lipschitzness of \(g\). Since \(\mathcal{M}_{\Delta}\) is the optimal unbiased randomizer with output labels \(\hat{\mathcal{Y}}^{\prime}\), we have that \(\mathcal{G}(\mathcal{M}_{\Delta};\mathcal{P})\leq\mathcal{G}(\mathcal{M}_{u}; \mathcal{P})\leq\mathcal{G}(\mathcal{M};\mathcal{P})+K\Delta\). 

## Appendix F DP Mechanisms Definitions

In this section, we recall the definition of various DP notions that we use throughout the paper.

**Definition 16** (Global Sensitivity).: Let \(f\) be a function taking as input a dataset and returning as output a vector in \(\mathbb{R}^{d}\). Then, the _global sensitivity_\(\Delta(f)\) of \(f\) is defined as the maximum, over all pairs (\(X,X^{\prime}\)) of adjacent datasets, of \(||f(X)-f(X^{\prime})||_{1}\).

The (discrete) Laplace distribution with scale parameter \(b>0\) is denoted by \(\mathrm{DLap}(b)\). Its probability mass function is given by \(p(y)\propto\exp(-|y|/b)\) for any \(y\in\mathbb{Z}\).

**Definition 17** (Discrete Laplace Mechanism, [16]).: Let \(f\) be a function taking as input a dataset \(X\) and returning as output a vector in \(\mathbb{Z}^{d}\). The _discrete Laplace mechanism_ applied to \(f\) on input \(X\) returns \(f(X)+(Y_{1},\dots,Y_{d})\) where each \(Y_{i}\) is sampled i.i.d. from \(\mathrm{DLap}(\Delta(f)/\varepsilon)\). The output of the mechanism is \(\varepsilon\)-DP.

Next, recall that the (continuous) Laplace distribution \(\mathrm{Lap}(b)\) with scale parameter \(b>0\) has probability density function given by \(h(y)\propto\exp(-|y|/b)\) for any \(y\in\mathbb{R}\).

**Definition 18** (Continuous Laplace Mechanism, [16]).: Let \(f\) be a function taking as input a dataset \(X\) and returning as output a vector in \(\mathbb{R}^{d}\). The _continuous Laplace mechanism_ applied to \(f\) on input \(X\) returns \(f(X)+(Y_{1},\dots,Y_{d})\) where each \(Y_{i}\) is sampled i.i.d. from \(\mathrm{Lap}(\Delta(f)/\varepsilon)\). The output of the mechanism is \(\varepsilon\)-DP.

We next define the discrete and continuous versions of the staircase mechanism [1].

**Definition 19** (Discrete Staircase Distribution).: Fix \(\Delta\geq 2\). The _discrete staircase distribution_ is parameterized by an integer \(1\leq r\leq\Delta\) and has probability mass function given by:

\[p_{r}(i)=\begin{cases}a(r)&\text{for }0\leq i<r,\\ e^{-\varepsilon}a(r)&\text{for }r\leq i<\Delta\\ e^{-k\varepsilon}p_{r}(i-k\Delta)&\text{for }k\Delta\leq i<(k+1)\Delta\text{ and }k\in \mathbb{N}\\ p_{r}(-i)\text{ for }i<0,\end{cases}\] (3)

where

\[a(r)=:=\frac{1-b}{2r+2b(\Delta-r)-(1-b)}.\]

Let \(f\) be a function taking as input a dataset \(X\) and returning as output a scalar in \(\mathbb{Z}\). The _discrete staircase mechanism_ applied to \(f\) on input \(X\) returns \(f(X)+\tilde{Y}\) where \(Y\) is sampled from the discrete staircase distribution given in (3).

**Definition 20** (Continuous Staircase Distribution).: The _continuous staircase distribution_ is parameterized by \(\gamma\in(0,1)\) and has probability density function given by:

\[h_{\gamma}(x)=\begin{cases}a(\gamma)&\text{for }x\in[0,\gamma\Delta)\\ e^{-\varepsilon}a(\gamma)&\text{for }x\in[\gamma\Delta,\Delta)\\ e^{-k\varepsilon}h_{\gamma}(x-k\Delta)&\text{for }x\in[k\Delta,(k+1)\Delta) \text{ and }k\in\mathbb{N}\\ h_{\gamma}(-x)\text{ for }x<0,\end{cases}\] (4)where

\[a(\gamma)=:=\frac{1-e^{-\varepsilon}}{2\Delta(\gamma+e^{-\varepsilon}(1-\gamma)}.\]

Let \(f\) be a function taking as input a dataset \(X\) and returning as output a scalar in \(\mathbb{R}\). The _continuous staircase mechanism_ applied to \(f\) on input \(X\) returns \(f(X)+Y\) where \(Y\) is sampled from the continuous staircase distribution given in (4).

**Definition 21** (Randomized Response, [20]).: Let \(\varepsilon\geq 0\), and \(q\) be a positive integer. The _randomized response_ mechanism with parameters \(\varepsilon\) and \(q\) (denoted by \(\mathsf{RR}_{\varepsilon,q}\)) takes as input \(y\in\{1,\ldots,q\}\) and returns \(\hat{y}\sim\hat{Y}\), where the random variable \(\hat{Y}\) is distributed as:

\[\Pr[\hat{y}=\hat{y}]=\begin{cases}\frac{e^{\varepsilon}}{e^{\varepsilon}+q-1}& \text{for }\hat{y}=y\\ \frac{1}{e^{\varepsilon}+q-1}&\text{otherwise}.\end{cases}\] (5)

The output of \(\mathsf{RR}_{\varepsilon,q}\) is \(\varepsilon\text{-DP}\).

**Definition 22** (Randomized Response on Bins, [14]).: Let \(\varepsilon>0\), for map \(\Phi:\mathcal{Y}\to\hat{\mathcal{Y}}\), \(\mathsf{RR}\text{-on-Bins}_{\varepsilon}^{\Phi}\) is defined as the mechanism that on input \(y\) samples \(\hat{y}\sim\hat{Y}\), where the random variable \(\hat{Y}\) is distributed as

\[\Pr[\hat{Y}=\hat{y}]\ =\ \begin{cases}\frac{e^{\varepsilon}}{e^{\varepsilon}+| \mathcal{Y}|-1}&\text{if }\hat{y}=\Phi(y)\\ \frac{1}{e^{\varepsilon}+|\mathcal{Y}|-1}&\text{otherwise}.\end{cases}\]

**Definition 23** (Debiased Randomized Response).: For \(\varepsilon>0\), the _\(\varepsilon\)-debiased randomized response_ (\(\varepsilon\text{-dbRR}\)) operates by performing randomized response on \((\Phi(y))_{y\in\mathcal{Y}}\), where

\[\Phi(y)=\left((e^{\varepsilon}+|\mathcal{Y}|-1)y-\sum_{y^{\prime}\in\mathcal{ Y}}y^{\prime}\right)/(e^{\varepsilon}-1),\]

namely, the mechanism on input \(y\) samples \(\hat{y}\sim\hat{Y}\), where the random variable \(\hat{Y}\) is distributed as

\[\Pr[\hat{Y}=\hat{y}]\ =\ \begin{cases}\frac{e^{\varepsilon}}{e^{\varepsilon}+| \mathcal{Y}|-1}&\text{if }\hat{y}=\Phi(y)\\ \frac{1}{e^{\varepsilon}+|\mathcal{Y}|-1}&\text{otherwise}.\end{cases}\]

Finally, we prove Proposition7, restated below for convenience.

**Proposition 7**.: _If \(\hat{\mathcal{Y}}\) contains just the smallest and largest values among the outputs of \(\varepsilon\text{-dbRR}\), the LP in \(\mathsf{ComputeOptUnbiasedRand}_{\varepsilon}(P,\hat{\mathcal{Y}})\) is feasible._

Proof.: Let \(y_{0}=\min(\mathcal{Y})\) and \(y_{1}=\max(\mathcal{Y})\). The smallest and largest values among the outputs of \(\varepsilon\text{-dbRR}\) are given as

\[\hat{y}_{0} =\ \left((e^{\varepsilon}+|\mathcal{Y}|-1)\cdot y_{0}-\sum_{y\in \mathcal{Y}}y\right)/(e^{\varepsilon}-1),\] \[\hat{y}_{1} =\ \left((e^{\varepsilon}+|\mathcal{Y}|-1)\cdot y_{1}-\sum_{y\in \mathcal{Y}}y\right)/(e^{\varepsilon}-1).\]

Consider the mechanism \(\mathcal{M}\) such that for any input \(y\in\mathcal{Y}\) it holds that

\[\mathcal{M}(y)\ =\ \begin{cases}\hat{y}_{0}&\text{w.p. }\frac{\hat{y}_{1}-y}{\hat{y}_{1}-y_{0}}, \\ \hat{y}_{1}&\text{w.p. }\frac{\hat{y}_{0}}{\hat{y}_{1}-\hat{y}_{0}}.\end{cases}\]

Note \(\mathcal{M}\) is unbiased, namely \(\mathbb{E}[\mathcal{M}(y)]=y\) for all \(y\in\mathcal{Y}\). We show

\[\frac{\max_{y}\Pr[\mathcal{M}(y)=\hat{y}_{0}]}{\min_{y}\Pr[ \mathcal{M}(y)=\hat{y}_{0}]} =\ \frac{\Pr[\mathcal{M}(y_{0})=\hat{y}_{0}]}{\Pr[\mathcal{M}(y_{1})= \hat{y}_{0}]}\] \[=\ \frac{(e^{\varepsilon}+|\mathcal{Y}|-1)\cdot y_{1}-\sum_{y\in \mathcal{Y}}y-(e^{\varepsilon}-1)y_{0}}{(e^{\varepsilon}+|\mathcal{Y}|-1) \cdot y_{1}-\sum_{y\in\mathcal{Y}}y-(e^{\varepsilon}-1)y_{1}}\] \[=\ \frac{\sum_{y\neq y_{0}}(y_{1}-y)+e^{\varepsilon}(y_{1}-y_{0})}{ \sum_{y\neq y_{0}}(y_{1}-y)+(y_{1}-y_{0})}\ \ \leq\ \ e^{\varepsilon},\]

where the last step follows because \(\sum_{y\neq y_{0}}(y_{1}-y)\geq 0\) and for \(a,b,c>0\) it holds that \((a+b)/(a+c)\leq b/c\). Similarly, it holds that \(\max_{y}\Pr[\mathcal{M}(y)=\hat{y}_{1}]/\min_{y}\Pr[\mathcal{M}(y)=\hat{y}_{1} ]\leq e^{\varepsilon}\)Additional Details of Experiments

All our experiments were performed using NVidia P100 GPUs.

### Hyperparameter Details

Criteo Sponsored Search Conversion.We use a neural network that takes as input concatenation of floating point features, as well as categorical features using an embedding table of dimension four for each. The neural network had two hidden layers of dimensions \(64\) and \(32\) respectively. The training was performed on a random \(80\%\) of the dataset using RMSProp algorithm using the squared loss objective, with learning rate of \(10^{-4}\), \(\ell_{2}\)-regularization of \(10^{-4}\), batch size of \(1,024\) for \(50\) epochs. The remaining \(20\%\) of the dataset was used to report the test loss.

This architecture choice was chosen by performing a hyperparameter search over various choices, and this choice seemed to give the best results for all the randomizers considered.

US Census.We use a neural network that takes as input concatenation of floating point features, as well as categorical features using an embedding table of dimension eight for each. The neural network had two hidden layers of dimensions \(128\) and \(64\) respectively. The training was performed on a random \(80\%\) of the dataset using RMSProp algorithm using the squared loss objective, with \(\ell_{2}\)-regularization of \(10^{-4}\), batch size of \(8,192\). A varying learning rate in \(\{10^{-2},10^{-3},10^{-4}\}\) and a varying number of epochs in \(\{50,200\}\) were used, and the best test loss was reported for training with each randomizer. The remaining \(20\%\) of the dataset was used to report the test loss.

### Early Stopping

In training on the AppAds dataset, we found that the training would often be unstable, with the training loss blowing up after several steps, especially for smaller values of \(\varepsilon\), which corresponded to faster blow up. We used early stopping, keeping track of intermediate states, to select the best model that minimizes the training loss, and used this to report the test loss.

### Complexity of Computing the Mechanisms

Even though the wall clock time for computing the optimal unbiased randomizer using an LP solver is significantly larger than that of computing the optimal RR-on-Bins randomizer (for which there is a highly efficient dynamic programming algorithm [15]), this is still orders of magnitude smaller than that of ML model training. We note that while the noisy label loss decreases as \(\Delta\) in Algorithm2 is made smaller, this causes the computational cost of solving the LP to increase. This trade-off is demonstrated in the following table, which shows the running time of the LP for the unbiased randomizer, the noisy label loss and the final test loss, for different mesh sizes (parameter \(n\) in Algorithm2) for \(\varepsilon=1\) on the US Census dataset we study (prediction of number of weeks worked in \(\{1,\dots,52\}\)).

\begin{tabular}{|c|c|c|c|c|} \hline \hline  & & \multicolumn{2}{c|}{**Computing mechanism**} & \multicolumn{1}{c|}{} \\
**Mechanism** & **Mesh size** & **wall-clock time (secs)** & **Noisy label loss** & **Test loss** \\ \hline RR-on-Bins & n/a & 0.154 & 79.71 & 172.44 \\ Opt. Unbiased & 52 & 2.38 & 1288.21 & 134.44 \\ Opt. Unbiased & 416 & 17.1 & 1275.22 & 134.43 \\ Opt. Unbiased & 1664 & 161 & 1274.71 & 134.43 \\ \hline \hline \end{tabular}

The test loss is quite similar for various mesh sizes, even though the noisy label loss does improve slightly with finer discretization. This suggests that the unbiasedness was key to the improvements over RR-on-Bins and the discretization of the output set does not hurt performance as much.

The computation time increases considerably when the number of input labels is large, e.g., in the Criteo Search Sponsored Conversion Logs dataset, where there were \(401\) input labels \(\{0,\dots,400\}\).