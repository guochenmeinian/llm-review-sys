# RECESS Vaccine for Federated Learning: Proactive Defense Against Model Poisoning Attacks

 Haonan Yan1,2, Wenjing Zhang2, Qian Chen1, Xiaoguang Li1

Wenhai Sun3, Hui Li1*, Xiaodong Lin2

1Xidian University, 2University of Guelph, 3Purdue University

yanhaonan.sec@gmail.com

Corresponding author

###### Abstract

Model poisoning attacks greatly jeopardize the application of federated learning (FL). The effectiveness of existing defenses is susceptible to the latest model poisoning attacks, leading to a decrease in prediction accuracy. Besides, these defenses are intractable to distinguish benign outliers from malicious gradients, which further compromises the model generalization. In this work, we propose a novel defense including detection and aggregation, named RECESS, to serve as a "vaccine" for FL against model poisoning attacks. Different from the passive analysis in previous defenses, RECESS proactively queries each participating client with a delicately constructed aggregation gradient, accompanied by the detection of malicious clients according to their responses with higher accuracy. Further, RECESS adopts a newly proposed trust scoring based mechanism to robustly aggregate gradients. Rather than previous methods of scoring in each iteration, RECESS takes into account the correlation of clients' performance over multiple iterations to estimate the trust score, bringing in a significant increase in detection fault tolerance. Finally, we extensively evaluate RECESS on typical model architectures and four datasets under various settings including white/black-box, cross-silo/device FL, etc. Experimental results show the superiority of RECESS in terms of reducing accuracy loss caused by the latest model poisoning attacks over five classic and two state-of-the-art defenses.

## 1 Introduction

Background and Problem.Recently, federated learning (FL) goes viral as a privacy-preserving training solution with the distributed learning paradigm [1], since data privacy attracts increasing attention from organizations like banks [2] and hospitals [3], governments like GDPR [4] and CPPA [5], and commercial companies like Google [6]. FL allows data owners to collaboratively train models under the coordination of a central server for better prediction performance by sharing local gradient updates instead of their own private/proprietary datasets, preserving the privacy of each participant's raw data. FL is promising as a trending privacy training technology. However, FL is vulnerable to various model poisoning attacks [7; 8]. The distributed structure of FL characterized by the privacy-preserving local training paradigm makes it impossible to verify overall gradient updates and distributions of local datasets. Accordingly, the attacker can corrupt the global model by hijacking compromised participants, and then uploading malicious local gradient updates, leading to a reduction in the final model's prediction performance, which significantly jeopardizes the application of FL.

###### Abstract

In this paper, we propose a novel defense against model poisoning attacks called RECESS. It offers a new defending angle for FL and turns the defender from passive analysis to proactive detection, which defeats the latest model poisoning attacks. RECESS can also identify benign outliers effectively.

## 1 Introduction

In this paper, we propose a novel defense against model poisoning attacks called RECESS. It offers a new defending angle for FL and turns the defender from passive analysis to proactive detection, which defeats the latest model poisoning attacks. RECESS can also identify benign outliers effectively.

In this paper, we propose a novel defense against model poisoning attacks called RECESS. It offers a new defending angle for FL and turns the defender from passive analysis to proactive detection, which defeats the latest model poisoning attacks. RECESS can also identify benign outliers effectively.

In this paper, we propose a novel defense against model poisoning attacks called RECESS. It offers a new defending angle for FL and turns the defender from passive analysis to proactive detection, which defeats the latest model poisoning attacks. RECESS can also identify benign outliers effectively.

In this paper, we propose a novel defense against model poisoning attacks called RECESS. It offers a new defending angle for FL and turns the defender from passive analysis to proactive detection, which defeats the latest model poisoning attacks. RECESS can also identify benign outliers effectively.

In this paper, we propose a novel defense against model poisoning attacks called RECESS. It offers a new defending angle for FL and turns the defender from passive analysis to proactive detection, which defeats the latest model poisoning attacks. RECESS can also identify benign outliers effectively.

In this paper, we propose a novel defense against model poisoning attacks called RECESS. It offers a new defending angle for FL and turns the defender from passive analysis to proactive detection, which defeats the latest model poisoning attacks. RECESS can also identify benign outliers effectively.

Figure 1: An intuitive example to illustrate the difference between previous defenses and RECESS. In an FL system with \(n\) clients, suppose the first \(c\) clients are malicious. Previous defenses find the malicious gradients by abnormality detection of all received gradients, while RECESS confirms the malicious gradients by comparing the constructed test gradient with the corresponding client’s response. Thus, RECESS can distinguish between malicious clients and benign outliers, achieving higher accuracy in the final trained model.

2. We improve the robust aggregation mechanism. A new trust scoring method is devised by considering clients' abnormality over multiple iterations, which significantly increases the detection fault tolerance of RECESS and further improves the model's accuracy.
3. We evaluate RECESS under various settings, including white/black-box, Non-IID degree of the dataset, number of malicious clients, and cross-silo/device FL. Experimental results show that RECESS outperforms previous defenses in terms of accuracy loss and achieves consistent effectiveness.

## 2 Related Works

### Latest Model Poisoning Attacks in FL

In this work, we focus on the stronger untargeted model poisoning attacks for three reasons:

1. Model poisoning attacks with the direct manipulation of gradients are more threatening than data poisoning attacks in FL [8; 13].
2. Data poisoning attacks are considered a special case of model poisoning attacks as malicious gradients can be obtained on poisoning datasets [11].
3. Untargeted poisoning is a more severe threat to model prediction performance than the targeted one in FL [12].

In the following, we introduce three latest model poisoning attacks.

**LIE Attack.** The LIE attack [14] lets the defender remove the non-byzantine clients and shift the aggregation gradient by carefully crafting byzantine values that deviate from the correct ones as far as possible.

**Optimization Attack.**[13] propose a new attack idea. They formulate the model poisoning attack as an optimization program, where the objective function is to let the poisoning aggregation gradient be far from the aggregation gradient without poisoning. By leveraging the halving search, they obtain a crafted malicious gradient to poison.

**AGR Attack Series.**[12] then improve the optimization program by introducing perturbation vectors and scaling factors. Then three instances (AGR-tailored, AGR-agnostic Min-Max, and Min-Sum) are proposed, which maximize the deviation between benign gradients and malicious gradients.

### Existing SOTA Defenses

There are two directions to defend against the model poisoning attack: robust aggregation and anomaly detection. Five classic byzantine-robust aggregation rules and FLTrust belong to the robust aggregation. DnC is one of the representative anomaly detectors. Here we mainly describe two SOTA defenses.

**FLTrust.**[11] devises FLTrust to mitigate the poisoning attack. In FLtrust, the server maintains a small clean dataset and act as a client to participate in the update, adding the deployment constraint. In each iteration, clients' gradients whose direction deviates more from the gradient of the server are discarded. Moreover, FLtrust normalizes the gradient magnitude to limit the impact of malicious gradients and achieves a competitive accuracy.

**DnC.**[12] leverages the spectral method to detect malicious gradients, which is proven effective in centered learning. They also use random sampling to reduce memory and computational cost. Note we will evaluate these two defense mechanisms, which have rigorous theoretical guarantees, as baselines.

## 3 Proposed Recess

We propose a new proactive defense against model poisoning attacks in FL called RECESS. We first take an overview of RECESS. Then we introduce the proactive detection and the new trust scoring based aggregation mechanism respectively, followed by the effectiveness analysis.

### Overview

Intuition.RECESS aims to distinguish malicious and benign gradients without implicating benign outliers. The essential difference between benign and malicious clients is that: benign clients including outliers always optimize the received aggregation result to the distribution of their local dataset to minimize the loss value, while malicious clients aim to make the poisoned aggregation direction as far away from the aggregation gradient without poisoning as possible to maximize the poisoning effect. In other words, benign gradients explicitly point to their local distribution, which is directional and relatively more stable, while the malicious gradients solved based on other benign gradients change inconsistently and dramatically. The workflow of RECESS is shown in Figure 2.

Proactive Detection.RECESS is different from previous passive analyses. The defender using RECESS proactively tests each client with elaborately constructed aggregation gradients, rather than the model weights to clients, and these two approaches are equivalent algorithmically. After clients fine-tune local models with received aggregation gradients and respond with new gradient updates, the defender can recognize malicious clients based on the abnormality of their gradient updates. We also redefine the abnormality in poisoning scenarios to distinguish malicious gradients and outliers, which promotes the generalization of the FL model. Details are shown in Section 3.2.

Robust Aggregation.RECESS adopts a new trust scoring based aggregation mechanism to improve the defense. A new scoring method is proposed to give weights to aggregate clients' gradients. Updates with greater abnormal extent account for a smaller proportion in aggregation, which increases fault tolerance. Details are shown in Section 3.3.

### Proactive Detection Algorithm

Construction Strategy of Test Gradient.The purpose of the defender is to observe the client's response by adding a slight perturbation to the test gradient. Thus, any slight varying gradient is feasible. To illustrate, we present a strategy based on previous gradients. In details, the defender firstly stores the aggregation gradient in the last iteration. When entering the detection mode, for example in \((k-1)\)-th round of detection, the defender idles this iteration, and for each uploaded gradient \(\bm{g}_{i}^{k-1}\) from \(i\)-th client (\(0<i\leq n\), \(n\) is the number of clients), the defender scale down the magnitude of \(\bm{g}_{i}^{k-1}\), i.e., \(\|\bm{g}_{i}^{t}\|_{2}=\bm{g}_{i}^{k-1}/\|\bm{g}_{i}^{k-1}\|_{2}\), and slightly adjust the direction of \(\bm{g}_{i}^{t}\) by heuristically selecting several elements and adding noise. Here we set a threshold \(\mathcal{A}\) to control the direction adjustment. Then, the defender feedbacks \(\bm{g}_{i}^{t}\) as the aggregation gradient to client \(i\). The client \(i\) updates this tailored "aggregation gradient" locally and respond with a newly updated gradient \(\bm{g}_{i}^{k}\). The defender

Figure 2: The overall detection process of RECESS against model poisoning attacks.

can perform poisoning detection by the comparison between \(\bm{g}_{i}^{t}\) and \(\bm{g}_{i}^{k}\). This process will be repeated for each client.

RECESS slightly adjusts constructed aggregation gradients, which magnifies the variance of malicious gradients and makes them more conspicuous, but benign clients and outliers are not affected. The reason is: for benign clients including outliers, their updated gradients always point to the direction of decreasing loss function values. Although some machine learning optimization algorithms, such as SGD, have deviations, they are still unbiased estimations of the normal gradient descent direction as a whole [15]. Conversely, malicious clients' gradients are usually opposite to the aggregation direction and come from the solution of the attack optimization program on other benign gradients. Thus, the variance of malicious gradients is enlarged by the optimization project. Many works [16; 17; 9; 8] also indicate that the fluctuation of the upload gradients cause the aggregation gradient to change more. Even when the defender uses the poisoned aggregation gradient to construct test gradient, this basis still remains unchanged and malicious and benign clients behave quite differently in this test. Besides, RECESS without extra model training is more efficient than other model-based detectors. Concluding, RECESS is effective to detect model poisoning attacks.

Poisoning Detection.After receiving clients' responses, the defender detects malicious clients from two dimensions of abnormality: _direction_ and _magnitude_ of gradient changes before and after the test. Formally, we use metric _cosine similarity_\(S_{C}^{k}\) to measure the angular change in direction between the constructed gradient \(\bm{g}_{i}^{t}\) and the response \(\bm{g}_{i}^{k}\),

\[S_{C}^{k}(\bm{g}_{i}^{t},\bm{g}_{i}^{k})=\frac{\bm{g}_{i}^{t}\cdot\bm{g}_{i}^ {k}}{\|\bm{g}_{i}^{t}\|_{2}\cdot\|\bm{g}_{i}^{k}\|_{2}}.\] (1)

Besides direction, the magnitude of the malicious gradient also dominates the poisoning effect, especially when larger than the benign gradients. Here we utilize the \(l_{2}\) distance to measure the magnitude, i.e., \(\|\bm{g}_{i}^{k}\|_{2}\).

After that, we redefine the abnormality of the client gradient by the deviation extent in direction and magnitude, instead of the distance from other selected gradients (e.g. Krum, Mkrum, Median, server's gradient [11]) or some benchmarks (e.g. Trmean, Bylan).

**Definition 1**: _(Abnormality) In model poisoning attacks of FL, the abnormality \(\alpha\) of \(k\)-th uploaded gradient from the \(i\)-th client should be measured by_

\[\alpha=-\frac{S_{C}^{k}}{\|\bm{g}_{i}^{k}\|_{2}}.\] (2)

The cosine similarity \(S_{C}^{k}\) controls the positive and negative of the abnormality \(\alpha\). When the direction of the client's response gradient \(\bm{g}_{i}^{k}\) is inconsistent with the defender's test gradient \(\bm{g}_{i}^{t}\), \(\alpha\) will be positive, and vice versa. The amount of change in \(\alpha\) is related to \(S_{C}^{k}\) and \(\|\bm{g}_{i}^{k}\|_{2}\). The smaller the deviation between the direction of \(\bm{g}_{i}^{t}\) and \(\bm{g}_{i}^{k}\), the smaller the \(\alpha\). Meanwhile, the larger the \(\|\bm{g}_{i}^{k}\|_{2}\), the higher the \(\alpha\). This setting encourages small-gradient updates, which avoids the domination of aggregation by malicious updates usually with a larger magnitude.

### New Robust Aggregation Mechanism

After detection, we propose a new trust scoring based mechanism to aggregate gradients, which increases RECESS's fault tolerance for false detection.

Trust Scoring.RECESS estimates the trust score for the long-term performance of each client, rather than the single iteration considered by previous schemes.

The detail of the scoring process is: the defender first sets the same initial trust score \(TS_{0}\) for each client. After each round of detection, a constant baseline score is deducted for clients who are detected as suspicious poisoners and not selected for aggregation. When the trust score of one client \(i\) reaches zero, i.e., \(TS_{i}=0\), the defender will label this client as malicious and no longer involve this client's updated gradients in the aggregation. The \(TS\) is calculated by

\[TS=TS_{0}-\alpha*baseline\_decreased\_score.\] (3)The \(TS_{0}\) and \(baseline\_decreased\_score\) controls the detection speed.

To prevent the attacker from increasing the trust score by suspending poisoning or constructing benign gradients, we let the defender defer adding \(TS\) to further restrict the attacker. Only after a period of good performance (e.g., 10 consecutive rounds), \(TS\) increase, but the score would be deducted once the client is detected as malicious, which effectively forces the attacker not to poison or reduce the intensity of poisoning to a negligible level.

Aggregation.After assigning \(TS\) to all clients, we transform \(TS\) into weights which are used to average clients' gradients as the aggregation gradient \(g\), i.e.,

\[g=\sum_{i=1}^{n}w_{i}*g_{i},\] (4)

the weight of client \(i\) is calculated by

\[w_{i}=\frac{e^{TS_{i}}}{\sum_{j=1}^{n}e^{TS_{j}}},\] (5)

where \(i,j=1,2,...,n\).

Advantages.Compared with previous methods, RECESS has four advantages:

1. Provide a more accurate measure of the abnormality extent for each client's update to assign the trust score.
2. Enable the defender to put clients' previous performance into account, not only in each iteration, enhancing the detection fault tolerance.
3. Protect benign outliers while effectively detecting carefully-constructed poisoning gradient updates, improving model accuracy and generalization.
4. Outperform previous FLTrust and ML-based detections in terms of efficiency with no need for extra data.

## 4 Evaluation

### Setup

Datasets and FL Setting.Table 1 shows four datasets and parameter settings used in the evaluation. The IID and Non-IID are both considered in the dataset division. We also consider two typical types of FL, i.e., cross-silo and cross-device.

Eight defenses are considered in the comparison experiments. Specifically, the common FedAvg is used as the benchmark. Five classic robust rules, Krum, Mkrum, Bulyan, Trmean, and Median, can provide convergence guarantees theoretically. Two SOTA defenses, FLtrust and DnC, are also involved. The parameters of these eight defenses are set to default. For RECESS, we set \(\mathcal{A}=0.95\), \(TS_{0}=1\), and \(baseline\_decreased\_score=0.1\) unless otherwise specified.

Attack Setting.We consider the five latest poisoning attacks including LIE, optimization attack, AGR-tailored attack, AGR-agnostic Min-Max attack, and Min-Sum attack. We also set two scenarios, i.e., white-box and black-box, where attackers have and does not have knowledge of other benign gradients. We assume 20% malicious clients as the default value unless specified otherwise. For the test gradient construction, we select and add noise to the first 10% of dimensions, iteratively adjusting until cosine similarity before/after meets the threshold.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline Dataset & Class & Size & Dimension & Model & Clients & Batch Size & Optimizer & Learning Rates & Epochs \\ \hline MNIST & 10 & 60,000 & \(28\times 28\) & FC (\(784\times 512\times 10\)) & 100 & 100 & Adam & 0.001 & 500 \\ CIFAR-10 & 10 & 60,000 & \(32\times 32\) & Alexnet [18] & 50 & 250 & SGD & 0.5 to 0.05 at 1000h epoch & 1200 \\ Purchase & 100 & 197,324 & 60 & \(600\times 1024\times 100\) & 80 & 500 & SGD & 0.5 & 1000 \\ FEMNIST & 62 & 671,585 & \(28\times 28\) & LeNet-S [19] & \(60\times 3400\) & client’s dataset & Adam & 0.001 & 1500 \\ \hline \end{tabular}
\end{table}
Table 1: Experiment datasets and FL settings.

Metric.\(Accuracy\) is used to measure the performance of the well-trained model with or without poisoning. Higher \(Accuracy\) indicates better defense.

### Comparison with SOTA

Table 2 shows the main comparison results. RECESS obtains higher accuracy than two SOTA defenses under latest poisoning attacks. Due to the space limit, here we mainly show the results on two datasets.

Defender's Goal (1): Defensive Effectiveness.As we can see from each row in Table 2, RECESS can defend against model poisoning attacks, but previous defenses have limited effect, especially for the strongest AGR attack series in the white-box case. The reasons are twofold:

1. Some malicious gradients evade defenses to be selected for aggregation.
2. Benign outliers are misclassified as malicious gradients and removed from the aggregation, especially for Non-IID datasets (e.g., FEMNIST) with more outliers.

Moreover, benign outliers are intractable for all previous defenses. FLTrust is better than other defenses since its normalization limits the large magnitude gradients. In contrast, RECESS can effectively distinguish benign outliers from malicious gradients, thus obtaining the highest accuracy, even in the most challenging white-box Non-IID case.

Defender's Goal (2): Model Performance.As shown in the first row in each set of Table 2 where no attacks are under consideration, robust rules will reduce the accuracy but have little impact on RECESS. This can be explained from three aspects:

1. Due to the existence of attackers, only 80% benign clients participate in the aggregation compared with no attack (e.g. 40 vs 50 for CIFAR-10).
2. Existing defenses discard benign gradients more or less, especially for benign outliers.
3. Malicious gradients are selected for aggregation.

The first aspect is inevitable since the defender cannot control the attacker, which also causes the main accuracy gap between FedAvg and our RECESS. However, RECESS can improve the other two aspects by more accurately distinguishing whether gradients are malicious or benign.

Impact of Attacker's Knowledge.Different knowledge affects the attack, and RECESS outperforms other defenses in both white/black-box. The stronger the attack, the better the effect of RECESS. In the black-box case, the attacker cannot offset the effect of benign gradients on the aggregation without the knowledge of other benign gradients, so the poisoning is greatly weakened and RECESS is not prominent as in the white-box case.

Changing Trend of Trust Score.Figure 3 presents the trust score of benign and malicious clients in each round of RECESS detection. Here we execute the strongest AGR-agnostic Min-max attack. Figure 4 shows the histogram of each client's \(Abnormality\) value in the third detection round to illustrate the effectiveness of RECESS. RECESS can effectively distinguish malicious and benign users in both the optimization attack and AGR attacks.

### Impact of FL Setting

In this part, we evaluate RECESS under various FL settings to illustrate the practicability of our proposed scheme.

Non-IID Degree of Dataset (More Outliers).The results on Non-IID FEMNIST in Table 2 show the effectiveness of our RECESS against latest poisoning attacks. Besides, we also involve the CIFAR-10 in the evaluation using the Non-IID dataset construction method same as [11, 12]. We consider four defenses (the best classic robust rules Trmean, SOTA FLtrust and DnC, and RECESS) against the strongest AGR-agnostic Min-Max attack in white/black-box cases.

Figure 5 shows that the increase in the Non-IID degree leads to an expansion of benign outliers which are similar to malicious gradients, and previous defenses cannot effectively distinguish outliers from malicious gradients. However, it has little impact on RECESS, since our method can accurately detect malicious gradients and benign outliers.

Number of Malicious Clients.Figure 6 shows that RECESS remains an outstanding defending effect all along, as the number of malicious clients increases, all poisoning attacks are more powerful, while the defending effect of other defenses decreases sharply. We vary the proportion of malicious clients from 5% to 25%, consistent with [12]. The other settings remain the same as Section 4.3.

Cross-device FL.Previous evaluations are mostly under cross-silo settings except for the FEMNIST which is naturally cross-device shown in Table 2. Further, we consider the cross-device setting using the dataset CIFAR-10. In each epoch, the server stochastically selects 10 updates from all clients to aggregate. The attack steps remain the same, and the other settings are the same as the default.

Table 3 shows that similar to the result of cross-silo, RECESS still achieves the best defense effect. Besides, the poisoning impacts under the cross-device setting are lower than the one of the cross-silo setting, because the server selects less number of clients for aggregation and ignores more malicious gradients in cross-device FL, thus the attacker cannot continuously poison, weakening the impact of the poisoning. Consequently, this leaves less improvement space for RECESS and other defenses.

### Adaptive Attacks

Active Evasion.With knowledge of RECESS, the attacker checks if it's a test gradient before deciding to poison or not. The adaptive strategy involves estimating an aggregated gradient \(g_{p}\) in each iteration from controlled benign clients. Then, the attacker compares the received aggregated gradient \(g_{agg}\) with \(g_{p}\) and \(g_{i}^{k-1}\) from the last iteration. If \(S_{C}(g_{agg},g_{i}^{k-1})\geq S_{C}(g_{agg},g_{p})\), the poisoning begins, while if \(S_{C}(g_{agg},g_{i}^{k-1})<S_{C}(g_{agg},g_{p})\), the poisoning stops, as it's considered detected.

As a countermeasure, the defender boosts detection frequency and intersperses it during the entire training, instead of fixed consecutive detection, and increases the sensitivity of parameters. Note that the defense should aim to make the model converge, it is not necessary to find the attacker. In other words, RECESS's presence increases the attack cost, so even if the attacker evades detection, the loss of model accuracy is negligible, which is acceptable.

Table 4 shows that the adaptive attack has little impact on the final model accuracy though it is stealthier than the continuous attack. This is due to the similarity between the test gradient and the normal aggregated gradient, leading to inaccurate estimation by the attacker and a decrease in aggregation weight. Also, increasing detection frequency lowers poisoning frequency, so the poisoning impact will be gradually alleviated by normal training over time.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \multirow{2}{*}{Dataset} & Attacker’s & \multirow{2}{*}{Attacks} & \multicolumn{4}{c}{RECESS Detection Frequency} \\ \cline{3-6}  & Knowledge & & 0 & 10 & 50 & 100 & 200 \\ \hline \multirow{4}{*}{\begin{tabular}{c} white-box \\ adaptive optimization attack \\ \end{tabular} } & no attack & 0.8235 & 0.8233 & 0.8221 & 0.8224 & 0.8215 \\ \cline{2-6}  & optimization attack & 0.2182 & 0.7983 & 0.8048 & 0.8106 & 0.8114 \\ \cline{1-1} \cline{2-6}  & adaptive optimization attack & 0.2482 & 0.3456 & 0.6847 & 0.7815 & 0.8011 \\ \hline \multirow{2}{*}{
\begin{tabular}{c} black-box \\ adaptive optimization attack \\ \end{tabular} } & optimization attack & 0.5152 & 0.7937 & 0.8045 & 0.8117 & 0.8048 \\ \cline{1-1} \cline{2-6}  & adaptive optimization attack & 0.5248 & 0.6481 & 0.7847 & 0.8048 & 0.8148 \\ \hline \end{tabular}
\end{table}
Table 4: FL accuracy with enhanced RECESS defenses against the adaptive poisoning attack. The task is FEMNIST. The detection frequency is controlled by \(TS_{0}\) and \(baseline\_decreased\_score\).

\begin{table}
\begin{tabular}{c c c c c c} \hline \multirow{2}{*}{Dataset} & Attacker’s & \multirow{2}{*}{Attacks} & \multicolumn{4}{c}{Defences} \\ \cline{3-6}  & Knowledge & & Trmean & FLTrust & DnC & RECESS \\ \hline \multirow{6}{*}{CIFAR-10} & \multirow{6}{*}{White-box} & No Attack & **0.6490** & 0.6178 & 0.6284 & 0.6448 \\ \cline{3-6}  & & LIE & 0.5559 & 0.6101 & 0.6157 & **0.6228** \\ \cline{3-6}  & & Optimization attack & 0.5655 & 0.6171 & 0.5913 & **0.6439** \\ \cline{3-6}  & & AGR-tailored & 0.5080 & 0.4538 & 0.5915 & **0.6046** \\ \cline{3-6}  & & AGR-agnostic Min-Max & 0.5548 & 0.5226 & 0.5478 & **0.6379** \\ \cline{3-6}  & & AGR-agnostic Min-Sum & 0.5758 & 0.5964 & 0.5793 & **0.6393** \\ \cline{2-6}  & & LIE & 0.5959 & 0.6137 & 0.6128 & **0.6353** \\ \cline{2-6}  & & Optimization attack & 0.6168 & 0.6159 & 0.6209 & **0.6277** \\ \cline{2-6}  & & AGR-tailored & 0.5509 & 0.5485 & 0.5956 & **0.6142** \\ \cline{2-6}  & & AGR-agnostic Min-Max & 0.5833 & 0.5715 & 0.6095 & **0.6416** \\ \cline{2-6}  & & AGR-agnostic Min-Sum & 0.6068 & 0.5937 & 0.6074 & **0.6385** \\ \hline \end{tabular}
\end{table}
Table 3: The impact of cross-device FL on the defenses. The model poisoning attack is AGR Min-Max.

Poisoning before the Algorithm Initialization.The detection basis is that the variance of the malicious gradient is larger than the benign gradient. Malicious gradients, solved by the optimization problem, are more inconsistent. It is also theoretically proven that the optimization problem amplifies this variance. Thus, RECESS is unaffected by initial poisoned gradients as it does not change this basis. Besides, when the poisoned aggregated gradient was used to detect, most clients will be identified as malicious. This violates the assumption of Byzantine robustness that requires more than 51% of users to be benign. Hence, the defender can easily discern a potentially poisoned initial gradient based on the new abnormality definition of RECESS.

Inconsistent attacks.Attackers can also first pretend as benign client to increase the trust score, and then provide the poisoning gradients. However, RECESS detects such behavior inconsistencies over time. The trust scoring of RECESS also incorporates delayed penalties for discrepancies between a client's current and past behaviors (shown in Section 3.3). Additionally, three factors limit intermittent attacks' impact:

1. [label=()]
2. FL's self-correcting property means inconsistent poisoning is much less impactful. Attackers would not take this approach in practice.
3. In real settings, clients participate briefly, often just once. Attackers cannot afford to waste rounds acting benign before attacking.
4. Defenses aim to accurately detect malicious updates for model convergence. Even if poisoning temporarily evaded detection, attack efficacy would diminish significantly, making it no longer a serious security concern.

## 5 Discussion

This work primarily focuses on untargeted model poisoning, along with previous SOTA works. We believe that model poisoning has a significant impact on real-world FL deployments for three reasons:

1. [label=()]
2. Model poisoning through direct gradient manipulation poses a greater threat than backdoors.
3. Backdoors are considered special cases of model poisoning, as malicious gradients can be obtained on poisoning datasets. However, it is challenging to solely imitate model poisoning gradients through training backdoors.
4. Untargeted poisoning has a more severe impact on overall model performance degradation compared to backdoors in FL.

As RECESS detects inconsistencies between client behaviors, it could mitigate targeted attacks from clients with backdoored data by deploying additional strategies. Due to space limitation, we refer the readers to our technical report [20] for more details.

## 6 Conclusion

In this work, we proposed RECESS, a novel defense for FL against the latest model poisoning attacks. We shifted the classic reactive analysis to proactive detection and offered a more robust aggregation mechanism for the server. In the comprehensive evaluation, RECESS achieves better performance than SOTA defenses and solves the outlier detection problem that previous methods can not handle. We anticipate that our RECESS will provide a new research direction for poisoning attack defense and promote the application of highly robust FL in practice.

## Acknowledgments

We thank all the anonymous chairs and reviewers for their valuable guidance and constructive feedback. Hui Li is partially supported by the National Natural Science Foundation of China (61932015), the National Key Research and Development Program of China (2022YFB3104100), Shaanxi innovation team project (2018TD-007), and Higher Education Discipline Innovation 111 project (B16037). Wenjing Zhang and Xiaodong Lin are partially supported by Natural Sciences and Engineering Research Council of Canada (NSERC). Part of Haonan Yan's work is done when he visits the School of Computer Science at the University of Guelph.

## References

* [1] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. _IEEE Signal Processing Magazine_, 37(3):50-60, 2020.
* [2] Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu. Ffd: A federated learning based method for credit card fraud detection. In _International conference on big data_, pages 18-32. Springer, 2019.
* [3] Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei Wang. Federated learning for healthcare informatics. _Journal of Healthcare Informatics Research_, 5(1):1-19, 2021.
* [4] Nguyen Truong, Kai Sun, Siyao Wang, Florian Guitton, and YiKe Guo. Privacy preservation in federated learning: An insightful survey from the gdpr perspective. _Computers & Security_, 110:102402, 2021.
* [5] Derek A Lackey. Data protection and privacy in canada: A balanced approach. _Applied Marketing Analytics_, 6(4):366-376, 2021.
* [6] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* [7] Rachid Guerraoui, Sebastien Rouault, et al. The hidden vulnerability of distributed learning in byzantium. In _International Conference on Machine Learning_, pages 3521-3530. PMLR, 2018.
* [8] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In _International Conference on Machine Learning_, pages 634-643. PMLR, 2019.
* [9] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. _Advances in Neural Information Processing Systems_, 30, 2017.
* [10] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In _International Conference on Machine Learning_, pages 5650-5659. PMLR, 2018.
* [11] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust federated learning via trust bootstrapping. In _NDSS_, 2021.
* [12] Virat Shejwalkar and Amir Houmansadr. Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning. In _NDSS_, 2021.
* [13] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to {Byzantine-Robust} federated learning. In _29th USENIX Security Symposium (USENIX Security 20)_, pages 1605-1622, 2020.
* [14] Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [15] Leon Bottou. Large-scale machine learning with stochastic gradient descent. In _Proceedings of COMPSTAT'2010_, pages 177-186. Springer, 2010.
* [16] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. _arXiv preprint arXiv:1806.00582_, 2018.
* [17] Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-iid data: A survey. _Neurocomputing_, 465:371-390, 2021.
* [18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.

* [19] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [20] Haonan Yan, Wenjing Zhang, Qian Chen, Xiaoguang Li, Wenhai Sun, Hui Li, and Xiaodong Lin. Recess vaccine for federated learning: Proactive defense against model poisoning attacks. https://arxiv.org/abs/2310.05431.
* [21] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2938-2948. PMLR, 2020.
* [22] Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed. Universal multi-party poisoning attacks. In _International Conference on Machine Learning_, pages 4274-4283. PMLR, 2019.
* [23] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant sgd by inner product manipulation. In _Uncertainty in Artificial Intelligence_, pages 261-270. PMLR, 2020.
* [24] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In _2018 IEEE Symposium on Security and Privacy (SP)_, pages 19-35. IEEE, 2018.
* [25] Luis Munoz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In _Proceedings of the 10th ACM workshop on artificial intelligence and security_, pages 27-38, 2017.
* [26] Xueluan Gong, Yanjiao Chen, Qian Wang, and Weihan Kong. Backdoor attacks and defenses in federated learning: State-of-the-art, taxonomy, and future directions. _IEEE Wireless Communications_, 2022.