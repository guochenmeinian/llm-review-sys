# FedFed: Feature Distillation against Data Heterogeneity in Federated Learning

 Zhiqin Yang\({}^{1,2}\) Yonggang Zhang\({}^{2}\) Yu Zheng\({}^{3}\) Xinmei Tian\({}^{5}\)

**Hao Peng\({}^{1,6}\) Tongliang Liu\({}^{4}\) Bo Han\({}^{2}\)**

\({}^{1}\)Beihang University \({}^{2}\)Hong Kong Baptist University \({}^{3}\)Chinese University of Hong Kong

\({}^{4}\)Sydney AI Centre, The University of Sydney \({}^{5}\)University of Science and Technology of China

\({}^{6}\) Kunming University of Science and Technology

Equal contributions.Corresponding author (penghao@act.buaa.edu.cn)

###### Abstract

Federated learning (FL) typically faces data heterogeneity, i.e., distribution shifting among clients. Sharing clients' information has shown great potentiality in mitigating data heterogeneity, yet incurs a dilemma in preserving privacy and promoting model performance. To alleviate the dilemma, we raise a fundamental question: _Is it possible to share partial features in the data to tackle data heterogeneity?_ In this work, we give an affirmative answer to this question by proposing a novel approach called **Fed**erated **F**eature **d**istillation (FedFed). Specifically, FedFed partitions data into performance-sensitive features (i.e., greatly contributing to model performance) and performance-robust features (i.e., limitedly contributing to model performance). The performance-sensitive features are globally shared to mitigate data heterogeneity, while the performance-robust features are kept locally. FedFed enables clients to train models over local and shared data. Comprehensive experiments demonstrate the efficacy of FedFed in promoting model performance. The code is publicly available at: https://github.com/tmlr-group/FedFed

## 1 Introduction

Federated learning (FL), beneficial for training over multiple distributed data sources, has recently received increasing attention [1, 2, 3]. In FL, many clients collaboratively train a global model by aggregating gradients (or model parameters) without the need to share local data. However, the major concern is heterogeneity issues [4, 5, 6] caused by Non-IID distribution of distributed data and diverse computing capability across clients. The heterogeneity issues can cause unstable model convergence and degraded prediction accuracy, hindering further FL deployments in practice [1].

To address the heterogeneity challenge, the seminal work, federated averaging (FedAvg), introduces the model aggregation of locally trained models [4]. It addresses the diversity of computing and communication but still faces the issue of client drift induced by data heterogeneity [7]. Therefore, a branch of works for defending data heterogeneity has been explored by devising new learning objectives [6], aggregation strategies [8], and constructing shareable information across clients [9]. Among explorations as aforementioned, sharing clients' information has been considered to be a straightforward and promising approach to mitigate data heterogeneity [9, 10].

However, the dilemma of preserving data privacy and promoting model performance hinders the practical effectiveness of the information-sharing strategy. Specifically, it shows that a limited amount of shared data could significantly improve model performance [9]. Unfortunately, no matter for sharing raw data, synthesized data, logits, or statistical information [10, 11, 12, 7] canincur privacy concerns [13, 14, 15]. Injecting random noise to data provides provable security for protecting privacy [16, 17]. Yet, the primary concern for applying noise to data lies in performance degradation [18] as the injected noise negatively contributes to model performance. Consequently, effectively fulfilling the role of shared information necessitates addressing the dilemma of data privacy and model performance.

We revisit the purpose of sharing information to alleviate the dilemma in information sharing and performance improvements and ask:

_Q.1 Is it possible to share partial features in the data to mitigate heterogeneity in FL?_ This question is fundamental to reaching a new phase for mitigating heterogeneity with the information-sharing strategy. Inspired by the partition strategy of spurious feature and robust feature [19], the privacy and performance dilemma could be solved if the data features were separated into performance-robust and performance-sensitive parts without overlapping, where the performance-robust features contain almost all the information in the data. Namely, the performance-robust features that limitedly contribute to model performance are kept locally. Meanwhile, the performance-sensitive features that could contribute to model generalization are selected to be shared across clients. Accordingly, the server can construct a global dataset using the shared performance-sensitive features, which enables clients to train their models over the local and shared data.

_Q.2 How to divide data into performance-robust features and performance-sensitive features?_ The question is inherently related to the spirit of the information bottleneck (IB) method [20]. In IB, ideal features should discard all information in data features except for the minimal sufficient information for generalization [21]. Concerning the information-sharing and performance-improvement dilemma, the features discarded in IB may contain performance-robust features, i.e., private information, thus, they are unnecessary to be shared for heterogeneity mitigation. Meanwhile, the performance-sensitive features contain information for generalization, which are the minimum sufficient information and should be shared across clients for heterogeneity mitigation. Therefore, IB provides an information-theoretic perspective of dividing data features for heterogeneity mitigation in FL.

_Q.3 What if performance-sensitive features contain private information?_ This question lies in the fact that a non-trivial information-sharing strategy should contain necessary data information to mitigate the issue of data heterogeneity. That is, the information-sharing strategy unavoidably causes privacy risks. Fortunately, we can follow the conventional style in applying random noise to protect performance-sensitive features because the noise injection approach can provide a de facto standard way for provable security [16]. Notably, applying random noise to performance-sensitive features differs from applying random noise for all data features. More specifically, sharing partial features in the data is more accessible to preserve privacy than sharing complete data features, which is fortunately consistent with our theoretical analysis, see Theorem 3.3 in Sec. 3.3.

**Our Solution.** Built upon the above analysis, we propose a novel framework, named **Fed**erated **F**eature **d**istillation (FedFed), to tackle data heterogeneity by generating and sharing performance-sensitive features. According to the question _Q.1_, FedFed introduces a competitive mechanism by decomposing data features \(\mathbf{x}\in\mathbb{R}^{d}\) with dimension \(d\) into performance-robust features \(\mathbf{x}_{r}\in\mathbb{R}^{d}\) and performance-sensitive features \(\mathbf{x}_{s}\in\mathbb{R}^{d}\), i.e., \(\mathbf{x}=\mathbf{x}_{r}+\mathbf{x}_{s}\). Then following the question _Q.2_, FedFed generates performance-robust features \(\mathbf{x}_{r}\) in an IB manner for data \(\mathbf{x}\). In line with _Q.3_, FedFed enables clients to securely share their protected features \(\mathbf{x}_{p}\) by applying random noise \(\mathbf{n}\) to performance sensitive features \(\mathbf{x}_{s}\), i.e., \(\mathbf{x}_{p}=\mathbf{x}_{s}+\mathbf{n}\), where \(\mathbf{n}\) is drawn from a Gaussian distribution \(\mathcal{N}(\mathbf{0},\sigma_{s}^{2}\mathbf{I})\) with variance \(\sigma_{s}^{2}\). To this end, the server can construct a global dataset to tackle data heterogeneity using the protected features \(\mathbf{x}_{p}\), enabling clients to train models over the local private and globally shared data.

We deploy FedFed on four popular FL algorithms, including FedAvg [4], FedProx [6], SCAF-FOLD [7], and FedNova [22]. Atop them, we conduct comprehensive experiments on various scenarios regarding different amounts of clients, varying degrees of heterogeneity, and four datasets. Extensive results show that the FedFed achieves considerable performance gains in all settings.

Our contributions are summarized as follows:

1. We pose a foundation question to challenge the necessity of sharing all data features for mitigating heterogeneity in FL with information-sharing strategies. The question sheds light on solving the privacy and performance dilemma, in a way of sharing partial features that contribute to data heterogeneity mitigation (Sec 3.1)2. To solve the dilemma in information sharing and performance improvements, we propose a new framework FedFed. In FedFed, each client performs feature distillation--partitioning local data into performance-robust and performance-sensitive features (Sec 3.2) --and shares the latter with random noise globally (Sec 3.3). Consequently, FedFed can mitigate data heterogeneity by enabling clients to train their models over the local and shared data.
3. We conduct comprehensive experiments to show that FedFed consistently and significantly enhances the convergence rate and generalization performance of FL models across different scenarios under various datasets (Sec 4.2).

## 2 Preliminary

**Federated Learning.** Federated learning allows multiple clients to collaboratively train a global model parameterized by \(\phi\) without exposing clients' data [4; 1]. In general, the global model aims to minimize a global objective function \(\mathcal{L}(\phi)\) over all clients' data distributions:

\[\min_{\phi}\mathcal{L}(\phi)=\sum_{k=1}^{K}\lambda_{k}\mathcal{L}_{k}(\phi_{k }),\] (1)

where \(K\) represents the total number of clients, \(\lambda_{k}\) is the weight of the \(k\)-th client. The local objective function \(\mathcal{L}_{k}(\phi_{k})\) of client \(k\) is defined on the distribution \(P(X_{k},Y_{k})\) with random variables \(X_{k},Y_{k}\):

\[\mathcal{L}_{k}(\phi_{k})\triangleq\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim P( X_{k},Y_{k})}\ell(\phi_{k};\mathbf{x},\mathbf{y}),\] (2)

where \(\mathbf{x}\) is input data with its label \(\mathbf{y}\) and \(\ell(\cdot)\) stands for the loss function, e.g., cross-entropy loss. Due to the distributed property of clients' data, the global objective function \(\mathcal{L}(\phi)\) is optimized round-by-round. Specifically, within the \(r\)-th communication round, a set of clients \(C\) are selected to perform local training on their private data, resulting in \(|C|\) optimized models \(\{\phi_{k}^{r}\}_{k=1}^{|C|}\). These optimized models are then sent to a central server to derive a global model for the \((r+1)\)-th communication round by an aggregation mechanism \(AGG(\cdot)\), which may vary from different FL algorithms:

\[\phi^{r+1}=AGG(\{\phi_{k}^{r}\}_{k=1}^{|C|}).\] (3)

**Differential Privacy.** Differential privacy [16] is a framework to quantify to what extent individual privacy in a dataset is preserved while releasing the data.

**Definition 2.1**.: _(Differential Privacy). A randomized mechanism \(\mathcal{M}\) provides \((\epsilon,\delta)\)-differential privacy (DP) if for any two neighboring datasets \(D\) and \(D^{\prime}\) that differ in a single entry, \(\forall S\subseteq Range(\mathcal{M})\),_

\[\Pr(\mathcal{M}(D)\in S)\leq e^{\epsilon}\cdot\Pr(\mathcal{M}(D^{\prime})\in S )+\delta.\]

_where \(\epsilon\) is the privacy budget and \(\delta\) is the failure probability._

The definition of \((\epsilon,\delta)\)-DP shows the difference of two neighboring datasets in the probability that the output of \(\mathcal{M}\) falls within an arbitrary set \(S\) is related to \(\epsilon\) and an error term \(\delta\). Similar outputs of \(\mathcal{M}\) on \(D,D^{\prime}\) (i.e., smaller \(\epsilon\)) represent a stronger privacy guarantee. In sum, DP as a de facto standard of quantitative privacy, provides provable security for protecting privacy.

**Information Bottleneck.** Traditionally, in the context of the Information Bottleneck (IB) framework, the goal is to effectively capture the information relevant to the output label \(Y\), denoted as \(Z\), while simultaneously achieving maximum compression of the input \(X\). \(Z\) represents the latent embedding, which serves as a compressed and informative representation of \(X\), preserving the essential information while minimizing redundancy. This objective can be formulated as:

\[\mathcal{L}_{\text{IB}}=I(X;Y|Z),\quad s.t.\ I(X;Z)\leq I_{\text{IB}}\] (4)

where \(I(\cdot)\) denote the mutual information and \(I_{\text{IB}}\) is a constant. This function is defined as a rate-distortion problem, indicating that IB is to extract the most efficient and informative features.

## 3 Methodology

We detail the Federated Feature distillation (FedFed) proposed to mitigate data heterogeneity in FL. FedFed adopts the information-sharing strategy with the spirits of information bottleneck (IB). Roughly, it shares the minimal sufficient features, while keeping other features at clients1.

### Motivation

Data heterogeneity inherently comes from the difference in data distribution among clients. Aggregating models without accessing data would inevitably bring performance degradation. Sharing clients' data benefits model performance greatly, but intrinsically violates privacy. Applying DP to protect shared data looks feasible; however, it is not a free lunch with paying the accuracy loss on the model.

Draw inspiration from the content and style partition of data causes [19], we investigate the dilemma in information sharing and performance improvements through a feature partition perspective. By introducing an appropriate partition, we can share performance-sensitive features while keeping performance-robust features locally. Namely, _performance-sensitive features in the data are all we need for mitigating data heterogeneity!_

We will elucidate the definitions of performance-sensitive features and performance-robust features. Firstly, we provide a precise definition of a valid partition (Definition 3.1), which captures the desirable attributes when partitioning features. Subsequently, adhering to the rules of a valid partition, we formalize the two types of features (Definition 3.2).

**Definition 3.1**.: _(Valid Partition). A partition strategy is to partition a variable \(X\) into two parts in the same measure space such that \(X=X_{1}+X_{2}\). We say the partition strategy is valid if it holds: (i) \(H(X_{1},X_{2}|X)=0\); (ii) \(H(X|X_{1},X_{2})=0\); (iii) \(I(X_{1};X_{2})=0\); where \(H(\cdot)\) denotes the information entropy and \(I(\cdot)\) is the mutual information._

**Definition 3.2**.: _(Performance-sensitive and Performance-robust Features). Let \(X=X_{s}+X_{r}\) be a valid partition strategy. We say \(X_{s}\) are performance-sensitive features such that \(I(X;Y|X_{s})=0\), where \(Y\) is the label of \(X\). Accordingly, \(X_{r}\) are the performance-robust features._

Intuitively, performance-sensitive features contain all label information, while performance-robust features contain all information about the data except for the label information. More discussions about Definition 3.1 and Definition 3.2 can be found in Appendix C.1. Now, the challenge turns out to be: Can we derive performance-sensitive features to mitigate data heterogeneity? FedFed provides an affirmative answer from an IB perspective, boosting a simple yet effective data-sharing strategy.

### Feature Distillation

Given the motivation above, we propose to distil features such that data features can be partitioned into performance-robust features depicting mostly data and performance-sensitive features favourable to model performance. We design feature distillation and answer Q.2 below.

We draw inspiration from the information bottleneck method [21]. Specifically, only minimal sufficient information is preserved in learned representations while the other features are dismissed [21]. This can be formulated as 2:

Footnote 2: We follow the formulation used in Tishby and Zaslavsky [21, 23].

\[\min_{Z}I(X;Y|Z),\text{ s.t. }I(X;Z)\leq I_{\text{IB}},\] (5)

where \(Z\) represents the desired representation extracted from input \(X\) with its label \(Y\), \(I(\cdot)\) is the mutual information, and \(I_{\text{IB}}\) stands for a constant. Namely, given the learned representation \(Z\), the mutual information between \(X\) and \(Y\) is minimized. Meanwhile, \(Z\) dismisses most information about the input \(X\) so that the mutual information \(I(X;Z)\) is less than a constant \(I_{\text{IB}}\).

Similarly, in FedFed, only minimal sufficient information is necessary to be shared across clients to mitigate data heterogeneity, while the other features are dismissed before sharing data, i.e., kept locally on the client. The differences between IB and FedFed have two folds: 1) FedFed aims to make

Figure 1: Have a Guessing Game! Question: Which one is the first image from? A or B or C?the dismissed features close to the original features (i.e., depict most private data information), while IB focuses on the dissimilarity between the preserved and original features (i.e., contain minimal sufficient information about data); 2) FedFed dismisses information in the data space while IB does that in the representation space. More specifically, the objective of feature distillation is:

\[\min_{Z}I(X;Y|Z),\text{ {s.t.} }I(X;X-Z|Z)\geq I_{\text{FF}},\] (6)

where \(Z\) represents the performance-sensitive features in \(X\), \(Y\) is the label of \(X\), \((X-Z)\) stands for performance-robust features that are unnecessary to be shared, and \(I_{\text{FF}}\) is a constant. In Eq. (6), the \((X-Z)\) should represent data mostly conditioned on performance-sensitive features \(Z\), while the \(Z\) should contain necessary information about the label \(Y\). Consequently, learning with the objective can divide data features into performance-sensitive features and performance-robust features, achieving the goal of feature distillation.

To make the feature distillation tractable, we derive an objective equal to the original objective in Eq. (6) (see Appendix C.2 for more details) for client \(k\) as follows:

\[\min_{\theta}-\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim P(X_{k},Y_{k})}\log p( \mathbf{y}|\mathbf{z}(\mathbf{x};\theta)),\text{ {s.t.} }\|\mathbf{z}(\mathbf{x};\theta)\|_{2}^{2}\leq\rho,\] (7)

where \(\theta\) is the parameter to be optimized for generating performance-sensitive features \(\mathbf{z}(\cdot;\theta)\), \((\mathbf{x},\mathbf{y})\) represents the input pair drawn from the joint distribution \(P(X_{k},Y_{k})\) of client \(k\), \(p(\mathbf{y}|\cdot)\) is the probability of predicting \(Y=\mathbf{y}\), and \(\rho>0\) stands for a constant. The underlying insight of the objective function in Eq. (7) is intuitive. Specifically, the learned performance-sensitive features \(\mathbf{z}(\cdot;\theta)\) are capable of predicting the label \(\mathbf{y}\) while having the minimal \(\ell_{2}\)-norm.

Unfortunately, the original formulation in Eq. (7) can hardly be used for dividing data features. This is because the feature distillation in Eq. 7 cannot make the preserved features \(\mathbf{x}-\mathbf{z}(\mathbf{x};\theta)\) similar to the raw features \(\mathbf{x}\). Namely, the original Eq. (7) cannot guarantee the desired property of the preserved features. To solve the problem, we propose an explicit competition mechanism in the data space. Specifically, we model preserved features, i.e., performance-robust features, with \(q(\mathbf{x};\theta)\) explicitly and model the performance-sensitive features \(\mathbf{z}(\mathbf{x};\theta)\triangleq\mathbf{x}-q(\mathbf{x};\theta)\) implicitly:

\[\min_{\theta}-\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim P(X_{k},Y_{k})}\log p( \mathbf{y}|\mathbf{x}-q(\mathbf{x};\theta)),\text{ {s.t.} }\|\mathbf{x}-q(\mathbf{x};\theta)\|_{2}^{2}\leq\rho.\] (8)

Thus, by Eq. 8, performance-sensitive features can predict labels and performance-robust features are almost the same as raw features.

The realization of Eq. (8) is straightforward. To be specific, we can employ a generative model parameterized with \(\theta\) to generate performance-robust features, i.e., \(q(\mathbf{x};\theta)\) in Eq. (8). Meanwhile, we train a local classifier \(f(\cdot;\mathbf{w}_{k})\) parameterized with \(\mathbf{w}_{k}\) for client-\(k\) to model the process of predicting labels, i.e., \(-\log p(\mathbf{y}|\cdot)\) in Eq. (8). Accordingly, the realization of feature distillation is formulated as follows with \(\mathbf{z}(\mathbf{x};\theta)\triangleq\mathbf{x}-q(\mathbf{x};\theta)\):

\[\min_{\theta,\mathbf{w}_{k}}-\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim P(X_{k},X_{k})}\ell(f(\mathbf{z}(\mathbf{x};\theta);\mathbf{w}_{k}),\mathbf{y}), \text{ {s.t.}}\|\mathbf{z}(\mathbf{x};\theta)\|_{2}^{2}\leq\rho,\] (9)

where \(\ell(\cdot)\) is the cross-entropy loss, \(q(\mathbf{x};\theta)\) stands for a generative model, and \(\rho\) represents a tunable hyper-parameter. Built upon Eq. (9), we can perform feature distillation, namely, the outputs of a generator \(q(\mathbf{x};\theta)\) serve as performance-robust features and \(\mathbf{x}-q(\mathbf{x};\theta)\) are used as performance-sensitive features. We merely share \(\mathbf{x}-q(\mathbf{x};\theta)\) to tackle data heterogeneity by training models over both local and shared data. Algorithm 1 summarizes the procedure of feature distillation.

### Protection for Performance-Sensitive Features

Until now, we have intentionally overlooked the overlap between performance-sensitive features and performance-robust features. Since we prioritize data heterogeneity, overlapping is almost unavoidable in practice, i.e., performance-sensitive features containing certain data privacy. Accordingly, merely sharing performance-sensitive features can risk privacy. Thus, we answer Q. 3 below.

**Why employ DP?** The constructed performance-sensitive features may contain individual privacy, thus, the goal of introducing a protection approach is for individual privacy. In addition, the employed protection approach is expected to be robust against privacy attacks [24; 25]. According to the above analysis, differential privacy (DP) is naturally suitable in our scenario of feature distillation. Thus, we employ DP to protect performance-sensitive features before sending them to the server.

**Algorithm 1** Feature Distillation

**Algorithm 2** Feature Distillation

**How to apply DP?** Applying noise (e.g., Gaussian or Laplacian) to performance-sensitive features before sharing can protect features with DP guarantee [26, 16], i.e., \(\mathbf{x}_{p}=\mathbf{x}_{s}+\mathbf{n}\). Consequently, the server can collect protected features \(\mathbf{x}_{p}\) from clients to construct a global dataset and send the dataset back to clients. To give a vivid illustration, we provide a guessing game3 in Figure 1, showing \(4\) images. The first image represents the DP-protected performance-sensitive features of one image (A, B, or C). It is hard to identify which one of the raw images (A, B, C) the first image belongs to.

Footnote 3: We reveal the answer in Appendix F.2

**Training with DP.** The protected performance-sensitive features are shared, thus, the server can construct a globally shared dataset. Using the global dataset, clients can train local classifier \(f(\cdot;\phi_{k})\) parameterized by \(\phi_{k}\) with the private and shared data:

\[\min_{\phi_{k}}\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim P(X_{k},Y_{k})}\ell(f( \mathbf{x};\phi_{k}),\mathbf{y})+\mathbb{E}_{(\mathbf{x}_{p},\mathbf{y})\sim P (\mathbf{h}(X_{k}),Y_{k})}\ell(f(\mathbf{x}_{p};\phi_{k}),\mathbf{y}),\] (10)

where \(f(\cdot;\phi_{k})\) is trained for model aggregation, \((\mathbf{x}_{p},\mathbf{y})\) are protected performance-sensitive features that are collected from all clients, and \(\mathbf{h}(X_{k}):=\mathbf{z}(X_{k})+n\) with noise \(n\). Pseudo-code of how to apply FedFed are listed in Appendix B.

**DP guarantee.** Following DP-SGD [27], we employ the idea of the \(\ell_{2}\)-norm clipping relating to the selection of the noise level \(\sigma\). Specifically, we realize the constraint in Eq. (9) as follows: \(\|\mathbf{z}(\mathbf{x};\theta)\|_{2}\leq\rho\|\mathbf{x}\|\), with \(0<\rho<1\) (i.e., \(\|\mathbf{x}_{s}\|\leq\rho\|\mathbf{x}\|\)). For aligning analyses with conventional DP guarantee, we let the random mechanism on performance-robust features \(\mathbf{x}_{r}\) be \(\mathbf{x}_{r}+n,n\sim\mathcal{N}(0,\sigma_{r}^{2}\mathbf{I})\). Similar for performance-sensitive features, we have \(\mathbf{x}_{s}+n,n\sim\mathcal{N}(0,\sigma_{s}^{2}\mathbf{I})\). Since \(\mathbf{x}_{r}\) is kept locally, an adversary views nothing (or random data without any entropy). This keeps equal to adding a sufficiently large noise, i.e., \(\sigma_{r}=\infty\) on \(\mathbf{x}_{r}\) to make it random enough. Consequently, FedFed can provide a strong privacy guarantee. Moreover, for each client, we show that FedFed requires a relatively small noise level \(\sigma\) for achieving identical privacy, which is given in Theorem 3.3 (detailed analysis is left in Appendix D).

**Theorem 3.3**.: _Let \(\sigma\) be the noise scale for FedFed and \(\sigma^{\prime}\) be the noise scale for sharing raw \(\mathbf{x}\). Given identical \(\epsilon,\delta\), we attain \(\sigma<\sigma^{\prime}\) such that \(\sigma\propto\|\mathbf{x}_{s}\|^{2}\)._

Besides each client, we give the privacy analysis for the FL system paired with the proposed FedFed.

**Theorem 3.4** (Composition of FedFed).: _For \(k\) clients with \((\epsilon,\delta)\)-differential privacy, FedFed satisfies \((\hat{\epsilon}_{\hat{\delta}},1-(1-\hat{\delta})\Pi_{i}(1-\hat{\delta}_{i}))\)-differential privacy,

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{centralized training ACC = 95.64\% w/(w/o) **FedFed**} \\ \cline{2-7}  & ACC\(\uparrow\) & Gain\(\uparrow\) & Round \(\downarrow\) & Speedup\(\uparrow\) & ACC\(\uparrow\) & Gain\(\uparrow\) & Round \(\downarrow\) & Speedup\(\uparrow\) \\ \hline FedAvg & **92.34**(86.73) & 5.61\(\uparrow\) & **14**(121) \(\times\)**8.6**(\(\times\)1.0) & **90.69**(78.34) & 12.35\(\uparrow\) & **16**(420) \(\times\)**26.3**(\(\times\)1.0) \\ FedProx & **92.09**(87.73) & 4.36\(\uparrow\) & **32**(129) \(\times\)**2.1**(\(\times\)0.9) & **89.68**(82.03) & 7.65\(\uparrow\) & **16**(44) \(\times\)**26.3**(9.5) \\ SCAFFOLD & **91.62**(86.31) & 3.89\(\uparrow\) & **29**(147) \(\times\)**4.2**(\(\times\)0.8) & **80.48**(76.63) & 3.85\(\uparrow\) & **139**(None) \(\times\)**6.2**(None) \\ FedNova & **92.39**(87.03) & 5.36\(\uparrow\) & **18**(88) \(\times\)**6.7**(\(\times\)1.4) & **89.72**(79.98) & 9.74\(\uparrow\) & **16**(531) \(\times\)**26.3**(\(\times\) 0.8) \\ \hline \hline  & \multicolumn{6}{c}{\(\alpha=0.1,E=5,K=10\) (Target ACC =87\%)} & \multicolumn{6}{c}{\(\alpha=0.1,E=1,K=100\) (Target ACC =90\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =97\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =97\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.1,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.1,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.1,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.05,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =78\%)} & \multicolumn{6}{c}{\(\alpha=0.1,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.1,E=1,K=10\) (Target ACC =86\%)} & \multicolumn{6}{c}{\(\alpha=0.1,E=1,K=100\) (Target ACC =78\%)} \\ \cline{2-7}  & \multicolumn{1}{c}{\(\alpha=0.

**Models, Metrics and Baselines.** We use ResNet-18 [35] both in the feature distillation and classifier in FL. We evaluate the model performance via two popular metrics in FL: a) communication rounds and b) best accuracy. Typically, the target accuracy is set to be the best accuracy of vanilla FedAvg. As a plug-in approach, we apply FedFed to prevailing existing FL algorithms, such as FedAvg [4], FedProx [6], SCAFFOLD [7], and FedNova [22] to compare the efficiency of our method. More experimental settings and details are listed in Appendix B.1.

### Main Results

**Generative Model Selection.** We consider two generative models in this paper to distill raw data, i.e., Resnet Generator, a kind of vanilla generator used in many works [36; 37], and \(\beta\)-VAE [38], an encoder-decoder structure by variational inference. We verify the effectiveness of FedFed with different generative models. According to results shown in Figure 2 (a), we observe that \(\beta\)-VAE gets better performance in FedFed. Thus, we mainly report the results of \(\beta\)-VAE in the rest.

**Result Analysis.** The experimental results on CIFAR-10, CIFAR-100, FMNIST, and SVHN are shown respectively in Tables 1, 2, 3, and 4. It can be seen that the proposed method can consistently and significantly improve model accuracy under various settings. Moreover, FedFed can promote the convergence rate of different algorithms4. Specifically, FedFed brings significant performance gain in CIFAR-10 under the \(K=100\) setting shown in Table 1 and also notably speeds up the convergence rate in FMNIST under the \(\alpha=0.05,E=1,K=10\) setting. However, attributable to the original FL methods almost reaching a performance bottleneck, FedFed achieves limited performance gain in SVHN and FMNIST. FedFed also has limited improvement in CIFAR-100. A possible reason

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{centralized training ACC = 96.56\% w/(w/o) **FedFed**} \\ \cline{2-9}  & ACC\(\uparrow\) & Gain\(\uparrow\) & Round \(\downarrow\) & Speedup\(\uparrow\) & ACC\(\uparrow\) & Gain\(\uparrow\) & Round \(\downarrow\) & Speedup\(\uparrow\) \\ \cline{2-9}  & \(\alpha=0.1,E=1,K=10\) & (Target ACC =88\%) & \(\alpha=0.05,E=1,K=10\) & **137**(503) & \(\times\)**3.7(\(\times 1.0\))** \\ FedProx & **90.02**(65.34) & 4.68\(\uparrow\) & **233**(None) & \(\times\)**2.1(None)** & **69.03**(61.29) & 7.74\(\uparrow\) & **141**(485) & \(\times\)**3.6(\(1.0\))** \\ SCAFFOLD & **70.14**(67.23) & 2.91\(\uparrow\) & **198**(769) & \(\times\)**2.5(\(\times 0.6\))** & **69.32**(58.78) & 10.54\(\uparrow\) & **81**(None) & \(\times\)**6.2**(None) \\ FedNova & **70.48**(67.98) & 2.57 & **147**(432) & \(\times\)**3.4(\(\times 1.1\))** & **68.92**(60.53) & 8.39\(\uparrow\) & **87**(None) & \(\times\)**5.8**(None) \\ \hline \hline  & \(\alpha=0.1,E=5,K=10\) & (Target ACC =69\%) & \(\alpha=0.1,E=1,K=100\) & (Target ACC =88\%) \\ \hline FedAvg & **70.96**(69.34) & 1.62\(\uparrow\) & **79**(276) & \(\times\)**3.5(\(\times 1.0\))** & **60.58**(48.21) & 12.37\(\uparrow\) & **448**(967) & \(\times\)**2.2**(\(\times 1.0\)) \\ FedProx & **69.66**(62.32) & 7.34\(\uparrow\) & **285**(None) & \(\times\)**1.0(None)** & **67.69**(48.78) & 18.91\(\uparrow\) & **200**(932) & \(\times\)**4.8**(\(\times 1.0\)) \\ SCAFFOLD & **70.76**(70.23) & 0.53\(\uparrow\) & **108**(174) & \(\times\)**2.6**(\(\times 1.6\)) & **66.67**(51.03) & 15.64\(\uparrow\) & **181**(832) & \(\times\)**5.3**(\(\times 1.2\)) \\ FedNova & **69.98**(69.78) & 0.2\(\uparrow\) & **89**(290) & \(\times\)**3.1**(\(\times 1.0\)) & **67.62**(48.03) & 19.59\(\uparrow\) & **198**(976) & \(\times\)**4.9**(\(\times 1.0\)) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Top-1 accuracy with(without) FedFed under different heterogeneity degree, local epochs, and clients number on CIFAR-100.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{centralized training ACC = 75.56\% w/(w/o) **FedFed**} \\ \cline{2-9}  & ACC\(\uparrow\) & Gain\(\uparrow\) & Round \(\downarrow\) & Speedup\(\uparrow\) & ACC\(\uparrow\) & Gain\(\uparrow\) & Round \(\downarrow\) & Speedup\(\uparrow\) \\ \cline{2-9}  & \(\alpha=0.1,E=1,K=10\) & (Target ACC =67\%) & \(\alpha=0.05,E=1,K=10\) & (Target ACC =61\%) \\ \hline FedAvg & **69.64**(67.84) & 1.8\(\uparrow\) & **283**(495) & \(\times\)**1.7**(\(\times 1.0\)) & **68.49**(62.01) & 6.48\(\uparrow\) & **137**(503) & \(\times\)**3.7**(\(\times 1.0\)) \\ FedProx & **70.02**(65.34) & 4.68\(\uparrow\) & **233**(None) & \(\times\)**2.1**(None) & **69.03**(61.29) & 7.74\(\uparrow\) & **141**(485) & \(\times\)**3.6**(1.0) \\ SCAFFOLD & **70.14**(67.23) & 2.91\(\uparrow\) & **198**(769) & \(\times\)**2.5**(\(\times 0.6\)) & **69.32**(58.78) & 10.54\(\uparrow\) & **81**(None) & \(\times\)**6.2**(None) \\ FedNova & **70.48**(67.98) & 2.5\(\uparrow\) & **147**(432) & \(\times\)**3.4**(\(\times 1.1\)) & **68.92**(60.53) & 8.39\(\uparrow\) & **87**(None) & \(\times\)**5.8**(None) \\ \hline \hline  & \(\alpha=0.1,E=5,K=10\) & (Target ACC =69\%) & \(\alpha=0.1,E=1,K=100\) & (Target ACC =88\%) \\ \hline FedAvg & **70.96**(69.34) & 1.62\(\uparrow\) & **79**(276) & \(\times\)**3.5**(\(\times 1.0\)) & **60.58**(48.21) & 12.37\(\uparrow\) & **448**(967) & \(\times\)**2.2**(\(\times 1.0\)) \\ FedProx & **69.66**(62.32) & 7.34\(\uparrow\) & **285**(None) & \(\times\)**1.0**(None) & **67.69**(48.78) & 18.91\(\uparrow\) & **200**(932) & \(\times\)**4.8**(\(\times 1.0\)) \\ SCAFFOLD & **70.76**(70.23) & 0.53\(\uparrow\) & **108**(174) & \(\times\)**2.6**(\(\times 1.6\)) & **66.67**(51.03) & 15.64\(\uparrow\) & **181**(832) & \(\times\)**5.3**(\(\times 1.2\)) \\ FedNova & **69.98**(69.78) & 0.2\(\uparrow\) & **89**(290) & \(\times\)**3.1**(\(\times 1.0\)) & **67.62**(48.03) & 19.59\(\uparrow\) & **198**(976) & \(\times\)**4.9**(\(\times 1.0\)) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Top-1 accuracy with(without) FedFed under different heterogeneity degree, local epochs, and clients number on SVHN.

is that existing methods can achieve performance comparable to centralized training. Moreover, we conducted an additional experiment involving sharing the full data with DP protection. The results, presented in Appendix F.3, indicate that sharing the full data with DP protection leads to a degradation in the performance of the FL system. This degradation occurs because protecting the full data necessitates a relatively large noise to achieve the corresponding protection strength required to safeguard performance-sensitive features.

**Surprising Observations.** We find that training among \(100\) clients in CIFAR-10 and CIFAR100 reaches a significant improvement (e.g., at most \(40.67\%\)!). A possible reason is that the missed data knowledge can be well replenished by FedFed. Moreover, all methods paired with FedFed under various settings can achieve similar prediction accuracies, demonstrating that FedFed endows FL models robustness against data heterogeneity. Table 5 shows that two kinds of heterogeneity partition cause more performance decline than LDA (\(\alpha=0.1\)). Yet, FedFed attains noteworthy improvement, indicating the robustness against Non-IID partition.

### Ablation Study

**DP Noise.** To explore the relationship between privacy level \(\epsilon\) and prediction accuracy, we conduct experiments with different noise levels \(\sigma_{s}^{2}\). As shown in Figure 2(c), the prediction accuracy decreases with increasing noise level (more results in Appendix F.5). To verify whether the FedFed is robust against the selection of noise, we also consider Laplacian noise in applying DP to privacy protection. The results of Laplacian noise are reported in Table 6, demonstrating the robustness of FedFed.

**Hyper-parameters.** We further evaluate the robustness of FedFed against hyper-parameters. During the feature distillation process, as the constraint parameter \(\rho\) in Eq. 9 decreases, the DP strength to protect performance-sensitive features decreases, and the performance of the global model decreases, owing to the less information contained in performance-sensitive features.

### Privacy Verification

Besides the theoretical analysis, we provide empirical analysis to support the privacy guarantee of FedFed. We wonder whether the globally shared data can be inferred by some attacking methods. Thus, we resort to model inversion attack [39], widely used in the literature to reconstruct data. The results5 in Figure 3 (a) and (b) indicate that FedFed could protect globally shared data. We also conduct another model inversion attack [40] and the results can be found in Appendix E.1.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Noise Type} & \multicolumn{4}{c}{Test Accuracy on Different Noise with **FedFed**} \\ \cline{2-5}  & FedAvg & FedProx & SCAFFOLD & FedNova \\ \hline Gaussian Noise & 92.34 & 92.12 & 89.66 & 92.23 \\ Laplacian Noise & 92.30 & 91.36 & 91.24 & 91.73 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Experiment results with different noise adding on CIFAR-10.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Partition} & \multicolumn{4}{c}{Test Accuracy on (w/oo) **FedFed**} \\ \cline{2-5}  & FedAvg & FedProx & SCAFFOLD & FedNova \\ \hline \(a=0.1\) & **92.34**(79.35) & **92.12**(83.06) & **89.66**(83.67) & **92.23**(80.95) \\ \(\#C=2\) & **89.23**(42.54) & **88.17**(758.45) & **84.43**(36.82) & **89.54**(45.42) \\ Subset & **90.29**(95.93) & **89.11**(32.87) & **89.92**(52.96) & **90.00**(38.52) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Experiment results of different Non-IID partition methods on CIFAR-10 with 10 clients.

Figure 2: More facts of FedFed. (a) Convergence rate of different generative models (i.e., \(\beta\)-VAE[38] and ResNet generator[37]) compared with vanilla FedAvg. (b) Test accuracy and convergence rate on different federated learning algorithms with or without FedFed under \(\alpha=0.1,E=1,K=100\). (c) Test accuracy on FMNIST with different noise level \(\sigma_{s}^{2}\) in Theorem 3.3, obtaining various privacy \(\epsilon\) (lower \(\epsilon\) is preferred). As the noise increased, the level of protection gradually increased.

Additionally, we perform membership inference attack [41] to illustrate the difference between FedFed and sharing all data features with DP protection. The results illustrated in Figure 3 (c) show that noise level \(\sigma^{2}=0.3\) over raw data \(\mathbf{x}\) can achieve similar protection as noise \(\sigma_{s}^{2}=0.15\) over globally shared data \(\mathbf{x}_{p}\), which aligns with Theorem 3.3. More details can be found in Appendix E.2

## 5 Related Work

Federated Learning (FL) models typically perform poorly when meeting severe Non-IID data [2; 7]. To tackle data heterogeneity, advanced works introduce various learning objectives to calibrate the updated direction of local training from being far away from the global model, e.g., FedProx[6], FedIR [42], SCAFFOLD [7], and MOON [43]. Designing model aggregation schemes like FedAvgM [33], FedNova [22], FedMA [44], and FedBN [45] shows the efficacy on heterogeneity mitigation. Another promising direction is the information-sharing strategy, which mainly focuses on synthesizing and sharing clients' information to mitigate heterogeneity [9; 46; 47]. To avoid exposing privacy caused by the shared data, some methods utilize the statistics of data [48], representations of data [12], logits [49; 10], embedding [50]. However, advanced attacks pose potential threats to methods with data-sharing strategies [51].

FedFed is inspired by [34], where pure noise is shared across clients to tackle data heterogeneity. In this work, we relax the privacy concern by sharing partial features in the data with DP protection. In addition, our work is technically similar to an adversarial learning approach [52]. Our method distinguishes by defining various types of features and delving into the exploration of data heterogeneity within FL. More discussion can be found in Appendix G.

## 6 Conclusion

In this work, we propose a novel framework called FedFed to tackle data heterogeneity in FL by employing the promising information-sharing approach. Our work extends the research line of constructing shareable information of clients by proposing to share partial features in data. This shares a new route of improving training performance and maintaining privacy. Furthermore, FedFed has served as a source of inspiration for a new direction focused on improving performance in open-set scenarios [53]. Another avenue of exploration involves deploying FedFed in other real-time FL application scenarios, such as recommendation systems [54] and healthcare system [55], to uncover its potential benefits.

**Limitation.** In reality, limited local hardware resources may limit the power of FedFed, since FedFed introduces some extra overheads like communication and storage overheads. We leave it as future works to explore a hardware-friendly version or real-world application [56; 57]. Additionally, FedFed raises potential privacy concerns that we leave as a future research exploration, such as integrating cryptography [58; 59]. We anticipate that our work will inspire further investigations to comprehensively evaluate the privacy risks associated with information-sharing strategies aimed at mitigating data heterogeneity.

Figure 3: Attack results on FedFed. (a) shows the protected data \(\mathbf{x}_{p}\). (b) reports the model inversion attacked data. (c) shows results of membership inference attacks: the green star represents the recall for FedFed, while the blue stars show the searching process varying with \(\sigma_{s}^{2}\) for sharing raw data.

## Acknowledgments and Disclosure of Funding

We thank the reviewers for their valuable comments. Hao Peng was supported by the National Key R&D Program of China through grant 2021YFB1714800, NSFC through grants 62002007, U21B2027, 61972186, and 62266027, S&T Program of Hebei through grant 20310101D, Natural Science Foundation of Beijing Municipality through grant 422030, and the Fundamental Research Funds for the Central Universities, Yunnan Provincial Major Science and Technology Special Plan Projects through grants 202103AA080015, 202202AD080003 and 202303AP140008, General Projects of Basic Research in Yunnan Province through grant 202301AS070047. Yonggang Zhang and Bo Han were supported by the NSFC Young Scientists Fund No. 62006202, NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, CCF-Baidu Open Fund, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. Tongliang Liu was partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031. Xinmei Tian was supported in part by NSFC No. 6222117.

## References

* [1] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [2] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: vision, hype and reality for data privacy and protection. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [3] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 10(2):1-19, 2019.
* [4] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* [5] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. In _2022 IEEE 38th International Conference on Data Engineering (ICDE)_, pages 965-978. IEEE, 2022.
* [6] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine Learning and Systems_, 2:429-450, 2020.
* [7] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* [8] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In _International Conference on Machine Learning_, pages 7252-7261. PMLR, 2019.
* [9] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. _arXiv preprint arXiv:1806.00582_, 2018.
* [10] Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity: Classifier calibration for federated learning with non-iid data. _Advances in Neural Information Processing Systems_, 34:5972-5984, 2021.
* [11] Jack Goetz and Ambuj Tewari. Federated learning via synthetic data. _arXiv preprint arXiv:2008.04489_, 2020.

* Hao et al. [2021] Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen, and Lawrence Carin Duke. Towards fair federated learning with zero-shot data augmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3310-3319, 2021.
* Nasr et al. [2019] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In _2019 IEEE symposium on security and privacy (SP)_, pages 739-753. IEEE, 2019.
* Mothukuri et al. [2021] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali Dehghantanha, and Gautam Srivastava. A survey on security and privacy of federated learning. _Future Generation Computer Systems_, 115:619-640, 2021.
* Yan et al. [2022] Hongyang Yan, Shuhao Li, Yajie Wang, Yaoyuan Zhang, Kashif Sharif, Haibo Hu, and Yuanzhang Li. Membership inference attacks against deep learning models via logits distribution. _IEEE Transactions on Dependable and Secure Computing_, 2022.
* Dwork et al. [2014] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Du et al. [2023] Minxin Du, Xiang Yue, Sherman S. M. Chow, Tianhao Wang, Chenyu Huang, and Huan Sun. Dp-forward: Fine-tuning and inference on language models with differential privacy in forward pass. In _CCS_, 2023.
* Tramer and Boneh [2021] Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more data). In _ICLR_, 2021.
* Zhang et al. [2021] Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Scholkopf, and Kun Zhang. Adversarial robustness through the lens of causality. In _International Conference on Learning Representations_, 2021.
* Tishby et al. [1999] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. In _Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing_, 1999.
* Shwartz-Ziv and Tishby [2022] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. _Information Flow in Deep Neural Networks_, page 24, 2022.
* Wang et al. [2020] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. _Advances in neural information processing systems_, 33:7611-7623, 2020.
* Tishby and Zaslavsky [2015] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In _2015 ieee information theory workshop (itw)_, pages 1-5. IEEE, 2015.
* Shokri et al. [2017] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _S&P_, 2017.
* Zhang et al. [2021] Wanrong Zhang, Shruti Tople, and Olga Ohrimenko. Leakage of dataset properties in multi-party machine learning. In _USS_, 2021.
* Blum et al. [2008] Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to non-interactive database privacy. In _Proceedings of the 40th Annual ACM Symposium on Theory of Computing_, pages 609-618, 2008.
* Abadi et al. [2016] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* Kairouz et al. [2015] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _JMLR Workshop and Conference Proceedings_, pages 1376-1385, 2015.

* [29] Jie Zhang, Zhiqi Li, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, and Chao Wu. Federated learning with label distribution skew via logits calibration. In _International Conference on Machine Learning_, 2022.
* [30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _Technical report, University of Toronto_, 2009.
* [31] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [32] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In _NIPS Workshop_, 2011.
* [33] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. _arXiv preprint arXiv:1909.06335_, 2019.
* [34] Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xin He, Bo Han, and Xiaowen Chu. Virtual homogeneity learning: Defending against data heterogeneity in federated learning. In _International Conference on Machine Learning_, 2022.
* [35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [36] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Proceedings of the IEEE international conference on computer vision_, pages 2223-2232, 2017.
* [37] Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie. Generative adversarial perturbations. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4422-4431, 2018.
* [38] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In _ICLR_, 2017.
* [39] Zecheng He, Tianwei Zhang, and Ruby B Lee. Model inversion attacks against collaborative inference. In _Proceedings of the 35th Annual Computer Security Applications Conference_, pages 148-162, 2019.
* [40] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret revealer: Generative model-inversion attacks against deep neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2020.
* [41] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2017.
* [42] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Federated visual classification with real-world data distribution. In _European Conference on Computer Vision_, pages 76-92. Springer, 2020.
* [43] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10713-10722, 2021.
* [44] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In _International Conference on Learning Representations_, 2020.
* [45] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In _ICLR_, 2021.

* Jeong et al. [2018] Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data. _arXiv preprint arXiv:1811.11479_, 2018.
* Long et al. [2021] Yunhui Long, Boxin Wang, Zhuolin Yang, Bhavya Kailkhura, Aston Zhang, Carl Gunter, and Bo Li. G-pate: Scalable differentially private data generator via private aggregation of teacher discriminators. _Advances in Neural Information Processing Systems_, 34:2965-2977, 2021.
* Shin et al. [2020] MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. In _Proceedings of the International Conference on Machine Learning_, 2020.
* Chang et al. [2019] Hongyan Chang, Virat Shejwalkar, Reza Shokri, and Amir Houmansadr. Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer. _arXiv preprint arXiv:1912.11279_, 2019.
* Tan et al. [2022] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In _AAAI Conference on Artificial Intelligence_, volume 1, page 3, 2022.
* Zhao et al. [2021] Nanxuan Zhao, Zhirong Wu, Rynson WH Lau, and Stephen Lin. What makes instance discrimination good for transfer learning? In _ICLR_, 2021.
* Yang et al. [2021] Kaiwen Yang, Tianyi Zhou, Yonggang Zhang, Xinmei Tian, and Dacheng Tao. Class-disentanglement and applications in adversarial detection and defense. _Advances in Neural Information Processing Systems_, 34:16051-16063, 2021.
* Fang et al. [2022] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution detection learnable? _Advances in Neural Information Processing Systems_, 2022.
* Liu et al. [2022] Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S Yu. Federated social recommendation with graph neural network. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 13(4):1-24, 2022.
* Zhou et al. [2022] Houliang Zhou, Yu Zhang, Brian Y Chen, Li Shen, and Lifang He. Sparse interpretation of graph convolutional networks for multi-modal diagnosis of alzheimer's disease. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 469-478. Springer, 2022.
* Huang et al. [2023] Kong Huang, Yutong Zhou, Ke Zhang, Jiacen Xu, Jiongyi Chen, Di Tang, and Kehuan Zhang. HOMESPY: the invisible sniffer of infrared remote control of smart tvs. In _32nd USENIX Security Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023_. USENIX Association, 2023.
* Diao et al. [2018] Wenrui Diao, Rui Liu, Xiangyu Liu, Zhe Zhou, Zhou Li, and Kehuan Zhang. Accessing mobile user's privacy based on IME personalization: Understanding and practical attacks. _J. Comput. Secur._, 26(3):283-309, 2018.
* Ng and Chow [2023] Lucien K. L. Ng and Sherman S. M. Chow. Sok: Cryptographic neural-network computation. In _IEEE S&P_, 2023.
* Zheng et al. [2023] Yu Zheng, Qizhi Zhang, Sherman S. M. Chow, Yuxiang Peng, Sijun Tan, Lichun Li, and Shan Yin. Secure softmax/sigmoid for machine-learning computation. In _ACSAC_, 2023.
* Canetti [2001] Ran Canetti. Universally composable security: A new paradigm for cryptographic protocols. In _FOCS_, 2001.
* 19th International Conference_, volume 13043, pages 582-604, 2021.
* Chen et al. [2021] Si Chen, Mostafa Kahla, Ruoxi Jia, and Guo-Jun Qi. Knowledge-enriched distributional model inversion attacks. In _Proceedings of the IEEE/CVF international conference on computer vision_, 2021.

* [63] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In _2010 20th international conference on pattern recognition_, pages 2366-2369. IEEE, 2010.
* [64] Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Wenqi Ren, and Jianfeng Lu. Robust single image reflection removal against adversarial attacks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24688-24698, 2023.
* [65] Daliano Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. _arXiv preprint arXiv:1910.03581_, 2019.
* [66] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. _Advances in Neural Information Processing Systems_, 33:2351-2363, 2020.
* [67] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. Fine-tuning global model via data-free knowledge distillation for non-iid federated learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10174-10183, 2022.
* [68] Felix Sattler, Tim Korjakow, Roman Rischke, and Wojciech Samek. Fedaux: Leveraging unlabeled auxiliary data in federated learning. _IEEE Transactions on Neural Networks and Learning Systems_, 2021.
* [69] Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence analysis and power-of-choice selection strategies. _arXiv preprint arXiv:2010.01243_, 2020.
* [70] Antoine Chatalic, Vincent Schellekens, Florimond Houssiau, Yves-Alexandre De Montjoye, Laurent Jacques, and Remi Gribonval. Compressive learning with privacy guarantees. _Information and Inference: A Journal of the IMA_, 11(1):251-305, 2022.
* [71] Kuntai Cai, Xiaoyu Lei, Jianxin Wei, and Xiaokui Xiao. Data synthesis via differentially private markov random fields. _Proceedings of the VLDB Endowment_, 14(11):2190-2202, 2021.
* [72] Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In _USENIX Security_, 2019.
* [73] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In _NeurIPS_, 2019.
* [74] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In _SP_, 2019.
* [75] Zonghao Huang, Rui Hu, Yuanxiong Guo, Eric Chan-Tin, and Yanmin Gong. DP-ADMM: admm-based distributed learning with differential privacy. In _IEEE TIFS_, 2020.
* [76] Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek, and H. Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. In _IEEE TIFS_, 2020.
* [77] Galen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially private learning with adaptive clipping. _Advances in Neural Information Processing Systems_, 34:17455-17466, 2021.
* [78] Dirk van der Hoeven. User-specified local differential privacy in unconstrained adaptive online learning. In _NeurIPS_, 2019.
* [79] Lichao Sun, Jianwei Qian, and Xun Chen. LDP-FL: practical private aggregation in federated learning with local differential privacy. In _IJCAI_, 2021.
* [80] Hao Peng, Haoran Li, Yangqiu Song, Vincent Zheng, and Jianxin Li. Differentially private federated knowledge graphs embedding. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 1416-1425, 2021.

## Appendix A Overview of FedFed Framework

The training process of the federated learning system with FedFed can be summarized into four phases. Firstly, every participant of FL distills the local private raw data \(\mathbf{x}\) and decouples \(\mathbf{x}\) into performance-robust features \(\mathbf{x}_{r}\) and performance-sensitive features \(\mathbf{x}_{s}\) during the feature distillation phase. Then all clients send \(\mathbf{x}_{p}\) to construct a global dataset, \(\mathbf{x}_{s}\) with DP protection and keep \(\mathbf{x}_{r}\) locally. In the local training phase, the selected clients sample a subset from the globally shared dataset and update the local model with local raw data \(\mathbf{x}\) and sampled information \(\mathbf{x}_{p}\) from server. Finally, clients upload the latest local model and aggregate a global model based on a certain aggregation algorithm, like the weighted average strategy in FedAvg. Figure 4 shows the overview of FedFed framework, while Figure 5 depicts its pipeline of feature distillation. The loss function of feature distillation in Figure 5 can be formulated as Eq (9).

## Appendix B Federated Learning Algorithms with FedFed

### Implementation Details

We list all relevant parameters in this paper in Table 7. We fine-tune learning rate in the set of \(\{0.0001,0.001,0.01,0.1\}\) and report the best results and corresponding learning rate. Whereas, we use 0.01 as the learning rate. In our work, we deploy FedFed on SCAFFOLD and set the learning rate \(\eta_{k}\) to 0.0001 for the SVHN dataset. Furthermore, we use \(\eta_{k}=0.001\) in SVHN while FedMove is deployed with our method.

The batch size is set as 64 when \(K=10\) and 32 for \(K=100\). On the server side, we select 5 clients for aggregation per round when \(K=10\), and 10 clients per round when \(K=100\). This corresponds to a sampling rate of 50% for 10 clients (\(K=10\)) and 10% for 100 clients (\(K=100\)). The noise level in our experiments is \(\mathcal{N}(0,0.15)\). Besides, all experiments are performed on Python 3.8, 36 core 3.00GHz Intel Core i9 CPU, and NVIDIA RTX A6000 GPUs.

Figure 4: Overview of FedFed

Figure 5: The Pipeline of Feature Distillation

[MISSING_PAGE_FAIL:17]

Built upon the Definition 3.1, we present the definition of performance-sensitive features and performance-robust features, as in Definition 3.2. Intuitively, performance-sensitive features contain all label information, while performance-robust features contains all information about the data except for the label information. That is, the features to be partitioned are either performance-sensitive features or performance-robust features.

### FedFed on Information Bottleneck Perspective

In this section, we analyse FedFed under an information bottleneck(IB) aspect. In conventional, information bottleneck [21] can be formalized as:

\[\min_{Z}I(X;Y|Z),\text{ {s.t.} }I(X;Z)\leq I_{c}.\] (12)

where to restrict the complexity of encoding \(Z\). Information entropy is denoted as \(H(\cdot)\) and \(I(\cdot;\cdot)\) is the mutual information of two variables. In FedFed, our competitive mechanism stems from IB getting the following definition:

\[\begin{split}&\min_{Z}I(X;Y|Z),\text{ {s.t.} }I(X-Z;X|Z)\geq I_{c},\\ &\Leftrightarrow\min_{Z}H(Y|Z),\text{ {s.t.} }H(Z)\leq H_{c},\\ &\Leftrightarrow\min_{\mathbf{z}}-\mathbb{E}\ \log p(\mathbf{y}| \mathbf{z}),\text{ {s.t.} }||\mathbf{z}||_{2}^{2}\leq\rho.\end{split}\] (13)

According to our framework, \(\mathbf{z}\) can be defined as: \(\mathbf{z}=\mathbf{z}-\mathbf{x}_{r}=\mathbf{x}-q(\mathbf{x};\theta)\), and then we can the get the optimization view of FedFed:

\[\min_{\theta}-\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim P(X_{k},Y_{k})}\log p( \mathbf{y}|\mathbf{x}-q(\mathbf{x};\theta)),\text{ {s.t.} }||\mathbf{x}-q(\mathbf{x};\theta)||_{2}^{2}\leq\rho.\] (14)

Ultimately, in an information bottleneck perspective, \(\min_{\theta}-\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim P(X_{k},Y_{k})}\log p( \mathbf{y}|\mathbf{x}-q(\mathbf{x};\theta))\) intend to contain more information that can be used for generalization. \(||\mathbf{x}-q(\mathbf{x};\theta)||_{2}^{2}\leq\rho\) aims at distilling raw data for minimum features which means maximal compression of \(\mathbf{x}_{s}\).

Proof.: To get Eq (13), we firstly have:

\[\begin{split} I(X;Y|Z)&=I(X;Y)-I(X;Y;Z)\\ &=H(X)+H(Y)-H(X,Y)-[I(Y;Z)-I(Y;Z|X)]\\ &=H(X)+H(Y)-H(X,Y)-[H(Y)+H(Z)-H(Y,Z)]\\ &+[H(Y|X)-H(Y|X,Z)]\\ &=H(X)+H(Y)-H(X,Y)-H(Y)-H(Z)\\ &+H(Y,Z)+H(Y|X)-H(Y|X,Z)\\ &=H(X)-H(X,Y)-H(Z)+H(Y,Z)+H(Y|X)-H(Y|X,Z)\\ &=H(Y,Z)-H(Z)-H(Y|X,Z)\\ &=H(Z)+H(Y|Z)-H(Z)-H(Y|X,Z)\\ &=H(Y|Z)-H(Y|X)\\ &=H(Y|Z)-H(Y|X).\\ I(X-Z;X|Z)&=I(X-Z;X)-I(X-Z;X;Z)\\ &=H(X-Z)+H(x)-H(X-Z,X)-[I(X;Z)-I(X;Z|X-Z)]\\ &=H(X-Z)+H(X)-H(X-Z,X)-[H(X)+H(Z)-H(X,Z)]\\ &+[H(X|X-Z)-H(X|X-Z,Z)]\\ &=H(X-Z)+H(X)-H(X-Z,X)-H(X)-H(Z)\\ &+H(X,Z)+H(X|X-Z)-H(X|X-Z,Z)\\ &=H(X-Z)-H(X-Z,X)-H(Z)+H(X,Z)+H(X|X-Z)\\ &=H(X-Z)-H(X-Z,X)+H(X|X-Z)\\ &=H(X-Z)-H(X-Z,X)+H(X|X-Z)\\ &=H(X-Z)-H(X-Z,X)-H(Z)+H(X|X-Z)\\ &=H(X-Z)-H(Z)+H(X|X-Z)\\ &=H(X-Z)-H(Z)+H(X|X-Z)\\ &=H(X-Z)-H(Z)+H(X|X-Z)\\ &=H(X,X-Z)-H(Z)\\ &=H(X)-H(Z).\\ I(X-Z;X|Z)&\geq I_{c}\Leftrightarrow H(X)-H(Z)\geq I_{c} \Rightarrow H(Z)\leq H(x)-I_{c}=H_{c}\Rightarrow H(Z)\leq H_{c}.\end{split}\] (15)Then, we get \(\min_{Z}I(X;Y|Z),\;s.t.\;I(X-Z;X|Z)\geq I_{c}\Leftrightarrow\min_{Z}H(Y|Z),\;s.t. \;H(Z)\leq H_{c}\).

Furthermore, we have:

\[\begin{split} H(Y|Z)&=\int_{\mathbf{z}}p(z)H(Y|Z= \mathbf{z})d\mathbf{z}=\int_{\mathbf{z}}p(\mathbf{z})\int_{\mathbf{y}}p( \mathbf{y}|\mathbf{z})\log\frac{1}{p(\mathbf{y}|\mathbf{z})}d\mathbf{y}d \mathbf{z}\\ &=-\int_{\mathbf{z}}\int_{\mathbf{y}}p(\mathbf{y},\mathbf{z}) \log p(\mathbf{y}|\mathbf{z})d\mathbf{y}d\mathbf{z}\\ &=-\mathbb{E}\log p(\mathbf{y}|\mathbf{z}).\end{split}\] (16)

\[\begin{split} H(Z)&\leq\frac{1}{2}\sum_{i=1}^{d}\ln \text{Var}(\mathbf{z}_{i})2\pi e=\frac{1}{2}\sum_{i=1}^{d}\ln\text{Var}( \mathbf{z}_{i})+\frac{1}{2}\sum_{i=1}^{d}\ln 2\pi e\\ &\leq\frac{1}{2}\sum_{i=1}^{d}\ln\text{Var}(\mathbf{z}_{i})+ \frac{1}{2}d\ln\,2\pi e=\frac{1}{2}\sum_{i=1}^{d}\ln\mathbb{E}(\mathbf{z}_{i} ^{2})+\frac{1}{2}d\ln 2\pi e\\ &=\frac{1}{2}\ln\mathbb{E}(\sum_{i=1}^{d}\mathbf{z}_{i}^{2})+ \frac{1}{2}d\ln 2\pi e\leq H_{c}\\ \Rightarrow&\mathbb{E}||\mathbf{z}||_{2}^{2}\leq e^{2 H_{c}-d\ln 2\pi e}=\rho,\end{split}\] (17)

which completes the proof.

## Appendix D Full Analysis on Differentially Private Features

Our security analyses contain two steps, saying, differential privacy for each client and the overall analyses for FedFed. Built upon previous works [27, 16], we derive Lemma D.1 below. For FedFed, we attain Lemma D.2 by limiting \(\sigma_{r}\rightarrow\infty\).

**Lemma D.1**.: _For sharing raw features \(\mathbf{x}\), \((\epsilon,\delta)\)-DP holds if \(\epsilon^{\prime}=O(\sqrt{R\log(1/\delta)}(\rho/\sigma_{s}+(1-\rho)/\sigma_{r}))\)._

**Lemma D.2**.: _For sharing performance-sensitive features \(\mathbf{x}_{s}\), \((\epsilon,\delta)\)-DP holds if \(\epsilon=O(\rho\sqrt{R\log(1/\delta)}/\sigma_{s})\)._

### Analysis and Proof for Theorem 3.3

Recall that DP-SGD [27] samples i.i.d. noise from unbias Gaussian distribution \(\mathcal{N}(\mathbf{0},S_{f}^{2}\sigma^{2})\), where \(S_{f}\) is the sensitivity. At the algorithmic level, the noise sampled from \(\mathcal{N}(\mathbf{0},\sigma^{2}C^{2}\mathbf{I})\) is added to a batch of gradients handled by clipping value \(C\).

For aligning analyses with conventional DP guarantees, we adopt similar notations and expressions. Specifically, we denote the noise distribution operated on \(\mathbf{x}_{r}\) to be \(\mathcal{N}(\mathbf{0},\sigma_{r}^{2}\mathbf{I})\) and the noise distribution operated on \(\mathbf{x}_{s}\) to be \(\mathcal{N}(\mathbf{0},\sigma_{s}^{2}\mathbf{I})\). Notably, FedFed only adds DP noise to \(\mathbf{x}_{s}\) in practice. Since \(\mathbf{x}_{r}\) is kept by the corresponding client, an adversary could attain nothing (or be regarded as random data without any entropy). This could be regarded as adding a sufficiently large noise on \(\mathbf{x}_{r}\), thus we consider the \(\sigma_{r}\to\infty\). As for \(\mathbf{x}_{s}\), we employ the \(\ell_{2}\)-norm clipping when selecting the noise level \(\sigma_{s}\).

Now, let's come back to the setting of sharing all raw data \(\mathbf{x}\). We still regard \(\mathbf{x}\) to be two parts, i.e., \(\mathbf{x}=\mathbf{x}_{r}+\mathbf{x}_{s}\) for the latter comparison to FedFed. This trick is motivated by cryptographic standard proof with ideal/real paradigm [60]. For both \(\mathbf{x}_{r}\) and \(\mathbf{x}_{s}\), the noise addition follows DP-SGD's idea, except for adding noise to the sampled data for updating gradients, rather than the latter-computed real gradients. We remark that this method works for FL but may be different for centralized training (with identical training set in the whole training process).

Using the property of post-processing [16], the noise added to the data passes to (or projects to) the following gradient updates. For the view of each client, we know that Theorem D.36 holds for \(\sigma\) operated on \(\mathbf{x}\) from [27].

Footnote 6: Previously “\(\geq\)”, we take the special case “=” the same as DP-SGD.

**Theorem D.3**.: _There exist constants \(c_{1}\) and \(c_{2}\) so that given the sampling probability \(q=L/N\) and the number of steps \(T\), for any \(\epsilon<c_{1}q^{2}T\). The algorithm of DP-SGD is \((\epsilon,\delta)\)-differentially private for any \(\delta>0\) if we choose \(\sigma=c_{2}\frac{q\sqrt{T\log(1/\delta)}}{\epsilon}\)._

For strong composition theorem, the choice of \(\sigma\) in Theorem D.3 is \(O(q\sqrt{T\log(1/\delta)}/\epsilon)\). In DP-SGD, the \(T\) denotes the number of training steps, which means the times of using/sampling private data for DP training. Such meaning represents communication rounds in FL, in which each client receives globally shared data at each round of communication. In the FedFed, we denote the number of communication rounds to be \(R\). Now, let's handle the \(q\), where \(q\) represents the ratio of lot size and the inputting dataset size. In DP-SGD, the per-lot data is directly sampled from the database of size \(N\). In other words, a lot of data is the subset of the input dataset. In the FedFed, the training data fed to each client's model is the same as the database size, i.e., \(q=N/N=1\).

DP-SGD asks for the generality and thus bounds \(\|f(\cdot)\|=\mathbf{e}\). Yet, FedFed sets \(\sigma\propto\|\mathbf{x}_{s}\|\), which does not ask for a general case. We use the notation \(\|\mathbf{x}_{s}\|_{2}\) to represent the domain of performance-sensitive features. On the other hand, \(\|\mathbf{x}\|_{2}\) represents the domain of all features (namely, the "domain of datasets"). We additionally borrow the definition of \(l_{2}\)-norm and move \(\|\mathbf{x}_{s}\|\) out in the proof. By Equation 8, the client takes \(\rho\|\mathbf{x}\|\) for selecting \(\sigma\). Since we aim to make an asymptotic analysis here, we ignore the scalars and constants here. Thus, we attain \(O(\rho\sqrt{R\log(1/\delta)}/\epsilon)\) for selecting \(\sigma\). By rearranging variables, we have \(\epsilon=\rho\sqrt{R\log(1/\delta)}/\sigma\) for a loose analysis.

Before continuing the proof, we derive Lemma D.4 to get the relation between \(\|\mathbf{x}_{r}\|\) and \(\|\mathbf{x}\|\) for latter usage.

**Lemma D.4**.: _Let \(\mathbf{x}_{r}=\mathbf{x}-\mathbf{x}_{s}\). Then, it holds that \((1-\rho)\|\mathbf{x}\|\leq\|\mathbf{x}_{r}\|\leq(1+\rho)\|\mathbf{x}\|\) if \(\|\mathbf{x}_{s}\|=\rho\|\mathbf{x}\|\) satisfies._

Proof of Lemma D.4: \(\|\mathbf{x}-\mathbf{x}_{r}\|=\rho\|\mathbf{x}\|\Rightarrow\|\mathbf{x}- \mathbf{x}_{r}\|^{2}=\rho^{2}\|\mathbf{x}\|^{2}\Rightarrow\|\mathbf{x}\|^{2}+ \|\mathbf{x}_{r}\|^{2}-2\mathbf{x}^{\top}\mathbf{x}_{r}=\rho^{2}\|\mathbf{x} \|^{2}\Rightarrow\|\mathbf{x}\|^{2}+\|\mathbf{x}_{r}\|^{2}-2\|\mathbf{x}\|\| \mathbf{x}_{r}\|\cos(\mathbf{x},\mathbf{x}_{r})=\rho^{2}\|\mathbf{x}\|^{2}\). Since \(\|\mathbf{x}\|>0\) and \(\|\mathbf{x}_{r}\|>0\), we get,

\[\|\mathbf{x}\|/\|\mathbf{x}_{r}\|+\|\mathbf{x}_{r}\|/\|\mathbf{x}-2\cos \langle\mathbf{x},\mathbf{x}_{r}\rangle=\rho^{2}\|\mathbf{x}\|/\|\mathbf{x}_{r}\|\]

Let \(\|\mathbf{x}_{r}\|=\alpha\|\mathbf{x}\|\), and our objective is to get the \(\alpha\). By replacing \(\|\mathbf{x}\|/\|\mathbf{x}_{r}\|\) and \(\|\mathbf{x}_{r}\|/\|x\|\) with \(1/\alpha\) and \(\alpha\) respectively, we get,

\[1/\alpha+\alpha-2\cos\langle\mathbf{x},\mathbf{x}_{r}\rangle=\rho^{2}/\alpha.\]Since \(-1\leq\cos(\mathbf{x},\mathbf{x}_{r})\leq 1\), we get,

\[-2\leq 1/\alpha+\alpha-\rho^{2}/\alpha\leq 2\Rightarrow-2\alpha\leq 1/+\alpha^{2} -\rho^{2}\leq 2\alpha.\]

Since \(0<\rho<1\), we take \(1/+\alpha^{2}-\rho^{2}\leq 2\alpha\) and get \((\alpha-1)^{2}\leq\rho^{2}\). Then, we get the meaningful answers \(-\rho\leq\alpha-1\leq\rho\Rightarrow 1-\rho\leq\alpha\leq 1+\rho\). Therefore, we attain,

\[(1-\rho)\|\mathbf{x}\|\leq\alpha\|\mathbf{x}\|\leq(1+\rho)\|\mathbf{x}\| \Rightarrow(1-\rho)\|\mathbf{x}\|\leq\|\mathbf{x}_{r}\|\leq(1+\rho)\|\mathbf{x}\|.\]

Following above, we can establish the relationship between \(\|\mathbf{x}\|_{2}\) and \(\|\mathbf{x}_{s}\|_{2}\) as follows:

\[\|\mathbf{x}_{s}\|_{2}\leq\rho\|\mathbf{x}\|_{2}.\]

And the relation between noise level \(\sigma_{s}\) and \(\mathbf{x}_{s}\) is:

\[\sigma_{s}\propto\|\mathbf{x}_{s}\|_{2}\leq\rho\|\mathbf{x}\|_{2}=\rho M,\]

where we re-scale data into \([0,1]\) and assume the \(\ell_{2}\)-norm of data is less than \(M>0\). To ensure the \(\|\mathbf{x}_{e}\|_{2}\leq\rho M\), we clip the norm for each sample, which is widely used in DP [27]. 

Now, let's go back to derive Lemma D.1 and Lemma D.2, and continue the proof. For sharing raw data \(\mathbf{x}\), we employ the same method as aforementioned. Since \(\|\mathbf{x}_{s}\|=\rho\|\mathbf{x}\|\), we get \(\epsilon_{e}^{\prime}=\rho\sqrt{R\log(1/\delta)}/\sigma_{s}\). By Lemma D.4, we know that \((1-\rho)\leq\frac{\|\mathbf{x}_{r}\|}{\|\mathbf{x}\|}\leq(1+\rho)\) holds. Thus, we attain,

\[(1-\rho)\sqrt{R\log(1/\delta)}/\sigma_{r}\leq\epsilon_{r}^{\prime}\leq(1+\rho )\sqrt{R\log(1/\delta)}/\sigma_{r}.\]

Here, we take \(\epsilon_{r}^{\prime}\geq(1-\rho)\sqrt{R\log(1/\delta)}/\sigma_{r}\) since we expect a loose case. By combining \(\epsilon_{e}^{\prime}\) and \(\epsilon_{r}^{\prime}\), we attain,

\[\epsilon^{\prime}=O(\sqrt{R\log(1/\delta)}(\rho/\sigma_{s}+(1-\rho)/\sigma_{r }))\]

for sharing raw \(x\), as Lemma D.1. Now, let's take FedFed into consideration. The major difference is that \(\mathbf{x}_{r}\) is kept locally in the whole training process. The root cause lies in that the \(\mathbf{x}_{r}\) owned by the client is privacy-_insensitive_ to this corresponding client. We regard \(\sigma_{r}\) operated on \(\mathbf{x}_{r}\) to be a sufficiently large value. Thus, given \(\rho\ll 1\), we get,

\[\lim_{\sigma_{r}\rightarrow\infty}\epsilon_{r}=(1-\rho)\sqrt{R\log(1/\delta)} /\sigma_{r}\to 0.\]

That is, for sharing \(\mathbf{x}_{s}\) in FedFed, we have \(\epsilon=\epsilon_{s}=O(\rho/\sigma_{s}\sqrt{\log(\frac{1}{\delta})})\), as Lemma D.2.

Let's conversely think Lemma D.1 and Lemma D.2. Now, the proof in the following is very straightforward. If we take identical \(\sigma_{s}\) operated on \(\mathbf{x}_{s}\) for FedFed and sharing raw \(x\). We know that \(\epsilon^{\prime}>\epsilon\). Conversely, if we take identical \(\epsilon,\epsilon^{\prime}\), we should increase \(1/\sigma_{s}\) and thus reduce \(\sigma_{s}\), given constant \(\rho\sqrt{R\log(1/\delta)}\). Thus, for protecting training data in FL, we can attain Theorem 3.3 to summarize the superiority of FedFed when asking for an identical privacy guarantee. Theorem 3.3 explains the reason for the superior model performance of FedFed, which intrinsically boils down to a relatively small \(\sigma\) compared with sharing raw data \(\mathbf{x}\).

### Analysis and Proof for Theorem 3.4

Previously, we showcase the view of each client for analyzing privacy. Albeit we do not aim at a new DP theorem, we expect a tighter privacy analysis for FedFed by using the advanced result [28]. Vadhan and Wang [61] prove that when the interactive mechanisms being composed are pure differentially private, their concurrent composition achieves privacy parameters (with respect to pure or approximate differential privacy) that match the (optimal) composition theorem for noninteractive differential privacy. To be specific, we follow the proof logic of Theorem D.7 [28] for analyzing globally shared data from all clients.

Given \((\epsilon_{k},\delta)\)-DP at each client side, we utilize the composition theorem to analyze overall privacy in FedFed. The concept of sensitivity below is originally used for sharing a dataset for achieving \((\epsilon,\delta)\)-differential privacy.

**Definition D.5** (Sensitivity [28]).: _The sensitivity of a query function \(\mathcal{F}:\mathbb{D}\rightarrow\mathbb{R}\) for any two neighboring datasets \(D,D^{\prime}\) is \(\Delta=\max_{D,D^{\prime}}\|\mathcal{F}(D)-\mathcal{F}(D^{\prime})\|\), where \(\|\cdot\|\) denotes \(L_{1}\) or \(L_{2}\) norm._Privacy loss is a random variable that accumulates the random noise added to the algorithm/model, which is utilized in Theorem D.7.

**Definition D.6** (Privacy Loss [28]).: _. Let \(\mathcal{M}:\mathbb{D}\rightarrow\mathbb{R}\) be a randomized mechanism with input domain \(D\) and range \(R\). Let \(D,D^{\prime}\) be a pair of adjacent dataset and \(\mathsf{aux}\) be an auxiliary input. For an outcome \(o\in\mathbb{R}\), the privacy loss at \(o\) is defined by \(\mathcal{L}^{(o)}_{\mathsf{pri}}\triangleq\log(\Pr[\mathcal{M}(\mathsf{aux},D )=o]/\Pr[\mathcal{M}(\mathsf{aux},D^{\prime})=o])\)._

**Theorem D.7** (Composition Theorem [28]).: _For any \(\epsilon>0,\delta\in[0,1]\), and \(\hat{\delta}\in[0,1]\), the class of \((\epsilon,\delta)\)-differentially private mechanisms satisfies \((\hat{\epsilon}_{\hat{\delta}},1-(1-\hat{\delta})\Pi_{i}(1-\delta_{i}))\)-differential privacy under \(k\)-fold adaptive composition for \(\hat{\epsilon}_{\hat{\delta}}=\min\{k\epsilon,(e^{\epsilon}-1)\epsilon k/(e^{ \epsilon}+1)+\epsilon\sqrt{2k\log(e+\sqrt{k\epsilon^{2}/\hat{\delta}})},(e^{ \epsilon}-1)\epsilon k/(e^{\epsilon}+1)+\epsilon\sqrt{2k\log(1/\hat{\delta})}\}\)._

Proof.: Definition D.6 gets \(\mathcal{L}^{(o)}_{\mathsf{pri}}\) on an outcome variable \(o\) over databases \(D\) and \(D^{\prime}\). This proof starts with a particular random mechanism \(\mathcal{M}^{\dagger}\) with further generalization. The mechanism \(\mathcal{M}^{\dagger}\) does not depend on the database or the query but relies on hypothesis \(\mathsf{hp}\). For \(\mathsf{hp}=0\), the outcome \(O_{i}\) of \(\mathcal{M}^{\dagger}_{i}\) is independent and identically distributed from a discrete random distribution \(O^{\mathsf{hp}=0}\sim\mathcal{P}^{\dagger,0}\). \(\mathcal{P}^{\dagger,0}(o)\) is defined to be: \(\delta\) for \(o=0;(1-\delta)e^{\epsilon}/(1+e^{\epsilon})\) for \(o=1;(1-\delta)/(1+e^{\epsilon})\) for \(o=2;0\) for \(o=3\). For \(\mathsf{hp}=1\), the outcome \(O_{i}\) of \(\mathcal{M}^{\dagger}_{i}\) is \(O^{\mathsf{hp}=1}\sim\mathcal{P}^{\dagger,1}\). \(\mathcal{P}^{\dagger,1}(o)\) is defined to be: \(0\) for \(o=0;(1-\delta)/(1+e^{\epsilon})\) for \(o=1;(1-\delta)e^{\epsilon}/(1+e^{\epsilon})\) for \(o=2;\delta\) for \(o=3\).

Let \(\mathcal{R}(\epsilon,\delta)\) be privacy region of a single access to \(\mathcal{M}^{\dagger}\). The privacy region consists of two rejection regions with errors, i.e., rejecting true null-hypothesis (type-I error) and retaining false null-hypothesis (type-II error). Let \(\epsilon^{\dagger}_{k},\delta^{\dagger}_{k}\) be \(\mathcal{M}^{\dagger}_{i}\)'s parameters for defining privacy. \(\mathcal{R}(\mathcal{M},D,D^{\prime})\) of any mechanism \(\mathcal{M}\) can be regarded as an intersection of \(\{(\epsilon^{\dagger}_{k},\delta^{\dagger}_{k})\}\) privacy regions. For an arbitrary mechanism \(\mathcal{M}\), we need to compute its privacy region using the \((\epsilon^{\dagger}_{k},\delta^{\dagger}_{k})\) pairs. Let \(D,D^{\prime}\) be neighbouring databases and \(\mathcal{O}\) be the outputting domain. Define (symmetric) \(\mathcal{P},\mathcal{P}^{\prime}\) to be probability density function of the outputs \(\mathcal{M}(D),\mathcal{M}(D^{\prime})\), respectively. Assume a permutation \(\pi\) over \(\mathcal{O}\) such that \(\mathcal{P}^{\prime}(o)=\mathcal{P}(\pi(o))\).

Let \(S\) denote the complement of a rejection region. Since \(\mathcal{R}(\mathcal{M},D,D^{\prime})\) is convex, we have \(1-\mathcal{P}(S)\geq-e^{\epsilon^{\dagger}_{k}}\mathcal{P}^{\prime}(S)+1- \delta^{\dagger}_{k}\Rightarrow\mathcal{P}(S)-e^{\epsilon^{\dagger}_{k}} \mathcal{P}^{\prime}(S)\leq\delta^{\dagger}_{k}\). Define \(\mathsf{Dt}_{\epsilon^{\dagger}}(\mathcal{P},\mathcal{P}^{\prime})=\max_{S \subseteq\mathcal{O}}\{\mathcal{P}(S)-e^{\epsilon^{\dagger}}\mathcal{P}^{ \prime}(S)\}\). Thus, \(\mathcal{M}\)'s privacy region is the set: \(\{(\epsilon^{\dagger}_{k},\delta^{\dagger}_{k}):\epsilon^{\dagger}_{k}\in[0,\infty)\}\) s.t. \(\mathcal{P}(o)=e^{\epsilon^{\dagger}_{k}}\mathcal{P}^{\prime}(o),\delta^{ \dagger}_{k}=\mathsf{Dt}_{\epsilon^{\dagger}_{k}}(\mathcal{P},\mathcal{P}^{ \prime})\}\). Next, we consider composition on random mechanisms \(\mathcal{M}_{1},\ldots,\mathcal{M}_{i}\). By accessing \(\mathcal{M}^{\dagger}_{i}\), \(\mathcal{P}(O^{1,\mathsf{hp}}=o_{1},\ldots,O^{i,\mathsf{hp}}=o_{i})=\Pi^{i}_{j =1}\mathcal{P}^{\dagger,\mathsf{hp}}(o_{j})\). By algebra on two discrete distributions, \(\mathsf{Dt}_{(i-2j)\epsilon}(\mathcal{P}^{i},(\mathcal{P}^{\prime})^{i})=1-(1- \delta)^{i}+(1-\delta)^{i}\sum_{l=0}^{j-1}\big{(}\left\lfloor{e^{(\epsilon^{(i-l )}}-e^{\epsilon(i-2j+l)})}\right\rfloor/(1+e^{\epsilon})^{k}\). Hence, privacy region is an interaction of \(i\) regions, parameterized by \(1-(1-\hat{\delta})\Pi_{i}(1-\delta_{i})\).

Now, by Lemma D.2, we get Theorem 3.4 directly. In summary, FedFed protects two types of data features using two different protective manners, i.e., small noise for performance-sensitive features and extremely large noise for performance-robust features, and thus attains higher model performance and stronger security in the same time.

## Appendix E More Results on Attack

### More Details on Model Inversion Attack

In this section, we present additional results of the model inversion attack. When the central server assumes the role of the attacker without access to the original data, it exhibits a diminished performance in attacking the model. The raw data \(\mathbf{x}\) of Figure 3 (a) and (b) can be found in Figure 6(a). As we can see, \(\mathbf{x}_{s}\) still causes privacy leakage and Figure 6(b) also gives the intuitive necessity of DP protection on \(\mathbf{x}_{s}\).

Inspired by generative model inversion methods [40, 62], we conduct another experiment where one of the clients acts as the attacker. Similar to GMI [40], the method involves leveraging an auxiliary dataset and a public dataset to launch an attack on the target model. In our experiments, we utilize the globally shared dataset as the auxiliary dataset, while the local private data serves as the public dataset because the client has access to its own local data. Under this scenario (results in Figure 7), the attacker's performance is better than when the server acts as the attacker, while the attacker is still unable to recover data from the shared features. The model architectures of generative-based model inversion attack are listed in Table 8.

### More Details on Membership Inference Attack

_Is Theorem 3.3 true empirically?_

Membership inference attack(MIA) [41; 15] attempts to infer whether a particular data point is in the target model's training set. Specifically, the global model on the server side can be regarded as the target model in FL. In our setting, we train a shadow model with globally shared data. Then the membership attack can be regarded as a binary classification task, member data or non-member data. The input of the attack model is the top-k vector of the output from the shadow model. To compare the superiority of sharing partial data rather than the complete data with DP, we conduct MIA to explore the divergence experimentally. Sharing raw data cause more information leakage (higher

Figure 8: Attack process. Perform membership inference attack on global model per 10 communication rounds.

Figure 7: Generative-based model inversion attack results.

recall of MIA) than partial data with the same DP level. However, the FedFed needs a relatively small noise \(\sigma\) to achieve comparable protection. The attack process is shown in Figure 8.

## Appendix F Supplementary for Experiments

### Visualization of Data Heterogeneity

We show the visualization of data distribution in Figure 9. The LDA partition and the \(\#C=2\) partition have the label skew and the quantity skew simultaneously. And the Subse

\begin{table}
\begin{tabular}{c c c c} \hline \hline Type & Kernel & Dilation & Stride & Outputs \\ \hline conv & 5x5 & 1 & 1x1 & 32 \\ conv & 3x3 & 1 & 2x2 & 64 \\ conv & 3x3 & 1 & 1x1 & 64 \\ conv & 3x3 & 1 & 2x2 & 128 \\ conv & 3x3 & 1 & 1x1 & 128 \\ conv & 3x3 & 2 & 1x1 & 128 \\ conv & 3x3 & 4 & 1x1 & 128 \\ conv & 3x3 & 8 & 1x1 & 128 \\ conv & 3x3 & 16 & 1x1 & 128 \\ \hline \hline \end{tabular} The encoder structure of the generator takes as input the latent vector

\begin{tabular}{c c c} \hline \hline linear & & 2048 \\ deconv & 5x5 & 1/2x1/2 & 256 \\ deconv & 5x5 & 1/2x1/2 & 128 \\ \hline \hline \end{tabular} The decoder structure of the generator

\begin{tabular}{c c c} \hline \hline deconv & 5x5 & 1/2x1/2 & 128 \\ deconv & 5x5 & 1/2x1/2 & 64 \\ conv & 3x3 & 1x1 & 32 \\ conv & 3x3 & 1x1 & 3 \\ \hline \hline \end{tabular} The global discriminator structure

\begin{tabular}{c c c} \hline \hline conv & 3x3 & 2x2 & 64 \\ conv & 3x3 & 2x2 & 128 \\ conv & 3x3 & 2x2 & 256 \\ conv & 1x1 & 4x4 & 1 \\ \hline \hline \end{tabular} The model architectures used in generative-based model inversion attack

\end{table}
Table 8: The model architectures used in generative-based model inversion attack

Figure 9: Data distribution in various FL heterogeneity scenarios. Different colours denote different labels and the length of each line denotes the data number. As we can see, in our FL setting, we mainly perform on two kinds of Non-IID scenarios, including label skew and quantity skew.

the label skew. \(\#C=k\) means each client only has \(k\) different labels from dataset, and \(k\) controls the unbalanced degree. The subset method makes each client have all classes from the data, but one dominant class far away outnumbers other classes.

### More Guessing Games

Let's play more guessing games here. We selected samples from the same category for the purpose of reducing the difficulty. The answer in Figure 1 is C. The answers in Figure 10 are (B, A, A) in order.

### Sharing Protected Partial Features vs. Protected Raw Data

In this section, we compare the performance divergence of applying DP to protect different sharing information, i.e., performance-sensitive features \(\mathbf{x}_{p}\) and raw data \(\mathbf{x}\). The results is shown in Figure 11

### More experimental results

We provide more comparison results in this section to demonstrate the superiority of FedFed. Figure 12, Figure 13, Figure 14, Figure 15 have shown our enhancement in four datasets. In this paper, we mainly have four settings in our experiments: (1) \(\alpha=0.1,E=1,K=10\); (2) \(\alpha=0.1,E=5,K=10\); (3) \(\alpha=0.05,E=1,K=10\); (4) \(\alpha=0.1,E=1,K=100\).

Figure 11: Performance on different information sharing strategies with DP protection: raw data and \(\mathbf{x}_{p}\) under the same protection strength.

Figure 10: Guessing Games

[MISSING_PAGE_EMPTY:27]

### More Explanation: Performance-Sensitive Features vs. Performance-Robust Features

For an intuitive understanding of why there is no drastic performance degradation for utilizing \(\mathbf{x}_{s}\) as a substitute for raw data \(\mathbf{x}\). Formally, e.g. in the classification task, a classifier trained by \(\mathbf{x}\) gets comparable test accuracy on \(\mathbf{x}_{s}\) giving the verification that \(\mathbf{x}_{s}\) contains the primary features for generalization and vice versa. Specifically, three classifiers trained on \(\mathbf{x}\), \(\mathbf{x}_{s}\), and \(\mathbf{x}_{r}\), Then test the generalization ability on \(\mathbf{x}\), \(\mathbf{x}_{r}\), and \(\mathbf{x}_{s}\), separately. The result is shown in Figure 17.

As described above, \(\mathbf{x}_{r}\) extract from \(\mathbf{x}\) in an IB manner contains more visual information about private data \(\mathbf{x}\) than \(\mathbf{x}_{s}\). To measure the visual similarity among performance-robust features \(\mathbf{x}_{r}\), performance-sensitive features \(\mathbf{x}_{s}\), and raw data \(\mathbf{x}\). We choose PSNR [63; 64], prominent visual information comparisons of images, to calculate pixels error, the result is shown in Table 12.

Consequently, higher PSNR denotes that \(\mathbf{x}_{r}\) has the most visual information of \(\mathbf{x}\) than \(\mathbf{x}_{s}\). While \(\mathbf{x}_{r}\) shares similarities with \(\mathbf{x}\) in terms of its form or structure, it exhibits limitations that hinder its ability to enhance performance. At the same time, \(\mathbf{x}_{s}\) has the commensurate generalization ability like \(\mathbf{x}\) to achieve our goal.

### Overheads Analysis of FedFed

For the extra computation induced by FedFed, we provide two views to demonstrate the limited costs of extra computation, i.e., training time and FLOPs.

Table 13 shows that training a generator requires less than \(10\%\) of the time needed to train a classifier. Therefore, the additional training time required for the generator is limited.

Figure 14: Convergence and test accuracy comparison on SVHN.

Figure 15: Convergence and test accuracy comparison on FMNIST.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & CIFAR-10 & FMNIST & SVHN & CIFAR-100 \\ \hline FedAvg & 89.45 & 92.38 & 92.03 & 69.02 \\ \hline FedFed (Ours) & 94.01 & 94.31 & 93.89 & 70.31 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Top-1 Accuracy of \(\alpha=1.0,E=1,K=10\) with \(50\%\) sampling rate.

Figure 16: The impact of DP noise on different datasets.

Figure 17: The generalization ability graphs of three classifiers trained by different data types. The generalization ability means a well-trained model tests unseen information and measures the capability to finish a certain task, i.e., classification test accuracy. In our setting, we evaluate the generalization ability of classifiers on different information forms: raw data \(\mathbf{x}\), performance-sensitive features \(\mathbf{x}_{s}\), performance-robust features \(\mathbf{x}_{r}\).

\begin{table}
\begin{tabular}{c c c} \hline \hline  & \(\mathbf{x}_{r}\) & \(\mathbf{x}_{s}\) \\ \hline \(\mathbf{x}\) & \(31.47\pm 1.57\) & \(18.87\pm 0.98\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: The visual similarity of various feature types. PSNR of \(\mathbf{x}\) and \(\mathbf{x}_{r}\), \(\mathbf{x}\) and \(\mathbf{x}_{s}\).

Tables 15 and 14 show the model complexity results, which indicate that the computation required for the generator is approximately \(8.2\%\) of that required for the classifier. Additionally, the number of communication rounds needed for the generator is significantly lower than that required by the classifier, with only 15 rounds compared to 1000 rounds for the classifier (namely, \(T_{d}=15\) and \(T_{r}=1000\)).

The use of FedFed incurs additional memory usage, as the size of the globally-shared dataset is equivalent to the combined size of all clients' data. This may have limited impacts on the bandwidth, as the generated data is only sent once. To quantitatively measure its effects, we compared the size of the dataset with that of the classifiers in Table 16. Sharing the generated dataset is equivalent to sharing classifiers in multiple rounds. For example, for CIFAR10, the data requires approximately 586MB of memory, while the classifier requires about 48MB. Thus, sharing the global dataset requires the same amount of communication as sending a classifier in approximately 14 communication rounds.

Based on the aforementioned analysis, we provide a comprehensive examination of the communication overheads associated with the FedFed. Consider \(K\) clients in the FL system. Let \(m\) be the size of a single local model. The size of local private data is \(\|\mathcal{D}_{k}\|=a\). Then, the ratio of the entire dataset to a model parameter is \(\gamma\), where \(\gamma=\frac{aK}{m}\approx 14\). The extra communication cost for a single client is \((m+m)*T_{d}+a+AK\) where \((m+m)*T_{d}\) denotes the cost of download/upload models for \(T_{d}\) rounds in Algorithm 1. Here, the \(a\) denotes the performance-sensitive features sent by each client, and \(aK\) is the data received by each client from the globally shared dataset. In the general process of FL, the overall communication costs are \((m+m)T_{r}\beta\), where \(\beta\) is the sampling rate of a client. Therefore, the ratio of the extra communication overhead to the general FL process is:

\[\frac{(m+m)\cdot T_{d}+a(K+1)}{(m+m)\cdot T_{r}\cdot\beta}=\frac{T_{d}}{T_{r} \cdot\beta}+\frac{a(K+1)}{2m\cdot T_{r}\cdot\beta}=\frac{T_{d}}{T_{r}\cdot \beta}+\frac{\gamma}{2T_{r}\cdot\beta}+\frac{\gamma}{2K\cdot T_{r}\cdot\beta}\] (17)

Here, we detail two examples in our experiments:

* For \(K=10,T_{d}=15\), \(T_{r}=1000\), and \(\beta=50\%\), the extra communication costs are approximately \(4.54\%\).
* When \(K=100\), \(T_{d}=15\), \(T_{r}=1000\), and \(\beta=10\%\), the extra communication costs are approximately \(22.07\%\).

To facilitate the further deployment of FedFed, we offer three strategies tailored to different storage hardware configurations:

1. One-time download: Local clients download the globally shared dataset once. A globally shared dataset costs approximately \(14\times\) the storage of a classifier model.
2. Partial download: A small portion of the globally shared dataset is selected and downloaded. This strategy incurs approximately \(1.5\times\) communication cost compared to the previous strategy, while the storage required by the clients is the same as that of local private data. This represents a storage-friendly choice that may involve a trade-off in terms of performance.
3. Intermittent download: A small set of globally shared dataset is downloaded after every \(Z\) rounds. This approach reduces the communication overhead to \(\frac{1}{Z}\) of that of strategy 2 while maintaining the storage overhead at the size of the local data.

## Appendix G More Related Works

### Federated Learning with Heterogeneous Data.

FL allows the distributed clients to train a model across multiple datasets jointly. It "protects" user privacy by controlling data accessibility among different clients, i.e., only the data owner has the right to access the corresponding data. FedAvg [4] is the seminal work designed to reduce communication via more local training epochs and fewer communication rounds. In the following, many works [9, 5] observe that the FedAvg's divergence is considerable compared with centralized training if clients hold heterogeneous distribution. Even worse, the gap accumulates as the weight aggregates, potentially hurting model performance.

Recently, a series of works have tried to calibrate the updated direction of local training from the global model. FedProx [6] adds an \(L_{2}\) distance as the regularization term in the objective function, providing a theoretical guarantee of convergence. Similarly, FedIR [42] operates on a mini-batch by self-normalized weights to address the non-identical class distribution. SCAFFOLD [7] restricts the model using the previous knowledge. Besides, MOON [43] introduces contrastive learning at the model level to correct the divergence between clients and the server.

Meanwhile, recent works propose designing new model aggregation schemes. FedAvgM [33] performs momentum on the server side. FedNova [22] adopts a normalized averaging method to eliminate objective inconsistency. A study [69] also indicates that biasing client selection with higher local loss can speed up the convergence rate. The coordinate-wise averaging of weights also induces noxious performance. FedMA [44] conducts a Bayesian non-parametric strategy for heterogeneous data. FedBN [45] focuses on feature shift Non-IID and performs local batch normalization before averaging models.

Another existing direction for tackling data heterogeneity is sharing data. This line of work mainly assembles the data of different clients to construct a global IID dataset, mitigating client drift by replenishing the lack of information of clients [9]. Existing methods include synthesizing data based on the raw data by GAN [46]. However, the synthetic data is generally relatively similar to the raw data, leading to privacy leakage at some degree. Adding noise to the shared data is another promising strategy [70, 71]. Some methods employ the statistics of data [48] to synthesize for sharing, which still contains some raw data content. Other methods distribute intermediate features [12], logits [49, 10], or learn the new embedding [50]. These tactics will increase the difficulty of privacy protection because some existing methods can reconstruct images based on feature inversion methods [51]. Most of the above methods share information without a privacy guarantee or with strong privacy-preserving but poor performance, posing the privacy-performance dilemma.

Concretely, in FD [46] all clients leverage a generative model collaboratively for data generation in a homogeneous distribution. For better privacy protection, G-PATE [47] performs discriminators with local aggregation in GAN. Fed-ZDAC(Fed-ZDAS) [12], depending on which side to play augmentation, introduce zero-shot data augmentation by gathering intermediate activations and batch

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Shared Info & Protection on Shared Info & Attack Test \\ \hline FD+FAug [46] & Model output \& Synthetic info & — \\ FedMD [65] & logits & ✗ & — \\ XorMixFL [48] & Statistic of data & ✗ & — \\ FedDF [66] & Statistic of logits & ✗ & — \\ FedProto [50] & Abstract class prototypes & ✗ & — \\ FedFTG [67] & Synthetic info & ✗ & — \\ CCVR [10] & Statistic of logits & ✗ & Model inversion attack \\ Fed-ZDAC [12] & Batch normalization features & ✓ & — \\ FedAUX [68] & Synthetic info & ✓ & — \\ \hline
**FedFed (ours)** & Protected partial features & ✓ & Model inversion attack \\  & & & Membership inference attack \\ \hline \hline \end{tabular}
\end{table}
Table 16: Parameters of Local Classifier and Globally-shared Data

\begin{table}
\begin{tabular}{c c} \hline \hline FLOPs of training Generator & 16.03M FLOPs \\ \hline FLOPs of training Classifier & 556.66M FLOPs \\ \hline \hline \end{tabular} 
\begin{tabular}{c c} \hline \hline Parameters of Local Classifier & 42MB \\ \hline Parameters of Globally-shared Data & 586MB \\ \hline \hline \end{tabular}
\end{table}
Table 15: FLOPs of Classifier and Generatornormalization(BN) statistics to generate fake data. Inspired by mixup data. Cronus [49] transmits the logits information while CCVR [10] collects statistical information of logits to sample fake data. FedFTG [67] use a generator to explore the input space of the local model and transfer local knowledge to the global model. FedDF [66] utilizes knowledge distillation based on unlabeled data or a generator and then conducts _AVGLOGITS_. We summarize existing information-sharing methods in Table 17 to compare the dissimilarity with FedFed. The main difference between FedDF and FedFed is that our method distills raw data into two parts (performance-sensitive features \(x_{e}\) and performance-robust features \(x_{r}\)) rather than transferring distilled knowledge. We provide _hierarchical protections_ to preserve information privacy while overcoming the privacy-performance dilemma.

### Differential Privacy with Federated Learning

Carlini et.al [72] found that memorizing sensitive data occurs in early training, regardless of data rarity and model.

Training with differential privacy [73; 74] is a feasible solution to avoid its risk, albeit at some loss in utility. Differential privacy guarantees that an adversary should not discern whether a client's data was used.

Huang et al [75] and Wei et al [76] are the first (to their knowledge) to analyze the relation between convergence and utility in FL. Andrew et al [77] explore setting an adaptive clipping norm in the federated setting rather than using a fixed one. Andrew et al [77] explore setting an adaptive clipping norm in the federated setting rather than using a fixed one. They show that adaptive clipping to gradients can perform as well as any fixed clip chosen by hand. Hoeven et al [78] introduce data-dependent bounds and apply symmetric noise in online learning, which allows data providers to pick noise distribution. Sun et al [79] explicitly vary ranges of weights at different layers in a DNN and shuffle high-dimensional parameters at an aggregation for easing explodes of privacy budgets. Peng et al [80] study the knowledge embedding problem using DP protection in FL. Applying differential privacy and its variants to the federated setting become more prevailing nowadays.