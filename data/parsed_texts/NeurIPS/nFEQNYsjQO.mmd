Label Correction of Crowdsourced Noisy Annotations with an Instance-Dependent Noise Transition Model

 Hui Guo

Department of Computer Science

University of Western Ontario

hguo288@uwo.ca

&Boyu Wang

Department of Computer Science

University of Western Ontario

bwang@csd.uwo.ca

Corresponding authors.

Grace Y. Yi

Department of Statistical and Actuarial Sciences

Department of Computer Science

University of Western Ontario

gyi5@uwo.ca

###### Abstract

The predictive ability of supervised learning algorithms hinges on the quality of annotated examples, whose labels often come from multiple crowdsourced annotators with diverse expertise. To aggregate noisy crowdsourced annotations, many existing methods employ an annotator-specific instance-independent _noise transition matrix_ to characterize the labeling skills of each annotator. Learning an _instance-dependent_ noise transition model, however, is challenging and remains relatively less explored. To address this problem, in this paper, we formulate the noise transition model in a Bayesian framework and subsequently design a new label correction algorithm. Specifically, we approximate the instance-dependent noise transition matrices using a Bayesian network with a hierarchical spike and slab prior. To theoretically characterize the distance between the noise transition model and the true instance-dependent noise transition matrix, we provide a posterior-concentration theorem that ensures the posterior consistency in terms of the Hellinger distance. We further formulate the label correction process as a hypothesis testing problem and propose a novel algorithm to infer the true label from the noisy annotations based on the pairwise likelihood ratio test. Moreover, we establish an information-theoretic bound on the Bayes error for the proposed method. We validate the effectiveness of our approach through experiments on benchmark and real-world datasets.

## 1 Introduction

Deep neural networks (DNNs) have achieved remarkable performance in various tasks [1; 2], and they have proven to be useful in handling sizable labeled data. Acquiring large accurately annotated datasets, however, is usually expensive and time consuming. To enhance the efficiency of annotation, in many applications, _crowdsourcing_[3] is employed as an alternative way for data labeling, where the labels are provided by multiple annotators with varying and imperfect labeling skills, and thus, the collected labels suffer from unavoidable noise. As deep models have a strong memorization power, using these noisy labels as the ground truth deteriorates the performance of DNNs [4; 5], and most importantly, yields erroneous learning results. Further, potentially substantial disagreement among the annotators for each instance presents extra challenges in the application of traditional supervised learning algorithms. Hence, in the crowdsourcing scenario, to effectively train DNNs on noisy labeled datasets, a fundamental question is how to aggregate the noisy crowdsourced annotations and infer the latent true labels [6].

One naive approach to aggregate the crowdsourced labels is simply by computing the majority vote, which can be ineffective when the number of annotators is not large enough or the labeling task is difficult [7; 8]. Recent research has developed more powerful techniques for inferring the ground truth labels [7; 9; 10; 11], among which the _annotator-specific noise transition matrix_, aka _annotator confusion_, plays an important role by modeling the labeling process for each individual annotator. To estimate the transition matrix, available research [8; 11; 12; 13] usually makes the _instance-independent_ assumption that for annotator \(r\), given the true label \(\mathrm{y}\), the corruption process is independent of the input \(\mathbf{x}\), i.e., \(\mathbb{P}(\tilde{\mathrm{y}}^{(r)}=l|\mathrm{y}=k,\mathbf{x})=\mathbb{P}( \tilde{\mathrm{y}}^{(r)}=l|\mathrm{y}=k)\), where \(\mathbf{x}\) denotes the random variable for instance/feature, \(\tilde{\mathrm{y}}^{(r)}\) represents the noisy label given by annotator \(r\), and \(\mathrm{y}\) is the underlying ground truth label. This assumption, however, is often violated in applications. _Instance-dependent_ annotation noise is more realistic and appropriate for real-world datasets, as suggested by the example that factors such as the quality of ultrasound images and the domain expertise of human annotators can greatly influence the actual diagnostic process in medical analysis [14; 15]. For annotator \(r\), the transition matrix \(\tau^{(r)}(\mathbf{x})\) is a matrix-valued function, with the \((k,l)\) element defined as \(\tau^{(r)}_{kl}(\mathbf{x})=\mathbb{P}(\tilde{\mathrm{y}}^{(r)}=l|\mathrm{y} =k,\mathbf{x})\). Unfortunately, the case of instance-dependent annotation noise remains challenging and less explored. Most existing works considering instance-dependent noise are designed for the single annotator case [16; 17; 18]. For the case with multiple annotators, existing methods investigate the human annotation process and use different models to estimate instance-dependent noise matrices. Approaches in [3; 19; 20; 21; 22] use traditional classification models such as logistic regression, while others [23; 24; 25] cater to large datasets and deep models. Methods in [3; 19; 20; 21; 22] and [24; 25] are heuristic in nature and lack theoretical guarantees in estimating instance-dependent noise matrices. [23] makes some theoretical progress in justifying the use of the trace regularisation, and extends the work of [8] which establishes the theory only for settings with an instance-independent noise matrix. The theory in [23] is constrained to individual samples rather than the population setting. Importantly, the theoretical characterization of the distance of the noise model and the true annotator confusion remains absent from the literature.

In this paper, we address this notable problem by framing it within the Bayesian paradigm. We **formulate the instance-dependent annotator-specific label transition matrix**, and further propose **a novel algorithm to infer the underlying ground truth by aggregating the noisy annotations**. To model the noise transition matrix, we invoke the Bayesian generalized linear mixed effects model (GLMM), which can be learned by deploying anchor points within the deep learning framework [12; 26; 27]. To facilitate the fact that the number of anchor points learned from the noisy training data is relatively small compared to the sample size, we employ a hierarchical spike and slab prior on the network parameters. This approach offers an interpretable mechanism for variable selection and allows us to establish the theoretical result within the deep learning setup. Our study reveals that the proposed noise transition model is close to the underlying true transition matrix with respect to the Hellinger distance in the Bayesian framework. Such a result is established for independently, nonidentically distributed (i.n.i.d.) observations, substantially extending the existing sparse Bayesian theories within the deep learning paradigm. Further, we develop a label correction method using the pairwise likelihood ratio test to aggregate and infer the ground truth from the noisy crowdsourced annotations. This development is carried out by formulating the label correction process as a hypothesis testing problem and utilizing the proposed Bayesian model in place of the unknown transition matrix in the pairwise likelihood ratio test (LRT). More importantly, with the posterior consistency result, we also derive information-theoretic bounds on the Bayes error for the proposed algorithm even without access to the underlying true noise transition matrix.

This research brings forth several noteworthy advancements: (1) We formulate the annotator-specific noise transition matrix in the Bayesian framework (Section 3.1). This method offers a practical and flexible framework to address real-world problems with noisy annotators. (2) We theoretically characterize the closeness of the proposed model and the underlying annotator confusions with respect to the Hellinger distance. (Section 3.2). (3) We develop a novel label correction algorithm by aggregating the noisy annotations using the pairwise likelihood ratio test, and identify information-theoretic bounds on the Bayes error (Section 3.3). The effectiveness of the proposed algorithm is confirmed by the application to both synthetic and real-world noisy datasets (Section 5). Code is available at https://github.com/hguo1728/BayesianIDNT.

## 2 Problem Setup

Objective and Data.Consider a classification task with a feature space \(\mathcal{X}\subset\mathbb{R}^{p}\) and a label space \(\mathcal{Y}=[K]\), where \(p\) is the dimension of the features, \(K\) is the number of classes, and \([k]\) represents \(\{1,...,k\}\) for any positive integer \(k\). Our goal is to develop a classifier \(h:\mathcal{X}\mapsto\mathcal{Y}\), which can accurately predict the true label for a test instance. However, in applications, the true label \(\mathrm{y}\in\mathcal{Y}\) is often not observed for each input vector \(\mathbf{x}\in\mathcal{X}\). Instead, we receive a set of noisy crowdsourced labels \(\tilde{\mathbf{y}}=\{\tilde{\mathrm{y}}^{(1)},..,\tilde{\mathrm{y}}^{(R)}\}\) from \(R\) distinct annotators, where \(\tilde{\mathrm{y}}^{(r)}\in\mathcal{Y}\) represents the label given by the \(r\)th annotator for \(r\in[R]\). Thus, a noisy dataset \(\mathcal{D}\) of size \(N\) is defined as \(\mathcal{D}=\{\mathbf{x}_{i},\tilde{\mathrm{y}}_{i}^{(1)},..,\tilde{\mathrm{ y}}_{i}^{(R)}\}_{i=1}^{N}\), where for each instance \(\mathbf{x}_{i}\), the true label \(\mathrm{y}_{i}\) is unobserved. Under this setting, we aim to learn a reliable classifier \(h\) by utilizing the noisy crowdsourced dataset \(\mathcal{D}\).

In practice, on commercial crowdsourcing platforms, large-scale labels can often be collected from independent human annotators. We thereby make a common assumption that the \(R\) annotators independently label the instances [7; 8]. The conditional probability of the \(R\) noisy labels, given an instance, can then be formulated as

\[\mathbb{P}(\tilde{\mathrm{y}}^{(1)},..,\tilde{\mathrm{y}}^{(R)}|\mathbf{x})= \prod_{r=1}^{R}\mathbb{P}(\tilde{\mathrm{y}}^{(r)}|\mathbf{x})=\prod_{r=1}^{R} \sum_{k\in\mathcal{Y}}\left\{\mathbb{P}(\tilde{\mathrm{y}}^{(r)}|\mathrm{y}=k,\mathbf{x})P(\mathrm{y}=k|\mathbf{x})\right\},\] (1)

where for \(k\in\mathcal{Y}\), \(\mathbb{P}(y=k|\mathbf{x})\), called the _base model_[28], denotes the conditional probability of the latent true label \(\mathrm{y}\) given \(\mathbf{x}\), which can be modeled by the output of a DNN parameterized by a parameter vector, say \(\boldsymbol{\vartheta}\); and \(\mathbb{P}(\tilde{\mathrm{y}}^{(r)}|\mathrm{y}=k,\mathbf{x})\) is the _noise transition model_ for the \(r\)th annotator [8], satisfying \(\sum_{l=1}^{K}\mathbb{P}(\tilde{\mathrm{y}}^{(r)}=l|\mathrm{y}=k,\mathbf{x})=1\) for any \(\mathbf{x}\in\mathcal{X}\) and \(k\in[K]\). For ease of theoretical presentation, we assume the accessibility to all the annotations from the \(R\) workers for now, and consider more general situations in the experimental part in Section 5; extensions to accommodating the case where each instance is only annotated by a subset of annotators are straightforward.

Notation.In this paper, sets are denoted by calligraphic upper case letters, and vectors and matrices are denoted by bold lower and upper case letters, respectively. For a vector \(\boldsymbol{v}\), \(v_{j}\) denote its \(j\)th element, and \(\boldsymbol{v}^{\top}\) denotes its transpose. For \(\boldsymbol{v}=(v_{1},...,v_{d})^{\top}\), we denote \(\|\boldsymbol{v}\|_{q}=(\sum_{j=1}^{d}|v_{j}|^{q})^{1/q}\) for \(q>0\), \(\|\boldsymbol{v}\|_{\infty}=\max_{j}|v_{j}|\), and \(\|\boldsymbol{v}\|_{0}=\sum_{j=1}^{d}\mathbf{1}(v_{j}\neq 0)\), with \(\mathbf{1}(\cdot)\) denoting the indicator function. The \(L_{2}\) norm of \(\boldsymbol{v}\) is also denoted by \(\|\boldsymbol{v}\|\) for simplicity. For a matrix \(\boldsymbol{V}\), we use \(V_{i,j}\) to represent its \((i,j)\) element. Let \((\Omega,\mathcal{G},\mu)\) denote the measure space under consideration, where \(\Omega\) is a set, \(\mathcal{G}\) is the \(\sigma\)-field of subsets of \(\Omega\), and \(\mu\) is the associated measure. For a measurable function \(f:\Omega\to\mathbb{R}^{d}\), we write \(\|f\|_{q}\triangleq\|f\|_{L^{q}(\Omega)}\) when there is no ambiguity of the domain, where \(\|f\|_{L^{q}(\Omega)}=\left(\int_{\Omega}\sum_{j=1}^{d}|f_{j}(x)|^{q}d\mu \right)^{1/q}\) for \(q>0\). For two sequences, \(\{a_{n}\}\) and \(\{b_{n}\}\), we write \(a_{n}\preceq b_{n}\) if there exists a positive constant \(C\) such that \(a_{n}\leq Cb_{n}\) for large enough \(n\), and we write \(a_{n}\asymp b_{n}\) if \(a_{n}\preceq b_{n}\) and \(b_{n}\preceq a_{n}\).

## 3 Main Results

### Instance-dependent transition matrix with multiple annotators

Annotator-specific instance-dependent noise transition model.Given an instance \(\mathbf{x}\), the conditional probability mass function of noisy annotations can be characterized by \(R\) instance-dependent matrices of dimension \(K\times K\), termed _transition matrices_ or _annotator confusions_[8; 13], with the \(k\)th row of the \(r\)th matrix denoted \(\left(\mathbb{P}(\tilde{\mathrm{y}}^{(r)}=1|\mathrm{y}=k,\mathbf{x}),\ldots, \mathbb{P}(\tilde{\mathrm{y}}^{(r)}=K|\mathrm{y}=k,\mathbf{x})\right)\). Thus, the distribution of noisy annotation depends on the instance in different ways due to the differences in the annotator \(r\) and the underlying true label \(\mathrm{y}\), which can be characterized by a Bayesian generalized linear mixed effects model (GLMM) [29; 30] in the deep learning framework.

Specifically, conditioned on the true label \(\mathrm{y}=k\) and the feature vector \(\mathbf{x}\), we treat the noisy label \(\tilde{\mathrm{y}}^{(r)}\) from annotator \(r\) as a random variable generated from the distribution:

\[\tilde{\mathrm{y}}^{(r)}|\{\mathrm{y}=k,\mathbf{x}\}\sim\text{Cat}(\bm{s}^{(k,r )}),\] (2)

where \(\bm{s}^{(k,r)}=(s_{1}^{(k,r)},...,s_{K}^{(k,r)})^{\top}\in\mathcal{S}^{K-1}\) with \(\mathcal{S}^{K-1}=\{(s_{1},...,s_{K})^{\top}\in\mathbb{R}^{K}:s_{j}\geq 0\) for \(j\in[K]\) and \(\sum_{j=1}^{K}s_{j}=1\}\) representing the \((K-1)\)-dimensional simplex, and \(\text{Cat}(\bm{s}^{(k,r)})\) represents a categorical distribution specified by the parameter vector \(\bm{s}^{(k,r)}\). We extend existing works on mixed effects neural networks (MNN) [31; 32] by employing two nonlinear transformations \(\psi_{1}\) and \(\psi_{2}\) to incorporate different effects in the instance-dependent noise transition model, and set

\[\bm{s}^{(k,r)}=G(\bm{\omega}_{0}^{(k,r)})\text{ with }\bm{\omega}_{0}^{(k,r)}= \mathbf{A}_{0}^{(r)}\psi_{1}(\mathbf{x})+\mathbf{B}_{0}^{(k)}\psi_{2}( \mathbf{x}),\] (3)

where \(\mathbf{A}_{0}^{(r)}=(\bm{\alpha}_{10}^{(r)},...,\bm{\alpha}_{K0}^{(r)})^{\top}\) and \(\mathbf{B}_{0}^{(k)}=(\bm{\beta}_{10}^{(k)},...,\bm{\beta}_{K0}^{(k)})^{\top}\) are the regression weights; \(\psi_{1}(\mathbf{x})\) and \(\psi_{2}(\mathbf{x})\) can be modeled by some suitable networks; and \(G\) is a function mapping \(\mathbb{R}^{K}\) to \(\mathcal{S}^{K-1}\), which, in practice, is chosen to be the softmax function in the final layer. Utilizing two different network components \(\psi_{1}\) and \(\psi_{2}\) enables us to flexibly reflect possibly different effects of the annotator expertise (\(r\)) and the ground truth (\(k\)) in the annotation process, which can be interpreted as the input in mixed effects models.

Approximating the transition matrices.The proposed instance-dependent noise transition model can be learned by leveraging anchor points [12; 26; 27], or instances that are similar to anchor points learned from noisy training data [33]. An instance \(\mathbf{x}\) is defined to be an anchor point of class \(k\) if it belongs to the \(k\)th class almost surely, that is, \(\mathbb{P}(\mathrm{y}=k|\mathbf{x})=1\), and hence, \(\mathbb{P}(\tilde{\mathrm{y}}^{(r)}|\mathbf{x})=\mathbb{P}(\tilde{\mathrm{y}} ^{(r)}|\mathrm{y}=k,\mathbf{x})\). For \(k\in[K]\), let \(\overline{\mathcal{D}}_{0,k}\) be the set of anchor points of the \(k\)th class and the associated noisy annotations, i.e., \(\overline{\mathcal{D}}_{0,k}=\{\{\mathbf{x}_{i},\tilde{\mathbf{y}}_{i}\}: \mathbb{P}(\mathrm{y}_{i}=k|\mathbf{x}_{i})=1\}\). Define \(\overline{\mathcal{D}}_{0}=\overline{\mathcal{D}}_{0,1}\cup\overline{ \mathcal{D}}_{0,2}\cup\ldots\cup\overline{\mathcal{D}}_{0,K}\), and let \(n\) denote the subsample size of the learned anchor points, i.e., the cardinality of \(\overline{\mathcal{D}}_{0}\). Paired variables \(\{\mathbf{x}_{i},\tilde{\mathrm{y}}_{i}\}\) in \(\overline{\mathcal{D}}_{0}\) are independent, but **not necessarily identically** distributed (i.n.i.d). We write the input dimension \(p\) as \(p_{n}\) from now on to emphasize that its dependence on \(n\) is allowed.

In applications, overfitting can occur when the subsample size \(n\) of the learned anchor points is relatively small compared to the main sample size \(N\). To address this issue, we propose to learn \(\psi_{j}(\mathbf{x})\) with a sparse Bayesian DNN, denoted \(\psi_{j}(\mathbf{x};\bm{\theta}^{(j)})\), where \(\bm{\theta}^{(j)}\) represents the vector of all involved parameters in the network with \(j=1,2\). Furthermore, invoking the sparse Bayesian setting allows us to rigorously characterize the distance between the proposed model and the underlying true transition matrices, as presented in Theorem 1 of Section 3.2.

### Bayesian analysis and posterior consistency result

Prior specification.To implement sparse Bayesian analysis, we utilize the spike and slab prior [34] on the network parameters, offering an interpretable mechanism for variable selection. The spike and slab model is formulated by constructing a prior hierarchy of the involved parameters and selects nonzero coefficients according to the posterior inclusion probability. Marginally, these priors are mutually independent and have a mixture distribution consisting of a flat distribution (slab) and a distribution concentrated at zero (spike). Parameters with a small posterior mean will be set to zero to achieve sparsity.

Specifically, for network \(\psi_{j}(\mathbf{x};\bm{\theta}^{(j)})\), we write \(\bm{\theta}^{(j)}\) as \(\bm{\theta}^{(j)}=(\theta_{1}^{(j)},...,\theta_{J_{j}}^{(j)})^{\top}\) with \(J_{j}\) denoting the length of \(\bm{\theta}^{(j)}\) for \(j=1,2\). For \(k\in[J_{j}]\), we treat \(\theta_{k}^{(j)}\) as a random variable generated from the following prior hierarchy:

\[\gamma_{k}^{(j)} \sim\text{Bernoulli}(\lambda_{nj}),\] (4a) \[\theta_{k}^{(j)}|\gamma_{k}^{(j)} \sim\gamma_{k}^{(j)}\pi_{1}(\theta_{k}^{(j)};\sigma_{nj}^{2})+(1 -\gamma_{k}^{(j)})\pi_{0}(\theta_{k}^{(j)};c_{nj}\sigma_{nj}^{2}),\] (4b)

where \(\gamma_{k}^{(j)}\in\{0,1\}\) indicates whether or not \(\theta_{k}^{(j)}\) is nonzero, \(c_{nj}\) is specified as a very small positive number, \(\sigma_{nj}^{2}\) and \(c_{nj}\sigma_{nj}^{2}\) are the parameters related to the variances of distributions \(\pi_{1}(\cdot)\) and \(\pi_{0}(\cdot)\), respectively, and \(\lambda_{nj}\in(0,1)\) determines the ratio of the mixture distribution. As \(c_{nj}\to 0\)\(\pi_{0}(\theta^{(j)}_{k};c_{nj}\sigma^{2}_{nj})\) becomes the degenerate distribution at zero. The marginal distribution of \(\theta^{(j)}_{k}\) is then determined by

\[\theta^{(j)}_{k}\sim\lambda_{nj}\pi_{1}(\theta^{(j)}_{k};\sigma^{2}_{nj})+(1- \lambda_{nj})\pi_{0}(\theta^{(j)}_{k};c_{nj}\sigma^{2}_{nj}),\] (5)

which is presented as \(\pi^{(j)}(\cdot)\) for short; and this is taken as the prior distribution of \(\theta^{(j)}_{k}\).

To further incorporate the effects of the true label information and the randomness from different annotators in (3), we place the following probabilistic structure on the generic weights for \(\mathbf{A}^{(r)}_{0}\) and \(\mathbf{B}^{(k)}_{0}\) in (3), \(\mathbf{A}^{(r)}=(\bm{\alpha}^{(r)}_{1},...,\bm{\alpha}^{(r)}_{K})\) and \(\mathbf{B}^{(k)}=(\bm{\beta}^{(k)}_{1},...,\bm{\beta}^{(k)}_{K})\):

\[\bm{\alpha}^{(r)}_{j}\sim\mathcal{N}(\mathbf{0},\bm{\Sigma}^{(r)}_{\alpha}) \text{ and }\bm{\beta}^{(k)}_{j}\sim\mathcal{N}(\mathbf{0},\bm{\Sigma}^{(k)}_{\beta}).\] (6)

for \(j,k\in[K]\) and \(r\in[R]\), where \(\bm{\Sigma}^{(r)}_{\alpha}\) and \(\bm{\Sigma}^{(k)}_{\beta}\) are nonnegative definite matrices. We use \(\pi^{(r)}_{A}(\cdot)\) and \(\pi^{(k)}_{B}(\cdot)\) to denote the prior distribution of \(\mathbf{A}^{(r)}\) and \(\mathbf{B}^{(k)}\) in (6). Here the regression weights \(\mathbf{A}^{(r)}\) and \(\mathbf{B}^{(k)}\) can be seen as fully connected layers on top of \(\psi_{1}(\mathbf{x};\bm{\theta}^{(1)})\) and \(\psi_{2}(\mathbf{x};\bm{\theta}^{(2)})\), respectively. The conditions on the aforementioned priors are given in Appendix A.3.

Prior and posterior probability measure.Let \(\bm{\theta}=\left(\bm{\theta}^{(1)\top},\bm{\theta}^{(2)\top},\mathrm{vec}( \mathbf{B}^{(1)})^{\top},\ldots,\mathrm{vec}(\mathbf{B}^{(K)})^{\top},\right.\)

\(\left.\mathrm{vec}(\mathbf{A}^{(1)})^{\top},\ldots,\mathrm{vec}(\mathbf{A}^{ (R)})^{\top}\right)^{\top}\in\Theta\) stand for the vector of all involved parameters in the noise transition model, with \(\Theta\) denoting the parameter space. We use \(\bm{\theta}_{0}\) to represent the true value of \(\bm{\theta}\), which is an interior point of \(\Theta\). The foregoing specification of the prior distribution places a prior probability measure, denoted \(\Pi(\cdot)\), on \(\bm{\theta}\). With the data \(\overline{\mathcal{D}}_{0}\), the posterior probability measure \(\Pi(\cdot|\overline{\mathcal{D}}_{0})\) is given by

\[\Pi(G|\overline{\mathcal{D}}_{0})=\frac{\int_{G}p^{n}_{\bm{\theta}}(\overline {\mathcal{D}}_{0})d\Pi(\bm{\theta})}{\int_{\Theta}p^{n}_{\bm{\theta}}( \overline{\mathcal{D}}_{0})d\Pi(\bm{\theta})}\text{ for any }G\in\mathcal{G},\] (7)

where \(\mathcal{G}\) is the \(\sigma\)-field on \(\Theta\), and \(p^{n}_{\bm{\theta}}\) is the joint probability density or mass function for the observations in \(\overline{\mathcal{D}}_{0}\) under \(\bm{\theta}\). Let \(\mathbb{P}^{n}_{\bm{\theta}}(\cdot)\) denote the probability measure associated with \(p^{n}_{\bm{\theta}}(\cdot)\), and write \(\mathbb{P}^{n}_{0}(\cdot)\triangleq\mathbb{P}^{n}_{\bm{\theta}_{0}}(\cdot)\). Hence, the data \(\overline{\mathcal{D}}_{0}\) is generated from \(\mathbb{P}^{n}_{0}(\cdot)\) in our setup.

Let \(f\) denote the unknown density of \(\mathbf{x}\). For \(\{\mathbf{x},\tilde{\mathbf{y}}\}\in\overline{\mathcal{D}}_{0,k}\), let \(f^{(k,r)}_{0}\) and \(f^{(k)}_{0}\) respectively represent the underlying true distributions for \(\tilde{y}^{(r)}\) and \(\tilde{\mathbf{y}}\), given \(\{\mathrm{y}=k,\mathbf{x}\}\), determined by (2) and (3); and let \(f^{(k,r)}_{\bm{\theta}}\) and \(f^{(k)}_{\bm{\theta}}\) denote the corresponding distributions characterized by the model indexed by \(\bm{\theta}\). We let \(p_{\bm{\theta},i}\triangleq\frac{f_{\bm{\theta}}(k_{i})}{f}\) denote the probability density or mass function of the \(i\)th component in \(\overline{\mathcal{D}}_{0}\) under \(\bm{\theta}\), with \(k_{i}\in[K]\) denoting the class that the instance belongs to almost surely. Then, the joint probability density or mass function \(p^{n}_{\bm{\theta}}\) is calculated as \(p^{n}_{\bm{\theta}}\triangleq\prod_{i=1}^{n}p_{\bm{\theta},i}\). The following theorem describes the closeness of the proposed noise transition model and the true annotator confusions with respect to the Hellinger distance within the Bayesian framework.

**Theorem 1**.: _Suppose Conditions A.1-A.4 in Appendix A.2 and B.1-B.3 in Appendix A.3 are satisfied. Let \(d(\cdot,\cdot)\) and \(d_{n}(\cdot,\cdot)\)denote the Hellinger distance given in Definition 1 and the semimetric defined in (16) in Appendix B.1, respectively. Then there exists a sequence of constants \(\{\epsilon^{2}_{n}\}_{n=1}^{\infty}\) with \(\epsilon^{2}_{n}=O(\varpi_{n1}+\varpi_{n2}+\zeta_{n})\) and \(\log(1/\epsilon^{2}_{n})<n\epsilon^{2}_{n}\), satisfying \(0<\epsilon^{2}_{n}<1\), \(\epsilon_{n}\to 0\) and \(n\epsilon^{2}_{n}\rightarrow\infty\) as \(n\rightarrow\infty\), such that 2,_

Footnote 2: The anchor point assumption can be relaxed and Theorem 1 can be extended to a more general setting. For any given \(\delta\in(0,1)\), we define the \(\delta\)-pseudo anchor point for class \(k\) as \(\mathbb{P}(y=k|\mathbf{x})\geq 1-\delta\) and denote \(\overline{\mathcal{D}}_{\delta}\) as the set of all \(\delta\)-pseudo anchor point accordingly. Then, the following result holds: \(\Pi\{\theta\in\Theta:d_{n}(\theta,\theta_{0})>M_{n}\epsilon_{n}+C\delta| \overline{\mathcal{D}}_{\delta}\}\to 0\) in \(\mathbb{P}^{n}_{0}\) probability for every \(M_{n}\rightarrow\infty\), where \(C\) is a positive constant. From the modified theory, as \(\delta\) approaches \(0\) slowly, the Hellinger distance of the transition model and the true transition probability converges to zero at a slow rate. In other words, the transition model will still converge even if the collection of a set of anchor points is not guaranteed, albeit at a slow rate.

\[\Pi\left\{\bm{\theta}\in\Theta:d_{n}(\bm{\theta},\bm{\theta}_{0})>M_{n} \epsilon_{n}|\overline{\mathcal{D}}_{0}\right\}\to 0\] (8)

_in \(\mathbb{P}^{n}_{0}\) probability for every \(M_{n}\rightarrow\infty\), where \(\{\varpi_{jn}\}\) is a sequence of nonnegative numbers converging to 0 as \(n\rightarrow\infty\) for \(j=1,2\) as given in (14), and \(\{\zeta_{n}\}_{n=1}^{\infty}\) is a sequence given in Appendix A.3depending on the structures of \(\psi_{1}(\cdot|\bm{\theta}^{(1)})\) and \(\psi_{2}(\cdot|\bm{\theta}^{(1)})\) with \(\zeta_{n}\to 0\) as \(n\to\infty\). If we further assume that \(|\overline{\mathcal{D}}_{0,k}|/|\overline{\mathcal{D}}_{0}|>\varsigma_{1}\) for some positive constant \(\varsigma_{1}\), with \(|\cdot|\) representing the cardinality of a set, then for any \(k\in[K]\) and \(r\in[R]\)_

\[\Pi\left\{\bm{\theta}\in\Theta:d(f_{\bm{\theta}}^{(k,r)},f_{0}^{(k,r)})>M_{n} \epsilon_{n}|\overline{\mathcal{D}}_{0}\right\}\to 0,\] (9)

_in \(\mathbb{P}_{0}^{n}\) probability for any \(M_{n}\to\infty\)._

Intuitively, Theorem 1 reveals that the sparse noise transition model is close to the underlying true transition matrix with respect to the Hellinger distance under mild conditions. Notably, our posterior consistency result extends the existing theories in sparse Bayesian learning [35, 36, 37] to the setup of i.n.i.d observations. Moreover, this result on the convergence rate of the posterior measure allows us to infer the underlying true label from the noisy annotations with a theoretical guarantee on the bounds of the Bayes error, which will be discussed in detail in the following section.

### Pairwise likelihood ratio test for label correction

The asymptotic result (9) in Theorem 1 indicates that for each annotator, the underlying true instance-dependent transition matrix can be accurately modeled under the Bayesian framework. This enables us to aggregate and infer the ground truth label from noisy crowdsourced annotations.

A novel label correction algorithm.To highlight the idea, we first assume that the noise transition matrix \(\mathbb{P}(\tilde{\mathrm{y}}^{(r)}|y=k,\mathbf{x})\), or \(f_{0}^{k,r}(\cdot)\), is known. To simplify the notation, for each \(\mathbf{x}_{i}\) in the noisy dataset \(\mathcal{D}\), denote \(\tau_{i,k}^{(r)}\triangleq\tau_{kl}^{(r)}(\mathbf{x}_{i})\triangleq\mathbb{P }(\tilde{\mathrm{y}}^{(r)}=l|\mathrm{y}=k,\mathbf{x}_{i})\) for \(i\in[N]\) and \(k,l\in[K]\). We assign class prior [38]\(\bm{h}_{i}=(h_{i,1},...,h_{i,K})^{\top}\) for the ground truth label for the \(i\)th task, where the \(h_{i,k}\) for \(k\in[K]\) are nonnegative weights satisfying \(\sum_{k=1}^{K}\mathds{1}_{i,k}=1\). For each instance, with the class prior and the noise transition matrices, the label correction process can be formulated as a hypothesis testing problem, where different hypotheses are generated from different choices of the true label values. Specifically, selecting the label for the instance \(\mathbf{x}_{i}\) from \(\{g,g^{\prime}\}\), with \(1\leq g<g^{\prime}\leq K\), is equivalent to choosing from the two competitors \(\mathbb{P}(\tilde{\mathbf{y}}|y=g,\mathbf{x}_{i})\) and \(\mathbb{P}(\tilde{\mathbf{y}}|y=g^{\prime},\mathbf{x}_{i})\). We thereby consider the following hypothesis testing problem: \(H_{g}:\tilde{\mathbf{y}}_{i}|\{\mathrm{y}_{i},\mathbf{x}_{i}\}\sim\mathbb{P} (\tilde{\mathbf{y}}|y=g,\mathbf{x}_{i})\) versus \(H_{g^{\prime}}:\tilde{\mathbf{y}}_{i}|\{\mathrm{y}_{i},\mathbf{x}_{i}\} \sim\mathbb{P}(\tilde{\mathbf{y}}|y=g^{\prime},\mathbf{x}_{i})\). By the Neyman-Pearson Lemma [39], the Bayes testing error is minimized by the likelihood ratio test, and the decision region for hypothesis \(H_{g}\) is given by

\[\left\{\tilde{\mathbf{y}}:\frac{h_{i,g}\mathbb{P}(\tilde{\mathbf{y}}|y=g, \mathbf{x}_{i})}{h_{i,g^{\prime}}\mathbb{P}(\tilde{\mathbf{y}}|y=g^{\prime}, \mathbf{x}_{i})}=\frac{\h_{i,g}\prod_{r=1}^{R}\prod_{l=1}^{K}\left\{\tau_{i, gl}^{(r)}\right\}^{1(\tilde{g}^{(r)}=l)}}{\h_{i,g^{\prime}}\prod_{r=1}^{R} \prod_{l=1}^{K}\left\{\tau_{i,g^{\prime}}^{(r)}\right\}^{1(\tilde{g}^{(r)}=l)} >1}\right\}.\]

Building from the abovementioned reformulation of the label correction process, we now propose an algorithm to infer the underlying ground truth by aggregating noisy crowdsourced annotations with the help of the annotator confusions. Formally, we propose the following label correction method by setting the estimated label of \(\mathbf{x}_{i}\) to be \(\overline{\mathrm{y}}_{i}\triangleq g\) if

\[\frac{\h_{i,g}\prod_{r=1}^{R}\prod_{l=1}^{K}\left\{\tau_{i,gl}^{(r)}\right\}^{ \mathbf{1}(\tilde{g}_{i}^{(r)}=l)}}{\h_{i,g^{\prime}}\prod_{r=1}^{R}\prod_{l= 1}^{K}\left\{\tau_{i,g^{\prime}l}^{(r)}\right\}^{\mathbf{1}(\tilde{g}_{i}^{(r)} =l)}}>\Omega\text{ for any }g^{\prime}\neq g,\] (10)

where \(\Omega\geq 1\) is a pre-specified threshold.

Information-theoretic bounds on the Bayes error.To theoretically justify the effectiveness of the proposed label correction method (10), we derive information-theoretic bounds on the Bayes error, given the instances. Let \(\overline{\mathcal{D}}=\{\mathbf{x}_{i},\overline{\mathrm{y}}_{i}\}_{i=1}^{ \bar{n}}\) denote the collection of instances with estimated labels, where \(\bar{n}\) represents the size of \(\overline{\mathcal{D}}\). We write \(\overline{\mathrm{y}}=\{\overline{\mathrm{y}}_{i}\}_{i=1}^{\bar{n}}\) and the corresponding true label is denoted \(\mathbf{y}=\{\mathrm{y}_{i}\}_{i=1}^{\bar{n}}\). A loss measured by the accuracy of the estimated labels is given by \(\mathcal{L}(\overline{\mathrm{y}},\mathbf{y})=\frac{1}{\bar{n}}\sum_{i=1}^{\bar {n}}\mathbf{1}(\overline{\mathrm{y}}_{i}\neq\mathrm{y}_{i})\). Let \(\mathbb{P}(\cdot|\mathbf{y};\bm{\tau})\) denote the joint probability distribution of \(\{\tilde{\mathbf{y}}_{i}\}\), given \(\mathbf{y}\) and \(\bm{\tau}\), and let \(\mathbb{E}(\cdot|\mathbf{y};\bm{\tau})\) denote the associated expectation operator, where \(\bm{\tau}\triangleq\{\bm{\tau}_{i}\}_{i=1}^{\bar{n}}\triangleq\{\bm{\tau}_{i}^{ (r)}:\)\(r\in[R]\backslash\bar{i}_{i=1}^{\bar{n}}\) represents the collection of the corresponding transition matrices \(\bm{\tau}_{i}^{(r)}\) having \(\tau_{i,kl}^{(r)}\) as its \((k,l)\) element. Then, the Bayes risk is defined as [40]

\[\Re_{\text{Bayes}}(\bm{\hbar},\mathcal{L})=\inf_{\overline{\mathbf{y}}}\left[ \sum_{\mathbf{y}\in[K]^{n}}\hbar(\mathbf{y})\mathbb{E}\{\mathcal{L}(\overline {\mathbf{y}},\mathbf{y})|\mathbf{y};\bm{\tau}\}\right],\] (11)

or \(\Re_{\text{Bayes}}\) for short, where \(\hbar(\mathbf{y})\) is the joint prior probability of \(\mathbf{y}\) calculated from \(\bm{\hbar}\triangleq\{\bm{\hbar}_{i}\}_{i=1}^{\bar{n}}\). The following theorem identifies bounds for the Bayes risk.

**Theorem 2**.: _Let \(D_{{}_{KL}}\left(\bm{\tau}_{i,g*}^{(r)}\|\bm{\tau}_{i,g^{\prime}*}^{(r)}\right)\) denote the Kullback-Leibler (KL) divergence for discrete distributions \(\bm{\tau}_{i,g*}^{(r)}\) and \(\bm{\tau}_{i,g^{\prime}*}^{(r)}\), where, for \(i\in[\overline{n}]\), \(r\in[R]\), and \(g,g^{\prime}\in[K]\), \(\bm{\tau}_{i,g*}^{(r)}\) and \(\bm{\tau}_{i,g^{\prime}*}^{(r)}\) stand for the \(g\)th and \(g^{\prime}\)th rows of \(\bm{\tau}_{i}^{(r)}\), respectively. For \(\bm{\hbar}=\{\bm{\hbar}_{i}\}_{i=1}^{\overline{n}}\) and \(\bm{\tau}=\{\bm{\tau}_{i}\}_{i=1}^{\overline{n}}\), define_

\[\overline{D}_{{}_{KL}}(\bm{\hbar},\bm{\tau})=\frac{1}{\bar{n}}\sum_{i=1}^{ \bar{n}}\sum_{r=1}^{R}\sum_{g=1}^{K}\sum_{g^{\prime}=1}^{K}\hbar_{i,g}h_{i,g^{ \prime}}D_{{}_{KL}}\left(\bm{\tau}_{i,g*}^{(r)}\|\bm{\tau}_{i,g^{\prime}*}^{( r)}\right)\text{ and }\]

\[C_{gg^{\prime}}^{(i)}=-\min_{0\leq\lambda\leq 1}\frac{1}{R}\left[-\lambda \log\left(\frac{\Omega\hbar_{i,g}}{\hbar_{i,g^{\prime}}}\right)+\sum_{r=1}^{R }\log\left\{\sum_{l=1}^{K}\left(\tau_{i,gl}^{(r)}\right)^{1-\lambda}\left( \tau_{i,g^{\prime}l}^{(r)}\right)^{\lambda}\right\}\right].\]

_For \(i\in\bar{n}\) and \(g\in[K]\), let \(I_{\Omega}^{(g)}(\bm{\hbar}_{i},\bm{\tau}_{i})=\min_{g^{\prime}\neq g}C_{gg^{ \prime}}^{(i)}\). Then the Bayes error defined in (11) is bounded as follows:_

\[\frac{1}{\bar{n}}\left[1-\frac{\overline{D}_{{}_{KL}}(\bm{\hbar}, \bm{\tau})+\frac{1}{\bar{n}}\log(2-\prod_{i=1}^{\bar{n}}\max_{k\in[K]}\hbar_{i,k})}{\left\{\sum_{i=1}^{\bar{n}}\log(\max_{k\in[K]}\hbar_{i,k})\right\}/\bar{ n}}\right]\] \[\leq \Re_{\text{Bayes}}(\bm{\hbar},\mathcal{L})\leq\frac{K-1}{\bar{n}} \sum_{i=1}^{\bar{n}}\sum_{g=1}^{K}\hbar_{i,g}\exp\left\{-RI_{\Omega}^{(g)}(\bm {\hbar}_{i},\bm{\tau}_{i})\right\}.\]

_Remark 1_.: Theorem 2 establishes information-theoretic bounds on the Bayes error \(\Re_{\text{Bayes}}\) for arbitrary priors \(\bm{\hbar}_{i}\) with \(i\in[\overline{n}]\), which theoretically quantifies the combined impact of the prior knowledge and annotators' expertise on label accuracy using algorithm (10). The lower bound is proved in light of the concept of \(f\)-informativity [40, 41, 42], and is stronger than the commonly-used Bayes lower bound based on Fano's inequality [43, 44]. The proof of the upper bound considers the inference procedure of \(\overline{\mathbf{y}}\) and applies Markov's inequality. The details are given in Appendix B.

_Remark 2_.: The quantity \(C_{gg^{\prime}}^{(i)}\) in Theorem 2 reflects how the identified upper bound of the Bayes error may be influenced by the prior \(\bm{\hbar}_{i}\) and the ability of the \(R\) annotators to distinguish between labels \(g\) and \(g^{\prime}\) for instance \(i\). If we set \(\bm{\hbar}_{i}\) to be the uniform prior and let \(\Omega=1\), \(C_{gg^{\prime}}^{(i)}\) will degenerates to the average of the Chernoff information between \(\{\bm{\tau}_{i,g*}^{(r)}\}_{r=1}^{R}\) and \(\{\bm{\tau}_{i,g^{\prime}*}^{(r)}\}_{r=1}^{R}\)[45, 46], which is a statistical divergence measuring the deviation between two probability distributions.

Result under the sparse Bayesian model.The label correction method (10) is not immediately applicable if we have no access to the underlying true annotator confusions, which is usually the case in real-world applications. To get around the issue induced from the unknownness of the true noise transition probability \(f_{0}^{(k,r)}\), we consider the model \(f_{\bm{\theta}}^{(k,r)}\) given in Theorem 1 and write the corresponding transition matrices as \(\overline{\tau}_{i,kl}^{(r)}\triangleq\overline{\tau}_{kl}^{(r)}(\mathbf{x}_{ i})\triangleq f_{\bm{\theta}}^{(k,r)}(\tilde{y}|\mathbf{x}_{i})\big{|}_{\tilde{y}=l}\) for \(k,l\in[K]\) and \(r\in[R]\). We then replace the \(\tau_{i,gl}^{(r)}\) in (10) with \(\overline{\tau}_{i,gl}^{(r)}\) to determine corrected labels. With a slight abuse of notation, we still use \(\overline{\mathcal{D}}=\{\mathbf{x}_{i},\overline{y}_{i}\}_{i=1}^{\bar{n}}\) to denote the set of instances with estimated labels but let \(\overline{\Re}_{\text{Bayes}}\) denote the resulting Bayes error. We let \(\overline{\mathcal{D}}_{\mathbf{x}}=\{\mathbf{x}_{i}\}_{i=1}^{\overline{n}}\) represent the set of the considered instances. Combining Theorems 1 and 2 yields the following corollary.

**Corollary 3**.: _Suppose that the conditions in Theorem 1 are satisfied, and further assume that \(f(\mathbf{x}_{i})>\varsigma_{2}\) for \(\mathbf{x}_{i}\in\overline{\mathcal{D}}_{\mathbf{x}}\) and \(\min_{r\in[R],i\in[\overline{n}],k,l\in[K]}\overline{\tau}_{i,kl}^{(r)}\geq \varsigma_{3}\), where \(f(\cdot)\) is the probability density function of \(\mathbf{x}\), and \(\varsigma_{2}\) and \(\varsigma_{3}\) are some positive constants. Then, for any \(\epsilon>0\),_

\[\Pi\left[\bm{\theta}:\overline{\Re}_{\text{Bayes}}\leq\frac{K-1}{\bar{n}}\sum_{i= 1}^{\bar{n}}\sum_{g=1}^{K}\hbar_{i,g}\exp\left\{-RI_{\Omega}^{(g)}(\bm{\hbar}_ {i},\bm{\tau}_{i})+\epsilon\right\}\left|\overline{\mathcal{D}}_{0}\right| \longrightarrow 1\]

_in \(\mathbb{P}_{0}^{n}\) probability as \(n\to\infty\), where \(I_{\Omega}^{(g)}(\bm{\hbar}_{i},\bm{\tau}_{i})\) is given in Theorem 2 for \(i\in[\bar{n}]\) and \(g\in[K]\)._

## 4 Algorithm

Learning the noise transition model.In the warm-up stage, we train the base models on noisy training data and obtain the set of anchor points \(\overline{\mathcal{D}}_{0}\)[26]. With \(\overline{\mathcal{D}}_{0}\), we first obtain the maximum a posteriori (MAP) estimate of network parameters of the transition model \(\bm{\theta}\) by maximizing the log posterior distribution of \(\bm{\theta}\), with the constant term omitted,

\[\widehat{\bm{\theta}}=\arg\max_{\bm{\theta}}\left\{\sum_{i=1}^{n}\log p_{\bm {\theta},i}+\log\pi(\bm{\theta})\right\},\] (12)

where \(p_{\bm{\theta},i}\) is the probability mass function of the \(i\)th component in \(\overline{\mathcal{D}}_{0}\) given before Theorem 1, and \(\pi(\bm{\theta})\) is the probability density function of \(\bm{\theta}\) relative to the prior probability measure \(\Pi(\cdot)\). Given the MAP estimate \(\widehat{\bm{\theta}}\), according to the prior hierarchy (4), the posterior inclusion probability of the \(k\)th parameter in the network \(\psi_{j}(\cdot;\bm{\theta}^{(j)})\) is calculated as

\[\mathbb{P}(\gamma_{k}^{(j)}=1|\widehat{\bm{\theta}})=\frac{\lambda_{nj}\pi_{1 }(\widehat{\theta}_{k}^{(j)};\sigma_{nj}^{2})}{\lambda_{nj}\pi_{1}(\widehat{ \theta}_{k}^{(j)};\sigma_{nj}^{2})+(1-\lambda_{nj})\pi_{0}(\widehat{\theta}_{ k}^{(j)};c_{nj}\sigma_{nj}^{2})}\] (13)

for \(k\in[J_{j}]\) with \(j=1,2\). If the posterior inclusion probability is smaller than a pre-specified threshold, chosen to be 0.5 in our experiments, the associated parameter is zet to be zero. We then fine tune the sparse network and obtain the noise transition model.

Training the classifiers with corrected labels.With the noise transition model trained, we then train the base models with the label correction algorithm proposed in Section 3.3. Specifically, we train two base classifiers to reciprocally provide class priors for each instance in the label correction process. In the \(t\)th epoch, for instance \(\mathbf{x}_{i}\), if \(\overline{y}_{i}\) satisfies the condition (10) for the pre-specified threshold \(\Omega_{t}\), we put \(\{\mathbf{x}_{i},\overline{y}_{i}\}\) in \(\overline{D}_{t}\). The base models are then updated on the collected dataset \(\overline{D}_{t}\). The complete pseudocode of our algorithm is included in Appendix C.

## 5 Experiments

Datasets.We assess the effectiveness of our method on three image datasets with synthetic annotations, MNIST [47], CIFAR-10 [48], and CIFAR-100 [48], and two datasets with human annotations, CIFAR-10N [49] and LabelMe [50, 51]. Detailed information can be found in Appendix C. For all the datasets except LabelMe, we leave out 10% of the training data as a noisy validation set.

Noise generation.For the three datasets, MNIST, CIFAR10, and CIFAR100, we consider three groups of annotators with varying expertise, with an average labeling error rate of about 20%, 35% and 50%, respectively. We abbreviate these three groups as IDN-LOW, IDN-MID, and IDN-HIGH, which represent instance-dependent annotators with low, middle ("mid" for short), and high labeling error rates, respectively. To generate noisy annotations, we independently simulate \(R=5\) annotators for each group according to Algorihtm 2 in [12], with IDN-\(\tau\) denoting that the noise rate is upper bounded by \(\tau\) for each annotator. For each instance, we then randomly choose one of the annotations given by the \(R\) annotators, which is designed to evaluate the methods under incomplete annotator labeling. We manually corrupt the three datasets according to the following three groups of annotators:

**(I) IDN-LOW.**_IDN-10%, IDN-10%, IDN-20%, IDN-20%, IDN-30%;_

**(II) IDN-MID.**_IDN-30%, IDN-30%, IDN-40%, IDN-40%, IDN-50%;_

**(III) IDN-HIGH.**_IDN-50%, IDN-50%, IDN-60%, IDN-70%._Experiment setup.The network structure for the MNIST dataset is chosen to be Lenet-5 [52]. We choose ResNet-18 [2] for CIFAR-10 and CIFAR-10N, and ResNet-34 architecture [2] for CIFAR-100 dataset. As in [53], we employ the pretrained VGG-16 network followed by a fully connected layer and a softmax output layer for the LabelMe dataset, using 50% dropout. More implementation details can be found in Appendix C.

Competing methods.We compare the proposed method with the following state-of-art methods: (1) CE (Clean), which trains the network with the standard cross entropy loss on the clean datasets; (2) CE (MV), which trains the network using the labels from majority voting; (3) CE (EM) [9]; (4) DoctorNet [54]; (5) GCE [55]; (6) Co-teaching [56]; (6) Co-teaching+ [57]; (7) BLTM [17]; (8) MBEM [11]; (9) CrowdLayer [53]; (10) TraceReg [8]; (11) Max-MIG [7]; (12) CoNAL [58]; (13) GeoCrowdNet (F) [13]; and (14) GeoCrowdNet (W) [13]. Among these methods, GCE, Co-teaching, Co-teaching+, and BLTM are strong baselines dealing with single noisy label issues, and we adapt them to the multiple annotations setting by utilizing the majority vote labels for loss computation.

Ablation study.In Figure 1, We plot the average estimation error for the noise transition matrices to demonstrate the effectiveness of the proposed method in modeling the instance-dependent annotator confusions. For instance \(\mathbf{x}_{i}\) with clean class label \(\mathrm{y}_{i}\) in the validation set, we analyse the \(\mathrm{y}_{i}\)th row rather than the whole noise transition matrix as in previous studies [16; 12]. Specifically, let \(\widehat{\pi}^{(r)}(\mathbf{x}_{i})\) and \(\tau^{(r)}(\mathbf{x}_{i})\) represent the estimated and the true noise transition matrix for annotator \(r\), respectively. The estimation error on instance \(\mathbf{x}_{i}\) is defined as \(err_{i}^{(r)}=\max_{l\in K}|\widehat{\tau}^{(r)}(\mathbf{x}_{i})_{\mathrm{y} _{i},l}-\tau^{(r)}(\mathbf{x}_{i})_{\mathrm{y}_{i},l}|\), where \(\widehat{\tau}^{(r)}(\mathbf{x}_{i})_{\mathrm{y}_{i},l}\) and \(\tau^{(r)}(\mathbf{x}_{i})_{\mathrm{y}_{i},l}\) stand for the \((\mathrm{y}_{i},l)\) element in the corresponding matrices. The average estimation error for annotator \(r\) is then calculated as \(\frac{1}{n_{v}}\sum_{i=1}^{n_{v}}err_{i}^{(r)}\) with \(n_{v}\) denoting the size of the validation set. For each annotator, we compare the average estimation error of the proposed method with six baselines, CrowdLayer [53], TraceReg [8], GeoCrowdNet (F) [13], GeoCrowdNet (W) [13], MBEM [11], and BLTM [17], on the CIFAR10 dataset. In most of the cases, the proposed method leads to smaller estimation error especially when the noise rate is high, which shows the efficacy of the proposed sparse Bayesian model.

Figure 1: Average estimation error of annotator-specific instance-dependent noise transition matrices on CIFAR10. The error bar for standard deviation has been shaded.

Figure 2: Average accuracy of learning CIFAR-10 dataset with varying number of annotators. The error bar for standard deviation has been shaded.

Classification accuracy.Table 1 presents the average test accuracy of 5 random trials on the datasets of CIFAR-10, CIFAR-100, CIFAR-10N and LabelMe, together with the standard errors of the test accuracies of the random trials, expressed after the plus/minus sign \(\pm\), where the two highest accuraries are bold faced; standard errors of the accuracies are calculated based on repeating those experiments 5 times, each with a different random seed. All the results demonstrate the superior performance of the proposed method on both synthetic and real-world noisy datasets. Moreover, to investigate the influence of the sparsity of annotations, we conduct more experiments with the number of annotators varying from 5 to 100, and each instance only has one label. Figure 2 shows the average accuracy with various numbers of annotators, which further exhibit the advantages of the proposed method under different settings. Additional experimental results, including the test accuracy on MNIST, the average estimation error on MNIST and CIFAR100, and the accuracy of the corrected labels using algorithm (10), are deferred to Appendix C to save space.

## 6 Conclusion

In this paper, we address the challenge of training classifiers using noisy crowdsourced labels, a common issue in various applications. We formulate the annotator-specific instance-dependent noise transition matrix within the Bayesian framework, and theoretically characterize the closeness of the proposed model and the true annotator confusions with respect to the Hellinger distance. Our result is established for the setup of i.n.i.d. observations, which substantially broadens the application scope of our method. Building on the convergence rate of the posterior measure, we propose a novel algorithm to aggregate noisy annotations and infer the ground truth label based using pairwise LRT. Additionally, we provide information-theoretic bounds on the Bayes error of the proposed algorithm. Empirical evidence demonstrates the effectiveness of our algorithm on both synthetic and real-world noisy datasets.

## Limitations and Extensions

Our work can be further extended in different directions. It is interesting to generalize the setup here to the hierarchical classification setup. Instance-dependent transition matrices can be further refined with varying structures imposed and are learned with manifold regularization.

## Acknowledgements

Yi is the Canada Research Chair in Data Science (Tier 1). Her research was supported by the Canada Research Chairs Program and the Natural Sciences and Engineering Research Council of Canada (NSERC).

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-100} & \multicolumn{3}{c}{CIFAR-10N} & LabelMe \\ \cline{2-7}  & IDN-LOW & IDN-MID & IDN-HIGH & IDN-LOW & IDN-MID & IDN-HIGH & & \\ \hline CE (Clean) & \multicolumn{3}{c}{86.60\(\pm\)0.79} & \multicolumn{3}{c}{58.75\(\pm\)0.55} & \multicolumn{3}{c}{88.60\(\pm\)0.79} & 91.45\(\pm\)0.23 \\ \hline CE (MV) & \(80.90\pm 0.88\) & \(76.05\pm 0.70\) & \(69.65\pm 1.73\) & \(50.96\pm 0.49\) & \(44.80\pm 0.99\) & \(38.51\pm 0.66\) & \(82.82\pm 0.05\) & \(79.49\pm 0.48\) \\ CE (EM) [9] & \(81.15\pm 0.74\) & \(75.84\pm 0.97\) & \(69.85\pm 1.43\) & \(51.29\pm 1.00\) & \(45.24\pm 0.41\) & \(38.01\pm 0.90\) & \(83.14\pm 0.80\) & \(80.64\pm 0.55\) \\ DotereNet [54] & \(81.85\pm 0.41\) & \(78.69\pm 0.75\) & \(76.26\pm 1.28\) & \(52.61\pm 0.70\) & \(47.80\pm 0.86\) & \(43.50\pm 0.53\) & \(84.52\pm 0.69\) & \(79.09\pm 0.40\) \\ CGC [55] & \(82.78\pm 0.51\) & \(78.08\pm 1.18\) & \(72.91\pm 1.92\) & \(**55.88\pm 1.32\) & \(48.46\pm 0.36\) & \(50.53\pm 0.83\) & \(85.25\pm 0.46\) & \(80.27\pm 0.27\) \\ Co-teaching [56] & \(83.20\pm 0.53\) & \(88.02\pm 0.42\) & \(53.27\pm 0.42\) & \(47.58\pm 0.43\) & \(**45.49\pm 0.72\) & \(85.90\pm 0.50\) & \(80.24\pm 0.71\) \\ Co-teaching+ [57] & \(81.27\pm 0.44\) & \(78.26\pm 0.27\) & \(72.10\pm 0.98\) & \(93.81\pm 0.81\) & \(48.15\pm 0.36\) & \(42.07\pm 0.66\) & \(82.31\pm 0.89\) & \(81.67\pm 0.56\) \\ BLIM [17] & \(81.06\pm 0.23\) & \(77.34\pm 0.51\) & \(76.43\pm 1.95\) & \(52.21\pm 0.70\) & \(46.90\pm 0.85\) & \(41.26\pm 1.59\) & \(52.62\pm 0.17\) & \(80.44\pm 1.05\) \\ MBEM [11] & \(82.37\pm 0.77\) & \(78.05\pm 0.83\) & \(71.43\pm 2.43\) & \(52.20\pm 0.77\) & \(45.26\pm 0.38\) & \(38.92\pm 0.69\) & \(85.49\pm 0.43\) & \(80.10\pm 0.19\) \\ CrowdLayer [53] & \(83.98\pm 0.35\) & \(77.76\pm 1.06\) & \(67.77\pm 1.69\) & \(51.28\pm 0.64\) & \(45.28\pm 0.64\) & \(38.93\pm 0.76\) & \(82.84\pm 0.24\) & \(82.95\pm 0.21\) \\ TraceReg [8] & \(83.49\pm 1.68\) & \(78.69\pm 1.94\) & \(70.39\pm 1.68\) & \(68.60\pm 0.99\) & \(45.16\pm 0.39\) & \(39.10\pm 0.83\) & \(83.16\pm 0.24\) & \(82.93\pm 0.15\) \\ Max-MIG [7] & \(81.00\pm 0.72\) & \(75.90\pm 0.52\) & \(79.96\pm 0.96\) & \(51.76\pm 1.11\) & \(44.93\pm 0.71\) & \(38.70\pm 0.49\) & \(85.12\pm 0.36\) & \(83.25\pm 0.26\) \\ CoNAL [58] & \(81.60\pm 0.82\) & \(76.02\pm 0.79\) & \(69.50\pm 1.89\) & \(51.61\pm 1.14\) & \(44.19\pm 0.62\) & \(38.24\pm 0.29\) & \(83.01\pm 0.21\) & \(82.96\pm 0.30\) \\ GeoCrowNet (F) [13] & \(\textbf{86}.36\pm 0.46\) & \(\textbf{83.78}\pm 0.68\) & \(79.70\pm 0.42\) & \(51.37\pm 0.84\) & \(45.04\pm 0.56\) & \(38.94\pm 0.91\) & \(87.70\pm 0.51\) & \(\textbf{85.74}\pm 0.17\) \\ GeoCrowNet (W/13) & \(\textbf{83.95}\pm 0.41\) & \(76.94\pm 0.72\) & \(64.68\pm 2.53\) & \(51.58\pm 0.72\) & \(45.24\pm 1.15\) & \(39.24\pm 0.76\) & \(\textbf{87.84}\pm 0.21\) & \(38.28\pm 0.45\) \\ \hline Ours & \(\textbf{86}.88\pm 0.65\) & \(\textbf{85}.40\pm 0.50\) & \(\textbf{83.46}\pm 1.24\) & \(\textbf{59.81}.\pm 0.55\) & \(\textbf{54.88}.2\pm 0.60\) & \(\textbf{49.44}\pm 1.30\) & \(\textbf{88.19}\pm 0.47\) & \(\textbf{84.85}\pm 0.27\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average accuracy of learning CIFAR-10, CIFAR-100, CIFAR-10N and LabelMe datasets

## References

* [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 60(6):84-90, 2017.
* [2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2016.
* [3] Yan Yan, Romer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer Dy. Learning from multiple annotators with varying expertise. _Machine Learning_, 95:291-327, 2014.
* [4] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In _International Conference on Machine Learning_, 2017.
* [5] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [6] James Surowiecki. _The Wisdom of Crowds_. Anchor, 2005.
* [7] Peng Cao, Yilun Xu, Yuqing Kong, and Yizhou Wang. Max-mig: an information theoretic approach for joint learning from crowds. _arXiv preprint arXiv:1905.13436_, 2019.
* [8] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan Silberman. Learning from noisy labels by regularized estimation of annotator confusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019.
* [9] Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. _Journal of the Royal Statistical Society: Series C_, 28(1):20-28, 1979.
* [10] Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier Movellan, and Paul Ruvolo. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. _Advances in Neural Information Processing Systems_, 22, 2009.
* [11] Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data. _arXiv preprint arXiv:1712.04577_, 2017.
* [12] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? _Advances in Neural Information Processing Systems_, 32, 2019.
* [13] Shahana Ibrahim, Tri Nguyen, and Xiao Fu. Deep learning from crowdsourced labels: Coupled cross-entropy minimization, identifiability, and regularization. In _The International Conference on Learning Representations_, 2023.
* [14] Andrew B Rosenkrantz, Ruth P Lim, Mershad Haghighi, Molly B Somberg, James S Babb, and Samir S Taneja. Comparison of interreader reproducibility of the prostate imaging reporting and data system and likert scales for evaluation of multiparametric prostate mri. _American Journal of Roentgenology_, 201(4):W612-W618, 2013.
* [15] Davood Karimi, Haoran Dou, Simon K Warfield, and Ali Gholipour. Deep learning with noisy labels: Exploring techniques and remedies in medical image analysis. _Medical Image Analysis_, 65:101759, 2020.
* [16] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. _Advances in Neural Information Processing Systems_, 33:7597-7610, 2020.
* [17] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating instance-dependent bayes-label transition matrix using a deep neural network. In _International Conference on Machine Learning_, 2022.
* [18] Le Zhang, Ryutaro Tanno, Mou-Cheng Xu, Chen Jin, Joseph Jacob, Olga Cicarrelli, Frederik Barkhof, and Daniel Alexander. Disentangling human error from ground truth in segmentation of medical images. _Advances in Neural Information Processing Systems_, 33:15750-15762, 2020.
* [19] Peter Welinder, Steve Branson, Pietro Perona, and Serge Belongie. The multidimensional wisdom of crowds. _Advances in Neural Information Processing Systems_, 23, 2010.
* [20] Paul Ruvolo, Jacob Whitehill, and Javier R Movellan. Exploiting structure in crowdsourcing tasks via latent factor models. _Citeseer_, 2010.

* [21] Yan Yan, Romer Rosales, Glenn Fung, Mark Schmidt, Gerardo Hermosillo, Luca Bogoni, Linda Moy, and Jennifer Dy. Modeling annotator expertise: Learning when everybody knows a bit of something. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, 2010.
* [22] Wei Bi, Liwei Wang, James T Kwok, and Zhuowen Tu. Learning to predict from crowdsourced data. In _The Conference on Uncertainty in Artificial Intelligence_, 2014.
* [23] Joseph Jacob, Olga Ciccarelli, Frederik Barkhof, and Daniel C Alexander. Disentangling human error from the ground truth in segmentation of medical images. In _Advances in Neural Information Processing Systems_, 2021.
* [24] Zhengqi Gao, Fan-Keng Sun, Mingran Yang, Sucheng Ren, Zikai Xiong, Marc Engeler, Antonio Burazer, Linda Wildling, Luca Daniel, and Duane S Boning. Learning from multiple annotator noisy labels via sample-wise label fusion. In _European Conference on Computer Vision_, 2022.
* [25] Jingzheng Li, Hailong Sun, and Jiyi Li. Beyond confusion matrix: learning from multiple annotators with awareness of instance features. _Machine Learning_, 112(3):1053-1075, 2023.
* [26] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 38(3):447-461, 2015.
* [27] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2017.
* [28] Hwanjun Song, M Mineok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [29] Neal S Grantham, Yawen Guan, Brian J Reich, Elizabeth T Borer, and Kevin Gross. Mimix: A bayesian mixed-effects model for microbiome data from designed experiments. _Journal of the American Statistical Association_, 115(530):599-609, 2020.
* [30] Wei Liu, Huazhen Lin, Shurong Zheng, and Jin Liu. Generalized factor model for ultra-high dimensional correlated variables with mixed types. _Journal of the American Statistical Association_, 118:1-17, 2021.
* [31] Yunyang Xiong, Hyunwoo J Kim, and Vikas Singh. Mixed effects neural networks (menets) with applications to gaze estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019.
* [32] Giora Simchoni and Saharon Rosset. Using random effects to account for high-cardinality categorical features and repeated measures in deep neural networks. _Advances in Neural Information Processing Systems_, 34:25111-25122, 2021.
* [33] Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded instance and label-dependent label noise. In _International Conference on Machine Learning_, 2020.
* [34] Hemant Ishwaran and J Sunil Rao. Spike and slab variable selection: frequentist and bayesian strategies. _The Annals of Statistics_, 33(2):730-773, 2005.
* [35] Yan Sun, Qifan Song, and Faming Liang. Consistent sparse deep learning: Theory and computation. _Journal of the American Statistical Association_, 117(540):1-15, 2021.
* [36] Yuexi Wang and Veronika Rockova. Uncertainty quantification for sparse deep learning. In _International Conference on Artificial Intelligence and Statistics_, 2020.
* [37] Yan Sun, Wenjun Xiong, and Faming Liang. Sparse deep learning: A new framework immune to local traps and miscalibration. _Advances in Neural Information Processing Systems_, 34:22301-22312, 2021.
* [38] Shantanu Jain, Martha White, and Predrag Radivojac. Estimating the class prior and posterior from noisy positives and unlabeled data. _Advances in neural information processing systems_, 29, 2016.
* [39] Jerzy Neyman and Egon Sharpe Pearson. On the problem of the most efficient tests of statistical hypotheses. _Philosophical Transactions of the Royal Society of London. Series A_, 231(694-706):289-337, 1933.
* [40] Xi Chen, Adityanand Guntuboyina, and Yuchen Zhang. On bayes risk lower bounds. _The Journal of Machine Learning Research_, 17(1):7687-7744, 2016.
* [41] Imre Csiszar. A class of measures of informativity of observation channels. _Periodica Mathematica Hungarica_, 2(1-4):191-213, 1972.

* [42] Anuran Makur and Lizhong Zheng. Comparison of contraction coefficients for f-divergences. _Problems of Information Transmission_, 56:103-156, 2020.
* [43] Hideaki Imamura, Issei Sato, and Masashi Sugiyama. Analysis of minimax error rate for crowdsourcing and its application to worker clustering model. In _International Conference on Machine Learning_, 2018.
* [44] Bin Yu. _Festschrift for Lucien Le Cam_, chapter 29. Springer, 1997.
* [45] MTCAJ Thomas and A Thomas Joy. _Elements of information theory_. Wiley-Interscience, 2006.
* [46] Chao Gao, Yu Lu, and Dengyong Zhou. Exact exponent in optimal rates for crowdsourcing. In _International Conference on Machine Learning_, pages 603-611. PMLR, 2016.
* [47] Yann LeCun. The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* [48] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
* [49] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. In _International Conference on Learning Representations_, 2022.
* [50] Filipe Rodrigues, Mariana Lourenco, Bernardete Ribeiro, and Francisco C Pereira. Learning supervised topic models for classification and regression from crowds. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 39(12):2409-2422, 2017.
* [51] Antonio Torralba, Bryan C Russell, and Jenny Yuen. Labelme: Online image annotation and applications. _Proceedings of the IEEE_, 98(8):1467-1484, 2010.
* [52] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [53] Filipe Rodrigues and Francisco Pereira. Deep learning from crowds. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.
* [54] Melody Guan, Varun Gulshan, Andrew Dai, and Geoffrey Hinton. Who said what: Modeling individual labelers improves classification. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.
* [55] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. _Advances in Neural Information Processing Systems_, 31, 2018.
* [56] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In _Advances in Neural Information Processing Systems_, 2018.
* [57] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In _International Conference on Machine Learning_, pages 7164-7173. PMLR, 2019.
* [58] Zhendong Chu, Jing Ma, and Hongning Wang. Learning from crowds by modeling common confusions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2021.
* [59] Kolyan Ray, Botond Szabo, and Gabriel Clara. Spike and slab variational bayes for high dimensional logistic regression. _Advances in Neural Information Processing Systems_, 33:14423-14434, 2020.
* [60] Wenxin Jiang. Bayesian variable selection for high dimensional generalized linear models: convergence rates of the fitted densities. _The Annals of Statistics_, 35(4):1487-1511, 2007.
* [61] Nicholas G Polson and Veronika Rockova. Posterior concentration for sparse deep learning. _Advances in Neural Information Processing Systems_, 31, 2018.
* [62] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. _Advances in Neural Information Processing Systems_, 28, 2015.
* [63] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. Model compression and acceleration for deep neural networks: The principles, progress, and challenges. _IEEE Signal Processing Magazine_, 35(1):126-136, 2018.
* [64] Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In _International Conference on Machine Learning_, 2019.

* [65] Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. _Neural Networks_, 108:296-330, 2018.
* [66] Helmut Bolcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approximation with sparsely connected deep neural networks. _SIAM Journal on Mathematics of Data Science_, 1(1):8-45, 2019.
* [67] Minerva Mukhopadhyay and David B Dunson. Targeted random projection for prediction from high-dimensional features. _Journal of the American Statistical Association_, 115(532):1998-2010, 2020.
* [68] Michael R Sampford. Some inequalities on mill's ratio and related functions. _The Annals of Mathematical Statistics_, 24(1):130-132, 1953.
* [69] Rajarshi Guhaniyogi and David B Dunson. Bayesian compressed regression. _Journal of the American Statistical Association_, 110(512):1500-1514, 2015.
* [70] Abdulhamid A Alzaid and Maha A Omair. On the poisson difference distribution inference and applications. _Bulletin of the Malaysian Mathematical Sciences Society. Second Series_, 33(1):17-45, 2010.
* [71] C. M. Joshi and S. K. Bissu. Some inequalities of bessel and modified bessel functions. _Journal of the Australian Mathematical Society_, 50(2):333-342, 1991.
* [72] Martin J Wainwright. _High-Dimensional Statistics: A Non-Asymptotic Viewpoint_. Cambridge University Press, 2019.
* [73] Subhashis Ghosal and Aad Van Der Vaart. Convergence rates of posterior distributions for nonid observations. _The Annals of Statistics_, 1:192-223, 2007.
* [74] Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. _Journal of the Royal Statistical Society: Series B (Methodological)_, 28(1):131-142, 1966.
* [75] John Duchi. Information theory and statistics. _Lecture Notes for Statistics_, 311, 2019.
* [76] Michael Short. Improved inequalities for the poisson and binomial distribution and upper tail quantile functions. _International Scholarly Research Notices_, 2013.
* [77] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.

**Supplemental Materials**

In the supplementary materials, we first summarize the regularity conditions on the underlying true model and prior distributions of the network parameters in Section A. The proofs of Theorem 1, Theorem 2, and Corollary 3 are provided in Sections B.2-B.4 with all the needed preliminaries presented in Section B.1. Implementation details of the proposed method and additional experiment results are exhibited in Section C, including the accuracy and number of selected labels using the proposed label correction algorithm (10), the test accuracy on MNIST, the hyperparameter analysis on CIFAR100, the classification accuracy on CIFAR100 with varying number of annotators, and the average estimation error on CIFAR10 and CIFAR100 with varying number of annotators.

## Appendix A Regularity Conditions

### Network structure

To incorporate the sparse high dimensional setting [59], we utilize sparse Bayesian DNNs to reconstruct \(\psi_{1}(\mathbf{x})\) and \(\psi_{2}(\mathbf{x})\) in (3) [60, 37]. Specifically, to approximate \(\psi_{j}(\mathbf{x})\) with \(j=1,2\), we consider a network with \(H_{nj}-1\) hidden layers and a width vector \(\bm{L}^{(j)}=(L_{0}^{(j)},L_{1}^{(j)},...,L_{H_{nj}}^{(j)})^{\top}\), where the width of the \(h\)th layer is denoted \(L_{h}^{(j)}\) for \(h=0,...,H_{nj}\) with \(L_{0}^{(j)}=p_{n}\) for the input layer and \(L_{H_{nj}}^{(j)}\triangleq M^{(j)}\) for the output layer. Then the DNN with network architecture \(\{H_{nj},\bm{L}^{(j)}\}\) is the nonlinear function of the form:

\[\psi_{j}(\mathbf{x};\bm{\theta}^{(j)})=\mathbf{W}^{(j,H_{nj})}\sigma\left(... \sigma\left[\mathbf{W}^{(j,h)}\sigma\left\{...\sigma(\mathbf{W}^{(j,1)} \mathbf{x}+\mathbf{b}^{(j,1)})...\right\}+\mathbf{b}^{(j,h)}\right]...\right)+ \mathbf{b}^{(j,H_{nj})},\]

where for \(h=1,...,H_{nj}\), \(\mathbf{W}^{(j,h)}\) is a \(L_{h}^{(j)}\times L_{h-1}^{(j)}\) weight matrix, \(\mathbf{b}^{(j,h)}\in\mathbb{R}^{L_{h}^{(j)}}\) is the bias of layer \(h\), \(\sigma(\cdot)\) is a nonlinear activation function, and \(\bm{\theta}^{(j)}\) represents the \(J_{nj}\times 1\) vector formed from stacking \(\{\mathbf{W}^{(j,h)},\mathbf{b}^{(j,h)}\}_{h=1}^{H_{nj}}\) from bottom to the top, with \(J_{nj}\triangleq\sum_{h=1}^{H_{nj}}(L_{h-1}^{(j)}\times L_{h}^{(j)}+L_{h}^{(j)})\). We treat weights and biases equally without distinguishing them in \(\bm{\theta}^{(j)}\), and write \(\bm{\theta}^{(j)}\) as \(\bm{\theta}^{(j)}=(\theta_{1}^{(j)},...,\theta_{J_{nj}}^{(j)})^{\top}\). Let \(\bm{\gamma}^{(j)}=(\gamma_{1}^{(j)},...,\gamma_{J_{nj}}^{(j)})^{\top}\) denote the indicator vector, with \(\gamma_{k}^{(j)}=\bm{1}(\theta_{k}^{(j)}\neq 0)\) for \(k=1,...,J_{nj}\). For ease of presentation, we use \(\psi_{j}(\mathbf{x};\bm{\theta}^{(j)})\) and \(\psi_{j}(\mathbf{x};\bm{\theta}^{(j)},\bm{\gamma}^{(j)})\) exchangeably to represent the model for \(\psi_{j}(\mathbf{x})\), and let \(\mathcal{F}_{n}=\mathcal{F}(H_{n1},H_{n2},\bm{L}^{(1)},\bm{L}^{(2)},C_{1},C_ {2},\epsilon_{1})\) denote the space of all sparse networks that satisfy Condition A.3 in Appendix A.2 and are constrained by positive constants \(C_{1}\), \(C_{2}\) and \(\epsilon_{1}\).

To determine the parameters of the sparse DNNs \(\psi_{j}(\mathbf{x};\bm{\theta}^{(j)},\bm{\gamma}^{(j)})\) that best approximate \(\psi_{j}(\mathbf{x})\) for \(j=1,2\), we define

\[(\bm{\theta}^{(1)*},\bm{\gamma}^{(1)*},\bm{\theta}^{(2)*},\bm{ \gamma}^{(2)*})=\underset{\begin{subarray}{c}(\bm{\theta}^{(1)},\bm{\gamma}^{( 1)},\bm{\theta}^{(2)},\bm{\gamma}^{(2)})\in\mathcal{F}_{n}\\ \|\psi_{1}(\mathbf{x};\bm{\theta}^{(1)},\bm{\gamma}^{(1)})-\psi_{1}(\mathbf{x} )\|_{L^{2}(1)}\leq\varpi_{n1}\\ \|\psi_{2}(\mathbf{x};\bm{\theta}^{(2)},\bm{\gamma}^{(2)})-\psi_{2}(\mathbf{x} )\|_{L^{2}(1)}\leq\varpi_{n2}\end{subarray}}\quad\left\{|\bm{\gamma}^{(1)}|+| \bm{\gamma}^{(2)}|\right\},\] (14)

where for \(j=1,2\), \(\varpi_{nj}\) is an \(n\)-dependent positive constant satisfying \(\varpi_{nj}\to 0\) as \(n\to\infty\)3 We call \(\bm{\theta}^{(1)*}\), \(\bm{\gamma}^{(1)*}\), \(\bm{\theta}^{(2)*}\), and \(\bm{\gamma}^{(2)*}\) the _true parameters_ of \(\bm{\theta}^{(1)}\), \(\bm{\gamma}^{(1)}\), \(\bm{\theta}^{(2)}\), and \(\bm{\gamma}^{(2)}\), respectively.

Footnote 3: In this definition, the \(L_{2}\) norm of the network approximation error is bounded. Consider a measure space \((\Omega,\mathcal{G},\mu)\) and \(0<p<q\leq\infty\), we have that \(\|f\|_{p}\leq\mu(\Omega)^{1/p-1/q}\|f\|_{q}\) using Hlders inequality and therefore, \(\|f\|_{1}\leq\|f\|_{2}\) and \(L^{1}(\Omega,\mu)\subset L^{2}(\Omega,\mu)\) for probability measure \(\mu\). Thus, the \(L_{1}\) norm of the network approximation error is also bounded, which will be used in the following proofs.

### Conditions for the sparse GLM

1. The input vector is standardized so that \(\mathbf{x}\in\Omega\subset[-1,1]^{p_{n}}\), where \(\Omega\) is the support of \(\mathbf{x}\), and the probability density \(f(\cdot)\) of \(\mathbf{x}\) satisfies that \(\sup_{\mathbf{x}\in\Omega}|f(\mathbf{x})|\leq C_{0}\) for some positive constant \(C_{0}\).
2. The activation function \(\sigma(\cdot)\) is 1-Lipschitz.

* The true sparse DNN model satisfies the following conditions.
* For \(j=1,2\), let \(r_{nj}=\|\bm{\gamma}^{(j)}\|_{1}\) denote the connectivity of \(\bm{\gamma}^{(j)}\), let \(\overline{L}_{nj}=\max_{1\leq h\leq H_{nj}}L_{h}^{(j)}\) denote the maximum layer width, and let \(s_{nj}\) represent the input dimension of \(\bm{\gamma}^{(j)}\). Let \(\zeta_{n}=\big{\{}(r_{n1}+r_{n2})(H_{n1}+H_{n2})\log n+(r_{n1}+r_{n2})(\log \overline{L}_{n1}+\log\overline{L}_{n2})+(s_{n1}+s_{n2})\log p_{n}\big{\}}/n\). The true sparse DNN model satisfies that \(\zeta_{n}\leq C_{1}n^{-\epsilon_{1}}\) for some constants \(C_{1}>0\) and \(0<\epsilon_{1}<1\).
* For \(j=1,2\), \(\|\bm{\theta}^{(j)}\|_{\infty}\leq E_{nj}\), where positive constant \(E_{nj}\leq n^{C_{2}}\) for some constant \(C_{2}>0\).
* Write the function \(G(\cdot)\) in (3) as \((G_{1}(\cdot),...,G_{K}(\cdot))^{\top}\). For \(\mathbf{x}\in\Omega\), \(r\in[R]\) and \(k\in[K]\), write \(\bm{\omega}^{(k,r)}=\mathbf{A}^{(r)T}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)})+ \mathbf{B}^{(k)T}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)})\).
* For \(k\in[K]\) and \(r\in[R]\), in the neighbourhood of \(\bm{\theta}_{0}\), \[\sup_{j,l\in[K]}\Big{|}\frac{\partial G_{j}(\bm{\omega})}{\partial\omega_{l}} \big{|}_{\bm{\omega}=\bm{\omega}^{(k,r)}}\Big{|}=C_{3}\text{ and }\sup_{j\in[K]}\Big{|}\frac{G_{j}(\bm{\omega}^{(k,r)})}{G_{j}(\bm{\omega}_{0} ^{(k,r)})}-1\Big{|}=\epsilon_{2}\] for some constants \(C_{3}>0\) and \(\epsilon_{2}\in(0,1)\), where the latter requirement can be achieved if \(G_{j}(\bm{\omega}_{0}^{k,r})>\varsigma\) for some positive constant \(\varsigma>0\).
* For \(r\in[R]\) and \(j,k\in[K]\), \(\|\bm{\alpha}_{j0}^{(r)}\|_{\infty}\leq F_{1}\) and \(\|\bm{\beta}_{j0}^{(k)}\|_{\infty}\leq F_{2}\), where \(F_{1}\) and \(F_{2}\) are positive constants.

### Conditions for the prior

* For \(j=1,2\), assume each element of \(\bm{\theta}^{(j)}\) has independent continuous prior distribution, denoted \(\pi_{\theta}^{(j)}(\cdot)\). Thus, its minimum value on the interval \([-E_{nj}-1,E_{nj}+1]\) exits, and let \(\underline{\pi}_{\theta}^{(j)}\) denote it. For a sequence of positive constants \(I_{n}^{(j)}\) with \(\log I_{n}^{(j)}=O(\log n)\), let \(\delta_{nj}\) and \(\delta_{nj}^{\prime}\) be two sequences of constants satisfying that \(\delta_{nj}<1/nJ_{nj}(c_{0}I_{n}^{(j)})^{H_{nj}}(n/H_{nj})^{H_{nj}}\) and \(\delta_{nj}^{\prime}<1/nJ_{nj}(c_{0}E_{nj})^{H_{nj}}(r_{nj}/H_{nj})^{H_{nj}}\) for some constant \(c_{0}>1\), respectively. Assume that:
* \(\log(1/\underline{\pi}_{\theta}^{(j)})=O(H_{nj}\log n+\log\overline{L}_{nj})\);
* \(\pi_{\theta}^{(j)}([-\delta_{nj},\delta_{nj}])\)\(\geq 1\ -\ \frac{1}{J_{nj}}\exp\Big{[}\ -\ S_{0}^{(j)}\{(H_{n1}\ +\ H_{n2})\log n\) \[+\log\overline{L}_{n1}+\log\overline{L}_{n2}+\log p_{n}\}\Big{]}\) for some constant \(S_{0}^{(j)}>2\);
* \(-\log\Big{\{}J_{nj}\pi_{\theta}^{(j)}(|\theta_{1}^{(j)}|>I_{n}^{(j)})\Big{\}} \succ(2+\epsilon_{3}^{(j)})n\epsilon_{n}^{2}\) for some positive constant \(\epsilon_{3}^{(j)}\).
* For \(k\in[K]\), let \(\overline{B}_{n}^{(k)}\) and \(\underline{B}_{n}^{(k)}\) denote the largest and the smallest eigenvalues of \(\bm{\Sigma}_{\beta}^{(k)}\), respectively, and for \(r\in[R]\), let \(\overline{A}_{n}^{(r)}\) and \(\underline{A}_{n}^{(r)}\) denote the largest and the smallest eigenvalues of \(\bm{\Sigma}_{\alpha}^{(r)}\), respectively. Assume that for large enough \(n\),
* \(\overline{A}_{n}^{(r)}\leq S_{1}^{(1)}M^{(1)q_{1}}\) and \(\underline{A}_{n}^{(r)}\geq S_{1}^{(1)}\{\log M^{(1)}\}^{-1}\) for some positive constants \(S_{1}^{(1)}\),
* \(S_{2}^{(1)}\) and \(q_{1}\).
* For \(k\in[K]\), let \(\overline{B}_{n}^{(k)}\leq S_{1}^{(2)}M^{(2)q_{1}}\) and \(\underline{B}_{n}^{(k)}\geq S_{2}^{(2)}\{\log M^{(2)}\}^{-1}\) for some positive constants \(S_{1}^{(2)}\), \(S_{2}^{(2)}\) and \(q_{2}\);

### Remark

* Assumption A.1 specifies that the hypothesis set we consider is a class of DNNs, which is a common setting in the literature [61, 35].

2. Assumption A.2 ensures that the underlying noise transition probability can be approximated by a sparse model. Existing works [62, 63, 64] empirically show that large DNNs often contain a large number of redundant parameters and propose methods for compressing neural networks without affecting performance. Moreover, theoretical works [65, 66] in approximation theory provide theories that guarantee uniform approximation rates for a broad family of function classes. Similar assumptions can be found in [35, 67].
3. Assumption A.3 specifies the constraints on the prior distribution we use. As in our experiments in Section 5, we may employ the spike-and-slab prior \(\lambda_{n}\mathcal{N}(0,\sigma_{1n}^{2})+(1-\lambda_{n})\mathcal{N}(0,\sigma_ {0n}^{2})\) for each element of the parameter vector \(\boldsymbol{\theta}^{(j)}\) of the sparse Bayesian DNN \(\psi_{j}(\mathbf{x};\boldsymbol{\theta}^{(j)})\), with \(j=1,2\); we take the normal prior \(\mathcal{N}(0,\sigma_{n}^{2})\) for each element of the regression weights. It can be verified that Condition B.1 in A.3 is satisfied if the values of \(\lambda_{n}\), \(\sigma_{1n}\), and \(\sigma_{0n}\) are properly chosen [35]. In particular, the value of \(\lambda_{n}\) is related to the sparsity of the model and we require it to satisfy that \(\lambda_{n}=O(1/J_{nj}[n^{H_{n1}+H_{n2}}(\overline{L}_{n1}+\overline{L}_{n2}) p_{n})]^{c})\) for some positive constant \(c\) and \(j=1,2\), which should be chosen by considering the network structure and the number of data points, \(n\). Moreover, by using techniques such as Mill's ratio [68], Condition B.2 in A.3 is satisfied if \(c_{1}<\sigma_{n}<c_{2}\) for some positive constants \(c_{1}\) and \(c_{2}\), which are related to \(S_{1}^{(j)}\), \(S_{2}^{(j)}\), \(q_{j}\) and \(M^{(j)}\) for \(j=1,2\) in Condition B.2 [60, 67]. Similar assumptions can be found in [60, 35, 67].

## Appendix B Proofs

In this section, we present the proofs of Theorem 1, Theorem 2, and Corollary 3. Specifically, we provide all the need preliminaries in Section B.1, where the definitions and results in Sections B.1.1-B.1.4 will be used in the proof of Theorem 1, and the information-theoretical definitions and lemmas provided in Section B.1.5 will be utilized in the proof of Theorem 2 and Corollary 3.

### Preliminaries

#### b.1.1 Definitions of some discrepancy measures

**Definition 1** ([69]).: Let \(f\) and \(f_{0}\) denote two conditional probability density/mass functions of \(\tilde{\mathbf{y}}\), given \(\mathbf{x}\). Let \(\nu_{1}(d\mathbf{x})\) denote the probability measure for \(\mathbf{x}\) associated with the density \(f(\mathbf{x})\) and let \(\nu_{2}(d\tilde{\mathbf{y}})=\bigotimes_{r=1}^{R}\nu_{2,r}(d\tilde{\mathbf{y} }^{(r)})\) be a dominating measure for \(f\) and \(f_{0}\), and hence, a dominating measure of \((\mathbf{x},\tilde{\mathbf{y}})\) is taken as the product \(\nu_{1}(d\mathbf{x})\nu_{2}(d\tilde{\mathbf{y}})\).

1. The Hellinger distance between \(f\) and \(f_{0}\) is defined as \[d(f,f_{0})=\sqrt{\int\int(\sqrt{f}-\sqrt{f_{0}})^{2}\nu_{2}(d\tilde{\mathbf{y }})\nu_{1}(d\mathbf{x})}.\]
2. For any \(t>0\), define \[d_{t}(f,f_{0})=\frac{1}{t}\left\{\int\int f_{0}\bigg{(}\frac{f_{0}}{f}\bigg{)} ^{t}\nu_{2}(d\tilde{\mathbf{y}})\nu_{1}(d\mathbf{x})-1\right\}.\]
3. The Kullback-Leibler divergence between \(f\) and \(f_{0}\) is defined as \[d_{0}(f,f_{0})\triangleq K(f,f_{0})=\int\int f_{0}\log\bigg{(}\frac{f_{0}}{f} \bigg{)}\nu_{2}(d\tilde{\mathbf{y}})\nu_{1}(d\mathbf{x}).\]
4. For \(q>1\), define \[V_{q}(f,f_{0})=\int\int f_{0}\bigg{|}\log\bigg{(}\frac{f_{0}}{f}\bigg{)} \bigg{|}^{q}\nu_{2}(d\tilde{\mathbf{y}})\nu_{1}(d\mathbf{x}).\] For \(q=2\), the index of \(V_{2}(\cdot,\cdot)\) is omitted and the discrepancy measure is denoted \(V(\cdot,\cdot)\). [60] shows that (1) \(d(f,f_{0})\leq\sqrt{d_{0}(f,f_{0})}\); (2) \(d_{t}(f,f_{0})\) decreases to \(d_{0}(f,f_{0})\) as \(t\) decreases to 0.

#### b.1.2 Mathematical details about the regression weights

Adapting the proof in [69], we prove the following proposition.

**Proposition 1**.: _Assume that \(\bm{\beta}\sim\mathcal{N}(\bm{0},\bm{D}_{\bm{\beta}})\), where \(\bm{D}_{\beta}\) is a positive definite matrix. Then, for any given \(\text{dim}(\bm{\beta})\times 1\) vector of functions \(\phi(\bm{\mathrm{x}};\bm{\theta})\) of \(\bm{\mathrm{x}}\) and \(\bm{\theta}\), and for any constant \(\Delta>0\),_

\[P\left\{\left|(\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}-\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}_{0}|<\Delta\right\}\right.\] \[\qquad\left.>8\exp\left\{-\frac{\left\{\phi(\bm{\mathrm{x}};\bm{ \theta})^{\mathrm{ T}}\bm{\beta}_{0}\right\}^{2}+\Delta^{2}}{2B\|\phi(\bm{\mathrm{x}};\bm{ \theta})\|^{2}}\right\}\frac{\Delta^{4}}{\overline{B}^{2}\|\phi(\bm{\mathrm{x }};\bm{\theta})\|^{4}},\]

_where \(\overline{B}\) and \(\underline{B}\) are the largest and the smallest eigenvalues of \(\bm{D}_{\bm{\beta}}\), respectively._

Proof.: The proof is established in two steps.

**Step 1.** We first prove that

\[P\left\{\left|(\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}-\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}_{0}|<\Delta\right\}>P(X-Y\geq 2),\right.\] (15)

where \(X\sim Pois(\frac{\Delta_{1}}{2})\) and \(Y\sim Pois(\frac{\lambda}{2})\), with \(\Delta_{1}=\frac{\Delta^{2}}{\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{D}_{\beta}\phi(\bm{\mathrm{x}};\bm{\theta})}\) and \(\lambda=\frac{\left\{\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}_{0}\right\}^{2}}{\phi(\bm{\mathrm{x}};\bm{\theta})^{ \mathrm{ T}}\bm{D}_{\beta}\phi(\bm{\mathrm{x}};\bm{\theta})}\), and \(X\) and \(Y\) are independent.

By the definition of the noncentral chi-squared distribution, it can be easily seen that \(T\triangleq\frac{\left\{\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}-\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}_{0}\right\}^{2}}{\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{D}_{\beta}\phi(\bm{\mathrm{x}};\bm{\theta})}\) is distributed according to the noncentral chi-squared distribution \(\chi_{1}^{2}(\lambda)\). Thus, by utilizing the cumulative distribution function (CDF) of \(\chi_{1}^{2}(\lambda)\), we obtain that

\[P\left\{\left|(\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}-\phi(\bm{\mathrm{x}};\bm{\theta})^{\mathrm{ T}}\bm{\beta}_{0}|<\Delta\right\}\right.\] \[= P\left\{T<\Delta_{1}\right\}\] \[= \sum_{j=0}^{\infty}\frac{\exp\left(-\frac{\lambda}{2}\right)( \frac{\lambda}{2})^{j}}{j!}Q(\Delta_{1};1+2j),\]

where \(Q(\cdot;1+2j)\) is the CDF of \(\chi_{1+2j}^{2}\), the central chi-squared distribution with \(1+2j\) degrees of freedom.

Noting that \(Q(\Delta_{1};1+2j)>Q(\Delta_{1};2+2j)\), by the result that \(Z_{j1}+...+Z_{j,j+1}\sim\chi_{2+2j}^{2}\) if \(Z_{j1},...,Z_{j,j+1}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\chi_{2}^ {2}\stackrel{{\mathrm{d}}}{{=}}\exp(\frac{1}{2})\), we obtain that \(Q(\Delta_{1};1+2j)>P\{Z_{j1}+...+Z_{j,j+1}<\Delta_{1}\}\). According to the relationship between the Poisson counting process and exponential variables, we have that the counting process with \(Z_{j1},...,Z_{j,j+1}\) as inter-arrival times is the Poisson process with rate \(\frac{1}{2}\). Let \(N(\Delta_{1})\) denote the total number of occurrences or events that have happened up to time \(\Delta_{1}\). Then, \(N(\Delta_{1})\) follows the Poisson distribution with parameter \(\frac{\Delta_{1}}{2}\), i.e., \(N(\Delta_{1})\stackrel{{\mathrm{d}}}{{=}}X\), and \(\{Z_{j1}+...+Z_{j,j+1}<\Delta_{1}\}=\{N(\Delta_{1})>j+1\}\). Consequently, we have that

\[\sum_{j=0}^{\infty}\frac{\exp\left(-\frac{\lambda}{2}\right)( \frac{\lambda}{2})^{j}}{j!}Q(\Delta_{1};1+2j) >\sum_{j=0}^{\infty}\frac{\exp\left(-\frac{\lambda}{2}\right)( \frac{\lambda}{2})^{j}}{j!}P\{Z_{j1}+...+Z_{j,j+1}<\Delta_{1}\}\] \[=\sum_{j=0}^{\infty}\frac{\exp\left(-\frac{\lambda}{2}\right)( \frac{\lambda}{2})^{j}}{j!}P\{N(\Delta_{1})>j+1\}\] \[=\sum_{j=0}^{\infty}\frac{\exp\left(-\frac{\lambda}{2}\right)( \frac{\lambda}{2})^{j}}{j!}P\{X\geq j+2\}\] \[=\sum_{j=0}^{\infty}\mathbb{P}(Y=j)\mathbb{P}(X\geq j+2)\] \[=\sum_{j=0}^{\infty}\mathbb{P}(X\geq j+2,Y=j)\] \[=P\{X-Y\geq 2\},\]

[MISSING_PAGE_FAIL:19]

distance \(d\) is a set \(\{f_{1},...,f_{k}\}\subset\mathcal{P}_{n}\) such that for each \(f\in\mathcal{P}_{n}\), there exists \(j\in\{1,...,k\}\) such that \(d(f,f_{j})\leq\epsilon_{n}\). The \(\epsilon_{n}\)_-covering number_ is the cardinality of the smallest \(\epsilon_{n}\)-cover [72]. Let \(N(\epsilon_{n},\mathcal{P}_{n},d)\) denote the \(\epsilon_{n}\)-covering number of \(\mathcal{P}_{n}\) w.r.t. the distance \(d\).

Consider a vector of independently (not necessarily identically) distributed observations \(\mathcal{D}^{n}\), where the \(i\)th component is generated from distribution \(\mathbb{P}_{\bm{\theta},i}\) with the density \(p_{\bm{\theta},i}\) relative to a \(\sigma\)-finite measure \(\nu_{i}\) on \((\mathscr{X}_{i},\mathscr{A}_{i})\) for \(i\in[n]\), and \(\bm{\theta}\) is the vector of parameters in the parameter space \(\Theta\). We define \(\mathbb{P}_{\bm{\theta}}^{n}\) to be the product measure \(\bigotimes_{i=1}^{n}\mathbb{P}_{\bm{\theta},i}\) on the corresponding product measurable space \(\bigotimes_{i=1}^{n}(\mathscr{X}_{i},\mathscr{A}_{i})\). Assume that \(\mathcal{D}^{n}\) is generated from the true distribution \(\mathbb{P}_{\bm{\theta}_{0}}^{n}\). We define the square of the _semimetric_\(d_{n}\) as in [73]:

\[d_{n}^{2}(\bm{\theta},\bm{\theta}_{0})=\frac{1}{n}\sum_{i=1}^{n}\int(\sqrt{p_ {\bm{\theta},i}}-\sqrt{p_{\bm{\theta}_{0},i}})^{2}d\nu_{i},\] (16)

which can be seen as the average of the squares of the Hellinger distances. For \(\epsilon>0\), we define the \(\epsilon\)-neighborhood around \(\bm{\theta}_{0}\):

\[G_{n}^{*}(\bm{\theta}_{0},\epsilon)=\left\{\bm{\theta}:\frac{1}{n}\sum_{i=1}^ {n}K_{i}(\bm{\theta},\bm{\theta}_{0})\leq\epsilon^{2};\frac{1}{n}\sum_{i=1}^{ n}V_{i}(\bm{\theta},\bm{\theta}_{0})\leq\epsilon^{2}\right\},\] (17)

where \(K_{i}(\bm{\theta},\bm{\theta}_{0})=K(\mathbb{P}_{\bm{\theta},i},\mathbb{P}_{ \bm{\theta}_{0},i})\) and \(V_{i}(\bm{\theta},\bm{\theta}_{0})=V(\mathbb{P}_{\bm{\theta},i},\mathbb{P}_{ \bm{\theta}_{0},i})\), defined in Definition 1.

Let \(\Pi(\cdot)\) denote the prior probability measure on \(\bm{\theta}\), and let \(\Pi(\cdot|\mathcal{D}^{n})\) represent the associated posterior measure given the data \(\mathcal{D}^{n}\). For ease of exposition, we put \(\Pi(\Theta^{*})\) for \(\Pi(\{\theta\in\Theta^{*}\})\) for any \(\Theta^{*}\subset\Theta\). The following lemma is modified from Theorem 4 in [73] and will be used in the proof of Theorem 1.

**Lemma 3**.: _Suppose that for a sequence of sets \(\Theta_{n}\subset\Theta\) and for a sequence of positive numbers \(\{\epsilon_{n}\}_{n=1}^{\infty}\) such that \(\epsilon_{n}\to 0\) as \(n\to\infty\) and \(n\epsilon_{n}^{2}\) is bounded away from zero, the following conditions hold for large enough \(n\):_

1. \(\sup_{\epsilon>\epsilon_{n}}\log N(\epsilon/36,\{\bm{\theta}\in\Theta_{n}:d_{ n}(\bm{\theta},\bm{\theta}_{0})<\epsilon\},d_{n})\leq n\epsilon_{n}^{2}\)_;_
2. \(\Pi(\Theta\backslash\Theta_{n})=o(\exp\{-(r+2)n\epsilon_{n}^{2}\})\)_;_
3. \(\Pi(G_{n}^{*}(\bm{\theta}_{0},\epsilon_{n}))\geq\exp(-rn\epsilon_{n}^{2})\)__

_for some constant \(r>0\). Then for any \(M_{n}>0\)\(\mathbb{P}_{\bm{\theta}_{0}}^{n}\Pi(\bm{\theta}:d_{n}(\bm{\theta},\bm{ \theta}_{0})\geq M_{n}\epsilon_{n}|\mathcal{D}^{n})\to 0\) as \(M_{n}\to\infty\)._

#### b.1.5 Useful information-theoretical definitions and lemmas

We give some useful information-theoretical definitions and lemmas in this subsection, which will be used in the proof of Theorem 2 and Corollary 3.

**Lemma 4** (Log sum inequality, [45]).: _For positive numbers, \(a_{1},...,a_{n}\) and \(b_{1},...,b_{n}\),_

\[\sum_{i=1}^{n}a_{i}\log\left(\frac{a_{i}}{b_{i}}\right)\geq\left(\sum_{i=1}^{n }a_{i}\right)\log\left(\frac{\sum_{i=1}^{n}a_{i}}{\sum_{i=1}^{n}b_{i}}\right),\]

_with equality if and only if \(\frac{a_{i}}{b_{i}}\) equals for \(i=1,...,n\)._

**Definition 2** ([74, 75]).: Let \(P\) and \(Q\) be probability distributions on the set \(\mathcal{X}\), and let \(f:\mathbb{R}_{+}\longrightarrow\mathbb{R}\) be a convex function satisfying \(f(1)=0\). Without loss of generality, assume that \(P\) and \(Q\) are absolutely continuous with respect to the base measure \(\mu\). The \(f\)-divergence between \(P\) and \(Q\) is then defined as

\[D_{f}(P\|Q):=\int_{\mathcal{X}}q(x)f\left(\frac{p(x)}{q(x)}\right)d\mu(x)+f^{ \prime}(\infty)P\{q=0\},\]

where \(p\) and \(q\) are the densities of \(P\) and \(Q\) with respect to the measure \(\mu\), respectively, and \(f^{\prime}(\infty)\) represents \(\lim_{x\to\infty}f(x)/x\).

**Example 1**.: By taking different \(f\) functions, we provide some popular examples of \(f\)-divergences.

* Kullback-Leibler (KL) divergence: taking \(f(t)=t\log t\) gives \(D_{f}(P\|Q)\triangleq D_{\text{KL}}(P\|Q)=\int p\log(p/q)d\mu\), which is also denoted \(d_{0}(p,q)\) in Definition 1.

* The total variation distance: taking \(f(t)=\frac{1}{2}|t-1|\) yields \(D_{f}(P\|Q)\triangleq\|P-Q\|_{\text{TV}}=\frac{1}{2}\int\big{|}\frac{p}{q}-1\big{|} qd\mu=\sup_{A\subset\mathcal{X}}|P(A)-Q(A)|\), which is also denoted \(d_{0}(p,q)\) in Definition 1.
* The Hellinger distance: taking \(f(t)=(\sqrt{t}-1)^{2}=t-2\sqrt{t}+1\) leads to the squared Hellinger distance \(D_{f}(P\|Q)\triangleq H^{2}(P\|Q)=\int(\sqrt{p}-\sqrt{q})^{2}d\mu\), which is also denoted \(d^{2}(p,q)\) in Definition 1.
* The \(\chi^{2}\)-divergence: taking \(f(t)=\frac{1}{2}(t-1)^{2}\) produces the \(\chi^{2}\)-divergence \(D_{f}(P\|Q)\triangleq\chi^{2}(P\|Q)=\frac{1}{2}\int(\frac{p}{q}-1)^{2}d\mu\).

**Lemma 5** ([75]).: _For the quantities defined in Example 1, the following relationships hold:_

* _For the Hellinger distance,_ \[\frac{1}{2}H^{2}(P,Q)\leq\|P-Q\|_{\text{TV}}\leq H(P,Q)\sqrt{1-H^{2}(P,Q)/4}.\]
* _Pinsker's inequality: for any distributions_ \(P\) _and_ \(Q\)_,_ \[\|P-Q\|_{\text{TV}}^{2}\leq\frac{1}{2}D_{\text{KL}}(P\|Q).\]

**Definition 3** ([41, 40]).: Let \(\mathcal{P}=\{P_{\theta}:\theta\in\Theta\}\) be a family of probability measures on a space \(\mathcal{X}\), indexed by \(\theta\in\Theta\), and let \(\omega\) be a probability measure on \(\Theta\). For each \(f\) satisfying the conditions in Definition 2, the \(f\)-informativity, \(I_{f}(\omega,\mathcal{P})\), is defined as

\[I_{f}(\omega,\mathcal{P}):=\inf_{Q}\int D_{f}(P_{\theta}\|Q)\omega(d\theta),\]

where the infimum is taken over all possible probability measures \(Q\) on \(\mathcal{X}\). In particular, when \(f(t)=t\log t\), the \(f\)-informativity is equal to the mutual information and is denoted by \(I(\omega,\mathcal{P})\).

For each \(f\) satisfying the conditions in Definition 2, let \(\phi_{f}:[0,1]^{2}\to\mathbb{R}\) be the function defined as follows: for \(a,b\in[0,1]^{2}\), \(\phi_{f}(a,b)\) is the \(f\)-divergence between the two probability measures \(P\) and \(Q\) on \(\{0,1\}\) given by \(P\{1\}=a\) and \(Q\{1\}=b\). Then, \(\phi_{f}(a,b)\) has the following expression:

\[\phi_{f}(a,b)=\begin{cases}bf\left(\frac{a}{b}\right)+(1-b)f\left(\frac{1-a}{ 1-b}\right)&\text{ for }0<b<1;\\ f(1-a)+af^{\prime}(\infty)&\text{ for }b=0;\\ f(a)+(1-a)f^{\prime}(\infty)&\text{ for }b=1.\end{cases}\] (18)

The following lemma implies monotonicity and convexity properties of \(\phi_{f}\) by taking it as a univariate function with one argument of \(\phi_{f}\) fixed at a given value.

**Lemma 6** ([40]).: _For each \(f\) satisfying the conditions in Definition 2, consider \(\phi_{f}\) defined in (18). Then_

* _for every fixed_ \(b>0\)_, the map_ \(g(a):a\mapsto\phi_{f}(a,b)\) _is non-increasing for_ \(a\in[0,b]\) _and_ \(g(a)\) _is convex and continuous in_ \(a\)_;_
* _for every fixed_ \(a<1\)_, the map_ \(h(b):b\mapsto\phi_{f}(a,b)\) _is non-decreasing for_ \(b\in[a,1]\)_._

**Lemma 7** ([40]).: _Let \(\mathcal{P}=\{P_{\theta}:\theta\in\Theta\}\) be a family of probability measures on a space \(\mathcal{X}\) and let \(\omega\) be a probability measure on \(\Theta\). Suppose that the loss function \(\mathcal{L}\) is zero-one valued. Define the Bayes error as \(\Re_{\text{Bayes}}(\omega)=\inf_{\theta}\int_{\Theta}\mathbb{E}\mathcal{L}( \theta,\mathfrak{Q}(X))\omega(d\theta)\) with \(\mathfrak{Q}:\mathcal{X}\to\Theta\) denoting a mapping from the sample space to the parameter space. For any \(f\) satisfying the conditions in Definition 2, we have that_

\[I_{f}(\omega,\mathcal{P})\geq\phi_{f}(\Re_{\text{Bayes}}(\omega),\Re_{0}),\]

_where \(\phi_{f}\) is given by (18), and \(\Re_{0}\) is defined as \(\Re_{0}\triangleq\inf_{a\in\Theta}\int_{\Theta}\mathcal{L}(\theta,a)\omega(d \theta)\)._

### Proof of Theorem 1

We now establish the proof of Theorem 1 by checking the three conditions in Lemma 3 established for the sparse Bayesian deep learning framework. We first provide a useful result on the discrepancy measures for generalized linear models in Section B.2.1, and then verify the three conditions in Sections B.2.2-B.2.4 following the proof techniques in [37, 67].

#### b.2.1 Discrepancy measures for generalized linear models

Paired variables in the set of anchor points \(\overline{\mathcal{D}}_{0}=\overline{\mathcal{D}}_{0,1}\cup\ldots\cup \overline{\mathcal{D}}_{0,K}\) in Section 3.1 can be seen as independently (not necessarily identically) distributed, and we let \(\mathcal{D}^{n}\) denote \(\overline{\mathcal{D}}_{0}\) in the following derivations to emphasize its dependence on the sample size \(n\). For \(\{\mathbf{x},\tilde{\mathbf{y}}\}\in\overline{\mathcal{D}}_{0,k}\) with \(k\in[K]\), we write the \(r\)th element of \(\tilde{\mathbf{y}}\), \(\tilde{y}^{(r)}\), in the 1-of-\(K\) fashion, i.e., only the \(j\)th element is equal to 1 while others are all 0 if \(\tilde{y}^{(r)}\) is the \(j\)th class, and then, the conditional probability density/mass function of \(\tilde{y}^{(r)}\) induced by \(\bm{\theta}\) is given by

\[f^{(k,r)}_{\bm{\theta}}(\tilde{y}^{(r)})=G^{\top}(\bm{\omega}^{(k,r)})\tilde {y}^{(r)}=\left(G_{1}(\bm{\omega}^{(k,r)}),...,(G_{K}(\bm{\omega}^{(k,r)}) \right)\tilde{y}^{(r)}\] (19)

for \(r\in[R]\). We denote the joint conditional probability density/mass function of \(\tilde{\mathbf{y}}\) as \(f^{(k)}_{\bm{\theta}}(\tilde{\mathbf{y}})\), given by

\[f^{(k)}_{\bm{\theta}}(\tilde{\mathbf{y}})=\prod_{r=1}^{R}f^{(k,r)}_{\bm{ \theta}}(\tilde{y}^{(r)}),\] (20)

and in contrast, we denote the underlying true conditional probability density/mass functions for \(\tilde{y}^{(r)}\) and \(\tilde{\mathbf{y}}\) as \(f^{(k,r)}_{0}\) and \(f^{(k)}_{0}\), respectively.

**Result 1**.: _If Conditions A.4 in Section A.2 are satisfied, then, for any \(k\in[K]\),_

\[K(f^{(k)}_{\bm{\theta}},f^{(k)}_{0}) \leq C_{K}\sum_{r=1}^{R}\mathbb{E}_{\mathbf{x}}\|\bm{\omega}^{(k, r)}-\bm{\omega}^{(k,r)}_{0}\|_{1};\] (21) \[V(f^{(k)}_{\bm{\theta}},f^{(k)}_{0}) \leq C_{V}\sum_{r=1}^{R}\mathbb{E}_{\mathbf{x}}\|\bm{\omega}^{(k, r)}-\bm{\omega}^{(k,r)}_{0}\|_{1},\] (22)

_for some positive constants \(C_{K}\) and \(C_{V}\) in the neighbourhood of \(\bm{\theta}_{0}\)4._

Footnote 4: These two distances can also be upper bounded by the \(L_{2}\) norm of \(\bm{\omega}_{2}-\bm{\omega}_{1}\) since the \(L_{1}\) norm and \(L_{2}\) norm are equivalent on \(\mathbb{R}^{p}\) in the sense that \(|\bm{\omega}|_{2}\leq|\bm{\omega}|_{1}\leq\sqrt{p}|\bm{\omega}|_{2}\).

Proof.: By Definition (1) (iii) and (20), we have that

\[K(f^{(k)}_{\bm{\theta}},f^{(k)}_{0}) =\int\int f^{(k)}_{0}\log\bigg{(}\frac{f^{(k)}_{0}}{f^{(k)}_{\bm{ \theta}}}\bigg{)}\nu_{2}(d\tilde{\mathbf{y}})\nu_{1}(d\mathbf{x})\] \[=\sum_{r=1}^{R}\int\int f^{(k)}_{0}\log\bigg{(}\frac{f^{(k,r)}_{0 }}{f^{(k,r)}_{\bm{\theta}}}\bigg{)}\nu_{2}(d\tilde{\mathbf{y}})\nu_{1}(d \mathbf{x})\] \[=\sum_{r=1}^{R}\int\int f^{(k,r)}_{0}\log\bigg{(}\frac{f^{(k,r)}_{ 0}}{f^{(k,r)}_{\bm{\theta}}}\bigg{)}\nu_{2,r}(d\tilde{y}^{(r)})\nu_{1}(d \mathbf{x}).\]According to (19) and by taking the Taylor's expansion of \(\log\left(f_{\bm{\theta}}^{k,r}\right)\) at \(\bm{\omega}_{0}^{(k,r)}\), we obtain that

\[\int f_{0}^{(k,r)}\log\left(\frac{f_{0}^{(k,r)}}{f_{\bm{\theta}}^{( k,r)}}\right)\nu_{2,r}(d\tilde{y}^{(r)})\] \[= -\int f_{0}^{(k,r)}\left[\left(\frac{\partial\log f_{\bm{\theta}}^ {(k,r)}}{\partial\omega}\Big{|}_{\bm{\omega}=\overline{\omega}}\right)^{\top} \left(\bm{\omega}^{(k,r)}-\bm{\omega}_{0}^{(k,r)}\right)\right]\nu_{2,r}(d\tilde {y}^{(r)})\] \[= -\int f_{0}^{(k,r)}\left[\left(\frac{\partial\log(G(\bm{\omega}) ^{\top}\tilde{y}^{(r)})}{\partial\omega}\Big{|}_{\bm{\omega}=\overline{\omega }}\right)^{\top}\left(\bm{\omega}^{(k,r)}-\bm{\omega}_{0}^{(k,r)}\right) \right]\nu_{2,r}(d\tilde{y}^{(r)})\] \[= -\int f_{0}^{(k,r)}\left[\left(\frac{1}{G(\overline{\tilde{\bm{ \omega}}})^{\top}}\frac{\partial G(\overline{\tilde{\bm{\omega}}})^{\top}}{ \partial\bm{\omega}}\right)^{\top}\left(\bm{\omega}^{(k,r)}-\bm{\omega}_{0}^{( k,r)}\right)\right]\nu_{2,r}(d\tilde{y}^{(r)}),\]

where \(\overline{\tilde{\bm{\omega}}}=(\overline{\omega}_{1},...,\overline{\omega}_ {K})^{\top}\) is between \(\bm{\omega}^{(k,r)}\) and \(\bm{\omega}_{0}^{(k,r)}\) and

\[\frac{\partial G(\overline{\tilde{\bm{\omega}}})^{\top}}{\partial\bm{\omega} }=\left(\frac{\partial G_{1}(\overline{\tilde{\bm{\omega}}})}{\partial \bar{\omega}},...,\frac{\partial G_{K}(\overline{\tilde{\bm{\omega}}})}{ \partial\bar{\omega}}\right)=\begin{pmatrix}\frac{\partial G_{1}(\overline{ \tilde{\bm{\omega}}})}{\partial\overline{\omega}_{1}}&\cdots&\frac{\partial G _{K}(\overline{\tilde{\bm{\omega}}})}{\partial\overline{\omega}_{1}}\\ \vdots&&\vdots\\ \frac{\partial G_{1}(\overline{\tilde{\bm{\omega}}})}{\partial\overline{ \omega}_{K}}&\cdots&\frac{\partial G_{K}(\overline{\tilde{\bm{\omega}}})}{ \partial\overline{\omega}_{K}}\end{pmatrix}.\]

Since \(\tilde{y}^{(r)}\) is discretely distributed, according to (19) and Condition A.4.1, we further obtain that

\[\int f_{0}^{(k,r)}\log\left(\frac{f_{0}^{(k,r)}}{f_{\bm{\theta}}^ {(k,r)}}\right)\nu_{2,r}(d\tilde{y}^{(r)})\] \[= -\sum_{j=1}^{K}\frac{G_{j}(\bm{\omega}_{0}^{(k,r)})}{G_{j}( \overline{\tilde{\bm{\omega}}})}\left\{\frac{\partial G_{j}(\overline{\tilde{ \bm{\omega}}})}{\partial\bm{\omega}}\right\}^{\top}\left(\bm{\omega}^{(k,r)}- \bm{\omega}_{0}^{(k,r)}\right)\] \[\leq \frac{C_{3}K}{1-\epsilon_{2}}\|\bm{\omega}^{(k,r)}-\bm{\omega}_{0 }^{(k,r)}\|_{1}.\]

Thus, the proof of inequality (21) is completed.

Similarly, we write \(V(f_{\bm{\theta}}^{(k)},f_{0}^{(k)})\) as:

\[V(f_{\bm{\theta}}^{(k)},f_{0}^{(k)}) =\int\int f_{0}^{(k)}\left\{\log\left(\frac{f_{0}^{(k)}}{f_{\bm{ \theta}}^{(k)}}\right)\right\}^{2}\nu_{2}(d\tilde{y})\nu_{1}(d\mathbf{x})\] \[=\int\int f_{0}^{(k)}\left\{\sum_{r=1}^{R}\log\left(\frac{f_{0}^ {(k,r)}}{f_{\bm{\theta}}^{(k,r)}}\right)\right\}^{2}\nu_{2}(d\tilde{y})\nu_{1 }(d\mathbf{x})\] \[=\sum_{r=1}^{R}\int\int f_{0}^{(k,r)}\left\{\log\left(\frac{f_{0} ^{(k,r)}}{f_{\bm{\theta}}^{(k,r)}}\right)\right\}^{2}\nu_{2,r}(d\tilde{y}^{(r) })\nu_{1}(d\mathbf{x})\] \[\qquad+\sum_{r_{1}\neq r_{2}}\int\left[\int f_{0}^{(k,r_{1})} \left\{\log\left(\frac{f_{0}^{(k,r_{1})}}{f_{\bm{\theta}}^{(k,r_{1})}}\right) \right\}\nu_{2,r_{1}}(d\tilde{y}^{(r_{1})})\right]\] \[\qquad\cdot\left[\int f_{0}^{(k,r_{2})}\left\{\log\left(\frac{f_{0 }^{(k,r_{2})}}{f_{\bm{\theta}}^{(k,r_{2})}}\right)\right\}\nu_{2,r_{2}}(d \tilde{y}^{(r_{2})})\right]\nu_{1}(d\mathbf{x}),\]

where the second term can be upper bounded by

\[\left\{\frac{C_{3}K}{1-\epsilon_{2}}\right\}^{2}\sum_{r_{1}\neq r_{2}}\mathbb{E}_{ \mathbf{x}}\|\bm{\omega}^{(k,r_{1})}-\bm{\omega}_{0}^{(k,r_{1})}\|_{1}\|\bm{ \omega}^{(k,r_{2})}-\bm{\omega}_{0}^{(k,r_{2})}\|_{1},\]

and for the first term, we have

\[\int f_{0}^{(k,r)}\left\{\log\left(\frac{f_{0}^{(k,r)}}{f_{\bm{\theta}}^{(k,r)} }\right)\right\}^{2}\nu_{2,r}(d\tilde{y}^{(r)})=\sum_{j=1}^{K}G_{j}(\bm{\omega} _{0}^{(k,r)})\left\{\log\left(\frac{G_{j}(\bm{\omega}_{0}^{(k,r)})}{G_{j}(\bm{ \omega}^{(k,r)})}\right)\right\}^{2}.\]If \(G_{j}(\bm{\omega}_{0}^{(k,r)})\leq\|\bm{\omega}^{(k,r)})-\bm{\omega}_{0}^{(k,r)} \|_{1}\), we have that

\[G_{j}(\bm{\omega}_{0}^{(k,r)})\left\{\log\left(\frac{G_{j}(\bm{\omega}_{0}^{(k,r )})}{G_{j}(\bm{\omega}^{(k,r)})}\right)\right\}^{2}\leq\max\{|\log(1-\epsilon_ {2})|,|\log(1+\epsilon_{2})|\}\|\bm{\omega}^{(k,r)})-\bm{\omega}_{0}^{(k,r)})\|_{1}\]

according to Condition A.4.1. If \(G_{j}(\bm{\omega}_{0}^{(k,r)})>\|\bm{\omega}^{(k,r)})-\bm{\omega}_{0}^{(k,r)} \|_{1}\), similar to the proof for (21), we obtain that

\[G_{j}(\bm{\omega}_{0}^{(k,r)})\left\{\log\left(\frac{G_{j}(\bm{ \omega}_{0}^{(k,r)})}{G_{j}(\bm{\omega}^{(k,r)})}\right)\right\}^{2}= G_{j}(\bm{\omega}_{0}^{(k,r)})\left[\frac{1}{G_{j}(\overline{\bm{ \omega}})}\left\{\frac{\partial G_{j}(\bar{\bm{\omega}})}{\partial\bm{\omega} }\right\}^{\top}(\bm{\omega}^{(k,r)}-\bm{\omega}_{0}^{(k,r)})\right]^{2}\] \[\leq \frac{G_{j}(\bm{\omega}_{0}^{(k,r)})}{\left\{(1-\epsilon_{2})G_{ j}(\bm{\omega}_{0}^{(k,r)})\right\}^{2}}\cdot\left(C_{3}\|\bm{\omega}^{(k,r)}- \bm{\omega}_{0}^{(k,r)}\|_{1}\right)^{2}\] \[\leq \left(\frac{C_{3}}{1-\epsilon_{2}}\right)^{2}\|\bm{\omega}^{(k,r) }-\bm{\omega}_{0}^{(k,r)}\|_{1},\]

where \(\bar{\bm{\omega}}\) is between \(\bm{\omega}_{0}^{(k,r)}\) and \(\bm{\omega}^{(k,r)}\). Thus, the inequality (22) holds in the neighbourhood of \(\bm{\theta}_{0}\). 

#### b.2.2 Verification of Condition (c) in Lemma 3

Proof.: For the sequence \(\{\epsilon_{n}\}_{n=1}^{\infty}\) in Lemma 3, by (21) and (22) in Result 1 and (17), we obtain that

\[\Pi(G_{n}^{*}(\bm{\theta}_{0},\epsilon_{n}))\geq\Pi\left\{\bm{\theta}:\mathbb{ E}_{\bm{\bar{\omega}}}\|\bm{\omega}^{(k,r)}-\bm{\omega}_{0}^{(k,r)}\|_{1}\leq \Delta_{n}\text{ for }k\in[K],r\in[R]\right\},\]

where \(\Delta_{n}=U_{1}\epsilon_{n}^{2}\) for some positive constant \(U_{1}\).

For simplicity of presentation, we now omit \(k\) and \(r\) in \(f_{0}^{(k,r)}\), \(f_{\bm{\theta}}^{(k,r)}\), \(\bm{\omega}_{0}^{(k,r)}\), \(\bm{\omega}^{(k,r)}\), \(\bm{\mathbf{B}}^{(k)}\) and \(\mathbf{A}^{(r)}\) in this proof here. Note that

\[\|\bm{\omega}-\bm{\omega}_{0}\|_{1}=\|\mathbf{B}^{\top}\psi_{2}( \mathbf{x};\bm{\theta}^{(2)})+\mathbf{A}^{\top}\psi_{1}(\mathbf{x};\bm{\theta} ^{(1)})-\mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x})-\mathbf{A}_{0}^{\top}\psi_{1 }(\mathbf{x})\|_{1}\] \[\leq \|\mathbf{B}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)})-\mathbf{ B}_{0}^{\top}\psi_{2}(\mathbf{x})\|_{1}+\|\mathbf{A}^{\top}\psi_{1}(\mathbf{x}; \bm{\theta}^{(1)})-\mathbf{A}_{0}^{\top}\psi_{1}(\mathbf{x})\|_{1}\] \[\leq \|\mathbf{B}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)})-\mathbf{ B}_{0}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)})\|_{1}+\|\mathbf{B}_{0}^{\top} \psi_{2}(\mathbf{x};\bm{\theta}^{(2)})-\mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x}; \bm{\theta}^{(2)*})\|_{1}\] \[+\|\mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)*})- \mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x})\|_{1}+\|\mathbf{A}^{\top}\psi_{1}( \mathbf{x};\bm{\theta}^{(1)})-\mathbf{A}_{0}^{\top}\psi_{1}(\mathbf{x};\bm{ \theta}^{(1)})\|_{1}\] \[+\|\mathbf{A}_{0}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)})- \mathbf{A}_{0}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)*})\|_{1}+\|\mathbf{A} _{0}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)*})-\mathbf{A}_{0}^{\top}\psi_{ 1}(\mathbf{x})\|_{1}.\]

Corresponding to each term above, we consider the following six terms:

\[\text{(I)}:\Pi\{\bm{\theta}:\mathbb{E}_{\bm{\bar{\omega}}}\|\mathbf{B}^{\top} \psi_{2}(\mathbf{x};\bm{\theta}^{(2)})-\mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x}; \bm{\theta}^{(2)})\|_{1}\leq\Delta_{n}/6\};\] \[\text{(II)}:\Pi\{\bm{\theta}:\mathbb{E}_{\bm{\bar{\omega}}}\| \mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)})-\mathbf{B}_{0}^{ \top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)*})\|_{1}\leq\Delta_{n}/6\};\] \[\text{(III)}:\Pi\{\bm{\theta}:\mathbb{E}_{\bm{\bar{\omega}}}\| \mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)*})-\mathbf{B}_{0}^{ \top}\psi_{2}(\mathbf{x})\|_{1}\leq\Delta_{n}/6\};\] \[\text{(I)}^{\prime}:\Pi\{\bm{\theta}:\mathbb{E}_{\bm{\bar{\omega}}} \|\mathbf{A}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)})-\mathbf{A}_{0}^{\top} \psi_{1}(\mathbf{x};\bm{\theta}^{(1)})\|_{1}\leq\Delta_{n}/6\};\] \[\text{(II)}^{\prime}:\Pi\{\bm{\theta}:\mathbb{E}_{\bm{\bar{\omega}}} \|\mathbf{A}_{0}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)*})-\mathbf{A}_{0}^{ \top}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)*})\|_{1}\leq\Delta_{n}/6\};\] \[\text{(III)}^{\prime}:\Pi\{\bm{\theta}:\mathbb{E}_{\bm{\bar{\omega}}} \|\mathbf{A}_{0}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}^{(1)*})-\mathbf{A}_{0}^{ \top}\psi_{1}(\mathbf{x})\|_{1}\leq\Delta_{n}/6\}.\]

Corresponding to \(\mathbf{B}^{(k)}\) for (6), we write \(\mathbf{B}=(\bm{\beta}_{1},...,\bm{\beta}_{K})\) and \(\mathbf{B}_{0}=(\bm{\beta}_{10},...,\bm{\beta}_{K0})\). Then for (I), we have that

\[\Pi\Big{\{}\bm{\theta}:\mathbb{E}_{\bm{\bar{\omega}}}\|\mathbf{B}^{ \top}\psi_{2}(\mathbf{x};\bm{\theta}^{(2)})-\mathbf{B}_{0}^{\top}\psi_{2}( \mathbf{x};\bm{\theta}^{(2)})\|_{1}\leq\Delta_{n}/6\Big{\}}\] \[= \Pi\Big{\{}(\bm{\theta}^{(1)\top},\mathbf{B}^{\top})^{\top}: \sum_{j=1}^{K}\mathbb{E}_{\bm{\bar{\omega}}}|\bm{\beta}_{j}^{\top}\psi_{2}( \mathbf{x};\bm{\theta}^{(where the last equality holds since all columns of **B** have independent and identical prior.

Taking \(\Delta=\frac{\Delta_{n}}{6K}=\frac{U_{1}\epsilon_{n}^{2}}{6K}\), by Proposition 1, we have that for any given nonzero \(\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})\), \(\mathcal{M}_{1}>0\), and \(\mathcal{M}_{2}>0\),

\[\Pi\left\{\boldsymbol{\beta}_{1}:|\boldsymbol{\beta}_{1}^{\top} \psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})-\boldsymbol{\beta}_{10}^{\top} \psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})|\leq\Delta\Big{|}\psi_{2}( \mathbf{x};\boldsymbol{\theta}^{(2)})\right\}\] \[> \exp\left\{-\frac{\left\{\psi_{2}^{\top}(\mathbf{x};\boldsymbol{ \theta}^{(2)})\boldsymbol{\beta}_{0}\right\}^{2}+\Delta^{2}}{2\mathcal{M}_{1} \epsilon_{n}^{2}\cdot S_{2}^{(2)}}\right\}\frac{8\Delta^{4}}{\overline{B}^{2} \|\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})\|^{4}}\] \[> \exp\left\{-\mathcal{M}_{1}n\epsilon_{n}^{2}\cdot\frac{F_{2}^{2} \|\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})\|^{2}+\Delta^{2}}{2\mathcal{M }_{1}n\epsilon_{n}^{2}\cdot S_{2}^{(2)}(\log M^{(2)})^{-1}\|\psi_{2}(\mathbf{x };\boldsymbol{\theta}^{(2)})\|^{2}}\right\}\frac{8\Delta^{4}}{\{S_{1}^{(2)}M^{( 2)q_{2}}\}^{2}\|\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})\|^{4}}\] \[> \exp\left\{-\mathcal{M}_{1}n\epsilon_{n}^{2}\cdot\frac{(F_{2}^{2} +1)\log M^{(2)}}{2\mathcal{M}_{1}n\epsilon_{n}^{2}\cdot S_{2}^{(2)}}\right\}\] \[\cdot\exp\left[-\left\{2\log S_{1}^{(2)}+2q_{2}\log M^{(2)}+4 \log\|\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})\|-\log(8\Delta^{4}) \right\}\right]\] \[\geq \exp\{-\mathcal{M}_{1}n\epsilon_{n}^{2}\}\cdot\exp\{-\mathcal{M}_ {2}n\epsilon_{n}^{2}\},\]

for large enough \(n\), where the second inequality follows from Condition A.4.2 in Section A.2 and Condition B.2.2 in Section A.3, and the third inequality holds since \(\Delta=\frac{\Delta_{n}}{6K}=\frac{U_{1}\epsilon_{n}^{2}}{6K}\) by definition and \(\epsilon_{n}\to 0\) as \(n\to\infty\). The last inequality holds if \(4\log(1/\epsilon_{n}^{2})\leq\mathcal{M}_{2}n\epsilon_{n}^{2}\) for large enough \(n\) and \(\log\|\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})\|\prec\mathcal{M}_{2}n \epsilon_{n}^{2}\), where, according to Lemma 1, the latter holds if \(H_{n2}\left(\log E_{n2}+\log\frac{r_{n2}}{H_{n2}}\right)\prec\mathcal{M}_{2}n \epsilon_{n}^{2}\), which can be guaranteed by Condition A.3.1 in Section A.2. Since the result above holds for any given \(\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})\), by summarizing the discussion above, we obtain that

(I) \[\geq\] \[\geq \exp\{-K(\mathcal{M}_{1}+\mathcal{M}_{2})n\epsilon_{n}^{2}\}.\]

Now we consider the second term (II) in (23), which can be written as

(II) \[= \Pi\{\boldsymbol{\theta}^{(2)}:\mathbb{E}_{\mathbf{x}}\|\textbf{ B}_{0}^{\top}\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})-\textbf{B}_{0}^{\top} \psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)*})\|_{1}\leq\Delta_{n}/6\}\] \[= \Pi\left\{\boldsymbol{\theta}^{(2)}:\sum_{j=1}^{K}\mathbb{E}_{ \mathbf{x}}|\boldsymbol{\beta}_{j0}^{\top}\psi_{2}(\mathbf{x};\boldsymbol{ \theta}^{(2)})-\boldsymbol{\beta}_{j0}^{\top}\psi_{2}(\mathbf{x};\boldsymbol{ \theta}^{(2)*})|\leq\Delta_{n}/6\right\}\] \[\geq \Pi\left\{\boldsymbol{\theta}^{(2)}:\mathbb{E}_{\mathbf{x}}\| \psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})-\psi_{2}(\mathbf{x};\boldsymbol{ \theta}^{(2)*})\|_{1}\leq\frac{\Delta_{n}}{6KF_{2}}\right\},\]

where the last inequality follows from Condition A.4.2 in Section A.2.

Consider the set

\[\mathcal{S}^{(2)}=\{\boldsymbol{\theta}^{(2)}:|\boldsymbol{\theta}^{(2)}_{j}- \boldsymbol{\theta}^{(2)*}_{j}|\leq\eta_{n}\text{ for }j\in\boldsymbol{\gamma}^{(2)*};\ | \boldsymbol{\theta}^{(2)}_{j}-\boldsymbol{\theta}^{(2)*}_{j}|\leq\eta_{n}^{\prime} \text{ for }j\notin\boldsymbol{\gamma}^{(2)*}\},\] (24)

where \(\eta_{n}\) and \(\eta_{n}^{\prime}\) will be specified later. Let \(r_{h}\) denote the number of nonzero connections to the \(h\)th hidden layer which includes the bias for the \(h\)th hidden layer and the weights between the \((h-1)\)th and the \(h\)th layer, such that \(\sum r_{h}=\|\boldsymbol{\gamma}^{(2)*}\|_{1}\triangleq r_{n2}\). Then, for any \(\mathbf{x}\) satisfying \(\|\mathbf{x}\|_{\infty}\leq 1\),

\[\|\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)})-\psi_{2}(\mathbf{x };\boldsymbol{\theta}^{(2)*})\|_{1}\] \[\leq \eta_{n}H_{n2}(E_{n2}+\eta_{n})^{H_{n2}-1}\prod_{h=1}^{H_{n2}}r_{ h}+\eta_{n}^{\prime}J_{n2}\prod_{h=1}^{H_{n2}}\left(E_{n2}+\eta_{n}+\frac{\eta_{n}^{ \prime}L_{h}^{(2)}}{r_{h}}\right)r_{h}\] \[\leq \eta_{n}H_{n2}(U_{2}E_{n2})^{H_{n2}}\left(\frac{\sum r_{h}}{H_{n2} }\right)^{H_{n2}}+\eta_{n}^{\prime}J_{n2}(U_{2}E_{n2})^{H_{n2}}\left(\frac{ \sum r_{h}}{H_{n2}}\right)^{H_{n2}}\] \[\leq 2U_{3}\epsilon_{n}^{2},\]where the last asymptotic equality follows from Condition A.3 in Section A.2 and the fact that \(\log(1/\epsilon_{n})=O(\log n)\). Thus, \(\left(\text{II}\right)\geq\Pi(\mathcal{S}^{(2)})\geq\exp(-\mathcal{M}_{3}n \epsilon_{n}^{2})\) holds if \(r_{n2}H_{n2}\log n+r_{n2}\log\overline{L}_{n2}\leq U_{4}n\epsilon_{n}^{2}\) for some sufficiently small positive constant \(U_{4}\).

Now we consider the third term in (23):

\[\mathbb{E}_{\mathbf{x}}\|\mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x} ;\boldsymbol{\theta}^{(2)*})-\mathbf{B}_{0}^{\top}\psi_{2}(\mathbf{x})\|_{1} =\sum_{j=1}^{K}\mathbb{E}_{\mathbf{x}}\big{|}\boldsymbol{\beta}_{j0 }^{\top}\{\psi_{2}(\mathbf{x};\boldsymbol{\theta}^{(2)*})-\psi_{2}(\mathbf{x}) \}\big{|}\] \[\leq KF_{2}\mathbb{E}_{\mathbf{x}}\big{|}\psi_{2}(\mathbf{x}; \boldsymbol{\theta}^{(2)*})-\psi_{2}(\mathbf{x})\big{|}\] \[\leq KF_{2}\varpi_{n2},\]

where the second inequality follows from Condition A.4.2 in Section A.2 and the last inequality follows from the definition of the true model given in (14). Thus, we can take \(\epsilon_{n}^{2}=O(\varpi_{n2})\) so that \(\left(\text{III}\right)\approx 1\) for large enough \(n\).

Similar discussion can be applied to \(\left(\text{I}\right)^{\prime}\), \(\left(\text{II}\right)^{\prime}\) and \(\left(\text{III}\right)^{\prime}\) in (23) and we can obtain that

\[\left(\text{I}\right)^{\prime} \geq\left[\inf_{\boldsymbol{\theta}\in\mathcal{F}_{n}}\Pi\left\{ \boldsymbol{\alpha}_{1}:\mathbb{E}_{\mathbf{x}}|\boldsymbol{\alpha}_{1}^{\top }\psi_{1}(\mathbf{x};\boldsymbol{\theta}^{(1)})-\boldsymbol{\alpha}_{10}^{ \top}\psi_{1}(\mathbf{x};\boldsymbol{\theta}^{(1)})|\leq\frac{\Delta_{n}}{6K} \Big{|}\psi_{1}(\mathbf{x};\boldsymbol{\theta}^{(1)})\Big{\}}\right]^{K}\] \[\geq\exp\{-K(\mathcal{M}_{1}^{\prime}+\mathcal{M}_{2}^{\prime})n \epsilon_{n}^{2}\};\] \[\left(\text{II}\right)^{\prime} \geq\Pi(\boldsymbol{\theta}^{(1)}:\mathcal{S}^{(1)})\geq\exp(- \mathcal{M}_{3}^{\prime}n\epsilon_{n}^{2});\] \[\left(\text{III}\right)^{\prime} \approx 1\text{ for large enough }n,\]

where \(\mathcal{M}_{1}^{\prime}\), \(\mathcal{M}_{2}^{\prime}\) and \(\mathcal{M}_{3}^{\prime}\) are constants, and \(\mathcal{S}^{(1)}\) can be similarly defined as in (24). Since the discussion above holds for any \(k\in[K]\) and \(r\in[R]\), by choosing proper \(\mathcal{M}_{1}\), \(\mathcal{M}_{2}\), \(\mathcal{M}_{3}\), \(\mathcal{M}_{1}^{\prime}\), \(\mathcal{M}_{2}^{\prime}\) and \(\mathcal{M}_{3}^{\prime}\), Condition (c) of Lemma 3 can be verified.

[MISSING_PAGE_FAIL:27]

for \(k\in[K]\) and \(r\in[R]\). For ease of presentation, we omit \(k\) and \(r\) in \(\bm{B}_{u}^{(k)}\), \(\bm{B}_{v}^{(k)}\), \(\bm{A}_{u}^{(r)}\), \(\bm{A}_{v}^{(r)}\), \(\bm{\omega}_{u}^{(k,r)}\) and \(\bm{\omega}_{v}^{(k,r)}\), and further obtain that

\[\|\bm{\omega}_{u}-\bm{\omega}_{v}\|_{1} \leq\|\bm{\mathrm{B}}_{u}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}_{u }^{(2)})-\bm{\mathrm{B}}_{v}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}_{v}^{(2)}) \|_{1}+\|\bm{\mathrm{A}}_{u}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}_{u}^{(1)}) -\bm{\mathrm{A}}_{v}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}_{v}^{(1)})\|_{1}\] \[\leq\|\bm{\mathrm{B}}_{u}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}_{u }^{(2)})-\bm{\mathrm{B}}_{u}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}_{v}^{(2)}) \|_{1}+\|\bm{\mathrm{B}}_{u}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}_{v}^{(2)}) -\bm{\mathrm{B}}_{v}^{\top}\psi_{2}(\mathbf{x};\bm{\theta}_{v}^{(2)})\|_{1}\] \[\quad+\|\bm{\mathrm{A}}_{u}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}_ {u}^{(1)})-\bm{\mathrm{A}}_{v}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}_{v}^{(1)} )\|_{1}+\|\bm{\mathrm{A}}_{u}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}_{v}^{(1)} )-\bm{\mathrm{A}}_{v}^{\top}\psi_{1}(\mathbf{x};\bm{\theta}_{v}^{(1)})\|_{1}\] \[=\sum_{i=j}^{K}|\bm{\beta}_{uj}^{\top}\{\psi_{2}(\mathbf{x};\bm{ \theta}_{u}^{(2)})-\psi_{2}(\mathbf{x};\bm{\theta}_{v}^{(2)})\}|+\sum_{j=1}^{K }|(\bm{\beta}_{uj}-\bm{\beta}_{vj})^{\top}\psi_{2}(\mathbf{x};\bm{\theta}_{v}^ {(2)})|\] \[\quad+\sum_{i=1}^{K}|\bm{\alpha}_{ui}^{\top}\{\psi_{1}(\mathbf{x}; \bm{\theta}_{u}^{(1)})-\psi_{1}(\mathbf{x};\bm{\theta}_{v}^{(1)})\}|+\sum_{i=1 }^{K}|(\bm{\alpha}_{ui}-\bm{\alpha}_{vi})^{\top}\psi_{1}(\mathbf{x};\bm{\theta} _{v}^{(1)})|\] \[\leq Kb_{n}\|\psi_{2}(\mathbf{x};\bm{\theta}_{u}^{(2)})-\psi_{2}( \mathbf{x};\bm{\theta}_{v}^{(2)})\|_{1}+K\rho_{n}\|\psi_{2}(\mathbf{x};\bm{ \theta}_{v}^{(2)})\|\] \[\quad+Ka_{n}\|\psi_{1}(\mathbf{x};\bm{\theta}_{u}^{(1)})-\psi_{1} (\mathbf{x};\bm{\theta}_{v}^{(1)})\|_{1}+K\rho_{n}\|\psi_{1}(\mathbf{x};\bm{ \theta}_{v}^{(1)})\|,\]

where the preceding four boundness assumptions for \(\bm{\theta}_{u}\) and \(\bm{\theta}_{v}\), together with the definition of \(\mathcal{A}_{n}\) and \(\mathcal{B}_{n}\), are used.

Similar to the proof in Appendix B.2.2, by using Lemmas 1 and 2, we can further obtain that \(d_{n}^{2}(\bm{\theta}_{u},\bm{\theta}_{v})\leq(\epsilon_{n}/36)^{2}\) if we choose \(\rho_{n}\), \(\delta_{n1}^{\prime}\) and \(\delta_{n2}^{\prime}\) in the preceding derivations as follows:

1. \(\delta_{n1}^{\prime}=U_{6}\epsilon_{n}^{2}/b_{n}J_{n1}(U_{6}^{\prime}I_{n}^{(1 )})^{H_{n1}}(k_{n1}r_{n1}/H_{n1})^{H_{n1}}\) for some constants \(U_{6}>0\) and \(U_{6}^{\prime}>1\);
2. \(\delta_{n2}^{\prime}=U_{7}\epsilon_{n}^{2}/a_{n}J_{n2}(U_{7}^{\prime}I_{n}^{(2 )})^{H_{n2}}(k_{n2}r_{n2}/H_{n2})^{H_{n2}}\) for some constants \(U_{7}>0\) and \(U_{7}^{\prime}>1\);
3. \(\rho_{n}=\min\left\{U_{8,1}\epsilon_{n}^{2}/b_{n}H_{n1}(U_{8,1}^{\prime}I_{n}^{( 1)})^{H_{n1}}(k_{n1}r_{n1}/H_{n1})^{H_{n1}},\right.\) \[\left.U_{8,2}\epsilon_{n}^{2}/a_{n}H_{n2}(U_{8,2}^{\prime}I_{n}^{(2 )})^{H_{n2}}(k_{n2}r_{n2}/H_{n2})^{H_{n2}}\right\}\] for some constants \(U_{8,1},U_{8,2}>0\) and \(U_{8,1}^{\prime},U_{8,2}^{\prime}>1\).

Thus, the log covering number of \(\mathcal{P}_{n}\), \(\log N(\epsilon_{n}/36,\mathcal{P}_{n},d_{n})\), can be upper bounded by \(\log\mathcal{K}_{n}=\log\mathcal{K}_{n1}+\log\mathcal{K}_{n2}+\log\mathcal{K}_ {n3}+\log\mathcal{K}_{n4}\), with \(\mathcal{K}_{nj}\) given in (25) for \(j=1,2,3,4\).

Considering the fact that \(\log\chi^{(1)}(j)\leq\log\left(\begin{smallmatrix}\nu_{n_{1}s_{1}}^{P_{n_{1}}} \end{smallmatrix}\right)\left(\begin{smallmatrix}k_{n1}^{\prime}s_{n1}+H_{n1} \overline{L}_{n1}\\ \end{smallmatrix}\right)\leq k_{n1}^{\prime}s_{n1}\log(p_{n})+j\log(k_{n1}^{ \prime}s_{n1}+H_{n1}\overline{L}_{n1}^{2})\leq k_{n1}^{\prime}s_{n1}\log(p_{n})+k _{n1}r_{n1}\log\{H_{n1}(k_{n1}^{\prime}s_{n1}+\overline{L}_{n1})^{2}\}\), and by choosing \(\log I_{n}^{(1)}=O(\log n)\), \(\log I_{n}^{(2)}=O(\log n)\), \(a_{n}\asymp\sqrt{n\epsilon_{n}^{2}}\) and \(b_{n}\asymp\sqrt{n\epsilon_{n}^{2}}\), we have that

\[\log\mathcal{K}_{n1}\] \[\leq \log(k_{n1}r_{n1})+\log\chi^{(1)}(k_{n1}r_{n1})+k_{n1}r_{n1}\log \Big{(}\frac{I_{n}^{(1)}}{\rho_{n}}+1\Big{)}\] \[\leq \log(k_{n1}r_{n1})+k_{n1}^{\prime}s_{n1}\log(p_{n})+k_{n1}r_{n1} \log H_{n1}+2k_{n1}r_{n1}\log(k_{n1}^{\prime}s_{n1}+\overline{L}_{n1})\] \[+k_{n1}r_{n1}\bigg{\{}\text{constant}+\log I_{n}^{(1)}+\log b_{n} +\log H_{n1}+H_{n1}\log(U_{7,1}^{\prime}I_{n}^{(1)})+H_{n1}\log(k_{n1}r_{n1}/H_{n1})\] \[+\log a_{n}+\log H_{n2}+H_{n2}\log(U_{7,2}^{\prime}I_{n}^{(2)})+H_ {n2}\log(k_{n2}r_{n2}/H_{n2})+\log\frac{1}{\epsilon_{n}^{2}}\bigg{\}}\] \[= k_{n1}^{\prime}s_{n1}\log(p_{n})+k_{n1}r_{n1}\cdot O\left(H_{n1} \log n+H_{n2}\log n+\log\overline{L}_{n1}+\log\overline{L}_{n2}\right),\]

where the last equality holds since \(k_{n1}r_{n1}\leq n\), \(k_{n2}r_{n2}\leq n\), \(k_{n1}^{\prime}s_{n1}\leq n\) and \(k_{n1}^{\prime}s_{n1}\leq n\). By choosing \(k_{n1}^{\prime}\) and \(k_{n1}\) such that \(k_{n1}r_{n1}\left\(\{\epsilon_{n}\}_{n=1}^{\infty}\) satisfies that \(n\epsilon_{n}^{2}=O\big{\{}(r_{n1}+r_{n2})(H_{n1}+H_{n2})\log n+(r_{n1}+r_{n2})( \log\overline{L}_{n1}+\log\overline{L}_{n2})\)\(+(s_{n1}+s_{n2})\log p_{n}\big{\}}\).

#### b.2.4 Verification of Condition (b) in Lemma 3

Proof.: Since \(\Pi(\Theta_{n}^{c})\leq\Pi(\mathcal{S}_{n1}^{c})+\Pi(\mathcal{S}_{n2}^{c})+ \Pi(\mathcal{B}_{n}^{c})+\Pi(\mathcal{A}_{n}^{c})\), we first examine \(\Pi(\mathcal{B}_{n}^{c})\) and \(\Pi(\mathcal{A}_{n}^{c})\). For any \(b_{n}>0\), by the prior assumption given in (6), we have that

\[\Pi(\mathcal{B}_{n}^{c}) =\Pi\left\{\textbf{B}^{(k)}=(\boldsymbol{\beta}_{1}^{(k)},..., \boldsymbol{\beta}_{K}^{(k)})\text{ for }k\in[K]:|\beta_{l,j}^{(k)}|\leq b_{n}\text{ for }l,k\in[K],\text{ and }j\in[M^{(2)}] \right\}^{c}\] \[=\Pi(\cup_{k,l\in[K],j\in[M^{(2)}]}|\beta_{l,j}^{(k)}|>b_{n})\] \[\leq\sum_{k=1}^{K}\sum_{l=1}^{K}\sum_{j=1}^{M^{(2)}}\pi_{B}^{(k)} (|\beta_{l,j}^{(k)}|>b_{n})\] \[=K\sum_{k=1}^{K}\sum_{j=1}^{M^{(2)}}\pi_{B}^{(k)}(|\beta_{1,j}^{ (k)}|>b_{n}),\]

where \(\pi_{B}^{(k)}(\cdot)\) denotes the measure induced by the prior distribution of \(\textbf{B}^{(k)}\) as defined in Section 3.2.

Applying the bounds on the Mills ratio [76] for the standard normal distribution: \(\frac{1}{x}-\frac{1}{x^{3}}<\frac{1-\Phi(x)}{\varphi(x)}<\frac{1}{x}-\frac{1}{ x^{3}}+\frac{3}{x^{5}}\) for \(-\infty<x<\infty\), where \(\Phi(\cdot)\) and \(\varphi(\cdot)\) denote the CDF and probability density function (PDF) of the standard normal distribution, respectively, we further obtain that, by taking

\[b_{n}=\max_{k\in[K]}\sqrt{5\overline{B}_{n}^{(k)}}n\epsilon_{n} ^{2},\] \[\pi_{B}^{(k)}(|\beta_{1,j}^{(k)}|>b_{n}) <\frac{2\varphi\left(b_{n}/\sqrt{(\boldsymbol{\Sigma}_{\beta}^{( k)})_{j,j}}\right)}{b_{n}/\sqrt{(\boldsymbol{\Sigma}_{\beta}^{(k)})_{j,j}}}= \frac{2}{\sqrt{2\pi b_{n}^{2}/(\boldsymbol{\Sigma}_{\beta}^{(k)})_{j,j}}}\cdot \exp\left\{-\frac{1}{2}(b_{n}^{2}/(\boldsymbol{\Sigma}_{\beta}^{(k)})_{j,j})\right\}\] \[\leq\frac{2}{\sqrt{2\pi b_{n}^{2}/\overline{B}_{n}^{(k)}}}\cdot \exp\left\{-\frac{1}{2}(b_{n}^{2}/\overline{B}_{n}^{(k)})\right\}\prec\exp \left(-\frac{5}{2}n\epsilon_{n}^{2}\right)\]

where \(A_{j,j}\) represents the \((j,j)\) element of matrix \(A\), the first step is due to the Mills ratio bounds, the second step comes from the definition of the PDF of the standard normal distribution, the third step is due to the fact that the sum of the eigenvalues of a matrix equals the trace of the matrix and the positivity of \(\left(\boldsymbol{\Sigma}_{\beta}^{(k)}\right)_{j,j}\), and the last step comes from the choice of \(b_{n}\). Therefore, it follows that \(\Pi(\mathcal{B}_{n}^{c})<K^{2}M^{(2)}\exp(-\frac{5}{2}n\epsilon_{n}^{2})\) for large enough \(n\).

Similarly, we can obtain that \(\Pi(\mathcal{A}_{n}^{c})<K^{2}M^{(1)}\exp(-\frac{5}{2}n\epsilon_{n}^{2})\) for large enough \(n\) by taking \(a_{n}=\max_{r\in[R]}\sqrt{5\overline{A}_{n}^{(r)}n\epsilon_{n}^{2}}\).

We then consider \(\Pi(\mathcal{S}_{n}^{c})\), which can be upper bounded by

\[\Pi(\mathcal{S}_{n1}^{c}) \leq\Pi(\boldsymbol{\theta}^{(1)}:\cup_{j=1}^{J_{n1}}|\theta_{j}^{ (1)}|>I_{n}^{(1)})+\Pi(\boldsymbol{\theta}^{(1)}:|\boldsymbol{\gamma}_{ \theta}^{(1)}|=|\{j:|\theta_{j}^{(1)}|\geq\delta_{n1}^{\prime}\}|>k_{n1}r_{n1})\] \[\quad+\pi(\boldsymbol{\theta}^{(1)}:|\boldsymbol{\gamma}_{\theta} ^{(1)}|_{\text{in}}>k_{n1}^{\prime}s_{n1})\] \[=J_{n1}\pi_{\theta}^{(1)}(|\theta_{1}^{(1)}|>I_{n}^{(1)}|)+\mathbb{ P}\{\text{Binomial}(J_{n1},\nu_{n})>k_{n1}r_{n1}\}\] (26) \[\quad+\mathbb{P}\{\text{Binomial}(p_{n}L_{1}^{(1)},\nu_{n})>k_{n1 }^{\prime}s_{n1}\}\]

where \(\nu_{n}=1-\pi_{\theta}^{(1)}([-\delta_{n1}^{\prime},\delta_{n1}^{\prime}])\leq \frac{1}{J_{n1}}\exp\{(H_{n1}+H_{n2})\log n+\log\overline{L}_{n1}+\log \overline{L}_{n2}+\log p_{n}\}\) according to Condition B.1.2 in Section A.3. For the first term in (26), we have that \(J_{n1}\pi_{\theta}^{(1)}(|\theta_{1}^{(1)}|>I_{n}^{(1)}|)\prec\exp\{-(2+ \epsilon_{3}^{(k)})n\epsilon_{n}^{2}\}\) following Condition B.1.3 in Section A.3, where \(\epsilon_{3}^{(k)}>0\) is a constant. The second term can be upper bounded by

\[\mathbb{P}\{\mathrm{Binomial}(J_{n1},\nu_{n})>k_{n1}r_{n1}\}\] \[= \sum_{j=k_{n1}r_{n1}+1}^{J_{n1}}\binom{J_{n1}}{j}\nu_{n}^{j}(1-\nu _{n})^{J_{n1}-j}\] \[\leq \sum_{j=k_{n1}r_{n1}+1}^{J_{n1}}J_{n1}^{j}\nu_{n}^{j}(1-\nu_{n})^ {J_{n1}-j}\leq J_{n1}(J_{n1}\nu_{n})^{k_{n1}r_{n1}}\] \[\leq J_{n1}\exp\left[-S_{0}^{(1)}k_{n1}r_{n1}\big{\{}(H_{n1}+H_{n2}) \log n+\log\overline{L}_{n1}+\log\overline{L}_{n2}+\log p_{n}\big{\}}\right]\] \[\prec \exp\left\{-S_{0}^{(1)}n\epsilon_{n}^{2}\right\},\]

where the last inequality is due to the choice of \(k_{n1}\) in Appendix B.2.3, and \(S_{0}^{(1)}>2\) is a constant given in Condition B.1.2 in Section A.3.

Similarly, for the third term in (26) we have that

\[\mathbb{P}\{\mathrm{Binomial}(p_{n}L_{1}^{(1)},\nu_{n})>k_{n1}^{ \prime}s_{n1}\}\] \[= \sum_{j=k_{n1}^{\prime}s_{n1}+1}^{p_{n}L_{1}^{(1)}}j\binom{p_{n} L_{1}^{(1)}}{j}\nu_{n}^{j}(1-\nu_{n})^{p_{n}L_{1}^{(1)}-j}\] \[\leq \sum_{j=k_{n1}^{\prime}s_{n1}+1}^{p_{n}L_{1}^{(1)}}(p_{n}L_{1}^{( 1)})^{j}\nu_{n}^{j}(1-\nu_{n})^{p_{n}L_{1}^{(1)}-j}=p_{n}L_{1}^{(1)}(J_{n1}\nu _{n})^{k_{n1}^{\prime}s_{n1}}(\frac{p_{n}L_{1}^{(1)}}{J_{n1}})^{k_{n1}^{\prime }s_{n1}}\] \[\leq p_{n}L_{1}^{(1)}\exp\Big{[}-S_{0}^{(1)}k_{n1}^{\prime}s_{n1}\big{\{} (H_{n1}+H_{n2})\log n+\log\overline{L}_{n1}+\log\overline{L}_{n2}+\log p_{n} \big{\}}\] \[-k_{n1}^{\prime}s_{n1}\log\left(\frac{J_{n1}}{p_{n}L_{1}^{(1)}} \right)\Big{]}\] \[\prec \exp\{-S_{0}^{(1)}n\epsilon_{n}^{2}\}.\]

Thus, Condition (b) in Lemma 3 is verified.

### Proof of Theorem 2

We now prove Theorem 2. The proof of the upper bound adapts the techniques in [46], and the proof of the lower bound employs a general mechanism for lower bounding the Bayes risk [40], which is tighter than the Fano's inequality [44].

Proof.: **Upper Bound.** The Bayes risk can be written as

\[\Re_{\text{Bayes}} =\inf_{\overline{\mathbf{y}}}\left\{\sum_{\mathbf{y}\in[K]^{n}} \hbar(\mathbf{y})\mathbb{E}(\mathcal{L}(\overline{\mathbf{y}},\mathbf{y})| \mathbf{y};\boldsymbol{\tau})\right\}\] \[=\inf_{\overline{\mathbf{y}}}\left\{\sum_{\mathbf{y}\in[K]^{n}} \hbar(\mathbf{y})\frac{1}{\bar{n}}\sum_{i=1}^{\bar{n}}\mathbb{P}(\overline{ \mathbf{y}}_{i}\neq\mathrm{y}_{i}|\mathrm{y}_{i};\boldsymbol{\tau})\right\}\] \[=\inf_{\overline{\mathbf{y}}}\left\{\frac{1}{\bar{n}}\sum_{i=1}^{ \bar{n}}\sum_{\mathbf{y}\in[K]^{n}}\hbar(\mathbf{y})\mathbb{P}(\overline{ \mathbf{y}}_{i}\neq\mathrm{y}_{i}|\mathrm{y}_{i};\boldsymbol{\tau})\right\}\] \[=\inf_{\overline{\mathbf{y}}}\left\{\frac{1}{\bar{n}}\sum_{i=1}^{ \bar{n}}\sum_{g=1}^{K}\hbar_{i,g}\mathbb{P}(\overline{\mathbf{y}}_{i}\neq g| \mathrm{y}_{i}=g;\boldsymbol{\tau})\right\}.\]We first consider the situation \(g=1\):

\[\mathbb{P}(\overline{y}_{i}\neq 1|y_{i}=1;\bm{\tau})\leq\sum_{g^{\prime}=2}^{K} \mathbb{P}(\overline{y}_{i}=g^{\prime}|y_{i}=1;\bm{\tau}).\]

For each \(g^{\prime}\geq 2\), considering the inference procedure of \(\overline{y}_{i}\), we have that

\[\mathbb{P}(\overline{y}_{i}=g^{\prime}|y_{i}=1;\bm{\tau})\] \[\leq \mathbb{P}\left(\frac{\hbar_{i,g^{\prime}}}{\hbar_{i,1}}\prod_{r= 1}^{R}\prod_{l=1}^{K}\left(\frac{\tau_{i,g^{\prime}l}^{(r)}}{\tau_{i,1l}^{(r)} }\right)^{\nicefrac{{1}(y_{i}^{(r)}=l)}{{\bm{\tau}}}}>\Omega\Bigg{|}y_{i}=1; \bm{\tau}\right)\] \[= \mathbb{P}\left(\left(\frac{\hbar_{i,g^{\prime}}}{\Omega\hbar_{i,1}}\right)^{\lambda}\prod_{r=1}^{R}\prod_{l=1}^{K}\left(\frac{\tau_{i,g^{ \prime}l}^{(r)}}{\tau_{i,1l}^{(r)}}\right)^{\nicefrac{{\Lambda}(y_{i}^{(r)}=l )}{{\bm{\tau}}}}>1\Bigg{|}y_{i}=1;\bm{\tau}\right)\] \[\leq \min_{\lambda\geq 0}\left(\frac{\hbar_{i,g^{\prime}}}{\Omega\hbar_{i,1}}\right)^{\lambda}\prod_{r=1}^{R}\mathbb{E}\left\{\prod_{l=1}^{K}\left( \frac{\tau_{i,g^{\prime}l}^{(r)}}{\tau_{i,1l}^{(r)}}\right)^{\nicefrac{{ \lambda}(y_{i}^{(r)}=l)}{{\bm{\tau}}}}\Bigg{|}y_{i}=1;\bm{\tau}\right\}\] \[= \min_{\lambda\geq 0}\left(\frac{\hbar_{i,g^{\prime}}}{\Omega\hbar_{i,1}}\right)^{\lambda}\prod_{r=1}^{R}\sum_{l=1}^{K}\left(\tau_{i,g^{\prime}l}^{ (r)}\right)^{\lambda}\left(\tau_{i,1l}^{(r)}\right)^{1-\lambda}\] \[\leq \exp\left\{-R\left\{-\min_{0\leq\lambda\leq 1}\frac{1}{R} \left\{-\lambda\log\frac{\Omega\hbar_{i,1}}{\hbar_{i,g^{\prime}}}+\sum_{r=1}^ {R}\log\left\{\sum_{l=1}^{K}\left(\tau_{i,g^{\prime}l}^{(r)}\right)^{\lambda }\left(\tau_{i,1l}^{(r)}\right)^{1-\lambda}\right\}\right\}\right\}\right\},\]

where the inequality in the third line follows from Markov's inequality. Denote \(C_{gg^{\prime}}^{(i)}=-\min_{0\leq\lambda\leq 1}\frac{1}{R}\left\{- \lambda\log\frac{\Omega\hbar_{i,g}}{\hbar_{i,g^{\prime}}}+\sum_{r=1}^{R}\log \left\{\sum_{l=1}^{K}\left(\tau_{i,gl}^{(r)}\right)^{1-\lambda}\left(\tau_{i, g^{\prime}l}^{(r)}\right)^{\lambda}\right\}\right\}\) and \(I_{\Omega}^{(g)}(\bm{h}_{i},\bm{\tau}_{i})=\min_{g^{\prime}\neq g}C_{gg^{\prime}}^ {(i)}\), we further obtain that

\[\mathbb{P}(\overline{y}_{i}=l|y_{i}=1;\bm{\tau})\leq\exp\left\{- RC_{1g^{\prime}}^{(i)}\right\}\] \[\leq \exp\left\{-R\min_{g^{\prime}\neq 1}C_{gg^{\prime}}^{(i)}\right\}= \exp\left\{-RI_{\Omega}^{(1)}(\bm{h}_{i},\bm{\tau}_{i})\right\}.\]

Thus, we can obtain the upper bound of the Bayes risk:

\[\Re_{\text{Bayes}}\leq\frac{K-1}{\bar{n}}\sum_{i=1}^{\bar{n}}\sum_{g=1}^{K} \hbar_{i,g}e^{-RI_{\Omega}^{(g)}(\bm{h}_{i},\bm{\tau}_{i})}.\]

**Lower Bound.** Using Markov's inequality, we obtain that

\[\Re_{\text{Bayes}} =\inf_{\overline{\bm{y}}}\left\{\sum_{\bm{y}\in[K]^{n}}\hbar(\bm{ y})\mathbb{E}(\mathcal{L}(\overline{\bm{y}},\bm{y})|\bm{y};\bm{\tau})\right\}\] \[\geq\inf_{\overline{\bm{y}}}\left\{\sum_{\bm{y}\in[K]^{n}}\hbar( \bm{y})\frac{1}{\bar{n}}\mathbb{P}\left(\mathcal{L}(\overline{\bm{y}},\bm{y}) >\frac{1}{\bar{n}}|\bm{y};\bm{\tau}\right)\right\}\] \[=\frac{1}{\bar{n}}\inf_{\overline{\bm{y}}}\left\{\sum_{\bm{y}\in[ K]^{n}}\hbar(\bm{y})\mathbb{P}(\overline{\bm{y}}\neq\bm{y}|\bm{y};\bm{\tau})\right\}\] \[=\frac{1}{\bar{n}}\inf_{\overline{\bm{y}}}\left[\sum_{\bm{y}\in[ K]^{n}}\hbar(\bm{y})\mathbb{E}\left\{\bm{1}(\overline{\bm{y}}\neq\bm{y})|\bm{y};\bm{ \tau}\right\}\right]\] \[\triangleq\frac{1}{\bar{n}}\overline{\Re},\]where the second step is due to Markov's inequality. Using Lemma 7, we can obtain that

\[I_{f}(\hbar,\mathcal{P})\geq\phi_{f}(\overline{\Re},\Re_{0}),\]

where \(\mathcal{P}\) is the set of distributions of \(\{\tilde{\mathbf{y}}_{i}\}_{i=1}^{\bar{n}}\) induced by \(\mathbf{y}\), and for 0-1 loss, \(\Re_{0}\) has the expression in [40]: \(\Re_{0}=1-\sup_{\mathbf{a}\in[K]^{n}}\hbar(B(\mathbf{a}))\) with \(B(\mathbf{a})=\{\mathbf{y}\in[K]^{\bar{n}}:\mathbf{1}(\mathbf{y}\neq\bm{a})=0\}\). Then, it can be easily verified that \(\Re_{0}=1-\prod_{i=1}^{\bar{n}}\max_{k\in[K]}\hbar_{i,k}\).

Let \(g(t)=\phi_{f}(t,\Re_{0})\), which is, by Lemma 6, non-increasing for \(t\in[0,\Re_{0}]\), convex, and continuous in \(t\). Using the convexity of \(g(t)\), we obtain that for every \(t\in(0,\Re_{0}]\)

\[\phi_{f}(\overline{\Re},\Re_{0})\geq\phi_{f}(t,\Re_{0})+\phi_{f}^{\prime}(t-, \Re_{0})(\overline{\Re}-t),\]

where \(\phi_{f}^{\prime}(t-,\Re_{0})\) denotes the left derivative of \(x\mapsto\phi_{f}(x,\Re_{0})\) at \(x=t\). Then we can obtain that

\[\overline{\Re}\geq t+\frac{\phi_{f}(\overline{\Re},\Re_{0})-\phi_{f}(t,\Re_{0 })}{\phi_{f}^{\prime}(t-,\Re_{0})}\geq t+\frac{I_{f}(\hbar,\mathcal{P})-\phi_{ f}(t,\Re_{0})}{\phi_{f}^{\prime}(t-,\Re_{0})},\]

where the inequalities come from the fact that \(I(\hbar,\mathcal{P})\geq\phi_{f}(\overline{\Re},\Re_{0})\) by Lemma 7 and that \(\phi_{f}^{\prime}(t-,\Re_{0})\leq 0\) due to the non-increasing function \(g(t)\) over \(t\in[0,\Re_{0}]\). By taking \(f(t)=t\log t\) and \(t=\frac{\Re_{0}}{1+\Re_{0}}\), we obtain that

\[\overline{\Re}\geq 1+\frac{I(\hbar,\mathcal{P})+\log(1+\Re_{0})}{\log(1-\Re_{0 })},\]

where \(I(\hbar,\mathcal{P})\) is the mutual information of \(\hbar\) and \(\mathcal{P}\).

Let \(Y\) and \(Z\) denote two independent random variables such that \(Y\sim\hbar\) and \(Z\sim\mathcal{P}\), and let \(D_{{}_{KL}}(\cdot\|\cdot)\) denote the KL divergence of the associated distributions. For ease of notation, we use \(P(Y)\), \(P(Z)\), \(P(Y,Z)\) and \(P(Z|Y)\) to denote the \(\hbar\), \(\mathcal{P}\), the joint distribution of \(Y\) and \(Z\), and the conditional distribution of \(Z\), given \(Y\). Then \(I(\hbar,\mathcal{P})\) can be evaluated as follows [43]:

\[I(\hbar,\mathcal{P}) =I(Y;Z)\] \[=D_{{}_{KL}}(P(Y,Z)\|P(Y)P(Z))\] \[=\sum_{Y\in[K]^{n},Z\in[K]^{Rn}}P(Y)P(Z|Y)\log\left\{\frac{P(Y)P( Z|Y)}{P(Y)P(Z)}\right\}\] \[=\sum_{Y\in[K]^{n}}P(Y)D_{{}_{KL}}(P(Z|Y)\|P(Z))\] \[=\sum_{Y\in[K]^{n}}P(Y)D_{{}_{KL}}(P(Z|Y)\|\sum_{Y^{\prime}\in[K] ^{n}}P(Z|Y^{\prime})P(Y^{\prime})).\]

Using the log sum inequality in Lemma 4, we further obtain that

\[I(\hbar,\mathcal{P}) =\sum_{Y\in[K]^{n}}P(Y)\sum_{Y^{\prime}\in[K]^{n}}P(Y^{\prime})D_ {{}_{KL}}(P(Z|Y)\|P(Z|Y^{\prime}))\] \[=\sum_{Y\in[K]^{n}}\sum_{Y^{\prime}\in[K]^{n}}P(Y)P(Y^{\prime}) \left\{\sum_{i=1}^{\bar{n}}\sum_{r=1}^{R}D_{{}_{KL}}(P(Z_{i}^{(r)}|Y)\|P(Z_{i} ^{(r)}|Y^{\prime}))\right\}\] \[=\sum_{Y\in[K]^{n}}\sum_{Y^{\prime}\in[K]^{n}}\sum_{i=1}^{\bar{n} }\sum_{r=1}^{R}P(Y)P(Y^{\prime})D_{{}_{KL}}\left(\tau_{i,Y_{*}}^{(r)}\|\tau_{ i,Y^{\prime}_{i}*}^{(r)}\right)\] \[=\sum_{i=1}^{\bar{n}}\sum_{r=1}^{R}\sum_{g=1}^{K}\sum_{g^{\prime} =1}^{K}P(Y_{i}=g)P(Y^{\prime}_{i}=g^{\prime})D_{{}_{KL}}\left(\tau_{i,g*}^{( r)}\|\tau_{i,g*}^{(r)}\right)\] \[=\sum_{i=1}^{\bar{n}}\sum_{r=1}^{R}\sum_{g=1}^{K}\sum_{g^{\prime} =1}^{K}\hbar_{i,g}h_{i,g^{\prime}}D_{{}_{KL}}\left(\tau_{i,g*}^{(r)}\|\tau_{i,g *}^{(r)}\right)\] \[\triangleq\bar{n}\overline{D}_{KL}(\bm{\hbar},\bm{\tau}).\]Combining the discussion above, we obtain that

\[\Re_{\text{Bayes}}\geq\frac{1}{\bar{n}}\left[1-\frac{\overline{D}_{KL}(\bm{\hbar}, \bm{\tau})+\frac{1}{\bar{n}}\log(2-\prod_{i=1}^{\bar{n}}\max_{k\in[K]}\hbar_{i, k})}{\left\{\sum_{i=1}^{\bar{n}}\log(\max_{k\in[K]}\hbar_{i,k})\right\}/\bar{n}} \right].\]

### Proof of Corollary 3

Proof.: For \(k\in[K],\ r\in[R]\), and consider the following set:

\[\mathcal{S}^{c}=\left\{\bm{\theta}:\cup_{k\in[K],r\in[R]}\Big{\{}\max_{\tilde {y}\in[K],\mathbf{x}\in\overline{\mathcal{D}}_{\mathbf{x}}}\big{|}f^{(k,r)}( \tilde{y}|\bm{\omega}^{(k,r)})-f_{0}^{(k,r)}(\tilde{y}|\bm{\omega}^{(k,r)}_{0} )\big{|}>M_{n}\epsilon_{n}\Big{\}}\right\},\]

where \(\bm{\theta}\), \(\epsilon_{n}\) and \(M_{n}\) satisfy the conditions in Theorem 1, \(\bm{\omega}^{(k,r)}\) and \(\bm{\omega}^{(k,r)}_{0}\) depend on \(\mathbf{x}\), and \(\overline{\mathcal{D}}_{\mathbf{x}}\) represents the set of observations of the instances in \(\overline{\mathcal{D}}\). Then, according to Theorem 1 and using the union bound, we have that

\[\Pi(\mathcal{S}^{c}|\overline{\mathcal{D}}_{0}) \leq\sum_{k\in[K],r\in[R]}\Pi\left\{\bm{\theta}:\max_{\tilde{y} \in[K],\mathbf{x}\in\overline{\mathcal{D}}_{\mathbf{x}}}\big{|}f^{(k,r)}( \tilde{y}|\bm{\omega}^{(k,r)})-f_{0}^{(k,r)}(\tilde{y}|\bm{\omega}^{(k,r)}_{0} )\big{|}>M_{n}\epsilon_{n}\Big{|}\overline{\mathcal{D}}_{0}\right\}\] \[\leq\sum_{k\in[K],r\in[R]}\Pi\left(\bm{\theta}:\|f^{(k,r)}-f_{0}^{ (k,r)})\|_{\text{TV}}>U_{9}M_{n}\epsilon_{n}\Big{|}\overline{\mathcal{D}}_{0}\right)\] \[\leq\sum_{k\in[K],r\in[R]}\Pi\left(\bm{\theta}:d(f^{(k,r)},f_{0}^ {(k,r)})>\sqrt{2}U_{9}M_{n}\epsilon_{n}\Big{|}\overline{\mathcal{D}}_{0} \right)\longrightarrow 0\]

in \(\mathbb{P}_{0}^{n}\) probability as \(n\rightarrow\infty\), where \(\|\cdot\|_{\text{TV}}\) denotes the total variation distance of the associated distributions as defined in Example 1, and \(U_{9}\) is a positive constant. Here, the second inequality is due to the definition of the total variation distance of discrete distributions and the fact that \(f(\mathbf{x})>\varsigma_{2}\) for \(\mathbf{x}\in\overline{\mathcal{D}}_{\mathbf{x}}\), and the third inequality follows the (i) in Lemma 5. Thus, \(\Pi(\mathcal{S}|\overline{\mathcal{D}}_{0})\longrightarrow 1\) in \(\mathbb{P}_{0}^{n}\) probability as \(n\rightarrow\infty\), where \(\mathcal{S}\) can be written as

\[\mathcal{S}=\left\{\bm{\theta}:\max_{l\in[K],\mathbf{x}\in\overline{\mathcal{ D}}_{\mathbf{x}}}\big{|}\overline{\tau}_{kl}^{(r)}(\mathbf{x})-\tau_{kl}^{(r)}( \mathbf{x})\big{|}\leq M_{n}\epsilon_{n},k\in[K],r\in[R]\right\}.\]

We now consider the Bayes risk \(\Re_{\text{Bayes}}\), which, according to the proof of Theorem 2, can be written as \(\Re_{\text{Bayes}}=\inf_{\overline{\mathcal{Y}}}\left\{\frac{1}{\bar{n}}\sum_ {i=1}^{\bar{n}}\sum_{g=1}^{K}\hbar_{i,g}\mathbb{P}(\overline{y}_{i}\neq g| \mathrm{y}_{i}=g;\bm{\tau})\right\}\). We first consider \(g=1\):

\[\mathbb{P}(\overline{y}_{i}\neq 1|\mathrm{y}_{i}=1;\bm{\tau})\] \[\leq\sum_{g^{\prime}=2}^{K}\mathbb{P}(\overline{y}_{i}=g^{\prime} |\mathrm{y}_{i}=1;\bm{\tau})\] \[\leq\sum_{g^{\prime}=2}^{K}\mathbb{P}\left\{\left(\frac{\hbar_{i, g^{\prime}}}{\Omega\hbar_{i,1}}\right)\prod_{r=1}^{R}\prod_{l=1}^{K}\left(\frac{ \overline{\tau}_{i,g^{\prime}l}^{(r)}}{\overline{\tau}_{i,1l}^{(r)}}\right)^{ \mathbf{1}(\mathrm{y}_{i}^{(r)}=l)}>1\bigg{|}\mathrm{y}_{i}=1;\bm{\tau}\right\}.\]

If \(\{\overline{\tau}_{k\ast}^{(r)}(\cdot)\}_{k\in[K],r\in[R]}\in\mathcal{S}\), using Taylor's expansion, we have that

\[\log\left\{\prod_{r=1}^{R}\prod_{l=1}^{K}\left(\frac{\tau_{i,1l} ^{(r)}}{\overline{\tau}_{i,1l}^{(r)}},\frac{\overline{\tau}_{i,g^{\prime}l}^{ (r)}}{\tau_{i,g^{\prime}l}^{(r)}}\right)^{\mathbf{1}(\mathrm{y}_{i}^{(r)}=l)}\right\}\] \[= \sum_{r=1}^{R}\sum_{k=1}^{K}\left(\log\frac{\overline{\tau}_{i,g^{ \prime}l}^{(r)}}{\overline{\tau}_{i,g^{\prime}l}^{(r)}}-\log\frac{\overline{ \tau}_{i,1l}^{(r)}}{\overline{\tau}_{i,1l}^{(r)}}\right)\mathbf{1}(\mathrm{y}_{i }^{(r)}=l)\leq\kappa M_{n}\epsilon_{n},\]for some constant \(\kappa>0\) and large enough \(n\). For any \(\epsilon>0\), we have that \(\kappa M_{n}\epsilon_{n}<\epsilon\) by taking \(M_{n}=\epsilon/(2\kappa M_{n}\epsilon_{n})\). Thus, we further obtain that

\[\mathbb{P}(\overline{y}_{i}\neq 1|y_{i}=1;\bm{\tau}) \leq\sum_{g^{\prime}=2}^{K}\mathbb{P}\left(\exp(\epsilon)\left( \frac{\hbar_{i,g^{\prime}}}{\Omega\hbar_{i,1}}\right)\prod_{r=1}^{R}\prod_{l= 1}^{K}\left(\frac{\tau_{i,g^{\prime}l}^{(r)}}{\tau_{i,1l}^{(r)}}\right)^{1( \mathcal{Y}_{i}^{(r)}=l)}>1\bigg{|}y_{i}=1;\bm{\tau}\right)\] \[\leq(K-1)\exp\left\{-RI_{\Omega}^{(1)}(\bm{\hbar}_{i},\bm{\tau}_ {i})+\epsilon\right\}\]

for \(\{\overline{\tau}_{k\star}^{(r)}(\cdot)\}_{k\in[K],r\in[R]}\in\mathcal{S}\), where in the second step, we use Markov's inequality and the definition of \(I_{\Omega}^{(1)}(\bm{\hbar}_{i},\bm{\tau}_{i})\). Similar results also hold for \(g=2,...,K\). Hence, according to Theorem 1, we have that

\[\Pi\left\{\overline{\bm{\tau}}:\Re_{\text{Bayes}}\leq\frac{K-1}{\bar{n}}\sum_ {i=1}^{\bar{n}}\sum_{g=1}^{K}\hbar_{i,g}e^{-RI_{\Omega}^{(g)}(\bm{\hbar}_{i}, \bm{\tau}_{i})+\epsilon}\bigg{|}\overline{\mathcal{D}}_{0}\right\}\geq\Pi( \mathcal{S}|\overline{\mathcal{D}}_{0}),\]

where \(\Pi(\mathcal{S}|\overline{\mathcal{D}}_{0})\longrightarrow 1\) in \(\mathbb{P}_{0}^{n}\) probability as \(n\rightarrow\infty\). Thus, the proof is completed.

## Appendix C Implementation details and additional experimental results

### Implementation details

Dataset description.We assess the effectiveness of our method on three image datasets with synthetic annotations, MNIST [47], CIFAR-10 [48], and CIFAR-100 [48], and two datasets with human annotations, CIFAR-10N [49] and LabelMe [50, 51]. MNIST consists of \(28\times 28\) grayscale images with 10 classes, containing 60,000 training images and 10,000 test images. CIFAR10 has 10 classes of \(32\times 32\times 3\) color images, with 50,000 images used for training and 10,000 for testing. CIFAR100 also consists of 50,000 training images and 10,000 test images, whose size is \(32\times 32\times 3\), but with 100 fine-grained classes. We further consider two additional datasets with human annotations, CIFAR-10N [49] and LabelMe [50, 51]. For each instance in CIFAR10, CIFAR-10N provides three independent human annotated noisy labels, with the aggregation of three noisy labels by majority voting being \(9.03\%\). LabelMe is an image classification dataset consists of 10,000 training images, 500 validation images, and 1,188 test images. For images in the training set, LabelMe has noisy and incomplete labels provided by a total of \(R=59\) annotators, with each image being labeled by an average of 2.547 annotations. For all the datasets except LabelMe, we leave out 10% of the training data as a noisy validation set.

Experiment setup.The network structure for the MNIST dataset is chosen to be Lenet-5 [52]. We choose ResNet-18 [2] for CIFAR-10 and CIFAR-10N, and ResNet-34 architecture [2] for CIFAR-100. As in [53], we employ the pretrained VGG-16 network, followed by a fully connected layer and a softmax output layer for the LabelMe dataset, using 50% dropout. We take the batch size to be 128 for all the datasets. For MNIST, we use the SGD optimizer with momentum 0.9, weight decay \(5\times 10^{-4}\), and an initial learning rate of \(10^{-2}\). The learning rate is divided by 10 at the 40th epoch, and we set 80 epochs in total, in which the first 20 epochs are used to warm up the model on the noisy dataset and to determine anchor points. For CIFAR10, CIFAR100, CIFAR10N and LabelMe, the Adam optimizer [77] is utilized with the weight decay \(5\times 10^{-4}\). For CIFAR10, CIFAR100 and CIFAR10N, the initial learning rate is set to be \(10^{-3}\), and the network is trained for 120, 150, and \(120\) epochs for CIFAR10, CIFAR100 and CIFAR10N, respectively, with the first 30 epochs used as the warm-up stage. The model is trained on LabelMe for 100 epochs, with an initial learning rate \(5\times 10^{-4}\) and first 20 epochs as the warm-up stage.

Baselines.We compare the proposed method with the following state-of-art methods: (1) CE (Clean), which trains the network with the standard cross entropy loss on the clean datasets; (2) CE (MV), which trains the network using the labels from majority voting; (3) CE (EM) [9], which obtains the aggregated labels utilizing the EM algorithm; (4) DoctorNet [54], which models the annotators individually and then learns averaging weights for combining them; (5) GCE [55], which generalizesthe mean absolute error and the cross entropy loss to combat errors in training labels; (6) Co-teaching [56], which trains two networks and cross-trains on instances with small loss values; (6) Co-teaching+ [57], which bridges the "Update by Disagreement" strategy with the Co-teaching method; (7) BLTM [17], which directly models the transition matrix from Bayes optimal labels to noisy labels and learns a classifier to predict Bayes optimal labels; (8) MBEM [11], which alternates in rounds between estimating annotator quality from disagreement with the current model and updating the model by optimizing the a loss function that accounts for the current estimate of worker quality; (9) CrowdLayer [53], which concatenates the classifier with multiple annotator-specific layers and simultaneously learns the parameters; (10) TraceReg [8], which uses a loss function similar to CrowdLayer [53], but adds a regularization to establish identifiability of the confusion matrices and the classifier; (11) Max-MIG [7], which jointly aggregates the noisy crowdsourced labels and trains the classifier; (12) CoNAL [58], which decomposes the annotation noise into common and individual confusions; (13) GeoCrowdNet (F) [13]; and (14) GeoCrowdNet (W) [13], which are two regularized variants of the coupled cross-entropy minimization to enhance the identifiability of the confusion matrices. Among these methods, GCE and Co-teaching are strong baselines dealing with single noisy label issue, and we adapt them to the multiple annotations setting by utilizing the majority vote labels for loss computation.

Implementation details.We first warm up the base models on the noisy dataset with majority vote labels, obtain the set of anchor points \(\overline{\mathcal{D}}_{0}\), and train the sparse Bayesian model on \(\overline{\mathcal{D}}_{0}\) by maximizing the log posterior distribution of network parameters and excluding non-informative parameters with low posterior inclusion probability. With the noise transition model trained, we then iteratively implement the label correction algorithm (10) and update the base models, where linearly increase the threshold \(\Omega_{t}\) in the training process. Specifically, let \(ER\) denoted the estimated average noise rate, and let \(r_{0}\) and \(r_{1}\) represent two prespecified constants with \(r_{1}\geq r_{0}>0\), which determine the magnitude of \(\Omega_{t}\) at the beginning and the end of the training process. In the \(t\)th epoch, we set the threshold \(\Omega_{t}\) to be \(\Omega_{t}=(1-ER)\cdot(r_{0}+t\cdot\frac{r_{1}-r_{0}}{T})\) in the experiment. The implementation procedure of the proposed method is summarized in Algorithm 1.

``` Input :Noisy training data \(\mathcal{D}=\{\mathbf{x}_{i},\tilde{\mathbf{y}}_{i}\}_{i=1}^{N}\);
1// Warm Up the Base Models\(h_{1}\)and\(h_{2}\); Collect Anchor Points\(\overline{\mathcal{D}}_{0}\)[26]
2// Sections 3.1 and 3.2: Train Sparse Transition Matrices
3Maximize the log posterior distribution (12) of network parameters and obtain the MAP \(\widehat{\boldsymbol{\theta}}\);
4Calculate the posterior inclusion probability \(\mathbb{P}(\gamma_{k}^{(j)}=1|\widehat{\boldsymbol{\theta}})\) using (13); if \(\mathbb{P}(\gamma_{k}^{(j)}=1|\widehat{\boldsymbol{\theta}})<0.5\), zero out the corresponding parameter;
5 Fine tune the sparse network and obtain the noise transition model \(f\);
6// Section 3.3: Pairwise Likelihood Ratio Test and Update the Base Models
7for\(epoch\)\(t=1,...,T\)do
8 Update the linearly increasing threshold \(\Omega_{t}\) for pairwise LRT;
9fore each instance\(\mathbf{x}_{i}\)do
10 Use \(f_{1}(\mathbf{x}_{i})\) as the prior of \(\mathbf{x}_{i}\); if \(\overline{\mathbf{y}}_{i}\) satisfies (10) with threshold \(\Omega_{t}\), put \(\{\mathbf{x}_{i},\overline{\mathbf{y}}_{i}\}\) in \(\overline{D}_{t,1}\);
11 Use \(f_{2}(\mathbf{x}_{i})\) as the prior of \(\mathbf{x}_{i}\); if \(\overline{\mathbf{y}}_{i}\) satisfies (10) with threshold \(\Omega_{t}\), put \(\{\mathbf{x}_{i},\overline{\mathbf{y}}_{i}\}\) in \(\overline{D}_{t,2}\);
12 endfor
13 Update \(f_{1}\) using \(\overline{D}_{t,2}\);
14 Update \(f_{2}\) using \(\overline{D}_{t,1}\);
15 endfor Output :\(h_{1}\), \(h_{2}\) and \(f\). ```

**Algorithm 1**Annotator-Specific Instance-Dependent Label Noise Learning via Sparse Bayesian Network and Pairwise LRT

[MISSING_PAGE_EMPTY:36]

Classification accuracy on MNIST.The test accuracy on CIFAR10, CIFAR100, CIFAR10, and LabelMe are provided in Section 5. Here we present the average test accuracy on the MNIST dataset in Table 2, where the two highest accuraries are bold faced. Clearly, that the proposed method achieves the best performance.

Hyperparameter analysis on the CIFAR100 dataset.We conduct sensitivity analyses about hyperparameters \(r_{0}\) and \(r_{1}\) on the CIFAR100 dataset, where we choose \(r_{0}\) from \(\{3,5,10,15,20\}\) and \(r_{1}\) from \(\{200,250,300\}\). As discussed in the implementation details in Section C.1, \(r_{0}\) and \(r_{1}\) determine the magnitude of the threshold \(\Omega_{t}\) at the beginning and the end of the training process, respectively. With higher values of \(r_{0}\) and \(r_{1}\), the accuracy of the corrected labels using algorithm (10) will increase accordingly, but the number of corrected labels will decrease. As shown in Table 3, with different choices of \(r_{0}\) and \(r_{1}\), the proposed method consistently outperforms all the compared methods.

Classification accuracy on CIFAR100 with varying number of annotators.Figure 5 shows the average accuracy on CIFAR100 with the number of annotators varying from 5 to 100, which further demonstrate the superiority of the proposed method under different settings.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & IDN-LOW & IDN-MID & IDN-HIGH \\ \hline CE (Clean) & \multicolumn{3}{c}{\(99.14_{\pm 0.10}\)} \\ \hline CE (MV) & \(98.59_{\pm 0.13}\) & \(97.97_{\pm 0.13}\) & \(96.60_{\pm 0.52}\) \\ CE (EM) [9] & \(98.49_{\pm 0.11}\) & \(75.84_{\pm 0.97}\) & \(96.78_{\pm 0.52}\) \\ DoctorNet [54] & \(98.17_{\pm 0.12}\) & \(97.36_{\pm 0.23}\) & \(95.32_{\pm 0.51}\) \\ GCE [55] & \(99.02_{\pm 0.15}\) & \(98.51_{\pm 0.24}\) & \(98.05_{\pm 0.42}\) \\ Co-teaching [56] & \(98.85_{\pm 0.11}\) & \(\textbf{98.61}_{\pm 0.18}\) & \(\textbf{98.23}_{\pm 0.21}\) \\ Co-teaching+[57] & \(98.64_{\pm 0.10}\) & \(98.33_{\pm 0.10}\) & \(\textbf{97.67}_{\pm 0.44}\) \\ BLTM [17] & \(98.69_{\pm 0.06}\) & \(98.11_{\pm 0.09}\) & \(96.40_{\pm 0.81}\) \\ MBEM [11] & \(98.66_{\pm 0.07}\) & \(98.24_{\pm 0.05}\) & \(97.46_{\pm 0.21}\) \\ CrowdLayer [53] & \(97.29_{\pm 0.41}\) & \(94.88_{\pm 0.92}\) & \(90.51_{\pm 2.47}\) \\ TraceReg [8] & \(98.68_{\pm 0.05}\) & \(97.96_{\pm 0.18}\) & \(96.70_{\pm 0.57}\) \\ Max-MIG [7] & \(98.62_{\pm 0.06}\) & \(97.97_{\pm 0.05}\) & \(96.46_{\pm 0.26}\) \\ CoNAL [58] & \(98.60_{\pm 0.09}\) & \(97.89_{\pm 0.06}\) & \(96.03_{\pm 0.73}\) \\ GeoCrowdNet (F) [13] & \(\textbf{98.98}_{\pm 0.02}\) & \(97.70_{\pm 0.71}\) & \(96.91_{\pm 0.94}\) \\ GeoCrowdNet (W) [13] & \(97.33_{\pm 0.13}\) & \(94.74_{\pm 0.67}\) & \(90.79_{\pm 0.97}\) \\ \hline Ours & \(\textbf{99.13}_{\pm 0.05}\) & \(\textbf{98.98}_{\pm 0.11}\) & \(\textbf{98.80}_{\pm 0.07}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average accuracy of learning MNIST dataset

Figure 4: The accuracy and the number of corrected labels using the label correction algorithm (10) on real-world noisy datasets. The error bar for standard deviation has been shaded.

Average estimation error.For synthetic noisy datasets CIFAR10 and CIFAR100, we compare the average estimation error of the proposed method with six competing methods, CrowdLayer [53], TraceReg [8], GeoCrowdNet (F) [13], GeoCrowdNet (W) [13], MBEM [11], and BLTM [17]. The definition of the average estimation error and the results on CIFAR10 with 5 annotators are given in the Ablation study of Section 5. In Figures 6 and 7, we respectively present the average estimation error on the validation set of CIFAR10 and CIFAR100 with varying numbers of annotators, where the results of CrowdLayer, TraceReg, GeoCrowdNet (F) and GeoCrowdNet (W) overlap in some subfigures of Figure 7. The proposed method outperforms all the compared methods with lower average estimation error for each annotator in most of the cases, further demonstrating the effectiveness of the proposed sparse Bayesian model.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & & IDN-LOW & IDN-MID & IDN-HIGH \\ \hline \(r_{0}=3\) & \(r_{1}=200\) & \(58.88_{\pm 1.18}\) & \(52.98_{\pm 1.02}\) & \(45.50_{\pm 1.30}\) \\  & \(r_{1}=250\) & \(58.19_{\pm 0.92}\) & \(52.04_{\pm 1.77}\) & \(46.28_{\pm 2.03}\) \\  & \(r_{1}=300\) & \(58.16_{\pm 0.57}\) & \(53.03_{\pm 0.82}\) & \(45.48_{\pm 1.97}\) \\ \hline \(r_{0}=5\) & \(r_{1}=200\) & \(59.01_{\pm 0.24}\) & \(52.75_{\pm 1.38}\) & \(45.76_{\pm 2.02}\) \\  & \(r_{1}=250\) & \(58.58_{\pm 0.62}\) & \(52.76_{\pm 0.99}\) & \(46.59_{\pm 1.23}\) \\  & \(r_{1}=300\) & \(59.34_{\pm 0.66}\) & \(52.88_{\pm 1.68}\) & \(47.04_{\pm 1.74}\) \\ \hline \(r_{0}=10\) & \(r_{1}=200\) & \(59.39_{\pm 0.44}\) & \(53.23_{\pm 1.12}\) & \(47.10_{\pm 1.78}\) \\  & \(r_{1}=250\) & \(59.75_{\pm 0.74}\) & \(53.74_{\pm 0.54}\) & \(47.27_{\pm 1.79}\) \\  & \(r_{1}=300\) & \(59.06_{\pm 0.94}\) & \(53.73_{\pm 0.93}\) & \(48.09_{\pm 1.47}\) \\ \hline \(r_{0}=15\) & \(r_{1}=200\) & \(59.43_{\pm 0.56}\) & \(54.46_{\pm 0.37}\) & \(47.40_{\pm 1.23}\) \\  & \(r_{1}=250\) & \(59.38_{\pm 1.22}\) & \(54.48_{\pm 0.87}\) & \(48.59_{\pm 0.90}\) \\  & \(r_{1}=300\) & \(59.36_{\pm 0.33}\) & \(54.93_{\pm 0.63}\) & \(48.29_{\pm 1.85}\) \\ \hline \(r_{0}=20\) & \(r_{1}=200\) & \(59.81_{\pm 0.55}\) & \(54.88_{\pm 0.60}\) & \(49.44_{\pm 1.30}\) \\  & \(r_{1}=250\) & \(59.38_{\pm 0.62}\) & \(55.48_{\pm 0.75}\) & \(49.39_{\pm 1.48}\) \\  & \(r_{1}=300\) & \(60.14_{\pm 0.87}\) & \(55.10_{\pm 0.88}\) & \(49.07_{\pm 1.22}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average accuracy of learning the CIFAR100 dataset with different hyperparameters.

Figure 5: Average accuracy of learning CIFAR-100 dataset with varying number of annotators. The error bar for standard deviation has been shaded.

## 6 Conclusion

Figure 6: Average estimation error of annotator-specific instance-dependent noise transition matrices on CIFAR10. From the first to the fifth row, the number of annotators is 5, 10, 30, 50 and 100 respectively. Standard errors are represented by shaded regions.

## Appendix A

Figure 7: Average estimation error of annotator-specific instance-dependent noise transition matrices on CIFAR100. From the first to the fifth row, the number of annotators is 5, 10, 30, 50 and 100 respectively. Standard errors are represented by shaded regions.