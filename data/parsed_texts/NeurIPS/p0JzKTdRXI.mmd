# Efficient Experimentation for Estimation of Continuous and Discrete Conditional Treatment Effects

 Muhammed T. Razzak *

OATML Group

University of Oxford

&Panos Tigas *

OATML Group

University of Oxford

&Andrew Jesson

Blei Lab

Columbia University

&Yarin Gal

OATML Group

University of Oxford

&Uri Shalit

Technion - Israel Institute of Technology

###### Abstract

Accurately estimating personalized treatment effects often demands substantial data, incurring high costs across diverse applications such as personalized advertisement delivery and clinical trials. Existing methodologies employ deep models to estimate treatment effects in high-dimensional data, often relying on randomly selected experiments. We explore the potential of active learning techniques to enhance the efficiency of experimentation. Our focus centers on a relatively underexplored yet common scenario where each unit is subject to experimentation only once. We build upon the Bayesian active learning framework, to select units, and a treatment to apply to the unit, that maximize the information gain from each experiment. Our approach is flexible, accommodating both discrete and continuous treatment settings. Furthermore, we address the inefficiencies in batch experimentation by employing a greedy and a policy gradient-based optimization strategy. We validate the effectiveness of our proposed method on synthetic and high-dimensional semi-synthetic datasets (based on IHDP and TCGA). Our results show significant improvements in experimentation efficiency over the baseline methods.

## 1 Introduction

Experimentation often incurs significant costs in terms of resources, time, and funding. This necessitates the development of more efficient and targeted experimental designs. Adaptive trial design is a promising paradigm for conducting experiments, allowing more efficient use of scarce resources (Foster et al., 2020). Given a pool of samples and a set of (possibly continuous) interventions or treatments we wish to study, our goal is to construct an experiment that yields the most informative results about the effects of the interventions on a predefined outcome, while minimizing costs and resources used.

The majority of experimental design literature has primarily focused on a setting in which one may conduct experiments on the same object multiple times, usually with differing actions (also known as treatments, or interventions) (Rainforth, 2017). A somewhat less explored scenario is where one can actively select a unit from a pool of available units, and then select only a _single_ action to apply to this unit. This scenario has been referred to as active learning for trials (Deng et al., 2011) or adaptive trials (Chen et al., 2021). This scenario is relevant in clinical trials, animal studies, and any otherscenario where the units are heterogeneous and can change after being subjected to the intervention, or can only be treated once for ethical or cost reasons.

Our paper makes several contributions to the field of active learning for experimentation:

1. We propose a new setting for active learning in which the objective is to efficiently select the most informative unit-treatment combinations for experimentation (Section 2).
2. We utilize deep learning methods enabling us to model units with high-dimensional covariates and explore a wide range of treatment options (binary, discrete, and continuous options). We select unit-treatment combinations with the highest Expected Information Gain by introducing both a greedy algorithm and novel gradient-based algorithm to optimize the joint mutual information between all experiments selected in each batch (Section 3).
3. We demonstrate the efficacy of our methods on synthetic and high-dimensional semi-synthetic datasets, resulting in significant improvements in efficiency (Section 4).

## 2 Problem Formulation

Within the Neyman-Rubin causal model (Rubin, 1974), each unit \(\mathrm{u}\) has a potential outcome \(\mathrm{Y}^{\mathrm{t}}(\mathrm{u})\) associated with every intervention \(\mathrm{t}\in\mathcal{T}\), where \(\mathcal{T}\) is the (possibly infinite) set of interventions under consideration. We are interested in efficiently learning the conditional average potential outcome (CAPO) function, \(\mathbb{E}[\mathrm{Y}^{\mathrm{t}}\mid\mathbf{X}=\mathbf{x}]\). In the regime of continuous interventions, this is often referred to as the (conditional) "dose response function."

The expectation of the potential outcome given a set of covariates \(\mathbf{X}=\mathbf{x}\) describing \(\mathrm{u}\) is identifiable from data where \(\mathrm{T}\) is directly intervened on, assuming that \(\mathbf{X}\) and \(\mathrm{T}\) are causal parents of \(\mathrm{Y}\), that the observed outcome corresponds to the potential outcome of the assigned treatment, and that the treatment assigned to one unit, \(\mathrm{u}\), does not effect the outcome observed for any other unit, \(\mathrm{u}^{\prime}\). Under these conditions, the CAPO takes the form of the expectation

\[\mu(\mathbf{x},\mathrm{t})\coloneqq\mathbb{E}\left[\mathrm{Y}\mid\mathbf{X}= \mathbf{x},\mathrm{T}=\mathrm{t}\right].\] (1)

We are specifically interested in a \(k\)-round, hybrid, "pool based" setting, consisting of a non-replenishable pool of units, \(\mathrm{U}=\{\mathrm{u}_{i}\}_{i=1}^{k}\subseteq\mathcal{U}\), and a set of possible interventions \(T\subseteq\mathcal{T}\). This is a common setting, where experimenters will do sequential experiments either in batch or non-batch setting and have access to each unit once. A common example is preclinical trials, done in stages on animals to study safety, efficacy, and biological activity of a drug. Each unit, described by high-dimensional covariates, is unique and non-replenishable.

We consider both discrete and continuous intervention spaces, \(\mathcal{T}\). At each round \(i\in[k]\), the experimenter uses a policy, \(\pi:\mathcal{U}\times\mathcal{T}\rightarrow\mathcal{S}\subseteq\mathcal{X} \times\mathcal{T}\), to select a \(b\)-sized batch of unit-intervention pairs, \(s=\{\mathbf{X}(\mathrm{u}_{j})=\mathbf{x}_{j},\mathrm{t}_{j}\}_{i=j}^{b}\). The experiments are then run and the experimenter observes the outcomes, \(\{\mathrm{y}_{j}\}_{j=1}^{b}\), for each.

At the end of each round, the experimenter fits an estimator, \(\mu(\mathbf{x},\mathrm{t};\boldsymbol{\theta})\), of the CAPO, \(\mu(\mathbf{x},\mathrm{t})\), to the data accumulated, \(\mathcal{D}_{\mathrm{train}}=\cup_{i=1}^{k}\left\{\left\{\mathbf{x}_{j}, \mathrm{t}_{j},\mathrm{y}_{j}\right\}_{j=1}^{b}\right\}_{i}\).

## 3 Method

This section outlines our proposed selection policies which aim to maximize the Expected Information Gain over a set of experiment designs in a batch of size \(b\), denoted by \(\xi_{i=1:b}=(\mathbf{x}_{i},\mathrm{t}_{i})\).

In contrast to traditional experimental design, we need to acquire units, \(\mathbf{x}\in\mathrm{U}\), **without replacement** and select the treatment, \(\mathrm{t}\in\mathcal{T}\), **with replacement**. Furthermore, while units are discrete and drawn from a pool set, treatments can be discrete or continuous.

The problem is defined as selecting a batch of experiments that maximize the EIG objective:

\[\{(\mathbf{x},\mathrm{t})_{1:b}\}^{*}=\operatorname*{arg\,max}_{\{(\mathbf{x },\mathrm{t})_{1:b}\}}\text{EIG}(\{(\mathbf{x},\mathrm{t})_{1:b}\})\] (2)

where the utility set function is defined as the EIG objective \(\text{EIG}(\cdot)\triangleq I(Y;\boldsymbol{\Theta}\mid\cdot,\mathcal{D}_{ \mathrm{pool}})\).

We propose several acquisition policies for both discrete and continuous treatment settings:

Top-K.This strategy selects the top-k design with the highest estimated EIG per design. However, this does not maximize the EIG of the batch, as it does not take into account how other experiments in the batch affect the mutual information of each individual score. This can result in redundant experiments being conducted and results in inefficient experimentation (Foster et al., 2021; Kirsch et al., 2019).

Softmax Top-K.Kirsch et al. (2022) note this inefficiency and propose sampling the top-k experiments from a Softmax distribution, with temperature \(\beta\), to the EIG scores. This selection policy has shown superior performance in terms of efficiency. It is hypothesized that this is due to the perturbation of the distribution taking into account that the mutual information changes when other experiments are selected.

Greedy Optimization.When the set function we wish to optimize is known to be submodular and non-monotonic, then a greedy optimization-based algorithm can be used to maximize the set, which enjoys \(1-\frac{1}{\epsilon}\) approximation guarantees (Nemhauser et al., 1978). As shown in (Tigas et al., 2022; Kirsch et al., 2019), Expected Information Gain over continuous outcomes (i.e. regression problems) is both submodular and non-monotonic thus it is suitable for applying a Greedy optimization-based algorithm.

Policy-Gradient Based Optimization.To alleviate the approximation shortcomings of the greedy optimization-based algorithm, we notice that the objectives are differentiable with respect to the trial parameters (units and dosages), thus we can employ gradient-based methods to design experiments that maximize our objectives. We parametrize the policy \(\pi_{\phi}(X,T)\) as joint distribution over units \(X\) and dosages \(T\).

We provide a more details on each method in Appendix B.

## 4 Experiments

We evaluate our acquisition policies on synthetic and semi-synthetic datasets, and show significant improvements over the baseline.

Datasets.We evaluate on 3 datasets: a synthetic dataset, and two semi-synthetic datasets based on the features/covariates in the IHDP (Hill, 2011; Shalit et al., 2017) and TCGA datasets (Cancer Genome Atlas Research Network et al., 2013). For the **synthetic** dataset, suitable for discrete and continuous treatments, we develop a dose-response function based on the generalized dose response function developed in Taleb & West (2023). The covariates of the units are 1-dimensional and normally distributed, with the treatment being either a continuous range or discrete values. For the two semi-synthetic datasets, we obtain covariates from two standard benchmark datasets in causal inference, IHDP and TCGA. The units in **IHDP** dataset contain 25 features, while the units in the **TCGA** dataset contain 4000 features. Further details are provided in Appendix C.

Model.Our objectives rely on methods that are capable of modelling uncertainty and handling high-dimensional data modalities. For this we rely upon Deep Bayesian Neural Networks (BNNs). We utilize Deep Ensembles (Lakshminarayanan et al., 2017) which can be seen as approximate Bayesian Inference (Wilson & Izmailov, 2020). We utilize a simple multi-layer perceptron S-learner (Rumelhart et al., 1986; Kunzel et al., 2019). We provide the hyperparameters in Appendix D.

Metrics.The Mean Integrated Square Error (MISE) measures how well the model estimates the conditional average potential outcome (CAPO) across the entire dosage space:

\[\text{MISE}=\frac{1}{N}\frac{1}{k}\sum_{\mathrm{u}\in\mathrm{U}} \sum_{i=1}^{N}\int_{\mathcal{D}_{\mathrm{t}}}\Big{(}y^{i}(\mathrm{u},t)-\hat{ y}^{i}(\mathrm{u},t)\Big{)}^{2}\text{d}t\,.\] (3)

The metric is computed over a held-out test-set. This metric reflects our objective: to efficiently build an understanding of the entire unit-treatment-response function, through as few experiments as necessary.

Results.Table 1 shows the MISE for proposed acquisition functions on the continuous synthetic dataset, for various acquisition sizes and number of experiments conducted. We provide further discussion and results in Appendix A.

Our experiments demonstrate several key findings:

EIG optimization Enhances Covariate Coverage and Counterfactual Estimation.We investigate how designing experiments for high EIG changes the covariate and treatment training density, on the discrete synthetic dataset. As illustrated in Figure 1 (in A.1), the use of the EIG objective resulted in a more strategic distribution of the covariate space, with a higher density in regions where the outcome is more uncertain or variable. In the treatment space, we find a level of overlap that allows for more accurate estimation of the counterfactuals (Jesson et al., 2021). This improved coverage and estimation leads to a reduction in MISE, as evidenced by the more accurate covariate-response curves shown in the figure.

Synthetic Dataset Performance.The Greedy and Soft Top-K acquisition functions demonstrate strong performance on the discrete synthetic dataset. Notably, the Greedy method consistently achieves the lowest MISE values across various acquisition sizes and numbers of experiments conducted, making it the best-performing method for discrete treatments. When comparing the performance on the continuous synthetic dataset to the discrete synthetic dataset, we observe that the MISE values are generally lower for the discrete case. The policy gradient method begins to outperform the other acquisition functions as the acquisition size increases.

Policy-gradient offers robust and scalable performance on high-dimensional data.We evaluate our acquisition functions' performance on two semi-synthetic datasets: IHDP (Table 3 and TCGA (Table 4 in Appendix A). These datasets are designed to test the efficiency of our methods in high-dimensional settings, the primary focus of our work. The Greedy and Policy methods consistently achieve the lowest MISE values. For instance, on the IHDP dataset, Greedy achieves a MISE value of 0.1107 for the smallest acquisition size (4) when conducting 192 experiments, outperforming the random baseline by **27.1%**. The Policy method performs well, particularly for larger acquisition sizes and a higher number of experiments conducted.

The results on these datasets demonstrate that our methods and acquisition functions are scalable, versatile, and effective in real-world scenarios. The greedy and policy-gradient acquisition functions, in combination with deep learning for effective representation learning, provide an effective framework for efficient experimentation and estimation of conditional treatment effects.

## 5 Conclusion

In this paper, we have proposed a novel framework for efficiently estimating treatment effects. We provided theoretical justification for our information-theoretic based objective functions, along with our combinatorial optimization. We demonstrated our methods on both synthetic and semi-synthetic datasets, showing significant gains in efficiency over the standard baseline. Future work could include extending our methods to handle larger batch sizes, and incorporating additional objectives for additional utility.

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c} \hline \hline \multicolumn{2}{c|}{Acquisition Size} & \multicolumn{2}{c|}{4} & \multicolumn{2}{c|}{8} & \multicolumn{2}{c}{16} \\ \hline Experiment & Combined & 34 & 24 & 26 & 42 & 32 & 42 & 32 & 34 \\ \hline Random & 0.2796\(\pm\)0.0208 & 0.1814\(\pm\)0.015 & 0.1197\(\pm\)0.010 & 0.2551\(\pm\)0.0075 & 0.1794\(\pm\)0.0156 & 0.1842\(\pm\)0.0085 & 0.259\(\pm\)0.0152 & 0.180\(\pm\)0.0064 & 0.1416\(\pm\)0.0108 \\ Soft Top-K & 0.2727\(\pm\)0.0247 & 0.0033\(\pm\)0.0185 & 0.0189\(\pm\)0.0448 & 0.2724\(\pm\)0.0110 & 0.1153\(\pm\)0.0014 & 0.0684\(\pm\)0.0059 & **0.0442\(\pm\)0.0091** & 0.457\(\pm\)0.1153 & 0.150\(\pm\)0.0075 & 0.047\(\pm\)0.0113 \\ Greedy & 0.1722\(\pm\)0.0061 & 0.0077\(\pm\)0.0166 & 0.0047\(\pm\)0.0035 & **0.0433\(\pm\)0.037** & 0.0044\(\pm\)0.0072 & 0.0041\(\pm\)0.0012 & 0.147\(\pm\)0.0032 & 0.175\(\pm\)0.0036 & 0.175\(\pm\)0.0035 & 0.0159\(\pm\)0.0170 \\ Policy & **0.1833\(\pm\)0.0032** & **0.0077\(\pm\)0.0015** & 0.0045\(\pm\)0.0009 & 0.2208\(\pm\)0.0166 & **0.0032\(\pm\)0.0021** & 0.0473\(\pm\)0.0036 & 0.2906\(\pm\)0.1397 & **0.0071\(\pm\)0.0045** & **0.0001\(\pm\)0.0025** \\ \hline \hline \end{tabular}
\end{table}
Table 1: MISE for proposed acquisition functions on the continuous synthetic dataset, for various acquisition sizes and number of experiments conducted.

\begin{table}
\begin{tabular}{c|c c c|c c c|c c c} \hline \multicolumn{2}{c|}{Acquisition Size} & \multicolumn{2}{c|}{4} & \multicolumn{2}{c|}{16} & \multicolumn{2}{c}{16} \\ \hline Experiment & Combined & 34 & 24 & 26 & 42 & 32 & 42 & 34 & 26 \\ \hline Random & 0.2796\(\pm\)0.0208 & 0.1814\(\pm\)0.015 & 0.1197\(\pm\)0.010 & 0.2551\(\pm\)0.0075 & 0.1794\(\pm\)0.0156 & 0.1842\(\pm\)0.0085 & 0.259\(\pm\)0.0152 & 0.180\

## References

* Agrawal and Goyal (2013) Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In _International conference on machine learning_, pp. 127-135. PMLR, 2013.
* Atan et al. (2019) Onur Atan, William R. Zame, and Mihaela van der Schaar. Sequential patient recruitment and allocation for adaptive clinical trials. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pp. 1891-1900. PMLR, 16-18 Apr 2019. URL https://proceedings.mlr.press/v89/atan19a.html.
* Auer (2002) Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. _Journal of Machine Learning Research_, 3(Nov):397-422, 2002.
* Atlas Research Network et al. (2013) Cancer Genome Atlas Research Network, J N Weinstein, E A Collisson, G B Mills, K R Shaw, B A Ozenberger, K Ellrott, I Shmulevich, C Sander, and J M Stuart. The cancer genome atlas pan-cancer analysis project. _Nat Genet_, 45(10):1113-1120, October 2013. doi: 10.1038/ng.2764. URL https://www.ncbi.nlm.nih.gov/pubmed/24071849?dopt=Abstract.
* Char et al. (2019) Ian Char, Youngseog Chung, Willie Neiswanger, Kirthevasan Kandasamy, Andrew O Nelson, Mark Boyer, Egemen Kolemen, and Jeff Schneider. Offline contextual bayesian optimization. _Advances in Neural Information Processing Systems_, 32, 2019.
* Chen et al. (2021) Yanzhi Chen, Dinghuai Zhang, Michael U. Gutmann, Aaron Courville, and Zhanxing Zhu. Neural approximate sufficient statistics for implicit models. In _International Conference on Learning Representations_, 2021.
* Chu et al. (2011) Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pp. 208-214. JMLR Workshop and Conference Proceedings, 2011.
* Deng et al. (2011) Kun Deng, Joelle Pineau, and Susan Murphy. Active learning for personalizing treatment. In _2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)_, pp. 32-39. IEEE, 2011.
* Foster et al. (2020) Adam Foster, Martin Jankowiak, Matthew O'Meara, Yee Whye Teh, and Tom Rainforth. A unified stochastic gradient approach to designing bayesian-optimal experiments. In _International Conference on Artificial Intelligence and Statistics_, pp. 2959-2969. PMLR, 2020.
* Foster et al. (2021) Adam Foster, Desi R Ivanova, Ilyas Malik, and Tom Rainforth. Deep adaptive design: Amortizing sequential bayesian experimental design. _Proceedings of the 38th International Conference on Machine Learning (ICML), PMLR 139_, 2021.
* Gal et al. (2017) Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data, 2017.
* Ginsbourger et al. (2014) David Ginsbourger, Jean Baccou, Clement Chevalier, Frederic Perales, Nicolas Garland, and Yann Monerie. Bayesian adaptive reconstruction of profile optima and optimizers. _SIAM/ASA Journal on Uncertainty Quantification_, 2(1):490-510, 2014.
* Han et al. (2020) Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W Glynn, and Yinyu Ye. Sequential batch learning in finite-action linear contextual bandits. _arXiv preprint arXiv:2004.06321_, 2020.
* Hill (2011) Jennifer L Hill. Bayesian nonparametric modeling for causal inference. _Journal of Computational and Graphical Statistics_, 20(1):217-240, 2011.
* Ivanova et al. (2021) Desi R Ivanova, Adam Foster, Steven Kleinegesse, Michael Gutmann, and Tom Rainforth. Implicit Deep Adaptive Design: Policy-Based Experimental Design without Likelihoods. In _Advances in Neural Information Processing Systems_, volume 34, pp. 25785-25798. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/d811406316b669ad3d370d78b51b1d2e-Paper.pdf.
* Krizhevsky et al. (2014)Desi R. Ivanova, Joel Jennings, Cheng Zhang, and Adam Foster. Efficient real-world testing of causal decision making via bayesian experimental design for contextual optimisation, 2022. URL https://arxiv.org/abs/2207.05250.
* Jesson et al. (2021) Andrew Jesson, Panagiotis Tigas, Joost van Amersfoort, Andreas Kirsch, Uri Shalit, and Yarin Gal. Causal-bald: Deep bayesian active learning of outcomes to infer treatment-effects from observational data. _Advances in Neural Information Processing Systems_, 34, 2021.
* Kirsch et al. (2019) Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. _Advances in neural information processing systems_, 32:7026-7037, 2019.
* Kirsch et al. (2022) Andreas Kirsch, Sebastian Farquhar, Parmida Atighehchian, Andrew Jesson, Frederic Branchaud-Charron, and Yarin Gal. Stochastic batch acquisition for deep active learning, 2022.
* Kleinegesse and Gutmann (2020) Steven Kleinegesse and Michael Gutmann. Bayesian experimental design for implicit models by mutual information neural estimation. In _Proceedings of the 37th International Conference on Machine Learning_, Proceedings of Machine Learning Research, pp. 5316-5326. PMLR, 2020.
* Krause and Ong (2011) Andreas Krause and Cheng Ong. Contextual gaussian process bandit optimization. _Advances in neural information processing systems_, 24, 2011.
* Kunzel et al. (2019) Soren R. Kunzel, Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. _Proceedings of the National Academy of Sciences_, 116(10):4156-4165, 2019. doi: 10.1073/pnas.1804597116.
* Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in Neural Information Processing Systems_, 30, 2017.
* Nemhauser et al. (1978) George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions--i. _Mathematical programming_, 14:265-294, 1978.
* Pearce et al. (2020) Michael Pearce, Janis Klaise, and Matthew Groves. Practical bayesian optimization of objectives with conditioning variables. _arXiv preprint arXiv:2002.09996_, 2020.
* Rainforth (2017) Tom Rainforth. _Automating Inference, Learning, and Design using Probabilistic Programming_. PhD thesis, University of Oxford, 2017.
* Rubin (1974) Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. _Journal of educational Psychology_, 66(5):688, 1974.
* Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. _nature_, 323(6088):533-536, 1986.
* Sainburg et al. (2020) Tim Sainburg, Leland McInnes, and Timothy Q. Gentner. Parametric umap: learning embeddings with deep neural networks for representation and semi-supervised learning. _ArXiv e-prints_, 2020.
* Schwab et al. (2020) Patrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M. Buhmann, and Walter Karlen. Learning counterfactual representations for estimating individual dose-response curves. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):5612-5619, apr 2020. doi: 10.1609/aaai.v34i04.6014. URL https://doi.org/10.1609%2Faaai.v34i04.6014.
* Shalit et al. (2017) Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In _International Conference on Machine Learning_, pp. 3076-3085. PMLR, 2017.
* Taleb and West (2023) Nassim Nicholas Taleb and Jeffrey West. Working with convex responses: Antifragility from finance to oncology. _Entropy_, 25(2), 2023. ISSN 1099-4300. doi: 10.3390/e25020343. URL https://www.mdpi.com/1099-4300/25/2/343.
* Tigas et al. (2022) Panagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Scholkopf, Yarin Gal, and Stefan Bauer. Interventions, where and how? experimental design for causal models at scale. _arXiv preprint arXiv:2203.02016_, 2022.
* Taleb et al. (2019)

[MISSING_PAGE_FAIL:7]

### TCGA Dataset Results

Table 4 shows the MISE for proposed acquisition functions on the TCGA dataset, for various acquisition sizes and number of experiments conducted.

### Discussion of Additional Results

The results on both IHDP and TCGA datasets demonstrate the effectiveness of our proposed methods in high-dimensional settings. For the IHDP dataset, we observe that the Greedy method performs particularly well for smaller acquisition sizes, while the Policy method shows strengths in larger acquisition sizes and with more experiments conducted.

On the TCGA dataset, which has a much higher dimensionality (4000 features), we see that both Soft Top-K and Policy methods outperform the random baseline consistently. The Policy method, in particular, shows robust performance across different acquisition sizes and number of experiments, highlighting its effectiveness in handling high-dimensional data.

These additional results further support our main findings that our proposed methods, especially the Policy-gradient based approach, offer robust and scalable performance on high-dimensional data, providing significant improvements over random experimentation in estimating conditional treatment effects.

## Appendix B Detailed Algorithm Descriptions

This section provides more detailed descriptions of the algorithms used in our study.

### Top-K Algorithm

The Top-K algorithm selects the top-k designs with the highest estimated Expected Information Gain (EIG) per design. Here's a more detailed description of the algorithm:

```
1for\(b=1\)to\(B\)do
2\(\mathbf{x}^{*},\mathrm{t}^{*}=\arg\max_{\mathbf{x}\in D_{pool},t\in T}\text{ EIG}(\{(\mathbf{x},\mathrm{t})\})\)
3 Add \((\mathbf{x}^{*},\mathrm{t}^{*})\) to the batch
4\(D_{pool}\gets D_{pool}\setminus\{\mathbf{x}^{*}\}\)\(\triangleright\)Removes selected unit from pool
5 end for ```

**Algorithm 1**Top-K batch selection

### Softmax Top-K Algorithm

The Softmax Top-K algorithm introduces stochasticity to the selection process by sampling from a softmax distribution of the EIG scores:

```
1forupdate step \(c=1\dots B\)do
2\(\mathbf{x}^{*},\mathrm{t}^{*}=\arg\max_{\mathbf{x}\in D_{pool},t\in T}\text{ EIG}(\{(\mathbf{x},\mathrm{t})\})+\epsilon\)\(\triangleright\)Select unit without replacement
3\(D_{pool}\gets D_{pool}-\mathbf{x}^{*}\) ```

**Algorithm 2**Softmax top-k batch selection

\begin{table}
\begin{tabular}{c|c c c c c c c c|c c c c c} \hline \multicolumn{1}{c|}{\multirow{2}{*}{
\begin{tabular}{c} Optimactic Size \\ \end{tabular} } & \multicolumn{1}{c|}{\(\mathbf{G}\)} & \multicolumn{1}{c|}{\(\mathbf{x}\)} & \multicolumn{1}{c|}{\(\mathbf{x}\)} & \multicolumn{1}{c|}{\(\mathbf{x}\)} & \multicolumn{1}{c|}{\(\mathbf{x}\)} & \multicolumn{1}{c|}{\(\mathbf{x}\)} & \multicolumn{1}{c|}{\(\mathbf{x}\)} & \multicolumn{1}{c|}{\(\mathbf{x}\)} \\ \hline Random & 0.7208 \(\pm\) 0.0032 & 0.1688 \(\pm\) 0.0054 & 0.7108 \(\pm\) 0.0166 & 0.778 \(\pm\) 0.0256 & 0.1688 \(\pm\) 0.0125 & 0.1748 \(\pm\) 0.0057 & 0.3194 \(\pm\) 0.0052 & 0.1532 \(\pm\) 0.0030 \\ Soft Top-K & 0.2040 \(\pm\) 0.0175 & 0.1415 \(\pm\) 0.0064 & **0.1106 \(\pm\) 0.0058** & **0.2626 \(\pm\) 0.0185** & 0.1648 \(\pm\) 0.0116 & **0.3194 \(\pm\) 0.004** & 0.2756 \(\pm\) 0.0023 & 0.1725 \(\pm\) 0.0009 & 0.1129 \(\pm\) 0.0045 \\ Policy & **0.257 \(\pm\) 0.0016** & **0.1303 \(\pm\) 0.0149** & 0.1234 \(\pm\) 0.0004 & 0.2686 \(\pm\) 0.0263 & **0.1654 \(\pm\) 0.005** & 0.1554 \(\pm\) 0.0047 & **0.2514 \(\pm\) 0.0078** & **0.1544 \(\pm\) 0.0063** & **0.1543 \(\pm\) 0.0086** \\ \hline \end{tabular}
\end{table}
Table 4: MISE for proposed acquisition functions on the TCGA dataset, for various acquisition sizes and number of experiments conducted.

### Greedy Optimization Algorithm

The Greedy Optimization algorithm iteratively selects the experiment that maximizes the marginal gain in EIG:

```
1\(A\leftarrow\emptyset\)
2forupdate step \(c=1\dots B\)do
3\(\mathbf{x}^{*},\mathrm{t}^{*}=\arg\max_{\mathbf{x}\in D_{pool},t\in T}\text{ EIG}(A\cup\{(\mathbf{x},\mathrm{t})\})\)
4\(A\gets A\cup\{\mathbf{x}^{*},\mathrm{t}^{*}\}\)\(\triangleright\) Select unit without replacement
5\(D_{pool}\gets D_{pool}-\mathbf{x}^{*}\) ```

**Algorithm 3**\(1-\frac{1}{\epsilon}\) Greedy batch selection

### Policy-Gradient Based Optimization Algorithm

The Policy-Gradient Based Optimization algorithm uses gradient ascent to optimize a parameterized policy for selecting experiments:

```
1Initialize policy parameters \(\phi_{x}\) for units and \(\phi_{t}\) for treatments
2for\(c=1\)to\(C\)do
3 Sample batch \(\{(\mathbf{x},\mathrm{t})_{1:B}\}\sim\pi_{\phi}(X,T)\)
4 Calculate \(\text{EIG}(\{(\mathbf{x},\mathrm{t})_{1:B}\})\)
5 Update \(\phi_{x}\leftarrow\phi_{x}+\alpha_{x}\nabla_{\phi_{x}}\text{EIG}(\{(\mathbf{x}, \mathrm{t})_{1:B}\})\)
6 Update \(\phi_{t}\leftarrow\phi_{t}+\alpha_{t}\nabla_{\phi_{t}}\text{EIG}(\{(\mathbf{x}, \mathrm{t})_{1:B}\})\)
7
8 end for
9 Sample final batch \(\{(\mathbf{x}^{*},\mathrm{t}^{*})_{1:B}\}\sim\pi_{\phi}(X,T)\) ```

**Algorithm 4**Policy-Gradient Based Optimization batch selection

### Expected Information Gain (EIG) Calculation

The Expected Information Gain is calculated using a Monte Carlo estimation:

\[\text{EIG}(\{(\mathbf{x},\mathrm{t})_{1:B}\})\approx\frac{1}{M}\sum_{m=1}^{M} \log\frac{p(y_{m}|\mathbf{x},\mathrm{t},\theta_{m})}{\frac{1}{N}\sum_{n=1}^{N} p(y_{m}|\mathbf{x},\mathrm{t},\theta_{n})}\] (4)

where \(\theta_{m}\sim p(\theta|\mathcal{D})\) are samples from the posterior distribution over model parameters, \(y_{m}\sim p(y|\mathbf{x},\mathrm{t},\theta_{m})\) are samples from the predictive distribution, and \(M\) and \(N\) are the number of Monte Carlo samples used.

These algorithms form the core of our approach to efficient experimentation for estimating conditional treatment effects. Each algorithm offers different trade-offs between computational complexity and the quality of selected experiments, as discussed in the main text.

## Appendix C Datasets

### Synthetic Dataset

The synthetic dataset is designed to be suitable for both discrete and continuous treatments. We develop a dose-response function based on the generalized dose response function introduced in Taleb & West (2023).

The **covariate**\(x\) is generated from a normal distribution reflecting some underlying continuous variable:

\[x\sim N(70,20)\]

**Treatments \(t\)** are continuous and normally distributed:

\[t\sim N(15,5)\]

The **outcome \(Y\)** for each unit is computed through a response function that depends on both the covariate \(x\) and treatment \(t\). The function is defined as:

\[Y=\min(f_{1},f_{2})\]

where

\[f_{1}=\frac{1}{1+e^{-t+\text{offset}-5}}\]

\[f_{2}=-\frac{2}{1+e^{(-t+\text{offset}+2)\cdot 3}}+1\]

and

\[\text{offset}=\frac{x-20}{4}\]

We standardize the covariates, treatments, and output to enable the model to learn the function more easily.

For the discrete version of this dataset, we simply choose two values for \(t\), \(10\) and \(20\), and use the above dose-response function.

### IHDP Semi-Synthetic Dataset

The Infant Health and Development Program (IHDP) is a semi-synthetic dataset (Shalit et al., 2017) commonly used in literature to study the performance of causal effect estimation methods. Each unit is represented by 25 covariates describing different aspects of the infants and their mothers.

The **covariate \(x\)** is a normalized 25-dimensional feature vector derived from the IHDP dataset. We produce a response function from it by projecting it into a 1-D space, using the parametric Uniform Manifold Approximation and Projection (UMAP) Sainburg et al. (2020) algorithm. We then normalize it to a normal distribution reflecting the following distribution:

\[x\sim N(70,20)\]

**Treatments \(t\)** are continuous and normally distributed:

\[t\sim N(15,5)\]

Figure 2: The dose response from which our datasets are constructed, visualized for 3 different covariate values.

The **outcome**\(Y\) for each unit is computed using the same response function as in the synthetic dataset.

This semi-synthetic dataset uses high-dimensional covariates seen by the model, but the underlying dose-response function is based on the 1-D projection.

### TCGA Semi-Synthetic Dataset

The TCGA dataset consists of gene expression measurements for cancer patients (Cancer Genome Atlas Research Network et al., 2013). There are 9659 samples for which we used the measurements from the 4000 most variable genes. The gene expression data was log-normalized and each feature was scaled in the [0, 1] interval. For each patient, the features were scaled to have norm 1. We used the same version of the TCGA dataset as used by DRNet (Schwab et al., 2020).

The construction of this semi-synthetic dataset follows the construction of the IHDP Semi-Synthetic Dataset, except for the parametric UMAP algorithm reducing the dimensionality of the covariates from 4000 to 1, as an input to the dose-response function. Again, the model sees the high-dimensional covariates.

## Appendix D Training S-Learners

For all experiments, we use Deep Ensembles (Lakshminarayanan et al., 2017) as our model, which can be seen as an approximate Bayesian inference method (Wilson and Izmailov, 2020). The specific architectures and training details for each dataset are as follows:

### Synthetic Dataset

* MLP with 2 hidden layers and 512 hidden units
* Optimizer: Adam, with a learning rate of 0.001
* Deep ensemble of 10 MLPs
* Trained for a maximum of 200 epochs per round

### ITGA Dataset

* MLP with 3 hidden layers and 1024 hidden units
* Optimizer: Adam, with a learning rate of 0.0005
* Deep ensemble of 10 MLPs
* Trained for a maximum of 200 epochs per round

### ITGA Dataset

* MLP with 3 hidden layers and 1024 hidden units
* Optimizer: Adam, with a learning rate of 0.0005
* Deep ensemble of 10 MLPs
* Trained for a maximum of 200 epochs per round

## Appendix E Policy-Gradient Hyperparameters

The policy-gradient method requires two additional hyperparameters: learning rates for variables that describe the unit and treatment. We performed a simple sweep over the parameters and found that learning rates of 0.01 and 0.001 for the unit and treatment, respectively, are relatively robust. We optimize these parameters using Adam.

Compute Used

We used an internal cluster of 24 Nvidia Titan RTX cards for our experiments. The running time of individual experiments was relatively short, typically less than 4 hours each. However, an attempt to use the greedy algorithm on the TCGA dataset exceeded 14 hours before it was cancelled due to time constraints.

In total, we estimate that across 4 datasets (synthetic discrete and continuous, IHDP, and TCGA), 4 methods, 3 acquisition sizes, and 10 seeds, approximately 480 experiments were conducted for this paper. With an average running time of 2 hours, this amounts to approximately 960 GPU-hours, along with associated CPU usage.

## Appendix G Related Work

This section provides a more detailed overview of related work in the fields of Bayesian Optimisation, Bayesian Optimal Experimental Design, Contextual Bandits, and Active Learning.

### Bayesian Optimisation

Our setting is related to contextual Bayesian optimisation. Some of the more closely related methods include Profile Expected Improvement (Ginsbourger et al., 2014), Multi-task Thompson Sampling (Char et al., 2019), and conditional Bayesian optimization (Pearce et al., 2020). All of these methods are, however, restricted to GPs and the criteria they use to choose optimal designs are not information-based. Our work differs from this family of methods, in that we aim to estimate the entire treatment-effect relationship, rather than merely the optimal treatment. Furthermore, Bayesian Optimisation typically operates in the setting in which one can repeat experiments on the unit of interest, whereas our work assumes that each unit can only be experimented on once.

### Bayesian Optimal Experimental Design

Our objective of selecting units/treatments bears the greatest similarity to experimental design. Experimental design aims to select the most informative experiments to conduct, given a limited budget. The difference in setting between experimental design and our work is that experimental design assumes that one can repeat experiments on the same unit, whereas our work assumes that each unit can only be experimented on once. The most closely related to our EIG objective function in the context of sequential experimental design is Ivanova et al. (2022). However, Ivanova et al. (2022) are primarily interested in a secondary utility objective added in the EIG. Similar methods for obtaining variational EIG objectives have been used in implicit likelihood BED methods for _parameter learning_, but not for contextual optimisation. Gradient-based methods for large batch experimentation include SG-BOED (Foster et al., 2020) and MINEBED (Kleinegesse and Gutmann, 2020), while the policy-based iDAD (Ivanova et al., 2021) applies to batch and adaptive settings. Our ability to handle discrete designs is another important distinction of our framework.

### Contextual Bandits

Contextual bandits is another broad framework that our work is related to. An extensive line of research is focused on _online linear_ bandits and discrete actions chosen using (variations of) UCB, Thompson sampling or \(\epsilon\)-greedy strategy (Auer, 2002; Chu et al., 2011; Agrawal and Goyal, 2013; Han et al., 2020). Krause and Ong (2011) instead model the reward as a GP defined over the context-action space and develop CGP-UCB. More recently, Zanette et al. (2021) proposed designing a batch of experiments _offline_ to collect a good dataset from which to learn a policy.

### Active Learning

Deng et al. (2011) propose the use of Active Learning for recruiting patients to assign treatments that will reduce the uncertainty of an Individual Treatment Effect model. They focused on a small number of heterogeneous groups rather than individuals, with discrete treatments. As such, they tackle their setting through multi-armed bandits. While their objective remains the same as ours, in part our work could be seen as an extension of this work, with this work focusing on the individual level and operating in a setting with high-dimensional covariates. Atan et al. (2019) similarly focus on small heterogeneous groups with discrete treatments, and propose using Knowledge-Gradients to optimize the allocation process.

### Comparison of Methods

Table 5 provides a comparison between various Adaptive Trial Methods and Active Learning approaches, highlighting the unique features of our proposed method.

As shown in Table 5, our framework is uniquely positioned to handle non-replaceable units, high-dimensional covariates, both discrete and continuous treatments, and batch acquisition. This combination of features allows our approach to be more flexible and applicable to a wider range of experimental design scenarios compared to existing methods.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Methods** & **Non-replaceable Units** & **High-Dimensional Covariates** & **Discrete Treatments** & **Continuous Treatments** & **Batch Acquisition** \\ \hline Deng et al. (2011) & ✓ & & ✓ & \\ Atan et al. (2019) & ✓ & & ✓ & ✓ \\ \hline Gal et al. (2017) & ✓ & ✓ & & ✓ \\ Kirsch et al. (2019) & ✓ & ✓ & & ✓ \\ Jesson et al. (2021) & ✓ & ✓ & & ✓ \\ \hline
**Ours** & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison between various Adaptive Trial Methods and Active Learning.