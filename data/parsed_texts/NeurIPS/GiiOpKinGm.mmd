# Multi-Player Zero-Sum Markov Games with

Networked Separable Interactions

 Chanwoo Park

MIT

cpark97@mit.edu

&Kaiqing Zhang

University of Maryland, College Park

kaiqing@umd.edu

&Asuman Ozdaglar

MIT

asuman@mit.edu

Alphabetical order.

###### Abstract

We study a new class of Markov games, _(multi-player) zero-sum Markov Games_ with _Networked separable interactions_ (zero-sum NMGs), to model the local interaction structure in non-cooperative multi-agent sequential decision-making. We define a zero-sum NMG as a model where the payoffs of the auxiliary games associated with each state are zero-sum and have some separable (i.e., polymatrix) structure across the neighbors over some interaction network. We first identify the necessary and sufficient conditions under which an MG can be presented as a zero-sum NMG, and show that the set of Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash equilibrium (NE) in these games, in that the product of per-state marginalization of the former for all players yields the latter. Furthermore, we show that finding approximate Markov _stationary_ CCE in infinite-horizon discounted zero-sum NMGs is PPAD-hard, unless the underlying network has a "star topology". Then, we propose fictitious-play-type dynamics, the classical learning dynamics in normal-form games, for zero-sum NMGs, and establish convergence guarantees to Markov stationary NE under a star-shaped network structure. Finally, in light of the hardness result, we focus on computing a Markov _non-stationary_ NE and provide finite-iteration guarantees for a series of value-iteration-based algorithms. We also provide numerical experiments to corroborate our theoretical results.

## 1 Introduction

Nash Equilibrium (NE) has been broadly used as a solution concept in game theory, since the seminal works of [1, 2]. Perhaps equally important, NE is also deeply rooted in the prediction and analysis of _learning dynamics_ in multi-agent strategic environments: it may appear as a natural outcome of many non-equilibrating learning processes of multiple agents interacting with each other [3, 4]. A prominent example of such learning processes is fictitious play (FP) [5, 6], in which myopic agents estimate the opponents' play using history, and then choose a best-response action (based on their payoff matrix) against this estimate, as if the opponents use it as their stationary strategy. The focus of these studies has initially been on the convergence to NE in zero-sum games (see [5, 6], and also [7, 4]), and in games with aligned objective (identical-interest and potential games, see [8]). Since then, FP has been shown to converge to NE in more important classes of games, including \(2\mathrm{x}n\) games [9, 10], "one-against-all" games [11], and zero-sum polymatrix games [12], justifying the prediction power of NE in learning in normal-form/matrix games.

Some of these results have recently been extended to the _stochastic game_ (also known as the _Markov_ game (MG)) setting, a model for multi-agent sequential decision-making with state transition dynamics, first introduced in [13]. In particular, [14, 15, 16, 17, 18] have studied best-response-type learning dynamics in two-player zero-sum MGs, and [19, 17] have studied that in multi-playeridentical-interest games. Following the same path as studying matrix games, one natural question arises: _Are there other classes of MGs beyond two-player zero-sum and identical-interest cases that allow natural learning dynamics, e.g., fictitious play, to justify NE as the long-run emerging outcome?_

On the other end of the spectrum, it is well-known that for _general-sum_ normal-form games, the special case of MGs without the state transition dynamics, _computing_ an NE is intractable [20, 21]. Relaxed solution concepts as (coarse) correlated equilibrium ((C)CE) have thus been favored when it comes to equilibrium computation for general-sum, multi-player games [22, 3]. Encouragingly, when the interactions among players have some networked separable structure, also known as being _polymatrix_, computing NE may be made tractable even in multi-player settings. This has been instantiated in the seminal works [23, 24] in the normal-form game setting, which showed that any CCE collapses to the NE in such games when the payoffs are zero-sum. Thus, any algorithms that can efficiently compute the CCE in such games will lead to the efficient computation of the NE.

In fact, besides being of theoretical interest, multi-player zero-sum games with networked separable interactions also find a range of applications, including security games [24], fashion games [25, 26, 27], and resource allocation problems [28]. These examples, oftentimes, naturally involve some _state_ transition that captures the dynamics of the evolution of the environment in practice. For example, in the security game, the protection level or immunity of a target increases as a function of the number of past attacks, leading to a smaller probability of an attack on the target being successful. Hence, it is imperative to study such multi-player zero-sum games with state transitions. As eluded in the recent results [29, 30], such a transition from _stateless_ to _stateful_ cases may not always yield straightforward and expected results, e.g., computing stationary CCE can be computationally intractable in stochastic games, in stark contrast to the normal-form case where CCE can be efficiently computed. This naturally prompts another question: _Are there other types of (multi-player) MGs that may circumvent the computational hardness of computing NE/CCE?_

In an effort to address these two questions, we introduce a new class of Markov games - _(multi-player) zero-sum Markov games with networked separable interactions_ (zero-sum NMGs). We summarize our contributions as follows, and defer a more detailed literature review to Appendix A.

Contributions.First, we introduce a new class of non-cooperative Markov games: (multi-player) zero-sum MGs with Networked separable interactions (zero-sum NMGs), wherein the payoffs of the _auxiliary-games_ associated with each state, i.e., the sum of instantaneous reward and expectation of any estimated state-value functions, possess the multi-player zero-sum and networked separable (i.e., polymatrix) structure as in [31, 28, 23, 24] for normal-form games, a strict generalization of the latter. We also provide structural results on the reward and transition dynamics of the game, as well as examples of this class of games. Specifically, for a Markov game to qualify as a zero-sum NMG, if and only if its reward function has the zero-sum polymatrix structure, and its transition dynamics is an _ensemble_ of multiple single-controller transition dynamics that are sampled randomly at each state (see Remark 3 for more details). This transition dynamics covers the common ones in the MG literature, including the single-controller and turn-based dynamics. Second, we show that Markov CCE and Markov NE _collapse_ in that the product of per-state marginal distributions of the former yields the latter, making it sufficient to focus on the former in equilibrium computation. We then show the PPAD-hardness [32] of computing the _Markov stationary_ equilibrium, a natural solution concept in infinite-horizon discounted MGs, unless the underlying network has a star-topology. This is in contrast to the normal-form case where CCE is always computationally tractable. Third, we study the fictitious-play property [8] of zero-sum NMGs, showing that the fictitious-play dynamics [16, 19] converges to the Markov stationary NE, for zero-sum NMGs with a star-shaped network structure. Finally, in light of the hardness of computing stationary equilibria, we develop a series of value-iteration-based algorithms for computing a Markov _non-stationary_ NE of zero-sum NMGs, with finite-iteration guarantees. We also provide numerical experiments to corroborate our theoretical results in Section G. We hope our results serve as a starting point for studying this networked separable interaction structure in non-cooperative Markov games.

Notation.For a real number \(c\), we use \((c)_{+}\) to denote \(\max\{c,0\}\). For an event \(\mathcal{E}\), we use \(\mathbf{1}(\mathcal{E})\) to denote the indicator function such that \(\mathbf{1}(\mathcal{E})=1\) if \(\mathcal{E}\) is true, and \(\mathbf{1}(\mathcal{E})=0\) otherwise. We define multinomial distribution with probability \((w_{i})_{i\in\mathcal{N}}\) as \(\text{Multinomial}((w_{i})_{i\in\mathcal{N}})\). We denote the uniform distribution over a set \(\mathcal{S}\) as \(\text{Unif}(\mathcal{S})\). We denote the Bernoulli distribution with probability \(p\) as \(\text{Bern}(p)\). The sgn function is defined as \(\text{sgn}(x)=2\times\mathbf{1}(x\geq 0)-1\). The KL-divergence between two probability distributions \(p,q\) is denoted as \(\mathrm{KL}(p,q)=\mathbb{E}_{p}[\log(p/q)]\). For a graph \(\mathcal{G}=(\mathcal{N},\mathcal{E})\), we denote the set of neighboring nodes of node \(i\in\mathcal{N}\) as \(\mathcal{E}_{i}\) (without including \(i\)). The maximum norm of a matrix \(X\in\mathbb{R}^{m\times n}\), denoted as \(\left\|X\right\|_{\max}\), is defined as \(\left\|X\right\|_{\max}:=\max_{i\in[m],j\in[n]}|X_{i,j}|\).

Preliminaries

### Markov games

We define a Markov game as a tuple \((\mathcal{N},\mathcal{S},\mathcal{A},H,(\mathbb{P}_{h})_{h\in[H]},(r_{h,i})_{h\in[ H],i\in\mathcal{N}},\gamma)\), where \(\mathcal{N}=[n]\) is the set of players, \(\mathcal{S}\) is the state space with \(|\mathcal{S}|=S\), \(\mathcal{A}_{i}\) is the action space for player \(i\) with \(|\mathcal{A}_{i}|=A_{i}\) and \(\mathcal{A}=\prod_{i\in\mathcal{N}}\mathcal{A}_{i}\), \(H\leq\infty\) is the length of the horizon, \(\mathbb{P}_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) captures the state transition dynamics at timestep \(h\), \(r_{h,i}\in[0,R]\) is the reward function for player \(i\) at timestep \(h\), bounded by some \(R>0\), and \(\gamma\in(0,1]\) is a discount factor. An MG with a finite horizon (\(H<\infty\)) is also referred to as an _episodic_ MG, while an MG with an infinite horizon (\(H=\infty\)) and \(\gamma<1\) is referred to as an infinite-horizon \(\gamma\)-discounted MG. When \(H=\infty\), we will consider the transition dynamics and reward functions, denoted by \(\mathbb{P}\) and \((r_{i})_{i\in\mathcal{N}}\), respectively, to be independent of \(h\). Hereafter, we may use _agent_ and _player_ interchangeably.

**Policy.** Consider the stochastic Markov policy for player \(i\), denoted by \(\pi_{i}\), as \(\pi_{i}:=\{\pi_{h,i}:\mathcal{S}\rightarrow\Delta(\mathcal{A}_{i})\}_{h\in[H]}\). A _joint_ Markov policy is a policy \(\pi:=\{\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\}_{h\in[H]}\), where \(\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) decides the joint action of all players that can be potentially correlated. A joint Markov policy is a _product_ Markov policy if \(\pi_{h}:\mathcal{S}\rightarrow\prod_{i\in\mathcal{N}}\Delta(\mathcal{A}_{i})\) for all \(h\in[H]\), and is denoted as \(\pi=\pi_{1}\times\pi_{2}\times\cdots\times\pi_{n}\). When the policy is independent of \(h\), the policy is called a _stationary_ policy. We let \(\bm{a}_{h}\) denote the joint action of all agents at timestep \(h\). Unless otherwise noted, we will work with Markov policies throughout. We denote \(\pi(s)\in\Delta(\mathcal{A})\) as the joint policy at state \(s\in\mathcal{S}\).

**Value function.** For player \(i\), the value function under joint policy \(\pi\), at timestep \(h\) and state \(s_{h}\) is defined as \(V^{\pi}_{h,i}(s_{h}):=\mathbb{E}_{\pi}\big{[}\sum_{h^{\prime}=h}^{H}\gamma^{h^ {\prime}-h}r_{h^{\prime},i}(s_{h^{\prime}},\bm{a}_{h^{\prime}})\big{|}\,s_{h} \big{]},\) which denotes the expected cumulative reward for player \(i\) at step \(h\) if all players adhere to policy \(\pi\). We also define \(V^{\pi}_{h,i}(\rho):=\mathbb{E}_{s_{h}\sim\rho}[V^{\pi}_{h,i}(s_{h})]\) for some state distribution \(\rho\in\Delta(\mathcal{S})\). We denote the \(Q\)-function for the \(i\)-th player under policy \(\pi\), at step \(h\) and state \(s_{h}\) as \(Q^{\pi}_{h,i}(s_{h},\bm{a}_{h}):=\mathbb{E}_{\pi}\big{[}\sum_{h^{\prime}=h}^{ H}\gamma^{h^{\prime}-h}r_{h^{\prime},i}(s_{h^{\prime}},\bm{a}_{h^{\prime}}) \big{|}\,s_{h},\bm{a}_{h}\big{]},\) which determines the expected cumulative reward for the \(i\)-th player at step \(h\), when starting from the state-action pair \((s_{h},\bm{a}_{h})\). For the infinite-horizon discounted setting, we also use \(V^{\pi}_{i}\) and \(Q^{\pi}_{i}\) to denote \(V^{\pi}_{1,i}\) and \(Q^{\pi}_{1,i}\) for short, respectively.

**Approximate equilibrium.** Define an \(\epsilon\)-approximate _Markov perfect Nash equilibrium_ as a product policy \(\pi\), which satisfies \(\max_{i\in\mathcal{N}}\max_{\mu_{i}\in(\Delta(\mathcal{A}_{i}))^{|S|\times H}} (V^{\mu_{i},\pi^{-i}-}_{h,i}(\rho)-V^{\pi}_{h,i}(\rho))\leq\epsilon\) for _all_\(\rho\in\Delta(\mathcal{S})\) and \(h\in[H]\), where \(\pi_{-i}\) represents the marginalized policy of all players except player \(i\). Define an \(\epsilon\)-approximate _Markov coarse correlated equilibrium_ as a joint policy \(\pi\), which satisfies \(\max_{i\in\mathcal{N}}\max_{\mu_{i}\in(\Delta(\mathcal{A}_{i}))^{|S|\times H}} (V^{\mu_{i},\pi^{-i}}_{h,i}(\rho)-V^{\pi}_{h,i}(\rho))\leq\epsilon\) for _all_\(\rho\in\Delta(\mathcal{S})\) and \(h\in[H]\). In the infinite-horizon setting, they can be equivalently defined as satisfying \(\max_{s\in\mathcal{S}}\max_{i\in\mathcal{N}}\max_{\mu_{i}\in(\Delta(\mathcal{A} _{i}))^{|S|}}(V^{\mu_{i},\pi^{-i}-}_{i}(s)-V^{\pi}_{i}(s))\leq\epsilon\). If the above conditions only hold for certain \(\rho\) and \(h=1\), we refer to them as Markov _non-perfect_ NE and CCE, respectively. Unless otherwise noted, we hereafter focus on Markov perfect equilibria, and sometimes refer to them simply as _Markov equilibria_ when it is clear from the context. In the infinite-horizon setting, if additionally, the policy is _stationary_, then they are referred to as a _Markov stationary_ NE and CCE, respectively.

### Multi-player zero-sum games with networked separable interactions

As a generalization of _two-player_ zero-sum matrix games, _(multi-player)_ zero-sum polymatrix games have been introduced in [31; 28; 23; 24]. A _polymatrix game_, also known as a _separable network game_ is defined by a tuple \((\mathcal{G}=(\mathcal{N},\mathcal{E}_{r}),\mathcal{A}=\prod_{i\in\mathcal{N}} \mathcal{A}_{i},(r_{i,j})_{(i,j)\in\mathcal{E}_{r}})\). Here, \(\mathcal{G}\) is an undirected connected graph where \(\mathcal{N}=[n]\) denotes the set of players and \(\mathcal{E}_{r}\subseteq\mathcal{N}\times\mathcal{N}\) denotes the set of edges describing the rewards' networked structures, where the graph neighborhoods represent the interactions among players. For each edge, a two-player game is defined for players \(i\) and \(j\), with action sets \(\mathcal{A}_{i}\) and \(\mathcal{A}_{j}\), and reward functions \(r_{i,j}:\mathcal{A}_{i}\times\mathcal{A}_{j}\rightarrow\mathbb{R}\), and similarly for \(r_{j,i}\). The reward for player \(i\) for a given joint action \(\bm{a}=(a_{i})_{i\in\mathcal{N}}\in\prod_{i\in\mathcal{N}}\mathcal{A}_{i}\) is calculated as the sum of the rewards for all edges involving player \(i\), that is, \(r_{i}(\bm{a})=\sum_{j:(i,j)\in\mathcal{E}_{r}}r_{i,j}(a_{i},a_{j})\). To be consistent with our terminology later, hereafter, we also refer to such games as _(multi-player) Games with Networked separable interactions (NGs)_.

In a _zero-sum_ polymatrix game (i.e., a (multi-player) _zero-sum_ Game with Networked separable interactions (zero-sum NG)), the sum of rewards for all players at any joint action \(\bm{a}=(a_{i})_{i\in\mathcal{N}}\in\prod_{i\in\mathcal{N}}\mathcal{A}_{i}\) equals zero, i.e., \(\sum_{i\in\mathcal{N}}r_{i}(\bm{a})=0\). One can define the policy of agent \(i\), i.e., \(\pi_{i}\in\Delta(\mathcal{A}_{i})\), so that the agent takes actions by sampling \(a_{i}\sim\pi_{i}(\cdot)\). Note that \(\pi_{i}\) can be viewed as the reduced case of the policy defined in Section 2.1 when \(\mathcal{S}=\emptyset\) and \(H=1\). The expected reward for player under \(\pi\) can then be computed as:

\[r_{i}(\pi):=\sum_{j:(i,j)\in\mathcal{E}_{r}}\sum_{a_{i}\in\mathcal{A}_{i},a_{j} \in\mathcal{A}_{j}}r_{i,j}\left(a_{i},a_{j}\right)\pi_{i}(a_{i})\pi_{j}(a_{j})= \pi_{i}^{\mathsf{T}}\bm{r}_{i}\bm{\pi},\] (1)

where \(\bm{r}_{i}\) denotes the matrix \(\bm{r}_{i}:=(r_{i,1},\ldots,r_{i,(i-1)},\bm{0},r_{i,(i+1)},\ldots,r_{i,n})\in \mathbb{R}^{|\mathcal{A}_{i}|\times\sum_{i\in\mathcal{N}}|\mathcal{A}_{i}|}\) and \(\bm{\pi}:=(\pi_{1}^{\mathsf{T}},\pi_{2}^{\mathsf{T}},\ldots,\pi_{n}^{\mathsf{ T}})^{\mathsf{T}}\in\mathbb{R}^{\sum_{i\in\mathcal{N}}|\mathcal{A}_{i}|}\). We define \(\bm{r}:=(\bm{r}_{1}^{\mathsf{T}},\bm{r}_{2}^{\mathsf{T}},\ldots,\bm{r}_{n}^{ \mathsf{T}})^{\mathsf{T}}\in\mathbb{R}^{\sum_{i\in\mathcal{N}}|\mathcal{A}_{ i}|\times\sum_{i\in\mathcal{N}}|\mathcal{A}_{i}|}\). Then in this case we have \(\sum_{i\in\mathcal{N}}r_{i}(\pi)=0\) for any policy \(\pi\). See more prominent application examples of zero-sum polymatrix games in [23; 24].

## 3 Multi-Player (Zero-Sum) MGs with Networked Separable Interactions

We now introduce our model of multi-player zero-sum MGs with networked separable interactions.

### Definitions

**Definition 1**.: An infinite-horizon \(\gamma\)-discounted MG is called a _(multi-player) MG with Networked separable interactions (NMG)_ characterized by a tuple \((\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q}),\mathcal{S},\mathcal{A},\mathbb{P },(r_{i})_{i\in\mathcal{N}},\gamma)\) if for any function \(V:\mathcal{S}\to\mathbb{R}\), defining \(Q_{i}^{V}(s,\bm{a}):=r_{i}(s,\bm{a})+\gamma\sum_{s^{\prime}\in\mathcal{S}} \mathbb{P}(s^{\prime}\mid s,\bm{a})V(s^{\prime})\), there exist a set of functions \((Q_{i,j}^{V})_{(i,j)\in\mathcal{E}_{Q}}\) and an undirected connected graph \(\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q})\) such that \(Q_{i}^{V}(s,\bm{a})=\sum_{j\in\mathcal{E}_{Q}}Q_{i,j}^{V}(s,a_{i},a_{j})\) holds for every \(i\in\mathcal{N}\), \(s\in\mathcal{S}\), \(\bm{a}\in\mathcal{A}\), where \(\mathcal{E}_{Q,i}\) denotes the neighbors of player \(i\) induced by the edge set \(\mathcal{E}_{Q}\) (without including \(i\)). When it is clear from the context, we represent the NMG tuple simply as \(\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q})\).

A finite-horizon MG is called a _(multi-player) MG with Networked separable interactions_ if for any set of functions \(V:=\{V_{h}\}_{h\in[H+1]}\) where \(V_{h}:\mathcal{S}\to\mathbb{R}\), defining \(Q_{h,i}^{V}(s,\bm{a}):=r_{h,i}(s,\bm{a})+\gamma\sum_{s^{\prime}\in\mathcal{S} }\mathbb{P}_{h}(s^{\prime}\mid s,\bm{a})V_{h+1}(s^{\prime})\), there exist a set of functions \((Q_{h,i,j}^{V})_{(i,j)\in\mathcal{E}_{Q},h\in[H]}\) such that \(Q_{h,i}^{V}(s,\bm{a})=\sum_{j\in\mathcal{E}_{Q},Q}Q_{h,i,j}^{V}(s,a_{i},a_{j})\) holds for every \(i\in\mathcal{N}\), \(s\in\mathcal{S}\), \(s\in[H]\), \(\bm{a}\in\mathcal{A}\).

A (multi-player) NMG is called a _(multi-player) zero-sum MG with Networked separable interactions (zero-sum NMG)_ if additionally \((\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q}),\mathcal{A}=\prod_{i\in\mathcal{N} }\mathcal{A}_{i},(r_{i,j}(s):=Q_{i,j}^{\bm{0}}(s))_{(i,j)\in\mathcal{E}_{Q}})\) forms a zero-sum NG for all \(s\in\mathcal{S}\) in the infinite-horizon \(\gamma\)-discounted case, or \((\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q}),\mathcal{A}=\prod_{i\in\mathcal{N} }\mathcal{A}_{i},(r_{h,i,j}(s):=Q_{h,i,j}^{\bm{0}}(s))_{(i,j)\in\mathcal{E}_{ Q}})\) forms a zero-sum NG for all \(s\in\mathcal{S}\) and \(h\in[H]\) in the finite-horizon case.

Regarding the assumption that the above conditions hold under any (set of) functions \(V\), one may understand this as a structural requirement to inherit the polymatrix structure in the Markov game case. It is natural since \(\{Q_{i}^{V}\}_{i\in\mathcal{N}}\) would play the role of the _payoff matrix_ in the normal-form case, when value-(iteration) based algorithms are used to solve the MG. As our hope is to exploit the networked structure in the payoff matrices to develop efficient algorithms for solving such MGs, if we do not know _a priori_ which value function estimate \(V\) will be encountered in the algorithm update, the networked structure may easily break if we do not assume them to hold for _all_ possible \(V\). Moreover, such a definition easily encompasses the normal-form case, by preserving the polymatrix structure of the _reward functions_ (when substituting \(V\) to be a zero function). Some alternative definition (see Remark 2) may not necessarily preserve the polymatrix structure of even the reward functions in a consistent way (see Section B.4 for a concrete example). We thus focus on Definition 1, which at least covers the polymatrix structure of the reduced case regarding only reward functions.

Indeed, such a networked structure in Markov games may be fragile. We now propose both sufficient and _necessary_ conditions for the reward function's structure and the transition dynamics of the MG, to be an NMG. Here we focus on the infinite-horizon discounted setting for a simpler exposition. For finite-horizon cases, a similar statement holds, which is deferred to Appendix B. We also defer the full statement and proof of the following result to Appendix B. We first introduce the definition of decomposability and the set \(\mathcal{N}_{\mathcal{C}}\), which will be used in establishing the conditions. For a graph \(\mathcal{G}=(\mathcal{N},\mathcal{E})\), we define \(\mathcal{N}_{C}:=\{i\mid(i,j)\in\mathcal{E}\text{ for all }j\in\mathcal{N}\}\), which may be an empty set if no such node \(i\) exists.

**Definition 2** (Decomposability).: A non-negative function \(f:X^{|\mathcal{D}|}\to\mathbb{R}^{+}\cup\{0\}\) is decomposable with respect to a set \(\mathcal{D}\neq\emptyset\) if there exists a set of non-negative functions \((f_{i})_{i\in\mathcal{D}}\) with \(f_{i}:X\to\mathbb{R}^{+}\cup\{0\}\), such that \(f(x)=\sum_{i\in\mathcal{D}}f_{i}(x_{i})\) holds for any \(x\in X^{|\mathcal{D}|}\). A non-negative function \(f:X^{|\mathcal{D}|}\to\mathbb{R}^{+}\cup\{0\}\) is decomposable with respect to a set \(\mathcal{D}=\emptyset\), if there exists a non-negative constant \(f_{o}\) such that \(f(x)=f_{o}\) holds for any \(x\in X^{|\mathcal{D}|}\).

**Proposition 1**.: For a given graph \(\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q})\), an MG \((\mathcal{N},\mathcal{S},\mathcal{A},\mathbb{P},(r_{i})_{i\in\mathcal{N}},\gamma)\) with more than two players is an NMG with respect to \(\mathcal{G}\) if and only if: (1) \(r_{i}(s,a_{i},\cdot)\)**is decomposable with respect to \(\mathcal{E}_{Q,i}\)** for each \(i\in\mathcal{N},s\in\mathcal{S},a_{i}\in\mathcal{A}_{i}\), i.e., \(r_{i}(s,\boldsymbol{a})=\sum_{j\in\mathcal{E}_{Q,i}}r_{i,j}(s,a_{i},a_{j})\) for a set of functions \(\{r_{i,j}(s,a_{i},\cdot)\}_{j\in\mathcal{E}_{Q,i}}\), and (2) **the transition dynamics \(\mathbb{P}(s^{\prime}\,|\,s,\cdot)\) is decomposable with respect to** the set \(\mathcal{N}_{C}\) of this \(\mathcal{G}\), i.e., \(\mathbb{P}(s^{\prime}\,|\,s,\boldsymbol{a})=\sum_{i\in\mathcal{N}_{C}}\mathbb{ F}_{i}(s^{\prime}\,|\,s,a_{i})\) for a set of functions \(\{\mathbb{F}_{i}(s^{\prime}\,|\,s,\cdot)\}_{i\in\mathcal{N}_{C}}\) if \(\mathcal{N}_{C}\neq\emptyset\), or \(\mathbb{P}(s^{\prime}\,|\,s,\boldsymbol{a})=\mathbb{F}_{o}(s^{\prime}\,|\,s)\) for some constant function (of \(\boldsymbol{a}\)) \(\mathbb{F}_{o}(s^{\prime}\,|\,s)\) if \(\mathcal{N}_{C}=\emptyset\). Moreover, an MG qualifies as a zero-sum NMG if and only if it satisfies an additional condition: the NG, characterized by \((\mathcal{G},\mathcal{A},(r_{i,j}(s))_{(i,j)\in\mathcal{E}_{Q}})\), is a zero-sum NG for all \(s\in\mathcal{S}\). In the case of two players, every (zero-sum) Markov game becomes a (zero-sum) NMG.

**Remark 1** (Stronger sufficient condition).: We note that for an MG \((\mathcal{N},\mathcal{S},\mathcal{A},\mathbb{P},(r_{i})_{i\in\mathcal{N}},\gamma)\), if for every agent \(i\), \(r_{i}(s,a_{i},\cdot)\) is decomposable with respect to some \(\mathcal{E}_{r}\subseteq\mathcal{E}_{Q}\), and \(\mathbb{P}(s^{\prime}\,|\,s,\cdot)\) is decomposable with respect to some \(\mathcal{N}_{P}\subseteq\mathcal{N}_{C}\), then one can still prove the _if_ part, i.e., there exists some \(\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q})\) such that the game is an NMG with respect to this \(\mathcal{G}\). See Figure 1 for the illustration. This is because by our definition, being decomposable with respect to a subset implies being decomposable with respect to a larger set, as one can choose the functions \(f_{i}\) for the \(i\) in the complement of the subset to be simply zero. We chose to state as in Proposition 1 just for the purpose of presenting both the _if_ and _only if_ conditions in a concise and unified way.

**Remark 2** (An alternative NMG definition).: Another reasonable definition of NMG may be as follows: if for any _policy_\(\pi\), there exist a set of functions \((Q_{i,j}^{\pi})_{(i,j)\in\mathcal{E}_{Q}}\) and an undirected connected graph \(\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q})\) such that \(Q_{i}^{\pi}(s,\boldsymbol{a})=\sum_{j\in\mathcal{E}_{Q,i}}Q_{i,j}^{\pi}(s,a_{i },a_{j})\) holds for every \(i\in\mathcal{N}\), \(s\in\mathcal{S}\), \(\boldsymbol{a}\in\mathcal{A}\). Note that such a definition can be useful in developing _policy_-based algorithms (while Definition 1 is more amenable to developing _value_-based algorithms), e.g., policy iteration, policy gradient, actor-critic methods, where the \(Q\)-value under certain _policy_\(\pi\) will appear in the updates and may need to preserve certain decomposability structure, for any policy \(\pi\) encountered in the algorithm updates. However, in this case, we cannot always guarantee the decomposability of \(\mathbb{P}(s^{\prime}\,|\,s,\cdot)\) or \(r_{i}(s,a_{i},\cdot)\). For example, if we assume that \(r_{i}(s,\boldsymbol{a})=0\) for every \(i\in\mathcal{N}\), \(s\in\mathcal{S}\), \(\boldsymbol{a}\in\mathcal{A}\), then \(Q_{i}^{\pi}(s,\boldsymbol{a})=\sum_{j\in\mathcal{E}_{Q,i}}0\) and thus \(Q_{i}^{\pi}(s,a_{i},\cdot)\) is always decomposable regardless of \(\mathbb{P}(s^{\prime}\,|\,s,\boldsymbol{a})\). However, interestingly, we can show that the decomposability of the transition dynamics and the reward function as in Proposition 1 can still be guaranteed, as long as some _degenerate_ cases as above do not occur. In particular, if there exist no \(i\in\mathcal{N}\) and \(s\in\mathcal{S}\) such that \(Q_{i}^{\pi}(s,\boldsymbol{a})\) is a constant function of \(\boldsymbol{a}\) for any \(\pi\), then the results in Proposition 1 and hence after still hold. We defer a detailed discussion on this alternative definition to Appendix B.

**Remark 3** (Implication of decomposable transition dynamics).: For an MG to be an NMG, by Proposition 1 the transition dynamics should be decomposable, i.e., \(\mathbb{P}(\cdot\,|\,s,\boldsymbol{a})=\sum_{j\in\mathcal{N}_{C}}\mathbb{F}_{j }(\cdot\,|\,s,a_{j})\) or \(\mathbb{P}(\cdot\,|\,s,\boldsymbol{a})=\mathbb{F}_{o}(s^{\prime}\,|\,s)\). We first focus on the discussion of the former case. Define \(w_{j}(s,a_{j}):=\sum_{s^{\prime}\in\mathcal{S}}\mathbb{F}_{j}(s^{\prime}\,|\,s,a _{j})\). If we fix the value of \(s\) and \(a_{-j}\), then \(w_{j}(s,a_{j})\) has to be the same for different values of \(a_{j}\) due to the fact \(\sum_{j\in\mathcal{N}_{C}}w_{j}(s,a_{j})=1\). Also note that by definition, \(w_{j}(s,a_{j})\) does not depend on the choice of \(a_{-j}\). Therefore, such a \(w_{j}(s,a_{j})\) can be written as \(w_{j}(s)\), where \(w_{j}(s)=\sum_{s^{\prime}\in\mathcal{S}}\mathbb{F}_{j}(s^{\prime}\,|\,s,a_{j})\) for all \(a_{j}\in\mathcal{A}_{j}\). We can thus rewrite \((\mathbb{F}_{j})_{j\in\mathcal{N}_{C}}\) using some actual probability distributions \((\mathbb{P}_{j})_{j\in\mathcal{N}_{C}}\), such that if \(w_{j}(s)\neq 0\), then we rewrite \(\mathbb{F}_{j}\) as \(\mathbb{F}_{j}(s^{\prime}\,|\,s,a_{j})=w_{j}(s)\frac{\mathbb{F}_{j}(s^{\prime}\,| \,s,a_{j})}{w_{j}(s)}=w_{j}(s)\mathbb{P}_{j}(s^{\prime}\,|\,s,a_{j})\), and if \(w_{j}(s)=0\), we rewrite \(\mathbb{F}_{j}\) as \(\mathbb{F}_{j}(s^{\prime}\,|\,s,a_{j})=w_{j}(s)\mathbb{P}_{j}(s^{\prime}\,|\,s,a _{j})\) for an arbitrary probability distribution \(\mathbb{P}_{j}(\cdot\,|\,s,a_{j})\). Notice that \(\sum_{s^{\prime}\in\mathcal{S}}\mathbb{P}_{j}(s^{\prime}\,|\,s,a_{j})=1\) for any \(j\in\mathcal{N}_{C}\). Then, the decomposable transition dynamics can be represented as \(\mathbb{P}(\cdot\,|\,s,\boldsymbol{a})=\sum_{j\in\mathcal{N}_{C}}w_{j}(s)\mathbb{ P}_{j}(\cdot\,|\,s,a_{j})\), i.e., an _ensemble_ of the transition dynamics that is only controlled by single controllers. The model's transition dynamics thus act according to the following two steps: (1) sampling the controller according to the distribution \(\text{Multinomial}\big{(}(w_{i}(s))_{i\in\mathcal{N}_{C}}\big{)}\), and (2) transitioning the state following the sampled controller's dynamics. Such a model has also been investigated under the name of transition dynamics with _additive structures_ in [33]. Note that our model is more general and thus covers the single-controller MG setting [34], where there is only one agent controlling the transition dynamics at _all_ states. It also covers the setting of turn-based MGs [34], where in each round, depending on the current state \(s\), the transition dynamics is by turns affected by only one of the agents. This can be captured by the proper choice of \((w_{i}(s))_{i\in\mathcal{N}_{C}}\)) that takes value \(1\) only for one agent at each state \(s\) (while takes value \(0\) for all other non-controller agents at each state \(s\)). Additionally, the second case where \(\mathbb{P}(\cdot\,|\,s,\boldsymbol{a})=\mathbb{F}_{o}(s^{\prime}\,|\,s)\) corresponds to the one with no ensemble of controller agents.

**Proposition 2** (Decomposition of \((Q_{i}^{V})_{i\in\mathcal{N}}\)).: For an infinite-horizon \(\gamma\)-discounted NMG with \(\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q})\) such that \(\mathcal{N}_{C}\neq\emptyset\), if we know that \(\mathbb{P}(s^{\prime}\,|\,s,\boldsymbol{a})=\sum_{i\in\mathcal{N}_{C}} \mathbb{F}_{i}(s^{\prime}\,|\(r_{i}(s,\bm{a})=\sum_{j\in\mathcal{E}_{Q,i}}r_{i,j}(s,a_{i},a_{j})\) for some \(\{\mathbb{F}_{i}\}_{i\in\mathcal{N}_{C}}\) and \(\{r_{i,j}\}_{(i,j)\in\mathcal{E}_{Q}}\), then the \(Q^{V}_{i,j}\) given in Definition 1 can be represented as

\[Q^{V}_{i,j}(s,a_{i},a_{j})=r_{i,j}(s,a_{i},a_{j})+\sum_{s^{\prime}\in\mathcal{ S}}\gamma\left(\bm{1}(j\in\mathcal{N}_{C})\mathbb{F}_{j}(s^{\prime}\mid s,a_{j})+ \bm{1}(i\in\mathcal{N}_{C})\lambda_{i,j}(s)\mathbb{F}_{i}(s^{\prime}\mid s,a_{ i})\right)V(s^{\prime})\]

for any non-negative \((\lambda_{i,j}(s))_{(i,j)\in\mathcal{E}_{Q}}\) such that \(\sum_{j\in\mathcal{E}_{Q,i}}\lambda_{i,j}(s)=1\), for all \(i\in\mathcal{N}\) and \(s\in\mathcal{S}\). We call it the _canonical_ decomposition of \(\{Q^{V}_{i}\}_{i\in\mathcal{N}}\) when \(Q^{V}_{i,j}\) can be represented as above with \(\lambda_{i,j}(s)=1/|\mathcal{E}_{Q,i}|\) for \(j\in\mathcal{E}_{Q,i}\). The case with \(\mathcal{N}_{C}=\emptyset\) is deferred to Appendix B.

We introduce this canonical decomposition since the representation of \(Q^{V}_{i,j}\) is in general not unique, and we may use this canonical form to simplify the algorithm design later.

### Examples of multi-player (zero-sum) NMGs

We now provide several examples of (multi-player) MGs with networked separable interactions here and in Section B.5.

Example 1 (Markov fashion games).Fashion games are an intriguing class of games [25; 26; 27] that plays a vital role not only in Economics theory but also in practice. A fashion game is a networked extension of the Matching Pennies game, in which each player has the action space \(\mathcal{A}_{i}=\{-1,+1\}\), which means _light_ and _dark_ color fashions, respectively, for example. There are two types of players: _conformists_ (\(\mathfrak{c}\)), who prefer to conform to their neighbors' fashion (action), and _rebels_ (\(\mathfrak{r}\)), who prefer to oppose their neighbors' fashion (action). Such interactions with the neighbors are exactly captured by polymatrix games. We denote the interaction network between players as \(\mathcal{G}=(\mathcal{N},\mathcal{E})\).

Such a game naturally involves the following state transition dynamics: we introduce the state \(s\in\mathcal{S}=\mathbb{Z}\) by setting \(s_{0}=0\) and \(s_{t+1}\sim s_{t}+\text{Unif}((a_{t,c})_{c\in\mathcal{C}})\), which indicates the _fashion trend_ where \(\mathcal{C}\subseteq\mathcal{N}\) is the set of influencers. The fashion trend favors either light or dark colors if \(s\geq 0\) or \(s<0\), respectively. We can think of dynamics as the impact of the influencers on the fashion trend at time \(t\). For each \((s,\bm{a})\), the reward function for player \(i\), depending on whether she is a conformist or a rebel, are defined as \(r_{\mathfrak{c},i}(s,\bm{a})=\sum_{j\in\mathcal{E}_{i}}r_{\mathfrak{c},i_{j}} (s,a_{i},a_{j})=\sum_{j\in\mathcal{E}_{i}}(\frac{1}{|\mathcal{E}_{i}|}\bm{1}( \text{sgn}(s)=a_{i})+\bm{1}(a_{i}=a_{j}))\) and \(r_{\mathfrak{r},i}(s,\bm{a})=\sum_{j\in\mathcal{E}_{i}}r_{\mathfrak{r},i_{j}} (s,a_{i},a_{j})=\sum_{j\in\mathcal{E}_{i}}(\frac{1}{|\mathcal{E}_{i}|}\bm{1}( \text{sgn}(s)\neq a_{i})+\bm{1}(a_{i}\neq a_{j}))\), respectively. This is an NMG as defined in Definition 1. Moreover, if the conformists and rebels constitute a bipartite graph, i.e., the neighbors of a conformist are all rebels and vice versa, it becomes a multi-player constant-sum MG with networked separable interactions, and we can subtract the constant offset to make it a zero-sum NMG.

### Relationship between CCE and NE in zero-sum NMGs

A well-known property for zero-sum NGs is that marginalizing a CCE leads to a NE, which makes it computationally tractable to find the NE [23; 24]. We now provide below a counterpart in the Markov game setting, and provide a more detailed statement of the result in Appendix B.

Proposition 3.: Given an \(\epsilon\)-approximate Markov CCE of an infinite-horizon \(\gamma\)-discounted zero-sum NMG, marginalizing it at each state results in an \(\frac{(n+1)}{(1-\gamma)}\epsilon\)-approximate Markov NE of the zero-sum NMG. The same argument also holds for the finite-horizon episodic setting with \((1-\gamma)^{-1}\) being replaced by \(H\).

This result holds for both stationary and non-stationary \(\epsilon\)-approximate Markov CCEs. We defer the proof of Proposition 3 to Appendix B. This proposition suggests that if we can have some algorithms to find an approximate Markov CCE for a zero-sum NMG, we can obtain an approximate Markov NE by marginalizing the approximate CCE at each state. We also emphasize that the _Markovian_ property of the equilibrium policies is important for the result to hold. As a result, the learning algorithm in [30], which learns an approximate Markov _non-stationary_ CCE with polynomial time and samples, may thus be used to find an approximate Markov _non-stationary_ NE in zero-sum NMGs. However, as the focus of [30] was the more challenging setting of _model-free learning_, the complexity therein has a high dependence on the problem parameters, and the algorithm can only find non-perfect equilibria. When it comes to (perfect) equilibrium computation, one may exploit the multi-player zero-sum structure of zero-sum NMGs, and develop more natural and faster algorithms to find a Markov non-stationary NE. Moreover, when it comes to _stationary_ equilibrium computation, even Markov CCE is _not_ tractable in general-sum cases [30, 29]. Hereafter, we will focus on approaching zero-sum NMGs from these perspectives.

## 4 Hardness for Stationary CCE Computation

Given the results in Section 3.3, it seems tempting and sufficient to compute the Markov CCE of the zero-sum NMG. Indeed, computing CCE (and thus NE) in zero-sum polymatrix games is known to be tractable [23, 24]. It is thus natural to ask: _Is finding Markov CCE computationally tractable?_ Next, we answer the question with different answers for finding stationary CCE (in infinite-horizon \(\gamma\)-discounted setting) and non-stationary CCE (in finite-horizon episodic setting), respectively.

For _two-player_ infinite-horizon \(\gamma\)-discounted _zero-sum_ MGs, significant progress in computing/learning the (Markov) stationary NE has been made recently [35, 36, 37, 38, 39, 40, 41, 42]. On the other hand, for _multi-player general-sum_ MGs, recent results in [30, 29] showed that computing (Markov) stationary CCE can be PPAD-hard and thus believed to be computationally intractable. We next show that this hardness persists in most non-degenerate cases even if one enforces the zero-sum and networked interaction structures in the multi-player case. We state the formal result as follows, whose detailed proof is available in Section C.

**Theorem 1**.: There is a constant \(\epsilon>0\) for which computing an \(\epsilon\)-approximate Markov perfect stationary CCE in infinite-horizon \(\frac{1}{2}\)-discounted zero-sum NMGs, whose underlying network structure contains either a triangle or a 3-path subgraph, is PPAD-hard. Moreover, given the PCP for PPAD conjecture [43], there is a constant \(\epsilon>0\) such that computing even an \(\epsilon\)-approximate Markov non-perfect stationary CCE in such zero-sum NMGs is PPAD-hard.

Proof Sketch of Theorem 1.: Due to space constraints, we focus on the case with three players, and the underlying network structure has a triangle subgraph. Proof for the \(3\)-path case is similar and can be found in Section C. We will show that for _any_ general-sum two-player _turn-based_ MG **(A)**, the problem of computing its Markov stationary CCE, which is inherently a PPAD-hard problem [30], can be reduced to computing the Markov stationary CCE of a three-player zero-sum MG with a triangle structure networked separable interactions **(B)**. Consider an MG **(A)** with two players, players 1 and 2, and reward functions \(r_{1}(s,a_{1},a_{2})\) and \(r_{2}(s,a_{2},a_{1})\), where \(a_{i}\) is the action of the \(i\)-th player and \(r_{i}\) is the reward function of the \(i\)-th player. The transition dynamics is given by \(\mathbb{P}(s^{\prime}\,|\,s,a_{1},a_{2})\). In even rounds, player 2's action space is limited to Noop2, and in odd rounds, player 1's action space is limited to Noop1, where Noop1 is an abbreviation of "no-operation", i.e., the player does not affect the transition dynamics or the reward in that round. We denote player 1's action space in even rounds as \(\mathcal{A}_{1,\text{even}}\) and player 2's action space in odd rounds as \(\mathcal{A}_{2,\text{odd}}\), respectively.

Now, we construct a three-player zero-sum NMG. with a triangle network structure. We set the reward function as \(\widetilde{r}_{i}(s,\bm{a})=\sum_{j\neq i}\widetilde{r}_{i,j}(s,a_{i},a_{j})\) and \(\widetilde{r}_{i,j}(s,a_{i},a_{j})=-\widetilde{r}_{j,i}(s,a_{j},a_{i})\). The reward functions are designed so that \(\widetilde{r}_{i,j}=-\widetilde{r}_{j,i}\) for all \(i,j\), \(\widetilde{r}_{1,2}+\widetilde{r}_{1,3}=r_{1}\), and \(\widetilde{r}_{2,1}+\widetilde{r}_{2,3}=r_{2}\), where \(r_{1},r_{2}\) are the reward functions in game **(A)**, by introducing a dummy player, player 3. In even rounds, player 2's action space is limited to Noop2, and in odd rounds, player 1's action space is limited to Noop1. Player 3's action space is always limited to Noop3 in all rounds. The transition dynamics is defined as \(\widetilde{\mathbb{P}}(s^{\prime}\,|\,s,a_{1},a_{2},a_{3})=\mathbb{P}(s^{ \prime}\,|\,s,a_{1},a_{2})\), since \(a_{3}\) is always chosen from Noop3. In other words, player 3's action does not affect the rewards of the other two players, nor the transition dynamics, and players 1 and 2 will receive the reward as in the two-player turn-based MG. Also, note that due to the turn-based structure of the game **(A)**, the transition dynamics satisfy the decomposable condition in our Proposition 1, and it is thus a zero-sum NMG. In fact, turn-based dynamics can be represented as an ensemble of single controller dynamics, as we have discussed in Section 3.

Note that the new game **(B)** is still a turn-based game, and thus the Markov stationary CCE is the same as the Markov stationary NE. Also, note that by construction, the equilibrium policies of players \(1\) and \(2\) at the Markov stationary CCE of the game **(B)** constitute a Markov stationary CCE of the game **(A)**. If the underlying network is more general than a triangle, but contains a triangle subgraph, we can specify the reward and transition dynamics of these three players as above, and specify all other players to be dummy players, whose reward functions are all zero, and do not affect the reward functions of these three players, nor the transition dynamics. This completes the proof. 

Figure 2 briefly explains how we may reduce the equilibrium computation problem of **(A)** to that of **(B)**. In fact, a connected graph that does not contain a subgraph of a triangle or a 3-path has to be a _star-shaped_ network (Proposition 7), which is proved in Section C. Hence, by Theorem 1, we know that in the infinite-horizon discounted setting, finding Markov stationary NE/CE/CCE is a computationally hard problem unless the underlying network is star-shaped. This may also imply that _learning_ Markov stationary NE in zero-sum NMGs, e.g., using natural dynamics like fictitious play to reach the NE, can be challenging, unless in the star-shaped case. In turn, one may hope fictitious-play dynamics to converge for star-shaped zero-sum NMGs. We instantiate this idea next in Section 5. Furthermore, in light of Theorem 1, we will shift gear to computing Markov _non-stationary_ NE by utilizing the structure of networked separable interactions, as to be detailed in Section 6.

## 5 Fictitious-Play Property

In this section, we study the fictitious-play property of multi-player zero-sum games with networked separable interactions, for both the matrix and Markov game settings. Following the convention in [8], we refer to the games in which fictitious-play dynamics converge to the NE as the games that have the _fictitious-play property_. We defer the matrix game case results to Section D, where we have also established convergence of the well-known variant of FP, smooth FP [7], in zero-sum NGs.

Echoing the computational intractability of computing CCE of zero-sum NMG unless the underlying network structure is star-shaped in the infinite-horizon discounted setting (c.f. Theorem 1), we now consider the FP property in such games. Note that by Proposition 1, \(\mathcal{E}_{Q}\) is a star-shape if and only if the reward structure is a star shape and \(\mathcal{N}_{C}=\{1\}\), where player 1 is the center of the star (Figure 2), or there are only two players in zero-sum NMG. There is already existing literature for the latter case [15, 16], so we focus on the former case, which is a single-controller case where player 1 controls the transition dynamics, i.e., \(\mathbb{P}(s^{\prime}\,|\,s,\boldsymbol{a})=\mathbb{P}_{1}(s^{\prime}\,|\,s,a_{1})\) for some \(\mathbb{P}_{1}\). We now introduce the fictitious-play dynamics for such zero-sum NMGs.

Each player \(i\) first initializes her beliefs of other players' policies as uniform distributions, and also initializes her belief of the \(Q\)-value estimates with arbitrary values. Then, at iteration \(k\), player \(i\) takes the _best-response_ action based on her belief of other players' policies \((\widehat{\pi}_{-i}^{(k)}(s^{(k)}))\), and their \(Q\) beliefs \(\widehat{Q}_{i}^{(k)}(s^{(k)},\boldsymbol{a})\):

\[a_{i}^{(k)}\in\operatorname*{argmax}_{a_{i}\in\mathcal{A}_{i}}\ \widehat{Q}_{i}^{(k)}(s^{(k)},e_{a_{i}},\widehat{\pi}_{-i}^{(k)}(s^{(k)})).\]

Then, player \(i\) implements the action \(a_{j}^{(k)}\), observes other players' actions \(a_{-i}^{(k)}\), and updates her beliefs as follows: for each player \(i\in\mathcal{N}\), she updates her belief of the opponents' policies as

\[\widehat{\pi}_{-i}^{(k+1)}(s)=\widehat{\pi}_{-i}^{(k)}(s)+\boldsymbol{1}(s=s ^{(k)})\alpha^{N(s)}(e_{a_{-i}^{(k)}}-\widehat{\pi}_{-i}^{(k)}(s))\]

for all \(s\in\mathcal{S}\), with stepsize \(\alpha^{N(s)}\geq 0\) where \(N(s)\) is the visitation count for the state \(s\); then if \(i=1\), this player \(1\) updates the belief of \(Q_{1,j}\) for all \(j\in\mathcal{N}/\{1\}\) and her own \(\widehat{Q}_{1}(s,\boldsymbol{a})\) for all \(s\in\mathcal{S}\) as

Figure 2: (Left, Middle): PPAD-hardness reduction visualization of \(\mathcal{E}_{Q}\). (Right): A star-shaped zero-sum NMG.

\[\widehat{Q}_{1,j}^{(k+1)}(s,a_{1},a_{j})=\widehat{Q}_{1,j}^{(k)}(s,a _{1},a_{j})\\ +\mathbf{1}(s=s^{(k)})\beta^{N(s)}\Big{(}r_{1,j}(s,a_{1},a_{j})+ \gamma\sum_{s^{\prime}\in\mathcal{S}}\frac{\mathbb{P}_{1}(s^{\prime}\mid s,a_{ 1})}{n-1}\cdot\widehat{V}_{1}^{(k)}(s^{\prime})-\widehat{Q}_{1,j}^{(k)}(s,a_{ 1},a_{i})\Big{)},\]

which is based on the canonical decomposition given in Proposition 2, where \(\widehat{V}_{1}^{(k)}(s)=\max_{a_{1}\in\mathcal{A}_{1}}\widehat{Q}_{1}^{(k)}(s,e_{a_{1}},\widehat{\pi}_{-1}^{(k)}(s))\), and \(\beta^{N(s)}\geq 0\) is the stepsize. The agent then updates \(\widehat{Q}_{1}^{(k+1)}(s,\boldsymbol{a})=\sum_{j\in\mathcal{N}/\{1\}} \widehat{Q}_{1,j}^{(k+1)}(s,a_{1},a_{j})\), for all \(s\in\mathcal{S},\boldsymbol{a}\in\mathcal{A}\). Otherwise, if \(i\neq 1\), then player \(i\) updates the belief of her \(\widehat{Q}_{i,1}(s,\boldsymbol{a})\) for all \(s\in\mathcal{S},\boldsymbol{a}\in\mathcal{A}\) as

\[\widehat{Q}_{i,1}^{(k+1)}(s,a_{i},a_{1})=\widehat{Q}_{i,1}^{(k)} (s,a_{i},a_{1})\\ +\mathbf{1}(s=s^{(k)})\beta^{N(s)}\Big{(}r_{i,1}(s,a_{i},a_{1})+ \gamma\sum_{s^{\prime}\in\mathcal{S}}\mathbb{P}_{1}(s^{\prime}\mid s,a_{1}) \cdot\widehat{V}_{i}^{(k)}(s^{\prime})-\widehat{Q}_{i,1}^{(k)}(s,a_{i},a_{1}) \Big{)},\]

where \(\widehat{V}_{i}^{(k)}(s)=\max_{a_{i}\in\mathcal{A}_{i}}\widehat{Q}_{i}^{(k)}(s,e_{a_{i}},\widehat{\pi}_{-i}^{(k)}(s))\), and we let \(\widehat{Q}_{i}^{(k+1)}(s,\boldsymbol{a})=\widehat{Q}_{i,1}^{(k+1)}(s,a_{i},a _{1})\) for these \(i\neq 1\). The overall dynamics are summarized in Algorithm 4, which resembles the FP dynamics for two-player zero-sum [16] and identical-interest [19] MGs. Now we are ready to present the convergence guarantees.

**Assumption 1**.: The sequences of step sizes \(\big{\{}\alpha^{k}\in(0,1]\big{\}}_{k\geq 0}\) and \(\big{\{}\beta^{k}\in(0,1]\big{\}}_{k\geq 0}\) satisfy the following conditions: (1) \(\sum_{k=0}^{\infty}\alpha^{k}=\infty\), \(\sum_{k=0}^{\infty}\beta^{k}=\infty\), and \(\lim_{k\to\infty}\alpha^{k}=\lim_{k\to\infty}\beta^{k}=0\); (2) \(\lim_{k\to\infty}\frac{\beta^{k}}{\alpha^{k}}=0\), indicating that the rate at which the beliefs about \(Q\)-functions are updated is slower than the rate at which the beliefs about policies are updated.

**Theorem 2**.: Suppose Assumption 1 holds and Algorithm 4 visits every state infinitely often with probability \(1\). Then, for a star-shaped multi-player zero-sum NMG, the belief \((\widehat{\pi}^{(k)})_{k\geq 0}\) converges to a Markov stationary NE and the belief \((\widehat{Q}^{(k)})_{k\geq 0}\) converges to the corresponding NE value of the zero-sum NMG with probability 1, as \(k\to\infty\).

We defer the proof to Section D.2 due to space constraints. Note that to illustrate the idea, we only present the result for the _model-based_ case, i.e., when the transition dynamics \(\mathbb{P}\) is known. With this result, it is direct to extend to the model-free and learning case, where \(\mathbb{P}\) is not known [16, 19, 17], still using the tool of stochastic approximation [44]. See Section D for more details.

**Remark 4** (Challenges for analyzing general cases).: One might ask why we had to focus on a star-shaped structure. First, for general networked structures, even in the matrix-game case, it is known that the NE _values_ of a zero-sum NG may not be unique [24]. Hence, suppose one performs _Nash-value iteration_, i.e., solving for the NE of the stage game and conducting backward induction, this value iteration process does not converge in general as the number of backward steps increases, since the solution at each stage is not even unique, and there may not exist a unique fixed point. This is in stark contrast to the \(\max\) and \(\max\min\) operators in the value iteration updates for single-player and two-player zero-sum cases, respectively. By exploiting a star-shaped structure, we managed to reformulate a _minimax_ optimization problem when solving each stage game, which makes the corresponding value iteration operator _contracting_, and thus iterating it infinitely converges to the unique fixed point. Second, suppose there exists some other network structure (other than star-shaped ones) that also leads to a contracting value iteration operator, then for a fixed constant \(\gamma\), the fixed point (which corresponds to the Markov stationary CCE/NE of the zero-sum NMG) becomes unique and can be computed efficiently, which contradicts our hardness result in Theorem 1. Indeed, it was the exclusion of a star-shaped structure in Theorem 1 that inspired us to consider this structure in proving the convergence of FP dynamics. That being said, we note that having a contracting value iteration operator is only a _sufficient_ condition for the FP dynamics to converge. It would be interesting to explore other structures that enjoy the FP property for reasons beyond this contraction property. We leave this as an immediate future work.

Next, we present another positive result in light of the hardness in Theorem 1, regarding the computation of _non-stationary_ equilibria in multi-player zero-sum NMGs.

Non-Stationary NE Computation

We now focus on computing an (approximate) Markov _non-stationary_ equilibrium in zero-sum NMGs. In particular, we show that when relaxing the stationarity requirement, not only CCE, but NE, can be computed efficiently. Before introducing our algorithm, we first recall the folklore result that approximating Markov non-stationary NE in _infinite-horizon_ discounted settings can be achieved by finding approximate Markov NE in _finite-horizon_ settings, with a large enough horizon length (c.f. Proposition 10). Hence, we will focus on the finite-horizon setting from now on.

Before delving into the details of our algorithm, we introduce the notation \(\bm{Q}_{h,i}(s)\) and \(\bm{Q}_{h}(s)\) for \(h\in[H],i\in\mathcal{N},s\in\mathcal{S}\) as follows:

\[\bm{Q}_{h,i}(s):=(Q_{h,i,1}(s),\ldots,Q_{h,i,i-1}(s),\bm{0},Q_{h,i,i+1}(s) \ldots,Q_{h,i,n}(s))\in\mathbb{R}^{|\mathcal{A}_{i}|\times\sum_{i\in\mathcal{N }}|\mathcal{A}_{i}|}\]

\[\bm{Q}_{h}(s):=((\bm{Q}_{h,1}(s))^{\intercal},(\bm{Q}_{h,2}(s))^{\intercal}, \ldots,(\bm{Q}_{h,n}(s))^{\intercal})^{\intercal}\in\mathbb{R}^{\sum_{i\in \mathcal{N}}|\mathcal{A}_{i}|\times\sum_{i\in\mathcal{N}}|\mathcal{A}_{i}|}.\]

Here, \(Q_{h,i,j}\) represents an estimate of the equilibrium value function with canonical decomposition (Proposition 2). Hereafter, we similarly define the notation of \(\bm{Q}_{h,i}^{\intercal}\) and \(\bm{Q}_{h}^{\intercal}\). Our algorithm is based on value iteration, and iterates three main steps from \(h=H\) to 1 as follows: (1) \(Q\)-value computation: compute \(Q_{h,i,j}\), which estimates the equilibrium \(Q\)-value with a canonical decomposition form; in particular, when \(\mathcal{N}_{C}\neq\emptyset\), \(Q_{h,i,j}\) is updated for all \(s\in\mathcal{S},(i,j)\in\mathcal{E}_{Q},a_{i}\in\mathcal{A}_{i}\), and \(a_{j}\in\mathcal{A}_{j}\):

\[Q_{h,i,j}(s,a_{i},a_{j})=r_{h,i,j}(s,a_{i},a_{j})+\sum_{s^{\prime}\in \mathcal{S}}\Bigl{(}\frac{1}{|\mathcal{E}_{Q,i}|}\bm{1}(i\in\mathcal{N}_{C}) \mathbb{F}_{h,i}(s^{\prime}\mid s,a_{i})+\bm{1}(j\in\mathcal{N}_{C})\mathbb{ F}_{h,j}(s^{\prime}\mid s,a_{j})\Bigr{)}V_{h+1,i}(s^{\prime}),\]

(2) Policy update: update \(\pi_{h}(s)\) with an NE-ORACLE: finding (approximate)-NE of some zero-sum NG \((\mathcal{G},\mathcal{A},(Q_{h,i,j}(s))_{(i,j)\in\mathcal{E}_{Q}})\) for all \(s\in\mathcal{S}\), and (3) Value function update: compute \(V_{h,i}\), which estimates the equilibrium value function as follows for all \(s\in\mathcal{S},i\in\mathcal{N}\): \(V_{h,i}(s)=\pi_{h,i}^{\intercal}(s)\bm{Q}_{h,i}(s)\pi_{h}(s)\). The overall procedure is summarized in Algorithm 6.

Ne-ORACLE and iteration complexity.The NE-ORACLE in Algorithm 6 can be instantiated by several different algorithms that can find an NE in a zero-sum NG. Depending on the algorithms, the convergence guarantees can be either in terms of average-iterate, best-iterate, or last-iterate. Note that for algorithms with average-iterate convergence, one may additionally need to _marginalize_ the output joint policy, i.e., the approximate CCE, and combine them as a _product_ policy that is an approximate NE (Proposition 6). For those with best-/last-iterate convergence, by contrast, the best-/last-iterate is already in product form, and one can directly output it as an approximate NE. Moreover, last-iterate convergence is known to be a more favorable metric than the average-iterate one in learning in games [45; 46; 47; 48; 49], which is able to characterize the _day-to-day_ behavior of the iterates and implies the stability of the update rule. Hence, one may prefer to have last-iterate convergence for solving zero-sum N(M)Gs. To this end, two algorithmic ideas may be useful: adding regularization to the payoff matrix [39; 42; 50; 51; 52], and/or using the idea of optimism [47; 36; 53]. Recent results [54; 51] have instantiated the ideas of _optimism-only_ and _optimism + regularization_, respectively, for best-/last-iterate convergence in zero-sum polymatrix games. We additionally established results for the idea of _regularization-only_ in obtaining last-iterate convergence in these games. Specifically, we propose to study the vanilla Multiplicative Weight Update (MWU) algorithm [55] in the regularized zero-sum NG, as tabulated in Algorithm 9. We have also introduced a variant with diminishing regularization, and summarize the update rule in Algorithm 10.

Given the results above, aggregating \(\epsilon\)-approximate NE for the zero-sum NGs \((\mathcal{G},\mathcal{A},(Q_{h,i,j}(s))_{(i,j)\in\mathcal{E}_{Q}})\) for all \(h\in[H],i\in\mathcal{N},s\in\mathcal{S}\) provides an \(H\epsilon\)-approximate NE for the corresponding zero-sum NMG. We have the following formal result.

**Proposition 4.** Suppose that for all \(h\in[H],i\in\mathcal{N},s\in\mathcal{S}\), \(\text{NE-ORACLE}(\mathcal{G},\mathcal{A},(Q_{h,i,j}(s))_{(i,j)\in\mathcal{E}_{Q}})\) provides an \(\epsilon_{h,s}\)-approximate NE for the zero-sum NG \((\mathcal{G},\mathcal{A},(Q_{h,i,j}(s))_{(i,j)\in\mathcal{E}_{Q}})\) in Algorithm 6. Then, the output policy \(\pi\) in Algorithm 6 is an \((\sum_{h\in[H]}\max_{s\in\mathcal{S}}\epsilon_{h,s})\)-approximate NE for the corresponding zero-sum NMG \((\mathcal{G}=(\mathcal{N},\mathcal{E}_{Q}),\mathcal{S},\mathcal{A},H,(\mathbb{ P}_{h})_{h\in[H]},(r_{h,i,j}(s))_{(i,j)\in\mathcal{E}_{Q},s\in\mathcal{S}})\).

The proof of Proposition 4 is deferred to Section F. In light of Proposition 4 and Table 1, we obtain Table 2, which summarizes the iteration complexities required to find an \(\epsilon\)-NE for zero-sum NMGs, with different NE-ORACLE subroutines. Note that the iteration complexities are all polynomial in \(H,n,|\mathcal{S}|\), and inherit the order of dependencies on \(\epsilon\) from Table 1 for the matrix-game case. In particular, Algorithm 6 with the OMWU in [51] yields the fast rate of \(\widetilde{\mathcal{O}}(1/\epsilon)\) for the last iterate.

## Acknowledgement

The authors would like to thank the anonymous reviewers of NeurIPS for their helpful feedback. C.P. acknowledges support from the Xianhong Wu Fellowship, the Korea Foundation for Advanced Studies, and the Siebel Scholarship. K.Z. acknowledges support from the Northrop Grumman - Maryland Seed Grant Program. A.O. acknowledges support from the MIT-Air Force AI Innovation Accelerator Grant and the MIT-DSTA grant 031017-00016.

## References

* [1] John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. _Princeton University Press_, 1947.
* [2] John F Nash Jr. Equilibrium points in n-person games. _Proceedings of the National Academy of sciences_, 36(1):48-49, 1950.
* [3] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge University press, 2006.
* [4] Drew Fudenberg and David K Levine. _The theory of learning in games_, volume 2. MIT Press, 1998.
* [5] George W Brown. Iterative solution of games by fictitious play. _Act. Anal. Prod Allocation_, 13(1):374, 1951.
* [6] Julia Robinson. An iterative method of solving a game. _Annals of mathematics_, pages 296-301, 1951.
* [7] Drew Fudenberg and David M Kreps. Learning mixed equilibria. _Games and Economic Behavior_, 5:320-367, 1993.
* [8] Dov Monderer and Lloyd S Shapley. Fictitious play property for games with identical interests. _Journal of economic theory_, 68(1):258-265, 1996.
* [9] Koichi Miyasawa. On the convergence of the learning process in a 2 x 2 non-zero-sum two-person game. _Princeton University Press_, 1961.
* [10] Ulrich Berger. Fictitious play in 2 x n games. _Journal of Economic Theory_, 120(2):139-154, 2005.
* [11] Aner Sela. Fictitious play in "one-against-all" multi-player games. _Economic Theory_, 14(3):635-651, 1999.
* [12] Christian Ewerhart and Kremena Valkanova. Fictitious play in networks. _Games and Economic Behavior_, 123:182-206, 2020.
* [13] Lloyd S Shapley. Stochastic games. _Proceedings of the national academy of sciences_, 39(10):1095-1100, 1953.
* [14] David S Leslie, Steven Perkins, and Zibo Xu. Best-response dynamics in zero-sum stochastic games. _Journal of Economic Theory_, 189:105095, 2020.
* [15] Muhammed Sayin, Kaiqing Zhang, David Leslie, Tamer Basar, and Asuman Ozdaglar. Decentralized Q-learning in zero-sum Markov games. _NeurIPS_, 2021.
* [16] Muhammed O Sayin, Francesca Parise, and Asuman Ozdaglar. Fictitious play in zero-sum stochastic games. _SIAM Journal on Control and Optimization_, 60(4):2095-2114, 2022.
* [17] Lucas Baudin and Rida Laraki. Fictitious play and best-response dynamics in identical interest and zero-sum stochastic games. _ICML_, 2022.
* [18] Zaiwei Chen, Kaiqing Zhang, Eric Mazumdar, Asuman Ozdaglar, and Adam Wierman. A finite-sample analysis of payoff-based independent learning in zero-sum stochastic games. _NeurIPS_, 2023.

* [19] Muhammed O Sayin, Kaiqing Zhang, and Asuman Ozdaglar. Fictitious play in markov games with single controller. _EC_, 2022.
* [20] Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of computing a nash equilibrium. _SIAM Journal on Computing_, 39(1):195-259, 2009.
* [21] Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player nash equilibria. _Journal of the ACM (JACM)_, 56(3):1-57, 2009.
* [22] Christos H Papadimitriou and Tim Roughgarden. Computing correlated equilibria in multiplayer games. _Journal of the ACM (JACM)_, 55(3):1-29, 2008.
* [23] Yang Cai and Constantinos Daskalakis. On minmax theorems for multiplayer games. _SODA_, 2011.
* [24] Yang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos Papadimitriou. Zero-sum polymatrix games: A generalization of minmax. _Mathematics of Operations Research_, 41(2):648-655, 2016.
* [25] Zhigang Cao, Haoyu Gao, Xinglong Qu, Mingmin Yang, and Xiaoguang Yang. Fashion, cooperation, and social interactions. _PLoS One_, 8(1):e49441, 2013.
* [26] Zhigang Cao and Xiaoguang Yang. The fashion game: Network extension of matching pennies. _Theoretical Computer Science_, 540:169-181, 2014.
* [27] Boyu Zhang, Zhigang Cao, Cheng-Zhong Qin, and Xiaoguang Yang. Fashion and homophily. _Operations Research_, 66(6):1486-1497, 2018.
* [28] L M Bergman and I N Fokin. On separable non-cooperative zero-sum games. _Optimization_, 44(1):69-84, 1998.
* [29] Yujia Jin, Vidya Muthukumar, and Aaron Sidford. The complexity of infinite-horizon general-sum stochastic games. _ITCS_, 2022.
* [30] Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. The complexity of markov equilibrium in stochastic games. _COLT_, 2023.
* [31] L M Bergman and I N Fokin. Methods of determining equilibrium situations in zero-sum polymatrix games. _Optimizatsia_, 40(57):70-82, 1987.
* [32] Christos H Papadimitriou. On the complexity of the parity argument and other inefficient proofs of existence. _Journal of Computer and System Sciences_, 48(3):498-532, 1994.
* [33] Janos Flesch, Frank Thuijsman, and Okko Jan Vrieze. Stochastic games with additive transitions. _European Journal of Operational Research_, 179(2):483-497, 2007.
* [34] Jerzy Filar and Koos Vrieze. _Competitive Markov decision processes_. Springer Science & Business Media, 2012.
* [35] Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods for competitive reinforcement learning. _NeurIPS_, 2020.
* [36] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive Markov games. _COLT_, 2021.
* [37] Yulai Zhao, Yuandong Tian, Jason D Lee, and Simon S Du. Provably efficient policy gradient methods for two-player zero-sum markov games. _AISTATS_, 2022.
* [38] Ziyi Chen, Shaocong Ma, and Yi Zhou. Sample efficient stochastic policy extragradient algorithm for zero-sum markov game. _ICLR_, 2021.
* [39] Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. _NeurIPS_, 2021.

* [40] Ahmet Alacaoglu, Luca Viano, Niao He, and Volkan Cevher. A natural actor-critic framework for zero-sum markov games. _ICML_, 2022.
* [41] Sihan Zeng, Thinh T Doan, and Justin Romberg. Regularized gradient descent ascent for two-player zero-sum markov games. _NeurIPS_, 2022.
* [42] Shicong Cen, Yuejie Chi, Simon S Du, and Lin Xiao. Faster last-iterate convergence of policy optimization in zero-sum markov games. _ICLR_, 2023.
* [43] Yakov Babichenko, Christos Papadimitriou, and Aviad Rubinstein. Can almost everybody be almost happy? pcp for ppad and the inapproximability of nash. _arXiv preprint arXiv:1504.02411_, 2015.
* [44] Michel Benaim, Josef Hofbauer, and Sylvain Sorin. Stochastic approximations and differential inclusions. _SIAM Journal on Control and Optimization_, 44(1):328-348, 2005.
* [45] Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. _SODA_, 2018.
* [46] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. _ICLR_, 2018.
* [47] Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. _ITCS_, 2019.
* [48] James P Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. _EC_, 2018.
* [49] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. _ICLR_, 2019.
* [50] Shicong Cen, Fan Chen, and Yuejie Chi. Independent natural policy gradient methods for potential games: Finite-time global convergence with entropy regularization. _CDC_, 2022.
* [51] Ruicheng Ao, Shicong Cen, and Yuejie Chi. Asynchronous gradient play in zero-sum multi-agent games. _ICLR_, 2023.
* [52] Sarath Pattathil, Kaiqing Zhang, and Asuman Ozdaglar. Symmetric (optimistic) natural policy gradient for multi-agent learning with parameter convergence. _AISTATS_, 2023.
* [53] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. _ICLR_, 2022.
* [54] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate convergence beyond zero-sum games. _ICML_, 2022.
* [55] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: A meta-algorithm and applications. _Theory of Computing_, 8(1):121-164, 2012.
* [56] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. _IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)_, 38(2):156-172, 2008.
* [57] Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. _Handbook of Reinforcement Learning and Control_, pages 321-384, 2021.
* [58] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. _Machine learning proceedings 1994_, pages 157-163, 1994.
* [59] Michael L Littman et al. Friend-or-foe q-learning in general-sum games. _ICML_, 2001.
* [60] Michael L Littman. Value-function reinforcement learning in markov games. _Cognitive systems research_, 2(1):55-66, 2001.

* [61] Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. _JMLR_, 2003.
* [62] Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. _ICML_, 2020.
* [63] Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player games with near-optimal time and sample complexity. _AISTATS_, 2020.
* [64] Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. _COLT_, 2020.
* [65] Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. _NeurIPS_, 2020.
* [66] Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. _ICML_, 2021.
* [67] Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. _NeurIPS_, 2020.
* [68] Gen Li, Yuejie Chi, Yuting Wei, and Yuxin Chen. Minimax-optimal multi-agent rl in Markov games with a generative model. _NeurIPS_, 2022.
* [69] Jayakumar Subramanian, Amit Sinha, and Aditya Mahajan. Robustness and sample complexity of model-based MARL for general-sum Markov games. _Dynamic Games and Applications_, pages 1-33, 2023.
* [70] Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum markov games with a large number of players sample-efficiently? _ICLR_, 2021.
* [71] Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning-a simple, efficient, decentralized algorithm for multiagent RL. _ICLR 2022 workshop "Gamification and Multiagent Solutions"_, 2022.
* [72] Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms for decentralized multi-agent reinforcement learning. _ICML_, 2022.
* [73] Weichao Mao and Tamer Basar. Provably efficient reinforcement learning in decentralized general-sum Markov games. _Dynamic Games and Applications_, pages 1-22, 2022.
* [74] Qiwen Cui, Kaiqing Zhang, and Simon S Du. Breaking the curse of multiagents in a large state space: RL in Markov games with independent linear function approximation. _COLT_, 2023.
* [75] Yuanhao Wang, Qinghua Liu, Yu Bai, and Chi Jin. Breaking the curse of multiagency: Provably efficient decentralized multi-agent RL with function approximation. _COLT_, 2023.
* [76] Liad Erez, Tal Lancewicki, Uri Sherman, Tomer Koren, and Yishay Mansour. Regret minimization and convergence to equilibria in general-sum markov games. _ICML_, 2023.
* [77] Aviad Rubinstein. Settling the complexity of computing approximate two-player Nash equilibria. _ACM SIGecom Exchanges_, 15(2):45-49, 2017.
* [78] Constantinos Daskalakis. Non-concave games: A challenge for game theory's next 100 years. 2022.
* [79] Matthew O Jackson and Yves Zenou. Games on networks. In _Handbook of game theory with economic applications_, volume 4, pages 95-163. Elsevier, 2015.
* [80] Michael Kearns, Michael L Littman, and Satinder Singh. Graphical models for game theory. _UAI_, 2001.
* [81] Sham Kakade, Michael Kearns, John Langford, and Luis Ortiz. Correlated equilibria in graphical games. _EC_, 2003.

* [82] Constantinos Daskalakis and Christos H Papadimitriou. On a network generalization of the minmax theorem. In _International Colloquium on Automata, Languages, and Programming_, pages 423-434. Springer, 2009.
* [83] Stefanos Leonardos, Georgios Piliouras, and Kelly Spendlove. Exploration-exploitation in multi-agent competition: convergence with bounded rationality. _NeurIPS_, 2021.
* [84] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-agent reinforcement learning with networked agents. _ICML_, 2018.
* [85] Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Networked multi-agent reinforcement learning in continuous spaces. _CDC_, 2018.
* [86] Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning for multiagent networked systems. _Operations Research_, 2022.
* [87] Xin Liu, Honghao Wei, and Lei Ying. Scalable and sample efficient distributed policy gradient algorithms in multi-agent networked systems. _arXiv preprint arXiv:2212.06357_, 2022.
* [88] Yizhou Zhang, Guannan Qu, Pan Xu, Yiheng Lin, Zaiwei Chen, and Adam Wierman. Global convergence of localized policy iteration in networked multi-agent reinforcement learning. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 7(1):1-51, 2023.
* [89] Zhaoyi Zhou, Zaiwei Chen, Yiheng Lin, and Adam Wierman. Convergence rates for localized actor-critic in networked markov potential games. _UAI_, 2023.
* [90] Georgios Piliouras, Lillian Ratliff, Ryann Sim, and Stratis Skoulakis. Fast convergence of optimistic gradient ascent in network zero-sum extensive form games. _SAGT_, 2022.
* [91] Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. _Connection Science_, 3(3):241-268, 1991.
* [92] Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. 2010.
* [93] Gergely Neu, Anders Jonsson, and Vicenc Gomez. A unified view of entropy-regularized Markov decision processes. _NeurIPS_, 2017.
* [94] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. _ICML_, 2017.
* [95] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. _ICML_, 2020.
* [96] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. _Operations Research_, 70(4):2563-2578, 2022.
* [97] Mingyang Liu, Asuman E. Ozdaglar, Tiancheng Yu, and Kaiqing Zhang. The power of regularization in solving extensive-form games. _ICLR_, 2023.
* [98] Samuel Sokota, Ryan D'Orazio, J Zico Kolter, Nicolas Loizou, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, and Christian Kroer. A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games. _ICLR_, 2023.
* [99] Lloyd Shapley. Some topics in two-person games. _Advances in game theory_, 52:1-29, 1964.
* [100] Muhammed O Sayin. On the global convergence of stochastic fictitious play in stochastic games with turn-based controllers. _CDC_, 2022.
* [101] Asuman Ozdaglar, Muhammed O Sayin, and Kaiqing Zhang. Independent learning in stochastic games. _International Congress of Mathematicians_, 2022.
* [102] Fivos Kalogiannis and Ioannis Panageas. Zero-sum polymatrix Markov games: Equilibrium collapse and efficient computation of Nash equilibria. _NeurIPS_, 2023.

* [103] Jens Grossklags, Nicolas Christin, and John Chuang. Secure or insure? a game-theoretic analysis of information security games. _WWW_, 2008.
* [104] Robert Wilson. Game-theoretic analysis of trading processes. Technical report, Stanford University, 1985.
* [105] Robert Wilson. Game theoretic analysis of trading. In _Advances in Economic Theory: Fifth World Congress_, number 12, page 33. CUP Archive, 1989.
* [106] Dayong Zhang, Lei Lei, Qiang Ji, and Ali M Kutan. Economic policy uncertainty in the us and china and their impact on the global markets. _Economic Modelling_, 79:47-56, 2019.
* [107] Michael Beckley. The power of nations: Measuring what matters. _International Security_, 43(2):7-44, 2018.
* [108] Charles Freedman, Michael Kumhof, Douglas Lxaton, Dirk Muir, and Susanna Mursula. Global effects of fiscal stimulus during the crisis. _Journal of monetary economics_, 57(5):506-526, 2010.
* [109] Klaus Armingeon. The politics of fiscal responses to the crisis of 2008-2009. _Governance_, 25(4):543-565, 2012.
* [110] Fabio Augusto Reis Gomes, Sergio Naruhiko Sakurai, and Gian Paulo Soave. Government spending multipliers in good times and bad times: The case of emerging markets. _Macroeconomic Dynamics_, 26(3):726-768, 2022.
* [111] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form games. _Games and economic behavior_, 10(1):6-38, 1995.
* [112] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for extensive form games. _Experimental economics_, 1:9-41, 1998.
* [113] Panayotis Mertikopoulos and William H Sandholm. Learning in games via reinforcement and regularization. _Mathematics of Operations Research_, 41(4):1297-1324, 2016.
* [114] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. _NeurIPS_, 2021.
* [115] Ioannis Anagnostides, Gabriele Farina, Christian Kroer, Chung-Wei Lee, Haipeng Luo, and Tuomas Sandholm. Uncoupled learning dynamics with \(o(\backslash\log t)\) swap regret in multiplayer games. _NeurIPS_, 2022.
* [116] Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer, and Tuomas Sandholm. Near-optimal no-regret learning dynamics for general convex games. _NeurIPS_, 2022.
* [117] David Reeb and Michael M Wolf. Tight bound on relative entropy by entropy difference. _IEEE Transactions on Information Theory_, 61(3):1458-1473, 2015.
* [118] Haipeng Luo. Introduction to online optimization/learning (fall 2022), lecture note 1.
* [119] Yang Cai, Haipeng Luo, Chen-Yu Wei, and Weiqiang Zheng. Uncoupled and convergent learning in two-player zero-sum markov games. _arXiv preprint arXiv:2303.02738_, 2023.