# Free Lunch in Pathology Foundation Model:

Task-specific Model Adaptation with Concept-Guided Feature Enhancement

 Yanyan Huang

The University of Hong Kong

yanyanh@connect.hku.hk

&Weiqin Zhao

The University of Hong Kong

wqzhao98@connect.hku.hk

&Yihang Chen

The University of Hong Kong

yihangc@connect.hku.hk

&Yu Fu

Lanzhou University

fuyu@lzu.edu.cn

&Lequan Yu

The University of Hong Kong

lqyu@hku.hk

Corresponding Author.

###### Abstract

Whole slide image (WSI) analysis is gaining prominence within the medical imaging field. Recent advances in pathology foundation models have shown the potential to extract powerful feature representations from WSIs for downstream tasks. However, these foundation models are usually designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types. In this work, we present _Concept_**An**_chor-guided_**T**_ask-specific Feature_**_E**_nhancement_ (CATE), an adaptable paradigm that can boost the expressivity and discriminativeness of pathology foundation models for specific downstream tasks. Based on a set of task-specific concepts derived from the pathology vision-language model with expert-designed prompts, we introduce two interconnected modules to dynamically calibrate the generic image features extracted by foundation models for certain tasks or cancer types. Specifically, we design a Concept-guided Information Bottleneck module to enhance task-relevant characteristics by maximizing the mutual information between image features and concept anchors while suppressing superfluous information. Moreover, a Concept-Feature Interference module is proposed to utilize the similarity between calibrated features and concept anchors to further generate discriminative task-specific features. The extensive experiments on public WSI datasets demonstrate that CATE significantly enhances the performance and generalizability of MIL models. Additionally, heatmap and umap visualization results also reveal the effectiveness and interpretability of CATE. The source code is available at https://github.com/HKU-MedAI/CATE.

## 1 Introduction

Multiple Instance Learning (MIL) [26; 34; 23; 2] is widely adopted for weakly supervised analysis in computational pathology, where the input of MIL is typically a set of patch features generated by a pre-trained feature extractor (_i.e._, image encoder). Although promising progress has been achieved, the effectiveness of MIL models heavily relies on the quality of the extracted features. A robust feature extractor can discern more distinctive pathological features, thereby improving the predictive capabilities of MIL models. Recently, several studies have explored using pretrained foundation models on large-scale pathology datasets with self-supervised learning as the feature extractors for WSI analysis [37; 7; 3; 36]. Additionally, drawing inspiration from the success of ContrastiveLanguage-Image Pretraining (CLIP) [30; 20] in bridging visual and linguistic modalities, some works have aimed to develop a pathology vision-language foundation model (VLM) to simultaneously learn representations of pathology images and their corresponding captions [15; 25]. The intrinsic consistency between the image feature space and caption embedding space in the pathology VLM enables the image encoder to extract more meaningful and discriminative features for downstream WSI analysis applications [25].

Although the development of these pathology foundation models has significantly advanced computational pathology, these models are designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types, as the features extracted by the image encoder may contain generic yet task-irrelevant information that will harm the performance of specific downstream tasks. For example, as illustrated in Figure 1(a), the features extracted by the image encoder of a pathology VLM can include both task-relevant information (_e.g._, arrangement or morphology of tumor cells) and task-irrelevant elements(such as background information, stain styles, etc.). The latter information may act as "noise", distracting the learning process of MIL models tailored to specific tasks, and potentially impairing the generalization performance of these models across different data sources. Consequently, it is crucial to undertake task-specific adaptation to enhance feature extraction of generic foundation models and enable MIL models to concentrate on task-relevant information and thus improve analysis performance and generalization [32; 38].

In this paper, we propose a novel paradigm, named **C**oncept **A**nctor-guided **T**ask-specific **F**a**hancement (CATE), to enhance the generic features extracted by the pathology VLM for specific downstream tasks (_e.g._, cancer subtyping). Without requiring additional supervision or significant computational resources, CATE offers an approximately _"free lunch"_ in the context of pathology VLM. Specifically, we first derive a set of task-specific concept anchors from the pathology VLM with task-specific prompts, and these prompts rely on human expert design or are generated through querying large language models (LLMs), necessitating a certain level of pathological background knowledge. Based on these concept anchors, we design two concept-driven modules, _i.e._, the Concept-guided Information Bottleneck (CIB) module and the Concept-Feature Interference (CFI) module, to calibrate and generate task-specific features for downstream analysis. Particularly, with the task-specific concepts as the guidance, the CIB module enhances task-relevant features by maximizing the mutual information between the image features and the concept anchors and also eliminates task-irrelevant information by minimizing the superfluous information, as shown in Figure 1(a). Moreover, the CFI module further generates discriminative task-specific features by utilizing the similarities between the calibrated image features and concept anchors (_i.e._, concept scores). By incorporating the CATE into existing MIL frameworks, we not only obtain more discriminative features but also improve generalization regarding domain shift by eliminating task-irrelevant features and concentrating on pertinent information, as shown in Figure 1(b).

In summary, the main contributions of this work are threefold:

* We introduce a novel method, named **CATE**, for model adaptation in computational pathology. To the best of our knowledge, this is the first initiative to conduct _task-specific_ feature enhancement based on the pathology foundation model for MIL tasks.
* We design a new **CIB** module to enhance the task-relevant information and discard irrelevant information with the guidance of task-specific concepts, and a new **CFI** module to generate task-specific features by exploiting the similarities between image features and concept anchors.

Figure 1: (a) Illustration of the key idea of concept-guided information bottleneck to enhance the task-relevant information and discard the task-irrelevant information. (b) Task-specific model adaptation with CATE to enhance the generalization across different data sources.

* Extensive experiments on Whole Slide Image (WSI) analysis tasks demonstrate that CATE significantly enhances the performance and generalization capabilities of MIL models.

## 2 Related Work

**Multiple Instance Learning (MIL) for WSI Analysis.** MIL is the predominant paradigm for WSI analysis, treating each WSI as a bag of patch instances and classifying the entire WSI based on aggregated patch-level features. Attention-based methods [16; 26; 17; 39] are highly regarded for their ability to determine the significance of each instance within the bag. For instance, Ilse _et al._[16] introduced an attention-based MIL model, while Lu _et al._[26] proposed clustering-constrained-attention to refine this mechanism further. To model the relationships among instances, graph-based and Transformer-based methods have been developed [10; 2; 13]. For example, Chen _et al._[2] introduced a Transformer-based hierarchical network to capitalize on the inherent hierarchical structure of WSIs.

**Pathology Foundation Model.** With the advancement of foundation models in computer vision, several pathology foundation models have been developed to serve as robust image encoders for WSI analysis. Riasatian _et al._[31] proposed fine-tuning the DenseNet [12] on the TCGA dataset, while Filiot _et al._[7] utilized iBOT [42] to pretrain a vision Transformer using the Masked Image Modeling framework. Recently, Chen _et al._[3] pre-trained a general-purpose foundation model on large-scale pathology datasets using DINov2 [28], which has demonstrated strong and readily usable representations for WSI analysis. Inspired by CLIP [30], Ikezogwo _et al._[15], Huang _et al._[14], and Lu _et al._[25] developed vision-language foundation models by training on large-scale pathology datasets with image-caption pairs. These foundation models have demonstrated superior performance in downstream tasks due to their ability to extract more discriminative features for WSI analysis.

**Feature Enhancement in Computational Pathology.** Several methods have been developed to obtain more discriminative features for WSI analysis by adapting pathology foundation models [41; 24] or designing new plug-and-play modules [35]. For instance, Zhang _et al._[41] suggested aligning the image features with text features extracted from a pre-trained natural language model to enhance the feature representation of WSI patch images, while it operates solely at the patch level, without considering the informational relationship between image and text features. Recently, Tang _et al._[35] introduced Re-embedded Regional Transformer for feature re-embedding, aimed at enhancing WSI analysis when integrated with existing MIL methods. However, while this method considers the spatial information of WSIs and adds flexibility to MIL models, it falls short in extracting task-specific discriminative information for WSI analysis.

## 3 Method

### Overview

The proposed CATE can be seamlessly integrated with any MIL framework to adapt the existing pathology foundation model (Pathology VLM) for performance-improved WSI analysis via task-specific enhancement. Specifically, consider a training set \(\mathcal{D}=\{(\mathbf{x},\mathbf{y})\}\) of WSI-label pairs, where \(\mathbf{x}=\{\bm{x}_{1},\bm{x}_{2},...,\bm{x}_{N}\}\) is a set of patch features with dimension of \(C\) (_i.e._, \(\bm{x}_{i}\in\mathbb{R}^{C}\)) extracted by the image encoder of pathology VLM, \(N\) denotes the number of patches, and \(\mathbf{y}\) is the corresponding label. The objective of CATE is to obtain the corresponding enhanced task-specific feature set \(\mathbf{z}\) from the original feature \(\mathbf{x}\) with the guidance of pre-extracted concepts anchors \(\mathbf{c}\) (see description below) for downstream usage:

\[\mathbf{z}=\mathbf{CATE}\left(\mathbf{x},\mathbf{c}\right),\hat{\mathbf{y}}= \mathrm{MIL}\left(\mathbf{z}\right).\] (1)

As illustrated in Figure 2, we design two different modules to enhance the extracted features from foundation models: (1) Concept-guided Information Bottleneck (CIB) module calibrates original image features with the guidance of concept anchors with information bottleneck principle; and (2) Concept-Feature Interference (CFI) module generates discriminative task-specific features by leveraging the similarities between the calibrated image features and concept anchors. Specifically, the enhanced patch features can be represented as \(\mathbf{z}=\{\bm{z}_{1},\bm{z}_{2},...,\bm{z}_{N}\}\), where \(\bm{z}_{i}\) is the concatenation of the calibrated feature \(\bm{\alpha}_{i}\) and the interference feature \(\bm{\beta}_{i}\) generated by CIB and CFI module:

\[\bm{z}_{i}=\mathrm{Concat}\left[\bm{\alpha}_{i},\bm{\beta}_{i}\right]=\mathrm{ Concat}\left[\mathbf{CIB}\left(\bm{x}_{i},\mathbf{c}\right),\mathbf{CFI}\left( \bm{x}_{i},\mathbf{c}\right)\right].\] (2)

**Concept Extraction.** We extract two kinds of task-specific concept anchors, \(\mathbf{c}=\{\mathbf{c}^{\mathrm{cs}},\mathbf{c}^{\mathrm{ca}}\}\), comprising class-specific concepts \(\mathbf{c}^{\mathrm{cs}}=\{\mathbf{c}^{\mathrm{cs}}_{i}\}_{i=1}^{m}\) (_e.g._, subtyping classes) and class-agnostic concepts \(\mathbf{c}^{\mathrm{ca}}=\{\mathbf{c}^{\mathrm{ca}}_{i}\}_{i=1}^{n}\) (_e.g._, adipose, connective, and normal tissues), with \(m\) and \(n\) representing the numbers of class-specific and class-agnostic concepts, respectively. These concepts are generated by the text encoder of pathology VLM with prompt \(\mathbf{p}\). Each prompt consists of a class name (_e.g._, "invasive ductal carcinoma") and a template (_e.g._, "An image of <_CLASSNAME_>"). To obtain more robust concepts, we use multiple prompts for each class and the final concept anchor is the average of the embeddings generated by different prompts. Details of class names and templates for various tasks are provided in Appendix G. Note that due to the inherent consistency between the image and text embedding space in VLM, these extracted concepts can also be regarded as image concept vectors.

### Concept-guided Information Bottleneck

The objective of this module is to find a distribution \(p(\boldsymbol{\alpha}|\mathbf{x})\) that maps the original image feature \(\mathbf{x}\) into a representation \(\boldsymbol{\alpha}\), which contains enhanced task-discriminative characteristics and suppressed task-irrelevant information. WSIs typically contain various cell types or tissues (_e.g._, tumor cells, normal cells, adipose tissue, connective tissue), while only a subset of patches (_e.g._, with tumor cells) is crucial for certain tasks such as tumor subtyping. We thus define \(\hat{\mathbf{x}}=\{\hat{\boldsymbol{x}}_{i}\}_{i=1}^{k}\subseteq\mathbf{x}\) as the representative subset of the original feature set (_e.g._, tumor tissue patches), where \(k\) denotes the number of representative patches. Note that this selection can be conducted with a simple comparison of image features with class-specific concepts (see discussion below). To this end, the corresponding enhanced feature set is \(\hat{\boldsymbol{\alpha}}=\{\hat{\boldsymbol{\alpha}}_{i}\}_{i=1}^{k}\subseteq \boldsymbol{\alpha}\) and we want to find the conditional distribution \(p(\hat{\boldsymbol{\alpha}}_{i}|\hat{\boldsymbol{x}}_{i})\) to map the selected patch feature \(\hat{\boldsymbol{x}}_{i}\in\mathbb{R}^{C}\) into a more discriminative enhanced feature \(\hat{\boldsymbol{\alpha}}_{i}\in\mathbb{R}^{C}\), which is discriminative enough to identify the label \(\mathbf{y}\).

**Sufficiency and Consistency Requirements.** To quantify the informativeness requirement of the calibrated feature \(\hat{\boldsymbol{\alpha}}\), we consider the **sufficiency** of \(\hat{\boldsymbol{\alpha}}\) for \(\mathbf{y}\). As defined in Appendix E.1, the encoded feature \(\hat{\boldsymbol{\alpha}}\) derived from the original feature \(\hat{\boldsymbol{x}}\) is sufficient for determining the label \(\mathbf{y}\) if and only if the amount of task-specific information remains unchanged after calibration, _i.e._, \(I(\hat{\boldsymbol{x}};\mathbf{y})=I(\hat{\boldsymbol{\alpha}};\mathbf{y})\). However, the label \(\mathbf{y}\) pertains to the slide level and specific labels cannot be assigned to each instance due to the absence of patch-level annotations.

To address this challenge, we propose using the task-specific concept anchor as the guidance for each single \(\hat{\boldsymbol{\alpha}}\). Specifically, we posit that the concept anchor \(\mathbf{c}\) is distinguishable for the task and contains task-relevant information for label \(\mathbf{y}\). Given the consistency between image and text features in pathology VLM, any representation \(\hat{\boldsymbol{\alpha}}\) containing all information accessible from both image feature \(\hat{\boldsymbol{x}}\) and concept \(\mathbf{c}\) will also encapsulate the discriminative information required for the

Figure 2: (a) Overview of CATE: the outputs of the CIB and CFI modules are concatenated to form the enhanced feature for downstream MIL models. (b) Task-relevant concept generation. (c) Concept-guided Information Bottleneck (CIB) module. (c) Concept-Feature Interference (CFI) module.

label. This **consistency** requirement is detailed in Appendix E.1. Thus, if \(\hat{\bm{\alpha}}\) is sufficient for \(\mathbf{c}\) (_i.e._, \(I(\hat{\bm{x}};\mathbf{c}|\hat{\bm{\alpha}})=0\)), then \(\hat{\bm{\alpha}}\) is as predictive for label \(\mathbf{y}\) as the joint of original feature \(\hat{\bm{x}}\) and concept anchor \(\mathbf{c}\). Applying the chain rule of mutual information, we derive:

\[I(\hat{\bm{x}};\hat{\bm{\alpha}})=\underbrace{I(\hat{\bm{\alpha}};\mathbf{c})}_ {\text{Predictive Information}}+\underbrace{I(\hat{\bm{x}};\hat{\bm{\alpha}}| \mathbf{c})}_{\text{Superfluous Information}}.\] (3)

According to the consistency between the concept anchor and original feature, the mutual information term \(I(\hat{\bm{\alpha}};\mathbf{c})\) represents the predictive information for the task, while the conditional information term \(I(\hat{\bm{x}};\hat{\bm{\alpha}}|\mathbf{c})\) denotes task-irrelevant information (_i.e._, superfluous information) in original patch feature \(\hat{\bm{x}}\), which can be minimized to enhance the robustness and generalization ability of downstream MIL models. As a result, the main objective of the feature calibration in this module can be formalized as _maximize predictive information_\(I(\hat{\bm{\alpha}};\mathbf{c})\) while _minimize the superfluous information_\(I(\hat{\bm{x}};\hat{\bm{\alpha}}|\mathbf{c})\).

**Predictive Information Maximization (PIM).** The predictive information in Equ (3) equals to the mutual information between the calibrated feature and concept anchors. To maximize this, we choose the InfoNCE [27] to estimate the lower bound of the mutual information, which can be obtained by comparing positive pairs sampled from the joint distribution \(\hat{\bm{\alpha}},\bm{c}_{\text{pos}}^{\text{cs}}\sim p(\hat{\bm{\alpha}}, \bm{c}^{\text{cs}})\) to pairs \(\hat{\bm{\alpha}},\bm{c}_{j}^{\text{cs}}\) and \(\hat{\bm{\alpha}},\bm{c}_{j}^{\text{cs}}\) built using a set of negative class concepts \(\bm{c}_{j}^{\text{cs}}\sim p\left(\bm{c}^{\text{cs}}\right)\) and class-agnostic concepts \(\bm{c}_{j}^{\text{ca}}\sim p\left(\bm{c}^{\text{ca}}\right)\):

\[I_{NCE}\left(\hat{\bm{\alpha}};\mathbf{c}\right)=\mathop{\mathbb{E}}_{\hat{ \bm{\alpha}},\bm{c}_{\text{pos}}^{\text{cs}}}\left[\sum_{i=1}^{k}\log\frac{f \left(\bm{c}_{\text{pos}}^{\text{cs}},\hat{\bm{\alpha}}_{i}\right)}{\sum_{j= 1}^{m}f\left(\bm{c}_{j}^{\text{cs}},\hat{\bm{\alpha}}_{i}\right)+\sum_{j=1}^{ n}f\left(\bm{c}_{j}^{\text{ca}},\hat{\bm{\alpha}}_{i}\right)}\right].\] (4)

We set \(f\left(\mathbf{c},\hat{\bm{\alpha}}_{i}\right)=\exp\left(\hat{\bm{\alpha}}_{i} ^{\mathsf{T}}\mathbf{c}/\tau\right)\) with \(\tau>0\) in practice following [27]. By maximizing this mutual information lower bound, \(f\left(\mathbf{c},\hat{\bm{\alpha}}_{i}\right)\) will be proportional to the density ratio \(p\left(\mathbf{c},\hat{\bm{\alpha}}_{i}\right)/p\left(\mathbf{c}\right)p\left( \hat{\bm{\alpha}}_{i}\right)\) as proved in [27]. Hence, \(f\left(\mathbf{c},\hat{\bm{\alpha}}_{i}\right)\) preserves the mutual information between the calibrated feature and concept anchor. The detailed derivation can be found in Appendix E.2. The loss function for PIM can be denoted as:

\[\mathcal{L}_{PIM}=\mathop{\mathbb{E}}_{\hat{\bm{\alpha}},\bm{c}_{\text{pos}}^ {\text{cs}}}\left[-\sum_{i=1}^{k}\frac{\hat{\bm{\alpha}}_{i}^{\mathsf{T}}\bm{c }_{\text{pos}}^{\text{cs}}}{\tau}\right]+\mathop{\mathbb{E}}_{\hat{\bm{\alpha }},\bm{c}_{\text{pos}}^{\text{cs}}}\left[\sum_{i=1}^{k}\log\left(\sum_{j=1}^{ m}\exp\frac{\hat{\bm{\alpha}}_{i}^{\mathsf{T}}\bm{c}_{j}^{\text{cs}}}{\tau}+ \sum_{j=1}^{n}\exp\frac{\hat{\bm{\alpha}}_{i}^{\mathsf{T}}\bm{c}_{j}^{\text{ ca}}}{\tau}\right)\right].\] (5)

**Superfluous Information Minimization (SIM).** To compress task-irrelevant information, we aim to minimize the superfluous information term as defined in Equ (3). This objective can be achieved by minimizing the mutual information \(I(\hat{\bm{x}};\hat{\bm{\alpha}})\). In practice, we conduct SIM for _all patches_ in the subset \(\mathbf{x}\), as each patch may contain task-irrelevant information. Following [1], it can be represented as:

\[I(\bm{x};\bm{\alpha})=\int p\left(\bm{x},\bm{\alpha}\right)\log p\left(\bm{ \alpha}|\bm{x}\right)d\bm{x}d\bm{\alpha}-\int p\left(\bm{\alpha}\right)\log p \left(\bm{\alpha}\right)d\bm{\alpha}.\] (6)

After that, we let the distribution of \(\bm{\alpha}\): \(r\left(\bm{\alpha}\right)\) (_e.g._, Gaussian distribution in this work), be a variational approximation to the marginal distribution \(p\left(\bm{\alpha}\right)\), and we can obtain the upper bound for \(I(\bm{x};\bm{\alpha})\):

\[I(\bm{x};\bm{\alpha})\leq\int p\left(\bm{x}\right)p\left(\bm{\alpha}|\bm{x} \right)\log\frac{p\left(\bm{\alpha}|\bm{x}\right)}{r\left(\bm{\alpha}\right)}d \bm{x}d\bm{\alpha}.\] (7)

Furthermore, we use a variational distribution \(q_{\theta}\left(\bm{\alpha}|\bm{x}\right)\) with parameter \(\theta\) to approximate \(p\left(\bm{\alpha}|\bm{x}\right)\) and we implement the parameterization of the variational distribution with MLP by predicting the mean and variance of the Gaussian distribution and sample the calibrated feature \(\bm{\alpha}\) from this distribution:

\[\bm{\alpha}\sim\mathcal{N}\left(MLP^{\mu}\left(\bm{x}\right),MLP^{\Sigma} \left(\bm{x}\right)\right).\] (8)

In practice, we implement this by utilizing the reparameterization trick [18] to obtain an unbiased estimate of the gradient and further optimize the variational distribution. The detailed derivation can be found in Appendix E.3. The minimization of the upper bound of \(I(\bm{x};\bm{\alpha})\) equals to the minimization of the Kullback-Leibler divergence between \(q_{\theta}\left(\bm{\alpha}|\bm{x}\right)\) and \(r\left(\bm{\alpha}\right)\). Therefore, the loss function can be represented as:

\[\mathcal{L}_{SIM}=\mathbb{E}\left[\sum_{i=1}^{k}D_{KL}\left(q_{\theta}\left( \bm{\alpha}_{i}|\bm{x}_{i}\right)||r\left(\bm{\alpha}_{i}\right)\right)\right].\] (9)

**Discussion.** We further provide explanation of CIB module with the information plane [8; 6] in Appendix F. It should be noted that the PIM supervises only the representative subset \(\hat{\mathbf{x}}\) containing task-relevant information (selected by the similarity between image features and corresponding class-specific concepts). Meanwhile, the SIM is applied to all patches in \(\mathbf{x}\), as any patch may carry information irrelevant to the task (_e.g._, background information and stain styles). Besides, SIM cannot be directly optimized without the guidance of concept anchors (_i.e._, PIM) due to the absence of patch-level labels. As demonstrated in the ablation study in Section 4.4, the absence of concept anchor guidance leads to the collapse of discriminative information in the calibrated feature, adversely affecting downstream task performance. By maximizing predictive information and minimizing superfluous details, the CIB module effectively enhances the discriminative capacity of the original features and aligns them with the task-specific concept anchors for improved prediction.

### Concept-Feature Interference

We also propose the Concept-Feature Interference (CFI) module to utilize the similarity characteristic between calibrated features and concept anchors to further obtain robust and discriminative information for the downstream tasks. Our primary focus is on the class-specific concept anchors \(\mathbf{c}^{\text{cs}}\). Specifically, for each CIB encoded feature \(\boldsymbol{\alpha}_{i}\), we calculate the cosine similarity between \(\boldsymbol{\alpha}_{i}\) and each class-specific concept \(\boldsymbol{c}^{\text{cs}}_{i}\). It is important to note that the number of class-specific concepts \(m\) is larger than the number of classes, as we use multiple <_CLASSNAME_> and templates to generate the concept anchor for each class, as shown in the Appendix G. Thus, we can obtain the similarity vector by concatenating the similarity scores between \(\boldsymbol{\alpha}_{i}\) and each class-specific concept \(\boldsymbol{c}^{\text{cs}}_{i}\). To integrate the interference information (similarity relationship) into the enhanced feature, we align the similarity vector with the calibrated feature \(\boldsymbol{\alpha}_{i}\) using a Self-Normalizing Network (SNN) layer [19]. This allows us to obtain the final interference vector \(\boldsymbol{\beta}_{i}\) of CFI:

\[\boldsymbol{\beta}_{i}=\mathrm{SNN}\left(\mathrm{Concat}\left[\{\mathrm{Sim} \left(\boldsymbol{\alpha}_{i},\boldsymbol{c}^{\text{cs}}_{i}\right)\}_{i=1}^{ m}\right]\right).\] (10)

The interference vector contains superficial information that indicates the similarity between the calibrated feature and concept anchor directly. This is completely different from the calibrated feature of the CIB module, which contains discriminative latent information for the downstream tasks. Therefore, integrating the interference feature can further provide robust and discriminative information for the downstream tasks.

**Discussion.** The CFI module is designed to utilize the similarity characteristic between calibrated feature and concept anchor as a discriminative feature, which can be further integrated into the calibrated feature for downstream tasks. This is different from other studies that directly compare the similarity between visual features and textual concept features of different classes to perform zero-shot classification [25].

### Training Objective

The overall training objective of the CATE framework can be represented as the combination of the cross entropy loss \(\mathcal{L}_{CE}\) for the downstream tasks, the predictive information maximization loss \(\mathcal{L}_{PIM}\), and the superfluous information minimization loss \(\mathcal{L}_{SIM}\):

\[\mathcal{L}=\mathcal{L}_{CE}+\lambda_{P}\mathcal{L}_{PIM}+\lambda_{S}\mathcal{ L}_{SIM},\] (11)

where \(\lambda_{P}\) and \(\lambda_{S}\) are hyperparameters and influence of them is discussed in Appendix C.

## 4 Experiments

### Experimental Settings

**Tasks and Datasets.** We conducted cancer subtyping tasks on three public WSI datasets from The Cancer Genome Atlas (TCGA) project: Invasive Breast Carcinoma (BRCA), Non-Small Cell Lung Cancer (NSCLC), and Renal Cell Carcinoma (RCC). Detailed dataset information is available in Appendix D.

[MISSING_PAGE_FAIL:7]

TransMIL [33], DTFD-MIL [40], and R\({}^{2}\)T-MIL [35]. The R\({}^{2}\)T-MIL [35] is a feature re-embedding method that utilizes ABMIL as the base MIL model.

**Implementation Details.** This study begins the image feature extraction process by segmenting the foreground tissue and then splitting the WSI into 512\(\times\)512 pixels patches at 20\(\times\) magnification. Subsequently, these patches are processed through a pre-trained image encoder from CONCH [25] to extract image features. For concept anchors, we utilize CONCH's text encoder to derive task-relevant concepts from predefined text prompts, with detailed prompt information available in Appendix G. Model parameters are optimized using the Adam optimizer with a learning rate of \(10^{-5}\). The batch size is set to 1, and all the experiments are conducted on a single NVIDIA RTX 3090 GPU.

### Experimental Results

**Quantitative Results on BRCA Dataset.** To fully evaluate the effectiveness of CATE, we assessed its impact on several state-of-the-art MIL models using the BRCA dataset. The results are shown in Table 1, where the MIL models integrated with CATE outperform their original counterparts in both in-domain (IND) and out-of-domain (OOD) testing, which demonstrates the effectiveness and generalization capabilities of CATE. Comparing with R\({}^{2}\)T-MIL, which is a feature re-embedding method that utilizes ABMIL as the base MIL model, CATE incorporated with ABMIL consistently achieves better performance in terms of both OOD and IND testing. To further investigate the effectiveness of CATE, we conducted experiments by altering the in-domain sites and applying traditional settings. Detailed results are available in Appendix B.

**Qualitative Analysis.** To qualitatively investigate the effectiveness of CATE, we visualized attention heatmaps, UMAP, and the similarities between original features and corresponding class concept features, as well as calibrated features and class concept features, as shown in Figure 3. Additional visualization results are provided in Appendix H. As shown in Figure 3 (a&b), attention heatmap comparisons reveal that CATE-MIL focuses more intensely on cancerous regions, with a clearer delineation between high and low attention areas. By comparing the similarities of original and calibrated features to class concept features in Figure 3 (c&d), it is evident that the enhanced similarity in cancerous regions is significantly higher than in original features. Moreover, the disparity between cancerous and non-cancerous regions' similarities is also expanded, which further verifies the ability of CATE to enhance task-relevant information and suppress irrelevant information. We further performed a UMAP visualization of class concept features, original features, and calibrated features. As depicted in Figure 3 (f), calibrated features are notably closer to the corresponding class (IDC)

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{NSCLC (\(N_{\text{IND}}\)=2)} & \multicolumn{4}{c}{NSCLC (\(N_{\text{IND}}\)=4)} \\ \cline{2-10}  & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{\textbf{*}}\) & IND-ACC\({}^{\textbf{*}}\) & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{\textbf{*}}\) & IND-ACC\({}^{\textbf{*}}\) \\ \hline ABMIL & 0.874\(\pm\)0.021 & 0.803\(\pm\)0.021 & 0.997\(\pm\)0.004 & 0.954\(\pm\)0.028 & 0.951\(\pm\)0.023 & 0.883\(\pm\)0.029 & 0.974\(\pm\)0.018 & 0.910\(\pm\)0.006 \\ CLAM & 0.875\(\pm\)0.029 & 0.801\(\pm\)0.021 & 0.997\(\pm\)0.007 & 0.963\(\pm\)0.024 & 0.931\(\pm\)0.037 & 0.870\(\pm\)0.006 & 0.977\(\pm\)0.023 & 0.926\(\pm\)0.048 \\ DSMIL & 0.839\(\pm\)0.046 & 0.764\(\pm\)0.031 & 0.993\(\pm\)0.004 & 0.963\(\pm\)0.028 & 0.934\(\pm\)0.019 & 0.864\(\pm\)0.026 & 0.974\(\pm\)0.013 & 0.913\(\pm\)0.042 \\ DTFD-MIL & 0.903\(\pm\)0.023 & 0.836\(\pm\)0.026 & 0.990\(\pm\)0.009 & 0.958\(\pm\)0.049 & 0.949\(\pm\)0.010 & 0.893\(\pm\)0.012 & 0.981\(\pm\)0.012 & 0.918\(\pm\)0.040 \\ TransMIL & 0.790\(\pm\)0.028 & 0.712\(\pm\)0.024 & 0.997\(\pm\)0.004 & 0.954\(\pm\)0.033 & 0.917\(\pm\)0.022 & 0.832\(\pm\)0.031 & 0.977\(\pm\)0.041 & 0.923\(\pm\)0.029 \\ R\({}^{2}\)T-MIL & 0.739\(\pm\)0.088 & 0.690\(\pm\)0.025 & 0.999\(\pm\)0.002 & 0.971\(\pm\)0.036 & 0.892\(\pm\)0.041 & 0.800\(\pm\)0.059 & 0.977\(\pm\)0.018 & 0.916\(\pm\)0.045 \\ \hline
**CATE-MIL** & **0.945\(\pm\)**0.016 & **0.840\(\pm\)**0.033 & 0.985\(\pm\)**0.011 & 0.938\(\pm\)0.037 & **0.969\(\pm\)**0.003 & **0.906\(\pm\)**0.011 & 0.967\(\pm\)**0.019 & 0.905\(\pm\)0.054 \\ \hline  & \multicolumn{4}{c}{RCC (\(N_{\text{IND}}\)=3)} & \multicolumn{4}{c}{RCC (\(N_{\text{IND}}\)=6)} \\ \cline{2-10}  & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{\textbf{*}}\) & IND-ACC\({}^{\textbf{*}}\) & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{\textbf{*}}\) & IND-ACC\({}^{\textbf{*}}\) \\ \hline ABMIL & 0.973\(\pm\)0.005 & 0.891\(\pm\)0.017 & 0.997\(\pm\)0.004 & 0.961\(\pm\)0.032 & 0.971\(\pm\)0.007 & 0.885\(\pm\)0.010 & 0.973\(\pm\)0.010 & 0.897\(\pm\)0.023 \\ CLAM & 0.972\(\pm\)0.004 & 0.893\(\pm\)0.012 & 0.991\(\pm\)0.005 & 0.961\(\pm\)0.032 & 0.969\(\pm\)0.009 & 0.888\(\pm\)0.015 & 0.975\(\pm\)0.011 & 0.896\(\pm\)0.031 \\ DSMIL & 0.977\(\pm\)0.002 & 0.893\(\pm\)0.010 & 0.996\(\pm\)0.006 & 0.965\(\pm\)0.026 & 0.969\(\pm\)0.008 & 0.883\(\pm\)0.016 & 0.980\(\pm\)0.012 & 0.901\(\pm\)0.022 \\ DTFD-MIL & 0.975\(\pm\)0.003 & 0.897\(\pm\)0.012 & 0.996\(\pm\)0.004 & 0.943\(\pm\)0.046 & 0.971\(\pm\)0.007 & 0.893\(\pm\)0.017 & 0.974\(\pm\)0.012 & 0.878\(\pm\)0.022 \\ TransMIL & 0.961\(\pm\)0.001 & 0.864\(\pm\)0.022 & 0.994\(\pm\)0.004 & 0.930\(\pm\)0.030 & 0.947\(\pm\)0.017 & 0.828\(\pm\)0.037 & 0.975\(\pm\)0.013 & 0.894\(\pm\)0.027 \\ R\({}^{2}\)T-MIL \({}^{\dagger}\) & 0.956\(\pm\)0.008 & 0.847\(\pm\)0.022 & 0.991\(\pm\)0.008 & 0.936\(\pm\)0.030 & 0.932\(\pm\)0.020 & 0.803\(\pm\)0.048 & 0.974\(\pm\)0.012 & 0.897\(\pm\)0.029 \\ \hline
**CATE-MIL** & **0.983\(\pm\)**0.002 & **0.911\(\pm\)**0.018 & 0.989\(\pm\)**0.009 & 0.944\(\pm\)0.031 & **0.979\(\pm\)**0.007 & **0.905\(\pm\)**0.017 & 0.963\(\pm\)**0.011 & 0.882\(\pm\)0.032 \\ \hline \hline \end{tabular}

* The best results are highlighted in **bold**, and the second-best results are underlined.
* \({}^{\dagger}\) R\({}^{2}\)T-MIL is designed for feature re-embedding that utilize ABMIL as base MIL model.
* The in-domain performance of NSCLC and RCC does not represent the true ability of the models, as each site contains only samples from one cancer subtype. We primarily focus on the **OOD performance** for these two datasets.

\end{table}
Table 2: Cancer Subtyping Results on NSCLC and RCC.

concept features compared to the original features, which demonstrates CATE's ability to effectively align features with task-relevant concepts and enhance task-relevant information.

### Results on Additional Datasets

For clarity and to highlight the superiority of ABMIL when enhanced with CATE, we developed **CATE-MIL** by incorporating CATE into ABMIL and compared it against other leading MIL models on NSCLC and RCC datasets. The comparative results in Table 2 confirm that CATE-MIL consistently outperforms other models in both OOD and IND performance. However, it is noted that CATE-MIL performs poorly on the in-domain testing data for NSCLC and RCC. This underperformance may be attributed to the elimination of task-irrelevant information, including site-specific patterns, by CATE, potentially degrading performance on in-domain data for these datasets. Consequently, OOD performance more accurately reflects the discriminative and generalization capabilities of the models.

### Ablation Analysis

We conduct ablation studies to assess the effectiveness of each component within CATE, and the results are shown in Table 3. Initially, incorporating Predictive Information Maximization (PIM) enables ABMIL to achieve improved performance in most experiments, which demonstrates PIM's efficacy in extracting task-relevant information. However, using Superfluous Information Minimization (SIM) alone results in performance degradation across most experiments, which suggests that SIM may discard some task-relevant information without guidance from a task-relevant concept anchor. Incorporating both PIM and SIM consistently enhances ABMIL's performance in all experiments, which further verifies that their combination effectively boosts the generalization capabilities of MIL models. We also conduct experiments by only using the interference features in CFI as the input of ABMIL, and the results show that the interference features are also informative for WSI classification tasks. More ablation analysis about the weights of PIM and SIM in CIB module the number of representative patches are in Appendix C.

## 5 Conclusion and Discussion

In this paper, we introduce CATE, a new approach that offers a _"free lunch"_ for task-specific adaptation of pathology VLM by leveraging the inherent consistency between image and text modalities.

Figure 3: (a) Attention heatmap of CATE-MIL. (b) Attention heatmap of the original ABMIL. (c) similarity between the calibrated features and the corresponding class concept feature. (d) similarity between the original features and the corresponding class concept feature. (e) Original WSI. (f) UMAP visualization of class concept features, original features, and enhanced features.

CATE shows the potential to enhance the generic features extracted by pathology VLM for specific downstream tasks, using task-specific concept anchors as guidance. The proposed CIB module calibrates the image features by enhancing task-relevant information while suppressing task-irrelevant information, while the CFI module obtains the interference vector for each patch to generate discriminative task-specific features. Extensive experiments on WSI datasets demonstrate the effectiveness of CATE in improving the performance and generalizability of state-of-the-art MIL methods.

**Limitations and Social Impact.** The proposed CATE offers a promising solution to customize the pathology VLM for specific tasks, significantly improving the performance and applicability of MIL methods in WSI analysis. However, the performance of CATE heavily depends on the quality of the concept anchors, which, in turn, relies on domain knowledge and the quality of the pre-trained pathology VLM. Additionally, while CATE is optimized for classification tasks such as cancer subtyping, it may not be readily applicable to other analytical tasks, such as survival prediction. However, there might be a potential solution to address this challenge. For instance, we could leverage LLMs or retrieval-based LLMs to generate descriptive prompts about the general morphological appearance of WSIs for specific cancer types. By asking targeted questions, we can summarize reliable and general morphological descriptions associated with different survival outcomes or biomarker expressions and further verify these prompts with pathologists. Moreover, since medical data may contain sensitive information, ensuring the privacy and security of such data is crucial.

## 6 Acknowledgements

This work was supported in part by the Research Grants Council of Hong Kong (27206123 and T45-401/22-N), in part by the Hong Kong Innovation and Technology Fund (ITS/274/22), in part by the National Natural Science Foundation of China (No. 62201483), and in part by Guangdong Natural Science Fund (No. 2024A1515011875).

## References

* [1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. _arXiv preprint arXiv:1612.00410_, 2016.
* [2] Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y Chen, Andrew D Trister, Rahul G Krishnan, and Faisal Mahmood. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16144-16155, 2022.
* [3] Richard J Chen, Tong Ding, Ming Y Lu, Drew FK Williamson, Guillaume Jaume, Andrew H Song, Bowen Chen, Andrew Zhang, Daniel Shao, Muhammad Shaban, et al. Towards a general-purpose foundation model for computational pathology. _Nature Medicine_, 30(3):850-862, 2024.
* [4] Shenghua Cheng, Sibo Liu, Jingya Yu, Gong Rao, Yuwei Xiao, Wei Han, Wenjie Zhu, Xiaohua Lv, Ning Li, Jing Cai, et al. Robust whole slide image analysis for cervical cancer screening using deep learning. _Nature communications_, 12(1):5639, 2021.
* [5] Kevin de Haan, Yijie Zhang, Jonathan E Zuckerman, Tairan Liu, Anthony E Sisk, Miguel FP Diaz, Kuang-Yu Jen, Alexander Nobori, Sofia Liou, Sarah Zhang, et al. Deep learning-based

\begin{table}
\begin{tabular}{l|c c c|c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{PIM} & \multirow{2}{*}{SIM} & \multirow{2}{*}{CFI} & \multicolumn{2}{c}{BRCA} & \multicolumn{2}{c}{NSCLC} & \multicolumn{2}{c}{NSCLC} & \multicolumn{2}{c}{RCC} \\  & & & & \((N_{\text{NDD}}\)=1) & \((N_{\text{NDD}}\)=2) & \((N_{\text{NDD}}\)=2) & \((N_{\text{NDD}}\)=4) & \((N_{\text{NDD}}\)=3) & \((N_{\text{NDD}}\)=6) \\ \hline ABMIL & & & & 0.914\(\pm\)0.015 & 0.899\(\pm\)0.035 & 0.874\(\pm\)0.021 & 0.951\(\pm\)0.023 & 0.973\(\pm\)0.005 & 0.971\(\pm\)0.007 \\  & ✓ & & & 0.932\(\pm\)0.011 & 0.939\(\pm\)0.012 & 0.9966\(\pm\)0.026 & 0.901\(\pm\)0.047 & 0.976\(\pm\)0.005 & 0.973\(\pm\)0.005 \\  & ✓ & & & 0.895\(\pm\)0.022 & 0.885\(\pm\)0.140 & 0.656\(\pm\)0.041 & 0.898\(\pm\)0.028 & 0.952\(\pm\)0.014 & 0.954\(\pm\)0.015 \\  & ✓ & ✓ & & 0.936\(\pm\)0.010 & 0.942\(\pm\)0.010 & 0.910\(\pm\)0.039 & 0.960\(\pm\)0.011 & 0.972\(\pm\)0.004 & 0.977\(\pm\)0.005 \\  & ✓ & & ✓ & 0.913\(\pm\)0.024 & 0.884\(\pm\)0.032 & 0.850\(\pm\)0.036 & 0.918\(\pm\)0.029 & 0.975\(\pm\)0.008 & 0.941\(\pm\)0.022 \\
**CATE-MIL** & ✓ & ✓ & ✓ & **0.951\(\pm\)**0.003 & **0.943\(\pm\)**0.006 & **0.945\(\pm\)**0.016 & **0.970\(\pm\)**0.003 & **0.983\(\pm\)**0.002 & **0.979\(\pm\)**0.007 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study of CATE.

transformation of h&e stained tissues into special stains. _Nature communications_, 12(1):1-13, 2021.
* [6] Marco Federici, Anjan Dutta, Patrick Forre, Nate Kushman, and Zeynep Akata. Learning robust representations via multi-view information bottleneck. _arXiv preprint arXiv:2002.07017_, 2020.
* [7] Alexandre Filiot, Ridouane Ghermi, Antoine Olivier, Paul Jacob, Lucas Fidon, Alice Mac Kain, Charlie Saillard, and Jean-Baptiste Schiratti. Scaling self-supervised learning for histopathology with masked image modeling. _medRxiv_, pages 2023-07, 2023.
* [8] Bernhard C Geiger. On information plane analyses of neural network classifiers--a review. _IEEE Transactions on Neural Networks and Learning Systems_, 33(12):7039-7051, 2021.
* [9] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [10] Wentai Hou, Lequan Yu, Chengxuan Lin, Helong Huang, Rongshan Yu, Jing Qin, and Liansheng Wang. H^ 2-mil: exploring hierarchical representation with heterogeneous multiple instance learning for whole slide image analysis. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, pages 933-941, 2022.
* [11] Frederick M Howard, James Dolezal, Sara Kochanny, Jerfee Schulte, Heather Chen, Lara Heij, Dezheng Huo, Rita Nanda, Olufunnilayo I Olopade, Jakob N Kather, et al. The impact of site-specific digital histology signatures on deep learning model accuracy and bias. _Nature communications_, 12(1):4423, 2021.
* [12] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [13] Yanyan Huang, Weiqin Zhao, Shujun Wang, Yu Fu, Yuming Jiang, and Lequan Yu. Conslide: Asynchronous hierarchical interaction transformer with breakup-reorganize rehearsal for continual whole slide image analysis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 21349-21360, 2023.
* [14] Zhi Huang, Federico Bianchi, Mert Yukekgonul, Thomas J Montine, and James Zou. A visual-language foundation model for pathology image analysis using medical twitter. _Nature medicine_, 29(9):2307-2316, 2023.
* [15] Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. _Advances in Neural Information Processing Systems_, 36, 2024.
* [16] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In _International conference on machine learning_, pages 2127-2136. PMLR, 2018.
* [17] Syed Ashar Javed, Dinkar Juyal, Harshith Padigela, Amaro Taylor-Weiner, Limin Yu, and Aaditya Prakash. Additive mil: Intrinsically interpretable multiple instance learning for pathology. _Advances in Neural Information Processing Systems_, 35:20689-20702, 2022.
* [18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [19] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. _Advances in neural information processing systems_, 30, 2017.
* [20] Zhengfeng Lai, Zhuoheng Li, Luca Cerny Oliveira, Joohi Chauhan, Brittany N Dugger, and Chen-Nee Chuah. Clipath: Fine-tune clip with visual feature fusion for pathology image analysis towards minimizing data collection efforts. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2374-2380, 2023.

* [21] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 14318-14328, 2021.
* [22] Hao Li, Ying Chen, Yifei Chen, Rongshan Yu, Wenxian Yang, Liansheng Wang, Bowen Ding, and Yuchen Han. Generalizable whole slide image classification with fine-grained visual-semantic interaction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11398-11407, 2024.
* [23] Jana Lipkova, Richard J Chen, Bowen Chen, Ming Y Lu, Matteo Barbieri, Daniel Shao, Anurag J Vaidya, Chengkuan Chen, Luoting Zhuang, Drew FK Williamson, et al. Artificial intelligence for multimodal data integration in oncology. _Cancer cell_, 40(10):1095-1110, 2022.
* [24] Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, and Shaoting Zhang. Pathotune: Adapting visual foundation model to pathological specialists. _arXiv preprint arXiv:2403.16497_, 2024.
* [25] Ming Y Lu, Bowen Chen, Drew FK Williamson, Richard J Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Long Phi Le, Georg Gerber, et al. A visual-language foundation model for computational pathology. _Nature Medicine_, pages 1-12, 2024.
* [26] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient and weakly supervised computational pathology on whole-slide images. _Nature biomedical engineering_, 5(6):555-570, 2021.
* [27] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [28] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* [29] Linhao Qu, Kexue Fu, Manning Wang, Zhijian Song, et al. The rise of ai language pathologists: Exploring two-level prompt learning for few-shot weakly-supervised whole slide image classification. _Advances in Neural Information Processing Systems_, 36, 2024.
* [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [31] Abtin Riasatian, Morteza Babaie, Danial Maleki, Shivam Kalra, Mojtaba Valipour, Sobhan Hemati, Manit Zaveri, Amir Safarpoor, Sobhan Shafiei, Mehdi Afshari, et al. Fine-tuning and training of densenet for histopathology image representation using tcga diagnostic slides. _Medical image analysis_, 70:102032, 2021.
* [32] Alexander Robey, George J Pappas, and Hamed Hassani. Model-based domain generalization. _Advances in Neural Information Processing Systems_, 34:20210-20229, 2021.
* [33] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. _Advances in Neural Information Processing Systems_, 34:2136-2147, 2021.
* [34] Artem Shmatko, Narmin Ghaffari Laleh, Moritz Gerstung, and Jakob Nikolas Kather. Artificial intelligence in histopathology: enhancing cancer research and clinical oncology. _Nature Cancer_, 3(9):1026-1038, 2022.
* [35] Wenhao Tang, Fengtao Zhou, Sheng Huang, Xiang Zhu, Yi Zhang, and Bo Liu. Feature re-embedding: Towards foundation model-level performance in computational pathology. _arXiv preprint arXiv:2402.17228_, 2024.
* [36] Eugene Vorontsov, Alican Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Siqi Liu, Philippe Mathieu, Alexander van Eck, Donghun Lee, Julian Viret, et al. Virchow: A million-slide digital pathology foundation model. _arXiv preprint arXiv:2309.07778_, 2023.

* Wang et al. [2022] Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, Jing Zhang, Wei Yang, Junzhou Huang, and Xiao Han. Transformer-based unsupervised contrastive learning for histopathological image classification. _Medical image analysis_, 81:102559, 2022.
* Wu and Gong [2021] Guile Wu and Shaogang Gong. Collaborative optimization and aggregation for decentralized domain generalization and adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6484-6493, 2021.
* Yufei et al. [2022] Cui Yufei, Ziquan Liu, Xiangyu Liu, Xue Liu, Cong Wang, Tei-Wei Kuo, Chun Jason Xue, and Antoni B Chan. Bayes-mil: A new probabilistic perspective on attention-based multiple instance learning for whole slide images. In _The Eleventh International Conference on Learning Representations_, 2022.
* Zhang et al. [2022] Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18802-18812, 2022.
* Zhang et al. [2023] Yunkun Zhang, Jin Gao, Mu Zhou, Xiaosong Wang, Yu Qiao, Shaoting Zhang, and Dequan Wang. Text-guided foundation model adaptation for pathological image classification. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 272-282. Springer, 2023.
* Zhou et al. [2021] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. _arXiv preprint arXiv:2111.07832_, 2021.

Overview

The structure of this supplementary material as shown below,

* Appendix B presents additional experimental results, including changes in in-domain and out-of-domain site splitting, further comparisons on the BRCA dataset under traditional settings, additional results on the PRAD dataset, and results under site-preserved cross-validation.
* Appendix C discusses additional ablation study results concerning hyperparameters, including the impact of loss weights for PIM and SIM, the effect of the number of representative patches \(k\), and further ablation studies on the CIB module.
* Appendix D presents detailed descriptions of datasets and experimental settings.
* Appendix E provides the detailed definition and formula derivation.
* Appendix F elaborates on the Concept Information Bottleneck (CIB) module using the information plane.
* Appendix G details the prompts used in our experiments.
* Appendix H presents additional visualization results.

## Appendix B Additional Results

### Additional Cancer Subtyping Results with Different IND and OOD Sites

To further evaluate the generalization ability of the proposed CATE method, we conduct additional experiments on TCGA-BRCA dataset with different settings of in-domain (IND) and out-of-domain (OOD) sites. The detailed experimental settings are shown below:

* BRCA (\(N_{\text{IND}}\)=1): One site as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain site are shown below:
* AR: 44 slides of IDC and 15 slides of ILC.
* BRCA (\(N_{\text{IND}}\)=2): Two sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below:
* AR: 44 slides of IDC and 15 slides of ILC.
* B6: 39 slides of IDC and 6 slides of ILC.

The additional results are presented in Table 4. It is evident that the proposed CATE-MIL consistently delivers superior performance in both in-domain and out-of-domain settings, demonstrating its effectiveness in enhancing task-specific information and minimizing the impact of irrelevant data.

### Additional Cancer Subtyping Results with Traditional Experimental Settings

In the main paper, we divided the dataset into in-domain and out-of-domain sites to evaluate the generalization ability of the proposed CATE method. To further evaluate its effectiveness, we conducted additional experiments under traditional settings, randomly splitting the dataset into training, validation, and testing sets. We report the performance metrics, including means and standard deviations over 10 Monte-Carlo Cross-Validation runs, in Table 5. Additionally, we provide comparisons of training times and parameter sizes for various methods.

It is evident that the proposed CATE-MIL consistently outperforms others in both AUC and ACC metrics, underscoring its superiority. Furthermore, CATE-MIL benefits from shorter training times and smaller parameter sizes compared to the R\({}^{2}\)T-MIL method.

### Additional Gleason Grading Results on PRAD

CATE is a general framework that can be applied to more complex tasks beyond cancer subtyping, such as Gleason grading in prostate cancer. We have conducted conducted experiments on the TCGA-PRAD dataset to evaluate the performance of CATE-MIL in Gleason grading. Specifically,

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{PRAD} \\ \cline{2-5}  & OOD-AUC & OOD-ACC & IND-AUC & IND-ACC \\ \hline ABMIL & 0.704 \(\pm\) 0.034 & 0.510 \(\pm\) 0.075 & 0.742 \(\pm\) 0.060 & 0.575 \(\pm\) 0.051 \\ \hline
**CATE-MIL** & **0.755**\(\pm\) 0.050 & **0.567**\(\pm\) 0.067 & **0.797**\(\pm\) 0.044 & **0.643**\(\pm\) 0.075 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Supplementary Gleason Grading Results on PRAD.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{BRCA (\(N_{\text{IND}}\)=1)} \\ \cline{2-5}  & OOD-AUC & OOD-ACC & IND-AUC & IND-ACC \\ \hline ABMIL & 0.916 \(\pm\) 0.017 & 0.882 \(\pm\) 0.022 & 0.977 \(\pm\) 0.029 & 0.953 \(\pm\) 0.036 \\ CLAM & 0.895 \(\pm\) 0.031 & 0.842 \(\pm\) 0.049 & 0.973 \(\pm\) 0.035 & 0.938 \(\pm\) 0.043 \\ DSMIL & 0.892 \(\pm\) 0.017 & 0.863 \(\pm\) 0.016 & 0.967 \(\pm\) 0.022 & 0.924 \(\pm\) 0.048 \\ TransMIL & 0.917 \(\pm\) 0.008 & 0.878 \(\pm\) 0.016 & 0.980 \(\pm\) 0.031 & 0.955 \(\pm\) 0.038 \\ DTFD-MIL & 0.920 \(\pm\) 0.023 & 0.881 \(\pm\) 0.024 & 0.982 \(\pm\) 0.030 & 0.957 \(\pm\) 0.038 \\ \({}^{\dagger}\)R\({}^{2}\)T-MIL & 0.902 \(\pm\) 0.022 & 0.835 \(\pm\) 0.063 & 0.963 \(\pm\) 0.037 & 0.944 \(\pm\) 0.039 \\ \hline \hline \multicolumn{5}{l}{\({}^{\dagger}\)**CATE-MIL**} & **0.938**\(\pm\) 0.014 & **0.895**\(\pm\) 0.021 & **0.984**\(\pm\) 0.021 & **0.959**\(\pm\) 0.038 \\ \hline \hline \multicolumn{5}{l}{\({}^{\dagger}\)**BRCA (\(N_{\text{IND}}\)=2)} \\ \cline{2-5} Method & OOD-AUC & OOD-ACC & IND-AUC & IND-ACC \\ \hline ABMIL & 0.914 \(\pm\) 0.012 & 0.879 \(\pm\) 0.017 & 0.954 \(\pm\) 0.035 & 0.910 \(\pm\) 0.043 \\ CLAM & 0.914 \(\pm\) 0.021 & 0.890 \(\pm\) 0.013 & 0.958 \(\pm\) 0.034 & 0.926 \(\pm\) 0.050 \\ DSMIL & 0.931 \(\pm\) 0.007 & 0.900 \(\pm\) 0.012 & 0.971 \(\pm\) 0.020 & 0.919 \(\pm\) 0.034 \\ TransMIL & 0.929 \(\pm\) 0.009 & 0.897 \(\pm\) 0.006 & 0.962 \(\pm\) 0.030 & 0.903 \(\pm\) 0.039 \\ DTFD-MIL & 0.898 \(\pm\) 0.009 & 0.868 \(\pm\) 0.017 & 0.950 \(\pm\) 0.034 & 0.890 \(\pm\) 0.037 \\ \({}^{\dagger}\)R\({}^{2}\)T-MIL & 0.919 \(\pm\) 0.017 & 0.893 \(\pm\) 0.017 & 0.962 \(\pm\) 0.025 & 0.906 \(\pm\) 0.026 \\ \hline \multicolumn{5}{l}{\({}^{\dagger}\)**CATE-MIL**} & **0.947**\(\pm\) 0.005 & **0.920**\(\pm\) 0.004 & **0.980**\(\pm\) 0.015 & **0.939**\(\pm\) 0.022 \\ \hline \hline \multicolumn{5}{l}{\({}^{\dagger}\)**CATE-MIL**} & **0.945**\(\pm\)0.033 & **0.917**\(\pm\)0.029 & 68.53 & 4.26 MB (1.26+3.00) \\ \hline \hline \multicolumn{5}{l}{\({}^{\dagger}\)**CATE-MIL**} & **0.

the samples in PRAD dataset are divided into Gleason pattern 3, 4, and 5, and the task is to classify the samples into these three categories. The results are shown in Table 6. It is evident that CATE-MIL consistently outperforms the base model ABMIL in both in-domain and out-of-domain settings, demonstrating its effectiveness in enhancing task-specific information and minimizing the impact of irrelevant data. In the future, as more studies reveal the connection between morphological features and molecular biomarkers and more powerful pathology VLMs are developed, our framework has the potential to benefit more complex tasks.

### Additional Experiments using Site-Preserved Cross-Validation

To provide a more comprehensive evaluation of the proposed CATE-MIL, we conduct additional experiments using site-preserved cross-validation [11], where the samples from the same site are preserved in the same fold. For each fold, we split the data into training and testing sets, and these testing sets are regarded as in-domain testing data. And the other sites are used as out-of-domain testing data. The results are shown in Table 7. It is evident that CATE-MIL consistently outperforms ABMIL in both in-domain and out-of-domain settings.

## Appendix C Additional Ablation Study

### Ablation Study of the Weight of PIM and SIM Losses

The overall objective of the proposed CATE-MIL is a weighted sum of the PIM and SIM losses, along with the classification loss:

\[\mathcal{L}=\mathcal{L}_{CE}+\lambda_{P}\mathcal{L}_{PIM}+\lambda_{S}\mathcal{ L}_{SIM}.\] (12)

We note that the hyperparameters for the weights of the PIM and SIM losses were not specifically tuned in the main paper. To investigate the effect of the weight of PIM and SIM losses on the model performance, we conduct an ablation study on BRCA (\(N_{\text{IND}}\)=2) with CATE-MIL (ABMIL integrated with CATE), varying the weights of PIM and SIM losses. The results are shown in Figure 4.

From Figure 4, it is evident that CATE enhances the performance of the base model ABMIL in most scenarios, particularly in out-of-domain settings. When the weight of PIM loss \(\lambda_{P}\) is too low, model performance suffers, underscoring the significant role of PIM loss in enhancing task-specific information. Conversely, an excessively high \(\lambda_{P}\) also diminishes performance, as the model overly

Figure 4: Ablation study of the weight of PIM and SIM losses on the model performance.

prioritizes maximizing mutual information between the original and calibrated features at the expense of optimizing classification loss. Regarding the weight of SIM loss \(\lambda_{S}\), optimal performance is achieved when \(\lambda_{S}\) is approximately 30. If the \(\lambda_{S}\) is too low, the model fails to effectively eliminate irrelevant information, thereby impairing performance. Conversely, if \(\lambda_{S}\) is too high, the model risks discarding task-relevant information, leading to performance degradation.

### Ablation Study of the Number of Representative Patches \(k\)

In this section, we conduct an ablation study to investigate the impact of the number of representative patches, \(k\), on model performance, as discussed in Section 3.2. In practice, the representative patches are selected based on the similarities between the original image feature and the corresponding class-specific concept anchor.

As shown in Figure 5, it is evident that CATE generally enhances the performance of the base model, ABMIL. When the number of representative patches \(k\) is too small, model performance degrades due to insufficient capture of task-specific information. Conversely, an excessively large \(k\) also leads to performance degradation in out-of-domain scenarios, as it introduces noise from irrelevant information. Consequently, we have set the number of representative patches \(k\) to 10 in the main paper.

### Additional Ablation Study of CIB Module

We further conducted experiments on CATE-MIL without concept alignment (discarding PIM loss and SIM loss of the CIB module) and replaced the CIB module with an MLP to investigate the effect of concept alignment and the increased number of parameters. The results are shown in Table 8. The performance of CATE-MIL significantly decreases in both cases, demonstrating the importance of concept alignment in the CIB module and that the improvements of CATE are not due to the increased number of parameters.

## Appendix D Datasets Description and Detailed Experimental Settings

In this section, we provide detailed descriptions of the datasets used in the experiments and the detailed experimental settings that we used in the experiments in Section 4.2.

Figure 5: Ablation study of \(k\).

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline \multirow{2}{*}{Method} & BRCA & BRCA & NSCLC & NSCLC & RCC & RCC \\  & (\(N_{\text{ND}}\)=1) & (\(N_{\text{ND}}\)=2) & (\(N_{\text{ND}}\)=2) & (\(N_{\text{ND}}\)=4) & (\(N_{\text{ND}}\)=3) & (\(N_{\text{ND}}\)=6) \\ \hline
**CATE-MIL** w/o CFI (Baseline) & **0.936\(\pm\)**0.010 & **0.942\(\pm\)**0.010 & **0.910\(\pm\)**0.030 & **0.960\(\pm\)**0.011 & **0.979\(\pm\)**0.004 & **0.977\(\pm\)**0.005 \\ - w/o concept alignment & 0.900\(\pm\)0.017 & 0.884\(\pm\)0.033 & 0.742\(\pm\)0.059 & 0.897\(\pm\)0.022 & 0.961\(\pm\)0.011 & 0.932\(\pm\)0.016 \\ - Replace CIB with MLP & 0.888\(\pm\)0.027 & 0.902\(\pm\)0.037 & 0.816\(\pm\)0.040 & 0.931\(\pm\)0.023 & 0.966\(\pm\)0.006 & 0.951\(\pm\)0.021 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Supplementary Ablation Study of CIB Module.

### Datasets Description

* **TCGA-BRCA**: The dataset contains nine disease subtypes, and this study focuses on the classification of Invasive ductal carcinoma (IDC, 726 slides from 694 cases) and invasive lobular carcinoma (ILC, 149 slides from 143 cases). The dataset is collected from 36 sites with 20 of them having both IDC and ILC slides, and the other 16 sites only have IDC slides or ILC slides.
* **TCGA-NSCLC**: The dataset contains two disease subtypes, including lung adenocarcinoma (LUAD, 492 slides from 430 cases) and lung squamous cell carcinoma (LUSC, 466 slides from 432 cases). The dataset is collected from 66 sites. Different from TCGA-BRCA, **each site only contains one disease subtype**.
* **TCGA-RCC**: The dataset contains three disease subtypes, including clear cell renal cell carcinoma (CCRCC, 498 slides from 492 cases), papillary renal cell carcinoma (PRCC, 289 slides from 267 cases), and chromophore renal cell carcinoma (CHRCC, 118 slides from 107 cases). The dataset is collected from 55 sites. Similar to TCGA-NSCLC, **each site only contains one disease subtype**.

### Detailed Experimental Settings

To validate that the proposed CATE effectively improves MIL model performance by enhancing the task-specific information and eliminating the disturbance of task-irrelevant information, we conduct extensive experiments across three datasets under various in-domain and out-of-domain settings.

Specifically, for TCGA-BRCA dataset, we conduct the following experiments:

* BRCA (\(N_{\text{IND}}\)=1): One site as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain site are shown below:
* D8: 59 slides of IDC and 8 slides of ILC.
* BRCA (\(N_{\text{IND}}\)=2): Two sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below:
* A8: 48 slides of IDC and 5 slides of ILC.
* D8: 59 slides of IDC and 8 slides of ILC.

For TCGA-NSCLC dataset, we conduct the following experiments:

* NSCLC (\(N_{\text{IND}}\)=2): Two sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below:
* 44: 43 slides of LUAD and 0 slides of LUSC.
* 22: 0 slides of LUAD and 36 slides of LUSC.
* NSCLC (\(N_{\text{IND}}\)=4): Four sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below:
* 44: 43 slides of LUAD and 0 slides of LUSC.
* 50: 20 slides of LUAD and 0 slides of LUSC.
* 22: 0 slides of LUAD and 36 slides of LUSC.
* 56: 0 slides of LUAD and 35 slides of LUSC.

For TCGA-RCC dataset, we conduct the following experiments:

* RCC (\(N_{\text{IND}}\)=3): Three sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below:
* A3: 48 slides of CCRCC, 0 slides of CHRCC, and 0 slides of PRCC.
* KL: 0 slides of CCRCC, 24 slides of CHRCC, and 0 slides of PRCC.
* 2Z: 0 slides of CCRCC, 0 slides of CHRCC, and 23 slides of PRCC.
* RCC (\(N_{\text{IND}}\)=6): Six sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below:
* A3: 48 slides of CCRCC, 0 slides of CHRCC, and 0 slides of PRCC.
* AK: 15 slides of CCRCC, 0 slides of CHRCC, and 0 slides of PRCC.
* KL: 0 slides of CCRCC, 24 slides of CHRCC, and 0 slides of PRCC.
* KM: 0 slides of CCRCC, 20 slides of CHRCC, and 0 slides of PRCC.
* 2Z: 0 slides of CCRCC, 0 slides of CHRCC, and 23 slides of PRCC.
* 5P: 0 slides of CCRCC, 0 slides of CHRCC, and 15 slides of PRCC.

## Appendix E Detailed Definition and Formula Derivation

### Definition of Sufficiency and Consistency

In this part, we will define the _sufficiency_ and _consistency_ in the context of the concept information bottleneck (CIB) module.

First, we define the mutual information between two random variables \(X\) and \(Y\) as:

\[I(X;Y)=\sum p(x,y)\log\frac{p(x,y)}{p(x)p(y)}.\] (13)

**Sufficiency:** To quantify the requirement that the calibrated features should be maximally informative about the label information \(\mathbf{y}\), we define the sufficiency as the mutual information between the label information and the calibrated features:

_Sufficiency._\(\hat{\bm{\alpha}}\) is sufficient for \(\mathbf{y}\iff I(\hat{\bm{x}};\mathbf{y}|\hat{\bm{\alpha}})=0\iff I(\hat{ \bm{x}};\mathbf{y})=I(\hat{\bm{\alpha}};\mathbf{y})\).

The sufficiency definition requires that the calibrated features \(\hat{\bm{\alpha}}\) encapsulate all information about the label information \(\mathbf{y}\) that is accessible from the original features \(\hat{\bm{x}}\). In essence, the calibrated feature \(\hat{\bm{\alpha}}\) of original feature \(\hat{\bm{x}}\) is sufficient for determining label \(\mathbf{y}\) if and only if the amount of information regarding the specific task is unchanged after the transformation.

**Consistency:** Since the image feature \(\hat{\bm{x}}\) and the concept anchor \(\mathbf{c}\) are consistent in the pathology VLM, we posit that any representation containing all information accessible from both the original feature and the concept anchor also encompasses the necessary discriminative label information. Thus, we define the consistency between the concept anchor and the original feature as:

_Consistency._\(\mathbf{c}\) is consistent with \(\hat{\bm{x}}\) for \(\mathbf{y}\iff I(\mathbf{y};\hat{\bm{x}}|\mathbf{c})=0\).

The consistency definition requires that the concept anchor \(\mathbf{c}\) and the original feature \(\hat{\bm{x}}\) should be consistent with the label information \(\mathbf{y}\).

### Maximize Predictive Information with InfoNCE

In this part, we will prove that the predictive information for the concept anchor can be maximized by maximizing the InfoNCE mutual information lower bound, which is defined in Equation 4. As in [27], the optimal value for \(f\left(\bm{c},\hat{\bm{\alpha}}_{i}\right)\) should be proportional to the density ratio:

\[f\left(\bm{c},\hat{\bm{\alpha}}_{i}\right)\propto\frac{p\left(\bm{c},\hat{\bm {\alpha}}_{i}\right)}{p\left(\bm{c}\right)p\left(\hat{\bm{\alpha}}_{i}\right)}.\] (14)And we can get:

\[I_{NCE}^{opt}\left(\hat{\bm{\alpha}};\mathbf{c}\right)=\underset{ \hat{\bm{\alpha}},\bm{c}_{\text{pos}}^{\text{cs}}}{\mathbb{E}}\left[\sum_{i=1}^{ k}\log\frac{\frac{p\left(\bm{c}_{\text{pos}}^{\text{cs}},\hat{\bm{\alpha}}_{i} \right)}{p\left(\bm{c}_{\text{pos}}^{\text{cs}}\right)p\left(\hat{\bm{\alpha}} _{i}\right)}}{\sum_{j=1}^{m}\frac{p\left(\bm{c}_{j}^{\text{cs}},\hat{\bm{ \alpha}}_{i}\right)}{p\left(\bm{c}_{j}^{\text{cs}}\right)p\left(\hat{\bm{ \alpha}}_{i}\right)}+\sum_{j=1}^{n}\frac{p\left(\bm{c}_{j}^{\text{cs}},\hat{ \bm{\alpha}}_{i}\right)}{p\left(\bm{c}_{j}^{\text{cs}}\right)p\left(\hat{\bm{ \alpha}}_{i}\right)}}\right]\] \[=-\underset{\hat{\bm{\alpha}},\bm{c}_{\text{pos}}^{\text{cs}}}{ \mathbb{E}}\left[\sum_{i=1}^{k}\log\left(1+\frac{p\left(\bm{c}_{\text{pos}}^{ \text{cs}}\right)p\left(\hat{\bm{\alpha}}_{i}\right)}{p\left(\bm{c}_{\text{pos} }^{\text{cs}},\hat{\bm{\alpha}}_{i}\right)}\left(m+n-1\right)\right)\right]\] \[\leq-\underset{\hat{\bm{\alpha}},\bm{c}_{\text{pos}}^{\text{cs}}}{ \mathbb{E}}\left[\sum_{i=1}^{k}\log\left(\frac{p\left(\bm{c}_{\text{pos}}^{ \text{cs}}\right)p\left(\hat{\bm{\alpha}}_{i}\right)}{p\left(\bm{c}_{\text{pos} }^{\text{cs}},\hat{\bm{\alpha}}_{i}\right)}\left(m+n\right)\right)\right]\] \[=\sum_{i=1}^{k}I\left(\bm{c}_{\text{pos}}^{\text{cs}};\hat{\bm{ \alpha}}_{i}\right)-k\log\left(m+n\right)\] \[=I\left(\bm{c}_{\text{pos}}^{\text{cs}};\hat{\bm{\alpha}}\right) -k\log\left(m+n\right),\] (15)

where \(\bm{C}_{\text{neg}}\) denotes the negative concepts, including both negative class concepts and other type concepts. We can see that the InfoNCE estimation is a lower bound of the mutual information between the concept anchor and the calibrated features:

\[I\left(\bm{c}_{\text{pos}}^{\text{cs}};\hat{\bm{\alpha}}\right) \geq I_{NCE}^{opt}\left(\hat{\bm{\alpha}};\mathbf{c}\right)+k\log\left(m+n \right)\geq I_{NCE}\left(\hat{\bm{\alpha}};\mathbf{c}\right).\] (16)

Thus, the predictive information of calibrated features for the concept anchor can be maximized by maximizing the InfoNCE mutual information lower bound.

### Superfluous Information Minimization

As shown in Equation 3, the mutual information between the original features and the calibrated features \(I(\hat{\bm{x}};\hat{\bm{\alpha}})\) can be decomposed into the mutual information \(I(\hat{\bm{\alpha}};\mathbf{c})\) between the calibrated features and the concept anchor and the mutual information \(I(\hat{\bm{x}};\hat{\bm{\alpha}}|\mathbf{c})\) between the concept anchor and the calibrated features:

\[I(\hat{\bm{x}};\hat{\bm{\alpha}})=\underbrace{I(\hat{\bm{\alpha}};\mathbf{c} )}_{\text{Predictive information for }\mathbf{c}}+\underbrace{I(\hat{\bm{x}};\hat{\bm{\alpha}}| \mathbf{c})}_{\text{Superfluous information}}.\] (17)

As introduced in section 3.2, the first item is the predictive information for the concept anchor, which can be maximized by maximizing the InfoNCE mutual information lower bound. Thus, the second superfluous information item can be minimized by minimizing \(I(\hat{\bm{x}};\hat{\bm{\alpha}})\). In practice, the superfluous information minimization is conducted on all patches in the subset \(\mathbf{x}\), since the task-irrelevant information is distributed across all patches. Thus, the superfluous information can be minimized by minimizing \(I(\bm{x};\bm{\alpha})\). As in [1], this mutual information can be represented as:

\[I(\bm{x};\bm{\alpha}) =\int p\left(\bm{x},\bm{\alpha}\right)\log\frac{p\left(\bm{ \alpha}|\bm{x}\right)}{p\left(\bm{\alpha}\right)}d\bm{x}d\bm{\alpha}\] (18) \[=\int p\left(\bm{x},\bm{\alpha}\right)\log p\left(\bm{\alpha}| \bm{x}\right)d\bm{x}d\bm{\alpha}-\int p\left(\bm{\alpha}\right)\log p\left(\bm {\alpha}\right)d\bm{\alpha}.\]

However, the computing of the marginal distribution \(p\left(\bm{\alpha}\right)=\int p\left(\bm{\alpha}|\bm{x}\right)p\left(\bm{x} \right)d\bm{x}\) is intractable. Thus, we can let the Gaussian distribution \(r\left(\bm{\alpha}\right)\) approximate the marginal distribution \(p\left(\bm{\alpha}\right)\). Since the Kullback-Leibler divergence between two distributions is non-negative, we can get:

\[\int p\left(\bm{\alpha}\right)\log p\left(\bm{\alpha}\right)d\bm{\alpha}\geq \int p\left(\bm{\alpha}\right)\log r\left(\bm{\alpha}\right)d\bm{\alpha}.\] (19)Thus, the mutual information \(I(\bm{x};\bm{\alpha})\) have the upper bound:

\[\begin{split} I(\bm{x};\bm{\alpha})&\leq\int p\left( \bm{x},\bm{\alpha}\right)\log p\left(\bm{\alpha}|\bm{x}\right)d\bm{x}d\bm{ \alpha}-\int p\left(\bm{\alpha}\right)\log r\left(\bm{\alpha}\right)d\bm{ \alpha}\\ &=\int p\left(\bm{x}\right)p\left(\bm{\alpha}|\bm{x}\right)\log \frac{p\left(\bm{\alpha}|\bm{x}\right)}{r\left(\bm{\alpha}\right)}d\bm{x}d\bm{ \alpha}.\end{split}\] (20)

In practice, to compute this upper bound, we approximate distribution \(p\left(\bm{x}\right)\) with the empirical distribution:

\[p\left(\bm{x}\right)=\frac{1}{N}\sum_{i=1}^{N}\delta_{\bm{x}_{i}}\left(\bm{x} \right).\] (21)

Thus, the upper bound of the mutual information \(I(\bm{x};\bm{\alpha})\) can be computed as:

\[\int p\left(\bm{x}\right)p\left(\bm{\alpha}|\bm{x}\right)\log\frac{p\left(\bm {\alpha}|\bm{x}\right)}{r\left(\bm{\alpha}\right)}d\bm{x}d\bm{\alpha}\approx \frac{1}{N}\sum_{i=1}^{N}q_{\theta}\left(\bm{\alpha}_{i}|\bm{x}_{i}\right)\log \frac{q_{\theta}\left(\bm{\alpha}_{i}|\bm{x}_{i}\right)}{r\left(\bm{\alpha}_{ i}\right)},\] (22)

where \(q_{\theta}\left(\bm{\alpha}|\bm{x}\right)\) is a variational distribution with parameter \(\theta\) to approximate \(p\left(\bm{\alpha}|\bm{x}\right)\), and we implement this variational distribution with MLP:

\[q_{\theta}\left(\bm{\alpha}|\bm{x}\right)=\mathcal{N}\left(\bm{\alpha}|MLP^{ \mu}\left(\bm{x}\right),MLP^{\Sigma}\left(\bm{x}\right)\right),\] (23)

where \(MLP^{\mu}\) and \(MLP^{\Sigma}\) are implemented with MLPs which output the mean \(\mu\) and covariance matrix \(\Sigma\) of the Gaussian distribution. In practice, we utilize the reparameterization trick [18] to sample from the Gaussian distribution to get an unbiased estimate of the gradient to optimize the objective. Thus, the upper bound can be optimized by minimizing the KL divergence between the variational distribution \(q_{\theta}\left(\bm{\alpha}|\bm{x}\right)\) and the Gaussian distribution \(r\left(\bm{\alpha}\right)\):

\[\min_{\theta}\mathbb{E}_{\bm{x}}\left[D_{KL}\left(q_{\theta}\left(\bm{\alpha} |\bm{x}\right)||r\left(\bm{\alpha}\right)\right)\right].\] (24)

## Appendix F Explanation of CIB with Information Plane

To better understand the concept information bottleneck (CIB) method, we provide an explanation of CIB with the information plane [8, 6].

As shown in Figure 6, the information plane is a two-dimensional space where the x-axis represents the mutual information between the original features and the calibrated features of CIB module \(I(\hat{\bm{x}};\hat{\bm{\alpha}})\), and the y-axis represents the mutual information between the label information and the calibrated features \(I(\mathbf{y};\hat{\bm{x}})\). The line colored orange in the information plane defines _sufficiency_, indicating where the calibrated features are maximally informative about the label information \(\mathbf{y}\). The _consistency_ definition falls to the left of the sufficiency line, denoting a state where the calibrated features, containing all information from both the original feature and the concept anchor, also include the label information. The InfoMAX principle [9] states that the optimal calibrated feature should be maximally informative about the original feature.

Figure 6: Explanation of CIB module with Information Plane.

Ideally, a good calibrated feature \(\hat{\bm{\alpha}}\) should be maximally informative about the label information \(\mathbf{y}\) (the sufficiency line in the information plane) while minimally informative about the original features \(\hat{\bm{x}}\) (_i.e._, the ideal situation in the information plane). This situation can be achieved by directly supervised learning. However, the weakly supervised nature of MIL methods complicates this goal. As demonstrated in the ablation analysis in Section 4.4, if we directly perform superfluous information minimization without the guidance of concept anchor (_i.e._, predictive information maximization), the calibrated features will be less informative about the label information \(\mathbf{y}\) and the model performance will be degraded significantly. To address this issue, the proposed CIB module introduces the concept anchor \(\mathbf{c}\) to guide the learning process of information bottleneck for each instance. As shown in Figure 6, the CIB module could effectively reduce the mutual information between the original features and the calibrated features \(I(\hat{\bm{x}};\hat{\bm{\alpha}})\) while preserving the mutual information between the label information and the calibrated features \(I(\mathbf{y};\hat{\bm{\alpha}})\).

## Appendix G Prompts

In this section, we provide the detailed prompt templates and concepts for the datasets, which are used to generate the concept anchor for the CIB module. The prompt templates are shown in Table 9, and the concepts for TCGA-BRCA, TCGA-NSCLC, and TCGA-RCC are shown in Table 10, Table 11, and Table 12, respectively. The templates and the class-agnostic prompts are referred from the original paper of CONCH [25], and the class-specific prompts are generated by querying the LLM with the question such as 'In addition to tumor tissues, what types of tissue or cells are present in whole slide images of breast cancer?' The quality of LLM generated prompts has been demonstrated in several recent studies [29; 22]. In principle, we can use LLM (_e.g._, GPT-4) to generate reliable expert-designed prompts and further verified by pathologists. This strategy can ensure the scalability and reliability of the prompts.

\begin{table}
\begin{tabular}{l} \hline \hline Templates \\ \hline \textless{}_CLASSNAME_\textgreater{}. \\ a photomicrograph showing \textless{}_CLASSNAME_\textgreater{}. \\ a photomicrograph of \textless{}_CLASSNAME_\textgreater{}. \\ an image of \textless{}_CLASSNAME_\textgreater{}. \\ an image of \textless{}_CLASSNAME_\textgreater{}. \\ an example of \textless{}_CLASSNAME_\textgreater{}. \\ \textless{}_CLASSNAME_\textgreater{} is shown. \\ this is \textless{}_CLASSNAME_\textgreater{}. \\ there is \textless{}_CLASSNAME_\textgreater{}. \\ a histopathological image showing \textless{}_CLASSNAME_\textgreater{}. \\ a histopathological image of \textless{}_CLASSNAME_\textgreater{}. \\ a histopathological photograph of \textless{}_CLASSNAME_\textgreater{}. \\ a histopathological photograph showing \textless{}_CLASSNAME_\textgreater{}. \\ shows \textless{}_CLASSNAME_\textgreater{}. \\ presence of \textless{}_CLASSNAME_\textgreater{}. \\ \textless{}_CLASSNAME_\textgreater{} is present. \\ an H\&E stained image of \textless{}_CLASSNAME_\textgreater{}. \\ an H\&E stained image showing \textless{}_CLASSNAME_\textgreater{}. \\ an H\&E image showing \textless{}_CLASSNAME_\textgreater{}. \\ an H\&E image of \textless{}_CLASSNAME_\textgreater{}. \\ \textless{}_CLASSNAME_\textgreater{}, H\&E stain. \\ \textless{}_CLASSNAME_\textgreater{}, H\&E \\ \hline \hline \end{tabular}
\end{table}
Table 9: Prompt Templates.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline Concept Type & Classes & Concept Prompts \\ \hline \multirow{6}{*}{Class-specific Concept} & \multirow{6}{*}{IDC} & invasive ductal carcinoma \\  & & breast invasive ductal carcinoma \\  & & invasive ductal carcinoma of the breast \\  & & invasive carcinoma of the breast, ductal pattern \\  & & idc \\ \cline{2-3}  & \multirow{6}{*}{ILC} & invasive lobular carcinoma \\  & & breast invasive lobular carcinoma \\  & & invasive carcinoma of the breast \\  & & invasive carcinoma of the breast, lobular pattern \\  & & ilc \\ \hline \multirow{6}{*}{Class-agnostic Concept} & \multirow{6}{*}{Adipocytes} & adipocytes \\  & & adipose tissue \\  & & fat cells \\  & & fat tissue \\  & & fat \\ \cline{2-3}  & \multirow{6}{*}{Connective tissue} & connective tissue \\  & & stroma \\  & & fibrous tissue \\  & & collagen \\ \cline{2-3}  & \multirow{6}{*}{Necrotic Tissue} & necrotic tissue \\  & & necrosis \\ \cline{2-3}  & \multirow{6}{*}{Normal Breast Tissue Cells} & normal breast tissue \\  & & normal breast cells \\ \cline{2-3}  & & normal breast \\ \hline \hline \end{tabular}
\end{table}
Table 10: Concepts for TCGA-BRCA.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline Concept Type & Classes & Concept Prompts \\ \hline \multirow{6}{*}{Class-specific Concept} & \multirow{6}{*}{LUAD} & adenocarcinoma \\  & & lung adenocarcinoma \\  & & adenocarcinoma of the lung \\  & & luad \\ \cline{2-3}  & \multirow{6}{*}{LUSC} & squamous cell carcinoma \\  & & lung squamous cell carcinoma \\  & & squamous cell carcinoma of the lung \\  & & lusc \\ \hline \multirow{6}{*}{Class-agnostic Concept} & \multirow{6}{*}{Connective tissue} & connective tissue \\  & & stroma \\  & & fibrous tissue \\  & & collagen \\ \cline{1-1} \cline{2-3}  & \multirow{6}{*}{Necrotic Tissue} & necrotic tissue \\  & & necrosis \\ \cline{1-1} \cline{2-3}  & & normal lung tissue \\ \cline{1-1} \cline{2-3}  & \multirow{6}{*}{Normal Lung Tissue Cells} & normal lung cells \\ \cline{1-1} \cline{2-3}  & & normal lung \\ \hline \hline \end{tabular}
\end{table}
Table 11: Concepts for TCGA-NSCLC.

[MISSING_PAGE_EMPTY:24]

## 6 Conclusion

Figure 7: Visualization results for samples of IDC in TCGA-BRCA. (a) Attention heatmap of CATE-MIL. (b) Attention heatmap of original ABMIL. (c) similarity between the calibrated features and the corresponding class concept feature. (d) similarity between the original features and the corresponding class concept feature. (e) Original WSI. (f) UMAP visualization.

Figure 8: Visualization results for samples of ILC in TCGA-BRCA. (a) Attention heatmap of CATE-MIL. (b) Attention heatmap of original ABMIL. (c) similarity between the calibrated features and the corresponding class concept feature. (d) similarity between the original features and the corresponding class concept feature. (e) Original WSI. (f) UMAP visualization.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discuss the limitations of the work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: For each theoretical result, the paper provide the full set of assumptions and a complete (and correct) proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper report error bars suitably and correctly defined. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provide sufficient information on the computer resources needed to reproduce the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discuss both potential positive societal impacts and negative societal impacts of the work performed. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: New assets introduced in the paper well documented and is the documentation provided alongside the assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.