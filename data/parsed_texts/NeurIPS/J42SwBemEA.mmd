# State Chrono Representation for Enhancing Generalization in Reinforcement Learning

 Jianda Chen\({}^{1}\)  Wen Zheng Terence Ng\({}^{1,2}\)  Zichen Chen\({}^{3}\)

**Sinno Jialin Pan\({}^{4}\)  Tianwei Zhang\({}^{1}\)**

\({}^{1}\)Nanyang Technological University \({}^{2}\)Continental Automotive Singapore

\({}^{3}\)University of California, Santa Barbara \({}^{4}\)The Chinese University of Hong Kong

{jianda001, ngwe0099}@ntu.edu.sg zichen_chen@ucsb.edu

sinnopan@cuhk.edu.hk tianwei.zhang@ntu.edu.sg

###### Abstract

In reinforcement learning with image-based inputs, it is crucial to establish a robust and generalizable state representation. Recent advancements in metric learning, such as deep bisimulation metric approaches, have shown promising results in learning structured low-dimensional representation space from pixel observations, where the distance between states is measured based on task-relevant features. However, these approaches face challenges in demanding generalization tasks and scenarios with non-informative rewards. This is because they fail to capture sufficient long-term information in the learned representations. To address these challenges, we propose a novel State Chrono Representation (SCR) approach. SCR augments state metric-based representations by incorporating extensive temporal information into the update step of bisimulation metric learning. It learns state distances within a temporal framework that considers both future dynamics and cumulative rewards over current and long-term future states. Our learning strategy effectively incorporates future behavioral information into the representation space without introducing a significant number of additional parameters for modeling dynamics. Extensive experiments conducted in DeepMind Control and Meta-World environments demonstrate that SCR achieves better performance comparing to other recent metric-based methods in demanding generalization tasks. The codes of SCR are available in https://github.com/jianda-chen/SCR.

## 1 Introduction

In deep reinforcement learning (Deep RL), one of the key challenges is to derive an optimal policy from highly dimensional environmental observations, particularly images [5; 12; 33]. The stream of images received by an RL agent contains temporal relationships and significant spatial redundancy. This redundancy, along with potentially distracting visual inputs, poses difficulties for the agent in learning optimal policies. Numerous studies have highlighted the importance of building state representations that can effectively distinguish task-relevant information from task-irrelevant surroundings [45; 46; 8]. Such representations have the potential to facilitate the RL process and improve the generalizability of learned policies. As a result, representation learning has emerged as a fundamental aspect in the advancement of Deep RL algorithms, gaining increased attention within the RL community [21].

**Related Works.** The main objective of representation learning in RL is to develop a mapping function that transforms high-dimensional observations into low-dimensional embeddings. This process helps reduce the influence of redundant signals and simplify the policy learning process. Previous research has utilized autoencoder-like reconstruction losses [43; 18], which have shownimpressive results in various visual RL tasks. However, due to the reconstruction of all visual input including noise, these approaches often lead to overfitting on irrelevant environmental signals. Data augmentation methods [42, 22, 23] have demonstrated promise in tasks involving noisy observations. These methods primarily enhance perception models without directly impacting the policy within the Markov Decision Process (MDP) framework. Other approaches involve learning auxiliary tasks [33], where the objective is to predict additional tasks related to the environment using the learned representation as input. However, these auxiliary tasks are often designed independently of the primary RL objective, which can potentially limit their effectiveness in improving the overall RL performance.

In recent advancements, behavioral metrics, such as the bisimulation metric [11, 10, 6, 4, 5, 47] and the MICo distance [7], have been developed to measure the dissimilarity between two states by considering differences in immediate reward signals and the divergence of next-state distributions. Behavioral metrics-based methods establish approximate metrics within the representation space, preserving the behavioral similarities among states. This means that state representations are constrained within a structured metric space, wherein each state is positioned or clustered relative to others based on their behavioral distances. Moreover, behavioral metrics have been proven to set an upper bound on state-value discrepancies between corresponding states. By learning behavioral metrics within representations, these methods selectively retain task-relevant features essential for achieving the optimal policy, which involves maximizing the value function and shaping agent behaviors. At the same time, they filter out noise that is unrelated to state values and behavioral metrics, thereby improving robustness in noisy environments.

However, behavioral metrics face challenges when handling demanding generalizable RL tasks and scenarios with sparse rewards [20]. While they can capture long-term behavioral metrics to some extent through the temporal-difference update mechanism, their reliance on one-step transition data limits their learning efficiency, especially in the case of sparse rewards. This limitation may result in representations that encode non-informative signals [20] and can restrict their effectiveness in capturing long-term reward information. Some model-based approaches attempt to mitigate these issues by learning transition models [14, 16, 27]. However, learning a large transition model with long trajectories requires a large number of parameters and significant computational resources. Alternatively, \(N\)-step reinforcement learning methods can be used to address the problem [48]. Nevertheless, these methods introduce higher variance in value estimation compared to one-step methods, which may lead to unstable training.

**Contributions.** To overcome the above challenges, we introduce the State Chrono Representation (SCR) learning framework. This metric-based approach enables the learning of long-term behavioral representations and accumulated rewards that span from present to future states. Within the SCR framework, we propose the training of two distinct state encoders. One encoder focuses on crafting a state representation for individual states, while the other specializes in generating a _Chronological Embedding_ that captures the relationship between a state and its future states. In addition to learning the conventional behavioral metric for state representations, we introduce a novel behavioral metric specifically designed for temporal state pairs. This new metric is approximated within the chronological embedding space. To efficiently approximate this behavioral metric in a lower-dimensional vector space, we propose an alternative distance metric that differs from the typical \(L_{p}\) norm. To incorporate long-term rewards information into these representations, we introduce a "measurement" that quantifies the sum of rewards between the current and future states. Instead of directly regressing this measurement, we impose two constraints to restrict its range and value. Note that SCR is a robust representation learning methodology that can be integrated into any existing RL algorithm.

In summary, our contributions are threefold: 1) We introduce a new framework, SCR, for representation learning with a focus on behavioral metrics involving temporal state pairs. We also provide a practical method for approximating these metrics; 2) We develop a novel measurement specifically adapted for temporal state pairs and propose learning algorithms that incorporate this measurement while enforcing two constraints; 3) Through experiments conducted on the Distracting DeepMind Control Suite [34], we demonstrate that our proposed representation exhibits enhanced generalization and efficiency in challenging generalization tasks.

## 2 Preliminary

**Markov Decision Process:** A Markov Decision Process (MDP) is a mathematical framework defined by the tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,r,\gamma)\). Here \(\mathcal{S}\) represents the state space, which encompasses all possible states. \(\mathcal{A}\) denotes the action space, comprising all possible actions that an agent can take in each state. The state transition probability function, \(P\), defines the probability of transitioning from the current state \(s_{t}\in\mathcal{S}\) to any subsequent state \(s_{t+1}\in\mathcal{S}\) at time \(t\). Given the action \(a_{t}\in\mathcal{A}\) taken, the probability is denoted as \(P(s_{t+1}|s_{t},a_{t})\). The reward function, \(r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), assigns an immediate reward \(r(s_{t},a_{t})\) to the agent for taking action \(a_{t}\) in state \(s_{t}\). The discount factor, \(\gamma\in[0,1]\), determines the present value of future rewards.

A policy, \(\pi:S\times A\), is a strategy that specifies the probability \(\pi(a|s)\) of taking action \(a\) in state \(s\). The objective in an MDP is to find the optimal policy \(\pi^{*}\) that maximizes the expected discounted cumulative reward, \(\pi^{*}=\arg\max_{\pi}\mathbb{E}_{a\sim\pi}[\sum_{t}\gamma^{t}r(s_{t},a_{t})]\).

**Behavioral Metric:** In DBC [47], the bisimulation metric defines a pseudometric \(d:\mathcal{S}\times\mathcal{S}\rightarrow\mathbb{R}\) to measure the distance between two states. Additionally, a variant of bisimulation metric, known as \(\pi\)-bisimulation metric, is defined with respect to a specific policy \(\pi\).

**Theorem 2.1** (\(\pi\)-bisimulation metric).: _The \(\pi\)-bisimulation metric update operator \(\mathcal{F}_{bisim}:\mathbb{M}\rightarrow\mathbb{M}\) is defined as_

\[\mathcal{F}_{bisim}d(\mathbf{x},\mathbf{y}):=|r_{\mathbf{x}}^{\pi}-r_{\mathbf{ y}}^{\pi}|+\gamma\mathcal{W}(d)(P_{\mathbf{x}}^{\pi},P_{\mathbf{y}}^{\pi}),\]

_where \(\mathbb{M}\) is the space of \(d\), \(r_{\mathbf{x}}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|\mathbf{x})r_{\mathbf{x}}^{a}\), \(P_{\mathbf{x}}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|\mathbf{x})P_{\mathbf{x}}^{a}\), \(\mathcal{W}\) is the Wasserstein distance, and \(r_{\mathbf{x}}^{a}\) is \(r(\mathbf{x},a)\) for short. \(\mathcal{F}_{bisim}\) has a unique least fixed point \(d_{bisim}^{\pi}\)._

MICo [7] defines an alternative metric that samples the next states instead of directly measuring the intractable Wasserstein distance.

**Theorem 2.2** (MICo distance).: _The MICo distance update operator \(\mathcal{F}_{MICo}:\mathbb{M}\rightarrow\mathbb{M}\) is defined as_

\[\mathcal{F}_{MICo}d(\mathbf{x},\mathbf{y}):=|r_{\mathbf{x}}^{\pi}-r_{\mathbf{ y}}^{\pi}|+\gamma\mathbb{E}_{\mathbf{x}^{\prime}\sim P_{\mathbf{x}}^{\pi}, \mathbf{y}^{\prime}\sim P_{\mathbf{y}}^{\pi}}d(\mathbf{x}^{\prime},\mathbf{y} ^{\prime}),\]

\(\mathcal{F}_{MICo}\) _has a fixed point \(d_{MICo}^{\pi}\)._

## 3 State Chrono Representation

Although the bisimulation metric [47] and MICo [7] have their strengths, they do not adequately capture future information. This limitation can degrade the effectiveness of state representations in policy learning. To address this issue and incorporate future details, we propose a novel approach, **State Chrono Representation** (SCR). The detailed architecture of SCR is illustrated in Figure 1.

SCR consists of two key components: a **state representation**\(\phi(\mathbf{x})\in\mathbb{R}^{n}\) for a given state \(\mathbf{x}\), and a **chronological embedding**\(\psi(\mathbf{x}_{i},\mathbf{x}_{j})\in\mathbb{R}^{n}\) that captures the relationship between a current state \(\mathbf{x}_{i}\) and its future state \(\mathbf{x}_{j}\) within the same trajectory. The state representation, \(\phi(\mathbf{x})\), is developed using a behavioral metric \(d\) that quantifies the reward and dynamic divergence between two states. On the other hand, the chronological embedding, \(\psi(\mathbf{x}_{i},\mathbf{x}_{j})\), fuses these two state representations using deep learning, with a focus on capturing the long-term behavioral correlation between the current state \(\mathbf{x}_{i}\) and the future state \(\mathbf{x}_{j}\). To compute the distance between any two chronological embeddings, we propose a "chronological" behavioral metric, which is further improved through a Bellman operator-like MSE loss [36]. Moreover, \(\phi(\mathbf{x})\) incorporates a novel **temporal measurement**\(m\) to evaluate the transition from the current state to a future state, effectively capturing sequential reward data from the optimal behavior. Due to the complexity of the regression target, this temporal measurement operates within the defined lower and upper constraints, providing a stable prediction target for both the measurement and state representation during the learning process.

Figure 1: Overall architecture of SCR.

### Metric Learning for State Representation

The state representation encoder \(\phi\) is trained by approximating a behavioral metric, such as the MICo distance. In our model, we utilize a MICo-based metric transformation operator. Instead of using sampling-based prediction in MICo, we employ latent dynamics-based modeling to assess the divergence between two subsequent state distributions. This approach draws inspiration from the methodology in SimSR [45]. The metric update operator for latent dynamics, denoted by \(\mathcal{F}\), is defined as follows.

**Theorem 3.1**.: _Let \(\hat{d}:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}\) be a metric in the latent state representation space, \(d_{\phi}(\mathbf{x}_{i},\mathbf{y}_{i^{\prime}}):=\hat{d}(\phi(\mathbf{x}_{i} ),\phi(\mathbf{y}_{i^{\prime}}))\) be a metric in the state domain. The metric update operator \(\mathcal{F}\) is defined as,_

\[\mathcal{F}d_{\phi}(\mathbf{x}_{i},\mathbf{y}_{i^{\prime}})=|r_{ \mathbf{x}_{i}}-r_{\mathbf{y}_{i^{\prime}}}|+\gamma\mathbb{E}_{\begin{subarray} {c}\phi(\mathbf{x}_{i+1})\sim\hat{P}(\cdot|\phi(\mathbf{x}_{i}),a_{\mathbf{x} _{i}})\\ \phi(\mathbf{y}_{i^{\prime}+1})\sim\hat{P}(\cdot|\phi(\mathbf{y}_{i^{\prime}}),a_{\mathbf{y}_{i^{\prime}}})\end{subarray}}\ \hat{d}(\phi(\mathbf{x}_{i+1}),\phi(\mathbf{y}_{i^{\prime}+1})),\] (1)

_where \(a_{\mathbf{x}_{i}}\) and \(a_{\mathbf{y}_{i^{\prime}}}\) are the actions in states \(\mathbf{x}_{i}\) and \(\mathbf{y}_{i^{\prime}}\), respectively, and \(\hat{P}\) is the learned latent dynamics model. \(\hat{\mathcal{F}}\) has a fixed point \(d_{\phi}^{\pi}\)._

Proof.: Refer to Appendix A.3.1 for the detailed proof. 

To learn the approximation for \(d_{\phi}^{\pi}\) in the embedding space, one needs to specify the form of distance \(\hat{d}\) for latent vectors. Castro et al. [7] showed that a behavioral metric with a sample-based next state distribution divergence is a diffuse metric, as the divergence of the next state distribution corresponds to the Lukaszyk-Karmowski distance [49] that measures the expected distance between two samples drawn from two distributions, respectively (detailed definition is in Appendix A.4.1).

**Definition 3.2** (Diffuse metric [7]).: A function \(d:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\) based on a vector space \(\mathcal{X}\) is a diffuse metric if the following axioms hold: 1) \(d(\mathbf{a},\mathbf{b})\geq 0\) for any \(\mathbf{a},\mathbf{b}\in\mathcal{X}\); 2) \(d(\mathbf{a},\mathbf{b})=d(\mathbf{b},\mathbf{a})\) for any \(\mathbf{a},\mathbf{b}\in\mathcal{X}\); 3) \(d(\mathbf{a},\mathbf{b})+d(\mathbf{b},\mathbf{c})\geq d(\mathbf{a},\mathbf{c})\) for any \(\mathbf{a},\mathbf{b},\mathbf{c}\in\mathcal{X}\).

MICo provides an approximation of the behavioral metric using an angular distance: \(\hat{d}_{MICo}(\mathbf{a},\mathbf{b})=\frac{\|\mathbf{a}\|_{2}^{2}+\|\mathbf{ b}\|_{2}^{2}}{2}+\beta\theta(\mathbf{a},\mathbf{b})\), where \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{n}\), \(\theta(\mathbf{a},\mathbf{b})\) represents the angle between vectors \(\mathbf{a}\) and \(\mathbf{b}\), and \(\beta=0.1\) is a hyperparameter. This distance calculation includes a non-zero self-distance, which makes it compatible with expressing the Lukaszyk-Karmowski distance [49]. However, the angle function \(\theta(\mathbf{a},\mathbf{b})\) only considers the angle between \(\mathbf{a}\) and \(\mathbf{b}\), which requires computations involving cosine similarity and the \(\arccos\) function. This can sometimes lead to numerical discrepancies. DBC [47] recommends using the \(L_{1}\) norm with zero self-distance, which is only suitable for the Wasserstein distance. On the other hand, SimSR [45] utilizes the cosine distance, derived from cosine similarity, but it does not satisfy the triangle inequality and does not have a non-zero self-distance.

To address the challenges mentioned above, we propose a revised distance, \(\hat{d}(\mathbf{a},\mathbf{b})\), in the embedding space. It is characterized as a diffuse metric and formulated as follows:

**Definition 3.3**.: We define \(\hat{d}:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}\) as a distance function, where \(\hat{d}(\mathbf{a},\mathbf{b})=\sqrt{\|\mathbf{a}\|_{2}^{2}+\|\mathbf{b}\|_{2}^ {2}-\mathbf{a}^{\top}\mathbf{b}}\), for any \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{n}\).

**Theorem 3.4**.: \(\hat{d}\) _is a diffuse metric._

Proof.: Refer to Appendix A.3.2 for the detailed proof. 

**Lemma 3.5** (Non-zero self-distance).: _The self-distance of \(\hat{d}\) is not strict to zero, i.e., \(\hat{d}(\mathbf{a},\mathbf{a})=\|\mathbf{a}\|_{2}\geq 0\). It becomes zero if and only if every element in vector \(\mathbf{a}\) is zero._

Theorem 3.4 validates that \(\hat{d}\) is a diffuse metric satisfying triangle inequality, and Lemma 3.5 shows \(\hat{d}\) has non-zero self-distance capable of approximating dynamic divergence, which is a Lukaszyk-Karmowski distance. Moreover, the structure of \(\hat{d}\) closely resembles the \(L_{2}\) norm, with the only difference being that the factor of \(\mathbf{a}^{\top}\mathbf{b}\) is \(-1\) instead of \(-2\). This construction, which involves only vector inner product and square root computations without divisions or trigonometric functions, helps avoid numerical computational issues and simplify the implementation.

To learn the representation function \(\phi\), a common approach is to minimize MSE loss between the two ends of (1). The loss, which incorporates \(\hat{d}\) and is dependent on \(\phi\), can be expressed as follows:

\[\mathcal{L}_{\phi}(\phi)=\mathbb{E}_{\begin{subarray}{c}\mathbf{x}_{i},\mathbf{ y}_{i^{\prime}},r_{\mathbf{x}_{i}},r_{\mathbf{y}_{i^{\prime}}}\sim\mathcal{D}\\ \phi(\mathbf{x}_{i+1}),\phi(\mathbf{y}_{i^{\prime}+1})\sim\hat{P}\end{subarray}} \left|\hat{d}(\phi(\mathbf{x}_{i}),\phi(\mathbf{y}_{i^{\prime}}))\right.-|r_{ \mathbf{x}_{i}}-r_{\mathbf{y}_{i^{\prime}}}|-\gamma\hat{d}(\phi(\mathbf{x}_{i+ 1}),\phi(\mathbf{y}_{i^{\prime}+1}))\right|^{2},\] (2)

where \(\mathcal{D}\) represents the replay buffer or the sampled rollouts used in the RL learning process.

#### 3.1.1 Long-term Temporal Information

The loss \(\mathcal{L}_{\phi}(\phi)\) in Eqn. (2) heavily relies on the temporal-difference update mechanism, utilizing only one-step transition information. However, this approach fails to effectively capture and encode long-term information from trajectories. Incorporating temporal information into the representation while maintaining the structure and adhering to behavioral metrics presents a complex challenge. To address this challenge, we propose two distinct methods: Chronological Embedding (Section 3.2) and Temporal Measurement (Section 3.3). Each technique is designed to capture the temporal essence of a rollout denoted as \(\tau(\mathbf{x}_{i};\mathbf{x}_{j})\), which represents a sequence starting from state \(\mathbf{x}_{i}\) and reaching a future state \(\mathbf{x}_{j}\).

As illustrated in Figure 2, the goal of the chronological embedding is to create a novel paired state embedding, denoted as \(\psi(\mathbf{x}_{i},\mathbf{x}_{j})\), capturing the temporal representation of the rollout \(\tau(\mathbf{x}_{i};\mathbf{x}_{j})\). This is achieved by transforming a pair of state representations \(\phi(\mathbf{x}_{i})\) and \(\phi(\mathbf{x}_{j})\) to a vector: \(\psi(\mathbf{x}_{i},\mathbf{x}_{j})\):=\(\psi(\phi(\mathbf{x}_{i}),\phi(\mathbf{x}_{j}))\in\mathbb{R}^{n}\). The objective of learning \(\psi\) is to develop a "chronological" behavioral metric \(d_{\psi}\) that quantifies the distance between rollouts \(\tau(\mathbf{x}_{i};\mathbf{x}_{j})\) and \(\tau(\mathbf{y}_{i^{\prime}};\mathbf{y}_{j^{\prime}})\). On the other hand, temporal measurement aims to compute a specific "distance" \(m\) between states \(\mathbf{x}_{i}\) and \(\mathbf{x}_{j}\), which provides insights into the cumulative rewards accumulated during the rollout \(\tau(\mathbf{x}_{i};\mathbf{x}_{j})\).

Note that learning the state representation solely based on either \(d_{\psi}\) or \(m\) poses challenges. While \(d_{\psi}\) improves the representation expressiveness, capturing both rewards and dynamics may be less efficient in the learning process. On the other hand, learning \(m\) is efficient but may not adequately capture the dynamics. Therefore, we propose a unified approach that leverages the strengths of both \(d_{\psi}\) and \(m\) to overcome the challenges and improve the overall state representation. Empirical results in Section 4.3 support the effectiveness of our proposal.

### Chronological Embedding

The **chronological embedding**, denoted by \(\psi(\mathbf{x}_{i},\mathbf{x}_{j})\in\mathbb{R}^{n}\), is designed to capture the relationship between a given state \(\mathbf{x}_{i}\) and any of its future states \(\mathbf{x}_{j}\) over the same trajectory. To capture the comprehensive behavioral knowledge, we introduce a distance function \(d_{\psi}:(\mathcal{S}\times\mathcal{S})\times(\mathcal{S}\times\mathcal{S}) \rightarrow\mathbb{R}\), which is intended to reflect the behavioral metric and enable the encoder \(\psi\) to incorporate behavioral information. Building upon the MICo distance in Theorem 2.2, we specify the metric update operator \(\mathcal{F}_{Chrono}\) for \(d_{\psi}\) in the following theorem. Proof is detailed in Appendix A.3.3.

**Theorem 3.6**.: _Let \(\mathbb{M}_{\psi}\) be the space of \(d_{\psi}\). The metric update operator \(\mathcal{F}_{Chrono}:\mathbb{M}_{\psi}\rightarrow\mathbb{M}_{\psi}\) is defined as follows,_

\[\mathcal{F}_{Chrono}d_{\psi}(\mathbf{x}_{i},\mathbf{x}_{j},\mathbf{y}_{i^{ \prime}},\mathbf{y}_{j^{\prime}})=|r_{\mathbf{x}_{i}}-r_{\mathbf{y}_{i^{\prime} }}|+\gamma\mathbb{E}_{\mathbf{x}_{i+1}\sim P_{\psi}^{\pi},\mathbf{y}_{i^{ \prime}+1}\sim P_{\psi}^{\pi}}d_{\psi}(\mathbf{x}_{i+1},\mathbf{x}_{j}, \mathbf{y}_{i^{\prime}+1},\mathbf{y}_{j^{\prime}}),\] (3)

_where time step \(i<j\) and \(i^{\prime}<j^{\prime}\). \(\mathcal{F}_{Chrono}\) has a fixed point \(d_{\psi}^{\pi}\)._

Here \(d_{\psi}^{\pi}\) represents the "chronological" behavioral metric. Our objective is to closely approximate \(d_{\psi}^{\pi}\), which measures the distance between two sets of states \((\mathbf{x}_{i},\mathbf{x}_{j})\) and \((\mathbf{y}_{i^{\prime}},\mathbf{y}_{j^{\prime}})\), taking into account the difference in immediate rewards and dynamics divergence. To co-learn the encoder \(\psi\) with \(d_{\psi}^{\pi}\), we represent \(d_{\psi}^{\pi}\) as \(d_{\psi}^{\pi}(\mathbf{x}_{i},\mathbf{x}_{j},\mathbf{y}_{i^{\prime}},\mathbf{y }_{j^{\prime}}):=\hat{d}(\psi(\mathbf{x}_{i},\mathbf{x}_{j}),\psi(\mathbf{y}_{ i^{\prime}},\mathbf{y}_{j^{\prime}}))\), where \(\hat{d}\) is defined in Definition 3.3. Similar to Eqn. (1), we construct \(d_{\psi}^{\pi}\) to compute the distance in the embedding space.

\[d_{\psi}^{\pi}(\mathbf{x}_{i},\mathbf{x}_{j},\mathbf{y}_{i^{\prime}},\mathbf{y }_{j^{\prime}})=|r_{\mathbf{x}_{i}}-r_{\mathbf{y}_{i^{\prime}}}|+\gamma \mathbb{E}_{\mathbf{x}_{i+1}\sim P_{\psi}^{\pi},\mathbf{y}_{i^{\prime}+1}\sim P _{\psi}^{\pi}}\hat{d}(\psi(\mathbf{x}_{i+1},\mathbf{x}_{j}),\psi(\mathbf{y}_{ i^{\prime}+1},\mathbf{y}_{j^{\prime}})).\] (4)

Figure 2: An example with two rollouts.

To ensure computational efficiency, the parameters of the encoders \(\phi\) and \(\psi\) are shared. The encoder \(\psi\) extracts outputs from \(\phi\), and the distance measure is appropriately adapted as \(d^{\pi}_{\psi}(\mathbf{x}_{i},\mathbf{x}_{j},\mathbf{y}_{i^{\prime}},\mathbf{y}_ {j^{\prime}}):=\hat{d}(\psi(\phi(\mathbf{x}_{i}),\phi(\mathbf{x}_{j})),\psi( \phi(\mathbf{y}_{i^{\prime}}),\phi(\mathbf{y}_{j^{\prime}})))\). The objective of learning the chronological embedding is to minimize the MSE loss between both sides of Eqn. (4) w.r.t. \(\phi\) and \(\psi\),

\[\mathcal{L}_{\psi}(\psi,\phi)=\mathbb{E}_{\begin{subarray}{c} \mathbf{x}_{i},\mathbf{x}_{j},\mathbf{y}_{i^{\prime}},\mathbf{y}_{j^{\prime} },r_{\mathbf{x}_{i}},\\ \mathbf{r}_{\mathbf{y}_{i^{\prime}}},\mathbf{x}_{i+1},\mathbf{y}_{i^{\prime} +1}\sim\mathcal{D}\end{subarray}}\left|\hat{d}(\psi(\phi(\mathbf{x}_{i}),\phi( \mathbf{x}_{j})),\psi(\phi(\mathbf{y}_{i^{\prime}}),\phi(\mathbf{y}_{j^{\prime }})))-|r_{\mathbf{x}_{i}}-r_{\mathbf{y}_{i^{\prime}}}|\right.\] \[\left.-\gamma\hat{d}(\psi(\phi(\mathbf{x}_{i+1}),\phi(\mathbf{x}_ {j})),\psi(\phi(\mathbf{y}_{i^{\prime}+1}),\phi(\mathbf{y}_{j^{\prime}}))) \right|^{2}.\] (5)

Minimizing Eqn. (5) is to encourage the embeddings of similar state sequences to become closer in the embedded space, thereby strengthening the classification of similar behaviors.

### Temporal Measurement

To enhance the ability of SCR to gain future insights, we introduce the concept of **temporal measurement**, which quantifies the disparities between a current state \(\mathbf{x}_{i}\) and a future state \(\mathbf{x}_{j}\). This measurement, denoted by \(m(\mathbf{x}_{i},\mathbf{x}_{j})\), aims to measure the differences in state values or the cumulative rewards obtained between the current and future states. We construct an approximate measurement based on the state representation \(\phi\), denoted by \(\hat{m}_{\phi}(\mathbf{x}_{i},\mathbf{x}_{j}):=\hat{m}(\phi(\mathbf{x}_{i}), \phi(\mathbf{x}_{j}))\). Here, \(\hat{m}:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}\) is a non-parametric asymmetric metric function (details are presented at the end of this section). This approach structures the representation space of \(\phi(\mathbf{x})\) around the "distance" \(m\), ensuring that \(\phi(\mathbf{x})\) contains sufficient information for future state planning.

We propose using \(m\) to represent the expected discounted accumulated rewards obtained by an optimal policy \(\pi^{*}\) from state \(\mathbf{x}_{i}\) to state \(\mathbf{x}_{j}\): \(m(\mathbf{x}_{i},\mathbf{x}_{j})=\mathbb{E}_{\pi^{*}}\left[\sum_{t=0}^{j-i} \gamma^{t}r_{\mathbf{x}_{i}}\left|\mathbf{s}_{0}=\mathbf{x}_{i},\mathbf{s}_{ j-i}=\mathbf{x}_{j}\right.\right]\). However, obtaining the exact value of \(m(\mathbf{x}_{i},\mathbf{x}_{j})\) is challenging because the optimal policy \(\pi^{*}\) is unknown and is, in fact, the primary goal of the RL task. Instead of directly approximating \(m(\mathbf{x}_{i},\mathbf{x}_{j})\), we learn the approximation \(\hat{m}(\mathbf{x}_{i},\mathbf{x}_{j})\) in an alternative manner that ensures it falls within a feasible range covering the true \(m(\mathbf{x}_{i},\mathbf{x}_{j})\).

To construct this range, we introduce two constraints. The first constraint, serving as a **lower** boundary, states that the expected discounted cumulative reward obtained by any policy \(\pi\), whether optimal or not, cannot exceed \(m\):

\[\mathbb{E}_{\pi}\left[\sum_{t=0}^{j-i}\gamma^{t}r_{\mathbf{s}_{t}}\left| \mathbf{s}_{0}=\mathbf{x}_{i},\mathbf{s}_{j-i}=\mathbf{x}_{j}\right]\leq m( \mathbf{x}_{i},\mathbf{x}_{j}).\] (6)

This constraint is based on a fact that any sub-optimal policy is inferior to the optimal policy. Based on this constraint, we propose the following objective to learn the approximation \(\hat{m}_{\phi}\):

\[\mathcal{L}_{low}(\phi)=\mathbb{E}_{\pi(\mathbf{x}_{i};\mathbf{x}_{j})\sim\pi} \left|\text{ReLU}\left(\sum_{t=0}^{j-i}\gamma^{t}r_{\mathbf{x}_{t}}-\hat{m}( \phi(\mathbf{x}_{i}),\phi(\mathbf{x}_{j}))\right)\right|^{2},\] (7)

where \(\text{ReLU}(x)=\text{max}(0,x)\). This objective is non-zero when the constraint in Eqn. (6) is not satisfied. Hence, it increases the value of \(m(\phi(\mathbf{x}_{i}),\phi(\mathbf{x}_{j}))\) until it exceeds the sampled reward sum.

The second constraint serving as the upper boundary is proposed based on Fig. 2. To ensure that the absolute value \(|m(\mathbf{x}_{i},\mathbf{x}_{j})|\) remains within certain limits, we propose the following inequality:

\[|\hat{m}(\mathbf{x}_{i},\mathbf{x}_{j})|\leq d(\mathbf{x}_{i},\mathbf{y}_{i^{ \prime}})+|\hat{m}(\mathbf{y}_{i^{\prime}},\mathbf{y}_{j^{\prime}})|+d( \mathbf{x}_{j},\mathbf{y}_{j^{\prime}}),\] (8)

where \(d\) is the behavioral metric introduced in Section 3.1. The right-hand side of the inequality represents the longer path from \(\mathbf{x}_{i}\) to \(\mathbf{x}_{j}\). This inequality demonstrates that the absolute temporal measurement \(|m(\mathbf{x}_{i},\mathbf{x}_{j})|\) should not exceed the sum of the behavioral metrics at the initial states (\(\mathbf{x}_{i}\) and \(\mathbf{y}_{i^{\prime}}\)), i.e., \(d(\mathbf{x}_{i},\mathbf{y}_{i^{\prime}})\), the final states pair (\(\mathbf{x}_{j}\) and \(\mathbf{y}_{j^{\prime}}\)), i.e. \(d(\mathbf{x}_{j},\mathbf{y}_{j^{\prime}})\), and the measurement \(|m(\mathbf{y}_{i},\mathbf{y}_{j})|\). This constraint leads to the following objective for training \(\hat{m}_{\phi}\):

\[\mathcal{L}_{up}(\phi)\!=\!\left|\text{ReLU}\!\left(\!\left|\hat{m}(\phi( \mathbf{x}_{i}),\phi(\mathbf{x}_{j}))\right|\!-\!\text{sg}\!\left(\!\hat{d}( \phi(\mathbf{x}_{i}),\phi(\mathbf{y}_{i^{\prime}}))\!+\!\hat{d}(\phi(\mathbf{x}_ {j}),\phi(\mathbf{y}_{j^{\prime}}))\!+\!\hat{m}(\phi(\mathbf{y}_{i^{\prime}}), \phi(\mathbf{y}_{j^{\prime}}))\!\right)\!\right)\!\right)\!\right|^{2}\!\!\!,\]

where sg means "stop gradient". This objective aims to decrease the value of \(\hat{m}_{\phi}\) when the constraint in Eqn. (8) is not satisfied. By simultaneously optimizing both constraints in a unified manner,we guarantee that the approximated temporal measurement, \(m\), is confined within a specific range, defined by the lower and upper constraints. The overall objective for \(\hat{m}\) is formulated as follows:

\[\mathcal{L}_{\hat{m}}(\phi)=\mathcal{L}_{low}(\phi)+\mathcal{L}_{up}(\phi).\] (9)

**Asymmetric Metric Function for \(\hat{m}\).** The measurement \(\hat{m}(\mathbf{x}_{i},\mathbf{x}_{j})\), designed to measure the distance based on the rewards, is intended to be **asymmetric** with respect to \(\mathbf{x}_{i}\) and \(\mathbf{x}_{j}\). This is because we assume that state \(\mathbf{x}_{i}\) comes before \(\mathbf{x}_{j}\), resulting in a distinct relationship compared to the progression from \(\mathbf{x}_{j}\) to \(\mathbf{x}_{i}\). Recent research has focused on studying quasimetrics in deep learning [28; 39; 38] and developed various methodologies to compute asymmetric distances. In our method, we choose to utilize Interval Quasimetric Embedding (IQE) [38] to implement \(\hat{m}\) (details are in Appendix A.5.1).

### Overall Objective

As shown in Figure 1, the encoders are designed to predict state representations \(\phi(\mathbf{x})\) for individual states and chrono embedding \(\psi(\mathbf{x}_{i},\mathbf{x}_{j})\) to capture the relationship between states \(\mathbf{x}_{i}\) and \(\mathbf{x}_{j}\). The measurement \(\hat{m}\) is then computed based on \(\phi(\mathbf{x}_{i})\) and \(\phi(\mathbf{x}_{j})\) to incorporate the accumulated rewards between these states. The components \(\psi\) and \(\hat{m}\) work together to enhance the state representations \(\phi\) and capture temporal information, thereby improving their predictive capabilities for future insights. The necessity of \(\psi\) and \(\hat{m}\) is demonstrated in the ablation study in Section 4.3. Therefore, a comprehensive objective to minimize is formulated in a unified manner as follows,

\[\mathcal{L}(\phi,\psi)=\mathcal{L}_{\phi}(\phi)+\mathcal{L}_{\psi}(\psi,\phi) +\mathcal{L}_{\hat{m}}(\phi).\] (10)

Our method, SCR, can be seamlessly integrated with a wide range of deep RL algorithms, which can effectively utilize the representation \(\phi(\mathbf{x})\) as a crucial input component. In our implementation, we specifically employ Soft Actor-Critic (SAC) [13] as our foundation RL algorithm. The state representation serves as the input state for both the policy network and Q-value network in SAC. Other implementation details can be found in Appendix A.5.

## 4 Experiments

### Configurations

**DeepMind Control Suite.** The primary objective of our proposed SCR is to develop a robust and generalizable representation for deep RL when dealing with high-dimensional observations. To evaluate its effectiveness, we conduct experiments using the DeepMind Control Suite (DM_Control) environment, which involves rendered pixel observations [37] and a distraction setting called Distracting Control Suite [34]. This environment utilizes the MuJoCo engine, offering pixel observations for continuous control tasks. The DM_Control environment provides diverse testing scenarios, including background and camera pose distractions, simulating real-world complexities of camera inputs. (1) _Default setting_ evaluates the effectiveness of each RL approach. Frames are rendered at a resolution of \(84\times 84\) RGB pixels as shown in Figure 3(a). We stack three frames as states for the RL agents. (2) _Distraction setting_ evaluates both the generalization and effectiveness of each RL approach. The distraction [34] consists of 3 components, as shown in Figure 3(b): 1) **background video** distraction, where the clean and simple background is replaced with a natural video; 2) **object color** distraction, which involves slight changes in the color of the robots; and 3) **camera pose** distraction, where the camera's position and angle are randomized during rendering from the simulator. We observe that tasks become significantly more challenging when the camera pose distraction is applied.

**Baselines.** We compare our method against several prominent algorithms in the field, including: 1) SAC [13], a baseline deep RL method for continuous control; 2) DrQ [42], a data augmentation

Figure 3: Examples of DM_Control with (a) default setting and (b) distraction setting.

method using random crop; 3) DBC [47], representation learning with the bisimulation metric; 4) MICo [7], representation learning with MICo distance; and 5) SimSR [45], representation learning with the behavioral metric approximated by the cosine distance.

### Main Results

**Default Setting.** To verify the sample efficiency of our method, we compare it with others on eight tasks in DM_Control and report the experimental results in Table 1. We trained each method on each task for 500K environment steps. All results are averaged over 10 runs. We observe that SCR achieves comparable results to the augmentation method DrQ and the state-of-the-art behavioral metric approach SimSR. It is worth noting that for a DM_Control task, the maximum achievable scores are 1000, and a policy that collects scores around 900 is considered nearly optimal. These findings highlight the effectiveness of our SCR in mastering standard RL control tasks.

**Distraction Setting.** To further evaluate the generalization ability of our method, we perform comparison experiments on DM_Control with Distraction using the same training configuration as in the default setting. We evaluate 10 different seeds, and conduct 100 episodes per run. The scores from each run are summarized in Table 2. The std is computed across the mean scores of these 10 runs. The results show that our method outperforms the others in all eight tasks, notably in the sparse reward task _cartpole-swing_up_sparse_, where other methods achieve nearly zero scores. The camera-pose distraction poses a significant challenge for metric-based methods like DBC, MICo, and SimSR, as it leads to substantial distortion of the robot's shape and position in the image state. DrQ outperforms other behavioral metric methods, possibly due to its random cropping technique, which facilitates better alignment of the robot's position. The aggregate metrics [2] are shown in Figure 4, where scores are normalized by dividing by 1000. Figure 5 presents the training curves, while additional curves can be found in Appendix B.3. These results demonstrate the superior performance of our method in handling distractions and highlight our strong generalization capabilities.

### Ablation Study

**Impact of Different Components.** To evaluate the impact of each component in SCR, we perform an ablation study where certain components are selectively removed or substituted. Figure 6 shows the training curves on _cheetah-run_ and _walker-walk_ in the distraction setting. SCR denotes the full

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline  & BC-Catch & C-SwingUp & C-SwingUpSparse & Ch-Run & F-Spin & H-Stand & R-Easy & W-Walk \\ \hline SAC & 447.5\(\pm\)14.1 & 63.3\(\pm\)10.3 & 30.1\(\pm\)12.7 & 354.4\(\pm\)15.4 & 518.6\(\pm\)29.3 & 79.3\(\pm\)15.6 & 321.1\(\pm\)95.4 & 363.7\(\pm\)154.4 \\ DrQ & **962.7\(\pm\)**19.6 & 640.3\(\pm\)8.2 & **709.0\(\pm\)**9.4 & 44.87\(\pm\)13.9 & 962.0\(\pm\)11.3 & **861.2\(\pm\)**0.9 & **970.2\(\pm\)**12.7 & **972.3\(\pm\)**14.5 \\ DBC & 104.2\(\pm\)33.3 & 281.3\(\pm\)27.6 & 65.9\(\pm\)80.3 & 386.1\(\pm\)16.6 & 730.3\(\pm\)13.6 & 43.5\(\pm\)5.6 & 179.8\(\pm\)27.4 & 330.3\(\pm\)41.4 \\ MICo & 215.4\(\pm\)14.1 & 803.2\(\pm\)12.0 & 0.2\(\pm\)0.0 & 4.9\(\pm\)18.1 & 2.0\(\pm\)0.1 & 800.8\(\pm\)21.1 & 186.1\(\pm\)19.4 & 297.3\(\pm\)3.3 \\ SimSR & 938.8\(\pm\)14.9 & **854.8\(\pm\)**11.5 & 217.9\(\pm\)307.5 & 255.3\(\pm\)327.5 & **962.4\(\pm\)**19.0 & 6.2\(\pm\)1.7 & 76.6\(\pm\)25.1 & **929.9\(\pm\)**21.1 \\ SCR & 944.2\(\pm\)11.7 & **849.4\(\pm\)**2.1 & **768.4\(\pm\)**15.1 & **778.4\(\pm\)**29.7 & **964.9\(\pm\)**25.5 & **851.3\(\pm\)**45.6 & 946.8\(\pm\)26.0 & **919.0\(\pm\)**20.0 \\ \hline \end{tabular}
\end{table}
Table 1: Results (mean\(\pm\)std) on DM_Control with the default setting at 500K environment steps.

Figure 4: Aggregate metrics on distract setting.

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline  & BC-Catch & C-SwingUp & C-SwingUpSparse & Ch-Run & F-Spin & H-Stand & R-Easy & W-Walk \\ \hline SAC & 82.6\(\pm\)20.2 & 218.4\(\pm\)4.9 & 0.7\(\pm\)0.7 & 177.4\(\pm\)8.4 & 22.5\(\pm\)27.7 & 19.6\(\pm\)26.2 & 149.4\(\pm\)77.5 & 167.2\(\pm\)8 \\ DrQ & 124.0\(\pm\)99.6 & 230.0\(\pm\)36.3 & 11.2\(\pm\)6.4 & 103.0\(\pm\)84.9 & 579.8\(\pm\)28.2 & 16.8\(\pm\)11.8 & 70.5\(\pm\)40.1 & 33.6\(\pm\)6.3 \\ DBC & 48.8\(\pm\)24.4 & 127.7\(\pm\)19.2 & 0.0\(\pm\)0.0 & 9.2\(\pm\)1.9 & 7.7\(\pm\)10.7 & 5.6\(\pm\)2.5 & 149.6\(\pm\)24.8 & 30.9\(\pm\)4.8 \\ MICo & 100.1\(\pm\)21.0 & 200.2\(\pm\)6.6 & 0.0\(\pm\)0.1 & 7.4\(\pm\)3.2 & 86.9\(\pm\)76.1 & 118.1\(\pm\)10.5 & 132.3\(\pm\)37.8 & 27.5\(\pm\)7.7 \\ SimSR & 106.4\(\pm\)13.5 & 148.4\(\pm\)17.3 & 0.0\(\pm\)0.0 & 28.2\(\pm\)23.8 & 0.4\(\pm\)0.1 & 6.6\(\pm\)1.2 & 78.4\(\pm\)17 & 28.6\(\pm\)3.1 \\ SCR & **211.3\(\pm\)**55.4 & **565.1\(\pm\)**59.9 & **185.7\(\pm\)**93.0 & **331.7\(\pm\)**1.4 & **738.3\(\pm\)**24.5 & **400.8\(\pm\)**19.2 & **666.1\(\pm\)**14.5 & **555.1\(\pm\)**31.2 \\ \hline \end{tabular}
\end{table}
Table 2: Results (mean\(\pm\)std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose.

model. SCR w/o \(\psi\) removes the chronological embedding \(\psi\). SCR w/o \(\hat{m}\) refers to the exclusion of the approximation \(\hat{m}\). SCR w/ \(\phi\) only removes losses \(\mathcal{L}_{\psi}(\phi,\psi)\) and \(\mathcal{L}_{\hat{m}}(\phi)\) but keep \(\mathcal{L}_{\phi}(\phi)\). SCR w/ **cos** replaces the distance function \(\hat{d}\) for computing metrics on the representation space with cosine distance, akin to SimSR does. SCR w/ MICo replaces \(\hat{d}\) with MICo's angular distance. SCR w/ **L1** replaces \(\hat{d}\) with the \(L_{1}\) distance as adopted by DBC. The results demonstrate the superior performance of the full model and the importance of \(\psi\) and \(\hat{m}\). The absence of these components can lead to worse performance and unstable training. The results also demonstrate that the proposed \(\hat{d}\) surpasses existing distance functions.

**Impact of Sampling Steps.** In our previous experiments, we uniformly sample the number of steps between \(i\) and \(j\) in the range \([1,100]\). In this experiment, we investigate the impact of this hyper-parameter on SCR. We conduct experiments on the _cheetah-run_ and _walker-walk_ tasks in the distraction setting with various step sampling ranges: \([1,10]\), \([1,50]\), \([1,100]\), and \([1,150]\). We also make comparisons with fixed step counts: 50 and 100. The results presented in Figure 7 show that sampling step counts in the range \([1,100]\) yields the optimal results and remains stable across tasks. Therefore, we set this hyper-parameter to uniformly sample in the range \([1,100]\) for all experiments.

### Experiments on Meta-World

We present experimental investigations conducted in Meta-World [44], a comprehensive simulated benchmark that includes distinct robotic manipulation tasks. Our focus is on six specific tasks: _basketball-v2_, _coffee-button-v2_, _door-open-v2_, _dranver-open-v2_, _pick-place-v2_, and _window-open-v2_, chosen for their diverse objects in environments. The observations are rendered as 84\(\times\)84 RGB pixels with a frame stack of 3, following the DM_Control format. Table 3 displays the average success rates over all tasks and five seeds, while Figure 8 shows the training curves for a subset of these tasks. Additional training curves for all tasks can be found in Appendix B.5. Our proposed SCR consistently achieves optimal performance in all tasks, while other baseline methods, except for DrQ, fail to converge to such high levels of performance. Even though DrQ demonstrates proficiency in achieving optimal performance, it shows less sampling efficiency than SCR, highlighting the effectiveness of SCR in the applied setting.

## 5 Conclusion

Our proposed State Chrono Representation learning framework (SCR) effectively captures information from complex, high-dimensional, and noisy inputs in deep RL. SCR improves upon previous metric-based approaches by incorporating a long-term perspective and quantifying state distances across temporal trajectories. Our extensive experiments demonstrate its effectiveness compared with several metric-based baselines in complex environments with distractions, making a promising avenue for future research on generalization in representation learning.

**Limitations.** This work does not address truly Partially Observable MDPs (POMDPs), which are common in real-world applications. In future work, we plan to integrate SCR with POMDP methods to solve real-world problems, or to design unified solutions that better accommodate these complexities.

## 6 Acknowledgement

We thank the anonymous reviewers for the constructive feedback. This study is supported under RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). Sinno J. Pan thanks the support of the Hong Kong Jockey Club Charities Trust to the JC STEM Lab of Integration of Machine Learning and Symbolic Reasoning.

## References

* Agarwal et al. [2021] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in Neural Information Processing Systems_, 2021.
* Barreto et al. [2017] Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. _Advances in neural information processing systems_, 30, 2017.
* Volume 1_, AAMAS '10, Richland, SC, 2010. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9780982657119.
* Castro [2020] Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov decision processes. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(06):10069-10076, Apr. 2020. doi: 10.1609/aaai.v34i06.6564. URL https://ojs.aaai.org/index.php/AAAI/article/view/6564.
* Castro et al. [2009] Pablo Samuel Castro, Prakash Panangaden, and Doina Precup. Equivalence relations in fully and partially observable markov decision processes. In _Proceedings of the 21st International Joint Conference on Artificial Intelligence_, IJCAI'09, San Francisco, CA, USA, 2009. Morgan Kaufmann Publishers Inc.

Figure 8: Training curves in Meta-World. Mean success rates on 5 runs with std (shadow shape). Training curves of all tasks are shown in Appendix B.5.

\begin{table}
\begin{tabular}{l|l l l l l l} \hline  & SAC & DrQ & DBC & MICo & SimSR & SCR \\ \hline Meta-World & 0.495 \(\pm\) 0.475 & 0.886 \(\pm\) 0.125 & 0.479 \(\pm\) 0.453 & 0.495 \(\pm\) 0.482 & 0.258 \(\pm\) 0.365 & **0.969**\(\pm\) 0.032 \\ \hline \end{tabular}
\end{table}
Table 3: Average success rates for six tasks in Meta-World.

* Castro et al. [2021] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. MICo: Improved representations via sampling-based state similarity for markov decision processes. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=wFp6KmQELgu.
* Chen and Pan [2022] Jianda Chen and Sinno Pan. Learning representations via a robust behavioral metric for deep reinforcement learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 36654-36666. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/eda9523faa5e7191aee1c2eaff669716-Paper-Conference.pdf.
* Chevalier-Boisvert et al. [2023] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo Perez-Vicente, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. In _Advances in Neural Information Processing Systems 36, New Orleans, LA, USA_, December 2023.
* Ferns and Precup [2014] Norm Ferns and Doina Precup. Bisimulation metrics are optimal value functions. UAI'14, pages 210-219, Arlington, Virginia, USA, 2014. AUAI Press. ISBN 9780974903910.
* Ferns et al. [2004] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. UAI '04, pages 162-169, Arlington, Virginia, USA, 2004. AUAI Press. ISBN 0974903906.
* Gelada et al. [2019] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. Deep-MDP: Learning continuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2170-2179. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/gelada19a.html.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 1861-1870. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html.
* Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In _International Conference on Learning Representations_, 2019.
* Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In _International Conference on Machine Learning_, pages 2555-2565, 2019.
* Hafner et al. [2023] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.
* Hansen-Estruch et al. [2022] Philippe Hansen-Estruch, Amy Zhang, Ashvin Nair, Patrick Yin, and Sergey Levine. Bisimulation makes analogies in goal-conditioned reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 8407-8426. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/hansen-estruch22a.html.
* Higgins et al. [2017] Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: Improving zero-shot transfer in reinforcement learning. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1480-1490. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/v70/higgins17a.html.

* Janner et al. [2020] Michael Janner, Igor Mordatch, and Sergey Levine. Gamma-models: Generative temporal difference learning for infinite-horizon prediction. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1724-1735. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/12ffb0968f2f56e51a59a6beb37b2859-Paper.pdf.
* Kemertas and Aumentado-Armstrong [2021] Mete Kemertas and Tristan Ty Aumentado-Armstrong. Towards robust bisimulation metric learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=ySFGLFjgIfN.
* Kirk et al. [2023] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktaschel. A survey of zero-shot generalisation in deep reinforcement learning. _J. Artif. Int. Res._, 76, feb 2023. ISSN 1076-9757. doi: 10.1613/jair.1.14174. URL https://doi.org/10.1613/jair.1.14174.
* Laskin et al. [2020] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 5639-5650. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/laskin20a.html.
* Laskin et al. [2020] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 19884-19895. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf.
* Lee et al. [2020] Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 741-752. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/08058bf500242562c0d031ff830ad094-Paper.pdf.
* Lee et al. [2020] Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio Guadarrama. Predictive information accelerates learning in rl. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 11890-11901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/89b9e0a6f6d1505fe13dea0f18a2dcfa-Paper.pdf.
* Liu et al. [2023] Qiyuan Liu, Qi Zhou, Rui Yang, and Jie Wang. Robust representation learning by clustering with bisimulation metrics for visual reinforcement learning with distractions. In _Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence_, AAAI'23/IAAI'23/EAAI'23. AAAI Press, 2023. ISBN 978-1-57735-880-0. doi: 10.1609/aaai.v37i7.26063. URL https://doi.org/10.1609/aaai.v37i7.26063.
* Nguyen et al. [2021] Tung D Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for model-based planning in latent space. In _International Conference on Machine Learning_, pages 8130-8139. PMLR, 2021.
* Pitis et al. [2020] Silviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba. An inductive bias for distances: Neural nets that respect the triangle inequality. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=HJeiDpVFPr.
* Pont-Tuset et al. [2017] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. _arXiv:1704.00675_, 2017.

* Reinke and Alameda-Pineda [2023] Chris Reinke and Xavier Alameda-Pineda. Successor feature representations. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=MTFf1rDDEI.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Schwarzer et al. [2021] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=uCQfPZwRaUu.
* Seo et al. [2022] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked world models for visual control. In _6th Annual Conference on Robot Learning_, 2022. URL https://openreview.net/forum?id=Bf6on2BHOJv.
* a challenging benchmark for reinforcement learning from pixels. _arXiv preprint arXiv:2101.02722_, 2021.
* Stooke et al. [2021] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 9870-9879. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/stooke21a.html.
* Sutton [2018] Richard S Sutton. Reinforcement learning: An introduction. _A Bradford Book_, 2018.
* Tunyasuvunakool et al. [2020] Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks for continuous control. _Software Impacts_, 6:100022, 2020. ISSN 2665-9638. doi: https://doi.org/10.1016/j.simpa.2020.100022. URL https://www.sciencedirect.com/science/article/pii/S2665963820300099.
* Wang and Isola [2022] Tongzhou Wang and Phillip Isola. Improved representation of asymmetrical distances with interval quasimetric embeddings. In _NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations_, 2022. URL https://openreview.net/forum?id=KRiST_rzKGl.
* Wang and Isola [2022] Tongzhou Wang and Phillip Isola. On the learning and learnability of quasimetrics. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=y0VvIg25yk.
* Wang et al. [2023] Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching reinforcement learning via quasimetric learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 36411-36430. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/wang23al.html.
* Yarats et al. [2021] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. 2021.
* Yarats et al. [2021] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=GY6-6sTvGaf.
* Yarats et al. [2021] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(12):10674-10681, May 2021. doi: 10.1609/aaai.v35i12.17276. URL https://ojs.aaai.org/index.php/AAAI/article/view/17276.

* Yu et al. [2019] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on Robot Learning (CoRL)_, 2019. URL https://arxiv.org/abs/1910.10897.
* Zang et al. [2022] Hongyu Zang, Xin Li, and Mingzhong Wang. Simsr: Simple distance-based state representations for deep reinforcement learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(8):8997-9005, Jun. 2022. doi: 10.1609/aaai.v36i8.20883. URL https://ojs.aaai.org/index.php/AAAI/article/view/20883.
* Zeng et al. [2023] Xianghua Zeng, Hao Peng, Angsheng Li, Chunyang Liu, Lifang He, and Philip S Yu. Hierarchical state abstraction based on structural information principles. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence_, pages 4549-4557, 2023.
* Zhang et al. [2021] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=-2FCwDRRREu.
* Zhong et al. [2023] Junmin Zhong, Ruofan Wu, and Jennie Si. A long n-step surrogate stage reward for deep reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 12733-12745. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/29ef811e72b2b97cf18dd5d866bf0f472-Paper-Conference.pdf.
* Lukaszyk [2004] Szymon Lukaszyk. A new concept of probability metric and its applications in approximation of scattered data sets. _Computational mechanics_, 33:299-304, 2004.

Appendix

### Broader Impact

This paper is a research on improving the representation of deep reinforcement learning. We think that there are no potential societal consequences that need to be highlighted.

### Additional Related Work

Recent studies have extensively explored representation learning in reinforcement learning (RL). Some previous works [18; 24; 43] employ autoencoders to encode image states into low-dimensional latent embeddings, resulting in improved visual perception and accelerated policy learning. Other approaches [42; 22; 23; 41; 35] utilize data augmentation techniques such as random crop or noise injection, combined with contrastive loss, to learn more generalizable state representations. Another set of approaches [25; 33; 15] focus on learning representation by predicting auxiliary tasks to extract more information from the environment. Successor representations [19; 30] tackles the state occupancy distribution to learn a more informative state representation, showing generalization capabilities between tasks.

In recent research, metric learning methods have been employed in RL to measure distances between state representations. Some approaches aim to approximate bisimulation metrics [47; 20] while others focus on learning sample-based distances [45; 7]. Bisimulation metrics have also been utilized in goal-based RL to enhance state representation [17]. A more recent work introduces quasimetrics learning as a novel RL objective for cost MDPs [40] but it is not for general MDPs.

Self-predictive representation (SPR) [32] learn representations regarding a future state but it focuses on predicting future states, enforcing the representation of predicted future states to be close to the true future states. SPR's representation focuses on dynamics learning, without directly considering the reward or value. In contrast, SCR focuses on predicting a metric/distance between current state and future state and the metrics are related to rewards or values regarding policy learning.

Successor Representations (SRs) [1; 30; 3] are a class of methods designed to learn representations that facilitate generalization. SRs achieve this by focusing on the occupancy of states, enabling generalization across various tasks and reward functions. In contrast, SCR measures the distance between states specifically to handle observable distractions.

### Proofs

#### a.3.1 Proof of Theorem 3.1

**Theorem 3**.: _Let \(\hat{d}:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}\) be a metric in the latent state representation space, \(d_{\phi}(\mathbf{x}_{i},\mathbf{y}_{i^{\prime}}):=\hat{d}(\phi(\mathbf{x}_{i} ),\phi(\mathbf{y}_{i^{\prime}}))\) be a metric in the state domain. The metric update operator \(\mathcal{F}\) is defined as,_

\[\mathcal{F}d_{\phi}(\mathbf{x}_{i},\mathbf{y}_{i^{\prime}})=|r_{\mathbf{x}_{i }}-r_{\mathbf{y}_{i^{\prime}}}|+\gamma\mathbb{E}_{\begin{subarray}{c}\phi( \mathbf{x}_{i+1})\sim\hat{P}(\cdot|\phi(\mathbf{x}_{i}),a_{\mathbf{x}_{i}}) \\ \phi(\mathbf{y}_{i^{\prime}+1})\sim\hat{P}(\cdot|\phi(\mathbf{y}_{i^{\prime}}),a _{\mathbf{y}_{i^{\prime}}})\end{subarray}}\ \hat{d}(\phi(\mathbf{x}_{i+1}),\phi(\mathbf{y}_{i^{\prime}+1})),\] (11)

_where \(\hat{\mathbb{M}}\) is the space of \(d\), with \(a_{\mathbf{x}_{i}}\) and \(a_{\mathbf{y}_{i^{\prime}}}\), being the actions at states \(\mathbf{x}_{i}\) and \(\mathbf{y}_{i^{\prime}}\), respectively, and \(\hat{P}\) is the learned latent dynamics model. \(\mathcal{F}\) has a fixed point \(d_{\phi}^{\pi}\)._

Proof.: We follow the proof techniques from [7] and [45]. By substituting \(\hat{d}(\phi(\mathbf{x}_{i+1}),\phi(\mathbf{y}_{i^{\prime}+1}))\) with \(d_{\phi}(\mathbf{x}_{i+1},\mathbf{y}_{i^{\prime}+1})\), we have

\[\mathcal{F}d_{\phi}(\mathbf{x}_{i},\mathbf{y}_{i^{\prime}})=|r_{\mathbf{x}_{i }}-r_{\mathbf{y}_{i^{\prime}}}|+\gamma\mathbb{E}_{\begin{subarray}{c}\phi( \mathbf{x}_{i+1})\sim\hat{P}(\cdot|\phi(\mathbf{x}_{i}),a_{\mathbf{x}_{i}}) \\ \phi(\mathbf{y}_{i^{\prime}+1})\sim\hat{P}(\cdot|\phi(\mathbf{y}_{i^{\prime}}),a _{\mathbf{y}_{i^{\prime}}})\end{subarray}}\ d_{\phi}(\mathbf{x}_{i+1}, \mathbf{y}_{i^{\prime}+1}).\] (12)

The operator \(\mathcal{F}d_{\phi}\) is a contraction mapping with respect to the \(L_{\infty}\) norm because,

\[|\mathcal{F}d_{\phi}(\mathbf{x},\mathbf{y})-\mathcal{F}d_{\phi^{\prime}}( \mathbf{x},\mathbf{y})|= \begin{vmatrix}\mathbb{E}_{\begin{subarray}{c}\phi(\mathbf{x}_{i+1}) \sim\hat{P}(\cdot|\phi(\mathbf{x}_{i}),a_{\mathbf{x}_{i}})\\ \phi(\mathbf{y}_{i^{\prime}+1})\sim\hat{P}(\cdot|\phi(\mathbf{y}_{i^{\prime}}), a_{\mathbf{y}_{i^{\prime}}})\end{subarray}}\ (d_{\phi}-d_{\phi^{\prime}})(\mathbf{x}_{i+1},\mathbf{y}_{i^{\prime}+1})\\ \leq \gamma\|(d_{\phi}-d_{\phi^{\prime}})(\mathbf{x}_{i+1},\mathbf{y}_{i^{\prime} +1})\|_{\infty}.\]

By Banach's fixed point theorem, operator \(\mathcal{F}\) has a fixed point \(d_{\phi}^{\pi}\).

#### a.3.2 Proof of Theorem 3.4

To prove the distance \(\hat{d}\) is a diffuse metric, we need to prove that \(\hat{d}\) satisfies all of three axioms in the Definition 3.2 (definition of diffuse metric).

**Lemma A.1**.: \(\|\mathbf{a}\|_{2}^{2}+\|\mathbf{b}\|_{2}^{2}-\mathbf{a}^{\top}\mathbf{b}\geq 0\) _for any \(\mathbf{a},\mathbf{b}\in\mathbb{R}\)._

Proof.: Because \(\mathbf{a}^{\top}\mathbf{b}\leq\|\mathbf{a}\|\|\mathbf{b}\|\), we have,

\[\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\mathbf{a}^{\top}\mathbf{b}\] \[\geq \|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\|\mathbf{a}\|\|\mathbf{b}\|\] \[\geq \|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-2\|\mathbf{a}\|\|\mathbf{b}\|\] (14) \[= (\|\mathbf{a}\|-\|\mathbf{b}\|)^{2}\] \[\geq 0.\]

This lemma indicates that the term under the square root of \(\hat{d}\) is always non-negative. \(\hat{d}\) is able to measure any two vectors \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{n}\).

**Lemma A.2** (Non-negative).: \(\hat{d}(\mathbf{a},\mathbf{b})\geq 0\) _for any \(\mathbf{a},\mathbf{b}\in\mathbb{R}\)._

Proof.: By definition, the square root is non-negative. 

**Lemma A.3** (Symmetric).: \(\hat{d}(\mathbf{a},\mathbf{b})=\hat{d}(\mathbf{b},\mathbf{a})\)

Proof.: \(\hat{d}(\mathbf{a},\mathbf{b})=\sqrt{\|\mathbf{a}\|_{2}^{2}+\|\mathbf{b}\|_{2 }^{2}-\mathbf{a}^{\top}\mathbf{b}}=\sqrt{\|\mathbf{b}\|_{2}^{2}+\|\mathbf{a} \|_{2}^{2}-\mathbf{b}^{\top}\mathbf{a}}=\hat{d}(\mathbf{b},\mathbf{a})\) 

**Lemma A.4** (Triangle inequality).: \(\hat{d}(\mathbf{a},\mathbf{b})+\hat{d}(\mathbf{b},\mathbf{c})\geq\hat{d}( \mathbf{a},\mathbf{c})\)_, for any \(\mathbf{a},\mathbf{b},\mathbf{c}\in\mathbb{R}\)._

Proof.: To prove this lemma, it is equivalent to prove the following inequality by definition of \(\hat{d}\),

\[\sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\mathbf{a}^{\top}\mathbf{b}}+ \sqrt{\|\mathbf{b}\|^{2}+\|\mathbf{c}\|^{2}-\mathbf{b}^{\top}\mathbf{c}}\geq \sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{c}\|^{2}-\mathbf{a}^{\top}\mathbf{c}}.\] (15)

Because \(-\|\mathbf{x}\|\|\mathbf{y}\|\leq\mathbf{x}^{\top}\mathbf{y}\leq\|\mathbf{x} \|\|\mathbf{y}\|,\forall\mathbf{x},\mathbf{y}\), we have

\[\sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\mathbf{a}^{\top} \mathbf{b}}+\sqrt{\|\mathbf{b}\|^{2}+\|\mathbf{c}\|^{2}-\mathbf{b}^{\top} \mathbf{c}}\] (16) \[\geq \sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\|\mathbf{a}\|\| \mathbf{b}\|}+\sqrt{\|\mathbf{b}\|^{2}+\|\mathbf{c}\|^{2}-\|\mathbf{b}\|\| \mathbf{c}\|},\]

and

\[\sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{c}\|^{2}+\|\mathbf{a}\|\|\mathbf{c}\|} \geq\sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{c}\|^{2}-\mathbf{a}^{\top}\mathbf{c}}.\] (17)

If

\[\sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\|\mathbf{a}\|\|\mathbf{b}\|}+ \sqrt{\|\mathbf{b}\|^{2}+\|\mathbf{c}\|^{2}-\|\mathbf{b}\|\|\mathbf{c}\|}\geq \sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{c}\|^{2}+\|\mathbf{a}\|\|\mathbf{c}\|}\] (18)

is true, then inequality (15) is true and Lemma A.4 is proven. To prove inequality (18), we can take squares on both sides without sign changing because both sides are non-negative. Then we have,

\[\left(\sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\|\mathbf{a}\|\|\mathbf{b}\| }+\sqrt{\|\mathbf{b}\|^{2}+\|\mathbf{c}\|^{2}-\|\mathbf{b}\|\|\mathbf{c}\|} \right)^{2}\geq\|\mathbf{a}\|^{2}+\|\mathbf{c}\|^{2}+\|\mathbf{a}\|\|\mathbf{c}\|.\] (19)

To prove inequality (18), it is equivalent to prove inequality (19). Expand and simplify inequality (19), we have

\[2\sqrt{\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}-\|\mathbf{a}\|\|\|\mathbf{b}\|} \sqrt{\|\mathbf{b}\|^{2}+\|\mathbf{c}\|^{2}-\|\mathbf{b}\|\|\mathbf{c}\|}\geq-2 \|\mathbf{b}\|^{2}+\|\mathbf{a}\|\|\mathbf{b}\|+\|\mathbf{b}\|\|\mathbf{c}\|+ \|\mathbf{a}\|\|\mathbf{c}\|.\] (20)

The left-hand side of inequality (20) is non-negative.

1) if right hand side \(-2\|\mathbf{b}\|^{2}+\|\mathbf{a}\|\|\mathbf{b}\|+\|\mathbf{b}\|\|\mathbf{c}\| +\|\mathbf{a}\|\|\mathbf{c}\|<0\), then inequality (20) is proven and backtrace to Lemma A.4 is proven.

[MISSING_PAGE_FAIL:17]

### Implementation Details

#### a.5.1 Implementation of \(\hat{m}\)

We adopt IQE [38] to implement \(\hat{m}\). Given two vectors \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{n}\), reshaping to \(\mathbb{R}^{k\times l}\) where \(k\times l=n\), IQE first computes the union of the interval for each component:

\[d_{i}(\mathbf{a},\mathbf{b})=|\bigcup_{j=1}^{l}[\mathbf{a}_{ij},\max(\mathbf{a }_{ij},\mathbf{b}_{ij})]|,\forall i=1,2,...,k,\] (27)

where \([\cdot,\cdot]\) is the interval on the real line. Then it computes the distance among all components \(d_{i}\) as

\[d_{IQE}(\mathbf{a},\mathbf{b})=\alpha\cdot\max(d_{1}(\mathbf{a},\mathbf{b}),.. d_{k}(\mathbf{a},\mathbf{b}))+(1-\alpha)\cdot\mathrm{mean}(d_{1}(\mathbf{a}, \mathbf{b}),..d_{k}(\mathbf{a},\mathbf{b})),\] (28)

where \(\alpha\in\mathbb{R}\) is an adaptive weight to balance the "max" and "mean" terms. In the scope of our method, we adopt \(d_{IQE}(\mathbf{a},\mathbf{b})\) to implement \(\hat{m}\).

#### a.5.2 Network Architecture

The encoder \(\phi\) takes an input of the states and consists of 4 convolutional layers followed by 1 fully-connected layer. The output dimension of \(\phi\) is 256. The encoder \(\psi\) takes an input of 512-dimensional vector (concatenated with \(\phi(\mathbf{x}_{i})\) and \(\phi(\mathbf{x}_{j})\)), feed it into two layer MLPs with 512 hidden units, and output a 256 dim embedding. Q network and policy network are 3-layer MLPs with 1024 hidden units.

#### a.5.3 Hyperparameters

\begin{table}
\begin{tabular}{l|l} \hline \hline Hyperparameters & Values \\ \hline Stack frames & 3 \\ Observation shape & \((3\times 3,84,84)\) \\ Action repeat & 2 for finger-spin, walker-walk \\  & 8 for cartpole-swing\_up, cartpole-swing\_up\_sparse \\  & 4 for others in DeepMind Control Suite \\  & 1 for Meta-World \\ Convolutional layers & 4 \\ Convolutional kernal size & \(3\times 3\) \\ Convolutional strides & \([2,1,1,1]\) \\ Convolutional channels & 32 \\ \(\phi\) dimension & 256 \\ \(\psi\) dimension & 256 \\ Learning rate & 1e-4 \\ Q function EMA \(\alpha_{Q}\) & 0.01 \\ Encoder \(\phi\) EMA \(\alpha_{\phi}\) & 0.05 \\ Initial steps & 1000 \\ Replay buffer size & 500K \\ Target update freq & 2 \\ Batch size & 128 \\ Discount factor \(\gamma\) & 0.95 for ball\_in\_cup-catch, Reacher-easy \\  & 0.99 for others \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameters

#### a.5.4 Algorithm

```
1defd(x,y):
2a=(x.pow(2.).sum(dim=-1,keepdim=True)+y.pow(2.).sum(dim=-1,keepdim=True))#x^2+y^2
3b=(x*y).sum(dim=-1,keepdim=True)#xy
4return(a-b).sqrt()#sqrt(x^2+y^2-xy)
5
6defencoder_loss(self,s_i,s_i_1,s_j,s_j_1,r_i,r_j,agg_rew_i_j):
7phi_i=self.encoder(s_i)
8phi_i_1=self.encoder(s_i_1)
9phi_j=self.encoder(s_j)
10phi_j_1=self.encoder(s_j_1)
11#concatenateiandjforcomputecloss(2)wherejisnotrequired
12phi_t=torch.cat([phi_i,phi_j])
13phi_t1=torch.cat([phi_i_1,phi_j_1])
14r_t=torch.cat([r_i,r_j])
15#computeloss(2)
16#permutebatchtocreatey
17perm=torch.randperm(phi_t.shape[0])
18d_phi_t=d(phi_t,phi_t[perm])
19d_phi_t1=d(phi_t1,phi_t1[perm])
20#loss(2)
21loss_phi=F.mse_loss(d_phi_t,r_t+self.discount*d_phi_t1.detach())
22
23#computecloss(5)
24psi=self.psi_m1p(phi_i,phi_j)
25psi_1=self.psi_m1p(phi_i_1,phi_j)
26#permutebatchtocreatey
27perm=torch.randperm(psi.shape[0])
28d_psi=d(psi,psi[perm])
29d_psi_1=d(psi_1,psi_1[perm])
30#loss(5)
31loss_psi=F.mse_loss(d_psi,r_i+self.discount*d_psi_1.detach())
32
33#computecup_bound=d(phi_i,phi_i[perm])+d(phi_j,phi_j[perm])+agg_rew_i_j[perm]up_bound=up_bound.detach()loss_up=(F.msg_loss(m.abs(),up_bound,reduction='none')*(m.abs()>up_bound).detach()).mean() returnloss_phi+loss_psi+loss_low+loss_up

Listing 1: Pseudo code of learning of representation

### Computational Resources

The experiment is done on servers with an 128-cores CPU, 512 GB RAM and NVIDIA A100 GPUs. Two instances are running in one GPU at the same time.

## Appendix B Experiments

### Additional Information for Distraction Settings in DeepMind Control Suite

For the distraction setting in Section 4.2, we utilized the Distracting Control Suite [34] with the setting "difficulty=easy". This involves mixing the background with videos from the DAVIS2017 [29] dataset. Specifically, the training environment samples videos from the DAVIS2017 train set, while the evaluation environment uses videos from the validation set. Each episode reset triggers the sampling of a new video. Additionally, it introduces variability in each episode by applying a uniformly sampled RGB color shift to the robot's body color and randomly selecting the camera pose. The specifics of the RGB color shift range and camera pose variations are in line with the Distracting Control Suite paper [34]. Different random seeds are used for the training and evaluating environments at the start of training to ensure diverse environments.

### Name of Tasks in DeepMind Control Suite

Due to limited space in main text, we use short name for tasks in DeepMind Control Suite. The full name of tasks are listed in below Table 5:

### Additional Results of Distracting Control Suite in Section 4.2

We extend the experiments in Section 4.2 with compared with RAP [8], showing results in Table 6. Figure 9 shows the training curves of SCR and baseline methods in distraction setting of DM_Control.

\begin{table}
\begin{tabular}{l|l} \hline Short Name & Full Name \\ \hline BiC-Catch & ball\_in\_cup-catch \\ C-SwingUp & cartpole-swing\_up \\ C-SwingUpSparse & cartpole-swing\_up\_sparse \\ Ch-Run & cheetah-run \\ F-Spin & finger-spin \\ H-Stand & hopper-stand \\ R-Easy & Reacher-easy \\ W-Walk & walker-walk \\ \hline \end{tabular}
\end{table}
Table 5: Full name of tasks in DeepMind Control Suite

### Additional Comparison Experiments with Data Augmentation Methods

Data augmentation is a category of methods efficient on reinforcement learning, toward sample efficiency and noise invariance. Additional baseline CBM [26] is bisimulation metric method combing with data augmentation method DrQ-v2, and it achieves significant performance in distracting DM_Control. To compare with CBM, we embed data augmentation method DrQ into our proposed method SCR. Table 7 indicates that the performance of SCR combined with DrQ surpass baseline CBM.

### Training Curves Meta-World

Figure 10 shows the training curves of SCR and baseline methods in Meta-World in Section 4.4.

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline  & Bic-Catch & C-SwingUp & C-SwingUpSparse & Ch-Run & F-Spin & H-Stand & R-Easy & W-Walk \\ \hline \hline SAC & 82.62:10.2 & 218.4\(\pm\)4.9 & 0.72\(\pm\)0.7 & 177.4\(\pm\)8.4 & 22.5\(\pm\)17.7 & 19.6\(\pm\)26.2 & 19.4\(\pm\)9.4\(\pm\)7.7 & 167.2\(\pm\)2.8 \\ DrQ & 124.0\(\pm\)99.6 & 230.0\(\pm\)36.3 & 11.2\(\pm\)6.4 & 100.3\(\pm\)84.9 & 579.8\(\pm\)22.8 & 16.8\(\pm\)11.8 & 70.5\(\pm\)40.1 & 33.6\(\pm\)6.3 \\ DRQ & 48.8\(\pm\)24.4 & 127.7\(\pm\)19.2 & 0.0\(\pm\)0.9 & 0.2\(\pm\)19.9 & 7.7\(\pm\)10.7 & 5.6\(\pm\)2.5 & 14.9\(\pm\)64.2 & 30.9\(\pm\)4.8 \\ MICo & 104.2\(\pm\)10.1 & 200.2\(\pm\)6.6 & 0.0\(\pm\)1.1 & 7.4\(\pm\)3.2 & 86.9\(\pm\)17.6 & 11.8\(\pm\)10.5 & 123.3\(\pm\)37.8 & 25.7\(\pm\)57.7 \\ SimSR & 106.4\(\pm\)13.5 & 148.4\(\pm\)17.3 & 0.0\(\pm\)0.0 & 28.2\(\pm\)23.8 & 0.4\(\pm\)0.1 & 6.6\(\pm\)1.2 & 78.4\(\pm\)17 & 28.6\(\pm\)3.1 \\ RAP & 82.6\(\pm\)15.93 & 451.7\(\pm\)74.1 & 0.1\(\pm\)0.7 & 243.2\(\pm\)26.4 & 577.7\(\pm\)140.4 & 6.1\(\pm\)11.7 & 87.8\(\pm\)86.7 & 183.7\(\pm\)108.7 \\ SCR & **221.3\(\pm\)**55.4 & **565.1\(\pm\)**59.9 & **185.7\(\pm\)**93 & **331.7\(\pm\)**1.4 & **738.3\(\pm\)**24.5 & **400.8\(\pm\)**19.2 & **666.1\(\pm\)**14.5 & **555.1\(\pm\)**31.2 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results (mean\(\pm\)std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose.

\begin{table}
\begin{tabular}{l|c c c c c c c c c} \hline  & Bic-Catch & C-SwingUp & C-SwingUpSparse & Ch-Run & F-Spin & H-Stand & R-Easy & W-Walk \\ \hline DrQ & 124.0\(\pm\)99.6 & 230.0\(\pm\)36.3 & 11.2\(\pm\)6.4 & 100.3\(\pm\)84.9 & 57.9\(\pm\)22.8 & 16.8\(\pm\)11.8 & 70.5\(\pm\)40.1 & 33.6\(\pm\)6.3 \\ CBM & 642.5\(\pm\)66.1 & 464.1\(\pm\)18.54 & 27.6\(\pm\)5.2 & 422.1\(\pm\)82.0 & **517.6\(\pm\)**21.7 & 12.1\(\pm\)92.4\(\pm\)6.3 & 319.5\(\pm\)27.46 & 68.8\(\pm\)65.6 \\ SCR & 221.3\(\pm\)55.4 & 565.1\(\pm\)59.9 & 185.7\(\pm\)93.0 & 331.7\(\pm\)1.4 & 738.3\(\pm\)24.5 & 400.8\(\pm\)19.2 & 666.1\(\pm\)14.5 & 555.1\(\pm\)31.2 \\ SCR w/ DrQ & **936.0\(\pm\)**97.0 & **783.9\(\pm\)**62.0 & **417.2\(\pm\)**89.5 & **480.2\(\pm\)**48.2 & **835.3\(\pm\)**37.3 & **706.4\(\pm\)**165.5 & **829.0\(\pm\)**32.3 & **835.6\(\pm\)**79.5 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results (mean\(\pm\)std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose.

Figure 9: Training curves of SCR and baseline methods in distraction setting of DM_Control. Curves are evaluation scores average on 10 runs and shadow shapes are std.

[MISSING_PAGE_EMPTY:22]

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have included the claims of contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss in the Conclusion Section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide complete proof in Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide implementation details in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: We provide pseudo codes in Appendix. We will release source code after published. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide details in Appendix. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the standard deviation of the experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This paper is a research on improving the representation of deep reinforcement learning. We think that there are no potential societal consequences that need to be highlighted. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited their original paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.