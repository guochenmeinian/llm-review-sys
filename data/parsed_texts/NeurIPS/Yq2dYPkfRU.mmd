# Stability and Sharper Risk Bounds with Convergence Rate \(O(1/n^{2})\)

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The sharpest known high probability excess risk bounds are up to \(O(1/n)\) for empirical risk minimization and projected gradient descent via algorithmic stability (Klochkov and Zhivotovskiy, 2021). In this paper, we show that high probability excess risk bounds of order up to \(O(1/n^{2})\) are possible. We discuss how high probability excess risk bounds reach \(O(1/n^{2})\) under strongly convexity, smoothness and Lipschitz continuity assumptions for empirical risk minimization, projected gradient descent and stochastic gradient descent. Besides, to the best of our knowledge, our high probability results on the generalization gap measured by gradients for nonconvex problems are also the sharpest.

## 1 Introduction

Algorithmic stability is a fundamental concept in learning theory (Bousquet and Elisseeff, 2002), which can be traced back to the foundational works of Vapnik and Chervonenkis (1974) and has a deep connection with learnability (Rakhlin et al., 2005; Shalev-Shwartz and Ben-David, 2014). It is not difficult for only providing in-expectation error bounds via stability arguments. However, high probability bounds are beneficial to understand the robustness of optimization algorithms (Bousquet et al., 2020; Klochkov and Zhivotovskiy, 2021) and are much more challenging (Feldman and Vondrak, 2019; Bousquet et al., 2020; Lv et al., 2021). In this paper, our goal is to improve the high probability risk bounds via algorithmic stability.

Let us start with some standard notations. We have a set of independent and identically distributed observations \(S=\{z_{1},\ldots,z_{n}\}\) sampled from a probability measure \(\rho\) defined on a sample space \(\mathcal{Z}:=\mathcal{X}\times\mathcal{Y}\). Based on the training set \(S\), our goal is to build a model \(h:\mathcal{X}\mapsto\mathcal{Y}\) for prediction, where the model is determined by parameter \(\mathbf{w}\) from parameter space \(\mathcal{W}\subset\mathbb{R}^{d}\). The performance of a model \(\mathbf{w}\) on an example \(z\) can be quantified by a loss function \(f(\mathbf{w};z)\), where \(f:\mathcal{W}\times\mathcal{Z}\mapsto\mathbb{R}_{+}\). Then the population risk and the empirical risk of \(\mathbf{w}\in\mathcal{W}\), respectively as

\[F(\mathbf{w}):=\mathbb{E}_{z}\left[f(\mathbf{w};z)\right],\quad F_{S}( \mathbf{w}):=\frac{1}{n}\sum_{i=1}^{n}f(\mathbf{w};z_{i}),\]

where \(\mathbb{E}_{z}\) denotes the expectation w.r.t. \(z\).

Let \(\mathbf{w}^{*}\in\arg\min_{\mathbf{w}\in\mathcal{W}}F(\mathbf{w})\) be the model with the minimal population risk in \(\mathcal{W}\) and \(\mathbf{w}^{*}(S)\in\arg\min_{\mathbf{w}\in\mathcal{W}}F_{S}(\mathbf{w})\) be the model with the minimal empirical risk w.r.t. dataset \(S\). Let \(A(S)\) be the output of a (possibly randomized) algorithm \(A\) on the dataset \(S\). Let \(\|\cdot\|_{2}\) denote the Euclidean norm and \(\nabla g(\mathbf{w})\) denote a subgradient of \(g\) at \(\mathbf{w}\).

Traditional generalization analysis aims to bound the generalization error \(F(A(S))-F_{S}(A(S))\) w.r.t the algorithm \(A\) and the dataset \(S\). Based on the technique developed by Feldman and Vondrak (2018,2019), Bousquet et al. (2020) provide the sharpest high probability bounds of \(O\left(L/\sqrt{n}\right)\), where the loss function \(f(\cdot,\cdot)\) is bounded by \(M\). No matter how stable the algorithm is, the high probability generalization bound will not be smaller than \(O(L/\sqrt{n})\). This is sampling error term scaling as \(O\left(1/\sqrt{n}\right)\) that controls the generalization error (Klochkov and Zhivotovskiy, 2021).

A frequently used alternative to generalization bounds, that can avoid the sampling error, are the excess risk bounds. The excess risk of algorithm \(A\) w.r.t. the dataset \(S\) is \(F(A(S))-F(\mathbf{w}^{*})\), which is more essential because it considers both generalization error and optimization error. Recently, Klochkov and Zhivotovskiy (2021) provided the best high probability excess risk bounds of order up to \(O(\log n/n)\) for empirical risk minimization (ERM) and projected gradient descent (PGD) algorithms via algorithmic stability.

On the other hand, Zhang et al. (2017), Li and Liu (2021), Xu and Zeevi (2024) derived high probability excess risk bounds with \(O\left(1/n^{2}\right)\) for ERM and stochastic gradient descent (SGD) via uniform convergence when the sample number satisfies \(n=\Omega(d)\), which implied that the rate \(O\left(1/n^{2}\right)\) is possible. However, the results obtained by the uniform convergence technique are related to the dimension \(d\), which is unacceptable in high-dimensional learning problems. Since stability analysis can yield dimension-free bounds, we naturally have the following question:

_Can algorithmic stability provide high probability excess risk bounds with the rate beyond \(O(1/n)\)?_

The main results of this paper answers this question positively. We provides the first high probability bounds that are dimension-free with the rate \(O(1/n^{2})\) for ERM, PGD and SGD. Our framework can also be used to solve other stable algorithms.

To this end, we develop the generalization gap measured by gradients. Our bounds under _nonconvex setting_ are tighter than existing works based on both algorithmic stability (Fan and Lei, 2024) and uniform convergence (Xu and Zeevi, 2024). This is why we can achieve dimension-free excess risk bounds of order \(O(1/n^{2})\). In fact, in nonconvex problems, optimization algorithms can only find a local minimizer and we can only obtain optimization error bounds for \(\|\nabla F_{S}(A(S))\|_{2}\)(Ghadimi and Lan, 2013). Therefore, it is important to study the generalization behavior of \(A(S)\) measured by gradients. Under Polyak-Lojasiewicz condition, we also obtain sharper results for both generalization bounds of gradients and excess risk bounds. Our route to excess risk bounds can also be applied to various stable algorithms and complex learning scenarios. In this paper, we take ERM, PGD, and SGD as examples to explore the stability of stochastic convex optimization algorithms with strongly convex losses. We provide tighter high probability dimension-free excess risk bounds of up to \(O(1/n^{2})\) comparing with existing works based on both algorithmic stability (Klochkov and Zhivotovskiy, 2021; Fan and Lei, 2024) and uniform convergence (Zhang et al., 2017; Li and Liu, 2021; Xu and Zeevi, 2024).

Besides, to obtain tighter bounds, we obtain a tighter \(p\)-moment bound for sums of vector-valued functions by introducing the optimal Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space in the proof, which has more potential applications in vector-valued functional data.

This paper is organized as follows. The related work is reviewed in Section 2. In Section 3, we present our main results for stability and generalization. We give applications to ERM, PGD and SGD in Section 4. The conclusion is given in Section 5. All the proofs and additional lemmata are deferred to the Appendix.

## 2 Related Work

**Algorithmic stability.** Algorithmic stability is a classical approach in generalization analysis, which can be traced back to the foundational works of (Vapnik and Chervonenkis, 1974). It gave the generalization bound by analyzing the sensitivity of a particular learning algorithm when changing one data point in the dataset. Modern framework of stability analysis was established by Bousquet and Elisseeff (2002), where they presented an important concept called uniform stability. Since then, a lot of works based on uniform stability have emerged. On one hand, generalization bounds with algorithmic stability have been significantly improved by Feldman and Vondrak (2018, 2019), Bousquet et al. (2020), Klochkov and Zhivotovskiy (2021). On the other hand, different algorithmic stability measures such as uniform argument stability (Liu et al., 2017; Bassily et al., 2020), on average stability (Shalev-Shwartz et al., 2010; Kuzborskij and Lampert, 2018), hypothesis stability [Bousquet and Elisseeff, 2002, Charles and Papailiopoulos, 2018], hypothesis set stability [Foster et al., 2019], pointwise uniform stability [Fan and Lei, 2024], PAC-Bayesian stability [Li et al., 2020], locally elastic stability [Deng et al., 2021], collective stability [London et al., 2016] and uniform stability in gradients [Lei, 2023, Fan and Lei, 2024] have been developed. Most of them provided the connection on stability and generalization in expectation. Bousquet and Elisseeff [2002], Elisseeff et al. [2005], Feldman and Vondrak [2018, 2019], Bousquet et al. [2020], Klochkov and Zhivotovskiy [2021], Fan and Lei [2024] considered high probability bounds. However, only Fan and Lei [2024] developed vector-valued bounds (eg: generalization bounds of gradients), which can be the order at most \(O\left(M/\sqrt{n}\right)\) and remain improvement.

**Uniform convergence.** Uniform convergence is another popular approach in statistical learning theory to study generalization bounds [Fisher, 1922, Vapnik, 1999, Van der Vaart, 2000]. The main idea is to bound the generalization gap by its supremum over the whole (or a subset) of the hypothesis space via some space complexity measures, such as VC dimension, covering number and Rademacher complexity. For finite-dimensional problem, Kleywegt et al. [2002] provided that the generalization error is \(O\left(\sqrt{d/n}\right)\) depended on the sample size \(n\) and the dimension of parameters \(d\) in high probability. In nonconvex settings, Mei et al. [2018] showed that the empirical of generalization error is \(O(\sqrt{d/n})\). Xu and Zeevi [2024] developed a novel "uniform localized convergence" framework using generic chaining for the minimization problem and provided the localized generalization bounds in gradients \(O\left(\max\left\{\|\mathbf{w}-\mathbf{w}^{*}\|_{2},\frac{1}{n}\right)\left( \sqrt{\frac{d}{n}}+\frac{d}{n}\right)\right)\), which is the optimal result when we only consider the order of \(n\). However, uniform convergence results are related to the dimension \(d\), which is unacceptable in high-dimensional learning problems.

## 3 Stability and Generalization

To derive sharper generalization bounds of gradients, we need to develop a novel concentration inequality which provide \(p\)-moment bound for sums of vector-valued functions. For a real-valued random variable \(Y\), the \(L_{p}\)-norm of \(Y\) is defined by \(\left\|Y\right\|_{p}:=\left(\mathbb{E}[\left|Y\right|^{p}]\right)^{\frac{1}{p}}\). Similarly, let \(\|\cdot\|\) denote the norm in a Hilbert space \(\mathcal{H}\). Then for a random variable \(X\) taking values in a Hilbert space, the \(L_{p}\)-norm of \(X\) is defined by \(\left\|\left\|\mathbf{X}\right\|\right\|_{p}:=\left(\mathbb{E}\left[\|\mathbf{ X}\|^{p}\right]\right)^{\frac{1}{p}}\).

### A Moment Bound for Sums of Vector-valued Functions

Here we present our sharper moment bound for sums of vector-valued functions of \(n\) independent variables.

**Theorem 1**.: _Let \(\mathbf{Z}=(Z_{1},\ldots,Z_{n})\) be a vector of independent random variables each taking values in \(\mathcal{Z}\), and let \(\mathbf{g}_{1},\ldots,\mathbf{g}_{n}\) be some functions: \(\mathbf{g}_{i}:\mathcal{Z}^{n}\mapsto\mathcal{H}\) such that the following holds for any \(i\in[n]\):_

* \(\|\mathbb{E}[\mathbf{g}_{i}(\mathbf{Z})|Z_{i}]\|\leq M\) _a.s.,_
* \(\mathbb{E}\left[\mathbf{g}_{i}(\mathbf{Z})|Z_{[n]\setminus\{i\}}\right]=0\) _a.s.,_
* \(\mathbf{g}_{i}\) _satisfies the bounded difference property with_ \(\beta\)_, namely, for any_ \(i=1,\ldots,n\)_, the following inequality holds_ \[\sup_{z_{1},\ldots,z_{n},z_{j}^{\prime}}\|\mathbf{g}_{i}(z_{1},\ldots,z_{j-1}, z_{j},z_{j+1},\ldots,z_{n})-\mathbf{g}_{i}(z_{1},\ldots,z_{j-1},z_{j}^{ \prime},z_{j+1},\ldots,z_{n})\|\leq\beta.\] (1)

_Then, for any \(p\geq 2\), we have_

\[\left\|\left\|\sum_{i=1}^{n}\mathbf{g}_{i}\right\|\right\|_{p}\leq 2(\sqrt{2p}+1 )\sqrt{n}M+4\times 2^{\frac{1}{2p}}\left(\sqrt{\frac{p}{e}}\right)(\sqrt{2p}+1)n \beta\left\lceil\log_{2}n\right\rceil.\]

**Remark 1**.: The proof is motivated by Bousquet et al. (2020). Under the same assumptions, Fan and Lei (2024) also established the following inequality1

Footnote 1: They assume \(n=2^{k},k\in\mathbb{N}\). Here we give the version of their result with general \(n\).

\[\left\|\left\|\sum_{i=1}^{n}\mathbf{g}_{i}\right\|\right\|_{p}\leq 2(\sqrt{2}+1) \sqrt{np}M+4(\sqrt{2}+1)np\beta\left\lceil\log_{2}n\right\rceil.\] (2)

It is easy to verify that our result is tighter than result provided by Fan and Lei (2024) for both the first and second term. Comparing Theorem 1 with (2), the larger \(p\) is, the tighter our result is relative to (2). In the worst case, when \(p=2\), the constant of our first term is \(0.879\) times tighter than (2), and the constant of our second term is \(0.634\) times tighter than (2). This is because we derive the optimal Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space in the proof.

The improvement of this concentration inequality is meaningful. On one hand, we derive the optimal Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space. On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures \(M=0\) at the same time. Under this condition, we can eliminate the first term. When we use Theorem 1 instead of (2) in the whole proofs, at least \(0.634\) times tighter bound can be obtained strictly.

### Sharper Generalization Bounds in Gradients

Let \(S=\{z_{1},\ldots,z_{n}\}\) be a set of independent random variables each taking values in \(\mathcal{Z}\) and \(S^{\prime}=\{z^{\prime}_{1},\ldots,z^{\prime}_{n}\}\) be its independent copy. For any \(i\in[n]\), define \(S^{(i)}=\{z_{i},\ldots,z_{i-1},z^{\prime}_{i},z_{i+1},\ldots,z_{n}\}\) be a dataset replacing the \(i\)-th sample in \(S\) with another i.i.d. sample \(z^{\prime}_{i}\). We introduce some basic definitions here and we want to emphasize that our main Theorem 2 and Theorem 3 do not need smoothness assumption and PL condition.

**Definition 1**.: _Let \(g:\mathcal{W}\mapsto\mathbb{R}\). Let \(\gamma,\mu<0\)._

* _We say_ \(g\) _is_ \(\gamma\)_-smooth if_ \[\|\nabla g(\mathbf{w})-\nabla g(\mathbf{w}^{\prime})\|_{2}\leq\gamma\| \mathbf{w}-\mathbf{w}^{\prime}\|_{2},\quad\forall\mathbf{w},\mathbf{w}^{ \prime}\in\mathcal{W}.\]
* _Let_ \(g*=\min_{\mathbf{w}\in\mathcal{W}}g(\mathbf{w})\)_. We say_ \(g\) _satisfies the Polyak-Lojasiewicz (PL) condition with parameter_ \(\mu>0\) _on_ \(\mathcal{W}\) _if_ \[g(\mathbf{w})-g*\leq\frac{1}{2\mu}\|\nabla g(\mathbf{w})\|_{2}^{2},\quad \forall\mathbf{w}\in\mathcal{W}.\]

Then we define uniform stability in gradients.

**Definition 2** (Uniform Stability in Gradients).: _Let \(A\) be a randomized algorithm. We say \(A\) is \(\beta\)-uniformly-stable in gradients if for all neighboring datasets \(S,S^{(i)}\), we have_

\[\sup_{z}\left\|\nabla f(A(S);z)-\nabla f(A(S^{(i)});z)\right\|_{2}\leq\beta.\] (3)

**Remark 2**.: Gradient-based stability was firstly introduced by Lei (2023); Fan and Lei (2024) to describe the generalization performance for nonconvex problems. In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values. Instead, the convergence of \(\|\nabla F_{S}(A(S))\|_{2}\) was often studied in the optimization community (Ghadimi and Lan, 2013). Since the population risk of gradients \(\|\nabla F(A(S))\|_{2}\) can be decomposed as the convergence of \(\|\nabla F_{S}(A(S))\|_{2}\) and the generalization gap \(\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\), the generalization analysis of \(\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\) is important, which can be achieved by uniform stability in gradients.

**Theorem 2** (Generalization via Stability in Gradients).: _Assume for any \(S\) and any \(z\), \(\|\nabla f(A(S);z)\|_{2}\leq M\). If \(A\) is \(\beta\)-uniformly-stable in gradients, then for any \(\delta\in(0,1)\), thefollowing inequality holds with probability at least \(1-\delta\)_

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq 2\beta+\frac{4M\left(1+e\sqrt{2\log\left(e/\delta\right)}\right)}{ \sqrt{n}}+8\times 2^{\frac{1}{4}}(\sqrt{2}+1)\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log\left(e/\delta\right).\]

**Remark 3**.: Theorem 2 is a direct application via Theorem 1 where we denote \(\mathbf{g}_{i}(S)=\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[\nabla f (A(S^{(i)}),Z)\right]-\nabla f(A(S^{(i)}),z_{i})\right]\) and find that \(\mathbf{g}_{i}(S)\) satisfies all the assumptions in Theorem 1. As a comparison, Fan and Lei (2024) also developed high probability bounds under same assumptions, but our bounds are sharper since our moment inequality for sums of vector-valued functions are tighter as we have discussed in Remark 1. Next, we derive sharper generalization bound of gradients under same assumptions.

**Theorem 3** (Sharper Generalization via Stability in Gradients).: _Assume for any \(S\) and any \(z\), \(\|\nabla f(A(S);z)\|_{2}\leq M\). If \(A\) is \(\beta\)-uniformly-stable in gradients, then for any \(\delta\in(0,1)\), the following inequality holds with probability at least \(1-\delta\)_

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq \sqrt{\frac{4\mathbb{E}_{Z}\left[\|\nabla f(A(S);Z)\|_{2}^{2} \right]\log\frac{6}{\delta}}{n}}+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n \beta^{2}\log(3/\delta)\right)\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{ \delta}}{n}\] \[\quad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\sqrt{\log 3e/\delta}.\]

**Remark 4**.: Note that the factor in Theorem 2 before \(1/\sqrt{n}\) is \(O\left(M\sqrt{\log\left(e/\delta\right)}\right)\), which depends on the bound of \(\|\nabla f(\cdot,\cdot)\|_{2}\). However, the factor in Theorem 3 before \(1/\sqrt{n}\) is \(O\left(\sqrt{\mathbb{E}_{Z}\left[\|\nabla f(A(S);Z)\|_{2}^{2}\right]\log 1/ \delta}+\beta\log(1/\delta)\right)\), not involving the possibly large term \(M\). As is known, optimization algorithms often provide parameters approaching the optimal solution, which make the term \(\mathbb{E}_{Z}[\|\nabla f(A(S);Z)\|_{2}^{2}]\) much more smaller than \(M\). We will give further reasonable results under more assumptions such as smoothness in Lemma 1 and Lemma 2.

On the other hand, best high probability bounds based on uniform convergence (Xu and Zeevi, 2024) is

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] (4) \[\lesssim \sqrt{\frac{\mathbb{E}_{Z}\left[\nabla\|f(\mathbf{w}^{*};Z)\|_{2 }^{2}\right]\log(1/\delta)}{n}}+\frac{\log(1/\delta)}{n}+\max\left\{\| \mathbf{w}-\mathbf{w}^{*}\|_{2},\frac{1}{n}\right\}\left(\sqrt{\frac{d}{n}}+ \frac{d}{n}\right),\]

which is the optimal result when we only consider the order of \(n\). However, uniform convergence results are related to the dimension \(d\), which is unacceptable in high-dimensional learning problems.

Note that (4) requires an additional smoothness-type assumption. As a comparison, when \(f\) is \(\gamma\)-smoothness, our result in Theorem 3 can be easily derived as

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\lesssim \beta\log n\log(1/\delta)+\frac{\log(1/\delta)}{n}+\sqrt{\frac{ \mathbb{E}_{Z}\left[\nabla\|f(\mathbf{w}^{*};Z)\|_{2}^{2}\right]\log(1/ \delta)}{n}}+\|A(S)-\mathbf{w}^{*}\|\sqrt{\frac{\log(1/\delta)}{n}}.\]

This result implies that when the uniformly stable in gradients parameter \(\beta\) is smaller than \(1/\sqrt{n}\), our bound is tighter than (4) and is dimension independent. Note that Theorem 3 holds in nonconvex problems, to the best of our knowledge, this is the sharpest upper bound in both uniform convergence and algorithmic stability analysis.

Here we give the proof sketch of Theorem 3, which is motivated by the analysis in Klochkov and Zhivotovskiy (2021). The key idea is to build vector functions \(\mathbf{q}_{i}(S)=\mathbf{h}_{i}(S)-\mathbb{E}_{S\{z_{i}\}}[\mathbf{h}_{i}(S)]\) where we define \(\mathbf{h}_{i}(S)=\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[\nabla f (A(S^{(i)}),Z)\right]-\nabla f(A(S^{(i)}),z_{i})\right]\). These functions satisfy all the assumptions in Theorem 1 and ensure the factor \(M\) in Theorem 1 to \(0\). Then the term \(O(1/\sqrt{n})\) can be eliminated.

**Lemma 1**.: _Let assumptions in Theorem 3 hold. Suppose the function \(f\) is \(\gamma\)-smooth and the population risk \(F\) satisfies the PL condition with parameter \(\mu\). Then for any \(\delta\in(0,1)\), when \(n\geq\frac{16\gamma^{2}\log\frac{6}{\delta}}{\mu^{2}}\), with probability at least \(1-\delta\), we have_

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq \|\nabla F_{S}(A(S))\|_{2}+4\sqrt{\frac{\mathbb{E}_{2}\left[\| \nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}\right]\log\frac{6}{\delta}}{n}}+2\sqrt{ \frac{\left(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3/\delta)\right)\log\frac{6 }{\delta}}{n}}\] \[\quad+\frac{2M\log\frac{6}{\delta}}{n}+32\times 2^{\frac{3}{4}} \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta\right)+64 \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta}.\]

**Remark 5**.: The following inequality can be easily derived using triangle inequality and Cauchy-Bunyakovsky-Schwarz inequality:

\[F(A(S))-F(\mathbf{w}^{*})\lesssim\|\nabla F_{S}(A(S))\|_{2}+\frac{F(\mathbf{w }^{*})\log(1/\delta)}{n}+\frac{\log^{2}(1/\delta)}{n^{2}}+\beta^{2}\log^{2}n \log^{2}(1/\delta).\] (5)

Above inequality implies that excess risk can be bound by the optimization gradient error \(\|\nabla F_{S}(A(S))\|_{2}\) and uniform stability in gradients \(\beta\). Note that the assumption \(F(\mathbf{w}^{*})=O(1/n)\) is common and can be found in Srebro et al. (2010); Lei and Ying (2020); Liu et al. (2018); Zhang et al. (2017); Zhang and Zhou (2019). This is natural since \(F(\mathbf{w}^{*})\) is the minimal population risk. On the other hand, we can derive that under \(\mu\)-strongly convex and \(\gamma\)-smooth assumptions for the objective function \(f\), uniform stability in gradients can be bounded of order \(O(1/n)\) for ERM and PGD. Thus high probability excess risk can be bounded of order up to \(O\left(1/n^{2}\right)\) under these common assumptions via algorithmic stability. Comparing with current best related work (Klochkov and Zhivotovskiy, 2021), they are insensitive to the stability parameter being smaller than \(O(1/n)\) and their best rates can only up to \(O(1/n)\). Although we involve extra smoothness and PL condition assumptions, these assumptions are also common in optimization community and our work can fully utilize these assumptions.

Besides, we discuss uniform stability in gradients for common algorithms such as ERM, PGD, and SGD in Section 4. Our results can be easily extended to other stable algorithms. Due to smoothness's property to link the uniform stability in gradients with uniform argument stability, many works (Bassily et al., 2020; Feldman and Vondrak, 2019; Hardt et al., 2016) exploring uniform argument stability can also use our framework.

Finally, the population risk of gradients \(\|\nabla F(A(S))\|_{2}\) can be gracefully bounded by the empirical risk of gradients \(\|\nabla F_{S}(A(S))\|_{2}\) under strong growth condition (SGC), that connects the rates at which the stochastic gradients shrink relative to the full gradient Vaswani et al. (2019).

**Definition 3** (Strong Growth Condition).: _We say SGC holds if_

\[\mathbb{E}_{Z}\left[\|\nabla f(\mathbf{w};Z)\|_{2}^{2}\right]\leq\rho\|\nabla F (\mathbf{w})\|_{2}^{2}.\]

**Remark 6**.: There has been some related work that takes SGC into assumption Solodov (1998); Vaswani et al. (2019); Lei (2023). Vaswani et al. (2019) has proved that the squared-hinge loss with linearly separable data and finite support features satisfies the SGC. Note that we only suppose this condition holds in Lemma 2.

**Lemma 2** (SGC case).: _Let assumptions in Theorem 3 hold and suppose SGC holds. Then for any \(\delta>0\), with probability at least \(1-\delta\), we have_

\[\|\nabla F(A(S))\|\lesssim(1+\eta)\|\nabla F_{S}(A(S))\|+\frac{1+\eta}{\eta} \left(\frac{M}{n}\log\frac{6}{\delta}+\beta\log n\log\frac{1}{\delta}\right).\]

**Remark 7**.: Lemma 2 build a connection between the population gradient error and the empirical gradient error under Lipschitz, nonconvex, nonsmooth and SGC case and elucidate that the population gradient error can be bounded of up to \(O(1/n)\) under nonconvex problems.

## 4 Application

In this section, we analysis stochastic convex optimization with strongly convex losses. The most common setting is where at each round, the learner gets information on \(f\) through a stochastic gradient oracle (Rakhlin et al., 2012). To derive uniform stability in gradients for algorithms, we firstly introduce the strongly convex assumption.

**Definition 4**.: _We say \(g\) is \(\mu\)-strongly convex if_

\[g(\mathbf{w})\geq g(\mathbf{w}^{\prime})+\langle\mathbf{w}-\mathbf{w}^{\prime}, \nabla g(\mathbf{w}^{\prime})\rangle+\frac{\mu}{2}\|\mathbf{w}-\mathbf{w}^{ \prime}\|_{2}^{2},\quad\forall\mathbf{w},\mathbf{w}^{\prime}\in\mathcal{W}.\]

### Empirical Risk Minimizer

Empirical risk minimizer is one of the classical approaches for solving stochastic optimization (also referred to as sample average approximation (SAA)) in machine learning community. The following lemma shows the uniform stability in gradient for ERM under \(\mu\)-strongly convexity and \(\gamma\)-smoothness assumptions.

**Lemma 3** (Stability of ERM).: _Suppose the objective function \(f\) is \(\mu\)-strongly-convex and \(\gamma\)-smooth. For any \(\mathbf{w}\in\mathcal{W}\) and any \(z\), suppose that \(\|\nabla f(\mathbf{w};z)\leq M\|\). Let \(\hat{\mathbf{w}}^{*}(S^{(i)})\) be the ERM of \(F_{S^{(i)}}(\mathbf{w})\) that denotes the empirical risk on the samples \(S^{(i)}=\{z_{1},...,z_{i}^{\prime},...,z_{n}\}\) and \(\hat{\mathbf{w}}^{*}(S)\) be the ERM of \(F_{S}(\mathbf{w})\) on the samples \(S=\{z_{1},...,z_{i},...,z_{n}\}\). For any \(S^{(i)}\) and \(S\), there holds the following uniform stability bound of ERM:_

\[\forall z\in\mathcal{Z},\quad\left\|\nabla f(\hat{\mathbf{w}}^{*}(S^{(i)});z )-\nabla f(\hat{\mathbf{w}}^{*}(S);z)\right\|_{2}\leq\frac{4M\gamma}{n\mu}.\]

Then, we present the application of our main sharper Theorem 3. In the strongly convex and smooth case, we provide a up to \(O\left(1/n^{2}\right)\) high probability excess risk guarantee valid for any algorithms depending on the optimal population error \(F(\mathbf{w}^{*})\).

**Theorem 4**.: _Let assumptions in Theorem 3 and Lemma 3 hold. Suppose the function \(f\) is nonnegative. Then for any \(\delta\in(0,1)\), when \(n\geq\frac{16\gamma^{2}\log\frac{\delta}{2}}{\mu^{2}}\), with probability at least \(1-\delta\), we have_

\[F(\hat{\mathbf{w}})-F(\mathbf{w}^{*})\lesssim\frac{F(\mathbf{w}^{*})\log\left( 1/\delta\right)}{n}+\frac{\log^{2}n\log^{2}(1/\delta)}{n^{2}}.\]

_Furthermore, assume \(F(\mathbf{w}^{*})=O(\frac{1}{n})\), we have_

\[F(\hat{\mathbf{w}})-F(\mathbf{w}^{*})\lesssim\frac{\log^{2}n\log^{2}(1/\delta )}{n^{2}}.\]

**Remark 8**.: Theorem 4 shows that when the objective function \(f\) is \(\mu\)-strongly convex, \(\gamma\)-smooth and nonnegative, high probability risk bounds can even up to \(O\left(1/n^{2}\right)\) for ERM. The most related work to ours is Zhang et al. (2017). They also obtain the \(O\left(1/n^{2}\right)\)-type bounds for ERM by uniform convergence of gradients approach. However, they need the sample number \(n=\Omega(\gamma d/\mu)\), which is related to the dimension \(d\). Our risk bounds are dimension independent and only require the sample number \(n=\Omega(\gamma^{2}/\mu^{2})\). Comparing with Klochkov and Zhivotovskiy (2021), we add two assumptions, smoothness and \(F(\mathbf{w}^{*})=O(1/n)\), but our bounds also tighter, from \(O(1/n)\) to \(O\left(1/n^{2}\right)\).

### Projected Gradient Descent

Note that when the objective function \(f\) is strongly convex and smooth, the optimization error can be ignored. However, the generalization analysis framework proposed by Klochkov and Zhivotovskiy (2021) does not use smoothness assumption, which only derive high probability excess risk bound of order \(O(1/n)\) after \(T=O(\log n)\) steps under strongly convex and smooth assumptions. In this subsection, we provide sharper risk bound under the same iteration steps, which is because our generalization analysis also fully utilized the smooth assumptions. Here we give the definition of PGD.

**Definition 5** (Projected Gradient Descent).: _Let \(\mathbf{w}_{1}=o\in\mathbb{R}^{d}\) be an initial point and \(\{\eta_{t}\}_{t}\) be a sequence of positive step sizes. PGD updates parameters by_

\[\mathbf{w}_{t+1}=\Pi_{\mathcal{W}}\left(\mathbf{w}_{t}-\eta_{t}\nabla F_{S} \left(\mathbf{w}_{t}\right)\right),\]

_where \(\nabla F_{S}(\mathbf{w}_{t})\) denotes a subgradient of \(F\) w.r.t. \(\mathbf{w}_{t}\) and \(\Pi_{\mathcal{W}}\) is the projection operator onto \(\mathcal{W}\)._

**Lemma 4** (Stability of Gradient Descent).: _Suppose the objective function \(f\) is \(\mu\)-strongly-convex and \(\gamma\)-smooth. For any \(\mathbf{w}\in\mathcal{W}\) and any \(z\), suppose that \(\|\nabla f(\mathbf{w};z)\|_{2}\leq M\). Let \(\mathbf{w}_{t}^{\ast}\) be the output of \(F_{S^{(i)}}(\mathbf{w})\) on \(t\)-th iteration on the samples \(S^{(i)}=\{z_{1},...,z_{i}^{\prime},...,z_{n}\}\) in running PGD, and \(\mathbf{w}_{t}\) be the output of \(F_{S}(\mathbf{w})\) on \(t\)-th iteration on the samples \(S=\{z_{1},...,z_{i},...,z_{n}\}\) in running PGD. Let the constant step size \(\eta_{t}=1/\gamma\). For any \(S^{(i)}\) and \(S\), there holds the following uniform stability bound of PGD:_

\[\forall z\in\mathcal{Z},\quad\left\|\nabla f(\hat{\mathbf{w}}^{\ast}(S^{(i)}) ;z)-\nabla f(\hat{\mathbf{w}}^{\ast}(S);z)\right\|_{2}\leq\frac{4M\gamma}{n\mu}.\]

**Remark 9**.: The derivations of Feldman and Vondrak (2019) in Section 4.1.2 (See also Hardt et al. (2016) in Section 3.4) imply that if the objective function \(f\) is \(\gamma\)-smooth in addition to \(\mu\)-strongly convexity and \(M\)-Lipschitz property, then PGD with the constant step size \(\eta=1/\gamma\) is \(\left(\frac{2M}{n\mu}\right)\)-uniformly argument stable for any number of steps, which means that PGD is \(\left(\frac{2M\gamma}{n\mu}\right)\)-uniformly-stable in gradients regardless of iteration steps.

**Theorem 5**.: _Let assumptions in Theorem 3 and Lemma 3 hold. Suppose the function \(f\) is nonnegative. Let \(\{\mathbf{w}_{t}\}_{t}\) be the sequence produced by PGD with \(\eta_{t}=1/\gamma\). Then for any \(\delta\in(0,1)\), when \(n\geq\frac{16\gamma^{2}\log\frac{\delta}{2}}{\mu^{2}}\), with probability at least \(1-\delta\), we have_

\[F(\mathbf{w})-F(\mathbf{w}^{\ast})\lesssim\left(1-\frac{\mu}{\gamma}\right)^{ 2T}+\frac{F(\mathbf{w}^{\ast})\log\left(1/\delta\right)}{n}+\frac{\log^{2}n \log^{2}(1/\delta)}{n^{2}}.\]

_Furthermore, assume \(F(\mathbf{w}^{\ast})=O(\frac{1}{n})\) and let \(T\asymp\log n\), we have_

\[F(\hat{\mathbf{w}})-F(\mathbf{w}^{\ast})\lesssim\frac{\log^{2}n\log^{2}(1/ \delta)}{n^{2}}.\]

**Remark 10**.: Theorem 5 shows that under the same assumptions as Klochkov and Zhivotovskiy (2021), our bound is \(O\left(\frac{F(\mathbf{w}^{\ast})\log(1/\delta)}{n}+\frac{\log^{2}n\log^{2}(1/ \delta)}{n^{2}}\right)\). Comparing with their bound \(O\left(\frac{\log n\log(1/\delta)}{n}\right)\), we are sharper because \(F(\mathbf{w}^{\ast})\) is the minimal population risk, which is a common assumption towards sharper risk bounds Srebro et al. (2010); Lei and Ying (2020); Liu et al. (2018); Zhang et al. (2017); Zhang and Zhou (2019).

### Stochastic Gradient Descent

Stochastic gradient descent optimization algorithm has been widely used in machine learning due to its simplicity in implementation, low memory requirement and low computational complexity per iteration, as well as good practical behavior. Here we give the definition of standard SGD.

**Definition 6** (Stochastic Gradient Descent).: _Let \(\mathbf{w}_{1}=o\in\mathbb{R}^{d}\) be an initial point and \(\{\eta_{t}\}_{t}\) be a sequence of positive step sizes. SGD updates parameters by_

\[\mathbf{w}_{t+1}=\Pi_{\mathcal{W}}\left(\mathbf{w}_{t}-\eta_{t}\nabla f\left( \mathbf{w}_{t};z_{i_{t}}\right)\right),\]

_where \(\nabla f(\mathbf{w}_{t};z_{i_{t}})\) denotes a subgradient of \(f\) w.r.t. \(\mathbf{w}_{t}\) and \(i_{t}\) is independently drawn from the uniform distribution over \([n]:=\{1,2,\ldots,n\}\)._

**Lemma 5** (Stability of SGD).: _Suppose the objective function \(f\) is \(\mu\)-strongly-convex and \(\gamma\)-smooth. For any \(\mathbf{w}\in\mathcal{W}\) and any \(z\), suppose that \(\|\nabla f(\mathbf{w};z)\|_{2}\leq M\). Let \(\mathbf{w}_{t}^{i}\) be the output of \(F_{S^{(i)}}(\mathbf{w})\) on \(t\)-th iteration on the samples \(S^{(i)}=\{z_{1},...,z_{i}^{\prime},...,z_{n}\}\) in running PGD and and \(\mathbf{w}_{t}\) be the output of \(F_{S}(\mathbf{w})\) on \(t\)-th iteration on the samples \(S=\{z_{1},...,z_{i},...,z_{n}\}\) in running SGD. For any \(S^{(i)}\) and \(S\), there holds the following uniform stability bound of SGD:_

\[\left\|\nabla f(\mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z)\right\|_{2} \leq 2\gamma\sqrt{\frac{2\epsilon_{opt}(\mathbf{w}_{t})}{\mu}}+\frac{4M\gamma}{n \mu},\quad\forall z\in\mathcal{Z},\]

_where \(\epsilon_{opt}(\mathbf{w}_{t})=F_{S}(\mathbf{w}_{t})-F_{S}(\hat{\mathbf{w}}^{ \ast}(S))\) and \(\hat{\mathbf{w}}^{\ast}(S)\) is the ERM of \(F_{S}(\mathbf{w})\)._

Next, we introduce a necessary assumption in stochastic optimization theory.

**Assumption 1**.: _Assume the existence of \(\sigma>0\) satisfying_

\[\mathbb{E}_{i_{t}}[\|\nabla f(\mathbf{w}_{t};z_{i_{t}})-\nabla F_{S}(\mathbf{w}_{ t})\|_{2}^{2}]\leq\sigma^{2},\quad\forall t\in\mathbb{N},\] (6)

_where \(\mathbb{E}_{i_{t}}\) denotes the expectation w.r.t. \(i_{t}\)._

**Remark 11**.: Assumption 1 is a standard assumption from the stochastic optimization theory (Nemirovski et al., 2009; Ghadimi and Lan, 2013; Ghadimi et al., 2016; Kuzborskij and Lampert, 2018; Zhou et al., 2018; Bottou et al., 2018; Lei and Tang, 2021), which essentially bounds the variance of the stochastic gradients for dataset \(S\).

**Theorem 6**.: _Let assumptions in Theorem 3 and Lemma 5 hold. Suppose Assumption 1 holds and the function \(f\) is nonnegative. Let \(\{\mathbf{w}_{t}\}_{t}\) be the sequence produced by SGD with \(\eta_{t}=\eta_{1}t^{-\theta},\theta\in(0,1)\) and \(\eta_{1}<\frac{1}{2\gamma}\). Then for any \(\delta\in(0,1)\), when \(n\geq\frac{16\gamma^{2}\log\frac{\theta}{2}}{\mu^{2}}\), with probability at least \(1-\delta\), we have_

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F(\mathbf{w}_{t})\|_{2}^{2}\] \[=\begin{cases}O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{T^{- \theta}}\right)+O\left(\frac{\log^{2}n\log^{2}(1/\delta)}{n^{2}}+\frac{F( \mathbf{w}^{*})\log^{2}(1/\delta)}{n}\right),&\text{if }\theta<1/2\\ O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{T^{-\frac{1}{2}}}\right)+O\left(\frac {\log^{2}n\log^{2}(1/\delta)}{n^{2}}+\frac{F(\mathbf{w}^{*})\log^{2}(1/\delta) }{n}\right),&\text{if }\theta=1/2\\ O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{T^{-\frac{1}{2}}}\right)+O\left( \frac{\log^{2}n\log^{2}(1/\delta)}{n^{2}}+\frac{F(\mathbf{w}^{*})\log^{2}(1/ \delta)}{n}\right),&\text{if }\theta>1/2.\end{cases}\]

**Remark 12**.: When \(\theta<1/2\), we take \(T\asymp n^{2/\theta}\). When \(\theta=1/2\), we take \(T\asymp n^{4}\) and when \(\theta>1/2\), we set \(T\asymp n^{2/(1-\theta)}\). Then according to Theorem 6, the population risk of gradient is bounded by \(O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{n^{2}}+\frac{F(\mathbf{w}^{*})\log^ {2}(1/\delta)}{n}\right)\). To the best of our knowledge, this is the first high probability population gradient bound \(\|\nabla F(\mathbf{w}_{t})\|_{2}\) for SGD via algorithmic stability.

**Theorem 7**.: _Let Assumptions in Theorem 3 and Lemma 5 hold. Suppose Assumption 1 holds and the function \(f\) is nonnegative. Let \(\{\mathbf{w}_{t}\}_{t}\) be the sequence produced by SGD with \(\eta_{t}=\frac{2}{\mu(t+t_{0})}\) such that \(t_{0}\geq\max\left\{\frac{4\gamma}{\mu},1\right\}\). Then for any \(\delta>0\), when \(n\geq\frac{16\gamma^{2}\log\frac{\theta}{2}}{\mu^{2}}\) and \(T\asymp n^{2}\), with probability at least \(1-\delta\), we have_

\[F(\mathbf{w}_{T+1})-F(\mathbf{w}^{*})=O\left(\frac{\log^{4}n\log^{5}(1/\delta )}{n^{2}}+\frac{F(\mathbf{w}^{*})\log(1/\delta))}{n}\right).\]

_Furthermore, assume \(F(\mathbf{w}^{*})=O(\frac{1}{n})\), we have_

\[F(\mathbf{w}_{T+1})-F(\mathbf{w}^{*})=O\left(\frac{\log^{4}n\log^{5}(1/\delta) }{n^{2}}\right).\]

**Remark 13**.: Theorem 7 implies that high probability risk bounds for SGD optimization algorithm can be up to \(O(1/n^{2})\) and the rate is dimension-free in high-dimensional learning problems. We compare Theorem 7 with most related work. For algorithmic stability, high probability risk bounds in Fan and Lei (2024) is up to \(O(1/n)\) when choosing optimal iterate number \(T\) for SGD optimization algorithm. To the best of knowledge, we are faster than all the existing bounds. The best high probability risk bounds of order \(O(1/n^{2})\) are given by Li and Liu (2021) via uniform convergence, which require the sample number \(n=\Omega(\gamma d/\mu)\) depending on dimension \(d\).

## 5 Conclusion

In this paper, we improve a \(p\)-moment concentration inequality for sums of vector-valued functions. By carefully constructing functions, we apply this moment concentration to derive sharper generalization bounds in gradients in nonconvex problems, which can further be used to obtain sharper high probability excess risk bounds for stable optimization algorithms. In application, we study three common algorithms: ERM, PGD, SGD. To the best of our knowledge, we provide the sharpest high probability dimension independent \(O(1/n^{2})\)-type for these algorithms.

## References

* [136] R. Bassily, V. Feldman, C. Guzman, and K. Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. In _Proceedings of the 34th International Conference on Neural Information Processing Systems (NeurIPS)_, volume 33, pages 4381-4391, 2020.
* [137] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. _SIAM review_, 60(2):223-311, 2018.
* [138] O. Bousquet and A. Elisseeff. Stability and generalization. _The Journal of Machine Learning Research_, 2:499-526, 2002.
* [139] O. Bousquet, Y. Klochkov, and N. Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In _Conference on Learning Theory_, pages 610-626. PMLR, 2020.
* [140] Z. Charles and D. Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In _International conference on machine learning_, pages 745-754. PMLR, 2018.
* [141] P. J. Davis. Gamma function and related functions. _Handbook of mathematical functions_, 256, 1972.
* [142] V. De la Pena and E. Gine. _Decoupling: from dependence to independence_. Springer Science & Business Media, 2012.
* [143] Z. Deng, H. He, and W. Su. Toward better generalization bounds with locally elastic stability. In _International Conference on Machine Learning_, pages 2590-2600. PMLR, 2021.
* [144] A. Elisseeff, T. Evgeniou, M. Pontil, and L. P. Kaelbing. Stability of randomized learning algorithms. _Journal of Machine Learning Research_, 6(1), 2005.
* [145] J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. _Applied and Computational Harmonic Analysis_, 70:101632, 2024.
* [146] V. Feldman and J. Vondrak. Generalization bounds for uniformly stable algorithms. _Advances in Neural Information Processing Systems_, 31, 2018.
* [147] V. Feldman and J. Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In _Conference on Learning Theory_, pages 1270-1279. PMLR, 2019.
* [148] R. A. Fisher. On the mathematical foundations of theoretical statistics. _Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character_, 222(594-604):309-368, 1922.
* [149] D. J. Foster, S. Greenberg, S. Kale, H. Luo, M. Mohri, and K. Sridharan. Hypothesis set stability and generalization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [150] S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM journal on optimization_, 23(4):2341-2368, 2013.
* [151] S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. _Mathematical Programming_, 155(1):267-305, 2016.
* [152] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pages 1225-1234. PMLR, 2016.
* [153] H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. In _ECML_, pages 795-811. Springer, 2016.
* [154] A. J. Kleywegt, A. Shapiro, and T. Homem-de Mello. The sample average approximation method for stochastic discrete optimization. _SIAM Journal on optimization_, 12(2):479-502, 2002.
* [155] Y. Klochkov and N. Zhivotovskiy. Stability and deviation optimal risk bounds with convergence rate \(o(1/n)\). _Advances in Neural Information Processing Systems_, 34:5065-5076, 2021.
* [156] I. Kuzborskij and C. Lampert. Data-dependent stability of stochastic gradient descent. In _Proceedings of the 35th International Conference on Machine Learning (ICML)_, pages 2815-2824. PMLR, 2018.
* [157] R. Latala and K. Oleszkiewicz. On the best constant in the khinchin-kahane inequality. _Studia Mathematica_, 109(1):101-104, 1994.
* [158] Y. Lei. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 191-227. PMLR, 2023.
* [159] Y. Lei and K. Tang. Learning rates for stochastic gradient descent with nonconvex objectives. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(12):4505-4511, 2021.
* [160] Y. Lei and Y. Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In _International Conference on Machine Learning_, pages 5809-5819. PMLR, 2020.
* [161] J. Li, X. Luo, and M. Qiao. On generalization error bounds of noisy gradient methods for non-convex learning. In _International Conference on Learning Representations_, 2020.
* [162] S. Li and Y. Liu. Improved learning rates for stochastic optimization: Two theoretical viewpoints. _arXiv preprint arXiv:2107.08686_, 2021.
* [163] M. Liu, X. Zhang, L. Zhang, R. Jin, and T. Yang. Fast rates of erm and stochastic approximation: Adaptive to error bound conditions. _Advances in Neural Information Processing Systems_, 31, 2018.

* [393] T. Liu, G. Lugosi, G. Neu, and D. Tao. Algorithmic stability and hypothesis complexity. In _International Conference on Machine Learning_, pages 2159-2167. PMLR, 2017.
* [395] B. London, B. Huang, and L. Getoor. Stability and generalization in structured prediction. _The Journal of Machine Learning Research_, 17(1):7808-7859, 2016.
* [396] X. Luo and D. Zhang. Khintchine inequality on normed spaces and the application to banach-mazur distance. _arXiv preprint arXiv:2005.03728_, 2020.
* [397] s. Lv, J. Wang, J. Liu, and Y. Liu. Improved learning rates of a functional lasso-type svm with sparse multi-kernel representation. _Advances in Neural Information Processing Systems_, 34:21467-21479, 2021.
* [401] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for nonconvex losses. _The Annals of Statistics_, 46(6A):2747-2774, 2018.
* [402] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
* [403] I. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. _The Annals of Probability_, pages 1679-1706, 1994.
* [404] A. Rakhlin, S. Mukherjee, and T. Poggio. Stability results in learning theory. _Analysis and Applications_, 3(04):397-417, 2005.
* [405] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, pages 1571-1578, 2012.
* [406] O. Rivasplata, E. Parrado-Hernandez, J. S. Shawe-Taylor, S. Sun, and C. Szepesvari. Pac-bayes bounds for stable algorithms with instance-dependent priors. _Advances in Neural Information Processing Systems_, 31, 2018.
* [41] S. Shalev-Shwartz and S. Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* [41] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform convergence. _The Journal of Machine Learning Research_, 11:2635-2670, 2010.
* [42] S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approximations. _Constructive approximation_, 26(2):153-172, 2007.
* [43] M. V. Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. _Computational Optimization and Applications_, 11:23-35, 1998.
* [44] N. Srebro, K. Sridharan, and A. Tewari. Optimistic rates for learning with a smooth loss. _arXiv preprint arXiv:1009.3896_, 2010.
* [45] A. W. Van der Vaart. _Asymptotic statistics_, volume 3. Cambridge university press, 2000.
* [46] V. Vapnik and A. Chervonenkis. _Theory of Pattern Recognition_. 1974.
* [47] V. N. Vapnik. An overview of statistical learning theory. _IEEE transactions on neural networks_, 10(5):988-999, 1999.
* [48] S. Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron. In _The 22nd international conference on artificial intelligence and statistics_, pages 1195-1204. PMLR, 2019.
* [49] R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [50] Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. _Mathematics of Operations Research_, 2024.
* [51] L. Zhang and Z.-H. Zhou. Stochastic approximation of smooth and strongly convex functions: Beyond the o(1/t) convergence rate. In _Conference on Learning Theory_, pages 3160-3179. PMLR, 2019.
* [52] L. Zhang, T. Yang, and R. Jin. Empirical risk minimization for stochastic convex optimization: O(1/n)-and o(1/n**)-type of risk bounds. In _Conference on Learning Theory_, pages 1954-1979. PMLR, 2017.
* [53] Y. Zhou, Y. Liang, and H. Zhang. Generalization error bounds with probabilistic guarantee for sgd in nonconvex optimization. _arXiv preprint arXiv:1802.06903_, 2018.

Additional definitions and lemmata

**Lemma 6** (Equivalence of tails and moments for random vectors [20]).: _Let \(X\) be a random variable with_

\[\left\|X\right\|_{p}\leq\sqrt{p}a+pb\]

_for some \(a,b\geq 0\) and for any \(p\geq 2\). Then for any \(\delta\in(0,1)\) we have, with probability at least \(1-\delta\),_

\[\left|X\right|\leq e\left(a\sqrt{\log\left(\frac{e}{\delta}\right)}+b\log\frac {e}{\delta}\right).\]

**Lemma 7** (Vector Bernstein's inequality [14, 20]).: _Let \(\{X_{i}\}_{i=1}^{n}\) be a sequence of i.i.d. random variables taking values in a real separable Hilbert space. Assume that \(\mathbb{E}[X_{i}]=\mu\), \(\mathbb{E}[\left\|X_{i}-\mu\right\|^{2}]=\sigma^{2}\), and \(\left\|X_{i}\right\|\leq M\), \(\forall 1\leq i\leq n\), then for all \(\delta\in(0,1)\), with probability at least \(1-\delta\) we have_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu\right\|\leq\sqrt{\frac{2\sigma^{2} \log(\frac{2}{\delta})}{n}}+\frac{M\log\frac{2}{\delta}}{n}.\]

**Definition 7** (Weakly self-Bounded Function).: _Assume that \(a,b>0\). A function \(f:\mathcal{Z}^{n}\mapsto[0,+\infty)\) is said to be \((a,b)\)-weakly self-bounded if there exist functions \(f_{i}:\mathcal{Z}^{n-1}\mapsto[0,+\infty)\) that satisfies for all \(Z^{n}\in\mathcal{Z}^{n}\),_

\[\sum_{i=1}^{n}(f_{i}(Z^{n})-f(Z^{n}))^{2}\leq af(Z^{n})+b.\]

**Lemma 8** ([11]).: _Suppose that \(z_{1},\ldots,z_{n}\) are independent random variables and the function \(f:\mathcal{Z}^{n}\mapsto[0,+\infty)\) is \((a,b)\)-weakly self-bounded and the corresponding function \(f_{i}\) satisfy \(f_{i}(Z^{n})\geq f(Z^{n})\) for \(\forall i\in[n]\) and any \(Z^{n}\in\mathcal{Z}^{n}\). Then, for any \(t>0\),_

\[Pr(\mathbb{E}f(z_{1},\ldots,z_{n})\geq f(z_{1},\ldots,z_{n})+t)\leq\exp\left( -\frac{t^{2}}{2a\mathbb{E}f(z_{1},\ldots,z_{n})+2b}\right).\]

**Definition 8**.: _A Rademacher random variable is a Bernoulli variable that takes values \(\pm 1\) with probability \(\frac{1}{2}\) each._

## Appendix B Proofs of Section 3.1

The proof of Theorem 1 is motivated by Bousquet et al. [2020], which need the Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space and the McDiarmid's inequality for vector-valued functions.

Firstly, we derive the optimal constants in the Marcinkiewicz-Zygmund's inequality for random variables taking values in a Hilbert space.

**Lemma 9** (Marcinkiewicz-Zygmund's Inequality for Random Variables Taking Values in a Hilbert Space).: _Let \(\mathbf{X}_{1},\ldots,\mathbf{X}_{n}\) be random variables taking values in a Hilbert space with \(\mathbb{E}[\mathbf{X}_{i}]=0\) for all \(i\in[n]\). Then for \(p\geq 2\) we have_

\[\left\|\left\|\sum_{i=1}^{n}\mathbf{X}_{i}\right\|\right\|_{p}\leq 2\cdot 2^{ \frac{1}{2p}}\sqrt{\frac{np}{e}}\left(\frac{1}{n}\sum_{i=1}^{n}\left\|\left\| \mathbf{X}_{i}\right\|\right\|_{p}^{p}\right)^{\frac{1}{p}}.\]

**Remark 14**.: Comparing with Marcinkiewicz-Zygmund's inequality given by Fan and Lei [2024], we provide best constants. Next, we give the proof of Lemma 9.

The Marcinkiewicz-Zygmund's inequality can be proved by using its connection to Khintchine-Kahane's inequality. Thus, we introduce the best constants in Khintchine-Kahane's inequality for random variables taking values from a Hilbert space here.

**Lemma 10** (Best constants in Khintchine-Kahane's inequality in Hilbert space (Latala and Oleszkiewicz, 1994; Luo and Zhang, 2020)).: _For all \(p\in[2,\infty)\) and for all choices of Hilbert space \(\mathcal{H}\), finite sets of vectors \(\mathbf{X}_{i},\ldots,\mathbf{X}_{n}\in\mathcal{X}\in\mathcal{H}\), and independent Rademacher variables \(r_{1},\ldots,r_{n}\),_

\[\left[\mathbb{E}\left\|\sum_{i=1}^{n}r_{i}\mathbf{X}_{i}\right\|^{p}\right]^{ \frac{1}{p}}\leq C_{p}\cdot\left[\sum_{i=1}^{n}\|\mathbf{X}_{i}\|^{2}\right]^ {\frac{1}{2}},\]

_where \(C_{p}=2^{\frac{1}{2}}\left\{\frac{\Gamma\left(\frac{p+1}{2}\right)}{\sqrt{\pi }}\right\}^{\frac{1}{p}}\)._

Proof of Lemma 9.: The symmetrization argument goes as follows: Let \((r_{1},\ldots,r_{n})\) be i.i.d. with \(\mathbb{P}(r_{i}=1)=\mathbb{P}(r_{i}=-1)=1/2\) and besides such that \(r_{1},\ldots,r_{n}\) and \((\mathbf{X}_{1},\ldots,\mathbf{X}_{n})\) are independent. Then by independence and symmetry, according to Lemma 1.2.6 of De la Pena and Gine (2012), conditioning on \((\mathbf{X}_{1},\ldots,\mathbf{X}_{n})\) yields

\[\mathbb{E}\left[\left\|\sum_{i=1}^{n}\mathbf{X}_{i}\right\|^{p} \right]=2^{p}\mathbb{E}\left[\left\|\sum_{i=1}^{n}r_{i}\mathbf{X}_{i}\right\| ^{p}\right]\leq 2^{p}\mathbb{E}\left[\mathbb{E}\left[\left\|\sum_{i=1}^{n}r_{i} \mathbf{X}_{i}\right\|^{p}\right]\left|\mathbf{X}_{1},\ldots,\mathbf{X}_{n} \right]\right].\] (7)

As for the conditional expectation in (7), notice that by independence

\[\mathbb{E}\left[\left\|\sum_{i=1}^{n}r_{i}\mathbf{X}_{i}\right\|^{p}\left| \mathbf{X}_{1}=\mathbf{x}_{1},\ldots,\mathbf{X}_{n}=\mathbf{x}_{n}\right]= \mathbb{E}\left[\left\|\sum_{i=1}^{n}r_{i}\mathbf{X}_{i}\right\|^{p}\right]\] (8)

According to Lemma 10, for \(v_{n}\)-almost every \(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\in\mathbb{R}^{n}\), where \(v_{n}:=\mathbb{P}\circ(\mathbf{X}_{1},\ldots,\mathbf{X}_{n})^{-1}\) denotes the distribution of \((\mathbf{X}_{1},\ldots,\mathbf{X}_{n})\), we have

\[\left[\mathbb{E}\left\|\sum_{i=1}^{n}r_{i}\mathbf{x}_{i}\right\|^{p}\right] \leq C\cdot\left[\sum_{i=1}^{n}\left\|\mathbf{x}_{i}\right\|^{2}\right]^{ \frac{p}{2}},\] (9)

where \(C=2^{\frac{p}{2}}\frac{\Gamma\left(\frac{p+1}{2}\right)}{\sqrt{\pi}}\) and \(C\) is optimal. This means that for any constant \(C^{\prime}\) such that

\[\left[\mathbb{E}\left\|\sum_{i=1}^{n}r_{i}\mathbf{x}_{i}\right\|^{p}\right] \leq C^{\prime}\cdot\left[\sum_{i=1}^{n}\left\|\mathbf{x}_{i}\right\|^{2} \right]^{\frac{p}{2}},\] (10)

for all \(n\in\mathbb{N}\) and for each collection of vectors \(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\), it follows that \(C^{\prime}\geq C\).

From (8) and (9), we can infer that

\[\mathbb{E}\left[\left\|\sum_{i=1}^{n}r_{i}\mathbf{X}_{i}\right\|^{p}\left| \mathbf{X}_{1}=\mathbf{x}_{1},\ldots,\mathbf{X}_{n}=\mathbf{x}_{n}\right]\leq C \cdot\left[\sum_{i=1}^{n}\left\|\mathbf{X}_{i}\right\|^{2}\right]^{\frac{p}{2}}.\]

Taking expectations in the above inequalities and (7) yield that

\[\mathbb{E}\left[\left\|\sum_{i=1}^{n}\mathbf{X}_{i}\right\|^{p}\right]\leq C \cdot\mathbb{E}\left[\sum_{i=1}^{n}\left\|\mathbf{X}_{i}\right\|^{2}\right]^{ \frac{p}{2}}.\] (11)

To see optimality let the above statement hold for some constants \(C^{\prime}\) in place of \(C\). Then if we choose \(\mathbf{X}_{i}:=\mathbf{x}_{i}r_{i},1\leq i\leq n\) with arbitrary reals vectors \(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\), it follows that

\[\mathbb{E}\left[\left\|\sum_{i=1}^{n}r_{i}\mathbf{x}_{i}\right\|^{p}\right] \leq C^{\prime}\cdot\mathbb{E}\left[\sum_{i=1}^{n}\left\|\mathbf{x}_{i}\right\| ^{2}\right]^{\frac{p}{2}},\]

whence we can conclude from (10) that \(C^{\prime}\geq C\). Thus we obtain that \(C^{\prime}=C\).

Notice that by Holder's inequality

\[\left[\sum_{i=1}^{n}\left\|\mathbf{X}_{i}\right\|^{2}\right]^{\frac{p}{2}}\leq n ^{p/2-1}\sum_{i=1}^{n}\|\mathbf{X}_{i}\|^{p}.\] (12)

Plugging (12) into (11), we have

\[\mathbb{E}\left[\left\|\sum_{i=1}^{n}\mathbf{X}_{i}\right\|^{p}\right]\leq C \cdot 2^{p}n^{p/2-1}\cdot\mathbb{E}\left[\sum_{i=1}^{n}\|\mathbf{X}_{i}\|^{p} \right],\]

where \(C=2^{\frac{p}{2}}\frac{\Gamma(\frac{p+1}{2})}{\sqrt{\pi}}\) is a constant.

Next, we use the following form of Stirling's formula for the Gamma-function, which follows from (6.1.5), (6.1.15) and (6.1.38) in Davis [1972] to bound the constant \(C\). For every \(x>0\), there exists a \(\mu(x)\in(0,1/(12x))\) such that

\[\Gamma(x)=\sqrt{2\pi}x^{x-1/2}e^{-x}e^{\mu(x)}.\]

Thus

\[C=2^{\frac{p}{2}}\frac{\Gamma\left(\frac{p+1}{2}\right)}{\sqrt{\pi}}=g(p) \sqrt{2}e^{-p/2}p^{p/2},\]

with \(g(p)=\left(1+\frac{1}{p}\right)^{p/2}e^{v(p)-1/2}\), where \(0<v(p)<1/(6(p+1))\). By Taylor's formula we have that

\[\log(1+x)=\sum_{m=1}^{\infty}\frac{1}{m}(-1)^{m-1}x^{m},\quad\forall x\in(-1, 1],\]

and that for every \(k\in\mathbb{N}_{0}\)

\[\sum_{m=1}^{2k}\frac{1}{m}(-1)^{m-1}x^{m}\leq\log(1+x)\leq\sum_{m=1}^{2k+1} \frac{1}{m}(-1)^{m-1}x^{m},\forall x\geq 0.\]

Therefor we obtain with \(k=1\) that

\[\log g(p)=\frac{p}{2}\log(1+\frac{1}{p})+v(p)-\frac{1}{2}\leq-\frac{1}{4p}+ \frac{1}{6p^{2}}+\frac{1}{6(p+1)}\leq-\frac{1}{18p},\]

where the last equality follows from elementary calculus. Similarly,

\[\log g(p)=\frac{p}{2}\log(1+\frac{1}{p})+v(p)-\frac{1}{2}\geq-\frac{1}{4p}+v( p)\geq-\frac{1}{4p},\]

Thus, we have

\[e^{-\frac{1}{4p}}\sqrt{2}e^{-p/2}p^{p/2}<C<e^{-\frac{1}{18p}}\sqrt{2}e^{-p/2}p ^{p/2},\]

which implies that \(C\) is strictly smaller than \(\sqrt{2}e^{-p/2}p^{p/2}\) for all \(p\geq 2\).

Since \(C=\frac{1}{g(p)}\sqrt{2}e^{-p/2}p^{p/2}\) and \(g(p)\geq e^{-\frac{1}{4p}}\), we can obtain that the relative error between \(C\) and \(\sqrt{2}e^{-p/2}p^{p/2}\) is equal to

\[\frac{1}{g(p)}-1\leq e^{-\frac{1}{4p}}-1\leq\frac{1}{4p}e^{\frac{1}{4p}}\]

using Mean Value Theorem. This implies that the corresponding relative errors between \(C\) and \(\sqrt{2}e^{-p/2}p^{p/2}\) converge to zero as \(p\) tends to infinity.

The proof is complete.

Then we introduce the McDiarmid's inequality for vector-valued functions. We firstly consider real-valued functions, which follows from the standard tail-bound of McDiarmid's inequality and Proposition 2.5.2 in Vershynin [2018].

**Lemma 11** (McDiarmid's Inequality for real-valued functions).: _Let \(Z_{i},\ldots,Z_{n}\) be independent random variables, and \(f:\mathcal{Z}^{n}\mapsto\mathbb{R}\) such that the following inequality holds for any \(z_{i},\ldots,z_{i-1},z_{i+1},\ldots,z_{n}\)_

\[\sup_{z_{i},z_{i}^{\prime}} |f(z_{1},\ldots,z_{i-1},z_{i},z_{i+1},\ldots,z_{n})-f(z_{1},\ldots,z_{i-1},z_{i}^{\prime},z_{i+1},\ldots,z_{n})|\leq\beta,\]

_Then for any \(p>1\) we have_

\[\|f(Z_{1},\ldots,Z_{n})-\mathbb{E}f(Z_{1},\ldots,Z_{n})\|_{p}\leq \sqrt{2pn}\beta.\]

To derive the McDiarmid's inequality for vector-valued functions, we need the expected distance between \(\mathbf{f}(Z_{1},\ldots,Z_{n})\) and its expectation.

**Lemma 12** ([Rivasplata et al., 2018]).: _Let \(Z_{i},\ldots,Z_{n}\) be independent random variables, and \(\mathbf{f}:\mathcal{Z}^{n}\mapsto\mathcal{H}\) is a function into a Hilbert space \(\mathcal{H}\) such that the following inequality holds for any \(z_{i},\ldots,z_{i-1},z_{i+1},\ldots,z_{n}\)_

\[\sup_{z_{i},z_{i}^{\prime}} \|\mathbf{f}(z_{1},\ldots,z_{i-1},z_{i},z_{i+1},\ldots,z_{n})- \mathbf{f}(z_{1},\ldots,z_{i-1},z_{i}^{\prime},z_{i+1},\ldots,z_{n})\|\leq\beta,\]

_Then we have_

\[\mathbb{E}\left[\|\mathbf{f}(Z_{1},\ldots,Z_{n})-\mathbb{E}\mathbf{f}(Z_{1}, \ldots,Z_{n})\|\right]\leq\sqrt{n}\beta.\]

Now, we can easily derive the \(p\)-norm McDiarmid's inequality for vector-valued functions which refines from Fan and Lei [2024] with better constants.

**Lemma 13** (McDiarmid's inequality for vector-valued functions).: _Let \(Z_{i},\ldots,Z_{n}\) be independent random variables, and \(\mathbf{f}:\mathcal{Z}^{n}\mapsto\mathcal{H}\) is a function into a Hilbert space \(\mathcal{H}\) such that the following inequality holds for any \(z_{i},\ldots,z_{i-1},z_{i+1},\ldots,z_{n}\)_

\[\sup_{z_{i},z_{i}^{\prime}} \|\mathbf{f}(z_{1},\ldots,z_{i-1},z_{i},z_{i+1},\ldots,z_{n})- \mathbf{f}(z_{1},\ldots,z_{i-1},z_{i}^{\prime},z_{i+1},\ldots,z_{n})\|\leq\beta,\] (13)

_Then for any \(p>1\) we have_

\[\|\|\mathbf{f}(Z_{1},\ldots,Z_{n})-\mathbb{E}\mathbf{f}(Z_{1},\ldots,Z_{n})\| \|_{p}\leq(\sqrt{2p}+1)\sqrt{n}\beta.\]

Proof of Lemma 13.: Define a real-valued function \(h:\mathcal{Z}^{n}\mapsto\mathbb{R}\) as

\[h(z_{1},\ldots,z_{n})=\|\mathbf{f}(z_{1},\ldots,z_{n})-\mathbb{E}[\mathbf{f}( Z_{1},\ldots,Z_{n})]\|.\]

We notice that this function satisfies the increment condition. For any \(i\) and \(z_{1},\ldots,z_{i-1},z_{i+1},\ldots,z_{n}\), we have

\[\sup_{z_{i},z_{i}^{\prime}} |h(z_{1},\ldots,z_{i-1},z_{i},z_{i+1},\ldots,z_{n})-h(z_{1}, \ldots,z_{i-1},z_{i}^{\prime},z_{i+1},\ldots,z_{n})|\] \[= \sup_{z_{i},z_{i}^{\prime}} \|\mathbf{f}(z_{1},\ldots,z_{n})-\mathbb{E}[\mathbf{f}(Z_{1}, \ldots,Z_{n})]\|-\|\mathbf{f}(z_{1},\ldots,z_{i-1},z_{i}^{\prime},z_{i+1}, \ldots,z_{n})-\mathbb{E}[\mathbf{f}(Z_{1},\ldots,Z_{n})]\|\] \[\leq \sup_{z_{1},z_{i}^{\prime}} \|\mathbf{f}(z_{1},\ldots,z_{n})-\mathbf{f}(z_{1},\ldots,z_{i-1}, z_{i}^{\prime},z_{i+1},\ldots,z_{n})\|\leq\beta.\]

Therefore, we can apply Lemma 11 to the real-valued function \(h\) and derive the following inequality

\[\|h(Z_{1},\ldots,Z_{n})-\mathbb{E}[h(Z_{1},\ldots,Z_{n})]\|_{p}\leq\sqrt{2pn}\beta.\]

According to Lemma 12, we know the following inequality \(\mathbb{E}[h(Z_{1},\ldots,Z_{n})]\leq\sqrt{n}\beta\). Combing the above two inequalities together and we can derive the following inequality

\[\|\|\mathbf{f}(Z_{1},\ldots,Z_{n})-\mathbb{E}\mathbf{f}(Z_{1}, \ldots,Z_{n})\|\|_{p}\] \[\leq \|h(Z_{1},\ldots,Z_{n})-\mathbb{E}[h(Z_{1},\ldots,Z_{n})]\|_{p}+ \|\mathbb{E}[h(Z_{1},\ldots,Z_{n})]\|_{p}\] \[\leq (\sqrt{2p}+1)\sqrt{n}\beta.\]

The proof is complete.

Proof of Theorem 1.: For \(\mathbf{g}(Z_{1},\ldots,Z_{n})\) and \(A\subset[n]\), we write \(\left|\left|\left\|\mathbf{g}\right|\right|\right|_{p}(Z_{A})=\left(\mathbb{E} \left[\left\|f\right\|^{p}Z_{A}\right]\right)^{\frac{1}{p}}\). Without loss of generality, we suppose that \(n=2^{k}\). Otherwise, we can add extra functions equal to zero, increasing the number of therms by at most two times.

Consider a sequence of partitions \(\mathcal{P}_{0},\ldots,\mathcal{P}_{k}\) with \(\mathcal{P}_{0}=\{\{i\}:i\in[n]\},\mathcal{P}_{k}\) with \(\mathcal{P}_{n}=\{[n]\}\), and to get \(\mathcal{P}_{l}\) from \(\mathcal{P}_{l+1}\) we split each subset in \(\mathcal{P}_{l+1}\) into two equal parts. We have

\[\mathcal{P}_{0}=\{\{1\},\ldots,\{2^{k}\}\},\quad\mathcal{P}_{1}=\{\{1,2\},\{3,4\},\ldots,\{2^{k}-1,2^{k}\}\},\quad\mathcal{P}_{k}=\{\{1,\ldots,2^{k}\}\}.\]

We have \(|\mathcal{P}_{l}|=2^{k-l}\) and \(|P|=2^{l}\) for each \(P\in\mathcal{P}_{l}\). For each \(i\in[n]\) and \(l=0,\ldots,k\), denote by \(P^{l}(i)\in\mathcal{P}_{l}\) the only set from \(\mathcal{P}_{l}\) that contains \(i\). In particular, \(P^{0}(i)=\{i\}\) and \(P^{K}(i)=[n]\).

For each \(i\in[n]\) and every \(l=0,\ldots,k\) consider the random variables

\[\mathbf{g}_{i}^{l}=\mathbf{g}_{i}^{l}(Z_{i},Z_{[n]\setminus P^{l}(i)})= \mathbb{E}[\mathbf{g}_{i}|Z_{i},Z_{[n]\setminus P^{l}(i)}],\]

i.e. conditioned on \(z_{i}\) and all the variables that are not in the same set as \(Z_{i}\) in the partition \(\mathcal{P}_{l}\). In particular, \(\mathbf{g}_{i}^{0}=\mathbf{g}_{i}\) and \(\mathbf{g}_{i}^{k}=\mathbb{E}[\mathbf{g}_{i}|Z_{i}]\). We can write a telescopic sum for each \(i\in[n]\),

\[\mathbf{g}_{i}-\mathbb{E}[\mathbf{g}_{i}|Z_{i}]=\sum_{l=1}^{k-1}\mathbf{g}_{i }^{l}-\mathbf{g}_{i}^{l+1}.\]

Then, by the triangle inequality

(14)

To bound the first term, since \(\left\|\mathbb{E}[\mathbf{g}_{i}|Z_{i}]\right\|\leq M\), we can check that the vector-valued function \(\mathbf{f}(Z_{1},\ldots,Z_{n})=\sum_{i=1}^{n}\mathbb{E}[\mathbf{g}_{i}|Z_{i}]\) satisfies (13) with \(\beta=2M\), and \(\mathbb{E}[\mathbb{E}[\mathbf{g}_{i}|Z_{i}]]=0\), applying Lemma 13 with \(\beta=2M\), we have

\[\left|\left|\left|\sum_{i=1}^{n}\mathbb{E}[\mathbf{g}_{i}|Z_{i}]\right|\right| \right|_{p}\leq 2(\sqrt{2p}+1)\sqrt{n}M.\] (15)

Then we start to bound the second term of the right hand side of (14). Observe that

\[\mathbf{g}_{i}^{l+1}(Z_{i},Z_{[n]\setminus P^{l+1}(i)})=\mathbb{E}\left[ \mathbf{g}_{i}^{l}(Z_{i},Z_{[n]\setminus P^{l}(i)})\big{|}Z_{i},Z_{[n]\setminus P ^{l+1}(i)}\right],\]

where the expectation is taken with respect to the variables \(Z_{j},j\in P^{l+1}(i)\backslash P^{l}(i)\). Changing any \(Z_{j}\) would change \(\mathbf{g}_{i}^{l}\) by \(\beta\). Therefore, we apply Lemma 13 with \(\mathbf{f}=\mathbf{g}_{i}^{l}\) where there are \(2^{l}\) random variables and obtain a uniform bound

\[\left|\left|\left|\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1}\right|\right|\right|_ {p}(Z_{i},Z_{[n]\setminus P^{l+1}(i)})\leq(\sqrt{2p}+1)\sqrt{2^{l}}\beta,\quad \forall p\geq 2,\]

Taking integration over \((Z_{i},Z_{[n]\setminus P^{l+1}(i)})\), we have \(\left|\left|\left|\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1}\right|\right|\right| _{p}\leq(\sqrt{2p}+1)\sqrt{2^{l}}\beta\) as well.

Next, we turn to the sum \(\sum_{i\in P^{l}}\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1}\) for any \(P^{l}\in\mathcal{P}_{l}\). Since \(\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1}\) for \(i\in P^{l}\) depends only on \(Z_{i},Z_{[n]\setminus P^{l}}\), the terms are independent and zero mean conditioned on \(Z_{[n]\setminus P^{l}}\). Applying Lemma 9, we have for any \(p\geq 2\),

\[\left|\left|\left|\sum_{i\in P^{l}}\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1} \right|\right|\right|_{p}^{p}(Z_{[n]\setminus P^{l}})\leq\left(2\cdot 2^{\frac{1}{2p}} \sqrt{\frac{2^{l}p}{e}}\right)^{p}\frac{1}{2^{l}}\sum_{i\in P^{l}}\left| \left|\left|\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1}\right|\right|\right|_{p}^{p }(Z_{[n]\setminus P^{l}}).\]

Integrating with respect to \((Z_{[n]\setminus P^{l}})\) and using \(\left|\left|\left|\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1}\right|\right| \right|_{p}\leq(\sqrt{2p}+1)\sqrt{2^{l}}\beta\), we have

\[\left|\left|\left|\sum_{i\in P^{l}}g_{i}^{l}-g_{i}^{l+1}\right| \right|\right|_{p}\leq \left(2\cdot 2^{\frac{1}{2p}}\sqrt{\frac{2^{l}p}{e}}\right)\frac{1}{2^{l} }\times 2^{l}(\sqrt{2p}+1)\sqrt{2^{l}}\beta\] \[= 2^{1+\frac{1}{2p}}\left(\sqrt{\frac{p}{e}}\right)(\sqrt{2p}+1)2^{l }\beta.\]Then using triangle inequality over all sets \(P^{l}\in\mathcal{P}_{l}\), we have

\[\left\|\left\|\sum_{i\in[n]}\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1} \right\|\right\|_{p}\leq \sum_{P^{l}\in\mathcal{P}_{l}}\left\|\left\|\sum_{i\in P^{l}} \mathbf{g}_{i}^{l}-\mathbf{g}_{i}^{l+1}\right\|\right\|_{p}\] \[\leq 2^{k-l}\times 2^{1+\frac{1}{2p}}\left(\sqrt{\frac{p}{e}}\right)( \sqrt{2p}+1)2^{l}\beta\] \[\leq 2^{1+\frac{1}{2p}}\left(\sqrt{\frac{p}{e}}\right)(\sqrt{2p}+1)2^ {k}\beta.\]

Recall that \(2^{k}\leq n\) due to the possible extension of the sample. Then we have

\[\sum_{i=0}^{k-1}\left\|\left\|\sum_{i=1}^{n}\mathbf{g}_{i}^{l}-\mathbf{g}_{i}^ {i+1}\right\|\right\|_{p}\leq 2^{2+\frac{1}{2p}}\left(\sqrt{\frac{p}{e}} \right)(\sqrt{2p}+1)n\beta\left\lceil\log_{2}n\right\rceil.\]

We can plug the above bound together with (15) into (14), to derive the following inequality

\[\left\|\left\|\sum_{i=1}^{n}\mathbf{g}_{i}\right\|\right\|_{p}\leq 2(\sqrt{2p}+1) \sqrt{n}M+2^{2+\frac{1}{2p}}\left(\sqrt{\frac{p}{e}}\right)(\sqrt{2p}+1)n\beta \left\lceil\log_{2}n\right\rceil.\]

The proof is completed.

## Appendix C Proofs of Section 3

Proof of Theorem 2.: Let \(S=\{z_{1},\ldots,z_{n}\}\) be a set of independent random variables each taking values in \(\mathcal{Z}\) and \(S^{\prime}=\{z_{1}^{\prime},\ldots,z_{n}^{\prime}\}\) be its independent copy. For any \(i\in[n]\), define \(S^{(i)}=\{z_{i},\ldots,z_{i-1},z_{i}^{\prime},z_{i+1},\ldots,z_{n}\}\) be a dataset replacing the \(i\)-th sample in \(S\) with another i.i.d. sample \(z_{i}^{\prime}\). Then we can firstly write the following decomposition

\[n\nabla F(A(S))-n\nabla F_{S}(A(S))\] \[= \sum_{i=1}^{n}\mathbb{E}_{Z}\left[\nabla f(A(S);Z)\right]- \mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i)}),Z)\right]\right]\] \[+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z} \left[\nabla f(A(S^{(i)}),Z)\right]-\nabla f(A(S^{(i)}),z_{i})\right]\] \[+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i )}),z_{i})\right]-\sum_{i=1}^{n}\nabla f(A(S),z_{i}).\]

We denote that \(\mathbf{g}_{i}(S)=\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[\nabla f (A(S^{(i)}),Z)\right]-\nabla f(A(S^{(i)}),z_{i})\right]\), thus we have

\[\left\|n\nabla F(A(S))-n\nabla F_{S}(A(S))\right\|_{2}\] (16) \[= \left\|\sum_{i=1}^{n}\mathbb{E}_{Z}\left[\nabla f(A(S);Z)\right]- \mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i)}),Z)\right]\right]\] \[+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z} \left[\nabla f(A(S^{(i)}),Z)\right]-\nabla f(A(S^{(i)}),z_{i})\right]\] \[+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i )}),z_{i})\right]-\sum_{i=1}^{n}\nabla f(A(S),z_{i})\right\|_{2}\] \[\leq 2n\beta+\left\|\sum_{i=1}^{n}\mathbf{g}_{i}(S)\right\|_{2},\]where the inequality holds from the definition of uniform stability in gradients.

According to our assumptions, we get \(\|\mathbf{g}_{i}(S)\|_{2}\leq 2M\) and

\[\mathbb{E}_{z_{i}}[\mathbf{g}_{i}(S)] =\mathbb{E}_{z_{i}}\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z} \left[\nabla f(A(S^{(i)});Z)\right]-\nabla f(A(S^{(i)});z_{i})\right]\] \[=\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[\nabla f(A(S ^{(i)});Z)\right]-\mathbb{E}_{z_{i}}\left[\nabla f(A(S^{(i)});z_{i})\right] \right]=0,\]

where this equality holds from the fact that \(z_{i}\) and \(Z\) follow from the same distribution. For any \(i\in[n]\), any \(j\neq i\) and any \(z_{j}^{\prime\prime}\), we have

\[\left\|\mathbf{g}_{i}(z_{1},\ldots,z_{j-1},z_{j},z_{j+1},\ldots,z_ {n})-\mathbf{g}_{i}(z_{1},\ldots,z_{j-1},z_{j}^{\prime\prime},z_{j+1},\ldots,z_ {n})\right\|_{2}\] \[\leq \left\|\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[ \nabla f(A(S^{(i)});Z)\right]-\nabla f(A(S^{(i)});z_{i})\right]-\mathbb{E}_{z_{ i}^{\prime}}\left[\mathbb{E}_{Z}\left[\nabla f(A(S^{(i)});Z)\right]-\nabla f(A(S^{(i)} );z_{i})\right]\right\|_{2}\] \[\leq \left\|\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[ \nabla f(A(S^{(i)});Z)-\nabla f(A(S^{(i)}_{j});Z)\right]\right]\right\|_{2}+ \left\|\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[\nabla f(A(S^{(i) });Z)\right]-\nabla f(A(S^{(i)}_{j});z_{i})\right]\right\|_{2}\] \[\leq 2\beta,\]

where \(S^{(i)}=\{z_{i},\ldots,z_{i-1},z_{i}^{\prime},z_{i+1},\ldots,z_{n}\}\). Thus, we have verified that three conditions in Theorem 1 are satisfied for \(\mathbf{g}_{i}(S)\). We have the following result for any \(p>2\)

\[\left\|\left\|\left\|\sum_{i=1}^{n}\mathbf{g}_{i}(S)\right\|\right\|\right\|_{p} \leq 4(\sqrt{2p}+1)\sqrt{n}M+8\times 2^{\frac{1}{4}}\left(\sqrt{\frac{p}{e}} \right)(\sqrt{2p}+1)n\beta\left\lceil\log_{2}n\right\rceil.\]

We can combine the above inequality and (16) to derive the following inequality

\[n\left\|\left\|\nabla F(A(S))-n\nabla F_{S}(A(S))\right\|\right\| _{p}\] \[\leq 2n\beta+4(\sqrt{2p}+1)\sqrt{n}M+8\times 2^{\frac{1}{4}}\left( \sqrt{\frac{p}{e}}\right)(\sqrt{2p}+1)n\beta\left\lceil\log_{2}n\right\rceil.\]

According to Lemma 6 for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have

\[n\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq 2n\beta+4\sqrt{n}M+8\times 2^{\frac{1}{4}}\sqrt{e}n\beta\left\lceil \log_{2}n\right\rceil\log(e/\delta)+(4e\sqrt{2n}M+8\times 2^{\frac{1}{4}} \sqrt{e}n\beta\left\lceil\log_{2}n\right\rceil)\sqrt{\log e/\delta}.\]

This implies that

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq 2\beta+\frac{4M\left(1+e\sqrt{2\log\left(e/\delta\right)}\right) }{\sqrt{n}}+8\times 2^{\frac{1}{4}}(\sqrt{2}+1)\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log(e/\delta).\]

The proof is completed. 

Proof of Theorem 3.: We can firstly write the following decomposition

\[n\nabla F(A(S))-n\nabla F_{S}(A(S))\] \[= \sum_{i=1}^{n}\mathbb{E}_{Z}\left[\nabla f(A(S);Z)\right]- \mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i)}),Z)\right]\right]\] \[+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z} \left[\nabla f(A(S^{(i)}),Z)\right]-\nabla f(A(S^{(i)}),z_{i})\right]\] \[+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i) }),z_{i})\right]-\sum_{i=1}^{n}\nabla f(A(S),z_{i}).\]We denote that \(\mathbf{h}_{i}(S)=\mathbb{E}_{z_{i}^{\prime}}\left[\mathbb{E}_{Z}\left[\nabla f(A(S^ {(i)}),Z)\right]-\nabla f(A(S^{(i)}),z_{i})\right]\), we have

\[n\nabla F(A(S))-n\nabla F_{S}(A(S))-\sum_{i=1}^{n}\mathbf{h}_{i}(S)\] \[= \sum_{i=1}^{n}\mathbb{E}_{Z}\left[\nabla f(A(S);Z)\right]- \mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i)}),Z)\right]\right]\] \[\qquad+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A( S^{(i)}),z_{i})\right]-\sum_{i=1}^{n}\nabla f(A(S),z_{i}),\]

which implies that

\[\left\|n\nabla F(A(S))-n\nabla F_{S}(A(S))-\sum_{i=1}^{n}\mathbf{ h}_{i}(S)\right\|_{2}\] (17) \[= \left\|\sum_{i=1}^{n}\mathbb{E}_{Z}\left[\nabla f(A(S);Z)\right] -\mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A(S^{(i)}),Z)\right]\right]\] \[\qquad+\sum_{i=1}^{n}\mathbb{E}_{z_{i}^{\prime}}\left[\nabla f(A( S^{(i)}),z_{i})\right]-\sum_{i=1}^{n}\nabla f(A(S),z_{i})\right\|_{2}\] \[\leq 2n\beta,\]

where the inequality holds from the definition of uniform stability in gradients.

Then, for any \(i=1,\ldots,n\), we define \(\mathbf{q}_{i}(S)=\mathbf{h}_{i}(S)-\mathbb{E}_{S\{z_{i}\}}[\mathbf{h}_{i}(S)]\). It is easy to verify that \(\mathbb{E}_{S\setminus\{z_{i}\}}[\mathbf{q}_{i}(S)]=\mathbf{0}\) and \(\mathbb{E}_{z_{i}}[\mathbf{h}_{i}(S)]=\mathbb{E}_{z_{i}}[\mathbf{q}_{i}(S)]- \mathbb{E}_{z_{i}}\mathbb{E}_{S\setminus\{z_{i}\}}[\mathbf{q}_{i}(S)]= \mathbf{0}-\mathbf{0}=\mathbf{0}\). Also, for any \(j\in[n]\) with \(j\neq i\) and \(z_{j}^{\prime\prime}\in\mathcal{Z}\), we have the following inequality

\[\|\mathbf{q}_{i}(S)-\mathbf{q}_{i}(z_{1},\ldots,z_{j-1},z_{j}^{ \prime\prime},z_{j+1},\ldots,z_{n})\|_{2}\] \[\leq \|\mathbf{h}_{i}(S)-\mathbf{h}_{i}(z_{1},\ldots,z_{j-1},z_{j}^{ \prime\prime},z_{j+1},\ldots,z_{n})\|_{2}\] \[+\|\mathbb{E}_{S\setminus\{z_{i}\}}[\mathbf{h}_{i}(S)]-\mathbb{E} _{S\setminus\{z_{i}\}}[\mathbf{h}_{i}(1,\ldots,z_{j-1},z_{j}^{\prime\prime},z_ {j+1},\ldots,z_{n})]\|_{2}.\]

For the first term \(\|\mathbf{h}_{i}(S)-\mathbf{h}_{i}(z_{1},\ldots,z_{j-1},z_{j}^{\prime\prime},z_ {j+1},\ldots,z_{n})\|_{2}\), it can be bounded by \(2\beta\) according to the definition of uniform stability. Similar result holds for the second term \(\|\mathbb{E}_{S\setminus\{z_{i}\}}[\mathbf{h}_{i}(S)]-\mathbb{E}_{S\setminus \{z_{i}\}}[\mathbf{h}_{i}(1,\ldots,z_{j-1},z_{j}^{\prime\prime},z_{j+1},\ldots,z_{n})]\|_{2}\) according to the uniform stability. By a combination of the above analysis, we get \(\|\mathbf{q}_{i}(S)-\mathbf{q}_{i}(1,\ldots,z_{j-1},z_{j}^{\prime\prime},z_ {j+1},\ldots,z_{n})\|_{2}\leq\|\mathbf{h}_{i}(S)-\mathbf{h}_{i}(1,\ldots,z_{j -1},z_{j}^{\prime\prime},z_{j+1},\ldots,z_{n})\|_{2}\leq 4\beta\).

Thus, we have verified that three conditions in Theorem 1 are satisfied for \(\mathbf{q}_{i}(S)\). We have the following result for any \(p\geq 2\)

\[\left|\left|\left|\sum_{i=1}^{n}\mathbf{q}_{i}(S)\right|\right|\right|_{p}\leq 2 ^{4+\frac{1}{4}}\left(\sqrt{\frac{p}{e}}\right)(\sqrt{2p}+1)n\beta\left\lceil \log_{2}n\right\rceil.\] (18)

Furthermore, we can derive that

\[n\nabla F(A(S))-n\nabla F_{S}(A(S))-\sum_{i=1}^{n}\mathbf{h}_{i} (S)+\sum_{i=1}^{n}\mathbf{q}_{i}(S)\] \[= n\nabla F(A(S))-n\nabla F_{S}(A(S))-\sum_{i=1}^{n}\mathbb{E}_{S \setminus\{z_{i}\}}[\mathbf{h}_{i}(S)]\] \[= n\nabla F(A(S))-n\nabla F_{S}(A(S))-n\mathbb{E}_{S^{\prime}}[ \nabla F(A(S^{\prime}))]+n\mathbb{E}_{S}[\nabla F(A(S))].\]Due to the i.i.d. property between \(S\) and \(S^{\prime}\), we know that \(\mathbb{E}_{S^{\prime}}[\nabla F(A(S^{\prime}))]=\mathbb{E}_{S}[\nabla F(A(S))]\). Thus, combined above equality, (17) and (18), we have

\[\left\|\left\|n\nabla F(A(S))-n\nabla F_{S}(A(S))-n\mathbb{E}_{S} [\nabla F(A(S))]+n\mathbb{E}_{S^{\prime}}[\nabla F_{S}(A(S^{\prime}))]\right\| \right\|_{p}\] \[\leq \left\|\left\|n\nabla F(A(S))-n\nabla F_{S}(A(S))-\sum_{i=1}^{n} \mathbf{h}_{i}(S)\right\|\right\|_{p}\] \[+ \left\|\left\|\sum_{i=1}^{n}\mathbf{h}_{i}(S)-n\mathbb{E}_{S}[ \nabla F(A(S))]+n\mathbb{E}_{S^{\prime}}F_{S}[A(S^{\prime})]\right\|\right\|_{p}\] \[= \left\|\left\|n\nabla F(A(S))-n\nabla F_{S}(A(S))-\sum_{i=1}^{n} \mathbf{h}_{i}(S)\right\|\right\|_{p}+\left\|\left\|\sum_{i=1}^{n}\mathbf{q}_ {i}(S)\right\|\right\|_{p}\] \[\leq 2n\beta+2^{4+\frac{1}{4}}\left(\sqrt{\frac{p}{e}}\right)(\sqrt {2p}+1)n\beta\left\lceil\log_{2}n\right\rceil\] \[\leq 16\times 2^{\frac{3}{4}}\left(\sqrt{\frac{1}{e}}\right)pn\beta \left\lceil\log_{2}n\right\rceil+32\left(\sqrt{\frac{1}{e}}\right)\sqrt{p}n \beta\left\lceil\log_{2}n\right\rceil.\]

According to Lemma 6 for any \(\delta\in(0,1)\), with probability at least \(1-\delta/3\), we have

\[\left\|\nabla F(A(S))-\nabla F_{S}(A(S))\right\|_{2}\] (19) \[\leq \|\mathbb{E}_{S^{\prime}}[\nabla F_{S}(A(S^{\prime}))]-\mathbb{E} _{S}[\nabla F(A(S))]\|_{2}\] \[\quad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\sqrt{\log 3e/\delta}.\]

Next, we need to bound the term \(\|\mathbb{E}_{S^{\prime}}[\nabla F_{S}(A(S^{\prime}))]-\mathbb{E}_{S}[\nabla F (A(S))]\|_{2}\). There holds that \(\|\mathbb{E}_{S}\mathbb{E}_{S^{\prime}}[\nabla F_{S}(A(S^{\prime}))]\|_{2}= \|\mathbb{E}_{S}[\nabla F(A(S))]\|_{2}\). Then, by the Bernstein inequality in Lemma 7, we obtain the following inequality with probability at least \(1-\delta/3\),

\[\left\|\mathbb{E}_{S^{\prime}}[\nabla F_{S}(A(S^{\prime}))]-\mathbb{E}_{S}[ \nabla F(A(S))]\right\|_{2}\leq\sqrt{\frac{2\mathbb{E}_{z_{i}}[\|\mathbb{E}_{S ^{\prime}}\nabla f(A(S^{\prime});z_{i})\|_{2}^{2}]\log\frac{6}{\delta}}{n}}+ \frac{M\log\frac{6}{\delta}}{n}.\] (20)

Then using Jensen's inequality, we have

\[\mathbb{E}_{z_{i}}[\|\mathbb{E}_{S^{\prime}}\nabla f(A(S^{\prime });z_{i})\|_{2}^{2}] \leq\mathbb{E}_{z}\mathbb{E}_{S^{\prime}}\|\nabla f(A(S^{\prime });z_{i})\|_{2}^{2}\] (21) \[=\mathbb{E}_{Z}\mathbb{E}_{S^{\prime}}\|\nabla f(A(S^{\prime});Z )\|_{2}^{2}\] \[=\mathbb{E}_{Z}\mathbb{E}_{S}\|\nabla f(A(S);Z)\|_{2}^{2}.\]

Combing (19), (20) with (21), we finally obtain that with probability at least \(1-2\delta/3\),

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] (22) \[\leq \sqrt{\frac{2\mathbb{E}_{Z}\mathbb{E}_{S}\|\nabla f(A(S);Z)\|_{2 }^{2}\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}}{n}\] \[\quad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\sqrt{\log 3e/\delta}.\]

Next, since \(S=\{z_{i},\ldots,z_{n}\}\), we define \(p=p(z_{1},\ldots,z_{n})=\mathbb{E}_{Z}[\|\nabla f(A(S);Z)\|_{2}^{2}]\) and \(p_{i}=p_{i}(z_{1},\ldots,z_{n})=\sup_{z_{i}\in\mathcal{Z}}p(z_{i},\ldots,z_{n})\). So there holds \(p_{i}\geq p\) for any \(i=1,\ldots,n\) and any \[\begin{split}&\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\\ \leq&\sqrt{\frac{2\left(2\mathbb{E}_{Z}[\|\nabla f(A(S);Z) \|_{2}^{2}\right)+\frac{1}{4}\beta^{2}+16n\beta^{2}\log(3/\delta)\right)\log \frac{6}{3}}{n}}+\frac{M\log\frac{6}{3}}{n}\\ &+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil \log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil \sqrt{\log 3e/\delta}.\end{split}\]

The proof is complete.

Proof of Remark 4.: According to the proof in Theorem 3, we have the following inequality that with probability at least \(1-\delta\)

\[\begin{split}&\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\\ \leq&\sqrt{\frac{4\mathbb{E}_{Z}[\|\nabla f(A(S);Z)\|_{ 2}^{2}]\log\frac{6}{\delta}}{n}}+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n \beta^{2}\log(3/\delta)\right)\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{ \delta}}{n}\\ &\quad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\sqrt{\log 3e/\delta}.\end{split}\] (26)

Since \(f(\mathbf{w})\) is \(\gamma\)-smooth, we have

\[\begin{split}&\mathbb{E}_{Z}[\|\nabla f(A(S);Z)\|_{2}^{2}]\\ \leq&\mathbb{E}_{Z}[\|\nabla f(A(S);Z)-\nabla f( \mathbf{w}^{*};Z)\|_{2}^{2}+\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\\ \leq&\gamma^{2}\|A(S)-\mathbf{w}^{*}\|_{2}^{2}+ \mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\end{split}\] (27)

Plugging (27) into (26), we have

\[\begin{split}&\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\\ \leq&\sqrt{\frac{4(\gamma^{2}\|A(S)-\mathbf{w}^{*}\|_ {2}^{2}+\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}])\log\frac{6}{ \delta}}{n}}+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3/\delta) \right)\log\frac{6}{\delta}}{n}}\\ &\quad+\frac{M\log\frac{6}{\delta}}{n}+16\times 2^{\frac{3}{4}} \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta\right)+32 \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta}\\ \leq& 2\gamma\|A(S)-\mathbf{w}^{*}\|_{2}\sqrt{\frac{ \log\frac{6}{\delta}}{n}}+\sqrt{\frac{4\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{ *};Z)\|_{2}^{2}]\log\frac{6}{\delta}}{n}}\\ &\quad+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3/ \delta)\right)\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}}{n}\\ &\quad\quad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2} n\right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\sqrt{\log 3e/\delta},\end{split}\] (28)

where the second inequality holds because \(\sqrt{a+b}+\sqrt{a}+\sqrt{b}\) for any \(a,b>0\), which means that

\[\begin{split}&\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\\ \lesssim&\beta\log n\log(1/\delta)+\frac{\log(1/ \delta)}{n}+\sqrt{\frac{\mathbb{E}_{Z}\left[\nabla\|f(\mathbf{w}^{*};Z)\|_{2} ^{2}\right]\log(1/\delta)}{n}}+\|A(S)-\mathbf{w}^{*}\|\sqrt{\frac{\log(1/ \delta)}{n}}.\end{split}\]

The proof is complete.

Proof of Lemma 1.: Inequality (28) implies that

\[\begin{split}&\|\nabla F(A(S))\|_{2}-\|\nabla F_{S}(A(S))\|_{2}\\ \leq&\sqrt{\frac{4(\gamma^{2}\|A(S)-\mathbf{w}^{*}\|_ {2}^{2}+\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}])\log\frac{6}{ \delta}}{n}}+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3/\delta )\right)\log\frac{6}{\delta}}{n}}\\ &\quad+\frac{M\log\frac{6}{\delta}}{n}+16\times 2^{\frac{3}{4}} \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta\right)+32 \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta}\\ \leq& 2\gamma\|A(S)-\mathbf{w}^{*}\|_{2}\sqrt{\frac{\log \frac{6}{\delta}}{n}}+\sqrt{\frac{4\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z) \|_{2}^{2}]\log\frac{6}{\delta}}{n}}+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32 n\beta^{2}\log(3/\delta)\right)\log\frac{6}{\delta}}{n}}\\ &\quad+\frac{M\log\frac{6}{\delta}}{n}+16\times 2^{\frac{3}{4}} \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta\right)+32 \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta},\end{split}\]

When \(F(\mathbf{w})\) satisfies the PL condition, there holds the following error bound property (refer to Theorem 2 in Karimi et al. (2016))

\[\|\nabla F(A(S))\|_{2}\geq\mu\|A(S)-\mathbf{w}^{*}\|_{2}.\]Thus, we have

\[\mu\|A(S)-\mathbf{w}^{*}\|_{2}\leq\|\nabla F(A(S))\|_{2}\] \[\leq \|\nabla F_{S}(A(S))\|_{2}+2\gamma\|A(S)-\mathbf{w}^{*}\|_{2}\sqrt{ \frac{\log\frac{6}{\delta}}{n}}+\sqrt{\frac{4\mathbb{E}_{Z}[\|\nabla f( \mathbf{w}^{*};Z)\|_{2}^{2}]\log\frac{6}{\delta}}{n}}\] \[\quad+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3/ \delta)\right)\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}}{n}\] \[\qquad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\sqrt{\log 3e/\delta}.\]

When \(n\geq\frac{16\gamma^{2}\log\frac{6}{\delta}}{\mu^{2}}\), we have \(2\gamma\sqrt{\frac{\log\frac{6}{\delta}}{n}}\leq\frac{\mu}{2}\), then we can derive that

\[\mu\|A(S)-\mathbf{w}^{*}\|_{2}\leq\|\nabla F(A(S))\|_{2}\] \[\leq \|\nabla F_{S}(A(S))\|_{2}+\frac{\mu}{2}\|A(S)-\mathbf{w}^{*}\|_{ 2}+\sqrt{\frac{4\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\log \frac{6}{\delta}}{n}}\] \[\quad+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3/ \delta)\right)\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}}{n}\] \[\qquad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil\log_{2}n \right\rceil\sqrt{\log 3e/\delta}.\]

This implies that

\[\|A(S)-\mathbf{w}^{*}\|_{2}\] \[\leq \frac{2}{\mu}\Big{(}\|\nabla F_{S}(A(S))\|_{2}+\sqrt{\frac{4 \mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\log\frac{6}{\delta}}{ n}}\] (29) \[\qquad+\sqrt{\frac{\left(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3 /\delta)\right)\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}}{n}\] \[\qquad\qquad+16\times 2^{\frac{3}{4}}\sqrt{e}\beta\left\lceil \log_{2}n\right\rceil\log\left(3e/\delta\right)+32\sqrt{e}\beta\left\lceil \log_{2}n\right\rceil\sqrt{\log 3e/\delta}\Big{)}.\]

Then, substituting (29) into (28), when \(n\geq\frac{16\gamma^{2}\log\frac{6}{\delta}}{\mu^{2}}\), with probability at least \(1-\delta\)

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|\] \[\leq \|\nabla F_{S}(A(S))\|+4\sqrt{\frac{\mathbb{E}_{Z}[\|\nabla f( \mathbf{w}^{*};Z)\|^{2}]\log\frac{6}{\delta}}{n}}+2\sqrt{\frac{\left(\frac{1 }{2}\beta^{2}+32n\beta^{2}\log(3/\delta)\right)\log\frac{6}{\delta}}{n}}\] \[\quad+\frac{2M\log\frac{6}{\delta}}{n}+32\times 2^{\frac{3}{4}} \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta\right)+64 \sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta}.\]

The proof is complete. 

Proof of Remark 5.: Here we briefly prove the results given in Remark 5. Since \(F\) satisfies the PL condition with \(\mu\), we have

\[F(A(S))-F(\mathbf{w}^{*})\leq\frac{\|\nabla F(A(S))\|^{2}}{2\mu},\quad\forall \mathbf{w}\in\mathcal{W}.\] (30)

So to bound \(F(A(S))-F(A(S))\), we need to bound the term \(\|\nabla F(A(S))\|^{2}\). And there holds

\[\|\nabla F(A(S))\|_{2}^{2}=2\left\|\nabla F(A(S))-\nabla F_{S}(A(S))\right\|^{2 }+2\|\nabla F_{S}(A(S))\|_{2}^{2}.\] (31)From Lemma 1, if \(f\) is \(M\)-Lipschitz and \(\gamma\)-smooth and \(F\) satisfies PL condition with \(\mu\), for any \(\delta>0\), when \(n\geq\frac{16\gamma^{2}\log\frac{6}{\delta}}{\mu^{2}}\), with probability at least \(1-\delta\), there holds

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq\|\nabla F_{S}(A(S))\|_{2}+C\left(\sqrt{\frac{2\mathbb{E}_{Z} [\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\log\frac{6}{\delta}}{n}}+\frac{M\log \frac{6}{\delta}}{n}+e\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta \right)\right)\] \[\leq\|\nabla F_{S}(A(S))\|_{2}+C\left(\sqrt{\frac{8\gamma F( \mathbf{w}^{*})\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}}{n}+e \beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta\right)\right),\]

where \(C\) is a positive constant and the last inequality follows from Lemma 4.1 of Srebro et al. (2010) when \(f\) is nonegative and \(\gamma\)-smooth (see (44)).

Combing above inequality with (30), (31), we can derive that

\[F(A(S))-F(\mathbf{w}^{*})\lesssim\|\nabla F_{S}(A(S))\|_{2}+\frac{F(\mathbf{w }^{*})\log\left(1/\delta\right)}{n}+\frac{M\log^{2}(1/\delta)}{n^{2}}+\beta^{2 }\log^{2}n\log^{2}(1/\delta).\]

The proof is complete. 

Proof of Lemma 2.: According to the proof in Theorem 3, we have the following inequality with probability at least \(1-\delta\)

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq \sqrt{\frac{2\left(2\mathbb{E}_{Z}[\|\nabla f(A(S);Z)\|_{2}^{2}] +\frac{1}{4}\beta^{2}+16n\beta^{2}\log(3/\delta)\right)\log\frac{6}{\delta}} {n}}\] (32) \[\quad+\frac{M\log\frac{6}{\delta}}{n}+16\times 2^{\frac{3}{ \delta}}\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta \right)+32\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta}.\]

Since SGC implies that \(\mathbb{E}_{Z}[\|\nabla f(\mathbf{w};Z)\|_{2}^{2}]\leq\rho\|\nabla F(\mathbf{ w})\|_{2}^{2}\), according to inequalities \(\sqrt{ab}\leq\eta a+\frac{1}{\eta}b\) and \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) for any \(a,b,\eta>0\), we have the following inequality with probability at least \(1-\delta\)

\[\|\nabla F(A(S))-\nabla F_{S}(A(S))\|_{2}\] \[\leq \sqrt{\frac{2\left(2\rho\|\nabla F(A(S))\|_{2}^{2}+\frac{1}{4} \beta^{2}+16n\beta^{2}\log(3/\delta)\right)\log\frac{6}{\delta}}{n}}\] \[\quad+\frac{M\log\frac{6}{\delta}}{n}+16\times 2^{\frac{3}{ \delta}}\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta \right)+32\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta}\] \[\leq \sqrt{\frac{(\frac{1}{2}\beta^{2}+32n\beta^{2}\log(3/\delta)) \log\frac{6}{\delta}}{n}}+\frac{\eta}{1+\eta}\|\nabla F(A(S))\|+\frac{1+\eta} {\eta}\frac{4\rho M\log\frac{6}{\delta}}{n}\] \[\quad+\frac{M\log\frac{6}{\delta}}{n}+16\times 2^{\frac{3}{ \delta}}\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta \right)+32\sqrt{e}\beta\left\lceil\log_{2}n\right\rceil\sqrt{\log 3e/\delta}.\]

which implies that

\[\|\nabla F(A(S))\|_{2}\leq(1+\eta)\|\nabla F_{S}(A(S))\|_{2}+C\frac{1+\eta}{ \eta}\left(\frac{M}{n}\log\frac{6}{\delta}+\beta\log n\log\frac{1}{\delta} \right).\]

The proof is complete.

Proofs of ERM

Proof of Lemma 3.: Since \(F_{S^{(i)}}(\mathbf{w})=\frac{1}{n}\left(f(\mathbf{w};z_{i}^{\prime})+\sum_{j\neq i }f(\mathbf{w},z_{j})\right)\), we have

\[F_{S}(\hat{\mathbf{w}}^{*}(S^{(i)}))-F_{S}(\hat{\mathbf{w}}^{*}(S))\] \[=\frac{f(\hat{\mathbf{w}}^{*}(S^{(i)});z_{i})-f(\hat{\mathbf{w}}^ {*}(S);z_{i})}{n}+\frac{\sum_{j\neq i}(f(\hat{\mathbf{w}}^{*}(S^{(i)});z_{j})-f (\hat{\mathbf{w}}^{*}(S);z_{j}))}{n}\] \[=\frac{f(\hat{\mathbf{w}}^{*}(S^{(i)});z_{i})-f(\hat{\mathbf{w}}^ {*}(S);z_{i})}{n}+\frac{f(\hat{\mathbf{w}}^{*}(S);z_{i}^{\prime})-f(\hat{ \mathbf{w}}^{*}(S^{(i)});z_{i}^{\prime})}{n}\] \[\quad+\left(F_{S^{(i)}}(\hat{\mathbf{w}}^{*}(S^{(i)}))-F_{S^{(i) }}(\hat{\mathbf{w}}^{*}(S))\right)\] \[\leq\frac{f(\hat{\mathbf{w}}^{*}(S^{(i)});z_{i})-f(\hat{\mathbf{ w}}^{*}(S);z_{i})}{n}+\frac{f(\hat{\mathbf{w}}^{*}(S);z_{i}^{\prime})-f( \hat{\mathbf{w}}^{*}(S^{(i)});z_{i}^{\prime})}{n}\] \[\leq\frac{2M}{n}\|\hat{\mathbf{w}}^{*}(S^{(i)})-\hat{\mathbf{w}}^ {*}(S)\|_{2},\]

where the first inequality follows from the fact that \(\hat{\mathbf{w}}^{*}(S^{(i)})\) is the ERM of \(F_{S^{(i)}}\) and the second inequality follows from the Lipschitz property. Furthermore, for \(\hat{\mathbf{w}}^{*}(S^{(i)})\), the convexity of \(f\) and the strongly-convex property of \(F_{S}\) imply that its closest optima point of \(F_{S}\) is \(\hat{\mathbf{w}}^{*}(S)\) (the global minimizer of \(F_{S}\) is unique). Then, there holds that

\[F_{S}(\hat{\mathbf{w}}^{*}(S^{(i)}))-F_{S}(\hat{\mathbf{w}}^{*}(S))\geq\frac{ \mu}{2}\|\hat{\mathbf{w}}^{*}(S^{(i)})-\hat{\mathbf{w}}^{*}(S)\|_{2}^{2}.\]

Then we get

\[\frac{\mu}{2}\|\hat{\mathbf{w}}^{*}(S^{(i)})-\hat{\mathbf{w}}^{*}(S)\|_{2}^{2 }\leq F_{S}(\hat{\mathbf{w}}^{*}(S^{(i)}))-F_{S}(\hat{\mathbf{w}}^{*}(S))\leq \frac{2M}{n}\|\hat{\mathbf{w}}^{*}(S^{(i)})-\hat{\mathbf{w}}^{*}(S)\|_{2},\]

which implies that \(\|\hat{\mathbf{w}}^{*}(S^{(i)})-\hat{\mathbf{w}}^{*}(S)\|_{2}\leq\frac{4M}{n\mu}\). Combined with the smoothness property of \(f\) we obtain that for any \(S^{(i)}\) and \(S\)

\[\forall z\in\mathcal{Z},\quad\left\|\nabla f(\hat{\mathbf{w}}^{*}(S^{(i)});z)- \nabla f(\hat{\mathbf{w}}^{*}(S);z)\right\|_{2}\leq\frac{4M\gamma}{n\mu}.\]

The proof is complete. 

Proof of Theorem 4.: Since \(F\) is \(\mu\)-strongly convex, we have

\[F(\mathbf{w})-F(\mathbf{w}^{*})\leq\frac{\left\|\nabla F(\mathbf{w})\right\|_ {2}^{2}}{2\mu},\quad\forall\mathbf{w}\in\mathcal{W}.\] (33)

So to bound \(F(\hat{\mathbf{w}}^{*})-F(\mathbf{w}^{*})\), we need to bound the term \(\left\|\nabla F(\hat{\mathbf{w}}^{*})\right\|_{2}^{2}\). And there holds

\[\|\nabla F(\hat{\mathbf{w}}^{*})\|_{2}^{2}=2\left\|\nabla F(\hat{\mathbf{w}}^{ *})-\nabla F_{S}(\hat{\mathbf{w}}^{*})\right\|_{2}^{2}+2\|\nabla F_{S}(\hat{ \mathbf{w}}^{*})\|_{2}^{2}.\] (34)

From Lemma 1, if \(f\) is \(M\)-Lipschitz and \(\gamma\)-smooth and \(F_{S}\) is \(\mu\)-strongly convex, for any \(\delta>0\), when \(n\geq\frac{16\gamma^{2}\log\frac{6}{\delta}}{\mu^{2}}\), with probability at least \(1-\delta\), there holds

\[\|\nabla F(\hat{\mathbf{w}}^{*})-\nabla F_{S}(\hat{\mathbf{w}}^{*} )\|_{2}\] \[\leq\|\nabla F_{S}(\hat{\mathbf{w}}^{*})\|_{2}+C\left(\sqrt{\frac{ 2\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\log\frac{6}{\delta}}{ n}}+\frac{M\log\frac{6}{\delta}}{n}+e\hat{\beta}\left[\log_{2}n\right]\log \left(3e/\delta\right)\right)\] \[\leq\|\nabla F_{S}(\hat{\mathbf{w}}^{*})\|_{2}+C\left(\sqrt{\frac{8 \gamma F(\mathbf{w}^{*})\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}}{ n}+e\hat{\beta}\left[\log_{2}n\right]\log\left(3e/\delta\right)\right),\]where the last inequality follows from Lemma 4.1 of Srebro et al. (2010) when \(f\) is nonegative (see (44)) and \(\gamma\)-smooth and \(\hat{\beta}=\left\|\nabla f(\hat{\mathbf{w}}^{*}(S);z)-\nabla f(\hat{\mathbf{w}} ^{*}(S^{\prime});z)\right\|_{2}\). \(C\) is a positive constant.

From Lemma 3, we have \(\left\|\nabla f(\hat{\mathbf{w}}^{*}(S);z)-\nabla f(\hat{\mathbf{w}}^{*}(S^{ \prime});z)\right\|_{2}\leq\frac{4M\gamma}{n\mu}\). Since \(\nabla F_{S}(\hat{\mathbf{w}}^{*})=0\), we have \(\|\nabla F_{S}(\hat{\mathbf{w}}^{*})\|_{2}=0\), then we can derive that

\[F(\mathbf{w})-F(\mathbf{w}^{*})\lesssim\frac{F(\mathbf{w}^{*}) \log{(1/\delta)}}{n}+\frac{\log^{2}{n}\log^{2}(1/\delta)}{n^{2}}.\]

## Appendix E Proofs of PGD

Proof of Theorem 5.: According to smoothness assumption and \(\eta=1/\gamma\), we can derive that

\[F_{S}(\mathbf{w}_{t+1})-F_{S}(\mathbf{w}_{t})\] \[\leq \langle\mathbf{w}_{t+1}-\mathbf{w}_{t},\nabla F_{S}(\mathbf{w}_{ t})\rangle+\frac{\gamma}{2}\|\mathbf{w}_{t+1}-\mathbf{w}_{t}\|_{2}^{2}\] \[= -\eta_{t}\|\nabla F_{S}(\mathbf{w}_{t})\|_{2}^{2}+\frac{\gamma}{ 2}\eta_{t}^{2}\|\nabla F_{S}(\mathbf{w}_{t})\|_{2}^{2}\] \[= \left(\frac{\gamma}{2}\eta_{t}^{2}-\eta_{t}\right)\|\nabla F_{S} (\mathbf{w}_{t})\|_{2}^{2}\] \[\leq -\frac{1}{2}\eta_{t}\|\nabla F_{S}(\mathbf{w}_{t})\|_{2}^{2}.\]

According to above inequality and the assumptions that \(F_{S}\) is \(\mu\)-strongly convex, we can prove that

\[F_{S}(\mathbf{w}_{t+1})-F_{S}(\mathbf{w}_{t})\leq-\frac{1}{2} \eta_{t}\|\nabla F_{S}(\mathbf{w}_{t})\|_{2}^{2}\leq-\mu\eta_{t}(F_{S}( \mathbf{w}_{t})-F_{S}(\hat{\mathbf{w}}^{*})),\]

which implies that

\[F_{S}(\mathbf{w}_{t+1})-F_{S}(\hat{\mathbf{w}}^{*})\leq(1-\mu \eta_{t})(F_{S}(\mathbf{w}_{t})-F_{S}(\hat{\mathbf{w}}^{*})).\]

According to the property for \(\gamma\)-smooth for \(F_{S}\) and the property for \(\mu\)-strongly convex for \(F_{S}\), we have

\[\frac{1}{2\gamma}\|\nabla F_{S}(\mathbf{w})\|_{2}^{2}\leq F_{S}( \mathbf{w})-F_{S}(\hat{\mathbf{w}}^{*})\leq\frac{1}{2\mu}\|\nabla F_{S}( \mathbf{w})\|_{2}^{2},\]

which means that \(\frac{\mu}{\gamma}\leq 1\).

Then If \(\eta_{t}=1/\gamma\), \(0\leq 1-\mu\eta_{t}<1\), taking over \(T\) iterations, we get

\[F_{S}(\mathbf{w}_{t+1})-F_{S}(\hat{\mathbf{w}}^{*})\leq(1-\mu\eta _{t})^{T}(F_{S}(\mathbf{w}_{t})-F_{S}(\hat{\mathbf{w}}^{*})).\] (36)

Combined (36), the smoothness of \(F_{S}\) and the nonnegative property of \(f\), it can be derive that

\[\|\nabla F_{S}(\mathbf{w}_{T+1}))\|_{2}^{2}=O\left((1-\frac{\mu}{ \gamma})^{T}\right).\]

Furthermore, since \(F\) is \(\mu\)-strongly convex, we have

\[F(\mathbf{w})-F(\mathbf{w}^{*})\leq\frac{\|\nabla F(\mathbf{w}) \|_{2}^{2}}{2\mu},\quad\forall\mathbf{w}\in\mathcal{W}.\] (37)

So to bound \(F(\mathbf{w}_{T+1})-F(\mathbf{w}^{*})\), we need to bound the term \(\left\|\nabla F(\mathbf{w}_{T+1})\right\|_{2}^{2}\). And there holds

\[\left\|\nabla F(\mathbf{w}_{T+1})\right\|_{2}^{2}=2\left\|\nabla F (\mathbf{w}_{T+1})-\nabla F_{S}(\mathbf{w}_{T+1})\right\|_{2}^{2}+2\|\nabla F _{S}(\mathbf{w}_{T+1})\|_{2}^{2}.\] (38)From Lemma 1, if \(f\) is \(M\)-Lipschitz and \(\gamma\)-smooth and \(F_{S}\) is \(\mu\)-strongly convex, for any \(\delta>0\), when \(n\geq\frac{16\gamma^{2}\log\frac{6}{\delta}}{\mu^{2}}\), with probability at least \(1-\delta\), there holds

\[\begin{split}&\quad\|\nabla F(\mathbf{w}_{T+1})-\nabla F_{S}( \mathbf{w}_{T+1})\|_{2}\\ &\leq\|\nabla F_{S}(\mathbf{w}_{T+1})\|_{2}+C\left(\sqrt{\frac{2 \mathbb{E}_{2}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\log\frac{6}{\delta}}{n} }+\frac{M\log\frac{6}{\delta}}{n}+e\beta\left\lceil\log_{2}n\right\rceil\log \left(3e/\delta\right)\right)\\ &\leq\|\nabla F_{S}(\mathbf{w}_{T+1})\|_{2}+C\left(\sqrt{\frac{8 \gamma F(\mathbf{w}^{*})\log\frac{6}{\delta}}{n}}+\frac{M\log\frac{6}{\delta}} {n}+e\beta\left\lceil\log_{2}n\right\rceil\log\left(3e/\delta\right)\right), \end{split}\] (39)

where the last inequality follows from Lemma 4.1 of Srebro et al. (2010) when \(f\) is nonegative and \(\gamma\)-smooth (see (44)) and \(\beta=\|\nabla f(\mathbf{w}_{T+1}(S);z)-\nabla f(\mathbf{w}_{T+1}(S^{\prime}); z)\|_{2}\). \(C\) is a positive constant.

From Lemma 4, we have \(\beta=\left\|\nabla f(\mathbf{w}_{T+1}(S);z)-\nabla f(\mathbf{w}_{T+1}(S^{ \prime});z)\right\|_{2}\leq\frac{2M\gamma}{n\mu}\). Since \(\|\nabla F_{S}(\mathbf{w}_{T+1})\|_{2}=O\left((1-\frac{\mu}{\gamma})^{T}\right)\), then we can derive that

\[F(\mathbf{w})-F(\mathbf{w}^{*})\lesssim\left(1-\frac{\mu}{\gamma}\right)^{2T} +\frac{F(\mathbf{w}^{*})\log\left(1/\delta\right)}{n}+\frac{\log^{2}n\log^{2}( 1/\delta)}{n^{2}}.\]

Let \(T\asymp\log n\), we have

\[F(\mathbf{w})-F(\mathbf{w}^{*})\lesssim\frac{F(\mathbf{w}^{*})\log\left(1/ \delta\right)}{n}+\frac{\log^{2}n\log^{2}(1/\delta)}{n^{2}}.\]

The proof is complete. 

## Appendix F Proofs of SGD

We first introduce some necessary lemmata on the empirical risk.

**Lemma 14** ([11]).: _Let \(\{\mathbf{w}_{t}\}_{t}\) be the sequence produced by SGD with \(\eta_{t}\leq\frac{1}{2\gamma}\) for all \(t\in\mathbb{N}\). Suppose Assumption 1 hold. Assume for all \(z\), the function \(\mathbf{w}\mapsto f(\mathbf{w};z)\) is \(M\)-Lipschitz and \(\gamma\)-smooth. Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), there holds that_

\[\sum_{k=1}^{t}\eta_{k}\|\nabla F_{S}(\mathbf{w}_{k})\|_{2}^{2}=O\left(\log \frac{1}{\delta}+\sum_{k=1}^{t}\eta_{k}^{2}\right).\]

**Lemma 15** ([11]).: _Let \(\{\mathbf{w}_{t}\}_{t}\) be the sequence produced by SGD with \(\eta_{t}=\frac{2}{\mu(t+t_{0})}\) such that \(t_{0}\geq\max\{\frac{4\gamma}{\mu},1\}\) for all \(t\in\mathbb{N}\). Suppose Assumption 1 hold. Assume for all \(z\), the function \(\mathbf{w}\mapsto f(\mathbf{w};z)\) is \(M\)-Lipschitz and \(\gamma\)-smooth and assume \(F_{S}\) satisfies PL condition with parameter \(\mu\). Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), there holds that_

\[F_{S}(\mathbf{w}_{T+1})-F_{S}(\hat{\mathbf{w}}^{*})=O\left(\frac{\log(T)\log^{ 3}(1/\delta)}{T}\right).\]

**Lemma 16** ([11]).: _Let \(e\) be the base of the natural logarithm. There holds the following elementary inequalities._

* _If_ \(\theta\in(0,1)\)_, then_ \(\sum_{k=1}^{t}k^{-\theta}\leq t^{1-\theta}/(1-\theta)\)_;_
* _If_ \(\theta=1\)_, then_ \(\sum_{k=1}^{t}k^{-\theta}\leq\log(et)\)_;_
* _If_ \(\theta>1\)_, then_ \(\sum_{k=1}^{t}k^{-\theta}\leq\frac{\theta}{\theta-1}\)_._

Proof of Lemma 5.: We have known that \(F_{S^{(i)}}(\mathbf{w})=\frac{1}{n}\left(f(\mathbf{w};z_{i}^{\prime})+\sum_{j \neq i}f(\mathbf{w};z_{j})\right)\). We denote \(\hat{\mathbf{w}}^{*}(S^{(i)})\) be the ERM of \(F_{S^{(i)}}(\mathbf{w})\) and \(\hat{\mathbf{w}}_{S}^{*}\) be the ERM of \(F_{S}(\mathbf{w})\). From Lemma 3, we know that

\[\forall z\in\mathcal{Z},\quad\left\|\nabla f(\hat{\mathbf{w}}^{*}(S^{(i)});z)- f(\hat{\mathbf{w}}^{*}(S);z)\right\|_{2}\leq\frac{4M\gamma}{n\mu}.\]Also, for \(\mathbf{w}_{t}\), the convexity of \(f\) and the strongly-convex property implies that its closest optima point of \(F_{S}\) is \(\hat{\mathbf{w}}^{*}(S)\) (the global minimizer of \(F_{S}\) is unique). Then, there holds that

\[\frac{\mu}{2}\|\mathbf{w}_{t}-\hat{\mathbf{w}}^{*}(S)\|_{2}^{2}\leq F_{S}( \mathbf{w}_{t})-F_{S}(\hat{\mathbf{w}}^{*}(S))=\epsilon_{opt}(\mathbf{w}_{t}).\]

Thus we have \(\|\mathbf{w}_{t}-\hat{\mathbf{w}}^{*}(S)\|_{2}\leq\sqrt{\frac{2\epsilon_{opt} (\mathbf{w}_{t})}{\mu}}\). A similar relation holds between \(\hat{\mathbf{w}}^{*}(S^{(i)})\) and \(\mathbf{w}_{t}^{i}\). Combined with the Lipschitz property of \(f\) we obtain that for \(\forall z\in\mathcal{Z}\), there holds that

\[\left\|\nabla f(\mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z) \right\|_{2}\] \[\leq\] \[\leq\] \[\leq \gamma\sqrt{\frac{2\epsilon_{opt}(\mathbf{w}_{t})}{\mu}}+\frac{4 M\gamma}{n\mu}+\gamma\sqrt{\frac{2\epsilon_{opt}(\mathbf{w}_{t}^{i})}{\mu}}.\]

According to Lemma 15, for any dataset \(S\), the optimization error \(\epsilon_{opt}(\mathbf{w}_{t})\) is uniformly bounded by the same upper bound. Therefore, we write \(\left\|\nabla f(\mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z)\right\|_{2} \leq 2\gamma\sqrt{\frac{2\epsilon_{opt}(\mathbf{w}_{t})}{\mu}}+\frac{4M \gamma}{n\mu}\) here.

The proof is complete. 

Now We begin to prove Lemma 6.

Proof of Lemma 6.: If \(f\) is \(L\)-Lipschitz and \(\gamma\)-smooth and \(F_{S}\) is \(\mu\)-strongly convex. According to Lemma 1, we know that for all \(\mathbf{w}\in\mathcal{W}\) and any \(\delta\in(0,1)\), with probability at least \(1-\delta/2\), when \(n>\frac{16\gamma^{2}\log\frac{\delta}{2}}{\mu^{2}}\), we have

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F(\mathbf{w}_{t})\|_{2}^{2}\] (40) \[\leq 16\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t} \|\nabla F_{S}(\mathbf{w}_{t})\|_{2}^{2}+\frac{4C^{2}L^{2}\log^{2}\frac{6}{ \delta}}{n^{2}}+\frac{8C^{2}\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^ {2}]\log^{2}\frac{6}{\delta}}{n}\] \[+\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}C^ {2}e^{2}\beta_{t}^{2}\left[\log_{2}n\right]^{2}\log^{2}(3e/\delta),\]

where \(\beta_{t}=\left\|\nabla f(\mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z) \right\|_{2}\) and \(C\) is a positive constant.

From Lemma 5, we have \(\left\|\nabla f(\mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z)\right\|_{2} \leq 2\gamma\sqrt{\frac{2\epsilon_{opt}(\mathbf{w}_{t})}{\mu}}+\frac{4M \gamma}{n\mu}\), thus

\[\beta_{t}^{2}= \left\|\nabla f(\mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z) \right\|_{2}^{2}\] (41) \[\leq \left(2\gamma\sqrt{\frac{2\epsilon_{opt}(\mathbf{w}_{t})}{\mu}}+ \frac{4M\gamma}{n\mu}\right)^{2}\] \[\leq \frac{16\gamma^{2}(F_{S}(\mathbf{w}_{t})-F_{S}(\hat{\mathbf{w}}^{ *}(S)))}{\mu}+\frac{32M^{2}\gamma^{2}}{n^{2}\mu^{2}}\] \[\leq \frac{8\gamma^{2}\|\nabla F_{S}(\mathbf{w}_{t})\|_{2}^{2}}{\mu^{2 }}+\frac{32M^{2}\gamma^{2}}{n^{2}\mu^{2}},\]

where the second inequality holds from Cauchy-Bunyakovsky-Schwarz inequality and the second inequality satisfies because \(F_{S}\) is \(\mu\)-strongly convex.

Plugging (41) into (40), with probability at least \(1-\delta/2\), when \(n>\frac{16\gamma^{2}\log\frac{\delta}{\mu^{2}}}{\mu^{2}}\), we have

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F(\mathbf{w}_{t})\|_{2}^{2}\] \[\leq \left(16+\frac{8\gamma^{2}C^{2}e^{2}\left\lceil\log_{2}n\right\rceil ^{2}\log^{2}\left(6e/\delta\right)}{\mu^{2}}\right)\left(\sum_{t=1}^{T}\eta_{t }\right)^{-1}\sum_{t=1}^{T}\eta_{t}\|\nabla F_{S}(\mathbf{w}_{t})\|_{2}^{2}\] \[+\frac{4C^{2}L^{2}\log^{2}\frac{12}{\delta}}{n^{2}}+\frac{8C^{2} \mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}\log^{2}\frac{12}{\delta }}{n}+\frac{32L^{2}\gamma^{2}C^{2}e^{2}\left\lceil\log_{2}n\right\rceil^{2} \log^{2}\left(6e/\delta\right)}{n^{2}\mu^{2}},\] (42)

When \(\eta_{t}=\eta_{1}t^{-\theta},\theta\in(0,1)\), with \(\eta_{1}\leq\frac{1}{2\beta}\) and Assumption 1, according to Lemma 14 and Lemma 16, we obtain the following inequality with probability at least \(1-\delta/2\),

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F_{S}(\mathbf{w}_{t})\|^{2}=\begin{cases}O\left(\frac{\log(1/\delta)}{ T^{-\theta}}\right),&\text{if }\theta<1/2\\ O\left(\frac{\log(1/\delta)}{T^{-\frac{1}{\delta}}}\right),&\text{if } \theta=1/2\\ O\left(\frac{\log(1/\delta)}{T^{\theta-1}}\right),&\text{if }\theta>1/2.\end{cases}\] (43)

On the other hand, when \(f\) is nonegative and \(\gamma\)-smooth, from Lemma 4.1 of Srebro et al. (2010), we have

\[\|\nabla f(\mathbf{w}^{*};z)\|_{2}^{2}\leq 4\gamma f(\mathbf{w}^{*};z),\]

which implies that

\[\mathbb{E}_{Z}[\|\nabla f(\mathbf{w}^{*};Z)\|_{2}^{2}]\leq 4\gamma\mathbb{E}_{Z }f(\mathbf{w}^{*};Z)=4\gamma F(\mathbf{w}^{*}).\] (44)

Plugging (44), (43) into (42), with probability at least \(1-\delta\), we derive that

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F(\mathbf{w}_{t})\|_{2}^{2}\] \[= \begin{cases}O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{T^{-\theta }}\right)+O\left(\frac{\log^{2}n\log^{2}(1/\delta)}{n^{2}}+\frac{F(\mathbf{w} ^{*})\log^{2}(1/\delta)}{n}\right),&\text{if }\theta<1/2\\ O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{T^{-\frac{1}{\delta}}}\right)+O \left(\frac{\log^{2}n\log^{2}(1/\delta)}{n^{2}}+\frac{F(\mathbf{w}^{*})\log^ {2}(1/\delta)}{n}\right),&\text{if }\theta=1/2\\ O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{T^{\theta-1}}\right)+O\left(\frac{ \log^{2}n\log^{2}(1/\delta)}{n^{2}}+\frac{F(\mathbf{w}^{*})\log^{2}(1/\delta )}{n}\right),&\text{if }\theta>1/2.\end{cases}\]

When \(\theta<1/2\), we set \(T\asymp n^{\frac{2}{\delta}}\) and assume \(F(\mathbf{w}^{*})=O(\frac{1}{n})\), then we obtain the following result with probability at least \(1-\delta\)

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F(\mathbf{w}_{t})\|_{2}^{2}=O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{ n^{2}}\right).\]

When \(\theta>1/2\), we set \(T\asymp n^{\frac{2}{1-\theta}}\) and assume \(F(\mathbf{w}^{*})=O(\frac{1}{n})\), then we obtain the following result with probability at least \(1-\delta\)

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F(\mathbf{w}_{t})\|_{2}^{2}=O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{ n^{2}}\right).\]

When \(\theta>1/2\), we set \(T\asymp n^{\frac{2}{1-\theta}}\) and assume \(F(\mathbf{w}^{*})=O(\frac{1}{n})\), then we obtain the following result with probability at least \(1-\delta\)

\[\left(\sum_{t=1}^{T}\eta_{t}\right)^{-1}\sum_{t=1}^{T}\eta_{t}\| \nabla F(\mathbf{w}_{t})\|_{2}^{2}=O\left(\frac{\log^{2}n\log^{3}(1/\delta)}{ n^{2}}\right).\]

The proof is complete.

Proof of Theorem 7.: Since \(F\) is \(\mu\)-strongly convex, we have

\[F(\mathbf{w})-F(\mathbf{w}^{*})\leq\frac{\left\|\nabla F(\mathbf{w})\right\|_{2}^ {2}}{2\mu},\quad\forall\mathbf{w}\in\mathcal{W}.\] (45)

So to bound \(F(\mathbf{w}_{T+1})-F(\mathbf{w}^{*})\), we need to bound the term \(\left\|\nabla F(\mathbf{w}_{T+1})\right\|_{2}^{2}\). And there holds

\[\left\|\nabla F(\mathbf{w}_{T+1})\right\|_{2}^{2}=2\left\|\nabla F(\mathbf{w}_ {T+1})-\nabla F_{S}(\mathbf{w}_{T+1})\right\|^{2}+2\|\nabla F_{S}(\mathbf{w}_ {T+1})\|_{2}^{2}.\] (46)

From Lemma 1, if \(f\) is \(L\)-Lipschitz and \(\gamma\)-smooth and \(F_{S}\) is \(\mu\)-strongly convex, for all \(\mathbf{w}\in\mathcal{W}\) and any \(\delta>0\), when \(n\geq\frac{16\gamma^{2}\log\frac{\delta}{\delta}}{\mu^{2}}\), with probability at least \(1-\delta/2\), there holds

\[\begin{split}&\left\|\nabla F(\mathbf{w}_{T+1})-\nabla F_{S}( \mathbf{w}_{T+1})\right\|_{2}\\ \leq&\left\|\nabla F_{S}(\mathbf{w}_{T+1})\right\|_ {2}+C\left(\sqrt{\frac{2\mathbb{E}_{Z}[\left\|\nabla f(\mathbf{w}^{*};Z) \right\|_{2}^{2}]\log\frac{12}{\delta}}{n}}+\frac{M\log\frac{12}{\delta}}{n}+ e\beta\left\lceil\log_{2}n\right\rceil\log\left(6e/\delta\right)\right)\\ \leq&\left\|\nabla F_{S}(\mathbf{w}_{T+1})\right\|_ {2}+C\left(\sqrt{\frac{8\gamma F(\mathbf{w}^{*})\log\frac{12}{\delta}}{n}}+ \frac{M\log\frac{12}{\delta}}{n}+e\beta\left\lceil\log_{2}n\right\rceil\log \left(6e/\delta\right)\right),\end{split}\] (47)

where the last inequality follows from Lemma 4.1 of Srebro et al. (2010) when \(f\) is nonegative and \(\gamma\)-smooth (see (44)) and \(C\) is a positive constant. Then we can derive that

\[\begin{split}&\left\|\nabla F(\mathbf{w}_{T+1})-\nabla F_{S}( \mathbf{w}_{T+1})\right\|_{2}^{2}\\ \leq& 4\|\nabla F_{S}(\mathbf{w}_{T+1})\|_{2}^{2}+ \frac{32C^{2}\gamma F(\mathbf{w}^{*})\log\frac{12}{\delta}}{n}+\frac{4M^{2}C^ {2}\log^{2}\frac{12}{\delta}}{n^{2}}+4e^{2}\beta_{T+1}^{2}\left\lceil\log_{2} n\right\rceil^{2}\log^{2}\left(6e/\delta\right).\end{split}\] (48)

From Lemma 5, we have \(\left\|\nabla f(\mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z)\right\|_{2} \leq 2\gamma\sqrt{\frac{2\epsilon_{opt}(\mathbf{w}_{t})}{\mu}}+\frac{4M \gamma}{n\mu}\), thus

\[\begin{split}\beta_{t}^{2}=&\left\|\nabla f( \mathbf{w}_{t};z)-\nabla f(\mathbf{w}_{t}^{i};z)\right\|_{2}^{2}\\ \leq&\left(2\gamma\sqrt{\frac{2\epsilon_{opt}( \mathbf{w}_{t})}{\mu}}+\frac{4M\gamma}{n\mu}\right)^{2}\\ \leq&\frac{16\gamma^{2}(F_{S}(\mathbf{w}_{t})-F_{S}( \hat{\mathbf{w}}^{*}(S)))}{\mu}+\frac{32M^{2}\gamma^{2}}{n^{2}\mu^{2}}\\ \leq&\frac{8\gamma^{2}\|\nabla F_{S}(\mathbf{w}_{t} )\|_{2}^{2}}{\mu^{2}}+\frac{32M^{2}\gamma^{2}}{n^{2}\mu^{2}},\end{split}\] (49)

where the second inequality holds from Cauchy-Bunyakovsky-Schwarz inequality and the second inequality satisfies because \(F_{S}\) is \(\mu\)-strongly convex.

Plugging (49) into (48), with probability at least \(1-\delta/2\), when, we have

\[\begin{split}&\left\|\nabla F(\mathbf{w}_{T+1})-\nabla F_{S}( \mathbf{w}_{T+1})\right\|_{2}^{2}\\ \leq&\left(4+32e^{2}\left\lceil\log_{2}n\right\rceil^ {2}\log^{2}\left(6e/\delta\right)\right)\|\nabla F_{S}(\mathbf{w}_{T+1})\|_{2}^ {2}+\frac{32C^{2}\gamma F(\mathbf{w}^{*})\log\frac{6}{\delta}}{n}\\ &\quad+\frac{4L^{2}C^{2}\log^{2}\frac{12}{\delta}}{n^{2}}+\frac{ 128M^{2}\gamma^{2}e^{2}\left\lceil\log_{2}n\right\rceil^{2}\log^{2}\left(6e/ \delta\right)}{n^{2}\mu^{2}}.\end{split}\] (50)

According to the smoothness property of \(F_{S}\) and Lemma 15, it can be derived that with propability at least \(1-\delta/2\)

\[\left\|\nabla F_{S}(\mathbf{w}_{T+1})\right\|_{2}^{2}=O\left(\frac{\log T\log^{ 3}(1/\delta)}{T}\right).\] (51)Substituting (51), (50) into (46), we derive that

\[\begin{split}&\left\|\nabla F(\mathbf{w}_{T+1})\right\|_{2}^{2}\\ =& O\left(\frac{\left\lceil\log_{2}n\right\rceil^{2} \log T\log^{5}(1/\delta)}{T}\right)+O\left(\frac{\left\lceil\log_{2}n\right\rceil ^{2}\log^{2}(1/\delta)}{n^{2}}+\frac{F(\mathbf{w}^{*})\log(1/\delta)}{n} \right).\end{split}\] (52)

Further substituting (52) into (45) and choosing \(T\asymp n^{2}\), we finally obtain that when \(n\), with probability at least \(1-\delta\)

\[F(\mathbf{w}_{T+1})-F(\mathbf{w}^{*})=O\left(\frac{\log^{4}n\log^{5}(1/\delta )}{n^{2}}+\frac{F(\mathbf{w}^{*})\log(1/\delta)}{n}\right).\]NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have checked that the abstract and introduction accurately reflect our contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have clearly stated the required assumptions for each theorem and lemma, and the conditions for the assumptions to hold are also stated in the main text. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We have clearly stated the required assumptions for each theorem and lemma, and all proofs are provided in the appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This paper focuses on learning theory. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [NA]

Justification: This paper focuses on learning theory and does not include experiments.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This paper focuses on learning theory and does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This paper focuses on learning theory and does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This paper focuses on learning theory and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper focuses on learning theory and there is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper focuses on learning theory and poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper focuses on learning theory and does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.