Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis

Alexander Meulemans11, Simon Schug11, Seijin Kobayashi11

**Nathaniel D Daw2,3,4, Gregory Wayne2**

Equal contribution; ordering determined by coin flip.Code available at https://github.com/seijin-kobayashi/cocoa

Footnote 1: https://github.com/seijin-kobayashi/cocoa

Footnote 2: https://github.com/seijin-kobayashi/cocoa

###### Abstract

To make reinforcement learning more sample efficient, we need better credit assignment methods that measure an action's influence on future rewards. Building upon Hindsight Credit Assignment (HCA) [1], we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual query: 'Would the agent still have reached this reward if it had taken another action?'. We show that measuring contributions w.r.t. rewarding _states_, as is done in HCA, results in spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or learned representations of the rewarding objects, resulting in gradient estimates with lower variance. We run experiments on a suite of problems specifically designed to evaluate long-term credit assignment capabilities. By using dynamic programming, we measure ground-truth policy gradients and show that the improved performance of our new model-based credit assignment methods is due to lower bias and variance compared to HCA and common baselines. Our results demonstrate how modeling action contributions towards rewarding outcomes can be leveraged for credit assignment, opening a new path towards sample-efficient reinforcement learning.2

Footnote 2: https://github.com/seijin-kobayashi/cocoa

## 1 Introduction

Reinforcement learning (RL) faces two central challenges: exploration and credit assignment [2]. We need to explore to discover rewards and we need to reinforce the actions that are instrumental for obtaining these rewards. Here, we focus on the credit assignment problem and the intimately linked problem of estimating policy gradients. For long time horizons, obtaining the latter is notoriously difficult as it requires measuring how each action influences expected subsequent rewards. As the number of possible trajectories grows exponentially with time, future rewards come with a considerable variance stemming from stochasticity in the environment itself and from the stochasticity of interdependent future actions leading to vastly different returns [3, 4, 5].

Monte Carlo estimators such as REINFORCE [6] therefore suffer from high variance, even after variance reduction techniques like subtracting a baseline [6, 7, 8, 9]. Similarly, in Temporal Difference methods such as Q-learning, this high variance in future rewards results in a high bias in the value estimates, requiring exponentially many updates to correct for it [5]. Thus, a common technique to reduce variance and bias is to discount rewards that are far away in time resulting in a biased estimator which ignores long-term dependencies [10; 11; 12; 13]. Impressive results have nevertheless been achieved in complex environments [14; 15; 16] at the cost of requiring billions of environment interactions, making these approaches sample inefficient.

Especially in settings where obtaining such large quantities of data is costly or simply not possible, model-based RL that aims to simulate the dynamics of the environment is a promising alternative. While learning such a world model is a difficult problem by itself, when successful it can be used to generate a large quantity of synthetic environment interactions. Typically, this synthetic data is combined with model-free methods to improve the action policy [17; 18; 19]. A notable exception to simply using world models to generate more data are the Stochastic Value Gradient method [20] and the closely related Dreamer algorithms [21; 22; 23]. These methods perform credit assignment by backpropagating policy gradients through the world model. Crucially, this approach only works for environments with a continuous state-action space, as otherwise sensitivities of the value with respect to past actions are undefined [20; 22; 24]. Intuitively, we cannot compute sensitivities of discrete choices such as a yes / no decision as the agent cannot decide 'yes' a little bit more or less.

Building upon Hindsight Credit Assignment (HCA) [1], we develop Counterfactual Contribution Analysis (COCOA), a family of algorithms that use models for credit assignment compatible with discrete actions. We measure the _contribution_ of an action upon subsequent rewards by asking a counterfactual question: 'would the agent still have achieved the rewarding outcome, if it had taken another action?' (c.f. Fig. 1A). We show that measuring contributions towards achieving a future _state_, as is proposed in HCA, leads to spurious contributions that do not reflect a contribution towards a reward. This causes HCA to degrade towards the high-variance REINFORCE method in most environments. Instead, we propose to measure contributions directly on rewarding outcomes and we develop various new ways of learning these contributions from observations. The resulting algorithm differs from value-based methods in that it measures the contribution of an action to individual rewards, instead of estimating the full expected sum of rewards. This crucial difference allows our contribution analysis to disentangle different tasks and ignore uncontrollable environment influences, leading to a gradient estimator capable of long-term credit assignment (c.f. Fig. 1B). We introduce a new method for analyzing policy gradient estimators which uses dynamic programming to allow comparing to ground-truth policy gradients. We leverage this to perform a detailed bias-variance analysis of all proposed methods and baselines showing that our new model-based credit assignment algorithms achieve low variance and bias, translating into improved performance (c.f. Fig. 1C).

Figure 1: **Counterfactual Contribution Analysis enables long-term credit assignment.** (A) Given a sample trajectory that eventually results in a rewarding outcome, we estimate the policy gradient by considering the contribution of actions along the trajectory towards arriving at a rewarding outcome. In this example, we measure how much more likely the rewarding outcome with reward \(r_{3}\) is when following action \(a_{1}\) versus the counterfactual actions \(a_{2}\) and \(a_{3}\) in state \(s_{2}\). This is quantified through the contribution coefficient \(w(s_{2},a_{1},r_{3})\) which is used to update all possible action probabilities of the policy \(\pi(a\mid s_{2})\). (B) In the linear key-to-door environment increasing the distance between picking up the key and opening the door that leads to reward necessitates credit assignment over increasing time spans. COCOA consistently achieves good performance (left) compared to HCA and baselines which deteriorate when increasing the distance between an action and the resulting rewarding outcome. This is reflected in a higher signal-to-noise ratio of the policy gradient estimator of COCOA compared to baselines (right).

## 2 Background and notation

We consider an undiscounted Markov decision process (MDP) defined as the tuple \((\mathcal{S},\mathcal{A},p,p_{r})\), with \(\mathcal{S}\) the state space, \(\mathcal{A}\) the action space, \(p(S_{t+1}\mid S_{t},A_{t})\) the state-transition distribution and \(p_{r}(R\mid S,A)\) the reward distribution with bounded reward values \(r\). We use capital letters for random variables and lowercase letters for the values they take. The policy \(\pi(A\mid S)\), parameterized by \(\theta\), denotes the probability of taking action \(A\) at state \(S\). We consider an undiscounted infinite-horizon setting with a zero-reward absorbing state \(s_{\infty}\) that the agent eventually reaches: \(\lim_{t\rightarrow\infty}p(S_{t}=s_{\infty})=1\). Both the discounted and episodic RL settings are special cases of this setting. (c.f. App. B), and hence all theoretical results proposed in this work can be readily applied to both (c.f. App J).

We use \(\mathcal{T}(s,\pi)\) and \(\mathcal{T}(s,a,\pi)\) as the distribution over trajectories \(T=(S_{t}\,A_{t},R_{t})_{t\geq 0}\) starting from \(S_{0}=s\) and \((S_{0},A_{0})=(s,a)\) respectively, and define the return \(Z_{t}=\sum_{t=0}^{\infty}R_{t}\). The value function \(V^{\pi}(s)=\mathbb{E}_{T\sim\mathcal{T}(s,\pi)}\left[Z_{t}\right]\) and action value function \(Q^{\pi}(s,a)=\mathbb{E}_{T\sim\mathcal{T}(s,a,\pi)}\left[Z_{t}\right]\) are the expected return when starting from state \(s\), or state \(s\) and action \(a\) respectively. Note that these infinite sums have finite values due to the absorbing zero-reward state (c.f. App. B).

The objective of reinforcement learning is to maximize the expected return \(V^{\pi}(s_{0})\), where we assume the agent starts from a fixed state \(s_{0}\). Policy gradient algorithms optimize \(V^{\pi}(s_{0})\) by repeatedly estimating its gradient \(\nabla_{\theta}V(s_{0})\) w.r.t. the policy parameters. REINFORCE [6] (c.f. Tab. 1) is the canonical policy gradient estimator, however, it has a high variance resulting in poor parameter updates. Common techniques to reduce the variance are (i) subtracting a baseline, typically a value estimate, from the sum of future rewards [2; 25] (c.f. 'Advantage' in Tab. 1); (ii) replacing the sum of future rewards with a learned action value function \(Q\)[2; 3; 25; 26] (c.f. 'Q-critic' in Tab. 1); and (iii) using temporal discounting. Note that instead of using a discounted formulation of MDPs, we treat the discount factor as a variance reduction technique in the undiscounted problem [10; 11; 13] as this more accurately reflects its practical use [4; 27]. Rearranging the summations of REINFORCE with discounting lets us interpret temporal discounting as a credit assignment heuristic, where for each reward, past actions are reinforced proportional to their proximity in time.

\[\hat{\nabla}_{\theta}^{\text{REINFORCE};\gamma}V^{\pi}(s_{0})=\sum\nolimits_ {t\geq 0}R_{t}\sum\nolimits_{k\leq t}\gamma^{t-k}\nabla_{\theta}\log\pi(A_{k} \mid S_{k}),\quad\gamma\in[0,1].\] (1)

Crucially, long-term dependencies between actions and rewards are exponentially suppressed, thereby reducing variance at the cost of disabling long-term credit assignment [4; 28]. The aim of this work is to replace the heuristic of time discounting by principled _contribution coefficients_ quantifying how much an action contributed towards achieving a reward, and thereby introducing new policy gradient estimators with reduced variance, without jeopardizing long-term credit assignment.

HCA [1] makes an important step in this direction by introducing a new gradient estimator:

\[\hat{\nabla}_{\theta}^{\text{HCA}}V^{\pi}=\sum_{t\geq 0}\sum_{a\in\mathcal{A}} \nabla_{\theta}\pi(a\mid S_{t})\Big{(}r(S_{t},a)+\sum_{k\geq 1}\frac{p^{\pi}(A_{t }=a\mid S_{t}=s,S^{\prime}=S_{t+k})}{\pi(a\mid S_{t})}R_{t+k}\Big{)}\] (2)

with \(r(s,a)\) a reward model, and the _hindsight_ ratio \(\frac{p^{\pi}(a\mid S_{t}=s,S^{\prime}=S_{t+k})}{\pi(a\mid S_{t})}\) measuring how important action \(a\) was to reach the state \(S^{\prime}\) at some point in the future. Although the hindsight ratio delivers precise credit assignment w.r.t. reaching future states, it has a failure mode of practical importance, creating the need for an updated theory of model-based credit assignment which we will detail in the next section.

## 3 Counterfactual Contribution Analysis

To formalize the 'contribution' of an action upon subsequent rewards, we generalize the theory of HCA [1] to measure contributions on _rewarding outcomes_ instead of states. We introduce unbiased policy gradient estimators that use these contribution measures, and show that HCA suffers from high variance, making the generalization towards rewarding outcomes crucial for obtaining low-variance estimators. Finally, we show how we can estimate contributions using observational data.

### Counterfactual contribution coefficients

To assess the contribution of actions towards rewarding outcomes, we propose to use counterfactual reasoning: 'how does taking action \(a\) influence the probability of obtaining a rewarding outcome, compared to taking alternative actions \(a^{\prime}\)?'.

**Definition 1** (Rewarding outcome).: A rewarding outcome \(U^{\prime}\sim p(U^{\prime}\mid s^{\prime},a^{\prime},r^{\prime})\) is a probabilistic encoding of the state-action-reward triplet.

If action \(a\) contributed towards the reward, the probability of obtaining the rewarding outcome following action \(a\) should be higher compared to taking alternative actions. We quantify the contribution of action \(a\) taken in state \(s\) upon rewarding outcome \(u^{\prime}\) as

\[w(s,a,u^{\prime})=\frac{\sum_{k\geq 1}p^{\pi}(U_{t+k}=u^{\prime}\mid S_{t}=s,A_{ t}=a)}{\sum_{k\geq 1}p^{\pi}(U_{t+k}=u^{\prime}\mid S_{t}=s)}-1=\frac{p^{\pi}(A_{ t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})}{\pi(a\mid s)}-1\] (3)

From a given state, we compare the probability of reaching the _rewarding outcome_\(u^{\prime}\) at any subsequent point in time, given we take action \(a\) versus taking counterfactual actions according to the policy \(\pi\), as \(p^{\pi}(U_{t+k}=u^{\prime}\mid S_{t}=s)=\sum_{a^{\prime}}\pi(a^{\prime}\mid s) p^{\pi}(U_{t+k}=u^{\prime}\mid S_{t}=s,A_{t}=a^{\prime})\). Subtracting this ratio by one results in an intuitive interpretation of the _contribution coefficient_\(w(s,a,u^{\prime})\): if the coefficient is positive/negative, performing action \(a\) results in a higher/lower probability of obtaining rewarding outcomes \(u^{\prime}\), compared to following the policy \(\pi\). Using Bayes' rule, we can convert the counterfactual formulation of the contribution coefficients into an equivalent _hindsight_ formulation (right-hand side of Eq. 3), where the hindsight distribution \(p^{\pi}(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})\) reflects the probability of taking action \(a\) in state \(s\), given that we encounter the rewarding outcome \(u^{\prime}\) at _some future point in time_. We refer the reader to App. C for a full derivation.

**Choice of rewarding outcome.** For \(u^{\prime}=s^{\prime}\), we recover state-based HCA [1].3 In the following, we show that a better choice is to use \(u^{\prime}=r^{\prime}\), or an encoding \(p(u^{\prime}\mid s^{\prime},a^{\prime})\) of the underlying object that causes the reward. Both options lead to gradient estimators with lower variance (c.f. Section 3.3), while using the latter becomes crucial when different underlying rewarding objects have the same scalar reward (c.f. Section 4).

Footnote 3: Note that Harutyunyan et al. [1] also introduce an estimator based on the return. Here, we focus on the state-based variant as only this variant uses contribution coefficients to compute the policy gradient (Eq. 4). The return-based variant instead uses the hindsight distribution as an action-dependent baseline as shown in Tab. 1. Importantly, the return-based estimator is biased in many relevant environments (c.f. Appendix L).

### Policy gradient estimators

We now show how the contribution coefficients can be used to learn a policy. Building upon HCA [1], we propose the Counterfactual Contribution Analysis (COCOA) policy gradient estimator

\[\hat{\nabla}_{\theta}^{U}V^{\pi}(s_{0})=\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_ {t}\mid S_{t})R_{t}+\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t}) \sum_{k\geq 1}w(S_{t},a,U_{t+k})R_{t+k}.\] (4)

When comparing to the discounted policy gradient of Eq. 1, we see that the temporal discount factors are substituted by the contribution coefficients, replacing the time heuristic with fine-grained credit assignment. Importantly, the contribution coefficients enable us to evaluate all counterfactual actions instead of only the observed ones, further increasing the quality of the gradient estimator (c.f. Fig. 1A). The contribution coefficients allow for various different gradient estimators (c.f. App. C). For example, independent action samples can replace the sum over all actions, making it applicable to large action spaces. Here, we use the gradient estimator of Eq. 4, as our experiments consist of small action spaces where enumerating all actions is feasible. When \(U=S\), the above estimator is almost equivalent to

\begin{table}
\begin{tabular}{l l} \hline \hline
**Method** & **Policy gradient estimator** (\(\hat{\nabla}_{\theta}V^{\pi}(s_{0})\)) \\ \hline REINFORCE & \(\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\sum_{k\geq 0}R_{t+k}\) \\ Advantage & \(\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\left(\sum_{k\geq 0}R_{t+k}-V (S_{t})\right)\) \\ Q-critic & \(\sum_{t\geq 0}\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})Q(S_{t},a)\) \\ HCA-Return & \(\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\Big{(}1-\frac{\pi(A_{t} \mid S_{t})}{p^{\pi}(A_{t}\mid S_{t},Z_{t})}\Big{)}Z_{t}\) \\ TrajCV & \(\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\left(Z_{t}-Q(A_{t},S_{t}) -\sum_{t^{\prime}>t}\left(Q(S_{t^{\prime}},A_{t^{\prime}})-V(S_{t^{\prime}}) \right)+\ldots\) \\  & \(\ldots\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})Q(S_{t},a)\) \\ \hline COCOA & \(\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})R_{t}+\sum_{a\in\mathcal{A}} \nabla_{\theta}\pi(a\mid S_{t})\sum_{k\geq 1}w(S_{t},a,U_{t+k})R_{t+k}\) \\ HCA+ & \(\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})R_{t}+\sum_{a\in\mathcal{A}} \nabla_{\theta}\pi(a\mid S_{t})\sum_{k\geq 1}w(S_{t},a,S_{t+k})R_{t+k}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of policy gradient estimators.

the state-based HCA estimator of Eq. 2, except that it does not need a learned reward model \(r(s,a)\). We use the notation HCA+ to refer to this simplified version of the HCA estimator. Theorem 1 below shows that the COCOA gradient estimator is unbiased, as long as the encoding \(U\) is fully predictive of the reward, thereby generalizing the results of Harutyunyan et al. [1] to arbitrary rewarding outcome encodings.

**Definition 2** (Fully predictive).: A rewarding outcome \(U\) is fully predictive of the reward \(R\), if the following conditional independence condition holds for all \(k\geq 0\): \(p^{\pi}(R_{k}=r\mid S_{0}=s,A_{0}=a,U_{k}=u)=p^{\pi}(R=r\mid U=u)\), where the right-hand side does not depend on the time \(k\).

**Theorem 1**.: _Assuming that \(U\) is fully predictive of the reward (c.f. Definition 2), the COCOA policy gradient estimator \(\hat{\nabla}_{\theta}^{U}V^{\pi}(s_{0})\) is unbiased, when using the ground-truth contribution coefficients of Eq. 3, that is_

\[\nabla_{\theta}V^{\pi}(s_{0})=\mathbb{E}_{T\sim\mathcal{T}(s_{0},\pi)}\hat{ \nabla}_{\theta}^{U}V^{\pi}(s_{0}).\]

### Optimal rewarding outcome encoding for low-variance gradient estimators

Theorem 1 shows that the COCOA gradient estimators are unbiased for all rewarding outcome encodings \(U\) that are fully predictive of the reward. The difference between specific rewarding outcome encodings manifests itself in the variance of the resulting gradient estimator. Proposition 2 shows that for \(U^{\prime}=S^{\prime}\) as chosen by HCA there are many cases where the variance of the resulting policy gradient estimator degrades to the high-variance REINFORCE estimator [6]:

**Proposition 2**.: _In environments where each action sequence leads deterministically to a different state, we have that the HCA+ estimator is equal to the REINFORCE estimator (c.f. Tab. 1)._

In other words, when all previous actions can be perfectly decoded from a given state, they trivially all contribute to reaching this state. The proof of Proposition 2 follows immediately from observing that \(p^{\pi}(a\mid s,s^{\prime})=1\) for actions \(a\) along the observed trajectory, and zero otherwise. Substituting this expression into the contribution analysis gradient estimator (4) recovers REINFORCE. A more general issue underlies this special case: State representations need to contain detailed features to allow for a capable policy but the same level of detail is detrimental when assigning credit to actions for reaching a particular state since at some resolution almost every action will lead to a slightly different outcome. Measuring the contribution towards reaching a specific state ignores that the same rewarding outcome could be reached in slightly different states, hence overvaluing the importance of previous actions and resulting in _spurious contributions_. Many commonly used environments, such as pixel-based environments, continuous environments, and partially observable MDPs exhibit this property to a large extent due to their fine-grained state representations (c.f. App. G). Hence, our generalization of HCA to rewarding outcomes is a crucial step towards obtaining practical low-variance gradient estimators with model-based credit assignment.

**Using rewards as rewarding outcomes yields lowest-variance estimators.** The following Theorem 3 shows in a simplified setting that (i) the variance of the REINFORCE estimator is an upper bound on the variance of the COCOA estimator, and (ii) the variance of the COCOA estimator is smaller for rewarding outcome encodings \(U\) that contain less information about prior actions. We formalize this with the conditional independence relation of Definition 2 by replacing \(R\) with \(U^{\prime}\): encoding \(U\) contains less or equal information than encoding \(U^{\prime}\), if \(U^{\prime}\) is fully predictive of \(U\). Combined with Theorem 1 that states that an encoding \(U\) needs to be fully predictive of the reward \(R\), we have that taking the reward \(R\) as our rewarding outcome encoding \(U\) results in the gradient estimator with the lowest variance of the COCOA family.

**Theorem 3**.: _Consider an MDP where only the states at a single (final) time step contain a reward, and where we optimize the policy only at a single (initial) time step. Furthermore, consider two rewarding outcome encodings \(U\) and \(U^{\prime}\), where \(S\) is fully predictive of \(U^{\prime}\), \(U^{\prime}\) fully predictive of \(U\), and \(U\) fully predictive of \(R\). Then, the following relation holds between the policy gradient estimators:_

\[\forall[\hat{\nabla}_{\theta}^{R}V^{\pi}(s_{0})]\preccurlyeq\forall[\hat{ \nabla}_{\theta}^{U}V^{\pi}(s_{0})]\preccurlyeq\forall[\hat{\nabla}_{\theta}^ {U^{\prime}}V^{\pi}(s_{0})]\preccurlyeq\forall[\hat{\nabla}_{\theta}^{S}V^{ \pi}(s_{0})]\preccurlyeq\forall[\hat{\nabla}_{\theta}^{REINF}V^{\pi}(s_{0})]\]

_with \(\hat{\nabla}_{\theta}^{X}V^{\pi}(s_{0})\) the COCOA estimator (4) using \(U=X\), \(\curlyeq[Y]\) the covariance matrix of \(Y\) and \(A\preccurlyeq B\) indicating that \(B-A\) is positive semi-definite._

As Theorem 3 considers a simplified setting, we verify empirically whether the same arguments hold more generally. We construct a tree environment where we control the amount of information a state contains about the previous actions by varying the overlap of the children of two neighbouring nodes (c.f. Fig 2), and assign a fixed random reward to each state-action pair. We compute the ground-truth contribution coefficients by leveraging dynamic programming (c.f. Section 4). Fig. 2B shows that the variance of HCA is as high as REINFORCE for zero state overlap, but improves when more states overlap, consistent with Proposition 2 and Theorem 3. To investigate the influence of the information content of \(U\) on the variance, we consider rewarding outcome encodings \(U\) with increasing information content, which we quantify with how many different values of \(u\) belong to the same reward \(r\). Fig. 2B shows that by increasing the information content of \(U\), we interpolate between the variance of COCOA with \(u=r\) and HCA+, consistent with Theorem 3.

**Why do rewarding outcome encodings that contain more information than the reward lead to higher variance?** To provide a better intuition on this question we use the following theorem:

**Theorem 4**.: _The policy gradient on the expected number of occurrences \(O^{\pi}(u^{\prime},s)=\sum_{k\geq 1}p^{\pi}(U_{k}=u^{\prime}\mid S_{0}=s)\) is proportional to_

\[\nabla_{\theta}O^{\pi}(u^{\prime},s)\propto\mathbb{E}_{S^{\prime\prime}\sim \mathcal{T}(\pi,s)}\sum\nolimits_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid S ^{\prime\prime})w(S^{\prime\prime},a,u^{\prime})O^{\pi}(u^{\prime},S^{\prime \prime})\]

Recall that the COCOA gradient estimator consists of individual terms that credit past actions \(a\) at state \(s\) for a current reward \(r\) encountered in \(u\) according to \(\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid s)w(s,a,u^{\prime})r^{\prime}\) (c.f. Eq. 4). Theorem 4 indicates that each such term aims to increase the average number of times we encounter \(u^{\prime}\) in a trajectory starting from \(s\), proportional to the corresponding reward \(r^{\prime}\). If \(U^{\prime}=R^{\prime}\), this update will correctly make all underlying states with the same reward \(r^{\prime}\) more likely while decreasing the likeliness of all states for which \(u^{\prime}\neq r^{\prime}\). Now consider the case where our rewarding outcome encoding contains a bit more information, i.e. \(U^{\prime}=f(R^{\prime},\Delta S^{\prime})\) where \(\Delta S^{\prime}\) contains a little bit of information about the state. As a result the update will distinguish some states even if they yield the same reward and increase the number of occurrences only of states containing the encountered \(\Delta S^{\prime}\) while decreasing the number of occurrences for unseen ones. As in a single trajectory, we do not visit each possible \(\Delta S^{\prime}\), this adds variance. The less information an encoding \(U\) has, the more underlying states it groups together, and hence the less rewarding outcomes are 'forgotten' in the gradient estimator, leading to lower variance.

### Learning the contribution coefficients

In practice, we do not have access to the ground-truth contribution coefficients, but need to learn them from observations. Following Harutyunyan et al. [1], we can approximate the hindsight distribution \(p^{\pi}(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})\), now conditioned on rewarding outcome encodings instead of states, by training a model \(h(a\mid s,u^{\prime})\) on the supervised discriminative task of classifying the observed action \(a_{t}\) given the current state \(s_{t}\) and some future rewarding outcome \(u^{\prime}\). Note that if the model \(h\) does not approximate the hindsight distribution perfectly, the COCOA gradient estimator (4) can be biased (c.f. Section 4). A central difficulty in approximating the hindsight distribution is that it is policy dependent, and hence changes during training. Proposition 5 shows that we can provide the policy logits as an extra input to the hindsight network without altering the learned hindsight

Figure 2: **HCA suffers from spurious contributions which can be alleviated by using less informative rewarding outcome encodings.** (A) and (C): Schematic of the tree environment where we parametrically adjust the amount of overlap between states by varying the amount of shared children of two neighboring nodes. We can decrease the information content of the rewarding outcome encoding \(u=f(s,a)\) by grouping state-action pairs that share the same reward value. (B) Normalized variance in dB using ground-truth coefficients and a random uniform policy (shaded region represents standard error over 10 random environments) comparing REINFORCE, HCA, COCOCA-reward and various degrees of intermediate grouping.

distribution, to make the parameters of the hindsight network less policy-dependent. This observation justifies and generalizes the strategy of adding the policy logits to the hindsight model output, as proposed by Alipov et al. [29].

**Proposition 5**.: \(p^{\pi}(a\mid s,u^{\prime},l)=p^{\pi}(a\mid s,u^{\prime})\)_, with \(l\) a deterministic function of \(s\), representing the sufficient statistics of \(\pi(a\mid s)\)._

As an alternative to learning the hindsight distribution, we can directly estimate the probability ratio \(p^{\pi}(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})/\pi(a\mid s)\) using a contrastive loss (c.f. App. D). Yet another path builds on the observation that the sums \(\sum_{k\geq 1}p^{\pi}(U_{t+k}=u^{\prime}\mid s,a)\) are akin to Successor Representations and can be learned via temporal difference updates [30; 31] (c.f. App. D). We experimented both with the hindsight classification and the contrastive loss and found the former to work best in our experiments. We leverage the Successor Representation to obtain ground truth contribution coefficients via dynamic programming for the purpose of analyzing our algorithms.

## 4 Experimental analysis

To systematically investigate long-term credit assignment performance of COCOA compared to standard baselines, we design an environment which pinpoints the core credit assignment problem and leverage dynamic programming to compute ground-truth policy gradients, contribution coefficients, and value functions (c.f. App E.2). This enables us to perform detailed bias-variance analyses and to disentangle the theoretical optimal performance of the various gradient estimators from the approximation quality of learned contribution coefficients and (action-)value functions.

We consider the _linear key-to-door_ environment (c.f. Fig. 3A), a simplification of the key-to-door environment [3; 4; 32] to a one-dimensional track. Here, the agent needs to pick up a key in the first time step, after which it engages in a distractor task of picking up apples with varying reward values. Finally, it can open a door with the key and collect a treasure. This setting allows us to parametrically increase the difficulty of long-term credit assignment by increasing the distance between the key and door, making it harder to pick up the learning signal of the treasure reward

Figure 3: **COCOA enhances policy gradient estimates and sample efficiency whereas HCA fails to improve over baselines.** (A) Schematic representation of the linear key-to-door environment. (B) Performance of COCOA and baselines on the main task of picking up the treasure, measured as the average fraction of treasure rewards collected. (Left) ground-truth policy gradient estimators computed using dynamic programming, (right) learning the contribution coefficients or (action-)value function using neural networks. Shaded regions are the standard error (30 seeds). (C) Violin plot of the signal-to-noise ratio (SNR) in Decibels for the various policy gradient estimators with learned coefficients and (action-)value functions, computed on the same trajectories of a shared base policy. (D) Comparison of the bias-variance trade-off incurred by different policy gradient estimators, computed as in (C), normalized by the ground-truth policy gradient norm (scatter plot showing 30 seeds per method).

among a growing number of varying distractor rewards [4]. We use the signal-to-noise ratio, \(\text{SNR}=\|\nabla_{\theta}V^{\pi}\|^{2}/\mathbb{E}[\|\hat{\nabla}_{\theta}V^{ \pi}-\nabla_{\theta}V^{\pi}\|^{2}]\), to quantify the quality of the different policy gradient estimators; a higher SNR indicates that we need fewer trajectories to estimate accurate policy gradients [33].

Previously, we showed that taking the reward as rewarding outcome encoding results in the lowest-variance policy gradients when using ground-truth contribution coefficients. In this section, we will argue that when _learning_ the contribution coefficients, it is beneficial to use an encoding \(u\) of the underlying _rewarding object_ since this allows to distinguish different rewarding objects when they have the same scalar reward value and allows for quick adaptation when the reward function but not the environment dynamics changes.

We study two variants of COCOA, COCOA-reward which uses the reward identity for \(U\), and COCOA-feature which acquires features of rewarding objects by learning a reward model \(r(s,a)\) and taking the penultimate network layer as \(U\). We learn the contribution coefficients by approximating the hindsight distribution with a neural network classifier \(h(a\mid s,u^{\prime},l)\) that takes as input the current state \(s\), resulting policy logits \(l\), and rewarding outcome \(u^{\prime}\), and predicts the current action \(a\) (c.f. App. E for all experimental details). As HCA+ (c.f. Tab. 1) performs equally or better compared to HCA [1] in our experiments (c.f. App. F), we compare to HCA+ and several other baselines: (i) three classical policy gradient estimators, REINFORCE, Advantage and Q-critic, (ii) TrajCV [34], a state-of-the-art control variate method that uses hindsight information in its baseline, and (iii) HCA-return [1], a different HCA variant that uses the hindsight distribution conditioned on the return as an action-dependent baseline (c.f. Tab. 1).

### COCOA improves sample-efficiency due to favorable bias-variance trade-off.

To investigate the quality of the policy gradient estimators of COCOA, we consider the linear key-to-door environment with a distance of 100 between key and door. Our dynamic programming setup allows us to disentangle the performance of the estimators independent of the approximation quality of learned models by using ground truth contribution coefficients and (action-)value functions. The left panel of figure 3B reveals that in this ground truth setting, COCOA-reward almost immediately solves the task performing as well as the theoretically optimal Q-critic with a perfect action-value function. This is in contrast to HCA and HCA-return which perform barely better than REINFORCE, all failing to learn to consistently pick up the key in the given number of episodes. This result translates to the setting of learning the underlying models using neural networks. COCOA-reward and -feature outperform competing policy gradient estimators in terms of sample efficiency while HCA only improves over REINFORCE. Notably, having to learn the full action-value function leads to a less sample-efficient policy gradient estimator for the Q-critic.

In Figure 3C and D we leverage dynamic programming to compare to the ground truth policy gradient. This analysis reveals that improved performance of COCOA is reflected in a higher SNR compared to other estimators due to its favorable bias-variance trade-off. Fig 12 in App. F indicates that COCOA maintains a superior SNR, even when using significantly biased contribution coefficients. As predicted by our theory in Section 3.3, HCA significantly underperforms compared to baselines due to its high variance caused by spurious contributions. In particular, the Markov state representation of the linear key-to-door environment contains the information of whether the key has been picked up. As a result, HCA always credits picking up the key or not, even for distractor rewards. These spurious contributions bury the useful learning signal of the treasure reward in noisy distractor rewards. HCA-return performs poorly as it is a biased gradient estimator, even when using the ground-truth hindsight distribution (c.f. Appendix F and L). Interestingly, the variance of COCOA is significantly lower compared to a state-of-the-art control variate method, TrajCV, pointing to a potential benefit of the multiplicative interactions between contribution coefficients and rewards in the COCOA estimators, compared to the additive interaction of the control variates: the value functions used in TrajCV need to approximate the full average returns, whereas COCOA can ignore rewards from the distractor subtask, by multiplying them with a contribution coefficient of zero.

### COCOA enables long-term credit assignment by disentangling rewarding outcomes.

The linear key-to-door environment allows us to parametrically increase the difficulty of the long-term credit assignment problem by increasing the distance between the key and door and thereby increasing the variance due to the distractor task [4]. Figure 1B reveals that as this distance increases, performance measured as the average fraction of treasure collected over a fixed number of episodesdrops for all baselines including HCA but remains relatively stable for the COCOA estimators. Hung et al. [4] showed that the SNR of REINFORCE decreases inversely proportional to growing distance between key and door. Figure 1B shows that HCA and all baselines follow this trend, whereas the COCOA estimators maintain a robust SNR.

We can explain the qualitative difference between COCOA and other methods by observing that the key-to-door task consists of two distinct subtasks: picking up the key to get the treasure, and collecting apples. COCOA can quickly learn that actions relevant for one task, do not influence rewarding outcomes in the other task, and hence output a contribution coefficient equal to zero for those combinations. Value functions in contrast estimate the expected sum of future rewards, thereby mixing the rewards of both tasks. When increasing the variance of the return in the distractor task by increasing the number of stochastic distractor rewards, estimating the value functions becomes harder, whereas estimating the contribution coefficients between state-action pairs and distinct rewarding objects remains of equal difficulty, showcasing the power of disentangling rewarding outcomes.

To further showcase the power of disentangling subtasks, we consider a simplified version of the _task interleaving_ environment of Mesnard et al. [3] (c.f. Fig. 4A, App. E.7). Here, the agent is faced with a sequence of contextual bandit tasks, where the reward for a correct decision is given at a later point in time, together with an observation of the relevant context. The main credit assignment difficulty is to relate the reward and contextual observation to the correct previous contextual bandit task. Note that the variance in the return is now caused by the stochastic policy, imperfectly solving future tasks, and by stochastic state transitions, in contrast to the linear key-to-door environment where the variance is caused by stochastic distractor rewards. Figure 4B shows that our COCOA algorithms outperform all baselines. The learned contribution coefficients of COCOA reward accurately capture that actions in one context only contribute to rewards in the same context as opposed to HCA that fails to disentangle the contributions (c.f. Figure 4C).

### Learned credit assignment

**features allow for disentangling aliased rewards**

For COCOA-reward we use the scalar reward value to identify rewarding outcomes in the hindsight distribution, i.e. \(U=R\). In cases where multiple rewarding outcomes yield an identical scalar reward value, the hindsight distribution cannot distinguish between them and has to estimate a common hindsight

Figure 4: **COCOA improves performance by disentangling subtasks.** (A) Schematic representation of the task interleaving environment where colored borders indicate the context of a room. (B) Performance of COCOA and baselines with learned contribution coefficients or value functions, measured as the fraction of correct choices. (C) Visualization of the contribution coefficient magnitudes of each query room on reward rooms for COCOA (top) and HCA (bottom), c.f. App. E.7.

Figure 5: **COCOA-features is robust to reward aliasing.** On a version of the linear key-to-door environment where one of the distractor reward values has the same magnitude as the treasure reward, COCOA-reward can no longer distinguish between the distractor and treasure reward and as a result its performance decreases. COCOA-feature is robust to this manipulation since it relies on learned features to distinguish rewarding objects.

probability, making it impossible to disentangle the tasks and hence rendering learning of the contribution coefficients potentially more difficult. In contrast, COCOA-feature learns hindsight features of rewarding objects that are predictive of the reward. Even when multiple rewarding objects lead to an identical scalar reward value their corresponding features are likely different, allowing COCOA-feature to disentangle the rewarding outcomes.

In Fig. 5, we test this _reward aliasing_ setting experimentally and slightly modify the linear key-to-door environment by giving the treasure reward the same value as one of the two possible values of the stochastic distractor rewards. As expected, COCOA-feature is robust to reward aliasing, continuing to perform well on the task of picking up the treasure while performance of COCOA-reward noticeably suffers. Note that the performance of all methods has slightly decreased compared to Fig. 3, as the magnitude of the treasure reward is now smaller relative to the variance of the distractor rewards, resulting in a worse SNR for all methods.

## 5 Discussion

We present a theory for model-based credit assignment compatible with discrete actions and show in a comprehensive theoretical and experimental analysis that this yields a powerful policy gradient estimator, enabling long-term credit assignment by disentangling rewarding outcomes.

Building upon HCA [1], we focus on amortizing the estimation of the contribution coefficients in an inverse dynamics model, \(p^{\pi}(a\mid s,u^{\prime})\). The quality of this model is crucial for obtaining low-bias gradient estimates, but it is restricted to learn from on-policy data, and rewarding observations in case of \(u=r\). Scaling these inverse models to complex environments will potentially exacerbate this tension, especially in sparse reward settings. A promising avenue for future work is to leverage forward dynamics models and directly estimate contribution coefficients from synthetic trajectories. While learning a forward model is a difficult problem in its own, its policy independence increases the data available for learning it. This would result in an algorithm close in spirit to Stochastic Value Gradients [20] and Dreamer [21, 22, 23] with the crucial advance that it enables model-based credit assignment on discrete actions. Another possibility to enable learning from non-rewarding observations is to learn a generative model that can recombine inverse models based on state representations into reward contributions (c.f. App. H).

Related work has explored the credit assignment problem through the lens of transporting rewards or value estimates towards previous states to bridge long-term dependencies [35, 36, 37, 4, 5, 4, 32, 4, 5, 38, 4, 5, 4, 4, 5, 6, 7, 8, 9, 4, 5, 4, 5, 4, 5, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]. This approach is compatible with existing and well-established policy gradient estimators but determining how to redistribute rewards has relied on heuristic contribution analyses, such as via the access of memory states [4], linear decompositions of rewards [35, 36, 37, 32, 38, 39] or learned sequence models [40, 41, 5]. Leveraging our unbiased contribution analysis framework to reach more optimal reward transport is a promising direction for future research.

While we have demonstrated that contribution coefficients with respect to states as employed by HCA suffer from spurious contributions, any reward feature encoding that is fully predictive of the reward can in principle suffer from a similar problem in the case where each environment state has a unique reward value. In practice, this issue might occur in environments with continuous rewards. A potential remedy in this situation is to assume that the underlying reward distribution is stochastic, smoothing the contribution coefficients as now multiple states could have led to the same reward. This lowers the variance of the gradient estimator as we elaborate in App. G.

Finally, we note that our contribution coefficients are closely connected to causality theory [42] where the contribution coefficients correspond to performing _Do-interventions_ on the causal graph to estimate their effect on future rewards (c.f. App I). Within causality theory, counterfactual reasoning goes a step further by inferring the external, uncontrollable environment influences and considering the consequences of counterfactual actions _given that all external influences remain the same_[42, 43, 44, 3, 20]. Extending COCOA towards this more advanced counterfactual setting by building upon recent work [43, 3] is an exciting direction for future research (c.f. App. I).

**Concluding remarks.** By overcoming the failure mode of _spurious contributions_ in HCA, we have presented here a comprehensive theory on how to leverage model information for credit assignment, compatible with discrete action spaces. COCOA-reward and COCOA-feature are promising first algorithms in this framework, opening the way towards sample-efficient reinforcement learning by model-based credit assignment.

## 6 Acknowledgements

We thank Angelika Steger, Yassir Akram, Ida Momennejad, Blake Richards, Matt Botvinick and Joel Veness for discussions and feedback. Simon Schug is supported by the Swiss National Science Foundation (PZ00P3_186027). Seijin Kobayashi is supported by the Swiss National Science Foundation (CRSII5_173721). Simon Schug would like to kindly thank the TPU Research Cloud (TRC) program for providing access to Cloud TPUs from Google.

## References

* [1] Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Gheshlaghi Azar, Bilal Piot, Nicolas Heess, Hado P van Hasselt, Gregory Wayne, Satinder Singh, Doina Precup, and Remi Munos. Hindsight Credit Assignment. In H. Wallach, H. Larochelle, A. Beygelzimer, F. Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 12488-12497. Curran Associates, Inc., 2019.
* [2] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning, second edition: An Introduction_. MIT Press, November 2018.
* [3] Thomas Mesnard, Theophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade, Anna Harutyunyan, Will Dabney, Tom Stepleton, Nicolas Heess, Arthur Guez, Marcus Hutter, Lars Buesing, and Remi Munos. Counterfactual Credit Assignment in Model-Free Reinforcement Learning. In _Proceedings of the 38 th International Conference on Machine Learning, PMLR 139_, 2021.
* [4] Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, Arun Ahuja, and Greg Wayne. Optimizing agent behavior over long time scales by transporting value. _Nature Communications_, 10(1):5223, November 2019. Number: 1 Publisher: Nature Publishing Group.
* [5] Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, and Sepp Hochreiter. RUDER: Return Decomposition for Delayed Rewards. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\(\ddot{\text{u}}\)textquotesing Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 13566-13577. Curran Associates, Inc., 2019.
* [6] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, 8(3):229-256, May 1992.
* [7] Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning. _Journal of Machine Learning Research_, 5(Nov):1471-1530, 2004.
* [8] Theophane Weber, Nicolas Heess, Lars Buesing, and David Silver. Credit Assignment Techniques in Stochastic Computation Graphs. In _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, pages 2650-2660. PMLR, April 2019. ISSN: 2640-3498.
* [9] Jonathan Baxter and Peter L. Bartlett. Infinite-Horizon Policy-Gradient Estimation. _Journal of Artificial Intelligence Research_, 15:319-350, November 2001.
* [10] Philip Thomas. Bias in Natural Actor-Critic Algorithms. In _Proceedings of the 31st International Conference on Machine Learning_, pages 441-448. PMLR, January 2014. ISSN: 1938-7228.
* [11] Sham Kakade. Optimizing Average Reward Using Discounted Rewards. In David Helmbold and Bob Williamson, editors, _Computational Learning Theory_, Lecture Notes in Computer Science, pages 605-615, Berlin, Heidelberg, 2001. Springer.
* [12] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-Dimensional Continuous Control Using Generalized Advantage Estimation. In _International Conference on Learning Representations_. arXiv, 2016. arXiv:1506.02438 [cs].
* [13] Peter Marbach and John Tsitsiklis. Approximate Gradient Methods in Policy-Space Optimization of Markov Reward Processes. _Discrete Event Dynamic Systems_, page 38, 2003.

* [14] OpenAI, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Jozefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with Large Scale Deep Reinforcement Learning, December 2019. arXiv:1912.06680 [cs, stat].
* [15] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving Rubik's Cube with a Robot Hand, October 2019. arXiv:1910.07113 [cs, stat].
* [16] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Remi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Denis Hassabis, Chris Apps, and David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, November 2019. Number: 7782 Publisher: Nature Publishing Group.
* [17] Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. _ACM SIGART Bulletin_, 2(4):160-163, July 1991.
* [18] Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion. In _32nd Conference on Neural Information Processing Systems_. arXiv, 2018. arXiv:1807.01675 [cs, stat].
* [19] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-Ensemble Trust-Region Policy Optimization. 2018.
* [20] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning Continuous Control Policies by Stochastic Value Gradients. In _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [21] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to Control: Learning Behaviors by Latent Imagination. In _International Conference on Learning Representations_, March 2020. Number: arXiv:1912.01603 arXiv:1912.01603 [cs].
* [22] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering Atari with Discrete World Models. In _International Conference on Learning Representations_, 2021. Number: arXiv:2010.02193 arXiv:2010.02193 [cs, stat].
* [23] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering Diverse Domains through World Models, January 2023. arXiv:2301.04104 [cs, stat].
* [24] Mikael Henaff, William F. Whitney, and Yann LeCun. Model-Based Planning with Discrete and Continuous Actions, April 2018. arXiv:1705.07177 [cs].
* [25] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In _Proceedings of The 33rd International Conference on Machine Learning_, pages 1928-1937. PMLR, June 2016. ISSN: 1938-7228.
* [26] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In _Advances in Neural Information Processing Systems 12_, 1999.
* [27] Chris Nota and Philip S Thomas. Is the Policy Gradient a Gradient? In _Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020)_, page 9, 2020.

* Schulman et al. [2018] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-Dimensional Continuous Control Using Generalized Advantage Estimation, October 2018. arXiv:1506.02438 [cs].
* Alipov et al. [2022] Vyacheslav Alipov, Riley Simmons-Edler, Nikita Putintsev, Pavel Kalinin, and Dmitry Vetrov. Towards Practical Credit Assignment for Deep Reinforcement Learning, February 2022. arXiv:2106.04499 [cs].
* Dayan [1993] Peter Dayan. Improving Generalization for Temporal Difference Learning: The Successor Representation. _Neural Computation_, 5(4):613-624, July 1993. Conference Name: Neural Computation.
* Kulkarni et al. [2016] Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J. Gershman. Deep Successor Reinforcement Learning, June 2016. Number: arXiv:1606.02396 arXiv:1606.02396 [cs, stat].
* Raposo et al. [2021] David Raposo, Sam Ritter, Adam Santoro, Greg Wayne, Theophane Weber, Matt Botvinick, Hado van Hasselt, and Francis Song. Synthetic Returns for Long-Term Credit Assignment. _arXiv:2102.12425 [cs]_, February 2021. arXiv: 2102.12425.
* Roberts and Tedrake [2009] John W. Roberts and Russ Tedrake. Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, _Advances in Neural Information Processing Systems 21_, pages 1361-1368. Curran Associates, Inc., 2009.
* Cheng et al. [2020] Ching-An Cheng, Xinyan Yan, and Byron Boots. Trajectory-wise Control Variates for Variance Reduction in Policy Gradient Methods. In _Proceedings of the Conference on Robot Learning_, pages 1379-1394. PMLR, May 2020. ISSN: 2640-3498.
* Ren et al. [2022] Zhizhou Ren, Ruihan Guo, Yuan Zhou, and Jian Peng. Learning Long-Term Reward Redistribution via Randomized Return Decomposition. January 2022.
* Efroni et al. [2021] Yonathan Efroni, Nadav Merlis, and Shie Mannor. Reinforcement Learning with Trajectory Feedback. In _The Thirty-Fifth AAAI Conference on Artificial Intelligenc_. arXiv, March 2021.
* Seo et al. [2019] Minah Seo, Luiz Felipe Vecchietti, Sangkeum Lee, and Dongsoo Har. Rewards Prediction-Based Credit Assignment for Reinforcement Learning With Sparse Binary Rewards. _IEEE ACCESS_, 7:118776-118791, 2019. Accepted: 2019-09-24T11:21:52Z Publisher: IEEE-INST ELECTICAL ELECTRONICS ENGINEERS INC.
* Ausin et al. [2021] Markel Sanz Ausin, Hamoon Azizsoltani, Song Ju, Yeo Jin Kim, and Min Chi. InferNet for Delayed Reinforcement Tasks: Addressing the Temporal Credit Assignment Problem. In _2021 IEEE International Conference on Big Data (Big Data)_, pages 1337-1348, December 2021.
* Azizsoltani et al. [2019] Hamoon Azizsoltani, Yeo Jin Kim, Markel Sanz Ausin, Tiffany Barnes, and Min Chi. Unobserved Is Not Equal to Non-existent: Using Gaussian Processes to Infer Immediate Rewards Across Contexts. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, pages 1974-1980, 2019.
* Patil et al. [2022] Vihang P. Patil, Markus Hofmarcher, Marius-Constantin Dinu, Matthias Dorfer, Patrick M. Blies, Johannes Brandstetter, Jose A. Arjona-Medina, and Sepp Hochreiter. Align-RUDER: Learning From Few Demonstrations by Reward Redistribution. _Proceedings of Machine Learning Research_, 162, 2022. arXiv: 2009.14108.
* Ferret et al. [2020] Johan Ferret, Raphael Marinier, Matthieu Geist, and Olivier Pietquin. Self-Attentional Credit Assignment for Transfer in Reinforcement Learning. In _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence_, pages 2655-2661, July 2020. arXiv:1907.08027 [cs].
* Peters et al. [2018] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of causal inference: foundations and learning algorithms_. Adaptive Computation and Machine Learning. November 2018.
* Buesing et al. [2019] Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, and Nicolas Heess. Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search. In _International Conference on Learning Representations_, 2019. arXiv: 1811.06272.

* Bica et al. [2020] Ioana Bica, Ahmed M. Alaa, James Jordon, and Mihaela van der Schaar. Estimating Counterfactual Treatment Outcomes over Time Through Adversarially Balanced Representations. In _International Conference on Learning Representations_. arXiv, February 2020. arXiv:2002.04083 [cs, stat].
* Zhang et al. [2021] Pushi Zhang, Li Zhao, Guoqing Liu, Jiang Bian, Minlie Huang, Tao Qin, and Tie-Yan Liu. Independence-aware Advantage Estimation. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence_, pages 3349-3355, Montreal, Canada, August 2021. International Joint Conferences on Artificial Intelligence Organization.
* Young [2020] Kenny Young. Variance Reduced Advantage Estimation with SdeltaS Hindsight Credit Assignment. _arXiv:1911.08362 [cs]_, September 2020. arXiv: 1911.08362.
* Ma and Pierre-Luc [2020] Michel Ma and Bacon Pierre-Luc. Counterfactual Policy Evaluation and the Conditional Monte Carlo Method. In _Offline Reinforcement Learning Workshop, NeurIPS_, 2020.
* Bratley et al. [1987] Paul Bratley, Bennett L. Fox, and Linus E. Schrage. _A Guide to Simulation_. Springer, New York, NY, 1987.
* Hammersley [1956] J. M. Hammersley. Conditional Monte Carlo. _Journal of the ACM_, 3(2):73-76, April 1956.
* Arumugam et al. [2021] Dilip Arumugam, Peter Henderson, and Pierre-Luc Bacon. An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning. _arXiv:2103.06224 [cs, math]_, March 2021. arXiv: 2103.06224.
* Young [2022] Kenny Young. Hindsight Network Credit Assignment: Efficient Credit Assignment in Networks of Discrete Stochastic Units. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(8):8919-8926, June 2022. Number: 8.
* Gu et al. [2017] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic. 2017.
* Thomas and Brunskill [2017] Philip S. Thomas and Emma Brunskill. Policy Gradient Methods for Reinforcement Learning with Function Approximation and Action-Dependent Baselines, June 2017. arXiv:1706.06643 [cs].
* Liu* et al. [2022] Hao Liu*, Yihao Feng*, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent Control Variates for Policy Optimization via Stein Identity. February 2022.
* Wu et al. [2022] Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M. Bayen, Sham Kakade, Igor Mordatch, and Pieter Abbeel. Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines. February 2022.
* Tucker et al. [2018] George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin Ghahramani, and Sergey Levine. The Mirage of Action-Dependent Baselines in Reinforcement Learning. In _Proceedings of the 35th International Conference on Machine Learning_, pages 5015-5024. PMLR, July 2018. ISSN: 2640-3498.
* Nota et al. [2021] Chris Nota, Philip Thomas, and Bruno C. Da Silva. Posterior Value Functions: Hindsight Baselines for Policy Gradient Methods. In _Proceedings of the 38th International Conference on Machine Learning_, pages 8238-8247. PMLR, July 2021. ISSN: 2640-3498.
* Guez et al. [2020] Arthur Guez, Fabio Viola, Theophane Weber, Lars Buesing, Steven Kapturowski, Doina Precup, David Silver, and Nicolas Heess. Value-driven Hindsight Modelling. _Advances in Neural Information Processing Systems_, 33, 2020.
* Venuto et al. [2022] David Venuto, Elaine Lau, Doina Precup, and Ofir Nachum. Policy Gradients Incorporating the Future. January 2022.
* Huang and Jiang [2020] Jiawei Huang and Nan Jiang. From Importance Sampling to Doubly Robust Policy Gradient. In _Proceedings of the 37th International Conference on Machine Learning_, pages 4434-4443. PMLR, November 2020. ISSN: 2640-3498.
* Ng et al. [1999] Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping. In _Proceedings of the Sixteenth International Conference on Machine Learning_, ICML '99, pages 278-287, San Francisco, CA, USA, June 1999. Morgan Kaufmann Publishers Inc.

* Schmidhuber [2010] Jurgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010). _IEEE Transactions on Autonomous Mental Development_, 2(3):230-247, September 2010.
* Bellemare et al. [2016] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying Count-Based Exploration and Intrinsic Motivation. In _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* Burda et al. [2018] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. December 2018.
* Marom and Rosman [2018] Ofir Marom and Benjamin Rosman. Belief Reward Shaping in Reinforcement Learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 32(1), April 2018. Number: 1.
* Memarian et al. [2021] Farzan Memarian, Wonjoon Goo, Rudolf Lioutikov, Scott Niekum, and Ufuk Topcu. Self-Supervised Online Reward Shaping in Sparse-Reward Environments, July 2021. arXiv:2103.04529 [cs].
* Suay et al. [2016] Halit Bener Suay, Tim Brys, Matthew E. Taylor, and Sonia Chernova. Learning from Demonstration for Shaping through Inverse Reinforcement Learning. In _Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems_, AAMAS '16, pages 429-437, Richland, SC, May 2016. International Foundation for Autonomous Agents and Multiagent Systems.
* Wu et al. [2020] Yuchen Wu, Melissa Mozifian, and Florian Shkurti. Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models, November 2020. arXiv:2011.01298 [cs].
* Schulman et al. [2015] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient Estimation Using Stochastic Computation Graphs. In _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* Ma et al. [2021] Michel Ma, Pierluca D'Oro, Yoshua Bengio, and Pierre-Luc Bacon. Long-Term Credit Assignment via Model-based Temporal Shortcuts. In _Deep RL Workshop NeurIPS_, October 2021.
* Ke et al. [2018] Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal, and Yoshua Bengio. Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems 31_, pages 7640-7651. Curran Associates, Inc., 2018.
* Schaul et al. [2015] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal Value Function Approximators. In _Proceedings of the 32nd International Conference on Machine Learning_, pages 1312-1320. PMLR, June 2015. ISSN: 1938-7228.
* Andrychowicz et al. [2017] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Rauber et al. [2018] Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, and Jurgen Schmidhuber. Hindsight policy gradients. December 2018.
* Goyal et al. [2019] Anirudh Goyal, Philemon Brakel, William Fedus, Soumye Singhal, Timothy Lillicrap, Sergey Levine, Hugo Larochelle, and Yoshua Bengio. Recall Traces: Backtracking Models for Efficient Reinforcement Learning. In _International Conference on Learning Representations_. arXiv, January 2019. arXiv:1804.00379 [cs, stat].
* Just Map Them to Actions, June 2020. arXiv:1912.02875 [cs].
* Srivastava et al. [2021] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Jaskowski, and Jurgen Schmidhuber. Training Agents using Upside-Down Reinforcement Learning, September 2021. arXiv:1912.02877 [cs].

* [78] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision Transformer: Reinforcement Learning via Sequence Modeling. In _Advances in Neural Information Processing Systems_, volume 34, pages 15084-15097. Curran Associates, Inc., 2021.
* [79] Michael Janner, Qiyang Li, and Sergey Levine. Offline Reinforcement Learning as One Big Sequence Modeling Problem. In _Advances in Neural Information Processing Systems_, volume 34, pages 1273-1286. Curran Associates, Inc., 2021.
* [80] Richard S. Sutton. Learning to predict by the methods of temporal differences. _Machine Learning_, 3(1):9-44, August 1988.
* [81] Hado van Hasselt, Sephora Madjiheurem, Matteo Hessel, David Silver, Andre Barreto, and Diana Borsa. Expected Eligibility Traces. In _Association for the Advancement of Artificial Intelligence_. arXiv, February 2021. arXiv:2007.01839 [cs, stat].
* [82] Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020.
* [83] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep Variational Information Bottleneck. 2017.
* [84] Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method, April 2000. arXiv:physics/0004057.
* [85] Matthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs. In _Association for the Advancement of Artificial Intelligence_. arXiv, 2015. arXiv:1507.06527 [cs].
* [86] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning Latent Dynamics for Planning from Pixels. In _Proceedings of the 36th International Conference on Machine Learning_, pages 2555-2565. PMLR, May 2019. ISSN: 2640-3498.
* [87] Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron van den Oord. Shaping Belief States with Generative Environment Models for RL. In _33rd Conference on Neural Information Processing Systems (NeurIPS 2019)_. arXiv, June 2019. Number: arXiv:1906.09237 arXiv:1906.09237 [cs, stat].
* [88] Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, and Theophane Weber. Temporal Difference Variational Auto-Encoder. 2019.
* [89] Matthijs T J Spaan. Partially Observable Markov Decision Processes. _Reinforcement Learning_, page 27.
* [90] K. J. Astrom. Optimal Control of Markov decision processes with incomplete state estimation. _J. Math. Anal. Applic._, 10:174-205, 1965.
* [91] Edward C. Tolman. Cognitive maps in rats and men. _Psychological Review_, 55:189-208, 1948. Place: US Publisher: American Psychological Association.
* [92] Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy. Fixing a Broken ELBO. In _Proceedings of the 35 th International Conference on Machine Learning,_. arXiv, February 2018. arXiv:1711.00464 [cs, stat].
* [93] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. July 2022.

* [94] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv, May 2014. Number: arXiv:1312.6114 arXiv:1312.6114 [cs, stat].
* [95] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018.
* [96] Lukas Biewald. Experiment Tracking with Weights and Biases, 2020.
* [97] Plotly Technologies Inc. Collaborative data science, 2015. Place: Montreal, QC Publisher: Plotly Technologies Inc.

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

these approaches rely on heuristic contribution analyses, such as via the access of memory states [4], linear decompositions of rewards [32, 35, 36, 37, 38, 39] or learned sequence models [5, 40, 41]. Leveraging our unbiased contribution analysis framework to reach more optimal reward transport is a promising direction for future research.

When we have access to a (learned) differentiable world model of the environment, we can achieve precise credit assignment by leveraging path-wise derivatives, i.e. backpropagating value gradients through the world model [20, 21, 22, 23, 69]. For stochastic world models, we need access to the noise variables to compute the path-wise derivatives. The Dreamer algorithms [21, 22, 23] approach this by computing the value gradients on _simulated_ trajectories, where the noise is known. The Stochastic Value Gradient (SVG) method [20] instead _infers_ the noise variables on real _observed_ trajectories. To enable backpropagating gradients over long time spans, Ma et al. [70] equip the learned recurrent world models of SVG with an attention mechanism, allowing the authors to leverage Sparse Attentive Backtracking [71] to transmit gradients through skip connections. Buesing et al. [43] leverages the insights from SVG in partially observable MDPs, using the inferred noise variables to estimate the effect of counterfactual policies on the expected return. Importantly, the path-wise derivatives leveraged by the above model-based credit assignment methods are not compatible with discrete action spaces, as sensitivities w.r.t. discrete actions are undefined. In contrast, COCOA can leverage model-based information for credit assignment, while being compatible with discrete actions.

Incorporating hindsight information has a wide variety of applications in RL. Goal-conditioned policies [72] use a goal state as additional input to the policy network or value function, thereby generalizing it to arbitrary goals. Hindsight Experience Replay [73] leverages hindsight reasoning to learn almost as much from undesired outcomes as from desired ones, as at hindsight, we can consider every final, possibly undesired state as a 'goal' state and update the value functions or policy network [74] accordingly. Goyal et al. [75] train an inverse environment model to simulate alternative past trajectories leading to the same rewarding state, hence leveraging hindsight reasoning to create a variety of highly rewarding trajectories. A recent line of work frames RL as a supervised sequence prediction problem, learning a policy conditioned on goal states or future returns [76, 77, 78, 79]. These models are trained on past trajectories or offline data, where we have in hindsight access to states and returns, considering them as targets for the learned policy.

Finally, Temporal Difference (TD) learning [80] has a rich history of leveraging proximity in time as a proxy for credit assignment [2]. TD(\(\lambda\)) [80] considers eligibility traces to trade off bias and variance, crediting past state-action pairs in the trajectory for the current reward proportional to how close they are in time. van Hasselt et al. [81] estimate expected eligibility traces, taking into account that the same rewarding state can be reached from various previous states. Hence, not only the past state-actions on the trajectory are credited and updated, but also counterfactual ones that lead to the same rewarding state. Extending the insights from COCOA towards temporal difference methods is an exciting direction for future research.

## Appendix B Undiscounted infinite-horizon MDPs

In this work, we consider an undiscounted MDP with a finite state space \(\mathcal{S}\), bounded rewards and an infinite horizon. To ensure that the expected return and value functions remain finite, we require some standard regularity conditions [2]. We assume the MDP contains an absorbing state \(s_{\infty}\) that transitions only to itself and has zero reward. Moreover, we assume _proper_ transition dynamics, meaning that an agent following any policy will eventually end up in the absorbing state \(s_{\infty}\) with probability one as time goes to infinity.

The discounted infinite horizon MDP formulation is a special case of this setting, with a specific class of transition dynamics. An explicit discounting of future rewards \(\sum_{k\geq 2}\gamma^{k}R_{k}\) with discount factor \(\gamma\in[0,1]\), is equivalent to considering the above undiscounted MDP setting, but modifying the state transition probability function \(p(S_{t+1}\mid S_{t},A_{t})\) such that each state-action pair has a fixed probability \((1-\gamma)\) of transitioning to the absorbing state [2]. Hence, all the results considered in this work can be readily applied to the discounted MDP setting, by modifying the environment transitions as outlined above. We can also explicitly incorporate temporal discounting in the policy gradients and contribution coefficients, which we outline in App. J.

The undiscounted infinite-horizon MDP with an absorbing state can also model episodic RL with a fixed time horizon. We can include time as an extra feature in the states \(S\), and then have a probability of 1 to transition to the absorbing state when the agent reaches the final time step.

heorems, proofs and additional information for Section 3

### Contribution coefficients, hindsight distribution and graphical models

Here, we provide more information on the derivation of the contribution coefficients of Eq. 3.

**Abstracting time.** In an undiscounted environment, it does not matter at which point in time the agent achieves the rewarding outcome \(u^{\prime}\). Hence, the contribution coefficients (3) sum over all future time steps, to reflect that the rewarding outcome \(u^{\prime}\) can be encountered at any future time, and to incorporate the possibility of encountering \(u^{\prime}\) multiple times. Note that when using temporal discounting, we can adjust the contribution coefficients accordingly (c.f. App J).

**Hindsight distribution.** To obtain the hindsight distribution \(p(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})\), we convert the classical graphical model of an MDP (c.f. Fig. 5(a)) into a graphical model that incorporates the time \(k\) into a separate node (c.f. Fig. 5(b)). Here, we rewrite \(p(U_{k}=u^{\prime}\mid S=s,A=a)\), the probability distribution of a rewarding outcome \(U_{k}\)\(k\) time steps later, as \(p(U^{\prime}=u^{\prime}\mid S=s,A=a,K=k)\). By giving \(K\) the geometric distribution \(p(K=k)=(1-\beta)\beta^{k-1}\) for some \(\beta\), we can rewrite the infinite sums used in the time-independent contribution coefficients as a marginalization over \(K\):

\[\sum_{k\geq 1}p(U_{k}=u^{\prime}\mid S=s,A=a) =\lim_{\beta\to 1}\sum_{k\geq 1}p(U_{k}=u^{\prime}\mid S=s,A=a) \beta^{k-1}\] (5) \[=\lim_{\beta\to 1}\frac{1}{1-\beta}\sum_{k\geq 1}p(U^{\prime}=u^{ \prime}\mid S=s,A=a,K=k)p(K=k)\] (6) \[=\lim_{\beta\to 1}\frac{1}{1-\beta}p(U^{\prime}=u^{\prime}\mid S=s,A=a)\] (7)

Note that this limit is finite, as for \(k\rightarrow\infty\), the probability of reaching an absorbing state \(s_{\infty}\) and corresponding rewarding outcome \(u_{\infty}\) goes to 1 (and we take \(u^{\prime}\neq u_{\infty}\)). Via Bayes rule, we then have that

\[w(s,a,u^{\prime}) =\frac{\sum_{k\geq 1}p(U_{k}=u^{\prime}\mid S=s,A=a)}{\sum_{k\geq 1 }p(U_{k}=u^{\prime}\mid S=s)}-1\] (8) \[=\frac{p(U^{\prime}=u^{\prime}\mid S=s,A=a)}{p(U^{\prime}=u\mid S =s)}-1=\frac{p(A=a\mid S=s,U^{\prime}=u^{\prime})}{\pi(a\mid s)}-1\] (9)

where we assume \(\pi(a\mid s)>0\) and where we dropped the limit of \(\beta\to 1\) in the notation, as we will always take this limit henceforth. Note that when \(\pi(a\mid s)=0\), the hindsight distribution \(p^{\pi}(a\mid s,u^{\prime})\) is also equal to zero, and hence the right-hand-side of the above equation is undefined. However, the middle term using \(p^{\pi}(U=u^{\prime}\mid S=s,A=a)\) is still well-defined, and hence we can use the contribution coefficients even for actions where \(\pi(a\mid s)=0\).

### Proof Theorem 1

We start with a lemma showing that the contribution coefficients can be used to estimate the advantage \(A^{\pi}(s,a)=Q^{\pi}(s,a)-V^{\pi}(s)\), by generalizing Theorem 1 of Harutyunyan et al. [1] towards rewarding outcomes.

**Definition 3** (Fully predictive, repeated from main text).: A rewarding outcome encoding \(U\) is fully predictive of the reward \(R\), if the following conditional independence condition holds:

Figure 6: (a) Graphical model of the MDP. (b) Graphical model of the MDP, where we abstracted time.

\(S_{0}=s,A_{0}=a,U_{k}=u)=p^{\pi}(R=r\mid U=u)\), where the right-hand side does not depend on the time \(k\).

**Lemma 6**.: _For each state-action pair \((s,a)\) with \(\pi(a\mid s)>0\), and assuming that \(u=f(s,a,r)\) is fully predictive of the reward (c.f. Definition 3), we have that_

\[A^{\pi}(s,a)=r(s,a)-\sum_{a^{\prime}\in\mathcal{A}}\pi(a\mid s)r(s,a)+\mathbb{E }_{T\sim\mathcal{T}(s,\pi)}\left[\sum_{k\geq 1}w(s,a,U_{k})R_{k}\right]\] (10)

_with the advantage function \(A^{\pi}\), and the reward function \(r(s,a)\triangleq\mathbb{E}[R\mid s,a]\)._

Proof.: We rewrite the undiscounted state-action value function in the limit of a discounting factor \(\beta\to 1_{-}\) (the minus sign indicating that we approach 1 from the left):

\[Q(s,a) =\lim_{\beta\to 1_{-}}\mathbb{E}_{T\sim\mathcal{T}(s,a,\pi)} \left[\sum_{k\geq 1}\beta^{k}R_{k}\right]\] (11) \[=r(s,a)+\lim_{\beta\to 1_{-}}\sum_{r\in\mathcal{R}}\sum_{k \geq 1}\beta^{k}p^{\pi}(R_{k}=r\mid s,a)r\] (12) \[=r(s,a)+\lim_{\beta\to 1_{-}}\sum_{r\in\mathcal{R}}\sum_{u\in U }\sum_{k\geq 1}\beta^{k}p^{\pi}(R_{k}=r,U_{k}=u\mid s,a)r\] (13) \[=r(s,a)+\lim_{\beta\to 1_{-}}\sum_{r\in\mathcal{R}}\sum_{u\in U }\sum_{k\geq 1}\beta^{k}p^{\pi}(R=r\mid U=u)rp^{\pi}(U_{k}=u\mid s,a)\] (14) \[=r(s,a)+\lim_{\beta\to 1_{-}}\sum_{u\in\mathcal{U}}r(u)\sum_{k \geq 1}\beta^{k}p^{\pi}(U_{k}=u\mid s,a)\] (15) \[=r(s,a)+\lim_{\beta\to 1_{-}}\sum_{u\in\mathcal{U}}r(u)\sum_{k \geq 1}\beta^{k}p^{\pi}(U_{k}=u\mid s)\frac{\sum_{k^{\prime}\geq 1}\beta^{k^{ \prime}}p^{\pi}(U_{k^{\prime}}=u\mid s,a)}{\sum_{k^{\prime}\geq 1}\beta^{k^{ \prime}}p^{\pi}(U_{k^{\prime}}=u\mid s)}\] (16)

where we use the property that \(u\) is _fully predictive_ of the reward (c.f. Definition 2), and where we define \(r(s,a)\triangleq\mathbb{E}[R\mid s,a]\) and \(r(u)\triangleq\mathbb{E}[R\mid u]\) Using the graphical model of Fig. 6b where we abstract time \(k\) (c.f. App. C.1), we have that \(\frac{1}{1-\beta}p_{\beta}^{\pi}(R=r^{\prime}\mid s,a)=\sum_{k=0}\beta^{k}p^ {\pi}(R_{k}=r\mid s,a)\). Leveraging Bayes rule, we get

\[p^{\pi}(a\mid s,U^{\prime}=u)=\lim_{\beta\to 1_{-}}\frac{p_{\beta}^{\pi}(U^{ \prime}=u\mid s,a)\pi(a\mid s)}{p_{\beta}^{\pi}(U^{\prime}=u\mid s)}=\lim_{ \beta\to 1_{-}}\frac{\sum_{k\geq 1}\beta^{k}p^{\pi}(U_{k}=u\mid s,a)}{\sum_{k \geq 1}\beta^{k}p^{\pi}(U_{k}=u\mid s)}\pi(a\mid s)\] (17)

Where we dropped the limit of \(\beta\to 1_{-}\) in the notation of \(p^{\pi}(a\mid s,R=r)\), as we henceforth always consider this limit. Taking everything together, we have that

\[Q^{\pi}(s,a) =r(s,a)+\sum_{u\in\mathcal{U}}\sum_{k\geq 1}p^{\pi}(U_{k}=u\mid s) \frac{p^{\pi}(a\mid s,U^{\prime}=u)}{\pi(a\mid s)}r(u)\] (18) \[=r(s,a)+\sum_{u\in\mathcal{U}}\sum_{k\geq 1}p^{\pi}(U_{k}=u\mid s )(w(s,a,u)+1)r(u)\] (19) \[=r(s,a)+\sum_{u\in\mathcal{U}}\sum_{k\geq 1}p^{\pi}(U_{k}=u\mid s )(w(s,a,u)+1)\sum_{r\in\mathcal{R}}p(R_{k}=r\mid U_{k}=u)r\] (20) \[=r(s,a)+\mathbb{E}_{T\sim\mathcal{T}(s,\pi)}\left[\sum_{k\geq 1}(w (s,a,U_{k})+1)R_{k}\right]\] (21)

Subtracting the value function, we get

\[A^{\pi}(s,a)=r(s,a)-\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s)r(s,a^{ \prime})+\mathbb{E}_{T\sim\mathcal{T}(s,\pi)}\left[\sum_{k\geq 1}w(s,a,U_{k})R_{k}\right]\] (22)Now we are ready to prove Theorem 1.

Proof.: Using the policy gradient theorem [26], we have

\[\nabla_{\theta}V^{\pi}(s_{0}) =\mathbb{E}_{T\sim\mathcal{T}(s_{0},\pi)}\left[\sum_{t\geq 0}\sum_{a \in\mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})A^{\pi}(S_{t},a)\right]\] (23) \[=\mathbb{E}_{T\sim\mathcal{T}(s_{0},\pi)}\left[\sum_{t\geq 0}\sum_{a \in\mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})\left[r(S_{t},a)+\sum_{k\geq 1 }w(S_{t},a,U_{t+k})R_{t+k}\right]\right]\] (24) \[=\mathbb{E}_{T\sim\mathcal{T}(s_{0},\pi)}\left[\sum_{t\geq 0} \nabla_{\theta}\log\pi(A_{t}\mid S_{t})R_{t}+\sum_{a\in\mathcal{A}}\nabla_{ \theta}\pi(a\mid S_{t})\sum_{k\geq 1}w(S_{t},a,U_{t+k})R_{t+k}\right]\] (25)

where we used that removing a baseline \(\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s)r(s,a^{\prime})\) independent from the actions does not change the policy gradient, and replaced \(\mathbb{E}_{T\sim\mathcal{T}(s_{0},\pi)}[\sum_{a\in\mathcal{A}}\nabla_{\theta} \pi(a\mid S_{t})r(S_{t},a)]\) by its sampled version \(\mathbb{E}_{T\sim\mathcal{T}(s_{0},\pi)}[\nabla_{\theta}\log\pi(A_{t}\mid S_ {t})R_{t}]\). Hence the policy gradient estimator of Eq. 4 is unbiased. 

### Different policy gradient estimators leveraging contribution coefficients.

In this work, we use the COCOA estimator of Eq. 4 to estimate policy gradients leveraging the contribution coefficients of Eq. 3, as this estimator does not need a separate reward model, and it works well for the small action spaces we considered in our experiments. However, one can design other unbiased policy gradient estimators compatible with the same contribution coefficients.

Harutyunyan et al. [1] introduced the HCA gradient estimator of Eq. 2, which can readily be extended to rewarding outcomes \(U\):

\[\hat{\nabla}_{\theta}V^{\pi}=\sum_{t\geq 0}\sum_{a\in\mathcal{A}}\nabla_{ \theta}\pi(a\mid S_{t})\Big{(}r(S_{t},a)+\sum_{k\geq 1}w(S_{t},a,U_{t+k})R_{t+k} \Big{)}\] (26)

This gradient estimator uses a reward model \(r(s,a)\) to obtain the rewards corresponding to counterfactual actions, whereas the COCOA estimator (4) only uses the observed rewards \(R_{k}\).

For large action spaces, it might become computationally intractable to sum over all possible actions. In this case, we can sample independent actions from the policy, instead of summing over all actions, leading to the following policy gradient.

\[\hat{\nabla}_{\theta}V^{\pi}=\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S _{t})R_{t}+\frac{1}{M}\sum_{m}\nabla_{\theta}\log\pi(a^{m}\mid S_{t})\sum_{k =1}^{\infty}w(S_{t},a^{m},U_{t+k})R_{t+k}\] (27)

where we sample \(M\) actions from \(a^{m}\sim\pi(\cdot\mid S_{t})\). Importantly, for obtaining an unbiased policy gradient estimate, the actions \(a^{m}\) should be sampled independently from the actions used in the trajectory of the observed rewards \(R_{t+k}\). Hence, we cannot take the observed \(A_{t}\) as a sample \(a^{m}\), but need to instead sample independently from the policy. This policy gradient estimator can be used for continuous action spaces (c.f. App. G, and can also be combined with the reward model as in Eq. 26.

One can show that the above policy gradient estimators are unbiased with a proof akin to the one of Theorem 1.

Comparison of COCOA to using time as a credit assignment heuristic.In Section 2 we discussed briefly the following discounted policy gradient:

\[\hat{\nabla}_{\theta}^{\text{REINFORCE},\gamma}V^{\pi}(s_{0})=\sum\nolimits _{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\sum\nolimits_{k\geq t}\gamma ^{k-t}R_{k}\] (28)

with discount factor \(\gamma\in[0,1]\). Note that we specifically put no discounting \(\gamma^{t}\) in front of \(\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\), which would be required for being an unbiased gradient estimate of the discounted expected total return, as the above formulation is most used in practice [12; 27]. Rewritingthe summations reveals that this policy gradient uses time as a heuristic for credit assignment:

\[\hat{\nabla}_{\theta}^{\text{REINFORCE},\gamma}V^{\pi}(s_{0})=\sum\nolimits_{t \geq 0}R_{t}\sum\nolimits_{k\leq t}\gamma^{t-k}\nabla_{\theta}\log\pi(A_{k} \mid S_{k}).\] (29)

We can rewrite the COCOA gradient estimator (4) with the same reordering of summation, showcasing that it leverages the contribution coefficient for providing precise credit to past actions, instead of using the time discounting heuristic:

\[\hat{\nabla}_{\theta}^{U}V^{\pi}(s_{0})=\sum\limits_{t\geq 0}R_{t}\left[\nabla_{ \theta}\log\pi(A_{t}\mid S_{t})+\sum\limits_{k<t}\sum\limits_{a\in\mathcal{A} }w(S_{k},a,U_{t})\nabla_{\theta}\pi(a\mid S_{k})\right]\] (30)

### Proof of Proposition 2

Proof.: Proposition 2 assumes that each action sequence \(\{A_{m}\}_{m=t}^{t+k}\) leads to a unique state \(s^{\prime}\). Hence, all previous actions can be decoded perfectly from the state \(s^{\prime}\), leading to \(p^{\pi}(A_{t}=a\mid S_{t},S^{\prime}=s^{\prime})=\delta(a=a_{t})\), with \(\delta\) the indicator function and \(a_{t}\) the action taken in the trajectory that led to \(s^{\prime}\). Filling this into the COCOA gradient estimator leads to

\[\hat{\nabla}_{\theta}^{S}V^{\pi}(s_{0}) =\sum\limits_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})R_{t}+ \sum\limits_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})\sum\limits_{k \geq 1}\left(\frac{\delta(a=A_{t})}{\pi(a\mid S_{t})}-1\right)R_{t+k}\] (31) \[=\sum\limits_{t\geq 0}\frac{\nabla_{\theta}\pi(A_{t}\mid S_{t})} {\pi(A_{t}\mid S_{t})}\sum\limits_{k\geq 0}R_{t+k}\] (32) \[=\sum\limits_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t}) \sum\limits_{k\geq 0}R_{t+k}\] (33)

where we used that \(\sum_{a}\nabla_{\theta}\pi(a\mid s)=0\). 

### Proof Theorem 3

Proof.: Theorem 3 considers the case where the environment only contains a reward at the final time step \(t=T\), and where we optimize the policy only on a single (initial) time step \(t=0\). Then, the policy gradients are given by

\[\hat{\nabla}_{\theta}^{U}V^{\pi}(U_{T},R_{T}) =\sum\limits_{a}\nabla_{\theta}\pi(a\mid s)w(s,a,U_{T})R_{T}\] (34) \[\hat{\nabla}_{\theta}^{\text{REINFORCE}}V^{\pi}(A_{0},R_{T}) =\nabla_{\theta}\log\pi(A_{0}\mid s)R_{T}\] (35)

With \(U\) either \(S\), \(R\) or \(Z\), and \(s\) the state at \(t=0\). As only the last time step can have a reward by assumption, and the encoding \(U\) needs to retain the predictive information of the reward, the contribution coefficients for \(u\) corresponding to a nonzero reward are equal to

\[w(s,a,u)=\frac{p^{\pi}(A_{0}=a\mid S_{0}=s,U_{T}=u)}{\pi(a\mid s)}-1\] (36)

The coefficients corresponding to zero-reward outcomes are multiplied with zero in the gradient estimator, and can hence be ignored. Now, we proceed by showing that \(\mathbb{E}[\hat{\nabla}_{\theta}^{\text{REINFORCE}}V^{\pi}(A_{0},R_{T})\mid S _{T},R_{T}]=\hat{\nabla}_{\theta}^{S}V^{\pi}(S_{T},R_{T})\), \(\mathbb{E}[\hat{\nabla}_{\theta}^{S}V^{\pi}(S_{T},R_{T})\mid U_{T}^{\prime},R _{T}]=\hat{\nabla}_{\theta}^{U^{\prime}}V^{\pi}(U_{T}^{\prime},R_{T})\), \(\mathbb{E}[\hat{\nabla}_{\theta}^{U^{\prime}}V^{\pi}(U_{T}^{\prime},R_{T}) \mid U_{T},R_{T}]=\hat{\nabla}_{\theta}^{U}V^{\pi}(U_{T},R_{T})\), and \(\mathbb{E}[\hat{\nabla}_{\theta}^{U}V^{\pi}(U_{T},R_{T})\mid R_{T}]=\hat{ \nabla}_{\theta}^{U}V^{\pi}(R_{T})\), after which we can use the law of total variance to prove our theorem.

As \(S_{T}\) is fully predictive of \(R_{T}\), the following conditional independence holds \(p^{\pi}(A_{0}\mid S_{0}=s,S_{T},R_{T})=p^{\pi}(A_{0}\mid S_{0}=s,S_{T})\) (c.f. Definition 2). Hence, we have that

\[\mathbb{E}[\hat{\nabla}_{\theta}^{\text{REINFORCE}}V^{\pi}(A_{0},R_{T})\mid S _{T},R_{T}]\] (37) \[=\sum\limits_{a\in\mathcal{A}}p(A_{0}=a\mid S_{0}=s,S_{T})\frac{ \nabla_{\theta}\pi(a\mid s)}{\pi(a\mid s)}R_{T}=\hat{\nabla}_{\theta}^{S}V^{ \pi}(S_{T},R_{T})\] (38)

where we used that \(\sum_{a}\nabla_{\theta}\pi(a\mid s)=\nabla_{\theta}\sum_{a}\pi(a\mid s)=0\).

Similarly, as \(S_{T}\) is fully predictive of \(U^{\prime}_{T}\) (c.f. Definition 2), we have that

\[p(A\mid S,U^{\prime}_{T}) =\sum_{s_{T}\in\mathcal{S}}p(S_{T}=s_{T}\mid S,U^{\prime}_{T})p(A \mid S,S_{T}=s_{T},U^{\prime}_{T})\] (39) \[=\sum_{s_{T}\in\mathcal{S}}p(S_{T}=s_{T}\mid S,U^{\prime}_{T})p(A \mid S,S_{T}=s_{T})\] (40)

Using the conditional independence relation \(p^{\pi}(S_{T}\mid S_{0},U^{\prime}_{T},R_{T})=p^{\pi}(S_{T}\mid S_{0},U^{ \prime}_{T})\), following from d-separation in Fig. 6b, this leads us to

\[\mathbb{E}[\hat{\nabla}^{S}_{\theta}V^{\pi}(S_{T},R_{T})\mid U^{ \prime}_{T},R_{T}]\] (41) \[=\sum_{a\in\mathcal{A}}\sum_{s_{T}\in\mathcal{S}}p(S_{T}=s_{T} \mid S_{0}=s,U^{\prime}_{T})p(A_{0}=a\mid S=s,S_{T}=s_{T})\frac{\nabla_{ \theta}\pi(a\mid s)}{\pi(a\mid s)}R_{T}\] (42) \[=\sum_{a\in\mathcal{A}}p(A_{0}=a\mid S=s,U^{\prime}_{T})\frac{ \nabla_{\theta}\pi(a\mid s)}{\pi(a\mid s)}R_{T}\] (43) \[=\hat{\nabla}^{U^{\prime}}_{\theta}V^{\pi}(U^{\prime}_{T},R_{T})\] (44)

Using the same derivation leveraging the fully predictive properties (c.f. Definition 2), we get

\[\mathbb{E}[\hat{\nabla}^{U^{\prime}}_{\theta}V^{\pi}(U^{\prime}_{ T},R_{T})\mid U_{T},R_{T}] =\hat{\nabla}^{U}_{\theta}V^{\pi}(U_{T},R_{T})\] (45) \[\mathbb{E}[\hat{\nabla}^{U}_{\theta}V^{\pi}(U_{T},R_{T})\mid R_{T}] =\hat{\nabla}^{R}_{\theta}V^{\pi}(R_{T})\] (46)

Now we can use the law of total variance, which states that \(\mathbb{V}[X]=\mathbb{E}[\mathbb{V}[X\mid Y]]+\mathbb{V}[\mathbb{E}[X\mid Y]]\). Hence, we have that

\[\mathbb{V}[\hat{\nabla}^{\text{REINFORCE}}_{\theta}V^{\pi}] =\mathbb{V}[\mathbb{E}[\hat{\nabla}^{\text{REINFORCE}}_{\theta}V ^{\pi}\mid S_{T}]]+\mathbb{E}[\mathbb{V}[\hat{\nabla}^{\text{REINFORCE}}_{ \theta}V^{\pi}\mid S_{T}]]\] (47) \[=\mathbb{V}[\hat{\nabla}^{S}_{\theta}V^{\pi}]+\mathbb{E}[\mathbb{ V}[\hat{\nabla}^{\text{REINFORCE}}_{\theta}V^{\pi}\mid S_{T}]]\succcurlyeq\mathbb{V}[ \hat{\nabla}^{S}_{\theta}V^{\pi}]\] (48)

as \(\mathbb{E}[\mathbb{V}[\hat{\nabla}^{\text{REINFORCE}}_{\theta}V^{\pi}\mid S_{T }]]\) is positive semi definite. Using the same construction for the other pairs, we arrive at

\[\mathbb{V}[\hat{\nabla}^{R}_{\theta}V^{\pi}(s_{0})]\preccurlyeq\mathbb{V}[ \hat{\nabla}^{U}_{\theta}V^{\pi}(s_{0})]\preccurlyeq\mathbb{V}[\hat{\nabla}^{U ^{\prime}}_{\theta}V^{\pi}(s_{0})]\preccurlyeq\mathbb{V}[\hat{\nabla}^{S}_{ \theta}V^{\pi}(s_{0})]\preccurlyeq\mathbb{V}[\hat{\nabla}^{\text{REINFORCE}}_{ \theta}V^{\pi}(s_{0})]\] (49)

thereby concluding the proof. 

Additional empirical verification.To get more insight into how the information content of the rewarding-outcome encoding \(U\) relates to the variance of the COCOA gradient estimator, we repeat the experiment of Fig. 2, but now plot the variance as a function of the amount of information in the rewarding outcome encodings, for a fixed state overlap of 3. Fig. 7 shows that the variance of the resulting COCOA gradient estimators interpolate between COCOA-reward and HCA+, with more informative encodings leading to a higher variance. These empirical results show that the insights of Theorem 3 hold in this more general setting of estimating full policy gradients in an MDP with random rewards.

### Proof of Theorem 4

Proof.: This proof follows a similar technique to the policy gradient theorem [26]. Let us first define the expected number of occurrences of \(u\) starting from state \(s\) as \(O^{\pi}(u,s)=\sum_{k\geq 1}p^{\pi}(U_{k}=u\mid S_{0}=s)\), and its action equivalent \(O^{\pi}(u,s,a)=\sum_{k\geq 1}p^{\pi}(U_{k}=u\mid S_{0}=s,A_{0}=a)\). Expanding \(O^{\pi}(u,s)\) leads to

\[\nabla_{\theta}O^{\pi}(u,s) =\nabla_{\theta}\left[\sum_{a\in\mathcal{A}}\pi(a\mid s)\sum_{s^ {\prime}\in\mathcal{S}}p(s^{\prime}\mid s,a)\big{(}\delta(s^{\prime}=s)+O^{ \pi}(u,s^{\prime})\big{)}\right]\] (50) \[=\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid s)O^{\pi}(u,s,a)+ \sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}\mid s)\nabla_{\theta}O^{\pi}(u,s^{ \prime})\] (51)Now define \(\phi(u,s)=\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid s)O^{\pi}(u,s,a)\), and \(p^{\pi}(S_{l}=s^{\prime}\mid S_{0}=s)\) as the probability of reaching \(s^{\prime}\) starting from \(s\) in \(l\) steps. We have that \(\sum_{s^{\prime\prime}\in\mathcal{S}}p^{\pi}(S_{l}=s^{\prime\prime}\mid S_{0} =s)p^{\pi}(S_{1}=s^{\prime}\mid S_{0}=s^{\prime\prime})=p^{\pi}(S_{l+1}=s^{ \prime}\mid S_{0}=s)\). Leveraging this relation and recursively applying the above equation leads to

\[\nabla_{\theta}O^{\pi}(u,s) =\sum_{s^{\prime}\in\mathcal{S}}\sum_{l=0}^{\infty}p^{\pi}(S_{l}= s^{\prime}\mid S_{0}=s)\phi(u,s^{\prime})\] (52) \[=\sum_{s^{\prime}\in\mathcal{S}}\sum_{l=0}^{\infty}p^{\pi}(S_{l}= s^{\prime}\mid S_{0}=s)\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid s^{ \prime})O^{\pi}(u,s^{\prime},a)\] (53) \[\propto\mathbb{E}_{S\sim\mathcal{T}(s,\pi)}\sum_{a\in\mathcal{A} }\nabla_{\theta}\pi(a\mid S)O^{\pi}(u,S,a)\] (54)

where in the last step we normalized \(\sum_{l=0}^{\infty}p^{\pi}(S_{l}=s^{\prime}\mid S_{0}=s)\) with \(\sum_{s^{\prime}\in\mathcal{S}}\sum_{l=0}^{\infty}p^{\pi}(S_{l}=s^{\prime}\mid S _{0}=s)\), resulting in the state distribution \(S\sim\mathcal{T}(s,\pi)\) where \(S\) is sampled from trajectories starting from \(s\) and following policy \(\pi\).

Finally, we can rewrite the contribution coefficients as

\[w(s,a,u)=\frac{O^{\pi}(u,s,a)}{O^{\pi}(u,s)}-1\] (55)

which leads to

\[\nabla_{\theta}O^{\pi}(u,s)\propto\mathbb{E}_{S\sim\mathcal{T}(s,\pi)}\sum_{a \in\mathcal{A}}\nabla_{\theta}\pi(a\mid S)w(S,a,u)O^{\pi}(u,S)\] (56)

where we used that \(\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid s)=0\), thereby concluding the proof. 

## Appendix D Learning the contribution coefficients

### Proof Proposition 5

Proof.: In short, as the logits \(l\) are a deterministic function of the state \(s\), they do not provide any further information on the hindsight distribution and we have that \(p^{\pi}(A_{0}=a\mid S_{0}=s,L_{0}=l,U^{\prime}=u^{\prime})=p^{\pi}(A_{0}=a \mid S_{0}=s,U^{\prime}=u^{\prime})\).

Figure 7: **Less informative rewarding outcome encodings lead to gradient estimators with lower variance.** Normalized variance in dB using ground-truth coefficients and a random uniform policy, for various gradient estimators on the tree environment (shaded region represents standard error over 10 random environments). To increase the information content of \(U\), we increase the number of different encodings \(u\) corresponding to the same reward value (c.f. \(n_{g}\) in Section E.6), indicated by the x-axis. COCOA-u-group indicates the COCOA estimator with rewarding outcome encodings of increasing information content, whereas COCOA-reward and HCA+ have fixed rewarding outcome encodings of \(U=R\) and \(U=S\) respectively.

We can arrive at this result by observing that \(p(A_{0}=a\mid S_{0}=s,L_{0}=l)=p(A_{0}=a\mid S_{0}=s)=\pi(a\mid s)\), as the policy logits encode the policy distribution. Using this, we have that

\[p^{\pi}(A_{0}=a\mid S_{0}=s,L_{0}=l,U^{\prime}=u^{\prime})\] (57) \[=\frac{p^{\pi}(A_{0}=a,U^{\prime}=u^{\prime}\mid S_{0}=s,L_{0}=l) }{p^{\pi}(U^{\prime}=u^{\prime}\mid S_{0}=s,L_{0}=l)}\] (58) \[=\frac{p^{\pi}(U^{\prime}=u^{\prime}\mid S_{0}=s,A_{0}=a,L_{0}=l )p(A_{0}=a\mid S_{0}=s,L_{0}=l)}{\sum_{a^{\prime}\in\mathcal{A}}p^{\pi}(A_{0}= a^{\prime},U^{\prime}=u^{\prime}\mid S_{0}=s,L_{0}=l)}\] (59) \[=\frac{p^{\pi}(U^{\prime}=u^{\prime}\mid S_{0}=s,A_{0}=a)p(A_{0} =a\mid S_{0}=s)}{\sum_{a^{\prime}\in\mathcal{A}}p^{\pi}(U^{\prime}=u^{\prime }\mid S_{0}=s,A_{0}=a^{\prime},)p(A_{0}=a^{\prime}\mid S_{0}=s)}\] (60) \[=\frac{p^{\pi}(A_{0}=a,U^{\prime}=u^{\prime}\mid S_{0}=s)}{p^{\pi }(U^{\prime}=u^{\prime}\mid S_{0}=s)}\] (61) \[=p^{\pi}(A_{0}=a\mid S_{0}=s,U^{\prime}=u^{\prime})\] (62)

### Learning contribution coefficients via contrastive classification.

Instead of learning the hindsight distribution, we can estimate the probability ratio \(p^{\pi}(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})/\pi(a\mid s)\) directly, by leveraging contrastive classification. Proposition 7 shows that by training a binary classifier \(D(a,s,u^{\prime})\) to distinguish actions sampled from \(p^{\pi}(A_{t}=a\mid S_{t}=s,R^{\prime}=r^{\prime})\) versus \(\pi(a\mid s)\), we can directly estimate the probability ratio.

**Proposition 7**.: _Consider the contrastive loss_

\[L=\mathbb{E}_{s,u^{\prime}\sim\mathcal{T}(s_{0},\pi)}\left[\mathbb{E}_{a\sim p ^{\pi}(a\mid s,u^{\prime})}\left[\log D(a,s,u^{\prime})\right]+\mathbb{E}_{a \sim\pi(a\mid s)}\left[\log\left(1-D(a,s,u^{\prime})\right)\right]\right],\] (63)

_and \(D^{*}(s,a,u^{\prime})\) its minimizer. Then the following holds_

\[w(s,a,u^{\prime})=\frac{D^{*}(a,s,u^{\prime})}{1-D^{*}(a,s,u^{\prime})}-1.\] (64)

Proof.: Consider a fixed pair \(s,u^{\prime}\). We can obtain the discriminator \(D^{*}(a,s,u^{\prime})\) that maximizes \(\mathbb{E}_{a\sim p^{\pi}(a\mid s,u^{\prime})}\left[\log D(a,s,u^{\prime}) \right]+\mathbb{E}_{a\sim\pi(a\mid s)}\left[\log\left(1-D(a,s,u^{\prime}) \right)\right]\) by taking the point-wise derivative of this objective and equating it to zero:

\[p^{\pi}(a\mid s,u^{\prime})\frac{1}{D^{*}(a,s,u^{\prime})}-\pi(a \mid s)\frac{1}{1-D^{*}(a,s,u^{\prime})}=0\] (65) \[\Rightarrow \frac{p^{\pi}(a\mid s,u^{\prime})}{\pi(a\mid s)}=\frac{D^{*}(a,s, u^{\prime})}{1-D^{*}(a,s,u^{\prime})}\] (66)

As for any \((a,b)\in\mathbb{R}_{/0}^{2}\), the function \(f(x)=a\log x+b\log(1-x)\) achieves its global maximum on support \(x\in[0,1]\) at \(\frac{a}{a+b}\), the above maximum is a global maximum. Repeating this argument for all \((s,u^{\prime})\) pairs concludes the proof. 

We can approximate the training objective of \(D\) by sampling \(a^{(m)},s^{(m)},u^{\prime(m)}\) along the observed trajectories, while leveraging that we have access to the policy \(\pi\), leading to the following loss

\[L=\sum_{m=1}^{M}\left[-\log D(a^{(m)},s^{(m)},u^{\prime(m)})-\sum_{a^{\prime} \in\mathcal{A}}\pi(a^{\prime}\mid s^{(m)})\log\left(1-D(a^{\prime},s^{(m)},u ^{\prime(m)})\right)\right]\] (67)

Numerical stability.Assuming \(D\) uses the sigmoid nonlinearity on its outputs, we can improve the numerical stability by computing the logarithm and sigmoid jointly. This results in

\[\log\sigma(x)=-\log(1+\exp(-x))\] (68) \[\log(1-\sigma(x))=-x-\log(1+\exp(-x))\] (69)

The probability ratio \(D/(1-D)\) can then be computed with

\[\frac{\sigma(x)}{1-\sigma(x)}=\exp\big{[}-\log(1+\exp(-x))+x+\log(1+\exp(-x)) \big{]}=\exp(x)\] (70)

### Successor representations.

If we take \(u^{\prime}\) equal to the state \(s^{\prime}\), we can observe that the sum \(\sum_{k\geq 1}p^{\pi}(S_{t+k}=s^{\prime}\mid s,a)\) used in the contribution coefficients (3) is equal to the successor representation \(M(s,a,s^{\prime})\) introduced by Dayan [30]. Hence, we can leverage temporal differences to learn \(M(s,a,s^{\prime})\), either in a tabular setting [30], or in a deep feature learning setting [31]. Using the successor representations, we can construct the state-based contribution coefficients as \(w(s,a,s^{\prime})=M(s,a,s^{\prime})/[\sum_{\tilde{a}\in\mathcal{A}}\pi(\tilde {a}\mid s)M(s,\tilde{a},s^{\prime})]-1\).

For a rewarding outcome encoding \(U\) different from \(S\), we can recombine the state-based successor representations to obtain the required contribution coefficients using the following derivation.

\[\sum_{k\geq 0}p^{\pi}(U_{k}=u^{\prime}\mid S_{0}=s,A_{0}=a) =\sum_{k\geq 0}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(U_{k}=u^{ \prime},S_{k}=s^{\prime}\mid S_{0}=s,A_{0}=a)\] (71) \[=\sum_{k\geq 0}\sum_{s^{\prime}\in\mathcal{S}}p(U^{\prime}=u^{ \prime}\mid S^{\prime}=s^{\prime})p^{\pi}(S_{k}=s^{\prime}\mid S_{0}=s,A_{0}=a)\] (72) \[=\sum_{s^{\prime}\in\mathcal{S}}p(U^{\prime}=u^{\prime}\mid S^{ \prime}=s^{\prime})\sum_{k\geq 0}p^{\pi}(S_{k}=s^{\prime}\mid S_{0}=s,A_{0}=a)\] (73) \[=\sum_{s^{\prime}\in\mathcal{S}}p(U^{\prime}=u^{\prime}\mid S^{ \prime}=s^{\prime})M(s,a,s^{\prime})\] (74)

where in the second line we used that \(S\) is fully predictive of \(U\) (c.f. Definition 2). Note that \(p(U^{\prime}\mid S^{\prime})\) is policy independent, and can hence be approximated efficiently using offline data. We use this recombination of successor representations in our dynamic programming setup to compute the ground-truth contribution coefficients (c.f. App. E).

## Appendix E Experimental details and additional results

Here, we provide additional details to all experiments performed in the manuscript and present additional results.

### Algorithms

Algorithm 1 shows pseudocode for the COCOA family of algorithms.

### Dynamic programming setup

In order to delineate the different policy gradient methods considered in this work, we develop a framework to compute ground-truth policy gradients and advantages as well as ground-truth contribution coefficients. We will first show how we can recursively compute a quantity closely related to the successor representation using dynamic programming which then allows us to obtain expected advantages and finally expected policy gradients.

#### e.2.1 Computing ground truth quantities

To compute ground truth quantities, we assume that the environment reaches an absorbing, terminal state \(s_{\infty}\) after \(T\) steps for all states \(s\), i.e. \(p^{\pi}(S_{T}=s_{\infty}|s_{0}=s)=1\). This assumption is satisfied by the linear key-to-door and tree environment we consider. As a helper quantity, we define the successor representation:

\[M(s,a,s^{\prime},T)=\sum_{k=1}^{T}p^{\pi}(S_{k}=s^{\prime}|S_{0}=s,A_{0}=a)\] (75)

which captures the cumulative probability over all time steps of reaching state \(s^{\prime}\) when starting from state \(S_{0}=s\), choosing initial action \(A_{0}=a\) and following the policy \(\pi\) thereafter. We can compute \(M(s,a,s^{\prime},T)\) recursively as

\[M(s,a,s^{\prime},t)=\sum_{s^{\prime\prime}\in\mathcal{S}}p(S_{1}=s^{\prime \prime}|S_{0}=s,A_{0}=a)\sum_{a^{\prime\prime}\in\mathcal{A}}\pi(a^{\prime \prime}|s^{\prime\prime})(\mathbb{1}_{s^{\prime\prime}=s^{\prime}}+M(s^{ \prime\prime},a^{\prime\prime},s^{\prime},t-1))\] (76)

where \(\mathbb{1}\) is the indicator function and \(M(s,a,s^{\prime},0)\) is initialized to \(0\) everywhere. Then, the various quantities can be computed exactly as follows.

**Algorithm 1** COCOA

```
0: Initial \(\pi\), \(h\), episode length \(T\), number of episodes \(N\), number of pretraining episodes \(M\), batch size \(L\), \(K\) number of pretraining update steps.
1:for\(i=1\) to \(M\)do\(\triangleright\) Collect random trajectories
2: Sample \(L\) trajectories \(\tau=\{\{S_{t},A_{t},R_{t}\}_{t=0}^{T}\}_{l=1}^{L}\) from a random policy
3: Add trajectories to the buffer
4:endfor
5:for\(j=1\) to \(K\)do\(\triangleright\) Pretrain reward features
6: Sample batch from buffer
7: Train reward features \(U=f(S,A)\) to predict rewards \(R\) via mean squared error
8:endfor
9:for\(i=1\) to \(N-M\)do
10: Sample \(L\) trajectories \(\tau=\{\{S_{t},A_{t},R_{t}\}_{t=0}^{T}\}_{l=1}^{L}\) from \(\pi\)
11:for\(t=1\) to \(T\)do\(\triangleright\) Update the contribution module
12:for\(k>t\)do
13: Train \(h(A_{t}|S_{t},U_{k},\pi_{t})\) via cross-entropy loss on all trajectories \(\tau\)
14:endfor
15:endfor
16:for\(t=1\) to \(T\)do\(\triangleright\) Update the policy
17:for\(a\in\mathcal{A}\)do
18:\(X_{a}=\nabla_{\theta}\pi(a\mid S_{t})\sum_{k\geq 1}w(S_{t},a,U_{t+k})R_{t+k}\)
19:endfor
20:\(\widehat{\nabla}_{\theta}^{U}V^{\pi}(s_{0})=\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t} \mid S_{t})R_{t}+\sum_{a\in\mathcal{A}}X_{a}\)
21:endfor
22:endfor ```

**Algorithm 1** COCOA

**Contribution coefficients**

\[w(s,a,u^{\prime})=\frac{M(s,a,u^{\prime},T)}{\sum_{a^{\prime}\in\mathcal{A}}\pi (a^{\prime}\mid s)M(s,a^{\prime},u^{\prime},T)}-1\] (77)

where

\[M(s,a,u^{\prime},T)=\sum_{s^{\prime}\in\mathcal{S}}M(s,a,s^{\prime},T)\sum_{a ^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s^{\prime})\sum_{r^{\prime}}p(r^{ \prime}\mid s^{\prime},a^{\prime})\mathbb{1}_{f(s^{\prime},a^{\prime},r^{ \prime})=u^{\prime}},\] (78)

similar to the successor representations detailed in Section D. To discover the full state space \(\mathcal{S}\) for an arbitrary environment, we perform a depth-first search through the environment transitions (which are deterministic in our setting), and record the encountered states.

**Value function**

\[V(s) =\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s)r(s,a^{\prime})+ \sum_{k=1}^{T}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S_{k}=s^{\prime}|S_{0}=s,\pi)\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})r(s^{\prime},a^ {\prime})\] (79) \[=\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s)r(s,a^{\prime})+ \sum_{s^{\prime}\in\mathcal{S}}\sum_{a\in\mathcal{A}}\pi(a\mid s)M(s,a,s^{ \prime},T)\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})r(s^{ \prime},a^{\prime})\] (80)

where \(r(s^{\prime},a^{\prime})=\sum_{r^{\prime}}r^{\prime}p(r^{\prime}\mid s^{ \prime},a^{\prime})\)

**Action-value function**

\[Q(s,a)= r(s,a)+\sum_{k=1}^{T}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S_{k}=s ^{\prime}|S_{0}=s,A_{0}=a)\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{ \prime})r(s^{\prime},a^{\prime})\] (81) \[= r(s,a)+\sum_{s^{\prime}\in\mathcal{S}}M(s,a,s^{\prime},T)\sum_{a ^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})r(s^{\prime},a^{\prime})\] (82)

where \(r(s^{\prime},a^{\prime})=\sum_{r^{\prime}}r^{\prime}p(r^{\prime}\mid s^{ \prime},a^{\prime})\)

#### e.2.2 Computing expected advantages

As a next step we detail how to compute the true expected advantage given a potentially imperfect estimator of the contribution coefficient \(\hat{w}\), the value function \(\hat{V}\) or the action-value function \(\hat{Q}\).

**COCOA**

\[\mathbb{E}_{\pi}\big{[}\hat{A}^{\pi}(s,a)\big{]} =r(s,a)-\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s)r(s,a ^{\prime})+\mathbb{E}_{T\sim\mathcal{T}(s,\pi)}\left[\sum_{k=1}^{T}\hat{w}(s,a,U_{k})R_{k}\right]\] (83) \[=r(s,a)-\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s)r(s,a ^{\prime})+\sum_{u^{\prime}\in\mathcal{U}}\sum_{k=1}^{T}p^{\pi}(U_{k}=u^{ \prime}\mid S_{0}=s)\hat{w}(s,a,u^{\prime})r(u^{\prime})\] (84) \[=r(s,a)-\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s)r(s,a ^{\prime})+\sum_{u^{\prime}\in\mathcal{U}}\hat{w}(s,a,u^{\prime})r(u^{\prime} )\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s)M(s,a^{\prime},u^{ \prime},T)\] (85)

where \(r(u^{\prime})=\sum_{r^{\prime}}r^{\prime}p(r^{\prime}\mid u^{\prime})\) and \(M(s,a^{\prime},u^{\prime},T)\) is defined as in E.2.1.

**Advantage**

\[\mathbb{E}_{\pi}\big{[}\hat{A}^{\pi}(s,a)\big{]}=Q(s,a)-\hat{V}(s)\] (86)

where \(Q\) is the ground truth action-value function obtained following E.2.1.

**Q-critic**

\[\mathbb{E}_{\pi}\big{[}\hat{A}^{\pi}(s,a)\big{]}=\hat{Q}(s,a)- \sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}\mid s)\hat{Q}(s,a^{\prime})\] (87)

#### e.2.3 Computing expected policy gradients

Finally, we show how we can compute the expected policy gradient, \(\mathbb{E}_{\pi}\big{[}\hat{\nabla}_{\hat{\theta}}V^{\pi}\big{]}\) of a potentially biased gradient estimator, given the expected advantage \(\mathbb{E}_{\pi}\big{[}\hat{A}^{\pi}(s,a)\big{]}\).

\[\mathbb{E}_{\pi}\big{[}\hat{\nabla}_{\hat{\theta}}V^{\pi}\big{]} =\sum_{k=0}^{T}\mathbb{E}_{S_{k}\sim\mathcal{T}(s_{0},\pi)}\sum_ {a\in\mathcal{A}}\nabla_{\theta}\pi(a|s)\mathbb{E}_{\pi}\big{[}\hat{A}^{\pi} (S_{k},a)\big{]}\] (88) \[=\sum_{k=0}^{T}\sum_{s\in\mathcal{S}}p^{\pi}(S_{k}=s|s_{0})\sum_ {a\in\mathcal{A}}\nabla_{\theta}\pi(a|s)\mathbb{E}_{\pi}\big{[}\hat{A}^{\pi} (s,a)\big{]}\] (89)

Using automatic differentiation to obtain \(\nabla_{\theta}\pi(a|s_{0})\), and the quantity \(M(s,a,s^{\prime},T)\) we defined above, we can then compute the expected policy gradient.

\[\mathbb{E}_{\pi}\big{[}\hat{\nabla}_{\hat{\theta}}V^{\pi}\big{]} =\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a|s_{0})\mathbb{E}_{\pi} \big{[}\hat{A}^{\pi}(s_{0},a)\big{]}+\] (90) \[\sum_{s\in\mathcal{S}}\sum_{a_{0}\in\mathcal{A}}\pi(a_{0}\mid s_{ 0})M(s_{0},a_{0},s,T)\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a|s)\mathbb{E}_ {\pi}\big{[}\hat{A}^{\pi}(s,a)\big{]}\] (91)

Computing the ground-truth policy gradient.To compute the ground-truth policy gradient, we can apply the same strategy as for Eq. 90 and replace the expected (possibly biased) advantage \(\mathbb{E}_{\pi}\big{[}\hat{A}^{\pi}(s_{0},a)\big{]}\) by the ground-truth advantage function, computed with the ground-truth action-value function (c.f. Section E.2.1).

### Bias, variance and SNR metrics

To analyze the quality of the policy gradient estimators, we use the signal-to-noise ratio (SNR), which we further subdivide into variance and bias. A higher SNR indicates that we need fewer trajectories to estimate accurate policy gradients, hence reflecting better credit assignment. To obtain meaningfulscales, we normalize the bias and variance by the norm of the ground-truth policy gradient.

\[SNR =\frac{\|\nabla_{\theta}V^{\pi}\|^{2}}{\mathbb{E}_{\pi}\left[\| \hat{\nabla}_{\theta}V^{\pi}-\nabla_{\theta}V^{\pi}\|^{2}\right]}\] (92) \[\text{Variance} =\frac{\mathbb{E}_{\pi}\left[\|\hat{\nabla}_{\theta}V^{\pi}- \mathbb{E}_{\pi}[\hat{\nabla}_{\theta}V^{\pi}]\|^{2}\right]}{\|\nabla_{\theta} V^{\pi}\|^{2}}\] (93) \[\text{Bias} =\frac{\|\mathbb{E}_{\pi}[\hat{\nabla}_{\theta}V^{\pi}]-\nabla_{ \theta}V^{\pi}\|^{2}}{\|\nabla_{\theta}V^{\pi}\|^{2}}\] (94)

We compute the full expectations \(\mathbb{E}_{\pi}[\hat{\nabla}_{\theta}V^{\pi}]\) and ground-truth policy gradient \(\nabla_{\theta}V^{\pi}\) by leveraging our dynamic programming setup, while for the expectation of the squared differences \(\mathbb{E}_{\pi}\left[\|\hat{\nabla}_{\theta}V^{\pi}-\nabla_{\theta}V^{\pi}\|^ {2}\right]\) we use Monte Carlo sampling with a sample size of 512. We report the metrics in Decibels in all figures.

Focusing on long-term credit assignment.As we are primarily interested in assessing the long-term credit assignment capabilities of the gradient estimators, we report the statistics of the policy gradient estimator corresponding to learning to pick up the key or not. Hence, we compare the SNR, variance and bias of a partial policy gradient estimator considering only \(t=0\) in the outer sum (corresponding to the state with the key) for all considered estimators (c.f. Table 1).

Shadow training.Policy gradients evaluated during training depend on the specific learning trajectory of the agent. Since all methods' policy gradient estimators contain noise, these trajectories are likely different for the different methods. As a result, it is difficult to directly compare the quality of the policy gradient estimators, since it depends on the specific data generated by intermediate policies during training. In order to allow for a controlled comparison between methods independent of the noise introduced by different trajectories, we consider a _shadow training_ setup in which the policy is trained with the Q-critic method using ground-truth action-value functions. We can then compute the policy gradients for the various estimators on the same shared data along this learning trajectory without using it to actually train the policy. We use this strategy to generate the results shown in Fig. 1B (right), Fig. 2B and Fig. 3C-D.

### Linear key-to-door environment setup

We simplify the key-to-door environment previously considered by various papers [e.g. 3, 4, 32], to a one-dimensional, linear track instead of the original two-dimensional grid world. This version still captures the difficulty of long-term credit assignment but reduces the computational burden allowing us to thoroughly analyze different policy gradient estimators with the aforementioned dynamic programming based setup. The environment is depicted in Fig. 3A. Here, the agent needs to pick up a key in the first time step, after which it engages in a distractor task of picking up apples which can either be to the left or the right of the agent and which can stochastically assume two different reward values. Finally, the agent reaches a door which it can open with the key to collect a treasure reward.

In our simulations we represent states using a nine-dimensional vector, encoding the relative position on the track as a floating number, a boolean for each item that could be present at the current location (empty, apple left, apple right, door, key, treasure) as well as boolean indicating whether the agent has the key and a boolean for whether the agent does not have the key.

There are four discrete actions the agent can pick at every time step: pick the key, pick to the left, pick to the right and open the door. Regardless of the chosen action, the agent will advance to the next position in the next time step. If it has correctly picked up the key in the first time step and opened the door in the penultimate time step, it will automatically pick up the treasure, not requiring an additional action.

Hung et al. [4] showed that the signal-to-noise ratio (SNR) of the REINFORCE policy gradient [6] for solving the main task of picking up the key can be approximated by

\[\text{SNR}^{\text{REINF}}\approx\frac{\|\mathbb{E}[\hat{\nabla}_{\theta}^{ \text{REINF}}V^{\pi}]\|^{2}}{C(\theta)\mathbb{V}[\sum_{t\in T2}R_{t}]+\text{ Tr}\left[\mathbb{V}[\hat{\nabla}_{\theta}^{\text{REINF}}V^{\pi}\mid\text{no T2}]\right]}.\] (95)with \(\tilde{\nabla}_{\theta}^{\text{REINF}}V^{\pi}\) the REINFORCE estimator (c.f. Table 1), \(C(\theta)\) a reward-independent constant, \(T2\) the set of time steps corresponding to the distractor task, and \(\operatorname{Tr}\left[\mathbb{V}[\tilde{\nabla}_{\theta}^{\text{REINF}}V^{\pi} \mid\text{no T2}]\right]\) the trace of the covariance matrix of the REINFORCE estimator in an equivalent task setup without distractor rewards. Hence, we can adjust the difficulty of the task by increasing the number of distractor rewards and their variance. We perform experiments with environments of length \(L\in\{20,40,\dots,100\}\) choosing the reward values such that the total distractor reward remains approximately constant. Concretely, distractor rewards are sampled as \(r_{\text{distractor}}\sim\mathcal{U}(\{\frac{2}{L},\frac{18}{L}\})\) and the treasure leads to a deterministic reward of \(r_{\text{treasure}}=\frac{4}{L}\).

### Reward switching setup

While learning to get the treasure in the key-to-door environment requires long term credit assignment as the agent needs to learn to 1) pickup the key and 2) open the door, learning to stop picking up the treasure does not require long term credit assignment, since the agent can simply learn to stop opening the door.

We therefore reuse the linear key-to-door environment of length \(L=40\), with the single difference that we remove the requirement to open the door in order to get the treasure reward. The agent thus needs to perform similar credit assignment to both get the treasure and stop getting the treasure. When applying reward switching, we simply flip the sign of the treasure reward while keeping the distractor reward unchanged.

### Tree environment setup

We parameterize the tree environment by its depth \(d\), the number of actions \(n_{a}\) determining the branching factor, and the _state overlap_\(o_{s}\), defined as the number of overlapping children from two neighbouring nodes. The states are represented by two integers: \(i<d\) representing the current level in the tree, and \(j\) the position of the state within that level. The root node has state \((i,j)=(0,0)\), and the state transitions are deterministic and given by \(i\gets i+1\) and \(j\gets j(n_{a}-o_{s})+a\), where we represent the action by an integer \(0\leq a<n_{a}\). We assign each state-action pair a reward \(r\in\{-2,-1,0,1,2\}\), computed as

\[r(s,a)=\left((\operatorname{idx}(s)+ap+\operatorname{seed})\bmod n_{r}\right) -n_{r}//2\] (96)

with the modulo operator \(\bmod\), the number of reward values \(n_{r}=5\), \(\operatorname{idx}(s)\) a unique integer index for the state \(s=(i,j)\), a large prime number \(p\), and an environment \(\operatorname{seed}\).

To introduce rewarding outcome encodings with varying information content, we group state-action pairs corresponding to the same reward value in \(n_{g}\) groups:

\[u=(\operatorname{idx}(s)+ap+\operatorname{seed})\bmod(n_{r}n_{g})-n_{r}//2\] (97)

Environment parameters for Figure 2.For the experiment of Fig. 2, we use 6 actions and a depth of 4. We plotted the variance of the COCOA estimator for encodings \(U\) corresponding to \(n_{g}=4\) and \(n_{g}=32\).

### Task interleaving environment setup

We simplify the _task interleaving_ environment of Mesnard et al. [3] in the following way: the environment is parameterized by the number of contexts \(C\), the number of objects per context \(O\), the maximum number of open contexts \(B\), and finally the dying probability \(1-\gamma\). In each context, a set of \(2O\) objects is given, and a subset of \(O\) objects is randomly chosen as the rewarding objects for that context. The task consists in learning, for all contexts, to chose the right objects when presented a pair of objects, one rewarding and one non rewarding. While this is reminiscent of a contextual bandit setting, crucially, the agent receives the reward associated to its choice only at a later time, after potentially making new choices for other contexts, and receiving rewards from other contexts, resulting in an interleaved bandit problem. The agent must thus learn to disentangle the reward associated to each context in order to perform proper credit assignment.

State and actionAt every timestep, there are \(2\) actions the agent can take: choose right, or choose left.

The state is defined by the following variables:* A set of open contexts, i.e. contexts where choices have been made but no reward associated to those choices has been received
* For each open context, a bit (or key) indicating whether the correct choice has been made or not
* The context of the current task
* A pair of objects currently being presented to the agent, if the current context is a query room. Each element of the pair is assigned to either the left side or the right side.

Transition and rewardThe transition dynamics work as follows. If the agent is in an answer room, then regardless of the action, it receives a context-dependent reward if it also has the key of the corresponding context (i.e. it has made the correct choice when last encountering query room corresponding to the context). Otherwise, it receives \(0\) reward. Then, the current context is removed from the set of open contexts.

If the agent is in a query room, then the agent is presented with \(2\) objects, one rewarding and one not, and must choose the side of the rewarding object. Regardless, the agent receives \(0\) reward at this timestep, and the current context is added to the set of open contexts. Furthermore, if it did make the correct choice, a key associated to the current context is given.

Finally, the next context is sampled. With a probability \(\frac{C_{o}}{B}\) (where \(C_{o}\) is the number of open context), the agent samples uniformly one of the open contexts. Otherwise, the agent samples uniformly one of the non-open contexts. The \(2\) objects are then also sampled uniformly out of the \(O\) rewarding objects and \(O\) unrewarding objects. The rewarding object is placed either on the right or left side with uniform probability, and the unrewarding object placed on the other side.

Crucially, there is a probability \(1-\gamma\) of dying, in which case the agent receives the reward but is put into a terminal state at the next timestep.

Environment parametersFor all our experiments, we choose \(C=5,B=3,O=2\) and \(\gamma=0.95\).

Visualization of contribution coefficient magnitudes.For the heatmaps shown in Fig 4C, we computed each entry as \(\sum_{a\in\mathcal{A}}\pi(a|s_{t})\cdot|w(s_{t},a,r_{t+k})|\) where states \(s_{t}\) and rewards \(r_{t+k}\) are grouped by the context of their corresponding room. We average over all query-answer pairs grouped by contexts excluding query-query or answer-answer pairs and only consider answer rooms with non-zero rewards.

### Training details

#### e.8.1 Architecture

We use separate fully-connected ReLU networks to parameterize the policy, value function, and action-value function. For the policy we use two hidden layers of size 64, for the value function and action-value function respectively we use a single hidden layer of size 256. For the hindsight model of both HCA and COCOA we found a simple multilayer perceptron with the state \(s\) and rewarding outcome encoding \(u^{\prime}\) as inputs to perform poorly. We hypothesize that this is due to the policy dependence of the hindsight distribution creating a moving target during learning as the

Figure 8: Schematic of the neural network architecture of the hindsight models

policy changes. Leveraging Proposition 5, we therefore add the policy logits as an extra input to the hindsight network to ease tracking the changing policy. We found good performance using a simple hypernetwork, depicted in Fig. 8 that combines the policy logits with the state and hindsight object inputs through a multiplicative interaction, outputting a logit for each possible action. The multiplicative interaction denoted by \(\otimes\) consists of a matrix multiplication of a matrix output by the network with the policy logits, and can be interpreted as _selecting_ a combination of policy logits to add to the output channel. In order to allow gating with both positive and negative values, we use a gated version of the ReLU nonlinearity in the second layer which computes the difference between the split, rectified input effectively halving the output dimension:

\[\text{ReLU-g}(x)=\text{ReLU}(x_{0:n/2})-\text{ReLU}(x_{n/2:n})\] (98)

with \(n\) the dimension of \(x\). Gating in combination with the multiplicative interaction is a useful inductive bias for the hindsight model, since for actions which have zero contribution towards the rewarding outcome \(u\) the hindsight distribution is equal to the policy. To increase the performance of our HCA+ baseline, we provide both the policy logits and one minus the policy logits to the multiplicative interaction.

#### e.8.2 Optimization

For training of all models we use the AdamW optimizer with default parameters only adjusting the learning rates and clipping the global norm of the policy gradient. We use entropy regularization in combination with epsilon greedy to ensure sufficient exploration to discover the optimal policy. To estimate (action-) value functions we use TD(\(\lambda\)) treating each \(\lambda\) as a hyperapperemeter. For all linear layers we use the default initialization of Haiku [82] where biases are initialized as zero and weights are sample from a truncated Gaussian with standard deviation \(\frac{1}{\sqrt{n_{\text{input}}}}\) where \(n_{\text{input}}\) is the input dimension of the respective layer and.

#### e.8.3 Reward features

Learned reward features should both be fully predictive of the reward (c.f. Theorem 1), and contain as little additional information about the underlying state-action pair as possible (c.f. Theorem 3). We can achieve the former by training a network to predict the rewards given a state-action pair, and take the penultimate layer as the feature \(u=f(s,a)\). For the latter, there exist multiple approaches. When using a deterministic encoding \(u=f(s,a)\), we can bin the features such that similar features predicting the same reward are grouped together. When using a stochastic encoding \(p(U\mid S,A)\) we can impose an information bottleneck on the reward features \(U\), enforcing the encoding to discard as much information about \(U\) as possible [83, 84]. We choose the deterministic encoding approach, as our Dynamic Programming routines require a deterministic encoding.4 We group the deterministic rewarding outcome encodings by discretizing the reward prediction network up to the penultimate layer.

Footnote 4: In principle, our dynamic programming routines can be extended to allow for probabilistic rewarding outcome encodings and probabilistic environment transitions, which we leave to future work.

Architecture.For the neural architecture of the reward prediction network we choose a linear model parameterized in the following way. The input is first multiplicatively transformed by an action-specific mask. The mask is parameterized by a vector of the same dimensionality as the input, and which is squared in order to ensure positivity. A ReLU nonlinearity with straight through gradient estimator is then applied to the transformation. Finally, an action-independent readout weight transforms the activations to the prediction of the reward. The mask parameters are initialized to \(1\). Weights of the readout layer are initialized with a Gaussian distribution of mean \(0\) and std \(\frac{1}{\sqrt{d}}\) where \(d\) is the dimension of the input.

Loss function.We train the reward network on the mean squared error loss against the reward. To avoid spurious contributions (c.f. 3.3), we encourage the network to learn sparse features that discard information irrelevant to the prediction of the reward, by adding a L1 regularization term to all weights up to the penultimate layer with a strength of \(\eta_{L_{1}}=0.001\). The readout weights are trained with standard L2 regularization with strength \(\eta_{L_{2}}=0.03\). For the linear key-to-door experiments, we choose \((\eta_{L_{1}},\eta_{L_{2}})=(0.001,0.03)\). For the task interleaving environment, we choose \((\eta_{L_{1}},\eta_{L_{2}})=(0.05,0.0003)\).

All weights regularization are treated as weight decay, with the decay applied after the gradient update.

Pretraining.To learn the reward features, we collect the first \(B_{\text{feature}}\) mini-batches of episodes in a buffer using a frozen random policy. We then sample triplets \((s,a,r)\) from the buffer and train with full-batch gradient descent using the Adam optimizer over \(N_{\text{feature}}\) steps. Once trained, the reward network is frozen, and the masked inputs, which are discretized using the element-wise threshold function \(\mathbbm{1}_{x>0.05}\), are used to train the contribution coefficient as in other COCOA methods. To ensure a fair comparison, other methods are already allowed to train the policy on the first \(B_{\text{feature}}\) batches of episodes. For the linear key-to-door experiments, we chose \((B_{\text{feature}},N_{\text{feature}})=(30,20000)\). For the task interleaving environment, we chose \((B_{\text{feature}},N_{\text{feature}})=(90,30000)\).

#### e.8.4 Hyperparameters

Linear key-to-door setup (performance)For all our experiments on the linear key-to-door environment, we chose a batch size of 8, while using a batch size of 512 to compute the average performance, SNR, bias and variance metrics. We followed a 2-step selection procedure for selecting the hyperparameters: first, we retain the set of hyperparameters for which the environment can be solved for at least 90% of all seeds, given a large amount of training budget. An environment is considered to be solved for a given seed when the probability of picking up the treasure is above 90%. Then, out of all those hyperparameters, we select the one which maximizes the cumulative amount of treasures picked over 10000 training batches. We used 30 seeds for each set of hyperparameters to identify the best performing ones, then drew 30 fresh seeds for our evaluation. The range considered

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Hyperparameter & COCOA-reward & COCOA-feature & HCA+ & Q-critic & Advantage & REINFORCE & TrajCV \\ \hline lr\_agent & \(0.0003\) & \(0.0003\) & \(0.0001\) & \(0.0003\) & \(0.001\) & \(0.001\) & \(0.001\) \\ lr\_hindsight & \(0.001\) & \(0.001\) & \(0.001\) & - & - & - & - \\ lr\_value & - & - & - & - & \(0.001\) & - & - \\ lr\_qvalue & - & - & - & \(0.003\) & - & - & 0.001 \\ lr\_features & - & \(0.001\) & - & - & - & - & - \\ entropy\_reg & \(0.01\) & \(0.01\) & \(0.01\) & \(0.01\) & \(0.01\) & \(0.01\) & \(0.01\) \\ td\_lambda\_value & - & - & - & - & \(1.\) & - & - \\ td\_lambda\_qvalue & - & - & - & \(0.9\) & - & - & \(0.9\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameter values on the task interleaving environment.

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & Range \\ \hline lr\_agent & \(\{0.003,0.001,0.0003,0.0001\}\) \\ lr\_hindsight & \(\{0.01,0.003,0.001\}\) \\ lr\_value & \(\{0.01,0.003,0.001\}\) \\ lr\_qvalue & \(\{0.01,0.003,0.001\}\) \\ lr\_features & \(\{0.01,0.003,0.001\}\) \\ td\_lambda\_value & \(\{0.,0.5,0.9,1.\}\) \\ entropy\_reg & \(\{0.3,0.1,0.03,0.01\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: The range of values swept over for each hyperparameter in a grid search for the linear key-to-door environment and task interleaving environment.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Hyperparameter & COCOA-r & COCOA-f & HCA+ & Q-critic & Advantage & REINFORCE & TrajCV & HCA-return \\ \hline lr\_agent & \(0.0003\) & \(0.0003\) & \(0.0003\) & \(0.0003\) & \(0.001\) & \(0.0003\) & \(0.003\) & \(0.0001\) \\ lr\_hindsight & \(0.003\) & \(0.003\) & \(0.003\) & - & - & - & \(0.001\) \\ lr\_value & - & - & - & \(0.003\) & - & - & \(0.01\) & - \\ lr\_features & - & \(0.003\) & - & - & - & - & - \\ td\_lambda\_value & - & - & - & \(1.\) & - & - & - \\ td\_lambda\_qvalue & - & - & - & \(0.9\) & - & - & \(0.9\) & - \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameter values on the linear key-to-door environment. The best performing hyperparameters were identical accross all environment lengths. COCOA-r stands for COCOA-return, COCOA-f for COCOA-feature.

for our hyperparameter search can be found in Table 2, and the selected hyperparameters in Table 3. Surprisingly, we found that for the environment length considered, the same hyperparameters were performing best, with the exception of entropy_reg. For the final values of entropy_reg for each environment length, we linearly interpolated in log-log space between the best performing values, \(0.03\) for length \(20\) and \(0.01\) for \(100\). The values can be found in Table 5.

Linear key-to-door setup (shadow training)For measuring the bias and variances of different methods in the shadow training setting, we used the best performing hyperparameters found in the performance setting. We kept a batch size of 8 for the behavior policy and shadow training, while using a batch size of 512 during evaluation.

Reward switching setupFor the reward switching experiment, we chose hyperparameters following a similar selection procedure as in the linear key-to-door setup, but in the simplified door-less environment of length 40, without any reward switching. We found very similar hyperparameters to work well despite the absence of a door compared to the linear key-to-door setup. However, in order to ensure that noisy methods such as REINFORCE fully converged before the moment of switching the reward, we needed to train the models for 60000 training batches before the switch. To stabilize the hindsight model during this long training period, we added coarse gradient norm clipping by \(1..\) Furthermore, we found that a slightly decreased learning rate of \(0.001\) for the Q-critic performed best. Once the best hyperparameters were found, we applied the reward switching to record the speed of adaptation for each algorithm. We kept a batch size of 8 for the behavior policy and shadow training, while using a batch size of 512 during evaluation.

Task interleaving experimentWe choose a batch size of 8 and train on 10000 batches, while using a batch size of 512 to compute the average performance. We used 5 seeds for each set of hyperparameters to identify the best performing ones, then drew 5 fresh seeds for our evaluation. The selected hyperparameters can be found in Table 4.

## Appendix F Additional results

We perform additional experiments to corroborate the findings of the main text. Specifically, we investigate how _reward aliasing_ affects COCOA-reward and COCOA-feature and show that there is no significant difference in performance between HCA and our simplified version, HCA+.

### HCA vs HCA+

Our policy gradient estimator for \(U=S\) presented in Eq. 4 differs slightly from Eq. 2, the version originally introduced by Harutyunyan et al. [1], as we remove the need for a learned reward model \(r(s,a)\). We empirically verify that this simplification does not lead to a decrease in performance in Fig. 9. We run the longest and hence most difficult version of the linear key-to-door environment considered in our experiments and find no significant difference in performance between our simplified version (HCA+) and the original variant (HCA).

### Learned credit assignment features allow for quick adaptation to a change of the reward function.

Disentangling rewarding outcomes comes with another benefit: When the reward value corresponding to a rewarding outcome changes, e.g. the treasure that was once considered a reward turns out to be poisonous, we only have to relearn the contribution coefficients corresponding to this rewarding outcome. This is in contrast to value-based methods for which potentially many state values are altered. Moreover, once we have access to credit assignment features \(u\) that encode rewarding outcomes which generalize to the new setting, the contribution coefficients remain invariant to changes in reward contingencies. For example, if we remember that we need to open the door with

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Environment length & 20 & 40 & 60 & 80 & 100 & 100 (reward-aliasing) \\ \hline entropy\_reg & \(0.03\) & \(0.0187\) & \(0.0142\) & \(0.0116\) & \(0.01\) & \(0.0062\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The entropy regularization value selected for each environment length of the linear key-to-door environment. The values were obtained by linearly interpolating in log-log space between the best performing entropy regularization strength between environment length 20 and 100.

the key to get to the treasure, we can use this knowledge to avoid picking up the key and opening the door when the treasure becomes poisonous.

To illustrate these benefits, we consider a variant of the linear key-to-door environment, where picking up the key always leads to obtaining the treasure reward, and where the treasure reward abruptly changes to be negative after 60k episodes. In this case, COCOA-feature learns to encode the treasure as the same underlying rewarding object before and after the switch, and hence can readily reuse the learned contribution coefficients adapting almost instantly (c.f. Fig. 10). COCOA-reward in contrast needs to encode the poisonous treasure as a new, previously unknown rewarding outcome. Since it only needs to relax the contribution coefficients for the disentangled subtask of avoiding poison it nevertheless adapts quickly (c.f. Fig. 10). Advantage and Q-critic are both faced with relearning their value functions following the reward switch. In particular, they lack the property of disentangling different rewarding outcomes and all states that eventually lead to the treasure need to be updated.

### Using returns instead of rewards as hindsight information

In Appendix L, we discussed that HCA-return is a biased estimator in many relevant environments, including our key-to-door and task-interleaving environments. This explains its worse performance compared to our COCOA algorithms. To isolate the difference of using returns instead of rewards as

Figure 10: **COCOA-feature quickly adapts the policy to a change of the reward function.** Percentage of treasure reward collected before and after the change in reward contingencies. COCOA-feature quickly adapts to the new setting, as its credit assignment mechanisms generalize to the new setting, whereas COCOA-reward, Q-critic and Advantage need to adjust their models before appropriate credit assignment can take place.

Figure 9: **No performance difference between the original HCA method and our modified variant HCA+.** In the linear key-to-door environment of length 103 both the original HCA method with an additional learned reward model and our simplified version HCA+ perform similarly in terms of performance measured as the percentage of treasure reward collected.

hindsight information for constructing the contribution coefficients, we introduce the following new baseline:

\[\sum_{t\geq 0}\sum_{a}\nabla_{\theta}\pi(a\mid S_{t})\Big{(}\frac{p^{\pi}(a \mid S_{t},Z_{t})}{\pi(a\mid S_{t})}-1\Big{)}Z_{t}\] (99)

Similar to HCA-return, this _Counterfactual Return_ variant leverages a hindsight distribution conditioned on the return. Different from HCA-return, we use this hindsight distribution to compute contribution coefficients that can evaluate all counterfactual actions. We prove that this estimator is unbiased.

Figure 11 shows that the performance of HCA-return and Counterfactual return lags far behind the performance of COCOA-reward on the key-to-door environment. This is due to the high variance of HCA-return and Counterfactual Return, and the biasedness of the former. As the return is a combination of all the rewards of a trajectory, it cannot be used to disentangle rewarding outcomes, causing the variance of the distractor subtask to spill over to the subtask of picking up the treasure.

### Investigation into the required accuracy of the hindsight models

To quantify the relation between the approximation quality of the hindsight models and value functions upon the SNR of the resulting policy gradient estimate, we introduce the following experiment. We start from the ground-truth hindsight models and value functions computed with our dynamic programming setup (c.f. E.2), and introduce a persistent bias into the output of the model by elementwise multiplying the output with \((1+\sigma\epsilon)\), with \(\epsilon\) zero-mean univariate Gaussian noise and \(\sigma\in\{0.001,0.003,0.01,0.03,0.1,0.3\}\) a scalar of which we vary the magnitude in the experiment.

Figure 12 shows the SNR of the COCOA-reward estimator and the baselines, as a function of the perturbance magnitude \(\log\sigma\), where we average the results over 30 random seeds. We see that the sensitivity of the COCOA-reward estimator to its model quality is similar to that of Q-critic. Furthermore, the SNR of COCOA-reward remains of better quality compared to Advantage and HCA-state for a wide range of perturbations.

## Appendix G Contribution analysis in continuous spaces and POMDPs

In Section 3.3 we showed that HCA can suffer from spurious contributions, as state representations need to contain detailed features to allow for a capable policy. The same level of detail however is detrimental when assigning credit to actions for reaching a particular state, since at some resolution almost every action will lead to a slightly different outcome. Measuring the contribution towards a specific state ignores that often the same reward could be obtained in a slightly different state, hence overvaluing the importance of past actions. Many commonly used environments, such as pixel-based environments, continuous environments, and partially observable MDPs exhibit this property to a large extent due to their fine-grained state representations. Here, we take a closer look at how spurious contributions arise in continuous environments and Partially Observable MDPs (POMDPs).

Figure 11: Performance of COCOA and baselines on the main task of picking up the treasure, measured as the average fraction of treasure rewards collected including the _Counterfactual return_ method.

### Spurious contributions in continuous state spaces

When using continuous state spaces, \(p^{\pi}(S_{k}=s^{\prime}\mid s,a)\) represents a probability density function (PDF) instead of a probability. A ratio of PDFs \(p(X=x)/p(X=x^{\prime})\) can be interpreted as a likelihood ratio of how likely a sample \(X\) will be close to \(x\) versus \(x^{\prime}\). Using PDFs, the contribution coefficients with \(U=S\) used by HCA result in

\[w(s,a,s^{\prime})=\frac{\sum_{k\geq 1}p^{\pi}(S_{t+k}=s^{\prime}\mid S_{t}=s,A_{t}=a)}{\sum_{k\geq 1}p^{\pi}(S_{t+k}=s^{\prime}\mid S_{t}=s)}-1=\frac{p^{\pi}(A_ {t}=a\mid S_{t}=s,S^{\prime}=s^{\prime})}{\pi(a\mid s)}-1\] (100)

and can be interpreted as a likelihood ratio of encountering a state 'close' to \(s^{\prime}\), starting from \((s,a)\) versus starting from \(s\) and following the policy. The contribution coefficient will be high if \(p^{\pi}(S^{\prime}=s^{\prime}\mid s,a)\) has a high probability density at \(s^{\prime}\) conditioned on action \(a\), compared to the other actions \(a^{\prime}\neq a\). Hence, the less \(p^{\pi}(S_{k}=s^{\prime}\mid s,a)\) and \(p^{\pi}(S_{k}=s^{\prime}\mid s,a^{\prime})\) overlap around \(s^{\prime}\), the higher the contribution coefficient for action \(a\). The variance of the distribution \(p^{\pi}(S_{k}=s^{\prime}\mid s,a)\), and hence its room for overlap, is determined by how diffuse the environment transitions and policy are. For example, a very peaked policy and nearly deterministic environment transitions leads to a sharp distribution \(p^{\pi}(S_{k}=s^{\prime}\mid s,a)\).

The randomness of the policy and environment is a poor measure for the contribution of an action in obtaining a reward. For example, consider the musician's motor system, that learns to both: (i) play the violin and (ii) play the keyboard. We assume a precise control policy and near deterministic environment dynamics, resulting in peaked distributions \(p^{\pi}(S_{k}=s^{\prime}\mid s,a)\). For playing the violin, a slight change in finger position significantly influences the pitch, hence the reward function sharply declines around the target state with perfect pitch. For playing the keyboard, the specific finger position matters to a lesser extent, as long as the correct key is pressed, resulting in a relatively flat reward function w.r.t. the precise finger positioning. The state-based contribution coefficients of Eq. 100 result in a high contribution for the action taken in the trajectory for both tasks. For playing violin, this could be a good approximation of what we intuitively think of as a 'high contribution', whereas for the keyboard, this overvalues the importance of the past action in many cases. From this example, it is clear that measuring contributions towards rewarding _states_ in continuous state spaces can lead to spurious contributions, as the contributions mainly depend on how diffuse the policy and environment dynamics are, and ignore that different reward structures can require vastly different contribution measures.

### Deterministic continuous reward functions can lead to excessive variance

Proposition 2 shows that HCA degrades to REINFORCE in environments where each action sequence leads to distinct states. In Section G.1, we discussed that continuous state spaces exhibit this property to a large extend due to their fine-grained state representation. If each environment state has a unique reward value, COCOA can suffer from a similar degradation to HCA. As the rewarding outcome encoding \(U\) needs to be fully predictive of the reward (c.f. Theorem 1), it needs to have a distinct encoding for each unique reward, and hence for each state.

Figure 12: **The improved SNR of the COCOA estimator is robust to imperfect contribution coefficients.** The SNR of the COCOA-reward estimator and the baselines, as a function of the perturbance magnitude \(\log\sigma\), where we average the results over 30 random seeds.

With a continuous reward function this can become a significant issue, if the reward function is deterministic. Due to the fine-grained state representation in continuous state spaces, almost every action will lead to a (slightly) different state. When we have a deterministic, continuous reward function, two nearby but distinct states will often lead to nearby but distinct rewards. Hence, to a large extent, the reward contains the information of the underlying state, encoded in its infinite precision of a real value, resulting in COCOA degrading towards the high-variance HCA estimator.

The above problem does not occur when the reward function \(p(R\mid S,A)\) is probabilistic, even for continuous rewards. If the variance of \(p(R\mid S,A)\) is bigger than zero, the variance of \(p(S,A\mid R)\) is also bigger than zero under mild assumptions. This means that it is not possible to perfectly decode the underlying state using a specific reward value \(r\), and hence COCOA does not degrade towards HCA in this case. Intuitively, different nearby states can lead to the same sampled reward, removing the spurious contributions. In the following section, we will use this insight to alleviate spurious contributions, even for deterministic reward functions.

### Smoothing can alleviate excess variance by trading variance for bias

When each state has a unique reward, an unbiased COCOA estimator degrades to the high-variance HCA estimator, since \(U\) needs to be fully predictive of the reward, and hence each state needs a unique rewarding outcome encoding. Here, we propose two ways forward to overcome the excess variance of the COCOA estimator in this extreme setting that trade variance for bias.

Rewarding outcome binning.One intuitive strategy to avoid that each state has a unique rewarding outcome encoding \(U\), is to group rewarding outcome encodings corresponding to nearby rewards together, resulting in discrete bins. As the rewarding outcome encodings now contain less details, the resulting COCOA estimator will have lower variance. However, as now the rewarding outcome encoding is not anymore fully predictive of the reward, the COCOA estimator will be biased. An intimately connected strategy is to change the reward function in the environment to a discretized reward function with several bins. Policy gradients in the new discretized environments will not be exactly equal to the policy gradients in the original environment, however for fine discretizations, we would not expect much bias. Similarly for binning the rewarding outcome encodings, when we group few nearby rewarding outcome encodings together, we expect a low bias, but also a low variance reduction. Increasing the amount of rewarding outcomes we group together we further lower the bias, at a cost of an increasing bias, hence creating a bias-variance trade-off.

Stochastic rewarding outcomes.We can generalize the above binning technique towards rewarding outcome encodings that bin rewards stochastically. In Section G.2, we discussed that when the reward function is probabilistic, the excessive variance problem is less pronounced, as different states can lead to the same reward. When dealing with a deterministic reward function, we can introduce stochasticity in the rewarding outcome encoding to leverage the same principle and reduce variance at the cost of increasing bias. For example, we can introduce the rewarding outcome encoding \(U\sim\mathcal{N}(R,\sigma)\), with \(\mathcal{N}\) corresponding to a Gaussian distribution. As this rewarding outcome encoding is not fully predictive of the reward, it will introduce bias. We can control this bias-variance trade-off with the variance \(\sigma\): a small sigma corresponds to a sharp distribution, akin to a fine discretization in the above binning strategy, and hence a low bias. Increasing \(\sigma\) leads to more states that could lead to the same rewarding outcome encoding, hence lowering the variance at the cost of increasing the bias.

Implicit smoothing by noise perturbations or limited capacity networks.Interestingly, the above strategy of defining stochastic rewarding outcomes is equivalent to adjusting the training scheme of the hindsight model \(h(a\mid s,u^{\prime})\) by adding noise to the input \(u^{\prime}\). Here, we take \(U\) equal to the (deterministic) reward \(R\), but add noise to it while training the hindsight model. Due to the noise, the hindsight model cannot perfectly decode the action \(a\) from its input, resulting in the same effects as explicitly using stochastic rewarding outcomes. Adding noise to the input of a neural network is a frequently used regularization technique. Hence, an interesting route to investigate is whether other regularization techniques on the hindsight model, such as limiting its capacity, can result in a bias-variance trade-off for HCA and COCOA.

Smoothing hindsight states for HCA.We can apply the same (stochastic) binning technique to hindsight states for HCA, creating a more coarse-grained state representation for backward credit assignment. However, the bias-variance trade-off is more difficult to control for states compared to rewards. The sensitivity of the reward function to the underlying state can be big in some regions of state-space, whereas small in others. Hence, a uniform (stochastic) binning of the state-spacewill likely result in sub-optimal bias-variance trade-offs, as it will be too coarse-grained in some regions causing large bias, whereas too fine-grained in other regions causing a low variance-reduction. Binning rewards in contrast does not suffer from this issue, as binning is directly performed in a space relevant to the task.

Proposition 8 provides further insight on what the optimal smoothing or binning for HCA looks like. Consider the case where we have a discrete reward function with not too many distinct values compared to the state space, such that COCOA-reward is a low-variance gradient estimator. Proposition 8 shows that we can recombine the state-based hindsight distribution \(p^{\pi}(A_{0}=a\mid S_{0}=s,S^{\prime}=s^{\prime})\) into the reward-based hindsight distribution \(p^{\pi}(A_{0}=a\mid S_{0}=s,R^{\prime}=r^{\prime})\), by leveraging a smoothing distribution \(\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S^{\prime}=s^{\prime}\mid S_{0}=s,R^{ \prime}=r^{\prime})\). When we consider a specific hindsight state \(s^{\prime\prime}\), this means that we can obtain the low-variance COCOA-reward estimator, by considering all states \(S^{\prime}\) that could have lead to the same reward \(r(s^{\prime\prime})\), instead of only \(s^{\prime\prime}\). Hence, instead of using a uniform stochastic binning strategy with e.g. \(S^{\prime}\sim\mathcal{N}(s^{\prime\prime},\sigma)\), a more optimal binning strategy is to take the reward structure into account through \(p^{\pi}(S^{\prime}\mid S_{0}=s,R^{\prime}=r(s^{\prime\prime}))\)

**Proposition 8**.: \[p^{\pi}(A_{0}=a\mid S_{0}=s,R^{\prime}=r^{\prime})=\sum_{s^{\prime}\in \mathcal{S}}p^{\pi}(S^{\prime}=s^{\prime}\mid S_{0}=s,R^{\prime}=r^{\prime})p^ {\pi}(A_{0}=a\mid S_{0}=s,S^{\prime}=s^{\prime})\] (101)

Proof.: As \(A_{0}\) is d-separated from \(R^{\prime}\) conditioned on \(S^{\prime}\) and \(S_{0}\) (c.f. Fig. 6b) we can use the implied conditional independence to prove the proposition:

\[p^{\pi}(A_{0}=a\mid S_{0}=s,R^{\prime}=r^{\prime})\] (102) \[=\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S^{\prime}=s^{\prime},A_{ 0}=a\mid S_{0}=s,R^{\prime}=r^{\prime})\] (103) \[=\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S^{\prime}=s^{\prime} \mid S_{0}=s,R^{\prime}=r^{\prime})p^{\pi}(A_{0}=a\mid S_{0}=s,S^{\prime}=s^{ \prime},R^{\prime}=r^{\prime})\] (104) \[=\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S^{\prime}=s^{\prime} \mid S_{0}=s,R^{\prime}=r^{\prime})p^{\pi}(A_{0}=a\mid S_{0}=s,S^{\prime}=s^{ \prime})\] (105)

### Continuous action spaces

In the previous section, we discussed how continuous state spaces can lead to spurious contributions resulting high variance for the HCA estimator. Here, we briefly discuss how COCOA can be applied to continuous action spaces, and how Proposition 2 translates to this setting.

Gradient estimator.We can adjust the COCOA gradient estimator of Eq. 4 towards continuous action spaces by replacing the sum over \(a^{\prime}\mathcal{A}\) by an integral over the action space \(A^{\prime}\).

\[\hat{\nabla}_{\theta}^{U}V^{\pi}(s_{0})=\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_ {t}\mid S_{t})R_{t}+\int_{\mathcal{A}}\mathrm{d}a\;\nabla_{\theta}\pi(a\mid S _{t})\sum_{k\geq 1}w(S_{t},a,U_{t+k})R_{t+k}\] (106)

In general, computing this integral is intractable. We can approximate the integral by standard numerical integration methods, introducing a bias due to the approximation. Alternatively, we introduced in App. C.3 another variant of the COCOA gradient estimator that samples independent actions \(A^{\prime}\) from the policy instead of summing over the whole action space. This variant can readily be applied to continuous action spaces, resulting in

\[\hat{\nabla}_{\theta}V^{\pi}=\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S _{t})R_{t}+\frac{1}{M}\sum_{m}\nabla_{\theta}\log\pi(a^{m}\mid S_{t})\sum_{k=1} ^{\infty}w(S_{t},a^{m},U_{t+k})R_{t+k}\] (107)

where we sample \(M\) actions independently from \(a^{m}\sim\pi(\cdot\mid S_{t})\). This gradient estimator is unbiased, but introduces extra variance through the sampling of actions.

Spurious contributions.Akin to discrete action spaces, HCA in continuous action spaces can suffer from spurious contributions when distinct action sequences lead to unique states. In this case, previous actions can be perfectly decoded from the hindsight state, and we have that the probability density function \(p^{\pi}(A_{0}=a\mid S_{0}=s,S^{\prime}=s^{\prime})\) is equal to the Dirac delta function \(\delta(a=a_{0})\), with \(a_{0}\) the action taken in the trajectory that led to \(s^{\prime}\). Substituting this Dirac delta function into the policy gradient estimator of Eq. 106 results in the high-variance REINFORCE estimator. When we use the COCOA estimator of Eq. 107 using action sampling, HCA will even have a higher variance compared to REINFORCE.

### Partially Observable MDPs

In many environments, agents do not have access to the complete state information \(s\), but instead get observations with incomplete information. Partially observable Markov decision processes (POMDPs) formalize this case by augmenting MDPs with an observation space \(\mathcal{O}\). Instead of directly observing Markov states, the agent now acquires an observation \(o_{t}\) at each time step. The probability of observing \(o\) in state \(s\) after performing action \(a\) is given by \(p_{o}(o\mid s,a)\).

A popular strategy in deep RL methods to handle partial observability is learning an internal state \(x_{t}\) that summarizes the observation history \(h_{t}=\{o_{t^{\prime}},a_{t^{\prime}},r_{t^{\prime}}\}_{t^{\prime}=0}^{t^{ \prime}}\), typically by leveraging recurrent neural networks [85, 86, 87, 88]. This internal state is then used as input to a policy or value function. This strategy is intimately connected to estimating _belief states_[89]. The observation history \(h_{t}\) provides us with information on what the current underlying Markov state \(s_{t}\) is. We can formalize this by introducing the _belief state_\(b_{t}\), which captures the sufficient statistics for the probability distribution over the Markov states \(s\) conditioned on the observation history \(h_{t}\). Theoretically, such belief states can be computed by doing Bayesian probability calculus using the environment transition model, reward model and observation model:

\[p_{B}(s\mid b_{t})=p(S_{t}=s\mid H_{t}=h_{t})\] (108)

Seminal work has shown that a POMDP can be converted to an MDP, using the belief states \(b\) as new Markov states instead of the original Markov states \(s\)[90]. Now the conventional optimal control techniques can be used to solve the belief-state MDP, motivating the use of standard RL methods designed for MDPs, combined with learning an internal state \(x\).

HCA suffers from spurious contributions in POMDPs.As the internal state \(x\) summarizes the complete observation history \(h\), past actions can be accurately decoded based on \(h\), causing HCA to degrade to the high-variance REINFORCE estimator (c.f. Proposition 2). Here, the tension between forming good state representations for enabling capable policies and good representations for backward credit assignment is pronounced clearly. To enable optimal decision-making, a good internal state \(x\) needs to encode which underlying Markov states the agent most likely occupies, as well as the corresponding uncertainty. To this end, the internal state needs to incorporate information about the full history. However, when using the same internal state for backward credit assignment, this leads to spurious contributions, as previous actions are encoded directly into the internal state.

To make the spurious contributions more tangible, let us consider a toy example where the state space \(\mathcal{S}\) consists of three states \(x\), \(y\) and \(z\). We assume the internal state represents a belief state \(b=\{b^{1},b^{2}\}\), which is a sufficient statistic for the belief distribution:

\[p(S=x\mid b)=b^{1},\ \ p(S=y\mid b)=b^{2},\ \ p(S=z\mid b)=1-b^{1}-b^{2}\] (109)

We assume that \(b_{t}\) is deterministically computed from the history \(h_{t}\), e.g. by an RNN. Now consider that at time step \(k\), our belief state is \(b_{k}=\{0.5,0.25\}\) and we get a reward that resulted from the Markov state \(x\). As the belief states are deterministically computed, the distribution \(p^{\pi}(B^{\prime}=b^{\prime}\mid b,a)\) is a Dirac delta distribution. Now consider that the action \(a\) does not influence the belief distribution over the rewarding state \(x\), but only changes the belief distribution over the non-rewarding states (e.g. \(B_{k}=\{0.5,0.23\}\) instead of \(B_{k}=\{0.5,0.25\}\) when taking action \(a^{\prime}\) instead of \(a\)). As the Dirac delta distributions \(p^{\pi}(B^{\prime}=b^{\prime}\mid b,a)\) for different \(a\) do not overlap, we get a high contribution coefficient for the action \(a_{0}\) that was taken in the actual trajectory that led to the belief state \(b^{\prime}\), and a low contribution coefficient for all other actions, even though the actions did not influence the distribution over the rewarding state.

The spurious contributions originate from measuring contributions towards reaching a certain internal belief state, while ignoring that the same reward could be obtained in different belief states as well.

Adapting Proposition 8 towards these internal belief states provides further insight on the difference between HCA and COCOA-reward:

\[p^{\pi}(A_{0}=a\mid X_{0}=x,R^{\prime}=r^{\prime})=\sum_{X^{\prime}\in\mathcal{ X}}p^{\pi}(X^{\prime}=x^{\prime}\mid X_{0}=x,R^{\prime}=r^{\prime})p^{\pi}(A_{0}=a \mid X_{0}=x,X^{\prime}=x^{\prime})\] (110)

Here, we see that the contribution coefficients of COCOA-reward take into account that the same reward can be obtained while being in different internal belief states \(x^{\prime}\) by averaging over them, while HCA only considers a single internal state.

## Appendix H Learning contribution coefficients from non-rewarding observations

### Latent learning

Building upon HCA [1], we learn the contribution coefficients (3) by approximating the hindsight distribution \(p^{\pi}(a\mid s,u^{\prime})\). The quality of this model is crucial for obtaining low-bias gradient estimates. However, its training data is scarce, as it is restricted to learn from on-policy data, and rewarding observations in case the reward or rewarding object is used as encoding \(U\). We can make the model that approximates the hindsight distribution less dependent on the policy by providing the policy logits as input (c.f. Proposition 5). Enabling COCOA-reward or COCOA-feature to learn from non-rewarding states is a more fundamental issue, as in general, the rewarding outcome encodings corresponding to zero rewards do not share any features with those corresponding to non-zero rewards.

Empowering COCOA with _latent learning_ is an interesting direction to make the learning of contribution coefficients more sample efficient. We refer to _latent learning_ as learning useful representations of the environment structure without requiring task information, which we can then leverage for learning new tasks more quickly [91]. In our setting, this implies learning useful representations in the hindsight model without requiring rewarding observations, such that when new rewards are encountered, we can quickly learn the corresponding contribution coefficients, leveraging the existing representations.

### Optimal rewarding outcome encodings for credit assignment.

Theorem 3 shows that the less information a rewarding outcome encoding \(U\) contains, the lower the variance of the corresponding COCOA gradient estimator (4). Latent learning on the other hand considers the sample-efficiency and corresponding bias of the learned contribution coefficients: when the hindsight model can learn useful representations with encodings \(U\) corresponding to zero rewards, it can leverage those representations to quickly learn the contribution coefficients for rewarding outcome encodings with non-zero rewards, requiring less training data to achieve a low bias.

These two requirements on the rewarding outcome encoding \(U\) are often in conflict. To obtain a low-variance gradient estimator, \(U\) should retain as little information as possible while still being predictive of the reward. To enable latent learning for sample-efficient learning of the contribution coefficients, the hindsight model needs to pick up on recurring structure in the environment, requiring keeping as much information as possible in \(U\) to uncover the structural regularities. Using the state as rewarding outcome encoding is beneficial for enabling latent learning, as it contains rich information on the environment structure, but results in spurious contributions and a resulting high variance. Using the rewards or rewarding object as rewarding outcome encoding removes spurious contributions resulting in low variance, but renders latent learning difficult.

A way out of these conflicting pressures for an optimal rewarding outcome encoding is to use rewards or rewarding objects as the optimal encoding for low variance, but extract these contribution coefficients from models that allow for sample-efficient, latent learning. One possible strategy is to learn probabilistic world models [20, 21, 88] which can be done using both non-rewarding and rewarding observations, and use those to approximate the contribution coefficients of Eq. 3. Another strategy that we will explore in more depth is to learn hindsight models based on the state as rewarding outcome encoding to enable latent learning, but then recombine those learned hindsight models to obtain contribution coefficients using the reward or rewarding object as \(U\).

### Counterfactual reasoning on rewarding states

HCA results in spurious contributions because it computes contributions towards reaching a precise rewarding state, while ignoring that the same reward could be obtained in other (nearby) states.

of the state-based contribution coefficients by leveraging counterfactual reasoning on rewarding states. Here, we obtain contribution coefficients for a certain rewarding outcome encoding (e.g. the rewarding object) by considering which other states \(s^{\prime}\) could lead to the same rewarding outcome, and averaging over the corresponding coefficients \(w(s,a,s^{\prime})\).

**Proposition 9**.: _Assuming \(S^{\prime}\) is fully predictive of \(U^{\prime}\), we have that_

\[w(s,a,u^{\prime})=\sum_{s^{\prime}\in S}p^{\pi}(S^{\prime}=s^{\prime}\mid S_{0 }=s,U^{\prime}=u^{\prime})w(s,a,s^{\prime})\] (111)

The proof follows the same technique as that of Proposition 8. Proposition 8) shows that it is possible to learn state-based contribution coefficients, enabling latent learning, and obtain a low-variance COCOA estimator by recombining the state-based contribution coefficients into coefficients with less spurious contributions, if we have access to the generative model \(p^{\pi}(S^{\prime}=s^{\prime}\mid S_{0}=s,U^{\prime}=u^{\prime})\). This model embodies the counterfactual reasoning on rewarding states: 'which other states \(s^{\prime}\) are likely considering I am currently in \(u^{\prime}\) and visited state \(s\) somewhere in the past'.

In general, learning this generative model is as difficult or more difficult than approximating the hindsight distribution \(p^{\pi}(a\mid s,u^{\prime})\), as it requires rewarding observations as training data. Hence, it is not possible to directly use Proposition 9 to combine latent learning with low-variance estimators. In the following section, we propose a possible way forward to circumvent this issue.

### Learning credit assignment representations with an information bottleneck

Here, we outline a strategy where we learn a latent representation \(Z\) that retains useful information on the underlying states \(S\), and crucially has a latent space structure such that \(p^{\pi}(Z^{\prime}=z^{\prime}\mid S=s,U^{\prime}=u^{\prime})\) is easy to approximate. Then, leveraging Proposition 9 (and replacing \(S^{\prime}\) by \(Z^{\prime}\)) allows us to learn a hindsight representation based on \(Z\), enabling latent learning, while reducing the spurious contributions by counterfactual reasoning with \(p^{\pi}(z^{\prime}\mid s,u^{\prime})\).

To achieve this, we use the Variational Information Bottleneck approach [83; 92], closely related to the \(\beta\) Variational Autoencoder [93]. Fig. 13 shows the graphical model with the relations between the various variables and encoding: we learn a probabilistic encoding \(p(Z\mid S,A;\theta)\) parameterized by \(\theta\), and we assume that the latent variable \(Z\) is fully predictive of the rewarding outcome encoding \(U\) and reward \(R\). We aim to maximize the mutual information \(\mathcal{I}(Z^{\prime};S^{\prime},A^{\prime}\mid S,U^{\prime})\) under some information bottleneck. We condition on \(S\) and \(U^{\prime}\), to end up later with a decoder and prior variational model that we can combine with Proposition 9.

Following rate-distortion theory, Alemi et al. [92] consider the following tractable variational bounds on the mutual information

\[H-D\leq\mathcal{I}(Z^{\prime};S^{\prime},A^{\prime}\mid S,U^{\prime})\leq \text{Rate}\] (112)

with entropy \(H\), distortion \(D\) and rate defined as follows:

\[H =-\mathbb{E}_{S^{\prime},A^{\prime},S,U^{\prime}}[\log p^{\pi}(S^ {\prime},A^{\prime}\mid S,U^{\prime})]\] (113) \[D =-\mathbb{E}_{S,U^{\prime}}\left[\mathbb{E}_{S^{\prime},A^{\prime }\mid S,U^{\prime}}\left[\int\mathrm{d}z^{\prime}p(z^{\prime}\mid s^{\prime},a^{ \prime};\theta)\log q(s^{\prime},a^{\prime}\mid s,z^{\prime};\psi)\right]\right]\] (114) \[\text{Rate} =\mathbb{E}_{S,U^{\prime}}\left[\mathbb{E}_{S^{\prime},A^{\prime }\mid S,U^{\prime}}\left[D_{\mathrm{KL}}\big{(}p(z^{\prime}\mid s^{\prime},a^{ \prime};\theta)\|q(z^{\prime}\mid s,u^{\prime};\phi)\big{)}\right]\right]\] (115)

where the _decoder_\(q(s^{\prime},a^{\prime}\mid s,z^{\prime};\psi)\) is a variational approximation to \(p(s^{\prime},a^{\prime}\mid s,z^{\prime})\), and the _marginal_\(q(z^{\prime}\mid s,u^{\prime};\phi)\) is a variational approximation to the true marginal \(p(z^{\prime}\mid s,u^{\prime})\). The distortion \(D\) quantifies how well we can decode the state-action pair \((s^{\prime},a^{\prime})\) from the encoding

Figure 13: Schematic of the graphical model used in our variational information bottleneck approach.

\(z^{\prime}\), by using a decoder \(q(s^{\prime},a^{\prime}\mid s,z^{\prime};\psi)\) parameterized by \(\psi\). The distortion is reminiscent of an autoencoder loss, and hence encourages the encoding \(p(z^{\prime}\mid s^{\prime},a^{\prime})\) to retain as much information as possible on the state-action pair \((s^{\prime},a^{\prime})\). The rate measures the average KL-divergence between the encoder and variational marginal. In information theory, this rate measures the extra number of bits (or nats) required to encode samples from \(Z^{\prime}\) with an optimal code designed for the variational marginal \(q(z^{\prime}\mid s,u^{\prime};\phi)\).

We can use the rate to impose an information bottleneck on \(Z^{\prime}\). If we constrain the rate to \(\text{Rate}\leq a\) with \(a\) some positive constant, we restrict the amount of information \(Z^{\prime}\) can encode about \((S^{\prime},A^{\prime})\), as \(p(z^{\prime}\mid s^{\prime},a^{\prime};\theta)\) needs to remain close to the marginal \(q(z^{\prime}\mid s,u^{\prime};\phi)\), quantified by the KL-divergence. We can maximize the mutual information \(\mathcal{I}(Z^{\prime};S^{\prime},A^{\prime}\mid S,U^{\prime})\) under the information bottleneck by minimizing the distortion under this rate constraint. Instead of imposing a fixed constraint on the rate, we can combine the rate and distortion into an unconstrained optimization problem by leveraging the Lagrange formulation

\[\min_{\theta,\phi,\psi}D+\beta\text{Rate}\] (116)

Here, the \(\beta\) parameter determines the strength of the information bottleneck. This formulation is equivalent to the \(\beta\) Variational Autoencoder [93], and for \(\beta=1\) we recover the Variational Autoencoder [94].

To understand why this information bottleneck approach is useful to learn credit assignment representations \(Z\), we examine the rate in more detail. We can rewrite the rate as

\[\text{Rate}= \mathbb{E}_{S,U^{\prime}}\left[D_{\text{KL}}\big{(}p(z^{\prime} \mid s,u^{\prime})\|q(z^{\prime}\mid s,u^{\prime};\phi)\big{)}\right]+\] (117) \[\mathbb{E}_{S,U^{\prime},Z^{\prime}}\left[D_{\text{KL}}\big{(}p( s^{\prime},a^{\prime}\mid s,z^{\prime})\|p(s^{\prime},a^{\prime}\mid s,u^{ \prime})\big{)}\right]\] (118)

Hence, optimizing the information bottleneck objective (116) w.r.t. \(\phi\) fits the variational marginal \(q(z^{\prime}\mid s,u^{\prime};\phi)\) to the true marginal \(p(z^{\prime}\mid s,u^{\prime})\) induced by the encoder \(p(z^{\prime}\mid s^{\prime},a^{\prime};\theta)\). Proposition 9 uses this true marginal to recombine coefficients based on \(Z\) into coefficients based on \(U\). Hence, by optimizing the information bottleneck objective, we learn a model \(q(z^{\prime}\mid s,u^{\prime};\phi)\) that approximates the true marginal, which we then can use to obtain contribution coefficients with less spurious contributions by leveraging Proposition 9. Furthermore, minimizing the rate w.r.t. \(\theta\) shapes the latent space of \(Z^{\prime}\) such that the true marginal \(p(z^{\prime}\mid s,u^{\prime})\) moves closer towards the variational marginal \(q(z^{\prime}\mid s,u^{\prime};\phi)\).

In summary, the outlined information bottleneck approach for learning a credit assignment representation \(Z\) is a promising way forward to merge the powers of latent learning with a low-variance COCOA gradient estimator. The distortion and rate terms in the information bottleneck objective of Eq. 116 represent a trade-off parameterized by \(\beta\). Minimizing the distortion results in a detailed encoding \(Z^{\prime}\) with high mutual information with the state-action pair \((S^{\prime},A^{\prime})\), which can be leveraged for latent learning. Minimizing the rate shapes the latent space of \(Z^{\prime}\) in such way that the true marginal \(p(z^{\prime}\mid s,u^{\prime})\) can be accurately approximated within the variational family of \(q(z^{\prime}\mid s,u^{\prime};\phi)\), and fits the parameters \(\phi\) resulting in an accurate marginal model. We can then leverage the variational marginal \(q(z^{\prime}\mid s,u^{\prime};\phi)\) to perform counterfactual reasoning on the rewarding state encodings \(Z^{\prime}\) according to Proposition 9, resulting in a low-variance COCOA-estimator.

## Appendix I Contribution analysis and causality

### Causal interpretation of COCOA

COCOA is closely connected to causality theory [42], where the contribution coefficients (3) correspond to performing \(Do-interventions\) on the causal graph to estimate their effect on future rewards. To formalize this connection with causality, we need to use a new set of tools beyond conditional probabilities, as causation is in general not the same as correlation. We start with representing the MDP combined with the policy as a directed acyclic graphical model (c.f. Fig. (a)a in App. C.1). In causal reasoning, we have two different'modes of operation'. On the one hand, we can use observational data, corresponding to 'observing' the states of the nodes in the graphical model, which is compatible with conditional probabilities measuring correlations. On the other hand, we can perform _interventions_ on the graphical model, where we manually set a node, e.g. \(A_{t}\) to a specific value \(a\) independent from its parents \(S_{t}\), and see what the influence is on the probability distributions of other nodes in the graph, e.g. \(R_{t+1}\). These interventions are formalized with _do-calculus_[42],here we denote an intervention of putting a node \(V_{i}\) equal to \(v\) as \(\mathrm{Do}(V_{i}=v)\), and can be used to investigate causal relations.

Using the graphical model of Fig. 6b that abstracts time, we can use do-interventions to quantify the causal contribution of an action \(A_{t}=a\) taken in state \(S_{t}=s\) upon reaching the rewarding outcome \(U^{\prime}=u^{\prime}\) in the future as,

\[w_{\mathrm{Do}}(s,a,u^{\prime})=\frac{p^{\pi}\big{(}U^{\prime}=u^{\prime}\mid S _{t}=s,\mathrm{Do}(A_{t}=a)\big{)}}{\sum_{\hat{a}\in\mathcal{A}}\pi(\hat{a} \mid s)p^{\pi}\big{(}U^{\prime}=u^{\prime}\mid S_{t}=s,\mathrm{Do}(A_{t}=\hat{ a})\big{)}}-1.\] (119)

As conditioning on \(S\) satisfies the _backdoor criterion_[42] for \(U^{\prime}\) w.r.t. \(A\), the interventional distribution \(p^{\pi}\big{(}U^{\prime}=u^{\prime}\mid S_{t}=s,\mathrm{Do}(A_{t}=a)\big{)}\) is equal to the observational distribution \(p^{\pi}\big{(}U^{\prime}=u^{\prime}\mid S_{t}=s,A_{t}=a\big{)}\). Hence, the causal contribution coefficients of Eq. 119 are equal to the contribution coefficients of Eq. 3 used by COCOA.

### Extending COCOA to counterfactual interventions

Within causality theory, _counterfactual reasoning_ goes one step further than causal reasoning, by incorporating the hindsight knowledge of the external state of the world in its reasoning. Applied to COCOA, this more advanced counterfactual reasoning would evaluate the query: 'How does taking action \(a\) influence the probability of reaching a rewarding outcome, compared to taking alternative actions \(a^{\prime}\), _given everything else remains the same_'. To formalize the difference between causal and counterfactual reasoning, we need to convert the causal DAG of Figure 6a into a structural causal model (SCM), as shown in Figure 14. The SCM expresses all conditional distributions as deterministic functions with independent noise variables \(N\), akin to the reparameterization trick [94]. In causal reasoning, we perform do-interventions on nodes, which is equivalent to cutting all incoming edges to a node. To compute the resulting probabilities, we still use the prior distribution over the noise variables \(N\). Counterfactual reasoning goes one step further. First, it infers the posterior probability \(p^{\pi}(\{N\}\mid T=\tau)\), with \(\{N\}\) the set of all noise variables, given the observed trajectory \(\tau\). Then it performs a Do-intervention as in causal reasoning. However now, to compute the resulting probabilities on the nodes, it uses the posterior noise distribution combined with the modified graph.

One possible strategy to estimate contributions using the above counterfactual reasoning is to explicitly estimate the posterior noise distribution and combine it with forward dynamics models to obtain the counterfactual probability of reaching specific rewarding outcomes. Leveraging the work of Buesing et al. [43] is a promising starting point for this direction of future work. Alternatively, we can avoid explicitly modeling the posterior noise distribution, by leveraging the hindsight distribution combined with the work of Mesnard et al. [3]. Here, we aim to learn a parameterized approximation \(h\) to the counterfactual hindsight distribution \(p_{\tau}^{\pi}\big{(}A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime}\big{)}\), where the \(\tau\)-subscript indicates the counterfactual distribution incorporating the noise posterior. Building upon the approach of Mesnard et al. [3], we can learn \(h(a\mid s^{\prime},s,\Phi_{t}(\tau))\) to approximate the counterfactual hindsight distribution, with \(\Phi_{t}(\tau)\) a summary statistic trained to encapsulate the information of the posterior noise distribution \(p(\{N\}\mid T=\tau)\). Mesnard et al. [3] show that such a summary statistic \(\Phi_{t}(\tau)\) can be used to amortize the posterior noise estimation if it satisfies the following two conditions: (i) it needs to provide useful information for predicting the counterfactual hindsight distribution and (ii) it needs to be independent from the action \(A_{t}\). We can achieve both characteristics by (i) training \(h(a\mid s^{\prime},s,\Phi_{t}(\tau))\) on the hindsight action classification task and backpropagating the gradients to \(\Phi_{t}(\tau)\), and (ii) training \(\Phi_{t}\) on an _independence maximization_ loss \(\mathcal{L}_{\mathrm{IM}}(s)\), which is minimized iff

Figure 14: Structural causal model (SCM) of the MDP. Squares represent deterministic functions and circles random variables.

\(A_{t}\) and \(\Phi_{t}\) are conditionally independent given \(S_{t}\). An example is to minimize the KL divergence between \(\pi(a_{t}\mid s_{t})\) and \(p(a_{t}\mid s_{t},\Phi_{t}(\tau))\) where the latter can be approximated by training a classifier \(q(a_{t}\mid s_{t},\Phi_{t}(\tau))\). Leveraging this approach to extend COCOA towards counterfactual interventions is an exciting direction for future research.

## Appendix J Contribution analysis with temporal discounting

As discussed in App. B, we can implicitly incorporate discounting into the COCOA framework by adjusting the transition probabilities to have a fixed probability of \((1-\gamma)\) of transitioning to the absorbing state \(s_{\infty}\) at each time step.

We can also readily incorporate explicit time discounting into the COCOA framework, which we discuss here. We consider now a discounted MDP defined as the tuple \((\mathcal{S},\mathcal{A},p,p_{r},\gamma)\), with discount factor \(\gamma\in[0,1]\), and \((\mathcal{S},\mathcal{A},p,p_{r})\) as defined in Section 2. The discounted value function \(V_{\gamma}^{\pi}(s)=\mathbb{E}_{T\sim\mathcal{T}(s,\pi)}\sum_{t=0}^{\infty} \gamma^{t}R_{t}]\) and action value function \(Q_{\gamma}^{\pi}(s,a)=\mathbb{E}_{T\sim\mathcal{T}(s,a,\pi)}\sum_{t=0}^{ \infty}\gamma^{t}R_{t}]\) are the expected discounted return when starting from state \(s\), or state \(s\) and action \(a\) respectively. Table 6 shows the policy gradient estimators of \(V_{\gamma}^{\pi}(s)\) for REINFORCE, Advantage and Q-critic.

In a discounted MDP, it matters at which point in time we reach a rewarding outcome \(u\), as the corresponding rewards are discounted. Hence, we adjust the contribution coefficients to

\[w_{\gamma}(s,a,u^{\prime}) =\frac{\sum_{k\geq 1}\gamma^{k}p^{\pi}(U_{t+k}=u^{\prime}\mid S _{t}=s,A_{t}=a)}{\sum_{k\geq 1}\gamma^{k}p^{\pi}(U_{t+k}=u^{\prime}\mid S_{t}=s)}-1\] (120) \[=\frac{p_{\gamma}^{\pi}(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime })}{\pi(a\mid s)}-1\] (121)

Here, we define the discounted hindsight distribution \(p_{\gamma}^{\pi}(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})\) similarly to the undiscounted hindsight distribution explained in App. C.1, but now using a different probability distribution on the time steps \(k\): \(p_{\beta}(K=k)=(1-\beta)\beta^{k-1}\), where we take \(\beta=\gamma\). We can readily extend Theorems 1-4 to the explicit discounted setting, by taking \(\beta=\gamma\) instead of the limit of \(\beta\to 1\), and using the discounted COCOA policy gradient estimator shown in Table 6.

To approximate the discounted hindsight distribution \(p_{\gamma}^{\pi}(A_{t}=a\mid S_{t}=s,U^{\prime}=u^{\prime})\), we need to incorporate the temporal discounting into the classification cross-entropy loss:

\[L_{\gamma}=\mathbb{E}_{\pi}\left[\sum_{t\geq 0}\sum_{k\geq 1}\gamma^{k}CE\big{(}h _{\gamma}(\cdot\mid S_{t},U_{t+k}),\delta(a=A_{t})\big{)}\right],\] (122)

with \(CE\) the cross-entropy loss, \(h_{\gamma}(\cdot\mid S_{t},U_{t+k})\) the classification model that approximates the discounted hindsight distribution, and \(\delta(a=A_{t})\) a one-hot encoding of \(A_{t}\).

## Appendix K Bootstrapping with COCOA

Here, we show how COCOA can be combined with n-step returns, and we make a correction to Theorem 7 of Harutyunyan et al. [1] which considers n-step returns for HCA.

Consider the graphical model of Fig. 14(a) where we model time, \(K\), as a separate node in the graphical model (c.f. App. C.1). To model \(n\)-step returns, we now define the following prior probability on \(K\)

\begin{table}
\begin{tabular}{l l} \hline \hline
**Method** & **Policy gradient estimator** (\(\hat{\nabla}_{\theta}V^{\pi}(s_{0})\)) \\ \hline REINFORCE & \(\sum_{t\geq 0}\gamma^{t}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\sum_{k\geq 0} \gamma^{k}R_{t+k}\) \\ Advantage & \(\sum_{t\geq 0}\gamma^{t}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\left(\sum_{k \geq 0}\gamma^{k}R_{t+k}-V_{\gamma}^{\pi}(S_{t})\right)\) \\ Q-critic & \(\sum_{t\geq 0}\gamma^{t}\sum_{a\in\mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})Q_{ \gamma}^{\pi}(S_{t},a)\) \\ \hline COCOA & \(\sum_{t\geq 0}\gamma^{t}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})R_{t}+\sum_{a\in \mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})\sum_{k\geq 1}\gamma^{k}w_{\gamma}(S_{t},a,U_{t+k})R _{t+k}\) \\ HCA+ & \(\sum_{t\geq 0}\gamma^{t}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})R_{t}+\sum_{a\in \mathcal{A}}\nabla_{\theta}\pi(a\mid S_{t})\sum_{k\geq 1}\gamma^{k}w_{\gamma}(S_{t},a,S_{t+k})R _{t+k}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of discounted policy gradient estimatorsparameterized by \(\beta\):

\[p_{n,\beta}(K=k)=\begin{cases}\frac{\beta^{k}}{z}&\text{if }\;1\leq k\leq n-1\\ 0&\text{else}\end{cases}\quad z=\beta\frac{1-\beta^{n-1}}{1-\beta}\] (123)

Using \(p_{n,\beta}^{\pi}\) as the probability distribution induced by this graphical model, we have that

\[p_{n,\beta}^{\pi}(U^{\prime}=u\mid S=s,A=a) =\sum_{k}p_{n,\beta}^{\pi}(U^{\prime}=u,K=k\mid S=s,A=a)\] (124) \[=\sum_{k}p_{n,\beta}(K=k)p_{n,\beta}^{\pi}(U^{\prime}=u\mid S=s, A=a,K=k)\] (125) \[=\frac{1-\beta}{\beta(1-\beta^{n-1})}\sum_{k=1}^{n-1}\beta^{k}p^ {\pi}(U_{k}=u\mid S_{0}=s,A_{0}=a)\] (126)

We introduce the following contribution coefficients that we will use for n-step returns

\[w_{n,\beta}(s,a,u^{\prime}) =\frac{\sum_{k=1}^{n-1}\beta^{k}p^{\pi}(U_{k}=u^{\prime}\mid S_{0 }=s,A_{0}=a)}{\sum_{k=1}^{n-1}\beta^{k}p^{\pi}(U_{k}=u^{\prime}\mid S_{0}=s)}\] (127) \[=\frac{p_{n,\beta}^{\pi}(U^{\prime}=u^{\prime}\mid S=s,A=a)}{p_{ n,\beta}^{\pi}(U^{\prime}=u^{\prime}\mid S=s)}-1=\frac{p_{n,\beta}^{\pi}(A=a \mid S=s,U^{\prime}=u^{\prime})}{\pi(a\mid s)}-1\] (128) \[w_{n}(s,a,s^{\prime}) =\frac{p^{\pi}(S_{n}=s^{\prime}\mid S_{0}=s,A_{0}=a)}{p^{\pi}(S_ {n}=s^{\prime}\mid S_{0}=s)}-1=\frac{p^{\pi}(A=a\mid S=s,S_{n}=s^{\prime})}{ \pi(a\mid s)}-1\] (129)

Now we are ready to prove the n-step return theorem for the discounted MDP setting. We can recover the undiscounted setting by taking the limit of \(\gamma\to 1_{-}\).

**Theorem 10**.: _Consider state \(s\) and action \(a\) for which it holds that \(\pi(a\mid s)>0\) and take \(\beta\) equal to the discount factor \(\gamma\in[0,1]\). Furthermore, assume that the rewarding outcome encoding \(u=f(s,a,r)\) is fully predictive of the reward (c.f. Definition 2). Then the advantage \(A^{\pi}(s,a)=Q^{\pi}(s,a)-V^{\pi}(s)\) is equal to_

\[A^{\pi}(s,a)=r(s,a)-r^{\pi}(s)+\mathbb{E}_{\mathcal{T}(s,\pi)}\left[\sum_{k=1 }^{n-1}\gamma^{k}w_{n,\beta}(s,a,U_{k})R_{k}+\gamma^{n}w_{n}(s,a,S_{n})V^{\pi} (S_{n})\right]\]

_with \(r(s,a)\) the reward model and \(r^{\pi}(s)=\sum_{a}\pi(a\mid s)r(s,a)\)._

Proof.: We start with the action-value function \(Q^{\pi}\), and will subtract the value \(V^{\pi}\) to obtain the result on the advantage function.

\[Q(s,a) =\mathbb{E}_{T\sim\mathcal{T}(s,a,\pi)}\left[\sum_{k\geq 1} \gamma^{k}R_{k}\right]\] (130) \[=r(s,a)+\sum_{r^{\prime}\in\mathcal{R}}\sum_{k\geq 1}\gamma^{k}p^ {\pi}(R_{k}=r^{\prime}\mid s,a)r^{\prime}\] (131)

Figure 15: (a) Graphical model where we abstract time. (b) Graphical model implicitly used in the proof of Theorem 7 in Harutyunyan et al. [1].

\[=r(s,a)+\sum_{r^{\prime}\in\mathcal{R}}\sum_{u^{\prime}\in\mathcal{U}} \sum_{k=1}^{n-1}\gamma^{k}p^{\pi}(R_{k}=r^{\prime},U_{k}=u^{\prime}\mid s,a)r^{ \prime}+\] (132) \[\qquad\gamma^{n}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S_{n}=s^{ \prime}\mid s,a)V^{\pi}(s^{\prime})\] (133) \[=r(s,a)+\sum_{r^{\prime}\in\mathcal{R}}\sum_{u^{\prime}\in \mathcal{U}}\sum_{k=1}^{n-1}\gamma^{k}p^{\pi}(R^{\prime}=r^{\prime}\mid U^{ \prime}=u^{\prime})p^{\pi}(U_{k}=u^{\prime}\mid s,a)r^{\prime}+\] (134) \[\qquad\gamma^{n}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S_{n}=s^{ \prime}\mid s,a)V^{\pi}(s^{\prime})\] (135) \[=r(s,a)+\sum_{u^{\prime}\in\mathcal{U}}r(u^{\prime})\sum_{k=1}^{n -1}\gamma^{k}p^{\pi}(U_{k}=u^{\prime}\mid s,a)+\gamma^{n}\sum_{s^{\prime}\in \mathcal{S}}p^{\pi}(S_{n}=s^{\prime}\mid s,a)V^{\pi}(s^{\prime})\] (136) \[=r(s,a)+\sum_{u^{\prime}\in\mathcal{U}}r(u^{\prime})\sum_{k=1}^{ n-1}\gamma^{k}p^{\pi}(U_{k}=u^{\prime}\mid s)\frac{\sum_{k^{\prime}=1}^{n-1} \gamma^{k}p^{\pi}(U_{k^{\prime}}=u\mid s,a)}{\sum_{k^{\prime}=1}^{n-1}\gamma^{ k^{\prime}}p^{\pi}(U_{k^{\prime}}=u\mid s)}+\] (137) \[\qquad\gamma^{n}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S_{n}=s^{ \prime}\mid s)\frac{p^{\pi}(S_{n}=s^{\prime}\mid s,a)}{p^{\pi}(S_{n}=s^{\prime }\mid s)}V^{\pi}(s^{\prime})\] (138) \[=r(s,a)+\sum_{u^{\prime}\in\mathcal{U}}r(u^{\prime})\sum_{k=1}^{ n-1}\gamma^{k}p^{\pi}(U_{k}=u^{\prime}\mid s)(w_{n,\beta}(s,a,u^{\prime})+1)\] (139) \[\qquad+\gamma^{n}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S_{n}=s^ {\prime}\mid s)(w_{n}(s,a,s^{\prime})+1)V^{\pi}(s^{\prime})\] (140)

where we use that \(U^{\prime}\) is fully predictive of the reward \(R^{\prime}\), and define \(r(u^{\prime})=\sum_{r^{\prime}\in\mathcal{R}}p(R^{\prime}=r^{\prime}\mid U^{ \prime}=u^{\prime})r^{\prime}\) By subtracting the value function, we get

\[A(s,a) =r(s,a)-r^{\pi}(s)+\sum_{u^{\prime}\in\mathcal{U}}r(u^{\prime}) \sum_{k=1}^{n-1}\gamma^{k}p^{\pi}(U_{k}=u^{\prime}\mid s)w_{n,\beta}(s,a,u^{ \prime})\] (142) \[\qquad+\gamma^{n}\sum_{s^{\prime}\in\mathcal{S}}p^{\pi}(S_{n}=s^ {\prime}\mid s)w_{n}(s,a,s^{\prime})V^{\pi}(s^{\prime})\] (143) \[=r(s,a)-r^{\pi}(s)+\operatorname{\mathbb{E}}_{\mathcal{T}(s,\pi)} \left[\sum_{k=1}^{n-1}\gamma^{k}w_{n,\beta}(s,a,U_{k})R_{k}+\gamma^{n}w_{n}(s,a,S_{n})V^{\pi}(S_{n})\right]\] (144)

Finally, we can sample from this advantage function to obtain an n-step COCOA gradient estimator, akin to Theorem 1.

Note that we require to learn the state-based contribution coefficients \(w_{n}(s,a,s^{\prime})\) to bootstrap the value function into the n-step return, as the value function requires a Markov state \(s^{\prime}\) as input instead of a rewarding outcome encoding \(u^{\prime}\). Unfortunately, these state-based contribution coefficients will suffer from spurious contributions, akin to HCA, introducing a significant amount of variance into the n-step COCOA gradient estimator. We leave it to future research to investigate whether we can incorporate value functions into an n-step return, while using rewarding-outcome contribution coefficients \(w(s,a,u^{\prime})\) instead of state-based contribution coefficients \(w_{n}(s,a,s^{\prime})\).

Learning the contribution coefficients.We can learn the contribution coefficients \(w_{\beta,n}(s,a,u^{\prime})\) with the same strategies as described in Section 3, but now with training data from \(n\)-step trajectories instead of complete trajectories. If we use a discount \(\gamma\neq 1\), we need to take this discount factor into account in the training distribution or loss function (c.f. App. J).

Correction to Theorem 7 of Harutyunyan et al. [1].Harutyunyan et al. [1] propose a theorem similar to Theorem 10, with two important differences. The first one concerns the distribution on \(K\) in the graphical model of Fig. 14(a). Harutyunyan et al. [1] implicitly use this graphical model, but with a different prior probability distribution on \(K\):

\[p_{n,\beta}^{HCA}(K=k)=\begin{cases}\beta^{k-1}(1-\beta)&\text{if}\ \ 1\leq k\leq n-1\\ \beta^{n-1}&\text{if}\ \ k=n\\ 0&\text{else}\end{cases}\] (145)

The graphical model combined with the distribution on \(K\) defines the hindsight distribution \(p_{n,\beta,HCA}^{\pi}(A=a\mid S=s,S^{\prime}=s^{\prime})\). The second difference is the specific \(Q\)-value estimator Harutyunyan et al. [1] propose. They use the hindsight distribution \(p_{n,\beta,HCA}^{\pi}(A=a\mid S=s,S^{\prime}=s^{\prime})\) in front of the value function (c.f. Theorem 10), which considers that \(s^{\prime}\) can be reached at any time step \(k\sim p_{n,\beta}^{HCA}(k)\), whereas Theorem 10 uses \(w_{n}(s,a,s^{\prime})\) which considers that \(s^{\prime}\) is reached exactly at time step \(k=n\).

To the best of our knowledge, there is an error in the proposed proof of Theorem 7 by Harutyunyan et al. [1] for which we could not find a simple fix. For the interested reader, we briefly explain the error. One indication of the problem is that for \(\beta\to 1\), all the probability mass of \(p_{n,\beta}^{HCA}(K=k)\) is concentrated at \(k=n\), hence the corresponding hindsight distribution \(p_{n,\beta,HCA}^{\pi}(A=a\mid S=s,S^{\prime}=s^{\prime})\) considers only hindsight states \(s^{\prime}\) encountered at time \(k=n\). While this is not a mathematical error, it does not correspond to the intuition of a 'time independent hindsight distribution' the authors provide. In the proof itself, a conditional independence relation is assumed that does not hold. The authors introduce a helper variable \(Z\) defined on the state space \(\mathcal{S}\), with a conditional distribution

\[\mu_{k}(Z=z\mid S^{\prime}=s^{\prime})=\begin{cases}\delta(z=s^{\prime})& \text{if}\ \ 1\leq k\leq n-1\\ \tilde{d}^{\pi}(z\mid s^{\prime})&\text{if}\ \ k=n\end{cases}\] (146)

with the normalized discounted visit distribution \(\tilde{d}^{\pi}(z\mid s^{\prime})=(1-\gamma)\sum_{k}\gamma^{k}p^{\pi}(S_{k}= z\mid S_{0}=s)\). We can model this setting as the graphical model visualized in Fig. 14(b). In the proof (last line on page 15 in the supplementary materials of Harutyunyan et al. [1]), the following conditional independence is used:

\[p^{\pi}(A_{0}=a\mid S_{0}=s,S^{\prime}=s^{\prime},Z=z)=p^{\pi}(A_{0}=a\mid S _{0}=s,S^{\prime}=s^{\prime})\] (147)

However, Fig. 14(b) shows that \(S^{\prime}\) is a collider on the path \(A_{0}\to S^{\prime}\gets K\to Z\). Hence, by conditioning on \(S^{\prime}\) we open this collider path, making \(A_{0}\) dependent on \(Z\) conditioned on \(S_{0}\) and \(S^{\prime}\), thereby invalidating the assumed conditional independence. For example, if \(Z\) is different from \(S^{\prime}\), we know that \(K=n\) (c.f. Eq. 146), hence \(Z\) can contain information about action \(A_{0}\), beyond \(S^{\prime}\), as \(S^{\prime}\) ignores at which point in time \(s^{\prime}\) is encountered.

## Appendix L HCA-return is a biased estimator in many relevant environments

### HCA-return

Besides HCA-state, Harutyunyan et al. [1] introduced HCA-return, a policy gradient estimator that leverages the hindsight distribution conditioned on the return:

\[\sum_{t\geq 0}\nabla_{\theta}\log\pi(A_{t}\mid S_{t})\Big{(}1-\frac{\pi(A_{t} \mid S_{t})}{p^{\pi}(A_{t}\mid S_{t},Z_{t})}\Big{)}Z_{t}\] (148)

When comparing this estimator with COCOA-reward, we see two important differences: (i) HCA-return uses a hindsight function conditioned on the return instead of individual rewards, and (ii) HCA-return leverages the hindsight function as an action-dependent baseline for a Monte Carlo policy gradient estimate, instead of using it for contribution coefficients to evaluate counterfactual actions. Importantly, the latter difference causes the HCA-return estimator to be biased in many environments of relevance, even when using the ground-truth hindsight distribution.

### HCA-return can be biased

An important drawback of HCA-return is that it can be biased, even when using the ground-truth hindsight distribution. Theorem 2 of Harutyunyan et al. 2019, considering the unbiasedness HCA-return, is valid under the assumption that for any possible random return \(Z\) for all possible trajectoriesstarting from state \(s\), it holds that \(p^{\pi}(a\mid s,z)>0\). This restrictive assumption requires that for each observed state-action pair \((s_{t},a_{t})\) along a trajectory, all counterfactual returns \(Z\) resulting from a counterfactual trajectory starting from \(s_{t}\) (not including \(a_{t}\)) result in \(p^{\pi}(a_{t}\mid s_{t},Z)>0\). This implies that all returns (or rewarding states) reachable from \(s_{t}\) should also be reachable from \((s_{t},a_{t})\).

Consider the following bandit setting as a simple example where the above assumption is not satisfied. The bandit has two arms, with a reward of \(1\) and \(-2\), and a policy probability of \(\frac{2}{3}\) and \(\frac{1}{3}\) respectively. The advantage for both arms is \(1\) and \(-2\). Applying eq. 6 from Harutyunyan et al. results in \(A^{\pi}(s,a_{1})=(1-\frac{2}{3})=\frac{1}{3}\) and \(A^{\pi}(s,a_{2})=-2(1-1/3)=-4/3\). This shows that the needed assumptions for an unbiased HCA-return estimator can be violated even in simple bandit settings.

## Appendix M Additional details

### Author contributions

This paper was a collaborative effort of all shared first authors working closely together. To do this fact better justice we give an idea of individual contributions in the following.

**Alexander Meulemans\({}^{*}\)**. Original idea, conceptualizing the theory and proving the theorems, conceptual development of the algorithms, experiment design, implementation of main method and environments, debugging, neural network architecture design, running experiments, connecting the project to existing literature, writing of manuscript, first draft and supplementary materials, feedback to the figures.

**Simon Schug\({}^{*}\)**. Conceptual development of the algorithms, experiment design, implementation of main method, baselines and environments, neural network architecture design, debugging, tuning and running experiments, writing of manuscript, creation of figures, writing of supplementary materials.

**Seijin Kobayashi\({}^{*}\)**. Conceptual development of the algorithms, experiment design, implementation of environments, baselines, main method and Dynamic Programming-based ground-truth methods, debugging, tuning and running experiments, feedback to the manuscript, writing of supplementary materials.

**Nathaniel Daw**. Regular project meetings, conceptual input and feedback for method and experimental design, connecting the project to existing literature, feedback to the manuscript and figures.

**Gregory Wayne**. Senior project supervision, conceptualising of the project idea, conceptual development of the algorithms, regular project meetings, technical and conceptual feedback for method and experimental design, connecting the project to existing literature, feedback to the manuscript and figures.

### Compute resources

We used Linux workstations with Nvidia RTX 2080 and Nvidia RTX 3090 GPUs for development and conducted hyperparameter searches and experiments using 5 TPUv2-8, 5 TPUv3-8 and 1 Linux server with 8 Nvidia RTX 3090 GPUs over the course of 9 months. All of the final experiments presented take less than a few hours to complete using a single Nvidia RTX 3090 GPU. In total, we spent an estimated amount of 2 GPU months.

### Software and libraries

For the results produced in this paper we relied on free and open-source software. We implemented our experiments in Python using JAX [95, Apache License 2.0] and the Deepmind Jax Ecosystem [82, Apache License 2.0]. For experiment tracking we used wandb [96, MIT license] and for the generation of plots we used plotly [97, MIT license].