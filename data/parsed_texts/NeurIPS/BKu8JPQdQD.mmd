# PUZZLES: A Benchmark for Neural

Algorithmic Reasoning

 Benjamin Estermann

ETH Zurich

esternann@ethz.ch

&Luca A. Lanzendorfer

ETH Zurich

lanzendoerfer@ethz.ch

Yannick Niedermayr

ETH Zurich

yannickn@ethz.ch

&Roger Wattenhofer

ETH Zurich

wattenhofer@ethz.ch

###### Abstract

Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments. These advancements have been enabled in part by the availability of benchmarks. In this work we introduce PUZZLES, a benchmark based on Simon Tatham's Portable Puzzle Collection, aimed at fostering progress in algorithmic and logical reasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizes and varying levels of complexity; many puzzles also feature a diverse set of additional configuration parameters. The 40 puzzles provide detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we evaluate various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research. All the software, including the environment, is available at https://github.com/ETH-DISCO/rlp.

Human intelligence relies heavily on logical and algorithmic reasoning as integral components for solving complex tasks. While Machine Learning (ML) has achieved remarkable success in addressing many real-world challenges, logical and algorithmic reasoning remains an open research question [1; 2; 3; 4; 5; 6; 7]. This research question is supported by the availability of benchmarks, which allow for a standardized and broad evaluation framework to measure and encourage progress [8; 9; 10].

Reinforcement Learning (RL) has made remarkable progress in various domains, showcasing its capabilities in tasks such as game playing [11; 12; 13; 14; 15], robotics [16; 17; 18; 19] and control systems [20; 21; 22]. Various benchmarks have been proposed to enable progress in these areas [23; 24; 25; 26; 27; 28; 29]. More recently, advances have also been made in the direction of logical and algorithmic reasoning within RL [30; 31; 32]. Popular examples also include the games of Chess, Shogi, and Go [33; 34]. Given the importance of logical and algorithmic reasoning, we propose a benchmark to guide future developments in RL and more broadly machine learning.

Logic puzzles have long been a playful challenge for humans, and they are an ideal testing ground for evaluating the algorithmic and logical reasoning capabilities of RL agents. A diverse range of puzzles, similar to the Atari benchmark [24], favors methods that are broadly applicable. Unlike tasks with a fixed input size, logic puzzles can be solved iteratively once an algorithmic solution is found. This allows us to measure how well a solution attempt can adapt and generalize to larger inputs. Furthermore, in contrast to games such as Chess and Go, logic puzzles have a known solution, making reward design easier and enabling tracking progress and guidance with intermediate rewards.

In this paper, we introduce PUZZLES, a comprehensive RL benchmark specifically designed to evaluate RL agents' algorithmic reasoning and problem-solving abilities in the realm of logical and algorithmic reasoning. Simon Tatham's Puzzle Collection [35], curated by the renowned computer programmer and puzzle enthusiast Simon Tatham, serves as the foundation of PUZZLES. This collection includes a set of 40 logic puzzles, shown in Figure 1, each of which presents distinct challenges with various dimensions of adjustable complexity. They range from more well-known puzzles, such as _Solo_ or _Mines_ (commonly known as _Sudoku_ and _Minesweeper_, respectively) to lesser-known puzzles such as _Cube_ or _Slant_. PUZZLES includes all 40 puzzles in a standardized environment, each playable with a visual or discrete input and a discrete action space.

Contributions.We propose PUZZLES, an RL environment based on Simon Tatham's Puzzle Collection, comprising a collection of 40 diverse logic puzzles. To ensure compatibility, we have extended the original C source code to adhere to the standards of the Pygame library. Subsequently, we have integrated PUZZLES into the Gymnasium framework API, providing a straightforward, standardized, and widely-used interface for RL applications. PUZZLES allows the user to arbitrarily scale the size and difficulty of logic puzzles, providing detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we have evaluated various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research.

## 1 Related Work

RL benchmarks.Various benchmarks have been proposed in RL. Bellemare et al. [24] introduced the influential Atari-2600 benchmark, on which Mnih et al. [11] trained RL agents to play the games directly from pixel inputs. This benchmark demonstrated the potential of RL in complex, high-dimensional environments. PUZZLES allows the use of a similar approach where only pixel inputs are provided to the agent. Todorov et al. [23] presented MuJoCo which provides a diverse set of continuous control tasks based on a physics engine for robotic systems. Another control benchmark is the DeepMind Control Suite by Duan et al. [26], featuring continuous actions spaces and complex control problems. The work by Cote et al. [28] emphasized the importance of natural language understanding in RL and proposed a benchmark for evaluating RL methods in text-based domains. Lanctot et al. [29] introduced OpenSpiel, encompassing a wide range of games, enabling researchers to evaluate and compare RL algorithms' performance in game-playing scenarios. These benchmarks and frameworks have contributed significantly to the development and evaluation of RL algorithms. OpenAI Gym by Brockman et al. [25], and its successor Gymnasium by the Farama Foundation [36], helped by providing a standardized interface for many benchmarks. As such, Gym and Gymnasium

Figure 1: All puzzle classes of Simon Tathamâ€™s Portable Puzzle Collection.

have played an important role in facilitating reproducibility and benchmarking in reinforcement learning research. Therefore, we provide PUZZLES as a Gymnasium environment to enable ease of use.

Logical and algorithmic reasoning within RL.Notable research in RL on logical reasoning includes automated theorem proving using deep RL [16] or RL-based logic synthesis [37]. Dasgupta et al. [38] find that RL agents can perform a certain degree of causal reasoning in a meta-reinforcement learning setting. The work by Jiang and Luo [30] introduces Neural Logic RL, which improves interpretability and generalization of learned policies. Eppe et al. [39] provide steps to advance problem-solving as part of hierarchical RL. Fawzi et al. [31] and Mankowitz et al. [32] demonstrate that RL can be used to discover novel and more efficient algorithms for well-known problems such as matrix multiplication and sorting. Neural algorithmic reasoning has also been used as a method to improve low-data performance in classical RL control environments [40; 41]. Logical reasoning might be required to compete in certain types of games such as chess, shogi and Go [33; 34; 42; 13], Poker [43; 44; 45; 46] or board games [47; 48; 49; 50]. However, these are usually multi-agent games, with some also featuring imperfect information and stochasticity.

Reasoning benchmarks.Various benchmarks have been introduced to assess different types of reasoning capabilities, although only in the realm of classical ML. IsarStep, proposed by Li et al. [8], specifically designed to evaluate high-level mathematical reasoning necessary for proof-writing tasks. Another significant benchmark in the field of reasoning is the CLRS Algorithmic Reasoning Benchmark, introduced by Velickovic et al. [9]. This benchmark emphasizes the importance of algorithmic reasoning in machine learning research. It consists of 30 different types of algorithms sourced from the renowned textbook "Introduction to Algorithms" by Cormen et al. [51]. The CLRS benchmark serves as a means to evaluate models' understanding and proficiency in learning various algorithms. In the domain of large language models (LLMs), BIG-bench has been introduced by Srivastava et al. [10]. BIG-bench incorporates tasks that assess the reasoning capabilities of LLMs, including logical reasoning.

Despite these valuable contributions, a suitable and unified benchmark for evaluating logical and algorithmic reasoning abilities in single-agent perfect-information RL has yet to be established. Recognizing this gap, we propose PUZZLES as a relevant and necessary benchmark with the potential to drive advancements and provide a standardized evaluation platform for RL methods that enable agents to acquire algorithmic and logical reasoning abilities.

## 2 The Puzzles Environment

In the following section, we give an overview of the PUZZLES environment.1The environment is written in both Python and C. For a detailed explanation of all features of the environment as well as their implementation, please see Appendices B and C.

Footnote 1: The puzzles are available to play online at https://www.chiark.greenend.org.uk/~sgtatham/ puzzles/; excellent standalone apps for Android and iOS exist as well.

### Environment Overview

Within the PUZZLES environment, we encapsulate the tasks presented by each logic puzzle by defining consistent state, action, and observation spaces. It is also important to note that the large majority of the logic puzzles are designed so that they can be solved without requiring any guesswork. By default, we provide the option of two observation spaces, one is a representation of the discrete internal game state of the puzzle, the other is a visual representation of the game interface. These observation spaces can easily be wrapped in order to enable PUZZLES to be used with more advanced neural architectures such as graph neural networks (GNNs) or Transformers. All puzzles provide a discrete action space which only differs in cardinality. To accommodate the inherent difficulty and the need for proper algorithmic reasoning in solving these puzzles, the environment allows users to implement their own reward structures, facilitating the training of successful RL agents. All puzzles are played in a two-dimensional play area with deterministic state transitions, where a transition only occurs after a valid user input. Most of the puzzles in PUZZLES do not have an upper bound on the number of steps, they can only be completed by successfully solving the puzzle. An agent with a bad policy is likely never going to reach a terminal state. For this reason, we provide the option for early episode termination based on state repetitions. As we show in Section 3.4, this is an effective method to facilitate learning.

### Difficulty Progression and Generalization

The PUZZLES environment places a strong emphasis on giving users control over the difficulty exhibited by the environment. For each puzzle, the problem size and difficulty can be adjusted individually. The difficulty affects the complexity of strategies that an agent needs to learn to solve a puzzle. As an example, _Sudoku_ has tangible difficulty options: harder difficulties may require the use of new strategies such as _forcing chains_3 to find a solution, whereas easy difficulties only need the _single position_ strategy.4

Footnote 3: _Forcing chains_ works by following linked cells to evaluate possible candidates, usually starting with a two-candidate cell.

Footnote 4: The _single position_ strategy involves identifying cells which have only a single possible value.

The scalability of the puzzles in our environment offers a unique opportunity to design increasingly complex puzzle configurations, presenting a challenging landscape for RL agents to navigate. This dynamic nature of the benchmark serves two important purposes. Firstly, the scalability of the puzzles facilitates the evaluation of an agent's generalization capabilities. In the PUZZLES environment, it is possible to train an agent in an easy puzzle setting and subsequently evaluate its performance in progressively harder puzzle configurations. For most puzzles, the cardinality of the action space is independent of puzzle size. It is therefore also possible to train an agent only on small instances of a puzzle and then evaluate it on larger sizes. This approach allows us to assess whether an agent has learned the correct underlying algorithm and generalizes to out-of-distribution scenarios. Secondly, it enables the benchmark to remain adaptable to the continuous advancements in RL methodologies. As RL algorithms evolve and become more capable, the puzzle configurations can be adjusted accordingly to maintain the desired level of difficulty. This ensures that the benchmark continues to effectively assess the capabilities of the latest RL methods.

## 3 Empirical Evaluation

We evaluate the baseline performance of numerous commonly used RL algorithms on our PUZZLES environment. Additionally, we also analyze the impact of certain design decisions of the environment and the training setup. Our metric of interest is the average number of steps required by a policy to

Figure 2: Code and library landscape around the PUZZLES Environment, made up of the rlp Package and the puzzle Module. The figure shows how the puzzle Module presented in this paper fits within Tathamsâ€™s Puzzle Collectionâ€™code, the Pygame package, and a userâ€™s Gymnasium reinforcement learning code. The different parts are also categorized as Python language and C language.

successfully complete a puzzle, where lower is better. We refer to the term _successful episode_ to denote the successful completion of a single puzzle instance. We also look at the success rate, i.e. what percentage of the puzzles was completed successfully.

To provide an understanding of the puzzle's complexity and to contextualize the agents' performance, we include an upper-bound estimate of the optimal number of steps required to solve the puzzle correctly. This estimate is a combination of both the steps required to solve the puzzle using an optimal strategy, and an upper bound on the environment steps required to achieve this solution, such as moving the cursor to the correct position. The upper bound is denoted as _Optimal_. Please refer to Table 6 for details on how this upper bound is calculated for each puzzle. Further, we include the performance of a human expert as reference. The human expert is able to solve all puzzles in our evaluated difficulty levels within the optimal upper bound. For detailed results on the performance of the human expert, please refer to Appendix F.2.

We run experiments based on all the RL algorithms presented in Table 9. We include both popular traditional algorithms such as PPO, as well as algorithms designed more specifically for the kinds of tasks presented in PUZZLES. Where possible, we used the implementations available in the RL library Stable Baselines 3 [52], using the default hyperparameters. For MuZero and DreamerV3, we used the code available at [53] and [54], respectively. We provide a summary of all algorithms in Appendix Table 9. In total, our experiments required approximately 10'000 GPU hours.

All selected algorithms are compatible with the discrete action space required by our environment. This circumstance prohibits the use of certain other common RL algorithms, such as Soft-Actor Critic (SAC) [55] or Twin Delayed Deep Deterministic Policy Gradients (TD3) [56].

### Baseline Experiments

For the general baseline experiments, we trained all agents on all puzzles and evaluate their performance. Due to the challenging nature of our puzzles, we have selected an easy difficulty and small size of the puzzle where possible. Every agent was trained on the discrete internal state observation using five different random seeds. We trained all agents by providing rewards only at the end of each episode upon successful completion or failure. For computational reasons, we truncated all episodes during training and testing at 10,000 steps. For such a termination, reward was kept at 0. We evaluate the effect of this episode truncation in Section 3.4. We provide all experimental parameters, including the exact parameters supplied for each puzzle in Appendix F.1.

To track an agent's progress, we use episode lengths, i.e., how many actions an agent needs to solve a puzzle. A lower number of actions indicates a stronger policy that is closer to the optimal solution. To obtain the final evaluation, we run each policy on 1000 random episodes of the respective puzzle, again with a maximum step size of 10,000 steps. All experiments were conducted on NVIDIA 3090 GPUs. The training time for a single agent with 2 million PPO steps varied depending on the puzzle and ranged from approximately 1.75 to 3 hours. The training for DreamerV3 and MuZero was more demanding and training time ranged from approximately 10 to 20 hours.

Figure 2(b) shows the average successful episode length for all algorithms, created following the recommendations outlined in [57]. It can be seen that DreamerV3 performs best when looking at success rate and episode length, with TRPO, PPO and DQN following closely. MuZero suffers from instable training, where a successful strategy was only learned for a low number of puzzles, indicating the need for puzzle-specific hyperparameter tuning. The results are especially interesting since PPO and TRPO follow much simpler training routines than DreamerV3 and MuZero. It seems that the implicit world models learned by DreamerV3 struggle to appropriately capture some puzzles. Upon closer inspection of the detailed results, presented in Appendix Table 10 and 11, DreamerV3 manages to solve 62.7% of all puzzle instances. In 14 out of the 40 puzzles, it has found a policy that solves the puzzles within the _Optimal_ upper bound. PPO and TRPO managed to solve an average of 61.6% and 70.8% of the puzzle instances, however only 8 and 11 of the puzzles have consistently solved within the _Optimal_ upper bound. The algorithms A2C, RecurrentPPO, DQN and QRDQN perform worse than a pure random policy. Overall, it seems that some of the environments in PUZZLES are quite challenging and well suited to show the difference in performance between algorithms. It is also important to note that all the logic puzzles are designed so that they can be solved without requiring any guesswork.

### Difficulty

We further evaluate the performance of a subset of the puzzles on the easiest preset difficulty level for humans. We selected all puzzles where a random policy was able to solve them with a probability of at least 10%, which are Netslide, Same Game and Untangle. By using this selection, we estimate that the reward density should be relatively high, ideally allowing the agent to learn a good policy. Again, we train all algorithms listed in Table 9. We provide results for the two strongest algorithms, PPO and DreamerV3 in Table 1, with complete results available in Appendix Table 10. Note that as part of Section 3.4, we also perform ablations using DreamerV3 on more puzzles on the easiest preset difficulty level for humans.

We observe that for both PPO and DreamerV3, the percentage of successful episodes decreases, with a large increase in steps required. DreamerV3 performs clearly stronger than PPO, requiring

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Puzzle** & **Parameters** & **PPO** & **DreamerV3** & **Optimal** & **Human Expert** \\ \hline \multirow{2}{*}{Netslide} & 2x3b1 & \(35.3\pm 0.7\) & (100.0\%) & \(12.0\pm 0.4\) & (100.0\%) & 48 & 16.7 \\  & 3x3b1 & \(4742.1\pm 2960.1\) & (9.2\%) & \(3586.5\pm 676.9\) & (22.4\%) & 90 & 40.9 \\ \hline \multirow{2}{*}{Same Game} & 2x3c3a2 & \(11.5\pm 0.1\) & (100.0\%) & \(7.3\pm 0.2\) & (100.0\%) & 42 & 8.7 \\  & 5x5c3a2 & \(1009.3\pm 1089.4\) & (30.5\%) & \(527.0\pm 162.0\) & (30.2\%) & 300 & 37.0 \\ \hline \multirow{2}{*}{Untangle} & 4 & \(34.9\pm 10.8\) & (100.0\%) & \(6.3\pm 0.4\) & (100.0\%) & 80 & 6.0 \\  & 6 & \(2294.7\pm 2121.2\) & (96.2\%) & \(1683.3\pm 73.7\) & (82.0\%) & 150 & 30.5 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of how many steps agents trained with PPO and DreamerV3 need on average to solve puzzles of two difficulty levels. In brackets, the percentage of successful episodes is reported. The difficulty levels correspond to the overall easiest and the easiest-for-humans settings. We also give the upper bound of optimal steps needed for each configuration.

Figure 3: Subfigure (a) shows the success rate aggregated over all puzzles in the easiest setting, while Subfigure (b) shows aggregated episode length. Interval estimates are based on stratified bootstrap confidence intervals, computed using rliable [57]. Some puzzles, namely Loopy, Pearl, Pegs, Solo, and Unruly, were intractable for all algorithms and were therefore excluded in this aggregation. Optimal refers to the upper bound of the performance of an optimal policy. A human expert is able to solve all puzzles within this bound. We report median, interquartile mean on the middle 50% of runs (IQM), mean, as well as the optimality gap with respect to the upper bound of an optimal policy. We see that DreamerV3, DQN and TRPO are able to solve the largest amount of puzzles, however, DreamerV3 seems to learn better policies. Overall, all algorithms fall short of optimal or human expert level performance.

consistently fewer steps, but still more than the optimal policy. Our results indicate that puzzles with relatively high reward density at human difficulty levels remain challenging. We propose to use the easiest human difficulty level as a first measure to evaluate future algorithms. The details of the easiest human difficulty setting can be found in Appendix Table 7. If this level is achieved, difficulty can be further scaled up by increasing the size of the puzzles. Some puzzles also allow for an increase in difficulty with fixed size.

### Effect of Action Masking and Observation Representation

We evaluate the effect of action masking, as well as observation type, on training performance. Firstly, we analyze whether action masking, as described in paragraph "Action Masking" in Appendix B.4, can positively affect training performance. Secondly, we want to see if agents are still capable of solving puzzles while relying on pixel observations. Pixel observations allow for the exact same input representation to be used for all puzzles, thus achieving a setting that is very similar to the Atari benchmark. We compare MaskablePPO to the default PPO without action masking on both types of observations. We summarize the results in Figure 4. Detailed results for masked RL agents on the pixel observations are provided in Appendix Table 12.

As we can observe in Figure 4, action masking has a strongly positive effect on training performance. This benefit is observed both in the discrete internal game state observations and on the pixel observations. We hypothesize that this is due to the more efficient exploration, as actions without effect are not allowed. As a result, the reward density during training is increased, and agents are able to learn a better policy. Particularly noteworthy are the outcomes related to _Pegs_. They show that an agent with action masking can effectively learn a successful policy, while a random policy without action masking consistently fails to solve any instance. As expected, training RL agents on pixel observations increases the difficulty of the task at hand. The agent must first understand how the pixel observation relates to the internal state of the game before it is able to solve the puzzle. Nevertheless, in combination with action masking, the agents manage to solve a large percentage of all puzzle instances, with 10 of the puzzles consistently solved within the optimal upper bound.

Furthermore, Figure 4 shows the individual training performance on the puzzle _Flood_. It can be seen that RL agents using action masking and the discrete internal game state observation converge significantly faster and to better policies compared to the baselines. The agents using pixel observations and no action masking struggle to converge to any reasonable policy.

Figure 4: (left) We demonstrate the effect of action masking in both RGB observation and internal game state. By masking moves that do not change the current state, the agent requires fewer actions to explore, and therefore, on average solves a puzzle using fewer steps. (right) Moving average episode length during training for the _Flood_ puzzle. Lower episode length is better, as the episode gets terminated as soon as the agent has solved a puzzle. Different colors describe different algorithms, where different shades of a color indicate different random seeds. Sparse dots indicate that an agent only occasionally managed to find a policy that solves a puzzle. It can be seen that both the use of discrete internal state observations and action masking have a positive effect on the training, leading to faster convergence and a stronger overall performance.

### Effect of Episode Length and Early Termination

We evaluate whether the cutoff episode length or early termination have an effect on training performance of the agents. For computational reasons, we perform these experiments on a selected subset of the puzzles on the easiest preset human level difficulty and only for DreamerV3 (see Appendix F.4 for details). As we can see in Table 2, increasing the maximum episode length during training from 10,000 to 100,000 does not improve performance. Only when episodes get terminated after visiting the exact same state more than 10 times, the agent is able to solve more puzzle instances on average (31.5% vs. 25.2%). Given the sparse reward structure, terminating episodes early seems to provide a better trade-off between allowing long trajectories to successfully complete and avoiding wasting resources on unsuccessful trajectories. Interestingly, the solution for the puzzle Cube found by DreamerV3 requires fewer steps than the human expert.

### Generalization

PUZZLES is explicitly designed to facilitate the testing of generalization capabilities of agents with respect to different puzzle sizes or puzzle difficulties. For our experiments, we select puzzles with the highest reward density. We utilize a custom observation wrapper and transformer-based encoder in order for the agent to be able to work with different input sizes, see Appendices A.3 and A.4 for details. We call this approach PPO (Transformer)

The results presented in Table 3 indicate that while it is possible to learn a policy that generalizes it remains a challenging problem. Furthermore, it can be observed that selecting the best model during training according to the performance on the generalization environment yields a performance benefit in that setting. This suggests that agents may learn a policy that generalizes better during the training process, but then overfit on the environment they are training on. It is also evident that generalization performance varies substantially across different random seeds. For Netslide, the best agent is capable of solving 23.3% of the puzzles in the generalization environment whereas the worst experiment.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**\#Steps** & **ET** & **DreamerV3** \\ \hline \multirow{2}{*}{\(1e5\)} & 10 & \(2950.9\pm 1260.2\) (31.6\%) \\  & - & \(2975.4\pm 1503.5\) (25.2\%) \\ \hline \multirow{2}{*}{\(1e4\)} & 10 & \(3193.9\pm 1044.2\) (26.1\%) \\  & - & \(2892.4\pm 908.3\) (26.8\%) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of the effect of the maximum episode length (# Steps) and early termination (ET) on final performance. For each setting, we report average success episode length with standard deviation with respect to the random seed, all averaged over all selected puzzles. In brackets, the percentage of successful episodes is reported.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Puzzle** & **Parameters** & **Trained on** & **PPO (Transformer)** & **PPO (Transformer)\({}^{\dagger}\)** \\ \hline \multirow{2}{*}{Nestslide} & 2x3b1 & âœ“ & \(244.1\pm 313.7\) (100.0\%) & \(242.0\pm 379.3\) (100.0\%) \\  & 3x3b1 & âœ— & \(9014.6\pm 2410.6\) (18.6\%) & \(9002.8\pm 2454.9\) (18.0\%) \\ \hline \multirow{2}{*}{Same Game} & 2x3c3s2 & âœ“ & \(9.3\pm 10.9\) (99.8\%) & \(26.2\pm 52.9\) (99.7\%) \\  & 5x5c3s2 & âœ— & \(379.0\pm 261.6\) (9.4\%) & \(880.1\pm 675.4\) (18.1\%) \\ \hline \multirow{2}{*}{Untangle} & 4 & âœ“ & \(38.6\pm 58.2\) (99.8\%) & \(69.8\pm 66.4\) (100.0\%) \\  & 6 & âœ— & \(3340.0\pm 3101.2\) (87.3\%) & \(2985.8\pm 2774.7\) (93.7\%) \\ \hline \hline \end{tabular}
\end{table}
Table 3: We test generalization capabilities of agents by evaluating them on puzzle sizes larger than their training environment. We report the average number of steps an agent needs to solve a puzzle, and the percentage of successful episodes in brackets. The difficulty levels correspond to the overall easiest and the easiest-for-humans settings. For PPO (Transformer), we selected the best checkpoint during training according to the performance in the training environment. For PPO (Transformer)\({}^{\dagger}\), we selected the best checkpoint during training according to the performance in the generalization environment.

agent is only able to solve 11.2% of the puzzles, similar to a random policy. Our findings suggest that agents are generally capable of generalizing to more complex puzzles. However, further research is necessary to identify the appropriate inductive biases that allow for consistent generalization without a significant decline in performance.

## 4 Discussion

The experimental evaluation demonstrates varying degrees of success among different algorithms. For instance, puzzles such as _Tracks_, _Map_ or _Flip_ were not solvable by any of the evaluated RL agents, or only with performance similar to a random policy. This points towards the potential of intermediate rewards, better game rule-specific action masking, or model-based approaches. To encourage exploration in the state space, a mechanism that explicitly promotes it may be beneficial. On the other hand, the fact that some algorithms managed to solve a substantial amount of puzzles with presumably optimal performance demonstrates the advances in the field of RL. In light of the promising results of DreamerV3, the improvement of agents that have certain reasoning capabilities and an implicit world model by design stay an important direction for future research.

Experimental Results.The experimental results presented in Section 3.1 and Section 3.3 underscore the positive impact of action masking and the correct observation type on performance. While a pixel representation would lead to a uniform observation for all puzzles, it currently increases complexity too much compared the discrete internal game state. Our findings indicate that incorporating action masking significantly improves the training efficiency of reinforcement learning algorithms. This enhancement was observed in both discrete internal game state observations and pixel observations. The mechanism for this improvement can be attributed to enhanced exploration, resulting in agents being able to learn more robust and effective policies. This was especially evident in puzzles where unmasked agents had considerable difficulty, thus showcasing the tangible advantages of implementing action masking for these puzzles.

Limitations.While the PUZZLES framework provides the ability to gain comprehensive insights into the performance of various RL algorithms on logic puzzles, it is crucial to recognize certain limitations when interpreting results. The sparse rewards used in this baseline evaluation add to the complexity of the task. Moreover, all algorithms were evaluated with their default hyperparameters. Additionally, the constraint of discrete action spaces excludes the application of certain RL algorithms.

Benchmarking LLMs.We also explore the potential of using PUZZLES as a novel framework for evaluating the reasoning abilities of both large language models (LLMs) and vision language models (VLMs). While existing reasoning benchmarks have been the subject of debate regarding their ability to accurately assess the reasoning abilities of LLMs [58; 59; 60], PUZZLES offers a unique advantage by enabling true out-of-distribution evaluation. Our preliminary experiments, conducted with Gemini 1.5 Flash [61] and GPT-4o mini [62], indicate that current LLMs have limited success in solving PUZZLES. A detailed analysis of these results is presented in Appendix F.5. This research lays the groundwork for future investigations using PUZZLES to provide a more nuanced understanding of the reasoning processes and limitations of LLMs.

In the context of RL, the different challenges posed by the logic-requiring nature of these puzzles necessitates a good reward system, strong guidance of agents, and an agent design more focused on logical reasoning capabilities. It will be interesting to see how alternative architectures such as graph neural networks (GNNs) perform. GNNs are designed to align more closely with the algorithmic solution of many puzzles. While the notion that "reward is enough" [63; 64] might hold true, our results indicate that not just _any_ form of correct reward will suffice, and that advanced architectures might be necessary to learn an optimal solution.

## 5 Conclusion

In this work, we have proposed PUZZLES, a benchmark that bridges the gap between algorithmic reasoning and RL. In addition to containing a rich diversity of logic puzzles, PUZZLES also offers an adjustable difficulty progression for each puzzle, making it a useful tool for benchmarking, evaluating and improving RL algorithms. Our empirical evaluation shows that while RL algorithms exhibit varying degrees of success, challenges persist, particularly in puzzles with higher complexity or those requiring nuanced logical reasoning. We are excited to share PUZZLES with the broader research community and hope that this benchmark will foster further research to improve the algorithmic reasoning abilities of machine learning models.

## Broader Impact

This paper aims to contribute to the advancement of the field of machine learning (ML). Given the current challenges in ML related to algorithmic reasoning, we believe that our newly proposed benchmark will facilitate significant progress in this area, potentially elevating the capabilities of ML systems. Progress in algorithmic reasoning can contribute to the development of more transparent, explainable, and fair ML systems. This can further help address issues related to bias and discrimination in automated decision-making processes, promoting fairness and accountability.

## References

* Serafini and Garcez [2016] Luciano Serafini and Artur d'Avila Garcez. Logic tensor networks: Deep learning and logical reasoning from data and knowledge. _arXiv preprint arXiv:1606.04422_, 2016.
* Dai et al. [2019] Wang-Zhou Dai, Qiuling Xu, Yang Yu, and Zhi-Hua Zhou. Bridging machine learning and logical reasoning by abductive learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Li et al. [2020] Yujia Li, Felix Gimeno, Pushmeet Kohli, and Oriol Vinyals. Strong generalization and efficiency in neural programs. _arXiv preprint arXiv:2007.03629_, 2020.
* Velickovic and Blundell [2021] Petar Velickovic and Charles Blundell. Neural algorithmic reasoning. _Patterns_, 2(7), 2021.
* Masry et al. [2022] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 2263-2279, 2022.
* Jiao et al. [2022] Fangkai Jiao, Yangyang Guo, Xuemeng Song, and Liqiang Nie. Merit: Meta-path guided contrastive learning for logical reasoning. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 3496-3509, 2022.
* Bardin et al. [2023] Sebastien Bardin, Somesh Jha, and Vijay Ganesh. Machine learning and logical reasoning: The new frontier (dagstuhl seminar 22291). In _Dagstuhl Reports_, volume 12. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2023.
* Li et al. [2021] Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C Paulson. Isarstep: a benchmark for high-level mathematical reasoning. In _International Conference on Learning Representations_, 2021.
* Velickovic et al. [2022] Petar Velickovic, Adria Puigdomenech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, and Charles Blundell. The CLRS algorithmic reasoning benchmark. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 22084-22102. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/velickovic22a.html.
* Srivastava et al. [2022] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing Atari with Deep Reinforcement Learning. _CoRR_, abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.

* [12] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. _Advances in neural information processing systems_, 30, 2017.
* [13] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018.
* [14] Adria Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In _International conference on machine learning_, pages 507-517. PMLR, 2020.
* [15] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers with deep reinforcement learning. _Nature_, 602(7896):223-228, 2022.
* [16] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In _Conference on Robot Learning_, pages 651-673. PMLR, 2018.
* [17] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2021.
* [18] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In _Conference on Robot Learning_, pages 91-100. PMLR, 2022.
* [19] Krishan Rana, Ming Xu, Brendan Tidd, Michael Milford, and Niko Sunderhauf. Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics. In _Conference on Robot Learning_, pages 2095-2104. PMLR, 2023.
* [20] Zhe Wang and Tianzhen Hong. Reinforcement learning for building controls: The opportunities and challenges. _Applied Energy_, 269:115036, 2020.
* [21] Di Wu, Yin Lei, Maoen He, Chunjiong Zhang, and Li Ji. Deep reinforcement learning-based path control and optimization for unmanned ships. _Wireless Communications and Mobile Computing_, 2022:1-8, 2022.
* [22] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. _Annual Review of Control, Robotics, and Autonomous Systems_, 5:411-444, 2022.
* [23] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_, pages 5026-5033. IEEE, 2012.
* [24] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013.
* [25] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* [26] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In _International conference on machine learning_, pages 1329-1338. PMLR, 2016.
* [27] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.

* Cote et al. [2018] Marc-Alexandre Cote, Akos Kadar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. _CoRR_, abs/1806.11532, 2018.
* Lanctot et al. [2019] Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien Perolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, Janos Kramar, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis. OpenSpiel: A framework for reinforcement learning in games. _CoRR_, abs/1908.09453, 2019. URL http://arxiv.org/abs/1908.09453.
* Jiang and Luo [2019] Zhengyao Jiang and Shan Luo. Neural logic reinforcement learning. In _International conference on machine learning_, pages 3110-3119. PMLR, 2019.
* Fawzi et al. [2022] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammad Bernstein, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. _Nature_, 610(7930):47-53, 2022.
* Mankowitz et al. [2023] Daniel J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discovered using deep reinforcement learning. _Nature_, 618(7964):257-263, 2023.
* Lai [2015] Matthew Lai. Giraffe: Using deep reinforcement learning to play chess. _arXiv preprint arXiv:1509.01549_, 2015.
* Silver et al. [2016] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. _Nature_, 529:484-489, 2016. URL https://doi.org/10.1038/nature16961.
* Tatham [2004] Simon Tatham. Simon tatham's portable puzzle collection, 2004. URL https://www.chiark.greenend.org.uk/~sgtatham/puzzles/. Accessed: 2023-05-16.
* Foundation [2022] Farama Foundation. Gymnasium website, 2022. URL https://gymnasium.farama.org/. Accessed: 2023-05-12.
* Wang et al. [2022] Chao Wang, Chen Chen, Dong Li, and Bin Wang. Rethinking reinforcement learning based logic synthesis. _arXiv preprint arXiv:2205.07614_, 2022.
* Dasgupta et al. [2019] Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal reasoning from meta-reinforcement learning. _arXiv preprint arXiv:1901.08162_, 2019.
* Eppe et al. [2022] Manfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong DH Nguyen, Martin V Butz, and Stefan Wermter. Intelligent problem-solving as integrated hierarchical reinforcement learning. _Nature Machine Intelligence_, 4(1):11-20, 2022.
* Deac et al. [2021] Andreea-Ioana Deac, Petar Velickovic, Ognjen Milinkovic, Pierre-Luc Bacon, Jian Tang, and Mladen Nikolic. Neural algorithmic reasoners are implicit planners. _Advances in Neural Information Processing Systems_, 34:15529-15542, 2021.
* He et al. [2022] Yu He, Petar Velickovic, Pietro Lio, and Andreea Deac. Continuous neural algorithmic planners. In _Learning on Graphs Conference_, pages 54-1. PMLR, 2022.
* Silver et al. [2017] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. _arXiv preprint arXiv:1712.01815_, 2017.

* Dahl [2001] Fredrik A Dahl. A reinforcement learning algorithm applied to simplified two-player texas hold'em poker. In _European Conference on Machine Learning_, pages 85-96. Springer, 2001.
* Heinrich and Silver [2016] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-information games. _arXiv preprint arXiv:1603.01121_, 2016.
* Steinberger [2019] Eric Steinberger. Pokerrl. https://github.com/TinkeringCode/PokerRL, 2019.
* Zhao et al. [2022] Enmin Zhao, Renye Yan, Jinqiu Li, Kai Li, and Junliang Xing. Alphaholdlem: High-performance artificial intelligence for heads-up no-limit poker via end-to-end reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 4689-4697, 2022.
* Ghory [2004] Imran Ghory. Reinforcement learning in board games. 2004.
* Szita [2012] Istvan Szita. Reinforcement learning in games. In _Reinforcement Learning: State-of-the-art_, pages 539-577. Springer, 2012.
* Xenou et al. [2019] Konstantia Xenou, Georgios Chalkiadakis, and Stergos Afantenos. Deep reinforcement learning in strategic board game environments. In _Multi-Agent Systems: 16th European Conference, EUMAS 2018, Bergen, Norway, December 6-7, 2018, Revised Selected Papers 16_, pages 233-248. Springer, 2019.
* Perolat et al. [2022] Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego with model-free multiagent reinforcement learning. _Science_, 378(6623):990-996, 2022.
* Cormen et al. [2022] Thomas H. Cormen, Charles Eric Leiserson, Ronald L. Rivest, and Clifford Stein. _Introduction to Algorithms_. The MIT Press, 4th edition, 2022.
* Raffin et al. [2021] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.
* Duvaud [2019] Aurele Hainaut Werner Duvaud. Muzero general: Open reimplementation of muzero. https://github.com/werner-duvaud/muzero-general, 2019.
* Hafner et al. [2023] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. https://github.com/danijar/dreamerv3, 2023.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Fujimoto et al. [2018] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* Agarwal et al. [2021] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in Neural Information Processing Systems_, 2021.
* McCoy et al. [2024] R Thomas McCoy, Shunyu Yao, Dan Friedman, Mathew D Hardy, and Thomas L Griffiths. Embers of autoregression show how large language models are shaped by the problem they are trained to solve. _Proceedings of the National Academy of Sciences_, 121(41):e2322420121, 2024.
* Prabhakar et al. [2024] Akshara Prabhakar, Thomas L Griffiths, and R Thomas McCoy. Deciphering the factors influencing the efficacy of chain-of-thought: Probability, memorization, and noisy reasoning. _arXiv preprint arXiv:2407.01687_, 2024.
* Mirzadeh et al. [2024] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. _arXiv preprint arXiv:2410.05229_, 2024.

* Reid et al. [2024] Rachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* Hurst et al. [2024] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welhiinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. _arXiv preprint arXiv:2410.21276_, 2024.
* Silver et al. [2021] David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. _Artificial Intelligence_, 299:103535, 2021.
* Vamplew et al. [2022] Peter Vamplew, Benjamin J Smith, Johan Kallstrom, Gabriel Ramos, Roxana Radulescu, Diederik M Roijers, Conor F Hayes, Fredrik Heintz, Patrick Mannion, Pieter JK Libin, et al. Scalar reward is not enough: A response to silver, singh, precup and sutton (2021). _Autonomous Agents and Multi-Agent Systems_, 36(2):41, 2022.
* Community [2000] Pygame Community. Pygame github repository, 2000. URL https://github.com/pygame/pygame/. Accessed: 2023-05-12.
* Tatham [2004] Simon Tatham. Developer documentation for simon tatham's puzzle collection, 2004. URL https://www.chiark.greenend.org.uk/~sgtatham/puzzles/devel/. Accessed: 2023-05-23.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL http://arxiv.org/abs/1707.06347.
* Huang et al. [2022] Shengyi Huang, Rouslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun Wang. The 37 implementation details of proximal policy optimization. In _ICLR Blog Track_, 2022. URL https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/. https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/.
* Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. _CoRR_, abs/1602.01783, 2016. URL http://arxiv.org/abs/1602.01783.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 1889-1897, Lille, France, 07-09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/schulman15.html.
* Dabney et al. [2017] Will Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional reinforcement learning with quantile regression. _CoRR_, abs/1710.10044, 2017. URL http://arxiv.org/abs/1710.10044.
* Schrittwieser et al. [2020] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Hafner et al. [2023] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.

Puzzles Environment Usage Guide

### General Usage

A Python code example for using the PUZZLES environment is provided in Listing 1. All puzzles support seeding the initialization, by adding #{seed} after the parameters, where {seed} is an int. The allowed parameters are displayed in Table 6. A full custom initialization argument would be as follows: {parameters}#{seed}.

```
1importgymnasiumasgym
2importrlp
3
4#initanagentsuitableforGymnasiumenvironments
5agent=Agent.create()
6
7#inittheenvironment
8env=gym.make('rlp/Puzzle-v0',puzzle="bridges",
9render_mode="rgb_array",params="4x4#42")
10observation,info=env.reset()
11
12#completeanepisode
13terminated=False
14whilenotterminated:
15action=agent.choose(env)#theagentchoosesthenextaction
16observation,reward,terminated,truncated,info=env.step(action)
17env.close() ```

_Listing 1_Code example of how to initialize an environment and have an agent complete one episode. The PUZZLES environment is designed to be compatible with the Gymnasium API. The choice of Agent is up to the user, it can be a trained agent or random policy.

### Custom Reward

A Python code example for implementing a custom reward system is provided in Listing 3. To this end, the environment's step() function provides the puzzle's internal state inside the info Python dict.

```
1importgymnasiumasgym
2classPuzzleRewardWrapper(gym.Wrapper):
3defstep(self,action):
4obs,reward,terminated,truncated,info=self.env.step(action)
5#Modifytherewardbyusingmembersofinfo["puzzle_state"]
6returnobs,reward,terminated,truncated,info
7
8
9
10Listing 2: Code example of a custom reward implementation using Gymnasium'sWrapper class. A user can use the game state information provided in info["puzzle_state"] to modify the rewards received by the agent after performing an action.

### Custom Observation

A Python code example for implementing a custom observation structure that is compatible with an agent using a transformer encoder. Here, we provide the example for Netslide, please refer to our GitHub for more examples.

```
1importgymnasiumasgym
2importnumpyasmp
3classReslideTransferWrapper(gym.ObservationWrapper):
4def_init_(self,env):
5super(NetslideTransferWrapper,self).__init_(env)
6self.original_space=env.observation_space
7
8self.max_length=512
9self.embedding_dim=16+4
10self.observation_space=gym.spaces.Box() ```

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

### Observation Space

There are two kinds of observations which can be used by the agent. The first observation type is a representation of the discrete internal game state of the puzzle, consisting of a combination of arrays and scalars. This observation is provided by the underlying code of Tathams's puzzle collection. The composition and shape of the internal game state is different for each puzzle, which, in turn, requires the agent architecture to be adapted.

The second type of observation is a representation of the pixel screen, given as an integer matrix of shape (3\(\times\)width\(\times\)height). The environment deals with different aspect ratios by adding padding. The advantage of the pixel representation is a consistent representation for all puzzles, similar to the Atari RL Benchmark [11]. It could even allow for a single agent to be trained on different puzzles. On the other hand, it forces the agent to learn to solve the puzzles only based on the visual representation of the puzzles, analogous to human players. This might increase difficulty as the agent has to learn the task representation implicitly.

### Action Space

Natively, the puzzles support two types of input, mouse and keyboard. Agents in PUZZLES play the puzzles only through keyboard input. This is due to our decision to provide the discrete internal game state of the puzzle as an observation, for which mouse input would not be useful.

The action space for each puzzle is restricted to actions that can actively contribute to changing the logical state of a puzzle. This excludes "memory aides" such as markers that signify the absence of a certain connection in _Bridges_ or adding candidate digits in cells in _Sudoku_. The action space also includes possibly rule-breaking actions, as long as the game can represent the effect of the action correctly.

The largest action space has a cardinality of 14, but most puzzles only have five to six valid actions which the agent can choose from. Generally, an action is in one of two categories: selector movement or game state change. Selector movement is a mechanism that allows the agent to select game objects during play. This includes for example grid cells, edges, or screen regions. The selector can be moved to the next object by four discrete directional inputs and as such represents an alternative to continuous mouse input. A game state change action ideally follows a selector movement action. The game state change action will then be applied to the selected object. The environment responds by updating the game state, for example by entering a digit or inserting a grid edge at the current selector position.

### Action Masking

The fixed-size action space allows an agent to execute actions that may not result in any change in game state. For example, the action of moving the selector to the right if the selector is already placed at the right border. The PUZZLES environment provides an action mask that marks all actions that change the state of the game. Such an action mask can be used to improve performance of model-based and even some model-free RL approaches. The action masking provided by PUZZLES does not ensure adherence to game rules, rule-breaking actions can most often still be represented as a change in the game state.

### Reward Structure

In the default implementation, the agent only receives a reward for completing an episode. Rewards consist of a fixed positive value for successful completion and a fixed negative value otherwise. This reward structure encourages an agent to solve a given puzzle in the least amount of steps possible. The PUZZLES environment provides the option to define intermediate rewards tailored to specific puzzles, which could help improve training progress. This could be, for example, a negative reward if the agent breaks the rules of the game, or a positive reward if the agent correctly achieves a part of the final solution.

### Early Episode Termination

Most of the puzzles in PUZZLES do not have an upper bound on the number of steps, where the only natural end can be reached via successfully solving the puzzle. The PUZZLES environment also provides the option for early episode termination based on state repetitions. If an agent reaches the exact same game state multiple times, the episode can be terminated in order to prevent wasteful continuation of episodes that no longer contribute to learning or are bound to fail.

## Appendix C PUZZLES Implementation Details

In the following, a brief overview of PUZZLES's code implementation is given. The environment is written in both Python and C, in order to interface with Gymnasium [36] as the RL toolkit and the C source code of the original puzzle collection. The original puzzle collection source code is available under the MIT License.5 In maintext Figure 2, an overview of the environment and how it fits with external libraries is presented. The modular design in both PUZZLES and the Puzzle Collection's original code allows users to build and integrate new puzzles into the environment.

Footnote 5: The source code and license are available at https://www.chiark.greenend.org.uk/~sgtatham/ puzzles/.

Environment ClassThe reinforcement learning environment is implemented in the Python class PuzzleEnv in the rlp package. It is designed to be compatible with the Gymnasium-style API for RL environments to facilitate easy adoption. As such, it provides the two important functions needed for progressing an environment, reset() and step().

Upon initializing a PuzzleEnv, a 2D surface displaying the environment is created. This surface and all changes to it are handled by the Pygame [65] graphics library. PUZZLES uses various functions provided in the library, such as shape drawing, or partial surface saving and loading.

The reset() function changes the environment state to the beginning of a new episode, usually by generating a new puzzle with the given parameters. An agent solving the puzzle is also reset to a new state. reset() also returns two variables, observation and info, where observation is a Python dict containing a NumPy 3D array called pixels of size (3 \(\times\) surface_width \(\times\) surface_height). This NumPy array contains the RGB pixel data of the Pygame surface, as explained in Appendix B.2. The info dict contains a dict called puzzle_state, representing a copy of the current internal data structures containing the logical game state, allowing the user to create custom rewards.

The step() function increments the time in the environment by one step, while performing an action chosen from the action space. Upon returning, step() provides the user with five variables, listed in Table 4.

Intermediate RewardsThe environment encourages the use of Gymnasium's Wrapper interface to implement custom reward structures for a given puzzle. Such custom reward structures can provide an easier game setting, compared to the sparse reward only provided when finishing a puzzle.

Puzzle ModuleThe PuzzleEnv object creates an instance of the class Puzzle. A Puzzle is essentially the glue between all Pygame surface tasks and the C back-end that contains the puzzle

\begin{table}
\begin{tabular}{l l} \hline \hline
**Variable** & **Description** \\ \hline observation & 3D NumPy array containing RGB pixel data \\ reward & The cumulative reward gained throughout all steps of the episode \\ terminated & A bool stating whether an episode was completed by the agent \\ truncated & A bool stating whether an episode was ended early, for example by reaching \\  & the maximum allowed steps for an episode \\ info & A dict containing a copy of the internal game state \\ \hline \hline \end{tabular}
\end{table}
Table 4: Return values of the environmentâ€™s step() function. This information can then be used by an RL framework to train an agent.

[MISSING_PAGE_FAIL:20]

Figure 11: **Flip**: Flip groups of squares to light them all up at once.

Figure 14: **Guess**: Guess the hidden combination of colours.

Figure 12: **Flood**: Turn the grid the same colour in as few flood fills as possible.

Figure 13: **Galaxies**: Divide the grid into rotationally symmetric regions each centred on a dot.

Figure 9: **Fifteen**: Slide the tiles around to arrange them into order.

Figure 16: **Keen**: Complete the latin square in accordance with the arithmetic clues.

Figure 17: **Light Up**: Place bulbs to light up all the squares.

Figure 20: **Map**: Colour the map so that adjacent regions are never the same colour.

Figure 18: **Loopy**: Draw a single closed loop, given clues about number of adjacent edges.

Figure 22: **Mosaic**: Fill in the grid given clues about number of nearby black squares.

Figure 23: **Net**: Rotate each tile to reassemble the network.

Figure 24: **Netslide**: Slide a row at a time to reassemble the network.

Figure 27: **Pearl**: Draw a single closed loop, given clues about corner and straight squares.

Figure 28: **Pegs**: Jump pegs over each other to remove all but one.

Figure 29: **Range**: Place black squares to limit the visible distance from each numbered cell.

Figure 31: **Same Game**: Clear the grid by removing touching groups of the same colour squares.

Figure 34: **Sixteen**: Slide a row at a time to arrange the tiles into order.

Figure 35: **Slamt**: Draw a maze of slanting lines that matches the clues.

Figure 30: **Rectangles**: Divide the grid into rectangles with areas equal to the numbers.

Figure 33: **Singles**: Black out the right set of duplicate numbers.

Figure 35: **Slamt**: Draw a maze of slanting lines that matches the clues.

Figure 36: **Solo**: Fill in the grid so that each row, column and square block contains one of every digit.

Figure 41: **Unequal**: Complete the latin square in accordance with the right numbers of them can be seen in mirrors.

Figure 43: **Unruly**: Fill in the black and white grid to avoid runs of three.

Figure 37: **Tents**: Place a tent next to each tree.

Figure 38: **Tracks**: Complete the latin square of towers in accordance with the clues.

Figure 40: **Twiddle**: Rotate the tiles around themselves to arrange them into order.

Figure 39: **Tracks**: Fill in the railway track according to the clues.

## Appendix E Puzzle-specific Metadata

### Action Space

We display the action spaces for all supported puzzles in Table 5. The action spaces vary in size and in the types of actions they contain. As a result, an agent must learn the meaning of each action independently for each puzzle.

Figure 44: **Untangle**: Reposition the points so that the lines do not cross.

### Optional Parameters

We display the optional parameters for all supported puzzles in Table 6. If none are supplied upon initialization, a set of default parameters gets used for the puzzle generation process.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Puzzle** & **Cardinality** & **Action space** \\ \hline Black Box & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Bridges & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Cube & 4 & UP, DOWN, LEFT, RIGHT \\ Dominosa & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Fifteen & 4 & UP, DOWN, LEFT, RIGHT \\ Filling & 13 & UP, DOWN, LEFT, RIGHT, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\ Flip & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Flood & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Galaxies & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Guess & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Inertia & 9 & 1, 2, 3, 4, 6, 7, 8, 9, UNDO \\ Keen & 14 & UP, DOWN, LEFT, RIGHT, SELECT2, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\ Light Up & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Loopy & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Magnets & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Map & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Mines & 7 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2, UNDO \\ Mosaic & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Net & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Netslide & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Palisade & 5 & UP, DOWN, LEFT, RIGHT, CTRL \\ Pattern & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Pearl & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Pegs & 6 & UP, DOWN, LEFT, RIGHT, SELECT, UNDO \\ Range & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Rectangles & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Same Game & 6 & UP, DOWN, LEFT, RIGHT, SELECT, UNDO \\ Signpost & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Singles & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Sixteen & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Slant & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Solo & 13 & UP, DOWN, LEFT, RIGHT, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\ Tents & 6 & UP, DOWN, LEFT, RIGHT, SELECT2, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\ Towers & 14 & UP, DOWN, LEFT, RIGHT, SELECT2, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\ Tracks & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ Twiddle & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Undead & 8 & UP, DOWN, LEFT, RIGHT, SELECT2, 1, 2, 3 \\ Unequal & 13 & UP, DOWN, LEFT, RIGHT, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\ Unruly & 6 & UP, DOWN, LEFT, RIGHT, SELECT, SELECT2 \\ Untangle & 5 & UP, DOWN, LEFT, RIGHT, SELECT \\ \hline \hline \end{tabular}
\end{table}
Table 5: The action spaces for each puzzle are listed, along with their cardinalities. The actions are listed with their name in the original Puzzle Collection C code.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Puzzle** & **Example** & **Parameter** & **Description** & **Optimal Step Upper Bound** \\ \hline Black Box & w@bdm5M5 & w(int) & grid width & (w - h + w + h + 1) \\  & h(int) & grid height & & \(\cdot\cdot\cdot\) (w + 2) \(\cdot\cdot\) (h + 2) \\  & & m(int) & minimum number of balls & \\  & & M(int) & maximum number of balls & \\ \hline Bridges & 7x715e2m2d0 & (int)x(int) & grid width \(\times\) grid height & \(3\cdot w\cdot h\cdot(w+h+8)\) \\  & & i(int) & percentage of island squares & \\  & & e(int) & expansion factor & \\  & & m(int) & max bridges per direction & \\  & & d(int) & difficulty (0 = easy, 1 = medium, 2 = hard) & \\ \hline Cube & c4x4 & (char) & type \{c = cube, t = tetrahedron, & w - h - F \\  & & (int)x(int) & grid width \(\times\) grid height & F = number of the bodyâ€™s faces \\ \hline Dominosa & 6db & (int) & maximum number of dominoes & \(\frac{1}{2}\) (w + 3w + 2) \\  & & d(char) & difficulty (t = trivial, b = basic, h = hard, & \(\cdot\cdot\cdot\)(4\(\sqrt{w^{2}+3w+2}+1\)) \\  & & e = extreme, a = ambiguous) & \\ \hline Fifteen & 4x4 & (int)x(int) & grid width \(\times\) grid height & \((w\cdot h)^{4}\) \\ \hline Filing & 13x9 & (int)x(int) & grid width \(\times\) grid height & \((w\cdot h)\cdot(w+h+1)\) \\ \hline Flip & 5x5c & (int)x(int) & grid width \(\times\) grid height & \((w\cdot h)\cdot(w+h+1)\) \\  & & (char) & type \{c = crosses, r = random} & \\ \hline Flood & 12x126m5 & (int)x(int) & grid width \(\times\) grid height & \((w\cdot h)\cdot(w+h+1)\) \\  & & c(int) & number of colors & \\  & & m(int) & extra moves permitted (above the & \\  & & & solverâ€™s minimum) & \\ \hline Galaxies & 7x7dn & (int)x(int) & grid width \(\times\) grid height & \((2\cdot w\cdot h-w-h)\) \\  & & d(char) & difficulty (n = normal, u = unreasonable) & \(\cdot(2\cdot w+2\cdot h+1)\) \\ \hline Guess & c6p4g10Bm & c(int) & number of colors & \((p+1)\cdot g\cdot(c+p)\) \\  & & p(int) & peg per guess & \\  & & g(int) & maximum number of guesses & \\  & & (char) & allow blanks \{B = no, b = yes} & \\  & & (char) & allow duplicates \{M = no, m = yes} & \\ \hline Inertia & 10x8 & (int)x(int) & grid width \(\times\) grid height & \(0.2\cdot w^{2}\cdot h^{2}\) \\ \hline Keen & 6dn & (int) & grid size & \((2\cdot w+1)\cdot w^{2}\) \\  & & d(char) & difficulty (e = easy, n = normal, h = hard, \\  & & (char) & x = extreme, u = unreasonable) & \\  & & (char) & (Optional) multiplication only \{m = yes} & \\ \hline Light Up & 7x7b20s4d0 & (int)x(int) & grid width \(\times\) grid height & \(\frac{1}{2}\cdot(w+h+1)\) \\  & & b(int) & percentage of black squares & \(\cdot(w\cdot h+1)\) \\  & & s(int) & symmetry \{0 = none, 1 = 2-way mirror, & \\  & & 2 = 2-way rotational, 3 = 4-way mirror, & \\  & & d(int) & difficulty (0 = easy, 1 = tricky, 2 = hard) & \\ \hline Loopy & 10x10t12dh & (int)x(int) & grid width \(\times\) grid height & \((2\cdot w\cdot h+1)\cdot 3\cdot(w\cdot h)^{2}\) \\  & & t(int) & type \{0 = squares, 1 = triangular, & \\  & & 2 = honeycomb, 3 = sub-subar, & \\  & & & 4 = cario, 5 = great-hexagonal, & \\  & & & 6 = octagonal, 7 = kites, & \\  & & & 8 = flort, 9 = dodecagonal, & \\  & & & 10 = great-dodecagonal, & \\  & & & 11 = Penrose (kite/dart), & \\  & & & 12 = Penrose (rh/obs), & \\  & & & 13 = great-great-dodecagonal, & \\  & & & 14 = kagome, 15 = compass-dodecagonal, & \\  & & & 16 = hats) & \\  & & d(char) & difficulty (e = easy, n = normal, & \\  & & & t = tricky, h = hard) & \\ \hline Magnets & 6x5d5 & \{int\}x(int) & grid width \(\times\) grid height & \(w\cdot h\cdot(w+h+2)\) \\  & & d(char) & difficulty (e = easy, t = tricky & \\  & & & (char) & (Optional) strip clues \{S = yes} & \\ \hline Map & 20x15m30dn & (int)x(int) & grid width \(\times\) grid height & \(2\cdot n\cdot(1+w+h)\) \\  & & m(int) & number of regions & \\  & & & & Continued on next page & \\ \hline \hline \end{tabular}
\end{table}
Table 6: For each puzzle, all optional parameters a user may supply are shown and described. We also give the required data type of variable, where applicable (e.g., int or char). For parameters that accept one of a few choices (such as difficulty), the accepted values and corresponding explanation are given in braces. As as example: a difficulty parameter is listed as d{int} with allowed values {0 = easy, 1 = medium, 2 = hard}. In this case, choosing medium difficulty would correspond to d1.

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]

Detailed Results

### Baseline Parameters

In Table 7, the parameters used for training the agents used for the comparisons in Section 3 is shown.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Puzzle** & **Supplied Parameters** & **Easiest Human Level Present** & **Notes** \\ \hline Black Box & x2h2n2n2K & w5h5c3n3 & \\ Bridges & x3s3 & 7x7i30e10n2d0 & \\ Cube & x3s3 & c4x4 & \\ Dominosa & 1dt & 3dt & \\ Fifteen & 2x2 & 4x4 & \\ Filling & zx3 & 9x7 & \\ Flip & 3x3c & 3x3c & \\ Flood & x3x6n5 & 12x12c6n5 & \\ Galaxies & x3s4c & 7x7dn & \\ Guess & c2p3g10n & c6p4g100n & Episodes were terminated and negatively rewarded & \\  & & & after the maximum number of guesses was made & \\ Inertia & 4x4 & 10x8 & \\ Keen & 3dem & 4de & Even the minimum allowed problem size \\  & & & proved to be infeasible for a random agent \\ Light Up & x3h20s0d0d0 & 7x7h20s4d0 & \\ Loopy & x3s4c0d & 7x7+0de & \\ Magnets & x3sde3 & c5de & \\ Map & x3s5de & 20x15n30de & \\ Mines & x4s6n2 & 9x6n10 & \\ Mosaic & 3x3 & 3x3 & \\ Net & 2x2 & 8x5 & \\ Nestide & 2x3b1 & 3x3b1 & \\ Palisade & 2x3a3 & 5x5n5 & \\ Pattern & 3x2 & 10x10 & \\ Pearl & 5x5de & 6x6de & \\ Pegs & 4x4randon & 5x7cross & \\ Range & 3x3 & 9x6 & \\ Rectangles & 3x2 & 7x7 & \\ Same Game & 2x3c3s2 & 5x5c3s2 & \\ Signpost & 2x3 & 4x4c & \\ Singles & 2x3de & 5x5de & \\ Sixteen & 2x3 & 3x3 & \\ Slant & 2x2de & 5x5de & \\ Solo & 2x2 & 2x2 & \\ Tents & 4x4de & 8x8de & \\ Towers & 3de & 4de & \\ Tracks & 4x4de & 8x8de & \\ Twiddle & 2x3n2 & 3x3n2r & \\ Undead & 3x3de & 4x4de & \\ Unequal & 3de & 4de & \\ Unruly & 6x6dt & 8x8dt & Even the minimum allowed problem size & \\  & & & proved to be infeasible for a random agent \\ Untangle & 4 & 6 & \\ \hline \hline \end{tabular}
\end{table}
Table 7: Listed below are the generation parameters supplied to each puzzle instance before training an agent, as well as some puzzle-specific notes. We propose the easiest preset difficulty setting as a first challenge for RL algorithms to reach human-level performance.

### Human Expert Evaluation

In order to provide more context on the difficulty of the puzzles, we report the results of a human expert solving all puzzles using the difficulties defined in Table 7.

\begin{table}
\begin{tabular}{l c c} \hline \hline Puzzle & Config & Episode Length \\ \hline \multirow{2}{*}{Black Box} & w2h2m2M2 & 20.3 \\  & w5h5m3M3 & 120.0 \\ \hline \multirow{2}{*}{Bridges} & 3x3 & 8.3 \\  & 7x7m2 & 49.3 \\ \hline \multirow{2}{*}{Cube} & c3x3 & 46.0 \\  & c4x4 & 31.0 \\ \hline \multirow{2}{*}{Dominosa} & 1dt & 10.7 \\  & 3dt & 51.5 \\ \hline \multirow{2}{*}{Fifteen} & 2x2 & 4.7 \\  & 4x4 & 287.0 \\ \hline \multirow{2}{*}{Filling} & 2x3 & 8.0 \\  & 9x7 & 123.0 \\ \hline \multirow{2}{*}{Flip} & 3x3c & 20.3 \\ \hline \multirow{2}{*}{Flood} & 3x3c6m5 & 10.3 \\  & 12x12c6m5 & 63.0 \\ \hline \multirow{2}{*}{Galaxies} & 3x3de & 20.7 \\  & 7x7dn & 244.5 \\ \hline \multirow{2}{*}{Guess} & c2p3g10Bm & 21.3 \\  & c6p4g10Bm & 63.0 \\ \hline \multirow{2}{*}{Inertia} & 4x4 & 4.3 \\  & 10x8 & 33.5 \\ \hline \multirow{2}{*}{Keen} & 3dem & 22.3 \\  & 4de & 89.0 \\ \hline \multirow{2}{*}{Light Up} & 3x3b20s0d0 & 10.3 \\  & 7x7b20s4d0 & 115.7 \\ \hline \multirow{2}{*}{Loopy} & 3x3t0de & 40.7 \\  & 7x7t0de & 292.0 \\ \hline \multirow{2}{*}{Magnets} & 3x3deS & 13.7 \\  & 6x5de & 103.0 \\ \hline \multirow{2}{*}{Map} & 3x3n5de & 6.7 \\  & 20x15n30 & 149.0 \\ \hline \multirow{2}{*}{Mines} & 4x4n2 & 14.3 \\  & 9x9n10 & 152.0 \\ \hline \multirow{2}{*}{Mosaic} & 3x3 & 34.0 \\ \hline \multirow{2}{*}{Net} & 2x2 & 10.3 \\  & 5x5 & 125.0 \\ \hline \multirow{2}{*}{Netslide} & 2x3b1 & 16.7 \\  & 3x3b1 & 40.5 \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c} \hline \hline Puzzle & Config & Episode Length \\ \hline \multirow{2}{*}{Palisade} & 2x3n3 & 17.0 \\  & 5x5n5 & 147.0 \\ \hline \multirow{2}{*}{Pattern} & 3x2 & 15.3 \\  & 10x10 & 660.0 \\ \hline \multirow{2}{*}{Pearl} & 5x5de & 46.0 \\  & 6x6de & 245.0 \\ \hline \multirow{2}{*}{Pegs} & 4x4random & 27.3 \\  & 5x7cross & 253.0 \\ \hline \multirow{2}{*}{Range} & 3x3 & 7.7 \\  & 9x6 & 305.5 \\ \hline \multirow{2}{*}{Rect} & 3x2 & 7.0 \\  & 7x7 & 168.5 \\ \hline \multirow{2}{*}{Samegame} & 2x3c3s2 & 8.7 \\  & 5x5c3s2 & 37.0 \\ \hline \multirow{2}{*}{Signpost} & 2x3 & 21.7 \\  & 4x4 & 127.0 \\ \hline \multirow{2}{*}{Singles} & 2x3de & 7.0 \\  & 5x5de & 29.5 \\ \hline \multirow{2}{*}{Sixteen} & 2x3 & 41.0 \\  & 3x3 & 88.5 \\ \hline \multirow{2}{*}{Slant} & 2x2de & 10.0 \\  & 5x5de & 162.3 \\ \hline \multirow{2}{*}{Solo} & 2x2 & 51.3 \\ \hline \multirow{2}{*}{Tents} & 4x4de & 15.3 \\  & 8x8de & 262.0 \\ \hline \multirow{2}{*}{Towers} & 4de & 27.7 \\  & 4de & 104.0 \\ \hline \multirow{2}{*}{Tracks} & 4x4de & 37.7 \\  & 8x8de & 430.0 \\ \hline \multirow{2}{*}{Twiddle} & 2x3n2 & 22.3 \\  & 3x3 rows only & 31.0 \\ \hline \multirow{2}{*}{Undead} & 3x3de & 16.3 \\  & 4x4de & 68.5 \\ \hline \multirow{2}{*}{Unequal} & 3de & 18.7 \\  & 4de & 85.0 \\ \hline \multirow{2}{*}{Unruly} & 6x6dt & 94.7 \\  & 8x8dt & 375.5 \\ \hline \multirow{2}{*}{Untangle} & 4 & 6.0 \\  & 6 & 30.5 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Number of steps required on average to solve the puzzles by a human expert. The human expert was able to solve 100% of the puzzles.

### Detailed Baseline Results

We summarize all evaluated algorithms in Table 9.

As we limited the agents to a single final reward upon completion, where possible, we chose puzzle parameters that allowed random policies to successfully find a solution. Note that if a random policy fails to find a solution, an RL algorithm without guidance (such as intermediate rewards) will also be affected by this. If an agent has never accumulated a reward with the initial (random) policy, it will be unable to improve its performance at all.

The chosen parameters roughly corresponded to the smallest and easiest puzzles, as more complex puzzles were found to be intractable. This fact is highlighted for example in _Solo/Sudoku_, where the reasoning needed to find a valid solution is already rather complex, even for a grid with 2\(\times\)2 sub-blocks. A few puzzles were still intractable due to the minimum complexity permitted by Tatham's puzzle-specific problem generators, such as with _Unruly_.

For the RGB pixel observations, the window size chosen for these small problems was set at 128\(\times\)128 pixels.

\begin{table}
\begin{tabular}{l l c} \hline \hline Algorithm & Policy Type & Action Masking \\ \hline Proximal Policy Optimization (PPO) [67] & On-Policy & No \\ Recurrent PPO [68] & On-Policy & No \\ Advantage Actor Critic (A2C) [69] & On-Policy & No \\ Asynchronous Advantage Actor Critic (A3C) [69] & On-Policy & No \\ Trust Region Policy Optimization (TRPO) [70] & On-Policy & No \\ Deep Q-Network (DQN) [11] & Off-Policy & No \\ Quantile Regression DQN (QRDQN) [71] & Off-Policy & No \\ MuZero [72] & Off-Policy & Yes \\ DreamerV3 [73] & Off-Policy & No \\ \hline \hline \end{tabular}
\end{table}
Table 9: Summary of all evaluated RL algorithms.

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_EMPTY:35]

[MISSING_PAGE_EMPTY:36]

### Episode Length and Early Termination Parameters

In Table 13, the puzzles and parameters used for training the agents for the ablation in Section 3.4 are shown in combination with the results. Due to limited computational budget, we included only a subset of all puzzles at the easy human difficulty preset for DreameV3. Namely, we have selected all puzzles where a random policy was able to complete at least a single episode successfully within 10,000 steps in 1000 evaluations. It contains a subset of the more challenging puzzles, as can be seen by the performance of many algorithms in Table 10. For some puzzles, e.g. Netslide, Samegame, Sixteen and Untangle, terminating episodes early brings a benefit in final evaluation performance when using a large maximal episode length during training. For the smaller maximal episode length, the difference is not always as pronounced.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Puzzle** & **Supplied Parameters** & \# Steps & **ET** & **DreamerV3** \\ \hline \multirow{3}{*}{Bridges} & \multirow{3}{*}{7x7i30e10m2d0} & \(1e4\) & \(10\) & \(4183.0\pm 2140.5\) & (0.2\%) \\  & & & - & - & \\  & & & 10 & \(4017.9\pm 1390.1\) & (0.3\%) \\  & & & - & \(4396.2\pm 2517.2\) & (0.3\%) \\ \hline \multirow{3}{*}{Cube} & \multirow{3}{*}{cdx4} & \multirow{3}{*}{\(1e4\)} & \(10\) & \(21.9\pm 1.4\) & (100.0\%) \\  & & & - & \(21.4\pm 0.9\) & (100.0\%) \\  & & & 10 & \(22.6\pm 2.0\) & (100.0\%) \\  & & & - & \(21.3\pm 1.2\) & (100.0\%) \\ \hline \multirow{3}{*}{Flood} & \multirow{3}{*}{12x12c6n5} & \multirow{3}{*}{\(1e4\)} & \(10\) & - & \\  & & & - & & \\  & & & 10 & - & \\ \hline \multirow{3}{*}{Guess} & \multirow{3}{*}{c6p4g10bm} & \multirow{3}{*}{\(1e4\)} & \(10\) & - & \\  & & & - & \(1060.4\pm 851.3\) & (0.6\%) \\  & & & 10 & \(2405.5\pm 2476.4\) & (0.5\%) \\  & & & - & \(3165.2\pm 1386.8\) & (0.6\%) \\ \hline \multirow{3}{*}{Nestlide} & \multirow{3}{*}{3x3b1} & \multirow{3}{*}{\(1e4\)} & \(10\) & \(3820.3\pm 681.0\) & (18.4\%) \\  & & & - & \(3181.3\pm 485.5\) & (21.1\%) \\  & & & \(1e5\) & \(10\) & \(3624.9\pm 746.5\) & (23.0\%) \\  & & & - & \(4050.6\pm 505.5\) & (10.6\%) \\ \hline \multirow{3}{*}{Samegame} & \multirow{3}{*}{5x5c3a2} & \multirow{3}{*}{\(1e4\)} & \(10\) & \(53.8\pm 7.5\) & (38.3\%) \\  & & & - & \(717.4\pm 309.0\) & (29.1\%) \\  & & & 10 & \(47.3\pm 6.6\) & (36.7\%) \\  & & & - & \(1542.9\pm 824.0\) & (26.4\%) \\ \hline \multirow{3}{*}{Sigppost} & \multirow{3}{*}{4x4c} & \multirow{3}{*}{\(1e4\)} & \(10\) & \(6848.9\pm 677.7\) & (1.1\%) \\  & & & - & \(6861.8\pm 301.8\) & (1.5\%) \\  & & & 10 & \(6983.7\pm 392.4\) & (1.6\%) \\  & & & - & - & \\ \hline \multirow{3}{*}{Sixteen} & \multirow{3}{*}{3x3} & \multirow{3}{*}{\(1e4\)} & \(10\) & \(4770.5\pm 890.5\) & (2.9\%) \\  & & & - & \(4480.5\pm 2259.3\) & (25.5\%) \\  & & & 10 & \(3193.3\pm 2262.0\) & (57.0\%) \\  & & & - & \(35171.7\pm 1846.7\) & (23.5\%) \\ \hline \multirow{3}{*}{Undead} & \multirow{3}{*}{4x4de} & \multirow{3}{*}{\(1e4\)} & \(10\) & \(5378.0\pm 1552.7\) & (0.5\%) \\  & & & - & \(5324.4\pm 557.9\) & (0.6\%) \\  & & & \(1e5\) & \(10\) & \(5666.2\pm 553.3\) & (0.5\%) \\  & & & - & \(5771.3\pm 2323.6\) & (0.4\%) \\ \hline \multirow{3}{*}{Untangle} & \multirow{3}{*}{6} & \multirow{3}{*}{\(1e4\)} & \(10\) & \(4747.4\pm 1176.6\) & (99.1\%) \\  & & & - & \(1491.9\pm 193.8\) & (89.3\%) \\ \cline{1-1}  & & & \(1e5\) & \(10\) & \(597.0\pm 305.5\) & (96.3\%) \\ \cline{1-1}  & & & - & \(1338.4\pm 283.6\) & (88.7\%) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Listed below are the puzzles and their corresponding supplied parameters. For each setting, we report average success episode length with standard deviation with respect to the random seed, all averaged over all selected puzzles. In brackets, the percentage of successful episodes is reported. # Steps stands for the maximal episode length, ET stands for early termination after a given number of state repeats.

### LLM Evaluation

We have extended the PUZZLES library with an interface for large language models (LLMs) and vision language models (VLMs). This allows for easy evaluation of these models on truly out-of-distribution data. Thanks to the scalability of our benchmark, puzzle sizes can be continuously increased to match the current capabilities of the models. Currently, we focus on zero-shot evaluation of LLMs/VLMs, where the models are not provided with any specific training on the puzzle they need to solve. This is similar to a human solving a puzzle for the first time. For future evaluations, however, this could be improved by either training the model on a smaller version of the puzzle, or giving it some examples and allowing it to explicitly come up with a strategy. The LLM/VLM had to play the puzzle using the same cursor interface as the RL agents and the human expert, requiring it to plan ahead and execute single steps. This also means that the model was not able to solve the puzzle directly by outputting a solution in text format.

To reduce the computational cost of LLM evaluation, we implemented early termination after state repetition. Specifically, if an LLM enters the exact same state five times, indicating no progress in solving the puzzle, the evaluation terminates. Additionally, for each puzzle, we set the maximum number of steps to the upper bound of an optimal policy.

During evaluation, the LLM is not provided with the complete history of all past actions and states. Instead, it receives only the explanation of the game, the current discrete state, an image of the current puzzle state, and the most recent past action. Action masking is employed to limit the LLM's choices to reasonable actions.

The game explanations and instructions for playing the game using the keyboard are sourced from https://www.chiark.greenend.org.uk/~sgatham/puzzles/doc/. Further details about the LLM evaluation framework are available on the official GitHub repository at https://github.com/ETH-DISCO/rlp/tree/main/llm. The code is designed to easily support the addition and evaluation of new LLMs. For more information, please refer to https://github.com/ETH-DISCO/rlp?tab=readme-ov-file#run-an-llm-that-is-available-via-api.

We conducted experiments with Gemini 1.5 Flash and GPT-4o mini, allowing each LLM five attempts to solve each puzzle. Our results show that GPT-4o mini solved slightly more puzzles than Gemini 1.5 Flash. Interestingly, the evaluation with GPT-4o-mini took approximately 5 hours, while Gemini 1.5 Flash required only 1 hour.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [Yes] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A]

\begin{table}
\begin{tabular}{l c c c} \hline \hline Puzzle & Supplied Parameters & Gemini 1.5 Flash Success Rate (\%) & GPT-4o mini Success Rate (\%) \\ \hline Blackbox & w2h2m2M2 & 0 & 0 \\ Bridges & 3x3 & 0 & 0 \\ Cube & c3x3 & 0 & 0 \\ Dominosa & 1dt & 0 & 0 \\ Fifteen & 2x2 & 20 & 100 \\ Fifteen & 3x3n5de & 0 & 0 \\ Filling & 2x3 & 0 & 0 \\ Flip & 3x3c & 0 & 0 \\ Flood & 3x3c6m5 & 20 & 20 \\ Galaxies & 3x3de & 0 & 0 \\ Guess & c2p3g10Bm & 0 & 0 \\ Inertia & 4x4 & 0 & 0 \\ Keen & 3dem & 0 & 0 \\ Lightup & 3x3b20s0d0 & 0 & 0 \\ Loopy & 3x3t0de & 0 & 0 \\ Magnets & 3x3deS & 0 & 20 \\ Map & 3x3n5de & 0 & 0 \\ Mines & 4x4n2 & 0 & 0 \\ Mosaic & 3x3 & 0 & 0 \\ Net & 2x2 & 0 & 0 \\ Netslide & 2x3b1 & 0 & 0 \\ Palisade & 2x3n3 & 0 & 0 \\ Pattern & 3x2 & 0 & 0 \\ Pearl & 5x5de & 0 & 0 \\ Pegs & 4x4random & 0 & 0 \\ Range & 3x3 & 0 & 0 \\ Rect & 3x2 & 0 & 0 \\ Samegame & 2x3c3s2 & 0 & 40 \\ Signpost & 2x3 & 0 & 0 \\ Singles & 2x3de & 0 & 0 \\ Sixteen & 2x3 & 0 & 0 \\ Slant & 2x2de & 0 & 0 \\ Solo & 2x2 & 0 & 0 \\ Tents & 4x4de & 0 & 0 \\ Towers & 3de & 0 & 0 \\ Tracks & 4x4de & 0 & 0 \\ Twiddle & 2x3n2 & 0 & 20 \\ Undead & 3x3de & 0 & 0 \\ Unequal & 3de & 0 & 0 \\ Unruly & 6x6dt & 0 & 0 \\ Untangle & 4 & 0 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Success rates of LLMs on the easiest setting for all puzzles.

2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? All code we use for creating the benchmark is released under the MIT license, see Appendix C. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]