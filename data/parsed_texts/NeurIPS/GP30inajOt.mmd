# Retraction-free optimization over the Stiefel manifold with application to the LoRA fine-tuning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Optimization over the Stiefel manifold has played a significant role in various machine learning tasks. Many existing algorithms either use the retraction operator to keep each iterate staying on the manifold, or solve an unconstrained quadratic penalized problem. The retraction operator in the former corresponds to orthonormalization of matrices and can be computationally costly for large-scale matrices. The latter approach usually equips with an unknown large penalty parameter. To address the above issues, we propose a retraction-free and penalty parameter-free algorithm, which lands on the manifold. A key component of the analysis is the convex-like property of the quadratic penalty of the Stiefel manifold, which enables us to explicitly characterize the penalty parameter. As an application, we introduce a new algorithm, Manifold-LoRA, which employs the landing technique and a carefully designed step size strategy to accelerate low-rank adaptation (LoRA) in fine-tuning large language models. Numerical experiments on the benchmark datasets demonstrate the efficiency of our proposed method.

## 1 Introduction

Optimization over the Stiefel manifold has attracted considerable attention in the context of machine learning, e.g., RNN [3], batch normalization [10], and distributionally robust optimization [8]. The mathematical formulation of this class of problems is:

\[\min_{X\in\mathbb{R}^{d\times r}}\ \ f(X)\ \ \mathrm{subject\ to}\ \ \ X\in\mathrm{St}(d,r):=\{X\in\mathbb{R}^{d\times r}:X^{\top}X=I_{d}\}, \tag{1}\]

where \(r\leq d\) and \(f:\mathbb{R}^{d\times r}\rightarrow\mathbb{R}\) is a continuously differentiable function. The most popular methods for solving (1) are retraction-based algorithms, which have been extensively studied in the context of manifold optimization [2; 23; 6]. Recently, to alleviate the possible computational burden of the retraction operator, some retraction-free methods have been developed in [19; 18; 41; 1]. The ideas in these papers are based on a combination of the manifold geometry and a penalty function for the manifold constraint, which involves an unknown but sufficiently large penalty parameter. For large-scale machine learning applications, retraction-free algorithms are preferred. However, designing retraction-free algorithms with a known penalty parameter for solving (1) remains a challenge.

Another motivation for studying retraction-free methods arises from its application in the fine-tuning of large language models (LLMs). Recently, LLMs have revolutionized the field of natural language processing (NLP), achieving unprecedented performance across various applications [33; 32]. To tailor pretrained LLMs for specific downstream tasks, the most common approach is full fine-tuning, which requires prohibitively large computational resources due to the need to adapt all model weights, hindering the deployment of large models. As a result, parameter-efficient fine-tuning (PEFT) has gained widespread attention for requiring few trainable parameters while delivering comparableor even superior results to full fine-tuning. This paradigm involves inserting learnable modules or designating only a small portion of weights as trainable, keeping the main model frozen [21, 26, 44]. Among fine-tuning methods, low-rank adaptation (LoRA) [22] has become the de factor standard among parameter-efficient fine-tuning techniques. It assumes that the change in weights lies in a "low intrinsic dimension", thereby modelling the update \(\Delta W\in\mathbb{R}^{d\times m}\) by two low-rank (not greater than a small integer \(r\)) matrices \(A\in\mathbb{R}^{r\times m}\) and \(B\in\mathbb{R}^{d\times r}\), i.e., \(\Delta W=BA\). Since \(r\ll d\), the requirements on both storage and computation are significantly reduced. Due to its decompositional nature, there is redundancy in the representation of \(\Delta W\). Traditional optimization methods for LoRA do not exploit this redundancy, which consequently undermines model performance. Instead, we reformulate LoRA fine-tuning as an optimization problem over the product of Stiefel manifolds and Euclidean spaces. Therefore, we propose an algorithmic framework called Manifold-LoRA to accelerate the fine-tuning process and enhance model performance. Moreover, by exploiting projected gradients and incorporating a parameter-free penalty, the overhead that our method incurs is relatively negligible. Our contributions are as follows:

* We first prove the existence of explicit choice for the penalty parameter by establishing a strong convexity-like condition of the nonconvex penalty problem associated with the Stiefel manifold constraint. Furthermore, for the given penalty parameter, under mild conditions, we prove that the iterates of our proposed retraction-free gradient descent method eventually land on the Stiefel manifold and achieve the optimality of (1).
* Building upon the established landing theory of retraction-free and penalty parameter-free method and the AdamW framework, we proposed a new method, Manifold-LoRA, which employs a carefully designed step size strategy to accelerate the training process of fine-tuning. Compared with the conventional AdamW method, we use the penalized gradient instead of the usual gradient, and the computational overhead is negligible.
* Numerical experiments are conducted on a wide range of NLP tasks, demonstrating the efficiency of our algorithm. Specifically, compared to the vanilla LoRA, our Manifold-LoRA with half the trainable parameters not only delivers fast convergence but also yields improved generalization. In particular, Our method converges twice as fast as baseline methods on several typical datasets, including the SQuAD 2.0 dataset and the CoLA dataset.

### Related Work

**Optimization over the Stiefel manifold.** Optimization over the Stiefel manifold has attracted lots of attention due to its broad applications. Through the use of retraction, known as the generalization of the exponential map, the Riemannian gradient descent is proposed [2, 6, 23], where all iterates lie on the manifold. When such retraction is computationally costly, the authors [19] develop a retraction-free algorithm based on the augmented Lagrangian method. More recently, by defining the constraint dissolving operator and adding a sufficiently large penalty term, the authors [41] convert the manifold constrained problem (1) into an unconstrained problem and then apply unconstrained optimization algorithms. In [1], motivated by the convergence of the Oja's flow, a landing flow, consisting of the projected gradient and the gradient of the penalty function, is developed to retraction-free method for the squared Stiefel manifold, i.e., \(d=r\). All of these methods rely on an unknown penalty parameter to ensure the convergence. This motivates us to design penalty parameter-free algorithms, which could significantly reduce the need for tuning parameters in practical implementations.

**LoRA.** There are numerous variants of LoRA aiming to improve performance or reduce memory usage. AdaLoRA [46], a well-known successor, introduces the idea of adaptively adjusting the rank of different layers by incorporating an additional vector \(\mathbf{g}\) to serve as the diagonal of a singular value matrix. This approach leverages a revised sensitivity-based importance measure to decide whether to disable entries in vector \(\mathbf{g}\) and in matrices \(A\) and \(B\). A similar work, SoRA [15], adopts the same model architecture as AdaLoRA, but proposes a different way to update vector \(\mathbf{g}\) after training. This update rule is the proximal gradient of \(\mathcal{L}_{1}\) loss, acting as a post-pruning method. Additionally, a recently emerged method called VeRA [25] significantly reduces memory overhead while maintaining competitive performance. Based on the idea that networks with random initialization contain subnetworks that are near-optimal or optimal [17], VeRA only uses two frozen low-rank matrices shared by all layers, training scaling vectors unique to each layer. Although LoRA has gained significant popularity and various variants have been developed, the potential for efficient training through leveraging the manifold geometry to reduce redundancy has not been well-explored.

### Notation

For a matrix \(X\in\mathbb{R}^{d\times r}\), we use \(\|X\|\) to denote its Frobenius norm. For a squared matrix \(A\in\mathbb{R}^{d\times d}\), we define \(\mathrm{sym}(A)=\frac{A+A^{\top}}{2}\) and use \(\mathrm{diag}(A)\in\mathbb{R}^{d}\) to denote its diagonal part. For two matrices \(X,Y\in\mathbb{R}^{d\times r}\), we use \(\langle X,Y\rangle:=\sum_{i=1}^{d}\sum_{j=1}^{r}X_{ij}Y_{ij}\) to denote their Euclidean inner product. For a differential function \(f:\mathbb{R}^{d\times r}\to d\), we use \(\nabla f(X)\) to denote its Euclidean gradient at \(X\).

## 2 Retraction-free and penalty parameter-free optimization over the Stiefel manifold

In this section, we focus on the design of retraction-free and penalty parameter-free algorithms for solving problem (1). We will first present the retraction-free algorithm and then show how the penalty parameter can be explicitly determined by characterizing the landscape of the penalty function.

### Retraction-free algorithms

Inspired by the retraction-free algorithms [19; 41; 1], we consider the following retraction-free gradient descent method for problem (1):

\[X_{k+1}=X_{k}-\alpha\mathrm{grad}f(X_{k})-\mu X_{k}(X_{k}^{\top}X_{k}-I_{d}), \tag{2}\]

where \(\alpha,\mu>0\) are step sizes and the projected gradient \(\mathrm{grad}f(X_{k}):=\nabla f(X_{k})-X_{k}\mathrm{sym}(X_{k}^{\top}\nabla f (X_{k}))\). Note that the tangent space of \(\mathrm{St}(d,r)\) is \(T_{X_{k}}\mathrm{St}(d,r):=\{\xi\in\mathbb{R}^{d\times r}:X_{k}^{\top}\xi+\xi ^{\top}X_{k}=0\}\). Then, for \(X_{k}\in\mathrm{St}(d,r)\), \(\mathrm{grad}f(X_{k})\) is the projection of the Euclidean gradient \(\nabla f(X_{k})\) to the tangent space, i.e., \(\mathrm{grad}f(X_{k})=\mathcal{P}_{T_{X_{k}}\mathrm{St}(d,r)}(\nabla f(X_{k}))\). Note that the term \(X_{k}(X_{k}^{\top}X_{k}-I_{d})\) is exactly the gradient of the following quadratic penalty function

\[\varphi(X):=\frac{1}{4}\|X^{\top}X-I\|^{2}.\]

As will be shown in our theorem, the use of the projected gradient is essential for landing on the manifold. This differs with the usual penalty method, which optimizes \(f(X)+\mu\varphi(X)\) using the update \(X_{k+1}=X_{k}-\alpha\nabla f(X_{k})-\mu X_{k}(X_{k}^{\top}X_{k}-I_{d})\), needs \(\mu\to\infty\) to guarantee the feasibility.

### Explicit choice for the penalty parameter

It is known that a large penalty parameter yields better feasibility [29; Chapter 17]. To make the iterative scheme (2) be penalty parameter-free, we need a careful investigation on the landscape of the following optimization problem:

\[\min_{X\in\mathbb{R}^{d\times r}}\ \ \varphi(X). \tag{3}\]

It can be easily verified that problem (3) is nonconvex and its the optimal solution set is \(\mathrm{St}(d,r)\). The key of obtaining an explicit formula of \(\mu\) is to establish certain strong convexity-type inequality and show the gradient descent method with step size \(\mu\) has linear convergence.

For any \(X\in\mathrm{St}(d,r)\), let us denote \(\bar{X}:=\mathcal{P}_{\mathrm{St}(d,r)}(X)\). Let \(X=USV^{\top}\) be the singular value decomposition with orthogonal matrices \(U\in\mathbb{R}^{d\times r},V\in\mathbb{R}^{d\times d}\) and diagonal matrix \(S\in\mathbb{R}^{d\times d}\), then \(\bar{X}=UV^{\top}\). Building on these notations, we demonstrate that problem (3) satisfies the restrict secant inequality (RSI) [45], which serves as an alternative to the strong convexity in the linear convergence analysis of gradient-type methods.

**Lemma 1**.: _For any \(X\in\mathbb{R}^{d\times r}\) with \(\|X-\bar{X}\|\leq\frac{1}{8}\), we have_

\[\left\langle\nabla\varphi(X),X-\bar{X}\right\rangle\geq\|X-\bar{X}\|^{2}. \tag{4}\]

With the above RSI, we have the linear convergence of the gradient descent update for (3), i.e.,

\[X_{k+1}=X_{k}-\mu\nabla\varphi(X_{k}). \tag{5}\]

**Lemma 2**.: _Let the sequence \(\{X_{k}\}\) be generated by (5) with \(\mu=\frac{1}{3}\). Suppose that \(\|X_{0}-\bar{X}_{0}\|\leq\frac{1}{8}\). We have_

\[\|X_{k+1}-\bar{X}_{k+1}\|^{2}\leq\frac{2}{3}\|X_{k}-\bar{X}_{k}\|^{2}. \tag{6}\]

The proofs of Lemmas 1 and 2 can be found in Appendix B.

### Landing on the Stiefel manifold

Building on the established linear convergence of gradient descent for problem (3), we are now able to show that the iterates generated by (2) will land on the Stiefel manifold eventually, and the limiting point is a stationary point of (1), i.e., \(\mathrm{grad}f(X_{\infty})=0\).

Let us start with the Lipschitz continuity of \(\mathrm{grad}f(X)\). For any \(X\in\bar{U}_{\mathrm{St}(d,r)}(\frac{1}{8})\), we define \(\mathcal{P}_{T_{X}\mathrm{St}(d,r)}(U)=U-X\mathrm{sym}(X^{\top}U)\) for \(U\in\mathbb{R}^{d\times r}\). We first have the following quadratic upper bound on \(f\) from its twice differentiability and the compactness of \(\mathrm{St}(d,r)\).

**Lemma 3**.: _There exists a constant \(L>0\) such that for any \(X,Y\in\mathrm{St}(d,r)\), the following quadratic upper bound holds:_

\[f(Y)\leq f(X)+\langle\mathrm{grad}f(X),Y-X\rangle+\frac{L}{2}\|Y-X\|^{2}. \tag{7}\]

_In addition, there exists a constant \(\hat{L}>0\) such that for any \(X\in\mathrm{St}(d,r),Y\in U_{\mathcal{M}}(\frac{1}{8})\),_

\[\|\mathrm{grad}f(X)-\mathrm{grad}f(Y)\|\leq\hat{L}\|X-Y\|. \tag{8}\]

By the linear convergence result in Lemma 2, we have the following bound on the feasibility error.

**Lemma 4**.: _Let \(\{X_{k}\}\) be the sequence generated by (2) with \(\mu=\frac{1}{3}\) and \(\|X_{0}-\bar{X}_{0}\|\leq\frac{1}{8}\). We have_

\[\|X_{k+1}-\bar{X}_{k+1}\|\leq\frac{2}{3}\|X_{k}-\bar{X}_{k}\|+\alpha\|\mathrm{ grad}f(X_{k})\|. \tag{9}\]

The following one-step descent lemma on \(f\) is crucial in establishing the convergence.

**Lemma 5**.: _Let \(\{X_{k}\}\) be the sequence generated by (2) with \(\mu=\frac{1}{3}\) and \(\|X_{0}-\bar{X}_{0}\|\leq\frac{1}{8}\). We have_

\[f(\bar{X}_{k+1})-f(\bar{X}_{k})\leq -(\alpha-(4\hat{L}^{2}+4L+1)\alpha^{2})\|\mathrm{grad}f(X_{k})\|^{ 2}+\frac{1}{2}\|X_{k+1}-\bar{X}_{k+1}\|^{2} \tag{10}\] \[+\frac{1}{2}\left(4\hat{D}_{f}+16\hat{L}^{2}+16L+3\right)\|X_{k}- \bar{X}_{k}\|^{2}.\]

From the above lemma, the one-step descreasing on \(f\) is related to both the gradient norm of \(f\) and the feasibility error. In terms of convergence, we need both \(\mathrm{grad}f(X_{k})\) and \(\|X_{k}^{\top}X_{k}-I\|\) converge to 0. The following theorem demonstrates that the retraction-free and penalty parameter-free update (2) converges.

**Theorem 1**.: _Let \(\{X_{k}\}\) be the sequence generated by (2) with \(\mu=\frac{1}{3}\) and \(\|X_{0}-\bar{X}_{0}\|\leq\frac{1}{8}\). If the step size \(\alpha<\frac{1}{2c_{1}}\) for some \(c_{1}\) large enough, then we have_

\[\min_{k=0,\dots,K}\|\mathrm{grad}f(X_{k})\|^{2}\leq\frac{1}{K},\quad\min_{k=0, \dots,K}\|X_{k}^{\top}X_{k}-I\|^{2}\leq\frac{1}{K}. \tag{11}\]

The proofs of the above lemmas and theorem are presented in Appendix B.

## 3 Accelerate LoRA fine-tuning with landing

In this section, we will first clarify where the Stiefel manifold constraint comes from in the LoRA fine-tuning. Then, we will apply the above developed retraction-free and penalty parameter-free method to enhance LoRA fine-tuning.

### Manifold optimization formulation of LoRA fine-tuning

In neural networks, the dense layers perform matrix multiplication, and the weight matrices in these layers usually have a full rank. However, when adapting to a specific task, pre-trained language models have been shown to have a low intrinsic dimension, allowing them to learn efficiently even with a random projection to a smaller subspace. One possible drawback in the current LoRA fine-tuning framework is that the low-rank decomposition \(\Delta W\) into product \(BA\) is not unique. Specifically, for any invertible matrix \(C\), it holds that \(BA=(BC)(C^{-1}A)\). Note that \(BC\) shares the samecolumn space with \(B\). This suggests us optimizing the subspace generated by \(B\) instead of \(B\) itself. Numerous studies in the field of low-rank optimization, e.g., [7; 13; 12], investigate the manifold geometry of the low-rank decomposition and develop efficient algorithms. However, such geometry has not been explored in the LoRA fine-tuning.

To address such redundancy (i.e., the non-uniqueness of \(BA\) representations), we regard \(B\) as the basis through the manifold constraint and \(A\) as the coordinate of \(\Delta W\) under \(B\). Hence, the optimization problem can be formulated as

\[\min_{A,B}\quad L(BA),\quad\mathrm{subject\ to}\quad\ B\in\mathrm{St}(d,r)\; \mathrm{or}\;B\in\mathrm{Ob}(d,r), \tag{12}\]

where \(\mathrm{Ob}(d,r):=\{B\in\mathbb{R}^{d\times r}:\mathrm{diag}(B^{\top}B)=\mathbf{1}\}\). Compared to the Stiefel manifold \(\mathrm{St}(d,r)\), the oblique manifold \(\mathrm{Ob}(d,r)\) necessitates that the matrix \(B\) has unit norms in its columns, without imposing requirements for orthogonality between the columns. Problem (12) is an optimization problem over the product of manifolds and Euclidean spaces.

### Manifold-LoRA

The retraction-free method is well-suited to address (12), simultaneously minimizing the loss function \(L(BA)\) and constraint violation. To control the constraint violation, we use the quadratic penalties \(R_{s}(B):=\|B^{\top}B-I\|^{2}\) and \(R_{o}(B):=\|\mathrm{diag}(B^{\top}B)-1\|^{2}\) for the Stiefel manifold and oblique manifold, respectively. As shown in the landing theory in Section 2, we shall use the projected gradient of the loss part instead of the Euclidean gradient. For the Stiefel manifold and the oblique manifold, the respective projected gradients are

\[\mathrm{grad}_{B}L(BA)=\nabla_{B}L(BA)-B\mathrm{sym}(B^{\top}\nabla_{B}L(BA)) \tag{13}\]

and

\[\mathrm{grad}_{B}L(BA)=\nabla_{B}L(BA)-B\mathrm{diag}(\mathrm{diag}(B^{\top} \nabla_{B}L(BA))), \tag{14}\]

where \(\mathrm{sym}(X):=(X+X^{\top})/2\). Thus, the gradients of our retraction-free method for \(A\) and \(B\) are \(\nabla_{A}L(BA)\) and \(\mathrm{grad}_{B}L(BA)+\mu\nabla R_{s}(B)(\mathrm{or}\;\nabla R_{o}(B))\).

Note that \(B\) and \(A\) represent the basis and the coordinate of \(\Delta W\), respectively. This results in different magnitudes and different Lipschitz constants of their gradient function. In fact, let \(X=BA\). It follows

\[\nabla_{A}L(BA)=B^{\top}\nabla_{X}L(X),\quad\nabla_{B}L(BA)=\nabla_{X}L(X)A^{ \top}.\]

Then,

\[\|\nabla_{A}L(BA_{1})-\nabla L(BA_{2})\|\leq\|B\|_{2}L_{g}\|A_{1}-A_{2}\|,\]

\[\|\nabla_{B}L(B_{1}A)-\nabla L(BA_{2}A)\|\leq\|A\|_{2}L_{g}\|B_{1}-B_{2}\|,\]

where \(L_{g}\) is the Lipschitz constant of \(\nabla_{X}L(X)\) and \(\|\cdot\|_{2}\) represent the matrix \(\ell_{2}\) norm (i.e., the largest singular value). Note that the step size generally should be propositional to the reciprocal of Lipschitz constant for the gradient type algorithms [29; 5]. Hence, we schedule the learning rates for the two matrices based on their respective \(\ell_{2}\) norms. Having prepared the above, we incorporate the AdamW optimizer [28] with our manifold-accelerated technique to enhance the LoRA fine-tuning, as presented in Algorithm 1.

```
1:Input:\(B\), \(\ell_{2}\), \(\ell_{1}\), \(\ell_{2}\), \(\ell_{3}\), \(\ell_{4}\), \(\ell_{5}\), \(\ell_{6}\), \(\ell_{7}\), \(\ell_{8}\), \(\ell_{9}\), \(\ell_{10}\), \(\ell_{11}\), \(\ell_{12}\), \(\ell_{13}\), \(\ell_{14}\), \(\ell_{15}\closely with that of the LoRA method and the new layers are inserted into the attention layer and feed-forward layer. The update of LoRA is scaled by a hyper-parameter \(\alpha\). This value is typically left unmodified, as it is usually set as 16 or 32 and never tuned [22; 43]. The exponential moving average parameters \(\beta_{1}\) and \(\beta_{2}\) of AdamW [27] are set to their default values of 0.9 and 0.999, respectively. All the experiments are conducted on NVIDIA A800 GPUs. More details are presented in Appendix C.

### Natural language understanding

We first evaluate our backbone model DeBERTaV3-base [20] on GLUE [37] benchmark containing nine sub datasets, including MNLI [39], SST-2 [36], CoLA [38], QQP [37], QNLI [35], RTE [4], MRPC [16], and STS-B [37].

Experimental results of the GLUE dataset are recorded in Table 1. It can be seen that our method is consistently superior to other baselines. Notably, for RTE and STS-B datasets, both sphere-constrained (i.e., oblique manifold-constrained) and Stiefel-constrained have an obvious performance gain even with only half the trainable parameters compared to the LoRA baseline, i.e., Sphere\({}_{r=8}\) and Stiefel\({}_{r=8}\) beat LoRA\({}_{r=16}\). In addition, with the help of manifold geometry, the fine-tuning process can be significantly accelerated compared to the vanilla AdamW optimizer, achieving a lower training loss, as shown in Figure 1. Particularly on the CoLA dataset presented in Figure 0(a), our approach achieves the same training loss as the standard Adam optimizer but requires nearly half the number of epochs.

### Question Answering

We conduct an evaluation on two question answering datasets: SQuAD v1.1 [35] and SQuADv2.0 [34]. Manifold-LoRA is used to fine-tune DeBERTaV3-base for these tasks, which are treated as sequence labeling problems predicting the probability of each token as the start or end of an answer span.

The main experimental results are presented in Table 2. For LoRA and our algorithms, new layers are inserted into \(W_{q},W_{k},W_{v},W_{o},FC_{1},FC_{2}\). Notably, both manifold-regularized LoRA variants consistently outperform all fine-tuning methods. Additionally, we plot the training loss, evaluation exact match, and evaluation F1 scores against epochs in Figure 2. We conclude that the proposed Manifold-LoRA method achieves a 2x speed-up in training epochs compared to AdamW, while simultaneously improving model performance. We also illustrate the heat map of \(B^{\top}B\) in Figure 3, which indicates that the matrix \(B\) lands on the manifold eventually. This supports our assertion that landing on manifold enhances the performance of LoRA.

### Natural Language Generation

The E2E NLG Challenge[30], as introduced by Novikova, provides a dataset for training end-to-end, data-driven natural language generation systems, widely used in data-to-text evaluations. The E2E dataset comprises approximately 42,000 training examples, 4,600 validation examples, and 4,600 test examples, all from the restaurant domain. We test our method on the E2E dataset using GPT-2 Medium and Large models, following the experimental setup outlined by LoRA [22]. For LoRA, we set the hyperparameters to match those specified in the original paper.

The results from the E2E dataset are recorded in Table 3, where we focus on comparing LoRA and Manifold-LoRA. The results clearly indicate that our proposed algorithm outperforms the established baselines. Also, as shown in Figure 4, the matrix \(B\) resides on the manifold even at the early training stage, validating the feasibility of our method.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Method \# Params & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & All \\  & m / mm & Acc & Mcc & Acc / F1 & Acc & Acc & Acc & Corr & Ave. \\ \hline Full & \(184.42\)M & \(90.45\)**/90.60** & \(95.48\) & \(68.17\) & **91.99/89.12** & \(93.60\) & \(79.28\) & \(88.93\) & \(90.92\) & \(87.85\) \\ FT & \(0.61\)M & \(90.13\)/90.16 & \(94.86\) & \(69.37\) & \(91.38\)/\(88.46\) & \(93.54\) & \(81.87\) & \(89.12\) & \(91.52\) & \(88.06\) \\ BitFit & \(0.06\)M & \(87.08\)/\(86.39\) & \(94.88\) & \(69.11\) & \(87.96\)/\(84.35\) & \(92.19\) & \(76.52\) & \(87.06\) & \(90.96\) & \(85.65\) \\ LoRA\({}_{r=8}\) & \(0.30\)M & \(90.20\)/\(90.08\) & \(94.93\) & \(68.14\) & \(90.78\)/\(87.68\) & \(93.85\) & \(80.15\) & \(90.40\) & \(90.29\) & \(87.60\) \\ LoRA\({}_{r=16}\) & \(0.59\)M & \(90.44\)/\(90.12\) & \(95.41\) & \(68.19\) & \(90.92\)/\(87.77\) & \(94.00\) & \(Figure 3: The heat map of \(B^{\top}B\) with the Stiefel manifold (the first and second rows) and the oblique manifold (the third and fourth rows) at the end of training on SQuADv2.0 dataset.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Methods & Params & SQuADv1.1 & SQuADv2.0 \\ \hline Full FT & 184M & 86.30 / 92.85 & 84.30 / 87.58 \\ Adapter\({}_{r=16}\) & 0.61M & 87.46 / 93.41 & 85.30 / 88.23 \\ Adapter\({}_{r=32}\) & 1.22M & 87.53 / 93.51 & 85.42 / 88.36 \\ Biffi & 0.07M & 80.26 / 88.79 & 74.21 / 87.19 \\ LoRA\({}_{r=8}\) & 1.33M & 87.90 / 93.88 & 85.56 / 88.52 \\ LoRA\({}_{r=16}\) & 2.65M & 87.94 / 93.75 & 85.90 / 88.81 \\ Sphere\({}_{r=8}\) & 1.33M & 88.51 / **94.25** & 86.33 / 89.20 \\ Sphere\({}_{r=16}\) & 2.65M & 88.32 / 94.03 & 86.15 / 89.03 \\ Stiefel\({}_{r=8}\) & 1.33M & **88.68** / 94.23 & 86.35 / 89.09 \\ Stiefel\({}_{r=16}\) & 2.65M & 88.25 / 94.04 & **86.41** / **89.22** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. We report EM/F1. The best results in each setting are shown in **bold**.

Figure 2: The figures compare the training loss, evaluation exact match, and evaluation F1 metrics against the number of epochs for the SQuADv2.0 dataset.

## 5 Conclusion

Optimization over the Stiefel manifold has been widely used in machine learning tasks. In this work, we develop a retraction-free and penalty parameter-free gradient method, and prove that the generated iterates eventually land on the manifold and achieve the optimality simultaneously. We then apply this landing theory to avoid the possible redundancy of LoRA fine-tuning in LLMs. Specifically, we reformulate the LoRA fine-tuning as an optimization problem over the Stiefel manifold, and propose a new algorithm, Manifold-LoRA, which incorporates a careful analysis of step sizes to enable fast training using the landing properties. Extensive experimental results demonstrate that our approach not only accelerates the training process but also yields significant performance improvements.

Our study suggests several potential directions for future research. Although the established landing theory focuses on the Stiefel manifold, extending this theory to general manifolds is one potential direction. Additionally, evaluating the performance of Manifold-LoRA on LLMs with billions of parameters would be valuable. Due to the heterogeneity of different layers, incorporating adaptive ranks for \(\Delta W\) across different layers is another possible direction. This may be achievable by adding sparsity regularization to the coordinate matrix \(A\).

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Model & Parameters & BLEU & NIST & MET & ROUGE-L & CIDEr \\ \hline GPT-2 M (FT)* & 354.92M & 68.2 & 8.62 & 46.2 & 71.0 & 2.47 \\ GPT-2 M (Adapter\({}^{\rm L}\))* & 0.37M & 66.3 & 8.41 & 45.0 & 69.8 & 2.40 \\ GPT-2 M (Adapter\({}^{\rm L}\))* & 11.09M & 68.9 & 8.71 & 46.1 & 71.3 & 2.47 \\ GPT-2 M (Adapter\({}^{\rm H}\))* & 11.09M & 67.3\({}_{\pm.6}\) & \(8.50_{\pm.07}\) & \(46.0_{\pm.2}\) & \(70.7_{\pm.2}\) & \(2.44_{\pm.01}\) \\ GPT-2 M (FT\({}^{\rm Ho}\))* & 25.19M & 68.1 & 8.59 & 46.0 & 70.8 & 2.41 \\ GPT-2 M (PrLex)* & 0.35M & 69.7 & 8.81 & 46.1 & 71.4 & 2.49 \\ GPT-2 M (LoRA) & 0.35M & 68.9 & 46.5 & 71.5 & 2.51 \\ GPT-2 M(Stiefel) & 0.35M & 70.1 & 8.82 & **46.8** & **71.7** & **2.53** \\ GPT-2 M(Sphere) & 0.35M & **70.3** & **8.83** & 46.7 & **71.7** & 2.52 \\ \hline GPT-2 L (FT)* & 774.03M & 68.5 & 8.78 & 46.0 & 69.9 & 2.45 \\ GPT-2 L (Adapter\({}^{\rm L}\))* & 0.88M & \(69.1_{\pm.1}\) & \(8.68_{\pm.03}\) & \(46.3_{\pm.0}\) & \(71.4_{\pm.2}\) & \(2.49_{\pm.0}\) \\ GPT-2 L (Adapter\({}^{\rm L}\))* & 23.00M & \(68.9_{\pm.3}\) & \(8.70_{\pm.04}\) & \(46.1_{\pm.1}\) & \(71.3_{\pm.2}\) & \(2.45_{\pm.02}\) \\ GPT-2 L (PreLayer)* & 0.77M & 70.3 & 8.85 & 46.2 & 71.7 & 2.47 \\ GPT-2 L (LoRA) & 0.77M & 70.1 & 8.82 & 46.7 & 72.0 & 2.53 \\ GPT-2 L(Stiefel) & 0.77M & 70.4 & 8.86 & **46.8** & 72.1 & 2.53 \\ GPT-2 L(Sphere) & 0.77M & **70.9** & **8.92** & **46.8** & **72.5** & **2.55** \\ \hline \hline \end{tabular}
\end{table}
Table 3: GPT-2 medium (M) and large (L) models were evaluated on the E2E NLG Challenge. * denotes results from previously published works.

Figure 4: The heat map of \(B^{\top}B\) with the Stiefel manifold (left) and the oblique manifold (right) on E2E dataset.

## References

* Ablin and Peyre [2022] Pierre Ablin and Gabriel Peyre. Fast and accurate optimization on the orthogonal manifold without retraction. In _International Conference on Artificial Intelligence and Statistics_, pages 5636-5657. PMLR, 2022.
* Absil et al. [2008] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. _Optimization algorithms on matrix manifolds_. Princeton University Press, 2008.
* Arjovsky et al. [2016] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In _International conference on machine learning_, pages 1120-1128. PMLR, 2016.
* Bentivogli et al. [2009] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. _TAC_, 7(8):1, 2009.
* Bottou et al. [2018] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM review_, 60(2):223-311, 2018.
* Boumal [2023] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023.
* Boumal and Absil [2011] Nicolas Boumal and Pierre-antoine Absil. Rtrmc: A riemannian trust-region method for low-rank matrix completion. _Advances in neural information processing systems_, 24, 2011.
* Chen et al. [2017] Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non-convex objectives. _Advances in Neural Information Processing Systems_, 30, 2017.
* Chen et al. [2021] Shixiang Chen, Alfredo Garcia, Mingyi Hong, and Shahin Shahrampour. Decentralized Riemannian gradient descent on the Stiefel manifold. In _International Conference on Machine Learning_, pages 1594-1605. PMLR, 2021.
* Cho and Lee [2017] Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. _Advances in Neural Information Processing Systems_, 30, 2017.
* Clarke et al. [1995] Francis H Clarke, Ronald J Stern, and Peter R Wolenski. Proximal smoothness and the lower-C2 property. _Journal of Convex Analysis_, 2(1-2):117-144, 1995.
* Dai et al. [2012] Wei Dai, Ely Kerman, and Olgica Milenkovic. A geometric approach to low-rank matrix completion. _IEEE Transactions on Information Theory_, 58(1):237-247, 2012.
* Dai et al. [2011] Wei Dai, Olgica Milenkovic, and Ely Kerman. Subspace evolution and transfer (set) for low-rank matrix completion. _IEEE Transactions on Signal Processing_, 59(7):3120-3132, 2011.
* Deng and Hu [2023] Kangkang Deng and Jiang Hu. Decentralized projected riemannian gradient method for smooth optimization on compact submanifolds. _arXiv preprint arXiv:2304.08241_, 2023.
* Ding et al. [2023] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse low-rank adaptation of pre-trained language models. _arXiv preprint arXiv:2311.11696_, 2023.
* Dolan and Brockett [2005] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In _Third international workshop on paraphrasing (IWP2005)_, 2005.
* Frankle and Carbin [2018] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv preprint arXiv:1803.03635_, 2018.
* Gao et al. [2022] Bin Gao, Guanghui Hu, Yang Kuang, and Xin Liu. An orthogonalization-free parallelizable framework for all-electron calculations in density functional theory. _SIAM Journal on Scientific Computing_, 44(3):B723-B745, 2022.
* Gao et al. [2018] Bin Gao, Xin Liu, Xiaojun Chen, and Ya-xiang Yuan. A new first-order algorithmic framework for optimization problems with orthogonality constraints. _SIAM Journal on Optimization_, 28(1):302-332, 2018.

* [20] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. _arXiv preprint arXiv:2111.09543_, 2021.
* [21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International conference on machine learning_, pages 2790-2799. PMLR, 2019.
* [22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [23] Jiang Hu, Xin Liu, Zai-Wen Wen, and Ya-Xiang Yuan. A brief introduction to manifold optimization. _Journal of the Operations Research Society of China_, 8:199-248, 2020.
* [24] Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, and Maosong Sun. Opendelta: A plug-and-play library for parameter-efficient adaptation of pre-trained models. _arXiv preprint arXiv:2307.03084_, 2023.
* [25] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrix adaptation. _arXiv preprint arXiv:2310.11454_, 2023.
* [26] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. _arXiv preprint arXiv:2101.00190_, 2021.
* [27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2018.
* [29] Jorge Nocedal and Stephen J Wright. _Numerical optimization_. Springer, 1999.
* [30] Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The e2e dataset: New challenges for end-to-end generation. _arXiv preprint arXiv:1706.09254_, 2017.
* [31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [32] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? _arXiv preprint arXiv:2302.06476_, 2023.
* [33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. _arXiv preprint arXiv:1806.03822_, 2018.
* [35] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. _arXiv preprint arXiv:1606.05250_, 2016.
* [36] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* [37] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [38] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. _Transactions of the Association for Computational Linguistics_, 7:625-641, 2019.

* [39] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. _arXiv preprint arXiv:1704.05426_, 2017.
* [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics.
* [41] Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for riemannian optimization. _Mathematics of Operations Research_, 49(1):366-397, 2024.
* [42] Jinming Xu, Shanying Zhu, Yeng Chai Soh, and Lihua Xie. Augmented distributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes. In _2015 54th IEEE Conference on Decision and Control (CDC)_, pages 2055-2060. IEEE, 2015.
* [43] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. _arXiv preprint arXiv:2011.14522_, 2020.
* [44] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. _arXiv preprint arXiv:2106.10199_, 2021.
* [45] Hui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under weaker conditions. _arXiv preprint arXiv:1303.4645_, 2013.
* [46] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In _The Eleventh International Conference on Learning Representations_, 2023.

Proximal smoothness

The notion of proximal smoothness, as introduced by [11], refers to the characteristic of a closed set whereby the nearest-point projection becomes a singleton when the point is in close enough to the set. This property facilitates algorithmic and theoretical advancements by endowing nonconvex sets with convex-like structural attributes. Specifically, for any positive real number \(\gamma\), we define the \(\gamma\)-tube around \(\mathcal{M}\) as \(U_{\mathcal{M}}(\gamma):=\{x:\operatorname{dist}(x,\mathcal{M})<\gamma\}\). We say a closed set \(\mathcal{M}\) is \(\gamma\)-proximally smooth if the projection operator \(\mathcal{P}_{\mathcal{M}}(x):=\text{argmin}_{y\in\mathcal{M}}\|y-x\|^{2}\) is a singleton whenever \(x\in U_{\mathcal{M}}(\gamma)\).

Obviously, any closed and convex set is proximally smooth for arbitrary \(\gamma\in(0,\infty)\). According to [11, Corollary 4.6], a closed set \(\mathcal{M}\) is convex if and only if it is proximally smooth with a radius of \(\gamma\) for every \(\gamma>0\). It is worth noting that the Stiefel manifold is \(1\)-proximally smooth. By following the proof in [11, Theorem 4.8],

\[\big{\|}\mathcal{P}_{\operatorname{St}(d,r)}(x)-\mathcal{P}_{ \operatorname{St}(d,r)}(y)\big{\|}\leq 2\|x-y\|,\ \ \forall x,y\in\bar{U}_{ \operatorname{St}(d,r)}(\frac{1}{2}), \tag{15}\]

where \(\bar{U}_{\operatorname{St}(d,r)}(\frac{1}{2}):=\{x:\operatorname{dist}(x, \operatorname{St}(d,r))\leq\frac{1}{2}\}\) is the closure of \(U_{\operatorname{St}(d,r)}(\frac{1}{2})\). It is worth noting that for any closed convex set \(\mathcal{M}\subset\mathbb{R}^{d\times r}\), the projection operator \(\mathcal{P}_{\mathcal{M}}\) is \(1\)-Lipschitz continuous over \(\mathbb{R}^{d\times r}\).

## Appendix B Proofs

### Proof of Lemma 1

Proof.: Denote the SVD of \(X\) by \(X=USV^{\top}\). Then, it holds that \(\operatorname{dist}(X,\operatorname{St}(d,r))=\|X-\bar{X}\|=\|s-1\|_{2}\), where \(s=\operatorname{diag}(S)\). Furthermore, we have

\[\big{\langle}\nabla\varphi(X),X-\bar{X}\big{\rangle} =\big{\langle}USV^{\top}(VS^{2}V^{\top}-I),USV^{\top}-UV^{\top} \big{\rangle}\] \[=\big{\langle}U(S^{3}-S)V^{\top},U(S-I)V^{\top}\big{\rangle}\] \[=\operatorname{tr}((S^{3}-S)(S-I))\] \[\geq\frac{3}{2}\|s-1\|_{2}^{2}=\frac{3}{2}\|X-\bar{X}\|^{2},\]

where the last inequality is from \(\min_{i}s_{i}(s_{i}+1)\geq\frac{105}{64}\geq\frac{3}{2}\). This completes the proof. 

### Proof of Lemma 2

Proof.: Assume that \(\|X_{k}-\bar{X}_{k}\|\leq\frac{1}{8}\). Denote the SVD of \(X_{k}\) by \(USV^{\top}\). Let \(s=\operatorname{diag}(S)\). Then, we have \(\frac{7}{8}\leq s_{i}\leq\frac{9}{8}\) for any \(i\). This implies

\[\|\nabla\varphi(X_{k})\|^{2}=\operatorname{tr}((S^{3}-S)^{2})\leq 6\|X_{k}- \bar{X}_{k}\|^{2}. \tag{16}\]

Hence, we have

\[\|X_{k+1}-\bar{X}_{k+1}\|^{2} \leq\|X_{k+1}-\bar{X}_{k}\|^{2}\] \[=\|X_{k}-\frac{1}{3}\nabla\varphi(X_{k})-\bar{X}_{k}\|^{2}\] \[\leq(1-1+\frac{2}{3})\|X_{k}-\bar{X}_{k}\|^{2}\] \[=\frac{2}{3}\|X_{k}-\bar{X}_{k}\|^{2},\]

where the first inequality is from \(\bar{X}_{k+1}=\text{argmin}_{X\in\operatorname{St}(d,r)}\|X-X_{k}\|^{2}\) and the second inequality is due to Lemma 1 and (16). 
Proof.: Due to the twice differentiability of \(f\) and the compactness of \(\operatorname{St}(d,r)\), the inequality (7) directly follows from (9, Lemma 2.4) and (14, Lemma 4.2), where \(L:=L_{f}+D_{f}\) with \(L_{f}\) being the Lipschitz constant of \(\nabla f(X)\) over \(\operatorname{St}(d,r)\) and \(D_{f}:=\max_{X\in\operatorname{St}(d,r)}\|\nabla f(X)\|\).

For the second argument, we have

\[\|\mathrm{grad}f(X)-\mathrm{grad}f(Y)\|\] \[\leq \|\mathcal{P}_{T_{X}\operatorname{St}(d,r)}(\nabla f(X))-\mathcal{ P}_{T_{X}\operatorname{St}(d,r)}(\nabla f(Y))\|+\|\mathcal{P}_{T_{X}\operatorname{ St}(d,r)}(\nabla f(Y))-\mathrm{grad}f(Y)\|\] \[\leq L_{f}\|X-Y\|+\frac{1}{2}\|X(X^{\top}\nabla f(Y)+\nabla f(Y)^{ \top}X)-Y(Y^{\top}\nabla f(Y)+\nabla f(Y)^{\top}Y)\|\] \[\leq L_{f}\|X-Y\|+\frac{1}{2}\|X((X-Y)^{\top}\nabla f(Y)+\nabla f(Y)^{ \top}(X-Y))\|\] \[+\frac{1}{2}\|(X-Y)(Y^{\top}\nabla f(Y)+\nabla f(Y)^{\top}Y)\|\] \[\leq L_{f}\|X-Y\|+\frac{1}{2}(2\hat{D}_{f}+3\hat{D}_{f})\|X-Y\|\] \[= (L_{f}+\frac{5}{2}\hat{D}_{f})\|X-Y\|,\]

where \(\hat{D}_{f}:=\max_{X\in\hat{U}_{\operatorname{St}(d,r)}(\frac{1}{8})}\|\nabla f (X)\|\), the second inequality is due to the contractive property of \(\mathcal{P}_{T_{X}\operatorname{St}(d,r)}\), and the last inequality is from the fact that \(\|Y\|_{2}\leq\frac{3}{2}\). By setting \(\hat{L}=L_{f}+\frac{5}{2}\hat{D}_{f}\), we complete the proof. 

Proof of Lemma 4

Proof.: It follows that

\[\|X_{k+1}-\bar{X}_{k+1}\| \leq\|X_{k+1}-\bar{X}_{k}\|\] \[\leq\|X_{k}-\mu\varphi(X_{k})-\bar{X}_{k}\|+\alpha\|\mathrm{grad }f(X_{k})\|\] \[\leq\frac{2}{3}\|X_{k}-\bar{X}_{k}\|+\alpha\|\mathrm{grad}f(X_{k} )\|.\]

We complete the proof. 
Proof.: It follows from (7) that

\[f(\bar{X}_{k+1})-f(\bar{X}_{k})\leq\left\langle\mathrm{grad}f(\bar{X} _{k}),\bar{X}_{k+1}-\bar{X}_{k}\right\rangle+\frac{L}{2}\|\bar{X}_{k+1}-\bar{X} _{k}\|^{2}\] \[\leq \left\langle\mathrm{grad}f(\bar{X}_{k}),\bar{X}_{k+1}-X_{k+1}+X_{k }-\bar{X}_{k}\right\rangle+\left\langle\mathrm{grad}f(\bar{X}_{k}),X_{k+1}-X_{k}\right\rangle\] \[+2L\|X_{k+1}-X_{k}\|^{2}\] \[\leq \left\langle\mathrm{grad}f(\bar{X}_{k}),\bar{X}_{k+1}-X_{k+1} \right\rangle+\left\langle\mathrm{grad}f(\bar{X}_{k}),X_{k+1}-X_{k}\right\rangle\] \[+4L(\alpha^{2}\|\mathrm{grad}f(X_{k})\|^{2}+\mu^{2}\|\nabla\varphi (X_{k})\|^{2})\] \[= \left\langle\mathrm{grad}f(\bar{X}_{k})-\mathrm{grad}f(\bar{X}_{k +1}),\bar{X}_{k+1}-X_{k+1}\right\rangle+\left\langle\mathrm{grad}f(X_{k}),X_{k +1}-X_{k}\right\rangle\] \[+\left\langle\mathrm{grad}f(\bar{X}_{k})-\mathrm{grad}f(X_{k}),X_ {k+1}-X_{k}\right\rangle\] \[+4L(\alpha^{2}\|\mathrm{grad}f(X_{k})\|^{2}+\mu^{2}\|\nabla\varphi (X_{k})\|^{2})\] \[\leq 2\hat{L}^{2}\|X_{k+1}-X_{k}\|^{2}+\frac{1}{2}\|X_{k+1}-\bar{X}_{k +1}\|^{2}-\alpha\|\mathrm{grad}f(X_{k})\|^{2} \tag{17}\] \[-\mu\left\langle\mathrm{grad}f(X_{k}),\nabla\varphi(X_{k})\right\rangle +\frac{1}{2}(\hat{L}^{2}\|X_{k}-\bar{X}_{k}\|^{2}+\|X_{k+1}-X_{k}\|^{2})\] \[+4L(\alpha^{2}\|\mathrm{grad}f(X_{k})\|^{2}+\mu^{2}\|\nabla\varphi (X_{k})\|^{2})\] \[\leq -\alpha\|\mathrm{grad}f(X_{k})\|^{2}-\mu\left\langle\nabla f(X_{k }),\mathcal{P}_{T_{X_{k}}\mathrm{St}(d,r)}(\nabla\varphi(X_{k}))\right\rangle +\frac{1}{2}\|X_{k+1}-\bar{X}_{k+1}\|^{2}\] \[+\frac{1}{2}\|X_{k}-\bar{X}_{k}\|^{2}+(4\hat{L}^{2}+4L+1)(\alpha^ {2}\|\mathrm{grad}f(X_{k})\|^{2}+\mu^{2}\|\nabla\varphi(X_{k})\|^{2})\] \[\leq -(\alpha-(4\hat{L}^{2}+4L+1)\alpha^{2})\|\mathrm{grad}f(X_{k})\|^ {2}+\frac{1}{2}\|X_{k+1}-\bar{X}_{k+1}\|^{2}\] \[+(6\mu\hat{D}_{f}+\frac{1}{2}+16(4\hat{L}^{2}+4L+1)\mu^{2})\|X_{k }-\bar{X}_{k}\|^{2},\]

where the second inequality is from the 2-Lipschitz continuity of \(\mathcal{P}_{\mathrm{St}(d,r)}\) over \(\bar{U}_{\mathrm{St}(d,r)}(\frac{1}{8})\), the third inequality is due to the facts that \(X_{k}-\bar{X}_{k}\in N_{\bar{X}_{k}}\mathrm{St}(d,r)\) and \(\langle A,B\rangle\leq\frac{1}{2}(\|A\|^{2}+\|B\|^{2})\) for any \(A,B\in\mathbb{R}^{n\times d}\), and the last inequality comes from

\[\|\mathcal{P}_{T_{X_{k}}\mathrm{St}(d,r)}(\nabla\varphi(X_{k}))\|=\|X_{k}(X_{k }^{\top}X_{k}-I)^{2}\|\leq 6\|X_{k}-\bar{X}_{k}\|^{2}.\]

Plugging \(\mu=\frac{1}{3}\) into (17) gives (10). 

Proof of Theorem.: Applying [42, Lemma 2] to (9) yields

\[\sum_{k=0}^{K}\|X_{k}-\bar{X}_{k}\|^{2}\leq 18\alpha^{2}\sum_{k=0}^{K}\| \mathrm{grad}f(\bar{X}_{k})\|^{2}+4. \tag{18}\]

Then, summing (10) over \(k=0,\ldots,K\) gives

\[f(\bar{X}_{k+1})-f(\bar{X}_{0})\] \[\leq -(\alpha-(4\hat{L}^{2}+4L+1)\alpha^{2})\sum_{k=0}^{K}\|\mathrm{ grad}f(X_{k})\|^{2}\] \[+\frac{1}{2}\left(4\hat{D}_{f}+16\hat{L}^{2}+16L+3\right)\sum_{k =0}^{K+1}\|X_{k}-\bar{X}_{k}\|^{2} \tag{19}\] \[\leq -(\alpha-(4\hat{L}^{2}+4L+1)\alpha^{2}+9(4\hat{D}_{f}+16\hat{L}^{ 2}+16L+3)\alpha^{2})\sum_{k=0}^{K}\|\mathrm{grad}f(X_{k})\|^{2}\] \[+\frac{1}{2}\left(4\hat{D}_{f}+16\hat{L}^{2}+16L+3\right)(18 \alpha^{2}\|\mathrm{grad}f(X_{k+1})\|^{2}+4).\]

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_FAIL:17]

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our empirical results in Section 4 justify ours claims. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss our limitations in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide complete proofs in Appendix B and full set of assumptions in Section 2

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

#### Experimental Result Reproducibility

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [Yes]

Justification: We specify the training details in Section 4 and Appendix C

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

#### Open access to data and code

Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We specify the code and dataset in Section 4. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify training details in Section 4 and hyperparameters in Appendix C. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We specify these in our Section 4. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We use the Hugging face and opendelta as our base code and make some modifications. We use GLUE, E2E, and Suqad three dataset. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our research is compatible with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of our work performed Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We correctly cite the code and datasets we used in Section 4. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ** If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new asset. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our study does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.