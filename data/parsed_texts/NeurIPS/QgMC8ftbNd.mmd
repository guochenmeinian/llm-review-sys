On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games

Awni Altabaa

Department of Statistics & Data Science

Yale University

awni.altabaa@yale.edu

&Zhuoran Yang

Department of Statistics & Data Science

Yale University

zhuoran.yang@yale.edu

###### Abstract

In sequential decision-making problems, the _information structure_ describes the causal dependencies between system variables, encompassing the dynamics of the environment and the agents' actions. Classical models of reinforcement learning (e.g., MDPs, POMDPs) assume a restricted and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure. In this paper, we formalize a novel reinforcement learning model which explicitly represents the information structure. We then use this model to carry out an information-structural analysis of the statistical complexity of general sequential decision-making problems, obtaining a characterization via a graph-theoretic quantity of the DAG representation of the information structure. We prove an upper bound on the sample complexity of learning a general sequential decision-making problem in terms of its information structure by exhibiting an algorithm achieving the upper bound. This recovers known tractability results and gives a novel perspective on reinforcement learning in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems.

## 1 Introduction

The _information structure_ of a sequential decision-making problem is a description of the causal dependencies between system variables. In particular, the information structure specifies the subset of past events that causally influence the present state. This includes information affecting system dynamics and information available to each agent when choosing an action. The control community has long recognized the importance of information structure, leading to the development of the celebrated Witsenhausen intrinsic model [1], and extensive study since the 1970s [e.g., 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].

In general, sequential decision-making problems with arbitrary causal dependencies between system variables can be computationally and statistically intractable in both control and learning settings [3, 15]. In response, early research has identified classes of tractable information structures. For example, Markov Decision Processes (MDP) and Markov teams/games assume an observable Markovian state, which serves as a sufficient statistic for the system's evolution. These structural assumptions enable practical planning and learning algorithms, for example based on dynamic programming [16, 17]. Although such restricted model classes can enable more fruitful analysis, they lack the expressiveness needed to naturally capture the causal structures of complex real-world sequential decision-making problems, where each event in the system may depend arbitrarily on past events.

The concept of information structure is also fundamental to studying the phenomenon of _partial observability_. In general, partial observability refers to situations where a system's evolution depends on a potentially large number of sequential events, but only a subset of these events is observable by the learning agent. However, commonly studied models capture only a restrictive form of this phenomenon. For example, in a Partially-Observable Markov Decision Process (POMDP)--the standard model of partial observability --it is assumed that a Markovian state exists and that each observation is a noisy measurement of the current state. This assumption is often unrealistic, as general systems may not have clearly defined "states", and observations may be generated by more complex dependencies. Information structure provides a more powerful framework for understanding partial observability, capturing a general notion in which system variables evolve with an arbitrary causal structure (not necessarily Markovian), and the set of observables is any subset of all system variables.

The highly-regular information structures of classical models make analysis more tractable, enabling favorable theoretical results [e.g., 18, 19, 20, 21, 22] and driving notable empirical success across a wide range of domains [e.g., 23, 24, 25, 26, 27, 28]. Despite this empirical success, a general theory addressing the role of information structure in the statistical aspects of reinforcement learning is missing. We argue for the perspective that information structure is an important component of analyzing and solving reinforcement learning problems. A rich and flexible representation of information structure is essential to faithfully capture real-world sequential decision-making problems, where the system evolves according to a complex and time-varying dependence on the past, and different agents have different information available to them at different points in time.

_In this work, we formulate a general model of sequential decision-making with an explicit representation of information structure, and study the role of information structure in the statistical complexity of reinforcement learning._ Explicitly modeling information structure allows us to identify a broader class of tractable decision-making problems and ultimately develop more tailored reinforcement learning approaches that leverage this structure.

**Summary of Contributions**

_A model for representing information structure._ We propose _partially-observable sequential teams_ (POST) and _partially-observable sequential games_ (POSG) as general models that contain an explicit representation of information structure as part of the problem specification. This forms a unifying framework that enables an analysis of the role of information structure in RL and captures commonly studied RL models as special cases, including MDPs, Markov teams/games, POMDPs, and Dec-POMDPs/POMGs (Figure 1). The models introduced in this work draw inspiration from Witsenhausen's intrinsic model [1] and its variants from the control literature [29, 30], with added elements to model partial observability in the context of reinforcement learning.

_Theoretical analysis of sequential decision-making through information structure._ We characterize the complexity of the observable dynamics of any sequential decision-making problem through a graph-theoretic analysis of the information structure. Moreover, we propose a generalization of predictive state representations--which may be of independent interest--and construct such a representation for POSTs/POSGs by exploiting information structure. This provides a robust and efficient parameterization amenable to learning.

_Learning theory & information-structural analysis of statistical complexity._ We prove an upper bound on the sample complexity of learning general sequential decision-making problems, expressed as a function of information structure. In particular, the dependence is on an interpretable quantity derived from a graphical representation that can be thought of as an effective "information-structural state", generalizing the typical notion of a Markovian state. We prove this result by exhibiting an algorithm achieving this upper bound, making use of the generalized predictive state representation constructed earlier which provides a robust representation amenable to learning. In doing so, we identify a larger class of statistically tractable reinforcement learning problems.

Related Work.We provide a detailed discussion of related work in Appendix B.

Figure 1: A depiction of the generality of our proposed models. POSTs and POSGs capture MDPs, POMDPs, Dec-POMDPs, and POMGs as special cases.

Background & Preliminaries

What follows is an abridged description of the relevant background and preliminaries. We refer the reader to Appendix C for a more detailed treatment, and to Appendix A for a summary of notations.

### Generic Sequential Decision Making Problems

Consider a controlled stochastic process \((X_{1},\ldots,X_{H})\), where \(X_{h}\) is a random variable corresponding to the variable at time \(h\). At each time \(h\in[H]\), the variable \(X_{h}\) may be either an 'observation' (i.e., observable system variable) or an 'action'. The dynamics of this stochastic process are described by a tuple \((H,\{\mathbb{X}_{h}\}_{h})_{h},\mathcal{O},\mathcal{A},\mathbb{P})\), where \(H\) is the time horizon, \(\mathbb{X}_{h}\) is the variable space at time \(h\), \(\mathcal{O}\subset[H]\) is the index set of observations (i.e., \(X_{h}\) is an observation if \(h\in\mathcal{O}\)), \(\mathcal{A}\subset[H]\) is the index set of actions, and \(\mathbb{P}=\{\mathbb{P}_{h}\}_{h\in\mathcal{O}}\) is a set of probability kernels which describes the probability of any trajectory \(x_{1},\ldots,x_{H}\) given the actions, \(\mathbb{P}\left[\{x_{s}:s\in\mathcal{O}\}\mid\{x_{s}:s\in\mathcal{A}\}\right] =\prod_{h\in\mathcal{O}}\mathbb{P}_{h}\left[x_{h}\mid x_{1},\ldots,x_{h-1}\right]\). A choice of policy \(\pi\) induces a probability distribution \(\mathbb{P}^{\pi}\) on \(\mathbb{X}_{1}\times\cdots\times\mathbb{X}_{H}\). The objective of the agent(s) is to choose a policy which maximizes their expected reward \(V^{R}(\pi)=\mathbb{E}^{\pi}\left[R(X_{1},\ldots,X_{H})\right]\).

Let \(\mathbb{H}_{h}=\prod_{s\in 1:h}\mathbb{X}_{s}\) denote the space of histories at time \(h\) and \(\mathbb{F}_{h}=\prod_{s\in h+1:H}\mathbb{X}_{s}\) denote the space futures at time \(h\). Similarly, let \(\mathbb{H}_{h}^{a}=\mathtt{obs}(\mathbb{H}_{h})=\prod_{s\in\mathcal{O}_{:h}} \mathbb{X}_{s}\) denote the observation component of histories and let \(\mathbb{H}_{h}^{a}=\mathtt{act}(\mathbb{H}_{h})=\prod_{s\in\mathcal{A}_{:h}} \mathbb{X}_{s}\) denote the action component. The observation and action components of the futures, \(\mathbb{F}_{h}^{a}\) and \(\mathbb{F}_{h}^{a}\) respectively, are defined similarly. Here, \(\mathtt{obs}(\cdot)\) and \(\mathtt{act}(\cdot)\) extract the observation and action components, respectively, of any trajectory.

We define the _system dynamics matrix_\(\bm{D}_{h}\in\mathbb{R}^{|\mathbb{H}_{h}|\times|\mathbb{F}_{h}|}\) as the matrix giving the probability of each possible pair of history and future at time \(h\) given the execution of the actions, \([\bm{D}_{h}]_{\tau_{n},\omega_{h}}:=\overline{\mathbb{P}}\left[\tau_{h},\omega _{h}\right]\equiv\mathbb{P}\left[\tau_{h}^{o},\omega_{h}^{o}\mid\mathrm{do}( \tau_{h}^{a},\omega_{h}^{o})\right]\), where \(\tau_{h}\) is the history, \(\omega_{h}\) is the future, and \(\tau_{h}^{o},\tau_{h}^{a},\omega_{h}^{o},\omega_{h}^{o}\) separate the history and future into observation components and action components.

The _rank_ of the dynamics of a sequential decision-making problem is a measure of its complexity. It is defined as the maximal rank of its dynamics matrices.

**Definition 1** (Rank of dynamics).: _The rank of the dynamics \(\{\bm{D}_{h}\}_{h\in[H]}\) is \(r=\max_{h\in[H]}\mathrm{rank}(\bm{D}_{h})\)._

### Generalized Predictive State Representations

Predictive state representations (PSR) [31; 32] are a representation of dynamical systems and sequential decision-making problems based on predicting future observations given the past, without explicitly modeling a latent state. In this section, we propose and formalize a generalization of standard PSRs which allows for an arbitrary order of observations and actions. This generalization is necessary to capture POSTs/POSGs (introduced in Section 3), but may also be of independent interest.

The "PSR rank" of a sequential decision-making problem coincides with the rank of its dynamics (Definition 1). Denote \(r_{h}:=\mathrm{rank}(\bm{D}_{h})\). At the heart of predictive state representations is the concept of "core test sets." A core test set at time \(h\) is a set of futures such that the vector of probabilities of those futures conditioned on the past encodes all the information that the past contains about the future.

**Definition 2** (Core test sets).: _A core test set at time \(h\) is a subset of \(d_{h}\geq r_{h}\) futures, \(\mathbb{Q}_{h}:=\{q_{h}^{1},\ldots,q_{h}^{d_{h}}\}\subset\mathbb{F}_{h}\), such that the submatrix \(\bm{D}_{h}[\mathbb{Q}_{h}]\in\mathbb{R}^{|\mathbb{H}_{h}|\times d_{h}}\) is full-rank._

The \(d_{h}\)-dimensional vector \(\psi_{h}(\tau_{h}):=(\overline{\mathbb{P}}[\tau_{h},q_{h}^{1}],\ldots, \overline{\mathbb{P}}[\tau_{h},q_{h}^{d_{h}}])\) serves as a sufficient statistic for computing the probability of any future conditioned on \(\tau_{h}\). Intuitively, core test sets relate the rank of the dynamics to a representation based on predicting future outcomes. A PSR parameterization of the system dynamics represents the probability of any trajectory using the sufficient statistics \(\psi_{h}\) via a set of operators that iteratively update the predictive representations at each step as new observations come in. We proceed to formally define our generalized predictive state representation.

**Definition 3** (Generalized Predictive State Representations).: _Consider a sequential decision-making problem \((X_{h})_{h\in[H]}\) where \(\mathcal{A},\mathcal{O}\) partition \([H]\) into actions and observations, respectively. Then, a generalized predictive state representation for this sequential decision-making problem is a tuple \((\{\mathbb{Q}_{h}\}_{0\leq h\leq H-1},\phi_{H},\{M_{h}\}_{1\leq h\leq H-1},\psi _{0})\) satisfying_

\[\overline{\mathbb{P}}\left[x_{1},\ldots,x_{H}\right] =\phi_{H}(x_{H})^{\top}M_{H-1}(x_{H-1})\cdots M_{1}(x_{1})\psi_{0}\] (1) \[\psi_{h}(x_{1},\ldots,x_{h}) =M_{h}(x_{h})\cdots M_{1}(x_{1})\psi_{0},\,\forall h,\] (2)_where \(M_{h}:\mathbb{X}_{h}\to\mathbb{R}^{d_{h}\times d_{h-1}}\), \(\phi_{H}:\mathbb{X}_{H}\to\mathbb{R}^{d_{H-1}}\), and \(\psi_{0}=\psi(\emptyset)\)._

An important condition for the learnability of PSR models, which was used in prior work [including 33, 34, 35], is the so-called "well-conditioning assumption". We state the analogous assumption for our generalized PSR model below. For a core test set \(\mathbb{Q}_{h}\), let \(\mathbb{Q}_{h}^{A}=\mathtt{act}(\mathbb{Q}_{h})\) be the set of core action sequences, let \(Q_{A}=\max_{h}|\mathbb{Q}_{h}^{A}|\) be the number of core action sequences, and let \(d=\max_{h}d_{h}\).

**Assumption 1** (\(\gamma\)-well-conditioned generalized PSR).: _A generalized PSR model is said to be \(\gamma\)-well conditioned for \(\gamma>0\) if, for any \(h\in[H]\) and any \(\pi\), it satisfies_

\[\max_{\begin{subarray}{c}z\in\mathbb{R}^{d_{h}}\\ \|z\|_{1}\leq 1\end{subarray}}\sum_{\omega_{h}\in\mathbb{F}_{h}}\pi(\omega_{h}) \left|m_{h}(\omega_{h})^{\top}z\right|\leq\frac{1}{\gamma},\quad\max_{ \begin{subarray}{c}z\in\mathbb{R}^{d_{h}}\\ \|z\|_{1}\leq 1\end{subarray}}\sum_{x_{h}\in\mathbb{X}_{h}}\left\|M_{h}(x_{h})z \right\|_{1}\pi(x_{h})\leq\frac{\left|\mathbb{Q}_{h+1}^{A}\right|}{\gamma},\]

_where \(m_{h}(\omega_{h})^{\top}=\phi_{H}(x_{H})^{\top}M_{H-1}(x_{H-1})\cdots M_{h+1}(x _{h+1})\) and \(\forall\,\omega_{h}^{a}\), \(\sum_{\omega_{h}^{a}}\pi(\omega_{h}^{o},\omega_{h}^{a})=1\)._

To understand this condition, recall that \(m_{h}(\omega_{h})^{\top}\psi_{h}(\tau_{h})=\overline{\mathbb{F}}\left[\tau_{h},\omega_{h}\right]\). Thus, we can interpret \(z\) as the error in estimating the prediction features \(\psi_{h}\). This condition ensures that the error in estimating the probability of system trajectories remains controlled when the estimation error of \(\psi_{h}\) is small.

Although we focus on finite spaces \(\mathbb{X}_{h}\) in this work, we briefly discuss possible extensions to continuous spaces. In finite settings, the core tests \(q\in\mathbb{Q}_{h}\) are represented by future trajectories \(q=(x_{h+1},...,x_{H})\in\mathbb{F}_{h}\). In continuous spaces, the core tests become trajectories over subsets of the underlying continuous space: \(q=(\mathcal{X}_{h+1},\ldots,\mathcal{X}_{H})\subset\mathbb{F}_{h}\), where each \(\mathcal{X}_{k}\subset\mathbb{X}_{k}\) is a measureable subset. We point to [33] for more discussion on how to extend (standard) PSRs to continuous spaces.

## 3 Information Structure

The "information structure" of a sequential decision-making problem defines the causal dependencies between events in the system occurring at different points time, whether those events are observable by the learning agent or not. In this section, we will introduce a novel reinforcement learning model that explicitly represents information structure. We demonstrate that this enables a rich analysis of the system's dynamics and is crucial for characterizing the statistical complexity of general RL problems.

### Partially-Observable Sequential Teams

We propose a model of sequential decision-making problems that includes an explicit representation of information structure, which we call partially-observable sequential teams (POST). A POST is a controlled stochastic process consisting of a sequence of variables, where each variable is either a "system variable" or an "action variable". POSTs also model the _observability_ of each system variable with respect to the learning algorithm (i.e., which system variables are available to the learning algorithm). The information structure of a POST describes the causal dependence between these variables. The _information set_ of each system variable describes the subset of past variables that are coupled to it in the dynamics. The information set of an action variable describes the information available to the agent when choosing an action, hence defining the policy class they optimize over.

**Definition 4** (Post).: _A partially-observable sequential team is a controlled stochastic process that specifies the joint distribution of \(T\) variables \((X_{t})_{t\in[T]}\), together with a specification of the observability of each variable. Here each \(X_{t}\) is either a system variable or an action variable, and is either observable by the learning agent or not. A POST is specified by the following components._

1. _Variable Structures._ _The variables_ \(\{X_{t}\}_{t\in[T]}\) _are partitioned into two disjoint subsets -- system variables and action variables._ \(\mathcal{S}\subset[T]\) _indexes system variables and_ \(\mathcal{A}\subset[T]\) _indexes action variables, with_ \(\mathcal{S}\cap\mathcal{A}=\emptyset\)_,_ \(\mathcal{S}\cup\mathcal{A}=[T]\)_._
2. _Information Structure._ _For_ \(t\in[T]\)_, the "information set"_ \(\mathcal{I}_{t}\subset[t-1]\) _of the variable_ \(X_{t}\) _is the set of past variables that are coupled to_ \(X_{t}\) _in the dynamics. That is, the transition to_ \(X_{t}\) _depends on the value of_ \(I_{t}:=(X_{s}\,:\,s\in\mathcal{I}_{t})\)_. We call_ \(I_{t}\) _the "information variable" at time_ \(t\)_, and call_ \(\mathbb{I}_{t}=\prod_{s\in\mathcal{I}_{t}}\mathbb{X}_{s}\) _the "information space"._
3. _System Kernels._ _For any_ \(t\in\mathcal{S}\)_,_ \(\mathcal{T}_{t}\in\mathcal{P}(\mathbb{X}_{t}\|\mathbb{I}_{t})\) _is kernel from_ \(\mathbb{I}_{t}\) _to_ \(\mathbb{X}_{t}\) _that specifies the conditional distribution of a system variable_ \(X_{t}\) _given the information variable_ \(I_{t}\)_._
4. _Decision Kernels._ _Each agent chooses a decision kernel_ \(\pi_{t}:\mathbb{I}_{t}\to\mathcal{P}(\mathbb{X}_{t})\)_, specifying a (potentially randomized) policy for choosing an action at time_ \(t\in\mathcal{A}\)5. _Observability._ _We denote the observable system variables by_ \(\mathcal{O}\subset\mathcal{S}\)_. We require that the information sets of the action variables are observable to the learning algorithm:_ \(\mathcal{O}\supset\cup_{t\in\mathcal{A}}(\mathcal{I}_{t}\cap\mathcal{S})\)_. We define_ \(\mathcal{U}\coloneqq\mathcal{O}\cup\mathcal{A}\)_, and let_ \(H\coloneqq|\mathcal{U}|\) _be the time-horizon of the observable variables._
6. _Reward Function._ _At the end of an episode, the team receives a reward determined by the function_ \(R:\prod_{s\in\mathcal{U}}\mathbb{X}_{s}\to[0,1]\)_._

With the above components, any set of decision kernels (i.e., joint policy) \(\bm{\pi}=(\pi_{t}:t\in\mathcal{A})\) induces a unique probability measure over \(\mathbb{X}_{1}\times\cdots\times\mathbb{X}_{T}\), which is given by

\[\mathbb{P}^{\bm{\pi}}\left[x_{1},\ldots,x_{T}\right]=\prod_{t\in\mathcal{S}} \mathcal{T}_{t}(x_{t}|\left\{x_{s}:s\in\mathcal{I}_{t}\right\})\prod_{t\in \mathcal{A}}\pi_{t}(x_{t}|\left\{x_{s}:s\in\mathcal{I}_{t}\right\}).\] (3)

We will be interested in modeling the _observable_ dynamics of the POST. We index the observable variables by their position among the observables \(h\in[H]\) rather than their position among all variables, as follows: \((X_{t})_{t\in\mathcal{U}}=(X_{t(1)},\ldots,X_{t(H)})\), where \(t:[H]\to\mathcal{U}\subset[T]\) maps the index over observables to the index over all variables. The distribution of the observables is obtained by marginalizing over the unobservable variables, \(\mathbb{P}^{\bm{\pi}}\left[x_{t(1)},\ldots,x_{t(H)}\right]=\sum_{s\in\mathcal{ O}^{\complement}}\sum_{x_{s}\in\mathbb{X}_{s}}\mathbb{P}^{\bm{\pi}}\left[X_{1}=x_{1}, \ldots X_{T}=x_{T}\right]\).

The value of a policy is given by its expected reward \(V(\bm{\pi})\coloneqq\mathbb{E}^{\bm{\pi}}\left[R(X_{t(1)},\ldots,X_{t(H)}) \right]\), where \(\mathbb{E}^{\pi}\) is the expectation associated with the probability measure \(\mathbb{P}^{\bm{\pi}}\). The objective of a POST is to learn a policy \(\bm{\pi}=(\pi_{t})_{t\in\mathcal{A}}\) which maximizes the expected reward, \(\sup_{\bm{\pi}}V(\bm{\pi})\). When the variable spaces \(\mathbb{X}_{t}\) are finite, this supremum is attained by a deterministic policy, \(\pi_{t}\colon\mathbb{I}_{t}\to\mathbb{X}_{t},t\in\mathcal{A}\).

**Representation of the information structure as a directed acyclic graph.** The information structure of a POST can be naturally represented as a labeled directed acyclic graph (DAG). Given the variable structure and information structure of a POST, \(\left(\mathcal{S},\mathcal{A},\left\{\mathcal{I}_{t}\right\}_{t}\right)\), its DAG representation is given by \(\mathcal{G}(\mathcal{V},\mathcal{E},\mathcal{L})\). The nodes of the graph are the set of variables, \(\mathcal{V}=[T]=\mathcal{S}\cup\mathcal{A}\). The edges \(\mathcal{E}\subset\mathcal{V}\times\mathcal{V}\) of the DAG are given by \(\mathcal{E}=\{(i,t):t\in[\mathcal{I}],i\in\mathcal{I}_{t}\}\). That is, there exists an edge from \(i\) to \(t\) if \(i\) is in the information set of \(t\). Finally, \(\mathcal{L}\) contains labels for each node as being a system variable (in \(\mathcal{S}\)) or an action variable (in \(\mathcal{A}\)). Further, the observability of system variables (in \(\mathcal{O}\)) is also labeled.

This DAG represents a directed _graphical model_ for the POST. In particular, the probability distribution on \(\mathbb{X}_{1}\times\cdots\times\mathbb{X}_{T}\) factors according to \(\mathcal{G}\),

\[\mathbb{P}\left[X_{1},\ldots,X_{T}\right]=\prod_{t\in\mathcal{V}}\mathbb{P} \left[X_{t}\mid\mathrm{pa}(X_{t})\right],\] (4)

where \(\mathrm{pa}(X_{t})\) is the set of parents of \(X_{t}\) in \(\mathcal{G}\) (which are \(\mathcal{I}_{t}\)). This representation of the information structure as a DAG will be crucial for our analysis of the dynamics of sequential teams in Section 3.2.

**Partially observable sequential games.** We define partially-observable sequential games (POSGs) in a similar manner. The main difference is that, in the game setting, there exists an expanded reward structure with \(N\) different reward objectives \(R^{1},...,R^{N}\), with different agents pursuing different objectives. The action index set is partitioned into \(N\) subsets \(\mathcal{A}=\mathcal{A}^{1}\cup\cdots\mathcal{A}^{N}\), where \(\mathcal{A}^{i}\subset\mathcal{A}\) denotes the action index set associated with the agent(s) optimizing for objective \(R^{i}\). The extension to the game setting is treated in detail in Appendix F.

### Information Structure Determines the Rank of POSTs/POSGs

POSTs and POSGs form a highly general framework that captures any sequential decision-making problem that can be described by a controlled stochastic process, subsuming classical models such as MDPs, POMDPs, Dec-POMDPs, and POMGs. By introducing a model with an explicit representation of information structure, we gain the ability to perform a richer analysis of sequential decision-making problems. In particular, we will show that the rank of the dynamics, and ultimately the statistical complexity of reinforcement learning, can be characterized as a function of the information structure.

We begin by defining the central quantity in our analysis, which we call the "information-structural state", hinting at the role it will play. The information structural state is defined for each point in time as a subset of the past (whether observed or latent) which forms a sufficient statistic for the future.

**Definition 5** (Information-structural state).: _Let \(\mathcal{G}^{\dagger}\) be the DAG obtained from \(\mathcal{G}\) by removing all edges directed towards actions. That is, it consists of the edges \(\mathcal{E}^{\dagger}\coloneqq\mathcal{E}\setminus\{(x,a):x\in\mathcal{N},a \in\mathcal{A}\}\). For each \(h\in[H]\), let \(\mathcal{I}_{h}^{\dagger}\subset[t(h)]\) be the minimal set of past variables (observed or unobserved) which \(d\)-separates the past observations \((X_{t(1)},\ldots,X_{t(h)})\) from the future observations \((X_{t(h+1)},\ldots,X_{t(H)})\) in the DAG \(\mathcal{G}^{\dagger}\). Define \(\mathbb{I}_{h}^{\dagger}:=\prod_{s\in\mathcal{I}_{h}^{\dagger}}\mathbb{X}_{s}\) as the joint space of those variables._The following theorem, whose proof is given in Appendix H, states that the rank of the observable system dynamics of POSTs and POSGs is bounded by the cardinality of \(\mathbb{I}_{h}^{\dagger}\).

**Theorem 1**.: _The rank of the observable system dynamics of a POST or POSG is bounded in terms of its information structure by \(r\leq\max_{h\in|H|}\mathbb{I}_{h}^{\dagger}|\)._

This result shows that the complexity of the observable system dynamics is characterized by the information structure through \(\mathbb{I}_{h}^{\dagger}\). In particular, \(i_{h}^{\dagger}\in\mathbb{I}_{h}^{\dagger}\) describes a set of system variables, either observable or latent, which provide a sufficient statistic of the past at time \(h\) for predicting future observations--\(I_{h}^{\dagger}\) "separates" the past from the future. Hence, the quantity \(\|\mathbb{I}_{h}^{\dagger}\|\) admits an interpretation as the size an effective state space at time \(h\). This is a generalization of the standard notion of a latent state. For example, in the case of POMDPs or Dec-POMDPs, the information-structural state indeed coincides with the latent Markovian state (Figure 1(a)). We emphasize that \(\mathbb{I}_{h}^{\dagger}\) may contain observable variables as well as _unobservable_ system variables. In fact, unobservable system variables can introduce crucial structure that simplifies the observable system dynamics.

### Examples of Information Structures and their Rank

In this section, to provide some intuition, we present examples of information structures and apply Theorem 1 to obtain a bound the rank of their observable system dynamics. We see that classical models such as MDPs, POMDPs, and POMGs are special cases of the POST/POSG framework, and known results about their rank [31] are recovered by the generalized graph-theoretic analysis of their information structure. For notational convenience, we adopt a modified notation in this section where information sets \(\mathcal{I}\) are indexed by the symbol of the variable rather than its time-index. For example, in an MDP, we write \(\mathcal{I}(s_{t})=\left\{s_{t-1},a_{t-1}\right\}\) for the information set of the state variable \(s_{t}\).

**Decentralized POMDPs and POMGs.** At each time \(t\), the system variables of a decentralized POMDP (or POMG) consists of a latent state \(s_{t}\), observations for each agent \(o_{t}^{1},\ldots,o_{t}^{N}\), and actions of each agent \(a_{t}^{1},\ldots,a_{t}^{N}\). The latent state transitions are Markovian and depend on the agents' joint action. The observations are sampled via a kernel conditioned on the latent state. Each agent can use their own history of observations to choose an action. Thus, the information structure is given by

\[\mathcal{I}(s_{t})=\left\{s_{t-1},a_{t-1}^{1},\ldots,a_{t-1}^{N}\right\},\, \mathcal{I}(o_{t}^{i})=\left\{s_{t}\right\},\,\mathcal{I}(a_{t}^{i})=\left\{o _{1:t-1}^{i},a_{1:t-1}^{i}\right\}.\]

Here, the observable variables are \(\mathcal{U}=\left\{o_{1:T}^{i},a_{1:T}^{i},i\in[N]\right\}\). By Theorem 1, we have \(\mathcal{I}^{\dagger}(o_{t}^{i})=\mathcal{I}^{\dagger}(a_{t}^{i})=\left\{s_{t }\right\},\,\forall t,i\), as shown in Figure 1(a). Thus, the rank of a Dec-POMDP is bounded by \(|\mathbb{S}|\), where \(\mathbb{S}\) is the state space. Note that in the case of models with a true latent state (e.g., POMDPs, Dec-POMDPs, and POMGs), the information-structural state coincides with the Markovian latent state.

**Point-to-Point Real-Time Communication with Feedback.** Consider the following model of real-time communication with feedback. Let \(x_{t}\) be a Markov source. At time \(t\), the encoder receives the source \(x_{t}\in\mathbb{X}\) and sends an encoded symbol \(z_{t}\in\mathbb{Z}\). The symbol is sent through a memoryless noisy channel which outputs \(y_{t}\) to the receiver. The decoder produces the estimate \(\widehat{x}_{t}\). The output of the noisy channel is also fed back to the encoder. The encoder and decoder have full memory of their observations and previous "actions". The observation variables are \(\mathcal{O}=\left\{x_{1:T},\,y_{1:T}\right\}\) and the "actions" are \(\mathcal{A}=\left\{z_{1:T},\,\widehat{x}_{1:T}\right\}\). Hence, the information structure is given by the following,

\[\mathcal{I}(x_{t})=\left\{x_{t-1}\right\},\,\mathcal{I}(z_{t})=\left\{x_{1:t}, y_{1:t-1},z_{1:t-1}\right\},\,\mathcal{I}(y_{t})=\left\{z_{t}\right\},\, \mathcal{I}(\widehat{x}_{t})=\left\{y_{1:t}\right\}.\]

By Theorem 1, we have that,

\[\mathcal{I}^{\dagger}(x_{t})=\left\{x_{t}\right\},\,\mathcal{I}^{\dagger}(z_{t })=\left\{x_{t}\right\},\,\mathcal{I}^{\dagger}(y_{t})=\left\{x_{t},z_{t} \right\},\,\mathcal{I}^{\dagger}(\widehat{x}_{t})=\left\{x_{t}\right\}.\]

Hence, the rank is bounded by \(|\mathbb{X}||\mathbb{Z}|\). This is depicted in Figure 1(c).

**Limited-memory information structures.** Consider a sequential decision making problem with variables \(o_{t},a_{t},t\in[T]\) and an information structure with length-\(m\) "memory". That is, observations do not directly depend on variables more than \(m\) steps in the past. That is, the information structure is

\[\mathcal{I}(o_{t})=\left\{o_{t-m:t-1},a_{t-m:t-1}\right\},\mathcal{I}(a_{t})= \left\{o_{1:t},a_{1:t-1}\right\}.\]

The observables consist of all observations and actions, \(\mathcal{U}=\left\{o_{1:T},a_{1:T}\right\}\). By Theorem 1 we have that \(\mathcal{I}^{\dagger}(o_{t})=\left\{o_{t-m:t-1},a_{t-m:t-1}\right\}\), as shown in Figure 1(d). Hence, the rank of this sequential decision-making problem is bounded by \(|\mathbb{O}|^{m}|\mathbb{A}|^{m}\).

The examples above show that the complexity of the dynamics of a sequential decision-making problem depends directly on its information structure. An expanded version of this discussion is provided in Appendix D. Next, we use an information-structural analysis to construct a generalized PSR representation for a class of POSTs/POSGs, which we will ultimately use to prove an upper bound on the _statistical complexity_ of reinforcement learning as a function of information structure.

## 4 Constructing a PSR Parameterization for POSTs and POSGs

A key challenge in reinforcement learning is constructing representations which enable robustly and efficiently modeling probabilities of system trajectories (i.e., probabilities of the form \(\mathbb{P}\left[\texttt{future}\mid\texttt{history}\right]\)). In this section, we will construct a generalized predictive state representation for a class of POSTs/POSGs, ultimately enabling sample-efficient reinforcement learning.

### Core test sets for POSTs/POSGs

Recall that a core test set is a set of futures such that the probabilities of those futures given the past encode all the information that the past contains about the future. For systems with a simple and

Figure 2: DAG representation of various information structures. Solid edges indicate the edges in \(\mathcal{E}^{\dagger}\) and light edges indicate the information sets of action variables. Grey nodes represent unobservable variables, blue nodes represent past observable variables, green nodes represent future observable variables, and red nodes represent \(\mathcal{I}_{h}^{\dagger}\). To find \(\mathcal{I}_{h}^{\dagger}\), as per Theorem 1, we first remove the incoming edges into the action variables, then we find the minimal set among all past variables (both observable and unobservable) which \(d\)-separates the past observations from the future observations.

regular information structure, a core test set may be simple to obtain. For example, undercomplete POMDPs with a full-rank emission matrix admit the 1-step observation futures as core test sets.

For POSTs/POSGs with arbitrary information structures, obtaining a core test set is much more challenging without knowing the system dynamics. In this section, we identify a condition in terms of the information structure under which \(m\)-step futures are a core test set for POSTs/POSGs. Let us denote the candidate core test set of \(m\)-step futures at time \(h\) by \(\mathbb{Q}_{h}^{m}:=\prod_{s\in\mathcal{U}_{h+1:\min(h+m,\,H)}}\mathbb{X}_{s}\). We define the matrix \(\bm{G}_{h}\) as encoding the probability of observing each \(m\)-step future conditioned on the information-structural state \(i_{h}^{\dagger}\in\mathbb{I}_{h}^{\dagger}\):

\[\bm{G}_{h}\coloneqq\left[\overline{\mathbb{P}}\big{[}q\ |\ i_{h}^{\dagger} \big{]}\right]_{q,i_{h}^{\dagger}}\in\mathbb{R}^{|\mathbb{Q}_{h}^{m}|\times| \mathbb{I}_{h}^{\dagger}|},\ q\in\mathbb{Q}_{h}^{m},\,i_{h}^{\dagger}\in \mathbb{I}_{h}^{\dagger}.\] (5)

The operational meaning of \(\bm{G}_{h}\) is depicted in Figure 3. Next, we formulate a condition in terms of information structure that we will show implies that the \(m\)-step futures are core test sets.

**Definition 6** (\(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing).: _We say that a sequential team is \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing if for all \(h\in[H]\), \(\operatorname{rank}(\bm{G}_{h})=|\mathbb{I}_{h}^{\dagger}|\). Furthermore, we say that the sequential team is \(\alpha\)-robustly \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing if for all \(h\in[H-m+1]\), \(\sigma_{|\mathbb{I}_{h}^{\dagger}|}(\bm{G}_{h})\geq\alpha\)._

The \(\mathcal{I}^{\dagger}\)-weakly revealing condition is essentially a statistical identifiability condition. If a POST/POSG is \(\mathcal{I}^{\dagger}\)-weakly revealing, then, for any two mixtures of the information-structural state, the distributions of the \(m\)-step futures are distinct. Formally, for any \(\nu_{1},\nu_{2}\in\mathcal{P}(\mathbb{I}_{h}^{\dagger})\) with \(\operatorname{supp}(\nu_{1})\cap\operatorname{supp}(\nu_{2})=\emptyset\), we have \(\bm{G}_{h}\nu_{1}\neq\bm{G}_{h}\nu_{2}\). That is, the future observations contain information that can distinguish between mixtures of the latent information-structural state. The \(\alpha\)-robust version of the \(\mathcal{I}^{\dagger}\)-weakly revealing condition requires that \(\bm{G}_{h}\) is not only full rank, but that its \(|\mathbb{I}_{h}^{\dagger}|\)-th singular value is bounded away from zero, so that \(\|G_{h}\nu_{1}-G_{h}\nu_{2}\|\geq\alpha\|\nu_{1}-\nu_{2}\|\).

The condition holds whenever there exists a sequence of actions within the \(m\)-step futures such that executing these actions results in a sequence of observations that is informative about the information-structural state \(i_{h}^{\dagger}\in\mathbb{I}_{h}^{\dagger}\). In general, this condition will be harder to satisfy when \(\mathbb{I}_{h}^{\dagger}\) is large since it would require the \(m\)-step future observations to encode more information. In particular, \(\bm{G}_{h}\) cannot be full rank when \(|\mathbb{Q}_{h}^{m}|<|\mathbb{I}_{h}^{\dagger}|\). As a heuristic, when we don't have prior knowledge about the dynamics (e.g., in the learning setting), we can choose \(m\) such that \(|\mathbb{Q}_{h}^{m}|\geq|\mathbb{I}_{h}^{\dagger}|\). In general, it will be possible to find a smaller core test set when the \(d\)-separating set \(\mathcal{I}_{h}^{\dagger}\) is small. This happens when the system dynamics contain state-like variables that are low-dimensional.

The \(\mathcal{I}^{\dagger}\)-weakly-revealing condition is a generalization of the "weakly-revealing" condition for POMDPs introduced by Liu et al. [36]. Liu et al. [37] proposed an algorithm for learning weakly-revealing POMGs. Our analysis here recovers weakly-revealing POMGs as a special case and enables learning a much more general class of problems. We note that such an identifiability condition is necessary, and is reflective of the fundamental difficulty of the partially-observable setting. For example, in the case of POMDPs, there exist hardness results that state that if an analogous condition does not hold, the statistical complexity can scale exponentially with the relevant quantities [36].

We now proceed to show that under the \(\mathcal{I}^{\dagger}\)-weakly-revealing condition, \(m\)-step futures are core test sets for POSTs/POSGs which can be used to construct a generalized PSR representation amenable

Figure 3: A depiction of the construction of a generalized generalized predictive state representation for POST/POSG models.

to learning. Recall that the vector of core test set probabilities for the history \(\tau_{h}\) is given by the mappings \(\psi_{h},\overline{\psi}_{h}:\mathbb{H}_{h}\to\mathbb{R}^{|\mathbb{Q}_{h}^{m}|}\),

\[\psi_{h}(\tau_{h})=\left[\mathbb{P}\left[q^{o},\tau_{h}^{o}\mid\mathrm{do}(\tau_ {h}^{a}),\,\mathrm{do}(q^{a})\right]\right]_{q\in\mathbb{Q}_{h}^{m}},\quad \overline{\psi}_{h}(\tau_{h})=\left[\mathbb{P}\left[q^{o}\mid\tau_{h}^{o};\, \mathrm{do}(\tau_{h}^{a}),\,\mathrm{do}(q^{a})\right]\right]_{q\in\mathbb{Q}_ {h}^{m}}.\]

Define the mapping \(m_{h}:\mathbb{F}_{h}\to\mathbb{R}^{|\mathbb{Q}_{h}^{m}|}\) as,

\[m_{h}(\omega_{h})\coloneqq(\bm{G}_{h}^{\dagger})^{\top}\left[\overline{ \mathbb{P}}[\omega_{h}\,|\,i_{h}^{\dagger}]\right]_{i_{h}^{\dagger}\in\mathbb{ I}_{h}^{\dagger}}.\] (6)

The following lemma, whose proof is given in Appendix I, shows that the \(m\)-step futures \(\mathbb{Q}_{h}^{m}\) are core test sets for any \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing POST/POSG. In particular, given any future \(\omega_{h}\in\mathbb{F}_{h}\) and history \(\tau_{h}\in\mathbb{H}_{h}\), the conditional probability \(\overline{\mathbb{P}}\left[\omega_{h}\mid\tau_{h}\right]\) can be written as a linear combination of the core test probabilities \(\overline{\psi}_{h}(\tau_{h})\), with weights given by \(m_{h}(\omega_{h})\), depending only on the future \(\omega_{h}\).

**Lemma 1** (Core test set for POSTs).: _Suppose that the POST is \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing. Then, \(\mathbb{Q}_{h}^{m}\) is a core test set for all \(h\in[H]\). Furthermore, we have \(\overline{\mathbb{P}}\left[\tau_{h},\omega_{h}\right]=\left\langle m_{h}( \omega_{h}),\psi_{h}(\tau_{h})\right\rangle\) and \(\overline{\mathbb{P}}\left[\omega_{h}\mid\tau_{h}\right]=\left\langle m_{h}( \omega_{h}),\overline{\psi}_{h}(\tau_{h})\right\rangle\)._

### Generalized PSR parameterization of POST/POSG

Consider a POST/POSG which is \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing. Lemma 1 shows that the \(m\)-step futures \(\mathbb{Q}_{h}^{m}\) are core test sets. In this section, we will explicitly construct a generalized PSR parameterization for this class of sequential decision-making problems. Moreover, we will show that this generalized PSR representation is well-conditioned when the weakly revealing condition is robust.

Let \(d_{h}\coloneqq|\mathbb{Q}_{h}^{m}|\). The key observation that allows us to construct the generalized PSR representation is that the vector mappings \(m_{h}:\mathbb{F}_{h}\to\mathbb{R}^{d_{h}}\) and \(\psi_{h}:\mathbb{H}_{h}\to\mathbb{R}^{d_{h}}\) can be used to derive a recursive form of the dynamics of the POST/POSG. We define the operator mapping \(M_{h}:\mathbb{X}_{t(h)}\to\mathbb{R}^{d_{h}\times d_{h-1}}\) by

\[\left[M_{h}(x_{t(h)})\right]_{q_{:}}=m_{h-1}(x_{t(h)},q)^{\top},\,q\in\mathbb{ Q}_{h}.\] (7)

That is, \(M_{h}(x_{t(h)})\) is the matrix whose rows are indexed by the core tests at the \(h\)-th observable step, where the row corresponding to each \(q\in\mathbb{Q}_{h}\) is the weights returned by the mapping \(m_{h-1}\) when applied to the future consisting of \(x_{t(h)}\) followed by core test \(q\). The operator map \(M_{h}\) allows us to update the probabilities of the core test sets after receiving an additional observation \(x_{t(h)}\): \(\psi_{h}(x_{t(1)},...,x_{t(h)})=M_{h}(x_{t(h)})\psi_{h-1}(x_{t(1)},...,x_{t(h-1)})\). The following result, whose proof is given in Appendix I, states that the set of operators \(\{M_{h}\}_{h}\) forms a generalized PSR.

**Theorem 2**.: _Consider an \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing POST/POSG. Let \(\{M_{h}\}_{h\in[H-1]}\) be defined as above, and let \(\psi_{0}=\left[\overline{\mathbb{P}}\left[q\right]\right]_{q\in\mathbb{Q}_{0}^{ m}},\,\phi_{H}(x_{t(H)})=\bm{e}_{x_{t(H)}}\). Then, \((\{\mathbb{Q}_{h}^{m}\}_{h},\phi_{H},\{M_{h}\}_{h\in[H-1]},\psi_{0})\) forms a generalized predictive state representation. Moreover, if the weakly-revealing property is \(\alpha\)-robust, then this PSR is \(\gamma\)-well-conditioned with \(\gamma=\alpha/\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert^{1/2}\)._

Thus, we have constructed a robust parameterization of the sequential decision-making problem, making use of its information structure, that will enable us to design an efficient learning algorithm.

Characterizing the Statistical Complexity of General Reinforcement Problems via Information Structure

In this section, we establish an upper bound on the achievable sample complexity of general reinforcement learning problems in terms of their information structure. In plain language, this is a result that roughly says "any sequential decision-making problem with an information structure \(\mathcal{I}\) can be learned with a sample complexity at most \(f(\mathcal{I})\)". This identifies a class of sequential decision-making problems that are statistically tractable via conditions on the information structure, expanding the set of known-tractable problems while recovering existing tractability results as a special case.

We will prove this result by exhibiting an algorithm that achieves this upper bound. Our approach will be to use the generalized predictive state representation constructed for POSTs/POSGs in Section 4, which provides a robust representation amenable to learning. We will introduce an algorithm for learning generalized PSRs and prove a corresponding sample complexity result, which will in turn imply a bound on the sample complexity of learning POST/POSG models via the information-structural characterization of the rank of observable dynamics established in Section 3.2.

There exist several works in the literature which study learning in PSRs (e.g. [33; 34; 35; 38; 39]). Using the technical tools developed in this paper, most of these algorithms can be directly extended to our generalization of PSRs. With such an algorithm, Theorems 1 and 2 then imply a bound on the achievable sample complexity for learning general sequential decision-making problems in terms of their information structure. In this work, we will adapt the model-based UCB-type algorithm of Huang et al. [35], extending it to generalized PSRs to obtain a bound on the achievable sample complexity for POSTs/POSGs. We will defer the details of the algorithm to Appendix E and formally verify the proof in Appendix J. Here, we will focus on discussing the role of information structure in determining the statistical complexity of reinforcement learning.

The following result states that the size of the information-structural state, \(|\mathbb{I}_{h}^{\dagger}|\), characterizes an upper bound on the statistical complexity of learning a sequential decision-making problem.

**Theorem 3**.: _Suppose a sequential decision-making problem described by a POST is \(\alpha\)-robustly \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing. Let \(Q_{m}\coloneqq\max_{h}\mathbb{Q}_{h}^{m}|\) be the size of the \(m\)-step observable trajectories, and let \(A=\max_{s\in\mathcal{A}}\mathcal{\mathbb{X}}_{s}|\) be the size of largest action space. Then, there exists an algorithm that can learn an \(\epsilon\)-optimal policy with a sample complexity (omitting log factors) bounded by_

\[\frac{1}{\epsilon^{2}}\times\mathrm{poly}\left(\frac{1}{\alpha},\max_{h} \left|\mathbb{I}_{h}^{\dagger}\right|,Q_{m},A,H\right).\]

_Under the game setting, the same assumption implies the existence of a self-play algorithm that learns an \(\epsilon\) (Nash or coarse-correlated) equilibrium with the same sample complexity._

This result identifies \(\mathcal{I}^{\dagger}\)-weakly revealing POSTs/POSGs as a class of statistically tractable sequential decision-making problems. The sample complexity is polynomial in the size of the information-structural state space \(\max_{h}\lvert\mathbb{I}_{h}^{\dagger}\rvert\), the size of the action space \(A\), and the time horizon \(H\). Further, it depends on \(Q_{m}\), the size of \(m\)-step observable trajectories, and the robustness parameter \(\alpha^{-1}\), which corresponds to the \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing identifiability condition. We note that the algorithm constructed to prove Theorem 3 only needs to know the parameters of the \(\mathcal{I}^{\dagger}\)-weakly revealing condition (i.e., \(m\) and \(\alpha\)), and does not need to know the full information structure.

This result shows that the size of the information-structural state is a fundamental measure of the statistical complexity of reinforcement learning. As a result, learning is tractable when \(\max_{h}\lvert\mathbb{I}_{h}^{\dagger}\rvert\) is of modest size, and the information structural state is strongly coupled to the observable system variables.

One notable special case of POSTs/POSGs is POMDPs. Learning in POMDPs has been studied extensively in the literature. Theorem 3 implies a \(\mathrm{poly}(S,O,A,H,\alpha^{-1})\cdot\epsilon^{-2}\) bound on the sample complexity of learning in \(\alpha\)-weakly POMDPs and \(\mathrm{poly}(S,(OA)^{m},H,\alpha^{-1})\cdot\epsilon^{-2}\) for learning \(m\)-step weakly revealing POMDPs. This recovers a similar sample complexity as was shown by more specialized analysis tailored to POMDPs (e.g., [40; 36]).

## 6 Conclusion

**Summary.** This paper examines the role of information structure in general reinforcement learning problems. We introduced novel models that explicitly represent information structure, and proved an upper bound on the sample complexity of general reinforcement learning problems in terms of information structure. The central quantity in this upper bound is derived from a graphical representation of the information structure, and admits an interpretation as an effective information-structural state, generalizing the typical notion of a Markovian state.

**Limitations.** The results of this paper are theoretical in nature, and the algorithm proposed to prove our main result is not computationally practical. Current SOTA algorithms for partially-observable algorithms are based on recurrent neural networks or other deep learning models, which create internal representations of latent states based on the history, akin to belief states. Our theory may offer insights into the design of improved architectures in such deep learning-based algorithms. For example, the information structure can be incorporated as an inductive bias of the neural network.

## References

* [1] Hans S Witsenhausen. "The Intrinsic Model for Discrete Stochastic Control: Some Open Problems". In: _Control Theory, Numerical Methods and Computer Systems Modelling_. Ed. by M. Beckmann, H. P. Kunzi, A. Bensoussan, and J. L. Lions. Berlin, Heidelberg: Springer Berlin Heidelberg, 1975 (cited on pages 1, 2, 17).
* [2] Hans S Witsenhausen. "On information structures, feedback and causality". In: _SIAM Journal on Control_ (1971) (cited on page 1).
* [3] Y. Ho and K. Chu. "Team decision theory and information structures in optimal control problems-Part I". In: _IEEE Transactions on Automatic Control_ (1972) (cited on pages 1, 17).
* [4] Y. Ho and K. Chu. "On the Equivalence of Information Structures in Static and Dynamic Teams". In: _IEEE Transactions on Automatic Control_ (1973) (cited on page 1).
* [5] Tsuneo Yoshikawa. "Decomposition of dynamic team decision problems". In: _IEEE Transactions on Automatic Control_ (1978) (cited on page 1).
* [6] Hans S Witsenhausen. "Equivalent Stochastic Control Problems". In: _Mathematics of Control, Signals, and Systems_ (1988) (cited on page 1).
* [7] Mark S Andersland and Demosthenis Teneketzis. "Information structures, causality, and nonsequential stochastic control I: Design-independent properties". In: _SIAM journal on control and optimization_ (1992) (cited on page 1).
* [8] Demosthenis Teneketzis. "On Information Structures and Nonsequential Stochastic Control". In: _CWI Quarterly_ (1996) (cited on page 1).
* [9] Aditya Mahajan and Sekhar Tatikonda. "A Graphical Modeling Approach to Simplifying Sequential Teams". In: _2009 7th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks_. 2009 (cited on page 1).
* [10] Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. "Optimal Control Strategies in Delayed Sharing Information Structures". In: _IEEE Transactions on Automatic Control_ (2011) (cited on page 1).
* [11] Aditya Mahajan, Nuno C. Martins, Michael C. Rotkowitz, and Serdar Yuksel. "Information Structures in Optimal Decentralized Control". In: _2012 IEEE 51st IEEE Conference on Decision and Control (CDC)_. Maui, HI, USA: IEEE, 2012 (cited on pages 1, 17).
* [12] Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. "The Common-Information Approach to Decentralized Stochastic Control". In: _Information and Control in Networks_. Ed. by Giacomo Como, Bo Bernhardsson, and Anders Rantzer. Cham: Springer International Publishing, 2014 (cited on pages 1, 18).
* [13] Andreas A. Malikopoulos. "On Team Decision Problems with Nonclassical Information Structures". 2022. arXiv: 2101.10992 [math] (cited on page 1).
* [14] Naci Saldi and Serdar Yuksel. "Geometry of Information Structures, Strategic Measures and Associated Stochastic Control Topologies". In: _Probability Surveys_ (2022) (cited on page 1).
* [15] Hans S Witsenhausen. "Separation of estimation and control for discrete time systems". In: _Proceedings of the IEEE_ (1971) (cited on pages 1, 17).
* [16] Richard Bellman. "Dynamic programming and stochastic control processes". In: _Information and control_ (1958) (cited on page 1).
* [17] Christopher John Cornish Hellaby Watkins. "Learning from delayed rewards". In: (1989) (cited on page 1).
* [18] Satinder Singh, Tommi Jaakkola, Michael L Littman, and Csaba Szepesvari. "Convergence results for single-step on-policy reinforcement-learning algorithms". In: _Machine learning_ (2000) (cited on page 2).
* [19] Richard S Sutton, Hamid Maei, and Csaba Szepesvari. "A convergent \(o(n)\) temporal-difference algorithm for off-policy learning with linear function approximation". In: _Advances in neural information processing systems_ (2008) (cited on page 2).
* [20] Remi Munos and Csaba Szepesvari. "Finite-Time Bounds for Fitted Value Iteration." In: _Journal of Machine Learning Research_ (2008) (cited on page 2).
* [21] Yasin Abbasi-Yadkori and Csaba Szepesvari. "Regret bounds for the adaptive control of linear quadratic systems". In: _Proceedings of the 24th Annual Conference on Learning Theory_. JMLR Workshop and Conference Proceedings. 2011 (cited on page 2).
* [22] Tor Lattimore and Marcus Hutter. "PAC bounds for discounted MDPs". In: _Algorithmic Learning Theory: 23rd International Conference, ALT 2012, Lyon, France, October 29-31, 2012. Proceedings 23_. Springer. 2012 (cited on page 2).

* [23] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. "Playing Atari with Deep Reinforcement Learning". 2013. arXiv: 1312.5602 [cs] (cited on page 2).
* [24] Jens Kober, J. Andrew Bagnell, and Jan Peters. "Reinforcement Learning in Robotics: A Survey". In: _The International Journal of Robotics Research_ (2013) (cited on pages 2, 18).
* [25] Volodymyr Mnih et al. "Human-Level Control through Deep Reinforcement Learning". In: _Nature_ (2015) (cited on page 2).
* [26] David Silver et al. "Mastering the Game of Go with Deep Neural Networks and Tree Search". In: _Nature_ (2016) (cited on pages 2, 18).
* [27] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving". 2016. arXiv: 1610.03295 [cs, stat] (cited on pages 2, 18).
* [28] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M. Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, and Richard Powell. "Alphastar: Mastering the Real-Time Strategy Game Starcraft Ii". In: _DeepMind blog_ (2019) (cited on pages 2, 18).
* [29] Aditya Mahajan and Sekhar Tatikonda. "An Axiomatic Approach for Simplification of Sequential Teams". In: (2011) (cited on page 2).
* [30] Aditya Mahajan and Sekhar Tatikonda. "An Algorithmic Approach to Identify Irrelevant Information in Sequential Teams". In: _Automatica_ (2015) (cited on page 2).
* [31] Michael Littman and Richard S Sutton. "Predictive representations of state". In: _Advances in neural information processing systems_ (2001) (cited on pages 3, 6, 17, 19, 20).
* [32] Herbert Jaeger. "Observable Operator Models for Discrete Stochastic Time Series". In: _Neural Computation_ (2000) (cited on pages 3, 17, 19).
* A Generic Model-based Algorithm for Partially Observable Sequential Decision Making". 2022. arXiv: 2209.14997 [cs, stat] (cited on pages 4, 10, 17, 18, 21, 25, 30, 39).
* [34] Masatoshi Uehara, Ayush Sekhar, Jason D. Lee, Nathan Kallus, and Wen Sun. "Provably Efficient Reinforcement Learning in Partially Observable Dynamical Systems". 2022. arXiv: 2206.12020 [cs, math, stat] (cited on pages 4, 10, 17).
* [35] Ruiquan Huang, Yingbin Liang, and Jing Yang. "Provably Efficient UCB-type Algorithms For Learning Predictive State Representations". 2023. arXiv: 2307.00405 [cs, stat] (cited on pages 4, 10, 17, 18, 21, 24, 26, 27, 38, 40, 41, 47, 53, 54).
* [36] Qinghua Liu, Alan Chung, Csaba Szepesvari, and Chi Jin. "When Is Partially Observable Reinforcement Learning Not Scary?" 2022. arXiv: 2204.08967 [cs, eess, stat] (cited on pages 8, 10, 17, 18, 25, 27).
* [37] Qinghua Liu, Csaba Szepesvari, and Chi Jin. "Sample-Efficient Reinforcement Learning of Partially Observable Markov Games". 2022. arXiv: 2206.01315 [cs, stat] (cited on pages 8, 18, 27).
* [38] Fan Chen, Yu Bai, and Song Mei. "Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms". 2022. arXiv: 2209.14990 [cs, math, stat] (cited on pages 10, 17, 18, 25, 48).
* [39] Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D. Lee. "PAC Reinforcement Learning for Predictive State Representations". 2022. arXiv: 2207.05738 [cs] (cited on pages 10, 17, 18).
* [40] Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, and Qinghua Liu. "Sample-Efficient Reinforcement Learning of Undercomplete POMDPs". 2020. arXiv: 2006.12484 [cs, math, stat] (cited on pages 10, 17).
* [41] Christos H. Papadimitriou and John N. Tsitsiklis. "The Complexity of Markov Decision Processes". In: _Mathematics of operations research_ (1987) (cited on page 17).
* [42] Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. "Optimal control strategies in delayed sharing information structures". In: _IEEE Transactions on Automatic Control_ (2010) (cited on page 17).
* [43] Ashutosh Nayyar, Abhishek Gupta, Cedric Langbort, and Tamer Basar. "Common information based Markov perfect equilibria for stochastic games with asymmetric information: Finite games". In: _IEEE Transactions on Automatic Control_ (2013) (cited on page 17).

* [44] Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. "Decentralized stochastic control with partial history sharing: A common information approach". In: _IEEE Transactions on Automatic Control_ (2013) (cited on page 17).
* [45] Yi Ouyang, Hamidreza Tavafoghi, and Demosthenis Teneketzis. "Dynamic games with asymmetric information: Common information based perfect bayesian equilibria and sequential decomposition". In: _IEEE Transactions on Automatic Control_ (2016) (cited on page 17).
* [46] Aditya Dave, Nishanth Venkatesh, and Andreas A Malikopoulos. "Decentralized Control of Two Agents with Nested Accessible Information". In: _2022 American Control Conference (ACC)_. IEEE. 2022 (cited on page 17).
* [47] Yue Guan, Mohammad Afshari, and Panagiotis Tsiotras. "Zero-Sum Games between Mean-Field Teams: A Common Information and Reachability based Analysis". 2023. arXiv: 2303. 12243 [eess.SY] (cited on page 17).
* [48] Serdar Yuksel and Tamer Basar. "Stochastic Teams, Games and Control under Information Constraints". Springer, 2023 (cited on page 17).
* [49] Joseph A Tatman and Ross D Shachter. "Dynamic programming and influence diagrams". In: _IEEE transactions on systems, man, and cybernetics_ (1990) (cited on page 17).
* [50] Daphne Koller and Brian Milch. "Multi-agent influence diagrams for representing and solving games". In: _Games and economic behavior_ (2003) (cited on page 17).
* [51] Ian Osband and Benjamin Van Roy. "Near-optimal reinforcement learning in factored mdps". In: _Advances in Neural Information Processing Systems_ (2014) (cited on page 17).
* [52] Peter Auer, Thomas Jaksch, and Ronald Ortner. "Near-Optimal Regret Bounds for Reinforcement Learning". In: _Advances in neural information processing systems_ (2008) (cited on page 17).
* [53] Shipra Agrawal and Randy Jia. "Optimistic Posterior Sampling for Reinforcement Learning: Worst-Case Regret Bounds". In: _Advances in Neural Information Processing Systems_ (2017) (cited on page 17).
* [54] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. "Minimax Regret Bounds for Reinforcement Learning". In: _International Conference on Machine Learning_. PMLR, 2017 (cited on page 17).
* [55] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism". In: _Advances in Neural Information Processing Systems_ (2021) (cited on page 17).
* [56] Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. "Complexity of Finite-Horizon Markov Decision Process Problems". In: _Journal of the ACM (JACM)_ (2000) (cited on page 17).
* [57] Nikos Vlassis, Michael L. Littman, and David Barber. "On the Computational Complexity of Stochastic Controller Optimization in POMDPs". In: _ACM Transactions on Computation Theory (TOCT)_ (2012) (cited on page 17).
* [58] Elchanan Mossel and Sebastien Roch. "Learning Nonsingular Phylogenies and Hidden Markov Models". In: _Proceedings of the Thirty-Seventh Annual ACM Symposium on Theory of Computing_. 2005 (cited on page 17).
* [59] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. "PAC Reinforcement Learning with Rich Observations". In: _Advances in Neural Information Processing Systems_ (2016) (cited on page 17).
* [60] Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. "Provable Reinforcement Learning with a Short-Term Memory". In: _International Conference on Machine Learning_. PMLR, 2022 (cited on page 17).
* [61] Jiacheng Guo, Zihao Li, Huazheng Wang, Mengdi Wang, Zhuoran Yang, and Xuezhou Zhang. "Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP". 2023. arXiv: 2306.12356 [cs.LG] (cited on page 17).
* [62] Hongming Zhang, Tongzheng Ren, Chenjun Xiao, Dale Schuurmans, and Bo Dai. "Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning". 2023. arXiv: 2311.12244 [cs.LG] (cited on page 17).
* [63] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. "Learning in observable pomdps, without computationally intractable oracles". In: _Advances in Neural Information Processing Systems_ (2022) (cited on page 17).

* [64] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. "Planning in Observable POMDPs in Quasipolynomial Time". 2022. arXiv: 2201.04735 [cs.LG] (cited on page 17).
* [65] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. "Planning and Learning in Partially Observable Systems via Filter Stability". In: _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_. 2023 (cited on page 17).
* [66] Qi Cai, Zhuoran Yang, and Zhaoran Wang. "Reinforcement learning from partial observation: Linear function approximation with provable sample efficiency". In: _International Conference on Machine Learning_. PMLR. 2022 (cited on page 17).
* [67] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. "Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency". 2022. arXiv: 2205.13476 [cs, eess, stat] (cited on page 17).
* [68] Satinder P. Singh, Michael L. Littman, Nicholas K. Jong, David Pardoe, and Peter Stone. "Learning Predictive State Representations". In: _Proceedings of the 20th International Conference on Machine Learning (ICML-03)_. 2003 (cited on page 17).
* [69] Satinder Singh, Michael R James, and Matthew R Rudary. "Predictive State Representations: A New Theory for Modeling Dynamical Systems". In: _Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI2004)_ (2004) (cited on page 17).
* [70] Michael R. James, Satinder Singh, and Michael L. Littman. "Planning with Predictive State Representations". In: _2004 International Conference on Machine Learning and Applications, 2004. Proceedings._ IEEE, 2004 (cited on page 17).
* [71] Peter McCracken and Michael Bowling. "Online Discovery and Learning of Predictive State Representations". In: _Advances in neural information processing systems_ (2005) (cited on page 17).
* [72] Byron Boots, Sajid M. Siddiqi, and Geoffrey J. Gordon. "Closing the Learning-Planning Loop with Predictive State Representations". In: _The International Journal of Robotics Research_ (2011) (cited on page 17).
* [73] Nan Jiang, Alex Kulesza, and Satinder Singh. "Completing State Representations Using Spectral Learning". In: _Advances in Neural Information Processing Systems_ (2018) (cited on page 17).
* [74] Zhi Zhang, Zhuoran Yang, Han Liu, Pratap Tokekar, and Furong Huang. "Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory". In: _International Conference on Learning Representations_. 2021 (cited on page 17).
* [75] Ahmed Hefny, Carlton Downey, and Geoffrey J. Gordon. "Supervised Learning for Dynamical System Learning". In: _Advances in neural information processing systems_ (2015) (cited on page 17).
* [76] Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. "GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond". 2023. arXiv: 2211.01962 [cs, math, stat] (cited on pages 17, 18, 48).
* [77] Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. "Posterior Sampling for Competitive RL: Function Approximation and Partial Observation". 2023. arXiv: 2310.19861 [cs.LG] (cited on pages 17, 18, 48).
* [78] Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, and Zhaoran Wang. "Maximize to explore: One objective function fusing estimation, planning, and exploration". In: _Advances in Neural Information Processing Systems_ (2024) (cited on pages 17, 18).
* [79] Richard S Sutton and Andrew G Barto. "Reinforcement learning: An introduction". MIT press, 2018 (cited on page 18).
* [80] Noam Brown and Tuomas Sandholm. "Superhuman AI for Multiplayer Poker". In: _Science_ (2019) (cited on page 18).
* [81] Ronen I. Brafman and Moshe Tennenholtz. "R-Max-a General Polynomial Time Algorithm for near-Optimal Reinforcement Learning". In: _Journal of Machine Learning Research_ (2002) (cited on page 18).
* [82] Yu Bai, Chi Jin, and Tiancheng Yu. "Near-Optimal Reinforcement Learning with Self-Play". In: _Advances in Neural Information Processing Systems_. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin. Curran Associates, Inc., 2020 (cited on page 18).
* [83] Ziang Song, Song Mei, and Yu Bai. "When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?" In: (2021). arXiv: 2110.04184 (cited on page 18).

* [84] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. "Regret Minimization in Games with Incomplete Information". In: _Advances in neural information processing systems_ (2007) (cited on page 18).
* [85] Tadashi Kozuno, Pierre Menard, Remi Munos, and Michal Valko. "Model-Free Learning for Two-Player Zero-Sum Partially Observable Markov Games with Perfect Recall". In: (2021). arXiv: 2106.06279 (cited on page 18).
* [86] Gabriele Farina and Tuomas Sandholm. "Model-Free Online Learning in Unknown Sequential Decision Making Problems and Games". In: _Proceedings of the AAAI Conference on Artificial Intelligence_. 2021 (cited on page 18).
* [87] Xiangyu Liu and Kaiqing Zhang. "Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing". 2024. arXiv: 2308.08705 [cs.LG] (cited on page 18).
* [88] Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. "Approximate information state for approximate planning and reinforcement learning in partially observed systems". In: _The Journal of Machine Learning Research_ (2022) (cited on page 18).
* [89] Weichao Mao, Kaiqing Zhang, Erik Miehling, and Tamer Basar. "Information state embedding in partially observable cooperative multi-agent reinforcement learning". In: _2020 59th IEEE Conference on Decision and Control (CDC)_. IEEE. 2020 (cited on page 18).
* [90] Ali Kara and Serdar Yuksel. "Near optimality of finite memory feedback policies in partially observed markov decision processes". In: _Journal of Machine Learning Research_ (2022) (cited on page 18).
* [91] Hsu Kao and Vijay Subramanian. "Common information based approximate state representations in multi-agent reinforcement learning". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2022 (cited on page 18).
* [92] Dengwang Tang, Ashutosh Nayyar, and Rahul Jain. "A novel point-based algorithm for multi-agent control using the common information approach". In: _2023 62nd IEEE Conference on Decision and Control (CDC)_. IEEE. 2023 (cited on page 18).
* [93] John Nash. "Non-cooperative games". In: _Annals of mathematics_ (1951) (cited on page 28).
* [94] Sara van de Geer. "Rates of Convergence for Maximum Likelihood Estimators". In: _Applications of Empirical Process Theory_. Reprint. Cambridge Series on Statistical and Probabilistic Mathematics. Cambridge: Cambridge Univ. Pr, 2006 (cited on pages 33, 40).
* [95] Thomas Verma and Judea Pearl. "Causal networks: Semantics and expressiveness". In: _Machine intelligence and pattern recognition_. Elsevier, 1990 (cited on page 35).
* [96] Varsha Dani, Thomas P Hayes, and Sham M Kakade. "Stochastic linear optimization under bandit feedback". In: (2008) (cited on pages 53, 54).
* [97] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. "Improved algorithms for linear stochastic bandits". In: _Advances in neural information processing systems_ (2011) (cited on pages 53, 54).
* [98] Alexandra Carpentier, Claire Vernade, and Yasin Abbasi-Yadkori. "The elliptical potential lemma revisited". In: (2020). arXiv: 2010.10182 [stat.ML] (cited on pages 53, 54).

## Appendix A Summary of Notation

\begin{tabular}{l l} \multicolumn{2}{l}{**Generic Sequential Decision-Making Problems**} \\ \hline \(\mathbb{X}_{t}\) & Space that the variable \(X_{t}\) lies in within the stochastic process \((X_{1},\ldots,X_{H})\). \\ \(\mathcal{O}\) & \(\mathcal{O}\subset[H]\) denotes the set of observations among the variables \((X_{1},\ldots,X_{H})\). \\ \(\mathcal{A}\) & \(\mathcal{A}\subset[H]\) denotes the set of actions among the variables \((X_{1},\ldots,X_{H})\). \\ \(\mathbb{H}_{h}\) & The space of histories at time \(h\). \(\mathbb{H}_{h}\coloneqq\prod_{s=1}^{h}\mathbb{X}_{s}\). \\ \(\mathbb{F}_{h}\) & The space of futures at time \(h\). \(\mathbb{F}_{h}\coloneqq\prod_{s=h+1}^{H}\mathbb{X}_{s}\). \\ \(\mathrm{obs}(\cdot)\) & The observation component of a trajectory. \(\mathrm{obs}(x_{i},\ldots,x_{j})\coloneqq(x_{s}:s\in\mathcal{O}_{i:j})\). \\ \(\mathrm{act}(\cdot)\) & The action component of a trajectory. \(\mathrm{act}(x_{i},\ldots,x_{j})\coloneqq(x_{s}:s\in\mathcal{A}_{i:j})\). \\ \(\mathbb{H}_{h}^{\{o,a\}}\) & The space of observation (resp., action) histories. E.g., \(\mathbb{H}_{h}^{o}\coloneqq\prod_{s\in\mathcal{O}_{1:h}}\mathbb{X}_{s}= \mathrm{obs}(\mathbb{H}_{h})\). \\ \(\mathbb{F}_{h}^{\{o,a\}}\) & The space of observation (resp., action) histories. E.g., \(\mathbb{F}_{h}^{o}\coloneqq\prod_{s\in\mathcal{O}_{h+1:H}}\mathbb{X}_{s}= \mathrm{obs}(\mathbb{F}_{h})\). \\ \(\overline{\mathbb{P}}\left[\tau_{h}\right]\) & The probability of a trajectory given actions are executed. \(\overline{\mathbb{P}}\left[\tau_{h}\right]\coloneqq\mathbb{P}\left[\mathtt{ obs}(\tau_{h})\mid\mathrm{do}(\mathtt{act}(\tau_{h}))\right]\). \\ \(\bm{D}_{h}\) & Dynamics matrix at time \(h\). \(\bm{D}_{h}\in\mathbb{R}^{\left|\mathbb{H}_{h}\right|\times\left|\mathbb{F}_{h }\right|},\left[\bm{D}_{h}\right]_{\tau_{h},\omega_{h}}\coloneqq\overline{ \mathbb{P}}\left[\tau_{h},\omega_{h}\right]\). \(r_{h}\coloneqq\mathrm{rank}(\bm{D}_{h})\). \\ \(\pi(\tau_{h})\) & For \(\tau_{h}=(x_{1},\ldots,x_{h})\) and policy \(\pi\), \(\pi(\tau_{h})\coloneqq\prod_{s\in\mathcal{A}_{1:h}}\pi(x_{s}\mid x_{1},\ldots, x_{s-1})\). \\ \(\pi(\omega_{h}\mid\tau_{h})\) & For \(\tau_{h}=(x_{1},\ldots,x_{h})\), \(\omega_{h}=\left(x_{h+1},\ldots,x_{h^{\prime}}\right)\), \(\pi(\omega_{h}\mid\tau_{h})=\prod_{s\in\mathcal{A}_{h+1:h}}\pi(x_{s}\mid x_{1},\ldots,x_{s-1})\). \\ \multicolumn{2}{l}{**POSTs and POSGs**} \\ \hline \(\mathbb{X}_{t}\) & Space that the variable \(X_{t}\) lies in within the stochastic process \((X_{1},\ldots,X_{T})\). \\ \(\mathcal{S}\) & \(\mathcal{S}\subset[T]\) denotes the set of system variables among the variables \((X_{1},\ldots,X_{T})\). \\ \(\mathcal{A}\) & \(\mathcal{A}\subset[T]\) denotes the set of action variables among the variables \((X_{1},\ldots,X_{T})\). \\ \(\mathcal{O}\) & \(\mathcal{O}\subset\mathcal{S}\) denotes the subset of system variables which are observable. \\ \(\mathcal{U}\) & The union of observable system variables and action variables. \(\mathcal{U}\coloneqq\mathcal{O}\cup\mathcal{A}\). Let \(H\coloneqq\left|\mathcal{U}\right|\). \\ \(t(h)\) & For \(h\in[H]\) indexing the order among observables, \(t(h)\in\mathcal{U}\) denotes the order among all variables. \\ \(\mathcal{I}_{t}\) & The information set of the \(t\)-th variable. \\ \(\mathbb{I}_{t}\) & \(\mathbb{I}_{t}\coloneqq\prod_{s\in\mathcal{I}_{t}}\mathbb{X}_{s}\) denotes the information space at time \(t\). \\ \(\mathcal{I}_{h}^{\dagger}\) & The minimal \(d\)-separating set at the \(h\)-th observable. See Definition 5. \\ \(\mathbb{I}_{h}^{\dagger}\) & \(\mathbb{I}_{h}^{\dagger}\coloneqq\prod_{s\in\mathcal{I}_{h}^{\dagger}}\mathbb{X }_{s}\) denotes the "information-structural state". \\ \multicolumn{2}{l}{**Generalized PSRs**} \\ \hline \(\mathbb{Q}_{h}\) & Core test set at time \(h\). Let \(d_{h}\coloneqq\left|\mathbb{Q}_{h}\right|\) and \(d=\max_{h}d_{h}\). \\ \(\mathbb{Q}_{h}^{A}\) & Action component of core test set at time \(h\). \(\mathbb{Q}_{h}^{A}=\mathtt{act}(\mathbb{Q}_{h})\). \\ \(Q_{A}\) & Maximum size of the action component of core test sets. \(Q_{A}\coloneqq\max_{h}\left|\mathbb{Q}_{h}^{A}\right|\). \\ \(M_{h}\) & Observable operators of PSR representation mapping \(M_{h}:\mathbb{X}_{h}\to\mathbb{R}^{d_{h}\times d_{h-1}}\). \\ \(\psi_{h}\) & Prediction features. \(\psi_{h}(\tau_{h})\coloneqq\left(\overline{\mathbb{P}}\left[\tau_{h},q\right] \right)_{q\in\mathbb{Q}_{h}}\). In PSR, \(\psi_{h}(x_{1},\ldots,x_{h})=M_{h}(x_{h})\cdots M_{1}(x_{1})\psi_{0}\). \\ \(m_{h}\) & Prediction coefficients. \(m_{h}(\omega_{h})^{\top}\coloneqq\phi_{H}(x_{H})^{\top}M_{H-1}(x_{H-1})\cdots M _{h+1}(x_{h+1})\). \\ \(\psi_{h}\) & Normalized prediction features. \(\overline{\psi}_{h}(\tau_{h})\coloneqq\psi_{h}(\tau_{h})/\overline{\mathbb{P}} \left[\tau_{h}\right]=\left(\overline{\mathbb{P}}\left[q\mid\tau_{h}\right] \right)_{q\in\mathbb{Q}_{h}}\). \\ \multicolumn{2}{l}{**General Mathematical Notation**} \\ \hline \(\mathsf{D}_{\mathsf{TV}}\left(p,q\right)\) & Total variation distance. \(\mathsf{D}_{\mathsf{TV}}\left(p,q\right)\coloneqq\sum_{x\in\mathbb{X}}[p(x)-q(x)]\). \\ \(\mathsf{D}_{h}^{2}\left(p,q\right)\) & Hellinger squared distance. \(\mathsf{D}_{h}^{2}\left(p,q\right)\coloneqq\frac{1}{2}\sum_{x\in\mathbb{X}}( \sqrt{p(x)}-\sqrt{q(x)})^{2}\). \\ \(\sigma_{k}(A)\) & \(k\)-th largest singular value of the matrix \(A\). \\ \(\left\|A\right\|_{p}\) & The matrix \(p\)-norm. \(\left\|A\right\|_{p}\coloneqq\max_{\left\|x\right\|_{p}}=1\left\|Ax\right\|_{p}\). \\ \(\left\|x\right\|_{A}\) & The vector norm induced by the positive semi-definite matrix \(A\). \(\left\|x\right\|_{A}\coloneqq\sqrt{x^{\top}Ax}\). \\ \(A^{\dagger}\) & Moore-Penrose pseudoinverse. \\ \(\mathcal{P}(\mathbb{A})\) & Space of probability distributions over \(\mathbb{A}\). \(\mathcal{P}(\mathbb{B}|\mathbb{A})\) is the space of kernels from \(\mathbb{A}\) to \(\mathbb{B}\). \\ \(\mathcal{N}_{i:j}\) & For an index set \(\mathcal{N}\subset[H]\), \(\mathcal{N}_{i:j}=\mathcal{N}\cap\{i,\ldots,j\}\). \\ \hline \end{tabular}

Related Work

**The study of information structure in the control literature.** In stochastic control, the construct of the "information structure" is used to model the structural properties of a system which may restrict the flow, storage, and processing of information. The role of information structure in decentralized control has been extensively studied since Witsenhausen [15] and Ho and Chu [3] began investigating information structures in the context of team decision theory. For example, early work showed that the information structure can determine the tractability of optimal decentralized control problems [15; 41]. More generally, information structure plays an important role in the analysis of multi-agent team decision problems and games, as well as in the design of efficient algorithms, especially in the decentralized setting. See, e.g., Mahajan et al. [11], Nayyar et al. [42; 43; 44], Ouyang et al. [45], Dave et al. [46], and Guan et al. [47] and the references therein. We also refer the reader to Yuksel and Basar [48] for a comprehensive overview of the interaction between information and control, including recent progress in the field. The models we propose in this paper are closely related to Witsenhausen's intrinsic model [1], but with some added elements to model partial-observability in the context of reinforcement learning. Related to this is the framework of multi-agent influence diagrams [e.g., 49; 50] which allow for an explicit representation of dependence relations among variables in games. Whereas the above-mentioned work studies the role of information structure in planning and control, we study the the role of information structure in _reinforcement learning_ (i.e., statistical estimation and sample complexity). We note that specific types of structural assumptions related to information structure have been studied in the learning setting as well. For example, previous work has studied factored MDPs [51], which assume an information structure where the state transitions and the reward function are factored, enabling improved learning results. In the present work, we study the role of information structure in reinforcement learning in greater generality, focusing particularly on the partially-observable setting.

**Learning under partial observations.** In an MDP, where the system dynamics obey a Markovian property and are fully observable, reinforcement learning has been shown to be both computationally and statistically efficient [e.g., 52; 53; 54; 55]. However, under partial observability, reinforcement learning can be computationally and statistically intractable in the worst case, even when assuming a Markovian latent state. Such worst-case hardness results are well-known. For example, [56; 41; 57] show that planning is computational intractable and [58; 59] show that learning is statistically intractable, in the worst-case. In these worst cases, the hardness comes from instances where the observations reveal little information about the latent state, which causes errors in learned representations to be uncontrollable. Accordingly, sub-classes of POMDPs have been identified in recent work where added structural conditions make efficient learning possible. One such condition is decodability [see e.g., 59; 60; 61; 62], which assumes that the latent state can be decoded from the current observation (i.e., Block MDP), or an \(m\)-step history of observations. Another set of conditions is the "observability" condition [63; 64; 65] and its cousin the "weakly revealing" condition [36; 40] which require different belief states to induce distinguishable distributions over observations. These conditions are further extended to POMDP models in the function approximation setting, where the state or observation spaces are large and function approximators (e.g., linear functions) are used to represent the model. See, e.g., Uehara et al. [34], Guo et al. [61], Zhang et al. [62], Cai et al. [66], and Wang et al. [67]. In our work, we build on the above by identifying a class of POSTs/POSGs which can be learned efficiently. This is significant since POSTs/POSGs are much more general than POMDPs and do not assume the existence of a latent state.

**Predictive state representations.** Predictive state representations were introduced by Littman and Sutton [31] building on prior work on observable operator models by Jaeger [32] which proposed the idea of predictive representations as an alternative to belief states for modeling HMMs and POMDPs [see also 68; 69; 70; 71]. PSRs are a way to _represent_ the dynamics of a sequential decision-making problem by modeling the (conditional) probabilities of a small set of future trajectories, typically called "core tests". In a PSR, the probability of any future trajectory is a deterministic function of the conditional probabilities of the core tests. That is, the probabilities of the core tests encode all the information that the past contains about the future. Littman and Sutton [31] showed that any POMDP can be represented as a PSR. Various reinforcement learning methods for PSRs have been proposed under the assumption that data distribution is explorative, including spectral algorithms [72; 73; 74] and supervised learning approaches [75]. In addition, when it comes to the online setting where the algorithm needs to explore, there is a line of work that extends the theory and algorithms for online POMDP learning to PSRs. Moreover, some of these works propose generic theory and algorithms that can be applied to a large class of models including MDPs, POMDPs, and two-player zero-sum dynamic games with partial observability. See, e.g., Liu et al. [33], Huang et al. [35], Liu et al. [36],Chen et al. [38], Zhan et al. [39], Zhong et al. [76], Qiu et al. [77], and Liu et al. [78]. Our work is particularly related to the works that combine the idea of optimism in the face of uncertainty [79] and maximum likelihood model estimation [33; 35; 36; 38; 39]. Specifically, our algorithm extends the UCB-type algorithm proposed by Huang et al. [35] for standard PSRs to learn a generalization of PSRs which captures POSTs/POSGs.

**Learning in multi-agent systems.** Most applications of interest in reinforcement learning involve the participation of multiple agents in the same environment. Empirical research has achieved striking success in several domains, including for example in the games of Go [26], Starcraft [28], and Poker [80], as well as in robotic control [24] and autonomous driving [27]. There also exists a growing literature of theoretical work. For example, Brafman and Tennenholtz [81], Bai et al. [82], and Song et al. [83] tackle learning in Markov games (MGs)--a generalization of single-agent MDPs that assumes the existence of a Markovian state which is observable by all agents. Another model which has been explored in the literature is imperfect-information extensive-form games (IIEFG), which assumes tree-structured transitions and deterministic emission, and can be viewed as a subclass of partially-observable Markov games (POMGs). Learning under this model has been studied in Zinkevich et al. [84], Kozuno et al. [85], and Farina and Sandholm [86]. More recently, Liu et al. [37] studied reinforcement learning in POMGs using an MLE-based algorithm building on their previous work in the single-agent setting [36]. To address the computational intractability of the planning step for such model-based algorithms, Liu and Zhang [87] proposed a quasi-efficient algorithm for multi-agent POMGs that runs in quasi-polynomial time with quasi-polynomial sample complexity. Their proposed algorithm leverages the common information approach [see 12] to construct an approximate Markov game where the state space of this new game corresponds to the space of approximate common information among agents. The idea of leveraging an information-sharing structure in multi-agent reinforcement learning has also appeared in Subramanian et al. [88], Mao et al. [89], Kara and Yuksel [90], Kao and Subramanian [91], and Tang et al. [92].

In each of the above-mentioned models (e.g., MDP, POMDP, MG, IIEFG, POMG, etc.), a particular fixed information structure is assumed. We emphasize that the POST/POSG model proposed in our work allows the information structure to be specified arbitrarily and hence captures these models as special cases within a unifying framework. Moreover, our analysis and proposed algorithm significantly expand the class of multi-agent sequential decision-making problems that can be efficiently learned.

## Appendix C Preliminaries

### Generic Sequential Decision-Making Problems

Consider a controlled stochastic process \((X_{1},\ldots,X_{H})\), where \(X_{h}\) is a random variable corresponding to the variable at time \(h\). At each time \(h\in[H]\), the variable \(X_{h}\) may be either an 'observation' (i.e., observable system variable) or an 'action'. The dynamics of this stochastic process are described by a tuple \((H,\left\{\mathbb{X}_{h}\right\}_{h},\mathcal{O},\mathcal{A},\mathbb{P})\), where \(H\) is the time horizon, \(\mathbb{X}_{h}\) is the variable space at time \(h\) (i.e., \(X_{h}\in\mathbb{X}_{h}\)), \(\mathcal{O}\subset[H]\) is the index set of observations (i.e., \(X_{h}\) is an observation if \(h\in\mathcal{O}\)), \(\mathcal{A}\subset[H]\) is the index set of actions, and \(\mathbb{P}=\left\{\mathbb{P}_{h}\right\}_{h\in\mathcal{O}}\) is a set of probability kernels which describes the the probability of any trajectory \(x_{1},\ldots,x_{H}\) given that the actions are executed,

\[\mathbb{P}\left[\left\{x_{s}:s\in\mathcal{O}\right\}\mid\left\{x_{s}:s\in \mathcal{A}\right\}\right]=\prod_{h\in\mathcal{O}}\mathbb{P}_{h}\left[x_{h} \mid x_{1},\ldots,x_{h-1}\right].\] (8)

A choice of policy \(\pi=\left\{\pi_{h}\right\}_{h\in\mathcal{A}}\) induces a probability distribution on \(\mathbb{X}_{1}\times\cdots\times\mathbb{X}_{H}\) as follows

\[\mathbb{P}^{\pi}\left(x_{1},\ldots,x_{H}\right)=\prod_{h\in\mathcal{O}} \mathbb{P}_{h}\left(x_{h}\mid x_{1},\ldots,x_{h-1}\right)\cdot\prod_{h\in \mathcal{A}}\pi_{h}\left(x_{h}\mid x_{1},\ldots,x_{h-1}\right).\] (9)

We now define some notation. Let \(\mathbb{H}_{h}=\prod_{s\in 1:h}\mathbb{X}_{s}\) denote the space of histories at time \(h\) and \(\mathbb{F}_{h}=\prod_{s\in h+1:H}\mathbb{X}_{s}\) denote the space futures at time \(h\). Similarly, let \(\mathbb{H}_{h}^{o}=\mathsf{obs}(\mathbb{H}_{h})=\prod_{s\in\mathcal{O}_{1:h}} \mathbb{X}_{s}\) denote the observation component of histories and let \(\mathbb{H}_{h}^{a}=\mathsf{act}(\mathbb{H}_{h})=\prod_{s\in\mathcal{A}_{1:h}} \mathbb{X}_{s}\) denote the action component. Here, \(\mathcal{O}_{i:j}\) denotes \(\mathcal{O}\cap\left\{i,\ldots,j\right\}\), and similarly for \(\mathcal{A}_{i:j}\). The observation and action components of the futures, \(\mathbb{F}_{h}^{o}\) and \(\mathbb{F}_{h}^{a}\) respectively, are defined similarly.

We define the _system dynamics matrix_\(\bm{D}_{h}\in\mathbb{R}^{|\mathbb{H}_{h}|\times|\mathbb{F}_{h}|}\) as the matrix giving the probability of each possible pair of history and future at time \(h\) given the execution of the actions,

\[\left[\bm{D}_{h}\right]_{\tau_{h},\omega_{h}}=\overline{\mathbb{P}}\left[\tau_{ h},\omega_{h}\right]=\mathbb{P}\left[\tau_{h}^{o},\omega_{h}^{o}\mid\mathrm{do}( \tau_{h}^{a},\omega_{h}^{a})\right],\quad\tau_{h}\in\mathbb{H}_{h},\omega_{h} \in\mathbb{F}_{h},\] (10)

where \(\omega_{h}^{o}=\texttt{obs}(\omega_{h})\) are is the observation component of the future \(\omega_{h}\), \(\omega_{h}^{a}=\texttt{act}(\omega_{h})\) is the action component, and similarly for \(\tau_{h}^{o},\tau_{h}^{a}\). Note that the actions are actively executed via the \(\mathrm{do}\)-operation. Hence, the system dynamics matrices are independent of any action-selection criteria. Note that \(\bm{D}_{H}\in\mathbb{R}^{|\mathbb{H}_{H}|\times 1}\) is defined as \(\left[\bm{D}_{H}\right]_{\tau_{H}}=\overline{\mathbb{P}}\left[\tau_{H}\right]\), and \(\bm{D}_{0}=\bm{D}_{H}^{\top}\).

We introduce the notion of the _rank_ of the dynamics. The rank of such a controlled stochastic process is the maximal rank of its dynamics matrices. This is a measure of the complexity of the dynamics.

**Definition** (Rank of dynamics; Definition 1).: _The rank of the dynamics \(\left\{\bm{D}_{h}\right\}_{h\in[H]}\) is \(r=\max_{h\in[H]}\mathrm{rank}(\bm{D}_{h})\)._

This defines the dynamics of the system. A sequential decision-making problem is such a controlled stochastic process together with an _objective_. The objective is defined by a reward function \(R:\mathbb{X}_{1}\times\cdots\times\mathbb{X}_{H}\to[0,1]\) mapping a trajectory to a reward in \([0,1]\). The agent(s) can affect the dynamics of the system through their choice of actions or policies. Each action \(X_{h},h\in\mathcal{A}\) may be chosen by either a single agent or one of several agents (e.g., a team). The policy at time \(h\in\mathcal{A}\) is a mapping \(\pi_{h}:\mathbb{H}_{h-1}\to\mathcal{P}(\mathbb{X}_{h})\) from previous observations to an action (or a distribution over actions, if randomized). The collection of policies at all time steps is denoted \(\bm{\pi}=(\pi_{h}:h\in\mathcal{A})\), and induces a probability distribution over trajectories, denoted \(\mathbb{P}^{\bm{\pi}}\). Then, the value of a policy \(\bm{\pi}\) is the expected value of the reward under the measure \(\mathbb{P}^{\bm{\pi}}\), \(V^{R}(\bm{\pi})\coloneqq\mathbb{E}^{\bm{\pi}}\left[R(X_{1},\ldots,X_{H})\right]\), where \(\mathbb{E}^{\bm{\pi}}\) is the expectation associated with \(\mathbb{P}^{\bm{\pi}}\).

The formalism of sequential decision-making problems introduced in this section is highly generic, but does not explicitly model the _information structure_. In the next section, we introduce the models of _partially observable sequential teams/games_, which explicitly represent information structures. We then show that the information structure characterizes the rank of a sequential decision-making problem as per Definition 1.

### (Generalized) Predictive State Representations

Predictive state representations (PSR) [31; 32] are a model of dynamical systems and sequential decision-making problems based on predicting future observations given the past, without explicitly modeling a latent state. In this section, we propose and formalize a generalization of standard PSRs.

In the standard formulation of sequential decision-making and predictive state representations, the sequence of variables is such that observations and actions always occur in an alternating manner (i.e., \(o_{h},a_{h},o_{h+1},a_{h+1},\ldots\)). The POST/POSG models we will propose are more general, and hence require a more flexible formalization of PSRs which allows for arbitrary order of observations and actions as well as arbitrary variable spaces at each time point. This generalization of PSRs will be used in our reinforcement learning algorithms.

The "PSR rank" of a sequential decision-making problem coincides with the rank of its dynamics, as defined in Definition 1. Recall that the system dynamics matrix \(\bm{D}_{h}\in\mathbb{R}^{|\mathbb{H}_{h}|\times|\mathbb{F}_{h}|}\) is indexed by all possible observable histories \(\tau_{h}\) and futures \(\omega_{h}\). Denote the rank of the system dynamics at time \(h\) by \(r_{h}:=\mathrm{rank}(\bm{D}_{h})\).

Consider a sequential decision-making problem as defined in Section 2.1 (i.e., with an arbitrary order of observations and actions, and arbitrary variable spaces). At the heart of predictive state representations is the concept of "core test sets." A core test set at time \(h\) is a set of futures such that the set of probabilities of those futures conditioned on the past encodes all the information that the past contains about the future. This is formalized in the definition below as a set of futures such that the submatrix of the full dynamics matrix restricted to those futures is full rank.

**Definition** (Core test sets).: _A core test set at time \(h\) is a subset of \(d_{h}\geq r_{h}\) futures, \(\mathbb{Q}_{h}\coloneqq\left\{q_{h}^{1},\ldots,q_{h}^{d_{h}}\right\}\subset \mathbb{F}_{h}\), such that the submatrix \(\bm{D}_{h}[\mathbb{Q}_{h}]\in\mathbb{R}^{|\mathbb{E}_{h}|\times d_{h}}\) is full-rank, \(\mathrm{rank}(\bm{D}_{h}[\mathbb{Q}_{h}])=\mathrm{rank}(\bm{D}_{h})=r_{h}\)._

A core test set implies the existence of a matrix \(\bm{W}_{h}\in\mathbb{R}^{|\mathbb{F}_{h}|\times d_{h}}\) such that \(\bm{D}_{h}=\bm{D}_{h}[\mathbb{Q}_{h}]\cdot\bm{W}_{h}^{\top}\).

Denote the \(\tau_{h}\)-th row of \(\bm{D}_{h}[\mathbb{Q}_{h}]\) by,

\[\psi_{h}(\tau_{h}):=\left(\overline{\mathbb{P}}\left[\tau_{h},q_{h}^{1}\right], \ldots,\overline{\mathbb{P}}\left[\tau_{h},q_{h}^{d_{h}}\right]\right)\in \mathbb{R}^{d_{h}}.\] (11)

The vector \(\psi_{h}(\tau_{h})\) is a sufficient statistic for the history \(\tau_{h}\) in predicting the probabilities of all futures conditioned on \(\tau_{h}\). This is sometimes called the _prediction features_ of a history \(\tau_{h}\).

For any integer \(d_{h}\geq r_{h}\), there exists a core test set of size \(d_{h}\). In particular, for any low-rank sequential decision-making problem, there exists a minimal core test set of size \(r_{h}\) at each \(h\). However, the minimal core test set depends on the system dynamics matrix \(\bm{D}_{h}\), which is unknown in the learning setting. In the literature on reinforcement learning in PSRs, it is typically assumed that a core test set is known. We address the problem of constructing a PSR representation for POSTs/POSGs in Section 4.

For a core test set \(\mathbb{Q}_{h}\), let \(\mathbb{Q}_{h}^{A}=\{\mathrm{act}(q):q\in\mathbb{Q}_{h}\}\), where \(\mathrm{act}(q)\) denotes the action components of the test \(q\in\mathbb{Q}_{h}\). Let \(Q_{A}=\max_{h}\left|\mathbb{Q}_{h}^{A}\right|\) and \(d=\max_{h}d_{h}\).

With core test sets defined, we are now ready to present the definition of a generalized predictive state representation. The essential element in a PSR is a set of operators \(M_{h}:\mathbb{X}_{h}\to\mathbb{R}^{d_{h}\times d_{h-1}}\) for each time point \(h\in[H]\). Given the prediction features at time \(h-1\), \(\psi_{h-1}(x_{1},\ldots,x_{h-1})\in\mathbb{R}^{d_{h-1}}\), the linear map \(M_{h}(x_{h})\) computes the prediction features at time \(h\), incorporating the additional observation \(x_{h}\). That is, \(\psi_{h}(x_{1},\ldots,x_{h})=M_{h}(x_{h})\psi_{h-1}(x_{1},\ldots,x_{h-1})\). The full definition is given below.

**Definition** (Generalized Predictive State Representations; Definition 3).: _Consider a sequential decision-making problem \((X_{h}\in\mathbb{X}_{h})\) where \(\mathcal{A},\mathcal{O}\) partition \([H]\) into actions and observations, respectively. Then, a predictive state representation of this sequential decision-making problem is a tuple \(\theta=(\{\mathbb{Q}_{h}\}_{0\leq h\leq H-1},\phi_{H},\bm{M},\psi_{0})\) given by_

1. \(\{\mathbb{Q}_{h}\}_{0\leq h\leq H-1}\) _are core test sets, including for_ \(h=0\)_, where_ \(\mathbb{Q}_{0}=\{q_{0}^{1},\ldots,q_{0}^{d_{0}}\}\subset\mathbb{F}_{0}\) _are core tests before the system begins._
2. \(\psi_{0}\in\mathbb{R}^{d_{0}}\) _is the vector_ \(\psi(\emptyset)=(\overline{\mathbb{P}}[q_{0}^{1}],\ldots,\overline{\mathbb{P}} [q_{0}^{d_{0}}])\)_._
3. \(\bm{M}=\{M_{h}\}_{1\leq h\leq H-1}\) _is a set of mappings_ \(M_{h}:\mathbb{X}_{h}\to\mathbb{R}^{d_{h}\times d_{h-1}}\)_, from an observation/action to a matrix of size_ \(d_{h}\times d_{h-1}\)_._
4. \(\phi_{H}:\mathbb{X}_{H}\to\mathbb{R}^{d_{H-1}}\) _is a mapping from the final observation to a_ \(d_{H-1}\)_-dimensional vector._

_This tuple satisfies,_

\[\overline{\mathbb{P}}\left[x_{1},\ldots,x_{H}\right] =\phi_{H}(x_{H})^{\top}M_{H-1}(x_{H-1})\cdots M_{1}(x_{1})\psi_{0}\] (12) \[\psi_{h}(x_{1},\ldots,x_{h}) =M_{h}(x_{h})\cdots M_{1}(x_{1})\psi_{0},\,\forall h\] (13)

To obtain a probability for a trajectory \(\tau_{h}=(x_{1},\ldots,x_{h})\), with \(h<H\), note that \(\sum_{\omega_{h}\in\mathcal{F}_{h}}\overline{\mathbb{P}}\left[\tau_{h},\omega_{ h}\right]=\left|\mathbb{F}_{h}^{a}\right|\overline{\mathbb{P}}\left[\tau_{h}\right]\). Hence

\[\overline{\mathbb{P}}\left[\tau_{h}\right] =\frac{1}{\left|\mathbb{F}_{h}^{a}\right|}\sum_{\omega_{h}} \overline{\mathbb{P}}\left[\tau_{h},\omega_{h}\right]\] \[=\frac{1}{\prod_{s\in h+1:H}\left|\mathbb{X}_{s}\right|^{\bm{1} \left(s\in\mathcal{A}\right)}}\sum_{x_{H}}\cdots\sum_{x_{h+1}}\phi_{H}^{\top }M_{H}(x_{H})\cdots M_{h+1}(x_{h+1})\psi_{h}(\tau_{h}).\]

Thus, if we recursively define \(\phi_{h},\,h<H\) via

\[\frac{1}{\left|\mathbb{X}_{h}\right|^{\bm{1}\left\{h\in\mathcal{A}\right\}}} \sum_{x_{h}}\phi_{h}^{\top}M_{h}(x_{h})=\phi_{h-1}^{\top},\] (14)

with \(\phi_{H}\) as the terminating condition, then, we can obtain \(\overline{\mathbb{P}}\left[\tau_{h}\right]\) for any \(h<H\), via an inner product between \(\phi_{h}\) and \(\psi_{h}(\tau_{h})\),

\[\overline{\mathbb{P}}\left[\tau_{h}\right]=\phi_{h}^{\top}\psi_{h}(\tau_{h}).\] (15)

Finally, if we define \(\overline{\psi}_{h}(\tau_{h})=\psi_{h}(\tau_{h})/\overline{\mathbb{P}}[\tau_{h}]\), then we obtain the _conditional_ probability of the core tests given the history, \(\overline{\psi}_{h}(\tau_{h})=(\overline{\mathbb{P}}[q_{h}^{1}\,|\,\tau_{h}],\,\ldots,\,\overline{\mathbb{P}}[q_{h}^{d_{h}}\,|\,\tau_{h}])\in\mathbb{R}^{d _{h}}\). \(\overline{\psi}_{h}(\tau_{h})\) is known as the (normalized) prediction feature of the history \(\tau_{h}\)[31].

**Remark 1** (Generality and difference from standard PSRs).: _In standard PSRs, observations and actions are assumed to occur in an alternating manner, and hence observable operators are defined on pairs of observations and actions (i.e., \(M_{h}(o_{h},a_{h})\)). This structure leads to a somewhat simpler description compared to the above. However, our formulation is more general, as it allows each variable to be treated independently, and allows for an arbitrary sequence of variables with arbitrary spaces. This generality will be needed when modeling problems with an explicit representation of information structure._

An important condition for the learnability of PSR models, which was used in prior work [including 33, 35], is the so-called "well-conditioning assumption". We state the analogous assumption for our generalized PSR model below.

**Assumption** (\(\gamma\)-well-conditioned generalized PSR; Assumption 1).: _A PSR model \(\theta=\left(\{\mathbb{Q}_{h}\}_{0\leq h\leq H-1},\phi_{H},\bm{M},\psi_{0}\right)\), as defined in Definition 3, is said to be \(\gamma\)-well conditioned for \(\gamma>0\) if it satisfies_

1. _For any_ \(h\in[H]\)_,_ \[\max_{\begin{subarray}{c}z\in\mathbb{R}^{h}\\ \|z\|_{1}\leq 1\end{subarray}}\max_{\pi}\sum_{\omega_{h}\in\mathbb{F}_{h}} \pi(\omega_{h}|\tau_{h})\left|m_{h}(\omega_{h})^{\top}z\right|\leq\frac{1}{ \gamma},\] (16) _where_ \(m_{h}(\omega_{h})^{\top}=\phi_{H}(x_{H})^{\top}M_{H-1}(x_{H-1})\cdots M_{h+1 }(x_{h+1})\) _with_ \(\omega_{h}=(x_{h+1},\ldots,x_{H})\in\mathbb{F}_{h}\)_. The maximization is over policies_ \(\pi\) _such that for any fixed future observations_ \(\omega_{h}^{\omega}\)_,_ \(\sum_{\omega_{h}^{\omega}}\pi(\omega_{h}^{\omega},\omega_{h}^{a})=1\)_._
2. _For any_ \(h\in[H-1]\)_,_ \[\max_{\begin{subarray}{c}z\in\mathbb{R}^{h}\\ \|z\|_{1}\leq 1\end{subarray}}\sum_{x_{h}\in\mathbb{X}_{h}}\left\|M_{h}(x_{h})z \right\|_{1}\pi(x_{h})\leq\frac{\left|\mathbb{Q}_{h+1}^{A}\right|}{\gamma},\] _where_ \(\pi(x_{h})=1\) _when_ \(h\notin\mathcal{A}\) _and_ \(\sum_{x_{h}}\pi(x_{h})=1\) _when_ \(h\in\mathcal{A}\)_._

To understand this condition, recall that \(m_{h}(\omega_{h})^{\top}\psi_{h}(\tau_{h})=\overline{\mathbb{P}}\left[\tau_{h},\omega_{h}\right]\). We may think of \(z\) in Assumption 1 as representing the error in estimating \(\psi_{h}(\tau_{h})\), the probabilities of core tests at time \(h\) given the history \(\tau_{h}\). The \(\gamma\)-well-conditioned assumption ensures that the error in estimating the overall PSR (i.e., the probability of a particular trajectory) does not blow up when the estimation error of \(\psi_{h}(\tau_{h})\) is small.

The following result states that any sequential decision-making problem of the form described in Section 2.1 admits a generalized PSR representation. The proof and explicit construction are given in Appendix G.

**Proposition 1**.: _Let \((X_{1},\ldots,X_{H})\) be any sequential decision-making problem with observation index set \(\mathcal{O}\), action index set \(\mathcal{A}\), and variable spaces \(\{\mathbb{X}_{h}\}_{h\in[H]}\). Let \(r_{h}=\operatorname{rank}(\bm{D}_{h})\), where \(D_{h},h\in[H]\) are the system dynamics matrices. Then, there exists a PSR representation \(\psi_{0}\), \(\phi_{H}:\mathbb{X}_{H}\to\mathbb{R}^{r_{H-1}}\), \(M_{h}:\mathbb{X}_{h}\to\mathbb{R}^{r_{h+1}\times r_{h}},h\in[H-1]\), satisfying Definition 3._

Proof.: The proof is given in Appendix G. 

## Appendix D Examples of Information Structures and their Rank

The analysis in Section 3 and Theorem 1 characterizes the rank of any sequential decision-making problem as a function of its information structure. In this section, we illustrate this on several sequential decision-making problems, characterizing the information-structural complexity of their dynamics. The procedure is as follows: **1)** formulate the sequential decision-making problem as a POST/POSG; **2)** represent the information structure as a labeled directed acyclic graph \(\mathcal{G}\); **3)** remove incoming edges into the action variables to produce \(\mathcal{G}^{\uparrow}\); **4)** apply Theorem 1 to find the information structural state at each point in time through a \(d\)-separation analysis.

**Illustration: translating to the POST/POSG framework.** We begin by illustrating how an arbitrary sequential decision-making problem can be formulated in the POST/POSG framework. Considera POMDP with variables \((s_{1},o_{1},a_{1},s_{2},o_{2},a_{2},...)\). This can be formulated as a POST/POSG by a simple relabelling of variables as follows.

\[\begin{array}{cccccccccccc}s_{1}&o_{1}&a_{1}&s_{2}&o_{2}&a_{2}&&s_{t}&o_{t}&a_{ t}\\ \downarrow&\downarrow&\downarrow&\downarrow&\downarrow&\downarrow&\downarrow& \cdots&\downarrow&\downarrow&\downarrow&\downarrow&\cdots\\ x_{1}&x_{2}&x_{3}&x_{4}&x_{5}&x_{6}&&x_{3t-2}&x_{3t-1}&x_{3t}\end{array}\]

Here, the system variables \(\mathcal{S}\) are the \(s\)-type and \(o\)-type variables, with system index set \(\mathcal{S}=\{1,2,4,5,7,8,...\}\), and the action variables are the \(a\)-type variables with action index set \(\mathcal{A}=\{3,6,9,...\}\). The observable system variables are the \(o\)-type variables only, with index set \(\mathcal{O}=\{2,5,8,...\}\subset\mathcal{S}\). This can be done for any sequential decision-making problem.

To ease notation, let us not explicitly write the indices in this section, but rather use the original notation for the variables in the problem formulation. For example, we'll write \(\mathcal{S}=\{s_{t},o_{t},t\in[T]\}\). Similarly, we use the notation \(\mathcal{I}(x)\) to mean the information set corresponding to the variable \(x\). Similarly, \(\mathcal{I}^{\dagger}(x)\) denotes the information-structural state at the time when \(x\) occurs. For example, in a POMDP \(\mathcal{I}(s_{t})=\{s_{t-1},a_{t-1}\}\), \(\mathcal{I}(o_{t})=\{s_{t}\}\), and \(\mathcal{I}(a_{t})=\{o_{1:t},a_{1:t}\}\).

Below, we will consider several examples of sequential decision-making problems, and apply the information-structural analysis of Theorem 1 to obtain a bound on the rank of the observable dynamics (which in turn implies a bound on the sample complexity, by Theorem 3).

**Decentralized POMDPs and POMGs.** At each time \(t\), the system variables of a decentralized POMDP (or POMG) consist of a latent state \(s_{t}\), observations for each agent \(o_{t}^{1},\ldots,o_{t}^{N}\), and actions of each agent \(a_{t}^{1},\ldots,a_{t}^{N}\). The latent state transitions are Markovian and depend on the agents' joint action. The observations are sampled via a kernel conditional on the latent state. Each agent can use their own history of observations to choose an action. Thus, the information structure is given by,

\[\mathcal{I}(s_{t})=\left\{s_{t-1},a_{t-1}^{1},\ldots,a_{t-1}^{N}\right\},\, \mathcal{I}(o_{t}^{i})=\left\{s_{t}\right\},\,\mathcal{I}(a_{t}^{i})=\left\{ o_{1:t-1}^{i},a_{1:t-1}^{i}\right\}.\]

Here, the observable variables are \(\mathcal{U}=\left\{o_{1:T}^{i},a_{1:T}^{i},\,i\in[N]\right\}\). By Theorem 1, we have \(\mathcal{I}^{\dagger}(o_{t}^{i})=\left\{s_{t}\right\},\,\forall t,i,\) as shown in Figure 4(a). Thus, the rank of a Dec-POMDP is bounded by \(|\mathbb{S}|\), where \(\mathbb{S}\) is the state space. Note that in the case of models with a true latent state (e.g., POMDPs, Dec-POMDPs, and POMGs), the information-structural state coincides with the true latent state.

**Limited-memory information structures.** Consider a sequential decision making problem with variables \(o_{t},a_{t},t\in[T]\) and an information structure with \(m\)-length memory. That is, observations can only depend directly on at most \(m\) of the most recent observations and actions. That is, the information structure is

\[\mathcal{I}(o_{t})=\left\{o_{t-m:t-1},a_{t-m:t-1}\right\},\,\mathcal{I}(a_{t}) =\left\{o_{1:t},a_{1:t-1}\right\}.\]

The observables are all observations and actions, \(\mathcal{U}=\left\{o_{1:T},a_{1:T}\right\}\). By Theorem 1 we have that \(\mathcal{I}^{\dagger}(o_{t})=\left\{o_{t-m:t-1},a_{t-m:t-1}\right\}\), as shown in Figure 4(d). Hence, the rank of this sequential decision-making process is bounded by \(|\mathbb{O}|^{m}\,|\mathbb{A}|^{m}\).

Figure 4: An illustrative example of the information-structural state for POMDPs. **Left.** The DAG representation of the information structure \(\mathcal{G}\). **Right.** The DAG \(\mathcal{G}^{\dagger}\) is depicted by drawing the edges corresponding to the information sets of the action variables with dotted lines. The information-structural state coincides with the Markovian state \(s_{t}\), and is depicted in red. Future observables are drawn in green, and past observables are drawn in blue.

**Symmetric / "Mean-field" Information Structures.** Consider a sequential decision-making problem with \(N\) agents. Each agent has their own local state, \(s^{i}_{t}\in\mathbb{S}_{\mathrm{local}}\). Similarly, at each time point, each agent takes an action \(a^{t}_{i}\in\mathbb{A}_{\mathrm{loc}}\). The global state \(s_{t}=(s^{1}_{t},\ldots,s^{N}_{t})\in\mathbb{S}^{N}_{\mathrm{loc}}=:\mathbb{S}\) is composed by of all agents' local states. Similarly, the joint action space is \(\mathbb{A}\coloneqq\mathbb{A}^{N}_{\mathrm{loc}}\). Consider a symmetric information structure where the evolution of each agent's local state depends only on a symmetric aggregation of all agents' states and actions, rather than on the local state/action of any particular agent. That is, the identity of who is in what state or takes which action does not matter--only the distribution of states and actions. This is often referred to as a "mean-field" setting (in the limit). Here, the transition depends only on the distribution of local states and actions, defined as \(s^{\mathrm{mf}}_{t}=\mathrm{dist}(s_{t})\coloneqq\frac{1}{N}\delta_{s^{i}_{t}}\), \(a^{\mathrm{mf}}_{t}=\mathrm{dist}(a_{t})\coloneqq\frac{1}{N}\delta_{a^{i}_{t}}\), for \(s_{t}\in\mathbb{S},a_{t}\in\mathbb{A}\). Different agents can have different transition kernels for their local state. Hence, by introducing \(\mathrm{dist}(s_{t}),\mathrm{dist}(a_{t})\) as auxiliary unobserved variables at each time \(t\), we obtain the following information structure,

\[\mathcal{I}(s^{i}_{t})=\left\{\mathrm{dist}(s_{t-1}),\mathrm{dist}(a_{t-1}) \right\},\;\mathcal{I}(a^{i}_{t})=\left\{s^{i}_{t}\right\}\]

Figure 5: DAG representation of various information structures. Solid edges indicate the edges in \(\mathcal{E}^{\dagger}\) and light edges indicate the information sets of action variables. Grey nodes represent unobservable variables, blue nodes represent past observable variables, green nodes represent future observable variables, and red nodes represent the information structural state \(\mathcal{I}^{\dagger}_{h}\). To find \(\mathcal{I}^{\dagger}_{h}\), as per Theorem 1, we first remove the incoming edges into the action variables, then we find the minimal set among all past variables (both observable and unobservable) which \(d\)-separates the past observations from the future observations.

and an application of Theorem 1 bounds the rank by

\[\left|\mathbb{I}^{\dagger}(s_{t}^{i})\right|<|\mathcal{S}_{\mathrm{loc}}|| \mathbb{A}_{\mathrm{loc}}|\left(\frac{N}{|\mathcal{S}_{\mathrm{loc}}|-1}+1 \right)^{|\mathcal{S}_{\mathrm{loc}}|-1}\left(\frac{N}{|\mathbb{A}_{\mathrm{ loc}}|-1}+1\right)^{|\mathcal{A}_{\mathrm{loc}}|-1}.\]

This is compared to \(|\mathcal{S}_{\mathrm{loc}}|^{N}\cdot|\mathbb{A}_{\mathrm{loc}}|^{N}\) (e.g., if we modeled this as an MDP with the state \(s_{t}\)), which is much larger when the number of agents is large. The information structure and \(d\)-separation decomposition are depicted in Figure 4(b).

**Point-to-Point Real-Time Communication with Feedback.** Consider the following model of real-time communication with feedback. Let \(x_{t}\) be the Markov source. At time \(t\), the encoder receives the source \(x_{t}\in\mathbb{X}\) and encodes sending a symbol \(z_{t}\in\mathbb{Z}\). The symbol is sent through a memoryless noisy channel which outputs \(y_{t}\) to the receiver. The decoder produces the estimate \(\widehat{x}_{t}\). The output of the noisy channel is also fed back to the encoder. The encoder and decoder have full memory of their observations and previous "actions". The observation variables are \(\mathcal{O}=\{x_{1:T},\,y_{1:T}\}\) and the "actions" are \(\mathcal{A}=\{z_{1:T},\,\widehat{x}_{1:T}\}\). Hence, the information structure is given by the following,

\[\mathcal{I}(x_{t})=\{x_{t-1}\}\,,\,\mathcal{I}(z_{t})=\{x_{1:t},y_{1:t-1},z_{1 :t-1}\}\,,\,\mathcal{I}(y_{t})=\{z_{t}\}\,,\,\mathcal{I}(\widehat{x}_{t})=\{ y_{1:t}\}\,.\]

By Proposition 1, we have that,

\[\mathcal{I}^{\dagger}(x_{t})=\{x_{t}\}\,,\,\mathcal{I}^{\dagger}(z_{t})=\{x_{ t}\}\,,\,\mathcal{I}^{\dagger}(y_{t})=\{x_{t},z_{t}\}\,,\,\mathcal{I}^{\dagger}( \widehat{x}_{t})=\{x_{t}\}\,.\]

Hence, the rank is bounded by \(|\mathbb{X}||\mathbb{Z}|\). This is depicted in Figure 4(c).

**Fully-Connected Information Structures.** Consider a sequential decision making problem with variables \(o_{t},a_{t},t\in[T]\) and a fully-connected information structure. That is, each observation directly depends on the entire history of observations and actions. Thus, the information structure is

\[\mathcal{I}(o_{t})=\{o_{1:t-1},a_{1:t-1}\}\,,\,\mathcal{I}(a_{t})=\{o_{1:t},a_ {1:t-1}\}\]

The observables are all observations and actions, \(\mathcal{U}=\{o_{1:T},a_{1:T}\}\). By Theorem 1 we have that \(\mathcal{I}^{\dagger}(o_{t})=\{o_{1:t-1},a_{1:t-1}\}\), as shown in Figure 4(e). Hence, the rank of this sequential decision-making process can be exponential in the time horizon.

The examples above show that the tractability of a sequential decision-making problem in terms of the complexity of its dynamics depends directly on its information structure. This gives an interpretation of why certain models, like POMDPs, are more tractable than those with arbitrary information structures. Previous work primarily considers particular problem classes with fixed and highly regular information structures. In this work we argue for the importance of explicitly modeling the information structure of a sequential decision-making problem.

**Remark 2** (Necessity of generalized PSRs).: _The formalization of generalized PSRs in Section 2.2 was necessary to enable the study of information structure through POSTs/POSGs. An alternative (naive) solution to construct PSR representations for models with non-alternating observations and actions is to aggregate consecutive observations and actions to force them to obey the standard formulation of PSRs. This approach results in a loss of "resolution" in the information structure. That is, when you aggregate consecutive system variables, you also aggregate the DAG which represents the information structure, losing potentially important structure. In particular, in the worst case, such aggregation could result in an exponential increase in the rank of the dynamics. The examples given above elucidate this. Consider for example the "mean-field" information structure. If we aggregated local states and actions into a combined global state and joint action, the PSR rank would indeed be \(|\mathcal{S}_{\mathrm{loc}}|^{N}|\mathbb{A}_{\mathrm{loc}}|^{N}\). By comparison, by considering each local state separately without aggregation, we are able to obtain a decomposition with a much smaller PSR rank._

## Appendix E UCB-Type Reinforcement Learning Algorithm for _Generalized_ PSRs

We will adapt the model-based UCB-type algorithm of Huang et al. [35], extending it to generalized PSRs, including those representing POSTs. The algorithm involves the estimation of an upper confidence bound which captures the uncertainty in the estimated model and drives exploration so as to minimize this uncertainty. The UCB-based approach has the advantage of providing a last-iterate guarantee and requiring a weaker notion of planning oracle (a standard planning oracle instead of an optimistic planning oracle as required by similar algorithms). The technical contribution of this section is to extend the algorithm and its theoretical guarantees to generalized PSRs. The tools developed in doing so can be used to directly extend any other PSR-based algorithm to generalized PSRs.

When learning generalized PSRs, we suppose that the core test sets \(\{\mathbb{Q}_{h}\}_{0\leq h\leq H-1}\) are known by the algorithm. For example, if the sequential decision-making problem is a POST, Section 4 provides conditions under which \(m\)-step futures form core test sets. Let \(\Theta\) be the set of \(\gamma\)-well-conditioned generalized PSR representations with \(\{\mathbb{Q}_{h}\}_{0\leq h\leq H-1}\) as core test sets. Denote by \(\overline{\Theta}_{\epsilon}\) an optimistic \(\epsilon\)-cover of \(\Theta\) (defined formally in Appendix G).

Recall that \(d_{h}\coloneqq|\mathbb{Q}_{h}|\) and \(d=\max_{h}d_{h}\). Moreover, \(\mathbb{Q}_{h}^{A}\coloneqq\mathtt{act}(\mathbb{Q}_{h})\) are the action components of the core test sets and \(Q_{A}\coloneqq\max_{h}\left|\mathbb{Q}_{h}^{A}\right|\) is the maximal size of those action components. We define the exploration action sequences at time \(h\) to be \(\mathbb{Q}_{h-1}^{\mathtt{exp}}=\mathtt{act}(\mathbb{X}_{h}\times\mathbb{Q}_ {h}\cup\mathbb{Q}_{h-1})\). Moreover, we define \(\mathfrak{u}_{h-1}^{\mathtt{exp}}\) as the policy, defined from time \(h-1\) onwards, in which each selection of action sequences in \(\mathbb{Q}_{h-1}^{\mathtt{exp}}\) are chosen uniformly at random. For a model \(\theta\) and reward function \(R\), we define the value of a policy under this model and reward as \(V_{\theta}^{R}(\pi)\coloneqq\sum_{\tau_{H}}R(\tau_{H})\mathbb{P}_{\theta}^{ \pi}(\tau_{H})\).

The algorithmic description is given in Algorithm 1. At each iteration \(k\), the learner collects a trajectory \(\tau_{H}^{k,h}\) for each time index \(h\in[H]\) by using a particular policy that drives exploration so as to better estimate the parameters associated with the \(h\)-th time step. To collect the trajectory \(\tau_{H}^{k,h}\), the learner executes the policy at the previous iteration, \(\pi^{k-1}\), until time \(h-1\) collecting the trajectory \(\tau_{h-1}^{k,h}\) then executes \(\mathfrak{u}_{h-1}^{\mathtt{exp}}\) which samples action sequences from \(\mathbb{Q}_{h-1}^{\mathtt{exp}}\) uniformly. The particular choice of the exploratory action sequences \(\mathbb{Q}_{h-1}^{\mathtt{exp}}\) comes out of the proof (see proof of Lemma 5 in the appendix). Intuitively, \(\mathtt{act}(\mathbb{Q}_{h-1})\) allows us to estimate the prediction features \(\overline{\psi}^{*}\left(\tau_{h-1}^{k,h}\right)=[\overline{\mathbb{P}}(q\,| \,\tau_{h-1}^{k,h})]_{q\in\mathbb{Q}_{h-1}}\), and \(\mathtt{act}(\mathbb{X}_{h}\times\mathbb{Q}_{h})\) allows us to estimate \(M_{h}^{*}(x_{h})\widetilde{\psi}^{*}(\tau_{h-1}^{k,h})\).

The collected trajectories are added to the dataset, together with the policies used to collect them. The next step is model estimation via (constrained) maximum likelihood estimation. The algorithm estimates a model \(\widehat{\theta}^{k}\) by selecting any model in a constrained set \(\mathcal{B}^{k}\) defined as

\[\begin{split}&\Theta_{\min}^{k}=\left\{\theta\in\Theta:\forall h,\,(\tau_{h},\pi)\in\mathcal{D}_{h}^{k},\;\mathbb{P}_{\theta}^{\pi}(\tau_{h}) \geq p_{\min}\right\},\\ &\mathcal{B}^{k}=\left\{\theta\in\Theta_{\min}^{k}:\sum_{(\tau_{H },\pi)\in\mathcal{D}^{k}}\log\mathbb{P}_{\theta}^{\pi}(\tau_{H})\geq\max_{ \theta^{\prime}\in\Theta_{\min}^{k}}\sum_{(\tau_{H},\pi)\in\mathcal{D}^{k}} \log\mathbb{P}_{\theta^{\prime}}^{\pi}(\tau_{H})-\beta\right\}.\end{split}\] (17)

The introduction of \(\Theta_{\min}^{k}\) ensures that \(\mathbb{P}_{\theta}^{\pi^{k-1}}(\tau_{h-1}^{k,h})\) is not too small so that the estimates of the prediction features \(\widetilde{\psi}^{*}(\tau_{h-1}^{k,h})=[\overline{\mathbb{P}}(q\,|\,\tau_{h-1 }^{k,h})]_{q\in\mathbb{Q}_{h-1}}\) are accurate. This design differs from other MLE-based estimators [e.g., 33, 36, 38] due to the estimation of parameters capturing _conditional_ probabilities.

Next, the algorithm chooses a policy which drives the algorithm to trajectories \(\tau_{h}\) whose prediction features have so far been unexplored. To do this, Algorithm 1 constructs an upper confidence bound on the total variation distance between the estimated model and the true model. This is done via a bonus function \(\widehat{b}^{k}(\tau_{H})\),

\[\begin{split}\widehat{b}^{k}(\tau_{H})&=\min\left\{ \alpha\sqrt{\sum_{h=0}^{H-1}\left\|\widehat{\widehat{\psi}}(\tau_{h})\right\| _{(\widehat{U}_{h}^{k})^{-1}}^{2}},1\right\},\quad\text{where},\\ \widehat{U}_{h}^{k}&=\lambda I+\sum_{\tau_{h}\in \mathcal{D}_{h^{k}}}\widehat{\widetilde{\psi}}^{k}(\tau_{h})\widehat{ \widetilde{\psi}}^{k}(\tau_{h})^{\top},\end{split}\] (18)

where \(\lambda\) and \(\alpha\) are pre-specified parameters to the algorithm. Thus, the bonus function captures the degree of uncertainty in the estimated prediction features \(\widehat{\overline{\psi}}(\tau_{h})\). In particular, the bonus \(\widehat{b}(\tau_{H})\) will be large for trajectories whose prediction feature \(\widehat{\overline{\psi}}(\tau_{h})\) lie far away from the empirical distribution of prediction features sampled in the dataset \(\mathcal{D}_{h}^{k}\). This is captured by computing the norm with respect to the covariance \(\widehat{U}_{h}^{k}\).

The algorithm then chooses an exploration policy for the next iteration which maximizes this upper confidence bound, hence collecting trajectories that have high uncertainty in their prediction features. When the estimated model is sufficiently accurate on all trajectories, the algorithm terminates and returns the optimal policy with respect to the reward function \(R\) under the estimated model.

We extend Huang et al.'s theoretical guarantees to show that Algorithm 1 enjoys polynomial sample complexity for _generalized_ PSRs (Definition 3).

**Theorem 4**.: _Suppose Assumption 1 holds. Suppose the parameters \(p_{\min},\lambda,\alpha,\beta\) are chosen appropriately. In particular, let_

\[p_{\min}\leq\frac{\delta}{KH\prod_{h=1}^{H}|\mathbb{X}_{h}|},\; \lambda=\frac{\gamma\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|^{2}Q_{A}\beta\max \{\sqrt{r},Q_{A}\sqrt{H}/\gamma\}}{\sqrt{dH}},\] \[\alpha=O\left(\frac{Q_{A}\sqrt{dH\lambda}}{\gamma^{2}}+\frac{\max _{s\in\mathcal{A}}|\mathbb{X}_{s}|\,Q_{A}\sqrt{\beta}}{\gamma}\right),\;\beta=O \left(\log\left|\overline{\Theta}_{\varepsilon}\right|\right),\;\varepsilon \leq\frac{p_{\min}}{KH}.\]

_Then, with probability at least \(1-\delta\), Algorithm 1 returns a model \(\theta^{\epsilon}\) and a policy \(\pi\) that satisfy_

\[V_{\theta^{\epsilon}}^{R}(\pi^{*})-V_{\theta^{\epsilon}}^{R}(\pi)\leq\varepsilon,\; \text{and}\;\forall\tilde{\pi},\;\texttt{D}_{\texttt{TV}}\left(\mathbb{P}_{ \theta^{*}}^{\tilde{\pi}}(\tau_{H}),\mathbb{P}_{\theta^{*}}^{\pi}(\tau_{H}) \right)\leq\varepsilon.\]

_In addition, the algorithm terminates with a sample complexity of,_

\[\bar{O}\left(\left(r+\frac{Q_{A}^{2}H}{\gamma^{2}}\right)\cdot\frac{rdH^{3} \cdot\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|^{2}\cdot Q_{A}^{4}\beta}{\gamma^{4 }\epsilon^{2}}\right).\]

Proof.: The proof is given in Appendix J. 

This result shows that the sample complexity of learning a generalized PSR depends on the problem size through a few key quantities. In particular, the sample complexity scales polynomially in the underlying rank \(r\), the dimension of the PSR parameterization \(d\), the size of the action component of the core tests \(Q_{A}\), the time horizon \(H\), the conditioning number \(\gamma^{-1}\), the size of the action spaces \(\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|\), the log covering number \(\log|\overline{\Theta}_{\epsilon}|\), and the desired suboptimality error \(\epsilon\). Note that \(\tilde{O}\) omits logarithmic dependence.

To apply this algorithm to a POST, we can use the generalized PSR parameterization constructed in Section 4. By Theorem 1 the PSR rank is bounded by \(r\leq\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert\). If this POST is \(\alpha\)-robustly \(\mathcal{I}^{\dagger}\)-weakly revealing, then by Theorem 2 it admits a \(\gamma\)-well-conditioned generalized PSR parameterization with \(\gamma=\alpha/\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert^{1/2}\) and the \(m\)-step futures as core test sets. Moreover, we have \(d=\max_{h}d_{h}=\max_{h}|\mathbb{Q}_{h}^{m}|\). The following corollary states that Algorithm 1 can learn a partially-observable sequential team with a sample complexity which is polynomial in the size of the information-structural state space \(\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert\).

**Corollary 1**.: _Suppose a partially-observable sequential team is \(m\)-step \(\alpha\)-robustly \(\mathcal{I}^{\dagger}\)-weakly revealing as per Definition 6. Applying Algorithm 1 to this PSR representation, with parameters \(p_{\min},\lambda,\alpha,\beta\) chosen as in Theorem 4, returns a \(\varepsilon\)-optimal policy with a sample complexity of,_

\[\tilde{O}\left(\left(1+\frac{Q_{A}^{2}H}{\alpha^{2}}\right)\frac{\max_{h} \lVert\mathbb{I}_{h}^{\dagger}\rVert^{7}\cdot\max_{h}|\mathbb{Q}_{h}^{m}| \cdot H^{5}\cdot\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|^{2}\cdot\max_{s\in \mathcal{U}}|\mathbb{X}_{s}|\cdot Q_{A}^{4}}{\alpha^{4}\epsilon^{2}}\right).\]

We can interpret this result as saying that the information structure of a sequential decision-making problem, through the quantity \(\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert\), is fundamentally a measure of the complexity of the dynamics which need to be modeled. As a result, learning is tractable when \(\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert\) is of modest size, and intractable otherwise. Recall that \(\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert\) is small when there exists "state-like" variables, whether they are observable or unobservable. In this sense, \(\max_{h}\lVert\mathbb{I}_{h}^{\dagger}\rVert\) is a fundamental quantity which generalizes the notion of a "state". For example, in the case of an \(m\)-step \(\alpha\)-weakly revealing POMDP, our algorithm has a sample complexity of \(\mathrm{poly}(S,(OA)^{m},H,\alpha^{-1})\cdot\epsilon^{-2}\), where \(S\) is the size of the state space, \(O\) is the size of the observation space, and \(A\) is the size of the action space. This is similar to the sample complexity of [36, 37], which designed an algorithm tailored specifically for weakly-revealing POMDPs. Our algorithms, together with the POST/POSG models, enable sample-efficient reinforcement learning for a much broader class of models all within a unified framework.

In this section, we extended the algorithm in Huang et al. [35] to generalized PSRs, enabling sample-efficient learning of POSTs. We emphasize that other PSR-based algorithms can be extended in a similar manner. In the next section, we tackle the problem of learning in the game setting where different agents have different objectives.

## Appendix F Extension to the game setting

### Partially Observable Sequential Games

In a POST, all agents share the same objective. In the game setting, different agents may have different objectives which compete with each other in interesting ways. Information structures play a crucial role in the study of games. The information available to one agent when making its decisions, compared to the information available to competing agents, determines how well it can achieve its objective. In particular, the information structure of a problem determines the set of equilibria it admits. There has been a plethora of work in the game theory community studying such problems.

Analogously to partially-observable sequential teams, we define partially-observable sequential _games_ (POSGs). The dynamics of a POSG are identical to a POST, with the same formalization of variable structure, variable spaces, information structure, system kernels, and decision kernels. In contrast to a POST, agents in a POSG may have different objectives. In a POSG, there exists \(N\) agents, with agent \(i\in[N]\) deciding the actions at times \(t\in\mathcal{A}^{i}\), where \(\mathcal{A}^{i}\subset\mathcal{A}\). Each agent has its own objective defined by a reward function \(R^{i}\). This is defined formally below.

**Definition 7** (Partially-Observable Sequential Game Model).: _A partially-observable sequential game (POSG) is a controlled stochastic process consisting of the following components: variable structure, variable spaces, information structure, system kernels, decision kernels, and observability. These are defined in an identical manner to Definition 4. Additionally, POSGs define a **reward structure** as follows. Let \(N\) be the number of agents. Each agent may act several times. Denote by \(\mathcal{A}^{i}\subset\mathcal{A}\) the index of action variables associated to agent \(i\in[N]\). Each agent has a reward function \(R^{i}:\prod_{t\in\mathcal{U}}\mathbb{X}_{t}\rightarrow[0,1]\) which they aim to maximize._

Denote by \(\pi^{i}=(\pi_{t}:t\in\mathcal{A}^{i})\) the collection of decision kernels belonging to agent \(i\), one for each action they take. Denote by \(\bm{\pi}=(\pi^{1},\dots,\pi^{N})\) the collection of all agents' policies. Fixing \(\bm{\pi}\) inducesa probability distribution over \(\mathbb{X}_{1}\times\cdots\mathbb{X}_{T}\) in the same way as in the team setting,

\[\mathbb{P}^{\bm{\pi}}\left[X_{1}=x_{1},\ldots X_{T}=x_{t}\right]=\prod_{t\in \mathcal{S}}\mathcal{T}_{t}(x_{t}|\left\{x_{s}:s\in\mathcal{I}_{t}\right\}) \prod_{t\in\mathcal{A}}\pi_{t}(x_{t}|\left\{x_{s}:s\in\mathcal{I}_{t}\right\}).\] (19)

The value of a policy \(\bm{\pi}\) for agent \(i\in[N]\) is defined as the expected value of their reward \(R^{i}\) under \(\mathbb{P}^{\bm{\pi}}\),

\[V^{i}(\bm{\pi})\equiv V^{i}(\pi^{i},\bm{\pi}^{-i})\coloneqq\mathbb{E}^{\bm{ \pi}}\left[R^{i}(X_{t(1)},\ldots,X_{t(H)})\right],\] (20)

where \(\bm{\pi}^{-i}=(\pi^{j}:j\neq i)\).

The nature of randomization in agents' policies is crucial to the analysis of solution concepts in the game setting. To model randomized policies, which are potentially correlated, we introduce a random seed \(\omega\in\Omega\) which is sampled at the beginning of an episode. Then, the policy at time \(t\in\mathcal{A}\) can be modeled as a deterministic function mapping the seed \(\omega\) and information variable \(i_{t}\in\mathbb{I}_{t}\) to an action \(\mathbb{X}_{t}\). That is, \(\pi_{t}:\Omega\times\mathbb{I}_{t}\rightarrow\mathbb{X}_{t}\). To model independently randomized policies with each agent having private randomness, we consider the special case where the seed has the product structure \(\omega=(\omega_{1},\ldots,\omega_{N})\in\Omega_{1}\times\cdots\times\Omega_{N}\), and \(\omega_{i}\) is the seed belonging to agent \(i\in[N]\). Then, for \(t\in\mathcal{A}^{i}\), \(\pi_{t}:\Omega_{i}\times\mathbb{I}_{t}\rightarrow\mathbb{X}_{t}\). For each agent \(i\in[N]\), define the three policy spaces,

1. Deterministic policies, \(\Gamma^{i}_{\mathrm{det}}=\left\{\pi^{i}:\pi^{i}=\left(\pi_{t}:\mathbb{I}_{t} \rightarrow\mathbb{X}_{t},t\in\mathcal{A}^{i}\right)\right\}\),
2. Independently-randomized policies, \(\Gamma^{i}_{\mathrm{ind}}=\left\{\pi^{i}:\pi^{i}=\left(\pi_{t}:\Omega_{i} \times\mathbb{I}_{t}\rightarrow\mathbb{X}_{t},t\in\mathcal{A}^{i}\right)\right\}\),
3. Correlated randomized policies, \(\Gamma^{i}_{\mathrm{cor}}=\left\{\pi^{i}:\pi^{i}=\left(\pi_{t}:\Omega\times \mathbb{I}_{t}\rightarrow\mathbb{X}_{t},t\in\mathcal{A}^{i}\right)\right\}\).

Define the joint deterministic policy space, as \(\bm{\Gamma}_{\mathrm{det}}=\Gamma^{1}_{\mathrm{det}}\times\cdots\times\Gamma ^{N}_{\mathrm{det}}\), and similarly for the independently-randomized policy space \(\bm{\Gamma}_{\mathrm{ind}}\), and the correlated randomized policy space \(\bm{\Gamma}_{\mathrm{cor}}\).

When studying games, a common question is to find an _equilibrium_ within a particular policy space. At a high-level, an equilibrium is a joint policy where no agent can do better by deviating from their policy when the other agents keep their policies fixed. We will consider several notions of equilibrium. We begin by defining the notion of a _best-response_. Suppose that agent \(i\)'s policy space is \(\Gamma^{i}\) (e.g., \(\Gamma^{i}_{\mathrm{det}}\), \(\Gamma^{i}_{\mathrm{ind}}\), or \(\Gamma^{i}_{\mathrm{cor}}\)). Then, we say that agent \(i\)'s policy \(\pi^{i}\) is a best response to \(\bm{\pi}^{-i}\) if there is no policy in \(\Gamma^{i}\) which achieves a higher value. This is formalized in the definition below.

**Definition 8** (Best response).: _For a joint policy \(\bm{\pi}\), \(\pi^{i}\) is said to be a best-response to \(\bm{\pi}^{-i}\) in the policy space \(\Gamma^{i}\) (e.g., \(\Gamma^{i}_{\mathrm{det}}\), \(\Gamma^{i}_{\mathrm{ind}}\), or \(\Gamma^{i}_{\mathrm{cor}}\)), if \(V^{i}(\pi^{i},\bm{\pi}^{-i})=\max_{\tilde{\pi}^{i}\in\Gamma^{i}}V^{i}(\tilde{ \pi}^{i},\bm{\pi}^{-i})=:V^{i,\dagger}(\bm{\pi}^{-i})\)._

This leads to the definition of two notions of equilibria. A _Nash Equilibrium_ (NE) is a joint policy where all agents are best-responding in the space of independently-randomized policies. A _Coarse Correlated Equilibrium_ (CCE) is a joint policy where all agents are best-responding in the space of correlated randomized policies. The difference between NE and CCE is that the randomness in the joint policy must be independent in an NE but can be correlated in a CCE. Since \(\Gamma_{\mathrm{ind}}\subset\Gamma_{\mathrm{cor}}\), coarse correlated equilibria are a generalization of Nash equilibria. We define them formally below.

**Definition 9** (Nash Equilibrium).: _A joint policy \(\bm{\pi}\in\Gamma_{\mathrm{ind}}\) is said to be a Nash equilibrium if for all agents \(i\in[N]\), \(V^{i}(\bm{\pi})=\max_{\tilde{\pi}^{i}\in\Gamma^{i}_{\mathrm{ind}}}V^{i}(\tilde{ \pi}^{i},\bm{\pi}^{-i})=:V^{i,\dagger}(\bm{\pi}^{-i})\). A joint policy \(\bm{\pi}\in\Gamma_{\mathrm{ind}}\) is said to an \(\varepsilon\)-approximate Nash equilibrium if \(V^{i}(\bm{\pi})\geq V^{i,\dagger}(\bm{\pi}^{-i})-\varepsilon\) for all \(i\in[N]\)._

**Definition 10** (Coarse Correlated Equilibrium).: _A joint policy \(\bm{\pi}\in\Gamma_{\mathrm{cor}}\) is said to be a coarse correlated equilibrium if for all agents \(i\in[N]\), \(V^{i}(\bm{\pi})=\max_{\tilde{\pi}^{i}\in\Gamma^{i}_{\mathrm{cor}}}V^{i}(\tilde{ \pi}^{i},\bm{\pi}^{-i})=:V^{i,\dagger}(\bm{\pi}^{-i})\). A joint policy \(\bm{\pi}\in\Gamma_{\mathrm{cor}}\) is said to an \(\varepsilon\)-approximate Nash equilibrium if \(V^{i}(\bm{\pi})\geq V^{i,\dagger}(\bm{\pi}^{-i})-\varepsilon\) for all \(i\in[N]\)._

Since we consider finite-space sequential games, an equilibrium is guaranteed to exist [93].

**Remark 3** (Notion of equilibrium can be represented through information structure).: _The policy classes defined above (i.e., deterministic, independently-randomized, correlated randomized) can be directly modeled by the information structure. For example, to represent correlated randomized policies, the random seed \(\omega\in\Omega\) can be modeled as an observable variable at time \(t=0\) which is in all agents' information sets. Similarly, independently randomized policies can be represented through a different random seed for each agent at time \(t=0\), and including the appropriate random seed in each action's information set. Hence, the information structure itself can decide which equilibrium notion we are interested in. Moreover, this allows us to consider additional notions of equilibrium _where, for example, only subsets of agents can be correlated with each other (e.g., this may be useful in modeling multi-team problems). Note that adding random seeds in order to model randomized policies does not affect the information-structural state \(\mathcal{I}_{h}^{\dagger}\) since the seeds don't appear in \(\mathcal{G}^{\dagger}\). For concreteness, we focus on NE and CCE in our presentation._

Characterizing the Sample Complexity of Learning an Equilibrium via a Self-Play Algorithm as a function of the Information Structure

We now introduce a sample-efficient reinforcement learning algorithm for learning well-conditioned generalized predictive state representations in the _game_ setting with each agent having their own objective. In particular, since partially-observable sequential games with a \(\mathcal{I}^{\dagger}\)-weakly revealing information structure admit a well-conditioned generalized PSR representation, they can also be learned sample-efficiently by this algorithm.

The algorithm we propose is a _self-play_ algorithm for learning an _equilibrium_ of the dynamic game problem. That is, the algorithm specifies the policies of all agents during the learning phase, collecting the trajectory of observables at each episode to improve its estimate of the system dynamics. This can be thought of as a centralized agent playing against itself. We will propose an algorithm which can find a Nash equilibrium or coarse correlated equilibrium in a sample-efficient manner. We begin with some preliminaries.

**Game setting.** Recall that a sequential decision-making problem falls within the game setting if each agent has their own objective. Following Section 2.1, we consider a sequential decision-making problem \((X_{1},\ldots,X_{H})\) where \(\mathcal{O}\) denotes the index set of (observable) system variables and \(\mathcal{A}\) denotes the set of action variables. We suppose the game involves \(N\) agents, and denote the action index set of each agent by \(\mathcal{A}^{i}\subset\mathcal{A}\), where \(\{\mathcal{A}^{i}\}_{i\in[N]}\) partitions \(\mathcal{A}\). Each agent has their own reward function \(R^{i}(X_{1},\ldots,X_{H})\). Note that POSGs as defined in Definition 7 are structured models which fall within this framework.

**Equilibria and policy classes.** Recall that in the game setting, the type of randomization in each agent's policy affects the set of equilibria in the game. In Appendix F.1, we formalized this randomization by introducing a random seed \(\omega\in\Omega\) and allowing each agent's policy to be a function of their information set and this seed. If the seed has a product structure \(\omega=(\omega_{1},\ldots,\omega_{N})\) with each agent observing their own seed, this results in independently-randomized policies, denoted by \(\Gamma_{\mathrm{ind}}^{i}\). If all agents use the same seed, this results in correlated randomized policies, which we denote by \(\Gamma_{\mathrm{cor}}^{i}\). An equilibrium among independently randomized policies is called a Nash equilibrium and an equilibrium among correlated randomized policies is called a coarse correlated equilibrium.

**Estimating probabilities in the planner.** The probability of any trajectory under a joint policy \(\boldsymbol{\pi}\) is given by \(\mathbb{P}^{\boldsymbol{\pi}}(\tau_{H})=\sum_{\omega}\overline{\mathbb{P}} \left[\tau_{H}\right]\boldsymbol{\pi}(\tau_{H}|\omega)\mathbb{P}\left[\omega\right]\), where \(\overline{\mathbb{P}}\left[\tau_{H}\right]=\mathbb{P}\left[\tau_{H}^{o}\mid \tau_{H}^{a}\right]\) as before, and \(\boldsymbol{\pi}(\tau_{H}\,|\,\omega)=\prod_{h\in\mathcal{A}}\boldsymbol{1} \{x_{h}=\pi_{h}(\tau_{h-1},\omega)\}\). Recall that the probabilities \(\overline{\mathbb{P}}\left[\tau_{H}\right]\) are estimated by the generalized PSR model \(\widehat{\theta}\). We assume that the planner has knowledge of the randomization, \(\mathbb{P}\left[\omega\right]\). Hence, the planner in the self-play algorithm is able to compute the probability of any trajectory for each choice of policy.

**Algorithm.** The algorithmic description is presented in Algorithm 2. In the first stage of the algorithm, the centralized learning agent has a unified goal: to explore the environment. This is done by executing policies which maximize the bonus function \(\widehat{\theta}^{k}(\tau_{H})\) by visiting trajectories with imprecise estimates of their probability, as measured by the upper confidence bound on the total variation distance. This part is identical to Algorithm 1. Once the algorithm is sufficiently confident about the estimated probabilities of all trajectories, it computes the equilibrium using the estimated model directly. That is, \(\mathtt{ComputeEquilibrium}\) computes either NE or CCE. The only difference in the exploration stage of the algorithm compared to Algorithm 1 is that the termination condition involves \(\varepsilon/4\) rather than \(\varepsilon/2\) in order to guarantee an \(\varepsilon\)-approximate equilibrium under the added complications of the game setting.

**Theorem 5**.: _Suppose Assumption 1 holds. Suppose the parameters \(p_{\min},\lambda,\alpha,\beta\) are chosen as in Theorem 4. Then, with probability at least \(1-\delta\), Algorithm 2 returns a model \(\theta^{\epsilon}\) and a policy \(\pi\) which is an \(\varepsilon\)-approximate equilibrium (either NE or CCE). That is,_

\[V_{\theta^{\epsilon}}^{i}(\pi)\geq V_{\theta^{\epsilon}}^{i,\dagger}(\pi^{-i })-\varepsilon,\,\forall i\in[N].\]_In addition, the algorithm terminates with a sample complexity of,_

\[\tilde{O}\left(\left(r+\frac{Q_{A}^{2}H}{\gamma^{2}}\right)\cdot\frac{rdH^{3} \cdot\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}\cdot Q_{A}^{4}\beta }{\gamma^{4}\epsilon^{2}}\right).\]

Proof.: The proof is given in Appendix K. 

To apply this algorithm to a partially-observable sequential game, we can use the generalized PSR parameterization constructed in Section 4.

**Corollary 2**.: _Suppose a partially-observable sequential game is \(m\)-step \(\alpha\)-robustly \(\mathcal{I}^{\dagger}\)-weakly revealing as per Definition 6. Applying Algorithm 2 to this PSR representation, with parameters \(p_{\min},\lambda,\alpha,\beta\) chosen as in Theorem 5, returns a \(\varepsilon\)-approximate equilibrium \(\pi\) with a sample complexity of._

\[\tilde{O}\left(\left(1+\frac{Q_{A}^{2}H}{\alpha^{2}}\right)\cdot\frac{\max_{ h}\left|\mathbb{J}_{h}^{\dagger}\right|^{7}\cdot\max_{h}\left|\mathbb{Q}_{h}^{m} \right|\cdot H^{5}\cdot\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2} \cdot\max_{s\in\mathcal{U}}\left|\mathbb{X}_{s}\right|\cdot Q_{A}^{4}}{ \alpha^{4}\epsilon^{2}}\right).\]

``` for\(k\gets 1,\ldots,K\)do for\(h\gets 1,\ldots,H\)do Collect \(\tau_{H}^{k,h}=(\omega_{h-1}^{k,h},\tau_{h-1}^{k,h})\) using \(\nu(\pi^{k-1},\mathfrak{u}_{h-1}^{\texttt{exp}})\). \(\mathcal{D}_{h-1}^{k}\leftarrow\mathcal{D}_{h-1}^{k-1}\cup\left\{\left(\tau_{H }^{k,h},\nu\left(\pi^{k-1},\mathfrak{u}_{h-1}^{\texttt{exp}}\right)\right)\right\}\). end for \(\mathcal{D}^{k}=\left\{\mathcal{D}_{h}^{k}\right\}_{h=0}^{H-1}\)  Compute MLE \(\widehat{\theta}\in\mathcal{B}^{k}\), where \[\Theta_{\min}^{k}=\left\{\theta:\forall h,(\tau_{h},\pi)\in\mathcal{D}_{h}^{k},\mathbb{P}_{\theta}^{\pi}(\tau_{h})\geq p_{\min}\right\},\] \[\mathcal{B}^{k}=\left\{\theta\in\Theta_{\min}^{k}:\sum_{(\tau_{H}, \pi)\in\mathcal{D}^{k}}\log\mathbb{P}_{\theta}^{\pi}(\tau_{H})\geq\max_{ \theta^{\prime}\in\Theta_{\min}^{k}}\sum_{(\tau_{H},\pi)\in\mathcal{D}^{k}} \log\mathbb{P}_{\theta^{\prime}}^{\pi}(\tau_{H})-\beta\right\}.\] Define the bonus function, \(\widehat{b}^{k}(\tau_{H})=\min\left\{\alpha\sqrt{\sum_{h=0}^{H-1}\left\| \widehat{\widehat{\psi}}(\tau_{h})\right\|_{(\widehat{U}_{h}^{k})^{-1}}^{2}},1\right\}\), where \(\widehat{U}_{h}^{k}=\lambda I+\sum_{\tau_{h}\in\mathcal{D}_{h^{k}}}\widehat{ \widetilde{\psi}}^{k}(\tau_{h})\widehat{\widetilde{\psi}}^{k}(\tau_{h})^{\top}\).  Solve the planning problem \(\pi^{k}=\arg\max_{\pi}V_{\widehat{\theta}^{k}}^{\widehat{b}^{k}}(\pi)\). if\(V_{\widehat{\theta}^{k}}^{\widehat{b}^{k}}(\pi^{k})\leq\epsilon/4\)then \(\theta^{\epsilon}=\widehat{\theta}^{k}\). break.  end if end for return\(\pi=\texttt{ComputeEquilibrium}(\theta^{\epsilon},\left\{R^{1},\ldots,R^{N} \right\})\) ```

**Algorithm 2**Self-play UCB Algorithm for Sequential Games

## Appendix G Existence of Generalized PSR representations and their covering number

In this section we show that any rank-\(r\) sequential decision-making problem (as per Section 2.1) can be represented via a rank-\(r\) generalized PSR (Definition 3). Next, we bound the covering number of the class of rank \(r\) PSRs, which will be important for our MLE analysis. Similar results have been established in previous work for sequential decision-making problems with alternating observations and actions [e.g., 33]. Recall that our formulation of the generic sequential decision-making problem and generalized PSRs is more general than the standard formulation since it allows for an arbitrary sequence of variables. Here, we follow a similar procedure to prove a slightly generalized result.

**Proposition 2** (Existence of Generalized PSR representation).: _Consider a sequential decision-making problem with \(\operatorname{rank}(\bm{D}_{h})=r_{h},\,h\in 0:H-1\). There exists a generalized PSR representation (i.e, observable operator model) \(b_{0},\left\{B_{h}(x_{h})\right\}_{h\in[H],x_{h}\in\mathbb{X}_{h}},\left\{v_{h} \right\}_{h\in 0:H}\) such that,_

1. \(B_{h}(x_{h})\in\mathbb{R}^{r_{h}\times r_{h-1}}\) _and_ \(\left\|B_{h}(x_{h})\right\|_{2}\leq 1\) _for any_ \(x_{h}\)_._
2. \(|b_{0}|\leq\sqrt{|\mathbb{F}_{H}^{a}|}\)_._
3. \(\|v_{h}\|_{2}\leq\sqrt{\left|\mathbb{F}_{h}^{a}\right|/\left|\mathbb{F}_{h}^{a }\right|}\)_._
4. _For any_ \(h\)_,_ \(\frac{1}{\left|\mathbb{X}_{h}\right|^{2\left\|h\in\mathcal{A}\right\|}}v_{h}^ {\top}\sum_{x_{h}\in\mathbb{X}_{h}}B_{h}(x_{h})=v_{h-1}^{\top}\)_._
5. _For any_ \(\tau_{h}\in\mathbb{H}_{h}\)_,_ \(\mathbb{P}\left[\tau_{h}\right]=v_{h}^{\top}B_{h}(x_{h})\cdots B_{1}(x_{1})b_{ 0}\)_._

Proof.: We construct the representation via the singular value decomposition of the matrix \(\bm{D}_{h}^{\top}\). Let \(U_{h}\in\mathbb{R}^{\left|\mathbb{F}_{h}\right|\times r_{h}},\Sigma_{h}\in \mathbb{R}^{r_{h}\times r_{h}},V_{h}^{\top}\in\mathbb{R}^{r_{h}\times\left| \mathbb{H}_{h}\right|}\) be the SVD such that \(\bm{D}_{h}^{\top}=U_{h}\Sigma_{h}V_{h}^{\top}\). Define \(b_{0},B_{h},v_{h}^{\top}\) as follows,

\[b_{0}=\left\|\bm{D}_{0}\right\|_{2},\,\,\,B_{h}(x_{h})=U_{h}^{\top}\left[U_{h- 1}\right]_{(x_{h},\mathbb{F}_{h}),:},\,\,v_{h}^{\top}=\frac{1}{\left|\mathbb{F }_{h}^{a}\right|}\bm{1}^{\top}U_{h}.\]

Here, \(\left[U_{h-1}\right]_{(x_{h},\Omega_{h}),:}\) denotes an \(\left|\mathbb{F}_{h}\right|\) by \(r_{h-1}\) submatrix of \(U_{h-1}\) consisting of the rows \((x_{h},\omega_{h}),\,\omega_{h}\in\mathbb{F}_{h}\) (i.e., the set of futures where the variable at time \(h\) is \(x_{h}\)). Note that \(\left|\mathbb{F}_{H}^{a}\right|=1\) by convention, a product over an empty set. We verify each property in turn.

First, \(\left\|B_{h}(x_{h})\right\|_{2}=\left\|U_{h}^{\top}[U_{h-1}]_{(x_{h},\mathbb{F }_{h}),:}\right\|_{2}\leq 1\) since \(U_{h},U_{h-1}\) are unitary matrices. Second,

\[\left|b_{0}\right| =\left\|\bm{D}_{0}\right\|_{2}=\sqrt{\sum_{\tau_{H}}\overline{ \mathbb{P}}\left[\tau_{H}\right]^{2}}\] \[\leq\sqrt{\sum_{\tau_{H}}\overline{\mathbb{P}}\left[\tau_{H} \right]}=\sqrt{\sum_{\tau_{H}^{a}}\sum_{\tau_{H}^{a}}\mathbb{P}\left[\tau_{H}^ {a}\mid\tau_{H}^{a}\right]}=\sqrt{\sum_{\tau_{H}^{a}}1}=\sqrt{\prod_{s\in \mathcal{A}}\left|\mathbb{X}_{s}\right|},\]

where the inequality is since \(\overline{\mathbb{P}}\left[\tau_{H}\right]\in[0,1]\). For property 3, we have

\[\left\|v_{h}\right\|_{2} =\frac{1}{\left|\mathbb{F}_{h}^{a}\right|}\left\|\bm{1}^{\top}U_{ h}\right\|_{2}\] \[\leq\frac{1}{\left|\mathbb{F}_{h}^{a}\right|}\left\|\bm{1}\right\| _{2}=\frac{\sqrt{\left|\mathbb{F}_{h}\right|}}{\left|\mathbb{F}_{h}^{a}\right| }=\sqrt{\left|\mathbb{F}_{h}^{o}\right|/\left|\mathbb{F}_{h}^{a}\right|},\]

where the inequality is since \(U_{h}\) is unitary, and the final equality is since \(\left|\mathbb{F}_{h}\right|=\left|\mathbb{F}_{h}^{o}\right|\left|\mathbb{F}_{h }^{a}\right|\).

Next, to prove properties 4 and 5, we first show the following claim.

**Claim**.: _For any history \(\tau_{h}=(x_{1},\ldots,x_{h})\in\mathbb{H}_{h}\), \(h\in 0:H\), we have \(B_{h}(x_{h})\cdots B_{1}(x_{1})b_{0}=U_{h}^{\top}\left[\bm{D}_{h}^{\top}\right]_{ :,\tau_{h}}\)._

Proof of claim.: We prove the claim by induction. In the base case, \(h=0\), \(\bm{D}_{0}^{\top}\) is a vector in \(\mathbb{R}^{\mathbb{F}_{0}}\) (note that \(\mathbb{F}_{0}=\mathbb{H}_{H}\)). Hence, \(U_{0}\) is simply the normalized vector \(U_{0}=\bm{D}_{0}^{\top}/\|\bm{D}_{0}^{\top}\|_{2}\), and hence \(U_{0}^{\top}\bm{D}_{0}^{\top}=\bm{D}_{0}\bm{D}_{0}^{\top}/\|\bm{D}_{0}\|_{2}=\| \bm{D}_{0}\|_{2}=b_{0}\). Proceeding by induction, suppose the claim holds for \(h-1\). Then, we have,

\[B_{h}(x_{h})\cdots B_{1}(x_{1})b_{0} =B_{h}(x_{h})U_{h-1}^{\top}\left[\bm{D}_{h-1}^{\top}\right]_{:, \tau_{h-1}}\] \[=U_{h}^{\top}\left[U_{h-1}\right]_{(x_{h},\mathbb{F}_{h}),:}U_{h- 1}^{\top}\left[\bm{D}_{h-1}^{\top}\right]_{:,\tau_{h-1}}\] \[=U_{h}^{\top}\left[U_{h-1}U_{h-1}^{\top}\bm{D}_{h-1}^{\top}\right] _{(x_{h},\mathbb{F}_{h}),\tau_{h-1}}\] \[=U_{h}^{\top}\left[\bm{D}_{h-1}^{\top}\right]_{(x_{h},\mathbb{F}_{h }),\tau_{h-1}}\] \[=U_{h}^{\top}\left[\bm{D}_{h}^{\top}\right]_{:,\tau_{h}},\]where the final equality is because \(\left[\bm{D}_{h-1}^{\top}\right]_{(x_{h},\omega_{h}),\tau_{h-1}}=\overline{\mathbb{ P}}\left[\tau_{h-1},x_{h},\omega_{h}\right]=\overline{\mathbb{P}}\left[\tau_{h}, \omega_{h}\right]=\left[\bm{D}_{h}^{\top}\right]_{\omega_{h},\tau_{h}}\). 

Using this fact, we can now show property 5 as follows,

\[v_{h}^{\top}B_{h}(x_{h})\cdots B_{1}(x_{1})b_{0} =\frac{1}{|\mathbb{F}_{h}^{a}|}\mathbf{1}^{\top}U_{h}U_{h}^{\top }\left[\bm{D}_{h}^{\top}\right]_{:,\tau_{h}}=\frac{1}{|\mathbb{F}_{h}^{a}|} \mathbf{1}^{\top}\left[\bm{D}_{h}^{\top}\right]_{:,\tau_{h}}\] \[=\frac{1}{|\mathbb{F}_{h}^{a}|}\sum_{\begin{subarray}{c}\omega_{ h}\in\mathbb{F}_{h}\end{subarray}}\overline{\mathbb{P}}\left[\tau_{h}, \omega_{h}\right]=\frac{1}{|\mathbb{F}_{h}^{a}|}\sum_{\begin{subarray}{c} \omega_{h}^{\prime}\in\mathbb{F}_{h}^{a}\end{subarray}}\sum_{\begin{subarray} {c}\omega_{h}^{\prime}\in\mathbb{F}_{h}^{a}\end{subarray}}\mathbb{P}\left[ \tau_{h}^{o},\omega_{h}^{o}\mid\tau_{h}^{a},\omega_{h}^{a}\right]\] \[=\frac{1}{|\mathbb{F}_{h}^{a}|}\mathbb{P}\left[\tau_{h}^{o}\mid \tau_{h}^{a}\right]\sum_{\begin{subarray}{c}\omega_{h}^{\prime}\in\mathbb{F} _{h}^{a}\end{subarray}}\sum_{\begin{subarray}{c}\omega_{h}^{\prime}\in \mathbb{F}_{h}^{a}\end{subarray}}\mathbb{P}\left[\omega_{h}^{o}\mid\omega_{h}^ {a},\tau_{h}^{a},\tau_{h}^{o}\right]\] \[=\frac{1}{|\mathbb{F}_{h}^{a}|}\mathbb{P}\left[\tau_{h}^{o}\mid \tau_{h}^{a}\right]\sum_{\begin{subarray}{c}\omega_{h}^{\prime}\in\mathbb{F} _{h}^{a}\end{subarray}}\sum_{\begin{subarray}{c}\omega_{h}^{\prime}\in \mathbb{F}_{h}^{a}\end{subarray}}\mathbb{P}\left[\omega_{h}^{o}\mid\omega_{h}^ {a},\tau_{h}^{a},\tau_{h}^{o}\right]\] \[=\frac{1}{|\mathbb{F}_{h}^{a}|}\mathbb{P}\left[\tau_{h}^{o}\mid \tau_{h}^{a}\right]\sum_{\begin{subarray}{c}\omega_{h}^{\prime}\in\mathbb{F} _{h}^{a}\end{subarray}}\sum_{\begin{subarray}{c}\omega_{h}^{\prime}\in \mathbb{F}_{h}^{a}\end{subarray}}1\] \[=\mathbb{P}\left[\tau_{h}^{o}\mid\tau_{h}^{a}\right].\]

Finally, it remains to show property 4. Consider the linear equation \(x^{\top}U_{h}^{\top}\bm{D}_{h}^{\top}=|\mathbb{F}_{h}^{a}|^{-1}\mathbf{1}^{ \top}\bm{D}_{h}^{\top}\). Note that \(U_{h}^{\top}\bm{D}_{h}^{\top}\in\mathbb{R}^{r_{h}\times|\mathbb{H}_{h}|}\) is rank \(r_{h}\). Thus, this equation has a unique solution. Our strategy is to show that \(v_{h}^{\top}\) and \(v_{h+1}^{\top}\sum_{x_{h+1}}B_{h+1}(x_{h})\) are both solutions to this linear equation, and hence \(v_{h}^{\top}=v_{h+1}^{\top}\sum_{x_{h+1}}B_{h+1}(x_{h})\). That \(v_{h}^{\top}\) is a solution is clear by definition of \(v_{h}\), \(v_{h}^{\top}U_{h}^{\top}\bm{D}_{h}^{\top}=|\mathbb{F}_{h}^{a}|^{-1}\mathbf{1}^{ \top}U_{h}U_{h}^{\top}\bm{D}_{h}^{\top}=|\mathbb{F}_{h}^{a}|^{-1}\mathbf{1}^{ \top}\bm{D}_{h}^{\top}\). First, recall by the calculation above that \(|\mathbb{F}_{h}^{a}|^{-1}\mathbf{1}^{\top}\bm{D}_{h}^{\top}\) is a vector in \(\mathbb{R}^{\mathbb{H}_{h}}\) where the \(\tau_{h}\)-th entry is \(\mathbb{P}\left[\tau_{h}^{o}\mid\tau_{h}^{a}\right]\). We will calculate the \(\tau_{h}\)-th entry of the vector \(x^{\top}U_{h}^{\top}\bm{D}_{h}\) when \(x^{\top}=v_{h+1}^{\top}\sum_{x_{h+1}}B_{h+1}(x_{h+1})\),

\[\left(v_{h+1}^{\top}\sum_{x_{h+1}}B_{h+1}(x_{h+1})\right)\left[U_{ h}^{\top}D_{h}^{\top}\right]_{:,\tau_{h}} =\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|}\sum_{x_{h+1}}\mathbf{ 1}^{\top}U_{h+1}U_{h+1}^{\top}\left[U_{h}\right]_{(x_{h+1},\mathbb{F}_{h+1}); }\left[U_{h}^{\top}D_{h}^{\top}\right]_{:,\tau_{h}}\] \[=\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|}\sum_{x_{h+1}}\mathbf{ 1}^{\top}U_{h+1}U_{h+1}^{\top}\left[U_{h}U_{h}^{\top}D_{h}^{\top}\right]_{(x_{h +1},\mathbb{F}_{h+1}),\tau_{h}}\] \[=\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|}\sum_{x_{h+1}}\left[ \mathbf{1}^{\top}D_{h}^{\top}\right]_{(x_{h+1},\mathbb{F}_{h+1}),\tau_{h}}\] \[=\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|}\sum_{x_{h+1}}\sum_{ \begin{subarray}{c}\omega_{h+1}\end{subarray}}\overline{\mathbb{P}}\left[\tau_ {h},x_{h+1},\omega_{h+1}\right]=\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|} \sum_{\begin{subarray}{c}\omega_{h}\in\mathbb{F}_{h}\end{subarray}}\overline{ \mathbb{P}}\left[\tau_{h},\omega_{h}\right]\] \[=\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|}\mathbb{P}\left[ \tau_{h}^{a}\mid\tau_{h}^{a}\right]\sum_{\begin{subarray}{c}\omega_{h}^{\prime} \in\mathbb{F}_{h}^{a}\end{subarray}}\sum_{\begin{subarray}{c}\tau_{h}^{\prime} \in\mathbb{F}_{h}^{a}\end{subarray}}\mathbb{P}\left[\omega_{h}^{o}\mid\tau_{h}, \omega_{h}^{a}\right]\] \[=\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|}\mathbb{P}\left[\tau_ {h}^{o}\mid\tau_{h}^{a}\right]\] \[=\frac{1}{\left|\mathbb{F}_{h+1}^{a}\right|}\overline{\mathbb{P}} \left[\tau_{h}^{o}\mid\tau_{h}^{a}\right]\sum_{\begin{subarray}{c}\omega_{h}^{ \prime}\in\mathbb{F}_{h}^{a}\end{subarray}}1=\frac{1}{\left|\mathbb{F}_{h+1}^{a }\right|}\left|\mathbb{F}_{h}^{a}\right|\mathbb{P}\left[\tau_{h}^{o}\mid\tau_{h}^{ a}\right]\] \[=\frac{1}{\left|\mathbb{X}_{h+1}\right|^{\mathbf{1}\left(h+1\in \mathcal{A}\right)}\mathbb{P}\left[\tau_{h}^{o}\mid\tau_{h}^{a}\right],}\]

where the final inequality is since \(|\mathbb{F}_{h}^{a}|=\prod_{s\in h+1:H}(|\mathbb{X}_{s}|^{\mathbf{1}\{s\in \mathcal{A}\}})\). 

**Corollary 3**.: _Consider a sequential decision-making problem with \(\operatorname{rank}(D_{h})\leq r\). Then, there exists a generalized PSR \(b_{0}\in\mathbb{R}^{r}\), \(\left\{B_{h}(x_{h})\right\}_{h\in[H],x_{h}\in\mathbb{X}_{h}}\subset\mathbb{R}^{r \times r}\), \(v_{H}\in\mathbb{R}^{r}\) such that,_

1. \(\left\|B_{h}(x_{h})\right\|_{2}\leq 1,\,\forall h,x_{h}\in\mathbb{X}_{h}\)_,_ \(\left\|b_{0}\right\|_{2}\leq\sqrt{|\mathbb{H}_{H}^{a}|}\)_, and_ \(\left\|v_{H}\rightProof.: In Proposition 2 we constructed such a representation with dimensions in terms of \(r_{h}\) instead of \(r\). Since \(r_{h}\leq r\), we can pad this representation with dummy columns and/or rows filled with zeros to obtain a representation with dimensions in terms of \(r\). 

An important part of maximum likelihood analysis is the notion of a "bracketing number" which controls the complexity of the model class \(\Theta\)[e.g., 94]. In our analysis, the model class is the set of generalized PSRs of a given rank. As shown in the results above, rank-\(r\) generalized PSRs can represent any rank-\(r\) sequential decision-making problem, with operators whose norm is bounded. In the next result, we will consider a closely related notion to the bracketing number which crucially incorporates optimism. \(\overline{\Theta}_{\varepsilon}\) is said to be an "optimistic \(\varepsilon\)-cover" for \(\Theta\) if for each \(\theta\in\Theta\), there exists \(\widehat{\theta}\in\overline{\Theta}_{\varepsilon}\) with an associated probability measure \(\overline{\mathbb{P}}_{\widehat{\theta}}^{\varepsilon}\) such that,

\[\forall h,\tau_{h},\;\mathbb{P}_{\widehat{\theta}}^{\varepsilon}( \tau_{h})\geq\overline{\mathbb{P}}_{\theta}\left[\tau_{h}\right],\] \[\forall h,\tau_{h},\;\sum_{\tau_{h}}\left|\overline{\mathbb{P}}_ {\widehat{\theta}}^{\varepsilon}(\tau_{h})-\overline{\mathbb{P}}_{\theta} \left[\tau_{h}\right]\right|\leq\varepsilon.\]

The first condition ensures optimism and the second condition ensures that \(\overline{\Theta}_{\varepsilon}\)\(\varepsilon\)-covers \(\Theta\), in the sense that the probability of any trajectory is approximated within an error \(\varepsilon\). Recall that the parameter \(\beta\) in Algorithms 1 and 2, which appears in the sample complexity results in Theorems 4 and 5, is defined in terms of \(\left|\overline{\Theta}_{\varepsilon}\right|\). The next proposition bounds the size of \(\left|\overline{\Theta}_{\varepsilon}\right|\).

**Proposition 3** (Optimistic cover of sequential decision making problems).: _Let \(\mathfrak{M}\) be the set of all rank-\(r\) sequential decision-making problems with a horizon of length \(H\), observation index set \(\mathcal{O}\subset[H]\), action index set \(\mathcal{A}\subset[H]\), and variable spaces \(\mathbb{X}_{1},\ldots,\mathbb{X}_{H}\). Then, there exists an optimistic \(\varepsilon\)-cover \(\overline{\Theta}_{\varepsilon}\) of \(\Theta\) with cardinality bounded by,_

\[\log\left|\overline{\Theta}_{\varepsilon}\right|\leq O\left(r^{2}\max_{h} \left|\mathbb{X}_{h}\right|H^{2}\log\left(\frac{\max_{h}\left|\mathbb{X}_{h} \right|}{\epsilon}\right)\right).\]

Proof.: Define the set of generalized PSR representations constructed in Corollary 3,

\[\Theta\coloneqq\left\{b_{0}\in\mathbb{R}^{r},\left\{B_{h}(x_{h}) \right\}_{h,x_{h}},v_{H}\in\mathbb{R}^{r}: \left\|B_{h}(x_{h})\right\|_{2}\leq 1,\,\forall h,x_{h},\,\left\|b_{0} \right\|_{2}\leq\sqrt{\left|\mathbb{H}_{H}^{a}\right|},\,\left\|v_{H}\right\| _{2}\leq 1,\] \[\text{and}\;\forall\,\tau_{H}\in\mathbb{H}_{H},\,\overline{ \mathbb{P}}_{m}\left[\tau_{H}\right]=v_{H}^{\top}B_{H}(x_{H})\cdots B_{1}(x_{1} )b_{0},\] \[\text{where}\;m\;\text{is a sequential decision making problem in}\;\mathfrak{M}\right\}.\]

Let \(\mathcal{C}_{\delta}\) be a \(\delta\)-cover of the above set with respect to the \(\ell_{\infty}\)-norm. For \(\widehat{\theta}=\left(b_{0},\left\{B_{h}(x_{h})\right\},v_{H}\right)\in \mathcal{C}_{\delta}\), define the \(\varepsilon\)-optimistic probabilities as,

\[\overline{\mathbb{P}}_{\widehat{\theta}}^{\varepsilon}(\tau_{H})\coloneqq v_{H}^{\top}B_{H}(x_{h})\cdots B _{1}(x_{1})b_{0}+\varepsilon/2\]

We will show that for an appropriate choice of \(\delta\), \(\mathcal{C}_{\delta}\) is an optimistic \(\varepsilon\)-cover. In particular, for each \(\theta\in\Theta\), there exists \(\widehat{\theta}\in\mathcal{C}_{\delta}\) such that,

\[\forall h,\tau_{h},\;\overline{\mathbb{P}}_{\widehat{\theta}}^{ \varepsilon}(\tau_{h})\geq\overline{\mathbb{P}}_{\theta}\left[\tau_{h}\right],\] \[\forall h,\tau_{h},\;\sum_{\tau_{h}}\left|\overline{\mathbb{P}}_{ \widehat{\theta}}^{\varepsilon}(\tau_{h})-\overline{\mathbb{P}}_{\theta} \left[\tau_{h}\right]\right|\leq\varepsilon.\]To choose the value of \(\delta\) for which the above holds, observe that

\[\sum_{\tau_{H}}\left|\widehat{v}_{H}^{\top}\widehat{B}_{H}(x_{H}) \cdots B_{1}(x_{1})\widehat{b}_{0}-v_{H}^{\top}B_{H}(x_{H})\cdots B_{1}(x_{1}) b_{0}\right|\] \[\leq\sum_{h=1}^{H}\sum_{\tau_{H}}\left|\widehat{v}_{H}^{\top} \widehat{B}_{H}(x_{H})\cdots\widehat{B}_{h+1}(x_{h+1})(\widehat{B}_{h}(x_{h}) -B_{h}(x_{h}))B_{h-1}(x_{h-1})\cdots B_{1}(x_{1})b_{0}\right|\] \[\quad+\sum_{\tau_{H}}\left|\widehat{v}_{H}^{\top}B_{H}(x_{H}) \cdots B_{1}(x_{1})(\widehat{b}_{0}-b_{0})\right|\] \[\leq\sum_{h}\sum_{\tau_{H}}r\left\|\widehat{B}_{h}(x_{h})-B_{h}( x_{h})\right\|_{\max}\sqrt{\left|\mathbb{H}_{H}^{a}\right|}+\sum_{\tau_{H}} \sqrt{r}\left\|\widehat{b}_{0}-b_{0}\right\|_{\infty}\] \[\leq H\max_{h}\left|\mathbb{X}_{h}\right|^{H+|\mathcal{A}|/2}r \delta+\max_{h}\left|\mathbb{X}_{h}\right|^{H}\sqrt{r}\delta,\]

where the second inequality uses \(\|\widehat{v}_{H}\|_{2}=\|v_{H}\|_{2}=\|B_{h}(x_{h})\|_{2}=1\), \(\|\widehat{B}_{h}(x_{h})-B_{h}(x_{h})\|_{2}\leq r\|\widehat{B}_{h}(x_{h})-B_{h }(x_{h})\|_{\max}\leq r\delta,\|b_{0}\|_{2}\leq\sqrt{\left|\mathbb{H}_{H}^{a} \right|}\), and \(\|\widehat{b}_{0}-b_{0}\|_{2}\leq\sqrt{r}\|\widehat{b}_{0}-b_{0}\|_{\infty}\leq \sqrt{r}\delta\). Hence, choosing \(\delta\coloneqq\varepsilon\cdot\max_{h}\left|\mathbb{X}_{h}\right|^{-cH}\) for \(c\) an absolute constant large enough achieves a \(\varepsilon\)-optimistic covering of \(\Theta\). Hence, we let \(\overline{\Theta}_{\varepsilon}=\mathcal{C}_{\delta}\), with \(\delta=\varepsilon\cdot\max_{h}\cdot\left|\mathbb{X}_{h}\right|^{-cH}\). It remains to bound the size of \(|\overline{\Theta}_{\varepsilon}|\).

Recall that \(\left\|\cdot\right\|_{\infty}\leq\left\|\cdot\right\|_{2}\) and that an interval \([-x,x]\) in \(\mathbb{R}\) admits a \(\delta\)-cover of size bounded by \(2x/\delta\). Now, observe that \(\max_{ij}|[B_{h}(x_{h})]_{ij}|\leq\|B_{h}(x_{h})\|_{2}\leq 1\). Hence, for a fixed \(h\), \(\{B_{h}(x_{h})\}_{x_{h}}\) admits a cover of size bounded by \((2/\delta)^{r^{2}\left|\mathbb{X}_{h}\right|}\). Considering all \(h\), the cover is bounded by \((2/\delta)^{r^{2}\sum_{h}\left|\mathbb{X}_{h}\right|}\leq(2/\delta)^{r^{2}\max _{h}\left|\mathbb{X}_{h}\right|H}\). For, \(b_{0}\), we have \(\left\|b_{0}\right\|_{\infty}\leq\|b_{0}\|_{2}\leq\sqrt{\left|\mathbb{H}_{H}^{a }\right|}\), hence the covering number is bounded by \((2\sqrt{\left|\mathbb{H}_{H}^{a}\right|}/\delta)^{r}\). Finally for \(v_{H}\), we have \(\|v_{H}\|_{\infty}\leq\|v_{H}\|_{2}\leq 1\), hence the covering number is bounded by \((2/\delta)^{r}\). Thus, we have,

\[\log\left|\overline{\Theta}_{\varepsilon}\right|\leq O\left(r^{2}\max_{h} \left|\mathbb{X}_{h}\right|H\log\left(\frac{1}{\delta}\right)\right).\]

Recalling that \(\delta=\varepsilon\max_{s}\left|\mathbb{X}_{s}\right|^{-cH}\), we obtain that,

\[\log\left|\overline{\Theta}_{\varepsilon}\right|\leq O\left(r^{2}\max_{h} \left|\mathbb{X}_{h}\right|H^{2}\log\left(\frac{\max_{h}\left|\mathbb{X}_{h} \right|}{\epsilon}\right)\right).\]

## Appendix H Proofs of Section 3.2

**Theorem** (Restatement of Theorem 1).: _The rank of the observable system dynamics of a POST or POSG is bounded by_

\[r\leq\max_{h\in[H]}\left|\mathbb{I}_{h}^{\dagger}\right|\]

Proof.: We have

\[\left[\bm{D}_{h}\right]_{\tau_{h},\omega_{h}} =\mathbb{P}\left[\tau_{h}^{o},\omega_{h}^{o}\mid\operatorname{do} (\tau_{h}^{a},\tau_{h}^{a})\right]\] \[=\mathbb{P}\left[\tau_{h}^{o}\mid\operatorname{do}(\tau_{h}^{a}) \right]\mathbb{P}\left[\omega_{h}^{o}\mid\tau_{h}^{o};\operatorname{do}(\tau_{h} ^{a},\omega_{h}^{a})\right]\] \[\overset{(a)}{=}\mathbb{P}\left[\tau_{h}^{o}\mid\operatorname{do} (\tau_{h}^{a})\right]\sum_{\begin{subarray}{c}x_{k}\in\mathbb{X}_{k}\\ k\in\mathcal{I}_{h}^{\dagger}\end{subarray}}\mathbb{P}\left[\left\{x_{k},\,k \in\mathcal{I}_{h}^{\dagger}\right\}\Bigm{|}\tau_{h}^{o};\operatorname{do}(\tau_ {h}^{a},\omega_{h}^{a})\right]\mathbb{P}\left[\omega_{h}^{o}\mid\left\{x_{k},\,k \in\mathcal{I}_{h}^{\dagger}\right\},\tau_{h}^{o};\operatorname{do}(\tau_{h}^{a},\omega_{h}^{a})\right]\] \[\overset{(b)}{=}\sum_{\begin{subarray}{c}x_{k}\in\mathbb{X}_{k}\\ k\in\mathcal{I}_{h}^{\dagger}\end{subarray}}\mathbb{P}\left[\tau_{h}^{o}\mid \operatorname{do}(\tau_{h}^{a})\right]\mathbb{P}\left[\left\{x_{k},\,k\in \mathcal{I}_{h}^{\dagger}\right\}\Bigm{|}\tau_{h}^{o};\operatorname{do}(\tau_{h}^{a} )\right]\mathbb{P}\left[\omega_{h}^{o}\mid\left\{x_{k},\,k\in\mathcal{I}_{h}^{ \dagger}\right\},\tau_{h}^{o};\operatorname{do}(\tau_{h}^{a},\omega_{h}^{a})\right]\] \[\overset{(c)}{=}\sum_{\begin{subarray}{c}x_{k}\in\mathbb{X}_{k}\\ k\in\mathcal{I}_{h}^{\dagger}\end{subarray}}\mathbb{P}\left[\tau_{h}^{o}\mid \operatorname{do}(\tau_{h}^{a})\right]\mathbb{P}\left[\left\{x_{k},\,k\in \mathcal{I}_{h}^{\dagger}\right\}\Bigm{|}\tau_{h}^{o};\operatorname{do}(\tau_{h}^{a} )\right]\mathbb{P}\left[\omega_{h}^{o}\mid\left\{x_{k},\,k\in\mathcal{I}_{h}^{ \dagger}\right\};\operatorname{do}(\omega_{h}^{a})\right],\]where step (a) is simply the law of total probability, step (b) is that \(\{x_{k},k\in\mathcal{I}_{h}^{\dagger}\}\) is conditionally independent of \(\operatorname{do}(\omega_{h}^{a})\) (future actions) given \((\tau_{h}^{o};\,\operatorname{do}(\tau_{h}^{a}))\) (the past), and step (c) is that \(\omega_{h}^{o}\) is conditionally independent of \((\tau_{h}^{o};\,\operatorname{do}(\tau_{h}^{a}))\) given \(\{x_{k},\,k\in\mathcal{I}_{h}^{\dagger}\}\). This is due to a result by Verma and Pearl [95] which states: for three sets of variables \(A,B,C\) in a directed graphical model, if \(A\) and \(B\) are \(d\)-separated by \(C\), then \(A\perp B\,|\,C\). Recall that \(\mathcal{I}_{h}^{\dagger}\) is defined as the minimal set which \(d\)-separates \((X_{t(1)},\ldots,X_{t(h)})\) from \((X_{t(h+1)},\ldots,X_{t(H)})\).

As a technical remark, note that \(i_{h}^{\dagger}=(x_{k},k\in\mathcal{I}_{h}^{\dagger})\) may include actions and hence,

\[\mathbb{P}\left[\left\{x_{k},\,k\in\mathcal{I}_{h}^{\dagger}\right\}\,\Big{|} \,\tau_{h}^{o};\,\operatorname{do}\left(\tau_{h}^{a}\right)\right]=\mathbb{P} \left[\left\{x_{k},\,k\in\mathcal{I}_{h}^{\dagger}\cap\mathcal{S}\right\}\, \Big{|}\,\tau_{h}^{o};\,\operatorname{do}\left(\tau_{h}^{a}\right)\right] \mathbf{1}\left\{\left(x_{k},\,k\in\mathcal{I}_{h}^{\dagger}\cap\mathcal{A} \right)\text{ matches }\tau_{h}^{a}\right\},\]

since the action components of \(i_{h}^{\dagger}\) are contained in the history \(\tau_{h}\).

Now define two matrices

\[\bm{D}_{h,1} \coloneqq\left[\mathbb{P}\left[\tau_{h}^{o}\mid\operatorname{do} \left(\tau_{h}^{a}\right)\right]\mathbb{P}\left[\left\{x_{k},\,k\in\mathcal{I }_{h}^{\dagger}\right\}\,\Big{|}\,\tau_{h}^{o};\,\operatorname{do}\left(\tau_ {h}^{a}\right)\right]\right]_{\tau_{h},i_{h}^{\dagger}},\quad\tau_{h}\in\mathbb{ H}_{h},\,i_{h}^{\dagger}\equiv\left(x_{k},\,k\in\mathcal{I}_{h}^{\dagger} \right)\in\mathbb{I}_{h}^{\dagger},\] \[\bm{D}_{h,2} \coloneqq\left[\mathbb{P}\left[\omega_{h}^{o}\,\,\Big{|}\,\left\{ x_{k},\,k\in\mathcal{I}_{h}^{\dagger}\right\};\,\operatorname{do}\left(\omega_ {h}^{a}\right)\right]\right]_{i_{h}^{\dagger},\omega_{h}},\quad i_{h}^{\dagger }\equiv\left(x_{k},\,k\in\mathcal{I}_{h}^{\dagger}\right)\in\mathbb{I}_{h}^{ \dagger},\,\omega_{h}\in\mathbb{F}_{h}.\]

We have that \(\bm{D}_{h}=\bm{D}_{h,1}\bm{D}_{h,2}\), where both \(\bm{D}_{h,1}\) and \(\bm{D}_{h,2}\) have rank upper bounded by \(|\mathbb{I}_{h}^{\dagger}|=\prod_{s\in\mathcal{I}_{h}^{\dagger}}|\mathbb{X}_{s}|\). Hence, \(\operatorname{rank}(\bm{D}_{h})\leq|\mathbb{I}_{h}^{\dagger}|\), and the result follows. 

## Appendix I Proofs of Section 4

**Lemma** (Restatement of Lemma 1).: _Suppose that the POST/POSG is \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing. Then, \(\mathbb{Q}_{h}^{m}\) is a core test set for all \(h\in[H]\). Furthermore, we have_

\[\mathbb{P}\left[\tau_{h},\omega_{h}\right]=\left\langle m_{h}(\omega_{h}), \psi_{h}(\tau_{h})\right\rangle,\text{ and }\,\,\mathbb{P}\left[\omega_{h}\mid\tau_{h}\right]=\left\langle m_{h}(\omega_{h}), \overline{\psi}_{h}(\tau_{h})\right\rangle.\] (21)

Proof.: Let \(\tau_{h}\in\mathbb{H}_{h},\,\omega_{h}\in\mathbb{F}_{h}\) be any history and future, respectively. By Theorem 1, recall that we have

\[\overline{\mathbb{P}}[\omega_{h}\,|\,\tau_{h}]=\sum_{i_{h}^{\dagger}\in\mathbb{ I}_{h}^{\dagger}}\overline{\mathbb{P}}\left[\omega_{h}\,\,\Big{|}\,i_{h}^{ \dagger}\right]\mathbb{P}\left[i_{h}^{\dagger}\,\,\Big{|}\,\tau_{h}\right].\] (22)

Recall that \(i_{h}^{\dagger}\) may overlap with \(\tau_{h}\). In particular, the action component of \(i_{h}^{\dagger}\) is contained in \(\tau_{h}\). Thus, \(\mathbb{P}[i_{h}^{\dagger}\,\,|\,\tau_{h}]=\mathbb{P}[\{x_{k},\,k\in\mathcal{I }_{h}^{\dagger}\setminus\mathcal{U}_{1:h}\}\,|\,\tau_{h}]\cdot\mathbf{1}\{(x_{k },\,k\in\mathcal{I}_{h}^{\dagger}\cap\mathcal{U}_{1:h})\text{ matches }\tau_{h}\}\). Note that \(\mathcal{I}_{h}^{\dagger}\setminus\mathcal{U}_{1:h}\subset\mathcal{S}\) does not contain any actions. Hence, the summation over \(\mathbb{I}_{h}^{\dagger}\) is equivalent to summing over its unobservable components with the restriction that its observable components match \(\tau_{h}\).

Define the mappings \(\tilde{m}_{h}\colon\mathbb{F}_{h}\to\mathbb{R}^{\left|\mathbb{I}_{h}^{\dagger} \right|}\) and \(p_{h}\colon\mathbb{H}_{h}\to\mathbb{R}^{\left|\mathbb{I}_{h}^{\dagger}\right|}\) by

\[\tilde{m}_{h}(\omega_{h})=\left[\overline{\mathbb{P}}\left[\omega_{h}\,\,\Big{|} \,i_{h}^{\dagger}\right]\right]_{i_{h}^{\dagger}\in\mathbb{I}_{h}^{\dagger}}, \quad p_{h}(\tau_{h})=\left[\mathbb{P}\left[i_{h}^{\dagger}\,\,\Big{|}\,\tau_{h} \right]\right]_{i_{h}^{\dagger}\in\mathbb{I}_{h}^{\dagger}}.\]

Then, we have that the conditional probability of the future \(\omega_{h}\) given the past \(\tau_{h}\) is given by the inner product of the above mappings, \(\overline{\mathbb{P}}\left[\omega_{h}\,\,|\,\,\tau_{h}\right]=\left\langle\tilde{m }_{h}(\omega_{h}),p_{h}(\tau_{h})\right\rangle\). Recall that the vector of (conditional) core test set probabilities for the history \(\tau_{h}\) is given by \(\overline{\psi}_{h}(\tau_{h})=\left[\mathbb{P}\left[q^{o}\mid\tau_{h}^{o};\, \operatorname{do}(\tau_{h}^{a}),\,\operatorname{do}(q^{a})\right]\right]_{q\in \mathbb{Q}_{h}^{m}}\in\mathbb{R}^{\left|\mathbb{Q}_{h}^{m}\right|}\). By the definition of \(\bm{G}_{h}\) and Equation (22), we have \(\bm{G}_{h}\,p_{h}(\tau_{h})=\psi_{h}(\tau_{h})\), since, for \(q\in\mathbb{Q}_{h}^{m}\),

\[\left(\bm{G}_{h}\,p_{h}(\tau_{h})\right)_{q} =\sum_{i_{h}^{\dagger}}\left(\bm{G}_{h}\right)_{q,i_{h}^{ \dagger}}\left(p_{h}(\tau_{h})\right)_{i_{h}^{\dagger}}\] \[=\sum_{i_{h}^{\dagger}}\mathbb{P}\left[q^{o}\,\,\Big{|}\,\,i_{h}^{ \dagger};\,\operatorname{do}(q^{a})\right]\mathbb{P}\left[i_{h}^{\dagger}\, \,\Big{|}\,\tau_{h}\right]\] \[=\mathbb{P}\left[q^{o}\mid\tau_{h}^{o};\,\operatorname{do}(\tau_ {h}^{a}),\operatorname{do}(q^{a})\right]\] \[=:\left[\overline{\psi}_{h}(\tau_{h})\right]_{q}\]Since by assumption \(\operatorname{rank}(\bm{G}_{h})=\left|\mathbb{I}_{h}^{\dagger}\right|\), its pseudo-inverse \(\bm{G}_{h}^{\dagger}\) is a left inverse of \(\bm{G}_{h}\) (i.e., \(\bm{G}_{h}^{\dagger}\bm{G}_{h}=I\)). Hence, multiplying on the left by \(\bm{G}_{h}^{\dagger}\), we obtain \(p_{h}(\tau_{h})=\bm{G}_{h}^{\dagger}\overline{\psi}_{h}(\tau_{h})\). Hence,

\[\overline{\mathbb{P}}\left[\omega_{h}\mid\tau_{h}\right] =\left\langle\tilde{m}_{h}(\omega_{h}),\bm{G}_{h}^{\dagger} \overline{\psi}_{h}(\tau_{h})\right\rangle\] \[=\left\langle\underbrace{\left(\bm{G}_{h}^{\dagger}\right)^{ \top}\tilde{m}_{h}(\omega_{h})}_{m_{h}(\omega_{h})},\overline{\psi}_{h}(\tau _{h})\right\rangle.\]

That \(\overline{\mathbb{P}}\left[\tau_{h},\omega_{h}\right]=\left\langle m_{h}( \omega_{h}),\psi_{h}(\tau_{h})\right\rangle\) follows directly by noting the definition of \(\overline{\psi}_{h}(\tau_{h}):=\psi_{h}(\tau_{h})/\overline{\mathbb{P}}\left[ \tau_{h}\right]\).

Hence, we have shown that for the test set \(\mathbb{Q}_{h}^{m}\), the probability of each future \(\omega_{h}\) given a history \(\tau_{h}\) is a linear combination of the probabilities of each test in the core test set with weights \(m_{h}(\omega_{h})\coloneqq(\bm{G}_{h}^{\dagger})^{\top}\tilde{m}_{h}(\omega_{ h})\in\mathbb{R}^{|\mathbb{Q}_{h}^{m}|}\) depending only on the future and not the history. 

**Theorem** (Restatement of Theorem 2).: _Suppose a POST/POSG is \(\alpha\)-robustly \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing. Then, the corresponding generalized PSR as constructed in Section 4 is \(\gamma\)-well-conditioned with \(\gamma=\alpha/\max_{h}\lvert\mathbb{I}_{h}^{\dagger}\rvert^{1/2}\)._

We will first show that \((\{\mathbb{Q}_{h}^{m}\}_{h},\phi_{H},\{M_{h}\}_{h},\psi_{0})\) indeed forms a PSR through a series of simple calculations.

A direct corollary of Lemma 1 is the following.

**Lemma 2**.: _For any \(h\in[H],\,\tau_{h-1}\in\mathbb{H}_{h-1},\,x_{t(h)}\in\mathbb{X}_{t(h)},\, \omega_{h}\in\mathbb{F}_{h}\), we have_

\[\overline{\mathbb{P}}\left[\tau_{h-1},x_{t(h)},\omega_{h}\right]=\left\langle m _{h-1}(x_{t(h)},\omega_{h}),\psi_{h}(\tau_{h-1})\right\rangle.\] (23)

Hence, given a history \(\tau_{h-1}=(x_{t(1)},\ldots,x_{t(h-1)})\), having observed another variable \(x_{t(h)}\), we can update our predictions of the future and obtain the probability of trajectories of the form \((\tau_{h-1},x_{t(h)},\omega_{h})\) for any future \(\omega_{h}\in\mathbb{F}_{h}\). Note that \(x_{t(h)}\) may be either an observation or an action. Hence, we can update our prediction of the future after deciding an action, and before receiving the next observation. This is in contrast to the standard PSR formulation where predictions of the future can only be updated with a _pair_ of observation and action. Our formulation provides additional flexibility, which is crucial for the general information structures modeled by POSTs and POSGs.

This means that, after observing \(x_{t(h)}\), we can use the \(m_{h}:\mathbb{F}_{h}\to\mathbb{R}^{d_{h}}\) mapping constructed in Lemma 1 to update the probability of any candidate future \(\omega_{h}\). We are particularly interested in updating the probabilities of the futures corresponding to the _core test set_ at the next time point, since this provides a sufficient statistic of the past. Thus, we define the matrix mapping \(\bm{M}_{h}:\mathbb{X}_{t(h)}\to\mathbb{R}^{d_{h}\times d_{h-1}}\) by,

\[\left[M_{h}(x_{t(h)})\right]_{q,\cdot}=m_{h-1}(x_{t(h)},q)^{\top},\,q\in \mathbb{Q}_{h}.\] (24)

That is, \(M_{h}(x_{t(h)})\) is the matrix whose rows are indexed by the core tests at the \(h\)-th observable step, where the \(q\in\mathbb{Q}_{h}\) row is the weights given by the \(m_{h-1}\) mapping for the future consisting of \(x_{t(h)}\) followed by \(q\). This mapping enables us to update the probabilities of the core test sets.

**Lemma 3**.: _For any \(h\in[H-1],\,\tau_{h}\in\mathbb{H}_{h},\,x_{t(h+1)}\in\mathbb{X}_{t(h+1)}\), we have_

\[\psi_{h}(\tau_{h-1},x_{t(h)})=M_{h}(x_{t(h)})\psi_{h-1}(\tau_{h-1}).\] (25)

_Hence, for a history \(\tau_{h}=\big{(}x_{t(1)},\ldots,x_{t(h)}\big{)}\in\mathbb{H}_{h}\), we have_

\[\psi_{h}(\tau_{h})=M_{h}(x_{t(h)})M_{h-1}(x_{t(h-1)})\cdots M_{1}(x_{t(1)}) \psi_{0},\] (26)

_where \(\psi_{0}=\psi_{0}(\emptyset)\)._

Finally, observe that \(\mathbb{Q}_{H-1}^{m}=\mathbb{X}_{t(H)}\). Hence,

\[\psi_{H-1}(\tau_{H-1})=\left(\overline{\mathbb{P}}\left[\tau_{h-1},x_{t(H)} \right]\right)_{x_{t(H)}\in\mathbb{X}_{t(H)}}\in\mathbb{R}^{\left|\mathbb{X}_ {t(H)}\right|}.\]Thus, letting \(\phi_{H}:\mathbb{X}_{t_{(H)}}\to\mathbb{R}^{|\mathbb{X}_{t_{(H)}}|}\) be \(\phi_{H}(x_{t_{(H)}})=\bm{e}_{x_{t_{(H)}}}\) (the canonical basis vector), yields

\[\overline{\mathbb{P}}\left[x_{t(h)}:\,h\in[H]\right]=\phi_{H}(x_{t(H)})^{\top}M _{H-1}(x_{t(H-1)})\cdots M_{1}(x_{t(1)})\psi_{0}.\] (27)

Hence, Equation (27) together with Equation (26) imply that \((\bm{M},\phi_{H},\psi_{0})\) is a valid generalized PSR representation for the POST/POSG (as per Definition 3).

What remains is to show that if the POST/POSG is \(\alpha\)-robustly \(\mathcal{I}^{\dagger}\)-weakly revealing, then the PSR constructed above is well-conditioned.

**Theorem** (Restatement of Theorem 2).: _Suppose a POST/POSG is \(\alpha\)-robustly \(m\)-step \(\mathcal{I}^{\dagger}\)-weakly revealing. Then, the corresponding generalized PSR as constructed in Section 4 is \(\gamma\)-well-conditioned with \(\gamma=\alpha/\max_{h}\left\lVert\mathbb{I}_{h}^{\dagger}\right\rVert^{1/2}\)._

Proof.: We first show condition (1) in Assumption 1. Suppose \(h>H-m\) and hence the core tests are the full futures, which have length smaller than \(m\). Then for any \(x\in\mathbb{R}^{d_{h}}\), \(d_{h}=\prod_{s=h}^{H}\left\lvert\mathbb{X}_{s}\right\rvert\), we have

\[\max_{\pi}\sum_{\omega_{h}}\left\lvert m_{h}(\omega_{h})^{\top}x\right\rvert \cdot\pi(\omega_{h})=\max_{\pi}\sum_{\omega_{h}}\left\lvert x[\omega_{h}] \right\rvert\pi(\omega_{h})\leq\left\lVert x\right\rVert_{1},\]

where \(x[\omega_{h}]\) indexes the component of the vector \(x\) corresponding to the future \(\omega_{h}\).

Now suppose \(h\leq H-m\) (and hence the core tests consist of \(m\)-step futures). Then, we have,

\[\max_{\pi}\sum_{\omega_{h}}\left\lvert m_{h}(\omega_{h})^{\top}x \right\rvert\pi(\omega_{h}) =\max_{\pi}\sum_{\omega_{h}}\left\lvert m(\omega_{h})^{\top}\bm{G }_{h}\bm{G}_{h}^{\dagger}x\right\rvert\cdot\pi(\omega_{h})\] \[\leq\max_{\pi}\sum_{\omega_{h}}\sum_{i^{\dagger}\in\mathbb{I}_{h}^ {\dagger}}\left\lvert m(\omega_{h})^{\top}\bm{G}_{h}\bm{e}_{i^{\dagger}} \right\rvert\left\lvert\bm{e}_{i^{\dagger}}^{\top}\bm{G}_{h}^{\dagger}x \right\rvert\cdot\pi(\omega_{h}).\]

Now observe that for any policy \(\pi\) and any \(i^{\dagger}\in\mathbb{I}_{h}^{\dagger}\), we have

\[\sum_{\omega_{h}}\left\lvert m(\omega_{h})^{\top}\bm{G}_{h}\bm{e} _{i^{\dagger}}\right\rvert\cdot\pi(\omega_{h}) =\sum_{\omega_{h}}\left\lvert\tilde{m}(\omega_{h})^{\top}\bm{G}_ {h}^{\dagger}\bm{G}_{h}\bm{e}_{i^{\dagger}}\right\rvert\cdot\pi(\omega_{h})\] \[=\sum_{\omega_{h}}\overline{\mathbb{P}}\left[\omega_{h}\mid i^{ \dagger}\right]\pi(\omega_{h})\] \[=\sum_{\omega_{h}}\mathbb{P}^{\pi}\left[\omega_{h}\mid i^{\dagger }\right]=1,\]

where we used the definition of \(m_{h}(\omega_{h})\coloneqq\tilde{m}_{h}(\omega_{h})^{\top}\bm{G}_{h}^{\dagger}\), and \([\tilde{m}_{h}(\omega_{h})]_{i^{\dagger}}\coloneqq\overline{\mathbb{P}}[\omega_ {h}\mid i^{\dagger}]\). Recall that \(\pi(\omega_{h})\) is such that for any fixed sequence of observations \(\omega_{h}^{0}\), \(\sum_{\omega_{h}^{0}}\pi(\omega_{h}^{0},\omega_{h}^{0})=1\).

Putting this observation together with the preceding inequality yields

\[\max_{\pi} \sum_{\omega_{h}}\left\lvert m_{h}(\omega_{h})^{\top}x\right\rvert \pi(\omega_{h})\] \[\leq\sum_{i^{\dagger}\in\mathbb{I}_{h}^{\dagger}}\left\lvert\bm{e }_{i^{\dagger}}^{\top}\bm{G}_{h}^{\dagger}x\right\rvert\] \[=\left\lVert\bm{G}_{h}^{\dagger}x\right\rVert_{1}\leq\left\lVert \bm{G}_{h}^{\dagger}\right\rVert_{1}\cdot\left\lVert x\right\rVert_{1}\] \[\leq\frac{\sqrt{\left\lVert\mathbb{I}_{h}^{\dagger}\right\rVert}}{ \alpha}\left\lVert x\right\rVert_{1},\]

where the final inequality is from the relation between the one-norm and two-norm \(\left\lVert\bm{G}_{h}^{\dagger}\right\rVert_{1}\leq\sqrt{\left\lVert\mathbb{I}_ {h}^{\dagger}\right\rVert}\left\lVert\bm{G}_{h}^{\dagger}\right\rVert_{2}\), and \(\left\lVert\bm{G}_{h}^{\dagger}\right\rVert_{2}\leq\frac{1}{\alpha}\), by the assumption on its singular values.

Now we show condition (2) in Assumption 1. For ease of notation, we denote \(x_{t(h)}\) by \(x_{h}\). When \(h>H-m\), note that \(\left[M_{h}(x_{h})\right]_{q_{h+1},q_{h}}=\mathbf{1}\{q_{h}=(x_{h},q_{h+1})\}\), for all \(q_{h}\in\mathbb{Q}_{h},q_{h+1}\in\mathbb{Q}_{h+1}\). Hence,we have

\[\max_{\pi}\sum_{x_{h}}\left\|M_{h}(x_{h})z\right\|_{1}\pi(x_{h})=\left\|z \right\|_{1}.\]

Now, when \(h\leq H-m\), by a similar line of reasoning to the proof for condition (1), we have,

\[\max_{\pi}\sum_{x_{h}}\left\|M_{h}(x_{h})z\right\|_{1}\pi(x_{h}| \tau_{h-1}) \leq\max_{\pi}\sum_{(x_{h},q_{h+1})\in\mathbb{X}_{h}\times\mathbb{ Q}_{h+1}}\sum_{i^{\prime}\in\mathbb{I}_{h}^{\dagger}}\left|e_{q_{h+1}}^{ \top}M_{h}(x_{h})\bm{G}_{h}e_{i^{\prime}}\right|\cdot\left|e_{i^{\prime}}\bm{G }_{h}^{\dagger}z\right|\pi(x_{h}|\tau_{h-1})\] \[\overset{(a)}{=}\max_{\pi}\sum_{(x_{h},q_{h+1})\in\mathbb{X}_{h} \times\mathbb{Q}_{h+1}}\sum_{i^{\prime}\in\mathbb{I}_{h}^{\dagger}}\left|m_{h }(x_{t(h)},q_{h+1})\bm{G}_{h}e_{i^{\prime}}\right|\cdot\left|e_{i^{\prime}}\bm {G}_{h}^{\dagger}z\right|\pi(x_{h}|\tau_{h-1})\] \[\overset{(b)}{=}\max_{\pi}\sum_{i^{\prime}}\left(\sum_{(x_{h},q_{ h+1})}\overline{\mathbb{P}}\left[x_{h},q_{h+1}\;\middle|\;i^{\dagger}\right]\pi(x_{h} |\tau_{h-1})\right)\left|e_{i^{\prime}}^{\top}\bm{G}_{h}^{\dagger}z\right|\]

where step (a) uses the definition of \(M_{h}\) and step (b) uses the definition of \(m_{h}(\omega_{h})^{\top}\coloneqq\tilde{m}_{h}(\omega_{h})^{\top}\bm{G}_{h}^{\dagger}\) and \(\left[\tilde{m}_{h}(\omega_{h})\right]_{i^{\dagger}}\coloneqq\overline{\mathbb{ P}}[\omega_{h}\;\middle|\;i^{\dagger}]\). Now note that,

\[\sum_{(x_{h},q_{h+1})}\overline{\mathbb{P}}\left[x_{h},q_{h+1} \;\middle|\;i^{\dagger}\right]\pi(x_{h}|\tau_{h-1}) =\sum_{x_{h}}\sum_{\mathtt{act}(q_{h+1})}\sum_{\mathtt{obs}(q_{h +1})}\overline{\mathbb{P}}\left[x_{h},\mathtt{obs}(q_{h+1})\;\middle|\;i^{ \dagger},\mathtt{act}(q_{h+1})\right]\pi(x_{h})\] \[=\sum_{\mathtt{act}(q_{h+1})}1\] \[=\left|\mathbb{Q}_{h+1}^{A}\right|,\]

where the second line is since for any fixed action sequence, the sum over the probabilities of all observation sequences is 1.

Thus, putting this together, we obtain the following,

\[\max_{\pi}\sum_{x_{h}}\left\|M_{h}(x_{h})z\right\|_{1}\pi(x_{h}| \tau_{h-1}) \leq\left|\mathbb{Q}_{h+1}^{A}\right|\cdot\left\|\bm{G}_{h}^{ \dagger}z\right\|_{1}\] \[\leq\frac{\sqrt{\left|\mathbb{I}_{h}^{\dagger}\right|}\left| \mathbb{Q}_{h+1}^{A}\right|}{\alpha}\left\|z\right\|_{1},\]

where the last line again follows by the assumption on the singular values of \(\bm{G}_{h}\). 

## Appendix J Proof of Theorem 4: UCB Algorithm for Generalized PSRs (Team Setting)

In this section, we prove Theorem 4 which states that Algorithm 1 returns a near-optimal policy in a polynomial number of iterations. The proof is adapted from [35] and generalized to our setting with generalized PSRs (Definition 3). The proof is organized into several subsections. In Appendix J.1, we show that the total variation distance between trajectories under the true model and the estimated model can be bounded in terms of the estimation error of the observable operators \(\{M_{h}\}_{h}\). In Appendix J.2 we state some general results on maximum likelihood estimation which show that the MLE model has small error on the collected dataset. In Appendix J.3 we prove that the bonus term is an upper confidence bound for the total variation distance. In Appendix J.4 we show that the estimation error is sublinear in the number of iterations (i.e., \(O(\sqrt{K})\)). Finally, in Appendix J.5 we put this all together to prove the theorem.

### Properties of Generalized PSRs

Recall that a PSR model \(\theta=(\bm{M},\psi_{0},\phi_{H})\) consists of operators \(\bm{M}=\{M_{h}\}_{h=1}^{H-1}\), \(M_{h}:\mathbb{X}_{h}\rightarrow\mathbb{R}^{d_{h}\times d_{h-1}}\), \(\phi_{H}:\mathbb{X}_{H}\rightarrow\mathbb{R}^{d_{H-1}}\) (assumed to be the identity mapping), and \(\psi_{0}\) (assumed to be known for the purposes of presentation). Recall that, for any trajectory \(\tau_{h-1}=(x_{1},\ldots,x_{h-1})\), under model \(\theta\), we have

\[\begin{split} M_{h}(x_{h})\overline{\psi}_{h-1}(\tau_{h-1})& =\frac{\psi_{h}(\tau_{h})}{\overline{\mathbb{P}}_{\theta}\left( \tau_{h-1}\right)}\\ &=\frac{\psi_{h}(\tau_{h})}{\overline{\mathbb{P}}_{\theta}\left(x_ {h}\mid\tau_{h-1}\right)\overline{\mathbb{P}}_{\theta}\left(\tau_{h-1}\right) }\overline{\mathbb{P}}_{\theta}\left(x_{h}\mid\tau_{h-1}\right)\\ &=\overline{\psi}_{h}(\tau_{h})\overline{\mathbb{P}}_{\theta} \left(x_{h}\mid\tau_{h-1}\right)\end{split}\] (28)

Here, the notation \(\overline{\mathbb{P}}_{\theta}\left(x_{h}\mid\tau_{h-1}\right)\) means the probability of \(x_{h}\) conditioned on the history \(\tau_{h-1}\), with all actions executed. In particular, if \(x_{h}\) is an action, then \(\overline{\mathbb{P}}_{\theta}\left(x_{h}\mid\tau_{h-1}\right)=1\) and \(M_{h}(x_{h})\overline{\psi}_{h-1}(\tau_{h-1})=\overline{\psi}_{h}(\tau_{h})\).

The following proposition shows that the total variation distance between the distribution of trajectories of two PSR models can be bounded in terms of the difference in their observable operators.

**Proposition 4**.: _For any policy \(\pi\) and \(\theta,\widehat{\theta}\in\Theta\), we have,_

\[\begin{split}\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\widehat{ \theta}}^{\pi},\,\mathbb{P}_{\theta}^{\pi}\right)&\leq\sum_{h=1}^ {H}\sum_{\tau_{H}\in\mathbb{H}_{H}}\pi(\tau_{h})\left|\widehat{m}_{h}(\omega_{h })^{\top}\left(\widehat{M}_{h}(x_{h})-M_{h}(x_{h})\right)\psi_{h-1}(\tau_{h-1 })\right|,\\ \mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\widehat{\theta}}^{ \pi},\,\mathbb{P}_{\theta}^{\pi}\right)&\leq\sum_{h=1}^{H}\sum_{ \tau_{H}\in\mathbb{H}_{H}}\pi(\tau_{h})\left|m_{h}(\omega_{h})^{\top}\left( \widehat{M}_{h}(x_{h})-M_{h}(x_{h})\right)\widehat{\psi}_{h-1}(\tau_{h-1}) \right|,\end{split}\]

Proof.: The probability of any trajectory \(\tau_{H}=(x_{1},\ldots,x_{H})\) can be written in terms of products of the observable operators \(M_{h}(x_{h})\) of a PSR model (Equation (1)). Hence, we have,

\[\begin{split}\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\widehat{ \theta}}^{\pi},\,\mathbb{P}_{\theta}^{\pi}\right)&=\frac{1}{2} \sum_{\tau_{H}}\left|\mathbb{P}_{\widehat{\theta}}^{\pi}(\tau_{H})-\mathbb{P}_ {\theta}^{\pi}(\tau_{H})\right|\\ &=\frac{1}{2}\sum_{\tau_{H}}\pi(\tau_{H})\cdot\left|\left(\prod_{h =1}^{H}\widehat{M}_{h}(x_{h})\right)\psi_{0}-\left(\prod_{h=1}^{H}M_{h}(x_{h}) \right)\psi_{0}\right|\\ &\leq\frac{1}{2}\sum_{\tau_{H}}\pi(\tau_{H})\sum_{h=1}^{H}\left| \widehat{m}_{h}(x_{h+1:H})^{\top}\left(\widehat{M}_{h}(x_{h})-M_{h}(x_{h}) \right)\psi_{h-1}(\tau_{h-1})\right|,\end{split}\]

where the second line follows by the triangle inequality after noting that for any trajectory \(\tau_{H}=x_{1:H}\in\mathbb{H}_{H}\), the following holds for any \(h=1,\ldots,H\),

\[\left(\prod_{h=1}^{H}\widehat{M}_{h}(x_{h})\right)\psi_{0}- \left(\prod_{h=1}^{H}M_{h}(x_{h})\right)\psi_{0}=\widehat{m}_{h}(x_{h+1:H})^{ \top}\widehat{M}_{h}(x_{h})\widehat{\psi}_{h-1}(x_{1:h-1})-m_{h}(x_{h+1:H})^{ \top}M_{h}(x_{h})\psi_{h-1}(x_{1:h-1}).\]

By the same argument, we obtain the second inequality,

\[\begin{split}\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\widehat{ \theta}}^{\pi},\,\mathbb{P}_{\theta}^{\pi}\right)&=\frac{1}{2} \sum_{\tau_{H}}\left|\mathbb{P}_{\widehat{\theta}}^{\pi}(\tau_{H})-\mathbb{P}_ {\theta}^{\pi}(\tau_{H})\right|\\ &\leq\frac{1}{2}\sum_{\tau_{H}}\pi(\tau_{H})\sum_{h=1}^{H}\left|m _{h}(x_{h+1:H})^{\top}\left(\widehat{M}_{h}(x_{h})-M_{h}(x_{h})\right)\widehat {\psi}_{h-1}(\tau_{h-1})\right|.\end{split}\]

In this result, recall that we assume \(\psi_{0}\) is known to the agent, to simplify the presentation. If \(\psi_{0}\) was not known, there would be another term due to the estimation as \(\widehat{\psi}_{0}\) [see 33, Lemma C.3]. Note that the sample complexity of estimating \(\psi_{0}\) is small compared to learning the other parameters.

### General Results on MLE

In this section, we state some general results on maximum likelihood estimation which ultimately guarantee that the estimated model produced by the procedure in Algorithm 1 has a small estimation error. The results are stated without proof. The proofs are given in [35] and use standard techniques on MLE analysis [94]. This ultimately leads us to a lemma which states that the estimation error of the MLE model is small on the collected data.

The first proposition states that the log-likelihood of the true model \(\theta^{*}\) is large compared to any other model.

**Proposition 5** (Proposition 4 of [35]).: _Fix \(\varepsilon<\frac{1}{KH}\). With probability at least \(1-\delta\), for any \(\overline{\theta}\in\overline{\Theta}_{\varepsilon}\) and any \(k\in[K]\), the following holds:_

\[\forall\overline{\theta}\in\overline{\Theta}_{\varepsilon},\sum_ {h}\sum_{(\tau_{h},\pi)\in\mathcal{D}_{h}}\log\mathbb{P}_{\overline{\theta}}^ {\pi}(\tau_{h})-3\log\frac{K\left|\overline{\Theta}_{\varepsilon}\right|}{ \delta}\leq\sum_{h}\sum_{(\tau_{h},\pi)\in\mathcal{D}_{h}^{k}}\log\mathbb{P}_ {\theta^{*}}^{\pi}(\tau_{h})\] \[\forall\overline{\theta}\in\overline{\Theta}_{\varepsilon},\sum_ {(\tau_{H},\pi)\in\mathcal{D}^{k}}\log\mathbb{P}_{\overline{\theta}}^{\pi}( \tau_{H})-3\log\frac{K\left|\overline{\Theta}_{\varepsilon}\right|}{\delta} \leq\sum_{(\tau_{h},\pi)\in\mathcal{D}_{h}^{k}}\log\mathbb{P}_{\theta^{*}}^{ \pi}(\tau_{h})\]

The second proposition provides an upper bound on the total variation distance between the distributions of futures given histories on the empirical history of trajectories. This result ensures that the model estimated by Algorithm 1 is accurate on the sampled trajectories.

**Proposition 6** (Proposition 5 in [35]).: _Fix \(p_{\min}\) and \(\varepsilon\leq\frac{p_{\min}}{KH}\). Let \(\Theta_{\min}^{k}=\left\{\theta:\forall h,(\tau_{h},\pi)\in\mathcal{D}_{h}^{k},\,\mathbb{P}_{\overline{\theta}}^{\pi}(\tau_{h})\geq p_{\min}\right\}\). Then, with probability at least \(1-\delta\), for any \(k\in[K],\theta\in\Theta_{\min}^{k}\), we have,_

\[\sum_{h}\sum_{(\tau_{h},\pi)\in\mathcal{D}_{h}^{k}}\mathsf{D}_{\mathtt{IV}}^{ \mathsf{I}}\left(\mathbb{P}_{\overline{\theta}}^{\pi}(\omega_{h}|\tau_{h}), \mathbb{P}_{\theta^{*}}^{\pi}(\omega_{h}|\tau_{h})\right)\leq 6\sum_{h}\sum_{( \tau_{h},\pi)\in\mathcal{D}_{h}^{k}}\log\frac{\mathbb{P}_{\theta^{*}}^{\pi}( \tau_{H})}{\mathbb{P}_{\overline{\theta}}^{\pi}(\tau_{H})}+31\log\frac{K\left| \overline{\Theta}_{\varepsilon}\right|}{\delta}.\]

The next proposition is standard in the analysis of maximum likelihood estimation. \(\mathsf{D}_{\mathtt{H}}\) denotes the Hellinger distance.

**Proposition 7** (Proposition 6 of [35]).: _Let \(\varepsilon<\frac{1}{K^{2}H^{2}}\). Then, with probability at least \(1-\delta\), the following holds for all \(\theta\in\Theta\) and \(k\in[K]\),_

\[\sum_{\pi\in\mathcal{D}^{k}}\mathsf{D}_{\mathtt{H}}^{2}\left(\mathbb{P}_{ \overline{\theta}}^{\pi}(\tau_{H}),\mathbb{P}_{\theta^{*}}^{\pi}(\tau_{H}) \right)\leq\frac{1}{2}\sum_{(\tau_{H},\pi)\in\mathcal{D}^{k}}\log\frac{ \mathbb{P}_{\theta^{*}}^{\pi}(\tau_{H})}{\mathbb{P}_{\theta}^{\pi}(\tau_{H})} +2\log\frac{K\left|\overline{\Theta}_{\varepsilon}\right|}{\delta}.\]

The final proposition of this section states that when \(p_{\min}\) is chosen as in Theorem 4, the true model \(\theta^{*}\) lies in the constraint \(\Theta_{\min}^{k}\) with high probability.

**Proposition 8**.: _Fix \(p_{\min}\leq\frac{\delta}{KH\prod_{h=1}^{H}\left|\mathbb{X}_{h}\right|}\). Then, with probability at least \(1-\delta\), we have \(\theta^{*}\in\Theta_{\min}^{k}\)\(\forall k\)._

Proof.: For each \(k\in[K]\), we have \(\theta^{*}\in\Theta_{\min}^{k}\) if \(\mathbb{P}_{\theta^{*}}^{\pi^{k}}(\tau_{h}^{k})\geq p_{\min}\) for all \(h\in[H],\,(\tau_{h}^{k},\pi^{k})\in\mathcal{D}_{h}^{k}\). Consider the probability of \(\theta^{*}\) violating this constraint for some trajectory in the dataset. For each \(k,h,(\tau_{h}^{k},\pi^{k})\), we have

\[\mathbb{P}\left[\mathbb{P}_{\theta^{*}}^{\pi^{k}}(\tau_{h}^{k})<p _{\min}\right] =\mathbb{E}_{\pi}\left[\mathbb{P}\left[\mathbb{P}_{\theta^{*}}^{ \pi^{k}}(\tau_{h}^{k})<p_{\min}\ \Big{|}\ \pi^{k}=\pi\right]\right]\] \[=\mathbb{E}_{\pi}\left[\sum_{\tau_{h}\in\mathbb{H}_{h}}\mathbb{P} _{\theta^{*}}^{\pi}(\tau_{h}^{k}=\tau_{h})\mathbf{1}\{\mathbb{P}_{\theta^{*}}^{ \pi}(\tau_{h})<p_{\min}\}\right]\] \[<\sum_{\tau_{h}\in\mathbb{H}_{h}}p_{\min}\] \[=\left|\mathbb{H}_{h}\right|p_{\min}\] \[\leq\frac{\delta}{KH}.\]In the above, the first line is by the law of total probability, where the expectation is over the policy \(\pi^{k}\) used while collecting the \((h,k)\)-th trajectory, and the inner probability is over trajectories \(\tau_{h}^{k}\). The second line calculates the probability of the event \(\{\mathbb{P}_{\theta^{*}}^{\pi^{k}}(\tau_{h}^{k})<p_{\min}\}\). Taking a union bound over \(k\in[K]\), \(h\in[H]\), and \((\tau_{h},\pi)\in\mathcal{D}_{h}\) implies that \(\mathbb{P}\left[\theta^{*}\in\Theta_{\min}^{k}\right]\geq 1-\delta\). 

In what follows, let \(\mathcal{E}_{\omega},\mathcal{E}_{\pi},\mathcal{E}_{\min}\) be the events in Propositions 6 to 8, respectively. Let \(\mathcal{E}=\mathcal{E}_{\omega}\cap\mathcal{E}_{\pi}\cap\mathcal{E}_{\min}\) be the intersection of all events. Propositions 6 to 8 guarantee the event \(\mathcal{E}\) occurs with high probability, \(\mathbb{P}\left[\mathcal{E}\right]\geq 1-3\delta\), by a union bound.

The following result states that the estimated model is accurate on the past exploration policies and dataset of collected trajectories. This holds for both the conditional probabilities of futures given past trajectories in the dataset as well as over full trajectories. The result follows from the MLE analysis in Propositions 6 to 8.

**Lemma 4**.: _Let \(\beta=31\log\frac{K\left|\overline{\Theta}_{\varepsilon}\right|}{\delta}\), and suppose \(\varepsilon\leq\frac{\delta}{K^{2}H^{2}\prod_{h}\left|\overline{\Lambda}_{h} \right|}\), where \(\overline{\Theta}_{\varepsilon}\) is the optimistic \(\varepsilon\)-net in Proposition 3. Then, under event \(\mathcal{E}\), the following holds,_

\[\sum_{h}\sum_{(\tau_{h},\pi)\in\mathcal{D}_{h}^{k}}\mathsf{D}_{ \mathsf{TV}}^{2}\left(\mathbb{P}_{\tilde{\theta}^{k}}^{\pi}(\omega_{h}|\tau_ {h}),\mathbb{P}_{\theta^{*}}^{\pi}(\omega_{h}|\tau_{h})\right)\leq 7\beta,\text{and}\] \[\sum_{\pi\in\mathcal{D}^{k}}\mathsf{D}_{\mathsf{H}}^{2}\left( \mathbb{P}_{\tilde{\theta}^{k}}^{\pi}(\tau_{H}),\mathbb{P}_{\theta^{*}}^{\pi }(\tau_{H})\right)\leq 7\beta,\]

Proof.: The proof follows by Propositions 6 to 8. The argument is direct and is identical to Lemma 1 of [35]. 

### UCB for Total Variation Distance

**Notation.** Let \(m^{*},\{M_{h}^{*}\}_{h}\) be the observable operators of the true PSR \(\theta^{*}\), and let \(\{\widehat{M}_{h}^{k}\}_{h}\) be the algorithm's estimates of the observable operators corresponding to \(\widehat{\theta}^{k}\).

Recall that Proposition 4 shows that the total variation distance between the distribution over trajectories of two PSRs is bounded by the estimation error of the observable operators \(M_{h}\). The following result constructs a bound on the estimation error of the observable operators \(M_{h}(x_{h})\). The proof is adapted from [35, Lemma 2] to our setting with generalized PSRs.

**Lemma 5**.: _Under event \(\mathcal{E}\), for any policy \(\pi\) and \(k\in[K]\), we have,_

\[\sum_{\tau_{H}}\left|m^{*}(\omega_{h})^{\top}\left(\widehat{M}_{h}^{k}(x_{h})- M_{h}^{*}(x_{h})\right)\widehat{\psi}_{h-1}^{k}(\tau_{h-1})\right|\pi(\tau_{H}) \leq\mathbb{E}_{\tau_{h-1}\sim\mathbb{P}_{\tilde{\theta}^{k}}}^{\pi}\left[ \alpha_{h-1}^{k}\left\|\widehat{\overline{\psi}}_{h-1}^{k}(\tau_{h-1})\right\| _{(\widehat{U}_{h-1}^{k})^{-1}}\right]\]

_where,_

\[\widehat{U}_{h-1}^{k} =\lambda I+\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{k}}\left[ \widehat{\overline{\psi}}_{h}^{k}(\tau_{h-1})\widehat{\overline{\psi}}_{h}^{k }(\tau_{h-1})^{\top}\right]\] \[\left(\alpha_{h-1}^{k}\right)^{2} =\frac{4\lambda Q_{A}^{2}d}{\gamma^{4}}+\frac{4\max_{s\in\mathcal{ A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A}^{2}}{\gamma^{2}}\sum_{\tau_{h-1}\in \mathcal{D}_{h-1}^{k}}\mathsf{D}_{\mathsf{TV}}^{2}\left(\mathbb{P}_{\tilde{ \theta}^{k}}^{\pi^{\text{exp}}_{h-1}}\left(\omega_{h-1}^{o}\bigm{|}\tau_{h-1}, \omega_{h-1}^{a}\right),\mathbb{P}_{\theta^{*}}^{\pi^{\text{exp}}_{h-1}}\left( \omega_{h-1}^{o}\bigm{|}\tau_{h-1},\omega_{h}^{a}\right)\right)\]

Proof.: To ease notation, we index the future trajectories \(\omega_{h-1}=(x_{h},\ldots,x_{H})\in\mathbb{F}_{h-1}\) by \(i\) and history trajectories \(\tau_{h-1}=(x_{1},\ldots,x_{h-1})\in\mathbb{H}_{h-1}\) by \(j\). We denote \(m^{*}(\omega_{h})^{\top}\left(\widehat{M}_{h}^{k}(x_{h})-M_{h}^{*}(x_{h})\right)\) as \(w_{i}^{\top}\), \(\widehat{\overline{\psi}}_{h}^{k}(\tau_{h-1})\) as \(x_{j}\), and \(\pi(\omega_{h-1}|\tau_{h-1})\) as \(\pi_{i|j}\).

The following bound follows from the Cauchy-Schwarz inequality,

\[\sum_{\tau_{H}}\left|m^{\star}(\omega_{h})^{\top}\left(\widehat{M}_{h }^{k}(x_{h})-M_{h}^{\star}(x_{h})\right)\widehat{\psi}_{h-1}^{k}(\tau_{h-1}) \right|\pi(\tau_{H})\] \[\stackrel{{(a)}}{{=}}\sum_{\omega_{h-1}}\sum_{\tau_{h -1}}\left|m^{\star}(\omega_{h})^{\top}\left(\widehat{M}_{h}^{k}(x_{h})-M_{h}^{ \star}(x_{h})\right)\widehat{\overline{\psi}}_{h}^{k}(\tau_{h-1})\right|\pi( \omega_{h-1}|\tau_{h-1})\mathbb{P}_{\widehat{\theta}^{k}}^{\pi}(\tau_{h-1})\] \[=\sum_{i}\sum_{j}\left|w_{i}^{\top}x_{j}\right|\pi_{ij}\mathbb{P }_{\widehat{\theta}^{k}}^{\pi}(j)\] \[=\sum_{i}\sum_{j}\left(\pi_{i|j}\cdot\mathrm{sign}(w_{i}^{\top}x_ {j})w_{i}\right)^{\top}x_{j}\cdot\mathbb{P}_{\widehat{\theta}^{k}}^{\pi}(j)\] \[=\sum_{j}\left(\sum_{i}\pi_{i|j}\cdot\mathrm{sign}(w_{i}^{\top}x_ {j})w_{i}\right)^{\top}x_{j}\cdot\mathbb{P}_{\widehat{\theta}^{k}}^{\pi}(j)\] \[=\mathbb{E}_{j\sim\mathbb{P}_{\widehat{\theta}^{k}}^{\pi}}\left[ \left(\sum_{i}\pi_{i|j}\cdot\mathrm{sign}(w_{i}^{\top}x_{j})w_{i}\right)^{\top }x_{j}\right]\] \[\stackrel{{(b)}}{{\leq}}\mathbb{E}_{j\sim\mathbb{P }_{\widehat{\theta}^{k}}^{\pi}}\left[\|x_{j}\|_{\left(\widehat{U}_{h-1}^{k} \right)^{-1}}\left\|\sum_{i}\pi_{i|j}\cdot\mathrm{sign}(w_{i}^{\top}x_{j}) \cdot w_{i}\right\|_{\widehat{U}_{h-1}^{k}}\right].\]

Step (a) follows from the fact that \(\widehat{\psi}_{h-1}^{k}(\tau_{h-1})=\widehat{\overline{\psi}}_{h}^{k}(\tau_{ h-1})\cdot(\widehat{\phi}_{h-1}^{k})^{\top}\widehat{\psi}_{h-1}^{k}(\tau_{h-1})= \widehat{\overline{\psi}}_{h}^{k}(\tau_{h-1})\cdot\overline{\mathbb{P}}_{ \widehat{\theta}^{k}}\left[\tau_{h-1}\right]\) and \(\overline{\mathbb{P}}_{\widehat{\theta}^{k}}\left[\tau_{h-1}\right]\cdot \pi(\tau_{H})=\pi(\omega_{h-1}|\tau_{h-1})\cdot\mathbb{P}_{\widehat{\theta}^ {k}}^{\pi}(\tau_{h-1})\). Step (b) is the Cauchy-Schwarz inequality.

Fix \(\tau_{h-1}=j_{0}\). Let \(I_{1}:=\left\|\sum_{i}\pi_{i|j_{0}}\cdot\mathrm{sign}(w_{i}^{\top}x_{j_{0}}) \cdot w_{i}\right\|_{\widehat{U}_{h-1}^{k}}^{2}\), which we bound next. By the definition of \(\widehat{U}_{h-1}^{k}\), we partition this term into two parts,

\[I_{1}=\underbrace{\lambda\left\|\sum_{i}\pi_{i|j_{0}}\cdot\mathrm{sign}(w_{i}^ {\top}x_{j_{0}})\cdot w_{i}\right\|_{I_{2}}^{2}}_{I_{2}}+\underbrace{\sum_{j \in D_{h-1}^{\prime}}\left[\left(\sum_{i}\pi_{i|j_{0}}\cdot\mathrm{sign}(w_{i }^{\top}x_{j_{0}})\cdot w_{i}\right)^{\top}x_{j}\right]^{2}}_{I_{3}}.\]

We bound \(I_{2}\) and \(I_{3}\) separately. By the triangle inequality, \(\sqrt{I_{2}}\) is bound by a sum of two terms,

\[\sqrt{I_{2}} =\sqrt{\lambda}\max_{z\in\mathbb{R}^{d_{h-1}}\cdot\|z\|_{2}=1} \left|\sum_{i}\pi_{i|j_{0}}\cdot\mathrm{sign}(w_{i}^{\top}x_{j_{0}})\cdot w_{i }^{\top}z\right|\] \[\stackrel{{(a)}}{{\leq}}\sqrt{\lambda}\max_{\|z\|_{ 2}=1}\sum_{\omega_{h-1}}\left|m^{\star}(\omega_{h}^{\top})\left(\widehat{M}_{h }^{k}(x_{h})-M_{h}^{\star}(x_{h})\right)z\right|\pi(\omega_{h-1}|j_{0})\] \[\stackrel{{(b)}}{{\leq}}\sqrt{\lambda}\max_{\|z\|_{ 2}=1}\sum_{\omega_{h-1}}\left|m^{\star}(\omega_{h})^{\top}\widehat{M}_{h}^{k}(x _{h})z\right|\pi(\omega_{h-1}|j_{0})\] \[\quad+\sqrt{\lambda}\max_{\|z\|_{2}=1}\sum_{\omega_{h-1}}\left|m^ {\star}(\omega_{h})^{\top}M_{h}^{\star}(x_{h})z\right|\pi(\omega_{h-1}|j_{0}),\]

where step (a) is by the definition of \(w_{i}^{\top},\pi_{i|j_{0}}\) and the triangle inequality, and step (b) is by the triangle inequality.

Consider the first term. It can be bound via the definition of \(\gamma\)-well-conditioning as follows,

\[\max_{\left\|z\right\|_{2}=1} \sum_{\omega_{h-1}}\left|m^{\star}(\omega_{h})^{\top}\widehat{M}_{h }^{k}(x_{h})z\right|\pi(\omega_{h-1}|j_{0})\] \[=\max_{\left\|z\right\|_{2}=1}\sum_{x_{h}}\left(\sum_{\omega_{h}} \left|m^{\star}(\omega_{h})^{\top}\widehat{M}_{h}^{k}(x_{h})z\right|\pi(\omega _{h}|j_{0},x_{h})\right)\pi(x_{h}|j_{0})\] \[\overset{(a)}{\leq}\frac{1}{\gamma}\max_{\left\|z\right\|_{2}=1} \sum_{x_{h}}\left\|\widehat{M}_{h}^{k}(x_{h})z\right\|_{1}\pi(x_{h}|j_{0})\] \[\overset{(b)}{\leq}\frac{1}{\gamma}\max_{\left\|z\right\|_{2}=1} \frac{\left|\mathbb{Q}_{h+1}^{A}\right|\left\|z\right\|_{1}}{\gamma}\] \[\overset{(c)}{\leq}\frac{\sqrt{d}Q_{A}}{\gamma^{2}}\]

where step (a) is by the first condition in Assumption 1, step (b) is by the second condition of Assumption 1, and step (c) is by the fact that \(\max_{z\in\mathbb{R}^{d_{h-1}}:\left\|z\right\|_{2}=1}\left\|z\right\|_{1}= \sqrt{d_{h-1}}\leq\sqrt{d}\) and \(\left|\mathbb{Q}_{h+1}^{A}\right|\leq Q_{A}\). In the above, note that we used the \(\gamma\)-well-conditioning of PSR \(\widehat{\theta}^{k}\) in step (a) and the \(\gamma\)-well-conditioning of PSR \(\theta^{*}\) in step (b). The second term in \(\sqrt{I_{2}}\) admits an identical bound, simply by using the well-conditioning of the PSR \(\theta^{*}\) in both steps. Hence, we have that

\[I_{2}\leq 4\frac{\lambda dQ_{A}^{2}}{\gamma^{4}}.\] (29)

Now we upper bound \(I_{3}\),

\[I_{3} \leq\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{k}}\left(\sum_{\omega_{ h-1}}\left|m^{\star}(\omega_{h})^{\top}\left(\widehat{M}_{h}^{k}(x_{h})-M_{h}^{ \star}(x_{h})\right)\widehat{\overline{\psi}}^{k}(\tau_{h-1})\right|\pi(\omega _{h-1}|j_{0})\right)^{2}\] \[\leq\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{k}}\underbrace{\left( \sum_{\omega_{h-1}}\left|m^{\star}(\omega_{h})^{\top}\left(\widehat{M}_{h}^{k} (x_{h})\widehat{\overline{\psi}}^{k}(\tau_{h-1})-M_{h}^{\star}(x_{h})\overline {\psi}^{\star}(\tau_{h-1})\right)\right|\pi(\omega_{h-1}|j_{0})}_{I_{4}}\] \[+\underbrace{\sum_{\omega_{h-1}}\left|m^{\star}(\omega_{h})^{\top }M_{h}^{\star}(x_{h})\left(\widehat{\overline{\psi}}^{k}(\tau_{h-1})-\overline {\psi}^{\star}(\tau_{h-1})\right)\right|\pi(\omega_{h-1}|j_{0})}_{I_{5}}\] \[=:\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{k}}(I_{4}+I_{5})^{2}\]

where the second equality follows from the triangle inequality by adding and subtracting \(m^{\star}(\omega_{h})^{\top}M_{h}^{\star}(x_{h})\overline{\psi}^{\star}(\tau_ {h-1})\) inside the absolute value. We now bound each of \(I_{4}\) and \(I_{5}\).

\[I_{4} \coloneqq\sum_{\omega_{h-1}}\left|m^{\star}(\omega_{h})^{\top} \left(\widehat{M}_{h}^{k}(x_{h})\widehat{\overline{\psi}}^{k}(\tau_{h-1})-M_{h}^ {\star}(x_{h})\overline{\psi}^{\star}(\tau_{h-1})\right)\right|\pi(\omega_{h-1} |j_{0})\] \[\quad\stackrel{{(a)}}{{=}}\sum_{\omega_{h-1}}\left|m^ {\star}(\omega_{h})^{\top}\left(\overline{\mathbb{P}}_{\widehat{\theta}^{k}} \left[x_{h}\mid\tau_{h-1}\right]\widehat{\overline{\psi}}_{h}(\tau_{h})- \overline{\mathbb{P}}_{\theta^{\star}}\left[x_{h}\mid\tau_{h-1}\right] \overline{\psi}_{h}^{\star}(\tau_{h})\right)\right|\pi(\omega_{h-1}|j_{0})\] \[\quad\stackrel{{(b)}}{{=}}\sum_{x_{h}}\left(\sum_{ \omega_{h}}\left|m^{\star}(\omega_{h})^{\top}\left(\overline{\mathbb{P}}_{ \widehat{\theta}^{k}}\left[x_{h}\mid\tau_{h-1}\right]\widehat{\overline{\psi }}_{h}(\tau_{h})-\overline{\mathbb{P}}_{\theta^{\star}}\left[x_{h}\mid\tau_{h -1}\right]\overline{\psi}_{h}^{\star}(\tau_{h})\right)\right|\pi(\omega_{h}|j_ {0},x_{h})\right)\pi(x_{h}|j_{0})\] \[\quad\stackrel{{(d)}}{{=}}\frac{1}{\gamma}\sum_{x_{h }}\sum_{q_{h}\in\mathbb{Q}_{h}}\left|\overline{\mathbb{P}}_{\widehat{\theta}^{ k}}\left[x_{h},q_{h}\mid\tau_{h-1}\right]-\overline{\mathbb{P}}_{\theta^{\star}} \left[x_{h},q_{h}\mid\tau_{h-1}\right]\right|\pi(x_{h}|j_{0})\]

where step (a) is by the fact that \(M_{h}(x_{h})\overline{\psi}_{h-1}(\tau_{h-1})=\overline{\mathbb{P}}\left[x_{h} \mid\tau_{h-1}\right]\overline{\psi}(\tau_{h})\), as shown in Equation (28), step (b) uses \(\omega_{h-1}=(x_{h},\omega_{h})\) and \(\pi(\omega_{h-1}|j_{0})=\pi(x_{h}|j_{0})\pi(\omega_{h}|j_{0},x_{h})\), step (c) is by Assumption 1, and step (d) follows by the definition \(\overline{\psi}_{h}\), \(\left[\overline{\psi}_{h}(\tau_{h})\right]_{l}=\overline{\mathbb{P}}_{\theta} \left[q_{h}^{l}\mid\tau_{h}\right]\).

Now, we turn to bound the \(I_{5}\) term. We have

\[I_{5} =\sum_{\omega_{h}}\sum_{x_{h}}\left|m_{h}^{\star}(\omega_{h})^{ \top}M_{h}^{\star}(x_{h})\left(\widehat{\overline{\psi}}^{k}(\tau_{h-1})- \overline{\psi}^{\star}(\tau_{h-1})\right)\right|\pi(\omega_{h}|j_{0},x_{h}) \pi(x_{h}|j_{0})\] \[\quad\stackrel{{(a)}}{{=}}\sum_{\omega_{h-1}}\left|m_ {h-1}^{\star}(\omega_{h-1})^{\top}\left(\widehat{\overline{\psi}}^{k}(\tau_{h- 1})-\overline{\psi}^{\star}(\tau_{h-1})\right)\right|\pi(\omega_{h-1}|j_{0})\] \[\quad\stackrel{{(a)}}{{\leq}}\frac{1}{\gamma}\left\| \widehat{\overline{\psi}}^{k}(\tau_{h-1})-\overline{\psi}^{\star}(\tau_{h-1}) \right\|_{1}\] \[\quad=\frac{1}{\gamma}\sum_{q_{h-1}\in\mathbb{Q}_{h-1}}\left| \overline{\mathbb{P}}_{\widehat{\theta}^{k}}\left[q_{h-1}\mid\tau_{h-1}\right] -\overline{\mathbb{P}}_{\theta^{\star}}\left[q_{h-1}\mid\tau_{h-1}\right] \right|,\]

where step (a) is since \(m_{h}^{\star}(\omega_{h})^{\top}M_{h}^{\star}(x_{h})=m_{h-1}^{\star}(\omega_{h-1 })^{\top}\), step (b) is by the first condition of Assumption 1, and the final equality is again by the definition of \(\overline{\psi}\).

Combining the above, we have that,

\[I_{3} \leq\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{\star}}(I_{4}+I_{5})^{2}\] \[\leq\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{\star}}\left(\frac{1}{ \gamma}\sum_{x_{h}\in\mathbb{X}_{h}}\sum_{q_{h}\in\mathbb{Q}_{h}}\left|\overline {\mathbb{P}}_{\widehat{\theta}^{k}}\left[x_{h},q_{h}\mid\tau_{h-1}\right]- \overline{\mathbb{P}}_{\theta^{\star}}\left[x_{h},q_{h}\mid\tau_{h-1}\right] \right|\pi(x_{h}|\tau_{h-1})\] \[\quad\quad\quad\quad\quad+\frac{1}{\gamma}\sum_{q_{h-1}\in \mathbb{Q}_{h-1}}\left|\overline{\mathbb{P}}_{\widehat{\theta}^{k}}\left[q_{h-1} \mid\tau_{h-1}\right]-\overline{\mathbb{P}}_{\theta^{\star}}\left[q_{h-1}\mid \tau_{h-1}\right]\right|\right)^{2}\] \[\leq\frac{1}{\gamma^{2}}\cdot\sum_{\tau_{h-1}\in\mathcal{D}_{h-1 }^{\star}}\Biggl{(}\sum_{(x_{h},q_{h})\in\mathbb{X}_{h}\times\mathbb{Q}_{h}} \left|\overline{\mathbb{P}}_{\widehat{\theta}^{k}}\left[x_{h},q_{h}\mid\tau_{h-1} \right]-\overline{\mathbb{P}}_{\theta^{\star}}\left[x_{h},q_{h}\mid\tau_{h-1} \right]\right|\pi(x_{h}|\tau_{h-1})\] \[\quad\quad\quad\quad\quad+\sum_{q_{h-1}\in\mathbb{Q}_{h-1}}\left| \overline{\mathbb{P}}_{\widehat{\theta}^{k}}\left[q_{h-1}\mid\tau_{h-1}\right]- \overline{\mathbb{P}}_{\theta^{\star}}\left[q_{h-1}\mid\tau_{h-1}\right] \right|\Biggr{)}^{2}\]

Now, we decompose the summations above over \(\mathbb{X}_{h}\times\mathbb{Q}_{h}\) and \(\mathbb{Q}_{h-1}\) into separate summations over observation futures and action futures. That is, \((x_{h},q_{h})\) is decomposed into \((\omega_{h-1}^{a},\omega_{h-1}^{\omega})\), where \(\omega_{h-1}^{a}=\mathtt{act}(x_{h},q_{h})\) and \(\omega_{h-1}^{\omega}=\mathtt{obs}(x_{h},q_{h})\), and the summations are over \(\omega_{h-1}^{a}\in\mathtt{act}(\mathbb{X}_{h}\times\mathbb{Q}_{h})\)and \(\omega_{h-1}^{o}\in\mathtt{obs}(\mathbb{X}_{h}\times\mathbb{Q}_{h})\). Similarly, \(q_{h-1}\) can be decomposed into \((q_{h-1}^{o},q_{h-1}^{a})\in\mathtt{obs}(\mathbb{Q}_{h-1})\times\mathtt{act}( \mathbb{Q}_{h-1})\). Hence, the bound on \(I_{3}\) can be written as,

\[I_{3}\leq\frac{1}{\gamma^{2}}\cdot\sum_{\tau_{h-1}\in\mathcal{D} _{h-1}^{k}} \Bigg{(}\sum_{\omega_{h-1}^{o}}\sum_{\omega_{h-1}^{o}}\left|\overline{ \mathbb{P}}_{\widehat{\theta}^{k}}\left[\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right]- \overline{\mathbb{P}}_{\theta^{*}}\left[\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h}^{a}\right]\right|\pi(x_{h}| \tau_{h-1})\] \[+\sum_{q_{h-1}^{o}}\sum_{q_{h-1}^{o}}\left|\overline{ \mathbb{P}}_{\widehat{\theta}^{k}}\left[q_{h-1}^{o}\ \big{|}\ \tau_{h-1},q_{h-1}^{a}\right]- \overline{\mathbb{P}}_{\theta^{*}}\left[q_{h-1}^{o}\ \big{|}\ \tau_{h-1},q_{h-1}^{a}\right]\right| \Bigg{)}^{2}\] \[\leq\frac{1}{\gamma^{2}}\cdot\sum_{\tau_{h-1}\in\mathcal{D}_{h-1} ^{k}}\Bigg{(}\sum_{\omega_{h-1}^{o}\in\mathbb{Q}_{h-1}^{\mathtt{exp}}}\sum_{ \omega_{h-1}^{o}}\left|\overline{\mathbb{P}}_{\widehat{\theta}^{k}}\left[ \omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right]- \overline{\mathbb{P}}_{\theta^{*}}\left[\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right]\right| \Bigg{)}^{2}\] \[=\frac{1}{\gamma^{2}}\left|\mathbb{Q}_{h-1}^{\mathtt{exp}}\right| ^{2}\cdot\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{k}}\mathsf{D}_{\mathtt{TV}}^{ \mathtt{TV}}\left(\mathbb{P}_{\widehat{\theta}^{k}}^{\mathtt{sup}_{1}}\left( \omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right),\mathbb{P}_{\theta^{*}}^{ \mathtt{sup}_{1}}\left(\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right)\right).\]

Where the second inequality is by the definition of \(\mathbb{Q}_{h-1}^{\mathtt{exp}}=\mathtt{act}\left(\mathbb{X}_{h}\times \mathbb{Q}_{h}\cup\mathbb{Q}_{h-1}\right)\). Here, the second summation is over \(\omega_{h-1}^{o}\in\mathtt{obs}\left(\mathbb{X}_{h}\times\mathbb{Q}_{h}\cup \mathbb{Q}_{h-1}\right)\). The final equality uses the fact the under the policy \(\mathtt{u}_{h-1}^{\mathtt{exp}}\) the probability of each action sequence \(\omega_{h-1}^{o}\) is \(1/\left|\mathbb{Q}_{h-1}^{\mathtt{exp}}\right|\). Note that \(\left|\mathbb{Q}_{h-1}^{\mathtt{exp}}\right|\leq\left|\mathtt{act}\left( \mathbb{X}_{h}\times\mathbb{Q}_{h}\right)\right|+\left|\mathtt{act}\left( \mathbb{Q}_{h-1}\right)\right|\), and hence we have \(\left|\mathbb{Q}_{h-1}^{\mathtt{exp}}\right|\leq 2\max_{s\in\mathcal{A}}\left| \mathbb{X}_{s}\right|Q_{A}\) for all \(h\). Hence, we have,

\[I_{3}\leq 4\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A }^{2}\frac{1}{\gamma^{2}}\sum_{\tau_{h-1}\in\mathcal{D}_{h-1}^{k}}\mathsf{D}_ {\mathtt{TV}}^{2}\left(\mathbb{P}_{\widehat{\theta}^{k}}^{\mathtt{sup}_{1}} \left(\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right),\mathbb{P}_{\theta^{*}}^{ \mathtt{u}_{h-1}^{\mathtt{exp}}}\left(\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h}^{a}\right)\right).\] (30)

Putting this together with the bounds on \(I_{2}\) and \(I_{3}\), we get that,

\[I_{1}\leq\frac{4\lambda Q_{A}^{2}d}{\gamma^{4}}+4\max_{s\in \mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A}^{2}\frac{1}{\gamma^{2}}\sum_{ \tau_{h-1}\in\mathcal{D}_{h-1}^{k}}\mathsf{D}_{\mathtt{TV}}^{2}\left(\mathbb{P} _{\widehat{\theta}^{k}}^{\mathtt{sup}_{h-1}}\left(\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right),\mathbb{P}_{\theta^{*}}^{ \mathtt{sup}_{h-1}}\left(\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h}^{a}\right)\right)\] \[=:\left(\alpha_{h-1}^{k}\right)^{2},\]

completing the proof. 

Using the above bound on the difference between the observable operators of the true model and the estimated model, we now bound the total variation distance between the distributions of trajectories through Proposition 4.

**Lemma 6**.: _Under even \(\mathcal{E}\), the total variation distance between the estimated model at iteration \(k\), \(\widehat{\theta}^{k}\), and the true model \(\theta^{*}\), is bounded by,_

\[\mathsf{D}_{\mathtt{TV}}\left(\mathbb{P}_{\widehat{\theta}^{k}}^{\pi}(\tau_{H}), \mathbb{P}_{\theta^{*}}^{\pi}(\tau_{H})\right)\leq\alpha\cdot\mathbb{E}_{\tau_{H }\sim\mathbb{P}_{\widehat{\theta}^{k}}^{\pi}}\left[\sqrt{\sum_{h=0}^{H-1} \left\|\overline{\widehat{\psi}}^{k}(\tau_{h})\right\|_{(\widehat{U}_{h}^{k}) ^{-1}}^{2}}\right],\] (31)

_for any policy \(\pi\), where_

\[\alpha^{2}=\frac{4\lambda HQ_{A}^{2}d}{\gamma^{4}}+28\max_{s\in \mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A}^{2}\frac{1}{\gamma^{2}}\beta\]

Proof.: Consider \(\alpha_{h-1}^{k}\) in the previous lemma. We have that,

\[\sum_{h=1}^{H}\left(\alpha_{h-1}^{k}\right)^{2}\] \[=\frac{4\lambda HQ_{A}^{2}d}{\gamma^{4}}+4\max_{s\in\mathcal{A}} \left|\mathbb{X}_{s}\right|^{2}Q_{A}^{2}\frac{1}{\gamma^{2}}\sum_{h=1}^{H}\sum_{ \tau_{h-1}\in\mathcal{D}_{h-1}^{k}}\mathsf{D}_{\mathtt{TV}}^{2}\left(\mathbb{P}_{ \widehat{\theta}^{k}}^{\mathtt{sup}_{h-1}}\left(\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h-1}^{a}\right),\mathbb{P}_{\theta^{*}}^{ \mathtt{sup}_{h-1}}\left(\omega_{h-1}^{o}\ \big{|}\ \tau_{h-1},\omega_{h}^{a}\right)\right)\] \[\leq\frac{4\lambda HQ_{A}^{2}d}{\gamma^{4}}+4\max_{s\in \mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A}^{2}\frac{1}{\gamma^{2}}7\beta=: \alpha^{2},\]where the inequality is by the bound on the total variation distance established in Lemma 4.

Now, by Proposition 4, the total variation distance is bounded by the estimation error:

\[\begin{split}&\mathsf{D}_{\mathsf{TV}}\left(\mathbb{P}_{\widetilde{ \theta}^{k}}^{\pi}(\tau_{H}),\mathbb{P}_{\widetilde{\theta}^{*}}^{\pi}(\tau_{ H})\right)\\ &\stackrel{{(a)}}{{\leq}}\sum_{h=1}^{H}\sum_{\tau_{H }}\left|m^{\star}(\omega_{h})^{\top}\left(\widehat{M}_{h}^{k}(x_{h})-M_{h}^{ \star}(x_{h})\right)\widehat{\psi}_{h-1}^{k}(\tau_{h-1})\right|\pi(\tau_{H}) \\ &\stackrel{{(b)}}{{\leq}}\sum_{h=1}^{H}\mathbb{E}_{ \tau_{h-1}\sim\mathbb{P}_{\widetilde{\theta}^{k}}}^{\pi}\left[\left.\alpha_{h- 1}^{k}\left\|\widehat{\overline{\psi}}_{h-1}^{k}(\tau_{h-1})\right\|_{( \widehat{U}_{h-1}^{k})^{-1}}\right]\right.\\ &\stackrel{{(c)}}{{\leq}}\alpha\cdot\mathbb{E}_{\tau _{H}\sim\mathbb{P}_{\widetilde{\theta}^{k}}^{\pi}}\left[\left.\sqrt{\sum_{h=0} ^{H-1}\left\|\widehat{\overline{\psi}}^{k}(\tau_{h})\right\|_{(\widehat{U}_{h }^{k})^{-1}}^{2}}\right],\right.\end{split}\]

where step (a) is by Proposition 4, step (b) is by Lemma 5, and step (c) is by the Cauchy-Schwarz inequality and the calculation above bounding \(\sum_{h}\left(\alpha_{h-1}^{k}\right)^{2}\). 

A direct corollary is the following bound on the error in the estimated value function, which establishes that the bonus term \(\widehat{b}^{k}\) gives an upper confidence bound.

**Corollary 4** (Upper confidence bound).: _Under the event \(\mathcal{E}\), for any \(k\in[K]\), any reward function \(R:\prod_{h\in[H]}\mathbb{X}_{h}\rightarrow[0,1]\), and any policy \(\pi\), we have,_

\[\left|V_{\widetilde{\theta}^{k}}^{R}(\pi)-V_{\theta^{*}}^{R}(\pi)\right|\leq V _{\widetilde{\theta}^{k}}^{\widehat{b}^{k}},\]

_where \(\widehat{b}^{k}(\tau_{H})=\min\left\{\alpha\sqrt{\sum_{h}\left\|\widehat{ \overline{\psi}}^{k}(\tau_{h})\right\|_{(\widehat{U}_{h}^{k})^{-1}}^{2}},1 \right\}\)._

Proof.: By a direct calculation,

\[\begin{split}\left|V_{\widetilde{\theta}^{k}}^{R}(\pi)-V_{ \theta^{*}}^{R}(\pi)\right|&=\left|\sum_{\tau_{H}}R(\tau_{H}) \mathbb{P}_{\widetilde{\theta}^{k}}^{\pi}(\tau_{H})-\sum_{\tau_{H}}R(\tau_{H} )\mathbb{P}_{\widetilde{\theta}^{*}}^{\pi}(\tau_{H})\right|\\ &\stackrel{{(a)}}{{\leq}}\sum_{\tau_{H}}\left| \mathbb{P}_{\widetilde{\theta}^{k}}^{\pi}(\tau_{H})-\mathbb{P}_{\widetilde{ \theta}^{*}}^{\pi}(\tau_{H})\right|\\ &=\mathsf{D}_{\mathsf{TV}}\left(\mathbb{P}_{\widetilde{\theta}^{k }}^{\pi}(\tau_{H}),\mathbb{P}_{\widetilde{\theta}^{*}}^{\pi}(\tau_{H})\right) \\ &\stackrel{{(b)}}{{\leq}}\alpha\cdot\mathbb{E}_{\tau _{H}\sim\mathbb{P}_{\widetilde{\theta}^{k}}^{\pi}}\left[\left.\sqrt{\sum_{h=0} ^{H-1}\left\|\widehat{\overline{\psi}}^{k}(\tau_{h})\right\|_{(\widehat{U}_{ h}^{k})^{-1}}^{2}}\right]\\ &\stackrel{{(c)}}{{\leq}}\alpha\sum_{\tau_{H}} \widehat{b}^{k}(\tau_{H})\mathbb{P}_{\widetilde{\theta}^{k}}^{\pi}(\tau_{H}) \\ &=:V_{\widetilde{\theta}^{k}}^{\widehat{b}^{k}}\end{split}\]

where step (a) is by the triangle inequality and the fact that \(R(\tau_{H})\in[0,1]\), step (b) is by Lemma 6, and step (c) is by the definition of \(\widehat{b}^{k}\). 

### \(\sum_{k=1}^{K}V_{\widetilde{\theta}^{k}}^{\widehat{b}^{k}}\) is sublinear

The next step is to prove that \(\sum_{k=1}^{K}V_{\widetilde{\theta}^{k}}^{\widehat{b}^{k}}=O(\sqrt{K})\). To do that, we first prove that the estimated prediction features and the ground-truth prediction features can be related through the total-variation distance between the estimated model and the true model.

**Lemma 7**.: _Under event \(\mathcal{E}\), for any \(k\in[K]\), we have:_

\[\mathbb{E}_{\tau_{H}\sim\mathbb{P}^{\pi}_{\theta^{*}}}\left[\sqrt{ \sum_{h=0}^{H-1}\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})\right\|^{2}_{( \widehat{U}^{k}_{h})^{-1}}}\right]\] \[\leq\frac{2HQ_{A}}{\sqrt{\lambda}}\mathbb{D}_{\mathtt{TV}}\left( \mathbb{P}^{\pi}_{\theta^{*}}(\tau_{h}),\mathbb{P}^{\pi}_{\widehat{\theta}^{* }}(\tau_{h})\right)+\left(1+\frac{2\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|\,Q_ {A}\sqrt{\tau r\beta}}{\sqrt{\lambda}}\right)\sum_{h=0}^{H-1}\mathbb{E}_{\tau_ {h}\sim\mathbb{P}^{\pi}_{\theta^{*}}}\left\|\overline{\psi^{*}}(\tau_{h}) \right\|_{(U^{k}_{h})^{-1}}\]

Proof.: First, we recall the definition of \(\widehat{U}^{k}_{h}\), and we define its ground-truth counterpart replacing estimated features with true features,

\[\widehat{U}^{k}_{h} =\lambda I+\sum_{\tau\in\mathcal{D}^{k}_{h}}\widehat{\overline{ \psi^{k}}}(\tau_{h})\widehat{\overline{\psi^{k}}}(\tau_{h})^{\top},\] \[U^{k}_{h} =\lambda I+\sum_{\tau\in\mathcal{D}^{k}_{h}}\widehat{\overline{ \psi^{*}}}(\tau_{h})\overline{\psi^{*}}(\tau_{h})^{\top}.\]

For any trajectory \(\tau_{H}\in\mathbb{H}_{H}\), we have,

\[\sqrt{\sum_{h=0}^{H-1}\left\|\widehat{\overline{\psi^{k}}}(\tau_ {h})\right\|^{2}_{(\widehat{U}^{k}_{h})^{-1}}} \overset{(a)}{\leq} \sum_{h=0}^{H-1}\left\|\widehat{\overline{\psi^{k}}}(\tau_{h}) \right\|_{(\widehat{U}^{k}_{h})^{-1}}\] \[\leq\frac{1}{\sqrt{\lambda}}\sum_{h=0}^{H-1}\left\|\widehat{ \overline{\psi^{k}}}(\tau_{h})-\overline{\psi^{*}}(\tau_{h})\right\|_{2}+\sum _{h=0}^{H-1}\left(1+\frac{\sqrt{r}\sqrt{\sum_{\tau_{h}\in\mathcal{D}^{k}_{h}} \left\|\widehat{\overline{\psi^{k}}}(\tau_{h})-\overline{\psi^{*}}(\tau_{h}) \right\|^{2}_{2}}}{\sqrt{\lambda}}\right)\left\|\overline{\psi^{*}}(\tau_{h}) \right\|_{(U^{k}_{h})}\]

where step (a) is simply using \(\left\|x\right\|_{2}\leq\left\|x\right\|_{1}\) and step (b) is by the identity [35, Lemma 13]. Note that \(r\) is the rank of the PSR and \(r\geq\mathrm{rank}(\{\widehat{\overline{\psi^{k}}}(\tau_{h})\,:\,\tau_{h}\in \mathbb{H}_{h}\}),\mathrm{rank}(\{\overline{\psi^{*}}(\tau_{h})\,:\,\tau_{h} \in\mathbb{H}_{h}\})\).

Moreover, we have,

\[\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})-\overline{\psi^{*} }(\tau_{h})\right\|_{2} \leq\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})-\overline{ \psi^{*}}(\tau_{h})\right\|_{1}\] \[\overset{(a)}{=} \sum_{q_{h}\in\mathbb{Q}_{h}}\left|\overline{\mathbb{P}}_{\widehat {\theta}^{k}}\left[q_{h}^{o}\mid\tau_{h},q_{h}^{a}\right]-\overline{\mathbb{ P}}_{\theta^{*}}\left[q_{h}^{o}\mid\tau_{h},q_{h}^{a}\right]\right|\] \[\overset{(b)}{\leq} 2\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|Q_{A} \mathbb{D}_{\mathtt{TV}}\left(\mathbb{P}^{\pi_{h-1}^{\mathtt{sup}}}_{\widehat {\theta}^{\mathtt{u}_{h-1}}}(\cdot|\tau_{h}),\mathbb{P}^{\pi_{h-1}^{\mathtt{ sup}}}_{\theta^{*}}(\cdot|\tau_{h})\right),\]

where we used the definition of \(\overline{\psi}\) in (a) and the definition of the \(\mathtt{u}_{h-1}^{\mathtt{exp}}\) in (b).

Now, by Lemma 4, we have,

\[\sqrt{\sum_{h=0}^{H-1}\left\|\widehat{\overline{\psi^{k}}}(\tau_{ h})\right\|^{2}_{(\widehat{U}^{k}_{h})^{-1}}} \leq\frac{1}{\sqrt{\lambda}}\sum_{h=0}^{H-1}\left\|\widehat{ \overline{\psi^{k}}}(\tau_{h})-\overline{\psi^{*}}(\tau_{h})\right\|_{2}+\sum_{h =0}^{H-1}\left(1+\frac{\sqrt{r}\sqrt{\sum_{\tau_{h}\in\mathcal{D}^{k}_{h}} \left\|\widehat{\overline{\psi^{k}}}(\tau_{h})-\overline{\psi^{*}}(\tau_{h}) \right\|^{2}_{2}}}{\sqrt{\lambda}}\right)\left\|\overline{\psi^{*}}(\tau_{h}) \right\|_{(U^{k}_{h})}\] \[\leq\frac{1}{\sqrt{\lambda}}\sum_{h=0}^{H-1}\left\|\widehat{ \overline{\psi^{k}}}(\tau_{h})-\overline{\psi^{*}}(\tau_{h})\right\|_{2}+\left(1+ \frac{2\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|Q_{A}\sqrt{\tau r\beta}}{ \sqrt{\lambda}}\right)\sum_{h=0}^{H-1}\left\|\overline{\psi^{*}}(\tau_{h}) \right\|_{(U^{k}_{h})^{-1}},\]

where the first line is combining the calculations above and the second line is by the estimation guarantee of Lemma 4.

The first term can be bounded in expectation under \(\mathbb{P}^{\pi}_{\theta^{*}}\) for any \(\pi\) as follows,

\[\sum_{h=0}^{H-1}\mathbb{E}_{\tau_{h}\sim\mathbb{P}^{\pi}_{\theta^{*}}} \left[\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})-\overline{\psi^{*}}(\tau_ {h})\right\|_{2}\right] \leq\sum_{h=0}^{H-1}\mathbb{E}_{\tau_{h}\sim\mathbb{P}^{\pi}_{ \theta^{*}}}\left[\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})-\overline{ \psi^{*}}(\tau_{h})\right\|_{1}\right]\] \[\leq\sum_{h=0}^{H-1}\sum_{\tau_{h}}\left\|\widehat{\overline{\psi ^{k}}}(\tau_{h})\left(\mathbb{P}^{\pi}_{\theta^{*}}(\tau_{h})-\mathbb{P}^{\pi} _{\bar{\theta}^{*}}(\tau_{h})\right)+\widehat{\overline{\psi^{k}}}(\tau_{h}) \mathbb{P}^{\pi}_{\bar{\theta}^{*}}(\tau_{h})-\overline{\psi^{*}}(\tau_{h}) \mathbb{P}^{\pi}_{\theta^{*}}(\tau_{h})\right\|_{1}\] \[\stackrel{{(a)}}{{\leq}}\sum_{h=0}^{H-1}\sum_{\tau_ {h}}\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})\right\|_{1}\left|\mathbb{ P}^{\pi}_{\theta^{*}}(\tau_{h})-\mathbb{P}^{\pi}_{\bar{\theta}^{k}}(\tau_{h}) \right|+\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})\mathbb{P}^{\pi}_{\bar{ \theta}^{*}}(\tau_{h})-\overline{\psi^{*}}(\tau_{h})\mathbb{P}^{\pi}_{\theta^{ *}}(\tau_{h})\right\|_{1}\] \[\stackrel{{(b)}}{{\leq}}\sum_{h=0}^{H-1}\sum_{\tau_ {h}}\left(\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})\right\|_{1}\left| \mathbb{P}^{\pi}_{\theta^{*}}(\tau_{h})-\mathbb{P}^{\pi}_{\bar{\theta}^{k}}( \tau_{h})\right|+\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})-\psi^{*}(\tau_ {h})\right\|_{1}\pi(\tau_{h})\right)\] \[\stackrel{{(c)}}{{\leq}}2Q_{A}\sum_{h=0}^{H-1} \mathsf{D_{TV}}\left(\mathbb{P}^{\pi}_{\theta^{*}}(\tau_{h}),\mathbb{P}^{\pi} _{\bar{\theta}^{k}}(\tau_{h})\right)\] \[\stackrel{{(d)}}{{\leq}}2HQ_{A}\mathsf{D_{TV}}\left( \mathbb{P}^{\pi}_{\theta^{*}}(\tau_{h}),\mathbb{P}^{\pi}_{\bar{\theta}^{k}}( \tau_{h})\right),\]

where step (a) is the triangle inequality, step (b) is the definition of \(\overline{\psi}(\tau_{h})\), step (c) is since \(\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})\right\|_{1}\leq\left|\mathbb{Q }_{h}^{A}\right|\leq Q_{A}\) for any \(\tau_{h}\) and the definition of \(\psi(\tau_{h})\), and step (d) is simply \(\mathsf{D_{TV}}\left(\mathbb{P}^{\pi}_{\theta^{*}}(\tau_{h}),\mathbb{P}^{\pi} _{\bar{\theta}^{k}}(\tau_{h})\right)\geq\mathsf{D_{TV}}\left(\mathbb{P}^{\pi} _{\theta^{*}}(\tau_{h}),\mathbb{P}^{\pi}_{\bar{\theta}^{k}}(\tau_{h})\right)\).

Putting this together concludes the proof,

\[\mathbb{E}_{\tau_{H}\sim\mathbb{P}^{\pi}_{\theta^{*}}}\left[ \sqrt{\sum_{h=0}^{H-1}\left\|\widehat{\overline{\psi^{k}}}(\tau_{h})\right\|_{ (\widehat{U}_{h}^{k})^{-1}}^{2}}\right]\] \[\leq\frac{2HQ_{A}}{\sqrt{\lambda}}\mathsf{D_{TV}}\left(\mathbb{P} ^{\pi}_{\theta^{*}}(\tau_{h}),\mathbb{P}^{\pi}_{\bar{\theta}^{k}}(\tau_{h}) \right)+\left(1+\frac{2\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|Q_{A} \sqrt{\gamma_{t}\beta}}{\sqrt{\lambda}}\right)\sum_{h=0}^{H-1}\mathbb{E}_{\tau_ {h}\sim\mathbb{P}^{\pi}_{\theta^{*}}}\left\|\overline{\psi^{*}}(\tau_{h}) \right\|_{(U_{h}^{k})^{-1}}.\]

The following lemma bounds the cumulative estimation error of the probability of trajectories. It can be proved via an \(\ell_{2}\) Eluder argument [38, 76, 77]. A significant portion of the proof is very similar to that of Proposition 4, involving an exchange of \((\widehat{\cdot})\) and \((\cdot)^{*}\). We include the proof for completeness.

**Lemma 8**.: _Under event \(\mathcal{E}\), for any \(h\in\{0,\ldots,H-1\}\), we have_

\[\sum_{k}\mathsf{D_{TV}}\left(\mathbb{P}^{\pi^{k}}_{\theta^{*}}(\tau_{H}), \mathbb{P}^{\pi^{k}}_{\bar{\theta}^{k}}(\tau_{H})\right)\lesssim\frac{\max_{s \in\mathcal{A}}\left|\mathbb{X}_{s}\right|Q_{A}\sqrt{\beta}}{\gamma}\sqrt{rHK \log\left(1+\frac{dQ_{A}K}{\gamma^{4}}\right)}.\]

_Here, \(a\lesssim b\) indicates that there is an absolute positive constant c s.t. \(a\leq c\cdot b\)._

Proof.: Recall that, by the first inequality in Proposition 4, we have:

\[\mathsf{D_{TV}}\left(\mathbb{P}^{\pi^{k}}_{\theta^{*}}(\tau_{H}), \mathbb{P}^{\pi^{k}}_{\bar{\theta}^{k}}(\tau_{H})\right)\leq\sum_{h=1}^{H} \sum_{\tau_{H}}\left|\widehat{m}^{k}(\omega_{h})^{\top}\left(\widehat{M}^{k}_{h}(x _{h})-M^{*}_{h}(x_{h})\right)\psi^{*}(\tau_{h-1})\right|\pi^{k}(\tau_{H})\]

This is very similar to the inequality in Lemma 5, with the difference being that the quantities associated with the estimated model and the true model are exchanged. Since both correspond to a PSR, the analysis follows a similar series of steps. We will use analogous notation to Lemma 5. We index the future trajectory \(\omega_{h-1}=(x_{h},\ldots,x_{H})\) by \(i\) and history trajectory \(\tau_{h-1}=(x_{1},\ldots,x_{h-1})\) by \(j\). We denote \(\widehat{m}^{k}(\omega_{h})^{\top}\left(\widehat{M}^{k}_{h}(x_{h})-M^{*}_{h}(x_{h })\right)\) as \(w_{i}\), \(\overline{\psi}^{*}(\tau_{h-1})\) as \(x_{j}\), and \(\pi(\omega_{h-1}|\tau_{h-1})\) as \(\pi_{i|j}\).

Define the matrix,

\[\Lambda_{h}^{k}=\lambda_{0}I+\sum_{t<k}\mathbb{E}_{j\sim\mathbb{P}_{ \theta^{*}}^{\pi^{t}}}\left[x_{j}x_{j}^{\top}\right]\]

where \(\lambda_{0}\) is a constant to be determined later.

For any policy \(\pi\), using a similar calculation as in Lemma 5, we have,

\[\sum_{\tau_{H}}\left|\widehat{m}^{k}(\omega_{h})^{\top}\left( \widehat{M}_{h}^{k}(x_{h})-M_{h}^{\star}(x_{h})\right)\psi^{\star}(\tau_{h-1}) \right|\pi^{k}(\tau_{H})\] \[=\mathbb{E}_{j\sim\mathbb{P}_{\theta^{*}}^{\pi^{t}}}\left[\sum_{i }\pi_{i|j}\left|w_{i}^{\top}x_{j}\right|\right]\] \[=\mathbb{E}_{j\sim\mathbb{P}_{\theta^{*}}^{\pi^{k}}}\left[\left( \sum_{i}\pi_{i|j}\mathrm{sign}(w_{i}^{\top}x_{j})w_{i}\right)^{\top}x_{j}\right]\] \[\leq\mathbb{E}_{j\sim\mathbb{P}_{\theta^{*}}^{\pi^{k}}}\left[\|x_ {j}\|_{\Lambda_{h}^{1}}\left\|\sum_{i}\pi_{i|j}\mathrm{sign}(w_{i}^{\top}x_{j} )w_{i}\right\|_{\Lambda_{h}}\right]\]

where the last line is the Cauchy-Schwarz inequality.

Fix \(j=j_{0}\) and consider the term: \(\left\|\sum_{i}\pi_{i|j_{0}}\mathrm{sign}(w_{i}^{\top}x_{j_{0}})w_{i}\right\|_ {\Lambda_{h}}\) in the above. This term can be partitioned in the same manner as in Lemma 5 by simply using the definition of \(\Lambda_{h}\) and expanding,

\[\left\|\sum_{i}\pi_{i|j_{0}}\mathrm{sign}(w_{i}^{\top}x_{j_{0}})w _{i}\right\|_{\Lambda_{h}}^{2}\] \[=\underbrace{\lambda_{0}\left\|\sum_{i}\pi_{i|j_{0}}\cdot\mathrm{ sign}(w_{i}^{\top}x_{j_{0}})\cdot w_{i}\right\|_{I_{1}}^{2}}_{I_{1}}+ \underbrace{\sum_{t<k}\mathbb{E}_{j\sim\mathbb{P}_{\theta^{*}}^{\pi^{k}}}\left[ \left(\sum_{i}\pi_{i|j_{0}}\cdot\mathrm{sign}(w_{i}^{\top}x_{j_{0}})\cdot w_{ i}^{\top}x_{j}\right)^{2}\right]}_{I_{2}}.\]

We bound each term separately. The process is nearly identical to the proof of Lemma 5, but we show it for completeness.

\(\sqrt{I_{1}}\) is bounded by the sum of two terms,

\[\sqrt{I_{1}} =\sqrt{\lambda_{0}}\max_{z\in\mathbb{R}^{d_{h-1}}:\left\|z\right\| _{2}=1}\left|\sum_{i}\pi_{i|j_{0}}\cdot\mathrm{sign}(w_{i}^{\top}x_{j_{0}}) \cdot w_{i}^{\top}z\right|\] \[\overset{(a)}{\leq}\sqrt{\lambda_{0}}\max_{\left\|z\right\|_{2}= 1}\sum_{\omega_{h-1}}\left|\widehat{m}^{k}(\omega_{h})^{\top}\Big{(}\widehat{M }_{h}^{k}(x_{h})-M_{h}^{\star}(x_{h})\Big{)}z\right|\pi(\omega_{h-1}|j_{0})\] \[\overset{(b)}{\leq}\sqrt{\lambda_{0}}\max_{\left\|z\right\|_{2}= 1}\sum_{\omega_{h-1}}\left|\widehat{m}^{k}(\omega_{h})^{\top}\widehat{M}_{h}^ {k}(x_{h})z\right|\pi(\omega_{h-1}|j_{0})\] \[\quad+\sqrt{\lambda_{0}}\max_{\left\|z\right\|_{2}=1}\sum_{\omega _{h-1}}\left|\widehat{m}^{k}(\omega_{h})^{\top}M_{h}^{\star}(x_{h})z\right|\pi( \omega_{h-1}|j_{0}),\]

where step (a) is the definition of \(w_{i},\pi_{i|j_{0}}\), and the triangle inequality, and step (b) is the triangle inequality.

Both terms can be bounded by the \(\gamma\)-well-conditioning assumption on \(\widehat{\theta}^{k}\) and \(\theta^{*}\). Consider the first term,

\[\max_{\left\|z\right\|_{2}=1} \sum_{\omega_{h-1}}\left|\widehat{m}^{k}(\omega_{h})^{\top} \widehat{M}_{h}^{k}(x_{h})z\right|\pi(\omega_{h-1}|j_{0})\] \[=\max_{\left\|z\right\|_{2}=1} \sum_{x_{h}}\left(\sum_{\omega_{h}}\left|\widehat{m}^{k}(\omega_{h})^{\top} \widehat{M}_{h}^{k}(x_{h})z\right|\pi(\omega_{h}|j_{0},x_{h})\right)\pi(x_{h}| j_{0})\] \[\stackrel{{(a)}}{{\leq}} \max_{\left\|z\right\|_{2}=1} \sum_{x_{h}}\frac{1}{\gamma}\left\|\widehat{M}_{h}^{k}(x_{h})z\right\|_{1}\pi( x_{h}|j_{0})\] \[\stackrel{{(b)}}{{\leq}} \frac{1}{\gamma}\max_{\left\|z\right\|_{2}=1}\frac{\left|\mathbb{Q}_{h +1}^{A}\right|\left\|z\right\|_{1}}{\gamma}\] \[\stackrel{{(c)}}{{\leq}} \frac{\sqrt{d}Q_{A}}{\gamma^{2}}\]

where step (a) is by the first condition in Assumption 1, step (b) is by the second condition of Assumption 1, and step (c) is by the fact that \(\max_{z\in\mathbb{R}^{d_{h-1}}:\left\|z\right\|_{2}=1}\left\|z\right\|_{1}= \sqrt{d_{h-1}}\leq\sqrt{d}\) and \(\left|\mathbb{Q}_{h+1}^{A}\right|\leq Q_{A}\). In the above, note that we used the \(\gamma\)-well-conditioning of PSR \(\widehat{\theta}^{k}\) in both step (a) and step (b). The second term in \(\sqrt{I_{1}}\) admits an identical bound, simply by using the well-conditioning of the PSR \(\widehat{\theta}^{k}\) in the first step and \(\theta^{*}\) in the second step. Hence, we have that

\[I_{1}\leq 4\frac{\lambda_{0}dQ_{A}^{2}}{\gamma^{4}}.\] (32)

Now, we consider the term \(I_{2}\)

\[I_{2} \leq\sum_{t<k}\mathbb{E}_{\tau_{h-1}\sim\mathbb{P}_{\theta^{*}}^ {k}}\left[\left(\sum_{\omega_{h-1}}\left|\widehat{m}^{k}(\omega_{h})^{\top} \left(\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\star}(x_{h})\right)\overline{\psi}^{ *}(\tau_{h-1})\right|\pi(\omega_{h-1}|j_{0})\right)^{2}\right]\] \[\leq\sum_{t<k}\mathbb{E}_{j\sim\mathbb{P}_{\theta^{*}}^{k}}\left[ \left(\sum_{\omega_{h-1}}\left|\widehat{m}^{k}(\omega_{h})^{\top}\widehat{M}_ {h}(x_{h})\left(\overline{\psi}^{*}(\tau_{h-1})-\widehat{\overline{\psi}}^{k} (\tau_{h-1})\right)\right|\pi(\omega_{h-1}|j_{0})\right.\right.\] \[\left.\left.+\underbrace{\sum_{\omega_{h-1}}\left|\widehat{m}^{k }(\omega_{h})^{\top}\left(\widehat{M}_{h}^{k}(x_{h})\widehat{\overline{\psi}} ^{k}(\tau_{h-1})-M_{h}^{\star}(x_{h})\overline{\psi}^{*}(\tau_{h-1})\right) \right|\pi(\omega_{h-1}|j_{0})}_{I_{4}}\right)^{2}\right]\] \[=:\sum_{t<k}\mathbb{E}_{j\sim\mathbb{P}_{\theta^{*}}^{k}}(I_{3}+I _{4})^{2}\]

where the line follows by the fact that \(x\leq|x|\) and the line follows from the triangle inequality by adding and subtracting \(\widehat{m}_{h}(\omega_{h})\widehat{M}_{h}(x_{h})\widehat{\overline{\psi}}^{ k}\)\((\tau_{h-1})\) inside the absolute value. We now bound each of \(I_{3}\) and \(I_{4}\).

First, we bound \(I_{3}\) as follows,

\[I_{3} =\sum_{\omega_{h-1}}\left|\widehat{m}_{h}^{k}(\omega_{h})^{\top} \widehat{M}_{h}(x_{h})\left(\overline{\psi}^{*}(\tau_{h-1})-\widehat{\overline{ \psi}}^{k}(\tau_{h-1})\right)\right|\pi(\omega_{h-1}|j_{0})\] \[\stackrel{{(a)}}{{=}} \sum_{\omega_{h-1}}\left|\widehat{m}_{h-1}^{k}(\omega_{h-1})^{ \top}\left(\overline{\psi}^{*}(\tau_{h-1})-\widehat{\overline{\psi}}^{k}(\tau_ {h-1})\right)\right|\pi(\omega_{h-1}|j_{0})\] \[\stackrel{{(b)}}{{\leq}} \frac{1}{\gamma}\left\|\overline{\psi}^{k}(\tau_{h-1})-\overline{\psi}^{ *}(\tau_{h-1})\right\|_{1}\] \[=\frac{1}{\gamma}\sum_{q_{h-1}\in\mathbb{Q}_{h-1}}\left|\overline {\mathbb{P}_{\widehat{\theta}^{k}}}\left[q_{h-1}\mid\tau_{h-1}\right]-\mathbb{ P}_{\theta^{*}}\left[q_{h-1}\mid\tau_{h-1}\right]\right|,\]

[MISSING_PAGE_EMPTY:51]

Now, combining the bound on \(I_{1}\) and \(I_{2}\) allows us to finally bound \(\left\|\sum_{i}\pi_{\mathrm{i}|j}\mathrm{sign}(w_{i}^{\top}x_{j})w_{i}\right\|_{ \Lambda_{h}}^{2}\) as follows,

\[\left\|\sum_{i}\pi_{\mathrm{i}|j}\mathrm{sign}(w_{i}^{\top}x_{j})w _{i}\right\|_{\Lambda_{h}}^{2}\] \[\leq\frac{4\lambda_{0}Q_{A}^{2}d}{\gamma^{4}}+\frac{4\max_{s\in \mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A}^{2}}{\gamma^{2}}\cdot\sum_{t< k}\mathtt{D}_{\mathbb{H}}^{2}\left(\mathbb{P}_{\widehat{\theta}^{k}}^{\nu_{h}( \pi^{t},u_{h-1}^{\mathrm{aug}})}\left(\tau_{H}\right),\mathbb{P}_{\theta^{*}}^ {\nu_{h}(\pi^{t},u_{h-1}^{\mathrm{aug}})}\left(\tau_{H}\right)\right)\] \[=:\left(\tilde{\alpha}_{h-1}^{k}\right)^{2},\]

We choose \(\lambda_{0}=\frac{\gamma^{4}}{4Q_{A}^{2}d}\), and bound \(\tilde{\alpha}^{2}\coloneqq\sum_{h}\left(\tilde{\alpha}_{h-1}^{k}\right)^{2}\) as follows,

\[\sum_{h}\left(\tilde{\alpha}_{h-1}^{k}\right)^{2} =H+\frac{4\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q _{A}^{2}\beta}{\gamma^{2}}\sum_{\pi\in\mathcal{D}^{k}}\mathtt{D}_{\mathbb{H}}^ {2}\left(\mathbb{P}_{\widehat{\theta}^{k}}^{\nu_{h}(\pi^{t},u_{h-1}^{\mathrm{ aug}})}\left(\tau_{H}\right),\mathbb{P}_{\theta^{*}}^{\nu_{h}(\pi^{t},u_{h-1}^{ \mathrm{aug}})}\left(\tau_{H}\right)\right)\] \[\leq H+\frac{28\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^ {2}Q_{A}^{2}\beta}{\gamma^{2}}\] \[\lesssim\frac{\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{ 2}Q_{A}^{2}\beta}{\gamma^{2}},\]

where the second line is by the estimation guarantee of Lemma 4.

Thus, we have,

\[\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\theta^{*}}^{\pi^{k}}( \tau_{H}),\mathbb{P}_{\widehat{\theta}^{k}}^{\pi^{k}}(\tau_{H})\right) \leq\sum_{h=1}^{H}\sum_{\tau_{H}}\left|\widehat{m}^{k}(\omega_{h}) ^{\top}\left(\widehat{M}_{h}^{k}(x_{h})-M_{h}^{*}(x_{h})\right)\psi^{*}(\tau_{ h-1})\right|\pi^{k}(\tau_{H})\] \[\leq\sum_{h=1}^{H}\mathbb{E}_{\tau_{h-1}\sim\mathbb{P}_{\theta^{*} }^{\pi^{k}}}\left[\left\|\overline{\psi}^{*}(\tau_{h-1})\right\|_{\Lambda_{h} ^{\dagger}}\left\|\sum_{i}\pi_{\mathrm{i}|j}\mathrm{sign}(w_{i}^{\top}x_{j})w _{i}\right\|_{\Lambda_{h}}\right]\] \[\leq\mathbb{E}_{\tau_{h-1}\sim\mathbb{P}_{\theta^{*}}^{\pi^{k}}} \left[\sum_{h=1}^{H}\left\|\overline{\psi}^{*}(\tau_{h-1})\right\|_{\Lambda_{h }^{\dagger}}\left\|\sum_{i}\pi_{\mathrm{i}|j}\mathrm{sign}(w_{i}^{\top}x_{j}) w_{i}\right\|_{\Lambda_{h}}\right]\] \[\overset{(a)}{\leq}\tilde{\alpha}\cdot\mathbb{E}_{\tau_{h-1}\sim \mathbb{P}_{\theta^{*}}^{\pi^{k}}}\left[\sqrt{\sum_{h=1}^{H}\left\|\overline{ \psi}^{*}(\tau_{h-1})\right\|_{\Lambda_{h}^{\dagger}}^{2}}\sqrt{\sum_{h=1}^{H} \left\|\sum_{i}\pi_{\mathrm{i}|j}\mathrm{sign}(w_{i}^{\top}x_{j})w_{i}\right\|_ {\Lambda_{h}}^{2}}\right]\] \[\overset{(b)}{\leq}\tilde{\alpha}\cdot\mathbb{E}_{\tau_{h-1}\sim \mathbb{P}_{\theta^{*}}^{\pi^{k}}}\left[\sqrt{\sum_{h=1}^{H}\left\|\overline{ \psi}^{*}(\tau_{h-1})\right\|_{\Lambda_{h}^{\dagger}}^{2}}\right]\] \[\leq\tilde{\alpha}\cdot\sqrt{\sum_{h=1}^{H}\mathbb{E}_{\tau_{h-1} \sim\mathbb{P}_{\theta^{*}}^{\pi^{k}}}\left[\left\|\overline{\psi}^{*}(\tau_{h -1})\right\|_{\Lambda_{h}^{\dagger}}^{2}\right]},\]

where step (a) is by the Cauchy-Schwarz inequality and step (b) is by the bound established above. Since the total variation distance is bounded above by 2, we have

\[\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\theta^{*}}^{\pi^{k}}(\tau_{H}), \mathbb{P}_{\widehat{\theta}^{k}}^{\pi^{k}}(\tau_{H})\right)\leq\min\left\{ \tilde{\alpha}\cdot\sqrt{\sum_{h=1}^{H}\mathbb{E}_{\tau_{h-1}\sim\mathbb{P}_{ \theta^{*}}^{\pi^{k}}}\left[\left\|\overline{\psi}^{*}(\tau_{h-1})\right\|_{ \Lambda_{h}^{\dagger}}^{2}\right]},2\right\}.\]Finally, the proof is completed by summing over \(k\) using the elliptical potential lemma as follows,

\[\sum_{k=1}^{K}\mathsf{D_{TV}}\left(\mathbb{P}_{\theta^{*}}^{\mathbb{ P}^{k}}(\tau_{H}),\mathbb{P}_{\bar{\theta}^{k}}^{\mathbb{P}^{k}}(\tau_{H})\right) \leq\sum_{k=1}^{K}\min\left\{\tilde{\alpha}\cdot\sqrt{\sum_{h=1}^ {H}\mathbb{E}_{\tau_{h-1}\sim\mathbb{P}_{\theta^{*}}^{\mathbb{P}^{k}}}\left[ \left\|\overline{\psi}^{*}(\tau_{h-1})\right\|_{\lambda_{h}^{1}}^{2}\right]}^{2 },2\right\}\] \[\stackrel{{(a)}}{{\leq}}\sqrt{K}\sqrt{\sum_{k=1}^{K }\sum_{h=1}^{H}\min\left\{\tilde{\alpha}^{2}\cdot\mathbb{E}_{\tau_{h-1}\sim \mathbb{P}_{\theta^{*}}^{\mathbb{P}^{k}}}\left[\left\|\overline{\psi}^{*}(\tau _{h-1})\right\|_{\lambda_{h}^{1}}^{2}\right],4\right\}}\] \[\leq\sqrt{K}\tilde{\alpha}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\min \left\{\mathbb{E}_{\tau_{h-1}\sim\mathbb{P}_{\theta^{*}}^{\mathbb{P}^{k}}} \left[\left\|\overline{\psi}^{*}(\tau_{h-1})\right\|_{\lambda_{h}^{1}}^{2} \right],4/\tilde{\alpha}^{2}\right\}}\] \[\stackrel{{(b)}}{{\leq}}\sqrt{K}\bar{H}\tilde{ \alpha}\sqrt{(1+4/\tilde{\alpha}^{2})r\log(1+K/\lambda_{0})}\] \[\stackrel{{(c)}}{{\lesssim}}\frac{\max_{s\in\mathcal{ A}}\left\|\mathbb{X}_{s}\right\|Q_{A}}{\gamma}\sqrt{rKH\beta\log(1+K/\lambda_{0})}\] \[\stackrel{{(d)}}{{\lesssim}}\frac{\max_{s\in \mathcal{A}}\left\|\mathbb{X}_{s}\right\|Q_{A}}{\gamma}\sqrt{rKH\beta\log(1+dQ _{A}K/\gamma)}.\]

Here, step (a) is uses the relationship between the \(\ell_{1}\) and \(\ell_{2}\) norms \(\left\|\cdot\right\|_{1}\leq\sqrt{d}\left\|\cdot\right\|_{2}\). Step (b) is by the elliptical potential lemma ([35, Lemma 14]; see also [96, 97, 98]). Step (c) uses the bound on \(\tilde{\alpha}\) established above and the fact that \(\sqrt{1+4/\tilde{\alpha}^{2}}\) is bounded by an absolute constant. Step (d) uses the definition of \(\lambda_{0}\) and the fact that \(\sqrt{28(1+4/\tilde{\alpha}^{2})}\) is bounded by an absolute constant. 

Using the two lemmas above, we are now ready to show that \(\sum_{k=1}^{K}V_{\hat{\theta}^{k}}^{\widehat{\mathbb{P}}^{k}}(\pi^{k})=O(\sqrt {K})\). The argument is identical to [35, Lemma 6] and does not require modification for generalized PSRs. We recount the argument for completeness.

**Lemma 9**.: _Under the event \(\mathcal{E}\), with probability at least \(1-\delta\), we have:_

\[\sum_{k=1}^{K}V_{\hat{\theta}^{k}}^{\widehat{\mathbb{P}}^{k}}(\pi^{k})\lesssim \left(\sqrt{r}+\frac{Q_{A}\sqrt{H}}{\gamma}\right)\frac{\max_{s\in\mathcal{A}} ^{2}Q_{A}^{2}H\sqrt{drH\beta K\beta_{0}}}{\gamma^{2}}\]

_where \(\beta_{0}=\max\{\log(1+K/\lambda),\log(1+dQ_{A}K/\gamma)\}\), and \(\lambda=\frac{\gamma\max_{s\in\mathcal{A}}\left\|\mathbb{X}_{s}\right\|Q_{A} \beta\max\{\sqrt{r},Q_{A}\sqrt{H}/\gamma\}}{\sqrt{dH}}\)_

Proof.: First, we note that,

\[V_{\hat{\theta}^{k}}^{\widehat{\mathbb{P}}^{k}}(\pi^{k})=\sum_{\tau}\mathbb{ P}_{\hat{\theta}^{k}}^{\mathbb{P}^{k}}(\tau)\widehat{b}^{k}(\tau)=\sum_{\tau} \mathbb{P}_{\hat{\theta}^{*}}^{\mathbb{P}^{k}}(\tau)\widehat{b}^{k}(\tau)+ \sum_{\tau}(\mathbb{P}_{\hat{\theta}^{*}}^{\mathbb{P}^{k}}(\tau)-\mathbb{P}_{ \theta^{*}}^{\mathbb{P}^{k}}(\tau))\widehat{b}^{k}(\tau)\leq V_{\theta^{*}}^{ \widehat{\mathbb{P}}^{k}}(\pi^{k})+\mathsf{D_{TV}}\left(\mathbb{P}_{\hat{ \theta}^{k}}^{\mathbb{P}^{k}},\mathbb{P}_{\theta^{*}}^{\mathbb{P}^{k}}\right),\]

where we recall that \(\widehat{b}^{k}(\cdot)\in[0,1]\). Hence, we may focus on bounding the value of \(\widehat{b}^{k}\) under the true model \(\theta^{*}\) and use the bound on the cumulative total variation estimation error established in Lemma 8.

Recall the definition of the bonus term,

\[\widehat{b}^{k}(\tau_{H})\coloneqq\min\left\{\alpha\sqrt{\sum_{h}\left\| \widehat{\overline{\psi}}^{k}(\tau_{h})\right\|_{(\widehat{D}_{h}^{k})^{-1}}},1 \right\},\]

which is defined in terms of the estimated prediction features \(\widehat{\overline{\psi}}\). Recall also that in Lemma 7 we established a bound on the expectation of the prediction features under the true model, which corresponds to \(V_{\theta^{*}}^{\widehat{b}^{k}}\). Hence, we proceed to bound \(\sum_{k}V_{\theta^{*}}^{\widehat{b}^{k}}(\pi^{k})\) as follows,

\[\sum_{k}V_{\theta^{*}}^{\widehat{b}^{k}}(\pi^{k})\] \[=\sum_{k}\mathbb{E}_{\tau_{H}\sim\mathbb{P}_{\theta^{*}}^{\pi^{k}} }\left[\widehat{b}^{k}(\tau_{H})\right]\] \[=\sum_{k}\mathbb{E}_{\tau_{H}\sim\mathbb{P}_{\theta^{*}}^{\pi^{k}} }\left[\min\left\{\alpha\sqrt{\sum_{h}\left\|\widehat{\psi}^{k}(\tau_{h}) \right\|^{2}},1\right\}\right]\] \[\overset{(a)}{\leq}\sum_{k=1}^{K}\min\left\{\alpha\left(1+\frac{2 \max_{s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A}\sqrt{\tau_{H}\beta}}{\sqrt{\lambda }}\right)\sum_{h=0}^{H-1}\mathbb{E}_{\tau_{H}\sim\mathbb{P}_{\theta^{*}}^{k}} \left[\left\|\overline{\psi}^{*}(\tau_{h})\right\|_{(U_{h}^{k})^{-1}}\right] +\sum_{k=1}^{K}\frac{\alpha HQ_{A}}{\sqrt{\lambda}}\mathsf{D}_{\mathsf{TV}} \left(\mathbb{P}_{\theta^{*}}^{\pi^{k}},\mathbb{P}_{\widehat{\theta}^{k}}^{ \pi^{k}}\right),1\right\}\] \[\overset{(b)}{\leq}\underbrace{\sum_{k=1}^{K}\min\left\{\alpha \left(1+\frac{2\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A}\sqrt{\tau_{H}\beta} }{\sqrt{\lambda}}\right)\sum_{h=0}^{H-1}\mathbb{E}_{\tau_{H}\sim\mathbb{P}_{ \theta^{*}}^{k}}\left[\left\|\overline{\psi}^{*}(\tau_{h})\right\|_{(U_{h}^{k })^{-1}}\right],1\right\}}_{I_{1}}+\sum_{k=1}^{K}\frac{\alpha HQ_{A}}{\sqrt{ \lambda}}\mathsf{D}_{\mathsf{TV}}\left(\mathbb{P}_{\theta^{*}}^{\pi^{k}}, \mathbb{P}_{\widehat{\theta}^{k}}^{\pi^{k}}\right),\]

where step (a) is by Lemma 7 and step (b) is since \(\min(a+b,c)\leq\min(a,c)+b\) when \(a,b,c\) are non-negative.

Next, we bound the term \(I_{1}\). Recall the definition of \(U_{h}^{k}\coloneqq\lambda I+\sum_{\tau_{h}\in\mathcal{D}_{h}^{k}}\overline{ \psi}^{*}(\tau_{h})\overline{\psi}^{*}(\tau_{h})^{\top}\). Also, note that the process

\[\left(\mathbb{E}_{\tau_{h}\sim\mathbb{P}_{\theta^{*}}^{\pi^{k}}}\left[\left\| \overline{\psi}^{*}(\tau_{h})\right\|_{(U_{h}^{k})^{-1}}\right]-\left\| \overline{\psi}^{*}(\tau_{h}^{k+1,h+1})\right\|_{(\widehat{U}_{h}^{k})^{-1}} \right)_{k=1}^{K}\]

is a martingale. Hence, by the Azuma-Hoeffding inequality, we have that with probability at least \(1-\delta\),

\[I_{1} \leq\sqrt{2K\log(2/\delta)}+\sum_{k=1}^{K}\min\left\{\alpha \left(1+\frac{2\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A}\sqrt{\tau_{H}\beta }}{\sqrt{\lambda}}\right)\sum_{h=0}^{H-1}\left\|\overline{\psi}^{*}(\tau_{h}^{ k+1,h+1})\right\|_{(U_{h}^{k})^{-1}},1\right\}\] \[\lesssim\sqrt{2K\log(2/\delta)}+\alpha\left(1+\frac{2\max_{s\in \mathcal{A}}|\mathbb{X}_{s}|Q_{A}\sqrt{\tau_{H}\beta}}{\sqrt{\lambda}}\right)H \sqrt{rK\log(1+K/\lambda)}\] \[\lesssim\alpha\left(1+\frac{\max_{s\in\mathcal{A}}|\mathbb{X}_{s} |Q_{A}\sqrt{\tau_{H}\beta}}{\sqrt{\lambda}}\right)H\sqrt{rK\log(1+K/\lambda)}\]

where the second line is by the Elliptical potential lemma ([35, Lemma 14]; see also [96, 97, 98]).

We now return to bounding \(\sum_{k=1}^{K}V_{\widehat{\theta}^{k}}^{\widehat{b}^{k}}(\pi^{k})\). For convenience we define \(\beta_{0}\coloneqq\max\{\log(1+K/\lambda),\log(1+dQ_{A}K/\gamma)\}\) and we choose \(\lambda\) as follows,

\[\lambda=\frac{\gamma\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|^{2}\,Q_{A}\beta\max \{\sqrt{\tau},Q_{A}\sqrt{H}/\gamma\}}{\sqrt{dH}}.\]We have,

\[\sum_{k=1}^{K}V_{\widehat{\theta}^{k}}^{\widehat{\beta}^{k}}(\pi^{k}) \leq\sum_{k=1}^{K}V_{\theta^{*}}^{\widehat{\beta}^{k}}(\pi^{k})+\sum _{k=1}^{K}\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\widehat{\theta}^{*}}^{ \pi^{k}},\mathbb{P}_{\theta^{*}}^{\pi^{k}}\right)\] \[\leq I_{1}+\left(1+\frac{\alpha HQ_{A}}{\sqrt{\lambda}}\right)\sum _{k=1}^{K}\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\theta^{*}}^{\pi^{k}}, \mathbb{P}_{\widehat{\theta}^{*}}^{\pi^{k}}\right)\] \[\stackrel{{(a)}}{{\lesssim}}\alpha\left(1+\frac{ \max_{s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A}\sqrt{\tau r\beta}}{\sqrt{\lambda}} \right)H\sqrt{rK\beta_{0}}+\frac{\alpha H}{\sqrt{\lambda}}\frac{Q_{A}^{2}\max_ {s\in\mathcal{A}}|\mathbb{X}_{s}|\sqrt{\beta}}{\gamma}\sqrt{rHK\beta_{0}}\] \[=\alpha\left(1+\frac{\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A} \sqrt{\tau r\beta}}{\sqrt{\lambda}}+\frac{\max_{s\in\mathcal{A}}|\mathbb{X}_{ s}|Q_{A}^{2}\sqrt{H\beta}}{\gamma\sqrt{\lambda}}\right)H\sqrt{rK\beta_{0}}\] \[\stackrel{{(b)}}{{\lesssim}}\left(\frac{Q_{A}\sqrt{ Hd\lambda}}{\gamma^{2}}+\frac{\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A} \sqrt{\beta}}{\gamma}\right)\left(1+\frac{\max_{s\in\mathcal{A}}|\mathbb{X}_{s} |Q_{A}\sqrt{\beta}\max\{\sqrt{r},Q_{A}\sqrt{H}/\gamma\}}{\sqrt{\lambda}}\right) H\sqrt{rK\beta_{0}}\] \[=\left(1+\frac{\sqrt{dH\lambda}}{\max_{s\in\mathcal{A}}|\mathbb{X }_{s}|\sqrt{\beta}\gamma}\right)\left(1+\frac{\max_{s\in\mathcal{A}}|\mathbb{X }_{s}|Q_{A}\sqrt{\beta}\max\{\sqrt{r},Q_{A}\sqrt{H}/\gamma\}}{\sqrt{\lambda}} \right)\frac{\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A}H\sqrt{rK\beta\beta_{ 0}}}{\gamma}\] \[\stackrel{{(c)}}{{=}}\left(1+\sqrt{\frac{Q_{A}\max\{ \sqrt{r},Q_{A}\sqrt{H}/\gamma\}\sqrt{dH}}{\gamma}}\right)^{2}\frac{\max_{s\in \mathcal{A}}|\mathbb{X}_{s}|Q_{A}H\sqrt{rK\beta\beta_{0}}}{\gamma}\] \[\lesssim\left(1+\frac{Q_{A}\sqrt{dH}\max\left\{\sqrt{r},Q_{A} \sqrt{H}/\gamma\right\}}{\gamma}\right)\frac{\max_{s\in\mathcal{A}}|\mathbb{X }_{s}|Q_{A}H\sqrt{rK\beta\beta_{0}}}{\gamma}\] \[\leq\left(\sqrt{r}+\frac{Q_{A}\sqrt{H}}{\gamma}\right)\frac{\max_ {s\in\mathcal{A}}|\mathbb{X}_{s}|Q_{A}^{2}H\sqrt{rdHK\beta\beta_{0}}}{\gamma^ {2}},\]

where step (a) is by Lemma 8 and the bound on \(I_{1}\) established above, step (b) uses the definition of \(\alpha\) and the fact that \(\alpha\lesssim\frac{Q_{A}\sqrt{Hd\lambda}}{\gamma^{2}}+\frac{\max_{s\in \mathcal{A}}|\mathbb{X}_{s}|Q_{A}\sqrt{\beta}}{\gamma}\), and step (c) is by plugging in the choice of \(\lambda\). 

### Proof of Theorem 4

**Theorem** (Restatement of Theorem 4).: _Suppose Assumption 1 holds. Let \(p_{\min}=O\left(\frac{\delta}{KH\prod_{k=1}^{n}|\mathbb{X}_{h}|}\right)\), \(\lambda=\frac{\gamma\max_{s\in\mathcal{A}}|\mathbb{X}_{s}|^{2}Q_{A}\beta\max \left\{\sqrt{r};Q_{A}\sqrt{H}/\gamma\right\}}{\sqrt{dH}}\), \(\alpha=O\left(\frac{Q_{A}\sqrt{Hd}}{\gamma^{2}}\sqrt{\lambda}+\frac{\max_{s\in \mathcal{A}}|\mathbb{X}_{s}|Q_{A}\sqrt{\beta}}{\gamma}\right)\), and let \(\beta=O(\log\left|\overline{\Theta}_{\varepsilon}\right|)\), where \(\varepsilon=O(\frac{p_{\min}}{KH})\). Then, with probability at least \(1-\delta\), Algorithm 1 returns a model \(\theta^{\epsilon}\) and a policy \(\pi\) that satisfy_

\[V_{\theta^{\epsilon}}^{R}(\pi^{*})-V_{\theta^{\epsilon}}^{R}(\pi)\leq \varepsilon,\text{ and }\forall\pi,\;\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\theta^{*}}^{\pi}(\tau_{ H}),\mathbb{P}_{\theta^{*}}^{\pi}(\tau_{H})\right)\leq\varepsilon.\]

_In addition, the algorithm terminates with a sample complexity of,_

\[\tilde{O}\left(\left(r+\frac{Q_{A}^{2}H}{\gamma^{2}}\right)\frac{rdH^{3}\max_ {s\in\mathcal{A}}|\mathbb{X}_{s}|^{2}\,Q_{A}^{4}\beta}{\gamma^{4}\epsilon^{2}} \right).\]

Proof.: By Propositions 6 to 8, the event \(\mathcal{E}\) occurs with high probability, \(\mathbb{P}\left[\mathcal{E}\right]\geq 1-3\delta\). Suppose \(\mathcal{E}\) holds. Then, by the upper confidence bound established in Corollary 4, if Algorithm 1 terminates, then the following must hold,

\[\forall\pi,\;\mathtt{D}_{\mathtt{TV}}\left(\mathbb{P}_{\theta^{\epsilon}}^{ \pi}(\tau_{H}),\mathbb{P}_{\theta^{*}}^{\pi}(\tau_{H})\right)=2\max_{R}\left|V_{ \theta^{\epsilon}}^{R}(\pi)-V_{\theta^{*}}^{R}(\pi)\right|\leq V_{\theta^{ \epsilon}}^{\widehat{\beta}^{\epsilon}}(\pi)\leq\epsilon,\]

where the maximization is over reward functions \(R:\mathbb{H}_{H}\rightarrow[0,1]\). The last inequality is simply the termination condition of Algorithm 1.

Now, the difference between the optimal value and the value of \(\pi\) (the policy returned by the algorithm) can be bounded as follows,

\[V^{R}_{\theta^{*}}(\pi^{*})-V^{R}_{\theta^{*}}(\pi) =V^{R}_{\theta^{*}}(\pi^{*})-V^{R}_{\theta^{*}}(\pi^{*})+V^{R}_{ \theta^{*}}(\pi^{*})-V^{R}_{\theta^{*}}(\pi)+V^{R}_{\theta^{*}}(\pi)-V^{R}_{ \theta^{*}}(\pi)\] \[\leq 2\max_{\pi}V^{\widehat{b}^{*}}_{\theta^{*}}(\pi)\leq\epsilon,\]

where the inequality follows from the fact that \(\pi=\arg\max_{\pi}V^{R}_{\theta^{*}}(\pi)\) and by Corollary 4.

Recall that by Lemma 9, we have,

\[\sum_{k=1}^{K}V^{\pi^{k}}_{\widetilde{\theta}^{*},\widetilde{b}^{k}}\lesssim \left(\sqrt{r}+\frac{Q_{A}\sqrt{H}}{\gamma}\right)\frac{\max_{s\in\mathcal{A}} \left|\mathbb{X}_{s}\right|Q_{A}^{2}H\sqrt{rdHK\beta\beta_{0}}}{\gamma^{2}}.\]

By the pigeon-hole principle and the termination condition of Algorithm 1, the algorithm must terminate within

\[K=\tilde{O}\left(\left(r+\frac{Q_{A}^{2}H}{\gamma^{2}}\right)\frac{rdH^{2}Q_{ A}^{4}\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}\beta}{\gamma^{4} \epsilon^{2}}\right)\]

episodes. Since each episode contains \(H\) iterations, this implies a sample complexity of

\[K=\tilde{O}\left(\left(r+\frac{Q_{A}^{2}H}{\gamma^{2}}\right)\frac{rdH^{3}Q_{ A}^{4}\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}\beta}{\gamma^{4} \epsilon^{2}}\right).\]

This concludes the proof of Theorem 4. 

## Appendix K Proof of Theorem 5: UCB Algorithm for Generalized PSRs (Game Setting)

**Theorem** (Restatement of Theorem 5).: _Suppose Assumption 1 holds. Let \(p_{\min}=O\left(\frac{\delta}{KH\prod_{n=1}^{n}\left|\mathbb{X}_{n}\right|}\right)\), \(\lambda=\frac{\gamma\max_{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A \beta}\max\left\{\sqrt{r},Q_{A}\sqrt{H}/\gamma\right\}}{\sqrt{dH}}\), \(\alpha=O\left(\frac{Q_{A}\sqrt{Hd}}{\gamma^{2}}\sqrt{\lambda}+\frac{\max_{s\in \mathcal{A}}\left|\mathbb{X}_{s}\right|Q_{A}\sqrt{\beta}}{\gamma}\right)\), and let \(\beta=O(\log\left|\overline{\Theta}_{\varepsilon}\right|)\), where \(\varepsilon=O(\frac{p_{\min}}{KH})\). Then, with probability at least \(1-\delta\), Algorithm 2 returns a model \(\theta^{\epsilon}\) and a policy \(\pi\) which is an \(\varepsilon\)-approximate equilibrium (either NE or CCE). That is,_

\[V^{i}_{\theta^{*}}(\pi)\geq V^{i,\dagger}_{\theta^{*}}(\pi^{-i})-\varepsilon, \;\forall i\in[N].\]

_In addition, the algorithm terminates with a sample complexity of,_

\[\tilde{O}\left(\left(r+\frac{Q_{A}^{2}H}{\gamma^{2}}\right)\frac{rdH^{3}\max _{s\in\mathcal{A}}\left|\mathbb{X}_{s}\right|^{2}Q_{A}^{4}\beta}{\gamma^{4} \epsilon^{2}}\right).\]

Proof.: Recall that the model-estimation portion of Algorithm 2 is identical to Algorithm 1. Hence, by Theorem 4, the returned estimated model \(\theta^{\varepsilon}\) satisfies,

\[\texttt{D}_{\texttt{TV}}\left(\mathbb{P}^{\pi}_{\theta^{*}}(\pi_{H}),\mathbb{ P}^{\pi}_{\theta^{*}}(\pi_{H})\right)\leq\varepsilon/2,\]

for any collection of policies \(\boldsymbol{\pi}=(\pi^{i}:i\in[N])\). This implies that \(V^{i}_{\theta^{*}}(\pi)\geq V^{i}_{\theta^{*}}(\pi)-\varepsilon/2\) for all \(i\in[N]\).

Let \(\Gamma^{i}=\Gamma^{i}_{\mathrm{ind}}\) in the case of running the algorithm to find a Nash equilibrium and \(\Gamma^{i}=\Gamma^{i}_{\mathrm{cor}}\) in the case of a coarse correlated equilibrium. Recall that the collection of policies \(\pi=(\pi^{1},\ldots,\pi^{N})\) returned by the algorithm are an equilibrium under \(\theta^{\varepsilon}\). That is, for all \(i\in[N]\),

\[V^{i}_{\theta^{*}}(\pi)=\max_{\tilde{\pi}^{i}\in\Gamma^{i}_{\mathrm{ind}}}V^{i}_ {\theta^{*}}(\tilde{\pi}^{i},\pi^{-i})=:V^{i,\dagger}_{\theta^{*}}(\pi^{-i}).\]

Moreover, note that,

\[\left|V^{i,\dagger}_{\theta^{*}}(\pi^{-i})-V^{i,\dagger}_{\theta^ {*}}(\pi^{-i})\right| =\left|\max_{\tilde{\pi}^{i}}V^{i}_{\theta^{*}}(\tilde{\pi}^{i}, \pi^{-i})-\max_{\tilde{\pi}^{i}}V^{i}_{\theta^{*}}(\tilde{\pi}^{i},\pi^{-i})\right|\] \[\leq\max_{\tilde{\pi}^{i}}\left|V^{i}_{\theta^{*}}(\tilde{\pi}^{i },\pi^{-i})-V^{i}_{\theta^{*}}(\tilde{\pi}^{i},\pi^{-i})\right|\] \[\leq\varepsilon/2,\]where the final inequality is since \(\mathsf{D}_{\mathsf{T}\mathsf{U}}\left(\mathbb{P}_{\theta^{c}}^{\pi}(\tau_{H}), \mathbb{P}_{\theta^{*}}^{\pi}(\tau_{H})\right)\leq\varepsilon/2\) for any \(\bm{\pi}\). Thus, \(V_{\theta^{c}}^{i,\dagger}(\pi^{-i})\geq V_{\theta^{*}}^{i,\dagger}(\pi^{-i})- \varepsilon/2\).

Putting this together, we have,

\[V_{\theta^{*}}^{i}(\pi) \geq V_{\theta^{c}}^{i}(\pi)-\varepsilon/2\] \[=V_{\theta^{*}}^{i,\dagger}(\pi^{-i})-\varepsilon/2\] \[\geq V_{\theta^{*}}^{i,\dagger}(\pi^{-i})-\varepsilon.\]

Hence, \(\pi\) is an \(\varepsilon\)-approximate equilibrium (either NE or CCE).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction summarize the main message and technical results of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: This is discussed in the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper provides a complete, self-contained description of all technical assumptions, and a full proof (in the appendix) for all theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: N/A Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA] Justification: There is no data or code. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work is theoretical in nature and does not have immediately-foreseeable negative or positive societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use any existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: We do not introduce new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.