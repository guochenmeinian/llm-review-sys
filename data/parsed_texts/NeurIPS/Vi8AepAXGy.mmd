# _Cambrian_-1: A Fully Open, _Vision-Centric_

Exploration of Multimodal LLMs

 Shengbang Tong

Ellis Brown1

Penghao Wu1

Sanghyun Woo

Manoj Middepogu

Sai Charitha Akula

Jihan Yang

Shusheng Yang

Adithya Iyer

Xichen Pan

Austin Wang

Rob Fergus

Yann LeCun

Saining Xie1

New York University

Project Lead

Corresponding Author

Footnote 1: footnotemark:

###### Abstract

We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures--self-supervised, strongly supervised, or combinations thereof--based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.

**Project page:** https://cambrian-mllm.github.io/

## 1 Introduction

There is a long-standing debate in philosophy about whether understanding and meaning in language require sensory grounding. Aristotle's emphasis on acquiring knowledge through sensory experience and empirical observation was central to his ancient Peripatetic school and remains influential to this day [8]; Aquinas famously formalized these ideas in the 13th century with the Peripatetic axiom: "_Nihil est in intellectual quod non sit prius in sensu_" (Nothing is in the intellect that was not first in the senses) [7]. Though many philosophers disagree [23], it is evident that having robust and highly capable sensory grounding is at least beneficial. Consider the _Cambrian explosion_, during which the emergence of vision is believed [105] to have been crucial for early animals to not only find food and avoid predators but also to evolve and improve. In fact, most human knowledge (and nearly

[MISSING_PAGE_FAIL:2]

* **Connector Design**: We design a new dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens. SS3
* **Instruction Tuning Data**: We curate high-quality visual instruction-tuning data from public sources, emphasizing the importance of distribution balancing. SS4
* **Instruction Tuning Recipes**: We discuss instruction tuning strategies and practices. SS2.3
* **Benchmarking**: We analyze existing MLLM benchmarks, cluster them into 4 intuitive groups, and introduce a new vision-centric benchmark "CV-Bench". SS2.1, SS2.2

We defer a detailed review of the fundamental components and methodologies that underpin MLLM research to Appendix B

## 2 Evaluating Visual Representations through MLLMs

Current MLLMs predominantly rely on CLIP [109] as the visual encoder due to its pre-alignment with language and ease of adaptation to the LLM token space. However, strong language priors can be a double-edged sword--they compensate for deficiencies in learning effective visual representations [126] and diminish insights gained from extensive visual representation learning research. In this section, we systematically evaluate how various visual encoder choices (see Fig. 2) impact the multimodal capabilities of MLLMs. We also advocate for using MLLM evaluation as a robust framework for assessing visual representation methods, moving beyond traditional protocols like linear probing and end-to-end fine-tuning to more faithfully reflect the diverse perception challenges in real-world scenarios and to better guide the development of improved visual representations.

### Analyzing the Benchmarks

To effectively evaluate visual representations and MLLMs, we first need to select benchmarks that accurately assess the _multimodal_ capabilities of these models. We use a suite of commonly used benchmarks [24; 45; 54; 57; 83; 84; 91; 92; 96; 97; 120; 126; 137; 143], which is the intersection of those used in recent MLLM research [75; 137; 77]. To help interpret our results, we begin by analyzing the benchmarks themselves. Here, we train MLLMs with 23 different vision backbones (see Table 6) from a variety of model families (see Fig. 2) using a 2-stage instruction tuning process initially proposed in [82]: first training connector on 1.2M adapter data from ShareGPT-4V [27] followed by fine-tuning both the connector and LLM on 737K instruction tuning data (see more details in Appendices G.5 and H). Full benchmark results in Table 9.

**Who's answering the question: the LLM or MLLM?** Determining whether a benchmark _truly_ needs visual input to be solved has been a persistent challenge in vision-language research [2; 26; 50; 94]. In this study, we compare the performance of MLLMs with and without visual input8, and also calculate the expected score via randomly guessing. These three conditions are visualized in Fig. 3-left, with benchmarks sorted by the difference between the average score with vision enabled and disabled. SQA-I9, MMMU, MathVista, and AI2D display less than a 5% gap between vision enabled and disabled, suggesting that these benchmarks may not significantly depend on visual input and rather heavily rely on the base LLM. TextVQA and GQA both demonstrate a nearly 40% positive gap between random guessing and vision-disabled scores, implying a strong language bias in these benchmarks. On the other hand, the vision-disabled performance on benchmarks like MMVP is notably worse than random guessing, suggesting that strong visual grounding is particularly crucial.

Footnote 8: We note that our instruction-tuning data includes text-only data, so text-only questions are not OOD.

**Clustering the Benchmarks** To better understand the different aspects of MLLM performance, we analyze the correlations between the performance of our 23 MLLMs on each benchmark. A

Figure 2: Examples of various vision models, objectives, and architectures studied. Image from [48].

confusion matrix (Fig. 10) reveals that certain benchmarks, such as MMMU, are largely uncorrelated with the others. We perform principal component analysis on the benchmark scores and observe the formation of clusters corresponding to "General," "Knowledge," "Chart & OCR," and "Vision-Centric" categories (Fig. 3-right). We assign MMMU to the knowledge category based on the types of questions it includes (see Appendix D). We also find that existing vision-centric benchmarks [126; 137] are of insufficient size (see Fig. 3-right), challenging the robustness of evaluating such capabilities. These benchmarks do not cover crucial visual elements such as depth and spatial awareness.

#### Cambrian Vision-Centric Benchmark (CV-Bench)

To address the limitations of existing vision-centric benchmarks, we introduce the Cambrian Vision-Centric Benchmark (CV-Bench). With **2638 manually-inspected examples**, CV-Bench provides significantly more examples than other vision-centric MLLM benchmarks--\(3.5\times\) more than Real-WorldQA [137] and \(8.8\times\) more than MMVP [126]. By repurposing standard vision benchmarks [18; 79; 154]1, we can assess models at classic vision tasks within a multimodal context. Leveraging the rich ground truth annotations from the benchmarks, we formulate natural language questions that probe the fundamental 2D and 3D understanding of the models.

Footnote 1: Omni3D assets are sourced from [111; 121; 3; 13; 20; 13].

As visualized in Fig. 11, CV-Bench evaluates 2D understanding via spatial relationships & object counting, and 3D understanding via depth order & relative distance. We refer details to Appendix E.

#### Instruction Tuning Recipes

MLLMs start with pre-trained LLM and vision backbones, connecting these modules with a connector such as a projector (MLP). The original LLaVA [80; 82] proposes a 2-stage frozen training process: first, pre-training a connector between frozen LLM and vision backbones using adapter data, and then fine-tuning both the connector and LLM with instruction tuning data while leaving the vision encoder frozen. Various studies [27; 63; 81; 98] have drawn different conclusions regarding the optimal training methodology for MLLMs. Here, we revisit this topic with extensive experiments.

For our experiments, we tune a set of MLLMs using Vicuna-1.5-7B as the LLM backbone and each of our 23 vision models (Table 6) as the visual encoder. We use a 737K instruction tuning data mix for all experiments here (see Appendix H). All hyperparameters are matched across each experimental setting--highlighting the impact of different tuning strategies with each visual encoder. All experimental settings and results are tabulated in Appendix F.2.

Figure 3: **Left:** Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. **Right:** Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as “General” in green, “Knowledge” in yellow, “Chart & OCR” in red, and “Vision-Centric” in blue.

**One Stage vs Two Stage Training** Recent work [63] advocates for skipping connector pre-training, claiming this "_reduces compute cost without harming downstream performance._" To explore whether this claim holds--especially when using non-language-supervised visual encoders--we conduct experiments using 0, 0.5M, and 1.2M adapter data. Following LLaVA's recipe [82], we tune only the connector on the adapter data during this first phase, before unfreezing the LLM and connector during instruction tuning on the 737K mix. Fig. 4 shows that pre-training the connector first enhances model performance and that more adapter data further improves performance across all domains. Thus, we subsequently adopt 2-stage training with 1.2M adapter data as our standard setup.

**Finding 3**:Two-stage training is beneficial; more adapter data further improves results.

**Freeze vs Unfreeze Vision Encoder** There are also mixed practices in freezing [63, 80, 82] or unfreezing [44, 81] vision backbones during fine-tuning. Some argue that unfreezing the vision backbone significantly degrades performance [63]. Our experiments demonstrate that unfreezing benefits performance across all benchmarks except for a marginal change in knowledge benchmarks (Fig. 4). We suspect this is due to the composition of the 737K instruction tuning data and the LLM-heavy focus of these benchmarks (see Section 2.1). We note that unfreezing the vision backbone introduces additional computational overhead, which prohibits testing on some larger vision models under current sharding strategies (see more details in Appendix H).

**Finding 4**:Unfreezing the vision encoder is widely beneficial. Language-supervised models always benefit; SSL models particularly benefit on vision-centric benchmarks.

### MLLMs as a Visual Representation Evaluator

As discussed in earlier sections, MLLMs provide a new interface to explore aspects of vision models beyond traditional benchmarks like ImageNet-1k linear probing. We study the 2-stage instruction tuning setting using 1.2M adapter data, 737K fine-tuning data, and frozen visual encoders to allow comparison of the widest range of models.

Figure 4: **Effect of Training Recipe on Model Performance. Boxplots display the distribution of benchmark scores across benchmark categories for different training recipes and types of visual encoders. The four training recipes include freezing the visual encoder with various amounts of adapter data (0M, 0.5M, 1.2M) as well as unfreezing it with 1.2M adapter data. Amount of Adapter Data: All model types show increased performance on general and vision-centric benchmarks; knowledge benchmarks show mixed results; OCR & chart benchmarks benefit from more data for language-supervised models. Unfreezing: Unfreezing the visual encoder with 1.2M adapter data generally benefits all categories.**We evaluate on benchmarks detailed in Section 2.1, calculating the average performance**+ for each category and visualize the results in Fig. 5 (full results in Appendix F). Our findings highlight the advantages of language-supervised models over non-CLIP models across all benchmark categories, with significantly better performance on chart and OCR-related benchmarks. We hypothesize that this is due to CLIP's _training data_, such as LAION [115], containing abundant OCR and text-heavy data, whereas SSL and other vision models primarily train on natural images with significantly less text content. It is also noteworthy that language-supervised models are typically trained with a very large pool of data, ranging from 400 million [109] to 10 billion [28] samples, whereas the largest vision self-supervised training dataset, like DINov2, consists of only _142 million samples_[103].

Footnote †: **: Before averaging, we divide the MME Perception score by 20 to have the same scale as other benchmarks.**

Additionally, we observe that higher-resolution models particularly enhance performance on chart and vision-centric benchmarks while remaining neutral on general VQA and knowledge-based VQAs. While the majority of the backbones we examine are ViT-based [39], **ConvNet-based architectures** (such as OpenCLIP ConvNeXt [86]) are inherently well-suited for high-resolution image processing [130] and can produce superior results on OCR & Chart and Vision-Centric benchmarks. In vision-centric benchmarks, the gap between language-supervised and other types of vision models is smaller, with a well-trained self-supervised DINov2 model even outperforming some language-supervised models.

**Finding 5:** High-res encoders greatly enhance performance on chart & vision-centric benchmarks, and ConvNet-based architectures are inherently well-suited for such tasks.

**Narrowing the gap between Language- and Self-Supervised models** Above, we observe that DINov2 stands midway between SSL models and language-supervised models on general and knowledge benchmarks, even outperforming some language-supervised models on vision-centric benchmarks. Here, we study whether the continued finetuning of an MLLM based on a SSL model

Figure 5: **Evaluating Visual Representations with MLLMs** While language-supervised models outperform self-supervised or other models, a well-trained self-supervised model like DINov2 can also achieve competitive performance on vision-centric tasks.

Figure 6: **Continued Fine-Tuning Narrows the Gap Between CLIP and DINov2.** Performance is compared with 0.7M and 5M instruction tuning data in both frozen () and unfrozen () settings. DINov2 shows significant performance improvement with increased data and unfreezing—surpassing the 0.7M  CLIP model in several benchmarks and narrowing the gap to the 5M  model in knowledge and vision-centric tasks.

can achieve performance similar to that of a language-supervised model. Specifically, we scale up the instruction tuning data from 737K to 5M (see more details in Appendix G.5), and instruction tune MLLMs with DINov2 ViT-L/14@336 and OpenAI CLIP ViT-L/14@336 encoders in both frozen and unfrozen settings. In Fig. 6, we observe that by unfreezing the vision backbone, the DINov2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on 0.7M data. Additionally, the gap between DINov2 and the CLIP models is reduced under the 5M setting.

[leftmargin=*]
**Finding 6:** Language supervision offers strong advantages, but the performance gap can be narrowed with SSL methods given enough data and proper tuning.

### Combining Multiple Vision Encoders

As observed in Fig. 5, different vision encoders excel in different aspects of MLLM performance. In this study, we explore the potential of combining multiple vision encoders to leverage their distinctive representations, aiming to build a more capable MLLM. Given that different vision encoders use varying architectures and image resolutions, we interpolate to a fixed number of visual tokens (576) in this subsection (see details in Appendix F.3). We then concatenate these tokens along the feature dimension, following a method similar to A-MoF proposed in [126].

Our study indicates that adding a non-language-supervised model (DINov2) can improve benchmark performance, especially in vision-centric tasks. Notably, even OCR benchmarks benefit from incorporating DINov2. This highlights the importance of self-supervised learning models in complementing language-supervised models to achieve robust multimodal understanding. Detailed results and configurations are available in Appendix F.3.

However, this naive strategy has two limitations: 1) it employs interpolation, which can lead to information loss, especially with vision encoders with high-resolution feature maps, and 2) it treats each model equally via simple concatenation. Therefore, we seek a more effective strategy that can more flexibly leverage model combinations with less information loss.

[leftmargin=*]
**Finding 7:** Combining multiple vision encoders, including SSL models, can enhance MLLM performance across various benchmarks, particularly in vision-centric tasks.

## 3 Spatial Vision Aggregator (SVA): A New Connector Design

To effectively aggregate features from multiple vision encoders and prevent the information loss introduced by interpolation, we use a set of learnable latent queries that interact with multiple vision features via cross-attention layers [37]. In particular, our approach incorporates two new vision-centric design principles:

1. We introduce spatial inductive bias by explicitly defining the aggregation space for each token in the query.
2. We aggregate vision features multiple times across the LLM layers, enabling the model to repeatedly access and integrate necessary visual information.

To facilitate information aggregation via cross-attention, we create a \(C\)-dimension learnable latent token \(\mathbf{x}\in\mathbb{R}^{C}\) that is repeated \(L\times L\) times to form a 2D grid, serving as the query \(\mathbf{X}\in\mathbb{R}^{L^{2}\times C}\). The set of visual features \(\mathbf{F}\) from \(N\) vision encoders serve as the context (i.e., key and value). We ensure the output resolution of every vision encoder is a multiple of \(L\). Formally, the feature map of the \(k\)-th vision encoder (\(\mathbf{F}_{k}\)) has a resolution of \(m_{k}L\times m_{k}L\times C\), where \(m_{k}\) is a positive integer multiplier, and \(L\) is the height/width of the learnable 2D grid with hidden dimension \(C\).

**Spatial inductive bias** To maintain the spatial structure during cross-attention, we align each token in the query with a specific sub-region of the feature maps in all vision encoders. Formally, a token at row \(i\) and column \(j\) in the query \(\mathbf{x}_{i,j}\) corresponds to the sub-region

\[\mathbf{F}_{k}[m_{k}\cdot i:m_{k}\cdot(i+1),m_{k}\cdot j:m_{k}\cdot(j+1)]\in \mathbb{R}^{m_{k}^{2}\times C}\]

of the \(k\)-th vision feature map. As a result, a token \(\mathbf{x}_{i,j}\) aggregates a total of \(\sum_{k}m_{k}^{2}\) features from \(N\) vision encoders through cross-attention (see Fig. 7-left).

Specifically, the updated query vector \(\mathbf{q}^{*}_{\mathbf{i,j}}\in\mathbb{R}^{1\times C}\) at position \((i,j)\) is computed as \[\mathbf{q}^{*}_{\cdot,j}=\text{softmax}\left(\frac{\mathbf{q}_{i,j}\cdot[\mathbf{k}_{ i,j,1},\mathbf{k}_{i,j,2},\ldots,\mathbf{k}_{i,j,N}]^{\top}}{\sqrt{C}} \right)\left[\mathbf{v}_{i,j,1},\mathbf{v}_{i,j,2},\ldots,\mathbf{v}_{i,j,N} \right],\] (1)

where

\[\mathbf{q}_{i,j} =\mathbf{W}^{Q}\mathbf{x}_{i,j}\in\mathbb{R}^{1\times C},\] \[\mathbf{k}_{i,j,k} =\mathbf{W}_{k}^{K}\mathbf{F}_{k}[m_{k}\cdot i:m_{k}\cdot(i+1),\ m _{k}\cdot j:m_{k}\cdot(j+1)]\in\mathbb{R}^{m_{k}^{2}\times C},\] \[\mathbf{v}_{i,j,k} =\mathbf{W}_{k}^{V}\mathbf{F}_{k}[m_{k}\cdot i:m_{k}\cdot(i+1),\ m _{k}\cdot j:m_{k}\cdot(j+1)]\in\mathbb{R}^{m_{k}^{2}\times C}.\]

Here, \(\mathbf{q}_{i,j}\) is the query vector at position \((i,j)\), calculated using the query projection matrix \(\mathbf{W}^{Q}\in\mathbb{R}^{C\times C}\). The key vectors \(\mathbf{k}_{i,j,k}\) and value vectors \(\mathbf{v}_{i,j,k}\) are computed for each vision encoder \(k\) using their respective key and value projection matrices \(\mathbf{W}_{k}^{K}\in\mathbb{R}^{C\times C}\) and \(\mathbf{W}_{k}^{V}\in\mathbb{R}^{C\times C}\). Since \(\sum_{k}m_{k}^{2}\) features are aggregated into a single token, we effectively reduce the number of tokens.

Multi-layer vision aggregationAlthough our proposal effectively aggregates features from multiple vision encoders, there is still potential information loss with high-resolution input (large \(m_{k}\)) or multiple vision encoders (large \(N\)). Here, a single token would have to handle a larger amount of context information during aggregation. To prevent this, we allow cross-attention to occur multiple times by inserting our proposal throughout the LLM layers--allowing consistent access to the uncompressed visual information (see Fig. 7-right).

HyperparametersTo flexibly modulate capacity, we introduce two hyperparameters \(D\) and \(G\), which indicate the number of cross-attention layers and distinct groups of learnable queries used between the vision models and the LLM, respectively. \(D\) and \(G\) are always set to 1 for cross-attention layers within LLM layers. We provide ablation studies on the selection of \(D\) and \(G\) in Appendix H.

We demonstrate the efficacy of SVA module using the best vision model combination results from the previous section and a Vicuna-1.5-7B base LLM. Specifically, we employ a combination of four vision encoders: OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, OpenCLIP ConvNeXt-XXL@1024, and DINOcv2 ViT-L/14@518. We compare our method with two strong baselines: 1) concatenation-based [126] and 2) Re-sampler [11, 72]. Here, we include two variants of our SVA module. The standard one, "**SVA**", uses \(D=3\), \(G=1\), and inserts cross-attention blocks inside the LLM with a layer stride of 3. To isolate the advantages of spatial inductive biases, we include another SVA variant, "SVA-no-multi-agg", that does not add cross-attention blocks inside the LLM and sets \(D=3\) and \(G=3\). Table 1 shows that SVA outperforms both baselines, with a significant improvement in the OCR & chart category. In contrast, the Resampler--which lacks spatial inductive biases--struggles to condense concatenated tokens from various vision towers into

\begin{table}
\begin{tabular}{l c c c c} Connector & General & Knowledge & OCR \& Chart & Vision-Centric \\ \hline Concat. [126] & 67.2 & 48.9 & 50.1 & 52.6 \\ Resampler [58] & 63.1 & 46.5 & 27.1 & 42.6 \\ SVA-no-multi-agg & 68.0 & 49.5 & 55.2 & 52.6 \\
**SVA** & **68.5** & **49.7** & **55.5** & **53.2** \\ \end{tabular}
\end{table}
Table 1: **Comparison between our SVA and other aggregation approaches. The SVA module consistently outperforms other baselines and excels in aggregating high-resolution vision information.**

Figure 7: **Spatial Vision Aggregator (SVA). We propose SVA, a dynamic and spatially-aware connector that integrates multiple vision features with LLMs while reducing the number of tokens.**

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

## Acknowledgements

We are grateful to LLaVA [82] for their excellent codebase, which served as the launching point for our research. Special thanks to Hexu Zhao for extensive discussions and knowledge-sharing around FSDP and large-scale training techniques, and to Jiasen Lu for helpful discussions on TPU and JAX distributed training infrastructure. We also appreciate the assistance and responses from the PyTorchXLA team via GitHub.

We are thankful to Kaiming He for early discussions on multi-modal large language models. We also thank Zhuang Liu, Junlin Han, Yuexiang Zhai, Tianzhe Chu, Daohan Lu, Weiyang Jin, Boyang Zhang, and Jiayi Pan for reviewing this manuscript. We also acknowledge DeepSeek [87] for the paper template inspiration.

This work was primarily supported by the Google TPU Research Cloud (TRC) program and the Google Cloud Research Credits program (GCP19980904). Additional support was provided by the NYU IT High Performance Computing resources, services, and staff expertise. S.X. would like to thank the OpenAI Researcher Access program, Open Path AI Foundation, and an Amazon Research award for their support. S.T. is supported by the OpenAI SuperAlignment Fellowship, and E.B. is supported by the NDSEG Fellowship.

## References

* [1] M. Acharya, K. Kafle, and C. Kanan. "TallyQA: Answering complex counting questions". In: _AAAI_. 2019.
* [2] A. Agrawal et al. "Don't just assume; look and answer: Overcoming priors for visual question answering". In: _CVPR_. 2018.
* [3] A. Ahmadyan et al. "Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations". In: _CVPR_ (2021).
* [4] AI@Meta. "Llama 3 Model Card". In: (2024).
* [5] H. A. Alawwad et al. "Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation". In: _arXiv preprint arXiv:2402.05128_ (2024).
* [6] J.-B. Alayrac et al. "Flamingo: a visual language model for few-shot learning". In: _NeurIPS_. 2022.
* [7] T. Aquinas. _Quaestiones Disputatae de Veritate_. q.2 a.3 arg.19, 1259.
* [8] Aristotle. _Metaphysics_. Ed. by T. by W. D. Ross. The Internet Classics Archive, 350BCE.
* [9] M. Assran et al. "Self-supervised learning from images with a joint-embedding predictive architecture". In: _CVPR_. 2023.
* [10] J. Bai et al. "Qwen Technical Report". In: _arXiv preprint arXiv:2309.16609_ (2023).
* [11] J. Bai et al. "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond". In: (2023).
* [12] M. E. Banani et al. "Probing the 3D Awareness of Visual Foundation Models". In: _arXiv preprint arXiv:2404.08636_ (2024).
* A Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile RGB-D Data". In: _NeurIPS Datasets and Benchmarks Track (Round 1)_. 2021.
* [14] J. Belouadi, A. Lauscher, and S. Eger. "Automatikz: Text-guided synthesis of scientific vector graphics with tikz". In: _ICLR_. 2024.
* [15] R. Birkl, D. Wofk, and M. Muller. "Midas v3. 1-a model zoo for robust monocular relative depth estimation". In: _arXiv preprint arXiv:2307.14460_ (2023).
* [16] A. F. Biten et al. "Latt: Layout-aware transformer for scene-text vqa". In: _CVPR_. 2022.
* [17] A. F. Biten et al. "Scene text visual question answering". In: _ICCV_. 2019.
* [18] G. Brazil et al. "Omni3d: A large benchmark and model for 3d object detection in the wild". In: _CVPR_. 2023.
* [19] J. Buchner. _imagehash (fork)_. https://github.com/JohannesBuchner/imagehash. 2021.
* [20] H. Caesar et al. "nuscenes: A multimodal dataset for autonomous driving". In: _CVPR_. 2020.
* [21] J. Cha et al. "Honeybee: Locality-enhanced projector for multimodal llm". In: _CVPR_. 2024.
* [22] S. Cha et al. "Visually Dehallucinative Instruction Generation: Know What You Don't Know". In: _arXiv preprint arXiv:2402.09717_ (2024).
* [23] D. J. Chalmers. "Does Thought Require Sensory Grounding? From Pure Thinkers to Large Language Models". In: _Proceedings and Addresses of the American Philosophical Association_ 97 (2023), pp. 22-45.
* [24] Y. Chang et al. "A survey on evaluation of large language models". In: _ACM Transactions on Intelligent Systems and Technology_ 15.3 (2024), pp. 1-45.
* [25] G. H. Chen et al. "ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model". In: _arXiv preprint arXiv:2402.11684_ (2024).
* [26] L. Chen et al. "Are We on the Right Way for Evaluating Large Vision-Language Models?" In: _arXiv preprint arXiv:2403.20330_ (2024).
* [27] L. Chen et al. "Sharegpt4v: Improving large multi-modal models with better captions". In: _arXiv preprint arXiv:2311.12793_ (2023).
* [28] X. Chen et al. "Pali: A jointly-scaled multilingual language-image model". In: _ICLR_. 2023.
* [29] X. Chen, S. Xie, and K. He. "An empirical study of training self-supervised vision transformers". In: _ICCV_. 2021.

* [30] Z. Chen et al. "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites". In: _arXiv preprint arXiv:2404.16821_ (2024).
* [31] Z. Chen et al. "Finqa: A dataset of numerical reasoning over financial data". In: _EMNLP_. 2021.
* [32] Z. Cheng et al. "HiTab: A hierarchical table dataset for question answering and natural language generation". In: _ACL_. 2022.
* [33] M. Cherti et al. "Reproducible scaling laws for contrastive language-image learning". In: _CVPR_. 2023.
* [34] W.-L. Chiang et al. "Chatbot arena: An open platform for evaluating llms by human preference". In: _arXiv preprint arXiv:2403.04132_ (2024).
* [35] X. Chu et al. "Mobilevlm v2: Faster and stronger baseline for vision language model". In: _arXiv preprint arXiv:2402.03766_ (2024).
* [36] M. Conover et al. _Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM_. 2023. url: https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm (visited on 06/30/2023).
* [37] W. Dai et al. "Instructblip: Towards general-purpose vision-language models with instruction tuning". In: _NeurIPS_. 2024.
* [38] H. Dong et al. "Rlhf workflow: From reward modeling to online rhlf". In: _arXiv preprint arXiv:2405.07863_ (2024).
* [39] A. Dosovitskiy et al. "An image is worth 16x16 words: Transformers for image recognition at scale". In: _ICLR_. 2021.
* [40] A. Fang et al. "Data filtering networks". In: _ICLR_. 2024.
* [41] X. Fu et al. "BLINK: Multimodal Large Language Models Can See but Not Perceive". In: _arXiv preprint arXiv:2404.12390_ (2024).
* [42] J. Gao et al. "G-llava: Solving geometric problem with multi-modal large language model". In: _arXiv preprint arXiv:2312.11370_ (2023).
* [43] P. Gao et al. "Llama-adapter v2: Parameter-efficient visual instruction model". In: _arXiv preprint arXiv:2304.15010_ (2023).
* [44] P. Gao et al. "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models". In: _arXiv preprint arXiv:2402.05935_ (2024).
* [45] Y. Ge et al. "Planting a seed of vision in large language model". In: _arXiv preprint arXiv:2307.08041_ (2023).
* [46] A. Geiger, P. Lenz, and R. Urtasun. "Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite". In: _CVPR_. 2012.
* [47] R. Geirhos et al. "Shortcut learning in deep neural networks". In: _Nature Machine Intelligence_ (2020).
* [48] R. Girshick et al. "Rich feature hierarchies for accurate object detection and semantic segmentation". In: _CVPR_. 2014.
* [49] Google. _Gemini_. 2023.
* [50] Y. Goyal et al. "Making the v in vqa matter: Elevating the role of image understanding in visual question answering". In: _CVPR_. 2017.
* [51] D. Gurari et al. "Vizwiz grand challenge: Answering visual questions from blind people". In: _CVPR_. 2018.
* [52] K. He et al. "Masked autoencoders are scalable vision learners". In: _CVPR_. 2022.
* [53] X. He et al. "PathVQA: 30000+ Questions for Medical Visual Question Answering". In: _CoRR_ abs/2003.10286 (2020).
* [54] T. Hiippala et al. "AI2D-RST: A multimodal corpus of 1000 primary school science diagrams". In: _Language Resources and Evaluation_ 55 (2021), pp. 661-688.
* [55] J. Hoffmann et al. "Training compute-optimal large language models". In: _NeurIPS_ (2023).
* [56] Y.-C. Hsiao, F. Zubach, M. Wang, et al. "Screenqa: Large-scale question-answer pairs over mobile app screenshots". In: _arXiv preprint arXiv:2209.08199_ (2022).
* [57] D. A. Hudson and C. D. Manning. "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering". In: _CVPR_. 2019.
* [58] A. Jaegle et al. "Perceiver: General perception with iterative attention". In: _ICML_. 2021.

* [59] J. Johnson et al. "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning". In: _CVPR_. 2017.
* [60] N. Jouppi et al. "Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings". In: _Proceedings of the 50th Annual International Symposium on Computer Architecture_. 2023.
* [61] K. Kafle et al. "Dvqa: Understanding data visualizations via question answering". In: _CVPR_. 2018.
* [62] S. Kantharaj et al. "Chart-to-text: A large-scale benchmark for chart summarization". In: _ACL_. 2022.
* [63] S. Karamcheti et al. "Prismatic vlms: Investigating the design space of visually-conditioned language models". In: _arXiv preprint arXiv:2402.07865_ (2024).
* [64] M. Kazemi et al. "Geomverse: A systematic evaluation of large models for geometric reasoning". In: 2023.
* [65] A. Kembhavi et al. "A diagram is worth a dozen images". In: _ECCV_. 2016.
* [66] D. Kiela et al. "The hateful memes challenge: Detecting hate speech in multimodal memes". In: _NeurIPS_. 2020.
* [67] G. Kim et al. "Donut: Document understanding transformer without ocr". In: _ECCV_. 2022.
* [68] A. Kirillov et al. "Segment anything". In: _ICCV_. 2023.
* [69] R. Krishna et al. "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations". In: _IJCV_ (2016).
* [70] LAION. _laion/spt4v-dataset_. 2023.
* [71] H. Laurencon, L. Tronchon, and V. Sanh. "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset". In: _arXiv preprint arXiv:2403.09029_ (2024).
* [72] H. Laurencon et al. "What matters when building vision-language models?" In: _arXiv preprint arXiv:2405.02246_ (2024).
* [73] A. C. Li et al. "Internet Explorer: Targeted Representation Learning on the Open Web". In: _ICML_. 2023.
* [74] A. C. Li et al. "Your diffusion model is secretly a zero-shot classifier". In: _ICCV_. 2023.
* [75] B. Li et al. _LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild_. 2024.
* [76] L. Li et al. "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models". In: _arXiv preprint arXiv:2403.00231_ (2024).
* [77] Y. Li et al. "Mini-gemini: Mining the potential of multi-modality vision language models". In: _arXiv preprint arXiv:2403.18814_ (2024).
* [78] W. Lian et al. _OpenOrca: An Open Dataset of GPT Augmented FLAN Reasoning Traces_. https://https://huggingface.co/Open-Orca/OpenOrca. 2023.
* [79] T.-Y. Lin et al. "Microsoft coco: Common objects in context". In: _ECCV_. 2014.
* [80] H. Liu et al. "Improved baselines with visual instruction tuning". In: _arXiv preprint arXiv:2310.03744_ (2023).
* [81] H. Liu et al. _LLaVA-NeXT: Improved reasoning, OCR, and world knowledge_. 2024.
* [82] H. Liu et al. "Visual Instruction Tuning". In: _NeurIPS_. 2023.
* [83] Y. Liu et al. "Mmbench: Is your multi-modal model an all-around player?" In: _arXiv preprint arXiv:2307.06281_ (2023).
* [84] Y. Liu et al. "On the hidden mystery of ocr in large multimodal models". In: _arXiv preprint arXiv:2305.07895_ (2023).
* [85] Z. Liu and K. He. "A Decade's Battle on Dataset Bias: Are We There Yet?" In: _arXiv preprint arXiv:2403.08632_ (2024).
* [86] Z. Liu et al. "A convnet for the 2020s". In: _CVPR_. 2022.
* [87] H. Lu et al. "DeepSee-VL: towards real-world vision-language understanding". In: _arXiv preprint arXiv:2403.05525_ (2024).
* [88] P. Lu et al. "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning". In: _ICLR_. 2023.
* [89] P. Lu et al. "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning". In: _NeurIPS_. 2021.

* [90] P. Lu et al. "Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning". In: _ACL_. 2021.
* [91] P. Lu et al. "Learn to explain: Multimodal reasoning via thought chains for science question answering". In: _NeurIPS_. 2022.
* [92] P. Lu et al. "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts". In: _ICLR_ (2023).
* [93] Z. Luo et al. "Wizardcoder: Empowering code large language models with evol-instruct". In: _ICLR_. 2024.
* [94] A. Majumdar et al. "OpenEQA: Embodied Question Answering in the Era of Foundation Models". In: _2nd Workshop on Mobile Manipulation and Embodied Intelligence at ICRA 2024_. 2024. 2024.
* [95] K. Marino et al. "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge". In: _CVPR_. 2019.
* [96] A. Masry et al. "Chartqa: A benchmark for question answering about charts with visual and logical reasoning". In: _ACL_. 2022.
* [97] M. Mathew, D. Karatzas, and C. Jawahar. "Docvqa: A dataset for vqa on document images". In: _WACV_. 2021.
* [98] B. McKinzie et al. "Mm1: Methods, analysis & insights from multimodal llm pre-training". In: _arXiv preprint arXiv:2403.09611_ (2024).
* [99] A. Mitra et al. _Orca-Math: Unlocking the potential of SLMs in Grade School Math._ 2024. arXiv: 2402.14830 [cs.CL].
* [100] "OCR-VQA: Visual Question Answering by Reading Text in Images". In: 2019.
* [101] OpenAI. _ChatGPT_. 2022.
* [102] OpenAI. _gpt4o_. 2024.
* [103] M. Oquab et al. "Dinov2: Learning robust visual features without supervision". In: _TMLR_ (2023).
* [104] L. Ouyang et al. "Training language models to follow instructions with human feedback". In: _NeurIPS_. 2022.
* [105] A. Parker. _In the blink of an eye: how vision sparked the big bang of evolution_. 2003.
* [106] P. Pasupat and P. Liang. "Compositional semantic parsing on semi-structured tables". In: _ACL_. 2015.
* [107] J. Piaget, M. Cook, et al. _The origins of intelligence in children_. Vol. 8. 5. International Universities Press New York, 1952.
* [108] J. Pont-Tuset et al. "Connecting Vision and Language with Localized Narratives". In: _ECCV_. 2020.
* [109] A. Radford et al. "Learning transferable visual models from natural language supervision". In: _ICML_. 2021.
* [110] R. Rafailov et al. "Direct preference optimization: Your language model is secretly a reward model". In: _NeurIPS_. 2024.
* [111] M. Roberts et al. "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding". In: _ICCV_. 2021.
* [112] R. Rombach et al. "High-Resolution Image Synthesis With Latent Diffusion Models". In: _CVPR_. 2022.
* [113] O. Russakovsky et al. "Imagenet large scale visual recognition challenge". In: _IJCV_ (2015).
* [114] O. Sanseviero. _LLM Evals and Benchmarking_. 2022.
* [115] C. Schuhmann et al. "Laion-5b: An open large-scale dataset for training next generation image-text models". In: _NeurIPS_. 2022.
* [116] D. Schwenk et al. "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge". In: _ECCV_. 2022.
* [117] M. Shridhar et al. "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning". In: _ICLR_. 2021.
* [118] C. Si et al. "Design2Code: How Far Are We From Automating Front-End Engineering?" In: _arXiv preprint arXiv:2403.03163_ (2024).

* [119] O. Sidorov et al. _TextCaps: a Dataset for Image Captioning with Reading Comprehension_. 2020. arXiv: 2003.12462 [cs.CV].
* [120] A. Singh et al. "Towards vqa models that can read". In: _CVPR_. 2019.
* [121] S. Song, S. P. Lichtenberg, and J. Xiao. "Sun rgb-d: A rgb-d scene understanding benchmark suite". In: _CVPR_. 2015.
* [122] Q. Sun et al. "Eva-clip: Improved training techniques for clip at scale". In: _arXiv preprint arXiv:2303.15389_ (2023).
* [123] R. Tanaka, K. Nishida, and S. Yoshida. "VisualMRC: Machine Reading Comprehension on Document Images". In: _AAAI_. 2021.
* [124] B. J. Tang, A. Boggust, and A. Satyanarayan. "Vistext: A benchmark for semantically rich chart captioning". In: _arXiv preprint arXiv:2307.05356_ (2023).
* [125] S. Tong, E. Jones, and J. Steinhardt. "Mass-producing failures of multimodal systems with language models". In: _NeurIPS_. 2024.
* [126] S. Tong et al. "Eyes wide shut? exploring the visual shortcomings of multimodal llms". In: _CVPR_. 2024.
* [127] H. Touvron et al. "LLaMA 2: Open foundation and fine-tuned chat models". In: (2023).
* [128] H. Touvron et al. "LLaMA: Open and efficient foundation language models". In: _arXiv preprint arXiv:2302.13971_ (2023).
* [129] H. Tu et al. "How many unicorns are in this image? a safety evaluation benchmark for vision llms". In: _arXiv preprint arXiv:2311.16101_ (2023).
* [130] K. Vishniakov, Z. Shen, and Z. Liu. "ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy". In: _ICML_. 2024.
* [131] J. Wang et al. "To see is to believe: Prompting gpt-4v for better visual instruction tuning". In: _arXiv preprint arXiv:2311.07574_ (2023).
* [132] K. Wang et al. "Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset". In: _arXiv preprint arXiv:2402.14804_ (2024).
* [133] J. Wei et al. "Chain-of-thought prompting elicits reasoning in large language models". In: _NeurIPS_. 2022.
* [134] C. Wendler. _wendler/RenderedText_. 2023.
* [135] H. Wu et al. "Q-instruct: Improving low-level visual abilities for multi-modality foundation models". In: _arXiv preprint arXiv:2311.06783_ (2023).
* [136] P. Wu and S. Xie. "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs". In: _CVPR_. 2024.
* [137] xAI. _grok_. 2024.
* [138] H. Xu et al. "Demystifying clip data". In: _ICLR_. 2024.
* [139] A. Young et al. "Yi: Open foundation models by 01. ai". In: _arXiv preprint arXiv:2403.04652_ (2024).
* [140] L. Yu et al. _Modeling Context in Referring Expressions_. 2016. arXiv: 1608.00272 [cs.CV].
* [141] T. Yu et al. "Rhlf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback". In: _arXiv preprint arXiv:2312.00849_ (2023).
* [142] X. Yue et al. "Mammoth: Building math generalist models through hybrid instruction tuning". In: _ICLR_. 2024.
* [143] X. Yue et al. "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi". In: _CVPR_. 2024.
* [144] M. Yuksekgonul et al. "When and why vision-language models behave like bags-of-words, and what to do about it?" In: _ICLR_. 2022.
* [145] X. Zhai et al. "Sigmoid loss for language image pre-training". In: _ICCV_. 2023.
* [146] Y. Zhai et al. "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning". In: _arXiv preprint arXiv:2405.10292_ (2024).
* [147] Y. Zhai et al. "Investigating the catastrophic forgetting in multimodal large language models". In: _CPAL_. 2024.
* [148] C. Zhang et al. "Raven: A dataset for relational and analogical visual reasoning". In: _CVPR_. 2019.

* [149] Y. Zhang et al. "Llavar: Enhanced visual instruction tuning for text-rich image understanding". In: _arXiv preprint arXiv:2306.17107_ (2023).
* [150] Y. Zhao et al. "Pytorch fsdp: experiences on scaling fully sharded data parallel". In: _arXiv preprint arXiv:2304.11277_ (2023).
* [151] L. Zheng et al. "Judging llm-as-a-judge with mt-bench and chatbot arena". In: _NeurIPS_. 2024.
* [152] T. Zheng et al. "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement". In: _arXiv preprint arXiv:2402.14658_ (2024).
* [153] V. Zhong, C. Xiong, and R. Socher. "Seq2sql: Generating structured queries from natural language using reinforcement learning". In: 2017.
* [154] B. Zhou et al. "Semantic understanding of scenes through the ade20k dataset". In: _IJCV_ (2019).
* [155] K. Zhou et al. "Don't Make Your LLM an Evaluation Benchmark Cheater". In: _arXiv preprint arXiv:2311.01964_ (2023).
* [156] B. Zhu et al. _Starling-7b: Improving llm helpfulness & harmlessness with rlaif_. 2023.
* [157] D. Zhu et al. "Minigpt-4: Enhancing vision-language understanding with advanced large language models". In: _arXiv preprint arXiv:2304.10592_ (2023).
* [158] F. Zhu et al. "TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance". In: _ACL_. 2021.
* [159] Y. Zhu et al. "Visual7w: Grounded question answering in images". In: _CVPR_. 2016.

Broader Discussion

We advocate for using MLLMs as an interface to evaluate visual representations, as previous benchmarks are becoming saturated and do not adequately reflect the diverse and complex perception challenges of the real world. Our work highlights the current gap between language-supervised models and self-supervised learning models and demonstrates the potential of bridging this gap. However, it is known that features of language-supervised models behave like a _bag-of-words_[125; 144], underscoring the need for advancements in vision-only models to ensure better visual understanding. We hope to inspire future research into developing better vision-only models intended to be adapted into the MLLM setting, that more effectively leverage large-scale datasets [85] and preserve the advantages in visual grounding [126].

As we observe in Table 4, a well-trained open-source model such as Cambrian-1 can match or even outperform proprietary models on many existing benchmarks. However, the use and evaluation of MLLMs extend far beyond the current scope of benchmarks--to conversational ability, creativity, reliability, and overall user experience. Developing models solely based on benchmark results can result in an "answer machine", over-optimized for benchmarks but lacking in practical interaction capabilities. Therefore, the development of MLLMs that better align with human and societal needs is a continuously evolving process, both in terms of evaluation and model development.

Our current Cambrian-1 model uses a moderate number of visual tokens and does not adopt the any-resolution strategy [30; 77; 81] to handle ultra high-resolution images or those with extreme aspect ratios, which require a larger number of visual tokens. For specialized tasks like V*Bench [136], which require processing ultra high-resolution images, increasing the resolution and number of visual tokens could lead to an HD version of the Cambrian-1 model.

One promising direction for post-training alignment is through reinforcement learning rather than supervised fine-tuning. Many MLLM studies, including Cambrian, primarily focus on supervised fine-tuning. Yet, recent advancements in LLMs [38; 104; 110; 156] and some in MLLMs [141; 146] suggest that reinforcement learning from human or environmental feedback can further improve models, potentially surpassing the limits of supervised fine-tuning, especially in decision-making abilities.

Cambrian-10M (Fig. 8 and Section 4) provides a rich pool of data for studying data curation in fine-tuning MLLMs. Our work takes an initial step in curating higher-quality data to enable more efficient and effective instruction tuning. We believe there is room for further improvement in the data curation pipeline, and we hope this work can serve as a foundation for future research.

Additionally, training large-scale models requires careful design of model sharding, data sharding, and infrastructure adaptations. In this work, we train our model on TPU-V4 [60] with FSDP [150] using TorchXLA. We share our experiences, technical challenges, and solutions in Appendix C. We also open-source our implementation and provide tutorials to help the community undertake large-scale training more efficiently.

To conclude, Cambrian-1 introduces a family of state-of-the-art MLLM models that achieve top performance across diverse benchmarks and excel in visual-centric tasks. We provide model weights, open-source code, datasets, and detailed recipes for model training and evaluation. We hope our work will strengthen the open research community and accelerate future advancements in both visual representation learning and multimodal systems.

## Appendix B Multimodal LLMs: Preliminaries and Related Work

The key components of MLLM research include the _Large Language Model_, _Visual Encoder_, _Multimodal Connector_, _Data Curation Pipeline_, _Instruction Tuning Strategy_, and _Evaluation & Benchmarking_. Each component has its intricacies, and understanding their interactions presents significant challenges. Our study investigates these aspects from a vision-centric perspective.

Large Language ModelAdvanced LLMs [127; 4; 101; 4] are the foundation of an MLLM. After instruction-tuning on multimodal data, these models can be prompted to solve a variety of complex tasks and generate free-form responses leveraging input from a visual encoder. Recent MLLM research focuses on enhancing the LLM backbone [10; 75; 81], resulting in improved performance on benchmarks like MMMU [143] and AI2D [54]. However, this improvement raises the concern that our current _multimodal_ evaluation is biased by the development of LLMs, neglecting a true assessment of visual perception. For example, some benchmarks such as MMMU [143] are dominated by LLM capabilities, underscoring the need for evaluations that genuinely assess multimodality (see Section 2.1).

Visual EncoderMost MLLMs utilize language-supervised models like CLIP [109, 122, 145], which benefit from the massive scale of noisy web image-text data. However, there is a much broader pool of visual models that learn representations using only visual signals--such as self-supervised models [9, 103], segmentation [68], depth-supervised [15], and diffusion models [74, 112] (see Fig. 2). Recent work [87, 126] advocates for incorporating these diverse vision models into MLLMs. In this study, we systematically examine the impact of various vision backbones on MLLM performance (Section 1) and explore the benefits of model ensembles (Section 2.5).

Multimodal ConnectorRepresentations from a visual encoder cannot be natively processed by an LLM--they must be mapped into the LLM token space by a _connector_. There are three primary approaches to connector design: Resamplers [6], Q-Formers [11, 37], and MLP Projectors [43, 80, 82, 157]. We begin our exploration using an MLP projector, which is highly effective but presents challenges: the visual token count grows quadratically with image resolution, inhibiting scaling context length input resolution. For example, LLaVA-Next [81] requires 2880 visual tokens to process one 672px image. To address this, we explore new vision connector designs that process high-resolution images while maintaining a smaller number of visual tokens (Section 3).

Instruction Tuning DataVisual instruction tuning data is crucial but hard to collect, as it rarely naturally exists on the internet. Previous work [37, 80, 98] transforms existing VQA benchmarks [50, 69] into instruction tuning data, showing marked MLLM performance improvements. With this inspiration, we collect all VQA benchmarks and visual interaction data that we can find (Fig. 8), study data balancing and category mixtures (Section 4.2), and develop an internet data collection engine to fill in the gaps (Section 4.1).

Instruction TuningMost current MLLMs leverage pre-trained LLMs and visual encoders, fine-tuning the LLM and connector using visual instruction tuning data. Some aspects of the tuning recipe are up for debate, including whether to pre-train the connector before joint fine-tuning with the LLM, and whether to freeze or unfreeze the vision encoder during fine-tuning [63, 98]. Additionally, some recent proprietary models explore end-to-end training from scratch [49, 102]. In this work, we use pre-trained models and revisit the debated recipe aspects with extensive studies, providing more insights for future MLLM research (Section 2.3).

Evaluation & BenchmarkingThere is an extensive set of benchmarks that evaluate various aspects of MLLMs, such as perception [45, 83], knowledge [91, 92], chart interpretation [84, 96], and visual capabilities [126, 136]. Instead of over-optimizing for specific benchmarks, we advocate for examining aggregates of benchmarks that focus on specific capabilities. To achieve this, we analyze existing benchmarks, categorize them, and assess the extent to which they measure _multimodality_ (Section 2.1). Additionally, we find there are currently few benchmarks focused on vision-centric evaluation, and those that do exist contain relatively few images, leading to higher variance during evaluation. To address this issue, we propose a new vision-centric benchmark by reformulating classic vision tasks (Section 2.2).

## Appendix C Training, Infrastructure, and Implementation

All models in this paper were trained using TPU-V4 pods [60]; we evaluate using NVIDIA A6000, A100, and H100 cards. The experiments in Section 2.4 require less than 24 hours on a TPU-V4-128, while our final Cambrian-1 models are trained in less than 4 days on a TPU-V4-512.

To enable and facilitate large-scale parallel training on TPUs, we employ TorchXLA with FSDP [150] to handle training sharding and parallelism. Training a large-scale multimodal model with TorchXLA on TPU is a challenging journey, as there are no open-source codebases and many critical features are not supported in the TorchXLA or TorchXLA FSDP libraries. To provide a brief taste of the difficulties: TPUs require a static graph throughout the program, which requires ground-up rewrites of dynamically-written open-source PyTorch codebases; model resuming is not implemented in TorchXLA, which is especially crucial when training on preemptable TPUs; existing TorchXLA FSDP tutorials fail to compile due to version changes in TorchXLA, updates in Hugging FaceTransformers & Accelerate, or simply inherent issues with the tutorial; loading very large models (over 30 billion parameters) with the TorchXLA FSDP library is natively impossible due to the 100GB memory constraints of TPU-V4s, and requires extensive workarounds.

To this end, we have rewritten or developed many new functions to make this research possible. For instance, we rewrote the TorchXLA FSDP Sharding API to load very large models; we implemented model resuming on TorchXLA; we rewrote parts of the Hugging Face Transformers FSDP and gradient checkpointing implementations to enable large-scale FSDP training. We are committed to open-sourcing our codebase and publishing a comprehensive tutorial to share our insights, with the hope of inspiring and supporting future research and open-source contributions to the TPU and TorchXLA ecosystem.

## Appendix D Analyzing the Benchmarks

**MLLM Benchmark Performance Confusion Matrix**

We evaluate the benchmark scores for our one-stage, two-stage finetune-only and hybrid models, and then plot the correlation matrix for the pool of MLLM benchmarks. The correlation plot displays in Fig. 10. The result demonstrates that MMMU is less correlated in measuring model performance to other benchmarks. Nonetheless, we acknowledge it is widely used and therefore cluster it into knowledge-based QAs based on the nature of their questions.

## Appendix E Cambrian Vision-Centric Benchmark (CV-Bench)

**CV-Bench** CurationBelow we describe the procedure for programmatically constructing questions for each task. To ensure reliability, we also _manually inspect each question_, removing those that are unclear, ambiguous, or erroneous.

_Spatial Relationship (2D)._ We consider images with two distinct ground-truth object categories and use visual prompts (bounding boxes) to avoid ambiguity when multiple instances are present. In these questions, we designate an anchor object, and the question asks for the direction of the other object relative to this anchor.

_Object Counting (2D)._ This tests the model's ability to count objects. When generating options for these questions, we construct multiple-choice options that are similar to the correct answer. For

Figure 10: **Correlation matrix for MLLM benchmarks. The correlation matrix for MLLM benchmarks with respect to different vision backbones. The correlation matrix helps us to analysis and group benchmarks.**

example, if the correct answer is 4, the options might be 2, 3, 4, 5, & 6. We also include existence check examples where the correct count is 0.

Depth Order (3D).We consider images with two distinct categories (i.e., object A and object B) and use visual prompts (e.g., bounding boxes with two different colors) to avoid ambiguity. We define "closer" as follows: object A is closer to the camera than object B only if the farthest vertex of object A is closer3 to the camera than the nearest vertex of object B by a specified offset.

Footnote 3: We use the Euclidean distance.

Relative Distance (3D).We consider images with three distinct categories (i.e., anchor, object A, and object B), and use visual prompts (e.g., bounding boxes with three different colors) to avoid ambiguity. Object A is closer than object B only if the farthest distance from A's vertices is shorter than the shortest distance from B's vertices to the anchor object by a certain offset.

Curation ProcedureWe provide an overview of the data curation process in Fig. 12, which is conducted in a semi-automatic manner. The procedure consists of two main steps:

First, using the original benchmarks and their associated ground truth annotations, we generate query and answer pairs. These pairs are tailored to specific tasks: 2D-related tasks with COCO and ADE20K datasets, and 3D-related tasks with Omni3D.

Second, after generating the query and answer pairs, we engage human experts to manually filter out any incorrect or ambiguous queries to enhance the quality of benchmark. Each query is assigned one of three statuses: accepted (used as is), modified (where the incorrect answer is modified), and rejected (queries that are ambiguous, such as those too small or difficult to discern, even for human experts).

Following this two-stage process, we finalize the benchmark, which results in a total of 2638 image queries with improved accuracy and reliability. Subsequently, we will discuss the methods of human verification and the evaluation metrics used in this process.

Human verification

Figure 11: **Cambrian Vision-Centric Benchmark (CV-Bench). We repurpose standard vision benchmarks to evaluate the fundamental 2D and 3D visual understanding of MLLMs. See Section 2.2 for more details.**

\begin{table}
\begin{tabular}{c l l l l} \hline \hline
**Type** & **Task** & **Description** & **Sources** & **\# Samples** \\ \hline
**2D** & **Spatial** & \begin{tabular}{l} Determine the relative position of an object w.r.t. \\ the anchor object. Consider left-right or top- \\ bottom relationship. \\ \end{tabular} & \begin{tabular}{l} ADE20K \\ COCO \\ \end{tabular} & 650 \\ \cline{2-5}  & **Object** & \begin{tabular}{l} Determine the number of instances present in the \\ image. \\ \end{tabular} & 
\begin{tabular}{l} ADE20K \\ COCO \\ \end{tabular} & 788 \\ \hline
**3D** & **Depth** & \begin{tabular}{l} Determine which of the two distinct objects is \\ closer to the camera. \\ \end{tabular} & \begin{tabular}{l} Omni3D \\ \end{tabular} & 600 \\ \cline{2-5}  & **Relative** & \begin{tabular}{l} Determine which of the two distinct objects is \\ closer to the anchor object. \\ \end{tabular} & 
\begin{tabular}{l} Omni3D \\ \end{tabular} & 600 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Breakdown of the 2D and 3D tasks evaluated in the Cambrian Vision-Centric Benchmark (CV-Bench). The examples are sourced from ADE20K [154], COCO [79], and Omni3D [18].

There are multiple reasons for the above generated data to be inaccurate. One of the main reasons is sparse annotations, but occasionally there could be wrong annotations as well.

Thus, we need manual inspection to change/remove these examples generated. Here are a few criteria we followed while manually filtering COCO and ADE20k data.

For **Counting** question types, if all instances of a category are not annotated, the ground truth would have lower count than the actual number of instances appearing in the image. In a few cases where the image distinctly has different countable instances of the object, we change the options/answer. In case the count is ambiguous, we reject the data sample altogether.

For **Relative Distance** question types without annotation, if the question is asked about two objects A and B and if there are two instances of a specific category (say A), the relative location of A w.r.t B can be have multiple correct answers. We reject the sample in this case. We also reject cases with clear incorrect annotations.

Benchmark EvaluationTo ensure that equal importance is given to both 2D and 3D tasks, we use an evaluation metric that is the average of the accuracies obtained from these tasks. Specifically, the overall performance is calculated as follows:

\[\text{Accuracy}_{2D}=\Big{(}\frac{\text{Accuracy}_{COCO}+\text{Accuracy}_ {ADE20k}}{2}\Big{)}\]

\[\text{Overall Accuracy}=\Big{(}\frac{\text{Accuracy}_{2D}+\text{Accuracy}_ {3D}}{2}\Big{)}\]

## Appendix F Vision Models in MLLMs

As mentioned in Section 2.4, we use MLLM as an interface to evaluate vision model's different capabilities. Here, we list details in terms of the model selection, full results, and data split.

### Details of Vision Models

In our exploration of versatile vision models, we select thirteen models and group them into four categories: _language-supervised models_ (i.e., OpenAI CLIP [109], SigLIP [145], DFN-CLIP [40], EVA-CLIP [122] and OpenCLIP [33]), _self-supervised models_ (i.e., DINOv2 [103], I-JEPA [9], MAE [52], MoCo v3 [29]), _class-supervised models_ (ImagetNet22K ViT [39]) and _other models_ such as stable diffusion [112], segmentation models like SAM [68], and depth estimation models like MiDaS [15]. To provide a clear understanding of the specific variant evaluated, we meticulously detail their backbone architectures, resolution, number of tokens, and hidden dimension sizes in Table 6. For models that output a large number of patches in the last layer (e.g., SAM and ConvNeXt) we interpolate to the number of tokens specified in Table 6, and denote interpolation with 1.

Figure 12: **CV-CB Benchmark Filtering. We reformulate classic 2D and 3D CV benchmarks into Q&A questions to evaluate MLLM’s visual capabilities.**

### Full Results of Different Vision Backbones

Here, we also show a ranking version of Fig. 5. We observe a clear advantage of CLIP models over non-CLIP models. We also observe that within the family of CLIP models, each model perform differently in different domains. This provide insight into both vision model development and data curation in training large vision models.

For the above-listed vision models in Table 6, they are integrated as the vision encoder of the MLLMs. These MLLMs are trained on various adapter adapter data splits (i.e., 0, 0.5 and 1.2 million), and subsequently fine-tuned on a 737K instruction tuning dataset provided in LLaVA-1.5[80]. For the adapter data splits, the 0M split indicates that no initial adapter pertaining phase is employed for the

\begin{table}
\begin{tabular}{l l l r r r r} \hline
**Supervision** & **Method** & **Architecture** & **Patch Size** & **Res.** & **\# Tok.** & **Hidden Size** \\ \hline Language-Supervised & & & & & & \\ \hline Language & OpenAI CLIP & ViT-L & 14 & 336 & 576 & 768 \\  & DFN-CLIP & ViT-L & 14 & 224 & 256 & 1024 \\  & DFN-CLIP & ViT-H & 14 & 378 & 729 & 1280 \\  & EVA-CLIP-02 & ViT-L & 14 & 336 & 576 & 1024 \\  & SigLIP & ViT-L & 16 & 384 & 576 & 1024 \\  & SigLIP & ViT-SO400M & 14 & 384 & 729 & 1152 \\  & OpenCLIP & ConvNeXT-L & - & 512 & 1576 & 1536 \\  & OpenCLIP & ConvNeXT-L & - & 1024 & 1576 & 1536 \\  & OpenCLIP & ConvNeXT-XXL & - & 1024 & 1576 & 3072 \\ \hline Self-Supervised & & & & & & \\ \hline Contrastive & DINov2 & ViT-L & 14 & 336 & 576 & 1024 \\  & DINov2 & ViT-L & 14 & 518 & 1576 & 1024 \\  & MoCo v3 & ViT-B & 16 & 224 & 196 & 768 \\  & MoCo v3 & ViT-L & 16 & 224 & 196 & 1024 \\ Masked & MAE & ViT-L & 16 & 224 & 196 & 1024 \\  & MAE & ViT-H & 14 & 224 & 256 & 1280 \\ JEPA & I-JEPA & ViT-H & 14 & 224 & 256 & 1280 \\ \hline Other & & & & & & \\ \hline Segmentation & SAM & ViT-L & 16 & 1024 & 1576 & 1024 \\  & SAM & ViT-L & 16 & 1024 & 1576 & 1280 \\ Depth & MiDaS 3.0 & ViT-L & 16 & 384 & 576 & 1024 \\  & MiDaS 3.1 & ViT-L & 16 & 518 & 1024 & 1024 \\ Diffusion & Stable Diffusion & VAE+UNet & 16 & 512 & 1024 & 3520 \\  & 2.1 & & & & & \\ Class Labels & SupViT & ViT-L & 16 & 224 & 196 & 1024 \\  & SupViT & ViT-H & 14 & 224 & 256 & 1280 \\ \hline \end{tabular}
\end{table}
Table 6: Catalog of all vision backbones tested. 1 denotes that the visual tokens have been interpolated down to the specified length.

\begin{table}
\begin{tabular}{l l l r r r} \hline \hline
**Method** & **Architecture** & **Patch Size** & **Resolution** & **\# Tokens** & **Linear Probing (\%)** \\ \hline EVA-CLIP-02 & ViT-L & 14 & 336 & 576 & 85.0 \\ DFN-CLIP & ViT-L & 14 & 224 & 256 & 83.6 \\ DINov2 & ViT-L & 14 & 336 & 576 & 83.1 \\ OpenCLIP & ConvNeXt-L & - & 512 & 576 & 82.9 \\ OpenAI CLIP & ViT-L & 14 & 336 & 576 & 80.3 \\ I-JEPA & ViT-H & 14 & 224 & 256 & 77.0 \\ Supervised & ViT-L & 16 & 224 & 196 & 74.5 \\ MoCo v3 & ViT-B & 16 & 224 & 196 & 71.9 \\ MiDaS & ViT-L & 16 & 384 & 576 & 70.1 \\ MAE & ViT-L & 16 & 224 & 196 & 68.3 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Linear Probing Results of Different Vision Backbones

[MISSING_PAGE_FAIL:24]

[MISSING_PAGE_FAIL:25]

from the last layer. The shape of each model's output differs depending on the resolution and patch size of each vision model. To resolve these differences, we interpolate the output of each model to a fixed number of tokens, using 576 tokens in our implementation, as described in Section 2.5. Our example code for interpolation can be seen below.

Example code for interpolation b, num_tokens, dim = image_features.shape if num_tokens = self_image_token.len: target_n = target_n = target_n = img_sort(self_image_token_len) h = v = img_sort(norm_token) image features = image_features.size(b, h, y, dim) image_features = image_features = image_features.permute(0, 3, 1, 2).contiguous() image_features = F.interpolate(image_features), size={target_h, target_w}, node='bilinear', slign_corners-False) image_features = image_features.permute(0, 2, 3, 1).contiguous().flatten(1, 2)

We then concatenate the model outputs along the feature dimension and use a larger MLP to project the concatenated visual tokens into the LLM token space.

Full results on Model EnsembleWe present all the benchmarks from the model ensemble experiment in Section 2.5 in Table 14. As discussed in Section 1 and Section 2.4, this comprehensive view of benchmarks provides a better understanding of the model's performance compared to simply averaging across benchmarks. Adding a vision-only SSL model enhances the MLLM's performance in vision-centric benchmarks while maintaining strong capabilities in other categories.

## Appendix G Data

### Catalog of Visual Instruction Data

Here, we provide a comprehensive catalog of visual instruction datasets utilized in our study. The datasets are categorized based on their primary focus, including general conversation and VQA data, OCR-related data, counting data, knowledge-based data, and language-only data. Table 15 summarizes these datasets and their respective references.

\begin{table}
\begin{tabular}{l l|c|c c c c|c c c c|c c c c|c c c c} \multicolumn{1}{c}{} & \multicolumn{1}{c|}{Vision Backbone} & \multicolumn{1}{c|}{General} & \multicolumn{3}{c|}{Knowledge} & \multicolumn{3}{c|}{OCR \& Chart} & \multicolumn{3}{c}{Vision-Centric} \\ Method & Backbone & Unfreeze & & & & & & & & & & & & & & & & & & & & & & & & \\ OpenALCIF & VLT/1449 336 & 3 & 53.85 & 1.5773.3 & 3.96 & 70.02 & 62.63 & 33.76 & 36.76 & 36.19 & 36.60 & 64.81 & 48.12 & 34.90 & 63.33 & 39.79 & 26.73 & 56.56 & 56.81 & 58.97 \\ DINO/2 & VLT/1449 336 & \(\times\) & 3.45 & 3.63 & 1.731.4 & 37.04 & 62.81 & 61.67 & 61.73 & 36.19 & 37.00 & 60.61 & 62.91 & 30.40 & 34.69 & 31.72 & 26.57 & 52.65 & 59.81 & 57.91 \\ OpenAI CLIP & VLT/1449 336 & \(\times\) & **37.941** & **1.585.34** & 66.68 & **71.42** & **63.96** & 71.39 & 36.69 & **37.30** & 65.81 & **29.36** & 48.00 & 62.39 & 45.24 & 31.33 & **56.21** & 61.09 & 56.16 \\ DINO/2 & VLT/1449 336 & \(\times\) & **37.941** & **1.585.34** & 66.92 & **61.62** & 69.72 & 63.63 & 68.72 & **60.29** & 35.50 & 60.83 & 18.64 & 4.00 & 47.92 & 14.66 & **54.64** & 69.98 & 57.83 \\ \end{tabular}
\end{table}
Table 13: **All Benchmark Results for 1.2M Adapter Data + 5M Instruction Tuning Data**

[MISSING_PAGE_EMPTY:27]

We suspect that this issue stems from instruction tuning data containing an excessive number of short-response VQA tasks, leading to catastrophic forgetting in LLMs. To address this, we incorporate additional system prompts during training. We append prompts such as "_Answer the question using a single word or phrase._" before questions that generate a single word or phrase in the response. Full details of the system prompts used are provided in Appendix G.2. After integrating these system prompts, we observe that while the model's benchmark performance remains unchanged, its conversational ability improves dramatically. For example, in Fig. 14, models with system prompts produce longer and more engaging responses while answering questions correctly. The system prompts also enhance the model's performance on reasoning-related tasks, such as math problems, by encouraging a chain of thoughts [133] followed by the answer.

This underscores the necessity of developing evaluation protocols like the Chatbot Arena [34] for MLLMs, despite the challenges in collecting large-scale, real-world interaction data. While performing well on benchmarks is important, it is equally crucial to ensure the model can engage in meaningful and natural interactions. The overall user experience and the model's conversational abilities are paramount, as a model that excels in benchmarks but fails to converse effectively cannot meet the needs of practical applications.

As our Cambrian data includes instructions/questions and responses of different types and formats (e.g., Short response with a single word or regular response as a complete sentence), it is important to specify the required response format in the instruction prompt to avoid ambiguity and possible conflicts. Some of the datasets already include such prompts and we add proper prompts for the remaining datasets. The detailed response formatting prompts we additionally add are listed in Table 16.

### Data Engine

**Comprehensive Implementation Details of the Data Engine**

The data engine is designed to generate instruction tuning data for knowledge-based fields, where previous works rarely covers and MLLMs are not reliable to distill for from. The data engine takes in a given field, such as "Physics", utilizing reliable web sources like Wikipedia. Below are the various stages involved in the process. We also visualize this process in Fig. 15:

**Stage 1 - Topic Generation:** We start by compiling a list of fields and subfields and subsequently generate topics for each field using a Large Language Model (LLM), such as GPT-4. In this stage,

\begin{table}
\begin{tabular}{l l} \hline \hline
**Category** & **Datasets** \\ \hline
**General Conversation** & LVIS-Instruct4V [131], SketchyVQA [129], OODVQA [129], VizWiz [51], ALLaVA [25], IDK [22], Q-Instruct [135], LAION GPT-4V [70], Hateful-Memes [66], Visual7W [159], Visualmrc [123], AlfWorld [117], LNQA [108], LLaVA150K [82], ShareGPT [27], VQAV2 [50], GQA [57], OKVQA [95], A-OKVQA [116], RefCOCO [140], VisualGenome [69], GPT-4V recorded chat \\ \hline
**OCR Related Data** & LLAVAR [149], ChartQA [96], DocVQA [97], DVQA [61], ArxivQA [76], AI2D [65], ScreenQA [56], SynthDog [67], IconQA [89], WTQ [106], WikiSQL [153], FinQA [31], HiTab [32], TAT-QA [158], TabMWP [88], Chart2Text [62], VisText [124], InfoVQA [16], ST-VQA [17], Rendered-Text [134], OCRQA [100], TextCaps [119], ShareGPTOCRData [27] \\ \hline
**Counting Data** & TallyQA [1], CLEVR [59] \\ \hline
**Knowledge-Based Data** & **Code:** Design2Code [118], WebSight [71], Datikz [14] \\  & **Math:** MathVision [132], Geo170K [42], TQA [5], Inter-GPS [90], RAVEN [148], GeomVrese [64] \\  & **Science:** ScienceQA [91], PathVQA [53] \\ \hline
**Language Only Data** & Dolly [36], MathInstruct [142], WizardCoder [93], OrcaMath [99], OpenCodel interpreter [152], OpenOrca [78] \\ \hline \hline \end{tabular}
\end{table}
Table 15: Visual Instruction-Tuning Data Catalogwe processed 30 fields, resulting in 3660 topics. We then post-process the output of LLMs into json formats. For example, the topic data for Physics looks like below.

Physics

 "Classical Mechanics": [ "Setuons" Laws of Motion", "Conservation of Energy", "Conservation of Momentum", "Harmonic Motion", "Rotational Dynamics", "Gravitation and Orbits", "Fluid Dynamics", "Elasticity and Plasticity", "Friction", "Lawness and Sound", "Felocity and Acceleration", "Barlight Momentum", "Statics and Equilibrium", "Kinematics of Particles", "Dynamic of Systems of Particles", "Collisions", "Contripetal Force and Acceleration", "Lagrangian and Hamiltonian Mechanics", "Chos Theory", "Equations of Motion" ], "Electromagnetism": [ "Coulomb-Laws", "Electricity Field and Electric Potential", "Gauss-5 Low", "Capacitance and Dielectric", "Current and Resistance",

Figure 14: **Incorporating System Prompt in Instruction Tuning Data alleviates the “Answer Machine Phenomenon”** By adding system prompts in Cambrian-7M, the model exhibits better chat ability while retaining strong question answering abilities. The model without system prompts requires additional prompting to elicit longer responses.

Figure 15: **Targeted Internet Data Collection Engine. We build a targeted internet data engine to collect high-quality and large-scale multimodal instruction tuning data for domains like knowledge.**

"Direct Current Circuits", "Respactic Fields and Magnetic Forces", "Agree" is Law", "Faraday's Law of Induction", "Indactance", "Alternating Current Circuits", "Electromagnetic Waves", "Maxwell's Equations", "Electromagnetic Radiation", "Optics and Light", "Quantum Electrodynamics", "Special Theory and Relativity Implication", "Magnetostatic", "Electrostatic", "Electrostatic", "Bioelectromagnetic", "Bioelectromagnetic",...

**Stage 2 - Filtering Web Data:** For each generated topic, we utilize search engine APIs to fetch relevant high-quality web pages. For each topic, we query for 10 relevant links. Thus, we get 36,600 webpages post this stage. Here is an example of the data retrieved for the topic "Electric Field and Electric Potential":

"Electric Field and Electric Potential": [

"https://en.wikipedia.org/wiki/Electric_potential", "https://en.wikipedia.org/wiki/Electric_field", "https://en.wikipedia.org/wiki/Electric_potential_energy".

\begin{table}
\begin{tabular}{l|l} \hline Index & Response formatting prompts \\ \hline
1 & Answer the question using a single word or phrase. \\
2 & Answer the question using a single number or phrase. \\
3 & Answer with the option’s letter from the given choices directly. \\
4 & Give the short answer directly. \\
5 & Answer the question using a single word or phrase. \\
6 & When the provided information is insufficient, respond with <no answer>. \\
7 & Directly provide the HTML code. \\
8 & First show your reasoning process and then give the final answer. \\
9 & When the provided information is insufficient, respond with ’Unanswerable’. Answer the question using a single word or phrase. \\
10 & Answer with the letter. \\ \hline \hline Dataset & Prompts added \\ \hline SketchVQA & 1 \\ OODVQA & 1 \\ VizWiz & 9 \\ Q-Instruct & 1, 3 \\ ChartQA & 2 \\ DocVQA & 4 \\ DVQA & 1 \\ AI2D & 1 \\ ScreenQA & 1, 6 \\ CLEVR & 1 \\ TallyQA & 1 \\ PathVQA & 1 \\ MathInstruct & 8 \\ Design2Code & 7 \\ IconQA & 1, 10 \\ HiTab & 1 \\ WTQ & 1 \\ WikiSQL & 1 \\ Inter-GPS & 10 \\ Visual7W & 3 \\ TQA & 10 \\ RAVEN & 1 \\ \hline \end{tabular}
\end{table}
Table 16: Response formatting prompts for Cambrian Data* "https://en.wikipedia.org/wiki/List/stage" "https://en.wikipedia.org/wiki/Electricity", "https://en.wikipedia.org/wiki/Electrictics", "https://en.wikipedia.org/wiki/List/_topic_global_moment", "https://en.wikipedia.org/wiki/Bagnetic_vector_potential", "https://en.wikipedia.org/wiki/Electric_field_screening", "https://en.wikipedia.org/wiki/Electric_flux" }.

**Stage 3 - Parsing:** In this stage, we parse each web page to extract image-caption-text tuples. We aim to identify the blocks containing an image, the image's caption, and relevant textual content. Below is an example of the parsed data for the same topic, "Electric Field and Electric Potential":

{  "Electric Field and Electric Potential",  {  {  "section": "Electrostatics",  "text": "An electric potential at a point r in a static electric field E is given by the line integral where C is an arbitrary path from some fixed reference point to r; it is uniquely determined up to a constant... The generalization of electric potential to this case is described in the section Generalization to electrodynamics.",  "imagea": [  "url": https://upload.wikimedia.org/wikimedia/commons/thumb/l/la/VFPt_plus_thumb_potential+contour.svg/142ps-VFPt_plus_thumb_potential+contour.svg.png,  "caption": "Electric potential of separate positive and negative point charges shown as color ranges from magenta (+), through yellow (0), to cyan (-). Circular contours are equipotential lines. Electric field lines leave the positive charge and enter the negative charge."  },  {  "url": https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/VFPt_charge_plus_minus_potential+contour.svg/280ps-VFPt_charges_plus_minus_potential+contour.svg.png,  "caption": "Electric potential in the vicinity of two opposite point charges."  }  }  "link": https://en.wikipedia.org/wiki/Electric_potential,  "title": "Electric potential",  "field": "Physics",  "aufield": "Electromagnetism",  "topic": "Electric Field and Electric Potential"  }, ...  }  }  }

**Stage 4 - Data Generation:** We generate dataset in this stage, ensuring high quality. We first filter out data samples with _fewer than 50 words_ in the text. Then, instead of downloading images directly from the links retrieved during web parsing, we download high-resolution images from the original sources. We then convert formats like SVG or GIF into a common standardized format, PNG.

Question-Answer pairs are generated by using LLM such as GPT-3.5 from the image metadata, caption, and contextual text. These Q&A pairs and the image form our VQA dataset. We generated 165k data samples. Here is an example of the generated data:

 {  "id": "96232.png",  "image_id": "450px-Magnetic_Vector_Potential_Circular_Toroid",  "image_url": "...",  "text": "...",  "caption": "Representing the Coulomb gauge magnetic vector potential A, magnetic flux density B, and current density J fields around a toroidal inductor of circular cross section. Thicker lines indicate field lines of higher average intensity. Circles in the cross section of the core represent the B-field coming out of the picture, plus signs represent B-field going into the picture. \(\backslash\)u2207 \(\backslash\)u22c5 A = 0has been assumed",

"section": "Depiction of the A-field",

"...",

"Question": "What do the thicker lines signify?",

"Answer": "The thicker lines in the image represent paths of higher average intensity in the magnetic vector potential A field. Shorter paths have higher intensity, ensuring that the path integral remains constant. These lines are drawn to visually convey the general behavior and intensity distribution of the A-field around the toroidal inductor"

}

This data engine is designed for scalability and efficiency and is capable of handling extensive data generation tasks using multithreading techniques.

### Full results on data curation experiment

**Data Balance via Filtering \(t\)** As discussed in Section 4.2, if left unfiltered, the data pool is dominated by noisy, unbalanced data sources such as CLEVR and DVQA, leading to pronounced exponential tails. However, as we apply different \(t\) values to filter data from each source, the exponential tails become less pronounced, resulting in a more balanced dataset. We also present all the results in Table 17. \(t\) value 250k has the highest average across all benchmarks; 250k and 350k also have the highest performance across many individual benchmarks.

Here, we plot the cumulative sum of counts for entries sorted by counts from tail to head. From Fig. 17, we see this intermediate threshold prevents explosive heavy tail.

**Data Ratio Studies** We present the full results of our data ratio study in Table 18. The table highlights the importance of finding an optimal data ratio that balances different aspects of MLLM. Experiment 5 achieves well-rounded performance with its selected data ratio.

\begin{table}
\begin{tabular}{c|c|c c c c c|c c c c c|c c c c c c} \multicolumn{13}{c}{} \\ \multicolumn{13}{c|}{\# of Data} & \multicolumn{1}{c|}{General} & \multicolumn{3}{c|}{Knowledge} & \multicolumn{3}{c|}{OCR \& Chart} & \multicolumn{3}{c}{Vision-Centric} \\ \hline \(t=150\)k 4015k & 53.74 & 1,512.3 & 67.0 & 68.3 & 61.2 & 73.4 & 35.1 & 34.3 & 62.4 & 44.6 & 39.0 & 58.5 & 38.5 & 30.0 & 55.2 & 61.98 & 54.7 \\ \(t=250\)k 5218k & 54.31 & 1,475.9 & 67.3 & 69.2 & 61.6 & 73.4 & 35.9 & 34.5 & 62.4 & 46.5 & 36.9 & 59.2 & 38.6 & **32.0** & **56.6** & 63.68 & 57.5 \\ \(t=350k\) 5883k & 54.27 & 1,461.9 & 66.2 & 68.9 & 61.6 & **73.8** & **36.4** & 32.8 & 62.5 & **46.8** & 38.3 & 59.3 & 39.3 & 31.3 & 54.9 & 62.68 & **60.4** \\ \(t=450k\) 6383k & 54.15 & 1,534.1 & 67.6 & 66.3 & 61.9 & 72.9 & 35.1 & **36.9** & 63.8 & 45.8 & 38.6 & 58.4 & 39.4 & 28.0 & 53.6 & **64.0** & 56.8 \\ \end{tabular}
\end{table}
Table 17: **All Benchmark Results for Data Balancing Experiments**

Figure 16: **Dataset Image Id: 96232.png**

### 737K and 5M Mixes

**0.7M** For the 0.7M data we used in Section 2.4, We add a small number of OCR and chart data to LLaVA 665K, specifically 15,501 AI2D, 14,999 DocVQA, and 13,000 DVQA data points. This results in a 737K mix, which covers all categories in training MLLMs. This data mix allows us to study visual representations efficiently.

**5.0M** For the 5M data mixes we use in Section 2.4, we apply data filtering discussed in Section 4.2 and apply \(t\)=150k on all multimodal instruction data in Cambrian-10M.

### Test Image Leakage in Visual Instruction Training Data

One potential concern with our targeted data engine (Section 4.1) is that instruction-tuning data collected from the open web could introduce data leakage. To address this, we systematically analyze the extent of direct image matches between our training data and our test sets. Using difference hashing (dHash) [19], we compute hashes for all images in the training data and test sets. We then compare these hash sets to determine how many test images overlap with our training data, reporting the number of collisions in Table 19.

Across all fifteen datasets, our targeted data engine finds only 32 test images in total, amounting to just 0.06% of the test data. This low overlap percentage dispels concerns that our data engine inadvertently targets specific test sets. When analyzing the full Cambrian10M dataset--which is 15x larger than LLaVA-665k--we observe only 6x more matching test images (7,244 compared to 1,034 in LLaVA-665k). This discrepancy suggests that Cambrian10M's scale does not inherently result in excessive overlap with test sets. Instead, any overlap likely arises from the natural reuse of training images across benchmark datasets rather than targeted duplication.

Figure 17: **Data Balancing via Applying Thresholds on Data Sources**. Applying threshold \(t\) alleviates the exponential tail of Cambrian-10M.

\begin{table}
\begin{tabular}{c|c|c c c c|c c c c|c c c c|c c c c}  & & \multicolumn{3}{c|}{General} & \multicolumn{3}{c|}{Knowledge} & \multicolumn{3}{c|}{OCR \& Chart} & \multicolumn{3}{c}{Vision-Centric} \\ \cline{3-14}  & & & & & & & & & & & & & & & & & & & & \\ \hline exp1 & 47.49 & 1,309.10 & 58.00 & 60.10 & 54.00 & 72.40 & 34.80 & 31.20 & 59.10 & 34.20 & 34.50 & 54.20 & 33.00 & 13.30 & 47.60 & 57.40 & 50.58 \\ exp2 & 47.78 & 1,351.70 & 60.30 & 61.20 & 55.40 & 72.80 & 35.20 & 29.50 & 59.10 & 31.20 & 33.40 & 54.20 & 30.50 & 15.00 & 34.80 & 52.25 \\ exp3 & 48.28 & 1,299.53 & 605.61 & 61.79 & 55.74 & 72.04 & 34.90 & 28.10 & 59.40 & 33.20 & 33.90 & 54.15 & 31.90 & 21.30 & 48.60 & 58.52 & 49.41 \\ exp4 & 47.47 & 1,288.98 & 58.16 & 61.47 & 55.00 & 71.05 & 37.10 & 28.20 & 58.50 & 33.72 & 34.50 & 58.00 & 31.69 & 20.66 & 47.06 & 56.30 & 46.58 \\ exp5 & 48.96 & 1,363.26 & 60.48 & 63.18 & 55.92 & 70.35 & 35.70 & 31.40 & 57.19 & 32.88 & 54.60 & 54.74 & 32.10 & 22.70 & 47.30 & 58.83 & 57.75 \\ \end{tabular}
\end{table}
Table 18: **All Benchmark Results for Data Ratio Experiments with fixed 1350k data**It is important to emphasize that while some exact image matches are found, this does not imply that the exact image-question pairs have been encountered during training. Unlike in traditional unimodal computer vision research, where an image alone constitutes a data point, the multimodal paradigm treats each image-text (question-answer) pair as unique. Consequently, seeing a test image during training is not equivalent to "training on the test set" as long as the associated text (question-answer) pairs differ. This distinction ensures that Cambrian10M respects the integrity of test evaluations, even in cases where images might appear in both training and test sets.

We encourage future research exploring the impact of image-only leakage on the performance of MLLMs. Understanding this influence may yield insights into the boundaries of model generalization and guide future best practices for dataset construction in multimodal learning.

### Broader Impacts

We conducted a preliminary analysis of the Cambrian dataset, focusing on the distribution of male, female, and neutral pronouns. Our findings show the following distribution: 38.35% male pronouns, 17.99% female pronouns, and 43.66% neutral pronouns.

We recognize that training models on biased data can perpetuate these biases. Addressing bias by artificially modifying data distributions--such as through rebalancing or applying fairness constraints--can help mitigate this issue, but it also presents challenges. These include the potential loss of generalization and the risk of introducing new biases. Additionally, identifying and mitigating bias in Multimodal Large Language Models (MLLMs) is particularly complex, given the interaction

\begin{table}
\begin{tabular}{l l r r r r r} \hline \hline Category & Test Set & \# Images & _Data Eng._ & _Cambrian10M_ & _LLaVA-665k_ \\ \hline General & MMEP & 2,374 & 0 (0.00\%) & 332 (13.98\%) & 82 (3.45\%) \\  & MMB & 4,377 & 7 (0.16\%) & 1,122 (25.63\%) & 533 (12.18\%) \\  & SEEDI & 17,990 & 6 (0.03\%) & 26 (0.14\%) & 0 (0.00\%) \\  & GQA & 398 & 0 (0.00\%) & 1 (0.25\%) & 0 (0.00\%) \\ \hline Knowledge & SQAI & 2,017 & 0 (0.00\%) & 1,263 (62.62\%) & 0 (0.00\%) \\  & MMMUV & 900 & 1 (0.11\%) & 3 (0.33\%) & 0 (0.00\%) \\  & MathVistaM & 1,000 & 2 (0.20\%) & 259 (25.90\%) & 15 (1.50\%) \\  & AI2D & 3,088 & 0 (0.00\%) & 1,458 (47.22\%) & 0 (0.00\%) \\ \hline OCR \& Chart & ChartQA & 2,500 & 14 (0.56\%) & 670 (26.80\%) & 0 (0.00\%) \\  & OCRBench & 1,000 & 0 (0.00\%) & 177 (17.70\%) & 59 (5.90\%) \\  & TextVQA & 5,000 & 2 (0.04\%) & 1,122 (22.44\%) & 9 (0.18\%) \\  & DocVQA & 5,188 & 0 (0.00\%) & 53 (1.02\%) & 0 (0.00\%) \\ \hline Vision-Centric & MMVP & 300 & 0 (0.00\%) & 0 (0.00\%) & 0 (0.00\%) \\  & RealWorldQA & 765 & 0 (0.00\%) & 0 (0.00\%) & 0 (0.00\%) \\  & CV-Bench & 2,638 & 0 (0.00\%) & 758 (28.73\%) & 336 (12.74\%) \\ \hline
**Total** & & 49,535 & 32 (0.06\%) & 7,244 (14.62\%) & 1,034 (2.07\%) \\ \hline \hline \end{tabular}
\end{table}
Table 19: **Number of leaked test set images**. Using image hashing, we assess the overlap of test images across three training datasets: _Cambrian10M Data Engine 161k subset (“Data Eng.”)_, _Cambrian10M_, and _LLaVA-665k_. We list the number of images in each test set, as well as the number of matching images and percentage of overlap for each training set in blue. Our Data Engine finds a neglible 0.06% of test images, dispelling any concerns that it is targeting the test sets. The full Cambrian10M training set contains 7,244 test set images, whereas LLaVA-665k contains 1,034. Despite being a 15x larger dataset, Cambrian10M only has 6x more overlapping images. Such overlap is inevitable since many test sets use validation images from standard benchmarks (like COCO). It is worth highlighting: **although exact image matches are found, this does not mean that exact image-question pairs have been found.** Unlike in prior _unimodal_ paradigms of computer vision research, in the _multimodal_ setting, a single data point is composed of an image-text (question) pair, not just the image itself. Thus, seeing a test image during training is not equivalent to “training on the test set” so long as the training image does not have the same text pair as the test data point.

between different data modalities. We believe that openness in model development and data curation will accelerate research aimed at understanding and mitigating these potential harms.

## Appendix H Implementation Details

**Cambrian Models** For our final Cambrian models, we use 2.5M adapter data which is comprised of 1.2M captioning data from shareGPT4V [27] and 1.2M captioning data used in MiniGemini [77].

**SVA** We provide here ablation studies of SVA module.

We further conduct ablation experiments using OpenAI CLIP ViT-L/14@336 + OpenCLIP ConvNeXt-L@1024 as our base model combination. We focus on the OCR & chart categories to assess the impact on high-resolution visual understanding. The results show that increasing capacity via \(D\) or \(G\) improves performance and that allowing vision aggregation across multiple layers by adding cross-attention layers within the LLM also enhances performance.

Compared with other spatial-based connectors like C/D-Abstractor [21] which are designed for single vision feature maps, our SVA module can dynamically combine visual features from multiple vision models with varying resolutions. Besides, our spatial inductive bias in SVA can better compress spatial information compared with such methods. To isolate the effect of spatial inductive bias, we consider the case of token reduction using a single vision encoder. Specifically, we use OpenAI CLIP ViT-L as the vision model and compress its original 576 tokens to 36 tokens using our SVA module and other connectors. We compare our SVA module with three baselines: 1) Direct interpolation + MLP, 2) C-Abstractor [21], and 3)LDpv2 Projector [35] (similar to C-Abstractor but more lightweight). For fair comparisons, we do not include multi-layer aggregation inside the LLM for our SVA baseline, and the results are shown in Table 21. Compared with the simple MLP baseline, C-Abstractor performs better on General and Vision-Centric tasks but inferior on Knowledge and OCR & Chart tasks. LDPv2 performs similarly to the MLP baseline. Our SVA consistently demonstrates superior performance across all categories, especially in OCR & Chart and Vision-Centric tasks, demonstrating its effectiveness in information compression.

We introduce learnable \(k_{m}\times k_{m}\) positional encodings in the vision features when \(k_{m}>1\). Besides, during cross-attention, the query is augmented with a global feature obtained by global pooling over the vision features, which is concatenated with \(\mathbf{q}_{i,j}\) to better guide the aggregation process. In our experiments, the feature maps of all vision encoders except for ConvNext are interpolated to 576\(\times\)576 (\(m_{k}=1\) for \(L=24\)). For ConvNext, we first interpolate the feature maps from its 4 stages to \(96\times 96\) (\(m_{k}=4\) for \(L=24\)) and then channel-wise concatenate them to form its final vision feature map similar to [77].

\begin{table}

\end{table}
Table 21: **Comparison between SVA and other spatial-based connectors vision token compression.** The SVA module with spatial inductive bias more effectively compresses the vision information.

\begin{table}

\end{table}
Table 20: **Ablations on hyperparameter choices for SVA.** Enlarging the model capacity of the SVA module can further improve the performance.

[MISSING_PAGE_FAIL:36]

You will be given an 'answer' and a 'gt_answer' (ground truth answer),and you must reply with either CORRECT or INCORRECT based on the response. Tolerate a 0.05 relative error for numerical answers. answer: 25 gt_answer: 29 evaluation: INCORRECT  answer: Yes  gt_answer: Yes  evaluation: CORRECT  answer: 80  evaluation: CORRECT  answer: Ireland  gt_answer: Italy  evaluation: INCORRECT  answer: UK  gt_answer: UK  evaluation: CORRECT  answer: 2019  gt_answer: 2011  evaluation: INCORRECT  answer: {answer}  gt_answer: {gt_answer}  evaluation:

We conduct an ablation study on the benchmarks that require fuzzy matching and present the results in Table 24. We discover that fuzzy matching provides reliable results compared to an LLM grader. We recommend using a more capable model (such as GPT-4-turbo) for grading benchmarks that have more subjective responses (such as numbers and words).

\begin{table}
\begin{tabular}{|l|c|l|l|l|l|l|l|l|} \hline \multirow{2}{*}{Experiment} & \multirow{2}{*}{LLM} & \multicolumn{2}{c|}{Paychine} & \multicolumn{2}{c|}{Data} & \multicolumn{2}{c|}{Adjerer} & \multicolumn{2}{c|}{Instructions} & \multicolumn{2}{c|}{Tuning} \\  & LLM & Vision & & & & & & & \\ \hline
03 Adjapter+737K TT & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 0 & 737k & - & - & 2-5 & 0 & 512 & - \\
05 Adjapter+737K TT & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 0.5M & 737k & - & 1-3 & 0 & 512 & - 0 & 512 & - \\
1.2M Adjapter+737K TT & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 1.2M & 737k & - & 1-3 & 0 & 512 & - 0 & 512 & - \\ Difference Vision & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 1.2M & 737k & - & 1-3 & 0 & 512 & - 0 & 512 & - \\ Model Ensemble & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 1.2M & 737k & - & 1-3 & 0 & 512 & - 0 & 512 & - \\ Data Balance & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 0 & Mix Based on three \(t\) & - & - & - & 2-5 & 0 & 512 & - \\ Data Ratio & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 0 & 1350k Based on Ratio & - & - & - & 2-5 & 0 & 512 & - \\ LLVA 665K & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 0 & LLVA 665K & - & - & - & 2-5 & 0 & 512 & - \\ Cambrain-10M (Data) & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 0 & Cambrain-10M & - & - & - & 2-5 & 0 & 512 & - \\ Cambrain-7M (Data) & Vicuna-1.5-7B & OpenAI CLP VFT-L14@336 & 0 & Cambrain-7M & - & - & - & 2-5 & 0 & 512 & - \\ Cambrain-1.8B & Lima-3.1-88 & SVA with 4 encoders & 2.5M & Cambrain-7M & 1-e & 0 & 512 & - & - & - \\ Cambrain-1.13B & Vicuna-1.5-13B & SVA with 4 encoders & 2.5M & Cambrain-7M & 1-e & 0 & 512 & - 0 & 512 & - \\ Cambrain-1.34B & Hermes-2.7-34B & SVA with 4 encoders & 2.5M & Cambrain-7M & 1-e & 0 & 512 & - 0 & 1024 & - \\ \hline \end{tabular}
\end{table}
Table 23: **Implementation details and hyperparameters for all experiments.** “4 encoders are: OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, DINOv2 ViT-L/14@518, OpenCLIP ConvNeXt-XXL@1024

\begin{table}
\begin{tabular}{l|l|} \hline LLM & \multirow{2}{*}{System Prompt} \\ Backbone & System Prompt \\ \hline Vicuna 1.5 7B & A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. \\ \hline LLAMA-3 8B & You are Cambrian, a highly intelligent multimodal AI trained by NYU Vision X. As a multimodal AI, you have the ability to process and analyze images. Whenever an image is present in the conversation, very carefully examine it and consider its content when formulating your response. You should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions. \\ \hline Nous-Yi 34B & You are Cambrian, a highly intelligent multimodal AI trained by NYU Vision X. As a multimodal AI, you have the ability to process and analyze images. Whenever an image is present in the conversation, very carefully examine it and consider its content when formulating your response. You should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions. \\ \hline Nous-Yi 34B & You are Cambrian, a highly intelligent multimodal AI trained by NYU Vision X. As a multimodal AI, you have the ability to process and analyze images. Whenever an image is present in the conversation, very carefully examine it and consider its content when formulating your response. You should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions. \\ \hline \end{tabular}
\end{table}
Table 25: LLM Backbone System Prompts

\begin{table}
\begin{tabular}{p{56.9pt}|p{56.9pt}|p{142.3pt}} \hline Benchmark & Prompt & Example \\ \hline AI2D & unAnswer with the option’s letter from the given choices directly. & USER: \textless{}image\textgreater{}nWhich of these define dairy item(A) cu(B) Du(C) bn(D) auAnswer with the option’s letter from the given choices directly. ASSISTANT: \\ \hline ChartQA & unAnswer the question using a single number or phrase. & USER: \textless{}image\textgreater{}nHow many food item is shown in the bar graph?uAnswer the question using a single number or phrase. ASSISTANT: \\ \hline DocVQA & unGive the short answer directly. & USER: \textless{}image\textgreater{}nWhat is the dividend payout in 2012?uGive the short answer directly. ASSISTANT: \\ \hline GQA & unAnswer the question using single word or phrase. & USER: \textless{}image\textgreater{}nIs it overcast?uAnswer the question using single word or phrase. ASSISTANT: \\ \hline MathVista & unFirst show your reasoning process and then give the final answer. & USER: \textless{}image\textgreater{}nwhat is the total volume of the measuring cup?nFirst show your reasoning process and then give the final answer. ASSISTANT: \\ \hline MM-Bench EN & unAnswer with the option’s letter from the given choices directly. & USER: \textless{}image\textgreater{}nWhich of the following was a dependent variable in this experiment?n(A) cocoonu(B) chrysalisu(C) nanu(D) nanuAnswer with the option’s letter from the given choices directly. ASSISTANT: \\ \hline MME & unPlease answer the question using a single word or phrase. & USER: \textless{}image\textgreater{}nIs a python code shown in the picture? Please answer yes or no.uAnswer the question using a single word or phrase. ASSISTANT: \\ \hline MMMU & unAnswer with the option’s letter from the given choices directly. & USER: \textless{}image\textgreater{}nWhat causes these unusual formations on Mountain papaya? Options:nA. AbioticnB. ConfuseduC. BioticnD. NormaluAnswer with the option’s letter from the given choices directly. ASSISTANT: \\ \hline MMVP & unAnswer with the option’s letter from the given choices directly. & USER: \textless{}image\textgreater{}nAre the butterfly’s wings closer to being open or closed? Options:\textless{}n(a) Openu(b) CloseduAnswer with the option’s letter from the given choices directly. ASSISTANT: \\ \hline OCR Bench & unGive the short answer directly. & USER: \textless{}image\textgreater{}nwhat is written in the image?uGive the short answer directly. ASSISTANT: \\ \hline RealWorld QA & unAnswer the question using a single word or phrase. & USER: \textless{}image\textgreater{}nIn which direction is the front wheel of the car on the right side facing?unAn. LeftuB. StraightuC. RightAnnswer the question using a single word or phrase. ASSISTANT: \\ \hline SQA-I & unAnswer with the option’s letter from the given choices directly. & USER: \textless{}image\textgreater{}nWhat is the name of the colony shown?uAn. MarylandnB. New HampshireuC. Rhode IslandnD. VermontnAnswer with the option’s letter from the given choices directly. ASSISTANT: \\ \hline SEED-I & unAnswer with the option’s letter from the given choices directly. & USER: \textless{}image\textgreater{}nHow many towels are in the image? Options:\textless{}nA. OneuB. TwouC. ThreeuD. FournAnswer with the option’s letter from the given choices directly. ASSISTANT: \\ \hline Text-VQA & unAnswer the question using a single word or phrase. & USER: \textless{}image\textgreater{}nwhat is the time?uReference OCR tokens: N, u, g0nAnswer the question using a single word or phrase. ASSISTANT: \\ \hline ADE & unAnswer with the option’s letter from the given choices directly. & USER: \textless{}image\textgreater{}nConsidering the relative positions of the cushion and the sofa in the image provided, where is the cushion located with respect to the sofa? Select from the following choices. un(A) rightn(B) leftnAnswer with the option’s letter from the given choices directly. ASSISTANT: \\ \hline \end{tabular}
\end{table}
Table 26: Listing the prompts used in the evaluation of each benchmark

## Appendix J More results of Cambrian-1 Model

In Fig. 18, we also plot our Cambrian-1 performance as well as GPT-4 performances. In the plot, it is clear that Cambrian-1 offers competitive performance compared to proprietary models in most categories. We also showcase some examples in Fig. 19, demonstrating that the model effectively attends to details in images despite using only 576 tokens.

## Appendix K Potential Misuse & Mitigation Strategies

We recognize that there are ethical concerns regarding the potential misuse of multimodal large language models like Cambrian-1, particularly in generating misleading content or spreading misinformation. Below, we outline the main risks and provide strategies to address them:

1. **Misinformation** Cambrian-1 could be used to create misleading text descriptions of images, leading to false narratives or misrepresentations. For instance, such models might be leveraged by social media bots to manipulate public opinion during elections or other critical events.
2. **Hallucination** Similar to any large language model, Cambrian-1 may produce information that is not based on facts or actual input data. This phenomenon, often called "hallucination," can be dangerous if users assume the model's output is entirely accurate without verification.

To mitigate these risks, users should exercise caution and critical thinking when interpreting outputs generated by Cambrian-1. It is important to verify the information produced by the model, particularly if the results are intended for sensitive or high-stakes applications. Users must be aware of the potential for hallucinations, where the model produces information not grounded in facts, and take steps to cross-check and validate any critical outputs. Additionally, implementing content filtering as a safeguard can help flag potentially harmful or misleading content before it is disseminated.

Figure 18: **Comparison of model average performances on each category. Cambrian-1 outperforms other open-source models across all sizes. The lead is especially large on OCR & Chart and Vision-Centric benchmarks, highlighting the advantage of our vision-centric design.**Figure 19: **Examples of Cambrian-1-34B.** Cambrian-1 showcases impressive abilities in visual intersection. The model demonstrates instruction-following ability such as output in json format, as illustrated in the bottom-left example. Cambrian-1 also demonstrates remarkable OCR ability (See model handles different Comma \({}^{*}\),” in the right down example).

## Appendix L Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have detailed all claims mentioned in the abstract and introduction in the remaining part of the paper and Appendix. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] Justification: We have discussed the limitations of our work in the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: We do not include any theoretical assumptions and proofs in our paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We will publicly release all code, hyperparameters, model checkpoints, and datasets for reproducing our experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Yes, as highlighted in the abstract, we will release our model weights, code, tools, dataset, and all recipes. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes. We include all training and evaluation recipes in the main paper and Appendix, and will additionally include them in the code release. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We do report error bars and other statistical results for our experiments. Part of these results are included in Fig. 3, Fig. 4 and Fig. 6. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We will include these in our code release. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conform NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work has no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We used pre-trained LLM, which already has safeguards. As for datasets, we have conducted filtering to avoid safety risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have credited and mentioned the assets we used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have explained the data engine and new benchmark in our paper. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.