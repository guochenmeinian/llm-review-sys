Online RL in Linearly \(q^{\pi}\)-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore

 Gellert Weisz

Google DeepMind, London, UK

University College London, London, UK

&Andras Gyorgy

Google DeepMind, London, UK

Csaba Szepesvari

Google DeepMind, Montreal, Canada

University of Alberta, Edmonton, Canada

###### Abstract

We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear \(q^{\pi}\)-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly \(q^{\pi}\)-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly \(q^{\pi}\)-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an \(\varepsilon\)-optimal policy after \(\operatorname{polylog}(H,d)/\varepsilon^{2}\) interactions with the MDP, where \(H\) is the time horizon and \(d\) is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error.

## 1 Introduction

We consider reinforcement learning where an agent interacts in an online fashion with an environment modeled as a Markov decision process: The agent, observing a state, takes an action that results in a random next state and reward, the latter of which is to be maximized over time. To tackle large, possibly infinite state spaces, additional structure needs to be introduced to this problem. One such structure is a "feature-map" that maps state-action pairs to \(d\)-dimensional vectors (for some positive integer \(d\)) with the intention that a "good feature-map extracts important information from the state-action pairs so that learning with this extra information becomes tractable. An example is the case of _linear MDPs_(Jin et al., 2020), where the assumption is that both the transition and reward functions are linearly factorizable and their left factors are given by the feature-map. In contrast, value-based approaches, such as \(q^{\pi}\)-_realizability_(Du et al., 2019; Lattimore et al., 2020) aim to model only the action-values with the features. In this work, we focus on the latter, a strictly more general setting than that of linear MDPs (Zanette et al., 2020, Proposition 4).

There are several sample-efficient algorithms discovering near-optimal policies in linear MDPs under various MDP access models and settings (online access: Jin et al. (2020); batch setting: Jin et al.

[2021]; reward-free setting: Wagenmaker et al. [2022]). The best known sample-complexity bound for the online access model is achieved by the computationally inefficient algorithm of Zanette et al. [2020], called Eleanor, which serves as a starting point of our work.

In this work we consider the setting of linearly \(q^{\pi}\)-realizable MDPs. As opposed to linear MDPs, before this work, sample efficient solutions were only known for this case when the MDP is accessed through a simulator that implements some form of a state-reset function (Lattimore et al., 2020; Yin et al., 2022; Weisz et al., 2022) (Table 1). In this work we resolve an open problem by Du et al. [2019], and show that having access to a state-reset is not essential in this setting. To this end, we present SkippyEleanor (Algorithm 1) and a corresponding theorem (Theorem 4.1) that shows that SkippyEleanor, which uses online interactions only, is a provably sample-efficient solution to this problem. The rest of this paper is organized as follows. In Section 2 we introduce the basic definitions. In Section 3 we give an insight into the difference between linear \(q^{\pi}\)-realizability and linear MDPs, which motivates our approach. In Section 4 we describe our algorithm and the most important technical tools we discovered for its analysis. Notably, in Section 4.2 we establish a rich structure inherent in \(q^{\pi}\)-realizable MDPs, which acts as the technical foundation to this work, and may be of independent interest. Finally, Section 5 gives a summary of the proof of our main result (Theorem 4.1), before concluding with some notes on future work in Section 6.

## 2 Preliminaries

For a linear subspace \(X\) of \(\mathbb{R}^{d}\), let \(\operatorname{Proj}_{X}\) denote the orthogonal projection matrix onto \(X\). Throughout we fix \(d\in\mathbb{N}^{+}\). For \(L>0\), let \(\mathcal{B}(L)=\{x\in\mathbb{R}^{d}:\|x\|_{2}\leq L\}\) denote the \(d\)-dimensional Euclidean ball of radius \(L\) centered at the origin, where \(\|\cdot\|_{2}\) denotes the Euclidean norm. Let PD denote the set of positive definite matrices in \(\mathbb{R}^{d\times d}\). We write \(a\approx_{e}b\) for \(a,b,\varepsilon\in\mathbb{R}\) if \(|a-b|\leq\varepsilon\). Let \(\mathbb{I}\{B\}\) be the indicator function of a boolean-valued (possibly random) \(B\) taking value \(1\) if \(B\) is true and \(0\) if false. Let \(\mathcal{M}_{1}(X)\) denote the set of probability distributions supported on set \(X\). The rest of our notation is standard, but described in Appendix A for completeness.

For the setting of episodic finite horizon RL, with horizon \(H\), a finite-action Markov decision process (MDP) describes an environment for sequential decision-making. It is defined by a tuple \((\mathcal{S},[\mathcal{A}],P,\mathcal{R})\) as follows. The state space \(\mathcal{S}\) is split across stages: \(\mathcal{S}=(\mathcal{S}_{t})_{t\in[H]}\) with \(\mathcal{S}_{1}=\{s_{1}\}\) for some designated initial state \(s_{1}\). Without loss of generality, we assume the \((\mathcal{S}_{t})_{t\in[H]}\) are disjoint sets. We define the function stage \(:\mathcal{S}\rightarrow[H]\) as \(\operatorname{stage}(s)=t\) if \(s\in\mathcal{S}_{t}\). We consider finite action spaces of size \(\mathcal{A}\) for some \(\mathcal{A}\in\mathbb{N}^{+}\), and without loss of generality, define the set of actions to be \([\mathcal{A}]:=\{1,\ldots,\mathcal{A}\}\). The transition kernel is \(P:(\bigcup_{t\in[H-1]}\mathcal{S}_{t})\times[\mathcal{A}]\rightarrow\mathcal{ M}_{1}(\mathcal{S})\), with the property that transitions happen between successive stages, that is, for any \(t\in[H-1]\), state \(s_{t}\in\mathcal{S}_{t}\), and action \(a\in[\mathcal{A}]\), \(P(s_{t},a)\in\mathcal{M}_{1}(\mathcal{S}_{t+1})\). The reward kernel is \(\mathcal{R}:\mathcal{S}\times[\mathcal{A}]\rightarrow\mathcal{M}_{1}([0,1])\). An agent interacts sequentially with this environment in an episode lasting \(H\) steps by taking some action \(a\in[\mathcal{A}]\) in the current state. The environment responds by transitioning to some next-state according to \(P\), and giving a reward in \([0,1]\) according to \(\mathcal{R}\).1

Footnote 1: Here, the reward and next-state are independent, given the current state and last action. Independence is nonessential and is assumed only to simplify the presentation.

We describe an agent interacting with the MDP by a _policy_\(\pi\), which, to each history of interaction (including states, actions and rewards) assigns a probability distribution over the actions. Policies where this distribution only depend on the last state in the history are called _memoryless_, and these are identified with elements of the set \(\Pi=\{\pi:\mathcal{S}\rightarrow\mathcal{M}_{1}([\mathcal{A}])\}\). Using a policy \(\pi\), starting at some state \(s\) in an MDP induces a probability distribution over histories, which we denote by \(\mathcal{P}_{\pi,s}\). For

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & \multicolumn{2}{c|}{Online RL} & \multicolumn{2}{c|}{Planning with simulator} \\ MDP class & \(\operatorname{poly}(\cdot)\) sample & \(\operatorname{poly}(\cdot)\) compute & \(\operatorname{poly}(\cdot)\) sample & \(\operatorname{poly}(\cdot)\) compute \\ \hline Linear MDP & \multicolumn{2}{c|}{Jin et al. [2020]} \\ \hline \(q^{\pi}\)-realizable MDP & **This work** & Open problem & Yin et al. [2022] \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of efficiency results for linear MDPs and \(q^{\pi}\)-realizable MDPs under online RL and planning with a simulator. This work establishes that \(q^{\pi}\)-realizable MDPs are also sample efficiently solvable under online RL. The computational complexity of this problem remains open.

any \(a\in[\mathcal{A}]\), \(\mathcal{P}_{\pi,s,a}\) is the distribution over the histories when first action \(a\) is used in state \(s\), after which policy \(\pi\) is followed. \(\mathbb{E}_{\bullet}\) is the expectation operator corresponding to a distribution \(\mathcal{P}_{\bullet}\) (e.g., \(\mathbb{E}_{\pi,s}\) is the expectation with respect to \(\mathcal{P}_{\pi,s}\)). The state- and action-value functions \(v^{\pi}\) and \(q^{\pi}\) are defined as the expected total reward within the first episode while \(\pi\) is used:

\[v^{\pi}(s)=\operatorname*{\mathbb{E}}_{\pi,s}\sum_{u=\operatorname{stage}(s)} ^{H}R_{u}\quad\text{for }s\in\mathcal{S}\quad\text{ and }\quad q^{\pi}(s,a)=\operatorname*{\mathbb{E}}_{\pi,s,a}\sum_{u= \operatorname{stage}(s)}^{H}R_{u}\quad\text{ for }s\in\mathcal{S},\,a\in[\mathcal{A}].\]

Let \(\pi^{\star}\in\Pi\) be an optimal policy, satisfying \(q^{\pi^{\star}}(s,a)=\sup_{\pi\in\Pi}q^{\pi}(s,a)=\sup_{\pi\in\text{all policies}}q^{\pi}(s,a)\) for all \((s,a)\in\mathcal{S}\times[\mathcal{A}]\). Let \(q^{\star}(s,a)=q^{\pi^{\star}}(s,a)\) and \(v^{\star}(s)=\sup_{a^{\prime}\in[\mathcal{A}]}q^{\star}(s,a)\) for all \((s,a)\).

## 3 From linear \(q^{\pi}\)-realizability to linear MDPs

As described in the introduction, we endow our MDP with a feature map \(\varphi:\mathcal{S}\times[\mathcal{A}]\to\mathcal{B}(L_{1})\) for some \(L_{1}>0\). For reference, we start with a definition of linear MDPs with a parameter norm bound \(L_{2}>0\), formalizing that the transition kernel and the expected rewards are approximately linear functions of the features:2

Footnote 2: Compared to the definition of Jin et al. (2020), our definition does not require the existence of a vector-valued measure to represent the transition kernel. This is a generalization that is compatible with all existing algorithms for linear MDPs.

**Definition 3.1**.: _[\(\kappa\)-approximately linear MDP] For any \(\kappa\leq 1\), an MDP is a \(\kappa\)-approximately linear MDP if \(\mathrm{(i)}\) there exists \(\theta_{1},\dots,\theta_{H}\in\mathcal{B}(L_{2})\) such that for any \(h\in[H]\) and \((s,a)\in\mathcal{S}_{h}\times[\mathcal{A}]\), \(\left\lvert\mathbb{E}_{\mathcal{R}\cdot\mathcal{R}(s,a)}\,R-\langle\varphi(s, a),\theta_{h}\rangle\right\rvert\leq\kappa\) and \(\mathrm{(ii)}\) for any \(f:\mathcal{S}\to[0,H]\) and \(h\in[H-1]\), there exists \(\theta^{\prime}_{h}\in\mathcal{B}(L_{2})\) such that for all \((s,a)\in\mathcal{S}_{h}\times[\mathcal{A}]\), \(\left\lvert\mathbb{E}_{\mathcal{S}^{\prime}\sim P(s,a)}\,f(S^{\prime})-\left \langle\varphi(s,a),\theta^{\prime}_{h}\right\rangle\right\rvert\leq\kappa\)._

A key consequence of the linear MDP assumption is that the _inherent Bellman error_

\[\sup_{\theta_{h+1}\in\mathcal{B}(L_{2})}\inf_{\theta_{h}\in\mathcal{B}(L_{2})} \sup_{(s,a)\in\mathcal{S}_{h}\times[\mathcal{A}]}\bigg{\rvert}\operatorname*{ \mathbb{E}}_{\mathcal{R}\cdot\mathcal{R}(s,a),\mathcal{S}^{\prime}\sim P(s,a)} R(s,a)+\max_{a^{\prime}\in[\mathcal{A}]}\,\langle\varphi(S^{\prime},a^{\prime}), \theta_{h+1}\rangle-\langle\varphi(s,a),\theta_{h}\rangle\,\Big{\rvert}\,,\]

scales with the misspecification \(\kappa\). This property is also referred to as the _closedness to the Bellman operator_, and is a crucial component in the analysis of approximation errors for algorithms tackling linear MDPs.

In this work we consider a weaker linearity assumption where we only assume that the action-value functions are approximately linear:

**Definition 3.2** (\(q^{\pi}\)-realizability: uniform linear function approximation error of value-functions).: _Given an MDP, the uniform value-function approximation error (or misspecification) induced by a feature map \(\varphi:\mathcal{S}\times[\mathcal{A}]\to\mathcal{B}(L_{1})\), over a set of parameters in \(\mathcal{B}(L_{2})\) is_

\[\eta=\sup_{\pi\in\Pi}\max_{h\in[H]}\inf_{\theta^{(h)}\in\mathcal{B}(L_{2})} \sup_{(s,a)\in\mathcal{S}_{h}\times[\mathcal{A}]}\bigg{\rvert}q^{\pi}(s,a)- \left\langle\varphi(s,a),\theta^{(h)}\right\rangle\bigg{\rvert}\,\,.\]

_For the MDP and the corresponding feature map, for all \(h\in[H]\) fix any \(\theta_{h}:\Pi\to\mathcal{B}(L_{2})\) mapping each memoryless policy \(\pi\in\Pi\) to its "parameter", such that_

\[q^{\pi}(s,a)\approx_{\eta}\langle\varphi(s,a),\theta_{h}(\pi)\rangle\qquad \text{for all }\pi\in\Pi,\,s\in\mathcal{S}_{h},\,\text{and }a\in[\mathcal{A}]\,\,.\] (1)

_The set of all parameters \(\Theta_{h}\subseteq\mathcal{B}(L_{2})\) for a stage \(h\in[H]\) is given by \(\Theta_{h}=\{\theta_{h}(\pi)\;:\;\pi\in\Pi\}\,\)._

Note that \(\theta_{h}\) satisfying Eq. (1) always exist (Weisz et al., 2022, Appendix C). We focus on the feasible regime where \(\eta\) is polynomially small in the relevant parameters. Specifically, we assume that \(\eta\) is bounded according to Eq. (21). The main problem of interest in this work is the following:

**Problem 3.3** (informal).: _For any \(\varepsilon,\zeta>0\) and any MDP with corresponding uniform value-function approximation error \(\eta\), derive an algorithm that, with probability at least \(1-\zeta\), will find an \(\varepsilon\)-optimal policy (i.e., a policy \(\pi\) such that \(v^{\pi}(s_{1})\geq v^{\pi}(s_{1})-\varepsilon\)) by interacting with the MDP online for \(T\) steps with \(T\) bounded by a polynomial function of \((d,H,\varepsilon^{-1},\log\,\zeta^{-1},\log L_{1},\log L_{2})\). That the interaction with the MDP is online means that it is only possible to observe the features corresponding to the current state, and to take an action and subsequently observe the resulting reward and next state, which then becomes the current state. We consider the fixed horizon episodic setting, that is, the next state is reset to the initial state \(s_{1}\) after every \(H\) steps._Algorithms developed for linear MDPs are not directly applicable to Problem 3.3 when the MDP is only \(q^{\pi}\)-realizable: While a linear MDP is also \(q^{\pi}\)-realizable, a \(q^{\pi}\)-realizable MDP may be neither a linear MDP, nor one with a low inherent Bellman error (Zanette et al., 2020). As an illustrative example, Fig. 1, left shows an MDP that is \(q^{\pi}\)-realizable but not linear. To see this, observe that the features for both actions in \(s_{1}\) are identical, but their transitions and rewards are not. As illustrated in the figure however, if we _skip_ over the red states (with identical actions) by taking the first action on them and summing up the rewards received until we reach a black state, we arrive at a linear MDP. This serves as the main intuition behind our work: the red states have no bearing on action-values, so they can be skipped, and the resulting MDP is linear.

More generally, we can define the _range_ of any state as the maximum possible difference in action-value that the choice of action in that state can make:

\[\operatorname{range}(s)=\sup_{\theta\in\Theta_{\operatorname{step}(s)}}\max_{ i,j\in[\mathcal{A}]}\left\langle\varphi(s,i,j),\theta\right\rangle\text{ for all }h\in[H],s\in\mathcal{S}_{h}\,\] (2)

where \(\varphi(s,i,j)=\varphi(s,i)-\varphi(s,j)\) is the notation for feature differences. Clearly, the choice of action in low-range states is not too important, as

\[v^{\pi}(s)-q^{\pi}(s,a)\leq\operatorname{range}(s)+2\eta\qquad\text{for any }\pi\in\Pi\text{ and all }a\in[\mathcal{A}].\] (3)

Not only are the action choices in low-range states unimportant for the task of finding a near-optimal policy for the MDP, these choices can affect transitions and rewards in a nonlinear way. Interestingly, the existence of low-range states is the reason why \(q^{\pi}\)-realizable MDPs are not necessarily linear, as shown by the next result (proved in Appendix C), which follows easily from Lemma 4.7.

**Proposition 3.4**.: _Consider an MDP with uniform value-function approximation error \(\eta\geq 0\). If there are no states \(s\in\mathcal{S}\) with \(\operatorname{range}(s)<\alpha\) for some \(\alpha>0\), then the transitions and rewards of the MDP are linear (Definition 3.1) with misspecification scaling with \(\eta\), and parameter norms scaling inversely with \(\alpha\)._

Our approach.The above result immediately offers a strategy to learn under the (linear) \(q^{\pi}\)-realizability assumption. Assuming access to an oracle that can determine whether or not \(\operatorname{range}(s)<\alpha\) for any state \(s\), the MDP could be "converted" to one that has no low-range states but has near-identical state and action-value functions of any policy (compared to the original MDP), by skipping over low-range states (by executing an arbitrary action) until a state with a range at least \(\alpha\) is reached. We will call such a multi-state transition a _skippy step_ and refer to such a policy as a _skippy policy_. The reward presented for a skippy step is the cumulative reward over the skipped states. When the oracle is correct, the new MDP is a linear MDP, allowing techniques such as Eleanor to efficiently learn a near-optimal policy. This conversion argument is part of the intuition of our method, but it is not strictly part of the proof, so we defer the details to Appendix C. The only missing piece for solving the general case, Problem 3.3, is learning an oracle that can suggest when to skip over a state, and combining it with the learning algorithm for the linear MDP. This general approach leads to our algorithm, SkippyEleanor, which runs a modified version of Eleanor with guessed oracles. During the algorithm, we detect when an incorrect oracle leads to suboptimal results, and refine the oracle accordingly. The details of the algorithm are explained in the next section.

## 4 Algorithm

In this section we present our main results following our plan outlined above. We first give Algorithm 1, along with a high-level overview of the algorithm; the details are explained throughout the section. The parameters of the algorithm are presented in Appendix B.

Figure 1: **Left:** MDP with deterministic transitions and rewards (edges are labeled with action/reward). **Right:** The same MDP with the red “low-range” states “skipped” over. \(\varphi(s_{1},\cdot)=(1),\varphi(s_{3},\cdot)=(0.5),\varphi(\cdot,\cdot)=(0)\) otherwise. Both MDPs are \(q^{\pi}\)-realizable, but only the right MDP is linear.

For every stage \(h\in[H]\), the algorithm keeps a progressively refined estimate of the geometry of the parameter space \(\Theta_{h}\), by maintaining an ever shrinking ellipsoid enclosing \(\Theta_{h}\). This ellipsoid is parametrized by an 'inverse covariance matrix'-like quantity \(Q_{h}\), determined by \(\tilde{\mathcal{O}}(d)\) vectors, which guarantees \(\max_{\theta_{h}\in\Theta_{h}}\left\|\theta_{h}\right\|_{Q_{h}^{-2}}=\tilde{ \mathcal{O}}(\sqrt{d})\). Looking at the definition of range in Eq. (2), it is clear that the smaller the ellipsoid becomes, the better estimate we can give for the ranges.

Given some data collected so far and \((Q_{h})_{h\in[H]}\), SkippyEleanor computes optimistic estimates of the action-values by calculating an optimistic policy parameter \(\bar{\theta}\), as well as a guess \(\hat{G}\) to a near-optimal design which is used to estimate the range for the states (due to technical reasons, \(\hat{G}\) will guess a near-optimal design for the transformed parameter space \(Q_{h}^{-1}\Theta_{h}\)).

Data is collected by running stochastic versions of skipped policies on the MDP, where the states to be skipped over are determined based on the range estimates; when a state is skipped, an action is selected using a deterministic policy \(\pi^{0}\) that always chooses the first action in every state. To ensure that the estimation problem is smooth in terms of \(\hat{G}\), we use a smoothed version of skipped policies, where states are skipped randomly, and the probability of skipping is larger for states with lower ranges, while high-range states are never skipped. Similarly to Eleanor, we aim to estimate the action-value function of a state-action pair by adding the estimated one-step reward to the estimated value-function of the next state. However, unlike Eleanor, we would like to do this in the reduced MDP, where the low-range states that are skipped over are removed (and the corresponding transitions are replaced by skipped steps). Since we do not know these states in advance, we run exploratory policies that skip over next states starting from any state: namely,we run SkippyPolicy\((\hat{G},\bar{\theta},k)\) for all \(k\in[H]\) with a maximum number of unskipped states \(k\) (Phase I), and once this is skip budget is exhausted, all remaining states are skipped over by rolling out \(\pi^{0}\) (Phase II), which ensures that we collect enough data at every stage of the MDP to be able to estimate the one-skippy-step reward of any skipping mechanism. Compared to Eleanor, this introduces an additional loop in Line 6 of SkippyEleanor; see Appendix D for additional details. For any execution, SkippyPolicy maintains a stage-mapping function \(p\), which, for any stage \(h\) of the trajectory in the reduced MDP gives the stage index in the original MDP. In other words, \(p(j)\) is the stage of the landing state of the \(j^{th}\) skippy step.

Finally, we check if the data collected is consistent with our estimates \(\hat{G}\) and \(\bar{\theta}\), by calculating the maximal discrepancy of the estimates of the action-value difference at the last non-skipped state of \(\pi^{mk}=\textsc{SkippyPolicy}(\hat{G},\bar{\theta},k)\) and that of the fixed skipping policy \(\pi^{0}\) in different directions in the parameter space. If the discrepancy is too large for any \(k\), we add the discrepancy-maximizing direction to \(Q\) and throw away the data collected in this (i.e., the \(m^{th}\)) iteration; this is achieved by reducing the iteration counter \(m\) by \(1\). On the other hand, if the discrepancy is small enough, we can guarantee that the gap between the value of \(\pi^{mH}\) and \(v^{\star}(s_{1})\) scales with how much new information we collected, thus the algorithm can terminate returning this policy if this term is sufficiently small (which it eventually has to be).

The following theorem shows that with high probability, SkippyEleanor finds a near-optimal policy after polynomially many interactions with the MDP. Rhe proof sketch is provided in Section 5, while our method and proof strategy is explained from the perspective of Eleanor in Appendix D.

**Theorem 4.1**.: _With probability at least \(1-\zeta\), SkippyEleanor interacts with the MDP for at most \(\tilde{\mathcal{O}}\left(H^{11}d^{7}/e^{2}\right)\) many steps, before returning a policy \(\pi\) that satisfies \(v^{\star}(s_{1})\leq v^{\pi}(s_{1})+\varepsilon\)._

### Preconditioning: the enclosing ellipsoid

In this section we give the technical details about the effects of using the matrix \(Q_{h}\) describing an enclosing ellipsoid for \(\Theta_{h}\) (see Lemma 4.3) as preconditioning the features.

**Definition 4.2** (Valid preconditioning).: \(Q=(Q_{h})_{h\in[H]}\) _is a valid preconditioning matrix sequence if for all \(h\in[H]\)_

\[Q_{h}=\left(L_{2}^{-2}I+\sum_{v\in C_{h}}vv^{\top}\right)^{-1/2}\] (4)

_for some sequence \(C_{h}=(v_{1},\ldots,v_{n})\) of vectors in \(\mathbb{R}^{d}\) such that for all \(1\leq i\leq n\),_

\[\sup_{\theta\in\Theta_{h}}|\langle\theta,v_{i}\rangle|\leq 1\quad\text{and} \quad\left\|\left(L_{2}^{-2}I+\sum_{j=1}^{i-1}v_{j}v_{j}^{\top}\right)^{-\frac {1}{2}}v_{i}\right\|_{2}^{2}\geq\tfrac{1}{2}\quad\text{and}\quad\|v\|_{2}\leq L _{3}\,,\] (5)

_where \(L_{3}\) is some fixed polynomial of the problem parameters \((d,H,\varepsilon^{-1},\log\zeta^{-1},\log L_{1},\log L_{2})\). (see Eq. (35) for its precise value)._

_For a valid preconditioning \(Q\) and some \(h\in[H]\), let \(Z(Q,h)\) be the linear subspace spanned by those eigenvectors of \(Q\) whose corresponding eigenvalues are at least \(L_{3}^{-2}\). Let \(\operatorname{Proj}_{Z(Q,h)}\) be the orthogonal projection matrix onto this subspace._

Sometimes it will be convenient to _precondition_ the features and parameters so that the enclosing ellipsoid is transformed to a ball of controlled radius (as Lemma 4.3 will show). To this end, introduce for all \(h\in[H]\) and \((s,a,b)\in\mathcal{S}_{h}\times[\mathcal{A}]\times[\mathcal{A}]\) the following:3

Footnote 3: Note that \(Q_{h},h\in[H]\) is invertible by construction.

\[\varphi_{Q}(s,a)=Q_{h}\varphi(s,a),\qquad\varphi_{Q}(s,a,b)=Q_{h} \varphi(s,a,b)\] (6) \[\theta_{h}^{Q}(\pi)=Q_{h}^{-1}\theta_{h}(\pi),\qquad\Theta_{h}^{Q }=\left\{\theta_{h}^{Q}(\pi)\;:\;\pi\in\Pi\right\}=\left\{Q_{h}^{-1}\theta\;: \;\theta\in\Theta_{h}\right\}\] \[\hat{q}^{\pi}(s,a)=\langle\varphi(s,a),\theta_{h}(\pi)\rangle= \left\langle\varphi_{Q}(s,a),\theta_{h}^{Q}(\pi)\right\rangle\text{ for all }\pi\in\Pi\,.\]

The next lemma (proved in Appendix F) shows that for all \(h\in[H]\), \(Q_{h}\) defines an enclosing ellipsoid for \(\Theta_{h}\); that is, \(\Theta_{h}\subset\{\theta:\|\theta\|_{Q_{h}^{-2}}\leq\sqrt{d_{1}+1}\}\).

**Lemma 4.3**.: _Let \(d_{1}=4d\log(1+6L_{3}^{4}L_{2}^{4})=\tilde{O}(d)\). Then, for any valid preconditioning \(Q\) and \(h\in[H]\),_

\[\sup_{\theta\in\Theta_{h}}\|\theta\|_{Q_{h}^{-2}}=\sup_{\theta\in\Theta_{h}^{0}} \|\theta\|_{2}\leq\sqrt{d_{1}+1}\,.\]

Clearly, every time a new vector is added to \(C_{h}\), the enclosing ellipsoid \(\{\theta:\|\theta\|_{Q_{h}^{-2}}\leq\sqrt{d_{1}+1}\}\) shrinks (as a positive semidefinite matrix is added to \(Q_{h}^{-2}\)). The following lemma (also proved in Appendix F) uses an elliptical potential argument to bound the number of times this can happen.

**Lemma 4.4**.: _For any valid preconditioning \(Q\), for all \(h\in[H]\), the length of sequence \(C_{h}\) corresponding to \(Q_{h}\) according to Definition 4.2 is at most \(d_{1}\)._

Near-optimal design for \(\Theta_{h}^{Q}\).As \(Q_{h}\) only provides an enclosing ellipsoid for \(\Theta_{h}\), we introduce an (unknown) ellipsoid that aligns better with \(\Theta_{h}^{Q}\). For all \(h\in[H]\), fix a set \(G_{h}^{Q}\) of policies of size \(d_{0}:=4d\log\log(d)+16\), together with a probability distribution \(\rho_{h}^{Q}\) on \(G_{h}^{Q}\), such that \((G_{h}^{Q},\rho_{h}^{Q})\) is a near-optimal design for \(\Theta_{h}^{Q}\) (i.e., satisfying Definition F.1). The existence of such a near-optimal design follows from [13, Part (ii) of Lemma 3.9].

We apply \(G_{h}^{Q}\) to define a cruder version of range that depends only on a small set of policies, and can therefore be succinctly parametrized to inform SkippyPolicy:

\[\operatorname{range}_{Q}(s)=\max_{\pi\in G_{h}^{Q}}\max_{i,j\in[\mathcal{A}]} \left\langle\varphi(s,i,j),\theta_{h}(\pi)\right\rangle\qquad\text{for all }h\in[H],s\in\mathcal{S}_{h}\,.\] (7)

\(\operatorname{range}_{Q}\) is easy to estimate, and can be used to bound the range function (proved in Appendix F):

**Proposition 4.5**.: _For all \(s\in\mathcal{S}\) and \(Q\in\text{PD}^{H}\), \(\operatorname{range}(s)\leq\sqrt{2d}\operatorname{range}_{Q}(s)\)._

### Linearly realizable functions

\(q^{\pi}\)-realizability (Definition 3.2) implies the linearity of many more functions than the action-value functions. In this section we characterize an interesting set of such functions, whose (approximate) linearity plays a crucial role in our algorithm and analysis, as their parameters can be conveniently estimated by least squares using the features. We rely on functions \(f:\mathcal{S}_{h}\to\mathbb{R}\) (for some \(h\in[H]\)) being small for all states, relative to the states' \(\operatorname{range}_{Q}\)-value:

**Definition 4.6**.: _For any \(h\in[H]\), \(f:\mathcal{S}_{h}\to\mathbb{R}\) is \(\alpha\)-admissible for some \(\alpha>0\) if for all \(s\in\mathcal{S}_{h}\), \(|f(s)|\leq\operatorname{range}_{Q}(s)/\alpha\)._

The key observation is that expected (admissible) \(f\) values are linearly realizable.

**Lemma 4.7** (Admissible-realizability).: _If \(f:\mathcal{S}_{h}\to\mathbb{R}\) is \(\alpha\)-admissible then it is realizable, that is, for all \(t\in[h-1]\) and \(\pi\in\Pi\), there exists some \(\tilde{\theta}\in\mathbb{R}^{d}\) with \(\left\|\tilde{\theta}\right\|_{2}\leq 4d_{0}L_{2}/\alpha\) such that for all \((s,a)\in\mathcal{S}_{t}\times[\mathcal{A}]\),_

\[\operatorname*{\mathbb{E}}_{\pi,s,a}f(S_{h})\approx_{\eta_{0}}\left(\varphi( s,a),\tilde{\theta}\right)\qquad\qquad\text{where }\eta_{0}=5d_{0}\eta/\alpha.\]

The proof relies on constructing a set of policies that at states \(s\in\mathcal{S}_{h}\) take a higher value action as opposed to a lower one with a certain probability, configured such that the expected action-value difference of some pairs within the set of policies is (approximately) proportional to \(f(s)\). Thus, a linear combination of the action-values of policies in this set are also (approximately) proportional to \(f(s)\). The statement of the lemma then follows from setting \(\tilde{\theta}\) to the corresponding linear combination of the policies' parameters. The full proof is presented in Appendix G.

Next, we define matrix-valued functions with a special admissibility guarantee even when the underlying scalar-valued function does not satisfy any non-trivial admissibility criterion. We introduce a _guess_ on the near-optimal design parameters that define \(\operatorname{range}_{Q}\) (Eq. (7)) for some valid preconditioning \(Q\):

**Definition 4.8**.: _For \(h\in[2:H]\), fix some arbitrary order of the policies in the set \(G_{h}^{Q}\) (recall that this set is the support of the near-optimal design for \(\Theta_{h}^{Q}\)). Let the parameter of the \(i^{th}\) policy in \(G_{h}^{Q}\) be \(\theta_{h}^{i}\) for \(i\in[d_{0}]\). Call a "guess" of these parameters \(\hat{G}=(\hat{G}_{h})_{h\in[2:H]}=(\hat{\theta}_{h}^{i})_{h\in[2:H],i\in[d_{0}]}\) "valid", if for all \(h\in[2:H],i\in[d_{0}]\), \(\hat{\theta}_{h}^{i}\in\mathcal{B}(\sqrt{d_{1}+1})\). Let the set of valid guesses be \(\mathbf{G}\).4 By Lemma 4.3, \((\vartheta_{h}^{i})_{h\in[2:H],i\in[d_{0}]}\in\mathbf{G}\), that is, it is a valid guess, and we call this the "correct" guess._From a guess \(\hat{G}=(\hat{\theta}_{h}^{i})_{h\in[2:H],i\in[d_{0}]}\) we can calculate corresponding guesses of the \(\operatorname{range}_{Q}\)-values:

\[\operatorname{range}_{Q}^{\hat{G}}(s)=\max_{k\in[d_{0}]}\max_{i,j\in[\mathcal{ N}]}\left\langle\varphi_{Q}(s,i,j),\hat{\theta}_{\operatorname{stage}(s)}^{k} \right\rangle\qquad\text{for all }h\in[2:H],s\in\mathcal{S}_{h}\;.\]

Note that for any \(h\in[2:H]\) and \(s\in\mathcal{S}_{h}\), \(\operatorname{range}_{Q}^{\hat{G}}(s)=\operatorname{range}_{Q}(s)\) if \(\hat{G}\) is the correct guess for stage \(h\).

Let \(\bar{\varphi}_{Q}(s)\) be the unit vector in the direction of the largest feature difference between actions in \(s\) and the zero vector if all feature vectors are the same (see Eq. (27) for a formal definition). Then, for any \(\hat{G}\in\mathbf{G}\), \(h\in[2:H]\), and \(f:\mathcal{S}_{h}\to[-H,H]\), let

\[\mathbf{f}(s)=\bar{\varphi}_{Q}(s)\bar{\varphi}_{Q}(s)^{\top}\min\left\{1, \operatorname{range}_{Q}^{\hat{G}}(s)\frac{\sqrt{2d}H}{\varepsilon}\right\} f(s)\quad\text{for }s\in\mathcal{S}_{h}\;.\]

For such \(\mathbf{f}:\mathcal{S}_{h}\to\mathbb{R}^{d\times d}\), we adopt the notation \(a^{\top}\mathbf{f}b\) for any \(a,b\in\mathbb{R}^{d}\) to denote the function \(s\in\mathcal{S}_{h}\mapsto a^{\top}\mathbf{f}(s)b\), and similarly, \(\operatorname{Tr}(\mathbf{f})\) to denote the function \(s\in\mathcal{S}_{h}\mapsto\operatorname{Tr}(\mathbf{f}(s))\).

Let \(\operatorname{Proj}_{\|(Q,h)}\) be the projection matrix onto the linear subspace spanned by those eigenvectors of the design matrix \(V(G_{h}^{Q},\rho_{h}^{Q})\) (defined in Eq. (25)) whose corresponding eigenvalues are at least \(\gamma\) (for some \(\gamma>0\) specified in Appendix B). Intuitively, this is the subspace where \(\Theta_{h}^{Q}\) has a sufficiently large width. Let \(\operatorname{Proj}_{\perp(Q,h)}\) be the projection to the orthogonal complement subspace. For any \(v\in\mathbb{R}^{d}\), we write \(v_{\|(Q,h)}\) and \(v_{\perp(Q,h)}\) for \(\operatorname{Proj}_{\|(Q,h)}v\) and \(\operatorname{Proj}_{\perp(Q,h)}v\), respectively.

We are now ready to state our special admissibility guarantee, which is proved in Appendix G. Let \(\alpha=\tilde{\mathcal{O}}(\varepsilon/(d^{1.5}H^{2}))\) be as in Eq. (16).

**Lemma 4.9**.: _For any \(h\in[2:H]\), \(\hat{G}\in\mathbf{G}\), any function \(\mathbf{f}\) constructed as above from some \(f:\mathcal{S}_{h}\to[-H,H]\), and any \(v,w\in\mathcal{B}(1)\), \(v_{\|(Q,h)}^{\top}\)fw is \(\alpha\)-admissible. Furthermore, if \(\hat{G}=(\vartheta_{h}^{i})_{h\in[2:H],i\in[d_{0}]}\) (the correct guess), \(\operatorname{Tr}(\mathbf{f})\) is also \(\alpha\)-admissible._

### Least-squares targets and Optimization Problem 4.10

Recall that SkippyEleanor estimates action-values of states by first adding the estimated one-step reward and the estimated value-function of the next state in the reduced MDP (where low-range states are skipped). Due to the linearity of \(q^{\pi}\)-values, these can be used as target variables of a least-squares estimator to estimate the policy parameters. This estimator is only guaranteed to be accurate if the right (low-range) states are skipped; otherwise, we will argue in Section 4.4 that a discrepancy is detected and it is handled by changing the preconditioning \(Q\). Finally, to ensure optimism, we select parameter estimates that lead to the largest estimated policy values. The whole estimation process leads to Optimization Problem 4.10, which we define in this section along with the functions that it uses as least-square targets. Each estimation is for a particular stage \(h\) and may use the estimates \(\bar{\theta}_{i}\) of Optimization Problem 4.10 for stages \(i>h\). In this subsection, we consider the \(m^{\text{th}}\) iteration of the optimization called by SkippyEleanor, and consider \(Q\) fixed. As a shorthand, we introduce the following notation for \(l\in[m],j\in[n],k\in[H]\):

\[\operatorname{p}(lkj)=p^{1kj}(k)\qquad\text{as recorded in Line \ref{p} of Algorithm \ref{p}, and}\] \[S_{\operatorname{p}(k)}^{lkj}=S_{p^{1kj}(k)}^{lkj},\ A_{ \operatorname{p}(k)}^{lkj}=A_{p^{1kj}(k)}^{lkj},\ R_{\operatorname{p}(k)}^{ lkj}=R_{p^{1kj}(k)}^{lkj},\ \varphi_{t}^{lkj}=\varphi(S_{t}^{lkj},A_{t}^{lkj}),\ \varphi_{ \operatorname{p}(k)}^{lkj}=\varphi(S_{p(k)}^{lkj},A_{\operatorname{p}(k)}^{lkj} )\;.\]

We collect the set of \((l,k,j)\) tuples for which the \(k^{\text{th}}\) skippy step lands at stage \(t\), for \(t\in[H]\), as

\[\mathbf{I}^{m}(t)=\{(l,k,j):\,l\in[m-1],j\in[n],k\in[H],\operatorname{p}(lkj)=t\}\]

Note in particular that here \(l\in[m-1]\), so \(\mathbf{I}^{m}\) only considers data collected prior to iteration \(m\).

To estimate the parameters \(\hat{G}\) and \(\bar{\theta}\), we consider (simulated) trajectories of SkippyPolicy starting from stage \(t\). For simplicity, we suppress the dependence of quantities on \(\hat{G}\) and \(\bar{\theta}\), which will be brought back later. The skipping probability \(1-\tau\), the policy \(\pi^{+}\) (to be also used in SkippyPolicy), and corresponding clipped action-value estimates are defined as

\[\tau(s) =\min\left\{1,\operatorname{range}_{Q}^{\hat{G}}(s)\frac{\sqrt{2d}H}{ \varepsilon}\right\} \qquad\text{if }\operatorname{stage}(s)>1,\,\text{and}\,\,\tau(s_{1})=1;\] (8) \[\pi^{+}(s_{i}) =\operatorname*{arg\,max}_{a\in[\mathcal{A}]}\left\langle\varphi(s_ {i},a),\bar{\theta}_{i}\right\rangle, C(s_{i})=\operatorname{clip}_{[0,H]}\left\langle\varphi(s_{i},\pi^{+}(s_{i})), \bar{\theta}_{i}\right\rangle.\]

[MISSING_PAGE_FAIL:9]

this by another indicator \(c^{j}_{k\,i}\) (defined in Appendix B) that requires the data-point's least-squares uncertainty term to be sufficiently low, and the prediction non-negative (the contribution of the rest of the data will be analyzed separately). Next, we define the least-squares solution for estimating the matrix-valued \(F\), as well as the empirical average prediction and realization of \(F\) on the data collected in the \(m^{\text{th}}\) round. For any \(i\in[2:H]\), \(k\in[i-1]\) (recall that \(\otimes\) denotes the tensor product):

\[\begin{split}\hat{\theta}^{\prime i}_{G\,\hat{\theta}}& =X_{mt}^{-1}\sum_{lkj\in\mathbf{P}^{m}(t)}\varphi^{klj}_{k\,\hat{ \theta}}\otimes F_{\hat{G}\,\hat{\theta}}(S^{lkj}_{i},\ldots,R^{lkj}_{H}) \qquad\text{for $t\in[i-1]$}\\ & y^{ki}_{G\,\hat{\theta}}&=\frac{1}{n}\sum_{j\in[n ]}c^{j}_{ki}\varphi^{mkj}_{p(k)}{}^{\prime}\hat{\theta}^{p(mkj),i}_{\hat{G} \,\hat{\theta}}\qquad\qquad\hat{F}^{ki}_{\hat{G}\,\hat{\theta}}=\frac{1}{n} \sum_{j\in[n]}c^{j}_{ki}F_{\hat{G}\,\hat{\theta}}(S^{mkj}_{i},\ldots,R^{mkj}_ {H})\end{split}\] (12)

In Appendix E.1, it is established via the usual least-squares analysis techniques and covering arguments, that with high probability the norm of the product of the matrix \(y^{ki}_{G\,\hat{\theta}}-\hat{F}^{ki}_{\hat{G}\,\hat{\theta}}\) and the projection matrix \(\text{Proj}_{\|\{Q,i\}}\) is small (Lemmas E.2 and E.3). The next optimization problem tests if this is true in arbitrary directions:

**Optimization Problem 4.12** (Consistency check).: _Input: \((\hat{G},\tilde{\theta})\)_

\[\operatorname*{arg\,max}_{k\in[H-1],\,i\in[k+1:H],\,v\in\mathbb{R}^{d}:\|v\|_ {2}=1}v^{\top}\left(y^{ki}_{\hat{G}\,\tilde{\theta}}-\hat{F}^{ki}_{\hat{G}\, \tilde{\theta}}\right)v\]

Lemma E.1 shows that the projection \(w=\text{Proj}_{Z(Q,t)}\,v\) is close to \(v\), where \(v\) is the outcome of Optimization Problem 4.12. Also, Lemmas E.1-E.3 imply that if the consistency check fails (i.e., Line 13 is executed because the value of Optimization Problem 4.12 is large), then \(w\) aligns well with the subspace \(\text{Proj}_{\perp(Q,i)}\) projects to, and therefore \(Q\) stays a valid preconditioning after appending \(w\) to the list of values \(Q\) is calculated from (Lemma E.4). Thus, \(Q\) is always a valid preconditioning.

## 5 Proof overview

The proof of Theorem 4.1 is presented in Appendix E. It is composed of the following main steps: First, we bound the number of times the consistency check can fail (i.e., Line 13 is executed) by Lemma 4.4. Combining this with Lemma E.5, an elliptical potential argument bounding the number of times the average uncertainty can be large (these are the only two ways that the main iteration can continue) implies a sample-complexity result for Skipyeleanor (Corollary E.6). Having limited the number of times the consistency check can fail, we derive guarantees regarding the performance of the policy returned by the algorithm: Via an induction argument (Lemma E.8) we show Corollary E.9, which shows that with high probability the difference between the optimization value of Optimization Problem 4.10, \(C_{\hat{G},\,\hat{\theta}}(s_{1})\) and \(v^{\pi^{mH}}\) scales with the average uncertainty term \(\sum_{i=1}^{H}\tilde{\sigma}_{k}^{m}\). Thus, they are close when Skipyeleanor returns in Line 17. This is complemented with the _optimism_ property proved in Lemma E.10, stating that the optimization value \(C_{\hat{G},\,\hat{\theta}}(s_{1})\) is close to \(v^{\star}(s_{1})\). Combined, this proves Theorem 4.1.

## 6 Future work

Since we are not aware of a computationally efficient implementation of SkippyEleanor, it remains an open question whether the problem of learning near-optimal policies from online interactions with a \(q^{\pi}\)-realizable MDP (Problem 3.3) is possible if the computational resources as well as the sample complexity are bounded by a polynomial in the relevant parameters. One approach is to replace Eleanor with LSVI-UCB as the underlying algorithm, as the latter, despite having worse sample complexity, has a computationally efficient implementation (Jin et al., 2020). The challenge is to compute the optimal solution for the parameter \(\hat{G}\) in Optimization Problem 4.10. This parameter interacts with the least-squares targets in a highly nonlinear way. We have been unable to derive a computationally efficient approximation that has an additive instead of a multiplicative approximation error (additive errors increase linearly in \(H\), while multiplicative errors increase exponentially). Alternatively, it may be possible to show a computational hardness result for Problem 3.3 by e.g., reducing it to the satisfiability problem. These are left for future work. Our work on the realizability of auxiliary functions (Section 4.2) may be of independent interest for designing provably efficient algorithms for related problem settings, e.g., the setting of \(q^{\pi}\)-realizability in batch RL, where the data collection is not controlled.

## References

* Du et al. (2019) Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient for sample efficient reinforcement learning? In _International Conference on Learning Representations_, 2019.
* Jin et al. (2020a) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143, 2020a.
* Jin et al. (2020b) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020b.
* Jin et al. (2021) Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021.
* Lattimore and Szepesvari (2020) T. Lattimore and Cs. Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* Lattimore et al. (2020) Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits and in RL with a generative model. In _ICML_, pages 9464-9472, 2020.
* Todd (2016) Michael J Todd. _Minimum-volume ellipsoids: Theory and algorithms_. SIAM, 2016.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wagenmaker et al. (2022) Andrew Wagenmaker, Yifang Chen, Max Simchowitz, Simon S Du, and Kevin Jamieson. Reward-free RL is no harder than reward-aware RL in linear Markov decision processes. _arXiv preprint arXiv:2201.11206_, 2022.
* Weisz et al. (2022) Gellert Weisz, Andras Gyorgy, Tadashi Kozuno, and Csaba Szepesvari. Confident approximate policy iteration for efficient local planning in \(q^{\pi}\)-realizable MDPs. In _Advances in Neural Information Processing Systems_, 2022.
* Yin et al. (2022) Dong Yin, Botao Hao, Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesvari. Efficient local planning with linear function approximation. In _International Conference on Algorithmic Learning Theory_, pages 1165-1192. PMLR, 2022.
* Zanette et al. (2020) Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent Bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.

Notation

As usual, we use \(\mathbb{R}\), \(\mathbb{N}\), and \(\mathbb{N}^{+}\) to denote the set of reals, non-negative and positive integers, respectively. For \(i\in\mathbb{N}^{+}\), let \([i]=\{1,\ldots,i\}\); for another positive integer \(j\), let \([i:j]=\{i,\ldots,j\}\) if \(i\leq j\), and \([i:j]=\{\}\) otherwise. For \(a,b,x\in\mathbb{R}\), let \(\text{clip}_{[a,b]}(x)=\min\{\max\{x,a\},b\}\) and let \(\lceil x\rceil\) denote the smallest integer \(\mathrm{i}\) such that \(i\geq x\). Let \(\mathbf{0}\) be the all-\(0\) vector in \(\mathbb{R}^{d}\) and \(I\) the \(d\)-dimensional identity matrix. For a (square) matrix \(V\), let \(V^{\dagger}\) denote its Moore-Penrose inverse, and \(\text{Tr}(V)\) denote its trace. Let PD (and PSD) denote the set of positive definite (and positive semi-definite, respectively) matrices in \(\mathbb{R}^{d\times d}\). For some \(A\in\text{PSD let }A^{\frac{1}{2}}\) denote the unique matrix \(B\in\text{PSD}\) such that \(A=BB\). For \(V\in\text{PD}\) and \(x\in\mathbb{R}^{d}\), let \(\|x\|_{V}^{2}=x^{\top}Gx\). For matrices \(A\) and \(B\), we say that \(A\succeq B\) (or \(A\preceq B\)) if \(B-A\) (or \(A-B\), respectively) is positive semidefinite. \(\text{Ker}(A)\) and \(\text{Im}(A)\) are the kernel (or null space), and image, respectively, of matrix \(A\). For compatible vectors \(x\), \(y\), let \(\langle x,y\rangle\) be their inner product: \(\langle x,y\rangle=x^{\top}y\). We write \(y\otimes A\) for the tensor product between \(y\) and matrix \(A\), and then \(\langle x,y\otimes A\rangle=\langle x,y\rangle A\). Where \(Q\) and \(h\) are obvious from the context, we write \(v_{\parallel}\) and \(v_{\perp}\) for \(v_{\parallel(Q,h)}\) and \(v_{\perp(Q,h)}\), respectively. Throughout the paper, we omit commas between quantities in subscripts or superscript for clarity of presentation, for example, by writing \(A_{bc}\) for \(A_{b,c}\).

For the big-Oh notation \(\mathcal{O}\), we introduce its counterpart \(\tilde{\mathcal{O}}\) that hides logarithmic factors of the problem parameters \((d,H,\varepsilon^{-1},\zeta^{-1},L_{1},L_{2})\).

## Appendix B Parameters of Algorithm 1

\[n =\tilde{\mathcal{O}}\left(d^{5}H^{6}/\varepsilon^{2}\right) \text{(for precise value see Eq.~{}\eqref{eq:m_max})}\] \[\omega =7(d_{1}+1)+7/3=\tilde{\mathcal{O}}(d)\] (13) \[\gamma^{-1} =8d=\tilde{\mathcal{O}}(d)\] (14) \[\beta =\tilde{\mathcal{O}}(H^{1.5}d) \text{(for precise value see Eq.~{}\eqref{eq:m_max})}\] (15) \[\alpha^{-1} =\frac{\sqrt{2d}\sqrt{d_{1}+1}H^{2}}{\sqrt{\gamma}e}=\tilde{ \mathcal{O}}(d^{1.5}H^{2}/\varepsilon)\] (16) \[\lambda^{-1} =(4d_{0}L_{2}/\alpha)^{2}\] (17) \[m_{\max} =\beta^{2}\log\left(1+\frac{HmnL_{1}^{2}}{d\lambda}\right)+1= \tilde{\mathcal{O}}\left(H^{3}d^{2}\right)\] \[m_{\max}^{\prime} =m_{\max}+Hd_{1}=\tilde{\mathcal{O}}(H^{3}d^{2})\] \[\tilde{\sigma}_{k}^{m} =\frac{1}{n}\sum_{j\in[n]}\tilde{c}_{k,H+1}^{j}\min\left\{2(\beta \omega dH)^{-1},\left\|\varphi_{\mathrm{p}(k)}^{mkj}\right\|_{X_{m,\mathrm{p}( mk)}^{-1}}\right\}\] (18) \[c_{ki}^{j} =\mathbbm{1}\left\{\mathrm{p}(mkj)<i\text{ and }\left\| \varphi_{\mathrm{p}(k)}^{mkj}\right\|_{X_{m,\mathrm{p}(mkj)}^{-1}}<2(\beta \omega dH)^{-1}\text{ and }\left\langle\varphi_{\mathrm{p}(k)}^{mkj},\bar{\theta}_{ \mathrm{p}(mkj)}\right\rangle\geq 0\right\}\] (20) \[\text{average\_uncertainty} =\sum_{k=1}^{H}\tilde{\sigma}_{k}^{m}\] \[\text{uncertainty\_threshold} =\varepsilon/(dH^{2}\beta\omega)\] \[\text{discrepancy\_threshold} =\tilde{\sigma}_{k}^{m}\beta\omega+3\frac{\varepsilon}{dH^{2}}\]

Assumption on the maximum discrepancy:

\[\eta\leq\frac{\alpha}{10d_{0}}\min\left\{\varepsilon/(dH^{3}\omega),1/\sqrt{ m_{\max}^{\prime}nH}\right\}=\tilde{\mathcal{O}}\left(\frac{\varepsilon^{2}}{d^{6}H^{8}}\right)\] (21)Proof of Proposition 3.4

Proof of Proposition 3.4, and the MDP conversion argument.: First, for _(i)_, we show the linearity of rewards with \(\theta_{1},\ldots,\theta_{H}\). For this take any \(h\in[H]\). Fix any policy \(\pi\in\Pi\) and let \(\bar{\theta}_{h}\in\mathcal{B}(L_{2})\) be such that for all \((s,a)\in\mathcal{S}_{h}\times[\mathcal{A}]\), \(q^{\pi}(s,a)\approx_{\eta}\left\langle\varphi(s,a),\bar{\theta}_{h}\right\rangle\) (the existence of such a \(\bar{\theta}\) follows from Definition 3.2). If \(h=H\), \(\mathbb{E}_{R}\rightarrow_{\mathcal{R}(s,a)}[R]=q^{\pi}(s,a)\), so \(\theta_{H}=\bar{\theta}_{H}\) satisfies Definition 3.1. For \(h<H\), let \(f:\mathcal{S}_{h+1}\rightarrow\mathbb{R}\) be defined as \(f(s)=v^{\pi}(s)\). Fix an arbitrary \(Q\in\text{PD}^{H}\), e.g., \(Q=(I,\ldots,I)\). Since \(v^{\pi}(s)\in[0,H]\) and \(\text{range}_{Q}(s)\geq\text{range}(s)/\sqrt{2d}\geq\alpha/\sqrt{2d}\) by Proposition 4.5, \(f\) is \(\alpha/(\sqrt{2d}H)\)-admissible, and therefore by Lemma 4.7 we can take \(\bar{\theta}_{h}\in\mathcal{B}(4Hd_{0}\sqrt{2d}L_{2}/\alpha)\) such that for all \((s,a)\in\mathcal{S}_{h}\times[\mathcal{A}]\),

\[\mathbb{E}(v^{\pi}(S_{h+1})\,|\,s,a)\approx_{\sqrt{2d}H\,\eta_{0}}\left\langle \varphi(s,a),\bar{\theta}_{h}\right\rangle\,,\]

where, as before, \(\eta_{0}=5d_{0}\eta/\alpha\). Since

\[\underset{R\rightarrow\mathcal{R}(s,a)}{\mathbb{E}}(R)=q^{\pi}(s,a)-\mathbb{ E}(v^{\pi}(S_{h+1})\,|\,s,a)\,,\]

letting \(\theta_{h}=\bar{\theta}_{h}-\bar{\theta}_{h}\) satisfies _(i)_ of Definition 3.1 with \(\kappa=\eta+\sqrt{2d}H\eta_{0}=\eta+5H\sqrt{2d}d_{0}\eta/\alpha\).

To show _(ii)_, take any \(f:\mathcal{S}\rightarrow[0,H]\) and \(h\in[H-1]\). As before, \(f\) is \(\alpha/(\sqrt{2d}H)\)-admissible, therefore Lemma 4.7 immediately provides \(\theta_{h}^{\prime}\) satisfying the required conditions.

Therefore, the MDP is shown to be linear with misspecification \(\eta+\sqrt{2d}H\eta_{0}\), and parameter bound \(L_{2}(4Hd_{0}\sqrt{2d}/\alpha+1)\). 

Sketch of the \(q^{\pi}\)-to-linear MDP conversion argument.: We elaborate on the conversion to linear MDP mechanism presented in Section 3. As the basis of this argument is that an idealistic range-determining oracle is present, we note that this argument only serves as intuition and is otherwise tangential to our proof. Instead of a direct approach of learning this oracle, our proof argues that learning about this oracle happens whenever there is a need (performance shortfall) for it. A formal reduction to linear MDPs given this oracle however is fairly straightforward but cumbersome, with the caveat that the linear MDP will end up with \(dH\) (instead of \(d\)) dimensional features. One would proceed by copying the features of each state \(s\) in stage \(h\) into the \(h^{\text{th}}\) chunk of size \(d\) of this vector of size \(dH\) (the rest of the vector remains zero). A similar transformation is applied to all \(\theta_{h}(\pi)\). Then, \(H\) copies are made of each high-enough-range state, with all possible stages (but keeping the feature vectors). These will be the states of the new MDP we construct. When a transition from state \(s\) leads to skipped states, the linear MDP returns with the copy of the first non-skipped state that has a stage counter of \(\text{stage}(s)+1\), so that in this linear MDP the stage numbers are consecutive (as required by our definitions). \(q^{\pi}\)-realizability of this modified MDP is easy to show, and (as it has no low-range states) Proposition 3.4 can be used to show that the modified MDP is linear. To account for the fact that this new MDP may finish an episode in fewer than \(H\) steps due to the skips, we add a special, zero-reward, self-transitioning state called "episode-over". To ensure that the MDP stays linear, we extend the feature vectors of each state by a scalar 1, and a scalar indicator of being in this state, with all original features of the "episode-over" state defined to be zero. It is easy to see that this construction leads to a linear MDP with the desired action-value functions.

## Appendix D Intuition behind our method and proof strategy from the perspective of Eleanor [24]

The starting point of our method is the Eleanor algorithm, which is designed for linear MDPs. Similarly to SkipPyEleanor, Eleanor solves an optimistic optimization problem inside a loop. The optimization problem computes optimistic estimates \(\bar{\theta}_{t}\) of the parameters of the MDP simultaneously for all \(t\in[H]\), and in each iteration of the loop, more data is collected according to the policy that is optimal for the MDP defined by the estimated parameters. Initial estimates \(\bar{\theta}_{t}\) are computed via solving least-squares problems whose covariates are the features corresponding to state-action pairs \((S_{t},A_{t})\) from all the data collected so far, while the corresponding least-squares targets are computed as the sum of the immediate reward \(R_{t}\) and the estimated value for \(S_{t+1}\), computed from \(\bar{\theta}_{t+1}\). \(\bar{\theta}_{t}\) is then optimistically chosen as the solution of the optimization problem, in the neighborhood (confidence ellipsoid) of \(\hat{\theta}_{t}\), the solution to this least-squares problem. It is shown that this optimistic choice of estimates results in an optimistic estimate of the value of \(v^{\star}\) of the initial state, and the regret is upper bounded in terms of the sum of elliptic potentials of the covariates.

This argument appears in our analysis too, with minor modifications due to our PAC-like setting (instead of aiming to bound the regret), leading to our final-iteration condition of Line 16 in Algorithm 1. Our Optimization Problem 4.10 is similar to Eleanor's, and the parameters \(\tilde{\theta}_{t}\) and \(\tilde{\theta}_{t}\) have the same meaning. A key difference between the optimization problems of Eleanor and SkippyEleanor are how the least-squares targets are determined. For Eleanor, it is the sum of the immediate reward \(R_{t}\) and its estimated value for \(S_{t+1}\)); with this target, only one on-policy rollout is required for each episode in order to get the least-squares parameter estimate for all \(H\) stages. In contrast, our least-squares targets are formed as the sum of \(R_{t}+\ldots+R_{t+i}\) and the estimated value for \(S_{t+i+1}\), where \(i\), the number of stages "skipped", depends on the guess \(\tilde{G}\). The guess \(\hat{G}\) is selected only in Optimization Problem 4.10, and we do not know its value at the time of data collection, so we cannot know which stages will have to be skipped for each rollout. Therefore, (i) we need access to the rewards of the current policy at any stage (similarly to Eleanor), and hence we run the current policy to any stage (including the last one); and (ii) perform rollouts with the fixed policy \(\pi^{0}\) (from any stage) to be able to estimate the reward \(R_{t}+\ldots+R_{t+i}\) collected while skipping over \(i\) stages (for any \(i\)). To ensure this happens for every stage, we start Phase II from every stage \(k\), resulting in the additional for loop in Line 6 of Algorithm 1 compared to Eleanor. Finally, the randomization in Phase I is applied to make the optimization problem smooth, as described in Section 4.

One could analyze this algorithm similarly to the analysis of Eleanor if it were not for the fact that the least-squares targets we just introduced are not realizable in general. We can, however, prove the realizability of certain components of the matrix-valued version of these targets, \(F\) (Lemma 4.9 and Corollary 4.11). This enables us to detect when the realizability of our least-squares targets fail, measure the direction (component) of the largest error, and learn from that. This is the job of Optimization Problem 4.12: \(\hat{F}^{ki}_{\tilde{G}\tilde{\theta}}\) corresponds to the matrix-valued empirical measurements of \(F\), while the \(y^{ki}_{\tilde{G}\tilde{\theta}}\) are the average predictions of the same quantities. If the targets are realizable, which happens if we manage to skip the right number of stages), these matrices are very close; if not, the direction of their largest discrepancy tells us something about \(\bot\,(Q,i)\), and allows us to learn.

Optimism ties all this together: either there is no shortfall between predicted and measured \(q\)-values (and we are done) or we grow the elliptical potential of \(X\) (the two cases present in the analysis of Eleanor, Zanette et al. (2020)), or we grow the elliptical potential of \(Q\) (the new case due to the lack of realizability guarantees).

## Appendix E Proof of Theorem 4.1

In this section we present the proof of Theorem 4.1. Recall that some quantities are defined in Appendix B.

### Checking consistency

We introduce some lemmas to establish the required guarantees of the consistency checker. Their proofs, which rely on the usual least squares analysis techniques and covering arguments, are presented in Appendix H.

**Lemma E.1**.: _Let \((k,i,v)\) be the outcome of Optimization Problem 4.12 any time during the execution of SkippyEleanor, and let \(w=\operatorname{Proj}_{Z(Q,i)}v\) as in the algorithm. Then,_

\[w^{\top}\left(y^{ki}_{\tilde{G}\tilde{\theta}}-\hat{F}^{ki}_{\tilde{G}\tilde{ \theta}}\right)w\geq v^{\top}\left(y^{ki}_{\tilde{G}\tilde{\theta}}-\hat{F}^{ ki}_{\tilde{G}\tilde{\theta}}\right)v-\frac{\varepsilon}{dH^{2}\omega}\]

**Lemma E.2**.: _There is an event \(\mathcal{E}_{1}\) that happens with probability at least \(1-\zeta\), such that under \(\mathcal{E}_{1}\), during the execution of SkippyEleanor, when the beginning of any iteration (Line 5) is executed, for any \(t\in[H-1]\), \(i\in[t+1:H]\), for any \(\tilde{G}\in\mathbf{G}\), \(\tilde{\theta}\in\tilde{\Theta}\), and \(v,w\in\mathcal{B}(1)\), for all \((s,a)\in\mathcal{S}_{t}\times[\mathcal{A}]\),_

\[\left|v^{\top}_{\|}\left(\varphi(s,a)^{\top}\tilde{\theta}^{i}_{\tilde{G} \tilde{\theta}}-\mathop{\mathbb{E}}_{\pi^{0},s,a}\tilde{F}_{\tilde{G}\tilde{ \theta}}(S_{i})\right)w\right|\leq\left\|\varphi(s,a)\right\|_{X^{-1}_{mt}} \beta+\frac{\varepsilon}{dH^{2}\omega}\,,\]

_where \(\bullet_{\|}\) denotes \(\bullet_{\|(Q,i)}\)._The next lemma uses the average least-squares predictions' (capped) uncertainty term \(\bar{\sigma}_{k}^{m}\) (defined in Eq. (18)), where the average is taken over predictions from the state-action pair where Phase I of SkippyPolicy\((\cdot,\cdot,k)\) ends.

**Lemma E.3**.: _There is an event \(\mathcal{E}_{2}\) with probability at least \(1-\zeta\), such that under \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), during the execution of SkippyEleanor, when Optimization Problem 4.12 is solved (Line 10), for \((\tilde{G},\bar{\theta})\) as recorded in Line 5 for all \(k\in[H-1]\), \(i\in[k+1:H]\), and \(v,w\in\mathcal{B}(1)\),_

\[\left|v_{\|}^{\top}\left(y_{\tilde{G}\,\bar{\theta}}^{ki}-\hat{F}_{\tilde{G} \,\bar{\theta}}^{ki}\right)w\right|\leq\bar{\sigma}_{k}^{m}\beta+3\frac{ \varepsilon}{dH^{2}\omega}\]

_where \(\bullet_{\|}\) denotes \(\bullet_{\|(Q,i)}\)._

Together, these lemmas can be used to show that the vector \(w\) derived from Line 10 in SkippyEleanor is sufficiently aligned with both \(Z(Q,\cdot)\) and the subspace \(\text{Proj}_{\bot(Q,\cdot)}\) projects to, which leads to the following important result:

**Lemma E.4**.: _Under the \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), if Line 13 is executed any time during the execution of SkippyEleanor (i.e., when the consistency check fails), then the resulting \(Q\) continues to be a valid preconditioning._

From now on, our lemmas assume the high-probability events of Lemmas E.2 and E.3 hold, and therefore \(Q\) is a valid preconditioning at any time during the execution by Lemma E.4.

### Sample complexity bounds

We bound the number of iterations of \(m\) that SkippyEleanor can execute. The proof of the following lemma is presented in Appendix I:

**Lemma E.5**.: _Throughout the execution of SkippyEleanor, \(m\leq m_{\text{max}}\)._

Note that throughout the execution of SkippyEleanor, \(m^{\prime}\leq m^{\prime}_{\text{max}}\). As \(m^{\prime}-m\) equals the number of times Line 13 is executed, i.e., the sum of sequence lengths corresponding to \(Q\), by Lemma 4.4,

**Corollary E.6**.: _Under \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\), SkippyEleanor returns with a policy before exiting the while loop of Line 3, and as each iteration executes \(Hn\) trajectories in Line 8, the number of interactions of SkippyEleanor with the MDP is bounded by \(\tilde{O}\left(H^{11}d^{7}/\varepsilon^{2}\right)\)._

### Performance guarantee

We next consider the \(m^{\text{th}}\) iteration of SkippyEleanor under the assumption that the consistency check passes, that is, Line 16 is executed. We intend to guarantee the performance of \(\pi^{mH}\) in terms of \(\sum_{t=1}^{H}\bar{\sigma}_{k}^{m}\), given that the optimization value \(x\) satisfies \(x\leq\bar{\sigma}_{k}^{m}\beta\omega+3\frac{\varepsilon}{dH^{2}}\) (which follows from the execution reaching Line 16). Next we introduce variants of \(c_{ki}^{j}\) and \(\bar{c}_{ki}^{j}\) (Eq. (20)) which act, instead of the data collected during the execution of the algorithm, on a trajectory \((S_{h},A_{h},R_{h})_{h\in[H]}\) and corresponding stage mapping \(p\) obtained by an independent run of SkippyPolicy, which will be clear from the context: \(\hat{c}_{ki}=\mathbbm{1}\left\{p(k)<i\right\}\), and

\[c_{ki}=\mathbbm{1}\left\{p(k)<i\text{ and }\left\|\varphi(S_{p(k)},A_{p(k)}) \right\|_{X_{m,p(k)}^{-1}}<2(\beta\omega dH)^{-1}\text{ and }\left\langle\varphi(S_{p(k)},A_{p(k)}),\bar{\theta}_{p(k)} \right\rangle\geq 0\right\}\,.\]

**Remark E.7**.: _In our analysis we rely on the obvious fact that the laws of the trajectories of SkippyPolicy\((\hat{G},\bar{\theta},k)\) and SkippyPolicy\((\hat{G},\bar{\theta},k+1)\) are the same until stage \(p(k+1)\) (as the policies are the same until then), for any parameters \(\hat{G}\) and \(\bar{\theta}\). This includes \(S_{p(k+1)}\) but not \(A_{p(k+1)}\) if \(p(k+1)\leq H\), and includes the whole trajectory ending with \(R_{H}\) otherwise._

We prove the following using induction on \(k=H,\ldots,1\) in Appendix J:

**Lemma E.8**.: _There is an event \(\mathcal{E}_{3}\) with probability at least \(1-3\zeta\), such that under \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), during the execution of SkippyEleanor, whenever Line 16 is executed, for \((\tilde{G},\bar{\theta})\) as recorded in Line 5 of the current iteration, for \(k\in[H]\),_

\[\bar{\mathcal{C}}^{k}:=\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\bar{c}_{k,H+1}C_{ \tilde{G}\,\bar{\theta}}(S_{p(k)})\leq\mathop{\mathbb{E}}_{\pi^{mH},s_{1}} \sum_{u=p(k)}^{H}R_{u}+2\sum_{i=k}^{H}\bar{\sigma}_{k}^{m}\beta\omega dH+4(H-k+ 1)\frac{\varepsilon}{H}\,.\] (22)As \(S_{1}=s_{1}\) is fixed and \(\tau(s_{1})=1\), we get the following corollary, which shows that the value \(\mathcal{C}_{\tilde{G}\tilde{\theta}}\) of the solution \((\tilde{G},\tilde{\theta})\) of Optimization Problem 4.10 can be used as a lower bound on the value of the policy \(\pi^{mH}\) up to the uncertainty and some \(\varepsilon\) terms:

**Corollary E.9**.: _Under \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), the value of Optimization Problem 4.10 with the solution \((\hat{G},\tilde{\theta})\) satisfies_

\[C_{\tilde{G}\tilde{\theta}}(s_{1})=\bar{C}^{1}\leq\mathop{\mathbb{E}}_{\pi^{ mH},s_{1}}\sum_{u=1}^{H}R_{u}+2\sum_{i=1}^{H}\bar{\sigma}_{k}^{m}\beta\omega dH ^{2}+4\varepsilon=v^{\pi^{mH}}\left(s_{1}\right)+2\sum_{i=1}^{H}\bar{\sigma}_{ k}^{m}\beta\omega dH^{2}+4\varepsilon\,.\]

### Optimism of Optimization Problem 4.10

The following establishes the optimistic property, that is, that the value of Optimization Problem 4.10 competes with \(v^{\star}(s_{1})\). The proof relies on the fact that the correct guess \(\hat{G}\) and a good choice of \(\bar{\theta}\) are feasible for the optimization problem, combined with the fact that this \(\bar{\theta}\) induces a policy \(\pi=\textsc{SkippyPolicy}(\hat{G},\bar{\theta},H)\) that takes action-value maximizing actions according to a very accurate approximation of action-values almost everywhere. In fact, it only skips states whose range is at most \(\varepsilon/H\). The proof is presented in Appendix K.

**Lemma E.10**.: _There is an event \(\mathcal{E}_{4}\) with probability at least \(1-\zeta\), such that under \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{4}\), throughout the execution of SkippyEleanor, the value of Optimization Problem 4.10 is at least \(v^{\star}(s_{1})-2\varepsilon\)._

Proof of Theorem 4.1.: We combine Lemma E.10 with Corollary E.9, Corollary E.6, and the fact that the condition of Line 16 is satisfied when SkippyEleanor returns with a policy, to get that under \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\cap\mathcal{E}_{4}\), that is, with probability at least \(1-6\zeta\), SkippyEleanor interacts with the MDP for at most \(\tilde{\mathcal{O}}\left(H^{11}d^{7}/\varepsilon^{2}\right)\) many steps, before returning with the policy \(\pi^{mH}\) that satisfies

\[v^{\star}(s_{1})\leq C_{\tilde{G}\tilde{\theta}}(s_{1})+2\varepsilon\leq v^{ \pi^{mH}}\left(s_{1}\right)+2\sum_{i=1}^{H}\bar{\sigma}_{k}^{m}\beta\omega dH ^{2}+6\varepsilon\leq v^{\pi^{mH}}\left(s_{1}\right)+8\varepsilon\,,\]

where the final inequality follows from the fact that when SkippyEleanor returns in Line 17, \(\sum_{k=1}^{H}\bar{\sigma}_{k}^{m}\leq\varepsilon/(\beta\omega dH^{2})\). By scaling the parameters, this finishes the proof of Theorem 4.1. 

## Appendix F Deferred definitions and proofs for Section 4.1

Proof of Lemma 4.3.: For any \(\theta\in\Theta_{h}^{Q}\), it holds that \(\theta=Q_{h}^{-1}\hat{\theta}\) for some \(\hat{\theta}\in\Theta_{h}\). Since \(\left\|\hat{\theta}\right\|_{2}\leq L_{2}\), and writing \(Q_{h}\) as in Definition 4.2,

\[\left\|\theta\right\|_{2}^{2}=\hat{\theta}^{\top}\left(L_{2}^{-2}I+\sum_{v\in C _{h}}vv^{\top}\right)\hat{\theta}\leq L_{2}^{-2}L_{2}^{2}+\left|C_{h}\right| \leq 1+d_{1}\,,\]

where we used Definition 4.2 and Lemma 4.4. Finally, we conclude that \(\left\|\theta\right\|_{2}\leq\sqrt{d_{1}+1}\). 

**Definition F.1**.: \((G_{h}^{Q},\rho_{h}^{Q})\) _is a near-optimal design for \(\Theta_{h}^{Q}\), if for any \(\theta\in\Theta_{h}^{Q}\),_

\[\langle v,\theta\rangle=0\quad\text{for all }v\in\operatorname{ Ker}(V(G_{h}^{Q},\rho_{h}^{Q})),\text{ and}\] (23) \[\left\|\theta\right\|_{V(G_{h}^{Q},\rho_{h}^{Q})^{\top}}^{2}\leq 2d,\] (24) \[\text{where }V(G_{h}^{Q},\rho_{h}^{Q})=\sum_{\pi\in G_{h}^{Q}} \rho_{h}^{Q}(\pi)(\theta_{h}^{Q}(\pi))(\theta_{h}^{Q}(\pi))^{\top}\,.\] (25)

An important corollary of the above definition is that if \(M=\operatorname{Proj}_{\operatorname{Im}(V(G_{h}^{Q},\varphi_{h}^{Q}))}\), then \(V(G_{h}^{Q},\rho_{h}^{Q})^{\top\frac{1}{2}}V(G_{h}^{Q},\rho_{h}^{Q})^{\frac{1} {2}}Mv=Mv\), and \(\langle\theta,Mv\rangle=\langle\theta,v\rangle\) due to Eq. (23), and so

\[\theta^{\top}v=\theta^{\top}V(G_{h}^{Q},\rho_{h}^{Q})^{\top\frac{1}{2}}V(G_{h}^ {Q},\rho_{h}^{Q})^{\frac{1}{2}}v\quad\text{ for all }\theta\in\Theta_{h}^{Q}\text{ and }v\in\mathbb{R}^{d}.\] (26)

[MISSING_PAGE_FAIL:17]

\([d_{0}]\) the index of the policy maximizing the range of the action-value function in state \(s\), that is, \(G^{Q}_{h,\text{ord}(s)}=\text{arg max}_{\pi\in G^{Q}_{h}}\max_{i,j\in[\mathcal{A} ]}(q^{\pi}(s,i)-q^{\pi}(s,j))\); to simplify notation, we define \(\tilde{G}(s)=G^{Q}_{h,\text{ord}(s)}\). For \(s\in\mathcal{S}_{h}\) let

\[(a^{+}(s),a^{-}(s))=\left\{\begin{array}{ll}\text{arg max}_{i,j\in[ \mathcal{A}]}\,\hat{q}^{\tilde{G}(s)}(s,i)-\hat{q}^{\tilde{G}(s)}(s,j)&\text{ if }f(s)\geq 0\\ \text{arg min}_{i,j\in[\mathcal{A}]}\,\hat{q}^{\tilde{G}(s)}(s,i)-\hat{q}^{ \tilde{G}(s)}(s,j)&\text{otherwise.}\end{array}\right.\]

By Eq. (7) and Definition 4.6 have that

\[\left|\hat{q}^{\tilde{G}(s)}(s,a^{+}(s))-\hat{q}^{\tilde{G}(s)}(s,a^{-}(s)) \right|=\text{range}_{Q}(s)\geq\alpha|f(s)|\geq 0\,.\]

Since \(q^{\tilde{G}(s)}(s,a^{+}(s))-q^{\tilde{G}(s)}(s,a^{-}(s))\approx_{2\eta}\, \,\hat{q}^{\tilde{G}(s)}(s,a^{+}(s))-\hat{q}^{\tilde{G}(s)}(s,a^{-}(s))\), if \(\alpha|f(s)|\geq 4\eta\), we have

\[q^{\tilde{G}(s)}(s,a^{+}(s))-q^{\tilde{G}(s)}(s,a^{-}(s))\geq\alpha f(s)-2 \eta\geq\frac{\alpha}{2}f(s)>0\qquad\text{if }f(s)\geq 0\] (28)

\[q^{\tilde{G}(s)}(s,a^{+}(s))-q^{\tilde{G}(s)}(s,a^{-}(s))\leq\alpha f(s)+2 \eta\leq\frac{\alpha}{2}f(s)<0\qquad\text{otherwise.}\]

Let us define \(f^{\prime}:\mathcal{S}_{h}\to\mathbb{R}\) as

\[f^{\prime}(s)=\left\{\begin{array}{ll}\frac{\alpha f\,(s)/2}{q^{\tilde{G}( s)}(s,a^{+}(s))-q^{\tilde{G}(s)}(s,a^{-}(s))}&\text{if }\alpha|f(s)|\geq 4\eta\\ 0&\text{otherwise.}\end{array}\right.\]

By Eq. (28), there can be no division by zero in the above definition, and \(0\leq f^{\prime}(s)\leq 1\).

Now we are ready to define \(\pi^{+}_{k}\) and \(\pi^{-}_{k}\). Both policies follow \(\pi\) up to stage \(h-1\), when they switch to \(G^{Q}_{h,k}\), except if at stage \(h\) a state \(s\in\mathcal{S}_{h}\) is such that \(G^{Q}_{h,k}\) has the maximal action-value function range. In this case \(\pi^{+}_{k}\) selects \(a^{+}(s)\) with probability \(f^{\prime}(s)\) and \(a^{-}(s)\) with probability \(1-f^{\prime}(s)\), while \(\pi^{-}_{k}\) always selects \(a^{-}(s)\). Formally, for \(k\in[d_{0}]\), we define for \(s\in\mathcal{S}\)

\[\pi^{+}_{k}(s)=\left\{\begin{array}{ll}\pi(s)&\text{if }\text{stage}(s)<h; \\ a^{+}(s)\text{ w.p. }f^{\prime}(s),\text{ and }a^{-}(s)\text{ w.p. }1-f^{\prime}(s)&\text{if }\text{stage}(s)=h\text{ and }\text{ord}(s)=k;\\ G^{Q}_{h,k}(s),&\text{otherwise,}\end{array}\right.\]

where w.p. stands for _with probability_. Similarly,

\[\pi^{-}_{k}(s)=\left\{\begin{array}{ll}\pi(s)&\text{if }\text{stage}(s)<h; \\ a^{-}(s)\text{ w.p. }1&\text{if }\text{stage}(s)=h\text{ and }\text{ord}(s)=k; \\ G^{Q}_{h,k}(s)&\text{otherwise.}\end{array}\right.\]

Note that \(\pi^{+}_{k}\in\Pi\) and \(\pi^{-}_{k}\in\Pi\), as desired. Since for all \(k\in[d_{0}]\), the policies follow \(G_{h,k}\) for \(s\in\mathcal{S}_{t^{\prime}}\) for \(t^{\prime}>h\), therefore for all \(k\in[d_{0}]\),

\[v^{\pi^{-}_{k}}(s)=v^{\pi^{+}_{k}}(s)=v^{G^{Q}_{h,k}}(s)\quad\text{for all }s\in\mathcal{S}_{h+1},\text{ and}\] (29)

\[q^{\pi^{-}_{k}}(s,a)=q^{\pi^{+}_{k}}(s,a)=q^{G^{Q}_{h,k}}(s,a)\quad\text{for all }(s,a)\in\mathcal{S}_{h}\times[\mathcal{A}].\] (30)

Also, for any \(s\in\mathcal{S}\) with \(\text{stage}(s)<h\) and any \(a\in[\mathcal{A}]\),

\[\sum_{k\in[d_{0}]}\left(q^{\pi^{+}_{k}}(s,a)-q^{\pi^{-}_{k}}(s,a)\right) =\mathop{\mathbb{E}}_{\pi,s,a}\sum_{k\in[d_{0}]}\left(v^{\pi^{+} _{k}}(S_{h})-v^{\pi^{-}_{k}}(S_{h})\right)\] \[=\mathop{\mathbb{E}}_{\pi,s,a}\left(v^{\pi^{+}_{\text{ord}(S_{h}) }}(S_{h})-v^{\pi^{-}_{\text{ord}(S_{h})}}(S_{h})\right)\] \[=\mathop{\mathbb{E}}_{\pi,s,a}\left(q^{\tilde{G}(S_{h})}(S_{h},a^ {+}(S_{h}))f^{\prime}(S_{h})+q^{\tilde{G}(S_{h})}(S_{h},a^{-}(S_{h}))(1-f^{ \prime}(S_{h}))\right.\] \[\qquad\qquad-q^{\tilde{G}(S_{h})}(S_{h},a^{-}(S_{h}))\right)\] \[=\mathop{\mathbb{E}}_{\pi,s,a}\left(f^{\prime}(S_{h})\left(q^{ \tilde{G}(S_{h})}(S_{h},a^{+}(S_{h}))-q^{\tilde{G}(S_{h})}(S_{h},a^{-}(S_{h})) \right)\right)\] \[=\mathop{\mathbb{E}}_{\pi,s,a}\{\alpha|f(S_{h})|\geq 4\eta\}\frac{ \alpha}{2}f(S_{h})\approx_{2\eta}\frac{\alpha}{2}\mathop{\mathbb{E}}_{\pi,s,a}f(S_{h})\,,\]where the first line is due to both \(q^{\pi^{*}_{k}}\) and \(q^{\pi^{*}_{k}}\) following \(\pi\) on states with stage less than \(h\), the second line follows from the fact that for any \(s\in\mathcal{S}_{h}\), \(\pi^{*}_{k}(s)=\pi^{*}_{k}(s)\) for any \(k\neq\operatorname{ord}(s)\); combining this with Eq. (29) leads to all \(k\neq\operatorname{ord}(s)\) terms of the sum to cancel. The third line follows from expanding the definition of the policies and Eq. (30).

Let \(\tilde{\theta}=\frac{2}{\alpha}\sum_{k\in[d_{0}]}\big{(}\theta_{t}(\pi^{*}_{k} )-\theta_{t}(\pi^{*}_{k})\big{)}\). Since \(\left\|\theta_{t}(\cdot)\right\|_{2}\leq L_{2}\) by definition, we have \(\left\|\tilde{\theta}\right\|_{2}\leq 4d_{0}L_{2}/\alpha\). By Definition 3.2, for all \((s,a)\in\mathcal{S}_{h}\times[\mathcal{A}]\),

\[\left\langle\varphi(s,a),\frac{\alpha}{2}\tilde{\theta}\right\rangle\approx_{2 d_{0}\eta}\sum_{k\in[d_{0}]}q^{\pi^{*}_{k}}(s,a)-q^{\pi^{*}_{k}}(s,a)\approx_{2 \eta}\frac{\alpha}{2}\sum_{\pi,s,a}f(S_{h}),\]

and hence

\[\left\langle\varphi(s,a),\tilde{\theta}\right\rangle\approx_{4(d_{0}+1)\eta/ \alpha}\sum_{\pi,s,a}f(S_{h})\,.\]

Since \(4(d_{0}+1)\eta/\alpha\leq\eta_{0}=5d_{0}\eta/\alpha\) as \(d_{0}\geq 4\) by definition, this completes the proof. 

Proof of Lemma 4.9.: Take any \(s\in\mathcal{S}_{h}\). For the correct guess, \(\operatorname{range}^{\hat{G}}_{Q}(s)=\operatorname{range}_{Q}(s)\). Then, using that \(\left\|\tilde{\varphi}_{Q}(\cdot)\right\|_{2}\leq 1\), \(\operatorname{Tr}(\mathbf{f}(s))\leq\operatorname{range}_{Q}(s)\frac{\sqrt{2d} H^{2}}{\varepsilon}\), proving the second claim of the lemma (as \(\gamma\leq 1\)). For the first claim, take any \(\hat{G}=(\hat{\theta}^{i}_{h})_{h\in[2:H]_{i}\in[d_{0}]}\in\mathbf{G}\). Let \(\varphi^{\prime}\) be the unnormalized version of \(\tilde{\varphi}_{Q}(s)\) of Eq. (27), that is, \(\varphi^{\prime}=\varphi_{Q}(s,i,j)\) for the same \(i,j\) as in Eq. (27) (i.e., with the largest \(\ell_{2}\)-norm). Then, using that \(\hat{G}\in\mathbf{G}\),

\[\operatorname{range}^{\hat{G}}_{Q}(s)=\max_{k\in[d_{0}]}\max_{i,j}\left\langle \varphi_{Q}(s,i,j),\hat{\theta}^{k}_{k}\right\rangle\leq\left\|\varphi^{ \prime}\right\|_{2}\max_{k\in[d_{0}]}\left\|\hat{\theta}^{k}_{h}\right\|_{2} \leq\left\|\varphi^{\prime}\right\|_{2}\sqrt{d_{1}+1}.\]

Using that above in combination with \(\left|f(s)\right|\leq H\), \(v,w\in\mathcal{B}(1)\), \(\left\|\tilde{\varphi}_{Q}(s)\right\|_{2}\leq 1\), we obtain

\[\left|v^{\top}_{\parallel}\mathbf{f}(s)w\right| \leq\left|\left\langle\tilde{\varphi}_{Q}(s),v_{\parallel}\right\rangle \left\langle\tilde{\varphi}_{Q}(s),w\right\rangle\right|\operatorname{range}^{ \hat{G}}_{Q}(s)\frac{\sqrt{2d}H^{2}}{\varepsilon}\] \[\leq\left\|\tilde{\varphi}_{Q}(s)\right\|_{2}\left\|\varphi^{ \prime}\right\|_{2}\sqrt{d_{1}+1}\frac{\sqrt{2d}H^{2}}{\varepsilon}=\left\| \varphi^{\prime}_{\parallel}\right\|_{2}\sqrt{d_{1}+1}\frac{\sqrt{2d}H^{2}}{ \varepsilon}\,.\]

As the eigenvalues of \(V(G^{Q}_{h},\rho^{Q}_{h})=\sum_{\pi\in G^{Q}_{h}}\rho^{Q}_{h}(\pi)(\theta^{Q}_ {h}(\pi))(\theta^{Q}_{h}(\pi))^{\top}\) corresponding to the subspace in which \(\varphi^{\prime}_{\parallel}\) lies are by definition at least \(\gamma\), we can write

\[(\operatorname{range}_{Q}(s))^{2}\geq\max_{\pi\in G^{Q}_{h}}\left\langle \varphi^{\prime},\theta^{Q}_{h}(\pi)\right\rangle^{2}\geq\varphi^{\prime\, \top}V(G^{Q}_{h},\rho^{Q}_{h})\varphi^{\prime}\geq\varphi^{\prime\,\top}_{ \parallel}V(G^{Q}_{h},\rho^{Q}_{h})\varphi^{\prime}_{\parallel}\geq\left\| \varphi^{\prime}_{\parallel}\right\|_{2}^{2}\gamma\,.\]

Combining with the previous result, we get that

\[\operatorname{range}_{Q}(s)\geq\sqrt{\gamma}\left\|\varphi^{\prime}_{\parallel} \right\|_{2}\geq\frac{\sqrt{\gamma}\varepsilon}{\sqrt{2d}\sqrt{d_{1}+1}H^{2} }|v^{\top}_{\parallel}\mathbf{f}(s)w|=\alpha|v^{\top}_{\parallel}\mathbf{f}( s)w|,\]

finishing the proof. 

## Appendix H Deferred proofs for Appendix E.1

The definitions (Eqs. (9) and (10)) immediately give rise to the following facts:

\[D(s_{i}\rightarrow)\in\left[-\sum_{u=i}^{H}r_{u},H\right]\subseteq[-H,H] \text{ and }\tau(\cdot)\in[0,1]\text{, implying}\] (31) \[E^{\rightarrow}(s_{i}\rightarrow)\in\left[-\sum_{u=i}^{H}r_{u},H \right]\subseteq[-H,H]\text{, implying}\] \[E(s_{i}\rightarrow)\in\left[-2\tau(s_{i})H,2\tau(s_{i})H\right] \subseteq[-2H,2H]\,.\]

Furthermore, since either \(\tau(s_{i})=0\) or \(\left\|\tilde{\varphi}_{Q}(s_{i})\right\|_{2}=1\) (as \(\left\|\tilde{\varphi}_{Q}(s_{i})\right\|=0\) implies that \(\operatorname{range}_{Q}(s_{i})\) and hence \(\tau(s_{i})\) are both zero), we have

\[\operatorname{Tr}(F(s_{i}\rightarrow))=E(s_{i}\rightarrow)\,,\] (32)

which was used to establish the last part of Corollary 4.11.

Proof of Lemma e.1.: We drop the subscripts \((\hat{G},\bar{\theta})\). Let \((\hat{\phi}^{i}_{t})_{h\in[2:H],i\in[d_{0}]}=\hat{G}\in\mathbf{G}\). Let \(z=v-w\) be the projection of \(v\) to the subspace orthogonal to \(Z(Q,i)\), denoted by \(Z(Q,i)^{\perp}\). In other words, \(z=\operatorname{Proj}_{Z(Q,i)^{\perp}}v\). Let \(\mathbf{M}=y^{ki}-\hat{F}^{ki}\). By the symmetry of \(\mathbf{M}\),

\[v^{\top}\mathbf{M}v=z^{\top}\mathbf{M}(v+w)+w^{\top}\mathbf{M}w\,.\]

It is enough to prove therefore that

\[\frac{\varepsilon}{dH^{2}\omega}\geq z^{\top}\mathbf{M}(v+w)\,.\]

As \(\left\|v\right\|_{2}\leq 1\) and \(\left\|v+w\right\|_{2}\leq 2\), and using the definitions and Eq. (31), for any input (\(s_{i}\)\(\rightarrow\)),

\[\left|\varepsilon^{\top}F(s_{i}\)\(\rightarrow\)\(\left\rangle+w\right)\right| =\left|\left\langle z,\bar{\varphi}_{Q}(s_{i})\right\rangle \left\langle v+w,\bar{\varphi}_{Q}(s_{i})\right\rangle E(s_{i}\)\right|\] \[\leq 4H\tau(s_{i})\left|\left\langle z,\bar{\varphi}_{Q}(s_{i}) \right\rangle\right|\leq 4\operatorname{range}_{Q}^{\hat{G}}(s_{i})\frac{\sqrt{2d}H^{2}} {\varepsilon}\left|\left\langle z,\bar{\varphi}_{Q}(s_{i})\right\rangle\right|\] \[\leq 4\left|\left\langle z,\bar{\varphi}_{Q}(s_{i})\right\rangle \right|\max_{a,b,\,\leqslant\,[d_{0}]}\left\langle\varphi_{Q}(s,a,b),\hat{\phi }^{k}_{h}\right\rangle\frac{\sqrt{2d}H^{2}}{\varepsilon}\] \[\leq 4\left\|\operatorname{Proj}_{Z(Q,i)^{\perp}}\bar{\varphi}_{Q} (s)\right\|_{2}\left\|\varphi^{\prime}\right\|_{2}\max_{k\,\in\,[d_{0}]}\left\| \hat{\phi}^{k}_{h}\right\|_{2}\frac{\sqrt{2d}H^{2}}{\varepsilon}\] \[\leq 4\left\|\operatorname{Proj}_{Z(Q,i)^{\perp}}\bar{\varphi}^{ \prime}\right\|_{2}\sqrt{d_{1}+1}\frac{\sqrt{2d}H^{2}}{\varepsilon}\,,\]

where \(\varphi^{\prime}\) is the unnormalized version of \(\bar{\varphi}_{Q}(s_{i})\) of Eq. (27), that is, \(\varphi^{\prime}=\varphi_{Q}(s_{i},a,b)\) for the same \(a,b\) as in Eq. (27) (i.e., with the largest \(\ell_{2}\)-norm).

As \(\operatorname{Proj}_{Z(Q,i)^{\perp}}\varphi^{\prime}=\operatorname{Proj}_{Z(Q,i )^{\perp}}(\varphi_{Q}(s,a)-\varphi_{Q}(s,b))=\operatorname{Proj}_{Z(Q,i)^{ \perp}}Q_{i}(\varphi(s,a)-\varphi(s,b))\) for some \(s\in\mathcal{S}_{i},a,b\in[\mathcal{A}]\), and by definition \(\operatorname{Proj}_{Z(Q,i)^{\perp}}Q_{i}\preceq L_{3}^{-2}I\), \(\left\|\operatorname{Proj}_{Z(Q,i)^{\perp}}\varphi^{\prime}\right\|_{2} \leq L_{3}^{-2}\left\|\varphi(s,a)-\varphi(s,b)\right\|_{2}\leq 2L_{3}^{-2}L_{1}\), so

\[\left|z^{\top}F(s_{i}\)\(\rightarrow\)\(\left\rangle+w\right)\right|\leq 8L_{3}^{-2}L_{1} \sqrt{d_{1}+1}\frac{\sqrt{2d}H^{2}}{\varepsilon},\] (33)

and hence

\[\left|z^{\top}\hat{F}^{ki}(v+w)\right|\leq 8L_{3}^{-2}L_{1}\sqrt{d_{1}+1} \frac{\sqrt{2d}H^{2}}{\varepsilon}\,.\] (34)

To bound \(\left|z^{\top}y^{ki}(v+w)\right|\), note that by the definition \(y^{ki}\),

\[z^{\top}y^{ki}(v+w) =\frac{1}{n}\sum_{j\in[n]}c^{j}_{ki}\left\langle\varphi_{p(k)}^{mkj },\bar{\theta}^{p(mkj),t}\right\rangle\] \[\text{where}\qquad\bar{\theta}^{ti} =X_{mt}^{-1}\sum_{lkj\in\mathbf{I}^{m}(t)}\varphi_{t}^{lkj}\left(z ^{\top}F(S^{lkj}_{i},\ldots,R^{lkj}_{H})(v+w)\right)\qquad\text{for }t\in[i-1]\]

Therefore

\[\left|z^{\top}y^{ki}(v+w)\right|\leq\max_{t\,\in\,[i-1],s\in\mathcal{S}_{i},a \in[\mathcal{A}]}\left\langle\varphi(s,a),\bar{\theta}^{ti}\right\rangle\,.\]

Fix any \(t\in[i-1]\), \(s\in\mathcal{S}_{i},a\in[\mathcal{A}]\). By repeated application of the Cauchy-Schwarz inequality, the fact that \(X_{mt}\geq\lambda I\), the triangle inequality, and using Eq. (33),

\[\left|\left\langle\varphi(s,a),\bar{\theta}^{ti}\right\rangle\right| \leq\left\|\varphi(s,a)\right\|_{X_{mt}^{-1}}\left\|\sum_{lkj\in \mathbf{I}^{m}(t)}\varphi_{t}^{lkj}\left(z^{\top}F(S^{lkj}_{i},\ldots,R^{lkj}_{ H})(v+w)\right)\right\|_{X_{mt}^{-1}}\] \[\leq\left\|\varphi(s,a)\right\|_{2}\lambda^{-1/2}\cdot 8L_{3}^{-2}L_{1} \sqrt{d_{1}+1}\frac{\sqrt{2d}H^{2}}{\varepsilon}\sum_{lkj\in\mathbf{I}^{m}(t)} \left\|\varphi_{t}^{lkj}\right\|_{X_{mt}^{-1}}^{\infty}\] \[\leq 8L_{3}^{-2}L_{1}^{2}\lambda^{-1/2}\sqrt{d_{1}+1}\frac{\sqrt{2d} H^{2}}{\varepsilon}\sqrt{\left\|\mathbf{I}^{m}(t)\right|}\sqrt{\sum_{lkj\in \mathbf{I}^{m}(t)}\left\|\varphi_{t}^{lkj}\right\|_{X_{mt}^{-1}}^{2}}\] \[\leq 8L_{3}^{-2}L_{1}^{2}\lambda^{-1/2}\sqrt{d_{1}+1}\frac{\sqrt{2d} H^{2}}{\varepsilon}\sqrt{m_{\max}nHd}\,,\]where we use that \(|\mathbf{I}^{m}(t)|\leq mnH,\,m\leq m_{\max}\) by Lemma E.5, and that

\[\sqrt{\sum_{lkj\in\mathbf{I}^{m}(t)}\left\|\hat{\varphi}_{t}^{lkj}\right\|_{X_{m }^{m}}^{2}}=\sqrt{\sum_{lkj\in\mathbf{I}^{m}(t)}\operatorname{Tr}(X_{mt}^{-1} \varphi_{t}^{lkj}\varphi_{t}^{lkj\top})}\leq\sqrt{\operatorname{Tr}X_{mt}^{-1} X_{mt}}=\sqrt{d}\,.\]

Combining with Eq. (34), with an appropriate choice of \(L_{3}\), we obtain

\[\left|z^{\top}\mathbf{M}(v+w)\right|\leq 8L_{3}^{-2}L_{1}\sqrt{d_{1}+1}\frac{ \sqrt{2}dH^{2}}{\varepsilon}\left(1+L_{1}\lambda^{-1/2}\sqrt{m_{\max}nHd} \right)\leq\frac{\varepsilon}{dH^{2}\omega}\] (35)

as desired. 

Proof of Lemma e.2.: Choose \(\beta\)

\[\beta\leq 2+2H\sqrt{2dH(d_{0}+1)\log\frac{12d_{0}HL_{2}}{\alpha\xi}+2\log \frac{m_{\max}^{\prime}H^{2}}{\zeta}+d\log\left(\lambda+m_{\max}^{\prime}nHL_ {1}^{2}/d\right)},\] (36)

satisfying \(\beta=\tilde{\mathcal{O}}(H^{3/2}d)\) as given in Eq. (15), and define

\[\xi=\frac{\varepsilon}{5\sqrt{2d}(H+1)^{3}L_{1}}\left(\min\left\{\varepsilon/ (dH^{2}\omega),1/\sqrt{m_{\max}^{\prime}nH}\right\}-\eta_{0}\right).\]

Note that subtracting \(\eta_{0}\) keeps \(\xi\) positive, and of the same order, by our assumption that \(\eta\) is small enough: \(\eta_{0}\leq\frac{1}{2}\min\left\{\varepsilon/(dH^{2}\omega),1/\sqrt{m_{\max} ^{\prime}nH}\right\}\), which follows from Eq. (21).

We start with a covering argument for the set of functions of the form \(v_{\parallel}^{\top}\tilde{F}_{\tilde{\mathcal{O}}\tilde{\mathcal{O}}}w\), for different choices of \(\tilde{\mathcal{G}}\), \(\tilde{\theta}\), \(v\), and \(w\). By [21, Corollary 4.2.13], there is a set \(C_{\xi}\subset\mathcal{B}(1)\) with \(|C_{\xi}|\leq(3/\xi)^{d}\) such that for all \(x\in\mathcal{B}(1)\) there exists a \(y\in C_{\xi}\) with \(\|x-y\|_{2}\leq\xi\). Therefore, there is a set \(C_{\xi}^{\times}\subset\left(\bigtimes_{h\in[2:H],k\in[d_{0}]}\mathcal{B}( \sqrt{d_{1}+1})\right)\times\left(\bigtimes_{h\in[2:H]}\mathcal{B}(4d_{0}HL_{ 2}/\alpha)\right)\times\mathcal{B}(1)\times\mathcal{B}(1)\) with \(|C_{\xi}^{\times}|\leq(12d_{0}HL_{2}/(\alpha\xi))^{dH(d_{0}+1)}\) such that for any \(\tilde{\mathcal{G}}=(\tilde{\theta}_{h}^{i})_{h\in[2:H],i\in[d_{0}]}\in\mathbf{ G}\), \(\tilde{\theta}\in\tilde{\Theta}\), and \(v,w\in\mathcal{B}(1)\), there exists a \(y\in C_{\xi}^{\times}\), such that if we let \(\tilde{G}=(\tilde{\theta}^{i})_{h\in[2:H],i\in[d_{0}]}=(y_{(h-1)d_{0}+i})_{h \in[2:H],i\in[d_{0}]}\), \(\tilde{\theta}=(\tilde{\theta}_{h})_{h\in[2:H]}=(y_{(H-1)d_{0}+h})_{h\in[2:H]}\), and \(a=y_{(H-1)(d_{0}+1)+1}\), \(b=y_{(H-1)(d_{0}+1)+2}\), then \(\tilde{G}\in\mathbf{G}\), \(\tilde{\theta}\in\tilde{\Theta}\), \(a,b\in\mathcal{B}(1)\), and

\[\left\|a-v\right\|_{2}\leq\xi\quad\text{and}\quad\left\|b-w\right\|_{2}\leq \xi\,,\quad\text{and}\]

\[\left\|\tilde{\theta}_{h}^{i}-\tilde{\theta}^{i}\right\|_{2}\leq\xi\text{ and }\left\|\tilde{\theta}_{h}-\tilde{\theta}_{h}\right\|_{2}\leq\xi\text{ for all }h\in[2:H],i\in[d_{0}]\,.\]

As a result, for all \(s\in\mathcal{S}\setminus\mathcal{S}_{1}\), \(|\operatorname{range}_{Q}^{\mathcal{G}}(s)-\operatorname{range}_{Q}^{\mathcal{G} }(s)|\leq 2L_{1}\xi\), and therefore \(|\tau_{\tilde{\mathcal{O}}\tilde{\theta}}(s)-\tau_{\tilde{\mathcal{O}}\tilde{ \theta}}(s)|\leq 2\sqrt{2d}HL_{1}\xi/\varepsilon\). Furthermore, \(|D_{\tilde{\mathcal{O}}\tilde{\theta}}(s,\ldots,r_{H})-D_{\tilde{\mathcal{O}} \tilde{\theta}}(s,\ldots,r_{H})|\leq L_{1}\xi\). Combining these with the facts that in either case, \(\tau(\cdot)\in[0,1]\), \(D(\cdot)\in[-H,H]\), and \(E^{\rightarrow}(\cdot)\in[-H,H]\) (Eq. (31)), and using the definition of \(E^{\rightarrow}\),we have that for any \(i\in[H+1]\) and inputs,

\[|E_{\tilde{\mathcal{O}}\tilde{\theta}}(s_{i\rightarrow})-E_{\tilde{ \mathcal{O}}\tilde{\theta}}(s_{i\rightarrow})| \leq 4\sqrt{2d}H^{2}L_{1}\xi/\varepsilon+L_{1}\xi+|E_{\tilde{ \mathcal{O}}\tilde{\theta}}^{-}(s_{i+1}\rightarrow)-E_{\tilde{\mathcal{O}} \tilde{\theta}}^{-}(s_{i+1}\rightarrow)|\] \[= 4\sqrt{2d}H^{2}L_{1}\xi/\varepsilon+L_{1}\xi+\sum_{j=i+1}^{H}|E_{ \tilde{\mathcal{O}}\tilde{\theta}}(s_{j\rightarrow})-E_{\tilde{\mathcal{O}} \tilde{\theta}}(s_{j\rightarrow})|\] \[\leq (H+1)5\sqrt{2d}H^{2}L_{1}\xi/\varepsilon\,,\]

where the first inequality sums over the contributions of \(\tau\), \(D\), and \(E^{\rightarrow}\), and the second applies induction. By combining this bound with the bounds on \(\|v-a\|_{2}\) and \(\|w-b\|_{2}\), and that \(E(\cdot)\in[-2H,2H]\) (Eq. (31)) implying that \(\tilde{F}(\cdot)\in[-2H,2H]\), for all \(s\in\mathcal{S}\setminus\mathcal{S}_{1}\), we have that

\[\left|v_{\parallel}^{\top}\tilde{F}_{\tilde{\mathcal{O}}\tilde{ \theta}}(s)w-a_{\parallel}^{\top}\tilde{F}_{\tilde{\mathcal{O}}\tilde{ \theta}}b\right|(s) \leq 6H\xi+(H+1)5\sqrt{2d}H^{2}L_{1}\xi/\varepsilon\] (37) \[\leq 5\sqrt{2d}(H+1)^{3}L_{1}\xi/\varepsilon=\min\{\varepsilon/(dH^{2 }\omega),1/\sqrt{m_{\max}^{\prime}nH}\}-\eta_{0}\]

Take any \(m^{\prime}\in[m_{\max}^{\prime}]\) (this includes the entire execution of SkippyEleanor). and let the quantities of Section 4.3 (such as \(F\)) be calculated with the value of \(Q\) at the beginning iteration (Line 5). Take any \(t\in[H-1]\), \(i\in[t+1:H]\). Take any \(y\in C_{\xi}^{\times}\) and assign values to \(a,b,\hat{G}\), and \(\bar{\theta}\) based on \(y\) as above. For any \(lkj\in\mathbf{I}^{m}(t)\), observe that given all the history of SkippyEleanor interacting with the MDP up to (and including) \(S_{t}^{lkj},A_{t}^{lkj}\), the trajectory \(S_{t+1}^{lkj},A_{t+1}^{lkj},\ldots,R_{H}^{lkj}\) is an independent rollout with policy \(\pi^{0}\), with its law given by \(\mathcal{P}_{\pi^{0},S_{t}^{lkj},A_{t}^{lkj}}\). The random variable \(a_{\parallel}^{\top}F_{\hat{G}\,\bar{\theta}}(S_{i}^{lkj}\ldots,R_{H}^{lkj})b\) has range \([-2H,2H]\) and expectation (conditioned on this history) \(\mathbb{E}_{\pi^{0},S_{t}^{lkj},A_{t}^{lkj}}\,A_{t}^{\top}\bar{F}_{\hat{G}\, \bar{\theta}}(S_{i})b\). Let \(\bar{\theta}_{ti}\) be \(\bar{\theta}_{ti}\) from Corollary 4.11, satisfying \(\left\|\bar{\theta}_{ti}\right\|_{2}\leq 1/\sqrt{\lambda}\) and Eq. (11) for \(a_{\parallel}\), \(b\), \(\tilde{G}\), and \(\bar{\theta}\) instead of \(v_{\parallel}\), \(w\), \(\hat{G}\), and \(\bar{\theta}\):

\[\mathop{\mathbb{E}}_{\pi^{0},s,a}a_{\parallel}^{\top}\bar{F}_{\hat{G}\,\bar{ \theta}}(S_{i})b\approx_{\eta_{0}}\left\langle\varphi(s,a),\tilde{\theta}_{ti} \right\rangle\,.\] (38)

Take the sequence \(A\) formed of \(\varphi_{t}^{lkj}\) (for \(lkj\in\mathbf{I}^{m}(t)\), in the order that these random variables are observed), and the sequence \(X\) formed of \(v_{\parallel}F_{\hat{G}\,\bar{\theta}}(S_{i}^{lkj},\ldots,R_{H}^{lkj})_{W}\) (for \(lkj\in\mathbf{I}^{m}(t)\), in the same order), and the sequence \(\Delta\) formed of \(\mathbb{E}_{\pi^{0},S_{t}^{lkj},A_{t}^{lkj}}\,v_{\parallel}F_{\hat{G}\,\bar{ \theta}}(S_{i})w-\left\langle\varphi_{t}^{lkj},\tilde{\theta}_{ti}\right\rangle\) (for \(lkj\in\mathbf{I}^{m}(t)\), in the same order, for any \(v,w\), \(\hat{G}\), and \(\bar{\theta}\) as in the statement of this lemma). Then the sequences \(A\), \(X\), and \(\Delta\) satisfy the conditions of Lemma M.4 with a subgaussianity parameter \(\sigma=2H\). Due to this lemma, with probability at least \(1-\zeta/(m^{\prime}_{\max}H^{2}|C_{\xi}^{\times}|)\), for any choice of \(v,w,\hat{G}\), and \(\bar{\theta}\) (as above),

\[\left\|\tilde{\theta}_{ti}-\tilde{\theta}_{ti}\right\|_{X_{mt}} <\sqrt{\lambda}\left\|\tilde{\theta}_{ti}\right\|_{2}+\left\| \Delta\right\|_{\infty}\sqrt{|\mathbf{I}^{m}(t)|}+2H\sqrt{2\log\left(\frac{m^ {\prime}_{\max}H^{2}|C_{\xi}^{\times}|}{\zeta}\right)+\log\left(\frac{\det X_ {mt}}{\lambda^{d}}\right)}\] (39) \[\text{where}\qquad\tilde{\theta}_{ti} =X_{mt}^{-1}\sum_{lkj\in\mathbf{I}^{m}(t)}\varphi_{t}^{lkj}v_{ \parallel}^{\top}F_{\hat{G}\,\bar{\theta}}(S_{i}^{lkj},\ldots,R_{H}^{lkj})_{W}\]

A union bound over all \(m^{\prime}\in[m^{\prime}_{\max}]\), \(t,i\), and \(y\in C_{\xi}^{\times}\) guarantees with probability at least \(1-\zeta\), the above holds for all choice of these variables, any time beginning of any iteration (Line 5) is executed. Note that we need the union bound over \(m\) because the value of \(Q\) underlying the targets of least-squares estimations can potentially change between iterations.

To finish the proof, under this high-probability event, take any \(m,t,i,\hat{G},\text{and }\bar{\theta}\) as in the statement of this lemma, and choose \(y\in C_{\xi}^{\times}\) as before, to satisfy Eq. (37). Combined with Eq. (38), this immediately implies that the sequence \(\Delta\) formed of quantities with absolute value

\[\left|\begin{aligned} &\mathop{\mathbb{E}}_{\pi^{0},S_{t}^{lkj},A_{t}^{lkj}}v_{ \parallel}\bar{F}_{\hat{G}\,\bar{\theta}}(S_{i})w-\left\langle\varphi_{t}^{ lkj},\tilde{\theta}_{ti}\right\rangle\right|\\ &\leq\left|\mathop{\mathbb{E}}_{\pi^{0},S_{t}^{lkj},A_{t}^{lkj}}v _{\parallel}\bar{F}_{\hat{G}\,\bar{\theta}}(S_{i})w-a_{\parallel}\bar{F}_{ \hat{G}\,\bar{\theta}}(S_{i})b\right|+\left|a_{\parallel}\bar{F}_{\hat{G}\, \bar{\theta}}(S_{i})b-\left\langle\varphi_{t}^{lkj},\tilde{\theta}_{ti}\right\rangle \right|\\ &\leq\min\{\varepsilon/(dH^{2}\omega),1/\sqrt{m^{\prime}_{\max}nH }\}-\eta_{0}+\eta_{0}\end{aligned}\right.\] (40)

satisfies \(\left\|\Delta\right\|_{\infty}\leq\min\{\varepsilon/(dH^{2}\omega),1/\sqrt{m^{ \prime}_{\max}nH}\}\). Take any \((s,a)\in\mathcal{S}_{t}\times[\mathcal{A}]\), and let \(\tilde{\theta}_{ti}\) and \(\tilde{\theta}_{ti}\) be as above (in Eq. (39)) for \(v_{\parallel}\), \(w\), \(\tilde{G}\), and \(\bar{\theta}\). Note that

\[v_{\parallel}^{\top}\varphi(s,a)^{\top}\tilde{\theta}_{\hat{G}\,\bar{\theta}}^{ i}w=\left\langle\varphi(s,a),\tilde{\theta}_{ti}\right\rangle\,,\]By the triangle inequality, using Cauchy-Schwarz, and Eqs. (39) and (40),

\[\left|v_{\parallel}^{\top}\left(\varphi(s,a)^{\top}\tilde{\theta}_{G \,\tilde{\theta}}^{\prime\,i}-\mathop{\mathbb{E}}_{\pi^{0},s,a}\tilde{F}_{G\, \tilde{\theta}}(S_{i})\right)w\right|\] \[\quad\leq\left|\left\langle\varphi(s,a),\tilde{\theta}_{ii}- \tilde{\theta}_{ii}\right\rangle\right|+\mathop{\mathbb{E}}_{\pi^{0},s,a}v_{ \parallel}^{\top}\tilde{F}_{\tilde{G}\,\tilde{\theta}}(S_{i})w-\left\langle \left\langle\varphi(s,a),\tilde{\theta}_{ii}\right\rangle\right\rangle\right|\] \[\quad\leq\left\|\varphi(s,a)\right\|_{X_{m}^{-1}}\left(\sqrt{ \lambda}\left\|\tilde{\theta}_{ii}\right\|_{2}+\frac{\sqrt{|\mathbf{I}^{m}(t) |}}{\sqrt{m_{\max}^{\prime}nH}}+2H\sqrt{2\log\left(\frac{m_{\max}^{\prime}H^{2} |C_{\xi}^{\times}|}{\zeta}\right)+\log\left(\frac{\det X_{mt}}{\lambda^{d}} \right)}\right)+\frac{\varepsilon}{dH^{2}\omega}\] \[\quad\leq\left\|\varphi(s,a)\right\|_{X_{m}^{-1}}\left(2+2H\sqrt{ 2dH(d_{0}+1)\log\frac{12d_{0}HL_{2}}{\alpha\xi}}+2\log\frac{m_{\max}^{\prime}H ^{2}}{\zeta}+d\log\left(\lambda+m_{\max}^{\prime}nHL_{1}^{2}/d\right)\right)+ \frac{\varepsilon}{dH^{2}\omega}\] (41) \[\quad\leq\left\|\varphi(s,a)\right\|_{X_{m}^{-1}}\beta+\frac{ \varepsilon}{dH^{2}\omega}\,,\]

where in the fourth line we used that \(|\mathbf{I}^{m}(t)|\leq m_{\max}^{\prime}nH\), \(|C_{\xi}^{\times}|\leq(12d_{0}HL_{2}/(\alpha\xi))^{dH(d_{0}+1)}\), and we used the inequality of arithmetic and geometric means to bound \(\det X_{mt}\leq\left(\frac{1}{d}\operatorname{Tr}X_{mt}\right)^{d}\leq\left( \frac{\operatorname{Tr}\lambda H+|\mathbf{I}^{m}(t)|L_{1}^{2}}{d}\right)^{d}\). 

Proof of Lemma e.3.: Choose \(n\) to satisfy

\[n=\left[64\frac{(dH^{2}\omega)^{2}}{\varepsilon^{2}}H^{2}\left(2d\log\frac{18 dH^{3}}{\varepsilon}+\log\frac{2m_{\max}^{\prime}H^{2}}{\zeta}\right)\right]\,.\] (42)

This leads to \(n=\tilde{\mathcal{O}}(d^{5}H^{6}/\varepsilon^{2})\).

Similarly to the proof of Lemma e.2, we start with a covering argument. This time, as \(\tilde{G}\) and \(\tilde{\theta}\) are fixed, we only consider \(v\) and \(w\), to cover \(v_{\parallel}^{\top}\tilde{F}_{r^{\prime}}^{(j)}w\) and \(v_{\parallel}\tilde{F}_{r^{\prime}}^{(j)}w\). Let \(\xi^{\prime}=\frac{\varepsilon}{12dH^{3}}\). There is a set \(C_{\xi^{\prime}}^{+}\subset\mathcal{B}(1)\times\mathcal{B}(1)\) with \(|C_{\xi^{\prime}}|\leq(3/\xi^{\prime})^{2d}\) such that for all \(v,w\in\mathcal{B}(1)\), there exists an \((a,b)\in C_{\xi^{\prime}}^{+}\) with \(\left\|v-a\right\|_{2}\leq\xi^{\prime}\) (and therefore \(\left\|v_{\parallel}-a\right\|_{2}\leq\xi^{\prime}\)), and \(\left\|w-b\right\|_{2}\leq\xi^{\prime}\). Take such a choice of \((a,b)\) for any \((v,w)\). As \(E(\cdot)\in[-2H,2H]\) by Eq. (31), and \(\left\|\tilde{\varphi}_{\tilde{Q}}(\cdot)\right\|_{2}\leq 1\), For \(i\in[2:H]\) and any input,

\[\left|v_{\parallel}^{\top}F(s_{i^{\rightarrow}})w-a_{\parallel}^{\top}F(s_{i^{ \rightarrow}})b\right|\leq 6H\xi^{\prime}=\frac{\varepsilon}{2dH^{2}}\,,\]

and therefore for any \(s\in\mathcal{S}\setminus\mathcal{S}_{1}\), \(\left|v_{\parallel}^{\top}\tilde{F}(s)w-a_{\parallel}^{\top}\tilde{F}(s)b \right|\leq\varepsilon/(2dH^{2})\). For \(j\in[n]\) let

\[\tilde{F}_{j}^{ki}=\mathop{\mathbb{E}}_{\pi^{0},S_{p(k)}^{mkJ},A_{p(k)}^{mkJ}}F _{\tilde{G}\,\tilde{\theta}}(S_{i}^{mkj},\ldots,R_{H}^{mkj})=\mathop{\mathbb{ E}}_{\pi^{0},S_{p(k)}^{mkJ},A_{p(k)}^{mkJ}}\tilde{F}_{\tilde{G}\,\tilde{\theta}}(S_{i}^{ mkj})\]

By the triangle inequality, for any \(k\in[H-1]\), \(i\in[k+1:H]\),

\[\left|v_{\parallel}^{\top}\left(v_{\tilde{G}\,\tilde{\theta}}^{ki }-\tilde{F}_{\tilde{G}\,\tilde{\theta}}^{ki}\right)w\right|\] \[\quad\leq\left|\frac{1}{n}\sum_{j\in[n]}c_{ki}^{j}v^{\top}\left( \varphi_{\mathrm{p}(k)}^{mkj}\tilde{\sigma}_{\tilde{G}\,\tilde{\theta}}^{p(mkj),i}-\tilde{F}_{j}^{ki}\right)w\right|+\left|\frac{1}{n}\sum_{j\in[n]}c_{ki}^{j} v^{\top}\left(\tilde{F}_{j}^{ki}-F_{\tilde{G}\,\tilde{\theta}}(S_{i}^{ mkj},\ldots,R_{H}^{mkj})\right)w\right|\] \[\quad\leq\frac{1}{n}\sum_{j\in[n]}c_{ki}^{j}\left\|\varphi_{ \mathrm{p}(k)}^{mkj}\right\|_{X_{m,\pi(mk)}^{-1}}\beta+\frac{\varepsilon}{dH^{2} \omega}+\frac{\varepsilon}{dH^{2}\omega}+\left|\frac{1}{n}\sum_{j\in[n]}c_{ki}^ {j}a^{\top}\left(\tilde{F}_{j}^{ki}-F_{\tilde{G}\,\tilde{\theta}}(S_{i}^{mkj}, \ldots,R_{H}^{mkj})\right)b\right|\,,\] (43)

where the second inequality uses Lemma e.2 and applies the triangle inequality twice again. Observe that for all \(j\in[n]\), given all the history of SkippyEleanor interacting with the MDPup to (and including) \(S^{mkj}_{\mathrm{p}(k)}\), \(A^{mkj}_{\mathrm{p}(k)}\) (which also includes the value of \(c^{j}_{ki}\) for \(i\in[H+1]\)), the trajectory \(S^{mkj}_{\mathrm{p}(k)+1}\), \(A^{mkj}_{\mathrm{p}(k)+1},\cdots,R^{mkj}_{H}\) is an independent rollout with policy \(\pi^{0}\), with its law given by \(\mathcal{P}_{\pi^{0},S^{mkj}_{\mathrm{p}(k)},s^{mkj}_{\mathrm{p}(k)}}\). Therefore, for any fixed \((a,b)\in C^{+}_{\xi^{\prime}}\), \(c^{j}_{ki}a^{\top}\left(\tilde{F}^{ki}_{j}-F_{\tilde{O}\tilde{\theta}}(S^{mkj}_ {i},\ldots,R^{mkj}_{H})\right)b\) are independent zero-mean random variables with range \([-4H,4H]\). Applying Hoeffding's inequality with a union bound over \(m^{\prime},k,i,a\), and \(b\), with probability at least \(1-\zeta\), for any of the \(m^{\prime}\in[m^{\prime}_{\mathrm{max}}]\) times the beginning of the iteration (Line 5) is executed (this includes the entire execution of Skip-pyEleanor),

\[\left|\frac{1}{n}\sum_{f\in[n]}c^{j}_{ki}a^{\top}\left(\tilde{F} ^{ki}_{j}-F_{\tilde{O}\tilde{\theta}}(S^{mkj}_{i},\ldots,R^{mkj}_{H})\right)b \right| \leq\frac{8H}{\sqrt{n}}\sqrt{\log\frac{2m^{\prime}_{\mathrm{max} }H^{2}|C^{+}_{\xi^{\prime}}|}{\zeta}}\] \[=\frac{8H}{\sqrt{n}}\sqrt{2d\log\frac{18dH^{3}}{\varepsilon}+\log \frac{2m^{\prime}_{\mathrm{max}}H^{2}}{\zeta}}\leq\frac{\varepsilon}{dH^{2} \omega}\,,\]

where we used Eq. (42). To finish, note that unless \(c^{j}_{ki}=0\), \(\left\|\varphi^{mkj}_{\mathrm{p}(k)}\right\|_{X^{m,\mathrm{p}(mk)}_{m,\mathrm{ p}(mk)}}<2(\beta\omega dH)^{-1}\), so we can continue from Eq. (43) by bounding the average feature-norm by \(\tilde{\sigma}^{m}_{k}\) as

\[\left|v^{\top}_{\mathbb{I}}\left(v^{ki}_{\tilde{O}\theta}-\tilde{F}^{ki}_{ \tilde{O}\theta}\right)w\right|\leq\tilde{\sigma}^{m}_{k}\beta+3\frac{ \varepsilon}{dH^{2}\omega}\,.\qed\]

Proof of Lemma e.4.: Recall that \((k,i,v)\) are the arguments and \(x\) the value of Optimization Problem 4.12. Throughout the proof we write \(Q\) to refer to its value _just before_ Line 13 is executed. We write \(\bullet_{\parallel}\) for \(\bullet_{\parallel(Q,i)}\), and \(\bullet_{\perp}\) for \(\bullet_{\perp(Q,i)}\). Let \(\mathbf{M}=y^{ki}_{\tilde{O}\tilde{\theta}}-\tilde{F}^{ki}_{\tilde{O}\tilde{ \theta}}\). Therefore, \(v^{\top}\mathbf{M}v=x>\tilde{\sigma}^{m}_{k}\beta\omega+3\frac{\varepsilon}{ dH^{2}}\), and by Lemma E.1, \(w^{\top}\mathbf{M}w>\tilde{\sigma}^{m}_{k}\beta\omega+2\frac{\varepsilon}{dH^{2}}\).

Line 13 changes \(Q_{i}\) by appending \(Q_{i}^{-1}w\) to the sequence \(C_{i}\) of vectors from which \(Q\) is calculated according to Eq. (4). Eq. (5) lists the conditions on the new sequence \(C_{i}\) that need to be satisfied for \(Q\) to stay a valid preconditioning. Consider the third condition, i.e., \(\left\|Q_{i}^{-1}w\right\|_{2}\leq L_{3}\). Observe that \(Q_{i}^{-1}\operatorname{Proj}_{Z(Q,i)}\leq L_{3}^{2}f\) and \(\left\|v\right\|_{2}=1\), therefore \(\left\|Q_{i}^{-1}w\right\|_{2}=\left\|Q_{i}^{-1}\operatorname{Proj}_{Z(Q,i)} v\right\|_{2}\leq L_{3}\).

Now consider the second condition. To prove that it holds, we need to show that \(\left\|Q_{i}Q_{i}^{-1}w\right\|_{2}=\left\|w\right\|_{2}\geq\frac{1}{2}\). Let \(x=\left\|w\right\|_{2}^{-1}\). Since \(v\) was the argument of the optimization problem, and using Lemma E.1,

\[x^{2}w^{\top}\mathbf{M}w\leq v^{\top}\mathbf{M}v\leq w^{\top}\mathbf{M}w+ \frac{\varepsilon}{dH^{2}\omega}\leq w^{\top}\mathbf{M}w(1+1/2)\]

Therefore, \(\left\|w\right\|_{2}^{2}\geq\frac{2}{3}\). We immediately get that

\[\left\|Q_{i}Q_{i}^{-1}w\right\|_{2}^{2}\geq\frac{2}{3}\,,\]

satisfying the second condition.

It remains to prove that the first condition also holds. First, noting that \(\mathbf{M}\) is symmetric, we can decompose \(w^{\top}\mathbf{M}w\) as

\[w^{\top}\mathbf{M}w=w^{\top}_{\parallel}\mathbf{M}w+w^{\top}_{\parallel} \mathbf{M}w_{\perp}+w^{\top}_{\perp}\mathbf{M}w_{\perp}\,.\]

Applying Lemma E.3 on the first two terms,

\[w^{\top}\mathbf{M}w\leq 2\tilde{\sigma}^{m}_{k}\beta+6\frac{\varepsilon}{dH^{2} \omega}+w^{\top}_{\perp}\mathbf{M}w_{\perp}\,.\]

Due to \(\omega>3\) and \(w^{\top}\mathbf{M}w>\tilde{\sigma}^{m}_{k}\beta\omega+2\frac{\varepsilon}{dH^{2}}\) and the above, \(w_{\perp}\neq\mathbf{0}\). Let \(w^{\prime}=w_{\perp}/\left\|w_{\perp}\right\|_{2}\). Since \(v\) was the argument of the optimization problem, have that \(v^{\top}\mathbf{M}v\geq w^{\prime\top}\mathbf{M}w^{\prime}\). Putting this together,

\[\left\|w_{\perp}\right\|_{2}^{-2}w^{\top}_{\perp}\mathbf{M}w_{\perp}=w^{\prime \top}\mathbf{M}w^{\prime}\leq v^{\top}\mathbf{M}v\leq w^{\top}\mathbf{M}w+ \frac{\varepsilon}{dH^{2}\omega}\leq 2\tilde{\sigma}^{m}_{k}\beta+7\frac{ \varepsilon}{dH^{2}\omega}+w^{\top}_{\perp}\mathbf{M}w_{\perp}\,,\]Since \(v^{\top}\mathbf{M}v>\bar{\sigma}_{k}^{m}\beta\omega+3\frac{e}{dH^{2}}\), \(w_{\perp}^{\top}\mathbf{M}w_{\perp}\geq(\omega-7/3)\left(\bar{\sigma}_{k}^{m} \beta+3\frac{e}{dH^{2}\omega}\right)>0\) and therefore dividing the above by \(w_{\perp}^{\top}\mathbf{M}w_{\perp}\),

\[\|w_{\perp}\|_{2}^{-2} \leq\frac{7/3}{\omega-7/3}+1\] \[\|w_{\perp}\|_{2}^{2} \geq\frac{1}{1+c}\qquad\text{for }c=\frac{7/3}{\omega-7/3}\] \[\left\|w_{\parallel}\right\|_{2}^{2} \leq 1-\frac{1}{1+c}\qquad\text{as }\left\|w_{\parallel}\right\|_{2}\leq 1\,.\]

Now to prove that the first condition also holds,

\[\sup_{\theta\in\Theta_{t}}\left|\left\langle\theta,Q_{t}^{-1}w \right\rangle\right| =\sup_{\theta\in\Theta_{t}^{Q}}|\left\langle\theta,w\right\rangle |\leq\sup_{\theta\in\Theta_{t}^{Q}}\|\theta\|_{2}\left\|w_{\parallel}\right\|_ {2}+\sup_{\theta\in\Theta_{t}^{Q}}|\left\langle\theta,w_{\perp}\right\rangle|\] \[\leq\sqrt{d_{1}+1}\sqrt{1-\frac{1}{1+c}}+\sup_{\theta\in\Theta_{t }^{Q}}\|\theta\|_{V(G_{h}^{Q}\varphi_{h}^{Q})^{\top}}\|w_{\perp}\|_{V(G_{h}^{Q} \varphi_{h}^{Q})}\] \[\leq\sqrt{d_{1}+1}\sqrt{1-\frac{1}{1+c}}+\sqrt{2dw_{\perp}^{\top} (\gamma I)w_{\perp}}\] \[\leq\sqrt{d_{1}+1}\sqrt{1-\frac{1}{1+c}}+\sqrt{2d\gamma}=\sqrt{d_ {1}+1}\sqrt{1-\frac{1}{1+c}}+\frac{1}{2}\,,\]

where in the second line we used Lemma 4.3 to bound \(\sup_{\theta\in\Theta^{Q}}\|\theta\|_{2}\), and for the second term we used Eq. (26) with Cauchy-Schwarz. In the third line we used Eq. (24), and the definition of \(\text{Proj}_{\perp}\). Finally in the last line we use that \(w_{\perp}\) is perpendicular to \(a_{i}\) for \(i\leq d^{\prime}\) (by definition) and that \(\lambda_{i}\leq\gamma\) for \(i>d^{\prime}\). It is left to prove that \(\sqrt{d_{1}+1}\sqrt{1-\frac{1}{1+c}}\leq\frac{1}{2}\). This holds if \(c\geq 1/(4(d_{1}+1)-1)\), which is satisfied as \(c=1/(3(d_{1}+1))\), due to \(\omega=7(d_{1}+1)+7/3\) (Eq. (13)). 

## Appendix I Deferred proofs for Appendix E.2

Proof of Lemma e.5.: The features \(\varphi_{\text{p}(k)}^{lkj}\) are observed by SkippyEleanor in the order of increasing \(l\), within that increasing \(k\), and within that, increasing \(j\). Each time the next \(\varphi_{\text{p}(k)}^{lkj}\) is observed, we sum the elliptic potential as follows.

For \(i\in[m],r\in[H],u\in[n],t\in[H]\), let the set of indices observed before \(\varphi_{\text{p}(r)}^{triu}\) whose Phase II (rollout phase) starts at some stage \(t\) be:

\[\mathbf{I}^{tri}(t)=\{l\in[i],k\in[H],j\in[n]\,:\,lHn+kn+j<iHn+rn+u\text{ and }\text{p}(lkj)=t\}\]

Let a version of this where only the whole iteration \(i\)'s data is included be

\[\mathbf{J}^{i}(t)=\{l=i,k\in[H],j\in[n]\,:\,\text{p}(lkj)=t\}\]

Let

\[X_{tri}(t)=\lambda I+\sum_{lkj\in\mathbf{I}^{triu}(t)}\varphi_{\text{p}(k)}^ {lkj}\varphi_{\text{p}(k)}^{lkj\,\top}\]

Observe that \(X_{it}\), defined in Optimization Problem 4.10, is the version of this that only updates at the start of each iteration \(i\), that is,

\[X_{it}=X_{i11}(t)\,.\]

The total elliptic potential, observed by the end of iteration \(m\) is, writing \(k=\text{p}(iru)\) on the left hand side:

\[\sum_{i\in[m],r\in[H],u\in[n]}\mathbb{1}\left\{k<H+1\right\}\min\left\{1,\left\| \varphi_{k}^{iru}\right\|_{X_{iru}(k)^{-1}}^{2}\right\}=\sum_{i\in[m],t\in[H] }\sum_{lkj\in\mathbf{I}^{\prime}(t)}\min\left\{1,\left\|\varphi_{r}^{lkj} \right\|_{X_{ikj}(t)^{-1}}^{2}\right\}\,.\]

Applying the elliptical potential lemma (Lemma L.1) \(H\) times for \(t\in[H]\), this can be bounded as

\[\sum_{t\in[H],i\in[m]}\sum_{lkj\in\mathbf{I}^{\prime}(t)}\min\left\{1,\left\| \varphi_{t}^{lkj}\right\|_{X_{ikj}(t)^{-1}}^{2}\right\}\leq 2dH\log\left(1+\frac{ HmnL_{1}^{2}}{d\lambda}\right)\]On the other hand, by Lemma L.2, then switching to an \(\ell_{1}\)-bound, then observing that by definition, \(\sum\tilde{\sigma}_{k}^{i}\) sums the same quantities but caps them by some threshold,

\[\sum_{t\in[H],t\in[m]}\sum_{lk_{j}\in\mathbf{J}^{\prime}(t)}\min \left\{1,\left\|\tilde{\varphi}_{t}^{lk_{j}}\right\|_{X_{lk_{j}}(t)^{-1}}^{2}\right\} \geq\sum_{t\in[H],t\in[m]}\min\left\{1,\frac{1}{2}\sum_{lk_{j}\in \mathbf{J}^{\prime}(t)}\left\|\tilde{\varphi}_{t}^{lk_{j}}\right\|_{X_{li}^{-1 }}^{2}\right\}\] \[\geq\sum_{i\in[m]}\min\left\{1,\frac{1}{2}\sum_{t\in[H]}\sum_{lk_{ j}\in\mathbf{J}^{\prime}(t)}\left\|\tilde{\varphi}_{t}^{lk_{j}}\right\|_{X_{li}^{-1 }}^{2}\right\}\] \[\geq\sum_{i\in[m]}\min\left\{1,\frac{1}{2Hn}\left(\sum_{t\in[H]} \sum_{lk_{j}\in\mathbf{J}^{\prime}(t)}\left\|\tilde{\varphi}_{t}^{lk_{j}} \right\|_{X_{li}^{-1}}\right)^{2}\right\}\] \[\geq\sum_{i\in[m]}\min\left\{1,\frac{1}{2Hn}\left(n\sum_{k\in[H] }\tilde{\sigma}_{k}^{i}\right)^{2}\right\}\]

Whenever an iteration finishes without returning in Line 17, \(\sum_{k\in[H]}\tilde{\sigma}_{k}^{m}>\varepsilon/(dH^{2}\beta\omega)\). Therefore,

\[2dH\log\left(1+\frac{HmnL_{1}^{2}}{d\lambda}\right) \geq\sum_{i\in[m]}\min\left\{1,\frac{1}{2Hn}\left(n\sum_{k\in[H]} \tilde{\sigma}_{k}^{i}\right)^{2}\right\}\] \[\geq\sum_{i\in[m]}\min\left\{1,\frac{1}{2H}n\left(\frac{ \varepsilon}{dH^{2}\beta\omega}\right)^{2}\right\}\] \[\geq\sum_{i\in[m]}\min\left\{1,Hd/\beta^{2}\right\}=mHd/\beta^{2}\,,\]

Therefore, even for the iteration that returns in Line 17,

\[m\leq\beta^{2}\log\left(1+\frac{HmnL_{1}^{2}}{d\lambda}\right)+1=m_{\max}\,.\qed\]

## Appendix J Deferred proofs for Appendix E.3

Proof of Lemma e.8.: For notational simplicity we drop the subscripts \((\hat{G},\tilde{\theta})\). We first use the usual high-probability bounds on the least squares predictor and Hoeffding's inequality on the empirical mean quantities, to prove that with probability at least \(1-3\zeta\), during the execution of SkippyLeanor whenever Line 16 is executed, for all \(k\in[H]\),

\[\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k,H+1}C(S_{p(k)}) \leq\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\sum_{u=p(k)}^{H}R_{u}+\tilde{c}_{k,H +1}E^{\rightarrow}(S_{p(k)+1},\ldots,R_{H})+2\tilde{\sigma}_{k}^{m}\beta \omega dH+4\frac{\varepsilon}{H}\,.\] (44)

The proof of this is presented as Lemma J.1.

Next, to prove the statement for \(k\in[H]\), assume by induction that Eq. (22) holds for \(i\in[k+1:H]\). Observe that SkippyPolicy performs a rollout with policy \(\pi^{0}\) for the rest of the episode starting from stage \(p(k)+1\), that is, \(1=A_{p(k)+1}=\cdots=A_{H}\). Therefore, the law of the random variables \(S_{p(k)+1},\ldots,R_{H}\), given \((S_{p(k)},A_{p(k)})\) is fully determined by the dynamics of the MDP, and is independent of the values of \(p(k+1),\ldots,p(H)\). Therefore,

\[\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k+1,H+1}C(S_{p(k+1) }) =\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k+1,H+1}D(S_{p(k+1) },\ldots,R_{H})+\sum_{u=p(k+1)}^{H}R_{u}\] (45) \[=\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k,H+1}E^{ \rightarrow}(S_{p(k)+1},\ldots,R_{H})+\sum_{u=p(k+1)}^{H}R_{u}\,,\]where we use Eq. (9), and that \(\pi^{mk}\) (SkippyPolicy) is in phase II after stage \(p(k)\), but defines the the mapping \(p(\cdot)\) independently of whether the policy is in phase I or phase II, in such a way that for any \(H\geq j>p(k)\),

\[\begin{split}\mathcal{P}_{\pi^{mk},s_{1}}\left[p(k+1)=j\,\big{|} \,p(k),S_{p(k)},A_{p(k)}\right]=\mathcal{P}_{\pi^{mk},s_{1}}\left[\tau(S_{j}) \prod_{j^{\prime}=p(k)+1}^{j-1}(1-\tau(S_{j^{\prime}}))\,\big{|}\,p(k),S_{p(k) },A_{p(k)}\right]\,.\end{split}\]

Combining Eq. (45) with Eq. (44),

\[\begin{split}\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k,H+ 1}C(S_{p(k)})\leq\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\sum_{u=p(k)}^{p(k+1)-1}R _{u}+\tilde{c}_{k+1,H+1}C(S_{p(k+1)})+2\tilde{\sigma}_{k}^{m}\beta\omega dH+4 \frac{\varepsilon}{H}\,.\end{split}\]

By Remark E.7, \(\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k+1,H+1}C(S_{p(k+1)})=\mathop {\mathbb{E}}_{\pi^{m,k+1},s_{1}}\tilde{c}_{k+1,H+1}C(S_{p(k+1)})=\tilde{C}^{k +1}\,.\) Therefore, combining with the inductive hypothesis,

\[\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k,H+1}C(S_{p(k)}) \leq\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\sum_{u=p(k)}^{p(k+1)-1}R _{u}+\tilde{C}^{k+1}+2\tilde{\sigma}_{k}^{m}\beta\omega dH+4\frac{\varepsilon }{H}\] \[\leq\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\sum_{u=p(k)}^{p(k+1)-1}R _{u}+\mathop{\mathbb{E}}_{\pi^{mH},s_{1}}\sum_{u=p(k+1)}^{H}R_{u}+2\sum_{i=k} ^{H}\bar{\sigma}_{k}^{m}\beta\omega dH+4(H-k+1)\frac{\varepsilon}{H}\] \[=\mathop{\mathbb{E}}_{\pi^{mH},s_{1}}\sum_{u=p(k)}^{H}R_{u}+2 \sum_{i=k}^{H}\bar{\sigma}_{k}^{m}\beta\omega dH+4(H-k+1)\frac{\varepsilon}{H}\]

where the last equation uses Remark E.7 again, finishing the induction. 

**Lemma J.1**.: _Adopt the notation of Lemma E.8. With probability at least \(1-3\zeta\), during the execution of SkippyEleanor, whenever Line 16 is executed, for all \(k\in[H]\),_

\[\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k,H+1}C(S_{p(k)})\leq\mathop{ \mathbb{E}}_{\pi^{mk},s_{1}}\sum_{u=p(k)}^{H}R_{u}+\tilde{c}_{k,H+1}E^{\to}( S_{p(k)+1},\ldots,R_{H})+2\tilde{\sigma}_{k}^{m}\beta\omega dH+4\frac{ \varepsilon}{H}\,.\]

Proof.: We refer as \(\hat{\theta}\) to the value of the argument of Optimization Problem 4.10 recorded in Line 5. For \(k\in[H]\), recall the definition of \(\bar{\sigma}_{k}^{m}\) (Eq. (18)), along with the fact that unless \(c_{k,H+1}^{j}=0\), \(\left\|\varphi_{p(k)}^{mkj}\right\|_{X_{m,g(mk,i)}^{-1}}<2(\beta\omega dH)^{-1}\), we get a useful bound on the average norm of the features under consideration:

\[\frac{1}{n}\sum_{j\in[n]}c_{k,H+1}^{j}\left\|\varphi_{p(k)}^{mkj}\right\|_{X_{ m,g(mk,j)}^{-1}}\leq\bar{\sigma}_{k}^{m}\,.\] (46)

If Line 16 is executed, the consistency check passed, and therefore for all \(k\in[H-1],i\in[k+1:H]\),

\[\text{Tr}\left(y^{ki}-\hat{F}^{ki}\right)\leq\bar{\sigma}_{k}^{m}\beta\omega d +3\frac{\varepsilon}{H^{2}}\] (47)

For \(t\in[H]\) let the least-squares predictor of rewards sums under the policy \(\pi^{0}\) be

\[\bar{\vartheta}^{t,H+1}=X_{mt}^{-1}\sum_{lkj\in\mathbb{I}^{m}(t)}\varphi_{t}^{ lkj}\sum_{u=t}^{H}R_{u}^{lkj}\,.\]

For \(k\in[H]\) and \(j\in[n]\) let us introduce the shorthand

\[R_{k\to}^{mkj}=\sum_{u=p(mkj)}^{H}R_{u}^{mkj}\,,\]and similarly when the trajectory is clear from context: \(R_{k\to}=\sum_{u=p(k)}^{H}R_{u}\). For \(k\in[H]\) let

\[\hat{E}^{k} =\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left(E^{\to}(S^{mkj}_{\mathrm{ p}(k)+1},\ldots,R^{mkj}_{H})+R^{mkj}_{k\to}\right)\] \[\hat{C}^{k} =\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}C(S^{mkj}_{\mathrm{p}(k)})\] \[y^{k,H+1} =\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left\langle\varphi^{mkj}_{ \mathrm{p}(k)},\tilde{\theta}^{\mathrm{p}(mkj),H+1}\right\rangle\] \[z^{k,H+1} =\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}R^{mkj}_{k\to}\]

For \(t\in[H-1]\), \(i\in[t+1:H]\), along with \(\tilde{\theta}^{t,H+1}\), let

\[\tilde{\theta}^{tt}=X_{mt}^{-1}\sum_{lkj\in\mathbb{I}^{m}(t)}\varphi^{kj}_{t} \operatorname{Tr}(F(S^{kj}_{i},\ldots,R^{lkj}_{H}))=X_{mt}^{-1}\sum_{lkj\in \mathbb{I}^{m}(t)}\varphi^{lkj}_{t}E(S^{jk}_{i},\ldots,R^{lkj}_{H})\,,\]

where the second equality is by Eq. (32). Observe that for any \(v\in\mathbb{R}^{d}\), \(\operatorname{Tr}(v^{\top}\tilde{\theta}^{tt})=\left\langle v,\tilde{\theta}^ {tt}\right\rangle\). Therefore, for \(k\in[H]\),

\[y^{k,H+1}\!\!+\!\sum_{i=k+1}^{H}\operatorname{Tr}(y^{ki})=\frac{1}{n}\sum_{j\in[ n]}\sum_{i=k+1}^{H+1}c^{j}_{ki}\left\langle\varphi^{mkj}_{\mathrm{p}(k)}, \tilde{\theta}^{\mathrm{p}(mkj),i}\right\rangle=\frac{1}{n}\sum_{j\in[n]}c^{j }_{k,H+1}\left\langle\varphi^{mkj}_{\mathrm{p}(k)},\sum_{i=\mathrm{p}(mkj)+1} \tilde{\theta}^{\mathrm{p}(mkj),i}\right\rangle\]

For any \(t\in[H]\), by the definitions,

\[\sum_{i=t+1}^{H+1}\tilde{\theta}^{tt} =X_{mt}^{-1}\sum_{lkj\in\mathbb{I}^{m}(t)}\varphi^{lkj}_{t}\left( \sum_{i=t+1}^{H}E(S^{lkj}_{i},\ldots,R^{lkj}_{H})+\sum_{u=t}^{H}R^{mkj}_{u}\right)\] \[=X_{mt}^{-1}\sum_{lkj\in\mathbb{I}^{m}(t)}\varphi^{kj}_{t}\left(E^ {-}(S^{lkj}_{t+1},\ldots,R^{lkj}_{H})+\sum_{u=t}^{H}R^{mkj}_{u}\right)=\theta_ {t}\]

Plugging this into the previous calculation,

\[y^{k,H+1}+\sum_{i=k+1}^{H}\operatorname{Tr}(y^{ki}) =\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left\langle\varphi^{mkj}_{ \mathrm{p}(k)},\hat{\theta}_{\mathrm{p}(mkj)}\right\rangle\] (48) \[\geq\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left\langle\varphi^{ mkj}_{\mathrm{p}(k)},\bar{\theta}_{\mathrm{p}(mkj)}\right\rangle\] \[\geq\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left\|\varphi^{mkj}_{ \mathrm{p}(k)}\right\|_{X_{m,p(mkj)}^{-1}}\left\|\tilde{\theta}_{\mathrm{p}( mkj)}-\hat{\theta}_{\mathrm{p}(mkj)}\right\|_{X_{m,p(mkj)}}\] \[\geq\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left\langle\varphi^{ mkj}_{\mathrm{p}(k)},\bar{\theta}_{\mathrm{p}(mkj)}\right\rangle-\bar{\sigma}^{m}_{k}\beta H\] \[\geq\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\operatorname{clip}_{ \mathrm{p}(0,H]}\left\langle\varphi^{mkj}_{\mathrm{p}(k)},\bar{\theta}_{ \mathrm{p}(mkj)}\right\rangle-\bar{\sigma}^{m}_{k}\beta H\] \[=\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}C(S^{mkj}_{\mathrm{p}(k)})- \bar{\sigma}^{m}_{k}\beta H=\hat{C}^{k}-\bar{\sigma}^{m}_{k}\beta H\,,\]

where the first inequality uses Cauchy-Schwarz. The second inequality bounds the average of the first norm by Eq. (46), and the bound on the second norm (for any \(j\)) is by definition of Optimization Problem 4.10. The third inequality relies on the fact that \(c^{j}_{k,H+1}=0\) if the clipped inner product is negative, and the final equality is due to the definition of \(C\) along with the fact that \(A^{mkj}_{\mathrm{p}(k)}=\pi^{+}(S^{mkj}_{\mathrm{p}(k)})\), as this is the last state in the trajectory where SkippyPolicy takes the inner-product maximizing action (\(\pi^{+}\)) before rolling out with \(\pi^{0}\).

By Eqs. (12) and (32), we have that

\[\begin{split} z^{k,H+1}+\sum_{i\in[k+1:H]}\operatorname{Tr}(\hat{F}^ {ki})&=\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left(\sum_{i= \operatorname{p}(mkj)+1}^{H+1}E(S^{mkj}_{i},\ldots,R^{mkj}_{H})+R^{mkj}_{k \to}\right)\\ &=\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left(E^{\to}(S^{mkj}_{ \operatorname{p}(k)+1},\ldots,R^{mkj}_{H})+R^{mkj}_{k\to}\right)=\hat{E}^{k} \end{split}\] (49)

Combining Eqs. (48) and (49),

\[\begin{split}\hat{C}^{k}-\hat{E}^{k}&\leq\bar{ \sigma}^{m}_{k}\beta H+\left(y^{k,H+1}-z^{k,H+1}\right)+\sum_{i\in[k+1:H]} \operatorname{Tr}(y^{ki}-\hat{F}^{ki})\\ &\leq\bar{\sigma}^{m}_{k}\beta H+\left(\left|\operatorname*{ \mathbb{E}}_{\pi^{mk},s_{1}}c_{k,H+1}R_{k\to}-z^{k,H+1}\right|+\left|y^{k,H+1} -\operatorname*{\mathbb{E}}_{\pi^{mk},s_{1}}c_{k,H+1}R_{k\to}\right|\right)+ \bar{\sigma}^{m}_{k}\beta\omega dH+3\frac{\varepsilon}{H}\end{split}\] (50)

where the sum (last term) is bounded by Eq. (47), and we apply a triangle inequality on the second term. To continue bounding this term, we apply Hoeffding's inequality on the independent random variables \(c^{j}_{k,H+1}R_{k\to}\) (for \(j\in[n]\)) that have range \([0,H]\), along with a union bound over the iteration \(m^{\prime}\in[m^{\prime}_{\max}]\) and \(k\in[H]\), to get that with probability at least \(1-\zeta\),

\[\left|\operatorname*{\mathbb{E}}_{\pi^{mk},s_{1}}c_{k,H+1}R_{k\to}-z^{k,H+1} \right|\leq\frac{H}{\sqrt{n}}\sqrt{\log\frac{2m^{\prime}_{\max}H}{\zeta}}\leq \frac{\varepsilon}{dH^{2}\omega}\,.\] (51)

The remaining term \(\left|y^{k,H+1}-\operatorname*{\mathbb{E}}_{\pi^{mk},s_{1}}c_{k,H+1}R_{k\to}\right|\) is bounded using the realizability of \(q^{\pi^{0}}\) (Definition 3.2) as follows. Take any \(t\in[H]\). By definition there exists \(\theta^{\star}_{t}\in\Theta^{Q}_{t}\subseteq\mathcal{B}(L_{2})\), such that for all \(s\in\mathcal{S}_{t}\) and \(a\in[\mathcal{A}]\), \(q^{\pi^{0}}(s,a)\approx_{\eta}\left\langle\varphi(s,a),\theta^{\star}_{t}\right\rangle\). Take the sequence \(A\) formed of \(\varphi^{lkj}_{t}\) (for \(lkj\in\mathbf{I}^{m}(t)\), in the order that these random variables are observed), and the sequence \(X\) formed of \(R^{mkj}_{k\to}\) (for \(lkj\in\mathbf{I}^{m}(t)\), in the same order), and the sequence \(\Delta\) formed of \(q^{\pi^{0}}(S^{lkj}_{t},A^{lkj}_{t})-\left\langle\varphi^{lkj}_{t},\theta^{ \star}_{t}\right\rangle\) (for \(lkj\in\mathbf{I}^{m}(t)\), in the same order). Then the sequences \(A\), \(X\), and \(\Delta\) satisfy the conditions of Lemma M.4 with a subgaussianity parameter \(\sigma=H\). Due to this lemma, applied with a union bound over \(m^{\prime}\in[m^{\prime}_{\max}]\) and \(t\in[H]\), with probability at least \(1-\zeta\),

\[\left|\left|\hat{\theta}^{t,H+1}-\theta^{\star}_{t}\right|\right| _{X_{mt}} <\sqrt{\lambda}\left|\theta^{\star}_{t}\right|\right|_{2}+\left| \Delta\right|_{\infty}\sqrt{|\mathbf{I}^{m}(t)|}+H\sqrt{2\log\left(\frac{m^{ \prime}_{\max}H}{\zeta}\right)+\log\left(\frac{\det X_{mt}}{\lambda^{d}} \right)}\] \[\leq 2+H\sqrt{2\log\frac{m^{\prime}_{\max}H}{\zeta}+\log\left( \frac{\det X_{mt}}{\lambda^{d}}\right)}\leq\beta\,,\]

by Eq. (41). Therefore by Cauchy-Schwarz and Eq. (46),

\[\left|y^{k,H+1}-\operatorname*{\mathbb{E}}_{\pi^{mk},s_{1}}c_{k,H+1 }R_{k\to}\right| \leq\frac{1}{n}\sum_{j\in[n]}c^{j}_{k,H+1}\left(\left\|\varphi^{ mkj}_{\operatorname{p}(k)}\right\|_{X^{-1}_{m,\operatorname{p}(mkj)}}\left\| \tilde{\theta}^{\operatorname{p}(mkj),H+1}-\theta^{\star}_{\operatorname{p}( mkj)}\right\|_{X_{mt}}+\eta\right)\] \[\leq\bar{\sigma}^{m}_{k}\beta+\eta\leq\bar{\sigma}^{m}_{k}\beta+ \frac{\varepsilon}{dH^{2}\omega}\,.\]

Combining this with Eqs. (50) and (51),

\[\hat{C}^{k}-\hat{E}^{k}\leq 1.5\bar{\sigma}^{m}_{k}\beta\omega dH+3\frac{ \varepsilon}{H}+2\frac{\varepsilon}{dH^{2}\omega}\,.\] (52)

We introduce the following notation for \(j\in[n]\), \(k\in[H+1]\), \(i\in[H+1]\):

\[\begin{split}\tilde{c}^{j}_{ki}&=\mathbbm{1}\left\{ \operatorname{p}(mkj)<i\text{ and }\left\|\varphi^{mkj}_{\operatorname{p}(k)}\right\|_{X^{-1}_{m, \operatorname{p}(mkj)}}\geq 2(\beta\omega dH)^{-1}\text{ and }\left\langle\varphi^{mkj}_{ \operatorname{p}(k)},\bar{\theta}_{\operatorname{p}(mkj)}\right\rangle\geq 0 \right\}\\ \hat{c}^{j}_{ki}&=\mathbbm{1}\left\{ \operatorname{p}(mkj)<i\text{ and }\left\langle\varphi^{mkj}_{\operatorname{p}(k)},\bar{\theta}_{ \operatorname{p}(mkj)}\right\rangle<0\right\}\,,\end{split}\]such that for all \(j\),

\[\tilde{c}^{j}_{ki}=c^{j}_{ki}+\tilde{c}^{j}_{ki}+\tilde{c}^{j}_{ki}\,.\] (53)

Continuing from Eq. (52), as \(E^{\rightarrow}(s_{i}\rightarrow)+\sum_{u=i}^{H}r_{u}\geq 0\) by Eq. (31), and if \(\tilde{c}^{j}_{k,H+1}=1\) then \(C(S^{mkj}_{\mathrm{p}(k)})=0\), we have that

\[\frac{1}{n}\sum_{j\in[n]}\left(c^{j}_{k,H+1}+\tilde{c}^{j}_{k,H+1}\right) \left(C(S^{mkj}_{\mathrm{p}(k)})-\left(E^{\rightarrow}(S^{mkj}_{\mathrm{p}(k)+1 },\ldots,R^{mkj}_{H})+R^{mkj}_{k\rightarrow}\right)\right)\leq 1.5\tilde{\sigma} ^{m}_{k}\beta\omega dH+3\frac{\varepsilon}{H}+2\frac{\varepsilon}{dH^{2} \omega}\,.\]

As (even if \(\tilde{c}^{j}_{k,H+1}=1\)) \(C(S^{mkj}_{\mathrm{p}(k)})\leq H\),

\[\frac{1}{n}\sum_{j\in[n]}\tilde{c}^{j}_{k,H+1}C(S_{p(k)})\leq H\tilde{\sigma} ^{m}_{k}/(2(\beta\omega dH)^{-1})=\frac{1}{2}\tilde{\sigma}^{m}_{k}\beta\omega dH\,,\]

which combined with the previous inequality and Eq. (53) yields

\[\frac{1}{n}\sum_{j\in[n]}\tilde{c}^{j}_{k,H+1}\left(C(S^{mkj}_{\mathrm{p}(k)}) -\left(E^{\rightarrow}(S^{mkj}_{\mathrm{p}(k)+1},\ldots,R^{mkj}_{H})+R^{mkj} _{k\rightarrow}\right)\right)\leq 2\tilde{\sigma}^{m}_{k}\beta\omega dH+3 \frac{\varepsilon}{H}+2\frac{\varepsilon}{dH^{2}\omega}\,.\]

Observe that the random variables \(\tilde{c}^{j}_{k,H+1}\left(C(S^{mkj}_{\mathrm{p}(k)})-\left(E^{\rightarrow}( S^{mkj}_{\mathrm{p}(k)+1},\ldots,R^{mkj}_{H})+R^{mkj}_{k\rightarrow}\right)\right)\) are independent (for \(j\in[n]\)) with range \([-2H,H]\) (Eq. (31)). By Hoeffding's inequality, with probability at least \(1-\zeta\), for all iteration \(m^{\prime}\in[m^{\prime}_{\max}]\) (this includes the entire execution of SkippyEleanor) and \(k\in[H]\),

\[\left|\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k,H+1}\left( C(S_{p(k)})-\left(E^{\rightarrow}(S_{p(k)+1},\ldots,R_{H})+R_{k\rightarrow} \right)\right)\right.\] \[\qquad-\frac{1}{n}\sum_{j\in[n]}\tilde{c}^{j}_{k,H+1}\left(C(S^{ mkj}_{\mathrm{p}(k)})-\left(E^{\rightarrow}(S^{mkj}_{\mathrm{p}(k)+1},\ldots,R^{ mkj}_{H})+R^{mkj}_{k\rightarrow}\right)\right)\Bigg{|}\] \[\qquad\leq\frac{4H}{\sqrt{n}}\sqrt{\log\frac{2m^{\prime}_{\max}H }{\zeta}}\leq\frac{\varepsilon}{dH^{2}\omega}\,.\]

Combining with the previous bound, under the intersection of the high-probability events referred to above, which by a union bound has a probability of at least \(1-3\zeta\), we have that for all \(k\in[H]\),

\[\mathop{\mathbb{E}}_{\pi^{mk},s_{1}}\tilde{c}_{k,H+1}C(S_{p(k)})\leq\mathop{ \mathbb{E}}_{\pi^{mk},s_{1}}\sum_{u=p(k)}^{H}R_{u}+\tilde{c}_{k,H+1}E^{ \rightarrow}(S_{p(k)+1},\ldots,R_{H})+2\tilde{\sigma}^{m}_{k}\beta\omega dH+4 \frac{\varepsilon}{H}\,.\qed\]

## Appendix K Deferred proofs for Appendix E.4

Proof of Lemma e.10.: Let \(m\) be the current iteration. Unlike in previous lemmas, here we introduce \((\hat{G},\tilde{\theta})\) that does _not_ refer to the outcome of Optimization Problem 4.10. Instead, let \(\hat{G}=(\vartheta^{j}_{h})_{h\in[2:H],j\in[d_{0}]}\in\mathbf{G}\) be the correct guess. For \(h=H,\ldots,1\), \(\tilde{\theta}_{h}\) is defined in sequence along with the behavior of a policy \(\pi\) on stage \(h\).

For \(h=H,\ldots,1\), assuming that this process already defined \(\tilde{\theta}_{h+1},\ldots,\tilde{\theta}_{H}\) (in Eq. (55)), let \(\pi\) be the policy that, for any \(t>h\) and \(s\in\mathcal{S}_{t}\), takes action on \(s\) as \(\pi^{+}_{\hat{G}\hat{\theta}}(s)\) with probability \(\tau_{\hat{G}\hat{\theta}}(s)\), and action \(1\) with probability \(1-\tau_{\hat{G}\hat{\theta}}(s)\) (\(\tau\) is defined in Eq. (8)). Simultaneously, using the second part of Corollary 4.11, define \(\tilde{\theta}_{hi}\in\mathcal{B}(4d_{0}L_{2}/\alpha)\) for \(i\in[h+1:H]\) to satisfy for all \(s\in\mathcal{S}_{h}\), \(a\in[\mathcal{A}]\):

\[\mathop{\mathbb{E}}_{\pi^{0},s,a}\operatorname{Tr}(\tilde{F}_{\hat{G}\hat{ \theta}}(S_{i}))\approx_{\eta_{0}}\left\langle\varphi(s,a),\tilde{\theta}_{hi} \right\rangle\,.\]

We also define \(\tilde{\theta}_{h,H+1}\in\mathcal{B}(L_{2})\) to satisfy for all \(s\in\mathcal{S}_{h}\), \(a\in[\mathcal{A}]\):

\[\mathop{\mathbb{E}}_{\pi^{0},s,a}\sum_{u=h}^{H}R_{u}\approx_{\eta}\left\langle \varphi(s,a),\tilde{\theta}_{h,H+1}\right\rangle\,.\]By Eq. (32),

\[\mathop{\mathbb{E}}_{n^{0},s,a}\sum_{t\in[h+1:H]}\operatorname{Tr}(\hat{F}_{\hat{G }\theta}(S_{i}))+\sum_{u=h}^{H}R_{u}=\mathop{\mathbb{E}}_{n^{0},s,a}E_{\hat{G} \theta}^{\rightarrow}(S_{h+1},\ldots,R_{H})+\sum_{u=h}^{H}R_{u}\approx_{H\eta _{0}}\left\langle\varphi(s,a),\bar{\theta}_{h}\right\rangle\,,\] (54)

where we define

\[\bar{\theta}_{h}=\sum_{i\in[h+1:H+1]}\bar{\theta}_{hi}\,.\] (55)

We first show that \((\hat{G},\bar{\theta})\) is feasible for Optimization Problem 4.10. Clearly, \(\left\|\bar{\theta}_{h}\right\|_{2}\leq 4d_{0}HL_{2}/\alpha\). For any \(i\in[h+1:H]\), let

\[\hat{\theta}_{hi}=X_{mh}^{-1}\sum_{lkj\in\mathbf{I}^{m}(h)}\varphi_{h}^{lkj} \operatorname{Tr}(F_{\hat{G}\hat{\theta}}(S_{h+1}^{lkj},\ldots,R_{H}^{lkj}))\,,\]

and let

\[\hat{\theta}_{h,H+1}=X_{mh}^{-1}\sum_{lkj\in\mathbf{I}^{m}(h)}\varphi_{h}^{ lkj}\sum_{u=h}^{H}R_{u}^{lkj}\,.\]

Then, \(\hat{\theta}\) of Optimization Problem 4.10 satisfies for all \(h\in[H]\), by Eq. (32),

\[\hat{\theta}_{h}=\sum_{i\in[h+1:H+1]}\theta_{hi}\,.\]

To show that \((\hat{G},\bar{\theta})\) is feasible, it thus suffices to show for all \(h\in[H]\), \(i\in[h+1:H+1]\), that \(\left\|\bar{\theta}_{hi}-\bar{\theta}_{hi}\right\|_{X_{mh}}\leq\beta\).

Fix any \(h\in[H]\) and \(i\in[h+1:H+1]\). Take the sequence \(A\) formed of \(\varphi_{t}^{lkj}\) (for \(lkj\in\mathbf{I}^{m}(h)\), in the order that these random variables are observed). For \(i<H+1\) take the sequence \(X\) formed of \(\operatorname{Tr}(F_{\hat{G}\hat{\theta}}(S_{i}^{lkj},\ldots,R_{H}^{lkj}))\) (for \(lkj\in\mathbf{I}^{m}(h)\), in the same order), and the sequence \(\Delta\) formed of \(\mathop{\mathbb{E}}_{n^{0},S_{h}^{lkj},A_{h}^{lkj}}\operatorname{Tr}(F_{\hat {G}\hat{\theta}}(S_{i}))-\left\langle\varphi_{h}^{lkj},\bar{\theta}_{hi}\right\rangle\) (for \(lkj\in\mathbf{I}^{m}(h)\), in the same order). For \(i=H+1\), the sequence \(X\) is formed of \(\sum_{u=h}^{H}R_{u}^{lkj}\), and \(\Delta\) is formed of \(q^{\pi^{0}}(S_{h}^{lkj},A_{h}^{lkj})-\left\langle\varphi_{h}^{lkj},\bar{ \theta}_{hi}\right\rangle\). Then the sequences \(A\), \(X\), and \(\Delta\) satisfy the conditions of Lemma M.4 with a subgaussianity parameter \(\sigma=H\). Due to this lemma, applied with a union bound over \(m^{\prime}\in[m^{\prime}_{\max}]\), \(t\), and \(i\), with probability at least \(1-\zeta\),

\[\left\|\hat{\theta}_{hi}-\tilde{\theta}_{hi}\right\|_{X_{mh}} <\sqrt{\lambda}\left\|\tilde{\theta}_{hi}\right\|_{2}+\left\| \Delta\right\|_{\infty}\sqrt{\left\|\mathbf{I}^{m}(t)\right\|}+H\sqrt{2\log \left(\frac{m^{\prime}_{\max}H^{2}}{\zeta}\right)+\log\left(\frac{\det X_{mt} }{\lambda^{d}}\right)}\] \[\leq 2+H\sqrt{2\log\frac{m^{\prime}_{\max}H^{2}}{\zeta}+\log \left(\frac{\det X_{mt}}{\lambda^{d}}\right)}\leq\beta\,,\]

by Eq. (41).

Next, we show that the resulting policy \(\pi\) is near-optimal. Assume by induction on \(h=H,\ldots,1\), that for all \(t\in[h+1:H]\), all \(s\in\mathcal{S}_{t}\) and \(a\in[\mathcal{A}]\),

\[v^{\pi}(s) \geq v^{\star}(s)-(H-t+1)(\varepsilon/H+2H^{2}\eta_{0})\quad\text{ and}\] (56) \[\left\langle\varphi(s,a),\bar{\theta}_{t}\right\rangle \approx_{(H-t+1)H\eta_{0}}q^{\pi}(s,a)\,.\] (57)

To prove the above for \(t=h\) as well, take any \(s\in\mathcal{S}_{h},a\in[\mathcal{A}]\). Introduce the random variable \(P\) that, for a trajectory following \(\mathcal{P}_{\pi^{0},s,a}\), takes as its value the index of the first Bernoulli draw of \(1\) (starting from index \(h+1\)), when the Bernoulls have means \(\tau_{\hat{G}\theta}(S_{j})\) for \(j\in[h+1:H]\), and takes the value \(H+1\) if all of these Bernoulls have outcome \(0\). Write \(\mathop{\mathbb{E}}_{\pi^{0},s,a,P}[\cdot]\) for \(\mathop{\mathbb{E}}_{\pi^{0},s,a}\mathop{\mathbb{E}}_{P}[\cdot|S_{h+1},\ldots,R _{H}]\)Then,

\[\mathop{\mathbb{E}}_{\pi^{0},s,a}E_{\tilde{G}\,\tilde{\theta}}^{\to}(S_ {h+1},\ldots,R_{H})+\sum_{u=h}^{H}R_{u} =\mathop{\mathbb{E}}_{\pi^{0},s,a,P}D_{\tilde{G}\,\tilde{\theta}}(S_ {P},\ldots,R_{H})+\sum_{u=h}^{H}R_{u}\] \[=\mathop{\mathbb{E}}_{\pi^{0},s,a,P}\sum_{u=h}^{P-1}R_{u}+\mathbb{ 1}\left\{P<H+1\right\}C_{\tilde{G}\,\tilde{\theta}}(S_{P})\]

where we use Eq. (9). Combining with Eq. (54),

\[\left\langle\varphi(s,a),\bar{\theta}_{h}\right\rangle \approx_{H\,\eta_{0}}\mathop{\mathbb{E}}_{\pi^{0},s,a,P}\sum_{u=h }^{P-1}R_{u}+\mathbb{1}\left\{P<H+1\right\}C_{\tilde{G}\,\tilde{\theta}}(S_{P})\] \[=\mathop{\mathbb{E}}_{\pi^{0},s,a,P}\sum_{u=h}^{P-1}R_{u}+ \mathbb{1}\left\{P<H+1\right\}\mathop{\mathrm{clip}}_{\left\{0,H\right\}} \left\langle\varphi(S_{P},\pi_{\tilde{G}\,\tilde{\theta}}^{+}(S_{P})),\bar{ \theta}_{P}\right\rangle\] \[\approx_{(H-h)H\,\eta_{0}}\mathop{\mathbb{E}}_{\pi^{0},s,a,P}\sum _{u=h}^{P-1}R_{u}+\mathbb{1}\left\{P<H+1\right\}q^{\pi}(S_{P},\pi_{\tilde{G} \,\tilde{\theta}}^{+}(S_{P}))\,,\]

where we used the inductive assumption along with the fact that action-values are bounded in \([0,H]\). Observe also that

\[q^{\pi}(s,a)=\mathop{\mathbb{E}}_{\pi^{0},s,a,P}\sum_{u=h}^{P-1}R_{u}+ \mathbb{1}\left\{P<H+1\right\}q^{\pi}(S_{P},\pi_{\tilde{G}\,\tilde{\theta}}^{ +}(S_{P}))\,,\]

and therefore

\[\left\langle\varphi(s,a),\bar{\theta}_{h}\right\rangle\approx_{(H-h+1)H\,\eta _{0}}q^{\pi}(s,a)\,,\]

proving Eq. (57) of the inductive assumption for \(t=h\).

To show Eq. (56) for \(t=h\), by Eq. (57) for \(t=h\) and the inductive assumption for \(t>h\),

\[\left\langle\varphi(s,a),\bar{\theta}_{h}\right\rangle\approx_{H^{2}\eta_{0}} q^{\pi}(s,a)\geq q^{\star}(s,a)-(H-h)(\varepsilon/H+2H^{2}\eta_{0})\,.\]

Either \(\pi\) chooses the action \(a^{\prime}\) maximizing the inner product above, for which

\[q^{\pi}(s,a^{\prime})\geq\max_{a\in[\mathcal{A}]}q^{\star}(s,a)-(H-h)( \varepsilon/H+2H^{2}\eta_{0})-2H^{2}\eta_{0}\geq v^{\star}(s)-(H-h+1)( \varepsilon/H+2H^{2}\eta_{0})\,,\]

or it chooses action 1. This can only happen with non-zero probability if \(\tau_{\tilde{G}\,\tilde{\theta}}(s)<1\), in which case we have by definition that \(\operatorname{range}_{\tilde{Q}}^{\tilde{G}}(s)=\operatorname{range}_{\tilde{Q }}(s)\leq\frac{\varepsilon}{\sqrt{2}dH}\). Combining with Eq. (3) and Proposition 4.5, \(\operatorname{range}(s)\leq\frac{\varepsilon}{H}\), and therefore, using Eq. (56) for \(t=h+1\), in this case

\[q^{\pi}(s,1) \geq q^{\star}(s,1)-(H-h)(\varepsilon/H+2H^{2}\eta_{0})\] \[\geq v^{\star}(s)-\frac{\varepsilon}{H}-2\eta-(H-h)(\varepsilon/H +2H^{2}\eta_{0})\geq v^{\star}(s)-(H-h+1)(\varepsilon/H+2H^{2}\eta_{0})\,.\]

Therefore for any choice of action \(a^{\prime}\) of policy \(\pi\) in state \(s\), \(q^{\pi}(s,a^{\prime})\geq v^{\star}(s)-(H-h+1)(\varepsilon/H+2H^{2}\eta_{0})\). Therefore

\[v^{\pi}(s)\geq v^{\star}(s)-(H-h+1)(\varepsilon/H+2H^{2}\eta_{0})\,,\]

finishing the induction.

We thus conclude that

\[v^{\pi}(s_{1})\geq v^{\star}(s_{1})-\varepsilon-2H^{3}\eta_{0}\,.\]

Combined with Eq. (57) of the inductive assumption, the value of Optimization Problem 4.10 can be bounded as

\[C_{\tilde{G}\,\tilde{\theta}}(s_{1})=\operatorname{clip}_{\left[0,H\right]} \left\langle\varphi(s_{1},\pi(s_{1})),\bar{\theta}_{1}\right\rangle\geq H^{2} \eta_{0}+v^{\pi}(s_{1})\geq v^{\star}(s_{1})-2\varepsilon\,,\]

by assumption on \(\eta\) being relatively small (Eq. (21)).

Deferred lemmas

**Lemma L.1** (Elliptical potential, Lemma 19.4 from Lattimore and Szepesvari (2020)).: _Let \(V_{0}\in\mathbb{R}^{d\times d}\) be positive definite and \(a_{1}\ldots,a_{n}\in\mathbb{R}^{d}\) be a sequence of vectors with \(\left\|a_{t}\right\|_{2}\leq L<\infty\) for all \(t\in[n]\), \(V_{t}=V_{0}+\sum_{s\leq t}a_{s}a_{s}^{\top}\). Then,_

\[\sum_{t=1}^{n}\min\left\{1,\left\|a_{t}\right\|_{V_{t-1}^{-1}}^{2}\right\} \leq 2\log\left(\frac{\det V_{n}}{\det V_{0}}\right)\leq 2d\log\left(\frac{ \operatorname{Tr}V_{0}+nL^{2}}{d\det(V_{0})^{1/d}}\right)\,.\]

**Lemma L.2**.: _Let \(V\in\mathbb{R}^{d\times d}\) be a symmetric positive definite matrix and \((a_{i})_{i\in[n]}\) be a sequence of \(n\)\(d\)-dimensional real vectors. Let \(V_{i}=V+\sum_{j\in[t]}a_{j}a_{j}^{\top}\). Then,_

\[\sum_{i\in[n]}\left\|a_{i}\right\|_{V_{t}^{-1}}^{2}\geq\min\left\{1,\frac{1} {2}\sum_{i\in[n]}\left\|a_{i}\right\|_{V^{-1}}^{2}\right\}\]

Proof.: If \(\sum_{i\in[n]}a_{i}a_{i}^{\top}\leq V\), then \(V_{i}\leq 2V\), and therefore

\[\sum_{i\in[n]}\left\|a_{i}\right\|_{V_{t}^{-1}}^{2}\geq\sum_{i\in[n]}\left\|a _{i}\right\|_{2V^{-1}}^{2}=\frac{1}{2}\left\|a_{i}\right\|_{V^{-1}}\,.\]

Otherwise, \(\sum_{i\in[n]}a_{i}a_{i}^{\top}V^{-1}\) has an eigenvalue that is at least 1. As all the other eigenvalues are non-negative (as \(V\) is symmetric positive definite), we have that

\[\sum_{i\in[n]}\left\|a_{i}\right\|_{V^{-1}}^{2}=\operatorname{Tr}\left(\sum_{ i\in[n]}a_{i}a_{i}^{\top}V^{-1}\right)\geq 1\,.\qed\]

## Appendix M Estimation error blow-up guarantees

We borrow Assumption M.1 and Theorem M.2 from Lattimore and Szepesvari (2020) and refer the reader to the book for the corresponding proof.

**Assumption M.1** (Prerequisites for Theorem M.2).: _Let \(\lambda>0\). For \(k\in\mathbb{N}^{+}\), let \(A_{k}\) be random variables taking values in \(\mathbb{R}^{d}\). For some \(\theta_{\star}\in\mathbb{R}^{d}\), let \(X_{k}=\left\langle A_{k},\theta_{\star}\right\rangle+\eta_{k}\) for all \(k\in\mathbb{N}^{+}\). Here, \(\eta_{k}\) is a conditionally 1-subgaussian random variable ("noise"), ie. it satisfies:_

\[\text{for all }\alpha\in\mathbb{R}\text{ and }t\geq 1,\qquad\mathbb{E}[\exp( \alpha\eta_{k})\,|\,\mathcal{F}_{k-1}]\leq\exp\left(\frac{\alpha^{2}}{2}\right) \quad\text{a.s.},\]

_where \(\mathcal{F}_{k-1}\) is such that \(A_{1},X_{1},\ldots,A_{k-1},X_{k-1},A_{k}\) are \(\mathcal{F}_{k-1}\)-measurable._

**Theorem M.2** (Lattimore and Szepesvari (2020), Theorem 20.5).: _Let \(\zeta\in(0,1)\). Under Assumption M.1, with probability at least \(1-\zeta\), it holds that for all \(k\in\mathbb{N}\),_

\[\left\|\hat{\theta}_{k}-\theta_{\star}\right\|_{V_{k}\left(\lambda\right)}< \sqrt{\lambda}\left\|\theta_{\star}\right\|_{2}+\sqrt{2\log\left(\frac{1}{ \zeta}\right)+\log\left(\frac{\det V_{k}\left(\lambda\right)}{\lambda^{d}} \right)}\,,\]

_where for \(k\in\mathbb{N}\),_

\[V_{k}(\lambda) =\lambda I+\sum_{s=1}^{k}A_{s}A_{s}^{\top}\] \[\hat{\theta}_{k} =V_{k}(\lambda)^{-1}\sum_{s=1}^{k}X_{s}A_{s}\]

We generalize this theorem to handle non-zero-mean noise with parametrized subgaussianity. To handle non-zero-mean noise, we use (Zanette et al., 2020, Lemma 8). We state the lemma here and refer the reader to Zanette et al. (2020) for the proof:

**Lemma M.3** (Zanette et al. [2020], Lemma 8).: _For \(n\in N^{+}\), let \(\{A_{i}\}_{i=1,\ldots,n}\) be any sequence of vectors in \(\mathbb{R}^{d}\) and \(\{\Delta_{i}\}_{i=1,\ldots,n}\) be any sequence of scalars such that \(|\Delta_{i}|\leq\xi\in\mathbb{R}\) with \(\xi\geq 0\). For any \(\lambda\geq 0\) and \(V(\lambda)=\sum_{i=1}^{n}A_{i}\Delta_{i}^{\top}+\lambda I\) we have:_

\[\left\|\sum_{i=1}^{n}A_{i}\Delta_{i}\right\|_{V(\lambda)^{-1}}^{2}\leq n\xi^{2}\]

**Lemma M.4**.: _Let \(\zeta\in(0,1)\), \(\lambda>0\), \(\sigma>0\), and \(\xi\geq 0\). For \(k\in\mathbb{N}^{+}\), let \(A_{k}\) be random variables taking values in \(\mathbb{R}^{d}\). For some \(\theta_{\star}\in\mathbb{R}^{d}\), let \(\tilde{X}_{k}=\langle A_{k},\theta_{\star}\rangle+\eta_{k}\) for all \(k\in\mathbb{N}^{+}\). Here, \(\eta_{k}\) is a conditionally \(\sigma\)-subgaussian random variable, ie. it satisfies:_

\[\text{for all }\alpha\in\mathbb{R}\text{ and }t\geq 1,\qquad\mathbb{E}[\exp( \alpha\eta_{k})\,|\,\mathcal{F}_{k-1}]\leq\exp\left(\frac{\alpha^{2}\sigma^{2 }}{2}\right)\quad\text{a.s.},\]

_where \(\mathcal{F}_{k-1}\) is such that \(A_{1},\tilde{X}_{1},\ldots,A_{k-1},\tilde{X}_{k-1},A_{k}\) are \(\mathcal{F}_{k-1}\)-measurable. With probability at least \(1-\zeta\), it holds that for any sequence \(\{\Delta_{i}\}_{i=1,\ldots}\) such that \(|\Delta_{i}|\leq\xi\), for all \(k\in\mathbb{N}\),_

\[\left\|\hat{\theta}_{k}-\theta_{\star}\right\|_{V_{k}(\lambda)}<\sqrt{\lambda} \left\|\theta_{\star}\right\|_{2}+\xi\sqrt{k}+\sigma\sqrt{2\log\left(\frac{1} {\zeta}\right)+\log\left(\frac{\det V_{k}(\lambda)}{\lambda^{d}}\right)}\,.\]

_where for \(k\in\mathbb{N}\),_

\[X_{k} =\tilde{X}_{k}+\Delta_{k}\] \[V_{k}(\lambda) =\lambda I+\sum_{s=1}^{k}A_{s}A_{s}^{\top}\] \[\hat{\theta}_{k} =V_{k}(\lambda)^{-1}\sum_{s=1}^{k}X_{s}A_{s}\]

Proof.: Let \(X_{k}^{\prime}=(X_{k}-\Delta_{k})/\sigma_{k}\), \(A_{k}^{\prime}=A_{k}/\sigma_{k}\), \(\lambda^{\prime}=\lambda/\sigma_{k}^{2}\), and \(\theta_{\star}^{\prime}=\theta_{\star}\), \(V_{k}^{\prime}(\lambda^{\prime})=\lambda^{\prime}I+\sum_{s=1}^{k}A_{s}^{\prime }{A_{s}^{\prime}}^{\top}\), and \(\hat{\theta}_{k}^{\prime}=V_{k}^{\prime}(\lambda^{\prime})^{-1}\sum_{s=1}^{k} X_{s}^{\prime}{A_{s}^{\prime}}^{\top}\). By assumption, \(X_{k}^{\prime}\), \(A_{k}^{\prime}\), \(\lambda^{\prime}\) and \(\theta_{\star}^{\prime}\) then satisfy Assumption M.1. Therefore by applying Theorem M.2, with probability at least \(1-\zeta\), it holds that for all \(k\in\mathbb{N}\),

\[\left\|\hat{\theta}_{k}^{\prime}-\theta_{\star}\right\|_{V_{k}^{\prime}( \lambda^{\prime})}<\sqrt{\lambda^{\prime}}\left\|\theta_{\star}\right\|_{2}+ \sqrt{2\log\left(\frac{1}{\zeta}\right)+\log\left(\frac{\det V_{k}^{\prime}( \lambda^{\prime})}{{\lambda^{\prime}}^{d}}\right)}\,.\]

Under this high-probability event, since \(V_{k}^{\prime}(\lambda^{\prime})=V_{k}(\lambda)/\sigma^{2}\), substituting into the previous display yields

\[\left\|\hat{\theta}_{k}^{\prime}-\theta_{\star}\right\|_{V_{k}( \lambda)}<\sqrt{\lambda}\left\|\theta_{\star}\right\|_{2}+\sigma\sqrt{2\log \left(\frac{1}{\zeta}\right)+\log\left(\frac{\det V_{k}(\lambda)}{\lambda^{d} }\right)}\,.\] (58)

Take any sequence \(\{\Delta_{i}\}_{i=1,\ldots}\) such that \(|\Delta_{i}|\leq\xi\) and apply the triangle inequality:

\[\left\|\hat{\theta}_{k}-\theta_{\star}\right\|_{V_{k}(\lambda)}\leq\left\|\hat {\theta}_{k}^{\prime}-\theta_{\star}\right\|_{V_{k}(\lambda)}+\left\|\hat{ \theta}_{k}^{\prime}-\hat{\theta}_{k}\right\|_{V_{k}(\lambda)}\,,\] (59)

so it remains to bound \(\left\|\hat{\theta}_{k}^{\prime}-\hat{\theta}_{k}\right\|_{V_{k}(\lambda)}\).

\[\left\|\hat{\theta}_{k}^{\prime}-\hat{\theta}_{k}\right\|_{V_{k}( \lambda)} =\left\|V_{k}^{\prime}(\lambda^{\prime})^{-1}\sum_{s=1}^{k}X_{s}^ {\prime}A_{s}^{\prime}-V_{k}(\lambda)^{-1}\sum_{s=1}^{k}X_{s}A_{s}\right\|_{V_{ k}(\lambda)}\] (60) \[=\left\|V_{k}(\lambda)^{-1}\sum_{s=1}^{k}(X_{s}-\Delta_{s})A_{s}-V _{k}(\lambda)^{-1}\sum_{s=1}^{k}X_{s}A_{s}\right\|_{V_{k}(\lambda)}\] \[=\left\|V_{k}(\lambda)^{-1}\sum_{s=1}^{k}\Delta_{s}A_{s}\right\| _{V_{k}(\lambda)}=\left\|\sum_{s=1}^{k}\Delta_{s}A_{s}\right\|_{V_{k}(\lambda)^{ -1}}\] \[\leq\sqrt{k}\xi\,,\]

where the final inequality uses Lemma M.3. The proof is finished by plugging in the bounds of Eqs. (58) and (60) into the triangle inequality of Eq. (59).