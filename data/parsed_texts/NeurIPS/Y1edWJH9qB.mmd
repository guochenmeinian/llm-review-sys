# Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network

Xiao Guo, Vishal Asnani, Sijia Liu, Xiaoming Liu

Michigan State University

{guoxia11, asnanivi, liusiji5, liuxm}@msu.edu

###### Abstract

_Model Parsing_ defines the task of predicting hyperparameters of the generative model (GM), given a GM-generated image as the input. Since a diverse set of hyperparameters is jointly employed by the generative model, and dependencies often exist among them, it is crucial to learn these hyperparameter dependencies for improving the model parsing performance. To explore such important dependencies, we propose a novel model parsing method called Learnable Graph Pooling Network (LGPN), in which we formulate model parsing as a graph node classification problem, using graph nodes and edges to represent hyperparameters and their dependencies, respectively. Furthermore, LGPN incorporates a learnable pooling-unpooling mechanism tailored to model parsing, which adaptively learns hyperparameter dependencies of GMs used to generate the input image. Also, we introduce a Generation Trace Capturing Network (GTC) that can efficiently identify generation traces of input images, enhancing the understanding of generated images' provenances. Empirically, we achieve state-of-the-art performance in model parsing and its extended applications, showing the superiority of the proposed LGPN. The source code is available at link.

## 1 Introduction

Generative Models (GMs) [22, 73, 11, 44, 33, 60, 32, 34, 50], _e.g._, Generative Adversarial Networks (GANs), Variational Autoencoder (VAEs), and Diffusion Models (DMs), have gained significant attention, offering remarkable capabilities in generating visually compelling images. However, the proliferation of such Artificial Intelligence Generated Content (AIGC) can inadvertently propagate inaccurate or biased information. To mitigate such negative impact, various image forensics [52] methods have been proposed [62, 54, 20, 6, 24, 64, 31, 76, 56, 71, 1]. Alongside these defensive efforts, the recent work [2] defines a novel research topic called "_model parsing_", which predicts \(37\) GM hyperparameters using the generated image as the input, as detailed in Fig. 0(a).

Model parsing requires analyzing GM hyperparameters and gaining insights into origins of generated images, which facilitate defenders to develop effective countermeasures. For example, one can reasonably determine if there exists coordinated attacks [2] -- two images are generated from the same GM that is _unseen_ during the training, using predicted hyperparameters from the model parsing algorithm. In light of this, the previous method [2] introduces a clustering-based approach that achieves effective model parsing performance. However, this approach neglects the learning of hyperparameter dependencies. For instance, an inherent dependency exists between GM's layer number and parameter number, as GM's layer number is positively proportional to its parameter number. A similar proportional relationship exists between the number of convolutional layers and convolutional filters. In contrast, the use of the L1 loss is negatively correlated with the use of the L2 loss, as GMs typically do not employ both losses as objective functions simultaneously. We believe the neglect of such dependencies might cause a suboptimized performance. Therefore, inthis work, we propose to use graph nodes and edges to explicitly represent hyperparameters and their dependencies, respectively, and then propose a model parsing algorithm that utilizes the effectiveness of Graph Convolution Network (GCN) [35; 61; 18; 51] to capture dependencies among graph nodes.

Specifically, we first use training samples in the RED dataset [2] to construct a directed graph (Fig. 1b). The directed graph, based on the label co-occurrence pattern, illustrates the fundamental correlation between different categories and helps prior GCN-based methods achieve remarkable performances in various applications [10; 7; 68; 46; 16; 58]. In this work, this directed graph is tailored to the model parsing -- we define discrete-value and continuous-value graph nodes to represent hyperparameters, shown in Fig. 1c. Then, we use this graph to formulate model parsing as a graph node classification problem, in which the discrete-value graph node feature decides if a given hyperparameter is used in the given GM, and the continuous-value node feature decides which range the hyperparameter resides. This formulation helps obtain effective representations of hyperparameters and dependencies among them for the model parsing task, detailed in Sec. 3.1.

To this end, we propose a novel model parsing framework called Learnable Graph Pooling Network (LGPN), which contains a Generation Trace Capturing Network (GTC) and a GCN refinement block (Fig. 2). Our GTC differs from neural network backbones used in State-of-The-Art (SoTA) forgery detection methods [62; 74; 63; 4; 63], using down-sampling operations (_e.g._, pooling) gradually reduce the learned feature map resolution during the forward propagation. This down-sampling can cause the loss of already subtle generation traces left by GMs. Instead, our GTC leverages a high-resolution representation that largely preserves generation traces throughout the forward propagation (Sec. 3.2.1). Therefore, the learned image representation from the GTC deduces crucial information (_i.e._, generation trace) of used GMs and benefits model parsing. Subsequently, this representation is transformed into a set of graph node features, along with the pre-defined directed graph, which are fed to the GCN refinement block. The GCN refinement block contains trainable pooling layers that progressively convert the correlation graph into a series of coarsened graphs by merging original graph nodes into supernodes. Then, the graph convolution is conducted to aggregate node features at all levels of graphs, and trainable unpooling layers are employed to restore supernodes to their corresponding children nodes. This learnable pooling-unpooling mechanism helps LGPN generalize to parsing hyperparameters in unseen GMs and improves the GCN representation learning. In summary, our contributions are:

\(\diamond\) We innovatively formulate _model parsing_ as a graph node classification problem, using a directed graph to help capture hyperparameter dependencies for better model parsing performance.

\(\diamond\) A learnable pooling-unpooling mechanism is introduced with GCN to enhance representation learning in model parsing and its generalization ability.

\(\diamond\) We propose a Generation Trace Capturing Network (GTC) that utilizes high-resolution representations to capture generation traces, facilitating a deeper understanding of the image's provenance.

Figure 1: (a) Hyperparameters define a GM that generates images. _Model parsing_[2] refers to the task of predicting hyperparameters given the generated image. (b) We study the co-occurrence pattern among different hyperparameters in various GMs from the RED\(140\) dataset whose composition is shown as the pie chart1, and subsequently construct a directed graph to capture dependencies among these hyperparameters. (c) We define the discrete-value graph node () (_e.g._, L1 and Batch Norm) for each discrete hyperparameter. For each continuous hyperparameter (), we partition its range into \(n\) distinct intervals, and each interval is then represented by a graph node: Parameter Number has three corresponding continuous-value graph nodes. Representations on these graph nodes are used to predict hyperparameters.

\(\diamond\) Extensive empirical results demonstrate the SoTA performance of the proposed LGPN in model parsing and identifying coordinated attacks. Additionally, the GTC's effectiveness is validated through CNN-generated image detection and image attribution tasks.

## 2 Related Works

Model ParsingPrevious model parsing works [59; 30; 17; 3; 47] require prior knowledge of machine learning models and their inputs to predict model hyperparameters, and such predictions are primarily limited to architecture-related hyperparameters [17; 47]. In contrast, Asnani _et al._[2] recently propose a technique to estimate \(37\) pre-defined hyperparameters covering loss functions and architectures by only leveraging generated images as inputs. Specifically, this work [2] designs FEN-PN that uses a clustering-based approach to estimate the mean and standard deviation of hyperparameters for network architecture and loss function types. Then, FEN-PN uses a fingerprint estimation network that is trained with four constraints to estimate the fingerprint for each image. These fingerprints are employed to predict hyperparameters. However, FEN-PN overlooks dependencies among hyperparameters, which cannot be adequately captured by the estimated mean and standard deviation. In contrast, we propose LGPN, using GCN to model dependencies among different hyperparameters, improving overall model parsing performance.

GCN-based MethodGraph Convolution Neural Network (GCN) shows effectiveness in encoding dependencies among different graph nodes [35; 61; 18; 26; 28], and in the computer vision community, directed graphs based on label co-occurrence patterns are used with GCN in tasks such as multi-label image recognition [10; 7; 68], semantic segmentation [9; 29; 41; 16], and person ReID and action localization [46; 65; 8; 58]. These works rely on original graph structures, whereas our method uses a pooling algorithm to modify this graph structure, which can benefit GCN's representation learning. Also, our GCN refinement block differs from Graph U-Net [19] in that it reduces the graph size by removing certain nodes. However, we formulate model parsing as a graph node classification task, where each node represents a specific hyperparameter, meaning no nodes should be discarded.

Learning Image Generation TracesGMs leave particular traces in their generated images [43; 12], which can be visual artifacts [72; 4] or through evident peaks in the frequency domain [74; 62]. These traces serve as important clues for image forensic tasks such as detection [12; 74; 62], attribution [70; 49] and model parsing [2; 66; 67; 21]. Current methods [62; 74; 63; 4; 63; 57] use backbones like ResNet and XceptionNet that have high generalization abilities, and vision-language foundation models [55; 75; 23] also show effectiveness in detecting unseen forgeries. However, these prior methods often use backbones that rely on downsampling operations, such as convolutions with large strides and pooling, which capture global semantics but discard high-frequency details that contain critical generation traces. In contrast, we propose a Generation Trace Capturing network that mainly operates on a high-resolution representation for capturing such generation traces. Its effectiveness is shown in our experiment by comparing against recent detection methods that use pre-trained CLIP features [48] and the novel representation [63], and competitive image attribution methods.

Figure 2: **Learnable Graph Pooling Network**. Given an input image \(\mathbf{I}\), the proposed LGPN first uses the Generation Trace Capturing Network (Fig. 3) to extract the representation \(\mathbf{f}\). Then, \(\mathbf{f}\) is transformed into \(\mathbf{H}\), which represents a set of graph node features and is fed into the GCN refinement block. The GCN refinement block stacks GCN layers with paired pooling-unpooling layers (Sec. 3.2.2) and produces the refined feature \(\mathbf{V}\) for model parsing. Our method is jointly trained with \(3\) different objective functions (Sec. 3.3).

## 3 Method

In this section, we first revisit some fundamental preliminaries in Sec. 3.1, and then introduce the proposed Learnabled Graph Pooling Network (LGPN) in Sec. 3.2. Lastly, we describe training and inference procedures in Sec. 3.3.

### Preliminaries

This section provides the problem statement of _model parsing_ and its formulation as a graph node classification task with the Graph Convolutional Network (GCN).

**Revisiting Model Parsing** Given an input image \(\mathbf{I}\in\mathbb{R}^{3\times W\times H}\) generated by a GM (_i.e._, \(\mathcal{G}\)), the model parsing algorithm generates three vectors (\(\mathbf{y}^{d}\in\mathbb{R}^{18}\), \(\mathbf{y}^{c}\in\mathbb{R}^{9}\), and \(\mathbf{y}^{l}\in\mathbb{R}^{10}\)) as predictions of \(G\) different hyperparameters used in the \(\mathcal{G}\). Specifically, defined by the previous work [2], these predictable \(G\) hyperparameters include discrete and continuous architecture hyperparameters, as well as loss functions, which are denoted as \(\mathbf{y}^{d}\), \(\mathbf{y}^{c}\), and \(\mathbf{y}^{l}\), respectively. For \(\mathbf{y}^{d}\) and \(\mathbf{y}^{l}\), each element is a binary value representing if the corresponding feature is used or not. Each element of \(\mathbf{y}^{c}\) is the value of certain continuous architecture hyperparameters, such as the layer number and parameter number. As these hyperparameters are in different ranges, we normalize them into [0, 1], same as the previous work [2]. Predicting \(\mathbf{y}^{d}\) and \(\mathbf{y}^{l}\) is a classification task while the regression is used for \(\mathbf{y}^{c}\). Detailed definitions are in Tab. 4, 5, and 6 of the Supplementary. We augment the previous model parsing dataset (_e.g._, RED\(116\)[2]) with different diffusion models such as DDPM [27], ADM [15] and Stable Diffusions [50], increasing the spectrum of GMs. Also, we add real images on which these GMs are trained into the dataset. In the end, we collect \(140\) GMs in total and denote such a collection as the RED\(140\) dataset. Details are in the Supplementary Sec. D.

**Correlation Graph Construction** We design a graph structure of model parsing, where graph nodes and edges represent hyperparameters and their dependencies, respectively. We first use conditional probability \(P(L_{j}|L_{i})\) to denote the probability of hyperparameter \(L_{j}\) occurrence when hyperparameter \(L_{i}\) appears. We count the occurrence of such pairs in the RED\(140\) to construct the matrix \(\mathbf{G}\in\mathbb{R}^{C\times C}\), where \(C\) and \(\mathbf{G}_{ij}\) denotes the number of graph nodes and the conditional probability of \(P(L_{j}|L_{i})\), respectively. Next, we apply a fixed threshold \(\tau\) to remove edges with low correlations in \(\mathbf{G}\) and then obtain a directed graph, denoted as \(\mathbf{A}\in\mathbb{R}^{C\times C}\), where \(\mathbf{A}_{ij}\) indicates if there exists an edge between node \(i\) and \(j\).

Specifically, as depicted in Fig. 0(c), each discrete hyperparameter (_e.g._, discrete architecture hyperparameters and loss functions) is represented by one graph node of \(\mathbf{A}\), denoted as a discrete-value graph node. For continuous hyperparameters, we first divide its value range into \(n\) different intervals, and each interval is represented by one graph node of \(\mathbf{A}\) denoted as a continuous-value graph node. In other words, \(C\) is larger than \(G\) since each continuous hyperparameter is represented by \(n\) continuous-value graph nodes. Subsequently, we use discrete-value graph nodes to decide if given hyperparameters are present, and the continuous-value node decides which range the hyperparameter resides. Therefore, we denote the constructed graph as \(\mathbf{A}\) and apply stacked graph convolution on \(\mathbf{A}\) as follows:

\[\mathbf{h}_{i}^{l}=\mathrm{ReLU}(\sum_{j=1}^{C}\mathbf{A}_{i,j}\mathbf{W}^{l }\mathbf{h}_{j}^{l-1}+\mathbf{b}^{l}),\] (1)

where \(\mathbf{h}_{i}^{l}\) represents the \(i\)-th node feature in graph \(\mathbf{A}\). \(\mathbf{W}^{l}\) and \(\mathbf{b}^{l}\) are weight and bias terms.

Figure 3: **Generation Trace Capturing Network**. First, convolution layers with different kernel sizes extract feature maps of the input image \(\mathbf{I}\). A fusion layer concatenates these feature maps and then proceeds the concatenated feature to the ResNet branch and High-res branch.

### Learnable Graph Pooling Network

In this section, we detail the Generation Trace Capturing Network (GTC) and GCN refinement block--two major components used in the proposed LGPN, as depicted in Fig. 2.

#### 3.2.1 Generation Trace Capturing Network

As shown in Fig. 3, GTC uses one branch (_i.e._, ResNet branch) to propagate the original image information. Meanwhile, the other branch, denoted as _high-res branch_, harnesses the high-resolution representation that helps detect high-frequency generation artifacts stemming from various GMs. More formally, three separate \(2\)D convolution layers with different kernel sizes (_e.g._, \(3\times 3\), \(5\times 5\) and \(7\times 7\)) are utilized to extract feature maps of \(\mathbf{I}\). We concatenate these feature maps and feed them to the fusion layer -- the \(1\times 1\) convolution for the channel dimension reduction. Then, we obtain the feature map \(\mathbf{F}^{h}\in\mathbb{R}^{D\times W\times H}\), with the same resolution as \(\mathbf{I}\). After that, we proceed \(\mathbf{F}^{h}\) to a dual-branch backbone. Specifically, we upsample intermediate features output from each ResNet block and incorporate them into the high-res branch, as depicted in Fig. 3. The high-res branch also has four different convolution blocks (_e.g._, \(\Phi_{b}\) with \(b\in\{1\ldots 4\}\)), which do not employ down-sampling operations, such as the \(2\)D convolution with large strides or pooling layers. Then, intermediate feature maps throughout the high-res branch possess the same resolution as \(\mathbf{F}^{h}\).

The ResNet branch and high-res branch output feature maps are concatenated and then passed through an AVGPOOL layer. Then, we obtain the final learned representation, \(\mathbf{f}\in\mathbb{R}^{D}\), that captures generation artifacts of the input image \(\mathbf{I}\). Subsuetly, we learn \(C\) independent linear layers, _i.e._, \(\Theta=\{\theta_{i=0}^{C-1}\}\) to transform \(\mathbf{f}\) into a set of graph node features \(\mathbf{H}=\{\mathbf{h}_{0},\mathbf{h}_{1},...,\mathbf{h}_{(C-1)}\}\), where \(\mathbf{H}\in\mathbb{R}^{C\times D}\) and \(\mathbf{h}_{i}\in\mathbb{R}^{1\times D}\) (\(i\in\{0,1,...,C-1\}\)). We use \(\mathbf{H}\) to denote graph node features of the directed graph (_i.e._, graph topology) \(\mathbf{A}\in\mathbb{R}^{C\times C}\).

#### 3.2.2 GCN Refinement Block

The GCN refinement block has a learnable pooling-unpooling mechanism that progressively coarsens the original graph \(\mathbf{A}_{0}\) into a series of coarsened graphs, _i.e._, \(\mathbf{A}_{1},\mathbf{A}_{2}...\mathbf{A}_{n}\), and graph convolution is conducted on graphs at all different levels. Specifically, such a pooling operation is achieved by merging graph nodes, namely, via a learned matching matrix \(\mathbf{M}\). Also, correlation matrices of different graphs, denoted as \(\mathbf{A}_{l}\)2, which are learned using MLP layers, which are also influenced by the GM responsible for generating the input image. This further emphasizes the significant impact of GM on the correlation graph generation process.

Footnote 2: A \(l\)-th layer graph has nodes and connectivities (_e.g._, correlations), and we use \(\mathbf{A}_{l}\) to denote the \(l\)-th layer graph or only its correlations.

**Learnable Graph Pooling Layer** First, \(\mathbf{A}_{l}\in\mathbb{R}^{m\times m}\) and \(\mathbf{A}_{l+1}\in\mathbb{R}^{n\times n}\) denote directed graphs at \(l\) th and \(l+1\) th layers, with \(m\) and \(n\) (\(m\geq n\)) graph nodes, respectively. An assignment matrix \(\mathbf{M}_{l}\in\mathbb{R}^{m\times n}\) converts \(\mathbf{A}_{l}\) to \(\mathbf{A}_{l+1}\) as:

\[\mathbf{A}_{l+1}=\mathbf{M}_{l}{}^{T}\mathbf{A}_{l}\mathbf{M}_{l}.\] (2)

Also, we use \(\mathbf{H}_{l}\in\mathbb{R}^{m\times D}\) and \(\mathbf{H}_{l+1}\in\mathbb{R}^{n\times D}\) to denote graph node features of \(\mathbf{A}_{l}\) and \(\mathbf{A}_{l+1}\), respectively. Therefore, we can use \(\mathbf{M}_{l}\) to perform the graph node aggregation operation via:

\[\mathbf{H}_{l+1}=\mathbf{M}_{l}{}^{T}\mathbf{H}_{l}.\] (3)

For simplicity, we use \(f_{GCN}\) to denote the mapping function imposed by a GCN block that has multiple GCN layers. Assuming the \(q\)-th and \(l\)-th graph layer are the first and last layer of the GCN block, then we have:

\[\mathbf{H}_{l}=f_{GCN}(\mathbf{H}_{q}).\] (4)

Ideally, the learnable pooling operation should be dependent on the learned representation of the input image, and such representation is converted into \(\mathbf{H}_{l}\). Therefore, we employ a trainable weight \(\mathbf{W}_{\mathbf{m}}\) to transform \(\mathbf{H}_{l}\) into the assignment matrix \(\mathbf{M}_{l}\):

\[\mathbf{M}_{l}=\frac{1}{1+e^{-\alpha(\mathbf{W}_{\mathbf{m}}\mathbf{H}_{l})}},\] (5)

where \(\alpha\) is set as \(1e9\). Values of the resultant \(\mathbf{M}_{l}\) are approximately equal to \(0\) or \(1\). It is worth noting that prior works [39, 40] also adopt techniques similar to Eq. 5 for making the thresholding operation differentiable.

**Learnable Unpooling Layer** We perform the graph unpooling operation that progressively restores pooled graphs to the graph at the original resolution for the graph node classification task. As shown in Fig. 2, to avoid confusion, we use \(\mathbf{H}\) and \(\mathbf{V}\) to represent the graph node feature on the pooling and unpooling branches, respectively. The correlation matrix on the unpooling branch is denoted by \(\mathbf{A}^{\prime}\),

\[\mathbf{A}^{\prime}_{l-1}=\mathbf{M}_{l}\mathbf{A}^{\prime}_{l}\mathbf{M}^{T}_{ l};\mathbf{V}_{l-1}=\mathbf{M}_{l}\mathbf{V}_{l},\] (6)

where \(\mathbf{A}^{\prime}_{l}\) and \(\mathbf{A}^{\prime}_{l-1}\) are the \(l\) th and \(l-1\) th layers in the unpooling branch, respectively. Finally, we use the refined feature \(\mathbf{V}\) for model parsing.

**Discussion** This learnable pooling-unpooling mechanism offers three distinct advantages. First, each supernode in the coarsened graph serves as the combination of features from its children nodes, and graph convolutions on supernodes have a large receptive field for aggregating features. Secondly, the learnable correlation models hyperparameter dependencies dynamically based on generation artifacts of input image features (_e.g._, \(\mathbf{f}\)). Lastly, learned correlation graphs \(\mathbf{A}\) vary across different levels, helping address the over-smoothing issue commonly encountered in GCN learning [38, 45, 5].

### Training and Inference

We jointly train our method with three objective functions. Graph node classification loss (_e.g._, \(\mathcal{L}^{graph}\)) encourages each graph node feature to predict the corresponding hyperparameter label. Artifacts isolation loss (_e.g._, \(\mathcal{L}^{iso}\)) helps the LGPN only parse the hyperparameters for generated images, and hyperparameter hierarchy constraints (_e.g._, \(\mathcal{L}^{hier}\)) imposes hierarchical constraints among different hyperparameters while stabilizing the training.

**Training Samples** We denote a training sample as \(\{\mathbf{I},\mathbf{y}\}\), in which \(\mathbf{y}=\{\mathbf{y}^{d},\mathbf{y}^{c},\mathbf{y}^{l}\}=\{y_{0},y_{1},..., y_{(C-1)}\}\) is annotations of \(C\) graph nodes for \(G\) parsed hyperparameters as introduced in Sec. 3.1. Specifically, \(y_{c}\) is assigned as \(1\) if the sample has \(c\)-th hyperparameter and \(0\) otherwise, where \(c\in\{0,1,...,C-1\}\). Details are in Supplementary Sec. B.

**Graph Node Classification Loss** Given the image \(\mathbf{I}\), we convert the refined feature \(\mathbf{V}\) into the predicted score vector, denoted as \(\mathbf{s}=\{s_{0},s_{1},...,s_{(C-1)}\}\). We employ the sigmoid activation to retrieve the probability vector \(\mathbf{p}=\{p_{0},p_{1},...,p_{(C-1)}\}\), namely, \(p_{c}=\texttt{SIGMOID}(s_{c})\). Then, we have:

\[\mathcal{L}^{graph}=\sum_{c=0}^{C-1}(y_{c}\log p_{c}+(1-y_{c})\log(1-p_{c})).\] (7)

**Hyperparameter Hierarchy Prediction** Fig. 4 shows that different hyperparameters can be grouped, so we define the hyperparameter hierarchy assignment \(\mathbf{M}^{s}\) to reflect this inherent nature. More details of such the assignment are in Supplementary Sec. A. Suppose, at the \(l\) th layer, we minimize the \(L_{2}\) norm of the difference between the predicted matching matrix \(\mathbf{M}_{l}\) and \(\mathbf{M}^{s}_{l}\),

\[\mathcal{L}^{hier}=\left\|\mathbf{M}^{s}_{l}-\mathbf{M}_{l}\right\|_{2}=\sqrt {\sum_{i,j=0}(m^{s}_{ij}-m_{ij})}^{2}.\] (8)

Figure 4: (a) A toy example of the hyperparameter hierarchy assignment \(\mathbf{M}^{s}_{l}\): both L1 and MSE belong to the category of pixel-level loss function, so they are merged into the supernode A. Nonlinearity functions (_e.g._, ReLu and Tanh) and normalization methods (_e.g._, Layer Norm. and Batch Norm.) are merged into supernodes B and C, respectively. (b) In inference, discrete-value graph node features are used to classify if discrete hyperparameters are used in the given GM. We concatenate corresponding continuous-value node features and regress the continuous hyperparameter value.

**Artifacts Isolation Loss** We denote the image-level binary label as \(y^{img}\) and use \(p^{img}\) as the probability that \(\mathbf{I}\) is a generated image. Then we have:

\[\mathcal{L}^{iso}=\sum_{i=0}^{M-1}(y^{img}\log p^{img}+(1-y^{img})\log(1-p^{img})).\] (9)

In summary, our joint training loss function can be written as \(\mathcal{L}^{all}=\lambda_{1}\mathcal{L}^{graph}+\lambda_{2}\mathcal{L}^{ hier}+\lambda_{3}\mathcal{L}^{iso}\), where \(\lambda_{1}\) and \(\lambda_{2}\) equal \(0\) when \(\mathbf{I}\) is real.

**Inference** As Fig. 3(b), we use the discrete-value graph node feature to perform the binary classification to decide the presence of given hyperparameters. For the continuous architecture hyperparameter, we first concatenate \(n\) corresponding node feature and train a linear layer to regress it to the estimated value. Empirically, we set \(n\) as \(3\) and show this concatenated feature improves the robustness in predicting the continuous value (see Supplementary Tab. 7).

## 4 Experiment

### Model Parsing

**Setup** Our experiment utilizes RED\(140\) dataset. In RED\(140\), each GM contains \(1,000\) images, resulting in a total of \(140,000\) generated images that encompass a wide range of semantics, including objects, handwritten digits, and human faces. Also, RED\(140\) has real images on which these GMs are trained, such as CelebA [42], MNIST [14], CIFAR10 [36], ImageNet [13], facades [77], edges2shoes [77] and, apple2oranges [77]. We follow the protocol of [2]: we divide samples into \(4\) disjoint sets, each of which comprises different GM categories such as GAN, VAE, DM, _etc_. Next, we do leave-one-out testing, _i.e._, train on \(125\) GMs from three sets, and test on GMs of the remaining set. The performance is averaged across four test sets, measured by F1 score and accuracy for discrete hyperparameters (loss function and discrete architecture hyperparameters) and L1 error for continuous architecture hyperparameters. Implementation details are in Supplementary Sec. B.

**Main Performance** We report model parsing performance in Tab. 1, where our proposed LGPN (line \(\#6\)) largely outperforms previous model parsing algorithms. We first employ commonly used backbones to set up competitive model parsing baselines for a more comprehensive comparison. More formally, four baselines in lines \(\#1\)--\(4\) that use ResNet-50, ViT-B, HR-Net, and GTC as backbones with \(2\) layers MLP as the model parsing head, respectively. Baseline\(4\) (line \(\#4\)) achieves the best performance, which indicates that GTC is the most suitable backbone for model parsing. Specifically, Baseline\(4\) has \(1.8\%\) and \(3.5\%\) higher F1 score over Baseline\(2\) that uses HR-Net on predicting hyperparameters of loss functions and discrete architecture hyperparameters. After that, we report FEN-PN's performance, which already proves the effectiveness on the model parsing task since it has specific model parsing architectures containing a fingerprint estimate network (FEN) and a parsing network that predicts hyperparameters. Surprisingly, although FEN-PN has achieved competitive results on RED\(116\), it only has comparable performance to Baseline\(2\). This indicates that FEN-PN reduces its effectiveness in predicting hyperparameters of diffusion models, as RED\(140\)

\begin{table}
\begin{tabular}{c|c|c|c||c c||c c||c} \hline  & \multicolumn{3}{c||}{} & \multicolumn{2}{c||}{Loss} & \multicolumn{2}{c||}{Dis. Archi.} & \multicolumn{2}{c}{Con. Archi.} \\  & \multicolumn{2}{c||}{Method} & \multicolumn{2}{c||}{Function} & \multicolumn{2}{c||}{Para.} & \multicolumn{2}{c}{Para.} \\ \cline{2-9}  & ID & Backbone & MP Head & F1 \(\uparrow\) & Acc. \(\uparrow\) & F1 \(\uparrow\) & Acc. \(\uparrow\) & L1 error \(\downarrow\) \\ \hline \hline \(1\) & Baseline\(1\) & ResNet-50 & MLP & \(79.0\) & \(77.7\) & \(72.1\) & \(69.0\) & \(0.163\) \\ \(2\) & Baseline\(2\) & HR-Net & MLP & \(80.7\) & \(81.9\) & \(72.2\) & \(70.3\) & \(0.149\) \\ \(3\) & Baseline\(3\) & ViT-B & MLP & \(76.3\) & \(75.9\) & \(68.3\) & \(66.4\) & \(0.177\) \\ \(4\) & Baseline\(4\) & GTC & MLP & \(82.5\) & \(80.9\) & \(75.7\) & \(70.9\) & \(0.135\) \\ \hline \hline \(5\) & FEN-PN [2] & FEN. & Parsing Net. & \(80.5\) & \(78.9\) & \(73.0\) & \(70.8\) & \(0.139\) \\ \(6\) & LGPN & GTC & GCN refinement & \(\mathbf{84.6}\) & \(\mathbf{83.3}\) & \(\mathbf{79.5}\) & \(\mathbf{77.5}\) & \(\mathbf{0.120}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: We report model parsing performance on RED\(140\), where each method has an individual ID that represents different backbones and model parsing (MP) heads. The comparison among different backbones (\(\quad\)) shows the effectiveness of the proposed GTC. _Loss Function_ reports the averaged prediction performance on \(10\) loss functions. The averaged performance on \(18\) discrete architecture hyperparameters and \(9\) continuous hyperparameters are reported in _Dis. Archi. Para._ and _Con. Archi. Para._, respectively. [**Bold**: best result].

contains more images from diffusion models than RED\(116\). The complete performance comparison on RED\(116\) is reported in Tab. (c)c. Lastly, we replace MLP with the GCN refinement block, which is the full model of LGPN (line \(\#6\)) and performs better on all metrics, demonstrating that the GCN refinement block indeed refines graph node features and makes a more effective model parsing head than MLP layers.

**Hyperparameter Dependency Capturing** Tab. (a)a reports model parsing performance using different GCN variants as the model parsing head. Overall, the proposed GCN refinement block, which refines graph node features to better capture hyperparameter dependencies, helps achieve the best model parsing performance. By comparing lines \(\#1\) and \(\#2\), we conclude that replacing MLP layers with the GCN refinement benefits the model parsing task. After all, GCN leverages structural information of the pre-defined graph, improving the learning of correlations among different hyperparameters. Next, we use _Graph attention networks_[61] (Att-GCN) at line \(\#3\), which employs the attention mechanism to update the graph node feature. As a result, Att-GCN achieves \(1.1\%\) higher F1 than stacked GCN (_e.g.,_ line \(\#2\)) on predicting discrete architecture parameters. Lastly, line \(\#4\) uses _Graph U-Net_[19], which has a similar pooling-unpooling mechanism to our GCN refinement block, but its pooling operation discards graph nodes in the previous layer for forming a smaller graph. Using the Graph U-Net as the model parsing head produces the worse performance on discrete architecture hyperparameters -- \(4.3\%\) lower than Att-GCN (\(\#3\)). We believe this is because dropping graph nodes is not optimal for model parsing, in which all graph nodes represent corresponding hyperparameters and are important for the final performance. In contrast, LGPN merges children nodes into the supernode, so all node information in the previous layer remains in the smaller pooled graph, helping achieve the best performance on discrete hyperparameters in Tab. (a)a. On the other hand, _continuous hyperparameter prediction_ can also benefit from the GCN refinement block. In Tab. (b)b, LGPN shows L1 errors of \(0.147\) and \(0.081\) on Layers Num. and Param. Num. respectively, whereas the model with MLP layers only achieves \(0.149\) and \(0.148\), respectively. This is because the GCN refinement learns the dependency between Param. Num. and Layer Num., which aligns with the observation that models with more layers typically have more parameters. Therefore, modeling such dependencies ultimately decreases the L1 error in Param. Num. prediction. Likewise, when predicting Conv. Layer Num. and Filters Num., the GCN refinement achieves \(0.137\) and \(0.149\) L1 error, whereas GTC with MLP layers have \(0.151\) and \(0.161\) L1 error.

**GM-dependent Graph** Fig. (a)a shows that learned correlation graphs (\(\textbf{A}^{\prime}{}_{0}\)) from image pairs exhibit significant similarity when both images belong to the same _unseen_ GM. This result demonstrates that our correlation graph largely depends on the GM instead of image contents, given that we have

\begin{table}

\end{table}
Table 2: (a) Model parsing performance comparison with different GCN variants [**Bold**: best result]. (b) The GCN refinement block improves prediction performance on continuous hyperparameters.

Figure 5: a) Cosine similarity between generated correlation graphs (_i.e._, \(\textbf{A}^{\prime}{}_{0}\)) for _unseen_ GMs in one of four test sets. Each element of this matrix is the average cosine similarities of \(2,000\) pairs of generated correlation graphs \(\textbf{A}^{\prime}{}_{0}\) from corresponding GMs. b) The ablation on three objective functions, defined in Sec. 3.3. c) The model parsing performance on RED116 dataset. [Key: **Best**; S-GCN: stacked GCN]

different contents (_e.g._, human face and objects) in unseen GMs from each test. In addition, we empirically observe our method remains robust when using different thresholds to construct the graph, which is detailed in the Supplementary Tab. 9. However, the performance declines more when the threshold increases to \(0.65\), which causes the correlation graph to have very sparse connectivities, hindering the learning of hyperparameter dependencies.

**Objective Functions Analysis**  Fig. 4(b) shows the ablation of different training objective functions introduced in Sec. 3.3. Line \(\#1\) only optimizes the LGPN with \(\mathcal{L}^{\textit{grpah}}\), producing results comparable to simply stacking GCN with the attention mechanism (_e.g._, line \(\#3\) in Tab. 1(a)). Lines \(\#2\) and \(\#3\) show contributions from \(\mathcal{L}^{\textit{iso}}\) and \(\mathcal{L}^{\textit{hier}}\), which improve the performance by \(1.1\%\) and \(2.0\%\) than only using \(\mathcal{L}^{\textit{grpah}}\), on predicting discrete architecture hyperparameters, respectively. This is because \(\mathcal{L}^{\textit{iso}}\) and \(\mathcal{L}^{\textit{hier}}\) make the LGPN concentrate on learning generation traces from generated images and impose the hierarchical constraints, respectively.

**RED116 Results**  Fig. 4(c) reports that LGPN achieves the best performance on all metrics in RED116 dataset. Interestingly, LGPN obtains \(79.5\%\) F1 score on predicting discrete architecture hyperparameters in RED140 (Tab. 1), whereas only \(74.3\%\) in RED116, which does not contain diffusion model generated images. We believe this is because all diffusion models share similar architectures, and such similarities make the prediction of their architecture hyperparameters easier.

### Coordinate Attack Detection

We evaluate the proposed LGPN on coordinated attacks detection [2], which aims to classify whether two fake images are generated from the same GM or not. This is achieved by computing the cosine similarity between predicted hyperparameters from given images. Specifically, we evaluate coordinated attack detection on RED\(140\) in a \(4\)-fold cross-validation. In each fold, the train set has \(125\) GMs, and the test set has \(30\) GMs where \(15\) GMs are exclusive (unseen) from the train set and \(15\) GMs are seen in the train set. We generate \(89,000\) training image pairs from train-set GMs for training \(1,000\) image pairs for validation. We generate \(25,000\) test image pairs from test-set GMs, and the average of the \(4\) folds is used as the final result. For the measurement, we use accuracy, AUC, and detection probability at a fixed false alarm rate (Pd@FAR) _e.g._, Pd@\(5\%\) as metrics. Specifically, aside from FEN-PN [2], we also compare with the recent work HiFi-Net [24] that show SoTA results in attributing different forgery methods. Specifically, we train HiFi-Net to classify \(125\) GMs and take learned features from the last fully-connected layer for coordinated attack detection. The performance is reported in Tab. 2(a), which demonstrates that our proposed method surpasses both prior works. We observe the HiFi-Net performs much worse on AUC than the model parsing baseline (_e.g._, GTC \(w\) S-GCN) and FEN-PN. Furthermore, Tab. 2(b) shows the FEN-PN and LGPN perform comparably as HiFi-Net on seen GMs (first two bar charts), yet much better than HiFi-Net when images are generated by unseen GMs (last three bar charts). Lastly, LGPN has a better Pd@\(5\%\) performance than other methods.

### Capturing Generation Traces

To study GTC's ability to identify generation traces, we adopt it for CNN-generated image detection and image attribution. For a fair comparison, _no_ model parsing dataset is used for the pre-training.

**CNN-generated Image Detection**  We append fully-connected layers at the end of GTC to obtain a binary detector that distinguishes CNN-generated images from real ones (detailed in supplementary

\begin{table}
\begin{tabular}{c|c|c|c} \hline Method & Acc. & AUC & Pd@5\% \\ \hline HiFi-Net [24] & \(72.3\) & \(75.4\) & \(30.4\) \\ FEN-PN [2] & \(83.0\) & \(92.4\) & \(61.2\) \\ \hline \hline GTC \(w\) MLP & \(83.9\) & \(92.2\) & \(62.5\) \\ GTC \(w\) S-GCN & \(84.3\) & \(94.2\) & \(68.6\) \\ LGPN & \(85.9\) & \(95.7\) & \(77.2\) \\ \hline \end{tabular}
\end{table}
Table 3: (a) The average coordinate attack detection performance on \(4\)-fold cross validation. (b) Test image pairs in coordinate attack detection are from one of \(5\) cases, bar charts from left to right: same and different GMs in seen set; same or different GMs in unseen set; one GM is from seen set and the other from unseen set. [Key: S-GCN: stacked GCN]Fig. 8). We follow the experiment setup from prior works [62; 48; 57], which trains the model on images generated by ProGAN [32], and test it on images generated by \(11\) unseen forgery methods, using average precision (AP) and accuracy for the measurement. Fig. 5(a) reports that our method achieves premium detection performance compared to prior methods. The second-best method, NPR [57], focuses on learning local up-sampling artifacts from pixels, helping detect images from unseen GMs. Instead, GTC's high-resolution representation more effectively exploits both local and global traces left from generation processes, obtaining a better performance.

**Image Attribution** Tab. 5(b) and Tab. 5(c) report the image attribution performance in two different protocols. Specifically, we define the protocol \(1\) based on the previous work [2], which trains methods on \(100,000\) real and \(100,000\) images generated from four different GMs (_e.g._, SNGAN, MMDGAN, CRAMERGAN, and ProGAN), conducting a five-way classification (_i.e._, \(4\) GMs and real). In protocol \(2\), we add two more generative methods (_e.g._, styleGANv2, styleGANv3), resulting in a more challenging task: a \(7\)-way classification task, classifying whether the image is real samples or generated by which one of \(6\) GMs. As depicted in Supplementary Fig. 8, we apply fully-connected layers on the top of GTC, which leverages the final representation of the generation trace for the multi-category classification task, _e.g._, image attribution. Our proposed method achieves the best image attribution performance on CelebA and competitive results on LSUN, indicating that GTC has a promising ability to capture the generation trace.

## 5 Conclusion

In this study, our focus is _model parsing_, which predicts pre-defined hyperparameters of a GM given an input image. We propose a novel method that incorporates a learnable pooling-unpooling mechanism devised for the model parsing task. This mechanism serves multiple purposes: modeling GM-dependent hyperparameter dependencies, expanding the receptive field of graph convolution, and mitigating the over-smoothing issue in GCN learning. In addition, we provide the Generation Trace Capturing network to capture generation artifacts, which proves effective in two different image forensic applications: CNN-generated image detection and coordinated attacks detection.

**Limitation** We empirically observe two limitations in our proposed method, both of which can be interesting directions for future research. First, while our model parsing approach delivers excellent performance on the RED\(140\) dataset, it is worth exploring its effectiveness on specific GMs that fall outside our dataset scope. This investigation would provide valuable insights into the generalizability of our method to a broader range of GMs. Secondly, we formulate the model parsing task as a closed-set classification problem, which limits its ability to predict undefined hyperparameters, _e.g._, LeakyReLU. One interesting solution could be adding a few new graph nodes for undefined hyperparameters while keeping original graph nodes -- the learned dependency between graph nodes representing ReLU and LeakyReLU should be high.

**Broader Impact** We strongly advocate for the machine learning and computer vision community to actively work towards mitigating potential negative societal implications of research. It is possible that generated face images used in training could leak the identity information of subjects who have not provided consent forms. We shall strive to work on real face imagery whose collection is reviewed by an Institutional Review Board (IRB).

**Ackonwledge** This work was supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112090131 to Xiaoming Liu at Michigan State University.

Figure 6: (a) CNN-generated image detection performance. (b) and (c) report image attribution performance in two different protocols.

[MISSING_PAGE_FAIL:11]

* [24] Xiao Guo, Xiaohong Liu, Zhiyuan Ren, Steven Grosz, Iacopo Masi, and Xiaoming Liu. Hierarchical fine-grained image forgery detection and localization. In _CVPR_, 2023.
* [25] Xiao Guo, Yaojie Liu, Anil Jain, and Xiaoming Liu. Multi-domain learning for updating face anti-spoofing models. In _ECCV_, 2022.
* [26] Zhijiang Guo, Yan Zhang, and Wei Lu. Attention guided graph convolutional networks for relation extraction. In _ACL_, 2019.
* [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.
* [28] I Hsu, Xiao Guo, Premkumar Natarajan, and Nanyun Peng. Discourse-level relation extraction via graph pooling. In _AAAI DLG workshop_, 2021.
* [29] Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, and Junjie Yan. Class-wise dynamic graph convolution for semantic segmentation. In _ECCV_, 2020.
* [30] Weizhe Hua, Zhiru Zhang, and G Edward Suh. Reverse engineering convolutional neural networks through side-channel information leaks. In _DAC_, 2018.
* [31] Kaixiang Ji, Feng Chen, Xin Guo, Yadong Xu, Jian Wang, and Jingdong Chen. Uncertainty-guided learning for improving image manipulation detection. In _ICCV_, 2023.
* [32] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In _ICLR_, 2018.
* [33] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _CVPR_, 2019.
* [34] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. DiffusionCLIP: Text-guided diffusion models for robust image manipulation. In _CVPR_, 2022.
* [35] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.
* [36] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.
* [37] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 1998.
* [38] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In _ICCV_, 2019.
* [39] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me where to look: Guided attention inference network. In _CVPR_, 2018.
* [40] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Guided attention inference network. _PAMI_, 2019.
* [41] Xia Li, Yibo Yang, Qijie Zhao, Tiancheng Shen, Zhouchen Lin, and Hong Liu. Spatial pyramid based graph reasoning for semantic segmentation. In _CVPR_, 2020.
* [42] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _ICCV_, 2015.
* [43] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and Giovanni Poggi. Do GANs leave artificial fingerprints? In _MIPR_, 2019.
* [44] Safa C. Medin, Bernhard Egger, Anoop Cherian, Ye Wang, Joshua B. Tenenbaum, Xiaoming Liu, and Tim K. Marks. MOST-GAN: 3D morphable StyleGAN for disentangled face image manipulation. In _AAAI_, 2022.
* [45] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering GCN: Overcoming oversmoothness in graph convolutional networks. In _NeurIPS_, 2020.
* [46] Binh X Nguyen, Binh D Nguyen, Tuong Do, Erman Tjiputra, Quang D Tran, and Anh Nguyen. Graph-based person signature for person re-identifications. In _CVPR_, 2021.

* [47] Seong Joon Oh, Bernt Schiele, and Mario Fritz. Towards reverse-engineering black-box neural networks. _Explainable AI: interpreting, explaining and visualizing deep learning_, pages 121-144, 2019.
* [48] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In _CVPR_, 2023.
* [49] Yongyang Pan, Xiaohong Liu, Siqi Luo, Yi Xin, Xiao Guo, Xiaoming Liu, XiongRuo Min, and Guangtao Zhai. Towards effective user attribution for latent diffusion models via watermark-informed blending. _arXiv preprint arXiv:2409.10958_, 2024.
* [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.
* [51] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In _ESWC_, 2018.
* [52] Husrev Taha Sencar, Luisa Verdoliva, and Nasir Memon. _Multimedia Forensics_. Springer Nature, 2022.
* [53] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. De-fake: Detection and attribution of fake images generated by text-to-image generation models. In _Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security_, pages 3418-3432, 2023.
* [54] Kaede Shiohara and Toshihiko Yamasaki. Detecting deepfakes with self-blended images. In _CVPR_, 2022.
* [55] Xiufeng Song, Xiao Guo, Jiache Zhang, Qirui Li, Lei Bai, Xiaoming Liu, Guangtao Zhai, and Xiaohong Liu. On learning multi-modal forgery representation for diffusion generated video detection. In _NeurIPS_, 2024.
* [56] Zhihao Sun, Haoran Jiang, Danding Wang, Xirong Li, and Juan Cao. Safl-net: Semantic-agnostic feature learning network with auxiliary plugins for image manipulation detection. In _ICCV_, 2023.
* [57] Chuangchuang Tan, Huan Liu, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking the up-sampling operations in cnn-based generative network for generalizable deepfake detection. In _CVPR_, 2024.
* [58] Praveen Tirupattur, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. Modeling multi-label action dependencies for temporal action localization. In _CVPR_, 2021.
* [59] Florian Tramer, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction APIs. In _USENIXSS_, 2016.
* [60] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled representation learning GAN for pose-invariant face recognition. In _CVPR_, 2017.
* [61] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _ICLR_, 2017.
* [62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot... for now. In _CVPR_, 2020.
* [63] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection. In _ICCV_, 2023.
* [64] Yue Wu, Wael AbdAlmageed, and Premkumar Natarajan. Mantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features. In _CVPR_, 2019.
* [65] Yingmao Yao, Xiaoyan Jiang, Hamido Fujita, and Zhijun Fang. A sparse graph wavelet convolution neural network for video-based person re-identification. _Pattern Recognition_, 2022.
* [66] Yuguang Yao, Xiao Guo, Vishal Asnani, Yifan Gong, Jiancheng Liu, Xue Lin, Xiaoming Liu, Sijia Liu, et al. Reverse engineering of deception s on machine-and human-centric attacks. _Foundations and Trends(r) in Privacy and Security_, 6(2):53-152, 2024.
* [67] Yuguang Yao, Jiancheng Liu, Yifan Gong, Xiaoming Liu, Yanzhi Wang, Xue Lin, and Sijia Liu. Can adversarial examples be parsed to reveal victim model information? In _WACV_, 2024.
* [68] Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, and Yu Qiao. Attention-driven dynamic graph convolutional network for multi-label image recognition. In _ECCV_, 2020.

* [69] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
* [70] Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake images to GANs: Learning and analyzing GAN fingerprints. In _ICCV_, 2019.
* [71] Yuanhao Zhai, Tianyu Luan, David Doermann, and Junsong Yuan. Towards generic image manipulation detection with weakly-supervised self-consistency learning. In _ICCV_, 2023.
* [72] Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, and Jianbo Shi. Perceptual artifacts localization for image synthesis tasks. In _ICCV_, 2023.
* [73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _ICCV_, 2023.
* [74] Xu Zhang, Svebor Karaman, and Shih-Fu Chang. Detecting and simulating artifacts in gan fake images. In _WIFS_, 2019.
* [75] Yue Zhang, Ben Colman, Ali Shahriyari, and Gaurav Bharaj. Common sense reasoning for deep fake detection. In _ECCV_, 2024.
* [76] Jizhe Zhou, Xiaochen Ma, Xia Du, Ahmed Y Alhammadi, and Wentao Feng. Pre-training-free image manipulation localization through non-mutually exclusive contrastive learning. In _ICCV_, 2023.
* [77] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _ICCV_, 2017.

In this supplementary, we provide:

\(\diamond\) Predictable hyperparameters introduction.

\(\diamond\) Training and implementation details.

\(\diamond\) Additional results of model parsing and CNN-generated image detection.

\(\diamond\) The construction of RED\(140\) dataset.

\(\diamond\) Hyperparameter ground truth and the model parsing performance for each GM

## Appendix A Predictable Hyperparameters Introduction

We investigate \(37\) hyperparameters that exhibit the predictability according to Asnani _et al._[2]. These hyperparameters are categorized into three groups: (1) Loss Function (Tab. 4), (2) Discrete Architecture Hyperparameters (Tab. 5), (3) Continuous Architecture Hyperparameters (Tab. 6). We report our proposed method performance of parsing hyperparameters in these three groups via Fig. 6(a), Fig. 6(b), and Fig. 6(c), respectively. Moreover, in the main paper's Eq. 8 and Fig. 5(a), we employ the assignment hierarchy \(\mathbf{M}^{s}\) to group different hyperparameters together, which supervises the learning of the matching matrix \(\mathbf{M}\). The construction of such the \(\mathbf{M}^{s}\) is also based on Tab. 4, 5, and 6, which not only define three coarse-level categories, but also fine-grained categories such as pixel-level objective (loss) function (_e.g._, L1 and MSE) in Tab. 4, and normalization methods (_e.g._, ReLu and Tanh) as well as nonlinearity functions (_e.g._, Layer Norm. and Batch Norm.) in Tab. 5.

## Appendix B Training and Implementation Details

Training DetailsGiven the directed graph \(\mathbf{A}\in\mathbb{R}^{C\times C}\), which contains \(C\) graph nodes. We empirically set \(C\) as \(55\), as mentioned in the main paper Sec. 3.3. In the training, LGPN takes the given image \(\mathbf{I}\) and output the refined feature \(\mathbf{V}\in\mathbb{R}^{55\times D}\), which contains learned features for each graph node, namely, \(\mathbf{V}=\{\mathbf{v}_{0},\mathbf{v}_{1},...,\mathbf{v}_{54}\}\). As a matter of fact, we can view \(\mathbf{V}\) as three separate sections: \(\mathbf{V}^{l}=\{\mathbf{v}_{0},\mathbf{v}_{1},...,\mathbf{v}_{9}\}\), \(\mathbf{V}^{d}=\{\mathbf{v}_{10},\mathbf{v}_{11},...,\mathbf{v}_{27}\}\), and \(\mathbf{V}^{c}=\{\mathbf{v}_{28},\mathbf{v}_{29},...,\mathbf{v}_{54}\}\), which denote learned features for graph nodes of \(10\) loss functions (_e.g._, L1 and MSE), \(18\) discrete architecture hyperparameter (_e.g._, Batch Norm. and ReLU), and \(9\) continuous architecture hyperparameter (_i.e._, Parameter Num.), respectively. Note \(\mathbf{V}^{c}\) represents learned features of \(9\) continuous architecture hyperparameters because each continuous hyperparameter is represented by \(3\) graph nodes, as illustrated in Fig. 0(c) of the main paper. Furthermore, via Eq. 7 in the main paper, we use \(\mathbf{V}\) to obtain the corresponding probability score \(\mathbf{p}=\{p_{0},p_{1},...,p_{54}\}\) for each graph node. Similar to \(\mathbf{V}\), this \(\mathbf{p}\) can be viewed as three sections: \(\mathbf{p}^{l}\in\mathbb{R}^{10}\), \(\mathbf{p}^{d}\in\mathbb{R}^{18}\) and \(\mathbf{p}^{c}\in\mathbb{R}^{27}\) for loss functions, discrete architecture hyperparameters, continuous architecture hyperparameters, respectively. In the end, we use \(\mathbf{p}\) to help optimize LGPN via the graph node classification loss (Eq. 7). After the training converges, we further apply individual fully connected layers on the top of frozen learned features of continuous architecture hyperparameters (_e.g._, \(\mathbf{V}^{c}\)). via minimizing the \(\mathcal{L}_{1}\) distance between predicted and ground truth value.

In the inference (the main paper Fig. 5(b)), for loss function and discrete architecture hyperparameters, we use output probabilities (_e.g._, \(\mathbf{p}^{l}\) and \(\mathbf{p}^{d}\)) of discrete value graph nodes, for the binary "used v.s. not" classification. For the continuous architecture hyperparameters, we first concatenate learned features of corresponding graph nodes. We utilize such a concatenated feature with pre-trained, fully connected layers to estimate the continuous hyperparameter value.

Model Parsing Implementation DetailsDenote the output feature from the Generation Trace Capturing Network as \(\mathbf{f}\in\mathbb{R}^{2048}\). To transform \(\mathbf{f}\) into a set of features \(\mathbf{H}=\{\mathbf{h}_{0},\mathbf{h}_{1},...,\mathbf{h}_{54}\}\) for the \(55\) graph nodes, \(55\) independent linear layers (\(\Theta\)) are employed. Each feature \(\mathbf{h}_{i}\) is of dimension \(\mathbb{R}^{55\mathbb{Z}}\). The \(\mathbf{H}\) is fed to the GCN refinement block, which contains \(5\) GCN blocks, each of which has \(2\) stacked GCN layers. In other words, the GCN refinement block has \(10\) layers in total. We use the correlation graph \(\mathbf{A}\in\mathbb{R}^{55\times 55}\) (Fig. 10) to capture the hyperparameter dependency and during the training the LGPN pools \(\mathbf{A}\) into \(\mathbf{A}_{1}\in\mathbb{R}^{18\times 18}\) and \(\mathbf{A}_{2}\in\mathbb{R}^{6\times 6}\) as the Fig. 2 of the main paper. The LGPN is implemented using the PyTorch framework. During training, a learning rate of 3e-2 is used. The training is performed with a batch size of \(400\), where \(200\) images are generated by various GMs and \(200\) images are real.

**Implementation Details for Detection and Attributions** We validate GTC's effectiveness in capturing the generation trace in Fig. 6. Specifically, Fig. 8 shows the detailed implementation. We employ FC layers to convert output feature \(\mathbf{f}\in\mathbb{R}^{2048}\) to \(\mathbf{f}_{det.}\in\mathbb{R}^{2}\) and \(\mathbf{f}_{att.}\in\mathbb{R}^{5}\) for CNN-generated image detection and image attribution respectively.

## Appendix C Additional Results

We report detailed performance on RED\(116\) via Tab. 8, demonstrating that our proposed LGPN achieves the best performance on both datasets. Also, in Fig. 4(a) of the main paper, we visualize

\begin{table}
\begin{tabular}{c|c|c} \hline \hline Category & Range & Discrete Architecture Hyperparameters \\ \hline \multirow{6}{*}{Layer Number} & \((0{--}717)\) & Layers Number \\  & \([0{--}289]\) & Convolution Layer Number \\  & \([0{--}185]\) & Fully-connected Layer Number \\  & \([0{--}46]\) & Pooling Layer Number \\  & \([0{--}235]\) & Normalization Layer Number \\  & \((0{--}20]\) & Layer Number per Block \\ \hline \multirow{2}{*}{Unit Number} & \((0{--}8,365]\) & Filter Number \\  & \((0{--}155]\) & Block Number \\  & \((0{--}56,008,488]\) & Parameter Number \\ \hline \end{tabular}
\end{table}
Table 6: Continuous Architecture Hyperparameters used by all GMs, where "[" denotes inclusive and ”(" denotes exclusive intervals. We report the range for \(9\) continuous hyperparameters.

\begin{table}
\begin{tabular}{c|c} \hline \hline Category & Loss Function \\ \hline \hline \multirow{4}{*}{Pixel-level} & \(L_{1}\) \\  & \(L_{2}\) \\  & Mean squared error (MSE) \\  & Maximum mean discrepancy (MMD) \\  & Least squares (LS) \\ \hline \multirow{4}{*}{Discriminator} & Wasserstein loss for GAN (WGAN) \\  & Kullback–Leibler (KL) divergence \\  & Adversarial \\  & Hinge \\ \hline Classification & Cross-entropy (CE) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Loss Function types used by all GMs. We group the \(10\) loss functions into three categories. We use the binary representation to indicate the presence of each loss type in training the respective GM.

\begin{table}
\begin{tabular}{c|c} \hline \hline Category & Discrete Architecture Hyperparameters \\ \hline \multirow{4}{*}{Normalization} & Batch Normalization \\  & Instance Normalization \\  & Adaptive Instance Normalization \\  & Group Normalization \\ \hline \multirow{4}{*}{Nonlinearity in the Last Layer} & ReLU \\  & Tanh \\  & Leaky\_ReLU \\  & Sigmoid \\  & SiLU \\ \hline \multirow{4}{*}{Nonlinearity in the Last Block} & ELU \\  & ReLU \\  & Leaky\_ReLU \\  & Sigmoid \\  & SiLU \\ \hline \multirow{2}{*}{Up-sampling} & Nearest Neighbour Up-sampling \\  & Deconvolution \\ \hline Skip Connection & Feature used \\ \hline Down-sampling & Feature used \\ \hline \hline \end{tabular}
\end{table}
Table 5: Discrete Architecture Hyperparameters used by all GMs. We group the \(18\) discrete architecture hyperparameters into \(6\) categories. We use the binary representation to indicate the presence of each hyperparameter type in training the respective GM.

the correlation graph similarities among different GMs in the first test set. In this section, we offer a similar visualization (_e.g.,_ Fig. 9 of the supplementary) for other test sets. In the main paper's Sec 3.3, we use \(n\) graph nodes for each continuous hyperparameter and \(n\) is set as \(3\). Tab. 7 shows the advantage of choice, which shows the lowest \(\mathcal{L}_{1}\) regression error is achieved when \(n\) is \(3\).

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline \(n\) Value & \(2\) & \(3\) & \(4\) & \(5\) & \(6\) \\ \hline L1 Error & \(0.123\) & \(0.120\) & \(0.124\) & \(0.132\) & \(0.130\) \\ \hline \end{tabular}
\end{table}
Table 7: Using different \(n\) graph nodes for the continuous hyperparameter regression.

Figure 7: (a) The F1 score on the loss function reveals that MMD and KL are two easiest loss functions to predict. (b) The F1 score on the discrete architecture hyperparameters demonstrates that predicting these hyperparameters is more challenging than predicting the loss function. This finding aligns with the empirical results reported in the previous work [2]. (c) The L1 error on the continuous architecture hyperparameters indicates that it is challenging to predict Block Num. and Filter Num..

Figure 8: We construct simple classifiers based on GTC. Then, we train these two classifiers for CNN-generated image detection and image attribution, respectively. Please note that GTC only leverages ImageNet pre-trained weights as the initialization, same as the previous method [62]. For a fair comparison, no model parsing datasets such as RED116 and RED140 are used for pre-training.

Figure 10: The initial correlation graph \(\mathbf{A}\) that we construct based on the probability table in Sec. 3.1 of the main paper. The optimum threshold we use is \(0.45\).

Figure 9: Each element of these two matrices is the average cosine similarities of \(2,000\) pairs of generated correlation graphs \(\mathbf{A^{\prime}}_{0}\) from corresponding GMs in the second, third and forth test sets.

\begin{table}
\begin{tabular}{l||c|c|c|c|c} \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c||}{**Continuous type**} & \multicolumn{2}{c}{**Discrete type**} \\ \cline{2-5}  & \(L_{1}\) error \(\downarrow\) & P-value \(\downarrow\) & Corr. coef. \(\uparrow\) & Coef. of det. \(\uparrow\) & F1 score \(\uparrow\) & Accuracy \(\uparrow\) \\ \hline \hline Random ground-truth & \(0.184\pm 0.019\) & \(0.006\pm 0.001\) & \(0.261\pm 0.181\) & \(0.315\pm 0.095\) & \(0.529\pm 0.078\) & \(0.575\pm 0.097\) \\ Mean/mode & \(0.164\pm 0.011\) & \(0.035\pm 0.005\) & \(0.326\pm 0.112\) & \(0.467\pm 0.115\) & \(0.612\pm 0.048\) & \(0.604\pm 0.046\) \\ No fingerprint & \(0.170\pm 0.035\) & \(0.017\pm 0.004\) & \(0.738\pm 0.014\) & \(0.605\pm 0.152\) & \(0.700\pm 0.032\) & \(0.663\pm 0.104\) \\ Using one parser & \(0.161\pm 0.028\) & \(0.032\pm 0.002\) & \(0.226\pm 0.030\) & \(0.512\pm 0.116\) & \(0.607\pm 0.034\) & \(0.593\pm 0.104\) \\ FEN-PN & \(0.149\pm 0.019\) & \(0.022\pm 0.007\) & \(0.744\pm 0.098\) & \(0.612\pm 0.161\) & \(0.718\pm 0.036\) & \(0.706\pm 0.040\) \\ Ours & \(0.130\pm 0.011\) & N/A & \(0.833\pm 0.098\) & \(0.732\pm 0.177\) & \(0.743\pm 0.033\) & \(0.755\pm 0.030\) \\ \hline \end{tabular}
\end{table}
Table 11: Performance of architecture hyperparameters prediction. We use \(L_{1}\) error, p-value, correlation coefficient, and coefficient of determination for continuous type parameters. For discrete architecture hyperparameters, we use the F1 score and classification accuracy. The first value is the standard deviation across sets, while the second one is across samples. The p-value is estimated for every ours-baseline pair. [KEYS: corr.: correlation, coef.: coefficient, det.: determination]

\begin{table}
\begin{tabular}{c||c c||c} \hline \multirow{2}{*}{Threshold} & Loss & Dis. Archi. \\  & Function & Para. \\ \cline{2-3}  & \multicolumn{2}{c||}{F1/Accuracy \(\uparrow\)} \\ \hline \hline \(0.35\) & \(84.0/83.0\) & \(79.2/77.0\) \\
0.45 & \(\mathbf{84.6/83.3}\) & \(\mathbf{79.5/77.5}\) \\ \hline \(0.55\) & \(84.5/82.8\) & \(78.9/77.0\) \\
0.65 & \(82.7/82.5\) & \(77.0/74.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: More parsing performance with different thresholds constructing the correlation graph \(\mathbf{A}\).

\begin{table}
\begin{tabular}{c||c c||c|c} \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c||}{Loss} & Dis. Archi. \\  & Function & Para. \\ \cline{2-3}  & \multicolumn{2}{c||}{F1 \(\uparrow\)} & Acc. \(\uparrow\) & F1 \(\uparrow\) & Acc. \(\uparrow\) & L1 error \(\downarrow\) \\ \hline \hline Random GT [2] & \(0.636\) & \(0.716\) & \(0.529\) & \(0.575\) & \(0.184\) \\ FEN-PN [2] & \(0.813\) & \(0.792\) & \(0.718\) & \(0.706\) & \(0.149\) \\ FEN-PN\({}^{*}\)[2] & \(0.801\) & \(0.811\) & \(0.701\) & \(0.708\) & \(0.146\) \\ \hline \hline GTC \(w\) MLP & \(0.778\) & \(0.801\) & \(0.689\) & \(0.701\) & \(0.169\) \\ GTC \(w\) Stacked -GCN & \(0.790\) & \(0.831\) & \(0.698\) & \(0.720\) & \(0.145\) \\ LGPN & \(\mathbf{0.841}\) & \(\mathbf{0.833}\) & \(\mathbf{0.727}\) & \(\mathbf{0.755}\) & \(\mathbf{0.130}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: The model parsing performance on RED\(116\). In the last row, our proposed LGPN that contains GTC and GCN Refinement block, which achieves the best model parsing performance in all metrics. [**Key**: GCN refine.: GCN refinement block; **Bold**: best.].

\begin{table}
\begin{tabular}{c||c|c|c|c} \hline \multirow{2}{*}{**Threshold**} & \multicolumn{2}{c||}{Loss} & Dis. Archi. \\  & Function & Para. \\ \cline{2-3}  & \multicolumn{2}{c||}{F1/Accuracy \(\uparrow\)} \\ \hline \hline \(0.35\) & \(84.0/83.0\) & \(79.2/77.0\) \\
0.45 & \(\mathbf{84.6/83.3}\) & \(\mathbf{79.5/77.5}\) \\ \hline \(0.55\) & \(84.5/82.8\) & \(78.9/77.0\) \\
0.65 & \(82.7/82.5\) & \(77.0/74.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: More parsing performance with different thresholds constructing the correlation graph \(\mathbf{A}\).

Figure 11: We report detailed model parsing results on different GMs in each test set. These results include loss function and discrete architecture hyperparameter prediction accuracy, as well as the L1 error on the continuous architecture hyperparameter prediction. Specifically, (a), (b), (c), and (d) are the performance for GMs in the \(1\)st, \(2\)nd, \(3\)rd, and \(4\)th test sets, respectively.

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We propose a model parsing algorithm, which is claimed in both the abstract and introduction sections. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation of our proposed method at the end of the paper, which helps the deeper understanding. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide extensive empirical results and detailed insights that justify each of our contribution statements. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes. We include an extensive supplementary that includes the dataset description, implementation details, and detailed performance on each generative model and hyperparameters. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes. We will release the source code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We offer the detailed description regarding the experimental setups in this work. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our proposed method uses different metrics, such as ROCAUC, F1 score, accuracy, and average precision, to report the performance. This helps show the statistical significance of the performance difference between the previous work and ours. Also, considering the large number of experiments, including error bars for all of them is computationally expensive. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We indicate the computation resources and architecture implementations in the supplementary. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We discuss the potential harms and societal impact at the end of our work, following NeurIPS code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]Justification: We include societal impact contents at the end of our work, following NeurIPS code of ethics. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The dataset consists of digital images, which do not need safeguards. This is not applicable to our work. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Both the RED116 and RED140 datasets used in this work are publicly available and can be used for research purposes. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We will release the dataset and code based on the acceptance. They are well-documented. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does involve crowdsourcing and human subjects' biometric information. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [No]Justification: Due to the page limit, we provide societal impact here: We strongly advocate for the machine learning community to actively work towards mitigating the potential negative societal implications of research. It is possible that generated face image may leak the identity information of subjects in the training set of a GM who do not sign the consent form. We shall strive to work on real face imagery whose collection is reviewed by an Institutional Review Board (IRB).

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.