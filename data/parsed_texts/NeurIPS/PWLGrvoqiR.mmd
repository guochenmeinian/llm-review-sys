# RaLEs: a Benchmark for Radiology Language Evaluations

 Juan M Zambrano Chaves

Stanford University

jmz@stanford.edu

&Nandita Bhaskhar

Stanford University

&Maayane Attias

Stanford University

Jean-Benoit Delbrouck

Stanford University

&Daniel L. Rubin

Stanford University

&Andreas Loening

Stanford University

&Curtis Langlotz

Stanford University

&Akshay S. Chaudhari

Stanford University

akshaysc@stanford.edu

###### Abstract

The radiology report is the main form of communication between radiologists and other clinicians. Prior work in natural language processing in radiology reports has shown the value of developing methods tailored for individual tasks such as identifying reports with critical results or disease detection. Meanwhile, English and biomedical natural language understanding benchmarks such as the General Language Understanding and Evaluation as well as Biomedical Language Understanding and Reasoning Benchmark have motivated the development of models that can be easily adapted to address many tasks in those domains. Here, we characterize the radiology report as a distinct domain and introduce RaLEs, the Radiology Language Evaluations, as a benchmark for natural language understanding and generation in radiology. RaLEs is comprised of six natural language understanding and generation evaluations including the extraction of anatomical and disease entities and their relations, procedure selection, and report summarization. We characterize the performance of models designed for the general, biomedical, clinical and radiology domains across these tasks. We find that advances in the general and biomedical domains do not necessarily translate to radiology, and that certain more advanced models from the general domain can perform comparably to smaller clinical-specific models. The limited performance of existing pre-trained models on RaLEs highlights the opportunity to improve domain-specific self-supervised models for natural language processing in radiology. We propose RaLEs as a benchmark to promote and track the development of such domain-specific radiology language models. RaLEs is available at https://github.com/StanfordMIMI/RaLEs.

## 1 Introduction

Radiology reports convey a radiologist's interpretation of a medical image. The reports have characteristic content and structure that differentiate them from other types of text. Natural language processing of radiology reports can aid research efforts and ultimately lead to improved quality of care. However, many recent approaches focus on solving single radiology tasks, often report performance only on private datasets, fail to compare proposed methods with relevant baselines, and do not publish code or models [5]. Further, reports are typically not publicly available due to patient privacy concerns. The development of private, single-task models limits the measurement of progress in NLP for radiology broadly.

Model benchmarking in other domains, such as general English or biomedical text, has enabled thorough comparisons of existing methods across tasks that evaluate model performance and alignment with human evaluation [53; 20; 35]. This has promoted the development of state of the art models that can be adapted to address multiple tasks. Radiology reports are excluded from existing biomedical or clinical benchmarks, which often feature evaluations on datasets that do not reflect real-world clinical use cases. In contrast, 75% of current current FDA-approved AI applications target radiology ([18]), with a potential for impact on over 700 million studies (and associated reports) annually [39]. However, it is unclear how advances in other domains translate to the unique domain of radiology language and reports.

To address the aforementioned challenges, in this work we develop RaLEs, a benchmark for evaluations of natural language understanding (NLU) and generation (NLG) in the radiology domain. Our main contributions are:

1. Curating a set of 6 datasets across 4 tasks, all of which are publicly available. Among these datasets, 1 is newly created and introduced in this work (procedure selection), and 1 is newly de-identified and released into the public domain (Stanza radiology named entity recognition).
2. We benchmark and report RaLEs multi-metric performance on 16 models from the general, biomedical, clinical and radiology domains. We find that on average, clinical and radiology-specific models outperform general and biomedical models by 1.5 and 0.5% on preferred metrics.
3. We consolidate progress by developing RaLEs NLU and NLG scores, and release code for dataset standardization, model fine-tuning and evaluation to spur future benchmarking.

## 2 Related work

### Radiology natural language processing

Prior work has reviewed the status of NLP in radiology. In a systematic review, Casey et al. [5] identified that <20% of previously published radiology NLP methods used deep learning, with the majority relying on other machine learning or rule-based systems. A significant portion of these studies primarily concentrated on tasks like information extraction (accounting for 45%) and classification (making up 50%). Regarding reproducibility, only 14 and 15 of the 164 studies reviewed made their data and code available, respectively. Fewer than 20% of studied compared proposed methods with alternate approaches. In a separate study, Pons et al. [44] reported that 20 of 67 studies reported operational use, the majority of which were not intended for integration into a clinical workflow.

Most existing applications of NLP in radiology can be framed as NLU or NLG tasks. These include information extraction (named entity recognition and/or relation extraction), text classification, and text summarization [40; 38]. These methods can enable applications such as extracting labels from reports to annotate images [26; 48], automatic image protocol selection, defining patient cohorts, monitoring appropriate use and clinical follow-up of medical images, and summarizing prior imaging studies [40; 38].

### Benchmarks in other domains

Benchmarks that systematically compare the performance of existing models in the general English domain, such as GLUE or SuperGLUE [53; 52], have enabled comparisons of models across a variety of NLU tasks, promoting the development of pre-trained models that can be adapted to a variety of tasks using transfer learning or other adaptation methods. Other benchmarks have been proposed for biomedical language understanding and reasoning, such as the BLURB Benchmark [20], which contains 13 datasets and 6 tasks, focusing on the performance of systems across scientific biomedical text. The Biomedical Language Understanding Evaluation (BLUE) benchmark [43], includes 6 biomedical scientific text datasets and 4 clinical datasets evaluating sentence similarity, named entity recognition, relation extraction, document classification and inference. While comprehensive, the BLUE benchmark does not include any radiology text-based tasks. This highlights a broader issue: no existing benchmarks broadly compare the performance of models in a curated set of radiology tasks, which makes it challenging to quantify the efficacy of general-purpose language models on domain-specific radiology tasks.

Radiology reports as a domain

Three key characteristics define the domain of radiology reports: a) content, b) context and c) closed source. In terms of content, radiology reports contain a limited specialized vocabulary, often existing only in the context of images. For example, words like _pneumothorax_, _cardiomegaly_, and _radiodensity_, referring to air in the space surrounding the lungs, an enlarged heart, and opacity to X-rays, are frequently found in radiology reports. Such words are contained in Radiology Lexicon1, a comprehensive set of radiology terms that contains approximately 30,000 entries. An additional aspect of content involves structure. Radiology reports typically follow a document structure comprised of a header containing patient history and exam-related information, followed by mentions of relevant comparison studies, details about the imaging technique utilized, a detailed description of image findings, and an impression that summarizes findings contextualized to the patient's condition [30; 23]. Sentences within reports are typically short, declarative and factual, and are written in the present tense. The content (unique vocabulary, format, and narrative style) of radiology reports is an important feature of this specialized domain.

Footnote 1: https://www.radlex.org

Another aspect that characterizes this domain is the context surrounding radiology reports. Reports only exist in the presence of an accompanying medical image. Typically they exist within the context of electronic medical records, which are collections of documents, images and other signals that describe a patient's medical history. The content of these reports is embedded within the current radiology and medical knowledge.

Finally, due to existing concerns and regulations that protect patient health information, the vast majority of radiology reports exist within private data warehouses, often requiring institutional review board approval for access. The largest publicly available collections of reports, such as MIMIC-III, MIMIC-CXR, PadChest, and Open-i datasets [29; 28; 4; 15], have undergone de-identification and are typically made available subject to agreement to terms of data use protecting patient privacy.

## 4 RaLEs: Radiology Language Evaluations

The following sections outline RaLEs. The tasks and datasets were selected to reflect real-world use cases, ensuring they are both challenging and achievable. We outline multiple metrics of success for well-rounded evaluation for each dataset-task pair, as well as differentiate the performance of the model in data from institutions unseen during training where available.

### Tasks and datasets

Table 1 summarizes tasks and datasets within RaLEs. We include four tasks as part of the initial RaLEs: named entity recognition (NER), relation extraction (RE), document classification and document summarization. The datasets selected for each task are described as follows.

**RadGraph**[27] consists of 600 manually annotated chest x-ray reports from MIMIC-CXR and CheXpert datasets [28; 26]. A board-certified radiologist labeled the reports with named entities, consisting of _Observation - definitely present_, _Observation - definitely absent_, _Observation - uncertain_, and _Anatomy_, reflecting key entities in reports corresponding to observations (which may be negated or hedged) as well as anatomical structures. Pairs of entities may be related by one of three types: _suggestive of_, _located at_, and _modify_. We use this dataset to evaluate models on NER and RE. Test

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Category** & **Dataset** & **Task** & **\# Train/Dev/Test** & **Anatomy/Modality** & **\# Sources** & **New** \\ \hline \multirow{5}{*}{NLU} & RadGraph [27] & RE & 425 / 75 / 100 & Chest / XR & 2 & \\  & RadGraph [27] & NER & 425 / 75 / 100 & Chest / XR & 2 & \\  & RadSpRL [11] & RE & 848 / 105 / 106 & Chest / XR & 1 & \\  & Stanza Radiology [58] & NER & 2,461 / 200 / 295 & Chest / CT & 3 & \\  & CT Procedure Selection & Clf. & 58,091 / 19,364 / 19,364 & Varied / CT & 1 & \\ \hline \multirow{2}{*}{NLG} & MEDIOA 2021[3] & Summ. & 91,544 / 4,000 / 600 & Chest / XR & 2 & \\  & BioNLP 2023[14] & Summ. & 59,320 / 7,413 / 13,057 & Varied / CT, MR & 1 & \\ \hline \hline \end{tabular}

* XR=X-ray, CT=Computed Tomography, MR=Magnetic Resonance Imaging, NLU=Natural Language Understanding, NLG=Natural Language Generation, New= newly released with this work, Varied=Abdomen, Pelvis, Neck, Spine, Head, etc., see A and B

\end{table}
Table 1: Overview of datasets and tasks in RaLEs.

set evaluations are carried out separately on MIMIC-CXR and CheXpert, reflecting in and out of distribution performance, respectively.

**RadSpRL**[11] consists of 2000 manually annotated chest x-ray reports from the Open-i dataset [15]. A medical librarian and an MD annotated entities and relations that represent spatial relations. Spans of text were annotated as a relationship between a _Spatial indicator_ with another span of text consisting of one of four spatial roles: _Trajector_, _Landmark_, _Hedge_, and _Diagnosis_. We evaluate models on RE with this dataset. We use only documents with labeled relations for training and evaluation. Though prior performance is reported on cross-validation sets, we create a fixed train/dev/test split on the report level to limit excessive compute requirements of hyperparameter exploration across each split for each model.

**Stanza Radiology**[58] consists of 150 manually annotated chest computed tomography reports from three hospitals. Two radiologists annotated spans of text in the reports with five entity types: _Anatomy_, _Observation_, _Anatomy modifier_, _Observation modifier_, and _Uncertainty_. This dataset is used for NER evaluations. As part of RaLEs, we have deidentified these reports using a publicly available hidden-in-plain-sight de-identification algorithm [7], and release this previously private dataset to the public.

**MIMIC-III procedure selection** is a newly created dataset, released alongside this work, that consists of 96,819 documents extracted from individual computed tomography reports from the MIMIC-III dataset [29]. The reason for exam and procedure title were extracted from each report using regular expressions. The task consists of appropriately classifying reason for exam documents into one of 46 normalized procedure titles. The procedure titles were normalized to a standardized vocabulary [50] by manually mapping a set of extracted procedure titles to the vocabulary. Normalization was carried out by an MD and a board-certified radiologist. Additional details of the curation of this dataset are in A. This evaluation was developed to simulate the selection of a procedure given a clinician provided reason for exam, a task that often requires expert human oversight in current practice.

**MEDIQA 2021 report summarization**[3] consists of 96,144 chest X-ray reports with extracted _Findings_ and _Impression_ sections. The task is summarizing the _Findings_ section of reports, using the _Impression_ as ground truth. Models are trained using reports from one dataset (MIMIC-CXR) and validated using reports from MIMIC-CXR and the Open-I dataset (from Indiana). The test evaluation is carried out on reports from an institution seen during validation (Indiana), as well as an institution present only in the test set (Stanford), which aims to measure out-of-domain generalization.

**BioNLP 2023 report summarization**[14] consists of 79,790 multi-modal reports [8] extracted from the MIMIC-III dataset [29] that are separated into _Findings_ and _Impression_ sections. The task is to create a summary in the same fashion as MEDIQA 2021. The reports are of computed tomography and magnetic resonance imaging examinations, with head, chest, abdomen, spine and sinuses present as different anatomies (Details in B. Test evaluations include anatomies/modalities unseen in training data.

### Evaluation strategy

#### 4.2.1 Models

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Domain** & **Model** & **\# Params** \\ \hline \multirow{3}{*}{English} & BERT & [110, 340] \\  & RoBERTa & [125, 355] \\  & ELECTRA & [14, 110, 345] \\  & DeBERTa-v3 & [86, 304] \\ \hline \multirow{3}{*}{Biomedical} & PubMedBERT & 110 \\  & BioLinkBERT & [110, 340] \\ \hline \multirow{3}{*}{Clinical} & BioClinicalBERT & 110 \\  & GatorTron & 345 \\ \hline \multirow{3}{*}{Radiology} & RadBERT\({}_{1}\) & 110 \\  & RadBERT\({}_{2}\) & 125 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Masked language models evaluated. Number of parameters in millions.

We evaluate various pre-trained masked language models from the general, biomedical, clinical and radiology domains. Table 2 lists the models evaluated. We refer the reader to the respective publication for details on the pretraining strategy including source of vocabulary, corpus and model-specific optimizations.

From the general English domain, where models are trained using sources such as Wikipedia and Google Books, we evaluate BERT [17] in its base and large configurations, RoBERTa [36] in its base and large configurations, ELECTRA [9] in its small, base and large configurations, and DeBERTa-v3 [24] in its base and large configuration.

From the biomedical domain, we examine models pre-trained on scientific text contained in PubMed: PubMedBERT-base [19], which pretrains a BERT-base model using biomedical scientific abstracts and full-text articles as a corpus, and BioLinkBERT-base and large [56], which uses a similar pretraining corpus but incorporates an additional pretraining objective consisting of identifying document links.

From the clinical domain, we evaluate BioClinicalBERT [2], a BioBERT [34] biomedical model (i.e., a BERT-base model continually pretrained on PubMed text), which is further continually pretrained on MIMIC-III clinical notes. In addition, we examine GatorTron-base[55], a MegaTronBERT model [47] pretrained using clinical notes from the University of Florida, PubMed text and Wikipedia articles (500GB of text).

Finally, we examine the performance of models developed specifically for the radiology domain, both named RadBERT by their creators. One model, which we refer to as RadBERT\({}_{1}\), corresponds to a BioBERT model continually pretrained on 4 million radiology reports from Stanford Health Care [6]. The second model, which we refer to as RadBERT\({}_{2}\), corresponds to a BioMed-RoBERTa (RoBERTa-base model continually pretrained on 2.7 million scientific papers [22] ) model further pretrained on 4.4 million radiology reports from various facilities of the U.S. Department of Veterans Affairs health system.

#### 4.2.2 NLU evaluations

We fine-tune each model using all documents in training split and perform task-specific hyperparameter optimization as detailed in C. We select the best model according to the best run preferred metric on the validation set, as defined by each dataset. We report the performance of models on test sets using the best model for each model type/task. We employ different fine-tuning strategies for each task, described as follows.

For RE, datasets (RadGraph, RadSpRL), we use DyGIE++ [51], a multi-task NER and RE framework that learns a dynamic graph that models relationships between text spans. Span representations are obtained from a language model embedding. We use DyGIE++ as a NER extraction method for RadGraph models as we empirically observed improved performance compared to the Stanza NER approach which cannot leverage the relation labels. For document classification, a randomly initialized head of dimensions \((h,c)\) where \(h\) corresponds to the hidden size and \(c\) corresponds to the number of classes, is added as a final layer to classify the classification [CLS] token. For Stanza NER, a randomly initialized head of dimensions \((h,c)\) is added to classify each token. For words comprised of multiple sub-word tokens, the label is assigned to the first token.

Consistent with prior evaluations for existing datasets, the preferred metric for all evaluations is the micro-averaged F1 score. For the newly created procedure selection task, we select accuracy as the preferred metric as it more closely reflects success in clinical settings. We report a RaLEs NLU score as the average of the preferred metrics across the NLU datasets. In addition, we perform label-stratified sampling evaluations using 1% and 10% labels during training and validation to assess how dataset scaling across domain-specific and domain-agnostic models affects their performance. We also examine separability of representations generated by each model by keeping language model weights frozen and training only a linear probe, or only the graph layers in the case of DyGIE++ models. Furthermore, we evaluate models in metrics that may be relevant for deployment, focusing on model calibration and uncertainty. We include in RaLEs metrics measuring calibration and uncertainty, prediction quality, and information criteria. We detail the results for the document classification (procedure selection) task on these metrics in Appendix D. Model fine-tuning is performed in private infrastructure using a single NVIDIA TITAN RTX or RTX A6000 GPU. For each model, hyperparameter exploration takes on average 1 hour for DyGIE++ models, 1 hour for Stanza NER, and 3 hours for the procedure selection task (in total approximately 400 GPU hours).

[MISSING_PAGE_FAIL:6]

obtains improvements in both in GLUE as well as in RaLEs NLU. Similarly, though BioLinkBERT and PubMedBERT share similar scientific pretraining corpora, the document relation prediction objective of BioLinkBERT leads to improved performance in both BLURB as well as RaLEs NLU.

However, improvements in performance in other benchmarks, such as GLUE or BLURB, do not directly translate to improvements in RaLEs NLU. This is illustrated in Figure 3 which shows that 10 percentage point improvements in GLUE or BLURB lead to a 1-2 percent improvement in RaLEs NLU performance. We hypothesize that the decrease in benefit stems from the domain characteristics of radiology that differentiate it from other text, as proposed in Section 3. Furthermore, we observe that existing radiology domain adaptation hurts alternate domain performance, exemplified by the relatively low BLURB score for RadBERT\({}_{1,2}\) models seen in Figure 3. Finally, we find that given a fixed architecture, an increase in parameter count (\(O\)(300M) vs \(O\)(100M)) does not lead to improved overall performance. This observations holds for both English and biomedical models, which are the only ones publicly available in different sizes. Similar results have been previously observed in other domains [20; 45], though the impact of base model size on domain adaptation is yet to be systematically studied.

### Nlg

Table 4 presents the results for the report summarization task, with prior results referenced for comparison. Similarly to NLU results, no model is consistently superior across all metrics. GatorTron, the model from the clinical domain, outperforms the other evaluated fine-tuned models slightly on lexical similarity (ROUGE-2 and ROUGE-L). Using these metrics as reference, the encoder-decoder framework as implemented here performs inferiorly to the best existing performing models. The best MEDIQA 2021 prior model [10], in addition to an abstractive-summarization-specific architecture, uses a domain adaptation module to improve performance on Indiana reports. However, as can be seen from our evaluation on fully held out Stanford reports in Appendix G, most of the benefits of such specialized approaches may not generalize to sites not seen during training. Further, our approach seems to favor factual correctness, with BioLinkBERT and GatorTron models having the best performance according to the CheXbert and RG metrics. RadAdapt [49], a label efficient adaptation of a clinical large language model, matches the overall NLG performance of our best fine-tuned model. As the RadAdapt authors recognize, however, it is unclear to what extent its

Figure 1: Model size and domain vs RaLEs NLU performance

performance is affected by possible leakage of testing data during the base model pre-training stage. We aim to further evaluate other large language models in future versions of RaLEs.

Figure 3: General (English, left) and biomedical (right) benchmark vs RaLEs NLU performance.

Figure 2: RaLEs NLU performance by model domain. Values are averaged across model domain, error bars are standard deviation. There are no statistically significant pairwise differences between model categories within the same label availability, as determined by Mann-Whitney U-tests with Bonferroni correction.

### Ethical considerations

RaLEs provides a framework for benchmarking advances in radiology NLP. The current version of RaLEs uses the RadSpRL dataset, which has a CC BY 4.0 license. Datasets stemming from MIMIC, including RadGraph, have a PhysioNetCredentialed Health Data License 1.5.0 which prohibit commercial use, data sharing and patient or institution identification attempts. For the newly released Stanza NER dataset, the dataset will be accompanied by an analogous Research Use Agreement following institutional review board approval, which was obtained to access the reports. For all datasets we have followed the appropriate research use, have not attempted re-identification of individuals, and provide instructions for data access in the accompanying code. Demographic characteristics of individuals included in RadGraph [27] and MIMIC-III [29] datasets are described in their original publications. Demographic characteristics for the Stanza NER [58], MEDIQA 2021 [3] and Indiana [16] dataset (from which the RadSpRL dataset is derived) are not reported since the radiology reports have been de-identified and these characteristics are not otherwise available. We do not foresee any potential risks following appropriate use of RaLEs as a tool to measure progress in NLP research. We strongly discourage the use of models trained using our framework for clinical care or advice without adequately studying the performance and limitations on specific patient populations that reflect intended use.

### Limitations

An important limitation of our analysis stems from the limited availability of publicly available radiology reports and datasets. This is consistent with prior observations that healthcare algorithms trained on US patient data rely on data from a handful of states [32]. While we release a new dataset from a new institution, future efforts should promote the availability of training and/or evaluation datasets from additional institutions, geographic locations, and languages. Aside from geographic diversity, we note that most publicly available reports and evaluations focus on chest X-ray reports which tend to be shorter and simpler than reports of other modalities and anatomies (see example lengths in Table 13). Our newly introduced procedure selection dataset expands the scope of current data by focusing on a different modality (computed tomography) across all anatomies. Finally, while we present extensive evaluations across models of different domains, we intend RaLEs to be a dynamic benchmark, with expansions across newer datasets, tasks, and model evaluations (including adding newer models and estimates of variance of performance).

## 6 Conclusion

Radiology reports are defined by their content, context and restricted access. RaLEs defines a benchmark for measuring progress in NLP in the radiology domain, focusing on multifaceted evaluations that reflect real-world use cases in radiology research and/or practice. The results showed that no single model, including existing radiology-specific domain adapted models, outperforms others across all evaluations.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{MEDIQA 2021} & \multicolumn{4}{c}{BioNLP 2023} \\ \cline{2-9}
**Model** & **R-2** & **R-L** & **CheXbert** & **RG** & **R-2** & **R-L** & **RG** & **NLG score** \\ \hline ELECTRA\({}_{\text{base}}\) &.238 &.381 &.710 &.378 &.156 &.274 &.229 &.316 \\ BioLinkBERT\({}_{\text{base}}\) &.245 &.388 & **.725** &.391 &.183 &.297 &.272 &.337 \\ GatorTron &.250 &.386 &.719 & **.406** &.189 &.303 &.283 & **.345** \\ RadBERT\({}_{2}\) &.237 &.382 &.709 &.381 &.184 &.300 &.271 &.334 \\ RadAdapt\$ & **.253** & **.393** & - &.345 & **.212** & **.324** & **.342** & **.345** \\ RadiologyGPT\$ &.074 &.148 &.601 &.135 &.127 &.209 &.242 &.184 \\ \hline Prior SOTA\(\dagger\) & **.436** & **.557** &.718 & - & - & - & - \\ Prior Baseline\$ &.264 &.389 &.610 & - & - & - & - & - \\ \hline \hline \end{tabular}

* R-2/L: ROUGE 2/L, RG: F1-RadGraph, \(\dagger\):MEDIQA[10], \(\ddagger\):from [3], \(\lx@sectionsign\):our evaluation of [49] and [37]

\end{table}
Table 4: Summary of results for NLG tasks (abstractive report summarization).

Acknowledgements

We thank Dave Van Veen for his contribution providing results for RadAdapt summarization models. We also thank the institutions that have provided funding for this work. Research reported in this publication was made possible in part by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health under contracts 75N92020C0008 and 75N92020C00021, GE Healthcare, and Stanford Knight Hennessy Scholars.

## References

* [1]T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama (2019) Optuna: a next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Cited by: SS1.
* [2]E. Alsentzer, J. Murphy, W. Boag, W. Weng, D. Jindi, T. Naumann, and M. McDermott (2019) Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, Minneapolis, USA, pp. 72-78. External Links: Document Cited by: SS1.
* [3]A. Ben Abacha, Y. Mrabet, Y. Zhang, C. Shivade, C. Langlotz, and D. Demner-Fushman (2021) Overview of the MEDIQA 2021 shared task on summarization in the medical domain. In Proceedings of the 20th Workshop on Biomedical Language Processing, Online, pp. 74-85. External Links: Document Cited by: SS1.
* [4]A. Bustos, A. Pertusa, J. Salinas, and M. de la Iglesia-Vaya (2020) Padchest: a large chest x-ray image dataset with multi-label annotated reports. Medical image analysis66, pp. 101797. External Links: Document Cited by: SS1.
* [5]A. Casey, E. Davidson, M. Poon, H. Dong, D. Duma, A. Grivas, C. Grover, V. Suarez-Paniagua, R. Tobin, W. Whiteley, et al. (2021) A systematic review of natural language processing applied to radiology reports. BMC medical informatics and decision making21 (1), pp. 1-18. External Links: Document Cited by: SS1.
* [6]P. Chambon, T. S. Cook, and C. P. Langlotz (2022) Improved fine-tuning of in-domain transformer model for inferring covid-19 presence in multi-institutional radiology reports. Journal of Digital Imaging, pp. 1-14. External Links: Document Cited by: SS1.
* [7]P. J. Chambon, C. Wu, J. M. Steinkamp, J. Adleberg, T. S. Cook, and C. P. Langlotz (2022) Automated deidentification of radiology reports combining transformer and "hide in plain sight" rule-based methods. Journal of the American Medical Informatics Association. Cited by: SS1.
* [8]Z. Chen, M. Varma, X. Wan, C. Langlotz, and J. Delbrouck (2023) Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Toronto, Canada, pp. 469-484. External Links: Document Cited by: SS1.
* [9]K. Clark, M. Luong, Q. V. Le, and C. D. Manning (2020) Electra: pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, Cited by: SS1.
* [10]S. Dai, Q. Wang, Y. Lyu, and Y. Zhu (2021) BDKG at MEDIQA 2021: system report for the radiology report summarization task. In Proceedings of the 20th Workshop on Biomedical Language Processing, Online, pp. 103-111. External Links: Document Cited by: SS1.
* [11]S. Datta, Y. Si, L. Rodriguez, S. E. Shooshan, D. Demner-Fushman, and K. Roberts (2020) Understanding spatial language in radiology: representation framework, annotation, and spatial relation extraction from chest x-ray reports using deep learning. Journal of biomedical informatics108, pp. 103473.

* Delbrouck et al. [2022] Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, and Curtis Langlotz. 2022. Improving the factual correctness of radiology report generation with semantic rewards. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 4348-4360, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
* Delbrouck et al. [2022] Jean-benoit Delbrouck, Khaled Saab, Maya Varma, Sabri Eyuboglu, Pierre Chambon, Jared Dunnmon, Juan Zambrano, Akshay Chaudhari, and Curtis Langlotz. 2022. ViLMedic: a framework for research at the intersection of vision and language in medical AI. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pages 23-34, Dublin, Ireland. Association for Computational Linguistics.
* Delbrouck et al. [2023] Jean-Benoit Delbrouck, Maya Varma, Pierre Chambon, and Curtis Langlotz. 2023. Overview of the RadSum23 shared task on multi-modal and multi-anatomical radiology report summarization. In _The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks_, pages 478-482, Toronto, Canada. Association for Computational Linguistics.
* Demner-Fushman et al. [2016] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J McDonald. 2016. Preparing a collection of radiology examinations for distribution and retrieval. _Journal of the American Medical Informatics Association_, 23(2):304-310.
* Demner-Fushman et al. [2016] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J McDonald. 2016. Preparing a collection of radiology examinations for distribution and retrieval. _Journal of the American Medical Informatics Association_, 23(2):304-310.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_.
* Drug Administration [2023] Federal Drug Administration: FDA. 2023. Artificial intelligence and machine learning enabled medical devices.
* Gu et al. [2020] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domain-specific language model pretraining for biomedical natural language processing.
* Gu et al. [2022] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2022. Domain-specific language model pretraining for biomedical natural language processing. _ACM Transactions on Computing for Healthcare_, 3(1):1-23.
* Guo et al. [2017] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In _Proceedings of the 34th International Conference on Machine Learning (ICML)_.
* Gururangan et al. [2020] Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8342-8360, Online. Association for Computational Linguistics.
* Hartung et al. [2020] Michael P Hartung, Ian C Bickle, Frank Gaillard, and Jeffrey P Kanne. 2020. How to create a great radiology report. _RadioGraphics_, 40(6):1658-1670.
* He et al. [2021] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.
* Hu et al. [2022] Jinpeng Hu, Zhuo Li, Zhihong Chen, Zhen Li, Xiang Wan, and Tsung-Hui Chang. 2022. Graph enhanced contrastive learning for radiology findings summarization. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4677-4688.

* Irvin et al. [2019] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silivana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. 2019. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 590-597.
* Jain et al. [2021] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, et al. 2021. Radgraph: Extracting clinical entities and relations from radiology reports. _arXiv preprint arXiv:2106.14463_.
* Johnson et al. [2019] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. 2019. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. _Scientific data_, 6(1):1-8.
* Johnson et al. [2016] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9.
* Jr et al. [2009] Charles E Kahn Jr, Curtis P Langlotz, Elizabeth S Burnside, John A Carrino, David S Channin, David M Hovsepian, and Daniel L Rubin. 2009. Toward best practices in radiology reporting. _Radiology_, 252(3):852-856.
* Karn et al. [2022] Sanjeev Kumar Karn, Ning Liu, Hinrich Schutze, and Oladimeji Farri. 2022. Differentiable multi-agent actor-critic for multi-step radiology report summarization. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1542-1553.
* Kaushal et al. [2020] Amit Kaushal, Russ Altman, and Curt Langlotz. 2020. Geographic distribution of us cohorts used to train deep learning algorithms. _Jama_, 324(12):1212-1213.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_.
* Lee et al. [2020] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4):1234-1240.
* Liang et al. [2022] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_.
* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_.
* Liu et al. [2023] Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, et al. 2023. Radiology-gpt: A large language model for radiology. _arXiv preprint arXiv:2306.08666_.
* Luo and Chong [2020] Jack W Luo and Jaron JR Chong. 2020. Review of natural language processing in radiology. _Neuroimaging Clinics_, 30(4):447-458.
* Mahesh et al. [2022] Mahadevappa Mahesh, Armin J Ansari, and Fred A Mettler Jr. 2022. Patient exposure from radiologic and nuclear medicine procedures in the united states and worldwide: 2009-2018. _Radiology_, page 221263.
* Mozayan et al. [2021] Ali Mozayan, Alexander R Fabbri, Michelle Maneevese, Irena Tocino, and Sophie Chheang. 2021. Practical guide to natural language processing for radiology. _RadioGraphics_.
* Naeini et al. [2015] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrech. 2015. Obtaining well calibrated probabilities using bayesian binning. In _Proceedings of the Proceedings of the AAAI Conference on Artificial Intelligence_.
* Nixon et al. [2019] Jeremy Nixon, Mike Dusenberry, Ghassen Jerfel, Timothy Nguyen, Jeremiah Liu, Linchuan Zhang, and Dustin Tran. 2019. Measuring calibration in deep learning.

* Peng et al. [2019] Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. In _Proceedings of the 2019 Workshop on Biomedical Natural Language Processing (BioNLP 2019)_.
* Pons et al. [2016] Ewoud Pons, Loes MM Braun, MG Myriam Hunink, and Jan A Kors. 2016. Natural language processing in radiology: a systematic review. _Radiology_, 279(2):329-343.
* Qiu et al. [2022] Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and Kristina Toutanova. 2022. Evaluating the impact of model scale for compositional generalization in semantic parsing.
* Rothe et al. [2020] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for sequence generation tasks. _Transactions of the Association for Computational Linguistics_, 8:264-280.
* Shoeybi et al. [2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_.
* Smit et al. [2020] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren. 2020. Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1500-1519, Online. Association for Computational Linguistics.
* Veen et al. [2023] Dave Van Veen, Cara Van Uden, Maayane Attias, Anuj Pareek, Christian Bluethgen, Malgorzata Polacin, Wah Chiu, Jean-Benoit Delbrouck, Juan Zambrano Chaves, Curtis Langlotz, Akshay Chaudhari, and John Pauly. 2023. RadAdapt: Radiology report summarization via lightweight domain adaptation of large language models. In _The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks_, pages 449-460, Toronto, Canada. Association for Computational Linguistics.
* Vreeman et al. [2018] Daniel J Vreeman, Swapna Abhyankar, Kenneth C Wang, Christopher Carr, Beverly Collins, Daniel L Rubin, and Curtis P Langlotz. 2018. The loinc rsna radiology playbook-a unified terminology for radiology procedures. _Journal of the American Medical Informatics Association_, 25(7):885-893.
* Wadden et al. [2019] David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 5784-5789, Hong Kong, China. Association for Computational Linguistics.
* Wang et al. [2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. _SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems_. Curran Associates Inc., Red Hook, NY, USA.
* Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.
* Yan et al. [2022] An Yan, Julian McAuley, Xing Lu, Jiang Du, Eric Y Chang, Amilcare Gentili, and Chun-Nan Hsu. 2022. Radbert: Adapting transformer-based language models to radiology. _Radiology: Artificial Intelligence_, 4(4):e210258.
* Yang et al. [2022] Xi Yang, Nima Pour Nejatian, Hoo Chang Shin, Kaleb Smith, Christopher Parisien, Colin Compas, Mona Flores, Ying Zhang, Tanja Magoc, Christopher Harle, et al. 2022. Gatortron: A large clinical language model to unlock patient information from unstructured electronic health records. _medRxiv_.

* Yasunaga et al. [2022] Michihiro Yasunaga, Jure Leskovec, and Percy Liang. 2022. LinkBERT: Pretraining language models with document links. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8003-8016, Dublin, Ireland. Association for Computational Linguistics.
* Zhang et al. [2020] Yuhao Zhang, Derek Merck, Emily Tsai, Christopher D Manning, and Curtis Langlotz. 2020. Optimizing the factual correctness of a summary: A study of summarizing radiology reports. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 5108-5120.
* Zhang et al. [2021] Yuhao Zhang, Yuhui Zhang, Peng Qi, Christopher D Manning, and Curtis P Langlotz. 2021. Biomedical and clinical english model packages for the stanza python nlp library. _Journal of the American Medical Informatics Association_, 28(9):1892-1899.

MMIC-III procedure selection dataset

The MIMIC-III dataset consists of electronic health record data of individuals admitted to critical care units at a large tertiary care hospitals [29]. Within the data made in MIMIC-III are clinical notes, among which are 500,000+ radiology reports. Using regular expressions on the report titles, we select 97,304 containing _CT_ in the title. From the report header, the procedure title is extracted and procedure titles are grouped into a set of distinct procedure titles. These were manually reviewed by an MD and a board-certified radiologist, and mapped to the LOINC/RSNA Radiology Playbook2, a standardized naming and coding convention for 1,000 commonly performed radiology procedures. Each report was then labeled with an corresponding LOINC/RSNA procedure code. Procedure codes that appeared less than 100 times in the corpus were aggregated into an "Other category". This led to a total of 45 categories.

Footnote 2: https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon/procedure-names-radlex-playbook

To obtain the source data for each report, regular expressions were used to extract the _Clinical History_ section of the report. The extracted segment of text is used as a proxy for the _Reason for Exam_, the motivating clinical context expressed by the clinician when ordering a radiology procedure. The task consists of mapping a free-text _Reason for Exam_ to a corresponding procedure code.

A stratified sampling strategy based on the report label was used to obtain final train/dev/test splits consisting of 60/20/20 percent of the total reports, respectively.

Figure 4 illustrates the distribution of report labels in the dataset. We note that the majority of entries correspond to a minority of classes, with _CT Head without contrast_ being the most common class. Notably, there is a long tail of procedures, motivating the stratified sampling strategy. Examples of entries in the dataset include: _SEIZURES_\(\rightarrow\)_CT Head WO contr_, _please eval for residual stones_\(\rightarrow\)

Figure 4: Distribution of CT procedure codes in MIMIC-III procedure selection task.

_CT Abd+Pelvis WO contr_, and _Evaluate colon, status of ischemic colitis. \(\rightarrow\) CT Abd+Pelvis W contr IV._

In addition to stratifying by labels, our provided annotated data provides a separate dev and test sets, consisting of patients that do not appear in the train, or train \(\cup\) dev splits, respectively. The number of patients and documents in each set are detailed in Table 5. Performance of models on the set of previously unseen patients is further examined in I, where in general a decrease in performance compared to the overall test set is observed. We hypothesize that in application settings there may be a subset of patients seen during model training, as well as a subset of patients new to the institution. Therefore, it would be relevant to know how a model will perform on a mix of the two, as well as on unseen patients only as a measure of model generalizability. Thus, we provide two dev/test sets so that this difference can be kept in mind in the model design stage.

## Appendix B BioNLP Summarization Data

## Appendix C Additional experimental details

For each DyGIE++ model fine-tuning and evaluation, we leverage the implementation by the original authors3, which is based on the allennlp and Pytorch libraries. We perform a grid hyperparameter search varying the learning rate (1e-2 - 1e-4), weight decay (0-1), and fine-tuning vs. freezing the language model layers. We use a fixed seed, 2 hidden graph layers of dimension 150, and keep other hyperparameters as described in [51].

Footnote 3: https://github.com/dwadden/dygiepp

For classification and Stanza NER, we leverage the HuggingFace transformers library4. We perform hyperparameter optimization using a Tree-structured Parzen Estimator algorithm implemented by Optuna [1]. Using a fixed seed, we perform up to 10 trials varying the learning rate (1e-5 - 1e-4), number of epochs (3-5), batch size (32, 64, 128) and weight decay (1e-12 - 1e-1). We use the

\begin{table}
\begin{tabular}{l r r r r} \hline \hline
**Modality/Body Part** & \multicolumn{4}{c}{**Number of Reports**} \\ \cline{2-5}  & Train & Dev & Test & Total \\ \hline CT Abdomen-pelvis & 12791 & 1598 & 798 & 15187 \\ CT Chest & 10228 & 1277 & 639 & 12144 \\ CT Head & 25121 & 3139 & 1569 & 29829 \\ CT Neck & 911 & 113 & 56 & 1080 \\ CT Sinus & 0 & 0 & 633 & 633 \\ CT Spine & 4413 & 550 & 275 & 5238 \\ MR Abdomen & 0 & 0 & 530 & 530 \\ MR Head & 5850 & 730 & 365 & 6945 \\ MR Neck & 0 & 0 & 114 & 114 \\ MR Pelvis & 0 & 0 & 126 & 126 \\ MR Spine & 0 & 0 & 1410 & 1410 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Number of reports available in the BioNLP2023 report summarization task.

\begin{table}
\begin{tabular}{l r r} \hline \hline
**Set** & **n Patients** & **n Samples** \\ \hline Train & 19,811 & 58,091 \\ Dev & 11,467 & 19,364 \\ Dev\({}_{\texttt{new only}}\) & 2,170 & 2,603 \\ Test & 11,428 & 19,364 \\ Test\({}_{\texttt{new only}}\) & 1,497 & 1,746 \\ \hline \hline \end{tabular} 
\begin{tabular}{l r r} \hline \hline
**Dev\({}_{\texttt{new only}}\)** & **Test\({}_{\texttt{new only}}\)** & **Test\({}_{\texttt{new only}}\)** \\ sets of Dev and Test that only contain documents of patients not present in Train, or Train \(\cup\) Dev splits.

\end{table}
Table 5: Summary of data splits for MIMIC III Procedure datasetHuggingFace evaluate library to evaluate models for FI scores and accuracy. We use the same framework to implement the complementary metrics described in D.

For report summarization, model fine-tuning and evaluation is carried out using the ViLMedic library [13]. Models are trained using an initial learning rate of 5e-5 using the Adam optimizer [33]. A batch size of 32 with gradient accumulations over 0, 2 and 4 batches are explored. The ROUGEL metric is monitored for early stopping (10 epochs). The learning rate is decayed by 0.8 if there is no improvement on the validation set for two epochs.

## Appendix D Complementary metrics for NLU evaluation

It is important to evaluate models on their ability to express their uncertainty and calibration that make them suitable for deployment. Calibration ensures that estimated class probabilities match their naturally occurring prevalence. It is typically measured using the scalar summary statistic, **Expected Calibration Error** (ECE) [41]. Sorted predictions are divided into multiple bins (usually 10) and the absolute difference between the average accuracy and average confidence in the bin is termed the calibration gap. ECE is the weighted average of the calibration gaps [21]. **Static Calibration Error** (SCE) [42] was proposed as an extension to ECE to include every class in the multi-class setting, since ECE uses only the maximum probability and ignores the other class probabilities. Mathematically, the two quantities are defined as

\[\text{ECE}=\sum_{b=1}^{B}\ \frac{n_{b}}{N}\ \mid\text{acc}(b)-\text{ conf}(b)\mid\]

\[\text{SCE}=\frac{1}{K}\sum_{k=1}^{K}\sum_{b=1}^{B}\ \frac{n_{bk}}{N}\ \mid\text{ acc}(b,k)-\text{ conf}(b,k)\mid\]

where \(N\) is the total number of samples, \(K\) is the total number of classes, \(n_{b}\) and \(n_{bk}\) are number of samples in bin \(b\) and for class \(k\), respectively.

**Weighted Model Confidence**. We propose Weighted Model Confidence (WMC) as a metric that measures the quality of the model's confidence, as weighted by accuracy. This measure assigns positive weight when it's accurate (rewards confident & accurate predictions) and negative weight when inaccurate (penalizes confident & inaccurate predictions). We define it mathematically to be

\[\text{WMC}=\frac{1}{N}\sum_{i=1}^{N}\ \text{conf}(i)\.\ (-1)^{\text{acc}(i)+1}\]

where \(\text{conf}(i)\) and \(\text{acc}(i)\) denote confidence and accuracy of prediction \(i\). The higher the value of this metric, the better the model is at correlating confidence with accuracy.

**Average KL divergence with the Uniform Distribution** (aKLU) measures the how peaked the the predicted distribution is by comparing against a flat, uniform distribution via the KL divergence, averaged over the dataset. Mathematically, it is given by

\[\text{aKLU}=\frac{1}{N}\sum_{i=1}^{N}\ \text{KL}\big{(}\,\hat{p}(y_{i}| \mathbf{x}_{i})\,||\,U\,\big{)}\]

where \(U\) is the uniform distribution over the classes. Higher values of the metric indicate better discriminative power of the model.

**Average Predictive Entropy** (aPE) measures the entropy of the predicted distribution averaged over the dataset. Lower values of aPE imply lower uncertainty in model predictions. Mathematically, it is defined as

\[\text{aPE}=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K}\hat{p}(y_{i}=k|\mathbf{x}_ {i})\log\hat{p}(y_{i}=k|\mathbf{x}_{i})\]

### Results

We evaluate all document classification models on the MIMIC-III Procedure Selection dataset using all the complementary metrics and summarize their performance in Table 7. To provide an overview of performance across these metrics, we rank the models based on each metric, with 1 being the highest (best) rank and 16 being the lowest. We average each model's rank across the different metrics to get the final Average Rank. We do not directly average these additional scores since, unlike F1 and accuracy scores (Table 3), these are not all percentage-based metrics. Overall, similar to what is observed with other evaluation methods, no single model outperforms all others across all tasks. General domain models have inferior performance than biomedical models, of which BioLinkBERT ranks higher across all metrics. These findings further suggest that both pretraining corpora and vocabulary, as well as improved representation learning objectives during pretraining may lead to improved performance across calibration, uncertainty and prediction quality. Notably, GatorTron, which achieves the highest accuracy, ranks 7.4/16 on average across these metrics. Alternatively, ELECTRAsmall achieves the best calibration, uncertainty and discriminative scores, yet has the lowest accuracy. Our proposed WMC metric, aimed at rewarding both confidence and accuracy clarifies the discordance: GatorTron achieves the best performance and ELECTRAsmall the worst. We believe that combining this set of metrics with Accuracy and F1 scores to the benchmark has complementary utility and is an important step towards making RaLEs model comparisons more comprehensive. We hope that these evaluations can guide future model developers to train models that are not just accurate but also well-calibrated with high prediction quality and able to express their uncertainty to enable deployment and their integration into larger systems.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline
**Model** & **ECE \(\downarrow\)** & **SCE \(\downarrow\)** & **aPE \(\downarrow\)** & **WMC \(\uparrow\)** & **aKLU \(\uparrow\)** & **Avg Rank \(\downarrow\)** \\ \hline BERTbase & 0.065 & 0.004 & 0.994 & 0.339 & 3.897 & 10.8 \\ BERTlarge & 0.045 & 0.003 & 1.112 & 0.332 & 3.385 & 8.2 \\ ROBERTAbase & 0.040 & **0.002** & 1.144 & 0.320 & 3.472 & 9 \\ ROBERTAlarge & 0.048 & 0.003 & 1.087 & 0.329 & 3.368 & 8 \\ ELECTRAsmall & **0.025** & **0.002** & 1.323 & 0.292 & **3.294** & 7.2 \\ ELECTRAbase & 0.069 & 0.004 & 0.994 & 0.324 & 4.558 & 12.6 \\ ELECTRAlarge & 0.046 & 0.003 & 1.121 & 0.321 & 3.722 & 12 \\ DeBERTbase & 0.046 & 0.003 & 1.077 & 0.340 & 3.438 & 6.4 \\ DeBERTTlarge & 0.061 & 0.003 & 1.065 & 0.325 & 3.640 & 11 \\ PubMedBERT & 0.050 & 0.003 & 1.035 & 0.343 & 3.669 & 8 \\ BioLinkBERTbase & 0.046 & 0.003 & 1.077 & 0.340 & 3.438 & 6.2 \\ BioLinkBERTlarge & 0.035 & **0.002** & 1.107 & 0.337 & 3.384 & **5.4** \\ BioClinicalBERT & 0.047 & 0.003 & 1.089 & 0.337 & 3.377 & 7 \\ GatorTron & 0.061 & 0.003 & **0.977** & **0.355** & 3.556 & 7.4 \\ RadBERT1 & 0.070 & 0.004 & 0.988 & 0.340 & 3.697 & 10.4 \\ RadBERT2 & 0.052 & 0.003 & 1.059 & 0.343 & 3.415 & 6.4 \\ \hline \hline \end{tabular} Abbreviations: ECE: Expected Calibration Error, SCE: Static Calibration Error, aPE: average Predictive Entropy, WMC: Weighted model confidence, aKLU: average KL divergence with uniform distribution.

\end{table}
Table 7: Complementary NLU Evaluations for document classification models on the MIMIC-III Procedure selection dataset using multiple families of metrics.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & **ECE** & **SCE** & **aPE** & **WMC** & **aKLU** \\ \hline
**ECE** & 1.00 & 0.93 & -0.89 & 0.51 & 0.70 \\
**SCE** & 0.93 & 1.00 & -0.72 & 0.25 & 0.78 \\
**aPE** & -0.89 & -0.72 & 1.00 & -0.82 & -0.54 \\
**WMC** & 0.51 & 0.25 & -0.82 & 1.00 & 0.02 \\
**aKLU** & 0.70 & 0.78 & -0.54 & 0.02 & 1.00 \\ \hline \hline \end{tabular} Abbreviations: ECE: Expected Calibration Error, SCE: Static Calibration Error, aPE: average Predictive Entropy, WMC: Weighted model confidence, aKLU: average KL divergence with uniform distribution.

\end{table}
Table 8: Pearson Correlation Coefficient between added complementary metrics on NLU evaluation.

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_FAIL:22]

Figure 5: Per-class performance as a function of number of reports.

Figure 6: Effect of excluding patients seen during training from test set. Error bars indicate standard deviation across the models within each category.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Procedure** & **Precision** & **Recall** & **F1** & **N** \\ \hline CT Head WO contr & 84.3 & 95.3 & 89.4 & 6894 \\ CTA Chest ves WO+W contr IV & 80.8 & 87.3 & 83.9 & 1002 \\ CT Guided Str loc–W contr IV & 69.0 & 85.3 & 76.3 & 34 \\ CT C-spine WO contr & 68.5 & 69.5 & 69.0 & 984 \\ CT Chest WO contr & 64.3 & 67.3 & 65.7 & 1021 \\ CT Abd+Pelvis W contr IV & 54.5 & 71.1 & 61.7 & 1717 \\ CT Sinuses+Mandible WO contr + CT maxillofacial & 67.4 & 51.3 & 58.3 & 298 \\ WO contr & & & & \\ CTA Head ves WO+W contr IV & 57.9 & 54.2 & 56.0 & 312 \\ CT UE WO contr & 55.9 & 55.9 & 55.9 & 34 \\ CT Abd+Pelvis WO contr & 56.7 & 47.5 & 51.7 & 1037 \\ CT Neck W contr IV & 48.4 & 48.7 & 48.6 & 156 \\ CTA Chest+Abd+Pelv ves WO+W contr IV & 43.1 & 50.0 & 46.3 & 170 \\ CT Abd WO + CT Chest+Abd+Pel W contr IV & 41.5 & 51.6 & 46.0 & 366 \\ CTA Abd ves+Pelvis ves WO+W contr IV & 38.5 & 54.8 & 45.2 & 155 \\ CT Head+Brain perf+CTA Head WO+W IV & 69.4 & 32.7 & 44.4 & 104 \\ CT L-spine WO contr & 52.7 & 37.1 & 43.6 & 132 \\ CT Chest+Abd+Pelvis W contr IV & 42.1 & 44.3 & 43.2 & 1020 \\ CT Pelvis bones WO contr & 45.7 & 40.0 & 42.7 & 40 \\ CT Chest W contr IV & 45.8 & 37.3 & 41.1 & 565 \\ Other & & 49.8 & 33.7 & 40.2 & 978 \\ CT Abd+Pelvis W contr IV + CTA Chest ves & 55.0 & 31.4 & 40.0 & 210 \\ WO+W contr IV & 52.9 & 32.1 & 40.0 & 28 \\ CTA Abd Aorta+ROves-BI WO+WcontrIV & 44.4 & 33.9 & 38.5 & 118 \\ CT T-spine WO contr & 45.6 & 28.5 & 35.1 & 165 \\ CTA Head+Neck Ves WO+W contr IV & 32.1 & 35.3 & 33.6 & 102 \\ CTA Neck ves WO+W contr IV & 36.2 & 29.8 & 32.7 & 57 \\ CT Abd+Pelvis WO+W contr IV & 41.0 & 27.0 & 32.6 & 159 \\ CT Abd WO + CT Abd+Pel w contr IV & 31.7 & 32.7 & 32.2 & 223 \\ CTA Abd ves WO+W contr IV & 29.3 & 27.5 & 28.4 & 80 \\ CT Head WO+W contr IV & 31.0 & 22.3 & 25.9 & 220 \\ CT Chest+Abd+Pelvis WO contr & 44.6 & 17.5 & 25.1 & 355 \\ CT Abd WO contr & 45.5 & 15.6 & 23.3 & 64 \\ CT Head+Orbit-BI WO contr & 33.3 & 13.6 & 19.4 & 22 \\ CT Pelvis WO contr & 26.3 & 11.6 & 16.1 & 43 \\ CT Pelvis W contr IV & 1 & 7.4 & 13.8 & 27 \\ CTA Abd ves+Pelvis ves W contr IV & 18.8 & 10.9 & 13.8 & 55 \\ CT Abd+Pel + CTA AA WO+W contr IV & 25.0 & 5.7 & 9.3 & 35 \\ CT Neck WO contr & 25.0 & 4.4 & 7.5 & 45 \\ CT Chest p 3D proc WO contr & 1 & 3.2 & 6.2 & 31 \\ CT Guided Peritoneal Absc drain+cath plc & 20.0 & 2.9 & 5.1 & 34 \\ CT T+L-Spine WO contr & 0 & 0 & 0 & 22 \\ CT Abd W contr IV & 0 & 0 & 0 & 45 \\ CT Abd WO + CT Abd+Chest W contr IV & 0 & 0 & 0 & 19 \\ CT Abd+Pel WO + Chest+Abd+Pel W contr IV & 0 & 0 & 0 & 32 \\ CT Abd+Pelvis p 3D proc WO contr & 0 & 0 & 0 & 101 \\ CT Head W contr IV & 0 & 0 & 0 & 53 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Per-class performance for procedure prediction task for model with highest overall accuracy. Number of samples are reported in the test set.

## Appendix J Report Summarization Errors

Table 16 lists example reference and generated impressions for MEDIQA2021 and BioNLP2023, the report summarization tasks examined in RaLEs. We highlight the limitations of current automated evaluation approaches. For example, when additional recommendations based on medical expertise are provided in the hypothesis, metrics such as R-L impose additional penalty on the prediction if these are not present in the reference. Conversely, in scenarios where no specific disease entities are mentioned in the ground truth, metrics like RG unfairly penalize the models given that they rely on the explicit mention of diseases and their relation to anatomies. The RG metric may be further affected by the lack of validation of RadGraph labeler on non-Chest-X-ray reports, as well as lack of normalization to a standardized nomenclature for entities. Such examples illustrate the opportunity for improved summarization evaluations. Further, they motivate importance of developing models that not only understand the immediate report but can also draw upon broader context, enabling them to offer recommendations rooted in both the findings and best clinical practice.

[MISSING_PAGE_FAIL:26]