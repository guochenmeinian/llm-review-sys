# Auditing Fairness by Betting

 Ben Chugg\({}^{1}\), Santiago Cortes-Gomez\({}^{1}\), Bryan Wilder\({}^{1}\), Aaditya Ramdas\({}^{1,2}\)

Departments of Machine Learning\({}^{1}\) and Statistics\({}^{2}\)

Carnegie Mellon University

{benchugg, scortesg, builder, aramdas}@cmu.edu

###### Abstract

We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics--the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, and easy to implement. We demonstrate the efficacy of our approach on three benchmark fairness datasets.

## 1 Introduction

As algorithmic decision-making continues to increase in prevalence across both the private and public sectors [1; 2], there has been an increasing push to scrutinize the fairness of these systems. This has lead to an explosion of interest in so-called "algorithmic fairness", and a significant body of work has focused on both defining fairness and training models in fair ways (e.g., [3; 4; 5]). However, preventing and redressing harms in real systems also requires the ability to _audit_ models in order to assess their impact; such algorithmic audits are an increasing area of focus for researchers and practitioners [6; 7; 8; 9]. Auditing may begin during model development [10], but as model behavior often changes over time throughout real-world deployment in response to distribution shift or model updates [11; 12], it is often necessary to repeatedly audit the performance of algorithmic systems over time [13; 8]. Detecting whether deployed models continue to meet various fairness criteria is of paramount importance to deciding whether an automated decision-making system continues to act reliably and whether intervention is necessary.

In this work, we consider the perspective of an auditor or auditing agency tasked with determining if a model deployed "in the wild" is fair or not. Data concerning the system's decisions are gathered over time (perhaps with the purpose of testing fairness but perhaps for another purpose) and our goal is to determine if there is sufficient evidence to conclude that the system is unfair. If a system is in fact unfair, we want to determine so as early as possible, both in order to avert harms to users and because auditing may require expensive investment to collect or label samples [13; 8]. Following the recent work of Taskesen et al. [14] and Si et al. [15], a natural statistical framework for thinking about this problem is hypothesis testing. Informally, consider the null and alternative hypotheses, \(H_{0}\) and \(H_{1}\), defined respectively as

\[H_{0}:\text{the model is fair},\quad H_{1}:\text{the model is unfair}.\]Unfortunately, traditional hypothesis testing requires stringent assumptions on the data; a fixed number of iid data points, for example. Such assumptions are unrealistic in our setting. We should, for instance, be able to continually test a system as we receive more information, i.e., perform _sequential_ hypothesis testing. Moreover, we would like to be able stop collecting additional samples at arbitrary data-dependent stopping times if we have sufficient evidence against the null. This is not allowed in traditional statistical frameworks, where it is known as "peeking" or "p-hacking."

To overcome these challenges, we take advantage of recent progress in safe, anytime-valid inference (SAVI) to construct sequential procedures for determining whether an existing decision-making system is fair. SAVI is part of _sequential analysis_, a branch of statistics concerned--as the name suggests--with analyzing data sequentially while maintaining statistical validity. This subfield traces origins back to Wald, Lai, Robbins, and several others beginning in the 1940s [16, 17, 18, 19, 20]. More recently, the power of sequential analysis to enable inference under continuous monitoring of data and data-dependent stopping rules (hence _safe_ and _anytime-valid_) has led to an explosion of work in the area [21, 22, 23]. A further exciting development has been the connection between such methods and so-called "game-theoretic probability" [24, 25] which is consistent with modern measure theoretic probability1 but provides an alternative foundation based on repeated games. Importantly for our purposes, this renders many of the tools of SAVI interpretable as well as statistically powerful. We refer the interested reader to the recent survey by Ramdas et al. [27] for further detail on the relationship between sequential analysis, SAVI, and game-theoretic statistics.

Footnote 1: There are some subtleties to this remark, but they are outside the scope of this work. However, as an example of how the insights of game-theoretic statistics can be applied to modern probability, we highlight the recent work of Waudby-Smith and Ramdas [26] which draws inspiration from Shafer and Vovk [25].

**Contributions.** We develop tools to sequentially audit both classifiers and regressors. In particular:

1. We formulate the problem of auditing the fairness of classification and regression models in terms of sequential hypothesis testing. The focus on _sequential_ testing is distinct from other work, and highlights various desiderata that are important in practice--specifically, (i) being able to continuously monitor the data and (ii) a focus on rejecting the null as early as possible (i.e., reducing the number of costly samples needed to detect unfairness).
2. Next, we design nonparametric sequential hypothesis tests which hold under various definitions of group fairness. We treat auditing as sequential two-sample testing and adapt the recent work of Shekhar and Ramdas [28] on two-sample testing by betting to the fairness setting. We also provide novel bounds on the expected stopping time of our tests under the alternative, and demonstrate how to handle distribution drift (due to either model changes or changes in the underlying population), time-varying data collection policies, and composite nulls which more accurately reflect practical demands.
3. Finally, we demonstrate the real world applicability of our methods on three datasets: credit default data, US census data, and insurance data. We show that our method is robust to distribution shift resulting from model retraining and to various randomized data-collection policies whose densities deviate significantly from the underlying population. All code is publicly available at https://github.com/bchugg/auditing-fairness.

In order to provide a preview of our methodology, we suggest the following thought experiment. Imagine a fictitious better who is skeptical that a machine learning system is fair. She sets up an iterated game wherein she bets on the results of the audit as it is conducted. Her bets are structured such that, if the system is unfair, her expected payoff will be large. Conversely, if the system is fair, her expected payoff is small. Thus, if her wealth increases over time, it is evidence that the system is unfair. Our sequential test thus rejects the null if her wealth crosses some predetermined threshold. Of course, the trick is to design her bets in such a way the above conditions are satisfied, and that under the alternative, her wealth grows as quickly as possible. Mathematically, the above is made rigorous via nonnegative (super)martingales and results concerning their behavior over time [29, 30].

**Additional Related Work.** This work sits at the intersection of several fields. On the fairness side, the closest work to ours is that of Taskesen et al. [14] and Si et al. [15], both of which study fairness through the lens of (fixed-time) hypothesis testing, based on a batch of \(n\) iid observations. Given iid data \(Z_{t}\sim\rho\), they formulate the null as \(H_{0}:\rho\in\mathcal{G}\), where \(\mathcal{G}\) is the set of all "fair" distributions and derive a test statistic based on projecting the empirical distribution onto \(\mathcal{G}\) (minimizing a Wasserstein distance). They work with classifiers only. On the more technical side, we view our work as a continuation and application of the testing by betting framework [21] and more specifically of nonparametric two sample testing by betting [28]. We will deploy similar "betting strategies" (to be defined later) as [28], but provide novel analyses which are particular to our setting, and consider several extensions. Relatedly, but slightly further removed from our setting, Duan et al. [31] also employ game-theoretic ideas to construct interactive rank tests. The idea of a auditing a system to verify its veracity extends well beyond the domain of algorithmic fairness. Statistical procedures have been developed, for instance, to audit election results [32] and recently several SAVI inspired methods have been developed for several such scenarios [33; 34]. More broadly and outside the scope of this paper, betting techniques have been deployed with great success in optimization, online learning, and statistics [35; 36; 37; 38; 26].

## 2 Preliminaries

Consider a feature space \(\mathcal{X}\) and model \(\varphi:\mathcal{X}\to[0,1]\). Depending on the application, \(\varphi(x)\) might be a risk score for an individual with covariates \(x\), a classification decision, or the probability that some action is taken. Each \(x\in\mathcal{X}\) is associated with some "sensitive attribute" \(A=A_{x}\in\{0,1,\ldots,J\}\) (indicating, e.g., the result of a private health test, whether they went to college, their income strata, etc). We will often refer to \(A\) as group membership. The classifier may or may not observe \(A\). For the sake of exposition, we will assume that \(A\in\{0,1\}\), i.e, there are two groups. However, our methods extend straightforwardly to more than two groups. The details are provided in Appendix B. Before formally stating the problem, let us discuss the notion of fairness we will employ in this work.

**Fairness.** We focus on the concept of "group" fairness. Roughly speaking, this involves ensuring that groups of individuals sharing various attributes are treated similarly. There have been multiple notions of group fairness proposed in the literature. We introduce the following generalized notion of group fairness, which can be instantiated to recapture various others.

**Definition 1**.: Let \(\{\xi_{j}(A,X,Y)\}_{j\in\{0,\ldots,J\}}\) denote a family of conditions on the attributes \(A\), covariates \(X\), and outcomes \(Y\). We say a predictive model \(\varphi:\mathcal{X}\to[0,1]\) is fair with respect to \(\{\xi_{j}\}\) and a distribution \(\rho\) over \(\mathcal{X}\) if, for all \(i,j\in[J]\), \(\mathbb{E}_{X\sim\rho}[\varphi(X)|\xi_{i}(A,X,Y)]=\mathbb{E}_{X\sim\rho}[ \varphi(X)|\xi_{j}(A,X,Y)]\).

As was mentioned above, unless otherwise stated we will assume that there are only two conditions \(\xi_{0}\), \(\xi_{1}\). For our purposes, the important feature of Definition 1 is that it posits the equality of means. Indeed, letting \(\mu_{b}=\mathbb{E}_{X\sim\rho}[\varphi(X)|\xi_{b}]\) for \(b\in\{0,1\}\) we can write our null as \(H_{0}:\mu_{0}=\mu_{1}\) and alternative as \(H_{1}:\mu_{0}\neq\mu_{1}\). Different choices of conditions \(\xi_{j}\) lead to various fairness notions in the literature. For instance, if \(\varphi\) is a classification model, then:

1. Taking \(\xi_{0}=\{A=0,Y=1\}\) and \(\xi_{1}=\{A=1,Y=1\}\) results in _equality of opportunity_[39]. _Predictive equality_[40] is similar, corresponding to switching \(Y=1\) with \(Y=0\).
2. Taking \(\xi_{0}=\{A=0\}\), \(\xi_{1}=\{A=1\}\) results in _statistical parity_[41].
3. Taking \(\xi_{j}=\{A=j,\ell(X)\}\) for some projection mapping \(\ell:\mathcal{X}\to F\subset\mathcal{X}\) onto "legimitate factors" results in _conditional statistical parity_[42; 41].

**Problem Formulation.** We consider an _auditor_ who is receiving two streams of predictions \(Z^{0}=(\varphi(X^{0}_{t}))_{t\in T_{0}}\) and \(Z^{1}=(\varphi(X^{1}_{t}))_{t\in T_{1}}\), where \(X^{b}_{t}\) obeys condition \(\xi_{b}\) (i.e., is drawn from some distribution over \(\mathcal{X}|\xi_{b}\)). For brevity, we will let \(\widehat{Y}^{b}_{t}=\varphi(X^{b}_{t})\), \(b\in\{0,1\}\). The index sets \(T_{0},T_{1}\subset\mathbb{N}\cup\{\infty\}\) denote the times at which the predictions are received. We let the index differ between groups as it may not be feasible to receive a prediction from each group each timestep. We refer to this entire process as an _audit_ of the model \(\varphi\).

For \(b\in\{0,1\}\), let \(T_{b}[t]=T_{b}\cap[t]\) be the set of times at which we receive predictions from group \(b\) up until time \(t\). The auditor is tasked with constructing a _sequential hypothesis test_\(\phi\equiv(\phi_{t})_{t\geq 1}\) where \(\phi_{t}=\phi_{t}((\cup_{t\in T_{0}[t]}Z^{0}_{t})\cup(\cup_{t\in T_{1}[t]}Z^{ 1}_{t}))\in\{0,1\}\) is a function of all predictions received until time \(t\). We interpret \(\phi_{t}=1\) as "reject \(H_{0}\)", and \(\phi_{t}=0\) as "fail to reject \(H_{0}\)." Once we reject the null, we stop gathering data. That is, our stopping time is \(\tau=\arg\inf_{t}\{\phi_{t}=1\}\). We say that \(\phi\) is a _level-\(\alpha\)_ sequential test if

\[\sup_{P\in H_{0}}P(\exists t\geq 1:\phi_{t}=1)\leq\alpha,\ \ \text{or equivalently}\ \ \sup_{P\in H_{0}}P(\tau<\infty)\leq\alpha.\] (1)

In words, the test should have small false positive rate (type I error) _simultaneously across all time steps_. (Note that a fixed-time level-\(\alpha\) test would simply drop the quantifier \(\exists t\geq 1\) and concern itself with some fixed time \(t=n\).) We also wish to design tests with high power. Formally, we say that \(\phi\) has _asymptotic power_\(1-\beta\) if \(\sup_{P\in H_{1}}P(\forall t\geq 1:\phi_{t}=0)\leq\beta\), or equivalently \(\sup_{P\in H_{1}}P(\tau=\infty)\leq\beta\). That is, in all worlds in which the alternative is true, we fail to reject with probability at most \(\beta\) (type II error). Typically the type-II error \(\beta\equiv\beta_{n}\) decreases with the sample size \(n\). In this work we will develop asymptotic power one tests, meaning that \(\beta_{n}\to 0\) as \(n\to\infty\).

**Martingales and filtrations.** Our techniques rely on the machinery of nonnegative (super)martingales and results concerning their behavior over time. We introduce some of the technicalities here. A (forward) filtration \(\mathcal{F}\equiv(\mathcal{F}_{t})_{t\geq 0}\) is an increasing sequence of \(\sigma\)-fields \(\mathcal{F}_{t}\subset\mathcal{F}_{t+1}\). Throughout this paper, we will consider the "canonical filtration" \(\mathcal{F}_{t}=\sigma(Z_{1},\ldots,Z_{t})\) which can heuristically (but very usefully) be thought of as all the information known at time \(t\). We say that a stochastic process \(S=(S_{t})_{t\geq 1}\) is _adapted_ to \((\mathcal{F}_{t})\) if \(S_{t}\) is \(\mathcal{F}_{t}\) measurable for all \(t\geq 1\), and _predictable_ if \(S_{t}\) is \(\mathcal{F}_{t-1}\) measurable for all \(t\geq 1\). A \(P\)_-martingale_ is an adapted stochastic process \(M=(M_{t})_{t\geq 1}\) such that \(\mathbb{E}_{P}[M_{t+1}|\mathcal{F}_{t}]=M_{t}\) for all \(t\geq 1\). If the equality is replaced with \(\leq\), then \(M\) is a \(P\)_-supermartingale_. A particularly useful result in sequential analysis is _Ville's inequality_[29], which states that if \(M\) is a nonnegative \(P\)-supermartingale, then for all \(\alpha>0\), \(P(\exists t\geq 0:M_{t}\geq 1/\alpha)\leq\alpha\mathbb{E}_{P}[M_{0}]\). We will also employ the following randomized improvement to Ville's inequality [43, Corollary 4.1.1]: For \(M\) as above and any \(\mathcal{F}\)-stopping time \(\tau\), \(P(\exists t\leq\tau:M_{t}\geq 1/\alpha\) or \(M_{\tau}\geq U/\alpha)\leq\alpha\), where \(U\) is a uniform random variable on \([0,1]\)_which is independent of_\(M\) and \(\tau\).

## 3 Methods

We begin by introducing the methodology in the setting where the predictions are received uniformly at random from the population. We will then progressively generalize the setting: Sections 3.2 and 3.3 will enable time-varying data collection policies, Section 3.4 will allow the means \(\mu_{0}\) and \(\mu_{1}\) to change with time, and Section 3.5 will consider composite nulls of the form \(|\mu_{0}-\mu_{1}|\leq\epsilon\).

To ease the presentation, let us make the assumption that we receive an audit from both groups each timestep, so that \(T_{0}=T_{1}=\mathbb{N}\). This is without loss of generality; one can simply wait until multiple audits from each group are available. However, Appendix A provides a more detailed discussion on how to modify our framework and theorems if this condition is unmet.

### Testing by betting

Returning to the intuition for a moment, recall the fictitious bettor discussed in the introduction. She attempts to prove that the system is unfair by betting on the results of the audits before they are revealed. If the system _is_ unfair, then the average difference between \(\widehat{Y}^{0}_{t}\) and \(\widehat{Y}^{1}_{t}\) (i.e., the model's outputs across different groups) will be non-zero. The skeptic's bets are therefore structured such that if \(\mu_{0}=\mathbb{E}\widehat{Y}^{0}_{t}\neq\mathbb{E}\widehat{Y}^{1}_{t}=\mu_{1}\), then her expected wealth will increase over time. More formally, at time \(t\), the skeptic designs a _payoff function_\(S_{t}:\mathbb{R}\times\mathbb{R}\to\mathbb{R}_{\geq 0}\) which is \(\mathcal{F}_{t-1}\) measurable and \(\mathbb{E}_{P}[S_{t}(\widehat{Y}^{0}_{t},\widehat{Y}^{1}_{t})|\mathcal{F}_{t- 1}]\leq 1\) if \(P\in H_{0}\) (i.e, \(\mu_{0}=\mu_{1}\)). Next, the skeptic receives the model predictions \(\widehat{Y}^{0}_{t}=\varphi(X^{0}_{t})\) and \(\widehat{Y}^{1}_{t}=\varphi(X^{1}_{t})\). We assume the skeptic starts with wealth of \(\mathcal{K}_{0}=1\). At each time \(t\), she reinves all her wealth \(\mathcal{K}_{t-1}\) on the outcome and her payoff is \(\mathcal{K}_{t}=S_{t}(\widehat{Y}^{0}_{t},\widehat{Y}^{1}_{t})\cdot\mathcal{K} _{t-1}=\prod_{i=1}^{t}S_{t}(\widehat{Y}^{0}_{t},\widehat{Y}^{1}_{t})\). We call \((\mathcal{K}_{t})_{t\geq 0}\) the skeptic's _wealth process_. The wealth process is a supermartingale starting at \(1\) under the null, due to the constraint on the payoff function. Thus, by Ville's inequality, the probability that \(\mathcal{K}_{t}\) ever exceeds \(1/\alpha\) is at most \(\alpha\) when the model is fair. The skeptic's goal, as it were, is to design payoff functions such that the wealth process grows quickly under the alternative, and thus rejection occurs sooner rather than later.

Inspired by a common idea in game-theoretic statistics (cf. [21, 26, 27, 28]), let us consider the following payoff function:

\[S_{t}(\widehat{Y}^{0}_{t},\widehat{Y}^{1}_{t})=1+\lambda_{t}(\widehat{Y}^{0}_{t }-\widehat{Y}^{1}_{t}),\] (2)

where \(\lambda_{t}\) is predictable and lies in \([-1,1]\) to ensure that \(S_{t}(\widehat{Y}^{0}_{t},\widehat{Y}^{1}_{t})\geq 0\). Note that for \(P\in H_{0}\), \(\mathbb{E}_{P}[\mathcal{K}_{t}|\mathcal{F}_{t-1}]=\mathcal{K}_{t-1}\mathbb{E}_ {P}[1+\lambda_{t}(\widehat{Y}^{0}_{t}-\widehat{Y}^{1}_{t})|\mathcal{F}_{t-1}] =\mathcal{K}_{t-1}\), so \((\mathcal{K}_{t})_{t\geq 1}\) is a nonnegative \(P\)-martingale. Rejecting when \(\mathcal{K}_{t}>1/\alpha\) thus gives to a valid level-\(\alpha\) sequential test as described above. We will select \(\lambda_{t}\) using Online Newton Step (ONS) [44, 45], which ensures exponential growth of the wealth process under the alternative. Figure 1 illustrates the behavior of the wealth process under various hypotheses when using ONS to choose \(\lambda_{t}\). As the difference between the meansincreases, the wealth grows more quickly which leads to faster rejection of the null. To define ONS, let \(g_{t}=\widehat{Y}_{t}^{0}-\widehat{Y}_{t}^{1}\) and initialize \(\lambda_{1}=0\). For all \(t\geq 1\), recursively define

\[\lambda_{t}=\bigg{(}\bigg{(}\frac{2}{2-\ln(3)}\frac{z_{t-1}}{1+\sum_{i=1}^{t-1 }z_{i}^{2}}-\lambda_{t-1}\bigg{)}\wedge 1/2\bigg{)}\vee-1/2,\ \ \text{where}\ \ z_{i}=\frac{g_{i}}{1-\lambda_{i}g_{i}}.\] (3)

The machinery just introduced is sufficient to define a level-\(\alpha\) sequential test. However, we add a final ingredient to increase the power of this procedure. Motivated by the _randomized_ Ville's inequality mentioned in Section 2, we introduce one final step: If we stop the procedure at some stopping time \(\tau\) but have not yet rejected the null (because, for instance, our budget ran out), we can check if \(\mathcal{K}_{\tau}\geq U/\alpha\), where \(U\) is uniform on [0,1] and _drawn independently from everything observed so far_. We emphasize that the final step can only be performed once. Thus, if it is possible that more data will be collected in the future, we advise waiting. We summarize the process in Algorithm 1.

``` Input:\(\alpha\in(0,1)\) \(\mathcal{K}_{0}\gets 1\) for\(t=1,2,\ldots,\tau\)do #\(\tau\) may not be known in advance  Perhaps receive audits \(\widehat{Y}_{t}^{0}\) and \(\widehat{Y}_{t}^{1}\)  Construct payoff \(S_{t}\) (e.g., (2), (16), or (7))  Update \(\mathcal{K}_{t}\leftarrow\mathcal{K}_{t-1}\cdot S_{t}\)  If \(\mathcal{K}_{t}\geq 1/\alpha\) then stop and reject the null endfor if the null has not been rejected then  Draw \(U\sim\text{Unif}(0,1)\), reject if \(\mathcal{K}_{\tau}\geq U/\alpha\) endif ```

**Algorithm 1** Testing group fairness by betting

The following proposition gives a bound on the expected stopping time of this sequential test under the alternative. We note that Shekhar and Ramdas [28] also provide a bound on the expected stopping time of a multivariate betting-style sequential test. However, due to the generality of their setting, their result is defined in terms of quantities which are difficult to analyze. We thus provide a more direct analysis specific to the difference of univariate means. The proof may be found in Appendix C.1.

**Proposition 1**.: _Algorithm 1 with input \(\alpha\in(0,1)\) and betting strategy (2) is a level-\(\alpha\) sequential test with asymptotic power one. Moreover, letting \(\Delta=|\mu_{0}-\mu_{1}|\), under the alternative the expected stopping time \(\tau\) obeys_

\[\mathbb{E}[\tau]\lesssim\frac{1}{\Delta^{2}}\log\bigg{(}\frac{1}{\Delta^{2} \alpha}\bigg{)}.\] (4)

It is possible to demonstrate a lower bound of \(\mathbb{E}[\tau]\gtrsim\sigma^{2}\log(1/\alpha)/\Delta^{2}\) where \(\sigma^{2}=\mathbb{E}[(\widehat{Y}^{0}-\widehat{Y}^{1})^{2}]\)[28, Prop. 2]. Since \(\sigma^{2}\gtrsim 1\) in the worst case, our result is optimal up to a factor of \(\log(1/\Delta^{2})\).

### Time-varying data collection policies

Here we extend the setting to allow for time-dependent data collection policies. This is motivated by the fact that organizations are often collecting data for various purposes and must therefore evaluate fairness on data which is not necessarily representative of the population at large. We also allow the data collection policies to differ by group. Mathematically, for \(b\in\{0,1\}\) let \(\pi_{t}^{b}(\cdot)=\pi_{t}(\cdot|\xi_{b})\) denote the randomized policies which determine the probability with which a set of covariates are

Figure 1: **Left:** Description of main algorithm. **Right:** The wealth process \((\mathcal{K}_{t})\) under different conditions on the means. Observations follow a Bernoulli distribution for a given mean. As \(\Delta=|\mu_{0}-\mu_{1}|\) increases, the wealth grows more quickly. If a test rejects, the procedure ends, hence the plateauing of the blue and purple lines. For \(\Delta=0\), the wealth fluctuates around 1 and the test never rejects. Shaded regions indicated the standard deviation after 100 trials.

selected from the population. Let \(\rho^{b}\) be the density of \(X|\xi_{b}\). Following standard propensity weighting techniques [46; 47], introduce the weighted estimates

\[\omega_{t}^{b}(x):=\frac{\rho^{b}(x)}{\pi_{t}^{b}(x)}.\] (5)

We may write simply \(\omega_{t}^{b}\) when \(x\) is understood from context. The policy \(\pi_{t}^{b}\) (and hence the weight \(\omega_{t}^{b}\)) need not be deterministic, only \(\mathcal{F}_{t-1}\) measurable (so it can be considered a deterministic quantity at time \(t\)). This is motivated by various practical applications of sequential decision-making, in which the policy used to collect data often changes over time in response to either learning or policy [48; 49]. In many applications, \(\pi_{t}^{b}\) is a function of a select few covariates only--for instance, income, education level, or career. In some settings it is reasonable to assume knowledge of \(\rho^{b}\) (by means of a census, for instance). However, this is not always the case. Section 3.3 will therefore discuss strategies which do not require precise knowledge of the density. We will assume that for all \(x,t\) and \(b,\omega_{t}^{b}(x)<\infty\). In this case, (5) enables an unbiased estimate of \(\mu_{b}=\mathbb{E}_{\rho}[\varphi(X)|\xi_{b}]\) when \(X_{t}\) is sampled according to \(\pi_{t}^{b}\):

\[\mathbb{E}_{X\sim\pi_{t}^{b}}[\widehat{Y}_{t}^{b}\omega_{t}^{b}(X)|\mathcal{F} _{t-1},\xi_{b}]=\int_{\mathcal{X}}\varphi(x)\omega_{t}^{b}(x)\pi_{t}^{b}(x) \text{d}x=\int_{\mathcal{X}}\varphi(x)\rho^{b}(x)\text{d}x=\mu_{b}.\] (6)

Our payoff function is similar to the previous section, but reweights the samples by \(\omega_{t}^{b}(=\omega_{t}^{b}(X_{t}^{b}))\), and then adds a corrective factor to ensure that the payoff is nonnegative:

\[S_{t}(\widehat{Y}_{t}^{0},\widehat{Y}_{t}^{1})=1+\lambda_{t}L_{t}(\widehat{Y }_{t}^{0}\omega_{t}^{0}-\widehat{Y}_{t}^{1}\omega_{t}^{1}),\ \ \text{where}\ \ L_{t}:=\min_{b\in\{0,1\}}\underset{x\in\mathcal{X}}{\text{ess inf}}\frac{1}{2 \omega_{t}^{b}(x)},\] (7)

and \(\lambda_{t}\) is once again selected via ONS. Multiplication by \(L_{t}\) is required to ensure that the values \(L_{t}(\widehat{Y}_{t}^{0}\omega_{t}^{0}-\widehat{Y}_{t}^{1}\omega_{t}^{1})\) lie in \([-1,1]\) in order to be compatible with ONS. Here "ess inf" is the essential infimum, which is the infimum over events with nonzero measure. For most practical applications one can simply replace this with a minimum. For such a strategy, we obtain the following guarantee.

**Proposition 2**.: _Algorithm 1 with input \(\alpha\in(0,1)\) and betting strategy (7) is a level-\(\alpha\) sequential test with asymptotic power one. Moreover, suppose that \(0<L_{\inf}:=\inf_{t\geq 1}L_{t}\) and let \(\kappa=\Delta L_{\inf}\) and \(\Delta=|\mu_{0}-\mu_{1}|\). Then, treating log-log factors as constant, under the alternative the expected stopping time \(\tau\) obeys_

\[\mathbb{E}[\tau]\lesssim\frac{1}{\kappa^{2}}\log\bigg{(}\frac{1}{\kappa^{2} \alpha}\bigg{)}.\] (8)

Thus we see that we pay the price of allowing randomized data collection policies by a factor of roughly \(1/L_{\inf}\). Note that the payoff function (7) does not require knowledge of \(L_{\inf}\). Indeed, this strategy remains valid even if \(L_{\inf}=0\) as long as \(L_{t}\) is nonzero for each \(t\). It is only the analysis that requires \(L_{\inf}\) to be finite and known. The proof of Proposition 2 is provided in Appendix C.2.

### Time-varying policies for unknown densities

A reasonable objection to the discussion in the previous section is that the densities \(\rho^{b}\) may not always be known. In this case we cannot compute the propensity weights in (5). Here we provide an alternative payoff function which uses an estimate of the density, \(\widehat{\rho}^{b}\). We assume we know upper and lower bounds on the multiplicative error of our estimate: \(\delta^{\max}\geq\max_{b}\sup_{x}\{\widehat{\rho}^{b}(x)/\rho^{b}(x)\}\), and \(\delta^{\min}\leq\min_{b}\inf_{x}\{\widehat{\rho}^{b}(x)/\rho^{b}(x)\}\). We assume that \(\delta^{\min}>0\).

Let \(\widehat{\omega}_{t}^{b}\) be the propensity weights at time \(t\) using the estimated density, i.e., \(\widehat{\omega}_{t}^{b}(x)=\widehat{\rho}^{b}(x)/\pi_{t}^{b}(x)\). Notice that \(\mathbb{E}_{X\sim\pi_{t}^{b}}[\widehat{\omega}_{t}^{b}(X)\varphi(X)| \mathcal{F}_{t-1}]=\int_{\mathcal{X}}\varphi(x)\widehat{\rho}^{b}(x)\text{d}x =\int_{\mathcal{X}}\varphi(x)(\widehat{\rho}^{b}(x)/\rho^{b}(x))\rho^{b}(x) \text{d}x\leq\delta^{\max}\mu_{b}.\) Similarly, \(\mathbb{E}_{X\sim\pi_{t}^{b}}[\widehat{\omega}^{b}(X)\varphi(X)|\mathcal{F}_ {t-1},\xi_{b}]\geq\delta^{\min}\mu_{b}\). Recall that \(\widehat{Y}_{t}^{b}=\varphi(X_{t}^{b})\). Consider the payoff function

\[S_{t}(\widehat{Y}_{t}^{0},\widehat{Y}_{t}^{1})=1+\lambda_{t}B_{t}\bigg{(} \frac{\widehat{\omega}_{t}^{0}\widehat{Y}_{t}^{0}}{\delta^{\max}}-\frac{\widehat {\omega}_{t}^{1}\widehat{Y}_{t}^{1}}{\delta^{\min}}\bigg{)},\] (9)

where \(B_{t}\) is \(\mathcal{F}_{t-1}\) measurable. Then

\[\mathbb{E}[S_{t}|\mathcal{F}_{t-1}]=1+\lambda_{t}B_{t}\bigg{(}\frac{\mathbb{E} _{\pi_{t}^{b}}[\widehat{\omega}_{t}^{0}\widehat{Y}_{t}^{0}|\mathcal{F}_{t-1}]} {\delta^{\max}}-\frac{\mathbb{E}_{\pi_{t}^{b}}[\widehat{\omega}_{t}^{1}\widehat {Y}_{t}^{1}|\mathcal{F}_{t-1}]}{\delta^{\min}}\bigg{)}\leq 1+\lambda_{t}B_{t}(\mu_{0}-\mu_{1}).\]Under the null, \(\mu_{0}-\mu_{1}=0\), implying that \(\mathbb{E}[S_{t}|\mathcal{F}_{t-1}]\leq 1\). We select \(B_{t}\) so that \(S_{t}\) is nonnegative (thus ensuring that the capital process is a nonnegative supermartingale) and is compatible with ONS. The following condition on \(B_{t}\) suffices: \(B_{t}\leq\delta^{\min}\min_{b}\inf_{x}(2\tilde{\omega}_{t}^{b}(x))^{-1}\). Note that if we know the true densities then we can take \(\delta^{\max}=\delta^{\min}=1\) and we recover (7).

### Handling distribution shift

Until this point we've assumed that the means \(\mu_{b}\) remain constant over time. A deployed model, however, is susceptible to distribution shift due to changes in the underlying population, changes in the model (retraining, active learning, etc), or both. Here we demonstrate that our framework handles both kinds of drift. In fact, we need not modify our strategy (we still deploy Algorithm 1), but we must reformulate our null and alternative hypotheses and our analysis will change. Before introducing the mathematical formalities, we draw the reader's attention to Figure 2 which illustrates two cases of distribution shift and the response of our sequential test to each. The left panel is a case of "smooth" drift in which case \(\mu_{0}(t)=\mu_{1}(t)\) are equal for some number of timesteps after which \(\mu_{1}(t)\) begins to drift upward. The right panel exemplifies a situation in which both means change each timestep (the weekly prevalence of disease in a population, for instance), but \(\mu_{1}(t)\) has a marked drift upward over time. In both cases, the wealth process is sensitive to the drift and grows over time.

To handle changes in the underlying population we let \(\rho_{t}\) denote the population distribution over \(\mathcal{X}\) at time \(t\). Likewise, to handle changes to the model, we let \(\varphi_{t}:\mathcal{X}\to[0,1]\) be the model at time \(t\). In order to define our hypotheses, let \(\Delta_{t}:=|\mu_{0}(t)-\mu_{1}(t)|\) where \(\mu_{b}(t)=\mathbb{E}_{X\sim\rho_{t}}[\varphi_{t}(X)|\xi_{b},\mathcal{F}_{t- 1}]\), \(b\in\{0,1\}\). Then, we write the null and alternative hypothesis as

\[H_{0}:\Delta_{t}=0\ \ \forall t\geq 1,\quad H_{1}:\exists\,T\in\mathbb{N} \ \text{s.t.}\ \ \Delta_{t}>0\ \ \forall t\geq T.\] (10)

Of course, \(H_{0}\) and \(H_{1}\) do not cover the space of possibilities. In particular, neither contains the event that \(\mu_{1}(t)\neq\mu_{0}(t)\) for some finite interval \(t\in[a,b]\) and are otherwise equal. However, we believe (10) strikes a desirable balance between analytical tractability and practical utility; defining \(H_{1}\) such that we will reject the null for each finite window \([a,b]\) places too extreme a burden on the growth of the wealth process. Moreover, as illustrated by Figure 2, if \(\Delta_{t}>0\) for a large enough window, then our sequential tests will reject the null.

If we are working with randomized data collection policies per Section 3.2, then one should change the definition of the weights in Equation (5) to incorporate the time-varying distribution \(\rho_{t}\). That is, \(\omega_{t}^{b}(x)=\rho_{t}^{b}(x)/\pi_{t}^{b}(x)\). Otherwise, all procedures detailed in the previous sections remain the same, and we can provide the following guarantee on their performance.

**Proposition 3**.: _Algorithm 1 with input \(\alpha\in(0,1)\) and letting strategies (2) and (7) is a level-\(\alpha\) sequential test for problem (10). It has power one under the alternative if \(\Delta_{\inf}:=\inf_{t\geq n}\Delta_{t}>0\) where \(n\) is some time at which drift begins. Moreover, under the alternative the expected stopping

Figure 2: Two illustrations of our sequential test adapting to distribution shift. In both settings, the observations at time \(t\) are Bernoulli with bias determined by the respective mean at that time. Shaded areas in the bottom plots represent the standard deviation across 100 trials. **Left:** For the first 100 time steps, we have \(\mu_{0}(t)=\mu_{1}(t)=0.3\). At time \(100\), \(\mu_{1}(t)\) begins to smoothly slope upward. **Right:** Here we assume that both means are non-stationary and non-smooth. Both are sinusoidal with Gaussian noise but \(\mu_{1}(t)\) drifts slowly upwards over time.

time \(\tau\) obeys_

\[\mathbb{E}[\tau]\lesssim n+\frac{1}{\Delta_{\inf}^{2}}\log\bigg{(} \frac{1}{\Delta_{\inf}^{2}\alpha}\bigg{)}.\] (11)

_Furthermore, if the data are gathered by randomized policies \(\pi_{t}^{b}\) and \(L_{\inf}>0\) for \(L_{\inf}\) as in Proposition 2, then the expected stopping time follows by replacing \(\Delta_{\inf}\) in (14) with \(\Delta_{\inf}L_{\inf}\)._

Let us make two remarks about this result. First, the condition \(\Delta_{\inf}>0\) ensures that, after some point, the means \(\mu_{0}(t)\) and \(\mu_{1}(t)\) remain separated. If they diverge only to later reconverge, there is no guarantee that the test will reject (though, in practice, it will if they are separated for sufficiently long). Second, the reader may have guessed that the expected stopping time under drift beginning at time \(n\) follows from Propositions 1 and 2 after simply adding \(n\). However, it is a priori feasible that the reliance of ONS on past predictions would result in slower convergence under distribution shift. Proposition 3 verifies that this is not the case, and that the rate is the same up to constants.

### Composite Nulls

In practice, it may be unreasonable to require that the means between two groups are precisely zero. Instead, we may only be interested in detecting differences greater than \(\epsilon\) for some \(0<\epsilon\ll 1\). In this case we may write the null and alternative as

\[H_{0}:|\mu_{0}-\mu_{1}|\leq\epsilon\ \ \text{vs}\ H_{1}:|\mu_{0}-\mu_{1}|>\epsilon.\] (12)

To formulate our test, we introduce two pairs of auxiliary hypotheses: \(H_{0}^{\prime}:\mu_{0}-\mu_{1}\leq\epsilon\) vs \(H_{1}^{\prime}:\mu_{0}-\mu_{1}>\epsilon\) and \(H_{0}^{\prime\prime}:\mu_{1}-\mu_{0}\leq\epsilon\) vs \(H_{1}^{\prime\prime}:\mu_{1}-\mu_{0}>\epsilon\). Observe that if either \(H_{0}^{\prime}\) or \(H_{0}^{\prime\prime}\) is false, then the alternative \(H_{1}\) is true. Our approach will therefore entail testing both \(H_{0}^{\prime}\) vs \(H_{1}^{\prime}\) and \(H_{0}^{\prime\prime}\) vs \(H_{1}^{\prime\prime}\). Let \(\phi_{0}^{Q}\) denote the sequential test for the former, and \(\phi_{t}^{R}\) for the latter. The test given by \(\phi_{t}=\max\{\phi_{t}^{Q},\phi_{t}^{R}\}\) (i.e., rejecting \(H_{0}\) if _either_\(H_{0}^{\prime}\) or \(H_{0}^{\prime\prime}\) is rejected) is then a test for (12). Game-theoretically, this can be interpreted as splitting our initial capital in half and playing two games simultaneously. To test \(H_{0}^{\prime}\) and \(H_{0}^{\prime\prime}\) consider the two payoff functions

\[Q_{t}(\widehat{Y}_{t}^{0},\widehat{Y}_{t}^{1})=1+\lambda_{t}( \widehat{Y}_{t}^{0}-\widehat{Y}_{t}^{1}-\epsilon),\ \ \text{and}\ \ R_{t}(\widehat{Y}_{t}^{0},\widehat{Y}_{t}^{1})=1+\lambda_{t}( \widehat{Y}_{t}^{1}-\widehat{Y}_{t}^{0}-\epsilon).\] (13)

If \(\lambda_{t}\in[\frac{-1}{1-\epsilon},\frac{1}{1+\epsilon}]\) then both \(Q_{t}\) and \(R_{t}\) are nonnegative. We thus select \(\lambda_{t}\) via ONS as usual. The wealth processes \((\mathcal{K}_{t}^{Q})\) and \((\mathcal{K}_{t}^{R})\) defined by \(Q_{t}\) and \(R_{t}\) (i.e., \(\mathcal{K}_{t}^{Q}=\prod_{i\leq t}Q_{i}(\widehat{Y}_{i}^{0},\widehat{Y}_{i}^{1})\) and \(\mathcal{K}_{t}^{R}=\prod_{i\leq t}R_{i}(\widehat{Y}_{i}^{0},\widehat{Y}_{i}^{1})\)) are then nonnegative supermartingales under \(H_{0}^{\prime}\) and \(H_{0}^{\prime\prime}\), respectively. We reject \(H_{0}\) if either \(\mathcal{K}_{t}^{R}\geq 2/\alpha\) or \(\mathcal{K}_{t}^{Q}\geq 2/\alpha\) which results in an overall level-\(\alpha\) sequential test.

**Proposition 4**.: _Algorithm 1 with input \(\alpha/2\in(0,1)\) with betting strategy \(Q_{t}\) (resp., \(R_{t}\)) is a level-\(\alpha/2\) sequential test for \(H_{0}^{\prime}\) vs \(H_{1}^{\prime}\) (resp., \(H_{0}^{\prime\prime}\) vs \(H_{1}^{\prime\prime}\)). Rejecting \(H_{0}\) if either \(H_{0}^{\prime}\) or \(H_{0}^{\prime\prime}\) is rejected results in a level-\(\alpha\) sequential test with asymptotic lower one for problem (12). Let \(\Delta=|\mu_{0}-\mu_{1}|\). Then, under the alternative the expected stopping time \(\tau\) obeys_

\[\mathbb{E}[\tau]\lesssim\frac{1}{(\Delta-\epsilon)^{2}}\log\bigg{(} \frac{1}{\alpha(\Delta-\epsilon)^{2}}\bigg{)}.\] (14)

Figure 3: Comparisons of false positives rates (FPRs) and stopping times on credit loan data and US census data. The left two columns plot \(\tau\) under \(H_{1}\) versus the FPR as \(\alpha\) is varied from 0.1 to 0.01. The FPR is grossly inflated when under method M1, as illustrated the first and third columns. Betting is a Pareto improvement over the permutation tests.

We note that under the alternative, \(\epsilon<\Delta\), so \(0<\Delta-\epsilon<\Delta\). Proposition 4 states that the expected stopping time depends on the interplay of \(\Delta\) and \(\epsilon\), and will increase as they approach each other. This is intuitive. For a fixed \(\Delta\) one would expect that the problem becomes harder as \(\epsilon\uparrow\Delta\).

## 4 Experiments

Here we provide several sets of experiments to demonstrate the benefits of our sequential test compared to fixed-time tests.2 First we need to consider how fixed-time tests might be applied in practice to deal with sequential settings. We consider two such methods.

Footnote 2: We refrain from comparing our betting-style sequential tests to other _sequential_ tests; we refer the reader to [28] for such comparisons. Instead, our goal in this section is to persuade the fairness community that sequential tests are superior tools to fixed-time tests for auditing deployed systems.

**M1.** For some prespecified \(k\in\mathbb{N}\), wait until we have collected \(k\) audits, then perform the test. If the test does not reject, collect another \(k\) audits and repeat. We emphasize that if one does not adjust the significance level over time, then _this is not a valid level-\(\alpha\) test_. However, it may be used unwittingly in practice, so we study it as one baseline.

**M2.** We batch and test in the same way as above, but we apply a Bonferroni-like correction in order to ensure it is level \(\alpha\). More precisely, for the \(j\)th-batch, \(j\geq 1\), we set the significance level to \(\alpha/2^{j}\). The union bound then ensures that the FPR is at most \(\alpha\) over all batches.

We run experiments on three real world datasets: a credit default dataset [50], US census data [51], and health insurance data [52]. All can be made to violate equality of opportunity if trained with naive models; we provide the details in Appendix D. Suffice it to say here that the absolute difference in means, \(\Delta\), is \(\approx\) 0.03 for the credit default dataset, \(\approx\) 1.1 for the census data, and \(\approx 0.06\) for the insurance data. We employ various distinct models to test our methods, from forest-based models to logistic regression. We use a permutation test as our baseline fixed-time test because of its ubiquity in practice, exact type-I error control under exchangeability, and minimax optimality in various scenarios [53].

Figure 3 compares the results of Algorithm 1 (betting strategy (2)) with the permutation test baselines M1 and M2. The left two columns plot the empirical false positive rate (FPR) against the stopping time under the alternative. Values concentrated in the lower left corner are therefore preferable. We plot values for each test as \(\alpha\) is varied from 0.01 to 0.1. The betting test typically Pareto-dominates the (stopping time, FPR) values achievable by the baselines. In a small number of cases, M1 (baseline without Bonferroni correction) achieves a faster stopping time; however, this is only possible with a very high FPR (> 0.5). This inflated FPR is verified by the final column, which shows that M1 does not respect the desired type-I error rate. This encourages the use of the Bonferroni-style correction of M2. However, doing so results in an overly conservative test which is slower to reject than betting, as seen in the center column of Figure 3. Betting, meanwhile, respects the desired FPR over all \(\alpha\), and almost always has the fastest stopping time at any given FPR. We note that while it may appear that the permutation tests improve as \(k\) gets larger, this trend does not hold for large \(k\). Indeed, \(k\) is

Figure 4: Response of the tests to distribution shift on the census data. We use a fair model for 400 timesteps, after which we switch to an unfair model with \(\Delta\approx 1.0\). The leftmost plot uses permutation tests under M1, resulting in inflated type-I error. Values are plotted as \(\alpha\) ranges from 0.01 to 0.1.

also the minimum rejection time for the permutation test. Increasing \(k\) to 1,500 on the credit default dataset, for instance, would result in vertical line at \(\tau=1,500\).

The robustness of betting to distribution shift is illustrated by Figure 4. Here we use a fair model for the first 400 timesteps, after which we switch to a random forest classifier which is unfair (\(\Delta\approx 1.1\)). The rejection time for all tests suffers accordingly, but betting remains a Pareto improvement over the permutation tests using M2. As before, using M1 results in an enormous FPR. Interestingly, we see that M2 also results in FPR higher than 0.1 (the maximum value of \(\alpha\)) a nontrivial percentage of the time. Indeed, under distribution shift, the the permutation test using M2 is no longer level-\(\alpha\) since the data are not exchangeable. Betting, on the other hand, maintains its coverage guarantee.

Finally, Figure 5 illustrates the performance of various algorithms when audits are not conducted on predictions received uniformly at random from the population (discussed in Section 3.2). Instead, predictions were received based on an individual's region. We test three distinct strategies, \(\pi_{1}\), \(\pi_{2}\) and \(\pi_{3}\), each of which deviates from the population distribution to varying degrees (LHS of Figure 5). As expected, the rejection time of our strategy suffers compared to when the predictions are received uniformly from the population (denoted by the red crosses). The desired level-\(\alpha\) guarantee is still met, however, and betting continues to outperform permutation tests in all cases.

## 5 Summary

We have argued that practitioners in the field of algorithmic fairness should consider adopting sequential hypothesis tests in lieu of fixed-time tests. The former enjoy two desirable properties absent in the latter: (i) the ability to continually monitor incoming data, and (ii) the ability to reject at data-dependent stopping times. Both (i) and (ii) are useful for high-stakes and/or time-sensitive applications in which fixing the budget beforehand and waiting for more data to arrive may be unsatisfactory, such as auditing the fairness of healthcare models [54] or resource allocation strategies during a public health crisis [55; 56]. We provided a sequential test with asymptotic power one for group fairness (Algorithm 1) inspired by the paradigm of game-theoretic probability and statistics [25; 21; 28]. It is fast, easy to implement, and can handle time-varying data collection policies, distribution shift, and composite nulls, all of which are common in practice. We also provided bounds on the expected stopping time of the test. We hope that the simplicity of our methods combined with their strong theoretical guarantees prove useful for practitioners in the field.

**Limitations and societal impact.** Auditing is a complex socio-technical problem which involves a range of questions: choosing appropriate audit targets, gaining access to data, ensuring the credibility and independence of external auditors, creating accountability based on audit results, and more [57]. Indeed, poorly designed audits may certify that a system is "fair" while masking harder to verify issues such as data provenance. Our work addresses only one portion of the overall auditing task: providing statistically sound methods for the sequential testing of a specific criteria. This is complementary to, and does not substitute for, careful overall design of an auditing framework.

Figure 5: Illustration of our betting method when using various data collection strategies, \(\pi_{1}\), \(\pi_{2}\), and \(\pi_{3}\), which were based on each individualâ€™s region (NE, NW, SE, SW). Operationally, \(\pi_{i}\) samples an individual by first sampling a region with the given probability, and then sampling an individual uniformly at random from that region. We compare results against our method with data sampled uniformly from the population (red crosses), and to permutation tests (M2), also with uniformly sampled data. Even with randomized policies, we continue to outperform permutation tests.

**Acknowledgements.** We thank Jing Yu Koh and Shubhanshu Shekhar for helpful conversations. We also thank the anonymous referees for helpful feedback which improved the paper. BC and AR acknowledge support from NSF grants IIS-2229881 and DMS-2310718. BC was supported in part by the NSERC PGS D program, grant no. 567944. BW and SCG were supported in part by the AI2050 program at Schmidt Futures, grant no. G-22-64474.

## References

* [1]S. Mitchell, E. Potash, S. Barocas, A. D'Amour, and K. Lum (2021) Algorithmic fairness: choices, assumptions, and definitions. Annual Review of Statistics and Its Application8, pp. 141-163. Cited by: SS1.
* [2]P. Henderson, B. Chugg, B. Anderson, and D. E. Ho (2022) Beyond ads: sequential decision-making algorithms in law and public policy. In Proceedings of the 2022 Symposium on Computer Science and Law, pp. 87-100. Cited by: SS1.
* [3]D. Hellman (2020) Measuring algorithmic fairness. Virginia Law Review106 (4), pp. 811-866. Cited by: SS1.
* [4]J. Kleinberg, J. Ludwig, S. Mullainathan, and A. Rambachan (2018) Algorithmic fairness. In AEA papers and proceedings, Vol. 108, pp. 22-27. Cited by: SS1.
* [5]A. Woodruff, S. E. Fox, S. Rousso-Schindler, and J. Warshaw (2018) A qualitative exploration of perceptions of algorithmic fairness. In Proceedings of the 2018 CHI conference on human factors in computing systems, pp. 1-14. Cited by: SS1.
* [6]I. Deborah Raji and J. Buolamwini (2019) Actionable auditing: investigating the impact of publicly naming biased performance results of commercial AI products. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 429-435. Cited by: SS1.
* [7]J. Bandy (2021) Problematic machine behavior: a systematic literature review of algorithm audits. Proceedings of the ACM on human-computer interaction5, pp. 1-34. Cited by: SS1.
* [8]D. Metaxa, J. Sung Park, R. E. Robertson, K. Karahalios, C. Wilson, J. Hancock, and C. Sandvig (2021) Auditing algorithms: understanding algorithmic systems from the outside in. Foundations and Trends(r) in Human-Computer Interaction14 (4), pp. 272-344. Cited by: SS1.
* [9]B. Vecchione, K. Levy, and S. Barocas (2021) Algorithmic auditing and social justice: lessons from the history of audit studies. In Equity and Access in Algorithms, Mechanisms, and Optimization, pp. 1-9. Cited by: SS1.
* [10]I. Rejil, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud, D. Theron, and P. Barnes (2020) Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 33-44. Cited by: SS1.
* [11]J. R. Zech, M. A. Badgeley, M. Liu, A. B. Costa, J. J. Titano, and E. Karl Oermann (2018) Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine15 (11), pp. e1002683. Cited by: SS1.
* [12]S. Beery, G. Van Horn, and P. Perona (2018) Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), pp. 456-473. Cited by: SS1.
* [13]X. Liu, B. Glocker, M. M. McCradden, M. Ghassemi, A. K. Denniston, and L. Oakden-Rayner (2022) The medical algorithmic audit. The Lancet Digital Health. Cited by: SS1.
* [14]B. Taskesen, J. Blanchet, D. Kuhn, and V. A. Nguyen (2021) A statistical test for probabilistic fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 648-665. Cited by: SS1.
* [15]S. Mitchell, E. Potash, S. Barocas, A. D'Amour, and K. Lum (2021) Algorithmic fairness: choices, assumptions, and definitions. Annual Review of Statistics and Its Application8, pp. 141-163. Cited by: SS1.
* [16]S. Mitchell, E. Potash, S. Barocas, A. D'Amour, and K. Lum (2021) Algorithmic fairness: choices, assumptions, and definitions. Annual Review of Statistics and Its Application8, pp. 141-163. Cited by: SS1.
* [17]S. Mitchell, E. Potash, S. Barocas, A. D'Amour, and K. Lum (2021) Algorithmic fairness: choices, assumptions, and definitions. Annual Review of Statistics and Its Application8, pp. 141-163. Cited by: SS1.
* [18]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [19]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [20]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [21]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [22]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [23]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [24]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [25]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [26]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [27]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [28]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [29]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [30]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [31]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [32]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [33]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [34]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [35]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [36]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [37]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [38]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [39]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [40]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [41]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [42]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [43]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
* [44]S. Ri, S. R. Salam, and J. Buolamwini (2021) A survey of machine learning models for health and health. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 1-14. Cited by: SS1.
** [15] Nian Si, Karthyek Murthy, Jose Blanchet, and Viet Anh Nguyen. Testing group fairness via optimal transport projections. In _International Conference on Machine Learning_, pages 9649-9659. PMLR, 2021.
* [16] Abraham Wald. _Sequential analysis_. John Wiley and Sons, 1947.
* [17] Abraham Wald and Jacob Wolfowitz. Optimum character of the sequential probability ratio test. _The Annals of Mathematical Statistics_, pages 326-339, 1948.
* [18] Herbert Robbins. Some aspects of the sequential design of experiments. _Bulletin of the American Mathematical Society_, 58(5):527-535, 1952.
* [19] Donald A Darling and Herbert Robbins. Confidence sequences for mean, variance, and median. _Proceedings of the National Academy of Sciences_, 58(1):66-68, 1967.
* [20] Tze Leung Lai. On confidence sequences. _The Annals of Statistics_, pages 265-280, 1976.
* [21] Glenn Shafer. Testing by betting: A strategy for statistical and scientific communication. _Journal of the Royal Statistical Society: Series A (Statistics in Society), with discussion_, 184(2):407-431, 2021.
* [22] Aaditya Ramdas, Johannes Ruf, Martin Larsson, and Wouter Koolen. Admissible anytime-valid sequential inference must rely on nonnegative martingales. _arXiv preprint arXiv:2009.03167_, 2020.
* [23] Peter Grunwald, Rianne de Heide, and Wouter M Koolen. Safe testing. _Journal of the Royal Statistical Society, Series B (Methodological), forthcoming with discussion_, 2023.
* [24] Glenn Shafer and Vladimir Vovk. _Probability and finance: It's only a game!_, volume 491. John Wiley & Sons, 2005.
* [25] Glenn Shafer and Vladimir Vovk. _Game-theoretic foundations for probability and finance_, volume 455. John Wiley & Sons, 2019.
* [26] Ian Waudby-Smith and Aaditya Ramdas. Estimating means of bounded random variables by betting. _Journal of the Royal Statistical Society, Series B (Methodological), forthcoming with discussion_, 2023.
* [27] Aaditya Ramdas, Peter Grunwald, Vladimir Vovk, and Glenn Shafer. Game-theoretic statistics and safe anytime-valid inference. _Statistical Science_, 2023.
* [28] Shubhanshu Shekhar and Aaditya Ramdas. Nonparametric two-sample testing by betting. _IEEE Transactions on Information Theory_, 2023.
* [29] Jean Ville. Etude critique de la notion de collectif. _Bull. Amer. Math. Soc_, 45(11):824, 1939.
* [30] Johannes Ruf, Martin Larsson, Wouter M Koolen, and Aaditya Ramdas. A composite generalization of Ville's martingale theorem. _Electronic Journal of Probability (forthcoming)_, 2023.
* [31] Boyan Duan, Aaditya Ramdas, and Larry Wasserman. Interactive rank testing by betting. In _Conference on Causal Learning and Reasoning_, pages 201-235. PMLR, 2022.
* [32] Philip B Stark. Conservative statistical post-election audits. _The Annals of applied statistics_, 2 (2):550-581, 2008.
* [33] Ian Waudby-Smith, Philip B Stark, and Aaditya Ramdas. RiLACS: Risk limiting audits via confidence sequences. In _Electronic Voting: 6th International Joint Conference, E-Vote-ID 2021, Proceedings 6_, pages 124-139. Springer, 2021.
* [34] Shubhanshu Shekhar, Ziyu Xu, Zach Lipton, Pierre J. Liang, and Aaditya Ramdas. Risk-limiting financial audits via weighted sampling without replacement. _Uncertainty in Artificial Intelligence (UAI)_, 2023.

* [35] Francesco Orabona and David Pal. Coin betting and parameter-free online learning. _Advances in Neural Information Processing Systems_, 29, 2016.
* [36] Kwang-Sung Jun and Francesco Orabona. Parameter-free online convex optimization with sub-exponential noise. In _Conference on Learning Theory_, pages 1802-1823. PMLR, 2019.
* [37] Keyi Chen, John Langford, and Francesco Orabona. Better parameter-free stochastic optimization with ODE updates for coin-betting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6239-6247, 2022.
* [38] Ian Waudby-Smith, Lili Wu, Aaditya Ramdas, Nikos Karampatziakis, and Paul Mineiro. Anytime-valid off-policy inference for contextual bandits. _ACM/IMS Journal of Data Science_, 2023.
* [39] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. _Advances in neural information processing systems_, 29, 2016.
* [40] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data mining_, pages 797-806, 2017.
* [41] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In _Proceedings of the 3rd innovations in theoretical computer science conference_, pages 214-226, 2012.
* [42] Faisal Kamiran, Indre Zliobaite, and Toon Calders. Quantifying explainable discrimination and removing illegal discrimination in automated decision making. _Knowledge and information systems_, 35:613-644, 2013.
* [43] Aaditya Ramdas and Tudor Manole. Randomized and exchangeable improvements of Markov's, Chebyshev's and Chernoff's inequalities. _arXiv preprint arXiv:2304.02611_, 2023.
* [44] Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in Banach spaces. In _Conference On Learning Theory_, pages 1493-1529. PMLR, 2018.
* [45] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69(2-3):169-192, 2007.
* [46] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from a finite universe. _Journal of the American statistical Association_, 47(260):663-685, 1952.
* [47] RD Narain. On sampling without replacement with varying probabilities. _Journal of the Indian Society of Agricultural Statistics_, 3(2):169-175, 1951.
* [48] Peter Henderson, Ben Chugg, Brandon Anderson, Kristen Altenburger, Alex Turk, John Guyton, Jacob Goldin, and Daniel E Ho. Integrating reward maximization and population estimation: Sequential decision-making for Internal Revenue Service audit selection. _AAAI Conference on Artificial Intelligence_, 2023.
* [49] Ben Chugg, Peter Henderson, Jacob Goldin, and Daniel E Ho. Entropy regularization for population estimation. _AAAI Conference on Artificial Intelligence_, 2023.
* [50] I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. _Expert systems with applications_, 36(2):2473-2480, 2009.
* [51] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* [52] Brett Lantz. _Machine learning with R: expert techniques for predictive modeling_. Packt Publishing Limited, 2019.

* [53] Ilmun Kim, Sivaraman Balakrishnan, and Larry Wasserman. Minimax optimality of permutation tests. _The Annals of Statistics_, 50(1):225-251, 2022.
* [54] Muhammad Aurangzeb Ahmad, Arpit Patel, Carly Eckert, Vikas Kumar, and Ankur Teredesai. Fairness in machine learning for healthcare. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3529-3530, 2020.
* [55] Douglas B White and Derek C Angus. A proposed lottery system to allocate scarce COVID-19 medications: promoting fairness and generating knowledge. _Jama_, 324(4):329-330, 2020.
* [56] Ben Chugg, Lisa Lu, Derek Ouyang, Benjamin Anderson, Raymond Ha, Alexis D'Agostino, Anandi Sujeer, Sarah L Rudman, Analilia Garcia, and Daniel E Ho. Evaluation of allocation schemes of COVID-19 testing resources in a community-based door-to-door testing program. In _JAMA health forum_, volume 2, pages e212260-e212260. American Medical Association, 2021.
* [57] Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho. Outsider oversight: Designing a third party audit ecosystem for AI governance. In _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_, pages 557-571, 2022.
* [58] Nick Harvey. A second course in randomized algorithms. 2022.

Testing in batches

Of course, it is the rare application in which we can expect to receive an audit from each group at each timestep. We can modify the strategy in Section 3 to handle different arrival times by simply waiting to bet until we have new audits from each group. If multiple audits from the other group have accumulated during that period, we bet with their average. More formally, define

\[N_{t}^{b}=T_{b}[t]\cap\{n:n\geq\max(T_{1-b}[t])\},\;\;\text{and}\;\;G_{t}^{b}= \frac{1}{|N_{t}^{b}|}\sum_{j\in N_{t}^{b}}\widehat{Y}_{j}^{b},\] (15)

where we recall that \(T_{b}[t]=T_{b}\cap[t]\) is the set of times at which we receive audits from group \(b\) up until time \(t\). In words, \(N_{t}^{b}\) is simply all timesteps for which we receive an audit from group \(b\) without receiving an audit from the other group. From here, we can define a payoff function as

\[S_{t}=\begin{cases}1+\lambda_{t}(G_{t}^{0}-G_{t}^{1}),&\text{if}\;N_{t}^{0}\neq \emptyset\text{ and }N_{t}^{1}\neq\emptyset,\\ 1,&\text{otherwise}.\end{cases}\] (16)

If \(S_{t}=1\) then \(\mathcal{K}_{t}=\mathcal{K}_{t-1}\), so this can be interpreted as abstaining from betting at time \(t\). This occurs when we have no new audits from one group (\(N_{t}^{b}=\emptyset\) for some \(b\)). Note that if \(N_{t}^{0}\) and \(N_{t}^{1}\) are both non-empty, then one of them contains only \(t\). Thus, in this case, \(G_{t}^{0}-G_{t}^{1}\in\{\widehat{Y}_{t}^{0}-G_{t}^{1},G_{t}^{0}-\widehat{Y}_{ t}^{1}\}\). As in Section 3 we choose \(\lambda_{t}\) using ONS, but we use \(g_{t}=G_{t}^{0}-G_{t}^{1}\) in its definition. The expected stopping time of the test defined by (16) follows as a corollary from Proposition 1 after correcting for the number of timesteps during which non-trivial bets were placed. More specifically, the stopping-time in Proposition 1 is with reference to the number of times we actually bet, i.e., with respect to the times \(\{t:N_{t}^{0}\neq\emptyset,N_{t}^{1}\neq\emptyset\}\). The same remark holds for other propositions throughout this paper and, as such, we will state these propositions under the assumption that \(T_{0}=T_{1}=\mathbb{N}\).

Finally, let us emphasize that the above discussion is still assuming an online data gathering procedure. If we are in a nonsequential setting (i.e., all the data has already been gathered), then we may simulate a sequential setting by simply revealing the outcomes one at a time. Thus, all of our procedures hold for fixed-time hypothesis testing as well.

## Appendix B Auditing multiple groups

Here we consider the case when there are more than two groups. Suppose we have \(J+1\) groups \(\{0,1,\ldots,J\}\). In accordance with Definition 1, the null and alternative become

\[H_{0} :\mathbb{E}_{\rho}[\varphi(X)|\xi_{i}]=\mathbb{E}_{\rho}[\varphi (X)|\xi_{j}],\quad\forall i,j\in\{0,\ldots,J\},\] (17) \[H_{1} :\exists i,j\in\{0,\ldots,J\}\;\;\text{such that}\;\;\mathbb{E}_{ \rho}[\varphi(X)|\xi_{i}]\neq\mathbb{E}_{\rho}[\varphi(X)|\xi_{j}].\] (18)

As before, let \(\mu_{i}=\mathbb{E}_{\rho}[\varphi(X)|\xi_{i}]\), \(i\in\{0,\ldots,J\}\). One could derive a sequential test by applying Algorithm 1 to each pair of means \(\mu_{i}\) and \(\mu_{j}\). Game-theoretically, this can be interpreted as splitting your initial wealth among multiple games and playing each simultaneously. If you grow rich enough in any one game, you reject the null. Of course, one needs to adjust the significance level to account for the number of games being played, thus reducing the (nonasymptotic) power of the test.

Of course, it is not necessary to test each mean against all others. We need only test whether \(\mu_{b}=\mu_{b+1}\) for all \(b\in\{0,\ldots,J\}\). That is, we can play \(J\) games instead of \(\Omega(J^{2})\) games. In order to ensure this constitutes a level-\(\alpha\) test, we reject when the wealth process of any game is at least \(J/\alpha\). For any single game, this occurs with probability \(\alpha/J\) under the null. Therefore, the union bound then ensures that the type-I error of this procedure is bounded by \(\alpha\). Moreover, the asymptotic power remains one since, if \(\mu_{i}\neq\mu_{j}\) for some \(i,j\) then \(\mu_{b}\neq\mu_{b+1}\) for some \(b\). The guarantees we've provided on Algorithm 1 ensure that the wealth process for this particular game will eventually grow larger than \(J/\alpha\) and our test will reject. We summarize this discussion with the following proposition, which is the equivalent of Proposition 1 for auditing multiple groups.

**Proposition 5**.: _Let \(\alpha\in(0,1)\). Consider running Algorithm 1 on groups \(b,b+1\), for \(b\in\{0,1,\ldots,J-1\}\) in parallel with input parameter \(\alpha/J\). This constitutes a level-\(\alpha\) sequential test for (17) with aymptotic power one against (18). If we receive an audit from each group at each timestep, then the expected stopping time \(\tau\) of this procedure obeys_

\[\mathbb{E}[\tau]\lesssim\min_{b\in\{0,\ldots,J-1\}}\frac{1}{|\mu_{b}-\mu_{b+1} |^{2}}\log\bigg{(}\frac{J}{|\mu_{b}-\mu_{b+1}|^{2}\alpha}\bigg{)}.\] (19)The expected stopping time follows from Proposition 1 after correcting for the significance level and the difference between the means. We take the minimum over all \(b\) because the procedure rejects as soon as _any_ of the wealth processes grow too large. Equivalent versions of Propositions 2 and 3 for multiple groups can be obtained similarly.

Let us end by remarking that similar ideas may be applied to test other notions of group fairness which posit the equivalence of multiple means. This is the case of equalized odds, for instance. As above we simply imagine playing multiple games simultaneously, each testing for the equivalence of one pair of means.

## Appendix C Omitted proofs

### Proof of Proposition 1

We break the proof into three components.

Level-\(\alpha\) sequential test.Combining the discussion at the beginning of Section 3 with Ville's inequality demonstrates why our procedure constitutes a level-\(\alpha\) sequential test. However, let us prove it formally here for completeness. Let \(P\in H_{0}\) and note that \(\mathbb{E}_{P}[\widehat{Y}_{t}^{0}-\widehat{Y}_{t}^{1}]=\mathbb{E}_{P}[ \varphi(X_{t}^{0})-\varphi(X_{t}^{1})]=\mu_{0}-\mu_{1}=0\). Therefore, using the fact that \(\lambda_{t}\) is predictable (i.e., \(\mathcal{F}_{t-1}\)-measurable)

\[\mathbb{E}_{P}[\mathcal{K}_{t}|\mathcal{F}_{t-1}]=\mathbb{E}_{P}\bigg{[}\prod _{j=1}^{t}(1+\lambda_{j}(\widehat{Y}_{j}^{0}-\widehat{Y}_{j}^{1}))\bigg{|} \mathcal{F}_{t-1}\bigg{]}=\mathcal{K}_{t-1}(1+\lambda_{t}\mathbb{E}_{P}[ \widehat{Y}_{t}^{0}-\widehat{Y}_{t}^{1}])=\mathcal{K}_{t-1},\]

so \((\mathcal{K}_{t})_{t\geq 1}\) is a \(P\)-martingale, with initial value 1. Moreover, it is nonegative since \(|\lambda_{t}|\leq 1/2\) for all \(t\) by definition of ONS. Thus, Ville's inequality implies \(P(\exists t\geq 1:\mathcal{K}_{t}\geq 1/\alpha)\leq\alpha\), meaning that rejecting at \(1/\alpha\) yields a level-\(\alpha\) sequential test. Finally, as discussed in the main paper, the last lines of Algorithm 1 are justified by the randomized Ville's inequality of Ramdas and Manole [43], which states that, for all stopping times \(n\),

\[P(\exists t\leq n:\mathcal{K}_{t}\geq 1/\alpha\;\;\text{or}\;\;\mathcal{K}_{n}>U /\alpha)\leq\alpha,\]

where \(U\sim\text{Unif}(0,1)\) is independent of everything else.

Asymptotic power.Next, let us demonstrate that Algorithm 1 has asymptotic power one. That is, for \(P\in H_{1}\), \(P(\tau<\infty)=1\). It suffices to show that \(P(\tau=\infty)=0\). To see this, define

\[g_{t}:=\widehat{Y}_{t}^{0}-\widehat{Y}_{t}^{1},\quad S_{t}:=\sum_{i=1}^{t}g_{ i},\quad V_{t}:=\sum_{i=1}^{t}g_{i}^{2}.\]

We have the following guarantee on the wealth process, which can be translated from results concerning ONS from Cutkosky and Orabona [44, Theorem 1]:

\[\mathcal{K}_{t}\geq\frac{1}{V_{t}}\exp\bigg{\{}\frac{S_{t}^{2}}{4(V_{t}+|S_{ t}|)}\bigg{\}}\geq\frac{1}{t}\exp\bigg{\{}\frac{S_{t}^{2}}{8t}\bigg{\}},\quad \forall t\geq 1.\] (20)

Since \(\{\tau=\infty\}\subset\{\tau\geq t\}\) for all \(t\geq 1\), we have \(P(\tau=\infty)\leq\liminf_{t\to\infty}P(\tau>t)\leq\liminf_{t\to\infty}P( \mathcal{K}_{t}<1/\alpha)\), where the final inequality is by definition of the algorithm. Using the second inequality of (20),

\[P(\mathcal{K}_{t}<1/\alpha)\leq P\bigg{(}\exp\bigg{\{}\frac{S_{t}^{2}}{8t} \bigg{\}}<t/\alpha\bigg{)}\leq P\bigg{(}-\sqrt{\frac{8\log(t/\alpha)}{t}}< \frac{S_{t}}{t}<\sqrt{\frac{8\log(t/\alpha)}{t}}\bigg{)}.\]

By the SLLN, \(S_{t}/t\) converges to \(\mu_{0}-\mu_{1}\neq 0\) almost surely. On the other hand, \(8\log(t/\alpha)/t\to 0\). Thus, if we let \(A_{t}\) be the event that \(\exp(S_{t}^{2}/8t)<t/\alpha\), we see that \(\mathbf{1}(A_{t})\to 0\) almost surely. Hence, by the dominated convergence theorem,

\[P(\tau=\infty)\leq\liminf_{t\to\infty}P(A_{t})=\liminf_{t\to\infty}\int \mathbf{1}(A_{t})dP=\int\liminf_{t\to\infty}\mathbf{1}(A_{t})dP=0.\]

This completes the argument.

Expected stopping time.Last, let us show the desired bound on the expected stopping time. Fix \(P\in H_{1}\). Let \(\tau\) be the stopping time of the test. Since it is nonnegative, we have

\[\mathbb{E}[\tau]=\sum_{t=1}^{\infty}P(\tau>t)=\sum_{t=1}^{\infty}P(\log{\cal K }_{t}<\log(1/\alpha))=\sum_{t=1}^{\infty}P(E_{t}),\]

for \(E_{t}=\{\log{\cal K}_{t}<\log(1/\alpha)\}\). Note that the second equality is by definition of the algorithm. Employing the first inequality of (20) yields

\[E_{t} \subset\{S_{t}^{2}<4(V_{t}+|S_{t}|)(\log(1/\alpha)-\log(1/V_{t})\}\] \[\subset\bigg{\{}S_{t}^{2}<4\bigg{(}V_{t}+\sum_{i\leq t}|g_{i}| \bigg{)}(\log(1/\alpha)-\log(1/V_{t})\bigg{\}}.\]

To analyze the probability of this event, we first develop upper bounds on \(W_{t}:=\sum_{i\leq t}|g_{i}|\) and \(V_{t}\). We begin with \(W_{t}\). Since \(W_{t}\) is the sum of independent random variables in \([0,1]\), we apply the multiplicative Chernoff bound (e.g., [58]) to obtain

\[P(W_{t}>(1+\delta)\mathbb{E}[W_{t}])\leq\exp(-\delta^{2}\mathbb{E}[W_{t}]/3).\]

Setting the right hand side equal to \(1/t^{2}\) and solving for \(\delta\) gives \(\delta=\sqrt{6\log t/\mathbb{E}W_{t}}\). Thus, with probability \(1-1/t^{2}\), we have

\[W_{t}\leq\mathbb{E}W_{t}+\sqrt{6\mathbb{E}[W_{t}]\log t}=t+\sqrt{6t\log t}\leq 2 t\quad\forall t\geq 17,\] (21)

where we've used that \(\mathbb{E}[W_{t}]=\sum_{i\leq t}\mathbb{E}[|g_{i}|]\leq t\) since \(|g_{i}|\leq 1\). Following a nearly identical process for \(V_{t}\), we have that with probability \(1-1/t^{2}\),

\[V_{t}\leq\mathbb{E}[V_{t}]+\sqrt{6\mathbb{E}[V_{t}]\log t}\leq t+\sqrt{6t\log t }\leq 2t,\quad\forall t\geq 17,\] (22)

where again we use that \(|g_{i}|^{2}\leq|g_{i}|\leq 1\). Let \(H_{t}=\{V_{t}\leq 2t\}\cap\{W_{t}\leq 2t\}\). Then,

\[E_{t}\cap H_{t}\subset\{S_{t}^{2}<16t(\log(1/\alpha)+\log(2t)\}\subset\{|S_{t} |<\underbrace{4\sqrt{t\log(2t/\alpha)}}_{:=D}\}.\]

We now argue that \(|S_{t}|\) is unlikely to be so small. Indeed, since \(S_{t}\) is the sum of independent random variables in \([-1,1]\), applying a Chernoff bound for the third time gives \(P(|S_{t}-\mathbb{E}S_{t}|\geq u)\leq 2\exp(-u^{2}/t)\). So, with probability \(1-1/t^{2}\), by the reverse triangle inequality,

\[||S_{t}|-|\mathbb{E}S_{t}||\leq|S_{t}-\mathbb{E}S_{t}|\leq\sqrt{t\log 2t^{2}},\]

implying that,

\[|S_{t}|\geq|\mathbb{E}S_{t}|-\sqrt{t\log 2t^{2}}\geq t\Delta-\sqrt{2t\log 2t}.\]

This final quantity is at least \(D\) for all \(t\geq\frac{81}{\Delta^{2}}\log(\frac{162}{\Delta^{2}\alpha})\). Now, combining what we've done thus far, by the law of total probability,

\[P(E_{t})=P(E_{t}\cap H_{t})+P(E_{t}|H_{t}^{c})P(H_{t}^{c})\leq P(|S_{t}|<D)+P( H_{t}^{c})\leq 3/t^{2},\]

and so, for \(t\) large enough such that (21), (22), and \(S_{t}>D\) all hold, that is

\[T=\frac{81}{\Delta^{2}}\log\bigg{(}\frac{162}{\Delta^{2}\alpha}\bigg{)},\]

we have

\[\mathbb{E}[\tau]=\sum_{t\geq 1}P(E_{t})\leq T+\sum_{t\geq T}\frac{3}{t^{2}} \leq T+\frac{\pi^{2}}{2}.\]

This completes the proof.

### Proof of Proposition 2

The proof is similar to that of Proposition 1, so we highlight only the differences.

The wealth process remains a martingale due to the IPW weights (5). Indeed, since \(\lambda_{t}\) and \(L_{t}\) are \(\mathcal{F}_{t-1}\) measurable, under the null we have

\[\mathbb{E}[\mathcal{K}_{t}|\mathcal{F}_{t-1}] =\mathbb{E}\bigg{[}\prod_{j=1}^{t}(1+\lambda_{j}L_{j}(\widehat{Y} _{j}^{0}\omega_{j}^{0}-\widehat{Y}_{j}^{1}\omega_{j}^{1}))\bigg{|}\mathcal{F}_ {t-1}\bigg{]}\] \[=\mathcal{K}_{t-1}(1+\lambda_{t}L_{t}\mathbb{E}[\widehat{Y}_{t}^ {0}\omega_{t}^{0}-\widehat{Y}_{t}^{1}\omega_{t}^{1}])=\mathcal{K}_{t-1}(1+ \lambda_{t}L_{t}(\mu_{0}-\mu_{1}))=\mathcal{K}_{t-1}.\]

Moreover, as described in the text, multiplication by \(L_{t}\) ensures that \(\mathcal{K}_{t}\) is nonnegative, since

\[|L_{t}||\widehat{Y}_{t}^{0}\omega_{t}^{0}(X_{t}^{0})-\widehat{Y}_ {t}^{1}\omega_{t}^{1}(X_{t}^{1})| \leq L_{t}|\widehat{Y}_{t}^{0}\omega_{t}^{0}(X_{t}^{0})|+L_{t}| \widehat{Y}_{t}^{1}\omega_{t}^{1}(X_{t}^{1})|\] \[\leq L_{t}\omega_{t}^{0}(X_{t}^{0})+L_{t}\omega_{t}^{1}(X_{t}^{1} )\leq 1,\]

since

\[L_{t}\leq\frac{1}{2\omega_{t}^{0}(X_{t}^{0})},\]

for each \(b\) by definition. Therefore, \((\mathcal{K}_{t})_{t\geq 1}\) is a nonnegative martingale and, as before, Ville's inequality implies that rejecting at \(1/\alpha\) gives a level-\(\alpha\) sequential test.

The asymptotic power follows by replacing \(g_{t}\) in Appendix C.1 with

\[h_{t}=L_{t}(\widehat{Y}_{t}^{0}\omega_{t}^{0}-\widehat{Y}_{t}^{1}\omega_{t}^{ 1}).\]

Under the alternative, \(h_{t}\) has non-zero expected value, so identical arguments apply.

Regarding, the expected stopping time, we again argue about \(h_{t}\) instead of \(g_{t}\). Since \(|h_{t}|\leq 1\) (see above), the bounds on \(V_{t}\) and \(W_{t}\) remain as they are in the proof of Proposition 1. The bound on \(|\mathbb{E}[S_{t}]|\) is where the proof departs that in Appendix C.1. In this case we have

\[\mathbb{E}[S_{t}|\mathcal{F}_{t-1}]=S_{t-1}+L_{t}\mathbb{E}[\widehat{Y}_{t}^{ 0}\omega_{t}^{0}-\widehat{Y}_{t}^{1}\omega_{t}^{1}|\mathcal{F}_{t-1}]=S_{t-1} +L_{t}(\mu_{0}-\mu_{1}).\]

Therefore,

\[\mathbb{E}[S_{t}]=\mathbb{E}[\mathbb{E}[S_{t}|\mathcal{F}_{t-1}]]=\mathbb{E}[ S_{t-1}+L_{t}(\mu_{0}-\mu_{1})]=\mathbb{E}[S_{t-1}]+(\mu_{0}-\mu_{1}) \mathbb{E}[L_{t}].\]

Induction thus yields

\[|\mathbb{E}[S_{t}]|=\bigg{|}(\mu_{0}-\mu_{1})\sum_{i\leq t}\mathbb{E}[L_{i}] \bigg{|}=\Delta\bigg{|}\sum_{i\leq t}\mathbb{E}[L_{i}]\bigg{|}\geq\Delta tL_{ \inf}.\]

From here, we may replace \(\Delta\) in the proof in Appendix C.1 with \(\Delta L_{\inf}\) and the arithmetic remains the same. This yields the desired result.

### Proof of Proposition 3

Again, the proof mirrors that of Proposition 1 so we highlight only the differences.

First let us ensure that Algorithm 1 yields a level-\(\alpha\) sequential test. As before, it suffices to demonstrate that the wealth process is a nonnegative martingale. The time-varying means do not change this fact from before:

\[\mathbb{E}[\mathcal{K}_{t}|\mathcal{F}_{t-1}]=\mathbb{E}\bigg{[}\prod_{j=1}^{ t}(1+\lambda_{j}(\widehat{Y}_{j}^{0}-\widehat{Y}_{j}^{1}))\bigg{|}\mathcal{F}_{t-1} \bigg{]}=\mathcal{K}_{t-1}(1+\lambda_{t}\mathbb{E}[\widehat{Y}_{t}^{0}- \widehat{Y}_{t}^{1}|\mathcal{F}_{t-1}])=\mathcal{K}_{t-1},\]

since, under the null, \(\mathbb{E}[Y_{t}^{0}|\mathcal{F}_{t-1}]=\mathbb{E}[\varphi(X)|\xi_{0}, \mathcal{F}_{t-1}]=\mu_{0}=\mu_{1}=\mathbb{E}[\varphi(X)|\xi_{1},\mathcal{F}_ {t-1}]=\mathbb{E}[Y_{t}^{1}|\mathcal{F}_{t-1}]\). Nonnegativity once again follows from the ONS strategy.

Asymptotic power follows an identical argument as in Appendix C.1, so we focus on expected stopping time. The event \(E_{t}\) remains the same as in Appendix C.1. We again apply a Chernoff boundto \(W_{t}\) (the values remain independent, even though they are not necessarily identically distributed), and obtain

\[W_{t}\leq\mathbb{E}W_{t}+\sqrt{6\mathbb{E}[W_{t}]\log t}\leq 2t,\]

for \(t\geq 17\) with probability \(1-1/t^{2}\), since again, \(|g_{i}|\leq 1\) for each \(i\). Similarly, \(\mathbb{E}V_{t}\leq 2t\) with probability \(1-1/t^{2}\) for \(t\geq 17\). Let the shift begin at time \(n\), and set \(\Delta=\inf_{t\geq n}|\mu_{0}(t)-\mu_{1}(t)|\). Then \(|\mathbb{E}S_{t}|\geq(t-n)\Delta\). As above, we want to find \(t\) such that

\[|S_{t}|\geq|\mathbb{E}S_{t}|-\sqrt{t\log 2t^{2}}\geq(t-n)\Delta-\sqrt{t\log 2t^{2} }\geq D.\]

Rearranging and simplifying this final inequality, we see that it suffices for \(t\) to satisfy

\[t-n\geq\frac{6}{\Delta}\sqrt{t\log(2t/\alpha)}.\] (23)

We claim this holds for all

\[t\geq n+\max\bigg{\{}n,\frac{108}{\Delta^{2}}\log\bigg{(}\frac{4\cdot 108}{ \Delta^{2}\alpha}\bigg{)}\bigg{\}}.\]

To see this, suppose first that \(n\geq\beta\) where

\[\beta=\frac{108}{\Delta^{2}}\log\bigg{(}\frac{4\cdot 108}{\Delta^{2}\alpha} \bigg{)}.\]

Then, at \(t=2n\), the right hand side of (23) is

\[\frac{6}{\Delta}\sqrt{2n\log(2n/\alpha)}\leq n,\]

where the final inequality holds for all \(n\geq\beta\), which was assumed. Now suppose that \(n<\beta\), so that (23) should hold for \(t\geq n+\beta\). Since the left hand side of (23) grows faster than the right hand side, it suffices to show that it holds at \(t=n+\beta\). To this end, write

\[\frac{6}{\Delta}\sqrt{t\log(2t/\alpha)}\bigg{|}_{t=n+\beta} \leq\frac{6}{\Delta}\sqrt{(n+\beta)\log(2n/\alpha+2\beta/\alpha)}\] \[\leq\frac{6}{\Delta}\sqrt{2\beta\log(4\beta/\alpha)}\] \[=\frac{72}{\Delta^{2}}\sqrt{\log\bigg{(}\frac{4\cdot 108}{\Delta^ {2}\alpha}\bigg{)}\log\bigg{(}\frac{4\cdot 108}{\Delta^{2}\alpha}\log\bigg{(} \frac{4\cdot 108}{\Delta^{2}\alpha}\bigg{)}\bigg{)}}\] \[=\frac{72}{\Delta^{2}}\sqrt{\log\bigg{(}\frac{4\cdot 108}{\Delta^ {2}\alpha}\bigg{)}\log\bigg{(}\frac{4\cdot 108}{\Delta^{2}\alpha}\bigg{)}+\log \log\bigg{(}\frac{4\cdot 108}{\Delta^{2}\alpha}\bigg{)}}\] \[\leq\frac{108}{\Delta^{2}}\log\bigg{(}\frac{4\cdot 108}{\Delta^ {2}\alpha}\bigg{)}=\beta,\]

where the final inequality uses the (loose) bound \(\log\log(x)\leq\log^{2}(x)\).

### Proof of Proposition 4

First let us note that \((\mathcal{K}_{t}^{Q})\) are indeed nonnegative supermartingales. Under the null \(H_{0}^{\prime}\), we have \(\mathbb{E}_{H_{0}^{\prime}}[\widehat{Y}_{t}^{0}-\widehat{Y}_{t}^{1}]\leq\epsilon\), so

\[\mathbb{E}_{H_{0}^{\prime}}[Q_{t}^{R}|\mathcal{F}_{t-1}]=1+\lambda_{t}(\mathbb{ E}_{H_{0}^{\prime}}[\widehat{Y}_{t}^{0}-\widehat{Y}_{t}^{1}]-\epsilon)\leq 1.\]

Moreover, for \(\lambda_{t}\in[\frac{-1}{1-\epsilon},\frac{1}{1+\epsilon}]\), \(Q_{t}\geq 0\). The argument is similar for \(R_{t}\). Therefore, by Ville's inequality,

\[P(\exists t\geq 1:\mathcal{K}_{t}^{Q}\geq 2/\alpha)\leq\alpha/2,\ \text{ and }\ P(\exists t\geq 1:\mathcal{K}_{t}^{R}\geq 2/\alpha)\leq \alpha/2,\]

so the union bound gives \(P(\exists t\geq 1:\mathcal{K}_{t}^{Q}\geq 2/\alpha\) or \(\mathcal{K}_{t}^{R}\geq 2/\alpha)\leq\alpha\). This implies that the proposed test, which involves rejecting if \(\max\{\mathcal{K}_{t}^{Q},\mathcal{K}_{t}^{R}\}\geq 2/\alpha\) is a level-\(\alpha\) sequential test.

Now, note that ONS restricts \(\lambda_{t}\) to be in \([-1/2,1/2]\), which is a subset of \([\frac{-1}{1-\epsilon},\frac{1}{1+\epsilon}]\). We can therefore use a similar ONS analysis as above, without needing to modify the guarantees to account for the range of \(\lambda\). Recall from Section C.1 the definitions \(g_{t}\), \(S_{t}\), and \(V_{t}\). Because \(Q_{t}\) and \(R_{t}\) have an extra \(\epsilon\) term, we need to modify these terms for this new setting. Define

\[S_{t}^{\prime}=\sum_{i=1}^{t}(g_{i}-\epsilon),\qquad V_{t}^{\prime}=\sum_{i=1} ^{t}(g_{i}-\epsilon)^{2}.\]

The guarantee given by (20) translates, in this setting, to

\[\mathcal{K}_{t}^{Q}\geq\frac{1}{V_{t}^{\prime}}\exp\left\{\frac{(S_{t}^{ \prime})^{2}}{4(V_{t}^{\prime}+|S_{t}^{\prime}|)}\right\},\]

where we've simply replaced \(S_{t}\) and \(V_{t}\) by \(S_{t}^{\prime}\) and \(V_{t}^{\prime}\). The same guarantee holds for \(\mathcal{K}_{t}^{Q}\) (there \(g_{i}\) is replaced with \(-g_{i}\) but it makes no difference).

Results for asymptotic power (for both \(\mathcal{K}_{t}^{Q}\) and \(\mathcal{K}_{t}^{R}\)) follow from similar arguments as in Section C.1 but replacing \(S_{t}\) and \(V_{t}\) with \(S_{t}^{\prime}\) and \(V_{t}^{\prime}\). Therefore, let us turn our attention to expected stopping time. We will consider the expected stopping time of the test based on \(\mathcal{K}_{t}^{Q}\). Since identical arguments hold for \(\mathcal{K}_{t}^{R}\), one need only multiply the result by 2 to get the overall expected stopping time.

Following what was done in previous proofs, write \(\mathbb{E}[\tau]=\sum_{t=1}^{\infty}P(E_{t})\), where \(E_{t}=\{\log\mathcal{K}_{t}^{Q}<\log(1/\alpha)\}\). We have

\[E_{t} \subset\{(S_{t}^{\prime})^{2}<4(V_{t}^{\prime}+|S_{t}^{\prime}|)( \log(1/\alpha)-\log(1/V_{t}^{\prime}))\}\] \[\subset\{(S_{t}^{\prime})^{2}<4\left(V_{t}+t\epsilon^{2}+|S_{t}|+t \epsilon\right)(\log(1/\alpha)-\log(1/V_{t}^{\prime}))\}\] \[\subset\left\{(S_{t}^{\prime})^{2}<4\left(V_{t}+t\epsilon^{2}+ \sum_{i\leq t}|g_{i}|+t\epsilon\right)(\log(1/\alpha)-\log(1/V_{t}^{\prime})) \right\}.\]

Here we've used that \(V_{t}^{\prime}\leq V_{t}+t\epsilon^{2}\) and \(|S_{t}^{\prime}|\leq|S_{t}|+t\epsilon\). As in Section C.1, we have \(W_{t}=\sum_{i\leq t}|g_{i}|\leq 2t\) and \(V_{t}\leq 2t\) with probability \(1-t^{2}\) for all \(t\geq 17\). Let \(H_{t}\) be the event that both \(V_{t}\leq 2t\) and \(W_{t}\leq 2t\). Note that by the reverse triangle inequality, \(|S_{t}^{\prime}|\geq|S_{t}|-t\epsilon\). Therefore,

\[E_{t}\cap H_{t} \subset\{|S_{t}^{\prime}|^{2}<(16t+t(\epsilon+\epsilon^{2}))( \log(1/\alpha)+\log(2t+t\epsilon^{2}))\}\] \[\subset\{|S_{t}^{\prime}|<\sqrt{(16t+t(\epsilon+\epsilon^{2}))( \log(1/\alpha)+\log(2t+t\epsilon^{2}))}\}\] \[\subset\{|S_{t}|<t\epsilon+\sqrt{(16t+t(\epsilon+\epsilon^{2}))( \log(1/\alpha)+\log(2t+t\epsilon^{2}))}\}\] \[\subset\{|S_{t}|<t\epsilon+\sqrt{18t(\log(1/\alpha)+\log(3t))}\},\]

where we've used that \(\epsilon<1\). With probability \(1-1/t^{2}\), \(|S_{t}|\geq t\Delta-\sqrt{2t\log(2t)}\). The minimum \(t=T\) such that \(t\Delta-\sqrt{2t\log(2t)}\geq t\epsilon+\sqrt{18t(\log(1/\alpha)+\log(3t))}\) is on the order of

\[T\lesssim\frac{1}{(\Delta-\epsilon)^{2}}\log\left(\frac{1}{\alpha(\Delta- \epsilon)^{2}}\right).\]

Therefore,

\[\mathbb{E}[\tau]=\sum_{t=1}^{\infty}P(E_{t})\leq T+\sum_{t>T}(P(E_{t}\cap H_{t })+P(H_{t}^{c}))\leq T+\sum_{t>T}\frac{3}{t^{2}}\lesssim T.\]

Note that this is under the alternative, so \(\Delta>\epsilon\) and \(0<\Delta-\epsilon<\Delta\).

## Appendix D Simulation Details

Code to recreate all plots and run the simulations is available at https://github.com/bchugg/auditing-fairness. Here we provide more extensive details on each figure.

Figure 1.Given \(\Delta\), we generate the two means \(\mu_{0}\) and \(\mu_{1}\) as \(\mu_{0}=0.5+\Delta/2\) and \(\mu_{1}=0.5-\Delta/2\). We take \(\varphi(X)|\xi_{b}\) to be distributed as \(\text{Ber}(\mu_{b})\). (Thus, this simulates a scenario for which we witness the classifcation decisions, not e.g., a risk score.) We set \(\alpha=0.01\), so we reject when the wealth process is at least \(100\). We receive a pair of observations each timestep. Each experiment was run 100 times to generate the plotted standard deviation around the mean of each wealth process.

Figure 2.As above, we take the distribution of model observations \(\varphi_{t}(X)|\xi_{b}\) to be \(\text{Ber}(\mu_{b}(t))\). For the left hand side of Figure 2 we take \(\mu_{0}(t)=\mu_{1}(t)=0.3\) for \(t=1,\dots,99\). At \(t=100\), we add a logistic curve to \(\mu_{1}\). In particular, we let

\[\mu_{1}(t)=0.3+\frac{0.5}{1+\exp((250-t)/25)},\quad t\geq 100.\]

We keep \(\mu_{0}(t)\) at 0.3 for all \(t\). For the right hand side of Figure 2, we let both \(\mu_{1}\) and \(\mu_{0}\) be noisy sine functions with different wavelengths. We let

\[\mu_{0}(t)=\frac{\sin(t/40)}{10}+0.4+\epsilon_{t}^{0},\]

for all \(t\), where \(\epsilon_{t}^{0}\sim N(0,0.01)\). Meanwhile,

\[\mu_{1}(t)=\frac{\sin(t/20)}{10}+0.4+\frac{t}{1000}+\epsilon_{t}^{1},\]

where, again, \(\epsilon_{t}^{1}\sim N(0,0.01)\). The mean \(\mu_{1}(t)\) thus has a constant upward drift over time. As before, we assume we receive a pair of observations at each timestep and we take \(\alpha=0.01\). We generate the means once, but run the sequential test 100 times in order to plot the standard deviation around the mean of the wealth process.

Figures 3 and 4.For a given sequential test and a given value of \(\alpha\), we run (i) the test under the null hypothesis (i.e., with a fair model; we describe how we generated a fair model below), and (ii) the test under the alternative. Repeating 300 times and taking the average gives the FPR and average rejection time for this value of \(\alpha\). This procedure is how the leftmost two columns of Figure 3 were constructed. The final column then simply plots the FPR versus the value of \(\alpha\).

We used a vanilla random forest for both the credit default dataset and the US census data. For the credit default dataset, the model does not satisfy equality of opportunity [39] when \(Y\) indicates whether an individual has defaulted on their loan, and \(A\) indicates whether or not they have any university-level education. One can imagine loans being given or withheld on the basis of whether they are predicted to be returned; we might wish that this prediction does not hinge on educational attainment. For the census data, the model does not satisfy equality of opportunity when \(A\) indicates whether an individual has an optical issues, and \(Y\) indicates whether they are covered by public insurance. Admittedly, this example is somewhat less normative than the other. It is unclear whether we should expect perfect equality of opportunity in this context. However, we emphasize that our experiments are for illustrative purposes only. They are not meant as comments on the actual fairness or unfairness of these datasets. We interface with the census data by means the folktables package [51].

For the credit default dataset and random forest classifier, we have \(\Delta=|\mu_{0}-\mu_{1}|=0.034\). We have \(\Delta=0.09\) for the census data. To construct the fair model (in order to test the null), we add \(\Delta\) to the model predictions of the group with the lower mean. Thus, the distributions of predictions are different but the means are identical.

Figure 4 follows similar experimental logic, but we begin with a fair model (i.e., group predictions with the same mean, generated as above), and then switch to the unfair random forest classifier at time \(t=400\).

Figure 5.We use US health insurance data [52], which is synthetic data generated based on US census information. We train a logistic regression model to predict the risk of non-repayment based on 11 covariates which include age, BMI, gender, region, and whether the individual is a smoker. We find that the predicted risks are different among males (0.235) and females (0.174), so we use \(A\) as gender and use equality of opportunity as our notion of group fairness.

We construct three data collection policies, \(\pi_{1},\pi_{2},\) and \(\pi_{3}\), which are probability distributions over four regions: NE, SE, NW, SW. Data-collection works by first sampling a region with the probability prescribed by \(\pi_{i}\), and then sampling an individual in that region uniformly at random. We defined the three strategies as follows:

\[\pi_{1}(NE) =0.1,\ \pi_{1}(NW)=0.2,\ \pi_{1}(SE),\ 0.3,\ \pi_{1}(SW)=0.4,\] \[\pi_{2}(NE) =0.05,\ \pi_{2}(NW)=0.15,\ \pi_{2}(SE),\ 0.25,\ \pi_{2}(SW)=0.55,\] \[\pi_{3}(NE) =0.05,\ \pi_{3}(NW)=0.1,\ \pi_{3}(SE),\ 0.15,\ \pi_{3}(SW)=0.7.\]

We also ran our-betting based method with data sampled uniformly for the population, which served as the comparison point. Of course, as is expected from Proposition 1 and 2, data sampled uniformly from the population yields better results. We also compared to permutation tests run on data sampled uniformly from the population. These were implemented as described above and we do not belabor the details here.