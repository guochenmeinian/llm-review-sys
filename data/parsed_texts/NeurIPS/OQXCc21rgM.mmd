# Van der Pol-informed Neural Networks for Multi-step-ahead Forecasting of Extreme Climatic Events

 Anurag Dutta

GCETT Serampore, India

anuragdutta.research@gmail.com &Madhyima Panja

IIIT Bangalore, India

madhurima.panja@iiitb.ac.in &Uttam Kumar

IIIT Bangalore, India

uttam@iiitb.ac.in &Chittaranjan Hens

IIIT Hyderabad, India

chitaranjan.hens@iiit.ac.in &Tanujit Chakraborty

Sorbonne University Abu Dhabi, UAE

tanujit.chakraborty@sorbonne.ae

###### Abstract

Deep learning has produced excellent results in several applied domains including computer vision, natural language processing, speech recognition, etc. Physics-informed neural networks (PINN) are a new family of deep learning models that combine prior knowledge of physics in the form of high-level abstraction of natural phenomena with data-driven neural networks. PINN has emerged as a flourishing area of scientific computing to deal with the challenges of shortage of training data, enhancing physical plausibility, and specifically aiming to solve complex differential equations. However, building PINNs for modeling and forecasting the dynamics of extreme climatic events of geophysical systems remains an open scientific problem. This study proposes Van der Pol-informed Neural Networks (VPINN), a physics-informed differential learning approach, for modeling extreme nonlinear dynamical systems such as climatic events, exploiting the physical differentials as the physics-derived loss function. Our proposal is compared to state-of-the-art time series forecasting models, showing superior performance. The codes and dataset used for the experiments are made available at https://github.com/mad-stat/VPINN.

## 1 Introduction

Recently with the growing threat of climate change, there has been a global surge in the emergence and intensity of extreme climatic events [20]. The abrupt occurrence of devastating natural calamities like earthquakes, hurricanes, droughts, and floods poses significant challenges to human lives, infrastructure, ecosystems, and economies. Such tragic real-world extreme events include the 2023 Marrakesh earthquake in Morocco, the earthquake in Turkey, the cyclone Biparjoy in India, the recent flood in Spain, and the oceanic rouge wave near Newfoundland. To address the severity of these extreme climatic phenomena, accurate forecasts of these abrupt events have become imperative to government agencies for disaster preparedness and adaptation.

In recent decades, researchers have increasingly delved into the study of extreme events using dynamical systems and data-driven machine learning techniques, although typically in isolation[10; 9; 5; 4]. Accurate prediction of extreme events through physics-based dynamical systems relies heavily on understanding the system's inherent dynamics. In contrast, machine learning models, such as reservoir computing network (RCN) [16], long short-term memory (LSTM) [7], bidirectional LSTM (Bi-LSTM) [17], Prophet [19], and NBeats [12], open up a realm of data-driven predictions for extreme events without necessitating a comprehensive understanding of the system's dynamics. These data-centric models are widely employed to explore the emergence of extreme climatic events through training over a significantly large dataset. While all these methods share the common goal of delivering precise short-term forecasts by capturing long-term system trajectories, they do not explicitly account for the physical laws that inherently govern these chaotic systems.

Recently, the integration of physical knowledge into data-driven models has received significant attention among researchers for their ability to model real-world phenomena by addressing the everlasting challenges of modeling chaotic systems. Physics-informed neural networks (PINN) [14; 8] integrates prior physics knowledge within data-driven neural networks using a series of differential equations. The physical knowledge of the PINN models enables them to enhance training data, modify the architectural design of the network, and perform physics-informed optimization depending on the location of the knowledge integration. In recent studies, PINN-based approaches have been widely used as a function approximator in various applications such as modeling 3D temperature data using physical laws [24], simultaneous prediction of observed and unobserved variables in chaotic systems [13], predicting the lake surface temperature [2], and predicting the effect of cloud processes on climate [1].

This work aims to enhance the forecasting ability of the LSTM networks for extreme climatic events dataset by embedding the physical laws of the Van der Pol Oscillator into the framework. The proposed Van der Pol-informed Neural Networks (VPINN) approach learns the dynamics of the nonlinear oscillator and integrates the learned representations with the temporal dynamics of the data to generate a multi-step ahead forecast of the desired horizon.

### Contributions of this work

VPINN is introduced to model the temporal evolution of real-world extreme climatic events, including seismic waves, sea surface temperature, humidity, wind speed, and temperature for varied locations. We obtain the ground truth data from various open sources and also simulate a dataset from the Van der Pol oscillator, as outlined in Section 2. We then infuse the dynamics of this oscillator into a conventional LSTM framework, both through a transfer learning mechanism and the utilization of a physics-informed loss function for capturing the complex patterns inherent in the extreme events time series data. In contrast to the statistical models such as exponential smoothing (SES) [6] and the deep learning models, the VPINN approach exhibits the capability to learn the complex behavior of the data and is also suitable for handling limited data problems.

Our experimental results demonstrate that the proposed VPINN outperforms existing state-of-the-art models for multi-step forecasts across various time horizons in majority scenarios. Notably, we observe a significant improvement in forecasting accuracy, with the VPINN model enhancing the performance of the LSTM network by 53.47% due to the integration of the physical laws in the model. These simple yet essential experiments underscore the importance of integrating physics-informed forecasting techniques, such as the VPINN framework, into the existing data-driven climate modeling paradigms.

## 2 Preliminaries

**Van der Pol system:** Nonlinear oscillator systems have applications across a broad spectrum of physical phenomena, spanning atmospheric physics, nonlinear optics, plasma physics, electronics, biophysics, and chemical reactions, among many others [18]. These systems are characterized by their nonlinearity, which often leads to complex and chaotic behavior. They exhibit multiple equilibria, modulation in amplitude and frequency, and sensitivity to initial conditions, making them crucial in modeling the complexity and diversity of real-world phenomena. For instance, Van der Pol oscillator systems serve as a valuable tool for the study of extreme events. The Van der Pol equation, used in modeling nonlinear dynamical systems, is a non-conservative self-oscillatory system with nonlinear damping [21]. This mathematical formulation evolves in time following a second-order differential equation, taking the following form:

\[\frac{d^{2}x}{dt^{2}}-\mu(1-x^{2})\frac{dx}{dt}+x=0,\] (1)

where the position coordinate \(x\) is a function of time \(t\) and the damping strength of the oscillation is expressed using a positive scalar parameter \(\mu\). The Van der Pol system has a unique stable limit cycle, i.e., when time is close to infinity all nearby solutions of Eq. 1 tend towards a periodic solution. Moreover, the Van der Pol oscillator exhibits a chaotic dynamical nature following [11], which allows this dynamical system to efficiently model chaotic datasets.

**Proposition 1**.: _The non-conservative Van der Pol oscillator's dynamics evolving in time is chaotic._

Proof.: The state-space equation of the Van der Pol oscillator (Eq. 1) can be expressed as,

\[\frac{d\psi_{1}}{dt}=\psi_{2}\quad\text{and}\quad\frac{d\psi_{2}}{dt}=-\psi_{1 }-\mu\psi_{2}\left(1-{\psi_{1}}^{2}\right),\] (2)

where \(\psi_{1}=x\) and \(\psi_{2}=\frac{dx}{dt}\). In vector notation Eq. 2 can be represented as:

\[\frac{d\Psi\left(t\right)}{dt}=\mathcal{H}\left(\psi\left(t\right),\;\mu \right),\] (3)

where, \(\Psi=\left[\begin{array}{cc}\psi_{1}\left(t\right)&\psi_{2}\left(t\right) \end{array}\right]^{T}\) is the space vector and \(\mathcal{H}=\left[\begin{array}{cc}\mathcal{H}_{1}&\mathcal{H}_{2}\end{array} \right]^{T}\) is the coefficient vector. The dynamics of Eq. 3 when subjected to small deviations from the defined trajectory would be:

\[\delta\left(\frac{d\Psi\left(t\right)}{dt}\right)=\mathcal{L}_{i,j}\left(\Psi \left(t\right)\right)\delta\Psi;\;\;i,j=1,2,\]

where \(\mathcal{L}_{i,j}=\frac{\partial\mathcal{H}_{i}}{\partial\psi_{j}}\) is the Jacobian Matrix, comprising of derivatives. The chaotic behavior of the dynamical system can be inferred based on the positive value of the maximal Lyapunov exponent. Following, [23] the maximal Lyapunov exponent of the system can be defined,

\[\lambda_{\max}=\lim_{t\rightarrow\infty}\frac{1}{t}\log\frac{\left\|\delta \Psi\left(t\right)\right\|}{\left\|\delta\Psi\left(0\right)\right\|}.\]

By utilizing the Runge-Kutta method of order 4, [11] showed that \(\lambda_{\max}\approx 0.095\) (> 0). Hence, the dynamics of the Van der Pol oscillator are chaotic in nature.

**Long-Short Term Memory (LSTM):** LSTM networks represent a modification of classical Recurrent Neural Networks (RNNs) designed to address the vanishing gradient problem, thereby enhancing training stability [7]. These networks are widely employed in various sequential learning tasks, including natural language processing, machine translation, image captioning, and time series analysis. Their distinctive chain-like architecture consists of three key components: the _input gate_, _output gate_, and _forget gate_. This gating mechanism regulates the information flow within the cell state, serving as the long-term memory storage and the hidden state, representing its short-term counterpart. Given the input vector (say \(x_{i}\)) at the \(i^{th}\) time step, the forget gate determines how much information from the previous hidden state \(\tilde{h}_{i-1}\) should be retained at time \(i\). It uses a sigmoidal activation function over a weighted combination of \(x_{i}\) and \(\tilde{h}_{i-1}\). Thus the resulting activation vector \(F_{i}\), indicating how much information to forget or keep obtained as:

\[F_{i}=\sigma\left(W_{1}^{x}x_{i}+W_{1}^{\tilde{h}}\tilde{h}_{i-1}+b_{1}\right),\]

with \(W_{1}^{x},W_{1}^{\tilde{h}}\) as the weights and \(b_{1}\) as the bias. The input gate, on the other hand, utilizes the sigmoidal and tanh activation functions to update the cell state with the current input. The two activation vectors of the input gate are computed as:

\[I_{i}=\sigma\left(W_{2}^{x}x_{i}+W_{2}^{\tilde{h}}\tilde{h}_{i-1}+b_{2}\right) \text{ and }\;g_{i}=\tanh\left(W_{3}^{x}x_{i}+W_{3}^{\tilde{h}}\tilde{h}_{i-1}+b_{3} \right),\]

where \(W\) and \(b\) indicate the weights and bias respectively. The current cell state \(C_{i}\) is then calculated by combining the output from the forget gate and the input gate as:

\[C_{i}=F_{i}\odot C_{i-1}\oplus I_{i}\odot g_{i},\]where \(\odot\) is the point-wise multiplication and \(\oplus\) is the direct sum operator. This combination ensures both long-term and short-term memory components are appropriately considered. In the output gate, the current hidden state is updated using sigmoidal and tanh activation functions. These activations determine the new hidden state as:

\[O_{i}=\sigma\left(W_{4}^{x}x_{i}+W_{4}^{\tilde{h}}\tilde{h}_{i-1}+b_{4}\right) \text{~{}~{}and~{}~{}}\tilde{h}_{i}=O_{i}\odot\tanh\left(C_{i}\right).\]

Finally, \(\tilde{h}_{i}\) is used to compute the output at the current time step as \(\hat{y}_{i}=\sigma\left(\tilde{W}\tilde{h}_{i}+b_{5}\right)\). Overall, LSTM's robust long-term memory retention capabilities make them valuable for modeling the complexities of real-world phenomena [22].

## 3 Proposed Approach

The proposed VPINN framework seamlessly integrates the information from both the real measurements and the chaotic dynamics of the Van der Pol oscillator. This model leverages the transfer learning approach and physics-based regularized loss function to significantly improve the modeling and forecasting capabilities of the LSTM network. Training the sequential structure of the LSTMs can be computationally intensive and time-consuming. To address this challenge, we provide prior knowledge via pre-training to our VPINN model in a task-agnostic manner. Our proposed architecture, as illustrated in Figure 1, is a sequential approach that learns through a combination of transfer learning and data-driven learning. In the first phase, we generate a synthetic dataset by simulating data points from the nonlinear Van der Pol oscillator (as in Eq. 1) with \(\mu=4\) using the Runge-Kutta method. The second phase of the architecture involves training a standard LSTM network [7] on both the real-time series and the time derivatives of the simulated series while enforcing the physical law as a regularization term in the network. The proposed framework aims to learn the complex patterns and the chaotic behavior of the data-generating mechanism using historical values and time derivatives. To compute the physics-based regularization term, we follow the transductive PINN model [14], where time-indexed inputs are provided to a regularized multi-layered perceptron to generate the solution of the differential equation as the output. The regularization term in PINN amounts to differentiating the network and computing the time derivatives using automatic differentiation. Since real-world extreme event datasets consist of discrete observations, it becomes challenging to use automatic differentiation for computing the time derivatives. To mitigate this issue, we compute the discrete derivatives of the time series using the First Principle of Derivatives. Thus for a simulated time series \(x(t)\) indexed at time \(t\), we compute the discrete-time derivatives as:

\[\frac{dx}{dt}=\frac{x\left(t+\delta t\right)-x(t)}{\delta t},\] (4)

where \(\delta t\) is the time lag. In our framework, we set \(\delta t=1\) since real-world time series datasets are recorded chronologically in time. Once the simulated data is generated, the VPINN network receives the tuple \(y_{\text{Real}}^{(t)}=\left\{y_{t}^{*},\frac{dx}{dt},\frac{d^{2}x}{d t^{2}}\right\}\) as input, where \(y_{t}^{*}\) represents the training data value and \(\frac{dx}{dt},\frac{d^{2}x}{dt^{2}}\) are the first-order and second-order time derivatives of \(x(t)\) computed using Eq. 4, respectively. The network aims to predict the values of the subsequent time steps in a multivariate setting. The physical dynamics of the Van der Pol oscillator are imposed on the proposed model through both transfer learning and the introduction of a physics-based loss function. To compute a quantifiable measure of physical consistency within the model, the predicted values \(\hat{y}_{\text{Pred}}^{(t)}=\left\{\hat{y}_{t}^{*},\frac{\hat{dx}}{dt},\frac{d ^{2}x}{dt^{2}}\right\}\) have to satisfy the Van der Pol equation. Therefore, based on Eq. 1, the physics-based loss function for enforcing the dynamics of the Van der Pol oscillator on the predicted values can be calculated as follows:

\[\text{Loss}^{\text{Phys}}=\frac{d^{2}\hat{y}_{\text{Pred}}^{(t)}}{dt^{2}}-\mu \left(\frac{d\hat{y}_{\text{Pred}}^{(t)}}{dt}-\left(\hat{y}_{\text{Pred}}^{(t)} \right)^{2}\frac{d\hat{y}_{\text{Pred}}^{(t)}}{dt}-\frac{\hat{y}_{\text{Pred}}^ {(t)}}{\mu}\right).\] (5)

This physical information is integrated into the objective function of our proposed VPINN model through a modification of the conventional loss function, \(\text{Loss}^{\text{Data}}\), which is calculated using the model predictions and the true output labels as

\[\text{Loss}^{\text{Data}}=\text{RMSE}\left(y_{t}^{*},\hat{y}_{t}^{*}\right),\] (6)where \(\mathrm{RMSE}\) is the Root Mean Square Error. In contrast, the modified loss function includes the additional physics-based loss, formulated using Eq. 6 and Eq. 5 as follows:

\[\mathrm{Loss}^{\mathrm{Total}}=\mathrm{Loss}^{\mathrm{Data}}+\lambda_{\text{Phy}} \,\mathrm{Loss}^{\text{Phy}},\] (7)

where \(\lambda_{\text{Phy}}\) represents the hyperparameter corresponding to the physics-based loss function. The Eq. 7 is designed to enhance the model's generalization performance by simultaneously optimizing accuracy and ensuring physical consistency. Since the final loss function is nearly differentiable everywhere, we employ the backpropagation algorithm to calculate and propagate gradients across various layers. For visualizing the working principle of our proposed VPINN model, a detailed architecture of the model is presented in Figure 1.

## 4 Experimental Setup and Results

**Dataset.** To ensure a fair comparison and evaluation of the proposed VPINN framework, we employ a set of real-world extreme climatic event datasets with varying dynamics. We selected five time series datasets with diverse temporal units to demonstrate the scalability of our approach. These publicly available datasets include 1: (1) _Turkey Seismic Waves_, (2) _El Nino Sea Surface Temperature (SST)_, (3) _Philippines Temperature_, (4) _Madrid Humidity_, and (5) _Delhi Wind Speed_. Each of these datasets features varied climatic events and their anomalous behavior has the potential to significantly disrupt the global climatic patterns. A brief overview of these datasets along with their statistical properties and forecast horizons is presented in Table 1. All the experiments conducted in this study encompass both short-term and long-term time series forecasting settings for these datasets.

Footnote 1: www.vunderground.com/; www.kaggle.com/; www.ncei.noaa.gov/

**Performance Indicators.** In our study we assess the effectiveness of our proposed approach by employing two commonly used evaluation metrics: _Root Mean Square Error_ defined as \(\mathrm{RMSE}(y^{*},\hat{y})=\sqrt{\sum_{t=1}^{n}\left(y_{t}^{*}-\hat{y}_{t} \right)^{2}/n}\), and _Mean Absolute Error_ computed as \(\mathrm{MAE}(y^{*},\hat{y})=\sqrt{\sum_{t=1}^{n}\left|y_{t}^{*}-\hat{y}_{t} \right|/n}\), where \(y^{*}\) denote the ground truth observations and \(\hat{y}\) indicate the corresponding predicted value for \(n\) time-steps. These evaluation metrics are widely recognized and are commonly used in the context of extreme events forecasting problems [15].

**Experimental Results.** Table 2 presents the main experimental results of the VPINN framework for both short-term and long-term time series forecasting tasks. Since our proposal and the baseline

Figure 1: **Van der Pol-informed neural networks (VPINN).** We generate simulated data from the Van der Pol oscillator and calculate its time derivatives. We concatenate these derivatives with the real target series and model them using LSTM layers and a dense layer to generate the subsequent predictions. A modified loss function, combining the conventional loss and the physics-based loss, is used to train the network with a backpropagation approach.

[MISSING_PAGE_FAIL:6]

MAE scores, respectively, indicating its overall "best" performance compared to other models such as NBeats and Prophet. This enhanced forecasting capability of the VPINN model is attributed to its hybrid approach, which allows it to leverage both time series data and associated physics knowledge acquired through prior training via transfer learning.

## 5 Discussion and Conclusion

We proposed a physics-informed forecasting model, namely VPINN, by inducing the physical dynamics of the Van der Pol oscillator into the data-driven LSTM network. Compared to state-of-the-art deep learners, VPINN enhances generalization through a combination of transfer learning and a physics-informed loss function. The modeling capabilities of the VPINN framework show promise for developing and refining additional physics-guided forecasters capable of handling the complex geophysical turbulence of extreme climatic events. Furthermore, exploring the integration of other nonlinear dynamical systems into machine learning and deep learning frameworks for tackling more complex geophysical challenges requires further investigation. This work will act as the middle ground between domain-specific knowledge and pure data-driven methods. However, the choice of physical laws for real-world applied problems plays a critical role in the proposal and may degrade the performance of our architecture. We plan to take these issues into consideration in our future research

Figure 3: Visualization of the MCB analysis w.r.t. RMSE (left) and MAE (right) metric. The Y-axis of the plot shows the average rank and the X-axis represents the corresponding model.

Figure 2: Forecast accuracy of the proposed model and the state-of-the-art for selected datasets. The images provide a comparison of the RMSE metric computed at each forecast step.

## References

* [1] Tom Beucler, Stephan Rasp, Michael Pritchard, and Pierre Gentine. Achieving conservation of energy in neural network emulators for climate modeling. _arXiv preprint arXiv:1906.06622_, 2019.
* [2] Arka Daw, R Quinn Thomas, Cayelan C Carey, Jordan S Read, Alison P Appling, and Anuj Karpatne. Physics-guided architecture (pga) of neural networks for quantifying uncertainty in lake temperature modeling. In _Proceedings of the 2020 siam international conference on data mining_, pages 532-540. SIAM, 2020.
* [3] Donald G Edwards and Jason C Hsu. Multiple comparisons with the best treatment. _Journal of the American Statistical Association_, 78(384):965-971, 1983.
* [4] Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, and Yuyang Wang. Prediff: Precipitation nowcasting with latent diffusion models. _arXiv preprint arXiv:2307.10422_, 2023.
* [5] Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Yuyang Bernie Wang, Mu Li, and Dit-Yan Yeung. Earthformer: Exploring space-time transformers for earth system forecasting. _Advances in Neural Information Processing Systems_, 35:25390-25403, 2022.
* [6] Everette S Gardner Jr. Exponential smoothing: The state of the art. _Journal of forecasting_, 4(1):1-28, 1985.
* [7] Sepp Hochreiter and Jurgen Schmidhuber. Lstm can solve hard long time lag problems. _Advances in neural information processing systems_, 9, 1996.
* [8] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* [9] Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting with neural networks at uber. In _International conference on machine learning_, volume 34, pages 1-5. sn, 2017.
* [10] Valerio Lucatini, Davide Faranda, Jorge Miguel Milhazes de Freitas, Mark Holland, Tobias Kuna, Matthew Nicol, Mike Todd, Sandro Vaienti, et al. _Extremes and recurrence in dynamical systems_. John Wiley & Sons, 2016.
* [11] Gamal M Mahmoud and Ahmed AM Farghaly. Chaos control of chaotic limit cycles of real and complex van der pol oscillators. _Chaos, Solitons & Fractals_, 21(4):915-924, 2004.
* [12] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. _arXiv preprint arXiv:1905.10437_, 2019.
* [13] Elise Ozalp, Georgios Margazoglou, and Luca Magri. Physics-informed long short-term memory for forecasting and reconstruction of chaos. _arXiv preprint arXiv:2302.10779_, 2023.
* [14] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* [15] Arnob Ray, Tanujit Chakraborty, and Dibakar Ghosh. Optimized ensemble deep learning framework for scalable forecasting of dynamics containing extreme events. _Chaos: An Interdisciplinary Journal of Nonlinear Science_, 31(11):111105, 2021.
* [16] Benjamin Schrauwen, David Verstraeten, and Jan Van Campenhout. An overview of reservoir computing: theory, applications and implementations. In _Proceedings of the 15th european symposium on artificial neural networks. p. 471-482 2007_, pages 471-482, 2007.
* [17] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. _IEEE transactions on Signal Processing_, 45(11):2673-2681, 1997.
* [18] Michael Tabor. _Chaos and integrability in nonlinear dynamics: an introduction_. Wiley-Interscience, 1989.
* [19] Sean J Taylor and Benjamin Letham. Forecasting at scale. _The American Statistician_, 72(1):37-45, 2018.
* [20] Wilfried Thuiller. Climate change and the ecologist. _Nature_, 448(7153):550-552, 2007.
* [21] Balthasar Van der Pol. theory of the amplitude of tfree. forced triode vibrations. _Radio review_, 1:701-710, 1920.

* [22] Pantelis R Vlachas, Wonmin Byeon, Zhong Y Wan, Themistoklis P Sapsis, and Petros Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 474(2213):20170844, 2018.
* [23] Alan Wolf, Jack B Swift, Harry L Swinney, and John A Vastano. Determining lyapunov exponents from a time series. _Physica D: nonlinear phenomena_, 16(3):285-317, 1985.
* [24] Jibing Xie, Ze Chai, Luming Xu, Xukai Ren, Sheng Liu, and Xiaoqi Chen. 3d temperature field prediction in direct energy deposition of metals using physics informed neural network. _The International Journal of Advanced Manufacturing Technology_, 119(5-6):3449-3468, 2022.