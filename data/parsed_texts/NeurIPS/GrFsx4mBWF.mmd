# Demographic Parity Constrained Minimax Optimal Regression under Linear Model

 Kazuto Fukuchi\({}^{1,3}\)1

Jun Sakuma\({}^{2,3}\)

\({}^{1}\) University of Tsukuba \({}^{2}\) Tokyo Institute of Technology \({}^{3}\) RIKEN

fukuchi@cs.tsukuba.ac.jp

sakuma@c.titech.ac.jp

Footnote 1: Corresponding author.

###### Abstract

We explore the minimax optimal error associated with a demographic parity-constrained regression problem within the context of a linear model. Our proposed model encompasses a broader range of discriminatory bias sources compared to the model presented by Chzhen and Schreuder [6]. Our analysis reveals that the minimax optimal error for the demographic parity-constrained regression problem under our model is characterized by \(\Theta(^{dM}/n)\), where \(n\) denotes the sample size, \(d\) represents the dimensionality, and \(M\) signifies the number of demographic groups arising from sensitive attributes. Moreover, we demonstrate that the minimax error increases in conjunction with a larger bias present in the model.

## 1 Introduction

Machine learning techniques have been incorporated into numerous automated decision-making systems, spanning critical domains such as employment, credit assessment, insurance, and security. Nevertheless, these systems can exhibit discriminatory behavior towards specific demographic groups, including gender, race, and ethnicity, potentially causing significant societal ramifications. This issue, known as the fairness problem, has attracted substantial attention within the machine learning research community. The growing focus on the fairness problem primarily arises from reported instances of unfair behavior in real-world systems, encompassing recidivism risk prediction [3], hiring practices [10], facial recognition [9, 17], and credit scoring [24].

Motivated by these concerns, a considerable body of research has explored regression problems subject to fairness constraints [13, 25, 16, 2, 7, 8, 14, 18]. Numerous regression algorithms incorporating various fairness constraints have been developed to accommodate diverse contexts, with demographic parity [21] and equalized odds [11] being the predominant fairness constraints adopted by these methods.

In this study, we focus on the regression problem under the fairness constraint of demographic parity [21]. Existing literature primarily concentrates on the development of fair regression algorithms, and their performance evaluation predominantly relies on empirical analyses. Such evaluations, however, only offer performance guarantees for specific scenarios explored in the experiments, which may result in poor performance in unexamined situations. To ensure the algorithm's robust performance across a wider range of contexts and obtain a comprehensive understanding of the fair regression problem, a theoretical analysis of statistical efficiency is indispensable.

Several studies have introduced fair regression algorithms accompanied by theoretical analyses of their statistical efficiency in terms of accuracy and fairness. Agarwal et al. [2] designed a demographic parity-based fair regression algorithm using reduction methods [1] and established upper bounds on its empirical approximation errors for accuracy and fairness using Rademacher complexity. Chzhenet al. [8] proposed a discretization-based fair regression algorithm, deriving upper bounds on the mean squared excess risk for accuracy and a Kolmogorov distance-based score for demographic parity as fairness guarantees. Chzhen et al. [7] derived the Bayes optimal regressor under a demographic parity constraint, providing upper bounds on the mean absolute deviation from the Bayes optimal regressor for accuracy and a Kolmogorov distance-based score for demographic parity as fairness guarantees. Despite ensuring low error and fair treatment even in non-linear models, it remains unclear if these guarantees represent optimal performance among possible algorithms.

**Minimax optimal fair regression.** Numerous researchers have investigated minimax optimal regression algorithms, the best possible algorithm, without addressing fairness considerations [22, 23, 19, 15]. In contrast to standard regression problems, minimax optimality in fair regression problems remains relatively unexplored, with a notable exception being the recent work by Chzhen and Schreuder [6]. They examine minimax optimality in fair regression problems, incorporating demographic parity constraints within the following linear model:

\[Y=\langle\beta^{*},X\rangle+b_{S}+\xi\text{ where }X\sim N(0,\Sigma). \tag{1}\]

In this model, \(Y\), \(X\), and \(S\) represent the outcome, non-sensitive features, and a sensitive attribute, respectively. \(\langle\cdot,\cdot\rangle\) denotes the inner product, \(\xi\) represents zero-mean noise, and \(\Sigma\) is an arbitrary covariance matrix. For example, in salary calculations, \(X\) and \(S\) correspond to working hours and gender, respectively, with \(b_{S}\) and \(\beta^{*}\) signifying the base salary and hourly wage. In Eq (1), the salary \(Y\) is determined by the base salary \(b_{S}\) and the product of working hours \(X\) and an hourly wage \(\beta^{*}\).

The model in Eq (1) exhibits limitations pertaining to its applicability across various scenarios. We elucidate these limitations by discussing the notion of _direct discrimination_ and _indirect discrimination_, summarized succinctly in the second row of Table 1. Direct discrimination occurs when the sensitive attribute influences the outcome, regardless of non-sensitive features. The model in Eq (1) can treat direct discrimination resulting from the dependency of the intercept \(b_{S}\) on \(S\); for example, it can capture discrimination due to basing base salary on gender (third column in Table 1). However, it is imperative to underscore that the model in Eq (1) fails to handle direct discrimination arising from the partial (regression) coefficients \(\beta^{*}\), as these are independent of \(S\); for instance, it cannot accommodate discrimination due to gender-dependent hourly wages (second column in Table 1).

Indirect discrimination (or redlining effect [4]) constitutes another source of unfair bias, arising when the sensitive attribute influences the outcome through its correlation with non-sensitive features. The presence of the dependency between non-sensitive features and the sensitive attribute signifies indirect discrimination. In the model in Eq (1), non-sensitive features \(X\) is independet from the sensitive attribute \(S\), thereby implying an absence of indirect discrimination (forth column in Table 1).

Chzhen and Schreuder [6] effectively revealed the minimax optimal error for fair regression problems involving direct discrimination due to varying intercepts associated with sensitive attributes. However, their research does not address direct discrimination from partial coefficients and indirect discrimination through non-sensitive features.

**Our model and contributions.** In this study, we investigate the minimax optimality of the fair regression problem in the context of the following model:

\[Y=\langle\beta_{S}^{*},X\rangle+\xi\text{ where }X\sim N(\mu_{S},\sigma_{X}I), \tag{2}\]

where \(\sigma_{X}>0\), and \(I\) denotes the identity matrix. The subscript in \(\beta_{S}^{*}\) and \(\mu_{S}\) signifies that our model varies regression coefficients and the mean of non-sensitive features based on the sensitive attribute.

Compared to the model proposed by Chzhen and Schreuder [6], our model accommodates a broader range of direct and indirect discrimination. These discriminations can be characterized as follows:

\begin{table}
\begin{tabular}{l|c c c} \hline \hline  & partial & intercept & non-sensitive features \\ \hline Chzhen and Schreuder [6] & & ✓ & \\ ours & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between Chzhen and Schreuder [6]’s and our models. Each checkmark signifies the presence of an influence exerted by the sensitive attribute on the respective variable.

* (Direct discrimination) Our model accommodates direct discrimination through discrepancies in \(\beta_{S}^{s}\) concerning \(S\), as the regression coefficients \(\beta_{S}^{s}\) hinge on the sensitive attribute \(S\) (second and third columns on the third row in Table 1). This includes, for instance, discrimination arising from varying base salaries and hourly wages. Divergent partial coefficients yield varied outcome variance amongst \(S\), while disparate intercepts relative to \(S\) merely alter the outcome's mean. Hence, our model introduces an additional challenge of attenuating direct discrimination through disparate variance, alongside mitigating direct discrimination through disparate mean. This presents a stark contrast to Chzhen and Schreuder [6]'s model, which solely focuses on mitigating discrimination via the mean without considering the variance.
* (Indirect discrimination) The sensitive attribute \(S\) affects the mean of non-sensitive features \(X\), as denoted by the subscript of \(\mu_{S}\). Our model thereby introduces indirect discrimination through variations in \(\mu_{S}\) with respect to \(S\) (e.g., disparate working hours by gender). To alleviate this form of indirect discrimination, \(\mu_{S}\) needs to be estimated to adjust the learned regressor, thereby ensuring its output remains invariant to differing \(\mu_{S}\). Therefore, our model presents an additional complexity in estimating \(\mu_{S}\) for mitigating indirect discrimination.

Overall, our model demonstrates an expanded dependency of partial coefficients (direct discrimination) and non-sensitive features (indirect discrimination) on the sensitive attribute (second and fourth columns of Table 1).

The principal contribution of this paper lies in the establishment of matching upper and lower bounds on the minimax optimal error (i.e., the error corresponding to the minimax optimal regression algorithm) and the proposition of a regression algorithm that achieves this optimal error under Eq (2). The optimal error elucidates several insights:

* (Direct discrimination) The optimal error comprises a term reflecting the outcome's variance heterogeneity but excludes that of the outcome's mean. This insight implies that mitigating direct discrimination due to the outcome's variance sacrifices statistical efficiency, whereas addressing direct discrimination due to the outcome's mean does not entail this cost. This term effectively quantifies the cost of mitigating direct discrimination in variance and is absent from the optimal error of the Chzhen and Schreuder [6]'s model. Its identification, thus, signifies a crucial contribution of our research.
* (Indirect discrimination) Our lower bound is independent of the term associated with indirect discrimination. Although this evidence is not definitive, it hints at the potential for mitigating indirect discrimination without additional costs under certain conditions. This observation sets the stage for future research focused on developing cost-effective strategies to tackle indirect discrimination.

Our technical contributions to establish these bounds are detailed in Section 4.

**Notations.** Given a positive integer \(m\), define \([m]=\{1,...,m\}\). For a finite set \(A\), denote its cardinality by \(|A|\). Given an event \(\mathcal{E}\), its complement is represented as \(\mathcal{E}^{c}\), and its probability is denoted by \(\mathbb{P}\{\mathcal{E}\}\). For a random variable \(X\), its expectation is \(\mathbf{E}[X]\), and its associated sigma-algebra is \(\sigma(X)\). For two real values \(a\) and \(b\), the notations \(a\lor b=\max\{a,b\}\) and \(a\wedge b=\min\{a,b\}\) are used. For a square matrix \(A\in\mathbb{R}^{d\times d}\), its maximum and minimum eigenvalues are denoted by \(\lambda_{\max}(A)\) and \(\lambda_{\min}(A)\), respectively, and its transpose is represented by \(A^{\top}\). The set of unit vectors is given by \(\mathbb{S}_{d-1}\). For a sequence \(a_{t}\) indexed by \(t\in\mathcal{T}\), the notation \(a\). denotes the sequence \((a_{t})_{t\in\mathcal{T}}\).

## 2 Problem Setup

### Model and Learning Algorithm

**Model.** The proposed model, described in the introduction, is formulated according to Eq (2). We consider \(X\in\mathbb{R}^{2}\) and \(S\in[M]\) where \(M\geq 2\). The noise variable, \(\xi\), is assumed to follow a Gaussian distribution with zero mean and variance \(\sigma_{\xi}^{2}>0\). We define \(p_{s}=\mathbb{P}\{S=s\}\) for all \(s\in[M]\), and the optimal regression function is denoted as \(f^{*}(x,s)=\langle\beta_{s}^{*},x\rangle\).

**Learning algorithm.** Given \(n\) i.i.d. copies of the tuple \((X,S,Y)\), denoted as \(D_{n}=\{(X_{1},S_{1},Y_{1}),...,(X_{n},S_{n},Y_{n})\}\), the goal is to construct a regression function \(f\) that maps \((X,S)\) to \(Y\), represented as \(\hat{f}_{n}\). The learner seeks to optimize the accuracy of \(\hat{f}_{n}\) while satisfying a fairness constraint. The definitions of fairness and accuracy are provided in subsequent subsections.

### Fairness

**Demographic parity.** We utilize demographic parity [21] as our fairness criterion. A regressor \(f\) adheres to demographic parity if its output distribution is invariant when conditioned on \(S=s\).

**Definition 1**.: _A regressor \(f\) satisfies (strong) demographic parity if, for all \(s,s^{\prime}\in[M]\), and for all \(E\in\sigma(f(X,S))\), \(\mathbb{P}\{f(X,S)\in E|S=s\}=\mathbb{P}\{f(X,S)\in E|S=s^{\prime}\}\)._

Denote the set of all regressors fulfilling demographic parity for a given distribution of \(X\), parameterized by \(\mu_{\cdot}\), as \(\mathcal{F}_{\mathrm{DP}}(\mu_{\cdot})\).

**Fairness consistency.** Instead of enforcing strict demographic parity (Definition 1), which results in the regressor to be a constant function due to the unknown \((X,S)\) distribution, we introduce _fairness consistency_ (Definition 2). This concept demands the learned regressor to converge to a fair regressor as the sample size \(n\) approaches infinity.

To define "convergence", we introduce the _unfairness score_\(U(f)\geq 0\), where a lower \(U(f)\) indicates a higher fairness level. \(U(f)=0\) if and only if \(f\) achieves demographic parity (Definition 1). We claim the learned regressor \(\hat{f}_{n}\) converges to an exactly fair regressor when \(U(\hat{f}_{n})\to 0\) as \(n\rightarrow\infty\).

**Definition 2**.: _A learning algorithm is \((\alpha,\delta)\)-consistently fair for an unfairness score \(U\) if there exist constants \(n_{0}\geq 0\) and \(C>0\), independent of \(n\), such that \(\mathbb{P}\{U(\hat{f}_{n})>Cn^{-\alpha}\}\leq\delta\) for all \(n\geq n_{0}\), with randomness arising from the training sample via \(\hat{f}_{n}\)._

Note that an \((\alpha,\delta)\)-consistently fair regressor \(\hat{f}_{n}\) exhibits \((\alpha^{\prime},\delta)\)-consistent fairness for any \(\alpha^{\prime}\in(0,\alpha]\).

We adopt a specific unfairness score using the Wasserstein distance. Given two probability measures \(\nu\) and \(\nu^{\prime}\) over \(\mathbb{R}\), \(\Pi(\nu,\nu^{\prime})\) denotes the set of all coupling measures \(\pi\) satisfying \(\pi(A\times\mathbb{R})=\nu(A)\) and \(\pi(\mathbb{R}\times A^{\prime})=\nu^{\prime}(A^{\prime})\) for every measurable sets \(A,A^{\prime}\subset\mathbb{R}\). The 2-Wasserstein distance \(W_{2}\) between \(\nu\) and \(\nu^{\prime}\) is expressed as \(W_{2}^{2}(\nu,\nu^{\prime})=\inf_{\pi\in\Pi(\nu,\nu^{\prime})}\int(z-z^{\prime })^{2}\pi(dz,dz^{\prime})\). Our unfairness score is then formulated as:

\[U(f)=\max_{s,s^{\prime}\in[M]}W_{2}(\nu_{f|s},\nu_{f|s^{\prime}}),\]

where \(\nu_{f|s}\) represents the distribution of \(f(X,S)\) conditioned on \(S=s\). Prior works, including [2; 7; 8; 6], have adopted different unfairness scores (see the appendix for details2).

Footnote 2: A version of this paper including appendices is available in the supplementary material.

### Accuracy

Under the fairness consistency constraint, the learner's objective is to obtain a fair approximation of \(f^{*}\), denoted as \(f^{*}_{\mathrm{DP}}\), which is the closest regressor to \(f^{*}\) within \(\mathcal{F}_{\mathrm{DP}}(\mu_{\cdot})\) using the \(L^{2}\) distance:

\[f^{*}_{\mathrm{DP}}=\operatorname*{arg\,min}_{f\in\mathcal{F}_{\mathrm{DP}}( \mu_{\cdot})}\mathbf{E}\Big{[}\big{(}f(X,S)-f^{*}(X,S)\big{)}^{2}\Big{]}.\]

To evaluate the inaccuracy of a regressor \(f\), we compute the mean squared deviation from \(f^{*}_{\mathrm{DP}}\):

\[\mathcal{E}(f;\beta^{*}_{\cdot},\mu_{\cdot})=\mathbf{E}\Big{[}\big{(}f(X,S)-f ^{*}_{\mathrm{DP}}(X,S)\big{)}^{2}\Big{]}. \tag{3}\]

Chzhen et al. [7; 8] employ similar definitions, differing only in the choice of deviation metric.

This paper aims to identify the minimax optimal regression algorithm, which minimizes Eq (3) while maintaining fairness consistency. Given parameters \(\alpha>0\) and \(\delta\in(0,1)\), the optimal error is formulated as:

\[\mathcal{E}_{n}(\alpha,\delta)=\inf_{f_{n}:(\alpha,\delta)\text{- consistently fair }\beta^{*}}\sup_{\beta^{*}\in\mathcal{B},\mu_{\cdot}\in\mathcal{M}}\mathbf{E}[ \mathcal{E}(\hat{f}_{n};\beta^{*}_{\cdot},\mu_{\cdot})],\]

where the infimum is taken over all \((\alpha,\delta)\)-consistently fair algorithms, and \(\mathcal{B}\) and \(\mathcal{M}\) represent the sets of possible \(\beta^{*}_{\cdot}\) and \(\mu_{\cdot}\), respectively.

Main Results

Our main result is to establish the minimax optimal error bound, delineating the dependency on the diversity of conditional outcome variances concerning the sensitive attribute. This diversity of the variances is quantified via a parameter \(B>0\), which is defined such that it satisfies:

\[\max_{s}\lVert\beta_{s}^{*}\rVert\leq B\text{ and }\frac{(\sum_{s}p_{s}\lVert \beta_{s}^{*}\rVert)^{2}}{M}\sum_{s}\frac{1}{\lVert\beta_{s}^{*}\rVert^{2}} \leq B^{2}. \tag{4}\]

The left-hand side of the second inequality in Eq (4) forms as a product of two factors: the weighted average norms, \((\sum_{s}p_{s}\lVert\beta_{s}^{*}\rVert)^{2}\), and the averaged inverse norms, \(\frac{1}{M}\sum_{s}\frac{1}{\lVert\beta_{s}^{*}\rVert^{2}}\). As the norms increase, the first factor (weighted average norms) has the propensity to grow, while the second factor (averaged inverse norms) tends to rise when the norms decrease. Maximizing the product of these two elements involves a delicate balancing act: the norms of some groups need to be large, while the norms of other groups need to be smaller. As such, the left-hand side of the second inequality in Eq (4) can increase when the norms \(\lVert\beta_{s}^{*}\rVert\) display diversity.

We adopt mild assumptions on \(\beta^{*}\) and \(\mu\). Let \(\mathcal{B}\) denote the set of \(\beta^{*}\) satisfying Eq (4). Assume there exists a finite universal constant \(U>0\) such that \(\lVert\mu_{s}\rVert\leq U\) for all \(s\in[M]\), leading to \(\mathcal{M}=\{\mu\in\mathbb{R}^{d\times M}:\forall s\in[M],\lVert\mu_{s}\rVert \leq U\}\). Our analysis relies on these assumptions.

Our main results are as follows:

**Theorem 1**.: _Given \(\alpha\in(0,\nicefrac{{1}}{{2}}]\) and \(\delta\in(0,1)\), suppose \(M(d-1)>16\) and \(n\geq 12(3d\vee 4\ln(M/\delta))/\min_{s\in[M]}p_{s}\). Then, there exist universal constants \(C>0\) and \(c>0\) such that_

\[c\frac{\sigma_{\xi}^{2}B^{2}dM}{n}-o\bigg{(}\frac{1}{n}\bigg{)}\leq\mathcal{E} _{n}(\alpha,\delta)\leq C\frac{\sigma_{\xi}^{2}B^{2}dM\vee\sigma_{X}^{2}B^{2}M \lor B^{2}U^{2}}{n}+o\bigg{(}\frac{1}{n}\bigg{)}.\]

Theorem 1 illustrates that the optimal error is \(\nicefrac{{\sigma^{2}\xi B^{2}dM}}{{n}}\) up to a constant factor which may potentially depend on \(\sigma_{X}\) and \(U\). The implications of Theorem 1 can be summarized as follows:

1. The optimal error for the standard linear regression problem can be denoted as \(\nicefrac{{d}}{{n}}\)[15]. The dependency on \(n\) and \(d\) is consistent with the standard case, provided \(\alpha\in(0,\nicefrac{{1}}{{2}}]\).
2. The term \(dM\) denotes the number of unknown parameters in Eq (2), comprising \(\beta_{1}^{*},..,\beta_{M}^{*}\in\mathbb{R}^{d}\) and \(\mu_{1},...,\mu_{M}\in\mathbb{R}^{d}\). This dependency on the number of unknown parameters is a common characteristics observed in statistical estimation problems.
3. **(Direct discrimination)** The minimax error delineated in Theorem 1 demonstrates a dependency on parameter \(B\). As the variation of \(\lVert\beta_{s}^{*}\rVert\) with respect to \(s\) increases, so does the magnitude of \(B\). Hence, \(B\) serves as a measure of the difficulty in mitigating direct discrimination due to the outcome's variance. This unique quantification of difficulty is absent in standard regression problems and specific to fair regression problems.
4. **(Indirect discrimination)** The lower bound precludes parameters associated with indirect discrimination. It is conceivable that biases arising from indirect discrimination can be reduced without extra costs, provided the dependence of \(X\) on \(S\) exists only in its mean. Investigating and clarifying this aspect offers a promising direction for future research.
5. The minimax error is invariant to \(\alpha\) and \(\delta\), implying that the learning process does not introduce unfair bias for \(\alpha\in(0,\nicefrac{{1}}{{2}}]\). However, the case for \(\alpha\geq\nicefrac{{1}}{{2}}\) remains unexplored and poses a significant research challenge.
6. The gap between the upper and lower bounds regarding \(\sigma_{X}\) and \(U\) remains, making narrowing this gap an essential future research direction.

**Remark 1**.: _Direct comparison of the minimax error between our model and that of Eq (1) is not feasible due to the differing \(f_{\mathrm{DP}}^{*}\) across the models. However, the emergence of the fairness-specific term \(B\) can be unequivocally identified as a novel contribution in our study. Notably, the minimax error validated by Chzhen and Schreuder [6] is congruent with the minimax optimal error of standard linear regression within their model, a contrast to our findings._

To prove Theorem 1, we initiate by constructing the estimator detailed in Section 5. We then prove in Section 6 that the estimator satisfies 1) \((\alpha,\delta)\)-fairness consistency for \(\alpha\in(0,\nicefrac{{1}}{{2}}]\), and 2) the error aligns with the upper bound specified in Theorem 1. Subsequently, we present a sketch of the proof for the lower bound in Theorem 1 in Section 7. All omitted proofs can be found in the appendices.

Technical Difficulties in Minimax Optimality Analyses

In this section, we expound on the challenges arising from the analysis of minimax optimality for our problem. First, we introduce the closed-form expression for the Bayes optimal fair regressor \(f^{*}_{\mathrm{DP}}\). We then outline the technical difficulties encountered during the analysis.

**Bayes optimal fair regressor under Eq (2).** Chzhen et al. [7] present a characterization of regression error and the corresponding regressor minimizing the mean squared error under the demographic parity constraint. Building upon the results from Chzhen et al. [7], we derive the closed-form expression for \(f^{*}_{\mathrm{DP}}\) in the following lemma.

**Lemma 1**.: _Given the model in Eq (2), the Bayes optimal regressor adhering to the demographic parity constraint can be formulated as_

\[f^{*}_{\mathrm{DP}}(x,s)=\overline{\|\beta^{*}\|}\bigg{\langle} \frac{\beta^{*}_{s}}{\|\beta^{*}_{s}\|},x-\mu_{s}\bigg{\rangle}+\sum p_{s^{ \prime}}\langle\beta^{*}_{s^{\prime}},\mu_{s^{\prime}}\rangle, \tag{5}\]

_where \(\overline{\|\beta^{*}_{s}\|}=\sum_{s\in[M]}p_{s}\|\beta^{*}_{s}\|\)._

**Technical difficulty in deriving the upper bound in Theorem 1.** To obtain the upper bound in Theorem 1, we first construct an estimator for the regression function in Eq (5) and analyze its regression error. This entails developing estimators for individual components in Eq (5) (e.g., \(\overline{\|\beta^{*}\|}\), \(\beta^{*}_{s}/\|\beta^{*}_{s}\|\), \(\mu_{s}\), etc.) and substituting them into Eq (5). The upper bound on \(\mathcal{E}_{n}(\alpha,\delta)\) is derived by combining estimation error bounds for each component's estimator. However, to our best knowledge, no existing estimators provide bounds for the norm (\(\|\beta^{*}_{s}\|\)) and direction (\(\beta^{*}_{s}/\|\beta^{*}_{s}\|\)) of regression coefficients. A direct approach involves computing the norm and direction of the OLS estimator, but standard analyses for OLS do not yield bounds on the estimation errors.

The main challenge in deriving the upper bound of Theorem 1 lies in analyzing the following problem: given \(X\) following a non-isotropic Gaussian distribution with mean \(\mu\), find upper bounds on \(\mathbf{E}[(X/\|X\|-\mu/\|\mu\|)^{2}]\) and \(\mathbf{E}[(\|X\|-\|\mu\|)^{2}]\). Solving this problem provides estimation errors for the norm and direction estimators, as the OLS estimator is an unbiased estimator with noise following the non-isotropic Gaussian distribution. Our key technical contribution is the derivation of these bounds (Theorems 4 and 5).

**Technical difficulty in deriving the lower bound in Theorem 1.** The minimax optimal error characterizes the intrinsic complexity of the regression problem, as no algorithm can surpass this error. In our analysis of the lower bound presented in Theorem 1, we demonstrate that the fair regression problem's complexity, under the model Eq (2), is characterized by the complexity in estimating the direction \(\beta^{*}_{s}/\|\beta^{*}_{s}\|\). The primary challenge lies in establishing this characterization.

To overcome this challenge, we investigate the geometric structure of the error term \(\mathcal{E}_{n}(f;\beta^{*},\mu_{.})\) concerning the parameters \(\beta^{*}\) and \(\mu_{.}\). We then reveal that the geometric structure of \(\mathcal{E}_{n}(f;\beta^{*}_{.},\mu_{.})\) is characterized by the geometric structure of the direction \(\beta^{*}_{s^{\prime}}/\|\beta^{*}_{s}\|\) (Theorem 7).

## 5 Estimator

In this section, we present a detailed construction of the estimators that attain the minimax error as delineated in Theorem 1. Existing theoretical results, such as those found in Agarwal et al. [2], Chzhen et al. [7; 8], are incapable of addressing unbounded non-sensitive features \(X\) or unbounded outcomes \(Y\), rendering them inapplicable to our problem. Consequently, we have developed a novel estimator accompanied by rigorous analytical techniques.

**Estimator construction.** In constructing the optimal regressor for model Eq (2), we leverage the results from Lemma 1 and employ a plugin estimator. The method involves estimating the components of terms in Eq (5) and substituting the obtained estimates into the same equation. Concretely, we derive estimators \(\overline{\|\beta\|}\), \(\tilde{\beta}_{s}\), \(\hat{\mu}_{s}\), \(\hat{p}_{s}\), \(\hat{\beta}^{\prime}_{s}\), and \(\hat{\mu}^{\prime}_{s}\), with the following correspondence:

\[\frac{\overline{\|\beta^{*}\|}\bigg{\langle}\frac{\beta^{*}_{s}}{\|\beta\|}}{ \overline{\|\beta\|}}\bigg{\langle}\frac{\beta^{*}_{s}}{\overline{\|\beta^{*}_ {s}\|}},x-\frac{\mu_{s}}{\hat{\mu}_{s}}\bigg{\rangle}+\sum_{s^{\prime}\in[M]} \underbrace{\big{[}\mathrm{p}^{\prime}_{s^{\prime}}\big{\langle}\frac{\beta ^{*}_{s^{\prime}}}{\hat{\beta}^{\prime}_{s^{\prime}}},\frac{\mu_{s^{\prime}}}{ \hat{\mu}^{\prime}_{s^{\prime}}}\big{\rangle}}_{\hat{\beta}^{\prime}_{s^{ \prime}}}\bigg{\rangle}.\]For technical reasons, we partition the sample to calculate each estimand. Each estimator is assigned a corresponding subset, as shown in Table 2. Under specific conditions, \(n_{s}>18d\) or \(n_{s}>12d\), estimators may exhibit altered behavior, primarily as technical considerations for subsequent analyses. We detail the partitioning process as follows. First, we create a histogram of the sensitive attribute \(S_{i}\), denoted as \(n_{.}=(n_{1},..,n_{M})\), with \(n_{s}=|\{i\in[n]:S_{i}=s\}|\). Simultaneously, we form group-wise samples \(D_{s}=\{(X_{i},Y_{i}):i\in[n],S_{i}=s\}\). For each \(s\in[M]\), we partition \(D_{s}\) into \(D_{1,s}\), \(D_{2,s}\), and \(D_{3,s}\), ensuring \(|D_{b,s}|\coloneqq n_{b,s}\geq\lfloor n_{s}/3\rfloor\) for \(b\in[3]\). Using \(n_{.}\), \(D_{1,s}\), \(D_{2,s}\), and \(D_{3,s}\), we estimate \(\hat{p}_{s}\), \(\|\widehat{\beta_{s}}\|\), \(\hat{\beta}_{s}\), and \(\hat{\mu}_{s}\), respectively. The combination of \(\hat{p}_{s}\) and \(\|\widehat{\beta_{s}}\|\) yields \(\|\widehat{\beta_{s}}\|\). Furthermore, we generate a duplicate of \(D_{s}\), denoted as \(D^{\prime}_{s}\), and partition it into \(D^{\prime}_{1,s}\) and \(D^{\prime}_{2,s}\), satisfying \(|D^{\prime}_{b,s}|\coloneqq n^{\prime}_{b,s}\geq\lfloor n_{s}/2\rfloor\) for \(b\in[2]\). We then use \(D^{\prime}_{1,s}\) and \(D^{\prime}_{2,s}\) to estimate \(\hat{\beta}^{\prime}_{s}\) and \(\hat{\mu}^{\prime}_{s}\). Precise definitions of the estimator construction and subset partitioning can be found in the appendices.

Incorporating the derived estimators, we construct the final regressor as:

\[\hat{f}_{n}(x,s)=\widehat{\|\beta_{s}\|}\Big{\langle}\tilde{\beta _{s}},x-\hat{\mu}_{s}\Big{\rangle}+\sum_{s^{\prime}\in[M]}\hat{p}_{s^{\prime}} \Big{\langle}\hat{\beta}^{\prime}_{s},\hat{\mu}^{\prime}_{s}\Big{\rangle}. \tag{6}\]

## 6 Upper Bound Analyses

In this section, we demonstrate the achievability of the upper bound presented in Theorem 1 utilizing the estimator delineated in Section 5. Initially, we conduct an analysis of the estimator's fairness guarantee, subsequently progressing to an examination of the estimator's mean squared deviation.

### Analysis of Fairness

For our fairness guarantee on \(\hat{f}_{n}\), we demonstrate the following theorem.

**Theorem 2**.: _If \(n\geq 48\ln(M/\delta)/\min_{s}p_{s}\), we have for \(\delta\in(0,1)\),_

\[\mathbb{P}\bigg{\{}\max_{s,s^{\prime}\in[M]}W_{2}\Big{(}\nu_{\hat{f}_{n}|s^{ \prime}},\nu_{\hat{f}_{n}|s^{\prime}}\Big{)}>4B\sigma_{X}\sigma_{X}\sqrt{\frac{ 48\ln(M/\delta)}{\min_{s^{\prime\prime}\in[M]}np_{s^{\prime\prime}}}}\bigg{\}} \leq\delta.\]

By proving Theorem 2, we can immediately confirm that the estimator adheres to \((\alpha,\delta)\)-fairness consistency with \(\alpha\in(0,\sfrac{1}{2}]\).

### Analysis of Estimation Error

In this subsection, we derive an upper bound for the estimation error presented in Theorem 1, focusing on the estimator introduced in Section 5. To derive the upper bound in Theorem 1, we begin by decomposing the mean squared deviation of the estimator in Eq (6) as follows:

\begin{table}
\begin{tabular}{c|c|l} \hline \hline Estimator & Sample & Definition \\ \hline \(\hat{p}_{s}\) & \(n_{.}\) & \(\hat{p}_{s}=\nicefrac{{n_{s}}}{{n}}\) \\ \(\|\widehat{\beta_{s}}\|\) & \(D_{1,s}\) & \(\|\widehat{\beta_{s}}\|=\|\hat{\beta}_{1,s}\|\) if \(n_{s}>18d\), and \(\widehat{\|\beta_{s}\|}=0\) otherwise \\ \(\|\widehat{\beta_{s}}\|\) & - & \(\|\widehat{\beta_{s}}\|=\sum_{s\in[M]}\hat{p}_{s}\|\widehat{\beta_{s}}\|\) \\ \(\tilde{\beta}_{s}\) & \(D_{2,s}\) & \(\tilde{\beta}_{s}=\hat{\beta}_{2,s}/\|\widehat{\beta}_{2,s}\|\) if \(n_{s}>18d\), and \(\tilde{\beta}_{s}=0\) otherwise \\ \(\hat{\mu}_{s}\) & \(D_{3,s}\) & \(\hat{\mu}_{s}=\frac{1}{n_{s}}\sum_{i=1}^{n_{s,s}}X_{3,s,i}\) \\ \(\hat{\beta}^{\prime}_{s}\) & \(D^{\prime}_{1,s}\) & \(\hat{\beta}^{\prime}_{s}\not\equiv\hat{\beta}^{\prime}_{1,s}\) if \(n_{s}>12d\), and \(\hat{\beta}^{\prime}_{s}=0\) otherwise \\ \(\hat{\mu}^{\prime}_{s}\) & \(D^{\prime}_{2,s}\) & \(\hat{\mu}^{\prime}_{s}=\frac{1}{n_{2,s}^{\prime}}\sum_{i=1}^{n_{s,s}}X^{\prime}_ {2,s,i}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Estimator construction. In this table, \(\hat{\beta}_{b,s}\) and \(\hat{\beta}^{\prime}_{b,s}\) denote OLS estimands obtained from subsets \(D_{b,s}\) and \(D^{\prime}_{b,s}\), respectively. “Sample” refers to the subset utilized for estimand calculation, while “Definition” provides the corresponding estimator’s definition. “Sample” in \(\widehat{\|\beta_{s}\|}\) is left empty, as it is derived from \(\hat{p}_{s}\) and \(\widehat{\|\beta_{s}\|}\).

**Theorem 3**.: _For the estimator defined in Eq (6), the mean square deviation from \(f^{*}_{\mathrm{DP}}\) is bounded above by_

\[\sum_{s\in[M]}p_{s}\mathbf{E}\Bigg{[}\Bigg{(}\mathbf{E}\bigg{[} \widehat{\|\beta.\|}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}\mathbf{E} \bigg{[}\bigg{<}\tilde{\beta}_{s},\mu_{s}-\hat{\mu}_{s}\bigg{>}^{2}\bigg{|}n. \bigg{]}^{\nicefrac{{1}}{{2}}}+\sigma_{X}\mathbf{E}\bigg{[}\bigg{(}\widehat{\| \beta.\|}-\overline{\|\beta^{*}\|}\bigg{)}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1 }}{{2}}}+\\ \sigma_{X}\overline{\|\beta^{*}\|}\mathbf{E}\bigg{[}\bigg{\|} \tilde{\beta}_{s}-\beta^{*}_{s}/\|\beta^{*}_{s}\|\bigg{\|}^{2}\bigg{|}n.\bigg{]} ^{\nicefrac{{1}}{{2}}}+\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s^{\prime}\in[M]}\hat{p} _{s^{\prime}}\bigg{<}\hat{\beta}^{\prime}_{s^{\prime}}-\beta^{*}_{s^{\prime}},\hat{\mu}^{\prime}_{s^{\prime}}\bigg{>}\Bigg{)}^{2}\Bigg{|}n.\Bigg{]}^{ \nicefrac{{1}}{{2}}}+\\ \mathbf{E}\Bigg{[}\Bigg{(}\sum_{s^{\prime}\in[M]}\hat{p}_{s^{ \prime}}\langle\beta^{*}_{s^{\prime}},\hat{\mu}^{\prime}_{s^{\prime}}-\mu_{s^{ \prime}}\rangle\Bigg{)}^{2}\Bigg{|}n.\Bigg{]}^{\nicefrac{{1}}{{2}}}+\Bigg{|} \sum_{s^{\prime}\in[M]}(\hat{p}_{s^{\prime}}-p_{s^{\prime}})\langle\beta^{*}_{ s^{\prime}},\mu_{s^{\prime}}\rangle\Bigg{|}\Bigg{)}^{2}\Bigg{]}. \tag{7}\]

In Eq (7), the terms correspond to the estimation errors of \(\hat{\mu}_{s},\widehat{\|\beta.\|},\tilde{\beta}_{s},\hat{\beta}^{\prime}_{s},\hat{\mu}^{\prime}_{s},\) and \(\hat{p}_{s},\) respectively. Standard techniques for the OLS estimator and empirical average yield upper bounds for the first, fourth, fifth, and sixth terms. Nevertheless, the second and third terms in Eq (7) involve non-linear transformations of the OLS estimator (i.e., taking the norm or dividing by the norm), complicating their error analysis. This section's primary technical contributions involve establishing tight upper bounds for the second and third terms in Eq (7).

**Estimation error of norm and direction of \(\beta^{*}_{s}\).** Consider \(X_{1},...,X_{n}\overset{\mathrm{iid}}{\sim}N(\mu,\sigma^{2}_{X}I)\), \(\beta^{*}\in\mathbb{R}^{d}\) with \(\|\beta^{*}\|\leq B\) for some \(B>0\), and \(\xi_{1},...,\xi_{n}\overset{\mathrm{iid}}{\sim}N(0,\sigma^{2}_{\xi})\). Define \(Y_{i}=\langle\beta^{*},X_{i}\rangle+\xi_{i}\). The OLS estimator of \(\beta^{*}\) is given by \(\hat{\beta}=(\frac{1}{n}X^{\top}X)^{-1}(\frac{1}{n}X^{\top}Y)\), where \(X=(X_{1}\cdots X_{n})^{\top}\) and \(Y=(Y_{1}\cdots Y_{n})^{\top}\). The direction estimator is \(\hat{\beta}/\|\hat{\beta}\|\), while the norm estimator is \(\|\hat{\beta}\|\).

We present the estimation errors for direction and norm in Theorems 4 and 5:

**Theorem 4**.: _For \(n>6d\), we have_

**Theorem 5**.: _For \(n>6d\), we have_

\[\mathbf{E}\bigg{[}\bigg{(}\|\hat{\beta}\|-\|\beta^{*}\|\bigg{)}^{2}\bigg{]} \leq\frac{21e^{10}\sigma^{2}_{\xi}d}{\sigma^{2}_{X}n}\bigg{(}1+\frac{6}{n-6} \bigg{)}.\]

The direction's estimation error (Theorem 4) is \(O(\sigma^{2}_{\xi}d/\sigma^{2}_{X}\|\beta^{*}\|^{2}n)\), while the norm's estimation error (Theorem 5) is \(O(B^{2}\sigma^{2}_{\xi}d/\sigma^{2}_{\chi}n)\). Integrating Theorems 3 to 5 yields the \(\sigma^{2}_{\xi}B^{2}dM/_{n}\) term in the upper bound in Theorem 1. The remaining part, \(\upsilon^{\sigma^{2}_{\xi}}_{\xi}/n\), arises from the estimation error of \(\hat{\beta}^{\prime}_{s}\) (the third term in Eq (7)), dominating other terms in Eq (7).

## 7 Lower Bound Analyses

In this section, we provide a proof sketch for the lower bound, outlined in Theorem 1. To facilitate a clear and concise presentation of the proof sketch, we introduce several notations. Let \(\theta\) denote the tuple of distribution parameters \((\beta,\mu,)\), and let \(\Theta\) represent the set of all such parameters, defined as \(\Theta=\mathcal{B}\times\mathcal{M}\). We use \(\mathbb{P}_{\theta}\) and \(\mathbf{E}_{\theta}\) to denote the probability and expectation operators, respectively, given \(X\sim N(\mu_{S},\sigma^{2}_{X}I)\) and \(Y=(\beta_{S},X)+\xi\), where \(\xi\sim N(0,\sigma^{2}_{\xi})\). We adopt the shorthand \(\mathcal{E}(f;\theta)=\mathcal{E}(f;\beta,\mu,)\) for \(\theta=(\beta,\mu.)\). Moreover, we define \(f_{\theta}=\arg\min_{f\in\mathcal{F}_{\mathrm{DP}}}R(f;\beta,\mu.)\) for \(\theta=(\beta,\mu.)\). For two probability distributions \(\pi\) and \(\pi^{\prime}\), the Kullback-Leibler (KL) divergence is denoted as \(D_{\mathrm{KL}}(\pi,\pi^{\prime})=\int\ln(\frac{d\pi}{d\pi^{\prime}}(z))\pi(dz)\). Finally, we denote the set of all \(L^{2}\) integrable functions \(f:\mathbb{R}^{d}\times[M]\rightarrow\mathbb{R}\) as \(\mathcal{L}^{2}\).

By utilizing Fano's inequality, we establish a lower bound for the minimax error as presented in Theorem 1. Due to the invariance of the distribution of \(S_{1},...,S_{n}\) under parameter alterations \(\theta\)Fano's inequality can be applied after conditioning on \(S_{1},...,S_{n}\), or equivalently, \(n.\) Consequently, we derive the following theorem:

**Theorem 6**.: _Let \(\hat{\Theta}\subseteq\Theta\) be a finite set of the parameters such that there exists \(\epsilon>0\) such that for any \(\theta,\theta^{\prime}\in\hat{\Theta},\inf_{f}\mathcal{E}(f;\theta)\vee \mathcal{E}(f;\theta^{\prime})\geq\epsilon\), where \(\hat{\Theta}\) and \(\epsilon\) is possibly dependent on \(n.\). Let \(|\hat{\Theta}|=K\). Then, for arbitrary \(\alpha>0\) and \(\delta\in(0,1)\), we have_

\[\mathcal{E}_{n}(\alpha,\delta)\geq\mathbf{E}\Bigg{[}\epsilon\Bigg{(}1-\frac{ \inf_{\pi}\frac{1}{K}\sum_{\theta\in\hat{\Theta}}D_{\mathrm{KL}}\big{(}\pi_{ \theta|n.},\pi\big{)}+\ln(2)}{\ln(K)}\Bigg{)}\Bigg{]},\]

_where \(\pi_{\theta|n.}\) denotes the distribution of \(D_{n}\) conditioned on \(n.\) with parameter \(\theta\), and the expectation is taken over \(n.\)._

As demonstrated in Theorem 6, the lower bound for the minimax error can be obtained by constructing \(\hat{\Theta}\) such that: 1) \(\inf_{f}\mathcal{E}(f;\theta)\vee\mathcal{E}(f;\theta^{\prime})\geq\epsilon\) for any \(\theta,\theta^{\prime}\in\hat{\Theta}\), and 2) \(\inf_{\pi}\frac{1}{K}\sum_{\theta\in\hat{\Theta}}D_{\mathrm{KL}}\big{(}\pi_{ \theta|n.},\pi\big{)}\leq\ln(K/4)/2\). With the construction of such a \(\hat{\Theta}\), a lower bound of \(\mathbf{E}[\frac{\epsilon}{2}]\) is attained.

We present a theorem that establishes a tight lower bound on \(\inf_{f}\mathcal{E}(f;\theta)\vee\mathcal{E}(f;\theta^{\prime})\).

**Theorem 7**.: _Let \(\theta\) and \(\theta^{\prime}\) be the parameters of the distributions such that \(\frac{1}{2\sigma_{X}^{2}}\|\mu_{s}-\mu_{s}^{\prime}\|^{2}\coloneqq d_{s}<1\) for all \(s\in[M]\). Then, we have_

\[\inf_{f\in\mathcal{L}^{2}}\mathcal{E}(f;\theta)\vee\mathcal{E}(f;\theta^{ \prime})\geq\sum_{s\in[M]}p_{s}\frac{\sigma_{X}^{2}e^{-d_{s}}}{4}\bigg{\|} \frac{\overline{\|\beta.\|}\beta_{s}}{\|\beta_{s}\|}-\frac{\overline{\|\beta^ {\prime}\|}\beta_{s}^{\prime}}{\|\beta_{s}^{\prime}\|}\bigg{\|}^{2}\bigg{(}1+ \frac{d_{s}}{2}\bigg{)}^{1+\frac{d}{2}}.\]

The term \(\|\overline{\|\beta.\|}\beta_{s}/\|\beta_{s}\|-\overline{\|\beta^{\prime}.\|} \beta_{s}^{\prime}/\|\beta_{s}^{\prime}\|\|^{2}\) characterizes the lower bound, which is different from the characteristic term in standard linear regression, \(\|\beta_{s}-\beta_{s}^{\prime}\|^{2}\).

We next present the construction of \(\hat{\Theta}\). We construct \(\hat{\Theta}\) such that each of its elements corresponds to an index from the set \(\mathcal{V}=\{-1,1\}^{M\times(d-1)}\), denoted by \(\theta_{v}=\{\beta_{v.},\mu_{v.}\}\in\hat{\Theta}\), where \(\beta_{v,s}\) is controlled such that its norm is equivalent to a specified value \(B_{s}\), i.e., \(\|\beta_{v,s}\|=B_{s}\). This construction ensures that \(\hat{\Theta}\subset\mathcal{B}\times\mathcal{M}\). Given positive values \(\epsilon_{1},...,\epsilon_{M}\) and \(B_{1},...,B_{M}\), we construct \(\hat{\Theta}\) as follows:

\[\mu_{v,s}=0,\|\beta_{v,s}\|=B_{s},\frac{\beta_{v,s,1}}{\|\beta_{v,s}\|}=\sqrt{ 1-\epsilon_{s}^{2}},\text{ and }\frac{\beta_{v,s,i}}{\|\beta_{v,s}\|}=v_{s,i-1} \frac{\epsilon_{s}}{\sqrt{d-1}}\text{ for }i=2,...,d. \tag{8}\]

We demonstrate the following properties for \(\hat{\Theta}\) defined in Eq (8).

**Theorem 8**.: _Given \(\epsilon_{1},...,\epsilon_{M}>0\) and \(B_{1},...,B_{M}>0\), let \(\hat{\Theta}\subset\Theta\) represent the set of parameters defined in Eq (8). Let \(\pi_{\theta|n.}\) be the distribution of the sample \(D_{n}\) conditioned on \(n.\) with the distribution parameter \(\theta\). Then, we have 1) for any \(v,v^{\prime}\in\mathcal{V}\),_

\[\inf_{f\in\mathcal{L}^{2}}\mathcal{E}(f;\theta_{v})\vee\mathcal{E}(f;\theta_{v ^{\prime}})\geq\sum_{s\in[M]}p_{s}\Bigg{(}\sum_{s^{\prime}\in[M]}p_{s^{\prime }}B_{s^{\prime}}\Bigg{)}^{2}\frac{\sigma_{X}^{2}\epsilon_{s}^{2}}{d-1}d_{H}(v_ {s},v^{\prime}s),\]

_and 2) for \(v,v^{\prime}\in\mathcal{V}\),_

\[D_{\mathrm{KL}}\big{(}\pi_{\theta_{v}|n.},\pi_{\theta_{v^{\prime}}|n.}\big{)} =\sum_{s\in[M]}\frac{2\sigma_{X}^{2}B_{s}^{2}n_{s}\epsilon_{s}^{2}}{\sigma_{ \xi}^{2}(d-1)}d_{H}(v_{s},v^{\prime}_{s}).\]

By integrating Theorems 6 to 8 and employing the renowned Varshamov-Gilbert bound, we derive the lower bound in Theorem 1.

## 8 Conclusion

This paper investigates a regression problem with \((\alpha,\delta)\)-fairness consistency as a fairness constraint. Specifically, we demonstrate that, under the constraint of \((\alpha,\delta)\)-fairness, the minimax optimal error scales as \(\sigma_{\ell}^{2}B^{2}dM/n\) up to a constant factor, when \(\alpha\in(0,\nicefrac{{1}}{{2}}]\). Additionally, we provide the fair regressor that achieves this optimal error.

**Potential negative societal impacts.** Our study aims to mitigate the negative impact of regression models on social groups, rather than to cause harm. However, our results are only valid for linear models, as defined in Eq (2). Misapplication of our findings to other models may result in discriminatory treatment, which should be avoided.

## Acknowledgments and Disclosure of Funding

This work was partly supported by JSPS KAKENHI Grant Numbers JP23K13011 and JP23H00483.

## References

* Agarwal et al. [2018] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A Reductions Approach to Fair Classification. In _the 35th International Conference on Machine Learning_, pages 60-69, 2018.
* Agarwal et al. [2019] Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu. Fair Regression: Quantitative Definitions and Reduction-based Algorithms. In _36th International Conference on Machine Learning, ICML 2019_, volume 2019-June, pages 120-129. International Machine Learning Society (IMLS), 2019.
* Angwin et al. [2016] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine Bias, 2016. URL [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).
* Calders and Verwer [2010] Toon Calders and Sicco Verwer. Three naive Bayes approaches for discrimination-free classification. _Data Mining and Knowledge Discovery_, 21(2):277-292, 2010.
* Chao and Strawderman [1972] M. T. Chao and W. E. Strawderman. Negative Moments of Positive Random Variables. _Journal of the American Statistical Association_, 67(338):429, 6 1972. ISSN 01621459. doi: 10.2307/2284399.
* Chzhen and Schreuder [2022] Evgenii Chzhen and Nicolas Schreuder. A minimax framework for quantifying risk-fairness trade-off in regression. _The Annals of Statistics_, 50(4):2416-2442, 8 2022. ISSN 0090-5364. doi: 10.1214/22-AOS2198.
* Chzhen et al. [2020] Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, and Massimiliano Pontil. Fair Regression with Wasserstein Barycenters. In _Advances in Neural Information Processing Systems_, volume 33, pages 7321-7331. Curran Associates, Inc., 2020.
* Chzhen et al. [2020] Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, and Massimiliano Pontil. Fair Regression via Plug-In Estimator and Recalibration. In _Advances in Neural Information Processing Systems_, volume 33, pages 19137-19148. Curran Associates, Inc., 2020.
* Crockford [2020] Kade Crockford. How is Face Recognition Surveillance Technology Racist?, 2020. URL [https://www.aclu.org/news/privacy-technology/how-is-face-recognition-surveillance-technology-racist/](https://www.aclu.org/news/privacy-technology/how-is-face-recognition-surveillance-technology-racist/).
* Dastin [2018] Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women, 2018. URL [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G).
* Hardt et al. [2016] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of Opportunity in Supervised Learning. In D D Lee, M Sugiyama, U V Luxburg, I Guyon, and R Garnett, editors, _Advances in Neural Information Processing Systems 29_, pages 3315-3323, Barcelona, Spain, 12 2016. Curran Associates, Inc.

* Masirevic [2017] Dragana Jankov Masirevic. On New Formulas for the Cumulative Distribution Function of the Noncentral Chi-Square Distribution. _Mediterranean Journal of Mathematics_, 14(2):1-13, 4 2017. ISSN 16605454. doi: 10.1007/S00009-017-0874-1/METRICS. URL [https://link.springer.com/article/10.1007/s00009-017-0874-1](https://link.springer.com/article/10.1007/s00009-017-0874-1).
* Komiyama et al. [2018] Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao. Nonconvex Optimization for Regression with Fairness Constraints. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 2737-2746, Stockholmsmassan, Stockholm Sweden, 2018. PMLR.
* Mary et al. [2019] Jeremie Mary, Clement Calauzenes, and Noureddine El Karoui. Fairness-Aware Learning for Continuous Attributes and Treatments. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 4382-4391, Long Beach, California, USA, 2019. PMLR.
* Mourtada [2022] Jaouad Mourtada. Exact minimax risk for linear least squares, and the lower tail of sample covariance matrices. _The Annals of Statistics_, 50(4):2157-2178, 8 2022. ISSN 0090-5364. doi: 10.1214/22-AOS2181. URL [https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-4/Exact-minimax-risk-for-linear-least-squares-and-the-lower/10.1214/22-AOS2181.full](https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-4/Exact-minimax-risk-for-linear-least-squares-and-the-lower/10.1214/22-AOS2181.full).
* Moyer et al. [2018] Daniel Moyer, Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Invariant Representations without Adversarial Training. In _Advances in Neural Information Processing Systems_, volume 31, pages 9084-9093, 2018.
* Najibi [2020] Alex Najibi. Racial Discrimination in Face Recognition Technology, 2020. URL [https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/).
* Narasimhan et al. [2020] Harikrishna Narasimhan, Andrew Cotter, Maya R Gupta, and Serena Wang. Pairwise Fairness for Ranking and Regression. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence_, pages 5248-5255. AAAI Press, 2020.
* Imbuzeiro Oliveira [2016] Roberto Imbuzeiro Oliveira. The lower tail of random quadratic forms with applications to ordinary least squares. _Probability Theory and Related Fields_, 166(3-4):1175-1194, 12 2016. ISSN 01788051. doi: 10.1007/S00440-016-0738-9/METRICS.
* Olkin and Pukelsheim [1982] I. Olkin and F. Pukelsheim. The distance between two random vectors with given dispersion matrices. _Linear Algebra and its Applications_, 48(C):257-263, 12 1982. ISSN 0024-3795. doi: 10.1016/0024-3795(82)90112-4.
* KDD 08_, pages 560-568, 2008. ISBN 9781605581934. doi: 10.1145/1401890.1401959.
* Stone [1980] Charles J Stone. Optimal rates of convergence for nonparametric estimators. _The annals of Statistics_, pages 1348-1360, 1980.
* Tsybakov [2003] Alexandre B. Tsybakov. Optimal rates of aggregation. _Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)_, 2777:303-313, 2003. ISSN 03029743. doi: 10.1007/978-3-540-45167-9{_}23/COVER.
* Vigdor [2019] Neil Vigdor. Apple Card Investigated After Gender Discrimination Complaints, 2019. URL [https://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html](https://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html).
* Xie et al. [2017] Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable Invariance through Adversarial Feature Learning. In _Advances in Neural Information Processing Systems_, volume 30, pages 585-596, 2017.

## Appendix A Comparison of Existing and Our Unfairness Scores

This section compares our unfairness score with existing ones. Recall that our unfairness score is defined as the maximum Wasserstein distance between any two distributions \(\nu_{f|s}\) and \(\nu_{f|s^{\prime}}\) over all pairs of groups \(s\) and \(s^{\prime}\), as follows:

\[U(f)=\max_{s,s^{\prime}\in[M]}W_{2}(\nu_{f_{n}|s},\nu_{f|s^{\prime}}).\]

In contrast, Agarwal et al. [2], Chzhen et al. [7; 8] use the Kolmogorov distance \(D_{\mathrm{Kol}}\) to measure unfairness, which is defined as:

\[U_{\mathrm{Kol}}(f)=\max_{s,s^{\prime}\in[M]}D_{\mathrm{Kol}}(\nu_{f|s},\nu_{f |s^{\prime}}).\]

The difference between our score and \(U_{\mathrm{Kol}}(f)\) is solely the choice of distance metric. Our score utilizes the Wasserstein distance, while \(U_{\mathrm{Kol}}(f)\) uses the Kolmogorov distance. This difference arises mainly from technical reasons.

In addition, Chzhen and Schreuder [6] proposed another unfairness score, denoted by \(U_{\mathrm{Avg}W_{2}}(f)\), which is defined as the average of the Wasserstein distance, as follows:

\[U_{\mathrm{Avg}W_{2}}(f)=\inf_{\nu}\sum_{s\in[M]}p_{s}W_{2}\big{(}\nu_{f|s}, \nu\big{)}.\]

Here, the score places more emphasis on the major groups, as reflected by the weight of \(p_{s}\). This may not be desirable if the unfairness is more prevalent in the minority groups, which may be common in real-world scenarios.

## Appendix B Estimator Details

This section describes the construction of our optimal estimator in detail. Recall that our estimator is a plugin estimator in which we first estimate the parts of the terms in Eq (5) and then substitute them into Eq (5). Specifically, we construct estimators for\(\overline{\|\beta.\|}\), \(\tilde{\beta}_{s}\), \(\tilde{p}_{s^{\prime}}\), \(\hat{\beta}^{\prime}_{s^{\prime}}\), and \(\hat{\mu}^{\prime}_{s^{\prime}}\), where they correspond to the terms in Eq (5) as follows:

Figure 1: Sample splitting for constructing estimators.

[MISSING_PAGE_EMPTY:13]

Recall that the final regressor is constructed as follows:

\[\hat{f}_{n}(x,s)=\widehat{\|\beta\cdot\|}\Big{\langle}\tilde{\beta}_{s},x-\hat{\mu }_{s}\Big{\rangle}+\sum_{s^{\prime}\in[M]}\hat{p}_{s^{\prime}}\Big{\langle} \hat{\beta}^{\prime}_{s},\hat{\mu}^{\prime}_{s}\Big{\rangle}.\]

Algorithm 1 shows the algorithm for our estimator.

## Appendix C Bayes Optimal Regressor under Our Modell

This section presents the proof of Lemma 1, demonstrating the Bayes optimal regressor under the model Eq (2). To establish this, we make use of a key result from the work of Chzhen et al. [7]:

**Theorem 9** (Chzhen et al. [7]).: _Assume, for each \(s\in[M]\), \(\nu_{f^{*}|s}\) has a density. Then,_

\[\inf_{f\text{:DP}}\mathbf{E}\Big{[}\big{(}f(X,S)-f^{*}(X,S)\big{)}^{2}\Big{]}= \inf_{\nu}\sum_{s\in[M]}p_{s}W_{2}^{2}(\nu_{f^{*}|s},\nu).\]

_where the infimum is taken over all the regressors that satisfy the demographic parity. Moreover, letting \(f^{*}_{\text{DP}}\) and \(\nu^{*}\) be the minimizer of the lhs and rhs, respectively, we have \(\nu_{f_{\text{DP}}}=\nu^{*}\) and_

\[f^{*}_{\text{DP}}(x,s)=\left(\sum_{s^{\prime}\in[M]}p_{s^{\prime}}F^{-1}_{f^{* }|s^{\prime}}\right)\circ F_{f^{*}|s}(f^{*}(x,s)). \tag{9}\]

Here, we denote \(\nu_{f^{*}}\) as the distribution of \(f^{*}(X,S)\), \(F_{f|s}\) as the cumulative distribution function of \(f(X,S)\) conditioned on \(S=s\), and \(F^{-1}_{f|s}\) as the inverse cumulative distribution function, given by \(F^{-1}_{f|s}(t)=\inf\{y\in\mathbb{R}|F_{f|s}(y)\geq t\}\).

Building upon the results of Theorem 9, we establish the proof of Lemma 1.

Proof of Lemma 1.: Building upon Theorem 9, we can derive the Bayes optimal regressor under the model Eq (2) by obtaining closed expressions of the cumulative and inverse cumulative distribution functions \(F_{f^{*}|s}\) and \(F^{-1}_{f^{*}|s}\). To obtain these closed forms, we apply certain transformations to \(f^{*}(X,S)\) that render it a random variable following a standard normal distribution. Let \(\Phi\) and \(\Phi^{-1}\) be the CDF and inverse CDF of the standard normal distribution, respectively. Through elementary calculations, we have:

\[F_{f^{*}|s}(t)= \mathbb{P}\{f^{*}(X,S)\leq t|S=s\}\] \[= \mathbb{P}\{\langle\beta^{*}_{s},X\rangle\leq t|S=s\}\] \[= \mathbb{P}\bigg{\{}\frac{1}{\sigma_{X}\|\beta^{*}_{s}\|}\langle \beta^{*}_{s},X-\mu_{s}\rangle\leq\frac{1}{\sigma_{X}\|\beta^{*}_{s}\|}(t- \langle\beta^{*}_{s},\mu_{s}\rangle)|S=s\bigg{\}}.\]

Here, we can readily observe that \(\frac{1}{\sigma_{X}\|\beta^{*}_{s}\|}\langle\beta^{*}_{s},X-\mu_{s}\rangle\) follows the standard normal distribution under conditioned on \(S=s\), as \(X\sim N\big{(}\mu_{s},\sigma_{X}I\big{)}\) conditioned on \(S=s\). Consequently, we have

\[F_{f^{*}|s}(t)=\Phi\bigg{(}\frac{1}{\sigma_{X}\|\beta^{*}_{s}\|}(t-\langle \beta^{*}_{s},\mu_{s}\rangle)\bigg{)}. \tag{10}\]

The inverse function of \(F^{-1}_{f^{*}|s}(t)\) can be obtained by equating the right-hand side to \(p\) and solving the resulting equation for \(t\), which leads to

\[F^{-1}_{f^{*}|s}(p)=\sigma_{X}\|\beta^{*}_{s}\|\Phi^{-1}(p)+\langle\beta^{*}_{ s},\mu_{s}\rangle. \tag{11}\]

By substituting Eqs (10) and (11) into Eq (9) in Theorem 9, we obtain the desired claim. 

## Appendix D Details of Fairness Analysis

In this section, we provide evidence of the guarantee of our estimator's fairness consistency. Specifically, we present the following theorem.

**Theorem 10**.: _For any \(\delta\in(0,1]\), the regressor in Eq (6) is \((\nicefrac{{1}}{{2}},\delta)\)-consistently fair._

We prove the above claim by utilizing Theorem 2, which is shown in the main body, as follows:

Proof of Theorem 10.: We can confirm the claim by comparing the bound obtained in Theorem 2 with the definition of \((\frac{1}{2},\delta)\)-consistent fairness in Definition 2. In particular, we can set \(C=4B\sigma_{X}\sqrt{\frac{48\ln(M/\delta)}{\min_{s\in[M]}p_{s}}}\) and \(n_{0}=n\geq 48\ln(M/\delta)/\min_{s}p_{s}\) to satisfy the definition of \((\frac{1}{2},\delta)\)-consistent fairness. 

Next, we provide the proof of Theorem 2. To this end, we prove the following two theorems:

**Theorem 11**.: _Let \(\hat{f}_{n}\) be the estimator of \(f_{\mathrm{DP}}^{*}\) defined in Eq (6). Then, almost surely, we have_

\[W_{2}\Big{(}\nu_{\hat{f}_{n}|s},\nu_{\hat{f}_{fn}|^{s}}\Big{)} \leq 2B\Big{(}\Big{|}\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{\mu}_{s }\Big{\rangle}\Big{|}\vee\Big{|}\Big{\langle}\tilde{\beta}_{s^{\prime}},\mu_{s ^{\prime}}-\hat{\mu}_{s^{\prime}}\Big{\rangle}\Big{|}\Big{)}.\]

**Theorem 12**.: _If \(n\geq(48\ln(M/\delta)-36d)/\min_{s}p_{s}\), we have for \(\delta\in(0,1)\),_

\[\mathbb{P}\Bigg{\{}\exists s\in[M],\Big{|}\Big{\langle}\tilde{ \beta}_{s},\mu_{s}-\hat{\mu}_{s}\Big{\rangle}\Big{|}>\sigma_{X}\sqrt{\frac{48 \ln(M/\delta)}{\min_{s}np_{s}+36d}}\Bigg{\}}\leq\delta.\]

Combining Theorems 11 and 12 immediately yields Theorem 2.

Proof of Theorem 11.: This proof investigates the distribution of \(\nu_{\hat{f}n|s}\). It is straightforward to verify that, conditioned on \(S=s\), \(\hat{f}_{n}(X,S)\) follows the Gaussian distribution with mean

\[\widehat{\|\beta.\|}\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{ \mu}_{s}\Big{\rangle}+\sum_{s^{\prime}\in[M]}\hat{p}_{s^{\prime}}\Big{\langle} \hat{\beta}_{s^{\prime}}^{\prime},\hat{\mu}_{s^{\prime}}^{\prime}\Big{\rangle},\]

and variance

\[\sigma_{X}^{2}\widehat{\|\beta.\|}^{2}.\]

We can thus evaluate the Wasserstein distance between the distributions \(\nu_{\hat{f}n|s}\) and \(\nu\hat{f}n|s^{\prime}\) using the Wasserstein distance between Gaussian distributions. Given two Gaussian distributions \(N(\mu,\sigma^{2})\) and \(N(\mu^{\prime},\sigma^{\prime 2})\), the 2-Wasserstein distance between them are obtained [20] as

\[W_{2}^{2}(N(\mu,\sigma^{2}),N(\mu^{\prime},\sigma^{2}))=(\mu- \mu^{\prime})^{2}+(\sigma-\sigma^{\prime})^{2}.\]

Therefore, we have

\[W_{2}^{2}(\nu_{\hat{f}_{n}|s},\nu_{\hat{f}_{n}|s^{\prime}})= \widehat{\|\beta.\|}^{2}\Big{(}\Big{\langle}\tilde{\beta}_{s},\mu _{s}-\hat{\mu}_{s}\Big{\rangle}-\Big{\langle}\tilde{\beta}_{s^{\prime}},\mu_{ s^{\prime}}-\hat{\mu}_{s^{\prime}}\Big{\rangle}\Big{)}^{2}\] \[\leq\]

which concludes the claim. 

Proof of Theorem 12.: We start by deriving the concentration inequality for \(\langle\tilde{\beta}_{s},\mu_{s}-\hat{\mu}_{s}\rangle\) conditioned on \(\tilde{\beta}_{s}\) and \(n.\) Note that \(\tilde{\beta}_{s}=0\) if \(n_{s}\leq 18d\). Conditioning on \(\tilde{\beta}_{s}\) and \(n.\), we observe that \(\langle\tilde{\beta}_{s},\mu_{s}-\hat{\mu}_{s}\rangle\) follows a Gaussian distribution with mean zero and variance \(\sigma_{X}^{2}/n_{3,s}\). Therefore, for any \(s\in[M]\) and \(t>0\),

\[\mathbb{P}\Big{\{}\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{ \mu}_{s}\Big{\rangle}>t\Big{|}\tilde{\beta}_{s},n.\Big{\}}\leq 1\{n_{s}>18d\}\exp \biggl{(}-\frac{n_{3,s}t^{2}}{2\sigma_{X}^{2}}\biggr{)}.\]

Taking the expectation with respect to \(\tilde{\beta}_{s}\) and using the fact that \(n_{3,s}\geq\lfloor n_{s}/3\rfloor\geq n_{s}/6\) for \(n_{s}\geq 6\), we obtain the following inequality for \(s\in[M]\) and \(t>0\):

\[\mathbb{P}\Big{\{}\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{\mu }_{s}\Big{\rangle}>t\Big{|}n.\Big{\}}\leq 1\{n_{s}>18d\}\exp\biggl{(}-\frac{n_{s}t^{2}}{1 2\sigma_{X}^{2}}\biggr{)}.\]Using the union bound for \(t>0\), we have

\[\mathbb{P}\Big{\{}\exists s\in[M],\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{\mu} _{s}\Big{\rangle}>t\Big{|}n.\Big{\}}\leq\sum_{s\in[M]}\mathbb{1}\{n_{s}>18d\} \exp\!\left(-\frac{n_{s}t^{2}}{12\sigma_{X}^{2}}\right)\!. \tag{12}\]

We now derive a sufficient condition on \(t\) such that the expectation of the right-hand side in Eq (12) is less than \(\delta\). First, we note that

\[\mathbb{1}\{n_{s}>18d\}\exp\!\left(-\frac{n_{s}t^{2}}{12\sigma_{X} ^{2}}\right)\] \[\leq \mathbb{1}\{n_{s}>18d\}\exp\!\left(-\frac{(n_{s}+18d)t^{2}}{12 \sigma_{X}^{2}}\frac{n_{s}}{n_{s}+18d}\right)\] \[\leq \exp\!\left(-\frac{(n_{s}+18d)t^{2}}{24\sigma_{X}^{2}}\right)\]

Taking the expectation and substituting the moment-generating function of the binomial distribution, we obtain

\[\mathbf{E}\!\left[\exp\!\left(-\frac{(n_{s}+18d)t^{2}}{24\sigma_{ X}^{2}}\right)\right]\] \[\leq \exp\!\left(-\frac{18dt^{2}}{24\sigma_{X}^{2}}\right)\!\left(1-p_ {s}+p_{s}e^{-\frac{t^{2}}{24\sigma_{X}^{2}}}\right)^{n}\] \[\leq \exp\!\left(-\frac{18dt^{2}}{24\sigma_{X}^{2}}-np_{s}\!\left(1-e ^{-\frac{t^{2}}{24\sigma_{X}^{2}}}\right)\right)\!.\]

Since \(1-e^{-x}\geq(1-e^{-1})x\) for \(x\in[0,1]\), if \(t^{2}/24\sigma_{X}^{2}\leq 1\), we have

\[\mathbf{E}\!\left[\exp\!\left(-\frac{(n_{s}+18d)t^{2}}{24\sigma_{ X}^{2}}\right)\right]\] \[\leq \exp\!\left(-\frac{18dt^{2}}{24\sigma_{X}^{2}}-\frac{(1-e^{-1})np _{s}t^{2}}{24\sigma_{X}^{2}}\right)\!.\]

Hence, \(\mathbf{E}[\exp(-\frac{(n_{s}+18d)t^{2}}{24\sigma_{X}^{2}})]\leq\nicefrac{{ \delta}}{{M}}\) if \(t\geq\sigma_{X}\sqrt{\frac{24\ln(M/\delta)}{(1-e^{-1})np_{s}+18d}}\leq\sigma_ {X}\sqrt{\frac{48\ln(M/\delta)}{\min_{s}np_{s}+36d}}\) because \((1-e^{-1})\geq 1/2\). To ensure \(t^{2}/24\sigma_{X}^{2}\leq 1\), we require \(n\geq(48\ln(M/\delta)-36d)/\min_{s}p_{s}\). 

## Appendix E Proofs for Norm and Direction Estimators

This section presents the proofs for Theorem 4 and Theorem 5. Our strategy for proving these theorems is to use the hyperellipsoid to interpret the distribution of the OLS estimator. Specifically, we begin by defining \(\Sigma_{n}=\frac{1}{n}X^{\top}X\) and expressing the OLS estimator \(\hat{\beta}\) as

\[\hat{\beta}=\beta^{*}+\Sigma_{n}^{-1}\bigg{(}\frac{1}{n}X^{\top}\xi\bigg{)}, \tag{13}\]

where \(\xi\) follows a zero-mean Gaussian distribution. Eq (13) shows that, conditioned on \(X\), \(\hat{\beta}\) follows a multivariate Gaussian distribution with mean \(\beta^{*}\) and covariance matrix \(\frac{\sigma_{x}^{2}}{n}\Sigma_{n}^{-1}\). We establish that, under the condition \(\|\hat{\beta}\|=r\), \(\hat{\beta}\) is supported on a hyperellipsoid \(E(r,\beta\cdot\frac{\beta}{d}\Sigma_{n})\), where \(E(r,c,A)=\{x\in\mathbb{R}^{d}:(x-c)^{\top}A(x-c)\leq r\}\) denotes the hyperellipsoid with \(r>0\), \(c\in\mathbb{R}^{d}\), and a symmetric and positive-definite matrix \(A\in\mathbb{R}^{d\times d}\).

To prove Theorem 4 and Theorem 5, we adopt the following strategy. First, we provide an approximation of the hyperellipsoid \(E(r,c,A)\) using the maximum eigenvalue of \(A^{-1}\), i.e., \(\lambda_{\max}(A^{-1})\). In our context, \(A=\frac{n}{\sigma_{x}^{2}}\Sigma_{n}\), and we then focus on the concentration inequalities regarding \(\lambda_{\max}(\Sigma_{n}^{-1})\). Finally, we combine these tools to prove both theorems.

**Lemmas regarding hyperellipsoid.** We present two lemmas that relate to the approximation of the hyperellipsoid \(E(r,c,A)\). Specifically, we demonstrate the following two lemmas:

**Lemma 2**.: _For \(r>0\), \(c\in\mathbb{R}^{d}\), and a symmetric and positive-definite matrix \(A\in\mathbb{R}^{d\times d}\), we have \(E(r,c,A)\subseteq E(r\lambda_{\max}(A^{-1}),c,I)\)._

**Lemma 3**.: _For \(r>0\), \(c\in\mathbb{R}^{d}\), and a symmetric and positive-definite matrix \(A\in\mathbb{R}^{d\times d}\), if \(r\lambda_{\max}(A^{-1})\leq\|c\|^{2}\), we have_

\[\inf_{x\in E(r,c,A)}\left\langle\frac{c}{\|c\|},\frac{x}{\|x\|} \right\rangle\geq\sqrt{1-\frac{r}{\|c\|^{2}}\lambda_{\max}(A^{-1})}.\]

These lemmas provide insight into the approximation of the hyperellipsoid \(E(r,c,A)\) for a given positive value of \(r\), vector \(c\) in \(\mathbb{R}^{d}\), and positive-definite symmetric matrix \(A\) in \(\mathbb{R}^{d\times d}\). Lemma 2 states that the hyperellipsoid \(E(r,c,A)\) is contained within a hyperellipsoid \(E(r\lambda_{\max}(A^{-1}),c,I)\). Lemma 3 shows that, under certain conditions, the minimum angle between a point in \(E(r,c,A)\) and the vector \(c\) is bounded below by a quantity that depends on \(r\), \(c\), and \(A\).

Proof of Lemma 2.: It is trivial that \(A-\lambda_{\min}(A)I\) is positive semi-definite. Equivalently, we have for any \(x\in\mathbb{R}^{d}\),

\[x^{\top}(A-\lambda_{\min}(A)I)x\geq 0\] \[\Longleftrightarrow x^{\top}Ax\geq x^{\top}\lambda_{\min}(A)Ix. \tag{14}\]

From Eq (14), for any \(x\in E(r,c,A)\), we have

\[x^{\top}\lambda_{\min}(A)Ix\leq x^{\top}Ax\leq r.\]

Hence, for any \(x\in E(r,c,A)\), we have

\[x^{\top}Ix\leq\frac{r}{\lambda_{\min}(A)}=\lambda_{\max}(A^{-1})r,\]

which indicates \(x\in E(r\lambda_{\max}(A^{-1}),c,I)\). 

Proof of Lemma 3.: Let \(\bar{c}=c/\|c\|\), and define a set \(\bar{E}(r,c,A)=\{x\in\mathbb{S}d-1:\exists\gamma>0,\gamma x\in E(r,c,A)\}\). Then, \(x\in\bar{E}(r,c,A)\) if and only if

\[\inf_{\gamma>0}(\gamma x-c)^{\top}A(\gamma x-c)\leq r. \tag{15}\]

We can rewrite the left-hand side of Eq (15) as

\[\gamma^{2}\langle x,Ax\rangle-2\gamma\langle c,Ax\rangle+\langle c,Ac\rangle\] \[= \langle x,Ax\rangle\bigg{(}\gamma-\frac{\langle c,Ax\rangle}{ \langle x,Ax\rangle}\bigg{)}^{2}+\langle c,Ac\rangle-\frac{\langle x,Ac \rangle^{2}}{\langle x,Ax\rangle}.\]

Hence,

\[\inf_{\gamma>0}(\gamma x-c)^{\top}A(\gamma x-c)=\langle c,Ac\rangle- \frac{(\langle x,Ac\rangle\lor 0)^{2}}{\langle x,Ax\rangle}.\]

Consequently, \(x\in\bar{E}(r,c,A)\) if and only if

\[\langle x,Ax\rangle\bigg{(}\langle\bar{c},A\bar{c}\rangle-\frac{ r}{\|c\|^{2}}\bigg{)}\leq(\langle x,A\bar{c}\rangle\lor 0)^{2}. \tag{16}\]

From Lemma 2, we have

\[\inf_{x\in E(r,c,A)}\left\langle\bar{c},\frac{x}{\|x\|}\right\rangle \geq \inf_{x\in E(\lambda_{\max}(A^{-1})r,c,I)}\left\langle\bar{c}, \frac{x}{\|x\|}\right\rangle\] \[= \inf_{x\in E(\lambda_{\max}(A^{-1})r,c,I)}\langle\bar{c},x\rangle. \tag{17}\]

By Eq (16), \(x\in\bar{E}(\lambda_{\max}(A^{-1})r,c,I)\) if and only if

\[1-\frac{r}{\|c\|^{2}}\lambda_{\max}(A^{-1})\leq(\langle x,\bar{c} \rangle\lor 0)^{2}. \tag{18}\]

Combining Eqs (17) and (18) and the assumption yields the claim.

**Least eigenvalue of the empirical covariance matrix.** The previous lemmas, Lemmas 2 and 3, provide valuable insight into analyzing the randomness regarding \(\xi\). However, to account for the randomness of \(X\), we must also control the lower bound on the least eigenvalue of \(A\) in Lemmas 2 and 3, which corresponds to the least eigenvalue of \(\frac{1}{A}X^{\top}X\) in our context. To this end, we leverage the high probability bound presented by Mourtada [15] based on the small-ball condition. We state the following probabilistic bound and expectation bound.

**Lemma 4**.: _For \(\mu\in\mathbb{R}^{d}\) and \(\sigma_{X}^{2}>0\), let \(X_{1},...,X_{n}\overset{\mathrm{iid}}{\sim}N(\mu,\sigma_{X}^{2}I)\), and let \(X=(X_{1}\cdots X_{n})^{\top}\). Then, for \(n>6d\), we have_

\[\mathbb{P}\bigg{\{}\lambda_{\min}\bigg{(}\frac{1}{n}X^{\top}X\bigg{)}<t\bigg{\}} \leq\bigg{(}\frac{21e^{10}}{\sigma_{X}^{2}}t\bigg{)}^{\nicefrac{{n}}{{6}}}.\]

**Lemma 5**.: _For \(\mu\in\mathbb{R}^{d}\) and \(\sigma_{X}^{2}>0\), let \(X_{1},...,X_{n}\overset{\mathrm{iid}}{\sim}N(\mu,\sigma_{X}^{2}I)\), and let \(X=(X_{1}\cdots X_{n})^{\top}\). Then, for \(n>6d\), we have_

\[\mathbf{E}\Bigg{[}\lambda_{\max}\bigg{(}\bigg{(}\frac{1}{n}X^{\top}X\bigg{)}^{ -1}\bigg{)}\Bigg{]}\leq\frac{21e^{10}}{\sigma_{X}^{2}}\bigg{(}1+\frac{6}{n-6} \bigg{)}.\]

To prove Lemma 4, we utilize Corollary 3 in Mourtada [15]. Specifically, we use the following theorem.

**Theorem 13** (Corollary 3 in Mourtada [15]).: _Let \(X\) be a random vector in \(\mathbb{R}^{d}\) such that \(\mathbf{E}[\|X\|^{2}]<+\infty\), and let \(\Sigma=\mathbf{E}[XX^{\top}]\). Let \(\hat{\Sigma}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}X_{i}^{\top}\), where \(X_{i}\) are i.i.d. copies of \(X\). Given \(C>0\) and \(\alpha\in(0,1]\), assume that for every \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and \(t>0\),_

\[\mathbb{P}\bigg{\{}\langle\theta,X\rangle^{2}\leq t^{2}\Big{\|}\Sigma^{ \nicefrac{{1}}{{2}}}\theta\Big{\|}^{2}\bigg{\}}\leq(Ct)^{\alpha}. \tag{19}\]

_Then, if \(\nicefrac{{d}}{{n}}\leq\nicefrac{{\alpha}}{{6}}\), for every \(t>0\),_

\[\hat{\Sigma}_{n}\succeq t\Sigma\]

_with probability at least \(1-(C^{\prime}t)^{\nicefrac{{\alpha}}{{6}}}\), where \(C^{\prime}=3C^{4}e^{1+\nicefrac{{9}}{{\alpha}}}\)._

Eq (19) is known as the small-ball condition.

Proof of Lemma 4.: To take an advantage of Theorem 13, we need to ensure that \(X_{i}\) satisfies the small-ball condition in Eq (19). Let \(\Sigma_{n}=\frac{1}{n}X^{\top}X\). Then, the expected value of \(\Sigma_{n}\) is equal to \(\sigma_{X}^{2}I+\mu\mu^{\top}\coloneqq\Sigma\), i.e., \(\mathbf{E}[\Sigma_{n}]=\Sigma\). Given \(\theta\in\mathbb{R}^{d}\setminus\{0\}\), \(\langle\theta,X_{i}\rangle^{2}/\sigma_{X}^{2}\|\theta\|^{2}\) follows the non-central \(\chi^{2}\) distribution with degree of freedom \(1\) and non-centrality parameter \(\langle\theta,\mu\rangle^{2}/\sigma_{X}^{2}\|\theta\|^{2}\). Consequently, we verify the satisfaction of the small-ball condition of \(X_{i}\) by confirming that for a random variable \(Z\) following the non-central \(\chi^{2}\) distribution with degree of freedom \(1\) and non-centrality parameter \(\lambda^{2}\), there exists \(C\) and \(\alpha\in(0,1]\) such that

\[\mathbb{P}\big{\{}Z\leq t^{2}\big{\}}\leq(Ct)^{\alpha}.\]

The cumulative distribution fucntion of the non-central \(\chi^{2}\) distribution with degree of freedom \(1\) has a closed-form using the error function (See [12] and references therein). Specifically, letting \(\mathrm{erf}(z)\) be the error function, defined as

\[\mathrm{erf}(z)=\frac{2}{\sqrt{\pi}}\int_{0}^{z}e^{-x^{2}}dx,\]

the cumulative distribution function of \(Z\) is obtained as

\[\mathbb{P}\big{\{}Z\leq t^{2}\big{\}}=\frac{1}{2}\bigg{(}\mathrm{erf}\bigg{(} \frac{t-\lambda}{\sqrt{2}}\bigg{)}+\mathrm{erf}\bigg{(}\frac{t+\lambda}{\sqrt {2}}\bigg{)}\bigg{)}.\]

Since \(e^{-x^{2}}\) is an even function, we have

\[\mathbb{P}\big{\{}Z\leq t^{2}\big{\}}= \frac{1}{\sqrt{2\pi}}\bigg{(}\int_{0}^{\lambda+t}e^{-\nicefrac{{x ^{2}}}{{2}}}dx+\int_{0}^{t-\lambda}e^{-\nicefrac{{x^{2}}}{{2}}}dx\bigg{)}\]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

Taking advantage of Lemma 3, we derive a lower bound on \(A_{n}\). Let \(r=(\hat{\beta}-\beta^{*})^{\top}(\frac{n}{\sigma_{\xi}^{2}}\Sigma_{n})(\hat{\beta }-\beta^{*})\). From Lemma 3, it follows that

\[A_{n}\geq\sqrt{1-\frac{\sigma_{\xi}^{2}r}{\|\beta^{*}\|^{2}n}\lambda_{\max} \big{(}\Sigma_{n}^{-1}\big{)}},\]

provided that \(r\leq n\|\beta^{*}\|^{2}/\sigma_{\xi}^{2}\lambda_{\min}(\Sigma_{n}^{-1})\). Since \(1-\sqrt{1-x}\leq x\) for \(x\in[0,1]\), it follows that

\[1-A_{n}\leq\frac{\sigma_{\xi}^{2}r}{\|\beta^{*}\|^{2}n}\lambda_{\max}\big{(} \Sigma_{n}^{-1}\big{)},\]

as long as \(r\leq n\|\beta^{*}\|^{2}/\sigma_{\xi}^{2}\lambda_{\max}(\Sigma_{n}^{-1})\).

Next, we derive an upper bound on the expectation of \(1-A_{n}\). Noting that conditioned on \(X\), \(r\) follows the \(\chi^{2}\) distribution with degree of freedom \(d\), we have

\[\mathbf{E}[1-A_{n}|X]\] \[= \mathbf{E}\Bigg{[}\Bigg{(}1\Bigg{\{}r\leq\frac{n\|\beta^{*}\|^{2 }}{\sigma_{\xi}^{2}\lambda_{\max}(\Sigma_{n}^{-1})}\Bigg{\}}+1\Bigg{\{}r>\frac{ n\|\beta^{*}\|^{2}}{\sigma_{\xi}^{2}\lambda_{\max}(\Sigma_{n}^{-1})}\Bigg{\}} (1-A_{n})\Bigg{|}X\Bigg{]}\] \[\leq \mathbf{E}\Bigg{[}\frac{\sigma_{\xi}^{2}r}{\|\beta^{*}\|^{2}n} \lambda_{\max}\big{(}\Sigma_{n}^{-1}\big{)}\Bigg{|}X\Bigg{]} \tag{25}\] \[= \frac{2\sigma_{\xi}^{2}d}{\|\beta^{*}\|^{2}n}\lambda_{\max}\big{(} \Sigma_{n}^{-1}\big{)}, \tag{26}\]

where we use the Markov inequality to obtain Eq (25).

By utilizing Eq (24), an upper bound on the expected error can be obtained by deriving an upper bound on the expectation of Eq (26). The random variable in Eq (26) is \(\lambda_{\max}\big{(}\Sigma_{n}^{-1}\big{)}\), which allows us to derive the upper bound on the expected error by obtaining an upper bound on the expectation of \(\lambda_{\max}\big{(}\Sigma_{n}^{-1}\big{)}\). To accomplish this, we apply Lemma 5. The upper bound from Lemma 5 can be substituted into Eq (26), resulting in the claimed upper bound. 

Proof of Theorem 5.: We first utilize Lemma 2 to get an upper bound on the squared error of the norm estimator. Let \(r=(\hat{\beta}-\beta^{*})^{\top}(\frac{n}{\sigma_{\xi}^{2}}\Sigma_{n})(\hat{ \beta}-\beta^{*})\). From Lemma 2, we have

\[\Big{\|}\hat{\beta}-\beta^{*}\Big{\|}^{2}\leq\frac{\sigma_{\xi}^{2}r}{n} \lambda_{\max}\big{(}\Sigma_{n}^{-1}\big{)}.\]

Application of the triangle and reverse triangle inequality yields

\[\|\beta^{*}\|-\sqrt{\frac{\sigma_{\xi}^{2}r}{n}\lambda_{\max}\big{(}\Sigma_{n} ^{-1}\big{)}}\leq\Big{\|}\hat{\beta}\Big{\|}\leq\|\beta^{*}\|+\sqrt{\frac{ \sigma_{\xi}^{2}r}{n}\lambda_{\max}\big{(}\Sigma_{n}^{-1}\big{)}},\]

equivalently

\[\Big{(}\Big{\|}\hat{\beta}\Big{\|}-\|\beta^{*}\|\Big{)}^{2}\leq\frac{\sigma_{ \xi}^{2}r}{n}\lambda_{\max}\big{(}\Sigma_{n}^{-1}\big{)}.\]

Taking expectation conditioned on \(X\) yields

\[\mathbf{E}\Bigg{[}\Big{(}\|\hat{\beta}\|-\|\beta^{*}\|\Big{)}^{2} \Bigg{|}X\Bigg{]}\] \[= \mathbf{E}\Bigg{[}\frac{\sigma_{\xi}^{2}r}{n}\lambda_{\max}\big{(} \Sigma_{n}^{-1}\big{)}\Bigg{|}X\Bigg{]}\] \[\leq \frac{\sigma_{\xi}^{2}d}{n}\lambda_{\max}\big{(}\Sigma_{n}^{-1} \big{)}, \tag{27}\]

where we use the fact that \(r\) follows the \(\chi^{2}\) distribution with degree of freedom \(d\) to obtain the last line. Again, application of Lemma 5 into expectation of Eq (27) yields the claim.

Details of Upper Bound Analyses

This section presents a detailed proof of the upper bound stated in Theorem 1, which is achieved through an analysis of the estimator constructed in Section 5. Specifically, we establish the following theorem:

**Theorem 14**.: _Let \(\hat{\beta}_{n}\) be the estimator constructed in Section 5. Then, there exists a universal constant \(C>0\) such that for any \(\delta\in(0,1)\) and \(n\geq 12(3d\lor 4\ln(M/\delta))/\min_{s\in[M]}p_{s}\),_

\[\mathcal{E}_{n}\bigg{(}\frac{1}{2},\delta\bigg{)}\leq C\frac{\sigma_{\xi}^{2}B^ {2}dM\lor\sigma_{X}^{2}B^{2}M\lor B^{2}U^{2}}{n}+o\bigg{(}\frac{1}{n}\bigg{)}.\]

To establish the validity of Theorem 14, we begin by proving Theorem 3, which demonstrates that the estimation error can be decomposed into the sum of errors associated with individual components. Subsequently, we derive upper bounds for the estimation errors of each component. Finally, we synthesize these results to provide a proof of Theorem 14.

### Proof of Theorem 3

We commence the error analysis of our estimator by decomposing the estimation error, as presented in Theorem 3. Recall the statement of Theorem 3

**Theorem 15**.: _For the estimator defined in Eq (6), the mean square deviation from \(f_{\mathrm{DP}}^{*}\) is bounded above by_

\[\sum_{s\in[M]}p_{s}\mathbf{E}\Bigg{[}\Bigg{(}\mathbf{E}\bigg{[} \big{\|}\widehat{\beta}_{\cdot}\big{\|}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1}}{ {2}}}\mathbf{E}\bigg{[}\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{\mu}_{s} \Big{\rangle}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}+\sigma_{X}\mathbf{E} \bigg{[}\Big{(}\|\widehat{\beta}_{\cdot}\|-\|\widehat{\beta}^{*}\|\Big{)}^{2} \bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}+\\ \sigma_{X}\overline{\|\beta^{*}\|}\mathbf{E}\bigg{[}\bigg{\|} \tilde{\beta}_{s}-\beta_{s}^{*}/\|\beta_{s}^{*}\|\Big{\|}^{2}\bigg{|}n.\bigg{]} ^{\nicefrac{{1}}{{2}}}+\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s^{\prime}\in[M]}\hat{p }_{s^{\prime}}\Big{\langle}\hat{\beta}_{s^{\prime}}^{\prime}-\beta_{s^{\prime }}^{*},\hat{\mu}_{s^{\prime}}^{\prime}\Big{\rangle}\Bigg{)}^{2}\Bigg{|}n. \Bigg{]}^{\nicefrac{{1}}{{2}}}+\\ \mathbf{E}\Bigg{[}\Bigg{(}\sum_{s^{\prime}\in[M]}\hat{p}_{s^{ \prime}}\langle\beta_{s^{\prime}}^{*},\hat{\mu}_{s^{\prime}}^{\prime}-\mu_{s^{ \prime}}\rangle\Bigg{)}^{2}\Bigg{|}n.\Bigg{]}^{\nicefrac{{1}}{{2}}}+\Bigg{|} \sum_{s^{\prime}\in[M]}(\hat{p}_{s^{\prime}}-p_{s^{\prime}})\langle\beta_{s^{ \prime}}^{*},\mu_{s^{\prime}}\rangle\Bigg{|}\Bigg{)}^{2}\Bigg{]}.\]

We provide a proof of Theorem 15 as follows:

Proof of Theorem 15.: We begin by decomposing \(\hat{f}_{n}(X,S)-f_{\mathrm{DP}}^{*}(X,S)\) into six terms. Recall the definitions of \(\hat{f}_{n}(x,s)\) and \(f_{\mathrm{DP}}^{*}(x,s)\):

\[\hat{f}_{n}(x,s)= \|\widehat{\beta}.\|\Big{\langle}\tilde{\beta}_{s},x-\hat{\mu}_{s }\Big{\rangle}+\sum_{s^{\prime}\in[M]}\hat{p}_{s^{\prime}}\Big{\langle}\hat{ \beta}_{s}^{\prime},\hat{\mu}_{s}^{\prime}\Big{\rangle}\] \[f_{\mathrm{DP}}^{*}(x,s)= \overline{\|\beta^{*}\|}\Big{\langle}\frac{\beta_{s}^{*}}{\| \beta_{s}^{*}\|},x-\mu_{s}\Big{\rangle}+\sum p_{s^{\prime}}\langle\beta_{s^{ \prime}}^{*},\mu_{s^{\prime}}\rangle.\]

Through elementary calculations, we obtain:

\[\hat{f}_{n}(X,S)-f_{\mathrm{DP}}^{*}(X,S)=\|\widehat{\beta}.\| \Big{\langle}\tilde{\beta}_{S},\mu_{S}-\hat{\mu}_{S}\Big{\rangle}+\Big{(}\| \widehat{\beta}.\|-\|\beta^{*}.\|\Big{)}\Big{\langle}\tilde{\beta}_{S},X-\mu_ {S}\Big{\rangle}\\ +\|\beta^{*}.\|\Big{\langle}\tilde{\beta}_{S}-\frac{\beta_{s}^{*} }{\|\beta_{S}\|},X-\mu_{S}\Big{\rangle}+\sum_{s^{\prime}\in[M]}\hat{p}_{s^{ \prime}}\Big{\langle}\hat{\beta}_{s^{\prime}}^{\prime}-\beta_{s^{\prime}}^{*}, \hat{\mu}_{s^{\prime}}^{\prime}\Big{\rangle}\\ +\sum_{s^{\prime}\in[M]}\hat{p}_{s^{\prime}}\langle\beta_{s^{ \prime}}^{*},\hat{\mu}_{s^{\prime}}^{\prime}-\mu_{s^{\prime}}\rangle+\sum_{s^{ \prime}\in[M]}(\hat{p}_{s^{\prime}}-p_{s^{\prime}})\langle\beta_{s^{\prime}}^{ *},\mu_{s^{\prime}}\rangle. \tag{28}\]By the Cauchy-Schwarz inequality, for two random variable \(Z_{1}\) and \(Z_{2}\), we have \(\mathbf{E}[(Z_{1}+Z_{2})^{2}]^{\nicefrac{{1}}{{2}}}\leq\mathbf{E}[Z_{1}^{2}]^{ \nicefrac{{1}}{{2}}}+\mathbf{E}[Z_{2}^{2}]^{\nicefrac{{1}}{{2}}}\). By applying this fact into the expectation of Eq (28) conditioned on \(S\) and \(n\). multiple times, we have

\[\mathbf{E}\bigg{[}\Big{(}\hat{f}_{n}(X,S)-f_{\mathrm{DP}}^{*}(X,S )\Big{)}^{2}\bigg{]}\] \[=\sum_{s\in[M]}p_{s}\mathbf{E}\bigg{[}\mathbf{E}\bigg{[}\Big{(} \hat{f}_{n}(X,S)-f_{\mathrm{DP}}^{*}(X,S)\Big{)}^{2}\bigg{|}S=s,n.\bigg{]} \bigg{|}S=s\bigg{]}\] \[\leq\sum_{s\in[M]}p_{s}\mathbf{E}\bigg{[}\Bigg{(}\mathbf{E}\bigg{[} \Big{(}\big{\|}\widehat{\beta}.\|\Big{\langle}\hat{\beta}_{S},\mu_{S}-\hat{\mu }_{S}\Big{\rangle}\Big{)}^{2}\bigg{|}S=s,n.\bigg{]}^{\nicefrac{{1}}{{2}}} \tag{29}\] \[\qquad\qquad\qquad\qquad+\mathbf{E}\bigg{[}\Big{(}\big{(}\big{\|} \widehat{\beta}.\|-\|\beta^{*}.\|\Big{)}\Big{\langle}\hat{\beta}_{S},X-\mu_{S} \Big{\rangle}\Big{)}^{2}\bigg{|}S=s,n.\bigg{]}^{\nicefrac{{1}}{{2}}}\] \[\qquad\qquad\qquad+\mathbf{E}\bigg{[}\bigg{(}\big{\|}\beta^{*}. \|\Big{\langle}\hat{\beta}_{S}-\frac{\beta_{S}^{*}}{\|\beta_{S}^{*}\|},X-\mu_{S }\Big{\rangle}\bigg{)}^{2}\bigg{|}S=s,n.\bigg{]}^{\nicefrac{{1}}{{2}}}\] \[\qquad\qquad\qquad+\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s^{\prime}\in [M]}\hat{p}_{s^{\prime}}\big{\langle}\hat{\beta}_{s^{\prime}}^{*},\hat{\mu}_{s ^{\prime}}^{*}-\mu_{s^{\prime}}\big{\rangle}\Bigg{)}^{2}\Bigg{|}S=s,n.\Bigg{]} ^{\nicefrac{{1}}{{2}}}\] \[\qquad\qquad\qquad+\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s^{\prime}\in [M]}(\hat{p}_{s^{\prime}}-p_{s^{\prime}})\langle\beta_{s^{\prime}}^{*},\mu_{s ^{\prime}}\big{\rangle}\Bigg{)}^{2}\Bigg{|}S=s,n.\Bigg{]}^{\nicefrac{{1}}{{2}} }\Bigg{)}^{2}\Bigg{|}S=s\Bigg{]}.\]

In the subsequent analyses, we derive upper bounds for each term in Eq (29).

(First term in Eq (29)) Due to the splitting of the sample, \(\widehat{\|\beta.\|}\), \(\hat{\beta}_{s}\), and \(\hat{\mu}_{s}\) are independent conditioned on \(n.\). Thus, we have:

\[\mathbf{E}\bigg{[}\Big{(}\big{\|}\widehat{\beta}.\|\Big{\langle} \hat{\beta}_{S},\mu_{S}-\hat{\mu}_{S}\Big{\rangle}\Big{)}^{2}\bigg{|}S=s,n. \bigg{]}^{\nicefrac{{1}}{{2}}}\] \[= \mathbf{E}\bigg{[}\Big{(}\big{\|}\widehat{\beta}.\|\Big{\langle} \hat{\beta}_{s},\mu_{s}-\hat{\mu}_{s}\Big{\rangle}\Big{)}^{2}\bigg{|}n.\bigg{]} ^{\nicefrac{{1}}{{2}}}\] \[= \mathbf{E}\bigg{[}\Big{(}\big{\|}\widehat{\beta}.\|^{2}\Big{)} \bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}\mathbf{E}\bigg{[}\Big{\langle}\hat{ \beta}_{s},\mu_{s}-\hat{\mu}_{s}\Big{\rangle}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1 }}{{2}}}.\]

This term matches the first term of the desired bound.

(Second term in Eq (29)) Since \(\widehat{\|\beta.\|}\), \(\tilde{\beta}_{s}\), and \(X\) are independent conditioned on \(n.\), we have

\[\mathbf{E}\bigg{[}\Big{(}\big{(}\big{\|}\widehat{\beta}.\|-\| \beta^{*}.\|\Big{)}\Big{\langle}\hat{\beta}_{s},X-\mu_{s}\Big{\rangle}\Big{)}^{2 }\bigg{|}S=s,n.\bigg{]}\] \[= \sigma_{X}^{2}\mathbf{E}\bigg{[}\Big{(}\big{\|}\widehat{\beta}. \|-\|\beta^{*}.\|\Big{)}^{2}\bigg{|}n.\bigg{]},\]

where we use the fact that \(X-\mu_{s}\sim N(0,\sigma_{X}^{2}I)\) conditioned on \(S=s\), and \(\tilde{\beta}_{s}\in\mathcal{S}_{d-1}\) almost surely. This result corresponds to the second term of the desired bound.

(Third term in Eq (29)) Since \(X-\mu_{s}\sim N(0,\sigma_{X}^{2}I)\), we have

\[\mathbf{E}\bigg{[}\bigg{(}\big{\|}\widehat{\beta^{*}.\|}\Big{\langle}\tilde{ \beta}_{s}-\frac{\beta_{s}^{*}}{\|\beta_{s}^{*}\|},X-\mu_{s}\Big{\rangle}\bigg{)} ^{2}\bigg{|}S=s,n.\bigg{]}\]\[= \sigma_{X}^{2}\overline{\|\beta^{*}\|}^{2}\mathbf{E}\Bigg{[}\bigg{\|} \tilde{\beta}_{s}-\frac{\beta^{*}_{s}}{\|\beta^{*}_{s}\|}\bigg{\|}^{2}\Bigg{|}n.\Bigg{]},\]

which corresponds to the third term of the desired bound.

(Forth and fifth terms in Eq (29)) These terms are independent of \(S\), so we can omit \(S=s\) from the condition, resulting in the fourth and fifth terms of the desired bound.

(Sixth term in Eq (29)) This term does not contain any random variable when \(n\). is fixed. Thus, we can remove the expectation, yielding the sixth term of the desired bound. 

### Estimation Error Analyses for Each Component

This subsection presents an analysis of the estimation errors associated with each component estimator. In particular, we investigate the estimation errors of \(\hat{\mu}_{s}\), \(\overline{\|\beta\cdot\|}\), \(\tilde{\beta}_{s}\), \(\hat{\beta}^{\prime}_{s}\), and \(\hat{\mu}^{\prime}_{s}\).

#### e.2.1 Estimation Error Analysis for \(\hat{\mu}_{s}\)

Here, we presents the proof of the following theorem.

**Theorem 16**.: _Given \(s\in[M]\), if \(n_{s}>18d\), we have_

\[\sup_{v\in\mathbb{S}_{d-1}}\mathbf{E}\Big{[}\langle v,\mu_{s}- \hat{\mu}_{s}\rangle^{2}\Big{|}n.\Big{]}\leq\frac{6\sigma_{X}^{2}}{n_{s}}.\]

Proof of Theorem 16.: Given \(v\in\mathbb{S}_{d-1}\), we have

\[\mathbf{E}\Big{[}\langle v,\mu_{s}-\hat{\mu}_{s}\rangle^{2}\Big{|}n.\Big{]}=\Big{\langle}v,\mathbf{E}\Big{[}(\mu_{s}-\hat{\mu}_{s})(\mu_{s}-\hat {\mu}_{s})^{\top}\Big{|}n.\Big{]}v\Big{\rangle}.\]

According to the definition, \(\hat{\mu}_{s}\) is an average of \(n_{3,s}\) i.i.d. random variables following \(N(\mu_{s},\sigma_{X}^{2}I)\). Hence, we have \(\mu_{s}-\hat{\mu}_{s}\sim N(0,\frac{\sigma_{X}^{2}}{n_{3,s}}I)\), which implies \(\mathbf{E}[(\mu_{s}-\hat{\mu}_{s})(\mu_{s}-\hat{\mu}_{s})^{\top}|n.]=\frac{ \sigma_{X}^{2}}{n_{3,s}}I\). Consequently, we obtain:

\[\mathbf{E}\Big{[}\langle v,\mu_{s}-\hat{\mu}_{s}\rangle^{2}\Big{|} n.\Big{]}= \bigg{\langle}v,\frac{\sigma_{X}^{2}}{n_{3,s}}Iv\bigg{\rangle}\] \[= \frac{\sigma_{X}^{2}}{n_{3,s}}\langle v,v\rangle=\frac{\sigma_{X }^{2}}{n_{3,s}}.\]

Since \(n_{3,s}\geq\lfloor n_{s}/3\rfloor\geq n_{s}/6\) for \(n_{s}\geq 6\), the claim follows. 

#### e.2.2 Estimation Error Analysis for \(\overline{\|\beta\cdot\|}\)

Here, we present the proof of the following theorem.

**Theorem 17**.: _For any \(s\in[M]\), we have_

\[\mathbf{E}\bigg{[}\bigg{(}\overline{\|\beta\cdot\|}-\overline{ \|\beta^{*}\cdot\|}\bigg{)}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}\leq \sqrt{\frac{189e^{10}\sigma_{\xi}^{2}Md}{\sigma_{X}^{2}n}+\sum_{s\in[M]} \mathbb{1}\left\{n_{s}\leq 18d\right\}\frac{n_{s}}{n}}+\Bigg{|}\sum_{s\in[M]}( \hat{p}_{s}-p_{s})\|\beta^{*}_{s}\|\Bigg{|}.\]

Proof of Theorem 17.: By combining the definitions of \(\overline{\|\beta\cdot\|}\) and \(\overline{\|\beta^{*}\cdot\|}\) and utilizing the Cauchy-Schwarz inequality, we obtain

\[\mathbf{E}\bigg{[}\bigg{(}\overline{\|\beta\cdot\|}-\overline{ \|\beta^{*}\cdot\|}\bigg{)}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}\] \[= \mathbf{E}\Bigg{[}\Bigg{(}\sum_{s\in[M]}\hat{p}_{s}\Big{(}\overline {\|\beta_{s}\|}-\overline{\|\beta^{*}_{s}\|}\Big{)}+\sum_{s\in[M]}(\hat{p}_{s }-p_{s})\overline{\|\beta^{*}_{s}\|}\Bigg{)}^{2}\Bigg{|}n.\Bigg{]}^{\nicefrac{{1 }}{{2}}}\]\[\leq \mathbf{E}\Bigg{[}\Bigg{(}\sum_{s\in[M]}\hat{p}_{s}\Big{(}\|\widehat{ \beta_{s}}\|-\overline{\|\beta_{s}^{*}\|}\Big{)}\Bigg{)}^{2}\Bigg{|}n.\Bigg{]}^{ \nicefrac{{1}}{{2}}}+\Bigg{|}\sum_{s\in[M]}(\hat{p}_{s}-p_{s})\overline{\|\beta_ {s}^{*}\|}\Bigg{|}. \tag{30}\]

Next, we derive an upper bound for the first term in Eq (30). Applying Jensen's inequality, we have:

\[\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s\in[M]}\hat{p}_{s}\Big{(}\| \widehat{\beta_{s}}\|-\overline{\|\beta_{s}^{*}\|}\Big{)}\Bigg{)}^{2}\Bigg{|}n. \Bigg{]}\] \[\leq\sum_{s\in[M]}\frac{n_{s}}{n}\mathbf{E}\bigg{[}\Big{(}\| \widehat{\beta_{s}}\|-\overline{\|\beta_{s}^{*}\|}\Big{)}^{2}\Bigg{|}n.\Bigg{]}\] \[=\sum_{s\in[M]}\frac{n_{s}}{n}\bigg{(}\mathbbm{1}\left\{n_{s}>18d \right\}\mathbf{E}\bigg{[}\Big{(}\|\widehat{\beta_{s}}\|-\overline{\|\beta_{s }^{*}\|}\Big{)}^{2}\bigg{|}n.\bigg{]}+\mathbbm{1}\left\{n_{s}\leq 18d\right\}\|\beta_{s} ^{*}\|^{2}\bigg{)}\]

Using the fact that \(n_{1,s}>6d\) for \(n_{s}>18d\) and employing Theorem 5, we obtain:

\[\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s\in[M]}\hat{p}_{s}\Big{(}\| \widehat{\beta_{s}}\|-\overline{\|\beta_{s}^{*}\|}\Big{)}\Bigg{)}^{2}\Bigg{|}n. \Bigg{]}\] \[\leq \sum_{s\in[M]}\frac{n_{s}}{n}\frac{21e^{10}\sigma_{\xi}^{2}d}{n_{ 1,s}}\bigg{(}1+\frac{6}{n_{1,s}-6}\bigg{)}+\sum_{s\in[M]}\mathbbm{1}\left\{n_{ s}\leq 18d\right\}\frac{\|\beta_{s}^{*}\|^{2}n_{s}}{n}\] \[\leq \frac{189e^{10}\sigma_{\xi}^{2}Md}{\sigma_{X}^{2}n}+\sum_{s\in[M ]}\mathbbm{1}\left\{n_{s}\leq 18d\right\}\frac{B^{2}n_{s}}{n},\]

where the last line follows from the fact that \(n_{1,s}\geq\lfloor n_{s}/3\rfloor\geq n_{s}/6\) for \(n_{s}\geq 6\), \(\frac{6}{n_{1,s}-6}\leq\nicefrac{{1}}{{2}}\) for \(n_{s}\geq 18\), and \(\|\beta_{s}^{*}\|\leq B\). 

#### f.2.3 Estimation Error Analysis for \(\tilde{\beta}_{s}\)

Here, we will prove the following theorem.

**Theorem 18**.: _For any \(s\in[M]\), we have_

\[\mathbf{E}\Bigg{[}\bigg{\|}\tilde{\beta}_{s}-\frac{\beta_{s}^{*}}{\|\beta_{s}^ {*}\|}\bigg{\|}^{2}\Bigg{|}n.\Bigg{]}\leq\begin{cases}\frac{756e^{10}\sigma_{ \xi}^{2}d}{\sigma_{X}^{2}\|\beta_{s}^{*}\|^{2}n_{s}}&\text{if }n_{s}>18d,\\ 1&\text{otherwise }.\end{cases}\]

Proof of Theorem 18.: If \(n_{s}\leq 18d\), \(\tilde{\beta}_{s}=0\), and we thus have \(\mathbf{E}[\|\tilde{\beta}_{s}-\frac{\beta_{s}^{*}}{\|\beta_{s}^{*}\|}\|^{2}|n.]=\|\frac{\beta_{s}^{*}}{\|\beta_{s}^{*}\|}\|^{2}=1\). For \(n_{s}>18d\), we have \(n_{2,s}>6d\). Application of Theorem 4 yields

\[\mathbf{E}\Bigg{[}\bigg{\|}\tilde{\beta}_{s}-\frac{\beta_{s}^{*}}{\|\beta_{s}^ {*}\|}\bigg{\|}^{2}\Bigg{|}n.\Bigg{]}\leq\frac{84e^{10}\sigma_{\xi}^{2}d}{ \sigma_{X}^{2}\|\beta_{s}^{*}\|^{2}n_{2,s}}\bigg{(}1+\frac{6}{n_{2,s}-6}\bigg{)}\]

We get the claim in the same manner as the proof of Theorem 17. 

#### f.2.4 Estimation Error Analysis for \(\hat{\beta}^{\prime}_{s^{\prime}}\)

Here, we will prove the following theorem.

**Theorem 19**.: _Given \(s\in[M]\), let \(\Sigma_{s}=\mathbf{E}[X_{s}X_{s}^{\top}]\) for \(X_{s}\sim N(\mu_{s},\sigma_{X}^{2}I)\). Then, if \(n_{s}>12d\), we have_

\[\mathbf{E}\bigg{[}\bigg{\|}\Sigma_{s}^{\nicefrac{{1}}{{2}}}\Big{(}\hat{\beta}_{ s}^{*}-\beta_{s}^{*}\Big{)}\bigg{\|}^{2}\bigg{|}n.\bigg{]}\leq\frac{4\sigma_{ \xi}^{2}d}{n_{s}}+504e^{10}\sigma_{\xi}^{2}\bigg{(}\frac{4d}{n_{s}}\bigg{)}^{2}.\]To prove Theorem 19, we utilize the following theorem presented by Mourtada [15].

**Theorem 20** (Theorem 3 in [15]).: _Let \(X\) be a random vector in \(\mathbb{R}^{d}\) such that it satisfies the small-ball condition of Eq (19) and \(\mathbf{E}[\|\Sigma^{-\nicefrac{{1}}{{2}}}X\|^{4}]\leq\kappa d\) for some \(\kappa>0\), where \(\Sigma=\mathbf{E}[XX^{\top}]\). Let \(\hat{\Sigma}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}X_{i}^{\top}\), where \(X_{i}\) are i.i.d. copies of \(X\). If \(n\geq 6\alpha^{-1}d\wedge 12\alpha^{-1}\ln(12\alpha^{-1})\),_

\[\frac{1}{n}\mathbf{E}\Big{[}\mathrm{Tr}\Big{(}\hat{\Sigma}_{n}^{-1}\Sigma\Big{)} \Big{]}\leq\frac{d}{n}+8C^{\prime}\kappa\bigg{(}\frac{d}{n}\bigg{)}^{2},\]

_where \(\alpha\) and \(C^{\prime}\) are as in Theorem 13._

Proof of Theorem 19.: We can easily confirm that \(\hat{\beta}_{s}^{\prime}\sim N(\beta_{s}^{*},\frac{\sigma_{\xi}^{2}}{n_{1,s}^ {\prime}}(\Sigma_{1,s}^{\prime})^{-1})\) conditioned on \(n\). and \(X_{1,s}^{\prime}\), where \(\Sigma_{1,s}^{\prime}=\frac{1}{n_{1,s}^{\prime}}X_{1,s}^{\prime}(X_{1,s}^{ \prime})^{\top}\). Noting that \(\mathbf{E}[(\hat{\beta}_{s}^{\prime}-\beta_{s}^{*})(\hat{\beta}_{s}^{\prime}- \beta_{s}^{*})^{\top}|X,n]=\frac{\sigma_{\xi}^{2}}{n_{1,s}^{\prime}}(\Sigma_{1, s}^{\prime})^{-1}\), we have

\[\mathbf{E}\bigg{[}\bigg{\|}\Sigma_{m,s}^{\nicefrac{{1}}{{2}}} \Big{(}\hat{\beta}_{s^{\prime}}^{\prime}-\beta_{s}^{*}\Big{)}\bigg{\|}^{2}\bigg{|} n.\bigg{]}\] \[= \mathrm{Tr}\bigg{(}\Sigma_{s}\mathbf{E}\bigg{[}\Big{(}\hat{\beta }_{s^{\prime}}^{\prime}-\beta_{s}^{*}\Big{)}\Big{(}\hat{\beta}_{s^{\prime}}^{ \prime}-\beta_{s}^{*}\Big{)}^{\top}\bigg{|}n.\bigg{]}\bigg{)}\] \[= \frac{\sigma_{\xi}^{2}}{n_{1,s}^{\prime}}\mathbf{E}\Big{[}\mathrm{ Tr}\Big{(}\Sigma_{s}\big{(}\Sigma_{1,s}^{\prime}\big{)}^{-1}\Big{)}\Big{|}n. \Big{]}. \tag{31}\]

We apply Theorem 20 to the expected trace term in Eq (31). To do so, we need to check \(X_{1,s}^{\prime}\) satisfies the small-ball condition of Eq (19) and the kurtosis condition \(\mathbf{E}[\|\Sigma_{s}^{-\nicefrac{{1}}{{2}}}X_{1,s}^{\prime}\|^{4}]\leq\kappa d\).

The small-ball condition is confirmed by the same manner in the proof of Lemma 5, with \(C=7^{\nicefrac{{1}}{{4}}}\) and \(\alpha=1\). Here, we prove the satisfaction of the kurtosis condition. For a multivariate Gaussian random variable \(X\sim N(\mu,\Lambda)\) such that \(\lambda_{\min}(\Lambda)>0\), \(\Sigma\coloneqq\mathbf{E}[XX^{\top}]=\mu\mu^{\top}+\Lambda\), and \(\|\Sigma^{-\nicefrac{{1}}{{2}}}X\|^{4}=\langle X,\Sigma^{-1}X\rangle\)

\[\mathbf{E}\bigg{[}\bigg{\|}\Sigma^{-\nicefrac{{1}}{{2}}}X\bigg{\|} ^{4}\bigg{]}= \mathbf{E}\big{[}\langle X,\Sigma^{-1}X\rangle^{2}\big{]}\] \[= \mathbf{Var}\big{[}\langle X,\Sigma^{-1}X\rangle\big{]}+\big{(} \mathbf{E}\big{[}\langle X,\Sigma^{-1}X\rangle\big{]}\big{)}^{2}.\]

Since we have

\[\mathbf{E}\big{[}\langle X,\Sigma^{-1}X\rangle\big{]}= \mathrm{Tr}\big{(}\Sigma^{-1}\big{(}\mu\mu^{\top}+\Lambda\big{)} \big{)}=d\] \[\mathbf{Var}\big{[}\langle X,\Sigma^{-1}X\rangle\big{]}=\] \[= 2\mathrm{Tr}\big{(}\Sigma^{-1}\Lambda\big{)}+2\mathrm{Tr}\big{(} \Sigma^{-1}\Lambda\Sigma^{-1}\mu\mu^{\top}\big{)}\] \[=\] \[= 2d-2\|\mu\|^{2}\big{\langle}\mu,\Sigma^{-1}\mu\big{\rangle}\leq 2d.\]

Hence, the kurtosis condition satisfies with \(\kappa=3\).

Application of Theorem 20 into Eq (31) yields

\[\mathbf{E}\bigg{[}\bigg{\|}\Sigma_{m,s}^{\nicefrac{{1}}{{2}}}\Big{(}\hat{\beta }_{s^{\prime}}^{\prime}-\beta_{s}^{*}\Big{)}\bigg{\|}^{2}\bigg{|}n.\bigg{]} \leq\frac{\sigma_{\xi}^{2}d}{n_{1,s}^{\prime}}+504e^{10}\sigma_{\xi}^{2}\bigg{(} \frac{d}{n_{1,s}^{\prime}}\bigg{)}^{2},\]

provided that \(n_{s}\geq 12d\). We get the claim from the fact that \(n_{1,s}^{\prime}\geq\lfloor n_{s}/2\rfloor\geq n_{s}/4\) for \(n_{s}\geq 4\),. 

#### f.2.5 Estimation Error Analysis for \(\hat{\mu}_{s}^{\prime}\)

Here, we will prove the following theorem.

**Theorem 21**.: _Given \(s\in[M]\) and \(v\in\mathbb{R}^{d}\), if \(n_{s}>12d\), we have_

\[\mathbf{E}\Big{[}\langle v,\hat{\mu}_{s}^{\prime}-\mu_{s}\rangle^{2}\Big{|}n. \Big{]}\leq\frac{4\sigma_{X}^{2}\|v\|^{2}}{n_{s}}.\]Proof of Theorem 21.: By definition, we have \(\hat{\mu}_{s}^{\prime}\sim N(\mu_{s},\frac{\sigma_{X}^{2}}{n_{2,s}^{2}}I)\) conditioned on \(n\). for \(n_{s}>12d\). Hence, we have

\[\mathbf{E}\Big{[}\langle v,\hat{\mu}_{s}^{\prime}-\mu_{s}\rangle^{2}\Big{|}n. \Big{]}\leq\frac{\sigma_{X}^{2}\|v\|^{2}}{n_{2,s}^{\prime}}.\]

We get the claim following the same manner of the proof of Theorem 19. 

### Some Auxiliary Lemmas

This subsections introduce some auxiliary lemmas for use to prove Theorem 14. Specifically, we demonstrate the following lemmas:

**Lemma 6**.: _Let \(a_{1},...,a_{M}\in\mathbb{R}\) be arbitrary numbers. Then, we have_

\[\mathbf{E}\Bigg{[}\left(\sum_{s\in[M]}a_{s}(\hat{p}_{s}-p_{s})^{2}\right)\Bigg{]} =\frac{1}{n}\mathbf{Var}[a_{S}].\]

**Lemma 7**.: _For a constant \(c>0\), we have for any \(s\in[M]\)_

\[\mathbf{E}\big{[}n_{s}^{-1}\,\mathbb{1}\{n_{s}>c\}\big{]}\leq\frac{1+c^{-1}}{p _{s}(n+1)}.\]

**Lemma 8**.: _Let \(c>0\) be a constant. If \(n>2c/\min_{s\in[M]}p_{s}\),we have for any \(s\in[M]\)_

\[\mathbb{P}\{n_{s}\leq c\}\leq e^{-np_{s}/8}.\]

Proof of Lemma 6.: Since \(n\hat{p}\). follows the multinomial distribution with the parameters \(n\) and \(p\)., using the variance and covariance of the multinomial distribution, we have

\[\mathbf{E}\Bigg{[}\left(\sum_{s\in[M]}a_{s}(\hat{p}_{s}-p_{s}) \right)^{2}\Bigg{]}\] \[=\sum_{s\in[M]}\frac{a_{s}^{2}p_{s}(1-p_{s})}{n}-\sum_{s,s^{\prime }\in[M]:s\neq s^{\prime}}\frac{a_{s}a_{s^{\prime}}p_{s}p_{s^{\prime}}}{n}\] \[=\sum_{s\in[M]}\frac{a_{s}p_{s}}{n}\left(a_{s}-\sum_{s^{\prime} \in[M]}a_{s^{\prime}}p_{s^{\prime}}\right).\]

Let \(\bar{a}=\sum_{s\in[M]}p_{s}a_{s}\). Then, we have

\[\sum_{s\in[M]}a_{s}p_{s}(a_{s}-\bar{a})\] \[=\sum_{s\in[M]}p_{s}(a_{s}-\bar{a})^{2}+\sum_{s\in[M]}\bar{a}p_{s }(a_{s}-\bar{a})\] \[=\sum_{s\in[M]}p_{s}(a_{s}-\bar{a})^{2}=\mathbf{Var}[a_{S}].\]

Hence,

\[\mathbf{E}\Bigg{[}\left(\sum_{s\in[M]}a_{s}(\hat{p}_{s}-p_{s}) \right)^{2}\Bigg{]}=\frac{1}{n}\mathbf{Var}[a_{S}].\]Proof of Lemma 7.: For a random variable \(X\) following the binomial distribution with the parameters \(n\) and \(p\), \(\mathbf{E}[\frac{1}{X+1}]=\frac{1}{p(n+1)}(1-(1-p)^{n+1})\)[5]. Since \(n_{s}\) follows the binomial distribution with the parameters \(n\) and \(p_{s}\), we have We have

\[\mathbf{E}\big{[}n_{s}^{-1}\operatorname{\mathbb{1}}\{n_{s}>c\} \big{]}\] \[= \mathbf{E}\bigg{[}\frac{1}{n_{s}+1}\bigg{(}1+\frac{1}{n_{s}} \bigg{)}\operatorname{\mathbb{1}}\{n_{s}>c\}\bigg{]}\] \[\leq \big{(}1+c^{-1}\big{)}\mathbf{E}\bigg{[}\frac{1}{n_{s}+1}\bigg{]}\] \[= \frac{1+c^{-1}}{p_{s}(n+1)}\big{(}1-(1-p_{s})^{n+1}\big{)}\leq \frac{1+c^{-1}}{p_{s}(n+1)}.\]

Proof of Lemma 8.: From the Chernoff bound, we have

\[\mathbb{P}\{n_{s}\leq c\}\leq\exp\biggl{(}-\frac{n}{2p_{s}}\Big{(}\frac{c}{n}- p_{s}\Big{)}^{2}\biggr{)}.\]

Under the assumption, we have \(c\leq np_{s}/2\). Then, we have \(\frac{n}{2p_{s}}\big{(}\frac{c}{n}-p_{s}\big{)}^{2}\geq n/8\), which gives the claim. 

### Proof of Theorem 14

Proof of Theorem 14.: We begin by characterizing the estimation error by each component's estimation error shown in Theorems 16 to 19 and 21. Specifically, we characterize the estimation error using the following error terms:

\[e_{\mathrm{mean},s}^{2}=\sup_{v\in\mathbb{S}_{d-1}}\mathbf{E} \Big{[}\langle v,\mu_{s}-\hat{\mu}_{s}\rangle^{2}\Big{|}n.\Big{]}\] \[e_{\mathrm{norm}}^{2}= \mathbf{E}[(\overline{\|\beta^{*}\|}-\overline{\|\beta_{\cdot}\| })^{2}|n.]\] \[e_{\mathrm{coef},s}^{2}= \mathbf{E}[\|\tilde{\beta}_{s}-\frac{\beta_{s}^{*}}{\|\beta_{s}^ {*}\|}\|^{2}|n.]\] \[e_{\mathrm{coef}^{\prime},s}^{2}= \mathbf{E}\bigg{[}\biggl{\|}\Sigma_{s}^{\nicefrac{{1}}{{2}}} \big{(}\hat{\beta}_{s}^{\prime}-\beta_{s}^{*}\big{)}\biggr{\|}^{2}\bigg{|}n. \bigg{]}\] \[e_{\mathrm{mean}^{\prime},s}^{2}= \sup_{v\in\mathbb{S}_{d-1}}\mathbf{E}\big{[}\langle\hat{\mu}_{s} ^{\prime}-\mu_{s},v\rangle^{2}\big{|}n.\big{]}\] \[e_{\mathrm{prob}}^{2}= \left|\sum_{s^{\prime}\in[M]}(\hat{p}_{s^{\prime}}-p_{s^{\prime}} )\langle\beta_{s^{\prime}}^{*},\mu_{s^{\prime}}\rangle\right|^{2},\]

where \(\Sigma_{x}=\mathbf{E}[X_{s}X_{s}^{\top}]\) for \(X_{s}\sim N(\mu_{s},\sigma_{X}^{2}I)\).

We analyze each term in Eq (7) one by one.

(First term in Theorem 3) Recall the first term in Theorem 3

\[\mathbf{E}\bigg{[}\overline{\|\beta_{\cdot}\|}^{2}\bigg{|}n.\bigg{]}^{ \nicefrac{{1}}{{2}}}\mathbf{E}\bigg{[}\bigg{\langle}\tilde{\beta}_{s},\mu_{s}- \hat{\mu}_{s}\bigg{\rangle}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}.\]

From the Cauchy-Schwarz inequality, we have

\[\mathbf{E}\bigg{[}\overline{\|\beta_{\cdot}\|}^{2}\bigg{|}n. \bigg{]}^{\nicefrac{{1}}{{2}}}\leq \mathbf{E}\bigg{[}\overline{\|\beta^{*}\|}^{2}\bigg{|}n.\bigg{]} ^{\nicefrac{{1}}{{2}}}+\mathbf{E}\bigg{[}\bigg{(}\overline{\|\beta^{*}\|}- \overline{\|\beta_{\cdot}\|}\bigg{)}^{2}\bigg{|}n.\bigg{]}^{\nicefrac{{1}}{{2}}}\] \[\leq B+e_{\mathrm{norm}}.\]By definition, \(\tilde{\beta}_{s}=0\) for \(n_{s}\leq 18d\), which indicates that

\[\mathbf{E}\bigg{[}\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{\mu}_{s}\Big{\rangle} ^{2}\bigg{|}n_{.}\bigg{]}^{\nicefrac{{1}}{{2}}}=0.\]

In the case of \(n_{s}>18d\), by utilizing the fact \(\tilde{\beta}_{s}\) and \(\hat{\mu}_{s}\) are independent conditioned on \(n\). due to the sample spilitting and \(\tilde{\beta}_{s}\in\mathbb{S}_{d-1}\), we obtain

\[\mathbf{E}\bigg{[}\Big{\langle}\tilde{\beta}_{s},\mu_{s}-\hat{\mu}_{s}\Big{\rangle} ^{2}\bigg{|}n_{.}\bigg{]}\leq\sup_{v\in\mathbb{S}_{d-1}}\mathbf{E}\Big{[} \big{\langle}v,\mu_{s}-\hat{\mu}_{s}\big{\rangle}^{2}\Big{|}n_{.}\Big{]}=e_{ \mathrm{mean},s}^{2}.\]

Consequently, we have

\[\mathbf{E}\bigg{[}\big{\|}\widehat{\beta.}\big{\|}^{2}\bigg{|}n_{.}\bigg{]}^{ \nicefrac{{1}}{{2}}}\mathbf{E}\bigg{[}\Big{\langle}\tilde{\beta}_{s},\mu_{s}- \hat{\mu}_{s}\Big{\rangle}^{2}\bigg{|}n_{.}\bigg{]}^{\nicefrac{{1}}{{2}}}\leq( B+e_{\mathrm{norm}})e_{\mathrm{mean},s}\,1\,\{n_{s}>18d\}. \tag{32}\]

(Second term in Theorem 3) Recall the second term in Theorem 3

\[\sigma_{X}\mathbf{E}\bigg{[}\Big{(}\big{\|}\widehat{\beta.}\big{\|}-\big{\|} \widehat{\beta^{*}}\big{\|}\Big{)}^{2}\bigg{|}n_{.}\bigg{]}^{\nicefrac{{1}}{{2} }}.\]

Using the notation of \(e_{\mathrm{norm}}\), we have

\[\sigma_{X}\mathbf{E}\bigg{[}\Big{(}\big{\|}\widehat{\beta.}\big{\|}-\big{\|} \widehat{\beta^{*}}\big{\|}\Big{)}^{2}\bigg{|}n_{.}\bigg{]}^{\nicefrac{{1}}{{2} }}=\sigma_{X}e_{\mathrm{norm}}. \tag{33}\]

(Third term in Theorem 3) Recall the third term in Theorem 3

\[\sigma_{X}\overline{\|\beta^{*}\|}\mathbf{E}\bigg{[}\bigg{\|}\tilde{\beta}_{s} -\frac{\beta_{s}^{*}}{\|\beta_{s}^{*}\|}\bigg{\|}^{2}\bigg{|}n_{.}\bigg{]}^{ \nicefrac{{1}}{{2}}}.\]

Substituting \(e_{\mathrm{coef},s}\) yields

\[\sigma_{X}\overline{\|\beta^{*}\|}\mathbf{E}\bigg{[}\bigg{\|}\tilde{\beta}_{s} -\frac{\beta_{s}^{*}}{\|\beta_{s}^{*}\|}\bigg{\|}^{2}\bigg{|}n_{.}\bigg{]}^{ \nicefrac{{1}}{{2}}}=\sigma_{X}\overline{\|\beta^{*}\|}e_{\mathrm{coef},s}. \tag{34}\]

(Fourth term in Theorem 3) Recall the fourth term in Theorem 3

\[\mathbf{E}\left[\left(\sum_{s\in[M]}\hat{p}_{s}\Big{\langle}\hat{\beta}_{s}^{ \prime}-\beta_{s}^{*},\hat{\mu}_{s}^{\prime}\Big{\rangle}\right)^{2}\Bigg{|}n _{.}\right]^{\nicefrac{{1}}{{2}}}.\]

Due to the sample splitting, \(\hat{\beta}_{s}^{\prime}\) and \(\hat{\mu}_{s}^{\prime}\) are mutually independent. Also, we have \(\mathbf{E}[\langle\hat{\beta}_{s}^{\prime}-\beta_{s}^{*},\hat{\mu}_{s}^{\prime }\rangle]=0\). Hence,

\[\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s\in[M]}\hat{p}_{s}\Big{\langle} \hat{\beta}_{s}^{\prime}-\beta_{s}^{*},\hat{\mu}_{s}^{\prime}\Big{\rangle}^{2} \Bigg{|}n_{.}\Bigg{]}\] \[=\sum_{s\in[M]}\hat{p}_{s}^{2}\mathbf{E}\bigg{[}\Big{\langle} \hat{\beta}_{s}^{\prime}-\beta_{s}^{*},\hat{\mu}_{s}^{\prime}\Big{\rangle}^{2} \bigg{|}n_{.}\bigg{]}\] \[=\sum_{s\in[M]}\hat{p}_{s}^{2}\mathbf{E}\bigg{[}\Big{\langle} \hat{\beta}_{s}^{\prime}-\beta_{s}^{*},\hat{\mu}_{s}^{\prime}\Big{\rangle}^{2} \bigg{|}n_{.}\bigg{]}\,1\,\{n_{s}>12d\}.\]

Since \(\mathbf{E}[(\hat{\mu}_{s}^{\prime})(\hat{\mu}_{s}^{\prime})^{\top}]=\mu\mu^{ \top}+\frac{\sigma_{X}^{2}}{n_{2,s}^{2}}I=\Sigma_{s}-(1-\frac{1}{n_{2,s}^{2}} )\sigma_{X}^{2}I\), we have

\[\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s\in[M]}\hat{p}_{s}\Big{\langle}\hat{\beta}_{ s}^{\prime}-\beta_{s}^{*},\hat{\mu}_{s}^{\prime}\Big{\rangle}\Bigg{)}^{2}\Bigg{|}n_{.} \Bigg{]}\]\[=\sum_{s\in[M]}\hat{p}_{s}^{2}\mathbf{E}\Bigg{[}\Bigg{\|}\Bigg{(} \Sigma_{s}-\Bigg{(}1-\frac{1}{n_{s,s}^{2}}\Bigg{)}\sigma_{X}^{2}I\Bigg{)}^{\nicefrac {{1}}{{2}}}\Big{(}\hat{\beta}_{s}^{\prime}-\beta_{s}^{*}\Big{)}\Bigg{\|}^{2} \Bigg{|}n.\Bigg{\}}\,\mathbb{1}\{n_{s}>12d\}\] \[\leq\sum_{s\in[M]}\hat{p}_{s}^{2}\mathbf{E}\Bigg{[}\Big{\|}\Sigma_ {s}^{\nicefrac{{1}}{{2}}}\Big{(}\hat{\beta}_{s}^{\prime}-\beta_{s}^{*}\Big{)} \Big{\|}^{2}\Bigg{|}n.\Bigg{]}\,\mathbb{1}\{n_{s}>12d\}=\sum_{s\in[M]}\hat{p}_{ s}^{2}e_{\mathrm{coef}^{\prime},s}^{2}\,\mathbb{1}\{n_{s}>12d\}, \tag{35}\]

where we use the fact \(\Sigma_{s}\succeq\Sigma_{s}-(1-\frac{1}{n_{s,s}^{2}})\sigma_{X}^{2}I\), and for symmetric matrices \(A\) and \(B\) such that \(A\succeq B\), \(\langle v,Av\rangle\geq\langle v,Bv\rangle\) for any \(v\in\mathbb{R}\). For \(n_{s}\leq 18d\), the error is zero because \(\hat{\mu}_{s}^{\prime}=0\)

(Fifth term in Theorem 3) Recall the fifth term in Theorem 3

\[\mathbf{E}\Bigg{[}\Bigg{(}\sum_{s\in[M]}\hat{p}_{s}\langle\beta_{s}^{*},\hat{ \mu}_{s}^{\prime}-\mu_{s}\rangle\Bigg{)}^{2}\Bigg{|}n.\Bigg{]}^{\nicefrac{{1}} {{2}}}.\]

Since \(\hat{\mu}_{s}^{\prime}\) are independent, we have

\[=\sum_{s\in[M]}\hat{p}_{s}^{2}\mathbf{E}\Big{[}\langle\beta_{s}^{ *},\hat{\mu}_{s}^{\prime}-\mu_{s}\rangle^{2}\Big{|}n.\Big{]}\,\mathbb{1}\{n_{s }>12d\}\] \[\leq\sum_{s\in[M]}\hat{p}_{s}^{2}\|\beta_{s}^{*}\|^{2}\sup_{v\in \mathbb{S}_{d-1}}\mathbf{E}\Big{[}\langle v,\hat{\mu}_{s}^{\prime}-\mu_{s} \rangle^{2}\Big{|}n.\Big{]}\,\mathbb{1}\{n_{s}>12d\}+\frac{144d^{2}M^{2}}{n^{2}}\] \[\leq\sum_{s\in[M]}\hat{p}_{s}^{2}B^{2}e_{\mathrm{mean}^{\prime}, s}^{2}\,\mathbb{1}\{n_{s}>12d\}+o\bigg{(}\frac{1}{n}\bigg{)}. \tag{36}\]

(Sixth term in Theorem 3) Recall the sixth term in Theorem 3

\[\Bigg{|}\sum_{s^{\prime}\in[M]}(\hat{p}_{s^{\prime}}-p_{s^{\prime}})\langle \beta_{s^{\prime}}^{*},\mu_{s^{\prime}}\rangle\Bigg{|},\]

which is equivalent to \(e_{\mathrm{prob}}\).

By combining Theorem 3 and Eqs (32) to (36), we get

\[\mathbf{E}\Bigg{[}\Big{(}\hat{f}_{n}(X,S)-f_{\mathrm{DP}}^{*}(X, S)\Big{)}^{2}\Bigg{]}\] \[\qquad\leq\sum_{s\in[M]}p_{s}\mathbf{E}\Bigg{[}\Bigg{(}(B+e_{ \mathrm{norm}})e_{\mathrm{mean},s}\,\mathbb{1}\{n_{s}>18d\}+\sigma_{X}e_{ \mathrm{norm}}+\sigma_{X}\overline{\|\beta^{*}\|}e_{\mathrm{coef},s}\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\Bigg{(}\sum_{s^{\prime}\in [M]}\hat{p}_{s^{\prime}}^{2}e_{\mathrm{coef}^{\prime},s^{\prime}}^{2}\, \mathbb{1}\{n_{s^{\prime}}>12d\}\Bigg{)}^{\nicefrac{{1}}{{2}}}\] \[\qquad\qquad\qquad\qquad+\Bigg{(}\sum_{s^{\prime}\in[M]}\hat{p}_ {s^{\prime}}^{2}B^{2}e_{\mathrm{mean}^{\prime},s^{\prime}}^{2}\,\mathbb{1}\{n _{s^{\prime}}>12d\}+o\bigg{(}\frac{1}{n}\bigg{)}\Bigg{)}^{\nicefrac{{1}}{{2}}}+ e_{\mathrm{prob}}\Bigg{)}^{2}\Bigg{]}.\]

The triangle inequality gives that 

[MISSING_PAGE_EMPTY:31]

Synthesizing the results so far, there exists an universal constant \(C>0\) such that

\[\mathbf{E}\bigg{[}\Big{(}\hat{f}_{n}(X,S)-f^{*}_{\mathrm{DP}}(X,S) \Big{)}^{2}\bigg{]}\\ \leq\sum_{s\in[M]}Cp_{s}\Bigg{(}\frac{\sigma_{X}^{2}B^{2}}{p_{s}n} +\frac{\sigma_{\xi}^{2}Md}{n}+\frac{\sigma_{X}^{2}B^{2}}{n}+\frac{\sigma_{\xi} ^{2}\|\overline{\beta^{*}}\|^{2}d}{\|\beta^{*}_{s}\|^{2}p_{s}n}\Bigg{)}\\ +\sum_{s\in[M]}C\bigg{(}\frac{p_{s}\sigma_{X}^{2}}{n}+\frac{p_{s} \sigma_{X}^{2}}{n}\bigg{)}+C\frac{B^{2}U^{2}}{n}+o\bigg{(}\frac{1}{n}\bigg{)}.\]

Consequently, there exists an universal constant \(C>0\) such that

\[\mathbf{E}\bigg{[}\Big{(}\hat{f}_{n}(X,S)-f^{*}_{\mathrm{DP}}(X,S) \Big{)}^{2}\bigg{]}\leq\\ C\Bigg{(}\frac{\sigma_{X}^{2}B^{2}M}{n}+\frac{\sigma_{\xi}^{2} Md}{n}+\frac{\sigma_{X}^{2}B^{2}}{n}+\frac{\sigma_{\xi}^{2}B^{2}Md}{n}+\frac{ \sigma_{X}^{2}}{n}+\frac{\sigma_{X}^{2}}{n}+\frac{B^{2}U^{2}}{n}\Bigg{)}+o \bigg{(}\frac{1}{n}\bigg{)}.\]

Then, the dominating terms match the claim. 

## Appendix G Details of Lower Bound Analyses

This section provides the proofs of the lower bound analyses results.

### Proof of Lower Bound in Theorem 1

**Theorem 22**.: _If \(M(d-1)>16\), there exists an universal constant \(C>0\) such that for any \(\alpha>0\) and \(\delta\in(0,1)\),_

\[\mathcal{E}_{n}(\alpha,\delta)\geq C\frac{\sigma_{\xi}^{2}B^{2}dM}{n}-o\bigg{(} \frac{1}{n}\bigg{)}.\]

Proof of Theorem 22.: The Varshamov-Gilbert bound guarantees that there exists a subset \(\mathcal{V}^{\prime}\subseteq\mathcal{V}\) such that \(|\mathcal{V}^{\prime}|\geq 2^{M(d-1)/8}\) and \(d_{H}(v_{s},v^{\prime}_{s})\geq(d-1)/8\) for any \(v,v^{\prime}\in\mathcal{V}^{\prime}\). With the choice of \(\epsilon_{s}^{2}=(\frac{d-1}{16}-\frac{1}{M})\sigma_{\xi}^{2}/2\sigma_{X}^{2} B_{s}^{2}n_{s}\), we confirm by Theorem 8 that \(\inf_{\pi}\frac{1}{K}\sum_{v\in\mathcal{V}^{\prime}}D_{\mathrm{KL}}\big{(}\pi_{ \theta_{v}|n}.,\pi\big{)}\leq\max_{v,v^{\prime}\in\mathcal{V}^{\prime}}D_{ \mathrm{KL}}\big{(}\pi_{\theta_{v^{\prime}|n}.},\pi\big{)}\leq\ln(|\mathcal{V} ^{\prime}|/4)/2\geq M(d-1)/16-1\). From Theorem 8 and the fact \(d_{H}(v_{s},v^{\prime}_{s})\geq(d-1)/8\), we can apply Theorem 6 with \(\epsilon=\sum_{s\in[M]}p_{s}\frac{(\sum_{\mathcal{V}^{\prime}\in[M]}p_{s}B_{s })^{2}}{B_{s}^{2}}(\frac{d-1}{16}-\frac{1}{M})\sigma_{\xi}^{2}/2n_{s}\). From the fact that \(\mathbf{E}[\frac{1}{n_{s}+1}]=\frac{1}{p_{s}(n+1)}(1-(1-p_{s})^{n})\) due to [5], there exists an universal constant \(C>0\) such that \(\mathbf{E}[\frac{\epsilon}{2}]\geq C(\frac{1}{M}\sum_{s\in[M]}\frac{(\sum_{ \mathcal{V}^{\prime}\in[M]}p_{s}B_{s^{\prime}})^{2}}{B_{s}^{2}})\frac{\sigma_{ \xi}^{2}Md}{n}-o(\frac{1}{n})\) We can get the claim by confirming that there exists \(B_{1},...,B_{M}\) such that \((\frac{1}{M}\sum_{s\in[M]}\frac{(\sum_{\mathcal{V}^{\prime}\in[M]}p_{s}B_{s^{ \prime}})^{2}}{B_{s}^{2}})=B^{2}\) and \(B_{s}\leq B\). Because for \(B_{2}=...=B_{M}=B\), tending \(B_{1}\) to 0 results in \(\big{(}\frac{1}{M}\sum_{s\in[M]}\frac{(\sum_{\mathcal{V}^{\prime}\in[M]}p_{s}B_ {s^{\prime}})^{2}}{B_{s}^{2}}\big{)}\) goes infinity, it is confirmed. 

### Proof of Theorem 6

Proof of Theorem 6.: Since the distribution of \(n.\) is invariant against \(\theta\in\Theta\), we have

\[\sup_{\theta\in\Theta}\mathbf{E}_{\theta}\Big{[}\mathcal{E}(\hat {f}_{n};\theta)\Big{]}\] \[= \mathbf{E}\bigg{[}\sup_{\theta\in\Theta}\mathbf{E}_{\theta} \Big{[}\mathcal{E}(\hat{f}_{n};\theta)\Big{|}n.\Big{]}\bigg{]}\] \[\geq \mathbf{E}\bigg{[}\max_{\theta\in\Theta}\mathbf{E}_{\theta}\Big{[} \mathcal{E}(\hat{f}_{n};\theta)\Big{|}n.\Big{]}\bigg{]}\]\[\geq \mathbf{E}\left[\frac{1}{|\hat{\Theta}|}\sum_{\theta\in\hat{\Theta}} \mathbf{E}_{\theta}\Big{[}\mathcal{E}(\hat{f}_{n};\theta)\Big{|}n.\Big{]}\right].\]

Given \(\epsilon\) possibly dependent on \(n.\), application of the Markov inequality yields

\[\sup_{\theta\in\hat{\Theta}}\mathbf{E}_{\theta}\Big{[}\mathcal{E} (\hat{f}_{n};\theta)\Big{]}\] \[\geq \mathbf{E}\left[\frac{\epsilon}{|\hat{\Theta}|}\sum_{\theta\in \hat{\Theta}}\mathbb{P}_{\theta}\Big{\{}\mathcal{E}(\hat{f}_{n};\theta)\geq \epsilon\Big{|}n.\Big{\}}\right].\]

If \(\inf_{f}\mathcal{E}(f;\theta)\vee\mathcal{E}(f;\theta^{\prime})\geq\epsilon\) for any \(\theta,\theta^{\prime}\in\hat{\Theta}\), \(\mathcal{E}(f;\theta)<\epsilon\) implies \(\mathcal{E}(f;\theta^{\prime})\geq\epsilon\) for any \(\theta^{\prime}\in\hat{\Theta}\) such that \(\theta\neq\theta^{\prime}\). Hence, there exists a partion \(\{\mathcal{F}_{\theta}\}_{\theta\in\hat{\Theta}}\) of all the measurable functions \(f:\mathbb{R}^{d}\times[M]\rightarrow\mathbb{R}\) such that \(\{f:\mathcal{E}(f;\theta)<\epsilon\}\subseteq\mathcal{F}_{\theta}\) for all \(\theta\in\hat{\Theta}\). Consequently, we have

Application of the Fano's inequality and data processing inequality yields the claim. 

### Proof of Theorem 7

To prove Theorem 7, we show the following more tight lower bound.

**Theorem 23**.: _Let \(\theta\) and \(\theta^{\prime}\) be the parameters of the distributions such that \(\frac{1}{2\sigma_{X}^{2}}\norm{\mu_{s}-\mu_{s}^{\prime}}^{2}\coloneqq d_{s}<1\) for all \(s\in[M]\). Then, we have_

\[\inf_{f\in\mathcal{L}^{2}}\mathcal{E}(f;\theta)\vee\mathcal{E}(f; \theta^{\prime})\geq\\ \sum_{s\in[M]}p_{s}\frac{e^{-d_{s}}}{4}\bigg{(}\sigma_{X}^{2} \bigg{\|}\frac{\overline{\|\beta.\|}\beta_{s}}{\|\beta_{s}\|}-\frac{\overline{ \|\beta^{\prime}\|}\beta_{s}^{\prime}}{\|\beta_{s}^{\prime}\|}\bigg{\|}^{2} \bigg{(}1+\frac{\|\mu_{s}-\mu_{s}^{\prime}\|^{2}}{4\sigma_{X}^{2}}\bigg{)}^{ 1+\frac{d}{2}}\\ +\bigg{(}-\bigg{\langle}\frac{\overline{\|\beta.\|}\beta_{s}}{\| \beta_{s}\|}+\frac{\overline{\|\beta^{\prime}\|}\beta_{s}^{\prime}}{\|\beta_{ s}^{\prime}\|},\frac{\mu_{s}-\mu_{s}^{\prime}}{2}\bigg{\rangle}\\ +\sum_{s^{\prime}\in[M]}p_{s^{\prime}}\bigg{(}\langle\beta_{s^{ \prime}}-\beta_{s^{\prime}}^{\prime},\bar{\mu}_{s^{\prime}}\rangle+\bigg{\langle} \beta_{s^{\prime}}+\beta_{s^{\prime}}^{\prime},\frac{\mu_{s^{\prime}}-\mu_{s^ {\prime}}^{\prime}}{2}\bigg{\rangle}\bigg{)}\bigg{)}^{2}\bigg{(}1+\frac{\|\mu_ {s}-\mu_{s}^{\prime}\|^{2}}{4\sigma_{X}^{2}}\bigg{)}^{\frac{d}{2}}\bigg{)}.\]

Theorem 23 immediately gives Theorem 7.

We utilize the sufficient condition for the constrained optimization problem over a Banach space. Let \(Z\) be a Banach space. We say a function \(f:Z\rightarrow\mathbb{R}\) is _Gateaux differentiable_ if the limit \(\lim_{\tau\to 0}\frac{f(z+\tau u)-f(z)}{\tau}\) exists for any open set \(U\subseteq Z\), any \(z\in U\), and any \(u\in Z\). We denote the Gateaux derivative of \(f\) at \(z\in Z\), a linear mapping from \(u\in Z\) to \(\lim_{\tau\to 0}\frac{f(z+\tau u)-f(z)}{\tau}\), as \(D_{G}f(z)\). We abuse \(0\) to denote the mapping that always outputs \(0\).

Proof of Theorem 23.: Let \(q_{s}\) and \(q_{s}^{\prime}\) be the density function of \(X_{s}\) with the parameters \((\beta.,\mu.)\) and \((\beta^{\prime},\mu^{\prime})\), respectively, regarding the base measure \(\lambda\). Since \(X_{s}\) follows the Gaussian distribution, we can choose \(\lambda\) as the Lebesgue measure. Given \(\eta\in[0,1]\), we have

\[\mathcal{E}(f;\beta.,\mu.)\vee\mathcal{E}(f;\beta^{\prime},\mu^ {\prime}.)\] \[\geq \eta\mathcal{E}(f;\beta.,\mu.)+(1-\eta)\mathcal{E}(f;\beta^{ \prime},\mu^{\prime}.)\] \[=\sum_{s\in[M]}p_{s}\int\Big{(}\eta(f(x,s)-f_{\beta.,\mu.}(x,s) )^{2}q_{s}(x)+(1-\eta)\big{(}f(x,s)-f_{\beta^{\prime},\mu^{\prime}}(x,s) \big{)}^{2}q_{s}^{\prime}(x)\Big{)}\lambda(dx).\]

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_FAIL:35]

where we use the fact that the hypergeometric function \({}_{2}F_{1}(a,b,b;z)=\sum_{m=0}^{\infty}\frac{\Gamma(a+m)}{\Gamma(a)}\frac{z^{m}}{m!}=(1-z)^{-a}\) for some \(b\), provided \(|z|<1\). By setting

\[v =\frac{\overline{\|\beta.\|}\beta_{s}}{\|\beta_{s}\|}-\frac{ \overline{\|\beta^{\prime}.\|}\beta_{s}^{\prime}}{\|\beta_{s}^{\prime}\|}\] \[c =-\left\langle\frac{\overline{\|\beta.\|}\beta_{s}}{\|\beta_{s} \|}+\frac{\overline{\|\beta^{\prime}.\|}\beta_{s}^{\prime}}{\|\beta_{s}^{ \prime}\|},\frac{\mu_{s}-\mu_{s}^{\prime}}{2}\right\rangle\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\sum_{s^{ \prime}\in[M]}p_{s^{\prime}}\bigg{(}\langle\beta_{s^{\prime}}-\beta_{s^{\prime }}^{\prime},\bar{\mu}_{s^{\prime}}\rangle+\bigg{\langle}\beta_{s^{\prime}}+ \beta_{s^{\prime}}^{\prime},\frac{\mu_{s^{\prime}}-\mu_{s^{\prime}}^{\prime}} {2}\bigg{\rangle}\bigg{)}\] \[c^{\prime} =\frac{1}{2\sigma_{X}^{2}}\|\mu_{s}-\mu_{s}^{\prime}\|\] \[k =\frac{d}{2},\text{ and }\theta=2\sigma_{X}^{2},\]

we have

\[\mathbf{E}\Bigg{[}\frac{(f_{\beta_{s},\mu}(\bar{X}_{s},s)-f_{ \beta^{\prime},\mu^{\prime}}(\bar{X}_{s},s))^{2}}{\cosh(\frac{1}{2\sigma_{X}^{ 2}}\|\bar{X}_{s}-\bar{\mu}_{s}\|\|\mu_{s}-\mu_{s}^{\prime}\|)}\Bigg{]}\] \[= \tag{38}\]

Combining Eqs (37) and (38) yields the claim. 

### Proof of Theorem 8

Proof of Theorem 8.: It is easy to check that \(d_{s}=0\), and for any \(v,v^{\prime}\in\mathcal{V}\),

\[\Bigg{\|}\frac{\overline{\|\beta_{v_{i}}\|}\beta_{v,s}}{\|\beta_{ v,s}\|}-\frac{\overline{\|\beta_{v^{\prime}}.\|}\beta_{v^{\prime},s}}{\|\beta_{v ^{\prime},s}\|}\Bigg{\|}^{2}\] \[= \Bigg{(}\sum_{s^{\prime}\in[M]}p_{s^{\prime}}\|\beta_{v,s}\|\Bigg{)} ^{2}\bigg{\|}\frac{\beta_{v,s}}{\|\beta_{v,s}\|}-\frac{\beta_{v^{\prime},s}}{ \|\beta_{v^{\prime},s}\|}\Bigg{\|}^{2}\] \[= \Bigg{(}\sum_{s^{\prime}\in[M]}p_{s^{\prime}}\|\beta_{v,s}\|\Bigg{)} ^{2}\Bigg{(}\sum_{i\in[d-1]}\frac{\epsilon_{s}^{2}}{d-1}\big{(}v_{s,i}-v_{s,i} ^{\prime}\big{)}^{2}\Bigg{)}\] \[= 4\Bigg{(}\sum_{s^{\prime}\in[M]}p_{s^{\prime}}B_{s}\Bigg{)}^{2} \frac{\epsilon_{s}^{2}}{d-1}d_{H}(v_{s},v_{s}^{\prime}).\]

Since the density function of the Gaussian distribution is \(L^{2}\) integrable, \(\mathcal{E}(f;\theta)=\infty\) if \(f\) is not \(L^{2}\) integrable. Hence, \(\inf_{f}\mathcal{E}(f;\theta_{v})\vee\mathcal{E}(f;\theta_{v^{\prime}})=\inf_{ f\in\mathcal{L}^{2}}\mathcal{E}(f;\theta_{v})\vee\mathcal{E}(f;\theta_{v^{\prime}})\), and we thus can apply Theorem 7. Then, we have

\[\inf_{f}\mathcal{E}(f;\theta_{v})\vee\mathcal{E}(f;\theta_{v^{\prime}})\geq\sum_ {s\in[M]}p_{s}\Bigg{(}\sum_{s^{\prime}\in[M]}p_{s^{\prime}}B_{s}\Bigg{)}^{2} \frac{\sigma_{X}^{2}\epsilon_{s}^{2}}{d-1}d_{H}(v_{s},v_{s}^{\prime})\]

Conditioned on \(n.\), the KL-divergence between \(\pi_{\theta_{v}|n.}\) and \(\pi_{\theta_{v^{\prime}}|n.}\) is obtained as

\[\sum_{s\in[M]}n_{s}\Bigg{(}\frac{1}{2\sigma_{X}^{2}}\|\mu_{v,s}-\mu_{v^{\prime},s}\|^{2}+\frac{\sigma_{X}^{2}}{2\sigma_{\xi}^{2}}\|\beta_{v,s}-\beta_{v^{ \prime},s}\|^{2}+\frac{1}{2\sigma_{\xi}^{2}}\langle\mu_{v,s},\beta_{v,s}-\beta _{v^{\prime},s}\rangle^{2}\Bigg{)}.\]

[MISSING_PAGE_EMPTY:37]